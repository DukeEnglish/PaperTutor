RaTEScore: A Metric for Radiology Report Generation
WeikeZhao1,2,ChaoyiWu1,2,XiaomanZhang1,2,
YaZhang1,2,YanfengWang1,2,†,WeidiXie1,2,†,
1ShanghaiJiaoTongUniversity,2ShanghaiAILaboratory
https://angelakeke.github.io/RaTEScore/
Abstract
This paper introduces a novel, entity-aware
metric,termedasRadiologicalReport(Text)
Evaluation (RaTEScore), to assess the qual-
ityofmedicalreportsgeneratedbyAImodels.
RaTEScore emphasizes crucial medical enti-
tiessuchasdiagnosticoutcomesandanatomi-
caldetails,andisrobustagainstcomplexmed-
ical synonyms and sensitive to negation ex-
pressions. Technically, we developed a com-
prehensivemedicalNERdataset,RaTE-NER,
andtrainedanNERmodelspecificallyforthis
purpose. This model enables the decomposi-
tionofcomplexradiologicalreportsintocon-
stituent medical entities. The metric itself is
derivedbycomparingthesimilarityofentity
embeddings,obtainedfromalanguagemodel,
based on their types and relevance to clini-
Figure1: Existingevaluationmetrics. Weillustrate
calsignificance. Ourevaluationsdemonstrate
thelimitationsofcurrentmetrics. Blueboxesrepresent
that RaTEScore aligns more closely with hu-
ground-truthreports;redandyellowboxesindicatecor-
manpreferencethanexistingmetrics,validated
rectandincorrectgeneratedreports,respectively. The
bothonestablishedpublicbenchmarksandour
examplesshowthatthesemetricsfailtoidentifyoppo-
newlyproposedRaTE-Evalbenchmark.
sitemeaningsandsynonymsinthereportsandareoften
disturbedbyunrelatedinformation.
1 Introduction
Withthegeneraladvancementinnaturelanguage thesemetricsfailtocapturenegationorsynonyms
processing(OpenAI,2023;Aniletal.,2023;Qiu in sentences, thus neglecting the semantic fac-
et al., 2024; Wu et al., 2024) and computer vi- tuality; (ii) Embedding Similarity Metrics, like
sion(Lietal.,2023;Alayracetal.,2022;OpenAI; BERTScore (Zhang et al., 2019), provide better
Zhangetal.,2023),thepursuitofgeneralistmed- semanticawarenessbutfailtoemphasizekeymed-
ical artificial intelligence has grown increasingly ical terms, leading to overlooked errors in criti-
promising and appealing (Moor et al., 2023; Wu calconclusions;(iii)MetricsbasedonNamedEn-
etal.,2023;Tuetal.,2024). Yet, thecomplexity tityRecognition(NER),suchasRadGraphF1(Yu
and specialized nature of clinical free-form texts, et al., 2023a) and MEDCON (Yim et al., 2023).
suchasradiologyreportsanddischargesummaries, Although tailored for the medical domain, they
presentsubstantialchallengesinevaluatingmedi- strugglewithsynonymunificationandaretypically
calfoundationmodels. restricted to analyzing Chest X-ray reports; (iv)
In the existing literature, four main types of Metricsrelyingonlargelanguagemodels(LLMs),
metrics have been adopted to assess the similar- such as those proposed by Wei et al.(Wei et al.,
ity between free-form texts in medical scenarios, 2024)andLiuetal.(Liuetal.,2023). Whilethese
as shown in Figure 1. These include: (i) Word metricsarebetteralignedwithhumanpreferences,
Overlap Metrics, such as BLEU (Papineni et al., theysufferfrompotentialsubjectivebiasesandare
2002) and ROUGE (Lin, 2004). While intuitive, prohibitivelycostlyforlarge-scaleevaluation.
1
4202
nuJ
42
]LC.sc[
1v54861.6042:viXraIn this study, we aim to develop a metric that disambiguationencoding(Sec.2.3),andthefinal
prioritizeskeymedicalentitiessuchasdiagnostic scoring procedure (Sec. 2.4). Lastly, we present
outcomesandanatomicalfeatures,whileexhibiting thedetailsfortrainingandevaluationateachstage.
robustnessagainstcomplexmedicalsynonymsand
2.1 GeneralPipeline
sensitivitytonegationexpressions. Wepresenttwo
contributions: First, weintroduceRaTEScore, a Giventworadiologicalreports,oneistheground
novel evaluation metric specifically designed for truth for reference, denoting as x, and the other
radiologyreports. Thismetricemphasizesentity- candidate for evaluation as xˆ. We aim to define
level assessments across various imaging modal- a new similarity metric S(x,xˆ), that enables to
ities and body regions. Specifically, it starts by comparetworadiologicalreportsattheentitylevel,
identifying medical entities and their types (e.g., thusbetterreflectingtheirclinicalconsistency.
anatomy,disease)fromeachsentence. Toaddress AsshowninFigure2,ourpipelinecontainsthree
thechallengesassociatedwithmedicalsynonyms, majorcomponents: namely,amedicalentityrecog-
we compute entity embeddings using a synonym nitionmodule(Φ (·)),asynonymdisambigua-
NER
disambiguationmoduleandassesstheircosinesim- tionencodingmodule(Φ (·)),andafinalscor-
ENC
ilarities. RaTEScore then calculates a final score ing module (Φ (·)). First, we extract the medi-
SIM
basedonweightedsimilaritiesthatemphasizethe cial entities from each piece of radiological text,
clinicalimportanceoftheentitytypesinvolved. thenencodeeachentityintoembeddingsthatare
Second, we have developed a comprehensive awareofmedicalsynonym,formulatedas:
medicalnamed-entityrecognition(NER)dataset,
F = Φ (Φ (x)), (1)
RaTE-NER, which encompasses data from 9 ENC NER
modalitiesand22anatomicalregions,derivedfrom
whereFcontainsasetofentityembeddings. Simi-
MIMIC-IV and Radiopaedia. In addition, we in-
larly,wecangetFˆ forxˆ. Then,wecancalculate
troduceRaTE-Eval,anovelbenchmarkdesigned
thefinalsimilarityontheentityembeddingsas:
toassessmetricsacrossvariousclinicaltexts. This
benchmark is structured around three sub-tasks: S(x,xˆ) = Φ (F,Fˆ) (2)
SCO
Sentence-levelHumanRating,Paragraph-levelHu-
manRating,andComparisonofSyntheticReports, Inthefollowingsections,wewilldetaileachofthe
eachtargetingspecificevaluationchallenges. Both components.
theRaTE-NERdatasetandtheRaTE-Evalbench-
2.2 MedicalNamedEntityRecognition
mark will be made publicly available, aiming to
fosterthedevelopmentofmoreeffectiveevaluation In the medical named entity recognition module,
metricswithinthefieldofmedicalinformatics. ourgoalistodecomposeeachradiologicalreport
Third, our extensive experiments demonstrate byidentifyingasetofentities:
the superiority of our proposed RaTEScore. We
Φ (x) = {e ,e ,...,e }
initiallytestedourmetricagainstothermetricson NER 1 2 M
the public dataset ReXVal (Yu et al., 2023a), it = {(n ,t ),(n ,t ),...(n ,t )}.
1 1 2 2 M M
showingsuperioralignmentwithhumanpreference.
GivenReXVal’slimitationtochestX-rayreports, Similarly,wecanalsogetΦ NER(xˆ) = {eˆ 1,eˆ 2,...,
further testing was conducted on the diverse sub- eˆ N}, where M,N denote the total number of en-
tasksofRaTE-Eval,whereRaTEScoreconsistently tities extracted from each text respectively. Each
outperformedothermetrics. Weconductedablation entity e i is defined as a tuple (n i,t i), where n i
studies to validate the effectiveness of individual is the name of the entity and t i denotes its corre-
componentsofourpipeline. sponding type. For instance, the tuple (‘pneumo-
nia’,‘Disease’)representstheentity‘pneumonia’
2 Methods categorizedundertheentitytype‘Disease’.
Overall, we categorize the entity types into
In this section, we start by properly formulating five distinct groups within radiological contexts:
theproblem,andintroducethepipelineofourmet- {Anatomy,Abnormality,Disease,Non-Abnormality,
ric(Sec.2.1). Then,wedetaileachofthemodule Non-Disease}. Specifically,‘Abnormality’refers
for our metric computation, for example, medi- tonotableradiologicalfeaturessuchasmasses,ef-
calnamedentityrecognition(Sec.2.2),synonym fusion,andedema. Conversely,‘Non-Abnormality’
2Figure2: IllustrationoftheComputationofRaTEScore. Givenareferenceradiologyreportx, acandidate
radiologyreportxˆ,wefirstextractthemedicalentityandthecorrespondingentitytype. Then,wecomputethe
entityembeddingandfindthemaximumcosinesimilarity. TheRaTEScoreiscomputedbytheweightedsimilarity
scoresthatconsiderthepairwiseentitytypes.
DataSource TrainSet DevSet TestSet MIMIC-IV
TrainSet DevSet TestSet
MIMIC-IV 10588 1323 1324 Anatomy 9034(4314) 1188(828) 1140(765)
Radiopaedia 30005 3600 3529 Abnormality 5579(4047) 760(657) 605(513)
TotalReports 40593 4923 4853 Non-Abnormaliy 4182(1528) 479(274) 514(253)
Disease 1675(1220) 189(169) 178(164)
Table1: RaTE-NERDatasetStatistics(Report-level):The Non-Disease 3482(965) 424(268) 457(264)
numberofsentencesfrommedicalreportsofeachdatasource. Radiopaedia
TrainSet DevSet TestSet
Anatomy 34110(14051) 4145(2629) 4471(2889)
Abnormality 33863(23352) 4021(3386) 4265(3365)
Non-Abnormaliy 3878(2280) 473(325) 605(420)
denotescaseswheresuchabnormalitiesarenegated
Disease 9639(7385) 1118(1044) 741(659)
inthecontext,asillustratedbytheclassificationof
Non-Disease 2467(1540) 268(220) 183(142)
‘pleuraleffusion’inthestatement‘Noevidenceof
TotalEntities 107909(60682) 13065(9800) 13159(9434)
pleuraleffusion’.
Table2: RaTE-NERDatasetStatistics(Entity-level):The
RaTE-NER Dataset. To support the develop-
numbersoutsideandinsidethebracketsdenotethetotalnum-
mentofourmedicalentityrecognitionmodule,we berofentitiesforeachtype,andthenumberofnon-duplicate
haveconstructedtheRaTE-NERdataset,alarge- entities,respectively.
scale,radiologicalnamedentityrecognition(NER)
dataset,including13,235manuallyannotatedsen-
2.3 SynonymDisambiguationEncoding
tences from 1,816 reports within the MIMIC-IV
database,thatspans9imagingmodalitiesand23 To address the challenge from synonyms in the
anatomicalregions,ensuringcomprehensivecov- evaluationprocess,suchasreconcilingtermslike
erage. GiventhatreportsinMIMIC-IVaremore “lung”and“pulmonary”,weproposetomapeach
likelytocovercommondiseases,andmaynotwell entity name into embedding space, where syn-
representrarerconditions,wefurtherenrichedthe onyms are positioned closely together, utilizing
datasetwith33,605sentencesfromthe17,432re- amedicalentityencodingmoduletrainedwithex-
portsavailableonRadiopaedia(Rad),byleverag- tensive medical knowledge. This module, repre-
ingGPT-4andothermedicalknowledgelibraries sented as: f i = Φ ENC(n i), with f i denotes the
tocaptureintricaciesandnuancesoflesscommon vector embedding for the entity name. Conse-
diseases and abnormalities. More details can be quently,wecompiletheseintoasetofentityem-
foundintheAppendixA.2. Wemanuallylabeled beddings: F = {(f 1,t 1),(f 2,t 2),...}. A simi-
3,529 sentences to create a test set, as shown in
larset,Fˆ,isconstructedforthecandidatetextus-
Table2andTable1,theRaTE-NERdatasetoffers ingthesamemethodology. Forthisencodingpro-
alevelofgranularitynotseeninpreviousdatasets, cess,Weadoptanoff-shelfretrievalmodel,namely,
withcomprehensiveentityannotationswithinsen- BioLORD (Remy et al., 2024), which is trained
tences, that enables to train models for medical specificallyonmedicalentity-definitionpairsand
entityrecognitionwithinouranalyticalpipeline. hasproveneffectiveinmeasuringentitysimilarity.
32.4 ScoringProcedure S(x,xˆ)×S(xˆ,x)
RaTEScore = 2× (3)
S(x,xˆ)+S(xˆ,x)
Upon obtaining the encoded entity set from each
decomposedradiologicalreport,weproceedtothe
2.5 ImplementationDetails
final scoring procedure. We first define the simi-
In this section, we describe the implementation
laritymetricbetweenacandidateentityandaref-
details for the three key modules. First, for med-
erence report, that is established by selecting an
ical named entity recognition, we train a BERT-
entityfromthereferencedtextbasedonthecosine
liked model on RaTE-NER dataset with two
similarityoftheirnameembeddings:
main-stream NER architectures, namely, Span-
i∗ = argmaxcos(f i,fˆ j), basedandIOB-basedmodels. FortheSpan-based
i≤M
method,wefollowthesettingofPURE(thePrince-
where cos(f ,fˆ) measures the cosine similarity tonUniversityRelationExtractionsystem)entity
i j
betweentwoentitynameembeddings. Theentity model (Zhong and Chen, 2020) and for the IOB-
e ,whichbestmatcheseˆ fromthereferencetext, basedmethod,wefollowDeBERTav3(Heetal.,
i∗ j
ischosenforfurthercomparison. Theoverallsimi- 2022, 2020). We show more detailed implemen-
larityscore,S(x,xˆ),isthencomputedasfollows: tationparametersforthetwotrainingschemesin
(cid:80) AppendixA.9. Additionally,wealsotrytoinitial-
W(t ,t )·sim(e ,eˆ )
j i∗ j i∗ j
S(x,xˆ) = , izetheNERmodelwithdifferentpre-trainedBERT.
(cid:80)
W(t ,t )
j i∗ j Morecomparisonofthetwotrainingschemesand
Here,W isalearnable5×5affinitymatrixbetween differentBERTinitializationswillbepresentinthe
thefiveentitytypes,whereW(t i,t j)representsan ablation study. Second, For the synonym disam-
elementofthematrix,andsim(e i,eˆ j)isanentity- biguation encoding, we directly use the off-shelf
wisesimilarityfunction,definedas: BioLORD-2023-Cmodelversion. Ablationstud-
(cid:40) pcos(f ,fˆ), if t ̸= t iesarealsoconductedinSection4. Third,forthe
i j i j
sim(e ,eˆ ) = ,
i j cos(f ,fˆ), if t = t finalscoringmodule,welearntheaffinitymatrix
i j i j
W and negative penalty factor p leveraging TPE
where we generally follow the cosine similarity (Tree-structuredParzenEstimator)(Bergstraetal.,
onthenameembedding,withalearnablepenalty 2011)withasmallsetofhumanratingdata.
value p to punish the type mismatch. For ex-
3 RaTE-EvalBenchmark
ample, when comparing entities with identical
namesbutdifferenttypes—suchas(‘pleuraleffu- Toeffectivelymeasurethealignmentbetweenau-
sion’,‘Abnormality’)and(‘pleuraleffusion’,‘Non- tomaticevaluationmetricsandradiologists’assess-
Abnormality’)—the penalty term p is applied to ments in medical text generation tasks, we have
adjustthesimilarityscoreappropriately. established a comprehensive benchmark, RaTE-
Additionally,thesimilaritybetweendifferenten- Eval, that encompasses three tasks, each with its
titytypesmaybeweighteddifferentlyinmedical officialtestsetforfaircomparison,asdetailedbe-
scenarios due to their clinical significance. For low. The comparison of RaTE-Eval Benchmark
example, the similarity between two ‘Abnormal- andexistedradiologyreportevaluationBenchmark
ity’entitiesisofmuchgreaterimportancethanthe islistedinTable3.
similaritybetweentwo‘Non-abnormality’entities.
Sentences-level Human Rating. Existing stud-
This is because all body parts are assumed to be
ies have predominantly utilized the ReXVal
normalinradiologyreportsbydefault,andminor
dataset(Yuetal.,2023b),whichrequiresradiolo-
expressionerrorsinnormalfindingswillnotcriti-
gistannotatorstoidentifyandcounterrorsinvar-
callyimpactthereport’scorrectness. Therefore,we
ious potential categories. The metric’s quality is
introduceW toaccountforthisclinicalrelevance.
assessedbytheKendallcorrelationcoefficientbe-
Finally,duetotheorderofperformingmaxin-
tweenthetotalnumberoferrorsandtheresultfrom
dexingandmeanpooling,thefinialsimilaritymet-
automaticmetric. Thepossibleerrorcategoriesare
ricS(x,xˆ)doesnotcomplywiththecommutative
asfollows:
law. S(x,xˆ)andS(xˆ,x)canbeanalogoustopre-
1.Falsepredictionoffinding;
cisionandrecallrespectively. Thus,totakecareof
both,ourfinalRaTEScoreisdefinedfollowingthe 2.Omissionoffinding;
classicalF -scoreformat,as: 3.Incorrectlocation/positionoffinding;
1
4Number Type ScoringPrinciple DataSource Modality Anatomy
ReXValDataset 200 Sent.+Para. ErrorCount MIMIC_CXR 1(X-ray) 1(Chest)
Sent.level 2215 Sent. ErrorCount/PotentialErrors
Ours Para.level 1856 Para. 5-PointScoringSystem MIMIC_IV 9 22
Sim.Report 847 Sent. Mistral8*7B
Table3: ComparisonofRaTE-EvalBenchmarkandexistedradiologyreportevaluationBenchmark.
4.Incorrectseverityoffinding; referstothecandidatereport’serrorbasedonthe
5.Mentionthecomparisonthatisabsent referencereport.
fromthereferenceimpression; Thefinalhumanratingresultiscomputedbydi-
vidingthetotalerrorcountbythethenumberofpo-
6.Omissionofcomparisondescribinga
tentialerrors. Thefinalsentence-levelbenchmark
changefromapreviousstudy.
is composed of 2215 reference report sentences,
Building on this framework, we introduce two
candidate report sentences and their rating score.
improvements to enhance the robustness and ap-
Forparametersearch(Sec.2.5),wedividedallre-
plicability of our benchmark: (1) normalization
portsintoatrainingsetandatestsetatan8:2ratio,
oferrorcounts: recognizingthatasimplecount
toidentifythemosteffectiveparametersthatalign
of errors may not fairly reflect the informational
withhumanscoringrules.
contentinsentences,wehaveadaptedthescoring
toannotatethenumberofpotentialerrors. This Paragraph-levelHumanRating. Giventhatmed-
icalimaginginterpretationcommonlyinvolvesthe
approachnormalizesthecounts,ensuringamore
evaluation of lengthy texts rather than isolated
balanced assessment across varying report com-
plexities. (2)diversificationofmedicaltexts: un- sentences,wealsoincorporateparagraph-levelas-
sessments into ouranalysis, from the MIMIC-IV
likeexistingbenchmarksthatarelimitedtoChest
reports. However, as it is challenging for hu-
x-rays from the MIMIC-CXR dataset (Johnson
etal.,2019),ourdatasetincludes2215reportsspan- mans to completely count all errors in long para-
ning9imagingmodalitiesand22anatomiesfrom graphs accurately, we established a 5-point scor-
ingsystemforourevaluations,followingtheRad-
theMIMIC-IVdataset(Johnsonetal.,2020),the
PEER(Goldberg-Steinetal.,2017)-aninternation-
involving imaging modalities and anatomies are
allyrecognizedstandardforradiologicpeerreview.
listedinAppendixA.3.
Thescoresrangefrom5,denotingaperfectlyac-
Specifically,ourannotationprocessisasfollows:
curatereport,to0,thatindicatesthereportwithout
First,wedividetheMIMIC-IVdatasetinto49sub-
anycorrectobservations. Detailedscoringcriteria
sets based on modality and anatomy. To conduct
areprovidedinAppendixA.4,guidingradiologists
sentence-levelevaluation,wesplitthereportpara-
onhowtoassignscoresatdifferentlevels.
graphs into individual sentences by periods and
Specifically,ourannotationprocessisasfollows:
removetheduplicates. Next,werandomlysample
first,wedividetheMIMIC-IVdatasetinto49sub-
25 sentences from each subset to create a refer-
sets based on modality and anatomy. Next, we
ence report list and sample another 1000 reports
sample20reportsfromeachsubsettocreatearef-
to form a candidate report list. Subsequently, we
erence list and 500 reports to form a candidate
useseveralevaluationmetrics—BLEU,ROUGE,
list. The report selection process is the same as
BERTScore,CIDEr,andourproposedRaTEScore
sentence-level human rating. For each candidate
toidentifythemostsimilaronefromthecandidate
reportandthecorrespondingreferencereport,the
reportlistforeachreportinthereferencereportlist.
radiologistsarerequiredtogivea5-pointscore.
We take the union of all the metric results as the
reportpairsformanualannotations. Finally,each Thefinalbenchmarkinparagraph-leveliscom-
case in the annotation reports was annotated by posedof1856referencereports,candidatereports
twoexperiencedradiologistswithoverfiveyearsof and their rating score. Similarly, for parameter
clinicalpractice. Foreachcandidatereportandthe search (Sec. 2.5), we also divide all reports into
correspondingreferencereport,theyarerequired trainingsetandatestsetatan8:2ratio.
to count errors in six provided categories as well RatingontheSyntheticReports. Here,weaim
asthenumberofpotentialerrors,wheretheerror to evaluate the sensitivity of our metric on han-
5dlingsynonymsandnegationsusingsyntheticdata.
RadGraphF1 BERTScore CheXbert BLEU Ours
Specifically, we employed Mixtral 8x7B (Jiang
Kendallτ 0.515* 0.511* 0.499* 0.462* 0.527
et al., 2024), a sophisticated open-source Large
Language Model (LLM), to rewrite 847 reports
Table4: ResultsinReXValdataset: *denotestheresult
from the MIMIC-IV dataset. The rewriting was reportin(Yuetal.,2023a).
guidedbytwotailoredprompts:
varysignificantlyinlength,predominantlyfeatur-
Youareaspecialistinmedicalreportwriting,please
rewritethesentence,youcanpotentiallychangethe ing longer documents, we applied a type weight
entitiesintosynonyms,butpleasekeepthemeaning matrixwithparametersspecificallyfine-tunedon
unchanged.
ourlong-reportbenchmarktrainingset. Asdetailed
inTable4,RaTEScoredemonstratedaKendallcor-
Ontheotherhand,oppositereportsweregener-
relationcoefficientof0.527withtheerrorcounts,
atedwith:
surpassingallexistingmetrics.
Whilefurtherexamininginstanceswithnotable
Youareaspecialistinmedicalreportwriting,please
deviations in Appendix A.7, a primary observa-
rewritethefollowingmedicalreporttoexpressthe
oppositemeaning. tionwasthatReXVal’sprotocoltendstocountsix
typesoferrorsuniformly,withoutaccountingfor
Thisprocessresultsinatestsetcomprisingtri- variationsinreportlength. Thisapproachleadsto
adsofreports: theoriginal,asynonymousversion, discrepancieswhereasingle-sentencereportwith
andananonymousversion,detailedfurtherinAp- one error type and a twenty-sentence report with
pendix A.5. Ideally, effective evaluation metrics thesameerrorcountreceiveequivalentscores. To
shoulddemonstratehigherscoresforsynonymous addressthisissue,ourRaTE-Evalbenchmarkcan
reports compared to anonymous reports, thereby bebettersuitedtodistinguishsuchvariations, by
moreaccuratelyreflectingthetruesemanticcontent normalising the total error counts with potential
ofthereports. errorcounts.
4.3 ResultsinRaTE-Evalbenchmark
4 Experiments
OnSentence-levelRating. AsillustratedinFig.3,
Inthissection,westartbyintroducingthebaseline
our model achieved a Pearson correlation coef-
evaluation metrics. Later, we compare the differ-
ficient of 0.54 on the RaTE-Eval short sentence
entmetricswithourproposedRaTEScoreonboth
benchmark,significantlyoutperformingtheexist-
ReXVal and RaTE-Eval benchmarks. Lastly, we
ingbaselines. Theseresultsunderscoretheinade-
presentdetailsfortheablationstudies.
quacyofmethodsthatpredominantlyrelyonterm
4.1 Baselines overlap for evaluations within a medical context.
Weusethefollowingmetricsasbaselinecompar- Whileentity-basedmetricslikeRadGraphF1show
isons: BLEU(Papinenietal.,2002),ROUGE(Lin, notableimprovements,theystilldonotreachthe
2004), METEOR (Banerjee and Lavie, 2005), desiredlevelofefficacyonanextensivebenchmark
CheXbert (Smit et al., 2020; Yu et al., 2023a), encompassing multi-modal, multi-region reports.
BERTScore(Zhangetal.,2019),SPICE(Anderson Thisshortfalllargelyattributestothelimitedscope
et al., 2016) and RadGraph F1 (Yu et al., 2023a). oftrainingvocabularyinthesemethods.
Detailedexplanationsofthesemetricscanbefound On Paragraph-level Rating. From the results
intheAppendixA.6. in Table 5, it can be observed that RaTEScore
showsasignificantlyhighercorrelationwithradi-
4.2 ResultsinReXValdataset
ologyexpertscomparedtootherexistingmetrics,
Our initial evaluation adopts the public ReXVal across various measures of correlation. Metrics
dataset, wherewecalculatedtheKendallcorrela- thatfocusonidentifyingkeyentities,suchasRad-
tioncoefficienttomeasuretheagreementbetween Graph F1, SPICE, and ours, consistently demon-
ourRaTEScoreandtheaveragenumberoferrors strate stronger correlations than those reliant on
identified by six radiologists. Our analysis was merewordoverlap,therebysupportingourprimary
conducted under identical conditions to those of assertionthatcriticalstatementsinmedicalreports
baselinemethods. GiventhatthereportsinReXVal areparamount. Furthermore,metricsthataccom-
6RaTEScore (𝜏=0.54) RadGraphF1 (𝜏=0.44) SPICE (𝜏=0.40) BERTScore (𝜏=0.40)
METEOR (𝜏=0.39) ROUGE L (𝜏=0.34） BLEU (𝜏=0.27) CheXbert (𝜏=0.25)
Figure3: ResultsinRaTE-EvalBenchmark: CorrelationCoefficientswithRadiologistsResults(sentence-
level). ourmetricexhibitsthehighestPearsoncorrelationcoefficientwiththeradiologists’scoring. Notethatthe
scoresonthehorizontalaxisareexpertscountingvarioustypesoferrorsnormalizedbythepotentialerrortypesthat
couldoccurinthegivensentence,andsubtractingthisnormalizedscorefrom1toachieveapositivecorrelation.
scores than their antonymous counterparts. The
Paragraph-levelCorrelations Simulations
Pearsonτ Kendallτ Spearmanτ Acc results,presentedinTable5,demonstratethatour
modelexcelsinmanagingsynonymandantonym
RadGraph 0.624 0.439 0.582 0.463
challenges,affirmingitsrobustnessinnuancedlan-
BERTScore 0.599 0.413 0.555 0.140
guageprocessingwithinamedicalcontext.
CheXbert 0.496 0.294 0.403 0.666
BLEU 0.409 0.289 0.404 0.119
4.4 AblationStudy
ROUGE_L 0.572 0.396 0.567 0.117
SPICE 0.623 0.453 0.605 0.140 Inthisablationsection,weinvestigatethepipeline
METEOR 0.599 0.422 0.567 0.168 from two aspects: namely, the design of NER
model,theeffectofdifferentoff-the-shelfsynonym
Ours 0.653 0.462 0.608 0.670
disambiguationencodingmodule.
Table5: ResultsinRaTE-EvalBenchmark: Correlation
4.4.1 NERModuleDiscussion
coefficientswithradiologistsandaccuracyforwhether
thesynonymsentencecanachievehigherscoresthan Here,wediscusstheperformanceofourNERmod-
theantonymousoneonSyntheticReports. uleinthreeparts: trainingschemes,initialization
models,anddatacomposition.
Training Schemes. To select the most suitable
modatesynonyms,suchasMETEOR,outperform
NER model for training, we compare IOB-based
those that do not, such as BLEU and ROUGE.
and Span-based NER training schemes on the
Significantly, RaTEScore benefits from a robust
wholeRaTE-NERtestset. AsshowninTable6,the
NERmodeltrainedonourcomprehensivedataset,
IOBschemeoverallextractsmorecomprehensive
RaTE-NER,whichspansmultiplemodalitiesand
entities, but the recall is lower against the Span-
anatomicalregions,notjustChestx-rays,resulting
basedapproach.
inmarkedlyhighercorrelations.
InitializationModels. Additionally,asshownin
Results on Synthetic Reports. To further show- Table6,wealsotryasequentialpre-trainedBERT
casetheeffectivenessofourproposedRaTEScore, model for initialization, i.e., DeBERTa_v3 (He
weexamineditsperformanceonthesynthetictest et al., 2022), Medical-NER (Clinical-AI-Apollo,
set. This dataset, being synthesized, allows us 2023), BioMedBERT (Chakraborty et al., 2020),
to use accuracy (ACC) as a measure to evaluate BlueBERT (Peng et al., 2019), MedCPT-Q-
performance. Specifically,weassesswhetherthe Enc. (Jin et al., 2023), and BioLORD-2023-
synonymouslysimulatedsentencesreceivedhigher C (Remy et al., 2024). Detailed description for
7InitializedBERT Pre Recall F1 Acc 0.02 in Pearson Consistency. Based on these re-
sults, we selected BioLORD-2023-C as the base
DeBERTa_v3 0.567 0.575 0.571 0.754
IOB. modelforourEntityEncodingModule.
Medical-NER 0.559 0.572 0.565 0.759
BiomedBERT 0.556 0.676 0.610 0.730
SapBERT 0.560 0.658 0.605 0.731 5 RelatedWork
Span. BlueBERT 0.554 0.657 0.601 0.726
MedCPT-Q-Enc. 0.470 0.682 0.556 0.678 5.1 GeneralTextEvaluationMetric
BioLORD-2023-C 0.555 0.664 0.605 0.727
Automatedscoringmethodsallowforafairevalua-
Table6: AblationStudyonNERModelSchemes. tionofthequalityofgeneratedtext. BLEU(Pap-
inenietal.,2002),ROUGE(Lin,2004)wasorigi-
each model can be found in Appendix A.8. We
nallydesignedformachinetranslationtasks,focus-
applyvariousmodelsindifferenttrainingschemes
ingonword-levelaccuracy. METEOR(Banerjee
based on their pre-training tasks. For example,
and Lavie, 2005) adopts a similar design, taking
Medical-NERispre-trainedwithIOB-basedNER
into account synonym matching and word order.
tasksonothertasksthuswestillfinetuneitinthe
SPICE(Andersonetal.,2016)usesthekeyobjects,
same setting. Comparing Medical-NER and De-
attributes, and their relationships to compute the
BERTa_v3,pretrainingonotherNERdatasetsdoes
metric. BERTScore(Zhangetal.,2019),amodel-
notimprovemuch. DifferenttypesofBERTalso
basedmethod,assignsscorestoindividualwords
performfairlyfortheSpan-basedmethod. Based
andaveragesthesescorestoevaluatethetext’sover-
ontheresults,ourfinalscoresareallbasedonthe
allquality,facilitatingamoredetailedanalysisof
IOBschemewithDeBERTa_v3.
eachword’scontribution.
DataAblation. OurRaTE-NERdataiscomposed
of two distinct parts, and we conducted experi-
5.2 RadiologicalTextEvaluationMetric
mentstohighlightthenecessityofboth. Asshown
in Table 7, ‘R.’ denotes data from Radiopaedia, With the advancement of medical image analy-
while‘M.’referstothedatafromMIMIC-IV.By sis,researchershaverecognizedtheimportanceof
combining these two parts (denoted as ‘R.+M.’), evaluatingthequalityofradiologytextgeneration.
weobserveasignificantimprovementinthefinal Metrics such as CheXbert F1 (Smit et al., 2020)
NERperformance,withanincreaseof0.030inF1 and RadGraph F1 (Yu et al., 2023a) are based
and0.010inACC.Thisunderscorestheimportance on medical entity extraction models. However,
ofincorporatingeachdatasetcomponent. CheXbert can only annotate 14 chest abnormal-
ities, and RadGraph F1 (Jain et al., 2021) is only
TrainingData Pre Recall F1 Acc trainedonchestX-raymodality. MEDCON(Yim
etal.,2023)expandstheextractionrangebyQuick-
R. 0.525 0.558 0.541 0.727
UMLS package (Soldaini and Goharian, 2016),
M. 0.515 0.550 0.531 0.744
which relies on a string match algorithm that is
R.+M. 0.567 0.575 0.571 0.754
notflexible. RadCliQ(Yuetal.,2023a)performs
ensembling with BLEU, BERTScore, CheXbert
Table 7: Ablation Study on NER Training Data. R.
denotes data from Radiopaedia and M. denotes data vectorsimilarity,andRadGraphF1foracompre-
fromMIMIC-IV. hensive yet less interpretable evaluation. These
metrics calculate the overlap between reference
4.4.2 EntityEncodingModuleDiscussion andcandidatesentenceswhileoverlookingtheis-
In our entity encoding evaluation, we compare sue of synonymy. Recently, metrics using Large
two off-the-shelf entity encoding models on the LanguageModels(LLMs)suchasGPT-4,suchas
sentence-levelcorrelationtaskofRaTE-Eval. The G-Eval(Liuetal.,2023),LLM-as-a-Judge(Zheng
first model, BioLORD-2023-C, is trained on et al., 2024), and LLM-RadJudge (Wang et al.,
medical entity-definition pairs, while the second, 2024) have emerged, closely mimic human eval-
MedCPT-Query-Encoder, is trained on PubMed uation levels. However, these methods are unex-
userclicksearchlogs. ThemodelsachievedPear- plainable and may have potential subjective bias.
son correlation coefficients of 0.54 and 0.52, re- Besides,theirhighcomputationalcostalsolimits
spectively. BioLORDoutperformsMedCPTwith themforstatisticrobustlarge-scaleevaluation.
85.3 MedicalNamed-EntityRecognition References
The MedNER task targets extracting medical- ICD-10-CM. https://www.icd10data.com/
relatedentitiesfromgivencontexts. Greatefforts ICD10CM/Codes.
have been made in this domain (Jin et al., 2023;
Radiopaedia.org. https://radiopaedia.org.
Monajatipooretal.,2024;Kelothetal.,2024;Li
andZhang,2023;Chenetal.,2023). Inspiredby Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,
Antoine Miech, Iain Barr, Yana Hasson, Karel
thesuccessofthiswork,webelieveMedNERmod-
Lenc,ArthurMensch,KatherineMillican,Malcolm
els are strong enough to simplify complex clini-
Reynolds,etal.2022. Flamingo: avisuallanguage
cal texts, thus reducing the difficulty of automat- model for few-shot learning. Advances in Neural
ically comparing two clinical texts. The most re- InformationProcessingSystems,35:23716–23736.
latedworktooursisRadGraph(Jainetal.,2021)
PeterAnderson,BasuraFernando,MarkJohnson,and
which trained an NER model for Chest X-ray re- StephenGould.2016. Spice: Semanticpropositional
ports while we are targeting the general clinical imagecaptionevaluation. InProceedingsofEuro-
reportregardlessoftheirtype. peanConferenceonComputerVision(ECCV),pages
382–398.
6 Conclusion
RohanAnil,AndrewMDai,OrhanFirat,MelvinJohn-
In this work, we propose a new lightweight, ex- son, Dmitry Lepikhin, Alexandre Passos, Siamak
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
plainablemedicalfree-formtextevaluationmetric,
Chen, et al. 2023. Palm 2 technical report. arXiv
RaTEScore, by comparing two medical reports
preprintarXiv:2305.10403.
on the entity level. In detail, first, we build up a
SatanjeevBanerjeeandAlonLavie.2005. Meteor: An
newmedicalNERdataset,RaTE-NERtargeting
automatic metric for mt evaluation with improved
awiderangeofradiologicalreporttypesandtrain
correlationwithhumanjudgments. InProceedings
a NER model on it. Then, we adopt this model oftheAclWorkshoponIntrinsicandExtrinsicEvalu-
to simplify the complex radiological reports and ationMeasuresforMachineTranslationand/orSum-
comparethemontheentityembeddinglevellever- marization,pages65–72.
aginganextrasynonymsdisambiguationencoding
James Bergstra, Rémi Bardenet, Yoshua Bengio, and
model. Our final RaTEScore correlates strongly BalázsKégl.2011. Algorithmsforhyper-parameter
withclinicians’truepreferences,significantlyout- optimization. AdvancesinNeuralInformationPro-
cessingSystems,24.
performing previous metrics both on the former
existingbenchmarkandournewproposedRaTE-
Olivier Bodenreider. 2004. The unified medical lan-
Eval,whilemaintainingcomputationalefficiency guagesystem(umls): integratingbiomedicaltermi-
andinterpretability. nology. NucleicAcidsResearch,32(suppl_1):D267–
D270.
Limitations
KathiCaneseandSarahWeis.2013. Pubmed: thebibli-
ographicdatabase. TheNCBIHandbook,2(1).
Although our proposed metric, RaTEScore, has
performed well across various datasets, there are Souradip Chakraborty, Ekaba Bisong, Shweta Bhatt,
stillsomelimitations. First,inthesynonymdisam- Thomas Otto Friedrich Wagner, Riley Elliott, and
Francesco Mosconi. 2020. Biomedbert: A pre-
biguation module, we evaluated the performance
trainedbiomedicallanguagemodelforqaandir. In
of several existing models and directly ultilized
InternationalConferenceonComputationalLinguis-
themwithoutfine-tuningspecificallyfortheevalu- tics.
ation scenario, which could be enhanced in the
PengChen,JianWang,HongfeiLin,DiZhao,andZhi-
future. Furthermore, while we expanded from
haoYang.2023. Few-shotbiomedicalnamedentity
single-modality radiological report evaluation to recognitionviaknowledge-guidedinstancegenera-
multimodalwhole-bodyimaging,westillonlycon- tionandpromptcontrastivelearning. Bioinformatics,
sidered the issues within the radiological report 39(8):btad496.
scenarioanddidnotextendtoothermedicalcon-
Clinical-AI-Apollo.2023. Clinical-AI-ApolloMedical-
texts beyond radiology, nor to the evaluation of NER. HuggingFace.
othermedicaltasks,likemedicalQA,summarisa-
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
tiontask. Theseareasrequireongoingresearchand
KristinaToutanova.2018. Bert: Pre-trainingofdeep
exploration.
bidirectionaltransformersforlanguageunderstand-
ing. arXivpreprintarXiv:1810.04805.
9KevinDonnellyetal.2006. Snomed-ct: Theadvanced AlistairEWJohnson,TomJPollard,LuShen,Li-weiH
terminologyandcodingsystemforehealth. Studies Lehman, Mengling Feng, Mohammad Ghassemi,
inHealthTechnologyandInformatics,121:279. BenjaminMoody,PeterSzolovits,LeoAnthonyCeli,
andRogerGMark.2016. Mimic-iii,afreelyaccessi-
HannahEyre,AlecBChapman,KellySPeterson,Jian- blecriticalcaredatabase. ScientificData,3(1):1–9.
linShi,PatrickRAlba,MakotoMJones,TamaraL
Box, Scott L DuVall, and Olga V Patterson. 2021. VipinaKKeloth,YanHu,QianqianXie,XueqingPeng,
Launchingintoclinicalspacewithmedspacy: anew Yan Wang, Andrew Zheng, Melih Selek, Kalpana
clinicaltextprocessingtoolkitinpython. InAMIA Raja,ChihHsuanWei,QiaoJin,etal.2024. Advanc-
AnnualSymposiumProceedings,volume2021,page ingentityrecognitioninbiomedicineviainstruction
438. tuning of large language models. Bioinformatics,
40(4):btae163.
Christiane Fellbaum. 2010. Wordnet. In Theory and
Applications of Ontology: Computer Applications, JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi.
pages231–243. 2023. Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large lan-
Shlomit Goldberg-Stein, L Alexandre Frigini, Scott guagemodels. InInternationalConferenceonMa-
Long,ZeyadMetwalli,XuanVNguyen,MarkParker, chineLearning,pages19730–19742.
andHaniAbujudeh.2017. Acrradpeercommittee
whitepaperwith2016updates: revisedscoringsys- Mingchen Li and Rui Zhang. 2023. How far is lan-
tem,newclassifications,self-review,andsubspecial- guage model from 100% few-shot named entity
ized reports. Journal of the American College of recognition in medical domain. arXiv preprint
Radiology,14(8):1080–1086. arXiv:2307.00186.
PengchengHe,JianfengGao,andWeizhuChen.2022. Chin-YewLin.2004. Rouge: Apackageforautomatic
Debertav3:Improvingdebertausingelectra-stylepre- evaluation of summaries. In Text Summarization
trainingwithgradient-disentangledembeddingshar- BranchesOut,pages74–81.
ing. In The Eleventh International Conference on
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,
LearningRepresentations.
RuochenXu,andChenguangZhu.2023. G-eval:Nlg
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and evaluationusinggpt-4withbetterhumanalignment.
WeizhuChen.2020. Deberta: Decoding-enhanced InProcessingofthe2023ConferenceonEmpirical
bert with disentangled attention. In International MethodsinNaturalLanguage(EMNLP).
ConferenceonLearningRepresentations.
Masoud Monajatipoor, Jiaxin Yang, Joel Stremmel,
Saahil Jain, Ashwin Agrawal, Adriel Saporta, Melika Emami, Fazlolah Mohaghegh, Mozhdeh
Steven QH Truong, Du Nguyen Duong, Tan Bui, Rouhsedaghat,andKai-WeiChang.2024. Llmsin
PierreChambon,YuhaoZhang,MatthewPLungren, biomedicine: Astudyonclinicalnamedentityrecog-
Andrew Y Ng, et al. 2021. Radgraph: Extracting nition. arXivpreprintarXiv:2404.07376.
clinicalentitiesandrelationsfromradiologyreports.
MichaelMoor,OishiBanerjee,ZahraShakeriHossein
arXivpreprintarXiv:2106.14463.
Abad, Harlan M Krumholz, Jure Leskovec, Eric J
Albert Q Jiang, Alexandre Sablayrolles, Antoine Topol,andPranavRajpurkar.2023. Foundationmod-
Roux,ArthurMensch,BlancheSavary,ChrisBam- elsforgeneralistmedicalartificialintelligence. Na-
ford,DevendraSinghChaplot,DiegodelasCasas, ture,616(7956):259–265.
Emma Bou Hanna, Florian Bressand, et al. 2024.
OpenAI. [link].
Mixtralofexperts. arXivpreprintarXiv:2401.04088.
OpenAI.2023. GPT-4TechnicalReport. arXivpreprint
QiaoJin,WonKim,QingyuChen,DonaldCComeau,
arXiv:2303.08774.
Lana Yeganova, W John Wilbur, and Zhiyong Lu.
2023. Medcpt: Contrastivepre-trainedtransformers KishorePapineni,SalimRoukos,ToddWard,andWei-
with large-scale pubmed search logs for zero-shot JingZhu.2002. Bleu: amethodforautomaticevalu-
biomedical information retrieval. Bioinformatics, ationofmachinetranslation. InProceedingsofthe
39(11):btad651. 40thannualmeetingoftheAssociationforComputa-
tionalLinguistics,pages311–318.
Alistair Johnson, Lucas Bulgarelli, Tom Pollard,
StevenHorng,LeoAnthonyCeli,andRogerMark. YifanPeng,ShankaiYan,andZhiyongLu.2019. Trans-
2020. Mimic-iv. PhysioNet. Available online at: ferlearninginbiomedicalnaturallanguageprocess-
https://physionet.org/content/mimiciv/1.0/(accessed ing: An evaluation of bert and elmo on ten bench-
August23,2021),pages49–55. markingdatasets. InProceedingsofthe18thBioNLP
WorkshopandSharedTask,pages58–65.
AlistairEWJohnson,TomJPollard,SethJBerkowitz,
NathanielRGreenbaum,MatthewPLungren,Chih- Pengcheng Qiu, Chaoyi Wu, Xiaoman Zhang, Weix-
ying Deng, Roger G Mark, and Steven Horng. iongLin,HaichengWang,YaZhang,YanfengWang,
2019. Mimic-cxr,ade-identifiedpubliclyavailable and Weidi Xie. 2024. Towards building multilin-
databaseofchestradiographswithfree-textreports. gual language model for medicine. arXiv preprint
ScientificData,6(1):317. arXiv:2402.13963.
10François Remy, Kris Demuynck, and Thomas De- Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weix-
meester.2024. Biolord-2023: semantictextualrepre- iong Lin, Ya Zhang, Yanfeng Wang, and Weidi
sentationsfusinglargelanguagemodelsandclinical Xie. 2023. Pmc-vqa: Visual instruction tuning for
knowledgegraphinsights. JournaloftheAmerican medicalvisualquestionanswering. arXivpreprint
MedicalInformaticsAssociation,pageocae029. arXiv:2305.10415.
AkshaySmit,SaahilJain,PranavRajpurkar,AnujPa- LianminZheng,Wei-LinChiang,YingSheng,Siyuan
reek,AndrewYNg,andMatthewPLungren.2020. Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Chexbert: combiningautomaticlabelersandexpert Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024.
annotations for accurate radiology report labeling Judging llm-as-a-judge with mt-bench and chatbot
usingbert. arXivpreprintarXiv:2004.09167. arena. AdvancesinNeuralInformationProcessing
Systems,36.
LucaSoldainiandNazliGoharian.2016. Quickumls:
a fast, unsupervised approach for medical concept Zexuan Zhong and Danqi Chen. 2020. A frustrat-
extraction. InMedIRWorkshop,Sigir,pages1–4. inglyeasyapproachforentityandrelationextraction.
arXivpreprintarXiv:2010.12812.
TaoTu,ShekoofehAzizi,DannyDriess,MikeSchaek-
ermann,MohamedAmin,Pi-ChuanChang,Andrew
Carroll,CharlesLau,RyutaroTanno,IraKtena,etal.
2024. Towardsgeneralistbiomedicalai. NEJMAI,
1(3):AIoa2300138.
ZilongWang,XufangLuo,XinyangJiang,Dongsheng
Li, and Lili Qiu. 2024. Llm-radjudge: Achieving
radiologist-levelevaluationforx-rayreportgenera-
tion. arXivpreprintarXiv:2404.00998.
JerryWei,ChengrunYang,XinyingSong,YifengLu,
Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu,
DaHuang,CosmoDu,etal.2024. Long-formfac-
tuality in large language models. arXiv preprint
arXiv:2403.18802.
ChaoyiWu,WeixiongLin,XiaomanZhang,YaZhang,
Weidi Xie, and Yanfeng Wang. 2024. Pmc-llama:
toward building open-source language models for
medicine. Journal of the American Medical Infor-
maticsAssociation,pageocae045.
ChaoyiWu,XiaomanZhang,YaZhang,YanfengWang,
andWeidiXie.2023. Towardsgeneralistfoundation
modelforradiologybyleveragingweb-scale2d&3d
medicaldata. arXivpreprintarXiv:2308.02463.
Wen-wai Yim, Yujuan Fu, Asma Ben Abacha, Neal
Snider,ThomasLin,andMelihaYetisgen.2023. Aci-
bench: anovelambientclinicalintelligencedataset
for benchmarking automatic visit note generation.
ScientificData,10(1):586.
Feiyang Yu, Mark Endo, Rayan Krishnan, Ian Pan,
Andy Tsai, Eduardo Pontes Reis, Eduardo Kaiser
Ururahy Nunes Fonseca, Henrique Min Ho Lee,
Zahra Shakeri Hossein Abad, Andrew Y Ng, et al.
2023a. Evaluatingprogressinautomaticchestx-ray
radiologyreportgeneration. Patterns,4(9).
Feiyang Yu, Mark Endo, Rayan Krishnan, Ian Pan,
AndyTsai,EduardoPontesReis,EKUFonseca,Hen-
riqueLee,ZahraShakeri,AndrewNg,etal.2023b.
Radiologyreportexpertevaluation(rexval)dataset.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q
Weinberger,andYoavArtzi.2019. Bertscore: Eval-
uating text generation with bert. arXiv preprint
arXiv:1904.09675.
11A Appendix (“insitu”,Non-Abnormality)and(“notinplace”,
Abnormality). Due to the comparison directions,
A.1 ScoringExample
in S(x,xˆ), W(Non-Abnormality,Abnormality)
Inthissection,wewillshowanexampleofcalcu- as 0.94 is adopted and in the other hand,
latingRaTEScore. Givenaradiologyreportpair: W(Abnormality,Non-Abnormality) as 0.83 is
adopted. Thefinalscoreiscomputedasfollows:
Referencedx: AFoleycatheterisinsitu.
S(x,xˆ)×S(xˆ,x)
Candidate xˆ: A Foley catheter is not in RaTEScore = 2× = 0.676.
S(x,xˆ)+S(xˆ,x)
place.
A.2 AutomaticAnnotationApproach
Forsimplicity,wewillonlydescribethecalculation
Here,weintroduceourautomaticapproachtocon-
procedure for S(x,xˆ) in text, and the calculation
struct a part of our RaTE-NER dataset, sourced
procedureforS(xˆ,x)issimilar.
from 19,263 original reports obtained from Ra-
WefirstconductMedicalNamedEntityRecog-
diopaedia(Rad)andcovering9modalitiesand11
nition to decompose the natural text into enti-
anatomies. As shown in Figure 4, leveraging the
ties. For the referenced report, the entities list
latest LLM GPT-4 combined with other compre-
is: {(“Foleycatheter”,Anatomy),(“insitu”,Non-
hensive medical knowledge bases, we develop a
Abnormality) } and for the candidate report is
newautomatedmedicalNERandrelationextrac-
{(“Foleycatheter”,Anatomy),(“notinplace”,Ab-
tiondatasetconstructionpipeline.
normality)}. Subsequently,theseextractedentities
Specifically, we manually annotate several re-
areprocessedthroughtheSynonymDisambigua-
portsattherequiredgranularityandadoptfew-shot
tion Encoding Module, which encodes the “Fo-
promptswithGPT-4toinitiallyestablishanNER
leycatheter”and“insitu”intofeatureembedding.
dataset. Following this, we build a robust medi-
Finally, during the Scoring Procedure, we pick
calentitylibrary,integratingUMLS(Bodenreider,
out the most similar entity in the referenced re-
2004), Snomed CT (Donnelly et al., 2006), ICD-
port for each entity in the candidate report, i.e.,
10 (ICD), and other knowledge bases, then, com-
“Foley catheter” paired with “Foley catheter” in
pareallextractedentitiesusingtheMedCPT(Jin
the reference, and “not in place” with “in situ”.
et al., 2023) model for similarity. During the
Then,wegettwocosinesimilarityscoresbasedon
comparisonprocess,entitieswithcosinesimilarity
the text embedding, 1.0 for “Foley catheter” and
lower than 0.83 were filtered out. Most entities
0.83 for “not in place”. The similarity score be-
belowthisthresholddidnotmeetourrequirements.
tween(“notinplace”,Abnormality)and(“insitu”,
Subsequently,weremovedsentenceswithanentity
Non-Abnormality)willbefurthermultipliedwith
annotation density lower than 0.7 at the sentence
apenaltyfactorpas0.37whiletheothersimilarity
level. Finally,weusemedspaCy(Eyreetal.,2021)
ismaintainedsincetheyhavethesameentitytype.
and also key negative words detection in reports,
AtLast,wecalculatetheweightedcombinationof
suchas“no”,“without”,“unremarkable”,“intact”,
thetwo. Theweightsarederivedfromalearnable
to determine the positive or negative polarity of
attributionmatrixW correspondingtothesetype
eachwordinthesentence.
combinations,as0.91and0.94respectively. The
calculationformulationisasfollows: A.3 InvolvingAnatomiesandModalitiesin
MIMIC-IVData
0.91×1+0.94×0.83×0.36
S(x,xˆ) =
0.91+0.94 In this section, we detail the imaging modalities
= 0.644. andanatomiesinvolvedinMIMIC-IVdataset.
Anatomy List: NECK, TEETH, BRAIN,
Similarly,wecangettheothersimilarity:
HEAD, CHEST, PELVIS, ABDOMEN, CAR-
DIAC, HEAD-NECK, SOFT TISSUE, UP-EXT,
0.91×1+0.83×0.83×0.36
S(xˆ,x) = OB, EXT, HIP, BREAST, SPINE, MAMMO,
0.91+0.83
BRAIN-FACE-NECK,LOW-EXT,BONE,VAS-
= 0.666.
CULAR,BLADDER.
Notably,theonlydifferencebetweenthetwosimi- ModalityList: CT,CTA,Fluoroscopy,Mammog-
larityscoresinthiscaseliesintheweightbetween raphy,MRA,MRI,MRV,Ultrasound,X-Ray.
12Figure4: DataCurationProcedure.
A.4 GuidelinesforRadiologists mented alignments, while also considering
words order and synonyms through Word-
Referencing RadPEER (Goldberg-Stein et al.,
Net(Fellbaum,2010).
2017), we set up a five-point scoring criteria, as
showninTable8. Duringtheannotationprocess, • CheXbert (Smit et al., 2020; Yu et al.,
eachreportiscompensatedwith$1perreport,with 2023a) computes the cosine similarity be-
fivereferencereportsseparately. tweenCheXbertmodelembeddingoftheref-
erencereportandcandidatereport.
A.5 ExampleforSimulationReports
• BERTScore(Zhangetal.,2019)utilizesapre-
Inthissection,wegiveanexampleforthesimula-
trainedBERTmodeltocalculatethesimilarity
tionreportgeneration:
ofwordembeddingsbetweencandidateand
referencetexts.
GT:Theappendixiswellvisualizedandair-
• SPICE(Andersonetal.,2016)extractskeyob-
filled.
jects,attributes,andtheirrelationshipsfrom
REWRITE: The appendix is seen and con-
descriptionstobuildascenegraph,andcom-
tainsgas.
paresthetwotextsonthescenegraphlevel.
OPPOSITE:Theappendixispoorlyvisual-
izedandnotair-filled. • RadGraph F1 (Yu et al., 2023a) extracts the
radiology entities and relations for Chest X-
A.6 Baselines ray modality and computes the F1 score on
theentitylevel.
Herein,wewillintroducetheconsideredbaselines:
A.7 FailureCasesinReXValDataset
• BLEU (Papineni et al., 2002) measures the
precision of generated text by comparing n- In this section, in order to better demonstrate the
gram overlap between the generated report drawbacksofReXValdataset,wewillgiveafailure
andreferencereports. casewheretworeportswithdifferententity-wise
errorswhileachievethesamescores.
• ROUGE(Lin,2004)focusesontherecallof
generated text by measuring the overlap of
n-grams,similartoBLEU.
• METEOR (Banerjee and Lavie, 2005) com-
binesprecision,recall,andapenaltyforfrag-
13Score Meaning Explaination
5 Correct Mostofthediagnosisresultsarecorrect. Mostdescriptionsarethesame. Some
wrongdescriptionunlikelytobeclinicallysignificant.
4 AlmostCorrect 75%ofthediagnosisresultsarecorrect. Mostdescriptionsarethesame. Some
wrongdescriptionlikelytobeclinicallysignificant.
3 PartlyCorrect 50%ofthediagnosisresultsarecorrect.
2 PartlyIncorrect 25%ofthediagnosisresultsarecorrect
1 MajorErrorsPresent Incorrectdiagnosis.Maybesomenegativedescriptionsarethesame.
0 TotalDifferent Nooverlapforthedescriptedinformation.
Table8:5-pointscoringsystemForRadiologiststoRateinParagraph-levelHumanRatingofRaTE-EvalBenchmark
ReportPair1: directlyusingabsoluteerrorcountingnumbersas
thescorelikeReXValmaypresentseverebiasthat
GT: ET tube within 1 cm of the carina. longer sentences scoring lower and shorter sen-
This was discussed with Dr. ___ at 4 p.m. tencesscoringhigher.
on___byDr. ___attimeofinterpretation.
Pred: ETtubeterminatesapproximately3. A.8 PretrainedBERTModelIntroduction
5cmfromthecarina.
In this section, we will introduce our considered
TotalErrors: 1.33
pre-trainedBERTmodelsindetail:
ReportPair2:
• DeBERTa_v3(Heetal.,2022)isanadvanced
version of the DeBERTa (He et al., 2020)
GT: In comparison with the study of xxx,
model, whichimprovesupontheBERTand
there is again enlargement of the cardiac
RoBERTamodelsbyincorporatingdisentan-
silhouettewithelevationofpulmonaryve-
gledattentionmechanisms,enhancingperfor-
nous pressure. Opacification at the right
mance on a wide range of natural language
baseagainisconsistentwithcollapseofthe
processingtasks.
rightmiddleandlowerlobesRECOMMEN-
DATION(S):ThetipoftherightIJcatheter • Medical-NER(Clinical-AI-Apollo,2023)isa
isinthemidtolowerSVC. fine-tunedversionofDeBERTatorecognize
Pred: In comparison with the study xxx, 41medicalentities. Thespecifictrainingdata
there is little change in the appearance of isnotpubliclyavailable.
the monitoring and support devices. Con-
• BioMedBERT(Chakrabortyetal.,2020)pre-
tinued substantial enlargement of the car-
viously named "PubMedBERT", pretrained
diacsilhouettewithrelativelymildelevation
fromscratchusingabstractsandfull-textarti-
of pulmonary venous pressure. Opacifica-
clesfromPubMed(CaneseandWeis,2013).
tionattherightbasesilhouettesthehemidi-
aphragmandisconsistentwithcollapseof • BlueBERT (Peng et al., 2019) is a BERT
therightmiddleandlowerlobes. model pre-trained on PubMed abstracts and
TotalErrors: 1.33 clinical notes (MIMIC-III) (Johnson et al.,
2016).
As shown in the examples, case 1 with only
• MedCPT-Q-Enc. (Jin et al., 2023) is pre-
two entity errors scores 1.3, and the report that
trained by 255M query-article pairs from
describesmorethantendifferententityerrorsalso
PubMedsearchlogs,andachieveSOTAper-
scores 1.3. Moreover, reports length less than
formanceonseveralzero-shotbiomedicalIR
10 words commonly has zero errors in ReXVal,
datasets.
whereas reports longer than 25 words had an av-
erage error count greater than 3, simply because • BioLORD-2023-C (Remy et al., 2024) is
the texts are longer and may contain more poten- basedonasentence-transformersmodeland
tialerrors. Therefore,ignoringnormalizationand furtherfinetunedontheentity-conceptpairs.
14A.9 NERModuleImplementationDetails
In the Medical Named Entity Recognition Mod-
ule training scheme, we train the model on one
NVIDIA GeForce GTX 3090 GPU with a batch
sizeof96for10epochswhileadoptdifferentlearn-
ing rates for different training schemes. For the
Span-basedmethod,wefollowthesettingofPURE
entitymodel(ZhongandChen,2020),whichuses
apre-trainedBERTmodeltoobtaincontextualized
representationsandthenfedintoafeedforwardnet-
worktopredicttheprobabilitydistributionofthe
entity. It combines a BERT (Devlin et al., 2018)
modelanda3-layerMLPwithheadhiddendimen-
sionof3096forspanclassification. Thespanmax
lengthis8. Inthetrainingstage,wesetthelearn-
ingrateas6e-6. FortheIOB-basedmethod,each
token is labeled as ’B-’ (beginning of an entity),
’I-’ (inside an entity), or ’O’ (outside of any en-
tity). Wedirectlyfine-tunethepre-trainedBERT
toperformatokenclassificationtask. Specifically,
weaddalinearlayertotheoutputembeddingofa
BERT-likedmodel,whichisfine-tunedutilizinga
corpusofannotatedentitydatatopredicttheentity
labelforeachtoken. Weusealearningrateof1e-5
fortheIOB-basedtrainingscheme.
15