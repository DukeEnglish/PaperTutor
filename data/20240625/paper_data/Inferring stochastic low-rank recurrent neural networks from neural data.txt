Inferring stochastic low-rank recurrent neural
networks from neural data
MatthijsPals1,2 AErdemSag˘tekin1,2,3
FelixPei1,2 ManuelGloeckler1,2
JakobHMacke1,2,4
1MachineLearninginScience,ExcellenceClusterMachineLearning,UniversityofTübingen,Germany
2TübingenAICenter,Tübingen,Germany
3GraduateTrainingCentreofNeuroscience,UniversityofTübingen,Germany
4DepartmentEmpiricalInference,MaxPlanckInstituteforIntelligentSystems,Tübingen,Germany
Abstract
A central aim in computational neuroscience is to relate the activity of large
populationsofneuronstoanunderlyingdynamicalsystem. Modelsoftheseneural
dynamicsshouldideallybebothinterpretableandfittheobserveddatawell. Low-
rank recurrent neural networks (RNNs) exhibit such interpretability by having
tractable dynamics. However, it is unclear how to best fit low-rank RNNs to
dataconsistingofnoisyobservationsofanunderlyingstochasticsystem. Here,
we propose to fit stochastic low-rank RNNs with variational sequential Monte
Carlo methods. We validate our method on several datasets consisting of both
continuous and spiking neural data, where we obtain lower dimensional latent
dynamicsthancurrentstateoftheartmethods. Additionally,forlow-rankmodels
withpiecewiselinearnonlinearities,weshowhowtoefficientlyidentifyallfixed
pointsinpolynomialratherthanexponentialcostinthenumberofunits,making
analysis of the inferred dynamics tractable for large RNNs. Our method both
elucidatesthedynamicalsystemsunderlyingexperimentalrecordingsandprovides
agenerativemodelwhosetrajectoriesmatchobservedtrial-to-trialvariability.
1 Introduction
A common goal of many scientific fields is to extract the dynamical systems underlying noisy
experimentalobservations. Inparticular,inneuroscience,muchworkisdevotedtounderstandingthe
coordinatedfiringofneuronsasbeingimplementedthroughunderlyingdynamicalsystems[1–5].
Recurrent neural networks (RNNs) constitute a common model-class of neural dynamics [6–13]
whichcanbereverse-engineeredtoformhypothesesaboutneuralcomputations[14,15].
Asaresult,severalrecentresearchdirectionshavecenteredoninterpretableoranalyticallytractable
RNNarchitectures. Inparticular, RNNswithlow-rankstructure[16–22]admitadirectmapping
betweenhigh-dimensionalpopulationactivityandanunderlyinglow-dimensionaldynamicalsystem.
RNNswithpiecewise-linearactivations[8,9,23–26]havefixedpointsandcyclesthatcanbeaccessed
analytically.
{firstname.secondname}@uni-tuebingen.de
4202
nuJ
42
]GL.sc[
1v94761.6042:viXraFigure1: Ourgoalistoobtaingenerativemodelsfromwhichwecansamplerealisticneuraldata
whilehavingatractableunderlyingdynamicalsystem. Weachievethisbyfittingstochasticlow-rank
RNNswithvariationalsequentialMonteCarlo.
Toserveasusefulmodelsofbrainactivity,itisimportantthatmodelsalsocapturetheobservedbrain
activity,includingtrial-to-trialvariability. ManymethodsthatfitRNNstodataarerestrictedtoRNNs
withdeterministictransitions[6–8,10–12]. Itisunlikelythat,ingeneral,allvariabilityinthedata
canbeexplainedbyvariabilityintheRNNsinitialstate. Thus, adoptingstochastictransitionsis
imperative. Whileprobabilisticsequencemodelsareusedeffectivelyinneuroscience[27],theyhave
sofarlargelyconsistedofstatespacemodelswithoutanobviousmechanisticinterpretation[28–32].
Here,wedemonstratethatwecanfitlargestochasticRNNstonoisyhigh-dimensionaldata. First,we
showthat,bycombiningvariationalsequentialMonteCarlomethods[33–35]withlow-rankRNNs,
wecanefficientlyfitstochasticRNNswithmanyunitsbylearningtheunderlyinglow-dimensional
dynamicalsystem. Second,weshowthat,forlow-ranknetworkswithpiecewise-linearactivation
functions, the resulting dynamics can be efficiently analyzed: In particular, we show how all fix
pointscanbefoundwithapolynomialcostinthenumberofunits–dramaticallymoreefficientthan
theexponentialcostinthegeneralcase.
We first validate our method using several teacher-student setups and show that we recover both
thegroundtruthdynamicsandstochasticity. Wethenfitourmodeltoseveralreal-worlddatasets,
spanningbothspikingandcontinuousdata,whereweobtainagenerativemodelwhichneedslower
dimensionallatentdynamicsthancurrentstateoftheartmethods. Wealsodemonstratehowinour
low-rankRNNsfixedpointscanbeefficientlyinferred—potentiallyatalowercostthanapproximate
methods[25],whileadditionallycomingwiththeguaranteethatallfixedpointsarefound.
2 Theoryandmethods
2.1 Low-rankRNNs
2.1.1 Accesstothelow-dimensionaldynamicsunderlyinglargenetworks
Ourgoalistoinferrecurrentneuralnetworkmodelsoftheform
dx
τ =−x(t)+Jϕ(x(t))+Γ ξ(t), (1)
dt x
withneuronactivityx(t)∈RN,time-constantτ ∈R ,recurrentweightsJ∈RN×N,element-wise
>0
nonlinearityϕ,anRdimensionalwhitenoiseprocessξ(t)andΓ ∈ RN×R. Inparticular,weare
x
interestedinthecasewheretheweightmatrixJhasrankR≤N,i.e. itcanbewrittenasJ=MNT,
withM,N∈RN×R ([18–21]). Assumingthatx(0)liesinthesubspacespannedbythecolumnsof
MandΓ =MΓ ,withΓ ∈RR×R ,wecanrewriteEq.1asanequivalentRdimensionalsystem,
x z z
dz
τ =−z(t)+NTϕ(Mz(t))+Γ ξ(t), (2)
dt z
where we can switch between Eq. 1 and Eq. 2 by means of linear projection, z(t) =
(MTM)−1MTx(t) and x(t) = Mz(t). Note that we can directly extend these equations to in-
cludeinput,representing,e.g.,experimentalstimuliorcontext(seeSupplementC.2).
22.1.2 Low-rankRNNsasstatespacemodels
Weconsidernonlinearlatentdynamicalsystemswithobservationsy :
t
T T
(cid:89) (cid:89)
p(z ,y )=p(z ) p(z |z ) p(y |z ),
1:T 1:T 1 t t−1 t t
t=2 t=1
p(z |z )=N(F(z ),Σ ), p(z )=N(µ ,Σ ),
t t−1 t−1 z 1 z1 z1
p(y |z )=G(z ),
t t−1 t
wherethetransitiondistributionisparameterisedbydiscretisingalow-rankRNNwithtimestep∆
t
(seeSupplementC.1),wehavemeanF(z t)=az t+N˜Tϕ(Mz t),witha=1− ∆ τt andN˜ = ∆ τtN,
andcovarianceΣ . ThespecificformoftheobservationfunctionG,dependsonthedata-modality,
z
e.g.,hereweuseaPoissondistributionforcountobservations. Thisformulationallowsonetokeep
theone-to-onecorrespondencebetweenRNNunits(orasubsetofthose)andrecordeddataneurons,
(aswasdesiredine.g.,[10–12,36]). Forexample,assumingGaussianobservationnoise,wecan
simplyusethatx =Mz anddefineG=N(Mz ,Σ ).
t t t y
Oncewelearnp(z ,y ),wecanusetheobtainedRNNasgenerativemodeltosampletrajectories,
1:T 1:T
andreverseengineertheunderlyingdynamicstogaininsightinthedatagenerationprocess. Given
thesequentialstructureoftheRNN,wecandomodellearningbyusingvariationalsequentialMonte
Carlo(alsocalledParticleFiltering)methods[33–35].
2.2 ModellearningwithvariationalsequentialMonteCarlo
2.2.1 SequentialMonteCarlo
SequentialMonteCarlo(SMC)canbeusedtoapproximatesequencesofdistributions,suchasthose
generatedbyourRNN,withasetofK trajectoriesoflatentsz (commonlycalledparticles)[37].
1:T
AcrucialchoicewhendoingSMCispickingtherightproposaldistributionr,fromwhichwecan
samplelatentsatagiventimestep,conditionedonthepreviouslatentz andobserveddatay ,or
t−1 1:t
asubsetofthose. Giveninitialsamplesz1:K ∼randcorrespondingimportanceweightsw1:K (as
1 1
definedbelow)SMCprogressesbyrepeatedlyexecutingthefollowingsteps:
resample ak ∼Discrete(ak |wk ),
t−1 t−1 t−1
propose zk ∼r(zk|y
,zak
t−1),
t t t t−1
p(y
,zk|zak
t−1)
reweight wk = t t t−1 ,
t ak
r(zk|y ,z t−1)
t t t−1
withwk = w tk . Weobtain,attimet,afilteringapproximationtotheposterior,
t (cid:80)K wj
j=1 t
K
(cid:88)
q (z |y )= wkδ(zk ). (3)
filt 1:t 1:t t 1:t
k=1
Theunnormalisedweightsgiveanunbiasedestimatetothemarginallikelihood,
T K
(cid:89) 1 (cid:88)
pˆ(y )= wk. (4)
1:T K t
t=1 k=1
We now detail how we pick the proposal distribution r. For linear Gaussian observations G =
N(Bz ,Σ ), we set r(z |y ,z ) = p(z |y ,z ), as this is available in closed form and is
t y t t t−1 t t t−1
optimal(inthesensethatitminimisesthevarianceoftheimportanceweights[37])
r(z |y ,z )=N((I−KB)F(z )+Ky ,(I−KB)Σ ), (5)
t t t−1 t−1 t z
with K the Kalman Gain: K = Σ BT(BΣ BT +Σ )−1. For non-linear observations, we can
z z y
notinverttheobservationprocessinclosedform,soweinsteadjointlyoptimizeaparameterized
3‘encoding’distributione(z |y )(asinavariationalautoencoder[38]). Inparticular,weassumee
t t−t′:t
tobeamultivariatenormalwithdiagonalcovariance,whichweparameterizebyacausalconvolutional
neuralnetwork,suchthateachlatentisconditionedonthet′ latestobservations. Wethenusethe
followingproposal:
r(z |z ,y )∝e(z |y )p(z |z ), (6)
t t−1 t−t′:t t t−t′:t t t−1
wherewenowalsoassumep(z |z )hasadiagonalcovariancematrix.
t t−1
2.2.2 RelationshiptoGeneralisedTeacherForcing
Inourapproach, themeanoftheproposaldistributionattimetisalinearinterpolationbetween
the RNN predicted state F(z ) and a data-inferred state zˆ . A recent study obtained state-of-
t−1 t
theartresultsforreconstructingdynamicalsystemsbyfittingdeterministicRNNswithamethod
calledGeneralisedTeacherForcing(GTF),whichalsolinearlyinterpolatesbetweenadata-inferred
andanRNNpredictedstateateverytime-step[8];themodelpropagatesforwardintimeasz =
t
(1−α)F(z )+αzˆ . [8]showedthatbychoosingtheappropriateα,onecancompletelyavoid
t−1 t
explodinggradients,whilestillallowingbackpropagationthroughtime,andthusobtaininglong-term
stablesolutions[39]. TheoptimalαcanbepickedbasedonthemaximumLyaponuvexponentofthe
system(ameasureofhowfasttrajectoriesdivergeinachaoticsystem)[8].
By including the RNN in the proposal distribution, we similarly to GTF allow backpropagation
throughtimethroughthesampledtrajectories. Theinterpolationisgivenbyα=Σ BT(BΣ BT+
z z
Σ )−1BinEq.5,andinEq.6byα=Σ (Σ +Σ )−1,whereΣ isthepredictedvarianceofthe
y z z zˆt zˆt
encodingnetwork. Thus,insteadofinterpolatingbasedonanestimateofhowchaoticthesystemis,
ourapproachinterpolatesadaptively(everytimestep,ifEq.6isused)basedonhowrelativelynoisy
thetransitiondistributioniswithrespecttothedata-inferredstatesattimet,analogousto,e.g.,the
gainofaKalmanfilter.
2.2.3 Variationalobjective
WecanfitourRNNstodatabyusingSMCtospecifyavariationalobjective[33–35]. Invariational
inference,wespecifyafamilyofparameterizeddistributionsQ,andoptimizethoseparameterssuch
thatadivergence(usuallytheKLdivergence)betweenthevariationaldistributionq(z )∈Qand
1:T
thetrueposteriorp(z |y )isminimized. Wedothisbymaximisingalowerbound(ELBO)to
1:T 1:T
theloglikelihoodp(y ). Inparticular,wecanuseEq.4tospecifytheELBOobjective[33–35]
1:T
L=E [logpˆ(y )], (7)
qsmc(z1 1: :K T,a1 1: :K T−1|y1:T) 1:T
withq thesamplingdistribution:
smc
q (z1:K,a1:K |y ) = (cid:81)K r(zk|y )(cid:81)K (cid:81)T r(zk|zak t−1y )Discrete(ak |wk ). We
smc 1:T 1:T−1 1:T k=1 1 1 k=1 t=2 t t−1 t t−1 t−1
approximatethisobjectivewithMonteCarlosamplesduringtraining. Assuggestedin[33–35,40],
we use biased gradients during optimization by dropping high-variance terms arising from the
resampling.
2.3 Findingfixedpointsinpiecewise-linearlow-rankRNNs
After having learned our model, we can gain insight into the mechanisms underlying the data
generation process by reverse engineering the learned dynamics [15], e.g. by calculating their
fixed points. Here, we show that the fixed points can be found analytically and efficiently for
low-rank networks with piecewise-linear activation functions. This class of activation functions
ϕ(x )=(cid:80)Db(i,d)max(x −h(d),0)includes,e.g.,thestandardReLU(ϕ(x )=max(x −h ,0))
i d i i i i i
orthe‘clipped’variant(ϕ(x )=max(x +h ,0)−max(x ,0))[8]whichweusedinallexperiments
i i i i
withreal-worlddatahere.
Naively,thecostoffindingallfixedpointspiecewise-linearnetworksscalesexponentiallywiththe
numberofunitsinthenetworks:wewouldhavetosolve(D+1)N systemsofN equations[9,24]. If
networksarelowrank,itisstraightforwardtoshowthatwecanreducethiscosttosolving(D+1)N
systemsofRequations(SeeSupplementA.1). Inaddition,however,weshowthatthecomputational
costcanbegreatlyreducedfurther: Onecanfindallfixedpointsinacostthatispolynomialinstead
ofexponentialinthenumberofunits:
4Proposition 1. Assume Eq. 1, with J of rank R and piecewise-linear activations ϕ. For fixed
rank R and fixed number of basis functions D, we can find all fixed points in the absence of
noise, that is all x for which dx = 0, by solving at most O(NR) linear systems of R equations.
dt
Proof. SeeSupplementA.1.
Sketch. AssumingD =1,activationsϕ=max(0,x −h );N unitswillpartition
i i
thefullphasespaceinto2N regionsinwhichthedynamicsarelinear(2units,4
regionsinFig.2). Wecanthus,inprinciple,solveforallfixedpointsbysolving
allcorrespondinglinearsystemsofequations[9,24]. Ifdynamicsareconfinedto
theR-dimensionalsubspacespannedbythecolumnsofM,onlyasubsetofthe
linearregions(3inFig.2)canbereached. Eachunitpartitionsthespacespanned
bythecolumnsofMwithahyperplane(pinkpointsinFig.2). Theamountof
linearregionsinM,becomesequivalentto‘howmanyregionscanwecreatein
Figure2: Proof
R-dimensionalspacewithN hyperplanes?’ UsingZaslavsky’stheorem[41],we
sketch.
canshowthatthisatmost(cid:80)R (cid:0)N(cid:1) ∈O(NR).
r=0 r
3 EmpiricalResults
3.1 RNNsrecovergroundtruthdynamicsinstudent-teachersetups
Figure3:RNNsrecoverdynamicsinteacher-studentsetups. a)Examplegroundtruthlatenttrajectory
andphaseplaneoflow-rankRNNtrainedtooscillate(topleft)andnoisyobservationsofneuron
activity(topright;6/20shown). Asecondlow-rankRNNtrainedontheactivityofthefirstrecovers
groundtruthdynamics. b)Sameset-up,butwithPoissonobservations. c)Theteachernetworkwas
trainedonataskwhereithastoprovideanoutputcorrespondingto8differentanglesdependingon
aninputcue.Thestudentnetwork,whengiventhesameinputduringfitting,recoverstheapproximate
ringattractorwith8stablefixedpoints. d)Mean(±1SD)autocorrelationofthelatentsofthemodels
frompanela,showtheoscillationfrequencyiscaptured,aswellasthedecorrelationduetorecurrent
noise. Thescaleoftheobservedratesalsoagreesbetweenstudentandteacher. e)MeanratesandISI
betweenstudentandteacherunitsofpanelbmatch. f)Exampleratedistributionofoneunitofthe
teacherandstudentRNN(ofpanelc),afteronsetofthe8differentstimuli.
5Wevalidatedourmethodusingseveralstudent-teachersetups(Fig.3;additionalstatisticsinFig.S4).
Wefirsttraineda‘teacher’RNN,withtheweightmatrixconstrainedtorank2,tooscillate. Wethen
simulatedmultipletrajectorieswithahighlevelofstochasticityinthelatentdynamics(Fig.3a,top
left)andadditionaladditiveGaussianobservationnoise(Fig.3b,topleft)ontheobservedneuron
activity(y ∼ N(x ,σ ),withx = Mz). Asecond‘student’RNNwasthenfittothedatadrawn
i i y
fromtheteacher,andbothrecoveredthetruelatentdynamicalsystem,aswellastherightlevelof
stochasticity(Fig.3a,bottom).
Giventhatneuronsemitactionpotentials,whicharecommonlyapproximatedasdiscreteevents,we
repeatedtheexperimentwithPoissonobservationsgeneratedaccordingtoy ∼Pois(softplus(w x −
i i i
b )). The student RNN again recovers the oscillatory latent dynamics. Note that because of the
i
affinetransformationintheobservationmodel,theinferreddynamicscanbescaledandtranslated
withrespecttotheteachermodel. Toverifythatsamplesfromourinferredmodelfollowthesame
distributionassamplesfromtheteachermodel,wecomputedseveralstatistics,whichallshowa
closematch(Fig.3e;Fig.S4).
Inourfinalteacher-studentsetup,weverifiedtheabilitytorecoverdynamicswhenthereareknown
stimuliorcontexts. Inparticular,wetrainedarank-2RNNonataskwhere,ateachtrial,itreceives
atransientpulseinputcorrespondingtoaparticularangleθ(givenassin(θ),cos(θ)),andisasked
toprovideoutputmatchingtheinputafterstimulusoffset. TheteacherRNNlearnstoperformthe
taskbyusinganapproximateringattractor,withastablefixedpointforeachofthe8angles-which
thestudentRNNaccuratelyinfers(Fig.3c). Here, weinferredallfixedpointsbymakinguseof
Preposition1.
3.2 Stochasticityallowsrecoveringlow-dimensionallatentsunderlyingEEGdata
After validating our model on a toy example,
we went on to several challenging real-world
datasets. WefirstusedanEEGdataset[42,43]
with64channelscontainingoneminuteofcon-
tinuousdatasampledat160Hz(Fig.4). This
datasetwasrecentlyusedinastudywheregen-
eralizedteacherforcing(GTF)wasusedtofit
deterministicRNNswithlow-rankstructure[8].
TheGTFmethodobtainsstate-of-the-artresults
on several dynamical systems reconstruction
tasks.ItoutperformedSINDy[44],neuraldiffer-
entialequations[45],Long-Expressive-Memory
Figure4: ExamplegroundtruthEEG[42,43]and
[46],andothermethods,whileusingasmaller
(unconditionally)generatedtracesbyourmodel.
latentdynamicalsystem.
Shownare5/64EEGchannels.
HereweshowthatusingastochasticRNNwith
SMCinsteadofadeterministicRNNwithGTF,
wecandecreasethelatentdimensionalityevenfurther, from16tojust3latents, whilematching
the original reconstruction accuracy (Table 1). We hypothesize this is because the data can be
wellexplainedbystochastictransitionswithsimpleunderlyingdynamicsasopposedtocomplex
deterministicchaos.
Table1: LowerdimensionallatentdynamicsthanSOTAatsamesamplequality. Wereportmedian±
medianabsolutedeviationover20independenttrainingruns,‘dim’referstothedimensionalityof
themodel’sunderlyingdynamicsand|θ|denotesthetotalnumberoftrainableparameters. Values
forGTFtakenfrom[8].
Dataset Method D ↓ D ↓ dim |θ|
stsp H
GTF[8] 2.1±0.2 0.11±0.01 16 17952
EEG
adaptiveGTF[8] 2.4±0.2 0.13±0.01 16 17952
(64d)
SMC(ours) 2.2±0.2 0.11±0.01 3 3920
WeevaluatedsamplesfromourRNNwithtwomeasureswhichwereusedin[8],oneKLdivergence-
basedmeasurebetweenthestates(D ),andonemeasureovertime,basedonthepowerspectraof
stsp
6generatedandinferreddynamics(D ;seeSupplementD.3.3). Unlike[8],whoappliedsmoothing,
H
weoptimizedourmodelsdirectlyontherawEEGdata.
3.3 Interpretablelatentdynamicsunderlyingspikesrecordedfromrathippocampus
Figure5: RNNsreproducethestationarydistributionofspikingdata. a)Wefitarank-3RNNto
spikedatarecordedfromrathippocampus[47,48](left),andgeneratenewsamplesfromtheRNN
(right). b)Singleneuronstatistics. Meanratesandmeansofinterspikeinterval(ISI)distributionsof
alongtrajectoryofdatageneratedbytheRNN(gen)matchthoseofaheld-outsetofdata(test). Asa
referenceweadditionallycomputedthesamestatisticsbetweenthetrainandtestset. c)Population
levelstatistics. Weplotthepairwisecorrelationsbetweenallneuronsforgenerateddataagainstthe
pairwisecorrelationsinthetestdata. d)ThecorrespondinglatentsgeneratedbyrunningtheRNN
lookvisuallysimilartothelocalfieldpotential(LFP).e)Thepeakinthepowerspectrummatches
betweenlatentsandLFP.f)TheposteriorlatentsshowcoherencewiththeLFP.Asareference,we
computethecoherencebetweentheLFPandthelatentsgeneratedbytheRNN.
We next investigated how well our model can capture the distribution of non-continuous time
series. In particular, we used publicly available electrophysiological recordings from the hip-
pocampus of rats running to drops of water or pieces of food [47, 48]. We binned the spik-
ing data into 10ms bins and fit a rank-3 RNN to ∼850 s of data. Samples generated by run-
ning the fit RNN autonomously closely matched the statistics of the recordings (Fig. 5a-c).
Previousinvestigationsintothisdatasethaveexaminedtherelationship
betweenspikesandtheta(5-10Hz)oscillationsinthelocalfieldpotential
([47]), and found that units were locked to the LFP rhythm, with the
relativephasedependingonthesubregionsfromwhichtheunitswere
recorded. ThelatentsgeneratedbytheRNNarevisuallysimilartothe
average local field potential (Fig. 5d) and match its power spectrum
(Fig.5e). Whilethemodelwassolelytrainedonthespikes,theposterior
latents(Eq.3)haveaclearphaserelationshipwiththeLFP,asevidenced
byahighcoherencebetweentheposteriorlatentsandLFP.Incontrast,
andasexpected,latentsfromrunningtheRNNarenotcorrelatedwith
Figure 6: Posterior la-
theLFP(Fig.5f).
tents of our model (fit
Units in rat hippocampus have been shown to code for position, i.e. solely spikes) can be
throughplacecells[49],whichtendtofireiftheanimalisataspecific used to predict rat posi-
location. Tofurtherinvestigatehowwellwecanmodelrecordingsfrom tion.
thehippocampus,wefitarank-4RNNtoanadditionalsetofrecordings
ofratsrunningonalineartrack[50–52]. Asin[53],wefocusonlyonthespikesrecordedwhile
the rat is moving, which we bin into 25 ms bins. The RNN again accurately reconstructs the
distributionofspikesandagainhaslatentoscillationswithfrequencymatchingtheLFP(Fig.S5).
Whilesolelytrainedonspikes,theposteriorlatentsalsoallowedustopredictthepositionoftherats
withreasonableaccuracy(R2 =0.62±0.073mean±SD,N=4RNNs;Fig.6).
7Figure7: Inferredandgenerateddynamicsfromthemodelfittomacaquespikingactivityduring
reachingtask. a)Latentstatesinferredfromthemacaquespikingdatapriortomovementinitiation
(‘pre-movement’) and during movement execution (‘movement’), colored by the intended reach
target. b)Reachtrajectoriesdecodedfrommodel-inferredneuralactivity. c)Dissimilaritymatrices
computedacrossthesevenconditions(i.e.,thesevencolorsina-b)forper-neuronmeanfiringrate
andISI.Wegenerateneuralactivityfromthemodelbyprovidingthesameconditioningstimulias
in the real data. Then, for each statistic, we compute and show the correlation distance between
conditionsintherealdata(left)andmodel-generateddata(right). d,e)Sameasa-b,butwithlatent
activity and behavioral predictions generated from the model with conditioning inputs including
directionsnotseenintherealdata(e.g.,limegreen). Forclarity,weshowonlyasubsetofconditions
inthedecodedreaches.
3.4 Extractingstimulus-conditioneddynamicsinmonkeyreachingtask
Wefurtherinvestigatedhowwellwecanrecoverstimulus-conditioneddynamics. Weappliedour
methodtospikingactivityrecordedfromthemotorandpremotorcorticesofamacaqueperforming
a delayed reaching task. This type of data has been popular for investigating neural dynamics
underlying the control of movement [2, 3] and evaluating neuroscientific latent variable models
[7,54,55]. Wefirstvalidatedtheabilityofourmethodtoobtainasensibleposteriorbyevaluatingit
ontheNeuralLatentsBenchmark[55](SupplementB.3,TableS2).
Wethenwentontoaset-upwhereweexplicitlyconditionedourmodelonexternalcontext. For
simplicity,weconstrainedourexperimenttotrialswithstraightreachtrajectoriesinthedata. We
fitarank-5modeltothesedatawhileconditioningitsdynamicsonthetargetpositionbyproviding
thetargetpositionasinputtotheRNN.Ourmodelwasabletoinfersingle-triallatentdynamicsand
neuronfiringratesthatpredictreachvelocitywithhighaccuracyatlowerlatentdimensionalitiesthan
modelswithoutinputs(Fig.7b,R2 =0.90forthismodel,seeTableS3foradditionalstatistics).
Weexaminedtheposteriorlatentsinferredbythemodelandfoundthatourmodelrecoversstructured
andinterpretablelatentdynamics. Beforemovementonset,latentstatescorrespondedtotheintended
reach targets, which were near the edges of a rectangular screen (Fig. 7a, left), in line with [54].
Duringthemovementperiod,thelatentsfollowedparallelcurvedtrajectoriesthatpreservetarget
information(Fig.7a,right)andcanbedecodedtopredictmonkeyreachbehavior(Fig.7b).
WethengeneratedneuraldatafromtheRNNconditionedonstimulusinput. Again,thedistribution
ofspikesiswell-captured(Fig.S6). Weadditionallyevaluatedwhetherthemodelfaithfullycaptures
differencesinspikingstatisticsacrossthesevenreachdirections,findingreasonablecorrespondence
indissimilaritiesbetweenconditionsinthegeneratedandtherealdata(Fig.7c).Finally,wesimulated
ourtrainedRNNwithconditioninginputs,includingreachdirectionsnotpresentinthedata,and
found that the structured latent space recovered by the model enables realistic generalization to
unseenreachconditions(Fig.7d,e,limegreencondition).
83.5 Searchingforfixedpoints
In Proposition 1, we derived a bound on the number of
systems of equations one has to solve in order to find
allfixedpointsinpiece-wiselinearlow-rankRNNs. Re-
cently,anapproximatealgorithmforfindingfixedpoints
inpiece-wiselinearnetworkswasproposed[25].Here,we
performanexplorationintohowthiscomparestoouran-
alyticmethodbysearchingforfixedpointsoftheRNNin
Fig.3c(top).Forthesamenumberofmatrixinversescom-
puted by our analytic method, the approximate method
generally does not find all 17 fixed points (Fig. 8). We
note,however,that(unlikeours)theconvergenceofthe
Figure 8: Comparison of our ana-
approximatemethoddependsonthedynamicsoftheRNN,
lyticmethod(star)andtheapproximate
andasaresult,therearetheoreticalscenarioswherethe
methodproposedin[25](blue)forfind-
approximate method can be shown to be faster. Yet we
ingthefixedpointsoftheteacherRNN
empiricallyalsofoundscenarioswheretheapproximate
inFig.3c. WecanalsouseProposition
methodsfailedtoconvergewithinthetime-frameofour
1 to constrain the search space of the
experiments(Fig.S7).
approximatemethod(orange). Wehere
Ouranalyticmethodreliesontheinsightthatonlyasubset showthenumberoffixedpointsfound
ofalllinearsubregionsformedbythepiece-wiselinear as a function of the number of matrix
activationscanbereachedinlow-ranknetworks.Fornet- inversescomputed,witherrorbarsdenot-
workswithmoderaterank,thecostofsearchingthroughall ingtheminimumandmaximumamount
ofthesubregionsmightstillbetoohigh. Wecan,however, of fixed points found over 20 indepen-
hugelyreducethesearchspaceoftheapproximatemethod dentrunsofthealgorithm.
[25](from(D+1)N to(cid:80)R Dr(cid:0)N(cid:1) ),atanupfrontcost
r=0 r
(SupplementB.5;orangelineinFig.8).
4 Discussion
Here we proposed to fit low-rank RNNs to neural data using variational sequential Monte Carlo.
TheresultingRNNsaregenerativemodelswithtractableunderlyingdynamics,fromwhichwecan
samplelong,stabletrajectoriesofrealisticdata. Wevalidatedourmethodonseveralteacher-student
setupsanddemonstratedtheeffectivenessofourmethodonmultiplechallengingreal-worldexamples,
wherewegenerallyneededalatentdynamicalsystemwithveryfewdimensionstoaccuratelymodel
thedata. Besidesourempiricalresults,weobtainedatheoreticalboundonthecostoffindingfixed
pointsforRNNswithpiecewise-linearactivationfunctionswhentheyarealsolow-rank.
Adding stochastic transitions to low-rank RNNs can potentially hugely reduce the rank required
toaccuratelymodelobserveddata,asdemonstratedherewithanetworkfittoEEGdatawherewe
couldreducethedimensionalityfrom16tojust3. WhilemanymethodsthatfitRNNstoneural
data (e.g, [6–8, 10–12]) assume deterministic transitions, there is a rich literature concentrating
on probabilistic sequence models in neuroscience (e.g., [28–32]). In particular, a recent work
termedFINDR[31]usesvariationalinference(butnotSMC),tosimilarlyfindverylow-dimensional
dynamicalsystemsunderlyingneuraldata. Thesestochasticdynamicalsystemswereparameterized
usingneuraldifferentialequations[45]. WhileEq.2canbeseenasaneuraldifferentialequationwith
onehiddenlayer[8],ourparticularformulationallowsustofinditsfixed-pointseffectivelyandmap
backtoaregular,mechanisticallyinterpretableRNN(Eq.1)afterfitting,whichenablesadditional
investigationsintoneuralpopulationdynamics[18,20–22].
Thereasonwecandothemappingbetweenalow-rankRNN(Eq.1)andalatentdynamicalsystem
(Eq.2)cruciallyreliesonourassumptionthatsamplesfromtherecurrentnoiseprocessarecorrelated,
suchthattheyliewithinthecolumn-spaceofM. [56]showedthatforlinearlow-rankRNNsarbitrary
covariancesinthefullN dimensionalspacecanbeused,whenincreasingthedimensionalityofthe
latentdynamicstotwicetherankR(tothecolumnspaceofbothMandN),thishoweverdoesnot
generalisetoournon-linearsetting. Wedoexpectcorrelatedrecurrentnoisetobeappropriatefor
modelingstochasticityarisingfromunobservedinputsorfrompartialobservations[56]—additionally,
correlatednoiseconstitutedapragmaticchoicethatallowsbuildinganstochasticmodelthatcan
allowfortrial-by-trialvariabilitywhilemaintainingthetractabilityoflow-rankdeterministicRNNs.
9Still,futureworkcaninvestigatetrainingnetworkswithmorerelaxedassumptionsontherecurrent
noisemodels,includingextensionstonon-Gaussiannoise-processes. Thelattercouldbeofparticular
interestifmorebiologicallyplausible(i.e.,spiking)neuronswereusedintherecurrence[36,57].
Ourresultsalsoopenupfurtheravenuestoexplorequestionsinneuroscience. Therelationbetween
LFP and spike (phase) in the hippocampus has been of great interest [47, 58–60]. While we
performed some preliminary investigation into the relation between the inferred latents and the
localfieldpotential,furtherstudiescouldperformasystematicinvestigationintotheirrelation,for
instance,byusingamulti-modalsetup[13],ortoinvestigatemulti-regiontemporalrelationshipsand
interactions[10].
Takentogether,byinferringlow-rankRNNswithvariationalSMC,weobtainedgenerativemodelsof
neuraldatawhosetrajectoriesmatchobservedtrial-to-trialvariability,andwhoseunderlyinglatent
dynamicsaretractable.
Acknowledgments
ThisworkwassupportedbytheGermanResearchFoundation(DFG)throughGermany’sExcellence
Strategy(EXC-Number2064/1, PN390727645)andSFB1233(PN276693517), SFB1089(PN
227953431)andSPP2041(PN34721065),theGermanFederalMinistryofEducationandResearch
(TübingenAICenter,FKZ:01IS18039;DeepHumanVision,FKZ:031L0197B),theElseKrönerFre-
seniusStiftung(ProjectClinbrAIn),andtheEuropeanUnion(ERC,DeepCoMechTome,101089288).
MPandMGaremembersoftheInternationalMaxPlanckResearchSchoolforIntelligentSystems
(IMPRS-IS). We thank Cornelius Schröder for feedback on the manuscript, and all members of
Mackelabfordiscussionsthroughouttheproject.
Codeavailability
Codetoreproduceourresultsisavailableathttps://github.com/mackelab/smc_rnns.
References
[1] M.MChurchland,B.MYu,MSahani,andK.VShenoy. Techniquesforextractingsingle-trialactivity
patternsfromlarge-scaleneuralrecordings. CurrentOpinioninNeurobiology,17(5):609–618,2007.
[2] K.VShenoy,MSahani,andM.MChurchland. Corticalcontrolofarmmovements:Adynamicalsystems
perspective. AnnualReviewofNeuroscience,36(1):337–359,2013.
[3] JGallego,MPerich,LMiller,andSSolla. Neuralmanifoldsforthecontrolofmovement. Neuron,94:
978–984,2017.
[4] SVyas,M.DGolub,DSussillo,andK.VShenoy. Computationthroughneuralpopulationdynamics.
AnnualReviewofNeuroscience,43(1):249–275,2020.
[5] D.LBarackandJ.WKrakauer. Twoviewsonthecognitivebrain. NatureReviewsNeuroscience,22(6):
359–371,2021.
[6] DSussilloandL.FAbbott. Generatingcoherentpatternsofactivityfromchaoticneuralnetworks. Neuron,
63:544–557,2009.
[7] CPandarinath,D.JO’Shea,JCollins,RJozefowicz,S.DStavisky,J.CKao,E.MTrautmann,M.T
Kaufman,S.IRyu,L.RHochberg,J.MHenderson,K.VShenoy,L.FAbbott,andDSussillo. Inferring
single-trialneuralpopulationdynamicsusingsequentialauto-encoders. NatureMethods,15(10):805–815,
2018.
[8] FHess,ZMonfared,MBrenner,andDDurstewitz. Generalizedteacherforcingforlearningchaotic
dynamics. InProceedingsofthe40thInternationalConferenceonMachineLearning,ICML’23,2023.
[9] D Durstewitz. A state space approach for piecewise-linear recurrent neural networks for identifying
computationaldynamicsfromneuralmeasurements. PLOSComputationalBiology,13(6):1–33,2017.
[10] M. G Perich, C Arlt, S Soares, M. E Young, C. P Mosher, J Minxha, E Carter, U Rutishauser, P. H
Rudebeck,C.DHarvey,andKRajan. Inferringbrain-wideinteractionsusingdata-constrainedrecurrent
neuralnetworkmodels. bioRxiv:2020.12.18.423348,2021.
10[11] AValente,J.WPillow,andSOstojic. Extractingcomputationalmechanismsfromneuraldatausing
low-rankrnns. InAdvancesinNeuralInformationProcessingSystems,volume35,2022.
[12] FDinc,AShai,MSchnitzer,andHTanaka. CORNN:Convexoptimizationofrecurrentneuralnetworks
forrapidinferenceofneuraldynamics. InThirty-seventhConferenceonNeuralInformationProcessing
Systems,2023.
[13] MBrenner,FHess,GKoppe,andDDurstewitz. Integratingmultimodaldataforjointgenerativemodeling
ofcomplexdynamics. InForty-firstInternationalConferenceonMachineLearning,2024.
[14] OBarak. Recurrentneuralnetworksasversatiletoolsofneuroscienceresearch. CurrentOpinionin
Neurobiology,46:1–6,2017. ComputationalNeuroscience.
[15] DSussilloandOBarak. OpeningtheBlackBox: Low-DimensionalDynamicsinHigh-Dimensional
RecurrentNeuralNetworks. NeuralComputation,25(3):626–649,2013.
[16] H.SSeung. Howthebrainkeepstheeyesstill. ProceedingsoftheNationalAcademyofSciences,93(23):
13339–13344,1996.
[17] CEliasmithandC.HAnderson.NeuralEngineering(ComputationalNeuroscienceSeries):Computational,
Representation,andDynamicsinNeurobiologicalSystems. MITPress,Cambridge,MA,USA,2002.
[18] FMastrogiuseppeandSOstojic. Linkingconnectivity,dynamics,andcomputationsinlow-rankrecurrent
neuralnetworks. Neuron,99(3):609–623.e29,2018.
[19] FSchuessler,ADubreuil,FMastrogiuseppe,SOstojic,andOBarak. Dynamicsofrandomrecurrent
networkswithcorrelatedlow-rankstructure. PhysicalReviewResearch,2(1):013111,2020.
[20] MBeiran,ADubreuil,AValente,FMastrogiuseppe,andSOstojic. ShapingDynamicsWithMultiple
PopulationsinLow-RankRecurrentNetworks. NeuralComputation,33(6):1572–1615,2021.
[21] ADubreuil,AValente,MBeiran,FMastrogiuseppe,andSOstojic. Theroleofpopulationstructurein
computationsthroughneuraldynamics. NatureNeuroscience,25(6):783–794,2022.
[22] MPals,J.HMacke,andOBarak. Trainedrecurrentneuralnetworksdevelopphase-lockedlimitcyclesin
aworkingmemorytask. PLOSComputationalBiology,20(2):1–23,2024.
[23] CCurto,JGeneson,andKMorrison. FixedPointsofCompetitiveThreshold-LinearNetworks. Neural
Computation,31(1):94–155,2019.
[24] MBrenner,FHess,J.MMikhaeil,L.FBereska,ZMonfared,P.-CKuo,andDDurstewitz. Tractable
dendriticRNNsforreconstructingnonlineardynamicalsystems. InProceedingsofthe39thInternational
ConferenceonMachineLearning,volume162ofProceedingsofMachineLearningResearch,2022.
[25] LEisenmann,ZMonfared,NGöring,andDDurstewitz. Bifurcationsandlossjumpsinrnntraining. In
AdvancesinNeuralInformationProcessingSystems,volume36,2023.
[26] KMorrison,ADegeratu,VItskov,andCCurto. Diversityofemergentdynamicsincompetitivethreshold-
linearnetworks. SIAMJournalonAppliedDynamicalSystems,23(1):855–884,2024.
[27] J. P Cunningham and B. M Yu. Dimensionality reduction for large-scale neural recordings. Nature
Neuroscience,17(11):1500–1509,2014.
[28] BPetreska,B.MYu,J.PCunningham,GSanthanam,SRyu,K.VShenoy,andMSahani. Dynamical
segmentationofsingletrialsfrompopulationneuraldata. InAdvancesinNeuralInformationProcessing
Systems,volume24,2011.
[29] J.HMacke,LBuesing,J.PCunningham,B.MYu,K.VShenoy,andMSahani. Empiricalmodelsof
spikinginneuralpopulations. InAdvancesinNeuralInformationProcessingSystems,volume24,2011.
[30] SLinderman,MJohnson,AMiller,RAdams,DBlei,andLPaninski. Bayesianlearningandinference
inrecurrentswitchinglineardynamicalsystems. InProceedingsofthe20thInternationalConference
onArtificialIntelligenceandStatistics,volume54ofProceedingsofMachineLearningResearch,pages
914–922,2017.
[31] T.DKim,T.ZLuo,TCan,KKrishnamurthy,J.WPillow,andC.DBrody. Flow-fieldinferencefrom
neuraldatausingdeeprecurrentnetworks. bioRxiv:2023.11.14.567136,2023.
[32] YZhao,JNassar,IJordan,MBugallo,andIPark. Streamingvariationalmontecarlo. IEEETransactions
onPatternAnalysis&MachineIntelligence,45(01):1150–1161,2023.
11[33] T.ALe,MIgl,TRainforth,TJin,andFWood. Auto-encodingsequentialmontecarlo. InInternational
ConferenceonLearningRepresentations,2018.
[34] C.JMaddison,JLawson,GTucker,NHeess,MNorouzi,AMnih,ADoucet,andYTeh. Filtering
variationalobjectives. InAdvancesinNeuralInformationProcessingSystems,volume30,2017.
[35] CNaesseth,SLinderman,RRanganath,andDBlei. Variationalsequentialmontecarlo. InProceed-
ingsoftheTwenty-FirstInternationalConferenceonArtificialIntelligenceandStatistics,volume84of
ProceedingsofMachineLearningResearch,2018.
[36] C Sourmpis, C Petersen, W Gerstner, and G Bellec. Trial matching: capturing variability with data-
constrainedspikingneuralnetworks. InAdvancesinNeuralInformationProcessingSystems,volume36,
2023.
[37] ADoucetandA.MJohansen. Atutorialonparticlefilteringandsmoothing: Fifteenyearslater. The
OxfordHandbookofNonlinearFiltering,pages656–704,2011.
[38] D.PKingmaandMWelling. Auto-EncodingVariationalBayes. In2ndInternationalConferenceon
LearningRepresentations,ICLR,ConferenceTrackProceedings,2014.
[39] KDoya. Bifurcationsofrecurrentneuralnetworksingradientdescentlearning. IEEETransactionson
NeuralNetworks,1993.
[40] JZennandRBamler. Resamplinggradientsvanishindifferentiablesequentialmontecarlosamplers. In
TheFirstTinyPapersTrackatICLR2023,TinyPapers@ICLR2023,2023.
[41] TZaslavsky. Facinguptoarrangements: face-countformulasforpartitionsofspacebyhyperplanes.
MemoirsofAmericanMathematicalSociety,154:1–95,1975.
[42] G Schalk, D McFarland, T Hinterberger, N Birbaumer, and J Wolpaw. Bci2000: a general-purpose
brain-computerinterface(bci)system. IEEETransactionsonBiomedicalEngineering,51(6):1034–1043,
2004.
[43] GMoody,RMark,andAGoldberger. Physionet:aresearchresourceforstudiesofcomplexphysiologic
andbiomedicalsignals. Computersincardiology,27:179–82,2000.
[44] S.LBrunton,J.LProctor,andJ.NKutz. Discoveringgoverningequationsfromdatabysparseiden-
tificationofnonlineardynamicalsystems. ProceedingsoftheNationalAcademyofScience,113(15):
3932–3937,2016.
[45] R.T.QChen,YRubanova,JBettencourt,andD.KDuvenaud. Neuralordinarydifferentialequations. In
AdvancesinNeuralInformationProcessingSystems,volume31,2018.
[46] T. K Rusch, S Mishra, N. B Erichson, and M. W Mahoney. Long expressive memory for sequence
modeling. InInternationalConferenceonLearningRepresentations,2022.
[47] KMizuseki,ASirota,EPastalkova,andGBuzsáki. Thetaoscillationsprovidetemporalwindowsforlocal
circuitcomputationintheentorhinal-hippocampalloop. Neuron,64(2):267–280,2009.
[48] KMizuseki,ASirota,EPastalkova,andGBuzsáki. Multi-unitrecordingsfromtherathippocampusmade
duringopenfieldforaging. Database:CRCNS,2009.
[49] JO’Keefe. Placeunitsinthehippocampusofthefreelymovingrat. ExperimentalNeurology,51(1):
78–109,1976.
[50] A. D Grosmark and G Buzsáki. Diversity in neural firing dynamics supports both rigid and learned
hippocampalsequences. Science,351(6280):1440–1443,2016.
[51] ZChen,A.DGrosmark,HPenagos,andM.AWilson. Uncoveringrepresentationsofsleep-associated
hippocampalensemblespikeactivity. ScientificReports,6,2016.
[52] L.JGrosmark,A.D.andGBuzsáki. Recordingsfromhippocampalareaca1,pre,duringandpostnovel
spatiallearning. Database:CRCNS,2016.
[53] DZhouandX.-XWei. Learningidentifiableandinterpretablelatentmodelsofhigh-dimensionalneural
activityusingpi-vae. InHLarochelle,MRanzato,RHadsell,MBalcan,andHLin,editors,Advancesin
NeuralInformationProcessingSystems,volume33,pages7234–7247.CurranAssociates,Inc.,2020.
[54] GSanthanam,B.MYu,VGilja,S.IRyu,AAfshar,MSahani,andK.VShenoy. Factor-analysismethods
forhigher-performanceneuralprostheses. JournalofNeurophysiology,102(2):1315–1330,2009.
12[55] FPei, JYe, D.MZoltowski, AWu, R.HChowdhury, HSohn, J.EO’Doherty, K.VShenoy, M.T
Kaufman,MChurchland,MJazayeri,L.EMiller,JPillow,I.MPark,E.LDyer,andCPandarinath.
Neurallatentsbenchmark’21:Evaluatinglatentvariablemodelsofneuralpopulationactivity. InAdvances
inNeuralInformationProcessingSystems(NeurIPS),TrackonDatasetsandBenchmarks,2021.
[56] AValente,SOstojic,andJ.WPillow.ProbingtheRelationshipBetweenLatentLinearDynamicalSystems
andLow-RankRecurrentNeuralNetworkModels. NeuralComputation,34(9):1871–1892,2022.
[57] LCimeša,LCiric,andSOstojic. Geometryofpopulationactivityinspikingnetworkswithlow-rank
structure. PLOSComputationalBiology,19(8):1–34,2023.
[58] JO’KeefeandM.LRecce. Phaserelationshipbetweenhippocampalplaceunitsandtheeegthetarhythm.
Hippocampus,3(3):317–330,1993.
[59] GBuzsáki. RhythmsoftheBrain. OxfordUniversityPress,1edition,2006.
[60] SLiebe,JNiediek,MPals,T.PReber,JFaber,JBostroem,C.EElger,J.HMacke,andFMormann.Phase
offiringdoesnotreflecttemporalorderinsequencememoryofhumansandrecurrentneuralnetworks.
bioRxiv:2022.09.25.509370,2022.
[61] LSchläfli. TheoriedervielfachenKontinuität. BirkhäuserBasel,Basel,1901.
[62] R.CBuck. Partitionofspace. TheAmericanMathematicalMonthly,50(9):541–544,1943.
[63] RStanley. Anintroductiontohyperplanearrangements. GeometricCombinatorics,13:389–496,2007.
[64] B.MYu,J.PCunningham,GSanthanam,SRyu,K.VShenoy,andMSahani. Gaussian-processfactor
analysisforlow-dimensionalsingle-trialanalysisofneuralpopulationactivity. InDKoller,DSchuurmans,
YBengio,andLBottou,editors,AdvancesinNeuralInformationProcessingSystems,volume21,2008.
[65] JYeandCPandarinath.Representationlearningforneuralpopulationactivitywithneuraldatatransformers.
Neurons,Behavior,Dataanalysis,andTheory,5(3),2021.
[66] APaszke,SGross,FMassa,ALerer,JBradbury,GChanan,TKilleen,ZLin,NGimelshein,LAntiga,
ADesmaison,AKopf,EYang,ZDeVito,MRaison,ATejani,SChilamkurthy,BSteiner,LFang,JBai,
andSChintala. Pytorch: Animperativestyle,high-performancedeeplearninglibrary. InAdvancesin
NeuralInformationProcessingSystems,volume32,2019.
[67] AOrvieto,S.LSmith,AGu,AFernando,CGulcehre,RPascanu,andSDe.Resurrectingrecurrentneural
networksforlongsequences. InProceedingsofthe40thInternationalConferenceonMachineLearning,
ICML’23,2023.
[68] LLiu,HJiang,PHe,WChen,XLiu,JGao,andJHan. Onthevarianceoftheadaptivelearningrateand
beyond. InInternationalConferenceonLearningRepresentations,2020.
A Supplementalmaterial
A.1 Proofofpreposition1
A.1.1 Problemdefinition
Weareinterestedinfindingallfixedpointsofthefollowingequation:
dx
τ =−x(t)+Jϕ(x(t)), (8)
dt
withx(t)∈RN,element-wisenonlinearityϕ(x )=(cid:80)Db(d)max(x −h(d))andlow-rankmatrix
i d i i i
J = MNT,withM,N ∈ RN×R andR ≤ N. Sinceτ onlyscalesthespeedofthedynamics,we
will,forconvenienceandwithoutlossofgenerality,assumeτ =1.
13A.1.2 Preliminaries: FixedpointsinPiecewise-linearRNNs
First,webrieflyrepeatresultsfrom[9]. AssumeD = 1,ϕ(x ) = max(x −h ). Tofindallfixed
i i i
pointsofEq.8,startbyredefiningϕbyintroducingadiagonalindicatormatrix:
d 
1
d
 2 
D Ω =

...  , (9)
d
N
(cid:26)
1, ifx >h
withd = i i.
i 0, otherwise
ThenourRNNequation,foragivenxandcorrespondingD reads:
Ω
dx
=−x(t)+JD x(t)−JD h.
dt Ω Ω
Eachofthe2N configurationofD correspondstoaregioninwhichthedynamicsarelinear. Thus,
Ω
foreachconfiguration,wecansolve:
0=−x+JD x−JD h,
Ω Ω
x∗ =(JD −I)−1JD h.
Ω Ω
Next,wecheckwhethertheobtainedx∗isconsistentwiththeassumedD (Eq.9). Ifso,wefound
Ω
afixedpointoftheRNN.Wehavetocheck,asthesolutiontothesystemoflinearequationscan
lieoutsideofthelinearregionsspecifiedbyD . NotethatifforsomeD thematrixJD −Iis
Ω Ω Ω
notinvertible,thenthereisnosinglefixedpoint,butwestillcanfindastructureofinterest(e.g.,a
directionwitheigenvalue0correspondstomarginalstability,i.e.,alineattractor).
A.2 Preliminaries: FixedpointsinPiecewise-linearlow-rankRNNs
First,assumex(0)isinthesubspacespannedbythecolumnsofM. Withthelow-rankassumption,
wecanrewriteEq.8forallt∈[0,∞),byprojectingitonM[18,20,21]:
dz
=−z(t)+NTϕ(Mz(t)−h) (10)
dt
withx(t)=Mz(t).
Now assume x(0) contains some part x⊥(0) not in the subspace spanned by M, i.e., we have
x(0) = Mz(0)+x⊥(0). Thedynamicsofx⊥(t)aresimplygivenby x⊥ = −x⊥(t)whichwill
dt
decaytoitsstablepointat0irrespectiveofz(t),andcanthusnotcontributeadditionalfixedpoints.
Naively,usingthesamestrategyasbeforetoobtainallfixedpointsz,wewouldneedtosolve2N
linearsystemsofRequations(againforallconfigurationsofD ):
Ω
z∗ =(NTD M−I)−1NTD h, (11)
Ω Ω
A.3 Preliminaries: Hyperplanearrangements
Inthesubsequentsection,wewillturntothequestionofhowmanyequationsweneedtosolvetofind
allpossiblefixedpoints. Recallthatitispossibletocalculatethefixedpointsanalyticallybecause
piecewiselinearnonlinearitiespartitionspaceintosubregionsinwhichdynamicsarelinear. Eachof
thelinearregionscorrespondstoaconfigurationofD . Fornetworkswithlow-rankconnectivity,
Ω
wehavetoconsideronlyasmallsubsetofthose,asonlyasmallsubsetofallconfigurationsofD
Ω
correspondtox’swithinthecolumnspaceofM(SeeFig.S1). Tofindoutexactlyhowmanyregions
lie within the column space, we will need to answer the question: in how many regions can we
divideR-dimensionalspace,withN hyperplanes? Toanswerthisquestioningeneral,wewillneeda
theoremfromthefieldofhyperplanearrangements[41,61–63]. Herewegiveabriefintroduction.
14SupplementaryFigure1: ProofsketchincludingD ’s. Thephase-spaceofanRNNwithN (here2)
Ω
unitswithactivationmax(0,x −h )ispartitionedinto2N (here4)regionsinwhichthedynamicsare
i i
linear,eachcorrespondingtoaconfigurationofD . IfdynamicsareconfinedtotheR-dimensional
Ω
subspacespannedbythecolumnsofM,onlyasubset(here3)canbereached. Eachunitintersects
the space spanned by the columns of M with a hyperplane (the pink points in the Figure). The
amount of linear regions in M, thus becomes equivalent to "how many regions can we create in
R-dimensionalspacewithN hyperplanes?"
Introduction to hyperplane arrangements: A finite arrangements of hyperplanes is a set of
N affine subspaces A = {a ,...,a } in some vector space V = RR. Recall a hyperplane
1 N
is a R − 1 dimensional subspace defined by a linear equation a := {v ∈ V|mTv = h} for
i
some m ∈ V,h ∈ R. Note that any linear system of equations Mv = h with M ∈ RN×R
equivalentsdefinesanarrangementofN hyperplanesinRdimensionalspace. InFig.S2a,c,weshow
arrangementsof3hyperplanesinR2. Inthiscase,ahyperplaneisaline,butthereareinfinitelymany
possibilitiesonhowwecanarrangetheselinesintwo-dimensionalspace. Weareinterestedin
N(A):= numberofregionsApartitionsRR,
where regions correspond to the connected components of RR \A. In this simple case, we can
visuallyverifythatthearrangementsinFig.S2apartitionsthespaceinto7regions,whereasthe
arrangement in Fig. S2c partitions the space into only 6 regions. Clearly, the number of regions
A partitions space in is strongly related to the number of unique intersections of lines. We have
fewerregionsinFig.S2c,simplybecausealllinesintersectatthesamepoint. Ifwecanwigglethe
hyperplanesalittle,andnotchangethenumberofregions(aswecandoinFig.S2a,butnotFig.S2c),
wecallthehyperplanesingeneralposition(seeTheorem1foraformaldefinition).
Tocounttheamountofregionsforanyarrangementofhyperplanes,wecanleverageanalgebraic
constructioncalledtheintersectionsposet L(A). Thisisthesetofallnonemptyintersectionsof
hyperplanesinAandincludesV. Elementsofthissetaregenerallyreferredtoasflats. Theflats
are ordered by reverse inclusion x ≤ y ⇐⇒ x ⊇ y in the intersection poset. We visualized
exampleintersectionposetsofthepreviousexamples(Fig.S2b,d). Hereweorganizedtheflatsby
dimensionality(suchavisualizationiscalledaHesseDiagram). Importantlyforanyrealarrangement
A,N(A)solelydependsonL(A)(Corollary2.1,[63]).
TocalculateN(A)fromL(A),weneedonelastconstruction,namelytheMöbiusfunction,recur-
sivelydefinedby
(cid:26)
1 ifs=X
µ(X,s)= −(cid:80) µ(X,s′), ifs⊂X. (12)
X⊇s′⊃s
ThenumericalvaluesfortheexampleareshowninFig.S2.
Theorem1(Zaslavsky’sTheorem;[41,63]). GivenavectorspaceV =RR andaarrangementof
N hyperplanesA={a ,...,a }onV,thenthenumberofregionsApartitionsV (denotedN(A),
1 N
canbeexpressedasfollows
(cid:88)
N(A)= µ(RR,s)(−1)dim(s)
s∈L(A)
15a b
a ∩a a ∩a a ∩a
a a 1 1 2 1 2 3 1 31
2 3
a 1 a 1 a 2 a 3
−1 −1 −1
R2
1
c d
2 a 1∩a 2∩a 3
a
2
a
3
a 1 a 1 a 2 a 3
−1 −1 −1
R2
1
SupplementaryFigure2: a)Anarrangementof3hyperplanesa ,a anda ingeneralposition. b)
1 2 3
the associated intersection poset of the arrangement in a. c) An alternative arrangement with its
associatedintersectionposet. d). BluenumbersindicatethevalueoftheMöbiusfunction.
furthermore,itholdsthat
R (cid:18) (cid:19)
(cid:88) N
N(A)≤ (13)
r
r=0
withequalityifandonlyifAisingeneralpositioni.e. Amustsatisfy
(i) {a ,...,a }⊂Aandp≤N
⇒dim((cid:84)p
a )=N −p
1 p i=1 i
(ii) {a ,...,a }⊂Aandp>N
⇒(cid:84)p
a =∅
1 p i=1 i
OnecanverifythisfactforthegivenexampleshowninFig.S2. WerefertoStanley[63]foran
in-depthformalintroductiontothistopic. Fundamentally,itisbasedonthefollowingrecursionthat
thenumberofregionsforanyarrangementsatisfies
N(A∪{a })=N(A)+N(AaN+1)
N+1
whereAaN+1 := {a
N+1
∩a
i
: a
i
∈ A,a
N+1
∩a
i
̸= ∅,a
N+1
⊈ a i}(Lemma2.1,Stanley[63]).
Notethata isitselfanR−1dimensionalvectorspace,andeachintersectiona ∩a isan
N+1 N+1 i
R−2 dimensional hyperplane within a (e.g., the intersection of two planes is a line within
N+1
theplanes). HenceAaN+1 isitselfanarrangementofN hyperplanes,butinanR−1dimensional
subspace. Infact,theintersectionposetexhaustivelyenumeratestheelementsofallpossibleAai,
andtheMöbiusfunctioncanbeshowntosatisfytheaboverecursion.
Ifwechooseϕ(x ) = max(x −h ,0)i.eD = 1,eachneuronwouldpartitionspacebyasingle
i i i
hyperplane x = h or equivalently the R dimensional subspace by the hyperplane MTz = h .
i i i i
Hence,thehyperplanearrangementisdeterminedbythematrixMandoffseth. Asthesequantities
arelearnedduringtrainingoftheRNN,thisarrangementisofteninageneralpositionbecauseitis
simplynumericallyunlikelythattwohyperplanesareexactlyparallelorintersectinexactlythesame
"point".Thisdoes,however,changeinthegeneralcaseD >1,forwhichwederiveatighterboundin
thesectionbelow.
Arrangementsofparallelfamilies Forthegeneralcaseϕ(x )=(cid:80)D bdmax(x −hd,0)each
i d=1 i i i
neurons will partition space with D hyperplanes bdx = bdh(d) ⇐⇒ x = h(d) as before;
i i i i i i
16equivalently each neuron partitions the R dimensional subspace with hyperplanes mTz = hd.
i i
Notably,alltheDhyperplanesherewillsharethesamerowofM,andthustheyareparallel. Clearly,
anysucharrangementcannotbeingeneralarrangementbydefinition.
Theresultingarrangementwillhaveaveryspecificstructure. Let’sdefine
A :={a ,...,a }
i i1 iD
asafamilyofDparallelhyperplanes. Anypairofhyperplanesa ,a ∈A isparallel. Alow-rank
il im i
RNNwithN neuronsandageneralactivationfunctionwillthusleadtoanarrangementconsistingof
N familiesofDparallelhyperplanes.
Wecanusethisspecificstructuretoobtainatighterbound.
Lemma1. LetA=A ∪···∪A beanarrangementofN −1familiesofDparallellinesthen
1 N−1
itsatisfiesthefollowingrecursion
D
(cid:88)
N(A∪A N)=N(A)+ N (AaNd)
d=1
Furthermore,denotebyN(N,R,D)themaximumnumberofregionsattainablebyanyarrangement
ofN familiesofDparallelhyperplanesinRdimensionalspacethen
N(N,R,D)≤N(N −1,R,D)+D·N(N −1,R−1,D)
Proof. ToaddaA toA,wehavetoaddDnewparallelhyperplanes. Wecandosobyiteratively
n
applyingLemma2.1[63]. Weobtain
N(A∪{a ,...,a })=N(A∪{a ,...,a
})+N((cid:0)
A∪{a ,...,a
}(cid:1)aND)
N1 ND N1 N(D−1) N1 N(D−1)
D (cid:32)(cid:34) d−1 (cid:35)aNd(cid:33)
(cid:88) (cid:91)
=N(A)+ N A∪ {a }
Ni
d=1 i=1
NownotethatAaNj :={a
Nj
∩a
lm
:a
lm
∈A,a
Nj
∩a
lm
̸=∅,a
Nj
⊈a lm},hencebydefinition
onlyhyperplanesthatdointersectwitha areincludedinthisset. Asa isparalleltoanyother
Nj Nj
a foralli̸=j,alla ∩a cannotbeintheset. Henceforanyd,wehavethat
Ni Nj Ni
(cid:32)(cid:34) d−1 (cid:35)aNd(cid:33)
(cid:91)
N A∪ {a Ni} =N(AaNd)
i=1
whichprovesthefirstequation.
RecallthatwedefineN(N,R,D)asthemaximumnumberofregionsattainablebyanyarrangement.
NoticethatAbyconstructionisanarrangementofN −1familiesofDparallelhyperplanesinR
dimension. ThusbydefinitionN(A)≤N(N −1,R,D).
Furthermore,theintersectionsetoftwohyperplanesindimensionRisitselfhyperplanesofdimension
R−1. Furthermore,theintersectionsetsofDparallelhyperplaneswitha ,remainparalleland
Nd
hence AaNd is an arrangement of at most N − 1 families of D parallel hyperplanes in R − 1
dimensions. ThusN(AaNd)≤N(N −1,R−1,D)leavinguswith
N(A∪{a ,...,a })≤N(N −1,R,D)+D·N(N −1,R−1,D)
N1 ND
Asthisholdsforanyarrangement,italsoholdsforthearrangementthathasN(N,R,D)regions
(i.e.,whichmaximizesthenumberofregions)and,therefore,provesthesecondequation.
Lemma2. LetAbeanarrangementofN familiesofDparallelhyperplanes. Then,itholdsthat
R (cid:18) (cid:19)
(cid:88) N
N(A)≤ Dr
r
r=0
withequalityifeachfamilyisinageneralposition,i.e. thateverysubarrangement{a ,...,a }
1j1 NjN
forall1≤j ≤Disingeneralposition.
i
17... (cid:84)R A |{(cid:84) a |d ,...,d ≤D} ...
k=1 k k kdk 1 k
. . .
. . .
. . .
A ∩A |{a ∩a |i,j ≤D} ... A ∩A |{a ∩a |i,j ≤D}
1 2 1i 2j 1 N 1i Nj
A |{a |i<=D} ... A |{a |i<=D}
1 1i N Ni
RR
SupplementaryFigure3:ConstructionoftheintersectionposetL(A)foraarrangementofN families
A ofDparallelhyperplanesin"generalposition".
i
Proof. We will first construct an intersection poset L(A) on the level of families A in general
i
position. Afterall,theintersectionpropertiesbetweenthesefamiliesisthesameasbetweentheir
elements,e.g. ifa intersectsa thenalsoalllinesinA intersectalllinesinA .
i1 j1 i j
TheresultingintersectionposetL(A)canbeclusteredintothecorrespondingfamilies. Wevisualize
theconstructioninFig.S3.
Ateachrankr(levelfrombottomtotop),wecanchooseexactly(cid:0)N(cid:1)
familiesofhyperplanesthat
r
intersect(exactlythecaseifwejusthaveN hyperplanesingeneralposition). Toobtainaflatof
dimensionN −rwehavetochooseroutoftheN hyperplanefamilieswithoutreplacement.
If,e.g.,twofamiliesofparallelhyperplanesA ,A intersect,thenanyelementa willintersectwith
i j ik
anyelementa forall1≤k,l≤DleadingtoatmostD2flatswithineachfamily(therecanbeless
jl
asotherfamiliesmightintersectinthesame"point"). Ingeneral,eachclusterofintersectionsofr
familieswillcontainatmostDr flats.
ByconstructionofL(A)andTheorem1,thelemmafollowsdirectly.
Toshowthatthisconstructionindeedisanupperboundforallarrangements,wecanuseLemma
1. There, we established a recursion, which any such upper bound must satisfy. Hence, assume
N(N,R,D)=(cid:80)R Dr(cid:0)N(cid:1)
. NoticethatusingPascal’sidentity,wecanrewrite
r=0 r
R (cid:18) (cid:19)
(cid:88) N
N(N,R,D)= Dr
r
r=0
R (cid:18)(cid:18) (cid:19) (cid:18) (cid:19)(cid:19)
(cid:88) N −1 N −1
= Dr +
r r−1
r=0
R (cid:18) (cid:19) R (cid:18) (cid:19)
(cid:88) N −1 (cid:88) N −1
= Dr + Dr
r r−1
r=0 r=0
R (cid:18) (cid:19) (cid:18) (cid:19) R (cid:18) (cid:19)
(cid:88) N −1 N −1 (cid:88) N −1
= Dr +D0 + Dr
r −1 r−1
r=0 r=1
(cid:124) (cid:123)(cid:122) (cid:125)
:=0
R−1 (cid:18) (cid:19)
(cid:88) N −1
=N(N −1,R,D)+ Dr+1
r
r=0
=N(N −1,R,D)+D·N(N −1,R−1,D)
18A.4 Proofofproposition
Usingthepreviouslyderivedtechniques,wewillproveherethemainproposition. Furthermore,in
Algorithm1,pseudo-codeisgiventocomputeallfixedpointsinpractice.
Proposition 1. Assume the RNN of Eq. 8 , with J of rank R and piecewise-linear activations:
ϕ(x ) = (cid:80)Db(d)max(x −h(d),0). ForfixedrankRandfixednumberofbasisfunctionsD,we
i d i i i
canfindallfixedpointsintheabsenceofnoise,thatisallxforwhich dx = 0,bysolvingatmost
dt
O(NR)linearsystemsofRequations.
Proof. By definition, each neuron partitions RN in D linear regions with hyperplanes described
by x(d) = h(d), for the i’th neuron. Using that in the columnspace of M, we have x = Mz., it
i i
followsthateachneuronpartitionstheRdimensionalsubspacespannedbycolumnsofM,withD
hyperplanesdescribedby(cid:80)RM z =h(d). Noticethatthesehyperplanesareparallel,astheyall
r i,r r i
sharethesamecoefficientsM buthaveadifferentoffseth(d). UsingLemma2weknowthatthere
r i
canonlybe(cid:80)R Dr(cid:0)N(cid:1)
suchregions.
r=0 r
Howdowefindthoseregions?Let’sfirstconsiderthecaseofD =1,andassumethatthehyperplanes
areingeneralposition.WecanfindthecorrespondingconfigurationsofD asfollows.Wefirstobtain
Ω
thesetofallintersectionsofRhyperplanes.
Forthiswetrytosolve(cid:0)N(cid:1)
systemsofRequations.
R
LetM ∈RR×R bethematrixobtainedbychoosingRdifferentrows1,...,RofM∈RN×R (i.e.
R
pickingRneurons),thenwemayfindthecorrespondingintersectionofRhyperplanesbysolving
thefollowinglinearsystemofRequations
z =M−1h and x =Mz .
∩ R R ∩ ∩
whichwillalwayshaveauniquesolutionifallhyperplanesareingeneralposition,asthenallM
R
have rank R. Each x has 2R possible bordering linear regions. We can find the corresponding
∩
D =diag([d ,...,d )’smatricesofeachofthosesubsectionsasfollows. Firstd =I(x <0)
Ω 1 N i ∩
foralli <= N. Byconstruction1,...,Ratx willbeexactlyatthethreshold,bymovingaway
∩
fromitd canbecomeeitherzeroorone, dependingoninwhichregionweandup. Hence, the
R
2R regionscorrespondtooneinwhicheithercombinationofneurons1,...Risactive(meaning
thatitisabovethethreshold). Wethusjusthavetocheckallcombinationsd ,...,d ∈ {0,1}R.
1 R
Usingthis,wewillfindatmost(cid:80)R (cid:0)N(cid:1)
uniqueconfigurations(asthisisthemaximalnumber
r=0 r
ofregionspossibleforD =1). TofindallthefixedpointswehencehavetosolveEq.11foreach
configuration.
Wethusendupwithsolving(cid:0)N(cid:1)
systemsofRlinearequationstofindallregions,and
R
another(cid:80)R (cid:0)N(cid:1) ∈O(NR)systemsofRlinearequationstofindallfixedpoints.
r=0 r
LetusnowconsiderthecaseforD >1. NotethatanRNNwithN unitsandDbasisfunctionsper
unit,canbeexpandedtoanRNNwithND unitswithactivationϕ(x ) = max(x −h ,0)([24],
i i i
Theorem 1). Any fixed point can then still be analytically computed using Eq. 11. We expand
the network but keep track of all
(cid:80)RDr(cid:0)N(cid:1)
possible intersections. It still holds that from each
r R
intersection, we can reach 2R regions. In total, we will now find at most (cid:80)R Dr(cid:0)N(cid:1) regions
r=0 r
(Lemma2). Tofindallthefixedpoints,wehencehavetosolve(cid:0)N(cid:1) Dr+(cid:80)R (cid:0)N(cid:1) Dr systemsof
R r=0 r
Rlinearequations,whichforconstantDandRhasacostofO(NR)
Finally,let’sconsiderthecasewhenhyperplanesarenotingeneralposition(whichisunlikelyto
happenwhendoingnumericaloptimization). IfthereareintersectionsofmorethanRhyperplanes,
weproceedasbefore, butincasetheintersectionofRhyperplaneswearecurrentlyconsidering
intersectsadditionalhyperplanes,setthediagonalelementsofD correspondingtotheseadditional
Ω
hyperplanes arbitrarily to 1 (as intersections including the additional hyperplanes are considered
separately). Ontheotherhand,incasesomehyperplanesareonlypartofintersectionsoflessthan
Rhyperplanes(becausetheybecameparallel),weproceedasfollows. Insteadofconsideringonly
intersectionsofRhyperplanes,wenowalsoconsiderallpossibleintersectionsofrhyperplanes,with
1≤r ≤R. Forthis,wesolvenomorethan(cid:80)R(cid:0)N(cid:1) systemsofrequations. LetM ∈Rr×R be
r r r
thematrixobtainedbychoosingrdifferentlinearlyindependentrows1,...,rofM∈RN×R;then
wemayfindapointonthecorrespondingintersectionofrhyperplanes(notethattheintersection
19itselfcannowalsobeahyperplane)bytosolvingthefollowinglinearsystemofrequations
z =M†h and x =Mz .
∩ r r ∩ ∩
with†beingthepseudoinverse.
Weherenowendupwithsolvingnomorethan(cid:80)R (cid:0)N(cid:1)
systemsof
r r
rlinearequationstofindallregions,whichhasanequalcostinN asthepreviouscases.
Wehereprovidepseudocode. Forsimplicity,werestrictourselvedtothecaseofD =1andassume
thatthearrangementspecifiedbyMandhisingeneralposition. Thiscanbegeneralizedtothe
generalsettingaspresentedintheproof.
Algorithm1:Improvedexhaustivesearchforallfixedpoints
Data: N∈RN×R,M∈RN×R,h∈RN
Result: z_setsetofallfixpoints,D_setthesetofallrelevantD configurations.
Ω
D_set:={};
z_set:={};
idx=[1,...,N];
// Findfeasibleconfigurations
idx_comb=all(cid:0)N(cid:1)
combinationsofindicesidx;
R
for(i ,...,i )inidx_combdo
1 R
M =M[(i ,...,i ),:];
R 1 R
h =h[(i ,...,i )];
R 1 R
// M isinvertibleasthearrangementisingeneralposition
R
z =solve(M ,h );
∩ R R
x =Mz ;
∩ ∩
d_init=x >h;
∩
for(v ,...,v )in{0,1}R do
1 R
d=d_init[(i ,...,i )].set(v ,...,v );
1 R 1 R
D =diag(d);
Ω
D_set=D_set∪{D };
Ω
end
end
//
Findfixedpoints,fortheatmost(cid:80)R (cid:0)N(cid:1)
configurations
r=0 r
forD inD_setdo
Ω
z∗ =solve(NTD M−I,NTD h);
Ω Ω
z_set=z_set∪{z∗}
end
20B Additionalfigures&tables
B.1 AdditionalstatisticsforTeacher-Studentsetups
SupplementaryFigure4: a-c)Pairwisecorrelationsbetweenunitsofthemodesforpanela-c)of
Fig.3,respectively. Notethatciscomputedoverallconditions.
B.2 StatisticsforHPC-11
SupplementaryFigure5: a)Wefitarank-4RNNtospikesrecordedfromrathippocampus[50–52],
andgeneratenewsamplesfromtheRNN(right). b)Singleneuronstatistics. Themeanratesand
coefficientofvariationsofinterspikeinterval(ISI)distributionsofalongtrajectoryofdatagenerated
by the RNN (gen) match those of a held-out set of data (test). As a reference we additionally
computedthesamestatisticsbetweenthetrainandtestset. c)Populationlevelstatistics. Thepairwise
correlations between neurons for generated data and the test data. d) The corresponding latents
generated by the RNN consists of 10Hz (fast theta) oscillations on top of slower oscillations. e)
Latentswithfurtherzoomingin(ontime),showntogetherwiththeLFPsignal.f)Thepowerspectrum
latentssampledfromtheRNNnexttothatoftheLFP.
B.3 NeuralLatentsBenchmarkevaluation
WeappliedourmethodtotheMC_MazedatasetoftheNeuralLatentsBenchmark(NLB)[55]at20
millisecondbinsize(Table2). Thebenchmarkevaluatesmethodsonanumberofmetrics: ‘co-bps’
(co-smoothing bits-per-spike) assesses the quality of firing rate predictions for a set of held-out
neuronsthatareunobservedinthetestdata,evaluatedwiththePoissonlikelihoodofthetruespiking
activitygiventheratepredictions. ‘velR2’evaluateshowwellthemodel’sinferredfiringratescan
predictthesubject’shandvelocity. ‘PSTHR2’evaluateshowwellperi-stimulustimehistograms
(PSTHs)computedfrommodel-inferredratesmatchempiricalPSTHsfromthedata. Wefoundthat
ourmethodoutperformsclassicalmethods(GPFA[64]andSLDS[30])whilecertainstate-of-the-art
21deeplearning(LFADS[7],NeuralDataTransformer[65])areslightlybetterthanourmethodonthis
particularbenchmark. WedonotethatNLBmetricscenteraroundevaluatingthequalityofsmooth
ratesinferredfromspikes,whichisnotthecentralfocusofourmethod. Rather,weaimtofitan
RNN,fromwhich—bydesign—wecansamplenoisylatenttrajectoriesthatreproducevariability
inthedata.
While our method also has comparatively lower dimensionality than the other deep learning ap-
proaches,alatentdimensionalityof36isstillconsiderablyhigherthanallnetworksconsideredinthe
Maintext. Wereasonthatweneedahighnumberoflatents,becausethefullMC_Mazedatasethasa
largenumberofconditions(108),spanningmultiplemaze-configurations,whichmaybedifficultto
fullymodelwithautonomouslow-dimensionallatentdynamics.
Table2: PerformanceofourmethodontheMC_MazedatasetoftheNeuralLatentsBenchmark,‘dim’
referstothedimensionalityofthemodel’sunderlyingdynamics(wherepossible).
method dim co-bps↑ velR2↑ PSTHR2↑
Spikesmoothing 137 0.2076 0.6111 −0.0005
GPFA 52 0.2463 0.6613 0.5574
SLDS 38 0.2117 0.7944 0.4709
LFADS 100 0.3554 0.8906 0.6002
NDT 274 0.3597 0.8897 0.6172
Ours 36 0.3210 0.8571 0.5902
B.4 Stimulus-conditioninginmonkeyreachingtask
Fortheexperimentwithstimulus-conditioneddynamicsinthemonkeyreachingtask,wetestedthe
performanceofmodelswithandwithouttheconditioninginputs. Wefoundthattheconditioning
inputsallowthenetworkstoperformbetteronvelocitydecodingatlowerdimensionalities.
Table3: Performancebenefitsofconditioningformonkeyreachingtask.
conditioning dim velR2↑
5 0.7897±0.0687
w/o 6 0.8944±0.0039
conditioning 8 0.9085±0.0048
16 0.9196±0.0041
with 5 0.8589±0.0493
conditioning 6 0.9018±0.0114
FollowingtheanalysisinFig. 7,wealsofurthervisualizedthemodel’smatchtothespikingstatistics,
includingmeanandstandarddeviation(SD)ofspikingrate,andmean,SD,andcoefficientofvariation
(CV)ofinter-spikeintervals. WeobservedagoodmatchtothemeanandSDofthespikingrate
acrossallconditions. MatchtoISIstatisticsisalsoquitereasonablegiventhenoiseobservedbetween
estimatesofthestatisticsfromtrainandtest.
22Supplementary Figure 6: Spiking statistics of model-generated (teal) and train data (brick red)
comparedagainsttestdata.
23B.5 Comparisontoapproximatemethodforfindingfixedpoints
SupplementaryFigure7: RepetitionoftheexperimentofFig.8,butnowwitharank-2RNNwith
128units. Again,weshowthenumberoffixedpointsfoundasafunctionofthenumberofmatrix
inversescomputed,witherrorbarsdenotingtheminimumandmaximumamountoffixedpointsfound
over20independentrunsofthealgorithm.
Recentlyanapproximatemethodforfindingfixedpointsinpiece-wiselinearRNNswasproposed
[25]. The method proceeds by randomly selecting a linear region (a configuration of D , see
Ω
SupplementA.1.2)andcalculatingthecorrespondingfixed-point. Ifitisindeeda’true’fixedpoint
oftheRNN(itisconsistentwiththeassumedD ),westoreit. Ifthefixedpointwasinconsistent
Ω
withtheassumedD ,weiterativelyinitializeD accordingtothe’virtual’fixedpointfoundand
Ω Ω
calculatethenewfixedpointcorrespondingtothisD ,untilweeitherreacha’true’fixedpointor
Ω
reachacertainamountofiterations. Then,wereinitializeatarandomlyselectednewconfiguration
ofD andrepeattheprocedure.
Ω
Undersomeconditions,theapproximatemethodcanbeshowntoconvergeinlineartime(∥MN˜T∥+
∥aI∥≤1)[8],whereitwillbefasterthanourexactmethod—howeveringeneraltheconvergence
oftheapproximatemethodstronglydependsonthedynamicsofthenetworks. Inparticular,thereare
reasonablesettingswheretheapproximatemethodfailstofindallfixedpoints,suchasofarank-2,
128unitRNNwith17fixedpoints(trainedsimilarlytotheteacherRNNofFig.3c;Fig.S7).Whilean
in-depthstudyoftheapproximatemethodisoutofscope,wehypothesizethatthefailuretoconverge
isbecausewheninitializingwithrandomlyselectedD soutof(D+1)N possibleconfigurations,
Ω
theapproximatemethodtendstoconvergestothesamesetofD .
Ω
Ourmethodiscompletelyindependentofthedynamicsofthesystemandhasafixedcost,afterwhich
oneisguaranteedthatallfixedpointsarefound. However,wedonotethattherecanbescenarios
whereourexactmethodisstilltoocostly. Inthisscenario,weproposetousetheapproximatemethod,
withoneadjustment-wefirstpre-computethesubsetof(cid:80)RDr(cid:0)N(cid:1)
configurationsthatcancontain
r r
fixedpoints,andtheninitializetheapproximatemethodusingrandomlyselectedD fromthissubset.
Ω
Empirically,thisleadstobetterconvergenceinatleastsomescenarios(Fig.8,Fig.S7)
Fortheapproximatemethod,weusedcodefromhttps://github.com/DurstewitzLab/CNS-2023,which
wasreleasedwiththeGNUGeneralPublicLicense.
C Additionaldetailsoflow-rankRNNs
C.1 Discretisation
Given
dz
τ =−z(t)+NTϕ(Mz(t))+Γ ξ(t),
dt z
UsingtheEuler–Maruyamamethodwithtimestep∆ :
t
√
∆ ∆ ∆
z =(1− t)z + tNTϕ(Mz )+ tΓ ϵ ,
t+1 τ t τ t τ z t
andwithϵ
t
∼N(0,I),definea=1−∆ τt,N˜ = ∆ τtN,andΣ
z
= ∆ τt2 Γ zΓT z,weobtainthetransition
distributionusedinourexperiments.
24C.2 Conditionalgeneration
GiveninputweightsH∈RN×Ns andstimuluss∈RNs,wedefineourmodelas
dx
τ =−x(t)+Jϕ(x(t))+Hs(t)+ξ .
dt x
Usingthesameassumptionsasbefore,xcanbedescribedbyR+N variables
s
dz
τ =−z(t)+NTϕ(Mz(t)+H˜s(t))+ξ ,
dt z
d˜s
τ =−˜s(t)+s(t),
dt
(cid:20) (cid:21)
z
withx=Mz+H˜s,and =([M,H]T[M,H])−1[M,H]Tx.
˜s
Forconstantinputs,˜swillconvergetos,andwecanignoretheadditionalN variables,assuming
s
x(0)=Mz(0)+s. Similarlyifsvariesonatimescaleslowerthanτ,s≈˜sisagoodapproximation
[21]. Here,forallexperiments,theinputiseitheraconstantcontextsignalorarectangularpulse,so
wealwayssubstitutesfor˜sandconsidertheRdimensionalsystemdescribedbyz(whichnowhas
additionalconditioningons).
Wecanwritetheconditionalsequentialdistributiongeneratedbydiscretizingourmodelas
T T
(cid:89) (cid:89)
p(z ,y |s )=p(z ) p(z |s ,z ) p(y |z ),
1:T 1:T 1:T−1 1 t t−1 t−1 t t
t=2 t=1
p(z |s ,z )=N(F(s ,z ),Σ ), p(z )=N(µ ,Σ ),
t t−1 t−1 t−1 t−1 z 1 z1 z1
wherethetransitiondistributionisF(s ,z )=az +N˜Tϕ(Mz +Hs ).
t t t t t
C.3 Lineartransformationsofthelatentspaceandorthogonalisation
Given
x =ax +MN˜Tϕ(x )+ϵ
t+1 t t x
z =az +N˜Tϕ(Mz )+ϵ
t+1 t t z
with ϵ ∼ N(0,Σ ), ϵ ∼ N(0,MΣ MT). We can do any linear transformation of the latent
z z x z
dynamicsz: zˆ=Az,aslongasAhasrankR,withoutchangingtheneuronactivityx. Toseethis,
defineMˆ =MA−1,Nˆ =AN˜,andϵ ∼N(0,AΣ AT),givingus:
zˆ z
x =ax +MˆNˆTϕ(x )+ϵ
t+1 t t x
zˆ =azˆ +NˆTϕ(Mˆzˆ )+ϵ ,
t+1 t t zˆ
whichwillleavexunchanged,whileourlatentszareexpressedinanewbasis. Wetypicallygot
amoreinterpretablevisualizationofthelatentsbyorthonormalisingthecolumnsofM. Thuswe
appliedforallvisualisationsaftertrainingA=UTM,withMˆ =U,whereUarethefirstRleft
singularvectorsofJ=MNT.
D Detailsofempiricalexperiments
D.1 Trainingdetails
D.1.1 Initialisation
Ourmodelsare(unlessnotedotherwise)initializedasfollows:
25N˜ ∼U ,
ij [−√1 ,√1 ]
N N
M ∼U ,
ij [−√1 ,√1 ]
R R
2
W ∼N(0, ),
ij R
H ∼U ,
ij [−√ 1 ,√ 1 ]
Ninp Ninp
h ∼U ,
i [−√1 ,√1 ]
N N
b←0,
a←.9,
Σ ←.01I,
z
Σ ←I,
z1
µ ←0,
z1
where W and b are the output weights and biases respectively. For Gaussian observations we
initialiseΣ ←.01I.
y
ForexperimentswithPoissonobservations,wejointlyoptimizedacausalCNNencoderaspartofthe
proposaldistribution. TheCNNwasconditionedonobservationsandpredictedthemeanandlog
varianceofanormaldistribution. Itconsistedofcommoninitiallayersconsistingof1Dconvolutions,
withaGeLUactivationfunction,andaseparateoutputconvolutionforthepredictedmeanand(log)
variance. TheCNNwasinitializedtothePytorch[66]defaults,exceptforthebiasofthelogvariance
outputlayer,towhichweaddedalog(.01)term,suchthattheoutputmatchestheinitiallypredicted
varianceoftheRNN.Theexactnumberoflayersandchannelsarereportedinthesectionsforeach
experiment.
For the teacher-student setups, we used as non-linearity ϕ(x ) = max(x −h ,0) for both the
i i i
students and the teachers, and for all experiments with real-world data, we used the ‘clipped’
ϕ(x )=max(x +h ,0)−max(x ,0)[8].
i i i i
D.1.2 Parameterisation
We constrain a to be between 0 and 1 by instead optimising a˜ with the following (sigmoidal)
parameterisationa=exp(−exp(a˜))[67]. Inexperimentswiththeoptimalproposal,weestimate
thefullΣ ,whichweconstraintobesymmetricpositivedefinite,byoptimizingalowertriangular
z
matrixCsuchthatΣ = CCT,whereweadditionallyconstrainthediagonalofCtobepositive
z
usingC =exp(C˜ /2). Foralldiagonalcovariances,weparameterizethediagonalelementsusing
ii ii
Σ =exp(Σ˜ ). ForPoissonobservations,weapplyaSoftplusfunctiontorectifythepredictedrate.
ii ii
D.1.3 Optimisation
DuringtrainingweminimisethevariationalSMCELBO[33–35](Eq.7)withstochasticgradient
descent,usingtheRAdam[68]optimiserinPytorch[66].Wegenerallyuseanexponentiallydecaying
learningrate(detailsundereachexperiment).
D.2 Teacherstudentexperiments
D.2.1 Datasetdescription
Wecreateddatasetsbyfirsttraining‘teacher’RNNstoperformataskandthengeneratingobservations
bysimulatingthetrainedteacherRNNs.
ForFig.3a,bweusedcodefrom[22]totrainrank-2RNNstoproduceoscillations,usingasine-wave
withaperiodicityof50time-stepsasatargetandanadditionalL2regularisationontherates. After
training,weextractedtherecurrentweightsM,Nandbiasesh,orthonormalizedthecolumnsofM,
andcreatedadatasetbysimulatingthemodelfor75timesteps,withΣ =.04I. ForFig.3aweused
z
N =20unitsandgeneratedobservationsaccordingtoG=N(Mz ,Σ ),withΣ =.01I. Fig.3b
t y y
26weusedN =40unitsandgeneratedobservationsaccordingtoG=Pois(Softplus(wMz −b),with
t
w =4andb=3.
ForFig.3c,wefollowedasimilarprocedurebutnowtrainedtheteacherRNNonataskwhereit
hastouseinput. Afteraninitialperiodof25timesteps,astimuluswaspresentedfor25timesteps
consistingof[sin(θ),cos(θ)]T,whereθwasrandomlyselectedeverytrialoutof8fixedangles. The
RNNwastaskedtoproduceoutputthatequalsthetransientstimulusforthenext100time-steps. Here
weusedN = 60units, Σ = .025IandgeneratedobservationsaccordingtoG = N(Mz ,Σ ),
z t y
withΣ =.01I. ThetrainingdataforthestudentRNNwasincludedforeachtrialthecorresponding
y
stimulus.
D.2.2 Trainingdetails
The‘student’RNNshad20,40,60units,respectivelyandrankR=2,matchingthatoftheteacher
RNNs. ForFig.3a,c. TheobservationmodelwasalinearGaussianaccordingtoG=N(Mz ,Σ ),
t y
andweusedtheoptimalproposaldistribution.ForFig.3bweusedG=Pois(Softplus(WMz −B)),
t
withW a diagonal matrix (scaling the output of each unit individually). For Fig. 3b, we used a
causalCNNencoderaspartoftheproposaldistribution. Itconsistedof3layers,withkernelsizes
(21,11,1),andchannels(64,64,2). Weused(causal)circularpadding.
Forallthreeexperiments,weusedk =64particles,batch-sizesof10,anddecreasedthelearning
rateexponentiallyfrom10−3 to10−5. ForFig.3a,b,weusedepochsof400trialsandtrainedfor
1000and1500epochs,respectively. ForFig.3cwetrainedfor3000epochsof96trialseach. We
usedaworkstationwithaNVIDIAGeForceRTX3090GPUfortheseruns. Onemodeltookabout3
to4hourstofinishtraining.
D.2.3 Evaluationsetup
ForFig.3wegeneratedlongtrajectoriesofT =10000time-stepsofdataforboththestudentand
teacher RNNs. To facilitate visual comparisons between student and teacher dynamics, we also
orthonormalizedthecolumnsofthestudentsweightsMaftertraining,andforFig.3a,cpickedsigns
ofthecolumnsofMsuchthatthestudentandteachermatch(notethatafterorthormalizing, the
columns of M are equal to the non-zero singular vectors of the full weight matrix J, which are
onlyuniqueuptoasignflip). Asnotedbefore,thisleavestheoutputofthemodelunchanged. The
autocorrelationinFig.3awascomputedbyconvolvingasequenceoflag=120stepsofdatawith
itself(withduration2×lag),andnormalisingsuchthatlag=0correspondstoacorrelationof1. We
repeatedthisfor80sequencesstartingatdifferenttime-pointsofthewholetrajectory.
D.3 EEGdata
D.3.1 Datasetdescription
We used openly accessible electroencephalogram (EEG) data from [42, 43] (
https://www.physionet.org/content/eegmmidb/1.0.0/,ODC-BYlicence). Thedatawasrecordedfrom
ahumansubjectsittingstillwitheyesopen(sessionS001R01),andwassampledat160Hz. Like
[8], we used the full 1 minute of recording, but unlike [8], we did not smooth the data (but just
standardizedthedata). Thus,tocompareourperformanceto[8],whorantheirevaluationusingthe
smootheddata,wesmoothedourgeneratedsamplesequivalently,usingaHannfilterwithawindow
lengthof15-timebins,sothatwecanalsocompareoursamplestothesmootheddata.
D.3.2 Trainingdetails
WeusedN =512units,andrankR=3. TheobservationmodelwasalinearGaussianconditioned
on the hidden state and we used the optimal proposal distribution. We trained for 1000 epochs
consisting of 50 batches of size of 10, and k = 10 particles. The learning rate was decreased
exponentiallyfrom10−3 to10−6. ModelsweretrainedusingNVIDIARTX2080TIGPUsona
computecluster. Asinglemodeltookbetween4and5hourstofinishtraining.
27D.3.3 Evaluationsetup
WeusedourRNNtogenerateonelongtrajectoryofT =9760stepsofdata,y (afterdiscardingthe
t
first2440steps),whichwecomparetotheEEGdata,yˆ ,usingtwoevaluationmeasuresfrom[8,24]
t
(usingcodefromhttps://github.com/DurstewitzLab/GTF-shPLRNN,GNUGeneralPublicLicense):
D : This is an estimate of the KL divergence between the ground truth and generated states.
stsp
To compute this, we obtained kernel density estimates of the probability density functions (over
states,nottime),usingaGaussiankernelwithstandarddeviationσ =1. WegetfortheEEGdata:
pˆ(y)= 1 (cid:80)T N(yˆ ,I),andforthegenerateddataqˆ(y)= 1 (cid:80)T N(y ,I). Wethenusedthe
T t=1 t T t=1 t
following Monte Carlo estimate of the KL divergence: D ≈ 1 (cid:80) logpˆ(yˆi), using n = 1000
stsp n qˆ(yˆi)
samplesyˆidrawnrandomlyfromtheEEGdata.
D : Thisisanestimateofthedifferenceinpowerspectrabetweenthegroundtruthandgenerated
H
states. Wefirstcomputedforeachdatadimensionthespectrayˆi,yi fortheEEGandgenerateddata,
ω ω
respectively. WeusedaFastFourierTransform,smoothedtheestimateswithaGaussiankernelwith
standarddeviationσ =20,andnormalizedthespectrasotheysumto1. Wecomputedthemeanof
theHellingerdistancesbetweenthespectra: D
H
= 61 4(cid:80)6 i4 √1 2∥(cid:112) yˆ ωi −(cid:112) y ωi∥.
D.4 HippocampusHC-2
D.4.1 Datasetdescription
We used openly accessible neurophysiological data recorded from layer CA1 of the right dorsal
hippocampus[47,48](https://crcns.org/data-sets/hc/hc-2/about-hc-2. Signalswererecordedasthe
ratsengagedinanopenfieldtask,chasingdropsofwaterorpiecesoffoodthatwererandomlyplaced.
Weusedthesessionec013.527fromratIDec13,whichisapproximately1062secondslong. From
37units(neurons)weused21neuronsthathavemaximalspikecounts,discardingtherestofthe
comparativelysilentneurons. Webinnedthespikedatato10ms. Weusedthefirst80percentofthe
datafortraining,andtherestwassavedfortestingpurposes.
D.4.2 Trainingdetails
WeusedN =512units,andrankR=3fortherunthatwasusedinourFig.5. Weusedacausal
CNNencoderaspartoftheproposaldistribution,whichconsistedof3layerswithkernelsizes(150,
11,1),with(64,64,3)channels. Duringourstudy,wesweptovermultipleranksandfoundthat
thetaoscillationsconsistentlyemergedfromrank3onwards,afterwhichreconstructionaccuracy
wasrelativelystable. Foreachrank,weusedthreedifferentseedsandtwodifferentfirstlayersizes
fortheencoder,25or150. Thedurationofarandomlysampledtrial(sequencelength)fromthe
wholedatawas94timestepswhenthefirstlayersizewas25,and219whenthefirstlayersizewas
150. We,however,alsofoundthatthechoiceofthedurationdidnotaffecttheresultsmuch. We
trainedthemodelusing3000epochs,eachepochconsistingof3000trialswith64batchesandk=64
particles. Thelearningratewasdecreasedexponentiallyfrom10−3 to10−6. Asinglemodeltook
approximately21hourstofinishtrainingonaNVIDIARTX2080TIGPUonacomputecluster.
D.4.3 Evaluationsetup
WeusedourRNNtogeneratedatathatmatchesthedurationofthetestdata,whichis20810timesteps
(∼208s)(afterdiscardingthefirst1000steps). Wecomparedifferentspikestatisticsofgenerated
datawithtestdata,andforcomparisonpurposes,wealsocomparedthesamestatisticsmeasurements
betweentrainandtestdataaswell. Wecalculatedthemeanfiringrateofeachneuron,meanofISI
distributions,andpairwisecorrelations. Weusedaband-passfilter1-40Hzforthelatentsandthe
LFPsignalbeforecalculatingthepowerspectrogram(Fig.5e).
D.5 HippocampusHC-11
D.5.1 Datasetdescription
WeusedopenlyaccessibleneurophysiologicaldatarecordedfromhippocampalCA1region[50–52]
(https://crcns.org/data-sets/hc/hc-11/about-hc-11). Weusedthesubsetofthedatasetcalledthemaze
28epoch,wherearatwasrunningona1.6-meterlineartrack,withrewardslocatedateachend(left
andright). Throughoutthistask,neuralactivitywasrecordedfrom120identifiedpyramidalneurons.
Asin[13],weonlyused60neuronsthathadsufficientactivityanddiscardedrestoftheunits. We
usedcodefrom[53](https://github.com/zhd96/pi-vae)topreprocessthespikedata,andonlyusedata
correspondingtotheratrunningandthelocationdatabeingavailable. Weused25msbins.
D.5.2 Trainingdetails
WeusedN =512units,andrankR=4. WeusedthecausalCNNencoderwithzeropadding,3layers
(24,11,1),and(64,64,4)channels. Themodelistrainedfor3000epochs,eachepochhaving3000
trialswithasequencelengthof94timebins(2.35s),usingbatchsize64andk=64particles. The
learningratewasdecreasedexponentiallyfrom10−3to10−6. Asinglemodeltookapproximately21
hourstofinishtrainingonaNVIDIARTX2080TIGPUonacomputecluster.
D.5.3 Evaluationsetup
WeusedourRNNtogeneratedatathatmatchesthedurationofthetestdata,whichis4289timesteps
(∼107s)(afterdiscardingthefirst1000steps). Wecalculatedthemeanfiringrateofeachneuron,
coefficientofvariationsofISIdistributions,andpairwisecorrelations. Wefitasimplelinearregressor
toposteriorlatentstopredictthelocationdata,andusedthisregressionmodeltopredictthetestdata
forthelocation.
D.6 MonkeyReach
D.6.1 Datasetdescription
WeusedthepubliclyavailableMC_MazedatasetfromtheNeuralLatentsBenchmark(NLB)[55]
(https://dandiarchive.org/dandiset/000128, CC-BY-4.0 licence). The data were recorded from a
macaqueperformingadelayedcenter-outreachingtaskwithbarriers,resultinginavarietyofstraight
andcurvedreaches. Forsimplicity,wetookonlythetrialswithnobarriersandthusstraightreach
trajectories,resultingin592trainingtrialsand197testtrials.Webinnedthedataat20msandaligned
eachtrialfrom250msbeforeto450msaftermovementonset.
Tocreateconditioninginputsforthemodel,wetookthexandycoordinatesofthetargetpositionfor
eachtrialandscaledthemtobebetween−1and1. Wethenprovidethisscaledtargetpositionas
constantcontextinputtotheRNNforthedurationofthetrial.
D.6.2 Trainingdetails
We ran a random search of 30 different models with rank r ∈ 3,4,5,6 and particle number k ∈
16,32,64. All models had 512 units and used a causal CNN encoder with kernel sizes (14,4,2)
andchannels(128,64,r). Weused(causal)reflectpadding. Wetrainedeachmodelforupto2000
epochs, terminating training early if no improvement was seen for 50 epochs. Each model took
around3to4hourstotrainonanNVIDIARTX2080TIGPUonacomputecluster. Seeingthata
rankof5wassufficientforvelocitydecodingR2 ≈0.9,wetookthebest-performingrank-5model
forsubsequentanalyses.
D.6.3 Evaluationsetup
Forqualitativeevaluationofreplicationofcross-conditiondifferences,wegroupedthereachtargets
inthedatainto7conditions,oneateachcornerandthemidpointofeachedgeoftherectangular
reachplane,excludingthemidpointdirectlyatthebottom. Wethengenerateddatafromthemodel
RNNusingconditioninginputsfromthetesttrialsoftherealdata. Then,forthetestdataandthe
model-generateddata, wecomputedmeanfiringrateandinter-spikeintervalforeachneuronfor
eachcondition. Wethencomputedcorrelationdistance(1−r,whereristhePearsoncorrelation
coefficient)ontheneuronstatisticsbetweenconditionsinthetestdataandmodel-generateddata.
ForgenerationofdataforFig. 7d,e,weselectedtargetlocationsbychoosinganglesfrom0to360◦,
evenlyspacedby22.5◦,anddeterminedthecorrespondingreachendpointonasquarespanningfrom
(−1,−1)to(1,1). Wethenconstructedconditioninginputssimilartotherealdatausingthesetarget
29locationsandsimulatedtheRNNwiththem. Todecodethereaches,weusedalineardecodertrained
frominferredfiringratestoreachvelocityfromtherealdata.
D.7 NeuralLatentsBenchmark
D.7.1 Datasetdescription
We again used the publicly available MC_Maze dataset from NLB (see Supplement D.6.1). We
resampledthedatato20msbinsizeandfollowedthestandarddatapreprocessingproceduresforthe
benchmark,asdescribedin[55].
D.7.2 Trainingdetails
Weranarandomsearchof30differentmodelswithvaryingrankfrom12to40andparticlenumber
k ∈16,32,64. Allmodelshad512unitsandusedacausalCNNencoderwithkernelsizes(14,4,2)
and channels (128,64,36), and reflect padding. We trained each model for up to 2000 epochs,
terminatingtrainingearlyifnoimprovementwasseenfor50epochs. Eachmodeltookaround10to
12hourstotrainonanNVIDIARTX2080TIGPUonacomputecluster.
Because the primary task of the benchmark is co-smoothing, i.e., prediction of held-out neuron
firingratesfromheld-inneurons,weprovidetheencoderwithonlytheactivityofheld-inneurons.
However,theobservationlikelihoodcomponentoftheELBOiscomputedonallneurons,held-in
andheld-out.
Aftertraining,weselectedthemodelwiththebestco-smoothingscoreonthevalidationsplitand
submitteditspredictionstothebenchmarkforthefinalevaluation.
D.7.3 Evaluationsetup
Automatedevaluationwasperformedonthebenchmarkplatform,asdescribedin[55].
Weusedforthepredictionattimestept,theexpectedPoissonrateofheld-outneurons,conditioned
ontheactivityofheld-inneuronsatthecurrentandprevioustimesteps,bymakinguseofthefiltering
Posterior(Eq.3). Weaveragedover32setsoftrajectorieswith192particleseach.
30