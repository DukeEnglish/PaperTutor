Data Debiasing with Datamodels (D3M):
Improving Subgroup Robustness via Data Selection
SaachiJain*, KimiaHamidieh*, KristianGeorgiev*,
AndrewIlyas, MarzyehGhassemi, AleksanderMa˛dry
MIT
{saachij,hamidieh,krisgrg,ailyas,mghassem,madry}@mit.edu
Abstract
Machinelearningmodelscanfailonsubgroupsthatareunderrepresentedduringtraining.
While techniques such as dataset balancing can improve performance on underperforming
groups, they require access to training group annotations and can end up removing large
portions of the dataset. In this paper, we introduce Data Debiasing with Datamodels (D3M),
a debiasing approach which isolates and removes specific training examples that drive the
model’s failures on minority groups. Our approach enables us to efficiently train debiased
classifiers while removing only a small number of examples, and does not require training
groupannotationsoradditionalhyperparametertuning.
1 Introduction
The advent of large datasets such as OpenImages [KRA+18] and The Pile [GBB+20] has led to
machinelearningmodelsbeingtrainedonexplicit[BP21]andillegal[Thi23]content,orondata
thatencodenegativesocietalbiases[BG18;Fer23;ALM+22;CP21]andotherspuriouscorrelations
[ODC+20; NAB+23]. Indeed,thereisincreasingevidencethatmodelsreflectthebiasesinthese
datasets,andtheenormousscaleofthesedatasetsmakesmanuallycuratingtheminfeasible.
Inthispaper,weproposeanapproachthataimstoremovedataresponsibleforbiasedmodel
predictions. In particular, we focus on a specific way of quantifying model bias—called worst-
group error—which captures the extent to which model performance degrades on pre-defined
subpopulationsofthedata. Weaimtoidentify(andremove)thepointsinthetrainingdatasetthat
contributemosttothismetrictoimprovethemodel’sgrouprobustness.
Thechallengeinherentinthisapproachisthatitrequiresanunderstandingofhowtraining
dataaffectmachinelearningmodelpredictions. Toovercomethischallenge,wefirstapproximate
predictionsassimple,directfunctionsofthetrainingdataset,usingaframeworkcalleddatamodeling
[IPE+22;PGI+23]. Wecanthenwriteourquantitativenotionofmodelbias(whichisafunctionof
predictions)asafunctionofthedataset. Finally,bystudyingthisfunction,weidentifythetraining
datapointsthatcontributemosttothismeasureofmodelbias. Withtheresultingmethod,which
wecallDataDebiasingwithDatamodels(D3M),weshowthat,acrossavarietyofdatasets,thereare
oftenasmallnumberofexamplesthatdisproportionatelydriveworst-grouperror. Removingthese
examples,inturn,greatlyimprovesmodels’worst-grouperrorwhilemaintainingdatasetsize.
*Equalcontribution.
1
4202
nuJ
42
]GL.sc[
1v64861.6042:viXraTraining data correlation between class (species) and Data balancing [IAP+22] removes examples
extra feature (color) leads to disparate performance. from majority groups to break this correlation.
Species (dog vs cat)
$ $ , , , $ $ + + + , , , -
+ + + - Equal representation from each group
( = deleted)
Goal: “debias” dataset to improve worst-group accuracy (WGA):
+ Only changes data − Can hurt average accuracy
WGA= min Acc(group) + Less accuracy gap − Needs training group labels
group∈{$,+,,, -}
Our approach: Data Debiasing via Datamodeling (D3M)
Compute impact of each training sample on WGA by Find selection that maximizes worst-group accuracy
predicting WGA as a function of dataset selection. by removing most harmful examples.
Worst-group accuracy on “Group alignment score” Group alignment scores
a (small) validation set Learned coefficient; impact 0.3 0.2 0.1 0.0 -1.2 1.2 0.1 0.2 -0.4
of point i on WGA $ $ + + + - , , ,
WGA ≈ ∑D i⋅A i
+ Only changes data + Competitive accuracy
Binary; whether we select + Less accuracy gap + Only needs test set labels
the i-th training sample
Figure 1: Our method (D3M) improves worst group accuracy by identifying and removing
the training samples which most negatively impact worst-group accuracy. Specifically, we use
TRAK [PGI+23]toidentifyexamplesthatexacerbatethediscrepancyingroupperformance. We
thenremoveandre-trainamodelontheremainingdata.
Ourcontributions. Intherestofthispaper,wepresentanddemonstratetheeffectivenessofour
DataDebiasingwithDatamodels(D3M).Concretely,weshowthat D3M enablesusto:
• Pinpointexamplesthatharmworst-groupaccuracy. Weshowthatthereareoftenasmall
numberofexamplesthatdisproportionatelydrivemodels’worst-grouperroronvalidation
data. Forexample,onCelebA-Age,ourmethodimprovesworstgrouperroroveranatural
baseline(databalancing)whileremoving2.4×fewerexamples. Furthermore,theseoffending
examplesoftenformcoherentsubpopulationswithinthedata.
• Achieve competitive debiasing performance. Our approach outperforms standard ap-
proaches(bothmodel-basedanddata-based)toimprovingworst-groupaccuracy[LHC+21;
KIW22;IAP+22],andisabletomatchtheperformanceofmethodswhichuseground-truth
traininggroupannotations[SKH+20].
• Discoverunlabeledbiases. Whenvalidationgrouplabelsareunavailable,weshowhowto
extracthiddenbiases(i.e.,unlabeledsubgroups)directlyfromthedata. Asaresult,wecan
performend-to-enddebiasingwithoutanygroupannotations.
WepresentourmethodinSection4,anddemonstratethesecapabilitiesinSection5. InSection6,
weleverageourframeworktodiscoverandmitigatebiaseswithintheImageNetdataset,where
D3M surfaces coherent color and co-occurrence biases. We then debias the model according to
thesefailures,andimproveaccuracyontheidentifiedpopulations.
2
roloc
ruF
)egieb
sv
kcalb(2 The group robustness problem
Weconsideran(unobserved)datadistributionD overtriplets(x ,y ,g ),eachcomprisinganinput
i i i
x ∈ X, alabel y ∈ Y, andasubgrouplabel g ∈ G, where G isthesetofdistinctsubpopulations
i i i
inthedata. Asarunningexample,considertheCelebAageclassificationtask—here,wetakethe
inputs x tobeimagesoffaces,thelabelsy tobeeither“old”or“young,”andthepossiblegroup
i i
labelstobe“oldman”,“oldwoman”,“youngman”,and“youngwoman”(seeFigure1).
Given a training dataset S and a (small) validation dataset S , the goal of the group
train val
robustnessproblemistoproduceaclassifier f thatminimizestheworst-caselossovergroups,i.e.,
maxE (x,y,g)∼D(cid:2) ℓ(f(x),y)(cid:12) (cid:12)g = g′(cid:3) , (1)
g′∈G
whereℓ(·,·)isalossfunction. Whenℓisthe0-1loss,Equation(1)is(oneminus)theworst-group
accuracy(WGA)oftheclassifier f,whichweusetoquantifysuccessintheremainderofthiswork.
Standard loss minimization can yield models that perform poorly with respect to (1). For
instance, returning to our example of CelebA age classification, suppose there was a spurious
correlationbetweenageandgenderinthetrainingsetS ,suchthatoldmenandyoungwomen
train
areoverrepresented. ApredictorthatminimizeslossonS mightleveragethiscorrelation,and
train
thusperformpoorlyontheunderrepresentedsubgroupsofoldwomenoryoungmen.
Inpractice,subgrouplabels g canbeexpensivetocollect. Thus,approachestothesubgroup
i
robustnessproblemvaryintermsofwhetherweobservethegrouplabel g inthetrainingsetS
i train
andinthevalidationsetS . Inparticular,therearethreesettingsofinterest:
val
•
Full-information(Train✓/Val✓):
Weobservethegrouplabelsforboththetrainingdataset
S andvalidationdatasetsetS .
train val
• Partial-information(Train✗/Val✓): WeobservethegrouplabelsforthevalidationsetS ,
val
butnotforthe(muchlarger)trainingsetS .
train
• No-information(Train✗/Val✗): WedonothavegroupinformationforeitherS orS .
train val
Notethattheoreticallythissettingisunsolvable,sinceforanynon-perfectclassifier f,there
existsanassignmentofgrouplabelssothattheworst-groupaccuracyiszero. Nevertheless,
subgroupsofrelevantpracticalinteresttypicallyhavestructurethatallowsfornon-trivial
resultsevenwithnoinformation.
Inthiswork,wefocusonthepartial-informationandno-informationsettings,sinceacquiring
grouplabelsfortheentiretrainingsetisoftenprohibitivelyexpensive. Still,inSection5,weshow
that our proposed methods (D3M for the partial-information setting, and AUTO-D3M for the
no-informationsetting)performcomparablytofull-informationapproaches.
3 Related work
Beforeintroducingourmethod(Section4),wediscussafewrelatedlinesofwork.
Approaches to subgroup robustness. The group robustness problem (Section 2) has attracted
a wide variety of solutions (see, e.g., [ABG+19; KXR+19; SKH+20; LHC+21; KIW22; QPI+23]).
Broadly,thesesolutionsfallintooneoftwocategories—modelinterventionsanddatainterventions.
Modelinterventionstargeteithermodelweights[STE+21;SIM24]orthetrainingprocedure[SKH+20;
KIW22]. Datainterventions,ontheotherhand,seektoimproveworst-groupaccuracybymodifying
3the training dataset. For example, data balancing removes or subsamples examples so that all
subgroups are equally represented. Idrissi et al. [IAP+22] find that this simple approach can
performsonparwithmuchmoreintricatemodelinterventionmethods.
In this work, we focus on data interventions, for two reasons. First, it is often training data
thatdrivesmodels’disparateperformanceacrossgroups[MMS+21],e.g.,viaspuriouscorrelations
[ODC+20]orunderrepresentation[BG18]. Second,datainterventionsdonotrequireanycontrol
over the model training procedure, which can make them a more practical solution (e.g., when
usingML-as-a-service). Indeed,sincedatainterventionapproachesonlymanipulatethedataset,
theyarealsoeasytocombinewithmodelinterventiontechniques.
Compared to our work, the main drawback of existing data interventions is that they often
(a)requiresubgrouplabelsforthetrainingdata(whichmightnotbeavailable),and(b)hurtthe
models’naturalaccuracyonskeweddatasets[CZP+23;SS22]. Inthisworkwecircumventthese
limitations,byproposingadata-basedapproachtodebiasingthatcanpreservenaturalaccuracy
withoutaccesstosubgroupinformation.
Biasdiscovery. Anotherrelatedlineofworkidentifiesbiasesinmachinelearningdatasetsand
algorithms. Fortheformer,previousworkshaveshownthatlarge,uncurateddatasetsusedfor
trainingmachinelearningmodelsoftencontainproblematicorbiaseddata[BP21;BPK21;Thi23].
Rajietal.[RKH+22]showthatdatabiascanbeahurdletowardsdeployingfunctionalmachine
learningmodels. Nadeemetal.[NBR20]curateadatasettoestimatebiasinNLPmodels. Adebayo
etal.[AHY+23]showthatlabelerrorscandisproportionatelyaffectdisparitymetrics.
Onthelearningalgorithmside,Shahetal.[STR+20]andPulietal.[PZW+23]showthatthe
inductive bias of neural networks may encourage reliance on spurious correlations. Pezeshki
etal.[PBI+23]leveragetwonetworkstrainedonrandomsplitsofdatawhileimitatingconfident
held-outmistakesmadebyitssiblingtoidentifythebias. Shahetal.[SPI+22]showthatalgorithmic
design choices (e.g., the choice of data augmentation) can significantly impact models’ reliance
onspuriouscorrelations. Finally,therehasbeenavarietyofworkon“slicediscovery”[JLM+22;
EVS+22],wherethegoalistodiscoversystematicerrorsmadebymachinelearningmodels.
Data selection for machine learning. Our work uses data selection to improve the subgroup
robustnessofmachinelearningmodels. Inthisway,webuildonarecentlineofworkhasexplored
dataselectionforimprovingvariousmeasuresofmodelperformance. Forexample,Engstrometal.
[EFM24]andXieetal.[XSM+23]selectpretrainingdataforLLMs. Similarly,Xiaetal.[XMG+24]
andNguyenandWong[NW23]selectdataforfinetuningandin-contextlearning,respectively. In
anotherrelatedwork,Wangetal.[WWH24]proposeamethodtoreweighttrainingdatainorderto
improvemodels’fairness.
Manyoftheseworksleveragedataattributionmethodstoselectdatathatimprovesmodelper-
formance. Onelineofworkaimstoapproximatetheinfluencefunction[HRR+11]—aclosed-form
approximationoftheeffectofdroppingoutasinglesample—througheitherHessianapproximation
[KL17;SZV+22;HL22;BNL+22]orsimilarity-basedheuristics[PLS+20]. Anotherrelatedlineof
worktakesagame-theoreticapproach,andestimatestheShapleycontribution[GZ19;LZL+22]of
eachdatapointtomodelperformance. Finally,herewerelyonalineofworktakingaprediction-
basedapproach,wherethegoalistopredictmodelbehaviordirectlyasafunctionofthetraining
data[IPE+22;PGI+23].
44 Debiasing datasets with datamodeling (D3M)
Inthissection,wepresentourdata-basedapproachtotrainingdebiasedclassifiers. Themainidea
behindourapproachistoidentify(andremove)thetrainingsamplesthatnegativelycontributeto
themodel’sworst-groupaccuracy,bywritingmodelpredictionsasafunctionofthetrainingdata.
Preliminaries. LetS = {(x ,y ),...,(x ,y )}beadatasetofinput-labelpairs. Foranysubsetof
1 1 n n
thedataset—as representedby indices D ⊂ [n]—let θ(D) ∈ Rp bethe parametersofaclassifier
trainedon D. Givenanexamplez = (x,y),let f(z;θ)bethecorrect-classmarginonzofaclassifier
withparametersθ (definedaslog( p ),where pistheconfidenceassignedtoclassyforinput x).
1−p
Adatamodel[IPE+22]fortheexamplezisasimplefunctionthatpredicts f(z;θ(D))directlyasa
functionof D,i.e.,afunction fˆ : 2[n] → [0,1]suchthat
z
fˆ(D) ≈ f(z;θ(D)) for D ⊂ [n].
z
Recent works (e.g., [IPE+22; LZL+22; PGI+23]) demonstrate the existence of accurate linear
datamodels—functions pˆ that decompose additively in terms of their inputs D. In other words,
theseworksshowthatonecancomputeexample-specificvectorsτ(z) ∈ Rn suchthat
fˆ(D) := ∑ τ(z) ≈ f(z;θ(D)). (2)
z i
i∈D
The coefficients τ(z) have a convenient interpretation as quantifying the “importance” of the
i
i-th training sample to model performance on example z (i.e., as a data attribution score [HL22;
WSM+23]). Inwhatfollows,wewillassumeaccesstocoefficientsτ(z)foranyexamplez—atthe
endofthissection,wewillshowhowtoactuallyestimatethecoefficientvectorsτ(z)efficiently.
Debiasingapproach. Howcanweleveragedatamodelingtodebiasadataset? Recallthatour
goalistoremovethesamplesinSthatleadtohighworst-grouperror. Stateddifferently,givena
dataset S ofsize n,wewanttomaximizetheworst-groupperformanceofaclassifier θ(D) with
respecttotheindices D ⊂ [n]thatwetrainon.
Ourmainideawillbetoapproximatethepredictionsofθ(D)usingthecorrespondingdatamod-
els fˆ(D). Toillustratethisidea,supposethatourgoalwastomaximizeperformanceonasingletest
z
examplez,i.e.,argmax f(z;θ(D)). Wecanapproximatethisgoalasfindingargmax fˆ(θ(D)):
D D z
then,duetothelinearityofthedatamodel fˆ,thetrainingsamplesthathurtperformanceonzare
z
simplythebottomindicesofthevectorτ(z).
Now,thisanalysisappliesnotonlytoasingleexamplez,buttoanylinearcombinationoftest
examples. Inparticular,ifwewishtomaximizeperformanceonalinearcombinationofvalidation
examples, we simply take the linear combination of their coefficients, and remove the training
examplescorrespondingtothesmallestcoordinatesoftheaveragedvector.
Debiasingwithgroup-annotatedvalidationdata. Givenasetofvalidationsamplesforwhich
thegrouplabelsg areobservable,ourlastobservationgivesrisetothefollowingsimpleprocedure:
i
1. Compute group coefficients τ(G) for each G. Since we have group annotations for each
validationsample,wecandefineavectorτ(G)foreachgroupG ∈ G assimplytheaverage
τ(z)withineachgroup.
52. Computegroupalignment. Next,wecomputeagroupalignmentscore A foreachtraining
i
sample i ∈ [n], whichcapturesthetheimpactofthesampleonworst-groupperformance.
Sincetheremaybemanylow-performinggroups,weusea“smoothmaximum”functionto
weighteachgroupaccordingtoitsaverageloss. Thus,foratrainingexamplei,
∑ exp(βℓ )·τ(g)
g∈G g i
A = , wherewesethyperparameter β = 1. (3)
i ∑ g′∈Gexp(βℓ g′)
Here,ℓ isthelossofabaseclassifierθ(S)ongroup g(evaluatedonthegivenvalidationset).
g
3. Removedriversofbias. Finally,weconstructanewtrainingsetS bykeepingonlythe
new
examples with the highest group alignment scores, i.e., removing the examples that most
degradeworst-groupaccuracy:
S = argtop-k({A : z ∈ S }).
new i i train
Wemaketwobriefobservationsabouthyperparametersbeforecontinuing. Whencomputing
the group alignment score in Step 2, the hyperparameter β controls the temperature of the soft
maximumfunctionin(3). When β → 0,thegroupalignment A measurestheimpactofthei-th
i
training example on the “balanced” performance (treating all groups equally). As β → ∞ , A
i
collapsestothetrainingexample’simportancetoonlytheworstgroup,whichissuboptimalifmodels
performpoorlyonmorethanonegroup. Forsimplicity,wetake β = 1andrefrainfromtuningit.
Anotherhyperparameterinthealgorithmaboveisthenumberofexamplestoremove,k. We
considertwodifferentwaysofsettingthishyperparameter. Oneapproachistosearchforthevalue
of k thatmaximizesworst-groupaccuracyonthevalidationset S . Alternatively, wefindthat
val
the simple (and much more efficient) heuristic of removing all examples with a negative group
alignment score (i.e., examples for which A < 0) tends to only slightly over-estimate the best
i
number of examples to remove (see, e.g., Figure 2). Thus, unless otherwise stated, we use this
heuristicwhenreportingourresults.
Debiasing without group annotations. Our procedure above relies on group annotations for
a validation set S to compute the “per-group coefficients” τ(G). In many real-world settings,
val
however,modelsmightexhibitdisparateperformancealongunannotatedsubpopulations—inthis
case,wemightnothaveavalidationsetonwhichwecanobservegroupannotations g. Canwe
i
stillfixdisparatemodelperformanceinthissetting?
In general, of course, the answer to this question is no: one can imagine a case where each
individual example is its own subpopulation, in which case worst-group accuracy will be zero
unlesstheclassifierisperfect. Inpracticalsettings,however,wetypicallycareaboutthemodel’s
disparateperformanceoncoherentgroupsoftestexamples. Thequestion,then,becomeshowto
findsuchcoherentgroups.
Wepositthataunifyingfeatureofthesesubpopulationsisthattheyaredata-isolated,i.e.,that
models’ predictions on these coherent groups rely on a different set of training examples than
models’ predictions on the rest of the test data. Conveniently, prior works [IPE+22; SPI+22]
showthattofinddata-isolatedsubpopulations,onecanleveragethedatamodelmatrix—amatrix
constructed by stacking the datamodel vectors τ(z) for each test example. Intuitively, the top
principal component of this matrix encodes the direction of maximum variability among the
vectorsτ(z). Thus,byprojectingthedatamodelvectorsτ(z)ofourvalidationexamplesontothis
topprincipalcomponent,wecanidentifytheexamplesthatare,inasense,“maximallydifferent”
6fromtherestofthetestexamplesintermsofhowtheyrelyonthetrainingset. Thesemaximally
differentexamplescorrespondtoanisolatedsubpopulation,towhichwecanapplyD3Mdirectly.
Thisapproach(whichwecallAUTO-D3M),enablesustoperformend-to-enddebiasingwithout
anygroupannotations. Thismethodproceedsinfoursteps. Foreachclass:
1. ConstructamatrixTofstackedattributionvectors,whereT = τ(z ) .
ij i j
2. LetvbethetopprincipalcomponentofT.
3. Projecttheattributionvectorτ(z)ontovandconstruct“grouppseudo-labels”
g = 1{τ(z )⊤ v ≥ λ}.
i i
whereλisahyperparameter1
4. Apply D3M withthegrouppseudo-labelstotrainadebiasedclassifier.
Adepictionof AUTO-D3M canbefoundinAppendixFigure7.
Estimatingthecoefficientsτ(z). InordertooperationalizeD3M and AUTO-D3M,itremainsto
showthatwecanactuallyestimatecoefficients τ(z) satisfying(2). Toaccomplishthis, weusea
methodcalledTRAK[PGI+23]. Leveragingdifferentiabilityofthemodeloutput f(z;θ)withrespect
tothemodelparametersθ, TRAKcomputesthecoefficientvectorτ(z)foranexamplezasfollows:
(a) Trainamodelθ∗ := θ(S)ontheentiretrainingdatasetS = {z ,...,z }.
1 n
(b) Sample a random Gaussian matrix P ∈ Rp×k where p is the dimensionality of θ∗ (i.e., the
numberofmodelparameters)andkisahyperparameter;
(c) For an example z, define g(z) := P⊤∇ f(z;θ∗) as the randomly-projected model output
θ
gradient(withrespecttothemodelparameters)evaluatedatz.
(d) Computethecoefficientvector
 −1
τ(z) i = g(z)⊤ ∑ g(z j)·g(z j)⊤  g(z i)·(1−σ(f(z;θ∗)))
i-thcoeffici(cid:124) en(cid:123) t(cid:122) fo(cid:125)
rexamplez
zj∈S
(e) Repeatsteps(a)-(d)for T trials,andaveragetheresultstogetafinalcoefficientvectorτ(z).
Thetrialsareidenticalsavefortherandomnessinvolvedinstep(a).
Remark1(Anoteonscalability.). Intermsofcomputationalcost,TRAKinvolvestakingasinglebackward
pass(i.e.,gradientcomputation)oneachtrainingandvalidationexample. The(projected)gradientsarethen
savedtocompute TRAK scores. Typically, TRAK iscomputedoveranensembleofMmodels(followingthe
originalpaper,weuse M = 100modelseachtrainedwith50%ofthetrainingdata). However,ourapproach
isgeneralandcanbeusedwithanydatamodelingtechnique(i.e.,anymethodforapproximatingτ(z)).
1Forourexperimentswechooseλsothatthelowerperforminggroupconsistsof35%ofthevalidationexamplesof
thatclass.
7GroupInfo WorstGroupAccuracy(%)
Method
Train/Val CelebA-Age CelebA-Blond Waterbirds MultiNLI
ERM 56.7 45.9 57.9 67.2
✗ ✗
/
AUTO-D3M (ours) 76.0 83.8 81.0 75.0
JTT[LHC+21] 61.0 81.6 63.6 72.6
✗ /✓ DFR∗ [KIW22] 70.4 88.4 89.0 74.7
D3M (ours) 75.6 90.0 87.2 76.0
RWG[IAP+22] 75.6 88.4 81.2 68.4
✓ /✓ SUBG[IAP+22] 68.5 88.3 85.5 67.8
GroupDRO[SKH+20] 74.8 90.6 72.5 77.7
Table1: Worst-groupaccuraciesonfourgrouprobustnessdatasets. A∗denotesmethodsthatuse
validationgrouplabelsforbothfinetuningandhyperparametertuning.
5 Results
InSection4,wepresentedD3M—anapproachfordebiasingaclassifierbyidentifyingexamples
which contribute to a targeted bias. In this section, we validate this framework by assessing its
performanceontaskswithknownbiases.
Weconsiderfourclassificationtaskswherethereisaspuriouscorrelationbetweenthetarget
label and a group label in the training dataset: CelebA-Age [LLW+15; JLM+22], CelebA-Blond
[LLW+15],Waterbirds [SRK+20],andMultiNLI [WNB17]. Weprovidemoreinformationabout
thedatasetsinAppendixB.1,andotherexperimentaldetailsinAppendixB.2.
5.1 Quantitativeresults
WefirstevaluateD3MandAUTO-D3Mquantitatively,bymeasuringtheworst-groupaccuracyof
modelstrainedontheselectedsubsetsofthebiaseddatasetsabove.
D3M:Debiasingthemodelinthepresenceofvalidationgrouplabels. InTable1,wecompare
✗ ✓
D3Magainstseveralbaselines,eachofwhichrequireseitheronlyvalidationgrouplabels( / )or
✓ ✓
bothtrainingandvalidationgrouplabels( / ). Wefindthat D3Moutperformsallothermethods
that use the same group information (i.e., only validation group labels) on all datasets except
Waterbirds2. Moreover, D3M performsonparwithmethodsthathavefullaccesstobothtraining
andvalidationgrouplabels.
AUTO-D3M:DiscoveringbiaseswithTRAK. Wenowconsiderthecasewherevalidationgroup
labels are not accessible. Using AUTO-D3M, we debias our model using pseudo-annotations
derivedfromthetopprincipalcomponentofthe TRAKmatrix(AUTO-D3M inTable1)3. Notethat
AUTO-D3MistheonlymethodotherthanERMthatdoesnotrequireeithertrainorvalidationgroup
labels. Despitethis,AUTO-D3M achievescompetitiveworst-groupaccuracyinourexperiments.
WeemphasizethatAUTO-D3M doesnotrequiregrouplabelsatall—inparticular,wedonotuse
grouplabelstodohyperparameterselectionormodelselectionwhenweretrain.
2NotethatWaterBirdshasmoreworst-groupexamplesinthevalsplit(133)thanthetrainsplit(56). SinceDFR
directlyfine-tunesonthevalidationset,ithasanadvantagehereoverothermethods.
3ForMultiNLI,wechosethePCAcomponentbyinspectionthatcapturesexampleswith/withoutnegation.
8Worst Group Accuracy
Random
Random Majority (oracle)
0.7
D3M
Balanced
0.6
0.5
0 5000 10000 15000 20000 25000 30000 35000
Examples Removed
Figure2: WorstgroupaccuracyonCelebA-Ageasafunctionofthenumberofexampleskremoved
from the training set, using various removal methods. In green, D3M removes the k training
exampleswiththemostnegativealignmentscores A . Thegreenstarmarksthevalueofkselected
i
byourheuristic(A < 0). Inblueistheperformanceofabaselinethatremoveskrandomexamples
i
fromthetrainingset,andinorangeisdatasetbalancing,whichremovesexamplesrandomlyfrom
themajoritygroup. Comparedtobaselines, D3M efficientlyimprovesworstgroupaccuracy.
The effect of the number of removed examples k. How well does D3M isolate the training
examples thatdrive disparate performance? To answerthis question, were-run themethod on
CelebA-Agewhilevaryingthehyperparameterk. Thatis,weiterativelyremovetrainingexamples
fromCelebA-Agestartingwiththemostnegative A andmeasuretheworst-groupandbalanced
i
accuracy(SeeFigure2). CelebA-Agehas40K“majority”examplesand10K“minority”examples;
thus, naive balancing requires removing 30K training examples. In contrast, by isolating which
specific majority examples contribute to the bias, our method is able to debias the classifier by
removingonly10thousandexamples.
Our heuristic of removing examples with negative A (the star in Figure 2) slightly over-
i
estimatesthebestnumberofexamplestoremove. Thus,whilethisheuristicgivesadecentstarting
pointfork,actuallysearchingforthebestkmightfurtherimproveperformance.
5.2 Qualitativeresults
What type of data does our method flag? Do the examples we identify as driving worst-group
errorsharesomecommoncharacteristics? Toanswerthesequestions,inthissectionweinspect
thedataremovedbyourmethodandidentifysubpopulations(usingauxiliaryannotations)that
contributedisproportionatelytoworst-grouperror. Wethenretrainthemodelafterexcludingall
trainingexamplesfromtheidentifiedsubpopulationsandshowthatthisisaviablestrategyfor
improvingworst-groupaccuracy,performingcompetitivelywithD3Mwhileofferingmoreinsight
intotheexamplesbeingremoved.
5.3 Qualitativeresults
What type of data does our method flag? In particular, do the examples we identify as driving
the targeted bias share some common characteristics? To test this hypothesis, we inspect the
data removed by our method and identify subpopulations within the majority groups that are
9
ycaruccAYoung, Young, Young, Old, Old, Old, Old, Old, Old, Young,
Gray Hair Gray Hair Gray Hair 5 o-clock shadow5 o-clock shadow5 o-clock shadow Bushy Eyebrows Bushy Eyebrows Sideburns Blond Hair
Figure 3: Randomly sampled examples from the subpopulations with the most negative group
alignmentscores. Wefindthatmanyoftheseexampleshavelabelingerrors(e.g.,platinumblond
insteadofgrayhair.)
Old Young Old Young Old Young
1000
5 o Clock Shadow Chubby Mustache
800
Arched Eyebrows Double Chin Narrow Eyes
600
Bags Under Eyes Eyeglasses No Beard 400
Bald Goatee Rosy Cheeks 200
Bangs Gray Hair Sideburns 0
Big Nose Heavy Makeup Wavy Hair
Black Hair High Cheekbones Wearing Earrings
Blond Hair Male Wearing Hat
Bushy Eyebrows Mouth Slightly Open Wearing Lipstick
200
Figure4: Theaveragegroupalignmentscoreofthetrainingexamplesineachsubpopulationof
CelebA-Age. Subpopulationssuchas“old”with“bushyeyebrows”or“young”with“grayhair”
haveparticularlynegativescores.
disproportionatelyresponsibleforthebias. Wethenretrainthemodelafterexcludingalltraining
examplesfromtheidentifiedsubpopulationsandshowthatthisisaviablestrategyformitigating
theunderlyingbias.
Findingsubpopulationsresponsibleformodelbias. Considerarunningexamplewherewe
train a model on the CelebA-Age dataset to predict whether a person is “young” or “old,” with
gender (only “male” or “female” are represented in CelebA-Age) being a spurious feature (i.e.,
youngwomenandoldmenareoverrepresented). CelebA-Agehasavarietyofannotationsbeyond
ageandgender,suchaswhetherthepersoniswearingeyeglasses. Inthissection,weusethese
extraannotationstoidentifycoherentsubpopulationsthatareflaggedbyourmethods.
In particular, we consider subpopulations formed by taking the Cartesian product of labels
andannotations(e.g.,subpopulationsoftheform{“young”,“wearingeyeglasses”}). Foreachof
thesesubpopulations,wecalculatetheaveragegroupalignmentscore A ofthetrainingexamples
i
within that subpopulation (see Figure 4). We find that subpopulations such as {“young”, “gray
hair”},{“old”,“5o’clockshadow”}or{“old,”“bushyeyebrows”}haveparticularlynegativegroup
alignmentscores. InFigure3,weshowexamplesfromthesubpopulationswiththemostnegative
groupalignmentscores,andobservethatmanyofthemcontainlabelingerrors.
Retrainingwithoutidentifiedsubpopulations. Oncewehaveidentifiedsubpopulationswith
negative average alignment scores, a natural strategy for mitigating the underlying bias is to
excludetheentiresubpopulationsfromthetrainingset. (ThisisincontrasttoordinaryD3M,which
onlyremovesthetrainingexamplesforwhichA < 0.) Toexplorethisapproach,weexcludethefive
i
subpopulationswiththemostnegativeaveragealignmentfromtheCelebA-Agedataset–theseare
10Tench Strawberries Red Wolf Cauliflower
(+)
(-)
Figure5: ForfourImageNetclasses,themostextreme(positiveornegative)examplesaccordingto
thetopPCAdirectionoftheTRAKmatrix. Ourmethodidentifiescolorandco-occurrencebiases.
{“young”,“grayhair”},{“old,”“5o’clockshadow”},{“old,”“bushyeyebrows”},{“young,”“blond
hair”},and{“old,”“sideburns”}. Afterremovingthesesubpopulationsandretrainingthemodelon
thismodifiedtrainingset,wegetaworst-groupaccuracy(WGA)of68.4%—animprovementof
~12%overtheWGAoftheoriginalmodel(56.7%).
6 Case study: Finding and mitigating biases on ImageNet
InSection5weevaluated D3MandAUTO-D3Mondatasetswhereaspuriouscorrelation(orbias)
leadingtopoorworst-groupaccuracywasalreadyknown. Inthissection,wedeploy AUTO-D3M
to discover and mitigate biases within the ImageNet dataset, which does not have pre-labelled
biasesoravailablegroupannotations.
Identifying ImageNet biases. We use TRAK to compute a coefficient matrix T (see Step 1 of
AUTO-D3M in Section 4) for a held out validation split (10% of the training set). Focusing on
sevenImageNetclasses,weusethefirstprincipalcomponentofthematrixTtoidentifypotential
biases. InFigure5,wedisplaythemostextremetrainingexamplesaccordingtothetopprincipal
componentforfouroftheseclasses. PCAidentifiessemanticallycolorandco-occurrencebiases
(e.g., tench fishes with or without humans or yellow/white cauliflowers that are either cooked
or uncooked.) In fact, our identified biases match the challenging subpopulations in Jain et al.
[JLM+22]and Moayerietal.[MSF22].
ERM 80
D3M
Auto-D3M
60
40
20
0
Red Wolf Tench Cauliflower Strawberry Snorkel Howler Monkey Dog Sled
(RedCoat) (Humanpresent) (NotCooked) (Notonaplate) (NotUnderwater) (Notfarawayinatree) (Onlydogsvisible)
Class
Figure6: Worst-groupaccuracyfortheImageNetclassesstudiedinSection6afterinterveningwith
either D3M or AUTO-D3M.
MitigatingImageNetbiaseswithAUTO-D3M. ForeachofthefourtargetedImageNetclasses,
weseektomitigatetheidentifiedfailuremodeswith AUTO-D3M.Weconsidertwosettingsbased
onthelevelofhumanintervention. Inthefirst,wemanuallyassigneachofthevalidationimages
11
ycaruccA
puorG
tsroWtoagroupaccordingtoahumandescriptionofidentifiedbias(e.g.,animageofatenchisingroup
1ifahumanispresentandgroup2otherwise),andthenusethosegrouplabelswith D3M.4 In
thesecondsetting,wedebiasinapurelyautomaticfashion,using AUTO-D3M toderivepseudo-
group labels from the top principal component. In Figure 6, we display worst group accuracy
on the test images of the targeted class (evaluated using manual group assignments of the 50
test examples). Both D3M and AUTO-D3M improve worst group accuracy over ERM without
significantlyimpactingtheoverallImageNetaccuracy(seeAppendixC.2).
7 Conclusion
WeproposeDataDebiasingwithDatamodels(D3M),asimplemethodfordebiasingclassifiersby
isolatingtrainingdatathatdisproportionatelycontributestomodelperformanceonunderperform-
inggroups. Unlikeapproachessuchasbalancing,ourmethodonlyremovesasmallnumberof
examplesanddoesnotrequiretraininggroupannotationsoradditionalhyperparametertuning.
Moregenerally,ourworktakesafirststeptowardsdata-centricmodeldebiasing.
Acknowledgements
Work supported in part by the NSF grant DMS-2134108. This material is based upon work
supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.
HR001120C0015.
4Here,weonlyconsiderthetargetclasswhencomputingthelossweighting.Asaresult,theheuristicoverestimates
thenumberofexamplesktoremove,andsoweinsteadsearchfortheoptimalkusingourheldoutvalidationset.
12References
[ABG+19] MartinArjovsky,LéonBottou,IshaanGulrajani,andDavidLopez-Paz.“Invariant
riskminimization”.In:arXivpreprintarXiv:1907.02893(2019).
[AHY+23] JuliusAdebayo,MelissaHall,BowenYu,andBobbieChern.“Quantifyingandmit-
igating the impact of label errors on model disparity metrics”. In: arXiv preprint
arXiv:2310.02533(2023).
[ALM+22] JuliaAngwin,JeffLarson,SuryaMattu,andLaurenKirchner.“MachineBias*”.In:
EthicsofDataandAnalytics.1stEdition.AuerbachPublications,May2022,pp.254–264.
[BG18] JoyBuolamwiniandTimnitGebru.“Gendershades:Intersectionalaccuracydispari-
tiesincommercialgenderclassification”.In:Conferenceonfairness,accountabilityand
transparency(FAccT).2018.
[BNL+22] Juhan Bae, Nathan Ng, Alston Lo, Marzyeh Ghassemi, and Roger Grosse. “If In-
fluence Functions are the Answer, Then What is the Question?” In: ArXiv preprint
arXiv:2209.05364.2022.
[BP21] Abeba Birhane and Vinay Uday Prabhu. “Large image datasets: A pyrrhic win for
computervision?”In:2021IEEEWinterConferenceonApplicationsofComputerVision
(WACV).IEEE,Jan.2021,pp.1536–1546.
[BPK21] AbebaBirhane,VinayUdayPrabhu,andEmmanuelKahembwe.“Multimodaldatasets:
misogyny,pornography,andmalignantstereotypes”.In:arXivpreprintarXiv:2110.01963
(2021).
[CP21] KateCrawfordandTrevorPaglen.“ExcavatingAI:thepoliticsofimagesinmachine
learningtrainingsets”.In:AI&society36.4(Dec.2021),pp.1105–1116.
[CZP+23] RhysCompton,LilyZhang,AahladPuli,andRajeshRanganath.“WhenMoreisLess:
IncorporatingAdditionalDatasetsCanHurtPerformanceByIntroducingSpurious
Correlations”.In:arXivpreprintarXiv:2308.04431(2023).
[EFM24] Logan Engstrom, Axel Feldmann, and Aleksander Madry. “DsDm: Model-Aware
DatasetSelectionwithDatamodels”.In:2024.
[EVS+22] SabriEyuboglu,MayaVarma,KhaledSaab,Jean-BenoitDelbrouck,ChristopherLee-
Messer, Jared Dunnmon, James Zou, and Christopher Ré. “Domino: Discovering
systematicerrorswithcross-modalembeddings”.In:arXivpreprintarXiv:2203.14960
(2022).
[Fer23] EmilioFerrara.“FairnessandBiasinArtificialIntelligence:ABriefSurveyofSources,
Impacts,andMitigationStrategies”.en.In:Sci6.1(Dec.2023),p.3.
[GBB+20] LeoGao,StellaBiderman,SidBlack,LaurenceGolding,TravisHoppe,CharlesFoster,
Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. “The pile: An 800gb
dataset of diverse text for language modeling”. In: arXiv preprint arXiv:2101.00027
(2020).
[GZ19] Amirata Ghorbani and James Zou. “Data shapley: Equitable valuation of data for
machinelearning”.In:InternationalConferenceonMachineLearning(ICML).2019.
[HL22] ZaydHammoudehandDanielLowd.“TrainingDataInfluenceAnalysisandEstima-
tion:ASurvey”.In:arXivpreprintarXiv:2212.04612.2022.
13[HRR+11] Frank R Hampel, Elvezio M Ronchetti, Peter J Rousseeuw, and Werner A Stahel.
Robuststatistics:theapproachbasedoninfluencefunctions.Vol.196.JohnWiley&Sons,
2011.
[HZR+15] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.DeepResidualLearningfor
ImageRecognition.2015.
[IAP+22] Badr Youbi Idrissi, Martin Arjovsky, Mohammad Pezeshki, and David Lopez-Paz.
“Simpledatabalancingachievescompetitiveworst-group-accuracy”.In:Conferenceon
CausalLearningandReasoning.PMLR.2022,pp.336–351.
[IPE+22] AndrewIlyas,SungMinPark,LoganEngstrom,GuillaumeLeclerc,andAleksander
Madry. “Datamodels: Predicting Predictions from Training Data”. In: International
ConferenceonMachineLearning(ICML).2022.
[JLM+22] Saachi Jain, Hannah Lawrence, Ankur Moitra, and Aleksander Madry. “Distilling
Model Failures as Directions in Latent Space”. In: arXiv preprint arXiv:2206.14754
(2022).
[KIW22] PolinaKirichenko,PavelIzmailov,andAndrewGordonWilson.“Lastlayerre-training
issufficientforrobustnesstospuriouscorrelations”.In:arXivpreprintarXiv:2204.02937
(2022).
[KL17] PangWeiKohandPercyLiang.“UnderstandingBlack-boxPredictionsviaInfluence
Functions”.In:InternationalConferenceonMachineLearning.2017.
[KRA+18] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi
Pont-Tuset,ShahabKamali,StefanPopov,MatteoMalloci,TomDuerig,etal.“The
open images dataset v4: Unified image classification, object detection, and visual
relationshipdetectionatscale”.In:arXivpreprintarXiv:1811.00982(2018).
[KXR+19] Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi
Feng,andYannisKalantidis.“Decouplingrepresentationandclassifierforlong-tailed
recognition”.In:arXivpreprintarXiv:1910.09217(2019).
[LHC+21] Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh,
ShioriSagawa,PercyLiang,andChelseaFinn.“JustTrainTwice:ImprovingGroup
RobustnesswithoutTrainingGroupInformation”.In:InternationalConferenceonMa-
chineLearning.2021.
[LLW+15] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. “Deep Learning Face At-
tributesintheWild”.In:InternationalConferenceonComputerVision(ICCV).2015.
[LZL+22] JinkunLin,AnqiZhang,MathiasLecuyer,JinyangLi,AurojitPanda,andSiddhartha
Sen.“MeasuringtheEffectofTrainingDataonDeepLearningPredictionsviaRan-
domizedExperiments”.In:arXivpreprintarXiv:2206.10013(2022).
[MMS+21] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram
Galstyan.“ASurveyonBiasandFairnessinMachineLearning”.In:(2021).
[MSF22] MazdaMoayeri,SahilSingla,andSoheilFeizi.“Hardimagenet:Segmentationsfor
objectswithstrongspuriouscues”.In:AdvancesinNeuralInformationProcessingSystems
35(2022),pp.10068–10077.
[NAB+23] YannicNeuhaus,MaximilianAugustin,ValentynBoreiko,andMatthiasHein.“Spuri-
ous featureseverywhere-large-scale detectionof harmfulspurious features inima-
genet”.In:ProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision.2023,
pp.20235–20246.
14[NBR20] MoinNadeem,AnnaBethke,andSivaReddy.“StereoSet:Measuringstereotypical
biasinpretrainedlanguagemodels”.In:arXivpreprintarXiv:2004.09456(2020).
[NW23] TaiNguyenandEricWong.“In-contextexampleselectionwithinfluences”.In:arXiv
preprintarXiv:2302.11042(2023).
[ODC+20] Lauren Oakden-Rayner, Jared Dunnmon, Gustavo Carneiro, and Christopher Ré.
“Hiddenstratificationcausesclinicallymeaningfulfailuresinmachinelearningfor
medicalimaging”.In:ProceedingsoftheACMconferenceonhealth,inference,andlearning.
2020.
[PBI+23] MohammadPezeshki,DianeBouchacourt,MarkIbrahim,NicolasBallas,PascalVin-
cent,andDavidLopez-Paz.“DiscoveringenvironmentswithXRM”.In:arXivpreprint
arXiv:2309.16748(2023).
[PGI+23] Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Alek-
sander Madry. “TRAK: Attributing Model Behavior at Scale”. In: Arxiv preprint
arXiv:2303.14186.2023.
[PLS+20] GarimaPruthi,FrederickLiu,MukundSundararajan,andSatyenKale.“Estimating
TrainingDataInfluencebyTracingGradientDescent”.In:NeuralInformationProcessing
Systems(NeurIPS).2020.
[PZW+23] Aahlad Puli, Lily Zhang, Yoav Wald, and Rajesh Ranganath. “Don’t blame dataset
shift!Shortcutlearningduetogradientsandcrossentropy”.In:arXivpreprintarXiv:2308.12553
(2023).
[QPI+23] ShikaiQiu,AndresPotapczynski,PavelIzmailov,andAndrewGordonWilson.“Sim-
pleandFastGroupRobustnessbyAutomaticFeatureReweighting”.In:arXivpreprint
arXiv:2306.11074(2023).
[RKH+22] InioluwaDeborahRaji,IElizabethKumar,AaronHorowitz,andAndrewSelbst.“The
fallacy of AI functionality”. In: Proceedings of the 2022 ACM Conference on Fairness,
Accountability,andTransparency.2022,pp.959–972.
[SIM24] Harshay Shah, Andrew Ilyas, and Aleksander Madry. “Decomposing and Editing
ModelPredictions”.In:arXivpreprint.2024.
[SKH+20] ShioriSagawa,PangWeiKoh,TatsunoriB.Hashimoto,andPercyLiang.“Distribution-
allyRobustNeuralNetworksforGroupShifts:OntheImportanceofRegularization
forWorst-CaseGeneralization”.In:InternationalConferenceonLearningRepresentations.
2020.
[SPI+22] HarshayShah,SungMinPark,AndrewIlyas,andAleksanderMadry.“ModelDiff:A
FrameworkforComparingLearningAlgorithms”.In:arXivpreprintarXiv:2211.12491.
2022.
[SRK+20] ShioriSagawa,AditiRaghunathan,PangWeiKoh,andPercyLiang.“Aninvestigation
of why overparameterization exacerbates spurious correlations”. In: International
ConferenceonMachineLearning.PMLR.2020,pp.8346–8356.
[SS22] RoySchwartzandGabrielStanovsky.“Onthelimitationsofdatasetbalancing:The
lostbattleagainstspuriouscorrelations”.In:arXivpreprintarXiv:2204.12708(2022).
[STE+21] ShibaniSanturkar,DimitrisTsipras,MahalaxmiElango,DavidBau,AntonioTorralba,
and Aleksander Madry. “Editing a classifier by rewriting its prediction rules”. In:
Preprint.2021.
15[STR+20] Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth
Netrapalli.“Thepitfallsofsimplicitybiasinneuralnetworks”.In:AdvancesinNeural
InformationProcessingSystems33(2020),pp.9573–9585.
[SZV+22] AndreaSchioppa,PolinaZablotskaia,DavidVilar,andArtemSokolov.“Scalingup
influence functions”. In: Proceedings of the AAAI Conference on Artificial Intelligence.
Vol.36.8.2022,pp.8179–8186.
[Thi23] DavidThiel.IdentifyingandeliminatingCSAMingenerativeMLtrainingdataandmodels.
Tech.rep.2023.
[WNB17] AdinaWilliams,NikitaNangia,andSamuelRBowman.“Abroad-coveragechallenge
corpusforsentenceunderstandingthroughinference”.In:arXivpreprintarXiv:1704.05426
(2017).
[WSM+23] TheodoraWorledge,JudyHanwenShen,NicoleMeister,CalebWinston,andCarlos
Guestrin. “Unifying corroborative and contributive attributions in large language
models”.In:arXivpreprintarXiv:2311.12233(2023).
[WWH24] HaonanWang,ZiweiWu,andJingruiHe.“FairIF:BoostingFairnessinDeepLearning
viaInfluenceFunctionswithValidationSetSensitiveAttributes”.In:Proceedingsofthe
17thACMInternationalConferenceonWebSearchandDataMining.2024,pp.721–730.
[XMG+24] MengzhouXia,SadhikaMalladi,SuchinGururangan,SanjeevArora,andDanqiChen.
“Less: Selecting influential data for targeted instruction tuning”. In: arXiv preprint
arXiv:2402.04333(2024).
[XSM+23] SangMichaelXie,ShibaniSanturkar,TengyuMa,andPercyLiang.“Dataselection
forlanguagemodelsviaimportanceresampling”.In:arXivpreprintarXiv:2302.03169
(2023).
16A Omitted Figures
Figure7: Procedurefordiscoveringspuriousattributes. Todiscoverspuriousattributes,wefirst
computethe TRAK matrixforthevalidationset. Wethensplitthevalidationexamplesintotwo
groupsbasedonthetopprincipalcomponentsofthe TRAKmatrix. Finally,weusethesegroupsto
createpseudo-annotationsforthevalidationset.
B Details of Experiments
B.1 ExperimentalSetup
Inthissection,wedescribethedatasets,modelsandevaluationprocedurethatweusethroughout
thepaper.
Datasets. Inordertocoverabroadrangeofpracticalscenarios,weconsiderthefollowingimage
classificationandtextclassificationproblems.
• Waterbirds [SKH+20]isabinaryimageclassificationproblem,wheretheclasscorresponds
tothetypeofthebird(landbirdorwaterbird),andthebackgroundisspuriouslycorrelated
withtheclass. Namely,mostlandbirdsareshownonland,andmostwaterbirdsareshown
overwater.
• CelebA-Blond [LLW+15]isabinaryimageclassificationproblem,wherethegoalistopredict
whetherapersonshownintheimageisblond;thegenderofthepersonservesasaspurious
feature,as94%oftheimageswiththe“blond”labeldepictfemales.
• CelebA-Age[LLW+15;JLM+22]isabinaryimageclassificationproblem,wherethegoalisto
predictwhetherapersonshownintheimageisyoung;thegenderofthepersonservesasa
spuriousfeature. Forthistask,wespecificallysubsamplethetrainingsetsuchthattheratio
ofsamplesinthemajorityvs. minoritygroupsis4:1.
• MultiNLI [WNB17;SKH+20]isaclassificationproblemwheregivenapairofsentences,the
taskistoclassifywhetherthesecondsentenceisentailedby,neuralwith,orcontradictsthe
firstsentence. Thespuriousattributefrom Sagawaetal.[SKH+20]describesthepresenceof
negationwords,whichappearmorefrequentlyintheexamplesfromthenegationclass.
17Methods. Webenchmarkourapproachagainstthefollowingmethods:
• ERMissimpleempiricalriskminimizationonthefulltrainingset.
• RWG[IAP+22]isERMappliedtorandombatchesofthedatawherethegroupsareequally
representedwithacombinationofupsampinganddownsamplingsuchthatthesizeofthe
datasetdoesnotchange.
• SUBG [IAP+22] is ERM applied to a random subset of the data where we subsample all
groupssuchthattheyhavethesamenumberofexamples.
• GroupDRO[SKH+20]trainsthatminimizestheworst-caseperformanceoverpre-defined
groupsinthetestdataset.
• JustTrainTwice(JTT)[LHC+21]trainsanERMmodelwithupsampinginitiallymisclassified
trainingexamplesbyaninitialERMmodel.
• DFR[KIW22]trainsanensempleoflinearmodelsonabalancedvalidationset,givenERM
features.
B.2 TrainingDetails
Inthissection,wedetailthemodelarchitecturesandhyperparametersusedbyeachapproach. We
usedthesamemodelarchitectureacrossallapproaches: RandomlyinitializedResNet-18[HZR+15]
forCelebAandImageNet-pretrainedResNet-18sforWaterbirds. WeusetheGroupDROimple-
mentationbySagawaetal.[SKH+20]andDFRimplementationbyKirichenkoetal.[KIW22].
Forallapproaches,wetunehyperparametersforERM-basedmethods(ERM,DFR,andD3M)
andre-weightingbasedmethods(RWG,SUBG,GroupDROandJTT)separately. ForRWG,SUBG,
GroupDROandJTT,weearlystopbasedonhighestworst-groupaccuracyonthevalidationsetas
well. WeoptimizeallapproacheswithAdamoptimizer.
For the CelebA dataset, we all methods with learning rate 1e−3, weight decay 1e−4, and
batchsize512. WetrainRWG,SUBG,GroupDROandJTTwithlearningrate1e−3,weightdecay
1e−4,andbatchsize512. WetrainallmodelsfortheCelebA-Agetasktoupto5epochsandall
modelsforCelebA-Blondtaskupto10epochs.
FortheWaterbirdsdataset,wetraintheapproachesthatusetheERMobjective(includingD3M)
withlearningrate1e−4,weightdecay1e−4,andbatchsize32. WetrainRWG,SUBG,GroupDRO
andJTTwithlearningrate1e−5,weightdecay0.1,andbatchsize32. Wetrainallmodelstoupto
20epochs.
Forallotherhyperparameters,weusethesamehyperparametersasKirichenkoetal.[KIW22]
forDFRandthesamehyperparametersasLiuetal.[LHC+21]forJTT.
WereporttheperformanceofthemodelsviaWorst-groupAccuracy,orBalancedAccuracyin
Table2,whichistheaverageofaccuraciesofallgroups. Ifallgroupsinthetestsethavethesame
numberofexamples,balancedaccuracywillbeequivalenttoaverageaccuracy.
Ourmodelwastrainedonamachinewith8A100GPUs.
18C Omitted Results
C.1 BalancedAccuracies
BelowweincludethebalancedaccuraciesfortheexperimentsinTable2.
GroupInfo CelebA-Age CelebA-Blond Waterbirds MultiNLI
Balanced WorstGroup Balanced WorstGroup Balanced WorstGroup Balanced WorstGroup
Method Train/Val
Accuracy Accuracy Accuracy Accuracy Accuracy Accuracy Accuracy Accuracy
ERM ✗/✗ 77.96 56.65 82.59 45.86 83.40 57.85 80.92 67.19
Auto-TRAK(ours) ✗/✗ 80.05 75.97 91.01 83.77 90.36 81.04
RWG[IAP+22] ✓/✓ 80.66 75.64 90.42 88.40 86.51 81.21 78.61 68.41
SUBG[IAP+22] ✓/✓ 77.57 68.49 91.30 88.26 86.97 85.46 73.64 67.76
GroupDRO[SKH+20] ✓/✓ 80.88 74.80 91.83 90.61 86.51 72.47 81.4 77.7
JTT[LHC+21] ✗/✓ 68.06 60.95 92.01 81.61 85.24 63.61 78.6 72.6
DFR[KIW22] ✗/✓✓ 80.69 70.37 91.93 88.40 90.89 88.96 82.1 74.7
TRAK(ours) ✗/✓ 81.05 75.55 91.08 90.03 91.46 87.15 81.54 75.46
Table 2: Balanced accuracy and worst-group accuracy on CelebA-Age, CelebA-Blond , and
Waterbirds. Adoublecheckmark(✓✓ )indicatesthatthemethodusesvalidationgrouplabelsfor
modelfinetuning,inadditiontohyperparametertuning.
19C.2 ImageNetAccuracies
BelowweincludedthedetailedaccuraciesfortheImageNetexperiment.
Class-Level ImageNet-Level
Class Balanced WorstGroup Overall
Method
(bias) Accuracy Accuracy Accuracy
ERM 46.87 22.62 63.97
RedWolf
D3M 65.63 52.38 63.71
(RedCoat)
AUTO-D3M 59.94 39.29 63.87
ERM 85.10 78.12 63.97
Tench
D3M 90.73 86.88 63.84
(Presenceofhuman)
AUTO-D3M 86.67 80.00 63.97
ERM 77.81 63.64 63.97
Cauliflower
D3M 85.77 79.55 63.70
(NotCooked)
AUTO-D3M 86.73 79.40 63.75
ERM 58.93 35.58 63.97
Strawberry
D3M 70.49 51.92 63.88
(Notonaplate)
AUTO-D3M 68.99 50.48 63.79
Table3: AUTO-D3MidentifiesandmitigatesbiasesinImageNet. ForfourImageNetclasses,abias
wasidentifiedfrominspectingtheTRAKPCAdirections. Then AUTO-D3Misappliedinorderto
mitigatethebiasforthatclass. AUTO-D3M isabletoimprovetheworstgroupaccuracyforthe
targetedclasswithoutsignificantlychangingtheoverallImageNetaccuracy.
20