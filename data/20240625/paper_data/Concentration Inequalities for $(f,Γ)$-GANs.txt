CONCENTRATION INEQUALITIES FOR (f,Γ)-GANS
A PREPRINT
JeremiahBirrell
DepartmentofMathematics
TexasStateUniversity
SanMarcos,TX78666,USA
jbirrell@txstate.edu
June25,2024
ABSTRACT
Generativeadversarialnetworks(GANs)areunsupervisedlearningmethodsfortrainingagenerator
distributiontoproducesamplesthatapproximatethosedrawnfromatargetdistribution.Manysuch
methodscanbeformulatedasminimizationofametricordivergence.Recentworkshaveproventhe
statisticalconsistencyofGANsthatarebasedonintegralprobabilitymetrics(IPMs),e.g.,WGAN
whichisbasedonthe1-Wassersteinmetric. IPMsaredefinedbyoptimizingalinearfunctional(dif-
ferenceofexpectations)overaspaceofdiscriminators. AmuchlargerclassofGANs,whichallow
for the use of nonlinear objective functionals, can be constructed using (f,Γ)-divergences; these
generalizeandinterpolatebetweenIPMsandf-divergences(e.g.,KLorα-divergences). Instances
of(f,Γ)-GANshavebeenshowntoexhibitimprovedperformanceinanumberofapplications. In
thisworkwestudythestatisticalconsistencyof(f,Γ)-GANsforgeneralf andΓ. Specifically,we
derive finite-sample concentration inequalities. These derivations require novel arguments due to
nonlinearityoftheobjectivefunctional. We demonstratethatournewresultsreducetotheknown
resultsforIPM-GANsintheappropriatelimitwhilealsosignificantlyextendingthedomainofap-
plicabilityofthistheory.
Keywords Generativeadversarialnetworks·(f,Γ)-divergence·statisticalconsistency·concentrationinequalities·
errordecomposition
1 Introduction
Generative adversarialnetworks (GANs) are unsupervised learning methods for training a generator distribution to
approximate a target distribution by using samples from the target in a minmax game between a generator and a
discriminatornetworkArjovskyetal.[2017],Dziugaiteetal.[2015],Goodfellowetal.[2014],Lietal.[2015]. Math-
ematically,manysuchmethodscanbeformulatedintermsofminimizinganintegralprobabilitymetric(IPM):
inf d (Q,P ), (1)
Γ θ
θ∈Θ
d (Q,P ):=sup{E [h]−E [h]}. (2)
Γ θ Q Pθ
h∈Γ
Here d is the IPM with test function space Γ (i.e., discriminators), Q is the distribution to be learned (i.e., the
Γ
distribution of the data) and P is the generator distribution, depending on parameters θ ∈ Θ. More specifically,
θ
P = (Φ ) P is the pushforwardof a noise source P undera generatornetwork Φ . For instance, Wasserstein
θ θ # Z Z θ
GANArjovskyetal.[2017]correspondstoΓbeingthesetof1-Lipschitzfunctions. Akeydistinguishingfeatureof
theIPM-GANsisthelinearityinhoftheobjectivefunctionalin(2).
AlargerclassofGANmethodscanbeformulatedbygeneralizingtheminmaxgame(1)touseanonlinearobjective
functional. Such methods can generally be viewed as minimizing a divergence(a generalized notion of “distance"
4202
nuJ
42
]LM.tats[
1v43861.6042:viXraA PREPRINT -JUNE 25,2024
ordiscrepancy)betweenQandP . The(f,Γ)-divergencesarealargeclassofdivergenceswithnonlinearobjectives
θ
that generalize and interpolate between IPMs and f-divergences (e.g., KL-divergence); see Birrelletal. [2022a,b]
for the general theory of these objects. Given a convexfunction f with f(1) = 0 and a test function space Γ, the
(f,Γ)-divergencebetweenprobabilitydistributionsQandP isdefinedby
DΓ(QkP):=sup E [h]−ΛP[h] , (3)
f Q f
h∈Γ
where,denotingtheLegendretransformoff
byf∗,wede(cid:8)
fine
(cid:9)
ΛP[h]:= inf{ν+E [f∗(h−ν)]}. (4)
f P
ν∈R
We refer to ΛP as the generalized cumulant generating function because in the KL-divergence case (f (z) =
f KL
zlog(z)) it is straightforward to show ΛP [h] = logE [eg], which is the classical cumulant generating function.
fKL P
The relation of ΛP to f-divergenceswas previously studied in Ben-Tal&Teboulle [2007], Broniatowski&Keziou
f
[2006],Nguyenetal.[2010],Rudermanetal.[2012].Underappropriateassumptionsonf andΓ,DΓ(QkP)provides
f
ameaningfulnotionofdiscrepancybetweenQandP duetoitsatisfyingthedivergenceproperty,i.e.,DΓ(QkP)≥0
f
withequalityifandonlyifQ=P;seeTheorem2.8inBirrelletal.[2022a]. Thisproperty,alongwiththevariational
characterization(3),motivatesthedefinitionof(f,Γ)-GANs:
inf DΓ(QkP )= inf sup E [h]−ΛP[h] . (5)
f θ Q f
θ∈Θ θ∈Θh∈Γ
(cid:8) (cid:9)
The family of (f,Γ)-GANs encompasses and generalizes a large number of successful methods in the literature,
including Arjovskyetal. [2017], Belghazietal. [2018], Dupuis,Paul&Mao,Yixiang [2022], Glaseretal. [2021],
Goodfellowetal. [2014], Grettonetal. [2012], Gulrajanietal. [2017], Miyatoetal. [2018], Nguyenetal. [2010],
Nowozinetal.[2016],Song&Ermon[2020];seeTable2inBirrelletal.[2022a].
The statistical consistency theory of IPM-GANs has recently been studied in a number of works
Chakraborty&Bartlett [2024], Chenetal. [2023b], Huangetal. [2022], Liang [2021]. These derivations take ad-
vantage of the special structure of IPMs, namely the linearity of the objective functional. However, the statistical
consistencytheoryfor moregeneralGANs with nonlinearobjectivesis lacking. In this workwe developnew tools
thatallow usto provestatistical consistencyofthe(f,Γ)-GANs, intheformoffinite-sampleconcentrationinequal-
ities. The key technicalhurdleis the nonlinearityof the objectivefunctionaldue to the presenceof the generalized
cumulant generating function. Section 2 is dedicated to deriving the properties of and bounds on ΛP that will be
f
neededtoprovethestatisticalconsistencyof(f,Γ)-GANs(5)inSection3.
1.1 SummaryofResults
Keytoourresultsisadecompositionofthe(f,Γ)-GANerrorintooptimizationerror,approximationerror,andvarious
statisticalerrors.GivenanapproximatediscriminatorspaceΓ⊂Γ(e.g.,aneuralnetworkwithspectralnormalization
as an approximation to the space of 1-Lipschitz functions), and empirical measures Q and P := (Φ ) P
n θ,m θ # Z,m
constructedusingi.i.d. samplesfromQandP
Z
respectivelye,letθ n∗
,m
beasolution(withoptimizationerrortolerance
ǫn,m ≥0)totheempirical(f,Γ)-GANproblem:
opt
e D fΓe (Q nkP θ n∗ ,m,m)≤ θi ∈n Θf D fΓe (Q nkP θ,m)+ǫn op,m t . (6)
Giventhis,wederivethefollowing(f,Γ)-GANerrorboundinLemma3.5:
D fΓ(QkP θ n∗ ,m)− θi ∈n Θf D fΓ(QkP θ) (7)
≤ sup ΛPZ,m[h◦Φ ]−ΛPZ[h◦Φ ]) + sup ΛPZ[h◦Φ ]−ΛPZ,m[h◦Φ ]
f θ f θ f θ f θ
e e
h∈Γ,θ∈Θ h∈Γ,θ∈Θ
n o n o
statisticalerrorfromsamplingPZ
+sup{E [h]−E [h]}+sup{E [h]−E [h]}
|
e
Q Qn
e
Qn {z Q }
h∈Γ h∈Γ
statisticalerrorfromsamplingQ
+| ǫo np ,mt + 1+(f∗ {)z′ +(z 0+β−α) s hu ∈p Γh˜in ∈f Γe}kh−h˜k
∞
.
optimizationerror (cid:0) (cid:1)
discriminatorapproximationerror
|{z}
| {z }
2A PREPRINT -JUNE 25,2024
ThedistributionP θ∗ =(Φ θ∗ ) #P Z isthe(f,Γ)-GANestimator,i.e.,theapproximationtoQobtainedbysolving
n,m n,m
theempiricalGANproblem(6). Theprimarydifferencebetweenthedecomposition(7)andthecorrespondingresult
forIPMsinLemma9ofHuangetal.[2022]isthepresenceofthegeneralizedcumulantgeneratingfunctionΛPZ in
f
thestatisticalerrorfromsamplingP .ThefunctionalΛPZ isnonlinearinthediscriminatorandsotreatingthoseterms
Z f
requiresnewtechniqueswhichwedevelopinSection2. Usingthesenewresults,inthecaseofadiscriminatorspace
Γsuchthatsup |h(x)−h(x˜)| ≤ 1forallh ∈ Γ, wederiveconcentrationinequalitiesofthefollowingform;see
x,x˜
Theorem3.10fordetails:
P (cid:18)D fΓ(QkP θ n∗ ,m)− θi ∈n Θf D fΓ(QkP θ)≥ǫ+ǫ aΓ p,Γe prox+ǫn op,m t +4R Γe ,Q,n+4K f,Γe ◦Φ,PZ,m (cid:19)≤exp − n2 + mǫ 22 ∆2 f,m! .
(8)
e
Here ǫ aΓ p,Γ prox is the discriminator approximation error, R Γe ,Q,n denotes the Rademacher complexity of Γ (see Ap-
pendix C), K f,Γe ◦Φ,PZ,m depends on R Γe ◦Φ,PZ,m, and ∆ f,m can be bounded uniformly in m. In particular, if the
discriminator and optimization errors are zero and assuming that one chooses discriminator and generaetor classes
whoseRademachercomplexitiesdecayasn,m→∞,thisresultshowsthatthe(f,Γ)-GANestimatorP θ∗ approx-
n,m
imately solves the exact(f,Γ)-GAN problem(5) with high probability. If the generatorclass is sufficiently rich to
ensureinf DΓ(QkP )=0then(8)impliesthe(f,Γ)-GANestimatorisclosetothedistributionofthedatasource,
θ∈Θ f θ
Q,withhighprobability,whereclosenessismeasuredbythe(f,Γ)-divergence;inthislatterscenarioweinfactobtain
a tighter result than (8), see (85). The exponentiallydecaying nature of the bounds(8) means that one can use the
Borel-Cantellilemmatoalsoconcludealmost-sureconvergenceofD fΓ(QkP θ n∗ ,m)toinf θ∈ΘD fΓ(QkP θ),providedthat
theerrorsandRademachercomplexitiesapproachzerowhenn,m ≥ k → ∞. Aspartofthesederivationsweprove
bounds on the mean of the error; see Remark 3.13. Due to the asymmetry of DΓ we also provide a concentration
f
inequalitywherethepositionsofQandP θ∗ arereversed;seeTheorem3.11. Inaddition,ourtechniquescanbeused
n,m
toderiveconcentrationinequalitiesforthe(f,Γ)-divergenceestimationproblem;seeTheorem3.14. Finally,wenote
thatthequantitiesK f,Γe ◦Φ,PZ,m and∆ f,m approachtheirIPMcounterpartsinthelimitwheref∗ becomeslinear;see
Remarks2.8and2.11.
Theconcentrationinequality(8)andotherrelatedresultspresentedbelowreducetheproblemofstatisticalperformance
guaranteesfor (f,Γ)-GANs to the problem of boundingthe Rademacher complexities of the discriminator and the
compositionofdiscriminatorandgeneratorfunctionclasses. Thisisawell-studiedproblemwhichalsoarisesthecase
ofIPM-GANsandthesametechniquesandresultsfromthatsettingcanbeappliedhere. We thereforedonotfocus
onthisaspectoftheprobleminthepresentwork,insteadreferringthereadertoChapter5inWainwright[2019]for
background,includingtheuseofDudley’sentropyintegralandcoveringnumberbounds;seealsoLemma12andits
proof in Huangetal. [2022]. The discussion in Section 3.1 of Huangetal. [2022] and also Chakraborty&Bartlett
[2024] showshow coveringnumberboundsimplythat, underappropriateassumptions, the IPM-GANperformance
dependson the intrinsic dimension, d∗, of the supportof Q and not the (possibly much larger)ambient dimension,
d. AsournewboundsdependontheRademachercomplexityinthesamequalitativemannerastheIPMresults,the
aforementionedinsightsregardingdimensiondependencealsoimmediatelyapplyto(f,Γ)-GANs.
2 Properties ofΛP
f
The fundamental difference between the more commonly studied IPM-GANs (1) and the (f,Γ)-GANs (5) is the
nonlinearity of the objective functional in (3), coming from the generalized cumulant generating function (4). A
detailedstudyofthepropertiesofthegeneralizedcumulantgeneratingfunction,ΛP,isthereforerequiredinorderto
f
extendthetechniquesusedforIPM-GANstoobtainstatisticalguaranteesfor(f,Γ)-GANs. Weundertakethatstudy
inthissection.
Goingforward,weletX beameasurablespaceandM (X)bethespaceofboundedmeasurablereal-valuedfunctions
b
on X. We letf : (a,b) → R, 0 ≤ a < 1 < b ≤ ∞, bea convexfunctionthat satisfies f(1) = 0; we denotethe
setofsuchfunctionsbyF (a,b). Wewillmakeregularuseofvariouspropertiesofsuchconvexfunctionsandtheir
1
LegendretransformsthatarecollectedinAppendixA.Inparticular,wenotethata ≥0impliesf∗ isnon-decreasing.
The value of the rightderivativeof f at1 will playa key role in manyof the proofsand so we make the following
definition:
z :=f′ (1). (9)
0 +
Byassumption,1isintheinteriorofthesetwheref isfiniteandsothisrightderivativeisguaranteedtoexistandbe
finite.
3A PREPRINT -JUNE 25,2024
The first key property we prove is that bounds on a function h translate to bounds on ΛP[h] and also allow the
f
minimizationoverν in(4)toberestrictedtoacorrespondingcompactinterval.
Lemma 2.1. Let f ∈ F (a,b) with a ≥ 0, P be a probability measure on X, and h ∈ M (X) with α ≤ h ≤ β.
1 b
Then:
1. α≤ΛP[h]≤β
f
2. Iff isalsostrictlyconvexonaneighborhoodof1andz ∈{f∗ <∞}othen
0
ΛP[h]= inf {ν+E [f∗(h−ν)]}. (10)
f P
ν∈[α−z0,β−z0]
Remark2.2. WeuseAo todenotetheinteriorofasetA.
Proof. 1. f∗ isnon-decreasing,therefore
inf{ν+f∗(α−ν)}≤ inf{ν+E [f∗(h−ν)]}≤ inf{ν+f∗(β−ν)} (11)
P
ν∈R ν∈R ν∈R
forallν. Foranyc∈Rwehave
inf{ν+f∗(c−ν)}=inf{c−z+f∗(c−(c−z))} (12)
ν∈R z
=c−sup{z−f∗(z)}=c−(f∗)∗(1)=c−f(1)=c.
z∈R
Thisprovestheclaim.
2. Forthefollowingitwillbeusefultorecallthatf∗(z ) = z and(f∗)′ (z ) = 1;seeLemmaA.1. Suppose
0 0 + 0
ν >β−z andfirstconsiderthecasewheref∗(h−ν)<∞. Wehaveh−ν <h−(β−z )≤z ,therefore
0 0 0
h−ν +1/n,h−(β−z ) ∈ {f∗ < ∞}o fornsufficientlylarge. f∗ isabsolutelycontinuousoncompact
0
intervals, therefore the fundamentaltheorem of calculus holds (see, e.g., Theorem 3.35 and exercise 42 in
Folland[2013])andwehave
h−(β−z0)
f∗(h−(β−z ))=f∗(h−ν+1/n)+ (f∗)′ (z)dz. (13)
0 +
Zh−ν+1/n
Forz ≤h−(β−z )wehavez ≤z ,hence(f∗)′ (z)≤(f∗)′ (z )=1. Therefore
0 0 + + 0
f∗(h−(β−z ))≤f∗(h−ν+1/n)+h−(β−z )−(h−ν+1/n) (14)
0 0
=f∗(h−ν+1/n)−(β−z )+ν−1/n.
0
Usingthecontinuityoff∗on{f∗ <∞}wecantaken→∞toget
ν+f∗(h−ν)≥(β−z )+f∗(h−(β−z )). (15)
0 0
Thiswasprovenunderthe assumptionthatf∗(h−ν) < ∞, butitalso triviallyholdswhenthisisinfinite.
Takingtheexpectationofbothsideswethereforefind
ν+E [f∗(h−ν)]≥(β−z )+E [f∗(h−(β−z ))]≥ inf {ν+E [f∗(h−ν)]} (16)
P 0 P 0 P
ν∈[α−z0,β−z0]
forallν >β−z .
0
Nowsupposeν <α−z andfirstconsiderthecasewheref∗(h−ν)<∞.Wehaveh−ν >h−(α−z )≥z .
0 0 0
Thereforeh−ν−1/n,h−(α−z )∈{f∗ <∞}ofornsufficientlylargeand
0
h−ν−1/n
f∗(h−ν−1/n)=f∗(h−(α−z ))+ (f∗)′ (z)dz. (17)
0 +
Zh−(α−z0)
Forz ≥h−(α−z )wehavez ≥z andso(f∗)′ (z)≥1. Hence
0 0 +
f∗(h−ν−1/n)≥f∗(h−(α−z ))+h−ν−1/n−(h−(α−z )) (18)
0 0
=f∗(h−(α−z ))−ν−1/n+(α−z ).
0 0
Takingn→∞gives
ν+f∗(h−ν)≥(α−z )+f∗(h−(α−z )). (19)
0 0
4A PREPRINT -JUNE 25,2024
Thiswasprovenunderthe assumptionthatf∗(h−ν) < ∞, butitalso triviallyholdswhenthisisinfinite.
Takingtheexpectationofbothsideswethereforefind
ν+E [f∗(h−ν)]≥(α−z )+E [f∗(h−(α−z ))]≥ inf {ν+E [f∗(h−ν)]} (20)
P 0 P 0 P
ν∈[α−z0,β−z0]
forallν <α−z . Thiscompletestheproof.
0
NextwederiveaLipschitzboundforΛP.
f
Lemma2.3. LetP beaprobabilitymeasureonX, andh,˜h ∈ M (X)withα ≤ h,h˜ ≤ β. Letf ∈ F (a,b)with
b 1
a≥0andassumef isstrictlyconvexinaneighborhoodof1andz +β−α∈{f∗ <∞}o. Then
0
|ΛP f[h˜]−ΛP f[h]|≤(f∗)′ +(z 0+β−α)kh˜−hk L1(P). (21)
Proof. f∗ is non-decreasing, hence z +β −α ∈ {f∗ < ∞}o implies z ∈ {f∗ < ∞}o. Therefore we can use
0 0
Lemma2.1towrite
ΛP[h]= inf {ν+E [f∗(h−ν)]}. (22)
f P
ν∈[α−z0,β−z0]
Thefactthatf∗isnon-decreasingalsoimpliesthath−z ≤f∗(h−ν)≤f∗(β−α+z )<∞forallν ∈[α−z ,β−z ]
0 0 0 0
andhenceE [f∗(h−ν)]isfinite. Thesameholdsforh˜,therefore
P
|ΛP[h˜]−ΛP[h]|≤ sup E [|f∗(h˜−ν)−f∗(h−ν)|]. (23)
f f P
ν∈[α−z0,β−z0]
Herewemadeuseofthesimplebound
±(infd −infc )≤sup{±(d −c )} (24)
i i i i
i∈I i∈I i∈I
wheneverc ,d ∈Rforalli∈I andinf c ,inf d arefinite.
i i i i i i
f∗ isabsolutelycontinuousontheintervalbetweenh−ν andh˜ −ν,thereforewecanusethefundamentaltheorem
ofcalculustowrite
h˜−ν
f∗(h˜−ν)−f∗(h−ν)= (f∗)′ (z)dz (25)
+
Zh−ν
andso
|f∗(h˜−ν)−f∗(h−ν)|≤|h˜−h| sup |(f∗)′ (z)|. (26)
+
z∈[z0−(β−α),z0+β−α]
The fact that f∗ is non-decreasingimplies (f∗)′ ≥ 0 on {f∗ < ∞}o. Combined with the fact that (f∗)′ is non-
+ +
decreasing,wecanthereforeconclude
|f∗(h˜−ν)−f∗(h−ν)|≤|h˜−h|(f∗)′ (z +β−α). (27)
+ 0
Takingtheexpectationofbothsidesandcombiningtheresultwith(23)provestheclaim.
e
NextwederiveatightboundonthedifferencebetweenΛPn[h]andΛPn[h]whenP andP areempiricalmeasures
f f n n
thatdifferbyonlyasinglepoint.Forthispurposeitwillbeconvenienttomakethefollowingdefinition.
Definition2.4. Letf ∈F (a,b)witha≥0. DefineΛ :Rn →Rby e
1 f
n
1
Λ (x):= inf{ν+ f∗(x −ν)}. (28)
f i
ν∈R n
i=1
X
TheconnectiontoΛP isgivenbythefollowinglemma,whichisasimpleconsequenceoftherespectivedefinitions.
f
Lemma2.5. Letf ∈ F (a,b)witha ≥ 0. Leth ∈ M (X)andP = 1 n δ beanempiricalmeasureonX.
1 b n n i=1 xi
Then
P
ΛPn[h]=Λ ◦h (x) (29)
f f n
whereh (x):=(h(x ),...,h(x )).
n 1 n
5A PREPRINT -JUNE 25,2024
TheLipschitzboundinLemma2.3translatesintothefollowingLipschitzboundforΛ .
f
Corollary2.6. Letf ∈ F (a,b)witha ≥ 0. Letα ≤ β andassumef isstrictlyconvexinaneighborhoodof1and
1
z +β−α∈{f∗ <∞}o. Forx,x˜∈[α,β]nwehave
0
n
1
|Λ (x˜)−Λ (x)|≤ (f∗)′ (z +β−α) |x˜ −x |. (30)
f f n + 0 i i
i=1
X
Inparticular,Λ iscontinuouson[α,β]n.
f
InthefollowinglemmawederiveatightperturbationboundsonΛ whentheinputsdifferinonlyasinglecomponent,
f
e
whichinturngivesthedesiredtightboundonΛPn[h]−ΛPn[h]whenP andP areempiricalmeasuresthatdifferby
f f n n
onlyasinglepointviaLemma2.5.
Lemma 2.7. Letf ∈ F (a,b)with a ≥ 0. Let α ≤ β andassume f isstricetly convexina neighborhoodof1and
1
z +β−α∈{f∗ <∞}o. Givenj ∈{1,...,n},letE ={(x,x˜)∈[α,β]n×[α,β]n :x =x˜ fori6=j}. Then
0 i i
n−1 1
sup {Λ (x˜)−Λ (x)}= inf −z+ f∗(z)+ f∗(β−α+z) (31)
f f
(x,x˜)∈E z∈[z0−(β−α),z0] (cid:26) n n (cid:27)
andwehavethesimplerloosebounds
β−α n−1 1
≤ inf −z+ f∗(z)+ f∗(β−α+z) (32)
n z∈[z0−(β−α),z0]
(cid:26)
n n
(cid:27)
1 β−α
≤ (f∗(β−α+z )−z )≤(f∗)′ (β−α+z ) .
n 0 0 + 0 n
Remark 2.8. Note that (32) implies that the bound (31) continuouslyapproachesthe result in the linear case (i.e.,
whereΛP =E ,whichcorrespondstof∗(z)=z onasufficientlylargeinterval)as(f∗)′ (β−α+z )approaches
f P + 0
1.
Proof. Firstletx =αforalli,x˜ =αfori6=j andx˜ =β. Wehave(x,x˜)∈E andtherefore
i i j
sup {Λ (x˜)−Λ (x)} (33)
f f
(x,x˜)∈E
n n
1 1
≥ inf ν+ f∗(x˜ −ν) − inf ν+ f∗(x −ν)
i i
ν∈[α−z0,β−z0]( n ) ν∈R( n )
i=1 i=1
X X
n−1 1
= inf ν−α+ f∗(α−ν)+ f∗(β−ν) +sup{α−ν−f∗(α−ν)}
ν∈[α−z0,β−z0] (cid:26) n n (cid:27) ν∈R
n−1 1
= inf −z+ f∗(z)+ f∗(β−α+z) +(f∗)∗(1)
z∈[z0−(β−α),z0]
(cid:26)
n n
(cid:27)
n−1 1
= inf −z+ f∗(z)+ f∗(β−α+z) .
z∈[z0−(β−α),z0]
(cid:26)
n n
(cid:27)
Toprovethereverseinequality,let(x,x˜)∈E bearbitraryandcompute
Λ (x˜)−Λ (x) (34)
f f
n n
1 1
= inf ν˜+ f∗(x˜ −ν˜) − inf ν+ f∗(x −ν)}
i i
ν˜∈[α−z0,β−z0]( n i=1 ) ν∈[α−z0,β−z0] (cid:26) n i=1
X X
n n
1 1
= sup inf ν˜−ν+ f∗(x −ν˜)− f∗(x −ν)
i i
ν∈[α−z0,β−z0]ν˜∈[α−z0,β−z0]
(cid:26)
n
i=1,i6=j
n
i=1,i6=j
X X
1 1
+ f∗(x˜ −ν˜)− f∗(x −ν)
j j
n n
(cid:27)
n n
1 1
≤ sup inf ν˜−ν+ f∗(x −ν˜)− f∗(x −ν)
i i
ν∈[α−z0,β−z0]ν˜∈[α−z0,β−z0]
(cid:26)
n
i=1,i6=j
n
i=1,i6=j
X X
1 1
+ f∗(β−ν˜)− f∗(α−ν) ,
n n
(cid:27)
6A PREPRINT -JUNE 25,2024
whereinthelastlineweusedthefactthatf∗ isnon-decreasing. Wehavex˜ −ν˜,x −ν ∈{f∗ < ∞}o andforeach
i i
i6=j thetermsinvolvingx areabsolutelycontinuousoncompactintervalswithrightderivative
i
d 1 1 1
f∗(x −ν˜)− f∗(x −ν) = (f∗)′ (x −ν˜)−(f∗)′ (x −ν) , (35)
dx+ n i n i n + i + i
i (cid:18) (cid:19)
(cid:0) (cid:1)
whichis non-positivewhenν˜ ≥ ν and non-negativewhenν˜ ≤ ν (dueto (f∗)′ beingnon-decreasing). Thefunda-
+
mentaltheoremofcalculusthenimpliesthat 1f∗(x −ν˜)− 1f∗(x −ν)is non-increasingin x whenν˜ ≥ ν and
n i n i i
non-decreasinginx whenν˜≤ν. Therefore
i
Λ (x˜)−Λ (x) (36)
f f
ν˜−ν+f∗(β−ν˜)− n−1f∗(β−ν)− 1f∗(α−ν) ifν˜<ν
≤ sup inf n n
ν∈[α−z0,β−z0]ν˜∈[α−z0,β−z0] (cid:26)ν˜−ν+ n− n1f∗(α−ν˜)−f∗(α−ν)+ n1f∗(β−ν˜) ifν˜≥ν
(cid:27)
α−z−ν+f∗(β−α+z)− n−1f∗(β−ν)− 1f∗(α−ν) ifα−ν <z
= sup inf n n ,
ν∈[α−z0,β−z0]z∈[z0−(β−α),z0] (cid:26)α−z−ν+ n− n1f∗(z)−f∗(α−ν)+ n1f∗(β−α+z) ifα−ν ≥z
(cid:27)
wherewechangedvariablestoz =α−ν˜inthelastline. Forz ∈(α−ν,z ],therightderivativeoftheobjectivewith
0
respecttozisgivenby
d n−1 1
α−z−ν+f∗(β−α+z)− f∗(β−ν)− f∗(α−ν)) =−1+(f∗)′ (β−α+z). (37)
dz+ n n +
(cid:18) (cid:19)
(f∗)′ isnon-decreasingandβ−α+z ≥z ,therefore−1+(f∗)′ (β−α+z)≥−1+(f∗)′ (z )=0. Thisimplies
+ 0 + + 0
thattheobjectiveisnon-decreasinginz ∈[α−ν,z ](theextensiontotheendpointα−ν followsfromcontinuityof
0
theobjective).Therefore
Λ (x˜)−Λ (x) (38)
f f
n−1 1
≤ sup inf α−z−ν+ f∗(z)−f∗(α−ν)+ f∗(β−α+z)
ν∈[α−z0,β−z0]z∈[z0−(β−α),α−ν]
(cid:26)
n n
(cid:27)
n−1 1
= sup −ν−f∗(α−ν)+ inf α−z+ f∗(z)+ f∗(β−α+z) .
ν∈[α−z0,β−z0](cid:26) z∈[z0−(β−α),α−ν] (cid:26) n n (cid:27)(cid:27)
Letz beaminimizerof
∗
n−1 1
inf α−z+ f∗(z)+ f∗(β−α+z) , (39)
z∈[z0−(β−α),z0]
(cid:26)
n n
(cid:27)
whichexistsduetocompactnessofthedomainandcontinuityoftheobjective.Theobjectiveisconvexinz,therefore
itisnon-increasingon(−∞,z ]andso
∗
Λ (x˜)−Λ (x) (40)
f f
α−z + n−1f∗(z )+ 1f∗(β−α+z ) ifα−ν ≥z
≤ sup −ν−f∗(α−ν)+ ∗ n ∗ n ∗ ∗
ν+ n−1f∗(α−ν)+ 1f∗(β−ν) ifα−ν <z
ν∈[α−z0,β−z0](cid:26) (cid:26) n n ∗ (cid:27)
w−f∗(w)−z + n−1f∗(z )+ 1f∗(β−α+z ) ifw≥z
= sup ∗ n ∗ n ∗ ∗ ,
1(f∗(β−α+w)−f∗(w)) ifw<z
w∈[z0−(β−α),z0](cid:26)n ∗ (cid:27)
wherewechangedvariablestow = α−ν. We havesup {w−f∗(w)} = (f∗)∗(1) = 0andw 7→ f∗(β −α+
w∈R
w)−f∗(w)isnon-decreasingonw ≤z (thisfollowsfrom(f∗)′ (β−α+w)−(f∗)′ (w)≥0),therefore
0 + +
n−1 1 1
Λ (x˜)−Λ (x)≤max −z + f∗(z )+ f∗(β−α+z ), (f∗(β−α+z )−f∗(z )) . (41)
f f ∗ ∗ ∗ ∗ ∗
n n n
(cid:26) (cid:27)
Usingtheboundf∗(z)≥zwecancompute
n−1 1 1
−z + f∗(z )+ f∗(β−α+z )− (f∗(β−α+z )−f∗(z ))
∗ ∗ ∗ ∗ ∗
n n n
=−z +f∗(z )≥0.
∗ ∗
7A PREPRINT -JUNE 25,2024
Hence
n−1 1
Λ (x˜)−Λ (x)≤−z + f∗(z )+ f∗(β−α+z ) (42)
f f ∗ ∗ ∗
n n
n−1 1
= inf −z+ f∗(z)+ f∗(β−α+z) .
z∈[z0−(β−α),z0]
(cid:26)
n n
(cid:27)
Thisholdsforall(x,x˜)∈Eandsowhencombinedwith(33)weobtaintheclaimedequality.
Toderivethelooserbounds,firstusef∗(z)≥ztocomputethelowerbound
n−1 1 β−α
inf −z+ f∗(z)+ f∗(β−α+z) ≥ . (43)
z∈[z0−(β−α),z0]
(cid:26)
n n
(cid:27)
n
Fortheupperboundwecompute
n−1 1
inf −z+ f∗(z)+ f∗(β−α+z) (44)
z∈[z0−(β−α),z0]
(cid:26)
n n
(cid:27)
n−1 1 1
≤−z + f∗(z )+ f∗(β−α+z )= (f∗(β−α+z )−z )
0 0 0 0 0
n n n
β−α
≤(f∗)′ (β−α+z ) ,
+ 0 n
wheretoobtainthelastlineweusedthefundamentaltheoremofcalculustogetherwiththefactsthatf∗(z )=z and
0 0
(f∗)′ isnon-decreasing.
+
Nextwederiveapairofuniformlawoflargenumbers(ULLN)resultsthatboundthemaximumdifferencebetween
ΛPn[h] and ΛP[h] over a set of test function h ∈ Γ in terms of the Rademacher complexity of Γ, where P is
f f n
the empirical measure for n i.i.d. samples from P. These results should be compared with the ULLN for means
whichprovidesuniformboundsonthe differencebetweenthe empiricalmeanandexpectation; see AppendixC for
the relevantbackground. First we obtain a simpler result that uses (10) along with a Lipschitz boundto reduce the
problem to the ULLN for means; this can be thought of as a substantially more general version of the estimate in
the proof of Lemma 2 in Belghazietal. [2018], which studied a KL-divergencebased mutual information method.
However, as we will show, the resulting estimate is overly pessimistic as it does not reduce to the IPM case in the
appropriatelimit. Followingthissimplerlemmawewillderiveatighterboundthatdoespossessthedesiredlimiting
property.
Lemma2.9. LetΨbeanonemptycountablecollectionofmeasurablefunctionsonY andsupposewehaveα,β ∈R
suchthatα ≤ ψ ≤ β forallψ ∈ Ψ. Letf ∈ F (a,b)witha ≥ 0andassumef isstrictlyconvexinaneighborhood
1
of1andz +β−α∈{f∗ <∞}o.
0
LetP beaprobabilitymeasureonY andY ,i = 1,...,nbeindependentsamplesfromP withP thecorresponding
i n
empiricalmeasure.Then
β−α
E sup ± ΛP[ψ]−ΛPn[ψ] ≤2(f∗)′ (β−α+z ) R + . (45)
"ψ∈Ψ n (cid:16)
f f
(cid:17)o#
+ 0
(cid:18)
Ψ,P,n 2n1/2
(cid:19)
Proof. Theresultistrivialifα=βsosupposeα<β. Notethatf∗isnon-decreasing,andhencez +β−α∈{f∗ <
0
∞}oimpliesz ∈{f∗ <∞}o. UsingLemma2.1alongwith(24)weobtain
0
± ΛP[ψ]−ΛPn[ψ] ≤ sup {±(E [f∗(ψ−ν)]−E [f∗(ψ−ν)])}. (46)
f f P Pn
ν∈[α−z0,β−z0]
(cid:16) (cid:17)
Notethatz −(β−α)≤f∗(ψ−ν)≤f∗(β−α+z )<∞andsotheexpectationsarefinite. Also,thesupremum
0 0
overν canberestrictedtorationalν duetocontinuity.Maximizingoverψandtakingexpectationsgives
E sup ± ΛP[ψ]−ΛPn[ψ] ≤E sup {±(E [f∗(ψ−ν)]−E [f∗(ψ−ν)])} . (47)
f f P Pn
"ψ∈Ψ # "ψ∈Ψ,ν∈[α−z0,β−z0]∩Q #
n (cid:16) (cid:17)o
TheULLN(seeTheoremC.4)implies
E sup {±(E [f∗(ψ−ν)]−E [f∗(ψ−ν))]} (48)
P Pn
"ψ∈Ψ,ν∈[α−z0,β−z0]∩Q #
n
1
=E sup ± E [h]− h(Y ) ≤2R ,
P i H,P,n
n
"h∈H( !)#
i=1
X
8A PREPRINT -JUNE 25,2024
where H := {f∗(ψ − ν) : ψ ∈ Ψ,ν ∈ [α − z ,β − z ] ∩ Q}. f∗ and (f∗)′ are decreasing, therefore f∗ is
0 0
(f∗)′ (β−α+z )-Lipschitzon(−∞,β−α+z ]andsoTalagrand’slemma(seeLemmaC.3)implies
+ 0 0
R ≤(f∗)′ (β−α+z )R . (49)
H,P,n + 0 Ψ−[α−z0,β−z0]∩Q,P,n
Wecancompute
R =R +R (50)
Ψ−[α−z0,β−z0]∩Q,P,n Ψ,P,n [α−z0,β−z0],P,n
and
n n
1 1
R = E sup ν σ = E sup (z+(β+α)/2−z ) σ
[α−z0,β−z0],P,n
n
σ i
n
σ 0 i
"ν∈[α−z0,β−z0]
i=1
# "z∈[−(β−α)/2,(β−α)/2]
i=1
#
X X
(51)
n n n 2 1/2
1 β−α β−α
= E sup z σ = E σ ≤ E σ
σ i σ i σ i
n "z∈[−(β−α)/2,(β−α)/2]
Xi=1
# 2n "(cid:12)
(cid:12)Xi=1
(cid:12) (cid:12)# 2n (cid:12)
(cid:12)Xi=1
(cid:12)
(cid:12)

(cid:12) (cid:12) (cid:12) (cid:12)
β−α (cid:12) (cid:12) (cid:12) (cid:12) 
= . (cid:12) (cid:12) (cid:12) (cid:12)
2n1/2
Wewillnowderivethemorepreciseboundthatbehavesappropriatelywhenf∗ becomeslinear.
Lemma 2.10. Let Ψ be a countable collection of measurable functions on Y that contains at least one constant
functionandsupposewehaveα,β ∈Rsuchthatα≤ψ ≤βforallψ ∈Ψ. Letf ∈F (a,b)witha≥0andassume
1
f isstrictlyconvexinaneighborhoodof1andz +β−α∈{f∗ <∞}o. Alsoassumethat(f∗)′ isL -Lipschitz
0 + α,β
on[z −(β−α),z +β−α].
0 0
LetP beaprobabilitymeasureonY andY ,i = 1,...,nbeindependentsamplesfromP withP thecorresponding
i n
empiricalmeasure.Then
E sup ± ΛP[ψ]−ΛPn[ψ] (52)
f f
"ψ∈Ψ #
n (cid:16) (cid:17)o
(β−α)2L β−α
≤2min (1+2(β−α)L )R + α,β ,(f∗)′ (β−α+z ) R + .
α,β Ψ,P,n 2n1/2 + 0 Ψ,P,n 2n1/2
(cid:26) (cid:18) (cid:19)(cid:27)
Remark2.11. TheresultinLemma2.9doesnotreducetotheULLNformeansinthecasewhere f∗(z) = z (ona
sufficientlylarge interval), i.e., when ΛP[h] = E [h]; specificallythe bound (45) differsfrom the ULLN for means
f P
(116)bytheterm β−α inthatcase. However,(52)doesreducetotheULLNformeanswhenf∗(z) = z. Moreover,
m1/2
the bound (52) approachesthe bound in (116) as the Lipschitz constant for (f∗)′ on [z −(β −α),z +β −α]
+ 0 0
approacheszero.
Proof. AsintheproofofLemma2.9,theresultistrivialwhenα=β. Forα<β wehave
E sup ± ΛP[ψ]−ΛPn[ψ] ≤E sup {±(E [f∗(ψ−ν)]−E [f∗(ψ−ν)])} . (53)
f f P Pn
"ψ∈Ψ # "ψ∈Ψ,ν∈[α−z0,β−z0]∩Q #
n (cid:16) (cid:17)o
Nowusethefundamentaltheoremofcalculustocompute
1
f∗(ψ−ν)=f∗(ψ−(β−z ))+(β−z −ν) (f∗)′ (ψ−(1−t)(β−z )−tν)dt (54)
0 0 + 0
Z0
9A PREPRINT -JUNE 25,2024
forψ ∈Ψ,ν ∈[α−z ,β−z ]∩Q. Therefore
0 0
sup {±(E [f∗(ψ−ν)]−E [f∗(ψ−ν)])} (55)
P Pn
ψ∈Ψ,ν∈[α−z0,β−z0]∩Q
= sup ±(E [f∗(ψ−(β−z ))]−E [f∗(ψ−(β−z ))]
P 0 Pn 0
ψ∈Ψ,ν∈[α−z0,β−z0]∩Q(cid:26)
1
+(β−z −ν) E [(f∗)′ (ψ−(1−t)(β−z )−tν)]−E [(f∗)′ (ψ−(1−t)(β−z −tν)]dt
0 P + 0 Pn + 0
Z0 (cid:27)
≤sup{±(E [f∗(ψ−(β−z ))]−E [f∗(ψ−(β−z ))]}
P 0 Pn 0
ψ∈Ψ
1
+ sup (β−z −ν) E [(f∗)′ (ψ−(1−t)(β−z )−tν)]
0 P + 0
ψ∈Ψ,ν∈[α−z0,β−z0]∩Q(cid:26) Z0 (cid:18)
−E [(f∗)′ (ψ−(1−t)(β−z −tν)] dt
Pn + 0
(cid:19) (cid:27)
≤ sup {±(E [h]−E [h]} (56)
P Pn
h∈f∗◦(Ψ−(β−z0))
1
+ sup (β−z −ν) E [(f∗)′ (ψ−(1−t)(β−z )−tν)]
0 P + 0
ψ∈Ψ,ν∈[α−z0,β−z0]∩Q(cid:26) Z0 (cid:18)
−E [(f∗)′ (ψ−(1−t)(β−z −tν)] dt .
Pn + 0
(cid:19) (cid:27)
TheULLN(TheoremC.4)implies
E sup {±(E P[h]−E Pn[h])} ≤2R f∗◦(Ψ−(β−z0)),P,n. (57)
"h∈f∗◦(Ψ−(β−z0)) #
We haveψ−(β −z ) ≤ z andf∗ is(f∗)′ (z )-Lipschitzon (−∞,z ] where(f∗)′ (z ) = 1, henceTalagrand’s
0 0 + 0 0 + 0
lemma(LemmaC.3)implies
R f∗◦(Ψ−(β−z0)),P,n ≤R Ψ−(β−z0),P,n =R Ψ,P,n. (58)
We note that the shift invariance of the Rademacher complexity follows from our use of the definition (113), i.e.,
withoutabsolutevalue.
Asforthesecondtermin(56),wehave
1
E sup (β−z −ν) E [(f∗)′ (ψ−(1−t)(β−z )−tν)] (59)
0 P + 0
(cid:20)ψ∈Ψ,ν∈[α−z0,β−z0]∩Q(cid:26) Z0 (cid:18)
−E [(f∗)′ (ψ−(1−t)(β−z −tν)] dt
Pn + 0
(cid:19) (cid:27)(cid:21)
1
≤(β−α) E sup E [(f∗)′ (ψ−(1−t)(β−z )−tν)]
P + 0
Z0 (cid:20)ψ∈Ψ,ν∈[α−z0,β−z0]∩Q(cid:26)
(cid:12)
(cid:12)
−E [(f∗)′ (ψ−(1−t)(β−z −tν)] dt.
Pn + 0
(cid:27)(cid:21)
UsingthefactthatΨcontainsaconstant(whichimpliesthatthetermsinthefollowingboundarenon(cid:12)-negative)and
(cid:12)
thenemployingtheULLN(TheoremC.4)wehave
E sup |E [(f∗)′ (ψ−(1−t)(β−z )−tν)]−E [(f∗)′ (ψ−(1−t)(β−z )−tν)]| (60)
P + 0 Pn + 0
"ψ∈Ψ,ν∈[α−z0,β−z0]∩Q #
≤E sup {E [(f∗)′ (ψ−(1−t)(β−z )−tν)]−E [(f∗)′ (ψ−(1−t)(β−z )−tν)]}
P + 0 Pn + 0
"ψ∈Ψ,ν∈[α−z0,β−z0]∩Q #
+E sup {−(E [(f∗)′ (ψ−(1−t)(β−z )−tν)]−E [(f∗)′ (ψ−(1−t)(β−z )−tν)])}
P + 0 Pn + 0
"ψ∈Ψ,ν∈[α−z0,β−z0]∩Q #
≤4R ,
Ht,P,n
10A PREPRINT -JUNE 25,2024
whereH :={(f∗)′ (ψ−(1−t)(β−z )−tν):ψ ∈Ψ,ν ∈[α−z ,β−z ]∩Q}.UsingtheLipschitzassumption
t + 0 0 0
on(f∗)′ togetherwithTalagrand’slemma(LemmaC.3)andtheresultofthecalculation(51)weobtain
+
R ≤L R (61)
Ht,P,n α,β Ψ−(1−t)(β−z0)−t[α−z0,β−z0]∩Q),P,n
β−α
=L (R +tR )≤L R +t .
α,β Ψ,P,n [α−z0,β−z0]∩Q),P,n α,β Ψ,P,n 2n1/2
(cid:18) (cid:19)
Puttingtheseboundstogetherwefind
1 β−α
E sup ± ΛP[ψ]−ΛPn[ψ] ≤2R +(β−α) 4L R +t dt (62)
"ψ∈Ψ
n (cid:16)
f f
(cid:17)o#
Ψ,P,n
Z0
α,β
(cid:18)
Ψ,P,n 2n1/2
(cid:19)
(β−α)2L
α,β
=2(1+2(β−α)L )R + .
α,β Ψ,P,n n1/2
CombiningthiswiththeboundfromLemma2.9givestheclaimedresult.
3 Error Bounds for(f,Γ)-GANs
InthissectionweusetheboundsonΛP derivedinSection2toderiveconcentrationinequalitiesfor(f,Γ)-GANs.
f
3.1 (f,Γ)-GANErrorDecomposition
We startbyderivingadecompositionofthe(f,Γ)-GANerrorintostatistical, approximation,andoptimizationerror
terms. First we consider the error that comes from approximatingthe idealized discriminator space Γ by a smaller
space(e.g.,aneuralnetwork)Γ,astepthatisgenerallyrequiredwhenimplementingaGANmethod:
Lemma3.1. LetΓ ⊂ Γ ⊂ M (X)benonemptyandsupposewehaveα,β ∈ Rsuchthatα ≤ h ≤ β forallh ∈ Γ.
b
Letf ∈ F (a,b)witha ≥ 0aendassumef isstrictlyconvexinaneighborhoodof1andz +β−α ∈ {f∗ < ∞}o.
1 0
Thenforanyprob eabilitymeasuresQ,P onX wehave
0≤DΓ(QkP)−DΓe (QkP)≤ 1+(f∗)′ (z +β−α) sup inf kh−h˜k . (63)
f f + 0 h∈Γh˜∈Γe ∞
(cid:0) (cid:1)
Proof. Foranyh∈Γ,h˜ ∈Γwecanusethedefinition(3)andLemma2.3tocompute
E [h]−ΛP[h]−DΓe (QkP)≤E [h]−E [h˜]+ΛP[h˜]−ΛP[h] (64)
Q ef f Q Q f f
≤kh−h˜k L1(Q)+(f∗)′ +(z 0+β−α)kh˜−hk L1(P)
≤ 1+(f∗)′ (z +β−α) kh−h˜k .
+ 0 ∞
Minimizingoverh˜ ∈Γgives (cid:0) (cid:1)
E [h]−ΛP[h]−DΓe (QkP)≤ 1+(f∗)′ (z +β−α) inf kh−h˜k (65)
eQ f f + 0 h˜∈Γe ∞
(cid:0) (cid:1)
forallh∈Γ. Maximizingoverh∈Γthencompletestheproof.
Nextweoutlinetheassumptionswemakeregardingthediscriminator,generator,andtheempiricalGANoptimization.
Assumption3.2((f,Γ)-GANAssumptions). Letf ∈F (a,b)witha≥0satisfythefollowing:
1
1. f isstrictlyconvexinanbhdof1.
2. z +β−α∈{f∗ <∞}o,wherez wasdefinedin(9).
0 0
Let(X,B )beatopologicalspacewiththeBorelsigmaalgebra. SupposeΓ ⊂ Γ ⊂ C (X)(thespaceofbounded
X b
continuousfunctions)arenonempty(thediscriminatorspaces)andsatisfythefollowing:
e
1. Wehaveα,β ∈Rsuchthatα<β andα≤h≤β forallh∈Γ.
2. ThereexistsacountableΓ ⊂ Γsuchthatforallh ∈ Γthereexistsasequenceh ∈ Γ suchthath → h
0 j 0 j
pointwise.
11A PREPRINT -JUNE 25,2024
3. ThereexistsacountableΓ ⊂ Γsuchthatforallh˜ ∈ Γthereexistsasequenceh˜ ∈ Γ suchthath˜ → h˜
0 j 0 j
pointwise.
e e e e
Let Z be anothermeasurablespace, P a probabilitymeasure on Z, andΦ : Z → X be measurablefor θ ∈ Θ,
Z θ
whereΘisaseparablemetricspace.Supposeθ 7→Φ (z)iscontinuousforallz ∈Z. DefineP :=(Φ ) P ,which
θ θ θ # Z
isaprobabilitymeasureonX.
LetQbeaprobabilitymeasureonX andX ,i = 1,...,n,Z ,i = 1,...,mbeindependentanddistributedasQ,P
i i Z
respectively. LetQ ,P . andP betheempiricalmeasurescorrespondingtoX ,Z ,andΦ ◦Z respectively.
n Z,m θ,m i i θ i
Finally,supposethatforeachm,nwe haveanerrortoleranceǫopt ≥ 0andΘ-valuedrandomvariablesθ∗ that
n,m n,m
areapproximateoptimizerstotheempiricalGANproblem,i.e.,thatsatisfy
e e
D fΓ(Q nkP θ n∗ ,m,m)≤ θi ∈n Θf D fΓ(Q nkP θ,m)+ǫo np ,mt P-a.s. (66)
Remark3.3. TheassumptionsregardingΓ ,Γ ,separabilityofΘ,andcontinuityofΦ andh∈Γallowustoaddress
0 0 θ
theissueofmeasurabilityofthevarioussupremathatariseinthederivationsbelowbyenablingonetorestrictthem
tocountablesubsets. Theseassumptionsholdinmostcasesofinterestandcanalsobereplacedbyanyalternatives
e
thatservethesamepurpose.
Note that asymmetry of the (f,Γ)-divergences requires us to separately treat the cases where the generator is the
firstargumentandwhereitisthesecond;weemphasizethatthischoicecanmakeasignificantdifferenceinpractice
as shown in the examples in Birrelletal. [2022a]. With this in mind we give a second version of the (f,Γ)-GAN
assumptionsandwillderiveconcentrationinequalitiesforbothcases.
Assumption 3.4 (Reverse (f,Γ)-GAN Assumptions). Assume that Assumption 3.2 holds, with the sole exception
beingthatθ∗ satisfies
n,m
e e
D fΓ(P θ n∗ ,m,mkQ n)≤ θi ∈n Θf D fΓ(P θ,mkQ n)+ǫo np ,mt P-a.s. (67)
asopposedto(66).
The concentration inequalities that we derive below rely on the following decompositionsof the (f,Γ)-GAN error.
TheseshouldbecomparedwiththeresultforIPM-GANs,Lemma9ofHuangetal.[2022].
Lemma3.5((f,Γ)-GANErrorDecomposition). UndertheAssumption3.2the(f,Γ)-GANerrorcanbedecomposed
P-a.s.asfollows:
D fΓ(QkP θ n∗ ,m)− θi ∈n Θf D fΓ(QkP θ) (68)
≤ sup ΛPZ,m[h◦Φ ]−ΛPZ[h◦Φ ]) + sup ΛPZ[h◦Φ ]−ΛPZ,m[h◦Φ ]
f θ f θ f θ f θ
e e
h∈Γ,θ∈Θ h∈Γ,θ∈Θ
n o n o
+sup{E [h]−E [h]}+sup{E [h]−E [h]}
Q Qn Qn Q
e e
h∈Γ h∈Γ
+ 1+(f∗)′ (z +β−α) supinf kh−h˜k +ǫopt .
+ 0 h∈Γh˜∈Γe ∞ n,m
(cid:0) (cid:1)
Proof. Using(66)wecancomputetheP-a.s. bound
D fΓ(QkP θ n∗ ,m)− θi ∈n Θf D fΓ(QkP θ) (69)
e e
=D fΓ(QkP θ n∗ ,m)− θi ∈n Θf D fΓ(Q nkP θ,m)+ θi ∈n Θf D fΓ(Q nkP θ,m)− θi ∈n Θf D fΓ(QkP θ)
e e
≤D fΓ(QkP θ n∗ ,m)−D fΓ(Q nkP θ n∗ ,m,m)+ǫo np ,mt + θi ∈n Θf D fΓ(Q nkP θ,m)− θi ∈n Θf D fΓ(QkP θ).
e
UsingthefactthatΓ⊂ΓimpliesDΓ ≤DΓalongwithLemma3.1wethenfind
f f
eD fΓ(QkP θ n∗ ,m)− θi ∈n Θf D fΓ(QkP θ) (70)
e e e e
≤D fΓ(QkP θ n∗ ,m)−D fΓ(Q nkP θ n∗ ,m,m)+ θi ∈n Θf D fΓ(Q nkP θ,m)− θi ∈n Θf D fΓ(QkP θ)
+ 1+(f∗)′ (z +β−α) supinf kh−h˜k +ǫopt .
+ 0 h∈Γh˜∈Γe ∞ n,m
(cid:0) (cid:1)
12A PREPRINT -JUNE 25,2024
Nextwemakeuseofthesimplebound
±(supd −supc )≤sup{±(d −c )} (71)
i i i i
i∈I i∈I i∈I
wheneverc ,d ∈ Rforalli ∈ I andsup c ,sup d arefinite. Usingthedefinition(3)alongwith(24)and(71)we
i i i i i i
cancompute
D fΓ(QkP θ n∗ ,m)− θi ∈n Θf D fΓ(QkP θ)≤sup
e
E Q[h]−Λ fPθn∗ ,m[h]− E Qn[h]−Λ fPθn∗ ,m,m[h] (72)
h∈Γ(cid:26) (cid:18) (cid:19)(cid:27)
+supsup E [h]−ΛPθ,m[h]−(E [h]−ΛPθ[h])
θ∈Θh∈Γe
Qn f Q f
n o
+ 1+(f∗)′ (z +β−α) supinf kh−h˜k +ǫopt
+ 0 h∈Γh˜∈Γe ∞ n,m
(cid:0) (cid:1)
≤sup{E [h]−E [h]}+ sup
ΛPθ,m[h]−ΛPθ[h]
e
Q Qn
e
f f
h∈Γ h∈Γ,θ∈Θ
n o
+sup{E [h]−E [h]}+ sup
ΛPθ[h]−ΛPθ,m[h]
e
Qn Q
e
f f
h∈Γ h∈Γ,θ∈Θ
n o
+ 1+(f∗)′ (z +β−α) supinf kh−h˜k +ǫopt .
+ 0 h∈Γh˜∈Γe ∞ n,m
WehaveΛPθ[h]=ΛPZ[h◦Φ
]andΛPθ,m[h]=Λ(cid:0)PZ,m[h◦Φ ]andsothis(cid:1)
completestheproof.
f f θ f f θ
We alsoobtainan errordecompositioninthecase wheretheargumentsoftheGAN arereversed. Theproofis very
similarandsoweomitthedetails.
Lemma3.6(Reverse(f,Γ)-GANErrorDecomposition). UnderAssumption3.4the(f,Γ)-GANerrorcanbedecom-
posedP-a.s. asfollows:
D fΓ(P θ n∗ ,mkQ)− θi ∈n Θf D fΓ(P θkQ) (73)
≤ sup E [h◦Φ ]−E [h◦Φ ] + sup E [h◦Φ ]−E [h◦Φ ]
PZ θ PZ,m θ PZ,m θ PZ θ
e e
h∈Γ,θ∈Θ h∈Γ,θ∈Θ
(cid:8) (cid:9) (cid:8) (cid:9)
+sup ΛQn[h]−ΛQ[h] +sup ΛQ[h]−ΛQn[h]
f f f f
e e
h∈Γ h∈Γ
n o n o
+ 1+(f∗)′ (z +β−α) sup inf kh−˜hk +ǫopt .
+ 0 h∈Γh˜∈Γe ∞ n,m
(cid:0) (cid:1)
We note thatthe setting of Lemmas3.5and 3.6differsin a few ways fromthatof Huangetal. [2022]. Namelywe
assume Γ ⊂ Γ and we also do not assume that inf DΓ(QkP ) = 0 (resp. inf DΓ(P kQ) = 0). Under
θ∈Θ f θ θ∈Θ f θ
appropriateassumptions,thelattercanbeproventoholdforasufficientlyrichclassofgenerators. Morespecifically,
e
the analoegue of Huangetal. [2022] would be to work under the assumption that inf DΓ(µ kP ) = 0 (resp.
θ∈Θ f n θ
e e
inf θ∈ΘD fΓ(P θkµ n) = 0) for all empirical distributions µ n. As D fΓ ≤ d Γe (see (112) in Appendix B), this zero
generatorapproximationerror propertyholds for the (f,Γ)-divergencewheneverit holds for the the corresponding
IPM;seeYangetal.[2022]andthediscussioninSection2.2.1ofHuangetal.[2022]forsufficientconditions. Error
decompositionsthatareadaptedtothezero-approximation-errorsetting,anditsargument-reversal,aregivenbelow.
e
Lemma3.7((f,Γ)-GANErrorDecomposition2). UnderAssumption3.2,andsupposingthatinf DΓ(µ kP )=
θ∈Θ f n θ
0forallempiricaldistributionsµ ,the(f,Γ)-GANerrorcanbedecomposedP-a.s. asfollows:
n
D fΓ(QkP θ n∗ ,m)≤ s eup ΛP fZ,m[h◦Φ θ]−ΛP fZ[h◦Φ θ] + s eup ΛP fZ[h◦Φ θ]−ΛP fZ,m[h◦Φ θ] (74)
h∈Γ,θ∈Θ h∈Γ,θ∈Θ
n o n o
+sup{E [h]−E [h]}+ 1+(f∗)′ (z +β−α) sup inf kh−h˜k +ǫopt .
h∈Γe Q Qn + 0 h∈Γh˜∈Γe ∞ n,m
(cid:0) (cid:1)
Proof. Using(66)alongwithLemma3.1wecancomputetheP-a.s.bound
e e e
D fΓ(QkP θ n∗ ,m)≤D fΓ(QkP θ n∗ ,m)+ θi ∈n Θf D fΓ(Q nkP θ,m)−D fΓ(Q nkP θ n∗ ,m,m) (75)
+ 1+(f∗)′ (z +β−α) supinf kh−h˜k +ǫopt .
+ 0 h∈Γh˜∈Γe ∞ n,m
(cid:0) (cid:1)
13A PREPRINT -JUNE 25,2024
Asasimpleconsequenceofthedefinition(3),forallθ ∈Θwehave
DΓe (Q kP )≤DΓe (Q kP )+ sup {ΛPθ[h]−ΛPθ,m[h]}. (76)
f n θ,m f n θ f f
e
h∈Γ,θ∈Θ
Minimizingoverθ ∈Θandusingthezero-approximation-errorpropertyweobtain
inf DΓe (Q kP )≤ inf DΓe (Q kP )+ sup {ΛPθ[h]−ΛPθ,m[h]} (77)
f n θ,m f n θ f f
θ∈Θ θ∈Θ e
h∈Γ,θ∈Θ
= sup
{ΛPθ[h]−ΛPθ,m[h]}.
f f
e
h∈Γ,θ∈Θ
Therefore
D fΓ(QkP θ n∗ ,m)≤ h∈s Γeu ,θp ∈Θ{ΛP fθ[h]−ΛP fθ,m[h]}+ 1+(f∗)′ +(z 0+β−α) s hu ∈p Γh˜in ∈f Γekh−h˜k ∞+ǫo np ,mt (78)
(cid:0) (cid:1)
e e
+D fΓ(QkP θ n∗ ,m)−D fΓ(Q nkP θ n∗ ,m,m)
≤ sup {ΛPθ[h]−ΛPθ,m[h]}+ 1+(f∗)′ (z +β−α) sup inf kh−h˜k +ǫopt
h∈Γe
,θ∈Θ
f f + 0 h∈Γh˜∈Γe ∞ n,m
(cid:0) (cid:1)
+sup{E [h]−E [h]}+sup
ΛPθn∗ ,m,m[h]−ΛPθn∗
,m[h] .
Q Qn f f
e e
h∈Γ h∈Γ(cid:26) (cid:27)
Boundingthelasttermbymaximizingoverθ ∈ΘandthenusingΛPθ[h]=ΛPZ[h◦Φ ]andΛPθ,m[h]=ΛPZ,m[h◦Φ ]
f f θ f f θ
completestheproof.
Lemma 3.8 (Reverse (f,Γ)-GAN Error Decomposition 2). Under Assumption 3.4, and supposing that
e
inf DΓ(P kµ )=0forallempiricaldistributionsµ ,the(f,Γ)-GANerrorcanbedecomposedP-a.s.asfollows:
θ∈Θ f θ n n
D fΓ(P θ n∗ ,mkQ)≤ s eup E PZ[h◦Φ θ]−E PZ,m[h◦Φ θ] + s eup {E PZ,m[h◦Φ θ]−E PZ[h◦Φ θ]} (79)
h∈Γ,θ∈Θ h∈Γ,θ∈Θ
(cid:8) (cid:9)
+sup ΛQn[h]−ΛQ[h] + 1+(f∗)′ (z +β−α) sup inf kh−h˜k +ǫopt .
h∈Γe f f + 0 h∈Γh˜∈Γe ∞ n,m
n o (cid:0) (cid:1)
Theproofof(3.8)issimilartothatof(3.7)andsoweomitthedetails. Notethatthetermsinthebounds(74)and(79)
reducetothetermsinIPMcase,Lemma9inHuangetal.[2022],iff∗(z)=zand−Γ⊂Γ.
3.2 ConcentrationInequalitiesfor(f,Γ)-GANs e e
We are now ready to derive concentration inequalities for (f,Γ)-GANs. Due to the asymmetry of DΓ(QkP) in Q
f
and P, the bounds differ based on whether the generator is in the first or second argument of DΓ. We also have
f
variantsdependingonwhetheroneassumesthezerogeneratorapproximationerrorpropertyforempiricalmeasures.
The keyingredientsare the ULLN for the generalizedcumulantgeneratingfunctionin Lemmas2.9and 2.10 along
withtheperturbationboundfromLemma2.7,thelatterbeingneededinordertoapplyMcDiarmid’sinequalitytothe
terms involving the generalized cumulant generating function. The following definition lists several quantities that
will appear in the concentrationinequalitiesbelow. We note that these quantitiesapproachtheir counterpartsin the
linear(i.e.,IPM)caseintheappropriatelimit;seethediscussioninRemarks2.8and2.11.
Definition3.9. WithnotationasineitherAssumption3.2or3.4:
1. Definethediscriminator-spaceapproximationerror
ǫΓ,Γe := 1+(f∗)′ (z +β−α) sup inf kh−h˜k . (80)
approx + 0 h∈Γh˜∈Γe ∞
(cid:0) (cid:1)
NotethatthisvanisheswhenΓ=Γ.
2. Forn∈Z+ define
e
n−1 1
∆ := inf n −z+ f∗(z)+ f∗(β−α+z) (81)
f,n
z∈[z0−(β−α),z0]
(cid:26) (cid:18)
n n
(cid:19)(cid:27)
andnotethat∆ ∈[β−α,f∗(β−α+z )−z ].
f,n 0 0
14A PREPRINT -JUNE 25,2024
3. Letn ∈ Z+,ΨbeanonemptyfamilyofmeasurablefunctionsonX,andP aprobabilitymeasureonX. If
(f∗)′ isL -Lipschitzon[z −(β−α),z +β−α]andΨcontainsaconstantfunctionthendefine
+ α,β 0 0
K (82)
f,Ψ,P,n
(β−α)2L β−α
:=min (1+2(β−α)L )R + α,β ,(f∗)′ (β−α+z ) R +
α,β Ψ,P,n 2n1/2 + 0 Ψ,P,n 2n1/2
(cid:26) (cid:18) (cid:19)(cid:27)
andotherwisedefine
β−α
K :=(f∗)′ (β−α+z ) R + . (83)
f,Ψ,P,n + 0 Ψ,P,n 2n1/2
(cid:18) (cid:19)
Theorem 3.10 ((f,Γ)-GAN Concentration Inequalities). Under Assumption 3.2, and in particular with θ∗ the
n,m
approximatesolutiontotheempirical(f,Γ)-GANproblem(66),forǫ>0wehave
P D fΓ(QkP θ n∗ ,m)− θi ∈n Θf D fΓ(QkP θ)≥ǫ+ǫ aΓ p,Γe prox+ǫn op,m t +4R Γe ,Q,n+4K f,Γe ◦Φ,PZ,m (84)
(cid:18) (cid:19)
ǫ2
≤exp − ,
n2(β−α)2+ m2∆2
f,m!
wherewerefertothequantitiesinDefinition3.9.
e
If,inaddition,inf DΓ(µ kP )=0forallpossibleempiricaldistributionsµ thenweobtainthetighterbound
θ∈Θ f n θ n
P D fΓ(QkP θ n∗ ,m)≥ǫ+ǫ aΓ p,Γe prox+ǫn op,m t +2R Γe ,Q,n+4K f,Γe ◦Φ,PZ,m ≤exp − 21 n(β−α)ǫ 22 + m2∆2 f,m! . (85)
(cid:16) (cid:17)
Proof. First note that Lemma 2.3 and Corollary 2.6 together with the dominated convergencetheorem imply θ 7→
ΛPZ[h◦Φ ]isarecontinuousandalsothath →hpointwiseimpliesΛPZ[h ◦Φ ]→ΛPZ[h◦Φ ],E [h ]→E [h],
f θ j f j θ f θ Q j Q
andsimilarlyfortheempiricalvariants. Therefore,lettingΘ denoteacountabledensesubsetofΘ,thesupremain
0
Lemma3.5canberestrictedtothecountablesubsetsΓ andΘ ,givingtheP-a.s. bound
0 0
D fΓ(QkP θ n∗ ,m)− θi ∈n Θf D fΓ(QkP θ)
e
(86)
≤ sup ΛPZ,m[h◦Φ ]−ΛPZ[h◦Φ ]) + sup ΛPZ[h◦Φ ]−ΛPZ,m[h◦Φ ]
f θ f θ f θ f θ
e e
h∈Γ0,θ∈Θ0n
o
h∈Γ0,θ∈Θ0n
o
+ sup {E [h]−E [h]}+ sup {E [h]−E [h]}
Q Qn Qn Q
e e
h∈Γ0 h∈Γ0
e
+ǫΓ,Γ +ǫopt .
approx n,m
NowwewillapplyMcDiarmid’sinequalitytotheright-handside. ThemapH :Xn×Zm →Rdefinedby
H(x,z)= sup ΛPZ,m[h◦Φ ]−ΛPZ[h◦Φ ]) + sup ΛPZ[h◦Φ ]−ΛPZ,m[h◦Φ ] (87)
f θ f θ f θ f θ
e e
h∈Γ0,θ∈Θ0n
o
h∈Γ0,θ∈Θ0n
o
+ sup {E [h]−E [h]}+ sup {E [h]−E [h]}
Q Qn Qn Q
e e
h∈Γ0 h∈Γ0
ismeasurableandifx,x˜∈Xn differonlyinthej’thcomponentthen
n n
1 1
|H(x,z)−H(x˜,z)|≤2 sup h(x )− h(x˜ ) (88)
i i
h∈Γe 0((cid:12) (cid:12)n
Xi=1
n
Xi=1
(cid:12) (cid:12))
2 (cid:12) 2 (cid:12)
≤ sup (cid:12)|h(x )−h(x′)| ≤ (β−(cid:12)α),
n e (cid:12) j j n (cid:12)
h∈Γ0
(cid:8) (cid:9)
while if z,z˜ ∈ Zm differonlyinthe j’thcomponentthenLemma2.5andthe perturbationboundfromLemma2.7
imply
2
|H(x,z)−H(x,z˜)|≤2 sup {|Λ ◦(h◦Φ ) (z)−Λ ◦(h◦Φ ) (z˜)|}≤ ∆ . (89)
f θ m f θ m f,m
e m
h∈Γ0,θ∈Θ0
15A PREPRINT -JUNE 25,2024
ThereforewecanapplyMcDiarmid’sinequalitytoH,see,e.g.,TheoremD.8inMohrietal.[2018],toobtain
ǫ2
P(H(X,Z)≥ǫ+E[H(X,Z)])≤exp − . (90)
n2(β−α)2+ m2∆2
f,m!
IntermsofH,theerrorbound(86)becomesthea.s. bound
e
D fΓ(QkP θ n∗ ,m)− θi ∈n Θf D fΓ(QkP θ)≤H(X,Z)+ǫ aΓ p,Γ prox+ǫo np ,mt . (91)
Using the ULLN for means, see Theorem C.4, along with the new ULLN for the generalized cumulant generating
functioninLemmas2.9and2.10weobtain
E[H(X,Z)]≤4Re +4K e . (92)
Γ,Q,n f,Γ◦Φ,PZ,m
Combining(90),(91),and(92)wearriveattheclaimedresult(84).
e
If we also assume inf DΓ(µ kP ) = 0 for all possible empirical distributions µ then we can write the error
θ∈Θ f n θ n
decomposition3.7as
e
D fΓ(QkP θ n∗ ,m)≤H(X,Z)+ǫ aΓ p,Γ prox+ǫo np ,mt (93)
P-a.s.,wherewenowdefine
H(X,Z):= sup ΛPZ,m[h◦Φ ]−ΛPZ[h◦Φ ] + sup ΛPZ[h◦Φ ]−ΛPZ,m[h◦Φ ] (94)
f θ f θ f θ f θ
e e
h∈Γ0,θ∈Θ0n
o
h∈Γ0,θ∈Θ0n
o
+ sup {E [h]−E [h]} .
Q Qn
e
h∈Γ0
Similartotheabove,whenxandx˜differinonlyasinglecomponentwecanbound
1
|H(x,z)−H(x˜,z)|≤ (β−α) (95)
n
andwhenzandz˜differinonlyasinglecomponentwecanbound
2
|H(x,z)−H(x,z˜)|≤ ∆ . (96)
f,m
m
ThemeanofH(X,Z)canbeboundedviaTheoremC.4andLemmas2.9and2.10:
E[H(X,Z)]≤2Re +4K e . (97)
Γ,Q,n f,Γ◦Φ,PZ,m
Combining(93),and(95)-(97)withMcDiarmid’sinequalitycompletestheproofof(85).
Similarly,wehaveconcentrationinequalitiesforthereverse(f,Γ)-GANs. Theproofisverysimilarandsoweomit
thedetails.
Theorem3.11(Reverse(f,Γ)-GANConcentrationInequalities). UnderAssumption3.4,andinparticularwithθ∗
n,m
theapproximatesolutiontotheempirical(f,Γ)-GANproblem(67),forǫ>0wehave
P D fΓ(P θ n∗ ,mkQ)− θi ∈n Θf D fΓ(P θkQ)≥ǫ+ǫ aΓ p,Γe prox+ǫn op,m t +4R Γe ◦Φ,PZ,m+4K f,Γe ,Q,n (98)
(cid:18) (cid:19)
ǫ2
≤exp − ,
m2(β−α)2+ n2∆2
f,n!
wherewerefertothequantitiesinDefinition3.9.
e
If,inaddition,inf DΓ(P kµ )=0forallpossibleempiricaldistributionsµ thenweobtainthetighterbound
θ∈Θ f θ n n
P D fΓ(P θ n∗ ,mkQ)≥ǫ+ǫ aΓ p,Γe prox+ǫn op,m t +4R Γe ◦Φ,PZ,m+2K f,Γe ,Q,n ≤exp − m2(β−α)ǫ 22 + 21 n∆2 f,n! . (99)
(cid:16) (cid:17)
16A PREPRINT -JUNE 25,2024
Remark3.12. Asstated,theaboveresultsassumeaspaceofdiscriminators,Γ,thatsatisfiesauniformboundofthe
formα ≤ h ≤ β forallh ∈ Γ. However, we notethatthe(f,Γ)-divergenceobjectivefunctionalin(3)isinvariant
underconstantshifts,duetotheidentityΛP[h+c]=ΛP[h]+cforallc∈R. Therefore,ifwehavediscriminatorsΨ
f f
suchthattherangesofψ ∈Ψhaveuniformlyboundeddiameter,i.e.,thereexistsβsuchthatsup |ψ(x)−ψ(x˜)|≤β
x,x˜
forallψ ∈Ψ,thenwecanwrite
DΨ(QkP)=DΓ(QkP), (100)
f f
whereΓ:={ψ−infψ :ψ ∈Ψ}satisfies0≤h≤β forallh∈Γ. ThusourtheorycanbeappliedtoDΓandhence
f
alsotoDΨvia(100). Inthisway,ourtheoremscanbeappliedto(f,Γ)-GANswithdiscriminatorswhoserangeshave
f
uniformlyboundeddiameter,e.g.,1-Lipschitzfunctionsonacompactdomain.
Remark 3.13 (Bounds on the mean of the (f,Γ)-GAN error). Bounds on the mean of the (f,Γ)-GAN error are
implicit in the above derivations. They are obtainedby combiningthe error decompositionsin Section3.1 with the
ULLNresultsinLemma2.9,Lemma2.10,andTheoremC.4. Inaddition,astandardtechniquecanbeusedtoturnthe
concentrationinequalitiesintoLq errorboundsforanyq > 1asfollows: LetY beanon-negativerandomvariable,
a∈[0,∞),andK :(0,∞)→[0,∞)bemeasurablesuchthat
P(Y ≥ǫ+a)≤K(ǫ) (101)
forallǫ>0. Thenforq >0wehave
∞
E[Yq]≤aq+q (a+ǫ)q−1K(ǫ)dǫ. (102)
Z0
Thisfollowsfromrewriting
∞
E|Yq]= P(Yq ≥r)dr, (103)
Z0
breakingthedomainofintegrationinto[0,aq]and[aq,∞),thenchangingvariablesinthesecondtermandusingthe
bound(101).
3.3 ConcentrationInequalitiesfortheEstimationof(f,Γ)-Divergences
ThetoolsdevelopedinSection2canalsobeusedtoaddressthesimplerproblemofconcentrationinequalitiesforthe
estimationofDΓ(QkP)usingsamplesfromQandP. Specialcasesofthisproblemwerepreviouslyconsideredin
f
Chenetal.[2023a,2024]. Bothconsideredthecasewheref correspondedtothefamilyofα-divergencesandΓwas
thesetofL-Lipschitzfunctions;theformerfocusedontheconsequencesofgroupsymmetrywhilethelatterfocused
onheavy-taileddistributions.Thetechniquesdevelopedinthisworkallowustoderiveconcentrationinequalitiesfora
muchwiderclassoff’sandΓ’s;wedonotconsidertheconsequencesofgroupsymmetryorheavy-taileddistributions
inthiswork.
Theorem3.14. LetΓbeanonemptysetofmeasurablefunctionsonX andsupposewehaveα,β ∈Rsuchthatα<β
andα≤h≤β forallh∈Γ. AssumethereexistsacountableΓ ⊂Γsuchthatforallh∈Γthereexistsasequence
0
h ∈Γ suchthath →hpointwise. Letf ∈F (a,b)witha≥0andassumef isstrictlyconvexinaneighborhood
j 0 j 1
of1andz +β−α∈{f∗ <∞}o.
0
LetQ,P beprobabilitymeasuresonX andX ,i = 1,...,n,X′,i = 1,...,mbejointlyindependentsamplesfromQ
i i
andP respectivelywithQ ,P thecorrespondingempiricalmeasures. Thenforǫ>0wehave
n m
2ǫ2
P DΓ(QkP)−DΓ(Q kP )≥ǫ ≤exp − , (104)
f f n m n1(β−α)2+ m1∆2
f,m!
(cid:0) (cid:1)
2ǫ2
P DΓ(Q kP )−DΓ(QkP)≥ǫ+2R +2K ≤exp − , (105)
f n m f Γ,Q,n f,Γ,P,m n1(β−α)2+ m1∆2
f,m!
(cid:0) (cid:1)
where∆ andK weredefinedinDefinition3.9.
f,m f,Γ,P,m
Proof. SimilartotheproofofTheorem3.10,ifwedefine
H(X,X′)=DΓ(Q kP ), (106)
f n m
17A PREPRINT -JUNE 25,2024
thenwhenxandx˜differbyasinglecomponentwehave
1
|H(x,x′)−H(x˜,x′)|≤ (β−α) (107)
n
andwhenx′ andx˜′ differbyasinglecomponent,Lemma2.7implies
1
|H(x,x′)−H(x,x˜′)|≤ ∆ . (108)
f,m
m
ThereforewecanuseMcDiarmid’sinequalitytoconcludethat
2ǫ2
P ±(DΓ(Q kP )−E[DΓ(Q kP ))≥ǫ ≤exp − . (109)
f n m f n m n1(β−α)2+ m1∆2
f,m!
(cid:0) (cid:1)
Combining(109)(lowersign)withtheresultofthecalculation
E DΓ(Q kP ) =E sup {E [h−ν]−E [f∗(h−ν)]} (110)
f n m Qn Pm
"h∈Γ,ν∈[α−z0,β−z0] #
(cid:2) (cid:3) ≥ sup E[E [h−ν]−E [f∗(h−ν)]]
Qn Pm
h∈Γ,ν∈[α−z0,β−z0]
= sup {E [h−ν]−E [f∗(h−ν)]}=DΓ(QkP)
Q P f
h∈Γ,ν∈[α−z0,β−z0]
weobtain(104). Combining(109)(uppersign)withtheresultofthecalculation
E DΓ(Q kP ) −DΓ(QkP)≤E sup{E [h]−E [h]} +E sup ΛP[h]−ΛPm[h] (111)
f n m f Qn Q f f
(cid:20)h∈Γ (cid:21) (cid:20)h∈Γ n o(cid:21)
(cid:2) (cid:3)
andthenusingTheoremC.4andLemmas2.9and2.10weobtain(105).
4 Conclusion
We haveproventhestatisticalconsistencyof(f,Γ)-GANs, alargeclassofGANswithnonlinearobjectivefunction-
als. TheseGANsarebasedonthe(f,Γ)-divergences,whichgeneralizeandinterpolatebetweenintegralprobability
metrics(IPMs,e.g.,1-Wasserstein)andf-divergences(e.g.,KL)andhavebeenshowtooutperformbothIPMandf-
divergence-basedmethodsinanumberofapplications. Thispaperextendsearliertechniquesforprovingconsistency
ofGANswithlinearobjectivefunctionals(IPM-GANs)tothenonlinearsetting. Thekeytechnicalresultsarethetight
perturbationboundinLemma2.7,uniformlawoflargenumbersboundsforaclassofnonlinearfunctionalsinLem-
mas2.9and2.10,andthe(f,Γ)-GANerrordecompositionsinSection3.1. Theseresultsallowforthederivationof
finite-sampleconcentrationinequalitiesfor(f,Γ)-GANsinTheorems3.10and3.11,therebyprovingtheirstatistical
consistency. TheboundshavethesamequalitativeformasthosepreviouslyobtainedforIPM-GANsandso wecan
alsoconcludethattheperformanceof(f,Γ)-GANsscaleswiththeintrinsicdimensionofthedatasourceinasimilar
fashiontoIPM-GANs.
A Properties off andf∗
Inthisappendixwecollectanumberofimportantpropertiesoftheconvexfunctionf anditsLegendretransformf∗
thatareneededinthestudyof(f,Γ)-GANs. Fora,bsatisfying−∞≤a < 1 <b ≤ ∞wedefineF (a,b)tobethe
1
setofconvexf :(a,b)→Rwithf(1)=0.Forf ∈F (a,b),standardconvexfunctionstheory,see,e.g.,Rockafellar
1
[1970] and Appendix A in Birrelletal. [2022a], implies that f and its Legendre transform f∗ : R → (−∞,∞],
f∗(z)=sup {zt−f(t)},havethefollowingproperties:
t∈(a,b)
1. f∗(z)≥zforallz ∈R(thisusesf(1)=0).
2. Ifa≥0thenf∗ isnon-decreasing.
3. (f∗)∗ =f.
4. f∗isconvexandLSC.
5. f∗iscontinuouson{f∗ <∞},whereAdenotestheclosureofasetA.
18A PREPRINT -JUNE 25,2024
6. The right derivative (f∗)′ exists and is finite on {f∗ < ∞}o, where Ao denotes the interior of a set A.
+
Similarly,f′ existsandisfiniteon(a,b).
+
7. (f∗)′ isnon-decreasingandabsolutelycontinuousoncompactintervals; thelatterimpliesthatf∗ satisfies
+
thefundamentaltheoremofcalculus(see,e.g.,Theorem3.35andexercise42inFolland[2013]).
Ofparticularrelevancewillbethevaluez := f′ (1),whichexistsandisfiniteduetotheassumptionthat1 ∈ (a,b).
0 +
Theimportanceofz tothe(f,Γ)-divergenceswasobservedinBirrelletal.[2022a],wherethefollowingproperties
0
wereproven(seeLemmaA.9):
LemmaA.1. Definez :=f′ (1).
0 +
1. f∗(z )=z
0 0
2. Iff isstrictlyconvexonaneighborhoodof1andz ∈{f∗ <∞}othen(f∗)′ (z )=1.
0 + 0
Thesepropertieswillbekeytoouranalysisandsowewillgenerallyassumethatf isstrictlyconvexonaneighborhood
of1andz ∈{f∗ <∞}o.
0
B Properties ofthe(f,Γ)-Divergences
In this appendix we collect several important properties of the (f,Γ)-divergences. For X a measurable space we
letM (X) denotethe spaceofboundedmeasurablereal-valuedfunctionsonX andP(X)be thesetof probability
b
measuresonX. ThefollowingresultsaretakenfromTheorem2.8ofBirrelletal.[2022a].
TheoremB.1. Letf ∈F (a,b),Γ⊂M (X)benonempty,andQ,P ∈P(X).
1 b
1.
DΓ(QkP)≤ inf {D (ηkP)+d (Q,η)}. (112)
f f Γ
η∈P(X)
Inparticular,DΓ(QkP)≤min{D (QkP),d (Q,P)}.
f f Γ
2. Themap(Q,P)∈P(S)×P(S)7→DΓ(QkP)isconvex.
f
3. Ifthereexistsc ∈Γ∩RthenDΓ(QkP)≥0.
0 f
4. Supposef andΓsatisfythefollowing:
(a) ThereexistanonemptysetΨ⊂Γwiththefollowingproperties:
i. ΨisP(X)-determining,i.e.,forallQ,P ∈P(X),E [ψ]=E [ψ]forallψ ∈ΨimpliesQ=P.
Q P
ii. Forallψ ∈Ψthereexistsc ∈R,ǫ >0suchthatc +ǫψ ∈Γforall|ǫ|<ǫ .
0 0 0 0
(b) f isstrictlyconvexonaneighborhoodof1.
(c) f∗isfiniteandC1onaneighborhoodoff′ (1).
+
ThenDΓhasthedivergenceproperty,i.e.,DΓ(QkP)≥0withequalityifandonlyifQ=P.
f f
Remark B.2. Under stronger assumptions one can show that (112) is in fact an equality; see Theorem 2.15 in
Birrelletal.[2022a].
RemarkB.3. Assumptions4(b)and4(c)hold,forinstance,iff isstrictlyconvexon(a,b)andf′ (1)∈{f∗ < ∞}o
+
(seeTheorem26.3inRockafellar[1970]).
C Rademacher Complexityand UniformLaw ofLarge Numbers
InthisappendixwerecallthedefinitionofRademachercomplexityanditsuseinprovinguniformlawoflargenumbers
(ULLN)results. TherearetwodefinitionsofRademachercomplexityincommonuse,thosebeingwithandwithout
absolutevalue. Inthisworkweusetheversionwithoutabsolutevalueasdefinedbelow.
DefinitionC.1. LetΨbeacollectionoffunctionsonY andn ∈ Z+. TheempiricalRademachercomplexityofΨat
y ∈Ynisdefinedby
1
R (y):=E sup σ·ψ (y) , (113)
Ψ,n σ n
n
"ψ∈Ψ(cid:26) (cid:27)#
b
19A PREPRINT -JUNE 25,2024
whereσ ,i = 1,...,nareindependentuniformrandomvariablestakingvaluesin{−1,1},i.e.,Rademacherrandom
i
variables,andψ (y) := (ψ(y ),...,ψ(y )). GivenaprobabilitydistributionP onY,theRademachercomplexityof
n 1 n
ΨrelativetoP isdefinedby
R :=E R (114)
Ψ,P,n Pn Ψ,n
h i
wherePnisthen-foldproductofP,i.e.,they i’sbecomei.i.d. sa bmplesfromP.
RemarkC.2. Notethatin(114)weareimplicitlyassumingthattheempiricalRademachercomplexityismeasurable.
Whenweuse(114)inthemaintext,thiswillbeguaranteedbyotherassumptions.
Thedefinition(113)isparticularlyconvenientforourpurposesduetothefollowingversionofTalagrand’slemma;see
Lemma5.7inMohrietal.[2018].
LemmaC.3. Letα,β ∈ RandΨbeacollectionofreal-valuedfunctionsonY suchthatα ≤ ψ ≤ β forallψ ∈ Ψ.
Ifφ:[α,β]→RisL-Lipschitzthen
R (y)≤LR (y) (115)
φ◦Ψ,n Ψ,n
foralln∈Z+,y ∈Yn,whereφ◦Ψ:={φ◦ψ :ψ ∈Ψ}.
b b
BoundsontheRademachercomplexitycanbeusedtoproveULLNbounds;see,e.g.,Theorem3.3,Eq. (3.8)-(3.13)
inMohrietal.[2018](thisreferenceassumes[0,1]-valuedfunctionsbuttheresultcanbefreelyshiftedandscaledto
applytoasetofuniformlyboundedfunctions):
Theorem C.4 (ULLN). Let P be a probabilitymeasure on Y and Ψ a countablefamily ofreal-valuedmeasurable
functionsonY thatareuniformlybounded.IfY ,i=1,...,narei.i.d.Y-valuedrandomvariablesthataredistributed
i
asP thenforn∈Z+ wehave
n
1
E sup ± ψ(Y )−E [ψ] ≤2R . (116)
i P Ψ,P,n
n
"ψ∈Ψ( !)#
i=1
X
RemarkC.5. WehavestatedtheULLNintermsofacountablecollectionoffunctionstoavoidmeasurabilityissues,
butunderappropriateassumptionsonecanapplyittoanuncountableΨ,e.g.,ifthereexistsacountableΨ ⊂Ψsuch
0
thatforeveryψ ∈Ψthereexistsasequenceψ ∈Ψ withψ →ψpointwise.
j 0 j
References
Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein generative adversarial networks. In Precup, D. and Teh,
Y.W.(eds.),Proceedingsofthe34thInternationalConferenceonMachineLearning,volume70ofProceedingsof
MachineLearning Research, pp. 214–223,InternationalConventionCentre, Sydney, Australia, 06–11Aug 2017.
PMLR.
Belghazi, M. I., Baratin, A., Rajeshwar, S., Ozair, S., Bengio, Y., Courville, A., and Hjelm, D. Mutualinformation
neuralestimation. In Dy, J. andKrause, A. (eds.), Proceedingsofthe 35thInternationalConferenceonMachine
Learning,volume80ofProceedingsofMachineLearningResearch,pp.531–540,Stockholmsmässan,Stockholm
Sweden,10–15Jul2018.PMLR. URLhttp://proceedings.mlr.press/v80/belghazi18a.html.
Ben-Tal, A. and Teboulle, M. An old-new concept of convex risk measures: The optimized certainty
equivalent. Mathematical Finance, 17(3):449–476, 2007. doi: 10.1111/j.1467-9965.2007.00311.x. URL
https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9965.2007.00311.x.
Birrell, J., Dupuis, P., Katsoulakis, M. A., Pantazis, Y., and Rey-Bellet, L. (f, Gamma)-Divergences: Interpolating
betweenf-divergencesandintegralprobabilitymetrics. Journalofmachinelearningresearch,23(39):1–70,2022a.
Birrell,J.,Katsoulakis,M.,Rey-Bellet,L.,andZhu,W.Structure-preservingGANs.ProceedingsofMachineLearning
Research,162,2022b.
Broniatowski, M. and Keziou, A. Minimization of divergences on sets of signed measures. Studia Scientiarum
MathematicarumHungarica,43(4):403–442,2006.
Chakraborty,S.andBartlett,P.L. Onthestatisticalpropertiesofgenerativeadversarialmodelsforlowintrinsicdata
dimension. arXivpreprintarXiv:2401.15801,2024.
Chen, Z., Katsoulakis, M., Rey-Bellet, L., andZhu, W. Samplecomplexityof probabilitydivergencesundergroup
symmetry. InInternationalConferenceonMachineLearning,pp.4713–4734.PMLR,2023a.
20A PREPRINT -JUNE 25,2024
Chen, Z., Katsoulakis, M. A., Rey-Bellet, L., and Zhu, W. Statistical guarantees of group-invariantGANs. arXiv
preprintarXiv:2305.13517,2023b.
Chen, Z., Gu, H., Katsoulakis, M. A., Rey-Bellet, L., and Zhu, W. Learning heavy-tailed distributions with
Wasserstein-proximal-regularizedα-divergences. arXivpreprintarXiv:2405.13962,2024.
Dupuis, Paul and Mao, Yixiang. Formulation and properties of a divergence used to compare probability
measures without absolute continuity. ESAIM: COCV, 28:10, 2022. doi: 10.1051/cocv/2022002. URL
https://doi.org/10.1051/cocv/2022002.
Dziugaite,G.K.,Roy,D.M.,andGhahramani,Z. Traininggenerativeneuralnetworksviamaximummeandiscrep-
ancyoptimization. arXivpreprintarXiv:1505.03906,2015.
Folland, G. Real Analysis: Modern Techniques and Their Applications. Pure and Applied Mathemat-
ics: A Wiley Series of Texts, Monographs and Tracts. Wiley, 2013. ISBN 9781118626399. URL
https://books.google.com/books?id=wI4fAwAAQBAJ.
Glaser,P.,Arbel,M.,andGretton,A. KALEflow: ArelaxedKLgradientflowforprobabilitieswithdisjointsupport.
AdvancesinNeuralInformationProcessingSystems,34:8018–8031,2021.
Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y.
Generativeadversarialnets. InProceedingsofthe27thInternationalConferenceonNeuralInformationProcessing
Systems-Volume2,NIPS’14,pp.2672–2680,Cambridge,MA,USA,2014.MITPress.
Gretton, A., Borgwardt, K. M., Rasch, M. J., Schölkopf, B., and Smola, A. A kernel
two-sample test. J. Mach. Learn. Res., 13(1):723–773, 2012. ISSN 1532-4435. URL
http://dl.acm.org/citation.cfm?id=2503308.2188410.
Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A. ImprovedTraining of Wasserstein GANs.
In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS’17, pp.
5769–5779,RedHook,NY,USA,2017.CurranAssociatesInc. ISBN9781510860964.
Huang, J., Jiao, Y., Li, Z., Liu, S., Wang, Y., andYang, Y. Anerroranalysisof generativeadversarialnetworksfor
learningdistributions. Journalofmachinelearningresearch,23(116):1–43,2022.
Li,Y.,Swersky,K.,andZemel,R. Generativemomentmatchingnetworks. InInternationalconferenceonmachine
learning,pp.1718–1727.PMLR,2015.
Liang,T. Howwellgenerativeadversarialnetworkslearndistributions. JournalofMachineLearningResearch,22
(228):1–41,2021.
Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y. Spectral normalization for generative ad-
versarial networks. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=B1QRgziT-.
Mohri, M., Rostamizadeh, A., and Talwalkar, A. Foundations of Machine Learning, second edition.
Adaptive Computation and Machine Learning series. MIT Press, 2018. ISBN 9780262039406. URL
https://books.google.com/books?id=V2B9DwAAQBAJ.
Nguyen,X.,Wainwright,M.J.,andJordan,M.I.Estimatingdivergencefunctionalsandthelikelihoodratiobyconvex
riskminimization. IEEETransactionsonInformationTheory,56(11):5847–5861,2010.
Nowozin,S.,Cseke,B.,andTomioka,R. F-GAN:TrainingGenerativeNeuralSamplersUsingVariationalDivergence
Minimization. In Proceedings of the 30th International Conference on Neural Information Processing Systems,
NIPS’16,pp.271–279,RedHook,NY,USA,2016.CurranAssociatesInc. ISBN9781510838819.
Rockafellar,R. ConvexAnalysis. PrincetonLandmarksinMathematicsandPhysics.PrincetonUniversityPress,1970.
ISBN9780691015866.
Ruderman,A.,Reid,M.D.,García-García,D.,andPetterson,J.Tightervariationalrepresentationsoff-divergencesvia
restrictiontoprobabilitymeasures.InProceedingsofthe29thInternationalCoferenceonInternationalConference
onMachineLearning,ICML’12,pp.1155–1162,Madison,WI,USA,2012.Omnipress. ISBN9781450312851.
Song,J. andErmon,S. Bridgingthe gapbetweenf-GANsandWasserstein GANs. InInternationalConferenceon
MachineLearning,pp.9078–9087.PMLR,2020.
Wainwright, M. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Series in Statis-
tical and Probabilistic Mathematics. Cambridge University Press, 2019. ISBN 9781108498029. URL
https://books.google.com/books?id=IluHDwAAQBAJ.
Yang,Y.,Li,Z.,andWang,Y. Onthecapacityofdeepgenerativenetworksforapproximatingdistributions. Neural
networks,145:144–154,2022.
21