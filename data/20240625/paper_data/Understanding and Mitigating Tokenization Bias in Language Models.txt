Understanding and Mitigating Tokenization Bias in Language Models
BuuPhan*1 MartonHavasi2 MatthewMuckley2 KarenUllrich2
Abstract enhancingvocabularydesignandencodingalgorithmsfor
betterperformanceindownstreamtasks. However,there-
State-of-the-artlanguagemodelsareautoregres-
lationship between compression and model performance
siveandoperateonsubwordunitsknownasto-
remainsunclear. Someresearchsuggeststheimpactofcom- kens. Specifically, one must encode the condi-
pressionisnotalwayspositive(Schmidtetal.,2024;Dagan
tioning string into a list of tokens before pass-
etal.,2024;Goyaletal.,2023). Consequently,understand-
ing to the language models for next-token pre-
ingtokenization’seffectonmodelperformancecontinues
diction. We show that, for encoding schemes
tobeanopenquestion.
suchasmaximumprefixmatching,tokenization
inducesasamplingbiasthatcannotbemitigated Tokenizationhasbeencriticizedforintroducingmanyshort-
withmore trainingor data. To counterthis uni- comingsinLMs. Theseincludesensitivitytospellingand
versalproblem,weproposeanovelalgorithmto morphologicalstructure(Xueetal.,2022),language-based
obtainunbiasedestimatesfromamodelthatwas biases(Petrovetal.,2024),subparperformanceinspecific
trainedontokenizeddata. Ourmethoddoesnot taskssuchasarithmetic(Singh&Strouse,2024), ornew
require finetuning the model, and its complex- domains(Liuetal.,2023a). Oneapproachtoaddressthese
ity,definedasthenumberofmodelruns,scales issuesisthroughfine-tuningthemodelwithnewvocabu-
linearly with the sequence length. As a conse- laries;however,thisoftencomplicatesthetrainingprocess
quence,weshowthatonecansimulatetoken-free andrequiresdomain-specificexpertise(Chenetal.,2023;
behaviorfromatokenizedlanguagemodel. We Liuetal.,2023b). Furthermore,theperformancegainsdo
empiricallyverifythecorrectnessofourmethod not provide a theoretical understanding of whether these
throughaMarkov-chainsetup,whereitaccurately limitationstrulyarisefromthetokenizationprocessorre-
recoversthetransitionprobabilities,asopposed sultfromsuboptimalmodeltraining. Anotherdirectionis
totheconventionalmethodofdirectlyprompting todeveloptoken-freeLMs(Yuetal.,2024;Nawrotetal.,
tokensintothelanguagemodel. 2022;Tayetal.,2021). Whilethisapproachhaspotential
asiteliminatestokenization-relatedissues,itsignificantly
increasesthecontextlength,resultinginperformancethat
1.Introduction stilllagsbehindtheSOTAtokenizedLMs1(Yuetal.,2024).
Tokenization is a preprocessing procedure used in many Inthisworkweoffernewtheoreticalinsightsonthebehav-
state-of-the-art (SOTA) language models (LMs) such as ioroftokenizedLMs. Weshowthattheyarestatistically
GPTs(Brownetal.,2020),Llama(Touvronetal.,2023)and equivalenttotheirtoken-freecounterparts. Specifically,we
Gemini(Gemini,2023).Itdividestheinputtextintosmaller examinethemaximumprefixencodingschemeemployed
subwordunitswhileretaininglinguisticimportance,helping intheWordPiecetokenizationmethod(Devlinetal.,2018;
toaddressvocabularylimitationssuchasunknownwords. Songetal.,2020)andfindthatthisprocessnotonlyresults
Tokenizationalsoshortens(compresses)theinputcontext inbiasedestimatesofnexttokenprobabilities,butalsoleads
length(Sennrichetal.,2015;Kudo&Richardson,2018). tooverallskewedestimatesofsubsequentcharacterproba-
Sinceeffectivecompressionallowstransformer-basedLMs bilities. Ingeneral,thisbiaspersistsdespiteanincreasein
tohandlelongercontextstrings,manyworks(Zouharetal., trainingdata,evenwithinthesimplesettingofa1st-order
2023;Galle´,2019;Goldmanetal.,2024)havefocusedon Markovchain. Suchbiasoccursduetotheimplicitdisparity
between the domain of the conditioning context, namely,
*Work done during internship at Meta FAIR 1University
charactersversustokens. Nevertheless,wewillshowthat
of Toronto 2Meta AI. Correspondence to: Buu Phan
<truong.phan@mail.utoronto.ca>, Karen Ullrich itispossibletocorrectthisbiaswithoutresortingtofine-
<karenu@meta.com>. tuning. Once adjusted, it becomes possible to simulate
Accepted in ICML 2024 Workshop on Theoretical Foundations 1We refer language models that process tokenized texts as
ofFoundationModels,Vienna,Austria. Copyright2024bythe tokenizedlanguagemodels(tokenizedLMs).
author(s).
1
4202
nuJ
42
]LC.sc[
1v92861.6042:viXraUnderstandingandMitigatingTokenizationBiasinLanguageModels
thetoken-freebehaviorlearnedimplicitlybythetokenized withtheencodingfunctionencode(.)andvocabulary ,and
V
LMandeven(theoretically)mimicthebehaviorofanother theparametersθareoptimizedtomaximizethepredictive
tokenized model employing a distinct vocabulary set, all likelihoodofthenexttokeninthetokenizeddataset.
withoutrequiringfinetuning. Ourspecificcontributionsare
2.2.Next-CharacterSamplingBias
asfollows:
Wefirstdefinethe(next-character)samplingbiasproblem
• Weshowthepresenceofabiasinthenext-tokendistribu- thatdescribesthediscrepancybetweenthecharacterlevel
tionthatarisesasaresultofthetokenizationprocess. andtokenlevelpredictionsfortokenizedLMs.
• We present a novel algorithm capable of transforming Definition2.1. (Next-CharacterSamplingBias)Letthein-
atokenizedLMintoitstoken-freecounterpart,thereby
putpromptstringxn
1
hasti 1=encode(xn 1)asthecorrespond-
effectivelyeliminatingthesamplingbias. ing encoding. The next-character sampling bias occurs
forthispromptwhenP (x xn)=P (x ti)where
• Weverifythecorrectnessofouralgorithmonlearningthe P (x ti)=(cid:80) Pgt (tn+1 =| t1 ti̸ ) wg ht eren+1 | =1 t
transitionmatrixofak-thorderMarkovchain. g dt ecn o+ d1 e| (t1 ) (xt∈E )gt . i+1 | 1 E { ∈
n+1
V| ∈S }
2.ProblemSetup Inotherwords,theprobabilityofthenextcharacterbeing
“c” may be different from the sum of the probabilities of
Webeginbyestablishingthetokenizationandlanguagemod-
alltokensthatstartwith“c”. Notethatthischaracter-level
elssetupinourpaper. Wethendescribethenext-character
probability offers a broader perspective compared to the
samplingbiasproblemduetotokenization.
probabilityofthesubsequenttokenbeingexactly“c”.
2.1.NotationsandSetup.
Example. Consider a first order Markov chain with two
String Notations. For any string s, we denote its sub- states “A”,“B” asshowninFigure1(left). Eachstring
string from i to j as xj i:=x ix i+1..x j, where each x is is toke{ nized with} = “AA”,“A”,“B” , which leads to
a character of the alphabet A. For a given string xN 1 , a new Markov chV ain{ whose states and} transition matrix
we define the prefix function that generates a set con- is shown in Figure 1 (right). Details on computing the
taining all possible prefix strings of xN 1 , represented as transitionmatrixofthenewMarkovchainisinAppendix
prefix(xN 1 )= {x1 1,x2 1,x3 1,...,xN 1 }. Also, we define a con- F. We first observe that for the prompt s 1=“AA” and
catenationfunctionconcat(.)thatconcatenatesthegiven s =”B”, there is no bias problem after marginalization2.
2
list of strings, e.g given s 1=xN 11 and s 2=y 1N2, we obtain However, for the prompt s 3=“A”, the sampling bias oc-
concat(s 1,s 2)=concat(xN 11,y 1N2)=x 1...x N1y 1...y N2. Fi- cursasP gt(x 2=“B”t 1=“A”)=1.0,whichisnotequalto
nally,wedenotethesetofallstringsthatstartwithaprefix P (x =“B”x =“A| ”)=α,i.e. theoptimallytrainedLM
gt 2 1
xn 1 as S(xn 1)= {s |xn 1∈prefix(s) }. willalwaysou| tput“B”. Infact,foranycontextstringthat
endswithtoken“A”,e.g“AAA”and“B A”(tokensare
Tokenization Setting. We assume having a predefined
| |
separatedby“”),suchLMwillalwaysoutput“B”.
vocabulary constructedusinganytokenizationalgorithm
|
V
suchasByte-Pair-Encoding(BPE)orLempel-Ziv-Welch SincethisappliestoanyoptimallytrainedLM,increasing
(LZW),withtheconditionthat . Weusettodenotea the training set size does not mitigate this problem. The
A⊆V
tokenin ,i.e.t .Importantly,weusethelongestprefix reasonforthissamplingbiasisthat,duringthetokenization
V ∈V
matchingstrategyfortokenization(encoding),denotedas processwithlongestprefixmatching,thetoken“A”must
encode(.),similartotheapproachusedintheWordpiece be followed by the token “B”. Otherwise, the encoding
algorithm(Devlinetal.,2018;Songetal.,2020). Givena process will merge to create a longer token “AA”. To
sequenceoftokenstk 1, thefunctiondecode(.)returnsthe generalize this phenomenon, we start with the definition
concatenatedstringresultingfromprocessingeachtokenin ofinvalidencodings.
thesequence. Finally,thesetofallstringsthatstartswith
Definition2.2. (InvalidEncodings)Thelistoftokens(an
thetokenstk isdefinedas (tk)= stk=encode(s)k .
1 S 1 { | 1 1} encoding)ti 1 isinvalidifencode(decode(ti 1)) ̸=ti 1. Other-
Tokenized LMs. We assume having access to a tok- wise,itisavalidencoding.
enizedautoregressiveLMwithparametersθthatistrained
For example, let = “c”,“a”,“t”,“at”,“cat” then
with tokens from and maximum prefix matching. The V { }
V [“c”,“at”,“t”]and[“c”,“a”,“t”,“t”]areinvalidencodings
targetdistributionsonthecharacterdomainisdenotedas
of“catt”. WenowshowinProposition2.3thattheexis-
P (xN xn)andonthetokendomainisP (t ti). For
gt n+1| 1 gt i+1 | 1 tenceofinvalidencodingsintroducessamplingbias,gen-
simplicity, unless otherwise stated, we implicitly assume
eralizing the observed phenomenon in the Markov chain
eachprobabilityterminvolvesθ. Usingthemodel,weas-
exampletoanyautoregressivedistribution.
sumethatonecancomputeP(t ti)foranyintegeri>0.
i+1 | 1
Inthiswork,weconsiderLMstrainedunderthestandard 2For example, we have P (t =“AA”|t =“AA”) +
gt i+1 i
setup,whereeachstringsinthedatasetisfirsttokenized P (t =“A”|t =“AA”)=α=P (x =“A”|x =“A”)
gt i+1 i gt n+1 n
2UnderstandingandMitigatingTokenizationBiasinLanguageModels
(1−α)2 α 1−β
AA B
TokenVocabulary (1−α)β
α
ID Token 0.0 αβ
1−α A B 1−β α(1−α) 1.0
1 A
β 2 B A
3 AA
BeforeTokenization 0.0
InputString:“AABABAAAABAAB”| −→ WordPieceEncoding −→ OutputTokens:“AA|B|A|B|AA|AA|B|AA|B”
Figure1:Next-CharactersamplingbiasintroducedbytheWordPieceencodingalgorithm.Inthisexample,giventhecontexttoken“A”,
themodelwillalwayspredictthenexttokenas“B”withprobability1.0.Wepresentatechniquethat,givenalanguagemodeltrainedon
tokenizeddomain,eliminatethisbiasandrecovertheaccurateunbiasedsamplingdistribution.
Proposition 2.3. (Token-Induced Zero Probability) Let this corresponds to the tokens “AA” and “B”. Also, we
ti be a sequence of input tokens. For any invalid en- assume that any string xN has the first token t ∗3.
1 1 1 ∈ V
coding ti, we have P (ti)=0.0 and the conditional Considertheinputstringxn anditscorrespondingencod-
1 gt 1 1
probability P (t ti) is undefined. In the case ti ing ti=encode(xn), Proposition 3.1 shows the sufficient
is valid, thengt P i+ (t1 | 1 ti)=0.0 if ti+1 is invalid. Fur1 - condi1 tionfor (ti1 )= (xn).
gt i+1 | 1 1 S 1 S 1
thermore, let xn 1=decode(ti 1), then for any string xN n+1 Proposition3.1. Lets∗ =xn,whereti =encode(s∗)=
s Pu gc th
(xN
nth +a 1t
|ti
1e )n =co 0d .0e .(concat(decode(ti 1),xN n+1)) ̸=ti 1, then e stn rc ino gde s(x wn
1
h) e. reT th
i
1en =w ee nch oa dv ee (sS1 )(
i
1t ,i
1
w)
e⊂ haS
v1 ( ex Pn 1) (, xi
n
1.e |t.
i
1)fo =r a 1n .0y
.
Inthecaset ∗,thenwealsohavethat (ti)= (xn),
Proof. SeeAppendixC. i ∈V S 1 S 1
i.e. anystringswherexn prefix(s)musthavethefirsti
Remark1. Proposition2.3impliesthatLMsmaynotfunc- tokensasti andP(ti xn1 )∈ =1.0.
tion as expected when presented with invalid encodings,
1 1| 1
becausethesemodelswillneverbeexposedtosuchinputs Proof. SeeAppendixD.
withinthedataset. Thisdirectlyimpliesthatthepracticeof
TheintuitionforProposition3.1isthatthesubsequentstring
evaluatingLMsunderdifferentencodings(Cao&Rimell, aftert ∗cannotchangethetokenizationforxn. Wenow
2021;Chirkovaetal.,2023)issuboptimal. i ∈V 1
establishoneofthemainresultsinCorollary3.2.
3.AlleviatingSamplingBias Corollary3.2. FollowingProposition3.1,supposet ∗
i
∈V
We propose a method to remove the described bias and thenwehaveP(xN xn)=P(xN ti). Similarly,wealso
n+1| 1 n+1| 1
recover the original token-free autoregressive model, i.e. haveP(tj xn)=P(tj ti).
expressing the implicitly learned P(xN xn) using the
i+1| 1 i+1| 1
n+1| 1 Proof. SeeAppendixD.
tokenized LM that outputs the conditional probability
P(t i+1 |ti 1). For N=n+1, this captures the behavior of WenotethatProposition3.1andCorollary3.2alwayshold,
atoken-freemodel,i.e. samplingthenextcharacterinstead regardless of the value of θ. In general, consider when
ofawholetoken. WeassumeourLMfollowsProposition thelasttokenofencode(xn)isnotin ∗,wecanrefactor
2.3 on zero probability events and undefined conditional P(xN xn)asfollow: 1 V
probabilityforinvalidencodings. AppendixGjustifiesthis
n+1| 1
P(xN tk)
assumptionandprovidesitspracticalimplementation. P(xN xn)= nk+1| 1 , (1)
n+1| 1 P(xn tk)
nk+1| 1
Our method consists of two stages. In the first stage,
the idea is to identify the condition when P(xN ti) = wherekisthelasttokeninencode(xn)suchthatt ∗and
P(xN xn)whereti =encode(xn). Onceidenn t+ ifi1| ed1 ,we xnk=decode(tk),wheren n. Pr1 oofdetailsok f∈ thV isstep
n+1| 1 1 1 1 1 k ≤
canrefactortheconditionalprobabilitytomatchthecondi- canbefoundintheAppendixE.WethenusetheBPTree
tioningevents.Inthesecondstage,wecomputeP(xN ti) algorithmtocomputeeachtermintheRHSindividually.
n+1| 1
usingtheLMoutputprobability,i.e. P(t ti),through
i+1 | 1 3.2.TheBranchandPassAlgorithm
thenovelBranchandPass(BPTree)Algorithm.
We present the BPTree algorithm in Algorithm 1, that
3.1.Refactoring allows us to compute the probabilities P(xN tk) and
nk+1| 1
P(xn tk)inEquation(1). Notethatthisalgorithmdoes
Our method removes the bias by connecting character nk+1| 1
notrequiret ∗. Detailsonthealgorithmiccorrectness
and token domains through a special subset of tokens k
∈V
∗ , whose elements t∗ ∗ are not a substring of areshowninAppendixE.
V ⊂V ∈V
any other tokens in but itself. For example, given
V
3Thisisnotrestrictive,asmanycurrentlanguagemodelsstarts
= “AAA”,“AA”,“CB”,“A”,“B”,“C” , then ∗ = withaspecialbeginningtokeninV∗,e.g.thestarttoken<start>
V { } V
“AAA”,“CB” . In the Markov example in Section 2, inSentencePiece(Kudo&Richardson,2018).
{ }
3UnderstandingandMitigatingTokenizationBiasinLanguageModels
Algorithm1BranchandPassAlgorithmonDiscreteProbability
Vocabulary: 𝒱 = { 𝑏 , 𝑒 , 𝑝 , 𝑟 , 𝑛 , 𝑒 𝑝 , 𝑒 𝑒 𝑛 , 𝑏 𝑒 𝑒 𝑟 } Language Model: 𝑃(𝑡 !"#|𝑡 #!)
Tree.ThisalgorithmrecursivelycomputesP(xN |tk). Encoding Rule: Maximum Prefix Matching
nk+1 1
Recursive Recursive Base
1 2:
:
proc //e Bd ru ar ne cB hiP nT gR SE teE p( :xN nk+1,tk 1) Q Inu pe ur
t
y
T
S oktr ein ng s:
:
“
𝑡
𝑏
#&
𝑒 𝑒 ” 𝑏C 𝑒al 𝑒l 𝑟1 C 𝑒a 𝑒ll
𝑛
2 C 𝑒a 𝑒s 𝑛e
53 4 :: : /B b /v Ba= l as={ et Ct(cid:80)∈
∈ aB
sV P e| :x (tN n kk ++ 11 =∈p tr (cid:12) (cid:12)e tfi k 1)x(decode(t))} *𝑃O *( Nu 𝑥t op $$ tu
!
e! ""t
*
*#P %
:
r O=ob n“a l𝑏 yb 𝑒 ti ol 𝑒it k”y | e: 𝑡 n#& s) Branch b val Branch b val
Br
Ban rc ah
nch
𝑒𝑝
6: ifencode(xN
i
)∈Vthen selectedbythe BPTree 𝑡 !" Pass 𝑏 Pass 𝑒 Branch 𝑒
7 8:
:
endr ie fturnb val a el ag co hr i rth em cu ra sr ie ves h co aw ll.n in P Tr oe kv eio nu ss p val p val b val
9: //ExtracttheNextToken: Figure2:BPTreeVisualization.Ateachrecursivecall,theBranch
10: t =encode(xN ) stepfindstokensthatstartswiththequerystringwhilethePass
11:
//k P+ a1
ssingStep:
nk+1 1
stepextractsandemploysthenexttokenandleftoverstringforthe
12: p
val
=P(t k+1(cid:12) (cid:12)tk 1) nextrecursivecalluntilmeetingthebasecase.
1 13 4:
:
p rev ta ul r= nbp vv aa ll +× pB vP aT lree(xN nk+1+1,tk 1+1) 01 .. 80 G Or uo ru Mn ed thTr ou dt :h: PPg (t x( nx +n 1+ |1 x|n 1x )n n−2) Baseline:P(xn+1|ti 1)
15: endprocedure
0.6
TheideaistomarginalizeoutP(xN tk)byconsidering 0.4
nk+1| 1
twocomplementaryevents: whenthenexttokent k+1hasa 0.2
prefixxN (b intheBranchStep)versuswhenthenext
t Fo ok re mn at lk ln y+ ,k 1+ Bi1 s PTco rv ena etl a cin oe md pw ui tt eh sin thx eN n fk o+ ll1 ow(p iv na gli pn roth be abP ia lis ts ieS st :ep). Figu0 r.0 e3:A OA uA rmeA tA hB odacAB cA urateA lI yB npB eu st tS it maB te aA s tA esthB eAB transiB tB ioA nprB oB bB abil-
ityofa3rdorderMarkovchainwhilethebaselinemethodfailsto.
b
val
=P(xN nk+1,t
k+1
∈B(xN nk+1))(cid:12) (cid:12)tk 1), (2)
= “A”,“B”,“AA”,“BAAB”,“BBAA”,“BBBA”,
p val =P(xN nk+1,t k+1 ∈/ B(xN nk+1))(cid:12) (cid:12)tk 1), (3) “V BA{ ”,“BBA” }. We train a LM model using GPT-2
architecture with 6 hidden layers. Since the model is
where (xN )= t xN prefix(decode(t)) and
B nk+1 { ∈V| nk+1∈ } agnostic to the Markov chain order, we average the
weimmediatelyseethatP(xN tk)=b +p .
nk+1| 1 val val probability from 100 runs on different context length
Weprovideanintuitiveexplanationforthealgorithmfol- whilefixingthelast3characters. Wecompareourmethod
lowing the example in Figure 2. Here, we would like to withthebaselineestimatorP(x ti),equivalenttoone
computetheprobabilityP(xnk+3=“bee”tk).Thefirstpos- Branch step in the BPTree algn o+ ri1 t| hm1 . Figure 3 shows
nk+1 | 1
sibility is that “bee” is a prefix of the next token, so we theresultswherethebaselinemethodexhibitssignificant
searchforallsuchtokens(line3inthealgorithm)andsum samplingbiasduetotokenization. FollowingProposition
uptheirprobability(line4),i.e. b =P(t =“beer”tk). 2.3,onecanclarifythezeroprobabilityeventsoutputfrom
val k+1 | 1
Figure 2 visualizes this step as branching out the tree by thebaselineestimator. Ourmethod,incontrast,accurately
finding all tokens completing the string. Since “beer” is estimatesthegroundtruthprobabilityusedtogeneratethe
nottheonlystringthatcontains“bee”,e.g. “beep”,“been”, data, showing that it is possible to recover the implicitly
etc. we need to compute the probability for these other learnedcharacterinformationfromthetokenizedLMs.
scenarios,eachofwhichhast =“b”(thefirsttokenin
k+1 5.Conclusion
“bee”, line 10 and 12) due to maximum prefix encoding.
Then, we want to compute the probability that the subse- This work identifies the next-character sampling gap be-
quent string is “ee” (line 13), given the previous tk and tweenatokenizedmodelandatoken-freeone,whichper-
1
t =“b”,whichistheoutputoftheBPTreealgorithmbut sistsevenforoptimallytrainedmodels. Wepresentaproba-
k+1
for xnk+3=“ee” and tk+1. Formally, inthe Passingstep: bilisticapproachtoeffectivelyeliminatethisbiaswithout
p
=n Pk+ (t2 =“b”tk)P1
(xnk+3=“ee”tk,t =“b”). We requiringadditionaltraining. Thisclosesthesamplinggap
val k+1 | 1 nk+2 | 1 k+1 betweentokenizedandtoken-freemodels,suggestingthat
continuetheprocedureuntilmeetingthebasecase,where
languagemodelsimplicitlyabsorbcharacter-levelinforma-
thestringmustbeaprefixofthenexttoken(usually,when
tion despite being trained solely on tokenized text. This
thereisonlyasinglecharacterleft). Finally,bycomputing
resultimpliesthatitistheoreticallypossibletosimulatethe
thesumofthebranchandpasssteps,weobtainthedesired
conditionalprobabilityb +p =P(xnk+3=“bee”tk). behaviorofanotherlanguagemodeltrainedusingdifferent
val val nk+1 | 1 vocabularywithoutanyfine-tuning,sinceitispossibleto
4.Experiments transferfromtoken-freemodelstotokenizedcounterparts.
We validate our method on a 3rd order Markov chain Futureworkseekstoextendourinvestigationforalternative
experiment with = “A”,“B” , where we randomly encodingstrategies,mostnotablythewidelyusedBPEtok-
construct the traA nsit{ ion matrix} and the vocabulary enization(Sennrichetal.,2015),instate-of-the-artmodels.
4
txeNfoytilibaborP AgniebretcarahCUnderstandingandMitigatingTokenizationBiasinLanguageModels
References guidanceai. Guidanceai,2023. URLhttps://github.
com/guidance-ai/guidance. GitHubrepository.
Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Gutierrez-Vasques,X.,Bentz,C.,andSamardzˇic´,T. Lan-
Askell,A.,etal. Languagemodelsarefew-shotlearners.
guages through the looking glass of bpe compression.
Advancesinneuralinformationprocessingsystems,33:
ComputationalLinguistics,49(4):943–1001,2023.
1877–1901,2020.
Kudo, T. and Richardson, J. Sentencepiece: A sim-
Cao,K.andRimell,L. Youshouldevaluateyourlanguage
ple and language independent subword tokenizer and
modelonmarginallikelihoodovertokenisations. InPro-
detokenizer for neural text processing. arXiv preprint
ceedingsofthe2021ConferenceonEmpiricalMethods
arXiv:1808.06226,2018.
inNaturalLanguageProcessing,pp.2104–2114,2021.
Chen, Y., Marchisio, K., Raileanu, R., Adelani, D., Liu,S.,Deng,N.,Sabour,S.,Jia,Y.,Huang,M.,andMihal-
Saito Stenetorp, P. L. E., Riedel, S., and Artetxe, M. cea,R. Task-adaptivetokenization: Enhancinglong-form
Improvinglanguageplasticityviapretrainingwithactive textgenerationefficacyinmentalhealthandbeyond. In
forgetting. AdvancesinNeuralInformationProcessing The2023ConferenceonEmpiricalMethodsinNatural
Systems,36:31543–31557,2023. LanguageProcessing,2023a.
Chirkova,N.,Kruszewski,G.,Rozen,J.,andDymetman,
Liu, Y., Lin, P., Wang, M., and Schu¨tze, H. Ofa: A
M. Shouldyoumarginalizeoverpossibletokenizations?
frameworkofinitializingunseensubwordembeddingsfor
InThe61stAnnualMeetingOfTheAssociationForCom-
efficientlarge-scalemultilingualcontinuedpretraining.
putationalLinguistics,2023.
arXivpreprintarXiv:2311.08849,2023b.
Cleary,J.andWitten,I. Datacompressionusingadaptive
Makkuva,A.V.,Bondaschi,M.,Girish,A.,Nagle,A.,Jaggi,
codingandpartialstringmatching. IEEEtransactionson
M., Kim, H., and Gastpar, M. Attention with markov:
Communications,32(4):396–402,1984.
Aframeworkforprincipledanalysisoftransformersvia
Cognetta,M.,Zouhar,V.,Moon,S.,andOkazaki,N. Two markovchains. arXivpreprintarXiv:2402.04161,2024.
counterexamplesto textit TokenizationandtheNoise-
\ {
lessChannel . arXivpreprintarXiv:2402.14614,2024. Minixhofer,B.,Ponti,E.M.,andVulic´,I. Zero-shottok-
}
enizertransfer. arXivpreprintarXiv:2405.07883,2024.
Dagan, G., Synnaeve, G., and Rozie`re, B. Getting the
mostoutofyourtokenizerforpre-traininganddomain
Nawrot,P.,Chorowski,J.,Łan´cucki,A.,andPonti,E.M.
adaptation. arXivpreprintarXiv:2402.01035,2024.
Efficienttransformerswithdynamictokenpooling. arXiv
Devlin,J.,Chang,M.-W.,Lee,K.,andToutanova,K. Bert: preprintarXiv:2211.09761,2022.
Pre-training of deep bidirectional transformers for lan-
Petrov,A.,LaMalfa,E.,Torr,P.,andBibi,A. Language
guageunderstanding. arXivpreprintarXiv:1810.04805,
modeltokenizersintroduceunfairnessbetweenlanguages.
2018.
AdvancesinNeuralInformationProcessingSystems,36,
Galle´,M. Investigatingtheeffectivenessofbpe: Thepower 2024.
ofshortersequences. InProceedingsofthe2019confer-
enceonempiricalmethodsinnaturallanguageprocess- Provilkov,I.,Emelianenko,D.,andVoita,E. Bpe-dropout:
ingandthe9thinternationaljointconferenceonnatural Simple and effective subword regularization. arXiv
languageprocessing(EMNLP-IJCNLP),pp.1375–1381, preprintarXiv:1910.13267,2019.
2019.
Rajaraman, N., Jiao, J., and Ramchandran, K. To-
Gemini,T. Gemini: afamilyofhighlycapablemultimodal
ward a theory of tokenization in llms. arXiv preprint
models. arXivpreprintarXiv:2312.11805,2023.
arXiv:2404.08335,2024.
Goldman,O.,Caciularu,A.,Eyal,M.,Cao,K.,Szpektor,I.,
Schmidt, C. W., Reddy, V., Zhang, H., Alameddine, A.,
andTsarfaty,R. Unpackingtokenization: Evaluatingtext
Uzan,O.,Pinter,Y.,andTanner,C. Tokenizationismore
compressionanditscorrelationwithmodelperformance.
than compression. arXiv preprint arXiv:2402.18376,
arXivpreprintarXiv:2403.06265,2024.
2024.
Goyal,S.,Ji,Z.,Rawat,A.S.,Menon,A.K.,Kumar,S.,and
Nagarajan,V.Thinkbeforeyouspeak:Traininglanguage Sennrich,R.,Haddow,B.,andBirch,A. Neuralmachine
modelswithpausetokens. InTheTwelfthInternational translation of rare words with subword units. arXiv
ConferenceonLearningRepresentations,2023. preprintarXiv:1508.07909,2015.
5UnderstandingandMitigatingTokenizationBiasinLanguageModels
Singh,A.K.andStrouse,D. Tokenizationcounts: theim-
pactoftokenizationonarithmeticinfrontierllms. arXiv
preprintarXiv:2402.14903,2024.
Song, X., Salcianu, A., Song, Y., Dopson, D., and
Zhou, D. Fast wordpiece tokenization. arXiv preprint
arXiv:2012.15524,2020.
Tay, Y., Tran, V. Q., Ruder, S., Gupta, J., Chung, H. W.,
Bahri,D.,Qin,Z.,Baumgartner,S.,Yu,C.,andMetzler,
D. Charformer: Fastcharactertransformersviagradient-
basedsubwordtokenization. InInternationalConference
onLearningRepresentations,2021.
Touvron,H.,Lavril,T.,Izacard,G.,Martinet,X.,Lachaux,
M.-A.,Lacroix,T.,Rozie`re,B.,Goyal,N.,Hambro,E.,
Azhar,F.,etal. Llama:Openandefficientfoundationlan-
guagemodels. arXivpreprintarXiv:2302.13971,2023.
Willems, F.M., Shtarkov, Y.M., andTjalkens, T.J. The
context-treeweightingmethod: Basicproperties. IEEE
transactionsoninformationtheory,41(3):653–664,1995.
Xue,L.,Barua,A.,Constant,N.,Al-Rfou,R.,Narang,S.,
Kale, M., Roberts, A., and Raffel, C. Byt5: Towards
a token-free future with pre-trained byte-to-byte mod-
els. TransactionsoftheAssociationforComputational
Linguistics,10:291–306,2022.
Yu,L.,Simig,D.,Flaherty,C.,Aghajanyan,A.,Zettlemoyer,
L., and Lewis, M. Megabyte: Predicting million-byte
sequences with multiscale transformers. Advances in
NeuralInformationProcessingSystems,36,2024.
Zouhar, V., Meister, C., Gastaldi, J., Du, L., Sachan, M.,
andCotterell,R. Tokenizationandthenoiselesschannel.
InProceedingsofthe61stAnnualMeetingoftheAsso-
ciationforComputationalLinguistics(Volume1: Long
Papers),pp.5184–5207,2023.
6UnderstandingandMitigatingTokenizationBiasinLanguageModels
A.RelatedWork
TheoryofTokenization. Existingworksontokenizationgenerallysupporttheideathatcompressingtokensenhances
modelperformance(Galle´,2019;Gutierrez-Vasquesetal.,2023;Zouharetal.,2023). However,theseempericallyfindings
areinconflictedwithotherlaterstudiesCognettaetal.(2024);Schmidtetal.(2024). Onthetheoreticalside,Rajaraman
etal.(2024)examinedtokenizationthroughthelensofunigrammodels,motivatedbytheobservationmadebyMakkuva
etal.(2024)thattransformersstrugglestolearn2nd-orderMarkovchains. We,however,donotobservethisphenomenonin
ourexperiment. Assuch,ourworkonbiasduetotokenizationisnotaffectedbytheirobservation.
Tokenization and Perplexity. Our work relates to the statistical evaluation of LMs, where we provide an algorithm
todirectlyevaluatethecharacter-levelperplexityp(xN xn),usingatokenizedLM.Intermsoftoken-levelperplexity
n+1| 1
evaluation,somerecentstudies(Cao&Rimell,2021;Chirkovaetal.,2023)havesuggestedusingstochastictokenization
(Provilkovetal.,2019)attesttimetoevaluateperplexityscoresofLMs(p(ti)). However,theseevaluationsweredoneon
1
LMstrainedwithdeterministictokenizationwhichcouldbesuboptimalasdemonstratedbyourexaminationofundefined
statesinSection2. Assuch,byutilizingourapproach,onecanobtainamuchmoreaccurateinsightsonLMsevaluation.
RelatedAlgorithms. Ouralgorithmisinspiredfromtheliteratureofuniversalcompressionsuchaspredictionbypartial
matching(Cleary&Witten,1984)andcontext-treeweighting(Willemsetal.,1995), whichhavebeenappliedfortext
predictionbutformuchsimplersettingswithoutanytokenizationinvolved. Recently,Minixhoferetal.(2024);Liuetal.
(2023a)proposetokenizationadaptationmethods,whichstillrequiresaheuristicoptimizationthatcomplicatesthetraining
pipeline. Somerecentstudieshaveproposedmethodtotargettheproblemoflanguagemodelsencounteringdifficulties
generating text nearprompt boundaries (Daganet al., 2024;guidance ai, 2023), which bears someresemblance to our
proposedalgorithm. Thesemethods,however,areheuristicandonlyapplicabletocertainscenarios. Ontheotherhand,our
biasremovalalgorithmistheoreticallycorrect,versatileforvarioussituations,andenablesconversionbetweentoken-free
andtokenizedLMsduetoitsaccuraterepresentationofconditionalsamplingdistributions.
B.SupportingTheoremsonTokenization
Thissectionprovidessupportingtheoremsfortheproofofthemainresults. Wefirstremindthereadersthattheset (xn)
S 1
correspondstothesetofallstringsthatcontainsxnasaprefix. Similarly,theeventset (ti)correspondstothesetofall
1 S 1
stringswhosefirstitokensareti. Considerwhenti =encode(xn),itshouldbenotedthatthetwosetsS(ti)andS(xn)
1 1 1 1 1
arenotguaranteedtobeequivalent. Thatisbecausethesubsequentcharactersafterxncanaffectthetokenizationwithinthe
1
firstncharacter. Weillustratethisinmoredetailsinthefollowingexample.
Example. Consider the Markov chain example in Section 2, where = “AA”,“A”,“B” . Then, the string s =
1
V { }
“AABAABAB”,thens (x =“A”)ands (t =“AA”)sincethefirstcharacterofs is“A”andthefirsttoken
1 1 1 1 1
∈S ∈S
ofs is“AA”. Ontheotherhand,s / (t =encode(x )=“A”)sinceitsfirsttokenis“AA”,not“A”.
1 1 1 1
∈S
Weintroducethefollowingpropositionthatcontainstwofactsregardingthemaximumprefixencodingprocess.
PropositionB.1. Letsbeastringwiththeprefixxn(xn prefix(s)). Definetheminimalsuperstringrtobetheprefixof
1 1 ∈
swiththefewesttokensthatcontainsxn asaprefix: r =argmin (k tk =encode(r) xn prefix(r) r prefix(s)).
1 r | 1 ∧ 1 ∈ ∧ ∈
Then,wehavethefollowings:
1. For1 i<k,encode(s) =encode(xn) . Furthermore,whenr =xn,wealsohaveencode(s) =encode(xn) .
≤ i 1 i 1 k 1 k
2. Letℓbethenumberoftokensinencode(xn),thenwehavedecode(encode(xn)ℓ) prefix(decode(encode(s) )).
1 1 k ∈ k
Proof. (Result1.) Proofbycontradiction. Letsbethecounter-examplewiththefewestnumberoftokens. Assumethatfor
1 i<k,encode(s) =encode(xn) . Letj bethesmallestofsuchi.
≤ i ̸ 1 i
Considerencode(s) andencode(xn) .
j 1 j
• Case1: decode(encode(s)j) < xn .
| 1 | | 1|
– Case1.a: decode(encode(s) ) < decode(encode(xn) ). Thisleadstoacontradiction,sincexn isaprefixofs,
| j | | 1 j | 1
thereforealongestprefixmatchingalgorithmwouldalwaysgeneratethelongertoken(encode(xn) )overtheshorter
1 j
one(encode(s) )whenitisavailable.
j
– Case1.b: decode(encode(s) ) > decode(encode(xn) ). Thisleadstoacontradiction,sinceconcat(encode(s)j)
| j | | 1 j | 1
7UnderstandingandMitigatingTokenizationBiasinLanguageModels
isaprefixofxn(Case1assumption),thereforealongestprefixmatchingalgorithmwouldalwaysgeneratethelonger
1
token(encode(s) )overtheshorterone(encode(xn) )whenitisavailable.
j 1 j
– Case1.c: decode(encode(s) ) = decode(encode(xn) ).Thismeansthatthetwotokensarethesame,contradicting
| j | | 1 j |
ourinitialassumption.
• Case2: decode(encode(s)j) xn . Inthiscase,r =decode(encode(s)j)isasuperstringofxnimplyingthatkisat
| 1 |≥| 1| 1 1
mostj,whichcontradictsourinitialassumptionthat1 j <k.
≤
Finally,inthecaser =xn,thismeansdecode(encode(s) )isasuffixofxn. Sinceallthetokensbeforekwithinxnhas
1 k 1 1
beenmatched,i.e. encode(s) =encode(xn) for1 i<k,thelasttokenmustalsomatchastheresult(else, r = xn ,
i 1 i ≤ | |̸ | 1|
leadstocontradiction),wehaveencode(s) =encode(xn) .
k 1 k
(Result 2.) The proof idea is that since r contains xn and any tokens within r and xn has been matched up to k 1,
1 1 −
then what is left in xn must be in the last token in r (which is the kth token of r). Formally, following Result 1,
1
we have decode(encode(xn)k−1) = decode(encode(s)k−1). Since r has k tokens in total and xn prefix(r), this
1 1 1 1 ∈
means that decode(encode(s) ) must cover the rest of xn, i.e. decode(encode(xn)ℓ). As the result, we must have
k 1 1 k
decode(encode(xn)ℓ) prefix(decode(encode(s) )).
1 k ∈ k
Weremindthereaderthedefinitionofinvalidencodingbelow.
DefinitionB.2. (InvalidEncodings)Thelistoftokens(anencoding)tk isinvalidifencode(decode(tk))=tk. Otherwise,it
1 1 ̸ 1
isavalidencoding.
CorollaryB.3. (tk)= ifandonlyiftk isinvalid.
S 1 ∅ 1
Proof. Weproveeachdirectionasfollow.
• If (tk) = thentk isinvalid: Since (tk) = ,weknowthatthereexistnostringssuchthatencode(s)k = tk. As
S 1 ∅ 1 S 1 ∅ 1 1
such,fors=decode(tk),wedonothaveencode(decode(tk))=tk,whichprovestheresult.
1 1 1
• Iftk isinvalidthen (tk) = : Weprovethisbyshowingthatitisimpossibletohaveastringwithprefixdecode(tk)
1 S 1 ∅ 1
whosefirstktokensaretk. Lets¯=decode(tk),wehaveanystrings (tk)musthaves¯ prefix(s). Assumesuch
1 1 ∈S 1 ∈
stringsexists,wehaveencode(s)k =tk. Letxn =decode(tk),accordingtoPropositionB.1,wehaver =s¯andasthe
1 1 1 1
result,for1 i k,wehaveencode(s) =t =encode(xn) ,whichcontradictsourinitialassumption. Thiscompletes
≤ ≤ i i 1 i
theproof.
C.ProofofProposition2.3intheMainPaper
Proposition2.3 (Token-InducedZeroProbability)Letti beasequenceofinputtokens.Foranyinvalidencodingti,wehave
1 1
P (ti)=0.0andtheconditionalprobabilityP (t ti)isundefined. Inthecaseti isvalid,thenP (t ti)=0.0ifti+1
gt 1 gt i+1 | 1 1 gt i+1 | 1 1
isinvalid. Furthermore,letxn=decode(ti),thenforanystringxN suchthatencode(concat(decode(ti),xN ))=ti,
1 1 n+1 1 n+1 ̸ 1
thenP (xN ti)=0.0.
gt n+1| 1
Proof. Forthefirsttwostatements,wehave:
• Foraninvalidti whereti =encode(decode(ti)),wehave (ti)= ,asimpliedbyCorollaryB.3. Assuch,wehave
1 1 ̸ 1 S 1 ∅
P (ti)=0.0whichleadsP (t ti)tobeanundefinedconditionalprobability.
gt 1 gt i+1 | 1
• Foravalidti butinvalidti+1,weknowthatP (ti+1)=0.0,whichresultsinP (t ti)=0.0.
1 1 gt 1 gt i+1 | 1
Forthelaststatement,wefirstnotethefollowing:
1. Lets′ =decode(ti),notethatP (xN ,ti)=P (s′,xN ,ti)=P (s¯,ti)wheres¯=concat(s′,xN )=xN.
1 gt n+1 1 gt n+1 1 gt 1 n+1 1
2. ConsiderP (s¯,ti)=P (s¯)P (ti s¯),wewillprovethatP (ti s¯)=0.0ifencode(s¯)i =ti.
gt 1 gt gt 1| gt 1| 1 ̸ 1
Theideaisthat,whenweappendadditionalcharacterstos¯,itwillneverchangethetokenizationsuchthatthefirstitokens
areti. Formally:
1
8UnderstandingandMitigatingTokenizationBiasinLanguageModels
• Letj bethefirstpositionsuchthatencode(s¯) =t thenweknowthat decode(encode(s¯) )) > decode(t ) (Proposi-
j j j j
̸ | | | |
tionB.1(Result2)).
• Following Proposition B.1 (Result 2), let s∗ (s¯), then we know that decode(encode(s¯) ) must be a substring of
j
∈ S
withinanotherlongertoken(itcannotbebrokendown). Thiscompletestheproof.
Finally,wenotethatP (ti)=0.0doesnotimpliesencode(decode(ti)) ,sinceitcanbeduetotheoriginaldistribution
gt 1 1 ∈V
onthecharacterdomain. AclassicexampleforthisisaMarkovmodelwithanabsorptionstate.
D.ProofofProposition3.1andCorollary3.2intheMainPaper
Proposition3.1Lets∗ = xn,whereti = encode(s∗) = encode(xn). Thenwehave (ti) (xn),i.e. foranystring
1 1 1 S 1 ⊂ S 1
swhereti =encode(s)i,wehaveP(xn ti)=1.0. Inthecaset ∗,thenwealsohavethat (ti)= (xn),i.e. any
1 1 1| 1 i ∈V S 1 S 1
stringswherexn prefix(s)musthavethefirstitokensasti andP(ti xn)=1.0.
1 ∈ 1 1| 1
Proof. Weproveeachcaseasfollow.
1)GeneralCase: Sincethereexistsastringswithprefixxnwhereencode(s)i =ti,whichfollowsdirectlyfromour1st
1 1 ̸ 1
orderMarkovchainexampleinthemainpaper,i.e. thestrings=“AA”has“A”asprefixbuthavethet =“AA”=“A”.
1
̸
Ontheotherhand,weknowthatanystringsthathasthefirstitokensasti musthavethefirstncharactersasxn,assuch
1 1
(ti) (xn)andP(xn ti)=1.0.
S 1 ⊂S 1 1| 1
2)t ∗: Theproofideaisthat,sincet cannotbeapartofanytokenin ,itisimpossibletomergeitbyappending
i i
∈ V V
additionalcharactersaftert . Formally,similartoPropositionB.1:
i
• Foranystrings (decode(ti)),letℓbethenumberoftokensintheminimalsuperstringrofsthatcontainsxnasa
∈S 1 1
prefix.
• FollowingPropositionB.1(Result2),weknowthatt mustbeasubstringofdecode(encode(s) ).
i ℓ
• Duetot ∗,thent =encode(s) . WealsoknowfromPropositionB.1(Result1)thatencode(s) =t for1 i<ℓ,
i i ℓ i i
∈V ≤
thismeansthatℓ=i. Thisgivesusti =encode(s)i andP(ti xn)=1.0.
1 1 1| 1
Thiscompletestheproof.
Remarks. Webrieflynotethattheconditiont ∗ isthesufficientcondition. Ingeneral,anytokensequenceti that
i ∈ V 1
satisfies (ti) = (xn)willhaveP(ti xn) = 1.0. Developinganefficientalgorithmtoverifywhether (ti) = (xn)
S 1 S 1 1| 1 S 1 S 1
remainsataskforfutureresearch.
Corollary3.2 FollowingProposition3.1,supposet ∗ thenwehaveP(xN xn)=P(xN ti). Similarly,wealso
i ∈ V n+1| 1 n+1| 1
haveP(tj xn)=P(tj ti).
i+1| 1 i+1| 1
Proof. Forthefirstcase,weprovethroughthefollowingequations:
P(xN ti)=P(xN ti,xn) (4)
n+1| 1 n+1| 1 1
P(xN ,ti xn)
= n+1 1| 1 (5)
P(ti xn)
1| 1
P(xN xn)P(ti xn,xN )
= n+1| 1 1| 1 n+1 (6)
P(ti xn)
1| 1
=P(xN xn) (7)
n+1| 1
wherethefirstequalityisduetoP(xn ti)=1.0andthelastequalityisduetoP(ti xn)=1.0fort ∗.
1| 1 1| 1 i ∈V
9UnderstandingandMitigatingTokenizationBiasinLanguageModels
Similarly,forthesecondcase,wehave:
P(tj ti)=P(tj xn,ti) (8)
i+1| 1 i+1| 1 1
P(tj xn)P(ti xn,tj )
= i+1| 1 1| 1 i+1 (9)
P(ti xn)
1| 1
=P(tj xn), (10)
i+1| 1
whichcompletestheproof.
E.ProofforTheBiasRemovalMethod
E.1.Refactoring
OurgoalistoexpressthequantityP(xN xn)usingthetokenizedLMthatoutputstheconditionalprobabilityP(t ti−1).
Letxnk prefix(xn)wheretk =encon d+ e1 (| xn1 k)andt ∗. FollowingProposition3.1,anystringswithprefixxni k| m1 ust
1 ∈ 1 1 1 k ∈V 1
havethefirstktokensastk. Wenowperformthefollowingfactorization:
1
P(xN xn)=P(xN xnk,xn ) (11)
n+1| 1 n+1| 1 nk+1
P(xN ,xn xnk)
= n+1 nk+1| 1 (12)
P(xn xnk)
nk+1| 1
P(xN xnk)
= nk+1| 1 (13)
P(xn xnk)
nk+1| 1
P(xN tk)
= nk+1| 1 , (14)
P(xn tk)
nk+1| 1
wherethelastinequalityisduetoCorollary3.2. Finally,wewillusetheBranchandPassAlgorithmtocomputeeachterm
in(14)individually. Notethatthealgorithmdoesnotrequiret ∗. Here,weexplicitlyhighlighttheimportanceof
k
∈ V
havingt ∗,asitbridgesbetweenthecharacterandtokendomainthroughEquation(14).
k
∈V
E.2.TheBranchandPassAlgorithm
Overview. TheBranchandPass(BPTree)algorithmhastwoinputs: thequerystringvaluexN andtheconditioning
nk+1
tokenstk,whichoutputsthevalueP(xN tk). Notethatwedonotrequiret ∗intheBPTreealgorithm. First,using
1 nk+1| 1 k ∈V
marginalization,wehavethefollowing:
(cid:88)
P(xN tk)= P(xN ,t =ttk) (15)
nk+1| 1 nk+1 k+1 | 1
t∈V
(cid:88) (cid:88)
= P(xN ,t =ttk)+ P(xN ,t =ttk) (16)
nk+1 k+1 | 1 nk+1 k+1 | 1
t∈Tbval t∈Tpval
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
bval pval
where = tt xN prefix(decode(t)) and = tt xN /prefix(decode(t)) ,where =
Tbval { | ∈V∧ nk+1∈ } Tpval { | ∈V∧ nk+1∈ } Tbval∩Tpval
,arethecorrespondingsetweiteratesovereachsuminthebranchstep,i.e. b ,andpassstep,i.e. p ,respectively.
val val
∅
Intuitvely, isthesetoftokensthathaveaprefixxN and istheonesthatdonot. Wenowshowhowtocompute
Tbval nk+1 Tpval
eachtermseparatelyusingatokenizedLMthatoutputsP(t ti).
i+1 | 1
BranchStep. Notethatb canbedescribedastheprobabilitythat,giventhelistofprevioustokenstk,thenexttokenof
val 1
thestringshasxN asaprefix. Assuch,toobtainb ,wefirstcomputeP(t =ttk)forallt usingonemodel
nk+1 val k+1 | 1 ∈V
run,thensumtheprobabilitiescorrespondstoalltokenswhoseprefixisxN . Specifically:
nk+1
(cid:88)
b = P(t =ttk), (17)
val k+1 | 1
t∈Tbval
10UnderstandingandMitigatingTokenizationBiasinLanguageModels
Proof. Toseethis,foreachsummandofb inEq.(16),wehave:
val
P(xN ,t =ttk)=P(t =ttk) P(xN tk,t =t) (18)
nk+1 k+1 | 1 k+1 | 1 × nk+1| 1 k+1
=P(t =ttk), (19)
k+1 | 1
wherethesecondequationisduetoxN prefix(t)soP(xN tk,t =t)=1.0. Thisconcludestheproof.
nk+1∈ nk+1| 1 k+1
PassStep. Incontrasttob ,thep istheprobabilitythat,giventhelistofprevious(valid)tokenstk,thestringshasthe
val val 1
subsequentstringasxN thatisnotaprefixofthenexttoken. Wewillshowthatunderthemaximumprefixencoding,we
nk+1
computethevaluep asfollow:
val
p =P(t =ttk) P(xN tk,t =t), (20)
val k+1 | 1 × nk+1+1| 1 k+1
wheret=encode(xN ) andxnk+1 =decode(t). Thatis,duringthepassingstep,therearetwosubroutines:
nk+1 1 nk+1
1. ObtainthenexttokentwithinxN andcomputeP(t =ttk). IfxN =decode(t),thenreturns0.0sincethisis
nk+1 k+1 | 1 nk+1
notallowedaccordingtotheconditionrequiredin .
Tpval
2. Else,recursivelycomputeP(xN tk,t =t).
nk+1+1| 1 k+1
Proof. We first rewrite as the union of the following disjoint sets, i.e = and = ,
Tpval Tpval Tpval,1∪Tpval,2 Tpval,1∩Tpval,2
∅
where:
• = tt xN / prefix(decode(t)) decode(t) / prefix(xN ) : thesubsetof wherethestring
Tpval,1 { | ∈ V ∧ nk+1 ∈ ∧ ∈ nk+1 } Tpval
representedbythetokentisnotaprefixofxN .
nk+1
• = tt xN / prefix(decode(t)) decode(t) prefix(xN ) : the subset of where string
Tpval,2 { | ∈ V ∧ nk+1 ∈ ∧ ∈ nk+1 } Tpval
representedbythetokentisactuallyaprefixofxN .
nk+1
Assuch,wehave:
(cid:88) (cid:88) (cid:88)
p = P(xN ,t =ttk)= P(xN ,t =ttk)+ P(xN ,t =ttk), (21)
val nk+1 k+1 | 1 nk+1 k+1 | 1 nk+1 k+1 | 1
t∈Tpval t∈Tpval,1 t∈Tpval,2
(cid:124) (cid:123)(cid:122) (cid:125)
0.0
wherewenotethatfort ,wehaveP(xN tk,t =t)=0.0sincetheconditionalt =timpliesthatthe
∈Tpval,1 nk+1| 1 k+1 k+1
stringrepresentedbytmustbeaprefixofxN ,whichleadstothefirstsumbeing0.0.
nk+1
Forthesecondterm, i.e. t , ift = encode(xN ) , byfollowingProposition2.3, wehaveP(xN ,t =
∈ Tpval,2 ̸ nk+1 1 nk+1 k+1
ttk) = 0.0sinceanytokenizationthatisnotavalidmaximumprefixencodingmusthappenwithprobability0.0. This
| 1
standscorrectedaslongastheoutputprobabilityofourLMfollowsProposition2.3(seeSectionG.2).
As a result, we are left with the token t = encode(xN ) , which is the Equation (20) after applying chain rule of
nk+1 1
probability.
BaseCase. WenotethatthebasecaseofouralgorithmcorrespondstothesituationwherexN =decode(t). Inthis
nk+1
scenario,weonlyneedstocomputeb (branchingstep)whilep =0.0.
val val
ComplexityAnalysis. Thecomplexityofouralgorithm(numberofinferencesonthelanguagemodel)scaleswiththe
lengthofthethequerystring,i.e. N n . NotethatthecomplexityofthesummationattheBranchingstepisrelatively
k
−
cheapcomparedtotheruntimeofthelanguagemodel.
F.ConvertingToken-FreeLanguageModeltoTokenizedLanguageModel.
WeintroduceanalgorithmtocomputeP(t tk)usingatoken-freelanguagemodelP(xN xn),despitehavingnoaccess
k+1 | 1 n+1| 1
toanytokenizedLM.Thisapproachenablestheoreticalconversionofatoken-freemodeltoatokenizedone. Themethod
involvestwostages. First,werefactortheconditionalprobabilitysimilartothetechniquepresentedinSectionE.Next,we
aggregatetheprobabilitiesofallpossiblestringsleadingtothedesiredtokenization. ItisimportanttonotethataMarkov
chainisaspecialtypeofautoregressivemodel,meaningthismethodcanbeemployedtoeffortlesslycalculateMarkovchain
transitionmatriceswithinthetokenizeddomain.
11UnderstandingandMitigatingTokenizationBiasinLanguageModels
F.1.Refactoring
ConsidertheprobabilityP(t ti)thatwewouldliketoexpressedusingP(xN xn). Lett bethelasttokenwithinti
i+1 | 1 n+1| 1 k 1
suchthatt ∗. Wenowperformthefollowingfactorization:
k
∈V
P(ti+1 tk)
P(t ti)= k+1| 1 (22)
i+1 | 1 P(ti tk)
k+1| 1
P(ti+1 xnk)
= k+1| 1 , (23)
P(ti xnk)
k+1| 1
where xnk = decode(tk). The second equality is due to Corollary 3.2. Each term can then be computed using the
1 1
aggregationprocedureshownnext.
F.2.Aggregation.
In this step, we would like to compute P(ti xnk) where encode(xnk) = tk and t ∗, using the token-free
representationP(x xn). Here,wedenotek d+ ec1 o| d1 e(ti )=xni and1 M =m1 ax k dec∈ odV e(t) bethelengthofthe
n+1 | 1 k+1 nk+1 t∈V | |
longesttokeninV andΩ= M istheenumerationofallstringoflengthM.
A
ComputingP(ti xnk)involvesconsideringallpossiblestringsswithprefixxni andti =encode(s)i . Although
k+1| 1 1 k+1 k+1
iterating through every possible string is infeasible, we can restrict our search by only examining strings with length
s =n +M,asanyadditionalstringbeyondthispointwillnotimpactthetokenizationofprefixxniduetoM beingthe
| m| aximui mtokenlength. Formally,wewillshowthatonecanexpressP(ti xnk)asfollows: 1
k+1| 1
(cid:88)
P(ti xnk)= P(xni+M=c (s′)xnk)1(ti =encode(c (s′))i ), (24)
k+1| 1 nk+1 1 | 1 k+1 2 k+1
s′∈AM
wherec (s′):=concat(xni ,s′)andc (s′):=concat(xni,s′). Thefirsttermcanbecomputedusingthegiventoken-
1 nk+1 2 1
freeLM,i.e. P(xni+M xnk). Thesecondtermisanindicatorfunctionthatcheckswhetherti =encode(s)i andcan
nk+1 | 1 k+1 k+1
becomputeddeterministically.
Proof. Wehave:
P(ti xnk)=P(ti ,xni xnk) (25)
k+1| 1 k+1 nk+1| 1
(cid:88)
= P(ti ,xni+M=c (s′)xnk) (26)
k+1 nk+1 1 | 1
s′∈AM
(cid:88)
= P(xni+M=c (s′)xnk)P(ti xni+M =c (s′)) (27)
nk+1 1 | 1 k+1| 1 2
s′∈AM
(cid:88)
= P(xni+M=c (s′)xnk)1(ti =encode(c (s′))i ) (28)
nk+1 1 | 1 k+1 2 k+1
s′∈AM
Therestistoprovethefollowingequality:
P(ti xni+M =c (s′))=1(ti =encode(c (s′))i ) (29)
k+1| 1 2 k+1 2 k+1
Wefirstnotethatthefirstktokensmustbetk =encode(xnk)duetoourconditionthatt ∗. SinceM isthelengthof
thelongesttokenin ,appendingextrachar1 acterscannotc1 hangethetokenizationhappek ne∈ dV forxni. Inotherwords,any
stringswithprefixcV (s′)musthavethesameminimalsuperstringrcontainingxni (seePropositio1 nB.1). Wethenapply
2 1
thisprincipletothetwocases:
• ti =encode(c (s′))i : In this case, we know that the string must contains the first i tokens as ti, hence
k+1 2 k+1 1
P(ti xni+M =c (s′))=1.0
k+1| 1 2
• ti =encode(c (s′))i : Incontrast,thiscaseisequivalenttoP(ti xni+M =c (s′))=0.0sincewearesurethat
k+1̸ 2 k+1 k+1| 1 2
thestringdonotcontainsthetokensti .
k+1
12UnderstandingandMitigatingTokenizationBiasinLanguageModels
Thisconcludestheproof.
F.3.TheMarkovChainExample.
WeprovideadetailcomputationoftheMarkovchainexampleinthemainpaper. Recallthatintheoriginalchain(inthe
characterdomain),wehavethefollowing:
P(x =“A”x =“A”)=α (30)
2 1
|
P(x =“B”x =“A”)=1 α (31)
2 1
| −
P(x =“A”x =“B”)=β (32)
2 1
|
P(x =“B”x =“B”)=1 β (33)
2 1
| −
Wealsoassumetheinitialprobabilityπ = γ,1 γ for“A”and“B”respectively. Inthetokendomain,letfirstcompute
{ − }
P(t = “A”t = “AA”),wherewedonothavetodotherefactoringstepsinceweknowthatt ∗. Followingthe
2 1 1
| ∈ V
Aggregationstep,wehave:
P(t =“A”t =“AA”)=P(x6 =“ABA”x2 =“AA”)+P(x6 =“ABB”x2 =“AA”) (34)
2 | 1 3 | 1 3 | 1
=P(x5 =“AB”x2 =“AA”) (35)
3 | 1
=α(1 α), (36)
−
whereinthefirstequality,wedonotincludethecasex6 =”AAA”andx6 =”AAB”sinceencode(”AAA”) =“AA”
3 3 1
and encode(”AAB”) = “AA”, which are not the token “A” that we are interested in. For other tokens and when
1
t =“B”,thecomputationfollowsthesamearguments.
1
WenowconsiderthecaseP(t =“B”t =“A”),wecanrefactoritas:
2 1
|
P(t =“B”,t =“A”)
P(t =“B”t =“A”)= 2 1 (37)
2 | 1 P(t =“A”)
1
WefirstcomputeP(t =“A”)usingtheaggregationstep:
1
P(t =“A”)=P(x3 =“ABB”)+P(x3 =“ABA”) (38)
1 1 1
=P(x2 =“AB”) (39)
1
=γ(1 α), (40)
−
wherewedoagainincludethecasex6 =”AAA”andx6 =”AAB”forthesamereasonabove.ForP(t =“A”,t =“A”)
3 3 2 1
wehave:
P(t =“B”,t =“A”)=P(x4=“ABAA”)+P(x4=“ABAB”)+P(x4=“ABBA”)+P(x4=“ABBB”) (41)
2 1 1 1 1 1
=P(x2 =“AB”) (42)
1
=γ(1 α) (43)
−
whichgivesusP(t =“B”t =“A”)=1.0. Finally,inthisspecificcase,sinceorderoftheMarkovchaininthecharacter
2 1
|
domainis1,wedonotneedtoconsiderthehigherorderoftheMarkovchaininthetokendomain.
G.OnPredictiveDistributionofLMs
Inpractice,ourLMdoesnotfollowProposition2.3duetosoftmaxactivations. Assuch,inourBPTreealgorithm,when
t and t = encode(xN ) , then P (xN ,t = ttk) may not be 0.0 (where θ is the model weights).
∈ Tpval,2 ̸ nk+1 1 θ nk+1 k+1 | 1
Eventually,thiscanpotentiallyincreasethecomplexityofourBPTreealgorithmduringthePassingstep.
Inthefirstpartofthissection,weshowthatgivenanytokenizedLM,wecanforceitsoutputprobabilitestoobeyProposition
2.3, without any loss in terms of perplexity score on the token domain. In the second part, for completeness, we will
showthathavingatokenizedLMsatifyingPropostion2.3willguaranteethecorrectnessofthePassingstepinourBPTree
algorithm.
Finally,beforegoingtothemethod,weremindthereadersthatProposition3.1andCorollary3.2arefactuallycorrectand
holdforallθ. Assuch,therefactoringstepholdsregardless.
13UnderstandingandMitigatingTokenizationBiasinLanguageModels
G.1.Truncate-RenormalizationProcess
WejustifytheassumptionthatourtokenizedlanguagemodelP (t ti)followsProposition2.3. Theideaisthatwecan
θ i+1 | 1
turnalanguagemodelthatdoesnotfollowProposition2.3totheonethatdoeswhileguaranteeingthatthenewmodelwill
alwaysresultsinahighertoken-levelperplexityscore.
WefirstintroducePropositionG.1. Inthisproposition,wearegivenatargetdiscreteprobabilitydistributionpwherewe
knowsomeofthevalueswillnothappen,saysΦ∗. Assumethatwehaveanotherdistributionqthatapproximatesp,thenwe
canproduceanotherdistributionq∗thatisclosertopintermsofKLdivergencebysettingcorrespondingprobabilitiesofq
inΦ∗to0.0andrenormalizeit(similartorejectionsampling).
PropositionG.1. Givenadiscretedistributionp= p ,p ,...,p andq = q ,q ,...,q withq >0.0foralli. Let
1 2 m 1 2 m i
Φ= i Zp =0.0 andΦ∗ Φ,wedefineq∗ = { q∗,q∗,...,q∗ } whereq∗ ={ 0.0fori Φ} ∗,andq∗ =q /((cid:80) q ).
{ ∈ | i } ⊆ { 1 2 m} i ∈ j j l∈/Φ∗ l
Thenwehave:
D (p q∗) D (p q), (44)
KL KL
|| ≤ ||
whichimpliesthatq∗isclosertopthanq. Werefertheprocessofproducingq∗astruncate-renormalization(TR).
Proof. Let Z = ((cid:80) q ) is the normalizing factor in q∗. Note that Z 1 and as such log(Z) 0. The proof is as
l∈/Φ l ≤ ≤
follows:
(cid:18) (cid:19)
D (p q∗)=(cid:88) p log p i (45)
KL || i q∗
i i
(cid:18) (cid:19)
= (cid:88) p log p i ,use0log0=0.0 (46)
i q∗
i∈/Φ∗ i
(cid:18) (cid:19)
= (cid:88) p log p i (47)
i q /Z
i
i∈/Φ∗
(cid:34) (cid:18) (cid:19)(cid:35)
= (cid:88) p log p i +log(Z) (48)
i q
i
i∈/Φ∗
(cid:18) (cid:19)
(cid:88) p log p i =D (p q), (49)
≤ i q KL ||
i
i∈/Φ∗
whichcompletestheproof.
Applyingthistoourscenario,wherewearegivenanyautoregressivelanguagemodelsPˆ (t ti)thatdoesnotfollow
θ i+1 | 1
Proposition2.3(duetothesoftmaxactivations),wecanperformtheTRprocess(sinceweknowwhichencodingisinvalid)
toobtainanewdistributionP (t ti),whichisguaranteedtobeclosertotheground-truthmodelP (t ti). Asthe
θ i+1 | 1 gt i+1 | 1
results,weareguaranteedthatthetoken-levelperplexityscoreofP (t ti)isalwayslowerthanorequaltoPˆ (t ti).
θ i+1 | 1 θ i+1 | 1
G.2.OnPassingStepinBranchandPassAlgorithm.
Once our tokenized LM follows Proposition 2.3, intuitively it can be regarded as an autogressive source (in the token
domain)soitdoesnotalternatethecorrectnessofthePassingstep. Thissectionprovidesaproofforcompleteness.
Notethatweonlyneedtoconsiderthesecondtermofp ,i.e.:
val
(cid:88) (cid:88)
p = P(xN ,t =ttk)+ P(xN ,t =ttk), (50)
val nk+1 k+1 | 1 nk+1 k+1 | 1
t∈Tpval,1 t∈Tpval,2
(cid:124) (cid:123)(cid:122) (cid:125)
0.0
as the other term is always 0.0 regardless. Specifically, we will prove that, during the Pass step, when t and
∈
Tpval,2
t=encode(xN ) ,thenP(xN ,t =ttk)=0.0giventhatourLMfollowsProposition2.3. Iftk+1isaninvalid
̸ nk+1 1 nk+1 k+1 | 1 1
encoding,weimmediatelyhaveP(xN ,t =ttk)=0.0,soweonlyconsiderwhentk+1isavalidencoding. Wefirst
nk+1 k+1 | 1 1
provethefollowingProposition.
14UnderstandingandMitigatingTokenizationBiasinLanguageModels
PropositionG.2. Iftk isinvalid,thentk+1isalsoinvalid.
1 1
Proof. Proofbycontradiction,supposetk+1isvalidthen (tk+1)= . However,anystrings (tk+1)mustalsobein
(tk). But (tk+1)= (CorollaryB.3).1 Assuch,tk+1isS inva1 lid. ̸ ∅ ∈S 1
S 1 S 1 ∅ 1
SincexN hastotallym = N (n +1)characters,itcanhavemaximummtokens. Let = τ m−1 xN
nk+1 − k C { ∈ V | nk+2 ∈
prefix(decode(τ)) bethesetofallpossiblesequenceofm 1tokens(bothvalidandinvalid). Wethenhave:
} −
(cid:88)
P(xN ,t =ttk)= P(xN ,t =t,tk+m =τ tk) (51)
nk+1 k+1 | 1 nk+1 k+1 k+2 | 1
τ∈C
(cid:88)
= P(t =t,tk+m =τ tk)P(xN tk,t =t,tm =τ) (52)
k+1 k+2 | 1 nk+1| 1 k+1 k+2
τ∈C
Then,itissufficienttoshowthatP(t =t,tk+m =τ tk)=0.0foranyτ aslongastdoesnotfollowthemaximum
k+1 k+2 | 1 ∈C
prefixencoding,i.e. t=encode(xN ) . Wehavethefollowings:
̸ nk+1 1
• Sincet =tdoesnotfollowthemaximumprefixencoding,thenencode(decode(tk+m))=tk+msincewecanalways
k+1 1 ̸ 1
changethetokenizationatt .
k+1
• Lett bethefirsttokenwheretk+j isinvalid. ThenfollowPropositionG.2,sincetk+j isinvalid,thenanyextratokens
k+j 1 1
aftertk+j mustalsobeinvalid.
1
Assuch,wehave: P(t tk+j−1)=0.0,whichleadstoP(t =t,tk+m =τ tk)=0.0,whichprovestheresults.
k+j | 1 k+1 k+2 | 1
15