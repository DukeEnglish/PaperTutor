Towards Physically Talented Aerial Robots with Tactically Smart
ffi
Swarm Behavior thereof: An E cient Co-design Approach
Prajit KrisshnaKumar1, Steve Paul1, Hemanth Manjunatha1, Mary Corra2,
Ehsan Esfahani1 and Souma Chowdhury1,2∗
Abstract—The collective performance or capacity of col- individual robots in a swarm autonomously assess their
laborative autonomous systems such as a swarm of robots surroundings, communicate findings with each other, and
is jointly influenced by the morphology and the behavior
collaboratively plan and execute future tasks or actions. The
of individual systems in that collective. In that context, this
challengeisthattherearenoclear-cut,principledapproaches
paper explores how morphology impacts the learned tactical
behavior of unmanned aerial/ground robots performing recon- to designing the low-level behaviors (individual decisions or
naissance and search & rescue. This is achieved by presenting policies) and individual robot morphology that will ensure
a computationally efficient framework to solve this otherwise the desired collective behaviors.
challenging problem of jointly optimizing the morphology and Emergent behavior in swarm robotics/collective intelli-
tactical behavior of swarm robots. Key novel developments
gence results from simple rules followed by each entity and
to this end include the use of physical talent metrics and
modification of graph reinforcement learning architectures to their interaction with each other and their environment [2].
allow joint learning of the swarm tactical policy and the talent These interactions give rise to complex and adaptive behav-
metrics (search speed, flight range, and cruising speed) that iors that are robust and efficient. However, this emergent
constrain mobility and object/victim search capabilities of the
behavior cannot be directly inferred from an individual’s
aerial robots executing these tactics. Implementation of this
behavior or capabilities; rather, it is a product of dynamic
co-design approach is supported by advancements to an open-
source Pybullet-based swarm simulator that allows the use interplay within the swarm. Minor modifications in the
of variable aerial asset capabilities. The results of the co- designofindividualrobotsmightaffecttherobot’soperating
design are observed to outperform those of tactics learning envelope,significantlyimpactingitsemergentbehavioratthe
withafixedParetodesign,whencomparedintermsofmission
collectivelevel.Mostoften,behavior istrainedordeveloped
performancemetrics.Significantdifferencesinmorphologyand
based on fixed or apriori-designed physical systems [3], [4].
learned behavior are also observed by comparing the baseline
design and the co-design outcomes. This approach inherently restricts each robot’s operational
capabilities, often resulting in designs that do not fully
I. Introduction optimizeperformanceandthuslimittheoveralleffectiveness
Collectiveintelligenceenablesaswarmofroboticsystems of the swarm. The intricate interplay between morphology
to adapt effectively to uncertain and unknown environments, (physical form/design) and behavior (e.g., robot’s decisions
autonomously organize themselves, and exhibit emergent thatenablecoordinatedmotionandtaskcompletion)mustbe
behaviors that lead to superior problem-solving capabilities.
optimizedtogethertoexplorehowefficientlytheswarmasa
Through the collaborative efforts of multiple robots working whole can perform desired operations without failure. This
in tandem, tasks (e.g., exploration, transportation, surveying, co-design process ensures that physical design and behav-
harvesting, search & rescue, and assembly of distributed ioral algorithms evolve together, facilitating the alignment
objects [1]) that are beyond the capabilities of any single of capabilities to achieve superior collective performance.
robot can be efficiently tackled. Yet, realizing the potential Machine learning-based policies are becoming increas-
of swarm robotics and collective intelligence involves ad- ingly popular in expressing perception and control/planning
dressingformidablechallengeswithrespecttodesignchoices loops in robots and autonomous systems, including those
that shape the operating envelope and functionalities of the that can work as a collaborative group. There has been
individual members of a swarm or multi-robot team. For some work on co-design for individual robots on control
example, consider the generic swarm scenario in which side [5]–[14]. In the area of co-design with ML-based
policies, Gupta et al., [15] introduced the DERL framework
† CorrespondingAuthor,soumacho@buffalo.edu tocreateembodiedagentsforacomplexanimalmorphology,
1Mechanical&AerospaceEng.Dep.,UniversityatBuffalo,Buffalo,NY
which uses both traditional evolutionary methods and RL
2 ComputerScienceandEng.Dep.,UniversityatBuffalo,Buffalo,NY
methods in parallel. Many of the earlier methods in this
*This work was supported by the NSF award CMMI 2048020 and the
ONR grant N00014-24-1-2003. Any opinions, findings, conclusions, or
fieldreliedonevolutionaryalgorithms,whichcansufferfrom
recommendations expressed in this paper are those of the authors and do computational inefficiency. Among others, notable methods
not Cn oe pc ye rs igsa hr tily ©r 2e 0fl 2e 4ct Ath Se Mv Eie .w Ps eo rsf oN naS lF ua sn ed/ oo fr tO hiN sR m.
aterial is permitted.
that are closest to our work are introduced by Schaff et
PermissionfromASMEmustbeobtainedforallotheruses,inanycurrentor al. [16], who introduced a Deep Reinforcement Learning
futuremedia,includingreprinting/republishingthismaterialforadvertising
method for co-designing agents’ morphology and behavior.
or promotional purposes, creating new collective works, for resale or
In their method, an additional distribution for designs is
redistribution to servers or lists, or reuse of any copyrighted component
ofthisworkinotherworks introduced, and its parameters are updated to maximize the
4202
nuJ
42
]OR.sc[
1v21661.6042:viXrapolicy reward. Another method that is closest to our work takes place, with the robots spawned at a nearby depot
is introduced by Luck et al. [17], where four individual location. The robots are commanded by a neural network-
networks, two for morphology and two for behavior, are basedpolicythatdecidesthetacticalbehavior(akaallocation
trained in parallel. These works are showcased in singular of different tasks and GoTo locations) to rescue victims
robotics environments using PyBullet, focusing on control or find/extract objects of interest from the environment as
systems with continuous state-action spaces. Firstly, most of quickly as possible while mitigating the loss of swarm
theseexistingapproachesseektodirectlyoperateontheraw agents to adversarial entities in the environment – these
morphological (design) space, which becomes computation- characteristics are typical of disaster response, humanitarian
ally prohibitive as the complexity of the system increases. missions, and planetary explorations. The neural network-
What is thus currently lacking is the understanding of how basedpolicyguidingthebehavioroftheswarmistrainedus-
morphology affects (usually a smaller set of) fundamental ingreinforcementlearning(RL)overexperiencecollectedin
or latent system capabilities, which in turn constrain or an open-source robot simulator.This application was chosen
shape the envelope of feasible behaviors and exploit this because it effectively showcases the benefits of co-designing
understanding to decompose the co-design problem into a swarm systems, particularly in managing the complexities
sequence of simpler search problems. Secondly, unlike in and dynamics of real-world missions where multiple swarm
classicalco-design,thereexistsverylittleworkonsystematic behaviors are required to achieve higher-level goals. Often,
co-design of swarm or multi-robot systems. the swarm search and rescue frameworks available currently
To address these gaps, in this paper, we propose a com- arerestrictedtogrid-basedenvironmentsandlackreal-world
putational framework that enables the concurrent design of characteristics. The application presented here takes place
i) the morphology of individual robots in a swarm and ii) in real-world maps obtained with OpenStreetMaps API and
their collective behavior. Here, we utilize our previously simulated with SHaSTA (an Open Source Simulator) [20].
proposed artificial-life-inspired talent metrics [18] that are
physical quantities of interest, reflective of the capabili-
ties of an individual robotic system (e.g., range, nominal Thus,theprimarycontributionofthispaperisacomputa-
power consumption, weight, sensing FoV, payload capacity, tionalframeworkcapableofconcurrentlylearningthetalents
turning radius, etc.). Talent metrics represent a compact (driven by morphology) and behavior of RL-guided swarm
yet physically interpretable parametric space that connects robotic systems performing real-world missions. The sec-
the behavior space and morphology space. We use this to ondary objective is to formulate a Markov Decision Process
decompose the morphology-behavior co-optimization into (MDP) on top of the graph for the SWARM-CAE Problem
a sequence of talent-behavior optimization problems that and Graph Capsule Convolution network (GCAPCN) to
can effectively reduce the overall search space (for each serve as the tactical policy network for the MDP. Though
individual problem) without negligible compromise in the the assumed application uses a heterogeneous team of two
ability to find optimal solutions. In other words, the decom- different types of robots, unmanned aerial vehicles (UAVs)
position approach presented here is nearly lossless, i.e., a and unmanned ground vehicles (UGVs), in this paper, for
solutionthatcanbefoundotherwisewithabrute-forcenested simplicity, we only optimize the morphology of UAVs. In
optimization approach to co-design will also exist in the this proposed approach, first, we derive talent metrics for
overall search space spanned by our decomposed co-design quadcopter-typeUAVsusingasetoflogicalprinciplesbased
approach (albeit assuming that each search process is ideal). on the application, perform multi-objective optimization to
We also propose a novel talent-infused actor-critic method obtain the Pareto front in talent space, and then create a
to optimize the talents and learn the behavior concurrently. regression surface representation of this talent Pareto. The
To demonstrate the efficacy of the proposed co-design talent Pareto is used in our proposed novel Talent-infused
approach, we examine its application in a complex Urban Actor-Critic approach to optimize for mission efficiency.
Search and Rescue operation using heterogeneous swarm Finally, we perform another optimization to find the exact
robots. We call this problem, SWArm robotic search and morphology corresponding to the learned talents.
Rescue Mission for Complex Adversarial Environment
(SWARM-CAE). Often, in complex robotics missions, it is
imperative to combine multiple swarm behaviors to accom- The remaining portion of this paper is organized as fol-
plish a higher-level goal. Behjat et al., [19] introduced a lows: in section II, we define the co-design problem using
tactical learning framework for complex swarm missions an example, thereby defining the concept of talent metrics;
where the higher-level goals are decomposed into multiple concurrently,weprovideanoverviewofourproposedtalent-
sub-goals that are achieved by combining individual swarm infusedactor-criticmethodinageneralizedform;sectionIII
and single-robot behaviors; further, this framework also pro- introduces the SWARM-CAE problem; section IV describes
vides abstraction methods to overcome the state and action thegraphneuralnetworkdevelopedfortheproposedTalent-
space explosion in multi-robot systems pertinent to learning infused Actor-Critic method; section V presents the results
basedmethods.Duetotheframework’sversatilenature,itis of using Talent-infused actor-critic method on the SWARM-
adapted for our SWARM-CAE problem. Here, we consider CAE Problem, compared to a baseline; finally in section VI,
an urban or semi-urban environment where the operation we present our conclusions.II. FrameworkforConcurrentDesignofBehaviorand
Morphology Max: f L(Y TL,Φ)
S. t.: Y ,Φ∈R
ConsideragroupofUAVsperformingasearchoperation TL
(3)
over an environment. Let X M represent the vector of mor- Y TLmin ≤Y TL≤Y TLmax
phological variables of individual UAVs, such as wing span, Φ ≤Φ≤Φ
min max
frame, or battery capacity. This vector encompasses all the
To effectively replace morphology with talents, the se-
variables necessary to build a complete system. Meanwhile,
Φ represents controllable parameters of the algorithm that lected talent metrics should satisfy four principles: 1) The
guideitsbehavior.Dependingonthecomplexity,Φcanbea collection of talent metrics should depend only on morphol-
ogy.2)Talentmetricsshouldexhibitthemonotonicgoodness
single heuristic parameter or the neural network’s weights if
property, meaning that for each metric, there should be a
thebehaviorisbasedonit.Theperformancemetric,denoted
as f , quantifies its effectiveness in the environment. In the direction (increasing or decreasing) corresponding to im-
L
provedperformance.3)Talentmetricsshouldbecollectively
context of RL, this can be a reward function. The primary
exhaustive in determining the impact on the performance
goalofco-designoptimizationinthiscontextistomaximize
of the behavior, meaning there cannot be a case where
this performance metric, and this can be represented as an
constraints or bounds of behavior can change with a fixed
optimization problem shown in Eq. (1).
talent. 4) Each talent metric should conflict with the others;
Max: f (X ,Φ) for example, increasing the payload reduces the range.
L M
Advantages of substituting X with Y are i) Directly
S. t.: X ≤X ≤X M TL
min M max
(1) optimizing the talent space allows focusing on the most
Φ ≤Φ≤Φ
min max relevant performance metrics, this often leads to finding the
g(X M)≤0 optimal talents and behavior more efficiently than when co-
Threeprimarymethodsareusedtosolvethisoptimization optimizing morphology and behavior, ii) provides a likely
problem: Sequential Design: first optimize the Morphology dimension reduction; Typically, the morphology space is
X and optimize for the behavior Φ or vice versa; this considerablylargerthanthetalentspace;iii)themonotonous
M
leads to a highly sub-optimal design/collective behavior and goodness property allows for safely eliminating solutions in
cannotbegeneralized[21];NestedDesign:optimizeboththe the dominated region and constraining the optimization to
behavior and morphology in a nested way, while it can be Pareto front.
thorough, it is computationally intensive [22]; Evolutionary
B. Talent Pareto Boundary Construction
methods: use evolutionary optimization methods to optimize
Given the monotonic goodness property of each talent
the behavior and morphology together, this approach is
metric,theoptimaltalentshouldliewithintheParetoregion.
computationallyfeasiblewhenthebehaviorormorphologyis
Therefore, we perform a multi-objective optimization to
simple, as the complexity increases, the computational cost
identify the non-dominated (Pareto) points of the talent
increases [23]. Using the talent metrics concept proposed
variables. The optimization problem can be expressed as
in our previous paper [18], we decompose the morphology-
behavior design into a series of sequential talent-based opti- Max: (Y ,...,Y )= f (X )
TL,1 TL,m M M
mizationproblems.Figure1showsthefourstepsinvolvedin S. t.: X min≤X M ≤X max (4)
our proposed co-design framework. The below subsections
g(X )≤0
delve deep into each step. M
This exposes the Pareto points for the multi-objective opti-
A. Morphology Constrained Talent Metrics Selection
mization problem. The pareto front can be modeled through
The talent metrics (Y TL) refer to the morphology-driven an approximation or surrogate method. The model can be
variables that directly influence the resultant behavior’s per- represented as
formance.Forinstance,givendifferentcombinationsofmor-
phology variables, we obtain different metrics representing Y TL,m= f S(cid:0) Y TL,1,...,Y TL,m−1(cid:1) (5)
the UAV’s operating envelope, such as flight range and
where f represents the approximation model, and m repre-
S
cruising speed. It is evident that these talent metrics form
sents the quantity of talents metrics.
a parametric layer given by
C. BehaviorLearningviaTalent-infusedActor-criticsubject
Y = f (X ) (2)
TL M M to Talent Boundary
where f denotes the model responsiblefor determining the In the context of a group of UAVs performing the search
M
talentscorrespondingtoamorphology.Thiscanbeobtained exampleprovidedbefore,consideracomplexneuralnetwork
withsimulations,supervisedlearningifadatasetisavailable, is used for the behavior of individual robots and trained
andanalyticalequations.Y isavectorconsistingofvalues through an actor-critic-based RL algorithm. The Actor-critic
TL
[Y ,...,Y ]. Upon introducing talents, the objective algorithm consists of 2 neural networks. The actor network
TL,1 TL,m
functionEq.(1)canbemodifiedasanoptimizationproblem maps the current state of the UAV to the actions, and it
expressed in Eq. (3). dictates how the UAV should behave. The actor policy canPhysical system Morphology/component Choices
Quadrotor/UAV
equipped with victim
detection sensor Talents
a) Talent Metric Selection based on Morphological Constraints b) Talent Pareto Boundary Construction
Simulation
Testing in Environment Talent Pareto
Simulation Boundary
Weights = 0
Morphology
Final Particle Swarm Space Physical System Optimization Learnt Talent
Learnt
Behavior
Actor Network trained with PPO
d) Morphology Finalization using Talent Boundary c) Behavior Learning subject to Talent Boundary c1) State Action Abstraction
Fig. 1: Flowchart of our co-design framework, a) Morphology and its dependent talent parameters are derived, b) Based on
the talents, we create a Pareto boundary, c) The Talent-infused Actor-critic method is used to train the associated behavior
and talents, d) Finalize the morphology for the optimized talents.
be denoted as π((a|s;θ) where θ are the weights of the actor In order to find the upper and lower limits of each talent
network and a represents the action given state s. While the value,weemployquantileRegressionmodelat5thand95th
actor decides the actions given a state, the critic network percentile respectively conditioned on previous talents. For
evaluates the potential value (state-value) of being in that the First Talent (Y ), we can directly get the lower and
TL,1
state, estimating the future rewards that can be obtained upperlimits.Hence,weusethebelowequationtoobtainthe
from that state. The value network can be represented as first talent.
V(s;w) where V is the value of state (s) with weights of
Y =Yˆ (max(Yˆ )−min(Yˆ ))+min(Yˆ ) (6)
the network (w). In order to solve our optimization problem TL,1 TL,1 TL,1 TL,1 TL,1
shown in Eq. (3), we need to incorporate talents into the Fromthe2ndtom−1talents,weusethefollowingequation,
actor and critic network. The below sections delve deep into
the modifications performed as well as the pseudo-code of Y = Yˆ (cid:0) Q(0.95|Y ,...,Y )
the proposed Talent-infused Actor-critic Algorithm. TL,i TL,i TL,1 TL,i−1
−Q(0.05|Y ,...,Y
)(cid:1)+
1) Modifications in Actor-Critic Framework: The actor
TL,1 TL,i−1
network that typically generates the action/behavior for the Q(0.05|Y TL,1,...,Y TL,i−1)
UAV is augmented with an additional neural network called ∀i∈{2,...,m−1}
thetalentnetwork.Figure1c)providesanexamplerepresent-
This scaling allows us to stay within the Pareto front. The
ingthisaugmentation.Theoutputshapeofthetalentnetwork
scaled values are passed onto the simulation for creating the
is m−1, where m represents the total number of talents. The
robots.
weights of the input neurons of the talent network are set
3) Training Phase: Here, the main objective is to opti-
to zero, and not-trainable, i.e., the network doesn’t require a
mize the distribution with the talent network and learn the
stateorinputtoworkandalwaysprovidesaconstantoutput.
behavior. During the first step of each episode, We do a
These outputs directly correspond to the mean of m−1
forward pass in the actor network, which is then followed
distributions, which is further processed through the talent
by sampling through distribution. The augmented output of
decoder to obtain the final talent values. Here, in order to
the actor network can be given by
limit the optimization inside the Pareto front, the final layer
of the talent network is given a sigmoid activation function A (s)=(a,Yˆ ,Yˆ ,...,Yˆ ), for all i∈{1,...,T}
θ i i TL,1 TL,2 TL,m-1
and is processed through a talent decoder. The policy of (7)
this actor network is given by π((a|s,Yˆ ,···,Yˆ );θ),
TL,1 TL,m-1 where A (s ) signifiesthe outputof actorpolicy attimestep
θ (i)
where a is the behavioral action θ indicates the weights and
i with input state s . a represents the action for state s ,
Yˆ TL,1,···,Yˆ TL,m-1 are the outputs from talent network. Yˆ ,Yˆ ,..,Yˆ (i) (i r) epresents the talent values from(i) 1
TL,1 TL, 2 TL, m-1
2) Talent Decoder: The talent decoder’s objective is to m−1. These m−1 values are subsequently processed by
to scale the values from the talent network to en- a talent decoder, which scales these values based on the
sure they remain within the bounds of the talent Pareto maximum and minimum bounds of their respective talents.
front. Once the actor network provides the talent values To get the final talent Yˆ , we use the approximation
TL, m
(Yˆ ,Yˆ ,..,Yˆ ), we need to scale these values model created with Eq. (5). Following the determination of
TL,1 TL, 2 TL, m-1
using upper and lower bounds to get the talent value (Y ). actions and talents, these values are fed into the simulation
TL
reganaM
etatS
Distribution
Talents
Actions
Decoder
noitcA
Talent
reganaM
hparG
lacigolopoT
puorG
noitcartsba
noitcartsbA
noitcA stnelat
toboR
roivaheB
oteraP
noitcartsbaenvironment. Based on these talents, robots are instantiated, of the optimization problem expressed in Eq. (3). Let us
andthechosenactionisexecuted,whichthenreturnsuswith consider the optimized talents as Y∗ .
TL
the reward and the new states. The new states are passed on
to the actor network again. Crucially, after the first step of
Algorithm 1 Talent-Infused Actor-Critic Method
the episode, talent values are not sampled from the talent
1: Input:Learningratesαθ andαw,discountfactorγ,batchsize B,andthetotal
network and the actor network continues to suggest actions numberoftalentvariablesm
based on the current state without any changes until the 2: InitializeactornetworkAθwithweightsθ,outputtingpolicyπ((a,YTL)|s;θ),where
YTL arethetalentvaluesandaisthebehavioralaction{Morphology-dependent
episode ends. Essentially, talents stay the same throughout weightsaresetto0andarenon-trainable}
an episode, but the states and actions update with each step, 3: Initializecriticnetworkwithweightsw,estimatingvaluefunctionV(s,Yˆ TL;w)
4: InitializeexperiencebufferE
as shown in the Eq. (7). 5: whilenotreachedendoftrainingdo
Thecriticnetworkupdatesprimarilyusingthestatevalue, 6: forb=1toBdo
incorporating an additional component called ”talents” into 7 8: : I On bit ti aa il niz ae (ts 1t )ar at ns dta Yˆte TLs ,1(t ,1 Y) ˆ Tf Lo ,2r ,t .h ..e ,Yb ˆ T-t Lh ,me -p 1is fo rod me Aθ(s(t1))
the state space. This integration results in the input to the 9: CalculateYˆ TL,musing fSM(Yˆ TL,1,...,Yˆ TL,m-1)
10: whilenotdonedo
network containing both state and talent values. Instead of 11: Useati andYˆ TLforsimulation,wheretidenotesthecurrenttimestep
calculating a state value using the critic network, the critic 12: Executeactionati,observerewardrandnextstates(t+1)
network now calculates the state-talent value pair. Consider
13: Storetransition(st,ati,r,s(t+1),Yˆ TL)inE
V(s t,Yˆ TL;w)representsthevalueofstatefromcriticnetwork
11 54 :: UFo pr dati te> st1 t, toret sa (ti +n 1)Yˆ TLwithoutre-sampling
16: endwhile
with weights w for states s and all the talents from actor
t 17: endfor
network Yˆ
TL
The talents employed by the UAV during 18: foralltransitions(st,a,r,s(t+1),Yˆ TL)inEdo
the episode are used in the state-talent input for the critic
19: ComputeTDerror:δ=r+γV(s(t+1),Yˆ TL;w)−V(st,Yˆ TL;w)
20: Updatecriticweights:w=w+αwδ∇ wV(st,Yˆ TL;w)
network. The critic assesses the state value based on these 21: Updateactorweightsθbasedongradient:∇ θJ(θ)=δ∇ θlogπ(a|st,Yˆ TL;θ)
talents, offering an estimate of the expected future reward 22: endfor
23: ClearexperiencebufferEfornextbatch
accumulation for the given talent and actions with state s. 24: endwhile
The Temporal Difference (TD) error is computed based on
δ=r+γV(s (t+1),Yˆ TL;w)−V(s t,Yˆ TL;w) (8) D. MorphologyFinalizationusingLearntBehaviorandTal-
ent Boundary
The critic network updates its weight w to minimize the TD
To finalize the morphology for optimized talents, another
error and the update rule can be represented as
optimization approach given by Eq. (11) should be handled
w t+1=w t+αδ∇ wV(s t,Yˆ TL;w) (9) Min: f
f
=||Y TL(X M)−Y∗ TL||
If the cumulative reward from the actor’s actions and talents Subject to: X M ∈R (11)
exceeds the critic’s estimate, the critic provides positive X ≤X ≤X
min M min
feedback, encouraging the actor to increase the probability
where f refers to the objective function value, and Y∗
of the current action and associated talent. f TL
represents the optimal talent. This optimization aims to
∇ J(θ)=δ∇ logπ(a|s,Yˆ ;θ) (10) obtain morphology that provides talents as close as possible
θ θ t TL
to the optimal talents obtained in learning.
Overtime,thisiterativeprocessenablestheactortorefine
both its policy and talent parameters, striving for an optimal III. Multi-robotSearchandRescue
balancethatmaximizescumulativerewards.Toensuregener- We consider a swarm search and rescue operation hap-
alizability,updatesshouldbeperformedoveralargebatchof pening in an urban environment involving UAVs and UGVs.
episodes. This necessity arises from accumulating gradients Here, the robots aim to locate a specific building with an
over a diverse set of talent values. object of interest (victim) inside and rescue it. Initially, all
The pseudo-code for talent-infused actor-critic method robots form a preset platoon in a depot for deployment.
is shown in Alg.1. Most of the open-source RL libraries, The urban environment contains multiple suspect buildings
includingStable-baselines3[24]andOpenAIBaselines[25], (targetbuildings)wherethevictimcanbe,andthegoalisto
follow updates over the batch method. In libraries, parallel find the true target building (goal location) where the victim
vectorized environments are created to collect experiences is. UAVs can search the building’s perimeter to determine
and update the policy after collecting a batch of episodes. whether it is the true target building (goal location) with
Formoredetailsontheimplementations,pleasereferto[26]. the exact location of the victim. Meanwhile, UGVs have
4) Testing Phase: During the testing phase, the distribu- indoor search capabilities, allowing them to search within
tionsareremoved,therebytakingdeterministicactions.Since the building and execute the rescue operation. If the UAVs
thetalentnetwork(partoftheactornetwork)hasweightsof0 successfully identify the target building and its location, the
intheinputlayer,thestatespacedoesn’taffecttheoutcome, UGVscanbypasscomprehensiveexploration,expeditingthe
and it always results in a single value. These final values rescue upon entry. The outdoor search progress is calculated
passed through the talent decoder will provide us with the as ψ =V ×(n ×P), considering the number of floors
l,out ψ f
optimized talents Y∗ , and the learned policy is the result (n ), total perimeter (P), and individual UAV search speed
TL f(V or Field of view/FOV). The Search speed is based on Graph Property Shape
ψ RemainingTime 1,
a sensor that can detect the presence of a victim, and the RemainingUAVplatoons 1,
Stateofthemission
RemainingUGVplatoons 1,
quality of the sensor depends on its weight; the higher the
shape (3,1)
weight of the sensor, the higher the V ψ value, and it allows Location 2,
faster perimeter search. range 1,
UAVstatesG UAV type 1,
Threetypesofadversariesareconsidered:Smoke,Bombs, GoalLocation 1,
and Dynamic adversaries. Dynamic adversaries follow fixed
shape (NUAV,5)
Location 2,
paths and continuously monitor the area. UGVs can neu- range 1,
tralize dynamic adversaries but can also be neutralized. UGVstatesG
UGV
h tye pa elth 1 1,
,
Bombs are not neutralizable and can destroy UGVs on GoalLocation 1,
contact. Smoke slows down UAVs but is undetectable by shape (NUGV,5)
location 2,
dynamic adversaries. We use an MDP formulation to solve probability 1,
the SWARM-CAE problem, focusing on optimal tactics for BuildingstatesG BLD indoorsearchprogress 1,
outdoorsearchprogress 1,
effective mission completion (see Section III-B for details). shape (NBLD,5)
To manage the state-action space, we employ encoding location 2,
type 1,
techniques, including group abstraction (robot platooning ActingPlatoonYACT range 1,
with various commands) and Pareto encoding of nodes shape 4,1
location 2,
(identifyingcriticalpointsthroughnon-dominatedsampling). AdversaryStatesG ADV type 1,
Further details can be found in [19]. Figure 1 c1) illustrates shape (NADV,3)
Range 1,
these abstractions and consists of a sample 3D environment Velocity 1,
where the mission is happening. A short video describing
UAVTalentsYTL
SearchSpeed 1,
shape 3,1
the environment and the mission can be found in this link1
TABLE I: States of the tactics model for a swarm with
A. Simulation
variable UAV Squads and UGV Squads
WeusedasimulatorcalledSHaSTA(SimulatorforHuman
and Swarm Team Applications) [20] for this study. SHaSTA
hasmultipleadvantagesoverothersimulators,someofwhich
optimality-based filtering, We identify the critical locations
are: 1) automated importing of any real-world maps using
throughanon-dominatedsortingprocess.TheParetofiltering
OpenStreetMap API and running swarm simulations, 2)
processcanbegivenbyk∗=argmin f(k)=P(G)×t(X →
inbuilt swarm-primitives such as formation control and path k i l k
G), l=1,2,...N, where X represents the spatial location
planning.Threedifferentprimitivesareused:Taskallocation, l l k
of the k-th graph node that can be allocated as a destination;
PathPlanning,andFormationcontrol.Forpathplanning,we
t(X →G) is the time taken to reach a potential target G
consider 3 different routes: i) Aggressive path: Fastest path k i i
from the point X , and P(G) is the probability that it is
to the destination, ii) Normal path: The path cannot have k l
the true target G. At the beginning of the mission, the
deadlyadversariessuchasbombsordynamicadversaries.iii) l
probability of each target building is set to 1/N, and once
Cautious path: No adversaries present. The policy model l
either the indoor or outdoor search is completed for any
provides the tactical decision of location to search and
target location, the probability of all the other locations gets
the path to take to reach that location. The entire map is
updated.
abstracted as a topological graph. The location of interests,
We formulated 4 individual graphs for the states of
such as buildings, intersections, and building entrances, are
UAV, UGV, Pareto node, and adversaries. Note that these
considerednodesofgraphs.Thepathplanningisdoneusing
graphs are input space for the Reinforcement Learning
thenetworkxlibraryusingthistopologicalmap.Theregion-
policy and differ from the environment graph used for
basedformationcontrolmethod[27]isusedheretonavigate
abstraction and simulation. The UAV states are represented
the platoons to the desired location. In order to implement
by G =(V ,E ,Ω ), the UGV states are repre-
our Co-design approach, a simulator must be able to import UAV UAV UAV UAV
sented by G =(V ,E ,Ω ), Pareto node states
custom robots. New modules have been implemented for UGV UGV UGV UGV
G = (V ,E ,Ω ) and the adversary nodes by
importing custom robots without completely closing the BLD BLD BLD BLD
G = (V ,E ,Ω ). In each graph, V represents
simulation. This helps collect experiences at a much faster ADV ADV ADV ADV
the nodes/vertices specific to the state, E represents the
pace and aids in learning faster.
edges connecting these vertices, and Ω is the corresponding
B. MDP Formulation weightedadjacencymatrix.Eachnodeinthesegraphsrepre-
sentsanentity-beitaUAV,UGV,Paretonode,oradversary
1) States: SHaSTA allows importing any real-world lo-
andeachedgeconnectsapairofthesenodes.Thetotalcount
cation as a graph structure. This already solves the problem
for each node type is denoted by N ,N ,N , and
of discretizing a continuous environment. Further, not all UAV UGV BLD
N , which represents the total number of UAVs, UGVs,
locations on the map are equally important. Through Pareto ADV
Pareto nodes, and adversaries respectively.
1https://buffalo.box.com/s/3tadqfqtgv7jw5kcez7vt5gfc54ny2wq The complete state formulation is provided in table I.Apart from these graphs, we also include linear vectors to The features F , F , F , F , and F are used
UAV UGV ADV ACT TAL
include the state of the mission, talents, and acting platoon to compute a context vector F (explained in section
context
inthestatespace.Forthe”stateofthemission,”weconsider IV-A.2). Since the goal of the policy is to select a Pareto
theremainingtimeandthecurrentfunctionalplatooncounts. node and a path (one out of three options), we compute
The UAV talent space consists of the talent vectors (Y ). logits for all the Pareto nodes across all three paths. We
TL
More details on the selection of talent metrics are explained compute 3 logits vector (LG ∈R10×1, LG ∈R10×1, and
P1 P2
in sec.V. At each time step, an idle platoon is selected LG ∈R10×1), for the three types of path. We use 3 Multi-
P3
for which the action is required, and the properties of this head Attention (MHA) based decoders to compute the logit
platoon are given in the acting platoon vector. vectors (explained in section IV-A.3).
2) Actions: The Behavioural action (a) here is discrete,
A. Policy Model
and policy should decide which Pareto node to visit and the
1) Graph capsule-based feature abstraction: In order to
path to be taken (aggressive path, normal path, and cautious
compute a learned representation of the 4 graphs of the
path) for the selected idle platoon. If the platoon runs out of
state space, we use a GCAPCN network to compute node
range or health, it is considered non-functional and will not
embeddings. We initialize 4 GCAPCN networks for the 4
be considered further in the mission.
3) Reward: The proposed reward function takes into ac- graphs. The GCAPCN networks (Fig. 2) take in a graph
(in the form of node properties and the weighted adjacency
countthemissionstatus,time,andcasualties.Ifthescenario
matrix)andoutputthenodeembeddings.Herewegiveavery
is successful, meaning the robots have rescued the victim
briefdescriptionofGCAPCN.ConsideragraphG=(V,E,Ω)
within the allowed time, we provide rewards as follows
with N nodes, where V is the set of nodes, E is the set
R=τ sc+(Λ sc) (12) of edges, and Ω is the weighted adjacency matrix. Let δ
i
represent the node properties of node i ∈ V, as a vector,
where (τ ) is the rescue time that is the duration taken
to
rescuesc
the victims and is normalized by the maximum
and X = [δ 1...δ N] ∈ RN×|δi| be the node property matrix,
allowed mission time. The survival rate (Λ ) represents the where |δ i| represents the cardinality of δ i. First, the node
sc properties undergo a linear transformation F ∈RN×h, where
ratio of the number of robots that survive the mission to the 0
N isthenumberofnodesinthegraphandhistheembedding
initial size of the swarm. If the operation is not successful,
length. This is followed by multiple graph capsule layers
we provide a negative reward of −1
[29] that make use of the transformed node properties and
IV. GraphbasedReinforcementLearning thegraphLaplacianmatrixtocomputepermutation-invariant
node embeddings fp (X,L)∈RN×h, ∀ p∈[1,P],l∈[1,L ],
The Reinforcement learning (RL) approach involves max- l e
where P is the highest order of statistical moment and L
imizingthetotalrewardperepisodebytrainingapolicynet- e
is the number of layers. This captures the nodal information
worktolearnactionssequentiallyforthemissionrepresented
(thenodepropertiesmatrix X)andthestructuralinformation
as an MDP whose objective is to maximize the total reward
( the Graph Laplacian L) of the nodes of the graph and has
per episode. In this work, we implement a policy gradient-
P representations of this information. The node properties
basedon-policymethodcalledProximalPolicyOptimization
of the four graphs in the state space can be found in Table
(PPO)[28]totrainthepolicy.Duringeverydecision-making
I. These embeddings are concatenated and done for multiple
instance, the policy takes in the state space variables and
layers(L ).Theoutputfromthefinallayer F ispassed
computes the action (in this case the which Pareto node to e Le(X,L)
through a feedforward layer to get an embedding length of
visit and the path to take, and also the talent metrics at the
h, which is then added with F to get the final embedding
startofanepisode.)Since4ofthemainstatespacevariables 0
F∈RN×h.ForfurtherinformationonGCAPCN,wereferthe
arerepresentedasagraph(asexplainedinsectionIII-B),we
reader to [29], [30]. In the main policy diagram (Fig. 4), the
develop a policy network based on Graph Neural Networks
outputs are represented as F , F , F , and F
(GNN).TheGNNsareusedtocomputenodeembeddingsfor BLD UAV UGV ADV
2) Context Vector: The context vector is computed using
thegraph.Inthiswork,weuseGraphCapsuleConvolutional
all the feature vectors.
Neural Networks (GCAPCN) [29] as the GNN. GCAPCN
has proved to be an excellent graph feature abstraction F =Concat(Mean(F ),Mean(F ), (13)
context BLD UAV
network (from our previous work on similar Multi-agent
Mean(F ),Mean(F ),F ,F ))
problems[30],[31])comparedtootherGNNssuchasGraph UGV ADV ACT TL
Convolutional Networks (GCN), Graph Attention Networks where Mean(F )∈R1×h is the mean of the mean feature
BLD
(GAT), etc. We initialize 4 GCAPCN network for the 4 vector across all the nodes, and similarly for Mean(F ),
UAV
grpahs (G ,G ,G ,G ) respectively, and com- Mean(F ),andMean(F ).ThecontextvectorF ∈
UAV UGV BLD ADV UGV ADV context
pute the corresponding node embeddings F
UAV
∈RNUAV×h, R1×6h will be used along with F
BLD
to compute the logits
F
UGV
∈ RNUGV×h, F
BLD
∈ RNBLD×h, and F
ADV
∈ RNADV×h, forthethreepathsusingtheMHAdecoder(explainedinthe
respectively. Here h is the embedding length. We compute next section).
feature vectors for representing the states of the acting 3) Logits Computation using MHA-based Decoder: As
platoon F ∈R1×h, and the UAV talents F ∈R1×h, by mentioned in the above section, given the current state,
Act Tal
twoseparatelineartransformations(withlearnableweights). the goal is to select which Pareto node to visit and path𝐹$ ℒ$ ℎ
𝐹!"#(𝑋,ℒ)∘# ℒ% 𝑊#( &!) Σ 𝑓#!(𝑋,ℒ)
ℎ
Graph
𝛿! ℎ
Nodes 𝛿" 𝑊#( )!) ℎ
𝜹𝟒 𝜹𝟑
𝐹
𝜹𝟏 𝜹𝟐
𝛿#
|𝛿𝒊|
ℒ$
Concat
𝐹!!(𝑋,ℒ)
𝜎
𝐹0
𝐹&+ 𝐹*
𝑊$
Output of layer - 𝑙 ℎ×𝑃
ℒ 𝛿 𝑋. - =- G N [𝛿r oa !dp ,𝛿eh
"
fL ,ea .a .p t 𝛿l ua #rc ]eia s n of node 𝑖 (𝜖 𝒱)
Graph
𝐹!"#(𝑋,ℒ)∘% ℒ% 𝑊%( &!) Σ 𝑓%!(𝑋,ℒ) 𝑊/
𝜎 – Sigmoid
N – Number of nodes Capsule-based
𝒱 – Set of nodes 𝑊%( )!)
𝑃 – Highest order of statistical Encoder 𝐿’ layers
moment
𝐿’ – Number of layers
Fig. 2: The GCAPCN-based encoder. The node properties undergo a linear transformation first, followed by multiple graph
capsule layers.
𝐹 (∈ℝ!!"#×#) to compute the final logits. This is done with three different
ℎ ℎ ℎ encoders for computing logits for the three types of path,
𝐹$%&’()’ (∈ℝ*×+#) ℎ ℎ ℎ which is then used to compute the action distribution.
- Query (∈ℝ$×&) ℎ ℎ
MHA- - K –Ve a My lu u( e∈ l t( i-∈ℝ h’ eℝ a! ’" d# ! A× "# t& t×) e& n) tion MH6ℎ A-baM seHA d Decoderℎ Matmul (∈ℝ!𝑍 ! "#×#)
V. CaseStudy-Co-designforSwarmTacticsLearning
Fig. 3: The MHA-based decoder. The input to the decoder
includesthenodeembeddingsandthecontextandtheoutput In this section, we showcase the results obtained by
is the computed logits. applyingourproposedco-designframeworktotheSWARM-
CAE problem. We delve deep into Talent selection based
on the morphology of UAVs, Pareto model creation us-
𝒢 $%& 𝒢 !"# 𝒢 !’# 𝒢 "&# 𝑌 "() 𝑌 )% ing polynomial regression, Co-learning using our proposed
Building encoder UAV encoder UGV encoder Adversary encoder Acting platoon Talents Talent-infused Actor-critic method, and finally, comparing
(GCAPCN) (GCAPCN) (GCAPCN) (GCAPCN) encoder encoder
𝐹!"# 𝐹$%& 𝐹$’& 𝐹%#& 𝐹%() 𝐹)" the results of a co-design policy with a sequential design
Context policy.
𝐹*+,-./-
Path1 decoder (MHA)
𝑍!" Cust do ism
tr
ip br uo tb ioa nbility
Path2 decoder (MHA)
𝑍!#
Concatenate Categorical
Path3 decoder (MHA)
𝑍!$
Normal
TalentBias Terms Sigmoid
Fig.4:Structureoftheoverallpolicymodelconsistingofthe
GCAPCN encoders, Context, MHA-based decoders, Talent
bias network, and the custom probability distribution.
to choose, and this selection is made based on computing
3 logits vectors (Z ,Z ,Z ). In order to compute the
P1 P2 P3
logits, we use an MHA-based decoder (Fig. 3). The decoder
takes in the Pareto embeddings F in the form of keys
BLD
(K) and values (V), and the context F in the form
context Fig.5:TalentParetofrontrepresentedbyPolynomialRegres-
of query (Q) and computes compatibility scores between
sionappliedtocomputedParetosolutionsobtainedbymulti-
F and every node embedding in F which is then
context BLD objective optimization of Talents; limits of talents captured
used to compute the attention heads. These attention heads
with quantile regression.
are then passed through a feedforward layer and multiplied
with another linear transformation of the node embeddings
.…………………
.………
ℎ×𝑃Type Variable C Lo Bn stra Uin Bt s CD o-e ds eig sn ig nO u Batc so em line e convergencehistoryforthethreetalentvariablesandrewards
𝑿𝑀 Length (m) 0.2 0.5 0.31 0.25
MoW toid r t Sh iz ( em () W ) 10 0.2 0 30 0.5 0 0 1. 45 30 0 1. 04 55 over 55,000 episodes.
Battery Size (W.h) 13.9 50 50 50
Propeller Size (m) 0.18 0.3 0.30 0.23
Payload(kg) 0.01 3.0 0.63 2.69
𝒀𝑛 Se Fla igrc hh
t
RSp ane ge ed (( km m/ )s ) 0 3. .0
4
31 1.0
.4
10 6. .1 94
5
50 .. 09
4
Cruising Speed (km/hr) 4.46 11.91 5.62 7.99
TABLE II: Talent Metrics and Design Variables of UAVs
achievedinCo-designcomparedwithBaselineFixedDesign
A. Talent Metrics and Pareto Model
(a) (b)
Here, we focus on developing a Blended-Wing-Body
(BWB) integrated Quadcopter (BIQU) used in our previous
studies [18]. Key morphological parameters that determine
the performance attributes are the dimensions (length and
width)ofthequadcopter’sarm,themotorpower,thebattery
capacity,propellerdiameters,andthepayload.Thelowerand
upper bounds of these parameters are shown in table II. We
identify three unique talents based on the characteristics of (c) (d)
the SWARM-CAE problem: search speed (Y ), cruising
TL,1
Fig.6:Traininghistory(Talentsandoverallreward):a)Flight
speed(Y )andflightrange(Y ).Forthesearchspeed,
TL,2 TL,3
range b) Cruise speed, c) Search speed, d) Mission Rewards
we assume a linear correlation between the sensor and its
weight; thus, the higher the payload, the higher the search
speed. We executed the NSGA-2 multi-objective optimizer Figure 6 d) shows the rewards converge at around 22000
6 times with an initial population of 120 and 40 generationsepisodes. At the onset of training, the confidence level or
each; the non-dominated samples from these runs werethe variance of the policy due to the Gaussian distribution
again filtered based on the non-dominated filtering processin RL policy is high, and this allows for higher exploration
to get the final Pareto points. For creating a Pareto modelin talent space. This is evident from Fig 6 a), b), and c). As
as explained in section II-B, we considered search speedthe training progresses and the rewards get to a steady level,
and the velocity to be independent variables and created athe confidence level increases, which reduces the variance
polynomiallinearregressionmodeltoapproximatetheFightin the talents. The final cumulative standard deviation in
Range. The resulting model is shown in Fig 5. the trained policy after 55,000 episodes is 13%. The policy
enforces higher range, lower speed, and lower search speed.
B. Behavior Learning subject to Talent Boundary The training scenarios are created in such a way that the
1) Policy Creation: We used Stable-baselines3 [24], agoal locations(victim’s location) are at different distances
standard open-source RL Library for creating a custom pol-from the depot location, and in order to be successful in
icy, distribution, and neural networks as discussed in sectionall scenarios, the UAVs require a higher range. Due to its
IV.ThepolicyoutputsthesearchspeedYˆ ,cruisingspeedhigh range, it has to sacrifice its speed and/or payload.
TL,1
Yˆ and a behavioral action(a). Since each UAV platoon consists of multiple robots and
TL,2
they collaboratively search different areas of the buildings,
2) Training: Wetrainedtheswarmtacticspolicyusingthe
it is not necessary for an individual vehicle to have high-
Talent-infused Actor-Critic method for 3 million timesteps
simulated in the Buffalo Downtown region, keeping thequality sensors leading to high payloads, and this explains
the convergence of search speed to a low value. The total
platoons counts fixed at N :4,N :4,N :10, and
UAV UGV BLD
training time is approximately 160 hours, and hence, for
N :6,withaconsistentdepotlocation.Eventhoughevery
ADV
each episode(single function evaluation of f (Y ,Φ)), it
episode of training can have any combination of the above- L TL
takes 10.47 seconds. The optimized talents Y∗ from the
mentioned parameters, we set it as constant during training TL
RL policy after the training are given in table II
since this enables us to stack the state space variables as
tensors for faster training using GPUs. Each episode can
C. Morphology Finalization
be considered as a function evaluation with respect to the
behavior and talents f (Y ,Φ)=R, where R is the mission Using the optimized talents Y∗ we got from the train-
L TL TL
completion reward given by Eq. (12). We introduced 30ing, We use Mixed-Discrete Particle Swarm Optimization
unique scenarios, varying goals, robot numbers(they form(MDPSO)[32] to optimize for the best morphology. The
as 4 platoons), target buildings, and adversaries, yet allobjective here is to find suitable morphology that is as
scenarios began from the same depot. In each episode, aclose as possible to the required talents. With an initial
scenariowasrandomlyselectedfromthispool,andthepolicypopulationof150,theoptimizationranfor80iterations.The
underwent training for 3 million timesteps with a learningconvergencehistoryisshowninFig7.Thefinalmorphology
rateof1e−3.Weconductedparalleltraining(10threads)onclosely matched the learned talents, with an error under 0.9,
a 24-core server with 64 GB of memory. Figure 6 displaysdemonstratingtheeffectivenessofthepolynomialregression-       
       
           
       
       
   
       
       
 & R  G H V L J Q  ) L [ H G  & R  G H V L J Q  ) L [ H G
               
         
         
    
         
         
    
       
 & R  G H V L J Q  ) L [ H G  & R  G H V L J Q  ) L [ H G
Fig. 9: Performance of Co-design Policy and Fixed Baseline
Fig. 7: Morphology Finalization convergence history.
TrainedPolicyintrainingmap.Themetricsusedforcompar-
isonsarea)Rewards,b)Survivalrate(Numberofremaining
robots at end of mission), c) Completion Time of the rescue
basedParetomodel(TableII).Theoptimizationprocesstook
operation,andd)SuccessRateofrescueoperations(Barplot
110.8 seconds.
showing total successful mission completion)
outputs the action a to be taken given the state s, whereas
the talents remain fixed. The state space, reward, and all
hyper-parameters are kept the same.
To evaluate the performance of the trained policies, we
handcrafted an additional 40 distinct scenarios with high
complexity for testing. These scenarios contain varying crit-
ical swarm parameters such as the number of robots, the lo-
cations of targets and goals, and the presence of adversaries.
Fig. 8: CAD model showing the comparison of optimized Each policy was tested for 250 episodes, and the outcomes
UAV design (left) and the Baseline UAV (right); note that are presented in Fig 9. As shown in the figure, the co-
they also are significantly different in their motor size and design policy achieves asuccess rate of about 82%, whereas
payload capacity, which are not illustrated here. the fixed design policy achieves only 61%. Our analysis
focuseson3keymetrics:totalrewards,completiontime,and
survivalrate.Thesemetricsareonlyapplicableforsuccessful
D. Performance Analysis scenarios and hence don’t reflect the performance. The total
In this section, we compare the results of our co-designed rewards, quantifying the cumulative reward achieved at the
policy with a fixed-design policy. To get a suitable talent end of each episode, are computed as per Eq. (12). Notably,
for fixed-design policy, we calculated the average distance the completion time and survival rate influence the reward
between the depot and target locations with a cautious path metric. Hence, we also provide a comparison of these pa-
selection, which is 758 meters, and the distance between rameters. The co-design policy also has a higher reward and
each target location is approximately 550 meters. There are higher survival rate with less variance than that of a fixed-
amaximumof8targetlocations,andifasingleUAVplatoon design policy. The co-design Policy has a higher completion
decides to go to all 8 target locations, the maximum range time; this is primarily due to low cruising speed. The CAD
it requires is around 5 KM. We randomly sampled a Pareto model comparing the Baseline UAV and Co-designed UAV
point from the Pareto solutions we got from section V-A is shown in Fig 8.
with a 5 KM range. Note that obtaining values from Pareto
E. Computing Costs Analysis
points results from the sequential design process explained
in section II. We narrowed down a set of values based on In this section, we compare the computational time taken
environmental factors and selected the optimal Pareto value by our proposed co-design framework to the nested co-
to ensure the most robust and effective comparison. Since optimization.
we are compromising on the range, we get higher search Our talent-behavior co-optimization was trained in a
speed and cruising speed, allowing the UAV platoons to go workstation with Intel CPU-12900k (24 Threads), NVIDIA
to different locations and complete the search faster. Both 3080ti, and 64 GB of RAM. The computation times for
the optimized talents and fixed talents are shown in Fig 5 each step in our co-design framework are as follows: 6.7
and in table II. We trained this RL policy for the same minutesfor6runsofNSGA-2toobtaintalentmetrics,nearly
number of episodes as the co-design policy; note that here, negligibletime(3.5seconds)forcreatingaParetoboundary,
thepolicydoesn’tcontainanadditionaltalentnetworkaswe approximately 160 hours for talent-behavior actor-critic op-
did for the co-design policy. The fixed design policy only timizationusing20parallelenvironmentsforexperiencecol-
 H P L 7  Q R L W H O S P R &
 V G U D Z H 5
 H W D 5  O D Y L Y U X 6
    H W D 5  V V H F F X 6lection, and 1.8 minutes for finalizing morphology. Overall, [3] D. Hachiya, E. Mas, and S. Koshimura, “A reinforcement learning
our co-design framework incurs a total computational cost model of multiple uavs for transporting emergency relief supplies,”
AppliedSciences,vol.12,no.20,p.10427,2022.
of approximately 160.10 hours, with a significant portion of
[4] C.Wang,J.Wang,X.Zhang,andX.Zhang,“Autonomousnavigation
this time allocated to the learning process. ofuavinlarge-scaleunknowncomplexenvironmentwithdeeprein-
For the same settings of NSGA-2, i.e., a population size forcementlearning,”in2017IEEEGlobalConferenceonSignaland
InformationProcessing(GlobalSIP). Ieee,2017,pp.858–862.
of 120 and 40 generations, and considering each behavioral
[5] K. Sims, “Evolving 3d morphology and behavior by competition,”
learning takes 20,000 episodes, a single run of NSGA-2
Artificiallife,vol.1,no.4,pp.353–372,1994.
will take an estimated 2333 hours. This estimate is based [6] H. Lipson and J. B. Pollack, “Automatic design and manufacture of
on the assumption that all 120 behavioral learning happen roboticlifeforms,”Nature,vol.406,no.6799,p.974,2000.
[7] B. Weel, E. Crosato, J. Heinerman, E. Haasdijk, and A. Eiben, “A
in parallel, while each behavioral learning uses 20 parallel
robotic ecosystem with evolvable minds and bodies,” in Evolvable
environments to collect experiences. With the nested co- Systems(ICES),2014IEEEInternationalConferenceon. IEEE,2014,
optimization, there is a necessity to search the overall pp.165–172.
[8] M. Khazanov, B. Humphreys, W. D. Keat, and J. Rieffel, “Exploit-
morphology space, whereas, in our co-design approach, the
ing dynamical complexity in a physical tensegrity robot to achieve
morphology-talent mapping and utilizing the pareto front locomotion.”inECAL. Citeseer,2013,pp.965–972.
for talent-behavior learning convert the morphology search [9] N. Cheney, R. MacCurdy, J. Clune, and H. Lipson, “Unshackling
space into Talent-Pareto search (search within the non- evolution:evolvingsoftrobotswithmultiplematerialsandapowerful
generative encoding,” in Proceedings of the 15th annual conference
dominatedsolutions),whichessentiallymakesourco-design
onGeneticandevolutionarycomputation. ACM,2013,pp.167–174.
framework extremely frugal in terms of computational time [10] M. Komosinski and J. Polak, “Evolving free-form stick ski jumpers
and computational hardware requirements compared to the and their neural control systems,” in Proceedings of the National
Conference on Evolutionary Computation and Global Optimization,
nested co-optimization approach.
Poland,2009,pp.103–110.
VI. Conclusion [11] J.Bongard,“Morphologicalchangeinmachinesacceleratestheevo-
lution of robust behavior,” Proceedings of the National Academy of
In this paper, we introduce an efficient co-design frame- Sciences,vol.108,no.4,pp.1234–1239,2011.
[12] G.Zardini,Z.Suter,A.Censi,andE.Frazzoli,“Task-drivenmodular
work to concurrently design the behavior and morphology
co-designofvehiclecontrolsystems,”in2022IEEE61stConference
by decomposing this optimization process into multiple onDecisionandControl(CDC). IEEE,2022,pp.2196–2203.
search processes, the most critical among which is a talent- [13] F. Bergonti, G. Nava, V. Wu¨est, A. Paolino, G. L’Erario, D. Pucci,
andD.Floreano,“Co-designoptimisationofmorphingtopologyand
behavior co-learning process that is also constrained by a
controlofwingeddrones,”arXivpreprintarXiv:2309.13948,2023.
pre-computedtalentPareto.ThisprocessusesanovelTalent-
[14] G. Bravo-Palacios, A. Del Prete, and P. M. Wensing, “One robot
infused Actor-Critic. To demonstrate the effectiveness of the formanytasks:Versatileco-designthroughstochasticprogramming,”
proposed framework, we apply it to design the morphology IEEERoboticsandAutomationLetters,vol.5,no.2,pp.1680–1687,
2020.
and behavior of quadcopter type UAVs that are operating as
[15] A. Gupta, S. Savarese, S. Ganguli, and L. Fei-Fei, “Embodied intel-
a swarm along with a team of UGVs. Here, the behavior ligencevialearningandevolution,”Naturecommunications,vol.12,
encompasses tactical decisions regarding tasks to allocate to no.1,p.5721,2021.
different UAVs/UGVs in order to complete the mission in [16] C. B. Schaff, D. Yunis, A. Chakrabarti, and M. R. Walter,
“Jointly learning to construct and control agents using deep
minimal time and with minimal loss of robots due to adver- reinforcement learning,” 2019 International Conference on Robotics
saries. These decisions are provided by the behavior policy and Automation (ICRA), pp. 9798–9805, 2018. [Online]. Available:
https://api.semanticscholar.org/CorpusID:3352260
model,trainedbygraphRL.Comparedtoabaselinesequen-
[17] K.S.Luck,H.B.Amor,andR.Calandra,“Data-efficientco-adaptation
tial design (with morphology chosen from the talent Pareto of morphology and behaviour with deep reinforcement learning,” in
and behavior learned separately), the co-design obtained ConferenceonRobotLearning. PMLR,2020,pp.854–869.
outcome performs significantly better in terms of mission [18] C. Zeng, P. KrisshnaKumar, J. Witter, and S. Chowdhury, “Efficient
concurrent design of the morphology of unmanned aerial systems
successrate.Theoverallco-designcostswerealsoestimated andtheircollective-searchbehavior,”in2022IEEE/RSJInternational
to be 14 times smaller than what a nested co-optimization ConferenceonIntelligentRobotsandSystems(IROS). IEEE,2022,
would have cost in terms of computing time. In its current pp.388–393.
[19] A.Behjat,H.Manjunatha,P.K.Kumar,A.Jani,L.Collins,P.Ghas-
form,theproposedapproachhingesontheabilitytoidentify
semi,J.Distefano,D.Doermann,K.Dantu,E.Esfahanietal.,“Learn-
talent metrics that are purely a function of morphology (i.e., ing robot swarm tactics over complex adversarial environments,” in
independent of the control/behavior models). Hence, future 2021internationalsymposiumonmulti-robotandmulti-agentsystems
(MRS). IEEE,2021,pp.83–91.
work could explore autoencoders or related approaches to
[20] H. Manjunatha, P. KrisshnaKumar, J. P. Distefano, A. Jani,
identify latent spaces to serve as the talent space instead A. Behjat, P. Ghassemi, S. Chowdhury, K. Dantu, and E. T.
and, therefore, allow the presented decomposition approach Esfahani, “SHaSTA: An Open-Source Simulator for Human and
Swarm Team Applications,” May 2024. [Online]. Available: https:
to work in a wider range of problems.
//www.researchsquare.com/article/rs-4338790/v1
References [21] C. Yu, W. Zhang, H. Lai, Z. Tian, L. Kneip, and J. Wang, “Multi-
embodiment legged robot control as a sequence modeling problem,”
[1] A.R.Cheraghi,S.Shahzad,andK.Graffi,“Past,present,andfutureof in2023IEEEInternationalConferenceonRoboticsandAutomation
swarmrobotics,”inIntelligentSystemsandApplications:Proceedings (ICRA). IEEE,2023,pp.7250–7257.
of the 2021 Intelligent Systems Conference (IntelliSys) Volume 3. [22] D.J.Christensen,J.Campbell,andK.Stoy,“Anatomy-basedorganiza-
Springer,2022,pp.190–233. tionofmorphologyandcontrolinself-reconfigurablemodularrobots,”
[2] L. Chen and S.-L. Ng, “Securing emergent behaviour in swarm NeuralComputingandApplications,vol.19,pp.787–805,2010.
robotics,” Journal of Information Security and Applications, vol. 64, [23] W.S.CheeandJ.Teo,“Simultaneousevolutionary-basedoptimization
p.103047,2022. ofcontrollerandmorphologyofsnake-likemodularrobots,”in20144th International Conference on Artificial Intelligence with Applica-
tionsinEngineeringandTechnology. IEEE,2014,pp.37–42.
[24] A. Raffin, A. Hill, A. Gleave, A. Kanervisto, M. Ernestus, and
N. Dormann, “Stable-baselines3: Reliable reinforcement learning
implementations,” Journal of Machine Learning Research, vol. 22,
no. 268, pp. 1–8, 2021. [Online]. Available: http://jmlr.org/papers/
v22/20-1364.html
[25] P.Dhariwal,C.Hesse,O.Klimov,A.Nichol,M.Plappert,A.Radford,
J. Schulman, S. Sidor, Y. Wu, and P. Zhokhov, “Openai baselines,”
https://github.com/openai/baselines,2017.
[26] S.Huang,R.F.J.Dossa,A.Raffin,A.Kanervisto,andW.Wang,“The
37implementationdetailsofproximalpolicyoptimization,”inICLR
Blog Track, 2022, https://iclr-blog-track.github.io/2022/03/25/ppo-
implementation-details/. [Online]. Available: https://iclr-blog-track.
github.io/2022/03/25/ppo-implementation-details/
[27] C. C. Cheah, S. P. Hou, and J. J. E. Slotine, “Region-based shape
controlforaswarmofrobots,”Automatica,vol.45,no.10,pp.2406–
2411,2009.
[28] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
“Proximal policy optimization algorithms,” arXiv preprint
arXiv:1707.06347,2017.
[29] S. Verma and Z. L. Zhang, “Graph capsule convolutional neural
networks,”2018.
[30] S.Paul,P.Ghassemi,andS.Chowdhury,“Learningscalablepolicies
overgraphsformulti-robottaskallocationusingcapsuleattentionnet-
works,”in2022InternationalConferenceonRoboticsandAutomation
(ICRA). IEEE,2022,pp.8815–8822.
[31] S. Paul and S. Chowdhury, “A scalable graph learning approach
to capacitated vehicle routing problem using capsule networks and
attentionmechanism,”inInternationalDesignEngineeringTechnical
Conferences and Computers and Information in Engineering
Conference, vol. Volume 3B: 48th Design Automation Conference
(DAC), 08 2022, v03BT03A045. [Online]. Available: https://doi.org/
10.1115/DETC2022-90123
[32] S. Chowdhury, W. Tong, A. Messac, and J. Zhang, “A mixed-
discreteparticleswarmoptimizationalgorithmwithexplicitdiversity-
preservation,”StructuralandMultidisciplinaryOptimization,vol.47,
pp.367–388,2013.