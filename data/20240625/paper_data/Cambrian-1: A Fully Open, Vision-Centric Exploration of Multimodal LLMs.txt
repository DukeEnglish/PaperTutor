Cambrian-1: A Fully Open, Vision-Centric
Exploration of Multimodal LLMs
ShengbangTong*,EllisBrown*,PenghaoWu*,SanghyunWoo,ManojMiddepogu,
SaiCharithaAkula,JihanYang,ShushengYang,AdithyaIyer,XichenPan,AustinWang,
RobFergus,YannLeCun,SainingXie†
New York University
Abstract
We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a vision-
centricapproach. Whilestrongerlanguagemodelscanenhancemultimodalcapabilities,the
designchoicesforvisioncomponentsareofteninsufficientlyexploredanddisconnectedfrom
visualrepresentationlearningresearch. Thisgaphindersaccuratesensorygroundinginreal-
worldscenarios. OurstudyusesLLMsandvisualinstructiontuningasaninterfacetoevaluate
variousvisualrepresentations,offeringnewinsightsintodifferentmodelsandarchitectures—
self-supervised, strongly supervised, or combinations thereof—based on experiments with
over20visionencoders. We criticallyexamineexistingMLLMbenchmarks, addressing the
difficulties involved in consolidating and interpreting results from various tasks, and intro-
duce a new vision-centric benchmark, CV-Bench. To further improve visual grounding, we
proposetheSpatialVisionAggregator(SVA),adynamicandspatially-awareconnectorthat
integrateshigh-resolutionvisionfeatureswithLLMswhilereducingthenumberoftokens. Ad-
ditionally,wediscussthecurationofhigh-qualityvisualinstruction-tuningdatafrompublicly
availablesources,emphasizingtheimportanceofdatasourcebalancinganddistributionratio.
Collectively, Cambrian-1notonlyachievesstate-of-the-artperformancebutalsoservesasa
comprehensive, open cookbook for instruction-tuned MLLMs. We provide model weights,
code,supportingtools,datasets,anddetailedinstruction-tuningandevaluationrecipes. We
hopeourreleasewillinspireandaccelerateadvancementsinmultimodalsystemsandvisual
representationlearning.
Website https://cambrian-mllm.github.io
Code https://github.com/cambrian-mllm/cambrian
Models https://huggingface.co/nyu-visionx/
Data https://huggingface.co/datasets/nyu-visionx/Cambrian-10M
CV-Bench https://huggingface.co/datasets/nyu-visionx/CV-Bench
Evaluation https://github.com/cambrian-mllm/cambrian
*ProjectLead
†CorrespondingAuthor
4202
nuJ
42
]VC.sc[
1v06861.6042:viXraContents
1 Introduction 3
2 MultimodalLLMs: PreliminariesandRelatedWork 4
3 EvaluatingVisualRepresentationsthroughMLLMs 6
3.1 AnalyzingtheBenchmarks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.2 CambrianVision-CentricBenchmark(CV-Bench) . . . . . . . . . . . . . . . . . . . 7
3.3 InstructionTuningRecipes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
3.4 MLLMsasaVisualRepresentationEvaluator . . . . . . . . . . . . . . . . . . . . . 10
3.5 CombiningMultipleVisionEncoders . . . . . . . . . . . . . . . . . . . . . . . . . 12
4 SpatialVisionAggregator(SVA):ANewConnectorDesign 12
5 InstructionTuningDataforTrainingMLLMs 15
5.1 DataCollection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
5.2 DataCuration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
5.3 Alleviatingthe“AnswerMachinePhenomenon”viaSystemPrompts . . . . . . . 17
6 StateoftheArtPerformance 18
7 Discussion 19
References 22
A Training,Infrastructure,andImplementation 29
B AnalyzingtheBenchmarks 29
C CambrianVision-CentricBenchmark(CV-Bench) 29
C.1 CurationProcedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
C.2 Humanverification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
C.3 BenchmarkEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
D VisionModelsinMLLMs 31
D.1 DetailsofVisionModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
D.2 FullResultsofDifferentVisionBackboneinMLLM . . . . . . . . . . . . . . . . . 31
D.3 ModelEnsemble . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
E Data 34
E.1 CatalogofVisualInstructionData . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
E.2 AdditionalSystemPromptsusedinCambrianData . . . . . . . . . . . . . . . . . 34
E.3 DataEngine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
E.4 Fullresultsondatacurationexperiment . . . . . . . . . . . . . . . . . . . . . . . . 39
E.5 737Kand5MMixes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
F ImplementationDetails 39
G EvaluationDetails 39
G.1 SystemPromptsUsedinEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . 39
G.2 AblationStudyonFuzzyMatchingVsLLMJudgement . . . . . . . . . . . . . . . 40
21. Introduction
Thereisalong-standingdebateinphilosophyaboutwhetherunderstandingandmeaningin
languagerequiresensorygrounding. Aristotle’semphasisonacquiringknowledgethrough
sensory experience and empirical observation was central to his ancient Peripatetic school
andremainsinfluentialtothisday[7]; Aquinasfamouslyformalizedtheseideasinthe13th
centurywiththePeripateticaxiom: “Nihilestinintellectuquodnonsitpriusinsensu”(Nothingis
intheintellectthatwasnotfirstinthesenses)[6]. Thoughmanyphilosophersdisagree[18],
it is evident that having robust and highly capable sensory grounding is at least beneficial.
ConsidertheCambrianexplosion,duringwhichtheemergenceofvisionisbelieved[98]tohave
been crucial for early animals to not only find food and avoid predators but also to evolve
andimprove. Infact,mosthumanknowledge(andnearlyallanimalknowledge)isacquired
throughsensoryexperienceslikesight,hearing,touch,taste,andsmell,throughinteractions
withthephysicalworld[100]. Thesesensoryexperiencesarefundamentaltounderstandingthe
worldaroundusandarecrucialforreal-worldactionsanddecision-making.
Beyond philosophical debates, recent advances in multimodal large language models
(MLLMs) have brought the topic of visual representation learning vs. language understanding
intopracticalfocus. Languagemodelshaveshownstrongscalingbehaviors[48],andrecent
advancementsinmultimodallearningarelargelydrivenbythedevelopmentofbetter,larger
LLMs[74]. Ontheotherhand,thedesignchoicesforvisioncomponentsareofteninsufficiently
explored and disconnected from visual representation learning research. For instance, many
pioneeringframeworkssuchasLLaVA[75]usevisiontransformer-basedCLIPmodels[102,
136],whicharestronglysupervisedbylanguage1,asthevisionfeatureextractor. Whileother
visualrepresentations,suchasself-supervisedDINO[96],arebeingexplored[117],thereisa
lackofcomprehensiveandsystematicstudyinthisdomain. Thisgapexistsprimarilybecause
suchstudiesarechallenging: MLLMsinvolveacomplextrainingandevaluationpipelinewith
numerousdesigndecisionstoconsider. Inthiswork,weaimtobridgethegapbyexploring
MLLMsfromavision-centricperspective. Morespecifically,weuseMLLMinstructiontuning
asanevaluationprotocolforvariousvisualrepresentations(illustratedinFig.1).
Ourmotivationforthisstudyalsostemsfromtwopotentialconcernsofthecurrentmulti-
modallearningresearch: 1)relyingtooheavilytooearlyonlanguagecanactasashortcut[40,
135],compensatingforthedeficienciesinlearningeffectivevisualrepresentations,and2)exist-
ingbenchmarksmaynotprovideadequateguidanceforreal-worldscenarios—wherevisual
groundingiscrucialforrobustmultimodalunderstanding. Theseconcernsarenotunfounded,
asresearchershavestartedtonoticethatvisualgroundingisbecomingabottleneckforapplying
MLLMsinsomechallengingreal-worldapplications,despitesignificantprogressinimproving
generalcapabilities[35,117,127].
Fromanotherperspective,traditionalevaluationprotocolsforvisualrepresentationlearning
(e.g.,linearprobingandend-to-endfine-tuningondatasetslikeImageNet-1K[105],COCO[72],
andADE20K[145])arebecomingsaturatedanddonotreflectthediverseperceptionchallenges
found in real-world distributions. On the other hand, using language in the form of visual
questionanswering(VQA)offersaflexibleandrobustevaluationprotocol. Ourstudyaimsto
explorethisnewprotocoldesign,settingituptogaininsightsthatwillguidethedevelopment
ofbettervisualrepresentationsinthefuture. Furthermore,tobetterevaluatevisualrepresenta-
tionsinthisintegratedsetting,wedevelopavision-centricMLLMbenchmark,CV-Bench,by
transformingtraditionalvisionbenchmarksintoVQAformat(Section3.2).
1WeemphasizethatCLIPtrainingshouldbeconsideredasstronglysupervised,aslanguageprovidessignificantly
richersupervisionthanclasslabels.
3VisualRepresentationLearning-EvaluationProtocols
Pretrained ImageNet-1k CLS
LinearProbing
VisionModel or COCO SEG DET
End-to-EndTuning
ADE20K SEG DET
MultimodalLargeLanguageModels
Pretrained MMB AI2D
VisualInstructionTuning ChartQA MMVP
VisionModel MME MathVista VQA
w/LLMs OCRBench SEED SQA
V*Bench …
Visual InstructionTuning InstructionTuning
ConnectorDesign EvaluationProtocol
Representations Data Recipe
Figure1|WedrawparallelsbetweentraditionalprotocolsandtheuseofMLLMsforevaluatingvisual
representations. MLLMsemployvisualquestionansweringtoaddressadiversearrayofreal-world
perceptiontasks. ThebottomsectionhighlightsthefivekeypillarsstudiedinCambrian-1.
Cambrian-1isstructuredaroundfivekeypillars,eachofferingimportantinsightsintothe
designspaceofMLLMs:
• VisualRepresentations: Weexplorevariousvisionencodersandtheircombinations.§3.4
• ConnectorDesign: Wedesignanewdynamicandspatially-awareconnectorthatinte-
gratesvisionfeatureswithLLMswhilereducingthenumberoftokens.§4
• Instruction Tuning Data: We curate high-quality visual instruction-tuning data from
publicsources,emphasizingtheimportanceofdistributionbalancing.§5
• InstructionTuningRecipes: Wediscussinstructiontuningstrategiesandpractices. §3.3
• Benchmarking: We analyze existing MLLM benchmarks, cluster them into 4 intuitive
groups,andintroduceanewvision-centricbenchmark“CV-Bench”.§3.1,§3.2
As a by-product of our exploration, Cambrian-1 introduces a family of state-of-the-art
MLLMsthatachievetopperformanceacrossdiversebenchmarksandexcelinvisual-centric
tasks(Section6). Weprovidemodelweights,open-sourcecode,datasets,anddetailedrecipesfor
modeltrainingandevaluation. Wehopeourworkwillstrengthentheopenresearchcommunity
andaccelerateresearchinbothvisualrepresentationlearningandmultimodalsystems.
2. Multimodal LLMs: Preliminaries and Related Work
The key components of MLLM research include the Large Language Model, Visual Encoder,
Multimodal Connector, Data Curation Pipeline, Instruction Tuning Strategy, and Evaluation &
Benchmarking. Eachcomponenthasitsintricacies,andunderstandingtheirinteractionspresents
significantchallenges. Ourstudyinvestigatestheseaspectsfromavision-centricperspective.
Large Language Model Advanced LLMs [3, 94, 118, 119] are the foundation of an MLLM.
Afterinstruction-tuningonmultimodaldata,thesemodelscanbepromptedtosolveavarietyof
complextasksandgeneratefree-formresponsesleveraginginputfromavisualencoder. Recent
MLLM research focuses on enhancing the LLM backbone [9, 68, 74], resulting in improved
performance on benchmarkslike MMMU [134] and AI2D [47]. However, this improvement
raisestheconcernthatourcurrentmultimodalevaluationisbiasedbythedevelopmentofLLMs,
neglecting a true assessment of visual perception. For example, some benchmarks such as
4
ConnectorCowboyHat
Pos.
Sorrel
CowboyBoot
Barrel Neg.
A cowboy rides a
Revolver horse at a rodeo.
ClassLabelSupervised LanguageSupervised SSL-Contrastive SSL-Masking Diffusion DepthSupervised SegmentationSupervised
ImageNet-1K[105] CLIP[102] DINOv2[96] MAE[45] StableDiffusion[104] MiDaS[13] SAM[61]
Figure2|Examplesofvariousvisionmodels,objectives,andarchitecturesstudied. Imagefrom[41].
MMMU[134]aredominatedbyLLMcapabilities,underscoringtheneedforevaluationsthat
genuinelyassessmultimodality(seeSection3.1).
VisualEncoder MostMLLMsutilizelanguage-supervisedmodelslikeCLIP[102,113,136],
whichbenefitfromthemassivescaleofnoisywebimage-textdata. However,thereisamuch
broaderpoolofvisualmodelsthatlearnrepresentationsusingonlyvisualsignals—suchasself-
supervisedmodels[8,96],segmentation[61],depth-supervised[67],anddiffusionmodels[104]
(seeFig.2). Recentwork[80,117]advocatesforincorporatingthesediversevisionmodelsinto
MLLMs. Inthisstudy,wesystematicallyexaminetheimpactofvariousvisionbackboneson
MLLMperformance(Section2)andexplorethebenefitsofmodelensembles(Section3.5).
MultimodalConnector Representationsfromavisualencodercannotbenativelyprocessedby
anLLM—theymustbemappedintotheLLMtokenspacebyaconnector. Therearethreeprimary
approachestoconnectordesign: Resamplers[5],Q-Formers[10,31],andMLPProjectors[73,
75]. WebeginourexplorationusinganMLPprojector,whichishighlyeffectivebutpresents
challenges: thevisualtokencountgrowsquadraticallywithimageresolution,inhibitingscaling
contextlengthinputresolution. Forexample,LLaVA-Next[74]requires2880visualtokensto
processone672pximage. Toaddressthis,weexplorenewvisionconnectordesignsthatprocess
high-resolutionimageswhilemaintainingasmallernumberofvisualtokens(Section4).
Instruction Tuning Data Visual instruction tuning data is crucial but hard to collect, as it
rarely naturally exists on the internet. Previous work [31, 73, 91] transforms existing VQA
benchmarks [43, 62] into instruction tuning data, showing marked MLLM performance im-
provements. Withthisinspiration,wecollectallVQAbenchmarksandvisualinteractiondata
thatwecanfind(Fig.9),studydatabalancingandcategorymixtures(Section5.2),anddevelop
aninternetdatacollectionenginetofillinthegaps(Section5.1).
InstructionTuning MostcurrentMLLMsleveragepre-trainedLLMsandvisualencoders,fine-
tuningtheLLMandconnectorusingvisualinstructiontuningdata. Someaspectsofthetuning
recipeareupfordebate,includingwhethertopre-traintheconnectorbeforejointfine-tuning
withtheLLM,andwhethertofreezeorunfreezethevisionencoderduringfine-tuning[56,91].
Additionally,somerecentproprietarymodelsexploreend-to-endtrainingfromscratch[42,95].
Inthiswork,weusepre-trainedmodelsandrevisitthedebatedrecipeaspectswithextensive
studies,providingmoreinsightsforfutureMLLMresearch(Section3.3).
Evaluation&Benchmarking Thereisanextensivesetofbenchmarksthatevaluatevarious
aspects of MLLMs, such as perception [39, 76], knowledge [84, 85], chart interpretation [77,
89],andvisualcapabilities[117,127]. Insteadofover-optimizingforspecificbenchmarks,we
advocateforexaminingaggregatesofbenchmarksthatfocusonspecificcapabilities. Toachieve
this, we analyze existing benchmarks, categorize them, and assess the extent to which they
measuremultimodality(Section3.1). Additionally,wefindtherearecurrentlyfewbenchmarks
focused on vision-centric evaluation, and those that do exist contain relatively few images,
leadingtohighervarianceduringevaluation. Toaddressthisissue,weproposeanewvision-
centricbenchmarkbyreformulatingclassicvisiontasks(Section3.2).
53. Evaluating Visual Representations through MLLMs
CurrentMLLMspredominantlyrelyonCLIP[102]asthevisualencoderduetoitspre-alignment
withlanguageandeaseofadaptationtotheLLMtokenspace. However,stronglanguagepriors
can be a double-edged sword—they compensate for deficiencies in learning effective visual
representations[117]anddiminishinsightsgainedfromextensivevisualrepresentationlearning
research. Inthissection,wesystematicallyevaluatehowvariousvisualencoderchoices(see
Fig. 2) impact the multimodal capabilities of MLLMs. We also advocate for using MLLM
evaluationasarobustframeworkforassessingvisualrepresentationmethods,movingbeyond
traditionalprotocolslikelinearprobingandend-to-endfine-tuningtomorefaithfullyreflect
thediverseperceptionchallengesinreal-worldscenariosandtobetterguidethedevelopment
ofimprovedvisualrepresentations. Specifically,inthissectionwe:
§3.1. AnalyzetheBenchmarks
§3.2. IntroduceCV-Bench
§3.3. StudyInstructionTuningRecipes
§3.4. UseMLLMsasaVisualRepresentationEvaluator
§3.5. InvestigateCombiningMultipleVisionEncoders
3.1. AnalyzingtheBenchmarks
ToeffectivelyevaluatevisualrepresentationsandMLLMs,wefirstneedtoselectbenchmarks
thataccuratelyassessthemultimodalcapabilitiesofthesemodels. Weuseasuiteofcommonly
usedbenchmarks[19,39,47,50,76,77,84,85,89,112,117,128,134],whichistheintersection
of those used in recent MLLM research [68, 70, 128]. To help interpret our results, we begin
by analyzing the benchmarks themselves. Here, we train MLLMs with 23 different vision
backbones(seeTable9)fromavarietyofmodelfamilies(seeFig.2)usinga2-stageinstruction
tuningprocessinitiallyproposedin[75]: firsttrainingconnectoron1.2Madapterdatafrom
ShareGPT-4V[22]followedbyfine-tuningboththeconnectorandLLMon737Kinstruction
tuningdata(seemoredetailsinAppendicesE.5andF).FullbenchmarkresultsinTable10.
Who’sansweringthequestion: theLLMorMLLM? Determiningwhetherabenchmarktruly
needsvisualinputtobesolvedhasbeenapersistentchallengeinvision-languageresearch[2,
21, 43, 87]. In this study, we compare the performance of MLLMs with and without visual
input2,andalsocalculatetheexpectedscoreviarandomlyguessing. Thesethreeconditions
2Wenotethatourinstruction-tuningdataincludestext-onlydata,sotext-onlyquestionsarenotOOD.
Figure 3 | Left: Performance comparison of MLLMs with visual input enabled and disabled across
variousbenchmarks. Benchmarksaresortedbythedifferencebetweentheaveragescorewithvision
enabledanddisabled. Right: Principalcomponentanalysisdisplayingclustersofbenchmarksbased
onperformancemetrics,withbubblesizecorrespondingtobenchmarksize. Welabeltheclustersas
“General”ingreen,“Knowledge”inyellow,“Chart&OCR”inred,and“Vision-Centric”inblue.
6are visualized in Fig. 3-left, with benchmarks sorted by the difference between the average
scorewithvisionenabledanddisabled. SQA-I3,MMMU,MathVista, andAI2Ddisplayless
thana5%gapbetweenvisionenabledanddisabled,suggestingthatthesebenchmarksmay
notsignificantlydependon visual inputandratherheavilyrelyonthebaseLLM.TextVQA
andGQAbothdemonstrateanearly40%positivegapbetweenrandomguessingandvision-
disabledscores,implyingastronglanguagebiasinthesebenchmarks. Ontheotherhand,the
vision-disabledperformanceonbenchmarkslikeMMVPandMMEPerceptionisnotablyworse
thanrandomguessing,suggestingthatstrongvisualgroundingisparticularlycrucial.
ClusteringtheBenchmarks TobetterunderstandthedifferentaspectsofMLLMperformance,
weanalyzethecorrelationsbetweentheperformanceofour23MLLMsoneachbenchmark.
A confusion matrix (Fig. 15) reveals that certain benchmarks, such as MMMU, are largely
uncorrelated with the others. We perform principal component analysis on the benchmark
scoresandobservetheformationofclusterscorrespondingto“General,”“Knowledge,”“Chart
&OCR,”and“Vision-Centric”categories(Fig.3-right). WeassignMMMUtotheknowledge
categorybasedonthetypesofquestionsitincludes(seeAppendixB).Wealsofindthatexisting
vision-centricbenchmarks[117,128]areofinsufficientsize(seeFig.3-right),challengingthe
robustnessofevaluatingsuchcapabilities. Furthermore,thesebenchmarksdonotcovercrucial
visualelementssuchasdepthandspatialawareness.
Finding1: Mostbenchmarksdonotproperlymeasurevision-centriccapabilities,and
theonesthatdohaveveryfewsamples.
3.2. CambrianVision-CentricBenchmark(CV-Bench)
Toaddressthelimitationsofexistingvision-centricbenchmarks,weintroducetheCambrian
Vision-CentricBenchmark(CV-Bench). With2638manually-inspectedexamples,CV-Bench
providessignificantlymoreexamplesthanothervision-centricMLLMbenchmarks—3.5×more
than RealWorldQA [128] and 8.8× more than MMVP [117]. By repurposing standard vision
benchmarks [16, 72, 145], we can assess models at classic vision tasks within a multimodal
context. Leveraging the rich ground truth annotations from the benchmarks, we formulate
naturallanguagequestionsthatprobethefundamental2Dand3Dunderstandingofthemodels.
AsvisualizedinFig.4anddetailedinTable1,CV-Benchevaluates2Dunderstandingviaspatial
relationships&objectcounting,and3Dunderstandingviadepthorder&relativedistance.
CV-Bench Curation Below we describe the procedure for programmatically constructing
questionsforeachtask. Toensurereliability,wealsomanuallyinspecteachquestion,removing
thosethatareunclear,ambiguous,orerroneous. SeeAppendixCfordetails.
SpatialRelationship(2D). Weconsiderimageswithtwodistinctground-truthobjectcategories
and use visual prompts (bounding boxes) to avoid ambiguity when multiple instances are
present. In these questions, we designate an anchor object, and the question asks for the
directionoftheotherobjectrelativetothisanchor.
ObjectCounting(2D). Thisteststhemodel’sabilitytocountobjects. Whengeneratingoptions
forthesequestions,weconstructmultiple-choiceoptionsthataresimilartothecorrectanswer.
For example, if the correct answer is 4, the options might be 2, 3, 4, 5, & 6. We also include
existencecheckexampleswherethecorrectcountis0.
DepthOrder(3D). Weconsiderimageswithtwodistinctcategories(i.e.,objectAandobject
B)andusevisualprompts(e.g.,boundingboxeswithtwodifferentcolors)toavoidambiguity.
3ThesubsetofSQA[84]withimages.
7Spatial Relationship Object Count Depth Order Relative Distance
Where is the cave located with How many cars Which is closer to the camera, sink Which is closer to the chair,
respect to the trees? are in the image? or pillow?​ refrigeratoror door?
Sourcebenchmark:ADE20K[145]andCOCO[72] Sourcebenchmark:Omini3D[16]
Figure4|CambrianVision-CentricBenchmark(CV-Bench). Werepurposestandardvisionbenchmarks
toevaluatethefundamental2Dand3DvisualunderstandingofMLLMs. SeeSection3.2formoredetails.
Type Task Description Sources #Samples
Spatial Determinetherelativepositionofanobjectw.r.t.thean- ADE20K 650
2D
Relationship chorobject.Considerleft-rightortop-bottomrelationship. COCO
Object Determinethenumberofinstancespresentintheimage. ADE20K 788
Count COCO
Depth Determinewhichofthetwodistinctobjectsisclosertothe Omni3D 600
3D
Order camera.
Relative Determinewhichofthetwodistinctobjectsisclosertothe Omni3D 600
Distance anchorobject.
Table 1 | Breakdown of the 2D and 3D tasks evaluated in the Cambrian Vision-Centric Benchmark
(CV-Bench). TheexamplesaresourcedfromADE20K[145],COCO[72],andOmni3D[16].
Wedefine“closer”asfollows: objectAisclosertothecamerathanobjectBonlyifthefarthest
vertexofobjectAiscloser4 tothecamerathanthenearestvertexofobjectBbyaspecifiedoffset.
RelativeDistance(3D). Weconsiderimageswiththreedistinctcategories(i.e.,anchor,object
A,andobjectB),andusevisualprompts(e.g.,boundingboxeswiththreedifferentcolors)to
avoidambiguity. ObjectAiscloserthanobjectBonlyifthefarthestdistancefromA’sverticesis
shorterthantheshortestdistancefromB’sverticestotheanchorobjectbyacertainoffset.
Finding2: ExistingvisionbenchmarkscanbeeffectivelyrepurposedintoVQAques-
tions,enablingtheassessmentofvision-centricMLLMcapabilities.
3.3. InstructionTuningRecipes
MLLMsstartwithpre-trainedLLMandvisionbackbones,connectingthesemoduleswitha
connector such as a projector (MLP). The original LLaVA [73, 75] proposes a 2-stage frozen
trainingprocess: first,pre-trainingaconnectorbetweenfrozenLLMandvisionbackbonesusing
adapterdata(suchasVQAbasedoncaptions),andthenfine-tuningboththeconnectorand
LLMwithinstructiontuningdatawhileleavingthevisionencoderfrozen. Variousstudies[22,
56,74,91]havedrawndifferentconclusionsregardingtheoptimaltrainingmethodologyfor
MLLMs. Here,werevisitthistopicwithextensiveexperiments.
Forourexperiments,wetuneasetofMLLMsusingVicuna-1.5-7BastheLLMbackbone
and each of our 23 vision models (Table 9) as the visual encoder. We use a 737K instruction
tuningdatamixforallexperimentshere(seeAppendixF).Allhyperparametersarematched
acrosseachexperimentalsetting—highlightingtheimpactofdifferenttuningstrategieswith
eachvisualencoder. AllexperimentalsettingsandresultsaretabulatedinAppendixD.2.
4WeusetheEuclideandistance.
8         ȱ                Ȭ                
 Ŗ  ȱ ȱ ȱ ȱ
 Ŗ ǯ ś  ȱ ȱ ȱ ȱ
 ŗ ǯ Ř  ȱ ȱ ȱ ȱ
 ŗ ǯ Ř  ȱ ȱ ȱ ȱ
 Ś Ŗ  ś Ŗ  Ŝ Ŗ  ŝ Ŗ  Ś ś  ś Ŗ  ś ś  Ŝ Ŗ  ř ś  Ś Ŗ  Ś ś  ś Ŗ  ś ś
 Ŗ  ȱ ȱ ȱ ȱ
 Ŗ ǯ ś  ȱ ȱ ȱ ȱ
 ŗ ǯ Ř  ȱ ȱ ȱ ȱ
 ŗ ǯ Ř  ȱ ȱ ȱ ȱ
 Ś Ŝ  Ś ŝ  Ś Ş  Ś ş  Ś Ś ǯ ś  Ś ś ǯ Ŗ  Ś ś ǯ ś  Ś Ŝ ǯ Ŗ  Ś Ŝ ǯ ś  Ś Ś ǯ Ŗ  Ś Ś ǯ ś  Ś ś ǯ Ŗ  Ś ś ǯ ś  Ś Ŝ ǯ Ŗ
 Ŗ  ȱ ȱ ȱ ȱ
 Ŗ ǯ ś  ȱ ȱ ȱ ȱ
 ŗ ǯ Ř  ȱ ȱ ȱ ȱ
 ŗ ǯ Ř  ȱ ȱ ȱ ȱ
 Ř Ŗ  ř Ŗ  Ś Ŗ  ś Ŗ  ŗ Ş ǯ Ŗ  ŗ Ş ǯ ś  ŗ ş ǯ Ŗ  ŗ ŝ ǯ ś  ŗ Ş ǯ Ŗ  ŗ Ş ǯ ś  ŗ ş ǯ Ŗ
 Ŗ  ȱ ȱ ȱ ȱ
 Ŗ ǯ ś  ȱ ȱ ȱ ȱ
 ŗ ǯ Ř  ȱ ȱ ȱ ȱ
 ŗ ǯ Ř  ȱ ȱ ȱ ȱ
 ř ś  Ś Ŗ  Ś ś  ś Ŗ  ś ś  ř ś  Ś Ŗ  Ś ś  ř Ś  ř Ŝ  ř Ş  Ś Ŗ  Ś Ř
Figure 5 | Effect of Training Recipe on Model Performance. Boxplots display the distribution of
benchmarkscoresacrossbenchmarkcategoriesfordifferenttrainingrecipesandtypesofvisualencoders
(Language-Supervised, Self-Supervised, and Other). The four training recipes include freezing the
visualencoderwithvariousamountsofadapterdata(0M ,0.5M ,1.2M )aswellasunfreezingit
with1.2M adapterdata. AmountofAdapterData: Allmodeltypesshowincreasedperformanceon
generalandvision-centricbenchmarkswithmoreadapterdata;knowledgebenchmarksshowmixed
results;OCR&chartbenchmarksbenefitfrommoredataforlanguage-supervisedmodels. Unfreezing:
Unfreezingthevisualencoderwith1.2M adapterdatagenerallybenefitsallcategories. Language-
supervisedmodelsbenefitfromunfreezingacrosstheboard;self-supervisedmodelsbenefitparticularly
wellinvision-centricbenchmarksbutcontinuetostruggleinOCR.
One Stage vs Two Stage Training Recent work [56] advocates for skipping connector pre-
training,claimingthis“reducescomputecostwithoutharmingdownstreamperformance.” Toexplore
whetherthisclaimholds—especiallywhenusingnon-language-supervisedvisualencoders—
weconductexperimentsusing0,0.5M,and1.2Madapterdata. FollowingLLaVA’srecipe[75],
wetuneonlytheconnectorontheadapterdataduringthisfirstphase,beforeunfreezingthe
LLMandconnectorduringinstructiontuningonthe737Kmix. Fig.5showsthatpre-training
theconnectorfirstenhancesmodelperformanceandthatmoreadapterdatafurtherimproves
performance across all domains. Thus, we subsequently adopt 2-stage training with 1.2M
adapterdataasourstandardsetup.
Finding3: Two-stagetrainingisbeneficial;moreadapterdatafurtherimprovesresults.
FreezevsUnfreezeVisionEncoder Therearealsomixedpracticesinfreezing[56,73,75]or
unfreezing[38,74]visionbackbonesduringfine-tuning. Somearguethatunfreezingthevision
backbonesignificantlydegradesperformance[56]. Ourexperimentsdemonstratethatunfreez-
ing benefits performance across all benchmarks except for a marginal change in knowledge
benchmarks(Fig.5). Wesuspectthisisduetothecompositionofthe737Kinstructiontuning
dataandtheLLM-heavyfocusofthesebenchmarks(seeSection3.1). Wenotethatunfreezing
thevisionbackboneintroducesadditionalcomputationaloverhead,whichprohibitstestingon
somelargervisionmodelsundercurrentshardingstrategies(seemoredetailsinAppendixF).
Finding4: Unfreezingthevisionencoderiswidelybeneficial. Language-supervised
modelsalwaysbenefit;SSLmodelsparticularlybenefitonvision-centricbenchmarks.
9
       	
          
      ȱ ǭ ȱ   
        Ȭ               ȱ          
       ȱ     ȱ    Ȭ  Ȧ ŗ Ś ȓ ř ř Ŝ
    Ȭ     ȱ    Ȭ  Ȧ ŗ Ś ȓ Ř Ř Ś
 Ŝ Ŗ     Ȭ     ȱ    Ȭ 
 Ȧ ŗ Ś ȓ ř ŝ Ş
    Ȭ     Ȭ Ŗ Ř ȱ    Ȭ  Ȧ ŗ Ś ȓ ř ř Ŝ
       ȱ    Ȭ  Ȧ ŗ Ŝ ȓ ř Ş Ś
       ȱ   Ś Ŗ Ŗ  Ȧ ŗ Ś ȓ ř Ş Ś
 ś Ŗ          ȱ         Ȭ  ȓ ś ŗ Ř
         ȱ         Ȭ  ȓ ŗ Ŗ Ř Ś
         ȱ         Ȭ    ȓ ŗ Ŗ Ř Ś
     Ȭ          
 Ś Ŗ       Ř ȱ    Ȭ  Ȧ ŗ Ś ȓ ř ř Ŝ
      Ř ȱ    Ȭ  Ȧ ŗ Ś ȓ ś ŗ Ş
     ȱ  ř ȱ    Ȭ  Ȧ ŗ Ŝ ȓ Ř Ř Ś
     ȱ  ř ȱ    Ȭ  Ȧ ŗ Ŝ ȓ Ř Ř Ś
    ȱ    Ȭ  Ȧ ŗ Ŝ ȓ Ř Ř Ś
 ř Ŗ
  Ȭ     ȱ    Ȭ 
 Ȧ ŗ Ŝ ȓ Ř Ř Ś
     
    ȱ    Ȭ 
 Ȧ ŗ Ŝ ȓ ŗ Ŗ Ř Ś
      ȱ ř ǯ Ŗ ȱ    Ȭ  Ȧ ŗ Ŝ ȓ ř Ş Ś
 Ř Ŗ
   Ř ǯ ŗ Ȧ ŗ Ŝ ȓ ś ŗ Ř
       ȱ    Ȭ  Ȧ ŗ Ŝ ȓ Ř Ř Ś
 	                      ȱ ǭ ȱ             Ȭ       
Figure6 | EvaluatingVisualRepresentationswithMLLMsWhilelanguage-supervisedmodelsout-
performself-supervisedorothermodels,awell-trainedself-supervisedmodellikeDINOv2canalso
achievecompetitiveperformanceonvision-centrictasks.
LanguageSupervised Self-Supervised&Other
Model Architecture All G K O V Model Architecture All G K O V
SigLIP ViT-SO400M/14@384 1 1 1 2 1 DINOv2 ViT-L/14@518 1 1 1 1 1
OpenCLIP ConvNeXt-XXL@1024 2 6 8 1 3 DINOv2 ViT-L/14@336 2 2 3 3 2
DFN-CLIP ViT-H/14@378 3 4 2 5 4 MAE ViT-L/16@224 3 5 2 2 4
OpenCLIP ConvNeXt-L@1024 4 8 7 3 8 I-JEPA ViT-H/14@224 4 3 6 8 3
SigLIP ViT-L/16@384 5 5 4 4 6 SD2.1 VAE+UNet/16@512 5 7 9 9 5
OpenAICLIP ViT-L/14@336 6 3 6 6 7 MiDaS3.0 ViT-L/16@384 6 6 8 5 6
EVA-CLIP-02 ViT-L/14@336 7 2 5 8 2 SupViT ViT-L/16@224 7 4 9 4 8
OpenCLIP ConvNeXt-L@512 8 7 3 7 9 MoCov3 ViT-B/16@224 8 8 4 7 7
DFN-CLIP ViT-L/14@224 9 9 9 9 10 MoCov3 ViT-L/16@224 9 9 5 6 9
DINOv2* ViT-L/14@518 10 10 10 10 5 SAM ViT-H/16@1024 10 10 10 10 10
Table 2 | Benchmark performance rankings for MLLMs built upon language-supervised and self-
supervisedvisionencodersacrossallbenchmarks(All),andacrossgeneral(G),knowledge(K),OCR&
chart(O),andvision-centric(V)benchmarkcategories. Fullresultsforallmodelsoneachbenchmarkare
tabulatedinTable12. *WeaddDINOv2heretoshowitsstandingamongsttheCLIPmodels.
3.4. MLLMsasaVisualRepresentationEvaluator
Asdiscussedinearliersections,MLLMsprovideanewinterfacetoexploreaspectsofvision
modelsbeyondtraditionalbenchmarkslikeImageNet-1klinearprobing. Westudythe2-stage
instructiontuningsettingusing1.2Madapterdata,737Kfine-tuningdata,andfrozenvisual
encoderstoallowcomparisonofthewidestrangeofmodels.
WeevaluateonbenchmarksdetailedinSection3.1,calculatingtheaverageperformance5
foreachcategoryandvisualizetheresultsinFig.6(fullresultsinAppendixD).Ourfindings
highlight the advantages of language-supervised models over non-CLIP models across all
benchmarkcategories,withsignificantlybetterperformanceonchartandOCR-relatedbench-
marks. WehypothesizethatthisisduetoCLIP’strainingdata,suchasLAION[107],containing
abundantOCRandtext-heavydata,whereasSSLandothervisionmodelsprimarilytrainonnat-
uralimageswithsignificantlylesstextcontent. Itisalsonoteworthythatlanguage-supervised
modelsaretypicallytrainedwithaverylargepoolofdata,rangingfrom400million[102]to10
billion[23]samples,whereasthelargestvisionself-supervisedtrainingdataset,likeDINOv2,
5Beforeaveraging,wedividetheMMEPerceptionscoreby20tohavethesamescaleasotherbenchmarks.
10
      ȱ        	                      ȱ ǭ ȱ             Ȭ       
 ś Ŗ ǯ Ŗ  ś ś ǯ Ŗ  ś Ś ǯ Ŗ
 ŝ Ř ǯ Ŗ  ś Ŗ ǯ Ŗ
 ŝ Ŗ ǯ Ŗ  Ś ş ǯ Ŗ  Ś ś ǯ Ŗ  ś Ř ǯ Ŗ
 Ŝ Ş ǯ Ŗ  Ś Ŗ ǯ Ŗ  ś Ŗ ǯ Ŗ
 Ŝ Ŝ ǯ Ŗ  Ś Ş ǯ Ŗ  ř ś ǯ Ŗ
 Ŝ Ś ǯ Ŗ  ř Ŗ ǯ Ŗ  Ś Ş ǯ Ŗ
 Ś ŝ ǯ Ŗ
 Ŝ Ř ǯ Ŗ  Ř ś ǯ Ŗ
 Ś Ŝ ǯ Ŗ
 Ŝ Ŗ ǯ Ŗ  Ś Ŝ ǯ Ŗ  Ř Ŗ ǯ Ŗ
 ś Ş ǯ Ŗ
 Ŗ ǯ ŝ  ȱ ȱ  Ŗ ǯ ŝ  ȱ ȉ  ś  ȱ  ś  ȱ ȉ  Ŗ ǯ ŝ  ȱ ȱ  Ŗ ǯ ŝ  ȱ ȉ  ś  ȱ  ś  ȱ ȉ  Ŗ ǯ ŝ  ȱ ȱ  Ŗ ǯ ŝ  ȱ ȉ  ś  ȱ  ś  ȱ ȉ  Ŗ ǯ ŝ  ȱ ȱ  Ŗ ǯ ŝ  ȱ ȉ  ś  ȱ  ś  ȱ ȉ
       ȱ     ȱ    Ȭ  Ȧ ŗ Ś ȓ ř ř Ŝ       Ř ȱ    Ȭ  Ȧ ŗ Ś ȓ ř ř Ŝ
Figure7 | ContinuedFine-TuningNarrowstheGapBetweenCLIPandDINOv2. Theaverageper-
formanceofMLLMsbuiltuponOpenAICLIPViT-L/14@336andDINOv2ViT-L/14@336encodersare
visualizedacrossbenchmarkcategories. Performanceiscomparedwith0.7Mand5Minstructiontuning
datainbothfrozen( )andunfrozen( )settings.DINOv2showssignificantperformanceimprovement
withincreaseddataandunfreezing—surpassingthe0.7M CLIPmodelinseveralbenchmarksand
narrowingandbridgingthegaptothe5M modelinknowledgeandvision-centrictasks,respectively.
consistsofonly142millionsamples[96].
TheperformancecomparisoninFig.6betweenDINOv2,otherSSLmodels,andlanguage-
supervised models underscores the potential for training superior vision-only models with
moredataandimprovedtechniques. Additionally,weobservethathigher-resolutionmodels
particularly enhance performance on chart and vision-centric benchmarks while remaining
neutralongeneralVQAandknowledge-basedVQAs. Whilethemajorityofthebackboneswe
examineareViT-based[33],ConvNet-basedarchitectures(suchasOpenCLIPConvNeXt[79])
areinherentlywell-suitedforhigh-resolutionimageprocessing[121]andcanproducesuperior
resultsonOCR&ChartandVision-Centricbenchmarks. Invision-centricbenchmarks,thegap
betweenlanguage-supervisedandothertypesofvisionmodelsissmaller,withawell-trained
self-supervisedDINOv2modelevenoutperformingsomelanguage-supervisedmodels.
Finding5: High-resencodersgreatlyenhanceperformanceonchart&vision-centric
benchmarks,andConvNet-basedarchitecturesareinherentlywell-suitedforsuchtasks.
NarrowingthegapbetweenLanguage-andSelf-Supervisedmodels Above,weobservethat
DINOv2standsmidwaybetweenself-supervisedmodelsandlanguage-supervisedmodelson
generalandknowledgebenchmarks,evenoutperformingsomelanguage-supervisedmodels
onvision-centricbenchmarksatahigherresolution. Here, westudywhetherthecontinued
finetuning of an MLLM based on a self-supervised model can achieve performance similar
to that of a language-supervised model. Given that DINOv2 is trained with much less data
compared to CLIP, we investigate increasing the amount of visual fine-tuning data while
unfreezing the vision backbones to bridge this gap. Specifically, we scale up the instruction
tuningdatafrom737Kto5M(seemoredetailsinAppendixE.5),andinstructiontuneMLLMs
with DINOv2 ViT-L/14@336 and OpenAI CLIP ViT-L/14@336 encoders in both frozen and
unfrozensettings. InFig.7,weobservethatbyunfreezingthevisionbackbone,theDINOv2-
basedMLLMfine-tunedwith5MdatasurpassestheMLLMtrainedwithaCLIPmodelon0.7M
data. Additionally,thegapbetweenDINOv2andtheCLIPmodelsisreducedunderthe5M
setting.
Finding6: Languagesupervisionoffersstrongadvantages,buttheperformancegap
canbenarrowedwithSSLmethodsgivenenoughdataandpropertuning.
11
            ȱ       VisionBackbone General Knowledge OCR&Chart Vision-Centric
Method
SigLIP+DINOv2 51.61 1,432.02 61.2865.9963.30 68.8235.6929.4060.01 43.0035.7060.4037.54 30.0053.9955.5253.58
SigLIP+DINOv2+ConvNext 54.52 1,503.51 63.8367.9763.95 70.4035.9929.3060.69 48.2036.9064.9745.53 34.6758.6955.7460.33
SigLIP+DINOv2+ConvNext+CLIP 54.74 1,479.46 63.3267.6364.04 71.3935.4929.1059.88 50.2439.6064.5546.12 32.6758.9558.5460.42
SigLIP+ConvNext 54.53 1,494.97 64.6067.9863.58 71.0534.9029.8060.85 50.6438.0064.5346.52 32.0057.9158.8356.58
CLIP+ConvNext 54.45 1,511.08 63.8367.4163.63 70.8035.0930.4059.91 51.3235.0064.4547.88 33.3357.2556.3259.08
SigLIP+DINOv2+ConvNext 53.78 1,450.64 63.5767.7963.63 71.3434.8030.2061.04 49.3237.7064.0545.83 30.0056.2158.0854.33
SigLIP+CLIP+ConvNext 54.53 1,507.28 63.2368.6463.63 71.1035.8930.9059.97 52.3638.5065.4047.92 28.6757.2557.6655.92
Table3 | AllBenchmarkResultsforModelEnsemblewith1.2MAdapterData+737KInstruction
Tuning Data. Here, “SigLIP” = ViT-SO400M/14@384, “DINOv2” = ViT-L/14@518, “ConvNext” =
OpenCLIPConvNeXt-XXL@1024,and“CLIP”=OpenAICLIPViT-L/14@336.
3.5. CombiningMultipleVisionEncoders
AsobservedinFig.6,differentvisionencodersexcelindifferentaspectsofMLLMperformance.
Inthisstudy,weexplorethepotentialofcombiningmultiplevisionencoderstoleveragetheir
distinctiverepresentations,aimingtobuildamorecapableMLLM.
Giventhatdifferentvisionencodersusevaryingarchitecturesandimageresolutions,wein-
terpolatetoafixednumberofvisualtokens(576)inthissubsection(seedetailsinAppendixD.3).
Wethenconcatenatethesetokensalongthefeaturedimension,followingamethodsimilarto
A-MoF proposed in [117]. The results are tabulated in Table 3, where we observe consistent
performanceimprovementswiththeadditionofmoremodels.
Ourstudyindicatesthataddinganon-language-supervisedmodel(DINOv2)canimprove
benchmark performance, especially in vision-centric tasks. Notably, even OCR benchmarks
benefitfromincorporatingDINOv2. Thishighlightstheimportanceofself-supervisedlearn-
ing models in complementing language-supervised models to achieve robust multimodal
understanding. DetailedresultsandconfigurationsareavailableinAppendixD.3.
However, this naive strategy has two limitations: 1) it employs interpolation, which can
leadtoinformationloss,especiallywithvisionencoderswithhigh-resolutionfeaturemaps,and
2)ittreatseachmodelequallyviasimpleconcatenation. Therefore,weseekamoreeffective
strategythatcanmoreflexiblyleveragemodelcombinationswithlessinformationloss.
Finding7: Combiningmultiplevisionencoders,includingvisionSSLmodels,enhances
MLLMperformanceacrossvariousbenchmarks,particularlyinvision-centrictasks.
4. Spatial Vision Aggregator (SVA): A New Connector Design
Toeffectivelyaggregatefeaturesfrommultiplevisionencodersandpreventtheinformationloss
introducedbyinterpolation,weuseasetoflearnablelatentqueriesthatinteractwithmultiple
visionfeaturesviacross-attentionlayers[31]. Inparticular,ourapproachincorporatestwonew
vision-centricdesignprinciples:
1. Weintroducespatialinductivebiasbyexplicitlydefiningtheaggregationspaceforeach
tokeninthequery.
2. WeaggregatevisionfeaturesmultipletimesacrosstheLLMlayers,enablingthemodelto
repeatedlyaccessandintegratenecessaryvisualinformation.
Our new formulation flexibly accommodates multiple vision encoders with varying feature
resolutions,whilepreservingthespatialstructureofvisualdataduringtheaggregationprocess
anditsintegrationwiththeLLM.Themethodiselaboratedbelow.
12
egarevA
PEMM BMM IDEES AQG IAQS
VUMMM
MatsiVhtaM
D2IA
AQtrahC
hcneBRCO
AQVtxeT AQVcoD
PVMM
AQdlroWlaeR
D2hcneB-VC D3hcneB-VCSpatial Vision LLM Transformer
Aggregator
Transformer Block k
Enc N
SVA
• • •
Enc k Transformer Block 1
• • • ns
%
&
oke
SVA ×D
T
Enc 1 ual L! × C G
"
%
%
Vis
',)
Cross
! Attention
Latent Tokens ! Text Tokens
Figure8|SpatialVisionAggregator(SVA).WeproposeSVA,adynamicandspatially-awareconnector
thatintegratesmultiplevisionfeatureswithLLMswhilereducingthenumberoftokens.
Tofacilitateinformationaggregationviacross-attention,wecreatea𝐶-dimensionlearnable
latenttokenx ∈ R𝐶 thatisrepeated𝐿×𝐿timestoforma2Dgrid,servingasthequeryX ∈ R𝐿2×𝐶 .
ThesetofvisualfeaturesFfrom 𝑁 visionencodersserveasthecontext(i.e.,keyandvalue). We
ensuretheoutputresolutionofeveryvisionencoderisamultipleof 𝐿. Formally,thefeature
map of the 𝑘-th vision encoder (F𝑘) has a resolution of 𝑚 𝑘𝐿×𝑚 𝑘𝐿×𝐶, where 𝑚 𝑘 is a positive
integermultiplier,and 𝐿istheheight/widthofthelearnable2Dgridwithhiddendimension𝐶.
Spatialinductivebias Tomaintainthespatialstructureduringcross-attention,wealigneach
tokeninthequerywithaspecificsub-regionofthefeaturemapsinallvisionencoders. Formally,
atokenatrow𝑖andcolumn 𝑗inthequeryx𝑖,𝑗 correspondstothesub-region
F𝑘[𝑚 𝑘·𝑖 : 𝑚 𝑘·(𝑖+1),𝑚 𝑘· 𝑗 : 𝑚 𝑘·(𝑗+1)] ∈ R𝑚2 𝑘×𝐶
ofthe𝑘-thvisionfeaturemap. Asaresult,atokenx𝑖,𝑗 aggregatesatotalof(cid:205) 𝑘𝑚2
𝑘
featuresfrom
𝑁 visionencodersthroughcross-attention(seeFig.8-left).
Specifically,theupdatedqueryvectorq∗ ∈ R1×𝐶 atposition (𝑖, 𝑗) iscomputedas
i,j
q∗
𝑖,𝑗
=softmax(cid:32) q𝑖,𝑗·(cid:2)
k𝑖,𝑗,1,k √𝑖,𝑗
𝐶,2,...,k𝑖,𝑗,𝑁(cid:3)⊤(cid:33)
(cid:2) v𝑖,𝑗,1,v𝑖,𝑗,2,...,v𝑖,𝑗,𝑁(cid:3)
, (1)
where
q𝑖,𝑗
=W𝑄
x
i,j
∈R1×𝐶
,
k𝑖,𝑗,𝑘 =W 𝑘𝐾 F𝑘[𝑚 𝑘·𝑖:𝑚 𝑘·(𝑖+1), 𝑚 𝑘· 𝑗:𝑚 𝑘·(𝑗+1)] ∈R𝑚2 𝑘×𝐶 ,
v𝑖,𝑗,𝑘 =W𝑉 𝑘F𝑘[𝑚 𝑘·𝑖:𝑚 𝑘·(𝑖+1), 𝑚 𝑘· 𝑗:𝑚 𝑘·(𝑗+1)] ∈R𝑚2 𝑘×𝐶 .
Here, q𝑖,𝑗 is the query vector at position (𝑖, 𝑗), calculated using the query projection matrix
W𝑄 ∈ R𝐶×𝐶 . Thekeyvectorsk𝑖,𝑗,𝑘 andvaluevectorsv𝑖,𝑗,𝑘 arecomputedforeachvisionencoder
𝑘usingtheirrespectivekeyandvalueprojectionmatricesW𝐾 ∈ R𝐶×𝐶 andW𝑉 ∈ R𝐶×𝐶 . Since
𝑘 𝑘
(cid:205) 𝑘𝑚2
𝑘
featuresareaggregatedintoasingletoken,weeffectivelyreducethenumberoftokens.
Multi-layervisionaggregation Althoughourproposaleffectivelyaggregatesfeaturesfrom
multiple vision encoders, there is still potential information loss with high-resolution input
(large 𝑚 𝑘) or multiple vision encoders (large 𝑁). Here, a single token would have to han-
dle a larger amount of context information during aggregation. To prevent this, we allow
13
•
•
•
•
•
•cross-attentiontooccurmultipletimesbyinsertingourproposalthroughouttheLLMlayers—
allowingconsistentaccesstotheuncompressedvisualinformation(seeFig.8-right).
Hyperparameters Toflexiblymodulatecapacity,weintroducetwohyperparameters 𝐷and𝐺,
whichindicatethenumberofcross-attentionlayersanddistinctgroupsoflearnablequeries
usedbetweenthevisionmodelsandtheLLM,respectively. Intuitively,alarger 𝐷 allowsfor
morestackedcross-attentionoperationstofacilitatetheaggregationprocess,whilealarger𝐺
enablesawiderrangeofaggregationpatternstobecaptured. The𝐺groupsofqueriesaggregate
visual information separately in parallel and then are concatenated to form the final visual
tokensfortheLLM. 𝐷and𝐺 arealwayssetto1forcross-attentionlayerswithinLLMlayers.
Connector General Knowledge OCR&Chart Vision-Centric
Concat.[117] 67.2 48.9 50.1 52.6
Resampler[51] 63.1 46.5 27.1 42.6
SVA-no-multi-agg 68.0 49.5 55.2 52.6
SVA 68.5 49.7 55.5 53.2
Table4|ComparisonbetweenourSVAandotheraggregationapproaches.TheSVAmoduleconsistently
outperformsotherbaselinesandexcelsinaggregatinghigh-resolutionvisioninformation.
WedemonstratetheefficacyofSVAmoduleusingthebestvisionmodelcombinationresults
fromtheprevioussectionandaVicuna-1.5-7BbaseLLM.Specifically,weemployacombination
offourvisionencoders: OpenAICLIPViT-L/14@336,SigLIPViT-SO400M/14@384,OpenCLIP
ConvNeXt-XXL@1024,andDINOv2ViT-L/14@518. Wecompareourmethodwithtwostrong
baselines: 1)concatenation-based[117]and2)Re-sampler[10,65],whichutilizesasimilarcross-
attentionformbutlacksbothspatialinductivebiasesandmulti-layervisionaggregation. Here,
weincludetwovariantsofourSVAmodule. Thestandardone,“SVA”,uses 𝐷 = 3,𝐺 = 1,and
insertscross-attentionblocksinsidetheLLMwithalayerstrideof3. Toisolatetheadvantages
of spatial inductive biases, we include another SVA variant, “SVA-no-multi-agg”, that does
not add cross-attention blocks inside the LLM and sets 𝐷 = 3 and 𝐺 = 3. Table 4 shows that
SVAoutperformsbothbaselinesinallbenchmarkcategories,withasignificantimprovementin
theOCR&chartcategory(requiringhigh-resolutionfeatureunderstanding). Incontrast,the
Resampler—whichlacksspatialinductivebiases—strugglestocondenseconcatenatedtokens
fromvariousvisiontowersintoalimitednumberoflearnablequeriesviaglobalcross-attention.
𝐷 OCR&Chart 𝐺 OCR&Chart Multi-agg OCR&Chart
2 52.1 1 52.4 No 52.4
3 52.4 2 52.6 Yes 53.3
4 52.8 3 53.1
(a)#layers (b)#groups (c)Multi-layeraggregation
Table5 | AblationsonhyperparameterchoicesforSVA.Enlargingthemodelcapacityofthe
SVAmodulecanfurtherimprovetheperformance.
WefurtherconductablationexperimentsusingOpenAICLIPViT-L/14@336+OpenCLIP
ConvNeXt-L@1024asourbasemodelcombination. WefocusontheOCR&chartcategoriesto
assesstheimpactonhigh-resolutionvisualunderstanding. Theresultsshowthatincreasing
capacityvia 𝐷or𝐺 improvesperformanceandthatallowingvisionaggregationacrossmultiple
layers by adding cross-attention layers within the LLM also enhances performance. More
detailedexperimentalsetupsandanalysesareprovidedintheAppendixF.
Finding8: SpatialinductivebiasanddeepinteractionbetweenLLMandvisionfeature
helptobetteraggregateandcondensevisionfeatures.
14OCR(27.6%) RenderedText[125](10.0K) RefCOCO[131](30.0K) CLEVR[52](350.0K)
FilteredDVQA(1550.0K) VisText[115](9.0K) VizWiz[44](20.0K) TallyQA[1](250.0K)
htaMecneicS D SyV nQ thA D[ o5 g4] [6( 07 ]75 (5.0 00K .0)
K)
F Inin foQ VA Q[ A26 [] 1( 46 ].0 (2K .0)
K)
V LAisu IOal N7W GP[1 T4 -49 V](1 [64 3.0 ](K 1)
1.0K)
FilteC reo dde W( e0 b.8 S% ig)
ht(790.0K)
ArxivQA[69](100.0K) TAT-QA[148](2.0K) IDK[17](11.0K) WebSight[64](10.0K)
OCRVQA[93](80.0K) HiTab[27](2.0K) OKVQA[88](9.0K) DaTikz[12](47.0K)
ScreenQA[49](79.0K) General(33.3%) HatefulMemes[59](8.0K) Design2Code[110](0.5K)
WIkiSQL[144](74.0K) ALLaVA[20](700.0K) OODVQA[120](8.0K) Math(3.2%)
Low-LevelVision[22](50.0K) Q-Instruct[126](400.0K) SketchyVQA[120](8.0K) Geo170K[37](170.0K)
Cambrian-7M DocVQA[90](39.0K) LNQA[101](302.0K) Visualmrc[114](3.0K) RAVEN[139](42.0K)
WTQ[99](38.0K) LVIS-Instruct4V[122](220.0K) Language(23.8%) GeomVerse[57](9.0K)
ChartQA[89](28.0K) LLaVA150K[75](150.0K) OpenOrca[71](994.0K) MathVision[123](3.0K)
IconQA[82](27.0K) VisualGenome[62](86.0K) MathInstruct[133](262.0K) Inter-GPS[83](1.0K)
Chart2Text[55](26.0K) VQAv2[43](83.0K) OrcaMath[92](200.0K) TQA[4](1.0K)
TabMWP[81](23.0K) GPT4VRewritten(77.0K) WizardCoder[86](143.0K) Science(2.9%)
TextCaps[111](22.0K) GQA[50](72.0K) OpenCodeInterpreter[143](66.0K) DataEngine(161.0K)
LLAVAR[140](20.0K) A-OKVQA[108](50.0K) Dolly[30](11.0K) PathVQA[46](32.0K)
ST-VQA[15](17.0K) AlfWorld[137](45.0K) Counting(8.5%) ScienceQA[84](12.0K)
AI2D[58](15.0K) ShareGPT[22](40.0K) FilteredCLEVR(350.0K)
Figure9|Cambrian-7M:ALarge-ScaleCuratedInstructionTuningDatasetforMLLM.Left: Theinner
circleshowstheoriginaldistributionofCambrian-10M.TheoutercircleshowsthecuratedCambrian-7M.
Right: AllthedatasourcesintheCambriandatasetaswellastheonesfilteredindatacuration.
5. Instruction Tuning Data for Training MLLMs
PreviousworkhighlightstheimportanceofdataintrainingMLLMs[38,73,91],butexplicit
investigationsarelimited. Here,wegatherallavailableinstructiontuningdataandexamine
data curation by enhancing diversity, balancing sources, and improving mixtures. Unless
specifiedotherwise,experimentsinvolvefine-tuninganOpenAICLIPViT-L/14@336pxvision
encoder[102]withaVicuna-1.5-7BLLMbase[142].
5.1. DataCollection
CollectingInstructionTuningDatafromexistingdatasources Unlikelanguagedata,mul-
timodal(visual)instruction-tuningdataismuchrarerandhardertocollect. Toaddressthis,
weuseexistingmultimodalbenchmarksanddatasetsinvolvingvisualinteractiondata,such
as Visual Question Answering (VQA) and OCR data. Previous work [138] highlights the
catastrophic forgetting that commonly occurs when fine-tuning multimodal LLMs. To help
maintainconversationalabilities,wealsocollectasmallvolumeofhigh-qualitylanguage-only
instruction-following data. We categorize data into General conversation, OCR, Counting,
Code,Math,Science,andLanguage-onlydata. WelistthedatasourcesinFig.9,andthedetails
ofdatapreparationinAppendixE.
Targeted Internet Data Collection Engine As observed in Fig. 9, there is an unbalanced
distributionofdata. Somecategories,suchasscience,haveveryfewdatasources,andeach
sourcehaslimitedsamples. Intheexistingdatasources,thereare32ksamplesinPathVQA[46]
and12kinScienceQA[84]. Thisscarcitymaybeduetothedifficultyofproducinglarge-scale
yetreliablescientificvisualinstructiontuningdata. Previouswork[66]hasdemonstratedthe
potentialofusingtheinternettoautomaticallygathertargetedvisualdataforspecifictask;we
employ similar ideas to address the scarcity, introducing a data engine to create large-scale,
reliable,high-qualityknowledge-basedinstructiontuningdata(seeFig.17). Theengineselects
a target field and subfield, such as “Physics”, and uses an LLM like GPT-4 [95] to identify
topics(e.g.,“Newton’sLaws”). ItthensearchesreliablesourceslikeWikipediaforeachtopic.
We find that image-text pairs extracted from Wikipedia pages are of high-quality. A parser
extracts image-caption tuples and feeds the caption text to an LLM, such as GPT-3.5 [94], to
generateinstruction-typeQ&Apairsabouttheimageusinganengineeredprompt. TheseQ&A
pairs and the image form our VQA dataset. Details are in Appendix E.3. Our data engine
producesalargevolumeofreliablescientificdata,increasingthediversityinthedatapool. We
generate161kscience-relateddatapoints—400%morethanthepreviouscombineddatasources.
15Cumulative Sum of Counts for Entries
10m
Data Mix 1 (unfiltered)
Data Mix 2 (t=450k)
Data Mix 3 (t=350k)
8m Data Mix 4 (t=250k)
Data Mix 5 (t=150k)
6m
4m
2m
0m
0 10 20 30 40 50 60 70
Sorted Entries
Figure10|DataBalancingviaApplyingThresholdsonDataSources. Applyingthreshold𝑡alleviates
theexponentialtailofCambrian-10M.
Average General Knowledge OCR&Chart Vision-Centric
150k 53.7 68.0 51.3 45.2 50.5
250k 54.3 68.1 51.5 45.3 52.2
350k 54.3 67.4 51.4 46.0 52.3
450k 54.2 68.0 52.2 45.5 50.7
Table6|Threshold𝑡valuebetween250kand350kobtainsbetterperformance. Weobservean“elbow”
effectinthedatabalancingexperiment. Athreshold𝑡between250kand350kworksbest.
Cambrian-10M We create a large pool of instruction tuning data, which we refer to as
Cambrian-10M.Thispoolcontainsapproximately9784kdatapoints,offeringadiverserangeof
dataforourworkandfutureresearch. WevisualizeitscompositioninFig.9.
5.2. DataCuration
Cambrian-10Misalargepoolofinstructiontuningdatasourcedfromavarietyofdatasources,
withanunbalanceddataratiobetweencategories. Here,wetakeapreliminarysteptostudy
datacurationbyimprovingdatabalancingandadjustingdataratios.
DataBalancing Wefollowpreviouswork[102,129]tosetthresholds𝑡 forthenumberofdata
pointsfromasingledatasource. Tostudytheeffectofthenumber𝑡,weplotthecumulative
sum of counts for entries sorted by counts from tail to head (see Fig. 10). We choose 𝑡 =
150𝑘, 250𝑘, 350𝑘,and450𝑘inthissectionandobserveanelboweffectinTable6—findingthata
thresholdbetween250𝑘and350𝑘workthebestforCambrian-10M.
Data Ratio Unlike previous work in VLM data curation [36, 129] which curate noisy raw
image-text pairs by scraping the internet, Cambrian-10M is designed for visual instruction
tuning. Giventhevariouscapabilitiesofdifferenttypesofdata, itisessentialtobalancethe
ratio of these data types. We conduct pilot experiments with a fixed dataset size of 1350k,
examiningtheimpactofdifferentdataratiosondownstreamperformance. Wevisualizethe
results in Fig. 11 and summarize our findings as follows: (i) Balancing General, OCR and
Languagedataiscrucial. Themodel’sOCRcapabilityisproportionaltotheOCRdataratio;
however,anexcessiveOCRratiocompromisesgeneralVQAandvision-centricperformance.
(ii)Performanceonknowledge-intensivetasksisinfluencedbymultiplefactors,oftenrequiring
amixofOCR,chart,reasoning,andgeneralperception. Increasingthesciencedataratiocan
help,butaverylowratioleadstopoorperformance.
16
)m(
muS
evitalumuC100 5.00% 47.8
10.00% Language
20.00% 20.00% 21.00%
47.6
General
80
35.00%
30.00% 47.4
20.00% OCR
25.00%
34.52% 47.2 60
Counting
47.0
25.00% 30.00% Math
40 39.10% 39.10% 46.8
27.22% Code
5.00% 5.00% 46.6
5.00% 5.00%
20 2.00% 2.00%
5.00% 5.00% Science
10.00% 10.00% 8.71% 46.4
0
44 .. 54 00 %% 15.00% 15.00% 44 .. 54 00 %% 7.2 00..880 87%%% 46.2 Average
exp-1 exp-2 exp-3 exp-4 exp-5
Figure11|Exploringinstructiontuningdatamixtureratios. Weexploretheimpactofdifferentratios
ontheoverallperformanceofthemodelwithacontrolleddatasizeof1.35M.Wefindthatdifferentratios
haveanon-trivialimpactontheoverallperformance,andexp-5isthemostoptimal.
Average General Knowledge OCR&Chart Vision-Centric
LLaVA-665K 40.7 64.7 45.2 20.8 32.0
Cambrian-10M 54.8 68.7 51.6 47.3 51.4
Cambrian-7M 55.9 69.6 52.6 47.3 54.1
Table7|Performanceimproveswithbetterinstructiontuningdatacuration.Themodelgainssignificant
improvements when scaling up to Cambrian-10M. With data curation, the model further improves
performanceacrossallcategorieswhileenjoyingmoreefficienttraining.
Cambrian-7M ByapplyingdatafilteringtoCambrian-10Mwithouridentifieddataratio,we
createasmallerbuthigher-qualitydatasetcalledCambrian-7M.Table7showcasesthebenefits
ofawell-balancedandcarefullycurateddataset. Despitehavingfewersamples,Cambrian-7M
demonstratesimprovedperformance.
5.3. Alleviatingthe“AnswerMachinePhenomenon”viaSystemPrompts
Here,weinvestigateaphenomenonwetermthe“answermachinephenomenon”. Weobserve
thatawell-trainedMLLMmayexcelatVQAbenchmarks,butlackbasicconversationalabilities
anddefaulttooutputtingshort,curtresponses(seeexamplesinFig.12). Thisdiscrepancyarises
becausebenchmarkquestionstypicallyrequireresponsesthat arelimitedtoasingleoption,
choice, or word—diverging from the more broad and realistic use cases of MLLMs. Similar
phenomenahavebeendiscussedinotherLLMstudies[106,142,146].
Wesuspectthatthisissuestemsfrominstructiontuningdatacontaininganexcessivenumber
ofshort-responseVQAtasks,leadingtocatastrophicforgettinginLLMs. Toaddressthis,we
incorporateadditionalsystempromptsduringtraining. Weappendpromptssuchas“Answer
thequestionusingasinglewordorphrase.” beforequestionsthatgenerateasinglewordorphrase
intheresponse. FulldetailsofthesystempromptsusedareprovidedinAppendixE.2. After
integratingthesesystemprompts,weobservethatwhilethemodel’sbenchmarkperformance
remainsunchanged,itsconversationalabilityimprovesdramatically. Forexample,inFig.12,
modelswithsystempromptsproducelongerandmoreengagingresponseswhileanswering
questionscorrectly. Thesystempromptsalsoenhancethemodel’sperformanceonreasoning-
relatedtasks,suchasmathproblems,byencouragingachainofthoughts[124]followedbythe
answer.
17
egatnecreP erocS
egarevAw/o system prompt w/ system prompt
How many people are in this image? Based on this chart, what is the most popular tea?
There are six people in the image. Black Tea
There are six people in the image. Based on the chart, black tea appears to be the most popular
type of tea in the U.S. Bubble Tea Market, as it consistently
has the highest sales volume across all years displayed.
Describethisimageplease. What city is this? If I want to tour around the landmark in this picture, where should I go?
Cat in bathtub New York City
The image captures a serene moment in a bathroom This is New York City, and the prominent landmark in the image
setting. A fluffy, gray cat is comfortably seated is the Washington Square Arch, which is located in the Greenwich
on the edge of a white bathtub. The cat's gaze is Village neighborhood. To tour around this landmark, you should
directed upwards, possibly at something outside the visit the nearby Washington Square Park, which is a popular spot
frame, giving it a contemplative expression. The for locals and tourists alike. The park offers a variety of
backdrop is a simple white tiled wall, which activities, including walking paths, a fountain, and a statue of
contrasts with the cat's fur, making it the focal George Washington. Additionally, the surrounding area is rich
point of the image. The overall scene suggests a with historical buildings, art galleries, and restaurants,
quiet, peaceful moment in a domestic setting. making it a vibrant and culturally diverse place to explore.
Please solve this question step by step. What is unusual about this image? Please answer in details.
Answer is C. The image is unusual because it features a church with a design
that incorporates elements typically associated with a face. The
two circular windows on the tower serve as eyes, and the cross
on top of the tower is positioned to resemble a nose. This
To solve the equation (\sqrt{k+2} -x = 0), we need anthropomorphic design is not commonly found in church
to isolate (k). Let's start by adding (x) to both architecture, which often favors more traditional and
sides of the equation to get rid of the subtraction: conventional styles. The presence of the palm trees adds a
(\sqrt{k+2} = x) tropical or subtropical feel to the scene, which may not be the
Now, to isolate (k), we square both sides of the usual setting for such a building. The overcast sky suggests a
equation: gloomy or somber mood, which contrasts with the whimsical nature
((\sqrt{k+2})^2 = x^2) of the church's design.
This simplifies to:
(k+2 = x^2)
Now, we substitute (x = 9) into the equation: The unusual aspect of this image is the anthropomorphic design of
(k+2 = 9^2) the church's bell tower, which is given human-like features.
(k+2 = 81) Specifically, the two windows on the tower are designed to
Finally, we subtract 2 from both sides to solve for resemble eyes, and the cross at the top of the tower is stylized
(k): to look like a nose. This creative design choice is not typical
(k = 81 -2) for church architecture, which usually favors more traditional
(k = 79) and less whimsical features. The intention behind such a design
Therefore, the value of (k) when (x = 9) is 79. The could be to add a playful or artistic element to the building,
correct answer to the question is D. possibly to attract attention or to reflect a unique character of
the church or the community it serves.
Figure12|IncorporatingSystemPromptinInstructionTuningDataalleviatesthe“AnswerMachine
Phenomenon”ByaddingsystempromptsinCambrian-7M,themodelexhibitsbetterchatabilitywhile
retainingstrongquestionansweringabilities. Themodelwithoutsystempromptsrequiresadditional
promptingtoelicitlongerresponses.
ThisunderscoresthenecessityofdevelopingevaluationprotocolsliketheChatbotArena[29]
forMLLMs,despitethechallengesincollectinglarge-scale,real-worldinteractiondata. While
performing well on benchmarks is important, it is equally crucial to ensure the model can
engageinmeaningfulandnaturalinteractions. Theoveralluserexperienceandthemodel’s
conversationalabilitiesareparamount,asamodelthatexcelsinbenchmarksbutfailstoconverse
effectivelycannotmeettheneedsofpracticalapplications.
6. State of the Art Performance
Finally,weleveragetheinsightsfromallourpreviousstudiestotrainafamilyofMLLMswecall
Cambrian-1. WetrainmodelsusingLLMbackbonesofvariousscales: LLaMA-3-Instruct-8B[3],
Vicuna-1.5-13B[142],andHermes-2-Yi-34B[130]. Ourvisioncomponentcombinesfourmodels—
OpenAICLIPViT-L/14@336,SigLIPViT-SO400M/14@384,OpenCLIPConvNeXt-XXL@1024,
andDINOv2ViT-L/14@518(Section3.5)—viatheSpatialVisionAggregator(Section4). We
pre-traintheconnectorusing2.5MadapterdataandinstructiontuneusingourCambrian-7M
datamix(Section5.2). OurmodelsareevaluatedonthebenchmarkscategorizedinSection3.1,
withresultspresentedinTable8andFig.136.
Cambrian-1surpassesopen-sourcemodelslikeLLaVA-NeXTandMini-Gemini. Thanksto
theSVA,Cambrian-1excelsintasksrequiringhigh-resolutionimageprocessing,evenwithonly
576imagetokens—about1/5ofthetokensusedbyLLaVA-NeXTandMini-Gemini. Cambrian-1
alsoachievescomparableperformancetothebestproprietarymodels,suchasGPT-4V,Gemini-
6FortheGeneralAverage,wenotethatGPT-4’sperformanceontheGQAtestsetislow,possiblybecauseother
modelsaretrainedontheGQAtrainingset,whereasthetrainingsetusedforGPT-4isunclear.
18Model General Knowledge OCR&Chart Vision-Centric
Method
GPT-4V UNK. 63.0 1409.4 75.8 69.1 36.8 65.2 75.7 56.8 49.9 78.2 77.4 78.5 64.5 78.0 88.4 62.4 50.0 61.4 64.3 73.8
Gemini-1.0Pro UNK. - 1496.6 73.6 70.7 - - 79.5 47.9 45.2 - - - 65.9 - - - - - - -
Gemini-1.5Pro UNK. - - - - - - - 58.5 52.1 80.3 - 81.3 - 73.5 86.5 - - 67.5 - -
Grok-1.5 UNK. - - - - - - - 53.6 52.8 88.3 - 76.1 - 78.1 85.6 - - 68.7 - -
MM-1-8B 144 - 1529.3 72.3 69.9 - - 72.6 37.0 35.9 - - - - - - - - - - -
MM-1-30B 144 - 1637.6 75.1 72.1 - - 81.0 44.7 39.4 - - - - - - - - - - -
BaseLLM:Llama-3-Ins-8B
Mini-Gemini-HD-8B 2880 72.7 1606.0 72.7 73.2 64.5 55.7 75.1 37.3 37.0 73.5 62.9 59.1 47.7 70.2 74.6 51.5 18.7 62.1 62.2 63.0
LLaVA-NeXT-8B 2880 72.5 1603.7 72.1 72.7 65.2 55.6 72.8 41.7 36.3 71.6 63.9 69.5 49.0 64.6 72.6 56.6 38.7 60.1 62.2 65.3
Cambrian-1-8B 576 73.1 1,547.1 75.9 74.7 64.6 61.3 80.4 42.7 49.0 73.0 71.3 73.3 62.4 71.7 77.8 65.0 51.3 64.2 72.3 72.0
BaseLLM:Vicuna-1.5-13B
Mini-Gemini-HD-13B 2880 70.7 1597.0 68.6 70.6 63.7 54.1 71.9 37.3 37.0 70.1 60.8 56.6 46.6 70.2 69.8 49.4 19.3 57.5 53.6 67.3
LLaVA-NeXT-13B 2880 69.9 1575.0 70.0 65.6 65.4 53.7 73.5 36.2 35.1 70.0 62.9 62.2 51.4 67.1 70.9 55.9 36.0 59.1 62.7 65.7
Cambrian-1-13B 576 73.7 1,610.4 75.7 74.4 64.3 60.2 79.3 40.0 48.0 73.6 71.3 73.8 61.9 72.8 76.8 62.2 41.3 63.0 72.5 71.8
BaseLLM:Hermes2-Yi-34B
Mini-Gemini-HD-34B 2880 76.2 1659.0 80.6 75.3 65.8 62.4 77.7 48.0 43.4 80.5 68.1 67.6 51.8 74.1 78.9 63.8 37.3 67.2 71.5 79.2
LLaVA-NeXT-34B 2880 76.0 1633.2 79.3 75.9 67.1 62.5 81.8 46.7 46.5 74.9 67.7 68.7 54.5 69.5 78.1 64.0 47.3 61.0 73.0 74.8
Cambrian-1-34B 576 76.8 1689.3 81.4 75.3 65.8 67.0 85.6 49.7 53.2 79.7 71.9 75.6 60.0 76.7 75.5 68.5 52.7 67.8 74.0 79.7
Table8|ComparisonofCambrian-1withotherleadingMLLMframework. Cambrian-1outperforms
otheropen-sourcemodelsandachievescompetitiveperformanceonanumberofbenchmarks,compared
to proprietary models such as GPT-4V, Gemini, and Grok-1.5. Despite using only 576 visual tokens,
Cambrian-1performsbetteronOCR&ChartandVision-CentricbenchmarkscomparedtoMini-Gemini-
HDandLLaVA-NeXT,whichuse2880tokens.
 	                      ȱ ǭ ȱ             Ȭ       
 Ş Ŗ ǯ Ŗ  ŝ Ŗ  Ş Ŗ
 ŝ Ŗ
 ŝ ŝ ǯ ś
 Ŝ ś  ŝ ś
 ŝ ś ǯ Ŗ  Ŝ ś
 ŝ Ř ǯ ś  Ŝ Ŗ  ŝ Ŗ
 Ŝ Ŗ
 ŝ Ŗ ǯ Ŗ
 Ŝ ŝ ǯ ś  ś ś  Ŝ ś  ś ś
 Ŝ ś ǯ Ŗ
 ś Ŗ  Ŝ Ŗ  ś Ŗ
 Ŝ Ř ǯ ś
 Ŝ Ŗ ǯ Ŗ  Ś ś  ś ś  Ś ś
 Ş   ŗ ř   ř Ś   Ş   ŗ ř   ř Ś   Ş   ŗ ř   ř Ś   Ş   ŗ ř   ř Ś 
     Ȭ 	      Ȭ 
        Ȭ              Ȭ ŗ  	   Ȭ Ś  ȱ           
Figure13|Comparisonofmodelaverageperformancesoneachcategory. Cambrian-1outperforms
otheropen-sourcemodelsacrossallsizes.TheleadisespeciallylargeonOCR&ChartandVision-Centric
benchmarks,highlightingtheadvantageofourvision-centricdesign.
Pro,andMM-1,onseveralbenchmarks. WeshowcasesomeexamplesinFig.14,demonstrating
thatthemodeleffectivelyattendstodetailsinimagesdespiteusingonly576tokens.
Additionally, we emphasize the importance of post-processing a model’s output and as-
sessingitsaccuracy. Forinstance,ifthecorrectansweris“(a) Apple”andthemodeloutputs
“Apple”,itiscrucialtorecognizetheresponseascorrect. Weusefuzzymatchingtoevaluate
theaccuracyofourmodel’soutputsandconductanablationstudywithLLMslikeGPT-3.5to
validatethismethod. Ourfindingsindicatethatfuzzymatchingprovidesreliablejudgments.
FurtherdetailscanbefoundinAppendixG.2.
7. Discussion
WeadvocateforusingMLLMsasaninterfacetoevaluatevisualrepresentations,asprevious
benchmarks are becoming saturated and do not adequately reflect the diverse and complex
perceptionchallengesoftherealworld. Ourworkhighlightsthecurrentgapbetweenlanguage-
supervised models and self-supervised learning models and demonstrates the potential of
19
.koTsiV#
gvA
PEMM BMM IDEES AQG
gvA
IAQS
VUMMM
MatsiVhtaM
D2IA
gvA
AQtrahC
hcneBRCO
AQVtxeT AQVcoD
gvA
PVMM
AQdlrowlaeR
D2hcneB-VC D3hcneB-VCWhy is this image funny? What does this image remind you of?
The humor in this image comes from the The image might remind you of a playful
unexpected twist on the classic poem and creative culinary presentation,
'Roses are red, violets are blue.' Instead possibly inspired by a world map, where
of a romantic or sweet sentiment, the poem each piece of food represents a different
is humorously altered to a more mundane continent. It could also evoke thoughts of
and slightly unsettling scenario, which is a fun cooking challenge or a themed meal
a bug in the bed. This juxtaposition of a where the food is arranged to mimic the
traditional romantic poem with a common, shape of the Earth.
albeit slightly unpleasant, household
occurrence creates a comedic effect.
Which character is this?
The character in the image is Homer Simpson,
a central character from the animated Please solve the problem step by step:
television series 'The Simpsons.' Homer is
known for his distinctive yellow skin, To solve this problem, we need to
casual attire, and often humorous or determine which option correctly completes
oblivious behavior. the sentence in a way that adheres to the
conventions of Standard English. The
sentence is:
"In 1937, Chinese American screen actor
Anna May Wong, who had portrayed numerous
List all the countries in this image in v ni el vl ea ri n as ha en rd o is ne ec ,o n fd ia nr ay l lc yh a gr oa tc t ae r ss t ab ru rt i ng
the format of a json {1: 2:, etc}. Why role in Paramount Pictures' Daughter of
are the flags here? Shanghai, a film that _____ "expanded the
range of possibilities for Asian images on
{ screen."
"1": "Austria", The correct option should be the one that
"2": "Belgium", maintains the subject-verb agreement and
"3": "France", the appropriate punctuation.
"4": "Germany", Option A: "critic, Stina Chyn, claims" -
"5": "Portugal", This option is incorrect because it uses a
"6": "Scotland", comma after the name, which is not
"7": "Spain", standard in English.
"8": "Turkey" Option B: "critic, Stina Chyn, claims," -
} This option is also incorrect because it
The flags are here to represent the uses a comma after the name and an extra
participating countries in the UEFA EURO comma at the end of the sentence, which is
2024 tournament, which is hosted by not standard in English.
Germany. Each flag corresponds to a Option C: "critic Stina Chyn claims" -
country that is either a host or a This option is correct because it
participant in the event, as indicated by maintains the subject-verb agreement and
the tickets below the flags. The tickets does not use any unnecessary commas.
suggest that these countries are either Option D: "critic Stina Chyn, claims," -
hosting matches or have qualified to play This option is incorrect because it uses a
in the tournament. comma after the name and an extra comma at
the end of the sentence, which is not
standard in English.
Therefore, the correct answer is Option C:
"critic Stina Chyn claims."
Figure14|ExamplesofCambrian-1-34B.Cambrian-1showcasesimpressiveabilitiesinvisualintersec-
tion. Themodeldemonstratesinstruction-followingabilitysuchasoutputinjsonformat,asillustrated
inthebottom-leftexample. Cambrian-1alsodemonstratesremarkableOCRability(Seemodelhandles
differentComma“,”intherightdownexample).
bridgingthisgap. However,itisknownthatfeaturesoflanguage-supervisedmodelsbehave
likeabag-of-words[116,135],underscoringtheneedforadvancementsinvision-onlymodels
to ensure better visual understanding. We hope to inspire future research into developing
bettervision-onlymodelsintendedtobeadaptedintotheMLLMsetting,thatmoreeffectively
leveragelarge-scaledatasets[78]andpreservetheadvantagesinvisualgrounding[117].
AsweobserveinTable8,awell-trainedopen-sourcemodelsuchasCambrian-1canmatch
orevenoutperformproprietarymodelsonmanyexistingbenchmarks. However,theuseand
evaluationofMLLMsextendfarbeyondthecurrentscopeofbenchmarks—toconversational
ability,creativity,reliability,andoveralluserexperience. Developingmodelssolelybasedon
benchmark results can result in an “answer machine”, over-optimized for benchmarks but
lackinginpracticalinteractioncapabilities. Therefore,thedevelopmentofMLLMsthatbetter
align with human and societal needs is a continuously evolving process, both in terms of
evaluationandmodeldevelopment.
OurcurrentCambrian-1modelusesamoderatenumberofvisualtokensanddoesnotadopt
the any-resolution strategy [25, 70, 74] to handle ultra high-resolution images or those with
extremeaspectratios,whichrequirealargernumberofvisualtokens. Forspecializedtaskslike
V*Bench[127],whichrequireprocessingultrahigh-resolutionimages,increasingtheresolution
andnumberofvisualtokenscouldleadtoanHDversionoftheCambrian-1model.
One promising direction for post-training alignment is through reinforcement learning
rather than supervised fine-tuning. Many MLLM studies, including Cambrian, primarily
focus on supervised fine-tuning. Yet, recent advancements in LLMs [32, 97, 103, 147] and
20someinMLLMs[132,137]suggestthatreinforcementlearningfromhumanorenvironmental
feedback can further improve models, potentially surpassing the limits of supervised fine-
tuning,especiallyindecision-makingabilities.
Cambrian-10M(Fig.9andSection5)providesarichpoolofdataforstudyingdatacuration
in fine-tuning MLLMs. Our work takes an initial step in curating higher-quality data to
enable more efficient and effective instruction tuning. We believe there is room for further
improvementinthedatacurationpipeline,andwehopethisworkcanserveasafoundation
forfutureresearch.
Additionally,traininglarge-scalemodelsrequirescarefuldesignofmodelsharding,data
sharding, and infrastructure adaptations. In this work, we train our model on TPU-V4 [53]
withFSDP[141]usingTorchXLA.Weshareourexperiences,technicalchallenges,andsolutions
in Appendix A. We also open-source our implementation and provide tutorials to help the
communityundertakelarge-scaletrainingmoreefficiently.
Toconclude,Cambrian-1introducesafamilyofstate-of-the-artMLLMmodelsthatachieve
top performance across diverse benchmarks and excel in visual-centric tasks. We provide
modelweights,open-sourcecode,datasets,anddetailedrecipesformodeltrainingandevalua-
tion. Wehopeourworkwillstrengthentheopenresearchcommunityandacceleratefuture
advancementsinbothvisualrepresentationlearningandmultimodalsystems.
Acknowledgements
WearegratefultoLLaVA[75]fortheirexcellentcodebase,whichservedasthelaunchingpoint
forourresearch. SpecialthankstoHexuZhaoforextensivediscussionsandknowledge-sharing
around FSDP and large-scale training techniques, and to Jiasen Lu for helpful discussions
on TPU and JAX distributed training infrastructure. We also appreciate the assistance and
responsesfromthePyTorchXLAteamviaGitHub.
WearethankfultoKaimingHeforearlydiscussionsonmulti-modallargelanguagemodels.
WealsothankZhuangLiu,JunlinHan,YuexiangZhai,TianzheChu,DaohanLu,WeiyangJin,
BoyangZhang,andJiayiPanforreviewingthismanuscript. WealsoacknowledgeDeepSeek[80]
forthepapertemplateinspiration.
ThisworkwasprimarilysupportedbytheGoogleTPUResearchCloud(TRC)programand
theGoogleCloudResearchCreditsprogram(GCP19980904). Additionalsupportwasprovided
by the NYU IT High Performance Computing resources, services, and staff expertise. S.X.
wouldliketothanktheOpenAIResearcherAccessprogram,OpenPathAIFoundation,andan
AmazonResearchawardfortheirsupport. S.T.issupportedbytheOpenAISuperAlignment
Fellowship,andE.B.issupportedbytheNDSEGFellowship.
21References
[1] M.Acharya,K.Kafle,andC.Kanan.“TallyQA:Answeringcomplexcountingquestions”.
In:AAAI.2019.
[2] A.Agrawaletal.“Don’tjustassume;lookandanswer:Overcomingpriorsforvisual
questionanswering”.In:CVPR.2018.
[3] AI@Meta.“Llama3ModelCard”.In:(2024).
[4] H.A.Alawwadetal.“EnhancingTextbookQuestionAnsweringTaskwithLargeLan-
guageModelsandRetrievalAugmentedGeneration”.In:arXivpreprintarXiv:2402.05128
(2024).
[5] J.-B. Alayrac et al. “Flamingo: a visual language model for few-shot learning”. In:
NeurIPS.2022.
[6] T.Aquinas.QuaestionesDisputataedeVeritate.q.2a.3arg.19,1259.
[7] Aristotle.Metaphysics.Ed.byT.byW.D.Ross.TheInternetClassicsArchive,350BCE.
[8] M.Assranetal.“Self-supervisedlearningfromimageswithajoint-embeddingpredictive
architecture”.In:CVPR.2023.
[9] J.Baietal.“QwenTechnicalReport”.In:arXivpreprintarXiv:2309.16609(2023).
[10] J.Baietal.“Qwen-vl:Aversatilevision-languagemodelforunderstanding,localization,
textreading,andbeyond”.In:(2023).
[11] M.E.Bananietal.“Probingthe3DAwarenessofVisualFoundationModels”.In:arXiv
preprintarXiv:2404.08636(2024).
[12] J.Belouadi,A.Lauscher,andS.Eger.“Automatikz:Text-guidedsynthesisofscientific
vectorgraphicswithtikz”.In:ICLR.2024.
[13] R.Birkl,D.Wofk,andM.Müller.“Midasv3.1–amodelzooforrobustmonocularrelative
depthestimation”.In:arXivpreprintarXiv:2307.14460(2023).
[14] A.F.Bitenetal.“Latr:Layout-awaretransformerforscene-textvqa”.In:CVPR.2022.
[15] A.F.Bitenetal.“Scenetextvisualquestionanswering”.In:ICCV.2019.
[16] G.Braziletal.“Omni3d:Alargebenchmarkandmodelfor3dobjectdetectioninthe
wild”.In:CVPR.2023.
[17] S.Chaetal.“VisuallyDehallucinativeInstructionGeneration:KnowWhatYouDon’t
Know”.In:arXivpreprintarXiv:2402.09717(2024).
[18] D. J. Chalmers. “Does Thought Require Sensory Grounding? From Pure Thinkers to
Large Language Models”. In: Proceedings and Addresses of the American Philosophical
Association97(2023),pp.22–45.
[19] Y.Changetal.“Asurveyonevaluationoflargelanguagemodels”.In:ACMTransactions
onIntelligentSystemsandTechnology15.3(2024),pp.1–45.
[20] G. H. Chen et al. “ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-
LanguageModel”.In:arXivpreprintarXiv:2402.11684(2024).
[21] L.Chenetal.“AreWeontheRightWayforEvaluatingLargeVision-LanguageModels?”
In:arXivpreprintarXiv:2403.20330(2024).
[22] L.Chenetal.“Sharegpt4v:Improvinglargemulti-modalmodelswithbettercaptions”.
In:arXivpreprintarXiv:2311.12793(2023).
[23] X.Chenetal.“Pali:Ajointly-scaledmultilinguallanguage-imagemodel”.In:ICLR.2023.
[24] X. Chen, S. Xie, and K. He. “An empirical study of training self-supervised vision
transformers”.In:ICCV.2021.
22[25] Z. Chen et al. “How far are we to gpt-4v? closing the gap to commercial multimodal
modelswithopen-sourcesuites”.In:arXivpreprintarXiv:2404.16821(2024).
[26] Z.Chenetal.“Finqa:Adatasetofnumericalreasoningoverfinancialdata”.In:EMNLP.
2021.
[27] Z.Chengetal.“HiTab:Ahierarchicaltabledatasetforquestionansweringandnatural
languagegeneration”.In:ACL.2022.
[28] M.Chertietal.“Reproduciblescalinglawsforcontrastivelanguage-imagelearning”.In:
CVPR.2023.
[29] W.-L. Chiang et al. “Chatbot arena: An open platform for evaluating llms by human
preference”.In:arXivpreprintarXiv:2403.04132(2024).
[30] M.Conoveretal.FreeDolly:IntroducingtheWorld’sFirstTrulyOpenInstruction-TunedLLM.
2023. URL: https://www.databricks.com/blog/2023/04/12/dolly-first-
open-commercially-viable-instruction-tuned-llm(visitedon06/30/2023).
[31] W. Dai et al. “Instructblip: Towards general-purpose vision-language models with
instructiontuning”.In:NeurIPS.2024.
[32] H.Dongetal.“Rlhfworkflow:Fromrewardmodelingtoonlinerlhf”.In:arXivpreprint
arXiv:2405.07863(2024).
[33] A.Dosovitskiyetal.“Animageisworth16x16words:Transformersforimagerecogni-
tionatscale”.In:ICLR.2021.
[34] A.Fangetal.“Datafilteringnetworks”.In:ICLR.2024.
[35] X.Fuetal.“BLINK:MultimodalLargeLanguageModelsCanSeebutNotPerceive”.In:
arXivpreprintarXiv:2404.12390(2024).
[36] S.Y.Gadreetal.“Datacomp:Insearchofthenextgenerationofmultimodaldatasets”.
In:vol.36.2024.
[37] J. Gao et al. “G-llava: Solving geometric problem with multi-modal large language
model”.In:arXivpreprintarXiv:2312.11370(2023).
[38] P.Gaoetal.“SPHINX-X:ScalingDataandParametersforaFamilyofMulti-modalLarge
LanguageModels”.In:arXivpreprintarXiv:2402.05935(2024).
[39] Y. Ge et al. “Planting a seed of vision in large language model”. In: arXiv preprint
arXiv:2307.08041(2023).
[40] R.Geirhosetal.“Shortcutlearningindeepneuralnetworks”.In:NatureMachineIntelli-
gence(2020).
[41] R. Girshick et al. “Rich feature hierarchies for accurate object detection and semantic
segmentation”.In:CVPR.2014.
[42] Google.Gemini.2023.
[43] Y.Goyaletal.“Makingthevinvqamatter:Elevatingtheroleofimageunderstanding
invisualquestionanswering”.In:CVPR.2017.
[44] D.Gurarietal.“Vizwizgrandchallenge:Answeringvisualquestionsfromblindpeople”.
In:CVPR.2018.
[45] K.Heetal.“Maskedautoencodersarescalablevisionlearners”.In:CVPR.2022.
[46] X.Heetal.“PathVQA:30000+QuestionsforMedicalVisualQuestionAnswering”.In:
CoRRabs/2003.10286(2020).
[47] T. Hiippala et al. “AI2D-RST: A multimodal corpus of 1000 primary school science
diagrams”.In:LanguageResourcesandEvaluation55(2021),pp.661–688.
23[48] J. Hoffmann et al. “Training compute-optimal large language models”. In: NeurIPS
(2023).
[49] Y.-C. Hsiao, F. Zubach, M. Wang, et al. “Screenqa: Large-scale question-answer pairs
overmobileappscreenshots”.In:arXivpreprintarXiv:2209.08199(2022).
[50] D.A.HudsonandC.D.Manning.“GQA:ANewDatasetforReal-WorldVisualReason-
ingandCompositionalQuestionAnswering”.In:CVPR.2019.
[51] A.Jaegleetal.“Perceiver:Generalperceptionwithiterativeattention”.In:ICML.2021.
[52] J.Johnsonetal.“Clevr:Adiagnosticdatasetforcompositionallanguageandelementary
visualreasoning”.In:CVPR.2017.
[53] N.Jouppietal.“Tpuv4:Anopticallyreconfigurablesupercomputerformachinelearning
withhardwaresupportforembeddings”.In:Proceedingsofthe50thAnnualInternational
SymposiumonComputerArchitecture.2023.
[54] K.Kafleetal.“Dvqa:Understandingdatavisualizationsviaquestionanswering”.In:
CVPR.2018.
[55] S.Kantharajetal.“Chart-to-text:Alarge-scalebenchmarkforchartsummarization”.In:
ACL.2022.
[56] S.Karamchetietal.“Prismaticvlms:Investigatingthedesignspaceofvisually-conditioned
languagemodels”.In:arXivpreprintarXiv:2402.07865(2024).
[57] M.Kazemietal.“Geomverse:Asystematicevaluationoflargemodelsforgeometric
reasoning”.In:2023.
[58] A.Kembhavietal.“Adiagramisworthadozenimages”.In:ECCV.2016.
[59] D. Kiela et al. “The hateful memes challenge: Detecting hate speech in multimodal
memes”.In:NeurIPS.2020.
[60] G. Kim et al. “Donut: Document understanding transformer without ocr”. In: ECCV.
2022.
[61] A.Kirillovetal.“Segmentanything”.In:ICCV.2023.
[62] R.Krishnaetal.“VisualGenome:ConnectingLanguageandVisionUsingCrowdsourced
DenseImageAnnotations”.In:IJCV (2016).
[63] LAION.laion/gpt4v-dataset.2023.
[64] H.Laurençon,L.Tronchon,andV.Sanh.“UnlockingtheconversionofWebScreenshots
intoHTMLCodewiththeWebSightDataset”.In:arXivpreprintarXiv:2403.09029(2024).
[65] H.Laurençonetal.“Whatmatterswhenbuildingvision-languagemodels?”In:arXiv
preprintarXiv:2405.02246(2024).
[66] A.C.Lietal.“InternetExplorer:TargetedRepresentationLearningontheOpenWeb”.
In:ICML.2023.
[67] A.C.Lietal.“Yourdiffusionmodelissecretlyazero-shotclassifier”.In:ICCV.2023.
[68] B. Li et al. LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild.
2024.
[69] L.Lietal.“MultimodalArXiv:ADatasetforImprovingScientificComprehensionof
LargeVision-LanguageModels”.In:arXivpreprintarXiv:2403.00231(2024).
[70] Y.Lietal.“Mini-gemini:Miningthepotentialofmulti-modalityvisionlanguagemodels”.
In:arXivpreprintarXiv:2403.18814(2024).
[71] W. Lian et al. OpenOrca: An Open Dataset of GPT Augmented FLAN Reasoning Traces.
https://https://huggingface.co/Open-Orca/OpenOrca.2023.
24[72] T.-Y.Linetal.“Microsoftcoco:Commonobjectsincontext”.In:ECCV.2014.
[73] H. Liu et al. “Improved baselines with visual instruction tuning”. In: arXiv preprint
arXiv:2310.03744(2023).
[74] H.Liuetal.LLaVA-NeXT:Improvedreasoning,OCR,andworldknowledge.2024.
[75] H.Liuetal.“VisualInstructionTuning”.In:NeurIPS.2023.
[76] Y. Liu et al. “Mmbench: Is your multi-modal model an all-around player?” In: arXiv
preprintarXiv:2307.06281(2023).
[77] Y.Liuetal.“Onthehiddenmysteryofocrinlargemultimodalmodels”.In:arXivpreprint
arXiv:2305.07895(2023).
[78] Z. Liu and K. He. “A Decade’s Battle on Dataset Bias: Are We There Yet?” In: arXiv
preprintarXiv:2403.08632(2024).
[79] Z.Liuetal.“Aconvnetforthe2020s”.In:CVPR.2022.
[80] H. Lu et al. “DeepSeek-VL: towards real-world vision-language understanding”. In:
arXivpreprintarXiv:2403.05525(2024).
[81] P.Luetal.“Dynamicpromptlearningviapolicygradientforsemi-structuredmathe-
maticalreasoning”.In:ICLR.2023.
[82] P.Luetal.“Iconqa:Anewbenchmarkforabstractdiagramunderstandingandvisual
languagereasoning”.In:NeurIPS.2021.
[83] P.Luetal.“Inter-GPS:Interpretablegeometryproblemsolvingwithformallanguage
andsymbolicreasoning”.In:ACL.2021.
[84] P. Lu et al. “Learn to explain: Multimodal reasoning via thought chains for science
questionanswering”.In:NeurIPS.2022.
[85] P. Lu et al. “Mathvista: Evaluating mathematical reasoning of foundation models in
visualcontexts”.In:ICLR(2023).
[86] Z.Luoetal.“Wizardcoder:Empoweringcodelargelanguagemodelswithevol-instruct”.
In:ICLR.2024.
[87] A.Majumdaretal.“OpenEQA:EmbodiedQuestionAnsweringintheEraofFoundation
Models”.In:2ndWorkshoponMobileManipulationandEmbodiedIntelligenceatICRA2024.
2024.
[88] K.Marinoetal.“OK-VQA:AVisualQuestionAnsweringBenchmarkRequiringExternal
Knowledge”.In:CVPR.2019.
[89] A.Masryetal.“Chartqa:Abenchmarkforquestionansweringaboutchartswithvisual
andlogicalreasoning”.In:ACL.2022.
[90] M. Mathew, D. Karatzas, and C. Jawahar. “Docvqa: A dataset for vqa on document
images”.In:WACV.2021.
[91] B. McKinzie et al. “Mm1: Methods, analysis & insights from multimodal llm pre-
training”.In:arXivpreprintarXiv:2403.09611(2024).
[92] A.Mitraetal.Orca-Math:UnlockingthepotentialofSLMsinGradeSchoolMath.2024.arXiv:
2402.14830[cs.CL].
[93] “OCR-VQA:VisualQuestionAnsweringbyReadingTextinImages”.In:2019.
[94] OpenAI.ChatGPT.2022.
[95] OpenAI.gpt4o.2024.
[96] M. Oquab et al. “Dinov2: Learning robust visual features without supervision”. In:
TMLR(2023).
25[97] L.Ouyangetal.“Traininglanguagemodelstofollowinstructionswithhumanfeedback”.
In:NeurIPS.2022.
[98] A.Parker.Intheblinkofaneye:howvisionsparkedthebigbangofevolution.2003.
[99] P.PasupatandP.Liang.“Compositionalsemanticparsingonsemi-structuredtables”.In:
ACL.2015.
[100] J. Piaget, M. Cook, et al. The origins of intelligence in children. Vol. 8. 5. International
UniversitiesPressNewYork,1952.
[101] J. Pont-Tuset et al. “Connecting Vision and Language with Localized Narratives”. In:
ECCV.2020.
[102] A.Radfordetal.“Learningtransferablevisualmodelsfromnaturallanguagesupervi-
sion”.In:ICML.2021.
[103] R. Rafailov et al. “Direct preference optimization: Your language model is secretly a
rewardmodel”.In:NeurIPS.2024.
[104] R.Rombachetal.“High-ResolutionImageSynthesisWithLatentDiffusionModels”.In:
CVPR.2022.
[105] O. Russakovsky et al. “Imagenet large scale visual recognition challenge”. In: IJCV
(2015).
[106] O.Sanseviero.LLMEvalsandBenchmarking.2022.
[107] C.Schuhmannetal.“Laion-5b:Anopenlarge-scaledatasetfortrainingnextgeneration
image-textmodels”.In:NeurIPS.2022.
[108] D. Schwenk et al. “A-OKVQA: A Benchmark for Visual Question Answering using
WorldKnowledge”.In:ECCV.2022.
[109] M.Shridharetal.“ALFWorld:AligningTextandEmbodiedEnvironmentsforInteractive
Learning”.In:ICLR.2021.
[110] C.Sietal.“Design2Code:HowFarAreWeFromAutomatingFront-EndEngineering?”
In:arXivpreprintarXiv:2403.03163(2024).
[111] O.Sidorovetal.TextCaps:aDatasetforImageCaptioningwithReadingComprehension.2020.
arXiv:2003.12462[cs.CV].
[112] A.Singhetal.“Towardsvqamodelsthatcanread”.In:CVPR.2019.
[113] Q.Sunetal.“Eva-clip:Improvedtrainingtechniquesforclipatscale”.In:arXivpreprint
arXiv:2303.15389(2023).
[114] R.Tanaka,K.Nishida,andS.Yoshida.“VisualMRC:MachineReadingComprehension
onDocumentImages”.In:AAAI.2021.
[115] B.J.Tang,A.Boggust,andA.Satyanarayan.“Vistext:Abenchmarkforsemanticallyrich
chartcaptioning”.In:arXivpreprintarXiv:2307.05356(2023).
[116] S.Tong, E.Jones,and J.Steinhardt.“Mass-producing failuresof multimodalsystems
withlanguagemodels”.In:NeurIPS.2024.
[117] S.Tongetal.“Eyeswideshut?exploringthevisualshortcomingsofmultimodalllms”.
In:CVPR.2024.
[118] H.Touvronetal.“LLaMA2:Openfoundationandfine-tunedchatmodels”.In:(2023).
[119] H.Touvronetal.“LLaMA:Openandefficientfoundationlanguagemodels”.In:arXiv
preprintarXiv:2302.13971(2023).
[120] H.Tuetal.“Howmanyunicornsareinthisimage?asafetyevaluationbenchmarkfor
visionllms”.In:arXivpreprintarXiv:2311.16101(2023).
26[121] K. Vishniakov, Z. Shen, and Z. Liu. “ConvNet vs Transformer, Supervised vs CLIP:
BeyondImageNetAccuracy”.In:ICML.2024.
[122] J.Wangetal.“Toseeistobelieve:Promptinggpt-4vforbettervisualinstructiontuning”.
In:arXivpreprintarXiv:2311.07574(2023).
[123] K. Wang et al. “Measuring Multimodal Mathematical Reasoning with MATH-Vision
Dataset”.In:arXivpreprintarXiv:2402.14804(2024).
[124] J.Weietal.“Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels”.
In:NeurIPS.2022.
[125] C.Wendler.wendlerc/RenderedText.2023.
[126] H.Wuetal.“Q-instruct:Improvinglow-levelvisualabilitiesformulti-modalityfounda-
tionmodels”.In:arXivpreprintarXiv:2311.06783(2023).
[127] P.WuandS.Xie.“V*:GuidedVisualSearchasaCoreMechanisminMultimodalLLMs”.
In:CVPR.2024.
[128] xAI.grok.2024.
[129] H.Xuetal.“Demystifyingclipdata”.In:ICLR.2024.
[130] A.Youngetal.“Yi:Openfoundationmodelsby01.ai”.In:arXivpreprintarXiv:2403.04652
(2024).
[131] L.Yuetal.ModelingContextinReferringExpressions.2016.arXiv:1608.00272[cs.CV].
[132] T.Yuetal.“Rlhf-v:Towardstrustworthymllmsviabehavioralignmentfromfine-grained
correctionalhumanfeedback”.In:arXivpreprintarXiv:2312.00849(2023).
[133] X.Yueetal.“Mammoth:Buildingmathgeneralistmodelsthroughhybridinstruction
tuning”.In:ICLR.2024.
[134] X.Yueetal.“Mmmu:Amassivemulti-disciplinemultimodalunderstandingandrea-
soningbenchmarkforexpertagi”.In:CVPR.2024.
[135] M. Yuksekgonul et al. “When and why vision-language models behave like bags-of-
words,andwhattodoaboutit?”In:ICLR.2022.
[136] X.Zhaietal.“Sigmoidlossforlanguageimagepre-training”.In:ICCV.2023.
[137] Y.Zhaietal.“Fine-TuningLargeVision-LanguageModelsasDecision-MakingAgents
viaReinforcementLearning”.In:arXivpreprintarXiv:2405.10292(2024).
[138] Y.Zhaietal.“Investigatingthecatastrophicforgettinginmultimodallargelanguage
models”.In:CPAL.2024.
[139] C. Zhang et al. “Raven: A dataset for relational and analogical visual reasoning”. In:
CVPR.2019.
[140] Y.Zhangetal.“Llavar:Enhancedvisualinstructiontuningfortext-richimageunder-
standing”.In:arXivpreprintarXiv:2306.17107(2023).
[141] Y.Zhaoetal.“Pytorchfsdp:experiencesonscalingfullyshardeddataparallel”.In:arXiv
preprintarXiv:2304.11277(2023).
[142] L.Zhengetal.“Judgingllm-as-a-judgewithmt-benchandchatbotarena”.In:NeurIPS.
2024.
[143] T.Zhengetal.“OpenCodeInterpreter:IntegratingCodeGenerationwithExecutionand
Refinement”.In:arXivpreprintarXiv:2402.14658(2024).
[144] V.Zhong,C.Xiong,andR.Socher.“Seq2sql:Generatingstructuredqueriesfromnatural
languageusingreinforcementlearning”.In:2017.
27[145] B.Zhouetal.“Semanticunderstandingofscenesthroughtheade20kdataset”.In:IJCV
(2019).
[146] K. Zhou et al. “Don’t Make Your LLM an Evaluation Benchmark Cheater”. In: arXiv
preprintarXiv:2311.01964(2023).
[147] B.Zhuetal.Starling-7b:Improvingllmhelpfulness&harmlessnesswithrlaif.2023.
[148] F. Zhu et al. “TAT-QA: A question answering benchmark on a hybrid of tabular and
textualcontentinfinance”.In:ACL.2021.
[149] Y.Zhuetal.“Visual7w:Groundedquestionansweringinimages”.In:CVPR.2016.
28A. Training, Infrastructure, and Implementation
All models in this paper were trained using TPU-V4 pods [53]; we evaluate using NVIDIA
A6000,A100,andH100cards. TheexperimentsinSection3.4requirelessthan24hoursona
TPU-V4-128,whileourfinalCambrian-1modelsaretrainedinlessthan4daysonaTPU-V4-512.
Toenableandfacilitatelarge-scaleparalleltrainingonTPUs,weemployTorchXLAwith
FSDP [141] to handle training sharding and parallelism. Training a large-scale multimodal
modelwithTorchXLAonTPUisachallengingjourney,astherearenoopen-sourcecodebases
andmanycriticalfeaturesarenotsupportedintheTorchXLAorTorchXLAFSDPlibraries. To
provide a brief taste of the difficulties: TPUs require a staticgraph throughout theprogram,
which requires ground-up rewrites of dynamically-written open-source PyTorch codebases;
modelresumingisnotimplementedinTorchXLA,whichisespeciallycrucialwhentrainingon
preemptableTPUs;existingTorchXLAFSDPtutorialsfailtocompileduetoversionchangesin
TorchXLA,updatesinHuggingFaceTransformers&Accelerate,orsimplyinherentissueswith
thetutorial;loadingverylargemodels(over30billionparameters)withtheTorchXLAFSDP
libraryisnativelyimpossibleduetothe100GBmemoryconstraintsofTPU-V4s,andrequires
extensiveworkarounds.
To this end, we have rewritten or developed many new functions to make this research
possible. For instance, we rewrote the TorchXLA FSDP Sharding API to load very large
models;weimplementedmodelresumingonTorchXLA;werewrotepartsoftheHuggingFace
TransformersFSDPandgradientcheckpointingimplementationstoenablelarge-scaleFSDP
training. Wearecommittedtoopen-sourcingourcodebaseandpublishingacomprehensive
tutorialtoshareourinsights,withthehopeofinspiringandsupportingfutureresearchand
open-sourcecontributionstotheTPUandTorchXLAecosystem.
B. Analyzing the Benchmarks
MLLMBenchmarkPerformanceConfusionMatrix
Weevaluatethebenchmarkscoresforourone-stage,two-stagefinetune-onlyandhybrid
models,andthenplotthecorrelationmatrixforthepoolofMLLMbenchmarks. Thecorrelation
plotdisplaysinFig.15. TheresultdemonstratesthatMMMUislesscorrelatedinmeasuring
modelperformancetootherbenchmarks. Nonetheless,weacknowledgeitiswidelyusedand
thereforeclusteritintoknowledge-basedQAsbasedonthenatureoftheirquestions.
C. Cambrian Vision-Centric Benchmark (CV-Bench)
C.1. CurationProcedure
We provide an overview of the data curation process in Fig. 16, which is conducted in a
semi-automaticmanner. Theprocedureconsistsoftwomainsteps:
First, using the original benchmarks and their associated ground truth annotations, we
generatequery andanswer pairs. These pairsaretailored tospecific tasks: 2D-related tasks
withCOCOandADE20Kdatasets,and3D-relatedtaskswithOmni3D.
Second,aftergeneratingthequeryandanswerpairs,weengagehumanexpertstomanually
filteroutanyincorrectorambiguousqueriestoenhancethequalityofbenchmark. Eachquery
isassignedoneofthreestatuses: accepted(usedasis),modified(wheretheincorrectanswer
ismodified),andrejected(queriesthatareambiguous,suchasthosetoosmallordifficultto
discern,evenforhumanexperts).
291.0
MME 1.00 0.97 0.96 0.96 0.82 0.65 0.75 0.82 0.78 0.82 0.84 0.79 0.87 0.84 0.78
MMB 0.97 1.00 0.97 0.95 0.85 0.73 0.75 0.84 0.80 0.87 0.87 0.81 0.87 0.88 0.79
SEED-I 0.96 0.97 1.00 0.97 0.79 0.70 0.77 0.85 0.78 0.83 0.84 0.79 0.91 0.91 0.83
0.9
GQA 0.96 0.95 0.97 1.00 0.70 0.69 0.71 0.76 0.68 0.73 0.74 0.69 0.87 0.83 0.78
SQA-I 0.82 0.85 0.79 0.70 1.00 0.62 0.75 0.78 0.89 0.94 0.95 0.93 0.71 0.83 0.72
MMMU 0.65 0.73 0.70 0.69 0.62 1.00 0.57 0.54 0.53 0.65 0.65 0.58 0.51 0.60 0.62
0.8
MathVista 0.75 0.75 0.77 0.71 0.75 0.57 1.00 0.65 0.78 0.78 0.80 0.78 0.74 0.74 0.52
AI2D 0.82 0.84 0.85 0.76 0.78 0.54 0.65 1.00 0.83 0.86 0.86 0.83 0.81 0.85 0.76
ChartQA 0.78 0.80 0.78 0.68 0.89 0.53 0.78 0.83 1.00 0.95 0.96 0.98 0.75 0.85 0.66
0.7
OCRBench 0.82 0.87 0.83 0.73 0.94 0.65 0.78 0.86 0.95 1.00 0.98 0.95 0.76 0.89 0.72
TextVQA 0.84 0.87 0.84 0.74 0.95 0.65 0.80 0.86 0.96 0.98 1.00 0.98 0.79 0.89 0.74
DocVQA 0.79 0.81 0.79 0.69 0.93 0.58 0.78 0.83 0.98 0.95 0.98 1.00 0.73 0.85 0.70
0.6
MMVP 0.87 0.87 0.91 0.87 0.71 0.51 0.74 0.81 0.75 0.76 0.79 0.73 1.00 0.90 0.79
RW QA 0.84 0.88 0.91 0.83 0.83 0.60 0.74 0.85 0.85 0.89 0.89 0.85 0.90 1.00 0.84
CV-CB 0.78 0.79 0.83 0.78 0.72 0.62 0.52 0.76 0.66 0.72 0.74 0.70 0.79 0.84 1.00
Figure15|CorrelationmatrixforMLLMbenchmarks. ThecorrelationmatrixforMLLMbenchmarks
with respect to different vision backbones. The correlation matrix helps us to analysis and group
benchmarks.
Following this two-stage process, we finalize the benchmark, which results in a total of
2638imagequerieswithimprovedaccuracyandreliability. Subsequently,wewilldiscussthe
methodsofhumanverificationandtheevaluationmetricsusedinthisprocess.
C.2. Humanverification
There are multiple reasons for the above generated data to be inaccurate. One of the main
reasonsissparseannotations,butoccasionallytherecouldbewrongannotationsaswell.
Thus,weneedmanualinspectiontochange/removetheseexamplesgenerated. Herearea
fewcriteriawefollowedwhilemanuallyfilteringCOCOandADE20kdata.
ForCountingquestiontypes,ifallinstancesofacategoryarenotannotated,theground
truthwouldhavelowercountthantheactualnumberofinstancesappearingintheimage. Ina
fewcaseswheretheimagedistinctlyhasdifferentcountableinstancesoftheobject,wechange
theoptions/answer. Incasethecountisambiguous,werejectthedatasamplealtogether.
ForRelativeDistancequestiontypeswithoutannotation,ifthequestionisaskedabouttwo
objectsAandBandiftherearetwoinstancesofaspecificcategory(sayA),therelativelocation
ofAw.r.tBcanbehavemultiplecorrectanswers. Werejectthesampleinthiscase. Wealso
rejectcaseswithclearincorrectannotations.
C.3. BenchmarkEvaluation
To ensure that equal importance is given to both 2D and 3D tasks, we use an evaluation
metricthatistheaverageoftheaccuraciesobtainedfromthesetasks. Specifically,theoverall
performanceiscalculatedasfollows:
30
EMM BMM I-DEES AQG I-AQS UMMM atsiVhtaM D2IA AQtrahC hcneBRCO AQVtxeT AQVcoD PVMM AQ
WR
BC-VCFigure16|CV-CBBenchmarkFiltering. Wereformulateclassic2Dand3DCVbenchmarksintoQ&A
questionstoevaluateMLLM’svisualcapabilities.
(cid:16)Accuracy +Accuracy (cid:17)
Accuracy =
𝐶𝑂𝐶𝑂 𝐴𝐷𝐸20𝑘
2𝐷
2
(cid:16)Accuracy +Accuracy (cid:17)
OverallAccuracy =
2𝐷 3𝐷
2
D. Vision Models in MLLMs
AsmentionedinSection3.4,weuseMLLMasaninterfacetoevaluatevisionmodel’sdifferent
capabilities. Here,welistdetailsintermsofthemodelselection,fullresults,anddatasplit.
D.1. DetailsofVisionModels
In our exploration of versatile vision models, we select thirteen models and group them
into four categories: language-supervised models (i.e., OpenAI CLIP [102], SigLIP [136], DFN-
CLIP [34], EVA-CLIP [113] and OpenCLIP [28]), self-supervised models (i.e., DINOv2 [96], I-
JEPA[8],MAE[45],MoCov3[24]),class-supervisedmodels(ImagetNet22KViT[33])andother
modelssuchasstablediffusion[104]7,segmentationmodelslikeSAM[61],anddepthestimation
models like MiDaS [13]. To provide a clear understanding of the specific variant evaluated,
wemeticulouslydetailtheirbackbonearchitectures,resolution,numberoftokens,andhidden
dimensionsizesinTable9. Formodelsthatoutputalargenumberofpatchesinthelastlayer
(e.g., SAM and ConvNeXt) we interpolate to the number of tokens specified in Table 9, and
denoteinterpolationwithI.
D.2. FullResultsofDifferentVisionBackboneinMLLM
Fortheabove-listedvisionmodelsinTable9,theyareintegratedasthevisionencoderofthe
MLLMs. TheseMLLMsaretrainedonvariousadapteradapterdatasplits(i.e.,0,0.5and1.2
million),andsubsequentlyfine-tunedona737KinstructiontuningdatasetprovidedinLLaVA-
1.5[73]. For the adapter data splits, the 0M split indicates that no initial adapter pertaining
phase is employed for the MLLM. The 0.5M data split utilizes the 558K adapter data from
LLaVA-1.5[73],whilethe1.2MvariantusesShareGPT4V-PTdataset[22].
7Weextractfeaturesfollowingthepracticein[11]
31Supervision Method Architecture Patch Res. #Tok. Hidden
Type Size Size
Language-Supervised
Language OpenAICLIP ViT-L 14 336 576 768
DFN-CLIP ViT-L 14 224 256 1024
DFN-CLIP ViT-H 14 378 729 1280
EVA-CLIP-02 ViT-L 14 336 576 1024
SigLIP ViT-L 16 384 576 1024
SigLIP ViT-SO400M 14 384 729 1152
OpenCLIP ConvNeXT-L - 512 I576 1536
OpenCLIP ConvNeXT-L - 1024 I576 1536
OpenCLIP ConvNeXT-XXL - 1024 I576 3072
Self-Supervised
Contrastive DINOv2 ViT-L 14 336 576 1024
DINOv2 ViT-L 14 518 I576 1024
MoCov3 ViT-B 16 224 196 768
MoCov3 ViT-L 16 224 196 1024
Masked MAE ViT-L 16 224 196 1024
MAE ViT-H 14 224 256 1280
JEPA I-JEPA ViT-H 14 224 256 1280
Other
Segmentation SAM ViT-L 16 1024 I576 1024
SAM ViT-L 16 1024 I576 1280
Depth MiDaS3.0 ViT-L 16 384 576 1024
MiDaS3.1 ViT-L 16 518 1024 1024
Diffusion StableDiffusion2.1 VAE+UNet 16 512 1024 3520
ClassLabels SupViT ViT-L 16 224 196 1024
SupViT ViT-H 14 224 256 1280
Table 9 | Catalog of all vision backbones tested. I denotes that the visual tokens have been
interpolateddowntothespecifiedlength.
0M Adapter Data + 737K Instruction Tuning Data As shown in Table 10, we provide 20
resultsfordifferentvariantsoftheabove-mentionedthirteenvisionbackbones. Amongthem,
language-supervisedmodelsshowsuperiorperformance. Especially,OpenCLIPConvNeXT-
XXL@1024modelsurpassesallothermodelsonDocVQAwithover12%,indicatingitspotential
tohandleOCR-relatedbenchmarks.
0.5M Adapter Data + 737K Instruction Finetune As shown in Table 11 and Table 10, the
inclusionofanalignmentstagewith0.5Mdatasplitresultsinanotableincreaseinperformance
forDFN-CLIPViT-H/14@378,from36.21to49.94. Thissubstantialimprovementhighlightsthe
valueofthealignmentstageforenhancingcertainvisionbackbones,suggestingitsimportance
inharnessingthefullpotentialofvisionmodels.
1.2M Adapter Data + 737K Instruction Finetune As we increase the amount of data in
the alignment phase, we observe a consistent performance improvement for SigLIP ViT-
SO400M/14@384from46.79to49.72to53.09across0M,0.5Mto1.2Mdatasplitsasshownin
Table10,Table11andTable12.
1.2M Adapter Data + 737K Instruction Finetune with Unfrozen Vision Model Here, we
present the results of different vision models trained with 1.2m adapter data and 737K in-
struction tuning data in Appendix D.2. Comparing to Appendix G.2, we observe nearly all
32VisionBackbone General Knowledge OCR&Chart Vision-Centric
Model Architecture
LanguageSupervised
OpenAICLIPViT-L/14@336 48.37 1,419.4361.45 59.8562.26 69.8734.5027.8059.82 33.7331.7055.3928.00 11.3353.4656.4457.40
DFN-CLIP ViT-L/14@224 38.78 1,172.5049.53 49.7452.94 67.7434.0027.3056.99 16.75 4.8744.8111.19 6.6746.9740.2952.08
DFN-CLIP ViT-H/14@378 36.79 1,091.7641.28 44.3250.48 65.5433.6526.5056.76 15.56 2.7043.4110.15 4.6747.0639.0752.91
EVA-CLIP-02 ViT-L/14@336 45.84 1,325.1758.21 62.9962.03 68.6735.0027.5058.26 19.4022.5051.0816.36 24.6752.6852.9854.83
SigLIP ViT-L/16@384 48.80 1,383.4261.02 63.5661.85 68.9135.2929.7057.87 34.9629.6056.7328.31 23.3352.6852.9554.83
SigLIP ViT-SO400M/14@384 47.57 1,376.7558.76 60.5960.92 69.0134.4026.5058.35 30.7228.6055.1028.31 19.3350.7152.3358.67
OpenCLIP ConvNeXt-L@512 47.38 1,404.0157.62 61.9060.34 69.0633.9029.1058.39 28.0425.2055.4528.41 24.0054.1253.4648.91
OpenCLIP ConvNeXt-L@1024 39.02 1,139.6014.64 49.5937.91 65.7134.3027.3054.13 32.9712.0552.6138.36 9.6747.4552.6838.04
OpenCLIP ConvNeXt-XXL@1024 41.83 1,219.4748.00 49.8855.09 66.1435.6927.6056.67 16.92 5.0046.9040.98 16.0047.3243.4052.75
SelfSupervised
DINOv2 ViT-L/14@336 41.18 1,262.6649.62 56.8060.30 65.1035.0026.4056.41 16.48 3.1044.0411.90 18.6750.2049.4352.25
DINOv2 ViT-L/14@518 40.60 1,242.4851.00 53.3960.38 64.5534.5026.2057.53 15.11 2.9044.2810.95 14.0048.6346.1357.90
MoCov3 ViT-B/16@224 34.94 966.4536.77 33.0047.35 62.9632.8026.2055.05 16.04 2.6043.8110.31 6.6745.3639.0352.83
MoCov3 ViT-L/16@224 34.70 1010.1834.64 41.7147.46 64.7033.7026.3055.05 16.24 2.7042.6010.39 4.0045.3644.6735.16
MAE ViT-L/16@224 37.69 1,114.0742.30 35.9355.20 63.5134.6026.0056.10 16.11 2.7043.6310.83 14.0044.8045.8155.75
MAE ViT-H/14@224 38.58 1,083.3541.15 50.9955.30 64.9034.1026.0056.49 15.63 3.2043.9811.00 12.0046.3047.1854.90
I-JEPA ViT-H/14@224 38.88 1,132.0744.68 51.7455.37 66.0434.2026.4056.09 15.84 3.0043.6611.48 10.6746.0146.7453.50
Other
SAM ViT-L/16@1024 31.74 585.7820.34 36.3439.85 65.4934.5025.1053.92 16.16 2.7042.37 9.25 2.0044.4435.6550.50
SAM ViT-H/16@1024 32.37 648.9622.30 36.3140.52 65.2034.1026.0054.44 15.56 2.4042.39 8.75 2.0045.3634.8355.25
MiDaS3.0 ViT-L/16@384 35.65 981.3638.57 40.9349.04 63.4131.8025.7054.72 16.36 2.6043.1911.24 6.6744.9738.7853.40
MiDaS3.1 ViT-L/16@518 35.44 983.3434.79 40.2048.53 64.6033.9025.0055.18 15.64 2.6042.7612.08 6.6643.6639.6352.58
Diffusion SD2.1/16@512 36.59 1,044.2837.71 42.0048.38 64.5533.4025.7056.99 15.56 3.1043.1410.40 9.3345.8844.6852.40
SupViT ViT-L/16@224 40.13 1,197.3946.55 54.7257.27 65.9434.0028.0056.22 16.44 3.1043.5211.82 16.6746.6748.4952.75
SupViT ViT-H/14@224 37.45 1,082.4342.61 48.4552.98 63.5135.2926.5055.78 15.16 3.3044.1611.49 4.6643.7944.5552.91
Table10 | AllBenchmarkResultsfor0MAdapterData+737KInstructionTuningData
themodelsseeimprovementonmostofthebenchmarks,especiallyontheOCR&Chartand
Vision-Centricbenchmarks.
1.2M Adapter Data + 5M Instruction Finetune We present the results of 5M instruction
tuning experiments in Fig. 7 here. In Table 13, we observe that after 5m instruction tuning,
the gap between DINOv2 and CLIP models continue to bridge on general, knowledge and
vision-centricbenchmarks.
D.3. ModelEnsemble
ModelEnsembleDetails Weintroducetheimplementationdetailsofthemodelensemble
in Section 3.5. For a given image, the image passes through each vision encoder to obtain
the features from the last layer. The shape of each model’s output differs depending on the
resolutionandpatchsizeofeachvisionmodel. Toresolvethesedifferences,weinterpolatethe
outputofeachmodeltoafixednumberoftokens,using576tokensinourimplementation,as
describedinSection3.5. Ourexamplecodeforinterpolationcanbeseenbelow.
# Example code for interpolation
b, num_tokens, dim = image_features.shape
if num_tokens != self.image_token_len:
target_h = target_w = int(np.sqrt(self.image_token_len))
h = w = int(np.sqrt(num_tokens))
image_features = image_features.view(b, h, w, dim)
image_features = image_features.permute(0, 3, 1, 2).contiguous()
image_features = F.interpolate(image_features), size=(target_h, target_w), mode=’bilinear’, align_corners=False)
image_features = image_features.permute(0, 2, 3, 1).contiguous().flatten(1, 2)
WethenconcatenatethemodeloutputsalongthefeaturedimensionandusealargerMLP
toprojecttheconcatenatedvisualtokensintotheLLMtokenspace.
33
egarevA
PEMM BMM IDEES AQG IAQS
VUMMM
MatsiVhtaM
D2IA
AQtrahC
hcneBRCO
AQVtxeT AQVcoD
PVMM
AQdlroWlaeR
D2hcneB-VC D3hcneB-VCVisionBackbone General Knowledge OCR&Chart Vision-Centric
Model Architecture
Language-Supervised
OpenAICLIPViT-L/14@336 49.03 1,413.5160.3462.1760.81 69.7636.4929.9058.48 36.8030.2057.6330.98 21.3351.6352.5754.75
DFN-CLIP ViT-L/14@224 45.59 1,382.7557.3663.2660.54 66.8135.0929.2057.71 22.8823.4552.2718.82 21.3351.3149.5950.63
DFN-CLIP ViT-H/14@378 50.62 1,500.4562.6466.4462.53 70.7535.6930.3058.78 39.2029.8056.9831.39 29.3353.5954.9652.58
EVA-CLIP-02 ViT-L/14@336 47.13 1,362.0762.6463.9661.66 69.4635.8927.9056.96 20.9626.1053.9319.07 20.0053.0756.2858.16
SigLIP ViT-L/16@384 48.11 1,381.4861.7961.8759.45 70.2535.9928.8057.58 28.7628.2054.9025.60 26.0052.2952.8956.33
SigLIP ViT-SO400M/14@384 50.41 1,327.7962.1363.9261.31 70.3836.9930.0059.52 40.0833.2060.3736.58 22.0053.9955.5954.08
OpenCLIP ConvNeXt-L@512 48.01 1,366.8559.6662.8961.31 68.7736.9928.5059.29 27.8828.5057.5729.48 16.0053.2053.8155.91
OpenCLIP ConvNeXt-L@1024 40.29 1,084.6212.9451.0249.78 65.4734.2027.6056.36 29.9213.2550.3743.67 13.3349.0851.8541.58
OpenCLIP ConvNeXt-XXL@1024 50.45 1,405.6557.9663.5862.41 68.0234.3029.4059.62 42.9626.2061.8242.67 28.6755.1649.9254.16
Self-Supervised
DINOv2 ViT-L/14@336 42.64 1,283.9554.6459.0360.19 66.3935.2925.7058.03 16.00 3.2045.3911.79 20.0050.5953.5158.33
MoCov3 ViT-B/16@224 38.50 1,159.1040.0051.3754.97 65.2533.7027.2055.51 16.36 3.3044.4211.42 10.6746.1445.8052.00
MoCov3 ViT-L/16@224 37.71 1,074.1341.1949.4653.61 63.6633.7027.4055.83 17.04 3.4043.8411.98 8.0046.2748.6945.59
MAE ViT-L/16@224 39.99 1,138.3544.6054.9156.69 65.6436.1927.9056.48 17.20 3.2044.4512.42 14.0047.3248.8353.08
I-JEPA ViT-L/14@224 39.91 1,180.1244.2652.8655.32 65.9434.4027.0057.16 15.88 3.2044.3611.61 13.3346.2752.1955.83
Other
SAM ViT-H/16@1024 32.18 649.9922.4736.3740.46 64.6032.5025.8054.66 15.80 2.7042.40 8.89 0.0045.6237.0253.08
MiDaS3.0 ViT-L/16@384 40.07 1,183.9547.4053.0056.15 66.1932.9027.6056.61 17.00 3.0044.3411.55 19.3347.3245.0154.58
Diffusion SD2.1/16@512 38.26 1,123.4642.0450.6653.63 65.7433.3024.5057.48 14.52 3.3043.9510.62 10.0043.5348.2454.50
SupViT ViT-L/16@224 39.66 1,186.8848.4354.2856.35 65.4933.0028.1057.16 17.56 2.8044.9212.23 12.6747.0643.5951.67
Table11 | AllBenchmarkResultsfor0.5MAdapterData+737KInstructionTuningData
FullresultsonModelEnsemble Wepresentallthebenchmarksfromthemodelensemble
experiment in Section 3.5 in Table 3. As discussed in Section 1 and Section 3.4, this compre-
hensive view of benchmarks provides a better understanding of the model’s performance
comparedtosimplyaveragingacrossbenchmarks. Addingavision-onlySSLmodelenhances
theMLLM’sperformanceinvision-centricbenchmarkswhilemaintainingstrongcapabilitiesin
othercategories.
E. Data
E.1. CatalogofVisualInstructionData
Here,weprovideacomprehensivecatalogofvisualinstructiondatasetsutilizedinourstudy.
Thedatasetsarecategorizedbasedontheirprimaryfocus,includinggeneralconversationand
VQAdata,OCR-relateddata,countingdata,knowledge-baseddata,andlanguage-onlydata.
Table15summarizesthesedatasetsandtheirrespectivereferences.
E.2. AdditionalSystemPromptsusedinCambrianData
AsourCambriandataincludesinstructions/questionsandresponsesofdifferenttypesand
formats(e.g.,Shortresponsewithasinglewordorregularresponseasacompletesentence),itis
importanttospecifytherequiredresponseformatintheinstructionprompttoavoidambiguity
andpossibleconflicts. Someofthedatasetsalreadyincludesuchpromptsandweaddproper
promptsfortheremainingdatasets. Thedetailedresponseformattingpromptsweadditionally
addarelistedinTable16.
E.3. DataEngine
ComprehensiveImplementationDetailsoftheDataEngine
Thedataengineisdesignedtogenerateinstructiontuningdataforknowledge-basedfields,
wherepreviousworksrarelycoversandMLLMsarenotreliabletodistillforfrom. Thedata
34
egarevA
PEMM BMM IDEES AQG IAQS
VUMMM
MatsiVhtaM
D2IA
AQtrahC
hcneBRCO
AQVtxeT AQVcoD
PVMM
AQdlroWlaeR
D2hcneB-VC D3hcneB-VCVisionBackbone General Knowledge OCR&Chart Vision-Centric
Model Architecture
Language-Supervised
OpenAICLIPViT-L/14@336 50.49 1,476.65 61.9665.4562.78 69.0635.0029.50 58.94 37.8430.9058.2132.11 28.6654.9054.1454.60
DFN-CLIP ViT-L/14@224 46.01 1,341.14 56.6863.7460.75 66.9633.8028.65 57.04 23.3223.2052.8518.97 26.6751.4452.9152.08
DFN-CLIP ViT-H/14@378 51.17 1,426.32 62.3867.2962.89 69.0135.8930.00 60.01 41.0830.6057.5331.69 32.6755.9555.4655.00
EVA-CLIP-02 ViT-L/14@336 49.71 1,449.78 64.0067.5363.60 69.9135.4928.40 59.16 24.7627.1055.3921.63 34.6755.6957.8357.75
SigLIP ViT-L/16@384 50.87 1,424.20 59.4065.4862.56 68.6735.9929.70 59.29 40.5233.5059.5935.20 28.0053.3355.4256.08
SigLIP ViT-SO400M/14@384 53.91 1,455.64 63.6667.6263.70 72.1036.0929.30 61.59 43.7637.2061.8240.19 36.6056.9959.6159.58
OpenCLIP ConvNeXt-L@512 49.16 1,416.87 60.6063.8761.87 69.9235.7929.50 59.36 34.4028.0058.3628.41 27.3351.9054.6451.80
OpenCLIP ConvNeXt-L@1024 51.00 1,392.92 58.2165.4762.89 67.4334.9029.90 59.13 46.0825.5062.1444.13 26.6755.2953.5755.08
OpenCLIP ConvNeXt-XXL@1024 52.18 1,402.94 59.4065.2162.73 68.2733.1029.30 59.84 48.0028.0063.2748.11 34.6755.9553.8355.08
Self-Supervsied
DINOv2 ViT-L/14@336 41.85 1,190.81 51.8356.9060.38 66.0434.2027.40 56.41 16.44 3.3045.1211.79 21.3349.6753.9155.33
MoCov3 ViT-B/16@224 38.88 1,129.32 41.6252.1955.03 65.8933.3028.30 56.44 16.48 3.0044.0911.47 12.0047.5845.1753.00
MoCov3 ViT-L/16@224 37.07 1015.20 37.2848.3152.63 65.4934.5027.60 55.41 16.92 3.1043.5711.50 14.6745.4945.1740.75
MAE ViT-L/16@224 40.39 1,132.80 43.4055.6757.42 66.0435.5927.80 56.48 17.36 3.3044.5312.30 16.0047.7149.2456.75
I-JEPA ViT-H/14@224 40.27 1,207.88 45.7954.5156.15 65.2934.4027.10 56.19 16.20 3.2043.4511.58 18.0045.8849.5756.58
Other
SAM ViT-H/16@1024 32.54 682.81 23.3236.1640.32 65.2033.2026.50 54.21 15.68 2.5041.76 8.98 1.3346.8037.6652.90
MiDaS3.0 ViT-L/16@384 39.15 1,132.18 46.2151.7555.57 66.3033.7026.70 56.06 17.08 3.1043.6511.66 15.3045.7544.4452.58
Diffusion SD2.1/16@512 39.51 1,168.52 40.0053.8055.33 64.6035.0026.10 57.16 15.36 3.1044.2311.06 18.6747.3248.0453.90
Supervised ViT-L/16@224 39.12 1,216.11 45.2851.4655.88 64.1534.7026.80 55.76 16.80 2.8044.4211.61 11.3347.9744.6251.60
Table12 | AllBenchmarkResultsfor1.2MAdapterData+737KInstructionTuningData
Web Search Image
Knowledge Engine
API Filtered Parser
Web Data VQA Data
LLM Relevant LLM
Fields Topics Q&A
Text
Figure17|TargetedInternetDataCollectionEngine. Webuildatargetedinternetdataenginetocollect
high-qualityandlarge-scalemultimodalinstructiontuningdatafordomainslikeknowledge.
enginetakesinagivenfield,suchas“Physics”,utilizingreliablewebsourceslikeWikipedia.
Belowarethevariousstagesinvolvedintheprocess. WealsovisualizethisprocessinFig.17:
Stage1-TopicGeneration: Westartbycompilingalistoffieldsandsubfieldsandsubsequently
generate topics for each field using a Large Language Model (LLM), such as GPT-4. In this
stage,weprocessed30fields,resultingin3660topics. Wethenpost-processtheoutputofLLMs
intojsonformats. Forexample,thetopicdataforPhysicslookslikebelow.
Physics
{
"Classical Mechanics": [
"Newton’s Laws of Motion",
"Conservation of Energy",
"Conservation of Momentum",
"Harmonic Motion",
"Rotational Dynamics",
"Gravitation and Orbits",
"Fluid Dynamics",
"Elasticity and Plasticity",
"Friction",
"Waves and Sound",
"Velocity and Acceleration",
"Angular Momentum",
"Statics and Equilibrium",
"Kinematics of Particles",
"Dynamics of Systems of Particles",
"Collisions",
"Centripetal Force and Acceleration",
"Lagrangian and Hamiltonian Mechanics",
"Chaos Theory",
"Equations of Motion"
],
"Electromagnetism": [
"Coulomb’s Law",
"Electric Field and Electric Potential",
"Gauss’s Law",
"Capacitance and Dielectrics",
"Current and Resistance",
35
egarevA
PEMM BMM IDEES AQG IAQS
VUMMM
MatsiVhtaM
D2IA
AQtrahC
hcneBRCO
AQVtxeT AQVcoD
PVMM
AQdlroWlaeR
D2hcneB-VC D3hcneB-VCVisionBackbone General Knowledge OCR&Chart Vision-Centric
Model Architecture
Other
OpenAICLIPViT-L/14@336 52.90 1,477.1563.1568.4964.24 69.2134.9028.7061.24 47.6840.2058.9235.22 26.0058.8259.6556.16
DFN-CLIP ViT-L/14@224 45.80 1,364.9155.1563.0659.94 67.2835.5929.8058.65 20.4823.4052.2318.29 22.6750.8553.5953.50
EVA-CLIP-02 ViT-L/14@336 51.30 1,492.3565.5369.7565.18 68.6235.0029.5060.78 29.3229.9056.8721.59 44.6758.9554.6655.91
SigLIP ViT-L/16@384 52.47 1,429.1163.5767.3463.44 68.0236.0929.7061.56 46.6835.7059.8635.93 32.6755.2955.2457.00
SigLIP ViT-SO400M/14@384 55.27 1,489.0566.5569.5964.58 70.4535.6929.2062.34 51.2840.8063.2843.02 38.0059.6161.5853.91
OpenCLIP ConvNeXt-L@512 52.76 1,467.3863.4066.9263.17 69.1634.9029.7058.45 52.0435.4061.8738.79 30.6756.2154.2455.91
Other
DINOv2 ViT-L/14@336 43.26 1,261.4353.9663.2262.61 65.4934.5027.7056.90 15.40 3.4044.8711.22 26.0054.3853.0656.40
MoCov3 ViT-B/16@224 39.51 1,175.3441.7053.3156.15 65.2533.4028.3055.76 15.48 3.2044.4211.10 18.0046.6745.3855.25
MoCov3 ViT-L/16@224 37.59 1075.3939.1550.1453.65 65.4934.6027.3055.44 17.28 3.0044.2111.70 14.6744.5845.3841.12
MAE ViT-L/16@224 41.43 1,181.5145.5358.9258.75 64.6535.0029.2057.12 16.88 3.1044.6711.74 18.6749.6753.0456.83
I-JEPA ViT-H/14@224 41.90 1,175.7048.0059.6059.35 64.4535.0927.6057.32 16.20 3.0045.5011.40 22.6749.9352.3859.08
Other
MiDaS3.0 ViT-L/16@384 38.28 1,065.2642.6450.9556.10 65.3935.0027.2052.98 15.96 2.8043.4911.23 12.0046.1443.4153.90
Supervised ViT-L/16@224 40.01 1,222.4147.4054.1557.26 64.3534.4026.4056.09 16.20 3.2044.7311.74 14.0046.4149.3353.40
Table13 | AllBenchmarkResultsfor1.2MAdapterData+737KInstructionTuningDatawith
Unfrozenvisionmodel.
VisionBackbone General Knowledge OCR&Chart Vision-Centric
Method Backbone Unfreeze
OpenAICLIPViT-L/14@336 × 55.85 1,577.3369.7070.2263.33 73.6736.1936.6064.80 49.1236.9060.3339.79 32.6755.5666.1159.75
DINOv2 ViT-L/14@336 × 45.36 1,373.1457.0264.5861.67 67.1336.1930.7060.62 19.04 3.4046.3913.27 26.6752.6859.8157.91
OpenAICLIPViT-L/14@336 ✓ 57.44 1,585.3468.6871.4763.96 77.3936.0937.3065.12 59.3648.0062.3945.24 31.3356.2161.0956.16
DINOv2 ViT-L/14@336 ✓ 47.40 1,366.6561.6269.7263.68 68.7236.2935.5060.88 18.64 4.4047.9214.66 34.6754.6460.9857.83
Table14 | AllBenchmarkResultsfor1.2MAdapterData+5MInstructionTuningData
"Direct Current Circuits",
"Magnetic Fields and Magnetic Forces",
"Ampere’s Law",
"Faraday’s Law of Induction",
"Inductance",
"Alternating Current Circuits",
"Electromagnetic Waves",
"Maxwell’s Equations",
"Electromagnetic Radiation",
"Optics and Light",
"Quantum Electrodynamics",
"Special Theory of Relativity Implication",
"Magnetostatics",
"Electrostatics",
"Bioelectromagnetism"
],
...
}
Stage2-FilteringWebData: Foreachgeneratedtopic,weutilizesearchengineAPIstofetch
relevanthigh-qualitywebpages. Foreachtopic,wequeryfor10relevantlinks. Thus,weget
36,600webpagespostthisstage. Hereisanexampleofthedataretrievedforthetopic"Electric
FieldandElectricPotential":
"Electric Field and Electric Potential": [
"https://en.wikipedia.org/wiki/Electric_potential",
"https://en.wikipedia.org/wiki/Electric_field",
"https://en.wikipedia.org/wiki/Electric_potential_energy",
"https://en.wikipedia.org/wiki/Voltage",
"https://en.wikipedia.org/wiki/Electricity",
"https://en.wikipedia.org/wiki/Electrostatics",
"https://en.wikipedia.org/wiki/Electric_dipole_moment",
"https://en.wikipedia.org/wiki/Magnetic_vector_potential",
"https://en.wikipedia.org/wiki/Electric-field_screening",
"https://en.wikipedia.org/wiki/Electric_flux"
],
Stage3-Parsing: Inthisstage,weparseeachwebpagetoextractimage-caption-texttuples.
Weaimtoidentifytheblockscontaininganimage,theimage’scaption,andrelevanttextual
36
egarevA
egarevA
PEMM
PEMM
BMM
BMM
IDEES
IDEES
AQG
AQG
IAQS
IAQS
VUMMM
VUMMM
MatsiVhtaM
MatsiVhtaM
D2IA
D2IA
AQtrahC
AQtrahC
hcneBRCO
hcneBRCO
AQVtxeT
AQVtxeT
AQVcoD
AQVcoD
PVMM
PVMM
AQdlroWlaeR
AQdlroWlaeR
D2hcneB-VC
D2hcneB-VC
D3hcneB-VC
D3hcneB-VCCategory Datasets
GeneralConversation LVIS-Instruct4V[122],SketchyVQA[120],OODVQA[120],VizWiz[44],
&VQAData ALLaVA [20], IDK [17], Q-Instruct [126], LAION GPT-4V [63], Hate-
fulMemes [59], Visual7W [149], Visualmrc [114], AlfWorld [109],
LNQA[101],LLaVA150K[75],ShareGPT[22],VQAv2[43],GQA[50],
OKVQA[88],A-OKVQA[108],RefCOCO[131],VisualGenome[62],GPT-
4Vrecordedchat
OCRRelatedData LLAVAR[140],ChartQA[89],DocVQA[90],DVQA[54],ArxivQA[69],
AI2D [58], ScreenQA [49], SynthDog [60], IconQA [82], WTQ [99],
WikiSQL [144], FinQA [26], HiTab [27], TAT-QA [148], TabMWP [81],
Chart2Text[55],VisText[115],InfoVQA[14],ST-VQA[15],Rendered-
Text[125],OCRVQA[93],TextCaps[111],ShareGPTOCRData[22]
CountingData TallyQA[1],CLEVR[52]
Knowledge-BasedData Code:Design2Code[110],WebSight[64],Datikz[12]
Math: MathVision [123], Geo170K [37], TQA [4], Inter-GPS [83],
RAVEN[139],GeomVerse[57]
Science:ScienceQA[84],PathVQA[46]
LanguageOnlyData Dolly[30],MathInstruct[133],WizardCoder[86],OrcaMath[92],Open-
CodeInterpreter[143],OpenOrca[71]
Table15 | VisualInstruction-TuningDataCatalog
content. Belowisanexampleoftheparseddataforthesametopic,"ElectricFieldandElectric
Potential":
{
"Electric Field and Electric Potential",
[
{
"section": "Electrostatics",
"text": "An electric potential at a point r in a static electric field E is given by the line integral where C is an arbitrary
path from some fixed reference point to r; it is uniquely determined up to a constant... The generalization of electric potential
to this case is described in the section Generalization to electrodynamics.",
"images": [
{
"url": https://upload.wikimedia.org/wikipedia/commons/thumb/1/1e/VFPt_plus_thumb_potential+contour.svg/142px-VFPt_pl
us_thumb_potential+contour.svg.png,
"caption": "Electric potential of separate positive and negative point charges shown as color range from magenta (+), through
yellow (0), to cyan (-). Circular contours are equipotential lines. Electric field lines leave the positive charge and enter the
negative charge."
},
{
"url": https://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/VFPt_charges_plus_minus_potential+contour.svg/288p
x-VFPt_charges_plus_minus_potential+contour.svg.png,
"caption": "Electric potential in the vicinity of two opposite point charges."
}
],
"link": https://en.wikipedia.org/wiki/Electric_potential,
"title": "Electric potential",
"field": "Physics",
"subfield": "Electromagnetism",
"topic": "Electric Field and Electric Potential"
},
37...
]
}
Stage4-DataGeneration: Wegeneratedatasetinthisstage,ensuringhighquality. Wefirst
filteroutdatasampleswithfewerthan50wordsinthetext. Then,insteadofdownloadingimages
directlyfromthelinksretrievedduringwebparsing,wedownloadhigh-resolutionimagesfrom
the original sources. We then convert formats like SVG or GIF into a common standardized
format,PNG.
Question-AnswerpairsaregeneratedbyusingLLMsuchasGPT-3.5fromtheimagemeta-
data,caption,andcontextualtext. TheseQ&ApairsandtheimageformourVQAdataset. We
generated165kdatasamples. Hereisanexampleofthegenerateddata:
Figure18|DatasetImageId: 96232.png
{
"id": "96232.png",
"image_id": "450px-Magnetic_Vector_Potential_Circular_Toroid",
"image_url": "...",
"text": "...",
"caption": "Representing the Coulomb gauge magnetic vector potential A, magnetic
flux density B, and current density J fields around a toroidal inductor of circular
cross section. Thicker lines indicate field lines of higher average intensity. Circles
in the cross section of the core represent the B-field coming out of the picture,
plus signs represent B-field going into the picture. \u2207 \u22c5 A = 0 has been
assumed",
"section": "Depiction of the A-field",
"...",
"Question": "What do the thicker lines signify?",
"Answer": "The thicker lines in the image represent paths of higher average
intensity in the magnetic vector potential A field. Shorter paths have higher intensity,
ensuring that the path integral remains constant. These lines are drawn to visually
convey the general behavior and intensity distribution of the A-field around the
toroidal inductor"
}
Thisdataengineisdesignedforscalabilityandefficiencyandiscapableofhandlingexten-
sivedatagenerationtasksusingmultithreadingtechniques.
38E.4. Fullresultsondatacurationexperiment
DataBalanceviaFitlering𝑡 AsdiscussedinSection5.2,ifleftunfiltered,thedatapoolisdomi-
natedbynoisy,unbalanceddatasourcessuchasCLEVRandDVQA,leadingtopronounced
exponentialtails. However,asweapplydifferent𝑡 valuestofilterdatafromeachsource,the
exponentialtailsbecomelesspronounced,resultinginamorebalanceddataset. Wealsopresent
alltheresultsinTable17. 𝑡 value250khasthehighestaverageacrossallbenchmarks;250kand
350kalsohavethehighestperformanceacrossmanyindividualbenchmarks.
Data Ratio Studies We present the full results of our data ratio study in Table 18. The table
highlights the importance of finding an optimal data ratio that balances different aspects of
MLLM.Experiment5achieveswell-roundedperformancewithitsselecteddataratio.
E.5. 737Kand5MMixes
0.7MForthe0.7MdataweusedinSection3.4,WeaddasmallnumberofOCRandchartdata
toLLaVA665K,specifically15,501AI2D,14,999DocVQA,and13,000DVQAdatapoints. This
resultsina737Kmix,whichcoversallcategoriesintrainingMLLMs. Thisdatamixallowsus
tostudyvisualrepresentationsefficiently.
5.0M For the 5M data mixes we use in Section 3.4, we apply data filtering discussed in
Section5.2andapply𝑡=150konallmultimodalinstructiondatainCambrian-10M.
F. Implementation Details
Cambrian Models. For our final Cambrian models, we use 2.5M adapter data which is
comprised of 1.2M captioning data from shareGPT4V [22] and 1.2M captioning data used
inMiniGemini[70].
SVA.Weintroducelearnable𝑘 𝑚×𝑘 𝑚 positionalencodingsinthevisionfeatureswhen𝑘 𝑚 > 1.
Besides,duringcross-attention,thequeryisaugmentedwithaglobalfeatureobtainedbyglobal
poolingoverthevisionfeatures,whichisconcatenatedwithq𝑖,𝑗 tobetterguidetheaggregation
process. Inourexperiments,thefeaturemapsofallvisionencodersexceptforConvNextare
interpolatedto576×576(𝑚 𝑘 = 1for 𝐿 = 24). ForConvNext,wefirstinterpolatethefeaturemaps
fromits4stagesto96×96(𝑚 𝑘 = 4for 𝐿 = 24)andthenchannel-wiseconcatenatethemtoform
itsfinalvisionfeaturemapsimilarto[70].
ForexperimentsinSection4,weset 𝐷 = 3,𝐺 = 1andaddcross-attentionlayersbetweenthe
layersofLLMwithastrideequalto3. ForourfinalCambrianmodels,weset 𝐷 = 3,𝐺 = 1and
insertmultiplecross-attentionlayersinLLMconsideringthetradeoffbetweenperformanceand
efficiency. ForCambrian-8B,Cambrian-13B,andCambrian-34B,thestridesofcross-attention
layersinsidetheLLMare3,4,and9respectively.
G. Evaluation Details
G.1. SystemPromptsUsedinEvaluation
Toensurethereproductionofourresults,wealsoincludethesystempromptsweusedinthis
work. ThesystempromptsforourmodelscanbefoundinTable21. Additionally,wereleasethe
promptsweusedwhileevaluatingourmodelsonthevariousbenchmarksinTable22. Wehope
thissetsaprecedentforfutureresearchtoimprovethereproducibilityofbenchmarkresults.
39G.2. AblationStudyonFuzzyMatchingVsLLMJudgement
Weusefuzzymatchingtoevaluateresponsesinsomebenchmarks,sinceMLLMscananswer
questionswithauxillaryphrases. Tostudytheeffectivenessofourfuzzymatching,wecompare
ourmodelaccuracythroughfuzzymatchingwiththemodelaccuracyobtainedwhenweuse
LLMasagrader.
TheLLMgraderissensitivetothepromptgiventoitwhilegrading,andwepromptthe
LLM(weuseOpenAIGPT-3.5-turboandGPT-4-turboasourgraders)withfewshotgrading
examples, which we notice significantly improves grading accuracy. An example of such a
promptisgivenbelow.
LLMGraderPrompt
You are a reliable grader. Reply with only either of the following
2 words: CORRECT or INCORRECT.
You will be given an ’answer’ and a ’gt_answer’ (ground truth answer)
,and you must reply with either CORRECT or INCORRECT based on the
response. Tolerate a 0.05 relative error for numerical answers.
answer: 25
gt_answer: 29
evaluation: INCORRECT
answer: Yes
gt_answer: Yes
evaluation: CORRECT
answer: 80
gt_answer: 80
evaluation: CORRECT
answer: Ireland
gt_answer: Italy
evaluation: INCORRECT
answer: UK
gt_answer: UK
evaluation: CORRECT
answer: 2019
gt_answer: 2011
evaluation: INCORRECT
answer: {answer}
gt_answer: {gt_answer}
evaluation:
Weconductanablationstudyonthebenchmarksthatrequirefuzzymatchingandpresent
theresultsinTable20. Wediscoverthatfuzzymatchingprovidesreliableresultscomparedto
anLLMgrader. Werecommendusingamorecapablemodel(suchasGPT-4-turbo)forgrading
benchmarksthathavemoresubjectiveresponses(suchasnumbersandwords).
40Index Responseformattingprompts
1 Answerthequestionusingasinglewordorphrase.
2 Answerthequestionusingasinglenumberorphrase.
3 Answerwiththeoption’sletterfromthegivenchoicesdirectly.
4 Givetheshortanswerdirectly.
5 Answerthequestionusingasinglewordorphrase.
6 Whentheprovidedinformationisinsufficient,respondwith<noanswer>.
7 DirectlyprovidetheHTMLcode.
8 Firstshowyourreasoningprocessandthengivethefinalanswer.
9 Whentheprovidedinformationisinsufficient,respondwith’Unanswerable’.
Answerthequestionusingasinglewordorphrase.
10 Answerwiththeletter.
Dataset Promptsadded
SketchyVQA 1
OODVQA 1
VizWiz 9
Q-Instruct 1,3
ChartQA 2
DocVQA 4
DVQA 1
AI2D 1
ScreenQA 1,6
CLEVR 1
TallyQA 1
PathVQA 1
MathInstruct 8
Design2Code 7
IconQA 1,10
HiTab 1
WTQ 1
WikiSQL 1
Inter-GPS 10
Visual7W 3
TQA 10
RAVEN 1
Table16 | ResponseformattingpromptsforCambrianData
General Knowledge OCR&Chart Vision-Centric
#ofData
𝑡=150𝑘 4015k 53.74 1,512.3 67.0 68.3 61.2 73.4 35.1 34.3 62.4 44.6 39.1 58.5 38.5 30.0 55.2 61.98 54.7
𝑡=250𝑘 5218k 54.31 1,475.9 67.3 69.2 61.6 73.4 35.9 34.5 62.4 46.5 36.9 59.2 38.6 32.0 56.6 63.68 57.5
𝑡=350𝑘 5883k 54.27 1,461.9 66.2 68.9 61.6 73.8 36.4 32.8 62.5 46.8 38.3 59.3 39.3 31.3 54.9 62.68 60.4
𝑡=450𝑘 6383k 54.15 1,534.1 67.6 66.3 61.9 72.9 35.1 36.9 63.8 45.8 38.6 58.4 39.4 28.0 53.6 64.60 56.8
Table17 | AllBenchmarkResultsforDataBalancingExperiments
41
egarevA
PEMM BMM IDEES AQG IAQS
VUMMM
MatsiVhtaM
D2IA
AQtrahC
hcneBRCO
AQVtxeT AQVcoD
PVMM
AQdlroWlaeR
D2hcneB-VC D3hcneB-VCGeneral Knowledge OCR&Chart Vision-Centric
exp1 47.49 1,309.10 58.00 60.10 54.00 72.40 34.80 31.20 59.10 34.20 34.50 54.20 33.00 13.30 47.60 57.40 50.58
exp2 47.78 1,351.70 60.30 61.20 55.40 72.80 35.20 29.50 59.10 31.20 33.40 54.20 30.50 15.30 48.20 58.40 52.25
exp3 48.28 1,299.53 60.56 61.79 55.74 72.04 34.90 32.10 59.40 33.20 33.90 54.15 31.90 21.30 48.60 58.52 49.41
exp4 47.47 1,288.98 58.16 61.47 55.00 71.05 37.10 28.20 58.50 33.72 34.50 55.07 31.69 20.66 47.06 56.30 46.58
exp5 48.96 1,363.26 60.48 63.18 55.92 70.35 35.70 31.40 57.19 32.88 34.60 54.74 32.10 22.70 47.30 58.83 57.75
Table18 | AllBenchmarkResultsforDataRatioExperimentswithfixed1350kdata
Backbone Data Adapter InstructionTuning
Experiment LLM Vision Adapter InstructionTuning lr wd bs lr wd bs visionlr
0MAdapter+737KIT Vicuna-1.5-7B OpenAICLIPViT-L/14@336 0 737k - - - 2e-5 0 512 -
0.5MAdapter+737KIT Vicuna-1.5-7B OpenAICLIPViT-L/14@336 0.5M 737k 1e-3 0 512 2e-5 0 512 -
1.2MAdapter+737KIT Vicuna-1.5-7B OpenAICLIPViT-L/14@336 1.2M 737k 1e-3 0 512 2e-5 0 512 -
UnfreezeVision Vicuna-1.5-7B OpenAICLIPViT-L/14@336 1.2M 737k 1e-3 0 512 2e-5 0 512 1e-5
ModelEnsemble Vicuna-1.5-7B ChosenCombination 1.2M 737k 1e-3 0 512 2e-5 0 512
DataBalance Vicuna-1.5-7B OpenAICLIPViT-L/14@336 0 MixBasedonthree𝑡 - - - 2e-5 0 512 -
DataRatio Vicuna-1.5-7B OpenAICLIPViT-L/14@336 0 1350kBasedonRatio - - - 2e-5 0 512 -
LLaVA665K Vicuna-1.5-7B OpenAICLIPViT-L/14@336 0 LLaVA665K - - - 2e-5 0 512 -
Cambrian-10M(Data) Vicuna-1.5-7B OpenAICLIPViT-L/14@336 0 Cambrian-10M - - - 2e-5 0 512 -
Cambrian-7M(Data) Vicuna-1.5-7B OpenAICLIPViT-L/14@336 0 Cambrian-7M - - - 2e-5 0 512 -
Cambrian-1-8B Llama-3-Ins-8B SVAwith4encoders∗ 2.5M Cambrian-7M 1e-4 0 512 2e-5 0 512 -
Cambrian-1-13B Vicuna-1.5-13B SVAwith4encoders∗ 2.5M Cambrian-7M 1e-4 0 512 2e-5 0 512 -
Cambrian-1-34B Hermes-2-Yi-34BSVAwith4encoders∗ 2.5M Cambrian-7M 1e-4 0 512 2e-5 0 1024 -
Table 19 | Implementation details and hyperparameters for all experiments. ∗4 encoders
are: OpenAICLIPViT-L/14@336,SigLIPViT-SO400M/14@384,DINOv2ViT-L/14@518,Open-
CLIPConvNeXt-XXL@1024
Method Eval
Cambrian-18B FuzzyMatching 74.7 64.6 80.4 42.7 73.0 73.3 62.4 71.7 51.3 64.2
Cambrian-18B GPT3.5Grading 78.4 65.8 82.0 38.9 78.1 71.2 67.0 69.2 49.3 63.5
Δ +3.7 +1.2 +1.6 -3.8 +5.1 -2.1 +4.6 -2.5 -2.0 -0.7
Cambrian-113B FuzzyMatching 74.7 64.6 80.4 40.4 73.0 73.3 62.4 71.7 51.3 64.2
Cambrian-113B GPT3.5Grading 77.3 64.7 81.3 37.2 78.2 71.4 67.1 75.6 46.0 64.3
Δ +2.9 +0.4 +2.0 -3.2 +4.6 -2.4 +5.2 +2.8 +4.7 +1.3
Table20 | ComparisonbetweenFuzzyMatchingAccuracyandLLMJudgedAccuracy. Fuzzy
matchingandLLMrefereeyieldsimilaraccuraciesforthebenchmarksthatrequirematching.
42
egarevA
PEMM BMM IDEES AQG IAQS
VUMMM
IDEES
MatsiVhtaM
AQG
D2IA
IAQS
AQtrahC
UMMM
hcneBRCO
D2IA
AQtrahC
AQVtxeT
hcneBRCO
AQVcoD
AQVtxeT
PVMM
PVMM
AQdlrowlaeR
AQdlroWlaeR
D2hcneB-VC D3hcneB-VCLLM
Backbone SystemPrompt
Vicuna 1.5 A chat between a curious user and an artificial intelligence assistant. The
7B assistantgiveshelpful,detailed,andpoliteanswerstotheuser’squestions.
LLAMA-3 YouareCambrian,ahighlyintelligentmultimodalAItrainedbyNYUVision
8B X.AsamultimodalAI,youhavetheabilitytoprocessandanalyzeimages.
Wheneveranimageispresentintheconversation,verycarefullyexamineit
and consider its content when formulating your response.You should give
conciseresponsestoverysimplequestions,butprovidethoroughresponsesto
morecomplexandopen-endedquestions.
Nous-Yi34B YouareCambrian,ahighlyintelligentmultimodalAItrainedbyNYUVision
X.AsamultimodalAI,youhavetheabilitytoprocessandanalyzeimages.
Wheneveranimageispresentintheconversation,verycarefullyexamineit
andconsideritscontentwhenformulatingyourresponse. Youshouldgive
conciseresponsestoverysimplequestions,butprovidethoroughresponsesto
morecomplexandopen-endedquestions.
Table21 | LLMBackboneSystemPrompts
43Table22 | Listingthepromptsusedintheevaluationofeachbenchmark
Benchmark Prompt Example
AI2D \nAnswer with USER: <image>\nwhich of these define dairy
theoption’sletter item\n(A) c\n(B) D\n(C) b\n(D) a\nAnswer with
from the given theoption’sletterfromthegivenchoicesdirectly. AS-
choicesdirectly. SISTANT:
ChartQA \nAnswer the USER:<image>\nHowmanyfooditemisshownin
question using a thebargraph?\nAnswerthequestionusingasingle
single number or numberorphrase. ASSISTANT:
phrase.
DocVQA \nGive the short USER: <image>\nWhat is the dividend payout in
answerdirectly. 2012?\nGivetheshortanswerdirectly. ASSISTANT:
GQA \nAnswer the USER:<image>\nIsitovercast?\nAnswertheques-
question using tionusingsinglewordorphrase. ASSISTANT:
single word or
phrase.
MathVista \nFirst show USER:<image>\nwhatisthetotalvolumeofthemea-
your reasoning suringcup?\nFirstshowyourreasoningprocessand
process and then thengivethefinalanswer. ASSISTANT:
give the final
answer.
MM-Bench \nAnswer with USER: <image>\nWhich of the following was a
EN theoption’sletter dependent variable in this experiment?\n(A) co-
from the given coon\n(B) chrysalis\n(C) nan\n(D) nan\nAnswer
choicesdirectly. withtheoption’sletterfromthegivenchoicesdirectly.
ASSISTANT:
MME \nPlease answer USER:<image>\nIsapythoncodeshowninthepic-
the question us- ture? Pleaseansweryesorno.\nAnswerthequestion
ingasingleword usingasinglewordorphrase. ASSISTANT:
orphrase.
MMMU \nAnswer with USER:<image>\nWhatcausestheseunusualforma-
theoption’sletter tionsonMountainpapaya? Options:\nA.Abiotic\nB.
from the given Confused\nC.Biotic\nD.Normal\nAnswerwiththe
choicesdirectly. option’sletterfromthegivenchoicesdirectly. ASSIS-
TANT:
MMVP \nAnswer with USER: <image>\nAre the butterfly’s wings closer
theoption’sletter to being open or closed? Options:\n(a) Open\n(b)
from the given Closed\nAnswer with the option’s letter from the
choicesdirectly. givenchoicesdirectly. ASSISTANT:
OCRBench \nGive the short USER: <image>\nwhat is written in the im-
answerdirectly. age?\nGivetheshortanswerdirectly. ASSISTANT:
RealWorld \nAnswer the USER:<image>\nInwhichdirectionisthefrontwheel
QA question using of the car on the right side facing?\n\nA. Left\nB.
a single word or Straight\nC.Right\nAnswerthequestionusingasin-
phrase. glewordorphrase. ASSISTANT:
SQA-I \nAnswer with USER: <image>\nWhat is the name of the colony
theoption’sletter shown?\nA. Maryland\nB. New Hampshire\nC.
from the given Rhode Island\nD. Vermont\nAnswer with the op-
choicesdirectly. tion’s letter from the given choices directly. ASSIS-
TANT:
44Benchmark Prompt Example
SEED-I \nAnswer with USER: <image>\nHow many towels are in the im-
theoption’sletter age? Options:\nA. One\nB. Two\nC. Three\nD.
from the given Four\nAnswerwiththeoption’sletterfromthegiven
choicesdirectly. choicesdirectly. ASSISTANT:
Text-VQA \nAnswer the USER:<image>\nwhatisthetime?\nReferenceOCR
question using tokens: N,u,g0\nAnswerthequestionusingasingle
a single word or wordorphrase. ASSISTANT:
phrase.
ADE \nAnswer with USER:<image>\nConsideringtherelativepositions
theoption’sletter of the cushion and the sofa in the image provided,
from the given whereisthecushionlocatedwithrespecttothesofa?
choicesdirectly. Select from the following choices. \n(A) right\n(B)
left\nAnswerwiththeoption’sletterfromthegiven
choicesdirectly. ASSISTANT:
COCO \nAnswerwiththe USER:<image>\nHowmanytrainsareintheimage? Se-
option’sletterfrom lect from the following choices. \n(A) 3\n(B) 0 \n(C) 1
the given choices \n(D)2\n(E)4\nAnswerwiththeoption’sletterfromthe
directly. givenchoicesdirectly. ASSISTANT:
Omni3D \nAnswer with USER:<image>\nEstimatethereal-worlddistances
theoption’sletter betweenobjectsinthisimage. Whichobjectiscloserto
from the given thetrafficcone(highlightedbyaredbox),themotorcy-
choicesdirectly. cle(highlightedbyabluebox)orthebus(highlighted
byagreenbox)?\n(A)motorcycle\n(B)bus\nAnswer
withtheoption’sletterfromthegivenchoicesdirectly.
ASSISTANT:
45