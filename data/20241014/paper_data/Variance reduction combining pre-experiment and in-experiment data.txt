Variance reduction combining pre-experiment and
in-experiment data
Zhexiao Lin∗ Pablo Crespo†
Abstract
Online controlled experiments (A/B testing) are essential in data-driven decision-
making for many companies. Increasing the sensitivity of these experiments, particu-
larly with a fixed sample size, relies on reducing the variance of the estimator for the
average treatment effect (ATE). Existing methods like CUPED and CUPAC use pre-
experiment data to reduce variance, but their effectiveness depends on the correlation
between the pre-experiment data and the outcome. In contrast, in-experiment data is
oftenmorestronglycorrelatedwiththeoutcomeandthusmoreinformative. Inthispa-
per,weintroduceanovelmethodthatcombinesbothpre-experimentandin-experiment
data to achieve greater variance reduction than CUPED and CUPAC, without intro-
ducing bias or additional computation complexity. We also establish asymptotic theory
and provide consistent variance estimators for our method. Applying this method to
multiple online experiments at Etsy, we reach substantial variance reduction over CU-
PAC with the inclusion of only a few in-experiment covariates. These results highlight
the potential of our approach to significantly improve experiment sensitivity and accel-
erate decision-making.
Keywords: variance reduction, online controlled experiments, average treatment effect,
regression adjustment, in-experiment data.
1 Introduction
Online controlled experiments, commonly referred to as A/B testing, play an important role
in guiding data-driven decisions for companies across various industries (Kohavi et al., 2020).
These experiments enable organizations to evaluate the impact of new features, products,
and improvements to search and recommendation algorithms on their business performance
∗Department of Statistics, University of California, Berkeley, CA 94720, USA; e-mail:
zhexiaolin@berkeley.edu
†Etsy, Inc., Brooklyn, NY 11201, USA; e-mail: pcrespo@etsy.com
1
4202
tcO
11
]EM.tats[
1v72090.0142:viXraby analyzing data from A/B testing. By measuring the Average Treatment Effect (ATE)
using experimental data, A/B testing provides deep insights that drive product development,
improveuserexperiences, andincreasebusinessoutcomes. However, afundamentalchallenge
in A/B testing is to improve the sensitivity of the online experiments given a fixed sample
size, since increasing the sample size might be costly or even impractical. Under this sample
size constraint, reducing the variance of the ATE estimator is essential for detecting effects
more quickly and accelerating the experimental cycle.
To achieve variance reduction while maintaining the consistency of the estimator, a com-
monapproachistoconsiderregressionadjustmentusingauxiliarydata(Freedman,2008;Lin,
2013). In the context of online controlled experiments, CUPED (Controlled-experiment Us-
ing Pre-Experiment Data) (Deng et al., 2013, 2023b) was developed to reduce the variance
by incorporating pre-experiment data through linear regression. Building upon CUPED,
CUPAC (Control Using Predictions as Covariates) (Tang et al., 2020) was proposed by us-
ing complex machine learning prediction models instead of linear models and is now widely
adopted in industry. The key intuition behind these methods is to utilize auxiliary informa-
tion that correlates with the outcome to adjust for uncertainty in randomized experiments,
thereby making the ATE estimator more precise. The degree of variance reduction achieved
depends on the correlation between the auxiliary data and the outcome of interest. Con-
sequently, both CUPED and CUPAC are limited by the information within pre-experiment
data, which may not directly influence the final outcome as they are observed before the
experiment.
In contrast, in-experiment data—information gathered during the experiment that often
directly leads to the final outcome—exhibits strong correlations with the outcome, offering
greatpotentialforvariancereduction. ExistingmethodslikeCUPEDandCUPAConlyfocus
on pre-experiment data, leaving significant room for improvement by carefully incorporating
data generated during the experiment.
In this paper, we propose a novel approach that combines both pre-experiment and in-
experiment data to reduce the variance of the ATE estimator. By integrating in-experiment
data into the CUPAC framework, we achieve superior variance reduction while maintaining
simplicity and computational efficiency. Importantly, our approach does not introduce any
bias into the estimator. We then provide asymptotic theory and consistent variance estima-
tors for our method. Empirically, we show the effectiveness of our approach by applying it to
multiple online experiments conducted at Etsy. Using only a few in-experiment covariates,
our method achieves substantial variance reduction compared to CUPAC, indicating its high
efficacy in increasing the sensitivity of online controlled experiments.
The remainder of this paper is organized as follows. In Section 2, we introduce the
potential outcome framework and review existing variance reduction techniques, including
CUPED, CUPAC, and other related work. In Section 3, we the motivation and methodology
of our proposed approach, discussing both theoretical results and practical considerations,
2and providing an illustrative example to demonstrate its effectiveness. In Section 4, we
apply our method to real-world experiments at Etsy, showcasing its practical advantages
and superiority over existing methods. Finally, we conclude with discussions in Section 5.
All proofs are relegated to the appendix.
2 Problem setting and related work
2.1 Potential outcome framework
We adopt the potential outcomes framework (Rubin, 1974) and model our problem from a
superpopulation perspective. Although we focus on the binary treatment case, our method
and results can be easily generalized to multiple treatment cases.
Considernindependentandidenticallydistributed(i.i.d.) samples{(W ,X ,Y (1),Y (0))}n
i i i i i=1
drawn from a superpopulation with joint distribution (W,X,Y(1),Y(0)). In practice, we ob-
serve the dataset {(W ,X ,Y )}n , where Y = Y (W ) is the observed outcome corresponding
i i i i=1 i i i
to the assigned treatment. Here, W ∈ {0,1} is the treatment assignment indicator for the
i
i-th unit - W = 1 if the unit is assigned to the treatment group and W = 0 if assigned to
i i
the control group. The variables Y (1) ∈ R and Y (0) ∈ R represent the potential outcomes
i i
under treatment and control, respectively, for unit i. We incorporate auxiliary covariates
X ∈ Rd, which are used for regression adjustment to improve estimation efficiency. We
i
focus on the case where X are pre-experiment covariates. In online controlled experiments,
i
treatment assignments are typically generated through Bernoulli trials with a fixed proba-
bility p ∈ (0,1), ensuring that each unit has an equal and independent chance of receiving
the treatment. Formally, we have W ∼ Bernoulli(p) for all i.
i
Our primary objective is to estimate the ATE, defined as τ = E[Y(1)−Y(0)]. To ensure
identifiability of the ATE, we assume that the potential outcomes are independent of the
treatment assignment, i.e., (Y (0),Y (1)) ⊥⊥ W . Under these assumptions, the ATE can be
i i i
nonparametrically identified using the observed data: τ = E[Y(1)]−E[Y(0)] = E[Y(1)|W =
1]−E[Y(0)|W = 0] = E[Y |W = 1]−E[Y |W = 0].
For notation simplicity, let n =
(cid:80)n
W denote the number of units in the treatment
1 i=1 i
group and n =
(cid:80)n
(1 − W ) denote the number of units in the control group. The
0 i=1 i
sample means of the outcomes in each group are given by Y =
n−1(cid:80)
Y and Y =
n−1(cid:80)
Y . Similarly, the sample means of the
pre-experim1
ent
co1 variaW tei= s1 ini
each
gro0
up
ar0
e X
W =i= n0 −i
1(cid:80)
X and X =
n−1(cid:80)
X .
1 1 Wi=1 i 0 0 Wi=0 i
2.2 Review on CUPED and CUPAC
Based on the identification τ = E[Y |W = 1] − E[Y |W = 0], a natural estimator for the
ATE is the difference-in-means estimator, defined as τ =
n−1(cid:80)
Y
−n−1(cid:80)
Y .
(cid:98)DIFF 1 Wi=1 i 0 Wi=0 i
3This estimator is unbiased in finite samples. Under the central limit theorem, its asymptotic
√
distribution is n(τ −τ) −→d N(0,σ2 ), where the asymptotic variance σ2 is given
(cid:98)DIFF DIFF DIFF
by σ2 = p−1Var[Y |W = 1] + (1 − p)−1Var[Y |W = 0]. Let V(cid:100)ar[Y |W = 1] = (n −
DIFF 1
1)−1(cid:80) (Y −Y )2 and V(cid:100)ar[Y |W = 0] = (n −1)−1(cid:80) (Y −Y )2 denote the sample
Wi=1 i 1 0 Wi=0 i 0
variances within each group. Then the variance estimator σ2 = n(n−1V(cid:100)ar[Y |W = 1]+
(cid:98)DIFF 1
n−1V(cid:100)ar[Y |W = 0]) is a consistent estimator of σ2 , i.e., σ2 converges in probability to
0 DIFF (cid:98)DIFF
σ2 .
DIFF
To utilize pre-treatment data for variance reduction, we exploit the fact that the treat-
ment assignment W is independent of the pre-experiment covariates X, i.e., W ⊥⊥ X. This
independence ensures that regression adjustments using X do not introduce bias into the
estimator. The CUPED estimator applies linear regression adjustment to reduce variance.
The estimator is of the form τ = n−1(cid:80) (Y −θ(cid:98)⊤X )−n−1(cid:80) (Y −θ(cid:98)⊤X ), where
(cid:98)CUPED 1 Wi=1 i i 0 Wi=0 i i
θ(cid:98)∈ Rd is estimated by regressing Y on X using the entire dataset. Specifically, we predict Y
using a linear function of X: θ⊤X+θ , where θ ∈ Rd and θ ∈ R. The optimal θ minimizing
0 0
the mean squared error (MSE) is given by θ = (Var[X])−1Cov[X,Y]. The estimation error
due to using θ(cid:98)instead of θ is (θ−θ(cid:98))⊤(X −X ). Since θ(cid:98)−θ = o (1) due to the properties of
1 0 P
least squares estimation, and X −X = O (n−1/2) by E[X|W = 1] = E[X|W = 0], the es-
1 0 P
timation error is o (n−1/2), which is asymptotically negligible. Therefore, we have τ =
P (cid:98)CUPED
n−1(cid:80) (Y −θ⊤X )−n−1(cid:80) (Y −θ⊤X )+o (n−1/2). Although the CUPED estimator
1 Wi=1 i i 0 Wi=0 i i P
isnotgenerallyunbiasedinfinitesampleswithoutsamplesplitting, theasymptoticunbiased-
ness can be guaranteed by E[X|W = 1] = E[X|W = 0]. The asymptotic distribution of
√
the CUPED estimator is n(τ −τ) −→d N(0,σ2 ), where the asymptotic variance
(cid:98)CUPED CUPED
σ2 isσ2 = p−1Var[Y −θ⊤X|W = 1]+(1−p)−1Var[Y −θ⊤X|W = 0]. Weestimate
CUPED CUPED
the variances within each group as V(cid:100)ar[Y −θ⊤X|W = 1] = (n −1)−1(cid:80) (Y −θ(cid:98)⊤X −
1 Wi=1 i i
Y +θ(cid:98)⊤X )2 and V(cid:100)ar[Y −θ⊤X|W = 0] = (n −1)−1(cid:80) (Y −θ(cid:98)⊤X −Y +θ(cid:98)⊤X )2. Then
1 1 0 Wi=0 i i 0 0
the variance estimator is σ2 = n(n−1V(cid:100)ar[Y −θ⊤X|W = 1]+n−1V(cid:100)ar[Y −θ⊤X|W = 0]),
(cid:98)CUPED 1 0
which is consistent for σ2 .
CUPED
TheCUPACestimatorgeneralizesCUPEDbyallowingfornonlinearmodelsintheregres-
sionadjustment. Insteadofrestrictingtolinearfunctions,CUPACusesapotentiallycomplex
function f(X) to predict Y. The estimator is defined as τ =
n−1(cid:80)
(Y −f(cid:98)(X ))−
(cid:98)CUPAC 1 Wi=1 i i
n−1(cid:80)
(Y −f(cid:98)(X )), where f(cid:98)is an estimate of the optimal function f obtained by fitting a
0 Wi=0 i i
machine learning model to predict Y from X. We assume f(cid:98)is L -consistent in estimating f.
2
The estimation error in this case is
n−1(cid:80)
(f(X )−f(cid:98)(X
))−n−1(cid:80)
(f(X )−f(cid:98)(X )).
1 Wi=1 i i 0 Wi=0 i i
By the minimax theory (Tsybakov, 2009), the pointwise convergence rate of f(cid:98) estimating
f is generally slower than n−1/2. However, since X|W = 1 and X|W = 0 have the
same distribution, under the Donsker conditions, as long as f(cid:98) is L consistent, the esti-
2
mation error is o (n−1/2) (van der Vaart, 1998, Section 19). This assumption allows us
P
to avoid using sample splitting. Therefore, we have τ =
n−1(cid:80)
(Y − f(X )) −
(cid:98)CUPAC 1 Wi=1 i i
4n−1(cid:80) (Y − f(X )) + o (n−1/2). Similar to CUPED, the CUPAC estimator is not un-
0 Wi=0 i i P
biased in finite samples without sample splitting, but it remains asymptotically unbiased
because W ⊥⊥ X implies that E[f(X)|W = 1] = E[f(X)|W = 0] for any measurable
√
d
function f. The asymptotic distribution of the CUPAC estimator is n(τ − τ) −→
(cid:98)CUPAC
N(0,σ2 ), where the asymptotic variance σ2 is σ2 = p−1Var[Y −f(X)|W =
CUPAC CUPAC CUPAC
1] + (1 − p)−1Var[Y − f(X)|W = 0]. We estimate the variances within each group as
V(cid:100)ar[Y − f(X)|W = 1] = (n − 1)−1(cid:80) (Y − f(cid:98)(X ) − Y + n−1(cid:80) f(cid:98)(X ))2 and
1 Wi=1 i i 1 1 Wi=1 i
V(cid:100)ar[Y −f(X)|W = 0] = (n −1)−1(cid:80) (Y −f(cid:98)(X )−Y +n−1(cid:80) f(cid:98)(X ))2. Then the
0 Wi=0 i i 1 0 Wi=0 i
variance estimator is σ2 = n(n−1V(cid:100)ar[Y −f(X)|W = 1]+n−1V(cid:100)ar[Y −f(X)|W = 0]),
(cid:98)CUPAC 1 0
which is consistent for σ2 .
CUPAC
2.3 Related work
This paper considers the ATE estimation problem in randomized experiment setting, a topic
with a rich history in causal inference (Imbens and Rubin, 2015; Ding, 2024). To improve the
efficiency of the estimator using regression adjustment with pre-experiment data, Freedman
(2008) and Lin (2013) explored linear models. Cohen and Fogarty (2024), Guo et al. (2021)
and Jin and Ba (2023) investigated nonlinear models with sample splitting and cross fitting,
which is a common procedure in ATE estimation literature, such as double machine learning
(Chernozhukov et al., 2018). Specifically, Jin and Ba (2023) focused on optimal variance
reduction by using separate outcome models for the treatment and control groups.
In our approach, similar to CUPED and CUPAC, we utilize a single model for both
groups, although all subsequent results are applicable to the two-model case as well. The
CUPAC estimator we consider, as introduced by Tang et al. (2020), is implemented without
sample splitting, akin to the methods used by Lin et al. (2023) and Lin and Han (2022) for
estimating the ATE using observational data. Without sample splitting, ensuring that the
asymptoticbiasremainsnegligiblefortheestimator’sasymptoticnormalityoftennecessitates
the Donsker condition (Van der Laan et al., 2011; Cattaneo et al., 2023).
Recent studies have increasingly focused on leveraging in-experiment, or post-treatment,
data to improve the estimation of the ATE. One popular method involves using short-term
surrogates to estimate unobservable long-term treatment effects; see Athey et al. (2019) and
references therein. Another approach estimates delayed outcomes in online experimental
settings (Deng et al., 2023a). A key assumption in surrogate methods is that the treatment
and the outcome of interest are independent given the surrogates (Prentice, 1989). Addi-
tionally, some works have focused on principal stratification (Frangakis and Rubin, 2002;
Ding and Lu, 2017; Jiang et al., 2022), which also relies on strong assumptions due to the
fundamental problem of causal inference. In contrast, we adopt an alternative approach by
utilizing in-experiment data that are not affected by the treatment, thereby avoiding the
need for such strong assumptions.
53 Method
3.1 Motivation
In this section, we compare the asymptotic variances of the three methods, difference-in-
means, CUPED, and CUPAC—to understand the extent of variance reduction achieved by
CUPED and CUPAC, thereby motivating our proposed approach. The asymptotic variances
of these estimators depend linearly on the variances of their residuals: Var[Y |W] for the
difference-in-means, Var[Y −θ⊤X|W] for CUPED, and Var[Y −f(X)|W] for CUPAC. This
relationship indicates that the amount of uncertainty in Y explained by X directly influences
the proportion of variance reduction attainable by CUPED and CUPAC. When Y and X are
strongly correlated, the residual variance is low, leading to substantial variance reduction.
Conversely, when Y and X are independent, we have θ = 0 for CUPED and f(X) = E[Y] for
CUPAC, resulting in no variance reduction. This observation motivates the incorporation of
in-experiment data, as such data are generally more closely related to the outcome and can
potentially lead to greater variance reduction.
To understand the sources of uncertainty in Y, consider an unobserved variable U rep-
resenting inherent randomness or unmeasured factors affecting Y. Fixing the treatment
assignment W, we examine the relationships among U, X, Y, and W, as depicted in Fig-
ure 1.
X Y W
U
Figure 1: Relationships among U, X, Y, and W.
In Figure 1, the bold arrow from W to Y represents the treatment effect of interest. The
uncertainty in Y, given fixed W, arises from two sources. The first is the direct effect of U
on Y, representing random errors or unobserved covariates not captured by X. The second
is the indirect effect of U on Y through X, since Y depends on X via the outcome model,
and X itself varies due to random sampling from its marginal distribution. By regressing Y
on X, we effectively remove the dashed arrow from X to Y, thereby blocking the indirect
path from U to Y through X. This reduces the variance of the estimator by eliminating the
variability in Y that can be explained by X.
Now, we consider incorporating in-experiment data. Suppose we have i.i.d. samples
{(W ,X ,Z ,Y (0),Y (1))}n drawnfromthesuperpopulationwithjointdistribution(W,X,Z,Y(0),Y(1)).
i i i i i i=1
Here, Z ∈ Rm represents in-experiment covariates, which are observed after the treatment
6assignment and may depend on W. This means the treatment may influence the outcome
through Z. The relationships among U, X, Y, W, and Z are illustrated in Figure 2.
X Y W
U Z
Figure 2: Relationships among U, X, Y, W, and Z.
In Figure 2, the treatment effect consists of two components: the direct effect from W
to Y, and the indirect effect from W to Y mediated through Z. The uncertainty in Z arises
from random sampling of its conditional distribution given W. If we treat Z similarly to X
and apply regression adjustment using Z, we remove the dashed arrow from Z to Y, which
blocks the indirect path from W to Y through Z. This would eliminate part of the treatment
effect, resulting in a biased estimator of the ATE.
ToensurethatwecanreducetheuncertaintyfromU toY throughZ withoutintroducing
bias, we need to carefully consider the role of Z in the causal pathway. Specifically, we must
determine whether the indirect effect from W to Y through Z exists and whether adjusting
forZ isappropriateinthiscontext. Byproperlyincorporatingin-experimentcovariateswhile
accounting for their potential dependency on the treatment, we can leverage their stronger
correlation with the outcome to achieve greater variance reduction in the estimation of the
ATE.
3.2 Method overview
In the context of online controlled experiments, the treatment may affect the outcome only
throughcertainin-experimentcovariates. Ourkeyideaforcombiningpre-experimentandin-
experiment data is to select those in-experiment covariates that have no indirect effect on the
outcome and incorporate them into the CUPAC framework via regression adjustment. The
assumptionofnoindirecteffectcorrespondstomeanequivalenceinCUPEDanddistribution
equivalenceinCUPAC.WefocusonCUPEDbecauseitsassumptionisweaker, allowingusto
select more in-experiment covariates that satisfy the condition. Specifically, we first assume
that all in-experiment covariates Z satisfy the assumption E[Z|W = 0] = E[Z|W = 1].
To incorporate the pre-experiment data, we begin by predicting Y using X and ob-
tain a fitted model f(cid:98)(·) in the same manner as in CUPAC. Let the fitted residuals be
R(cid:98) = Y − f(cid:98)(X ), which capture the information from the pre-experiment data that is not
i i i
explained by X. To adjust for differences related to the in-experiment covariates, we regress
R(cid:98) on Z using a linear model and obtain the coefficient vector γ. Ideally, γ estimates
i i (cid:98) (cid:98)
7γ = (Var[Z])−1Cov[Z,Y −f(X)]. Our final estimator is defined as:
1 (cid:88) 1 (cid:88)
τ = (R(cid:98) −γ⊤Z )− (R(cid:98) −γ⊤Z )
(cid:98) i (cid:98) i i (cid:98) i
n n
1 0
Wi=1 Wi=0
1 (cid:88) 1 (cid:88)
= (Y −f(cid:98)(X )−γ⊤Z )− (Y −f(cid:98)(X )−γ⊤Z ).
i i (cid:98) i i i (cid:98) i
n n
1 0
Wi=1 Wi=0
The estimation error from the first step is the same as in CUPAC,
n−1(cid:80)
(f(X )−
1 Wi=1 i
f(cid:98)(X ))−n−1(cid:80) (f(X )−f(cid:98)(X )), which is o (n−1/2) under certain conditions. Let Z =
n−1(cid:80)i 0
Z ,
ZWi= =0 n−1i
(cid:80)
Zi
be the
samP
ple means of the in-experiment
covaria1
tes
1 Wi=1 i 0 0 Wi=0 i
for each group. The estimation error from the second step is (γ − γ)⊤(Z − Z ). Since
(cid:98) 1 0
E[Z|W = 1] = E[Z|W = 0], we have Z − Z = O (n−1/2). Although we do not have
1 0 P
access to the oracle residuals Y − f(X ), we can still establish the following proposition,
i i
provided that f(cid:98)estimates f accurately.
Proposition 1. We have γ −γ = O (∥f(cid:98)−f∥ )+o (1).
(cid:98) P 2 P
As long as f(cid:98)is L -consistent in estimating f, the second estimation error is o (n−1/2).
2 P
Therefore, our estimator becomes
1 (cid:88) 1 (cid:88)
τ = (Y −f(X )−γ⊤Z )− (Y −f(X )−γ⊤Z )+o (n−1/2).
(cid:98) i i i i i i P
n n
1 0
Wi=1 Wi=0
The estimator is asymptotically unbiased. Regarding the asymptotic distribution, we have
√
n(τ −τ) −→d N(0,σ2),
(cid:98)
where
σ2 = p−1Var[Y −f(X)−γ⊤Z|W = 1]+(1−p)−1Var[Y −f(X)−γ⊤Z|W = 0].
LetusdefinetheestimatedvarianceswithineachgroupV(cid:100)ar[Y −f(X)−γ⊤Z|W = 1] = (n −
1
1)−1(cid:80) (Y −f(cid:98)(X )−γ⊤Z −Y +n−1(cid:80) f(cid:98)(X )+γ⊤Z )2 andV(cid:100)ar[Y−f(X)−γ⊤Z|W =
Wi=1 i i (cid:98) i 1 1 Wi=1 i (cid:98) 1
0] = (n −1)−1(cid:80) (Y −f(cid:98)(X )−γ⊤Z −Y +n−1(cid:80) f(cid:98)(X )+γ⊤Z )2. Then,thevariance
0 Wi=0 i i (cid:98) i 0 0 Wi=0 i (cid:98) 0
estimator is σ2 = n(n−1V(cid:100)ar[Y −f(X)−γ⊤Z|W = 1]+n−1V(cid:100)ar[Y −f(X)−γ⊤Z|W = 0]),
(cid:98) 1 0
which is consistent for σ2.
3.3 An example
To illustrate the level of variance reduction achieved by our method, consider an additive
model for the outcome variable Y: Y = g(X) + h(Z) + τW + ϵ. Here, g(·) : Rd → R
and h(·) : Rm → R are functions of the pre-experiment covariates X and the in-experiment
covariates Z, respectively. We assume that X and Z are standardized such that Var[X] = I
d
and Var[Z] = I , where I and I are identity matrices of dimensions d and m. Let the
m d m
treatment assignment probability be p = P(W = 1) = 1/2. We also assume that E[ϵ] = 0
8and Var[ϵ] = σ2, representing the variance of the exogenous noise ϵ, which is independent of
ϵ
X and Z. Furthermore, due to the randomization in the experiment, (X,Z,ϵ) is independent
of W.
To predict Y using only the pre-experiment covariates X, the optimal model under the
MSE criterion is E[Y |X] = g(X)+E[h(Z)|X]+τp. We assume our function class for the
prediction model is sufficiently rich to include E[Y |X]. Therefore, we set f(X) = g(X)+
E[h(Z)|X]+τp. The coefficient γ used in our method is given by γ = (Var[Z])−1Cov[Z,Y −
f(X)] = Cov[Z,h(Z)+τW+ϵ−E[h(Z)|X]−τp] = Cov[Z,h(Z)−E[h(Z)|X]] = E[Cov[Z,h(Z)|X]].
Next, we compute the asymptotic variances of the different estimators. The asymptotic
variance of the difference-in-means estimator is σ2 = Var[g(X)+h(Z)+ϵ] = Var[g(X)+
DIFF
h(Z)]+σ2 = Var[g(X)+E[h(Z)|X]]+E[Var[h(Z)|X]]+σ2. For the CUPAC estimator, the
ϵ ϵ
asymptotic variance is σ2 = Var[g(X)+h(Z)+ϵ−f(X)] = Var[h(Z)−E[h(Z)|X]+ϵ] =
CUPAC
Var[h(Z) − E[h(Z)|X]] + σ2 = E[Var[h(Z)|X]] + σ2. For our proposed estimator, the
ϵ ϵ
asymptotic variance is σ2 = Var[g(X)+h(Z)+ϵ−f(X)−γ⊤Z] = Var[h(Z)−E[h(Z)|X]−
γ⊤Z+ϵ] = Var[h(Z)−E[h(Z)|X]−γ⊤Z]+σ2 = E[Var[h(Z)|X]]−∥E[Cov[Z,h(Z)|X]]∥2+
ϵ 2
σ2.
ϵ
The variance reduction achieved by CUPAC over the difference-in-means estimator is
σ2 −σ2 = Var[g(X)+E[h(Z)|X]], which represents the variance in Y that can be
DIFF CUPAC
explained by the pre-experiment covariates X. The additional variance reduction achieved
by our method over CUPAC is σ2 −σ2 = ∥E[Cov[Z,h(Z)|X]]∥2, capturing the variance
CUPAC 2
not explained by X but explainable by a linear combination of Z.
Animportantobservationisthatbothσ2 −σ2 andσ2 −σ2 areindependentof
DIFF CUPAC CUPAC
thenoisevarianceσ2 anddependonlyontherelationshipsbetweenX, Z, andthefunctionsg
ϵ
andh. Thisisintuitivesinceregressionadjustmentscannotreducevarianceduetoexogenous
noise ϵ. Consequently, the proportion of variance reduction 1 − σ2 /σ2 and 1 −
CUPAC DIFF
σ2/σ2 depends on the relative magnitude of the noise variance. If the noise variance
CUPAC
σ2 is large, the proportion of variance reduction achievable through regression adjustment
ϵ
diminishes.
To understand the effect of the correlation between X and Z on variance reduction, con-
sider two scenarios. First, if X and Z are independent, the variance reduction by CUPAC is
σ2 −σ2 = Var[g(X)]whichisthevarianceexplainedbyX alone. Theadditionalvari-
DIFF CUPAC
ancereductionachievedbyourmethodoverCUPACis: σ2 −σ2 = ∥Cov[Z,h(Z)]∥2. Here
CUPAC 2
σ2 −σ2 = Var[g(X)]isaquadraticfunctionalofg,andσ2 −σ2 = ∥Cov[Z,h(Z)]∥2
DIFF CUPAC CUPAC 2
is a quadratic functional of h. Since Z is more strongly correlated with the outcome than X,
we can imagine that h has a larger scale than g. Consequently, σ2 −σ2 will be larger
CUPAC
than σ2 −σ2 . For example, consider linear functions g(X) = β⊤X and h(Z) = β⊤Z,
DIFF CUPAC g h
where β ∈ Rd and β ∈ Rm. The vectors β and β represent the sizes of the effects
g h g h
of X and Z on the outcome, respectively. Then we have σ2 − σ2 = ∥β ∥2 and
DIFF CUPAC g 2
σ2 −σ2 = ∥β ∥2. This indicates that we can achieve a larger variance reduction with
CUPAC h 2
9our method if the effect of the in-experiment covariates Z is substantial. On the other hand,
if X and Z are strongly correlated such that E[h(Z)|X] = h(Z) for almost all X, then
σ2 −σ2 = Var[g(X)+h(Z)] and σ2 −σ2 = 0. In this case, our method does not
DIFF CUPAC CUPAC
provide additional variance reduction since X and Z share the same information, leaving no
residual variance after using CUPAC.
Next, we consider the effect of the complexity of the underlying model h of Z on variance
reduction. The variance reduction achieved by CUPAC over the difference-in-means estima-
tor, σ2 −σ2 , depends on E[h(Z)|X]. Therefore, the complexity of the model h has
DIFF CUPAC
no effect on this variance reduction as long as the conditional mean remains the same. How-
ever, the additional variance reduction achieved by our method over CUPAC, σ2 −σ2,
CUPAC
depends on Cov[Z,h(Z)|X]. This indicates that the level of variance reduction depends on
the projection of h(Z)|X onto the space spanned by Z|X. For instance, if h(Z) = β⊤Z for
h
someβ ∈ Rm, weachievethemaximumvariancereductionbecausetherelationshipbetween
g
h(Z) and Z is linear and directly captured by Z. Conversely, if h(Z)|X is orthogonal to the
space spanned by Z|X for almost all X, there is no variance reduction from our method.
3.4 In-experiment covariates selection
In practice, not all in-experiment covariates will satisfy the assumption required by CUPED,
i.e., the assumption that the covariate means are equal across treatment and control groups.
This is because the treatment may influence the outcome through certain in-experiment
covariates. Therefore, it is crucial to identify which in-experiment covariates satisfy the
assumption and can be safely used in our regression adjustment without introducing bias.
To identify such covariates from a set of potential candidates Z = (Z(1),Z(2),...,Z(m)),
we employ two-sample statistical tests for each in-experiment covariate. Specifically, for
each covariate Z(j), where j ∈ {1,2,...,m}, we test the null hypothesis H : E[Z(j)|W =
0
1] = E[Z(j)|W = 0] using data from both treatment and control groups. Let Z =
i
(Z ,Z ,...,Z ) denote the observed values of the covariates for unit i, where i = 1,...,n.
i1 i2 im
For each covariate Z(j), we perform a two-sample test using the observations Z : W = 1
ij i
from the treatment group and Z : W = 0 from the control group, and obtain the corre-
ij i
sponding p-value p . Given a significance level α ∈ (0,1), we select the covariates for which
j
the null hypothesis cannot be rejected, i.e., those with p > α. These covariates are consid-
j
ered to satisfy the assumption of equal means across treatment and control groups and are
included in our regression adjustment.
We can show that, asymptotically, all selected in-experiment covariates satisfy the as-
sumption.
Proposition 2. Let S ⊂ {1,2,...,m} denote the indices of covariates satisfying H , and
0 0
let S ⊂ {1,2,...,m} denote the indices of covariates violating H . Consider a significance
1 0
10level α ∈ (0,1). Assume that the two-sample tests used are consistent. Then,
lim P(p ≤ α,∀j ∈ S ) = 1.
j 1
n→∞
Proposition 2 holds for any choice of α ∈ (0,1), provided that the tests are consistent.
The finite-sample performance of this selection procedure depends on the chosen significance
levelα andtheactualdifferencesinmeansbetweenthetreatmentandcontrolgroupsforeach
covariate. To analyze the finite-sample performance, one approach is to consider local alter-
natives where the differences between the group means decrease as the sample size increases.
Forefficienttests, thepropositionstillholdsaslongasthemeandifferencesdecreaseatarate
slower than n−1/2. Empirically, we can leverage domain knowledge or visualize the empirical
distributions of the selected covariates across the two groups to validate the selection.
While the proposition ensures that the selected covariates satisfy the assumption asymp-
totically, a natural question arises: Can we guarantee that all covariates satisfying the as-
sumption are included in our selection? This issue relates to controlling the family-wise error
rate (FWER), which is the probability of making one or more Type I errors when performing
multiple hypothesis tests, i.e., P(p ≤ α,∃j ∈ S ). To address this, we can apply multiple
j 0
testing correction procedures, such as the Bonferroni correction or the Holm-Bonferroni
method, to adjust the p-values and control the FWER at a desired level. Furthermore, if
we desire the FWER to approach zero, we can let α tend to zero, allowing us to correctly
identify S in Proposition 2 with probability one asymptotically, thereby achieving selection
0
consistency. We show this fact on the next Proposition 3. See Shi et al. (2024) and the
references therein for further discussions on selection consistency.
Proposition 3. Using the same notation as in Proposition 2, and assuming that the two-
sample tests used are consistent for any α ∈ (0,1), we have
lim lim P({j : p > α} = S ) = 1.
j 0
α→0n→∞
3.5 Practical issues
One practical advantage of using in-experiment data over pre-experiment data is that in-
experimentdataareavailableforallusers, whereaspre-experimentdatamaynotbeavailable
for everyone. For instance, if we conduct an A/B testing on new users of the platform, there
is no pre-experiment data to leverage for variance reduction. In contrast, in-experiment data
can always be collected during the experiment, eliminating the need to handle missing data.
In many companies, the sample sizes for A/B testing are very large, and multiple ex-
periments may be running simultaneously on the platform. Training a separate predic-
tion model for each experiment, as required in CUPAC, can be both time-consuming and
resource-intensive. Acommonpracticeistoassumehomogeneityacrossdifferentexperiments
regarding the prediction model. Thus, a single prediction model is pre-trained using a large
11dataset that combines data from past experiments, utilizing the same set of pre-experiment
covariates, and then applied to all current experiments.
To align with this pipeline, we aim to preselect the same set of in-experiment covariates
for all experiments using data from past experiments. Since the distribution of in-experiment
covariates may differ across experiments, we conduct the statistical tests separately for each
experiment. Because covariates may have different p-values in different experiments, we
can combine the p-values across experiments using Fisher’s method from meta-analysis.
This approach enhances the accuracy of the selection procedure, allowing us to identify
insignificant in-experiment covariates using data from multiple past experiments.
As a generalization of our method, one might consider incorporating in-experiment data
usingnonlinearmodelsinsteadofthelinearmodelsweuse, orconstructingasingleprediction
model using both pre-experiment and in-experiment data together as inputs. Both general-
izations are valid, and all previous theoretical results can be applied. However, as we argued
earlier, the assumption required for CUPED is weaker than that for CUPAC since we only
need to test for mean equivalence. Therefore, we use a two-step procedure with a nonlinear
first step and a linear second step. Furthermore, by conducting the procedure in two steps,
we do not alter the prediction model used in CUPAC. This allows us to directly compare
our method with CUPAC and observe the improvement in variance reduction, providing a
clear advantage in enhancing the sensitivity of online controlled experiments.
4 Empirical studies
In this section, we apply our proposed method to 29 online experiments conducted at Etsy
over the course of one month, each involving a binary treatment. The primary outcome
of interest is the customer conversion rate. For the CUPAC pipeline, we utilized 117 pre-
experiment covariates and built a single prediction model using LightGBM, trained on com-
bined data from all experiments.
For the selection of in-experiment covariates, we observed that they are all count data
with a large proportion of zeros across our experiments. To address this, we first filtered
the in-experiment covariates by selecting those with sufficient non-zero values uniformly
across all experiments. We then employed the Mann–Whitney U test to assess distributional
equivalence between the treatment and control groups, as it is suitable for count data with
non-normal distributions. The p-values from different experiments were combined using
Fisher’s method, and we set a conservative significance level of 0.05, without adjusting for
the FWER. This process resulted in the selection of 23 in-experiment covariates for our
method.
We utilized two metrics to assess the level of variance reduction achieved by our method.
The first metric is the difference in the square roots of the R-squared values between our
methodandtheCUPACestimator. Thismetric, inspiredbytheasymptoticvariance, reflects
12the improvement in the predictive accuracy of the model. The second metric is the ratio of
the asymptotic variances, estimated using the variance estimators constructed earlier.
√
(cid:112)
Figure 3(a) presents the first metric, calculated as R2 − R2 , where R2 is the
CUPAC
R-squared of the combined model f(cid:98)(X)+γ⊤Z, and R2 is the R-squared of f(cid:98)(X) when
(cid:98) CUPAC
predicting the outcome Y. Our results indicate that the improvement in the square root of
R-squared ranges from as low as 0.02 to over 0.14 across the experiments, demonstrating an
improvement in predictive performance.
Figure 3(b) illustrates the second metric: the blue bars represent the variance reduction
achieved by CUPAC over the difference-in-means estimator, calculated as 1−σ2 /σ2 ,
CUPAC DIFF
and the orange bars represent the additional variance reduction achieved by our method over
CUPAC, calculated as 1−σ2/σ2 . These results demonstrate that our method achieves
CUPAC
greater or comparable variance reduction while using significantly fewer covariates—23 in-
experiment covariates compared to the 117 pre-experiment covariates used in CUPAC.
(a) Improvement in predictive accuracy by exper- (b) Proportion of variance reduction by experi-
iment ment
Figure 3: Variance reduction achieved by our method
5 Discussion
We have proposed a novel method that integrates both pre-experiment and in-experiment
data to achieve variance reduction in the estimation of the ATE in online controlled experi-
ments. Incorporating in-experiment covariates has shown significant potential for enhancing
variance reduction and expediting the completion of experiments. Unlike pre-experiment
covariates, in-experiment covariates are highly correlated with contemporaneous outcomes
and are available for all users, including those who are new to the platform and lack histor-
ical data. This universal availability makes in-experiment covariates more informative and
valuable for improving the precision of treatment effect estimates.
Moreover, the selection of in-experiment covariates in our study was conducted conserva-
tively and based on convenience. Despite this cautious approach, the empirical results were
13highly encouraging, demonstrating substantial variance reduction across multiple experi-
ments. By identifying effective in-experiment covariates through using domain knowledge or
machine learning techniques, even greater variance reduction may be achievable.
Ourmethodalsoofferspracticaladvantagesintermsofcomputationalefficiencyandscal-
ability. By using fewer covariates and avoiding the need to train complex prediction models
for each experiment, our approach is well-suited for large-scale A/B testing environments
where multiple experiments are conducted simultaneously.
Acknowledgments
This work was primarily conducted during the first author’s summer internship at Etsy. The
first author gratefully acknowledges the support from the Two Sigma PhD Fellowship. The
authors would like to thank Peng Ding for his insightful comments, and thank Stephane
Shao, Ami Wulf, Li Zhang, and Kelly McManus at Etsy for their valuable discussions. The
authors also would like to thank Julie Beckley and Kevin Gaan at Etsy for their support
throughout the internship program, and would like to thank all attendees of the first author’s
mid-term and final presentations for their comments and feedback.
References
Athey, S., Chetty, R., Imbens, G. W., and Kang, H. (2019). The surrogate index: Combining
short-term proxies to estimate long-term treatment effects more rapidly and precisely.
Technical report, National Bureau of Economic Research.
Cattaneo, M. D., Han, F., and Lin, Z. (2023). On rosenbaum’s rank-based matching esti-
mator. Biometrika, to appear.
Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., and
Robins, J. (2018). Double/debiased machine learning for treatment and structural param-
eters. The Econometrics Journal, 21(1):C1–C68.
Cohen, P. L. and Fogarty, C. B. (2024). No-harm calibration for generalized oaxaca–blinder
estimators. Biometrika, 111(1):331–338.
Deng,A.,Du,M.,Matlin,A.,andZhang,Q.(2023a). Variancereductionusingin-experiment
data: Efficient and targeted online measurement for sparse and delayed outcomes. In
Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining, pages 3937–3946.
14Deng, A., Hagar, L., Stevens, N., Xifara, T., Yuan, L.-H., and Gandhi, A. (2023b).
From augmentation to decomposition: A new look at cuped in 2023. arXiv preprint
arXiv:2312.02935.
Deng, A., Xu, Y., Kohavi, R., and Walker, T. (2013). Improving the sensitivity of online
controlled experiments by utilizing pre-experiment data. In Proceedings of the sixth ACM
international conference on Web search and data mining, pages 123–132.
Ding, P. (2024). A first course in causal inference. CRC Press.
Ding, P. and Lu, J. (2017). Principal stratification analysis using principal scores. Journal
of the Royal Statistical Society Series B: Statistical Methodology, 79(3):757–777.
Frangakis, C. E. and Rubin, D. B. (2002). Principal stratification in causal inference. Bio-
metrics, 58(1):21–29.
Freedman, D. A. (2008). On regression adjustments to experimental data. Advances in
Applied Mathematics, 40(2):180–193.
Guo, Y., Coey, D., Konutgan, M., Li, W., Schoener, C., and Goldman, M. (2021). Machine
learning for variance reduction in online experiments. Advances in Neural Information
Processing Systems, 34:8637–8648.
Imbens, G. W. and Rubin, D. B. (2015). Causal inference in Statistics, Social, and Biomed-
ical Sciences. Cambridge University Press.
Jiang, Z., Yang, S., and Ding, P. (2022). Multiply robust estimation of causal effects un-
der principal ignorability. Journal of the Royal Statistical Society Series B: Statistical
Methodology, 84(4):1423–1445.
Jin, Y. and Ba, S. (2023). Toward optimal variance reduction in online controlled experi-
ments. Technometrics, 65(2):231–242.
Kohavi, R., Tang, D., and Xu, Y. (2020). Trustworthy online controlled experiments: A
practical guide to a/b testing. Cambridge University Press.
Lin,W.(2013). Agnosticnotesonregressionadjustmentstoexperimentaldata: Reexamining
freedman’s critique.
Lin, Z., Ding, P., and Han, F. (2023). Estimation based on nearest neighbor matching: from
density ratio to average treatment effect. Econometrica, 91(6):2187–2217.
Lin, Z. and Han, F. (2022). On regression-adjusted imputation estimators of the average
treatment effect. arXiv preprint arXiv:2212.05424.
15Prentice, R. L. (1989). Surrogate endpoints in clinical trials: definition and operational
criteria. Statistics in medicine, 8(4):431–440.
Rubin, D. B. (1974). Estimating causal effects of treatments in randomized and nonrandom-
ized studies. Journal of Educational Psychology, 66(5):688–701.
Shi, L., Wang, J., and Ding, P. (2024). Forward screening and post-screening inference in
factorial designs. The Annals of Statistics, to appear.
Tang, Y., Huang, C., Kastelman, D., and Bauman, J. (2020). Control using predictions as
covariates in switchback experiments.
Tsybakov, A. B. (2009). Introduction to Nonparametric Estimation. Springer.
Van der Laan, M. J., Rose, S., et al. (2011). Targeted learning: causal inference for obser-
vational and experimental data, volume 4. Springer.
van der Vaart, A. W. (1998). Asymptotic Statistics. Cambridge University Press.
A Proofs
A.1 Proof of Proposition 1
Proof. Let Y =
n−1(cid:80)n
Y and Z =
n−1(cid:80)n
Z denote the sample means of Y and Z, re-
i=1 i i=1 i
spectively. Let P and P represent the population distribution and the empirical distribution
n
of X. For any measurable function f, we have Pf = E[f(X)] and P f = n−1(cid:80)n f(X ).
n i=1 i
By the definition of γ, we have
(cid:98)
n n
(cid:16)1 (cid:88) (cid:17)−1(cid:16)1 (cid:88) (cid:17)
γ = (Z −Z)(Z −Z)⊤ (Z −Z)(Y −f(cid:98)(X )−Y +P f(cid:98))
(cid:98) i i i i i n
n n
i=1 i=1
n n
(cid:16)1 (cid:88) (cid:17)−1(cid:16)1 (cid:88) (cid:17)
= (Z −Z)(Z −Z)⊤ (Z −Z)(Y −f(X )−Y +P f)
i i i i i n
n n
i=1 i=1
n n
(cid:16)1 (cid:88) (cid:17)−1(cid:16)1 (cid:88) (cid:17)
+ (Z −Z)(Z −Z)⊤ (Z −Z)(f(X )−f(cid:98)(X )−P f +P f(cid:98)) ,
i i i i i n n
n n
i=1 i=1
where the first term corresponds to the true coefficient γ, and the second term represents
the estimation error due to using f(cid:98)instead of f.
By the properties of least squares estimation, we have
n n
(cid:16)1 (cid:88) (cid:17)−1(cid:16)1 (cid:88) (cid:17)
(Z −Z)(Z −Z)⊤ (Z −Z)(Y −f(X )−Y +P f) = γ +o (1).
i i i i i n P
n n
i=1 i=1
16Next, we analyze the second term. By the law of large number, the inverse of the
covariance matrix (n−1(cid:80)n (Z −Z)(Z −Z)⊤)−1 = O (1).
i=1 i i P
Note that by the Cauchy-Schwarz inequality,
(cid:13)1 (cid:88)n (cid:13)2 (cid:16)1 (cid:88)n (cid:17)(cid:16)1 (cid:88)n (cid:17)
(cid:13) (Z −Z)(f(X )−f(cid:98)(X ))(cid:13) ≤ ∥Z −Z∥2 (f(X )−f(cid:98)(X ))2 .
(cid:13)n i i i (cid:13) n i 2 n i i
2
i=1 i=1 i=1
We have
n n
1 (cid:88) (cid:16) (cid:88) (cid:17)
∥Z −Z∥2 = Tr n−1 (Z −Z)(Z −Z)⊤ = O (1),
n i 2 i i P
i=1 i=1
and by P converges to P in distribution,
n
n
1 (cid:88)
(f(X )−f(cid:98)(X ))2 = ∥f(cid:98)−f∥2 +(P −P)[(f(cid:98)−f)2] = ∥f(cid:98)−f∥2 +o (1).
n i i 2 n 2 P
i=1
Therefore,
n n
(cid:16)1 (cid:88) (cid:17)−1(cid:16)1 (cid:88) (cid:17)
(Z −Z)(Z −Z)⊤ (Z −Z)(f(X )−f(cid:98)(X )) = O (∥f(cid:98)−f∥ )+o (1).
i i i i i P 2 P
n n
i=1 i=1
Additionally, we note that
n
1 (cid:88)
(Z −Z)(P f(cid:98)−P f) = 0.
i n n
n
i=1
Combining all the above results, we conclude that
γ −γ = O (∥f(cid:98)−f∥ )+o (1).
(cid:98) P 2 P
This completes the proof.
A.2 Proof of Propositions 2 and 3
Proof. We first prove Proposition 2. We have
(cid:88)
P(p ≤ α,∀j ∈ S ) = 1−P(p > α,∃j ∈ S ) ≥ 1− P(p > α).
j 1 j 1 j
j∈S1
Since the tests are consistent, for any j ∈ S , we have lim P(p > α) = 0. As the set
1 n→∞ j
S is finite (with cardinality at most m), it follows that
1
lim P(p ≤ α,∀j ∈ S ) = 1.
j 1
n→∞
This completes the proof of Proposition 2.
Next, we prove Proposition 3. For any test with significance level α, we have
(cid:88)
limsupP(p ≤ α,∃j ∈ S ) ≤ limsup P(p ≤ α) ≤ |S |α = mα.
j 0 j 0
n→∞ n→∞
j∈S0
17Similarly, from the consistency of the tests, we have
limsupP(p > α,∃j ∈ S ) = 0.
j 1
n→∞
Combining these results, we obtain for any α ∈ (0,1),
liminfP({j : p > α} = S ) ≥ 1−limsupP(p ≤ α,∃j ∈ S )−limsupP(p > α,∃j ∈ S ) ≥ 1−mα.
j 0 j 0 j 1
n→∞ n→∞ n→∞
Finally, by letting α → 0, we have
lim lim P({j : p > α} = S ) = 1.
j 0
α→0n→∞
This completes the proof of Proposition 3.
18