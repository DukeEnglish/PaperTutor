ZERO-SHOT PUPIL SEGMENTATION WITH SAM 2: A CASE
STUDY OF OVER 14 MILLION IMAGES
VirmarieMaquiling* SeanAnthonyByrne*
Human-CenteredTechnologiesforLearning DipartimentodiElettronica,InformazioneeBioingegneria
TechnicalUniversityofMunich PolitecnicodiMilano
MunichGermany PiazzaLeonardodaVinci
virmarie.maquiling@tum.de MilanoItaly
seananthony.byrne@polimi.it
DiederickC.Niehorster
LundUniversityHumanitiesLab&Dept. ofPsychology
LundUniversity
Lund,Swedendiederick_c.niehorster@humlab.lu.se
MarcoCarminati EnkelejdaKasneci
DipartimentodiElettronica,InformazioneeBioingegneria Human-CenteredTechnologiesforLearning
PolitecnicodiMilano TechnicalUniversityofMunich
MilanoItaly MunichGermany
marco.carminati1@polimi.it enkelejda.kasneci@tum.de
October14,2024
ABSTRACT
WeexplorethetransformativepotentialofSAM2,avisionfoundationmodel,inadvancinggazeesti-
mationandeyetrackingtechnologies. Bysignificantlyreducingannotationtime,loweringtechnical
barriersthroughitseaseofdeployment,andenhancingsegmentationaccuracy,SAM2addresses
criticalchallengesfacedbyresearchersandpractitioners. Utilizingitszero-shotsegmentationcapa-
bilitieswithminimaluserinput—asingleclickpervideo—wetestedSAM2onover14millioneye
imagesfromdiversedatasets,includingvirtualrealitysetupsandtheworldslargestunifieddataset
recordedusingwearableeyetrackers. Remarkablyinpupilsegmentationtasks,SAM2matchesthe
performanceofdomainspecificmodelstrainedsolelyoneyeimages,achievingcompetitivemean
IntersectionoverUnion(mIoU)scoresofupto93%withoutfine-tuning. Additionally,weprovide
ourcodeandsegmentationmasksforthesewidely-useddatasetstopromotefurtherresearch.
1 Introduction
The increasing integration of eye tracking into technologies like virtual reality (VR) devices and smart
glasses[Byrneetal.(2024)]hasamplifiedthedemandforrobustgazeestimationsystems,whereakeytaskistheaccurate
localizationofthepupilwithinanimageorvideo[Kimetal.(2019),Maquilingetal.(2024)]. Traditionalmethodsfor
pupillocalization—includingthresholdingandcenterofmasscalculations[Nyströmetal.(2023),Shortisetal.(1994),
Perézetal.(2003)]andellipse-fittingalgorithms[Santinietal.(2018a),Santinietal.(2018b)]—whileeffectiveincon-
trolled environments, suffer catastrophic errors in the presence of noise such as occlusions or reflections, limit-
ing their utility in real-world settings[Kotharietal.(2022)]. To overcome these limitations, deep learning-based
approaches have emerged as powerful alternatives, addressing issues plaguing traditional methods like blinks
or reflections[Kimetal.(2019)] and improving the robustness and accuracy of pupil detection under challenging
conditions[Fuhletal.(2016a)]. However,deployingthesemodelsrequiresvastamountsofannotateddataandtech-
4202
tcO
11
]VC.sc[
1v62980.0142:viXraAPREPRINT-OCTOBER14,2024
Single mouse click
SAM 2 Model inference throughout entire video
Figure1:AnillustrationdemonstratingthedataannotationprocesswithSAM2:theuserprovidesasinglepointprompt
viaamouseclick,andSAM2automaticallyhandlestherestofthesegmentationprocess. Optionally,theusercan
refineandaddadditionalpromptsinmoredifficultareasofthevideotoimprovethemodel’soutput.
nicalexpertise. Dataannotationcanbeverycostly,requiringsignificanthumanlabordependingondatasetsizeand
complexity, and training gaze estimation models using supervised machine learning relies heavily on labeled data
[Sunetal.(2017),Sambasivanetal.(2021)].
Foundationmodelsrepresentaparadigmshiftinartificialintelligence,transforminghowpeopleinteractwith,develop,
anddeploydeeplearningmodels[Bommasanietal.(2021)]. Characterizedbyvastnumbersoftrainableparametersand
extensivetrainingdata,thesemodelsdemonstrateimpressiveadaptabilitytodownstreamtasksandperformwellondata
distributionstheyhavenotencounteredduringtraining[Bommasanietal.(2021),Kirillovetal.(2023)]. Theyhavelow-
eredbarrierstoentryforintegratingAIintoworkflows,simplifyingtheuseofAI-poweredtoolsandhandlingcomplex
tasksthatoncerequiredspecializedmodelsandcustomdatasets[Bommasanietal.(2021),Zhouetal.(2023)]. Building
ontheseadvancements,Maquilingetal.[Maquilingetal.(2024)]showcasedthepotentialofzero-shotvisionfoundation
modelsinannotatingeyetrackingdatabyevaluatingtheSegmentAnythingModel(SAM)[Kirillovetal.(2023)],a
visionfoundationmodelreleasedbyMetaAI,ontheOpenEDSdatasets[Garbinetal.(2019),Palmeroetal.(2020)].
However,SAMrequiredatleastonepromptperimage,necessitatingmanualclicksoneveryimageinthedataset—a
time-consumingprocess. Itssuccessor,SAM2[Ravietal.(2024)],addressesthislimitationbyenablingasingleprompt
to propagate across an entire video, allowing the model to track and segment objects even with occlusions. This
improvementdrasticallyreducestheneedformanualinteraction,makingtheannotationprocesssignificantlymore
efficient.
ThiscasestudyexploresthepotentialofSAM2[Ravietal.(2024)],inadvancinggazeestimationresearch. Importantly,
itaddresseslong-standingchallengesingazeestimation:theneedforlargeannotateddatasets,thelabor-intensiveprocess
offeatureannotation,thehighbarrierofexpertisefordevelopingcustommodels,andthedifficultyofdomainadaptation
wheremodelsstruggletogeneralizeacrossdatasets[Kimetal.(2019),Kotharietal.(2022),Byrneetal.(2023)]. This
feature is particularly relevant in gaze estimation, where models often fail due to variations in differences across
participantphysiology,recordingsetups,andenvironmentallightingconditions[Kimetal.(2019),Kotharietal.(2022),
Byrneetal.(2023)]. To evaluate SAM 2, we deployed the model across diverse gaze estimation datasets, includ-
ing VR environments and the world’s largest unified public dataset of eye images captured with head-mounted
devices[Fuhletal.(2021)]. ToevaluateanddemonstrateSAM2’seaseofuse,welimitedtheannotationtojustone
clickpervideo,regardlessofitslength. Aframewherethepupilwasclearlyvisiblewasselected,andasinglepoint
promptwasplacednearthecenterofthepupil. FortheOpenEDS2019dataset[Garbinetal.(2019)],whichconsistsof
non-sequentialeyeimages,weappliedasingleprompttotheentiredatasetbytakingthepromptfromthefirstimage
inthetrainingsetandpropagatingittothetestandvalidationsets,coveringatotalof152differentparticipants. We
thenassessSAM2’ssegmentationperformanceusingthreekeymetrics: (1)theintersection-over-union(IoU)ofthe
pupilmaskstoassesssegmentationaccuracy; (2)theratioofframeswherethepupilwasnotsuccessfullytracked
(when the pupil was visible) to the total number of frames (referred to as Pupil Lost); and (3) the ratio of frames
whereablinkiscorrectlydetected(i.e.,thepredictedmaskisempty)tothetotalnumberofframeswheretheground
truthpupilmaskisempty(referredtoasBlinkDetected). OurannotationprocessemployedSAM2’ssmallestmodel,
SAM_hiera_tiny,adaptingthecodereleasedbySAM2’sauthors[Ravietal.(2024)]sothatitcouldhandlearbitrarily
longvideoswithoutpreprocessingorrunningintomemoryissues. ThisimprovedversionofSAM2usedforthis
paperisavailablefromhttps://github.com/dcnieho/segment-anything-2whiletheresultingmaskscanbe
downloadedfromhttps://zenodo.org/records/13911636.
2APREPRINT-OCTOBER14,2024
Segment Anything Model 2 (SAM 2)
Mask
Image Memory decoder Memory Memory
encoder attention encoder bank
Prompt eP nr co om dp et r
A traditional segmentation model Segment Anything Model (SAM)
Features Image Mask
encoder decoder
Encoder Decoder
Prompt
Prompt encoder
Specialized model trained on eye
tracking datasets
Figure2: ComparisonbetweenSegmentAnythingModel2(top),atraditionalsegmentationmodeltrainedspecifically
oneyetrackingdatasets(left),andSegmentAnythingModel(right). ThesampleeyeimagesaretakenfromtheGW
dataset[Kotharietal.(2020)].
2 Methodology
We evaluated SAM 2’s performance on a diverse set of eye tracking datasets to test its generalizability across var-
ious domains. These datasets include both VR-based and mobile eye tracking environments, representing con-
trolledandreal-worldsettings. Specifically,weselectedfourVR-baseddatasets(includingonesynthetic)andthree
mobile eye tracking datasets captured in natural, uncontrolled environments. Two of these datasets are from the
OpenEDS challenges[Garbinetal.(2019), Palmeroetal.(2020)], which focus on creating generalizable and robust
semanticsegmentationswithinVRsettings. ThesyntheticNVGaze[Kimetal.(2019)]datasetincludesitsownpupil
segmentations for gaze estimation. For the remaining datasets, ground truth segmentations were sourced from
TEyeD[Fuhletal.(2021)],theworld’slargestunifiedpublicdatasetofeyeimagestakenwithhead-mounteddevices.
Belowisabriefdescriptionofthedatasetsused:
1. OpenEDS2019[Garbinetal.(2019)]: Contains12,759non-sequentialimages(400×640pixels)acquired
from152participantsusingaVRhead-mounteddisplay(HMD)witheye-facingcamerasat200Hzunder
controlledlighting. Providespixel-levelannotationsforthepupil,iris,andsclera.
2. OpenEDS2020[Palmeroetal.(2020)]: Featureseye-imagesequencesfrom80participantsusingaVRHMD
at100Hz. TheEyeSegmentationDatasetincludes200sequencessampledat5Hztotallingto29,500images,
ofwhich5%aremanuallyannotated(640×400pixels).
3. NVGaze[Kimetal.(2019)]: Comprisestwodatasetsfornear-eyegazeestimationunderinfraredillumination.
Thereal-worlddatasetincludes264,279images(640×480pixels)from14participantsinaVRsetting;the
syntheticdatasetcontains2millionimages(1280×960pixels). WeevaluatedSAM2onboth.
4. LabelledPupilsintheWild(LPW)[Tonsenetal.(2016)]: Consistsofvideosfrom22participantsrecorded
ineverydayenvironmentsusingahead-mountedeyetrackerat120Hz(640×480pixels),coveringdiverse
lightingconditionsandnaturalgazedistributions.
5. Gaze-in-Wild(GW)[Kotharietal.(2020)]: Providesnaturalisticrecordingsfrom19participantsperforming
everydaytaskswithamobileeyetrackerat120Hz(640×480pixels),includingeyeandheadmovements,
infraredeyeimages,andsceneimagery.
6. Dikablis datasets: A combination of datasets from ElSe [Fuhletal.(2016b)], ExCuSe [Fuhletal.(2015)],
PNET[Fuhletal.(2016a)],andadrivingstudy[Kasnecietal.(2014)],compiledinTEyeD[Fuhletal.(2021)].
Recordedat25Hz(384×288pixels),itfeatureseyerecordingsfrom30participants.
3APREPRINT-OCTOBER14,2024
Figure 3: SAM 2 results on various VR-(first and second rows) and mobile eye tracking (third and last rows)
datasets. ImagesaretakenfromtheOpenEDS2019[Garbinetal.(2019)],OpenEDS2020[Palmeroetal.(2020)],LPW
[Tonsenetal.(2016)], andtheDikablisdatasets[Fuhletal.(2021)]. SAM2handledocclusionsremarkablywellas
observedinrows1and4,andeffectivelysegmentedthepupilacrossawiderangeofdatasets,showingitsrobustnessto
differenteyetrackingconditions.
3 Results
OpenEDS EllSeg-Gen TEyeD SAM2
MeanIoU MeanIoU MeanIoU MeanIoU PupilLost BlinkDetected
OpenEDS2019 0.9528 0.956 – 0.8997 0.0075 0.9412
OpenEDS2020 0.9517 – – 0.9233 0.0104 0.9375
NVGaze(Synthetic) – 0.982 – 0.9259 0.0028 0.9602
NVGaze(Real) – – 0.65 0.9079 0.0876 0.9889
LPW – – * 0.9023 0.0940 0.7127
GW – – * 0.9212 0.0184 0.8513
Dikablis – – * 0.8835 0.1846 0.9483
Table1: PerformanceofSAM2onmultipledatasetscomparedtoleaderboardscorespublishedinOpenEDS2019and
OpenEDS2020challengepages,andbaselineresultfromTEyeDusingleave-one-outcrossvalidationforeacheye
tracker. Theasterisk(*)indicatesdatasetswhereasinglebaselinevaluewasreportedforallfourdatasets. AsTEyeD
providedthegroundtruthsegmentationforNVGaze(real),LPW,GW,andtheDikablisdatasets,nobaselinemeanIoU
couldbeextractedfromtheoriginalpapers. Similarly,nobaselineIoUwasreportedfortheNVGazesyntheticdataset,
therefore,weextractedthemIoUscorefrom[Kotharietal.(2022)]whoevaluatedtheirmodelonNVgaze. TheirmIoU
scoreforOpenEDS2019hasbeenincludedaswell.
Table1summarizesSAM2’sperformancemetrics—meanIoU,pupillostrate,andblinkdetectionrate—compared
to the leaderboard scores1,2 of OpenEDS and the baseline score of TEyeD. For instance, on the OpenEDS2019
dataset,SAM2achievedameanIoUof89.97%,slightlylowerthanspecialistmodelstrainedonOpenEDS,which
reached at most 95.6%, with a pupil lost rate of 0.75% and a blink detection rate of 94.12% [Garbinetal.(2019)].
Similarly, forOpenEDS2020, SAM2attainedameanIoUof92.33%, comparedtothespecialistmodel’s95.17%
[Palmeroetal.(2020)].
OntheNVGazedatasets[Kimetal.(2019)],SAM2performedwellonsyntheticdatawithameanIoUof92.59%and
onrealdatawith90.79%,despiteahigherpupillostrateof8.76%ontherealdata. Inmobiledatasets,SAM2achieved
meanIoUsof90.23%onLPW[Tonsenetal.(2016)]and92.12%onGW[Kotharietal.(2020)],withhigherpupillost
ratesduetoincreasednoiseandvisualobstructions,asevidencedbytheDikablisdatasets’pupillostrateof18.46%
[Fuhletal.(2016b),Fuhletal.(2015),Fuhletal.(2016a),Kasnecietal.(2014),Fuhletal.(2021)].
Overall,SAM2[Ravietal.(2024)]performedwellonbothVRandmobileeyetrackingdatasets,withVRdatasets
showinghigherperformancelikelyduetomorecontrolledenvironments. AlthoughSAM2slightlyunderperformedon
thenonsequentialOpenEDS2019dataset—composedofindividualimagesratherthancontinuousvideoframes—itstill
1https://eval.ai/web/challenges/challenge-page/353/leaderboard/1002
2https://eval.ai/web/challenges/challenge-page/603/leaderboard/1680
4APREPRINT-OCTOBER14,2024
demonstratedtheabilitytogeneralizeacrossmultipledatasets. Notably,SAM2outperformedTEyeD’stop-performing
model, whichachievedameanIoUof just 65%[Fuhletal.(2021)], anddeliveredcompetitiveresultscomparedto
[Kotharietal.(2022)], whereameanIoUof98.2%wasreportedonNVGaze’ssyntheticdataset. Itisimportantto
highlightthattheirmodelwastrainedonseveraldatasets,includingNVGaze’strainingset,beforebeingtestedon
theNVGazetestset. Incontrast,SAM2achievedtheseresultswithoutanyfine-tuningorsacrificingasubsetofthe
datasetsfortraining.
AmajoradvantageofSAM2isitsminimalhumanguidancerequirement—justasingleclickpervideotoindicatethat
itshouldsegmentthepupil—comparedtotraditionalmanualannotationthatwouldrequirethousandsofclicks. For
example,SAM2requiredonlyoneclickfortheentireOpenEDS2019datasetof12,759images. Similarreductions
wereobservedacrossotherdatasets: 200clicksforthe29,500imagesoftheOpenEDS2020dataset, 66clicksfor
LPW’s130,856images,54clicksforNVGaze’s2,264,279images,423clicksfortheDikablisdatasets’morethan
5.6millionimages,and148clicksforGW’s6millionimages. Thissignificantreductioninhumanlaborhighlights
SAM2’sefficiencyinannotatinglargedatasetswithminimalinputwhileachievinghighperformanceinpupiltracking
without model fine-tuning or specialized training. Moreover, SAM 2’s inference process did not require powerful
GPUs,makingitfeasibleforusewithlow-endhardware. ThislevelofaccessibilitywasnotpossiblebeforeSAM2and
demonstrateshowvisionfoundationmodelscandemocratizelarge-scaledataannotationineyetrackingresearch.
4 Discussion
ThequalityofthedatasetsignificantlyimpactsSAM2’sperformance. Forinstance,datasetswithcleareyeimages,
minimalnoiseandhighresolution,suchasOpenEDSandNVGaze,yieldedhigherIoUscoresandlowerpupilloss
rates. However,SAM2encounteredmoredifficultyinnoisierdatasets,liketheDikablisdatasets,resultinginmore
pupiltrackingfailures.
Intermsofhumaninteraction,theeffortrequiredismostlylimitedtomonitoringthequalityofthepredictedmasks
andaddingadditionalpromptsonlywhennecessary. Whilewelimitedourevaluationtoasinglepointpromptper
video,SAM2supportsvariousotherprompttypes,suchasmultiplepositiveornegativepointprompts(wherethe
signsindicateareasthatSAM2shouldandshouldnotincludeinitssegmentation)andboundingboxprompts,offering
flexibilityformorecomplexandnoisierdatasetsthatrequireadditionalguidance. Belowwehighlightseveralpractical
lessonsfromconductingourstudyforresearchersinterestedinimplementingSAM2foreyetrackingdatasegmentation
tasks:
4.1 PracticalLessonsLearned
1. SignificantTimeSavings: SAM2significantlyreducedthetimenecessarytoannotateentiredatasets. The
datasets in this study were annotated by two of the authors within a couple of days, demonstrating SAM
2’sefficiencyinannotatinglargevolumesofdataquickly. Importantly,theseauthorsspentmostofthetime
waitingforthemodeltofinishproducingitssegmentationforthedatasets,andonlyverylittletimesettingup
themodelandcheckingitsoutput.
2. ReducedTechnicalBarriertoEntry: SAM2notonlysavestimeinobtainingsegmentationmasksbutalso
greatlysimplifiestheprocessfornon-experts. Insteadofdevelopingcustompupilsegmentationmodels,users
canrunSAM2withjustafewlinesofcode. Thislowersthebarriertoentry,enablingmorepeopletodevelop
gazeestimationpipelines.
3. MinimalHumanInteraction:Theannotationprocessrequiredminimalhumaninvolvementbeyondpreparing
thepromptsandfindingappropriateframeswherethepupilisvisible. SAM2handledtheactualannotation,
whiletheuseronlyneedstoperformqualitychecksandrefinepromptswhennecessary.
4. Dealing with Noisy Data While SAM 2 performed well in most cases, a decrease in performance was
observedinnoisierdatasets. Tomitigatethis,furtherrefiningofprompts(e.g. usingamoreappropriateprompt
strategyoraddingpromptsonmoredifficultframes)isnecessary–althoughthisstillinvolveslesshumaneffort
comparedtotraditionalannotationprocesses.
5. Low Hardware Requirements SAM 2’s low GPU requirements make it accessible for researchers with
limitedcomputationalresources. Weusedbothahigh-endNVIDIAA100(80GBVRAM)andaGeForceRTX
4090(24GBVRAM),achievingcomputetimesofupto40framespersecond(fps)forboth. Impressively,
SAM2alsoranonmorebudget-friendlyGPU’ssuchastheRTX4060Ti(16GBVRAM)deliveringaround
12fps,andwasevenfunctionalonlaptop-classGPU’s,albeitatsignificantlylowerframeratesofjustafew
fps. However,itisworthnotingthatinferenceperformanceofthemodelappearedtobelimitedbyCPUand
notGPUperformanceinmostofthesecases.
5APREPRINT-OCTOBER14,2024
6. StrongGeneralizationSAM2demonstratedrobustperformanceacrossadiversesetofeyetrackingdatasets
includingVR,mobile,andevensyntheticdata,despitenotbeingspecificallytrainedoneyetrackingdata.
4.2 OpenChallenges,Limitations&FutureWork
WhileSAM2excelledinpupilsegmentation,challengesremainwithsegmentinglessdistincteyefeaturesliketheiris
andsclera. Toexplorethis,weevaluatedSAM2’sperformanceontheOpenEDS2020dataset,whereitachievedan
mIoUof76.53%fortheirisandonly7.36%forthesclerawithasingleboxprompt. Fine-tuningSAM2onspecific
eyefeatures,especiallyundervaryingconditionslikelighting,reflections,andnoise,couldimproveperformanceby
reducingitsrelianceonidealconditionsandsimpleprompts.
Additionally,alternativepromptingstrategies,suchasusingboundingboxesormultiplepointprompts,mayyieldbetter
resultsinchallengingcases. Alimitationofourstudyisthatwedidnotexploredifferentpromptstrategies,opting
forasinglepointprompttohighlightthesimplicityofannotationwithSAM2. Whileasinglepromptprovedtobe
effectiveforpupilsegmentationinmanycases,otherpromptsmayimproveresultsindifficultcasesorforlesswell
definedfeatures.
AnotherconsiderationisthepossibilitythatSAM2mayhaveencounteredsimilarimagesduringtrainingasallthe
datasetswehaveusedareopentothepublic. FutureworkshouldtestSAM2oncompletelynoveldatasetstovalidateits
generalizationcapabilities. Additionally,aseyetrackingmovestoconsumerdevices,akeychallengewillbeadapting
SAM2forlow-powerhardwarelikesmartglasses[Zhangetal.(2023)],makingitcrucialtobalanceperformancewith
reducedcomputationalrequirementsforreal-timeapplicationsinVR,ARdevices.
5 Conclusion
Inthisstudy,weassessedthepracticalsegmentationcapabilitiesoftheSAM2VisionFoundationmodel. UsingSAM
2,weefficientlyannotatedover14millionpupilimagesacrossmultipledatasetswithjustafewclickpromptsper
dataset,significantlystreamliningtraditionalannotationworkflows. Ourfindingsshowthatfoundationmodelslike
SAM2effectivelyaddresskeychallengesineyetrackingresearch: dataannotation,domainadaptation,andreducing
trainingdatarequirements. Notably,SAM2achievesrobustperformancewithoutfine-tuning,offeringauser-friendly
andaccuratesolutioncomparedtoitspredecessor,SAM.Itsabilitytoannotateentiredatasetswithminimalhuman
inputmakesitsuitablepracticallyforlarge-scaleapplications. Thisworkhighlightsthepotentialforgeneral-purpose
modelstobenefitotherHCIfieldswhereextensivelabeleddataisneeded. Asthesemodelsadvance,weanticipate
continuedprogressinbotheyetrackingresearchandbroaderhuman-computerinteractionapplications.
References
[Bommasanietal.(2021)] RishiBommasani,DrewAHudson,EhsanAdeli,RussAltman,SimranArora,Sydneyvon
Arx,MichaelSBernstein,JeannetteBohg,AntoineBosselut,EmmaBrunskill,etal.2021. Ontheopportunities
andrisksoffoundationmodels. arXivpreprintarXiv:2108.07258(2021).
[Byrneetal.(2024)] SeanAnthonyByrne,NoraCastner,EfeBozkir,DiederickCNiehorster,andEnkelejdaKasneci.
2024. FromLensestoLivingRooms: APolicyBriefonEyeTrackinginXRBeforetheImpendingBoom.In
2024IEEEInternationalConferenceonArtificialIntelligenceandeXtendedandVirtualReality(AIxVR).IEEE,
90–96.
[Byrneetal.(2023)] SeanAnthonyByrne,VirmarieMaquiling,MarcusNyström,EnkelejdaKasneci,andDiederickC.
Niehorster.2023. LEyes: ALightweightFrameworkforDeepLearning-BasedEyeTrackingusingSyntheticEye
Images. arXiv:2309.06129[cs.CV] https://arxiv.org/abs/2309.06129
[Fuhletal.(2021)] WolfgangFuhl,GjergjiKasneci,andEnkelejdaKasneci.2021. Teyed: Over20millionreal-world
eyeimageswithpupil,eyelid,andiris2dand3dsegmentations,2dand3dlandmarks,3deyeball,gazevector,and
eyemovementtypes.In2021IEEEInternationalSymposiumonMixedandAugmentedReality(ISMAR).IEEE,
367–375.
[Fuhletal.(2015)] WolfgangFuhl,ThomasKübler,KatrinSippel,WolfgangRosenstiel,andEnkelejdaKasneci.2015.
Excuse: Robust pupil detection in real-world scenarios. In Computer Analysis of Images and Patterns: 16th
InternationalConference,CAIP2015,Valletta,Malta,September2-4,2015Proceedings,PartI16.Springer,
39–51.
[Fuhletal.(2016a)] WolfgangFuhl,ThiagoSantini,GjergjiKasneci,andEnkelejdaKasneci.2016a. Pupilnet: Convo-
lutionalneuralnetworksforrobustpupildetection. arXivpreprintarXiv:1601.04902(2016).
6APREPRINT-OCTOBER14,2024
[Fuhletal.(2016b)] WolfgangFuhl,ThiagoCSantini,ThomasKübler,andEnkelejdaKasneci.2016b. Else: Ellipse
selection for robust pupil detection in real-world environments. In Proceedings of the ninth biennial ACM
symposiumoneyetrackingresearch&applications.123–130.
[Garbinetal.(2019)] Stephan J Garbin, Yiru Shen, Immo Schuetz, Robert Cavin, Gregory Hughes, and Sachin S
Talathi.2019. Openeds: Openeyedataset. arXivpreprintarXiv:1905.03702(2019).
[Kasnecietal.(2014)] EnkelejdaKasneci,KatrinSippel,KathrinAehling,MartinHeister,WolfgangRosenstiel,Ulrich
Schiefer,andElenaPapageorgiou.2014. Drivingwithbinocularvisualfieldloss? Astudyonasupervisedon-road
parcourswithsimultaneouseyeandheadtracking. PloSone9,2(2014),e87470.
[Kimetal.(2019)] JoohwanKim,MichaelStengel,AlexanderMajercik,ShaliniDeMello,DavidDunn,SamuliLaine,
MorganMcGuire,andDavidLuebke.2019. Nvgaze: Ananatomically-informeddatasetforlow-latency,near-eye
gazeestimation.InProceedingsofthe2019CHIconferenceonhumanfactorsincomputingsystems.1–12.
[Kirillovetal.(2023)] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,
TeteXiao,SpencerWhitehead,AlexanderCBerg,Wan-YenLo,etal.2023. Segmentanything.InProceedingsof
theIEEE/CVFInternationalConferenceonComputerVision.4015–4026.
[Kotharietal.(2020)] RakshitKothari,ZhizhuoYang,ChristopherKanan,ReynoldBailey,JeffBPelz,andGabrielJ
Diaz.2020. Gaze-in-wild: Adatasetforstudyingeyeandheadcoordinationineverydayactivities. Scientific
reports10,1(2020),2539.
[Kotharietal.(2022)] Rakshit S Kothari, Reynold J Bailey, Christopher Kanan, Jeff B Pelz, and Gabriel J Diaz.
2022. EllSeg-Gen,towardsDomainGeneralizationforhead-mountedeyetracking. ProceedingsoftheACMon
human-computerinteraction6,ETRA(2022),1–17.
[Maquilingetal.(2024)] VirmarieMaquiling,SeanAnthonyByrne,DiederickCNiehorster,MarcusNyström,and
Enkelejda Kasneci. 2024. Zero-shot segmentation of eye features using the segment anything model (sam).
ProceedingsoftheACMonComputerGraphicsandInteractiveTechniques7,2(2024),1–16.
[Nyströmetal.(2023)] MarcusNyström,DiederickCNiehorster,RichardAndersson,RoySHessels,andIgnaceTC
Hooge.2023. Theamplitudeofsmalleyemovementscanbeaccuratelyestimatedwithvideo-basedeyetrackers.
BehaviorResearchMethods55,2(2023),657–669.
[Palmeroetal.(2020)] CristinaPalmero,AbhishekSharma,KarstenBehrendt,KapilKrishnakumar,OlegVKomogort-
sev,andSachinSTalathi.2020. Openeds2020: Openeyesdataset. arXivpreprintarXiv:2005.03876(2020).
[Perézetal.(2003)] AntonioPeréz,MLuisaCórdoba,AGarcia,RafaelMéndez,MLMunoz,JoséLuisPedraza,andF
Sanchez.2003. Apreciseeye-gazedetectionandtrackingsystem. (2003).
[Ravietal.(2024)] NikhilaRavi,ValentinGabeur,Yuan-TingHu,RonghangHu,ChaitanyaRyali,TengyuMa,Haitham
Khedr,RomanRädle,ChloeRolland,LauraGustafson,etal.2024. Sam2: Segmentanythinginimagesand
videos. arXivpreprintarXiv:2408.00714(2024).
[Sambasivanetal.(2021)] NithyaSambasivan,ShivaniKapania,HannahHighfill,DianaAkrong,PraveenParitosh,and
LoraMAroyo.2021. “Everyonewantstodothemodelwork,notthedatawork”: DataCascadesinHigh-Stakes
AI.Inproceedingsofthe2021CHIConferenceonHumanFactorsinComputingSystems.1–15.
[Santinietal.(2018a)] ThiagoSantini,WolfgangFuhl,andEnkelejdaKasneci.2018a. PuRe: Robustpupildetection
forreal-timepervasiveeyetracking. ComputerVisionandImageUnderstanding170(2018),40–50.
[Santinietal.(2018b)] ThiagoSantini,WolfgangFuhl,andEnkelejdaKasneci.2018b. PuReST:Robustpupiltracking
forreal-timepervasiveeyetracking.InProceedingsofthe2018ACMsymposiumoneyetrackingresearch&
applications.1–5.
[Shortisetal.(1994)] MarkRShortis,TimothyAClarke,andTimShort.1994. Comparisonofsometechniquesfor
thesubpixellocationofdiscretetargetimages.InVideometricsIII,Vol.2350.SPIE,239–250.
[Sunetal.(2017)] ChenSun,AbhinavShrivastava,SaurabhSingh,andAbhinavGupta.2017. Revisitingunreasonable
effectivenessofdataindeeplearningera.InProceedingsoftheIEEEinternationalconferenceoncomputervision.
843–852.
[Tonsenetal.(2016)] MarcTonsen,XucongZhang,YusukeSugano,andAndreasBulling.2016. Labelledpupilsinthe
wild: adatasetforstudyingpupildetectioninunconstrainedenvironments.InProceedingsoftheninthbiennial
ACMsymposiumoneyetrackingresearch&applications.139–142.
[Zhangetal.(2023)] Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and
ChoongSeonHong.2023. Fastersegmentanything: Towardslightweightsamformobileapplications. arXiv
preprintarXiv:2306.14289(2023).
7APREPRINT-OCTOBER14,2024
[Zhouetal.(2023)] YukunZhou,MarkAChia,SiegfriedKWagner,MuratSAyhan,DominicJWilliamson,RobbertR
Struyven,TimingLiu,MouchengXu,MateoGLozano,PeterWoodward-Court,etal.2023. Afoundationmodel
forgeneralizablediseasedetectionfromretinalimages. Nature622,7981(2023),156–163.
8