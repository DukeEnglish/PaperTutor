Preprint
SEMANTIC SCORE DISTILLATION SAMPLING FOR
COMPOSITIONAL TEXT-TO-3D GENERATION
LingYang1†,∗ZixiangZhang1∗, JunlinHan2,BohanZeng1,RunjiaLi2
PhilipTorr2, WentaoZhang1†
1PekingUniversity 2UniversityofOxford
Project: https://github.com/YangLing0818/SemanticSDS-3D
ABSTRACT
Generating high-quality 3D assets from textual descriptions remains a pivotal
challenge in computer graphics and vision research. Due to the scarcity of 3D
data,state-of-the-artapproachesutilizepre-trained2Ddiffusionpriors,optimized
through Score Distillation Sampling (SDS). Despite progress, crafting complex
3D scenes featuring multiple objects or intricate interactions is still difficult. To
tackle this, recent methods have incorporated box or layout guidance. However,
theselayout-guidedcompositionalmethodsoftenstruggletoprovidefine-grained
control,astheyaregenerallycoarseandlackexpressiveness. Toovercomethese
challenges,weintroduceanovelSDSapproach,SemanticScoreDistillationSam-
pling (SEMANTICSDS), designed to effectively improve the expressiveness and
accuracy of compositional text-to-3D generation. Our approach integrates new
semantic embeddings that maintain consistency across different rendering views
and clearly differentiate between various objects and parts. These embeddings
are transformed into a semantic map, which directs a region-specific SDS pro-
cess,enablingpreciseoptimizationandcompositionalgeneration. Byleveraging
explicitsemanticguidance,ourmethodunlocksthecompositionalcapabilitiesof
existing pre-trained diffusion models, thereby achieving superior quality in 3D
content generation, particularly for complex objects and scenes. Experimental
results demonstrate that our SEMANTICSDS framework is highly effective for
generatingstate-of-the-artcomplex3Dcontent.
1 INTRODUCTION
Generating high-quality 3D assets from textual descriptions is a long-standing goal in computer
graphicsandvisionresearch. However, duetothescarcityof3Ddata, existingtext-to-3Dgenera-
tionmodelshaveprimarilyreliedonleveragingpowerfulpre-trained2Ddiffusionpriorstooptimize
3Drepresentations,typicallybasedonascoredistillationsampling(SDS)loss(Pooleetal.,2023).
NotableexamplesincludeDreamFusion,whichpioneeredtheuseofSDStooptimizeNeuralRadi-
anceField(NeRF)representations(Mildenhalletal.,2021),andMagic3D(Linetal.,2023a),which
furtheradvancedthisapproachbyproposingacoarse-to-fineframeworktoenhanceitsperformance.
Despite the advancements in lifting and SDS-based methods, generating complex 3D scenes with
multipleobjectsorintricateinteractionsremainsasignificantchallenge.Recenteffortshavefocused
on incorporating additional guidance, such as box or layout information(Po & Wetzstein, 2024;
Epstein et al., 2024; Zhou et al., 2024). Among them, Po & Wetzstein (2024) introduce locally
conditioned diffusion for compositional scene diffusion based on input bounding boxes with one
shared NeRF representation while Epstein et al. (2024) instantiate and render multiple NeRFs for
a given scene using each NeRF to represent a separate 3D entity with a set of layouts. Further
advancingthisfield,GALA3D(Zhouetal.,2024)utilizeslargelanguagemodels(LLMs)togenerate
coarselayoutstoguide3Dgenerationforcompositionalscenes.
However, existing layout-guided compositional methods often fall short in achieving fine-grained
controloverthegenerated3Dscenes.Thecurrentformofboxorlayoutguidanceisrelativelycoarse
∗Contributedequally.
†Correspondingauthors:yangling0818@163.com,wentao.zhang@pku.edu.cn
1
4202
tcO
11
]VC.sc[
1v90090.0142:viXraPreprint
GraphDreamer LucidDreamer GALA3D GSGEN Ours
A rabbit sits atop a large, expensive watch with many shiny gears, made half of
ironand half of gold, ea�ng a birthday cake that is in front of the rabbit
A mannequin adorned with a dress made of feathers and moss stands at the center,
flanked by a vase with a single bluetulip and another with blueroses.
A car with the front right side made of cheese, the front le� side made of sushi,
and the back made of LEGO.
Figure1: SEMANTICSDSachievessuperiorcompositionaltext-to-3dgenerationresultsoverstate-
of-the-artbaselines,particularlyingeneratingmultipleobjectswithdiverseattibutes.
andlackstheexpressivenessrequiredtoeffectivelyguidetheSDSprocessinoptimizingtheintricate
interactionsorintersectingpartsbetweenmultipleobjects,particularlywhengeneratingobjectswith
multipleattributes. Thislimitationstemsfromthefactthatpre-trained2Ddiffusionmodels,which
are used in SDS, struggle to estimate accurate scores for complex scenarios with consistent views
whenexplicitspatialguidanceisabsent(Lietal.,2023;Shietal.,2024). Asaresult,thegenerated
3Dscenesmaylackthelevelofdetailandrealismdesired, highlightingtheneedformoreprecise
guidancemechanismsthatcanprovidefiner-grainedcontroloverthegenerationprocess.
Toaddresstheselimitations, weproposeSemanticScoreDistillationSampling(SEMANTICSDS),
which boosts the expressiveness and precision of compositional text-to-3D generation. For more
explicit3Dexpression,weequipSEMANTICSDSwith3DGaussianSplatting(3DGS)(Kerbletal.,
2023)asthe3Drepresentation. Ourapproachconsistsofthreekeysteps: (1)Givenatextprompt,
weproposeaprogram-aidedapproachtoimprovetheaccuracyofLLM-basedlayoutplanningfor
3Dscenes. (2)Weintroducenovelsemanticembeddingsthatremainconsistentacrossvariousren-
deringviewsandexplicitlydistinguishdifferentobjectsandparts.(3)Wethenrenderthesesemantic
embeddingsintoasemanticmap,whichservesasguidanceforaregion-wiseSDSprocess,facilitat-
ingfine-grainedoptimizationandcompositionalgeneration. Ourapproachaddressesthechallenge
2Preprint
ofleveragingpre-traineddiffusionmodels, whichpossesspowerfulcompositionaldiffusionpriors
but are difficult to utilize (Wang et al., 2024a; Yang et al., 2024). By using explicit semantic map
guidance,weinnovativelyunlockthesecompositional2Ddiffusionpriorsforhigh-quality3Dcon-
tentgeneration.
Ourmaincontributionsaresummarizedasfollows:
• WeproposeSEMANTICSDS,anovelsemantic-guidedscoredistillationsamplingapproach
thateffectivelyenhancestheexpressivenessandprecisionofcompositionaltext-to-3Dgen-
eration,asshowninFigure1.
• Weintroduceprogram-aidedlayoutplanningtoimprovepositionalandrelationalaccuracy
ingenerated3Dscenes,derivingprecise3Dcoordinatesfromambiguousdescriptions.
• Wedevelopexpressivesemanticembeddingstoaugment3DGaussianrepresentations,and
proposearegion-wiseSDSprocesswiththerenderedsemanticmap,distinguishingdiffer-
entobjectsandpartsinthecompositionalgenerationprocess.
2 RELATED WORK
Text-to-3DGeneration Differentapproacheshavebeendevelopedtoachievetext-to-3Dcontent
generation(Deitkeetal.,2024;Zengetal.,2023),suchasemployingmulti-viewdiffusionmodels
(Shi et al., 2024; Wu et al., 2024a; Kong et al., 2024; Blattmann et al., 2023), direct 3D diffusion
models (Gupta et al., 2023; Shue et al., 2023; Wu et al., 2024b) and large reconstruction models
(Hong et al., 2024). For instance, multi-view diffusion models are trained and optimized by fine-
tuningvideodiffusionon3Ddatasets,aidingin3Dreconstruction(Voletietal.,2024;Chenetal.,
2024d; Han et al., 2024b). You et al. (2024) propose a training-free method that employs video
diffusion as a zero-shot novel view synthesizer. However, these methods require numerous 3D
data for training. In contrast, Score Distillation Sampling (SDS) (Poole et al., 2023; Wang et al.,
2023) is 3D data-free and generally produces higher quality assets. SDS approaches harness the
creativepotentialof2Ddiffusionandhaveachievedsignificantadvancements (Wangetal.,2024b;
Yang et al., 2023b; Hertz et al., 2023), resulting in realistic 3D content generation and enhanced
resolutionofgenerativemodels(Zhuetal.,2024). Inthispaper,weproposeanewSDSparadigm,
namely SEMANTICSDS, for text-to-3D generation in complex scenarios, which first incorporates
explicitsemanticguidanceintotheSDSprocess.
Compositional 3D Generation Modeling compositional 3D data distribution is a fundamental
andcriticaltaskforgenerativemodels. Currentfeed-forwardmethods(Shueetal.,2023;Shietal.,
2024) are primarily capable of generating single objects and face challenges when creating more
complex scenes containing multiple objects due to limited training data. Po & Wetzstein (2024)
fix the layout in multiple 3D bounding boxes and generate compositional assets with bounding-
box-specificSDS.Recently,aseriesoflearnable-layoutcompositionalmethodshavebeenproposed
(Epsteinetal.,2024;Vilesovetal.,2023;Hanetal.,2024a;Chenetal.,2024b;Lietal.,2024;Yan
etal.,2024;Gaoetal.,2024). Thesemethodscombinemultipleobject-ad-hocradiancefieldsand
thenoptimizethepositionsoftheradiancefieldsfromexternalfeedback.Forexample,Epsteinetal.
(2024)proposelearningadistributionofreasonablelayoutsbasedsolelyontheknowledgefroma
largepre-trainedtext-to-imagemodel. Vilesovetal.(2023)introduceanoptimizationmethodbased
onMonte-Carlosamplingandphysicalconstraints. Non-learnablelayoutmethodslike(Zhouetal.,
2024)andLinetal.(2023b)furtherutilizeLLMsorMLLMstoconverttextintoreasonablelayouts.
However,thecurrentformoflayoutguidanceisrelativelycoarseandnotexpressiveenoughforfine-
grainedcontrol. Weaddressthisproblem byincorporatingsemanticembeddingsthatensure view
consistencyanddistinctlydifferentiateobjectsintoSDSprocesses,whichareflexibleandexpressive
foroptimizing3Dscenes.
3 PRELIMINARIES
Compositional 3D Gaussian Splatting 3D Gaussian Splatting explicitly represents a 3D scene
asacollectionofanisotropic3DGaussians,eachcharacterizedbyameanµ∈R3 andacovariance
3Preprint
matrixΣ(Kerbletal.,2023). TheGaussianfunctionG(x)isdefinedas:
(cid:18) (cid:19)
1
G(x)=exp − (x−µ)⊤Σ−1(x−µ) (1)
2
Renderingacompositionalscenenecessitatesatransformationfromobjecttocompositioncoordi-
nates, involving a rotation R ∈ R3×3, translation t ∈ R3, and scale s ∈ R (Zhou et al., 2024;
Vilesovet al.,2023). Thistransformation isapplied to themean andvariance ofindividual Gaus-
sians,transitioningfromtheobject’slocalcoordinatestoglobalcoordinates:µglobal =sRµlocal+t,
Σglobal =s2RΣlocalR⊤.
Foroptimizedrenderingofcompositional3DGaussiansinto2Dimageplanes,atile-basedrasterizer
enhancesrenderingefficiency. Therenderedcoloratpixelviscomputedasfollows:
i−1
(cid:88) (cid:89)
I(v)= c α (1−α ), (2)
i i j
i∈N j=1
where c represents the color of the i-th Gaussian, N denotes the set of Gaussians within the tile,
i
andα istheopacity.
i
ScoreDistillationSampling Yangetal.(2023a);Wangetal.(2023)haveintroducedamethodto
leverageapretraineddiffusionmodel, ϵ (x ;y,t), tooptimizethe3Drepresentation, wherex , y,
ϕ t t
andtsignifythenoisyimage,textembedding,andtimestep,respectively.
Let g represent the differentiable rendering fcuntion, θ denote the parameters of the optimizable
3D representation and I = g(θ) be the resulting rendered image. The gradientfor optimization is
performedviaScoreDistillationSampling:
(cid:20) (cid:21)
∂I
∇ L =E w(t)(ϵ (x ;y,t)−ϵ) (3)
θ SDS ϵ,t ϕ t ∂θ
where ϵ is Gaussian noise and w(t) is a weighting function. In compositional 3D generation, lo-
cal object optimizations and global scene optimizations alternate in a compositional optimization
scheme(Zhouetal.,2024). Duringlocaloptimization,theparametersθ includethemean,covari-
ance,andcolorofindividualGaussians. Inglobalsceneoptimization,theparametersθadditionally
includetransformations—translation,scale,androtation—thatconvertlocaltoglobalcoordinates.
4 METHOD
4.1 PROGRAM-AIDEDLAYOUTPLANNING
A detailed characterization of multiple objects’ positions, dimensions, and orientations requires
numerous parameters, especially when additionally describing distinct attributes of various object
components. In scenarios involving multiple objects, utilizing Large Language Models (LLMs)
toderiveprecise3Dcoordinatesfromambiguousdescriptionswithinasceneisoftenchallenging.
Thisdifficultyarisesbecausepurely3Dnumericaldataandcorrespondingnaturallanguagedescrip-
tionsdonotfrequentlyco-occurinthetrainingdataofLLMs(Hongetal.,2023;Xuetal.,2023).
Consequently, issues such as overlapping objects or excessive distances between them may occur,
particularlyduringinteractionsamongobjects. Therefore,weproposetoleverageprogramsasthe
intermediatereasoningandplanningsteps (Gaoetal.,2023)toeffectivelymitigatethesechallenges.
Let y represent the complex user input, which includes multiple objects with various attributes.
c
First,WeutilizeLargeLanguageModelstoidentifyallobjects{O }K withiny ,whereKdenotes
k k=1 c
the total number of objects. For each object, the corresponding prompt y is recognized, and its
k
dimensionsareestimated. Thisincludesconsideringtheobject’sreal-worldsizeanditsrelationship
withotherobjectstodetermineitsrelativesize, facilitatingtheplacementofallobjectswithinthe
samescene.
Subsequently,LLMssequentiallypositioneachobjectwithinthescene. Indesigningeachobject’s
placement,LLMsarticulatethespatialrelationshipswithrelevantentitiesusingprogrammablelan-
guagedescriptionsthatexplicitlyoutlineallmathematicalcalculations. Thislanguageisthencon-
verted into a program executed by a runtime, such as a Python interpreter, to produce the layout
4Preprint
User prompt Program-aided Layout Planning
A carwith its
front half made 1. Spatial Layouts
of cheeseand its
2. Semantic Layouts
rear half made of
sushiis situated
Initialization of
to the right of a
Semantic 3DGS
LEGO house.
RGB Image & Semantic Map
Render
Noise
Back Propagation
Text-to-image
Diffusion 𝓛 𝑺𝒆𝒎𝒂𝒏𝒕𝒊𝒄)𝑺𝑫𝑺
A LEGO house，
Regional Denoising side view
with Semantic Map Compose
SemanticSDS
A car made of cheese,
overhead view
A car made of sushi,
overhead view
Figure 2: Overview of SEMANTICSDS, comprising of program-aided layout planning (top) and
regionaldenoisingwithsemanticmap(bottom).
solution. Theselayouts,whichincludescalefactors,Eulerangles,andtranslationvectors,areem-
ployedtotransform3DGaussiansfromlocalcoordinatestoglobalcoordinatesduringrendering.
Furthermore, for each object O , LLMs decomposes its layout space into n complementary re-
k k
gions, each with distinct attributes and different subprompts {y }nk . These complementary re-
k,l l=1
gionsaredesignedtobenon-overlappingandcollectivelyencompasstheentirelayoutspaceoftheir
respective object. To generate meaningful and accurate complementary regions, LLMs employ a
structured decomposition process that segments the space of object O into hierarchical divisions
k
basedondepth,width,andlengthdimensions.Thisprocessisdocumentedusingprogrammablelan-
guagedescriptionsandsubsequentlyconvertedintopreciseboundingboxesbyaprogram. Details
onthepromptsusedforthisprogram-aidedlayoutplanningareprovidedinAppendixA.1.
4.2 SEMANTICSCOREDISTILLATIONSAMPLING
Prompt-GuidedSemantic3DGaussianRepresentation Togenerate3Dscenesinvolvingmul-
tiple objects with diverse attributes and to precisely control the attributes of distinct spatial re-
gions within each object, it is essential to utilize features that represent the fine-grained seman-
tics of 3D Gaussians. We design new prompt-guided semantic 3D Gaussian representations.
During initialization, the subprompt y corresponding to the i-th Gaussian is encoded via the
k,l
CLIP text encoder Φ (Radford et al., 2021) to obtain the high-dimensional semantic embedding,
h
i
= Φ(y k,l) ∈ Rdh. Giventhesignificantmemorydemandsimposedbythelargedimensionsof
d , a lightweight autoencoder is employed. This autoencoder effectively compresses the scene’s
h
high-dimensional semantic embeddings into more manageable, low-dimensional representations,
representedasf
i
=E(h i)∈Rdf. Thelossfunctionfortheautoencoderisdefinedas:
(cid:88)
L = d (D(E(h )),h ) (4)
ae ae i i
i∈N
5Preprint
where d denotes the metric combining the L loss and the symmetric cross entropy loss from
ae 1
CLIP (Radfordetal.,2021).
Thei-thGaussianisthenaugmentedwithasemanticembeddingf ∈Rd.Andsemanticinformation
i
isintegratedintotherendered2Dimagebyrenderingthesemanticembeddingatpixelv usingthe
formula:
i−1
(cid:88) (cid:89)
F(v)= f α (1−α ) (5)
i i j
i∈N j=1
The rendered semantic embedding F(v), derived from equation 5, is fed into the decoder D to
reconstructS(v)=D(F(v))∈Rdh andthengeneratesasemanticmapS∈RH×W×dh indicating
therenderedimage’ssemanticattributes.
SemanticScoreDistillationSampling Toenablefine-grainedcontrollablegeneration,thegener-
atedsemanticmapisintegratedintothespatialcompositionofscoresfordistillationsampling. The
subprompty isprocessedthroughtheCLIPtextencoderΦtoproducethesubpromptembedding
k,l
q
k,l
=Φ(y k,l)∈Rdh. Theprobabilitythatpixelvcorrespondstosubprompty
k,l
iscomputedas:
exp(cos(q ,S(v))/τ)
p(k,l|v)= k,l (6)
(cid:80)K (cid:80)n k′ exp(cos(q ,S(v))/τ)
k′=1 l′=1 k,l
where τ is a temperature parameter learned by CLIP and cos(·,·) denotes cosine similarity. This
facilitates the derivation of the mask M (v), which indicates whether the semantic properties of
k,l
pixelvalignwithsubprompty .
k,l
(cid:26) 1 if(k,l)=argmax p(k′,l′ |v)
M (v)= k′,l′ (7)
k,l 0 otherwise
ThesemanticmaskM ∈{0,1}H×W issubsequentlyutilizedtoguidethescoredistillationsam-
k,l
pling. To ensure that the Gaussians near the edges of objects are not overlooked, the mask M
k,l
issubjectedtoamaxpoolingoperationwitha5×5kernel,resultinginMˆ . Althoughdiffusion
k,l
models generally lack an inherent distinction at the object and part levels in their latent spaces or
attention maps for fine-grained control (Lian et al., 2024), recent advancements in compositional
2Dimagegenerationhaveimplementedspatially-conditionedgeneration(Chenetal.,2024a;Yang
et al., 2024; Xie et al., 2023). This is achieved through regional denoising or attention manipula-
tion,allowingforfine-grainedcontroloverthesemanticsofthegeneratedimages. Specifically,the
overall denoising score is calculated as the aggregate of the individually masked denoising scores
foreachvisiblesubprompty :
k,l
(cid:104) (cid:105)
ϵˆ (x ;y,t)=E ϵ (x ;y ,t)⊙Mˆ (8)
ϕ t k,l ϕ t k,l k,l
where ⊙ denotes element-wise multiplication. Instead of conditioning the diffusion models on a
single text prompt, our semantic score distillation sampling employs the compositional denoising
scoreasfollows:
(cid:20) (cid:21)
∂x
∇ L =E w(t)(ϵˆ (x ;y,t)−ϵ) (9)
θ SemanticSDS ϵ,t ϕ t ∂θ
Thismethodologyeffectivelyleveragestheexpressivecompositionalgenerationcapabilitiesofpre-
trained2Ddiffusionmodelsfortext-to-3Dgeneration. Furtherdetailson SEMANTICSDS arepro-
videdinAppendixA.2.
Object-Specific View Descriptor for Global Scene Optimization Unlike object-centric opti-
mization, scenesdonotexhibitdistinctperspectivesasindividualobjectsdo. Effectivescenegen-
erationnecessitatesprecise,part-levelcontrolovertheoptimizationofdistinctobjectviews. Terms
suchas”sideview”or”backview”arerarelyapplicabletomulti-objectscenes,andpretraineddif-
fusion models often struggle to generate images accurately from such prompts (Li et al., 2023).
Moreover, within a single rendered image, different objects may be visible from varying perspec-
tives. UsingaunifiedviewdescriptorforanentirescenewithmultipleobjectsexacerbatestheJanus
Problem(Pooleetal.,2023). Althoughthecompositionaloptimizationschemealternatesbetween
localobjectoptimizationsandglobalsceneoptimizations(Zhouetal.,2024),allowingforthecor-
rectoptimizationofdifferentviewsofobjectsinlocalcoordinates,itisconfoundedbyoptimizations
6Preprint
Original Scene center BlockB Ours BlockB
Rendered image
BlockA BlockA
Camera Camera
Prompt 1: A photo of blockA, side view.
Prompt: A photo of blockAand blockB, front view. Prompt 2: A photo of blockB, front view.
Figure3: Illustrationofourproposedobject-specificviewdescriptorforglobalsceneoptimization.
under global coordinates. This limits the frequency of global scene optimizations and results in a
lackofscenecoherence,harmony,andlightingconsistency.
Toaddressthisissue,inourSEMANTICSDS,weappendanobject-specificviewdescriptoryview to
k
the corresponding subprompts {y }nK to optimize individual objects within the rendered image
k,l l=1
(inFigure3). Thesameviewdescriptoryview isconsistentlyappliedacrossdifferentpartsofeach
k
multi-attributeobject. Specifically,wedeterminethecamera’selevationandazimuthanglesrelative
toeachobjectbycomputingtheanglebetweenthevectornˆ, whichextendsfromtheobjecttothe
camera, andspecificreferenceaxisvectors, suchasthepositivez-axis. Thiscalculationfacilitates
the selection of the most appropriate object-specific view descriptor. For instance, if the angle
betweennˆandthepositivez-axisremainsbelowapredefinedthreshold,indicativeofahighazimuth
angle,thedescriptoryviewisassignedasanoverheadviewdescriptorforthatobject.
k
5 EXPERIMENTS
Implementation Details. The guidance model is implemented using the publicly accessible
diffusion model, StableDiffusion (Rombach et al., 2022), specifically utilizing the checkpoint
runwayml/stable-diffusion-v1-5. Positions of the Gaussians are initialized using Shap-E (Jun &
Nichol,2023),witheachobjectinitiallycomprising12288Gaussians. Fordensification,Gaussians
are cloned or split based on the view-space position gradient using a threshold T = 2, with se-
pos
manticembeddingscopied. Compactness-baseddensificationisalsoappliedevery2000iterations,
involving each Gaussian and one of its nearest neighbors, as described in GSGEN (Chen et al.,
2024c). PruninginvolvesremovingGaussianswithopacitylowerthanα =0.3,aswellasthose
min
withexcessivelylargeradiiineitherworld-spaceorview-space,every200iterations.
Trainingalternatesbetweenlocalandglobaloptimization. Duringglobaloptimization,therendered
objectsvarybyswitchingbetweentheentiresceneandpairsofobjects. Camerasamplingmaintains
thesamefocallength,elevation,andazimuthrangeasspecifiedin(Chenetal.,2024c). Thethresh-
oldforselectingobject-specificviewdescriptorsincludes:anoverheadviewdescriptorforelevation
anglesexceeding60°,afrontviewdescriptorforazimuthangleswithin±45°ofthepositivex-axis,
andabackviewdescriptorfor±45°anglesonthenegativex-axis.
Table1: QuantitativeComparison
Metrics GraphDreamer GSGEN LucidDreamer GALA3D SemanticSDS(Ours)
CLIPScore↑ 0.289 0.314 0.311 0.305 0.321
PromptAlignment↑ 56.9 63.3 64.4 85.0 91.1
SpatialArrangement↑ 53.8 62.8 65 80.0 85.7
GeometricFidelity↑ 53.8 71.1 71.8 80.3 83.0
SceneQuality↑ 54.9 71.2 65.9 82.3 86.9
Baseline methods. To evaluate the performance of SEMANTICSDS on the complex Text-to-3D
task involving multiple objects with varied attributes, we compare it with state-of-the-art (SOTA)
7Preprint
GraphDreamer LucidDreamer GALA3D GSGEN Ours
A corgi is posi�oned to the le� of a LEGOhouse, while a car with its front half made of
cheeseand its rear half made of sushiis situated to the right of the house made of LEGO.
In a library's reading room, a stoneblock table is flanked by two types of chairs: a high-
back leather chairon the le� side and alow-slung, blue chair on the right. Two lamps,
onewithaclassicdesignandtheotherwithamodernaesthe�c, areposi�onedabove
thetabletoprovideligh�ng.
In abotanic garden, a greenhouse is split into two climates. The le� side is a tropical
environmentwithlushgreenery, andtherightsideisa icysnowyclimatewithcac�and
succulents. Two watering cans, one largeand the other small, are placed at the entrance.
Figure4:Qualitativecomparisonsoftext-to-3Dgeneration.Comparisonresultsdemonstratethat
SEMANTICSDSsynthesizesmorepreciseandrealisticmulti-objectsceneswithbettervisualdetails,
geometricexpressiveness,andsemanticconsistency.
methods. Theseincludethecompositional3DgenerationmethodGALA3D(Zhouetal.,2024)and
GraphDreamer (Gao et al., 2024), noted for their ability to generate intricate scenes with multiple
objects. Additionally, we consider GSGEN (Chen et al., 2024c) and LucidDreamer (Liang et al.,
2024),botharecapableofproducinghigh-quality,complexobjectswithdiverseattributes.
Metrics. CLIP Score (Radford et al., 2021) is employed as the evaluation metric to assess the
qualityandconsistencyofthegenerated3Dsceneswithtextualdescriptions. However,CLIPtends
to focus on the primary objects within the rendered image, and when used to evaluate complex
text-to-3Dtasksinvolvingmultipleobjectswithvariedattributes, itmaynotadequatelyassessthe
geometry of all objects or the rationality of their spatial arrangements. This limitation results in a
misalignment with human judgment regarding evaluation criteria. Therefore, following Wu et al.
(2024c),GPT-4Visutilizedasahuman-alignedevaluatortocompare3Dassetsbasedonpredefined
8Preprint
criteria. Thesecriteriainclude: (1)PromptAlignment: ensuringthatallobjectsspecifiedintheuser
prompts are present and correctly quantified; (2) Spatial Arrangement: evaluating the logical and
thematicspatialarrangementofobjects;(3)GeometricFidelity: assessingthegeometricfidelityof
eachobjectforrealisticrepresentation;and(4)SceneQuality: determiningtheoverallscenequality
intermsofcoherenceandvisualharmony.MoredetailsonmetricsareprovidedintheAppendixA.3.
5.1 MAINRESULTS
Quantitative Analysis To evaluate the performance of SEMANTICSDS in Text-to-3D tasks in-
volving multiple objects with varied attributes, quantitative metrics were employed. As shown in
Table1,theCLIPScoreindicatesthat SEMANTICSDS exhibitsstrongalignmentwiththeprimary
semanticsofuserprompts. Specifically,SEMANTICSDSexcelsinPromptAlignment,ensuringthat
all objects specified in user prompts are present and correctly quantified. Additionally, it demon-
stratessuperiorperformanceinSpatialArrangement,effectivelydesigningthelayoutofinteractive
objectstosupportthescene’sintendedtheme.Furthermore,byexplicitlyguidingSDSwithrendered
semanticmaps,SEMANTICSDSachievesoutstandinggenerationofindividualobjectswithdiverse
attributesacrossdifferentspatialcomponents,resultinginhighscoresinobject-levelGeometricFi-
delity. Additionally,theuseofcompositional3DGaussianSplattingforscenerepresentationhelps
SEMANTICSDS to effectively disentangle objects within the scene. This, combined with explicit
semanticguidancetotheSDS,contributestoachievingthehighestscoreinSceneQuality.
QualitativeAnalysis Tointuitivelydemonstratethesuperiorityoftheproposedmethodingener-
atingcomplex3Dsceneswithmultipleobjectspossessingdiverseattributes,aqualitativecompari-
sonwithbaselinemodelsisconducted. AsillustratedinFigure4,GALA3D,withacompositional
optimizationscheme,successfullygeneratesindividualobjectsthatalignwithuserprompts. How-
ever, it fails to produce plausible results when objects have multiple attributes. Although GSGEN
andLucidDreamergeneratehigh-qualityindividualobjects, thepresenceofmultipleobjectsoften
leads to entanglement, compromising consistency with user prompts. Additionally, these models
are unable to generate reasonable objects when individual objects possess numerous attributes. In
contrast,SEMANTICSDSemploysguideddiffusionmodelswithexplicitsemantics,effectivelygen-
eratingscenesthatincludemultipleobjectswithdiverseattributes. Moreover,byutilizingprogram-
aidedlayoutplanning,SEMANTICSDSproducesmorecoherentlayoutsthanGALA3Dinscenarios
involvingcomplexspatialrelationshipsamongmultipleobjects.Forexample,inFigure1,bothtable
lampsarecorrectlyplacedonthetablewithoutappearingtofloatwhenusingSEMANTICSDS.
UserStudy Weconductedauserstudytocompareour
LucidDreamer
method with baseline methods across 30 scenes involv- GraphDreamer GSGEN
ingmorethan100objects. Eachparticipantwasshowna
7%10%10%
Gala3d
13%
userpromptalongside 3Dscenesgeneratedbyallmeth-
odssimultaneouslyandaskedtoselectthemostrealistic
60%
assets based on geometry, prompt alignment, and accu-
SemanticSDS(Ours)
rateplacement. Figure5illustratesthat SEMANTICSDS
significantly outperformed previous methods in terms of
humanpreference. Figure5: Userstudyresults. SEMAN-
TICSDSispreferred60%ofthetimeby
usersthanbaselinemethods.
5.2 MODELANALYSIS
EffectivenessofProgram-aidedLayoutPlanning Weassessthenecessityofprogram-aidedlay-
outplanningthroughanablationstudy.Thequalitativecomparisonofgeneratedlayoutsisillustrated
inFigure6. Withoutprogram-aidedplanning,layoutplacementoftenlacksrationaleandresultsin
poor spatial arrangements. In contrast, the program-aided strategy positions the layouts logically
anddividesthelayoutintomeaningfulandprecisecomplementaryregionsforobjectswithmultiple
attributes,resultinginaneffectivespatialarrangement.
Impact of Semantic Score Distillation Sampling Ablation experiments are performed on Se-
manticScoreDistillationSamplingtoevaluatetheeffectsofexplicitlyguidingSDSwithrendered
semanticmaps. InFigure7,without SEMANTICSDS,whileobjectswithsingleattributesaregen-
erated effectively, those with varied attributes often experience blending issues. For instance, the
9Preprint
Without program-aided With program-aided (Ours)
OverheadView OOvveerrhheeaadd VViieeww
Decomposed prompt
Side View Side View
Text prompt: A table, half made of wood and half white, holdsa roasted turkey, a salad, a glass
of orange juice, and a plate witha loaf of French bread.
Figure6: Qualitativecomparisonsbetweenwithoutandwithourprogram-aidedlayoutplanning.
”house”showssnowbricksmixedwithLEGObricks, failingtomeettheuserprompt’sspatialre-
quirements.ThesnowbricksareinaccuratelyrepresentedaswhiteLEGObricks,whichdonotalign
withtheintendedattributes. Additionally,oneattributemaydominate,causingotherstodisappear,
such as in the ”car” with three attributes in Figure 7. Conversely, SemanticSDS enables precise
controlovertheattributesindistinctspatialregionsofeachobject, producingobjectswithdiverse
attributesandsmoothtransitionsbetweenregionswithdifferentattributes.
Without SemanticSDS Without object-specific view descriptor With both
Text prompt: A corgi is positioned to the left of a house that is half made of LEGO and half of snow. To the right of the house, there is a car
with its front right side made of cheese, front left side made of sushi, and the back made of LEGO.
Figure7: Qualitativeanalysis. OurSEMANTICSDSprovidesmorepreciseandfine-grainedcontrol
andourproposedobject-specificviewdescriptorhelpswithbettermulti-viewunderstanding.
Object-SpecificViewDescriptor Toassesstheeffectivenessoftheobject-specificviewdescrip-
tor,wereplaceitwiththescene-centricviewdescriptorutilizedbyGSGENduringglobaloptimiza-
tion. This change increases the occurrence of the Janus Problem, as illustrated by the overhead
viewofthecorgiinthemiddleofFigure7. Thesefindingshighlightthecrucialroleofselectingan
appropriateviewdescriptortoenhancetheplausibilityofgenerated3Dscenes.
6 CONCLUSION
In this paper, we introduce SEMANTICSDS, a novel SDS method that significantly enhances the
expressivenessandprecisionofcompositionaltext-to-3Dgeneration. Byleveragingprogram-aided
layoutplanning,semanticembeddings,andexplicitsemanticguidance,weunlockthecompositional
priorsofpre-traineddiffusionmodelsandachieverealistichigh-qualitygenerationincomplexsce-
narios.OurextensiveexperimentsdemonstratethatSEMANTICSDSachievesstate-of-the-artresults
forgeneratingcomplex3Dcontent.Aswelooktothefuture,weenvisionSEMANTICSDSasafoun-
dationforevenmoreapplications,suchasautomaticeditingandclosed-looprefinement,pavingthe
wayforunprecedentedlevelsofcreativityandinnovationin3Dcontentgeneration.
10Preprint
REFERENCES
Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik
Lorenz,YamLevi,ZionEnglish,VikramVoleti,AdamLetts,etal.Stablevideodiffusion:Scaling
latentvideodiffusionmodelstolargedatasets. arXivpreprintarXiv:2311.15127,2023.
Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free layout control with cross-attention
guidance. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer
Vision,pp.5343–5353,2024a.
Yongwei Chen, Tengfei Wang, Tong Wu, Xingang Pan, Kui Jia, and Ziwei Liu. Comboverse:
Compositional 3d assets creation using spatially-aware diffusion guidance. arXiv preprint
arXiv:2403.12409,2024b.
ZilongChen,FengWang,YikaiWang,andHuapingLiu.Text-to-3dusinggaussiansplatting.InPro-
ceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.21401–
21412,2024c.
Zilong Chen, Yikai Wang, Feng Wang, Zhengyi Wang, and Huaping Liu. V3d: Video diffusion
modelsareeffective3dgenerators. arXivpreprintarXiv:2403.06738,2024d.
MattDeitke,RuoshiLiu,MatthewWallingford,HuongNgo,OscarMichel,AdityaKusupati,Alan
Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: A universe of
10m+3dobjects. AdvancesinNeuralInformationProcessingSystems,36,2024.
DaveEpstein,BenPoole,BenMildenhall,AlexeiAEfros,andAleksanderHolynski. Disentangled
3d scene generation with layout learning. In International Conference on Machine Learning,
2024.
Gege Gao, Weiyang Liu, Anpei Chen, Andreas Geiger, and Bernhard Scho¨lkopf. Graphdreamer:
Compositional3dscenesynthesisfromscenegraphs. InProceedingsoftheIEEE/CVFConfer-
enceonComputerVisionandPatternRecognition,pp.21295–21304,2024.
LuyuGao, AmanMadaan, ShuyanZhou, UriAlon, PengfeiLiu, YimingYang, JamieCallan, and
GrahamNeubig. Pal: Program-aidedlanguagemodels. InInternationalConferenceonMachine
Learning,pp.10764–10799.PMLR,2023.
Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Barlas Og˘uz. 3dgen: Triplane latent
diffusionfortexturedmeshgeneration. arXivpreprintarXiv:2303.05371,2023.
Haonan Han, Rui Yang, Huan Liao, Jiankai Xing, Zunnan Xu, Xiaoming Yu, Junwei Zha, Xiu
Li, and Wanhua Li. Reparo: Compositional 3d assets generation with differentiable 3d layout
alignment. arXivpreprintarXiv:2405.18525,2024a.
JunlinHan,FilipposKokkinos,andPhilipTorr. Vfusion3d:Learningscalable3dgenerativemodels
fromvideodiffusionmodels. EuropeanConferenceonComputerVision(ECCV),2024b.
Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta denoising score. In Proceedings of the
IEEE/CVFInternationalConferenceonComputerVision,pp.2328–2337,2023.
YicongHong,KaiZhang,JiuxiangGu,SaiBi,YangZhou,DifanLiu,FengLiu,KalyanSunkavalli,
TrungBui,andHaoTan. Lrm: Largereconstructionmodelforsingleimageto3d. ICLR,2024.
YiningHong,HaoyuZhen,PeihaoChen,ShuhongZheng,YilunDu,ZhenfangChen,andChuang
Gan. 3d-llm: Injectingthe3dworldintolargelanguagemodels. AdvancesinNeuralInformation
ProcessingSystems,36:20482–20494,2023.
HeewooJunandAlexNichol. Shap-e: Generatingconditional3dimplicitfunctions. arXivpreprint
arXiv:2305.02463,2023.
BernhardKerbl,GeorgiosKopanas,ThomasLeimku¨hler,andGeorgeDrettakis. 3dgaussiansplat-
tingforreal-timeradiancefieldrendering. ACMTrans.Graph.,42(4):139–1,2023.
11Preprint
XinKong,ShikunLiu,XiaoyangLyu,MarwanTaher,XiaojuanQi,andAndrewJDavison. Escher-
net:Agenerativemodelforscalableviewsynthesis.InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pp.9503–9513,2024.
Runjia Li, Junlin Han, Luke Melas-Kyriazi, Chunyi Sun, Zhaochong An, Zhongrui Gui, Shuyang
Sun,PhilipTorr,andTomasJakab. Dreambeast: Distilling3dfantasticalanimalswithpart-aware
knowledgetransfer,2024. URLhttps://arxiv.org/abs/2409.08271.
YuhengLi,HaotianLiu,QingyangWu,FangzhouMu,JianweiYang,JianfengGao,ChunyuanLi,
andYongJaeLee. Gligen: Open-setgroundedtext-to-imagegeneration. InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.22511–22521,2023.
LongLian,BoyiLi,AdamYala,andTrevorDarrell. LLM-groundeddiffusion: Enhancingprompt
understanding of text-to-image diffusion models with large language models. Transactions on
Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/
forum?id=hFALpTb4fR. FeaturedCertification.
YixunLiang,XinYang,JiantaoLin,HaodongLi,XiaogangXu,andYingcongChen.Luciddreamer:
Towards high-fidelity text-to-3d generation via interval score matching. In Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.6517–6526,2024.
Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten
Kreis,SanjaFidler,Ming-YuLiu,andTsung-YiLin. Magic3d: High-resolutiontext-to-3dcon-
tent creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pp.300–309,2023a.
YiqiLin,HaoWu,RuichenWang,HaonanLu,XiaodongLin,HuiXiong,andLinWang. Towards
language-guidedinteractive3dgeneration: Llmsaslayoutinterpreterwithgenerativefeedback.
arXivpreprintarXiv:2305.15808,2023b.
BenMildenhall,PratulPSrinivasan,MatthewTancik,JonathanTBarron,RaviRamamoorthi,and
RenNg. Nerf:Representingscenesasneuralradiancefieldsforviewsynthesis. Communications
oftheACM,65(1):99–106,2021.
RyanPoandGordonWetzstein. Compositional3dscenegenerationusinglocallyconditioneddif-
fusion. In2024InternationalConferenceon3DVision(3DV),pp.651–663.IEEE,2024.
BenPoole,AjayJain,JonathanT.Barron,andBenMildenhall. Dreamfusion: Text-to-3dusing2d
diffusion. InInternationalConferenceonLearningRepresentations,2023.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
modelsfromnaturallanguagesupervision. InInternationalconferenceonmachinelearning,pp.
8748–8763.PMLR,2021.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo¨rn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconfer-
enceoncomputervisionandpatternrecognition,pp.10684–10695,2022.
YichunShi,PengWang,JianglongYe,LongMai,KejieLi,andXiaoYang. Mvdream: Multi-view
diffusion for 3d generation. In The Twelfth International Conference on Learning Representa-
tions,2024.
J Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner, Jiajun Wu, and Gordon Wetzstein. 3d
neuralfieldgenerationusingtriplanediffusion. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,pp.20875–20886,2023.
Alexander Vilesov, Pradyumna Chari, and Achuta Kadambi. Cg3d: Compositional generation for
text-to-3dviagaussiansplatting. arXivpreprintarXiv:2311.17907,2023.
VikramVoleti,Chun-HanYao,MarkBoss,AdamLetts,DavidPankratz,DmitryTochilkin,Chris-
tian Laforte, Robin Rombach, and Varun Jampani. Sv3d: Novel multi-view synthesis and 3d
generation from a single image using latent video diffusion. arXiv preprint arXiv:2403.12008,
2024.
12Preprint
Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. Score jaco-
bian chaining: Lifting pretrained 2d diffusion models for 3d generation. In Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.12619–12629,2023.
RuichenWang,ZekangChen,ChenChen,JianMa,HaonanLu,andXiaodongLin. Compositional
text-to-image synthesis with attention map control of diffusion models. In Proceedings of the
AAAIConferenceonArtificialIntelligence,volume38,pp.5544–5552,2024a.
Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Pro-
lificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation.
AdvancesinNeuralInformationProcessingSystems,36,2024b.
Kailu Wu, Fangfu Liu, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, Yueqi Duan, and
Kaisheng Ma. Unique3d: High-quality and efficient 3d mesh generation from a single image.
arXivpreprintarXiv:2405.20343,2024a.
ShuangWu,YoutianLin,FeihuZhang,YifeiZeng,JingxiXu,PhilipTorr,XunCao,andYaoYao.
Direct3d: Scalable image-to-3d generation via 3d latent diffusion transformer. arXiv preprint
arXiv:2405.14832,2024b.
Tong Wu, Guandao Yang, Zhibing Li, Kai Zhang, Ziwei Liu, Leonidas Guibas, Dahua Lin, and
GordonWetzstein. Gpt-4v(ision)isahuman-alignedevaluatorfortext-to-3dgeneration. InPro-
ceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.22227–
22238,2024c.
Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and
MikeZhengShou.Boxdiff:Text-to-imagesynthesiswithtraining-freebox-constraineddiffusion.
InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pp.7452–7461,
2023.
Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, and Dahua Lin. Pointllm:
Empoweringlargelanguagemodelstounderstandpointclouds.arXivpreprintarXiv:2308.16911,
2023.
HanYan,YangLi,ZhennanWu,ShenzhouChen,WeixuanSun,TaizhangShang,WeizheLiu,Tian
Chen,XiaqiangDai,ChaoMa,etal.Frankenstein:Generatingsemantic-compositional3dscenes
inonetri-plane. arXivpreprintarXiv:2403.16210,2024.
Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang,
Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and
applications. ACMComputingSurveys,56(4):1–39,2023a.
LingYang,ZhaochenYu,ChenlinMeng,MinkaiXu,StefanoErmon,andCUIBin. Masteringtext-
to-imagediffusion: Recaptioning,planning,andgeneratingwithmultimodalllms. InForty-first
InternationalConferenceonMachineLearning,2024.
XiaofengYang,YiwenChen,ChengChen,ChiZhang,YiXu,XuleiYang,FayaoLiu,andGuosheng
Lin.Learntooptimizedenoisingscoresfor3dgeneration:Aunifiedandimproveddiffusionprior
onnerfand3dgaussiansplatting. arXivpreprintarXiv:2312.04820,2023b.
MengYou,ZhiyuZhu, HuiLiu,andJunhuiHou. Nvs-solver: Videodiffusionmodelaszero-shot
novelviewsynthesizer. arXivpreprintarXiv:2405.15364,2024.
BohanZeng,ShanglinLi,YutangFeng,HongLi,SichengGao,JiamingLiu,HuaxiaLi,XuTang,
JianzhuangLiu,andBaochangZhang. Ipdreamer: Appearance-controllable3dobjectgeneration
withimageprompts. arXivpreprintarXiv:2310.05375,2023.
Xiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun,
andMing-HsuanYang. Gala3d: Towardstext-to-3dcomplexscenegenerationvialayout-guided
generativegaussiansplatting.InForty-firstInternationalConferenceonMachineLearning,2024.
Junzhe Zhu, Peiye Zhuang, and Sanmi Koyejo. HIFA: High-fidelity text-to-3d generation with
advanceddiffusionguidance. InInternationalConferenceonLearningRepresentations,2024.
13Preprint
A MORE IMPLEMENTATION DETAILS
A.1 PROMPTSFORPROGRAM-AIDEDLAYOUTPLANNING
You are a proficient 3D scene designer with the ability to effec�vely posi�on 3D models within a 3D cubic space. Using a provided
scene descrip�on, please carry out the following tasks:
1. Iden�fy 3D Models:
-Iden�fy and list the 3D models men�oned in the descrip�on.
2. Write Python Code to Es�mate Dimensions and Arrange 3D models:
-Measure the dimensions of each 3D model as if they were toys. They don't need to be realis�c, but they should fit togetherin one
cubic space. Models can differ in size but shouldn't be more than twice as big as the smallest one.
-Arrange the iden�fied 3D models in a 3D cubic space centered at coordinates [0, 0, 0], with measurements in cen�meters. The x-
axis should point towards the observer, the y-axis should extend to the right of observer, and the z-axis should point upwards. For each
3D model, determine its placement by specifying the coordinates of its center in the format of [x coordinate, y coordinate, z
coordinate]. Ensure that the 3D models are posi�oned in a plausible manner, avoiding overlaps or extending beyond the confines of
the defined space. If necessary, you may make educatedes�ma�ons to achieve a coherent arrangement.
Here are some examples, follow the example to design the 3D scene:
Scene descrip�on: ……
Let's think step by step and write the python codes.
Task 1: Iden�fy Models. Iden�fy and list the 3D models men�oned in the descrip�on. If two models are closely associated, I will
iden�fy them as one model.
From the scene descrip�on, the tangible models men�oned are:
…..
models = {
"corgi": {"object descrip�on": "Corgi dog"},
"beret": {"object descrip�on": "Beret hat"},
"house": {"object descrip�on": "The house straddles a divide that separates spring and winter horizontally."},
"car": {"object descrip�on": "Car, with front layer made of wood and rear layer made of sushi and cheese. The le� half of the rear
layer is made of sushi, and the right half is made of cheese."}
}
Task 2: Es�mate Dimensions. For the scene descrip�on that involves a corgi, a beret, a house, and a car with layered materials, let's
es�mate the dimensions of each model to ensure they fit within a unified cubic space.
models["corgi"]["dimension"] = {"x": 30, "y": 15, "z": 20} # cm
models["beret"]["dimension"] = {"x": 15, "y": 15, "z": 5} # cm, diameter fi�ng the corgi's head, thickness/height when laid flat
models["house"]["dimension"] = {"x": 45, "y": 30, "z": 35} # cm, the size of a house can vary widely. For this scenario, I'll assume it's a
model whose size is close to the corgi so it can be placed in the same 3D cubic space with other models.
models["car"]["dimension"] = {"x": 40, "y": 20, "z": 15} # cm, toy car size fi�ng the scene
Task3: Calculate the posi�ons considering viewing from the front. Arrange the iden�fied 3D models in a 3D cubic space centered at
coordinates [0, 0, 0]. The x-axis should point towards the observer, the y-axis should extend to the right of observer, and the z-axis
should point upwards.
We'll start by placing the house at the center, then posi�on the corgi and car rela�ve to the house, and finally, place theberet on top
of thecorgi.
……
# Posi�on the corgi to the le� of the house, on the ground
models["corgi"]["posi�on"] = {
# Centered on x-axis, aligned with the house
"x": 0,
"y": models["house"]["posi�on"]["y"] -(models["house"]["dimension"]["y"] / 2 + models["corgi"]["dimension"]["y"] / 2 + 5),
# Half the height of the corgi off the ground to represent the corgi si�ng on the ground
"z": models["corgi"]["dimension"]["z"] / 2
}
……
Scene descrip�on: {{user_prompt}}
Let's think step by step and write the python codes.
Figure8: Thepromptforscene-leveldecompositioninprogram-aidedlayoutplanning.
Large Language Models (LLMs) have the potential for spatial awareness; however, precise 3D
layout generation from vague language descriptions is challenging. This difficulty arises because
3D digital data and corresponding natural language descriptions often do not appear simultane-
ously (Hong et al., 2023; Xu et al., 2023). Moreover, minor numerical changes, which might not
bereflectedinimpreciselanguage, canleadtounrealisticspatialarrangementsof3Dscenes. Ad-
ditionally, thespatialarrangementofmulti-objectscenesrequiresnumerousparameters, makinga
program-aidedapproachnecessarytobridgethegapbetweennaturallanguagedescriptionsand3D
digitaldata.
Specifically, we decompose the process of generating multiple objects with diverse attributes into
twosteps: scene-leveldecompositionandobject-leveldecomposition. Inscenedecomposition,we
guideLLMstotranslateuserpromptsintoPythonprograms,usingexplicitmathematicaloperations
14Preprint
As a 3D model designer, you are tasked with designing an object described in the user prompt. This object has mul�ple a�ributes, with
different parts possessing different a�ributes. Your job is to divide the object as described in the user prompt into parts,each with a
singlea�ribute, and rewrite the corresponding prompt for each part. Specifically, you need to divide the 3D bounding box
encompassing the object into different complementary smaller bounding boxes, and output in the specific format.
# The specific format descrip�on
The output should be a JSON object that represents the 3Dbounding box of the object. This object should have a key named "depth
split" that contains an array of objects. Each object represents a division of the bounding box along the depthaxis. The objectshould
have two keys: "size" and "ver�cal split". The "size" key represents the size of this part rela�ve to other parts in the same split.
The "ver�cal split" key should contain an array of objects. Each object represents a division of the bounding box along the ver�cal axis.
The object should have two keys: "size" and "horizontal split".
The "size" key represents the size of this part rela�ve to other parts in the same split.
The "horizontal split" key should contain an array of objects. Each object represents adivision of the bounding box along the horizontal
axis. The object should have two keys: "size" and "prompt".
The "size" key represents the size of this part rela�ve to other parts in the same split.
The "prompt" key should contain the prompt for the specific part of the object. The prompt should be a string that describes thepart
of theobject and its single a�ributes.
# Examples
......
Figure9: Thepromptfordecomposingeachobjectintocomplementaryregions.
torepresentrelationshipsbetweenobjects. Forobjectdecomposition,sincecomplementaryregions
aredesignedto benon-overlappingandcollectivelyencompass theentirelayoutspaceof their re-
spective objects, we devised a scheme employing structured JavaScript Object Notation (JSON)
to represent hierarchical divisions based on depth, width, and length dimensions. Figures 8 and 9
illustratethedetailedpromptsforsceneandobjectdecomposition,respectively.
A.2 SEMANTICSDS
CameraSampling Trainingalternatesbetweenlocalandglobaloptimization. Duringlocalopti-
mization,objectsarenottransformedintoglobalcoordinates. Inglobaloptimization,therendering
ofobjectsvariesbyswitchingbetweentheentiresceneandpairsofobjectstobetteroptimizethose
that interact or occlude each other. When rendering only a pair of objects, the camera’s look-at
pointissampledatthemidpointbetweenthetwoobjectsratherthanthecenteroftheentirescene.
Additionally, we apply a dynamic camera distance from the object pair to ensure the objects are
appropriately sized inthe rendered images. Specifically, the camera distanceis determined by the
scaleoftheobjectsandthedistancebetweentheircenters.
Pooling of Semantic Masks Given that the rendered RGB images and the semantic map have
sizesof512×512,whereasthelatentsfordenoisingareofsize64×64,weconvertthesemantic
mapSintomaskstocomposethedenoisingscorespredictedbydiffusionmodels.Subsequently,for
eachmaskM ∈{0,1}512×512,weapplyaveragepoolingwithastrideof8usingan8×8kernel
k,l
todownsamplethedata. ToensurethatGaussiansneartheedgesofobjectsandisolatedGaussians
arenotoverlooked,themaskM undergoesamaxpoolingoperationwitha5×5kernel,resulting
k,l
inMˆ .
k,l
Compositional Optimization Scheme The compositional optimization scheme encompasses
both global scene and local object optimizations. Only global scene optimizations apply affine
transformationstoconvertobjectsfromlocaltoglobalcoordinates. Duringlocaloptimization,θin
equation 9 includes the mean, covariance, and color of individual Gaussians. In global scene op-
timization,θadditionallyincludestheparametersofaffinetransformations—translation,scale,and
rotation—thatconvertlocaltoglobalcoordinates.
A.3 DETAILSOFMETRICS
CLIPScore TheCLIPscoreutilizesCLIPembeddings(Radfordetal.,2021)toevaluatetext-to-
3Dalignment. Followingpreviousmethods(Zhouetal.,2024;Gaoetal.,2024), wecalculatethe
cosine similarity between the user prompt and scene images rendered from different perspectives.
For each scene, we take the maximum CLIP score from all rendered images as the representative
score. We then compare the average of these maximum scores across different scenes for each
method.
15Preprint
Our task is to evaluate two complex 3D scenes that have been generated from the specific user prompt
"{{user_prompt}}". I will provide you with images of these scenes, specifically image renderings, for each
method used.
We want to assign a score from 1 to 100 (where 1 is the lowest and 100is the highest) according to the
providedfourcriteria:
1. User Prompt & Scene Alignment: Assess whether all objects men�oned in the user prompt
"{{user_prompt}}" are present in the 3D scenes generated by both methods and whether the quan�ty of
eachtypeofobjectmatchesthenumbersspecifiedintheprompt.Describeeachscenebrieflyandthen
evaluate the completeness and accuracy in replica�ng the described elements for both methods.
2. Spa�al Arrangement of Objects: Look at the RGB images to assess the arrangement and posi�oning of
objects within the scenes. Determine whether the spa�al rela�onships and layout of objects appear logical
andconducivetothescene'sintendedfunc�onorthemeforbothmethods.
3. Geometric Fidelity:Examine each object within the scenes through the RGB images for both methods.
Evaluatetheoverallshapeandstructureofeachobject,checkingforanygeometricinconsistenciesor
distor�ons that might affect the object's realis�c representa�on.
4. Overall Scene Quality: Evaluate the overall coherence and technical quality of the scenes as a composite
assessment, based on the integra�on of user prompt alignment, spa�al arrangement, and geometric fidelity.
Considerfactorslikevisualharmonyandtechnicalexecu�oninyouroverallassessment.
For each of the criteria, you will need to provide a score from 1 to 100 for each method. Addi�onally,
provideashortanalysisforeachoftheaforemen�onedevalua�oncriteriaforbothmethods.Theanalysis
should be very concise and accurate.
Let's step by step analyze the alignment of the scenes with the user prompt "{{user_prompt}}" and proceed
to score and describe each method systema�cally.
# Example output:
Analysis:
1. User Prompt & Scene Alignment:
-Method A: The scene includes objects such as trees, benches, and lamps; Score: 85
All described objects are present, and the quan��es are mostly accurate with minor devia�ons.
-Method B: The scene includes the same objects but with slight varia�ons in quan�ty; Score: 80
Mostobjectsarepresent,buttherearenotablediscrepanciesinobjectcount.
2. Spa�al Arrangement of Objects:……
3. Geometric Fidelity:……
4.OverallSceneQuality:……
Final scores:
-Method A: 85, 78, 82, 90
-Method B: 80, 83, 88, 84
Figure10: ThepromptforguidingGPT-4asahuman-alignedevaluator
GPT-4VasAHuman-AlignedEvaluator DuetothelimitationsoftheCLIPscoreincapturing
spatialarrangementandgeometricfidelity,wefollowWuetal.(2024c)andemployGPT-4Vtoeval-
uatecomplex3Dscenesinvolvingmultipleobjectswithvariedattributes. Specifically,weprovide
GPT-4Vwithrenderedimagesofthesame3Dscenegeneratedbydifferentmethodsandrequireit
toscoreeachsceneonfouraspects: PromptAlignment, SpatialArrangement, GeometricFidelity,
and Scene Quality, each on a scale from 1 to 100. For each scene and method pair, we perform
threeindependentevaluations. Thefinalscoreforeachmethodisobtainedbyaveragingthescores
acrossdifferentscenesandcomparisonswithothermethods. Figure10presentsthepromptusedto
guidetheGPT-4Vevaluator. Intheprompt,”methodA”and”methodB”areusedtoanonymizethe
methods,preventingnamebiasinGPT-4V’sjudgment.
B MORE SYNTHESIS RESULTS
16Preprint
A cozy scene with a plush triceratops toy surrounded by a plate of chocolate chip cookies, a
glistening cinnamon roll, and a flaky croissant.
Ahamburger, a loaf of bread, an order of fries, and a cup of Coke.
A glass block, a wooden block, a stone block, and a glowing lamp are displayed. They are
arranged sequentially from left to right: the wooden block is first, followed by the stone block,
then the glass block, and the glowing lamp is placed at the back of the stone block.
A table with a roasted turkey, a salad, a loaf of French bread, a glass of orange juice and plate.
A puppy is lying on the iron plate at the top of the Great Pyramid, which is made of snow bricks
and stone bricks.
A camping scene with a tent and two wooden stools with colorful patterns next to a campfire.
A white cat lies on a plank of wood, flanked by two sparkling balloons, one orange and one blue.
Figure11: MoresynthesisresultsofmultipleobjectswithourSEMANTICSDS.
17Preprint
A castle madeof snow bricks and stone bricks.
A pyramid-shapedburritoartistically blended with the Great Pyramid.
A train with a frontof cake and a back of steam engine.
A tray of sushi,applesand oranges.
A bust of TheodorosKolokotronismade of bronze and marble.
A motorcyclemadeof amigurumi and origami.
Figure12: MoresynthesisresultsofsingleobjectwithdiverseattributeswithourSEMANTICSDS.
18