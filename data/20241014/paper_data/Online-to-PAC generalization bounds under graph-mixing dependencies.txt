Online-to-PAC generalization bounds under
graph-mixing dependencies
Baptiste Ab´el`es Eugenio Clerico Gergely Neu
Universitat Pompeu Fabra, Barcelona,Spain
Abstract algorithmstrainedonsuchcorrelateddatasets,where
dependencies are encoded in graph structures.
Traditional generalization results in statisti- In machine learning, a model’s gap in accuracy on
cal learning require a training data set made training data and new, previously unseen, inputs is
of independently drawn examples. Most of known as generalization error. Controlling this quan-
the recent efforts to relax this independence tityofferstheoreticalguaranteesonthe typicalperfor-
assumption have considered either purely mance on future data, reflecting the algorithm’s abil-
temporal (mixing) dependencies, or graph- ity to infer patterns (Shalev-Shwartz and Ben-David,
dependencies, where non-adjacent vertices 2014). In the past decades, a vast body of literature
correspondtoindependentrandomvariables. on this area has emerged, developing tools such as
Both approaches have their own limitations, Rademacher complexity, VC dimension, uniform sta-
the former requiring a temporal ordered bility, and PAC-Bayesian inequalities (Vapnik, 2000;
structure, and the latter lacking a way to Bousquet and Elisseeff, 2002; Bousquet et al., 2004;
quantify the strength of inter-dependencies. Alquier, 2024). Yet, the great majority of current
Inthiswork,webridgethesetwolinesofwork analyses consider training data sets made of indepen-
by proposing a framework where dependen- dent and identically distributed (i.i.d.) examples, a
cies decay with graph distance. We derive strong requirement unrealistic for many applications
generalization bounds leveraging the online- (e.g., traffic forecasting (Yu et al., 2018), stock price
to-PAC framework, by deriving a concentra- prediction(Ariyo et al.,2014),ortheexamplesabove).
tionresultandintroducinganonlinelearning
Recently, interest in statistical learning frameworks
frameworkincorporatingthegraphstructure.
accounting for data correlations has surged. A ma-
The resulting high-probabilitygeneralization
jorresearchlinemodelsthesedependenciesviamixing
guarantees depend on both the mixing rate
assumptions (see Bradley 2005 for several notions of
and the graph’s chromatic number.
mixing, such as α-, β-, φ-, and ψ-mixing), which con-
trol how quickly the influence between random vari-
1 INTRODUCTION ables decays as the (temporal, spacial, etc.) distance
between them grows. This setting provide a quanti-
tative measure of the dependencies among the data
Considerthe problemofpredictinghouse pricesbased
points, but has the major drawback of requiring data
ondatacollectedfromavarietyoflocations. Thevalue
to have a well defined ordered structure. An alter-
does not only depend on factors like home size, age,
native common framework takes a more qualitative
and amenities, but is also influenced by the neighbor-
approach, where the dependencies are captured by a
hood. In the language of probability theory, this can
dependency graph that assigns an edge to any pair of
be modeled with a set of dependent randomvariables,
vertices whose associated data are dependent. This
withpricesofneighboringhousesshowingpositivecor-
approach can encode correlations among non-ordered
relation that decays with distance. Similar dependen-
data, but leads to loose results when the actual de-
cies occur between users’ opinions on social networks,
pendencies are weak (Janson, 2004). In this work, we
where connected members are more likely to share
propose combining the mixing and graph-based per-
similar views (Montanari and Saberi, 2010). In this
spectivestotacklesituationswherethestrengthofthe
paper, we study the generalization ability of learning
dependencies is somehow known, yet the data lack an
ordered structure. In this graph-mixing scenario, the
correlations decay as the graph distance increases.
© BaptisteAb´el`es,EugenioClerico,andGergelyNeu.
4202
tcO
11
]LM.tats[
1v77980.0142:viXraOnline-to-PAC generalization bounds under graph-mixing dependencies
To proveour generalizationresults, we follow an algo- stationary conditions were first established by
rithmicapproachthatderivesguaranteesforstatistical Mohri and Rostamizadeh(2008)viatheblockingtech-
learning using tools for regret analysis in online learn- nique from Yu (1994), and later extended to the
ing. Online learning is a framework that deals with non-stationary case by Kuznetsov and Mohri (2017).
sequential decision problems, where a learner (a.k.a. Excess risks bounds, comparing the algorithm’s out-
player) interacts with an evolving environment. The put with the best predictor in some given class,
learner’sgoalistoselectactionsovertimetominimize were established by Steinwart and Christmann (2009)
the regret (a quantity comparing the player’s actions under geometrically α-mixing conditions. Later,
to the best fixed action in hindsight). We refer to Alquier and Wintenberger (2012) and Alquier et al.
Cesa-Bianchi and Lugosi (2006) and Orabona (2019) (2013) employed PAC-Bayesian tools to upper bound
for thorough introductions to the subject. A recent the excess risk in model selection, when data are
line of research (commonly called algorithmic statis- coming from a time series generalizing the stan-
tics)hasexploredunconventionalwaystotackleclassi- dard notion of mixing (following ideas by Rio 2000).
calstatisticalchallenges,drawingconnectionswithon- Excess risk was also studied by Agarwaland Duchi
linelearning. Thisapproachhassuccessfullyaddressed (2013), who extended the online-to-batch conversion
problems such as hypothesis testing (Gru¨nwald et al., of Cesa-Bianchi et al. (2001) to the case of β- and φ-
2024), decision making (Foster et al., 2021), mean es- mixing data, under the hypothesis of a bounded, con-
timation (Orabona and Jun, 2024), and martingale vex, and Lipschitz loss function. Finally, we mention
concentration (Rakhlin and Sridharan, 2017). This thetworesultsthataretheclosesttoourcurrentwork.
strategy has also been applied to study generaliza- Both Chatterjee et al. (2024) and Abeles et al.(2024)
tion in statistical learning, for instance leading to build on the online-to-PAC framework introduced by
Rademacher (Kakade et al., 2008) and PAC-Bayesian Lugosi and Neu (2023). The former follow the ap-
bounds (Jang et al., 2023; Lugosi and Neu, 2023; proach of Agarwal and Duchi (2012) to deal with φ-
Abeles et al., 2024; Chatterjee et al., 2024). Most of andβ-mixingstationarydependencies,andhenceneed
these methods split the problem into two parts, a strong regularity for the losses. Conversely, the lat-
worst-case one that is dealt with online regret anal- ter consider a slightly different definition of station-
ysis, and a probabilistic one. When studying gener- arymixingand(aspreviouslymentioned)leveragethe
alization with i.i.d. data (as in Jang et al. 2023 and frameworkof online learningwith delayedfeedback to
Lugosi and Neu 2023), the probabilistic part reduces perform the online-to-PAC reduction.
toupperboundingthedeviationsofamartingale. This
For the standard graph-dependence setting (with in-
martingalestructureislostifdependenciesamongthe
dependent variables for non-adjacent vertices) a clas-
training data are present. Abeles et al. (2024) ad-
sical result is a Hoeffding-flavored concentration in-
dressed this issue for mixing processes, imposing a
equality by Janson (2004), where the graph’s frac-
delayed-feedback constraint on the player’s strategy
tional chromatic number (a graph-theoretic combi-
in the framework of Lugosi and Neu (2023), which al-
natorial quantity) appears and re-weights the sam-
lowedthemtodecomposetheoverallerrorintothere-
ple size. The core idea in Janson’s proof consists
gretofadelayedonlinelearningstrategyandthe fluc-
in splitting the graph into sets whose vertices are
tuations of a stationary mixing process. Here, we fol-
non-adjacent (and hence independent), an approach
low a similar approachto tackle more complex graph-
that resembles the blocking technique from Yu (1994)
mixing dependencies, by introducing a novel online
for mixing processes. The first generalization bound
learning framework on graphs, and studying the con-
in this framework was motivated by ranking (whose
centration of what we name graph-mixing processes.
lossformnaturallyleadstothesegraph-dependencies)
Several works established generalization guarantees and obtained by Usunier et al. (2005), who built on
under mixing assumptions. Mohri and Rostamizadeh Janson (2004) to establish Rademacher-like bounds.
(2007, 2010) obtained generalization bounds for Later, Ralaivola et al. (2010) derived PAC-Bayesian
uniformly stable algorithms under stationary φ- bounds,againleveragingthesameblockingtechnique.
and β-mixing, leveraging concentration tools from More recently, Zhang et al. (2019) proved a novel
Yu (1994) and Kontorovichand Ramanan (2008). McDiarmid-typeinequalityfortree-dependentrandom
Fu et al. (2023) tightened these results, adapt- variables,andextendedittogeneralgraphsbydecom-
ing stability techniques from Feldman and Vondrak posingthemintoforests. Viathisconcentrationresult
(2019) and Bousquet et al. (2020) to achieve op- they established generalization bounds for uniformly
timal rates under ψ-mixing assumptions. Stabil- stable algorithms. We refer to Zhang and Amini
ity bounds were also proved by He et al. (2016), (2024) for a recent survey on these and other results
in the context of ranking, under φ-mixing as- of generalizationon graphs.
sumptions. Rademacher bounds under β-mixingBaptiste Ab´el`es, Eugenio Clerico, Gergely Neu
Finally,ofspecialinterestistheworkofLampert et al. the expected value of this quantity. For a probability
(2018), establishing concentration inequalities for the measure P ∆ , with a slight abuse of notation, we
∈ W
sum of random variables with graph-encoded corre- define the expected population loss as (P) = P, ,
L h Li
lations. Their way to deal with dependencies shares where P,f denotestheexpectationunderP ofamea-
h i
many similarities with our model, which could actu- surable function f on . Similarly, we define the ex-
W
ally be seen as an instance of their broader setting. pected empirical loss as (P)= P, .
n n
L h L i
However,thishighergeneralitycomesatthepriceofa
For convenience, we denote the output of a random-
ratherconvolutedtechnicalanalysis,introducingmore b b
ized algorithm as P = (S ) ∆ . We stress
complex notions of interdependence. Their approach A n A n ∈ W
here that P is a stochastic quantity, as it depends on
is based on an approximation theorem from Bradley n
the randomtrainingdbata setS . Hence, the expected
(1983) that allows to replace a set of dependent ran- n
populationbloss (P ) is stochastic. We call general-
dom variables with independent copies, at a price of L n
ization bound ahigh-probabilityinequalityintheform
an additive term involving a suitably defined separa-
b
tion coefficient. Another closely related approach is
µ (P ) (P ),δ 1 δ, (1)
the work of F´eray (2018), proving central limit theo- n L n ≤B Ln n ≥ −
remsbyencodingdependenciesintoaweightedgraph, (cid:16) (cid:0) (cid:1)(cid:17)
where is sombe functionb anbd δ [0,1] is the confi-
whose edges’ weights measure the strength of the de- B ∈
dence level. For the sake of brevity, we introduce the
pendencies. Inthe presentpaper,we optedto develop
notation forinequalitiesholdingwithprobabilityat
δ
a simpler framework which, while slightly less flexi- ≤
least 1 δ, and (1) becomes (P ) ( (P ),δ).
n δ n n
ble,allowedustoconductamoretransparentanalysis, − L ≤ B L
easier to adapt to practical needs. We defer to future
2.1 Online-to-PAC reductibon b b
research combining our analysis with the techniques
developed in these two works.
Lugosi and Neu (2023) have recently established a
framework, which they named online-to-PAC conver-
sion, to obtain generalization bounds for statistical
2 THE GENERALIZATION
learning (in the i.i.d. setting) by upper bounding the
PROBLEM
regret of an online learner in the following associated
online learning game.
WeconsideradatasetS n =(Z 1,...,Z n),drawnfroma Definition1 (Generalizationgame). Fixan arbitrary
apr (o mba eb asil uit ry abd li es )tr ii nb su tt ai no cn eµ sn pao cv ee .r WZn e, aw ssh ue mre eZ thd ae tn eo at ce hs d aa nta ons le int eS pn lay= er( aZ n1 d,. a.. n,Z adn v) er∈ sarZ yn p. layFo tr hen for lo lou wn id ns g,
Z i has the same marginal µ. The simplest situation game. At each round t=1,2,...,n:
is when all the element of S are i.i.d., in which case
µ
n
=µn,butwewillfocusonn
moregeneralsituations.
• the online learner picks a distribution π
t
∈∆ W;
Wedenoteas ameasurableclassofhypotheses,and • theadversarypicksamapg :w (w) ℓ(w,Z );
t t
W 7→L −
we let ℓ : [0, ) be the loss function, with • the learner incurs a cost π ,g ;
W ×Z → ∞ t t
ℓ(w,z)measuringthequalityofthehypothesisw −h i
∈W • Z is revealed to the learner.
on the instance z . The statistical learner’s goal t
∈ Z
is to find a hypothesis that performs well on average,
Let Π = (π ) be an online strategy for the game
t t 1
ideally the w that minimizes the population loss ≥
(w)=E [ℓ(w∈ ,W Z)]. Yet, µ isunknownto thelearner, above. Weremarkthatthelearner’schoiceofπ thasto
L µ be done before Z t is revealed, and so can only depend
whoseonlyknowledgecomesfromthetrainingdataset
onthe pastobservations(uptoroundt 1). Fixedan
S . We define the empirical loss to be the average of −
n arbitraryP ∆ ,wedefinetheregretofΠagainstP
ℓ on the training data set, Ln(w)= n1 n t=1ℓ(w,Z t). atroundna∈ sR ΠW ,n(P)= n t=1( hP,g
t
i−hπ t,g
t
i). The
A learning algorithm is a procedure toPget a hypothe- online-to-PAC reduction is the next decomposition.
b P
sis w starting from a training data set S n. More Theorem 1 (Theorem 1, Lugosi and Neu 2023). Fix
∈W
generally,wewillconsiderarandomized learningalgo- anyonlinestrategyΠforthegeneralizationgame. Any
rithm, that is, a mapping : n ∆ , where ∆ statistical learning algorithm P = (S ) satisfies
A Z → W W n A n
denotes the set of probabilities over . Note that de-
W 1
terministic algorithms(mapping S n to asingle w)can (P n) n(P n)+ Rb Π,n(P n)+M Π,n , (2)
be seenasa particularcaseofthe randomizedsetting, L ≤L n
where the output distribution is a Dirac mass. As where Mb = bn b π ,g (cid:0) . b (cid:1)
previously mentioned, the ultimate goal of the statis- Π,n t=1h t t i
tical learner is to optimize the population loss. In the AkeyremarktoPmakeuseofthisdecompositioncomes
context of randomized algorithms, we aim to control from the fact that, when the training data set S is
nOnline-to-PAC generalization bounds under graph-mixing dependencies
drawn from µn (and hence i.i.d.), the negation M , can be controlled by stationary mixing assumptions.
Π,n
of the online learner’s cumulative cost, is a martin- The analysis of Chatterjee et al. (2024) was inspired
gale under the natural filtration induced by S , that byAgarwal and Duchi (2013)andinvolvescontrolling
n
is, the σ-fields = σ(X ,...,X ). This follows from theconcentrationpropertiesofM understrongreg-
t 1 t Π,n
F
the fact that the online strategy is a predictable se- ularityassumptionsfortheloss,leavingtheonlinefor-
quence of actions, as π does not depend on Z and mulationuntouched. Onthe otherhand, Abeles et al.
t t
is -measurable.1 In particular, one can leverage (2024)tookaperhapsmorenaturalperspective. They
t 1
F −
classical martingale concentration results to get high introducedadelayedfeedbackintheonlinegeneraliza-
probability generalization bounds in the form of (1). tiongame(a delayofdmeansthatZ is only revealed
t
We remark that in practice it is not necessary to ac- atroundt+d),ensuringthatM becomesastation-
Π,n
tually play the generalization game. Indeed, one can ary mixing process, whose concentration can be con-
replace the regret R by an upper bound, whenever trolled via a standard blocking technique (Yu, 1994).
Π,n
this is known. The study and derivation of regret up- Our current work extends this approach to more gen-
per bounds is a main topic of interest in the online eral dependencies, encoded by a graph. To do so,
learning community. we need to introduce a suitable online framework for
learning on graphs that generalizes the online learn-
For a concrete application of the above observa-
ing with delays setting. This will ensure that M
tions, we state a corollary of Theorem 1, which Π,n
is a sum of terms whose correlations can be suitably
uses the parameter-free online strategy introduced by
controlled, allowing to obtain high-probability gener-
Orabona and Pa´l (2016) for learning with expert ad-
alizationguarantees. Theseideaswillbe formalizedin
vice.
the nextsection,after the definitionsofseveralgraph-
Corollary 1(Corollary6,Lugosi and Neu2023). As-
theoretic concepts.
sumethatℓisboundedin[0,1],fixδ (0,1)andanar-
∈
bitrary P ∆ (whose choice cannot depend on S ).
∈ W n 3 TECHNICAL TOOLS
Then, the following generalization bound holds in high
probability, uniformly for all algorithms P = (S ),
n n
A Asalreadymentioned,wewillmodeldependenciesbe-
3KL(P P)+9b log1 tweenrandomvariablesinthelanguageofgraphs,and
(P n) δ n(P n)+ n | + δ . willextendtheonline-to-PACconversionframeworkof
L ≤ L s n s 2n
Lugosi and Neu(2023)todealwithdatapointswitha
b
b b b graph-dependencystructure. Thissectionpresentsthe
We notice that the above result is in the typical form
technicalbackgroundthatisnecessaryforformulating
of a PAC-Bayes bound (Guedj, 2019; Alquier, 2024),
our assumptions on the data, and formulates an on-
whichtypicallyinvolvesacomplexitytermintheform
line learning framework defined on a graph structure,
of the relative entropy, KL, between a data-agnostic
which will serve as basis for our reduction.
prior P andthedata-dependentposterior P . Indeed,
n
the framework introduced by Lugosi and Neu (2023)
3.1 Basic definitions
allowsto recoverseveralclassicPAC-Bayesbianresults,
and provides a range of generalizations thereof.
We first introduce here a few basic definitions related
tographs,whichwillbeusedthroughoutouranalysis.
2.2 Going beyond the i.i.d. assumption
Definition 2. A graph G is a pair of sets (V,E).
As it is the case for Corollary 1, also the other appli- The elements of V are called vertices, or nodes, and
cations of Theorem 1 in Lugosi and Neu (2023) lever- the elements of E are called edges. Each edge is an
age the fact that M Π,n is a martingale to derive high- unordered pair of elements of V.
probability generalization bound. However, as pre-
viously mentioned, this approach cannot be directly We willonly considerlooplessgraphswhereeachedge
applied when inter-dependencies among the training includestwodistinctvertices. GivenagraphG,theset
data are present, as these can prevent M from be- of its vertices is denoted as V(G), while E(G) refers
Π,n
ing a martingale. Two solutions (Chatterjee et al., to its edges. Two vertices u and v of G are said to
2024; Abeles et al., 2024) have been recently pro- be adjacent if u,v is an edge in E(G), otherwise
{ }
posed to extend the online-to-PAC reduction to sit- they are called non-adjacent. The number of edges a
uations where the correlationsin the training data set vertex v belongs to is called the degree of v, and the
degree of the graph is defined as the highest degree
1In general, one could let π t depend on other sources
among all its vertices. The order of a graph is the
of randomness, not encoded in the data. This can be ad-
dressed by suitably adapting the filtration, but leaves all numberofitsvertices. Asequenceofedgesintheform
theresults that we present unchanged. v 0,v 1 , v 1,v 2 ... v t 1,v t iscalledapath oflength
{ } { } { − }Baptiste Ab´el`es, Eugenio Clerico, Gergely Neu
t, connecting v to v . Two vertices are connected if 3.2 (G,φ)-mixing processes
0 t
there is a path connecting them. We define the graph
distance dist (u,v) as the length of the shortest path We will consider a dependency structure between the
G
from u to v. If u and v are not connected, then we let training data S n = Z 1,...,Z n specified in terms of
{ }
dist (u,v)=+ . a graph G = (V,E), with the set of nodes V asso-
G
∞ ciated to the set of data points, and the edges E de-
A subset S of V(G) is called a stable subset of the
scribingthepairwisedependenciesbetweenthem. The
graph G if any two vertices u and v in S are non-
strength of the dependence between any two points
adjacent.2 A family S of stable subsets of G is
k k Z and Z is assumed to decay with the graph dis-
{ } i j
a stable cover if S = V(G). Moreover, a stable
k k tance between the corresponding nodes v and v in
∪ i j
coversuchthatallthe S aredisjointiscalledastable
k the graph, with the graph distance between any pair
partition of G. The chromatic number χ of a graphG
(u,v) defined as the length of the shortest path be-
is the cardinality ofthe smallest stable partitionof G,
tween the two nodes. In order to define the precise
namelytheminimumnumberofstablesubsetsneeded
dependence structure between the data points (which
to form a stable partition of G.
will be formalisedin Assumption 1), we will make use
More broadly, one can consider weighted families of the concept of a dependence structure that we call
(w ,S ) of stable subsets of G, where the w are a (G,φ)-mixing process, defined as follows.
k k k k
{ }
non-negative coefficients. A stable fractional cover is Definition 6. Let X = X be a family of
G v v V(G)
aweightedfamilysuchthat w 1 1,foreach centred random variables, l{ abel} led∈ on a graph G. We
vertex v V(G). If w
1k k =v ∈ 1Sk fo≥
r any v, we say that X is a (G,φ)-mixing process if there exists
∈ k k Pv ∈Sk G
speak of a stable fractional partition. The fractional a non-negative non-increasing sequence φ = (φ )
d d>0
P
chromatic number χ of G is the minimal value of such that, for any v V,
f
∈
w , among all the stable fractional partitions of
k k E[X ] φ ,
G. As any stable partition is a stable fractional parti- v v,d d
|F ≤
P
tion with all the weights set to 1, we see that χ χ.
f ≤ where v,d =σ( X v′ :dist G(v,v ′) d ).
F { ≥ }
The previous definitions can be generalizedby replac-
WhenGisachain(withnodesindexedbytimet,and
ing the non-adjacency condition with one involving a
edges connecting consecutive time indices), the above
minimal distance. We give formal definitions for the
definitioniscloselyrelatedtostandardmixingassump-
resultingobjects,whichplayakeyroleinouranalysis.
tions, suggesting that the process effectively forgets
Definition 3. A d-stable subset S of G is a subset randomvariablesthataresufficientlyfarapartintime.
of V(G) such that dist (u,v) d for any two distinct The two main differences are that our condition fo-
G
≥
elements u and v in S. cuses on expectations rather than total variation dis-
tance(oralike),and,sinceweuseundirectedgraphs,it
doesnotaccountforthedirectionoftime asintypical
Notethatthe 2-stablesubsetsofGareexactlyits sta-
mixingprocesses. Furthermore,thegraph-dependency
ble subsets, while any subset of V(G) is 1-stable.
structure considered by Janson (2004), Usunier et al.
Definition4. Ad-stablefractionalpartitionofGisa (2005), and Zhang et al. (2019) is recovered by let-
weighted family of d-stable subsets of G, (w ,S ) , ting φ be a threshold sequence, such that φ = 0 for
k k k d
{ }
such that w 1 =1 for all v V(G). all d > d⋆, and φ = + , for d d⋆. In a way,
k k v ∈Sk ∈ d ∞ ≤
the (G,φ)-mixing processes capture both the qualita-
DefinitioPn5. The fractionald-chromaticnumberχ(d)
f tiveaspectofthestandardgraph-dependence,andthe
of G is the minimal value of w , among all the d-
k k quantitative side of the mixing conditions.
stable fractional partitions of G.
P Intuitively, one can expect the empirical mean of
(G,φ)-mixing processes to concentrate around their
Anotherwayofthinkingaboutd-stablesetsisinterms
true mean (zero) at a rate that is determined by the
ofpowergraphs. Thed-thpowergraphofGisagraph
overall strength of dependencies: densely connected
Gd such that V(Gd) = V(G), with an edge for any
graphs are expected to yield poor concentration as
two vertices whose distance (in G) is at most d. The
comparedtographswithfewerconnections. Themea-
d-stable subsets of G are exactly the stable subsets of
sure of “connectedness” of the graph that we use is
Gd 1, and therefore χ(d) =χ (Gd 1).
− f f − the fractional d-chromatic number χ(d). The following
f
proposition provides a bound on the empirical mean
of (G,φ)-mixing processes with bounded range.
2Stable subsets are also known as independent subsets.
Howeverwepreferredthe(alsocommonlyused)term‘sta- Proposition 1. Let X G be a (G,φ)-mixing process,
ble’ to avoid confusion with probabilistic independence. where G is a graph of order n. Assumeall the X v takeOnline-to-PAC generalization bounds under graph-mixing dependencies
values in a bounded interval of length ∆, are centered, Proposition 2. Assume that, for all v V(G), the
∈
and have all the same marginal distribution. Then, costC isconvexina. Ifthereexistsastandardonline
v
for any δ >0, the following high probability inequality strategy Π achieving regret R (a) F(T) for any
Π,T
≤
holds: T n, where F is a concave function, then, for any
≤
d [n], there is a d-sheltered learner with strategy Π ,
d
1 ∆2χ(d) 1 wh∈ ose regret is bounded as
X min φ + f log .
v δ d
n ≤ d=1...n r 2n δ !
v ∈XV(G) R Πd,n(a) ≤χ f(d)F n/χ f(d) .
3.3 Sequential learning on graphs (cid:0) (cid:1)
We obtain the above result in a constructive way, by
explicitly devising a d-sheltered learner’s strategy by
We aim to generalize the online-to-PAC approach in-
averaging the actions of several standard players.
troduced in Section 2 to derive generalization bounds
for data with a graph-dependency structure. In order Notably, the resulting class of games generalizes
to do this, we need to define a class of online learning the well-studied setting of online learning with
gamesthatrespectsthegraphstructurethatunderlies delayed feedback (Weinberger and Ordentlich, 2002;
the data. This section presents this class of games, Joulani et al., 2013). Indeed, this setting is seen as
which we call sequential learning on graphs. the special case where G is a chain and the player is
constrainedto be d-sheltered,withdcorrespondingto
Let and be two sets, dubbed the action space
A B the delay in observing the feedback, and χ(d) = d.
andthe outcome space. We assume that is a vector f
A The rates of Weinberger and Ordentlich (2002) and
space. Given a graph G of order n, for each v V(G)
∈ Joulani et al. (2013) are recovered by our result. We
we define two sets and . We assume
v v
A ⊆ A B ⊆ B defer a discussion of other related online settings to
that is a convexsubsetof . We alsodefine a cost
v
functA ionC : R. WeA consideranarbitraryor- Section 5.
v
A×B →
dering v n ofG,constituting apermutationofthe
{ t }t=1
vertices of G. In eachround t=1,2,...,n, the player 4 GENERALIZATION BOUNDS
moves to node v and picks an action a . Then, UNDER GRAPH-MIXING
t t ∈Avt
the outcome b is revealed. The player incurs
t
∈
Bvt
a cost c t(a t,b t), where c t = C vt. The player can se- We are now ready to state our assumptions on the
lect their actions using past information only, namely dependencestructureofthetrainingdata,andprovide
at round t the action can depend on b 1,...,b t 1 and ourmainresults: thegraph-mixingcounterpartsofthe
−
on the previous actions, but not on the present and generalization bounds of Section 2.
future outcomes. For a fixed comparator a and a
∈A Our main assumption onthe dependencies is that, for
player’s strategy Π=(a ) , we define the regret of
t t [n]
Π against a at round T n∈ as any hypothesis w , the losses ℓ(w,Z t) constitute
≤ a (G,φ)-mixing pro∈ ceW ss. This is formalized as follows.
T
Assumption 1. Let S = (Z ,...,Z ) be a train-
R (a)= c (a ,b ) c (a,b ) . n 1 n
Π,T t t t − t t ing data set drawn from a distribution µ n on n, such
t=1 Z
X(cid:0) (cid:1) thateach Z t has thesamemarginal distribution µ. We
assume that there exists a graph G (of order n), a bi-
Inthespecificgamethatweconsider,thegraphstruc-
jection ι:G [n], and a non-negative non-increasing
ture G is used to pose further constraints on how the →
sequence φ = (φ ) , such that, for all w , the
d d>0
player is allowed to select their actions. In particu- ∈ W
graph-labelled process X (w) = X (w) is a
lar, we will consider sheltered players, who are only G v v V(G)
∈
(G,φ)-mixing process, where
allowed to use information from nodes that are “suf- (cid:0) (cid:1)
ficiently far” from the currently selected node v . To
t X (w)= (w) ℓ(w,Z ).
v ι(v)
make this formal,we define the d-exterior of a node v L −
(where d [n]) as U = u G : dist (u,v) d .
v,d G
∈ { ∈ ≥ } This assumption is essentially an extension to the
Definition 7. In the online game defined above, a d-
graph setting of the mixing condition proposed by
sheltered learner is a player whose action a in round
t Abeles et al. (2024). It comes from the intuition
t can only depend on outcomes b , from rounds s < t
s that the loss associated with the observations Z be-
v
such that v U .
s ∈ vt,d comes almost independent with respect to the family
of points which are at least d edges away in the asso-
The following result shows that an upper bound on
ciated graph.
the regret of a standard learner often translates into
anupper boundforthe regretofad-shelteredlearner. We can now state the graph-mixing counterpart ofBaptiste Ab´el`es, Eugenio Clerico, Gergely Neu
Theorem 1. First, we notice that the generalization Proof. The proofcombines Theorem2, Proposition1,
game of Definition 1 induces an online problem on G. and Proposition 2. Fix P ∆ and d
∈ W ∈
Definition 8 (Generalization game on G). Consider [n]. By a slight generalization of Corollary 6 in
Orabona and Pa´l (2016) (see the proof of Corollary
a training data set S satisfying Assumption 1 with
n
graph G and bijection ι. Consider the following online 6 in Lugosi and Neu, 2023), we know that for any d
there is a (standard) online strategy for the game of
game on G. For all v V(G), let = = ∆ ,
v
= be the space of∈ measurable A functioA ns on W , Definition 8, whose regret (for any comparator P ′) is
v
B a an nd dC anvB (a a, db v) er=
sa−
ryha p, lb
ai
y. F tho er fn olr lo ou wn ind gs, ga an mo en .lin Ae tp rl oa uyW ne dr tu ip op ne 2r ,b to hu ern ede isd ab dy
-s
phe( l3 te+ reK dL o( nP li′ n|P e) s) t3 ran t. egB yy ΠP dr wop ho os si e-
t=1,...,n: regret is upper bounded by (3+KL(P′P))3nχ(d). Ap-
| f
ply Theorem 2. Since φ = φ , by Proposition 1
• the online player moves on v =ι 1(t); d p d
• the online player picks a distt ribut− ion π ∆ ; n1M Πn(d) ≤δ φ d+ 21 nχ f(d)log δ1,andsoweconclude.
t e
∈ W p
• theadversarypicksamapg :w (w) ℓ(w,Z );
t 7→L − t We stress that this is only one of the many possi-
• the learner incurs a cost π t,g t ; ble bounds that can be derived from our framework,
−h i
• Z is revealed to the learner. given that different online learning algorithms may
t
leadtodifferentregretbounds. WerefertoSection3of
Combining the results from Sections 3 and 3.3 we ob- Lugosi and Neu(2023)forfurtherexamples,including
tain the following generalization result. generalized PAC-Bayesian bounds where the relative
Theorem 2. Consider a data setS that satisfies As- entropy,KL,appearinginthe aboveboundis replaced
n
sumption 1. Fix d [n] and an arbitrary strategy Π by other strongly convex functionals of P n.
∈
of a d-sheltered player for the game of Definition 8.
The tightness of these bounds relies on the chromatic
For v V(G), define X = π ,g . Then, any b
∈ v −h ι(v) ι(v) i number of the power graph, and the coefficients φ
statistical learning algorithm P n = (S n) satisfies characterizing the strength of dependencies. Typical
A
e
assumptions regarding the latter include functions of
R (P )+M
(P ) (P )+ bΠ,n n Π , (3) the form φ = Ce d/τ, for some C,τ > 0 (called ge-
n n n d −
L ≤L n
ometric mixing), or φ = Cd r for some C,r > 0
b d −
with M =b b bX . Moreover, X is a (G,φ)- (called algebraic mixing). As for the chromatic in-
Π v V(G) v G
∈ dices, it is knownthat they canalwaysbe bounded as
mixing proce Pss, where w ee let φ d′ =φ d′ efor d ′ ≥d, a end χ(d) = (∆d) where ∆ is the degree of the original
φ d′ =+ for d ′ <d. f O
∞ graph. It is often possible, though, to show tighter
e
bounds for graphsthatarisenaturallyin practicalap-
Peroof. (3) is equivalent to (2), so we will only need to
plications. We demonstrate a few concrete examples
show that X is (G,φ)-mixing. Clearly, when d < d,
G ′ below,andrefertoAlon and Mohar(2002)foramore
we have E[X v |Fv,d′] ≤ φ d′ = + ∞. For d ′ ≥ d, exhaustive treatment.
v,d′ v,de, and so πe ι(v) is v,d′-measurable, by def-
iF nition⊇ oF f d-sheltered learenerF . Hence, E[X
v
|Fv,d′] =
Temporal processes. The simplest non-trivial ex-
−hπ ι(v),E[g ι(v) |Fv,d′] i, and E[g ι(v) |Fv,d′] ≤ φ d′ = φ d′ ampleistheclassofmixingprocessesintime,whichwe
by Assumption 1.
have already mentioned extensively. These processes
e
can be modeled by a graph G, whose nodes corre-
Theusefulnessofthe aboveresultcomesfromthe fact
spondtothetimeindices1,2,...,n,andedgesconnect
thatweknowhowtoupperbound(inhighprobability)
neighboring indices, namely E(G) = ( (t,t+1 ) .
t [n]
(G,φ)-mixingprocesses. Hence,wecanderiveagraph- This can model a variety temporally-d{ ependen} t d∈ata
mixing counterpart of Corollary 1.
sequences, such as stock prices, energy consumption,
Corollary 2. Consider a data set S n that satisfies or sensor data from physical environments (see, e.g.,
Assumption 1, assume that ℓ is bounded in [0,1], fix Ariyo et al., 2014; Takeda et al., 2016). In this case,
δ (0,1), d [n], and an arbitrary P ∆ (whose one can easily see that χ(d) = d. Thus, in this set-
∈ ∈ ∈ W f
choicecannotdependonS ). Then, thefollowing gen-
n ting our guarantees almost exactly recover the recent
eralization bound holds in high probability, uniformly
results of Abeles et al. (2024). We refer the reader to
for all algorithms P = (S ),
n n their work for details.
A
(P ) (P )+φ
n δ n nb d
L ≤ L Processes on a spatial grid. A direct general-
1 1 χ(d) ization of the previous case is where the graph is
b + b 3bKL(P P)+9+ log f .
n
q
| r2 δ !r n a 2-dimensional grid of size n = I
×
J, for some
bOnline-to-PAC generalization bounds under graph-mixing dependencies
integers I and J. Such graphs can model spa- ing the graph structure into account. Our analysis
tially organized data like the house-price example suggests an obvious way to do so: find the largest d-
mentioned in the introduction. A straightforward stable subset and then use only data points from this
calculation shows that χ(d) is of order d2 for this set. Ourtechniquescanbeusedtoshowthesamegen-
f
class of graphs3. For the sake of concreteness, let eralizationbound for this method as for the empirical
us suppose that the mixing is geometric. Then, mean, but the example above indicates that its actual
the guarantee of Corollary 2 implies a generalization performance could be much better. The downside, of
course, is that this approach requires full knowledge
bound of order O(cid:18)Ce−d/τ+ qd n2(KL(Pn|P)+log1 δ) (cid:19). Set-
of the graph and requires additional computation. In
ting d = τlog(Cn), neglecting lobg factors this be-
contrast, our bounds need only high-level information
comes
O
qτ2(KL(Pn|P)+log δ1)/n , thus demonstrating about the graph, as they only assume knowledge of
a lineare(cid:16)dependenbce with the m(cid:17)ixing-time parameter thechromaticnumbers,whichmighteasiertoestimate
τ. This argumentcanbe easily extendedto other pla- thanfindingstablesets. Weleaveadetailedinvestiga-
nar graphs of similar regularity, and generalized to k- tion of this interesting question open for future work.
dimensionalgridswherethe chromaticindicesscaleas
dk, eventually yielding a dependence of order τk/2 on
the mixing time. Online learning on graphs. To our knowledge,
the sequential learning framework we introduce in
Section 3.3 has not appeared in the previous liter-
5 CONCLUSION
ature. That said, several similar models have been
studied. The works of Guillory and Bilmes (2009)
We have introduced a new model for statistical learn-
andCesa Bianchi et al. (2010) consider learning label-
ing with dependent data, and provided a general
ings on graphs via actively querying a subset of the
framework for developing generalization bounds for
labels, and provide mistake bounds that depend on
learning algorithms. A key tool in our analysis was
a joint notion of complexity of the labeling and the
a reduction to a family of online learning games. We
graph. Obtainingguaranteesintermsofsuchproblem-
conclude by discussing some further related work and
dependentnotionsofcomplexitywouldbedesirablein
highlighting some interesting open problems.
our setting as well, but unfortunately their model is
rather different from ours. A more relevant setting
The tightness of our bounds. Our upper bounds is the one studied by Cesa-Bianchiet al. (2020), who
onthegeneralizationerrordependonvariationsofthe studyanonlinelearningprotocoldefinedonanetwork
chromatic number of the dependency graph. While it ofagents. Ineachround,oneagentwakesup,needsto
is easy to construct hard examples where this depen- makeaprediction,suffersaloss,andsharesthe obser-
denceistight(e.g.,whenGiscomposedofseveraldis- vationwithitsneighbors. Inacertainsense,thisprob-
connectedcliques),it is notclearif ourbounds canbe lem is the dual of ours: in our setting, a sheltered on-
furtherimprovedtoscalewithmorefine-grainedgraph line learneris not allowedto use informationfrom the
properties. On a related note, it is also easy to con- neighbors of the currently active node, whereas their
struct examples where our bounds are vacuous, yet it setting only allows using information from neighbor-
still should be possible to estimate the test error with ing nodes. The two settings can be transformed into
good rates. To this end, consider a graph of size n, each other by taking the complements of the under-
with n/2 isolated vertices and the remaining n/2 ver- lying graphs. Applying their algorithm to our setting
tices forming a clique. The chromatic number of this inthemoststraightforwardwayyieldsguaranteesthat
graph is n/2, which makes our bounds trivial. How- canberecoveredbyProposition2. Wefinditplausible
ever, in such a case it is clearly a bad idea to measure thatapproachingourproblemfromthisalternativedi-
the training error on all samples: the heavy depen- rectionmayleadto improveddata-dependentguaran-
dence of the second half of the data points leads to a tees (as suggestedby existing follow-upworklike that
massive bias. This bias, however, can be completely of Achddou et al. 2024), but so far we do not see suf-
removedbysimplydiscardingthesecondhalfofobser- ficientevidencetopreferthis ratherroundaboutroute
vations and only using the i.i.d. samples. This patho- over our rather simple formulation that addresses our
logical case suggests that the empirical mean can be overallprobleminmorenaturalterms. Weremainop-
an arbitrarily poor estimator of the mean, and much timistic nevertheless that further progress on online
more efficient estimators can be constructed by tak- learning with graph structures will enable improve-
mentsinthestatisticallearningsettingwestudyinthe
3To see this, note that the set of nodes reachable
through a path of length d roughly corresponds to the presentpaper. Asafinalremark,theonlineframework
nodes falling into a square of diagonal 2d on the two- thatwe introduced,andthe waywe developedto cou-
dimensional plane. ple it with graph-mixing processes’ concentration, areBaptiste Ab´el`es, Eugenio Clerico, Gergely Neu
likelytobeusefultoadaptotheralgorithmicstatistics Bradley, R. C. (1983). Approximation theorems for
approaches to graph-mixing dependent settings. strongly mixing random variables. Michigan Math-
ematical Journal, 30(1):69–81.
Acknowledgements
Bradley,R. C. (2005). Basic properties of strongmix-
ing conditions: A survey and some open questions.
The authors would like to thank Rui-Ray Zhang and
Probability Surveys, 2:107–144.
G´abor Lugosi for the insightful discussions that in-
spired this work. This project has received funding Cesa-Bianchi, N., Cesari, T., and Monteleoni, C.
from the European Research Council (ERC), under (2020). Cooperative online learning: Keeping your
the European Union’s Horizon 2020 research and in- neighbors updated. ALT.
novation programme (Grant agreement No. 950180).
Cesa-Bianchi,N.,Conconi,A.,andGentile,C.(2001).
On the generalization ability of on-line learning al-
References
gorithms. NeurIPS.
Abeles, B., Clerico, E., and Neu, G. (2024). Gener- CesaBianchi,N.,Gentile,C.,Vitale,F.,andZappella,
alization bounds for mixing processes via delayed G. (2010). Active learning on trees and graphs.
online-to-pac conversions. arXiv:2406.12600. COLT.
Achddou, J., Cesa-Bianchi, N., and Laforgue, P. Cesa-Bianchi, N. and Lugosi, G. (2006). Prediction,
(2024). Multitask online learning: Listen to the learning, and games. Cambridge university press.
neighborhood buzz. AISTATS.
Chatterjee, S., Mukherjee, M., and Sethi, A. (2024).
Agarwal, A. and Duchi, J. C. (2012). The generaliza- Generalization bounds for dependent data using
tionabilityofonlinealgorithmsfordependentdata. online-to-batch conversion. NeurIPS.
IEEE Transactions on Information Theory, 59(1).
Feldman, V. and Vondrak, J. (2019). High probabil-
Agarwal, A. and Duchi, J. C. (2013). The general- ity generalization bounds for uniformly stable algo-
ization ability of online algorithms for dependent rithms with nearly optimal rate. COLT.
data. IEEE Transactions on Information Theory,
Foster, D. J., Kakade, S. M., Qian, J., and Rakhlin,
59(1):573–587.
A. (2021). The statistical complexity of interactive
Alon, N. and Mohar, B. (2002). The chromatic num- decision making. arXiv:2112.13487.
berofgraphpowers.Combinatorics, Probabilityand
Fu, S., Lei, Y., Cao,Q., Tian, X., andTao,D. (2023).
Computing, 11(1):1–10.
Sharperboundsforuniformlystablealgorithmswith
Alquier, P. (2024). User-friendly introduction to pac- stationary mixing process. ICLR.
bayes bounds. Foundations and Trends in Machine
F´eray,V. (2018). Weighted dependency graphs. Elec-
Learning, 17(2):174–303.
tronic Journal of Probability, 23:1–65.
Alquier,P., Li, X., andWintenberger,O.(2013). Pre-
Gru¨nwald, P., de Heide, R., and Koolen, W. (2024).
dictionoftimeseriesbystatisticallearning: General
Safetesting. Journal of the Royal Statistical Society
lossesandfastrates. DependenceModeling,1:65–93.
Series B: Statistical Methodology.
Alquier,P.andWintenberger,O.(2012). Modelselec-
Guedj, B. (2019). A primer on pac-bayesian learning.
tion for weakly dependent time series forecasting.
arXiv:1901.05353.
Electronic Journal of Statistics, 6:1447–1464.
Guillory, A. and Bilmes, J. A. (2009). Label selection
Ariyo, A. A., Adewumi, A. O., and Ayo, C. K.
on graphs. NeurIPS.
(2014). Stock price prediction using the arima
model. UKSim-AMSS International Conference on He, F., Zuo, L., and Chen, H. (2016). Stability anal-
Computer Modelling and Simulation. ysis for ranking with stationary φ-mixing samples.
Neurocomputing, 171:1556–1562.
Bousquet, O., Boucheron, S., and Lugosi, G.
(2004). Introduction to Statistical Learning Theory. Jang, K., Jun, K.-S., Kuzborskij, I., and Orabona, F.
Springer. (2023). Tighter PAC-Bayes bounds through coin-
betting. COLT.
Bousquet, O. and Elisseeff, A. (2002). Stability and
generalization. Journal of Machine Learning Re- Janson,S. (2004). Largedeviations for sums of partly
search, 2. dependent random variables. Random Structures &
Algorithms, 24(3):234–248.
Bousquet, O., Klochkov, Y., and Zhivotovskiy, N.
(2020). Sharper bounds for uniformly stable algo- Joulani, P., Gy¨orgy, A., and Szepesv´ari, C. (2013).
rithms. COLT. Online learning under delayed feedback. ICML.Online-to-PAC generalization bounds under graph-mixing dependencies
Kakade, S. M., Sridharan, K., and Tewari, A. (2008). Steinwart,I.andChristmann,A.(2009).Fastlearning
Onthecomplexityoflinearprediction: Riskbounds, from non-i.i.d. observations. NeurIPS.
margin bounds, and regularization. NeurIPS.
Takeda, H., Tamura, Y., and Sato, S. (2016). Using
Kontorovich,L. and Ramanan,K. (2008). Concentra- the ensemble Kalman filter for electricity load fore-
tioninequalitiesfordependentrandomvariablesvia casting and analysis. Energy, 104.
the martingale method. The Annals of Probability,
Usunier, N., Amini, M. R., and Gallinari, P. (2005).
36(6):2126–2158.
Generalization error bounds for classifiers trained
Kuznetsov, V. and Mohri, M. (2017). Generalization with interdependent data. NeurIPS.
bounds for non-stationary mixing processes. Ma- Vapnik, V. (2000). The nature of statistical learning
chine Learning, 106(1):93–117. theory. Springer-Verlag.
Lampert, C. H., Ralaivola, L., and Zimin, A. (2018). Weinberger,M.andOrdentlich,E.(2002). Ondelayed
Dependency-dependent bounds for sums of depen- prediction of individual sequences. IEEE Transac-
dent random variables. arXiv:1811.01404. tions on Information Theory, 48(7).
Lugosi, G. and Neu, G. (2023). Online-to-PAC con- Yu, B.(1994). Ratesofconvergenceforempiricalpro-
versions: Generalizationbounds via regretanalysis. cesses of stationary mixing sequences. The Annals
arXiv:2305.19674. of Probability, 22(1):94–116.
Mohri, M. and Rostamizadeh, A. (2007). Stability Yu, B., Yin, H., and Zhu, Z. (2018). Spatio-temporal
bounds for non-i.i.d. processes. NeurIPS. graph convolutional networks: A deep learning
framework for traffic forecasting. IJCAI.
Mohri,M.andRostamizadeh,A.(2008). Rademacher
complexityboundsfornon-i.i.d.processes.NeurIPS. Zhang, R. R. and Amini, M. R. (2024). Generaliza-
tion bounds for learning under graph-dependence:
Mohri, M. and Rostamizadeh, A. (2010). Stabil-
A survey. Machine Learning, 113(7):3929–3959.
ity bounds for stationary φ-mixing and β-mixing
processes. Journal of Machine Learning Research, Zhang,R.R.,Liu,X.,Wang,Y.,andWang,L.(2019).
11(Feb):798–814. Mcdiarmid-type inequalities for graph-dependent
variables and stability bounds. NeurIPS.
Montanari,A.andSaberi,A.(2010).Thespreadofin-
novationsinsocialnetworks. Proceedings of the Na-
tional Academy of Sciences, 107(47):20196–20201.
Orabona, F. (2019). A modern introduction to online
learning. arXiv:1912.13213.
Orabona, F. and Jun, K.-S. (2024). Tight concentra-
tions and confidence sequences from the regret of
universalportfolio. IEEE Transactions on Informa-
tion Theory, 70(1):436–455.
Orabona, F. and Pa´l, D. (2016). Coin betting and
parameter-free online learning. NeurIPS.
Rakhlin,A.andSridharan,K.(2017). Onequivalence
of martingale tail bounds and deterministic regret
inequalities. COLT.
Ralaivola,L.,Szafranski,M.,andStempfel,G.(2010).
ChromaticPAC-Bayesboundsfornon-iiddata: Ap-
plications to ranking and stationary β-mixing pro-
cesses. Journal of Machine Learning Research,
11(Sep):1927–1956.
Rio, E. (2000). In´egalit´es de Hoeffding pour les
fonctions lipschitziennes de suites d´ependantes.
Comptes Rendus de l’Acad´emie des Sciences, S´erie
I, Math´ematiques, 330:905–908.
Shalev-Shwartz, S. and Ben-David, S. (2014). Under-
standing Machine Learning - From Theory to Algo-
rithms. Cambridge University Press.Baptiste Ab´el`es, Eugenio Clerico, Gergely Neu
Supplementary material
A Omitted proofs
A.1 Proof of Proposition 1
TheproofleveragestheapproachintroducedbyJanson(2004). Fixdandconsiderad-stablefractionalpartition
(w ,S ) of G. We can write
k k k
{ }
X = X w = w X .
v v k k v
v ∈XV(G) v ∈XV(G) k: Xv ∈Sk Xk v X∈Sk
In particular, for any λ>0, we have
logE enλ Pv∈V(G)Xv =logE enλ Pkwk Pv∈SkXv
≤
p klogE enλw pkk Pv∈SkXv , (4)
h i h i Xk h i
where p is a probability vector ( p =1 with p >0 for all k), and in the last step we have applied Jensen’s
k k k
inequality, since f logE[ef] is a convex mapping.
7→ P
Now,foranyk wecanlabelarbitrarilythe elementsinS asv(k),...,v(k),wheren isthecardinalityofS . Let
k 1 nk k k
us denote as (k) the sigma algebraσ( X : j i ). Since S is a d-stable subset of g, recallingthe notation
Fi { v j(k) ≤ } k
introducedinDefinition6,wehavethat (k) . Inparticular,thefactthatX isa(G,φ)-mixingprocess
Fi −1 ⊆Fv i(k),d G
implies that
E[X (k)]=E E[X ] (k) φ
v i(k)|Fi −1 v i(k)|Fv i(k),d Fi −1 ≤ d
by the tower property of conditional expectation. N(cid:2)ow, this implies(cid:12) that(cid:3)for any i n we have
(cid:12) ≤ k
E (cid:20)enλw pkk Pi j=1X vj(k) (cid:12)Fi( −k)
1
(cid:21)≤E (cid:20)enλw pkk Pji− =1 1X vj(k) (cid:21)E (cid:20)enλw pkk(X vi(k)−E[X vi(k)|Fi−1]) (cid:12)Fi( −k)
1
(cid:21)exp (cid:18)λ nφ dw
p
kk
(cid:19)
.
(cid:12) (cid:12)
Moreover,the factthate(cid:12) (cid:12)achX
v
isboundedinanintervalI oflength∆implies tha(cid:12) (cid:12)titis ∆2/4-subgaussianwith
respect to any measure, and hence
E "enλw pkk (cid:18)X vi(k)−E[X vi(k)|Fi−1] (cid:19)
(cid:12)
(cid:12)Fi( −k)
1
#≤exp (cid:18)λ 82 n∆ 22w
p2
kk2
(cid:19)
.
(cid:12)
Applying these arguments recursively n times we obtain(cid:12)
k (cid:12)
logE enλw pkk Pv∈SkXv ≤n
k
λ 82 n∆ 22w p2k2 + λ nφ d w pk .
h i (cid:18) k k(cid:19)
We can hence rewrite (4) as
λ2∆2w2 λφ λ2∆2n w2
logE enλ Pv∈V(G)Xv
≤
n
k 8n2
pk + nd w
k
=
8n2
k
p
k +λφ d,
h i Xk (cid:18) k (cid:19) Xk k
where in the last equality we used that
w n = w 1= w 1 = w = 1=n. (5)
k k k k v ∈Sk k
Xk Xk v X∈Sk Xk v ∈XV(G) v ∈XV(G)k: Xv ∈Sk v ∈XV(G)
We can now optimize the choice of p, by setting p =
wk√nk
. With this choice we have
k k′w k′√N k′
P
2
λ2
logE enλ Pv∈V(G)Xv
≤ 2n2
w k√n
k !
+λφ d.
h i XkOnline-to-PAC generalization bounds under graph-mixing dependencies
By Cauchy-Schwarz inequality we have
w √n = √w √w n w w n = n w ,
k k k k k k k k k
≤
k k s k s k s k
X X X X X
whereagainweused(5). Sincethechoiceofthed-stablefractionalpartitionisarbitrary,wecanchoseanoptimal
one, such that w =χ(d). In particular, we get
k k f
P λ2∆2χ(d)
logE enλ v∈V(G)Xv f +λφ d.
P ≤ 8n
h i
By Markov’s inequality, we have that for any t>φ
d
1
E enλ v∈V(G)Xv
λ2∆2χ(d) 2n (t φ )2
P X t inf P inf exp f λ(t φ ) =exp − d .
n v ≥ ≤λ>0 h eλt i ≤λ>0 8n − − d −χ(d) ∆2
v V(G) (cid:18) (cid:19) (cid:18) f (cid:19)
∈X
 
The conclusion now follows by setting the RHS above equal to δ and solving for t.
A.2 Proof of Proposition 2
First, let us fix an arbitrary d-stable fractional partition (w ,S ) of G. The idea is that we will run an
k k k
{ }
independent player on each S . Each of them will also be a d-sheltered learner, as, by definition, in a d-stable
k
fractionalpartitionanytwodistinctverticesaredistantatleastdfromeachother. Wewillseethat,bycarefully
averaging the actions of these players, it is possible to obtain a d-sheltered learner on the full graph G, whose
regret can be upper bounded as desired.
First, note that the ordering of V(G) induces an ordering on S , and we will write S = (v(k),...,v(k)), where
k k 1 nk
n is the cardinality of S . We now introduce some notation which will be helpful for what follows. Any vertex
k k
v V(G) corresponds to an element in the ordered sequence v ,...,v . We denote as ι(v) the index of this
1 n
∈
element(so thatι(v )=t forall t). Similarly,givenanelement v S , we denote asι its index inthe sequence
t k k
∈
(v(k),...,v(k)).
1 nk
For each S , we let run an independent copy of the standard player, and we denote their strategy as Π =
k k
(k) (k)
(a ,...,a ). We assume that although these players’ choices of the action are independent from each other,
1 nk
foreachvertexeachplayerwhopassesthroughitreceivesthesameoutcome,4 whichcorrespondstothe outcome
that the d-sheltered learner running on the full graph sees. By assumption, we can choose the strategy of the
standard player so that we can upper bound the regret of each Π as
k
nk
R (a)= c(k)(a(k),b(k)) c(k)(a,b(k)) F(n ),
Πk,nk t t t − t t ≤ k
t=1
X(cid:0) (cid:1)
where c(k) = C and b(k) = b (v(k)) is the outcome on v(k) (which, as previously stated, only depends on the
t v(k) t ι t t
t
vertexandnotonwhichplayeris observingit, as itcorrespondsto the outcomethatthe d-shelteredlearnersees
at round ι(v(k))).
t
We will now define the d-sheltered learner’s strategy Π= (a ,...,a ) on the full graph. For any v V(G), let
1 n
∈
κ(v)= k : v S . We set a =A(v ), with
k t t
{ ∈ }
A(v)= w a(k) .
k ιk(v)
k ∈Xκ(v)
First, we notice that A(v) , since is assumed to be convex and A(v) is a convex mixture of elements in
v v
∈A A
(note that w =1 by definition of d-stable fractional partition).
Av k κ(v) k
∈
4ThisissomePhowlimitingthepowerofapotentialadversaryforeachofthesegames,butthisdoesnotaffecttheregret
bounds that hold for any possible outcome sequence. Also, notice that for the d-sheltered game a potential adversary is
allowed to choose freely for any vertex v, and indeed it is this chosen outcome that each of the players of the d-stable
subsets will see when passing on v.Baptiste Ab´el`es, Eugenio Clerico, Gergely Neu
We now show that the above definition of a define an admissible strategy for a d-sheltered learner. First, we
t
see that a only depends on what has happened on the set H = v(k) : k κ(v ) and s ι (v ) 1 . Clearly,
t t s t k t
{ ∈ ≤ − }
H S . Since for any k κ(v ) all the element in S (excluded v itself) are distant at least d from
v
t
,
t⊆ he∪ dk ∈-sκ h(v et lt) erk
ed property of
the∈ learnt
er is ensured.
Moreovek
r, one can
eat
sily check that, for any v and any
t
k κ(v), it holds that ι (v) ι(v). Thus, H v : s < t , which means that the learner is only allowed to
k t s
∈ ≤ ⊆ { }
access past information, as required. We have hence proven that the strategy that we defined is admissible for
a d-sheltered learner.
We now study the regret of this d-sheltered learner. For any v, let r(v) = c (a ,b ) c (a,b ), and,
ι(v) ι(v) ι(v) ι(v) ι(v)
−
for v S , define r (v) = c (a(k) ,b ) c (a,b ), where we used that, for any v S , c(k) = c (v)
∈ k k ι(v) ιk(v) ι(v) − ι(v) ι(v) ∈ k ιk(v) ι
and b(k) =b . With these definitions in mind we can rewrite
ιk(v) ι(v)
R (a)= r(v) and R (a)= r (v).
Π,n Πk,nk k
v ∈XV(G) v X∈Sk
Now, notice that thanks to the convexity of the cost we have that
c (a ,b ) w c (a(k) ,b ),
ι(v) ι(v) ι(v) ≤ k ι(v) ιk(v) ι(v)
k ∈Xκ(v)
by Jensen’s inequality, as w = 1. In particular, r(v) w r (v) = 1 w r (v), and
hence
k ∈κ(v) k ≤ k ∈κ(v) k k k v ∈Sk k k
P P P
R (a)= r(v) 1 w r (v)= w r (v)= w R (a).
Π,n
≤
v ∈Sk k k k k k Πk,nk
v ∈XV(G) v ∈XV(G)Xk Xk v X∈Sk Xk
The fact that R (a) F(n ) yields
Πk,nk
≤
k
w n
k
R (a) w F(n )= w F(n ) w F ,
Π,n k k k k k
≤ k k ! k k′w k′ ≤ k ! (cid:18) kw k(cid:19)
X X X X
P P
where the last inequality follows from Jensen’s inequality (since F is concave) and (5). Finally, notice that
what we have provenso far holds for any arbitrary d-stable fractional partition. In particular, we can select the
partition such that w =χ(d), and hence obtain
k k f
P R (a) χ(d)F n/χ(d) ,
Π,n ≤ f f
(cid:0) (cid:1)
which is the regret upper bound that we wanted to prove.