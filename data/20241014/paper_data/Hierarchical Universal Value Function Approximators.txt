Hierarchical Universal Value Function Approximators
RushivArora12
Abstract when exploiting the chosen skill. This is beneficial for
scalingandplanning.
There have been key advancements to building
universalapproximatorsformulti-goalcollections Universalvaluefunctions(Schauletal.,2015)(V(s,g)and
ofreinforcementlearningvaluefunctions—key Q(s,a,g))andgeneralvaluefunctions(Suttonetal.,2011)
elementsinestimatinglong-termreturnsofstates (V g(s)andQ g(s,a))arekeywhenextendingthistomulti-
in a parameterized manner. We extend this to taskdomains. Theyincorporateinformationofthegoalinto
hierarchicalreinforcementlearning,usingtheop- the value function itself, making more expressive agents
tionsframework,byintroducinghierarchicaluni- thatcanactdifferentlyinthesamestateasappropriatefor
versalvaluefunctionapproximators(H-UVFAs). thecurrentgoal. Acollectionofuniversalandgeneralvalue
Thisallowsustoleveragetheaddedbenefitsof functionsoveradiversesetofgoalsiskeyinbuildingasin-
scaling, planning, and generalization expected gleunifiedexpressionforgeneratingpoliciesandbuilding
intemporalabstractionsettings. Wedevelopsu- agentswithmultiplecapabilities(multi-taskcapabilities).
pervisedandreinforcementlearningmethodsfor
Complex environments require parameterized representa-
learningembeddingsofthestates,goals,options,
tions of the value functions (V(s;θ) or Q(s,a;θ)), often
and actions in the two hierarchical value func-
with large neural networks. These parameterized repre-
tions: Q(s,g,o;θ) and Q(s,g,o,a;θ). Finally
sentationsexploit thestructureofthe statespacetolearn
we demonstrate generalization of the HUVFAs
amappingtothevaluefunctionsandgeneralizebehavior
andshowtheyoutperformcorrespondingUVFAs.
tounseenstates. Universalvaluefunctionapproximators
Keywords:ReinforcementLearning,Universal (Schaul et al., 2015) demonstrated that one can learn pa-
ValueFunctionApproximators,Hierarchical rameterized representations of universal value functions
ReinforcementLearning V(s,g;θ)thatexploittheunderlyingstructureofthegoal
and state space to generalize to unseen goals and states.
Theyexploittheuniversalfunctionapproximationcapabili-
1.Introduction tiesofneuralnetworkstolearnasinglevaluefunctionthat
representsstatesandgoals.
Valuefunctions(V(s)andQ(s,a))arekeytoreinforcement
learningalgorithms(Sutton&Barto,1998). Theyrepresent WeextendUVFAstohierarchicalreinforcementlearning,
how important it is for the agent to be in a certain state ortemporalabstraction,usingtheoptionsframework. Since
andtakecertainactionsinagivenstate. Usinganoptimal therearetwoormorelevelsofvaluefunctionsinahierarchy,
value function for a particular goal allows us to recreate wemustlearnrepresentations(bothparameterizedandnon-
theoptimalpolicytheagentmustfollowtomaximizethe parameterized)ofallthelevelsofthehierarchytoresults
discountedreturnforthatgoal. Hierarchicalreinforcement in a final state of value functions equal in number to the
learningextendsthistotemporalabstraction(Suttonetal., hierarchylevels.Thechallengeliesinextendingtheconcept
1999;Precup,2000)byusingahierarchyoftwoormore ofUVFAsfromtwoorthreedimensions(forthestateand
levelsofvaluefunctionstocreateacollectionofusefulskills actionvaluefunctionsrespectively)ton-dimensions. Our
orbehaviors(Silvaetal.,2012)thatsolveindependenttasks methodisnovelbecauseitshowsthatthereisunderlying
orsubgoals. Ameta-valuefunctionselectstheappropriate structureinthestates,goals,optionsandactionsthatresults
skill in the given state while a lower-level value function inauniversalrepresentationofthehierarchy. Thisuniver-
drivesthepolicythatexecutestheactionstheagenttakes sal representation results in universal temporally abstract
behaviorsorskillsthathaszero-shotgeneralizationtoun-
1UniversityofMassachusetts,ManningCollegeofInformation seengoals. Weshowthatthisrepresentationoutperforms
and Computer Science, Amherst MA, USA 2Dell AI Research
correspondingUVFAs.
Office,HopkintonMA,USA..Correspondenceto:RushivArora
<rrarora@umass.edu>.
Preliminarywork.Underreview.
4202
tcO
11
]GL.sc[
1v79980.0142:viXraHierarchicalUniversalValueFunctionApproximators
2.Background π (ω|s)thatgivesustheprobabilityofselectingoptionωin
Ω
statesandisdrivenbytheoption-valuefunctionQ (s,ω):
Ω
A Markov Decision Process (MDP) (S,A,T,R,γ) con- S×Ω→R.Oncetheoptionisselected,itperformsactions
sists of a set of states S, a set of actions A, a transition
usingtheintra-optionpolicyπ (a|s,ω)whichisdrivenby
functionT(s,a,s′):=P(S t+1 =s′|S t =s,A t =a),are- itsintra-optionvaluefunctionQU (s,o,a):S×Ω×A→R.
wardfunctionR:S×A→Randarewarddiscountfactor U
Thiscontinuesuntiltheoptionterminates,uponwhichthe
γ ∈[0,1). Sinceweconsideramulti-tasksetting,eachgoal
processrestartsinthethen-currentstate. Thetwosetsof
hasitsownrewardfunctionR anddiscountfactorsγ . A
g g valuefunctionsarerelatedas
Markovianpolicyπ :S×A→[0,1]:=P(A =a|S =s)
t t
isaprobabilitydistributionofalltheactionsconditioned
overthestates. Theagent’sgoalistoconstructapolicythat (cid:88)
Q (s,ω)= π (a|s)Q (s,ω,a) , (1)
maximisesthestatevaluefunctionV (s)andaction-value Ω ω,θ U
π
a
functionQ (s,a)definedas
π
(cid:34) (cid:88)∞ (cid:12) (cid:12) (cid:35) Q (s,ω,a)=r(s,a)+γ(cid:88) P (s′ |s,a)U(ω,s′) .
V (s)=E γtR (cid:12)s =s U
π π t+1 (cid:12) 0
(cid:12) s′
t=0 (2)
(cid:34) (cid:88)∞ (cid:12) (cid:12) (cid:35)
Q (s,a)=E γtR (cid:12)s =s,a =a where
π π t+1 (cid:12) 0 0
(cid:12)
t=0
U(ω,s′)=(1−β (s′))Q (s′,ω)+β (s′)V (s′)
ω,ϑ Ω ω,ϑ Ω
Solvingfortheoptimalaction-valuefunctionresultsallows
onetodeduceagreedyoptimalpolicy. FordiscreteMDPs isthevalueofexecutingωuponenteringastates′.
thereisatleastoneoptimalpolicythatisgreedywithrespect
toitsaction-valuefunction.
3.HierarchicalUniversalValueFunction
Universal Value Functions (Schaul et al., 2015) extend Approximators
valuefunctionstoincludethegoalsandproducetheuniver-
salstatevaluefunctionV (s,g)anduniversalactionvalue Hierarchical universal value function approximators (H-
π
functionQ (s,a,g). Theseuniversalvaluefunctionsare UVFAs)extendUVFAstohierarchicalreinforcementlearn-
π
defined in the context of multi-task learning over multi- ingtogivezero-shotgeneralizationtounseengoalsbyex-
plegoalsG andareconstructedusingmultipleindividual ploiting underlying structure in the states, goals, options
value functions V (s) and Q (s,a) defined for each andactionsthatresultsinauniversalrepresentationofthe
g,π g,π
goalg ∈G. Universalvaluefunctionscanbelearnedusing hierarchy. WeaimtoconstructH-UVFAsovereverylevel
functionapproximatorstobecomeuniversalvaluefunction in the hierarchy (in our case, we use a two level hierar-
approximators(UVFAs).GivenUVFAsthataretrainedona chy). Particularly, weaimtoconstructQ Ω(s,o,g;θ)and
diversesetofgoalsandparameterizedwithsufficientcapac- Q U(s,o,a,g;η)wheretheH-UVFAmeta-policyisparam-
ity,welearnanunderlyingstructureofthestatesandgoals eterisedbyθ ∈RmandtheH-UVFAintra-optionpolicyis
thatcanbeusedtoconstructvaluefunctionsthatextendto parameterizedbyη ∈Rn. Notethatmandndon’tneces-
unseenstatesorgoals. sarilyhavetobeequal. TheseH-UVFAsdrawuponhierar-
chicalgeneralvaluefunctions(Q (s,o,g;θ)≈Q∗ (s,o)
The Options Framework (Sutton et al., 1999; Precup, andQ (s,o,a,g;η)≈Q∗ (s,oΩ ,a))which,totheΩ, bg estof
2000) formalizes the concept of temporal abstractions in U U,g
our knowledge, haven’t been defined before and will be
reinforcementlearning,orhierarchicalreinforcementlearn-
createdintheprocessofbuildingH-UVFAs.
ing,byusingsemi-MDPs. AMarkovianoptionω ∈Ωisa
tuple(I ,π ,β )whereI ∈S isitsinitiationset,π is LikeUVFAs,weproposelearningbyusingamulti-stream
ω ω ω ω ω
itsintra-optionpolicy,andβ :S →[0,1]isitstermination architecture as seen in Figure 1. Each component of the
ω
function. Weassumethat∀s∈S,∀ω ∈Ω: s∈I –allop- valuefunctiongetsitsownstreamofembeddings.
ω
tionsareavailableeverywhere. Anoptionωispickedfrom
theinitiationsetI
ω
usingthepolicy-over-optionsπ Ω(s,o) • The meta-H-UVFA Q Ω(s,o,g;θ) can be split into
and follows its intra-option policy π (s,a) until it termi- threesetsofembeddings: Φ:S →Rm,Ψ:G →Rm
ω
natesbyβ (s)whichdeterminestheprobabilityofagiven andX :Ω→Rm
ω
optionterminatinginagivenstate.
• Theintra-optionH-UVFAQ (s,o,a,g;η)canbesplit
U
This is a call-and-return model where the option initia- intothreesetsofembeddings: ϕ:S →Rn,ψ :G →
tionisdeterminedbythepolicy-over-options(meta-policy) Rn,χ:Ω→Rn,andδ :A→Rn
2HierarchicalUniversalValueFunctionApproximators
tensorwillhaveN >2dimensionsandtheresultwillbeN
setsofm−orn−dimensionalembeddings
The second step is a simple function approximation task.
Wecanuseanyfunctionapproximatorandtrainingmethod
toapproximatetheresultingcollectionofembeddings. For
N streams,wewilluseN separateapproximators. Thatis,
weuseonenetworkeachforeachstream—asanexample,
themeta-policyH-UVFAhasthreeapproximatorsandthe
intra-optionpolicyH-UVFAhasfourapproximators.
Noticethatthemeta-policyH-UVFAQ (s,o,g;θ)isequiv-
Ω
Figure1.Themulti-streamarchitectureofH-UVFAs.Indifferent alenttotheUVFAforsingle-policyreinforcementlearning
temporalabstractionmethodswithmorehierarchies, therewill Q(s,a,g;θ)exceptweuseoptionsinsteadofactions. How-
bemorestreams. Thiswillresultinhigh-orderdimensionality
ever,theexperimentsin(Schauletal.,2015)arebasedona
reductions,whichourmethodcanaccommodateaswewillsee
twostreamarchitectureanduseSVDorOptSpace(Kesha-
in the upcoming sections. Left: The three-stream architecture
vanetal.,2009)whileourexperimentsdescribehigher-order
ofthepolicy-over-options(meta-policy)H-UVFA.Thestreams
streamarchitecturesandusePARAFACfordecomposition.
are:states,goals,options.Right:Thefour-streamarchitectureof
theintra-optionpolicyH-UVFA.Thestreamsare: states,goals,
options,actions. 4.SupervisedLearningofH-UVFAs
4.1.Method
Noticethatweuseupper-caseGreeklettersforthehigher
levelmeta-policyandlower-caseGreeklettersforthelower ThesupervisedlearningapproachtobuildingH-UVFAsis
levelintra-optionpolicy. bestapplicablewhenitispossibletoiterateoverthestates,
goals,options,andactionstoconstructthematrixthatwe
BuildingH-UVFAsisatwo-stepprocess.
candecomposeintoembeddings.
Thetwo-stepmulti-streamprocessdescribedintheprevious
• First,weneedtoconstructthehierarchicaluniversal
section is achieved using the following steps to learn the
value functions. That is, we need to construct the
m−orn−dimensionalembeddingsforthedifferent
parametersθandη:
streams. In our methods, we will construct a high-
ordertensorwiththenumberofdimensionsequalto • Iterateoverallthestates,goals,actions,andoptions
thenumberofstreamsinthedesiredH-UVFA(differ-
andorganizetheirQ-valuesintotwomulti-dimensional
entdependingonthelevel)anddecomposeitintothe tensors. ThefirsttensorrepresentsQ Ω(s,o,g)andisa
differentembeddings. threedimensionaltensorwithonedimensioneachfor
thestates,goals,andoptions. Thesecondtensoristhe
• Next,weneedtoconstructthehierarchicaluniversal four dimensional Q (s,o,a,g) with one dimension
U
function approximators. That is, we need to train a each for the states, goals, options, and actions. The
sufficientlyparameterizedapproximatorsuchasaneu- tensorsareofdimensions|S|×|Ω|×|G|and|S|×
ralnetwork(universalfunctionapproximator)onthe |Ω|×|A|×|G|respectively.
embeddings.
• Decomposethetwomulti-dimensionaltensorsintoa
TheresultsoftheH-UVFAsarepassedthroughh(.)
setofembeddingvectorscorrespondingtoeachdimen-
thatoperatesontheembeddingsandconvertsitintoa
sion using either PARAFAC (Kolda & Bader, 2009)
scalar. Inourmethod,asdescribedahead,thisisthe
or Tucker decompositions (Tucker, 1966). As dis-
dotproductofalltheembeddingsresultingfromthe
cussedbefore, theembeddingvectorscorrespondas
approximators. Q : (s,g,o) → (Φˆ,Ψˆ,Xˆ) and Q : (s,g,o,a) →
Ω U
(ϕˆ,ψˆ,χˆ,δˆ).
Thefirststepisequivalenttoahigher-ordersingularvalue
decomposition(SVD)ontwo-dimensionalmatrices,except • Trainafunctionapproximatortoapproximatetheem-
doneonhigher-dimensiontensors. Weachievethisdecom- beddingsfromtheembeddingvectorstolearnthepa-
positionintothem−andn−dimensionembeddingsusing rametersθ = {Φ,Ψ,X}andη = {ϕ,ψ,χ,δ}. This
thePARAFACdecomposition(Tomasi&Bro,2005;Kolda nowresultsinQ (s,o,g;θ)andQ (s,o,a,g;η).
Ω U
&Bader,2009). Alternatively,onecanusetheTuckerde-
composition(Tucker,1966). Ifweassumethatthegiven This method minimizes the mean-squared-errors (MSE)
H-UVFAhasN streams,thenweknowthatthehigher-order MSE = [Q (s,o)−Q (s,o,g;θ)]2 and MSE =
QΩ Ω,g Ω QU
3HierarchicalUniversalValueFunctionApproximators
[Q (s,o,a)−Q (s,o,a,g;η)]2 toconstructUVFAswhichprovidebetterperformancethan
U,g U
anagentactingrandomly; however,wenoticesignificant
ThefirstandsecondstepresultinthecreationofH-UVAs
decreaseinperformanceandhighervarianceforthebase
Q (s,o,g)andQ (s,o,a,g). Theseunparameterizedten-
Ω U method. Thisisduetothefactthat,inhierarchicalsettings,
sorsandtheirembeddingvectorsareusefulwhentheagent’s
thelower-levelUVFAspacksignificantbehaviorinacom-
policyneedstobeinferredforgoalsthathavebeenobserved
pact representation that doesn’t generalize over different
whencollectingthedata. Thefunctionapproximatorsthat
skills. ThisisfurtherdiscussedinSection6.
learnembeddingsfromtheembeddingvectorsconstructthe
H-UVFAs from the H-UVFs—the idea being that similar
state,goal,option,andactionpairswillhavesimilarvalues
tothecorrespondingpairsintheobserveddata. Wewillsee
intheexperimentsthatthefactorizationintotherespective
embeddingswillexploittheunderlyingstructure,thereby
resultingingeneralization.
4.2.SupervisedLearningExperiments
Setting: WeconstructH-UVFAsinthefourroomsdomain
(Sutton et al., 1999) with the goals spread out over three
rooms–topleft,topright,andbottom-left–withthefourth
room being used to test generalization of the H-UVFAs.
Weallowtheagenttoberandomlyinitializedinanyofthe
fourroomsatthestartofeachepisodeinordertoensure
that it can gain a representation of all rooms. The agent
receivesarewardof1forreachingthegoalandareward
of0otherwise. Allepisodesterminateafter1000timesteps.
MoredetailsfortheexperimentcanbefoundinAppendix
A
Results: WeconstructH-UVFAsusingacombinationof
25goals. Figure2plotsthevaluesandgreedyactionsof
theH-UVFAsonasubsetofthegoalsusedduringtraining
along with the ground truth values corresponding to the
option-valueandintra-optionvaluefunctions. Ineachstate,
the greedy option is picked and the corresponding value
andgreedyactionareplotted. Wefindthatanembedding
rankof50issufficienttorecreatethegroundtruthmatrix
accurately. We plot the entire decomposed matrices and
thecorrespondingrecreatedH-UVFAsforQ andQ in
Ω U
AppendixB.
Interestingly,weseethattherecreationofthevaluefunction
withaneuralnetworkallowsformorecontinuouschanges
Figure2.Acomparisonofthevaluesandactionsoftheground
inthevalues—aswemovefurtherawayfromthegoalthe
truthandH-UVFAs.Redrepresentshighervaluesforeachstate,
valueofthestatedecreasesgraduallyascomparedtothe
bluerepresentslowervaluesofbeinginastate,andtheorange
moresuddenchangesinvaluefunctionsseeninthediscrete
circlerepresentsthegoalstate. Thearrowsrepresentthegreedy
andtabulargroundtruth. Thisbehaviorbetterrepresentsa
actionineachstateobtainedfrompickingagreedyoption.Itisin-
valuefunctionandpolicyandallowsforbetterbehaviorin terestingtonotethatusingneuralnetworks/functionapproximators
longertrajectorieswheremoreoptimalactionsarepicked (H-UVFAs)allowsforgradualandsmoothchangesinthestates’
furtherawayfromthegoalasseeninFigure2. valuesascomparedtothemoredrasticonesseeninthediscrete
andtabulargroundtruth.Thisbetterrepresentspoliciesandtheir
Figure3plotsthecomparisonofthegroundtruth,H-UVFAs,
valuesandallowsforbetteragentbehaviorsinlong-trajectories
andUVFAson5trainedgoalsasrepresentedbytheaver-
wheretheagentisfarfromthegoal.
agestepstogoalanditsstandarddeviationmeasuredover
10episodesforeachgoal. H-UVFAsandthegroundtruth
Generalization: We measure generalization of the H-
H-UVFmatrixhavecomparableperformance. Itispossible
UVFAstounseenroomsbyconstructingsimilarplots,as
4HierarchicalUniversalValueFunctionApproximators
Figure3.Comparisonofthegroundtruth,H-UVFAs,andUVFAs Figure4.ComparisonofH-UVFAsandtheUVFAsbaselinein
ongoalsseenduringtrainingasmeasuredinaveragesandstandard generalization to unseengoals as measured in average steps to
deviationsover10episodesfor5goals. H-UVFAsandground goal and standard deviation over 10 episodes each for 3 goals.
truth are comparable in performance while UVFAs have poor H-UVFAsgeneralizewellinhierarchicalsettingstounseengoals.
performanceandhighervariance.ThepoorperformanceofUVFAs Thevarianceforbothmethodsincreasesascomparedtothaton
as compared to H-UVFAs in hierarchical settings is is due to trainedgoals.
the loss of information when a large amount of information is
compressedinacompactrepresentation,asdiscussedinSection6.
seeninFigures4and5,forgoalsinthefourthroom. We
noteasimilarcontinuousvaluefunctionandoptimalactions
for unseen goals. We also note that when measured over
multipleepisodesandunseengoalsH-UVFAsstillperform
wellandreachthegoalinanoptimalnumberofsteps. This
Figure5.Valuesandpoliciesforunseengoalsinthefourthroom.
generalizationtounseengoalsrevealsthattheH-UVFAsare
Redindicateshighervalueswhilethearrowsindicatethegreedy
abletoexploittheunderlyingstructureinthestates,goals, actionforthegreedypolicy.Thenear-optimalvaluefunctionsand
options,andactionstogenerateembeddingsforeachthat policiesindicatethatH-UVFAscanextrapolatetocreateoptimal
canbecombinedtoproducevaluefunctionsandpolicies hierarchicalbehaviors.
that extrapolate. This zero-shot generalization and better
performancethanthevanillaUVFAsbaselineindicatesthat
universalandgeneralvaluefunctionscanbecreatedinhier- 5.1.Method
archicalsettings.
WebuilduponthemethodpresentedbySchauletal.(2015)
toconstructtwomethodsofdevelopingH-UVFAs: usinga
5.ReinforcementLearningofH-UVFAs
Horde(Suttonetal.,2011)ofgeneralvaluefunctions,and
directlyusingbootstrapping.
Wenowpresentresultsinthemorepracticalsettingofrein-
forcementlearning.Thismethodisapplicabletocontinuous The first method using a Horde is depicted in Algorithm
domainswhereitisnotpossibletoaccessthevaluefunctions 1. Welearnmultiplepoliciesonadiversesetofgoals—a
foreverypossiblecombinationofstates,goals,options,and Horde—andconstructhierarchicalgeneralvaluefunctions
actions. WefindthatitispossibletoconstructH-UVFAs Q andQ . Usingthesehierarchicalgeneralvaluefunc-
Ω,g U,g
simplyusingtheobservedvaluesofthevaluefunctionsseen tions,webuildtwodatamatricesthatwefactorizeintothe
during episodes. We will continue to build on the multi- respectivestate,goal,option,andactionembeddingsthatthe
streamarchitectureweusedinsupervisedlearningsettings. H-UVFAsapproximate. Tothebestofourknowledge,no
Likemosthierarchicalreinforcementlearningapproaches, otherworkintroduceshierarchicalgeneralvaluefunctions
weassumeafinitesetofoptions. (H-GVFs)thatweuseinconstructingourH-UVFAs.
5HierarchicalUniversalValueFunctionApproximators
Algorithm1constructstheH-GVFsandH-UVFAsusing Algorithm1H-UVFAsfromHorde
aseriesofsampleepisodesobtainedfromtheHorde. The Input: rankm,rankn,traininggoalsG,budgetsb ,b ,
1 2
transitions in all the sample episodes can be used to con- b
3
structthedatamatricesM Ω;t,g andN U;t,g,fortheoption- InitializetransitionhistoryH
valueandintra-optionvaluefunctionsrespectively. Unlike fort=1tob do
1
UVFAswhereeachrowrepresentstime-indexofonetran- H←H∪(s ,o ,a ,s )
t t t t+1
sition in the history and each column corresponds to one endfor
goal,thedatamatricesinthisprocesscanhavemorethan fori=1tob do
2
onedimension: PickarandomtransitioninH
Pickarandomgoal(oriterateovergoals)
• For the option-value function Q we can expect a UpdateQ
Ω Ω,g
three dimensional matrix where the first dimension UpdateQ
U,g
correspondstothetime,thesecondtothenumberof endfor
goals,andthethirdtotheoptions. Wecanachievethis InitializedatamatricesM andN
Ω;t,g U;t,g
sinceweassumethatthenumberofoptionsisfinite. for(s ,o ,a ,s )inHdo
t t t t+1
forg ∈G do
Inmorelikelycasesitisalsopossibletodosomething
M ←Q (s ,o )
similartoUVFAsandsimplyusethevalueoftheob- Ω;t,g Ω,g t t
N ←Q (s ,o ,a )
servedoptioninsteadofallpossibleoptions,inwhich U;t,g U,g t t t
endfor
casewewouldobtainatwo-dimensionaltensorwith
endfor
the factorized streams being over Φ(s,o) and Ψ(g)
DecomposeintoembeddingsQ (s,o,g)=Φ.Ψ.X and
insteadofΨ(s),Ψ(g)andX(o) Ω
Q (s,o,a,g)=ϕ.ψ.χ.δ
U
• Fortheintra-optionvaluefunctionQ wecanexpect Initializethreefunctionapproximators(neuralnetworks
U
three or four dimensions. The first dimension corre- inourcase)forQ Ω(s,o,g;θ)andfourfunctionapproxi-
spondstothetime-indexofonetransitioninthehistory, matorsforQ U(s,o,a,g;η)
theseconddimensioncorrespondstoeachgoalinthe fori=1tob 3do
Horde. Inthefirstvariantthethirddimensioncorre- Learnparametersθandηby:
spondstothenumberofoptions,withthevalueforthe UpdatingΦˆ(s t)toΦ,Ψˆ(g t)toΨandXˆ(o t)toX
observed action being stored for all options—a case Updatingϕˆ(s t)toϕ,ψˆ(g t)toψ,χˆ(o t)toχandδˆ(a t)
morelikelywithinfiniteactions. Inthesecondvariant toδ
withafinitenumberofactions,thethirddimensioncan endfor
correspondtotheoptionsandthefourthtotheactions. return Q Ω(s,o,g;θ) := h(Φ(s),Ψ(g),X(o)) and
Q (s,o,a,g;η):=h(ϕ(s),ψ(g),χ(o),δ(a))
U
SincewewouldliketoconstructH-UVFAsusingonlyour
transitionhistoryandnoadditionaldataorassumptions,we
conductexperimentsusingatwo-dimensionaldatamatrix
forQ andathree-dimensionaldatamatrixforQ .
Ω U
transitionhistoryiscollectedusingtwocompleteepisodes
Thechallengeinthismethodarisesfromthelikelyscenario fromeachHordeandthedatamatrixispopulatedusingonly
of not observing enough transitions that cover the entire thetransitionhistoryasdescribedintheprevioussection.
state, goal, option, andactionspacesufficiently. Wefind Likethesupervisedlearningexperiments,weusearankof
thatwithenoughtransitionsandadiversesetoftargetin 50forfactorizingtheembeddingsboththevaluefunctions.
theHorde,themethodissufficienttolearnH-UVFAs. Sim-
WetestextrapolationofthelearnedH-UVFAstogoalsin
ilartotheresultsforthesupervisedlearningexperiments,
thefourthroomandplottheresultsinFigure6. Theagent
theseH-UVFAsgeneralizetounseengoalsandoutperform
guidedbyH-UVFAsisabletoperformwellandgeneralize
UVFAsonboththetrainedandunseengoals.
tounseengoals; however,thisisnotthecasewhenusing
UVFAs. This comparison in performance is further dis-
5.2.ReinforcementLearningExperiments
cussedinSection6. Itispossible,inboththesupervised
5.2.1.HORDEGENERALIZATION andreinforcementlearningexperiments,toachievesimilar
performancewithalowerrank.
Thefirstsetofreinforcementlearningexperimentsconstruct
H-UVFAs using a target of Hordes as described in Algo- Wefindthatthereinforcementlearningapproachhasslightly
rithm1. Weuseatargetof15diverseHordes,consisting highervariance,asexpectedduetothestochasticconditions
of 15 goals spread out over three of the four rooms. The ofdatacollection.
6HierarchicalUniversalValueFunctionApproximators
(cid:104) (cid:105)
Q (s ,o ,a ,g)←α r +γmaxQ (s ,o ,a′,g)+
U t t t t U t+1 t
a′
+(1−α)Q (s ,o ,a ,g)
U t t t
LikeSchauletal.(2015),wefindthatlearningH-UVFAsvia
bootstrappingishighlyunstableandpronetohighvariance.
Wewereable tolearn H-UVFAs usinga lowupdate rate.
WewereunabletolearnUVFAsusingbootstrapping,except
fortheoption-valuefunctionwhichisequivalenttolearning
UVFAswithactionsinsteadofoptions.
6.ComparisonwithUVFAs
LearningUVFAs,insteadofH-UVFAs,wouldresultintwo
two-dimensionalmatricesthatarefactoredas
Figure6.ComparisonofH-UVFAsandUVFAstounseengoals
whentrainedwithreinforcementlearningfromaHordeoftargets.
Q (s,o,g;θ)=h(Φ(s,o),Ψ(g))
H-UVFAsstilloutperformUVFAs.Thismethodhashighervari- Ω
ance. Q (s,o,a,g;η)=h(ϕ(s,o,a),ψ(g))
U
LearningQ (s,o,g;θ)isakintolearningUVFAswithop-
5.2.2.LEARNINGWITHBOOTSTRAPPING Ω
tionsinsteadofactions;however,learningQ (s,o,a,g;η)
U
SimilartoUVFAs,itispossibletolearnH-UVFAsdirectly involvescompressingalotofinformationaboutthestate,op-
usingbootstrappingfollowingvaluefunctionupdates:: tion,andactionintoasingleembedding. Thiscompression
ofalargeamountofinformationintothesingleembedding
leadstothepoorperformanceandhighvarianceofUVFAs
(cid:104) (cid:16) asthepolicies/valuefunctionsofdifferentoptionsarenot
Q (s ,o ,g)←α r +γ (1−β (s ))Q (s ,o ,g)
Ω t t t ot t+1 Ω t+1 t interchangeableorcomparablebyasimilaritymeasure.
(cid:17)(cid:105)
+β (s )maxQ (s ,o′,g) +(1−α)Q (s ,o ,g) Thisis visualizedinAppendix Cwheredifferent options
ot t+1
o′
Ω t+1 Ω t t
specializetodifferentroomsandgeneratebehaviorspertain-
ingtotheir”sub-goal”toachievethelargergoal. Sincewe
and
getoptionsthatspecializetoeachroom,itisnotpossibleto
usethevaluefunctionsofanoptionspecialisedinoneroom
to learn the behavior of an option specialized in another
(cid:104)
Q (s ,o ,a ,g)←α r + room.Thisisfurtherbolsteredinthereinforcementlearning
U t t t t
approach,sinceonewouldnotacquireenoughsamplesof
(cid:16)
γ (1−β ot(s t+1))Q U(s t+1,o t,a t,g)+ all the options-action pair in a room where that option is
(cid:17) (cid:105) barelypicked.
β (s )maxQ (s ,o ,a′,g) + +
ot t+1
a′
U t+1 t
Assuch, wedrawthefollowingcomparisonsbetweenH-
(1−α)Q U(s t,o t,a t,g) UVFAsandUVFAsinhierarchicalsettings:
We can assume ∀s ∈ S : β (s) = 1, that is, all options • H-UVFAsachievebetterperformanceandgeneralize
ot
terminateeverywhere,tomaketheequationsintoavariation better since they split all the embeddings into their
ofQ-Learning: respectivedimensionsinsteadofcompressingalarge
amountofinformationintoasingleembedding.Thisis
furtherapplicableinthereinforcementlearningmeth-
(cid:104) (cid:105) odsoftrainingH-UVFAs
Q (s ,o ,g)←α r +γmaxQ (s ,o′,g)
Ω t t t Ω t+1
o′
• H-UVFAsrequirelessdataandarelesscomputation-
+(1−α)Q (s ,o ,g)
Ω t t allyintense. Inthereinforcementlearningmethodone
wouldrequirealargeamountofdatatoexploremore
and option-actionpairs.
7HierarchicalUniversalValueFunctionApproximators
• H-UVFAs are more stable. This is seen in the rein- Precup,D. TemporalAbstractioninReinforcement
forcementlearningexperimentswheretheyhavesignif- Learning. PhDthesis,UniversityofMassachusetts
icantlylowervarianceintheHordeoftargetsapproach Amherst,2000.
andcanbelearnedwiththebootstrappingmethodfor
Schaul,T.,Horgan,D.,Gregor,K.,andSilver,D. Universal
thelower-levelvaluefunctions.
valuefunctionapproximators. InBach,F.andBlei,D.
(eds.),Proceedingsofthe32ndInternationalConference
7.OngoingWork
onMachineLearning,volume37ofProceedingsof
MachineLearningResearch,pp.1312–1320,Lille,
We are currently working on extending the results to a
France,07–09Jul2015.PMLR. URL
more complex domain in the Arcade Learning Environ-
https://proceedings.mlr.press/v37/
ment (Bellemare et al., 2013), and will update the paper
schaul15.html.
withfurtherresults.
Silva,B.D.,Konidaris,G.,andBarto,A. Learning
8.DiscussionandFutureDirections parameterizedskills. InProceedingsofthe29th
InternationalConferenceonMachineLearning2012,
WehaveintroducedHierarchicalUniversalValueFunction
2012. URL
Approximators(H-UVFAs),auniversalapproximatorfor
https://arxiv.org/abs/1206.6398.
goal-directed hierarchical tasks. We discuss supervised
learning and reinforcement learning based approaches to Sutton,R.S.andBarto,A.G. ReinforcementLearning: An
learnH-UVFAsandprovideexperimentsforthesame. We Introduction. ABradfordBook,Cambridge,MA,USA,
showgeneralizationoflearnedH-UVFAstounseentasks, 1998. ISBN0262039249.
andshowthatH-UVFAshavebetterperformanceandgen-
Sutton,R.S.,Precup,D.,andSingh,S. BetweenMDPs
eralizationtothebaselineofUVFAs.
andsemi-MDPs: Aframeworkfortemporalabstraction
Asafuturedirection,weproposeaworkthatextendsthe inreinforcementlearning. ArtificialIntelligence,112(1):
universalapproximatortoalsoincludetheterminationfunc- 181–211,1999. ISSN0004-3702. doi:
tionusedinhierarchicalreinforcementlearning. https://doi.org/10.1016/S0004-3702(99)00052-1. URL
https://www.sciencedirect.com/
Acknowledgements science/article/pii/S0004370299000521.
WewouldliketothankProfessorEliotMossandIgnacio Sutton,R.S.,Modayil,J.,Delp,M.,Degris,T.,Pilarski,
Gavierfortheirinsightfulcommentsanddiscussions. P.M.,White,A.,andPrecup,D. Horde: Ascalable
real-timearchitectureforlearningknowledgefrom
unsupervisedsensorimotorinteraction. InThe10th
ImpactStatement
InternationalConferenceonAutonomousAgentsand
Thispaperpresentsworkwhosegoalistoadvancethefield MultiagentSystems-Volume2,AAMAS’11,pp.
of Machine Learning. There are many potential societal 761–768,Richland,SC,2011.InternationalFoundation
consequences of our work, none which we feel must be forAutonomousAgentsandMultiagentSystems. ISBN
specificallyhighlightedhere. 0982657161.
Tomasi,G.andBro,R. PARAFACandmissingvalues.
References ChemometricsandIntelligentLaboratorySystems,75:
163–180,022005. doi:
Bellemare,M.G.,Naddaf,Y.,Veness,J.,andBowling,M.
10.1016/j.chemolab.2004.07.003.
TheArcadelearningenvironment: Anevaluation
platformforgeneralagents. JournalofArtificial Tucker,L.R. Somemathematicalnotesonthree-mode
IntelligenceResearch,47:253–279,jun2013. factoranalysis. Psychometrika,31:279–311,1966. doi:
https://doi.org/10.1007/BF02289464.
Keshavan,R.H.,Oh,S.,andMontanari,A. Matrix
completionfromafewentries. CoRR,2009. doi:
abs/0901.3150.
Kolda,T.G.andBader,B.W. Tensordecompositionsand
applications. SIAMReview,51(3):455–500,2009. doi:
10.1137/07070111X. URL
https://doi.org/10.1137/07070111X.
8HierarchicalUniversalValueFunctionApproximators
A.ExperimentDetails
WeusetheFourRoomsdomain(Suttonetal.,1999)forourexperiments. Theagentcanbeinitializedinanyofthefour
rooms. Duringtraining,thegoalcanbeinitializedinonlythreeofthefourrooms. Whentestingforgeneralization,thegoal
ispickedonlyinthefourthroom. Theagentreceivedarewardof1forreachingthegoaland0otherwise. Theepisodes
terminateafter1000timesteps.
WhenevaluatingH-UVFAsandUVFAs,weassumetheagentcanterminateinagivenstatewithaprobabilityof0.5. When
trainingH-UVFAsandUVFAs,weassumetheagentterminatesinonestepwhenapplicable. Allnetworksweuseconsistof
hiddenlayersofsize128andRectifiedLinearUnitsactivationfunctions. Weusealearningrateof0.05forthesupervised
learningapproachandreinforcementlearningHordeapproach. Wealsoexperimentedwithotherlearningrates. Thestates
andgoalsaretrainedwithabatchsizeof16,whiletheoptionsandactionsaretrainedwithabatchsizeof2.
B.FactorizedMatricesandRecreations
Figure7.AcomparisonofthegroundtruthandH-UVFAforQ (s,o,g;θ)andQ (s,o,a,g;η)whenH-UVFAsarecreatedusing
Ω U
supervisedlearning.
9HierarchicalUniversalValueFunctionApproximators
Figure8.AcomparisonofthegroundtruthandH-UVFAforQ (s,o,g;θ)andQ (s,o,a,g;η)whenH-UVFAsarecreatedusingthe
Ω U
reinforcementlearning.
10HierarchicalUniversalValueFunctionApproximators
Figure9.Therecreationofthemeta-policyandoption-policiesforasinglegoalwiththeembeddingstrainedon25goals.Theground
truthisinbluewiththerecreationsinred.
11HierarchicalUniversalValueFunctionApproximators
C.ExplainingUVFAPerformance: OptionsasSpecializedBehaviors
Figure10plotsthemeta-policyandintra-optionpolicieswhenthegoalislocatedrightbelowtherightcorridor.
Wecanseethattheoptionsspecializetoasubsetoftheroomsanddonotlearnapolicyinotherrooms. Thepolicyof
theoptioninaroomitdoesn’tinitializeinleadstosub-optimalorevenincorrectbehavior. Forinstance,option0doesn’t
initializeinthebottom-leftroom,option1doesn’tinitializeinthebottom-rightroomandoption3doesn’tinitializeinthe
top-rightroom. Thepoliciesoftheoptionsintheseroomsisnon-existentandevenmovestheagentawayfromthegoal.
Hence,theseoptionslearnspecializedskillsandbehaviorspertainingtoa”subgoal”construct. Inthereinforcementlearning
ofUVFAs,thehistorymightonlyincludetransitionspertainingtoagivenoptioninagivenareaofthemap,however,that
optionmightbespecializedtoanotherroomforanothergoalandhencetherelevantinformationisnotlearnedbytheUVFA.
UsingH-UVFAsgivesusthegranularcontrolneededtoselectthecorrectoptionsinthenecessaryrooms.
Figure10.ThePolicy-over-optionsandintra-optionpoliciesforasinglegoalandfouroptionswhenthegoalislocatedrightbelowthe
rightcorridor.
12