The Dynamics of Social Conventions in LLM populations:
Spontaneous Emergence, Collective Biases and Tipping
Points
Ariel Flint Ashery,1 Luca Maria Aiello,2,3 Andrea Baronchelli,1,4,*
1. DepartmentofMathematics,CityStGeorge’s,UniversityofLondon,NorthamptonSquare,
London,EC1V0HB,UK.
2. ComputerScienceDepartment,ITUniversityofCopenhagen,RuedLanggaardsVej7,2300,
Copenhagen,Denmark.
3. PioneerCentreforAI,3ØsterVoldgade,1350,Copenhagen,Denmark.
4. TheAlanTuringInstitute,BritishLibrary,96EustonRoad,London,NW12DB,UK.
*Correspondingauthor: a.baronchelli.work@gmail.com
Abstract
Socialconventionsarethefoundationforsocialandeconomiclife. AslegionsofAI
agents increasingly interact with each other and with humans, their ability to form
sharedconventionswilldeterminehoweffectivelytheywillcoordinatebehaviors,inte-
grate into society and influence it. Here, we investigate the dynamics of conventions
withinpopulationsofLargeLanguageModel(LLM)agentsusingsimulatedinteractions.
First,weshowthatgloballyacceptedsocialconventionscanspontaneouslyarisefrom
localinteractionsbetweencommunicatingLLMs. Second,wedemonstratehowstrong
collective biases can emerge during this process, even when individual agents appear
tobeunbiased. Third,weexaminehowminoritygroupsofcommittedLLMscandrive
socialchangebyestablishingnewsocialconventions. Weshowthatoncetheseminor-
itygroupsreachacriticalsize,theycanconsistentlyoverturnestablishedbehaviors. In
allcases,contrastingtheexperimentalresultswithpredictionsfromaminimalmulti-
agentmodelallowsustoisolatethespecificroleofLLMagents. Ourresultsclarifyhow
AI systems can autonomously develop norms without explicit programming and have
implicationsfordesigningAIsystemsthatalignwithhumanvaluesandsocietalgoals.
1 Introduction
Social conventions shape social and economic life, determining how individuals behave
and their expectations [1, 2, 3, 4]. They can be defined as unwritten, arbitrary pat-
terns of behavior that are collectively shared by a group. Examples range from con-
ventional greetings like handshakes or bows, to language and moral judgments [5, 6].
Recentnumerical[7,8]andexperimental[9]resultshaveconfirmedthehypothesisthat
conventions can arise spontaneously, without the intervention of any centralized insti-
tutions [5, 3, 10, 11]. Individuals’ efforts to coordinate locally with one another can
generate universally accepted conventions.
Do universal conventions also spontaneously emerge in populations of Large Lan-
guage Models (LLMs)? This question is critical for predicting and managing AI behavior
in real-world applications, given the proliferation of LLMs using natural language to in-
teractwithoneanotherandwithhumans[12,13]. Itisalsoimportantforensuringthat
AI systems behave in ways aligned with human values and societal goals [14].
AsecondkeyquestionconcernstheeffectofthebiasesoftheindividualLLMsonthe
processleadingtotheemergenceofuniversalconventions. Akeyinsightfromcomplexity
science is that collective processes can either suppress or amplify individual traits [15].
As great emphasis is given to assessing and countering biases in one-to-one interaction
between a human and an LLM [16, 17, 18], less attention has been paid so far to the
effects of those biases on repeated communications in populations of LLMs, and even-
tually in mixed human-LLM ecosystems [14]. However, predicting group behavior based
solely on the knowledge of a single agent is extremely challenging [19], and the safety of
a single LLM does not necessarily correspond to the safety of a multi-agent system [20].
1
4202
tcO
11
]AM.sc[
1v84980.0142:viXraFinally, a third question concerns the robustness of social conventions. Recent theo-
retical [21] and empirical [22] work has shown how a minority of committed agents can
exert an outsized influence on the group, provided they reach a threshold or ‘critical
mass’ [23, 24, 25]. Investigating how norms change through critical mass dynamics in
a population of LLMs will help anticipate and potentially steer the development of ben-
eficial norms in AI systems, while mitigating risks of harmful norms [26]. It will also
provide valuable models for how AI systems might influence and be influenced by social
dynamics in human-AI interactions, with potential impact on global challenges such as
antibiotic resistance [27] and the post-carbon transition [28].
In this paper, we address these three questions – on the spontaneous emergence of
conventions, the role of individual biases, and critical mass dynamics – in populations
of LLMs within an established theoretical and experimental framework. In particular,
weconsidertheprototypicalproblemofwhetherpurelylocalinteractionscantriggerthe
emergence of a universal naming convention [29, 30], and investigate the dynamics of
the process.
2 Experimental Setting
2.1 Background and Framework
Our approach builds on Wittgenstein’s general model of linguistic conventions, where
repeated interactions lead to collective agreement between two players [30]. Theoretical
extensionsofthisapproachhavearguedthatpurelylocalinteractionstakingplaceonso-
cial networks can lead to population-wide, or ‘global’, coordinated behavior [1, 2, 31, 6].
Theoretical predictions for our study are based on the naming game model of conven-
tion formation, where agents, aiming to coordinate in pairwise interactions, accumulate
a memory of past plays, which they then use to "guess" the words their subsequent
partnerswilluse[7,8]. Extensivenumericalandanalyticalstudieshaveshownhowthe
modelcapturestherapidgrowthofuniversallysharedsocialconventionsindifferentset-
tings[6]. Derivedlaboratoryexperimentsinvolvinghumanparticipantsinnaminggames
have provided the first empirical evidence for the spontaneous emergence of shared lin-
guistic conventions [9].
The naming game framework has also been applied to study norm change and criti-
calmasstheory, whichpositsthatcommittedminoritiescanoverturnstablesocialcon-
ventions once their size reaches a tipping point, or ‘critical mass’. Theoretical models
suggest critical masses between 10% and 40% of thepopulation [21, 32]. Empirical evi-
dence from controlled social coordination experiments, which closely follow the scheme
describedabove,supportsa25%threshold[22]. However,real-worldobservationsreveal
awiderrange, withsomestudiesproposing30-40%forgenderconventionsincorporate
leadership [24, 33], and others indicating that minorities as small as 0.3% can trigger
significant linguistic and social changes [34, 35, 36, 28].
2.2 Experimental Setup
A simulation ‘trial’ consists of a population of N interacting agents. At each time-step,
two agents are randomly selected for interaction. Both agents select a convention, or
‘name’,fromapoolofafinitesizeW,andattempttoblindlycoordinatewithoneanother.
If they manage to coordinate, they are rewarded with an increase in their game score,
otherwise they are penalized. Agents are not informed that their co-player is sampled
from a population and are not incentivized to reach a ‘global’ consensus but only to
coordinate in a pairwise manner with their partner on each round. Importantly, agents
areabletorememberdetailsaboutthepastM interactionstheyparticipatedin,including
their co-player’s convention choice, their own convention, whether the interaction was
successful or not, and their own accumulated score over these M interactions. The
agents’memoryisinitiallyempty,sothatattheirfirstinteractiontheyproducearandom
convention chosen from the pool of available names. After each interaction, agents see
2the conventions they and their co-player have chosen, and their cumulative score is
updated accordingly. Finally, in the experiments on norm change and critical mass
theory, we introduce a small number of adversarial agents (i.e., a ‘committed minority’)
intoeachpopulation,whoattempttooverturntheestablishedconventionbyconsistently
promoting a novel alternative [21, 22].
Thesedynamicsreflectcommontypesofonlineinteractionswherecommunitymem-
bersengagedirectlywithalarge,oftenanonymouspopulation–usingchatinterfacesor
messagingtechnologies–leadingtotheadoptionoflinguisticandbehavioralconventions
that enable effective coordination with other participants’ expectations [9, 37, 38, 22],
and here are implemented with four different LLM models: Llama 2 70b, Llama 3 70B,
Llama 3.1 70B, and Claude 3.5 Sonnet (see Methods).
2.3 Prompting
Interactions within the game take place in the form of a series of text-based moves. In
each interaction, the LLM agent is given a text prompt comprised of a system prompt
and a user input prompt. The system prompt contains all information about the game.
The user input requests the agent to predict a player’s next action based on the history
of choices in the M most recent interactions. This positions the agent as an external
observer of the game, tasked with forecasting the upcoming round. In practice, these
decisionsdictatethestateofplay. Agentsreceivenoinformationabouttheplayers’iden-
tities or personalities, such as whether they are rational actors. Consequently, we can
interpret the agent’s recommendations as their de-facto participation in the game. The
systemprompt(seeMaterialsandMethods)isdesignedsuchthattheagent’soutputfol-
lows a consistent format, from which we can extract its decision. Following previous
works on LLMs’ cognitive abilities [39], we ask the agent to ‘think step by step’ and to
explicitly consider the history of play. The prompt thus encourages agents to make a
decision based on their previous experience, but provides no instruction as to how it
should be used in the decision making process. Agents are asked to select a name from
the name pool, which is presented to them as a list of W unique letters sampled from
the English alphabet. Ordering bias is removed by randomizing the list of presented let-
ters for each player at every interaction. A successful interaction garners equal rewards
for the participating agents, whereas a failure to coordinate results in a penalty. In the
absence of human guidance, LLMs are notoriously bad at arithmetic. To avoid decision
errors based on a misjudgment of the game state, we explicitly provide the agent with
both the payoff they obtained at each round and their cumulative score within memory
range. Lastly, to ensure that the responses generated by the LLM are correctly guided
by the prompt and not merely the result of random hallucinations [40], we have im-
plemented a meta-prompting strategy to assess the LLM’s understanding of the given
instructions. This practice, previously used in evaluating LLMs within game-theoretical
frameworks [41], consists of posing a series of text comprehension queries to the LLM
andevaluatingtheprecisionofitsresponses. TheLLMssubjectedtoourtestingdemon-
strated good comprehension capabilities, as detailed in SI Section 6.4.
3 Results
To balance experimental time, which should allow for multiple repetitions, with param-
eters that provide agents a rich set of alternatives and meaningful awareness of their
history, we set the name pool size to W =10 and the individual memory length to M =5
forpopulationsofN =24agents,unlessotherwisespecified. Theresultspresentedbelow
remain robust with respect to variations in these parameters (see Fig. SI5).
3.1 Spontaneous emergence
Fig. 1 shows that group-wide linguistic conventions spontaneously emerge across all
models. Initial interactions have a low probability of being successful, but stochastic
3fluctuationsbreaktheinitialsymmetrybetweentheconventions,andeventuallyonebe-
comesdominant. TheinsetofFig.1showsthatthetheoreticalmodel(seeSISection6.2
for a description) captures the dynamics generated by the LLM populations. The curves
inFigure1concernapopulationsizeofN =24agents,butconvergenceisalsoobserved
for larger systems (N = 200, see Fig. SI5) and larger name pools (W = 26, see Fig. SI1).
A population round is defined as N microscopic interactions, a common approach in
multi-agent simulations [42].
A
Llama 3 70B Claude 3.5 Sonnet
Llama 3.1 70B Llama 2 70b
1.0
0.8
0.6
0.4 1
0.2
0
0.0 0 25
0 10 20 30 40 50
Population Round
B
1.0
0.8
0.6
0.4
0.2
0.0
2 4 6 8
Population Round
Figure1: The spontaneous emergence of conventions. (A)Thesuccessrate–i.e.,the
probabilityofobservingasuccessatagiventime–forpopulationsizeN =24andaname
pool of size W = 10, for each of the four models. Thick lines represent average curves
obtainedfrom40experimentalruns,whilethinlinesarerepresentativeindividualruns.
Inset: Success rate of the theoretical minimal naming game model, under the same
constraints. (B) Word competition in a single run in a population of Llama 3.1 agents.
Different markers and colours represent the trajectories of unique conventions. Each
data point is a bin averaging the past N interactions.
3.2 Collective bias in convention selection
Having established that social conventions emerge, a natural question arises: what are
these conventions? The single Latin alphabet letters available in the name pool are all
4
etaR
sseccuS
ytilibaborP
noitcudorPequallyvalidasglobalconventions,andsowewouldexpectthemtoalltohavethesame
probability to become the accepted social convention, as supported by the theoretical
model[8](seealsoSISection6.2). However,theexperimentalresultspresentadifferent
picture (Fig. 2A). The probability that a particular name becomes the social convention
is neither uniform nor deterministic. Some names appear to have a significantly higher
likelihood of becoming the adopted convention than others. This pattern holds across
models, although the preferred names vary between models.
Two hypotheses could explain the observed behavior. The selection process may be
non-uniform due to (i) intrinsic model (i.e., individual single-agent) biases or (ii) prompt
features, specifically the order in which names in the name pool are presented to the
agents,asnotedinadifferentcontext[43]. Thelatterhypothesiscanbediscardedsince,
asmentionedabove,thenamesarepresentedtotheagentsinalistinrandomizedorder
for each agent and at every interaction.
Havingruledouttheorderofnamepresentationasafactor, wecanfocusontherole
of individual (i.e. single-agent) biases in shaping collective behaviour. The hypothesis
that individual bias can be responsible for a collective bias is supported by the theoret-
ical model. When the model is run with only two names, a bias towards a particular
namequicklyresultsinunilateralconvergenceonthatnameatthepopulationlevel(see
Fig. SI3). To test this intuition in our experiment, we examine the selection preferences
of individual agents during their first round, when they have no prior memory. We find
that individual biases are indeed possible. For example, when agents can choose any
letter from the complete English alphabet, the population systematically converges on
the letter ‘A’ because individual agents overwhelmingly prefer to select it over all other
letters, even without prior memory (see Fig. SI1). However, a similar test on the fre-
quency of name selection by agents with no prior memory for the case of Fig. 1, where
the name pool contains ten elements but not the letter ’A’, yields mixed results. Under
these conditions, individual Llama 2 70b and Claude 3.5 Sonnet agents are unbiased
across conventions in this name pool (χ2-test, P = 0.100, 0.410), whereas individual
Llama 3/3.1 agents exhibit a significant statistical skew in their name selections (see
Fig. SI2). In all cases, the final consensus distribution shows that specific names are
favoured as a consensus option, even if they appeared to be less likely to be selected
in the initial step (Fig. 2A). Thus, both social conventions and collective biases in the
selection process emerge also in absence of individual biases.
The findings suggest that collective bias may stem from the convention formation
process itself, as agents are exposed to diverse memory states with different name com-
binationsandsuccess-failuresequences. Totestthishypothesis,wefocusonthecaseof
anamepoolsizeW =2,sincetrackingpotentialconfoundersofbiasbecomesimpractical
as the space of possible names increases. Fig. 2B shows shows that across all models,
although agents are initially unbiased, local communication and coordination lead to a
collective bias toward a specific convention, which we term the ‘strong convention’ (as
opposed to its ‘weak’ counterpart).
The top row of Table 1 shows a case where there is no individual bias towards a par-
ticular name in the first interaction (P=0.116 > 0.05, indicating that the evidence is not
strong enough to reject the hypothesis that the agent is unbiased). In the second inter-
action, agents have some memory influencing their decision, but the observed outcome
probability remains symmetric (P=0.110). We observe that if an agent succeeds in the
first interaction, it will almost surely continue to use the successful name in the next
interaction (99.4% of the time in the data in Table 1, with similar results in real simula-
tionsandforothermodels). However,ifanagentfails,itwillalmostsurelyswitchnames
(97.3%ofthetime). InalltestedcaseswithW =2,andacrossallmodels,anasymmetric
selectionbiasemergesbytheagent’sthirdinteraction,distinguishingbetweenthe‘weak’
anda‘strong’conventions. Acrossallobservedmemorystates(notingthatsomeconfigu-
rationsareextremelyrareordonotoccurduetotheagent’sdecision-making),agentsare
morelikelytoproducethestrongnamein5outof8memoryconfigurations. Thesecon-
figurations account for the vast majority of observed states (99.2% of the possible states
resulting from interaction 2 in Table 1, with similar patterns observed in other models).
Insubsequentinteractions,agentsaremorelikelytoencounterthestrongnameassoci-
5A
Llama 3 70B Claude 3.5 Sonnet
Llama 3.1 70B Llama 2 70b
1.00
0.75
0.50
0.25
0.00
Q M X Y F J P R C D
Convention
B
1.0 1.0
* * * *
0.5 0.5
0.0 0.0
B B t b B B t b
3
70
1
70 onne
2
70
3
70
1
70 onne
2
70
a
m
a
m
a
3.
3.5
S
a
m
a
a
m
a
m
a
3.
3.5
S
a
m
a
Ll a e Ll Ll a e Ll
Ll ud Ll ud
a a
Cl Cl
Figure2: Collective Bias in Convention Selection. (A)Distributionofconsensuscon-
ventions, for a name pool of size W = 10 (N = 24). Results of 40 runs for the Llama 3
and Llama 3.1 models, and 27 and 20 runs for Claude 3.5 and Llama 2, respectively.
Thecollectivedynamicssystematicallyamplifyindividualbiases(showninFig.SI2). (B),
IndividualvsCollectivebiasforW =2,namepool{Q,M}. Leftpanel: probabilityofselect-
ing the strong (weak) convention for agents with no prior memory. Raw values reported
in SI Table SI4. Asterisks (*) indicates that there is insufficient evidence to reject the
nullhypothesisthatthemodelisunbiasedatthe5%significancelevel(calculatedusing
an exact Binomial test from 10,000 samples per model, apart from Llama 3 which had
5,000 samples, see Methods). Corresponding p-values for the models (from left to right)
are P = 0.068, 0.116, 0.757, and 0.849. Right: the proportion of runs (40) that resulted
in consensus on the respective convention. Raw values reported in SI Table SI5.
atedwithsuccess,reinforcingitsuseandultimatelyleadingtoconsensusonthatname
as the social convention.
In summary, our results suggest that a collective bias emerges from repeated inter-
actions among agents who, when tested in isolation (i.e., in interaction 1), appear to be
unbiased in their decision making. This collective preference breaks the initial symme-
try among the different alternatives that could become the social convention, favouring
someoverothers(or,inthecaseofW =2,oneovertheother). Itisimportanttoempha-
sise that this dynamically emerging bias is not required for the spontaneous emergence
ofaconvention. Thecollectiveandindividualbiasesoftheseagentsdrivetheconsensus
towards particular conventions. For reference, the theoretical model produces conven-
6
saiB
evitcelloC
saiB
laudividnI
saiB
evitcelloCtionswithoutanyindividualbias, butaccommodatesitattheindividualleveltoexplain
thedominanceofspecificconventionsovercompetingalternatives[8,6](seeFig.SI3). In
LLMs,onthecontrary,weobservethatbiasemergeswhenagentsdevelopdiversemem-
ory states, which form through a collective process of agent-to-agent communication.
Interaction Memory p(Q) p(M) Aggregated p(M)
Interaction: Played, Observed
1 - .492 .508 .508*
1: Q, M .049 .951
1: M, Q .995 .005
2 .487*
1: Q, Q .997 .003
1: M, M .010 .990
1: Q, M 2: M, Q .451 .549
1: M, Q 2: Q, M .152 .848
1: Q, M 2: M, M .000 1.00
1: M, Q 2: Q, Q .996 .004
3 .563
1: Q, Q 2: Q, M .064 .936
1: M, M 2: M, Q .841 .159
1: M, M 2: M, M .001 .999
1: Q, Q 2: Q, Q .989 .011
Table 1: The origin of collective bias. The behavior of Llama 3.1 70B agents is sim-
ulated for the early phases of the experimental setting with W = 2 and a name pool {Q,
M}, up to the third interaction. The asterisk (*) indicates that the model is statistically
neutral in the respective interaction. In interaction 1, agents are initially unbiased (P
= 0.116, see also Fig. 2B), based on 10,000 name selections by agents with empty
memory. In interaction 2, the convention production probability remains unbiased (P
= 0.110) when aggregated across equally likely memory configurations. Agents gener-
ally adhere to a winning convention but switch to their co-player’s convention follow-
ing failure. By interaction 3, the dominant memory configurations, representing 99.2%
of expected configurations, display a significant bias towards the strong convention, M
(P < 2.2×10−16). In stochastic simulations, some agents will inevitably interact with
others who have more experience. These interactions create a bias toward the strong
convention, as experienced players are more likely to favour it. Thus, the table provides
a conservative estimate of the collective bias emerging for the strong convention.
3.3 Tipping Points and Critical Mass
Social conventions are steady states of the system: once a convention spontaneously
emerges, the population adheres to it indefinitely (see Fig. SI6). A natural question con-
cerns the stability of such steady states: how resistant is a convention to deliberate
efforts to overturn it? To address this question, we investigate whether a committed mi-
nority can ‘flip’ an equilibrium consensus on a convention. We consider the scenario in
which a population has long converged on a convention and every agent has solely ob-
servedthatconventioninthepastM interactions(whichwere,therefore,allsuccessful).
Wethenintroducea‘committedminority’ofagentsproducinganalternativeconvention
[21, 22]. These committed agents follow a fixed strategy and use the alternative conven-
tion at all times. We test populations using the same two-name (W =2) conditions as in
our convergence experiments. We simulate a consensus on each name per combination
and introduce its complementary name as an adversary.
7A
Llama 3 70B Llama 3.1 70B Claude 3.5 Sonnet Llama 2 70b
CM: 0.12 CM: 0.42 CM: 0.67
CM: 0.21
CM: 0.02 CM: 0.0 CM: 0.46
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
0 20 0 20 0 20 0 20
CM: 0.12 CM: 0.42 CM: 0.67
CM: 0.21
CM: 0.02 CM: 0.0 CM: 0.46
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
0 20 0 20 0 20 0 20
Population Round
B
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
Q M Q M Q M Q M
Majority Convention
Figure 3: Committed minority and critical mass dynamics with W =2. Populations
of N = 24 agents (N = 48 for Llama 3) were initialized in two conditions, with complete
consensus on either the weak (Q) or strong (M) convention. Each agent’s memory ex-
clusively stored one convention in each setting, with memory length M = 5 (M = 3 for
Llama 3). (A) The average probability of producing the alternative convention when the
majority holds the weak (top) or strong (bottom) convention. The legend shows the size
of the committed minority (CM). Bold (faint) lines represent the production probability
when the CM reaches the critical mass needed to flip the majority on the strong (weak)
convention. Solidlineswithfilledcirclesindicatethatalltrialsachievedpopulationcon-
sensus on the alternative convention (95% success rate in the past 3N rounds). (B)
Criticalmassneededtoflipthemajorityforeachmodel. RawvaluesreportedinSITable
SI6.
Figure 3 shows that when the committed minority reaches the critical threshold, the
whole population adopts their convention. Below this threshold, the population settles
into a mixed state, as committed agents always use the minority convention. Interest-
ingly, the critical mass of the committed minority needed to trigger a new convention
depends on the convention itself. The stronger name (i.e., the name more likely to be-
comethesocialconventionhadwestartedwithnopriormemory,asseenintheprevious
section) requires a larger committed minority to be overturned. Conversely, a smaller
number of adversarial agents can overturn a consensus on the weaker name.
Interestingly,therelativestrengthofthetwoconventionscanvarysowidelydepending
on the LLM that committed groups as small as 2% (Llama 3 70B) or as large as 67%
(Llama 2 70b) - effectively no longer a minority - were observed (see Fig. 3. In Llama 3.1
70B populations, the bias is so strongly weighted against the weaker convention that
the population spontaneously switches to the alternative, stronger convention without
requiring any committed agents at all.
8
ytilibaborP
noitcudorP
ssaM
lacitirC4 Discussion
Our findings demonstrate that social conventions can spontaneously emerge in popu-
lations of Large Language Models (LLMs) through purely local interactions, without any
centralcoordination. Theseresultsrevealhowtheprocessofsocialcoordinationcangive
rise to collective biases, increasing the likelihood of specific social conventions develop-
ing over others. Importantly, this collective bias is not easily deducible from analyzing
isolated agents, and its nature varies depending on the LLM model used. Additionally,
our work uncovers the existence of tipping points in social conventions, where a minor-
ity of committed agents can impose their preferred convention on a majority settled on
a different one. The critical size of this committed minority is influenced by two factors:
theinterplaybetweenthemajority’sestablishedconventionandtheminority’spromoted
alternative, and the specific LLM model employed.
Our approach aimed to minimize the complexity of both the interaction scheme and
the semantic space to enhance transparency when interpreting the results. It is impor-
tant to delimit the scope of our findings while highlighting possible avenues for future
work. Firstly, our results reveal key aspects of norm dynamics in populations of LLMs
withinanexperimentalsetupthatis,unavoidablyinLLMresearch,reliantonseveralpa-
rametersincludingtheLLMmodel,theprompt,andspecificconventions. Whilerigorous
testing,includingmetapromptingandexperimentrepetitionsusingdifferentparameters,
confirmstherobustnessoftheresultsinthiscontext,animportantaspectoffuturework
will consist of generalizing the results to different controlled experimental settings. In
this context, scaling to larger populations and semantic spaces should also be inves-
tigated [44]. Secondly, we considered only unstructured populations where interacting
pairs are randomly selected. A straightforward yet crucial extension of this work con-
sists of embedding the population in more realistic social networks, which may have a
profoundimpactonthecollectivedynamics[6],aswellasconsideringmicroscopicinter-
actionsinvolvingmorethantwoagents[45]. Finally,tobridgethegapbetweensynthetic
experimentsandreal-worldapplications,anexcitingfrontierforfuturestudyliesincon-
sidering more realistic conventions—such as moving from alphabet letters to sensitive
humannormsrelatedtogender,race,andothersocialcategories—andinvestigatingthe
dynamics of conventions in mixed LLM-human ecosystems, both in laboratory settings
andeventuallyinnaturalenvironmentslikesocialmedia. Ethicalconsiderationsshould
be of course foundational for these kind of experiments.
Within the expanding field of LLM multi-agent systems [46], our work explored the
so-far less-investigated aspect concerning the shared, poorly defined ways agents and
humans solve social problems, such as creating language, norms, and institutions [14].
Inthiscontext,ourresultsonnormchangecouldstimulateresearchintosimilardynam-
ics within the framework of cultural evolution, particularly in chains of communicating
agents [47]. Game theoretical approaches would naturally allow investigation of asym-
metric payoffs’ effects on collective consensus, potentially contrasting individual biases
withexplicitcollectivegoals[48,49]. Furtherpromisingresearchavenuesincludedevel-
opingframeworkstopromotetheemergenceofspecificconventions[50]andhigher-order
social norms [51], as well as testing interactions between agents based on different LLM
models within populations.
Taking a broader perspective, understanding how AI systems spontaneously develop
conventions and more sophisticated norms without explicit programming is critical for
predicting and managing AI behaviour in real-world applications. It is also essential for
ensuringAIsystemsbehaveinwaysthatalignwithhumanvaluesandsocietalgoals. In
particular, despite their rapid adoption, ethical concerns have arisen regarding the bi-
asesexhibitedbyLLMs. Thevast,unfilteredInternetdatausedtotrainLLMscancause
themtopropagateandamplifyharmfulbiases,disproportionatelyaffectingmarginalized
communities [52]. Accordingly, a significant goal of the alignment research community
hasbeentoimprovetheperformanceofLLMsinindividualbiastests[53,54]. Ourwork
shows that alignment also needs to be tested at the group level. So far, mixed results
havebeenachievedwhenmeasuringandimbuinghuman socialnormsinLLMs[55,56],
and as of yet AI agents struggle to represent multiple cultures [57] and continuously
9evolving social norms [58, 59, 26]. We argue that the challenge extends beyond merely
detecting ’undesirable behavior’, to understanding the evolution of social norms held by
agents and how these may influence humans through interactions in human-machine
societies[26]. Inthislight,ourworkrepresentsafirststeptowardsabetterunderstand-
ingofnormdynamicsinpopulationsofLLMs,andweanticipatethatitwillbeofinterest
to researchers and practitioners interested in making AI a tool for societal good.
5 Methods
5.1 Prompt
The system prompt comprises of three components: i) a fixed prompt that outlines the
game’s rules, including the payoff structure and the player’s objective, ii) a dynamic
memory prompt that provides contextual information about the state of play within the
player’smemoryrange,andiii)aninstructionalpromptthatprovidesinformationforhow
theagentshouldformatitsresponse. Theuserpromptaskstheagenttoselectanameto
useinthecurrentinteraction. Weusezero-shotpromptingtodirectlyextracttheagent’s
name decision in response to the state of play. We do not provide instructions as to how
agents should decide their next move, nor do we present them with example strategies.
Weasktheagenttobehaveinaself-interestedmanner,andtheonlypartofthepromptin
whichwesuggesttotheagentthatitshouldconsiderpartakingincoordinationiswhen
we state that the agent’s objective is to ‘maximise their own accumulated point tally,
conditional on the behaviour of their co-player’. We apply fixed payoffs for successful
and failed interactions, set at +100 and -50 points respectively.
5.2 Models and APIs.
For our experiments, we use homogeneous populations of the following LLM agents:
Llama370B12,Llama-3.170B,Llama270b(in4-bitquantisationformat), andClaude
Sonnet 3.5 (see Table 2 for specific versions). In these autoregressive LLMs, each newly
generated word is produced based on previously inputted and generated words, and
so the sequence of generation matters. More precisely, the probability distribution for
predicting the next word is conditional on the product of all previous word probability
distribution. To mimic LLMs deployed in real-world application, we demand all agents
in our experiments to behave non-deterministically by fixing them with a non-zero con-
stanttemperature. Thismeansthatforeachagentthenextgeneratedwordisrandomly
selected from the conditional probability distribution. We use K-sampling to restrict the
probabilitydistributionofthenextwordtothenextK mostlikelywords,thusincreasing
the likelihood of high probability words and decreasing the likelihood of low probability
words which are outside of the name pool (see Table SI3 for all parameter values).
Model Name Model Version
Llama 3 70B Meta-Llama-3-70B-Instruct
Llama 3.1 Meta-Llama-3.1-70B-Instruct
Claude 3.5 Sonnet claude-3-5-sonnet-20240620
Llama 2 70b Meta-Llama-2-70b-Chat
Table 2: Model Names and Versions
1All Llama family models are open-sourced LLMs, released under a commercial use license
(https://ai.meta.com/llama/license/).
2WeuseversionsoftheLlama3familymodelshostedbyHuggingFace,whichweaccessthroughtheInfer-
enceAPI(https://huggingface.co/inference-api/serverless). WequantiseLlama270bintoa4-bitversionusing
HuggingFace’sTransformerslibrary,andrunthemodellocally(https://huggingface.co/docs/transformers).
105.3 Measuring Individual Bias
We quantify the individual bias of agents by measuring the number of times each con-
vention was produced in the first round of the game, when their memory inventory is
empty, over T trials. Experiments with W = 2 are effectively a Bernoulli trial, and so
we measure whether the agent is biased by performing a two-tailed exact Binomial test
with the observed proportions. We calculate the p-value, P, using the null probability
p = 0.5, and reject the hypothesis that the model is biased if P<0.05. For the case of
W =10,weperformaχ2-test,andalsotestthenullhypothesisthatthemodelisneutral
in its convention selection. Thus, we use the expected value 0.1T in our calculations,
and again reject the null hypothesis that the model is unbiased if P<0.05.
5.4 Committed Minorities
To determine the critical size of the committed minority, we identify the point at which
the majority consensus is overturned. A consensus flip occurs when 95% of the past
3N interactions succeed after the introduction of the committed minority. For Llama
3, we tested the smallest minority needed to overturn a weak convention majority, then
repeated the experiment with a strong convention majority to measure the critical mass
within the same time frame. For other models, the critical mass threshold is defined
as the minimum proportion of committed agents that is required to flip the consensus
within 30 population rounds. These criteria account for potential fluctuations in non-
deterministic agent decisions.
Acknowledgements
LMA acknowledges the support from the Carlsberg Foundation through the COCOONS
project (CF21-0432). We acknowledge City St George’s, University of London’s Hyperion
cluster for computation time.
11References
[1] H.P.Young,“Theevolutionofconventions,”Econometrica,vol.61,no.1,pp.57–84,
1993.
[2] P. R. Ehrlich and S. A. Levin, “The evolution of norms,” PLoS Biol, vol. 3, no. 6,
p. e194, 2005.
[3] C.Bicchieri,Thegrammarofsociety: Thenatureanddynamicsofsocialnorms.Cam-
bridge, UK: Cambridge University Press, 2005.
[4] M. J. Gelfand, S. Gavrilets, and N. Nunn, “Norm dynamics: interdisciplinary per-
spectives on social norm emergence, persistence, and change,” Annu Rev Psychol,
vol. 75, pp. 341–378, 2024.
[5] D. Lewis, Convention: Aphilosophicalstudy. Oxford: Blackwell, 1969.
[6] A. Baronchelli, “The emergence of consensus: a primer,” R Soc Open Sci, vol. 5,
no. 2, p. 172189, 2018.
[7] L.Steels,“Aself-organizingspatialvocabulary,”ArtifLife,vol.2,no.3,pp.319–332,
1995.
[8] A. Baronchelli et al., “Sharp transition towards shared vocabularies in multi-agent
systems,” JStatMech, vol. 2006, no. 06, p. P06014, 2006.
[9] D. Centola and A. Baronchelli, “The spontaneous emergence of conventions: An
experimental study of cultural evolution,” Proc Natl Acad Sci USA, vol. 112, no. 7,
pp. 1989–1994, 2015.
[10] F. A. Hayek, Theconstitutionofliberty. Chicago: University of Chicago Press, 1960.
[11] R. Sugden, “Spontaneous order,” JEconPerspect, vol. 3, no. 4, pp. 85–97, 1989.
[12] J. Werfel, K. Petersen, and R. Nagpal, “Designing collective behavior in a termite-
inspired robot construction team,” Science, vol. 343, no. 6172, pp. 754–758, 2014.
[13] L. Brinkmann et al., “Machine culture,” Nat Hum Behav, vol. 7, no. 11, pp. 1855–
1868, 2023.
[14] A. Dafoe et al., “Cooperative ai: machines must learn to find common ground,”
Nature, vol. 593, pp. 33–36, 2021.
[15] M. M’ezard, G. Parisi, and M. A. Virasoro, Spin glass theory and beyond: An In-
troduction to the Replica Method and Its Applications. Singapore: World Scientific,
1987.
[16] D. Roselli, J. Matthews, and N. Talagala, “Managing bias in ai,” in Companion Pro-
ceedings of the 2019 World Wide Web Conference, (New York), pp. 539–544, ACM,
2019.
[17] E.Ferrara,“Shouldchatgptbebiased? challengesandrisksofbiasinlargelanguage
models,” FirstMonday, vol. 28, no. 11, 2023.
[18] T. Hu, Y. Kyrychenko, S. Rathje, N. Collier, S. van der Linden, and J. Roozen-
beek, “Generative language models exhibit social identity biases,” arXiv preprint
arXiv:2310.15819, 2023.
[19] P. W. Anderson, “More is different: Broken symmetry and the nature of the hierar-
chical structure of science,” Science, vol. 177, no. 4047, pp. 393–396, 1972.
[20] U. Anwar etal., “Foundational challenges in assuring alignment and safety of large
language models,” arXiv, vol. 2404.09932, pp. 1–30, 2024.
[21] J.Xieetal.,“Socialconsensusthroughtheinfluenceofcommittedminorities,”Phys
RevE, vol. 84, no. 1, p. 011130, 2011.
12[22] D. Centola, J. Becker, D. Brackbill, and A. Baronchelli, “Experimental evidence for
tipping points in social convention,” Science, vol. 360, no. 6393, pp. 1116–1119,
2018.
[23] T. Kuran, “Ethnic norms and their transformation through reputational cascades,”
JLegalStud, vol. 27, no. S2, pp. 623–659, 1998.
[24] R. M. Kanter, “Some effects of proportions on group life: Skewed sex ratios and
responses to token women,” American journal of Sociology, vol. 82, no. 5, pp. 965–
990, 1977.
[25] D.Dahlerup,“Thestoryofthetheoryofcriticalmass,”PoliticsGender,vol.2,no.4,
pp. 511–522, 2006.
[26] A.Baronchelli,“Shapingnewnormsforai,”PhilosTransRSocB,vol.379,no.1897,
p. 20230028, 2024.
[27] K.Nyborgetal.,“Socialnormsassolutions,”Science,vol.354,no.6308,pp.42–43,
2016.
[28] J. D. Farmer et al., “Sensitive intervention points in the post-carbon transition,”
Science, vol. 364, no. 6436, pp. 132–134, 2019.
[29] D. Hume, A treatise of human nature. Oxford: Oxford University Press, 2000.
Reprint.
[30] L. Wittgenstein, Philosophical investigations. Oxford: Blackwell, 1958. Trans. GEM
Anscombe.
[31] B. Skyrms, Evolutionofthesocialcontract. Cambridge University Press, 2014.
[32] X.Niu,C.Doyle,G.Korniss,andB.K.Szymanski,“Theimpactofvariablecommit-
ment in the naming game on consensus formation,” Scientific reports, vol. 7, no. 1,
p. 41750, 2017.
[33] S. Grey, “Numbers and beyond: The relevance of critical mass in gender research,”
Politics&Gender, vol. 2, no. 4, pp. 492–502, 2006.
[34] M. Diani, “The concept of social movement,” The sociological review, vol. 40, no. 1,
pp. 1–25, 1992.
[35] M. Gladwell, “Small change,” TheNewYorker, vol. 4, 2010.
[36] R. Amato, L. Lacasa, A. Díaz-Guilera, and A. Baronchelli, “The dynamics of norm
change in the cultural evolution of language,” Proceedings of the National Academy
ofSciences, vol. 115, no. 33, pp. 8260–8265, 2018.
[37] L.Backstrom, D.Huttenlocher, J.Kleinberg, andX.Lan, “Groupformationinlarge
social networks: membership, growth, and evolution,” in Proceedings of the 12th
ACM SIGKDD international conference on Knowledge discovery and data mining,
pp. 44–54, 2006.
[38] F. Kooti, H. Yang, M. Cha, K. Gummadi, and W. Mason, “The emergence of conven-
tionsinonlinesocialnetworks,” inProceedingsoftheInternationalAAAIConference
onWebandSocialMedia, vol. 6, pp. 194–201, 2012.
[39] T.Kojima,S.S.Gu,M.Reid,Y.Matsuo,andY.Iwasawa,“Largelanguagemodelsare
zero-shot reasoners,” Advances in Neural Information Processing Systems, vol. 35,
p. 22199–22213, Dec. 2022.
[40] Z.Xu,S.Jain,andM.Kankanhalli,“Hallucinationisinevitable: Aninnatelimitation
of large language models,” arXivpreprintarXiv:2401.11817, 2024.
[41] N. Fontana, F. Pierri, and L. M. Aiello, “Nicer than humans: How do large language
modelsbehaveintheprisoner’sdilemma?,”arXivpreprintarXiv:2406.13605,2024.
13[42] C. Castellano, S. Fortunato, and V. Loreto, “Statistical physics of social dynamics,”
Reviewsofmodernphysics, vol. 81, no. 2, pp. 591–646, 2009.
[43] G. De Marzo, L. Pietronero, and D. Garcia, “Emergence of scale-free networks in
socialinteractionsamonglargelanguagemodels,”arXivpreprintarXiv:2312.06619,
2023.
[44] G. De Marzo, C. Castellano, and D. Garcia, “Language Understanding as a Con-
straint on Consensus Size in LLM Societies,” Sept. 2024.
[45] I. Iacopini, G. Petri, A. Baronchelli, and A. Barrat, “Group interactions modulate
criticalmassdynamicsinsocialconvention,”CommunicationsPhysics,vol.5,no.1,
p. 64, 2022.
[46] T. Guo, X. Chen, Y. Wang, R. Chang, S. Pei, N. V. Chawla, O. Wiest, and X. Zhang,
“Large language model based multi-agents: A survey of progress and challenges,”
arXivpreprintarXiv:2402.01680, 2024.
[47] J. Perez, C. Léger, G. Kovač, C. Colas, G. Molinaro, M. Derex, P.-Y. Oudeyer, and
C. Moulin-Frier, “When llms play the telephone game: Cumulative changes and at-
tractorsiniteratedculturaltransmissions,”arXivpreprintarXiv:2407.04503,2024.
[48] M. Kearns, S. Judd, J. Tan, and J. Wortman, “Behavioral experiments on biased
votinginnetworks,”ProceedingsoftheNationalAcademyofSciences,vol.106,no.5,
pp. 1347–1352, 2009.
[49] J. Duan, R. Zhang, J. Diffenderfer, B. Kailkhura, L. Sun, E. Stengel-Eskin,
M. Bansal, T. Chen, and K. Xu, “Gtbench: Uncovering the strategic reasoning lim-
itations of llms via game-theoretic evaluations,” arXiv preprint arXiv:2402.12348,
2024.
[50] S. Ren et al., “Emergence of social norms in large language model-based agent so-
cieties,” arXiv, vol. 2403.08251, pp. 1–18, 2024.
[51] A.Lazaridou,K.M.Hermann,K.Tuyls,andS.Clark,“Emergenceoflinguisticcom-
munication from referential games with symbolic and pixel input,” in International
ConferenceonLearningRepresentations, 2018.
[52] I. O. Gallegos, R. A. Rossi, J. Barrow, M. M. Tanjim, S. Kim, F. Dernoncourt, T. Yu,
R. Zhang, and N. K. Ahmed, “Bias and Fairness in Large Language Models: A Sur-
vey,” Mar. 2024. arXiv:2309.00770 [cs] version: 2.
[53] Y. Wang, W. Zhong, L. Li, F. Mi, X. Zeng, W. Huang, L. Shang, X. Jiang, and
Q. Liu, “Aligning Large Language Models with Human: A Survey,” July 2023.
arXiv:2307.12966 [cs].
[54] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang,
S.Agarwal,K.Slama,A.Ray,J.Schulman,J.Hilton,F.Kelton,L.Miller,M.Simens,
A.Askell,P.Welinder,P.Christiano,J.Leike,andR.Lowe,“Traininglanguagemod-
elstofollowinstructionswithhumanfeedback,”Mar.2022. arXiv:2203.02155[cs].
[55] Y. Yuan, K. Tang, J. Shen, M. Zhang, and C. Wang, “Measuring Social Norms of
Large Language Models,” Apr. 2024. arXiv:2404.02491 [cs].
[56] K.Hämmerl,B.Deiseroth,P.Schramowski,J.Libovický,A.Fraser,andK.Kersting,
“Do Multilingual Language Models Capture Differing Moral Norms?,” Mar. 2022.
arXiv:2203.09904 [cs].
[57] A.RamezaniandY.Xu,“Knowledgeofculturalmoralnormsinlargelanguagemod-
els,” June 2023. arXiv:2306.01857 [cs].
[58] S. Li, T. Sun, Q. Cheng, and X. Qiu, “Agent Alignment in Evolving Social Norms,”
Feb. 2024. arXiv:2401.04620 [cs].
14[59] H. Shen, T. Li, T. J.-J. Li, J. S. Park, and D. Yang, “Shaping the Emerging Norms of
Using Large Language Models in Social Computing Research,” in Companion Publi-
cation of the 2023 Conference on Computer Supported Cooperative Work and Social
Computing, CSCW ’23 Companion, (New York, NY, USA), pp. 569–571, Association
for Computing Machinery, Oct. 2023.
[60] E. Akata, L. Schulz, J. Coda-Forno, S. J. Oh, M. Bethge, and E. Schulz, “Playing
repeatedgameswithlargelanguagemodels,”arXivpreprintarXiv:2305.16867,2023.
[61] T. Ullman, “Large language models fail on trivial alterations to theory-of-mind
tasks,” arXivpreprintarXiv:2302.08399, 2023.
[62] G. V. Aher, R. I. Arriaga, and A. T. Kalai, “Using large language models to simulate
multiplehumansandreplicatehumansubjectstudies,”inInternationalConference
onMachineLearning, pp. 337–371, PMLR, 2023.
[63] B. Lester, R. Al-Rfou, and N. Constant, “The power of scale for parameter-efficient
prompt tuning,” arXivpreprintarXiv:2104.08691, 2021.
156 Supplementary Information
6.1 Measuring Bias
6.1.1 Microscopic Bias
To measure the production bias in Table 1, we assess both the interaction-level bias
and the bias within each unique memory configuration. The interaction-level bias is
definedastheoverallproductionprobabilityofthestrongconventionacrossallpossible
configurations (per interaction), which we test using an exact Binomial test with a null
probability of p = 0.5, rejecting the null hypothesis if the p-value falls below 0.05. The
results of the tests are reported in the caption of Table 1. At the configuration level, we
first perform an exact Binomial test, as above, to check whether the model is biased. In
all cases, the p-value P† < 0.05, confirming that the model’s decision is biased towards
an extreme. Then, we use bootstrapping by resampling 70% of the observations for each
configuration 10,000 times and measure the proportion of samples showing a stronger
bias than the observed value in Table 1. In all cases, we obtain a bootstrapped P‡ >
0.05,indicatingthatwecannotrejectthehypothesisthatthemodel’sunderlyingbiasis
more extreme than the observed bias. Results are reported in Table SI1.
Interaction Memory p(M) P† P‡
Interaction: Played, Observed
1: Q, M .951 < 2.2 ×10−16 .522
2 1: M, Q .005 < 2.2 ×10−16 .551
1: Q, Q .003 < 2.2 ×10−16 .644
1: M, M .990 < 2.2 ×10−16 .608
1: Q, M 2: M, Q .549 .001 .495
1: M, Q 2: Q, M .848 < 2.2 ×10−16 .513
1: Q, M 2: M, M 1.00 < 2.2 ×10−16 1.00
1: M, Q 2: Q, Q .004 < 2.2 ×10−16 .461
3
1: Q, Q 2: Q, M .936 < 2.2 ×10−16 .490
1: M, M 2: M, Q .159 < 2.2 ×10−16 .512
1: M, M 2: M, M .999 < 2.2 ×10−16 .497
1: Q, Q 2: Q, Q .011 < 2.2 ×10−16 .494
TableSI1: MeasuringthebiasofthememoryconfigurationsinTable 1. Weshowthe
p-values for the null hypothesis that the model is unbiased (P†, rejected), and that the
model’s underlying bias is more extreme that our observation (P‡, insufficient evidence
to reject).
16A
0.4
0.2
0.0
A B C D E F G H I J K L MN O P Q R S T U VWX Y Z
Convention
B
1.0
0.5
0.0
A B C D E F G H I J K L MN O P Q R S T U VWX Y Z
Convention
Figure SI1: (A) Individual and (B) Collective bias in convention selection with
W = 26, the entire Latin Alphabet. Agents favour the convention ’A’ over all others
a priori, resulting in collective consensus on this convention. Individual bias shows the
conventionproductionprobabilityfrom480samplesusingLlama3agents,whereagents
haveemptymemory. Collectivebiasshowstheproportionofconsensusconventionsfrom
20 simulations.
A B
Llama 3 70B Llama 3.1 70B
0.10 0.10
0.05 0.05
0.00 0.00
Q M X Y F J P R C D Q M X Y F J P R C D
Convention Convention
C D
Claude 3.5 Sonnet Llama 2 70b
0.10 0.10
0.05 0.05
0.00 0.00
Q M X Y F J P R C D Q M X Y F J P R C D
Convention Convention
Figure SI2: Individual bias in conventions selection with W = 10. The production
probability of each convention when agents have no prior memory, for the LLM agents
indicated in the legend. For (A-D), we generated 15,000, 10,000, 4,500, and 10,000
samples. We performed a chi-squared hypothesis test to see whether the agents are
biased, and calculated the following p-values: P= < .001, .001, 0.100, 0.410. These p-
valuesindicatethatLlama270bandClaude3.5areunbiasedacrossconventionsinthis
name pool (at the 5% significance level), whereas the Llama 3 models exhibit significant
skew.
17
saiB
evitcelloC
saiB
evitcelloC
saiB
laudividnI
saiB
evitcelloC
saiB
evitcelloC
saiB
evitcelloC6.2 Theoretical Minimal Naming Game
The Naming Game model simulates a population of N agents engaging in pairwise ne-
gotiationinteractions,demonstratingtheemergenceofglobalconsensusonconventions
through local coordination mechanisms. In the canonical formulation [8], agents must
reach consensus on the name for an object using only local interactions, similar to our
experimentalframework. Agentspossessinternallexiconswithunlimitedwordcapacity
(althoughthisisnotanecessaryinitialconditionofthemodel),initiallyempty. Theinter-
action protocol involves random selection of agent pairs, where the designated speaker
transmitsarandomlychosenwordfromtheirlexicon(orinventsanewoneifitisempty)
to the hearer. If the hearer recognises the word in their own lexicon, both agents re-
tain only the communicated word, while in case of failure, the hearer incorporates the
novelwordintotheirlexicon. Thenon-equilibriumdynamicsofthissystemexhibitthree
distinct temporal phases: (i) an innovation phase characterised by word creation, (ii) a
propagation phase involving lexicon reorganization, and (iii) a convergence phase culmi-
nating in global consensus. In our experimental framework, we set an initial condition
whereby agents can only invent new words from a finite word pool of size W < N. This
condition means that the initial innovation phase is extremely short, as seen in the in-
set of Fig. 1. This model provides insights into the dynamics of language evolution and
convention formation in both human and artificial communication systems.
Fig.SI3showstheproductionprobabilitytrajectoriesofasimulationofthetheoretical
model with a lexicon of two words (W =2).
1.0
1
0.8
0.6
0.4
0.2
0
0.0
0 5 10 15 20
Population Round
Figure SI3: Production probability trajectories in the minimal naming game with
W = 2. Agents can only choose names from the pool {0,1}. We simulated 10,000 runs
of the minimal naming game in a population of 24 agents. We show biased trajectories
(in probability increments of 0.05) towards choosing the name ’1’. The bias corresponds
to the probability of choosing the name ’1’ when an agent has the option of producing
either name, such that a bias of 1 (0) corresponds to agents that will only choose name
’1’ (’0’). We note that as the bias increases, the convergence speed increases. Crucially,
even a small bias towards a certain name leads to inevitable global convergence on that
name.
18
ytilibaborP
noitcudorP
saiB6.3 Robustness Checks
A
1.00 1.00 1.00 1.00
0.75 0.75 0.75 0.75
0.50 0.50 0.50 0.50
0.25 Q, M 0.25 F, J 0.25 X, Y 0.25 Alice, Bob
0 20 0 20 0 20 0 20
Population Round
B
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
M F X Alice
0.0 0.0 0.0 0.0
0 20 0 20 0 20 5 10
Population Round
Figure SI4: The Spontaneous emergence of conventions for W = 2. We present
individual (faint lines) and average (thick lines) trajectories of a population of N − 24
Llama3agentswithmemorylengthM =5forfourdifferentnamepools. Foreachname
pool, we show (A) the success rate, and (B) the production probability of the strong
convention (as indicated by the legend). All name pools resulted in a strong collective
bias on a particular convention ({Q, M}: ’M’, 40/40 runs; {F, J}: ’F’, 24/40 runs; {X, Y}:
’X’, 40/40 runs; {Alice, Bob}: ’Alice’, 40/40 runs.
W: 2, M: 5, N: 200 W: 6, M: 5, N: 24 W: 2, M: 4, N: 24
W: 26, M: 5, N: 24 W: 2, M: 3, N: 24 W: 2, M: 5, N: 24
1.0
0.8
0.6
0.4
0.2
0 10 20 30
Population Round
FigureSI5: RobustnessoftheSpontaneousemergenceofconventions. Weshowthat
thespontaneousemergenceofconventionsholdsforavarietyofsimulationparameters,
using populations of Llama 3 agents. W = 26 indicates a name pool which uses the
entire Latin alphabet, W = 6 is the name pool {Q, M, F, J, X, Y}, and W = 2 is the name
pool {Q, M}.
19
etaR
sseccuS
ytilibaborP
noitcudorP
etaR
sseccuSA B
Llama 3 70B Instruct Llama 3.1 70B Instruct
1.0 1.0
0.5 F J 0.5 Q M
0.0 0.0
0 20 40 0 20 40
Population Round Population Round
C D
Claude 3.5 Sonnet Llama 2 70b Chat
1.0 1.0
0.5 Q M 0.5 Q M
0.0 0.0
0 20 40 0 20 40
Population Round Population Round
Figure SI6: Stability of consensus conventions We test the stability of each model in
the setting W =2, with the possible conventions shown in the legend. For each conven-
tion,webeginwithapopulationofN =24agents(N =48forLlama3),whereeveryagent
hasonlytherespectiveconventioninmemory. Wethenallowthepopulationtonaturally
evolve, and measure the production probability of this convention. We simulate the fol-
lowing number of runs (A-D): 4, 10, 3, 5. Faint lines show the trajectories of individual
runs. WeshowthatforallmodelsapartfromLlama3.1,thepopulationremainsentirely
stable at its initial consensus state. If Llama 3.1 is initialised with consensus on the
weak convention (Q), the population immediately switches to the alternative convention
(M, the strong convention) . Here, the strong convention remains stable, with some mi-
nor fluctuations. The instability of the weak convention is also observed in our study
of the committed minority required to flip a majority consensus (see Fig. 3), where we
see that M acts as a strong attractor state that can only be overcome by a large enough
committed minority on the weak convention, Q.
6.4 Prompting
6.4.1 Prompt Structure
The system prompt comprises of three components: 1. a fixed prompt that outlines the
game’s rules, including the payoff structure and the player’s objective, 2. a dynamic
memory prompt that provides contextual information about the state of play within the
player’s memory range, and 3. an instructional prompt that provides information for
how the agent should format its response. We find that agents generally struggle to
behave in a manner befitting a partnership game, and often opted for strategies aimed
at undermining their co-player’s payoff, effectively treating them as an opponent. In
practice, this meant that on some occasions the agent would willingly take an action
with a negative payoff, in order to harm their co-player’s accumulated point tally.
6.4.2 Output structure
Toextractanymeaningfuldecisionfromalanguageagent’soutput,whichmaybeverbose
and unstructured, it is necessary to distinguish between the reason, where the agent
20
ytilibaborP
noitcudorP
ytilibaborP
noitcudorP
ytilibaborP
noitcudorP
ytilibaborP
noitcudorP’showsitsworking’,andthefinaldecision. Onepopularapproachtopromptingasksthe
agenttogiveafinaldecisionattheendofitsanswer,allowingittogenerateareasoningfor
thedecisionbeforereportingtheactualresponse. Thismethodreliesontheassumption
that if the LLM is good at composing a well thought-out reasoning, having it spelled out
explicitlywouldguidethechoiceoftheLLMtowardsbetterperformanceduetotheLLM’s
autoregressive text generation method. Although this approach has shown promising
results in a variety of tasks and multi-player games, there is an ongoing debate as to
whetherLLMstrulypossesstheabilitytoreasonaboutthepossibleactions,beliefs,and
intentions of their fellow players [60, 61, 62].
One can argue that even if agents were able to reason, the wording generated to
explain the reasoning can potentially express the agent’s biases and influence the final
decision. As a result, the reason-first, answer-later structure would make it difficult
to identify the possible sources of bias within the prompt, especially since the agent’s
reasoning will only serve to amplify its own bias reflected in the decision. To strengthen
generalisationandensurethatthefinalactiondecisionisinfluencedsolelybytheinput
prompt,weinsteadasktheagentexplicitlytoprovideanoutputintheformanswer-first-
reason-later.
6.4.3 Example Prompt
We provide an example of the system and user prompts given to LLM agents in our
experiments. We exclude the beginning of text and end of text tokens, which are unique
to each model.
System Prompt
Context: Player 1is playing a multi-roundpartnership game with Player2 for 100
rounds. At each round, Player 1 and Player 2 simultaneously pick an action from
the following values: [F, J]. The payoff that both players get is determined by the
following rule:
1. If Players play the SAME action as each other, they will both be REWARDED
with payoff 100 points.
2. If Players play DIFFERENT actions to each other, they will both be PUNISHED
with payoff -50 points.
TheobjectiveofeachPlayeristomaximizetheirownaccumulatedpointtally,con-
ditional on the behavior of the other player. This is the history of choices in past
rounds:
{’round’:1, ’Player 1’: F, ’Player 2’: J, ’payoff’: -50}
.
.
.
It is now round i. The current score of Player 1 is SCORE. Answer saying which
value Player 1 should pick. Please think step by step before making a decision.
Remember,examininghistoryexplicitlyisimportant. Writeyouranswerusingthe
following format: {’value’: <VALUE_OF_PLAYER_1>; ’reason’: <YOUR_REASON>}.
User Prompt
Answer saying which action Player 1 should play.
6.4.4 Meta-Prompting
When LLMs are used to solve tasks where some form of ground truth is defined, such
asclassificationorregression,theeffectofpromptvariationsonthequalityofamodel’s
outputs can be measured on downstream performance [63]. However, that is not possi-
ble in generative tasks where a notion of error is undefined. Specifically in the naming
game,anygeneratedoutputisplausible,aslongasitiswithinthesetofallowedsymbols.
This ambiguity makes it difficult to assess whether the LLM’s outputs reflect a proper
21semantic understanding of the task’s rules or are merely products of statistical ‘hallu-
cinations’ [40]. To partially address this issue, we rely on a meta-prompting technique
to measure the LLMs’ level of comprehension of the given prompt [41]. This technique
provides the LLM with the prompt, and then asks three types of prompt comprehension
questions about: interaction rules, chronological sequence of actions in the history, and
payoff statistics (Table SI2).
To assess the LLMs’ proficiency in responding to meta-prompting questions, we ran-
domly selected a group of agents from a real stochastic simulation of the naming game.
For each agent, we used its history to replay each of its previous interactions, using the
samememorylengthithadinthesimulation. Retracingeveryinteraction,includingthe
agent’smemoryatthetime,weasktheagentallpossiblecomprehensionquestions. Note
that certain questions that rely on memory cannot be asked in the first interaction. We
pose the questions at each interaction and compute the average accuracy of the LLMs’
responsesacrossallinteractionsforallagents. Overall,allmodelsexhibitagoodlevelof
prompt comprehension, with response accuracy nearly always above 0.8 and most often
close to 1 ( Fig. SI7). The only model that went below 0.8 in any metric is Llama 2 70b,
which showed relatively poor accuracy in counting the number of times it played a con-
ventionwithinmemoryrange. Inmanycases,thisagentconfusedtheIDoftheplayerit
was asked to consider, or it answered how many times a convention has been observed
in total, across both players. Here, it is also worth noting that LLMs from Llama 2 70b’s
generation generally struggled with counting tasks [41].
Table SI2: Templates of prompt comprehension questions used in meta-prompting to
verify the LLM’s comprehension of the prompt.
Name Question
min_max What is the lowest/highest payoff player A can get in
a single round?
actions Which actions is player A allowed to play?
payoff WhichisplayerX’spayoffinasingleroundifX plays
p and Y plays q?
round Which is the current round of the game?
action Which action did player X play in round i?
i
points How many points did player X collect in round i?
i
#actions How many times did player X choose p?
#points What is player X’s current total payoff?
Llama 3 70B Llama 3.1 70B Claude 3.5 Sonnet Llama 2 70b
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
Comprehension Questions
Figure SI7: Metaprompting results Accuracy of the model responses to the prompt
comprehension questions defined in Table SI2. We selected 8 agents from a single run
(5 agents for Llama 3 70B), and recovered their game record. We replayed the game
using the memory length used in the simulated run (M =5), posing the comprehension
questionsateachinteraction. Theserunsprovideapproximately100testinteractionsfor
each model.
22
ycaruccA
esnopseR
nim xam
seluR
emiT
etatS
snoitca ffoyap dnuor i_noitca i_stniop snoitca_# stniop_# nim xam snoitca ffoyap dnuor i_noitca i_stniop snoitca_# stniop_# nim xam snoitca ffoyap dnuor i_noitca i_stniop snoitca_# stniop_# nim xam snoitca ffoyap dnuor i_noitca i_stniop snoitca_# stniop_#6.4.5 LLM parameters
ThetablebelowshowsthetextgenerationparametersweuseforallLLMmodelsusedin
this work.
Parameter Value
Temperature 0.5
Top-K 10
Max Tokens 6
Table SI3: Model Parameters
7 Figure Data
Model Name Strong Convention Weak Convention
Llama 3 2435 2565
Llama 3.1 5079 4921
Claude 3.5 5016 4984
Llama 2 5010 4090
Table SI4: Raw individual bias Data shown for the left panel in Fig. 2A. Values indi-
cate the counts of strong and weak productions in the case W=2 for individual agents,
showing their preferences apriori, when agents are initialized with empty memory.
Model Name Strong Convention Weak Convention
Llama 3 40 0
Llama 3.1 40 0
Claude 3.5 26 14
Llama 2 36 4
TableSI5: Rawcollectivebias. DatashownfortheleftpanelinFig. 2B.Valuesindicate
the count of consensus states on the strong and weak conventions after the population
converged. For each model, we conducted 40 trial runs. All models had memory length
M =5, apart from Llama 3 70B (M=3).
Model Name Strong Convention Weak Convention
Llama 3 6 1
Llama 3.1 10 0
Claude 3.5 5 5
Llama 2 16 11
Table SI6: Raw Critical mass values. Data shown for Fig. 3B. The reported values
corresponds to the number of agents required to overturn a majority consensus on the
convention: M (strong Convention), and Q (Weak Convention). Population size (top to
bottom), N= 48, 24, 24, 24.
23