JIANYU,WEI,BOGDAN:CVAM-POSE 1
CVAM-Pose: Conditional Variational
Autoencoder for Multi-Object Monocular
Pose Estimation
JianyuZhao ComputerVisionandMachineLearning
jzhao12@uclan.ac.uk (CVML)Group,
WeiQuan TheUniversityofCentralLancashire,
Preston,UK
wquan@uclan.ac.uk
BogdanJ.Matuszewski
bmatuszewski1@uclan.ac.uk
Abstract
Estimatingrigidobjects’posesisoneofthefundamentalproblemsincomputervi-
sion,witharangeofapplicationsacrossautomationandaugmentedreality. Mostexist-
ingapproachesadoptonenetworkperobjectclassstrategy,dependheavilyonobjects’
3Dmodels,depthdata,andemployatime-consumingiterativerefinement,whichcould
be impractical for some applications. This paper presents a novel approach, CVAM-
Pose,formulti-objectmonocularposeestimationthataddressestheselimitations. The
CVAM-Posemethodemploysalabel-embeddedconditionalvariationalautoencodernet-
work, to implicitly abstract regularised representations of multiple objects in a single
low-dimensionallatentspace. Thisautoencodingprocessusesonlyimagescapturedby
a projective camera and is robust to objects’ occlusion and scene clutter. The classes
of objects are one-hot encoded and embedded throughout the network. The proposed
label-embedded pose regression strategy interprets the learnt latent space representa-
tionsutilisingcontinuousposerepresentations.Ablationtestsandsystematicevaluations
demonstrate the scalability and efficiency of the CVAM-Pose method for multi-object
scenarios. TheproposedCVAM-Poseoutperformscompetinglatentspaceapproaches.
For example, it is respectively 25% and 20% better than AAE and Multi-Path meth-
ods,whenevaluatedusingtheAR metricontheLinemod-Occludeddataset. Italso
VSD
achievesresultssomewhatcomparabletomethodsrelianton3DmodelsreportedinBOP
challenges.Codeavailable:https://github.com/JZhao12/CVAM-Pose
1 Introduction
The rapid and precise estimation of rigid objects’ poses with six degrees of freedom (6-
DoF)iscrucialforawiderangeofreal-worldapplications,includingexplorativenavigation,
augmented reality, and automated medical intervention. The introduction of deep learning
techniques[11,50]markedasignificantevolutionincomputervision,yieldingremarkable
outcomesin6-DoFposeestimation. Notably,mostdeeplearning-basedmethods[6,7,25,
26, 30, 32, 37, 38, 40, 46, 47, 48, 51] tend to train individual networks for each object to
©2024.Thecopyrightofthisdocumentresideswithitsauthors.
Itmaybedistributedunchangedfreelyinprintorelectronicforms.
4202
tcO
11
]VC.sc[
1v01090.0142:viXra2 JIANYU,WEI,BOGDAN:CVAM-POSE
obtainhigherposeaccuracy. However,theseapproachesareresource-consumingcompared
to training a unified multi-object network, as memory usage increases with the number of
objects(networks). Additionally,mostmethodstypicallyrequire3Dmodels[4,23,25,47,
48] or establish 2D-3D correspondences based on the models [18, 26, 30, 32, 33, 37, 38,
43,49],wheretheneedfor3Dmodelscanbeseenasoneofthelimitingfactorsforbroader
applications.
Figure 1: During training, the label-embedded CVAE network abstracts information from
both images of objects and the corresponding categorical labels in the latent space, which
are then interpolated to multi-object 6-DoF poses using MLPs. The images of objects are
takenfromtheLinemodPBRdataset[15,17,19,20].
In this paper, a novel multi-object pose estimation method called CVAM-Pose is pro-
posed (Fig. 1), which contains two main stages. The first stage involves training a label-
embeddedconditionalvariationalautoencoder(CVAE)networkthatincorporatestheone-hot
encoding technique to facilitate the learning of regularised and constrained representations
of multi-object poses in the latent space. Different from the original CVAE network pro-
posedin[36],theadaptedlayer-wiseone-hotencodingtechniqueencodescategoricallabels
ascomplete featuremapsacross everylayerwithin thenetwork, enhancing thelearningof
high-level representations. The second stage applies label-embedded pose regression that
avoidsthediscretisationofposes. Thisinvolvesconcatenatingthelearntmulti-objectrepre-
sentationswithone-hotencodedlabelvectors,andtrainingmultilayerperceptrons(MLPs)to
regresstheseconcatenatedfeaturesintocontinuousposerepresentations. Thecontributions
oftheCVAM-Posemethodaresummarisedasfollows:
1. The method enhances the scalability and efficiency for multi-object pose estimation
usingasingleCVAEnetwork. Tothebestofourknowledge,itisthefirsttimeacon-
ditional generative model is employed to efficiently characterise multi-object poses.
Theadaptedlabel-embeddingtechniquealsoimprovesthecapabilityoflearninghigh-
levelrepresentations.
2. Themethoddoesnotrequireobject3Dmodels,depthdata,andpost-refinementduring
inference, which can facilitate real-time processing. It achieves results comparable
to the state-of-the-art approaches on the Linemod-Occluded benchmark dataset and
outperformsthosebasedonlatentspacerepresentation.JIANYU,WEI,BOGDAN:CVAM-POSE 3
3. Themethodeffectivelyaddressesvariouschallengingscenarios,includingtexture-less
objects,occlusion,truncation[32],andclutter.
2 Related Work
DeepLearning-basedApproaches Withtherapiddevelopmentofdeeplearningtech-
niques, numerousstate-of-the-artposeestimationmethodsemployingconvolutionalneural
networks(CNNs)havebeenproposed. Thesemethodscanbecategorisedintothreedistinct
groupsbasedontheirapproachtoutilisingCNNs: direct,indirect,andlatentrepresentation
methods.
The direct methods train CNNs to regress 3D rotation and translation from images di-
rectly. which either construct loss functions using 3D model points [4, 47, 48], or itera-
tively match the image rendered from a 3D model at its estimated pose with the observed
inputimage[23,25]. Typically,thesemethodsreparameteriserotationintorepresentations
more suitable for network training, such as unit quaternion [5], axis-angle [44], or contin-
uousrepresentation[52]. Theindirectmethodsfocusonlearning2D-3Dmodelcorrespon-
dences via CNNs, with the 6-DoF poses subsequently estimated using PnP [10, 24] and
RANSAC [9]. The model correspondences can be in the form of pixel-wise dense map-
ping[12,18,26,30,37,38,49],oraselectionofsparsekeypoints[32,33,43]. Thelatent
representationmethodslearnimplicitlatentspacerepresentationsusingspecificnetworkar-
chitectures, typically autoencoders. The pose of a test instance is often retrieved using a
lookuptable(LUT)technique,whichincludesfindingnearestneighbours[39,40]andcom-
putingobservationlikelihoods[6,7].
Bothdirectandindirectmethodsexplicitlyrequireaccurate3DmodelsfortrainingCNNs
or establishing 2D-3D correspondences. The latent representation methods, despite using
onlyimagesfromsingleperspectivecamera,oftensufferfromtheposediscretisationprob-
lemduetothenatureofLUT.
Conditional Variational Autoencoder The variational autoencoder (VAE) [21, 22]
wasintroducedinthecontextofgenerativemodels,whichisdifferentfromtypicalautoen-
codermodels[16,34,35,45].TheprimaryobjectiveoftheVAEistogeneratenew,typically
highlydimensionaldatapoints,withthegenerationprocesscontrolledbyalow-dimensional
latent code randomly drawn from a prior distribution, such as Gaussian. However, a no-
tablelimitationisitsinabilitytospecifythecharacteristicsofthegenerateddata. Toaddress
thisissue,Sohnetal.[36]introducedtheconditionalvariationalautoencoder(CVAE),which
extendstheVAEframeworktoincorporateconditionalparameters,therebyenablingthegen-
erationofdatawithdesiredattributes.
Inthecontextofobject6-DoFposeestimation,Zhaoetal.[51]proposedaVAE-based
methodcalledCVML-Pose,whichwasrestrictedtosingle-objectpredictions. Similarmeth-
ods,suchas[42],havealsobeendeveloped,butusingRGB-Dimagesasinput. Weextend
theCVML-PosemethodbytrainingaCVAEmodelwithalayer-wiseone-hotencodingtech-
nique. Thisadaptationfacilitatesthelearningofmulti-objectrepresentationsinasinglela-
tentspace,significantlyimprovingscalabilityandefficiencyinthepredictionofmulti-object
poses.4 JIANYU,WEI,BOGDAN:CVAM-POSE
3 Methodology
3.1 ImplicitLearningofMulti-ObjectRepresentations
Toeffectivelylearnmulti-objectrepresentations,alabel-embeddedCVAEnetworkistrained
toencodeimagesofobjectsx andtheircorrespondingone-hotencodedcategoriesy aslabel
i i
conditionsinaregularisedlatentspace,subsequentlyoutputtingcleanreconstructionsx′.
i
AsdepictedinFig.2,anasymmetricarchitectureisproposed,consistingofanencoder
networkE andadecodernetworkD ,withlearnableparametersφ andθ respectively. The
φ θ
encoderE (x,y)processesbothx andy,wherey areembeddedascompletefeaturemaps
φ i i i i i
ineveryconvolutionblock(block-wise),untilthelatentvariablesareobtainedinthelatent
space, including µ (x,y)∈Rn and log(σ2(x,y))∈Rn, where (µ ,σ2) represent mean
φ i i φ i i φ φ
andvariancevectorsofthemultivariatenormaldistribution. Duetothenon-differentiability
of sampling from N(z;µ (x,y),diag(σ (x,y))), a reparameterisation trick [21] is em-
i φ i i φ i i
ployed. The latent sampling z ∈Rn is reparameterised as µ (x,y)+diag(σ (x,y))·ε,
i φ i i φ i i
where ε ∼N(0,I). After sampling, the decoder network D (z,y) reconstructs the com-
θ i i
pleteandcleanviewx′ frombothz andy,wherey isalsoembeddedineveryconvolution
i i i i
layer(layer-wise)inthedecoder.
Figure 2: The proposed label-embedded conditional variational autoencoder network. The
imagesofobjectsaretakenfromtheLinemodPBRdataset[15,17,19,20].
For network training, the evidence lower bound (ELBO) loss [51] is used, assuming
a Gaussian prior distribution p(z)=N(z;0,I). This loss comprises two components: i) a
pixel-wisesquaredL2normbetweentheoutputimagex′andthegroundtruthreconstruction
i
image xˆ; ii) a Kullback-Leibler (KL) divergence loss with a scalar α, which controls the
i
regularisationofthelatentspace.
(cid:32) (cid:33)
m n (cid:16) (cid:17)
ELBO≃−∑ ||xˆ −x′||2 −α·∑ 1+log(σ2)−µ2−σ2 (1)
i i ij ij ij
i=1 j=1JIANYU,WEI,BOGDAN:CVAM-POSE 5
where µ refers to the j-th element of the vector µ, σ2 refers to the j-th element of the
ij i ij
vectorσ2,µ =µ (x,y),σ2=σ2(x,y),mrepresentsthenumberoftrainingdata,andnis
i i φ i i i φ i i
thedimensionalityofthelatentspace.
After training, the label-embedded CVAE network has learnt robust representations of
objects,possiblyincludingposes. Thiscanbeevidencedbythecleanandcompleterecon-
structionsproducedfromthetrainednetworkbasedonthetestdata. AsillustratedinFig.3,
thesereconstructionsnotonlypreservecompleteviewsofobjectsbutalsodiminishirrever-
entinformationsuchasocclusionandclutteredbackground.
Figure3: Theoutputimagesfromdecodershowobjects’representationswithocclusionand
clutter removed. The test input images also shown are taken from the Linemod-Occluded
dataset[2,3].
3.2 ContinuousRegressionforMulti-ObjectPoseEstimation
The subsequent stage employs a continuous pose regression strategy [51], with an adapta-
tiontohandlethemulti-objectscenario. AsdetailedinFig.4,thisstrategyutilisesone-hot
encodedobjectlabelsy totrainMLPs,enablingsmoothinterpolationofposes.
i
For estimating 3D rotation, the rotation MLP is trained to regress from (µ,y) to the
i i
continuous6Drotationrepresentationr∈R6 [52],andtheoutputrotationR∈SO(3)isde-
rivedfromrthroughaprocesssimilartoGram-Schmidtorthogonalisation. Thiscontinuous
representationhasprovenmoreeffectivethanotherssuchasunitquaternionandaxis-angle,
andhasbeensuccessfullyimplementedin[23,46].
Figure4: Theproposedlabel-embeddedposeregressionapproachinterpolatesmulti-object
representationstocontinuousposerepresentationsusingmultipleMLPheads.
Theestimationof3DtranslationT=(cid:0) T T T(cid:1)T ∈R3isdisentangledintoestimating
x y z
the2DprojectivecentrecoordinatesP =(c ,c )T ∈R2 andtheprojectivedistanceT ∈R.
c x y z
Specifically,thelatentvector µ isconcatenatedwithspatialinformationobtainedfromthe
i
object’s bounding box, including its width w, height h, and top-left corner coordinates
i i
P =(b ,b )T, aswellthelabel y. ThededicatedMLPisthentrainedtoregressthese
bbox xi yi i
concatenatedfeaturesto(c ,c )T. TheregressionprocedureisalsoappliedtoT bytraining
x y z6 JIANYU,WEI,BOGDAN:CVAM-POSE
thedistanceMLP,butonlyutilises µ,w,h,andy. OnceT and(c ,c )T aredetermined,
i i i i z x y
T andT arecalculatedusingthepinholecameramodel(Eq.2).
x y
(cid:20) T x(cid:21) =(cid:34) (c x−p x)T fxz(cid:35)
(2)
T y (c y−p y)T fyz
where f and f denote the focal lengths, (p ,p )T is the principal point, and all these pa-
x y x y
rameterscanbeobtainedfromcameracalibration.
4 Experiments
The CVAM-Pose method is benchmarked in two aspects. The first involves conducting a
series of ablation tests to determine favourable configurations of the method. The second
wayistofollowtheevaluationmethodologiesproposedintheBOPchallenges[19,41].
4.1 ExperimentalSetup
Data AlltheexperimentsareconductedusingtheLinemod-Occludeddataset[2,3],as
itpresentsawiderangeofchallengingscenarios,suchastexture-lessobjectswithsignificant
occlusionandbackgroundclutter. Tofacilitateafaircomparisonwithmethodsparticipating
intheBOPchallenges,thesametrainingandtestdataareemployed. Thephysicallybased
rendering(PBR)images[15,17,19,20]areusedfortraining,andtheBOPversiontestset
ischosenforevaluation.
Evaluation pipeline All the results, including those from the ablation tests and the
main evaluation, are reported using the metrics specified in the BOP challenges: VSD,
MSSD, and MSPD [19]. The overall performance score, AR, is calculated based on the
averagerecallofthesethreemetrics,definedasAR=(AR +AR +AR )/3.
VSD MSSD MSPD
4.2 AblationStudy
To obtain effective configurations of the CVAM-Pose method, extensive ablation tests are
conducted using the BOP version of the Linemod-Occluded dataset [2, 3, 19, 20]. These
testsincludeevaluationsoftheadaptedlabelembeddingtechnique,theregularisationofthe
label-embeddedCVAEnetwork,andthedimensionalityofthelatentspace.
Label Embedding Technique The effectiveness of the adapted layer-wise one-hot
encoding technique is assessed in both the CVAE network and the MLPs. The original
CVAE network [36], where the label conditions only exist in the initial layer in both the
encoderanddecoder,istrainedunderthesameconditionsasourproposedlabel-embedded
network.ThetestsonMLPsinvolvedeterminingwhethertherepresentationslearntfromthe
label-embeddedCVAEnetworkcanbeeffectivelyregressedwithoutthelabels.
TheresultspresentedinTable1demonstratethattheadaptedlabel-embeddingtechnique
enhancestheabilitytolearnandregressposerepresentations.Theproposedlabel-embedded
CVAEnetworkyieldsthemostpromisingresultscomparedtotheoriginalCVAEandMLPs,
showingimprovementsof10%and13%respectivelyinAR. Thisimprovementisattributed
toitscapabilitytoabstracthigh-levelfeaturesrelatedtoobjectposewithinthelatentspace.JIANYU,WEI,BOGDAN:CVAM-POSE 7
Network originalCVAE MLPswithoutlabels Ours
AR 0.256 0.251 0.346
VSD
AR 0.255 0.243 0.362
MSSD
AR 0.630 0.554 0.714
MSPD
AR 0.380 0.349 0.475
Table1: Ablationresultsontheadaptedlabelembeddingtechnique.
In contrast, the original CVAE network primarily incorporates low-level features, which is
lesseffectiveforlearningdistinctmulti-objectrepresentationsbecause, withtheincreasing
depth of the network, the conditioned features introduced at the initial layers may not be
evidentinthelatentspace. Similarly,theMLPsbenefitfromthelabel-embeddingtechnique
that helps regress distinct poses for each object. However, it is observed that even in the
absenceoflabelconditions,thelearntrepresentationsstillretaincertaincategoricalinforma-
tion,leadingtoreasonableresults.
LatentSpaceRegularisation WhentrainingtheCVAM-Pose,theweightingfactorα
oftheKLregularisationtermplaysacrucialroleindeterminingthesmoothnessofthelatent
space. Tofindagoodbalancethatallowsforbothinformativelatentspaceandrobustgener-
alisation,theproposedCVAEnetworkistrainedwithdifferentvaluesofα ∈[0,0.1,0.5,1].
TheposeestimationresultscorrespondingtothesevaluesarereportedinTable2.
Regularisation α=0 ααα=0.1 α=0.5 α=1
AR 0.316 0.346 0.319 0.336
VSD
AR 0.340 0.362 0.352 0.353
MSSD
AR 0.712 0.714 0.686 0.685
MSPD
AR 0.456 0.475 0.452 0.458
Table2: Ablationresultsontheregularisationofthelatentspace.
Basedonthereportedresults,anα valueof0.1isidentifiedasmosteffective,providing
sufficientregularisationofthelatentspacewithoutoverlyconstrainingit. Specifically,when
α =0,theCVAEnetworklackscontrolovertheassumedpriorGaussiandistributioninthe
latent space, leading to unrestricted µ, and σ2 approaching 0, which results in suboptimal
performanceincomparisontoα=0.1.Conversely,whenαissetto0or1,thelatentspaceis
greatlyaffectedbytheKLdivergence,whichseemstooverlysmooththedistribution. This
excessive smoothing may cause a loss of critical pose-related information, as the network
focusesonminimisingKLdivergenceoverretainingdistinctivefeaturesoftheinputdata.
DimensionalityoftheLatentSpace Thedimensionalityofthelatentspace,denoted
as n, determines the capacity of the proposed CVAE network to encapsulate information
aboutobjects. Previousresearch,suchasthatbySundermeyeretal.[40], hasexploredthe
effectoflatentspacedimensionalityonposeestimation; however, theirablationtestswere
limitedton≤128andfocusedsolelyonsingle-objectscenarios.Thislimitationpromptsfur-
therinvestigationintotheperformanceimpactofn>128acrossabroaderrangeofobjects,
asasingleobjectmaynotbesufficienttoreflectthecomplexitiesofanentiredataset.
Todetermineaneffectivesizeofthelatentspaceformulti-objectposeestimation,com-
prehensiveexperimentsareconductedusingalltheLinemod-Occludedobjectswithdimen-
sionalitiessetatn∈[32,64,128,256,512,1024]. Results, asdetailedinTable3, showthat
the highest accuracy is observed at n=256. However, notably, the accuracy decreases at8 JIANYU,WEI,BOGDAN:CVAM-POSE
Dimensionality n=32 n=64 n=128 nnn=256 n=512 n=1024
AR 0.226 0.301 0.283 0.346 0.306 0.263
VSD
AR 0.224 0.318 0.303 0.362 0.317 0.284
MSSD
AR 0.528 0.654 0.681 0.714 0.706 0.695
MSPD
AR 0.326 0.424 0.422 0.475 0.443 0.414
Table3: Ablationresultsonthedimensionalityofthelatentspace.
higher dimensionalities such as 512 and 1024, suggesting that an overly large latent space
may not effectively contribute to pose encoding, and could potentially lead to diminished
performance due to the CVAE network capturing too much variability in the latent space,
overfittingtothespecifictrainingset.
4.3 MainResultsandDiscussion
MainResults ThemainresultsoftheproposedCVAM-Posemethodarereportedin
Table4and5,whereitiscomparedagainstavarietyofstate-of-the-artmethodsontheBOP
versionoftheLinemod-Occludeddataset. Thesemethodsarecategorisedbasedonthecrite-
riaoutlinedinSec.2,withtheCVAM-Posemethodclassifiedwithinthelatentrepresentation
category.Symbol"*"nexttoamethodindicatesthatitemploysonenetworkperobjectclass
strategy. Additionally,weproducetheboxplotsinFig.5,whichaccesshowthevisibilityof
objects(occlusion)inthesceneimagesinfluencestheposeestimationaccuracy.
Method Ours CVML-Pose*[51] AAE*[40] AAE-ICP*[40] Multi-Path[39]
ARVSD 0.346 0.312 0.090 0.208 0.150
ARMSSD 0.362 0.338 0.095 0.218 0.153
ARMSPD 0.714 0.706 0.254 0.285 0.346
AR 0.475 0.452 0.146 0.237 0.217
Table4: Comparisonwithlatentrepresentationmethods.
Method Ours DPOD[49] Pix2Pose*[30] EPOS[18] CDPN*[26] PVNet*[32] CosyPose[23] SurfEmb[12] ZebraPose*[38] GDR-Net*[46]
ARVSD 0.346 0.101 0.233 0.389 0.393 0.428 0.480 0.497 0.547 0.549
ARMSSD 0.362 0.126 0.307 0.501 0.537 0.543 0.606 0.640 0.714 0.701
ARMSPD 0.714 0.278 0.550 0.750 0.779 0.754 0.812 0.851 0.860 0.887
AR 0.475 0.169 0.363 0.547 0.569 0.575 0.633 0.663 0.707 0.713
Table5: Comparisonwithdirectandindirectmethodsthatarerelianton3Dmodels.
Within the latent representation category, CVAM-Pose significantly outperforms meth-
ods such as AAE, AAE-ICP (AAE incorporates ICP refinement [1]), and Multi-Path (a
multi-object AAE approach). For example, it is respectively 25%, 14%, and 20% better
whenevaluatedusingtheAR metric. Theoverallperformancemarginsacrossthethree
VSD
metrics are substantial, with improvements of 33%, 24%, and 26% in AR respectively. In
comparisonwithmethodsfromothercategories, CVAM-Posealsosurpassessomeindirect
methodslikeDPODandPix2Pose,bymarginsof31%and11%,respectively. Additionally,
itachievesresultscomparabletoEPOS,CDPN,andPVNetinspecificmetrics,e.g. AR
VSD
andAR .
MSPD
Discussion The results on the challenging Linemod-Occluded dataset demonstrate
thatcompetitiveperformanceformulti-objectposeestimationcanbeachievedusingacon-
ditionalgenerativemodel. Theproposedlabel-embeddedCVAEnetworkwiththecontinu-
ousposeregressionapproachismoreeffectiveandaccuratethanotherlatentrepresentationJIANYU,WEI,BOGDAN:CVAM-POSE 9
Figure 5: Box plots of the MSPD metric as a function of the objects’ visibility rates. The
numberofdatainstancesforeachrateisshownaboveeachbox. Pleasenotethatforbetter
visualisation, the MSPD metric is calculated using thresholds ranging from 1 to 50 with a
stepof1,insteadofusingthethresholds(from5to50withastepof5)definedintheBOP
challenges.
methods.Comparedtothesingle-objectmethods,suchasCVML-Pose,AAE,andAAE-ICP,
theproposedlabel-embeddedCVAEnetworkcapturesregularisedandrobustrepresentations
for multiple objects. Unlike CVML-Pose, the adapted label-embedding technique avoids
trainingmultipleVAEnetworks,therebyenhancingtrainingefficiencywithoutdiminishing
the performance. Different from the Multi-Path method, which employs multiple decoder
networksdemandingsignificantGPUresources,oursingle-encoder-single-decoderarchitec-
tureachieveshigherposeaccuracy. Furthermore,thecontinuousposeregressionapproach,
asopposedtotheLUTtechniqueusedinAAE,AAE-ICP,andMulti-Path,effectivelyavoids
errorsassociatedwithposediscretisationduringinference.Ourcontinuousregressiononthe
2Dprojectivecentreanddistancealsomitigatestheeffectscausedbyincorrectdetectionof
occludedobjects.
PerformanceagainstocclusionisillustratedinFig.5,whichshowsboxplotsquantifying
the distribution of the MSPD metric (AR ) across different visibility rates from 10%
MSPD
to 100%. It is evident that as visibility increases, the median of pose estimation accuracy
improves and eventually achieves a value of 0.78. Even under heavy (20%-30% visibil-
ity) or mild (50%-60% visibility) occlusions, our method still achieves reasonable results,
indicatingitsrobustnessagainstchallengingocclusionscenarios.
Comparedtodirectandindirectmethodsthatrelyon3Dmodels,theproposedCVAM-
Pose method achieves higher results than some of them on the challenging occlusion data.
Thispossiblysuggeststhatapproachesbasedonpixel-wisemodelcorrespondences,suchas
DPODandPix2Pose,sufferperformancedegradationduetoaninsufficientnumberofpoints
available for assessing correspondences in heavily occluded scenes. In contrast, our pro-
posedmethodbenefitsfromreconstructingcompleteobjectsfrompartiallyobscuredviews,
therebyrobustlyhandlingocclusions.
Fortheresultsreportedinthepaper,theproposedCVAM-Posemethodistrainedwith8
differentobjects. Experimentswithlargernumbersofobjectswerealsoconductedbutnot10 JIANYU,WEI,BOGDAN:CVAM-POSE
reported. ItwasobservedthatincreasingthenumberofobjectsinCVAM-Posebeyond15
would lead to a decrease in pose estimation accuracy, without adjusting design parameters
suchasthesizeofthelatentspace.
AlthoughtheproposedCVAM-Posemethodavoidstrainingonenetworkperobjectcate-
gory,whichenhancesscalabilityandefficiency,itshowsacertaingapinposeaccuracycom-
paredtoleadingmethodslikeCosyPose,SurfEmb,andGDR-Net. Thesemethodsimprove
theiraccuracythroughtechniquessuchasiterativerefinementusing3Dmodelpoints(Cosy-
Pose),estimatingcontinuousmodelcorrespondencedistributions(SurfEmb),andcombining
poseregressionwithdensecorrespondences(GDR-Net). However, theyallrequireprecise
3Dmodelsforsettingup2D-3Dcorrespondencesormodelpoint-basedtraining.Incontrast,
the advantages of our method lie in addressing the 6-DoF pose estimation problem with-
outrelyingon3Dmodels,depthmeasurements,andpost-refinementprocesses,providinga
novelsolutioninscenarioswheresuchdataareunavailable.
5 Conclusion
This paper addresses one of the key challenges in computer vision: finding multi-object
6-DoF poses from images captured by a perspective camera in real time (with fixed infer-
ence processing time of 0.02s with CVAM-Pose run on RTX3090). The proposed method
demonstratesthatcompetitiveperformancecanbeachievedusingonlyasingleperspective
image,withoutrelianceon3Dmodels,depthmeasurements,oriterativepost-refinement. In
particular,thescalabilityofasinglelatentspacecanbeexpandedtomulti-objectrepresenta-
tionswithoutcompromisingposeaccuracy. Themaincontributionsofthereportedresearch
include the proposed use of a conditional generative model, the adapted label-embedding
technique,theconstructionofaregularisedandconstrainedlatentspaceformultipleobjects,
andthecontinuousposeregressionalgorithms,whichfacilitatefastandaccuratemulti-object
poseestimation.
6 Acknowledgement
Data access statement: The study reported in this paper has been supported by two exist-
ing openly available datasets, namely Linemod and Linemod-Occluded. Both datasets are
availablefromhttps://bop.felk.cvut.cz/datasets/.JIANYU,WEI,BOGDAN:CVAM-POSE 11
Supplementary Materials
Weprovideadditionalsupplementarymaterialsincluding:
1. FurtherquantitativeandqualitativeanalysesofourmethodontheBOPversionofthe
Linemod-Occludeddataset[2,3,19,20].
2. Moreinformationonnetworkimplementations.
7 Additional Results on Linemod-Occluded
7.1 QuantitativeResults
Pose Regression vs. LUT We conduct furtherablation tests comparing thepose re-
gressionstrategyusedinourmethodtothelookuptable(LUT)techniquedescribedin[39,
40]. TheLUTtechniqueassignstherotationandprojectivedistancefromthemostsimilar
instancetothetestinstance,andutilisesthecentreoftheboundingboxasthe2Dprojective
centre. Thisapproachmayleadtoinaccuracies, particularlywithheavilyoccludedobjects
orimpreciseboundingboxes. Inouranalysis,theresultsfor3Drotationarereportedusing
the AR metric [19], while results for projective centre and distance are evaluated us-
MSPD
ingthemeanabsoluteerror(MAE)metric. ThechoiceofMAEoverAR isduetoits
MSPD
parameter-freenature,whichsimplifiestheinterpretationoftranslationalerrors,asopposed
toAR thatdependsonpredefinedthresholdsasoutlinedin[19].
MSPD
Rotation AR ↑ Centre MAE ↓ Distance MAE ↓
MSPD pixel mm
LUT 0.666 LUT 4.064 LUT 60.981
Ours 0.714 Ours 2.913 Ours 43.278
Table 6: Comparison between LUT and our regression method for the estimation of 3D
rotation,2Dprojectivecentre,and2Dprojectivedistance.
AsshowninTable6,ourcontinuousposeregressionstrategydemonstratesbetterresults
thanusingtheLUTtechniqueinestimating3Drotation,2Dprojectivecentre,and2Dpro-
jectivedistance,e.g. ourmethodachievessmallererrorsindistancemeasurement(improved
byapproximately2%whencomputedinrelationtotheaverageobject’sdistanceinthetest
set). This can be attributed to the avoidance of the pose discretisation problem inherent in
theLUTtechnique, particularlywhenthetrainingdatadonotcovertheentireSO(3). The
performance of centre prediction is further illustrated in Fig. 6, which presents box plots
quantifying the distribution of errors (MAE ). It is evident that the median error in our
pixel
methodisconsistentlylowerthanthatproducedbytheLUTtechniqueacrossvariousvisi-
bilityrates. TheLUTmethodcanalsogeneratenoticeableoutliererrorsincentreprediction,
ashighas27pixels.
Results on Individual Objects We also present additional results on individual ob-
jects from the Linemod-Occluded dataset [2, 3] in Table 7. The average recall of a sin-
gle object, AR , is calculated from the average recall across the three metrics, AR ,
object VSD
AR ,andAR [19]. Theaveragevalue,denotedasAvg.,showsthemainresultsfor
MSSD MSPD
theentiredatasetasalreadyreportedinthepaper.
Amongthethreeevaluationmetrics,theMSPDmetricdemonstratesconsiderablyhigher
accuracythantheothers(25%higheronaverage). Asexplainedin[19], thismightbethat12 JIANYU,WEI,BOGDAN:CVAM-POSE
Figure6: BoxplotsoftheMAE metricasafunctionoftheobjects’visibilityrates. The
pixel
numberofdatainstancesforeachrateisshownaboveeachpairofboxes.
Object ape can cat driller duck eggbox glue holepuncher Avg.
AR 0.332 0.409 0.300 0.375 0.443 0.168 0.324 0.425 0.346
VSD
AR 0.360 0.471 0.286 0.490 0.397 0.084 0.356 0.455 0.362
MSSD
AR 0.830 0.681 0.826 0.571 0.794 0.488 0.760 0.764 0.714
MSPD
AR 0.507 0.520 0.471 0.479 0.545 0.247 0.480 0.548 0.475
object
Table7: ResultsontheindividualobjectsoftheLinemod-Occludeddataset.
theMSPDmetricdoesnotaccountforalignmentalongtheopticalaxis,whichissignificant
whenevaluatingonperspectiveimages.
In terms of individual objects, the eggbox object exhibits lower accuracy than others
(approximately20%inAR ),whichmightbeassociatedwithobjectsymmetries,i.e. the
object
poseambiguityproblem. Toimproveposeaccuracy,especiallyforsymmetricalobjects,our
method could be extended to estimate the distribution of potential poses through random
sampling in the latent space, thereby better accommodating variances induced by object
symmetries.
7.2 QualitativeResults
Fig.7visualisesposeestimationresultsontworandomlyselectedimagesfromtheLinemod-
Occludeddataset,withposesestimatedusingCVAM-Pose.Thetargetobjects,includingape,
cat,driller,duck,eggbox,glue,holepuncher,andiron,arerenderedbasedontheestimated
posesandreprojectedontotheoriginaltestimages. Correctestimationsarerepresentedby
aligned reprojection masks, e.g. the cat object in the first image, while misaligned masks
indicateincorrectestimations,e.g. theeggboxobjectinthefirstimage.JIANYU,WEI,BOGDAN:CVAM-POSE 13
Figure 7: Example visualisation of the estimated poses using CVAM-Pose. The rendering
process uses the Pyrender software [28]. The images of objects are taken from the BOP
versionoftheLinemod-Occludeddataset[2,3,19,20].14 JIANYU,WEI,BOGDAN:CVAM-POSE
8 Implementation Details
Network Architecture The proposed label-embedded CVAE network employs an
adapted ResNet-18 [13] as the encoder, and a sequence of convolutional layers as the de-
coder. The ReLU activation function [29] is replaced with SiLU [8] to avoid the zero-
gradientproblem. Thelabel-embeddedMLPnetworkconsistsofaseriesoffullyconnected
layers with neurons [256,128,64,32,16,out]. Each hidden layer uses the SiLU activation
and concatenates the one-hot encoded categorical labels with the output of the previous
layer. The final output, out, varies depending on the regression task, such as 6 neurons
forregressingthecontinuous6Drotationrepresentation[52].
Data Preprocessing The data preparation involves a crop-and-resize strategy pro-
posed in [51]. This strategy crops images of the target objects into a square shape from
thesceneimageusingthegroundtruthboundingbox,withthesquare’ssizedefinedbythe
longer side of the box. The cropped images of objects are resized to 128×128×3 using
bicubicinterpolation,whichmatchestheinputsizeoftheproposedCVAEnetwork. Images,
where less than 10% of the object’s area is visible, are excluded, based on the visibility
criteria defined in [19, 41]. Approximately 40k images per object are obtained, with 90%
designatedfortrainingandtheremaining10%forvalidation. Fortestdatapreparation,the
crop-and-resize strategy is also applied, using the detection bounding boxes provided by a
pre-trainedMask-RCNNdetector[14,23].
TrainingParameters AllexperimentsareimplementedinPyTorch[31]. Thelabel-
embeddedCVAEandMLPnetworksaretrainedusingtheAdamWoptimiser[27]withpa-
rameterssetasfollows: β =0.9,β =0.999,ε=1e−8,andλ =0.01. Theinitiallearning
1 2
rateissetto1e−4forCVAEand3e−3forMLPs,withscheduledreductionsbyafactorof
0.2whenthevalidationlossdoesnotimproveovera“patience”period(50epochsforCVAE,
500forMLPs). Trainingterminateswhenthelowestlearningrateof1e−6isreached,and
no improvement in validation loss occurs for N epochs (N =50 for CVAE and N =1000
forMLPs). TheCVAEnetworkistrainedwithabatchsizeof128,whileMLPsprocessall
inputsperbatch. Forreproducibility,allrandomseedsarefixedat0.JIANYU,WEI,BOGDAN:CVAM-POSE 15
References
[1] PJBeslandNeilDMcKay.Amethodforregistrationof3-dshapes.IEEETransactions
onPatternAnalysis&MachineIntelligence,14(02):239–256,1992.
[2] EricBrachmann. 6dobjectposeestimationusing3dobjectcoordinates[data], 2020.
URLhttps://doi.org/10.11588/data/V4MUMX.
[3] Eric Brachmann, Alexander Krull, Frank Michel, Stefan Gumhold, Jamie Shotton,
and Carsten Rother. Learning 6d object pose estimation using 3d object coordinates.
In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland,
September6-12,2014,Proceedings,PartII13,pages536–551.Springer,2014.
[4] Yannick Bukschat and Marcus Vetter. Efficientpose: An efficient, accurate and
scalable end-to-end 6d multi object pose estimation approach. arXiv preprint
arXiv:2011.04307,2020.
[5] ErikBDam,MartinKoch,andMartinLillholm. Quaternions,interpolationandani-
mation,volume2. Citeseer,1998.
[6] XinkeDeng,YuXiang,ArsalanMousavian,ClemensEppner,TimothyBretl,andDi-
eter Fox. Self-supervised 6d object pose estimation for robot manipulation. In 2020
IEEEInternationalConferenceonRoboticsandAutomation(ICRA),pages3665–3671.
IEEE,2020.
[7] Xinke Deng, Arsalan Mousavian, Yu Xiang, Fei Xia, Timothy Bretl, and Dieter Fox.
Poserbpf: Arao–blackwellizedparticlefilterfor6-dobjectposetracking. IEEETrans-
actionsonRobotics,37(5):1328–1342,2021.
[8] StefanElfwing,EijiUchibe,andKenjiDoya. Sigmoid-weightedlinearunitsforneural
network function approximation in reinforcement learning. Neural networks, 107:3–
11,2018.
[9] Martin A Fischler and Robert C Bolles. Random sample consensus: a paradigm for
modelfittingwithapplicationstoimageanalysisandautomatedcartography. Commu-
nicationsoftheACM,24(6):381–395,1981.
[10] Xiao-Shan Gao, Xiao-Rong Hou, Jianliang Tang, and Hang-Fei Cheng. Complete
solution classification for the perspective-three-point problem. IEEE transactions on
patternanalysisandmachineintelligence,25(8):930–943,2003.
[11] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press,
2016. http://www.deeplearningbook.org.
[12] Rasmus Laurvig Haugaard and Anders Glent Buch. Surfemb: Dense and continu-
ouscorrespondencedistributionsforobjectposeestimationwithlearntsurfaceembed-
dings. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pages6749–6758,2022.
[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning
forimagerecognition. InProceedingsoftheIEEEconferenceoncomputervisionand
patternrecognition,pages770–778,2016.16 JIANYU,WEI,BOGDAN:CVAM-POSE
[14] KaimingHe,GeorgiaGkioxari,PiotrDollár,andRossGirshick. Maskr-cnn. InPro-
ceedingsoftheIEEEinternationalconferenceoncomputervision,pages2961–2969,
2017.
[15] StefanHinterstoisser,VincentLepetit,SlobodanIlic,StefanHolzer,GaryBradski,Kurt
Konolige, and Nassir Navab. Model based training, detection and pose estimation of
texture-less 3d objects in heavily cluttered scenes. In Computer Vision–ACCV 2012:
11th Asian Conference on Computer Vision, Daejeon, Korea, November 5-9, 2012,
RevisedSelectedPapers,PartI11,pages548–562.Springer,2013.
[16] GeoffreyEHintonandRuslanRSalakhutdinov. Reducingthedimensionalityofdata
withneuralnetworks. science,313(5786):504–507,2006.
[17] TomášHodanˇ,VibhavVineet,RanGal,EmanuelShalev,JonHanzelka,TrebConnell,
PedroUrbina,SudiptaNSinha,andBrianGuenter. Photorealisticimagesynthesisfor
objectinstancedetection. In2019IEEEinternationalconferenceonimageprocessing
(ICIP),pages66–70.IEEE,2019.
[18] Tomas Hodan, Daniel Barath, and Jiri Matas. Epos: Estimating 6d pose of objects
withsymmetries. InProceedingsoftheIEEE/CVFconferenceoncomputervisionand
patternrecognition,pages11703–11712,2020.
[19] Tomas Hodan, Martin Sundermeyer, Bertram Drost, Yann Labbe, Eric Brachmann,
Frank Michel, Carsten Rother, and Jiri Matas. Bop challenge 2020 on 6d object lo-
calization. InComputerVision–ECCV2020Workshops: Glasgow,UK,August23–28,
2020,Proceedings,PartII16,pages577–594.Springer,2020.
[20] Tomas Hodan, Martin Sundermeyer, Bertram Drost, Yann Labbe, Eric Brachmann,
FrankMichel,CarstenRother,andJiriMatas. Datasets.,2020. URLhttps://bop.
felk.cvut.cz/datasets/.
[21] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Interna-
tionalConferenceonLearningRepresentations,2014.
[22] DiederikPKingma, MaxWelling, etal. Anintroductiontovariationalautoencoders.
FoundationsandTrends®inMachineLearning,12(4):307–392,2019.
[23] YannLabbé,JustinCarpentier,MathieuAubry,andJosefSivic. Cosypose: Consistent
multi-view multi-object 6d pose estimation. In Computer Vision–ECCV 2020: 16th
EuropeanConference,Glasgow,UK,August23–28,2020,Proceedings,PartXVII16,
pages574–591.Springer,2020.
[24] Vincent Lepetit, Francesc Moreno-Noguer, and Pascal Fua. Epnp: An accurate o (n)
solutiontothepnpproblem. Internationaljournalofcomputervision,81(2):155–166,
2009.
[25] Yi Li, Gu Wang, Xiangyang Ji, Yu Xiang, and Dieter Fox. Deepim: Deep iterative
matchingfor6dposeestimation. InternationalJournalofComputerVision,128:657–
678,2020.
[26] ZhigangLi,GuWang,andXiangyangJi. Cdpn: Coordinates-baseddisentangledpose
network for real-time rgb-based 6-dof object pose estimation. In Proceedings of the
IEEE/CVFInternationalConferenceonComputerVision,pages7678–7687,2019.JIANYU,WEI,BOGDAN:CVAM-POSE 17
[27] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In
International Conference on Learning Representations, 2019. URL https://
openreview.net/forum?id=Bkg6RiCqY7.
[28] MatthewMatl. Pyrender:Easy-to-usegltf2.0-compliantopenglrendererforvisualiza-
tionof3dscenes.,2018. URLhttps://github.com/mmatl/pyrender.
[29] VinodNairandGeoffreyEHinton. Rectifiedlinearunitsimproverestrictedboltzmann
machines. In Proceedings of the 27th international conference on machine learning
(ICML-10),pages807–814,2010.
[30] Kiru Park, Timothy Patten, and Markus Vincze. Pix2pose: Pixel-wise coordinate re-
gressionofobjectsfor6dposeestimation. InProceedingsoftheIEEE/CVFInterna-
tionalConferenceonComputerVision,pages7668–7677,2019.
[31] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal. Pytorch:
Animperativestyle,high-performancedeeplearninglibrary. Advancesinneuralinfor-
mationprocessingsystems,32,2019.
[32] Sida Peng, Xiaowei Zhou, Yuan Liu, Haotong Lin, Qixing Huang, and Hujun Bao.
Pvnet: Pixel-wisevotingnetworkfor6dofobjectposeestimation. IEEETransactions
onPatternAnalysisandMachineIntelligence,44(6):3212–3223,2020.
[33] MahdiRadandVincentLepetit. Bb8: Ascalable,accurate,robusttopartialocclusion
method for predicting the 3d poses of challenging objects without using depth. In
Proceedings of the IEEE international conference on computer vision, pages 3828–
3836,2017.
[34] Marc’Aurelio Ranzato, Y-Lan Boureau, Yann Cun, et al. Sparse feature learning for
deepbeliefnetworks. Advancesinneuralinformationprocessingsystems,20,2007.
[35] SalahRifai, PascalVincent, XavierMuller, XavierGlorot, andYoshuaBengio. Con-
tractive auto-encoders: Explicit invariance during feature extraction. In Proceedings
ofthe28thinternationalconferenceoninternationalconferenceonmachinelearning,
pages833–840,2011.
[36] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output represen-
tationusingdeepconditionalgenerativemodels. Advancesinneuralinformationpro-
cessingsystems,28,2015.
[37] Chen Song, Jiaru Song, and Qixing Huang. Hybridpose: 6d object pose estimation
underhybridrepresentations.InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,pages431–440,2020.
[38] Yongzhi Su, Mahdi Saleh, Torben Fetzer, Jason Rambach, Nassir Navab, Benjamin
Busam, Didier Stricker, and Federico Tombari. Zebrapose: Coarse to fine surface
encodingfor6dofobjectposeestimation.InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages6738–6748,2022.18 JIANYU,WEI,BOGDAN:CVAM-POSE
[39] Martin Sundermeyer, Maximilian Durner, En Yen Puang, Zoltan-Csaba Marton,
NarunasVaskevicius, KaiOArras, andRudolphTriebel. Multi-pathlearningforob-
jectposeestimationacrossdomains. InProceedingsoftheIEEE/CVFconferenceon
computervisionandpatternrecognition,pages13916–13925,2020.
[40] MartinSundermeyer,Zoltan-CsabaMarton,MaximilianDurner,andRudolphTriebel.
Augmentedautoencoders: Implicit3dorientationlearningfor6dobjectdetection. In-
ternationalJournalofComputerVision,128:714–729,2020.
[41] MartinSundermeyer,TomášHodanˇ,YannLabbe,GuWang,EricBrachmann,Bertram
Drost, Carsten Rother, and Jiˇrí Matas. Bop challenge 2022 on detection, segmenta-
tion and pose estimation of specific rigid objects. In Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages2784–2793,2023.
[42] Hiroki Tatemichi, Yasutomo Kawanishi, Daisuke Deguchi, Ichiro Ide, and Hiroshi
Murase. Category-level object pose estimation in heavily cluttered scenes by gener-
alizedtwo-stageshapereconstructor. IEEEAccess,2024.
[43] Bugra Tekin, Sudipta N Sinha, and Pascal Fua. Real-time seamless single shot 6d
objectposeprediction. InProceedingsoftheIEEEconferenceoncomputervisionand
patternrecognition,pages292–301,2018.
[44] CarloTomasi. Vectorrepresentationofrotations. ComputerScience,527:2–4,2013.
[45] PascalVincent,HugoLarochelle,YoshuaBengio,andPierre-AntoineManzagol. Ex-
tracting and composing robust features with denoising autoencoders. In Proceedings
ofthe25thinternationalconferenceonMachinelearning,pages1096–1103,2008.
[46] GuWang,FabianManhardt,FedericoTombari,andXiangyangJi.Gdr-net:Geometry-
guideddirectregressionnetworkformonocular6dobjectposeestimation. InProceed-
ingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
16611–16621,2021.
[47] JimmyWu,BoleiZhou,RebeccaRussell,VincentKee,SylerWagner,MitchellHebert,
AntonioTorralba,andDavidMSJohnson. Real-timeobjectposeestimationwithpose
interpreternetworks.In2018IEEE/RSJInternationalConferenceonIntelligentRobots
andSystems(IROS),pages6798–6805.IEEE,2018.
[48] Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox. Posecnn: A
convolutional neural network for 6d object pose estimation in cluttered scenes. In
Robotics: ScienceandSystems(RSS),2018.
[49] SergeyZakharov,IvanShugurov,andSlobodanIlic.Dpod:6dposeobjectdetectorand
refiner. InProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,
pages1941–1950,2019.
[50] Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. Dive into Deep
Learning. CambridgeUniversityPress,2023. https://D2L.ai.
[51] Jianyu Zhao, Edward Sanderson, and Bogdan J Matuszewski. Cvml-pose: Convolu-
tionalvaebasedmulti-levelnetworkforobject3dposeestimation. IEEEAccess, 11:
13830–13845,2023.JIANYU,WEI,BOGDAN:CVAM-POSE 19
[52] YiZhou,ConnellyBarnes,JingwanLu,JimeiYang,andHaoLi. Onthecontinuityof
rotationrepresentationsinneuralnetworks. InProceedingsoftheIEEE/CVFConfer-
enceonComputerVisionandPatternRecognition,pages5745–5753,2019.