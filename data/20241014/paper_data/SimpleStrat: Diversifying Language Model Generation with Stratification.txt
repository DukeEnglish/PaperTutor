SIMPLESTRAT: DIVERSIFYING LANGUAGE MODEL
GENERATION WITH STRATIFICATION
JustinWong YuryOrlovskiy MichaelLuo SanjitA.Seshia JosephE.Gonzalez
UCBerkeley UCBerkeley UCBerkeley UCBerkeley UCBerkeley
Low Temp Sampling High Temp Sampling SimpleStrat Sampling
(E/W) of Mississippi River
(N/S) of Missouri
Compromise
Line
California California California
New York New York New York
Washington Washington Texas
Virginia Virginia Georgia
Figure1: StratfiedSamplingvsTemperatureScalingConsidertheLLMuserrequest"NameaUS
State."SimpleStratemploysauto-stratificationtoutilizetheLLMtoidentifygooddimensionsofdiversity,
forinstance"East/WestoftheMississippiRiver."Then,SimpleStratusesstratifiedsamplingtodiversify
LLMgenerations.
ABSTRACT
Generatingdiverseresponsesfromlargelanguagemodels(LLMs)iscrucialfor
applicationssuchasplanning/searchandsyntheticdatageneration,wherediversity
providesdistinctanswersacrossgenerations. Priorapproachesrelyonincreasing
temperaturetoincreasediversity. However,contrarytopopularbelief,weshownot
onlydoesthisapproachproducelowerqualityindividualgenerationsastempera-
tureincreases,butitdependsonmodel’snext-tokenprobabilitiesbeingsimilarto
thetruedistributionofanswers. WeproposeSimpleStrat,analternativeapproach
thatusesthelanguagemodelitselftopartitionthespaceintostrata. Atinference,a
randomstratumisselectedandasampledrawnfromwithinthestrata. Tomeasure
diversity,weintroduceCoverageQA,adatasetofunderspecifiedquestionswith
multipleequallyplausibleanswers,andassessdiversitybymeasuringKLDiver-
gencebetweentheoutputdistributionanduniformdistributionovervalidground
truth answers. As computing probability per response/solution for proprietary
modelsisinfeasible,wemeasurerecallongroundtruthsolutions. Ourevaluation
showusingSimpleStratachieveshigherrecallby0.05comparedtoGPT-4oand
0.36averagereductioninKLDivergencecomparedtoLlama3.
1 INTRODUCTION.
Large language models (LLMs) are routinely resampled in order to get a wide set of plausible
generations. Threekeysettingswherethisisimportantare: 1)improvingdownstreamaccuracywith
planningorsearchforagentictasks(i.e. Tree-of-thought(Yaoetal.,2024),AgentQ(Puttaetal.,
2024)), 2) estimating prediction uncertainty (Aichberger et al., 2024), and 3) generating diverse
datasetsforpost-training(Dubeyetal.,2024)andfine-tuning(Daietal.,2023). Alltheseusecases
rely on the model generating multiple plausible generations for the same prompt when multiple
answersexists.
1
4202
tcO
11
]LC.sc[
1v83090.0142:viXraAuto-Stratification Heuristic Estimation Probabilistic Prompting
(E/W) of Mississippi River West East Prompt Distribution
Nam Ste
a
ta
e
US 0.34 0.34 N wN wa hN wa hN wm ea hm era hm ee erm ee er e ea
r
e ea aU aU S U S U SS SS t S at Satt ae tt aet, e t, e , ,
(N/S) of Missouri 0.18 0.14 ❖ ❖❖ ❖❖ ❖E S❖ ❖E S E So o E So o f fo o f fM Mo o f fM M f fM Mi i s sM Mi is ss si is ss si ii os ss s si ous s ss i ouss ri oi usp is ri up is rCip p irCip i p io Cp i oR Cp mi oR mii oR mv pi R mv pe i.v pe i.r v pe .r e .rr
Compromise Line
Figure 2: SimpleStrat workflow. SimpleStrat employs 3 phases: 1) auto-stratification to identify good
dimensionsofdiversitythatdividethesolutionspaceintoequalpartitions,2)heuristicestimationtoestimatethe
proportionofsolutionsineachstratum,and3)probabilisticpromptingwhereaconcretepromptisrandomly
sampledfromthepromptdistributionspecifiedbytheprevioustwophases.Critically,diverseresamplingcomes
fromboththerandomchoiceofpromptaswellasthetemperatureoftheLLMdecoding.
Naively, increasing temperature, a parameter that controllably flattens an LLM’s softmax, can
improveanLLM’sgenerationdiversity. However,temperatureintroducestwoproblems. First,higher
temperaturesdegradesgenerationquality. Recentevidencesuggestsremovingtemperaturescaling
is desirable for multi-step reasoning to reduce errors compounding (Zhang et al., 2024). This is
especiallycriticalinsyntaxsensitivesettingslikecodegenerationwherelowtemperatures(≤0.15)
areoftenused. Second,controllingfortemperaturedoesnotnecessarilyimprovediversityinthe
answerspace. InFigure1,weillustrateincreasingtemperaturedoesn’tleadtomeaningfulincrease
indiversityifthemodelisexcessivelyconfidentandsuffersfrommodecollapse. Whenaskedto
"NameaUSState,"themodelheavilyskewstowardsanswering"California",hightemperatureonly
marginallysoftenstheskewwhilesurfacingincorrectanswersandhurtinginstructionfollowing.
Our goal is to improve diversity when resampling LLMs, even in cases of severe mode collapse
innext-tokenprobabilitieswithoutmanualintervention. OuranalysisrevealsthatGPT-4assigns
87% of its logit weight to "California" when prompted to name a US state. This observed bias
canbeattributedtotheworseningofcalibrationduetopost-trainingasreportedintheGPT-4tech
report (OpenAI et al., 2024). This stark bias mirrors human cognitive bias, exemplified by the
blue-sevenphenomenon—whereindividualsdisproportionatelyselectblueandsevenwhenasked
tochoosearandomcolorandnumber. Tocounteractsimilarbiasesinhumanpopulations,social
scientists,particularlyinpoliticalpolling,employstratifiedsamplingtechniques(Simpson,1951;
Howell,1992;Morris,2022). WeproposeadaptingthismethodtoaddressmodecollapseinLLMs.
WeproposeSimpleStrat,atraining-freesamplingapproachtoincreasediversity.SimpleStratimproves
LLMgenerationdiversitywithoutdegradationtogenerationqualitywhileensuringthatanLLM’s
outputsarealignedwiththetruedistributionofanswers. SimpleStratconsistofthreestages: auto-
stratification,heuristicestimation,andprobabilisticprompting. Evenifalanguagemodelcannot
generatediversesolutions,wefindthatitcanbepromptedtoidentifyusefulpartitionsofthesolution
space based on the user request. We call this process auto-stratification. In Fig. 1, SimpleStrat
identifiestwosemanticallysignificantstratafromuserrequest,"NameaUSState": "(East/West)of
theMississippiRiver"and"(North/South)oftheMissouriCompromiseLine."
Next, the heuristic estimation computes the joint probabilities across all strata. Back to Fig. 1,
SimpleStrat then outputs the probability for all four possible regions in US. Finally, SimpleStrat
samplesfromthejointprobabilitydistributiontoaugmenttheoriginaluserpromptwiththeselected
stratas. Wenotethatthisapproachtodiversityisorthogonaltoincreasingtemperatureandhence
doesnotaffectgenerationquality.
WeevaluateSimpleStratonunderspecifiedquestions,specificallyquestionsthathavemorethanone
plausibleanswer. However,unlikeambiguousquestionsmorewidely,ananswertoanunderspecified
question can be easily verified to be a valid without additional context. These questions capture
settingswheretheuserisindifferenttotheparticularansweraslongasit’svalidorinsettingswhere
wewishtoresampletogetasetofcandidatessolutions. WeintroduceCoverageQA,abenchmarkof
underspecifiedquestionswithonaverage28.7equallyplausibleanswers.
WemeasurediversitybycomputingtheKullback-Leibler(KL)Divergencefromtheresponsedistri-
butiontoauniformdistributionoverallvalidanswers. Bycomputingtheresponsedistributionusing
next-tokenprobabilities,weshowSimpleStratsamplesfromalessbiaseddistribution.Forproprietary
2
LLM LLM htroN
htuoS
Sampler LLMmodels where we cannot close form express the response distribution, we measure the model’s
coverageviarecallofground-truthsolutionsover100samples. OnCoverageQA,SimpleStratleads
to0.36reductioninKLDivergenceonaverageonLlama3modelsandaconsistent0.05increase
inrecall. Weshowgainsontopoftemperaturescalingleadingtoimproveddiversityorthogonalto
increasingtemperature.
Concretely,ourworkcontributesthefollowing:
• CoverageQAdatasetof105under-specifiedquestionsautomaticallygeneratedfromWiki-
Data (Vrandecˇic´ & Krötzsch, 2014) annotated with on average 28.7 valid solutions per
question.
• We propose SimpleStrat a training-free approach for improving diversity with auto-
stratificationandprobabilisticprompting.
• WedemonstrateSimpleStratimprovesdiversityonCoverageQAwith0.36reductioninKL
DivergenceonaverageonLlama3modelsandaconsistent0.05increaseinrecallacrossall
temperaturesforGPT-4o.
2 RELATED WORK.
TemperatureScaling. GoingbackasfarasPlattscaling(Platt,2000)andlaterappliedtoneural
networks(Hinton,2015;Guoetal.,2017),temperaturescalingcontrolstherandomnessofprobability
distributions1. FordatasetgenerationwithLLMs, Chungetal.(2023)extendstemperature-based
diversitybyadditionallydownsamplingpreviouslysampledtokens.Toaddressthedecreaseinquality,
theyadvocateforhumaninterventiontomanuallyfilteroutirrelevantdiversityandmanuallyfixing
wronganswersinQAtasks. Weshowinourworktemperaturescalingleavesmuchtobedesired.
ImprovingLanguageModelDiversitywithSearch. Inautoregressivegeneration,choicesover
earlytokenstendtohavemoreimpactontheeventualcompletion. Beamsearchamelioratesthis
biasbyallowingformultiplecandidatesinsearchingfortheprobabilitymaximizingcompletion,
MaximumaPosteriori(MAP)Lowerre&Reddy(1976). Attheendofthesearch,beamsearchwill
havemultiplecandidatesolutionsencounteredduringsearch. DiverseBeamSearch(DBS)proposes
introducinganauxiliarydissimilarityobjectivequantifyingthediversityamongcandidatesinthe
beam(Vijayakumaretal.,2016). Especiallyonthetaskofimagecaptioning,DBSshowsimprove-
ment for discovering higher probability completions and discovering diverse continuations. Our
improvementsareorthogonaltobeamsearchandourin-contextapproachcorrectsforinaccuraciesin
themodeledlikelihoodsofcandidatesolutions.
Otherapproaches(Samvelyanetal.,2024;Bradleyetal.,2023)basedonMAP-Elites(Mouret&
Clune,2015)requiremanualdetermineddimensionsofrelevantdiversityanddiscretizationofthe
solution space into equally-sized bins. Diversity is then achieved by mutations and evolutionary
methodstocoveradjacentbins. Thissearchispotentiallyslowiftheseedsetofsolutionsdonot
alreadyprovidecoverageoverthesolutionsspace. Ourapproachdoesnotneedseedsolutionsand
avoidsmanuallyidentifyingdimensionsofdiversity. Instead,werelysolelyoncapabilitieswithinthe
model.
In-contextMethodstoIncreaseDiversity. WhenLLMswerefirstintroduced,LMswereusedto
augmentexistingdatasetswithmorediversity(Wei&Zou,2019;Ngetal.,2020;Daietal.,2023).
Asnaturallanguageisdifficulttoguaranteecorrectness,thespaceofaugmentationsisconservatively
limitedtothesaurusbasedsynonymreplacement.Morerecently,LanguageModelCrossoverproposes
presentingarandomsubsetofexistingdatapointstoanLLMandaskittohallucinatemoredata
pointsthatlikelycamefromthesamedistributionMeyersonetal.(2023). Thisislimitedcombining
aspectsofexistingdatapointsintonewgenerations. Althoughthesemethodsaddressthelimitations
ofusingthemodel’stokenprobabilitiesbyin-contextlearning, theyareineffectiveatgenerating
meaningful diversity. They are limited to either a human identified domains of interest or trivial
variationssourcedfromsynonymsorminickingrandomsubsetsoftheexistingdataset.
ApplicationsofDiversity. AsshownbyRaventósetal.(2024),datasetdiversityiscrucialformodel
generalization. Belowsufficientcoverageofthedesiredtask,themodelwillresorttomemorization,
1UseoftemperatureparametergoesbackatleasttoVerhulst’sdevelopmentoflogisticregressioninresponse
toMalthus’AnEssayonPrincipleofPopulation(Malthus,1798;Verhulst,1838).
3but when sufficient diversity is presented it will learn to generalize. As LLMs are increasingly
usedforgeneratingsyntheticdata(Dubeyetal.,2024),methodsfordiversitywillbecritical. This
insightfollowsfromextensiveworkdemonstratingthebenefitsofdataaugmentationforbiasmit-
igation(Sharmanskaetal.,2020)anddomainadaptation(Huangetal.,2018;Dunlapetal.,2023;
Trabuccoetal.,2023).
Incodeandmathapplications,checkingvalidityefficientlyenablesmoreaggressiveaugmentations.
Onesuchaugmentationfordiversifyingthelanguagessupportedbythemodel,dataistranslatedto
differentnaturalorprogramminglanguage(Chenetal.,2023;Cassanoetal.,2023). Inotherdomains
suchasimages,text-to-imagemodelshavebeenusedtododiversifydataintouncommonsettings. In
thesettingofdiversifyinganaccumulatingdataset,thesemethodscantakeadvantageofanexisting
sourceofvariance(fortranslation)orsetofpreviouslygenerateddatapoints. Ourprimaryfocusis
onsettingswhereSimpleStratisunawareofpastdatasamplestosupportawidersetofapplications.
AmbiguousorUnderspecifiedDatasets. ClariQ(Aliannejadietal.,2020),CLAQUA(Xuetal.,
2019), and AmbigQA (Min et al., 2020) focus on assessing LM’s ability to formulate clarifying
questions. Thesequestiontendtohaveonly2candidatesolutions, asthereexistsagroundtruth
clarifyingquestionwhoseanswerfullyspecifiesthequestion. AmbiguousTriviaQA(Kuhnetal.,
2022)alsolooksatunder-specifiedquestions,butassumeauserhascontextualinformationthat’s
hidden. Forinstance,"WhereinEnglandwassheborn?"or"Whowasthefirstwomantomakea
soloflightacrossthisocean?". Wedistinguishourunderspecifiedquestionsettinginthispaperas
onewheretheuserisindifferent. Inthissetting,thegivenanansweritshouldbeeasytoverifythe
answeriscorrectwithoutadditionalhiddencontext.
Coding datasets like Description2Code (Caballero et al., 2016), Wiki2SQL (Zhong et al., 2017),
SPIDER(Yuetal.,2019),code-contest(Lietal.,2022),Apps(Hendrycksetal.,2021),andLeetcode
HardShinnetal.(2023)admitmultiplevalidanswers. However,thespaceofvalidimplementations
is infinite, making diversity difficult to measure, and good coding practices enforce preferences
amongvalidimplementations. WeadditionallyconstructCoverageQAtohaveanexhaustivelistof
ground-truthanswersinordertomeasuretheimpactofdiversityoncoverage.
3 METHOD
3.1 WORKFLOWOVERVIEW
Asillustratedin2,SimpleStratconsistofthreestages,1)auto-stratification,2)heuristicestimation,
and3)probabilisticprompting. Foreachuniqueuserprompt,theoutputsofthefirsttwostagescan
becachedtoavoidrecomputingfeed-forwards.
3.2 AUTO-STRATIFICATION
Foragivenuserrequest,r ,wecallS,thespaceofvalidsolutions. Inmanysettings,thespaceof
user
potentialsolutions,S maybenaturallypartitionedbasedongeography,parity,ordemographics. The
partitionfunction,P :S →L,assignsanysolutionsfromStoapartitionlabell inLthesetofparti-
j
tionlabels. Partitionfunctionsaremostusefulifthey’reasbalancedaspossible. Abalancedpartition
function minimizes imbalance(P,L) = max (|{s|P(s)=l}|)−min (|{s|P(s)=l}|).
l∈L l∈L
Thegoalofauto-stratificationistosearchforasetofpartitionfunctionsP={P ,P ,...,P },that
1 2 n
arebalanced. Traditionally,insettingswherethereareoft-overlookedoralargeorinfinitenumber
ofvalidsolutions,stratifiedsamplingcanensureourlimitedbudgetofsamplescoversthespaceof
solutionsevenly.
Basedonthisinsight,wepromptthelanguagemodeltoidentifypromisingdimensionsofdiversity.
Concretely,thelanguagemodelproposesgoodclarifyingquestionsthatwillpotentiallyeliminate
halfofthepotentialsolutionsbasedontheuserrequest. Theseclarifyingquestionstendtoalignwith
semanticallysignificantdifferences. Intherunningexample,whenasked,"NameaUSState,"the
statescanbepartitionedbasedonEastorWestoftheMississippiRiver. SeeApp.Cforfullprompt.
43.3 HEURISTICESTIMATION
As previously observed in Zou et al. (2022); Yan et al. (2023); Halawi et al. (2024), LLMs can
usedinforecastingtoestimatewell-calibratedprobabilitiesofeventsthathavenotyetoccured. For
forecasting,themodelsuccessbenefitssubstantiallyfromhavingupdatednewsthroughwebsearch.
Althoughourunnecessaryfortheofflinebenchmarksweconsider,thismaybehelpfulforaccurate
estimationdependingontheapplication. However,asourgoalisdiversity,westandtobenefiteven
fromcoarse-grainapproximateproportions. WeemployasimilarreasoningtemplateasHalawietal.
(2024)toestimatetheproportionofvalidsolutionsliewithineachstrata.
Inheuristicestimation,welooktoestimatethejointdistributionforeachstratum,⃗l=[l ,l ,l ,...].
1 2 3
Formally, we define the weighted-stratification as W = (P,ρ), where ρ(⃗l) = Pr [P (s) =
s∼S 1
l ,P (s)=l ,P (s)=l ,...]forPidentifiedinauto-stratification. Toimprovescalability,we
1,j 2 2,j 3 1,j
assumethepartitionfunctionsareindependentandmultiplythemarginalprobabilitiestogetthejoint
probabilitiesassociatedwitheachstratum.
(cid:89)
ρ(l ,l ,...,l )= ρ (l ) (1)
1 2 m i i
i
We ask the LLM for each l , to estimate the marginal proportion of solutions this holds for. As
j
thismaynotaddupto1,wenormalizetheestimatestoformaproperprobabilitydistribution. For
simplicity,wefocusinthisworkonsettingswhereallsolutioninthesolutionspaceisequallylike.
AsnotedinSec.3.2, weencouragetheLLMtoproposebalancedpartitions. However, heuristic
estimationallowustosupportimbalancedpartitionsbyreweighingthesamplingtofavorstratum
withmorepotentialsolutions. MoredetailsonpromptinginApp.D.InFig.2,theLLMdetermines
thejointprobabilitiesacrosstwostratas,theMississippiRiverandtheMissouriCompromiseLine.
3.4 PROBABILISTICPROMPTING.
Postheuristicestimation,asetofstatumissampledfromthejointprobabilitydistributioninEqn.1.
Thisimplicitlyformsaprobabilisticprompt,whichspecifiesadistributionoverconcretelanguage
modelprompts. Afterapromptissampled,theLLMisthenusedtosamplefromwithinthestratum.
BacktoFig.2,EastandSoutharesampledfromtheMississippiandMissouristratarespectively,
augmentingthefinalpromptwithdiversespecifications.
Formally,call⃗lastratumdefinedbychoicesofl foreachP acrossalli. CallPromptafunction
i,j i
thatmapsthestratum,⃗ltoaconcreteprompt,Prompt(⃗l). Intuitively,theprobabilitiesoftheprompt
distributionisdefinedbyPr[Prompt(⃗l]=ρ(⃗l). Wecanthencomputetheprobabilityofasolution
Pr[s]=(cid:88) Pr[Prompt(⃗l)]∗Pr[s|Prompt(⃗l)]=(cid:88) ρ(⃗l)∗Pr[s|Prompt(⃗l)] (2)
⃗l ⃗l
Thespecificlanguagemodel’snext-tokenprobabilitiesdefinePr[s|Prompt(⃗l)].
As the probabilistic prompt is human readable form, the user can inspect the properties and the
proportionsandmodifyittoadjustforunwantedbiasorremoveunwantedfactors. Forinstance,
whenproposingEnglishbabynames, wemaywantthemodeltoproposemalevsfemalenames
equallyoften,eventhoughtherearemorefemalethanmalebabynames2. Thisinterpretabilityand
controllabilityisamajoradvantageofSimpleStratinpractice.
4 COVERAGEQA DATASET
4.1 OVERVIEW
Wewishtoevaluategenerationdiversityinsettingswhere1)userrequestshavemorethanonedistinct
correctanswer,2)andanswersareequallylikely,and3)answersdonotrequirehiddenorimplicit
2AsreportedinWilson(2016),thereare18,993uniquenamesforgirlsand13,959forboysin2015reportby
SocialSecurityAdministration.
5contexttoverify. Thesethreepropertiesallowustomeasurediversityinthesenseofwhetherthe
languagemodelwillprovidecoverageoverthefullsolutionspacewhenresampled. Unfortunately
existingbenchmarksdiscussedinSec.2donotsatisfytheseproperties. WeintroduceCoverageQA
toassessthelanguagemodelgenerationdiversity. Thedatasetconsistsoftwosplits: CoverageQA-
Curated, manually curated naturally underspecified questions, and CoverageQA-Wikipedia, and
auto-generateddatasetofunderspecifiedquestions.
4.2 COVERAGEQA-WIKIPEDIAAPPROACH
TogenerateCoverageQA-Wikipedia,weleveragetheWikidataknowledgebasewhichcontainsall
relationalmappingsbetweenentitiesandpropertiesinWikipedia. Ourgenerationprocessstartswith
aninitialitem-propertypairingandaconstraintonthenumberofcorrectanswers. Wethenperforma
recursivesearchthroughWikidatatofindallsetsofitem-propertyconstraintsandtheircorresponding
answersthatmeetourcriteria. Theseconstraintsaresubsequentlytransformedintonaturallanguage
questionsusingGPT-4.
ConsideraninitialpairingoftheWikidataitem"country"withtheproperty"instanceof". Wemight
specify that we want between 20 and 40 valid answers. Our search would then yield a set of all
constraintsfromtheknowledgebasethatfittheinitialconditions,suchas"instanceofcountry,located
inEurope,usesEuroascurrency". GPT-4wouldconvertthisintoanaturallanguagequestionlike
"NameacountrylocatedinEuropethatusestheEuroasitscurrency."
Thisapproachhasseveraladvantages: 1)itallowsustocreateadiverseandextensivebenchmark
thatcanbeeasilyupdatedwithweeklyupdatestoWikidata,2)itallowsustoarbitrarilyspecifythe
sizeofthesolutionspaceasconstraintscanbeaddedorremovedtoform; and3)thisprocessin
principlecancuratealargedatasetwithlittlemanualeffortorsupervision. Intheinitialinstantiation
ofCoverageQAdataset,wepublish105questionsacross4domains,eachcorrespondingtoadifferent
initialseeditem-propertypair. Toensurequality,weemploybothautomaticfilters(e.g.,excluding
certaingenericproperties)andmanualcurationtoremoveredundantorunsuitablequestions. This
datasetcanbesubstantiallyexpandedasweonlyused4domains,butweleavethisforfuturework.
Foradetailsonthedatasetbreakdownanddetailsonthequestiongenerationprocess,pleasereferto
Appendix A.1.
5 RESULTS
5.1 EVALUATIONSETUP
FortheprimaryempiricalevaluationofSimpleStrat,weusegpt4o-2024-08-06. Toobtaintrueanswer
distributionsandperformdivergencefromuniformanalysis,weuseopen-sourcemodelsfromthe
Llama3and3.1families,specificallythe8Band70Bvariants. Theinferenceofthesemodelswere
runon8A100-80GBGPUs. Additionally,weleverageclaude-3.5-sonnet-20240620forbaseline
evaluationsontwodatasets: CoverageQACuratedandCoverageQAWikipedia. ForCoverageQA,
weusedWikiDataversionfrom07-03-2024.
5.2 MEASURINGDIVERSITY
Weconsidertwomeasuresofdiversity. Formodelswithaccessiblesoftmaxnext-tokenprobabilities,
we compute the probability of each solution in the solution space. We then define distributional
diversityasthedistributionaldistancebetweentheresponsedistributionimpliedbythesampling
processandlogitsandtheground-truthdistributionderivedfromtheseprobabilities.ForCoverageQA,
theground-truthdistributionisuniformovervalidsolutionsandzeroelsewhere. Ingeneral,itcanbe
morecomplex.
In setting where we do not have access to the next-token distribution, we evaluate diversity by
resamplingresponsestoCoverageQA100timesperquestion. Thisallowsustoempiricallyobserve
thediversityintheformofcoverage. Wecallthiscoveragediversity. Tomeasurecoverage,wereport
therecall(uniquevalidsolutions/totaluniquevalidsolutions)onthereferencesolutions. Note
6Lakes
Erie
Erie Huron Michigan Ontario Superior Invalid Huron
Michigan
Ontario
100 GPT-4o 100 SimpleStrat (GPT-4o) S Inu vp ae lir dior
80 80
60 60
40 40
20 20
0 0
0.15 0.55 0.75 1 1.25 1.5 0.15 0.55 0.75 1 1.25 1.5
Temperature Temperature
Figure3: Diversityscaledwithtemperature. Weshow100resamplesof"NameoneGreatLakeinthe
UnitedStates."Ontheright,weshowtheresultofresamplingGPT-4o100timespertemperature.Incontrast
toSimpleStratontheleft,GPT-4oattemperature1.5stillonlysamplesLakeHurononceandneversamples
LakeOntario.SimpleStratimprovesthediversityacrossalltemperatures.
thatthisisanunconventionalmeasureofrecallasit’soverthesolutionspace3. Toensurethisdoesn’t
comeatthecostofquality,wealsoshowprecisionisnotreduced.
5.3 QUALITATIVEEXAMPLE
Consider the question "Name one Great Lake in the United States." as shown in Fig. 3. We see
that temperature scaling with GPT-4o results in a strong preference/bias for Lake Erie. This is
certainlyacorrectcontinuationandunderthelanguagemodelingobjectiveshouldbeincentivized.
Increasingthetemperaturehelpssamplethenextmostlikelycandidatesolutionsmoreoften.However,
even when increasing the temperature past 1 there is still low coverage over the solutions space.
Specifically,Huronisonlyseenonceoutof100samplesat1.5temperature,andLakeOntariois
neverobserved. Thisisundesirableifthedataisusedtoproposingcandidateplans,generatingtest
cases,orgeneratingtrainingdata. Notonlyisthereinsufficientcoverageoverallpossiblesolutions,
butthemodelconsistentlyhasastrongpreferenceforLakeErie. Thisundesiredbiasesingenerations
mayleadtoproblemsindownstreamusecases.
InFig.3,weobserveamuchmoreuniformdistributionovervalidsolutionswhenusingSimpleStrat.
Notably, we observe full coverage over all 5 Great Lakes. At lower temperatures, there is still a
preferenceofasinglelakeovertheothers,inthiscaseLakeHuron. However,thisislesspronounced
athighertemperaturesandisasignificantimprovementoverGPT-4owithoutSimpleStrat.
5.4 COVERAGEDIVERSITYONPROPRIETARYMODELS
Wefirstassesscoveragediversity,specifically,themodel’sabilitytorecallallthevalidsolutions
uponresampling. Thismeasureisclearlyimpactedbytemperatureastemperaturezeroorgreedy
decodingofLLMsleadstoasingledeterministicresult. Wecomparethecoveragediversity(recall)
of SimpleStrat, GPT-4o, and Claude 3.5 Sonnet as a function of temperature. We sweep over
temperatures from 0.15 to 1.5. Although not shown in the evaluation, note that SimpleStrat has
the advantage of providing diversity even when sampled at temperature zero. SimpleStrat with
GPT-4o leads to an improvement to recall across all temperatures as shown in Fig.4. On the
CoverageQA-Curated,weseeaconsistent0.2increaseinrecalloverthesamebasemodel,GPT-4o.
OnCoverageQA-Wikipedia,weseeasmuchas0.05increaseinrecall
OurquantitativeresultsareconsistentwiththeGreatLakesexampleinFig.3. Scalingtemperature
alone does not lead to as much coverage diversity as combining with SimpleStrat. The recall
importantlydoesnotcomeattheexpenseofqualityasmeasurebyprecisionasshowninApp. B.
3NottobeconfusedwithconventionalrecallwherewemightmeasurehowmanyvalidsolutionstheLLM
recognizeasvalid.
7
tnuoC tnuoC(a) CoverageQA-Curated. (b)CoverageQA-Wikipedia
Figure4: Diversitymeasuredwithrecallscaledwithtemperature.Thefigureshowstheimprovedrecallon
CoverageQAcomparedtoGPT-4oandClaude3.54.Recallindicatesthepercentageofgroundtruthquestions
observedaftersampling100times.ThebenefitofSimpleStratisespeciallypronouncedatlowtemperatures,but
thebenefitisevidentacrossalltemperatures.
(a)Llama3.18B (b)Llama3.170B
Figure5:KLdivergencefromuniformforBaselinevsSimpleStratonCoverageQAWikipedia.Lowerdiver-
genceindicatescloseralignmentwiththedesireduniformdistribution,arrowindicatesdirectionofmaximum
improvementfrombaseline
5.5 DISTRIBUTIONALDIVERSITYWITHLLAMA3
WeusetheLlama3modelfamilytoanalyzeanswerdistributionaldiversityinCoverageQA.Open-
sourcemodelsletuscalculateexactexpecteddistributionsbyexamininglogitsofallvalidcontinua-
tionsforaprompt. ThisisnotpossiblewithGPT-4oandClaude3.5Sonnet,wherereliablyestimating
thetrueprobabilitieswouldrequireextensivesampling. Ourapproachefficientlydeterminestrue
expecteddistributionsofvalidanswersforanyinput,improvinganalysisaccuracy,andovercoming
theresourceconstraintsofhighcountsampling.
Forourbaseline,wepromptthemodelsanddirectlycomputePr[s|Prompt(⃗l)]foreachsolution,s.
Thisissimplytheproductoftheindividualnext-tokenprobabilities. ForSimpleStrat,theprobability
involvesthenext-tokenprobabilityconditionedonthepromptweightedbytheprobabilitytheprompt
isselected. Concretely,theprobabilityananswerissampledbySimpleStratcanbecomputedbased
onEqn.2. Thenext-tokenprobabilitybasedresponsedistributionPr[s|Prompt(⃗l)]computedjustas
thebaseline,andwedoasumweightedbythejointprobabilitiesassignedinheuristicestimation.
Weassignremainingprobabilitydensitytoan"Invalid"categorytoformaproperdistribution. The
probabilisticformulationallowsustoeasilycomputetheresponsedistributionofSimpleStrat.
4Claudedoesnotallowfortemperaturesabove1.
8(a)Llama3.18B. (b)Llama3.170B.
Figure 6: Distributional Diversity Comparison. We show the response probability as defined by next-
token-probabilities for the top 20 ground truth answers on Llama 3.1. For both 8B and 70B, SimpleStrat
providesmeaningfulimprovementtotheresponsedistributionbothforvaluespreviouslyover-representedinthe
distributionandthosepreviouslyunderrepresented.
Table1:ComparisonofBaselineandSimpleStratAverageKLDivergenceforDifferentModelsonCoverageQA-
CuratedandCoverageQA-Wikipedia.Smallernumbersreflectcloseralignmenttouniformdistribution.
CoverageQACurated CoverageQAWikipedia
Model
Baseline SimpleStrat Baseline SimpleStrat
Meta-Llama-3-8B-Instruct 2.78 1.74 2.75 2.47
Meta-Llama-3.1-8B-Instruct 2.47 1.19 2.60 2.39
Meta-Llama-3-70B-Instruct 3.24 2.17 3.28 2.73
Meta-Llama-3.1-70B-Instruct 2.70 1.54 2.78 2.38
Ourground-truthdistributionisadistributionwhereeachvalidanswerhasequalprobability,and
no invalid answers have probability density. We use Kullback–Leibler (KL) Divergence as the
discrepancy metric to measure how much our distribution deviates from a uniform distribution.
Across the four models in the Llama 3 family, SimpleStrat achieves an average reduction in KL
divergence from uniform of 1.14 compared to the baseline on the curated CoverageQA dataset.
ForthegeneralCoverageQAdataset,thereductionis0.36. TheseresultsindicatethatSimpleStrat
producesaresponsedistributionmuchclosertothegroundtruthdistributionthanthebaselinemethod.
Additionally,weanalyzetheKLdivergenceonaper-questionbasistoidentifywhereourmethod
achievesimprovement(amoreuniformdistribution). ThescatterplotinFig 5bshowsKLdivergence
valuesforSimpleStrat(y-axis)versusthebaseline(x-axis)foreachquestionintheCoverageQA
Wikipediadataset. PointsabovethediagonallinerepresentquestionswhereSimpleStratoutperforms
thebaselinebyyieldingalowerKLdivergence. Thevastmajorityofpointsfallonorabovethis
line, indicating that SimpleStrat consistently produces improvement in generating more uniform
distributionsacrossthequestionsinthedataset.
InFig.6,weseeanexampleoftheresponsedistributionforLlama3.1withandwithoutthecorrection
duetoSimpleStrat. Thebasemodeldistributionisstronglybiasedtowardsit’smostdesiredoutput.
AsillustratedinFig.1,Californiaispreferredbyalongmargin. Thus,it’snotsurprisingthatwe
observedlittlediversitywhenbysimplyincreasingtemperature. Incontrast,SimpleStratprovides
amuchmoreuniformdistribution. Theoverrepresentedsolutionsareadjustedtobelowerandthe
under represented solutions are adjusted to be higher. We note that the distributions are still not
perfectly uniform. For more examples, see App. E. Although Fig. 6 shows the larger model has
aclosertoidealdistribution, theresultsinTable1indicatelargermodelshaveonaverageworse
diversity. Thismaybearesultofmemorizationexacerbatedinlargermodels.
Modelversions. Interestingly,Llama3.1handlesunderspecifiedquestionsbetterasithasconsistent
lower KL Divergence. Although we do not have access to the training methodology, the likely
increaseindatasetsizeandmorecarefulmodeldesignseemstoimprovebasedmodecapabilitieson
underspecifiedquestions.
96 LIMITATIONS
AlthoughSimpleStratshowsimprovementempirically,itissensitivetothemodelselectinggood
meaningfulaxisinauto-stratificationandcorrectjointprobabilitiesinheuristicestimation. Aswork
on LLMs for forecasting improve, we expect LLMs to produce better estimates especially when
givenaccesstoexternaldataanddataanalysistools. Forourprototype,werestrictedtostudyingthe
model’sintrinsiccapabilities. Further,themodelmayhavebiasesconcerningraceandgenderthat
maybereflectedalsointheauto-stratificationandheuristicestimation. Assuch,itisrecommended
theprobabilisticpromptdistributionofSimpleStratiscarefullyinspectedforcriticalapplications.
Finally,CoverageQAisadatasetofshortresponses. Althoughthismakeevaluationmorepractical,
webelieveSimpleStratwillbeespeciallyimpactfulinsettingsthatrequirelowtemperatureespecially
multi-stepreasoningasidentifiedbyZhangetal.(2024).
7 CONCLUSION
Inthispaper,weproposeSimpleStratwhichoffersaninnovativealternativebyleveragingtheLLM
itselftopartitionthesolutionspaceintodistinctstrata. Thisprocesswecallauto-stratification. At
inferencetime,arandomstratumisselected,andasampleisdrawnfromwithinthatstratum. This
approachachievesbetterdiversitywithoutsacrificingqualityasincreasingtemperaturewould.
To quantitatively measure diversity, we introduced the CoverageQA dataset, which consists of
underspecifiedquestionswithmultipleequallyvalidanswers. Wemeasurediversitywithtwometrics:
foropen-sourcemodels,wemeasuredistributionaldifferencewithKLDivergenceandforproprietary
models, wemeasurecoverageoverthesetofground-truthsolutions. Ourrigorousevaluationon
bothproprietaryandopen-sourceLLMsdemonstratedthatSimpleStratachievessignificantlyhigher
recallandproducesanswerdistributionsclosertouniformcomparedtotraditionaltemperature-based
samplingmethods.
ACKNOWLEDGEMENT
WethankAnastasiosAngelopoulos,JacobSteinhardt,BrandonTrubucco,KevinYang,ParthAsawa,
and Alan Zhu for their insightful discussion. Sky Computing Lab is supported by gifts from
Accenture, AMD, Anyscale, Google, IBM, Intel, Microsoft, Mohamed Bin Zayed University of
ArtificialIntelligence,SamsungSDS,SAP,Uber,andVMware.
REFERENCES
LukasAichberger, KajetanSchweighofer, MykytaIelanskyi, andSeppHochreiter. Semantically
diverse language generation for uncertainty estimation in language models. arXiv preprint
arXiv:2406.04306,2024.
Mohammad Aliannejadi, Julia Kiseleva, Aleksandr Chuklin, Jeff Dalton, and Mikhail Burtsev.
Convai3: Generatingclarifyingquestionsforopen-domaindialoguesystems(clariq),2020. URL
https://arxiv.org/abs/2009.11352.
HerbieBradley,AndrewDai,HannahTeufel,JennyZhang,KoenOostermeijer,MarcoBellagente,
Jeff Clune, Kenneth Stanley, Grégory Schott, and Joel Lehman. Quality-diversity through ai
feedback. arXivpreprintarXiv:2310.13032,2023.
EthanCaballero,.OpenAI,andIlyaSutskever. Description2CodeDataset,82016. URLhttps:
//github.com/ethancaballero/description2code.
FedericoCassano, JohnGouwar, FrancescaLucchetti, ClaireSchlesinger, AndersFreeman, Car-
olynJaneAnderson,MollyQFeldman,MichaelGreenberg,AbhinavJangda,andArjunGuha.
Knowledgetransferfromhigh-resourcetolow-resourceprogramminglanguagesforcodellms.
arXivpreprintarXiv:2308.09895,2023.
NuoChen,ZinanZheng,NingWu,LinjunShou,MingGong,YangqiuSong,DongmeiZhang,andJia
Li. Breakinglanguagebarriersinmultilingualmathematicalreasoning: Insightsandobservations.
arXivpreprintarXiv:2310.20246,2023.
10JohnJoonYoungChung,EceKamar,andSaleemaAmershi. Increasingdiversitywhilemaintaining
accuracy:Textdatagenerationwithlargelanguagemodelsandhumaninterventions.arXivpreprint
arXiv:2306.04140,2023.
Haixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Yihan Cao, Zihao Wu, Lin Zhao,
ShaochenXu,WeiLiu,NinghaoLiu,etal. Auggpt: Leveragingchatgptfortextdataaugmentation.
arXivpreprintarXiv:2302.13007,2023.
AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,AhmadAl-Dahle,Aiesha
Letman,AkhilMathur,AlanSchelten,AmyYang,AngelaFan,etal. Thellama3herdofmodels.
arXivpreprintarXiv:2407.21783,2024.
Lisa Dunlap, Alyssa Umino, Han Zhang, Jiezhi Yang, Joseph E Gonzalez, and Trevor Darrell.
Diversifyyourvisiondatasetswithautomaticdiffusion-basedaugmentation. Advancesinneural
informationprocessingsystems,36:79024–79034,2023.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural
networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
ConferenceonMachineLearning,volume70ofProceedingsofMachineLearningResearch,pp.
1321–1330.PMLR,06–11Aug2017. URLhttps://proceedings.mlr.press/v70/
guo17a.html.
Danny Halawi, Fred Zhang, Chen Yueh-Han, and Jacob Steinhardt. Approaching human-level
forecastingwithlanguagemodels. arXivpreprintarXiv:2402.18563,2024.
DanHendrycks,StevenBasart,SauravKadavath,MantasMazeika,AkulArora,EthanGuo,Collin
Burns,SamirPuranik,HoraceHe,DawnSong,andJacobSteinhardt. Measuringcodingchallenge
competencewithapps,2021. URLhttps://arxiv.org/abs/2105.09938.
GeoffreyHinton. Distillingtheknowledgeinaneuralnetwork. arXivpreprintarXiv:1503.02531,
2015.
DavidCHowell. Statisticalmethodsforpsychology. PWS-KentPublishingCo,1992.
Sheng-Wei Huang, Che-Tsung Lin, Shu-Ping Chen, Yen-Yi Wu, Po-Hao Hsu, and Shang-Hong
Lai. Auggan: Crossdomainadaptationwithgan-baseddataaugmentation. InProceedingsofthe
EuropeanConferenceonComputerVision(ECCV),pp.718–731,2018.
Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Clam: Selective clarification for ambiguous
questionswithlargelanguagemodels. arXivpreprintarXiv:2212.07769,2022.
YujiaLi,DavidChoi,JunyoungChung,NateKushman,JulianSchrittwieser,RémiLeblond,Tom
Eccles,JamesKeeling,FelixGimeno,AgustinDalLago,ThomasHubert,PeterChoy,Cyprien
de Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven
Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson,
PushmeetKohli,NandodeFreitas,KorayKavukcuoglu,andOriolVinyals.Competition-levelcode
generationwithalphacode.Science,378(6624):1092–1097,December2022.ISSN1095-9203.doi:
10.1126/science.abq1158. URLhttp://dx.doi.org/10.1126/science.abq1158.
BrucePLowerreandBRajReddy. Harpy,aconnectedspeechrecognitionsystem. TheJournalof
theAcousticalSocietyofAmerica,59(S1):S97–S97,1976.
ThomasRobertMalthus. AnEssayonthePrincipleofPopulation. J.Johnson,London,1798.
ElliotMeyerson,MarkJNelson,HerbieBradley,AdamGaier,ArashMoradi,AmyKHoover,and
JoelLehman. Languagemodelcrossover: Variationthroughfew-shotprompting. arXivpreprint
arXiv:2302.12170,2023.
SewonMin, JulianMichael, HannanehHajishirzi, andLukeZettlemoyer. Ambigqa: Answering
ambiguousopen-domainquestions. arXivpreprintarXiv:2004.10645,2020.
GElliottMorris. StrengthinNumbers: HowPollsWorkandwhyWeNeedThem. WWNorton&
Company,2022.
11Jean-BaptisteMouretandJeffClune. Illuminatingsearchspacesbymappingelites. arXivpreprint
arXiv:1504.04909,2015.
NathanNg,KyunghyunCho,andMarzyehGhassemi. Ssmba: Self-supervisedmanifoldbaseddata
augmentationforimprovingout-of-domainrobustness. arXivpreprintarXiv:2009.10195,2020.
OpenAI,JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoni
Aleman,DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,RedAvila,Igor
Babuschkin,SuchirBalaji,ValerieBalcom,PaulBaltescu,HaimingBao,MohammadBavarian,
JeffBelgum,IrwanBello,JakeBerdine,GabrielBernadett-Shapiro,ChristopherBerner,Lenny
Bogdonoff,OlegBoiko,MadelaineBoyd,Anna-LuisaBrakman,GregBrockman,TimBrooks,
MilesBrundage,KevinButton,TrevorCai,RosieCampbell,AndrewCann,BrittanyCarey,Chelsea
Carlson,RoryCarmichael,BrookeChan,CheChang,FotisChantzis,DerekChen,SullyChen,
RubyChen,JasonChen,MarkChen,BenChess,ChesterCho,CaseyChu,HyungWonChung,
DaveCummings,JeremiahCurrier,YunxingDai,CoryDecareaux,ThomasDegry,NoahDeutsch,
DamienDeville,ArkaDhar,DavidDohan,SteveDowling,SheilaDunning,AdrienEcoffet,Atty
Eleti,TynaEloundou,DavidFarhi,LiamFedus,NikoFelix,SimónPosadaFishman,JustonForte,
IsabellaFulford,LeoGao,ElieGeorges,ChristianGibson,VikGoel,TarunGogineni,Gabriel
Goh,RaphaGontijo-Lopes,JonathanGordon,MorganGrafstein,ScottGray,RyanGreene,Joshua
Gross,ShixiangShaneGu,YufeiGuo,ChrisHallacy,JesseHan,JeffHarris,YuchenHe,Mike
Heaton,JohannesHeidecke,ChrisHesse,AlanHickey,WadeHickey,PeterHoeschele,Brandon
Houghton,KennyHsu,ShengliHu,XinHu,JoostHuizinga,ShantanuJain,ShawnJain,Joanne
Jang,AngelaJiang,RogerJiang,HaozhunJin,DennyJin,ShinoJomoto,BillieJonn,Heewoo
Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar,
Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik
Kirchner,JamieKiros,MattKnight,DanielKokotajlo,ŁukaszKondraciuk,AndrewKondrich,
ArisKonstantinidis,KyleKosic,GretchenKrueger,VishalKuo,MichaelLampe,IkaiLan,Teddy
Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie
Lin,MateuszLitwin,TheresaLopez,RyanLowe,PatriciaLue,AnnaMakanju,KimMalfacini,
SamManning,TodorMarkov,YanivMarkovski,BiancaMartin,KatieMayer,AndrewMayne,
BobMcGrew,ScottMayerMcKinney,ChristineMcLeavey,PaulMcMillan,JakeMcNeil,David
Medina,AalokMehta,JacobMenick,LukeMetz,AndreyMishchenko,PamelaMishkin,Vinnie
Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély,
AshvinNair,ReiichiroNakano,RajeevNayak,ArvindNeelakantan,RichardNgo,Hyeonwoo
Noh,LongOuyang,CullenO’Keefe,JakubPachocki,AlexPaino,JoePalermo,AshleyPantuliano,
GiambattistaParascandolo,JoelParish,EmyParparita,AlexPassos,MikhailPavlov,AndrewPeng,
AdamPerelman,FilipedeAvilaBelbutePeres,MichaelPetrov,HenriquePondedeOliveiraPinto,
Michael,Pokorny,MichellePokrass,VitchyrH.Pong,TollyPowell,AletheaPower,BorisPower,
ElizabethProehl,RaulPuri,AlecRadford,JackRae,AdityaRamesh,CameronRaymond,Francis
Real,KendraRimbach,CarlRoss,BobRotsted,HenriRoussez,NickRyder,MarioSaltarelli,Ted
Sanders,ShibaniSanturkar,GirishSastry,HeatherSchmidt,DavidSchnurr,JohnSchulman,Daniel
Selsam,KylaSheppard,TokiSherbakov,JessicaShieh,SarahShoker,PranavShyam,Szymon
Sidor,EricSigler,MaddieSimens,JordanSitkin,KatarinaSlama,IanSohl,BenjaminSokolowsky,
Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie
Tang,NikolasTezak,MadeleineB.Thompson,PhilTillet,AminTootoonchian,ElizabethTseng,
Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun
Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang,
JonathanWard,JasonWei,CJWeinmann,AkilaWelihinda,PeterWelinder,JiayiWeng,Lilian
Weng,MattWiethoff,DaveWillner,ClemensWinter,SamuelWolrich,HannahWong,Lauren
Workman,SherwinWu,JeffWu,MichaelWu,KaiXiao,TaoXu,SarahYoo,KevinYu,Qiming
Yuan,WojciechZaremba,RowanZellers,ChongZhang,MarvinZhang,ShengjiaZhao,Tianhao
Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024. URL
https://arxiv.org/abs/2303.08774.
JohnPlatt. Probabilisticoutputsforsupportvectormachinesandcomparisonstoregularizedlikeli-
hoodmethods. Adv.LargeMarginClassif.,10,062000.
PranavPutta,EdmundMills,NamanGarg,SumeetMotwani,ChelseaFinn,DivyanshGarg,and
Rafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents. arXiv
preprintarXiv:2408.07199,2024.
12AllanRaventós,MansheejPaul,FengChen,andSuryaGanguli. Pretrainingtaskdiversityandthe
emergenceofnon-bayesianin-contextlearningforregression. AdvancesinNeuralInformation
ProcessingSystems,36,2024.
MikayelSamvelyan,SharathChandraRaparthy,AndreiLupu,EricHambro,AramHMarkosyan,
Manish Bhatt, Yuning Mao, Minqi Jiang, Jack Parker-Holder, Jakob Foerster, et al. Rainbow
teaming: Open-endedgenerationofdiverseadversarialprompts. arXivpreprintarXiv:2402.16822,
2024.
Viktoriia Sharmanska, Lisa Anne Hendricks, Trevor Darrell, and Novi Quadrianto. Contrastive
examplesforaddressingthetyrannyofthemajority. arXivpreprintarXiv:2004.06524,2020.
NoahShinn,FedericoCassano,EdwardBerman,AshwinGopinath,KarthikNarasimhan,andShunyu
Yao. Reflexion: Languageagentswithverbalreinforcementlearning,2023.
EdwardHSimpson. Theinterpretationofinteractionincontingencytables. JournaloftheRoyal
StatisticalSociety: SeriesB(Methodological),13(2):238–241,1951.
BrandonTrabucco,KyleDoherty,MaxGurinas,andRuslanSalakhutdinov. Effectivedataaugmenta-
tionwithdiffusionmodels. arXivpreprintarXiv:2302.07944,2023.
PFVerhulst. Noticeonthelawthatthepopulationfollowsinitsgrowth. CorrespMathPhys,10:
113–26,1838.
AshwinKVijayakumar,MichaelCogswell,RamprasathRSelvaraju,QingSun,StefanLee,David
Crandall,andDhruvBatra. Diversebeamsearch:Decodingdiversesolutionsfromneuralsequence
models. arXivpreprintarXiv:1610.02424,2016.
DennyVrandecˇic´ andMarkusKrötzsch. Wikidata: afreecollaborativeknowledgebase. Commun.
ACM,57(10):78–85,September2014. ISSN0001-0782. doi: 10.1145/2629489. URLhttps:
//doi.org/10.1145/2629489.
JasonWeiandKaiZou. Eda: Easydataaugmentationtechniquesforboostingperformanceontext
classificationtasks. arXivpreprintarXiv:1901.11196,2019.
Chris Wilson. Baby names: Why there are more for girls than boys. Time, May 2016. URL
https://time.com/4322881/baby-names-girls-boys/.
JingjingXu,YuechenWang,DuyuTang,NanDuan,PengchengYang,QiZeng,MingZhou,and
Xu Sun. Asking clarification questions in knowledge-based question answering. In Kentaro
Inui,JingJiang,VincentNg,andXiaojunWan(eds.),Proceedingsofthe2019Conferenceon
EmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceon
NaturalLanguageProcessing(EMNLP-IJCNLP),pp.1618–1629,HongKong,China,November
2019.AssociationforComputationalLinguistics. doi: 10.18653/v1/D19-1172. URLhttps:
//aclanthology.org/D19-1172.
QiYan,RaihanSeraj,JiaweiHe,LiliMeng,andTristanSylvain. Autocast++: Enhancingworld
eventpredictionwithzero-shotranking-basedcontextretrieval. arXivpreprintarXiv:2310.01880,
2023.
ShunyuYao,DianYu,JeffreyZhao,IzhakShafran,TomGriffiths,YuanCao,andKarthikNarasimhan.
Treeofthoughts: Deliberateproblemsolvingwithlargelanguagemodels. AdvancesinNeural
InformationProcessingSystems,36,2024.
Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene
Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. Spider: A large-scale
human-labeleddatasetforcomplexandcross-domainsemanticparsingandtext-to-sqltask,2019.
URLhttps://arxiv.org/abs/1809.08887.
EdwinZhang,VincentZhu,NaomiSaphra,AnatKleiman,BenjaminLEdelman,MilindTambe,
ShamMKakade,andEranMalach. Transcendence: Generativemodelscanoutperformtheexperts
thattrainthem. arXivpreprintarXiv:2406.11741,2024.
13VictorZhong,CaimingXiong,andRichardSocher. Seq2sql: Generatingstructuredqueriesfrom
naturallanguageusingreinforcementlearning. arXivpreprintarXiv:1709.00103,2017.
Andy Zou, Tristan Xiao, Ryan Jia, Joe Kwon, Mantas Mazeika, Richard Li, Dawn Song, Jacob
Steinhardt, Owain Evans, and Dan Hendrycks. Forecasting future world events with neural
networks. AdvancesinNeuralInformationProcessingSystems,35:27293–27305,2022.
14A APPENDIX
A.1 COVERAGEQADATASET
GenerationProcedure
Togeneratethequestions,wemanuallycameupwithinitialitemandpropertypairingstorunthe
recursivesearch. Weconstraintherecursivesearchtoyieldbetween20-40possibleanswerstokeep
thequestionswithincommonandrelevantcategories. Wefoundthatwithfewerthan20answers,
thequestionsbecometooobvious,whilewithmorethan40,theytendtogettoospecificandstray
fromgeneralknowledge. Therecursivesearchfirstfindsallitemsthatsatisfytheinitialconditions,
theniterativelyaddspropertiesinstepsuntileitherthemaximumdepth(numberofconstraints)is
reachedorthenumberofanswersfallsoutsidethedesiredrange. Weblacklistpropertiesthatare
detrimentaltohigh-qualityquestiongeneration,suchasanitem’spresenceinaspecificdatabase,
numericpropertieslikepopulation,andpropertiesthatintroducehighambiguity. Wethenmanually
evaluatethegeneratedconditionsandanswerstoensuretheymeetourcriteria. Withanappropriate
initialcondition,onequerycangeneratehundredsofvalidconstraintsthatcanlaterbeturnedinto
questions. Finally,weuseGPT-4toconverttheseconstraintsintonaturallanguage.
B F1 SCORES
WeshowF1scoresinFig.7toemphasizethattheprecisiondoesnotchangesubstantiallyasaresult
ofourmethod. Precisioniscalculatedoverthesetof100attemptshowmanyareinthegroundtruth.
Recallasmentionediscalculatedashowmanyuniquegroundtruthsolutionswereobservedinthe
100attempts.
(a)CoverageQA-Curated (b)CoverageQA-Wikipedia
Figure7:F1scorescaledwithtemperature.ThefigureshowssimilarcurvestorecallonCoverageQA.This
indicatestheimproveddiversitydoesnotcomeatacosttoprecision.
Table2:CoverageQADomains
Domain QuestionCount AverageNumberofAnswers
GeneralKnowledge(Curated) 10 64.1
USNationalParks 6 12.2
GeographyQuestions 75 27.4
ChemicalElements 14 17.5
C AUTO-STRATIFICATION PROMPT
We provide the full prompt in Tbl. 3. To improve prompt adherence, we provide one in context
exampleintheformofonesimulatedroundofmulti-turnconversation,i.e. weprovideanexample
setofreasoningfollowingthetemplate.
15SystemPrompt:
You’reahelpfulbrainstormingassistantthatiscarefultoconsiderallfactorstoaproblem.
User:
Iamtaskedwiththefollowingrequest:
%UserRequest
HelpmebrainstormhowtorespondtotheuserrequestbyprovidingalistofTrue/Falsepropertiesthesolutionmayormay
nothave.Usethefollowingstep-by-steptocomeupwithgoodproperties:
1. Ifyouwereplaying20questions,what’sagoodfirstquestiontoaskthatwouldsplitthepossibilitiesinhalf?
Listatleast5questionsandtheircorrespondingproperties.
Question:<Description>
2. RewriteeachquestionasaTrue/Falsepropertythat’strueforonehalfandfalsefortheother.
Question:<Description>
True/FalseProperty:<PropertyDescription>
3. Foreachproperty,comeupwithanexamplethatwouldsatisfytheproperty.
Property:<Description>
Example:<Description>
Isitavalidanswertotheuser’srequest?<Yes/No>
4. Foreachproperty,comeupwithanexamplethatwouldnotsatisfytheproperty.
Property:<Description>
Example:<Description>
Isitavalidanswertotheuser’srequest?<Yes/No>
5. Doesthepropertymentionacandidateanswerinit?
Property:<Description>
Doesthepropertymentionacandidateanswerinit?<Yes/No>
6. Foreachproperty,listwhetherweshouldincludeitornotinthefinallistofproperties. Donotincludeones
whereanexamplefromaboveisnotvalidorifitmentionsacandidateanswerinit.
Property:<Description>
Includeinfinallist?<Yes/No>
FinalListofTrue/FalseProperties:
1. <PropertyDescription1>
2. <PropertyDescription2>
EnsureallpropertiesarelistedaresentencesthatareeitherTrueorFalse
Table3:FullpromptforAuto-stratification.
D HEURISTIC ESTIMATION PROMPT
Wefirsttakeeachpartitionfunctionfromauto-stratificationandestimateastartingprobabilitywith
thepromptinTable4. ThispromptisheavilyinspiredbyHalawietal.(2024). Wethencollectallthe
proportionsandpassitthroughafinalHeuristicEstimationprompttoremoveredundantproperties
(negationsforinstance)andgivethemodelachancetocorrectanyincorrectprobabilities.SeeTable5
forfullprompt. Finally,weaskthemodeltoselectatmost3.
Notethatforperformancereasons,weestimatethemarginalprobabilitiesandmakeasimplifying
assumption of independence. This is not strictly true if one partition function is the negation of
theother. Thisleadspotentialstratumassignedpositiveprobabilitybutactuallythestratumhasno
solutions. Otherwise,therewouldbe2#ofPartitionFunctions stratatoestimateprobabilitiesof. Further,
LLMsseemlessreliablewhenaskedtoestimatefine-grainedprobabilities,whereasmostmarginal
probabilitiesarebydesigncloseto0.5.
Formally,ifP = ¬Q,thethestratumP ∧Qhaszeroprobability,eventhoughweassumeditto
be Pr[P]∗Pr[Q]. We handle approximation error in estimating the true prompt distribution by
16SystemPrompt:
Youareanexpertsuperforecaster,familiarwiththeworkofTetlockandothers. Yourmissionistogenerateaccurate
predictionsforforecastingquestions.Aggregatetheinformationprovidedbytheuser.Makesuretogivedetailedreasoning.
User:
Iamtaskedtoestimatetheprobabilitythatarandomsolutionto"UserRequest"hasthefollowingproperty"Partitioning
Property"
Instructions:
1. Provideatleast3reasonswhytheanswermightbeno.
{Insertyourthoughts}
2. Provideatleast3reasonswhytheanswermightbeyes.
{Insertyourthoughts}
3. Ratethestrengthofeachofthereasonsgiveninthelasttworesponses.Thinklikeasuperforecaster(e.g.Nate
Silver).
{Insertyourratingofthestrengthofeachreason}
4. Aggregateyourconsiderations.
{Insertyouraggregatedconsiderations}
5. Outputyouranswer(anumberbetween0and1)withanasteriskatthebeginningandendofthedecimal.
{Insertyouranswer}
Table4:PromptforPartition-specificHeuristicEstimation.
SystemPrompt:
Youareanexpertsuperforecaster,familiarwiththeworkofTetlockandothers. Yourmissionistogenerateaccurate
predictionsforforecastingquestions.Aggregatetheinformationprovidedbytheuser.Makesuretogivedetailedreasoning.
User:
I’mplayingagamewheremyfriendhasbeentaskedto:
"UserRequest"
IhavethefollowingY/NstatementsIcanaskmyfriend.IhaveprobabilitiesthatIthinkit’strue:%Insertnumberedlist
ofpartitionsandproportions.
Instructions:
1. ForeachY/Nstatement,isitredundantwithanotherstatement?
Y/Nstatement:<description>
Isredundant?<Y/N:Explanation>
2. Areanyoftheprobabilitiesinaccurate?Ifit’ssufficientlyaccuratejustreportbackthesamevalue.
Y/Nstatement:<Description>
Isaccurate?<Y/N:Explanation>
Probability:<Probability>
3. Pickatmostthreestatementsthatareleastredundantandpairwelltogether.Preferonesthatareclosestto50%
formostinformation.
FinalListofTrue/FalseProperties:
1. <Y/NProperties>::<Probability>
2. <Y/NProperties>::<Probability>
Table5:PromptforFinalHeuristicEstimation.
allowingthemodeltoreply"Invalid"totriggeraresample. Withthisadjustment,theprobabilistic
promptdistributionismaintainedforthisextremecase. Thiscorrectionhoweverdoesnotameliorate
potentialissueswith
17E ADDITIONAL PLOTS: DISTRIBUTIONAL ANALYSIS WITH LLAMA
WeprovideadditionalexamplesinFig9andscatterplotsforLlama3inFig8
(a)Llama38B (b)Llama370B
Figure8:KLdivergencefromuniformforBaselinevsSimpleStratonCoverageQAWikipedia.Additionalplots
forLlama38Band70Bmodels
18(a)Llama3.18B (b)Llama3.170B
(c)Llama3.18B (d)Llama3.170B
(e)Llama3.18B (f)Llama3.170B
(g)Llama3.18B (h)Llama3.170B
Figure9:BaselinevsSimpleStratProbabilityDistributionsThisfigureshowstheanswerdistributionsfor4
additionalquestionsfromCoverageQAcurated.Eachrowrepresentsadifferentquestion,showingdistributions
forLlama3.18Band70B.
19