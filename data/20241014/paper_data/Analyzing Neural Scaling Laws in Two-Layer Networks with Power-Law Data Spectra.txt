Preprint
ANALYZING NEURAL SCALING LAWS IN TWO-LAYER
NETWORKS WITH POWER-LAW DATA SPECTRA
RomanWorschech BerndRosenow
InstitutfürTheoretischePhysik InstitutfürTheoretischePhysik
UniversitätLeipzig UniversitätLeipzig
Brüderstraße16,04103Leipzig,Germany Brüderstraße16,04103Leipzig,Germany
MaxPlanckInstituteforMathematicsintheSciences
Inselstraße22,04103Leipzig,Germany
roman.worschech@uni-leipzig.de
ABSTRACT
Neuralscalinglawsdescribehowtheperformanceofdeepneuralnetworksscales
withkeyfactorssuchastrainingdatasize,modelcomplexity,andtrainingtime,of-
tenfollowingpower-lawbehaviorsovermultipleordersofmagnitude.Despitetheir
empiricalobservation,thetheoreticalunderstandingofthesescalinglawsremains
limited. Inthiswork,weemploytechniquesfromstatisticalmechanicstoanalyze
one-passstochasticgradientdescentwithinastudent-teacherframework,where
boththestudentandteacheraretwo-layerneuralnetworks. Ourstudyprimarily
focusesonthegeneralizationerroranditsbehaviorinresponsetodatacovariance
matricesthatexhibitpower-lawspectra. Forlinearactivationfunctions,wederive
analytical expressions for the generalization error, exploring different learning
regimesandidentifyingconditionsunderwhichpower-lawscalingemerges. Ad-
ditionally,weextendouranalysistonon-linearactivationfunctionsinthefeature
learningregime,investigatinghowpower-lawspectrainthedatacovariancematrix
impactlearningdynamics. Importantly,wefindthatthelengthofthesymmetric
plateaudependsonthenumberofdistincteigenvaluesofthedatacovariancematrix
andthenumberofhiddenunits,demonstratinghowtheseplateausbehaveunder
variousconfigurations. Inaddition,ourresultsrevealatransitionfromexponen-
tialtopower-lawconvergenceinthespecializedphasewhenthedatacovariance
matrixpossessesapower-lawspectrum. Thisworkcontributestothetheoretical
understandingofneuralscalinglawsandprovidesinsightsintooptimizinglearning
performanceinpracticalscenariosinvolvingcomplexdatastructures.
1 INTRODUCTION
Recentempiricalstudieshaverevealedthattheperformanceofstate-of-the-artdeepneuralnetworks,
trained on large-scale real-world data, can be predicted by simple phenomenological functions
Hestnessetal.(2017);Kaplanetal.(2020);Porianetal.(2024). Specifically,thenetwork’serror
decreasesinapower-lawfashionwithrespecttothenumberoftrainingexamples,modelsize,or
trainingtime,spanningmanyordersofmagnitude. Thisobservedphenomenonisencapsulatedby
neuralscalinglaws,whichdescribehowneuralnetworkperformancevariesaskeyscalingfactors
change. Interestingly,theperformanceimprovementduetoonescalingfactorisoftenlimitedby
another,suggestingthepresenceofbottleneckeffects. Understandingthesescalinglawstheoretically
iscrucialforpracticalapplicationssuchasoptimizingarchitecturaldesignandselectingappropriate
hyperparameters. However,thefundamentalreasonsbehindtheemergenceofneuralscalinglaws
have mainly been explored in the context of linear models Lin et al. (2024) and random feature
modelsBahrietal.(2024),andamorecomprehensivetheoreticalframeworkisstillabsent.
ScopeofStudy. Inthiswork,weemploytechniquesfromstatisticalmechanicstoanalyzeone-pass
stochasticgradientdescentwithinastudent-teacherframework. Bothnetworksaretwo-layerneural
networks: thestudenthasK hiddenneurons, theteacherhasM, andwetrainonlythestudent’s
1
4202
tcO
11
]LM.tats[
1v50090.0142:viXraPreprint
10 1 L=1 L=16
L=4 L=64
10 2
10 3
10 4
500 1500 2500 3500
Figure1: Generalizationerrorϵ asafunctionofαforK = M = 2,η = 0.1,β = 1,σ = 0.01,
g J
andN =1024,withvaryingnumbersofdistincteigenvaluesL. AsLincreases,theplateaulength
decreasesuntilitdisappears. Additionally, withincreasingL,theconvergenceoftheasymptotic
generalizationerrorslowsdown,transitioningfromexponentialtopower-lawscalingintheearly
asymptoticphase.
input-to-hiddenweights,realizingaso-calledcommitteemachineBiehl&Schwarze(1995). We
beginouranalysiswithlinearactivationfunctionsforbothnetworksandthenextendittonon-linear
activation functions, focusing on the feature learning regime where the student weights undergo
significantchangesduringtraining. Ourprimaryfocusisonanalyzingthegeneralizationerrorϵ by
g
introducingorderparametersthatelucidatetherelationshipsbetweenthestudentandteacherweights.
Despitethediversityofdatasetsacrossvariouslearningdomains,acriticalcommonalityisthattheir
feature-featurecovariancematricesoftenexhibitpower-lawspectraMaloneyetal.(2022). Tomodel
realisticdata,wethereforeutilizeGaussian-distributedinputswithcovariancematricesthatdisplay
power-lawspectra.
WhiteNoisevs.Power-LawSpectra. Thestudent-teachersetupwithisotropicinputdatahasbeen
extensivelystudiedandiswell-understoodintheliteratureSaad&Solla(1995). Intherealizable
scenariowhereK =M,thegeneralizationerrortypicallyundergoesthreedistinctphases: arapid
learningphase,aplateauphase,andanexponentiallydecayingphasewithtimeα. Introducinga
power-lawspectruminthecovariancematrixleadstoobservablechangesintheplateau’sheightand
duration,alongwithaslowdownintheconvergencetowardszerogeneralizationerror. Notably,as
thenumberofdistincteigenvaluesLinthedatacovariancespectrumincreases,theplateaushortens,
and the convergence to perfect learning becomes progressively slower, as depicted in Figure 1.
Thisobservationindicatesapotentialtransitionfromexponentialdecaytopower-lawscalinginthe
generalizationerrorovertime. Identifyingandunderstandingthistransitionisacriticalfocusofour
investigation. Ourmaincontributionsare:
• Forlinearactivationfunctions,wederiveanexactanalyticalexpressionforthegeneralization
errorasafunctionoftrainingtimeαandthepower-lawexponentβofthecovariancematrix.
We characterize different learning regimes for the generalization error and analyze the
conditionsunderwhichpower-lawscalingemerges.
• Inaddition,forlinearactivationfunctions,wedemonstrateascalinglawinthenumberof
trainablestudentparameters,effectivelyreducingtheinputdimensionofthenetwork. This
power-lawisdifferentfromthepower-lawcharacterizingthetrainingtimedependence.
• Wederiveananalyticalformulaforthedependenceoftheplateaulengthonthenumberof
distincteigenvaluesandthepower-lawexponentβofthecovariancematrix,illustratinghow
theseplateausbehaveunderdifferentconfigurations.
• Weinvestigatetheasymptoticlearningregimefornon-linearactivationfunctionsandfind
that,intherealizablecasewithM = K,theconvergencetoperfectlearningshiftsfrom
anexponentialtoapower-lawregimewhenthedatacovariancematrixhasapower-law
spectrum.
2
gPreprint
2 RELATED WORK
TheoryofNeuralScalingLawsforLinearActivationFunctions. Previousstudiesonneural
scaling laws have primarily focused on random feature models or linear (ridge) regression with
power-law features Weiet al. (2022). In particular, Maloney et al. (2022); Paquette et al. (2024)
andAtanasovetal.(2024)analyzedrandomfeaturemodelsforlinearfeaturesandridgeregression,
employing techniques from random matrix theory. Bahri et al. (2024) examined random feature
models for kernel ridge regression within a student-teacher framework using techniques from
statistical mechanics. In their analysis, either the number of parameters or the training dataset
sizewasconsideredinfinite,leadingtoscalinglawsinthetestlosswithrespecttotheremaining
finite quantity. Bordelon et al. (2024b) studied random feature models with randomly projected
features and momentum, trained using gradient flow. Using a dynamical mean field theory
approach,theyderiveda"bottleneckscaling"whereonlyoneoftime,datasetsize,ormodelsize
isfinitewhiletheothertwoquantitiesapproachinfinity. Additionally,Hutter(2021)investigated
abinarytoymodelandfoundnon-trivialscalinglawswithrespecttothenumberoftrainingexamples.
Bordelon&Pehlevan(2022)studiedone-passstochasticgradientdescentforrandomfeaturemodels,
derivingascalinglawforthetesterrorovertimeinthesmalllearningrateregime. Similarly,Lin
etal.(2024)investigatedinfinite-dimensionallinearregressionunderone-passstochasticgradient
descent,providinginsightsthroughastatisticallearningtheoryframework. Theyderivedupperand
lowerboundsforthetesterror,demonstratingscalinglawswithrespecttothenumberofparameters
anddatasetsizeunderdifferentscalingexponents.
Building upon the work of Lin et al. (2024) and Bordelon & Pehlevan (2022), we also consider
one-passstochasticgradientdescent. However,ourstudyextendstobothlinearandnon-linearneural
networkswherewetraintheweightsusedinthepre-activations(i.e.,featurelearning),andusefixed
hidden-to-outputconnections. UnlikeBordelon&Pehlevan(2022),weextendtheanalysisforlinear
activationfunctionstogenerallearningratesandvaryingnumbersofinputneurons. Additionally,
wederiveupperandlowerboundsforthetimeintervaloverwhichthegeneralizationerrorexhibits
power-lawbehavior. Asignificantdifferencefrompreviousworksisourfocusonfeaturelearning,
where all pre-activation weights are trainable. In this regime, certain groups of student weights,
organizedbystudentvectors,begintoimitateteachervectorsduringthelatetrainingphase,leading
tospecialization.
Other theoretical studies have explored different aspects of scaling laws. Some have focused on
learnablenetworkskillsorabilitiesthatdrivethedecayofthelossArora&Goyal(2023);Michaud
et al. (2023); Caballero et al. (2023); Nam et al. (2024). Others have compared the influence of
syntheticdatawithrealdataJainetal.(2024)orinvestigatedmodelcollapsephenomenaDohmatob
etal.(2024b;a). FurtherworksstudyingcorrelatedandrealisticinputdataareGoldtetal.(2020);
Loureiroetal.(2021);Cagnettaetal.(2024);Cagnetta&Wyart(2024).
StatisticalMechanicsApproach. Analyticalstudiesusingthestatisticalmechanicsframeworkfor
onlinelearninghavetraditionallyfocusedonuncorrelatedinputdataorwhitenoise. Saad&Solla
(1995)firstintroduceddifferentialequationsfortwo-layerneuralnetworkstrainedviastochastic
gradient descent on such data. Building upon this, Yoshida & Okada (2019) recently expanded
thesemodelstoincludeGaussian-correlatedinputpatterns,derivingasetofclosed-formdifferential
equations. Their research primarily involved numerically solving these equations for covariance
matriceswithuptotwodistincteigenvalues,exploringhowthemagnitudesoftheeigenvaluesaffect
theplateau’slengthandheight. Inourstudy,weextendthishierarchyofdifferentialequationsto
investigatethedynamicsoforderparametersfordatacovariancematriceswithpower-lawspectra,
consideringLdistincteigenvalues.
3 SETUP
Dataset. Weconsiderastudentnetworktrainedonoutputsgeneratedbyateachernetwork,usingp
inputexamplesξµ ∈RN,whereµ=1,...,p. EachinputξµisdrawnfromacorrelatedGaussian
distributionN(0,Σ),withcovariancematrixΣ∈RN×N. Althoughthecovariancematrixgenerally
hasN eigenvalues,weassumeithasonlyLdistincteigenvalues,eachoccurringwithmultiplicity
3Preprint
N/L,where1≤L≤N andN/Lisaninteger. Theeigenvaluesfollowapower-lawdistribution:
λ
λ = + (1)
l l1+β
whereβ >0isthepower-lawexponentofthecovariancematrix,λ =λ isthelargesteigenvalue,
+ 1
andl∈{1,...,L}. Wechooseλ
suchthatthetotalvariancesatisfies(cid:80)L (cid:0)Nλ (cid:1)
=N,ensuring
+ l=1 L l
thatthepre-activationsofthehiddenneuronsremainoforderoneinoursetup.
Student-TeacherSetup. Thestudentisasoftcommitteemachine–atwo-layerneuralnetwork
withaninputlayerofN neurons,ahiddenlayerofK neurons,andanoutputlayerwithasingle
neuron. Inthestatisticalmechanicsframework,werepresenttheweightsbetweentheinputlayer
andthehiddenlayerasvectors. Specifically,theconnectionbetweentheinputlayerandthei-th
hiddenneuronisrepresentedbythestudentvectorJ ∈RN. Thus,wehaveK studentvectorsJ ,
i i
eachrepresentingtheweightsconnectingtheentireinputlayertooneofthehiddenneurons. The
pre-activationreceivedbythei-thhiddenneuronisdefinedasx
i
= √1 ξµ·J i. Theoveralloutput
N
ofthestudentisgivenby
√
K
M (cid:88)
σ(J,ξ)= g(x ), (2)
K i
i=1
√
where g(x ) is the activation function, and the output weights are set to M/K. In this setup,
i
wetrainthestudentvectorsJ andkeepthehidden-to-outputweightsfixed. Theteachernetwork
i
has the same architecture but with M hidden neurons, and its weights are characterized by the
teacher vectors B
n
∈ RN. The pre-activations for the teacher are y
n
= √1 ξµ · B n, and its
N
overalloutputisζ(B,ξ)=(cid:80)M
g(y ). Weinitializethestudentandteachervectorsfromnormal
n=1 n
distributions: J ∼N(0,σ2)andB ∼N(0,1),whereσ2 isthevarianceofthestudentweights
ia J na J
anda ∈ {1,...,N}. Toquantifythediscrepancybetweenthestudent’soutputandtheteacher’s
output,weusethesquaredlossfunctionϵ= 1[ζ−σ]2. Ourmainfocusisthegeneralizationerror
2
ϵ =⟨ϵ(ξ)⟩ ,whichmeasuresthetypicalerrorofthestudentonnewinputs. Throughoutthiswork,
g ξ
(cid:16) (cid:17)
weconsidertheerrorfunctionasournon-linearactivationfunctiong(x)=erf √x .
2
TransitionfromMicroscopictoMacroscopicFormalism. Ratherthancomputingexpectation
values directly over the input distribution, we consider higher-order pre-activations defined as
√ √
x(l) =ξµ(Σ)lJ / N andy(l) =ξµ(Σ)lB / N,assuggestedinYoshida&Okada(2019).Here,
i i n n
(Σ)ldenotesthel-thpowerofthecovariancematrix,andwedefine(Σ)0 =I. Inthethermodynamic
limitN →∞,thesehigher-orderpre-activationsbecomeGaussianrandomvariableswithzeromean
and covariances given by: (cid:68) x(l)x(l)(cid:69) = Ji(Σ)lJj := Q(l), (cid:68) x(l)y(l)(cid:69) = Ji(Σ)lBn := R(l) and
i j N ij i n N in
(cid:68) (cid:69)
y(l)y(l) = Bn(Σ)lBm :=T(l). Thisproperty,wherethepre-activationsbecomeGaussianinthe
n m N nm
thermodynamiclimit,isknownastheGaussianequivalencepropertyGoldtetal.(2020;2022). The
higher-orderorderparametersQ(l),R(l),andT(l) capturetherelationshipsbetweenthestudentand
ij in nm
teacherweightsatdifferentlevels. Byexpressingthegeneralizationerrorasafunctionoftheseorder
parameters,wetransitionfromamicroscopicview–focusedonindividualweightcomponents–toa
macroscopicperspectivethatcentersontherelationshipsbetweenthestudentandteachervectors
withoutdetailingtheirexactcomponents. Understandingthedynamicsoftheseorderparameters
allowsustoeffectivelyanalyzethebehaviorofthegeneralizationerror.
Dynamical Equations. During the learning process, we update the student vectors J using
i
stochasticgradientdescentaftereachpresentationofaninputexample:
Jµ+1−Jµ =−η∇ ϵ(Jµ,ξµ), (3)
i i Ji i
whereηisthelearningrate.Inthethermodynamiclimit,asp,N →∞whilemaintainingafiniteratio
α=p/N,Yoshida&Okada(2019)derivedasetofhierarchicaldifferentialequationsdescribingthe
dynamicsoftheorderparametersunderstochasticgradientdescent. Applyingthesefindingstoour
4Preprint
specificsetup,weobtainthefollowingdifferentialequations:
dR(l) η (cid:16) (cid:17)
= F R(1),Q(1),R(l+1),Q(l+1)
dα K 1
dQ(l) η (cid:16) (cid:17) η2 (cid:16) (cid:17)
= F R(1),Q(1),R(l+1),Q(l+1) + ν F R(1),Q(1) , (4)
dα K 2 K2 l+1 3
whereν = 1 (cid:80)N λl. ThefunctionsF ,F ,andF aredefinedinAppendixA.Thetransition
l N k=1 k 1 2 3
fromEq.(3)toEq.(4)representsashiftfromdiscrete-timeupdatesindexedbyµtoacontinuous-time
frameworkwhereαservesasacontinuoustimevariable.
Atthisstage,thedifferentialequationsarenotclosedbecausetheleft-handsidesofEqs.(4)involve
derivativesofthel-thorderparameters,whiletheright-handsidesdependonthenexthigher-order
parametersR(l+1)andQ(l+1). Toclosethesystemofequations,weemploytheCayley–Hamilton
theorem,whichstatesthateverysquarematrixsatisfiesitsowncharacteristicequation. Specifically,
forthecovariancematrixΣ,thecharacteristicpolynomialisgivenbyP(Σ):=(cid:81)L
(Σ−λ I)=
k=1 k
(cid:80)L c Σk =0,wherec arethecoefficientsofthepolynomial,andλ arethedistincteigenvalues
k=0 k k k
ofΣ. Consequently,wecanexpressthehighest-orderorderparametersintermsoflower-orderones:
R(L) = −(cid:80)L−1c R(l), Q(L) = −(cid:80)L−1c Q(l), and T(L) = −(cid:80)L−1c T(l). By substituting
l=0 l l=0 l l=0 l
theseexpressionsbackintothedifferentialequations,weclosethesystem,resultingin(KM+K2)×
Lcoupleddifferentialequations. Furtherdetailsonthederivationofthesedifferentialequationsare
providedinAppendixA.
4 LINEAR ACTIVATION FUNCTION
4.1 SOLUTIONOFORDERPARAMETERS
Forthelinearactivationfunction,asignificantsimplificationoccurs:thegeneralizationerrorbecomes
independentofthesizesofthestudentandteachernetworks. Specifically,wecanreplacethestudent
andteachervectorswiththeirweightedsums,effectivelyactingassingleresultantvectors.Bydefining
B˜ = √1
M
(cid:80)M
n
B n,thestudenteffectivelylearnsthiscombinedteachervector. Consequently,we
focus on the case where K = M = 1. In this scenario, the generalization error simplifies to
ϵ =
1(cid:0) Q(1)−2R(1)+T(1)(cid:1)
,whichdependsonlyonthefirst-orderorderparameters. Therefore,
g 2
our main interest lies in solving the dynamics of these first-order parameters. Since we have
only one student and one teacher vector, we represent the order parameters in vector form R =
(cid:0) R(0),R(1),...,R(L−1)(cid:1)⊤
, Q =
(cid:0) Q(0),Q(1),...,Q(L−1)(cid:1)⊤
and T =
(cid:0) T(0),T(1),...,T(L−1)(cid:1)⊤
.
Usingthissetupandnotation,alongwithEq.(4),wederivethefollowingdynamicalequation:
(cid:18) (cid:19) (cid:18) (cid:19)(cid:18) (cid:19) (cid:18) (cid:19)
d R A 0 R u
=η 1 L×L +η , (5)
dα Q −2A 1−2ηU 2A 1+ηU Q ηu
where u = (cid:0) T(1),T(2),...,T(L)(cid:1)⊤ , U = ue⊤, and e = (0,1,0,...,0)⊤. The matrix A ∈
2 2 1
RL×LisdefinedinAppendixB.1.
FromEq.(5),weobservethatthedifferentialequationsgoverningthehigher-orderstudent-teacher
orderparametersRcanbesolvedindependentlyofthestudent-studentparametersQ. Therefore,to
understandthedynamicalbehaviorofR(α),weneedtodeterminetheeigenvaluesofA ,andforthe
1
asymptoticsolution,werequireitsinverse. Additionally,thesolutionforthestudent-studentorder
parametersQ(α)dependsonR(α)andthespectrumofA +ηU. InAppendixB.1,wederivean
1
expressionforthegeneralizationerroraveragedovertheteacherandinitialstudententriesB and
a
J0:
a
⟨ϵ ⟩ =
(cid:0) 1+σ J2(cid:1) (cid:88)L
b λ˜ exp(−2ηλ˜ α), (6)
g J a0,Ba 2L k k k
k=1
whereλ˜ aretheeigenvaluesofA +ηU,b = (cid:80)L (cid:0) W−1(cid:1) T(l),andW containstheeigen-
k 1 k l=1 kl
vectorsofA +ηU.
1
5Preprint
10 2
10 2
small limit 10 3 simulation g
10 3 no approximation g,int
100 101 102 100 101 102 103
Figure2: Generatlizationerrorϵ forlinearactivationfunction. Left: ϵ evaluatedusingEq.(8)
g g
(blue) and Eq. (6) (orange) for N = 128, K = M = 1, σ2 = 1, β = 1, and η = 1. Right: ϵ
J g
evaluatedusingEq.(9)(dashedorange)comparedtosimulationsofastudent-teachersetupaveraged
over5randominitializations(solidblue),withN =L=256,β =0.75,η =0.1,andσ =0.01.
J
This equation generally requires numerical evaluation. Although U is a rank-1 matrix, standard
perturbationmethodsarenotapplicabletofindtheeigenvaluesoftheshiftedmatrixA +ηU because
1
U mayhavealargeeigenvalue,makingitunsuitableasasmallperturbation. However,intheregime
ofsmalllearningratesη,whereweretaintermsuptoO(η)inEq.(5),wecandeterminethespectraof
allinvolvedmatricesanalyticallyandsolvethedifferentialequations. Thesolutionsforthefirst-order
orderparametersarethengivenby
L
1 (cid:88)
⟨R(1)⟩ =1− λ exp(−ηλ α),
J a0,Ba L k k
k
1+σ2 (cid:88)L 2 (cid:88)L
⟨Q(1)⟩ =1+ J λ exp(−2ηλ α)− λ exp(−ηλ α) , (7)
J a0,Ba L k k L k k
k k
andthegeneralizationerrorbecomes
1+σ2 (cid:88)L
⟨ϵ ⟩ = J λ exp(−2ηλ α). (8)
g J a0,Ba η→0 2L k k
k=1
Here,λ arethedistincteigenvaluesofthedatacovariancematrixasdefinedinEq.(1). Figure2
k
comparesthegeneralizationerrorobtainedfromtheexactsolutioninEq.(6)withthesmalllearning
rate approximation in Eq. (8). We observe that the generalization error without approximations
consistentlyliesabovethesmalllearningratesolution. Thisdiscrepancyarisesfromthefluctuations
inthestochasticgradientdescenttrajectory,whichbecomemorepronouncedatlargerlearningrates.
4.2 SCALINGWITHTIME
Toevaluatethesumontheright-handsideofEq.(8),weemploytheEuler-Maclaurinapproximation,
whichallowsustoapproximatethesumbyanintegral. InAppendixB.2,wederivethefollowing
approximationforthegeneralizationerror:
⟨ϵ (α)⟩ ≈ λ
1+σ J2 (2ηλ +α)− 1+β β (cid:34) Γ(cid:18) β ,2ηλ +α(cid:19) −Γ(cid:18) β
,2ηλ
α(cid:19)(cid:35)
, (9)
g J a0,Ba η→0 + 2L 1+β 1+β Lβ+1 1+β +
whereΓ(s,x)istheincompletegammafunction. Thisexpressionrevealsthatthegeneralizationerror
(cid:16) (cid:17)1+β
exhibitsapower-lawscalingwithinthetimewindow 1 <α< 1 Γ 2β+1 β L1+β. Inthis
2ηλ+ 2ηλ+ 1+β
regime,thegeneralizationerrorscalesasϵ g(α)∝α− 1+β β,aligningwiththeresultsofBordelon&
Pehlevan(2022)andBahrietal.(2024)fortherandomfeaturemodel. TherightpanelofFigure2
illustrates our analytical prediction from Eq. (9), alongside the generalization error observed in
a student neural network trained on Gaussian input data with a power-law spectrum. Additional
numericalanalysesareprovidedinAppendixB.2.
6
g gPreprint
10 1
10 2
Nl=50
10 3 Nl=50 Nl=150 Nl=100
Nl=100 Nl=200 Nl=200
10 2
100 101 102 103 104 100 101 102 103
Figure3: Generalizationerrorϵ fordifferenttrainableinputdimensionsN ofthestudentnetwork.
g l
Left: ϵ asafunctionofαforvariousN ,withL=N =256,K =M =1,σ =0.01,η =0.05,
g l J
andβ =1. Thestudentnetworkistrainedonsyntheticdataandtheteacher’soutputs. Right: ϵ asa
g
functionofα,withL=N =1024,K =M =1,σ =0.01,andη =0.05. Thestudentnetworkis
J
trainedontheCIFAR-5mdatasetNakkiranetal.(2021)usingtheteacher’soutputs. Weestimatethe
scalingexponentβ ≈0.3forthisdataset. Forthetheoreticalpredictions,theempiricaldataspectrum
isusedtoevaluateEq.(11). Bothplotscomparethesimulationresults(solidcurves)tothetheoretical
predictionfromEq.(11)(blackdashedlines). Forbothplots,thegeneralizationerrorisaveraged
over50randominitializationsofthestudentandteachervectors.
4.3 FEATURESCALING
WefirstconsideradiagonalcovariancematrixΣ,suchthateachentryofthestudentvectordirectly
correspondstoaneigenvalue(seeAppendixB.3). Welatergeneralizetoanon-diagonalcovariance
matrixandfindthatthesamescalinglawisobtained.
Students typically learn directions associated with the largest eigenvalues of the data covariance
matrixmorerapidlyAdvanietal.(2020). Tomodelthisbehavior,weassumethestudentcanlearnat
mostN ≤L=N distincteigenvaluesofthedatacovariancematrix. Consequently,onlythefirstN
l l
entriesofthestudentvectoraretrainable,whiletheremainingN −N entriesremainfixedattheir
l
initialrandomvalues. Ourobjectiveistoexaminehowthegeneralizationerrorscalesasthestudent
exploresmoreeigendirectionsofthedatacovariancematrix. Figure3displaysthegeneralization
errorasafunctionofαforvariousvaluesofN . Weobservethatthegeneralizationerrorapproaches
l
alimitingasymptoticvalueϵ . InAppendixB.3,wederivethefollowingexpressionforthe
g,asymp
expectedgeneralizationerrorinthismodel:
1+σ2 (cid:34) (cid:88)Nl (cid:88)L (cid:35)
⟨ϵ ⟩ = J λ exp(−2ηλ α)+ λ . (10)
g J k0,Bk η→0 2L k k k
k=1 k=Nl+1
UsingtheEuler-Maclaurinformula,weapproximatethesumsbyintegralsandfind:
(cid:32) (cid:33)
1+σ2 λ 1 1
⟨ϵ (N ,α)⟩ ≈ + − +⟨ϵ (α)⟩ . (11)
g l J a0,Ba η→0 2 βL Nβ Lβ g J a0,Ba
l
(cid:16) (cid:17)
Fromthis,wederivetheasymptoticgeneralizationerrorasϵ ≈ 1+σ2λ+ 1 − 1 . Thus,
g,asymp 2 βL Nβ Lβ
l
whenLβ > Nβ,wefindapower-lawscalingoftheasymptoticgeneralizationerrorwithrespect
l
tothenumberoflearnedfeatures: ϵ ∼ 1 . Asimilarscalingresultforfeaturescalingis
g,asymp Nβ
l
presentedinMaloneyetal.(2022)forrandomfeaturemodels. However,ourscalingexponentforthe
datasetsize(parameterizedbyα)differsfromthatforthenumberoffeatures. InAppendixB.3,we
analyzethestudentnetworktrainedwithanon-diagonaldatacovariancematrix. Inthissetting,we
findthesamepower-lawexponentϵ ∼ 1 .
g,asymp Nβ
l
7
g gPreprint
0.20
0.8 0.80.1445
0.18 0.1440 0.0071
0.6 0.6
500 1000 500 1000 0.4 0.4 0.0070
0.00711
0.2 0.2
0.0069
0.0 0.0 500 1000 1500
0 2000 4000 6000 0 2000 4000 6000 1000 2000 3000
Figure4: Symmetricplateauforanon-linearactivationfunction. Leftandcenter: Plateaubehavior
of the order parameters for L = 10, N = 7000, σ = 0.01, η = 0.1, and M = K = 4, using
J
one random initialization of the student and teacher vectors. We solve the differential equations
inthesmalllearningrateregime,retainingtermsuptoO(η). Theinsetsdisplaythehigher-order
orderparametersattheplateau. Forthestudent-teacherorderparameters,weobserveM distinct
plateauheights,whilethestudent-studentorderparametersexhibitasingleplateauheightwithminor
statisticaldeviationsinthematrixentriesQ(l). Thedashedhorizontallinesintheinsetscorrespondto
ij
theplateauheightspredictedbyEq.(13). Right: Correspondinggeneralizationerrorϵ forthesame
g
setup. TheverticaldashedlinesindicatetheestimatedplateaulengthbasedonEqs.(12)and(15).
5 NON-LINEAR ACTIVATION FUNCTION
5.1 PLATEAU
AsdiscussedintheintroductionandillustratedinFig.1,boththelengthandheightoftheplateauare
influencedbythenumberofdistincteigenvaluesinthedatacovariancematrix. Specifically,asthe
numberofdistincteigenvaluesincreases,theplateaubecomesshorterandcaneventuallydisappear.
Inthissection,wepresentourfindingsthatexplaintheunderlyingcausesofthisbehaviorforthe
casewhereK =M. Biehletal.(1996)derivedaformulatoestimatetheplateaulengthα forasoft
P
committeemachinetrainedviastochasticgradientdescentwithrandomlyinitializedstudentvectors.
Weadopttheirheuristicallyderivedformulaforoursetup,whichtakestheform
(cid:18) (cid:19)
α −α =τ D−
1 ln(cid:0) σ2(cid:1)
+
1
ln(N) , (12)
P 0 esc 2 J 2
whereDisaconstantoforderO(1)thatdependsonthevariancesatinitializationandduringthe
plateauphase,α isanarbitrarystartingpointontheplateau,andτ representstheescapetime
0 esc
fromtheplateau. Ourgoalistoshowhowtheescapetimeτ ismodifiedwhenthedatasethasa
esc
power-lawspectrum.
However,thereisnotasingleplateauorplateaulength. AsshownnumericallybyBiehletal.(1996),
multipleplateauscanexist,andtheirnumberdependsonfactorssuchasthenetworksizesK and
M, as well as hyperparameters like the learning rate η. To investigate how the plateau lengths
dependonthenumberofdistincteigenvalues,wefocusonthedifferentialequationsfortheerror
functionactivationuptoorderO(η). Thiscorrespondstothesmalllearningrateregime,although
theassociatedplateaubehaviorcanoccurforintermediatelearningratesaswell.
5.1.1 PLATEAUHEIGHT
In contrast to isotropic input correlations, the higher-order order parameters in our setup are no
longerself-averaging,resultinginmorecomplexlearningdynamics. Fortheteacher-teacherorder
(cid:68) (cid:69) (cid:16) (cid:17)
parameters,wefindtheexpectationvalue T(l) =δ 1 Tr(Σl)andthevarianceVar T(l) =
nm nmN nm
(δnm+1)(cid:80)Nλ2.
Therefore,forboththediagonalandoff-diagonaltermsofthehigher-orderteacher-
N2 k k
teacherorderparameters,thevariancedoesnotdecreasewithincreasinginputdimensionN when
l > 0. Consequently,theplateauheightandlengthcanfluctuatebetweendifferentinitializations,
eveninthesmalllearningrateregime. Figure4displaysthefirst-orderorderparametersasfunctions
ofα. Forthestudent-teacherorderparameters,weobserveM distinctplateauheights,whilethe
student-studentorderparametersexhibitasingleplateauheight. InAppendixC.2,weshowthatthese
uniqueplateauheightsaredeterminedbythesumofoff-diagonalelementsd(l) = (cid:80)M−1 T(l)
n m,m̸=n nm
8
)1(R ni )1(Q ji gPreprint
0.012
M=6 M=32 0.008 L=25 L=250
0.010 M=8 M=40 L=50 L=500
M=10
L=100
0.007
0.008
0.006 0.006
0.004
0.005
0.002
100 4000 8000 12000 16000 500 15000 30000 50000 70000
Figure5: Plateaubehaviorofthegeneralizationerrorϵ obtainedbysimulationsforonerandom
g
initializationofstudentandteachervectorswithK =M,η =0.01,σ =10−6andβ =0.25. Left:
J
ϵ fordifferentstudent-teachersizesforL=N =512. Right: ϵ fordifferentnumbersofdistinct
g g
eigenvaluesLandforK =M =6andN =500.
foreachrownoftheteacher-teachermatrix. Tosimplifytheanalysis,weassumethatalldiagonal
elementsareequaltoT(l),andalloff-diagonalelementsaregivenbyT(l) = 1 D(l),whereD(l)
nm M−1
representstheaveragesumofoff-diagonalentries. Thisapproximationcapturesthegeneralbehavior
oftheplateaus. ByconsideringthestationarysolutionstoEqs.(4),wefindthefixedpointsforl=1
1 1
R∗(1)
= ,
Q∗(1)
= . (13)
(cid:114) M (cid:16) MT(1) (cid:0) 1+ 1 (cid:1) −1(cid:17) T(M 1)+T( D1) (1) (cid:0) 1+ T1 (1)(cid:1) −1
T(1)+D(1) T(1)+D(1) T(1)
Expressionsforthefixedpointsofhigher-orderorderparametersareprovidedinAppendixC.2.
5.1.2 ESCAPEFROMTHEPLATEAU
Toescapefromtheplateau,thesymmetryineachorderloftheorderparametersmustbebroken. To
modelthissymmetrybreaking,weintroduceparametersS(l)andC(l),whichindicatetheonsetof
specializationforthestudent-teacherandstudent-studentorderparameters,respectively. Specifically,
weusetheparametrizationR(l) = R(l)δ +S(l)(1−δ )andQ(l) = Q(l)δ +C(l)(1−δ ).
im im im ij ij ij
To study the onset of specialization, we introduce small perturbation parameters r(l), s(l), q(l),
andc(l) torepresentdeviationsfromtheplateauvalues: R(l) = R∗(l) +r(l), S(l) = S∗(l) +s(l),
Q(l) = Q∗(l) +q(l), andC(l) = C∗(l) +c(l), whereS∗(l) = R∗(l) andC∗(l) = Q∗(l). Therefore,
insteadofanalyzingthedynamicsoftheorderparametersdirectly,wefocusonthedynamicsofthese
perturbativeparametersandlinearizethedifferentialequationsgiveninEq.(4).
InAppendixC.3,wedemonstratethat,duetothestructureoftheleadingeigenvectorsofthedynamical
system,wecansetc(l) = q(l) = 2T(l) R∗(l)(cid:0) r(l)+(M −1)s(l)(cid:1) ands(l) = −1 r(l). This
T(1)+D(1) (M−1)
allowsustoobtainareduceddynamicaldifferentialequationoftheform
dr
=ηA r, (14)
dα r
wherer =[r(1),r(2),...,r(L)]⊤,andA ∈RL×LisdefinedinAppendixC.3. Aftersolvingthese
r
differentialequations,wefindthattheescapefromtheplateaufollowsϵ∗ g−ϵ
g
∝eτeα sc,wherethe
escapetimeτ isgivenby
esc
π (cid:112) (M −1)T(1)−D(1)+M(cid:0) D(1)+(M +1)T(1)+M(cid:1)3 2
τ = . (15)
esc 2η (cid:16)
T(2)−
D(2)(cid:17)(cid:0) D(1)+T(1)(cid:1)
M−1
ForlargeL,onecanshowthatT(2) ∝L. Therefore,forlargeM andL,theescapetimescalesas
τ ∼ M2 . ThisbehaviorisillustratedinFigure5,wherewetrainastudentnetworkwithsynthetic
esc ηL
inputdata. AdditionalnumericalresultsfortheplateaulengthareprovidedinAppendixC.4.
9
g gPreprint
=0.5 =1.25 10 1
=0.75 =1.5 ODE
=1 10 3 Eq.(17)
10 2 10 2
104 10 4
10 3
0 500 1000
10 3
10 4
0 10000 20000 30000 40000 50000 0 500 1000 1500 2000 2500 3000
Figure6: Scalingbehaviorofthegeneralizationerrorϵ intheasymptoticregimeforanon-linear
g
activation function. Left: ϵ as a function of α for K = M = 40, η = 0.01, σ = 10−6 and
g J
L=N =512forsimulationsaveragedover10differentinitializations. Inset:comparisontoapower
lawwithexponent−β/(1+β). Right: ϵ obtainedfromequationEq. (17)(orange)incomparison
g
tothesolutionofdifferentialequationsofO(η)(blue)forK =M =2,η =0.25,β =1andL=9.
5.2 ASYMPTOTICSOLUTION
Inthissubsection,weinvestigatehowthegeneralizationerrorconvergestoitsasymptoticvalue. To
thisend,weconsiderthetypicalteacherconfigurationwhere⟨T(l)⟩=δ T(l),asthisconfiguration
nm nm
effectivelycapturesthescalingbehaviorofthegeneralizationerror. Fortheasymptoticfixedpoints
oftheorderparameters,wefindR∗(l) = T(l)δ andQ∗(l) = T(l)δ . Tomodeltheconvergence
im im ij ij
towardstheasymptoticsolution,weagaindistinguishbetweendiagonalandoff-diagonalentries,
parametrizingtheorderparametersasR(l) =R(l)δ +S(l)(1−δ )andQ(l) =Q(l)δ +C(l)(1−
im im im ij ij
δ ),similartotheplateaucase. Wethenlinearizethedynamicalequationsforsmallperturbations
ij
around the fixed points, setting R(l) = T(l) +r(l), S(l) = T(l) +s(l), Q(l) = T(l) +q(l), and
C(l) =T(l)+c(l),andretaintermsuptoO(η). Thisyieldsthefollowinglinearizedequation
dx
=aA x (16)
dα asym
√
where x = (cid:0) r(i−1),q(i−1),s(i−1),c(i−1)(cid:1)⊤ , a = 2 3, and A ∈ R4L×4L is defined in Ap-
i 3πM asym
pendixC.5. AftersolvingEq.(16),wefindforthegeneralizationerror
ϵ =
1 (cid:88)L g(1)e−a(2+√ 3)λkα(cid:16) 2√
3v(1)
−4√
3v(1)+3(M −1)v(1) −6(M −1)v(1)
(cid:17)
g 6π k k,2L+2 k,2 k,3L+2 k,L+2
k=1
√ (cid:16) √ √ (cid:17)
+g(2)e−a(2− 3)λkα 2 3v(2) −4 3v(2)+3(M −1)v(2) −6(M −1)v(2) ,
k k,2L+2 k,2 k,3L+2 k,L+2
(17)
whereλ aretheeigenvaluesofthedatacovariancematrix,v(1)andv(2)aretwogroupsofeigenvec-
k √ k√ k
torscorrespondingtotheeigenvalues(cid:0) 2+ 3(cid:1) λ and(cid:0) 2− 3(cid:1) λ ,andthecoefficientsg(1)and
k k k
g(2) dependontheinitialconditions,asdetailedinAppendixC.5. Theasymptoticconvergenceis
k √
(cid:0) (cid:1)
governedbythesmallergroupofeigenvalues 2− 3 λ .Theweightedsumofexponentialsresults
k
inaslowdownoftheconvergenceofthegeneralizationerror,similartothelinearactivationfunction
case. Figure6illustratesthegeneralizationerrorduringthelatephaseoftrainingfordifferentβ. In
allconfigurations,weobservethepreviouslyderivedscalingϵ
g
∝α− 1+β β,consistentwiththelinear
activationfunction.
6 CONCLUSION
Wehaveprovidedatheoreticalanalysisofneuralscalinglawswithinastudent-teacherframework
using statistical mechanics. By deriving analytical expressions for the generalization error, we
10
g gPreprint
demonstrated how power-law spectra in the data covariance matrix influence learning dynamics
acrossdifferentregimes. Forlinearactivationfunctions,wehaveestablishedtheconditionsunder
whichpower-lawscalingforthegeneralizationerrorwithαemergesandcomputedthepower-law
exponentforthescalingofthegeneralizationerrorwiththenumberofstudentparameters. Fornon-
linearactivations,wepresentedananalyticalformulafortheplateaulength,revealingitsdependence
onthenumberofdistincteigenvaluesandthecovariancematrix’spower-lawexponent. Inaddition,
wefoundthattheconvergencetoperfectlearningtransitionsfromexponentialdecaytopower-law
scalingwhenthedatacovariancematrixexhibitsapower-lawspectrum.Thishighlightsthesignificant
impactofdatacorrelationsonlearningdynamicsandgeneralizationperformance.
Noteadded: Aftercompletionofthiswork,webecameawareofthepreprintBordelonetal.(2024a),
whichalsostudiesneuralscalinglawsinthefeaturelearningregime.
ACKNOWLEDGMENTS
ThisworkwassupportedbytheIMPRSMiSLeipzig.
REFERENCES
Madhu S. Advani, Andrew M. Saxe, and Haim Sompolinsky. High-dimensional dynamics of
generalizationerrorinneuralnetworks. NeuralNetworks,132:428–446,2020. ISSN0893-6080.
doi: https://doi.org/10.1016/j.neunet.2020.08.022. URL https://www.sciencedirect.
com/science/article/pii/S0893608020303117.
SanjeevAroraandAnirudhGoyal. Atheoryforemergenceofcomplexskillsinlanguagemodels.
arXivpreprintarXiv:2307.15936,2023.
AlexanderBAtanasov,JacobAZavatone-Veth,andCengizPehlevan. Scalingandrenormalization
inhigh-dimensionalregression. arXivpreprintarXiv:2405.00592,2024.
YasamanBahri,EthanDyer,JaredKaplan,JaehoonLee,andUtkarshSharma. Explainingneural
scalinglaws. ProceedingsoftheNationalAcademyofSciences,121(27):e2311878121,2024.
MBiehlandHSchwarze. Learningbyon-linegradientdescent. JournalofPhysicsA:Mathematical
andGeneral,28(3):643,feb1995. doi: 10.1088/0305-4470/28/3/018. URLhttps://dx.doi.
org/10.1088/0305-4470/28/3/018.
MichaelBiehl,PeterRiegler,andChristianWöhler. Transientdynamicsofon-linelearningintwo-
layeredneuralnetworks. JournalofPhysicsA:MathematicalandGeneral,29(16):4769,aug1996.
doi: 10.1088/0305-4470/29/16/005. URLhttps://dx.doi.org/10.1088/0305-4470/
29/16/005.
BlakeBordelonandCengizPehlevan. LearningcurvesforSGDonstructuredfeatures. InInterna-
tionalConferenceonLearningRepresentations,2022. URLhttps://openreview.net/
forum?id=WPI2vbkAl3Q.
BlakeBordelon,AlexanderAtanasov,andCengizPehlevan. Howfeaturelearningcanimproveneural
scalinglaws. arXivpreprintarXiv:2409.17858,2024a.
BlakeBordelon,AlexanderAtanasov,andCengizPehlevan. Adynamicalmodelofneuralscaling
laws. InForty-firstInternationalConferenceonMachineLearning,2024b.
Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. Broken neural scaling laws. In
The Eleventh International Conference on Learning Representations, 2023. URL https://
openreview.net/forum?id=sckjveqlCZ.
Francesco Cagnetta and Matthieu Wyart. Towards a theory of how the structure of language is
acquiredbydeepneuralnetworks. arXivpreprintarXiv:2406.00048,2024.
Francesco Cagnetta, Leonardo Petrini, Umberto M. Tomasini, Alessandro Favero, and Matthieu
Wyart. Howdeepneuralnetworkslearncompositionaldata: Therandomhierarchymodel. Phys.
Rev.X,14:031001,Jul2024. doi: 10.1103/PhysRevX.14.031001. URLhttps://link.aps.
org/doi/10.1103/PhysRevX.14.031001.
11Preprint
ElvisDohmatob,YunzhenFeng,andJuliaKempe.Modelcollapsedemystified:Thecaseofregression.
arXivpreprintarXiv:2402.07712,2024a.
ElvisDohmatob,YunzhenFeng,PuYang,FrancoisCharton,andJuliaKempe. Ataleoftails: Model
collapseasachangeofscalinglaws. InForty-firstInternationalConferenceonMachineLearning,
2024b. URLhttps://openreview.net/forum?id=KVvku47shW.
SebastianGoldt,MarcMézard,FlorentKrzakala,andLenkaZdeborová. Modelingtheinfluence
ofdatastructureonlearninginneuralnetworks: Thehiddenmanifoldmodel. Phys.Rev.X,10:
041044, Dec 2020. doi: 10.1103/PhysRevX.10.041044. URL https://link.aps.org/
doi/10.1103/PhysRevX.10.041044.
SebastianGoldt,BrunoLoureiro,GalenReeves,FlorentKrzakala,MarcMézard,andLenkaZde-
borová. Thegaussianequivalenceofgenerativemodelsforlearningwithshallowneuralnetworks.
InMathematicalandScientificMachineLearning,pp.426–471.PMLR,2022.
JoelHestness,SharanNarang,NewshaArdalani,GregoryDiamos,HeewooJun,HassanKianinejad,
Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable,
empirically. arXivpreprintarXiv:1712.00409,2017.
MarcusHutter. Learningcurvetheory. arXivpreprintarXiv:2102.04074,2021.
AyushJain,AndreaMontanari,andErenSasoglu. Scalinglawsforlearningwithrealandsurrogate
data. arXivpreprintarXiv:2402.04376,2024.
JaredKaplan,SamMcCandlish,TomHenighan,TomBBrown,BenjaminChess,RewonChild,Scott
Gray,AlecRadford,JeffreyWu,andDarioAmodei. Scalinglawsforneurallanguagemodels.
arXivpreprintarXiv:2001.08361,2020.
LicongLin,JingfengWu,ShamMKakade,PeterLBartlett,andJasonDLee. Scalinglawsinlinear
regression: Compute,parameters,anddata. arXivpreprintarXiv:2406.08466,2024.
Bruno Loureiro, Cedric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc Mezard,
and Lenka Zdeborová. Learning curves of generic features maps for realistic datasets with a
teacher-studentmodel. AdvancesinNeuralInformationProcessingSystems,34:18137–18151,
2021.
AlexanderMaloney,DanielARoberts,andJamesSully. Asolvablemodelofneuralscalinglaws.
arXivpreprintarXiv:2210.16859,2022.
Eric J Michaud, Ziming Liu, Uzay Girit, and Max Tegmark. The quantization model of neural
scaling. InThirty-seventhConferenceonNeuralInformationProcessingSystems,2023. URL
https://openreview.net/forum?id=3tbTw2ga8K.
PreetumNakkiran,BehnamNeyshabur,andHanieSedghi. Thedeepbootstrapframework: Good
onlinelearnersaregoodofflinegeneralizers. InInternationalConferenceonLearningRepresenta-
tions,2021. URLhttps://openreview.net/forum?id=guetrIHLFGI.
YoonsooNam,NayaraFonseca,SeokHyeongLee,andArdLouis. Anexactlysolvablemodelfor
emergenceandscalinglaws. arXivpreprintarXiv:2404.17563,2024.
ElliotPaquette,CourtneyPaquette,LechaoXiao,andJeffreyPennington. 4+3phasesofcompute-
optimalneuralscalinglaws. arXivpreprintarXiv:2405.15074,2024.
Tomer Porian, Mitchell Wortsman, Jenia Jitsev, Ludwig Schmidt, and Yair Carmon. Resolving
discrepanciesincompute-optimalscalingoflanguagemodels. arXivpreprintarXiv:2406.19146,
2024.
GrantRotskoffandEricVanden-Eijnden. Trainabilityandaccuracyofartificialneuralnetworks:
Aninteractingparticlesystemapproach. CommunicationsonPureandAppliedMathematics,75:
1889–1935,092022. doi: 10.1002/cpa.22074.
David Saad and Sara A. Solla. On-line learning in soft committee machines. Phys. Rev. E, 52:
4225–4243, Oct 1995. doi: 10.1103/PhysRevE.52.4225. URL https://link.aps.org/
doi/10.1103/PhysRevE.52.4225.
12Preprint
AlexanderWei,WeiHu,andJacobSteinhardt. Morethanatoy: Randommatrixmodelspredicthow
real-worldneuralrepresentationsgeneralize. InKamalikaChaudhuri,StefanieJegelka,LeSong,
Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International
ConferenceonMachineLearning,volume162ofProceedingsofMachineLearningResearch,pp.
23549–23588.PMLR,17–23Jul2022. URLhttps://proceedings.mlr.press/v162/
wei22a.html.
RomanWorschechandBerndRosenow. Asymptoticgeneralizationerrorsintheonlinelearningof
randomfeaturemodels. Phys.Rev.Res.,6:L022049,Jun2024. doi: 10.1103/PhysRevResearch.
6.L022049. URL https://link.aps.org/doi/10.1103/PhysRevResearch.6.
L022049.
YukiYoshidaandMasatoOkada.Data-dependenceofplateauphenomenoninlearningwithneuralnet-
work—statisticalmechanicalanalysis. InH.Wallach,H.Larochelle,A.Beygelzimer,F.d'Alché-
Buc,E.Fox,andR.Garnett(eds.),AdvancesinNeuralInformationProcessingSystems,volume32.
Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_
files/paper/2019/file/287e03db1d99e0ec2edb90d079e142f3-Paper.pdf.
13Preprint
A DIFFERENTIAL EQUATIONS
From the stochastic gradient descent given in Eq. (3), one can derive the following differential
equationsinthethermodynamiclimitN →∞(seeYoshida&Okada(2019))
dQ( ijl)
=
η
(cid:34) (cid:88)M
I (x ,x(l),y )−
M
(cid:88)K
I (x ,x(l),x )+
(cid:88)M
I (x ,x(l),y )−
M
(cid:88)K
I (x ,x(l),x
)(cid:35)
dα K 3 i j m K 3 i j k 3 j i m K 3 j i k
m=1 k=1 m=1 k=1
η2 (cid:34) M K (cid:88),K M (cid:88),M 2 K (cid:88),M (cid:35)
+ ν I (x ,x ,x ,x )+ I (x ,x ,y ,y )− I (x ,x ,x ,y )
K2 l+1 K2 4 i j k l 4 i j n m K 4 i j k n
k,l=1 n,m=1 k,n=1
 
dR(l) (cid:88)M (cid:88)K
dαin =η I 3(x i,y n(l+1),y m)− I 3(x i,y n(l+1),x j), (18)
m=1 j=1
with ν = 1 (cid:80)Nλl, I (z ,z ,z ) = ⟨g′(z )z g(z )⟩ and I (z ,z ,z ,z ) =
l N k k 3 1 2 3 1 2 3 4 1 2 3 4
⟨g′(z )g′(z )g(z )g(z )⟩. Inthissetting,thegeneralizationerrorbecomes
1 2 3 4
 
M K K M
ϵ g = 21  (cid:88) I 2(y n(1),y m(1))+ (cid:88) I 2(x( j1),x( i1))−2(cid:88)(cid:88) I 2(x( i1),y n(1)). (19)
m,n=1 i,j=1 i=1n=1
where I (z ,z ) := ⟨g(z )g(z )⟩. Thereby, the I .I and I are integrals over the generalized
2 1 2 1 2 2 3 4
pre-activationsx(l)andy(l). Thereby,z arenormallydistributedvariablesandstandforeitherx(l)
i n i i
ory(l). Therefore,theintegralsI ,I ,andI ,aremultivariateGaussianexpectationvaluesthatare
n 2 3 4
determinedbytheexpectationvaluesandcovariancematrixofthegeneralizedpre-activations. I
2
isatwo-dimensionalGaussianintegral. Forexample,forI (x(1),x(1)),onobtainsthefollowing
2 j j
covariancematrix
(cid:32) (cid:33)
Q(1) Q(1)
C(i,j)= ii ij . (20)
Q(1) Q(1)
ij jj
Theresultingelementsofthecovariancematricesdependonthehigher-orderorderparameters. I is
3
athree-dimensionalGaussianintegral,andanexampleofthecovariancematrixis
 Q(1) Q(1) R(1) 
ii ij in
C(l)(i,j,n)=Q(l+1) Q(2l+1) R(l+1), (21)
 ij jj jn 
R(1) R(l+1) T(1)
in jn nn
forI (x(1),x(l),y(1)). NotethatI dependsonhigher-orderlascomparedtoI andI ,whichonly
3 j j n 3 2 4
dependonthefirst-order. Furthermore,I isafour-dimensionalGaussianintegralanddepends,for
4
example,onthefollowingcovariancematrixforI (x(1),x(1),y(1),y(1))
4 j j n m
 Q(1) Q(1) R(1) R(1)
ii ij in im
Q(1) Q(1) R(1) R(1)
C(i,j,n,m)= ij jj jn jm. (22)
 R(1) R(1) T(1) T(1)
 in jn nn nm
R(1) R(1) T(1) T(1)
im jm nm mm
Thespecificdifferentialequationsforthelinearanderrorfunctionactivationareprovidedintheir
correspondingsubsections.
14Preprint
B LINEAR ACTIVATION
B.1 SOLUTIONOFORDERPARAMETERS
Thelinearactivationfunctionleadstothefollowingteacheroutputζ(B,ξ)= √1 (cid:80)M ξ √Bn. This
M n=1 N
makesitpossibletorewritetheoutputasζ(B,ξ)= √1
M
(cid:80)M
n=1
ξ √B Nn = √ξ
N
√1
M
(cid:80)M n=1B
n
= √ξB N˜ ,
wherewehavedefinedanewteachervectorB˜ = √1
M
(cid:80)M n=1B n. SinceB˜ hasthesamestatistical
propertiesasonerandomteachervectorB itmakesnodifferencewhetherweconsiderthecase
n
M > 1 and define B˜ or M = 1. The same argument also applies to the linear student network.
Therefore,inthefollowing,weanalyzethecaseK =M =1.
ForK =M =1,thegeneralizationerrorbecomes
1(cid:16) (cid:17)
ϵ = Q(1)−2R(1)+T(1) . (23)
g 2
Thedifferentialequationsfortheorderparametersare
dR(l) (cid:104) (cid:105)
=η T(l+1)−R(l+1) ,
dα
dQ(l) (cid:104) (cid:105) (cid:104) (cid:105)
=2η R(l+1)−Q(l+1) +η2T(l+1) T(1)+Q(1)−2R(1) (24)
dα
with0≤l≤L−2andforthelastcomponent
dR(L−1) (cid:34)L (cid:88)−1 (cid:16) (cid:17)(cid:35)
=η c T(k)−R(k) ,
dα k
k
dQ(L−1) (cid:34)L (cid:88)−1 (cid:16) (cid:17)(cid:35) (cid:104) (cid:105)
=2η c R(k)−Q(k) +η2T(L) T(1)+Q(1)−2R(1) . (25)
dα k
k
Thereby,wehaveexploitedthecharacteristicpolynomialfortheorderparametersfortheL-thorder,
e.g. R(L) = −(cid:80)L−1c R(k),withthecoefficientsofthecharacteristicpolynomialc . Thesetof
k=0 k k
coupledlineardifferentialequationsgivenbyEqs. (24)and(25)canbewritteninthefollowingform
(cid:18) (cid:19) (cid:18) (cid:19)(cid:18) (cid:19) (cid:18) (cid:19)
d R A 0 R u
=η 1 L×L +η , (26)
dα Q −2A 1−2ηU 2A 1+ηU Q ηu
whereu=(cid:0) T(1),T(2),...,T(L)(cid:1)⊤ ,U =ue⊤withe =(0,1,0,...,0)⊤and
2 2
0 −1 0 ··· 0 0 
0 0 −1 ... . . . . . . 
 
A 1 =  

0
.
. .
0
.
. .
.0
..
. .. ..
.
−0
1
00   

, U =ue⊤ 2. (27)
 
0 0 ··· 0 0 −1 
c c c ··· c c
0 1 2 L−2 L−1
Therefore,thedifferentialequationsfortheorderparametersare
dR
=ηA R+ηu (28)
dα 1
dQ
=ηA R+ηA Q+η2u (29)
dα 3 4
15Preprint
withA =−2A −2ηU,A =2A +ηU.
3 1 4 1
Thus,wecansolvedifferentialequationsforthestudent-teacherorderparametersindependentfrom
thestudent-studentandfind
R(α)=eηA1αR +eηA1αA−1u−A−1u, (30)
0 1 1
where R are the student-teacher order parameters at initialization and we set R = 0 which
0 0
is achieved in the large N limit and on average. Before inserting this result into the differential
equationforthestudent-studentorderparameters,weevaluateEq. (30). Forthis,weneedtofindthe
eigenvaluesofthematrixA andevaluateA−1u.
1 1
First, westartwiththeeigenvalues. InordertofindthedeterminantofA −λI , weapplythe
1 L
Laplace expansion with respect to the last row of A −λI and evaluate the determinants of
1 2L
L different L−1×L−1 smallermatrices. Theresulting sub-matricesare triangular, andtheir
determinantis,therefore,simplygivenbytheproductofthediagonalentries. Afterapplyingthe
Laplaceexpansion,wefind
L−2
(cid:88)
det(A −λI )= c (−1)L+1+i(−1)L−1−i(−λ)i+(c −λ)(−1)2L(−λ)L−1
1 L i L−1
i=0
L
(cid:88)
= c (−λ)i =0 (31)
i
i=0
withc = 1. Sincec ,...,c arethecoefficientsofthecharacteristicpolynomialforthedistinct
L 0 L
eigenvalues of the data covariance matrix, we now know the roots of Eq. (31). Therefore, the
eigenvalues of A are given by the negative eigenvalues of the distinct eigenvalues of the data
1
covariancematrixλ =−λ for1≤l≤L. ByapplyingthematrixA onapotentialeigenvector
A1,l l 1
A v =λ v ,wefindthefollowingconditionsfortheeigenvectorentries
1 k k k
v =(−1)i−1λi−1v , (32)
k,i k k,1
obtainedbyarecursivemethod. Furthermore, wecanchoosev = 1foralleigenvectors. The
k,1
eigenvectormatrixV,forwhichaneigenvectorgiveseachcolumn,isgivenbythetransposeofthe
Vandermondematrix
 1 1 ··· 1 
λ λ ··· λ
 1 2 L 
 λ2 λ2 ··· λ2 
V = 1 2 L . (33)


. .
.
. .
.
... . .
.


λL−1 λL−1 ··· λL−1
1 2 L
Second,weevaluateallmatrixproductsgiveninEq. (30). Sincealleigenvaluesarestrictlynegative,
thestudent-teacherorderparametersconvergetolim R(α) = −A−1uthatwearegoingto
α→∞ 1
evaluate using the eigenvector matrix. We insert the eigendecomposition A−1 = VΛV−1 into
1
the asymptotic solution with Λ = −δ 1 and find for the entries of the student-teacher order
parameters
kj ljλl
L L L
lim R
=−(cid:88)
V
(cid:88)
Λ
(cid:88)(cid:0) V−1(cid:1)
u
α→∞ i ij 2,jk kl l
j k l
L L
=−(cid:88)
V Λ
(cid:88)(cid:0) V−1(cid:1)
u
ij 2,jj jl l
j l
L L
=−(cid:88) λ(i−1) 1 (cid:88)(cid:0) V−1(cid:1) u
j −λ jl l
j
j l
L L
=(cid:88) λ(i−2)(cid:88)(cid:0) V−1(cid:1)
u
j jl l
j l
(34)
16Preprint
andfurther,evaluate
L L
(cid:88)(cid:0) V−1(cid:1) u =(cid:88)(cid:0) V−1(cid:1) T(l)
jl l jl
l l=1
L L
=(cid:88)(cid:0) V−1(cid:1) 1 (cid:88) λl
jl L a
l=1 a
L L
= 1 (cid:88) λ (cid:88)(cid:0) V−1(cid:1) λl−1
L a jl a
a l=1
L L
=
1 (cid:88)
λ
(cid:88)(cid:0) V−1(cid:1)
V
L a jl l,a
a l=1
1
= λ
L j
(35)
wherewehaveused(cid:80)L (cid:0) V−1(cid:1)
V =δ obtainedbythedefinitionoftheproductbetweena
l=1 jl l,a j,a
matrixwithitsinverse. Thus,wefind
L
lim R = 1 (cid:88) λ(i−1) =T(i−1). (36)
α→∞ i L j
j
NotethatR =R(i−1)andthereforelim R(i) =T(i).
i α→∞
Next,wewanttoevaluateeA1αA−1uanddefine
1
L L L L
F
=(cid:88)
V
(cid:88)
exp(−ηλ α)δ
(cid:88)
Λ
(cid:88)(cid:0) V−1(cid:1)
u
i ij j jk kl lm m
j k l m
L L
=(cid:88)
V exp(−ηλ α)
1 (cid:88)(cid:0) V−1(cid:1)
u
ij j −λ lm m
j
j m
L
1 (cid:88)
=− λi−2exp(−ηλ α)λ
L j j j
j
L
1 (cid:88)
=− exp(−ηλ α)λi−1
L j j
j
(37)
whichleadsto⟨R(α)⟩ =F −A−1u=T +F. Thus,weobtainfortheexpectationvalueof
Ja,0,Ba 1
R(1)
⟨R(1)⟩ =⟨R ⟩
Ja,0,Ba 2 Ja,0,Ba
L
1 (cid:88)
=1− exp(−ηλ α)λ . (38)
L a a
a
Asanextstep,weinserttheresultgivenbyEq. (30)for⟨R ⟩=0intothedifferentialequationsfor
0
thestudent-studentorderparametersgivenbyEq. (29),inordertoobtainanewexpression
dQ =ηA (cid:0) eηA1αA−1u−A−1u(cid:1) +ηA Q+η2u (39)
dα 3 1 1 4
17Preprint
In order to simplify the differential equation, we evaluate A A−1T. The inverse of A can be
3 1 1
obtainedanalytically,wherewefind
c1 c2 c3 ··· cL−1 1
c0 c0 c0 c0 c0

−1 0 0
... . .
.
. .
.


 
A−1
=
 0 −1 0
...
0
0
 (40)
1  


. .
.
. .
.
... ...
0
0

 
 0 0 ··· −1 0 0
0 0 0 ··· −1 0
and obtain for the matrix-vector product −A A−1u = 2(η−1)u. We can further simplify
3 1
ηu−A A−1u=(η−2)uandinsertthisresultinEq. (39)inordertoobtain
3 1
dQ
=ηA eA1αA−1u+(2−η)u+ηA Q. (41)
dα 3 1 4
Thesolutionofthisdifferentialequationisgivenby
(cid:16) (cid:17)
Q(α)=eηA4αQ +eηA4α(A −A )−1 e(ηA1−ηA4)α−I A A−1u (42)
0 1 4 3 1
+(2−η)eηA4αA−1(cid:0) I−e−ηA4α(cid:1)
u (43)
4
with Q are the student-student order parameters at initialization. Note that the initial value Q
0 0
doesnotvanishinthethermodynamiclimitandisalsonotzeroonaverageincontrasttoR . By
0
thedefinitionofthematrices,wefindA −A = 1A makingitstraightforwardtoevaluatetheir
1 4 2 3
relations(A −A )−1A =2I . Furthermore,inordertoestimate(2−η)A−1u,weneedtoknow
1 4 3 L 4
theinverseofA forwhichwefind
4
 c1 − η c2 c3 ··· cL−1 1 
2c0 2−η 2c0 2c0 2c0 2c0
  1 0 0 ... . . . . . .  
 η−2 

 ηT(2) −1 0
...
0 0


A−1 = 2(η−2) 2 . (44)
4    . . . . . . ... ... 0 0   
  ηT(L−2) 0 ··· −1 0 0  
 2(η−2) 2 
ηT(L−1) 0 0 ··· −1 0
2(η−2) 2
Thus,weobtainfortheproduct(2−η)A−1u=−T. Therefore,wecansimplifyEq. (43)to
4
Q(α)=T +eηA4αQ +(cid:0) eηA4α−2eηA1α(cid:1) T (45)
0
B.1.1 SMALLLEARNINGRATES
Forsmalllearningrates,wecanapproximateA ≈ 2A andimmediatelyfindforthefirstorder
4 1
student-studentorderparameters
⟨Q(1)⟩ =⟨Q ⟩
Ja,0,Ba 2 Ja,0,Ba
L L
=1+(cid:0) 1+σ2(cid:1) 1 (cid:88)
exp(−2ηλ α)λ
−21 (cid:88)
exp(−ηλ α)λ , (46)
J L a a L a a
a a
wherewehaveexploitthat⟨Q ⟩ =σ2T. AfterinsertingEqs. (38)and(46)intoEq. (23),we
0 Ja,0,Ba J
obtainforthegeneralizationerror
L
⟨ϵ ⟩
=(cid:0) 1+σ2(cid:1) 1 (cid:88)
λ exp(−2ηλ α), (47)
g Ja,0,Ba J 2L a a
a=1
leadingtoEq. (8)inthemaintext.
18Preprint
B.2 SCALINGWITHTIME
InordertoevaluatethesumforthegeneralizationerrorgiveninEq. (47),weconsidertheEuler-
Maclaurinapproximation. Theformulafortheapproximationisgivenby
⌊p⌋
(cid:88)n f(i)=(cid:90) n
f(x)dx+
f(n)+f(m) +(cid:88)2 B
2k
(cid:16) f(2k−1)(n)−f(2k−1)(m)(cid:17)
+R , (48)
2 (2k)! p
i=m m k=1
whereB isthekthBernoullinumber. Forthedifferenceofthesumfromtheintegral,onecanfind
2k
(cid:88)n (cid:90) n f(m)+f(n) (cid:90) n
f(i)− f(x)dx= + f′(x)P (x)dx
2 1
i=m m m
f(m)+f(n) 1f′(n)−f′(m) (cid:90) n P (x)
= + + f′′′(x) 3 dx,
2 6 2! 3!
m
whereP (x)aretheperiodizedBernoullipolynomialsoforderk. Thesearedefinedas:
k
P (x)=B (x−⌊x⌋),
k k
whereB (x)aretheBernoullipolynomialsand⌊x⌋denotesthefloorfunction. Forourcase,the
k
functionanditsderivativeare
(cid:0) 1+σ2(cid:1)
λ
(cid:18)
2ηλ
α(cid:19)
f(k,α,β)= J + exp − +
L kβ+1 kβ+1
(cid:0) 1+σ2(cid:1) λ (cid:18) 2ηλ α(cid:19)(cid:18) 2ηλ α(β+1) β+1(cid:19)
f′(k)= J + exp − + + − .
L kβ+1 k2β+3 kβ+2
Wewanttoapproximatethesumbytheintegraland,therefore,estimatetheirmaximaldifference.
Duetotheexponentialpre-factorinallfunctions,themaximaldeviationoftheintegralfromthesum
isobtainedatinitializationα=0orforη →0. Therefore,wemakethefollowingansatz
(cid:88)n (cid:90) n f(m)+f(n) (cid:90) n
f(i)− f(x)dx< lim + f′(x)P (x)dx.
i=m m η→0 2 m 1
Ourgoalistoexpresstheterm(cid:82)n f′(x)P (x)dxforη →0,whichrepresentsthecorrectiontothe
m 1
integral,inamoreexplicitform. Todothis,weconsidertheintegralovertheinterval[k,k+1),
wherekisaninteger. Thisallowsustohandlethefloorfunctionmoreeasily,asitisconstantover
eachinterval. Withineachinterval,wecansimplifytheexpressionfor(cid:82)k+1 f′(x)P (x)dx,which
k 1
canbewrittenas:
L (cid:90) k+1
∆ = lim f′(x)P (x)dx.
k (1+σ J2)λ + η→0 k 1
Substitutingtheexpressionforf′(x)andtakingthelimitasη →0,∆ becomes:
k
1 1 1 1
∆ = + + − .
k 2(k+1)β+1 2kβ+1 β(k+1)β βkβ
Thisexpressionfor∆ providesasimplifiedformforthecorrectiontermassociatedwiththeEuler-
k
Maclaurinapproximationinthelimitasηapproacheszero. Toestimatethetotalcorrectionacrossthe
entireintervalfrom1toL,wesumthisexpressionfromk = 1toL−1forlargeL. Forthefirst
term,weobtainbyreindexing
L−1
1 (cid:88) 1 1 1
= ζ(β+1)− ,
2 (k+1)β+1 2 2
k=1
whereζ(β+1)istheRiemannzetafunctionevaluatedatβ+1. Thesecondtermisstraightforward
andsumsto:
L−1
1 (cid:88) 1 1
= ζ(β+1).
2 kβ+1 2
k=1
19Preprint
100
6×101
4×101
3×101 6×101
2×101 sum
sum integral without corrections
integral without corrections 4×101 integral + 0th order corrections
integral + 0th order corrections integral + upper bound
100 101 100 101 102 103
Figure7: GeneralizationerrorevaluatedbyEq. (47)(fullblue)andEq. (48)includingdifferent
order of corrections (dashed) for N = 128 , K = M = 1, σ2 = 1 and β = 0.5. Left: For
J
η = 1 ≈ 0.001, the upper bound of the error can be tightened and is given by the 0th order
2λ+
corrections. Right: Forlearningratesη < 1 theupperboundincludeshigherordercorrections
2λ+
uptotheworstcaseboundforsmallα<α= 1 (reddashed). However,forα> 1 ,the0th
2ηλ+ 2ηλ+
orderupperboundisagainvalid. Here,wehavechosenη = 0.00001andthereforethe0thorder
beginstobevalidatα= 1 ≈950.
2ηλ+
Forthedifferenceofthethirdtermfromthefourth,wefind
L−1
(cid:88) 1 1 1
− =−
β(k+1)β βkβ β
k=1
Combiningalltheseresults,thesumoftheintegralcorrectiontermoverallintervalsfrom1toL−1
is:
L−1
(cid:88) 1 1
∆ =ζ(β+1)− −
k 2 β
k=1
whereζ(β+1)istheRiemannzetafunctionevaluatedatβ+1.
Thus,wecanupperboundtheerrorby
(cid:88)n f(i)−(cid:90) n
f(x)dx<
(cid:0) 1+σ J2(cid:1) λ
+
(cid:20) 1
+ζ(β+1)−
1(cid:21)
2L Lβ+1 β
i=m m
This"worstcase"upperboundworksexcellentformoderateinputsizesN
∼O(cid:0) 101(cid:1)
aswell.
Thebehaviorofthedifferencewithαandfiniteηcanalsobeestimated. Ifweconsideralearningrate
largerthanη > 1 ,whichcanstillbeseenassmallsinceλ ∼N forα≳1,thenthedifference
2λ+α +
willbemainlygovernedbythe0thordercontributions. Especiallyf(k =1)playsthemostimportant
roleandwefind
(cid:88)n f(i)−(cid:90) n
f(x)dx<
1
f(1)=
(cid:0) 1+σ J2(cid:1) λ
+
exp(−2ηλ α). (49)
2 2L +
i=m m
Sincewehavechosenη > 1 ,thedifferencedecreasesexponentiallyintime. Forsmallervalues
2λ+α
ofthelearningrateη < 1 higherordermustbeincludeduptotheworstcaseboundasgivenin
2λ+α
Figure7. However,fortheconditionη < 1 orequivalentlyα< 1 nomeaningfullearning
2λ+α 2ηλ+
hasoccurred.
In the right plot of Figure 7, we have chosen η = 0.00001 in order to test the boundaries of our
approximation. However,suchalearningrateisveryuntypicalsincenolearningwouldoccurover
manytimeorders. Typically,thelearningratesthatweuseinoursimulationsarelargerthan0.001,
and0thordercorrectionsareenoughtoconsider(cf. Figure8).
20
g gPreprint
6×101
4×101
6×101
3×101
4×101
2×101 3×101
sum 2×101 sum
10 1 integral without corrections integral without corrections
integral + 0th order corrections integral + 0th order corrections
100 101 102 100 101 102
Figure8: GeneralizationerrorevaluatedbyEq. (47)(fullblue)andEq. (48)includingdifferentorder
ofcorrections(dashed)forN =128,K =M =1,σ2 =1andη =0.01Left: Forβ =0.5. Right:
J
Forβ =0.01.
Since the 0th order corrections decrease exponentially with α, we use the integral in order to
(cid:82)n
approximatethegeneralizationerrorϵ (L;α)≈ f(x)dxform=1ton=L. Thisintegralcan
g m
besolvedanalyticallyandweobtain
ϵ (L;α)≈
(cid:0) 1+σ J2(cid:1) λ +(2ηλ +α)− 1+β β (cid:34) Γ(cid:18) β ,2ηλ +α(cid:19) −Γ(cid:18) β
,2ηλ
α(cid:19)(cid:35)
(50)
g 2L 1+β 1+β Lβ+1 1+β +
whereΓ(s,x)istheincompletegammafunction. Intheexpressionofthegeneralizationerror,we
canobserveapower-lawfactorandwanttoclearunderwhichconditionsthegeneralizationerrorand
thefunctiongivenbyEq. (50)showsascalingaccordingtoϵ
g
∼α− 1+β β. Forthis,wenotethatthe
incompletegammafunctionΓ(s,z)canbeapproximatedby
zs
Γ(s,z)≈Γ(s)− (51)
s
forz ≪1. Foroursetup,weidentifys= β andz = 2ηλ+α forthefirstΓfunctioninthebrackets
β+1 L1+β
giveninEq. (50).
For the second gamma function within the brackets in (50), we note that its argument is L1+β
larger than for the first gamma function. Furthermore, from the previous discussion based on
empiricalobservations,weknowthatmeaningfullearninghappensforα > 1 . Thus,wecan
2ηλ+
introduce a scaled time variable by α˜ = 2ηλ α and insert this into the second gamma function
+
(cid:16) (cid:17) (cid:16) (cid:17)
Γ β ,2ηλ α = Γ β ,α˜ decreasingexponentiallyfastwithα˜. Therefore,wecanneglect
1+β + 1+β
thesecondgammafunctioncomparedtothefirstonesincebothoperateindifferenttimescalesas
presentedinFigure9. Notethattheconditionα> 1 ensuresthatthezeroth-ordercontributions
2ηλ+
fromthegammafunctiondonotcancelout. Forverysmalllearningrates(η → 0),bothgamma
functionscanbeapproximatedbyEq. (51),wherethezeroth-ordercontributionsareidentical(Γ(s)).
Thus,ourinitiallyempiricalobservationα> 1 nowhasanalyticaljustification. Thus,wefurther
2ηλ+
simplify
ϵ (L;α)≈
(cid:0) 1+σ J2(cid:1) λ +(2ηλ +α)− 1+β β Γ(cid:18) β ,2ηλ +α(cid:19)
(52)
g 2L 1+β 1+β Lβ+1
validfor 1 <α.
2ηλ+
Thus,fortheconditionΓ(s)> zs ,weobtainapower-lawscalingforthegeneralizationerror. This
s
(cid:16) (cid:17)1+β
conditionisfulfilledforα< 1 Γ 2β+1 β L1+β.
2ηλ+ 1+β
Inconclusion,thegeneralizationerrorshowsapower-lawscalingforthecondition 1 < α <
2ηλ+
(cid:16) (cid:17)1+β (cid:16) (cid:17)1+β
1 Γ 2β+1 β L1+β. NotethatΓ 2β+1 β ≲ 1. InsertingtheapproximationgivenbyEq.
2ηλ+ 1+β 1+β
21
g gPreprint
4×101
3×101
6×101
2×101
4×101
10 1
sum 3×101 sum
integral without corrections integral without corrections
6×102 integral with only G1 integral with only G1
100 101 100 101
Figure9: GeneralizationerrorevaluatedbyEq. (47)(fullblue),Eq. (50)(dashedorange)and(52)
(dashedgreen)forN = 128,K = M = 1,σ2 = 1andη = 0.01. Left: Forβ = 1. Right: For
J
β =0.01.
(51)intoEq. (50)andneglectingthesecondgammaterm,weobtain
ϵ (L;α)≈
(cid:0) 1+σ J2(cid:1) λ +(2ηλ +α)− 1+β β Γ(cid:18) β (cid:19)
(53)
g 2L 1+β 1+β
Weobservethatthepower-lawrangeisextendedwithanincreasingnumberofdistincteigenvalues
andthereforewiththeinputdimensionN inpracticeandcovariancematrixpower-lawexponentβ
(cid:16) (cid:17)1+β
whichcanbeseenveryeasilyfortherescaledversion1<α˜ <Γ 2β+1 β L1+β.
1+β
B.2.1 GENERALLEARNINGRATE
Here,wereconsiderEq. (45)andwanttoevaluatethestudent-studentorderparametersnotinthe
smalllearningratelimit. Forthis,wehavetounderstandhowtheeigenvectorsofA influenceT.
4
WecalltheeigenvectormatrixofA simplyV whichcontainsalleigenvectorsasitscolumnsand
4 4
introducetheeigendecompositioneA4α =V 4eηΛ4αV 4−1withΛ 4containingtheeigenvaluesofA
4
onitsdiagonal. TofindtheeigenvaluesofA eitheranalyticallyornumericallyisnolongereasy.
4
However,wecanpretendtoknowtheeigenvaluesofA andcallthemλ for1 ≤ k ≤ L. This
4 4,k
makesitpossibletofindthestructureoftheeigenvectormatrix.
Here,wepresentamoregeneralsolutionforthematrixB = aA +ϵU andcalltheeigenvalues
1
ofBsimplyλ anddefinetheeigenvectorsBv =λ v fork ∈{1,...,L}. Notethatfor
B,k B,k B,k B,k
a=2andϵ=η,wecanreproduceB(a=2,ϵ=η)=A . Foreachλ ,thecorrespondingk-th
4 B,k
eigenvectorhasthefollowingstructure
λ 1 1
v =1, v = B,k, v = (ϵT2−λ ),
B,1k B,2k ϵ−a B,3k aϵ−a B,k
(cid:32) L−2 (cid:33)
1 (cid:88)
v =− −λL−1+ϵ (−1)iaL−2−iλi TL−i .
B,ik aL−2(ϵ−a) B,k B,k
i=1
Theeigenvectorshaveaninterestingproperty.LikeforthecompanionmatrixA anditsVandermonde
1
eigenvectormatrixV ,thefirstl−1entriesoftheeigenvalueequationBv =λ v givea
1 B,k B,k B,k
zerobyconstruction(Bv −λ v ) =0forl=1,...,L−1. Onlythelastentryl=Lofthe
B,k B,k B,k l
eigenvalueequationprovidesinformationabouttheeigenvaluesofB
L
(cid:88)
(Bv −λ v ) = c λj .
B,k B,k B,k L B,j B,k
j=1
ThisisexactlythecharacteristicpolynomialforBwherewehaveidentified
(−1)L+1 1
c =ac , c = (54)
B,0 0 B,L aL−2 ϵ−a
1 (−1)j+1 (cid:32) L (cid:88)−1−i (cid:33)
c = ac +ϵ c T(L+1−j−l) (55)
B,j aj−1 ϵ−a j L−l
l=0
22
g gPreprint
L=10 L=50 102 L=10
L=20 L=75 L=20
101 L=30 L=100 101 L=30
L=40 L=125 L=40
L=50
100 L=75
L=100
L=125
100 10 1
10 2
100 101 102 100 101 102
k k
Figure10: Coefficientsb definedbyEq. (57)multipliedwithλ forη = 1.0. Left: β = 0.1.
k B,k
Right: β =1.0.
=1 =1
=0.5 =0.5
101
=0.1
101
=0.1
=0.01 =0.01
100 100
10 1 10 1
100 101 100 101
k k
101
100
=1
=0.5
10 1 =0.1
=0.01
100 101
k
Figure11: Coefficientsb definedbyEq. (57)multipliedwithλ andLforL=40(solid)and
k B,k
eigenvalues of the data covariance matrix (dashed). Upper left: η = 0.1. Upper right: η = 0.5.
Center: η =1.0.
astheshiftedcoefficientsofB comparedthecoefficientsc ofA forj = 1,...,L−1. Sincewe
j 1
haveassumedthatλ isaneigenvalueofB,theright-handsideofEq. hastobezero,andthec
B,k B,k
areindeedthenewcoefficients. Sinceweknowtheshiftedcoefficients,wecannumericallycalculate
thecorrespondingeigenvaluesastherootsofthenewcharacteristicpolynomial.
MostimportantisthesecondentryoftheeigenvectormatrixV sincethiswillhaveaninfluenceon
4
thegeneralizationerror. Next,weevaluateV−1T
4
L
⟨ϵ ⟩
=(cid:0) 1+σ2(cid:1) 1 (cid:88)
b λ exp(−2ηλ α), (56)
g Ja,0,Ba J 2L k B,k B,k
k=1
with b = (cid:80)L(cid:0) V−1(cid:1) T(l) and λ are the eigenvalues of B for a = 2 and ϵ = η. Thus, in
k l B kl B,k
ordertocalculatethegeneralizationerror,wehavetofindanexpressionforV−1. WeknowthatB
B
showssomepropertiesofacompanionmatrix. Therefore,weperformasimilaritytransformation
23
|k,B
|×kb×L
|k,B
|×kb×L
|k,B
|×kb×L
|k,B
|×kb×L
|k,B
|×kb×LPreprint
L L= =1 15 20
5
102 L L= =1 15 20
5
L=100 L=100
101 L L= =7 55
0
101 L L= =7 55
0
L=25 L=25
L=20 100 L=20
L=15 L=15
L=10 L=10
100 10 1
10 2
100 101 102 100 101 102
k k
101
L=150 101 L=150
L=125 L=125
100 L=100 100 L=100
L=75 10 1 L=75
L=50 L=50
10 1 L=25 10 2 L=25
L=20 10 3 L=20
10 2 L L= =1 15 0 10 4 L L= =1 15 0
10 5
10 3 10 6
10 7
100 101 102 100 101 102
k k
Figure12: SpectrumofmatrixB fora=2(upperfigures)andcorrespondingdifferencewiththe
spectraofthedatacovariancematrix. Left: ϵ=1andβ =0.1. Right: ϵ=0.5andβ =1.0.
small limit small limit
10 1 no approximation no approximation
10 2
10 2
100 101 102 100 101
small limit
no approximation
10 2
10 3
100 101 102
Figure 13: Generalization error evaluated by Eq. (8) (blue) and Eq. (6) (orange) for N = 128 ,
K =M =1,σ2 =0andβ =1. Upperleft: η =0.1. UpperRight: η =0.5. Bottom: η =1.0.
J
24
|k,B
|
|B
|
2
g
g
|k,B
|
|B
|
2
gPreprint
B =SB S−1. Thereby,B hasagainacompanionmatrixstructuresimilartoA ,butithasc
2 2 1 B,i
foritslastrowentryfori = 0,...,L−1givenbyEq. (55). ForthetransformationmatrixS,we
obtain
a−η 0 0 0 ··· 0 
0 −1 0 0 ··· 0
 
 0 −T2η 1 0 ··· 0 
1  a a 
S = (a−η)
 0 −T3η T2 η − 1
...
0


 a a2 a2 


. .
.
. .
.
. .
.
. .
.
... . .
.


0 −TL−1η TL−2 η −TL−3 η ··· (−1)L−1 1
a a2 a3 aL−1
havingatriangularstructure. Theentriesoftheinverseofatriangularmatrixcanbecalculatedby
thissimplerelation
1
S−1 =
ii S
ii
j−1
1 (cid:88)
S−1 =− S S−1, fori>j
ij S ik kj
ii
k=i
NotethattheeigenvectormatrixV isagainaVandermondematrix,meaningthatweknowthe
B2
entriesandtheirinverse. Therefore,weobtain
L
b
=(cid:88)(cid:16)
(SV
)−1(cid:17)
T(l). (57)
k B2
kl
l
Inpractice,wenolongerhavetodirectlyinvertanymatrix,whichallowsustoconsiderhighervalues
ofLandmoredifferentcovariancematrixpower-lawexponentsβ. CalculatingtheinverseofV is
B
numericallyveryunstableandjustpossibleforsmallnumbersofdistincteigenvaluesL.
Figure10comparesb multipliedwithλ withtheeigenvaluesofthedatacovariancematrixλ
k B,k k
forvariousL. Forsmalllearningrates,thedifferencesinthespectraareverysmallandincrease
withincreasinglearningratesuntilthetrendofpower-lawdecayisdestroyed. Notethatb = 1
k L
andλ =−aλ ifwejustconsiderthedifferentialequationsuptoorderO(η). Figure11shows
B,k k
the same behavior but for different β. The smaller β becomes, the larger the divination from a
consistentpower-lawscaling. Figure12showsthespectrumofλ anditsdifferencefromλ . We
B,k k
alsoobserveinthespectrathatthepurepower-lawbehaviorisperturbed. However,thiseffectseems
tojustincreasethegeneralizationerrorwithoutchangingitsscaling,asshowninFigure13.
B.3 FEATURESCALING
Insteadofconsideringthestatisticalmechanicsapproach,westudydirectlythestochasticgradient
descenthere
Jµ+1−Jµ =−η∇ ϵ(Jµ,ξµ). (58)
J
Fromthisdifferenceequation,onecanderiveaLangevinequationfortheweightdynamicsinthe
thermodynamic limit N → ∞ (see Rotskoff & Vanden-Eijnden (2022); Worschech & Rosenow
(2024)). Forthecontinuousequationdescribingthetrajectoriesoftheweights,weobtain
dJ η
=−η∇ ϵ + √ γ (59)
dα J g N
whereγ isarandomvectordescribingthepathfluctuationswith⟨γ⟩ = 0and⟨γ (α)γ (α′)⟩ =
i j
C δ(α−α′).
ij
Forsmalllearningratesoraproperscalingofthelearningratewiththenetworksize,wecanneglect
pathfluctuationsandapproximatethestochasticprocessbyitsmeanpath. Thisleadstothefollowing
differentialequation
dJ
≈ −η∇ ϵ
dα η→0 J g
=−ηΣ(J −B). (60)
25Preprint
ForadiagonalacovariancematrixΣ =δ λ ,thesolutionofEquation(60)is
kl kl kl
J =exp(−ηλ α)(cid:0) J0−B (cid:1) +B (61)
k k k k k
for k ∈ {1,...,L} and initial weight component J0. Thus, the individual weights are learned
k
exponentiallyfast,andeachcomponentJ convergestherebytothecomponentoftheteacher. For
k
thefirst-orderorderparameters,wefind
N N
R(1) =(cid:88)J kλ kB k = 1 (cid:88)(cid:0) exp(−ηλ α)(cid:0) J0−B (cid:1) +B (cid:1) λ B (62)
N N k k k k k k
k=1 k=1
N N
Q(1) =(cid:88)J kλ kJ k = 1 (cid:88)(cid:0) exp(−ηλ α)(cid:0) J0−B (cid:1) +B (cid:1)2 λ (63)
N N k k k k k
k=1 k=1
andfortheircorrespondingexpectationvalue
N
1 (cid:88)
⟨R(1)⟩ =1− λ exp(−ηλ α) (64)
J k0,Bk N k k
k=1
1+σ2 (cid:88)N 2 (cid:88)N
⟨Q(1)⟩ =1+ J λ exp(−2ηλ α)− λ exp(−ηλ α) (65)
J k0,Bk N k k N k k
k=1 k=1
Fortheexpectationvalueofthegeneralizationerror,weobtain
1+σ2 (cid:88)N
⟨ϵ ⟩ = J λ exp(−2ηλ α) (66)
g J k0,Bk 2N k k
k=1
1+σ2 (cid:88)L
= J λ exp(−2ηλ α) (67)
2L k k
k=1
wherewehaveexploitthat N ∈ Nasassumedforoursetup(seeSection3). NotethatEquation
L
(67)thatwederivedfromtheapproximatedLangevinequation,isthesameasthegeneralization
errorderivedfromthestatisticalmechanicsapproachinthesmalllearningratelimitη →0givenby
Equation(8). Therefore,neglectingthehigherorderofthelearningrateinthedifferentialequations
isequivalenttoneglectingpathfluctuationsofthestochasticgradientdescent.
Next, in order to model how the generalization error scales as more and more feature directions
areexplored,weassumethatN componentsofthestudentarealreadylearned. Thereby,theother
l
N −N componentsarekeptfixedandrandom. Here,wewanttoinvestigatethegeneralizationerror
l
asafunctionofN . Forthis,wemaketheansatz:
l
(cid:26)
B fork =1,...,N
J = k l (68)
k J0 else
k
Therefore,weobtaintwodifferentpartsfortheorderparameters
1
(cid:88)N
1
(cid:32) (cid:88)Nl (cid:88)N (cid:33)
R(1) = λ J B = λ B2+ λ J0B
N k k k N k k k k k
k=1 k=1 k=Nl+1
1
(cid:88)N
1
(cid:32) (cid:88)Nl (cid:88)N (cid:33)
Q(1) = λ J J = λ B2+ λ J0J0 (69)
N k k k N k k k k k
k=1 k=1 k=Nl+1
andtheirexpectationvaluesbecome
1
(cid:88)Nl
⟨R(1)⟩ = λ ,
Jk,0,Bk N a
a=1
1 (cid:88)Nl σ2 (cid:88)N
⟨Q(1)⟩ = λ + J λ (70)
Jk,0,Bk N k N k
k=1 k=Nl+1
26Preprint
Next,weinsertEquation(70)intotheexpressionofthegeneralizationerrorandobtain
1+σ2 (cid:32) 1 (cid:88)Nl (cid:33)
⟨ϵ ⟩ = J 1− λ
g Jk,0,Bk 2 L k
k=1
1+σ2 1 (cid:88)L
= J λ , (71)
2 L k
k=Nl
wherewehaveexploitthat(cid:80)Nλ =N and N ∈N. Next,weusethedefinitionoftheeigenvalues
k k L
λ =λ (cid:0)1(cid:1)1+β anddefinethepartialsumS(N )= λ+ (cid:80)L (cid:0)1(cid:1)1+β whichleadsto
i + i l L k=Nl i
1+σ2
ϵ = JS(N ) (72)
g 2 l
Inordertoapproximatethesum,weusetheEuler-MaclaurinformuladefinedinEquation(48)and
replacethesumwithanintegral. Forβ >0,wefind
λ (cid:90) L 1
S(N )≈ + di
l L iβ+1
Nl
(cid:32) (cid:33)
λ 1 1
= + − (73)
Nβ Nβ Lβ
l
Thus,weobtain
(cid:32) (cid:33)
1+σ2 λ 1 1
⟨ϵ ⟩≈ J + − . (74)
g,asymp 2 Lβ Nβ Lβ
l
The zeroth-order error of this approximation, which we consider as the worst-case, is δϵ =
g
1+σ J2 λ+ (cid:16) 1 + 1 (cid:17) .
4 L N1+β L1+β
l
IfwedonotassumethatN eigenvaluesarealreadyconverged,thenwecanmaketheansatz
l
(cid:26)
J fork =1,...,N
J˜ = k l (75)
k J0 else
k
whereJ aregiveninEquation(61). Therefore,weagainobtaintwodifferentpartsfortheorder
k
parameters
N
R(1) = 1 (cid:88) λ J˜ B
N k k k
k=1
1
(cid:32) (cid:88)Nl (cid:88)N (cid:33)
= λ J + λ J0B
N k k k k k
k=1 k=Nl+1
N N
= 1 (cid:88) λ (cid:2) exp(−ηλ α)(cid:0) J0−B (cid:1) +B (cid:3) B + 1 (cid:88) λ J0B
N k k k k k k N k k k
k=1 k=Nl+1
N
Q(1) = 1 (cid:88) λ J˜ J˜
N k k k
k=1
1
(cid:32) (cid:88)Nl (cid:88)N (cid:33)
= λ J2+ λ J0J0
N k k k k k
k=1 k=Nl+1
= 1
(cid:32) (cid:88)Nl
λ (cid:2) exp(−ηλ α)(cid:0) J0−B (cid:1) +B (cid:3)2 +
(cid:88)N
λ
J0J0(cid:33)
(76)
N k k k k k k k k
k=1 k=Nl+1
27Preprint
102
101
100
10 1
10 2
cifar5m
10 3 synthetic
100 101 102 103
k
Figure14: Left: Spectraofthefeature-featurecovariancematrixofCIFAR-5mdatasetvs. Eq. (1)
forβ =0.3andN =L=1024.
andtheirexpectationvaluesbecome
1
(cid:88)Nl
1
(cid:88)Nl
⟨R(1)⟩ = λ − λ exp(−ηλ α),
J k0,Bk N k N k k
k=1 k=1
1+σ2 (cid:88)Nl 2 (cid:88)Nl
⟨Q(1)⟩ = J λ exp(−2ηλ α)− λ exp(−ηλ α)
J k0,Bk N k k N k k
k=1 k=1
1 (cid:88)Nl σ2 (cid:88)N
+ λ + J λ (77)
N k N k
k=1 k=Nl+1
NotethatEquation(77reducestoEquation(70)forα→∞. Next,weinsertEquation(77)intothe
expressionofthegeneralizationerrorandobtain
1+σ2 (cid:34) (cid:88)Nl (cid:88)L (cid:35)
⟨ϵ ⟩ = J λ exp(−2ηλ α)+ λ (78)
g J k0,Bk 2L k k k
k=1 k=Nl
NotethatEquation(78)isbasicallyacombinationofEquation(71)andEquation(67). Ifweset
N =N inEquation(78),thenwecanreproduceEquation(67),andifweletα→∞inEquation
l
(78),thenwereachtheasymptoticgeneralizationerrorgivenbyEquation(71). Therefore,wecan
repeattheanalysisofthisandtheprevioussubsectionseparatelyandfinallyobtainthegeneralization
errorforsmalllearningrates
(cid:34) (cid:32) (cid:33)
1+σ2λ 1 1 1
ϵ (N ,α) ≈ + −
g l η→0 2 L β Nβ Lβ
l
+
(2ηλ +α)− 1+β β (cid:34) Γ(cid:18) β ,2ηλ +α(cid:19) −Γ(cid:18) β
,2ηλ
α(cid:19)(cid:35)(cid:35)
. (79)
1+β 1+β Lβ+1 1+β +
Next,weconsiderageneralΣ,whichisnolongerdiagonal. ThesolutionoftheLangevinequationis
J˜ =exp(−ηDα)(J˜(0)−B˜)+B˜, (80)
wherewehaveintroducedtheeigendecompositionΣ=WDW⊤andexpressedtheweightsinthe
eigenbasisJ˜ =W⊤J,B˜ =W⊤B. Therefore,weobtainfortheorderparameters
J⊤ΣJ J⊤WDW⊤J J˜⊤DJ˜
Q(1) = = =
N N N
J⊤ΣB J⊤WDW⊤B J˜⊤DB˜
R(1) = = = (81)
N N N
Notethattheexpectationvaluesfortheorderparametersand,therefore,forthegeneralizationerror
arestillthesame.
28Preprint
WeusetheparametrizationfromEq. (80)toobtaintheresultsshownintherightpanelofFigure
3. Thereby,wetestourpredictionforthegeneralizationerrorfromEq. (79)toastudentnetwork
trainedonCIFAR-5Mimagesusingapproximately106 (seeNakkiranetal.(2021)fordetailson
the dataset). We use only the first channel of the images resulting in a total input dimension of
N =1024afterflattening. ToapproximatethetruecovariancematrixΣ,wenumericallyestimate
thefeature-featurecovariancematrixbasedontheinputexamplesfromthetrainingdataset. During
training,weupdateonlythefirstentriesofJ˜,whileresettingtheremainingentriestotheirinitial
valuesaftereachiteration. Basedonthespectraofthefeature-featurecovariancematrixdepictedin
Figure14,weestimateβ ≈0.3andusethisspectrumtoevaluateEq. (80).
B.3.1 STUDENTTRAININGBEYONDCOVARIANCEEIGENBASIS
InthisSubsection,weconsiderthegeneralizationevaluatedforastudentwithN trainableweights
l
andN−N randomweights. Thereby,wenolongertraintheweightsofthestudentintheeigenbasis
l
ofthedata-covariancematrix. Inordertodistinguishbetweentrainableanduntrainablecomponents,
wedecomposethestudentvectorJ intwoparts: thetrainablecomponentsJ˜andthenon-trainable
componentsJˆ. ForthecovariancematrixΣ,weidentifythefollowingstructureinblockmatrixform
(cid:18) Σ˜ Σ (cid:19)
Σ= cross , (82)
Σ⊤ Σˆ
cross
where
• Σ˜ ∈RNl×Nl representsthesubmatrixofthecovariancematrix,actingexclusivelyonthe
subspacespannedbythefirstN componentsofthevectorJ,denotedasJ˜.
l
• Σˆ ∈R(N−Nl)×(N−Nl)referstothesubmatrixcorrespondingtothesubspaceoftheremain-
ingN −N componentsofJ,denotedasJˆ.
l
• Σ
cross
∈ RNl×(N−Nl) representsthecross-covariancematrixpart,describingtheinterac-
tionsbetweenthesubspaceofthefirstN componentsandthecomplementarysubspaceof
l
thelastN −N components
l
TheevolutionofthestudentvectorJ isgovernedbythedifferentialequationforsmalllearningrates
N
dJ i ≈ η(cid:88) Σ (B −J ). (83)
dα η→0 ik k k
k=1
ThesolutionforthetrainablecomponentsJ˜(α)attimeαisgivenby
(cid:16) (cid:17) (cid:16) (cid:17)(cid:16) (cid:17)
J˜(α)=e−ηΣ˜αJ˜0+B˜ 1−e−ηΣ˜α +Σ˜−1Σ Bˆ −Jˆ 1−e−ηΣ˜α , (84)
cross
whereJ˜0istheinitialconditionforthetrainablecomponents,andthenon-trainablecomponentsare
keptattheirrandominitialvalues. Moreover,B˜ andBˆ arethecomponentsoftheteachervector
equivalenttothetrainableandnon-trainablepartsofthestudentvector,respectively. Atstationarity
(α→∞),thetrainablecomponentsbecome
(cid:16) (cid:17)
J˜=B˜ +Σ˜−1Σ Bˆ −Jˆ . (85)
cross
Forthefirst-orderorderparameters,wefind
29Preprint
Nl=50 Nl=150
Nl=100 Nl=200
10 1
10 2
100 101 102 103 104
Figure15: ϵ asafunctionofαfordifferenttrainableinputdimensionsN ofthestudentvectorwith
g l
L=N =256,K =M =1,σ =0.01,η =0.05,andβ =1. Here,wetrainthestudentoutside
J
theeigenbasisofthedatacovariancematrix. Wecompareresultsbasedonsimulations(solidcurves)
tothetheoreticalpredictionfromEq. (88)(blackdashedlines). Thestudentnetworkistrainedon
syntheticdata,whereweaverageover300differentinitializationsofstudentandteachervectors.
1
R(1) = J⊤ΣB
N
1 (cid:16) (cid:17)
= J˜⊤Σ˜B˜ +J˜⊤Σ Bˆ +Jˆ⊤Σ⊤ B˜ +Jˆ⊤ΣˆBˆ
N cross cross
1
Q(1) = J⊤ΣJ
N
1 (cid:16) (cid:17)
= J˜⊤Σ˜J˜+2J˜⊤Σ Jˆ+Jˆ⊤ΣˆJˆ . (86)
N cross
TakingtheexpectationoverBandJˆyields
1 (cid:16) (cid:16) (cid:17)(cid:17)
⟨R(1)⟩ = Tr(Σ˜)+Tr Σ⊤ Σ˜−1Σ
Bi,Jˆ i N cross cross
1 (cid:16) (cid:16) (cid:17) (cid:16) (cid:17)(cid:17)
⟨Q(1)⟩ = Tr(Σ˜)+(1+σ2)Tr Σ⊤ Σ˜−1Σ +σ2Tr(Σˆ)−2σ2Tr Σ⊤ Σ˜−1Σ .
Bi,Jˆ i N J cross cross J J cross cross
(87)
Finally,weobtainforthegeneralizationerror
1(cid:20) 1 1 1+σ2 (cid:16) (cid:17)(cid:21)
⟨ϵ ⟩ = 1− Tr(Σ˜)+ σ2Tr(Σˆ)− JTr Σ⊤ Σ˜−1Σ . (88)
g,asymp Bi,Jˆ i 2 N N J N cross cross
NotethatfordiagonalcovariancematricesΣ = δ λ ,Eq. (88)reducestoEq. (10)forα → ∞.
ij ij i
Figure15presentsthegeneralizationerrorobtainedfromsimulationsforvariousN asafunction
l
of α. In this comparison, the asymptotic solution derived from Eq. (88) aligns closely with the
simulationresults,demonstratingexcellentagreement.
Next,wecomparethenumericalsolutionofEq. (88)withourtheoreticalpredictionfortrainingthe
studentvectorintheeigenbasisofthedatacovariancematrix,asgivenbyEq. (74). Theresultsare
presentedinFigure16. Forafixednumberoftrainableparameters,N ,thegeneralizationerroris
l
consistentlylowerwhenthestudentistrainedintheeigenbasisofthedatacovariancematrix. This
is expected, as training in the eigenbasis aligns with the directions of the largest N eigenvalues,
l
leadingtoamoreefficientlearningprocess. Consequently,theoverallgeneralizationerrorisreduced
comparedtotrainingoutsidetheeigenbasis. UndertheconditionNβ ≫Nβ,weobservethesame
l
power-lawscalingforthegeneralizationerror,ϵ ∝N−β,inbothscenarios. Additionally,for
g,asymp l
moregeneralconfigurations,wefindthatthescalingbehaviorofthegeneralizationerrorremains
consistentacrossbothsetups. ThedatacovariancematricesaregeneratedbyΣ=WΛW⊤,where
Λ = δ λ witheigenvaluesdefinedbyEq. (1)andW isarandomorthogonalmatrixwithzero
ij ij i
meanandvariancewhichscalesas⟨W2⟩∼ 1.
ij N
30
gPreprint
10 1 10 2
10 3
10 2
=0.3 =1.0
=0.75 10 4 =1.5
102 102
N N
l l
Figure16: ϵ asafunctionofN fordifferentβ andN =L=1024. Thestudentistrained
g,asymp l
withsyntheticdata. WecomparethenumericalsolutionofEq. (88)(dashed)withourtheoretical
predictionfortrainingthestudentvectorintheeigenbasisofthedatacovariancematrix,asgivenby
Eq. (74)(solid). Thenumericalsolutionisaveragedover100differentcovariancematrices.
C NON-LINEAR ACTIVATION
C.1 DIFFERENTIALEQUATIONS
Throughoutthiswork,weconsidertheerrorfunctionasournon-linearactivationfunctiong(x)=
(cid:16) (cid:17)
erf √x . Forthisactivationfunction,onecansolvetheintegralsI 2,I 3andI 4giveninSectionA
2
analytically(seeSaad&Solla(1995);Yoshida&Okada(2019)). Wefind
(cid:32) (cid:33)
2 C
I (C)= arcsin 12 , (89)
2 π (cid:112) (1+C )(1+C )
11 22
(cid:18) (cid:19)
2 1 C (1+C )−C C
I (C)= 23 11 12 13 , (90)
3 π(cid:112) (1+C 11)(1+C 33)−C 12 3 1+C 11
(cid:18) (cid:19)
4 1 A
I (C)= √ arcsin √ √0 (91)
4 π2 A A A
4 1 2
where
A =(1+C )(1+C )−C2 ,
4 11 22 12
and
A =A C +C C (1+C )+C C (1+C )+C2 C2 C2 ,
0 4 34 23 24 11 13 14 22 12 13 24
A =A (1+C )+C (1+C )−C (1+C )+2C C C ,
1 4 33 23 11 13 22 12 13 23
A =A (1+C )−C (1+C )−C (1+C )+2C C C ,
2 4 44 24 11 14 22 12 14 24
depending on the precise covariance matrix and its entries C as discussed in Section A. After
ij
evaluatingallnecessarycovariancematrices,weobtainforK =M
dR i( nl)
=
2η 1 (cid:34) (cid:88)M T n(l m+1)(1+Q ii)−R i( nl+1)R
im
−(cid:88)M R j(l n+1)(1+Q ii)−R i( nl+1)Q ij(cid:35)
,
(cid:112) (cid:113)
dα Mπ1+Q ii m (1+Q ii)(1+T mm)−R i2 m j (1+Q ii)(1+Q jj)−Q2 ij
(92)
dQ( ikl)
=
2η (cid:34) 1  (cid:88)M (cid:112)R k(l m+1)(1+Q ii)−Q( ikl+1)R
im
−(cid:88)M Q (cid:113)( kl j+1)(1+Q ii)−Q( ikl+1)Q ij

dα Mπ 1+Q ii m (1+Q ii)(1+T mm)−R i2 m j (1+Q ii)(1+Q jj)−Q2 ij
+
1  (cid:88)M (cid:112)R i( ml+1)(1+Q kk)−Q( ikl+1)R
km
−(cid:88)M Q (cid:113)( ijl+1)(1+Q kk)−Q( ikl+1)Q kj (cid:35)
1+Q kk m (1+Q kk)(1+T mm)−R k2 m j (1+Q kk)(1+Q jj)−Q2 kj
+O(cid:0) η2(cid:1)
, (93)
31
pmysa,g pmysa,gPreprint
wherewehaveomittedhigher-ordertermsinη fornotationalsimplicity. Similarly,wehavealso
omittedthesuperscriptsforthefirst-orderparameters,meaningthatallinstancesofR ,T ,and
in nm
Q withoutsuperscriptscorrespondtotheirfirst-orderforms: R(1),T(1),andQ(1). Notethatthe
ij in nm ij
differentialequationsarenotclosedgiveninEquation93. ForthelastcomponentL−1onehasto
applytheCayley-HamiltontheoremfortheorderparametersasdiscussedinSection3. Inthesame
notation,thegeneralizationerrorbecomes
(cid:34) (cid:18) (cid:19) (cid:18) (cid:19)
ϵ = 1 (cid:88) arcsin √ Q √ik +(cid:88) arcsin √ T n√m
g Mπ 1+Q 1+Q 1+T 1+T
i,k ii kk n,m nn mm
(cid:18) (cid:19)(cid:35)
−2(cid:88) arcsin √ R √in , (94)
1+Q 1+T
i,n ii nn
which just depends on the first-order order parameters. This system is gonna be analyzed in the
following.
C.2 PLATEAUHEIGHT
Here,weconsiderthedifferentialequationsgiveninEq. (93)forthehigher-orderorderparameters
up to the first order in the learning rate O(η). As already mentioned in the main text, the order
parametersarenolongerself-averaging,i.e. wecannotreplacetherandomvariablesT(l) bytheir
nm
expectationvalueinthethermodynamiclimitN →∞. However,withoutanyassumptionsonthe
teacher-teacherorderparameters,wefindapproximatelythefollowingfixedpointforl=1
M
R∗(1)
≈
1
,
Q∗(1)
≈
1 (cid:88) 1
in (cid:114)
Tn(1
n)M
+d( n1)
(cid:16) TnM
(1
n)T +n( d1 n
(
n)
1)
(cid:16) 1+ Tn1
(1
n)(cid:17) −1(cid:17) M n TnM (1 n)T +n( d1 n ( n) 1) (cid:16) 1+ Tn1 (1 n)(cid:17) −1
(95)
andforl̸=1
R∗(l)
≈
T n(l n)
,
Q∗(l)
≈
1 (cid:88)M T n(l n)
,
in (cid:114)
Tn(1
n)M
+d( n1)
(cid:16) TnM
(1
n)T +n( d1 n
(
n)
1)
(cid:16) 1+ Tn1
(1
n)(cid:17) −1(cid:17) M n TnM (1 n)T +n( d1 n ( n) 1) (cid:16) 1+ Tn1 (1 n)(cid:17) −1
(96)
withd(1) =(cid:80)M−1 T(1). Thus,forthestudent-teacherorderparameters,weobtainM different
n m,m̸=n nm
plateauheightsforeachorderldependingonthesumovertheoff-diagonalentriesofthehigher-order
teacher-teacherorderparametersd(1) andT(1). ThisapproximationisexactifallT(l) =D(l) for
n nn nm
n̸=mandT =T(l)andallplateauheightsarethesameR∗(l) =R∗(l). InFigure17,wecompare
nn in
thenumericallyfoundgeneralizationerrorbyevaluatingthedifferentialequationsgiveninEq. (93)
uptoO(η)withthegeneralizationerrorbasedontheapproximatelyfoundfixedpointsgiveninEq.
(95). ForsmallL,wefindverygoodagreementbetweenthetrueplateauandtheapproximation.
Inordertoproceed,wemaketheansatzfortheoff-diagonalentriesT(l) = D(l) = 1 (cid:80)Md(l)
nm M n n
forn ̸= m. Thisapproachpreservesthestatisticalpropertiesofthesumrepresentedbyasingle
parameterD(l). Moreover,forl=0,theteacher-teacherorderparametersarestillself-averagingin
thethermodynamiclimit,andwecanassumeT(0) =1andT(0) =0forlargeN. Thusinsummary,
nn nm
weassumeT(0) =δ andT(l) =δ T(l)+(1−δ )D(l) forl ̸=0. Aftertheseassumptions,
nm nm nm nm nm
wefindnewstationarypointsforourdifferentialequationsforl=1
1 1
R∗(1)
= ,
Q∗(1)
= , (97)
(cid:114) M (cid:16) MT(1) (cid:0) 1+ 1 (cid:1) −1(cid:17) T(M 1)+T( D1) (1) (cid:0) 1+ T1 (1)(cid:1) −1
T(1)+D(1) T(1)+D(1) T(1)
32Preprint
7.12×103
L=2 L=10
7.11×103 L=8 L=12
7.1×103
7.09×103
7.08×103
7.07×103
7.06×103
0 250 500 750 1000 1250 1500 1750
Figure17: Comparisonofthegeneralizationerrorplateauevaluatedbynumericalsolutionsofthe
differential equations given in Eq. (93) (solid) and approximated fixed point given by Eq. (95)
(dashed)forN =7000,K =M =4,σ =0.01,β =0.25andη =0.1. Wesolvethedifferential
J
equationsuptoO(η).
andl̸=1
1 1
R∗(0)
= ,
T(1)+D(1)(cid:114)
M (cid:16) MT(1) (cid:0) 1+ 1 (cid:1) −1(cid:17)
T(1)+D(1) T(1)+D(1) T(1)
1 1
Q∗(0)
=
T(1)+D(1) MT(1) (cid:0) 1+ 1 (cid:1) −1
T(1)+D(1) T(1)
T(l) 1
R∗(l)
= ,
T(1)(cid:114)
M (cid:16) MT(1) (cid:0) 1+ 1 (cid:1) −1(cid:17)
T(1)+D(1) T(1)+D(1) T(1)
T(l) 1
Q∗(l)
= . (98)
T(1) MT(1) (cid:0) 1+ 1 (cid:1) −1
T(1)+D(1) T(1)
As one can see in Eqs. (97), and (98), we end up in one plateau height for the order parameters
Q(l) =Q∗(l) andR(l) =R∗(l). InadditiontoFigure4giveninthemaintext,weprovidetheplateau
ij im
behaviorforhigherorder-orderparametersinFigure18andcompareournewlyobtainedstationary
solutionsgiveninEq. (98)withthesolutionsofthedifferentialequationsgiveninEq. (93)upto
O(η). Weobservethatthestudent-teacherorderparametersdefinedingiveninEq. (98)yieldan
approximation for the mean value of the M groups of order parameters. For the student-student
orderparameters,weobserveasmallsystematicerrorwhichappearstobesmallcomparedtothe
magnitudeofQ∗(l).
Next,weinsertEq. (97)intotheexpressionofthegeneralizationerrorandobtain
1(cid:32) (cid:18) D(1)+T(1)(cid:19) (cid:18) D(1) (cid:19)
ϵ∗ = Marcsin +(M −1)arcsin
g π MT(1)+M T(1)+1
 
1
−2Marcsin
√ (cid:113) (cid:113)


T(1)+1 −(D(1)M−M2−(M2−M)T(1)) MT(1)+M
(D(1))2+2D(1)T(1)+(T(1))2 (M−1)T(1)−D(1)+M
(cid:18) T(1) (cid:19)(cid:33)
+arcsin (99)
T(1)+1
fortheplateauheight. TherightpanelinFigure4showsanexamplefortheestimatedplateauheight
givenbyEquation(99)againstthenumericalsolutionofthedifferentialequations.
C.3 PLATEAUESCAPE
Inthissubsection,wewanttopresenttheescapefromtheplateau. Thefoundstationaryequations
givenbyEqs.(97)and(98)areunstablesuchthatafteracertaintimeontheplateau,thegeneralization
33
gPreprint
30 30 4.760
6.5 4.755
6.0 4.750
20 20
5.5 4.745
250 500 750 1000 250 500 750 1000
10 10
0 0
0 1000 2000 3000 4000 5000 6000 7000 0 1000 2000 3000 4000 5000 6000 7000
500 500
110 81.9
400 400 81.8
100
300 300
250 500 750 1000 250 500 750 1000
200 200
100 100
0 0
100 100
0 1000 2000 3000 4000 5000 6000 7000 0 1000 2000 3000 4000 5000 6000 7000
Figure18: PlateaubehavioroftheorderparameterswithL=10,N =7000,σ =0.01,η =0.1
J
and M = K = 4 for one random initialization of student and teacher vectors. We solve the
differentialequationsforthesmalllearningratecasewhereweconsidertermsuptoO(η). Theinset
showsthehigher-orderorderparametersattheplateau. Forthestudent-teacherorderparameters,we
obtainM differentplateauheights. Forthestudent-studentorderparameters,weobserveoneplateau
heightwithsmallstatisticaldeviationsfortheparticularmatrixentryQ(l). Thedashedhorizontal
ij
linesintheinsetsshowtheplateauheightsaccordingtoEq. (98).
error will eventually escape from it. In order to escape from the plateau, the unique solution of
the fixed points must be broken for each l at a certain time. We want to study the dynamics in
thevicinityofthefixedpointandclarifyhowthegeneralizationerrorleavesit. Forthis,introduce
parametersS(l) andC(l) indicatingtheonsetofspecializationforthestudentvectorstowardsone
teachervector. Therefore,parameterizedtheorderparametersbyR(l) =R(l)δ +S(l)(1−δ ),
im im im
Q(l) =Q(l)δ +C(l)(1−δ ). Moreover,tostudytheescapefromtheplateau,weintroducesmall
ij ij ij
perturbationparametersr(l),s(l),q(l)andc(l)modelingtherepellingcharacteristicoftheunstable
fixed point. Thus we parametrized the order parameters by their stationary solution and a small
perturbationR(l) =R∗(l)+r(l),S(l) =S∗(l)+s(l),Q(l) =Q∗(l)+q(l)andC(l) =C∗(l)+c(l)with
S∗(l) =R∗(l) andC∗(l) =Q∗(l). Next,weinsertthisparametrizationintothedifferentialequations
givenbyEq. (93)uptoO(η). Inordertostudythedynamicsinthevicinityofthefixedpoint,we
linearizedthedynamicalequationsin(cid:0) r(l),s(l),q(l),c(l)(cid:1)⊤
aroundzero.
Inthefollowing,wepresentthedifferentialequations,eigenvalues,andeigenvectorsforthespecific
casewhereD(l) =0andT(1) =1fornotationalsimplicity. Thefullsystemistoolargetodisplayin
itsentirety. However,weprovideinsightsthroughoutonhowtheseresultsgeneralize. Afterreducing
thesystemofdifferentialequations,weconcludebypresentingthefullsolutionforthegeneralcase.
Afterafirst-orderTaylorexpansionin(cid:0) r(l),s(l),q(l),c(l)(cid:1)⊤
,wefindthefollowinglinearizedequation
r r
(cid:114)
d s 2 2M −1 s
 = A   (100)
dαq πM 2M +1 pq
c c
34
)4(R
)6(R
ni
ni
)4(Q
)6(Q
ji
jiPreprint
with r = r(i−1), s = s(i−1), q = q(i−1), c = c(i−1) and A = A+B with A ∈ R4L×4L.
i i i i p p
TheindividualmatricescanbewrittenasKroneckerproductsA=G⊗A , B =H ⊗U with
1
U =ue⊤and
2
 1 M −1 0 0 
1 M −1 0 0
 (cid:113) (cid:113) 
G= −2 M −2 M (M −1) 2 2(M −1) , (101)
 (cid:113)2M−1 (cid:113)2M−1 
−2 M −2 M (M −1) 2 2(M −1)
2M−1 2M−1
√
 −M −2M(M−1) 1√ 2M2+3M √−2 (M √−1) 2M−1
(2M+1)(2M−1) (2M+1)(2M−1) 2 M(2M+1) 2M−1 M(2√M+1)


−2M −M(2M−3) 1√ 2M2+3M √−2 (M √−1) 2M−1

H =(2M+1) √(2M−1) (2M+1)(2M √−1) 2 M(2M+1) 2M−1 M(2M+1)  (102)


−2√M −2(M− √1) M M+2 2(M−1) 

(2M+1) √2M−1 (2M+1) 2√M−1 2M+1 2M+1 
−2√M −2(M− √1) M M+2 2(M−1).
(2M+1) 2M−1 (2M+1) 2M−1 2M+1 2M+1
WeobtainthefollowingeigenvaluesandeigenvectorsforG
(cid:32) √ √ (cid:33)⊤
2 M 2 M
λ =M, v = 1,1,√ ,√ , (103)
G,1 G,1
2M −1 2M −1
λ =2M, v =(0,0,1,1)⊤, (104)
G,2 G,2
(cid:18)
1
(cid:19)⊤ (cid:18)
1
(cid:19)⊤
λ =0, v = 1,− ,0,0 , v = 0,0,1,− , (105)
G,3 G,3 M −1 G,4 M −1
andforH
M
(cid:18)
1
(cid:19)⊤
λ = , v = 1,− ,0,0 , (106)
H,1 4M2−1 H,1 M −1
(cid:32) √ √ (cid:33)⊤
2M 2 M 2 M
λ = , v = 1,1,√ ,√ , (107)
H,2 2M +1 H,2 2M −1 2M −1
√
(cid:18) M3/2 2M −1(cid:19) (cid:18) 12M2+3M −2(cid:19)⊤
λ =0, v = 1,1,0, ,v = 0,0,1,− .
H,3 H,3 2M2−3M +1 H,4 22M2−3M +1
(108)
SinceAisofblockmatrixstructureexpressedbyaKroneckerproduct,weobtainforitsspectrum
λ = λ λ and corresponding eigenvectors v = v ⊗ v for which we multiply each
A G A1 A G A1
eigenvalue of A with each of G. The same also applies for the eigenvalues and -vectors of B:
1
λ =λ λ andcorrespondingeigenvectorsv =v ⊗v . TheeigenvaluesofA werealready
B G A1 B G A1 1
studiedinSubsectionB.1andarethenegativeeigenvaluesofthedatacovariancematrix−λ with
k
eigenvectors v summarized by the matrix V (cf. Eq. (33)). Since A possesses L eigenvalues
k 1
and -vectors, we obtain multiple groups of different eigenvalues and -vectors for A and B. The
eigenvaluesand-vectorsofU arealsoalreadyknown. Wehaveoneeigenvectoruwitheigenvalue
λ = T(2) andL−1eigenvectorse withzeroeigenvalueforl = 1,3,4...,L. Therebye isthe
u l l
lth unit vector. In the following, the superscript for the eigenvalues and -vectors indicates the
correspondinggroup.
For A, the first two groups of eigenvalue combinations λ(1) = −Mλ with eigenvector
A,k k
(cid:16) √ √ (cid:17)⊤
v A(1 ,) k = v k,v k,2v k√ 2MM −1,2v k√ 2MM −1 andλ( A2 ,) k =−2Mλ k withv A(2 ,) k =(0,0,v k,v k)⊤ are
plateau attractive. Their corresponding eigenvalues are negative and their directions are against
35Preprint
thebreakingoforderparametersymmetry. Thelatterfactcanbeseenthatthefirsttwoentriesof
the eigenvectors and the last two are the same. This would drive the dynamics in the direction
corresponding to r(l) = s(l) and q(l) = c(l) which is exactly the plateau condition. The third
(cid:16) (cid:17)⊤
groupofeigenvaluecombinationsλ(3) =0witheigenvectorsv(3) = v ,−v 1 ,0,0 and
A,k A,k k kM−1
(cid:16) (cid:17)⊤
v(4) = 0,0,v ,−v 1 areneitherattractivenorrepelling. However,theirdirectionsindicate
A,k k kM−1
asymmetrybreakingintheorderparameters,atleastforonegroupr(l) ̸=s(l)orq(l) ̸=c(l).
ForthematrixB,weobservethatλ(1) =0,withatotalof4L−2distincteigenvectors. However,
B,1
themoresignificantimpactcomesfromthedirectionsassociatedwithnon-zeroeigenvalues. These
eigenvaluesplayacrucialroleininfluencingthespectrumofA,particularlywhenBisviewedasa
non-negligibleperturbationofA. ThesecondeigenvalueofBisλ = MT(2) witheigenvector
B,2 4M2−1
(cid:16) (cid:17)⊤
v = u,−u 1 ,0,0 . For the third one, we obtain λ = 2MT(2) with eigenvector
B,2 M−1 B,3 2M+1
(cid:16) √ √ (cid:17)⊤
v
B,3
= u,u,√2 2MM −1u,√2 2MM −1u .
Insummary,weobtaintwoimportantdirectionsfortheescapeoftheplateau. Thefirstonecorre-
spondstotheeigenvectorsv(3) andv andthesecondoneisinthedirectionofv andv(1).
A,k B,2 B,3 A,k
NotethatthesedirectionsarealsopresentforthesumofA+B resultinginA . Therefore, we
p
makeasafirstansatzq(l) = c(l) sincethisconditionisfulfilledforallimportanteigendirections.
Moreover,wecanmakethefollowingsecondansatzq(l) =c(l) =2R∗(l)(cid:0) r(l)+(M −1)s(l)(cid:1) . The
secondansatzisfulfilledbybotheigendirectionsaswell. ForD(l) ̸= 0andgeneralT(l),wefind
withsimilarstepstherelationq(l) =c(l) = 2T(l) R∗(l)(cid:0) r(l)+(M −1)s(l)(cid:1) .
T(1)+D(1)
Next, we re-parametrize the dynamical equations under the condition q(l) = c(l) =
2R∗(l)(cid:0) r(l)+(M −1)s(l)(cid:1) andfind
(cid:18) (cid:19) (cid:114) (cid:18) (cid:19)
d r 2 2M −1 r
= A , (109)
dα s πM 2M +1 rs s
withA =A+BandA ∈R2L×2L. ThematricesAandBarere-definedasfollows:
rs rs
A=G⊗A , B =H ⊗U (110)
1
withre-definedGandH
1 (cid:20) 5M −3 4M2−7M +3(cid:21) (cid:20) 1 (M −1)(cid:21)
H = , G= (111)
(2M +1)(2M −1) 4M2−7M+3 4M2−6M +3 1 (M −1)
M−1
ForA,theeigenvectorsv aregivenbyv =v ⊗v ,whereλ =−λ andv =v for
A A G A1 A1,k k A1,k k
k ∈ (1,...,L). Thecorrespondinggroupsofeigenvaluesareλ(1) = −Mλ , witheigenvectors
A,k k
v(1) =(v ,v )⊤ . Thesecondgroupisgivenbyλ(2) =0andthecorrespondingeigenvectorsare
A,k k k A
v A(2 ,)
k
=(cid:0) v k, M−v −k 1(cid:1)⊤ .
For B, the first eigenvalue is λ = MT(2) , with the corresponding eigenvector
B,1 (2M−1)(2M+1)
v = (cid:0) u, −u (cid:1)⊤ . Thesecondeigenvalueisλ = 2MT(2) ,witheigenvectorv = (u,u)⊤ .
B,1 M−1 B,2 2M+1 B,2
Furthermore, we have multiple eigenvectors for the eigenvalue zero. For m ∈ (1,3,4,...,L),
we have λ
B,m
= 0, with corresponding eigenvectors v
B,m
= (cid:0) e m, M−e −m 1(cid:1)⊤ . Similarly, for
n∈(1,3,4,...,L),λ =0,witheigenvectorsv =(e ,e )⊤ .
B,n B,n n n
Notethatalleigenvalueswerealreadyencounteredforthelargersystemverifyingouranalytical
ansatz. Moreover, the new eigenvectors are the first two entries of the eigenvectors of the large
originalsystem.
36Preprint
DuetothespecialstructureofA,theeigenvectorv arealsoeigenvectorsofA,bothassociated
B,1
withtheeigenvalue0,meaningAv =0andAv =0. AmongtheeigenvaluesofA ,2L−2
B,m B,1 rs
of them are zero. The first non-zero eigenvalue is λ = MT(2) which is larger than
Ars,1 (2M−1)(2M+1)
zero indicating a repelling character for direction v and v(2). For the eigenvector v , we
B,1 A,k B,2
have Av = M(A u,A u)⊤ . Therefore, v is an eigenvector of A, provided that u is an
B,2 1 1 B,2
eigenvectorofA . Fortheproduct,wefindA u = −u ,withu =
(cid:0) T(2),T(3),...,T(L+1)(cid:1)⊤
.
1 1 + +
(cid:16) (cid:17)
A v(1) +v = −λ (v ,v )⊤ −M(u ,u )⊤ and this group of eigenvalues is therefore
A,k B,2 k k k + +
negative.
Finally, we obtain one important eigendirection showing an eigenvalue larger than zero. This
directioncorrespondstov andv(2). Therefore,wemakethefollowinglastansatzs(l) =− r(l)
B,1 A,k M−1
inordertoreducethesystemforasecondtime. Notethattheexactsamerelationalsoholdsfor
D(l) ̸=0andgeneralT(1).
Forthefinalformofthedifferentialequations,wereturntothecasewhereD(l) ̸=0andT(1) ̸=1,
astheexpressionsarenowmoremanageabletodisplayandnolongerexcessivelylarge. Thefinal
re-definitionofthedynamicalsystemyields
dr
=ηg U˜r, (112)
dα r
with g = 2√
(D(1)+T(1))
, U˜ = u˜e⊤ and u˜ =
r π (M−1)T(1)−D(1)+M(D(1)+(M+1)T(1)+M)23 2
(cid:16) (cid:17)⊤
T(1)− D(1),T(2)− D(2),...,T(L)− D(L) . Note that we define A = g U˜ for the main
M−1 M−1 M−1 r r
text. SinceU˜ isarank-1matrix,weobtainL−1zeroeigenvaluesandoneeigenvalue
D(2)
λ =T(2)− (113)
r M −1
largerthanzero. Thus,λ drivestheescapefromtheplateau. Wecansolvethedifferentialequation
r
directlyandfindforthefirst-orderperturbationparameter
r(1) =eηgrαr(1), (114)
0
where r(1) = r(1)(α ) and α denotes an arbitrary time at the plateau. For the escape of the
0 0 0
generalizationerrorwithinourre-defineddynamicalsystem,wefind
(cid:0) (M −1)T(1)+M −D(1)(cid:1)(cid:0) D(1)+T(1)(cid:1)
ϵ∗ g−ϵ g = (cid:18) (cid:19)3 eτeα sc r 0(1)2 (115)
(D(1)+T(1))2 2
8πM(M −1) T(1)−
4M2
wherewehaveintroducedtheescapetime
1
τ =
esc ηg λ
r r
π (cid:112) (M −1)T(1)−D(1)+M(cid:0) D(1)+(M +1)T(1)+M(cid:1)3 2
= . (116)
2η (cid:16)
T(2)−
D(2)(cid:17)(cid:0) D(1)+T(1)(cid:1)
M−1
Furthermore,wecanapproximateT(2) = 1 Tr(Σ2)= λ2 + (cid:80)L 1 ≈ λ2 + ∝LforlargeL. The
L L i i2(1+β) L
sameappliestoD(2). ForlargeM andL,wefindτ ∝ M2 .
esc ηT(2)
C.4 NUMERICALLYESTIMATEDPLATEAULENGTH
Inthissubsection,wedemonstratehowtocombinetheanalyticallyderivedformulafromEq. (12)
with our calculated escape time (Eq. (15)) to estimate the plateau length. The plateau escape is
37Preprint
0.0072 0.010 M=2 M=4 M=8
M=3 M=6
0.0071 0.008
0.0070 0.006
0.0069
0.004
0.0068
L=2 L=6 0.002
0.0067
L=4 L=8
0.000
0.0066
0 1000 2000 3000 4000 5000 6000 7000 0 2000 4000 6000 8000 10000
Figure19:Plateauphaseofthegeneralizationerrorevaluatedbynumericalsolutionsofthedifferential
equationsforonerandominitialization(solid)andplateaulengthestimationsgivenbyEq. (117)
(verticlelines)forN =7000,K =M =4,σ =0.01,β =0.25andη =0.1. Theblackverticle
J
lineindicatesthearbitrarilychosenplateaustartα =500,andthecoloredverticlelinesshowα
0 P
fordifferentL. WeretaintermsuptoO(η)forthedifferentialequations.
describedbytheequation
(cid:18) (cid:19)
α −α =τ D−
1 ln(cid:0) σ2(cid:1)
+
1
ln(N) , (117)
P 0 esc 2 J 2
whereDisaconstantoforderO(1),dependentonthevarianceatinitializationandtheplateau;α is
0
anarbitrarystartingpointontheplateau;andτ istheescapetimefromtheplateau. Toestimatethe
esc
constantD,weinterprettheresultsofBiehletal.(1996)andfindD
=ln(cid:0)B(cid:1)
,whereBrepresents
c
thedeviationoftheorderparameterresponsiblefortheplateauescapeatα fromitsvalueatα .
P 0
Thereby,cisaproportionalityconstantbetweenthefluctuationsattheplateauandatinitialization. In
ourcase,theorderparameterthatdrivestheplateauescapeisthefirst-orderstudent-teacherorder
parameter (cf. Subsection C.3). Thus, we define B = |R(1)(α )−R(1)(α )|. Additionally, to
0 P
estimateα −α ,wesetR(1)(α ) = eR∗(1),followingthedefinitionoftheescapetimeforthe
P 0 P
generalizationerror. Next,weestimateσ = cσ ,wheretheplateauvarianceσ isderivedfrom
P J P
numericalsimulationsofR∗(1)attheplateau.
FortheexampleshowninFigure4, wesetα = 300andfindσ ≈ 0.000279, withc ≈ 0.0279
0 P
(sinceσ = 0.01isgiven),B ≈ 0.33,andD ≈ 2.47. Fortheescapetime,wefindτ ≈ 239by
J esc
averagingthediagonaltermsofT(1) toobtainT(1)andusingtheaveragedsumoftheoff-diagonal
nm
entriesinordertoestimateD(1)andD(2). Finally,weobtainα −α ≈2751.
P 0
Thisprocedureprovidesvaluableinsightintohowtheplateaulengthbehaveswithrespecttovarious
parameters.Figure19showsthegeneralizationerrorbasedonthesolutionofthedifferentialequations
andpresentsadditionalexamplesfortheplateaulengthestimation.
C.5 ASYMPTOTICSOLUTION
Here, we want to investigate how the generalization error converges to its asymptotic value in
more detail. For this, we consider the typical teacher configuration ⟨T(l)⟩ = δ T(l) since this
nm nm
configurationalreadycapturesthescalingbehaviorofthegeneralizationerror. Fortheasymptotic
fixedpointsoftheorderparameters,wefindR(l) =T(l)δ andQ(l) =T(l)δ .Here,wedistinguish
im im in ij ij ij
againbetweendiagonalandoff-diagonalentriesforR(l) = R(l)δ +S(l)(1−δ )andQ(l) =
im im im ij
Q(l)δ +C(l)(1−δ )asfortheplateaucase. Furthermore,welinearizedthedynamicalequations
ij ij
forsmallperturbationarounditsfixedpointR(l) =T(l)+r(l),S(l) =T(l)+s(l),Q(l) =T(l)+q(l),
andC(l) =T(l)+c(l).
Wefindthefollowingdynamicequations
r
√
r
d s 2 3 s
 = A   (118)
dαq 3πM aq
c c
38
g gPreprint
with r = r(i−1), s = s(i−1), q = q(i−1), c = c(i−1) and A = A˜ + B˜ + g˜C˜ and g˜ =
√ √i i i i a
η2 3( 45+5(M−1)) . The individual matrices can be written as Kronecker products A˜ = G ⊗
15πM
A , B˜ =H ⊗U, C˜ =F ⊗U with
1
 1 √ 3 0 0   −1 −√ 3 1 √ 3
√ 2 3 4 √2 4
G=   − √23 2 −√1 3 √0 2 √0 3  , H =   −0 2
3
−0 √ 23 √183 √0 23   , (119)
− 3 −2 3 2 0 0 3 0
4
 
0 0 0 0
 
 0 0 0 0 
 
 
F = f f f f  (120)
 −2 − √1 3 1 1√3
 
 3 2 3
 
f f f f f f f f
 − √1 4 − 5 6 1√4 5 6
3 2 2 3 4
√ √ 15(M −1) 15
where f = 6 + M − 2,f = 45 + 5(M − 1),f = ,f = ,f =
1 2 3 b 4 b 5
√ 5
4(M −2) 6+3M2−15M +26,f = . Thereby,A andU arethesamematricesasforthe
6 b 1 1
linearactivationfunctioncase. Therefore,thelinearizedversionofthedynamicalequationforthe
non-linearactivationfunctionresemblesthedynamicalequationforthelinearactivation. However,
weencounteranadditional"perturbation"byB˜,whereasC˜ describestheinfluenceofhigher-order
termsinthelearninglearningrate. Moreover,comparedtothelinearcase,thedifferentialequation
hasmoreadditionalvariablesduetocorrelationsbetweendifferentstudentandteachervectors. In
ordertoanalyzethebehaviorofthedynamicalsystem,weneedtodeterminetheeigenvaluesand
eigenvectorsofallsub-matrices. Here,weanalyzethesystemforfirstorderinthelearningrateO(η)
neglectingthecontributionbyg˜C˜.
√ √
TheeigenvaluesofGareλ =2− 3witheigenvectorv =(0,0,1,−1)⊤,λ =2+ 3
G,1 √ G,1 G,2
witheigenvectorv = (0,0,1,1)⊤,λ = 1− 1 3witheigenvectorv = (1,−1,2,−2)⊤,
√ G,2 G,3 2 G,3
λ =1+ 1 3witheigenvectorv =(1,1,2,2)⊤. TheeigenvaluesofA werealreadystudied
G,4 2 G,4 1
inSubsectionB.1andarethenegativeeigenvaluesofthedatacovariancematrix−λ witheigenvec-
k
torsv summarizedbythematrixV (cf. Eq. (33)). SinceA˜isofblockmatrixstructureexpressed
k
byaKroneckerproduct,weobtainforitsspectrumλ =λ λ andcorrespondingeigenvectors
A˜ G A1
v =v ⊗v forwhichwemultiplyeacheigenvalueofA witheachofG. Thus,weobtainfour
A˜ G A1 1
differentgroupsofeigenvaluesforA˜ andintotal4Leigenvalues. Thefirstgroupisobtainedby
√
multiplyingthefirsteigenvalueofGwithalleigenvaluesofA leadingtoλ(1) =−(cid:0) 2− 3(cid:1) λ
1 A˜,k k
with eigenvector v(1) = (0,0,v ,−v )⊤. With the same procedure, we obtain for the other
A˜,k
√
k k
√
groups λ(2) = −(cid:0) 2+ 3(cid:1) λ with eigenvector v(2) = (0,0,v ,v )⊤, λ(3) = (cid:0) 1− 1 3(cid:1) λ
A˜,k k A˜,k k k
√
A˜,k 2 k
witheigenvectorv(3) = (v ,−v ,2v ,−2v )⊤, andλ(4) = −(cid:0) 1+ 1 3(cid:1) λ witheigenvector
A˜,k k k k k A˜,k 2 k
v(4) = (v ,v ,2v ,2v )⊤. The upper index for the eigenvalues and -vectors indicates the
A˜,k k k k k
correspondinggroup.
√
The eigenvalues of H are λ = 1 − 43 with eigenvector v =
H,1 3 12 H,1
(cid:16) 1,−√ 3(cid:0)√ 43+4(cid:1) ,2,−2√ 3(cid:0)√ 43+4(cid:1)(cid:17)⊤ , λ = 1 + √ 43 with eigenvector v =
9 9 H,2 3 12 H,2
(cid:16) 1,√ 3(cid:0)√ 43+4(cid:1) ,2,2√ 3(cid:0)√ 43+4(cid:1)(cid:17)⊤
,andλ =0witheigenvectorsv
=(cid:16) 1,0,0,2√ 3(cid:17)⊤
9 9 H,3 H,3 9
and v = (0,1,0,1)⊤. For the matrix U, we have just one eigenvalue distinct from zero
H,4
λ = T(2) witheigenvectorusinceU hasrankone. Theremainingeigenvectorsaregivenby
U,1
39Preprint
v = e forl ∈ (1,3,4,...,L)andespeciallyl ̸= 2. Thus, weobtaintwodifferenteigenvalues
U,l l
distinct from zero and 4L − 2 zero eigenvalues for B˜. The eigenvalues distinct from zero are
λ = (cid:16) 1 − √ 43(cid:17) T(2) and v = (cid:16) u,−√ 3(cid:0)√ 43+4(cid:1) u,2u,−2√ 3(cid:0)√ 43+4(cid:1) u(cid:17)⊤ and
B˜,1 3 12 B˜,1 9 9
λ = (cid:16) 1 + √ 43(cid:17) T(2) and v = (cid:16) u,√ 3(cid:0)√ 43+4(cid:1) u,2u,2√ 3(cid:0)√ 43+4(cid:1) u(cid:17)⊤ . Then, we
B˜,2 3 12 B˜,2 9 9
(cid:16) √ (cid:17)⊤
have two eigenvectors with zero eigenvalue v = u,0,0,2 3u and v = (0,u,0,u)⊤.
B˜,3 9 B˜,4
Furthereigenvectorshavethestructuree ⊗v ,e ⊗v ,e ⊗v ande ⊗v .
l H,1 l H,2 l H,3 l H,4
Strictlyspeaking,noneoftheeigenvectorsofA˜ andB˜ arethesameandwecannotcalculatethe
spectrumoftheirsumdirectly. However,wenoticethatB˜ hasjusttwoeigenvaluesdistinctfrom
zeroandthatitscorrespondingeigenvectorsv andv areofthesamestructureasthelasttwo
B˜,1 B˜,2
groupsofA˜namelyv(3)andv(4). Foreachofthevectors,thethirdandthefourthcomponentsare
A˜ A˜
twiceaslargeasthefirstandthesecondone. Therefore,onlytheeigenvaluesofthelasttwogroups
ofA˜ areinfluencedbyaddingthematrixB˜ leadingtotheeigenvectorsofA withthestructure
a
(cid:16) (cid:17)⊤ (cid:16) (cid:17)⊤
v(3) = z(3),w(3),2z ,2w(3) and v(4) = z(4),w(4),2z(4),2w(4) with vectors z(3),
Aa,k k k k k Aa,k k k k k k
w(3), z(4) and w(4) that has to be determined. Moreover, since the other eigenvalues of B˜ are
k k k
zero,theeigenvaluesofthefirstandsecondgroupλ(1) andλ(2) remainthesameforA asforA˜.
A˜,k A˜,k a
However,thecorrespondingeigenvectorsarenolongeranalyticallydeterminableandwehavetorely
onnumericalsolutions. Alltheseclaimsforthespectraandeigenvectorsareinexcellentagreement
withnumericalexperiments.
Next,weTaylorexpandthegeneralizationerroruptofirstorderinthesmallperturbationparameters
r(1),s(1),q(1)andc(1). Wefind
1 (cid:16) √ √ (cid:17)
ϵ = 2 3q(1)−4 3r(1)+3(M −1)c(1)−6(M −1)s(1) . (121)
g 6π
Fromthisexpansion,weobservethattheeigen-directionsv(3) andv(4) donotcontributetothe
Aa,k Aa,k
generalizationerrorinfirst-ordersincetheircomponentscancelout. Afterinsertingtheexpressions
forthefirstandsecondgroupsofeigenvectors,weobtain
ϵ =
1 (cid:88)N g(1)e−a(2−√ 3)λkα(cid:16) 2√
3v(1)
−4√
3v(1)+3(M −1)v(1) −6(M −1)v(1)
(cid:17)
g 6π k k,2L+2 k,2 k,3L+2 k,L+2
k=1
√ (cid:16) √ √ (cid:17)
+g(2)e−a(2+ 3)λkα 2 3v(2) −4 3v(2)+3(M −1)v(2) −6(M −1)v(2) ,
k k,2L+2 k,2 k,3L+2 k,L+2
(122)
√
where v(1),v(2) ∈ R4L are the eigenvectors of A to the eigenvalues −(cid:0) 2− 3(cid:1) λ
and −(cid:0)
2k +√k
3(cid:1) λ , respectively. Thereby, g(1)
a
= (cid:80)4L(cid:16)(cid:0) V(1)(cid:1)−1(cid:17) x , g(2)
=k
k k l l k
kl
(cid:80)4L(cid:16)(cid:0) V(2)(cid:1)−1(cid:17)
x where V(1) and V(2) containing the first and second group of eigenvec-
l l
kl
torsv(1) andv(2),respectivelyandx = (r(α ),s(α ),q(α ),c(α ))⊤ ∈ R4L issomereference
k k 0 0 0 0
pointatarbitrarychosenα intheasymptoticphase.
0
Figure20comparesthegeneralizationerrorderivedfromthefirst-orderTaylorexpansioninEq. (121)
whereweobtaintheparametersr,q,s,cbysolutionsofthedifferentialequationsandourtheoretical
resultsbasedonEq. (122). Forboth,weusethesameinitialconditions,withα =1000asinthe
0
numericalsolutions. Thecomparisonshowsexcellentagreement. Thesmalldiscrepanciesbetween
the graphs arise from the arbitrariness of α and the chosen initial conditions. Similar to linear
0
activationfunctions, weobserveaslowdowninconvergencetowardsperfectgeneralizationasL
increases,whicheventuallyleadstoatransitionfromexponentialconvergencetopower-lawscaling.
WesolveEq. (122)usingtheJuliaprogramminglanguage;however,weencounterlimitationsin
increasingLduetoconstraintsinnumericalprecision(seeSectionD).
40Preprint
10 1 L=3 4×104
10 2 L=9 3×104
10 3
10 3 2×104
10 4
10 5 10 4 Simluation 10 4 Simluation
Theory Theory
0 500 10001500200025003000 0 200 400 600 800 1000 0 200 400 600 800 1000
Figure 20: Left: Generalization error as a function of α obtained via the solution of differential
equationsoftheorderparametersuptoO(η)andK = M = 2,η = 0.25,σ = 0.01,N = 9000
J
andβ =1. CenterandRight: Comparisonofthefirst-orderTaylorexpansionofthegeneralization
errorbasedonEq. (121)fornumericallyobtainedr(1),s(1),q(1)andc(1)(Simulation)withEq. (122)
(Theory) where the initial conditions r(1)(α ),s(1)(α ),q(1)(α ) and c(1)(α ) are obtained from
0 0 0 0
simulations. Thereby,wechooseα =1000andparameterizeα˜ =α−α . Center: L=3. Right:
0 0
L=9.
D REMARKS ON NUMERICAL SOLUTIONS
EvaluatingalargenumberofdistincteigenvaluesbecomescomputationallychallengingasLincreases.
(cid:68) (cid:69)
Theexpectationvalueoftheteacher-teacherorderparametersisgivenby T(l) =δ 1 Tr(Σl).
nm nmN
Inthiscontext,thehighestordertraceterminthedifferentialequationsisL−1,andforlargeL,
wecanapproximateTr(Σl)asλL−1(cid:80)L 1 ≈(λ )L−1. Sinceλ scaleswithLforlarge
+ i=1 i(1+β)(L−1) + +
valuesofL,theexpectationvaluesincreaseina’super-exponential’mannerwithL. Thisgrowth
also applies to the standard deviation of the off-diagonal entries of T(L−1), further complicating
nm
numericalevaluationsasLgrows.
Asaresult,numericalinvestigationsarerestrictedtosmallvaluesofL. Thislimitationappliesto
solvingthedifferentialequations,evaluatingfixedpoints,andanalyzingthegeneralizationerrorin
theasymptoticphase. Forinstance,toevaluateEq. (17)andgenerateFigures10through13,we
utilizedJulia,ahigh-levelscriptinglanguage,witharbitraryprecisionarithmetic.
41
g g g