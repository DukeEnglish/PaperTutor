EDGE AI COLLABORATIVE LEARNING:
BAYESIAN APPROACHES TO UNCERTAINTY ESTIMATION
GlebRadchenko VictoriaAndreaFill
SiliconAustriaLabs FHJoanneum
Sandgasse34,8010Graz,Austria AltePoststraße149,8020Graz,Austria
gleb.radchenko@silicon-austria.com victoria.fill@edu.fh-joanneum.at
October14,2024
ABSTRACT
RecentadvancementsinedgecomputinghavesignificantlyenhancedtheAIcapabilitiesofInternet
of Things (IoT) devices. However, these advancements introduce new challenges in knowledge
exchangeandresourcemanagement,particularlyaddressingthespatiotemporaldatalocalityinedge
computingenvironments. Thisstudyexaminesalgorithmsandmethodsfordeployingdistributed
machinelearningwithinautonomous,network-capable,AI-enablededgedevices. Wefocusondeter-
miningconfidencelevelsinlearningoutcomesconsideringthespatialvariabilityofdataencountered
byindependentagents. Usingcollaborativemappingasacasestudy,weexploretheapplicationof
theDistributedNeuralNetworkOptimization(DiNNO)algorithmextendedwithBayesianneural
networks (BNNs) for uncertainty estimation. We implement a 3D environment simulation using
theWebotsplatformtosimulatecollaborativemappingtasks,decoupletheDiNNOalgorithminto
independentprocessesforasynchronousnetworkcommunicationindistributedlearning,andintegrate
distributeduncertaintyestimationusingBNNs. OurexperimentsdemonstratethatBNNscaneffec-
tivelysupportuncertaintyestimationinadistributedlearningcontext,withprecisetuningoflearning
hyperparameterscrucialforeffectiveuncertaintyassessment. Notably,applyingKullback–Leibler
divergence for parameter regularization resulted in a 12-30% reduction in validation loss during
distributedBNNtrainingcomparedtootherregularizationstrategies.
Keywords EdgeComputing·EdgeLearning·DistributedMachineLearning·FogComputing·MachineLearning·
IoT
1 Introduction
Recent advancements inedge computing have significantly increasedthe computational capabilitiesof Internetof
Things(IoT)devices,enablingmorecomplexdataprocessingatthenetworkedge[1,2,3]. Thisevolutioninedge
devicecapabilitiesintroducesnewchallengesandopportunitiesindistributeddataprocessingandanalysis.
Integratingadvancedprocessingcapabilitiesintoedgedevicesaimstooptimizelocaldatahandlingandenhancedevice
autonomy. Thisapproachisparticularlyrelevantwhenlow-latencyprocessing,dataprivacy,andnetworkefficiencyare
critical. However,implementingmachinelearning(ML)algorithmsonedgedevicesintroducesseveralchallenges:
• DistributedLearning: Howcanweeffectivelyimplementdistributedlearningonedgedevices,transitioning
from traditional inference-only models to active participation in the learning process while ensuring data
privacyandefficientknowledgesharing?[4]
• ResourceManagementandCommunicationEfficiency: Howdoweoptimallymanagelimitedresources
(e.g.,power,processingcapabilities,networkbandwidth)onedgedeviceswhilemaintainingefficientcommu-
nicationfordataandmodelupdatesindynamicnetworkconditions?[5,6]
4202
tcO
11
]GL.sc[
1v15680.0142:viXraAPREPRINT-OCTOBER14,2024
• Spatio-TemporalLocalityandNon-IIDData: Whatstrategiesmosteffectivelyincorporatelocalizedand
non-IIDdatatoimproveaccuracyindistributedMLmodels? Evaluatingtheuncertaintyatthelevelofboth
theindividualagentandtheaggregatedmodelisessential.[7,8]
1.1 Contribution
ThisstudyfocusesonalgorithmsandmethodsfordeployingdistributedMLwithinautonomous,network-capable,
sensor-equipped,AI-enablededgedevices.Specifically,itaddressesdeterminingconfidencelevelsinlearningoutcomes,
consideringthespatialvariabilityofdatasetsencounteredbyindependentagents. Toaddressthisissue,weinvestigate
thepotentialoftheDistributedNeuralNetworkOptimization(DiNNO)algorithm[9],aimingtoextenditfororganizing
distributeddataprocessinganduncertaintyestimationusingBayesianneuralnetworks(BNN)[7].
OurresearchexaminestheinteractionsofAI-enablededgedeviceswithinacollaborativemappingtask. Tomeetour
objectives,weneedtoaddressthefollowingtasks:
1. DecouplingtheDiNNOalgorithmimplementationintoindependentprocesses,enablingasynchronousnetwork
communicationfordistributedlearning.
2. IntegratingdistributeduncertaintyestimationintotheresultingneuralnetworkmodelsbyapplyingBNNsand
evaluatingtheapplicabilityofBNNsinthecontextofdistributedlearning.
3. Implementingtheproposedapproacheswithinacasestudy: simulationofrobots,augmentedwithLiDAR
sensorsforenvironmentalmapping,navigatinga3DenvironmentusingtheWebotsplatform.
4. EvaluatingtheeffectivenessofdistributeduncertaintyestimationusingBNNsinadistributedlearningcontext,
includingtuningoflearninghyperparametersandapplyingKullback–LeiblerdivergenceforNNparameter
regularization.
Thispaperisanextendedversionoftheworkentitled"UncertaintyEstimationinMulti-AgentDistributedLearningfor
AI-EnabledEdgeDevices"[10],whichwaspresentedatthe14th InternationalConferenceonCloudComputingand
ServicesScience(CLOSER).Thisextendedversionexpandsuponthepreviousworkbyprovidinganexpandedanalysis
ofdistributedMLalgorithmsandevaluatingthefeasibilityofanonlinelearningapproachwithinthecollaborative
mappingcase.
1.2 Outline
The remainder of this paper is structured as follows. In Section 2, we provide an overview of the related work in
distributededge-basedMLmethodsandBayesianNeuralNetworks. Section3presentsthecollaborativemapping
casestudy. Sections4and5elaborateonthedistributededgelearningapproachanduncertaintyestimationmethod.
InSection6, wedescribetheimplementationandevaluationoftheproposedmethods. Finally, Section7presents
conclusionsandsuggestionsforfuturework.
2 RelatedWork
2.1 EdgeLearningMethods
EdgecomputinghasrecentlyenabledlocaldevicestoperformAItasksthatwereoncelimitedtocentralizedsystems.
Initially,thesedevicesonlysupportededge-basedinference,usingpre-trainedmodelstoprocesslocaldata[11,12].
However,withthedevelopmentofEdgeLearning(EL),thesedevicescannowbothtrainandupdatemodelslocally[13].
Byenablingdataprocessingdirectlywhereitisgenerated,ELimprovesadaptabilityandpersonalization. Moreover,
becausethedataremainsonedgedevicesandisnottransferredtocentralizedcloudservers,ELreducesbandwidth
usageandmitigatesprivacyrisks[14,15,16]. Thisadvancementnotonlyenhancesthecapabilitiesofedgedevicesbut
alsomarksasignificantstepforwardinthefieldofdistributedML[17].
DistributedMLalgorithmscanbecategorizedbasedontheircommunicationmechanisms,whichprimarilyinvolve
theexchangeofmodelparameters,modeloutputs,orhiddenactivations. Theseexchangesaretypicallyfacilitated
throughpeer-to-peerorclient-serverarchitectures[6]. Amongthevariousapproachestoorganizingdistributedand
edgelearning,themostwidelyusedmethodsincludeFederatedLearning,FederatedDistillation,SplitLearning,anda
rangeofdecentralizedlearningtechniquesbasedontheAlternatingDirectionMethodofMultipliersanditsderivatives.
2APREPRINT-OCTOBER14,2024
2.1.1 FederatedLearning(FL)
(see Fig. 1a) orchestrates the periodic transmission of local training parameters from edge nodes to the Federated
LearningServer. TheserversendstheglobalmodelP toalledgenodes. Eachedgenodethentrainsthemodelon
itslocaldata[X ,Y ]collectedfromsensors,updatingitslocalparametersp . Theselocalparametersarereturned
i i i
totheserverandaggregated(AGGREGATE(p 1..n))toupdatetheglobalmodelP. Thisupdatedglobalmodelisthen
distributedbacktotheedgenodes,andtheprocessrepeatsiteratively.
(a)FederatedLearning
(b)FederatedDistillation
Figure1: Graphicalrepresentationof(a)FederatedLearningand(b)FederatedDistillationprocess.
FLhasemergedasapromisingstrategyforEL,offeringaviablesolutiontovariouschallengesinedgecomputing
environments,includingthepotentialtopreservedataprivacybyavoidingtheneedforrawdataexchange. Originally
proposedtoenabledistributedlearningusingmobiledevices[18],FLcanenhancecommunicationefficiencybyallowing
foradjustabletransmissionintervals,whichhelpstooptimizeresourceusageandreducebandwidthdemands[4]. The
authorsof[19]furtheremphasizethatFLaddressesissuessuchasunwantedbandwidthloss,dataprivacyconcerns,and
legalcompliance. TheyhighlightthatFLfacilitatestheco-trainingofmodelsacrossdistributedclients—frommobile
phonestoautomobilesandhospitals—viaacentralizedserverwhileensuringthatthedataremainslocalized. The
3APREPRINT-OCTOBER14,2024
authorsof[20]proposeanextensionoftheFLmodelcalledFedFogdesignedtoenableFLoverawirelessfog-cloud
system. Theauthorsaddresskeychallengessuchasnon-identicallydistributeddataanduserheterogeneity. TheFedFog
algorithmperformslocalaggregationofgradientparametersatfogserversandaglobaltrainingupdateinthecloud.
On the other hand, recent research has highlighted several limitations of FL, particularly in terms of privacy and
robustness. For instance, FL is vulnerable to malicious servers and actors, which can compromise the privacy of
sensitiveinformation[21].Furthermore,theworkofLyuetal.[22]discusseshowthesevulnerabilitiesmightundermine
theprivacyguaranteestypicallyassociatedwithFL.
2.1.2 FederatedDistillation(FD)
(seeFig.1b)takesadistinctapproachtocommunicatingknowledgeobtainedduringlocaltraining. Unliketraditional
FL,whichinvolvestransmittingmodelparameters,FDleveragesmodeldistillation. Inthistechnique,knowledgeis
transferredfromonemodel(the"teacher")toanother(the"student")bytrainingthestudentontheteacher’soutputs
(soft-label predictions) rather than the raw data. A widely known application of knowledge distillation is model
compression,throughwhichtheknowledgeofalargepre-trainedmodelmaybetransferredtoasmallerone[23].
Instead of sharing the parameterization of locally trained models, the knowledge in FD is communicated through
soft-label predictions on a public distillation dataset [24]. FD orchestrates an iterative knowledge-sharing process
betweenacentralserverandmultipleedgenodes. Atthestartofeachround,theserversendsaggregatedsoftlabels
YpubonapublicdatasetXpubtotheparticipatingedgenodes. Eachedgenodethenperformsthefollowingsteps:
1. Distillation: Thenodeupdatesitslocalmodelusingthesoftlabelsreceived,incorporatingglobalknowledge.
2. LocalTraining: Thenodefurtherrefinesthedistilledmodelonitsprivatelocaldata[X ,Y ],improvingthe
i i
localmodel’sperformance.
3. Prediction: ThenodegeneratesnewsoftlabelsYpubbyrunningtherefinedmodelonthepublicdatasetXpub.
i
4. Communication: ThesenewlygeneratedsoftlabelsYpubarereturnedtotheserver.
i
Theserveraggregatesthesoftlabelsreceivedfromallparticipatingnodes(AGGREGATE[Xpub,Ypub])toupdatethe
1..n
globalpublicdataset. Thisiterativeprocessimprovescollectiveknowledgewhileensuringthatrawlocaldataremains
private.
Thesoft-labelexchangeinFDsignificantlyreducesthedatathatneedstobetransferredduringtrainingcomparedtothe
exchangeoffullmodelparametersinFL.Accordingtotheauthorsof[25],FDcanreducecommunicationcostsbyupto
94.89%comparedtotheclassicalFedAvg[18]FLmethod. AnotheradvantageofFDisitsflexibilityinaccommodating
different model architectures on edge nodes. Since model parameters are not exchanged between the edge nodes
andtheFDserver,themodelsontheserverandtheedgenodescanvarysignificantlyinstructure. Additionally,FD
effectivelyaddresseschallengesincollaborativelearningundernon-independentandidenticallydistributed(non-IID)
dataconditions. TheSelective-FDalgorithmproposedin[26]demonstrateshighaccuracyeveninseverenon-IID
environments,enablingacommunication-efficientandheterogeneity-adaptiveEL.
2.1.3 SplitLearning(SL)
(seeFig.2a)partitionsamulti-layerneuralnetwork(NN)intosegments,enablingthetrainingoflarge-scaledeepNNs
thatexceedthememorycapacitiesofasingleedgedevice[27]. ThisapproachdividestheNNintotwosegments: a
lowerNNsegmentp ,whichresidesontheedgedevicescontainingtherawdata,andanupperNNsegmentP ,
i server
hostedonaparameterserver[28]. TheNNcutlayerservesastheboundarybetweenthesesegments.
Duringtheforwardpass,theedgedevicescomputetheactivationsattheNNcutlayerandtransmittheseactivations
a ,alongwiththetruelabelsY ,totheparameterserver. Theparameterserverthenusestheseactivationsasinputsto
i i
theupperNNsegmenttocontinuetheforwardpassandcomputethefinalpredictionsYpred. Subsequently,theserver
i
calculatesthelossL bycomparingthepredictionswiththetruelabelsandinitiatesthebackwardpass.
i
Thegradients∇a ,computedattheNNcutlayerduringbackpropagation,arereturnedtotheedgedevices. Theedge
i
devicesthenusethesegradientstoupdatetheweightsofthelowerNNsegments,completingthetrainingloop. This
iterativeprocesscontinuesuntilthemodelconverges.
WhilethisdescriptioncoversthemostbasicformofSplitLearning,it’simportanttonotethatseveralSLimplementations
differinhowtheNNispartitionedanddistributedbetweentheedgeandservernodes[29,30]. Thesevariantsaddress
differentchallengesandtrade-offs,particularlyregardingcommunicationefficiency,whichremainsanactiveareaof
research[31].
4APREPRINT-OCTOBER14,2024
(a)SplitLearning
(b)ADMM-basedP2Plearning
Figure2: Graphicalrepresentationof(a)SplitLearningand(b)adecentralizedlearningprocessusingtheAlternating
DirectionMethodofMultipliers.
5APREPRINT-OCTOBER14,2024
2.1.4 AlternatingDirectionMethodofMultipliers(ADMM)
is a general-purpose decentralized convex optimization algorithm, particularly well suited to problems in applied
statisticsandmachinelearning. Ittakestheformofadecomposition-coordinationprocedure,inwhichthesolutions
to small local subproblems are coordinated to find a solution to a large global problem [32]. Distributed machine
learningalgorithmsderivedfromADMM,suchasDistributedNeuralNetworkOptimization(DiNNO)[9]andGroup
AlternatingDirectionMethodofMultipliers(GADMM)[33],enabledecentralizedlearningwithouttheneedfora
coordinatingserverbyallowingdirectcommunicationbetweentheedgenodesinapeer-to-peer(P2P)manner(see
Fig.2b). ThestrongconvergencepropertiesofADMMensurethatalllearningP2Pagentseventuallyreachaconsensus
onthemachine-learningmodelparameters.
Inthesemethods,eachedgenodeindependentlytrainsitslocalmodelparametersp onitsprivatedata[X ,Y ]collected
i i i
fromsensors.Thetrainedparametersp arethensharedwithneighboringedgenodes.Eachnodereceivestheparameters
i
p fromitspeers,whicharethenusedtoregularizeitsmodelbyincorporatingtheknowledgefromtheneighboring
k..l
nodes. Thisprocessisrepeatedasnodesexchangeandregularizetheirmodelparameters.
OneofthecriticalissueswithsuchP2Plearningmethodsisthecommunicationoverhead,whichisproportionalto
thenumberofmodelparametersandlearningagents. ThisoverheadcanlimittheeffectivenessofADMM-derived
methodsinsupportingdeepNNs[33]. However,despitethislimitation,ADMM-derivedmethodsofferadecentralized
alternativetoFL,particularlywhenacentralserverisunavailableorimpractical.
2.2 UncertaintyEstimationandBayesianNeuralNetworks
2.2.1 BayesianNeuralNetworks
InaconventionalNNarchitecture,alinearneuronischaracterizedbyaweight(w),abias(b),andanactivationfunction
(f ). Givenaninputx,asinglelinearneuronperformsthefollowingoperation:
act
y =f (w·x+b) (1)
act
whereyistheoutputoftheneuron.
However,asweexploremorecomplexanduncertainenvironments,thedeterministicnatureofclassicalNNsbecomesa
limitation. TheabilityofNNstogeneralizetodatathatliesoutsidethetrainingdistributionremainsanareaofongoing
research. Thepotentiallackofgeneralization,coupledwiththeinherentinstabilityofNNs,canleadtotheappearance
offalsestructuresintheirpredictions. Thesefalsestructures,oftenreferredtoashallucinations,mayoccurwhenthe
reconstructionmethodincorrectlyestimatespartsoftheinitialdatasetthateitherdidnotcontributetotheobserved
measurementdataorcannotberecoveredinastablemanner[34]. Apromisingapproachtoaddressingthesechallenges
isusingBayesianneuralnetworks,whichincorporateuncertaintyestimatesdirectlyintotheirpredictions.
BayesianNeuralNetworks(BNNs)adoptaBayesianframeworktotrainneuralnetworkswithstochasticbehavior[7].
Insteadofrelyingonfixed,deterministicvaluesforweightsandbiases,BNNsemployprobabilitydistributions,typically
denotedasP(w)forweightsandP(b)forbiases. ThesedistributionsareoftenapproximatedbyGaussiandistributions,
with the mean and standard deviation determined from the training data. As a result, a Bayesian neuron does not
produceasingleoutputbutratherarangeofpossiblevalues. TheoperationofaBayesianLinearneuroncanthusbe
describedas:
P(y|x)=f (P(w)×x+P(b)) (2)
act
InthecontextofBNNs,theGaussiandistributionsforbothweightsandbiasesarecharacterizedbyamean(µ)and
astandarddeviation(σ). Specifically,theweightdistributionP(w)ismodeledasaGaussianwithameanw anda
µ
standarddeviationw ,where:
σ
w =log(1+ewρ) (3)
σ
The parameter w ensures that the standard deviation remains positive. Similarly, the bias distribution P(b) is
ρ
representedbyaGaussianwithameanb andastandarddeviationb ,definedas:
µ σ
b =log(1+ebρ) (4)
σ
IntheforwardpassofaBayesianneuron,weightsandbiasesaredrawnfromtheirrespectiveprobabilitydistributions
foreachneuron. Thesesampledvaluesarethenutilizedtocalculatetheneuron’soutput. Throughoutthetraining
process,theparametersw ,w ,b ,andb areadjustedtoenhancetheoverallperformanceofthenetwork.
µ ρ µ ρ
Unliketraditionalneuralnetworks,whererepeatedforwardpassesyieldthesameoutput,BNNsgeneratedifferent
outputswitheachforwardpassduetothestochasticnatureofthesampledweightsandbiases. Aftermultiplepasses,
6APREPRINT-OCTOBER14,2024
themeanandstandarddeviationoftheoutputscanbecalculated,effectivelycapturingtheuncertaintyinthepredictions.
ThisabilitytoquantifyuncertaintyprovidesBNNswiththeadvantageofofferingdetailedinsightsintohowconfident
themodelisabouteachprediction.
2.2.2 Kullback-LeiblerDivergence
Kullback-Leibler Divergence (KL Divergence) [35, 36] allows us to measure the difference between the Gaussian
distributionsrepresentingtheparametersinaBNN.KLDivergencequantifiesthedissimilaritybetweentwoprobability
distributionsandiscalculatedas:
(cid:90) (cid:18) g(x)(cid:19)
D (g||h)= g(x)log dx (5)
KL h(x)
whereg(x)andh(x)aretwoprobabilitydensityfunctionsoverthesamedomain.
Asexplainedin[37],whenthedistributionsN (µ ,σ )andN (µ ,σ )arebothnormal,equation(5)maybereduced
0 0 0 1 1 1
to:
1(cid:20) (cid:18) σ2(cid:19) σ2+(µ −µ )2 (cid:21)
D (N ||N )= log 1 + 0 0 1 −1 (6)
KL 0 1 2 σ2 σ2
0 1
InthecontextofBNNs,KLDivergenceisappliedtoquantifythedeviationofthenetwork’sparameterdistributions
fromapredefinedpriordistribution. ThetotallossinaBNNistypicallyexpressedas:
total =base +kl ×kl (7)
loss loss weight loss
wherebase correspondstothestandardlossfunction,suchasBinaryCross-EntropyorMeanSquaredError;the
loss
termkl representsahyperparameterthatcontrolstheinfluenceofuncertaintyonthemodel’spredictions;and
weight
kl representsthesumoftheKLDivergencebetweenthedistributionsoftheBNNparametersN (µ ,σ )anda
loss 0 0 0
specifiednormaldistributionN (µ ,σ ).
1 1 1
3 CollaborativeMappingCaseStudy
ForourcasestudyonedgeAIcollaborativelearning,wefocusedonaddressingacollaborativeenvironmentmapping
problemusinganetworkofroboticedgedevices. Theseautonomousrobots,eachstartingfromdifferentlocations,work
togethertoconstructacomprehensivemapoftheirsurroundingsbyutilizingonboardsensorsandsharingknowledge
witheachother.
EachrobotisdesignedtoupdateitslocalMLmodelwithnewdataacquiredfromitssensorswhilealsocommunicating
withotherrobotsviaanetworkinterface. ThearchitectureoftheseAI-enablededgedevicesisprovidedinFigure3.
Eachedgedeviceisequippedwithspecializedcomputationalcorestailoredtohandlespecifictasks:
• Real-timecore:Handlesimmediatesensordataprocessingandcontrolstheactuators,ensuringtimelyresponses
toenvironmentalchanges.
• General-purposecore: Managesoveralldeviceoperationsandcoordinatesbetweendifferentcomponents.
• AIcore:Supportstheedgetrainingcyclebyprocessingthedata,updatingthemodel,andfacilitatingknowledge
exchangewithotherdevicesinthenetwork.
These components work together to ensure that each robot can process data locally and interact with its peers to
contributetoaglobalunderstandingoftheenvironment.
To address the distributed machine learning challenge in our study, we employed the Distributed Neural Network
Optimization(DiNNO)algorithm[9]. DiNNOfacilitatesdecentralizedlearningbyallowingeachrobottooptimize
itsNNmodellocallyanditerativelysharethelearnedparameterswithitsneighbors. Unlikecentralizedmethodsthat
requireaggregatingalldataatacentralnode,DiNNOoperatesoverameshnetworkwhererobotscommunicatedirectly,
ensuringrobustnessagainstnodefailuresandpreservingdataprivacy.
DiNNObuildsontheADMMalgorithm,allowingittoefficientlyconvergetoaconsensusontheNNparametersacross
allrobots. Thealgorithm’sabilitytoworkwithtime-varyingcommunicationgraphsandstreamingdataisparticularly
suitedtothedynamicanddistributednatureofmulti-robotsystems. EachrobotrefinesitsNNmodelusinglocalsensory
inputs. Then, itexchangesupdatedparameterswithitsneighbors, implementingacollectivelearningprocessthat
eventuallyalignsthemodelsacrosstheentirenetwork.
Inourexperiment,weutilizedtheCubiCasa5Kdataset[38]togeneratefloorplansfortheenvironmentmappingtask.
Figure4illustratesthepathstakenbytherobotsduringthemappingprocess. Thesolidcoloredlinesrepresentthe
7APREPRINT-OCTOBER14,2024
Figure3: ComponentsofAI-Enablededgedevice[10].
trajectoriesofvariousrobotsastheynavigatetheenvironment,withtheredsolidlinehighlightingaparticularrobot’s
pathasitexploresadesignatedarea. ThedottedlinesurroundingtherobotindicatestherangeofitsLiDARsensor,
whichactsasasourceforthetrainingdata.
Figure4: Visualizationoftheenvironmentmapincludingstartingpoints,explorationpathways,andLiDARrangefor
theroboticagentsasdescribedby[9]. Adaptedfrom[10].
4 PeersStateExchangeAlgorithm
TheoriginalimplementationoftheDiNNOalgorithmwasconstrainedbyseverallimitationsthatsignificantlyimpacted
itsapplicabilitytoedgeAIcollaborativelearning. Theoriginalframeworkfeaturedasequentialapproachtothelearning
processandacentralizedexperimentalarchitecture,whichcouldbenefitfromamoredistributeddesigntosimulate
8APREPRINT-OCTOBER14,2024
dataexchangebetter. Inthissetup,agentswerepartofamonolithicdatastructureprocessedwithinasingle,tightly
coupledcomputationalprocess. Thisdesignledto"quasi-agents"thatdirectlyaccessedeachother’smemoryduringthe
learningprocess,which,whileefficient,didnotfullyembracethepotentialofdistributedlearning.
Toovercometheselimitations, werestructuredtheDiNNOalgorithmimplementationtomakeitsuitableforedge
computingenvironments. Eachagentnowoperatesautonomously,independentlyprocessinglocalLiDARdataand
optimizing NN parameters. A key improvement in this updated approach is the introduction of an epoch-based
decentralizedconsensusalgorithm,whichhelpsagentsexchangeNNparameterswitheachotherinapeer-to-peerway
(seeAlgorithm1).
Algorithm1PeersStateExchange[10]
Require: MaxRound,Socket,Id,State
1: Initialize: Round,PeerComplete[],PeerState[]
2: Message←(State,0)
3: SEND(Socket,Message,Id)
4: whileRound<MaxRounddo
5: (Message,PeerId)←RECEIVE(Socket)
6: ifMessageisRoundCompletethen
7: PeerComplete[PeerId]← TRUE
8: else
9: ifRound<Message.Roundthen
10: FINISHROUND
11: endif
12: PeerState[PeerId]←Message.State
13: endif
14: if∀s∈PeerState,s̸=∅then
15: State←NodeUpdate(State,PeerState)
16: ∀s∈PeerState,s←∅
17: PeerComplete[Id]← TRUE
18: PeerState[Id]←State
19: Message←RoundComplete
20: SEND(Socket,Message,Id)
21: endif
22: if∀p∈PeerComplete,p= TRUEthen
23: FINISHROUND
24: endif
25: endwhile
26: functionFINISHROUND
27: ∀p∈PeerComplete,p← FALSE
28: Round←Round+1
29: Message.State←State
30: Message.Round←Round
31: SEND(Socket,Message,Id)
32: endfunction
Thepeersstateexchangealgorithmstartswiththefollowinginputs: themaximumnumberofsynchronizationepochs
(MaxRound),networksocket(Socket),uniquepeeridentifier(Id),andtheinitialstateoftheNNparameters(State).
Thealgorithmusestwodatastructurestokeeptrackofthecommunicationsprocess: PeerComplete[],whichmonitors
whethereachpeerhasfinishedaround,andPeerState[],whichstoresthecurrentNNparametersforeachpeer.
The algorithm revolves around the P2P exchange of two message types: State and RoundComplete. The
RoundCompletemessagesignalsthecompletionofaroundbyapeer,whiletheStatemessagecontainsthepeer’sNN
parametersstateforthecurrentround. IncludingtheRoundCompletemessagealongwitharoundfinalizationlogic
addressesthechallengesposedbynetworklatency—suchasout-of-ordermessages,delayedstatusupdates,andpoten-
tialdesynchronizationbetweenpeers. Tomitigatethelatencyissues,thealgorithmensuresthataRoundComplete
messageisdispatchedbyapeeronlyaftersuccessfullyreceivingallStatemessagesfromtheotherpeers. Thisensures
thatnopeerprogressestothesubsequentrounduntilallpeershavesynchronizedonthecurrentround,preventingthe
riskofdesynchronizationcausedbydelayedormissingmessages.
9APREPRINT-OCTOBER14,2024
When a peer receives a State message from a future round, the algorithm triggers the FINISHROUND function,
prompting the peer to align with the correct round. This mechanism manages out-of-order deliveries, ensuring
consistencyandsynchronizationacrossallagents.
Thisversionofthealgorithmassumesthatallmessageswilleventuallyreachtheirintendedrecipients,excludingthe
considerationofscenariosinvolvingagentmalfunctions,computationalhalts,orpermanentnetworkfailuresthatcould
resultinirreversiblemessagelossortotalcommunicationbreakdown.
5 DistributedUncertaintyEstimation
Toaddressuncertaintyestimationinthedistributedmappingproblem,wedevelopedaBNNincorporatingBayesian
LinearLayersintheneuralnetwork(seeFigure5).
Figure5: VisualizationoftheproposedcollaborativemappingBNNarchitecture.
ThearchitectureoftheBNNisdetailedasfollows:
• InputLayer(2): Theinputconsistsofcoordinatesx,y,representingaglobalpositiononthemap.
• SIRENLayer(256): Alayerwithasinusoidalactivationfunction,designedforNeuralImplicitMapping,as
describedby[39].
• 4xBayesianLinearLayers(256): FourBayesianLinearLayers,eachwith256nodesandactivatedbythe
ReLUfunction. Theselayersareprobabilisticandsupportuncertaintyestimation.
• OutputLayer(1): Alinearlayerwithonenode,activatedbytheSigmoidfunction. Anoutputof0indicates
emptyspace,whileanoutputof1indicatesoccupiedspace(e.g.,awall).
Unlikedeterministicnetworksthatproduceonlyasingleoutput,BNNsaredistinguishedbytheirabilitytoestimate
predictionuncertainty. Byconductingmultipleforwardpassestocomputethemeanandstandarddeviationofthe
outputs,BNNsprovidevaluableinsightsintothemodel’sconfidenceforeachmeshgridpoint. Tocorrectlyregularize
theBNNparametersduringthedistributedlearningphase,wedevelopedAlgorithm2,whichtakesintoaccountthe
specificrolesofthemedian(µ)andstandarddeviation(ρ)parametersofBNNneurons. ForregularizingtheBNN
ρ-parametersacrossmodelsofindividualactors,weemployKLDivergence,asdetailedinEquation(6).
10APREPRINT-OCTOBER14,2024
Algorithm2OptimizationofBNNParameters[10]
Require: Model,Optimizer ,Optimizer ,W ,W ,Iter,θ ,θ ,Duals ,Duals
µ ρ µ ρ regµ regρ µ ρ
1: fori←1toIterdo
2: ResetgradientsofOptimizer µandOptimizer ρ
3: PredLoss←COMPUTELOSS(Model)
4: θ µ,θ ρ ←EXTRACTPARAMETERS(Model)
5: Reg µ ←L2REGULARIZATION(θ µ,θ regµ)
6: Reg ρ ←D KL(θ ρ,θ regρ)
7: Loss µ ←PredLoss+⟨θ µ,Duals µ⟩+W µ×Reg µ
8: Loss ρ ←⟨θ ρ,Duals ρ⟩+W ρ×Reg ρ
9: UPDATEPARAMETERS(Optimizer µ,Loss µ)
10: UPDATEPARAMETERS(Optimizer ρ,Loss ρ)
11: endfor
6 ImplementationandEvaluationoftheEdgeAICollaborativeLearning
Tosimulatearealisticenvironmentforroboticexploration,weutilizedfloorplandatafromtheCubiCasa5Kdatasetto
generatethree-dimensionalinteriormodelsinSTLformat. ThesemodelsweresubsequentlyimportedintotheWebots
simulationplatform,whereTurtleBotrobotswereprogrammedtonavigatethroughthespace(seeFigure6). Inour
simulation,theTurtleBotswereequippedwithessentialsensors,includingLiDAR,whichintroducedrealisticsensor
noiseandmeasurementuncertainties.
Figure6:Visualizationofthe3DmodeloftheenvironmentgeneratedfromthefloorplanshowcasingaLiDAR-equipped
TurtleBotnavigatingthespaceinaWebotssimulation[10].
Theenvironmentwasdesignedtorepresentreal-worldconditions,allowingforthetestingofmappingalgorithms. The
experimentinvolvedlaunchingsevenindependentTurtleBotagentsthatgraduallycollectedinformationfromLiDAR
sensorswhileexploringavirtualinteriorspace. TurtleBotmovementwascontrolledbyaPython-basedscriptthat
guidedtherobotsthroughtheenvironment. Thiscontrollermanagedrobotorientationandmovementusingacompass
andGPSdata. Thestudyassumedthatallrobotshadaccesstoglobalpositioninginformation.
Agents’movementpathswerepredefinedtocreatesimulationprogramsfortheirinteriornavigation. LiDARsensor
datacollectionwassimulatedasaWebotsdatastreamduringnavigation. TheLiDARsensorscannedthesurroundings
andupdatedanoccupancygridmap,whereeachcellwasclassifiedasunknown,free,oroccupied. LiDARdatawas
transformedtofittheNeuralImplicitMappingframework,convertingrawscansintoapointsetwithvaluesranging
from0to1. Inthismapping,avalueof1indicatedthepresenceofawall,while0representedemptyspace.
Withintheexperiment’sframework,eachagentranasaseparatePythonprocess. Agentcommunicationwasimple-
mentedviadirectTCPconnectionsbetweenprocessesonthesamevirtuallocalnetwork. TheZeroMQlibrarywas
usedforasynchronousdataexchange. ContainerizationofagentprocesseswasachievedusingSingularitycontainers
providedwithGPUaccess. Intheexperimentsoutlined,allprocesseswereinitiatedonGPU-enabledcomputingnodes
managedbytheSLURMworkloadmanager.
11APREPRINT-OCTOBER14,2024
6.1 UncertaintyEvaluation: IndividualAgentCase
ToevaluatetheeffectivenessoftheBNNarchitectureproposedinSection5forestimatinguncertaintyinNNoutcomes,
we designed an experiment to demonstrate how the BNN captures and represents uncertainty in its predictions,
particularlyinareaswheretheagenthasnopriordata. Theroboticagentwastrainedexclusivelyonlocaldataduring
theexperimentwithoutexchanginginformationwithotheragents. Thevisualizationofthetrainingresultsispresented
inFigure7.
(a)Singleforwardpass (b)Meanof50forwardpasses (c) Standard deviations of 50 forward
passes
Figure7: VisualizationoftheindividualagentBNNtrainingresults. Theredlinerepresentsthepathofthesingleagent
throughtheenvironment.
TogenerateoutputsfromtheBNN,50queriesweremadeforeachpairofinputcoordinates(x,y). Subsequently,a
visualizationwascreatedtoillustratethemeanvaluesandstandarddeviationsoftheNNresponses. Theredlineonthe
graphindicatestheactualpathtakenbytheagent,whilethesurroundingcolorsrepresentthenetwork’spredictionof
theenvironment’sstatebasedontheagent’scurrentunderstanding.
Subplot 7adepicts theresultsof asingle forwardpass throughthe BNN.Thisvisualization displaysa significant
amountofnoiseinregionstheagenthasnotexplored,indicatinghighuncertaintyinthoseareas. Subplot7bshows
themeanof50forwardpasses, resultinginamuchsmootherandmoreaccuraterepresentationoftheinvestigated
environment,highlightingtheBNN’scapabilitytorefineitspredictionsbyreducinguncertaintythroughrepeatedpasses.
Finally,subplot7cillustratesthestandarddeviationacrossthe50forwardpasses,effectivelyvisualizingtheuncertainty
associatedwiththenetwork’spredictions. Higherstandarddeviationsindicateregionswherethenetworkislesscertain
aboutitspredictions,oftencorrespondingtoareaswithlessdataorwheretheagenthaslessexperience.
Followingthisinitialtest,weconductedaseriesofexperimentstoevaluatetheimpactofthekl parameterfrom
weight
Equation(7)onthetrainingresults. ThevisualizationoftheexperimentsresultsispresentedinFigure8.
Alowvalueofthekl parameterwasobservedtoleadtoalowvarianceintheNN’sresults,whichdoesnotallow
weight
fordistinguishingthehallucinationsoftheNNfromareaswithsufficientdatatoformageneralunderstandingofthe
environment. onversely,ahighkl parametervalueresultsinexcessivenoiseandhighuncertaintyintheNN’s
weight
results. Therefore,toensurethattheBNNprovidesaneffectiveassessmentofuncertainty,fine-tuningthekl
weight
parameterduringthetrainingprocessisrequired.
6.2 OnlineLearningProcessEvaluation
Previously,weexploredtheresultsderivedfromanalyzingdatasetscollectedandprocessedcollectivelyaftertheagents
completedtheirtraversal. Thisapproachisnecessaryduetothesubstantialprocessingpowerandenergyresources
required for NN training, which may not be readily available to autonomous edge devices operating in a mobile
investigationmode.
To evaluate the applicability of online training — combining agent exploration with the learning process — we
implementedasimulationthatusedreal-timedatagatheringalongsideabatchtrainingapproach. Thissimulationwas
builtusingtheZeroMQlibrarytoimplementthecommunicationbetweencomponents. Astreamerbrokerwasinitiated
asasubprocessfromtheWebotscontroller,whiletheNNtrainingprocesswaslaunchedasaparallelsubprocess,acting
asthedatareceiver. Thedata-sendingfunctionwasembeddeddirectlywithintheWebotscontroller. Astherobot
traverseditspath, itperiodicallysentLiDARdatatothebrokerafterscanningasetnumberofpositions. TheNN
12APREPRINT-OCTOBER14,2024
(a)Meankl =10−4 (b)Meankl =5×10−3 (c)Meankl =5×10−1
weight weight weight
(d) Standard deviation kl =(e)Standarddeviationkl =5×(f)Standarddeviationkl = 5×
weight weight weight
10−4 10−3 10−1
Figure8:Comparativevisualizationoftheinfluenceofthekl parameteronthesingle-agentuncertaintyestimation.
weight
Adaptedfrom[10].
trainingprocessthenaddedthisdatatothetrainingpool,allowingforagradualcompilationoftrainingdataasthepath
wasnavigated.
Theexperimentaimedtoassesstheeffectivenessofthetrainingprocessbasedonpartialdatacollection. Tostructure
thisprocess,weintroducedthe"CommunicationRounds"(CR)concept,whichsegmentedthepathintodiscreteparts,
eachrepresentingaphaseofdatacollectionandsubsequenttraining. TheagentgatheredLiDARdataforeachCR
segmentandtransmittedthisinformationtothetrainingalgorithmforpartialdatatrainingatthesegment’send.
Wespecificallyinvestigatedtwoscenariosinourexperiments:
• CumulativeDataRetention(CDR):Inthisscenario,thealgorithmretainedallpreviouslyreceiveddata,
usingitalongsidenewdataforsubsequenttrainingsessions. Thismethodwasintendedtocontinuouslyenrich
thetrainingdataset,potentiallyenhancingtheNN’saccuracyandadaptability.
• DataRefresh(DR):Incontrast,thisapproachtrainedtheNNsolelyonthenewlyacquireddatasetforeach
CR,discardingallpreviousdatasets. Theonlyremnantsofpriordatawerethepre-trainedweightsoftheNN,
whichcouldinfluencethetrainingoutcomebasedonpastlearningbutdidnotdirectlyreuseolddata.
The results of these experiments are visualized in Figure 9. Each column in the figure corresponds to different
configurationsoftheonlinetrainingprocess,withvaryingnumbersofcommunicationroundsandtheimplementation
ofeitherCumulativeDataRetention(CDR)orDataRefresh(DR)strategies. Asthenumberofcommunicationrounds
increases,thequalityoftheenvironmentmappingimproves,reflectingmoreaccuratemeanpredictionsandreduced
standarddeviations. IntheCDRscenario,theNNdemonstratesbetterstabilityandconsistencyinitspredictionsacross
allcommunicationrounds,indicatingthattheaccumulationofdatasignificantlybenefitsthelearningprocess. Onthe
otherhand,intheDRscenario,whileinitialroundsshowpromisingresults,laterroundsexhibitincreasednoiseand
uncertainty,suggestingthattheDRstrategymayleadtoasignificantlossofvaluableinformation,affectingtheoverall
performanceoftheNN.
13APREPRINT-OCTOBER14,2024
(a)CDR,8CR (b)CDR,18CR (c)CDR,25CR (d)DR,18CR
Figure9: VisualizationoftheMean(toprow)andStandardDeviation(bottomrow)oftheonlinetrainingprocess
resultsunderdifferentconfigurationsofCumulativeDataRetention(CDR)andDataRefresh(DR)scenarios.
6.3 UncertaintyEvaluation: Multi-AgentCase
Insection6.1, wedemonstratedthat BNNscaneffectivelyestimateuncertaintyinthecontextofasingle learning
agent. Inthissection,weextendthatanalysistoexplorewhetherthesamecapabilitiesofBNNsareindecentralized
multi-agenttrainingenvironments. Toinvestigatethis,weevaluateddifferentregularizationstrategiestounderstand
theirimpactonthequalityofdecentralizedBNNtraining,withaparticularfocusonassessingvalidationlossunderthe
followingapproaches:
1. ApplyingauniformL2regularizationacrossallneuralnetworkparameterswithoutdifferentiatingbetween
parametertypes;
2. ApplyingseparateL2regularizationforstandardneuralnetworkparametersandBayesianparameters;
3. UsingdistinctregularizationtechniquesforstandardandBayesianparameters,withL2regularizationforthe
standardparametersandKullback-LeiblerdivergencefortheBayesianparameters(seeAlgorithm2).
ThefindingsfromtheseevaluationsareillustratedinFigure10. Notably,theuseofKLdivergenceforregularizing
Bayesianparameters,asdetailedinAlgorithm2)ledtoa12-30%reductioninvalidationlosscomparedtotheother
approaches. Thisstrategyalsoenhancedthestabilityofthetrainingprocess.
TheresultsofthedecentralizedBNNtrainingusingthismethodarevisualizedinFigure11.
Thefiguredepictstheresultsof50forwardpassesofageneralizedmodelthatthesevenindividualagentsconvergedto
duringthetrainingprocessutilizingAlgorithms1and2. Duringthetraining,allagentsreachedaconsensusonaunified
model. Subfigure11ashowsthemeanpredictionsacrosstheenvironment,representingageneralmapthatencapsulates
theknowledgegatheredbyallsevenagents. Subfigure11billustratesthestandarddeviationofthesepredictions,which
canbeinterpretedasthelevelofuncertaintyassociatedwiththemodel’soutput. Higherstandarddeviationsindicate
regionswherethemodelislesscertainaboutitspredictions.
Asanuncertaintybaseline,subfigure11cshowsthedensityofLiDARdatapointsusedformodeltraining. Inthis
subfigure,darkerareasrepresentregionswithahigherconcentrationofdatapoints,offeringareferenceforcomparing
theuncertaintyvisualizedinsubfiguree11bagainsttheactualdatadistribution. ToestimatethedensityoftheLiDAR
datapoints,weemployedtheKernelDensityEstimationmethod[40]withaGaussiankernel.
14APREPRINT-OCTOBER14,2024
Figure10: ComparisonofvalidationlossduringdistributedBNNtraining1)withuniformL2regularization(uniform
reg.); 2) separate L2 regularization (reg. w/o D_KL); 3) Kullback-Leibler divergence for regularization of BNN
ρ-parameters(reg. withD_KL)[10].
(a)Mean (b)Standarddeviation (c)Pointdensity
Figure11: VisualizationofthedecentralizedBNNtrainingresultsaccordingtoAlgorithm2: (a)mean;(b)standard
deviation;(c)pointdensityoftheoriginaldataset. Adaptedfrom[10].
7 Conclusions
Thispaperaddressedtheproblemofuncertaintyestimationinedgecollaborativelearning. Throughatargetedcase
studyoncollaborativemapping,wedevelopedandevaluatedadecentralizedlearningframeworkbasedontheWebots
simulation platform and Distributed Neural Network Optimization algorithm. Our main contributions include the
designofanepoch-baseddecentralizedconsensusalgorithmtosupportthepeer-to-peerexchangeofneuralnetwork
parametersamongdistributedagentsandintegratingBNNstoincorporateuncertaintyestimationintothisdecentralized
learningcontext.
WereviewedELmethodsandtheirapplicabilitytoAI-enablededgedevicesalongsideourpracticalimplementations.
Thisreviewcoveredseveralapproaches, includingFederatedLearning, FederatedDistillation, SplitLearning, and
decentralizedtechniquesbasedontheAlternatingDirectionMethodofMultipliers. Weexaminedthebenefitsand
drawbacksofthesemethodsanddiscussedhowtheycouldbeadaptedandappliedtoenhanceedgelearningenvironments.
Additionally,weexploredhowBNNscouldbeusedforuncertaintyestimation,emphasizingthepotentialofstochastic
modelingtoimprovethegeneralizationcapabilitiesofneuralnetworksincomplexanduncertainscenarios.
15APREPRINT-OCTOBER14,2024
OurfindingsshowthatBNNsareeffectiveinestimatinguncertaintywithindistributedlearningenvironments,butthey
alsohighlighttheimportanceofprecisehyperparametertuningtoachievereliableuncertaintyassessments. Among
the regularization strategies evaluated, using Kullback–Leibler divergence for Bayesian parameter regularization
was particularly beneficial, resulting in a 12-30% reduction in validation loss compared to other strategies. This
approachalsocontributedtogreaterstabilityduringtraining,demonstratingthepracticalvalueofintegratingBNNs
intodistributedlearningframeworks.
FutureworkshouldfocusonoptimizingthesedistributedlearningtechniquesforimplementationonembeddedAI
hardware, where computational resources are limited. This will require refining neural network architectures and
EL methods to fit the specific constraints of edge devices. Furthermore, there is a need to explore advanced task
managementandoffloadingstrategieswithinthemulti-layeredfogandhybridedge-fog-cloudinfrastructurestoenhance
computationalefficiencyandresourceutilizationindynamicandresource-constrainedenvironments.
References
[1] QianlinLiang,WalidA.Hanafy,AhmedAli-Eldin,andPrashantShenoy. Model-drivenclusterresourceman-
agementforaiworkloadsinedgeclouds. ACMTransactionsonAutonomousandAdaptiveSystems,18:1–26,3
2023.
[2] VivekParmar,SyedShakibSarwar,ZiyunLi,Hsien-HsinS.Lee,BarbaraDeSalvo,andMananSuri. Exploring
memory-orienteddesignoptimizationofedgeaihardwareforextendedrealityapplications.IEEEMicro,43:40–49,
112023.
[3] BoWang,KeDong,NurulAkhiraBinteZakaria,MohitUpadhyay,Weng-FaiWong,andLi-ShiuanPeh. Network-
on-chip-centricacceleratorarchitecturesforedgeaicomputing. pages243–244.IEEE,102022.
[4] Wei Yang Bryan Lim, Nguyen Cong Luong, Dinh Thai Hoang, Yutao Jiao, Ying-Chang Liang, Qiang Yang,
DusitNiyato,andChunyanMiao. Federatedlearninginmobileedgenetworks: Acomprehensivesurvey. IEEE
CommunicationsSurveys&Tutorials,22:2031–2063,2020.
[5] FarzadSamie,VasileiosTsoutsouras,LarsBauer,SotiriosXydis,DimitriosSoudris,andJorgHenkel.Computation
offloadingandresourceallocationforlow-poweriotedgedevices. pages7–12.IEEE,122016.
[6] JihongPark,SumuduSamarakoon,AnisElgabli,JoongheonKim,MehdiBennis,Seong-LyunKim,andMerouane
Debbah. Communication-efficientanddistributedlearningoverwirelessnetworks: Principlesandapplications.
ProceedingsoftheIEEE,109:796–819,52021.
[7] Laurent Valentin Jospin, Hamid Laga, Farid Boussaid, Wray Buntine, and Mohammed Bennamoun. Hands-
onbayesianneuralnetworks—atutorialfordeeplearningusers. IEEEComputationalIntelligenceMagazine,
17:29–48,52022.
[8] Kun Yang, Dingkang Yang, Jingyu Zhang, Mingcheng Li, Yang Liu, Jing Liu, Hanqi Wang, Peng Sun, and
LiangSong. Spatio-temporaldomainawarenessformulti-agentcollaborativeperception. In2023IEEE/CVF
InternationalConferenceonComputerVision(ICCV),pages23326–23335.IEEE,102023.
[9] JavierYu,JosephA.Vincent,andMacSchwager. Dinno: Distributedneuralnetworkoptimizationformulti-robot
collaborativelearning. IEEERoboticsandAutomationLetters,7:1896–1903,42022.
[10] GlebRadchenkoandVictoriaFill. Uncertaintyestimationinmulti-agentdistributedlearningforai-enablededge
devices. InProceedingsofthe14thInternationalConferenceonCloudComputingandServicesScience,pages
311–318.SCITEPRESS-ScienceandTechnologyPublications,2024.
[11] EnLi,LiekangZeng,ZhiZhou,andXuChen. Edgeai: On-demandacceleratingdeepneuralnetworkinference
viaedgecomputing. IEEETransactionsonWirelessCommunications,19:447–457,12020.
[12] JiaweiShaoandJunZhang. Communication-computationtrade-offinresource-constrainededgeinference. IEEE
CommunicationsMagazine,58:20–26,122020.
[13] MassimoMerenda,CarloPorcaro,andDemetrioIero. Edgemachinelearningforai-enablediotdevices: Areview.
Sensors,20:2533,42020.
[14] Afaf Tak and Soumaya Cherkaoui. Federated edge learning: Design issues and challenges. IEEE Network,
35:252–258,32021.
[15] JieZhang,ZhihaoQu,ChenxiChen,HaozhaoWang,YufengZhan,BaoliuYe,andSongGuo. Edgelearning:
Theenablingtechnologyfordistributedbigdataanalyticsintheedge. ACMComputingSurveys,54:1–36,92022.
16APREPRINT-OCTOBER14,2024
[16] BharathSudharsan, JohnG.Breslin, andMuhammadIntizarAli. Edge2train: aframeworktotrainmachine
learning models (svms) on resource-constrained iot edge devices. In Proceedings of the 10th International
ConferenceontheInternetofThings,pages1–8.ACM,102020.
[17] ZhihuiJiang,GuandingYu,YunlongCai,andYuanJiang. Decentralizededgelearningviaunreliabledevice-to-
devicecommunications. IEEETransactionsonWirelessCommunications,21:9041–9055,112022.
[18] H.BrendanMcMahan,EiderMoore,DanielRamage,SethHampson,andBlaiseAgüerayArcas. Communication-
efficientlearningofdeepnetworksfromdecentralizeddata. InProceedingsofthe20thInternationalConference
onArtificialIntelligenceandStatistics,AISTATS2017,2017.
[19] Haftay Gebreslasie Abreha, Mohammad Hayajneh, and Mohamed Adel Serhani. Federated learning in edge
computing: Asystematicsurvey,2022.
[20] VanDinhNguyen,SymeonChatzinotas,BjornOttersten,andTrungQ.Duong. Fedfog: Network-awareoptimiza-
tionoffederatedlearningoverwirelessfog-cloudsystems. IEEETransactionsonWirelessCommunications,21,
2022.
[21] LucaMelis,CongzhengSong,EmilianoDeCristofaro,andVitalyShmatikov. Exploitingunintendedfeature
leakageincollaborativelearning. In2019IEEESymposiumonSecurityandPrivacy(SP),pages691–706.IEEE,
52019.
[22] LingjuanLyu,HanYu,XingjunMa,ChenChen,LichaoSun,JunZhao,QiangYang,andPhilipS.Yu. Privacy
androbustnessinfederatedlearning: Attacksanddefenses. IEEETransactionsonNeuralNetworksandLearning
Systems,35:8726–8746,72024.
[23] JinHyunAhn,OsvaldoSimeone,andJoonhyukKang. Wirelessfederateddistillationfordistributededgelearning
withheterogeneousdata. volume2019-September,2019.
[24] FelixSattler,ArturoMarban,RomanRischke,andWojciechSamek. Cfd: Communication-efficientfederated
distillationviasoft-labelquantizationanddeltacoding. IEEETransactionsonNetworkScienceandEngineering,
9:2025–2038,72022.
[25] ChuhanWu,FangzhaoWu,LingjuanLyu,YongfengHuang,andXingXie. Communication-efficientfederated
learningviaknowledgedistillation. NatureCommunications,13:2032,42022.
[26] Jiawei Shao, Fangzhao Wu, and Jun Zhang. Selective knowledge sharing for privacy-preserving federated
distillationwithoutagoodteacher. NatureCommunications,15:349,12024.
[27] PraneethVepakomma,OtkristGupta,TristanSwedish,andRameshRaskar. Splitlearningforhealth: Distributed
deeplearningwithoutsharingrawpatientdata. arXivpreprintarXiv:1812.00564,2018.
[28] OtkristGuptaandRameshRaskar. Distributedlearningofdeepneuralnetworkovermultipleagents. Journalof
NetworkandComputerApplications,116:1–8,82018.
[29] YooJeongHa,MinjaeYoo,GusangLee,SoyiJung,SaeWonChoi,JoongheonKim,andSeehwanYoo. Spatio-
temporal split learning for privacy-preserving medical platforms: Case studies with covid-19 ct, x-ray, and
cholesteroldata. IEEEAccess,9:121046–121059,2021.
[30] WenWu,MushuLi,KaigeQu,ConghaoZhou,XueminShen,WeihuaZhuang,XuLi,andWeisenShi. Split
learningoverwirelessnetworks: Paralleldesignandresourcemanagement. IEEEJournalonSelectedAreasin
Communications,41:1051–1066,42023.
[31] Yusuke Koda, Jihong Park, Mehdi Bennis, Koji Yamamoto, Takayuki Nishio, Masahiro Morikura, and Kota
Nakashima. Communication-efficientmultimodalsplitlearningformmwavereceivedpowerprediction. IEEE
CommunicationsLetters,24,2020.
[32] Stephen Boyd. Distributed Optimization and Statistical Learning via the Alternating Direction Method of
Multipliers,volume3. 2010.
[33] AnisElgabli,JihongPark,AmritS.Bedi,MehdiBennis,andVaneetAggarwal. Gadmm: Fastandcommunication
efficientframeworkfordistributedmachinelearning. JournalofMachineLearningResearch,21,2020.
[34] SayantanBhadra,VarunA.Kelkar,FrankJ.Brooks,andMarkA.Anastasio. Onhallucinationsintomographic
imagereconstruction. IEEETransactionsonMedicalImaging,40:3249–3260,112021.
[35] SebastianClaici,MikhailYurochkin,SoumyaGhosh,andJustinSolomon. Modelfusionwithkullback-leibler
divergence. volumePartF168147-3,2020.
[36] S.KullbackandR.A.Leibler. Oninformationandsufficiency. TheAnnalsofMathematicalStatistics,22:79–86,
1951.
17APREPRINT-OCTOBER14,2024
[37] DmitryI.BelovandRonaldD.Armstrong. Distributionsofthekullback–leiblerdivergencewithapplications.
BritishJournalofMathematicalandStatisticalPsychology,64:291–309,52011.
[38] AhtiKalervo, JuhaYlioinas, MarkusHäikiö, AnttiKarhu, andJuhoKannala. Cubicasa5k: Adatasetandan
improvedmulti-taskmodelforfloorplanimageanalysis. LectureNotesinComputerScience(includingsubseries
LectureNotesinArtificialIntelligenceandLectureNotesinBioinformatics),11482LNCS:28–40,2019.
[39] VincentSitzmann,JulienN.P.Martel,AlexanderW.Bergman,DavidB.Lindell,andGordonWetzstein. Implicit
neuralrepresentationswithperiodicactivationfunctions. InAdvancesinNeuralInformationProcessingSystems,
volume2020-December,2020.
[40] ScottD. KernelDensityEstimators,pages137–216. 32015.
18