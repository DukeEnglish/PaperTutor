Optimal Downsampling for Imbalanced Classification with Generalized
Linear Models
Yan Chen Jose Blanchet Krzysztof Dembczynski Laura Fee Nern Aaron Flores
Duke University Stanford University Yahoo Yahoo Yahoo
Abstract model is used directly on test data, it will be expected
to predict according to the original distribution, so a
correction is needed either during training or during
Downsampling or under-sampling is a tech-
thepredictionphase. Therearemultiplewaysinwhich
nique that is utilized in the context of large
one can correct for downsampling, these methods are
and highly imbalanced classification mod-
well documented in the literature, one can either apply
els. We study optimal downsampling for im-
a proper rescaling of the estimated conditional proba-
balanced classification using generalized lin-
bilities [12], one can reweight the samples in training
ear models (GLMs). We propose a pseudo
procedure, or one can apply an additional correction
maximum likelihood estimator and study its
step (using, for example, isotonic regression [39]). All
asymptotic normality in the context of in-
of the above techniques will correct biases induced
creasingly imbalanced populations relative to
by downsampling, but obviously they have different
an increasingly large sample size. We provide
mean-squared error.
theoretical guarantees for the introduced esti-
mator. Additionally, we compute the optimal Perhaps surprisingly, given how prevalent downsam-
downsampling rate using a criterion that bal- pling is in practice, as far as we know, a systematic
ances statistical accuracy and computational studyofthe“best"estimator(atleastintheory)andits
efficiency. Our numerical experiments, con- practical implications for applying downsampling has
ducted on both synthetic and empirical data, not been studied. The most recent paper that partially
further validate our theoretical results, and addressestheseissuesis[50],wheretheauthorexplored
demonstrate that the introduced estimator the asymptotic property of downsample estimator un-
outperforms commonly available alternatives. der imbalanced classification. But analysis there was
constrained within logistic regression model and didn’t
include any guidelines for selecting downsample rate.
1 Introduction Thismotivatesourstudy. Whilemostofourdiscussion
focuses on generalized linear models, our results also
offer insights into the application of downsampling in
The problem of training a machine learning classifica-
classification tasks using neural networks.
tion model with imbalanced populations (or, equiva-
lently, predicting rare events given contextual data) Our goal is to offer new estimators that are not only
arises in a wide range of applications such online ad- consistent and asymptotically normal, but are guided
vertisement, healthcare, insurance or fraud detection[2, byoptimaldesignintermsofvariance. Itiswellknown
18, 28, 46, 47, 43, 23, 35, 20, 37, 52]). The rule of that the maximum likelihood estimator attains the
thumb in these imbalanced settings is downsampling Cramer-Rao lower bound and therefore it is optimal in
or under-sampling [32, 54, 51, 11, 48, 34, 2, 18, 28, 29]. this sense. We then use this minimum variance insight
Forexample,inonlineadvertisingtheconversionrateis to help users select an optimal downsampling rate.
usually less than 0.1% out of tens of millions of impres-
sions, making downsampling a necessity for training. Contributions and Overview of Results. Our
The downsampling process involves sampling a propor- contributions are as follows:
tion of the majority population according to a suitable
samplingrate,calledthedownsamplingrate. Thistech- 1) Provide an explicit characterization of the max-
nique involves sampling a proportion of the majority imum likelihood estimator in highly imbalanced
populationaccordingtoasuitablesamplingrate,called data.
the downsampling rate. After that, a model is directly 2) IntroducenewMLE-informedestimatorsthatoffer
trained on the down-sampled set of instances. If the significant improvements relative to the available
4202
tcO
11
]LM.tats[
1v49980.0142:viXraOptimal Downsampling for Imbalanced Classification with Generalized Linear Models
alternatives. which is consistent with our theoretical conclusions.
3) Provide guidance for the choice of optimal down-
Additionally, we introduce a notion that trades the sta-
sampling rates.
tistical error induced by the downsampling mechanism
with the computational benefits derived by downsam-
In order to formally capture highly imbalanced data, pling. We adapt the framework of [16] to introduce
we consider an asymptotic regime of the form P(Y = a concept of efficiency cost that reflects the trade-off
1|X =x)=1−F(τ n+θ ∗Tx)≡F¯(τ n+θ ∗Tx), where F between statistical error and the computational cost.
is the cumulative distribution function (c.d.f.) of an Our reasoning is as follows.
underlyinglatentvariable. Thehighlyimbalanceddata
Suppose that there is a given computational routine
setup is captured by the location parameter τ →∞
n
to be applied in the training process. We assume
so that n(1−F(τ ))→∞. By doing this, we capture
n
that the routine produces an estimator with an ϵ error
a wide range of situations for which data is imbalanced
with complexity O(log(1/ϵ))×n×(α(1−p )+p ),
relative to the sample size. We do not think that the 1 1
where n is the size of the original training set, α is
dataisbecomingincreasinglyimbalancedasthesample
the downsampling rate and p is the minority popu-
size grows, rather, the asymptotic statistics provide 1
lation rate. The contribution of log(1/ϵ) arises when
tools for approximate inference that are applied for a
training via gradient descent of a smooth convex loss
fixed sample size and, in the same spirit, our scaling
[42, 15, 3, 26] and is used as an illustration of our
introduces approximations that are valid within a wide
reasoning (and is applicable to logistic regression, for
rangeofimbalancedproportions. Thiscreatesaregime
example); the discussion that follows can be adapted
of data imbalance in which the minority proportion
to the cost incurred using other methods with differ-
convergestozeroasthelocationparameterτ becomes
n
ent assumptions. In the instance we adopted here,
extreme. This setup is useful in practice. For example,
consequently, with a computational budget of size b=
if n = 106, τ = 0.6log(n) ≈ 8, and θTx ≥ 1. If F
n ∗ c×(α(1−p )+p )nlog(n)log(log(n))forsomec>0we
follows a logistic regression model such that F(τ + 1 1
n
obtain an overall estimator with a mean squared error
θ ∗Tx) = eτn+θ ∗Tx/(1+eτn+θ ∗Tx), then the conversion
σ(α)2/n+o(1/n), where σ(α)2/n is the mean squared
rate P(Y = 1|X = x) = 1/(1+eτn+θ ∗Tx) < 0.1%. So
error of the asymptotic downsampled estimator. We
by appropriately controlling the growth rate of τ , we
n can write n in terms of the budget, obtaining, for large
can accurately recover the ratio of the rare events that
n,n=b(1+o(1))/(c×(α(1−p )+p )log(b)log(log(b))).
1 1
are the focus of our analysis.
Therefore,minimizingthetotalerrorsubjecttoagiven
Firstly, we show that the prediction score from down- budget constraint is asymptotically equivalent to just
sample still follows a GLM structure, such that P(Y˜ = minimizing σ(α)2×(α(1−p 1)+p 1) as a function of
1|X˜ = x) = 1−G(τ +θTx) ≡ G¯(τ +θTx), where α. We provide expressions to guarantee this optimal
0 ∗ 0 ∗
(Y˜,X˜) is the random variable induced from down- choice of α in practice in Section 5 and we apply our
sample data, α is the downsample rate, and G(z) = theoretical findings to logistic regression in Section 6.
αF(z) is also a c.d.f. Based on this discovery, Finally, we apply our estimator to real-world imbal-
1−(1−α)F(z)
we propose a pseudo maximum likelihood estimator anceddatasetsusingbothlogisticregressionandneural
(pseudo MLE) which can be computed directly on the network models. The numerical results indicate that
downsample from imbalanced data by (6). our estimator performs well in this experiment. More
details can be found in Section 7 and Appendix A.
Secondly, we obtain the asymptotic normality of the
proposedpseudoMLE(seeTheorem1andTheorem2).
Theestimatorisunbiasedandhasgenerallylargervari-
Related Work The concept of Generalized Linear
ance than the full-sample estimator (with zero down-
Model (GLM) has a long history dating back to as
sampling rate). But for some small values of α the
early as its introduction by [38] in the 1970s. Specif-
asymptotic variance stays the same as that of the full-
ically, GLM plays an important role in classification
sampling estimator, meaning that downsampling with
tasks both in theory and applications [17, 8, 6]. For
these α results in no efficiency loss at all. We conduct
example, a statistical test was developed by [17] based
numerical experiments and apply our estimator to lo-
on GLM for pattern classification in machine learning
gistic regression for different values of τ . We use the
n applications. A high-dimensional GLM classification
mean squared error metric and compare the pseudo
problem was explored by [22] via support vector ma-
MLE with two commonly used alternative estimators:
chine classifiers. GLM classifiers were also applied by
the inverse-weighting estimator (8) employed by [50],
[7, 1] in healthcare and computational biology.
andtheconditionalMLE(9). Thefindingsarethatour
estimator outperforms both of them when τ is large, Particularly, classification for imbalanced data has gen-
n
but as τ decreases, it loses its advantage gradually, erated the interest of many researchers [24, 27, 5, 4,
nYan Chen, Jose Blanchet, Krzysztof Dembczynski, Laura Fee Nern, Aaron Flores
9, 33, 13, 14, 19, 36, 41, 10, 40]. In this context, also variablesZ ,Z ,...inametricspaceZ,wesayZ →d Z
1 2 n
referred to as a rare event setting, one class within a if Z converges in distribution to Z, and Z →p Z if
n n
population has significantly fewer instances than the Z converges in probability to Z. Given matrix A,B,
n
other. Specifically, for binary labeled data, cases (the A ≻ B indicates A−B is positive definite. Given
labelgroupwithfewerobservations)areobservedmuch target parameter θ ∈Rd and its estimator θˆ∈Rd, we
less frequently than controls (the other label group). use mean-squared-error or mean squared estimation
Oneexampleisonlineadvertisingdata,wheretypically (cid:20)(cid:13) (cid:13)2(cid:21)
error to refer to E (cid:13)θˆ−θ(cid:13) . We denote µ(·) as the
only 1 conversion is observed out of 1,000 to 1,000,000 (cid:13) (cid:13)
impressions [31, 30, 44, 25]. An extensive survey of density of X and X as the covariate space.
the area is given by [5], where significant attention was
given to downsampling methods, in which some avail-
2 Problem Setup
able controls are removed for a balanced subsample.
Another approach is oversampling [9, 36, 50, 45, 53]
We observe an i.i.d. generated dataset {(Y ,X )}n ,
where additional instances are generated as cases. i i i=1
where X ∈ Rd,d ≥ 1, Y ∈ {0,1}. We use P to
i i
Our work focuses on exploring the effects of downsam- denote the joint probability distribution of (X ,Y ),
i i
pling[14,50,5],inwhichwemaintainallthecasesand and E[·] to denote the expectation induced by P.
uniformly sample an equal number of controls to make The data generating process follows Generalized Lin-
a balanced subsample dataset. This is also referred to ear Model (GLM), such that given a latent random
as “case-control sampling” by previous literature [14]. variable Z, the observed binary label Y given co-
A method of subsampling for logistic regression pro- variate X is defined as Y = 1(cid:0) Z >τ +θTX(cid:1), with
n ∗
posed by [14] was to adjust the class balance locally in F¯ Z(τ n+θ ∗Tx)=P(cid:0) Y =1(cid:12) (cid:12)X =x(cid:1),whereF¯(·):=F¯ Z(·)=
feature space via an accept-reject scheme. 1−F (·)andF (·)isthecumulative distribution func-
Z Z
tion (c.d.f.) of Z. We are interested in studying the
The findings of a recent paper [50] suggest that the
impact of downsampling for the unbalanced dataset
availableinformationinrareeventdataforbinarylogis-
with P(Y = 1) ≪ P(Y = 0). To do this we keep all
tic regression is at the scale of the number of positive
the positive samples (i.e., those with Y =1), and uni-
examples, and downsampling a small proportion of i
formly sample a proportion α ∈ (0,1] of the samples
the controls may induce an estimator with identical
with Y =0, so that we get a more balanced downsam-
asymptotic distribution to the full-data maximum like- i
ple {(Y˜,X˜ )}N of size N = np +(1−p )nα, where
lihood estimator (MLE). Besides, a different perspec- i i i=1 1 1
p is the ratio of positive samples. We use µ˜ to denote
tive was provided by [10] for the performance analysis 1
the joint density of downsample random pairs (Y˜,X˜).
of sampling technique by considering cost sensitivity
of misclassification. Motivated by the previous work, Specifically,weconsiderthecasewhereτ isaknownse-
n
we study the imbalanced binary classification problem quencesuchthatτ →∞asn→∞. Thiscorresponds
n
with generalized linear model structure. Similar to to the rare-event setup where F¯(τ +θTx)→0 for any
n ∗
[50], we derived the asymptotic distribution of the in- x ∈ X, where X is bounded in Rd. The assumption
duced downsampling estimator, and come to a similar thatτ →∞iswithoutlossofgeneralitybynotingthat
n
conclusion that downsampling a small proportion of if τ →−∞, then P(Y =0)≈0 and P(Y =1)≈1, so
n
the controls by selecting downsampling rate within a the analysis is similar to the case τ → by switching
n
certain range results in identical statistical efficiency the role of P(Y = 0) and P(Y = 1). We consider the
as the full-sample MLE. following data generation process: Given sample size n
and τ , we observe i.i.d. data {X ,Y }n with
However, our paper is more general than [50] in the n i i i=1
following aspects. Firstly, we consider generalized lin-
P(Y =1|X )=F¯(τ +θTX )=1−F(τ +θTX ),
earmodelasamoregeneralandcommonlyseenfamily i i n ∗ i n ∗ i
of models. Secondly, we use a new covariate-adapted
where θ is the estimation target.
estimator based on the correct maximum likelihood es- ∗
timator for the downsample distribution, which turned
outtobemoreefficientthantheestimatorusedby[50] 3 Proposed Estimator
under rare-event setup. Thirdly, we also provide clear
guidance on the selection of downsample rate.
Inthissection,wefirstdemonstratethatunderGeneral-
ized Linear Model (GLM) setup, the induced variables
Notation Givenathricedifferentiablefunctionf,we by downsampling still follow a GLM model. Based
use f(1) or f′, f(2) or f′′, and f(3) or f′′′ to denote the on this finding, we propose a pseudo maximum likeli-
first-order, second-order or the third-order derivatives. hood estimator which can be computed directly from
Weusef¯(·)todenote1−f(·). Forasequenceofrandom downsample.Optimal Downsampling for Imbalanced Classification with Generalized Linear Models
Given joint law P of the full-sample random variable downsample. Based upon this, the joint likelihood of
(Y,X), and denote the induced joint law of down- the downsample random variables is as follows.
sample random variable (Y˜,X˜) as P˜, there exists Proposition 2 (Downsample Joint Likelihood of
a monotone function m such that P(Y = 1|X;θ) = (Y˜,X˜)). Given any τ , the maximum likelihood es-
n
m(P˜(Y =1|X;θ)), where the probability distributions timator for the target estimand θ (i.e. the true value
∗
are parametrized by θ ∈ Θ. Note that this justifies of the unknown parameter) is
thecorrectionmethodfordownsamplingbyimposinga
N
monotonetransformationonthepredictedscoresbased θˆ:=argmax 1 (cid:88) ℓ(X˜ ,Y˜,θ ;τ ), (3)
on the model trained from the downsample. Besides, N i i 1 n
θ1
i=1
we explicitly formulate the downsample maximum like-
where
lihood estimator (MLE), where the target parameter
ℓ(x˜,y˜,θ ;τ )
for the downsample MLE coincides with the one from 1 n
the full-sample model. =y˜logF (cid:90)¯(τ n+θ 1Tx˜)+(1−y˜)logαF(τ n+θ 1Tx˜)
−log
(cid:2)
1−(1−α)F(τ
+θTx)(cid:3)
µ(x)dx.
One common practice for estimation on imbalanced n 1
X
dataset is that people usually fit the parameter as if (4)
the downsample model still belongs to the same class
ofmodelsasF(·),afterwhichtheyobtainF(τ +θˆTX) Proposition 2 exhibits the empirical maximum like-
n 1
asthescorefromthemodeltrainedonthedownsample, lihood estimator of downsample GLM with respect
and then use isotonic regression, i.e. utilizing a mono- to the joint distribution of (Y˜,X˜). However, we
tone mapping g(·) so that g(F(τ +θˆTX)) is obtained note that (4) utilizes the marginal density of full-
n 1
as the final output of their prediction score given any sample X, which is expensive to estimate for large-
covariate X , where given τ known, θˆ is achieved by scale imbalanced data. To tackle this, we first note
i n 1
solving the following problem: that the density of down-sample X˜ can be written as
µ˜(x) = [1−(1−α)F(τn+θ ∗Tx)]µ(x) (see Lemma 4 in Ap-
θˆ 1=argmax θ1 N1 (cid:80)N i=1Y˜ ilog(cid:16) 1−F(τ n+θ 1TX˜ i)(cid:17) (1) pendix B), wP( hY e= re1) µ+ (α ·P )(Y is= t0 h) e density of full-sample X.
+(1−Y˜)logF(τ +θTX˜ ). Thus the last term in (4) can be rewritten as
i n 1 i
(cid:90)
Lemma 1 in Appendix B shows a counterexample to log
(cid:2)
1−(1−α)F(τ n+θ
1Tx)(cid:3)
µ(x)dx
illustrate why this method can lead to biased predic- X
tion score for some covariates. Specifically, Lemma 1 (cid:90)
constructs a counterexample where θˆ
1
leads to biased- =log [1−(1−α)F(τ n+θ 1Tx)]×
ness. And if h(x)=θ˜ 1Tx maps two distinct x,x′ to the X P(Y =1)+αP(Y =0)
same value while θTx ≠ θTx′, and the c.d.f. F(·) is µ˜(x)dx.
∗ ∗ 1−(1−α)F(τ n+θ ∗Tx)
strictly increasing, then the true model has different
probabilities for x,x′ whereas the model trained on Notethatwhenτ n →∞, 1−(1−α)F(τ n+θ ∗Tx)→α,
and P(Y = 1)+αP(Y = 0) = E[1−(1−α)F(τ +
the downsample maps them to the same prediction n
θTX)]→α, thus as n→∞, we have
score, thus both parameter estimator and the induced ∗
p Pr re od pic oti so in tiosc nor 1e (m Do od we nl sa ar me pb li eas Ped re.
diction Score). The
log(cid:82) X[1− l( o1 g− (cid:82)α X) [F 1−(τ (n 1+ −θ α1T )x F) (] τ1 n−P +( (Y 1 θ−= 1Tα1 x) ) )+ F ]µα ( ˜τP (n( xY + )d= θ x∗T0) x)µ˜(x)dx →1.
prediction score of the downsampled random variables (5)
follows P(Y˜ = 1|X˜ ) = P(Yi=1|Xi=1) . Hence Furthermore, by the law of large numbers, the sample
{(Y˜ i,X˜ i)}N i=1i still foli lows Gα+ L( M1− ,α w)P i( tY hi= t1 h|X ei c= o1 n) ditional a pv roer ba ag be iliN1 ty(cid:80) toN i=(cid:82)1[ [1 1− −( (1 1− −αα )) FF (( ττ n ++ θθ T1T xX˜ )]i µ˜)] (xco )dn xve .r Bge as sein
d
probability P(Y˜ = 1|X˜ ) = G¯(τ +θTX˜ ), and G¯(z) = X n 1
i i n 1 i upon the discovery above, we propose the following
1−G(z), where
pseudo maximum likelihood estimator computed from
downsample data:
αF(z)
G(z):= (2)
1−(1−α)F(z) θˆ :=argmax 1 (cid:80)N ℓ˜(X˜ ,Y˜,θ ;τ ), (6)
∗ θ1 N i=1 i i 1 n
is the c.d.f. of some random variable. where
ℓ˜(X˜ ,Y˜,θ ;τ )
i i 1 n
Proposition 1 illustrates that the downsample random =Y˜ log(cid:16) 1−F(τ +θTX˜ )(cid:17)
variables generated from GLM still follow a GLM, i n 1 i
where F(z) = G(z) , indicating that the true +(1−Y˜ i)logαF(τ n+θ 1TX˜ i)
class probabilitiα e+ s( a1 r− eα t)G h( ez) monotonic transformations −log(cid:110) N1 (cid:80)N i=1(cid:104) 1−(1−α)F(τ n+θ 1TX˜ i)(cid:105)(cid:111) .
of the predicted class probabilities conditional on the (7)Yan Chen, Jose Blanchet, Krzysztof Dembczynski, Laura Fee Nern, Aaron Flores
In the following, we use θˆ to denote the pseudo MLE
∗
computed from (6).
Remark 1. Notethat(5)holdsonlywhenτ →∞,but
n
the estimator θˆ may be not consistent when τ <∞.
∗ n
So in practice the pseudo MLE should perform well
for large values of τ , and its benefit could disappear
n
as τ decays. We verify this claim through numerical
n
experiment on logistic regression models by comparing
the performance of θˆ under different values of τ in
∗ n
Section 6.1.
Figure 1: Estimation error for different α.
Additionally, there are two alternative natural estima-
tors for θ ∗, which are inverse-weighting estimator and Assumption 1. F ∈C1(R) is strictly increasing, and
conditional maximum likelihood estimator (a.k.a. con- the covariate space X ⊂Rd is compact.
ditional MLE). The inverse-weighting estimator θˆ is
I Assumption 2. The matrix E[XXT] is nonsingular.
defined as
(cid:16) (cid:17) Assumption 3. F is thrice differentiable and there
θˆI:=argmax θ1N1 (cid:80)N i=1Y˜ ilog 1−F(τ n+θ 1TX˜ i)
(8)
exists a thrice differentiable function h:R→R
+
such
+1− αY˜ i logF(τ n+θ 1TX˜ i), thatthefunctionF¯(Z >·+τ n)/F¯(Z >τ n)togetherwith
its first three derivatives converges uniformly over com-
and the condtional MLE is defined as
pact sets to h(.) and its three first derivatives denoted
θˆC:=argmax 1 (cid:80)N Y˜ log(cid:16) 1−G(τ +θTX˜ )(cid:17) as h(1),h(2),h(3), respectively.
θ1N i=1 i n 1 i (9)
+(1−Y˜)logG(τ +θTX˜ ),
i n 1 i The distributions that satisfy Assumption 3 include
exponential distribution, logistic regression model, and
where G(·) is defined as in (2). By the theory of M-
etc. We now provide our first main result.
estimators [49], both of the estimators are consistent,
i.e. θˆI →p θ and θˆC →p θ . We compare the perfor- Theorem 1 (Asymptotic Normality of MLE as
∗ ∗
manceofourproposedpseudoMLEθˆ ∗ as(6)withboth τ n →∞). Suppose Assumptions 1, 2, 3 hold, and for
θˆI and θˆC through numerical experiments on synthetic some Θ⊂Rd as a neighborhood of θ ∗, {F(τ n+θ 1Tx):
data (see Section 6.1) and empirical data (see Section θ 1 ∈Θ} is differentiable in quadratic mean at θ ∗. As-
7 and Appendix A). We find that θˆ ∗ outperforms θˆI sume that n(1−F(τ n))→∞, lim n→∞ (1−α)2(1 α−F(τn)) =
and θˆC for large values of τ , but it gradually under- c, and
n
p oue rrf co or nm jes ca tus rw ee asd ie ncr re ea ms ae rkth 1e ,v imal pu le yiτ nn g, iw tsh vi ach lueve ur nifi de es
r
E(cid:104) h( h1) (( θθ T∗T XX ))2 XXT(cid:105)
(10)
rare-event setup.
≻cE(cid:2) h(1)∗ (θTX)X(cid:3)E(cid:2) h(1)(θTX)XT(cid:3)
,
∗ ∗
4 Asymptotic Normality then as τ n → ∞, we have (cid:112) n(1−F(τ n))(θˆ ∗−θ ∗) →d
N(cid:0) 0,V−1(cid:1) , where
I tn het rh eis isse act ci eo rn t, aw ine rd ai nsc go eve or ft dh oa wt nfo sar mth pe lera rr ae teev αen ≪tse 1t ,u ap s, V =E(cid:104) h( h1) (( θθ T∗T XX ))2 XXT(cid:105)
long as it doesn’t go to zero too fast, downsampling
−cE(cid:2) h(∗ 1)(θTX)X(cid:3)E(cid:2) h(1)(θTX)XT(cid:3)
.
∗ ∗
with rate α maintains the statistical efficiency as that
of the full-sampling estimator. To demonstrate our Remark 2. Theorem 1 suggests that if
statement, we consider the case where P(Y = 1|X =
(1−α2)(1−F(τ ))
x)=1−F(τ +θTX) with τ →∞. lim n =0,
n ∗ n n→∞ α
Failure of Classical Maximum Likelihood Esti- then the resulting estimator is as efficient as the full-
mator Analysis Given the definition of ℓ˜(·,·,·;τ ) sample estimator (i.e. when α = 1). This happens
n
(cid:104) (cid:105)
above, we note that E ℓ˜(X˜,Y˜,θ ;τ ) → 0 regardless when either α is bounded from below by a positive
∗ n
of θ if α > 0. Thus as n → ∞, the criterion func- constant, or α →0 but 1−F(τn) →0, which is consis-
∗ α
tion doesn’t rely on the value of θ in the varying-rate tent with the discovery of Remark 2 in [50] for logistic
1
regime, hence the classical MLE theory cannot be ap- regression under rare event case.
plied here. A more detailed discussion is presented in Remark 3. We demonstrate the necessity of condition
the Appendix C. We now use an alternative way to (10) through a numerical illustration. Imagine that
derive the asymptotic normality of θˆ . when α is too close to zero, the right hand side of (10)
∗Optimal Downsampling for Imbalanced Classification with Generalized Linear Models
can blow up so the condition will be violated. We look V =E(cid:104) g(1)(θ ∗TX)2 XXT(cid:105)
g(θTX)
at a case of logistic regression where τ =10, θ =0.5, (cid:104) ∗ (cid:105) (cid:104) (cid:105)
and F(τ + θTx) =
eτn+θ∗Tx andn
X ∼
Un∗
if[0,1].
−cE g(1)(θ ∗TX)X E g(1)(θ ∗TX)XT .
n ∗ 1+eτn+θ∗Tx
We generate 105 random samples and P(Y = 1) ≈ Basedontheasymptoticcovariances,wearenowready
5.89×10−5. The estimation error is computed as the to formulate the optimization problem by considering
averagevalueof|θ −θˆ |from500randomexperiments. bothstatisticalefficiencyandcomputationalgains. We
∗ ∗
The x-axis corresponds to the α as the downsample now introduce the efficiency concept with a computa-
rate from negative samples. Figure 1 shows that when tional budget constraint.
α is too close to 0, the estimation error is large. But
when α falls in a proper range such that condition
5 Efficiency with a Budget Constraint
(10) holds, the estimation error for θˆ stabilizes and is
∗
kept as a small value, consistent with the theoretical
Theorem 1 and Theorem 2 indicate that full-sampling
findings that the mean squared error should be small
and E[∥θˆ −θ ∥2]≈tr[V−1]/(n(1−F(τ ))). (i.e. α = 1) and downsampling with a rate α such
∗ ∗ n that α ≪ 1 and 1−F(τn) = o(1) lead to the same
α
rate of convergence and asymptotic MSE. At the same
Generalized Scaled Asymptotic Normality Now
time,ifwechooseα≈P(Y =1),westillhavethesame
we generalize Assumption 3 to Assumption 4, which
convergenceratewhilethetracenormoftheasymptotic
is satisfied by more distributions in addition to those
covariance is kept as O(1). Intuitively, downsampling
satisfying Assumption 3, including both heavy-tailed
withamuchsmallerαcanhelptoreducecomputational
distributions (e.g. Pareto) and very light tailed distri-
costsignificantlywhilemaintainingstatisticalefficiency
butions (e.g. Gaussian tails).
underthecurrentrareeventsetup. Thismotivatesusto
Assumption 4. Suppose that F is thrice differen-
formulate the choice of optimal downsampling rate by
tiable and that there exists a thrice differentiable
considering this tradeoff between statistical efficiency
function g : R → R such that the function F¯(Z >
+ and computational cost with a budget constraint.
r(τ )·+τ )/F¯(Z >τ )togetherwithitsfirstthreederiva-
n n n
tivesconvergesuniformlyovercompactsetstog(.)and We adapt the framework of [16] for the definition of
its three first derivatives denoted as g(1),g(2),g(3), re- algorithm efficiency. Let the computational cost be
spectively. some strictly increasing function of the downsample
size, i.e. C(α;n,p ) = f(np +α(1−p )n) = f(n[p +
1 1 1 1
Assumption 4 includes more common distributions ex- α(1−p )]),wheref(·)isstrictlyincreasing,p =P(Y =
1 1
ceptforthosesatisfyingAssumption3. Forexample,for 1) and C(α;n,p ) is the cost function of sampling α
1
thestandardnormaldistribution,wecantaker(τ)= 1 proportion of the negative samples (i.e. observations
τ
and g(z) = e−z. For the Pareto distribution, we can with label Y = 0) with the original sample size n
(cid:16) (cid:17)γ
taker(τ)=τ andg(z)= 1 withγ >1,tosatisfy and positive ratio p 1. According to the asymptotic
1+z
efficiency principle in the canonical case of [16], we
the assumption but in this case we must assume X
define the asymptotic algorithm efficiency cost value as
has compact support so that we may assume (after
a rescaling) that θTX is less than 1, this will ensure the product of the sampling variance and the cost rate,
that zr˙(τ n)+τ
n
>∗ 0 when evaluating at z =θ ∗Tx and i.e. v(α;p 1) = lim n→∞C(α;n,p 1)Var(cid:16) θˆ ∗(cid:17) . Specifically,
keep the limits well defined. 4. Now we can extend our when f(·) is a linear function, i.e. with some constant
result of the asymptotic distribution to more general c , we have
0
cases.
(cid:16) (cid:17)
Theorem 2 (Generalized Scaled Asymptotic Normal- v(α;p 1)= lim c 0n(p 1+α(1−p 1))Var θˆ ∗ . (12)
n→∞
ity). Supposetheunderlyingbinaryclassificationmodel
is defined as Y =1(τ +r(τ )θTX)>0 with r(·) defined
n n ∗ Recall from Theorem 1 that under regular conditions,
as in Assumption 4. Assume that n(1 − F(τ )) →
∞,lim (1−α)2(1−F(τn)) =c, and n when τ n → ∞, we have (cid:112) n(1−F(τ n))(θˆ ∗ − θ ∗) →d
n→∞ α N(cid:0) 0,V−1(cid:1) . So n(1−F(τn))tr[Cov(θˆ ∗−θ∗)] → 1 as n → ∞.
E(cid:104) g(1)(θ ∗TX)2 XXT(cid:105) Underthedefinitionoftt hr( eV e− ffi1)
ciencycost(12), wehave
g(θTX) (11)
≻cE(cid:2) g(1)∗ (θTX)X(cid:3)E(cid:2) g(1)(θTX)XT(cid:3) . c0(p1+α(1−p1))d2 ≤ ν(α;p ) ≤ c0(p1+α(1−p1))κd2 , where κ
∗ ∗ tr((1−F(τn))V) 1 tr((1−F(τn))V)
is the condition number of V. A natural objective for
Then as τ →∞, we have
n efficiency optimization is min lim p1+α(1−p1).
α∈[0,1] n→∞ tr(V)
(cid:112) n(1−F(τ ))r(τ )(θˆ −θ )→d N (cid:0) 0,V−1(cid:1) , Theorem 3 (Optimal Downsampling Rate for Imbal-
n n ∗ ∗
anced Classification). Suppose Assumptions 1, 2, 3,
where and τ → ∞, n(1−F(τ )) → ∞, then the optimal
n nYan Chen, Jose Blanchet, Krzysztof Dembczynski, Laura Fee Nern, Aaron Flores
choice of downsampling rate is of τ and estimate θ for these different τ . Under
n ∗ n
eachα,wecomputethesolutionsto(3),i.e.,maximum
(cid:110) (cid:104) (cid:105) (cid:104) (cid:105)(cid:111)
2(1−F(τ ))tr E h(1)(θTX)X E h(1)(θTX)XT likelihood estimators, under 500 random environments,
n ∗ ∗
α∗ = tr{E[{h(1)(θTX)2/h(θTX)}XXT]} . and we also compute the mean-squared-error (MSE)
∗ ∗
for θ with respect to each α by averaging over the 500
∗
environments.
Henceforth, we have obtained explicit formulations of
the optimization problems for selecting the optimal Firstly, we compare the mean squared estimation error
downsample rate. Theorem 3 has provided guidelines of our estimator and the inverse-weighting estimator
for downsampling schemes in practice. The result can [50] for τ =6,7,8,9 for some of the α∈[0.00005,0.5].
n
easily be extended to generalized scaled result from These values τ correspond to P(Y =1) approximately
n
Theorem 2. equal to 0.002,0.0007,0.0002,0.000097. The numerical
results shown by Figure 6a (in Appendix A) are consis-
6 Application to Logistic Regression tent with our findings: when α is small and falls into
the proper range satisfying the conditions of Theorem
1, the mean-squared-error is close to that generated by
Equipped with all the findings from the previous sec-
α=0.5.
tions, we now focus on the logistic regression model
as an application. Given a sequence of τ n → ∞, de- Secondly, We focus on the range of α very close to
fine F(τ n+θ 1Tx):= 1+eτ en τn+ +θ1T θ1Tx x. A direct application of P m( eY an= sq1 u) arfo edr τ en rro= r1 fo0 r.0 l, o9 g. i8 st, i6 c.0 re, g5 r.0 es, sw ioe n,co wm hip cu ht ce ot rh ree
-
Theorem 1 indicates:
spond to the cases where P(Y = 1) is approximately
Proposition 3 (Asymptotic Normality for Lo-
equalto3.57×10−5,4.36×10−5,0.0019,0.0053respec-
gistic Regression). Assume n → ∞ and
1+eτn tively. We replicate our simulations 500 times for each
lim (1−α)2 = c where c is a constant. Then τ by comparing inverse-weighting estimator, condi-
n→∞ α(1+eτn) n
as τ →∞ and , we have tionalmaximumlikelihoodestimatorandourproposed
n
pseudo MLE.
(cid:112) nP(Y =1)(cid:16) θˆ ∗−θ ∗(cid:17) →d N (cid:16) 0,E X(cid:104) e−θ ∗TX(cid:105) V−1(cid:17) ,
From Figure 2 we see that for τ = 10,9.8 (large),
n
our proposed estimator outperforms both the inverse-
(cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)
whereV=E e−θ∗TXXXT −cE e−θ∗TXX E e−θ∗TXX . weighting estimator and the conditional maximum like-
Remark 4. If either α is bounded away from zero, lihoodestimator. ThisverifiesourstatementinRemark
1. However for τ = 6,5 (small), our proposed esti-
i.e., ∃ some absolute constant α ∈ (0,1) such that n
0 < α ≤ α ≤ 1, or α → 0 and 1 → 0, then mator is worse than the other two. Note that our
α(1+eτn) estimator is not consistent when τ is small by remark
c=0, and the asymptotic distribution is the same as n
1, so its under-performance is not surprising in this
that of the full-sampling case, so the estimator is as
scenario.
efficientasthefull-samplingone. Thisisalsoconsistent
with the findings of [50]. Finally, we plot the efficiency cost in Figure 6b (in
Remark 5. Our estimator is different from [50], where Appendix A) with computational budget constraint
[50] uses an inverse-weighting estimator for the inverse- accordingtoourdefinitionpreviouslyandthetradeoffis
weighting estimator (8). We illustrate the differences depicted numerically for the downsample rate selection
in the performance of our estimator and that of the as we discussed in Section 5. We have observed that
estimator considered by [50] further through numerical when τ n increases, the efficiency cost function becomes
experiments in Section 6.1. sharper, indicating it’s more sensitive to the choice
of α. Although our theoretical finding shows that
Proposition 4 (Optimal Downsampling Rate for Lo-
under the rare event case, a small choice of α such
gistic Regression). Suppose τ →∞, n →∞, the
n 1+eτn that (1−F(τ ))/α=o(1) doesn’t bringin information
optimal choice of downsampling rate is n
loss while reducing computational cost massively, the
2(1+eτn)−1tr(cid:110) E[e−θ ∗TXX]E[e−θ ∗TXX](cid:111) efficiencycostcanbeverysensitivetoαasthepositive
α∗ = . ratio goes to 0 according to Figure 6b, suggesting the
tr{E[e−θ ∗TXXXT]} necessity of a more prudent method of downsampling
rate selection.
6.1 Numerical Experiments
Wefocusonasettingwhereθ =0.5,andthecovariates
∗
X are drawn i.i.d. from a uniform distribution [0,1].
The sample size is n = 105. We fix different valuesOptimal Downsampling for Imbalanced Classification with Generalized Linear Models
7 Empirical Performance
ToverifytheperformanceoftheproposedpseudoMLE
on real imbalanced data, we compare the performance
of our estimator with the inverse-weighting estimator
on UCI imbalanced datasets in Table 1.
Dataset SampleSize Neg:PosRatio FeatureNumber
abalone_19 4,177 130:1 10
mammography 11,183 42:1 6
yeast_me2 1,484 28:1 8
abalone 4,177 9.7:1 10 (a) MSE:τ =10.0
n
ecoli 336 8.6:1 7
Table 1: Summary of UCI imbalanced datasets : sample
sizes, negative:positive ratios, feature numbers.
In order to adapt to our setup where we consider the
regimewithτ
n
→∞,wesetτ
n
suchthat1/(1+eτn)=
p , where the values of p are the positive ratio in the
1 1
imbalanced dataset. Then we use inverse-weighting
estimator and our pseudo-MLE estimator to fit θ
∗
(the coefficient for the features), and we compute the
(b) MSE:τ =9.8
log-losses for both estimators on the testing dataset. n
We replicate the experiment 500 times, and during
each round we randomly split the dataset into 80% for
training and 20% for testing. We then plot the average
log-losses and the confidence intervals for the log-losses
for each downsampling rate α in Figure 7, where these
α’s are chosen close to p (i.e. positive ratio) of each
1
dataset. We refer the readers to Appendix A.1 for
performancewithadditionalmoderateandsmallvalues
of τ .
n
Lastly, we also apply our method to neural networks (c) MSE:τ n =6.0
on some of those datasets. The simulation details and
insights are presented in Appendix A.2.
The numerical results suggest that the application of
the pseudo maximum likelihood estimator reduces the
out-of-samplelog-losserrorscomparedtothecommonly
used inverse-weighting estimators in practice.
8 Discussion
(d) MSE:τ =5.0
We propose a pseudo maximum likelihood estimator n
for a Generalized Linear Model binary classifier under Figure 2: On the left panel of each figure, we plot MSE
downsampling,withtheoreticalconvergenceguarantees of inverse-weighting (blue) vs. pseudo-MLE (green) vs.
for imbalanced data. We propose an efficiency cost conditional MLE (red) for α chosen around P(Y = 1)
notiontoguidedownsamplingrateselection. Forfuture for τ n = 10.0,9.8,6.0,5.0 with Logistic Regression. The
blue,green,red dashed lines correspond to the 95% confi-
work we are interested in exploring over-sampling and
dence intervals for the squared losses of inverse-weighting
focusingonotherperformancemetricsexceptformean- estimator, pseudo MLE and conditional MLE. The upper
squared-errors. and lower ends are computed by ±1.96∗ √σˆ and σˆ is the
500
standard deviation of squared losses at each alpha com-
putedover500randomenvironments. Ontherightpanelof
each figure the green solid lines correspond to the average
squaredlossdifferencesbetweeninverse-weightingestimator
and pseudo MLE, and the purple one corresponds to that
ofconditionalMLEminuspseudoMLE.Andthedashlines
are the 95% confidence intervals.Yan Chen, Jose Blanchet, Krzysztof Dembczynski, Laura Fee Nern, Aaron Flores
References noisy, and undersampled measured images. IEEE
transactions on image processing,6(12):1646–1658,
[1] Kellyn F Arnold, Vinny Davies, Marc de Kamps,
1997.
Peter WG Tennant, John Mbotwa, and Mark S
Gilthorpe. Reflection on modern methods: gen- [12] Charles Elkan. The foundations of cost-sensitive
eralized linear models for prognosis and interven- learning. JCAI’01, page 973–978, San Francisco,
tion—theory, practice and implications for ma- CA, USA, 2001. Morgan Kaufmann Publishers
chine learning. International journal of epidemiol- Inc.
ogy, 49(6):2074–2082, 2020.
[13] Andrew Estabrooks, Taeho Jo, and Nathalie Jap-
[2] Shaik Johny Basha, Srinivasa Rao Madala, Kolla kowicz. Amultipleresamplingmethodforlearning
Vivek, Eedupalli Sai Kumar, and Tamminina Am- from imbalanced data sets. Computational intelli-
mannamma. A review on imbalanced data clas- gence, 20(1):18–36, 2004.
sification techniques. In 2022 International con-
ference on advanced computing technologies and [14] William Fithian and Trevor Hastie. Local case-
applications (ICACTA), pages 1–6. IEEE, 2022. control sampling: Efficient subsampling in imbal-
anced data sets. Annals of statistics, 42(5):1693,
[3] Sébastien Bubeck et al. Convex optimization: 2014.
Algorithms and complexity. Foundations and
Trends® in Machine Learning, 8(3-4):231–357, [15] Dylan J Foster, Ayush Sekhari, Ohad Shamir,
2015. Nathan Srebro, Karthik Sridharan, and Blake
Woodworth. The complexity of making the gra-
[4] Nitesh V Chawla. Data mining for imbalanced dient small in stochastic convex optimization. In
datasets: An overview. Data mining and knowl- Conference on Learning Theory, pages 1319–1345.
edge discovery handbook, pages 875–886, 2010. PMLR, 2019.
[5] Nitesh V Chawla, Nathalie Japkowicz, and Alek-
[16] Peter W Glynn and Ward Whitt. The asymptotic
sander Kotcz. Special issue on learning from im- efficiency of simulation estimators. Operations
balanced data sets. ACM SIGKDD explorations research, 40(3):505–520, 1992.
newsletter, 6(1):1–6, 2004.
[17] JM Gorriz, Carmen Jimenez-Mesa, Fermín
[6] Zeyu Deng, Abla Kammoun, and Christos Thram-
Segovia, Javier Ramírez, SiPBA Group, and
poulidis. A model of double descent for high-
J Suckling. A connection between pattern classi-
dimensional binary linear classification. Infor-
fication by machine learning and statistical infer-
mation and Inference: A Journal of the IMA,
ence with the general linear model. IEEE Journal
11(2):435–495, 2022. ofBiomedicalandHealthInformatics,26(11):5332–
5343, 2021.
[7] Beiying Ding and Robert Gentleman. Classifica-
tion using generalized partial least squares. Jour-
[18] Guo Haixiang, Li Yijing, Jennifer Shang,
nal of Computational and Graphical Statistics,
Gu Mingyun, Huang Yuanyue, and Gong Bing.
14(2):280–298, 2005.
Learning from class-imbalanced data: Review of
[8] Annette J Dobson and Adrian G Barnett. An in- methods and applications. Expert systems with
troduction to generalized linear models. Chapman applications, 73:220–239, 2017.
and Hall/CRC, 2018.
[19] Hui Han, Wen-Yuan Wang, and Bing-Huan Mao.
Borderline-smote: a new over-sampling method in
[9] Georgios Douzas and Fernando Bacao. Self-
imbalanced data sets learning. In International
organizing map oversampling (somo) for imbal-
anced data set learning. Expert systems with Ap-
conferenceonintelligentcomputing,pages878–887.
plications, 82:40–52, 2017. Springer, 2005.
[20] AmiraKamilIbrahimHassanandAjithAbraham.
[10] Chris Drummond, Robert C Holte, et al. C4. 5,
Modeling insurance fraud detection using imbal-
class imbalance, and cost sensitivity: why under-
sampling beats over-sampling. In Workshop on anced data classification. In Advances in Nature
and Biologically Inspired Computing: Proceedings
learning from imbalanced datasets II, volume 11,
of the 7th World Congress on Nature and Biologi-
2003.
cally Inspired Computing (NaBIC2015) in Pieter-
[11] Michael Elad and Arie Feuer. Restoration of a maritzburg, South Africa, held December 01-03,
single superresolution image from several blurred, 2015, pages 117–127. Springer, 2016.Optimal Downsampling for Imbalanced Classification with Generalized Linear Models
[21] Nils Lid Hjort and David Pollard. Asymptotics [32] Wonjae Lee and Kangwon Seo. Downsampling
for minimisers of convex processes. arXiv preprint for binary classification with a highly imbalanced
arXiv:1107.3806, 2011. dataset using active learning. Big Data Research,
28:100314, 2022.
[22] Daniel Hsu, Vidya Muthukumar, and Ji Xu. On
the proliferation of support vectors in high dimen- [33] Guillaume LemaÃŽtre, Fernando Nogueira, and
sions. In International Conference on Artificial Christos K Aridas. Imbalanced-learn: A python
Intelligence and Statistics, pages 91–99. PMLR, toolbox to tackle the curse of imbalanced datasets
2021. in machine learning. Journal of machine learning
research, 18(17):1–5, 2017.
[23] ShamsulHuda,JohnYearwood,HerbertFJelinek,
[34] Jinyan Li, Simon Fong, Shimin Hu, Victor W
Mohammad Mehedi Hassan, Giancarlo Fortino,
Chu, Raymond K Wong, Sabah Mohammed, and
and Michael Buckland. A hybrid feature selection
Nilanjan Dey. Rare event prediction using simi-
withensembleclassificationforimbalancedhealth-
larity majority under-sampling technique. In Soft
care data: A case study for brain tumor diagnosis.
Computing in Data Science: Third International
IEEE access, 4:9145–9154, 2016.
Conference, SCDS 2017, Yogyakarta, Indonesia,
[24] Nathalie Japkowicz. Learning from Inbalanced November27–28,2017,Proceedings3,pages23–39.
Data Sets: Papers from the AAAI Workshop. Springer, 2017.
AAAI Press, 2000.
[35] Jinyan Li, Lian-sheng Liu, Simon Fong, Ray-
[25] Dan Jiang, Rongbin Xu, Xin Xu, and Ying Xie. mond K Wong, Sabah Mohammed, Jinan Fiaidhi,
Multi-view feature transfer for click-through rate Yunsick Sung, and Kelvin KL Wong. Adaptive
prediction. Information Sciences, 546:961–976, swarm balancing algorithms for rare-event pre-
2021. diction in imbalanced healthcare data. PloS one,
12(7):e0180830, 2017.
[26] Rie Johnson and Tong Zhang. Accelerating
[36] JoseyMathew,CheeKhiangPang,MingLuo,and
stochastic gradient descent using predictive vari-
ance reduction. Advances in neural information Weng Hoe Leong. Classification of imbalanced
processing systems, 26, 2013. data by oversampling in kernel space of support
vector machines. IEEE transactions on neural
[27] Gary King and Langche Zeng. Logistic regression networks and learning systems, 29(9):4065–4076,
in rare events data. Political analysis, 9(2):137– 2017.
163, 2001.
[37] AS More and Dipti P Rana. Review of random
[28] Bartosz Krawczyk. Learning from imbalanced forest classification techniques to resolve data im-
data: open challenges and future directions. balance. In 2017 1st International conference on
Progress in Artificial Intelligence, 5(4):221–232, intelligent systems and information management
2016. (ICISIM), pages 72–78. IEEE, 2017.
[29] Pawel Ksieniewicz. Undersampled majority class [38] John Ashworth Nelder and Robert WM Wedder-
ensemble for highly imbalanced binary classifica- burn. Generalized linear models. Journal of the
tion. In Second International Workshop on Learn- Royal Statistical Society Series A: Statistics in
ing with Imbalanced Domains: Theory and Appli- Society, 135(3):370–384, 1972.
cations, pages 82–94. PMLR, 2018.
[39] AlexandruNiculescu-MizilandRichCaruana. Pre-
dicting good probabilities with supervised learn-
[30] JungwonLee,OkkyungJung,YunhyeLee,Ohsung
ing. In Proceedings of the 22nd International Con-
Kim, and Cheol Park. A comparison and inter-
ference on Machine Learning, ICML ’05, page
pretation of machine learning algorithm for the
prediction of online purchase conversion. Journal 625–632, New York, NY, USA, 2005. Association
of Theoretical and Applied Electronic Commerce for Computing Machinery.
Research, 16(5):1472–1491, 2021. [40] Art B Owen. Infinitely imbalanced logistic regres-
sion. Journal of Machine Learning Research, 8(4),
[31] Kuang-chih Lee, Burkay Orten, Ali Dasdan, and
2007.
Wentong Li. Estimating conversion rate in dis-
play advertising from past erformance data. In [41] M Mostafizur Rahman and Darryl N Davis. Ad-
Proceedings of the 18th ACM SIGKDD interna- dressing the class imbalance problem in medical
tionalconferenceonKnowledgediscoveryanddata datasets. International Journal of Machine Learn-
mining, pages 768–776, 2012. ing and Computing, 3(2):224, 2013.Yan Chen, Jose Blanchet, Krzysztof Dembczynski, Laura Fee Nern, Aaron Flores
[42] Alexander Rakhlin, Ohad Shamir, and Karthik national symposium on multimedia (ISM), pages
Sridharan. Making gradient descent optimal for 483–488. IEEE, 2015.
strongly convex stochastic optimization. arXiv
preprint arXiv:1109.5647, 2011. [53] Yuguang Yan, Mingkui Tan, Yanwu Xu, Jiezhang
Cao, Michael Ng, Huaqing Min, and Qingyao Wu.
[43] Talayeh Razzaghi, Oleg Roderick, Ilya Safro, and Oversampling for imbalanced data via optimal
Nick Marko. Fast imbalanced classification of transport. In Proceedings of the AAAI Conference
healthcare data with missing values. In 2015 18th on Artificial Intelligence, volume 33, pages 5605–
International Conference on Information Fusion 5612, 2019.
(Fusion), pages 774–781. IEEE, 2015.
[54] Gang Zhang, Lixin Wang, Alistair P Duffy, Hugh
[44] Akshay Shah and Siddhesh Nasnodkar. The im- Sasse, Danilo Di Febo, Antonio Orlandi, and
pacts of user experience metrics on click-through Karol Aniserowicz. Downsampled and undersam-
rate (ctr) in digital advertising: A machine learn- pled datasets in feature selective validation (fsv).
ing approach. Sage Science Review of Applied IEEE transactions on electromagnetic compatibil-
Machine Learning, 4(1):27–44, 2021. ity, 56(4):817–824, 2014.
[45] Mayuri S Shelke, Prashant R Deshmukh, and
Vijaya K Shandilya. A review on imbalanced
data handling using undersampling and oversam-
pling technique. Int. J. Recent Trends Eng. Res,
3(4):444–449, 2017.
[46] Deepti Sisodia and Dilip Singh Sisodia. Data
sampling strategies for click fraud detection us-
ing imbalanced user click data of online advertis-
ing: an empirical review. IETE Technical Review,
39(4):789–798, 2022.
[47] Deepti Sisodia and Dilip Singh Sisodia. A hy-
briddata-levelsamplingapproachinlearningfrom
skewed user-click data for click fraud detection in
online advertising. Expert Systems, 40(2):e13147,
2023.
[48] Adil Yaseen Taha, Sabrina Tiun, Abdul Hadi
Abd Rahman, and Ali Sabah. Multilabel over-
samplingandunder-samplingwithclassalignment
forimbalancedmultilabeltextclassification. Jour-
nal of Information and Communication Technol-
ogy, 20(3):423–456, 2021.
[49] Aad W Van der Vaart. Asymptotic statistics, vol-
ume 3. Cambridge university press, 2000.
[50] HaiYing Wang. Logistic regression for massive
datawithrareevents. InInternational Conference
on Machine Learning, pages 9829–9836. PMLR,
2020.
[51] Xiaolin Wu, Xiangjun Zhang, and Xiaohan Wang.
Lowbit-rateimagecompressionviaadaptivedown-
sampling and constrained least squares upconver-
sion. IEEE Transactions on Image Processing,
18(3):552–561, 2009.
[52] Yilin Yan, Min Chen, Mei-Ling Shyu, and Shu-
Ching Chen. Deep learning for imbalanced mul-
timedia data classification. In 2015 IEEE inter-Optimal Downsampling for Imbalanced Classification with Generalized Linear Models
Supplementary Material
A Additional Simulation Details
A.1 Additional Results for Applying Logistic Regression
Varying values of τ for Logistic Regression on Empirical Data We run additional simulations on
n
abalone_19 and yeast_me2 data for varying values of τ . The previous setting of abalone_19 data is τ =
n n
log(1/p −1)=4.86 and we plot the results with τ =0.01,0.5,1.0,2.0,3.0 in Figure 3. The previous setting of
1 n
yeast_me2 is τ =log(1/p −1)=3.34 and we plot the results with τ =0.01,0.5,1.0,2.0,2.5 in Figure 4. The
n 1 n
plots show that our pseudo MLE estimator outperforms the inverse-weighting estimator even for those moderate
and small values of τ .
n
(a) log-loss:τ =0.5 (b) log-loss:τ =1.0
n n
(c) log-loss:τ =2.0 (d) log-loss:τ =3.0
n n
(e) log-loss:τ =0.01
n
Figure 3: Additional results for abalone_19 dataset for small and moderate values of τ .
nYan Chen, Jose Blanchet, Krzysztof Dembczynski, Laura Fee Nern, Aaron Flores
(a) log-loss:τ =0.5 (b) log-loss:τ =1.0
n n
(c) log-loss:τ =2.0 (d) log-loss:τ =2.5
n n
(e) log-loss:τ =0.01
n
Figure 4: Additional results for yeast_me2 dataset for small and moderate values of τ .
nOptimal Downsampling for Imbalanced Classification with Generalized Linear Models
A.2 Simulation Results and Insights for Neural Networks
Neural Networks We plot the log-losses of neural networks applied to imbalanced UCI real data (yeat_me2,
abalone_19, ecoli) for different downsampling rate α in Figures 5a, 5b, 5c. Dashed lines correspond to the
cross-entropy loss, solid lines correspond to the customized loss function implied by our estimator. The results
are average log-losses for each training epoch averaging across 500 random train/test splitting of the real data.
The solid lines are below the dashed lines within the same color, indicating the customized loss leading to better
performance under the fixed small downsample rate α given here. The neural networks are trained with 3 dense
layers with relu activation and one outer layer with sigmoid output.
Thoughamaximumlikelihoodanalysisofaneuralnetworkmodelissignificantlymorechallengingthangeneralized
linear model, the insight about downsampling alone is intuitive and should carry over to the case of neural
networks in additional to GLMs. For example, we could use a small dataset to train a neural network, and exploit
the fact that the output activation function is often a GLM. We can then apply our results to the GLM portion
and use this to adjust the sample size according to our optimal sample selection.
(a) Log-losses: UCI yeast_me2 data with NN (b) Log-losses: UCI abalone_19 data (NN)
(c) Log-losses: UCI ecoli data with NN
(a) Mean-squared-error of our estimator vs. Inverse- (b) Efficiencycostcombiningbothstatisticalefficiencyand
weighting estimator under different downsample rates. computational cost for τ =5,5.5,6,7.
n
Figure 6: Mean-squared-error and Efficiency costsYan Chen, Jose Blanchet, Krzysztof Dembczynski, Laura Fee Nern, Aaron Flores
(a) UCI abalone_19 (b) UCI mammmography
(c) UCI yeast_me2 (d) UCI abalone
(e) UCI ecoli
Figure 7: Log-loss of inverse-weighting (purple) vs. log-loss of pseudo-MLE (green) for α chosen around P(Y =1) for
each data set by applying Logistic Regression. We randomly split the original dataset into 80% for training and 20% for
testing during each replication. The log-losses are all computed on test datasets. The purple dashed lines correspond to
the 95% confidence intervals of the log-loss of inverse-weighting estimator, and the green dashed lines correspond to the
95% confidence intervals of the log-loss of pseudo MLE. The upper and lower ends are computed by ±1.96∗ √σˆ and σˆ is
500
the standard deviation of log-loss values at each alpha computed over 500 random environments.Optimal Downsampling for Imbalanced Classification with Generalized Linear Models
B Additional Lemmas and Proofs for the Proposed Estimator
For the rest of the paper, we use P˜ to denote the joint distibution of downsample variables (X˜,Y˜), use E˜ to
denote the expectation with respect to P˜, use P˜ for the empirical measure induced by {X˜ ,Y˜}N , and E˜ for
N i i i=1 N
the expectation taken with respect to P˜ .
N
Lemma 1 (Counterexample). Suppose F(z) is strictly increasing, and suppose
1) E (cid:2) F′(τ +θTX)X(cid:3) ̸=0,
X n ∗
2) there exists a unique θ˜
1
∈Θ such that E X(cid:104) [1−(1−α 1) −F (( 1τ −n+ α)θ F∗T (X τn)] +F θ˜′ 1T(τ Xn+ )θ˜ 1TX)X(cid:105) =0,
Then (1) leads to a biased estimator for θ .
∗
Furthermore, if {x ∈ X|θ˜Tx = 0} ̸= ∅ and {x ∈ X|θTx = 0}∩{x ∈ X|θ˜Tx = 0} ∈/ {∅,X}, then the prediction
1 ∗ 1
score obtained by the procedure described above (i.e. solving (1) and then applying isotonic regression) is also
biased.
Proof of Lemma 1. The first-order condition for (1) is
1 (cid:88)N
Y˜
−F′(τ n+θˆ 1TX˜ i)X˜ iT +(1−Y˜)F′(τ n+θˆ 1TX˜ i)X˜ iT
=0,
N i 1−F(τ +θˆTX˜ ) i F(τ +θˆTX˜ )
i=1 n 1 i n 1 i
i.e. θˆ satisfies
1
1 (cid:88)N (1−Y˜ i−F(τ n+θˆ 1TX˜ i))F′(τ n+θˆ 1TX˜ i)X˜ iT
=0.
N F(τ +θˆTX˜ )(1−F(τ +θˆTX˜ ))
i=1 n 1 i n 1 i
By Lemma 3 and the strong law of large numbers, for any θ ∈Θ,
1
1 (cid:80)N (1−Y˜ i−F(τn+θ 1TX˜ i))F′(τn+θ 1TX˜ i)X˜ iT
−a. →sN . E˜(cid:104)i (= 11 −Y˜ i−FF (( ττ nn ++ θθ 1T1T XX ˜˜ ii )) )( F1− ′(F τn(τ +n θ+ 1Tθ X˜1T iX )˜ Xi ˜) iT) (cid:105)
(cid:20)(cid:20)
F(τn+θ 1TX˜ i)(1−F(τn+θ 1TX˜ i))
(cid:12) (cid:21)(cid:21)
=E˜ (1−Y˜ Fi (− τF n+(τ θn 1T+ X˜θ i1T )X (˜ 1i −)) FF (′ τ( nτ +n+
θ
1Tθ 1T X˜X i˜ )i ))X˜ iT(cid:12) (cid:12) (cid:12)X˜ i
= E˜(cid:104) [G(τn+θ 1TX˜ i)−F(τn+θ 1TX˜ i)]F′(τn+θ 1TX˜ i)X˜ iT(cid:105)
=( −1) (1−α)E˜(cid:104)F( Fτn ′(+ τθ n1T +X θ˜ 1Ti) X( ˜1 i− )XF
˜
iT(τn+ (cid:105)θ 1TX˜ i))
E1− (cid:20)( [1 1− −α (1) −F α( )τ Fn (+ τnθ +1T θX ∗T˜ i X) )]F′(τn+θ1TX)X(cid:21)
= −(1−α)
X 1−(1−α)F(τn+θ1TX)
,
(2) P(Y=1)+αP(Y=0)
where (1) follows from Proposition 2 and (2) uses Lemma 4. Thus by Lemma 7, θˆ →p θ˜ such that
1 1
E X(cid:104) [1−(1−α) 1F −( (τ 1n −+ αθ )∗T FX (τ) n]F +′ θ˜( 1Tτn X+ )θ˜ 1TX)XT(cid:105) =0.
Furthermore, taking in true parameter θ , the expected value under probability measure P˜ (i.e. the joint
∗
distribution of (Y˜,X˜)) is equal to
E˜(cid:104) (1−Y˜ i−F(τn+θ ∗TX˜ i))F′(τn+θ ∗TX˜ i)X˜ iT(cid:105)
(cid:20)
(cid:20)F(τn+θ ∗TX˜ i)(1−F(τn+θ ∗TX˜ i))
(cid:12) (cid:21)(cid:21)
=E˜ E˜ (1−Y˜ Fi (− τF n+(τ θn ∗T+ X˜θ i∗T )X (˜ 1i −)) FF (′ τ( nτ +n+
θ
∗Tθ ∗T X˜X i˜ )i ))X˜ iT(cid:12) (cid:12) (cid:12)X˜ i
= E˜(cid:104) −F′(τn+θ ∗TX˜ i)G¯(τn+θ ∗TX˜ i)X˜ iT + F′(τn+θ ∗TX˜ i)G(τn+θ ∗TX˜ i)X˜ iT(cid:105)
(1) 1−F(τn+θ ∗TX˜ i) F(τn+θ ∗TX˜ i)
=
−(1−α)E X[F′(τn+θ ∗TX)XT]
,
(2) P(Y=1)+αP(Y=0)Yan Chen, Jose Blanchet, Krzysztof Dembczynski, Laura Fee Nern, Aaron Flores
where (1) uses Proposition 2 and (2) uses Lemma 4, and E is the expectation taken with respect to the
X
distribution of X from the full data. Thus under the given conditions, E (cid:2) F′(τ +θTX)XT(cid:3) ≠ 0, while there
X n ∗
exists some θ˜ ,
1
E X(cid:104) [1−(1−α) 1F −( (τ 1n −+ αθ )∗T FX (τ) n]F +′ θ˜( 1Tτn X+ )θ˜ 1TX)XT(cid:105) =E X(cid:104) [1−(1−α 1) −F (( 1τn −+ αθ )F∗TX (τ) n] +φ( θ˜τ 1Tn X+ )θ˜ 1TX)XT(cid:105) =0.
Thus θˆ →p θ˜ but θ˜ ̸=θ .
1 1 1 ∗
Consequently, under the third condition in the lemma, there exists x ,x ∈ X such that θ˜T(x −x ) = 0
1 2 1 1 2
while θT(x −x ) ̸= 0. This is because there exists ∆ ∈ X such that ∆ is in the subspace defined by the
∗ 1 2
hyperplane induced by θ˜ : ∆ ∈ {x ∈ X|θ˜Tx = 0} while θT∆ ̸= 0. Then for any x ∈ X, there exists λ
1 1 ∗ 1
sufficiently small such that x = x +λ∆ ∈ X, with θ˜T(x −x ) = 0 while θT(x −x ) ̸= 0. Therefore,
2 1 1 1 2 ∗ 1 2
τ +θ˜Tx =τ +θ˜Tx while τ +θTx ̸=τ +θTx . Thus suppose there exists a monotone transformation g
n 1 1 n 1 2 n ∗ 1 n ∗ 2
such that F(τ +θTx)=g◦F(τ +θ˜Tx) for all x∈X, then F(τ +θTx )=g◦F(τ +θ˜Tx ). Also note that
n ∗ n 1 n ∗ 1 n 1 1
τ +θ˜Tx =τ +θ˜Tx thusg◦F(τ +θ˜Tx )=g◦F(τ +θ˜Tx )=F(τ +θTx ),thusF(τ +θTx )=F(τ +θTx ),
n 1 1 n 1 2 n 1 1 n 1 2 n ∗ 2 n ∗ 1 n ∗ 2
which leads to contradiction because F is strictly increasing.
Lemma 2. Let P be the joint distribution of (Y,X), for any y ∈{0,1} and x∈X, define
yP(Y =1,X =x)+(1−y)αP(Y =0,X =x)
P˜(Y˜ =y,X˜ =x):= , (13)
P(Y =1)+αP(Y =0)
then (13) defines a probability distribution P˜ with respect to downsample random variables (Y˜,X˜ ).
i i
Proof of Lemma 2. Note that the downsampling procedure is equivalent to
(cid:16) (cid:17)
Y˜,X˜ =1(Y =1)(Y,X)+1(Y =0)1(U ≤α)(Y,X), (14)
where U ∼Uniform[0,1]and U ⊥⊥(Y,X). Thus the joint distribution (density) of (Y˜,X˜) with respect to P
(Y,X)
as the joint law of (Y,X) can be written as
(cid:16)(cid:16) (cid:17) (cid:17)
P Y˜,X˜ =(y,x) =1(y=1)P(Y =1,X =x)+1(y=0)αP(Y =0,X =x)
(Y,X)
=yP(Y =1,X =x)+(1−y)αP(Y =0,X =x). (15)
So integrating with respect to (y,x) we have
(cid:90)
P[(Y =1,X =x)+αP(Y =0,X =x)]dx=P(Y =1)+αP(Y =0).
X
So by definition
(cid:90)
(cid:88) P˜(Y˜ =y,X˜ =x)dx=1.
Xy∈{0,1}
Further note that P˜(Y˜ =y,X˜ =x)∈[0,1] for any x ∈ X and y ∈ {0,1}. Thus P˜ is indeed a valid probability
distribution defined for (Y˜,X˜).
Lemma 3 (i.i.d. Property). The downsampled data {(X˜ ,Y˜)}N are i.i.d. generated with respect to P˜ defined in
i i i=1
(13) of Lemma 2.
Proof of Lemma 3. Recall that the Generalized Linear Model is defined as follows: For some latent variable
Z, the label is defined as Y =1(cid:0) Z >τ n+θ ∗TX(cid:1) , and P(cid:0) Y =1(cid:12) (cid:12)X =x(cid:1) =F¯ Z(τ n+θ ∗Tx). For ∀i ∈ [N], let (Y i,X i)
denote the full-sample random variable. Thus for i,j ∈[N], i̸=j, given any pairs of event (y,x), (y′,x′), with
P denoting the joint distribution of the full-sample random variables (Y ,X ), and U denote a uniform random
i iOptimal Downsampling for Imbalanced Classification with Generalized Linear Models
variable on [0,1] such that U ⊥⊥{(X ,Y )}n , by (15) we have
i i i=1
(cid:16)(cid:16) (cid:17) (cid:16) (cid:17) (cid:17)
P Y˜,X˜ =(y,x), Y˜ ,X˜ =(y′,x′)
i i j j
(cid:16) (cid:16) (cid:17) (cid:17)
= P (Y ,X )=(1,x), Y˜ ,X˜ =(y′,x′)
(a) i i j j
(cid:16) (cid:16) (cid:17) (cid:17)
+P (Y ,X )=(0,x),U ≤α, Y˜ ,X˜ =(y′,x′)
i i j j
(cid:16)(cid:16) (cid:17) (cid:17)
= P((Y ,X )=(1,x))P Y˜ ,X˜ =(y′,x′)
(b) i i j j
(cid:16)(cid:16) (cid:17) (cid:17)
+P((Y ,X )=(0,x),U ≤α)P Y˜ ,X˜ =(y′,x′)
i i j j
(cid:16)(cid:16) (cid:17) (cid:17) (cid:16)(cid:16) (cid:17) (cid:17)
= P Y˜,X˜ =(y,x) P Y˜ ,X˜ =(y′,x′) ,
(c) i i j j
where (a) uses the fact that 1((Y ,X ) = (y,x),Y = 1) and 1((Y ,X ) = (y,x),Y = 0,U ≤ α) are disjoint
i i i i i i
events, and (b) uses the fact that (Y˜ ,X˜ )=L(Y ,X ,U) as some joint law of (Y ,X ,U), which is independent
j j j j j j
(cid:16)(cid:16) (cid:17) (cid:17)
of (Y ,X ). Moreover, (c) uses the fact that P Y˜,X˜ =(y,x) =P((Y ,X )=(1,x))+P((Y ,X )=(0,x),U ≤α)
i i i i i i i i
according to (14). Thus (Y˜,X˜ ) and (Y˜ ,X˜ ) are independent with respect to P.
i i j j
Note that P˜ =P/(P(Y =1)+αP(Y =0)), where given τ ,θ , P(Y =1)+αP(Y =0) is a constant, thus (Y˜,X˜ )
n ∗ i i
and (Y˜ ,X˜ ) are independent with respect to P˜. Obviously (Y˜ ,X˜ ) are identically generated. So the result
j j j j
follows.
Proof of Proposition 1. From (14) we know that for y ∈{0,1}, x∈X, and P as the joint law of (Y,X), we have
(cid:16)(cid:16) (cid:17) (cid:17)
P Y˜,X˜ ∈(y,x) =P((Y,X)∈(y,x),Y =1)+P((Y,X)=(y,x),Y =0,U ≤α)
=1(y=1)P(Y =1,X =x)+1(y=0)αP(Y =0,X =x)
Note that
P˜(cid:16) Y˜ =1(cid:12) (cid:12)X˜ =x(cid:17) = P˜(Y˜=1,X˜=x)
P˜(X˜=x)
=
P(Y˜=1,X˜=x)/(P(Y=1)+αP(Y=0))
P(Y˜=1,X˜=x)/(P(Y=1)+αP(Y=0))+P(Y˜=0,X˜=x)/(P(Y=1)+αP(Y=0))
= P(Y˜=1,X˜=x) =P(Y˜ =1|X˜ =x)
P(Y˜=1,X˜=x)+P(Y˜=0,X˜=x)
=
P(Y=1,X=x)
P(Y=1,X=x)+αP(Y=0,X=x)
=
P(Y=1|X=x)P(X=x)
P(Y=1|X=x)P(X=x)+P(Y=0|X=x)P(X=x)
(cid:12)
P(Y=1(cid:12)X=x)
= (cid:12) (cid:12) .
P(Y=1(cid:12)X=x)+αP(Y=0(cid:12)X=x)
When α=1, P(cid:16) Y˜ =1(cid:12) (cid:12)X˜ =x(cid:17) =P(Y =1(cid:12) (cid:12)X =x). Note that P(cid:0) Y =1(cid:12) (cid:12)X =x(cid:1) =F¯ Z(τ n+θ ∗Tx)=1−F Z(τ n+
θ ∗Tx), then P(cid:16) Y˜ =1(cid:12) (cid:12)X˜ =x(cid:17) =
F¯ Z(τn+θ
∗TF x¯ Z )+(τ (n 1−+ Fθ ¯∗T Zx (τ)
n+θ ∗Tx))α
= (1−αF¯ )Z
F¯
Z(τ (n τ+ n+θ ∗T
θ
∗Tx) x)+α. Let G¯(z) =
F¯
Z(zF )¯ (Z 1( −z α) )+α,
then P(Y˜ =1|X˜ =x)=G¯(τ +θTx). Note G¯(∞)=0, G¯′(z)<0, G¯(−∞)=1, thus there exists some W, such
n ∗
that G¯(z) = F¯ (z), where F¯ (z) = 1−F (z), and F (·) is the c.d.f. of W, and G¯(z) = 1 ⇐⇒
W W W W (1−α)+α/F¯
Z
F¯ = αG¯ .
Z 1−(1−α)G¯
Lemma 4 (Distribution of X˜ with respect to P˜). The density function of X˜ at X˜ = x is µ˜(x) =
i i i
[1−(1−α)F(τn+θ ∗Tx)]µ(x).
P(Y=1)+αP(Y=0)
Proof of Lemma 4. From (14) and previous proofs, with P denoting the joint law of (Y,X), we have
F¯(τn+θ ∗Tx)µ(x)
=
P(Y=1,X=x)
P(Y=1)+αP(Y=0) P(Y=1)+αP(Y=0)
=
P(Y˜=1,X˜=x) =P˜(cid:16)
Y˜ =1,X˜
=x(cid:17)
=G¯(τ +θTx)µ˜(x),
P(Y=1)+αP(Y=0) n ∗
αF(τn+θ ∗Tx)µ(x) = αP(Y=0,X=x)
P(Y=1)+αP(Y=0) P(Y=1)+αP(Y=0)
=
P(Y˜=0,X˜=x) =P˜(cid:16)
Y˜ =0,X˜
=x(cid:17)
=G(τ +θTx)µ˜(x).
P(Y=1)+αP(Y=0) n ∗Yan Chen, Jose Blanchet, Krzysztof Dembczynski, Laura Fee Nern, Aaron Flores
so the density function of X˜
i
wtih respect to P˜ at X˜
i
=x is equal to µ˜(x)= [1−( P1 (− Yα =)F 1)( +τn α+ P(θ Y∗T =x 0)] )µ(x).
Proof of Proposition 2. Note that by definition the joint distribution (density) of (Y˜,X˜) with respect to P˜ can
be written as
(cid:16) (cid:17)
P˜ Y˜ =y,X˜ =x
(cid:16) (cid:17) (cid:16) (cid:17)
=yP˜ Y˜ =1|X˜ =x µ˜(x)+(1−y)P˜ Y˜ =0|X˜ =x µ˜(x)
=yG¯(τ +θTx)µ˜(x)dx+(1−y)G(τ +θTx)µ˜(x)dx
n ∗ n ∗
(cid:16) (cid:17)y(cid:16) (cid:17)1−y
= F¯(τ +θTx) µ(x) αF(τ +θTx) µ(x) ,
n ∗ P(Y=1)+αP(Y=0) n ∗ P(Y=1)+αP(Y=0)
where the third equality in the above uses Lemma 4.
Let E˜[·] denote the expectation taken with respect to P˜, then we have
(cid:104) (cid:16) (cid:17)(cid:105)
M(θ ;τ ):=E˜ logP˜ Y˜,X˜
1 n i i
=E˜(cid:20) Y˜ (cid:16) log(cid:16) F¯(τ +θTX˜ )(cid:17) +log µ(X˜ i) (cid:17)
i n 1 i P(Y=1)+αP(Y=0)
+(1−Y˜)(cid:16) log(cid:16) αF(τ +θTX˜ )(cid:17) +log µ(X˜ i) (cid:17)(cid:21)
i n 1 i P(Y=1)+αP(Y=0)
(cid:104) (cid:105)
=E˜ Y˜ logF¯(τ +θTX˜ )+(1−Y˜)logαF(τ +θTX˜ )+logµ(X˜ )
i n 1 i i n 1 i i
−log[P(Y =1)+αP(Y =0)]
(cid:104) (cid:105)
=E˜ Y˜ logF¯(τ +θTX˜ )+(1−Y˜)logαF(τ +θTX˜ )+logµ(X˜ )
i n 1 i i n 1 i i
−log(cid:82) (cid:2)
1−(1−α)F(τ
+θTx)(cid:3)
µ(x)dx.
X n 1
We then have θ ∈ argmax M(θ ;τ ), and note from Lemma 3 (X˜ ,Y˜) are i.i.d. with respect to P˜, so by
∗ θ1∈Θ 1 n i i
strong law of large numbers,
1 (cid:80)N Y˜ logF¯(τ +θTX˜ )+(1−Y˜)logαF(τ +θTX˜ )+logµ(X˜ )
N −li o= g1(cid:82) i (cid:2) 1−(1n −α)F1 (τi +θTx)(cid:3) µi
(x)dx
n 1 i i
(cid:104) X n 1 (cid:105)
→p E˜ Y˜ logF¯(τ +θTX˜ )+(1−Y˜)logαF(τ +θTX˜ )+logµ(X˜ )
i n 1 i i n 1 i i
−log(cid:82) (cid:2)
1−(1−α)F(τ
+θTx)(cid:3)
µ(x)dx.
X n 1
Further note that given the down-sample (X˜ ,Y˜), 1 (cid:80)N logµ(X˜ ) doesn’t depend on τ or θ , thus the
i i N i=1 i n 1
maximum likelihood estimator can be defined as
(cid:16) (cid:17)
θˆ =argmax 1 (cid:80)N ℓ X˜ ,Y˜,θ ;τ ,
∗ θ1 N i=1 i i 1 n
where
(cid:90) (cid:104) (cid:105)
ℓ(x˜,y˜,θ ;τ )=y˜logF¯(τ +θTx˜)+(1−y˜)logαF(τ +θTx˜)−log 1−(1−α)F(τ +θTx)µ(x) dx.
1 n n 1 n 1 n 1
X
Thus the result follows.
C Proofs for Asymptotic Analysis
Discussion on the failure of classical MLE analysis Note that
(cid:104) (cid:105) (cid:104) (cid:105)
E ℓ˜(X˜,Y˜,θ ;τ ) =E Y˜ logF¯(τ +θTX˜)+(1−Y˜)logαF(τ +θTX˜)
∗ n (Y˜,X˜) n ∗ n ∗
−E {log 1 (cid:80)N [1−(1−α)F(τ +θTX˜ )]}
(Y˜,X˜) N i=1 n 1 i
(cid:104) (cid:105)
=E G¯(τ +θTX˜)logF¯(τ +θTX˜)+G(τ +θTX˜)logαF(τ +θTX˜)
X˜ n ∗ n ∗ n ∗ n ∗
−E {log 1 (cid:80)N [1−(1−α)F(τ +θTX˜ )]}
(Y˜,X˜) N i=1 n 1 i
=
E X[(1−F(τn+θ ∗TX))log(1−F(τn+θ 1TX))+αF(τn+θ ∗TX)logαF(τn+θ 1TX)]
P(Y=1)+αP(Y=0)
−E {log 1 (cid:80)N [1−(1−α)F(τ +θTX˜ )]}
(Y˜,X˜) N i=1 n 1 iOptimal Downsampling for Imbalanced Classification with Generalized Linear Models
Note that F(τ + θTx) → 1 as n → ∞ for any x ∈ X and θ ∈ Θ. Since xlogx → 0 as x → 0, so
n 1 1
(cid:104) (cid:105)
E ℓ˜(X˜,Y˜,θ ;τ ) → log(α)−log(α) = 0 regardless of the value of θ if α > 0. Thus as n → ∞, the criterion
∗ n ∗
function doesn’t rely on the value of θ in the varying-rate regime, thus the classical MLE theory cannot be
1
applied here.
C.1 Proof of Theorem 1
Proof of Theorem 1. Given τ , note that
n
θˆ =argmax 1 (cid:80)N Y˜ logF¯(τ +θTX˜ )+(1−Y˜)logαF(τ +θTX˜ )
∗ θ1∈Θ N i=1 i (cid:110) n 1 (cid:104) i i n (cid:105)(cid:111)1 i
−log 1 (cid:80)N 1−(1−α)F(τ +θTX˜ )
N i=1 n 1 i
Denote
L (θ )=∆ L(θ ;τ )
n 1 1 n
(cid:104) (cid:105)
=∆ (cid:80)N Y˜ logF¯(τ +θTX˜ )+(1−Y˜)logαF(τ +θTX˜ )
i=1 i n 1 i i n 1 i
−log{1 (cid:80)N [1−(1−α)F(τ +θTX˜ )]}
=(cid:80)N (cid:104) Y˜N log i=1 F¯(τn+θ1TX˜ i)n 1 i +(1−Y˜)log αF(τn+θ1TX˜ i) (cid:105)
i=1
(cid:20)
i { N1 (cid:80)N i=1[1−(1−α)F(τn+θ 1TX˜ i)]} i { N1 (cid:80)N i=1[1−(1−α)F(τn+θ 1TX˜ i)]}
=(cid:80)n
Y log
F¯(τn+θ1TXi)
i=1 i { N1 (cid:80)N i=1[1−(1−α)F(τn+θ 1TX˜ i)]}
(cid:21)
+(1−Y )1(U ≤α)log
αF(τn+θ1TXi)
,
i i { N1 (cid:80)N i=1[1−(1−α)F(τn+θ 1TX˜ i)]}
(cid:112)
where {U }n is i.i.d. uniform random variable on [0,1], and U ⊥⊥{Y ,X }n . Let a = n(1−F(τ )). So
i i=1 i i i i=1 n n
w =a (θˆ −θ ) is the maximizer of
n n ∗ ∗
H(w):=L (θ +a−1w)−L (θ ).
n ∗ n n ∗
For w =a (θˆ−θ ), define g(t)=L (θ +t(θˆ−θ )) for t∈[0,1]. By Taylor expansion, for some γ ∈(0,1), we
n ∗ n ∗ ∗
have g(1)−g(0)=g′(0)+ 1g′′(γ), i.e.
2
H(w) =∇ L (θ )T(a−1w)+ 1(θˆ−θ )T∇2 L (θ +γ(θˆ−θ ))(θˆ−θ )
=(aθ −1 1wn T)∗
∇
Ln
(θ )+
2 1a−2wT∗
∇2
Lθ1 (θn +∗
γ(θˆ−θ
))∗
w.
∗
n θ1 n ∗ 2 n θ1 n ∗ ∗
Denote J (θ ):= 1 (cid:80)N [1−(1−α)F(τ +θTX˜ )], and
N 1 N i=1 n 1 i
ℓ(x,y,u,θ 1;τ n) =ylog
{ N1 (cid:80)N
i=1[1F −¯( (τ 1n −+ αθ )1T Fx (τ)
n+θ 1TX˜ i)]}
+(1−y)1(u≤α)log αF(τn+θ 1Tx)
{ N1 (cid:80)N i=1[1−(1−α)F(τn+θ 1TX˜ i)]}
=ylogF¯(τn+θ 1Tx) +(1−y)1(u≤α)logαF(τn+θ 1Tx).
JN(θ1) JN(θ1)
Then ∇ J (θ )=−(1−α)1 (cid:80)N F′(τ +θTX˜ )X˜T, and
θ1 N 1 N i=1 n 1 i i
∇ ℓ(x,y,u,θ ;τ )
=y(cid:104)θ1
−F′(τn+θ
1Tx1 )xTn
+
(1−α) N1 (cid:80)N i=1F′(τn+θ 1TX˜ i)X˜ iT(cid:105)
+(11− −F( yτ )n 1+ (θ u1T ≤x) α)(cid:104) F′(τn+θ 1TxJ )xN T(θ +1) (1−α) N1 (cid:80)N i=1F′(τn+θ 1TX˜ i)X˜ iT(cid:105)
(cid:104)
F(τn+θ 1Tx) JN(θ1)
(cid:105)
=F′(τ +θTx)xT − y +1(u≤α) 1−y
+n (y+(1 1−y)1(u1 ≤−F α(τ )n )+ (θ 11T −x α)
) N1 (cid:80)N
i=1F′(τn+F θ(τ 1Tn X˜+ iθ )1T X˜x iT)
JN(θ1)
and
∇ L (θ )
θ1 n 1
(cid:104) (cid:105)
=(cid:80)n i=1F +′( (τ Yn i+ +θ (1T 1X −i) YX i)i 1(−
U
i1− ≤F α(τ )nY )+i (θ 11T −X αi )) N1+ (cid:80)1
N
i=( JU
1
NFi (θ′≤
( 1τ
)nα +) θF 1T( X˜τn i1 )+− X˜θY iT1Ti X
.
i)Yan Chen, Jose Blanchet, Krzysztof Dembczynski, Laura Fee Nern, Aaron Flores
Hence
∇2 L (θ )=(cid:80)n ∇2 ℓ(X ,Y ,U ,θ ;τ )
θ1 n 1 i=1 θ1 i (cid:104) i i 1 n (cid:105)
=(cid:80)n F′′(τ +θTX )X XT − Yi +1(U ≤α) 1−Yi
i=1 n 1 i i i (cid:104)1−F(τn+θ 1TXi) i F(τn+θ (cid:105)1TXi)
+(cid:80)n F′(τ +θTX )2X XT −Yi − 1(Ui≤α)(1−Yi)
i=1 n 1 i i i (1 (cid:20)−F(τn+θ 1TXi))2 F(τn+θ 1TXi)2
(cid:21)
+(cid:80)n (Y +(1−Y )1(U ≤α)) (1−α) N1 (cid:80)N i=1F′′(τn+θ1TX˜ i)X˜ iX˜ iT
i=1 i i i JN(θ1)
(cid:20) (cid:21)
+(cid:80)n (Y +(1−Y )1(U ≤α)) (1−α)2{ N1 (cid:80)N i=1F′(τn+θ1TX˜ i)X˜ i}{ N1 (cid:80)N i=1F′(τn+θ1TX˜ i)X˜ iT} ,
i=1 i i i JN(θ1)2
and H(w)=(a−1wT)∇ L (θ )+ 1a−2(cid:80)n Φ (θ +γa−1w), where
n θ1 n ∗ 2 n i=1 i ∗ n
Φ (θ )
i 1
(cid:104) (cid:105)
=F′′(τ n+θ 1TX i)X iX iT − (cid:104)1−F(τnY +i θ 1TXi) +1(U i ≤α) F(τn1 +− θY 1Ti (cid:105)Xi)
+++F (( YY′(
i
iτ ++n+ (( 11θ −−1TX YY iii ))) 112 ((X UUi iiX ≤≤iT α α) )− ) )(cid:104) (cid:104)(1 ( (− 1 1− −F α α(τ ) )n 2NY 1+ {i N(cid:80)θ 11T (cid:80)N iX =1i
N
i=) JF)
N
12 ′ F′ ((−
θ
′τ
1
(n
)
τ1 + nF( +θU ( 1Tτ θi n X 1T≤ ˜+ Xi Jα ˜)θ N) iX1T ˜(
)
(1 XiX θ˜− X 1˜ ii
}
)Y iT) 2{2i) (cid:105)
N1 (cid:80)N i=1F′(τn+θ 1TX˜ i)X˜ iT}(cid:105)
In the following, we want to show that for some matrices V and V˜ , we have
Φ
a−1∇ L (θ )→d N (0,V),
n θ1 n ∗
and for any u and γ ∈[0,1], a−2(cid:80)n Φ (θ +γa−1w)→p V˜ . Note that
n i=1 i ∗ n Φ
(cid:104) (cid:104) (cid:105)
lim +n→ (Y∞ i+E
(
(cid:104)F 1−′(τ Yn i+ )1(θ U∗T iX ≤i)X α)i
)
(cid:16)(− 1−1 α− )F N1(τ (cid:80)nY +i
N
i=θ J1∗T NFX (θ′i
(
∗)
τ
)n+ +θ1 ∗T( XU
˜
ii )X≤
˜
iTα (cid:105)) F(τn1 +− θY ∗Ti Xi)
(cid:17)(cid:105)
=lim n→∞E F′(τ n+θ ∗TX i)X i − 1−F(τnY +i θ ∗TXi) +1(U i ≤α) F(τn1 +− θY ∗Ti Xi)
+(1−α)[P(Y =1)+αP(Y =0)] E˜[F′(τn+θ ∗TX˜)X˜]
= lim (α−1)E(cid:2) F′(τ
+θTXE˜[ )1 X−(1 (cid:3)−α)F(τn+θ ∗TX˜)]
(1) n→∞ n ∗ i i
+(1−α)[P(Y=1)+αP(Y=0)]E[F′(τn+θ ∗TXi)(1−(1−α)F(τn+θ ∗TXi))Xi]
E[(1−(1−α)F(τn+θ ∗TX))2]
=0,Optimal Downsampling for Imbalanced Classification with Generalized Linear Models
where we use Lemma 4 in (1), so E(cid:2) a−1∇ L (θ )(cid:3) →0. Furthermore, by letting θ =θ in the below, we have
n θ1 n ∗ 1 ∗
lim Cov(cid:2) a−1∇ L (θ )(cid:3)
n→∞ n θ1 (cid:104)n ∗
(cid:104) (cid:105)
=lim a−2(cid:80)n Cov F′(τ +θTX )X − Yi +1(U ≤α) 1−Yi
+n (→ Y∞ +n (1−i Y=1
)1(U
≤α)n )(1−α1
)
N1i
(cid:80)N
i=i 1F′(1 τ− nF +( θτ ∗Tn X+
˜
iθ )1T X˜X iTi(cid:105)) i F(τn+θ 1TXi)
=lim i (cid:80)n Coi v(cid:104) F′i (τn+θ1TXi)X (cid:104) − JN(θ Y∗ i) +1(U ≤α) 1−Yi (cid:105)
+n (→ Y∞ +(i 1= −1
Y
)1(Un(1 ≤−F α(τ )n )) () 1−αi
) N1
(cid:80)1
N
i− =F 1( Fτn ′(+ τnθ 1T +X θ∗Ti)
X˜ i)X˜
iT(cid:105)i F(τn+θ 1TXi)
=lim
i
Cov(cid:104)
F√′(i τn+θ1Ti
Xi)X
(cid:104)
−
YiJN(θ∗)
+1(U ≤α) 1−Yi
(cid:105)
n→∞ 1−F(τn) i 1−F(τn+θ 1TXi) i F(τn+θ 1TXi)
+(Y +(1−Y )1(U ≤α))(1−α) N1 (cid:80)N i=1√F′(τn+θ∗TX˜ i)X˜ iT(cid:105)
i i i
=lim E(cid:20) F′(τn+θ1TXi)2 (cid:104) −
YiJN(θ∗) +1− 1F (U(τn ≤)
α) 1−Yi (cid:105)2 X XT(cid:21)
n→∞ 1−F(τn) 1−F(τn+θ 1TXi) i F(τn+θ 1TXi) i i
+E(cid:2)
(Y +(1−Y )1(U
≤α))2(cid:3)(1−α)2E˜[F′(τn+θ∗TX˜ i)X˜ i]E˜[F′(τn+θ∗TX˜ i)X˜ iT]
+2E(cid:104)
Fi ′(√τn+θ1TXii
)Xi
(cid:104)
−i
Yi
E˜ +[1− 1( (1 U−α ≤)F( ατn )+θ∗T 1X˜ −) Y]2 i(1−F (cid:105)(τn))
×
1 (Y− iF +( (τ 1n −) Yi)1(Ui1 ≤− αF ))(τn (+ 1θ −1TX αi )) E˜(cid:20) F′(√τi n+θ1TX˜F )X( ˜τ Tn+ (cid:21)θ (cid:105)1TXi)
E˜[1−(1−α)F(τn+θ∗TX˜)] 1−F(τn)
= lim E(cid:104) F′(τn+θ∗TXi)2 (cid:104) 1 + α (cid:105) X XT(cid:105)
(g) +(1n −→ α)∞
2E˜(cid:20)F′ √(τ 1n
−1 +−
Fθ
(F
∗T
τ( nXτ
˜
)n i))
X˜ i(cid:21)
E˜(cid:20)1− FF
′
√((
τ
1τ
n
−n
+
F+
θ
(θ
∗T
τ∗T nX˜X )i)i)
X˜ iT(cid:21)
(F P( (τ Yn i=+ 1θ )∗T +X αi P) (Yi=0i ))i
(cid:20)
E˜[1−(1−α)F(τn+θ∗TX˜)]2
(cid:20) (cid:21) (cid:21)
+2E F√′(τn+θ∗TXi) 1(Ui≤α)−1 (1−α)E˜ F′(√τn+θ∗TX˜ i)X˜ i XT
1−F(τn) E˜[1−(1−α)F(τn+θ∗TX˜)] 1−F(τn) i
=lim E(cid:104) F′(τn+θ∗TXi)2 (cid:104) 1 + α (cid:105) X XT(cid:105)
+n (→ 1−∞
α)2E˜(cid:20)F′
√1 (−
τ 1n
−F
+
F( θτ (∗Tn τn)
X˜ )i) X˜
i1 (cid:21)− E˜F (cid:20)F(τ
′
√n (τ+
1n
−θ
+
F∗T
θ
(X
∗T
τni X˜)
)i) X˜
iF T( (cid:21)τ (n P(+ Yθ i∗T =X 1)i +) αP(Yi i=0i
))
E˜[1−( (cid:20)1−α)F(τn+θ∗TX˜)]2
(cid:21) (cid:20) (cid:21)
−2 (1−α)2 E F√′(τn+θ∗TXi)X E˜ F′(√τn+θ∗TX˜ i)X˜ iT
E˜[1−(1−α)F(τn+θ∗TX˜)] 1−F(τn) i 1−F(τn)
=E(cid:104) F′(τn+θ∗TXi)2 (cid:104) 1 + α (cid:105) X XT(cid:105)
−(1−1− αF )2(τ En(cid:104)) F′(τn1 +− θF ∗T(τ Xn i+ )Xθ∗TX (cid:105)i E)
(cid:2)
F′F (( ττn+ +θ∗T θTX Xi) )Xi T(cid:3)i
,
α 1−F(τn) i n ∗ i i
where equation (g) uses the fact that E[(Y +(1−Y )1(U ≤α))2]=P(Y =1)+αP(Y =0). Thus by dominated
i i i
convergence theorem and Assumption 3,
lim Cov(cid:2) a−1∇ L (θ )(cid:3)
=E(cid:2) gn→ (θ∞ TX)2h(θn TXθ )1 XXn T(cid:3)∗ −E(cid:2)
g
(θTX)h(θTX)X(cid:3)E(cid:104)
lim
(1−α)2F′(τn+θ∗TX)XT(cid:105)
.
1 ∗ ∗ 1 ∗ ∗ n→∞ α
Furthermore, we check the Lindeberg-Feller CLT condition for the asymptotic normality result. Recall that
∇ ℓ(x,y,u,θ ;τ )
θ1 ∗ (cid:104)n
(cid:105)
=F′(τ +θTx)x − y +1(u≤α) 1−y
+(yn +(11 −y)1(u1 ≤−F α( )τ )n (+ 1θ −1T αx ))
N1 (cid:80)N
i=1F′(τn+F
θ
1( Tτ Xn ˜+ i)θ X˜1T iTx)
.
JN(θ1)Yan Chen, Jose Blanchet, Krzysztof Dembczynski, Laura Fee Nern, Aaron Flores
For ϵ>0, let A denote the event that ∥∇ ℓ(X ,Y ,U ,θ ;τ )∥>a ϵ, we then have
i θ1 i i i ∗ n n
(cid:80)n E(cid:2) ∥∇ ℓ(X ,Y ,U ,θ ;τ )∥21(∥∇ ℓ(X ,Y ,U ,θ ;τ )∥>a ϵ)(cid:3)
=nEi=(cid:2) ∥1
∇
ℓ(Xθ1
,Y
,i
U
,i
θ
;i
τ
)∗ ∥21n
(∥∇
ℓ(Xθ1
,Y
,i
U
,i
θ
;i
τ
)∗ ∥>n
a
ϵ)(cid:3)n
θ1(cid:20)(cid:16) i i i ∗ n θ1 i i i (cid:17)∗
2
n n
(cid:21)
≤ 2CnE 1−Yi 1(U ≤α)− Yi F′(τ +θTX )2∥X ∥21
(1) F(τn+θ∗TXi) i 1−F(τn+θ∗TXi) n ∗ i i Ai
+2CnE(cid:20) (Yi+(1−Yi)1(Ui≤α))2(1−α)2(cid:13)
(cid:13)E˜[F′(τ
+θTX˜)X˜](cid:13) (cid:13)2
1
(cid:21)
JN(θ∗)2 (cid:13) n ∗ (cid:13) Ai
=2nE(cid:20) E(cid:20)(cid:16) F(τn1 +− θY ∗Ti Xi)1(U
i
≤α)− 1−F(τnY +i θ∗TXi)(cid:17)2 F′(τ n+θ ∗TX i)2∥X i∥21 Ai(cid:12) (cid:12)X i(cid:21)(cid:21)
+2CnE(cid:20) E(cid:20) (Yi+(1−Yi J)1 N( (U θi ∗≤ )2α))2(1−α)2(cid:13) (cid:13) (cid:13)E˜[F′(τ n+θ ∗TX˜)X˜](cid:13) (cid:13) (cid:13)2 1 Ai(cid:12) (cid:12)X i(cid:21)(cid:21)
(cid:104)(cid:16) (cid:17) (cid:105)
=2nE 1(Ui≤α) + 1 F′(τ +θTX )2∥X ∥21
F(τn+θ∗TXi) 1−F(τn+θ∗TXi) n ∗ i i Ai
+2Cn(1−α)2∥E˜[F′(τn+θ∗TX˜)X˜]∥2 E(cid:2) (1−F(τ +θTX )+1(U ≤α)F(τ +θTX ))1 (cid:3)
JN(θ∗)2 n ∗ i i n ∗ i Ai
≤2n(1−F(τ ))
n
×E(cid:20)(cid:16) 1(Ui≤α) + 1 (cid:17) F′(τn+θ∗TXi)2 ∥X ∥2
F(τn+θ∗TXi) 1−F(τn+θ∗TXi) 1−F(τn) i
×1(cid:110) (1−Yi)1(Ui≤α)F′(τn+θ∗TXi)∥Xi∥
>
nϵ(cid:111)(cid:21)
F(τn+θ∗TXi)(1−F(τn)) 2
+2Cn(1−F(τ
))(1−α)2(cid:13) (cid:13)
(cid:13)
(cid:13)E˜(cid:20)F′ 1( −τn F+ (τθ n∗T )X˜) X˜(cid:21)(cid:13) (cid:13)
(cid:13)
(cid:13)2
×E(cid:2) (1−Fn (τ +θTXJ )N +(θ 1∗ ()2 U ≤α)F(τ +θTX ))1 (cid:3)
n ∗ i i n ∗ i Ai
( =∗) o(n(1−F(τ )))=o(a2),
n n
where(1)isbecauseE[∥A+B∥2]≤2E[∥A∥2+∥B∥2],and(*)usesdominatedconvergencetheoremandAssumption
3, and C is some absolute constant.
For the rest of the proof, we denote
F′ 1−F(τ ) −h(1)(·)
g (·):= n = ,
1 1−F(τ ) 1−F h
n
F′′ 1−F(τ ) −h(2)
g (·):= n = ,
2 1−F(τ ) 1−F h
n
F′′′ 1−F(τ ) −h(3)
g (·):= n = .
3 1−F(τ ) 1−F h
n
Thus applying the Lindeberg-Feller central limit theorem as Proposition 2.27 from [49] (i.e. Lemma 5), we have
a−1∇ L (θ )→d N (0,V),
n θ1 n ∗
where
V
=E(cid:2)
g
(θTX)2h(θTX)XXT(cid:3) −E(cid:2)
g
(θTX)h(θTX)X(cid:3)E(cid:104)
lim
(1−α)2F′(τn+θ∗TX)XT(cid:105)
1 ∗ ∗ 1 ∗ ∗ n→∞ α
=E(cid:2)
g
(θTX)2h(θTX)XXT(cid:3)
1 ∗ ∗
−(cid:16) lim (1−α)2(1−F(τn))(cid:17) E[h(θTX)g (θTX)X]E(cid:2) h(θTX)g (θTX)XT(cid:3)
n→∞ α ∗ 1 ∗ ∗ 1 ∗
Now we want to show that for any u and any γ ∈[0,1], for some matrix V˜ we have
Φ
a−2(cid:80)n Φ (θ +γa−1w)→p V˜ .
n i=1 i ∗ n Φ
Note that
lim
a−2(cid:80)n
Φ (θ )
= ++lim
n
11n n (cid:80) (cid:80)→ →∞ n
i
n∞ =1n1n F ((cid:80) Y′ i(
1
+τn i −n= (1+ Fi 1 −= θ (F τ1 Y∗T
n
i′ X′ ))1( 1i−i τ ) (n 2 UF+ X i∗ ( ≤θ τ∗ iT n αX)X ))ii T) (cid:104)X (cid:104) (1−i −X
(
α1i )T
−
E˜F(cid:104) [F−
(τ
′′n1 (Y +− τi nθF +∗T( θXτ ∗TnY i+ X)i ˜)θ
2
i∗ )T X− ˜X ii X˜1)
F
i( T+ U
(
]τi (cid:105)n≤1 +( αU θ) ∗T(i 1 X−≤ iY )2iα )) (cid:105)F(τn1 +− θY ∗Ti Xi)(cid:105)
n i=1 1−F(τn) JN(θ∗)
+1 (cid:80)n (1−α)2E˜[F′(τn+θ ∗TX˜)X˜]E˜[F′(τn+θ ∗TX˜)X˜T],
n i=1 JN(θ∗)2Optimal Downsampling for Imbalanced Classification with Generalized Linear Models
and note that by dominated convergence theorem and Assumption 3,
=l (i αm −n→ 1∞ )EE (cid:2) h(cid:104) (F θ′ T′ 1( −τ Xn F+ )( gθ τ∗T n ()X θi T) XX )iX XiT X(cid:16) T− (cid:3) ,1−F(τnY +i θ ∗TXi) +1(U i ≤α) F(τn1 +− θY ∗Ti Xi)(cid:17)(cid:105)
∗ 2 ∗
=l −im En (cid:2)→ g∞ (θE TX(cid:104) F )′ 2( 1 hτ −n (+ F θTθ (τ∗T XnX ) )i X)2 X Xi TX (cid:3)iT
,
(cid:104) − (1−F(τnY +i θ ∗TXi))2 − 1 F(U (τi n≤ +α θ) ∗T(1 X− iY )2i)(cid:105)(cid:105)
1 ∗ ∗
(cid:20) (cid:20)
lim n→∞E (Yi+(1 1− −Y Fi) (1 τn(U )i≤α)) (1− E˜α [1)E −˜[ (F 1−′′( ατ )n F+ (θ τ∗T n+X˜ θi ∗T)X˜ X˜i )X˜ ]iT]
(cid:21)(cid:21)
+(1−α)2E˜[F′(τn+θ ∗TX˜)X˜]E˜[F′(τn+θ ∗TX˜)X˜T]
E˜[1−(1−α)F(τn+θ ∗TX˜)]2
=(1−α)E[g (θTX)h(θTX)XXT]
2 ∗ ∗
+E[h(θ ∗TX)g 1(θ ∗TX)X]E(cid:104) lim
n→∞
(1−α)2F′ α(τn+θ ∗TX)XT(cid:105)
=(1−α)E[g (θTX)h(θTX)XXT]
2 ∗ ∗
+(cid:16) lim (1−α)2(1−F(τn))(cid:17) E[h(θTX)g (θTX)X]E(cid:2) h(θTX)g (θTX)XT(cid:3)
n→∞ α ∗ 1 ∗ ∗ 1 ∗
Thus
a−2(cid:80)n
Φ (θ )
n i=1 i ∗
→p −E(cid:2)
g
(θTX)2h(θTX)XXT(cid:3)
1 ∗ ∗
+(cid:16) lim (1−α)2(1−F(τn))(cid:17) E[h(θTX)g (θTX)X]E(cid:2) h(θTX)g (θTX)XT(cid:3) .
n→∞ α ∗ 1 ∗ ∗ 1 ∗
For the last step of the proof, we want to show that indeed
V˜ =−E(cid:2) g (θTX)2h(θTX)XXT(cid:3)
Φ 1 ∗ ∗
+(cid:16) lim (1−α)2(1−F(τn))(cid:17) E[h(θTX)g (θTX)X]E(cid:2) h(θTX)g (θTX)XT(cid:3) .
n→∞ α ∗ 1 ∗ ∗ 1 ∗
Note that
∇ Φ (θ )
θ1 i 1
(cid:104) (cid:105)
=F(3)(τ n+θ 1TX i)X iTX iX iT − 1−F(τnY +i θ 1TXi(cid:104)) +1(U i ≤α) F(τn1 +− θY 1Ti Xi)
(cid:105)
+F′′(τ n+θ 1TX i)F′(τ n+θ 1TX i)X iTX iX iT −
(cid:104)
(1−F(τnY +i
θ 1TXi))2
− 1 F(U (τi n≤ +α θ) 1T(1 X− iY )2i)
(cid:105)
+2F′(τ n+θ 1TX i)F′′(τ n+θ (cid:104)1TX i)X iTX iX iT − (1−F(τnY +i
θ 1TXi))2
− 1 F(U (τi n≤ +α θ) 1T(1 X− iY )2i)
+F′(τ +θTX )2XTX XT −2Y (1−F(τ +θTX ))−3F′(τ +θTX )
n 1 i i i i i n 1 i n 1 i
(cid:105)
+2·1(U ≤α)(1−Y )F(τ +θTX )−3F′(τ +θTX )
i i n 1 i n 1 i
(cid:20) (cid:21)
+(Y +(1−Y )1(U ≤α))
(1−α)E˜ N[F(3)(τn+θ 1TX˜ i)X˜ iTX˜ iX˜ iT]
i i i E˜ N[1−(1−α)F(τn+θ 1TX˜ i)]
+(Y i+(1−Y i)1(U
i
≤α))(cid:104) 2(1−α)2E˜[F(2)(τn E˜+ [1θ −1T (X 1˜ −)X α˜T )FX˜ (τ]E n˜ +[F θ′ 1T(τ X˜n )+ ]2θ 1TX˜)X˜T](cid:105) ,
hence
≤ ((cid:12) (cid:12) da )− n a2 − n(cid:80) 2(cid:13) (cid:13)n i a= − n1 1(cid:13) (cid:13) wΦ (cid:13) (cid:13)i( (cid:80)θ ∗ n i=+ 1γ (cid:12) (cid:12)∆a− n i(1 θw ∗) +(cid:13) (cid:13)− γ˜aa − n− n 12 w(cid:80) )(cid:12) (cid:12)n i ∥= X1 i∥ ∥Φ 3i +(θ a∗) − n∥ 2(cid:12) (cid:12) (cid:13) (cid:13)a− n1w(cid:13) (cid:13)(cid:80)n i=1(cid:12) (cid:12) (cid:12)∆˜ i(θ ∗+γ˜a− n1w)(cid:12) (cid:12)
(cid:12)
= ∥a− n1w∥(cid:80)n |∆i(θ∗+γ˜a− n1w)| ∥X ∥3+ ∥a− n1w∥(cid:80)n |∆˜ i(θ∗+γ˜a− n1w)| ,
n i=1 1−F(θn) i n i=1 1−F(θn)
where γ˜ is some constant such that γ˜ ∈ (0,1), inequality (d) uses mean value theorem and Cauchy-SchwarzYan Chen, Jose Blanchet, Krzysztof Dembczynski, Laura Fee Nern, Aaron Flores
inequality and the fact that γ ∈(0,1), and
∆ (θ +γ˜a−1w)
i ∗ n
(cid:104) (cid:105)
=F(3)(τ n+θ 1TX i) − 1−F(τnY +i θ 1TXi(cid:104)) +1(U i ≤α) F(τn1 +− θY 1Ti Xi)
(cid:105)
+F′′(τ n+θ 1TX i)F′(τ n+θ 1TX i) −
(cid:104)
(1−F(τnY +i
θ 1TXi))2
− 1 F(U (τi n≤ +α θ) 1T(1 X− iY )2i)
(cid:105)
+2F′(τ n+θ 1TX i) (cid:104)F′′(τ n+θ 1TX i) − (1−F(τnY +i
θ 1TXi))2
− 1 F(U (τi n≤ +α θ) 1T(1 X− iY )2i)
+F′(τ +θTX )2 −2Y (1−F(τ +θTX ))−3F′(τ +θTX )
n 1 i i n 1 i n 1 i
(cid:105)
+2·1(U ≤α)(1−Y )F(τ +θTX )−3F′(τ +θTX ) ,
i i n 1 i n 1 i
∆˜ (θ +γ˜a−1w)
i ∗ n
(cid:20) (cid:21)
=(Y +(1−Y )1(U ≤α))
(1−α)E˜ N[F(3)(τn+θ 1TX˜ i)X˜ iTX˜ iX˜ iT]
i i i E˜ N[1−(1−α)F(τn+θ 1TX˜ i)]
+(Y i+(1−Y i)1(U
i
≤α))(cid:104) 2(1−α)2E˜[F(2)(τn E˜+ [1θ −1T (X 1˜ −)X α˜T )FX˜ (τ]E n˜ +[F θ′ 1T(τ X˜n )+ ]2θ 1TX˜)X˜T](cid:105) .
Again, similar to the previous argument, by Assumption 3 and dominated convergence theorem, the terms
lim E(cid:104) |∆i(θ∗+γ˜a− n1w)|∥X ∥3(cid:105) ,lim E(cid:104) |∆˜ i(θ∗+γ˜a− n1w)|(cid:105) are bounded, so
n→∞ 1−F(θn) i n→∞ 1−F(θn)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12)a− n2(cid:88)n (cid:13) (cid:13)Φ i(θ ∗+γa− n1w)(cid:13) (cid:13)−a− n2(cid:88)n ∥Φ i(θ ∗)∥(cid:12) (cid:12) (cid:12)=o P(1).
(cid:12) (cid:12)
i=1 i=1
Combining with the previous arguments, we have proved that
V˜ =−E(cid:2) g (θTX)2h(θTX)XXT(cid:3)
Φ 1 ∗ ∗
+(cid:16) lim (1−α)2(1−F(τn))(cid:17) E[h(θTX)g (θTX)X]E(cid:2) h(θTX)g (θTX)XT(cid:3) .
n→∞ α ∗ 1 ∗ ∗ 1 ∗
Recall that a (θˆ −θ ) is the maximizer of
n ∗ ∗
1
H(w)=(a−1wT)∇ L (θ )+ a−2wT∇2 L (θ +γ(θˆ−θ ))w,
n θ1 n ∗ 2 n θ1 n ∗ ∗
which is equivalently the minimizer of −1a−2wT∇2 L (θ +γ(θˆ−θ ))u−(a−1wT)∇ L (θ ). Then by the
2 n θ1 n ∗ ∗ n θ1 n ∗
Basic Corollary of [21] (or Lemma 6), we have
a (θˆ−θ )=−V˜−1×a−1∇ L (θ )+o (1)=V−1×a−1∇ L (θ )+o (1),
n ∗ Φ n θ1 n ∗ P Φ n θ1 n ∗ P
where
V
=E(cid:2)
g
(θTX)2h(θTX)XXT(cid:3)
Φ 1 ∗ ∗
−(cid:16) lim (1−α)2(1−F(τn))(cid:17) E[h(θTX)g (θTX)X]E(cid:2) h(θTX)g (θTX)XT(cid:3) .
n→∞ α ∗ 1 ∗ ∗ 1 ∗
By the given condition that lim
(1−α)2(1−F(τn))
=c, we have
n→∞ α
V=V =E(cid:2) g (θTX)2h(θTX)XXT(cid:3) −cE[h(θTX)g (θTX)X]E(cid:2) h(θTX)g (θTX)XT(cid:3)
Φ 1 ∗ ∗ ∗ 1 ∗ ∗ 1 ∗
=E(cid:104) h(1)(θ ∗TX)2 XXT(cid:105) −cE(cid:2) h(1)(θTX)X(cid:3)E(cid:2) h(1)(θTX)XT(cid:3)
h(θTX) ∗ ∗
∗
Thus we have (cid:112) n(1−F(τ ))(θˆ −θ )→d N(cid:0) 0,V−1VV−1(cid:1) =N(cid:0) 0,V−1(cid:1) .
n ∗ ∗ Φ Φ
C.2 Proof for Theorem 2
Proof of Theorem 2. By definition, the (scaled) maximum likelihood estimator is now defined as
r(τ )θˆ =argmax 1 (cid:80)N Y˜ logF¯(τ +θTX˜ )+(1−Y˜)logαF(τ +θTX˜ )
n ∗ θ1∈Θ N i=1 −i
log{1
(cid:80)n
N
[1 1−i (1−α)F(τi
+θTX˜
)n
]},
1 i
N i=1 n 1 iOptimal Downsampling for Imbalanced Classification with Generalized Linear Models
then following similar steps as in the proof of Theorem 1, we use the same definition of L (θ ) and a , and we
n 1 n
can get
a−1∇ L (r(τ )θ )→d N(0,V),
n θ1 n n ∗
where
V = −lim
lin
m→∞E(cid:104) (1F −′( ατ )n 21+
E−
(cid:20)r F( Fτ
(
′n
(τ
τ) nθ
n)
1∗T
+
−X
r
F(i
τ
()
n
τ2
n)θ
)(cid:104)
∗T1 X− i)F X(τ in (cid:21)+ E[r
F1
(τ ′(n τ) nθ +∗T rX (i τ)
n)+
θ
∗TF X( iτ )n X+ iTr
](α
.τn)θ
∗TXi)(cid:105) X iX iT(cid:105)
n→∞ E˜[1−(1−α)F(τn+r(τn)θ ∗TX˜ i)]
Note that by Assumption 4 for any θ ∈Θ and x∈X, we have
1
1−F(r(τ )(θ′x)+τ )
g(θ′x)= n n ,
1−F(τ )
n
thus
F′(τn+r(τn)θ ∗Tx)
=
F′(τn 1−+ Fr( (τ τn n) )θ∗Tx) =−g′(θ ∗Tx).
So by dominated convergence theorem, we have
1−F(τn+r(τn)θ ∗Tx) 1−F(τn+r(τn)θ∗Tx) g(θ ∗Tx)
1−F(τn)
(cid:20) g(1)(θTX)2 (cid:21) (1−α)2(1−F(τ )) (cid:104) (cid:105) (cid:104) (cid:105)
V=E ∗ XXT − lim n E g(1)(θTX)X E g(1)(θTX)XT .
g(θ ∗TX) n→∞ α ∗ ∗
Similarly, we can also get
a−2(cid:80)n
Φ (r(τ )(θ
+γa−1w))→p
−V ,
n i=1 i n ∗ n Φ
where
(cid:20) g(1)(θTX)2 (cid:21) (1−α)2(1−F(τ )) (cid:104) (cid:105) (cid:104) (cid:105)
V =E ∗ XXT − lim n E g(1)(θTX)X E g(1)(θTX)XT .
Φ g(θTX) n→∞ α ∗ ∗
∗
Then the rest of the proof follows by similar steps of checking regularity conditions, etc. as in Theorem 1.
C.3 Technical Lemmas
Lemma 5 (Proposition 2.27 from [49]). For each n let Y ,...,Y be independent random vectors with finite
n,1 n,kn
variances such that for every ϵ > 0, (cid:80)kn E[∥Y ∥2]1{∥Y ∥ > ϵ} → 0, and (cid:80)kn Y → Σ, then the sequence
i=1 n,i n,i i=1 n,i
(cid:80)kn (Y −E[Y ]) converges in distribution to N(0,Σ).
i=1 n,i n,i
Lemma 6 (Basic Corollary of [21]). Let A (s)= 1s′Vs+U′s+C +r (s), where V is symmetric and postive
n 2 n n n
definite, U is stochastically bounded, C is arbitrary, and r (s) goes to zero in probability for every s. Then
n n n
α = argminA is o (1) away from −V−1U as the argmin of 1s′Vs + U′s + C . If also U →d U then
n n p n 2 n n n
α →d −V−1U.
n
Lemma 7 (Theorem 5.7 of [49]). Let M be random functions and let M be a fixed function of θ such that for
n
∀ϵ>0,
p
sup|M (θ)−M(θ)|→0, and sup M(θ)<M(θ ).
n ∗
θ∈Θ θ:d(θ,θ∗)≥ϵ
Then any sequence of θˆ
n
with M n(θˆ n)≥M n(θ ∗)−oP(1) converges in probability to θ ∗.
D Proofs for Efficiency with a Budget Constraint
Proof of Theorem 3. We use g (·) to denote −h(1)(·). The objective function is equal to
1 h(·)
lim p1+α(1−p1)
n→∞ tr{V}
=lim p1+α(1−p1) ,
n→∞ tr{E[g1(θ ∗TX)2h(θ ∗TX)XXT]−cE[g1(θ ∗TX)h(θ ∗TX)X]E[g1(θ ∗TX)h(θ ∗TX)X]}
where lim
(1−α)2(1−F(τn))
=c. Thus replacing c with
(1−α)2(1−F(τn))
in the above equation, the objective
n→∞ α α
becomes
J (α):= [p1+α(1−p1)] .
n tr(cid:26) E[g1(θ∗TX)2h(θ∗TX)XXT]−(1−α)2(1 α−F(τn))E[g1(θ∗TX)h(θ∗TX)X]E[g1(θ∗TX)h(θ∗TX)X](cid:27)Yan Chen, Jose Blanchet, Krzysztof Dembczynski, Laura Fee Nern, Aaron Flores
So
logJ (α) =log(p +α(1−p ))
n 1 1
−log(cid:2) tr(cid:8)E(cid:2)
g
(θTX)2h(θTX)XXT(cid:3)
1 ∗ ∗
−(1−α)2(1−F(τn))E(cid:2)
g
(θTX)h(θTX)X(cid:3)E(cid:2)
g
(θTX)h(θTX)X(cid:3)(cid:9)(cid:3)
,
α 1 ∗ ∗ 1 ∗ ∗
taking derivative with respect to α, we have
∂logJn(α)
∂α
= (1−p1)
p1+α(1−p1)
−
(1/α2−1)(1−F(τn))tr{E[g1(θ ∗TX)h(θ ∗TX)X]E[g1(θ ∗TX)h(θ ∗TX)X]}
.
tr(cid:110) E[g1(θ ∗TX)2h(θ ∗TX)XXT]−(1−α)2(1 α−F(τn))E[g1(θ ∗TX)h(θ ∗TX)X]E[g1(θ ∗TX)h(θ ∗TX)X](cid:111)
So
∂logJn(α) =0
∂α
⇐⇒ E[g1(θ ∗TX)2h(θ ∗TX)XXT] =(cid:104) (1−α)2 +(cid:16) p1 +1(cid:17) 1−α2(cid:105) (1−F(τ )),
E[g1(θ ∗TX)h(θ ∗TX)X]E[g1(θ ∗TX)h(θ ∗TX)X] α α(1−p1) α n
where we use abuse of notation with the equality sign holds above when n → ∞. Then we have α∗ = o(1),
because if α is bounded away from zero, the right hand side of the above equation is o(1) while the left hand
side is O(1). Let β =lim 1−F(τn), since α→0 as n→∞, and note that p1 →0, then the right hand side
n→∞ α 1−p1
converges to 2β, so we have
1
tr(cid:8)E(cid:2)
g
(θTX)2h(θTX)XXT(cid:3)(cid:9)
β = 1 ∗ ∗ ,
2tr{E[g (θTX)h(θTX)X]E[g (θTX)h(θTX)X]}
1 ∗ ∗ 1 ∗ ∗
indicating that
2(1−F(τ
))tr(cid:8)E(cid:2)
g
(θTX)h(θTX)X(cid:3)E(cid:2)
g
(θTX)h(θTX)X(cid:3)(cid:9)
α∗ = n 1 ∗ ∗ 1 ∗ ∗ .
tr{E[g (θTX)2h(θTX)XXT]}
1 ∗ ∗
E Proofs for Application to Logistic Regression
Proof of Proposition 3. Note that
h(θTx)= lim 1−F(τ n+θ 1Tx) =e−θ1Tx,
1 n→∞ 1−F(τ n)
g 1(θ 1Tx)=lim
n→∞
1F −′ F(τ (n τn+ +θ θ1T 1Tx x)
)
=lim n→∞F(τ n+θ 1Tx)=1,andalsowehaveg 2(θ 1Tx)=lim
n→∞
1F −′ F′( (τ τn n+ +θ θ1T 1Tx x)
)
=
−1, g 3(θ 1Tx)=lim
n→∞
1F −(3 F)( (τ τn n+ +θ θ1T 1Tx x)
)
=1, so by Theorem 1 we have
(cid:114) n (cid:16) θˆ −θ (cid:17) →d N(cid:0) 0,V−1(cid:1) , where
1+eτn ∗ ∗
V
=E(cid:2)
g
(θTX)2h(θTX)XXT(cid:3) −cE(cid:2)
g
(θTX)h(θTX)X(cid:3)E(cid:2)
g
(θTX)h(θTX)X(cid:3)
1 ∗ ∗ 1 ∗ ∗ 1 ∗ ∗
(cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)
=E e−θ∗TXXXT −cE e−θ∗TXX E e−θ∗TXX
Thus √ ne−τn(cid:16) θˆ ∗−θ ∗(cid:17) →d N (cid:0) 0,V−1(cid:1) ,
then by dominated convergence theorem and Slutsky’s theorem, we have
(cid:115)
(cid:20) 1 (cid:21)(cid:16) (cid:17) (cid:16) (cid:104) (cid:105) (cid:17)
nE
1+eτn+θ ∗TX
θˆ ∗−θ
∗
→d N 0,E
X
e−θ ∗TX V−1 ,
which gives
(cid:112) nP(Y =1)(cid:16) θˆ ∗−θ ∗(cid:17) →d N (cid:16) 0,E X(cid:104) e−θ ∗TX(cid:105) V−1(cid:17) .Optimal Downsampling for Imbalanced Classification with Generalized Linear Models
Proof of Proposition 4. First note that Assumption 2 holds. We use g (·),g (·),g (·) to denote
1 2 3
−h(1)(·)/h(·),−h(2)(·)/h(·),−h(3)(·)/h(·). By definition, for
F(τ n+θ 1Tx)=
1+eτn e+ τnθ +1T θx
1Tx,h(θ 1Tx)= nl →im
∞1− 1F −(τ
Fn
(+
τ
nθ )1Tx)
= nl →im
∞1+1 e+ τne +τn
θ 1Tx
=e−θ 1Tx,
F′(τ +θTx)
g (θTx)= lim n 1 =1,
1 1 n→∞1−F(τ n+θ 1Tx)
F′′(τ +θTx) F′′(τ +θTx)
g (θTx)= lim n 1 = lim n 1 =−1,
2 1 n→∞1−F(τ n+θ 1Tx) n→∞1−F(τ n+θ 1Tx)
g 3(θ 1Tx)=lim
n→∞
1F −(3 F)( (τ τn n+ +θ θ1T 1Tx x)
)
=0. Thus the conditions in Assumption 3 are satisfied. Further note that the
conditions of Theorem 1 also hold. Then by Theorem 3,
(cid:110) (cid:111)
2(1+eτn)−1tr E[e−θ ∗TXX]E[e−θ ∗TXX]
α∗ =
tr{E[e−θ ∗TXXXT]}