SceneCraft: Layout-Guided 3D Scene Generation
XiuyuYang∗1 YunzeMan∗2 Jun-KunChen2 Yu-XiongWang2
1ShanghaiJiaoTongUniversity 2UniversityofIllinoisUrbana-Champaign
https://orangesodahub.github.io/SceneCraft
Abstract
Thecreationofcomplex3Dscenestailoredtouserspecificationshasbeenatedious
andchallengingtaskwithtraditional3Dmodelingtools. Althoughsomepioneer-
ing methods have achieved automatic text-to-3D generation, they are generally
limited to small-scale scenes with restricted control over the shape and texture.
WeintroduceSceneCraft,anovelmethodforgeneratingdetailedindoorscenes
thatadheretotextualdescriptionsandspatiallayoutpreferencesprovidedbyusers.
Centraltoourmethodisarendering-basedtechnique,whichconverts3Dsemantic
layoutsintomulti-view2Dproxymaps. Furthermore,wedesignasemanticand
depthconditioneddiffusionmodeltogeneratemulti-viewimages,whichareused
tolearnaneuralradiancefield(NeRF)asthefinalscenerepresentation. Without
theconstraints of panoramaimage generation, wesurpass previous methodsin
supportingcomplicatedindoorspacegenerationbeyondasingleroom, evenas
complicatedasawholemulti-bedroomapartmentwithirregularshapesandlay-
outs. Throughexperimentalanalysis,wedemonstratethatourmethodsignificantly
outperformsexistingapproachesincomplexindoorscenegenerationwithdiverse
textures,consistentgeometry,andrealisticvisualquality.
1 Introduction
The generation of diverse and complex 3D scenes plays a critical role in enhancing virtual and
augmentedreality(VR/AR)experiences,videogamedevelopment,andtheadvancementofhuman-
centricembodiedAI.However,manuallycreatingthesecomplex3Dscenesisatediousprocedurethat
requiresextensiveknowledgeandproficiencyin3Dmodelingtools[13,14]. Therecentsuccessof
2Dgenerativemodels[23,50,55]fuelsthedevelopmentofalineoftext-to-3Dwork[31,46,63,66].
Althoughthesemethodshaveachievedimpressiveobjectgenerationperformance,scalingfromobject-
leveltoscene-levelgenerationpresentssignificantchallenges. Itinvolvesmanagingaconsiderably
largerspacewithcomplicatedsemanticswhileensuring3Dconsistency(intermsofshape,texture,
occlusion,etc.) acrossvariouscameraperspectives.
Recentadvancesinscene-level3Dgeneration[17,24,37,60,74]haveopenednewpathwaysfor
creatinglarger-scalevirtualenvironments. Mostworkleveragesimageinpainting[17,24,74]or
multi-viewdiffusionmethods[37,60]tooptimizeatext-guided3Dscene. Whilegeneratinglocally
convincingtexturedmeshes,thesemethodssharetwocommondrawbacks: (1)Focusingonlocal
coherence,theyoftenstruggletoaccuratelydepictgeometricallyconsistentroomswithplausible
layoutsandrichsemanticdetails. (2)Conditionedonlyontextualprompts,thesemethodsfallshortin
termsofofferingprecisecontrolovertheentirescene’scompositionandarrangement.Althoughsome
concurrentresearch [16,45,53]hasexploredthegenerationofanindoorenvironmentconditioned
onuser-defined3Dlayouts,itisrestrictedtocreatingsmall-scalecompositionsinvolvingmultiple
*Equalcontribution.
38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024).
4202
tcO
11
]VC.sc[
1v94090.0142:viXraFigure1:Ournovelmethodgeneratescomplexanddetailedindoorscenesfrom3Dspatiallayoutsandtextual
descriptions.Givenuser-specifiedlayoutsrepresentedasa“BoundingBoxScene(BBS),”ourmethodrenders
batchesof2Dlayoutsandcoarsedepthmapsandthentransformsthemintohigh-quality3Dscenes.
objects[12,45],orlackstheabilitytogeneratemultipleroomswithcomplexlayouts,shapes,and
freecameraviewpoints[16,53]duetotheuseofpanoramicrepresentation.
Inthispaper, weintroduceSceneCraft, anovelmethoddesignedtogeneratehigh-qualityindoor
scenesconditionedonuser-specifiedfree-formlayouts. Ahigh-levelillustrationofourworkisshown
inFigure1. Ourmethodfeaturestwokeyinnovativedesigns:
User-FriendlySemantic-AwareLayoutControl. Centraltoourapproachistheutilizationof
3Dboundingboxestoguidethelayoutsofthetargetspace,namelya“bounding-boxscene(BBS),”
whichallowsuserstodesigncomplexandfree-formroomarrangementswithsimpleboundingboxes.
Withthislayoutformat,userscaneasilydefineboththespatialarrangementandtheplacementof
objectswithinaroom,asconstructingabuildingintheMinecraftgame. AndSceneCraftleverages
this preliminary design to generate a detailed and realistic scene. We surpass previous methods
insupportingcomplicatedindoorlayoutsbeyondasingleroom, evenascomplicatedasawhole
three-storyhousewithmultiplelayersandirregularrooms.
High-QualityComplexSceneGenerationwitha2DDiffusionModel. Ourframeworkexcelsin
creating3Dscenesbyleveragingtheadvancedgenerationcapabilitiesofourpretrained2Ddiffusion
model,SceneCraft2D.SceneCraft2Dtakesthe“bounding-boximages(BBI)”renderedfromBBSas
aconditionthroughControlNets[75]togeneratehigh-fidelityviewsoftheroomthatfollowthegiven
simplepromptlike“Thisisoneviewofa[styledescription]room.” Byobtaininghigh-qualitymulti-
viewimagesthroughSceneCraft2D,wesuccessfullydistillahigh-resolution3Drepresentation[56]
ofthegeneratedindoorscene.
Trainedwithmulti-viewindoorscenedatasets[49,71],ourworkachievesstate-of-the-art3Dindoor
scenegenerationperformance,bothquantitativelyandqualitatively. Wepresentthefirsteffective
framework to generate complex text- and layout-guided 3D-consistent scenes with free camera
trajectoriesanddiversesemantics. Insummary,ourtechnicalcontributionsarethreefold:
• Weproposeanovellayout-guided3Dscenegenerationframeworktocreatecomplicated
indoorscenesadheringtouserspecifications,beingthefirsttooperateonfreemulti-view
trajectoriesandfreefromtheconstraintsofusingpanoramas.
• Weintroducethe“bounding-boxscene”asauser-friendlyformattoscratchadesiredroom
aseasyasbuildinghomesintheMinecraftgame,whichprovidesaccurategeometrycontrol.
• Wedesignahigh-quality2Ddiffusionmodel,SceneCraft2D,togeneratehigh-fidelityand
high-qualityroomsfollowingtherendered“bounding-boximage”fromthe“bounding-box
scene,”andtosupportthegenerationofvariousstylesviatextconditioning.
Withallthesecontributions,ourSceneCraftachieveshigh-qualitygenerationofvariousfine-grained
andcomplicatedindoorscenesthathavenotbeensupportedbypreviouswork.
2 RelatedWork
LearnableSceneRepresentation. Traditionalscenerepresentations[1,11,22,28,40,44,54,64]
directly model the 3D geometry information of the scene and thus suffer from limited flexibility
2orlowrenderingquality. Theneuralradiancefield(NeRF)[41]pioneersaneuralnetwork-based
scenerepresentation,providingtheabilitytoreconstructacompleteandprecise3Dscenefromonly
multi-viewimagesandcorrespondingcameraparameters. Follow-upvariantsofNeRF[2,6,7,20,
56,62,65,67,69,72,73]aimeithertoimprovetheoriginalframeworkindifferentaspects,e.g.,
renderingqualityandtrainingefficiency,ortosupportadditionaltasks,e.g.,relightingabilityand
editingability. Recently,3DGaussianSplatting[27]outperformstheNeRF-familyrepresentation
withhighrenderingqualityandefficiency. Inourwork,weusealearnablescenerepresentationas
thebackbonetomodeltheoutput. Asanyrepresentationcanbeusedinourframework,wechoose
Nerfacto[56]foritshigh-qualityrenderingofcomplicatedlarge-scalescenes.
Diffusion-GuidedText-to-3DGeneration. Therecentsuccessful2Ddiffusion-basedgenerative
models [3, 4, 23, 50, 51, 55] have inspired a series of innovative text-to-3D methods [10, 31, 43,
46,59,66,74]todistillpowerful2Dpretrainedmodelsfor3Dcontentcreation. DreamFusion[46]
proposes the score distillation sampling (SDS) module to optimize scene representations, e.g.,
NeRF [41] or Gaussian Splatting [27], of objects by denoising their rendered views. Building
on top of DreamFusion, SJC [63], Magic3D [31], and ProlificDreamer [66] alleviate the over-
saturationproblemandimprovethegenerationquality. Despiteimpressiveresults,thesemethodsare
restrictedingeneratingsmall-scaleobjectswithoutcomplexsemanticcomposition. Morerecently,
Text2Room[24],SceneScape[17],andText2NeRF[74]proposetoextendobjectgenerationtoscene
generationwithoff-the-shelftext-imageinpaintingmodels[39,50],wheretheyiterativelyinpaint
unseenpartsofthescenefromnovelcameraperspectives. Anotherwork[37]proposesprogressively
distillingatext-conditionedindoorpanoramagenerationmodel[60]usingdifferentgroupsofcamera
viewstooptimizeascenerepresentation. However,allofthesemethodslacksemanticcontrolover
thegenerationoutputotherthanasimpletextprompt. Hence,theycannotbeusedinthecreation
of3Dscenemodelswhereuserswanttospecifythestructureandlayoutsoftheenvironment. In
comparison,ourworklearnstogenerate3Dscenesthatadheretouser-specifiedroomlayoutsand
textualdescriptions,allowingprecisecontrolovertheenvironment.
SceneGenerationwithSemanticGuidance. Arecentlineofworkhasstudied2Dgeneration
withsemanticguidanceforbettercontrollability[9,15,18,30,50,70]. Attemptingtoextendimage
generation to 3D creation, prior work has studied single image to 3D object reconstruction [33,
34,36,38,47,57,68]andsingleimagetovideoreconstruction[5,61]. Thesemethodsfacegreat
challengesin3Dconsistency,duetothelackoflargescene-leveldatasetsfortrainingandtheuse
of the auto-regressive generation paradigm. Meanwhile, Set-the-scene [12], CompoNeRF [32],
and Compo3D [45] learn to generate object compositions from semantic layouts with the SDS
method[46]. DiffuScene[58]andGraphDreamer[19]utilizescenegraphstogetherwithtextual
descriptionsasconditionstogeneratecompositional3Dscenes.However,thesemethodsarerestricted
togeneratingsmall-scalescenescomposedofonlyseveralobjects. Theyalsoneglectrepresentations
of walls, doors, ceilings, and ground, which are essential in defining indoor scenes but difficult
to control in generation. Close to our work are three concurrent methods, ControlRoom3D [53],
Ctrl-Room[16],andUrbanArchitect[35]. Thefirsttwomethodsgenerate3Droommeshesfrom
user-definedorestimatedlayoutswithmulti-viewdiffusionfollowedbyamonoculardepthestimation
process. Whileachievingoutstandinggenerationperformance,theyrelyonpanoramaimages[60]as
theirpreliminaryresults,whichnotonlysimplifiesthescenegenerationproblem,butalsolimitsthe
complexityoftheirroomlayoutsanddiversityoftheircameraviewpoints. Ourmethod,bylearninga
3D-consistentmulti-viewgeneratorandascenerendererwithoutviewpointconstraints,isableto
generatemorecomplexandconsistentsceneswithdiversecameratrajectories. UrbanArchitect[35]
focusesonstreet-viewscenegenerationwithsemantic-awarelayoutcontrols. However,itallows
forgreatergeometricapproximationduetosimplerconditions: fewerobjectcategories,sparserand
non-overlapping object placement, and more predictable camera trajectories. In contrast, indoor
scenesfeaturedenseobjectsthatoverlapwithmorefine-grainedcategories,whicharealleffectively
addressedinourmethod.
3 SceneCraft: Methodology
OurSceneCraftisanovelmethodfortext-andlayout-guidedscenegeneration. Asillustratedin
Figure 2, the input to SceneCraft consists of (1) a prompt as a coarse description of the target
scene’sstyleandcontent,(2)a“bounding-boxscene”(BBS)servingasthelayoutguidanceofthe
3Figure2:SceneCraftisanovelframeworkforlayout-guidedscenegeneration,whichallowsuserstoprovide
thelayoutasabounding-boxscene(BBS,Sec.3.1),auser-friendlylayoutformatthatguidesthegeneration.
Ourframeworkcontainstwostages: (a)pretrainingofa2Ddiffusionmodel,SceneCraft2D,tosolvethe2D
versionofthelayout-guidedscenegenerationtask(Sec.3.2),and(b)distillationoftheSceneCraft2Dtolearna
scenerepresentationofthegeneratedscene(Sec.3.3).
targetscene,and(3)acameratrajectorydefinedinthespaceofBBS.SceneCraftrenderstheBBS
in the camera trajectory to construct “bounding-box images” (BBI) as the layout condition for a
pretrained2Ddiffusionmodel“SceneCraft2D”togeneratehigh-quality2Dimagesofthescene. With
thehigh-qualityimagesgeneratedbySceneCraft2D,SceneCraftisabletouseanSDS-equivalent
paradigm[46]toaggregatethemintoascenerepresentation(e.glet@tokeneonedot,NeRF[41]or
3DGaussiansplatting[27])ofthegenerated3Dscene. Notably,ourSceneCraftdoesnotrequirea
panoramicview. Instead,ourcameraviewcanmovefreelyinthe3Dspace,enablingthegeneration
ofmuchmorecomplicatedindoorlayoutsconsistingofmultiplerooms,unlikepriorworkwhichonly
supportssingle-roomscenes.
3.1 Bounding-BoxScene(BBS):AUser-FriendlyLayoutInterface
Toprovideauser-friendlyformatforfree-formindoorlayouts,wedesignthebounding-boxscene
(BBS) representation. As shown in Figure 2, BBS is similar to the “Proxy Room” of Control-
Room3D[53], buteachobjectinthescenecanberepresentedbyaunionofseveralintersecting
boundingboxesinBBSwithacategorylabel,toindicatethecoarseshapeandcategoryofanobject.
Thisprovidesuserswiththeabilitytoindicatetheshapeoftheobject,e.g.,anL-shapedorevenan
S-shapeddesk,whilestillmaintainingthefreedomofusingasingleboundingboxforgeneration.
3.2 SceneCraft2D:Layout-GuidedImageGeneration
BBScanberegardedasadraftoracoarseversionofthescene. Inordertogeneratetheactualroom
accuratelyconditionedonBBS,weuseadistillation-guidedframework. Eachviewofthegenerated
scenecorrespondstoa2Dgenerationtask,conditionedonthe“bounding-boximage(BBI)”ofthe
sameviewinBBS,whereeachpixelofBBIcontainsboththesemanticcategoryandthedepthofthe
pixelinBBS.ByrenderingBBSintoBBIontheprojectedcameratrajectoryprovided,wedecompose
thelayout-guided3Dscenegenerationtaskintoasetoflayout-guided2Dimagegenerationtasks,
withBBIasconditions. Tosolvethesetasks,weproposeSceneCraft2D,a2Ddiffusionmodelfor
high-qualitylayout-guided2Dimagegeneration.
AugmentedSDforBBIConditions. OurSceneCraft2DisaugmentedfromStableDiffusion[50],
withanadditionalBBIconditionatthecurrentviewpoint,whichcontainsboththesemanticcategory
4mapandtheBBSdepthmap. Thesemanticmap(convertedtoone-hotvectorsbasedonthecategory)
anddepthmapareinjectedintothemodel,asconditionsviatwoseparateControlNets[75].
Finetuning. We finetune the augmented Stable Diffusion with scenes in indoor datasets like
ScanNet++[71]andHypersim[49]. Eachsceneisconvertedtoagenerationtaskbygeneratingthe
prompt,convertingitssemanticpointcloudintoaBBS,andusingthecameratrajectoryprovided
by the dataset. We split the generation task into several 2D generation tasks at each view in the
dataset,andtraintheSDmodelwiththesetasks. Duringthefinetuningprocess,insteadofusing
existingcaptiontoolssuchasBLIP[29]togenerateprompts,weuseasinglebasepromptforall
trainingsamples.Duringinference-timegeneration,ourmodelsupportsmorespecificandcustomized
scene-specificpromptstoproducetheresultsthatusersdesire. Notethatthebasepromptdoesnot
needtocontainanyinformationdescribingtheimagecontent,itmerelyservesasaplaceholderto
avoidthemodeloverfittingtoanyparticularwordorsentence. Specifically,weuse“Thisisoneview
ofaroom.” asthebasepromptanduser-desiredtargetpromptslike“Thisisoneviewofabedroomin
VanGoghpaintingstyle.” forgeneration. Theresultsshowedthatthismethodeffectivelycontrolsthe
styleofthegeneratedoutputsviapromptswhilemaintainingagoodlayout-conditionedgeneration.
Afterfinetuning,SceneCraft2Dcangeneratehigh-qualityimagesaccordingtothegivenBBIandtext
prompt.
3.3 Distillation-GuidedSceneGeneration
DistillationProcesswithAnnealing. Togenerate3Dscenes,wedistillthegenerationabilityofour
pretrainedSceneScraft2DmodelinaScoreDistillationSampling(SDS)[46,63]-equivalentpipeline.
UnlikethevanillaSDS[46]thatworksinthelatentspaceanddirectlyworkswithgradients,our
pipelineappliesanIN2N[21]-style,whichisprovenSDS-equivalentbyHiFA[77]. Inthispipeline,
wemaintainamulti-viewdatasetforcontinualscenerepresentationtrainingwhilesimultaneouslyand
iterativelyreplacingthemulti-viewdatasetwithnewlygeneratedimagesbySceneCraft2D.Through
thisprocess,themulti-viewdatasetwillbegraduallyreplacedwithviewsofthegeneratedscenes,
whichareusedtofitthescenerepresentationtowardsthegeneration.
Withinthispipeline,wealsoproposeanannealing-baseddistillationstrategyinspiredby[39,77],
for a more efficient and high-quality distillation. Leveraging the SDEdit method [39] to control
the similarity of generated images with the currently modeled scene, we gradually decrease this
similarityalongwiththeentiredistillationprocedure. Inotherwords,atanearlystageofdistillation,
SceneCraft2DcanfreelygeneratetheroomtosatisfytheBBSandtheprompt;whileatalaterstage,
bygeneratingsimilarbuthigher-qualityimages,SceneCraft2Dcanalsoserveasarefinerofthescene
representationtorefinetherenderingresultandimprovethescenerepresentation. Withthispipeline,
ourSceneCraftisabletogeneratehigh-qualityscenes.
Layout-AwareDepthConstraint. Whengeneratingacomplexindoorscenebasedonfreecamera
trajectories,learningareasonablegeometryofthescenefromscratchisbothcrucialandchallenging.
However,wehavepriorknowledgeoftheBBSinput,whichallowsthemodeltoquicklycapturethe
geometryofthescenethroughthelayout-awaredepthconstraint. Specifically,attheinitialstageof
distillation,weaddanormalizeddepthlossL ,wherethepseudo-supervisionsignalcomesfrom
depth
ourBBSinput. WesetasoftthresholdδthatallowsthepixeldepthsD modeledbythescene
render
representationtofluctuatewithinareasonablerangearoundthepseudo-groundtruthdepthsD .
layout
Thisensuresthatthemodelquicklyconvergestoaninitialcoarsegeometry. Thislossismodeledin
thefollowingform:
L =[max(||D −D ||−δ, 0)]2. (1)
depth render layout
Laterinthedistillationprocess,wedisablethislosstermtoallowthemodeltolearnmorefine-grained
geometry.
Floc Removal with Periodical Migration. The images generated at the initial steps of the
distillationprocesshavealowerconsistency,whichcanresultinblurryflocsclosetothesurface
andintheairwhen“averaging”inconsistentmulti-viewimagesonthescenerepresentationside. At
alaterstage,evenwhenthediffusion’soutputisrelatively3D-consistentwithannealing,theflocs,
withcondensedvolumedensity,arestillhardtoremoveandmayresultinJanusproblems. Therefore,
insteadof“fixing”flocsissuesintheoriginalscenerepresentation,weproposeamethodtomigrate
thecurrentrelativelycoarsescenetoanotherscenefromscratch,toobtainafinerversion. Afterthe
5Figure3: GenerationresultsofSceneCraftonHypersim[49]providedroomlayouts. Foreachsample,we
demonstratethe3DBBSandBBIsemanticmapsandthegeneratedsceneRGBimagesandrendereddepthmap.
Ourmethodisabletogeneratecomplexandfree-formscenesfromchallengingroomlayouts.
firstseveraliterationsasearly-stagetraining,webegintomaintaintwoscenerepresentations,S and
c
S ,toindicatethepreviouscoarserepresentationandthemitigatedfinerepresentation,respectively.
f
WefreezeS andgeneratenewimagestosuperviseS bygeneratingimagessimilartoS ’srendering
c f c
results(byonlyapplyingt<T noiseaddingsteps),torefineS andstoreintoS withthediffusion
c f
model’s generation. We also periodically update S with S (with a smaller interval of training
f c
iterations)tosynchronizethelatestinformationinbothtwoscenerepresentations. Withtheperiodical
migration method, we achieve more and more fine-grained and clear scenes during the training
procedure.
TextureConsolidation. Thegenerationofhigh-qualityimagesbyourSceneCraft2Densuresthat
thescenerepresentationcanconvergeaccuratelytotheintendedscenegeometry. Thisadvancement
negatesthenecessityforexplicitmeshexportationfromscenerepresentationascommonlyrequired
inpreviouswork. Toassignthemodeledscenewithsharpandcleartextures,weincorporatetheuse
6Figure4:QualitativecomparisonsofSceneCraftandbaselineapproaches.Weshowourgeneratedcolorand
depthrenderingsundertwocommonlayoutconditions(abedroomandalivingroom)alongsidethreeother
baselines. SceneCraftdemonstrateshighercredibilityinfollowingthelayoutconditionsandiscapableof
handlingmorecomplexscenarios.
ofVGG[25]perceptualandstylizationlossduringthedistillationprocess. Thisstrategyallowsthe
scenerepresentationtoproducerenderedimagesthatsharesemanticmeaningandstylisticelements
withSceneCraft2D-generatedimages,ratherthanstrivingforpixel-perfectreplication,whichoften
leadstoblurredresults. Byemployingthisloss, ourSceneCraftframeworkemergesasaunified
modeltogeneratescenesinasharpandclearmanner,therebyeliminatingtheneedforlabor-intensive
processesofmeshexportationandoptimization.
4 Experiments
Inthissection,wefocusondemonstratingthequalityofSceneCraftgenerationundervariouslayout
conditionsandprompts,andcompareourperformancewithpubliclyavailablemethodsquantitatively
and qualitatively. Then we present more challenging generation that is beyond the scope of the
previousmethods.
ImplementationandDatasets. ForthedevelopmentofourSceneCraft2Ddiffusionmodel,we
finetune Stable Diffusion [50] with our produced layout data. We use multi-view images from
ScanNet++[71]andHyperSim[49]toconstructBBIdata. Inthedistillationprocess,wechoose
NerfactofromNeRFStudio[56]asourbackboneforscenerepresentation. Duringdistillation,weuse
aduo-GPUpipelinetoparallelizediffusiongenerationandNeRFtraining. Moredetailsareprovided
inAppendixSec.A.
BBSSources. Forefficiencyandeffectiveness, weemploytwodistinctapproachestoleverage
bounding-box scenes (BBS), one of which utilizes original 3D bounding boxes (axis-aligned or
oriented)bydirectlyrenderingtheminto2Dimages.Thisstraightforwardmethodisalreadysufficient
for the generation in our experiment, as we applied on Hypersim [49] data. Another approach
enhancestraditionalboundingboxesbyvoxelizingthemintoamoredetailedcollectionofsmaller,
7fine-grainedvoxels. Thismethodisparticularlyadeptatcapturingthenuancesofmorecomplex
geometriesandarrangementswithinascene,suchasL-shapedtablesorS-shapeddesks,whichoften
posechallengesformoresimplisticmodelingtechniques. Wefindthatthissignificantlyimprovesthe
model’sabilitytoaccuratelyrepresentandunderstandthespatialdynamicsandintricatedesignsof
variousobjectswithinascene. Weusethisstrategyforrealisticandchallengingscenes[71].
4.1 Layout-GuidedSceneGeneration
Baselines. Mostexistingworkdoesnotgeneratescenesconditionedonuser-specifiedlayouts[17,
24,37]. Theonlytwoconcurrentscenegenerationmethodsthatsupportlayoutguidancehavenot
releasedtheircodebases[16,53]forcomparison. Hence,wemakeourbestefforttocreateafair
comparisonwithopen-sourcedscenegenerationmethodsanddemonstratetheeffectivenessofour
methodthroughablationstudy(Sec.4.2). Text2Room[24]usesatext-conditionedinpaintingmodel
to construct the scene frame by frame. Following their original instructions, we change the text
promptalongthetrajectorytoreflectwhichobjectsarevisibleinthecurrentframe. Similarly,for
MVDiffusion [60], we construct different prompts for each of the eight views that make up the
panoramaimage. ForSet-the-scene[12],wefollowtheirofficialguidelines,using3Dmodeling
software (e.g., Blender) to create the same layout input for training and set the same prompts as
SceneCraft.
QualitativeResults. InFigure3,wedemonstratequalitativegenerationresultsofSceneCrafton
Hypersim[49]providedroomlayouts.Theseillustrationsvividlydemonstratethemodel’sproficiency
incraftingdetailed,complex,andfree-formscenes,showcasingitsapplicationacrossboththerealistic
andsyntheticdatasets. NotonlydoesithighlightthetechnicalprowessofSceneCraftinnavigating
theintricaciesofscenegeneration,butalsoitsadaptabilitytothediverserequirementsofreal-world
andartificiallyconstructedenvironments. InSec.4.3,wedemonstratemorechallenginggeneration,
whichincludesextremelychallengingcasesforpanorama-basedmethods,butnaturallysupportedby
ourframework.
Quantitative Results. We present Table 1: Quantitative comparisons of SceneCraft against
quantitative comparisons with base- baselines.
linemethods[12,24,60]usingboth
2Dand3DmetricsinTab.1. For2D
2DMetrics 3DQuality
metrics,wecomputetheCLIPScore Method
(CS) [48] and the Inception Score CS↑ IS↑ 3DC↑ VQ↑
(IS)[52],whichdonotrequireground
truth scenes from the dataset, and Text2Room[24] 22.98 4.20 3.11 3.06
therefore are agnostic to the dataset
MVDiffusion[60] 23.85 4.36 3.20 3.35
used in training. We also measure
3Dqualitybyconductingauserstudy Set-the-scene[12] 21.32 2.98 3.53 2.41
with32participants,whoscored3D
SceneCraft(Ours) 24.34 3.54 3.71 3.56
consistency(3DC)andoverallvisual
quality (VQ) of rooms generated by
differentmethodsonascaleof1to5. Ourexperimentaldesignfollowspreviouswork[51]. The
quantitativeresultshighlightthatourmethodconsistentlyoutperformspriorapproachesintermsof
theCLIPScore,3Dconsistency,andvisualquality. RegardingtheInceptionScore,weanticipatethat
ourdiffusionmodel’sfinetuningwithfixedcategoriesslightlylimitsgenerationdiversity. However,
thisisnotamajorconcernforourtask,aspreviousworkhasstruggledtoachievebothhighconsis-
tencyandvisualqualitywhilebeingcontrolledbylayoutprompts. Additionally,wedidnotprovide
othercommonmetricsongenerativetasks,e.g.,FréchetInceptionDistance(FID)Score,sinceitis
dependentonthegroundtruthdatasetandwouldresultinunfairandinaccuratecomparisonifapplied
toourexperiments.
ComparisonwithExistingMethods. InFigure4, wepresentourresultscomparedwiththree
baselines under two common layout conditions. SceneCraft significantly outperforms previous
methods. Forpanorama-basedmethods(MVDiffusion),thebiggestlimitationliesintheinability
to model rooms with complex shapes, such as L- or S-shaped structures. When using prompts
to describe layout conditions, MVDiffusion fails to generate the desired results accurately. For
inpainting-basedmethods(Text2Room),althoughtheysupportfreecameratrajectories,theiriterative
8Figure5: GenerationresultsofSceneCraftincomplexscenes. WedemonstrateSceneCraft’sability
togeneratemorecomplexindoorscenesleveragingarbitrarycameratrajectories. Suchnon-regular
shapeofroomscannotbenaturallyachievedbypreviouswork.
generationnatureoftenresultsinrepetitiveorcontradictoryframes.IntheexampleshowninFigure4,
Text2Roomgeneratesfourbedsinthatroomsimplybecausethepromptcontainstheword“bedroom,”
completely failing to adhere to the specified layout conditions. For NeRF-composition methods
(Set-the-scene),themaindrawbackistheinabilitytogenerateobjectswithsignificantsizedifferences.
Set-the-scene trains and combines different objects within the unified NeRF space. In Figure 4,
Set-the-scenefailstogenerateobjectshangingonwalls,suchasblindsortelevisions. Ourmodel,
however,addressesalltheseissues: itcangeneratescenesofanyscaleandcomplexityfollowingthe
givenlayoutconditionsandcanalsobeadjustedviaprompts.
4.2 AblationStudy
Weconductvariousablationstudiestovalidateourmethods. Specifically,wetesttheeffectofthe
basepromptusedinfinetuning,thelayout-awaredepthconstraint,andthetextureconsolidation. The
appendixsectionoffersacomprehensiveintroductiontoallofourevaluatedmodelsandadditional
experimentaldetails,andincludesfurthervisualizationandablationexperiments. Wealsoelaborate
onthelimitations,failurecases,broaderimpacts,andfuturedirectionsofourwork. Pleasereferto
AppendixSec.B.1formoredetails.
9Figure6:EffectofBasePrompt.Usingourbasepromptsuccessfullyavoidstheoverfittingandmaintainsthe
inherentpowerofpretrainedStableDiffusion,whileusingBLIP2captionsleadstocontrolfailure.
Figure7: StylevariantsonthefixedlayoutsofSceneCraft. WeshowthreevariantsA/B/Cwith
differentappearanceswhilethegeometriesremainunchanged.
EffectofBasePrompt. Toverifytheeffectivenessofusingourbaseprompt, wetestdifferent
promptsettings: e.g.,generatingimagecaptionsfromBLIP2[29]withuser-definedspecificprompts.
InFigure6,weshowthatourmethodsuccessfullyachievesthecontrolofthegenerationstylethrough
prompts,whilemaintainingagoodlayout-followingability. ConsideringthefailureoftheBLIP2
promptandthecomplexityofourlayoutconditions,webelievethatthemorecomplexthecondition,
themoregeneralthepromptweshouldtake. Inourcase,weuse“Thisisoneviewofaroom.” to
generally guide the model to fit the entire dataset of the indoor scene, rather than focusing on a
particularclassorobject.
4.3 MoreGenerationResults
GenerationonIrregularShape. InFigure5, weshowcasetheresultsofmorecomplexscene
generationwithfullycustomizedlayoutonthefree-cameratrajectory. Inthefirstexample(Scene
A),wecustomizeanindoorlayoutsinputwhereabedroomisconnectedtoalivingroom, along
withthecorrespondingarbitrarycameratrajectory. Theoretically,wecangenerateindoorscenesof
anyscale,forexample,complexindoorroomsystemscomposedofmultipleinterconnectedsmall
rooms(ScenesB-DofFigure5). Suchtasksarenotwell-supportedbymethodsbasedonpanorama
generation[60]orNeRFcomposition[12]. Althoughsomeotherwork[24]supportsarbitrarycamera
trajectories,itperformspoorlyinestablishingreasonablescenegeometryandcontrollingthescene
content.
StyleVariantsGenerationwithFixedLayouts. InFigure7,weshowthreevariantsofgeneration
withthesameroomlayoutanddifferentappearance,simplyachievedbyusingdifferentprompts.
TheresultsdemonstratethevariouscontrolabilitiesofSceneCraft,allowingustoaccuratelydefine
theshapeandappearanceofgeneration.
5 Conclusion
ThisworkhasintroducedSceneCraft,aninnovativemethodforgeneratingcomplexanddetailed
indoorscenesfromtextualdescriptionsandspatiallayouts.Byleveragingarendering-basedoperation,
andalayout-conditioneddiffusionmodel,ourworkeffectivelyconverts3Dsemanticlayoutsinto
multi-view2Dimagesandlearnsafinalscenerepresentationthatisnotonlyconsistentandrealistic
but also adheres closely to user specifications. Experimental results show the superiority of our
modeloverexistingstate-of-the-artmethods,highlightingitsabilitytogeneratediversetexturesand
maintaingeometricconsistencyacrosscomplexindoorscenes.
10References
[1] P. Achlioptas, O. Diamanti, I. Mitliagkas, and L. J. Guibas. Learning representations and
generativemodelsfor3Dpointclouds. InICML,2018. 2
[2] J.T.Barron, B.Mildenhall, M.Tancik, P.Hedman, R.Martin-Brualla, andP.P.Srinivasan.
Mip-NeRF:Amultiscalerepresentationforanti-aliasingneuralradiancefields. InICCV,2021.
3
[3] J.Betker,G.Goh,L.Jing,T.Brooks,J.Wang,L.Li,L.Ouyang,J.Zhuang,J.Lee,Y.Guo,
W.Manassra, P.Dhariwal, C.Chu, andY.Jiao. Emu: Enhancingimagegenerationmodels
usingphotogenicneedlesinahaystack. arXivpreprintarXiv:2309.15807,2023. 3
[4] J.Betker,G.Goh,L.Jing,T.Brooks,J.Wang,L.Li,L.Ouyang,J.Zhuang,J.Lee,Y.Guo,
WesamManassra, PrafullaDhariwal, CaseyChu, Y. Jiao, and A. Ramesh. Improving image
generationwithbettercaptions. ComputerScience.https://cdn.openai.com/papers/dall-e-3.
pdf,2(3):8,2023. 3
[5] E.R.Chan,K.Nagano,M.A.Chan,A.W.Bergman,J.J.Park,A.Levy,M.Aittala,S.DeMello,
T.Karras,andG.Wetzstein. Generativenovelviewsynthesiswith3D-awarediffusionmodels.
InICCV,2023. 3
[6] A.Chen,Z.Xu,F.Zhao,X.Zhang,F.Xiang,J.Yu,andH.Su. MVSNeRF:Fastgeneralizable
radiancefieldreconstructionfrommulti-viewstereo. InICCV,2021. 3
[7] J.-K.Chen,J.Lyu,andY.-X.Wang. NeuralEditor: Editingneuralradiancefieldsviamanipulat-
ingpointclouds. InCVPR,2023. 3
[8] J.-K.Chen,S.R.Bulò,N.Müller,L.Porzi,P.Kontschieder,andY.-X.Wang. ConsistDreamer:
3D-consistent2Ddiffusionforhigh-fidelitysceneediting. InCVPR,2024. 15
[9] M.Chen,I.Laina,andA.Vedaldi. Training-freelayoutcontrolwithcross-attentionguidance.
InWACV,2024. 3
[10] R.Chen,Y.Chen,N.Jiao,andK.Jia. Fantasia3D:Disentanglinggeometryandappearancefor
high-qualitytext-to-3Dcontentcreation. InICCV,2023. 3
[11] Z.ChenandH.Zhang. Learningimplicitfieldsforgenerativeshapemodeling. InCVPR,2019.
2
[12] D.Cohen-Bar,E.Richardson,G.Metzer,R.Giryes,andD.Cohen-Or. Set-the-scene: Global-
local training for generating controllable NeRF scenes. In ICCV Workshop, 2023. 2, 3, 8,
10
[13] B. O.Community. Blender -a 3Dmodelling and rendering package. BlenderFoundation,
StichtingBlenderFoundation,Amsterdam,2018. URLhttp://www.blender.org. 1
[14] EpicGames. Unrealengine. URLhttps://www.unrealengine.com. 1
[15] P.Esser,R.Rombach,andB.Ommer. Tamingtransformersforhigh-resolutionimagesynthesis.
InCVPR,2021. 3
[16] C.Fang,X.Hu,K.Luo,andP.Tan.Ctrl-Room:Controllabletext-to-3Droommeshesgeneration
withlayoutconstraints. arXivpreprintarXiv:2310.03602,2023. 1,2,3,8,17
[17] R.Fridman,A.Abecasis,Y.Kasten,andT.Dekel. SceneScape: Text-drivenconsistentscene
generation. InNeurIPS,2023. 1,3,8
[18] O. Gafni, A. Polyak, O. Ashual, S. Sheynin, D. Parikh, and Y. Taigman. Make-a-scene:
Scene-basedtext-to-imagegenerationwithhumanpriors. InECCV,2022. 3
[19] G.Gao,W.Liu,A.Chen,A.Geiger,andB.Schölkopf. GraphDreamer: Compositional3D
scenesynthesisfromscenegraphs. InCVPR,2024. 3
[20] Y.-C.Guo,D.Kang,L.Bao,Y.He,andS.-H.Zhang. NeRFReN:Neuralradiancefieldswith
reflections. InCVPR,2022. 3
[21] A.Haque,M.Tancik,A.Efros,A.Holynski,andA.Kanazawa. Instruct-NeRF2NeRF:Editing
3Dsceneswithinstructions. InICCV,2023. 5
[22] P.Hedman,T.Ritschel,G.Drettakis,andG.Brostow.Scalableinside-outimage-basedrendering.
ACMTransactionsonGraphics,35(6):231:1–231:11,2016. 2
[23] J.Ho,A.Jain,andP.Abbeel. Denoisingdiffusionprobabilisticmodels. InNeurIPS,2020. 1,3
11[24] L.Höllein,A.Cao,A.Owens,J.Johnson,andM.Nießner. Text2room: Extractingtextured3D
meshesfrom2Dtext-to-imagemodels. InICCV,2023. 1,3,8,10
[25] J.Johnson,A.Alahi,andL.Fei-Fei. Perceptuallossesforreal-timestyletransferandsuper-
resolution. InECCV,2016. 7
[26] T.L.KayandJ.T.Kajiya. Raytracingcomplexscenes. SIGGRAPHComputerGraphics,20
(4):269–278,Aug.1986. 15
[27] B.Kerbl,G.Kopanas,T.Leimkühler,andG.Drettakis. 3DGaussiansplattingforreal-time
radiancefieldrendering. ACMTransactionsonGraphics,42(4),July2023. 3,4
[28] M.LevoyandP.Hanrahan. Lightfieldrendering. InSIGGRAPH,1996. 2
[29] J.Li,D.Li,C.Xiong,andS.Hoi. BLIP:Bootstrappinglanguage-imagepre-trainingforunified
vision-languageunderstandingandgeneration. InICML,2022. 5,10
[30] Y.Li,H.Liu,Q.Wu,F.Mu,J.Yang,J.Gao,C.Li,andY.J.Lee. GLIGEN:Open-setgrounded
text-to-imagegeneration. InCVPR,2023. 3
[31] C.-H.Lin,J.Gao,L.Tang,T.Takikawa,X.Zeng,X.Huang,K.Kreis,S.Fidler,M.-Y.Liu,and
T.-Y.Lin. Magic3D:High-resolutiontext-to-3Dcontentcreation. InCVPR,2023. 1,3
[32] Y.Lin,H.Bai,S.Li,H.Lu,X.Lin,H.Xiong,andL.Wang. CompoNeRF:Text-guidedmulti-
objectcompositionalNeRFwitheditable3Dscenelayout. arXivpreprintarXiv:2303.13843,
2023. 3
[33] M.Liu,C.Xu,H.Jin,L.Chen,M.V.T,Z.Xu,andH.Su. One-2-3-45: Anysingleimageto
3Dmeshin45secondswithoutper-shapeoptimization. InNeurIPS,2023. 3
[34] R. Liu, R. Wu, B. Van Hoorick, P. Tokmakov, S. Zakharov, and C. Vondrick. Zero-1-to-3:
Zero-shotoneimageto3Dobject. InICCV,2023. 3
[35] F.Lu,K.-Y.Lin,Y.Xu,H.Li,G.Chen,andC.Jiang. Urbanarchitect: Steerable3Durban
scenegenerationwithlayoutprior. arXivpreprintarXiv:2404.06780,2024. 3,15
[36] Y.Man,Y.Sheng,J.Zhang,L.-Y.Gui,andY.-X.Wang. FloatingNoMore: Object-ground
reconstructionfromasingleimage. arXivpreprintarXiv:2407.18914,2024. 3
[37] W.Mao,Y.-P.Cao,J.-W.Liu,Z.Xu,andM.Z.Shou. ShowRoom3D:Texttohigh-quality3D
roomgenerationusing3Dpriors. arXivpreprintarXiv:2312.13324,2023. 1,3,8,15
[38] L.Melas-Kyriazi,C.Rupprecht,I.Laina,andA.Vedaldi. RealFusion: 360reconstructionof
anyobjectfromasingleimage. InCVPR,2023. 3
[39] C.Meng,Y.He,Y.Song,J.Song,J.Wu,J.-Y.Zhu,andS.Ermon. SDEdit: Guidedimage
synthesisandeditingwithstochasticdifferentialequations. InICLR,2022. 3,5,18
[40] L.Mescheder,M.Oechsle,M.Niemeyer,S.Nowozin,andA.Geiger. Occupancynetworks:
Learning3Dreconstructioninfunctionspace. InCVPR,2019. 2
[41] B.Mildenhall,P.P.Srinivasan,M.Tancik,J.T.Barron,R.Ramamoorthi,andR.Ng. NeRF:
Representingscenesasneuralradiancefieldsforviewsynthesis. InECCV,2020. 3,4
[42] L. Mou, J.-K. Chen, and Y.-X. Wang. Instruct 4D-to-4D: Editing 4D scenes as pseudo-3D
scenesusing2Ddiffusion. InCVPR,2024. 15
[43] N.Müller,Y.Siddiqui,L.Porzi,S.R.Bulo,P.Kontschieder,andM.Nießner.DiffRF:Rendering-
guided3Dradiancefielddiffusion. InCVPR,2023. 3
[44] M.Niemeyer,L.Mescheder,M.Oechsle,andA.Geiger. Differentiablevolumetricrendering:
Learningimplicit3Drepresentationswithout3Dsupervision. InCVPR,2020. 2
[45] R.PoandG.Wetzstein. Compositional3Dscenegenerationusinglocallyconditioneddiffusion.
In3DV,2024. 1,2,3
[46] B.Poole,A.Jain,J.T.Barron,andB.Mildenhall. DreamFusion:Text-to-3Dusing2Ddiffusion.
InICLR,2022. 1,3,4,5
[47] G.Qian,J.Mai,A.Hamdi,J.Ren,A.Siarohin,B.Li,H.-Y.Lee,I.Skorokhodov,P.Wonka,
S.Tulyakov,andG.Bernard. Magic123: Oneimagetohigh-quality3Dobjectgenerationusing
both2Dand3Ddiffusionpriors. InICLR,2024. 3
12[48] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,
P.Mishkin,J.Clark,G.Krueger,andI.Sutskever. Learningtransferablevisualmodelsfrom
naturallanguagesupervision. InICML,2021. 8
[49] M.Roberts,J.Ramapuram,A.Ranjan,A.Kumar,M.A.Bautista,N.Paczan,R.Webb,andJ.M.
Susskind. HyperSim: Aphotorealisticsyntheticdatasetforholisticindoorsceneunderstanding.
InICCV,2021. 2,5,6,7,8,15,18
[50] R.Rombach,A.Blattmann,D.Lorenz,P.Esser,andB.Ommer.High-resolutionimagesynthesis
withlatentdiffusionmodels. InCVPR,2022. 1,3,4,7,18
[51] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gon-
tijo Lopes, B. Karagol Ayan, T. Salimans, H. Jonathan, J. F. David, and N. Mohammad.
Photorealistictext-to-imagediffusionmodelswithdeeplanguageunderstanding. InNeurIPS,
2022. 3,8
[52] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved
techniquesfortrainingGANs. InNeurIPS,2016. 8
[53] J.Schult,S.Tsai,L.Höllein,B.Wu,J.Wang,C.-Y.Ma,K.Li,X.Wang,F.Wimbauer,Z.He,
Z.Peizhao,L.Bastian,V.Peter,andH.Ji. Controlroom3D:Roomgenerationusingsemantic
proxyrooms. InCVPR,2024. 1,2,3,4,8,15
[54] N.Snavely,S.M.Seitz,andR.Szeliski. Phototourism: Exploringphotocollectionsin3D.
ACMTransactionsonGraphics,25(3):835–846,2006. 2
[55] J.Sohl-Dickstein,E.Weiss,N.Maheswaranathan,andS.Ganguli. Deepunsupervisedlearning
usingnonequilibriumthermodynamics. InICML,2015. 1,3
[56] M.Tancik,E.Weber,E.Ng,R.Li,B.Yi,J.Kerr,T.Wang,A.Kristoffersen,J.Austin,K.Salahi,
A.Ahuja,D.McAllister,andA.Kanazawa. NeRFStudio: Amodularframeworkforneural
radiancefielddevelopment. InSIGGRAPH,2023. 2,3,7
[57] J.Tang,T.Wang,B.Zhang,T.Zhang,R.Yi,L.Ma,andD.Chen. Make-it-3D:High-fidelity
3Dcreationfromasingleimagewithdiffusionprior. InICCV,2023. 3
[58] J.Tang, Y.Nie, L.Markhasin, A.Dai, J.Thies, andM.Nießner. DiffuScene: Scenegraph
denoisingdiffusionprobabilisticmodelforgenerativeindoorscenesynthesis. InCVPR,2024. 3
[59] J.Tang,J.Ren,H.Zhou,Z.Liu,andG.Zeng. DreamGaussian: GenerativeGaussiansplatting
forefficient3Dcontentcreation. InICLR,2024. 3
[60] S. Tang, F. Zhang, J. Chen, P. Wang, and Y. Furukawa. MVDiffusion: Enabling holistic
multi-viewimagegenerationwithcorrespondence-awarediffusion. InNeurIPS,2023. 1,3,8,
10
[61] H.-Y.Tseng,Q.Li,C.Kim,S.Alsisan,J.-B.Huang,andJ.Kopf. Consistentviewsynthesis
withpose-guideddiffusionmodels. InCVPR,2023. 3
[62] D.Verbin,P.Hedman,B.Mildenhall,T.Zickler,J.T.Barron,andP.P.Srinivasan. Ref-NeRF:
Structuredview-dependentappearanceforneuralradiancefields. InCVPR,2022. 3
[63] H.Wang,X.Du,J.Li,R.A.Yeh,andG.Shakhnarovich. ScoreJacobianChaining: Lifting
pretrained2Ddiffusionmodelsfor3Dgeneration. InCVPR,2023. 1,3,5
[64] N.Wang,Y.Zhang,Z.Li,Y.Fu,W.Liu,andY.-G.Jiang. Pixel2Mesh: Generating3Dmesh
modelsfromsingleRGBimages. InECCV,2018. 2
[65] Q. Wang, Z. Wang, K. Genova, P. Srinivasan, H. Zhou, J. T. Barron, R. Martin-Brualla,
N.Snavely,andT.Funkhouser. IBRNet: Learningmulti-viewimage-basedrendering. InCVPR,
2021. 3
[66] Z.Wang,C.Lu,Y.Wang,F.Bao,C.Li,H.Su,andJ.Zhu. ProlificDreamer: High-fidelityand
diversetext-to-3Dgenerationwithvariationalscoredistillation. InNeurIPS,2023. 1,3
[67] L.Wu,J.Lee,A.Bhattad,Y.-X.Wang,andD.Forsyth. DIVeR:Real-timeandaccurateneural
radiancefieldswithdeterministicintegrationforvolumerendering. InCVPR,2022. 3
[68] D.Xu,Y.Jiang,P.Wang,Z.Fan,Y.Wang,andZ.Wang. NeuralLift-360: Liftinganin-the-wild
2Dphototoa3Dobjectwith360°views. InCVPR,2023. 3
[69] Q.Xu,Z.Xu,J.Philip,S.Bi,Z.Shu,K.Sunkavalli,andU.Neumann. Point-NeRF:Point-based
neuralradiancefields. InCVPR,2021. 3
13[70] Z.Yang,J.Wang,Z.Gan,L.Li,K.Lin,C.Wu,N.Duan,Z.Liu,C.Liu,M.Zeng,andL.Wang.
RECO:Region-controlledtext-to-imagegeneration. InCVPR,2023. 3
[71] C.Yeshwanth,Y.-C.Liu,M.Nießner,andA.Dai. ScanNet++: Ahigh-fidelitydatasetof3D
indoorscenes. InICCV,2023. 2,5,7,8,15,16,18
[72] A.Yu,R.Li,M.Tancik,H.Li,R.Ng,andA.Kanazawa. PlenOctreesforreal-timerenderingof
neuralradiancefields. InICCV,2021. 3
[73] A.Yu,V.Ye,M.Tancik,andA.Kanazawa. PixelNeRF:Neuralradiancefieldsfromoneorfew
images. InCVPR,2021. 3
[74] J.Zhang,X.Li,Z.Wan,C.Wang,andJ.Liao. Text2NeRF:Text-driven3Dscenegeneration
withneuralradiancefields. IEEETransactionsonVisualizationandComputerGraphics,2024.
1,3
[75] L.Zhang,A.Rao,andM.Agrawala. Addingconditionalcontroltotext-to-imagediffusion
models. InICCV,2023. 2,5
[76] Q.Zhang,C.Wang,A.Siarohin,P.Zhuang,Y.Xu,C.Yang,D.Lin,B.Zhou,S.Tulyakov,
and H.-Y. Lee. SceneWiz3D: Towards text-guided 3D scene composition. arXiv preprint
arXiv:2312.08885,2023. 17
[77] J.ZhuandP.Zhuang. HiFA:High-fidelityText-to-3DwithAdvancedDiffusionGuidance. In
ICLR,2024. 5
14SceneCraft: Layout-Guided 3D Scene Generation
Appendix
FigureA:Anillustrationofduo-GPUtrainingscheduling.
A AdditionalDetails
DataProcessing. AsmentionedinSec.4,weusetwodatasets:HyperSim[49]andScanNet++[71],
whicharesyntheticdataandreal-worlddata,respectively. WemainlyusetheHyperSimdatasetin
ourexperiments. ForHyperSim,theoriginaldataprovideapproximately77400imagesof461indoor
scenes,withthecorrespondingcameraparametersandsemanticboundingboxes. However,someof
themdonotmeetthequalityrequirementsofourtraining. Followingpriorwork[53],weexaminethe
entiredatasetandfilteroutthelower-qualityportions,suchasroomscontainingextremelycomplex
andirregularlyshapedobjects,unboundedoutdoorspace,androomswithexcessivelylargescales
(e.g.,churches,restaurants),ultimatelyretainingapproximatelyhalfoftheoriginaldata,amounting
to around 24k pairs. ScanNet++ shows more complicated scenes (450+ scenes) compared with
HyperSim. Wevoxelizethemwithaunitsizeequalto0.2mtomakethetrade-offbetweenrendering
costanddataquality. Ourprocesseddataarepubliclyavailable12. WeleveragetheRay-OBBmodel
(extendedfromtheRay-AABBmodel[26])tocompletetherasterizationprocess.
Duo-GPUTrainingScheduling. Inspiredby[8,42],wehaveimplementedaduo-GPUscheduler
(seeFigureA)forefficientdiffusionandNeRFgeneration.Specifically,thesecond(oranyGPUother
thanthefirst)GPUcontinuouslygeneratesnewimagestoupdatethedataset,whilethefirstGPU
continuouslytrainsNerfactowiththecurrentdataset. Oncethediffusionprocedureneedsimages
from the first GPU to refine, the first GPU will switch to an offline renderer. This configuration
effectivelydecouplesthediffusiongenerationprocess,whichisinherentlymoretime-intensive,from
thecomparativelyrapidNeRFtraining,thusstreamliningtheoveralldistillationworkflowwithout
compromisingonqualityorefficiency.
TrainingDetailsandCost. Forfinetuningthediffusionmodel,weuseatotalbatchsizeof16on
2NVIDIAA6000GPUswithaconstantlearningrateof5e-5,trainingforaround10kiterations.
Forthescenegenerationtask, weuse2A6000GPUstoperformallourexperiments. Generally,
eachwell-generatedscenetakesaround3-4hourswith~150frames,5-6hourswith~300frames
(whichhandlescameratrajectoriesofarbitrarylengthandcomplexity),thefirstGPUisresponsible
forthediffusionmodelandthememorycostis~6GBinFP16modewithanimagesizeof512×768.
TheotherGPUisresponsibleforthescenerepresentationprocessandthememorycostis~28GB
(with Nerfacto). Note that some concurrent methods cost more time: ShowRoom3D [37] costs
approximately10hourstoproduceasinglescene(Tab.3in[37]),UrbanArchitect[35]costs~12
hoursand32GBtoproduceasinglescene(Sec. 4.1in[35]). ForNeRFtraining,weuseaconstant
learningrateof1e-2forproposalnetworksand1e-3forfields.
B AdditionalExperiments
Inthissection,wefirstdiscussadditionalaspectsofablationstudies,whichshowthesuperiorityofour
method. ThenweshowcaseadditionalgenerationsonmorecomplexlayoutsviaourSceneCraft2D.
1LayoutScannet++:https://huggingface.co/datasets/gzzyyxy/layout_diffusion_scannetpp_voxel0.2
2LayoutHypersim:https://huggingface.co/datasets/gzzyyxy/layout_diffusion_hypersim
15FigureC:EffectofTextureConsolidation.Thegeneratedrenderingsareblurrywithoutthetextureconsolidation
strategy.OurSceneCraftproducesmoredetailedandtexturedresults.
B.1 AblationStudy
EffectofTextureConsolidation. Wedemonstratetheeffectivenessofourtextureconsoidation
showninFigureC.Withouttextureconsolidation,thelossfunctiononlyincludesthelatentimageloss
andRGBimageloss,similartoconventionalSDSmethods. Inthiscase,themodelfailstocapturethe
high-frequencycomponentsofthescenefrom2Dimages,resultinginveryblurrygeneratedscenes.
EffectofLayout-AwareDepthCon-
straint. Weconductexperimentsto
validatetheeffectivenessofthelayout-
awaredepthconstraint. Asdepicted
inFigureB,withoutthisstrategy,the
model is completely unable to learn
thecorrectgeometryofthescenefrom
2Dguidance. Althoughareasonable
appearance is achieved, it is mainly
duetothefullyflexiblecameratrajec-
FigureB:Layout-AwareDepthConstraint.Withdepthconstraint
toriesandcomplexlayoutconditions. strategy,ourSceneCraftcaneffectivelylearnscenegeometryfrom
Notably,usingfixedcameraposesas priorinput,whichiscrucialtoultimate3Dconsistency.
inpanoramagenerationcanpartially
alleviatethisissue. Withthelayout-awaredepthconstraint,thescenegeometryrapidlyconverges
tothegroundtruthduringtheveryinitialtrainingstage. Althoughsomeareamayfindincorrect
positions(redbox),theyarecorrectedduringsubsequenttrainingafterintermediatelydisablingthis
strategy,eventuallyleadingtofinalconvergence(greenbox). Oncethegeometryconvergeswell,the
colorandtexturebegintoemerge.
B.2 ComplexRoomGenerationwithIrregularObjectGeometryandFreeCamera
Trajectory
InFigureD,wetakethecomplexroomlayoutsfromtheScanNet++[71]datasetasanexample. The
firsttworowsrepresenttheinputlayoutcondition,andthelastrowshowstheSceneCraft2Doutput.
Notethatforroomswithirregularobjectgeometries,weconvertvoxelized3Dboundingboxesinto
morefine-grainedvoxels. Thismethodparticularlycaptureslayoutswithirregulargeometries,such
asL-shapedtablesorS-shapeddesks,whichposechallengestomoresimplisticmodelingtechniques.
TheresultsshowthatSceneCraft2Dperformswellinsuchcomplexroomlayouts.
C LimitationsandPotentialFutureDirection
Althoughwehaveachievedpromisingresultsincomplexscenegenerationfromuserprompts,3D
generationremainsaverychallengingtaskwithmanyunsolvedproblems,andourmethodislimited
insomeaspects. Thissectionprovidesadetaileddiscussionofthelimitationsandoutlinespotential
futuredirections.
DiscussiononFailureCases. Despiteourpromisingperformance,somefailurecasesalsoexist:
(1)ExtremelyComplicatedScenes. Ourmethodmaystruggletoreasonthelayoutsandgenerate
rooms when layouts are excessively complex, containing many closely placed objects or highly
overlappedboundingboxes(seeFigureE).Inthiscase,ouroptimizedmethodusingvoxelization
16FigureD:PerformanceofSceneCraft2DoncomplexScanNet++providedroomlayouts.
doesnotprovideclearandaccuraterepresentationofobjectlayouts,whichalsoreflectsthedifference
betweenindoorandoutdoor(street-view)scenes;(2)MismatchedLayoutandPromptInputs. When
the prompt does not align with the actual room layouts (e.g., a bedroom layout with a “kitchen”
prompt),ourmethodmayfailtogenerateappropriateroomcontentsorachievegoodconvergence
(seeFigureF).Hence,usersmayneedtoadjustthecorrespondingpromptswhengeneratingalarge
complexscenewithalong-termtrajectory. Allofthesefailurecasesreflectthelimitationsofour
method.
Quality of Images Requires Further Improvement. The quality of our generated 3D scenes
stillrequiresfurtherimprovement. Whendealingwithmorechallengingobjectsthattypicallyhave
irregulargeometries,suchashollowed-outchairs,lamps,orblinds,ourresultstendtobesomewhat
blurry. Moreover,complexlayoutconditioninputsstilllimitthecontrolabilityoftheprompt. For
instance, weareunabletogenerateobjectsasvividandrichlydetailedasthoseproducedbythe
originaldiffusionmodel.
Extension to Generation of Outdoor Scenes. Most existing literature separates the task of
generating indoor and outdoor scenes and focuses on one of the scenarios in their work. The
generationofindoorandoutdoorscenariosfeaturesdifferentchallenges,withindoorsceneshaving
denserandmorecomplexlayouts,andoutdoorsceneshavingmoredynamicobjectsandalarger
spacetocover. Althoughitisavalidchoicetostartfromindoorscenegenerationbecauseofits
relativelysmallerscaleandcomplexlayouts,itisnotcomprehensive. Thegenerationofoutdoor
sceneswilllikelyleadtouniquechallengesandinsights,andweconsiderthisadirectfuturedirection.
OtherFutureDirections. Therearealsochallengesindefiningfair,accurate,andcomprehensive
metricsforevaluating3Dscenegenerationmethodsinallaspects. Thedevelopmentofsuchmetrics
remainsanopenchallengeandservesasapotentialdirectionforexplorationinthegenerativeAI
community. Anotherpromisingdirectionisflexibleandcontrollablesceneeditingusingdecomposed
3Drepresentations,giventhatourlayoutinputisfreelydefinedandadjustable. Forcomplexscene
generation,creatinglayoutsbyhandcouldbetime-consumingandlaborious,henceintegratingsome
methodsofautomaticscenelayoutsandcameratrajectoriescreationwillalsobeafuturetopic,such
asLLM(largelanguagemodel)-based[76]andtransformer-basedapproaches[16]. Extendingour
frameworktoincorporateuserfeedbackloopsforiterativerefinementofgeneratedscenesoffers
anotherpromisingdirection.
D SocietalImpact
Weanticipateapotentialpositivesocialimpactfromourwork.Beingabletogeneratehigh-quality3D
digitalcontentbasedonuserqueries,SceneCrafthasthepotentialtorevolutionizevariousindustries,
fromVR/ARtoarchitecturaldesignandgaming. Bysignificantlyreducingthetimeandexpertise
requiredtocreatedetailed3Dscenes,ourworkalsoleadstomoreaccessibleandfaircontentcreation.
PotentialNegativeSocietalImpact. Wedonotseeadirectnegativesocietalimpactofourwork.
Indirect potential negative impact involves misusing scene generation models for digital content
creation. We believe that it is crucial for researchers to proactively consider these concerns and
establishguidelinestoensureresponsibleusageofthesemodels.
17FigureE:Failurecase#1: Extremelycomplicatedscenethatcontainsmanycloselyplacedobjects
andevenhighlyoverlappedboundingboxes.
Figure F: Failure case #2: Generations of a bedroom with a matched prompt “Bedroom” and a
mismatchedprompt“Kitchen.”
E LicenseofDatasetUsed
Inthissection,welistthelicensesofallthedatasetswehaveusedduringourevaluation:
• ScanNet++[71]: TheScanNet++dataarereleasedundertheScanNet++TermsofUse3.
• HyperSim[49]: CreativeCommonsAttribution-ShareAlike3.0UnportedLicense.
Inaddition,weutilizeanumberofpublicfoundationmodelcheckpointspretrainedonvariousdata
sourcesinourpaper. Pleaserefertotheiroriginalpapers[39,50]forthelicenseofdatasetstheyhave
usedinpretrainingtheirmodels.
3https://kaldir.vc.in.tum.de/scannetpp/static/scannetpp-terms-of-use.pdf
18