Mentor-KD: Making Small Language Models Better Multi-step Reasoners
HojaeLee1*, JunhoKim2*, SangKeunLee1,2
1DepartmentofComputerScienceandEngineering 2DepartmentofArtificialIntelligence
KoreaUniversity,Seoul,RepublicofKorea
{22leehojae, monocrat, yalphy}@korea.ac.kr
Abstract ters(Chungetal.,2022;Weietal.,2022a),which
requiresignificantcomputationalresourcesorex-
Large Language Models (LLMs) have dis-
pensiveAPIcalls,restrictingtheirdeploymenton
played remarkable performances across var-
resource-limitedscenarios.
ious complex tasks by leveraging Chain-of-
Tocircumventthesedeploymentchallenges,pre-
Thought (CoT) prompting. Recently, studies
haveproposedaKnowledgeDistillation(KD) viousworks(Hoetal.,2023;Lietal.,2023;Magis-
approach,reasoningdistillation,whichtrans- teretal.,2023)havefollowedaknowledgedistilla-
fers such reasoning ability of LLMs through tion(KD)approach,reasoningdistillation,which
fine-tuninglanguagemodelsofmulti-stepra- transfersthemulti-stepreasoningabilityofLLMs
tionalesgeneratedbyLLMteachers. However,
tosmallLMs. TheKDpipelinegenerallyapplies
they have inadequately considered two chal-
In-Context Learning (ICL) on the LLM teacher
lenges regarding insufficient distillation sets
model to generate outputs (e.g., multi-step ratio- from the LLM teacher model, in terms of 1)
nales)asdistillationsets,andthenutilizesthemto
dataqualityand2)softlabelprovision. Inthis
paper, we propose Mentor-KD, which effec- fine-tunethestudentmodel. Previousstudieshave
tivelydistillsthemulti-stepreasoningcapabil- shownthatreasoningdistillationcansignificantly
ity of LLMs to smaller LMs while address- improvestudentperformancesandmayevenout-
ing the aforementioned challenges. Specifi-
performtheirLLMteachersonspecifictasks(Ho
cally,weexploitamentor,intermediate-sized
etal.,2023;Chenetal.,2023).
task-specificfine-tunedmodel,toaugmentad-
However,previousapproachestoreasoningdis-
ditional CoT annotations and provide soft la-
tillationhavetwochallengesarisingfrominsuffi-
bels for the student model during reasoning
distillation. Weconductextensiveexperiments cientdistillationsetsgeneratedbyLLMteachers.
andconfirmMentor-KD’seffectivenessacross First,asLLMsmaynothaveaccesstotask-specific
variousmodelsandcomplexreasoningtasks1. data,thequalityoftherationalesfordistillationcan
be low (e.g., only 58% accuracy on GPT-3.5 for
1 Introduction
StrategyQA).ThelowqualityofLLMteacherratio-
LargeLanguageModels(LLMs)haveshownim- naleslimitsthenumberofreasoningrationalesto
pressiveemergentcapabilities,showingtheircom- onlyasmallsetofcorrectonesduetotheexclusion
petenceonavarietyofreasoningtasksintheNatu- ofincorrectrationalesthatnegativelyaffectstudent
ralLanguageProcessing(NLP)landscape(Brown performances (Hoet al., 2023). Second, because
etal.,2020;Raeetal.,2021;Hoffmannetal.,2022; accessibility of black-box LLM teachers is gen-
Chowdheryetal.,2023). Oneparticularlyinterest- erallyrestricted,thestudentmodelcannotmimic
ingstrategyforthisapproachisChain-of-Thought the predictive behavior and knowledge from the
(CoT)prompting,whichelicitsmulti-stepreason- soft labels (Hinton et al., 2015). Such oversights
ing abilities of LLMs by explicitly generating in- mayleadtothestudentmodelbeingover-fittedon
termediatereasoningstepsforcomplextasks(Wei limited distillation sets from teacher models and
et al., 2022b). However, such reasoning abilities undermineitsgeneralizationcapabilities.
havebeenshowntoonlymanifestinlanguagemod- Toaddressthesechallenges,weproposeMentor-
els (LMs) with over hundreds of billion parame- KD,anovelreasoningdistillationframeworkthat
effectivelydistillsthemulti-stepreasoningcapabil-
* Theseauthorscontributedequallytothiswork.
ityofLLMs. Ourcoreideaistointroduceamen-
1Our code and data are available at https://github.
com/2hojae/mentor-kd tor,anintermediate-sizedtask-specificmodel,that
4202
tcO
11
]LC.sc[
1v73090.0142:viXraFigure1: Comparisonbetween(a)previousapproachesofreasoningdistillationand(b)Mentor-KD(ours). Our
frameworkutilizesanintermediate-sizedtask-specificmentormodeltocomplementthedistillationsetsofteachers.
complementstheLLMteacher’sknowledgeduring 2 RelatedWorks
reasoningdistillation. Tothisend,wefirstfine-tune
2.1 Chain-of-ThoughtPrompting
thementormodelsonspecifictasksandgenerate
bothCoTrationalesandsoftlabelstoaugmentdis- CoTpromptingisamethodthatelicitsmulti-step
tillationsets. Byleveragingtask-specificmentors reasoningabilitiesofLMsthroughICL(Weietal.,
whosepowerisconcentratedtowardaspecifictar- 2022b). The essence of CoT is that it acts as a
get ability, Mentor-KD effectively addresses two guidance of logical progression for LMs to de-
issuesthroughtrainingonmorediverserationales composeandsolvecomplexreasoningtasks(Xia
andintrinsicknowledgefromsoftlabels. etal.,2024). Consequently,itallowedLMstoex-
We conduct extensive experiments on various celincomplexreasoningtasks(Kojimaetal.,2022;
typesofcomplexreasoningtasks,includingcom- Wangetal.,2023b;Zhangetal.,2023)whichtra-
monsense,arithmetic,logical,andsymbolicreason- ditionalfew-shotlearningmethodshavestruggled
ingtasks. Theexperimentalresultsclearlydemon- with (Raeet al., 2021). Recentworkstake a step
stratethesuperiorityofourmethodoverbaselines furthertoimproveCoTpromptingthroughenhanc-
leveraging knowledge only from LLMs. In addi- ing the quality of reasoning steps. Madaan et al.
tion, we verify that the mentor model can gener- (2023) had LMs to iteratively self-refine reason-
ateasubstantialnumberofcorrectreasoningsam- ingthroughself-feedback,whileGouetal.(2024)
plescomparedtootherLLMbaselines,highlight- leveraged external tools for obtaining feedback.
ing the effectiveness of our method as means of Trivedi et al. (2023); Zhao et al. (2023) incorpo-
dataaugmentation. Lastly,wedemonstratethatour ratedinformationretrievalsystemstoenhancethe
Mentor-KDsignificantlyimprovesstudentperfor- facticityofLMs’reasoning.
mances in low-resource scenarios, indicating its
Despitethesuccess,previousworks(Hoffmann
cost-efficiency. In summary, the contributions of
etal.,2022;Weietal.,2022b;Chuetal.,2024)re-
thispaperincludethefollowing:
portedthatthemeritsofreasoningonCoTprompt-
• We propose Mentor-KD, a novel reasoning ing emerge when LMs are scaled to hundreds of
distillation framework, which improves the billionsofparameters. Toaddresssuchproblems,
reasoning ability of small LMs considering our work focuses on enabling CoT reasoning to
thelimitationsofinsufficientdistillationsets small-scaledLMsthroughreasoningdistillation.
fromLLMteachers.
2.2 KnowledgeDistillationforLLMs
• Weintroduceamentormodeltoadditionally
KD(Hintonetal.,2015)hasbeenproventobea
generatebothrationalesamplesandsoftlabels
promisingapproachtocompressLMsbytransfer-
to complement the limited training datasets
ring the predictive behavior (e.g., soft labels) or
fromtheLLMteachers.
internal knowledge (e.g., hidden representations)
• We demonstrate that Mentor-KD improves fromlargerLMstosmallerones. However,existing
theeffectivenessofreasoningdistillationon KD methods for pre-trained LMs, which involve
studentswithvarioustypesofreasoningand distillingthesoftlabels(Sanhetal.,2019;Guetal.,
modelsthroughextensiveexperiments. 2024)orrepresentations(Wangetal.,2020,2021;Figure2: Ageneraloverviewofourproposedframework,Mentor-KD.Mentor-KDiscomposedofthreesteps.
First,CoTannotationsareinitiallycollectedfromtheteacherLLMandfiltered. Second,thepreservedannotations
areusedtotrainthementormodel,andthetrainedmentormodelaugmentsmulti-steprationales. Lastly,thestudent
modelistrainedonannotationsfromtheteacherandthestudent,aswellassoftlabelsfromthementormodel.
Kimetal.,2022),requireaccesstotheinternalpa- lation sets provided by LLMs, we posit that they
rameters of teachers. These requirements pose a may be insufficient and may undermine the stu-
significantchallengeforleveragingLLMsinKD, dent’s capabilities. In this sense, our work is dif-
regardingtheirblack-boxnatureandimpracticality. ferentinthatwecomplementsuchinsufficiencyof
LLMteachers.
Inturn,recentworkspracticedreasoningdistilla-
tion,whichenabledsmallerLMs(students)tocarry
3 Methodology
outmulti-stepreasoningsimilartoLLMsbyutiliz-
ingrationalesgeneratedbyLLMteachersinstead We elaborate on the detailed implementations of
ofsoftlabels. Forexample,Hoetal.(2023);Mag- our Mentor-KD. The core idea is to augment the
ister et al. (2023); Li et al. (2023) fine-tuned stu- distillationtrainingsetbyleveragingatask-specific
dentsonmulti-steprationalesthatLLMsgenerated. intermediate-sizedmentormodel. Tothisend,we
Similarly,Shridharetal.(2023)hadstudentslearn firstgenerateCoTannotationsfromLLMteacher
how to decompose a complex question through models(Section3.1). Wethenfine-tunethemen-
havingLLMstogeneratesub-problemstotheorig- tor model with the distillation set from the LLM
inalquestion. Wangetal.(2023c)iterativelyem- teacher, and the trained mentor model generates
ployedLLMstoprovidereal-timefeedbackspecif- additionaltrainingsets,includingbothrationales
ically tailored to the student’s generations. Kang andsoftlabels(Section3.2). Byaugmentingboth
etal.(2023);Zhaoetal.(2024)leveragedinforma- signalsfromthementor,wedistilltheknowledge
tion retrieval systems to enhance the facticity of tostudentmodels(Section3.3). Figure2illustrates
student’sreasoningonknowledge-intensivetasks. anoverviewofourframework.
Recently,Zhuetal.(2024a,b)incorporatedmulti-
steprationalesinacodeformatgeneratedfromthe 3.1 Chain-of-ThoughtAnnotations
LLMstoimprovethestudent’sarithmeticreason- WeusetheLLMtoobtainCoTannotationscom-
ing skills. Contemporaneous to our work, (Zhou posedofarationaleandafinalpredictiontoaques-
andAi,2024)alsoutilizedintermediate-sizedmod- tionviaZero-shot-CoT(Kojimaetal.,2022). Itis
elsforLLMdistillation. Ourworkdiffersinthat atwo-stagedstrategyconsistingofreasoningand
weuseintermediate-sizedmodelsforcomplement- answerextractionstages,andthus,weinducethe
ingtheteachermodel’sdistillationsignals,rather LLM to generate a CoT rationale first and subse-
thanforfilteringtheannotations. quentlyafinalpredictionafterwards.
Whilemostpreviousworkshavebeenconducted Specifically,wefirstappend“Let’sthinkstepby
toimprovereasoningdistillationbyutilizingdistil- step”tothequestionandprompttheLLMtoobtainthe rationale. In sequence, we prompt the LLM isdefinedasfollows:
againbyincorporatingthepreviouslyobtainedra-
tionale to induce its final prediction. Formally, D = D ∪D (1)
train teacher mentor
from a dataset D = {q ,y } where q denotes a
i i i
question and y denotes a golden label, our goal
i
3.3 ReasoningDistillation
is to induce the LLM to generate a step-by-step
rationale rt and a final prediction yˆt, given q as Fortrainingthestudentmodel,weincorporateboth
i i i
aninput. Thepromptingtemplatetakestheform fine-tuning(rationaledistillation)andknowledge
of: "Q: {q }. A: Let’s think step by step. {rt}. distillationthroughlogitvaluesobtainableviathe
i i
Therefore,theansweris{yˆt}". mentor model (soft label distillation). This is to
i
Afterward, we filter the annotations generated allow the student model to jointly 1) learn how
bytheLLM.Followingpreviousworks(Lietal., to practice step-by-step reasoning in a symbolic
2023; Magister et al., 2023; Fu et al., 2023; Lee manner(Hoetal.,2023;Lietal.,2023;Magister
etal.,2024),wepreserveannotationswherethefi- et al., 2023), as well as 2) mimic the predictive
nal prediction yˆt matches the golden answer y behaviorofalargermodel(Hintonetal.,2015). In
i i
of a sample. Then, the annotations are refor- correspondence,ourtrainingobjectiveconsistsof
matted into a question-label format for training twolossfunctions.
mentor and student models. More formally, for
all annotations i where yˆt = y , we reformat a RationaleDistillation. Identicaltotrainingthe
i i
data sample (q ,rt,yˆt,y ) into (q ,lt,y ), where mentor model, the step-by-step reasoning ability
i i i i i i i
lt takes the form of “{rt}. Therefore, the an- can be distilled through fine-tuning the student
i i
modelwithquestion-labelpairsobtainedfromthe
sweris{y }.” Consequently,wefinallyconstruct
i
D = {(q ,lt,y )}N . teacherandthementor. Morespecifically,theform
teacher i i i i=1
oflearningthemulti-stepreasoningabilitythrough
3.2 MentorModel fine-tuningisdefinedasfollows:
Here, we describe how our mentor models are
L = E logP ([q;r;y]), (2)
trained to concentrate their powers to a specific rd D train f
task, and utilized to complement the insufficient
wheref indicatesthestudentmodel,andthesquare
distillationsetsofLLMteachers.
bracketsindicatestringconcatenation.
Training. Fortrainingthementormodel,wedi-
Soft Label Distillation. Leveraging the LLM
rectly fine-tune it on the previously constructed
teacher’s internal knowledge can be impractical
D . Specifically, the mentor model receives
teacher
q as an input, lt as a label, and is trained with a due to its black-box nature or enormous size. In-
i i
stead, we employ our mentor model to provide
standardlanguagemodelingobjective.
thesoftlabelsfordistillation. Thesoftlabelsare
Rationale Augmentation. The trained mentor obtained through a forward pass, followed by a
modelisthenusedfortraindataaugmentation. For softmax function, given q as an input. Formally,
datasamplesfromD,weletthementormodelan- we obtain the soft label (probability distribution)
notatestep-by-steprationales,givenq i asaninput. p k ofthementorandstudentmodelsfromthelogit
The mentor in return generates a label lm, which valuez atthek-thpositionthroughthefollowing
i k
consistsofastep-by-steprationaleandaprediction equation:
ofitsown. Wefiltertheannotationsbythementor
identicaltofilteringtheteacher’sannotationsand exp(z k/τ)
p = , (3)
preserve data samples where yˆm = y . Through k (cid:80) exp(z /τ)
i i j j
thisstage,weconstructD = {(q ,lm,y )}N
mentor i i i i=1
perdataset. where τ indicates a temperature hyperparameter
With annotations obtained from the teacher forsofteningthedistribution. Afterobtainingprob-
(D )andthementor(D ),wefinallycon- ability distributions of the mentor (pm) and the
teacher mentor
structD fortrainingthestudentmodel2,which student(ps),weadopttheKullback-Leiblerdiver-
train
gencelosstominimizethedivergencebetweenthe
2Itisworthnotingthatwedonotdistinguishwherethe
twodistributions. Thisallowsthestudentmodelto
CoTannotationsweregeneratedfrom,butwerandomlysam-
pleinstancesfromD totrainthestudentmodels. mimicthepredictivebehaviorandlearntheinternal
trainModel #Params GSM8K ASDiv SVAMP CommonsenseQA
GPT-3.5-Turbo(teacher)* - 73.98 79.64 75.14 74.35
FlanT5-XXL(mentor) 11B 34.34 50.32 51.71 85.01
GPT-3-curie(Hoetal.,2023) 6.7B 6.75 - 12.67 56.76
T5-XXL(Magisteretal.,2023) 11B 21.99 42.12 - -
FlanT5-XL(Fuetal.,2023) 3B 22.40 28.40 23.80 -
FlanT5-XL(Vanilla-KD)* 3B 22.76 29.41 29.33 81.13
FlanT5-XL(MCC-KD)* 3B 24.28 31.35 30.00 82.88
FlanT5-XL(Mentor-KD(ours)) 3B 24.76 31.86 32.70 87.14
Table1: Comparisonwithdifferentbaselinesonarithmeticandcommonsensereasoningtasks. Thereportedresults
areaveragedaccuracyoverfourrunsusingrandomlyselectedseeds. Performancesmarkedwithanasterisk(*)were
excerptedfromMCC-KD(Chenetal.,2023). Thebestresultsarehighlightedinboldface.
knowledgeoflargermodels. Thetrainingobjective Language Models. We utilize gpt-3.5-turbo
forsoftlabeldistillationisdefinedasfollows: throughOpenAIAPIforourteachermodel. Forthe
(cid:88) pm mentorandstudentmodels,wemainlyuseFlanT5-
L (pm,ps) = pmlog k (4)
sld k ps XXL and FlanT5-XL (Chung et al., 2022) as our
k k
mentorandstudentmodels. Foradditionalanaly-
Joint Learning. Finally, we have the student
sis,weusevarioussizesofFlanT5andT5(Raffel
modeltojointlylearntheaforementionedtwoob-
etal.,2020),includinglarge,base,andsmall-sized
jectives. Thelossfunctionfortrainingthestudent
models.
modelisasfollows:
L = (1−λ)L +λL , (5) Chain-of-ThoughtAnnotations. ForGSM8K,
rd sld
ASDiv, SVAMP, and CommonsenseQA, we uti-
whereλisahyperparameterforinterpolatingthe
lizetheCoTannotationsprovidedby(Chenetal.,
twolossfunctions.
2023). TheannotationswerecollectedwithGPT-
4 Experiments 3.5-TurbousingZero-shot-CoTprompting,which
isidenticaltoourmethodologymentionedinSec-
Inthissection,wedescribetheexperimentdetails
tion3.1. Otherdatasetswerenewlypromptedand
andevaluateourMentor-KDonvariouscomplex
collectedbyourresearchinstitute.
reasoningtasks.
Baselines. Forthebaselines,weincorporatepre-
4.1 ExperimentSetup
vious methods of reasoning distillation. Specifi-
Tasks and Datasets. Following (Wei et al., cally,weimplementVanilla-KD,ageneralreason-
2022b; Kojima et al., 2022), we evaluate our ingdistillationmethodthatfine-tunesstudentmod-
Mentor-KDonfourcategoriesofcomplexreason- elsontheteachermodel’sgeneratedrationales(Ho
ingtasks,whicharecommonsense,arithmetic,logi- etal.,2023;Magisteretal.,2023),andMCC-KD,
cal,andsymbolicreasoning. Specifically,weadopt whichfurtheremphasizesdiversityandconsistency
up to three datasets per task in order to evaluate withinmultipleCoTrationales(Chenetal.,2023).
ourframeworkonvariousdatasetsofthesametask WealsocompareMentor-KD’sperformanceswith
type. DatasetsusedforthispaperareStrategyQA Fuetal.(2023),whichaimstospecializeLM’srea-
(Geva et al., 2021), CommonsenseQA (Talmor soningabilitytowardsaspecifictask. Wereportthe
etal.,2019)forcommonsensereasoning,GSM8K teachermodel’sperformancesviaZero-shot-CoT
(Cobbeetal.,2021),ASDiv(Miaoetal.,2020),and (ZS-CoT)prompting.
SVAMP(Pateletal.,2021)forarithmeticreason-
ing,TrackingShuffledObjects,DateUnderstand- Implementations. Weadoptmodelsprovidedby
ing(Srivastavaetal.,2023)forlogicalreasoning, HuggingFace(Wolfetal.,2020)ontwoNVIDIA
andLastLetterConcatenation(Weietal.,2022b; RTX A6000 GPUs. Specifically, we train mod-
Kojimaetal.,2022)forsymbolicreasoning. Fur- elsfor18epochsforXXL-/XL-sizedmodels, 10
therdetailsareprovidedinAppendixA. epochsforlarge,and20epochsforbase,andsmallCommonsense Arithmetic Logical Symbolic
Model #Params Method
SQA CSQA ASDiv SVAMP Shuffled Date LastLetter
GPT-3.5-Turbo - ZS-CoT(teacher) 58.07 74.35* 79.64* 75.14* 64.00 81.98 68.00
T5-large 780M Vanilla-KD(mentor) 63.32 68.80 12.42 13.05 90.22 84.68 68.00
Vanilla-KD 61.43 55.53 11.15 10.00 77.33 89.19 56.00
T5-base 250M MCC-KD 62.01 57.17 9.55 8.00 56.89 81.98 45.33
Mentor-KD(ours) 62.45 59.05 12.10 10.00 92.00 88.29 65.33
Vanilla-KD 55.60 42.75 5.10 6.67 39.11 81.98 48.67
T5-small 80M MCC-KD 56.77 38.25 5.73 7.33 38.22 77.48 28.67
Mentor-KD(ours) 57.93 45.37 7.01 8.67 79.56 87.39 56.67
Table2: Performancesofteacher,mentor,andstudentmodelsacrossfourdifferentcomplexreasoningtasks,where
thebackbonemodelisT5. GPT-3.5-Turboresultswithanasterisk(*)wereexcerptedfrom(Chenetal.,2023). The
bestandsecondbestresultsarehighlightedinboldfaceandunderline,respectively.
Commonsense Arithmetic Logical Symbolic
Model #Params Method
SQA CSQA ASDiv SVAMP Shuffled Date LastLetter
GPT-3.5-Turbo - ZS-CoT(teacher) 58.07 74.35* 79.64* 75.14* 64.00 81.98 68.00
FlanT5-large 780M Vanilla-KD(mentor) 64.48 79.36 20.70 14.00 90.22 88.29 65.33
Vanilla-KD 62.74 62.33 12.42 10.67 84.89 86.49 53.33
FlanT5-base 250M MCC-KD 64.92 68.47 13.69 12.00 69.78 85.59 46.00
Mentor-KD(ours) 65.21 67.24 15.29 11.33 93.78 87.39 65.33
Vanilla-KD 55.90 48.24 7.96 10.67 63.11 85.59 52.67
FlanT5-small 80M MCC-KD 58.37 45.21 7.01 10.00 43.11 81.98 35.33
Mentor-KD(ours) 59.97 48.98 10.83 10.67 82.67 83.78 58.67
Table3: Performancesofteacher,mentor,andstudentmodelsacrossfourdifferentcomplexreasoningtasks,where
thebackbonemodelisFlanT5. GPT-3.5-Turboresultswithanasterisk(*)wereexcerptedfrom(Chenetal.,2023).
Thebestandsecondbestresultsarehighlightedinboldfaceandunderline,respectively.
Model Method Shuffled LastLetter 4.2 MainResults
Mentor-KD(ours) 79.56 56.67
Forafaircomparison,wemainlycompareMentor-
T5 w/oRD 32.89 50.00
w/oSLD 76.00 52.00 KD utilizing FlanT5-XL models on three arith-
Mentor-KD(ours) 82.67 58.67 meticreasoningtasksandonecommonsenserea-
FlanT5 w/oRD 64.89 56.00 soningtask,whicharecommonlyusedinreason-
w/oSLD 82.22 54.00 ingdistillation(Hoetal.,2023;Chenetal.,2023).
ThemainresultsareprovidedinTable1. Weob-
Table 4: Ablation study of Mentor-KD on Tracking
servethatourMentor-KDachievesstate-of-the-art
ShuffledObjectsandLastLetterConcatenation.Weem-
ploylargemodelsofeachbackbonemodelasmentors performanceonfour differentreasoningdatasets.
andsmallmodelsasstudents. Specifically, our model achieves approximately
2.0% better performance on averaged accuracy
than MCC-KD, the previous SOTA model. The
modelsfollowingthepreviousworks(Chenetal.,
results demonstrate the effectiveness of Mentor-
2023; Ho et al., 2023). The maximum sequence
KDinaddressingchallengingcomplexreasoning
lengthissetto512throughoutallourexperiments,
tasks,includingbotharithmeticandcommonsense
and we sweep batch sizes in {2, 4, 6, 8}. To ac-
reasoning.
celeratetrainingandconservememoryusage,we
applymixedprecisionofbfloat16andLoRA(Hu
5 Analysis
etal.,2022)throughoutourmainexperimentsand
followtherelatedconfigurationsfrom(Chenetal., To delve into the benefits of our method, we per-
2023). Moreover, We use AdamW (Loshchilov formaseriesoffine-grainedanalyticalexperiments
andHutter,2019)optimizer,withalearningrateof withthefollowingresearchquestions(RQs):
{1e-4,2e-4,3e-4,5e-4}. Weapplythelossinterpo- • RQ1. CanMentor-KDbegeneralizedtothevari-
lationhyperparameterλto0.3,andthedistillation oussizesandtypesofstudentmodels? (§5.1)
temperatureτ to{1.0,2.0}. Wereporttheaverage • RQ2. HowdoeseachcomponentinMentor-KD
testaccuracyresultsfromfourrandomseeds. contributetoitsoverallperformance? (§5.2)StrategyQA ASDiv GPT­3.5­Turbo Vicuna­7B
Llama­3­8b­Instruct Mentor­0.7B (ours)
11 (a) Accuracy of Annotated Datasets on Teacher­incorrect Samples
60 10 60
40
9
58 20
1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 0 SQA CSQA Shuffled Date
Degree Degree Dataset
Tracking Shuffled Objects Last Letter Concatenation (b) Student Performances trained on Annotated Samples
90
60
60
80 40
58 20
70 0 SQA CSQA Shuffled Date
56 Dataset
1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9
Degree Degree Figure 4: Comparison of (a) accuracy of our mentor
model (FlanT5-large) and LLM baselines on teacher-
Figure 3: Performances by differentiating the degree
incorrectsamples,and(b)performancesofstudentmod-
(number)ofmentor-generatedCoTrationalesperques-
elstrainedwithaugmenteddistillationsetsfromLLM
tion. WeadoptFlanT5-largeandFlanT5-smallasmen-
baselinesandourmentormodels.
torandstudentmodels,respectively.
informativedistillationsignals.
• RQ3. Canthementormodelgenerateinforma-
tivedistillationsetsforstudents? (§5.3)
5.2 AblationStudies(RQ2)
• RQ4. DoesMentor-KDofferimprovementsun-
Weconductablationstudiestoexplorethecontri-
derlow-resourcescenarios? (§5.4)
butionsbroughtbyeachtechniqueofourmethod.
• RQ5. Doesthesizeofmentormodelsaffectthe
Specifically,wefocusontheeffectofrationaledis-
performanceofstudentmodels? (§5.5)
tillation(RD)andsoftlabeldistillation(SLD)from
thementormodel. Thedetailedresultsareshown
5.1 VariousStudentModels(RQ1)
inTable 4. WeobservethatomittingRDandSLD
TofurtherinvestigatethegeneralityofourMentor-
significantlyaffectsbothmodeltypesanddatasets.
KD,weconductexperimentsonvarioustypesof
These results emphasize the significance of RD
student models with different sizes. Notably, we
for both training samples and soft labels, which
furtherexpandourscopeofexperimentsbyaddi-
enhancetheinsufficientknowledgefromteachers.
tionallyincorporatinglogicalandsymbolicreason-
ingtasks. Specifically,weutilizeT5andFlanT5, 5.3 ImpactofDataAugmentation(RQ3)
whicharewidelyadoptedinLLMdistillationfol-
To further investigate the proposed data augmen-
lowingpreviousworks(Hoetal.,2023;Chenetal.,
tationmethodsofmentormodels,weadditionally
2023). We leverage large variants of T5 and
analyze the effectiveness in perspectives of both
FlanT5 as our mentor model, and {base, small}
quantityandquality.
variantsasourstudentmodel. Detailsonimplemen-
tationsofthissectionareelaboratedinAppendixB. Quantity of Augmented Dataset. We first an-
TheresultsareshowninTables2and3. Weob- alyze the impact of the number of generated dis-
servethatourMentor-KDconsistentlyoutperforms tillation sets from the mentor by diversifying the
the other baselines in four categories of complex numberofrationalesthatthementorproducesper
reasoningtasksonvariousstudentmodels. Inpar- question. TheresultsareshowninFigure3. Gener-
ticular,Mentor-KDhasshownlargeperformance ally,weobservethatstudentperformancesimprove
improvements in commonsense and logical rea- in line with the quantity of distillation sets. This
soning tasks, which the student model may even indicatesthatourmentormodelssuccessfullygen-
outperformtheperformancesoftheLLMteacher eraterationaleshelpfulforstudentmodelstolearn
(i.e.,GPT-3.5). Theseresultsdemonstratethatour multi-step reasoning. However, we also observe
task-specificmentormodelcansuccessfullycom- theperformanceusuallysaturatedoversixaugmen-
plementtheinsufficientLLMteacher’sknowledge, tationsandbeginstodeclinewhenmoredistillation
therebyleadingtoachievingbetterperformances setsareintroduced,whichmaybeduetothenoises
for various student models by transferring more generatedfrommodels(Liuetal.,2022).
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccATracking Shuffled Objects Last Letter Concatenation Tracking Shuffled Objects Last Letter Concatenation
60
80
50 58
80
60 40 56
30 70
Mentor­KD 54
40 20 Vanilla­KD Vanilla­KD Vanilla­KD
1020 40 60 80 100 1020 40 60 80 100 small base large XL small base large XL
Training set size (%) Training set size (%) Size of mentor model Size of mentor model
Figure5: ComparisonbetweenMentor-KD(Ours)and Figure6: Comparisonbetweenstudent(FlanT5-small)
Vanilla-KDbaselineonvariousdistillationsetsbydif- performanceusingdifferentmentormodelsconsidering
ferentiatingthepercentageofrationalesbeingused. variouscapacitygapsizes. Dottedlinesingrayindicate
Vanilla-KDbaselineperformances.
QualityofAugmentedDataset. Toinvestigate
thequalityofouraugmenteddistillationsets, we KDbaselinewithourMentor-KD,varyingthera-
compare our mentor models (i.e., FlanT5-large) tioofdistillationsetsgeneratedfromLLMteacher
with various LLMs that may be potential alter- models. TheresultsareshowninFigure5.
natives of mentors for augmentation (i.e. GPT-
WeobservethattheMentor-KDalsoallowsper-
3.5-Turbo3,Llama-3-8B-Instruct4,andVicuna-7B
formanceimprovementsforstudentmodelsinlow-
(Chiangetal.,2023)). Wefirstcomparetheaccu-
resourcescenarios,giventhatmentormodelspro-
racy of the augmentations mentors generate with
videinformativerationalesetsandsoftlabels. In
otherbaselines(throughZero-shot-CoTprompting)
particular, the Vanilla-KD baseline shows perfor-
onincorrectsamplespredictedbytheLLMteacher.
mance degradation on highly limited distillation
Wethenreporttheperformancesofthestudent(i.e.,
signals,whileourMentor-KDexhibitsrobustness
FlanT5-small)trainedoneachaugmentationtoan-
for limited datasets. These results demonstrate
alyze whether task-specific mentors can provide
that our mentor models can alleviate over-fitting
informativesetstothestudents.
problemsforstudentsfromthelimiteddistillation
The results are shown in Figure 4. While the
signals and can distill the LLM teacher’s knowl-
mentormodelsconsistofsmallerparametersthan
edge in a cost-efficient manner. We elaborate on
the LLMs (e.g., 10× smaller than Llama3-8B-
thisresearchquestioninAppendixC.
Instruct), they generate more accurate rationales
thanotherLLMbaselines,indicatingtheabilityto
5.5 EffectsofMentorSizes(RQ5)
providemorediverserationalesforstudentmodels.
In addition, we observe that the students trained TofurtherexploreMentor-KD’seffectivenessand
with distillation sets from mentor models indeed verifyourdesignchoice,weconductanadditional
achievehigherperformancethanthosetrainedwith experiment by differentiating the size of mentor
sets from LLM teachers. These results suggest models. Here, we employ FlanT5-small as a stu-
thatmentorscangeneratehigher-qualityrationales dentmodelandFlanT5-{XL,large,base,small}as
than LLM teachers. Overall results highlight the mentormodels. Fordistillingsmalltosmallmod-
superiority of task-specific fine-tuning of mentor els,weutilizeself-distillation,followingprevious
models. works(Allen-ZhuandLi,2023;Zhuetal.,2024a).
Figure6displaystheresults. Generally,weob-
5.4 Low-resourceScenarios(RQ4)
servethatthestudentmodelperformsbetterwhen
In reasoning distillation, collecting sufficiently largermentormodelsareincorporatedduringrea-
largedistillationsetscanbeprohibitivelyexpensive soningdistillation. Employingthesmallestmentor
due to the cost of API calls for black-box LLMs. results in a performance decline, but we observe
Therefore,weexaminetheeffectivenessofMentor- suchscenariosstilloutperformthebaselinesinTa-
KDonlow-resourcescenarios,wheredistillation ble 3. The results suggest that employing larger
setsarecollectedforonlyaproportionoftheorigi- modelsofbetterperformancescontributestoboost-
naldatasets. Specifically,wecomparetheVanilla- ingthesmallstudentmodels’performances,which
isalignedwithpreviousfindingsthatstudentper-
3WeadoptadifferentseedvaluefromtheinitialCoTan-
formances are correlated to their corresponding
notationphase(Section3.1)forthisexperiment.
4https://ai.meta.com/blog/meta-llama-3/ model’sperformances(Hoetal.,2023).
ycaruccA ycaruccA ycaruccA ycaruccA6 Conclusion fewerthan3billionparametersasthestudentmod-
els. Therefore,theapplicabilityofourframework
WehavepresentedMentor-KD,anovelframework
todecoder-onlymodelsremainsunder-exploredin
to transfer reasoning capabilities from LLMs to
thiswork. Nevertheless,basedonrecentevidence
smaller LMs. To this end, we have introduced a
suggestingthatreasoningdistillationcanbeeffec-
mentor model, a novel auxiliary model, for com-
tivelygeneralizedtovariousarchitectures(Hoetal.,
plementingthedistillationsetsfromLLMsbyaug-
2023; Chen et al., 2023; Wang et al., 2023c), we
menting multi-step rationales and providing soft
believethatMentor-KDisexpectedtodisplayper-
labelsforthestudentmodel. Throughextensiveex-
formanceboostsondecoder-basedstudentmodels
periments,wehavedemonstratedthatourMentor-
aswell.
KDsignificantlyimprovestheeffectivenessofrea-
soningdistillation. Specifically,ourstudentmodels Acknowledgements
outperformexistingreasoningdistillationbaselines
ThisworkwassupportedbytheNationalResearch
withvarioussizesandtypesofmodelsoncomplex
FoundationofKorea(NRF)grantfundedbytheKo-
reasoningtasks. Furthermore,wehaveverifiedthat
rea government (MSIT) (No.RS-2024-00415812
ourmentormodelcangenerateeffectivereasoning
andNo.2021R1A2C3010430)andInstituteofIn-
samplesandsoftlabelsfortrainingstudentmodels,
formation & communications Technology Plan-
resultinginconsistentperformanceimprovements.
ning&Evaluation(IITP)grantfundedbytheKo-
7 Limitations reagovernment(MSIT)(No.RS-2024-00439328,
Karma: Towards Knowledge Augmentation for
WhilewehavedemonstratedthatMentor-KDeffec- Complex Reasoning (SW Starlab), No.RS-2024-
tivelyimprovesthereasoningabilityofsmalllan- 00457882,AIResearchHubProject,andNo.RS-
guagemodelsbyaugmentingbothtrainingsetsand 2019-II190079, Artificial Intelligence Graduate
softlabels,therearesomelimitationsthatpresent SchoolProgram(KoreaUniversity)).
promisingavenuesforfutureresearch.
TrainingCostsforMentorModels. Ourframe- References
work requires additional computational costs for
Zeyuan Allen-Zhu and Yuanzhi Li. 2023. Towards
training mentor models for reasoning distillation.
understandingensemble,knowledgedistillationand
Besides the training costs in the distillation pro- self-distillationindeeplearning. InProceedingsof
cess, this study mainly focuses on improving the theInternationalConferenceonLearningRepresen-
inference efficiency of small student models, as tations(ICLR).OpenReview.net.
withmostreasoningdistillationresearch(Hoetal.,
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
2023;Chenetal.,2023;Wangetal.,2023a). We Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind
furtherelaborateonthisissueinAppendixC. Neelakantan,PranavShyam,GirishSastry,Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
ExplorationonDifferentReasoningStrategies. Gretchen Krueger, Tom Henighan, Rewon Child,
AdityaRamesh,DanielZiegler,JeffreyWu,Clemens
While we successfully demonstrate the perfor-
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
mance improvements in CoT reasoning abilities
teusz Litwin, Scott Gray, Benjamin Chess, Jack
forsmalllanguagemodels,itisanopenquestion Clark, ChristopherBerner, SamMcCandlish, Alec
whether our framework can be applied to other Radford, Ilya Sutskever, and Dario Amodei. 2020.
Languagemodelsarefew-shotlearners. InProceed-
types of reasoning strategies, such as program-
ingsoftheAdvancesinNeuralInformationProcess-
guided reasoning (Zhu et al., 2024a), retrieval-
ingSystems(NeurIPS),volume33,pages1877–1901.
based reasoning (Kang et al., 2023; Zhao et al., CurranAssociates,Inc.
2024), and reasoning based on contextualized,
HongzhanChen,SiyueWu,XiaojunQuan,RuiWang,
structuredknowledge(Parketal.,2024). Weleave
Ming Yan, and Ji Zhang. 2023. MCC-KD: Multi-
theexplorationofdistillationforvarioustypesof CoTconsistentknowledgedistillation. InFindings
reasoningstrategiesasafutureresearchdirection of the Association for Computational Linguistics:
inthisfield. EMNLP 2023, pages 6805–6820. Association for
ComputationalLinguistics.
Exploration on Different Architectures. We
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
have verified the effectiveness of our framework
ZhanghaoWu,HaoZhang,LianminZheng,Siyuan
onencoder-decodermodels(e.g.,FlanT5,T5)with Zhuang,YonghaoZhuang,JosephE.Gonzalez,IonStoica, and Eric P. Xing. 2023. Vicuna: An open- modelstowardsmulti-stepreasoning. InProceedings
sourcechatbotimpressinggpt-4with90%*chatgpt oftheInternationalConferenceonMachineLearning
quality. (ICML),volume202,pages10421–10430.PMLR.
AakankshaChowdhery,SharanNarang,JacobDevlin, MorGeva,DanielKhashabi,EladSegal,TusharKhot,
Maarten Bosma, Gaurav Mishra, Adam Roberts, DanRoth,andJonathanBerant.2021. Didaristotle
Paul Barham, Hyung Won Chung, Charles Sutton, usealaptop? aquestionansweringbenchmarkwith
Sebastian Gehrmann, Parker Schuh, Kensen Shi, implicit reasoning strategies. Transactions of the
Sasha Tsvyashchenko, Joshua Maynez, Abhishek Association for Computational Linguistics, 9:346–
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin- 361.
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob ZhibinGou,ZhihongShao,YeyunGong,yelongshen,
Austin,MichaelIsard,GuyGur-Ari,PengchengYin, Yujiu Yang, Nan Duan, and Weizhu Chen. 2024.
Toju Duke, Anselm Levskaya, Sanjay Ghemawat, CRITIC: Large language models can self-correct
Sunipa Dev, Henryk Michalewski, Xavier Garcia, with tool-interactive critiquing. In Proceedings of
VedantMisra,KevinRobinson,LiamFedus,Denny theInternationalConferenceonLearningRepresen-
Zhou,DaphneIppolito,DavidLuan,HyeontaekLim, tations(ICLR).OpenReview.net.
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
YuxianGu,LiDong,FuruWei,andMinlieHuang.2024.
DavidDohan,ShivaniAgrawal,MarkOmernick,An-
Minillm: Knowledgedistillationoflargelanguage
drew M. Dai, Thanumalayan Sankaranarayana Pil-
models. InProceedingsoftheInternationalConfer-
lai,MariePellat,AitorLewkowycz,EricaMoreira,
enceonLearningRepresentations(ICLR).OpenRe-
Rewon Child, Oleksandr Polozov, Katherine Lee,
view.net.
ZongweiZhou,XuezhiWang,BrennanSaeta,Mark
Diaz,OrhanFirat,MicheleCatasta,JasonWei,Kathy
Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.
Meier-Hellstern,DouglasEck,JeffDean,SlavPetrov,
2015. Distillingtheknowledgeinaneuralnetwork.
andNoahFiedel.2023. Palm:Scalinglanguagemod-
CoRR.
elingwithpathways. JournalofMachineLearning
Research,24:240:1–240:113. NamgyuHo,LauraSchmid,andSe-YoungYun.2023.
Large language models are reasoning teachers. In
ZhengChu,JingchangChen,QianglongChen,Weijiang
ProceedingsoftheAnnualMeetingoftheAssociation
Yu,TaoHe,HaotianWang,WeihuaPeng,MingLiu,
forComputationalLinguistics(ACL),pages14852–
BingQin,andTingLiu.2024. Navigatethroughenig-
14882.AssociationforComputationalLinguistics.
maticlabyrinthasurveyofchainofthoughtreason-
ing: Advances,frontiersandfuture. InProceedings
JordanHoffmann,SebastianBorgeaud,ArthurMensch,
of the 62nd Annual Meeting of the Association for
Elena Buchatskaya, Trevor Cai, Eliza Rutherford,
ComputationalLinguistics(Volume1: LongPapers),
DiegodelasCasas,LisaAnneHendricks,Johannes
pages 1173–1203. Association for Computational
Welbl, Aidan Clark, Tom Hennigan, Eric Noland,
Linguistics.
KatherineMillican,GeorgevandenDriessche,Bog-
dan Damoc, Aurelia Guy, Simon Osindero, Karen
HyungWonChung,LeHou,ShayneLongpre,Barret
Simonyan,ErichElsen,OriolVinyals,JackWilliam
Zoph,YiTay,WilliamFedus,EricLi,XuezhiWang,
Rae, andLaurentSifre.2022. Anempiricalanaly-
MostafaDehghani,SiddharthaBrahma,AlbertWeb-
sisofcompute-optimallargelanguagemodeltrain-
son, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-
ing. InProceedingsoftheAdvancesinNeuralInfor-
gun,XinyunChen,AakankshaChowdhery,Sharan
mationProcessingSystems(NeurIPS).CurranAsso-
Narang,GauravMishra,AdamsYu,VincentY.Zhao,
ciates,Inc.
YanpingHuang,AndrewM.Dai,HongkunYu,Slav
Petrov, EdH.Chi, JeffDean, JacobDevlin, Adam
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Roberts, DennyZhou, QuocV.Le, andJasonWei.
Allen-Zhu,YuanzhiLi,SheanWang,LuWang,and
2022. Scalinginstruction-finetunedlanguagemodels.
Weizhu Chen. 2022. LoRA: Low-rank adaptation
CoRR.
oflargelanguagemodels. InProceedingsoftheIn-
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, ternationalConferenceonLearningRepresentations
MarkChen,HeewooJun,LukaszKaiser,Matthias (ICLR).OpenReview.net.
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Minki Kang, Seanie Lee, Jinheon Baek, Kenji
Nakano, Christopher Hesse, and John Schulman.
Kawaguchi,andSungJuHwang.2023. Knowledge-
2021. Training verifiers to solve math word prob-
augmentedreasoningdistillationforsmalllanguage
lems. CoRR.
modelsinknowledge-intensivetasks. InProceedings
Dujian Ding, Ankur Mallick, Chi Wang, Robert Sim, of the Advances in Neural Information Processing
SubhabrataMukherjee,VictorRühle,LaksV.S.Lak- Systems(NeurIPS).CurranAssociates,Inc.
shmanan,andAhmedHassanAwadallah.2024. Hy-
Junho Kim, Jun-Hyung Park, Mingyu Lee, Wing-
brid LLM: Cost-efficient and quality-aware query
Lam Mok, Joon-Young Choi, and SangKeun Lee.
routing. InICLR.OpenReview.net.
2022. Tutoringhelpsstudentslearnbetter: Improv-
Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and ingknowledgedistillationforBERTwithtutornet-
Tushar Khot. 2023. Specializing smaller language work. InProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP), Jun-Hyung Park, Mingyu Lee, Junho Kim, and
pages 7371–7382. Association for Computational SangKeunLee.2024. Coconut: Contextualizedcom-
Linguistics. monsenseunifiedtransformersforgraph-basedcom-
monsenseaugmentationoflanguagemodels. InFind-
Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, ingsoftheAssociationforComputationalLinguistics
YutakaMatsuo,andYusukeIwasawa.2022. Large ACL2024,pages5815–5830.AssociationforCom-
language models are zero-shot reasoners. In Pro- putationalLinguistics.
ceedingsoftheAdvancesinNeuralInformationPro-
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
cessingSystems(NeurIPS),volume35,pages22199–
2021. AreNLPmodelsreallyabletosolvesimple
22213.CurranAssociates,Inc.
mathwordproblems? InProceedingsoftheNorth
AmericanChapteroftheAssociationforComputa-
JooyoungLee,FanYang,ThanhTran,QianHu,Emre
tionalLinguistics: HumanLanguageTechnologies
Barut, and Kai-Wei Chang. 2024. Can small lan-
(NAACL-HLT), pages 2080–2094. Association for
guagemodelshelplargelanguagemodelsreasonbet-
ComputationalLinguistics.
ter?: Lm-guidedchain-of-thought. InProceedingsof
the2024JointInternationalConferenceonComputa-
Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie
tional Linguistics, Language Resources and Eval-
Millican,JordanHoffmann,H.FrancisSong,John
uation (LREC-COLING 2024), pages 2835–2843.
Aslanides, Sarah Henderson, Roman Ring, Susan-
ELRAandICCL.
nah Young, Eliza Rutherford, Tom Hennigan, Ja-
cobMenick,AlbinCassirer,RichardPowell,George
Liunian Harold Li, Jack Hessel, Youngjae Yu, Xiang
van den Driessche, Lisa Anne Hendricks, Mari-
Ren, Kai-WeiChang, andYejinChoi.2023. Sym-
beth Rauh, Po-Sen Huang, Amelia Glaese, Jo-
bolicchain-of-thoughtdistillation: Smallmodelscan
hannes Welbl, Sumanth Dathathri, Saffron Huang,
also“think”step-by-step. InProceedingsoftheAn-
JonathanUesato,JohnMellor,IrinaHiggins,Antonia
nualMeetingoftheAssociationforComputational
Creswell,NatMcAleese,AmyWu,ErichElsen,Sid-
Linguistics(ACL),pages2665–2679.Associationfor
dhantM.Jayakumar,ElenaBuchatskaya,DavidBud-
ComputationalLinguistics.
den,EsmeSutherland,KarenSimonyan,MichelaPa-
ganini,LaurentSifre,LenaMartens,XiangLorraine
JiachengLiu,AlisaLiu,XimingLu,SeanWelleck,Pe-
Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena
terWest,RonanLeBras,YejinChoi,andHannaneh
Gribovskaya,DomenicDonato,AngelikiLazaridou,
Hajishirzi.2022. Generatedknowledgeprompting
ArthurMensch,Jean-BaptisteLespiau,MariaTsim-
forcommonsensereasoning. InProceedingsofthe
poukelli,NikolaiGrigorev,DougFritz,ThibaultSot-
AnnualMeetingoftheAssociationforComputational
tiaux,MantasPajarskas,TobyPohlen,ZhitaoGong,
Linguistics(ACL),pages3154–3169.Associationfor
DanielToyama,CypriendeMassond’Autume,Yujia
ComputationalLinguistics.
Li,TayfunTerzi,VladimirMikulik,IgorBabuschkin,
AidanClark,DiegodeLasCasas,AureliaGuy,Chris
Ilya Loshchilov and Frank Hutter. 2019. Decoupled
Jones,JamesBradbury,MatthewJ.Johnson,BlakeA.
weightdecayregularization. InProceedingsofthe
Hechtman,LauraWeidinger,IasonGabriel,William
International Conference on Learning Representa-
Isaac, Edward Lockhart, Simon Osindero, Laura
tions(ICLR).OpenReview.net.
Rimell,ChrisDyer,OriolVinyals,KareemAyoub,
JeffStanway,LorrayneBennett,DemisHassabis,Ko-
AmanMadaan, NiketTandon,PrakharGupta,Skyler
rayKavukcuoglu,andGeoffreyIrving.2021. Scaling
Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,
languagemodels: Methods,analysis&insightsfrom
Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
traininggopher. CoRR.
Shashank Gupta, Bodhisattwa Prasad Majumder,
Katherine Hermann, Sean Welleck, Amir Yazdan- ColinRaffel,NoamShazeer,AdamRoberts,Katherine
bakhsh, and Peter Clark. 2023. Self-refine: Itera- Lee,SharanNarang,MichaelMatena,YanqiZhou,
tiverefinementwithself-feedback. InAdvancesin WeiLi,andPeterJ.Liu.2020. Exploringthelimits
NeuralInformationProcessingSystems,volume36, oftransferlearningwithaunifiedtext-to-texttrans-
pages46534–46594.CurranAssociates,Inc. former. JournalofMachineLearningResearch,21:1–
67.
Lucie Charlotte Magister, Jonathan Mallinson, Jakub
Victor Sanh, Lysandre Debut, Julien Chaumond, and
Adamek, Eric Malmi, and Aliaksei Severyn. 2023.
ThomasWolf.2019. Distilbert,adistilledversionof
Teachingsmalllanguagemodelstoreason. InPro-
BERT:smaller,faster,cheaperandlighter. CoRR.
ceedingsoftheAnnualMeetingoftheAssociationfor
ComputationalLinguistics(ACL),pages1773–1781.
Kumar Shridhar, Alessandro Stolfo, and Mrinmaya
AssociationforComputationalLinguistics.
Sachan.2023. Distillingreasoningcapabilitiesinto
smaller language models. In Findings of the Asso-
Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.
ciation for Computational Linguistics: ACL 2023,
2020. A diverse corpus for evaluating and devel-
pages7059–7073,Toronto,Canada.Associationfor
opingEnglishmathwordproblemsolvers. InPro-
ComputationalLinguistics.
ceedings of the Annual Meeting of the Association
forComputationalLinguistics(ACL),pages975–984. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
AssociationforComputationalLinguistics. AbuAwalMdShoeb,AbubakarAbid,AdamFisch,AdamR.Brown,AdamSantoro,AdityaGupta,Adrià learningfromlargelanguagemodel. InProceedings
Garriga-Alonso,andetal.2023. Beyondtheimita- oftheConferenceonEmpiricalMethodsinNatural
tiongame:Quantifyingandextrapolatingthecapabil- LanguageProcessing(EMNLP),pages1948–1966.
itiesoflanguagemodels. TransactionsonMachine AssociationforComputationalLinguistics.
LearningResearch.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
JonathanBerant.2019. CommonsenseQA:Aques- MaartenBosma,DennyZhou,DonaldMetzler,EdH.
tion answering challenge targeting commonsense Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy
knowledge. InProceedingsoftheNorthAmerican Liang,JeffDean,andWilliamFedus.2022a. Emer-
Chapter of the Association for Computational Lin- gentabilitiesoflargelanguagemodels. Transactions
guistics: Human Language Technologies (NAACL- onMachineLearningResearch.
HLT),pages4149–4158.AssociationforComputa-
tionalLinguistics. JasonWei,XuezhiWang,DaleSchuurmans,Maarten
Bosma,brianichter,FeiXia,EdChi,QuocVLe,and
HarshTrivedi,NiranjanBalasubramanian,TusharKhot, Denny Zhou. 2022b. Chain-of-thought prompting
andAshishSabharwal.2023. Interleavingretrieval elicitsreasoninginlargelanguagemodels. InPro-
with chain-of-thought reasoning for knowledge- ceedingsoftheAdvancesinNeuralInformationPro-
intensive multi-step questions. In Proceedings of cessingSystems(NeurIPS),volume35,pages24824–
the61stAnnualMeetingoftheAssociationforCom- 24837.CurranAssociates,Inc.
putational Linguistics (Volume 1: Long Papers),
pages10014–10037,Toronto,Canada.Association Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
forComputationalLinguistics. Chaumond,ClementDelangue,AnthonyMoi,Pier-
ricCistac,TimRault,RemiLouf,MorganFuntowicz,
Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, JoeDavison,SamShleifer,PatrickvonPlaten,Clara
Yu Zheng, Jiachen Liu, Zhongnan Qu, Shen Yan, Ma, YacineJernite, JulienPlu, CanwenXu, Teven
YiZhu, QuanluZhang, MosharafChowdhury, and LeScao,SylvainGugger,MariamaDrame,Quentin
MiZhang.2024. Efficientlargelanguagemodels: A Lhoest, and Alexander Rush. 2020. Transformers:
survey. TransactionsonMachineLearningResearch. State-of-the-artnaturallanguageprocessing. InPro-
ceedings of the Conference on Empirical Methods
PeifengWang,ZhengyangWang,ZhengLi,YifanGao, in Natural Language Processing: System Demon-
Bing Yin, and Xiang Ren. 2023a. SCOTT: Self- strations (EMNLP), pages 38–45. Association for
consistentchain-of-thoughtdistillation. InProceed- ComputationalLinguistics.
ingsofthe61stAnnualMeetingoftheAssociationfor
ComputationalLinguistics(Volume1: LongPapers), YuXia,RuiWang,XuLiu,MingyanLi,TongYu,Xi-
pages 5546–5558. Association for Computational ang Chen, Julian J. McAuley, and Shuai Li. 2024.
Linguistics. Beyond chain-of-thought: A survey of chain-of-x
paradigmsforllms. CoRR.
WenhuiWang,HangboBao,ShaohanHuang,LiDong,
and Furu Wei. 2021. Minilmv2: Multi-head self- Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex
attention relation distillation for compressing pre- Smola.2023. Automaticchainofthoughtprompting
trainedtransformers. InFindingsoftheAssociation inlargelanguagemodels. InProceedingsoftheIn-
for Computational Linguistics: ACL 2021, pages ternationalConferenceonLearningRepresentations
2140–2151.AssociationforComputationalLinguis- (ICLR).OpenReview.net.
tics.
Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei
WenhuiWang,FuruWei,LiDong,HangboBao,Nan Qin, and Lidong Bing. 2023. Verify-and-edit: A
Yang, and Ming Zhou. 2020. Minilm: Deep self- knowledge-enhanced chain-of-thought framework.
attentiondistillationfortask-agnosticcompression In Proceedings of the 61st Annual Meeting of the
of pre-trained transformers. In Proceedings of the AssociationforComputationalLinguistics(Volume
AdvancesinNeuralInformationProcessingSystems 1: LongPapers),pages5823–5840.Associationfor
(NeurIPS).CurranAssociates,Inc. ComputationalLinguistics.
XuezhiWang, JasonWei, DaleSchuurmans, QuocV Yichun Zhao, Shuheng Zhou, and Huijia Zhu. 2024.
Le,EdH.Chi,SharanNarang,AakankshaChowd- Probe then retrieve and reason: Distilling probing
hery, and Denny Zhou. 2023b. Self-consistency andreasoningcapabilitiesintosmallerlanguagemod-
improves chain of thought reasoning in language els. InProceedingsofthe2024JointInternational
models. In Proceedings of the International Con- ConferenceonComputationalLinguistics,Language
ferenceonLearningRepresentations(ICLR).Open- Resources and Evaluation (LREC-COLING 2024),
Review.net. pages13026–13032.ELRAandICCL.
ZhaoyangWang,ShaohanHuang,YuxuanLiu,Jiahai Yuhang Zhou and Wei Ai. 2024. Teaching-assistant-
Wang,MinghuiSong,ZihanZhang,HaizhenHuang, in-the-loop: Improvingknowledgedistillationfrom
FuruWei, WeiweiDeng, FengSun, andQiZhang. imperfect teacher models in low-budget scenarios.
2023c. Democratizing reasoning ability: Tailored In Findings of the Association for ComputationalLinguisticsACL2024,pages265–282.Association
forComputationalLinguistics.
XuekaiZhu,BiqingQi,KaiyanZhang,XinweiLong,
Zhouhan Lin, and Bowen Zhou. 2024a. PaD:
Program-aided distillation can teach small models
reasoning better than chain-of-thought fine-tuning.
InProceedingsofthe2024ConferenceoftheNorth
AmericanChapteroftheAssociationforComputa-
tionalLinguistics: HumanLanguageTechnologies
(Volume1: LongPapers),pages2571–2597.Associ-
ationforComputationalLinguistics.
Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weip-
ingWang.2024b. Distillingmathematicalreasoning
capabilitiesintosmalllanguagemodels. NeuralNet-
works,179:106594.Appendix
Tracking Shuffled Objects Last Letter Concatenation
85
62.5
80
60.0
A DatasetStatistics 75
57.5
70
We provide the statistics of the datasets imple- 55.0
65
52.5
mented in our study in Table 6, including their 0.00.1 0.3 0.5 0.7 0.91.0 0.00.1 0.3 0.5 0.7 0.91.0
originallicenses. Wefollowthetrain-testdataset
splitsforGSM8K,ASDiv,SVAMP,andCommon- Figure7: Effectsofsoftlabeldistillation, byvarying
thevalueoflossinterpolationhyperparameter(λ).
senseQAfrom(Chenetal.,2023). ForStrategyQA,
Tracking Shuffled Objects, Date Understanding,
andLastLetterConcatenation,wefollowthetrain-
limitationssection,itmaybearguedthatMentor-
testdatasetsplitsfrom(Hoetal.,2023).
KDrequiresextracomputationalcostsfortraining
Meanwhile,inpractice,weutilizeCoTannota-
thementormodels.
tionsfrom(Chenetal.,2023)forGSM8K,ASDiv,
However,consideringthatMentor-KDachieves
SVAMP,CommonsenseQA,andnewlypromptthe
comparableperformancewithsmallerdistillation
LLM for other datasets. For other datasets, we
setsfromLLMteachers,wesuggestthatMentor-
prompttheLLMofsixCoTannotationsperques-
KD might be more efficient for training the stu-
tion. Furthermore,wereportinTable7thenumber
dentmodelsthanthebaselines. Thisisespecially
ofCoTrationalesaugmented(thesizeofD )
mentor
significant, in regard to the substantial inference
byourmentormodel(FlanT5-large)thathasbeen
costofLLMs(teachermodels)(Dingetal.,2024;
usedinexperimentsofSection5.1.
Wan et al., 2024). Specifically, Table 5 shows
B ImplementationDetailsonVarious that Mentor-KD works on par, or even exceeds
StudentModels theVanilla-KDbaselinetrainedon100%ofthedis-
tillationsetsfromtheLLMteacherwhileutilizing
For experiments on models smaller than 1B, we
only40%ofthem(Moredetailedresultsareshown
useT5andFlanT5asourbackbonemodelswithan
in Figure 5). This indicates the potential to save
AdamWoptimizer. Weconductahyperparameter
theinferencecostofgenerating60%ofthedistilla-
searchonτ of{1.0, 1.5, 2.0}, λof{0.1, 0.2, 0.3,
tionsetsbytheLLMteacher. TakingtheentireKD
0.4},andlearningrateof{1e-4,2e-4,3e-4,4e-4,
pipeline into account, Mentor-KD may train the
5e-4},andreportthebesttestaccuracyperepoch.
student more efficiently depending on the design
Meanwhile,forlabelsofthequestion-labelpairs,
choices,suchasthesizeofthementormodelsand
weadoptthetemplate“{r }. −−>{y }.” forsav-
i i thenumberofdistillationsetsfromtheLLM.
ingtokenizationspacesfollowing(Hoetal.,2023).
For experiments on the Vanilla-KD baseline and
D EffectsofSoftLabelDistillation
ourMentor-KD,werandomlyselectthreeoutofsix
CoTannotationsperquestion. Moreover,wehave
Inthissection,weexaminetheeffectsofsoftlabel
thementormodelgeneratethreeCoTrationalesper
distillationinMentor-KD,throughdifferentiating
questionforaugmentation.
the loss interpolation hyperparamter (λ) in Equa-
C AdditionalCostsforMentorModels tion5. Wediverselysetthevalueofλfrom0(no
softlabels)to1(onlysoftlabels)inthisexperiment,
and set the student model to FlanT5-small using
Method TrainSet Shuffled LastLetter
tworeasoningtasks.
Vanilla-KD 100% 63.11 52.67
TheresultsareshowninFigure7. Weinitially
100% 82.67 58.67
Mentor-KD
observethatthestudentmodel’sperformancesare
80% 82.22 56.00
(ours)
the lowest when no soft labels are introduced to
40% 67.11 52.67
reasoning distillation. However, we also observe
Table5: ComparisonbetweenVanilla-KDandMentor- thatintroducingsoftlabelssignificantlycontribute
KD(ours)withdifferenttrainingsetratios. toperformanceboostsofthestudent,implyingthat
the soft labels which mentor models provide are
Althoughourstudymainlyspotlightstheinfer- beneficialtostudentmodelscarryingoutmulti-step
enceefficiencyofsmallLMsasmentionedinthe reasoning.
ycaruccA ycaruccADataset Choices #TrainData #TestData License References
StrategyQA 2 1603 687 Apache-2.0 Gevaetal.2021
CommonsenseQA 5 8520 1221 Unspecified Talmoretal.2019
ASDiv - 1462 314 CCBY-NC4.0 Miaoetal.2020
SVAMP - 700 150 MIT Pateletal.2021
GSM8K - 7473 659 MIT Cobbeetal.,2021
TrackingShuffledObjects 3 525 225 Apache-2.0 Srivastavaetal.2023
DateUnderstanding 5-6 258 111 Apache-2.0 Srivastavaetal.2023
LastLetterConcatenation - 350 150 Unspecified Weietal.2022b;Kojimaetal.2022
Table6: Statisticsofdatasetsusedinourstudy.
Dataset #TrainData #AugData
StrategyQA 1603 4396
CommonsenseQA 8520 25413
ASDiv 1462 2667
SVAMP 700 1558
TrackingShuffledObjects 525 1392
DateUnderstanding 258 763
LastLetterConcatenation 350 1029
Table7: Statisticsofaugmentedsamplesbyourmentor
model(FlanT5-large)perdataset. Here,threeCoTratio-
nalesperquestionaregeneratedbyourmentormodels
foraugmentation,followedbyafilteringprocess.
E APIUsage
AsmentionedinSection4.1,weemployGPT-3.5-
Turboasourteachermodelthroughoutallexperi-
ments. Access to the model was provided by the
OpenAIAPI.Wesetthegenerationtemperatureto
0.7,followingpreviousworks(Wangetal.,2023b;
Hoetal.,2023). Ourtotalexpenditureforcollect-
ingCoTannotationswas$240.5USD.
F CaseStudy
InTable8,weprovidesomeexamplesofhowour
mentor model can successfully augment teacher-
incorrect samples on four datasets. Here, we em-
ploy GPT-3.5-Turbo as our teacher model, and
FlanT5-large as our mentor model. We observe
in commonsense reasoning tasks, there are cases
wheretheteachermodeldoesnotanswertheques-
tionfaithfullyorfailstonarrowdownitsfinalpre-
diction to a single choice, in contrast to the ra-
tionales that the trained mentor model generates.
Meanwhile for symbolic reasoning (Last Letter
Concatenation), we observe instances where the
teacher model makes a final prediction inconsis-
tentofitsmulti-steprationale,oncontrarywiththe
mentormodel.(a)StrategyQA
Question WouldJesusunderstandtheEasterBunny?
Answer No
Teacher 1. Jesuslivedover2,000yearsagointheMiddleEast,soitishighlyunlikelythathewouldhaveencounteredthe
EasterBunnyasitisknowninmodernWesternculture.2.TheEasterBunnyisasymbolofEasterthathasitsorigins
inGermanfolkloreandwasbroughttoAmericabyGermanimmigrantsinthe18thcentury.ItisnotapartofChristian
religioustradition.3.Jesuspreachedaboutlove,forgiveness,andcompassion,soitispossiblethathewouldunderstand
andappreciatetheideaoftheEasterBunnyasasymbolofjoy,renewal,andnewlife.4.Ultimately,whetherornot
JesuswouldunderstandtheEasterBunnyisamatterofpersonalinterpretationandbelief.SomemayarguethatJesus
wouldnothaveaneedtounderstandoracknowledgetheEasterBunny,ashisteachingsfocusonspiritualmattersrather
thanseculartraditions.OthersmayseetheEasterBunnyasaharmlessandfunpartofEastercelebrations.Therefore,
✗
theanswerisitdepends.
Mentor First,weneedtoconsiderthehistoricalcontextofEasterBunny. EasterBunnyisamythicalEasterbunnythatis
associatedwiththeEasterholiday.JesuswasahistoricalfigurewholivedduringthetimeoftheRomanEmpire,during
thetimeoftheRomanconquests.JesuswasamanwholivedinthefirstcenturyAD,duringthetimeoftheRoman
✓
Empire.Therefore,itishighlyunlikelythatJesuswouldhaveunderstoodtheEasterBunny.−−>No
(b)CommonsenseQA
Question Whatdoessomeoneusuallyproducealotofwhengettingdivorce?AnswerChoices:A.freedomB.tearsC.richerD.
depressionE.tissues
Answer (B)
Teacher First,whengettingadivorce,someonemayproducealotoftearsduetotheemotionalstrainoftheprocess. This
wouldmakeoptionBalikelychoice.Second,theindividualmayalsoexperienceasenseoffreedomfromadifficult
orunhappymarriage,makingoptionAapossibleanswer.Third,divorcecouldpotentiallyleadtofinancialchanges,
withtheindividualbecomingeitherricherorpoorerdependingonthecircumstances.However,itismorecommon
forindividualstoexperiencefinancialstrainduringdivorceproceedings,makingoptionClesslikely. Finally,the
emotionaltollofdivorcecancontributetofeelingsofdepression,makingoptionDareasonablechoice.Whiletissues
maybeneededtodrythetearsproducedduringadivorce,theyarenottypicallysomethingthatsomeonep¨roducesïn
largequantitiesduringtheprocess.Therefore,optionEistheleastlikelychoice.Inconclusion,themostlikelyanswers
✗
areB.tearsandD.depression.Therefore,theanswerisBandD.
Mentor B.tearsWhensomeoneisgoingthroughadivorce,theyarelikelytoexperienceemotionalturmoilandemotional
✓
distress.Thiscanleadtotearsastheyprocessthelossandtheemotionalimpactofthesituation.−−>B
(c)TrackingShuffledObjects
Question Alice,Bob,andClaireareplayingagame.Atthestartofthegame,theyareeachholdingaball:Alicehasaredball,
Bobhasapinkball,andClairehasablackball.Asthegameprogresses,pairsofplayerstradeballs.First,Claireand
Aliceswapballs.Then,AliceandBobswapballs.Finally,AliceandClaireswapballs.Attheendofthegame,Alice
hastheWhichchoiceistrue?Answerchoices:(A)redball,(B)pinkball,(C)blackball.
Answer (A)
Teacher Afterthefirstswap:Alicehasablackball,Bobhasapinkball,Clairehasaredball.Afterthesecondswap:Alicehas
aredball,Bobhasapinkball,Clairehasablackball.Afterthethirdswap:Alicehasablackball,Bobhasapinkball,
✗
Clairehasaredball.Therefore,attheendofthegame,Alicehasthe(C)blackball.Therefore,theansweris(C).
Mentor Afterthefirstswap:Alicehasablackball,Bobhasapinkball,Clairehasaredball.Afterthesecondswap:Alicehas
apinkball,Bobhasablackball,Clairehasaredball.Afterthethirdswap:Alicehasaredball,Bobhasablackball,
✓
Clairehasapinkball.Therefore,attheendofthegame,Alicehasthe(A)redball.−−>(A)
(d)LastLetterConcatenation
Question Takethelastlettersofeachwordsin“DinoTobyAbigailManuela"andconcatenatethem.
Answer oyla
Teacher 1.Dino-Lastletter:o,2.Toby-Lastletter:y,3.Abigail-Lastletter:l,4.Manuela-Lastletter:a.Concatenate:oyl.
✗
Therefore,theanswerisoyl.
Mentor 1.Dino-o,2.Toby-y,3.Abigail-l,4.Manuela-a.Concatenatingthelastlettersofeachword,weget:oyla.−−>
✓
oyla
Table8: Casestudyofrationaleaugmentationsbythementormodel(FlanT5-large).