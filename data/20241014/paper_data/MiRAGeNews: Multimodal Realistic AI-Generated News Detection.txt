MiRAGeNews: Multimodal Realistic AI-Generated News Detection
Runsheng(Anson)Huang, LiamDugan, YueYang, ChrisCallison-Burch
UniversityofPennsylvania
{rhuang99,ldugan,yueyang1,ccb}@seas.upenn.edu
Abstract
News Image + Caption MiRAGeNews Dataset
Theproliferationofinflammatoryormislead- In-Domain Out-of-Domain
ing “fake” news content has become increas-
inglycommoninrecentyears. Simultaneously,
ithasbecomeeasierthanevertouseAItools
President Obama was met
togeneratephotorealisticimagesdepictingany with protests on Tuesday at
a campaign rally at the Midjourney Stable Diffusion XL
sceneimaginable. Combiningthesetwo—AI- University of Wisconsin.
generated fake news content—is particularly
potent and dangerous. To combat the spread Real News?
Fake News?
ofAI-generatedfakenews,weproposetheMi-
RAGeNewsDataset,adatasetof12,500high-
quality real and AI-generated image-caption
pairsfromstate-of-the-artgenerators. Wefind Human
thatourdatasetposesasignificantchallengeto
humans(60%F-1)andstate-of-the-artmulti-
modalLLMs(<24%F-1). Usingourdataset,
wetrainamulti-modaldetector(MiRAGe)that
improves by +5.1% F-1 over state-of-the-art
baselinesonimage-captionpairsfromout-of- Figure 1: Multimodal fake news with hyperrealistic
domainimagegeneratorsandnewspublishers. generatedimagesfromMidjourneyposesasignificant
Wereleaseourcodeanddatatoaidfuturework challengeforbothstate-of-the-artMLLMs(<24%F-1)
ondetectingAI-generatedcontent.1 andhumans(60%F-1). Ourdetectorsachieveover98%
F-1onin-domain(ID)dataandcangeneralizeonout-
1 Introduction of-domain (OOD) data from unseen news publishers
andimagegenerators(85%F-1)
Diffusion models (Ho et al., 2020) have shown
remarkable advancements in generating hyper-
bachetal.,2022),Glide(Nicholetal.,2022),and
realisticimages. Particularly,modelslikeMidjour-
DALL-E2(Rameshetal.,2022). However,these
neycanproduceimagesthatevengraduateCSstu-
images are not always realistic and are often eas-
dentscannotdistinguish(Sec2.3). Thiscapability
ilydistinguishablebyhumansduetotheirobvious
hasprofoundimplications,especiallywithregard
anomalies. Since the datasets used in previous
to the dissemination of disinformation. Recently,
worksdonotaccuratelyrepresentthecurrentchal-
therehasbeenanoticeablesurgeinAI-generated
lengeposedbystate-of-the-art(SOTA)diffusion-
newsimagesonsocialmedia(MetzandHsu,2024).
basedmodels,thereisanevidentneedforachal-
When coupled with the proficiency of large lan-
lengingdatasetcomprisingrealisticAI-generated
guagemodels(LLMs)ingeneratinggrammatically
newsimagesandcaptionsthatprovidetheresearch
andcontextuallyappropriatecaptions,thepotential
communitywithafoundationtodevelopandtest
forAI-drivendisinformationcampaignsbecomes
newdetectionmethods.
anincreasinglycriticalconcern.
In this work, we present the MiRAGeNews
RecentworkondetectingAI-generatedimages
Dataset,adatasetofrealandfullygeneratednews
hasshownimpressiveperformanceonimagesgen-
image-captionpairswith12,500generatedimages
eratedbymodelssuchasStableDiffusion(Rom-
andcorrespondingcaptionsfromSOTAgenerators
1https://github.com/nosna/miragenews astrainingandvalidationsets. Toevaluatedetec-
4202
tcO
11
]VC.sc[
1v54090.0142:viXrators’ out-of-domain robustness, we also create a
testsetof2,500image-captionpairsfromvarious
unseenimagegeneratorsandnewspublishers.
Usingthisdata,wetrainMiRAGe,amultimodal
detectorthatfusesanimagedetectorandatextde-
tector,bothofwhichareensemblesofablack-box
linear model and an interpretable concept bottle-
neckmodel. WeshowthatMiRAGeexhibitsbetter
out-of-distribution(OOD)robustnesscomparedto
previousstate-of-the-artdetectorsandMLLMs.
2 MiRAGeNewsDataset
2.1 DatasetCreation
RealImagesandCaptions. Tocreateourdata,
wefirstsample6,500NewYorkTimesimageand
Figure2: ExampleofMiRAGeNewsdatasetgeneration.
captionpairsfromTARA(Fuetal.,2022)asour
WeuseGPT-4togenerateamisleadingcaption,which
“real”newssubset. WeselectTARAforthisdueto isthenusedbyMidjourneytogeneratetheimage.
thepresenceofspecificinformationonthelocation
andtimeofthenewseventsinthecaptions. Thisge-
To evaluate the detector’s generalization abil-
ographicalandtemporalinformationhelpsprovide
ity, we also collect 250 image-caption pairs each
extraconstraintstothemodelduringgeneration.
fromBBCandCNN4 tosimulatethedomaingaps
Fake Caption Generation. To simulate in- of news content from different news publishers.
stancesofreal-worlddisinformation,weexplicitly Wefollowthesameprocesstogeneratefakecap-
promptGPT-4(OpenAI,2024)totakearealcap- tionsanduseunseengenerativemodels,DALL-E3
tionand"generatefictionalcaptionsthatcouldbe andStableDiffusionXL(SDXL),togenerateOut-
consideredharmfulormisleading". Wealsoaskit of-Domain (OOD) fake images. We apply every
toincorporateallnamedentitiesfromtheoriginal combinationofunseennewsandimagegenerators
caption to ensure the generated caption does not to obtain four OOD datasets: BBC + DALL-E 3,
straytoofarfromtheoriginal. CNN + DALL-E 3, BBC + SDXL, and CNN +
SDXL.Withtheadditionof500in-domainrealor
Fake Image Generation. We choose Midjour-
fakeimage-captionpairs,weconstructourtestset
ney V5.2 as the image generator, considering its
with5smalldatasets,totaling2,500imagepairs.
hyper-realistic generations and relatively lenient
moderation.2 Togeneratefakeimages,weusefake 2.3 HumanEvaluation
captions from GPT-4 as prompts with additional
To evaluate human detection capability on our
keywords to restrict the photo style. We also in-
dataset,werecruited112studentswhoaretaking
cludetheaspectratioofthecorrespondingrealim-
a graduate-level NLP course with extra credit as
agesintheprompttoreflecttherealisticproperty
compensation. Eachstudentisrandomlyassigned
ofnewsimages.3
20imagecaptionpairsandisaskedtoseparately
determineiftheimageisgeneratedandifthecap-
2.2 TaskDesign
tionisgenerated. Eachpairinoursurveywasgiven
Ourdetectiontaskisdesignedtosimulatethereal-
tothreeparticipants,andweusedamajorityvote
worldscenariowherenewsonsocialmediaisoften
todeterminethefinalpredictionbyhumans.
presented as an image-caption pair. The detector
Ourevaluationresultsalignedwithourhypothe-
needstodetermineifboththeimageandcaption
sisthathumansarenotgoodatthisdetectiontask:
arereal(label0)orifbotharegenerated(label1).
theydetectedonly60.3%ofthegeneratedimages
and 53.5% of the generated captions. The well-
2Otherimagegeneratorshavestricterruleswithregardto
thegenerationofharmfulimagery(i.e.,fabricatedeventsof educatedparticipantsarerepresentativeofahigh-
publicfiguresorgraphiccrimescenes).
3Midjourney prompt: "{caption} News photo style –ar 4https://www.kaggle.com/datasets/szymonjanowski/internet-
width:height–styleraw". articles-data-with-users-engagementEVA-CLIP Image Image 0.8
Encoder ❄ Embedding
(a) MiRAGe-Img
(Ensemble of Linear
and Object-Class CBM) Owl V2 Human Linear 0.7
Head head
Plastic Linear 0.4
Bag P-bag Real News?
…
Fake News?
Does the text employ sensationalized 0.0 language or elements?
People looting a vegetable market in Zhengzhou, Are violent actions or behavior 1.0
depicted in the text?
Henan Province, China, on Tuesday. …
(b) MiRAGe-Txt
GPT-4 (Ensemble of CLIP Text Encoder ❄ Text Embedding 0.6
Linear and TBM)
Figure 3: Overview of our MiRAGe detector for multimodal AI-generated news detection, which combines
MiRAGe-ImgwithMiRAGe-Txt. MiRAGe-Imgtrainsalinearlayerontheoutputsfromtheimagelinearmodel
andObject-ClassConceptBottleneckModel(CBM),whileMiRAGe-Txttrainsalinearlayerontheoutputsfrom
thetextlinearmodelandTextBottleneckModel(TBM).Outputsfromtwomodelscanbeeitherearlyfusedorlate
fusedtomakethefinalpredictionontheimage-captionpair.
performingsubpopulationyetperformingapprox-
imatelyequallywithrandomguessing,whichim-
pliesthattheserealisticfakenewsstoriesarefully
capable of fooling humans, and we need models
thatcanassistinthistasktocombatdisinformation.
3 Detectors
Figure4:(a)EarlyFusiondetectorusesbothimageand
textfeaturestogetherforclassificationwhile(b)Late
Baselines. Wecompareourdetectorsagainstre-
Fusion detector uses outputs from previously trained
cent baselines in three different settings: Image-
unimodaldetectors.
only, Text-only, and Multimodal detection. For
image-only,wecomparetoDE-FAKE(Shaetal.,
2023),DIRE(Wangetal.,2023),andKNN(Ojha 2021) and (2) a Text Bottleneck Model (Ludan
et al., 2023). For text-only, we compare with the etal.,2023)thatextracts18textualconceptsinthe
TextBottleneckModel(TBM)(Ludanetal.,2023), caption. SimilartoMiRAGe-Img,weincorporate
andforMultimodal,wecomparewithHAMMER theinterpretableconcept-basedapproachtocapture
(Shaoetal.,2024). Wealsotestsimplelinearmod- theauxiliarysignalsthatthelinearmodelsmissed.
elsineachmodalityandbenchmarkstate-of-the-art
MLLMsontheimage-onlyandmultimodaldetec- MiRAGe fusesMiRAGe-ImgandMiRAGe-Txt
tion tasks. See Appendix A for a more detailed togetherformultimodalgeneratednewsdetection.
discussiononeachofthedetectorstested. We apply two fusion techniques as illustrated in
Figure4: EarlyFusioninvolvesconcatenatingthe
MiRAGe-Img trainsalinearlayerontopofthe
outputsfromMiRAGe-ImgandMiRAGe-Txtbe-
outputsfromtwomodels: (1)alinearmodeltrained
foretrainingalinearlayerforclassification,while
usingEVA-CLIP(Sunetal.,2023)imageembed-
LateFusioncomputesafinalpredictionfromthe
dingsand(2)anObject-ClassConceptBottleneck
outputsofthesetwomodelswithnoextratraining.
Model (CBM) containing 300 object-class clas-
We train all models until the evaluation loss
sifiers trained on crops of different objects from
plateausandapplytheclassificationthresholdthat
OwlV2(Mindereretal.,2024). Theinterpretable
givesthehighestevaluationaccuracyintesting. As
Object-ClassCBMfocusesonregionalanomalies,
forourdesigndecisions,weconductedadetailed
whilethelinearmodelcapturestheglobalfeatures.
ablation study for each part of the MiRAGe de-
MiRAGe-Txt trainsalinearlayerontopofthe tector in Appendix B. Due to the results of these
outputsfromtwomodels: (1)alinearmodelthatis ablations, we chose the late fusion model as our
trainedusingCLIPtextembeddings(Radfordetal., finalMiRAGedetector.
raeniL
raeniL
❄
❄
reyaL
raeniL
reyaL
raeniL
🔥
)egamI(
🔥
)txeT(Figure5: WeseethatMiRAGe-Imgoutperformsexist-
Figure 6: We see that MiRAGe outperforms existing
ing image-only detectors in both in-domain (ID) and
image-textdetectorsinout-of-domainsettings. ZSand
out-of-domain (OOD). ZS and FT are short for Zero-
FTareshortforZero-ShotandFine-Tuned,respectively.
ShotandFine-Tuned,respectively
MLLMs’zero-shotperformanceheavilyvariesde-
4 Results
pending on the training data distribution. While
4.1 Image-only utilizingadditionalsignalsfromsemanticinconsis-
As shown in Figure 5, the MiRAGe-Img model tencybetweenimagesandtextshelpsHAMMER
demonstrates better in-domain (ID) performance generalize on OOD data, MiRAGe, which fuses
and out-of-domain (OOD) generalization ability MiRAGe-ImgandMiRAGe-Txt,hasshownbetter
than our baselines. We find that zero-shot DIRE OODperformanceoverall(seeAppendixB).
struggles on all datasets, most likely due to the
major domain shift from training data (bedroom 5 RelatedWork
images)totestingdata(newsimages). Whilethe
models fine-tuned on ID data have substantially Multimodal Fake News Datasets. There are
lowerperformanceonDALL-E,wearesurprised many datasets for detecting generated images
tofindthatDIREFThasahigheraverageF-1on (Wangetal.,2020;Heetal.,2021;Zhuetal.,2023)
SDXL(70.5%)thanMidjourney(64.4%). Onerea- and generated text (Dugan et al., 2024; Li et al.,
sonablespeculationcouldbeasimilarbasemodel 2024;Wangetal.,2024). However,therearerela-
sharedbytwogenerators,whichisreflectedinsim- tivelyfewpubliclyavailabledatasetsfordetecting
ilar reconstruction errors. Our model shows that generatedimage-textpairs—especiallyinthenews
usingtheObject-ClassCBMalongwiththelinear domain. TheTwitterMediaEvaldataset(Boididou
modelhelpsimproveOODrobustness. etal.,2014)containsacorpusof17,000tweetson
twoeventswith514realorfakeimages. Weibo(Jin
4.2 Text-only
etal.,2017)collectsrealandrumornewspoststhat
As shown in Table 1, MiRAGe-Txt outperforms areverifiedbytheauthoritativedebunkingsystem
baselinesinbothIDcaptionsrewrittenfromNew withmostlyrealimages. FakeNewsNet(Shuetal.,
YorkTimesnewsandOODcaptionsfromunseen 2020) collects real and fake news from Politifact
newspublishers(BBCandCNN). andGossipCopandcontains17,214human-written
newsarticleswithimagesand1,986fakenewsarti-
4.3 Multimodal
cleswithimages. ThemorerecentDGM4dataset
Inthemultimodalsettingwithbothimagesandcap- (Shaoetal.,2023)offers230knewssampleswith
tions,ourMiRAGedetectorexhibitsbetterOOD image and text, which contain 77k pristine pairs
robustnessthanourbaselines(seeFigure6). Both and 152k manipulated pairs from small regional
state-of-the-artMLLMs(GPT-4oandGemini1.5) manipulationsonimageand/ortext. Whileprevi-
struggle on ID data. We further find that the low ous datasets offer the foundation of multi-modal
F1attributetoextremelylowrecall(fakeaccuracy) fake news, our dataset aims to address the more
asshowninTable2. However, GPT-4o’ssurpris- dangerous forms of fake news, namely the ones
ingperformanceonDALL-Emakesusspeculate thathaveconvincinglydeceptivevisualsfullygen-
itmightbetrainedwithDALL-Eimagesandthat eratedbydiffusionmodels.Generated Image Detection. Many previous thoughitispossibletomachinetranslatetheentire
works have explored different methods for effec- dataset,wewouldleavethistofuturework.
tivefakenewsdetection(Wangetal.,2018;Khattar
etal.,2019;Singhaletal.,2019). Stemmingfrom 8 EthicsStatement
detecting GAN-generated images (Gragnaniello
Since most of the fake images from our dataset
etal.,2021;Wangetal.,2020),manymethodshave
aregeneratedfrommisleadingorharmfulcaptions,
beenproposedtodetectdiffusion-basedgeneration.
andMidjourney’smoderationsystemisnotperfect,
DE-FAKE(Shaetal.,2023)usesBLIPtogenerate
somegenerationsmightbeconsideredtobeunsafe.
acaptionforeveryimage,thencombinesbothfea-
AlthoughtherealcaptionsthatGPT-4usedduring
turestopredict. DIRE(Wangetal.,2023)analyzes
thegenerationaredated,itisstillverylikelythat
the reconstruction error during denoising. Ojha
thegeneratedcaptioncanstandaloneasasource
et al. (2023) computes the distance between the
ofdisinformationaboutcurrentevents.
testingimageandthetrainingsetandusesKNNto
predictrealvs. fake. Comparedtotheseblack-box
Acknowledgement
methods,ourproposedobject-classCBMoffersa
newperspectiveoninterpretablegeneratedimage We thank the students from CIS 5300 at the Uni-
detectors. Moreover,withtheadditionofgenerated versityofPennsylvaniaforthehumanannotation
fakecaptions,ourdatasetlaysthegroundworkfor ongeneratedimage-captionpairs. Wealsothank
morecreativefutureworks. Muzi (Hex) Li for her support on the project.
This research is supported in part by the Office
6 Conclusion of the Director of National Intelligence (ODNI),
IntelligenceAdvancedResearchProjectsActivity
In this paper, we introduce MiRAGeNews, a
(IARPA),viatheHIATUSProgramcontract#2022-
dataset designed to facilitate the development
22072200005. The views and conclusions con-
and benchmarking of detection methods for AI-
tainedhereinarethoseoftheauthorsandshouldnot
generatednews. Ourdatasetisthefirstofitskind
beinterpretedasnecessarilyrepresentingtheoffi-
toincludehigh-qualityimagesfrommoderngen-
cialpolicies,eitherexpressedorimplied,ofODNI,
eratorsalongwithmisleadingorharmfulcaptions,
IARPA, or the U.S. Government. The U.S. Gov-
andourresultshighlightthesignificantchallenges
ernmentisauthorizedtoreproduceanddistribute
facedbyhumansandcurrentstate-of-the-artmul-
reprintsforgovernmentalpurposes,notwithstand-
timodal language models in detecting such news
inganycopyrightannotationtherein.
content. We show that classifiers trained on our
dataachievehighaccuracyonthemostdifficult-to-
detect images while still showing strong general- References
izationperformanceonout-of-domaingenerators
Christina Boididou, Symeon Papadopoulos, Yiannis
andnewssources.
Kompatsiaris, Steve Schifferes, and Nic Newman.
2014. Challenges of computational verification in
7 Limitations
social multimedia. In Proceedings of the 23rd In-
ternationalConferenceonWorldWideWeb,WWW
In the design of the testing set with OOD data, ’14Companion,page743–748,NewYork,NY,USA.
while both our real images and fake images are AssociationforComputingMachinery.
OOD,itisnottrulyOODforcaptions. Weusethe
Liam Dugan, Alyssa Hwang, Filip Trhlik, Josh Mag-
same procedure to prompt GPT-4 when generat-
nus Ludan, Andrew Zhu, Hainiu Xu, Daphne Ip-
ing, and the domain shift, if any, will come from polito, and Chris Callison-Burch. 2024. Raid:
the real captions of unseen news publishers. Our A shared benchmark for robust evaluation of
experimentswouldbemorecomprehensiveifwe machine-generated text detectors. Preprint,
arXiv:2405.07940.
finetune GPT-4o and Gemini 1.5 on our dataset.
However,trainingwithimagesthatcontainpublic XingyuFu,BenZhou,IshaanChandratreya,CarlVon-
figuresandfaceswouldviolatetheTermofService, drick,andDanRoth.2022. There’satimeandplace
thusmakingourdatasetmostlyunavailable. Also, for reasoning beyond the image. In Proceedings
of the 60th Annual Meeting of the Association for
sinceourrealnewsdatasetismostlyfromtheNew
ComputationalLinguistics(Volume1: LongPapers),
York Times, all of our real and fake captions are
pages1138–1149,Dublin,Ireland.Associationfor
Englishonly,makingitamonolingualdataset. Al- ComputationalLinguistics.Diego Gragnaniello, Davide Cozzolino, Francesco UtkarshOjha,YuhengLi,andYongJaeLee.2023. To-
Marra,GiovanniPoggi,andLuisaVerdoliva.2021. wardsuniversalfakeimagedetectorsthatgeneralize
AreGANgeneratedimageseasytodetect? Acrit- acrossgenerativemodels. In2023IEEE/CVFCon-
icalanalysisofthestate-of-the-art. arXivpreprint. ferenceonComputerVisionandPatternRecognition
ArXiv:2104.02617[cs]. (CVPR),pages24480–24489.
Yinan He, Bei Gan, Siyu Chen, Yichun Zhou, Guo- OpenAI. 2024. Gpt-4 technical report. Preprint,
junYin, LuchuanSong, LuSheng, JingShao, and arXiv:2303.08774.
Ziwei Liu. 2021. Forgerynet: A versatile bench-
markforcomprehensiveforgeryanalysis. Preprint, AlecRadford,JongWookKim,ChrisHallacy,Aditya
arXiv:2103.05630. Ramesh,GabrielGoh,SandhiniAgarwal,GirishSas-
try, Amanda Askell, Pamela Mishkin, Jack Clark,
JonathanHo,AjayJain,andPieterAbbeel.2020. De- GretchenKrueger,andIlyaSutskever.2021. Learn-
noising diffusion probabilistic models. Advances ingtransferablevisualmodelsfromnaturallanguage
inneuralinformationprocessingsystems,33:6840– supervision. Preprint,arXiv:2103.00020.
6851.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol,
ZhiweiJin,JuanCao,HanGuo,YongdongZhang,and
Casey Chu, and Mark Chen. 2022. Hierarchical
JieboLuo.2017. Multimodalfusionwithrecurrent
text-conditionalimagegenerationwithcliplatents.
neuralnetworksforrumordetectiononmicroblogs.
Preprint,arXiv:2204.06125.
InProceedingsofthe25thACMInternationalCon-
ferenceonMultimedia,MM’17,page795–816,New
NilsReimersandIrynaGurevych.2019. Sentence-bert:
York,NY,USA.AssociationforComputingMachin-
Sentenceembeddingsusingsiamesebert-networks.
ery.
Preprint,arXiv:1908.10084.
DhruvKhattar,JaipalSinghGoud,ManishGupta,and
RobinRombach,AndreasBlattmann,DominikLorenz,
Vasudeva Varma. 2019. Mvae: Multimodal vari-
Patrick Esser, and Björn Ommer. 2022. High-
ational autoencoder for fake news detection. In
resolutionimagesynthesiswithlatentdiffusionmod-
TheWorldWideWebConference,WWW’19,page
els. Preprint,arXiv:2112.10752.
2915–2921, New York, NY, USA. Association for
ComputingMachinery.
ZeyangSha,ZhengLi,NingYu,andYangZhang.2023.
De-fake: Detection and attribution of fake images
PangWeiKoh,ThaoNguyen,YewSiangTang,Stephen
generated by text-to-image generation models. In
Mussmann, Emma Pierson, Been Kim, and Percy
Proceedingsofthe2023ACMSIGSACConferenceon
Liang.2020. ConceptBottleneckModels. InPro-
ComputerandCommunicationsSecurity,CCS’23,
ceedings of the 37th International Conference on
page3418–3432,NewYork,NY,USA.Association
MachineLearning,pages5338–5348.PMLR. ISSN:
forComputingMachinery.
2640-3498.
RuiShao,TianxingWu,andZiweiLiu.2023. Detect-
YafuLi,QintongLi,LeyangCui,WeiBi,ZhilinWang,
ingandgroundingmulti-modalmediamanipulation.
LongyueWang,LinyiYang,ShumingShi,andYue
Preprint,arXiv:2304.02556.
Zhang.2024. Mage: Machine-generatedtextdetec-
tioninthewild. Preprint,arXiv:2305.13242.
RuiShao,TianxingWu,JianlongWu,LiqiangNie,and
Ziwei Liu. 2024. Detecting and grounding multi-
Josh Ludan, Qing Lyu, Yue Yang, Liam Dugan,
modalmediamanipulationandbeyond. IEEETrans-
Mark Yatskar, and Chris Callison-Burch. 2023.
actionsonPatternAnalysisandMachineIntelligence,
Interpretable-by-DesignTextUnderstandingwithIt-
46(8):5556–5574.
eratively Generated Concept Bottleneck. arXiv e-
prints,arXiv:2310.19660.
KaiShu,DeepakMahudeswaran,SuhangWang,Dong-
CadeMetzandTiffanyHsu.2024. AnA.I.Researcher wonLee,andHuanLiu.2020. Fakenewsnet: Adata
TakesOnElectionDeepfakes. TheNewYorkTimes. repositorywithnewscontent,socialcontext,andspa-
tiotemporalinformationforstudyingfakenewson
MatthiasMinderer,AlexeyGritsenko,andNeilHoulsby. socialmedia. BigData,8:171–188.
2024. Scalingopen-vocabularyobjectdetection. In
Proceedingsofthe37thInternationalConferenceon Shivangi Singhal, Rajiv Ratn Shah, Tanmoy
NeuralInformationProcessingSystems, NIPS’23, Chakraborty, Ponnurangam Kumaraguru, and
RedHook,NY,USA.CurranAssociatesInc. Shin’ichi Satoh. 2019. Spotfake: A multi-modal
frameworkforfakenewsdetection. In2019IEEE
Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Fifth International Conference on Multimedia Big
Pranav Shyam, Pamela Mishkin, Bob McGrew, Data(BigMM),pages39–47.
Ilya Sutskever, and Mark Chen. 2022. Glide:
Towards photorealistic image generation and edit- QuanSun,YuxinFang,LedellWu,XinlongWang,and
ing with text-guided diffusion models. Preprint, Yue Cao. 2023. Eva-clip: Improved training tech-
arXiv:2112.10741. niquesforclipatscale. Preprint,arXiv:2303.15389.Sheng-Yu Wang, Oliver Wang, Richard Zhang, An-
drew Owens, and Alexei A. Efros. 2020. Cnn-
generatedimagesaresurprisinglyeasytospot...for
now. Preprint,arXiv:1912.11035.
Yaqing Wang, Fenglong Ma, Zhiwei Jin, Ye Yuan,
Guangxu Xun, Kishlay Jha, Lu Su, and Jing Gao.
2018. Eann: Eventadversarialneuralnetworksfor
multi-modal fake news detection. In Proceedings
ofthe24thACMSIGKDDInternationalConference
onKnowledgeDiscovery&DataMining,KDD’18,
page849–857,NewYork,NY,USA.Associationfor
ComputingMachinery.
YuxiaWang,JonibekMansurov,PetarIvanov,Jinyan
Su,ArtemShelmanov,AkimTsvigun,ChenxiWhite-
house,OsamaMohammedAfzal,TarekMahmoud,
Toru Sasaki, Thomas Arnold, Alham Fikri Aji,
NizarHabash,IrynaGurevych,andPreslavNakov.
2024. M4:Multi-generator,multi-domain,andmulti-
lingualblack-boxmachine-generatedtextdetection.
Preprint,arXiv:2305.14902.
ZhendongWang,JianminBao,WengangZhou,Weilun
Wang, Hezhen Hu, Hong Chen, and Houqiang Li.
2023. Direfordiffusion-generatedimagedetection.
Preprint,arXiv:2303.09295.
Yue Yang, Artemis Panagopoulou, Shenghao Zhou,
DanielJin,ChrisCallison-Burch,andMarkYatskar.
2023. Languageinabottle: Languagemodelguided
conceptbottlenecksforinterpretableimageclassifi-
cation. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages
19187–19197.
Mingjian Zhu, Hanting Chen, Qiangyu Yan, Xudong
Huang,GuanyuLin,WeiLi,ZhijunTu,HailinHu,
Jie Hu, and Yunhe Wang. 2023. Genimage: A
million-scalebenchmarkfordetectingai-generated
image. Preprint,arXiv:2306.08571.NYT+MJ(ID) BBC+DALL-E CNN+DALL-E BBC+SDXL CNN+SDXL OOD-AVG
Acc F1 AP Acc F1 AP Acc F1 AP Acc F1 AP Acc F1 AP Acc F1 AP
GPT-4o ZS 51.6 6.2 100 93.0 92.7 97.4 96.2 96.2 96.4 67.2 52.9 93.9 78.2 73.5 93.8 83.7 78.8 95.4
Gemini1.5 ZS 48.0 12.2 39.1 75.9 75.0 91.7 83.8 85.6 93.4 47.7 22.3 67.6 45.4 30.1 76.6 63.2 53.3 82.3
ZS 61.4 42.0 35.6 63.8 66.4 58.8 61.6 66.6 50.5 69.4 73.0 60.1 67.4 73.0 63.4 65.6 69.8 58.2
DE-FAKE
FT 87.6 86.8 33.7 44.6 56.7 47.7 44.6 54.7 40.2 52.6 65.1 63.8 55.0 66.1 56.6 49.2 60.7 52.1
ZS 50.0 3.8 54.0 48.6 9.2 52.1 48.2 10.4 48.9 48.4 8.5 48.7 48.0 9.7 49.0 48.3 9.5 49.7
DIRE
FT 57.8 64.4 56.0 48.8 53.8 56.9 48.0 51.3 50.7 62.4 69.8 73.2 64.6 71.3 68.3 56.0 61.6 62.3
KNN ZS 73.8 76.5 69.4 43.2 57.1 45.9 53.8 65.8 52.2 57.2 67.8 54.3 51.0 64.9 50.6 51.3 63.9 50.8
MiRAGe-I FT 98.0 98.0 99.9 77.6 75.0 88.4 74.6 73.8 84.4 78.4 76.1 87.7 77.6 77.6 83.0 77.1 75.6 85.9
Linear(SB) FT 81.8 81.8 91.0 68.8 75.3 85.2 70.0 75.9 87.1 69.4 75.6 86.2
Linear(CL) FT 81.8 80.5 90.2 68.4 74.8 84.4 77.6 80.3 90.9 73.0 77.6 87.7
TBM FT 74.0 71.4 78.1 68.8 68.7 72.1 76.2 76.8 75.2 72.5 72.8 73.7
MiRAGe-T FT 83.2 81.9 91.0 72.6 77.0 85.2 78.4 80.9 92.0 75.5 79.0 88.6
GPT-4o ZS 51.8 7.0 100 90.1 89.0 100 92.8 92.2 100 62.7 40.4 100 70.7 58.2 100 79.1 70.0 100
Gemini1.5 ZS 56.4 24.4 94.6 78.9 76.4 87.2 81.2 80.2 85.1 64.2 51.8 79.2 69.1 62.4 79.4 73.4 67.7 82.7
ZS 39.0 11.6 37.2 49.2 22.6 48.7 45.2 21.7 42.9 51.0 27.3 51.4 48.8 30.4 47.5 48.6 25.5 47.6
HAMMER
FT 99.4 99.4 100 77.2 73.0 86.2 78.8 74.8 89.4 85.0 83.7 93.4 88.2 87.4 96.2 82.3 79.7 91.3
MiRAGe FT 98.4 98.4 99.9 85.4 86.0 94.1 79.6 81.0 90.4 86.0 86.7 95.9 83.8 85.5 94.7 83.7 84.8 93.8
Table1: TheAccuracy,F-1score,andAveragePrecision(AP)ofalldetectorsacrossMiRAGeNewstestsets.
NotethatAPforGPT-4oandGemini1.5arejustprecisionandnotcomparabletoothermodels. ZS,FT,SB,andCL
standforzero-shot,fine-tuned,SentenceBert,andCLIP,respectively. WeleaveSDXLcolumnsblankinText-only
detectorssincetheysharethesametextualdataastheDALL-Ecolumns. Weseethatzero-shotmodelsstruggleon
in-domaindata,whilefine-tunedmodelsachievestrongin-domainperformanceandrespectablegeneralizationon
OODsections. SeeFigure5and6forgraphicalhighlightsfromthistable.
NYT+MJ(ID) BBC+DALL-E CNN+DALL-E BBC+SDXL CNN+SDXL OOD-AVG
Real Fake Real Fake Real Fake Real Fake Real Fake Real Fake
GPT-4o ZS 100.0 3.2 97.6 88.4 96.4 96.0 97.6 36.8 96.0 60.4 96.9 70.4
Gemini1.5 ZS 88.8 7.2 88.4 63.4 88.6 79.0 82.0 13.4 72.0 18.8 82.8 43.7
ZS 94.8 28.0 56.0 71.6 46.8 76.4 56.0 82.8 46.8 88.0 51.4 79.7
De-Fake
FT 94.0 81.2 16.8 72.4 22.4 66.8 16.8 88.4 22.4 87.6 19.6 78.8
ZS 98.0 2.0 92.0 5.2 90.4 6.0 92.0 4.8 90.4 5.6 91.2 5.4
DIRE
FT 39.2 76.4 38.0 59.6 41.2 54.8 38.0 86.8 41.2 88.0 39.6 72.3
KNN ZS 62.4 85.2 10.8 75.6 18.8 88.8 24.4 90.0 11.2 90.8 16.3 86.3
MiRAGe-I FT 98.8 97.2 88.0 67.2 77.6 71.6 88.0 68.8 77.6 77.6 82.8 71.3
SBERT FT 82.0 81.6 42.4 95.2 45.6 94.4 44.0 94.8
CLIP FT 88.4 75.2 42.8 94.0 64.0 91.2 53.4 92.6
TBM FT 83.2 64.8 69.2 68.4 73.6 78.8 71.4 73.6
MiRAGe-T FT 90.4 76.0 53.6 91.6 65.2 91.6 59.4 91.6
GPT-4o ZS 100.0 3.6 100.0 80.2 100.0 85.5 100.0 25.3 100.0 41.1 100.0 58.0
Gemini1.5 ZS 98.8 14.0 89.8 68.0 86.6 75.8 89.9 38.5 86.8 51.4 88.3 58.4
ZS 70.0 8.0 83.6 14.8 75.2 15.2 83.6 18.4 75.2 22.4 79.4 17.7
HAMMER
FT 99.6 99.2 92.8 61.6 94.8 62.8 92.8 77.2 94.8 81.6 93.8 70.8
MiRAGe FT 98.4 98.4 80.8 90.0 72.0 87.2 80.8 91.2 72.0 95.6 76.4 91.0
Table2: Class-wiseaccuracy(%)ofalldetectorsacrossdatasets. "Real"and"Fake"columnsrepresentthe
accuracyoftherealandfake/generatedclasses,respectively. WeleaveSDXLcolumnsblankinText-onlydetectors
sincetheysharethesametextualdataastheDALL-Ecolumns. WefindthatMLLMsaregreatatrecognizingreal
images, especiallywithadditionaltextualinformation. Wealsofindthatourmodelismuchbetteratdetecting
generatedimage-captionpairsthanothermodels
egamI
txeT
txeT+egamI
egamI
txeT
txeT+egamIA ModelImplementationDetails Model Task MethodSummary
DE-FAKE Image Uses BLIP to caption
A.1 Image-onlyDetector
(Shaetal.,2023) images and uses extra
textfeats.todetect
Linear Model. For our linear model, we use a
DIRE Image Computesimagerecon-
frozenEVA-CLIPViTencoderfromBLIPtoem- (Wangetal.,2023) struction error during
bedourimagesandaddalinearlayerwithSigmoid diffusionanddenoising
KNN Image Mapsrealandfakeimg.
activationtoprojectdowntotheoutputdimension.
(Ojhaetal.,2023) tofeat. spaceanduses
ObjectClassCBM.ConceptBottleneckModels theclosestimagetopre-
dict
(CBM)(Kohetal.,2020;Yangetal.,2023)have
Linear(EVA-CLIP) Image UsesEVA-CLIPtoen-
beenshownto improve generalizabilityin image (Sunetal.,2023) codeimagesandtrains
alinearmodel
classificationtasks. Extendingthisapproachwith
Obj-ClassCBM Image Trainsoneclassifierper
theintuitionthatanomaliesingeneratedimagesare object class, and pre-
often regional and object-based (i.e., merged fin- dictsw/outputs
gers,curvedbuildings),wechoosecommonobject Linear(SBERT) Text UsesSentenceBERTto
classesastheconcepts. (Reimers and encode captions and
Gurevych,2019) trainsalinearmodel
WefirstutilizeOwlV2todetectandcropoutthe
Linear(CLIP) Text Uses CLIP to encode
objectsinbothrealandfakeimages. Thesecrops (Radfordetal.,2021) captionsandtrainsalin-
earmodel
areorganizedintoadatasetof300objectclasses,
TBM Text Prompts LLM to dis-
each containing real or fake crops of the object. (Ludanetal.,2023) cover textual concepts
Wethentrainalinearmodeltodetectfakeobjects andtrainsalinearlayer
withineachobjectclass. HAMMER Image Designs a Manipula-
Tocreatealistofconceptscoresforeachimage (Shaoetal.,2023) +Text tionAwareContrastive
Learning Loss to cap-
as a bottleneck, with each detected object class,
turethesemanticincon-
weusethecorrespondingclassifiertopredicteach sistency between im-
agesandtext.
instanceoftheobjectclassandkeepthemaximum
prediction score. With undetected object classes
Table3: Summariesofdetectorsusedinourimage-only
havingapredictionscoreof0,wemapanyinput
andtext-onlydetection.
image to 300 concept scores. Lastly, we train a
linearmodelasthebottleneckpredictorandonly
usethe300conceptscorestopredictagivenimage. methodanditerativelyextract18diverseconcepts
from our captions after filtering. We then train a
MiRAGe-ImgensemblesthelinearmodelandOb-
linear layer as a bottleneck predictor to map the
jectClassCBM.OurexperimentsonObjectClass
conceptscorestofinalpredictions.
CBMshowthat,althoughhavingloweroverallac-
MiRAGe-TxtensemblesthelinearmodelandText
curacy,itlearnstodetectfakeimagesmuchbetter
CBM.OurexperimentsonTextCBMshowasimi-
thanthelinearmodel(+14.2%FakeAccuracy). In-
larpattern: whileithasloweroverallaccuracy,it
corporatingthestrengthsofbothmodels,wecom-
is better at detecting real captions than the linear
putethepredictionscorefromthelinearmodelas
model(+10%FakeACC).Weensemblethesetwo
anextraconceptscoreandconcatenateitwiththe
modelsbyaddingapredictionscorefromthelinear
original300conceptscores. Thismodelachieves
modelasanextraconceptscoretothebottleneck
state-of-the-artperformanceinourtestingset.
andtraininganotherlinearlayerontopforbinary
A.2 Text-onlyDetectors classification.
Linear Model. Similar to the linear model for
A.3 MultimodalDetectors
images, we add a linear layer with sigmoid acti-
vationontopofafrozenpre-trainedtextencoder. In this multimodal task, MiRAGe combines out-
Weexploredvarioustextencodersandsurprisingly putsfromMiRAGe-ImgandMiRAGe-Txttopre-
foundthattheCLIPencoderperformedbetterthan dict if an image-caption pair is real or generated.
SentenceBERT(ReimersandGurevych,2019). Weexploredbothearlyfusionandlaterfusionap-
TBM (Text Bottleneck Model). harnesses the proaches:
powerofLLMtoautomaticallyextractdistinguish- Early Fusion. In this approach, we concatenate
ingconceptfeaturesfromthetext. Weadoptthis bothimageandtextfeaturesfromagivenpairandtrainalinearmodeltopredict. Thismethodallows
Generated Image Anomalies
themodeltouseinformationfrombothmodalities Unreadable Text
4.5%
inconjunctiontomakepredictions. Camera Angle Texture
5.5% 24.2%
Late Fusion. In this approach, we first utilize Body Parts 116 (8.3%) 337 (24.2%)
8.3%
theimage-onlyandcaption-onlymodelstomake
Fingers
correspondingpredictions. Wetaketheaverageof 11.4% 159 (11.4%)
the logits from these two models with a sigmoid
254 (18.2%) Object Shapes
activationtomakeasinglepredictionofthepair. Face 162 (11.6%) 18.2%
11.6%
Spatial Relation 227 (16.3%)
B AblationStudy 16.3%
As shown in Table 4, all components of the Mi- Figure7: Annotatedgeneratedimageanomaliesfrom
humanstudy
RAGedetectorareessentialfortheirperformance.
Wefindthatthelinearmodelsperformbetterthan
the concept bottleneck models, and early fusion Generated Caption Anomalies
performsslightlyworsethanlatefusion. Contradicts Facts
21.0% Biased Language
We further investigate the class-wise accuracy 161 (21.0%) 28.2%
216 (28.2%)
foreachcomponentasshowninTable5. Wefind
thatthelinearmodelisbetteratrecognizingrealim-
ages,whileObj-CBMisbetteratdetectingfakeim-
Generic Statement 180 (23.5%)
ages. Wealsoseethatthelinearmodelisbetterat 23.5%
209 (27.3%)
detectingfakecaptions,whileTBMisbetteratrec- Unlikely Action
27.3%
ognizingrealcaptions. Thisfindinggivesussome
idea as to why the ensemble methods (MiRAGe-
Figure8: Annotatedgeneratedcaptionanomaliesfrom
Img and MiRAGe-Txt) perform better than their humanstudy
individualcomponents.
WhileCBMsalwaysunderperformlinearmod-
theannotationsis0.22,whichshowsalowagree-
els, theyhelptheminthelower-performantclass
mentamongtheparticipantsandimpliesthatitis
withoutaffectingthehigher-performantclasswhen
difficult for humans to consistently detect gener-
ensembling in our MiRAGe detector. However,
atednews. Theseresultsfurtherreflectsthedanger
the tradeoff is that ensembling an interpretable
ofsuchhyperrealisticgeneratednews.
method with a black-box method takes away the
The participants are also asked to provide the
interpretability.
reasonswhytheythinkagivenimageorcaptionis
C HumanStudyDetails generated. Weprovidealistofcommonanomalies
foundingeneratedimagesandcaptionstochoose
We recruited 112 graduate students enrolled in a fromasshowninTable6. Forthegeneratedimages
CS class with extra credits as compensation, and that the participants correctly classified, the top
eachparticipantwasrandomlyassigned20image- clueshumansareusingareTexture(24.2%),Object
captionpairstoannotate. Theyareaskedtodeter- Shapes(18.2%),andSpatialRelation(16.3%)as
minewhethertheimageisgeneratedandwhether shown in Figure 7. Similarly, the top clues for
thecaptionraisestheirsuspicionsoffakenews,as generatedcaptionsareBiasedLanguage(28.2%)
showninFigure9. Eachimage-captionpairinthe andUnlikelyAction(27.3%)asshowninFigure8.
surveydatasetisshowntothreeparticipants,and
the majority of the votes determine the human’s D Examples
judgment. Note that if a participant thinks either
In Figure 10 we show examples from the MiRA-
theimageorthecaptionisgenerated,thefinalde-
GeNews dataset from different image generators.
cisionofthenewswouldbe“generated”.
WeseethatMidjourneyproducesimagesthatare
Theresultsshowanoverallaccuracyof71.4%,
much more difficult to detect than other similar
F-1 score of 60.4% and precision of 89%. More-
generators. Combinedwiththelackofmoderation,
over, humansaremuchbetteratrecognizingreal
wefeelthatthisgeneratoristhemostimportantto
news(97%realacc.) thandetectinggeneratednews
coverinadatasetsuchasours.
(45.8%fakeacc.). TheKrippendorff’sAlphafromNYT+MJ(ID) BBC+DALL-E CNN+DALL-E BBC+SDXL CNN+SDXL OOD-AVG
Acc F1 AP Acc F1 AP Acc F1 AP Acc F1 AP Acc F1 AP Acc F1 AP
MiRAGe-I 98.0 98.0 99.9 77.6 75.0 88.4 74.6 73.8 84.4 78.4 76.1 87.7 77.6 77.6 83.0 77.1 75.6 85.9
(-CBM) 97.6 97.6 99.9 70.4 63.0 84.2 66.6 62.3 75.1 77.2 73.7 87.3 74.2 73.2 81.2 72.1 68.1 82.0
(-Linear) 94.2 94.2 98.1 66.6 70.3 82.8 72.0 74.4 84.6 65.6 69.2 74.9 65.0 65.8 73.8 67.3 69.9 79.0
MiRAGe-T 83.2 81.9 91.0 72.6 77.0 85.2 78.4 80.9 92.0 75.5 79.0 88.6
(-TBM) 81.8 80.5 90.2 68.4 74.8 84.4 77.6 80.3 90.9 73.0 77.6 87.7
(-Linear) 74.0 71.4 78.1 68.8 68.7 72.1 76.2 76.8 75.2 72.5 72.8 73.7
MiRAGe 98.4 98.4 99.9 85.4 86.0 94.1 79.6 81.0 90.4 86.0 86.7 95.9 83.8 85.5 94.7 83.7 84.8 93.8
(-LateFusion) 98.4 98.4 99.9 80.6 78.4 92.7 79.4 78.0 89.3 83.2 81.8 93.9 85.8 85.8 93.4 82.3 81.0 92.3
(-CBMs) 99.0 99.0 99.9 81.4 81.9 90.6 72.4 73.8 82.5 85.0 85.9 94.7 80.8 83.1 91.8 79.9 81.2 89.9
(-Linears) 94.6 94.6 98.8 71.0 74.4 84.4 74.0 76.8 84.8 69.8 73.1 77.5 75.2 78.1 79.5 72.5 75.6 81.6
Table4: TheAccuracy,F-1score,andAveragePrecision(AP)forablationstudyonMiRAGe-Img,MiRAGe-
Txt,andMiRAGe. WefindthatwhileallcomponentsoftheMiRAGemodelarecentraltoitshighperformance,
thelinearmodelsperformbetterthantheconceptbottleneckmodels,andearlyfusionperformsslightlyworsethan
latefusion.
NYT+MJ(ID) BBC+DALL-E CNN+DALL-E BBC+SDXL CNN+SDXL OOD-AVG
Real Fake Real Fake Real Fake Real Fake Real Fake Real Fake
MiRAGe-I 98.8 97.2 88.0 67.2 77.6 71.6 88.0 68.8 77.6 77.6 82.8 71.3
(-CBM) 98.8 96.4 90.4 50.4 78.0 55.2 90.4 64.0 78.0 70.4 84.2 60.0
(-Linear) 94.8 93.6 54.0 79.2 62.8 81.2 54.0 77.2 62.8 67.2 58.4 76.2
MiRAGe-T 90.4 76.0 53.6 91.6 65.2 91.6 59.4 91.6
(-TBM) 88.4 75.2 42.8 94.0 64.0 91.2 53.4 92.6
(-Linear) 83.2 64.8 69.2 68.4 73.6 78.8 71.4 73.6
MiRAGe 98.4 98.4 80.8 90.0 72.0 87.2 80.8 91.2 72.0 95.6 76.4 91.0
(-LateFusion) 99.6 97.2 90.8 70.4 85.6 73.2 90.8 75.6 85.6 86.0 88.2 76.3
(-CBMs) 98.8 99.2 78.4 84.4 67.2 77.6 78.4 91.6 67.2 94.4 72.8 87.0
(-Linears) 95.2 94.0 57.6 84.4 62.0 86.0 57.6 82.0 62.0 88.4 59.8 85.2
Table5: Class-wisedaccuracyforablationstudyonMiRAGe-Img,MiRAGe-Txt,andMiRAGe. "Real"and
"Fake"columnsrepresenttheaccuracyoftherealandfake/generatedclasses,respectively. EFandLFstandfor
EarlyFusionandLateFusion,respectively. WeleaveSDXLcolumnsblankinText-onlydetectorssincetheyshare
the same textual data as the DALL-E columns. We find that linear models and CBMs are good at classifying
differentclasses. Wealsofindthatdifferentfusiontechniquesleadtodifferentstrengthsofthemodels
egamI
txeT
txT+gmI
egamI
txeT
txT+gmIAnomaly DetailDescription
Texture Unrealistically perfect and
smoothtexture
ObjectShapes Irregularshapeorcomposition
ofobjects
SpatialRelation Illogical or impossible spatial
relationofobjectsorpeople
Face Unnaturalfacialfeaturesorex-
pressions
Fingers Incorrectnumberoffingersor
twistedormergedfingers
BodyParts Extraormissingbodypartsor
mergedordeformedbodyparts
CameraAngle Irregularorimpossiblecamera
angle
UnreadableText Unreadableormisspelledtext
BiasedLanguage Usessuspiciouslybiasedorex-
aggeratedlanguage
UnlikelyAction Hasactionthatisunlikelyorim-
possibletobeperformedbythe
subject
GenericStatement Uses generic statement that
lacksnecessarydetails
ContradictsFacts Contradictswithknownfactsor
events
Table 6: Anomalies choices provided in the human
study. Participantscanchoosemorethanonereason.
Figure9: Theuserinterfaceofthehumanstudywhere
each participant is given a pair of news images and
captionandaskedtodeterminewhethertheyarerealor
generated
egamI
noitpaCGlide DALLE2 DALLE3 SDXL Midjourney
Figure10: ComparisonofdifferentimagegeneratorsacrossexamplesfromtheMiRAGeNewsdataset. Wesee
thatmoderngeneratorssuchasMidjourneyproducehigh-qualityimagesthataredifficulttodistinguishfromreal
images.