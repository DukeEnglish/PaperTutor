[
    {
        "title": "SceneCraft: Layout-Guided 3D Scene Generation",
        "authors": "Xiuyu YangYunze ManJun-Kun ChenYu-Xiong Wang",
        "links": "http://arxiv.org/abs/2410.09049v1",
        "entry_id": "http://arxiv.org/abs/2410.09049v1",
        "pdf_url": "http://arxiv.org/pdf/2410.09049v1",
        "summary": "The creation of complex 3D scenes tailored to user specifications has been a\ntedious and challenging task with traditional 3D modeling tools. Although some\npioneering methods have achieved automatic text-to-3D generation, they are\ngenerally limited to small-scale scenes with restricted control over the shape\nand texture. We introduce SceneCraft, a novel method for generating detailed\nindoor scenes that adhere to textual descriptions and spatial layout\npreferences provided by users. Central to our method is a rendering-based\ntechnique, which converts 3D semantic layouts into multi-view 2D proxy maps.\nFurthermore, we design a semantic and depth conditioned diffusion model to\ngenerate multi-view images, which are used to learn a neural radiance field\n(NeRF) as the final scene representation. Without the constraints of panorama\nimage generation, we surpass previous methods in supporting complicated indoor\nspace generation beyond a single room, even as complicated as a whole\nmulti-bedroom apartment with irregular shapes and layouts. Through experimental\nanalysis, we demonstrate that our method significantly outperforms existing\napproaches in complex indoor scene generation with diverse textures, consistent\ngeometry, and realistic visual quality. Code and more results are available at:\nhttps://orangesodahub.github.io/SceneCraft",
        "updated": "2024-10-11 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.09049v1"
    },
    {
        "title": "MiRAGeNews: Multimodal Realistic AI-Generated News Detection",
        "authors": "Runsheng HuangLiam DuganYue YangChris Callison-Burch",
        "links": "http://arxiv.org/abs/2410.09045v1",
        "entry_id": "http://arxiv.org/abs/2410.09045v1",
        "pdf_url": "http://arxiv.org/pdf/2410.09045v1",
        "summary": "The proliferation of inflammatory or misleading \"fake\" news content has\nbecome increasingly common in recent years. Simultaneously, it has become\neasier than ever to use AI tools to generate photorealistic images depicting\nany scene imaginable. Combining these two -- AI-generated fake news content --\nis particularly potent and dangerous. To combat the spread of AI-generated fake\nnews, we propose the MiRAGeNews Dataset, a dataset of 12,500 high-quality real\nand AI-generated image-caption pairs from state-of-the-art generators. We find\nthat our dataset poses a significant challenge to humans (60% F-1) and\nstate-of-the-art multi-modal LLMs (< 24% F-1). Using our dataset we train a\nmulti-modal detector (MiRAGe) that improves by +5.1% F-1 over state-of-the-art\nbaselines on image-caption pairs from out-of-domain image generators and news\npublishers. We release our code and data to aid future work on detecting\nAI-generated content.",
        "updated": "2024-10-11 17:58:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.09045v1"
    },
    {
        "title": "Alberta Wells Dataset: Pinpointing Oil and Gas Wells from Satellite Imagery",
        "authors": "Pratinav SethMichelle LinBrefo Dwamena YawJade BoutotMary KangDavid Rolnick",
        "links": "http://arxiv.org/abs/2410.09032v1",
        "entry_id": "http://arxiv.org/abs/2410.09032v1",
        "pdf_url": "http://arxiv.org/pdf/2410.09032v1",
        "summary": "Millions of abandoned oil and gas wells are scattered across the world,\nleaching methane into the atmosphere and toxic compounds into the groundwater.\nMany of these locations are unknown, preventing the wells from being plugged\nand their polluting effects averted. Remote sensing is a relatively unexplored\ntool for pinpointing abandoned wells at scale. We introduce the first\nlarge-scale benchmark dataset for this problem, leveraging medium-resolution\nmulti-spectral satellite imagery from Planet Labs. Our curated dataset\ncomprises over 213,000 wells (abandoned, suspended, and active) from Alberta, a\nregion with especially high well density, sourced from the Alberta Energy\nRegulator and verified by domain experts. We evaluate baseline algorithms for\nwell detection and segmentation, showing the promise of computer vision\napproaches but also significant room for improvement.",
        "updated": "2024-10-11 17:49:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.09032v1"
    },
    {
        "title": "CVAM-Pose: Conditional Variational Autoencoder for Multi-Object Monocular Pose Estimation",
        "authors": "Jianyu ZhaoWei QuanBogdan J. Matuszewski",
        "links": "http://arxiv.org/abs/2410.09010v1",
        "entry_id": "http://arxiv.org/abs/2410.09010v1",
        "pdf_url": "http://arxiv.org/pdf/2410.09010v1",
        "summary": "Estimating rigid objects' poses is one of the fundamental problems in\ncomputer vision, with a range of applications across automation and augmented\nreality. Most existing approaches adopt one network per object class strategy,\ndepend heavily on objects' 3D models, depth data, and employ a time-consuming\niterative refinement, which could be impractical for some applications. This\npaper presents a novel approach, CVAM-Pose, for multi-object monocular pose\nestimation that addresses these limitations. The CVAM-Pose method employs a\nlabel-embedded conditional variational autoencoder network, to implicitly\nabstract regularised representations of multiple objects in a single\nlow-dimensional latent space. This autoencoding process uses only images\ncaptured by a projective camera and is robust to objects' occlusion and scene\nclutter. The classes of objects are one-hot encoded and embedded throughout the\nnetwork. The proposed label-embedded pose regression strategy interprets the\nlearnt latent space representations utilising continuous pose representations.\nAblation tests and systematic evaluations demonstrate the scalability and\nefficiency of the CVAM-Pose method for multi-object scenarios. The proposed\nCVAM-Pose outperforms competing latent space approaches. For example, it is\nrespectively 25% and 20% better than AAE and Multi-Path methods, when evaluated\nusing the $\\mathrm{AR_{VSD}}$ metric on the Linemod-Occluded dataset. It also\nachieves results somewhat comparable to methods reliant on 3D models reported\nin BOP challenges. Code available: https://github.com/JZhao12/CVAM-Pose",
        "updated": "2024-10-11 17:26:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.09010v1"
    },
    {
        "title": "Semantic Score Distillation Sampling for Compositional Text-to-3D Generation",
        "authors": "Ling YangZixiang ZhangJunlin HanBohan ZengRunjia LiPhilip TorrWentao Zhang",
        "links": "http://arxiv.org/abs/2410.09009v1",
        "entry_id": "http://arxiv.org/abs/2410.09009v1",
        "pdf_url": "http://arxiv.org/pdf/2410.09009v1",
        "summary": "Generating high-quality 3D assets from textual descriptions remains a pivotal\nchallenge in computer graphics and vision research. Due to the scarcity of 3D\ndata, state-of-the-art approaches utilize pre-trained 2D diffusion priors,\noptimized through Score Distillation Sampling (SDS). Despite progress, crafting\ncomplex 3D scenes featuring multiple objects or intricate interactions is still\ndifficult. To tackle this, recent methods have incorporated box or layout\nguidance. However, these layout-guided compositional methods often struggle to\nprovide fine-grained control, as they are generally coarse and lack\nexpressiveness. To overcome these challenges, we introduce a novel SDS\napproach, Semantic Score Distillation Sampling (SemanticSDS), designed to\neffectively improve the expressiveness and accuracy of compositional text-to-3D\ngeneration. Our approach integrates new semantic embeddings that maintain\nconsistency across different rendering views and clearly differentiate between\nvarious objects and parts. These embeddings are transformed into a semantic\nmap, which directs a region-specific SDS process, enabling precise optimization\nand compositional generation. By leveraging explicit semantic guidance, our\nmethod unlocks the compositional capabilities of existing pre-trained diffusion\nmodels, thereby achieving superior quality in 3D content generation,\nparticularly for complex objects and scenes. Experimental results demonstrate\nthat our SemanticSDS framework is highly effective for generating\nstate-of-the-art complex 3D content. Code:\nhttps://github.com/YangLing0818/SemanticSDS-3D",
        "updated": "2024-10-11 17:26:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.09009v1"
    }
]