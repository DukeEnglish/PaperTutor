[
    {
        "title": "Unraveling and Mitigating Safety Alignment Degradation of Vision-Language Models",
        "authors": "Qin LiuChao ShangLing LiuNikolaos PappasJie MaNeha Anna JohnSrikanth DossLluis MarquezMiguel BallesterosYassine Benajiba",
        "links": "http://arxiv.org/abs/2410.09047v1",
        "entry_id": "http://arxiv.org/abs/2410.09047v1",
        "pdf_url": "http://arxiv.org/pdf/2410.09047v1",
        "summary": "The safety alignment ability of Vision-Language Models (VLMs) is prone to be\ndegraded by the integration of the vision module compared to its LLM backbone.\nWe investigate this phenomenon, dubbed as ''safety alignment degradation'' in\nthis paper, and show that the challenge arises from the representation gap that\nemerges when introducing vision modality to VLMs. In particular, we show that\nthe representations of multi-modal inputs shift away from that of text-only\ninputs which represent the distribution that the LLM backbone is optimized for.\nAt the same time, the safety alignment capabilities, initially developed within\nthe textual embedding space, do not successfully transfer to this new\nmulti-modal representation space. To reduce safety alignment degradation, we\nintroduce Cross-Modality Representation Manipulation (CMRM), an inference time\nrepresentation intervention method for recovering the safety alignment ability\nthat is inherent in the LLM backbone of VLMs, while simultaneously preserving\nthe functional capabilities of VLMs. The empirical results show that our\nframework significantly recovers the alignment ability that is inherited from\nthe LLM backbone with minimal impact on the fluency and linguistic capabilities\nof pre-trained VLMs even without additional training. Specifically, the unsafe\nrate of LLaVA-7B on multi-modal input can be reduced from 61.53% to as low as\n3.15% with only inference-time intervention.\n  WARNING: This paper contains examples of toxic or harmful language.",
        "updated": "2024-10-11 17:59:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.09047v1"
    },
    {
        "title": "Transforming In-Vehicle Network Intrusion Detection: VAE-based Knowledge Distillation Meets Explainable AI",
        "authors": "Muhammet Anil YagizPedram MohajerAnsariMert D. PesePolat Goktas",
        "links": "http://arxiv.org/abs/2410.09043v1",
        "entry_id": "http://arxiv.org/abs/2410.09043v1",
        "pdf_url": "http://arxiv.org/pdf/2410.09043v1",
        "summary": "In the evolving landscape of autonomous vehicles, ensuring robust in-vehicle\nnetwork (IVN) security is paramount. This paper introduces an advanced\nintrusion detection system (IDS) called KD-XVAE that uses a Variational\nAutoencoder (VAE)-based knowledge distillation approach to enhance both\nperformance and efficiency. Our model significantly reduces complexity,\noperating with just 1669 parameters and achieving an inference time of 0.3 ms\nper batch, making it highly suitable for resource-constrained automotive\nenvironments. Evaluations in the HCRL Car-Hacking dataset demonstrate\nexceptional capabilities, attaining perfect scores (Recall, Precision, F1 Score\nof 100%, and FNR of 0%) under multiple attack types, including DoS, Fuzzing,\nGear Spoofing, and RPM Spoofing. Comparative analysis on the CICIoV2024 dataset\nfurther underscores its superiority over traditional machine learning models,\nachieving perfect detection metrics. We furthermore integrate Explainable AI\n(XAI) techniques to ensure transparency in the model's decisions. The VAE\ncompresses the original feature space into a latent space, on which the\ndistilled model is trained. SHAP(SHapley Additive exPlanations) values provide\ninsights into the importance of each latent dimension, mapped back to original\nfeatures for intuitive understanding. Our paper advances the field by\nintegrating state-of-the-art techniques, addressing critical challenges in the\ndeployment of efficient, trustworthy, and reliable IDSes for autonomous\nvehicles, ensuring enhanced protection against emerging cyber threats.",
        "updated": "2024-10-11 17:57:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.09043v1"
    },
    {
        "title": "SimpleStrat: Diversifying Language Model Generation with Stratification",
        "authors": "Justin WongYury OrlovskiyMichael LuoSanjit A. SeshiaJoseph E. Gonzalez",
        "links": "http://arxiv.org/abs/2410.09038v1",
        "entry_id": "http://arxiv.org/abs/2410.09038v1",
        "pdf_url": "http://arxiv.org/pdf/2410.09038v1",
        "summary": "Generating diverse responses from large language models (LLMs) is crucial for\napplications such as planning/search and synthetic data generation, where\ndiversity provides distinct answers across generations. Prior approaches rely\non increasing temperature to increase diversity. However, contrary to popular\nbelief, we show not only does this approach produce lower quality individual\ngenerations as temperature increases, but it depends on model's next-token\nprobabilities being similar to the true distribution of answers. We propose\n\\method{}, an alternative approach that uses the language model itself to\npartition the space into strata. At inference, a random stratum is selected and\na sample drawn from within the strata. To measure diversity, we introduce\nCoverageQA, a dataset of underspecified questions with multiple equally\nplausible answers, and assess diversity by measuring KL Divergence between the\noutput distribution and uniform distribution over valid ground truth answers.\nAs computing probability per response/solution for proprietary models is\ninfeasible, we measure recall on ground truth solutions. Our evaluation show\nusing SimpleStrat achieves higher recall by 0.05 compared to GPT-4o and 0.36\naverage reduction in KL Divergence compared to Llama 3.",
        "updated": "2024-10-11 17:54:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.09038v1"
    },
    {
        "title": "Mentor-KD: Making Small Language Models Better Multi-step Reasoners",
        "authors": "Hojae LeeJunho KimSangKeun Lee",
        "links": "http://arxiv.org/abs/2410.09037v1",
        "entry_id": "http://arxiv.org/abs/2410.09037v1",
        "pdf_url": "http://arxiv.org/pdf/2410.09037v1",
        "summary": "Large Language Models (LLMs) have displayed remarkable performances across\nvarious complex tasks by leveraging Chain-of-Thought (CoT) prompting. Recently,\nstudies have proposed a Knowledge Distillation (KD) approach, reasoning\ndistillation, which transfers such reasoning ability of LLMs through\nfine-tuning language models of multi-step rationales generated by LLM teachers.\nHowever, they have inadequately considered two challenges regarding\ninsufficient distillation sets from the LLM teacher model, in terms of 1) data\nquality and 2) soft label provision. In this paper, we propose Mentor-KD, which\neffectively distills the multi-step reasoning capability of LLMs to smaller LMs\nwhile addressing the aforementioned challenges. Specifically, we exploit a\nmentor, intermediate-sized task-specific fine-tuned model, to augment\nadditional CoT annotations and provide soft labels for the student model during\nreasoning distillation. We conduct extensive experiments and confirm\nMentor-KD's effectiveness across various models and complex reasoning tasks.",
        "updated": "2024-10-11 17:53:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.09037v1"
    },
    {
        "title": "PEAR: A Robust and Flexible Automation Framework for Ptychography Enabled by Multiple Large Language Model Agents",
        "authors": "Xiangyu YinChuqiao ShiYimo HanYi Jiang",
        "links": "http://arxiv.org/abs/2410.09034v1",
        "entry_id": "http://arxiv.org/abs/2410.09034v1",
        "pdf_url": "http://arxiv.org/pdf/2410.09034v1",
        "summary": "Ptychography is an advanced computational imaging technique in X-ray and\nelectron microscopy. It has been widely adopted across scientific research\nfields, including physics, chemistry, biology, and materials science, as well\nas in industrial applications such as semiconductor characterization. In\npractice, obtaining high-quality ptychographic images requires simultaneous\noptimization of numerous experimental and algorithmic parameters.\nTraditionally, parameter selection often relies on trial and error, leading to\nlow-throughput workflows and potential human bias. In this work, we develop the\n\"Ptychographic Experiment and Analysis Robot\" (PEAR), a framework that\nleverages large language models (LLMs) to automate data analysis in\nptychography. To ensure high robustness and accuracy, PEAR employs multiple LLM\nagents for tasks including knowledge retrieval, code generation, parameter\nrecommendation, and image reasoning. Our study demonstrates that PEAR's\nmulti-agent design significantly improves the workflow success rate, even with\nsmaller open-weight models such as LLaMA 3.1 8B. PEAR also supports various\nautomation levels and is designed to work with customized local knowledge\nbases, ensuring flexibility and adaptability across different research\nenvironments.",
        "updated": "2024-10-11 17:50:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.09034v1"
    }
]