[
    {
        "title": "Unraveling and Mitigating Safety Alignment Degradation of Vision-Language Models",
        "authors": "Qin LiuChao ShangLing LiuNikolaos PappasJie MaNeha Anna JohnSrikanth DossLluis MarquezMiguel BallesterosYassine Benajiba",
        "links": "http://arxiv.org/abs/2410.09047v1",
        "entry_id": "http://arxiv.org/abs/2410.09047v1",
        "pdf_url": "http://arxiv.org/pdf/2410.09047v1",
        "summary": "The safety alignment ability of Vision-Language Models (VLMs) is prone to be\ndegraded by the integration of the vision module compared to its LLM backbone.\nWe investigate this phenomenon, dubbed as ''safety alignment degradation'' in\nthis paper, and show that the challenge arises from the representation gap that\nemerges when introducing vision modality to VLMs. In particular, we show that\nthe representations of multi-modal inputs shift away from that of text-only\ninputs which represent the distribution that the LLM backbone is optimized for.\nAt the same time, the safety alignment capabilities, initially developed within\nthe textual embedding space, do not successfully transfer to this new\nmulti-modal representation space. To reduce safety alignment degradation, we\nintroduce Cross-Modality Representation Manipulation (CMRM), an inference time\nrepresentation intervention method for recovering the safety alignment ability\nthat is inherent in the LLM backbone of VLMs, while simultaneously preserving\nthe functional capabilities of VLMs. The empirical results show that our\nframework significantly recovers the alignment ability that is inherited from\nthe LLM backbone with minimal impact on the fluency and linguistic capabilities\nof pre-trained VLMs even without additional training. Specifically, the unsafe\nrate of LLaVA-7B on multi-modal input can be reduced from 61.53% to as low as\n3.15% with only inference-time intervention.\n  WARNING: This paper contains examples of toxic or harmful language.",
        "updated": "2024-10-11 17:59:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.09047v1"
    },
    {
        "title": "MiRAGeNews: Multimodal Realistic AI-Generated News Detection",
        "authors": "Runsheng HuangLiam DuganYue YangChris Callison-Burch",
        "links": "http://arxiv.org/abs/2410.09045v1",
        "entry_id": "http://arxiv.org/abs/2410.09045v1",
        "pdf_url": "http://arxiv.org/pdf/2410.09045v1",
        "summary": "The proliferation of inflammatory or misleading \"fake\" news content has\nbecome increasingly common in recent years. Simultaneously, it has become\neasier than ever to use AI tools to generate photorealistic images depicting\nany scene imaginable. Combining these two -- AI-generated fake news content --\nis particularly potent and dangerous. To combat the spread of AI-generated fake\nnews, we propose the MiRAGeNews Dataset, a dataset of 12,500 high-quality real\nand AI-generated image-caption pairs from state-of-the-art generators. We find\nthat our dataset poses a significant challenge to humans (60% F-1) and\nstate-of-the-art multi-modal LLMs (< 24% F-1). Using our dataset we train a\nmulti-modal detector (MiRAGe) that improves by +5.1% F-1 over state-of-the-art\nbaselines on image-caption pairs from out-of-domain image generators and news\npublishers. We release our code and data to aid future work on detecting\nAI-generated content.",
        "updated": "2024-10-11 17:58:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.09045v1"
    },
    {
        "title": "AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation",
        "authors": "Zijun WangHaoqin TuJieru MeiBingchen ZhaoYisen WangCihang Xie",
        "links": "http://arxiv.org/abs/2410.09040v1",
        "entry_id": "http://arxiv.org/abs/2410.09040v1",
        "pdf_url": "http://arxiv.org/pdf/2410.09040v1",
        "summary": "This paper studies the vulnerabilities of transformer-based Large Language\nModels (LLMs) to jailbreaking attacks, focusing specifically on the\noptimization-based Greedy Coordinate Gradient (GCG) strategy. We first observe\na positive correlation between the effectiveness of attacks and the internal\nbehaviors of the models. For instance, attacks tend to be less effective when\nmodels pay more attention to system prompts designed to ensure LLM safety\nalignment. Building on this discovery, we introduce an enhanced method that\nmanipulates models' attention scores to facilitate LLM jailbreaking, which we\nterm AttnGCG. Empirically, AttnGCG shows consistent improvements in attack\nefficacy across diverse LLMs, achieving an average increase of ~7% in the\nLlama-2 series and ~10% in the Gemma series. Our strategy also demonstrates\nrobust attack transferability against both unseen harmful goals and black-box\nLLMs like GPT-3.5 and GPT-4. Moreover, we note our attention-score\nvisualization is more interpretable, allowing us to gain better insights into\nhow our targeted attention manipulation facilitates more effective\njailbreaking. We release the code at\nhttps://github.com/UCSC-VLAA/AttnGCG-attack.",
        "updated": "2024-10-11 17:55:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.09040v1"
    },
    {
        "title": "SimpleStrat: Diversifying Language Model Generation with Stratification",
        "authors": "Justin WongYury OrlovskiyMichael LuoSanjit A. SeshiaJoseph E. Gonzalez",
        "links": "http://arxiv.org/abs/2410.09038v1",
        "entry_id": "http://arxiv.org/abs/2410.09038v1",
        "pdf_url": "http://arxiv.org/pdf/2410.09038v1",
        "summary": "Generating diverse responses from large language models (LLMs) is crucial for\napplications such as planning/search and synthetic data generation, where\ndiversity provides distinct answers across generations. Prior approaches rely\non increasing temperature to increase diversity. However, contrary to popular\nbelief, we show not only does this approach produce lower quality individual\ngenerations as temperature increases, but it depends on model's next-token\nprobabilities being similar to the true distribution of answers. We propose\n\\method{}, an alternative approach that uses the language model itself to\npartition the space into strata. At inference, a random stratum is selected and\na sample drawn from within the strata. To measure diversity, we introduce\nCoverageQA, a dataset of underspecified questions with multiple equally\nplausible answers, and assess diversity by measuring KL Divergence between the\noutput distribution and uniform distribution over valid ground truth answers.\nAs computing probability per response/solution for proprietary models is\ninfeasible, we measure recall on ground truth solutions. Our evaluation show\nusing SimpleStrat achieves higher recall by 0.05 compared to GPT-4o and 0.36\naverage reduction in KL Divergence compared to Llama 3.",
        "updated": "2024-10-11 17:54:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.09038v1"
    },
    {
        "title": "Mentor-KD: Making Small Language Models Better Multi-step Reasoners",
        "authors": "Hojae LeeJunho KimSangKeun Lee",
        "links": "http://arxiv.org/abs/2410.09037v1",
        "entry_id": "http://arxiv.org/abs/2410.09037v1",
        "pdf_url": "http://arxiv.org/pdf/2410.09037v1",
        "summary": "Large Language Models (LLMs) have displayed remarkable performances across\nvarious complex tasks by leveraging Chain-of-Thought (CoT) prompting. Recently,\nstudies have proposed a Knowledge Distillation (KD) approach, reasoning\ndistillation, which transfers such reasoning ability of LLMs through\nfine-tuning language models of multi-step rationales generated by LLM teachers.\nHowever, they have inadequately considered two challenges regarding\ninsufficient distillation sets from the LLM teacher model, in terms of 1) data\nquality and 2) soft label provision. In this paper, we propose Mentor-KD, which\neffectively distills the multi-step reasoning capability of LLMs to smaller LMs\nwhile addressing the aforementioned challenges. Specifically, we exploit a\nmentor, intermediate-sized task-specific fine-tuned model, to augment\nadditional CoT annotations and provide soft labels for the student model during\nreasoning distillation. We conduct extensive experiments and confirm\nMentor-KD's effectiveness across various models and complex reasoning tasks.",
        "updated": "2024-10-11 17:53:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.09037v1"
    }
]