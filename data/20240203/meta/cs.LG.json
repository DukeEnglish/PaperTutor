[
    {
        "title": "Towards Optimal Feature-Shaping Methods for Out-of-Distribution Detection",
        "authors": "Qinyu ZhaoMing XuKartik GuptaAkshay AsthanaLiang ZhengStephen Gould",
        "links": "http://arxiv.org/abs/2402.00865v1",
        "entry_id": "http://arxiv.org/abs/2402.00865v1",
        "pdf_url": "http://arxiv.org/pdf/2402.00865v1",
        "summary": "Feature shaping refers to a family of methods that exhibit state-of-the-art\nperformance for out-of-distribution (OOD) detection. These approaches\nmanipulate the feature representation, typically from the penultimate layer of\na pre-trained deep learning model, so as to better differentiate between\nin-distribution (ID) and OOD samples. However, existing feature-shaping methods\nusually employ rules manually designed for specific model architectures and OOD\ndatasets, which consequently limit their generalization ability. To address\nthis gap, we first formulate an abstract optimization framework for studying\nfeature-shaping methods. We then propose a concrete reduction of the framework\nwith a simple piecewise constant shaping function and show that existing\nfeature-shaping methods approximate the optimal solution to the concrete\noptimization problem. Further, assuming that OOD data is inaccessible, we\npropose a formulation that yields a closed-form solution for the piecewise\nconstant shaping function, utilizing solely the ID data. Through extensive\nexperiments, we show that the feature-shaping function optimized by our method\nimproves the generalization ability of OOD detection across a large variety of\ndatasets and model architectures.",
        "updated": "2024-02-01 18:59:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.00865v1"
    },
    {
        "title": "Early Time Classification with Accumulated Accuracy Gap Control",
        "authors": "Liran RingelRegev CohenDaniel FreedmanMichael EladYaniv Romano",
        "links": "http://arxiv.org/abs/2402.00857v1",
        "entry_id": "http://arxiv.org/abs/2402.00857v1",
        "pdf_url": "http://arxiv.org/pdf/2402.00857v1",
        "summary": "Early time classification algorithms aim to label a stream of features\nwithout processing the full input stream, while maintaining accuracy comparable\nto that achieved by applying the classifier to the entire input. In this paper,\nwe introduce a statistical framework that can be applied to any sequential\nclassifier, formulating a calibrated stopping rule. This data-driven rule\nattains finite-sample, distribution-free control of the accuracy gap between\nfull and early-time classification. We start by presenting a novel method that\nbuilds on the Learn-then-Test calibration framework to control this gap\nmarginally, on average over i.i.d. instances. As this algorithm tends to yield\nan excessively high accuracy gap for early halt times, our main contribution is\nthe proposal of a framework that controls a stronger notion of error, where the\naccuracy gap is controlled conditionally on the accumulated halt times.\nNumerical experiments demonstrate the effectiveness, applicability, and\nusefulness of our method. We show that our proposed early stopping mechanism\nreduces up to 94% of timesteps used for classification while achieving rigorous\naccuracy gap control.",
        "updated": "2024-02-01 18:54:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.00857v1"
    },
    {
        "title": "SymbolicAI: A framework for logic-based approaches combining generative models and solvers",
        "authors": "Marius-Constantin DinuClaudiu Leoveanu-CondreiMarkus HolzleitnerWerner ZellingerSepp Hochreiter",
        "links": "http://arxiv.org/abs/2402.00854v1",
        "entry_id": "http://arxiv.org/abs/2402.00854v1",
        "pdf_url": "http://arxiv.org/pdf/2402.00854v1",
        "summary": "We introduce SymbolicAI, a versatile and modular framework employing a\nlogic-based approach to concept learning and flow management in generative\nprocesses. SymbolicAI enables the seamless integration of generative models\nwith a diverse range of solvers by treating large language models (LLMs) as\nsemantic parsers that execute tasks based on both natural and formal language\ninstructions, thus bridging the gap between symbolic reasoning and generative\nAI. We leverage probabilistic programming principles to tackle complex tasks,\nand utilize differentiable and classical programming paradigms with their\nrespective strengths. The framework introduces a set of polymorphic,\ncompositional, and self-referential operations for data stream manipulation,\naligning LLM outputs with user objectives. As a result, we can transition\nbetween the capabilities of various foundation models endowed with zero- and\nfew-shot learning capabilities and specialized, fine-tuned models or solvers\nproficient in addressing specific problems. In turn, the framework facilitates\nthe creation and evaluation of explainable computational graphs. We conclude by\nintroducing a quality measure and its empirical score for evaluating these\ncomputational graphs, and propose a benchmark that compares various\nstate-of-the-art LLMs across a set of complex workflows. We refer to the\nempirical score as the \"Vector Embedding for Relational Trajectory Evaluation\nthrough Cross-similarity\", or VERTEX score for short. The framework codebase\nand benchmark are linked below.",
        "updated": "2024-02-01 18:50:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.00854v1"
    },
    {
        "title": "LTAU-FF: Loss Trajectory Analysis for Uncertainty in Atomistic Force Fields",
        "authors": "Joshua A. VitaAmit SamantaFei ZhouVincenzo Lordi",
        "links": "http://arxiv.org/abs/2402.00853v1",
        "entry_id": "http://arxiv.org/abs/2402.00853v1",
        "pdf_url": "http://arxiv.org/pdf/2402.00853v1",
        "summary": "Model ensembles are simple and effective tools for estimating the prediction\nuncertainty of deep learning atomistic force fields. Despite this, widespread\nadoption of ensemble-based uncertainty quantification (UQ) techniques is\nlimited by the high computational costs incurred by ensembles during both\ntraining and inference. In this work we leverage the cumulative distribution\nfunctions (CDFs) of per-sample errors obtained over the course of training to\nefficiently represent the model ensemble, and couple them with a distance-based\nsimilarity search in the model latent space. Using these tools, we develop a\nsimple UQ metric (which we call LTAU) that leverages the strengths of\nensemble-based techniques without requiring the evaluation of multiple models\nduring either training or inference. As an initial test, we apply our method\ntowards estimating the epistemic uncertainty in atomistic force fields\n(LTAU-FF) and demonstrate that it can be easily calibrated to accurately\npredict test errors on multiple datasets from the literature. We then\nillustrate the utility of LTAU-FF in two practical applications: 1) tuning the\ntraining-validation gap for an example dataset, and 2) predicting errors in\nrelaxation trajectories on the OC20 IS2RS task. Though in this work we focus on\nthe use of LTAU with deep learning atomistic force fields, we emphasize that it\ncan be readily applied to any regression task, or any ensemble-generation\ntechnique, to provide a reliable and easy-to-implement UQ metric.",
        "updated": "2024-02-01 18:50:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.00853v1"
    },
    {
        "title": "Data Augmentation Scheme for Raman Spectra with Highly Correlated Annotations",
        "authors": "Christoph LangeIsabel ThieleLara SantolinSebastian L. RiedelMaxim BorisyakPeter NeubauerM. Nicolas Cruz Bournazou",
        "links": "http://arxiv.org/abs/2402.00851v1",
        "entry_id": "http://arxiv.org/abs/2402.00851v1",
        "pdf_url": "http://arxiv.org/pdf/2402.00851v1",
        "summary": "In biotechnology Raman Spectroscopy is rapidly gaining popularity as a\nprocess analytical technology (PAT) that measures cell densities, substrate-\nand product concentrations. As it records vibrational modes of molecules it\nprovides that information non-invasively in a single spectrum. Typically,\npartial least squares (PLS) is the model of choice to infer information about\nvariables of interest from the spectra. However, biological processes are known\nfor their complexity where convolutional neural networks (CNN) present a\npowerful alternative. They can handle non-Gaussian noise and account for beam\nmisalignment, pixel malfunctions or the presence of additional substances.\nHowever, they require a lot of data during model training, and they pick up\nnon-linear dependencies in the process variables. In this work, we exploit the\nadditive nature of spectra in order to generate additional data points from a\ngiven dataset that have statistically independent labels so that a network\ntrained on such data exhibits low correlations between the model predictions.\nWe show that training a CNN on these generated data points improves the\nperformance on datasets where the annotations do not bear the same correlation\nas the dataset that was used for model training. This data augmentation\ntechnique enables us to reuse spectra as training data for new contexts that\nexhibit different correlations. The additional data allows for building a\nbetter and more robust model. This is of interest in scenarios where large\namounts of historical data are available but are currently not used for model\ntraining. We demonstrate the capabilities of the proposed method using\nsynthetic spectra of Ralstonia eutropha batch cultivations to monitor\nsubstrate, biomass and polyhydroxyalkanoate (PHA) biopolymer concentrations\nduring of the experiments.",
        "updated": "2024-02-01 18:46:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.00851v1"
    }
]