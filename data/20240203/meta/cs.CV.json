[
    {
        "title": "AToM: Amortized Text-to-Mesh using 2D Diffusion",
        "authors": "Guocheng QianJunli CaoAliaksandr SiarohinYash KantChaoyang WangMichael VasilkovskyHsin-Ying LeeYuwei FangIvan SkorokhodovPeiye ZhuangIgor GilitschenskiJian RenBernard GhanemKfir AbermanSergey Tulyakov",
        "links": "http://arxiv.org/abs/2402.00867v1",
        "entry_id": "http://arxiv.org/abs/2402.00867v1",
        "pdf_url": "http://arxiv.org/pdf/2402.00867v1",
        "summary": "We introduce Amortized Text-to-Mesh (AToM), a feed-forward text-to-mesh\nframework optimized across multiple text prompts simultaneously. In contrast to\nexisting text-to-3D methods that often entail time-consuming per-prompt\noptimization and commonly output representations other than polygonal meshes,\nAToM directly generates high-quality textured meshes in less than 1 second with\naround 10 times reduction in the training cost, and generalizes to unseen\nprompts. Our key idea is a novel triplane-based text-to-mesh architecture with\na two-stage amortized optimization strategy that ensures stable training and\nenables scalability. Through extensive experiments on various prompt\nbenchmarks, AToM significantly outperforms state-of-the-art amortized\napproaches with over 4 times higher accuracy (in DF415 dataset) and produces\nmore distinguishable and higher-quality 3D outputs. AToM demonstrates strong\ngeneralizability, offering finegrained 3D assets for unseen interpolated\nprompts without further optimization during inference, unlike per-prompt\nsolutions.",
        "updated": "2024-02-01 18:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.00867v1"
    },
    {
        "title": "We're Not Using Videos Effectively: An Updated Domain Adaptive Video Segmentation Baseline",
        "authors": "Simar KareerVivek VijaykumarHarsh MaheshwariPrithvijit ChattopadhyayJudy HoffmanViraj Prabhu",
        "links": "http://arxiv.org/abs/2402.00868v1",
        "entry_id": "http://arxiv.org/abs/2402.00868v1",
        "pdf_url": "http://arxiv.org/pdf/2402.00868v1",
        "summary": "There has been abundant work in unsupervised domain adaptation for semantic\nsegmentation (DAS) seeking to adapt a model trained on images from a labeled\nsource domain to an unlabeled target domain. While the vast majority of prior\nwork has studied this as a frame-level Image-DAS problem, a few Video-DAS works\nhave sought to additionally leverage the temporal signal present in adjacent\nframes. However, Video-DAS works have historically studied a distinct set of\nbenchmarks from Image-DAS, with minimal cross-benchmarking. In this work, we\naddress this gap. Surprisingly, we find that (1) even after carefully\ncontrolling for data and model architecture, state-of-the-art Image-DAS methods\n(HRDA and HRDA+MIC)} outperform Video-DAS methods on established Video-DAS\nbenchmarks (+14.5 mIoU on Viper$\\rightarrow$CityscapesSeq, +19.0 mIoU on\nSynthia$\\rightarrow$CityscapesSeq), and (2) naive combinations of Image-DAS and\nVideo-DAS techniques only lead to marginal improvements across datasets. To\navoid siloed progress between Image-DAS and Video-DAS, we open-source our\ncodebase with support for a comprehensive set of Video-DAS and Image-DAS\nmethods on a common benchmark. Code available at\nhttps://github.com/SimarKareer/UnifiedVideoDA",
        "updated": "2024-02-01 18:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.00868v1"
    },
    {
        "title": "Towards Optimal Feature-Shaping Methods for Out-of-Distribution Detection",
        "authors": "Qinyu ZhaoMing XuKartik GuptaAkshay AsthanaLiang ZhengStephen Gould",
        "links": "http://arxiv.org/abs/2402.00865v1",
        "entry_id": "http://arxiv.org/abs/2402.00865v1",
        "pdf_url": "http://arxiv.org/pdf/2402.00865v1",
        "summary": "Feature shaping refers to a family of methods that exhibit state-of-the-art\nperformance for out-of-distribution (OOD) detection. These approaches\nmanipulate the feature representation, typically from the penultimate layer of\na pre-trained deep learning model, so as to better differentiate between\nin-distribution (ID) and OOD samples. However, existing feature-shaping methods\nusually employ rules manually designed for specific model architectures and OOD\ndatasets, which consequently limit their generalization ability. To address\nthis gap, we first formulate an abstract optimization framework for studying\nfeature-shaping methods. We then propose a concrete reduction of the framework\nwith a simple piecewise constant shaping function and show that existing\nfeature-shaping methods approximate the optimal solution to the concrete\noptimization problem. Further, assuming that OOD data is inaccessible, we\npropose a formulation that yields a closed-form solution for the piecewise\nconstant shaping function, utilizing solely the ID data. Through extensive\nexperiments, we show that the feature-shaping function optimized by our method\nimproves the generalization ability of OOD detection across a large variety of\ndatasets and model architectures.",
        "updated": "2024-02-01 18:59:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.00865v1"
    },
    {
        "title": "ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields",
        "authors": "Jiahua DongYu-Xiong Wang",
        "links": "http://arxiv.org/abs/2402.00864v1",
        "entry_id": "http://arxiv.org/abs/2402.00864v1",
        "pdf_url": "http://arxiv.org/pdf/2402.00864v1",
        "summary": "We introduce ViCA-NeRF, the first view-consistency-aware method for 3D\nediting with text instructions. In addition to the implicit neural radiance\nfield (NeRF) modeling, our key insight is to exploit two sources of\nregularization that explicitly propagate the editing information across\ndifferent views, thus ensuring multi-view consistency. For geometric\nregularization, we leverage the depth information derived from NeRF to\nestablish image correspondences between different views. For learned\nregularization, we align the latent codes in the 2D diffusion model between\nedited and unedited images, enabling us to edit key views and propagate the\nupdate throughout the entire scene. Incorporating these two strategies, our\nViCA-NeRF operates in two stages. In the initial stage, we blend edits from\ndifferent views to create a preliminary 3D edit. This is followed by a second\nstage of NeRF training, dedicated to further refining the scene's appearance.\nExperimental results demonstrate that ViCA-NeRF provides more flexible,\nefficient (3 times faster) editing with higher levels of consistency and\ndetails, compared with the state of the art. Our code is publicly available.",
        "updated": "2024-02-01 18:59:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.00864v1"
    },
    {
        "title": "Geometry Transfer for Stylizing Radiance Fields",
        "authors": "Hyunyoung JungSeonghyeon NamNikolaos SarafianosSungjoo YooAlexander Sorkine-HornungRakesh Ranjan",
        "links": "http://arxiv.org/abs/2402.00863v1",
        "entry_id": "http://arxiv.org/abs/2402.00863v1",
        "pdf_url": "http://arxiv.org/pdf/2402.00863v1",
        "summary": "Shape and geometric patterns are essential in defining stylistic identity.\nHowever, current 3D style transfer methods predominantly focus on transferring\ncolors and textures, often overlooking geometric aspects. In this paper, we\nintroduce Geometry Transfer, a novel method that leverages geometric\ndeformation for 3D style transfer. This technique employs depth maps to extract\na style guide, subsequently applied to stylize the geometry of radiance fields.\nMoreover, we propose new techniques that utilize geometric cues from the 3D\nscene, thereby enhancing aesthetic expressiveness and more accurately\nreflecting intended styles. Our extensive experiments show that Geometry\nTransfer enables a broader and more expressive range of stylizations, thereby\nsignificantly expanding the scope of 3D style transfer.",
        "updated": "2024-02-01 18:58:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.00863v1"
    }
]