Can Large Language Models Understand Context?
YilunZhu1∗,JoelRubenAntonyMoniz2,ShrutiBhargava2,JiaruiLu2
DhivyaPiraviperumal2,SiteLi2,YuanZhang2,HongYu2,Bo-HsiangTseng2
1DepartmentofLinguistics,GeorgetownUniversity
2Apple
yz565@georgetown.edu
{joelrubenantony_moniz, shruti_bhargava, jiarui_lu, dhivyaprp}@apple.com
{site_li, yzhang73, hong_yu, bohsiang_tseng}@apple.com
Abstract tifying carry-over information for the following
context(dialoguestatetracking),andrecognizing
Understandingcontextiskeytounderstanding
discourse-specificphenomena(ellipsis).
humanlanguage,anabilitywhichLargeLan-
LLMshavegarneredsubstantialattentionfrom
guageModels(LLMs)havebeenincreasingly
seen to demonstrate to an impressive extent. bothacademiaandtheindustryduetotheirremark-
However, thoughtheevaluationofLLMsen- able capability in comprehending language and
compasses various domains within the realm worldknowledge. Theirunparalleledperformance
ofNaturalLanguageProcessing,limitedatten- acrossadiverserangeofbenchmarksanddatasets
tion has been paid to probing their linguistic
has firmly established their significance in a rel-
capabilityofunderstandingcontextualfeatures.
atively short period of time. As LLMs continue
This paper introduces a context understand-
topushtheboundariesofscaleandcapability,the
ingbenchmarkbyadaptingexistingdatasetsto
evaluationoftheirmultifacetedabilitiesbecomes
suittheevaluationofgenerativemodels. This
benchmarkcomprisesoffourdistincttasksand anequallyvitalendeavor. Consequently,thedevel-
ninedatasets,allfeaturingpromptsdesignedto opmentofrobustevaluationmethodologiestoas-
assessthemodels’abilitytounderstandcontext. sessspecificaspectsofLLMsbecomesimperative.
First,weevaluatetheperformanceofLLMsun-
Inaddition,thesemethodologiesshouldfocuson
derthein-contextlearningpretrainingscenario.
helpingachieveacomprehensiveunderstandingof
Experimentalresultsindicatethatpre-trained
theiradvancementwhileclearlydelineatingtheir
densemodelsstrugglewithunderstandingmore
limitations. However, recently published LLMs,
nuancedcontextualfeatureswhencomparedto
state-of-the-artfine-tunedmodels. Second,as such as OPT (Zhang et al., 2022), LLaMA (Tou-
LLMcompressionholdsgrowingsignificance vronetal.,2023)andGPT-4(OpenAI,2023),are
in both research and real-world applications, onlyevaluatedonlimitedbenchmarks,andhavea
we assess the context understanding of quan- significantdrawback: theyneglecttheinclusionof
tizedmodelsunderin-context-learningsettings.
discourse-related datasets for evaluation, thereby
We find that 3-bit post-training quantization
limitingthecomprehensiveassessmentoftheirlan-
leadstovaryingdegreesofperformancereduc-
guageunderstandingcapabilities.
tiononourbenchmark. Weconductanexten-
Toprovideacomprehensiveevaluation,plenty
siveanalysisofthesescenariostosubstantiate
ourexperimentalresults.1 of benchmarks and datasets address various
facetsoflanguageunderstanding,includingbench-
1 Introduction
marks that delve into common sense knowledge
Discourseunderstanding,asoneofthefundamen- (Hendrycksetal.,2021a;Kwiatkowskietal.,2019),
talproblemsinNLP,focusesonmodelinglinguis- aswellaslinguisticcapabilitieslikesentimentanal-
tic features and structures that go beyond indi- ysis, natural language inference, summarization,
vidual sentences (Joty et al., 2019). Understand- text classification, and more (Bang et al., 2023b;
ing discourse requires resolving the relations be- Liangetal.,2022). Thesegeneralbenchmarksand
tweenwords/phrases(coreferenceresolution)and specificdatasetevaluationsexhibitcertainlimita-
discourseunits(discourseparsinganddiscoursere- tions. Despitetherequirementforcontextualinfor-
lationclassification)inthepreviouscontext,iden- mation in these benchmarks to effectively tackle
tasks(forexample,sentimentanalysisrequiresan
∗WorkperformedduringaninternshipatApple.
understandingofpolaritieswithinthegiventext),
1Thecodeispubliclyavailableathttps://github.com/
apple/ml-llm-contextualization-eval. none of these benchmarks cater to tasks that de-
4202
beF
1
]LC.sc[
1v85800.2042:viXraCoreference ImplicitDiscourse
DialogueStateTracking QueryRewriting
Resolution RelationClassification
Nominal&eventual reference Relations between discourse units Entity carryover within context Ellipsis and reference
WSC273 PDTB-3 MultiWoz MuDoCo
OntoNotes QReCC
InCar
Doc Arg1 GECOR
I pn e t oh pe l es u hm avm e e lor no gf 2 b0 e0 e5 n, l a o op kic intu g r fe o rth wa at r d A stn a nan dc s i ae mnt i dst to hn ee f ic eh ldu src ,xh CANARD
to started emerging with frequency in
various major Hong Kong media. With I am looking for a
t ch ae rtir o u on ni q imue a gc eh sa r om n, c t eh ae gs ae i nw ce all u- sk en do wn A thr eg s2 ound of bells cascading t gr oa ii nn g t h toa t C l ae mav be rs i do gn e T .hursday U Bs oe tr :: F oT rry b eto s r ae ta Wch a sF ho ir nb ge ts o nn o Pw os. t? Or
H ato ten ng
t
iK oo nn .g
..
to be a focus of worldwide f fr ao itm
hf
i uts
l t
t oo ew ve er n, sc oa nlli gn .xg
x
the F Uo sr eb re :s o Pf uP bu lb isli hs ih ni gn g DD ivi iv si is oi no .n?
Clusters
Rewrite
{Hong Kong, Hong Kong} Relation Label train-day: Thursday Forbes of Publishing
{their, these well - known Temporal train-destination: Cambridge Division.
cartoon images}
Figure1: Tasksanddatasetsinthecontextunderstandingbenchmark.
mandanuancedcomprehensionoflinguisticfea- capabilityoftheLLMs. Thecontributionsofthis
tureswithinaprovidedcontext. papercanbesummarizedasfollows:
On the other hand, recent LLMs, by virtue of
• Ourworkintroducesacontextualunderstand-
possessingbillionsofparameters,haveledtoanex-
ingbenchmark, includingfourtasks, forthe
ponentialsurgeincomputationalandstoragecosts
evaluationofLLMs. Wealsopresentprompts
(Brown et al., 2020b), which hinders the deploy-
designedforin-contextlearningoneachtask.
ment of large models to personal devices and re-
strictstheon-deviceperformanceoflanguageun-
• WeevaluateLLMsofvaryingsizesfromdif-
derstandingtasks. Toaddressthischallenge,model
ferentmodelfamiliesandprovideananalysis
compressionmethods,whichcanreducememory
onthesemodels’capabilityforcontextunder-
anddiskrequirementsofbothmodeltrainingand
standing.
inference,havegainedattention. Existingcompres-
siontechniques,suchas3-bitquantization(Frantar
• Weevaluatepost-trainingcompressedmodels
etal.,2022),havedemonstratedthepotentialtore-
inICLsettingsandconductananalysisofthe
ducemodelsizeswithonlymarginalperformance
reductionincontextunderstandingcapability
trade-offs. However, the evaluation of quantiza-
comparedtodensemodels.
tionmethodssuffersfromtwodeficiencies. Firstly,
quantization methods are primarily evaluated on 2 RelatedWork
limitedbenchmarksanddatasets,suchasLambada
2.1 In-contextLearningEvaluation
(Papernoetal.,2016),ARC(Boratkoetal.,2018),
PIQA(TataandPatel,2003),BoolQ(Clarketal., The paradigm of ICL (Brown et al., 2020a) is
2019),andStoryCloze(Mostafazadehetal.,2017). rapidlygainingimportance. Studieshavedemon-
Itisnotyetclearwhetherlarge,compressedmod- strated that the generalization of LLMs to var-
elsout-orunder-performtheirsmallercounterparts ious downstream NLP tasks, such as MMLU
when understanding context. Secondly, previous (Hendrycksetal.,2021b),issignificantlyenhanced
work has not delved into a linguistic analysis to when provided with a small number of examples
identifywherethemodelefficacywanes. asprompts(Brownetal.,2020a;Chowdheryetal.,
Giventheaboveshortcomings,thispaperevalu- 2022;Hoffmannetal.,2022;Raeetal.,2022;Anil
atesLLMsonacontextunderstandingbenchmark et al., 2023; Touvron et al., 2023; OpenAI, 2022,
constructed from varied discourse understanding 2023). Recentresearchhasextensivelyevaluated
datasets. WeconductanextensiveanalysisofLLM the performance of LLMs across a spectrum of
performanceonthisbenchmark,includingmodels language-relatedtasks,spanningfromtextgenera-
of varying sizes and those subjected to compres- tiontounderstandinginputsequences. Thisassess-
sion techniques, aiming to provide a more com- mentcontainsawidearrayofbenchmarks,includ-
prehensiveunderstandingofcontextunderstanding ingSUPER-GLUE(Wangetal.,2019;Laskaretal.,
…
…2023), and tasks such as question answering, in- Instruction:Pleasecarefullyreadthefollowingpassages.
Foreachpassageandtheoptions,youmustidentifywhich
formationretrieval,sentimentanalysis(Bangetal.,
option the mention marked in *bold* refers to. If the
2023b; Liang et al., 2022), dialogue (Heck et al.,
markedmentiondoesnothaveanyantecedent,pleasese-
2023),andtextclassification(YangandMenczer, lect“noantecedent”.
Context:...Toexpress*its*determination...theChinese
2023).
securitiesregulatorydepartment...thisstockreform...
Choices:
2.2 ModelCompressionforLLMs A.noantecedent
B.theChinesesecuritiesregulatorydepartment
Modelcompressiontechniquescanbebroadlycat- C.thisstockreform
egorizedintothreemainapproaches: compression ...
Question:Whatdoes*its*referto?
duringtraining,compressionassociatedwithfine-
Answer:B
tuning, and post-training methods. In terms of
quantizationduringtraining,thistechniqueenables Table1: AnOntoNotesexampleofpromptandanswer.
LLMstoadapttolow-precisionrepresentationsdur-
ingthetrainingprocess(Liuetal.,2023). Model
an ambiguous pronoun and select the referent of
compressionwithfine-tuninginvolvesquantization
that pronoun from two choices. OntoNotes is a
awareness into the fine-tuning stage (Kim et al.,
human-annotatedcorpusofdocumentsannotated
2023; Dettmers et al., 2023). Post-training tech-
with multiple layers of linguistic information in-
niques,ontheotherhand,areappliedafterthecom-
cludingsyntax,propositions,namedentities,word
pletion of an LLMs training phase and typically
sense, and in-document coreference. As it is one
involvetheuseofcalibrationdata. Thiscategory
of the most frequently used datasets for training
comprisestwoprimaryapproaches: pruning,which
coreference models, prior research has achieved
removesredundantornon-salientweightstoinduce
significantadvancementsunderthesupervisedfine-
sparsity(FrantarandAlistarh,2023),andquantiza-
tuningparadigm(Leeetal.,2017;Joshietal.,2020;
tion,whichemployslow-precisionnumericrepre-
Bohnetetal.,2023). However,thesemodeldesigns
sentationsofweightsandactivations(Nageletal.,
cannotbeextendedtogenerativemodelsunderICL
2020;Frantaretal.,2022;Yuanetal.,2023). Prior
settings. Recently,LeandRitter(2023)havelever-
researchshowsthatquantizationoutperformsprun-
ageddocumenttemplatesforLLMs;however,their
inginseveralsettings(Kuzminetal.,2023),thus
evaluationisconfinedtoprominentmodelssuchas
inthiswork,wefocusonmodelquantizationand
InstructGPT(Ouyangetal.,2022),neglectingthe
itsimpactontheselectedcontext-awaretasks.
factthatsmallermodelslackthegenerativecapac-
ityrequiredtoaccomplishsuchtasks. Duetothese
3 TaskSelection&Design
limitations, we propose a novel multiple-choice
Ourcontextualunderstandingbenchmarkincludes task design. In this design, we provide the men-
fourtaskswithninedatasets,aspresentedinFigure tionsandevaluatethemodelonresolution. Each
1. In the following sections, we provide detailed optionrepresentsapotentiallymarkablespan.2 Ta-
explanations of each task and the corresponding
ble1presentsanexampleoftheinputtothemodel3.
datasets,alongwiththedesignedpromptsforICL Theentirepromptconsistsoffiveparts: (1)anin-
evaluations. struction that provides guidance to the model for
thetask,(2)adocumentcontainingplaintextwith
3.1 CoreferenceResolution a selected mention span highlighted using a bold
symbol, (3) a list of choices, which includes all
Thecoreferenceresolution(CR)taskcontributesto
the gold mentions present in the document, (4) a
achievingacoherentunderstandingoftheoverall
questionthatdirectsthemodel’sattention,and(5)
meaningconveyedwithinthetext. Thus,itplaysa
a guiding word answer that prompts for the out-
criticalroleindivingintolanguagemodels’capa-
put. Weexperimentwithmultipleinstructionsand
bilitytograspcoreferencerelationsaswellascon-
promptsandprovidetheonewiththebestperfor-
textualnuanceswithindocuments. Weselecttwo
mance. Linkingscoresarecomputedforeachques-
coreference datasets: WSC273 (Levesque et al.,
2012)andOntoNotes5.0(Pradhanetal.,2013). 2Consideringtheinferiorperformanceofsmallmodelson
WSC273,whichcontainsthefirst273examples thementiondetectiontask,weutilizegoldmarkablespans
coreferencelinking.
fromtheWinogradSchemaChallenge,isadataset
3Detailedexamplesforeachtaskdesigncanbefoundin
that requires the system to read a sentence with AppendixA.Ontology: Instruction:Giventwoargumentsandalistofconnective
{“slots”:{“restaurant-pricerange”:“pricebudgetforthe words,pleaseselectthemostlikelyconnectivebetween
restaurant”,...}, twoarguments.
“categorical”:{“restaurant-pricerange”:[‘cheap’,‘expen- [RelationDescription]
sive’,‘moderate’],...}} Input:
Instruction: Now consider the following dialogue be- Arg1:Amcore,alsoabankholdingcompany,hasassets
tweentwopartiescalledthe“system”and“user”.Canyou of$1.06billion.
tellmewhichofthe“slot”wasupdatedbythe“user”in Arg2:Central’sassetsare$240million.
itslatestresponsetothe“system”?Presenttheupdatesin Question:Whatistheconnectivethatbestdescribesthe
JSONformat.Ifno“slots”wereupdates,returnanempty relationbetweentwoarguments?
JSONlist. Ifyouencounter“slot”thatwasrequestedby Choices:
the“user”thenfillthemwith“?”.Ifauserdoesnotseem A.TemporalB.ContingencyC.ComparisonD.Expansion
tocareaboutadiscussed“slot”fillitwith“dontcare”. Answer:C
[PreviousDialogueState]
[Conversation]: Table3: APDTBexampleofpromptandanswer.
“system”:“”
“user”:“I’mlookingforamoderatelypricedplacetoeat
that’sinthecentreoftown.”
wereportjointgoalaccuracy(JGA)(Mrkšic´ etal.,
Output: {“restaurant-pricerange”: “moderate”,
“restaurant-area”:“centre”} 2017)forevaluatingtheperformanceofDST.
Table2: ADSTexampleofpromptandanswer. 3.3 ImplicitDiscourseRelationClassification
Discoursedemonstratesitsimportancebeyondin-
tionandtheresultsaresubsequentlyaggregatedfor dividual sentences, which emphasizes the ways
evaluation. Weutilizetheofficialevaluationmet- inwhichdifferentsegmentsofatextinterconnect
rics from the CoNLL-2012 shared task (Pradhan andstructurethemselvestoconveyacoherentand
etal.,2012),whichemploystheCoNLLF1score, meaningfulmessage. ThePDTB-3corpus,asintro-
derived from the averaging of three coreference ducedbyWebberetal.(2019),annotatesimplicit
metrics: MUC,B3,andCEAF . discourse relations across elementary discourse
ϕ4
units(EDUs)4. Theserelationsimplyconnections
3.2 DialogueStateTracking between EDUs and may be made explicit by in-
Dialoguestatetracking(DST)isanimportanttask serting a connective. Within the context of the
intheareaoftask-orienteddialogue(TOD)model- understandingbenchmark,weoptfortheimplicit
ing(Youngetal.,2013),wherethedialogueagent discourse relation classification task for two pri-
tracksthekeyinformationprovidedbytheuseras maryreasons. Firstly,theorderofthetwoEDUsis
theconversationprogresses. Table 2providesan provided,enablingthemodeltodirectlyutilizethis
example from MultiWOZ (Budzianowski et al., information. Secondly, the connective triggering
2018) where the user expresses the constraints therelationisimplicit,increasingthetask’scom-
whenlookingforarestaurant. TheoutputofDST plexity. In this task, two EDUs are fed as input,
istypicallymaintainedinslot-valuepairformat. and the objective is to correctly identify the rela-
PreviousresearchhasexploredICLcapabilities tionbetweenthem. Duetothenuanceddifferences
onMultiWOZanddemonstratedpromisingresults betweeneachrelationandthedemandforannota-
compared to fine-tuning models (Hu et al., 2022; torswithrichlinguisticknowledgeandextensive
Heck et al., 2023). However, these studies either annotation training, the classification task poses
involvepartialtrainingorareuntestedwithsmaller challengestofine-tunedclassificationmodels.
and quantized models. Here we adopt a straight- ThePDTB3corpusclassifiesdiscourserelations
forwardandsimplifiedICLapproachproposedby into four categories - Temporal, Contingency,
Hecketal.(2023),andtestitonMultiWOZv2.2 Comparison,andExpansion. Weconvertthistask
(Zangetal.,2020). Theprompttothemodelcon- into a multiple-choice question and experiment
sists of domain knowledge from ontology, an in- withclassesasoptions. Intheclassesscenario,the
struction,previousdialoguestate(thebeliefstate taskoffersfouroptions,eachrepresentingadistinct
accumulateduntiltheprevioususerturn)andthe discourserelationclass. Table3exhibitsthecom-
conversation proceeding to the current turn. The ponents of the prompt. It includes an instruction
ontology could be lengthy if considering all do- atthebeginning,followedbyaconcisedescription
mainsinthedataset. Thus,giventheinputlength ofeachrelation, acontextwithtwoarguments, a
constraintofLLMs,onlytheknowledgerelevantto
4EDUreferstothesmallestsegmentofatextthatconveys
theconversationisprovided. Followingliterature, acompleteandcoherentmeaningwithinlargerdiscourse.Instruction:Rewritethelastqueryfollowinginteraction 4 Experiments
intoawell-formed,contextindependentquery. Resolve
anydisfluenciesorgrammaticalerrorsinthequery. 4.1 ImplementationDetails
Input:
User:TrytoreachForbesnow. Evaluationwasconductedonacomputationalin-
Bot:ForbesatWashingtonPost?OrForbesofPublishing
frastructurecomprising8×A100GPUs. Weex-
Division?
User:PublishingDivision. periment with three model families. For smaller
Rewrite:ForbesofPublishingDivision models, we consider OPT (Zhang et al., 2022),
ranging from 125M to 2.7B. Although OPT also
Table4: Aqueryrewritingexampleofpromptandan-
offerslargermodels,weoptforLLaMA(Touvron
swer.
etal.,2023)asthemid-sizedLMs,spanningfrom
7Bto65Bparameters,duetoshowcasedsuperior
questionalongwithanswerchoices,andatrigger performancebypriorworks. Forlarge-scaleLMs,
word. Weevaluateeachmodel’sperformanceon we leverage GPT-3.5-turbo5. For each model,
thisdatasetusingaccuracyasthemetric. oneverydataset,weassessfivedifferentsettings:
zero-shot, one-shot, 5-shot, 8-shot, and 10-shot.
3.4 QueryRewriting Werandomlyselecttheexamplesfromthetraining
setforthefew-shotprompting.6
While document-based CR (OntoNotes, Section
3.1)coversvarioustypesofcoreferencerelations 4.2 DenseModel
acrossmultiplegenres,itdoesnotallowtheability
Results of the three model families are reported
toevaluatecertainaspectswhichareimportantto
in Table 5, along with results of fine-tuned (FT)
understandcontext. Firstly,theCRtasktypically
models to help better interpret how well the pre-
focusesondocument-basedcoreferencechains,ne-
trainedmodelsbehavewithICL.Figure2alsovi-
glectingmentionresolutionindialogues. Secondly,
sualizesthegapbetweenvariouscommercial/non-
ellipsis,whichistheomissionofoneormorewords
commerciallanguagemodelsandfine-tuningmod-
fromaclause,isacruciallinguisticphenomenon
elsthatachievethebestperformanceonthesetasks.
inspeechandconversation. Itisessentialforlan-
Foreach,wepresenttheN-shotsettingthatyields
guagemodelstograspandaccuratelyidentifyel-
the highest score (see Appendix B for details).
lipseswithincontext. Incorporatingthesefeatures
Overall,performanceimprovesasthemodelsize
intothebenchmarkisthuspivotalwhenevaluating
increasesandpre-trainedmodelswithICLstruggle
contextunderstanding.
tocatchupwithFTmodelsonmosttasks.
QueryRewriting(QR)isataskofrewritingthe
last utterance of a user in a conversation into a CoreferenceResolution Largermodelsexhibit
context-free,independentutterancethatcanbein- promisingperformanceontheWSC273task,indi-
terpreted without dialog context. It requires the catingthatLLMscaneffectivelyhandle"simple"
model to identify the entity or events references coreference relations within limited contexts and
fromcontextandfurthergenerateacompleteutter- mentions. However, whenitcomestodocument-
ancewithresolvedcoreferenceorellipsis. basedCRwithcomplexclusters,theirperformance
WeincorporatefiveQRdatasetsintheproposed substantially drops 7. Even on providing ground-
benchmark: MuDoCowithQRannotations(Martin truthmentions,thehighest-performingGPTisonly
etal.,2020;Tsengetal.,2021),QReCC(Anantha onparwithrule-basedcoreferencesystems(Man-
etal.,2021),InCar(Reganetal.,2019),GECOR ning et al., 2014) and is far from the end-to-end
(Quanetal.,2019),andCANARD(Elgoharyetal., fine-tunedSpanBERT(Joshietal.,2020). Thegap
2019). These datasets span multiple genres and
5https://platform.openai.com/docs/models/
domainsindialogues. Weexperimentwithvarious
gpt-3-5
prompts used for fine-tuning models and present 6WSC273itselfisatestsetandthushasnofine-tuning
theresultswiththebestselections. Table4presents results.Weonlyreportthezero-shotresults.
7Note that the OntoNotes dataset is substantially larger
aconcisepromptcomprisinganinstructionalong
thantheothers. Weobservethatinferenceontheentiretest
withcontextforeachdialogue. Toassessthequal- setbecomesextremelytime-consuming,particularlywiththe
ityofgeneratedqueries,wefollowthemetricsfrom largermodels;further,thecostofrunninginferenceonGPT-
3.5startsbecomingnon-negligible.Consequently,wepropose
previous research, particularly BLEU (Papineni
limitingtheOntoNotestestsettoa10%sub-sample,whichis
etal.,2002)andROUGE(Lin,2004). thesettingweconsistentlyadopt.OPT LLaMA GPT
Task Dataset Metrics FT
125M 350M 1.3B 2.7B 7B 13B 30B 3.5-turbo
WSC273 Acc 58.24 66.67 76.19 77.66 86.81 89.38 89.01 88.64 N/A
MUC 12.66 7.58 13.21 8.29 10.31 31.80 33.56 56.32 77.26
CR B3 53.80 52.26 53.54 52.41 52.20 58.43 58.66 68.20 73.43
OntoNotes
CEAF 31.09 29.49 31.40 30.10 32.63 38.00 39.27 50.72 74.46
ϕ4
Avg.F1 32.52 29.78 32.72 30.27 31.71 42.74 43.83 58.41 76.03
DST MultiWOZ JGA 11.11 27.96 26.61 28.08 32.30 28.12 42.24 57.40 63.79
Disc. PDTB-3 Acc 10.04 10.04 10.04 16.15 17.16 26.01 39.77 43.83 76.23
BLEU 0.46 0.36 7.02 49.20 41.12 61.15 66.51 57.14 80.31
MuDoCo
ROUGE 1.52 12.18 10.98 65.61 56.07 74.78 77.88 79.37 92.01
BLEU 4.53 31.27 26.35 40.09 28.19 38.64 58.68 55.24 58.67
QReCC
ROUGE 13.91 58.18 53.10 68.32 48.27 56.40 78.74 79.98 81.75
QR BLEU 0.00 7.66 12.71 27.42 28.20 42.13 48.58 63.66 88.45
InCar
ROUGE 3.41 28.76 30.45 49.63 49.96 56.73 64.18 83.51 95.24
BLEU 0.20 26.40 26.32 49.99 53.27 66.30 73.80 63.34 82.56
GECOR
ROUGE 4.06 42.13 42.57 65.89 69.23 80.99 86.03 79.00 92.63
BLEU 2.61 19.39 24.24 34.66 21.34 29.32 47.24 47.12 57.46
CANARD
ROUGE 9.82 45.63 49.36 62.73 38.17 46.61 69.73 74.61 81.06
Table5: Few-shotresultsoftwoopen-sourcedmodelsandGPT-3.5onthecontextunderstandingbenchmark. The
resultswiththebestnumberoffew-shotexamplesarereportedforeachtask. Fine-tuning(FT)resultsservesasa
referencewhenevaluatingLLMs’capabilityunderICLsetup.
betweenICLandFTresultshighlightsthatunder for the selected relation. In addition, because of
theICLsetting,LLMsstruggletobuildcoreference animbalanceddistributionofclasses, thesemod-
chainswithoutadequatedomain-specificexamples. elspotentiallyperformworsethanrandomchance
Specifically, models except GPT perform signifi- (25%). This suggests that the models struggle to
cantlyworseontheMUCmetric. Erroranalysisre- distinguishthenuancesbetweendifferentrelation
vealsthatthesemodelsareinclinedtocreatemore classesandfailtocorrectlyidentifyrelationsacross
clusters,includingsingletonclusters. Thisimplies EDUswithincontext.
thatpre-trainedLLMsencounterdifficultiesinun-
Query Rewriting The gap between small and
derstandinglong-rangecontextualinformation.
largemodelsissignificantlyhuge,comparedtothe
DST A similar trend is observed as CR where othertasks. Forinstance,OPT-125Mcannoteven
OPTandLLaMAmodelsfallbehindGPT-3.5sig- complete the rewriting task. Analysis on predic-
nificantly. Thissuggeststhatthesemodelsfailto tionsofsmallmodelsindicatesthatthemodelisnot
extract key information as the conversation pro- capable of following the instructions or learning
ceeds, even with the provision of 5 to 10 demon- patternsfromthefew-shotexamples. Weidentifya
strationsandthedistilledrelevantdomainontology fewmajorerrortypes: (1)generatingthenextsen-
inprompt. Ourerroranalysisindicatesthatmostof tence,insteadofrewriting;(2)rewritingthewrong
theerrorshappenduetothemisdetectionofslots userturnfromtheconversation;(3)copyingthelast
or the wrong predicted value in a slot-value pair. userutterancewithoutanyrewriting. Theseerrors
OnlyGPT-3.5reachesthelevelofFTresultswhich getreducedasthemodelsizeincreases. However,
isafine-tunedT5basemodel(Bangetal.,2023a). similar to the previous three tasks, the best ICL
resultsachievedbyGPTisfarfromthefine-tuned
ImplicitDiscourseRelationClassification We
models.8 ItisworthnotingthatOPT-2.7Bperforms
observeanincreaseinscoreswhenthemodelsize
onparornotablybetterthanLLaMA-7B,whichis
exceeds 7B. However, even the best-performing
somewhatnotalignedwiththefindingsinBeeching
LLM, GPT, performs worse than the SOTA fine-
etal.(2023)whereLLaMA-7Bevenoutperforms
tunedmodel(LiuandStrube,2023)withthedrop
OPT-66B in many tasks, including ARC (Clark
of 32% accuracy. We carefully examine the pre-
dictionsforeachmodelandfoundthatallmodels 8Inliterature,thebestFTresultscomefromdifferentmod-
elsacrossfiveQRdatasets,wheresomearenotevenLLM
tend to predict the same relation class for every
based. Toensurefaircomparison,wefine-tunedaT5large
example, albeit with their individual preferences modeloneachQRdataset.Dataset Metrics 7B-D 30B-Q 30B-D
OPT-2.7B LLaMA-30B GPT-3.5 Fine-tuning
WSC273 Acc 86.81 87.18 89.01
MUC 10.31 25.37 33.56
CR B3 52.20 56.80 58.66
OntoNotes
CEAF 32.63 36.93 39.27
ϕ4
Avg.F1 31.71 39.70 43.83
MultiWOZ JGA 32.30 41.99 42.24
DST PDTB-3 Acc 17.16 31.29 39.77
BLEU 41.12 59.22 66.51
MuDoCo
ROUGE 56.07 71.38 77.88
BLEU 28.19 53.72 58.68
QReCC
Disc. ROUGE 48.27 74.13 78.74
BLEU 28.20 39.69 48.58
InCar
ROUGE 49.96 56.32 64.18
BLEU 53.27 70.41 83.36
GECOR
QR ROUGE 69.23 73.80 86.03
BLEU 21.34 45.07 47.24
CANARD
ROUGE 38.17 67.15 69.73
0.00 10.00 20.00 30.00 40.00 50.00 60.00 70.00 80.00 90.00
Table6:Comparisonbetweendenseandquantizedmod-
Figure 2: Comparison between commercial/non-
els. Dense LLaMA-7B and 3-bit quantized LLaMA-
commercial models and fine-tuning models for each
30B share similar memory and disk requirements. D
taskinthecontextunderstandingbenchmark.
representsdensemodelandQdenotesquantizedmodel.
4.3 ModelCompressionTechnique
etal.,2018),HellaSwag(Zellersetal.,2019),and
Aswefocusonevaluatingcontextunderstanding
MMLU(Hendrycksetal.,2021b).
ofLLMsinanICLsetup,weevaluatemodelsquan-
Allinall,thissectionpresentsaholisticcompar- tizedusingGPTQ(Frantaretal.,2022),whichis
isonofLLMs’behaviorsonthetargetcontextun- anefficientone-shotweightquantizationalgorithm
derstandingtasks. Onthetaskswithstructuredout- based on approximate second-order information
putssuchasCRorDST,evensmallmodelsshow thatcompressesthemodelpost-training. Itenables
acertainlevelofcontextunderstandingandseem a reduction in memory and disk requirements by
tofollowthetaskinstruction. Classificationtasks upto80%,comparedtothepre-quantizedmodel.
suchasdiscourserelationselectionaredeemedthe
4.4 QuantizedModelResults
easiest among all tasks; however, the small mod-
els are even worse than a random guess (25%). GPTQ (Frantar et al., 2022) has been shown to
Asforthegenerativetask,theabilitytocomplete effectively reduce the model size to 3 bits with-
query rewriting can be only observed in the case outincurringsubstantialperformancelossesacross
of larger models, as the model has the freedom a range of NLP tasks, such as MMLU, ARC,
to generate arbitrary content that does not follow StoryCloze. However, whether this performance
theprompt. WenoticethatOPT-2.7Boutperforms preservationcanbeextendedtocontextualunder-
LLaMA-7BinmultipleQRdatasets,includingMu- standingwasunclear.
DoCo,QReCC,andCANARD.Wecarefullycom- Table 6 presents the comparison between the
paretheoutputsbetweenthetwomodels. Asanex- dense and 3-bit quantized LLaMA models. In
ample,QReCC,aQA-basedconversationaldataset, contrasttopreviousstudieson3-bitquantization,
consistsofseveralQApairsascontextandalast we observed that quantization leads to fluctuated
query to be rewritten. We observe that LLaMA- dropsinperformanceacrossthefourtasks. Specif-
7Btendstorewritethequestionincontextinstead ically, in WSC273, MultiWOZ, and CANARD,
ofrewritingthelasttargetquery,whichisnotfre- post-training quantization incurs only a marginal
quentinOPT-2.7B.Itisalsonotedthatexceptfor performancedrop(∼1.7points). However,inthe
DST,FTmodelsdemonstratemarkedsuperiority remainingdatasets,quantizationresultsinsignifi-
overpre-trainedmodels,highlightingthepotential cantperformancedrops.
forimprovingLLMs’competenceonthesecontext The results further show that the quantized
understandingtasks. LLaMA-30BmodelconsistentlyoutperformsthedenseLLaMA-7Bmodelacrossalltasksdespitebe- 6.7/7B 13B 30B
Dataset
ingcomparableindiskandmemoryrequirements. O. L. O. L. O. L.
For CR, the 30B quantized model achieves sig- Mudoco 53.1 41.1 55.2 61.1 55.2 66.5
71.8 56.0 72.1 74.7 71.5 77.8
nificantlyhigherscoresontheOntoNotesdataset
46.6 28.1 43.7 38.6 43.8 58.6
across all metrics. The MUC metric shows the QReCC
73.4 48.2 71.6 56.4 71.9 78.7
mostsubstantialimprovement,indicatingthatthe
40.3 28.2 41.9 42.1 44.6 48.5
InCar
quantized30Bmodelpartiallyovercomestheten-
64.8 49.9 62.6 56.7 65.3 64.1
dency to create small clusters for mentions. For 58.8 53.2 60.9 66.3 58.2 73.8
GECOR
DSTonMultiWOZ,the30Bquantizedmodelshow 75.7 69.2 78.3 80.9 76.1 86.0
a30%relativeimprovementoverthe7Bmodelin 43.8 21.3 37.5 29.3 41.3 47.2
CANARD
JGA. On discourse parsing with PDTB-3, the ac- 72.0 38.1 66.0 46.6 69.3 69.7
curacy of quantized 30B model is almost double,
Table7: ComparisonbetweenOPT(O.)andLLaMA
17.16% vs 31.29%. Across all QR datasets, the
(L.)acrossfivequeryrewritedatasets. Foreachdataset,
quantized30BmodelsubstantiallyimprovesNLG thefirstandsecondrowsrepresentBLEUandROUGE
scorescomparedtothedense7Bmodel,withrela- scoresrespectively.
tivegainsrangingfrom15-50%. Thelargestgapis
observedonGECOR. Context
In general, we show that the quantized 30B
User:whatisthenameofindiapakistanborderline
Bot:TheRadcliffeLinewastheboundarydemarcation
LLaMAmodelconsistentlyandsignificantlyout-
linebetweentheIndianandPakistaniportionsofthe
performs the dense 7B model as a result of the PunjabandBengalprovincesofBritishIndia.
User:whocreatedtheradcliffeline
increased scale, despite using 3-bit quantization.
Bot:TheRadcliffeLinewasnamedafteritsarchitect,Sir
Thebenefitsofgreatermodelscalethusoutweigh CyrilRadcliffe,whowasthejointchairmanofthetwo
the impacts of quantization in understanding dis- boundarycommissionsforthetwoprovinces.
User:whenwasthelinepublished
course. Webelievethisfindingwouldbebeneficial
Goldanswer:whenwastheradcliffelinepublished
whendeployingLLMsinreal-worldapplications
Prediction1(repeatthelastquery):whenwastheline
withdiskandruntimeconstraints.
published
Prediction2(languagemodeling):1947
5 CaseStudy: QueryRewriting
Table8: Anexampleoftwomajortypesoferrorsfound
In this section, we provide in-depth analysis by
inthequeryrewritingtask.
comparing the two open-sourced model families
OPTandLLaMA,andtheimpactofquantization,
usingqueryrewritingasthetargettask. LLaMAoutperformsOPT.However,theirperfor-
We conduct a careful inspection of the query manceonQR,asshowninTable7,doesnotfollow
rewritingtaskbecauseofthreereasons: (1)bythe thispattern.
natureofthetask,queryrewritingistheonlyone Whenthemodelsizeisaround7B,OPTconsis-
withfree-formgeneration,whiletheotherseffec- tentlyperformsbetterthanLLaMAbyasignificant
tivelyareeitherclassification-basedtasksorheav- marginacrossthefiveQRdatasets. Thetwomod-
ilyconstrainedintheirpossibleoutputpredictions. els perform on par with each other at 13B. The
ThegenerationtaskallowsustoexploretheLLMs’ superiority of LLaMA is only obvious with 30B
outputinmoredetail,andtoprovidemoreinterest- modelsize. Fromanotherperspective,althoughwe
inginsights;(2)themanualanalysisoferrorsisa expect performance to improve as model size in-
time-consumingprocess,makingitchallengingto creases,weobservethistrendonLLaMA,butnot
conduct such an in-depth analysis across all four on OPT. These results suggest that it may not be
tasks;(3)thequeryrewritingtaskcoversadiverse correcttoconcludetheoverallsuperioritybetween
rangeoffivedatasets,enablingustocomparedif- twomodelfamiliesbyonlycomparingonacertain
ferencesbetweeneachdatasetandtotherebygain rangeofmodelsizesoronacertainsetoftasks.
adeeperunderstanding.
5.2 Densevs. Quantized
5.1 OPTvs. LLaMA
We conduct a quantitative analysis on the error
Prior works (Beeching et al., 2023) have consis- typesof queryrewriting toinvestigate theperfor-
tently shown that, under the same model size, mance gap between dense and quantized models.Type Dataset 7BD 30BQ 30BD 50%)errorscomparedtothe7Bdensemodel(125
MuDoCo 260 247 194 vs. 232). This indicates that 3-bit quantization
QReCC 86 90 26 maintains the ICL capability that allows models
Repeat InCar 17 15 8
to rewrite the user query successfully rather than
GECOR 59 62 37
performinglanguagemodelingtask.
CANARD 47 44 32
Total 469 458 297
6 Conclusion
MuDoCo 71 29 16
QReCC 80 28 16
Thispaperintroducesacontextualunderstanding
LM InCar 19 20 15
benchmarkdesignedtoassesstheperformanceof
GECOR 6 1 0
LLMs. Wecollectnineexistingdatasetsspanning
CANARD 127 76 59
Total 232 125 106 fourtasks,eachcarefullytailoredtosuitgenerative
models. This benchmark encompasses essential
Table9: Numberofthemajortwotypeserrorsonthree
elements for assessing linguistic comprehension
LLaMA models (7B dense, 30B quantized, and 30B
withincontext,includingbothdocumentanddia-
dense) found in query rewriting. Repeat stands for
logbasedcontextualunderstanding. Experimental
repeat-the-last-query error and LM denotes language
resultsrevealthatLLMsunderin-contextlearning
modelingerror.
struggle with nuanced linguistic features within
thischallengingbenchmark,exhibitinginconsisten-
Acrossthefivedatasets,weidentifytwomainer- cies with other benchmarks that emphasize other
ror types that account for nearly 80% of the to- aspectsoflanguage. Tothebestofourknowledge,
talerrors,withexamplesshowninTable8. First, wearealsothefirsttocomparedensemodelsand
the model repeats the last query without resolv- post-trainingquantizationmodelsincontextualun-
inganyreferredentityorellipsis. Inthiscase,the derstandingtasks. Thiscomparisonhighlightsthat
modelseemstounderstandtheinstructionbutfails 3-bitpost-trainingquantizationreducesthegeneral
at rewriting. This type of error can be primarily understandingcapacityofcontexttodifferentex-
associatedwiththemodel’scontextunderstanding tent across the 4 tasks. The proposed contextual
capability. Second, the model treats the task as a comprehensionbenchmarkthusprovidesaunique
language modeling (LM) task, where it provides perspective on the contextual dimension of lan-
a response to the last query. In this scenario, the guageunderstandingandoffersavaluableaddition
model appears to struggle to understand the task toexistingLLMevaluations.
instruction, even withseveralfew-shotexamples.
We classify this error type as more related to the Limitations
model’sICLability.
This work provides an evaluation of various pre-
Weperformmanualerrorannotationsonthefive
trainedLLMs,includingOPT,LLaMA,andGPT,
QRdatasets9. Table9illustratesthenumberofer-
on our understanding benchmark. However, we
rorsofthethreeselectedmodelsoneachdataset. A
havenotevaluatedotherLLMsdesignedforlonger
consistenttrendisobservedacrossallQRdatasets.
inputscenarios,suchasLongLLaMA(Tworkowski
Intermsofrepeaterrors,the30Bdensemodelex-
etal.,2023).
hibitssignificantlyfewererrorscomparedtothe7B
Inourevaluation,wefocusontheGPTQquan-
densemodel(297vs. 469). However,3-bitGPTQ
tizationmethod,analyzingitsperformanceonour
quantizationleadstoanincreaseinthistypeofer-
benchmark. Wedonotincludeotherpost-training
ror,reachingasimilarerrorcounttothe7Bdense
quantizationtechniques,suchasRPTQ(Yuanetal.,
model(458vs. 469). Thisimpliesthat3-bitquan-
2023),inthiswork.
tizationreducesthemodel’sabilitytocomprehend
OurevaluationconcentratesonEnglishdatasets,
thecontext. RegardingLM errors,the30Bdense
primarilyutilizingLLMspre-trainedwithEnglish
modelalsosignificantlyoutperformsthe7Bdense
data. Allofthefourtasksonourbenchmarkhave
model,with106errorscomparedto232. Itistobe
datasets from other languages. The coreference
notedthatthequantizedmodelgeneratesonly125
datasetOntoNotes5.0containsannotationsofAra-
LMerrors,slightlymorethanthe30Bdensemodel.
bicandChinese. Inaddition,recentreleasessuch
However,itgeneratessignificantlyfewer(around
as CorefUD (Nedoluzhko et al., 2022) promote
910%testdataonQReCCandCANARDwasgraded. standardizationofmultilingualcoreferenceanno-tations. In DST, CrossWOZ (Zhu et al., 2020) is Shelby, Ambrose Slone, Daniel Smilkov, David R.
across-domainwizard-of-oztask-orienteddataset. So, Daniel Sohn, Simon Tokumine, Dasha Valter,
Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang,
Longetal.(2020)developTED-CDB,aChinese
Pidong Wang, Zirui Wang, Tao Wang, John Wiet-
discourse relation dataset. The query rewriting
ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting
taskalsohasdatasetsinotherlanguages, suchas Xue,PengchengYin,JiahuiYu,QiaoZhang,Steven
REWRITE(Suetal.,2019)andRestoration-200K Zheng,CeZheng,WeikangZhou,DennyZhou,Slav
Petrov, and Yonghui Wu. 2023. Palm 2 technical
(Pan et al., 2019). Finally, specific LLMs opti-
report.
mizedforindividuallanguages,suchasChatGLM
(Du et al., 2022), exist and are not a part of our Namo Bang, Jeehyun Lee, and Myoung-Wan Koo.
evaluation. 2023a. Task-optimized adapters for an end-to-end
task-orienteddialoguesystem. InFindingsoftheAs-
sociationforComputationalLinguistics: ACL2023,
Acknowledgements
pages7355–7369,Toronto,Canada.Associationfor
ComputationalLinguistics.
The authors would like to thank Jeffrey Nichols,
RussWebbandtheanonymousreviewersfortheir YejinBang,SamuelCahyawijaya,NayeonLee,Wen-
helpandfeedback. liangDai,DanSu,BryanWilie,HolyLovenia,Ziwei
Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan
Xu,andPascaleFung.2023b. Amultitask,multilin-
gual,multimodalevaluationofchatgptonreasoning,
References
hallucination,andinteractivity.
Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu,
EdwardBeeching,ClémentineFourrier,NathanHabib,
Shayne Longpre, Stephen Pulman, and Srinivas
SheonHan,NathanLambert,NazneenRajani,Omar
Chappidi.2021. Open-domainquestionanswering
Sanseviero,LewisTunstall,andThomasWolf.2023.
goesconversationalviaquestionrewriting. InPro-
Openllmleaderboard. https://huggingface.co/
ceedingsofthe2021ConferenceoftheNorthAmer-
spaces/HuggingFaceH4/open_llm_leaderboard.
icanChapteroftheAssociationforComputational
Linguistics: HumanLanguageTechnologies,pages
BerndBohnet,ChrisAlberti,andMichaelCollins.2023.
520–534,Online.AssociationforComputationalLin-
Coreferenceresolutionthroughaseq2seqtransition-
guistics.
based system. Transactions of the Association for
ComputationalLinguistics,11:212–226.
RohanAnil,AndrewM.Dai,OrhanFirat,MelvinJohn-
son, Dmitry Lepikhin, Alexandre Passos, Siamak MichaelBoratko,HarshitPadigela,DivyendraMikki-
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng lineni,PritishYuvraj,RajarshiDas,AndrewMcCal-
Chen, Eric Chu, Jonathan H. Clark, Laurent El lum,MariaChang,AchilleFokoue-Nkoutche,Pavan
Shafey,YanpingHuang,KathyMeier-Hellstern,Gau- Kapanipathi, Nicholas Mattei, Ryan Musa, Kartik
ravMishra,EricaMoreira,MarkOmernick,Kevin Talamadupula,andMichaelWitbrock.2018. Asys-
Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, tematicclassificationofknowledge,reasoning,and
YuanzhongXu,YujingZhang,GustavoHernandez contextwithintheARCdataset. InProceedingsof
Abrego,JunwhanAhn,JacobAustin,PaulBarham, theWorkshoponMachineReadingforQuestionAn-
Jan Botha, James Bradbury, Siddhartha Brahma, swering,pages60–70,Melbourne,Australia.Associ-
KevinBrooks,MicheleCatasta,YongCheng,Colin ationforComputationalLinguistics.
Cherry,ChristopherA.Choquette-Choo,Aakanksha
Chowdhery,ClémentCrepy,ShachiDave,Mostafa Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind
Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Neelakantan,PranavShyam,GirishSastry,Amanda
Feng, Vlad Fienber, Markus Freitag, Xavier Gar- Askell, Sandhini Agarwal, Ariel Herbert-Voss,
cia,SebastianGehrmann,LucasGonzalez,GuyGur- Gretchen Krueger, Tom Henighan, Rewon Child,
Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua AdityaRamesh,DanielZiegler,JeffreyWu,Clemens
Howland, Andrea Hu, Jeffrey Hui, Jeremy Hur- Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
witz,MichaelIsard,AbeIttycheriah,MatthewJagiel- teusz Litwin, Scott Gray, Benjamin Chess, Jack
ski,WenhaoJia,KathleenKenealy,MaximKrikun, Clark, ChristopherBerner, SamMcCandlish, Alec
SnehaKudugunta,ChangLan,KatherineLee,Ben- Radford,IlyaSutskever,andDarioAmodei.2020a.
jaminLee,EricLi,MusicLi,WeiLi,YaGuangLi, Language models are few-shot learners. In Ad-
JianLi,HyeontaekLim,HanzhaoLin,ZhongtaoLiu, vances in Neural Information Processing Systems,
FrederickLiu,MarcelloMaggioni,AromaMahendru, volume 33, pages 1877–1901. Curran Associates,
JoshuaMaynez,VedantMisra,MaysamMoussalem, Inc.
Zachary Nado, John Nham, Eric Ni, Andrew Nys-
trom, Alicia Parrish, Marie Pellat, Martin Polacek, TomB.Brown,BenjaminMann,NickRyder,Melanie
AlexPolozov,ReinerPope,SiyuanQiao,EmilyReif, Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Bryan Richter, Parker Riley, Alex Castro Ros, Au- Neelakantan,PranavShyam,GirishSastry,Amanda
rkoRoy,BrennanSaeta,RajkumarSamuel, Renee Askell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child, Ahmed Elgohary, Denis Peskov, and Jordan Boyd-
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Graber. 2019. Can you unpack that? learning to
ClemensWinter,ChristopherHesse,MarkChen,Eric rewritequestions-in-context. InProceedingsofthe
Sigler,MateuszLitwin,ScottGray,BenjaminChess, 2019 Conference on Empirical Methods in Natu-
Jack Clark, Christopher Berner, Sam McCandlish, ralLanguageProcessingandthe9thInternational
Alec Radford, Ilya Sutskever, and Dario Amodei. JointConferenceonNaturalLanguageProcessing
2020b. Languagemodelsarefew-shotlearners. (EMNLP-IJCNLP),pages5918–5924,HongKong,
China.AssociationforComputationalLinguistics.
Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang
Tseng, Iñigo Casanueva, Stefan Ultes, Osman Ra- EliasFrantarandDanAlistarh.2023. SparseGPT:Mas-
madan,andMilicaGašic´.2018. MultiWOZ-alarge- sive language models can be accurately pruned in
scale multi-domain Wizard-of-Oz dataset for task- one-shot. arXivpreprintarXiv:2301.00774.
orienteddialoguemodelling. InProceedingsofthe
2018ConferenceonEmpiricalMethodsinNatural Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and
Language Processing, pages 5016–5026, Brussels, DanAlistarh.2022. GPTQ:Accuratepost-training
Belgium.AssociationforComputationalLinguistics. compressionforgenerativepretrainedtransformers.
arXivpreprintarXiv:2210.17323.
AakankshaChowdhery,SharanNarang,JacobDevlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, MichaelHeck,NurulLubis,BenjaminRuppik,Renato
Paul Barham, Hyung Won Chung, Charles Sutton, Vukovic,ShutongFeng,ChristianGeishauser,Hsien-
Sebastian Gehrmann, Parker Schuh, Kensen Shi, chinLin,CarelvanNiekerk,andMilicaGasic.2023.
Sasha Tsvyashchenko, Joshua Maynez, Abhishek ChatGPT for zero-shot dialogue state tracking: A
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin- solution or an opportunity? In Proceedings of the
odkumar Prabhakaran, Emily Reif, Nan Du, Ben 61stAnnualMeetingoftheAssociationforCompu-
Hutchinson, Reiner Pope, James Bradbury, Jacob tationalLinguistics(Volume2: ShortPapers),pages
Austin,MichaelIsard,GuyGur-Ari,PengchengYin, 936–950,Toronto,Canada.AssociationforCompu-
Toju Duke, Anselm Levskaya, Sanjay Ghemawat, tationalLinguistics.
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
VedantMisra,KevinRobinson,LiamFedus,Denny Dan Hendrycks, Collin Burns, Steven Basart, Andy
Zhou,DaphneIppolito,DavidLuan,HyeontaekLim, Zou,MantasMazeika,DawnSong,andJacobStein-
Barret Zoph, Alexander Spiridonov, Ryan Sepassi, hardt.2021a. Measuringmassivemultitasklanguage
DavidDohan,ShivaniAgrawal,MarkOmernick,An- understanding. ProceedingsoftheInternationalCon-
drew M. Dai, Thanumalayan Sankaranarayana Pil- ferenceonLearningRepresentations(ICLR).
lai,MariePellat,AitorLewkowycz,EricaMoreira,
DanHendrycks,CollinBurns,StevenBasart,AndyZou,
Rewon Child, Oleksandr Polozov, Katherine Lee,
MantasMazeika,DawnSong,andJacobSteinhardt.
ZongweiZhou,XuezhiWang,BrennanSaeta,Mark
2021b. Measuringmassivemultitasklanguageun-
Diaz,OrhanFirat,MicheleCatasta,JasonWei,Kathy
derstanding.
Meier-Hellstern,DouglasEck,JeffDean,SlavPetrov,
andNoahFiedel.2022. Palm:Scalinglanguagemod-
JordanHoffmann,SebastianBorgeaud,ArthurMensch,
elingwithpathways.
Elena Buchatskaya, Trevor Cai, Eliza Rutherford,
DiegodeLasCasas,LisaAnneHendricks,Johannes
Christopher Clark, Kenton Lee, Ming-Wei Chang,
Welbl, Aidan Clark, Tom Hennigan, Eric Noland,
Tom Kwiatkowski, Michael Collins, and Kristina
KatieMillican,GeorgevandenDriessche,Bogdan
Toutanova.2019. BoolQ:Exploringthesurprising
Damoc, Aurelia Guy, Simon Osindero, Karen Si-
difficultyofnaturalyes/noquestions. InProceedings
monyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,
ofthe2019ConferenceoftheNorthAmericanChap-
andLaurentSifre.2022. Trainingcompute-optimal
teroftheAssociationforComputationalLinguistics:
largelanguagemodels.
HumanLanguageTechnologies,Volume1(Longand
ShortPapers),pages2924–2936,Minneapolis,Min-
Yushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu,
nesota.AssociationforComputationalLinguistics.
Noah A. Smith, and Mari Ostendorf. 2022. In-
PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot, contextlearningforfew-shotdialoguestatetracking.
AshishSabharwal,CarissaSchoenick,andOyvind In Findings of the Association for Computational
Tafjord.2018. Thinkyouhavesolvedquestionan- Linguistics: EMNLP2022,pages2627–2643,Abu
swering? tryarc,theai2reasoningchallenge. Dhabi,UnitedArabEmirates.AssociationforCom-
putationalLinguistics.
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and
LukeZettlemoyer.2023. Qlora: Efficientfinetuning MandarJoshi,DanqiChen,YinhanLiu,DanielS.Weld,
ofquantizedllms. arXivpreprintarXiv:2305.14314. Luke Zettlemoyer, and Omer Levy. 2020. Span-
BERT:Improvingpre-trainingbyrepresentingand
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, predictingspans. TransactionsoftheAssociationfor
JiezhongQiu,ZhilinYang,andJieTang.2022. Glm: ComputationalLinguistics,8:64–77.
Generallanguagemodelpretrainingwithautoregres-
siveblankinfilling. InProceedingsofthe60thAn- Shafiq Joty, Giuseppe Carenini, Raymond Ng, and
nualMeetingoftheAssociationforComputational Gabriel Murray. 2019. Discourse analysis and its
Linguistics(Volume1: LongPapers),pages320–335. applications. In Proceedings of the 57th AnnualMeeting of the Association for Computational Lin- Chaudhary,WilliamWang,XuechenLi,YifanMai,
guistics: TutorialAbstracts,pages12–17,Florence, YuhuiZhang,andYutaKoreeda.2022. Holisticeval-
Italy.AssociationforComputationalLinguistics. uationoflanguagemodels.
JeonghoonKim,JungHyunLee,SungdongKim,Joon- Chin-Yew Lin. 2004. ROUGE: A package for auto-
sukPark,KangMinYoo,SeJungKwon,andDong- maticevaluationofsummaries. InTextSummariza-
sooLee.2023. Memory-efficientfine-tuningofcom- tionBranchesOut,pages74–81,Barcelona,Spain.
pressedlargelanguagemodelsviasub-4-bitinteger AssociationforComputationalLinguistics.
quantization.
WeiLiuandMichaelStrube.2023. Annotation-inspired
AndreyKuzmin,MarkusNagel,MartvanBaalen,Arash
implicitdiscourserelationclassificationwithauxil-
Behboodi,andTijmenBlankevoort.2023. Pruning
iarydiscourseconnectivegeneration. InProceedings
vsquantization: Whichisbetter?
of the 61st Annual Meeting of the Association for
ComputationalLinguistics(Volume1: LongPapers),
TomKwiatkowski, JennimariaPalomaki, OliviaRed-
pages15696–15712,Toronto,Canada.Association
field,MichaelCollins,AnkurParikh,ChrisAlberti,
forComputationalLinguistics.
DanielleEpstein,IlliaPolosukhin,JacobDevlin,Ken-
tonLee,KristinaToutanova,LlionJones,Matthew
Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie
Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob
Chang,PierreStock,YasharMehdad,YangyangShi,
Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-
Raghuraman Krishnamoorthi, and Vikas Chandra.
ralquestions: Abenchmarkforquestionanswering
2023. Llm-qat:Data-freequantizationawaretraining
research. TransactionsoftheAssociationforCompu-
forlargelanguagemodels.
tationalLinguistics,7:452–466.
Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur WanqiuLong,BonnieWebber,andDeyiXiong.2020.
Rahman, MdAmranHossenBhuiyan, ShafiqJoty, TED-CDB:Alarge-scaleChinesediscourserelation
and Jimmy Huang. 2023. A systematic study and dataset on TED talks. In Proceedings of the 2020
comprehensiveevaluationofChatGPTonbenchmark Conference on Empirical Methods in Natural Lan-
datasets. In Findings of the Association for Com- guageProcessing(EMNLP),pages2793–2803,On-
putational Linguistics: ACL 2023, pages 431–469, line.AssociationforComputationalLinguistics.
Toronto,Canada.AssociationforComputationalLin-
guistics. ChristopherD.Manning,MihaiSurdeanu,JohnBauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
NghiaT.LeandAlanRitter.2023. Arelargelanguage Closky. 2014. The Stanford CoreNLP natural lan-
modelsrobustzero-shotcoreferenceresolvers? guage processing toolkit. In ACL 2014 System
Demonstrations,pages55–60.
KentonLee,LuhengHe,MikeLewis,andLukeZettle-
moyer. 2017. End-to-end neural coreference reso-
ScottMartin,ShivaniPoddar,andKartikeyaUpasani.
lution. In Proceedings of the 2017 Conference on
2020. MuDoCo: Corpus for multidomain corefer-
EmpiricalMethodsinNaturalLanguageProcessing,
enceresolutionandreferringexpressiongeneration.
pages188–197,Copenhagen,Denmark.Association
InProceedingsofthe12thLanguageResourcesand
forComputationalLinguistics.
Evaluation Conference, pages 104–111, Marseille,
France.EuropeanLanguageResourcesAssociation.
HectorJ.Levesque,ErnestDavis,andLeoraMorgen-
stern.2012. Thewinogradschemachallenge. In13th
Nasrin Mostafazadeh, Michael Roth, Annie Louis,
InternationalConferenceonthePrinciplesofKnowl-
Nathanael Chambers, and James Allen. 2017. LS-
edgeRepresentationandReasoning,KR2012,Pro-
DSem 2017 shared task: The story cloze test. In
ceedingsoftheInternationalConferenceonKnowl-
Proceedingsofthe2ndWorkshoponLinkingModels
edgeRepresentationandReasoning,pages552–561.
of Lexical, Sentential and Discourse-level Seman-
InstituteofElectricalandElectronicsEngineersInc.
tics,pages46–51,Valencia,Spain.Associationfor
ComputationalLinguistics.
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris
Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian
Zhang,DeepakNarayanan,YuhuaiWu,AnanyaKu- Nikola Mrkšic´, Diarmuid Ó Séaghdha, Tsung-Hsien
mar,BenjaminNewman,BinhangYuan,BobbyYan, Wen,BlaiseThomson,andSteveYoung.2017. Neu-
CeZhang,ChristianCosgrove,ChristopherD.Man- ralbelieftracker: Data-drivendialoguestatetracking.
ning,ChristopherRé,DianaAcosta-Navas,DrewA. In Proceedings of the 55th Annual Meeting of the
Hudson, Eric Zelikman, Esin Durmus, Faisal Lad- AssociationforComputationalLinguistics(Volume
hak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue 1: LongPapers),pages1777–1788.
Wang,KeshavSanthanam,LaurelOrr,LuciaZheng,
Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Markus Nagel, Rana Ali Amjad, Mart Van Baalen,
NeelGuha, NiladriChatterji, OmarKhattab, Peter ChristosLouizos,andTijmenBlankevoort.2020. Up
Henderson, Qian Huang, Ryan Chi, Sang Michael ordown? adaptiveroundingforpost-trainingquanti-
Xie, Shibani Santurkar, Surya Ganguli, Tatsunori zation. InProceedingsofthe37thInternationalCon-
Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav ferenceonMachineLearning,ICML’20.JMLR.org.Anna Nedoluzhko, Michal Novák, Martin Popel, JunQuan,DeyiXiong,BonnieWebber,andChangjian
ZdeneˇkŽabokrtský,AmirZeldes,andDanielZeman. Hu. 2019. GECOR: An end-to-end generative el-
2022. CorefUD 1.0: Coreference meets Universal lipsis and co-reference resolution model for task-
Dependencies. InProceedingsoftheThirteenthLan- orienteddialogue. InProceedingsofthe2019Confer-
guageResourcesandEvaluationConference,pages enceonEmpiricalMethodsinNaturalLanguagePro-
4859–4872,Marseille,France.EuropeanLanguage cessingandthe9thInternationalJointConference
ResourcesAssociation. onNaturalLanguageProcessing(EMNLP-IJCNLP),
pages4547–4557,HongKong,China.Association
OpenAI. 2022. Optimizing language models for dia- forComputationalLinguistics.
logue.
Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie
OpenAI.2023. Gpt-4technicalreport. Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susan-
LongOuyang,JeffWu,XuJiang,DiogoAlmeida,Car-
nah Young, Eliza Rutherford, Tom Hennigan, Ja-
rollL.Wainwright,PamelaMishkin,ChongZhang,
cobMenick,AlbinCassirer,RichardPowell,George
SandhiniAgarwal,KatarinaSlama,AlexRay,John
van den Driessche, Lisa Anne Hendricks, Mari-
Schulman,JacobHilton,FraserKelton,LukeMiller,
beth Rauh, Po-Sen Huang, Amelia Glaese, Jo-
Maddie Simens, Amanda Askell, Peter Welinder,
hannes Welbl, Sumanth Dathathri, Saffron Huang,
Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
JonathanUesato,JohnMellor,IrinaHiggins,Anto-
Traininglanguagemodelstofollowinstructionswith
niaCreswell,NatMcAleese,AmyWu,ErichElsen,
humanfeedback.
SiddhantJayakumar,ElenaBuchatskaya,DavidBud-
den,EsmeSutherland,KarenSimonyan,MichelaPa-
Zhufeng Pan, Kun Bai, Yan Wang, Lianqiang Zhou,
ganini,LaurentSifre,LenaMartens,XiangLorraine
andXiaojiangLiu.2019. Improvingopen-domain
Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena
dialoguesystemsviamulti-turnincompleteutterance
Gribovskaya,DomenicDonato,AngelikiLazaridou,
restoration. InProceedingsofthe2019Conference
ArthurMensch,Jean-BaptisteLespiau,MariaTsim-
on Empirical Methods in Natural Language Pro-
poukelli,NikolaiGrigorev,DougFritz,ThibaultSot-
cessingandthe9thInternationalJointConference
tiaux,MantasPajarskas,TobyPohlen,ZhitaoGong,
onNaturalLanguageProcessing(EMNLP-IJCNLP),
DanielToyama,CypriendeMassond’Autume,Yujia
pages1824–1833,HongKong,China.Association
Li,TayfunTerzi,VladimirMikulik,IgorBabuschkin,
forComputationalLinguistics.
Aidan Clark, Diego de Las Casas, Aurelia Guy,
Chris Jones, James Bradbury, Matthew Johnson,
DenisPaperno,GermánKruszewski,AngelikiLazari-
Blake Hechtman, Laura Weidinger, Iason Gabriel,
dou,NgocQuanPham,RaffaellaBernardi,Sandro
WilliamIsaac,EdLockhart,SimonOsindero,Laura
Pezzelle,MarcoBaroni,GemmaBoleda,andRaquel
Rimell,ChrisDyer,OriolVinyals,KareemAyoub,
Fernández. 2016. The LAMBADA dataset: Word
JeffStanway,LorrayneBennett,DemisHassabis,Ko-
prediction requiring a broad discourse context. In
rayKavukcuoglu,andGeoffreyIrving.2022. Scaling
Proceedings of the54th Annual Meeting of the As-
languagemodels: Methods,analysis&insightsfrom
sociationforComputationalLinguistics(Volume1:
traininggopher.
Long Papers), pages 1525–1534, Berlin, Germany.
AssociationforComputationalLinguistics.
MichaelRegan,PushpendreRastogi,ArpitGupta,and
LambertMathias.2019. Adatasetforresolvingre-
KishorePapineni,SalimRoukos,ToddWard,andWei-
ferringexpressionsinspokendialogueviacontextual
JingZhu.2002. Bleu: amethodforautomaticevalu-
queryrewrites(cqr). ArXiv,abs/1903.11783.
ationofmachinetranslation. InProceedingsofthe
40thAnnualMeetingoftheAssociationforCompu-
tational Linguistics, pages 311–318, Philadelphia, HuiSu,XiaoyuShen,RongzhiZhang,FeiSun,Peng-
wei Hu, Cheng Niu, and Jie Zhou. 2019. Improv-
Pennsylvania,USA.AssociationforComputational
ing multi-turn dialogue modelling with utterance
Linguistics.
ReWriter. InProceedingsofthe57thAnnualMeet-
SameerPradhan,AlessandroMoschitti,NianwenXue, ingoftheAssociationforComputationalLinguistics,
HweeTouNg,AndersBjörkelund,OlgaUryupina, pages22–31,Florence,Italy.AssociationforCom-
Yuchen Zhang, and Zhi Zhong. 2013. Towards ro- putationalLinguistics.
bustlinguisticanalysisusingOntoNotes. InProceed-
ingsoftheSeventeenthConferenceonComputational S.TataandJ.M.Patel.2003. Piqa: analgebraforquery-
NaturalLanguageLearning,pages143–152,Sofia, ingproteindatasets. In15thInternationalConfer-
Bulgaria.AssociationforComputationalLinguistics. enceonScientificandStatisticalDatabaseManage-
ment,2003.,pages141–150.
SameerPradhan,AlessandroMoschitti,NianwenXue,
OlgaUryupina,andYuchenZhang.2012. CoNLL- HugoTouvron,ThibautLavril,GautierIzacard,Xavier
2012sharedtask:Modelingmultilingualunrestricted Martinet,Marie-AnneLachaux,TimothéeLacroix,
coreference in OntoNotes. In Joint Conference on BaptisteRozière,NamanGoyal,EricHambro,Faisal
EMNLPandCoNLL-SharedTask,pages1–40,Jeju Azhar,AurelienRodriguez,ArmandJoulin,Edouard
Island, Korea. Association for Computational Lin- Grave,andGuillaumeLample.2023. Llama: Open
guistics. andefficientfoundationlanguagemodels.Bo-Hsiang Tseng, Shruti Bhargava, Jiarui Lu, Joel A TaskDesignExamples
Ruben Antony Moniz, Dhivya Piraviperumal, Lin
Li,andHongYu.2021. Cread: Combinedresolution Table10presentstheinputexampleforeachtask.
ofellipsesandanaphoraindialogues. InProceedings ForCR,weonlyshowexamplesfromOntoNotes.
ofthe2021ConferenceoftheNorthAmericanChap-
teroftheAssociationforComputationalLinguistics: B Few-shotSettings
HumanLanguageTechnologies,pages3390–3406.
Table11showsthenumberofexamplesforeach
Szymon Tworkowski, Konrad Staniszewski, Mikołaj
dataset that yields the best scores. All datasets
Pacek,YuhuaiWu,HenrykMichalewski,andPiotr
Miłos´.2023. Focusedtransformer: Contrastivetrain- exceptWSC273andPDTB3userandomlyselected
ingforcontextscaling. examples from the training set. Since WSC273
doesnotincludeanytrainorvalidationset,weuse
AlexWang,YadaPruksachatkun,NikitaNangia,Aman-
thezero-shotsetting,asscorespresentedinTable
preetSingh,JulianMichael,FelixHill,OmerLevy,
andSamuelBowman.2019. Superglue: Astickier 5. For each class in PDTB3, we randomly select
benchmarkforgeneral-purposelanguageunderstand- twoexamplesfromthetrainingsetforprompting.
ing systems. In Advances in Neural Information
For some particular datasets, such as OntoNotes,
ProcessingSystems,volume32.CurranAssociates,
experiments are only performed in the zero-shot
Inc.
andone-shotsettingsduetothelimitationoninput
Bonnie Webber, Rashmi Prasad, Alan Lee,
length.
and Aravind Joshi. 2019. The penn dis-
course treebank 3.0 annotation manual.
C ReliabilityofExperimentResults
https://catalog.ldc.upenn.edu/docs/
LDC2019T05/PDTB3-Annotation-Manual.pdf.
Foreachtask,wehaverandomlyrunseveralexper-
Kai-Cheng Yang and Filippo Menczer. 2023. Large imentalsetupswithmultiplerounds,withover10
languagemodelscanratenewsoutletcredibility. settings in total. However, due to the challenges
posedbylimitedtime,budget,andcomputingre-
Steve Young, Milica Gašic´, Blaise Thomson, and Ja-
sonDWilliams.2013. Pomdp-basedstatisticalspo- sources,itisverydifficulttorunmultiplerounds
ken dialog systems: A review. Proceedings of the foreverysingleexperiment,giventhecomplexity
IEEE,101(5):1160–1179. of our experimental setup. In addition, for exist-
ingexperimentswithmultiplerounds,weempiri-
ZhihangYuan,LinNiu,JiaweiLiu,WenyuLiu,Xing-
gangWang,YuzhangShang,GuangyuSun,Qiang callyobservethatthereislowvarianceacrossthe
Wu, Jiaxiang Wu, and Bingzhe Wu. 2023. Rptq: rounds,whichleadsustoassumethatperforming
Reorder-based post-training quantization for large
theremainingexperimentswithasinglerundoes
languagemodels.
notsignificantlyimpacttheargumentspresentedin
Xiaoxue Zang, Abhinav Rastogi, Srinivas Sunkara, thispaper.
Raghav Gupta, Jianguo Zhang, and Jindong Chen.
2020. Multiwoz2.2: Adialoguedatasetwithaddi-
tionalannotationcorrectionsandstatetrackingbase-
lines. InProceedingsofthe2ndWorkshoponNatu-
ralLanguageProcessingforConversationalAI,ACL
2020,pages109–117.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019. Hellaswag: Can a
machinereallyfinishyoursentence?
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe,MoyaChen,ShuohuiChen,ChristopherDe-
wan,MonaDiab,XianLi,XiVictoriaLin,TodorMi-
haylov,MyleOtt,SamShleifer,KurtShuster,Daniel
Simig, Punit Singh Koura, Anjali Sridhar, Tianlu
Wang,andLukeZettlemoyer.2022. Opt: Openpre-
trainedtransformerlanguagemodels.
QiZhu,KailiHuang,ZhengZhang,XiaoyanZhu,and
MinlieHuang.2020. CrossWOZ:Alarge-scaleChi-
nese cross-domain task-oriented dialogue dataset.
TransactionsoftheAssociationforComputational
Linguistics,8:281–295.CoreferenceResolution
Instructions:Pleasecarefullyreadthefollowingpassages.Foreachpassageandtheoptions,youmustidentifywhichoptionthementionmarkedin*bold*refersto.Ifthemarkedmentiondoes
nothaveanyantecedent,pleaseselect“noantecedent”.
[Few-shotexamples]
Context: —basically,itwasunanimouslyagreeduponbythevariousrelevantparties. Toexpress*its*determination,theChinesesecuritiesregulatorydepartmentcompares
thisstockreformtoadiethathasbeencast.Ittakestimetoprovewhetherthestockreformcanreallymeetexpectations,andwhetheranydeviationsthatariseduringthestockreformcanbe
promptlycorrected.Dearviewers,theChinaNewsprogramwillendhere.ThisisXuLi.Thankyoueveryoneforwatching.ComingupistheFocusTodayprogramhostedbyWangShilin.
Good-bye,dearviewers.
Choice:
A.theChinesesecuritiesregulatorydepartment
B.thisstockreform
C.thestockreform
D.you
E.everyone
F.noantecedent
Question:Whatdoes*its*refersto?
Answer:A
DialogueStateTracking
Considerthefollowinglistofconcepts,called"slots"providedtoyouasajsonlist.
“slots”:{“restaurant-pricerange”:“pricebudgetfortherestaurant”,
“restaurant-area”:“areaorplaceoftherestaurant”,
“restaurant-food”:“thecuisineoftherestaurantyouarelookingfor”,
...
“hotel-postcode”:“postalcodeofthehotel”,
‘hotel-ref”:“referencenumberofthehotelbooking”
}
Some“slots”canonlytakeavaluefrompredefinedlist:
“categorical”:{“restaurant-pricerange”:[‘cheap’,‘expensive’,‘moderate’],
“restaurant-area”:[’centre’,’east’,’north’,’south’,’west’],
“restaurant-bookday”:[’monday’,’tuesday’,’wednesday’,’thursday’,’friday’,’saturday’,’sunday’],
...
“hotel-internet”:[’free’,’no’,’yes’],“hotel-area”:[‘centre’,‘east’,‘north’,‘south’,‘west’]
}
Nowconsiderthefollowingdialoguebetweentwopartiescalledthe“system”and“user”.Canyoutellmewhichofthe“slot”wasupdatedbythe“user”initslatestresponsetothe“system”?
PresenttheupdatesinJSONformat.Ifno“slots”wereupdates,returnanemptyJSONlist.Ifyouencounter“slot”thatwasrequestedbythe“user”thenfillthemwith“?”.Ifauserdoesnot
seemtocareaboutadiscussed“slot”fillitwith“dontcare”.
Input:
Previousstate:{}
“system”:“”
“user”:“I’mlookingforamoderatelypricedplacetoeatthat’sinthecentreoftown.”
Output:{“restaurant-pricerange”:“moderate”,“restaurant-area”:“centre”}
ImplicitDiscourseRelationClassification
Instructions:Giventwoargumentsandalistofconnectivewords,pleaseselectthemostlikelyconnectivebetweentwoarguments.
Belowarethedescriptionsoffourdiscourserelationlabels.Pleasefindthecorrectlabelforeachexample.
Temporal:Thetagtemporalisusedwhenthesituationsdescribedintheargumentsareintendedtoberelatedtemporally.
Contingency:ThetagContingencyisusedwhenthesituationdescribedbyoneargumentprovidesthereason,explanationorjustificationforthesituationdescribedbytheother.
Comparison:ThetagComparisonisusedwhenthediscourserelationbetweentwoargumentshighlightstheirdiffer-encesorsimilarities,includingdifferencesbetweenexpectedconsequences
andactualones.
Expansion:ThelabelExpansionisusedforrelationsthatexpandthediscourseandmoveitsnarrativeorexpositionforward.
[Few-shotexamples]
Input:
Arg1:Amcore,alsoabankholdingcompany,hasassetsof$1.06billion.
Arg2:Central’sassetsare$240million.
Question:Whatistheconnectivethatbestdescribestherelationbetweentwoarguments?
A.Temporal
B.Contingency
C.Comparison
D.Expansion
Answer:C
QueryRewrite
Instructions:Rewritethelastqueryfollowinginteractionintoawell-formed,contextindependentquery.Resolveanydisfluenciesorgrammaticalerrorsinthequery.
[Few-shotexamples]
Input:
User:TrytoreachForbesnow.
Bot:ForbesatWashingtonPost?OrForbesofPublishingDivision?
User:PublishingDivision.
Rewrite:ForbesofPublishingDivision
Table10: Examplesoftaskdesignforeachtaskinthecontextunderstandingbenchmark.
Task Coreference DST Discourse queryrewriting
Dataset WSC273 OntoNotes MultiWOZ PDTB3 MuDoCo QReCC InCar GECOR CANARD
N-example Zero-shot One-shot 5-shot 8-shot 10-shot 5-shot 10-shot 10-shot 5-shot
Table11: N-shotsettingsforeachtask&datasetthatyieldsthehighestscores. Foreachtaskandmodel,weuse
consistentN-shotsettingsforcomparison.