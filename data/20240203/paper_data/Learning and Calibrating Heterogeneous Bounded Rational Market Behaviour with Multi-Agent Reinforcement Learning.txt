Learning and Calibrating Heterogeneous Bounded Rational
Market Behaviour with Multi-Agent Reinforcement Learning
BenjaminPatrickEvans SumitraGanesh
JPMorganAIResearch JPMorganAIResearch
London,UnitedKingdom NewYork,USA
benjamin.x.evans@jpmorgan.com sumitra.ganesh@jpmorgan.com
ABSTRACT progressinreinforcementlearning(RL)helpstobringthiscloser
Agent-basedmodels(ABMs)haveshownpromiseformodelling toreality[1,14,62].However,somesignificantchallengesmust
variousrealworldphenomenaincompatiblewithtraditionalequilib- be addressed before this can happen. In this work, we address
riumanalysis.However,acriticalconcernisthemanualdefinition thefollowingessentialquestion:Howcanwelearnheterogeneous
ofbehaviouralrulesinABMs.Recentdevelopmentsinmulti-agent boundedrationalbehavioursinanABM?
reinforcementlearning(MARL)offerawaytoaddressthisissue Toaddressthisquestion,weintroduceanovelmulti-agentRL
fromanoptimisationperspective,whereagentsstrivetomaximise (MARL)approachwhereagentsexhibitskillheterogeneity[53],
theirutility,eliminatingtheneedformanualrulespecification.This constrainedbytheirstrategicprocessingcosts.Inthelimit,where
learning-focusedapproachalignswithestablishedeconomicand theseprocessingcosts→0,perfectlyrationalmutuallyconsistent
financialmodelsthroughtheuseofrationalutility-maximising equilibriumcanbeapproximated.Withuniformpriorbeliefsand
agents.However,thisrepresentationdepartsfromthefundamen- homogenousprocessingcostsamongagents,wecanapproximate
talmotivationforABMs:thatrealisticdynamicsemergingfrom quantalresponsetypeequilibrium.Withprocessingcosts→ ∞,
boundedrationalityandagentheterogeneitycanbemodelled.To agentsactbasedontheirpriorbeliefs,e.g.,drivenbyheuristics[27]
resolvethisapparentdisparitybetweenthetwoapproaches,wepro- orbiases[17].However,moregenerally,wecanmodelawiderange
poseanoveltechniqueforrepresentingheterogeneousprocessing- ofrealisticbehaviourwithheterogenousagentbounds.Thisframe-
constrainedagentswithinaMARLframework.Theproposedap- workaimstoenhancethesimulationofcomplexsocialsystems,
proachtreatsagentsasconstrainedoptimiserswithvaryingdegrees whichdiffersfrommanyMARLmethodologiesfocusedonlearning
ofstrategicskills,permittingdeparturefromstrictutilitymaximisa- optimalbehaviours.Instead,theworkalignswiththeliteratureon
tion.Behaviourislearntthroughrepeatedsimulationswithpolicy ABMs,focusingonunderstandingtheresultingrealisticdynamics
gradientstoadjustactionlikelihoods.Toallowefficientcomputa- emergingfromhumandecision-making.
tion,weuseparameterisedsharedpolicylearningwithdistribu- Contributions:Weproposeanapproachtoeffectivelylearn
tionsofagentskilllevels.Sharedpolicylearningavoidstheneed heterogeneousagentskilllevels(exhibitingdiversedeviationsfrom
foragentstolearnindividualpoliciesyetstillenablesaspectrum priorbeliefs)withinaMARLframework.Wedemonstratetheeffi-
ofboundedrationalbehaviours.Wevalidateourmodel’seffective- cacyoftheapproachacrossseveralfundamentalmulti-agenteco-
nessusingreal-worlddataonarangeofcanonical𝑛-agentsettings, nomicenvironments.Theproposedapproachofferssubstantially
demonstratingsignificantlyimprovedpredictivecapability. enhancedaccuracyinpredictinghumandecisions(alongwiththe
subsequentdynamics)incontrolledexperimentscomparedtocur-
KEYWORDS rentstate-of-the-artRLapproachesandotherequilibriumbench-
marks.Toimproveefficiency,weutiliseagentsupertypes[66]and
Multi-agentreinforcementlearning;Boundedrationality;Market
sharedpolicylearningtolearndiverseboundedrationalbehaviours.
simulation;Agent-basedmodelling;Skillheterogeneity
Heterogeneityisintroducedbyvaryingstrategicprocessingcostsof
agents,measuredthroughregulariseddivergencesfromtheirprior
1 INTRODUCTION
beliefs.Thisapproachisgeneral,expandinguponanewMARL
Agent-basedmodels(ABM)haveachievedsignificantsuccessin frameworkformodellingcomplexsystems,Phantom[3].
variousdomains,includingbusiness,epidemiology[33],economics,
and finance [5, 25]. However, despite these achievements, criti-
cismspersistwithindifferentcommunities[2],notablyineconom- 2 RELATEDWORK
ics[37,39,64],primarilyduetoconcernsregardingthedecision- While MARL algorithms have made significant advances in ap-
makingrulesinthesesystems.Frequently,theserulesaremanually proximatingequilibriawithincomplicatedenvironments[50],a
specifiedheuristics,placingsubstantialrelianceonthemodeller’s limitationisthatmostprevailingmethodologiesassumeagents
judgementasthesimulationresultsandvaliditydependuponthe tobeperfectlyrational.Thisassumptionisoftenoverlystringent
specificbehaviouralrulesutilised[48,64].Ontheotherhand,adap- whensimulatingcomplexsocialsystems[4],andmaymisscrucial
tiveagentsthatoptimiseautilityfunctionfindgreateracceptance real-worlddynamics[7].Tobroadentheapplicabilityoftheseap-
acrossdisciplines,astheagents’behaviourisautomaticallyderived proachesformodellingcomplexsystems,ourobjectiveistoextend
inaprincipledmanner. MARLframeworkstoaccountforagentheterogeneityandbounded
Hence,introducingadaptiveandlearningagentsintoABM,while strategicabilities.Indoingso,wemaketheconnectionwithABMs
allowingforheterogeneityandboundedrationality,couldalleviate whileautomatingsomeofthedifficultmodellingdesigndecisions
these concerns and improve the realism of the models. Recent (e.g.,determiningthebehaviouralrulesoftheagents)usingRL.
4202
beF
1
]AM.sc[
1v78700.2042:viXraRecentcomprehensiveexaminationscoveringRLtechniquesin 𝑎 ∈𝐴.Importantly,theseagentsmaynotactperfectlyrationally.
ABMarefeaturedin[51,63,70],emphasizingtheusefulnessof Thesystemischaracterisedbyastatespace𝑆,andagentspossess
learningagentbehaviours. a(potentiallypartial)observationofthecurrentstate𝑠 alongwith
𝑖
Behaviouraleconomicshasdevelopedmorerealisticmodelsof priorbeliefsabouttheirpotentialactions𝑞 (apriorprobability
𝑖
decision-makingthanthetraditionalhomoeconomicusperfectly distributionovertheactionspace).Thebehaviourofeachagent
rationalrepresentativeagent[40].Instead,thesemodelsoperate is governed by their policy 𝜋 , which is a mapping from states
𝑖
undertheframeworkofboundedrationality.Oneprominentap- to a distribution over actions. Agents act based on their policy
proachforrelaxingthestrictperfectlyrationalNashequilibrium 𝑎
𝑖
∼𝜋 𝑖,receivingreward𝑈 𝑖(𝑎 𝑖,𝑠 𝑖).Again,theseactionsmaynot
(NE) assumption and incorporating bounds in reasoning is the beperfectlyrationalandinsteadmaybesatisficing[9,58].
QuantalResponseEquilibrium(QRE)[43,44],whichallowsfor
deviationsfromoptimalresponsesandthepossibilityoferroneous 3.1 Components
play.Suchapproachesgenerallyfeatureconsistent(andcommon) 3.1.1 Reward. InstandardRL,agentsaimtolearnanoptimalpolicy
beliefsamongagents,forexample,byhavingthesameprocessing thatmaximisestheirexpectedcumulativediscountedreward𝑈:
costs across the population (mutual consistency). However, the
(cid:34) ∞ (cid:35)
n abe ie ld ityto toco "n bs ri id ne gr aa bg oen ut th ne et wero og ue tcn oe mity esha ns otbe foen res st er ee as bse led fd ru oe mto anth ae l- 𝜋 𝑖∗(𝑎|𝑠 𝑖)=m 𝜋a 𝑖xE 𝜋𝑖 ∑︁ 𝑡=0𝛾𝑡𝑈(𝑎 𝑡|𝑠 𝑖,𝑡) (1)
ysisofthehomogeneousdynamics"[45].Giventhatpopulations
However,often,inrealworldsystems,theagentsweseekto
inherentlyencompassaspectrumofbehavioursandbeliefs,a"rep-
modelareboundedlyrational,drivenbypriorbeliefsandalimited
resentative"agentoftenprovesinsufficient[30].Recognizingthe
amount of processing power to improve upon these priors. To
importance of strategic diversity, various extensions have been
addresstheseconsiderations,weincorporategenericlimitations
developedtoaccommodatearangeofagentbehaviours[52,53],re-
inagents’reasoningabilitiesbyreformulatingthemaximisation
laxingthemutualrationalityandmutualconsistencyassumptions
problemintoaconstrainedone:1
[12,19].Relaxingmutualconsistencyisbeneficialformulti-agent
settings,allowingforapopulationcharacterizedbyvaryinglevels
(cid:34) ∞ (cid:35)
o off ss utr ca hte gg ai mc eb -o tu hn ed oe rd etn ie cs ms[ o3 d8 e, ls49 is,6 o8 ft] e. nH lo imw ie tv ee dr, toth re elc ao tim vep lu yt sa ib mil pit ly
e
𝜋 𝑖𝜆 (𝑎|𝑠 𝑖)=m 𝜋axE 𝜋𝑖 ∑︁ 𝛾𝑡𝑈(𝑎 𝑡|𝑠 𝑖,𝑡)
(2)
𝑡=0
domains,promptingtheuseofapproximationmethods(suchas subjectto 𝐼(𝜋 𝑖,𝑠 𝑖,𝑡,𝑞 𝑖) <𝐼¯
MARL)withinmorecomplicatedenvironments[71].
DespitetheachievementsinbehaviouralgametheoryandRL,a whereagentsmaximise𝑈 whileadheringtoaconstraint𝐼¯ontheir
gappersistsincombiningthesemethodswithABMs,e.g.forlearn- processingcosts𝐼.Theprocessingcostrestrictshowfarthelearnt
ingthedecision-makingrules.WhileRLapproachesoftenprioritize policy𝜋 𝑖 candivergefromthepriorbelief𝑞 𝑖.Wecanequivalently
convergencetowardsrationalequilibrium,thisfocuscontrastswith reformulateEq.(1)asthemaximisationofamodifiedreward:
theprimaryobjectiveoftheABMcommunity:understandingthe (cid:34) ∞ (cid:35)
Tp oro tp he ert bie es stem ofe or ugi rn kg nf oro wm leh de gt ee ,r to hg een see mou ins ab lo wun od rked oly nr ca at pio tun ra il na gge ren ats l-. 𝜋 𝑖𝜆 (𝑎|𝑠 𝑖)=m 𝜋a 𝑖xE
𝜋𝑖
∑︁ 𝑡=0𝛾𝑡 (cid:0)𝑈(𝑎 𝑡|𝑠 𝑖,𝑡)−𝜆𝐼(𝜋 𝑖,𝑠 𝑖,𝑡,𝑞 𝑖)(cid:1) (3)
isticboundedbehavioursinMARListhatof[46],basedonrational where𝜆 controlsthestrengthofregularisation,modulatingthe
inattention(RI).Theauthorsof[46]underlinethattheexisting boundedness(orskill)oftheagent.As𝜆→∞,theagentisentirely
bodyofwork"failstoaddressthemodelingofboundedrationality drivenbytheirpriorbeliefs(performingnostrategicreasoning),
formoreaccurate[MARL]simulations,"proposinganinnovative whereasas𝜆→0,theagentisunboundedandapproximatesratio-
frameworktoaddressthislimitation.Ourworkdiffersinsomeim- nalbehaviour.Eq.(3)makestheformulationgeneralandcompatible
portantways.Specifically,weremovethedifficultprocessingcost withexistingRLalgorithmswithoutrequiringin-depthmodifica-
estimationrequiredin[46](asdiscussedinSection5)andallowfor tiontothelossoroptimisationprocess.
arbitrarypriorbeliefsforencodingbehaviouralbiases.Additionally,
weintroduceagentskillheterogeneity,learntthroughregularised 3.1.2 ProcessingCosts. Quantifyinginformationprocessingcosts
policies,andproposeanapproachforefficientlycalibratingthese inageneralisedmannerisdesirable,asthisenablescompatibility
policiestorealworlddynamics,allfeaturesyettobeconsidered. withexistingoptimisationalgorithms.Followingrecentachieve-
mentsinconstrainingagentdecision-makingusinginformation-
theoreticcosts[19],weadoptasimilarapproach.Thisinformation-
3 PROPOSEDAPPROACH theoretictreatmentabstractstheunderlyingcausesofsuchcon-
WeintroduceanovelMARLapproachtoeffectivelymodeladiverse straints,allowingafocusonlearningbehaviourwithoutnecessitat-
rangeofboundedrationalbehaviours(orvaryingskill).Thisap- inganin-depthunderstandingofthespecificpsychologicalfactors
proachprovesvaluableforcalibratingABMstoreal-worldsystems atplay.Fromanoptimisationstandpoint,thisisadvantageous,as
through learning regularised policies and, under limiting cases, theprocessremainsindependentoftheparticulardetailsofhow
establishinglinkstovariousexistingequilibriumsolutionconcepts. decisionsareformulated[59].
Our general formulation is as follows. We focus on 𝑁-agent
1Wemaintainexponentialdiscountinghere.However,hyperbolicdiscountingmight
systems,whereeachagent𝑖 ∈𝑁 seekstomaximisetheirreward
provideabetteralignmentwithhumandecision-making[54],althoughcurrentRL
(orutility)function𝑈 𝑖 bytakingactionsfromtheiractionspace approacheshavenotyetexhibitedsignificantdifferencesbetweenthetwo[22].Oneofthemostcommoninformation-theoreticconstraintsisan 3.1.3 HeterogeneousBehaviours. Effectivelylearningbehaviours
entropyconstraint,e.g.𝐻(𝜋 𝑖)=−(cid:205)𝑝(𝜋 𝑖)∗log(𝑝(𝜋 𝑖)),restricting thatcapturethediversityofthepopulation’sdecision-makingis
deviationsfromuniformbehaviour.Forexample,(thelogitform crucialforintegrationintoagent-basedsimulations.
of)QREcanbeseenasmaximisationunderanentropyconstraint. Twoinitialapproachescouldbeemployedforcapturinghet-
However,muchresearchhasshowntheusefulnessofincorporating erogeneous skills of agents: Firstly, learning optimal (homoge-
arbitrarypriorbeliefs(notjustuniform)[18],motivatingextensions nous) behaviours 𝜋∗ as in Eq. (1) and applying heterogeneous
thatmeasurethedivergencefromanarbitrarypriordistribution boundsatinference,e.g.,𝜋 𝑖 = 𝜋∗+𝜂 𝑖,where𝜂 𝑖 isanoiseterm.
basedontheKullback-Leibler(KL)divergenceD KL[47]. Secondly, individual learning with heterogeneous𝜆’s, i.e., 𝜋 𝑖 =
KLregularisationhasshownsuccessinrelevantdomains[36,60] max 𝜋E
𝜋
(cid:2)(cid:205) 𝑡∞ =0𝛾𝑡 (cid:0)𝑈(𝑎 𝑡|𝑠 𝑖,𝑡)−𝜆 𝑖𝐼(𝜋,𝑠 𝑖,𝑞 𝑖)(cid:1)(cid:3).Inthefollowingsec-
forformallycapturingthesecosts.Forexample,[36]demonstrates tion, we describe why these approaches are insufficient before
theusefulnessofapenaltyforminimisingD fromexpertpolicies proposinganalternativethatovercomestheselimitations.
KL
intreesearchand[60]analysesD inaRLcontextforimproving
KL
convergenceintwo-playergames.However,neitherofthesecon- Post-hocboundsatinference. Onepotentialapproachinvolves
optimising𝑈 withoutincludinginformationprocessingcostsdur-
sidersthedomainweproposehereforbettercapturinghuman-like
ingtraining(eliminatingboundedness)tolearn𝜋∗.Subsequently,
playincomplexmulti-agentsocialsystems.
thisboundednessparameterisappliedonlytothelearnedpoli-
Specifically,weproposeusingthefollowinginformationpro-
cessingcosts
ciesduringinference𝜋
𝑖
=𝜋∗+𝜂 𝑖,where𝜂
𝑖
isthenoiseterm.For
example,[8]appliesdropoutduringsimulation,andnoisyintro-
∑︁ 𝜋(𝑎|𝑠) spectionappliesnoiseintothedecisionsrelaxingtheequilibrium
𝐼(𝜋,𝑠,𝑞)=D KL(𝜋 ∥𝑞)= 𝜋(𝑎|𝑠)log
𝑞(𝑎|𝑠)
(4)
requirement[28].Suchmethodsintroducearangeofskilllevelsin
𝑎∈𝐴
actionexecution,e.g.throughintroducingnoise𝜂 intotheaction
𝑖
toconstrain𝜋 𝑖fromdivergingtoofarfromagents’priorbeliefs𝑞at selectionprocess.However,ifheterogeneousboundswereimple-
eachstate,limitingtheirstrategicabilities.Eq.(4)canalsobeseen mentedinthismanner,agentswouldnotlearnhowtoadaptto
asequivalenttoenforcingan𝐻constraintwhenassumingtheprior thebehaviourofotherboundedagents,asthebestresponsetothe
beliefsareuniform(makingconnectionswithQRE,asdiscussedin optimalpolicyisnotnecessarilythebestresponsetoanoisypolicy
Section5).Additionally,withthisrepresentation,thecontribution 𝐵𝑅(𝜋∗) (cid:46)𝐵𝑅(𝜋 𝑖).Toillustratethispoint,considerasimplerock-
ofaspecificaction𝑎tothedivergencecanbeidentified,e.g., paper-scissors(RPS)environment.InRPS,theperfectlyrational
equilibriumpolicyis𝜋∗={𝑝(R),𝑝(P),𝑝(S)}={1,1,1 }.However,
3 3 3
𝐼
𝑎(𝜋,𝑠,𝑞)=𝜋(𝑎|𝑠)log𝜋(𝑎|𝑠)
(5)
ifagent2spolicyisinsteadfixedas𝜋2 = {0 3, 30,3 3}(e.g.theyare
𝑞(𝑎|𝑠) boundedlyrationalandbiasedtowardsplaying𝑆),therationalbest
meaningtheadjustmenttotherewardfunctioninEq.(3)canbe responseforagent1is𝜋1 =𝐵𝑅(𝜋2) = {3 3,0 3,0 3}.Hadagent1not
directlylinkedto𝑎ratherthan𝜋 ,i.e.,
observed𝑎2 ∼ 𝜋2 duringtraining,theywouldnothavelearned
𝑖
toexploit𝜋2.Whileasimpleexample,thisconsiderationbecomes
pivotalindemonstratingtheemergenceofauto-curricula[6].To
(cid:34) ∞ (cid:35) capturetheinterplayamongheterogeneouslyskilledagents,the
𝜋 𝑖𝜆 (𝑎|𝑠)=m 𝜋a 𝑖xE
𝜋𝑖
∑︁ 𝑡=0𝛾𝑡 (cid:0)𝑈(𝑎 𝑡|𝑠 𝑖,𝑡)−𝜆𝐼 𝑎𝑡(𝜋 𝑖,𝑠 𝑖,𝑡,𝑞 𝑖)(cid:1) (6)
n pro oti co en sso rf ab tho eu rn td he ad nn oe nss lym du us rt inb ge ip nr fe es re enn ct et .hroughoutthelearning
whichisadvantageousforoptimisationpurposes.Weusethisfor-
mulationthroughout.Aswesamplemoreactionsfromthispolicy, IndividualLearning. Analternativeapproachinvolvesassigning
wewouldapproximateD KLas𝐼(𝜋,𝑠,𝑞)=(cid:205) 𝑎∈𝐴𝐼 𝑎(𝜋,𝑠,𝑞). h Ee qt .e (r 3o ),g ee .n ge .o wu is thpr ho ec te es rs oi gn eg np eoen usal lt oie gs it𝜆 r𝑖 et so poe nac dh era sg [e 3n 0t ].𝜋 I𝑖 n= th𝜋 is𝜆𝑖 sci en
-
InRL,information-theoreticregularisationisoftenemployedto
nario,anystandardMARLalgorithmcouldbeemployed,wherein
enhancetheconvergenceorrobustnessofalgorithms.Forinstance,
allagents,eachgovernedbytheiruniqueconstraint,strivetoopti-
ProximalPolicyOptimization(PPO)utilisesaD penaltytermto
KL
misetheirrewards,adjustingtheirbehavioursinresponsetothe
preventexcessivelylargechangesinthepolicyduringtrainingsteps
observedoutcomes.However,thismethodwouldproveinefficient
andimprovetheconvergence.Similarly,theSoftActor-Critic(SAC)
duetothenecessityoflearning𝑁individualpoliciesandcalibrating
algorithmemploysD initspolicyimprovementstep,limiting
KL
𝑁 differentindividualisedprocessingcosts𝜆 (oneforeachagent).
divergencefromthepreviousQ-function[31].Moreover,Maximum 𝑖
ThisinefficiencybecomesacrucialconcernasABMsoftenhave
EntropyRLintroducesanentropytermtoenhanceexploration,con-
alarge𝑁.Additionally,thelearntpolicieswouldnotgeneralise
vergence,androbustness[20,21].Incontrast,ourapproachrestricts
acrossdifferent𝜆 ,requiringnewtrainingeachtimeanew𝜆 is
divergencefromanarbitrarypriorbelief𝑞toreflecttheconstraints 𝑖 𝑖
introduced.
ininformationprocessingpresentduringhumandecision-making
Itbecomesclearweneedascalablealternativethatcandealwith
ratherthanbeingaimedatimprovingthealgorithm’sconvergence.
theheterogeneousboundedlyrationalbehaviourofagents.
Thesepriorbeliefs𝑞(alsocalled"magnets"[60]or"anchors"[36])
maychangethroughouttrainingandinference(e.g.withupdated
information)andcantakemanyforms,forexample,demonstrating 3.2 SharedPolicyLearning
biastowardscertainactions,encodingheuristics,averagingover Rather than learning individual policies with heterogeneous𝜆 ,
𝑖
pastdecisions,orpreferringhistoricallywell-performingactions. allaimingtosolveEq.(3),wewishtolearnageneralisedpolicy𝜋(...|𝑠 𝑖,𝑖,𝜆 𝑖,𝑞 𝑖).Thisrepresentationtreatsagents’priorbeliefs useofsupertypestocapturediverseboundedrationalbehaviours
andprocessingresourcesaspartoftheobservationspace(andfor basedontheregularisedpoliciesintroducedinEq.(3).
simplicityofnotation,wewilluse𝜋 𝑖(...|𝑠 𝑖) =𝜋(...|𝑠 𝑖,𝑖,𝜆 𝑖,𝑞 𝑖)), Undertheproposedapproach,aregularisedpolicyforthesuper-
enablinggeneralisedpolicylearningbasedonthesestateobserva- typeisestablished𝜋D,whichisexposedtodifferentregularisation
tions.Thisformulationprovidesawayofefficientlyrepresentinga strengths𝜆 𝑖 ∼ D throughouttraining.𝜋D learnstoextrapolate
diversepopulationofagents𝑖 ∈𝑁 withvaryingboundsinstrategic overtheregularisationstrengths,reducingthenumberofpolicies
reasoningabilities𝜆 𝑖 throughasingleparameterisedpolicywith to train while still enabling heterogeneous behaviour. Through
anaugmentedobservationspace. thisprocess,agentslearntoadapttheirbehaviourinresponseto
However,calibrating𝜆 𝑖 remainsanimportantissue.Whilecali- thevaryingprocessingresourcesacrosstheagentpopulation,ac-
brating𝜆 𝑖 toeachagent𝑖mayseemideal,thisiscomputationally countingforpotentialautocurricula.Theinputstothesupertype
impracticalwithlarger𝑁,andadditionally,couldleadtooverfit- (𝜇,𝜎)arecalibratedsuchthatthatthesimulationoutcomeclosely
tingtospecificbehaviouralparametersduetothelargenumberof matchestherealworlddynamics(fromthecalibrationdata).An
requiredparameters(𝑁).Furthermore,inpractice,as𝜆 𝑖 areunob- overviewofthishigh-levelprocessisdepictedinFig.1.
served,assigningthesevaluesexactlyisdifficult. Thesharedpolicy𝜋Dtakesanagent’sid𝑖,processingresources
Weadoptanalternativeapproachtoaddressthischallengeby 𝜆 ,andpriorbeliefs𝑞 asinputs(ascomponentsof𝑠 ).Including𝑖
𝑖 𝑖 𝑖
assigningindividualstrategicprocessingresourcesassamplesfrom facilitatesthelearningof(potentially)competitivebehaviourbe-
aprobabilitydistribution𝜆 𝑖 ∼ D,addressingtheuncertaintyof tween agents of the same supertype. By adjusting the input 𝜆 𝑖
theagents’exact𝜆 𝑖 valuesandkeepingthenumberoffreeparame- values,𝜋D caneffectivelydemonstrateaspectrumofskilllevels,
terslow.AnyDcouldbeutilised(andtheproposedapproachis whileonlyneedingtolearnasingle(generic)policy.Allowingfor
agnostictotheparticulardistributionused),buthere,weemploy arbitrary𝑞 accountsfortheeffectofvariouspriorbeliefs.
𝑖
theGaussiandistributionD =N(𝜇,𝜎),where𝜇controlsthemean Theunderlyingassumptionofthissupertypeapproachisthatall
processingcosts,and𝜎theheterogeneity.Thisway,weonlyneed agentsinthesupertypehavethesame𝑈 function;however,they
tocalibratetheparameters𝜇and𝜎(ratherthan𝑁 separateparam- possessvaryinglevelsofskillinmaximising𝑈.Giventheinher-
eters),whichistypically<<𝑁.Forinstance,whendealingwith entuncertaintysurroundingtheprecisenatureofagentdecision-
𝑁 =100agents,wearecalibratingjust2parametersinsteadof100, making,acompellingcaseismadeforcapturingaspectrumof
helpingtoavoidoverspecification.Duringlearning,thissampling regularisedbehaviours.Theregularisationoffersdualadvantages:
approachallowsforinterpolationacrossarangeof𝜆 𝑖,reducingthe firstly,itenablesdeviationsfromperfectrationalityinagentbe-
computationalcomplexityandenforcinga"smoother"policy.This haviour;secondly,iteffectivelyencompassesuncertaintiesfrom
smoothnessarisesfromobservingmanydifferentbehavioursdur- boththemodeller’sperspectiveandtheagentsbeingmodelled(i.e.,
ingtraining,resultinginamorerobustpolicyforcedtointerpolate uncertaintyinthemodel’sformulationandtheagents’decision
across𝜆 𝑖 values,reducingthepotentialofoverfitting. processes)[18].Thismotivationalignswiththeuseofbounded
rationalityinsituationscharacterisedbyfundamentaluncertainty
Reward r1, r2,... rn oftheagent[26]andalsohelpstoaddressconcernsregardingmod-
Action a1 ellerjudgement(e.g.modelmisspecification)bypermittingarange
Share 𝜋d 𝒟Policy Action a2 Environment ofinformation-constrainedbehaviour[55].
Action an
4 EMPIRICALRESULTS:𝑛−AGENTSETTINGS
New State s1, s2,... sn
Toverifythattheproposedapproachcancapturearangeofinterest-
ingbehaviournotpredictedbytheanalyticallyderivedequilibrium
λ1 λ2 λN
Real world outcomes
orstandardstate-of-the-artMARLapproaches,wecomparethe
predictionsfromtheproposedmodelagainsttheseapproacheson
μ, σ
Supertype Calibrated to data arangeofcanonical𝑛-agenteconomicenvironmentsinvolving
λi ~ 𝒟 humanparticipants.
μ, σ
Regularisation for Boundedness strength
4.1 ProcessOverview
Figure1:ProposedApproach:Sharedpolicylearningwith Weassesstheperformanceoftheproposedapproachinthreewell-
heterogeneousboundsthroughagentsupertypes. established multi-agent economic environments: supply chains,
oligopolies, and cobweb markets. To validate our approach, we
To learn a generalised policy 𝜋D for 𝜆 𝑖 ∼ D, we use agent leveragelaboratoryexperimentsconductedineachsetting,com-
supertypes[66],enablingefficientscalingthroughsharedpolicy paringthepredictionswithactualhumanbehaviour.Wecompare
learning,whilestillcapturingarangeofbehaviours.Agentsuper- theproposedapproachwithanalyticallyderivedsolutionsanda
typeshaveexhibitedpromise,particularlyinapplicationssuchas state-of-the-artMARLalgorithm(PPO).Ineachcase,weperform
calibratingrationalbehaviourinover-the-countermarkets[65].
However,thepotentialforincorporatingheterogeneousstrategic
2TheCournotcompetitionenvironmentseachcarry1weightwhencomputingtheav-
reasoningskillstobetterapproximatehumandecision-makinghas 2
eragetoaddresstheinterdependenceandavoidbiasingtheaverageranking(although
yettobeexplored.Here,weproposeanovelapproachtoextendthe inthiscase,therankingswouldnotchangewithoutsuchaweighting)Table 1: 5𝑥2-fold validation results for each environment. 4.2 Results
Each cell displays the root mean squared errors as mean The out-of-sample performance of each algorithm is compared
± standard deviation, along with (rankings) for between- inTable1.Theproposedapproachachievesthehighestaccuracy
environment comparison [15]. The last row presents the acrossallthreeenvironments,resultinginthebestoverallrank.
2
averagerank .Lowerrankingsindicatebetterperformance. Thestate-of-the-art(standard)MARLapproachandtheanalytically
derivedrationalcasegenerallyperformequivalently,indicatingthat
Rational MARL Proposed theMARLalgorithmapproximatedthetruerationalequilibrium
SupplyChain 0.33±0.004(2.5) 0.33±0.004(2.5) 0.02±0.005(1) well.However,boththealternativesperformedpoorlyincapturing
theexperimentaldata,demonstratingthatrationalityandhomo-
Cournot
-Duopoly 0.16±0.001(3) 0.13±0.001(2) 0.04±0.001(1) geneity are too strict of an assumption even in these relatively
-Triopoly 0.16±0.002(3) 0.15±0.002(2) 0.03±0.001(1) simplemulti-agentsettings.Theseresultsmotivatetherelaxation
Cobweb 0.02±<0.001(2) 0.03±<0.001(3) 0.01±<0.001(1) ofperfectrationalityandtheintroductionofskillheterogeneity
whenusingMARLtomodelcomplexsystems.
Rank 2.5 2.5 1
Tobetterunderstandtheresultsandthereasonfortheimproved
capabilitiesoftheproposedapproach,weanalyseeachenviron-
mentinmoredetail.Foreachenvironment,webeginwithabrief
Worst Worst description,beforepresentingtheresults.
4.2.1 SupplyChains.
Description. Thesupplychainenvironmentisacapacityalloca-
Best tionproblemwithasinglegoodwithcost𝑐andprice𝑝.Thereis
0.250.5 1.0 µ2.5 5.010.0 Best 0.00 0.25 0 σ.5 ∗0 0.75 1.00 onesupplierwithalimitedcapacity𝐾,and𝐼 retailers.Eachretailer
(a)𝜇,𝜎∗ (b)𝜎∗averagedacrossvaluesof𝜇 𝑖 makesarequest0 < 𝑥 𝑖 ≤ 𝑋,𝑥 𝑖 ∈ Z,andthesupplierresponds
byoffering𝑦 .Retailersareallocatedgoodsproportionatetotheir
𝑖
Figure2:Triopolycalibrationresultsforvaluesofthebound-
request𝑦
𝑖
∝𝑥 𝑖:
ednessparameter𝜇andheterogeneityparameter𝜎∗
𝑥
𝑦
𝑖
=𝐾×
(cid:205)
𝑖
𝑥
(7)
𝑗∈𝐼 𝑗
inducingthepotentialfor(rationally)inflatedordersizestoen-
repeated5x2cross-foldvalidation[15]toestimatethegeneralisa-
suretherequiredquantitiesaremet.Eachretailerreceivesafixed
tionability,ensuringthemodelsdonotoverfittothecalibration
demand𝐷 > 𝐾,i.e.,resourcesarelimited.Therewardisgivenby
data.Thesquared𝐿2-lossfunction(themeansquarederror)isused 𝐼
asourperformancemetric[16].Inthepresentedtables,wereport
therootmeansquarederrorforinterpretability.Tofacilitatecom-
𝑈(𝑥 𝑖)=𝐷×(𝑝−𝑐)−𝜔×max(𝑦 𝑖−𝐷,0)−𝑠×max(𝐷−𝑦 𝑖,0) (8)
parisonsacrossenvironments,weuserankingsbasedonresulting
errors(wherethelowesterrorreceivesrank=1,andtiesaresplit where𝜔isthewastagecost,and𝑠istheshortagecost(𝑠 =𝑝−𝑐).
byusingtheaveragerankhadtherebeennoties)[15].Tocalibrate Therational(Nash)solutiontothistask,irrespectiveof𝜔,𝑠(when
ourmodel,weperformagridsearchover𝜇,𝜎values,choosing𝜇,𝜎 inthelimitedcapacitycaseof𝐾 <𝐼∗𝐷),isforretailerstosubmit
withthelowesttrainingerrorforuseontheunseentestset.Theop- theirmaximumrequest𝑋,eachretailerthenreceiving𝑦 𝑖 = 𝐾 𝐼 units
timisationprocessneverseesthetestdata.Additionalexperiment duetotheproportionalallocation.Anylowerofarequestwould
detailsaregiveninAppendixA. resultintheretailerreceiving𝑦 < 𝐾 <𝐷.
𝑖 𝐼
Weutilisetheexperimentalresultsof[11],with𝐼 = 2,𝜔 = 2,
4.1.1 Calibration. The calibration results for one environment and𝑠 =5.Therewere30subjects,composedofuniversitystudents,
(triopolies), displaying the values of theheterogeneity (𝜎∗) and
randomlypairedin30repeateddecisionroundstomakeagame
boundedness(𝜇)parameters,areshowninFig.2.Similarplotsare
withtworetailersineachround.Thecapacityis𝐾 =90,witheach
availableforallenvironmentsinAppendixA.1.1.Theproposed
retailerreceivingdemand𝐷 = 50andabletomakeamaximum
approachofferstheflexibilitytoincorporateperfectrationalityor
requestof𝑋 =100.As2𝐷 >𝐾,wearefacedwithlimitedcapacity.
homogeneitybysetting𝜇 = 0(removingbounds)or𝜎∗ = 0(re-
movingheterogeneity).However,it’snoteworthythattheoptimal Results. TheexperimentalresultsaredisplayedinFig.3,showing
valuesneveralignwith𝜇 = 0or𝜎∗ = 0,highlightingtheuseful- substantialdeviationsfrompurelyrational(Nash)play.TheNE
nessofbothheterogeneityandprocessingcostsacrossallthree istorequestthemaximum𝑥 𝑖 = 𝑋 = 100.ThestandardMARL
environments.Furthermore,sincehomogeneityandunbounded approachlearnstheNEhere;however,thisisapoorpredictorof
reasoningcanbeconsideredspecialcasesofourproposedapproach, whathappensexperimentally(Fig.3).Experimentally,themost
thiseliminatestheneedtodeterminesuchassumptionsapriori(as commonlyoccurringrequestsareinthe60−80range,farlower
oftenrequiredinmanyexistingmethods).Instead,ourapproach than the NE. The proposed approach is a very good fit for the
enablesthecalibrationofthesepropertiesbasedonthespecific experimentallyobservedbehaviour,capturingtheoveralltrend,
environmentsofinterest. demonstratingthatsubjectshavevaryingstrategicbounds,giving
∗σ
0.050.01.052.05.00.1
knaR
naeM1.0 Proposed Proposed Proposed
MARL MARL MARL
0.8 Rational Experimental Experimental Data
Experimental Data
Rational Rational
0.6
1.0 1.0
0.4
0.8 0.8
0.2
0.0 0.6 0.6
0-3940-4950-5960-6970-7980-8990-99100
Request x i 0.4 0.4
0.2 0.2
Figure 3: Supply Chain. Experimental data from [11] are
shownasgreybars.Theproposedapproachisshownwith
0.0 0.0
theorangeline(foronecalibrationfold).ThestandardMARL 10 20 30 10 20 30
approachisshownasthedashedpurpleline,andtheNEis q i q i
denotedbytheblackbar.
(a)Duopoly (b)Triopoly
risetoarangeofoutcomesnotpredictedbyarationalrepresentative 0.2
agent,andhelpingtomotivatetheboundedrationalityassumptions.
4.2.2 CournotOligopoly. 0.0
10 15 20 25 30
Description. TheCournotcompetitionisanenvironmentmod- q i
ellingoligopoliesinamarket.InaCournotmarket,𝐾 firmsmust
simultaneouslychoosewhatquantities𝑞 𝑖 ∈ Zofahomogenous (c)Duopolywithaprioripreferencetowardsprominentnumbers
goodtoproduce.Therewardforfirm𝑖dependsonthemarketprice
(10,15,20,25,30),reflectingacognitivebias.
𝑝ofthegoodandtheindividual𝑞 ,i.e.:
𝑖
Figure4:Cournotcompetitions.Experimentaldatafrom[23]
𝑈
𝑖
=𝑝×𝑞
𝑖
(9) isshownasgreybars.Theproposedapproachisshownwith
theorangeline(foronecalibrationfold).ThestandardMARL
where𝑝isdeterminedbythetotalproductionofallgoods,i.e.,
approachisshownasthedashedpurpleline,andtheNEis
𝐾 denotedbytheblackbar.
∑︁
𝑝 =𝐴−𝐵× 𝑞
𝑘
(10)
𝑘=1
Weusetheexperimentaldatafrom[23,32],forduopoliesand whichthemodelwithuniformpriorscannotcapture.Thesepeaks
triopolies(experiments7,8,9,10from[23]).Following[32],wegroup cannotbeexplainedfromexpectedrewardalone,asthereisno
experiments7,10(duopoly)togetherandexperiments8,9(triopoly) particularreasonthat15wouldhavesuchhighpreference.Instead,
together.Intheseexperiments,𝐴=2.4,𝐵=0.04,and8≤𝑞 𝑖 ≤32. these demonstrate an a priori preference of the agents towards
Therewere64participantsfortheduopolyexperiments,and66for particularprominentnumbers(0,5,10,...),aknowncognitivebias
thetriopoly,composedofuniversitystudents. [10,13].Owingtothemodel’sflexibilityinallowingforarbitrary
priorbeliefs,thiscanbemodelledwith𝑞 withhigherweightings
Results. Theresultsforduopolies(triopolies)arepresentedin 𝑖
ontheseprominentnumbers.Anexampleoftheresultingdecisions
Fig.4a(Fig.4b).Withtheexperimentaldata,weseeasignificant
whenusingsuchpriorsisshowninFig.4c,providingasignificantly
deviationfromboththerationalbehaviourandthestandardMARL
improvedfit,capturingalloftheexperimentalpeaks.Wedonot
predictions.TheuniqueNEforduopoliesandtriopoliesis20and15
usesuchamodelwhencomparingresultsinTable1,asthismodifi-
respectively.Whiletheseactionsarethemostcommonineachcase,
cationwasmadepost-hoc(afterseeingtheexperimentaldata),but
theseoccurrencescomprise≤20%ofthetotaldecisions,andthe
itshowstheusefulnessofincorporatingpriorbeliefswhenknown,
remaining≈80%aresub-optimaldecisions(undertheassumption
demonstratinganadditionalstrengthofthemodel.
ofmutualrationality).Again,theproposedmodelisagoodfitforthe
experimentaldatainbothduopoliesandtriopolies,capturingthis
4.2.3 CobwebMarket.
significantdeviationfromtheoptimalchoicewhilestillcapturing
themaximalpeakfromtheexperimentaldata. Description. Inacobwebmarket[35],thereare𝐾producerswho
Undertheprocessingcostconstraint,agentschooseactionspro- mustestimatetheprice𝑝ˆ 𝑖,𝑡 ofagoodatthenexttimestep𝑡.The
portionatetotheexpectedrewardandthelevelofregularisation rewardforaproducer𝑖isbasedontheaccuracyoftheirprediction
intheirdecisionfunction(theirskilllevel).Thismeansthereare comparedtothemarketprice𝑝 𝑡:
specificoverrepresentativepeaksintheexperimentaldata,forex-
ample,at15and25intheduopolycaseand20inthetriopolycase, 𝑈 𝑖,𝑡 =max(0,1300−260(𝑝 𝑡 −𝑝ˆ 𝑖,𝑡)2 ) (11)
stseuqer
fo
noitroporP
noitroporP
noitroporP
noitroporPwhichislowerboundby0,e.g.,theproducerscannotreceivenega- 4.3 KeyTakeaways
tiveutilities.Producershavenocontactwithothers,butattheend Theproposedapproachdemonstratedstrongout-of-sampleper-
ofeachround,producersobservetherealisedmarketprice𝑝 𝑡. formanceacrossthesethreeeconomicandfinancialenvironments,
Themarketpricedependsonthedemand𝐷andsupplycurves outperformingthecomparisonsandvalidatingthemodelincon-
𝑆.𝐷islinearwithpriceandissubjecttosmallnormallydistributed trolledenvironments.Specifically,weshowcasedthevalueof:
demandshocks𝜂 ,and𝑆non-linearlyincreaseswiththeproducer’s
𝑡
expectedprice,i.e., • Boundedness:Incorporatingboundedrationalityresulted
insubstantiallyimprovedpredictiveaccuracy(Table1).
𝐷(𝑝 𝑡)=𝑎−𝑏𝑝
𝑡
+𝜂
𝑡
(12) • Heterogeneity:Allowingforheterogeneousprocessing
𝑆(𝑝ˆ 𝑖,𝑡)=tanh(𝜓(𝑝ˆ
𝑖,𝑡
−𝐾))+1
costsimproveduponassumingmutualconsistency(Fig.2b)
where𝜓 controlsthenon-linearityandstabilityofthemarket.The • Nonuniformpriors:Arbitrarypriorbeliefsexplained
realisedmarketpriceisgivenby phenomenaincompatiblewithdeviationsfromexpected
utilityalone(Fig.4c)
𝑝 𝑡 =
𝑎−(cid:205) 𝑘∈𝐾𝑆(𝑝ˆ 𝑘,𝑡)
+𝜖 𝑡 (13) Additionally,whiletheproposedapproachrelaxesthesethreeas-
𝑏
sumptions,ifdesired,thesecanstillberecoveredasspecial(limit)
where𝜖
𝑡
∝𝜂 𝑡.Underrationalexpectations,producersallpredictthe
casesasdiscussedinSection5.Thebenefitoftheproposedap-
pricetobetheintersectionof𝑆and𝐷,𝑝∗,e.g.,𝑝¯
𝑖,𝑡
=𝑝∗+𝜖 𝑡,∀𝑖 ∈𝐾,
meaningtherationalpredictionswill,onaverage,fallinlinewith
proachisthattheseassumptionsdonotneedtobeestablisheda
theequilibriumpricewithfluctuations∝𝜖 𝑡.
priori,rathertheyarecalibratedtotheenvironmentofinterest.
Weutilisetheexperimentaldataof[35],with𝑎=13.8,𝑏 =1.5,
𝜖 𝑡 ∼ N(0,0.5) and𝜓 = 2.Therewere36participants,generally 5 DISCUSSIONANDRELATIONTO
undergraduateeconomics,psychology,andsciencestudents. EQUILIBRIUMSOLUTIONS
Flexibilityisoneofthemodel’sstrengths.However,thisflexibility
comesattheexpenseofexactanalyticaltractability,andgenerally,
0.15
Rational wearelimitedbythetheoreticalguaranteesoftheunderlyingRL
MARL
algorithm(here,PPO).Despitethis,inthissectionweshowtherela-
Proposed
0.10 Experimental tiontothedecisionfunctionsofotherequilibriumsolutionconcepts
andprovidediscussionsontheequilibriumapproximations.
0.05 QuantalResponseEquilibrium. Withhomogeneousprocessing
costs𝜆 𝑖 = 𝜆 anduniformpriorbeliefs𝑞 𝑖(𝑎) = 𝑞,theapproach
canbeseenasapproximatingQRE(asQREconvergestoNEwith
0.00
0 2 4 6 8 10 𝜆→0[29],approximationofNEtoo).Usingasimilarformulation
Realized price p t toSection3.1.1,withQRE,eachagentchooses𝑎tomaximise𝑈,
subjecttoanentropy𝐻 constraint:
Figure5:Distributionof𝑝 𝑡 incobwebmarkets.Experimental
datafrom[35]isshownasgreybars.Theproposedapproach max𝜋 𝑖(𝑎)𝑈(𝑎|𝜋 −𝑖)
(14)
isshownwiththeorangeline(foronecalibrationfold).The subjectto 𝐻(𝜋 𝑖) ≥𝐻 min
standardMARLapproachisshownasthedashedpurpleline,
andtheblacklinedenotestherationalexpectationssolution. where𝜋 −𝑖 givestheactionprofileoftheotheragents.Toderive
thequantalresponsedecisionfunctionQR,weusethemethod
𝑖
Results. ThecobwebmarketresultsarevisualisedinFig.5,dis- ofLagrangemultipliersandtheprincipleofmaximumentropyto
playingthedistributionofrealisedprices.BoththestandardMARL convertthisintoanunconstrainedoptimisationproblem.Giventhe
approachandtherationalexpectationsarepoorpredictorsofthe usualconstraintsontheprobabilityfunction(thatQR 𝑖(𝑎) ≥0,∀𝑎
observedphenomenafromtheexperimentaldata.Whilethemean and(cid:205) 𝑎∈𝐴QR 𝑖(𝑎)=1),wegetthefollowingLagrangian[18]:
oftheexperimentaldataoftenalignswiththerationalandMARL
case,thedistributionspreadisfarbroader,indicatingpersistent
(cid:32) (cid:33) (cid:32) (cid:33)
excessvolatility,withmuchlargerstandarddeviationsthanthose ∑︁ ∑︁
expectedundertherationalexpectationshypothesisorMARLap-
L=− QR 𝑖(𝑎)𝑈(𝑎|QR −𝑖)−𝜁 QR 𝑖(𝑎)−1 +𝜆 𝐻(QR 𝑖)−𝐻 min
𝑎∈𝐴 𝑎∈𝐴
proach.Thisisanoteworthystylisedfactofmarketsincompatible (15)
withtherationalityassumptionofallagents[34,35].Asexcess wheretakingthefirstorderconditionsandsolvingforQR yields
𝑖
volatilityisknowntooccurinmanymarkets[67],understanding
thecausesandbeingabletomodelthisvolatilityisanimportantuse 𝑒𝑈(𝑎|QR −𝑖)/𝜆
o thf eA mBM ea. nTh ofe tp hr eop do as te ad aa np dpr thoa ec oh vo eff rae lr ls da ism tru ibch utb ioe ntte or ffi pt r, ic ca ep fltu ur ci tn ug
-
QR 𝑖(𝑎)=
(cid:205) 𝑎′∈𝐴𝑒𝑈(𝑎′|QR −𝑖)/𝜆
(16)
ations, reproducing the observed excess price volatility (Fig. 5), TodemonstratethatthedecisionfunctionimpliedbytheD
KL
providinganexplanationoftheendogenousformationofexcess constraintinEq.(4)(withuniformpriorsandhomogenous𝜆)re-
volatilitybasedonboundedrationality. ducestothesamefunctionalformasEq.(16),weget:
noitroporPutiliseinEq.(4)doesnothavethissamedependence,suchestima-
∑︁ 𝜋(𝑎|𝑠) ∑︁ tionisnotrequired,providinganalternativeformulationallowing
𝐼(𝜋,𝑠,𝑞)= 𝜋(𝑎|𝑠)log = 𝜋(𝑎|𝑠)(log(𝜋(𝑎|𝑠))−𝐶) forarbitrarypriorbeliefs𝑞 ,usefulforrepresentingcognitivebiases
𝑞(𝑎|𝑠) 𝑖
𝑎∈𝐴 𝑎∈𝐴 (asdemonstratedinFig.4c)orencodingbehaviouralheuristics.Fur-
(17)
thermore,asdiscussed,weallowforarangeofheterogeneousagent
pluggingintoL
skillslearntthroughregularisedpolicies,andproposeanapproach
(cid:32) (cid:33)
∑︁ ∑︁ forefficientlycalibratingthesepolicieswithagentsupertypesand
L=− 𝜋 𝑖(𝑎|𝑠 𝑖)𝑈(𝑎|𝜋 −𝑖)−𝜁 𝜋 𝑖(𝑎|𝑠 𝑖)−1 +
sharedpolicylearning,bothyettobeconsidered.
𝑎∈𝐴 𝑎∈𝐴
(18)
(cid:32) (cid:33)
𝜆 ∑︁ 𝜋(𝑎|𝑠)(log(𝜋(𝑎|𝑠))−𝐶)−𝐼¯ 6 CONCLUSIONS
Agent-basedmodelshavemuchpromiseforexplainingcomplex
𝑎∈𝐴
phenomenainabroadrangeofdisciplines.However,akeycriti-
andthedecisionfunctionreducesto:
cismishowthebehaviouralrulesaredefined.Learningrealistic
𝐶𝑒𝑈(𝑎|𝑠𝑖)/𝜆 𝑒𝑈(𝑎|𝑠𝑖)/𝜆
behaviouralrulescalibratedtoreal-worldsystemsisessentialtoim-
𝜋 𝑖(𝑎|𝑠 𝑖)= = (19)
(cid:205) 𝑎′∈𝐴𝐶𝑒𝑈(𝑎′|𝑠𝑖)/𝜆 (cid:205) 𝑎′∈𝐴𝑒𝑈(𝑎′|𝑠𝑖)/𝜆 provethemodelsandpromotecontinueduptake.Inthiswork,we
proposedanefficientMARLapproachforinferringthesedecisions
confirmingequivalentfunctionalformstoEq.(16)underuniformity
bycalibratingheterogeneouslyskilledlearningagentstoreal-world
andhomogeneity.ThekeydifferenceisQR dependsdirectlyonthe
𝑖 systemsthroughsharedpolicylearningandagentsupertypes.
policiesofotheragentsQR ,whereas𝜋 capturesthisindirectly
−𝑖 𝑖 Undertheproposedapproach,agentspossessdiversestrategic
viathestate𝑠 .
𝑖
processing abilities, represented through regularisation in their
TheQREthencorrespondstoafixedpointoftheseQRfunctions
decisionfunction.Thisregularisationisintheformofinformation
[29],assumingthat𝜆 ishomogeneousandcommonknowledge
processingcosts,leadingtovaryinglevelsofboundedlyrational
amongtheagents.Incontrast,undertheproposedapproach,rather
strategicbehaviour,dependingonthestrengthofregularisation.
thanexplicitlyattemptingtofindthefixedpointsolution,gradient
Thisagentskillheterogeneityisacriticalaspectofmanysystems
descentandsimulationareusedtofind𝜋 thatmaximises𝑈,witha
𝑖
andisadeparturefromtraditionalequilibriumdefinitions.How-
neuralnetwork𝑓 (withinputs𝑠 including𝑞,𝜆 ),andnocommon
𝑖 𝑖 𝑖
ever,wedemonstratethatthisheterogeneitybettercapturesmany
knowledgeof𝜆 𝑗,𝑞 𝑗,𝑗 ≠𝑖.Theoutputsof𝑓 are|𝐴|logits(onefor
phenomena,asdemonstratedunderthevariouslaboratorysettings
each𝑎∈𝐴),whicharepassedthroughasoftmaxfunction,giving
hereandobservedinmanyotherreal-worldsituations.Forexample,
learntpoliciesoftheform:
inmarketsettings,institutionalinvestorsmayhavehigheraccess
𝜋ˆ 𝑖(𝑎)=
𝑒𝑓𝑖(𝑎|𝑠𝑖)
(20) toinformationandmoreextensiveprocessingabilitiesthanretail
(cid:205) 𝑎′∈𝐴𝑒𝑓𝑖(𝑎|𝑠𝑖) investors,alteringtheresultingmarketdynamicsandpotentially
givingrisetobehaviourdeviatingfromthemutuallyconsistent
whereeachagentiscontinuallyattemptingtolearn𝑓 thatmax-
𝑖
imisestheirexpectedrewardfromusing𝜋ˆ (hereusingPPOwith equilibrium.Relaxingthisstrictnotionofequilibriumallowsmod-
𝑖
ellingamuchbroaderrangeofdynamics.
GeneralizedAdvantageEstimation[56]).Ofcourse,exceptinvery
Theproposedapproachdoesnotimposestrictassumptionsonra-
specificsettings[69],wedonothavegeneralconvergenceguaran-
tionality,mutualconsistency,orhomogeneitybutinsteadsimulates
tees,sowesaytheproposedapproachapproximatestheseequilibria.
theemergentoutcomesthroughlearningamongtheinteracting
Thebenefitoftheproposedapproachistheflexibilityof 𝑓 in
agents.Whiletheseassumptionsarenotimposed,theycanbere-
allowingforvariousbehavioursfromheterogenousagents(e.g.,
coveredasspecialcasesoftheproposedapproach,eliminatingthe
varying𝜆 and𝑞 )andcomputabilitywhenderivingtheequilibria
𝑖 𝑖
wouldotherwisebeintractable,suchaswhen𝜆 and𝑞 arenot
requirementofdeterminingwhichfeaturesarerelevantapriori.
𝑖 𝑖
Weevaluatedtheproposedapproachinvariouseconomicenviron-
commonknowledge.Whenallowingheterogeneous𝜆 and𝑞 ,we
𝑖 𝑖
ments,demonstratingimprovedout-of-samplepredictiveaccuracy
approximateaSubjectiveHeterogeneousQuantalResponseEqui-
comparedtoexistingstate-of-the-artMARLmethods(PPO)and
librium[53],atypeofBayesianequilibrium[24],whereagentsmay
analyticallyderivedequilibriumsolutions.Thisworkprovidesa
havedifferent(potentiallyincorrect)subjectivebeliefsaboutthe
valuabletoolformodellingcomplexsocialsystemsandcalibrating
typedistributionsoftheotheragents(inthiscase,thevaluesof𝜆
𝑖
thesemodelstoreal-worlddynamics,particularlywhenanalytical
and𝑞 inthepopulation).
𝑖
approachesbecomeintractable,settingthefoundationformore
RelationtoRationalInattention. AsmentionedinSection2,the advancedsimulations,e.g.limitorderbooks[42].
keyrelevantworkinthisareais[46].Whileweshareasimilargoal,
ourworkdiffersinsomeimportantways.[46]requiresestimating
themutualinformation(MI)forprocessingcostsusingaseparate
estimationengine.MIisdefinedoverthejointprobabilitiesas:
∑︁ 𝑝(𝑎,𝑠 𝑖)
𝑀𝐼 =− 𝑝(𝑎,𝑠 𝑖)log (21)
𝑝(𝑠 𝑖)𝑝(𝑎)
𝑎∈𝐴
whichhasadependenceontheunconditional𝑝(𝑎)whichmustbe
solvedwithapproximationtechniques[18].AsthedivergenceweDISCLAIMER [18] BenjaminPatrickEvansandMikhailProkopenko.2021.Amaximumentropy
modelofboundedrationaldecision-makingwithpriorbeliefsandmarketfeed-
ThispaperwaspreparedforinformationalpurposesbytheArtifi-
back.Entropy23,6(2021),669.
cialIntelligenceResearchgroupofJPMorganChase&Coandits [19] BenjaminPatrickEvansandMikhailProkopenko.2023.Boundedrationalityfor
affiliates(“J.P.Morgan”)andisnotaproductoftheResearchDe- relaxingbestresponseandmutualconsistency:thequantalhierarchymodelof
decisionmaking.TheoryandDecision(17May2023). https://doi.org/10.1007/
partmentofJ.P.Morgan.J.P.Morganmakesnorepresentationand
s11238-023-09941-z
warrantywhatsoeveranddisclaimsallliability,forthecomplete- [20] BenjaminEysenbachandSergeyLevine.2019.Ifmaxentrlistheanswer,what
ness,accuracyorreliabilityoftheinformationcontainedherein. isthequestion?arXivpreprintarXiv:1910.01913(2019).
[21] BenjaminEysenbachandSergeyLevine.2022. MaximumEntropyRL(Prov-
Thisdocumentisnotintendedasinvestmentresearchorinvest- ably)SolvesSomeRobustRLProblems.InInternationalConferenceonLearning
ment advice, or a recommendation, offer or solicitation for the Representations. https://openreview.net/forum?id=PtSAD3caaA2
[22] WilliamFedus,CarlesGelada,YoshuaBengio,MarcGBellemare,andHugo
purchaseorsaleofanysecurity,financialinstrument,financial
Larochelle.2019.Hyperbolicdiscountingandlearningovermultiplehorizons.
productorservice,ortobeusedinanywayforevaluatingthe arXivpreprintarXiv:1902.06865(2019).
meritsofparticipatinginanytransaction,andshallnotconstitute [23] L.E.FourakerandS.Siegel.1963.BargainingBehavior.McGraw-Hill.
[24] JohnGeanakoplos.1994.Commonknowledge.Handbookofgametheorywith
asolicitationunderanyjurisdictionortoanyperson,ifsuchsolici-
economicapplications2(1994),1437–1496.
tationundersuchjurisdictionortosuchpersonwouldbeunlawful. [25] JohnGeanakoplos,RobertAxtell,DoyneJFarmer,PeterHowitt,BenjaminConlee,
©2024JPMorganChase&Co.Allrightsreserved. JonathanGoldstein,MatthewHendrey,NathanMPalmer,andChun-YiYang.
2012.Gettingatsystemicriskviaanagent-basedmodelofthehousingmarket.
AmericanEconomicReview102,3(2012),53–58.
REFERENCES [26] GerdGigerenzer.2020.Whatisboundedrationality?InRoutledgehandbookof
boundedrationality.Routledge,55–69.
[1] LiAn,VolkerGrimm,YuBai,AbigailSullivan,BLTurnerII,NicolasMalleson,
[27] GerdGigerenzerandWolfgangGaissmaier.2011. Heuristicdecisionmaking.
AlisonHeppenstall,ChristianVincenot,DerekRobinson,XinyueYe,etal.2023.
Annualreviewofpsychology62(2011),451–482.
Modelingagentdecisionandbehaviorinthelightofdatascienceandartificial
[28] JacobKGoereeandCharlesAHolt.2004.Amodelofnoisyintrospection.Games
intelligence.EnvironmentalModelling&Software(2023),105713.
andEconomicBehavior46,2(2004),365–382.
[2] LiAn,VolkerGrimm,AbigailSullivan,BLTurnerIi,NicolasMalleson,Alison
[29] JacobKGoeree,CharlesAHolt,andThomasRPalfrey.2020.Stochasticgame
Heppenstall,ChristianVincenot,DerekRobinson,XinyueYe,JianguoLiu,etal.
theoryforsocialscience:Aprimeronquantalresponseequilibrium.Handbook
2021. Challenges,tasks,andopportunitiesinmodelingagent-basedcomplex
ofExperimentalGameTheory(2020),8–47.
systems.EcologicalModelling457(2021),109685.
[30] RussellGolman.2011.Quantalresponseequilibriawithheterogeneousagents.
[3] LeoArdon,JaredVann,DeepekaGarg,ThomasSpooner,andSumitraGanesh.
JournalofEconomicTheory146,5(2011),2013–2028.
2023.Phantom-ARL-drivenMulti-AgentFrameworktoModelComplexSystems.
[31] TuomasHaarnoja,AurickZhou,PieterAbbeel,andSergeyLevine.2018.Soft
InProceedingsofthe2023InternationalConferenceonAutonomousAgentsand
actor-critic:Off-policymaximumentropydeepreinforcementlearningwitha
MultiagentSystems.2742–2744.
stochasticactor.InInternationalconferenceonmachinelearning.PMLR,1861–
[4] JasminaArifovicandJohnDuffy.2018.Heterogeneousagentmodeling:exper-
1870.
imentalevidence. InHandbookofComputationalEconomics.Vol.4.Elsevier,
[32] Teck-HuaHo,So-EunPark,andXuanmingSu.2021.Abayesianlevel-kmodel
491–540.
inn-persongames.ManagementScience67,3(2021),1622–1638.
[5] RobertLAxtellandJDoyneFarmer.2022.Agent-basedmodelingineconomics
[33] NicolasHoertel,MartinBlachier,CarlosBlanco,MarkOlfson,MarcMassetti,
andfinance:Past,present,andfuture.JournalofEconomicLiterature(2022).
MarinaSánchezRico,FrédéricLimosin,andHenriLeleu.2020. Astochastic
[6] BowenBaker,IngmarKanitscheider,TodorMarkov,YiWu,GlennPowell,
agent-basedmodeloftheSARS-CoV-2epidemicinFrance.Naturemedicine26,9
Bob McGrew, and Igor Mordatch. 2020. Emergent Tool Use From Multi-
(2020),1417–1421.
AgentAutocurricula.InInternationalConferenceonLearningRepresentations.
[34] CarsHommesandThomasLux.2013. INDIVIDUALEXPECTATIONSAND
https://openreview.net/forum?id=SkxpxJBKwS
AGGREGATE BEHAVIOR IN LEARNING-TO-FORECAST EXPERIMENTS.
[7] Jean-PhilippeBouchaud.2008.Economicsneedsascientificrevolution.Nature
Macroeconomic Dynamics 17, 2 (2013), 373–401. https://doi.org/10.1017/
455,7217(2008),1181–1181.
S1365100511000162
[8] LuigiCampanaro,DanieleDeMartini,SiddhantGangapurwala,WolfgangMerkt,
[35] CarsHommes,JoepSonnemans,JanTuinstra,andHenkVanDeVelden.2007.
andIoannisHavoutis.2023.Roll-Drop:accountingforobservationnoisewith
Learningincobwebexperiments.MacroeconomicDynamics11,S1(2007),8–33.
asingleparameter.InLearningforDynamicsandControlConference.PMLR,
[36] AthulPaulJacob,DavidJWu,GabrieleFarina,AdamLerer,HengyuanHu,Anton
718–730.
Bakhtin,JacobAndreas,andNoamBrown.2022.Modelingstrongandhuman-
[9] AndrewCaplin,MarkDean,andDanielMartin.2011. Searchandsatisficing.
likegameplaywithKL-regularizedsearch.InInternationalConferenceonMachine
AmericanEconomicReview101,7(2011),2899–2922.
Learning.PMLR,9695–9728.
[10] TaoChen.2018.Round-numberbiasesandinformedtradinginglobalmarkets.
[37] ArnoldKling.2018. Agent-basedmodeling:Promisesandpitfalls. https:
JournalofBusinessResearch92(2018),105–117.
//www.econlib.org/archives/2011/02/agent-based_mod.html
[11] YefenChen,XuanmingSu,andXiaoboZhao.2012.ModelingBoundedRationality
[38] MaciejŁatek,RLAxtell,andBogumilKaminski.2009. Boundedrationality
inCapacityAllocationGameswiththeQuantalResponseEquilibrium.Manage-
viarecursion.InProceedingsofEighthInternationalConferenceonAutonomous
mentScience58,10(2012),1952–1962. https://doi.org/10.1287/mnsc.1120.1531
AgentsandMulti-AgentSystems(AAMAS2009).457–464.
arXiv:https://doi.org/10.1287/mnsc.1120.1531
[39] RobertoLeombruniandMatteoRichiardi.2005.Whyareeconomistssceptical
[12] Juin-KuanChong,Teck-HuaHo,andColinCamerer.2016.Ageneralizedcogni-
aboutagent-basedsimulations?PhysicaA:StatisticalMechanicsanditsApplica-
tivehierarchymodelofgames.GamesandEconomicBehavior99(2016),257–274.
tions355,1(2005),103–109.
[13] BenjaminAConverseandPatrickJDennis.2018. Theroleof“Prominent
[40] StevenDLevittandJohnAList.2008.Homoeconomicusevolves.Science319,
Numbers”inopennumericaljudgment:Straineddecisionmakerschoosefroma
5865(2008),909–910.
limitedsetofaccessiblenumbers.OrganizationalBehaviorandHumanDecision
[41] EricLiang,RichardLiaw,RobertNishihara,PhilippMoritz,RoyFox,KenGold-
Processes147(2018),94–107.
berg,JosephGonzalez,MichaelJordan,andIonStoica.2018.RLlib:Abstractions
[14] MoloodAleEbrahimDehkordi,JonasLechner,AminehGhorbani,IgorNikolic,
fordistributedreinforcementlearning.InInternationalconferenceonmachine
EmileChappin,andPaulienHerder.2023.UsingMachineLearningforAgent
learning.PMLR,3053–3062.
SpecificationsinAgent-BasedModelsandSimulations:ACriticalReviewand
[42] PenghangLiu,KshamaDwarakanath,andSvitlanaSVyetrenko.2022.Biased
Guidelines.JournalofArtificialSocietiesandSocialSimulation26,1(2023).
orlimited:Modelingsub-rationalhumaninvestorsinfinancialmarkets.arXiv
[15] JanezDemšar.2006.Statisticalcomparisonsofclassifiersovermultipledatasets.
preprintarXiv:2210.08569(2022).
TheJournalofMachinelearningresearch7(2006),1–30.
[43] RichardDMcKelveyandThomasRPalfrey.1995.Quantalresponseequilibria
[16] Gregd’Eon,SophieGreenwood,KevinLeyton-Brown,andJamesWright.2023.
fornormalformgames.Gamesandeconomicbehavior10,1(1995),6–38.
LossFunctionsforBehavioralGameTheory. arXivpreprintarXiv:2306.04778 [44] RichardDMcKelveyandThomasRPalfrey.1998.Quantalresponseequilibria
(2023).
forextensiveformgames.Experimentaleconomics1(1998),9–41.
[17] BenjaminEnke,UriGneezy,BrianHall,DavidMartin,VadimNelidov,Theo
[45] RichardDMcKelvey,ThomasRPalfrey,andRobertoAWeber.2000.Theeffects
Offerman,andJeroenVanDeVen.2023.Cognitivebiases:Mistakesormissing
ofpayoffmagnitudeandheterogeneityonbehaviorin2×2gameswithunique
stakes?TheReviewofEconomicsandStatistics105,4(2023),818–832.
mixedstrategyequilibria. JournalofEconomicBehavior&Organization42,4
(2000),523–548.[46] TongMu,StephanZheng,andAlexanderRTrott.2022. ModelingBounded [59] ChristopherASims.2003. Implicationsofrationalinattention. Journalof
RationalityinMulti-AgentSimulationsUsingRationallyInattentiveReinforce- monetaryEconomics50,3(2003),665–690.
mentLearning. TransactionsonMachineLearningResearch(2022). https: [60] SamuelSokota,RyanD’Orazio,JZicoKolter,NicolasLoizou,MarcLanctot,
//openreview.net/forum?id=DY1pMrmDkm IoannisMitliagkas,NoamBrown,andChristianKroer.2023.Aunifiedapproach
[47] PedroAOrtegaandDanielABraun.2013. Thermodynamicsasatheoryof toreinforcementlearning,quantalresponseequilibria,andtwo-playerzero-sum
decision-makingwithinformation-processingcosts. ProceedingsoftheRoyal games.ICLR(2023).
SocietyA:Mathematical,PhysicalandEngineeringSciences469,2153(2013), [61] YunhaoTangandShipraAgrawal.2020.Discretizingcontinuousactionspace
20120683. foron-policyoptimization.InProceedingsoftheAAAIconferenceonArtificial
[48] OsondeAOsoba,RaffaeleVardavas,JustinGrana,RushilZutshi,andAmber Intelligence,Vol.34.5981–5988.
Jaycocks.2020.Modelingagentbehaviorsforpolicyanalysisviareinforcement [62] CallumRhysTilbury.2023.ReinforcementLearningforEconomicPolicy:ANew
learning.In202019thIEEEInternationalConferenceonMachineLearningand Frontier? TechnicalReport.
Applications(ICMLA).IEEE,213–219. [63] YakupTurgutandCaferErhanBozdag.2023.Aframeworkproposalformachine
[49] BenjaminPatrickEvansandMikhailProkopenko.2023. Boundedstrategic learning-drivenagent-basedmodelsthroughacasestudyanalysis.Simulation
reasoningexplainscrisisemergenceinmulti-agentmarketgames.RoyalSociety ModellingPracticeandTheory123(2023),102707.
OpenScience10,2(2023),221164. [64] ArthurTurrell.2016.Agent-basedmodels:understandingtheeconomyfromthe
[50] JulienPerolat,BartDeVylder,DanielHennes,EugeneTarassov,FlorianStrub, bottomup.BankofEnglandQuarterlyBulletin(2016),Q4.
VincentdeBoer,PaulMuller,JeromeTConnor,NeilBurch,ThomasAnthony, [65] NelsonVadori,LeoArdon,SumitraGanesh,ThomasSpooner,SelimAmrouni,
etal.2022. MasteringthegameofStrategowithmodel-freemultiagentrein- JaredVann,MengdaXu,ZeyuZheng,TuckerBalch,andManuelaVeloso.[n.d.].
forcementlearning.Science378,6623(2022),990–996. Towardsmulti-agentreinforcementlearning-drivenover-the-countermarket
[51] AshreetaPrasanna,SaschaHolzhauer,andFriedrichKrebs.2019.Overviewof simulations.MathematicalFinancen/a,n/a([n.d.]). https://doi.org/10.1111/mafi.
machinelearninganddata-drivenmethodsinagent-basedmodelingofenergy 12416arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/mafi.12416
markets.INFORMATIK2019:50JahreGesellschaftfürInformatik–Informatikfür [66] NelsonVadori,SumitraGanesh,PrashantReddy,andManuelaVeloso.2020.
Gesellschaft(2019). CalibrationofsharedequilibriaingeneralsumpartiallyobservableMarkov
[52] JeevantRampalandFernandoStragliotto.2023.HeterogeneousAgentQuantal games. AdvancesinNeuralInformationProcessingSystems33(2020),14118–
ResponseEquilibrium.(2023). 14128.
[53] BrianWRogers,ThomasRPalfrey,andColinFCamerer.2009.Heterogeneous [67] SvitlanaVyetrenko,DavidByrd,NickPetosa,MahmoudMahfouz,DanialDer-
quantalresponseequilibriumandcognitivehierarchies. JournalofEconomic vovic,ManuelaVeloso,andTuckerBalch.2020. Getreal:Realismmetricsfor
Theory144,4(2009),1440–1467. robustlimitorderbookmarketsimulations.InProceedingsoftheFirstACM
[54] ArielRubinstein.2003.“Economicsandpsychology”?Thecaseofhyperbolic InternationalConferenceonAIinFinance.1–8.
discounting.InternationalEconomicReview44,4(2003),1207–1216. [68] YingWen,YaodongYang,andJunWang.2021.ModellingBoundedRationality
[55] EllisScharfenaker.2020.Implicationsofquantalresponsestatisticalequilibrium. inMulti-AgentInteractionsbyGeneralizedRecursiveReasoning.InProceed-
JournalofEconomicDynamicsandControl119(2020),103990. ingsoftheTwenty-NinthInternationalJointConferenceonArtificialIntelligence
[56] JohnSchulman,PhilippMoritz,SergeyLevine,MichaelI.Jordan,andPieter (Yokohama,Yokohama,Japan)(IJCAI’20).Article58,8pages.
Abbeel.2016. High-DimensionalContinuousControlUsingGeneralizedAd- [69] KaiqingZhang,ZhuoranYang,andTamerBasar.2019.Policyoptimizationprov-
vantageEstimation.In4thInternationalConferenceonLearningRepresentations, ablyconvergestoNashequilibriainzero-sumlinearquadraticgames.Advances
ICLR2016,SanJuan,PuertoRico,May2-4,2016,ConferenceTrackProceedings, inNeuralInformationProcessingSystems32(2019).
YoshuaBengioandYannLeCun(Eds.). http://arxiv.org/abs/1506.02438 [70] WeiZhang,AndreaValencia,andNi-BinChang.2021.Synergisticintegration
[57] JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. betweenmachinelearningandagent-basedmodeling:Amultidisciplinaryreview.
2017.Proximalpolicyoptimizationalgorithms.arXivpreprintarXiv:1707.06347 IEEETransactionsonNeuralNetworksandLearningSystems(2021).
(2017). [71] StephanZheng,AlexanderTrott,SunilSrinivasa,DavidCParkes,andRichard
[58] HerbertASimon.1979.Rationaldecisionmakinginbusinessorganizations.The Socher.2022. TheAIEconomist:Taxationpolicydesignviatwo-leveldeep
Americaneconomicreview69,4(1979),493–513. multiagentreinforcementlearning.Scienceadvances8,18(2022),eabk2607.Worst Worst A TRAINING
EachenvironmentisconfiguredinPhantom[3],withaRLLibback-
end[41].Agentsarestrategicagents,learningviaPPO[57],witha
neuralnetworkwith2hiddenlayers,of64nodesineachlayer,and
discreteordinaldiscreteactionspaces[61].Allotherparameters
keeptheirdefaultvaluesfromRLLib.Toensureequitablecompari-
0.250.5 1.0 2.5 5.010.0 Best 0.250.5 1.0 2.5 5.010.0 Best
µ µ son,theproposedapproachandthestandardMARLalgorithmuse
thesamehyperparameters,observationspaces,andactionspaces,
(a)SupplyChain (b)Duopoly
andthetrainingprocessisexecutedforanidenticalnumberof
Worst Worst iterations(500)acrossbothapproaches,ensuringampletimefor
convergence,asdemonstratedinFig.A.6.
1.0
0.250.5 1.0 2.5 5.010.0 Best 0.250.5 1.0 2.5 5.010.0 Best
µ µ 0.8
(c)Triopoly (d)Cobweb
0.6
FigureA.7:CalibrationResultsforvaluesof𝜇and𝜎∗
0.4
Worst Worst
0.2 Supply Chain
Oligopoly
Cobweb
0.0
0 100 200 300 400 500
Training Iterations
Best Best
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
σ∗ σ∗
FigureA.6:Trainingconvergence
(a)SupplyChain (b)Duopoly
Worst Worst
A.1 Calibration
Fortheproposedapproach,duetotheprincipleofinsufficientrea-
son,weassumeuniformpriorbeliefsamongtheagents.Although
Best 0.00 0.25 0.50 0.75 1.00 Best 0.00 0.25 0.50 0.75 1.00 weprovidediscussionwithvaryingpriorstoshowtheflexibility
σ∗ σ∗
oftheproposedapproach(e.g.Fig.4c),wedonotusethesefor
(c)Triopoly (d)Cobweb comparisonduetopotentialleakingeffectsfromsettingpriorsafter
observingdata.Wedonotcalibrateoralterthepriors,butshow
FigureA.8:CalibrationResultsforvaluesof𝜎∗(averaging
thepossibilityandbenefitofdoingso.
acrossvaluesof𝜇).Thelowertherank,thebetter.
Wecalibrate𝜇,𝜎from
𝜇 ∈{0,0.25,0.5,1,2.5,5,10}
Worst Worst
𝜎∗ ∈{0,0.05,0.1,0.25,0.5,1}
where𝜎 =𝜇×𝜎∗.Werestrict𝜎∗ ≤1aswearedealingwithnormal
distributionsanddonotwantnegativeprocessingpenalties(𝜆 <0
𝑖
Best Best
0.0 2.5 5 µ.0 7.5 10.0 0.0 2.5 5 µ.0 7.5 10.0 isclippedat𝜆
𝑖
=0).Thecalibrationistheresultofthelowestmean
squarederroronthetrainingfold.Thetestingfoldsareneverused
(a)SupplyChain (b)Duopoly forcalibration.
Worst Worst
A.1.1 CalibrationResults. Wevisualisetheresultsofthecalibra-
tioninFig.A.7.Toanalysetheimpactofeachparameterindividu-
ally,wepresenttheresultsforafixedvaluewhileaveragingacross
theotherparameterrangeinFigs.A.8andA.9.
Best Best
0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0
µ µ
(c)Triopoly (d)Cobweb
Figure A.9: Calibration Results for values of 𝜇 (averaging
acrossvaluesof𝜎∗).Thelowertherank,thebetter.
∗σ
∗σ
knaR
naeM
knaR
naeM
knaR
naeM
knaR
naeM
0.050.01.052.05.0
0.1
0.050.01.052.05.0
0.1
∗σ
∗σ
knaR
naeM
knaR
naeM
knaR
naeM
knaR
naeM
0.050.01.052.05.0
0.1
0.050.01.052.05.0
0.1
drawer
desilamroN