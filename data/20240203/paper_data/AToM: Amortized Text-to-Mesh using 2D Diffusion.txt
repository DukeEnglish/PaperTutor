AToM: Amortized Text-to-Mesh using 2D Diffusion
GuochengQian1,2 JunliCao1 AliaksandrSiarohin1 YashKant1,3 ChaoyangWang1
MichaelVasilkovsky1 Hsin-YingLee1 YuweiFang1 IvanSkorokhodov1 PeiyeZhuang1
IgorGilitschenski3 JianRen1 BernardGhanem2 KfirAberman1 SergeyTulyakov1
1SnapResearch 2KAUST 3UniversityofToronto
“Two raccoons playing poker” “A lionfish” “A skiing penguin wearing a puffy jacket”
“A broken eggshell with a chick standing next” “A sourdough loaf on a cutting board” “A hippo wearing a sweater”
Figure 1: Our Amortized Text-to-Mesh model (AToM), trained only on 2D diffusion prior, can generate textured meshes
fromtextsinlessthan1second. Seehttps://snap-research.github.io/AToMforimmersivevisualization.
Abstract hasbecomeincreasinglyintuitive,withcontrolstransition-
ing from complex, handcrafted graphics handles [50] to
We introduce Amortized Text-to-Mesh (AToM), a feed- simple textual inputs [14]. Current mainstream text-to-
forward text-to-mesh framework optimized across multiple mesh models [8,25,47] can generate impressive textured
text prompts simultaneously. In contrast to existing text- meshesthroughscoredistillationsampling[38]without3D
to-3Dmethodsthatoftenentailtime-consumingper-prompt supervision. Despite the growing interest, these methods
optimization and commonly output representations other require a per-prompt optimization that trains a standalone
than polygonal meshes, AToM directly generates high- model for each prompt, which is time and computational
quality textured meshes in less than 1 second with around consuming. Moreimportantly,per-promptsolutionscannot
10× reduction in the training cost, and generalizes to un- generalizetounseenprompts.
seen prompts. Our key idea is a novel triplane-based text-
Recently, ATT3D [29] presents amortized text-to-3D,
to-mesh architecture with a two-stage amortized optimiza-
which optimizes a text-to-3D system in many prompts si-
tionstrategythatensuresstabletrainingandenablesscal-
multaneously unlike per-prompt solutions. This amortized
ability. Through extensive experiments on various prompt
optimizationnotonlysignificantlyreducestrainingtimebut
benchmarks, AToM significantly outperforms state-of-the-
alsoallowsgeneralizabilityduetothefeaturesharingacross
art amortized approaches with over 4× higher accuracy
prompts. Unfortunately, ATT3D is limited predominantly
(inDF415dataset)andproducesmoredistinguishableand
to outputting 3D objects represented by Neural Radiance
higher-quality3Doutputs. AToMdemonstratesstronggen-
Fields (NeRF) [31]. An amortized text-to-mesh system is
eralizability, offering finegrained 3D assets for unseen in-
of more practical importance, but is under explored. First,
terpolated prompts without further optimization during in-
mesh is more widely used in most developments such as
ference,unlikeper-promptsolutions.
gaminganddesign. However,convertingNeRFstomeshes
isinaccurateandmightrequirefurtheroptimizationthatis
costly[28]. Second,trainingtext-to-meshdirectlyfacilities
1.Introduction
a more efficient rasterizer, allowing higher-resolution ren-
dersthathelprecoverdetailsingeometryandtexturecom-
Polygonalmeshesconstituteawidelyusedandefficient
paredtotext-to-NeRF[8,25].
representation of 3D shapes. As we enter a revolutionary
phaseofGenerativeAI[41,46],thecreationof3Dmeshes Extending ATT3D to amortized mesh generation
1
4202
beF
1
]VC.sc[
1v76800.2042:viXra“A zoomed out DSLR photo of a robot couple fine dining” dergo refinement, and an additional deformation network
is learned to manipulate the mesh vertices for finegrained
details. Utilizing efficient mesh rasterization allows for
512 × 512 resolution renders in this phase. After train-
ing,AToMenablesultra-fastinference,generatingtextured
Per-prompt text-to-mesh Naive amortized text-to-mesh AToM (Ours) meshes in under one second. The main contributions of
Shape Quality Efficiency Shape Quality Efficiency Shape Quality Efficiency
thisworkcanbesummarizedasfollows:
Figure 2. Per-prompt text-to-mesh [47] generates high-quality • We propose AToM, the first amortized text-to-mesh
results but demands expensive optimization. Naively extending model that is optimized across multiple text prompts
ATT3Dformeshgenerationleadstodivergenttrainingandpoor without 3D supervision. AToM trains a triplane-based
geometry. AToM introduces a triplane-based architecture with
meshgenerator,whichcontributestostableoptimization
two-stage amortized optimization for enhanced stability. AToM
andgeneralizabilitytolarge-scaledatasets.
efficiently generates textured meshes for various text prompts in
• Weintroduceatwo-stageamortizedoptimization,where
underonesecondduringinference.
the first stage uses low-resolution volumetric rendering,
and the second stage utilizes high-resolution mesh ras-
terization.Ourtwo-stageamortizedtrainingsignificantly
presents challenges in unstable training that causes poor
improvesthequalityofthetexturedmesh.
geometry. Our observations highlight two primary factors
• AToM generates high-quality textured meshes in less
contributing to this instability of ATT3D for mesh gen-
than1secondfromatextpromptandgeneralizestoun-
eration: the architecture and the optimization. First,
seenpromptswithnofurtheroptimization.
ATT3D adopted a HyperNetwork-based [18] Instant-NGP
[34]positional encodingfortext-conditionedNeRF gener-
2.RelatedWork
ation. This HyperNetwork introduces numerical instabil-
ity and demands special layers such as spectral normal- Feed-Forward 3D Generation. The evolution of feed-
ization [33] to alleviate. The instability is more severe in forward 3D generation models has followed the success
large-scaledatasets,leadingtoindistinguishable3Dcontent of 2D generation, drawing inspiration from generative
for different prompts, limiting generalizability of the sys- adversarial networks [5, 15, 22] and autoregressive net-
tem. Seethetwosimilarrobotsgeneratedfromtwodistinct works [32,56] to diffusion models [7,10,17,21,35,57].
promptsin4th row1st and3rd columninFig. 6. Second, Various 3D representations have been studied, including
the end-to-end optimization for text-to-mesh also triggers point clouds [23,44,54], volumes [11,26,45,55], and
instability due to the topological limitations of differential meshes [9,37,50,58]. Despite their success, these meth-
mesh representation [42], leading to suboptimal geometry. odsareboundedbytheavailabilityofhigh-quality3Ddata
Overall, naively extending ATT3D to mesh generation re- andthusmostpreviousworksmerelyappliedtocertaincat-
sultsindivergentoptimizationandtheinabilitytogenerate egories,suchascarsandhumanfaces[4,14]. Theconcur-
any3Dobjectaftertraining. Refertothesecondcolumn1 rentworkInstant3D[24]showsthepotentialtotrainagen-
inFig. 2forillustration. eralizable3Dgenerativemodelintherecentlarge-scale3D
We thus introduce AToM, the first amortized approach dataset[13]. Wenotethattrainingin3Ddatasetorthrough
for direct Text-to-Mesh generation. To address architec- score distillation sampling (SDS) [38] are two orthogonal
tureinstability,AToMintroducesatext-to-triplanenetwork directions. The latter does not require any 3D data, which
in replacement of HyperNetwork for the positional encod- alignswithourinterest.Weelaboratethetext-to-3DbySDS
ing. Our text-to-triplane demonstrates greater resilience to next.
parameter changes and generally yields higher-quality and Per-prompt 3D Optimization. Recent studies have sug-
significantlymoredistinguishable3Dcontentcomparedto gested leveraging pretrained text-to-image models for 3D
theATT3D’sHyperNetworkcounterpart. Wethenpropose generation without 3D data supervision [6, 20, 30, 38,
to use triplane features as input to subsequent signed dis- 39]. Subsequent works introduced enhancements in direc-
tance function (SDF), deformation, and color networks to tions such as multiview image gradient aggregation [49],
generategeometryandtexturefordifferentiablemesh[42]. two-stage training optimization [25], representation tech-
Moreover, to stabilize optimization, we propose a two- niques[8],increaseddiversity[53],andoptimizationtech-
stageamortizedtrainingincontrasttonaiveend-to-endop- niques [59]. Being able to generate high-quality 3D con-
timization. Our first stage trains text-to-triplane, SDF, and tent, per-promptoptimizationisreceivingincreasinginter-
color networks through low-resolution (64×64) volumet- est. However,thesemethodsaretime-intensive,aseachas-
ricrendering. Volumetricrendering’sconsiderationofmul- set needs a separate optimization process and usually re-
tiple points per ray contributes to a stable optimization of quires tedious parameter tuning. Per-prompt optimization
theSDFnetwork. Inoursecondstage, thesenetworksun- also overfits to the single training prompt. Instead, we are
2InferenceStage: Feedfoward Text-to-Mesh Generation
Text Prompts (xyz,p ,f )
xyz xyz
SDF Module
“a DSLR photo of a
d““raaa D gDoS nSL LsR tRa p tpuhheoo”toto o off a a Text-to-
ppeeaaccoocckk o onn a ”” Triplane Deformation Module
Text Encoder Differentiable Mesh
Texture Module
Training Stage 1: SDF Module Training Stage 2:
NeuSVolumetric Optimization Texture Module Mesh Optimization
SDS Loss
Text-to-Image Add Noise High-Resolution
Diffusion Model Mesh Rasterization
Low-Resolution
Volumetric Rendering Add Noise
Figure3. InferenceandtrainingofAToM.AToMinference(up): AToMgeneratestexturedmeshesfromgivenpromptsinlessthana
secondininference.Thetext-to-meshgeneratorproposedinAToMconsistsofthreecomponents:a)atextencoderthattokenizestheinput
prompt,b)atext-to-triplanenetworkthatoutputsatriplanerepresentationfromthetextembedding,andc)a3Dnetworkthatgenerates
SDF,vertexdeformation,andcolortoformadifferentialmeshfrompositionsandtriplanefeatures. AToMTraining(bottom): AToM
utilizesatwo-stageamortizedoptimization,wherethefirststageleveragesstablevolumetricoptimizationtotrainonlytheSDFandtexture
modulesusinglow-resolutionrenders. Thesecondsstageusesmeshrasterizationtooptimizethewholenetworkthroughhigh-resolution
renders. Inbothstages,AToMistrainedsimultaneouslyonmanypromptsthroughtheguidanceofatext-to-imagediffusionpriorwithout
any3Ddatasupervision.
interestedingeneralizableandefficienttext-to-3D. (1) a text encoder, (2) a text-to-triplane network, and (3) a
AmortizedOptimization. Unlikethetime-consumingper- triplane-to-meshgenerator.
promptoptimization,ATT3D[29]proposedtoamortize[1] Text encoder embeds the input text prompt. For simplic-
the optimization across multiple prompts. This enables ity,weusethesamefrozenpretrainedT5XXL[40]asthe
more efficient synthesis of 3D objects in seconds, facili- text-to-imagediffusionmodelDeepFloydIF[12]. Thetext
tating interpolation between prompts and generalizatNioeRnFto embeddinge∈RLe×Ce obtainedisusedasinputtothefol-
unseen prompts. However, ATT3D is limited to small- lowingnetworkstogeneratethedesired3Dcontent,where
scaledatasets, generating3Dcontentindistinguishablebe- L andC representthenumberoftokens(e.g.77)andthe
e e
tweenpromptsinlarger-scalebenchmark,e.g.DF415(415 dimensionoftheirembedding(e.g.4096),respectively.
prompts from DreamFusion [38]). Additionally, ATT3D Text-to-triplane network T outputs a triplane representa-
solelyproducesNeRFthatlimitsthequality. Arecentcon- tionfromthetextembeddinge.T iscomposedoftwoparts.
currentworkHyperFields[3]attemptedtoimproveATT3D The first part is a linear projection that maps the averaged
withastrongerdynamichypernetwork.Inthiswork,weare text embedding from RCe to R3CTHTWT, which is then
more interested in amortized text-to-mesh, that generates reshaped to a triplane [4] representation R3×CT×HT×WT.
texturedmeshesinunderonesecondandcanbeappliedto NotethatC isthenumberoftriplanefeaturesandH ×
T T
large-scalepromptdatasets. W denotes the height and width of each plane. The sec-
T
ondpartofT isatext-conditioned,3D-awareTriplaneCon-
3.Method vNeXt network to enhance the triplane features. We con-
structthenetworkbystackingN ConvNeXtblocks,where
3.1.AToMPipeline
eachblockconsistsoffourmodules.Thefirstisamultihead
Fig. 3demonstratesthepipelineoftheproposedAmor- cross-attention module [48] between text embedding and
tizedText-to-Mesh(AToM).Unlikemainstreamper-prompt triplanefeatures. Eachpixelofaplaneisusedasonequery
solutions [8,25,38,47] that train a standalone 3D model token, while each text token serves as the key and value.
foraspecificprompt,AToMtrainsatext-conditionedmesh This cross-attention module is beneficial for higher gener-
generator, which can efficiently produce textured meshes alizabilityandbetterqualityespeciallywhenthedatasetis
from various text prompts in inference stage. The net- large-scale(see§5).Thesecondmoduleisa3D-awarecon-
work architecture of AToM consists of three components: volutionborrowedfrom[52].Duetothefactthateachpixel
3
Token
Embedding
e(i,j)inaplanecanbeassociatedwiththewholecolumnor theratherpoorgeometrywithoutper-prompttuninginAp-
row in the other two planes, this 3D-aware convolution is pendix. GET3D[14]alsoshowsthepossibilityoftraining
proposed to concatenate the features of (i,j) with the av- an unconditional mesh generator, but is limited to specific
eraged features of (i,:) and (:,j), and then perform a 2D categories such as chair and car, and requires the ground-
convolution. The third is a depth-wise convolution with truth3Ddataduringtraining. Weshowthatatrivialend-to-
kernelsize7×7toaggregatespatialinformationperchan- endtrainingforAToMleadstodivergentoptimization(§5).
nel,asinspiredfromConvNeXt[27]. Thelastisafeedfor- To address this unstable optimization, we propose a two-
wardnetworkthatismadeupoftwoinvertedlinearlayers stage amortized optimization: a NeuS volumetric training
tomixchannelinformation. Weperformconvolutionsand aswarmupfollowedbyadifferentiablemeshtraining.
linearlayersperplane,sinceweempiricallyfindotherwise Firststage: volumetricoptimization. Weusevolumetric
theinformationofnon-correlatedregionswillbemixedthat renderinginthefirststagetowarmuptheSDFnetwork. In
mightslightlydegradetheperformance.The3D-awarecon- this stage, we use the NeuS [51] representation, and only
volutionandthefeedforwardnetworkcanbeefficientlyim- optimize the triplane generator, the SDF network, and the
plemented by group convolutions using PyTorch. A resid- color network. We render a low-resolution (e.g. 64×64)
ual connection is added to each module to avoid gradient image by accumulating points’ colors along the ray. The
vanishing. SeeAppendix fortheillustrationofourtext-to- obtainedrenderingsareaddednoiseandpassedtothetext-
triplaneachitectureandtheTriplaneConvNeXtblock. to-image diffusion prior to provide the guidance through
Triplane-to-Mesh generator ν generates a differential SDS loss [38]. Mathematically, given a pixel in the ren-
meshfromthetriplanefeatures. WeuseDMTet[42]asthe dered image, the ray emitted from this pixel is denoted as
meshrepresentation. DMTetrepresentsadifferentialtetra- {p =o+t v|i= 1,...,n,t <t }, where o is the
i i i i+1
hedral geometry as a signed distance field (SDF) defined center of the camera, v is the unit direction vector of the
onadeformabletetrahedralgrid. ThemeshverticesV and ray,nisthenumberofpointsperray,tdenotesthedistance
theirconnections(meshfaces)arepredefinedontheinitial fromo. NeuSvolumetricrenderingisgivenby:
grid. A deformation network is optimized to offset these
predefinedverticesforrefinedtrianglemeshesandfinerde-
Cˆ
=(cid:88)n
T α c , T
=i (cid:89)−1
(1−α )
tails. TheSDFvaluesoftheshiftedpointsarelearnedbyan i i i i j
i=1 j=1 (1)
SDF network to represent the final distance to the surface.
(cid:18) (cid:19)
Φ (f(p(t )))−Φ (f(p(t )))
The zero-level set of SDF values represents the surface of α =max s i s i+1 ,0
i Φ (f(p(t )))
thetrianglemeshe. Moreover,acolornetworkisleveraged s i
tolearnthecolorofeachvertex. Overall,weemploythree
where ϕ (x) =
se−sx/(1+e−sx)2
is the logistic density
separate networks, i.e. the SDF network, the deformation s
distribution.s,f,carethelearnableparameterofNeuS,the
network, and the color network, to optimize DMTet. The
SDFnetwork,andthecolorforpointi,respectively.
input of each network is the concatenation of the triplane
Second stage: mesh optimization. For the second
featuresandthepositionswiththeirsinusoidalencodingof
stage, we use differentiable mesh representation. Fast and
thepredefinedmeshvertices.
memory-efficientmeshrasterizationisleveraged, allowing
The inference stage of AToM is a feedforward progress
the system be trained with high-resolution renderings (e.g.
that gets a textured mesh directly from the text input, and
512×512). The same SDS loss as the first stage is used
is finished in less than one second without the need of op-
as guidance. The deformation network initialized with ze-
timization. During inference, once a text is given, AToM
ros is included in the optimization, i.e. the vertices of the
firstgetsthetextembeddingethroughthetextencoderand
initial mesh grid without offsets are used as query points.
next passes e to the triplane generator to obtain the fea-
Since SDF and color networks are warmed up during the
tures. AToMtheninputstheverticesoftheDTMetgridto
first stage, the main goal of the second stage is improve
query the triplane features, encodes the vertices positions,
the quality of the geometry and texture through the high-
andpassestheconcatenationoftriplanefeatures,positional
resolutionrendersinmeshrepresentation.
encoding, and positions to the SDF, the deformation, and
thecolornetworkstooutputthetexturedmesh.
4.Experiment
3.2.Two-StageAmortizedOptimization
We conduct comprehensive experiments on various
Optimizingatext-to-meshend-to-endisproblematicdue benchmarkstoshowtheeffectivenessofAToM.Weshow-
tothetopologyconstraintse.g.triangleconnections,inher- case our strong generalizability to unseen prompts, while
ent in the differentiable mesh. Fantasia3D [8] makes such the per-prompt solutions [8,25,38,47] cannot. We also
a direct training of text-to-mesh possible, but requires so- demonstratequantitativelyandqualitativelythatAToMout-
phisticated parameter tuning for each prompt. Refer to performsATT3D,thestate-of-the-artamortizedtext-to-3D.
4(a)AToM (b)AToMPer-Prompt
Figure4. ComparingAToMtoAToMPer-PromptonthePig64compositionalpromptset(“apigactivitytheme”),whereeachrowand
columnrepresentadifferentactivityandtheme,respectively.Themodelsaretrainedusing56promptsandtestedonall64prompts,while
8unseenpromptsareevaluatedonthediagonal.Asdepictedin(a),AToMconsistentlygeneratespigswithasimilaridentityandauniform
orientation,indicatingthatAToMalsopromotesfeaturesharingacrossprompts,similartoATT3D[29].Also,AToMgenerates3Dcontent
withconsistentquality, whileper-promptoptimizationcannotasshownin(b). Additionally, per-promptoptimizationismoreproneto
overlookingcertaindetails,suchasthetophatinrow2column4andtheshovelinrow4column2in(b),whileAToMpreservesthem.
Moreimportantly,AToMperformswellonunseenpromptswithoutfurtheroptimization,unliketheper-promptsolution.
Table 1. Compare AToM to the state-of-the-art. CLIP R- themes, and 5 hats. We experiment on the hardest spit
probability↑isreported.Theper-promptmethodsinseenprompts (12.5% split), where only 300 prompts are used in train-
are deemphasized. Per-prompt solutions have not been experi-
ing,andall2400promptsaretestedincluding2100unseen
mentedinAnimal2400andDF415duetotheirprohibitivecompu-
ones. SeeAppendixfordatasetdetails. Wealsoevaluateon
tation.ATT3D’sresultsarefromtheoriginalpaper[29].ATT3D-
twodatasetsdemonstratedinDreamFusion[38]: DF27,the
IF†denotesourreproducedversionusingDeepFloyd[12]asprior.
27promptsshownintheirmainpaperandDF415,the415
Method/Dataset Pig64unseen Pig64seen Pig64all Animal2400 DF27 DF415 promptsavailableontheirprojectwebpage.
DreamFusion-IF 0 0.7143 0.6250 - 0.8889 -
EvaluationMetrics. Weemploythesameevaluationmet-
TextMesh-IF 0 0.8036 0.7031 - 0.9259 -
Fantasia3D 0 0.5357 0.4688 - 0.7037 - ric, the CLIP R-probability, as in ATT3D. The CLIP R-
Magic3D-IF 0 0.8036 0.7031 - 0.8519 -
probability gauges the average distance of the associated
ATT3D 0.3750 0.6071 0.5781 0.11 0.6296 -
ATT3D-IF† 0.6250 0.6429 0.6406 0.1671 0.8519 0.1880 text with 4 uniformly rendered views from the generated
AToM(Ours) 0.7500 0.7500 0.7500 0.3442 0.9259 0.8193
3Dmodel. Thisdistancescoreindicatesthelevelofconfi-
dence CLIP holds in regards to the relevance between the
We show the capability of AToM in the large-scale bench- textpromptandthemutiplerenderingsfromeach3Dasset.
mark while per-prompt solutions are prohibitive to train, ImplementationDetails. WeimplementourAToM,reim-
andATT3Dproducesindistinguishableresults. plementATT3D,andrunper-promptbaselines[8,25,38,47]
Data. We evaluate on two compositional datasets from usingtheThreeStudio[16]library. Forallmethodsexcept
ATT3D [29]: Pig64 and Animal2400. Pig64 is structured Fantasia3D [8] that requires using Latent Diffusion Model
according to the pattern “a pig {activity} {theme}” from [41], weutlize DeepFloyd [12] asthe text-to-image prior,
8 activities and 8 themes. In total, there are 64 prompts as it is found to offer higher quality across methods [16].
in Pig64, where 56 are used as training and all prompts Fortextembedding,weusethesamefrozenT5textencoder
including 8 unseen ones are used as testing. Animal2400 forpromptprocessingandtext-to-triplaneinput.Duringthe
isconstructedfollowingthetemplate“{animal}{activity} first stage, we render 64 × 64 resolution images with 64
{theme} {hat}”. There are 10 animals, 8 activities, 6 uniformly sampled points per ray. One can use 32 points
5withoutsignificant difference. Weoptimizenetworks with ever, visually, AToM exhibits more consistent results as
learningrate4e-4andbatchsize16using2GPUsfor20K shown in Fig. 4. (3) Across all benchmarks shown in
iterationsonDF27,4GPUsfor10KiterationsonPig64,8 Tab. 1, AToM showcases superior performance compared
GPUsfor100KiterationsonDF415. Forthesecondstage, toATT3D,highlightingAToM’seffectivenessoverATT3D.
weoptimizewithlearningrate2e-4andbatchsize16using Specifically, in DF415, AToM attains 81.93% accuracy,
4GPUsfor10KiterationsonDF27andPig64,and8GPUs muchhigherthanATT3D(18.80%).
for50KiterationsonDF415. SeeAppendixfordetails. Fig. 6 show the qualitative comparison between AToM
and ATT3D in the large-scale bechmark DF415. ATT3D
4.1.UnseenInterpolationExperiments
mostly outputs indistinguishable 3D assets across various
prompts. Conversely, AToM excels in managing large-
As a significant benefit, AToM generalizes to interpo-
scalebenchmarks, handlingcomplexprompts, andachiev-
lated prompts that are unseen during training. This gen-
ing consistently higher accuracy and higher quality than
eralizability is not possessed by the per-prompt solutions.
ATT3D. For qualitative comparisons against ATT3D and
Fig. 4 showcases the differences of AToM compared to
per-promptsolutionsinPig64andDF27,seeAppendix. We
AToM per-prompt in the Pig64 compositional dataset. We
observe AToM can achieve a comparable performance to
highlight that AToM per-prompt shares the same architec-
the state-of-the-art with consistent quality across prompts
ture but is trained in a per-prompt fashion. We observe
unlikeper-promptsolutions.
thefollowing: (1)AToMcanproducehigh-qualityresults
ofunseenpromptswithoutfurtheroptimization,whileper-
5.AnalysisandAblationStudy
promtoptimziationcannot,asshowninthediagonalinFig.
4;(2)AToMgeneratespigswithasimilaridentityandauni-
We perform ablation studies in DF415 in Fig. 7. We
form orientation, which is not observed in per-prompt ex-
investigate: (1) the effect of two-stage training by com-
periments, indicating that AToM promotes feature sharing
paring AToMwith anend-to-end single-stagetraining ver-
acrossprompts;(3)Per-promptoptimizationismoreprone
sion,(2)theeffectsofthesecondstagebycomparingtothe
to overlooking certain details, such as the top hat in row 2
first-stageoutput,(3)theeffectoftriplanebycomparingto
column4andtheshovelinrow4column2,duetothene-
AToMwithHyperNet-InstantNGP(Hyper-INGPforshort)
cessityforper-promptparametertuning,whileAToMyilds
used in ATT3D as a replacement for positional encoding,
a consistent quality across prompts. In Appendix, we fur-
(4) the effect of architecture designs including ConvNeXt,
therillustratesthetrainingdynamicsofAToMcomparedto
3D-awareconvolution,andcrossattention.
AToMper-prompt,AToMsignificantlyoutperformsitsper-
Two-stage training significantly improves convergence.
prompt version under the same training budgets. Trained
TrainingAToMend-to-endusingasinglestage,i.e.training
bothtoconvergence,weobserveareductionoftrainingit-
solelybythesecondstagefromscratch,leadstopooropti-
erations by over 20 times of AToM vs. AToM per-prompt.
mization. In DF415, we observe that the training diverges
Appendix also qualitatively compare AToM to ATT3D in
at a very early stage (in iteration 2000), generating black
Pig64andtheharderdatasetAnimal2400, whereweagain
images without objects after that. We provides the results
showtheobviousimprovementsofAToMagainstATT3D.
without two-stage training at iteration 2000 before the di-
RefertoAppendixfordetails.
vergence in Fig. 7 column 1. AToM single stage results
in the lowest accuracy (7.47%), significantly worse than
4.2.ComparetotheState-of-the-Art
AToM full (81.93%). Fig. 7 demonstrates the coarse and
Tab. 1presentsourquantitativeresultsintermsofCLIP noisy visual results due to its divergent training. We also
R-probability on Pig64, DF27, and DF415 benchmarks, ablate single-stage training in a smaller dataset DF27 pro-
compared to the amortized text-to-NeRF method ATT3D, videdinAppendix,wherethemodeltrainedthroughsingle
andper-promptapproaches[8,25,38,47]. Inadditiontore- stage can converge but still produces lower-quality results
portingtheofficialresults,WealsoreproduceATT3Dusing thanthebaselineformanyprompts. Overall, theseexperi-
the same diffusion prior [12] as AToM, denoted ATT3D- mentsclearlyshowtheimportanceoftwo-stagetraining.
IF† for a fair comparison. From the experiments, one can Second-stage training yields meshes with improved vi-
observe the following: (1) AToM achieves a higher CLIP sual quality. Fig. 7 column 2&3 shows the usage of our
R-probabilityof75.00%thanATT3D(64.29%)onPig64’s secondstagecanslightlyimprovestheaccuracy. Thisisex-
unseen prompts, indicating its stronger capability to gen- pected because the CLIP R-probability is used to estimate
eralize to unseen prompts. (2) Across the training (seen) thecorrespondencebetweenthegenerated3Dshapeandthe
prompts in Pig64 and DF27, AToM surpasses DreamFu- textprompt,forwhich,thefirststageoutputisgoodenough.
sion [38] and Fantasia3D [8] on both datasets. In compar- The higher-quality renderings from the second stage only
ison to TextMesh [47] and Magic3D [25], AToM slightly have neglectable effect on the CLIP R-probability metric.
lags in CLIP R-probability in Pig64 seen prompts; how- Visuallyspeaking,thesecondstageofAToMincreasesthe
6Figure5.GalleryofAToMevaluatedinDF415.Hereˆ and$denote“azoomedoutDSLRphotoof”and“aDSLRphotoof”,respectively.
7ATT3D-IF† AToM(Ours) ATT3D-IF† AToM(Ours)
Figure6. CompareAToMtoATT3D-IF† evaluatedinDF415. Ineachrow,wemostlyshowresultsfromtwosimilarprompts. While
ATT3Dproducingindistinguishableresultsforsimilarprompts,AToMhandlesthecomplexityofpromptsandachievessignificantlyhigher
qualitythanATT3D.ˆ inthetextdenotes“azoomedoutDSLRphotoof”. OnecanalsoobserveclearimprovementsofAToMoverthe
originalATT3Dbycross-referencingwiththeirpaper.
rendering resolution, reduces noise, and enhances sharp- ing, less distinguishable results with Hyper-INGP are pro-
ness. duced compared to our text-to-triplane network. To verify
Triplane vs. HyperNetworks. We use text-to-triplane as thisperformancedropisnotduetothereducedcomplexity
a text-conditioned positional encoding instead of Hyper- of the network, we also removed all ConvNeXt blocks in
INGP used in ATT3D. Hyper-INGP is a HyperNetworks- text-to-triplaneandusedtwolinearlayerswithspectralnor-
based [18] positional encoding, which utilizes two linear malization to predict the features, which still significantly
layerstopredicttheweightsfortheInstant-NGPpositional outperforms the Hyper-INGP counterpart, as indicated in
encodingnetwork[34].Hyper-INGPisnotnumericallysta- column 5 vs. column 4 (77.11% vs. 35.18%). We high-
ble and therefore requires special layers such as spectral light that only difference between columns 4&5 is the po-
normalization[33]tostabilizeoptimization[29].Duetothe sitional encoding (Instant-NGP or triplane). These experi-
difficultyoflearningtheweightsofanotherneuralnetwork, mentsclearlyindicatethestrengthofourproposedtext-to-
Hyper-INGP shows low model capacity in the large-scale triplanepositionalencoding.
dataset, delivering a poor accuracy (15.18%) in DF415 as Triplane ConvNeXt designs. In Fig. 7 column 5 (w/o
indicatedinTab. 1. Itslowcapacityisalsoverifiedinour ConvNeXt), we experiment AToM without Triplane Con-
ablation study where we replace our text-to-triplane with vNeXt. We observe an accuracy drop of 4.8 points. The
Hyper-INGPinFig.7column4:theaccuracyofAToMfirst proposedTriplaneConvNeXtblocksarehelpfulinenhanc-
stagedropsfrom81.69%toonly35.18%. Visuallyspeak- ingdetails,reducingnoises,andmoreimportantly,preserv-
8single stage AToM (full) AToM first stage triplane Hyper-INGP w/o ConvNeXt
“ A
humanoid
robot
playing
the violin”
“ $ a
humanoid
robot
using a
laptop”
“ $ a
group of
dogs
playing
poker”
Avg R-Prob 0.0747 0.8193 0.8169 0.3518 0.7711
Figure7.Ablationstudy.WecompareAToMfullpipelineincol-
umn 2 against the end-to-end approach without two-stage train-
ingincolumn1,thefirst-stageoutputwithoutsecond-stagerefine-
mentincolumn3,AToMfirststagewithouttriplanebutemploying
Hyper-INGPusedinATT3D[29]incolumn4, AToMfirststage
withoutConvNeXtblocksbutusingtwolinearlayerswithspectral
normalizationfortext-to-triplaneincolumn5.Quantitativeresults
inaverageR-ProbabilityevaluatedintheentireDF415datasetare
providedatthebottom.$inthetextdenotes“aDSLRphotoof”.
ing components of complex prompts. We also tried to re-
placeConvNeXtblocksintoTransformerblocksbutfound
Transformerdidnotconverge. WehypothesizethatTrans-
former requires a significantly larger amount of data. We
also perform ablation studies on the components of Tri-
plane ConvNeXt blocks to investigate the effectiveness of
3D-awareconvolutionandcrossattention,andreachlower
CLIPR-Prob79.76%and79.28%, respectively. Thesein-
dicate that both 3D-aware convolution and cross attention
improvetheperformance.
6.Conclusion
This work proposes AToM, the first amortized text-to-
mesh framework. AToM introduces a 3D-aware text-to-
triplanenetwork,whichleadstosuperiorqualitycompared
to the HyperNetworks counterpart used in ATT3D. AToM
also adopts a two-stage amortized optimization to stabi-
lize the text-to-mesh generation. AToM significantly re-
duces training time compared to per-prompt solutions due
to geometry sharing of amortized optimization. More im-
portantly, AToM demonstrates strong generalizability, pro-
ducinghigh-quality3Dcontentforunseenpromptswithout
furtheroptimization. ComparedtoATT3D,AToMachieves
an accuracy more than 4 times higher in DF415. Qualita-
tively,AToMoutperformsATT3Dbyprovidingdistinguish-
able3Dassetswithbettergeometryandtexture.Webelieve
AToM,alongwiththecode,thepretrainedmodels,andthe
generated 3D assets that will be made publicly available,
willcontributetopushtheboundariesoftext-to-meshgen-
eration.
9AToM:AmortizedText-to-Meshusing2DDiffusion
—Appendix—
A.ImplementationDetails
N Text-conditioned 3D-aware ConvNeXt Block
A.1.AToMimplementationdetails
avg reshape AToM uses a similar camera and rendering setup to
TextMesh[47]inthefirststageandsimilartoMagic3D[25]
inthesecondstage. Weboundtheobjectsin2metersand
set the camere 3 meters away from the object center. We
employ a field of view ranging from 40 to 70 in the first
Figure I. Text-to-Triplane architecture. Triplane is generated
stageandfrom30to40inthesecondstage. Softrendering from the averaged text embedding followed by a linear projec-
with50%/50%probabilitiesfortextureless/diffuseshading tionandthenrefinedbymultipletext-conditioned3D-awareCon-
isusedtoavoidlearningflatgeometry. WeimplementSDF, vNeXtblocks.
deformation,andcolornetworksusingthreeseparatethree-
layer MLPs with hidden channels 64. We empirically find
that these separate networks slightly improve the genera- toMagic3D[25],thewell-knownper-prompttext-to-mesh
tion quality than the single model used in ATT3D. Origi- work, we are similar in two-stage training, but differ from
nal texts without direction are used as input for the text- each other. Magic3D uses the results of the first stage to
to-mesh network, while directional prompts with “, front initialize the SDF parameters of the second stage through
view”, “, sideview”, “, backview”, “, overheadview”are anoptimizationprogress[25],leadingtoinaccurateinitial-
used as text condition in the diffusion model. We utilize ization and cannot be trained amortized. Conversely, the
Deep Floyd [12] as the text-to-image prior with guidance AToMnetworkremainsunchangedacrossbothstages. The
scale20. Arandomnoisefrom(0.2,0.98)and(0.02,0.5) first stage training in AToM serves as a warmup phase for
the second stage. This approach uses the same SDF and
A.2.ATT3Dreimplementationdetails color networks in both stages and eliminates the need for
optimizationoftheSDFparameters,unlikeMagic3D.Last
In the main paper, we report the quantitative results
butnotleast,AToMdiffersfromATT3D[29]intwoimpor-
of the original ATT3D and our re-implemented version
tantaspects:(1)AToMisthefirsttoenableamortizedtrain-
ATT3D-IF†.
ing for text-to-mesh generation, while ATT3D only sup-
OriginalATT3Disnotyetreleased.Weretrievethequanti-
portstext-to-NeRF;(2)AToMusestriplanetoconditionthe
tativeresultsfromFigure6intheiroriginalpaper. Wecom-
generationof3Dcontent, whichismorerobusttotraining
parewiththeoriginalATT3DinTable1inourmanuscript.
parametersandismorestableinoptimizationcomparedto
ATT3D-IF† denotes our reimplementation using the exact
theHyperNet-basedsolutionsinATT3D.
same architecture, training parameters, and camera setups
Triplane ConvNeXt. We provides a pseudo code for the
as mentioned in the original paper, except for the unava-
proposedTriplaneConvNeXtinAlgorithm1. Weillustrate
iable ones (where we use the same as AToM). The only
TriplaneConvNeXtinFig. I.
difference of ATT3D-IF† from the original ATT3D is the
diffusionprior: whiletheoriginalATT3Dusedtheirinter-
C.AdditionalResults
nalversion,weadoptthepubliclyavailableIFmodelfrom
Deep Floyd [12]. We cannot achieve exact the same per- C.1.Pig64
formance as ATT3D mostly due to the different diffusion
Datasetdetails.Pig64isstructuredaccordingtothepattern
prior.
“apig{activity}{theme}”from8activitiesand8themes.
activities = [“riding a motorcycle”, “sitting on a chair”,
B.MethodDetails
“playing the guitar”, “holding a shovel”, “holding a blue
Method Comparison. AToM is trained on many prompts balloon”, “holdingabook”, “wieldingakatana”, “ridinga
simultaneouslythroughSDSlosswithoutany3Ddataand bike”].
generates textured meshes in less than 1 second during themes = [“made out of gold”, “carved out of wood”,
inference. This differs AToM from previous 3D recon- “wearingaleatherjacket”, “wearingatophat”, “wearinga
struction models such as GET3D [14], which requires 3D cape”,“wearingapartyhat”,“wearingasombrero”,“wear-
Ground Truth and is thus limited by the availability of ingmedievalarmor”]
3D data and the low diversity of 3D shapes. Compared TwostagesofAToMonPig64. Fig. IIshowsthecompar-
10
Embeddings
Text Linear Attention Cross
Convolution
3D-aware GELU DWConv Linear GELU Linear Triplane(a)AToMStage1 (b)AToMStage2
FigureII.ResultsofAToMfirststage(left)andsecondstage(right)onthePig64compositionalpromptset.Themeshrefinementstage
(secondstage)turnstheNeuSrepresentationtoahigh-resolutionmeshrepresentationandsharplyincreasesvisualquality.
Algorithm1CodeforTriplaneConvNeXt(PyTorch[36]like)
import torch.nn.functional as F
def forward(text_emb):
# map averaged text_emb to triplane
avg_emb = text_emb.mean(dim=1,keepdims=False)
x = self.linear(avg_emb)
# reshape to triplane
x = x.reshape(-1, self.c, self.h, self.w)
# Triplane ConvNeXt blocks
for i in range(self.num_blocks):
inp = x
# cross attention
x = x + self.crossatt(x,text_emb)
# 3D aware convolution
x = x + F.relu(self.aware3dconv(x))
# FeedForard network
x = x + self.ffn(x)
# residual connection
x = inp + x
FigureIII.TrainingdynamicscomparisonsbetweenAToMand
AToM Per-prompt. Amortized training significantly reduces
trainingcostper-prompt.
isons of AToM first-stage outputs and AToM second-stage prompt optimization typically requires 2000 - 8000 train-
outputs.Themeshrefinementstage(secondstage)turnsthe ingbatches,amortizedoptimizationwithAToMreachesthe
NeuS representation to a high-resolution mesh representa- same-level accuracy with only 142 training batches per-
tionandsharplyincreasesvisualquality. prompt. In other words, AToM reduces the training time
Training dynamics. Fig. III shows the training dynam- inthiscompositionaldatasetbymorethan10×.
ics of AToM compared to AToM Per-prompt (per-prompt
optimizedversionsofAToMnetwork). Amortizedtraining ATT3D reimplementation. We compare our reimple-
significantly reduces training cost per prompt. While per- mentedATT3D-IF†totheoriginalATT3DinFig. IV.
11(a)OriginalATT3D (b)ATT3D-IF†
FigureIV.CompareourreproducedATT3D-IF† atrighttooriginalATT3D[29]atleft. Duetothedistinctdiffusionemployedin
ATT3D-IF†,thedisparateoutcomesfromoriginalATT3Dareexpected. Asstrength,ourreimplementationusingDeepFloydguidance
facilitatesmoregeometrysharing,yieldsresultswithlessvariances,andreducesnoises. Especially,wehighlighttheunseenexamplesin
thediagonal,whereATT3D-IF†showsbettergeneralizabilitythenorigianlATT3D.Asdrawbacks,ourreimplementationhandlesprompts
sometimesworsethantheoriginalversion,e.g.notallpigsaremadeoutofwoodinthesecondcolumn.Desiptethedifferentimplementa-
tion,AToMoutperformsbothversions,seeFig.IIforqualitativeimprovementandTab.1inmainpaperforquantitativeimprovements.
C.2.Animal2400 andATT3D-IF†(0.1671). AToMtrainedinthis12.5%split
seemsevenoutperformstheoriginalATT3Dtrainedin50%
Dataset details. We also include comparisons of AToM
splitbycrossreferencingFig. 8inATT3D[29].
to ATT3D on Animal2400 12.5% split, where only 300
promptsareusedintrainingandall2400promptsareused C.3.DF27
in testing. Animal2400 is constructed following the tem-
We compare AToM with per-prompt solutions and
plate “{animal} {activity} {theme} {hat}”. There are 10
ATT3D-IF†inFig. VII.Notethatwedonotexpecttheper-
animals,8activities,6themes,and5hats.
formanceofamortizedtrainingtobebetterthanper-prompt
animals = [“a squirrel”, “a raccoon”, “a pig”, “a mon-
trainingfortheseenprompts.
key”, “a robot”, “a lion”, “a rabbit”, “a tiger”, “an
orangutan”,“abear”] D.AdditionalAblationStudy
activities = [“riding a motorcycle”, “sitting on a chair”,
“playing the guitar”, “holding a shovel”, “holding a blue Single-stage training in DF27. In manuscript, we show
balloon”, “holdingabook”, “wieldingakatana”, “ridinga that single-stage training in DF400 leads to divergent op-
bike”] timzation. Here, we further demonstrate that single-stage
training can converge in smaller dataset (DF27), but still
themes = [“wearing a leather jacket”, “wearing a
suffersfrompoorgeometry. SeeexamplesinFig. VI.
sweater”, “wearing a cape”, “wearing medieval armor”,
“wearingabackpack”,“wearingasuit”]
E.LimitationandFutureWork
hats = [“wearing a party hat”, “wearing a sombrero”,
“wearing a helmet”, “wearing a tophat”, “wearing a base- First, the quality of AToM is bounded by the diffusion
ballcap”] prior employed. Throughout the work, we utilized IF-
Results. AToM significantly outperforms ATT3D-IF† as stage1 [12] as the diffusion prior, which limits the high-
shown in Fig. V. Quantitatively, AToM achieves 0.3422 frequency details due to its low-resolution input. The use
CLIPR-Probability,higherthantheoriginalATT3D(0.11) of a higher-resolution diffusion like Stable Diffusion [41]
12(a)ATT3D-IF†.CLIPR-Probin2400prompts:0.1671.
(b)AToM.CLIPR-Probin2400prompts:0.3422.
FigureV.CompareAToMtoATT3D-IF†onAnimal240012.5%split. Trainedonlyin300prompts,AToMalsogeneralizestoall2400
prompts,andsignificantlyoutperformsATT3DandATT3D-IF†. SeetheoverallimprovedqualityandhowAToMperservestheprompts
whenATT3D-IF†overlooks(e.g.,thebackpacksinthesecondcolumn).
13single stage AToM single stage AToM
Figure VI. Compare Single-Stage AToM with AToM Full.
Single-stagetrainingcanconvergeinsmallerdataset(DF27),but
stillsuffersfrompoorgeometry,comparedtothetwo-stagetrain-
ingofAToM.
(SD)andIF-stage2mightimprovethequalityofAToMfur-
ther. WemadeinitialattemptstouseSD,SD’svariantsin-
cluding VSD [53] and MVDream [43], and IF-stage2 but
resulted in worse performance. We hypothesize the lower
quality stems from the difficulties of their optimization in
theamortizedsettings. Webelievehowtocorrectlyutilize
strongerdiffusionforamortizedtext-to-meshispromising.
Second,AToMusedDMTetwithSDFasthemeshrepresen-
tation,whichisnotcapableofmodelingsurfacesofnonzero
genus. Moreadvancedmeshrepresentationcanbeadopted
toaddressthislimitation,whichisorthogonaltoourstudy.
Third, JanusproblemalsoexistsinsomeresultsofAToM,
despitethefactthatAToMalleviateitalotmostlybygeom-
etry sharing in amortized optimization. We also tried pro-
gressiveviews,debiasingscores[19],Perp-Neg[2],butem-
piricallyfoundthattheydidnotworktriviallyintheamor-
tizedsetting. Weleavetheselimitationsandtheirpotential
solutionsasfuturework.
14DreamFusion- TextMesh- Magic3D- Fantasia3D AToM ATT3D- AToM
IF[16,38] IF[16,47] IF[16,25] [8,16] Per-prompt IF†[29] (Ours)
(Ours)
FigureVII.VisualcomparisonsofAToMagainstthestate-of-the-artper-promptsolutions(firstfourcolumns), AToMPer-prompt, and
ourreproducedATT3DinDF27dataset. AToMachieveshigherqualitythanATT3Dandaperformancecomparabletotheper-prompt
solutions.
15FigureVIII.MoreresultsofAToMevaluatedinDF415.
16References [13] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong
Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Chris-
[1] Brandon Amos et al. Tutorial on amortized optimization.
tian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli
FoundationsandTrends®inMachineLearning,16(5):592–
VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia
732,2023. 3
Gkioxari,KianaEhsani,LudwigSchmidt,andAliFarhadi.
[2] Mohammadreza Armandpour, Huangjie Zheng, Ali
Objaverse-xl:Auniverseof10m+3dobjects.arXivpreprint
Sadeghian, Amir Sadeghian, and Mingyuan Zhou. Re-
arXiv:2307.05663,2023. 2
imagine the negative prompt algorithm: Transform 2d
[14] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,
diffusioninto3d,alleviatejanusproblemandbeyond. arXiv
KangxueYin,DaiqingLi,OrLitany,ZanGojcic,andSanja
preprintarXiv:2304.04968,2023. 14
Fidler. Get3d: A generative model of high quality 3d tex-
[3] SudarshanBabu,RichardLiu,AveryZhou,MichaelMaire,
turedshapeslearnedfromimages. AdvancesinNeuralIn-
GregShakhnarovich,andRanaHanocka. Hyperfields: To-
formationProcessingSystems(NeurIPS),35:31841–31854,
wardszero-shotgenerationofnerfsfromtext.arXivpreprint
2022. 1,2,4,10
arXiv:2310.17075,2023. 3
[15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
[4] EricRChan,ConnorZLin,MatthewAChan,KokiNagano,
Xu,DavidWarde-Farley,SherjilOzair,AaronCourville,and
Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J
YoshuaBengio. Generativeadversarialnets. InAdvancesin
Guibas,JonathanTremblay,SamehKhamis,etal. Efficient
neuralinformationprocessingsystems(NIPS),2014. 2
geometry-aware3dgenerativeadversarialnetworks. InPro-
[16] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian
ceedingsoftheIEEE/CVFConferenceonComputerVision
Laforte, Vikram Voleti, Guan Luo, Chia-Hao Chen, Zi-
andPatternRecognition(CVPR),pages16123–16133,2022.
Xin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang.
2,3
threestudio: Aunifiedframeworkfor3dcontentgeneration.
[5] EricRChan, MarcoMonteiro, PetrKellnhofer, JiajunWu,
https://github.com/threestudio-project/
andGordonWetzstein. pi-gan: Periodicimplicitgenerative
threestudio,2023. 5,15
adversarialnetworksfor3d-awareimagesynthesis. InPro-
[17] AnchitGupta,WenhanXiong,YixinNie,IanJones,andBar-
ceedingsoftheIEEE/CVFConferenceonComputerVision
lasOg˘uz. 3dgen:Triplanelatentdiffusionfortexturedmesh
andPatternRecognition(CVPR),pages5799–5809,2021.2
generation. arXivpreprintarXiv:2303.05371,2023. 2
[6] Dave Zhenyu Chen, Haoxuan Li, Hsin-Ying Lee, Sergey
[18] David Ha, Andrew M. Dai, and Quoc V. Le. Hypernet-
Tulyakov,andMatthiasNießner.Scenetex:High-qualitytex-
works. InInternationalConferenceonLearningRepresen-
turesynthesisforindoorscenesviadiffusionpriors. arXiv
tations(ICLR).OpenReview.net,2017. 2,8
preprintarXiv:2311.17261,2023. 2
[19] SusungHong,DonghoonAhn,andSeungryongKim. Debi-
[7] HanshengChen,JiataoGu,AnpeiChen,WeiTian,Zhuowen
asingscoresandpromptsof2ddiffusionforrobusttext-to-3d
Tu, Lingjie Liu, and Hao Su. Single-stage diffusion nerf:
generation. arXivpreprintarXiv:2303.15413,2023. 14
Aunifiedapproachto3dgenerationandreconstruction. In
ProceedingsoftheIEEE/CVFInternationalConferenceon [20] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter
ComputerVision(ICCV),2023. 2 Abbeel, and Ben Poole. Zero-shot text-guided object gen-
erationwithdreamfields. InProceedingsoftheIEEE/CVF
[8] RuiChen,YongweiChen,NingxinJiao,andKuiJia. Fan-
Conference on Computer Vision and Pattern Recognition
tasia3d: Disentangling geometry and appearance for high-
(CVPR),pages867–876,2022. 2
quality text-to-3d content creation. In Proceedings of the
IEEE/CVF International Conference on Computer Vision [21] Heewoo Jun and Alex Nichol. Shap-e: Generat-
(ICCV), pages 22246–22256, October 2023. 1, 2, 3, 4, 5, ing conditional 3d implicit functions. arXiv preprint
6,15 arXiv:2305.02463,2023. 2
[9] Wenzheng Chen, Huan Ling, Jun Gao, Edward Smith, [22] Tero Karras, Samuli Laine, and Timo Aila. A style-based
Jaakko Lehtinen, Alec Jacobson, and Sanja Fidler. Learn- generator architecture for generative adversarial networks.
ingtopredict3dobjectswithaninterpolation-baseddiffer- In Proceedings of the IEEE/CVF Conference on Computer
entiablerenderer. AdvancesinNeuralInformationProcess- VisionandPatternRecognition(CVPR),pages4401–4410,
ingSystems(NeurIPS),2019. 2 2019. 2
[10] Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexan- [23] Chun-LiangLi,ManzilZaheer,YangZhang,BarnabasPoc-
derGSchwing,andLiang-YanGui. Sdfusion: Multimodal zos, and Ruslan Salakhutdinov. Point cloud gan. arXiv
3dshapecompletion,reconstruction,andgeneration.InPro- preprintarXiv:1810.05795,2018. 2
ceedingsoftheIEEE/CVFConferenceonComputerVision [24] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun
andPatternRecognition(CVPR),2023. 2 Luan,YinghaoXu,YicongHong,KalyanSunkavalli,Greg
[11] Zezhou Cheng, Menglei Chai, Jian Ren, Hsin-Ying Lee, Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d
KyleOlszewski, ZengHuang, SubhransuMaji, andSergey withsparse-viewgenerationandlargereconstructionmodel.
Tulyakov. Cross-modal3dshapegenerationandmanipula- CoRR,abs/2311.06214,2023. 2
tion. InProceedingsoftheEuropeanConferenceonCom- [25] Chen-HsuanLin,JunGao,LumingTang,TowakiTakikawa,
puterVision(ECCV),2022. 2 Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,
[12] DeepFloyd,StabilityAI. Deepfloydif,2023. GitHubreposi- Ming-YuLiu,andTsung-YiLin. Magic3d: High-resolution
tory. 3,5,6,10,12 text-to-3dcontentcreation.InProceedingsoftheIEEE/CVF
17Conference on Computer Vision and Pattern Recognition [38] BenPoole,AjayJain,JonathanTBarron,andBenMilden-
(CVPR),2023. 1,2,3,4,5,6,10,15 hall. Dreamfusion: Text-to-3d using 2d diffusion. Inter-
[26] ChiehHubertLin,Hsin-YingLee,WilliMenapace,Menglei national Conference on Learning Representations (ICLR),
Chai, Aliaksandr Siarohin, Ming-Hsuan Yang, and Sergey 2022. 1,2,3,4,5,6,15
Tulyakov. Infinicity: Infinite-scale city synthesis. In [39] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,
ProceedingsoftheIEEE/CVFInternationalConferenceon Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-
ComputerVision(ICCV),2023. 2 rokhodov,PeterWonka,SergeyTulyakov,etal. Magic123:
[27] ZhuangLiu,HanziMao,Chao-YuanWu,ChristophFeicht- One image to high-quality 3d object generation using both
enhofer,TrevorDarrell,andSainingXie. Aconvnetforthe 2dand3ddiffusionpriors.arXivpreprintarXiv:2306.17843,
2020s.InProceedingsoftheIEEE/CVFConferenceonCom- 2023. 2
puterVisionandPatternRecognition(CVPR),2022. 4 [40] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,
[28] WilliamELorensenandHarveyECline. Marchingcubes: SharanNarang, MichaelMatena, Yanqi Zhou, WeiLi, and
Ahighresolution3dsurfaceconstructionalgorithm.InSem- PeterJ.Liu. Exploringthelimitsoftransferlearningwitha
inalgraphics:pioneeringeffortsthatshapedthefield,pages unifiedtext-to-texttransformer. JournalofMachineLearn-
347–353.1998. 1 ingResearch,21(140):1–67,2020. 3
[29] Jonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan [41] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Lin,TowakiTakikawa,NicholasSharp,Tsung-YiLin,Ming- Patrick Esser, and Bjo¨rn Ommer. High-resolution image
Yu Liu, Sanja Fidler, and James Lucas. Att3d: Amortized synthesis with latent diffusion models. In Proceedings of
text-to-3dobjectsynthesis.InProceedingsoftheIEEE/CVF theIEEE/CVFConferenceonComputerVisionandPattern
InternationalConferenceonComputerVision(ICCV),pages Recognition(CVPR),pages10684–10695,2022. 1,5,12
17946–17956,October2023. 1,3,5,8,9,10,12,15
[42] TianchangShen,JunGao,KangxueYin,Ming-YuLiu,and
[30] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and
SanjaFidler. Deepmarchingtetrahedra: ahybridrepresen-
Andrea Vedaldi. Realfusion: 360{\deg} reconstruction
tation for high-resolution 3d shape synthesis. In Advances
of any object from a single image. In Proceedings of
in Neural Information Processing Systems (NeurIPS), vol-
theIEEE/CVFConferenceonComputerVisionandPattern
ume34,pages6087–6101,2021. 2,4
Recognition(CVPR),2023. 2
[43] YichunShi,PengWang,JianglongYe,MaiLong,KejieLi,
[31] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
andXiaoYang. Mvdream:Multi-viewdiffusionfor3dgen-
JonathanTBarron,RaviRamamoorthi,andRenNg. Nerf:
eration. CoRR,abs/2308.16512,2023. 14
Representingscenesasneuralradiancefieldsforviewsyn-
[44] Dong Wook Shu, Sung Woo Park, and Junseok Kwon.
thesis. InProceedingsoftheEuropeanConferenceonCom-
3d point cloud generative adversarial network based on
puterVision(ECCV),pages405–421.Springer,2020. 1
tree structured graph convolutions. In Proceedings of the
[32] ParitoshMittal,Yen-ChiCheng,ManeeshSingh,andShub-
IEEE/CVF International Conference on Computer Vision
ham Tulsiani. Autosdf: Shape priors for 3d comple-
(ICCV),2019. 2
tion, reconstruction and generation. In Proceedings of
[45] EdwardJSmithandDavidMeger.Improvedadversarialsys-
theIEEE/CVFConferenceonComputerVisionandPattern
temsfor3dobjectgenerationandreconstruction. InConfer-
Recognition(CVPR),2022. 2
enceonRobotLearning(CoRL),2017. 2
[33] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and
[46] JaschaSohl-Dickstein, EricWeiss, NiruMaheswaranathan,
Yuichi Yoshida. Spectral normalization for generative ad-
and Surya Ganguli. Deep unsupervised learning using
versarialnetworks.InInternationalConferenceonLearning
nonequilibriumthermodynamics. InProceedingsoftheIn-
Representations(ICLR).OpenReview.net,2018. 2,8
ternationalConferenceonMachineLearning(ICML),pages
[34] ThomasMu¨ller,AlexEvans,ChristophSchied,andAlexan-
2256–2265.PMLR,2015. 1
derKeller.Instantneuralgraphicsprimitiveswithamultires-
olution hash encoding. In ACM Transactions on Graphics [47] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni,
(SIGGRAPH),2022. 2,8 MichaelNiemeyer,andFedericoTombari. Textmesh: Gen-
eration of realistic 3d meshes from text prompts. In 2024
[35] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela
InternationalConferenceon3DVision(3DV),2024. 1,2,3,
Mishkin, and Mark Chen. Point-e: A system for generat-
ing3dpointcloudsfromcomplexprompts. arXivpreprint 4,5,6,10,15
arXiv:2212.08751,2022. 2 [48] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko-
[36] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, reit,LlionJones,AidanNGomez,ŁukaszKaiser,andIllia
James Bradbury, Gregory Chanan, Trevor Killeen, Zem- Polosukhin.Attentionisallyouneed.InAdvancesinNeural
ing Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: InformationProcessingSystems(NeurIPS),2017. 3
An imperative style, high-performance deep learning li- [49] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,
brary. In Advances in Neural Information Processing Sys- andGregShakhnarovich. Scorejacobianchaining: Lifting
tems(NeurIPS),2019. 11 pretrained 2d diffusion models for 3d generation. In Pro-
[37] Dario Pavllo, Graham Spinks, Thomas Hofmann, Marie- ceedingsoftheIEEE/CVFConferenceonComputerVision
FrancineMoens,andAurelienLucchi.Convolutionalgener- andPatternRecognition(CVPR),2023. 2
ationoftextured3dmeshes.AdvancesinNeuralInformation [50] NanyangWang,YindaZhang,ZhuwenLi,YanweiFu,Wei
ProcessingSystems(NeurIPS),2020. 2 Liu,andYu-GangJiang. Pixel2mesh: Generating3dmesh
18modelsfromsinglergbimages. InProceedingsoftheEuro-
peanconferenceoncomputervision(ECCV),pages52–67,
2018. 1,2
[51] PengWang,LingjieLiu,YuanLiu,ChristianTheobalt,Taku
Komura, and Wenping Wang. Neus: Learning neural im-
plicit surfaces by volume rendering for multi-view recon-
struction. In Advances in Neural Information Processing
Systems(NeurIPS),2021. 4
[52] TengfeiWang,BoZhang,TingZhang,ShuyangGu,Jianmin
Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang
Wen, Qifeng Chen, and Baining Guo. Rodin: A genera-
tive model for sculpting 3d digital avatars using diffusion.
In Proceedings of the IEEE/CVF Conference on Computer
VisionandPatternRecognition(CVPR),pages4563–4573,
June2023. 3
[53] ZhengyiWang,ChengLu,YikaiWang,FanBao,Chongxuan
Li,HangSu,andJunZhu.Prolificdreamer:High-fidelityand
diversetext-to-3dgenerationwithvariationalscoredistilla-
tion. arXivpreprintarXiv:2305.16213,2023. 2,14
[54] JiajunWu,ChengkaiZhang,TianfanXue,BillFreeman,and
Josh Tenenbaum. Learning a probabilistic latent space of
object shapes via 3d generative-adversarial modeling. Ad-
vancesinNeuralInformationProcessingSystems(NeurIPS),
2016. 2
[55] Jianwen Xie, Zilong Zheng, Ruiqi Gao, Wenguan Wang,
Song-Chun Zhu, and Ying Nian Wu. Learning descriptor
networks for 3d shape synthesis and analysis. In Proceed-
ingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition(CVPR),2018. 2
[56] BiaoZhang,MatthiasNießner,andPeterWonka. 3dilg: Ir-
regularlatentgridsfor3dgenerativemodeling. Advancesin
NeuralInformationProcessingSystems(NeurIPS),2022. 2
[57] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter
Wonka. 3dshape2vecset:A3dshaperepresentationforneu-
ralfieldsandgenerativediffusionmodels.ACMTransactions
onGraphics(SIGGRAPH),2023. 2
[58] Song-Hai Zhang, Yuan-Chen Guo, and Qing-Wen Gu.
Sketch2model: View-aware 3d modeling from single free-
handsketches. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition(CVPR),2021.
2
[59] Joseph Zhu and Peiye Zhuang. Hifa: High-fidelity text-
to-3d with advanced diffusion guidance. arXiv preprint
arXiv:2305.18766,2023. 2
19