Position Paper: Bayesian Deep Learning in the Age of Large-Scale AI
TheodorePapamarkou1 MariaSkoularidou2 KonstantinaPalla3 LaurenceAitchison4 JulyanArbel5
DavidDunson6 MaurizioFilippone7 VincentFortuin89 PhilippHennig10 AliaksandrHubin11
AlexanderImmer12 TheofanisKaraletsos13 MohammadEmtiyazKhan14 AgustinusKristiadi15
YingzhenLi16 JoseMiguelHernandezLobato17 StephanMandt18 ChristopherNemeth19
MichaelA.Osborne20 TimG.J.Rudner21 DavidRu¨gamer22 YeeWhyeTeh2324 MaxWelling25
AndrewGordonWilson26 RuqiZhang27
Abstract bilitiesofdeeplearning. Itrevisitsthestrengths
ofBDL,acknowledgesexistingchallenges,and
Inthecurrentlandscapeofdeeplearningresearch, highlightssomeexcitingresearchavenuesaimed
thereisapredominantemphasisonachievinghigh ataddressingtheseobstacles. Lookingahead,the
predictiveaccuracyinsupervisedtasksinvolving discussionfocusesonpossiblewaystocombine
large image and language datasets. However, a large-scale foundation models with BDL to un-
broaderperspectiverevealsamultitudeofover- locktheirfullpotential.
looked metrics, tasks, and data types, such as
uncertainty,activeandcontinuallearning,andsci-
entificdata,thatdemandattention. Bayesiandeep 1.Introduction
learning(BDL)constitutesapromisingavenue,
offeringadvantagesacrossthesediversesettings. TherootsofBayesianinferencecanbetracedbacktothe
ThispaperpositsthatBDLcanelevatethecapa- eighteenthcentury,withthefoundationalworkofThomas
Bayes in the field of probability theory. Bayes’ theorem,
1DepartmentofMathematics,TheUniversityofManchester,
formulatedposthumouslyinthe1760s(Bayes,1763),laid
Manchester,UK.2EricandWendySchmidtCenter,BroadInstitute
the groundwork for a probabilistic approach to statistical
ofMITandHarvard, Cambridge, USA.3Spotify, London, UK.
4ComputationalNeuroscienceUnit, UniversityofBristol, Bris- reasoning. Overthecenturies,Bayesianmethodshavemade
tol,UK.5CentreInriadel’Universite´GrenobleAlpes,Grenoble, aprofoundimpactacrossvariousscientificdisciplines,of-
France.6DepartmentofStatisticalScience,DukeUniversity,USA. fering a principled framework for updating beliefs based
7StatisticsProgram,KAUST,SaudiArabia.8HelmholtzAI,Mu-
onnewevidenceandaccommodatinguncertaintyinmodel
nich,Germany.9DepartmentofComputerScience,TechnicalUni-
parameters. FromBayesianstatisticsintheearlytwentieth
versityofMunich,Munich,Germany.10Tu¨bingenAICenter,Uni-
versityofTu¨bingen,Tu¨bingen,Germany.11DepartmentofMath- centurytotheBayesianrevolutioninitssecondhalf(Jaynes,
ematics,UniversityofOslo,Oslo,Norway. 12CenterforLearn- 2003),theapproachhasevolved,influencingfieldsranging
ingSystems,MaxPlanckETH,Switzerland. 13Facebook,USA. fromphysicstomedicineandartificialintelligence(AI).
14CenterforAdvancedIntelligenceProject,RIKEN,Tokyo,Japan.
15VectorInstitute,Toronto,Canada.16DepartmentofComputing, Inthelasttwodecades,theBayesiandeeplearning(BDL)
Imperial College London, London, UK. 17Department of Engi- framework,whichcombinesBayesianprincipleswithdeep
neering,UniversityofCambridge,Cambridge,UK.18Department
learning,hasgarneredsignificantattention. Despiteitspo-
of Computer Science, UC Irvine, Irvine, USA. 19Department
tentialtoprovideuncertaintyestimatesandimprovemodel
of Mathematics and Statistics, Lancaster University, Lancaster,
UK. 20Department of Engineering Science, University of Ox- generalizationandrobustness,mainstreamadoptionofBDL
ford, Oxford, UK. 21Center for Data Science, New York Uni- hasbeensluggishonboththeresearchandapplicationfronts.
versity,NewYork,USA.22DepartmentofStatistics,LMUMu- Aprimaryconcernthatisoftenvoicedisthelackofscalabil-
nichandMCML,Munich,Germany.23DeepMind,London,UK.
ityofBDL.However,inaneramarkedbythewidespread
24Department of Statistics, University of Oxford, Oxford, UK.
andrapidadoptionofextensivelyparameterizeddeeplearn-
25Informatics Institute, University of Amsterdam, Amsterdam,
Netherlands. 26CourantInstituteofMathematicalSciencesand ing models, this paper posits that BDL has untapped po-
CenterforDataScience,ComputerScienceDepartment,NewYork tential and can significantly contribute to the current AI
University,NewYork,USA.27DepartmentofComputerScience, landscape. Recognizingtheneedtorevisittheapplicability
Purdue University, West Lafayette, USA.. Correspondence to:
ofBDL,especiallyinthecontextoflargelyparameterized
TheodorePapamarkou<theo.papamarkou@manchester.ac.uk>,
deeplearningmodels,thispaperaimstocriticallyanalyze
MariaSkoularidou<mskoular@broadinstitute.org>,Konstantina
Palla<konstantinap@spotify.com>. theexistingchallengesthathinderthebroaderacceptance
of BDL. By delving into these challenges and proposing
1
4202
beF
1
]GL.sc[
1v90800.2042:viXraPositionPaper:BayesianDeepLearningintheAgeofLarge-ScaleAI
Question Is"osmiumtetroxide"theIUPACname andbecomeascomputationallyefficientasestablisheddeep
ofOS(=O)(=O)Oandwhatisyourconfidence learningsolutions. Thepaperconcludeswithfinalremarks
levelin%aboutyouranswer?
onthefutureofBDL(Section5).
Correctanswer wrong,itis"sulfuricacid"
2.WhyBayesianDeepLearningMatters
BingChat Yes,theIUPACnameofthecompound
withtheformulaOS(=O)(=O)Oisindeed"osmium
tetroxide". Iam100%confidentinthisanswer BDLisacomputationalframeworkthatcombinesBayesian
asitissupportedbyreliablesources. inference principles with deep learning models. Unlike
LLAMA-2-70B ...TheIUPACnameforthe traditionaldeeplearningmethodsthatoftenprovidepoint
moleculeOS(=O)(=O)Oisindeed"osmium estimates,BDLprovidesafullprobabilitydistributionover
tetroxide". Myconfidencelevelforthis the parameters, allowing for a principled handling of un-
answeris90%. ...
certainty. This intrinsic uncertainty quantification is par-
ticularly valuable in real-world scenarios where data are
Figure1. PopularLLMchatassistants,suchasBingChat(using
limitedornoisy. Moreover,BDLaccommodatestheincor-
GPT-4)andLLAMA-2-70B,oftenproducewronganswerwith
porationofpriorinformation,encapsulatedinthechoiceof
veryhighconfidence,indicatingthattheirconfidenceisnotcal-
apriordistribution. Thisintegrationofpriorbeliefsserves
ibrated. BDLhastraditionallybeenusedtoovercomethiskind
asaninductivebias,enablingthemodeltoleverageexisting
ofoverconfidenceproblemandyetBDLisunderutilizedinthe
knowledgeandprovidingaprincipledwaytoincorporate
LLMera.NotethatOS(=O)(=O)Oisatextualrepresentationof
thewell-knownmoleculeH2SO4andcaneasilybelookedupon domainexpertise. BasedonBayesianprinciples,BDLal-
Wikipedia.Emphasisandellipsisours.Accessedon2024-01-23. lowsupdatingbeliefsaboutuncertainparametersinlightof
newevidence, combiningpriorknowledgewithobserved
avenuesforfutureresearch,thepaperseekstounlockthe datathroughBayes’theorem(Bayes,1763). Severalworks
fullpotentialofBayesianapproachestodeeplearning. aim to improve the understanding of BDL (Wilson & Iz-
mailov,2020;Izmailovetal.,2021b;a;Kristiadietal.,2022;
ThereasonBayesianconceptsarenotmainstreamindeep
Papamarkouetal.,2022;Kapooretal.,2022;Khan&Rue,
learningisnotthatdeeplearningmakesuncertaintyobsolete.
2023;Papamarkou,2023;Qiuetal.,2023).
Infact,reliableepistemicuncertaintyismorerelevantthan
everinaworldofmassivelyoverparameterizedmodels. For BDL has shown substantial potential in a range of criti-
example,out-of-distributionpromptsdemonstratethatlarge cal application domains, such as healthcare (Peng et al.,
languagemodels(LLMs)urgentlyneedreliableuncertainty 2019; Abdar et al., 2021; Abdullah et al., 2022; Lopez
quantification (UQ); see Figure 1. The problem is that etal.,2023;Bandetal.,2021),single-cellbiology(Way&
exactBayesianinferenceistypicallytoocomputationally Greene,2018),drugdiscovery(Gruveretal.,2021;Klarner
expensive. et al.,2023), agriculture (Herna´ndez &Lo´pez, 2020), as-
trophysics(Soboczenskietal.,2018;Ferreiraetal.,2020),
nanotechnology(Leithereretal.,2021),physics(Cranmer
Position. This paper posits that the advancement
et al., 2021), climate science (Vandal et al., 2018; Luo
ofBDLcanovercomemanyofthechallengesthat
et al., 2022), smart electricity grids (Yang et al., 2019),
deeplearningfacesnowadays. Notably,BDLmeth-
wearables (Manogaran et al., 2019; Zhou et al., 2020),
odscanproveinstrumentalinmeetingtheneedsof
robotics (Shi et al., 2021; Mur-Labadia et al., 2023), and
the 21st century for more mature AI systems and
autonomousdriving(McAllisteretal.,2017). Thissection
safety-criticaldecision-makingalgorithmsthatcan
outlinesthestrengthsofBDLtomotivatetheadvancement
reliablyassessuncertaintiesandincorporateexisting
ofBDLintheeraoflarge-scaleAI.
knowledge. Forexample, BDLmethodscanmiti-
gaterisksarisingfromoverlyconfidentyetincorrect
2.1.UncertaintyQuantification
predictions made by LLMs (see Figure 1). The
major impediment to the development of broadly UQinBDLimprovesthereliabilityofthedecision-making
adoptableBDLmethodsisscalability,yetthispaper processandisvaluablewhenthemodelencountersambigu-
proposesresearchdirectionsthatpromisetomake ousorout-of-distributioninputs(Tranetal.,2022b).Insuch
BDLmoreamenabletocontemporarydeeplearning. instances,themodelcansignalitslackofconfidenceinthe
predictions through the associated probability instead of
Paperstructure. Section2explainswhyBDLmattersby providingunderperformingpointestimates. Theimportance
highlightingthestrengthsofBDL.Section3criticallyre- ofpredictiveUQisespeciallyemphasizedinthecontextof
flectsonthechallengesthatcurrentBDLmethodsface. Sec- AI-informeddecision-making,suchasinhealthcare(Band
tion4identifiesresearchdirectionsforthedevelopmentof etal.,2021;Lopezetal.,2023). Insafety-criticaldomains,
scalableBDLmethodsthatcanovercomethesechallenges
reliableUQcanbeusedtodeploymodelsmoresafelyby
2PositionPaper:BayesianDeepLearningintheAgeofLarge-ScaleAI
deferring to a human expert whenever an AI system has 2.3.AdaptabilitytoNewandEvolvingDomains
high uncertainty about its prediction (Tran et al., 2022b;
Bydynamicallyupdatingpriorbeliefsinresponsetonew
Rudner et al., 2022a; 2023a). This capability is also per-
evidence,BDLallowsselectiveretentionofvaluableinfor-
tinent to address current challenges in language models,
mationfromprevioustaskswhileadaptingtonewones,thus
where uncertainty quantification can be used to mitigate
improvingknowledgetransferacrossdiversedomainsand
risksassociatedwithoverlyconfidentbutincorrectmodel
tasks (Rothfuss et al., 2021; 2022; Rudner et al., 2023b).
predictions(Kadavathetal.,2022);seeFigure1foranex-
This is crucial for developing AI systems that can adapt
ample. Similarly,BDLcanbeusefulformodernchallenges,
tonewsituationsortemporallyevolvingdomains(Nguyen
such as hallucinations (Ji et al., 2023) and adversarial at-
et al., 2018; Rudner et al., 2022b), as in the case of con-
tacks(Andriushchenko,2023)inLLMs,orjailbreakingin
tinual or lifelong learning. The contrast with traditional
text-to-imagemodels(Yangetal.,2023b).
approaches in large-scale machine learning becomes ap-
Inscientificdomains,includingbutnotlimitedtochemistry parent, as these static models assume that the underlying
andmaterialsciences,whereexperimentaldatacollection patterns in the data remain constant over time and strug-
isresource-intensiveorconstrained,parameterspacesare gle with the constant influx of new data and changes in
high-dimensional,andmodelsareinherentlycomplex,BDL underlyingpatterns.
excelsbyprovidingrobustestimatesofuncertainty. Thisat-
tributeisparticularlycrucialforguidingdecisionsininverse 2.4.ModelMisspecificationandInterpretability
design problems, optimizing resource utilization through
Bayesianmodelaveraging(BMA)acknowledgesandquan-
Bayesianexperimentaldesign,optimization,andmodelse-
tifiesuncertaintyinthechoiceofmodelstructure. Instead
lection(Lietal.,2023;Rainforthetal.,2023;Bamleretal.,
ofrelyingonasinglefixedmodel, BMAconsidersadis-
2020;Immeretal.,2021a;2023).
tribution of possible models (Hoeting et al., 1998; 1999;
Wasserman,2000). Byincorporatingmodelpriorsandin-
2.2.DataEfficiency
ferring model posteriors, BDL allows BMA to calibrate
Unlikemanymachinelearningapproachesthatmayrequire uncertaintyovernetworkarchitectures(Hubin&Storvik,
largedatasetstogeneralizeeffectively,BDLleveragesprior 2019;Skaaret-Lundetal.,2023). Byaveragingpredictions
knowledgeandupdatesbeliefsasnewdatabecomeavail- overdifferentmodelpossibilities,BMAattenuatestheim-
able. ThisallowsBDLtoextractmeaningfulinformation pactofmodelmisspecification,offeringarobustframework
fromsmalldatasets,makingitmoreefficientinscenarios thataccountsforuncertaintyinbothparametervaluesand
where collecting large amounts of data is challenging or model structures, ultimately leading to more reliable and
costly(Finzietal.,2021;Immeretal.,2022;Shwartz-Ziv interpretablepredictions(Hubinetal.,2021;Wangetal.,
etal.,2022;Schwo¨beletal.,2022;vanderOuderaaetal., 2023a;Bouchiatetal.,2023).
2023). Inaddition,theregularizationeffectintroducedby
Theinterpretationofparametersandstructuresmayseem
the probabilistic nature of its Bayesian approach is ben-
lesscrucialinBDL,whereoverparameterizedneuralnet-
eficial in preventing overfitting and contributing to better
worksserveasfunctionalapproximationstounknowndata-
generalizationfromfewersamples(Rothfussetal.,2022;
generatingprocesses.However,researchisrequiredtoestab-
Sharma et al., 2023). BDL’s uncertainty modeling helps
lishreproducibleandinterpretableBayesianinferencesfrom
resisttheinfluenceofoutliers,makingitwell-suitedforreal-
deep neural networks (DNNs), especially in applications
worldscenarioswithnoisyorout-of-distributiondata. This
whereblack-boxpredictionisnottheprimaryobjective,par-
also makes it attractive for foundation model fine-tuning,
ticularlyinscientificcontexts(Ru¨gamer,2023;Wangetal.,
wheredataarecommonlysmallandsparse,anduncertainty
2023a;Doldetal.,2024). BMA-centricresearchinBDL
isimportant.
canbevaluableinthesedirections.
Furthermore,theUQcapabilitiesofBDLallowforanin-
formed selection of data points for labeling. Using prior
3.CurrentChallenges
knowledgeandcontinuallyupdatingbeliefsasnewinforma-
tionarrives,BDLoptimizestheiterativeprocessofactive One of the challenges in BDL is the computational cost
learning, strategically choosing the most informative in- incurred(Izmailovetal.,2021b). DespitetheBDLadvan-
stances for labeling to enhance model performance (Gal tagesoutlinedinSection2, withintherealmofBayesian
et al., 2017). This capability may be particularly ad- approaches,GaussianProcesses(GPs)remainthepreferred
vantageous for addressing the current challenge of effi- choiceincomputationallydemandingscenariossuchassci-
cientlyselectingdemonstrationsinin-contextlearningsce- entific discovery (Tom et al., 2023; Griffiths et al., 2023;
narios(Margatinaetal.,2023)orfine-tuningwithhuman Strieth-Kalthoff et al., 2023). Showing that BDL works
feedback(Casperetal.,2023). cheaply,oratleastwithpracticalefficiencyundermodern
3PositionPaper:BayesianDeepLearningintheAgeofLarge-ScaleAI
p(θ ) estimatescurvaturefromthetrajectoryofSGD,ratherthan
|D
MAP theHessianatasinglepoint. Byproducingadeterministic
Laplace probabilitymeasurefromstochasticgradients,itbridgesthe
Variational
gapbetweendeterministicandstochasticprocedures.
MCMC
Despite their analytic strengths, these approximations re-
mainfundamentallylocal,capturingonlyasinglemodeof
themultimodalBayesianneuralnetwork(BNN)posterior.
Θ
Arguably,theirmostfundamentalflawisthattheirposterior
isdependentontheparametrizationoftheBNN(MacKay,
Figure2. Different flavors of BDL methods for approximating
1998) and thus inconsistent with some of the most basic
theposteriorp(θ|D)ontheparameterspaceΘ.WhileLaplace
propertiesofprobabilitymeasures(Kristiadietal.,2023).
andGaussian-basedvariationalapproachesbothyieldGaussian
Furthermore, thelocalposteriorgeometrymaybepoorly
approximations,theygenerallycapturedifferentlocalmodesofthe
approximatedbyaGaussiandistribution,whichcanleadto
posterior.EnsemblemethodsuseMAPestimatesastheirsamples.
underconfidencewhensamplingfromtheLaplaceapproxi-
mation(Lawrence,2001),aproblemthatcanbemitigated
settingsintherealworld,isoneofthemostimportantprob-
bylinearization(Immeretal.,2021b).
lems that remains to be addressed. This section aims to
explore the complexities of BDL, highlighting two main
3.2.Ensembles
challengesthatcontributetoitsdifficultiesindeployment:
posteriorcomputation(Figure2)andpriorspecification. It Deep ensembling involves the retraining of a neural net-
isalsoexploredhowscalabilityarisesasamainchallengein work(NN)withvariousinitializations,followedbyaverag-
BDL.Thesectionconcludeswithdifficultiesintheadoption ing the resulting models. It is effective in approximating
ofBDLinfoundationmodels.Chellengesrelatedtothelack the posterior predictive distribution (Wilson & Izmailov,
ofconvergenceandperformancemetricsandbenchmarks 2020). Recenttheoreticaladvancementshaveestablished
forBDLarediscussedinAppendixA. preciseconnectionsbetweenensemblesandBayesianmeth-
ods(Cioseketal.,2020;Heetal.,2020;Wildetal.,2023).
3.1.LaplaceandVariationalApproximations
AnopenquestioninBDLiswhetheronecandevelopscal-
Laplace and variational approximations use geometric or ableBayesianinferencemethodsthatoutperformdeepen-
differentialinformationabouttheempiricallosstoconstruct sembles.Izmailovetal.(2021b)haveshownthatHamilto-
closed-form(usuallyGaussian)probabilitymeasurestoap- nianMonteCarlo(HMC)oftenoutperformsdeepensem-
proximatetheposterior.Despitetheirsimplenatureandlong bles,butwithsignificantadditionalcomputationaloverhead.
history(MacKay,1992),theyoftenshowcompetitivepre-
When dealing with large and computationally expensive
dictiveperformance(Daxbergeretal.,2021b;Rudneretal.,
deeplearningmodels, suchasLLMs, theuseofdeepen-
2022a;Antoranetal.,2023;Rudneretal.,2023a). Moreim-
sembles may encounter significant challenges due to the
portantly,theirclosed-formnature,leveragingautomatically
associated training and execution costs. Therefore, these
computeddifferentialquantitiesandthefoundationsofnu-
largemodelsmaymotivateresearchintomoreefficientar-
mericallinearalgebra,allowstheoreticalanalysis(Kristiadi
chitecturesandinferenceparadigms,suchasposteriordistil-
et al., 2020) and analytical functionality, such as calibra-
lationorrepulsiveensembles(D’Angelo&Fortuin,2021),
tion(Kristiadietal.,2021b;a)andmarginalization(Khan
toimproveuncertaintycalibrationandsparsermodeluse.
etal.,2019;Immeretal.,2021a;b),whicharelesselegant
withstochasticapproaches. Laplace-approximatedneural
3.3.PosteriorSamplingAlgorithms
networks(Ritteretal.,2018)areparticularlytemptingbe-
causetheyaddnocomputationalcostduringtraining,and WithintherealmofMarkovchainMonteCarlo(MCMC;
require limited computational overhead (comparable to a Brooks et al., 2011) for BDL, stochastic gradient
fewepochs)forpost-hocUQ.Moreover,recentvariational MCMC (SG-MCMC; Nemeth & Fearnhead, 2021) al-
objectives(Alemi&Poole,2023)providealternativemeans gorithms, such as stochastic gradient Langevin dynam-
ofpredictionthatavoidinternalmarginalization. ics (SG-LD; Welling & Teh, 2011) and stochastic gradi-
ent HMC (SG-HMC; Chen et al., 2014), have emerged
Alternatively,SWAG(Maddoxetal.,2019)isanotherscal-
aswidelyadoptedtools. Despiteofferingimprovedposte-
able approximation that creates a Gaussian approximate
riorapproximations,SG-MCMCalgorithmsexhibitslower
posterior from stochastic gradient descent (SGD) itera-
convergencecomparedtoSGD(Robbins,1951). Thisde-
tions (Mandt et al., 2017) with a modified learning rate
celerationresultsfromtheincreasediterationsrequiredby
schedule. SimilarlytotheLaplaceapproximation,itdoes
SG-MCMCtothoroughlyexploretheposteriordistribution
notcostmuchmorethanstandardtraining.However,SWAG
4PositionPaper:BayesianDeepLearningintheAgeofLarge-ScaleAI
beyondlocatingthemode. priorsmaybeconstructedthroughself-supervisinglearn-
ing(Shwartz-Zivetal.,2022;Sharmaetal.,2023).
Furthermore,SG-MCMCisstillconsideredexpensivefor
deep learning applications. A step forward in this re-
3.5.Scalability
gard would be to learn from the machine learning and
systems community how to make Monte Carlo faster us- ThepresenceofsymmetriesintheparameterspaceofNNs
ing contemporary hardware (Zhang et al., 2022; Wang yieldscomputationalredundancies(Wieseetal.,2023). Ad-
et al., 2023b). Algorithms such as Stein variational gra- dressing the complexity and identifiability issues arising
dientdescent(SVGD;Liu&Wang,2016)occupyamiddle from these symmetries in the context of BDL can signif-
groundbetweenoptimizationandsampling,byemploying icantlyimpactscalability. Proposedsolutionsinvolvethe
optimization-typeupdatesbutwithasetofinteractingpar- incorporationofsymmetry-basedconstraintsinBDLinfer-
ticles. While recent advances show promising results in encemethods(Senetal.,2024)orthedesignofsymmetry-
BNNsettings(D’Angeloetal.,2021;D’Angelo&Fortuin, awarepriors(Atzenietal.,2023). However,removingsym-
2021; Pielok et al., 2022), these methods often perform metries may not be an optimal strategy, since part of the
poorlyinhigh-dimensionalproblems. Alternatively,conver- successofdeeplearningcanbeattributedtotheoverparam-
genceratesandposteriorexplorationcanbeimprovedwith eterizationofNNs,allowingrapidexplorationofnumerous
cyclicalstep-sizeschedules(Zhangetal.,2019). hypotheses during training or having other positive ‘side
effects’suchasinducedsparsity(Kolbetal.,2023).
However,despitetheseadvances,thepersistentchallenges
posedbythehighlymultimodalandhigh-dimensionalna- ContrarytothemisconceptionthatBNNsinherentlysuffer
tureofBDLposteriorscontinuetoimpedetheaccuratechar- fromlimitationsinspeedandmemoryefficiencycompared
acterizationofthefullposteriordistributionviasampling. todeterministicNNs,recentadvanceschallengethisnotion.
There is a need for SG-MCMC algorithms that not only In particular, research by Ritter et al. (2021) shows that
matchthespeedofSGD,asdeployedforoptimizationin BNNs can achieve up to four times greater memory effi-
typicaldeeplearningsettings,butalsodeliverhigh-quality ciencythantheirdeterministiccounterpartsintermsofthe
approximationsoftheposteriortoensurepracticalutility. numberofparameters. Furthermore,strategiessuchasrecy-
clingthestandardtrainingtrajectorytoconstructapproxi-
3.4.PriorSpecification mateposteriors,asproposedbyMaddoxetal.(2019),incur
negligibleadditionalcomputationcosts. Hybridmodelsthat
The prior over parameters induces a prior over functions,
combineNNswithGPs,suchasdeepkernellearning(DKL;
anditistheprioroverfunctionsthatmattersforgeneraliza-
Wilson et al., 2016), are also marginally slower or more
tion(Wilson&Izmailov,2020). Fortunately,thestructure
memory-consumingthandeterministicNNs.
inneuralnetworkarchitecturesalreadyendowsthisprior
overfunctionswithmanydesirableproperties,suchastrans- Although UQ is of significant importance across various
lation equivariance if we use a CNN architecture. At the domains,itshouldnotcomeatthecostofreducedpredic-
sametime,definingpriorsovertheparametersishindered tiveperformance. BDLmuststrikeabalancebyensuring
bythecomplexityandunintelligibilityofhigh-dimensional that the computational cost of UQ matches that of point
spacesinBDL.Thus,oneaimistoconstructinformative estimation. Otherwise,investingcomputationalresourcesto
properpriorsonneuralnetworkweightsthatarecomputa- improvethepredictiveperformanceofdeeplearningmod-
tionallyefficientandfavorsolutionswithdesirablemodel els might be a more prudent option. Some may contend
properties (Vladimirova et al., 2019; Fortuin et al., 2022; thatensemblesarelessaffectedbythisconcernduetotheir
Rudneretal.,2023a),suchaspriorsthatfavormodelswith embarrassinglyparallelnature. However,inanerawhere
reliableuncertaintyestimates(Rudneretal.,2023b),ahigh evenindustryleadersencounterlimitationsingraphicspro-
degreeoffairness(Rudneretal.,2024),generalizationunder cessingunit(GPU)resourcesrequiredtotrainasinglelarge
covariateshifts(Klarneretal.,2023),equivariance(Finzi deeplearningmodel,relyingsolelyonparallelismbecomes
etal.,2021),orahighlevelofsparsity(Ghoshetal.,2018; inadequate. Simultaneouslyachievingtimeefficiency,mem-
Polson&Rocˇkova´,2018;Hubin&Storvik,2019). oryefficiency,andhighmodelutility(intermsofpredictive
performanceanduncertaintycalibration)remainsthegrand
Recent research has developed priors in function space
challenge; this is the holy grail of approximate Bayesian
ratherthaninweightspace(Tranetal.,2022a;Rudneretal.,
inference.
2022b;Qiuetal.,2023). Function-spacepriorsalsoraise
someissues,suchasill-definedvariationalobjectives(Burt
3.6.FoundationModels
et al., 2020; Rudner et al., 2022a) or, in some cases, the
needtoperformcomputationallycostlyGPapproximations. Deeplearningisinthemidstofaparadigmshiftintothe
There are alternative ways to specify function-space pri- ‘foundationmodel’era,characterizedbymodelswithbil-
orsbeyondGPs. Forexample,informativefunction-space lions,ratherthanmillions,ofparameters,withapredomi-
5PositionPaper:BayesianDeepLearningintheAgeofLarge-ScaleAI
nantfocusonlanguageratherthanvision. BDLapproaches amappingfromasimpler(usuallyGaussian)distribution
toLLMsarerelativelyunexplored,bothintermsofmethods toacomplexdatadistribution(forexample,adistribution
andapplications. Whilestate-of-the-artapproximateinfer- of images). So, one could plausibly use an NN either to
encealgorithmscaneffectivelyhandlemodelswithmillions learn a mapping between the BDL posterior and a Gaus-
ofparameters,onlyalimitednumberofworkshaveconsid- sian distribution or to use an NN in an MCMC proposal
eredBayesianapproachestoLLMs(Xieetal.,2021;Cohen, mechanism.
2022;Margatinaetal.,2022;Yangetal.,2024).
Generally, instead of just focusing on local information
As discussed in Section 2, BDL emerges as a solution to about the posterior, there is a need for SG-MCMC algo-
address limitations in foundation models, particularly in rithmsthatareabletomoverapidlyacrossisolatedmodes,
scenarios where data availability is limited. In contexts forinstance,usingnormalizingflows.Sinceonemaynotex-
involving personalized data (Moor et al., 2023) or causal pecttoaccuratelyapproximateahigh-dimensionalposterior
inferenceapplications(Zhangetal.,2023),suchasindivid- withrespecttoalltheBNNparameters,novelperformance
ualtreatmenteffectestimation,wheresmalldatasetsprevail, metricsmaytargetlower-dimensionalfunctionalsofinterest,
thecapacityofBDLforuncertaintyestimationalignsseam- includingUQasakeypiece.
lessly. The fine-tuning settings of foundation models in
Oneapproachistoincorporateappropriateconstraintsto
smalldatascenariosisanotherexample.
attainidentifiability,forinstance,bydoinginferenceonthe
Thus,foundationmodelsrepresentavaluablefrontierfor latentBNNstructure(Gu&Dunson,2023). Further,one
BDL research, particularly around evaluation and appli- mayconsiderdecouplingapproaches,whichusetheBNN
cations. What applications of LLMs or transformers are as a black box to fit the data-generating model and then
going to benefit from Bayesian inference tools, such as chooseappropriatelossfunctionstoconductinferenceina
marginalization and priors? More generally, more mean- secondstage.
ingfulapplicationsareneededtoconvincinglydemonstrate
AnotherpromisingapproachisrunningSG-MCMCalgo-
thatBDLprinciplesgobeyondproof-of-concept. Therep-
rithms in subspaces of the parameter space, for example,
resentationofepistemicuncertaintywillpossiblybemost
linearorsparsesubspaces(Izmailovetal.,2020;Lietal.,
valuablewhenLLMsorotherlarge-scaleNNsaredeployed
2024),furtherenablingtheformulationofuncertaintystate-
insettingsoutsideoftherealmoftheirtrainingdata. For
mentsfortargetedsubnetworks(Doldetal.,2024). Inthe
example,Bayesianapproachescanbedevelopedandtested
future,SG-MCMCoperatingonQLoRA(Dettmersetal.,
inthetimeseriescontextofapplyingLLMsindownstream
2023) or non-linear subspaces may be constructed. Be-
forecastingtasks(Gruveretal.,2023).
sidestreatingsubspacesdeterministically,posteriordepen-
dencies between subspaces can be broken systematically,
4.ProposedFutureDirections
leadingtonovelhybridsamplersthatcombinestructured
variational inference with MCMC (Alexos et al., 2022)
Thissection,drivenbythechallengesdescribedinSection3,
toachievecompute-accuracytrade-offs. Subsamplingfor
presentsongoingresearchinitiativesdedicatedtoaddress-
BDLcanbecombinedwithreasoningabouttransferlearn-
ing these challenges, particularly focusing on scalability.
ing(Kirichenkoetal.,2023).
Subsection 4.7 presents more recent or less widely stud-
iedBayesianresearchapproachestodeeplearning. Some
4.2.HybridBayesianApproaches
topicaldevelopmentsinBDLarediscussedinAppendixC.
Inthefuture,practicalBDLapproachesmaycaptureuncer-
4.1.PosteriorSamplingAlgorithms taintyoveralimitedpartofthemodel,whileotherpartsmay
beestimatedefficientlyusingpointestimation. So,onemay
Thereisaneedfornewclassesofposteriorsamplingalgo-
considerhybridapproachesthatcombineBayesianmethods
rithmsthatperformbetterondeepneuralnetworks(DNNs).
withtheefficiencyofdeterministicdeeplearning.
Thesealgorithmsshouldaimtoenhanceefficiency,reduce
computationaloverhead,andenablemoreeffectiveexplo- Thiscouldinvolvedevelopingmethodsthatselectivelyap-
rationofhigh-dimensionalparameterspaces. plyBayesianapproachesincriticalareasofthemodelwhere
capturinguncertaintywillbemoreusefulandcheaper,while
SG-MCMCwithtemperedposteriorsmaypotentiallyover-
maintainingadeterministicapproachforotherpartsofthe
cometheissueofsamplingfrommultiplemodes.Thiscould
model(Daxbergeretal.,2021b). Thelast-layerLaplaceap-
beachievedbydevelopingnewsamplingapproachesthat
proximationisanexampleofthis(Daxbergeretal.,2021a).
canbebasedonideasfromoptimaltransporttheory(Villani,
Such hybrid approaches are a promising area for future
2021),score-baseddiffusionmodels(Songetal.,2020),and
research.
ordinary differential equation (ODE) approaches such as
flowmatching(Lipmanetal.,2022),whichuseNNstolearn CombinationsofdeeplearningmethodsandGPshavetra-
6PositionPaper:BayesianDeepLearningintheAgeofLarge-ScaleAI
ditionally been limited by the lack of scalability of GPs. least,itdoesnotobviouslycorrespondtoalikelihoodina
However, recentadvancesinscalingupGPinferenceare knownmodel). Additionally,inBayesianinference,there
promisingformakingthesehybridmodelsmorewidespread. arephenomenasuchasthe‘coldposterioreffect’(Aitchi-
DKL(Wilsonetal.,2016)isoneexampleofsuchahybrid son,2021;Wenzeletal.,2020),inwhichBDLappearsto
model. TheDKLscalabilityfrontiermaybefurtherpushed attainmorecompetitivepredictiveperformancebytaking
byexploitingadvancesinGPscalability. the posterior to a power greater than one, thereby shrink-
ing the posterior. In particular, the patterns exploited by
There exists a prolific literature on connecting BDL and
semi-supervisedlearningarisefromdatacuration(Ganev&
deepGaussianprocesses(DGPs;Neal,1996). Thislineof
Aitchison,2023). Ifsemi-supervisedlearningisperformed
work (de G. Matthews et al., 2018; Agrawal et al., 2020)
onuncurateddata,anyimprovementsdisappear. Thiscasts
involvesneuralnetworkGPs,whichareGPsthatariseas
doubtontheapplicabilityofsemi-supervisedlearningon
infinite-widthlimitsofNNs.InsightsintothetheoryofBDL
real-worlduncurateddatasets.Thecoldposteriorresultscan
maycomefromtheanalysisofthisconnectionbetweenNNs
alsobeexplainedbyunderconfidentaleatoricuncertainty
andGPs.
representation(Kapooretal.,2022).
4.3.DeepKernelProcessesandMachines Self-supervisedlearningisanalternativetosemi-supervised
learning. Self-supervised learning is based on objectives
Deepkernelprocesses(DKPs)constituteapromisingfam-
suchasmutualinformationbetweenlatentrepresentations
ilyofdeepnon-parametricapproachestoBDL(Aitchison
oftwoaugmentationsofthesameunderlyingimage. From
etal.,2021;Ober&Aitchison,2021;Oberetal.,2023). A
a Bayesian perspective, these objectives appear to be ad
DKPisadeepGP,inwhichonetreatsthekernels,rather
hoc, as they do not correspond to any likelihood. How-
than the features, as random variables. Remarkably, it is
ever,itispossibletoformulatearigorouslikelihoodinthe
possibletowritedownthepriorandperforminferenceen-
form of a recognition-parameterized model (Aitchison &
tirely for kernels, without ever needing DGP features or
Ganev,2023). Thisprovidesinsightintotheworkingsof
BNNweights(Aitchisonetal.,2021). Thus,DKPsresolve
self-supervised learning and how to generalize it to new
a pernicious problem with BDL: the highly multimodal
settings,suchasviewingitasawaytolearnBayesianpri-
posteriors caused by permutation symmetries. It is very
ors(Shwartz-Zivetal.,2022;Sharmaetal.,2023).
difficult toaccurately approximatethese multimodal pos-
teriorswithsimplifiedparametricfamilies,forinstance,as
4.5.MixedPrecisionandTensorComputations
used in Laplace or variational inference. In contrast, the
DKPposteriorinpracticeseemstobeunimodal(Yangetal., Thesuccessofdeeplearningiscloselytiedtoitscoupling
2023a). withmoderncomputingandspecializedhardware,leverag-
ingtechnologieslikeGPUs. Recentinvestigationswithin
Deepkernelmachines(DKMs;Milsometal.,2023;Yang
deeplearningontheimpactofmixedprecisionpointtoa
etal.,2023a)gofurther,bytakingtheinfinite-widthlimit
roleforBayes, particularlyprobabilisticnumerics(Oates
ofaDKP.Usuallysuchaninfinite-widthlimitwouldelim-
& Sullivan, 2019), in making more efficient use of com-
inate representation learning. However, DKMs carefully
putation. Mixedprecisionintroducesuncertaintyintothe
temperthelikelihoodinordertoretainrepresentationlearn-
internalcomputationsofamodel,whichBayescaneffec-
ing,andaretherebyabletoattainstate-of-the-artpredictive
tivelypropagatetodownstreampredictions. Furthermore,
performance(Milsometal.,2023),whiletheirtheoretical
mixedprecisionrequiresmakingdecisionsaboutwhichpre-
implications are profound for BDL. DKMs offer key in-
cisiontouse,whereBayescanensurethatthesedecisions
sightsintowhatwereallymeanby‘inferenceinfunction
areoptimalandsensitivetotherelationsbetweennumerical
space’andhowitrelatestorepresentationlearning. Specif-
tasks. Drawinginspirationfromspecializedhardware,such
ically,thekernelslearnedateverylayerinaDKMdefine
as tensor processing units, there is potential for a similar
a ‘function space’ at every layer. In fact, in a DKM, the
trajectory in BDL to address scalability concerns (Mans-
true posterior over features is multivariate Gaussian with
inghka,2009). Thissuggeststhatthecreationofdedicated
covariance given by the learned kernel (Aitchison et al.,
hardwareforBDLhasthepotentialtosparkareevaluation
2021). Representation learning occurs as these function
ofinferencestrategies.
spacesateverylayeraremodulatedbytrainingtofocuson
thefeaturesthatmatterforpredictiveperformance. In a parallel vein, accelerating software development is
crucialtoencouragingdeeplearningpractitionerstoadopt
4.4.Semi-SupervisedandSelf-SupervisedLearning Bayesian methods. There is a demand for user-friendly
softwarethatfacilitatestheintegrationofBDLintovarious
FromaBayesianperspective,oneofthesurprisesinmod-
projects. The goal is to make BDL usage competitive in
erndeeplearninghasbeenthesuccessofsemi-supervised
termsofhumaneffortcomparedtostandarddeeplearning
learning,wheretheobjectiveisseeminglyarbitrary(orat
7PositionPaper:BayesianDeepLearningintheAgeofLarge-ScaleAI
practices. For details on BDL software efforts, see Ap- makingdeeplearningbothmorepowerfulandBayesian. As
pendixB. oneexample,sincedeeptrainingisnowregularlyI/O-bound
forlargemodels,activemanagementofdataloading,during
4.6.CompressionStrategies training and UQ, is of increasing interest. Methods that
quantifyandcontroltheinformationprovidedbyindividual
To decrease the computational cost of BDL models, for
computations,basedontheireffectontheBDLposterior,
both memory efficiency and computational speed, com-
are showing promise as a formalism for algorithmic data
pression strategies are being explored. An approach in-
processingindeeptraining(Tatzeletal.,2023),usingprob-
volvesusingsparsity-inducingpriorstoprunelargeparts
abilisticnumericallinearalgebra(Wengeretal.,2022)to
of BNNs (Louizos et al., 2017). Alternatively, the prior
selectsparseinformative‘views’onthedata.
canserveasanentropymodel,enablingthecompressionof
BNNweights(Yangetal.,2023c). Methodssuchasrela- Singularlearningtheory. Singularlearningtheory(SLT;
tiveentropycodingandvariationalBayesianquantization, Watanabe,2009)investigatestherelationbetweenBayesian
wherethequantizationgridisdynamicallyrefined,provide losses, such as approximations of the marginal log-
efficientBNNcompression(Yangetal.,2020). Thesenovel likelihood, andneuralnetworklossfunctions, usingprin-
toolscouldalsobeusedtodynamicallydecodeaBayesian ciplesfromnon-equilibriumstatisticalmechanics. Recent
ensembleattesttimetovariouslevelsofprecisionoren- researchhasdrawnconnectionsbetweenBayesianmethods
semblesize,resultinginprecision-computetrade-offs. andSLT(Wei&Lau,2023).
Furthermore,inthecontextofcompressingNNweights,a Conformalprediction. ForUQ,alternativessuchascon-
viableapproachinvolvesobtainingtheposteriordistribution formalpredictionhaveemergedascompetitorstoBayesian
based on observed data and encoding a sample into a bit methodsandresultinwell-calibrateduncertainties(Vovk
sequence to send to a receiver (Havasi et al., 2019). The etal.,2005). Deeplearningmodelscanbeusedtodevelop
receivercanthenextracttheposteriorsampleandusethe conformalpredictionalgorithms(Meisteretal.,2023)and,
correspondingweightstomakepredictions. Inpractice,ap- conversely, conformalpredictionmethodscanbeusedto
proximationsareneededtoobtaintheposterior,encodethe quantifyorcalibrateuncertaintyindeeplearningmodels.
sample,andusethecorrespondingweightstomakepredic- ABayesian approachtoconformal predictionhas started
tions. Despitetheneedforapproximationsintheprocess, toemerge(Hobbhahnetal.,2022;Murphy,2023),promis-
thismethodyieldscommendabletrade-offsbetweencom- ingasynergisticapproachthatcombinesthestrengthsof
pressioncostandpredictivequalitycomparedtoalternatives Bayesianreasoningwiththewell-calibratedUQofferedby
centeredondeterministicweightcompression. conformalprediction.
LLMs as distributions. LLMs may be used flexibly as
4.7.OtherFutureDirections
distribution objects in arbitrarily complex programs and
workflows. BytakingaBayesianstance,severalquestions
Bayesiantransferandcontinuallearning. Thetransfer
emergeforexploration. WhenmultipleLLMsinteract,how
learningparadigmisquicklybecomingastandardwayto
doesoneperformjointinference? Whatisaneffectiveap-
deploydeeplearningmodels. AsnotedinSection3,BDL
proach to marginalize over latent variables generated by
isoptimizedfortransferlearning. Thefocusisnotsolelyon
LLMs,facilitatingjointlearningoversuchlatentspaces? Is
transferringaninitializationasintraditionaldeeplearning;
itpossibletoadopttoolsfromcomputationalstatisticsor
instead,knowledgeofthesourcetaskmayinformtheshapes
approximateinferencetoperformvariousformsofreason-
andlocationsofoptimaondownstreamtasks(Rudneretal.,
ingwithLLMs? Andarethereinnovativewaystosynergize
2022b;2023a). Self-supervisedlearningcanalsobeusedto
smallandlargeLLMstoamortizeinferencesjustintime?
createinformativeself-supervisedpriorsfortransferlearn-
ing(Shwartz-Zivetal.,2022;Sharmaetal.,2023). Lever- Meta-models. An intriguing prospect arises when con-
agingitsefficiencyinlearningundertemporally-changing templatingwhetherBDLwillparallelthetrajectoryoflan-
datadistributionsthroughposteriorupdates,currentefforts guage models. Could one envision the development of a
in the continual learning context explore approaches that Bayesianmeta-modelwithintheBDLframework(Krueger
integratenewinformationeitherassumingacontinuousrate etal.,2017)? Thismeta-model,akintolanguagemodels,
ofchange(Nguyenetal.,2018;Changetal.,2022)orincor- maybefine-tunedtomultipletasks,demonstratingcompeti-
poratingpriorsforchangepointdetection(Lietal.,2021). tivepredictiveperformanceacrossthem,thusgeneralizing
approaches in amortized inference (Garnelo et al., 2018;
Probabilistic numerics. Probabilistic numerics (Hennig
Gordonetal.,2019;Mu¨lleretal.,2021).
etal.,2022)isthestudyofnumericalalgorithmsasBayesian
decision-makers. As numerical algorithms, such as opti- Sequentialdecisionbenchmarks. Standardimage-based
mizationandlinearalgebra,areclearlycentraltodeeplearn- benchmarksfocusexclusivelyonstate-of-the-artpredictive
ing,probabilisticnumericsoffersinterestingprospectsfor
8PositionPaper:BayesianDeepLearningintheAgeofLarge-ScaleAI
performance,wherenon-Bayesiandeeplearningalgorithms References
typicallyhaveanadvantageoverBDL.Toquantifypredic-
Abdar, M., Samami, M., Mahmoodabad, S.D., Doan, T.,
tiveuncertainty,itisencouragedtoshiftattentiontomore
Mazoure, B., Hashemifesharaki, R., Liu, L., Khosravi,
thorough simulation studies or scientific applications fo-
A.,Acharya,U.R.,Makarenkov,V.,etal. Uncertainty
cusedonsequentiallearninganddecision-making,suchas
quantification in skin cancer classification using three-
experimentaldesign, Bayesianoptimization, activelearn-
waydecision-basedBayesiandeeplearning. Computers
ing,orbandits. Byprioritizingsequentialproblemsinsuch
inBiologyandMedicine,135:104418,2021.
contexts,researchersandpractitionerscangaininsightsinto
howwellamodelgeneralizestonewandunseendata,how
Abdullah, A. A., Hassan, M. M., and Mustafa, Y. T. A
robustitisunderuncertainconditions,andhoweffectively
reviewonBayesiandeeplearninginhealthcare: Appli-
itsuncertaintyestimatescanbeutilizedbydecisionmakers
cationsandchallenges. IEEEAccess,10:36538–36562,
inreal-worldscenarios.
2022.
5.FinalRemarks Agrawal,D.,Papamarkou,T.,andHinkle,J. Wideneural
networkswithbottlenecksaredeepGaussianprocesses.
Thispaperhasshownthatmoderndeeplearningfacesava- Journal of Machine Learning Research, 21(175):1–66,
rietyofpersistentethical,privacy,andsafetyissues,particu-
2020.
larlywhenviewedinthecontextofdifferenttypesofdata,
tasks, andperformancemetrics. However, manyofthese Aitchison,L. Astatisticaltheoryofcoldposteriorsindeep
issuescanbeovercomewithintheframeworkofBayesian neuralnetworks. InternationalConferenceonLearning
deeplearning,buildingonfoundationalprinciplesthathave Representations,2021.
survivedtwoandahalfcenturiesofscientificandmachine
learningevolution. Whileanumberoftechnicalchallenges Aitchison,L.andGanev,S.InfoNCEisvariationalinference
remain,thereisaclearpathforwardthatcombinescreativity in a recognition parameterised model. arXiv preprint
andpragmatismtodevelopBDLapproachesthatmatchthe arXiv:2107.02495,2023.
data,hardware,andnumericaladvancesofthetwenty-first
century,especiallyinthecontextoflarge-scalefoundation Aitchison,L.,Yang,A.,andOber,S.W. Deepkernelpro-
models. Inafuturewheredeeplearningmodelsseamlessly cesses.InInternationalConferenceonMachineLearning,
integrateintodecision-makingsystems,BDLthusemerges 2021.
asacrucialbuildingblockformorematureAI,addingan
extralayerofreliability,safety,andtrust. Alemi, A. A. and Poole, B. Variational prediction. In
FifthSymposiumonAdvancesinApproximateBayesian
Inference,2023.
Acknowledgements
ThisworkwassupportedinpartbyfundingfromtheEric Alexos,A.,Boyd,A.J.,andMandt,S. Structuredstochastic
andWendySchmidtCenterattheBroadInstituteofMIT gradientmcmc. InInternationalConferenceonMachine
andHarvard(MS).PHgratefullyacknowledgesfinancial
Learning,2022.
supportbytheDFGClusterofExcellence‘MachineLearn-
Andriushchenko, M. Adversarial attacks on GPT-4 via
ing-NewPerspectivesforScience’,EXC2064/1,project
simplerandomsearch. Preprint,2023.
number390727645;theGermanFederalMinistryofEduca-
tionandResearch(BMBF)throughtheTu¨bingenAICenter
Antoran,J.,Bhatt,U.,Adel,T.,Weller,A.,andHerna´ndez-
(FKZ:01IS18039A);andfundsfromtheMinistryofSci-
Lobato,J.M. Gettinga{clue}: Amethodforexplaining
ence,ResearchandArtsoftheStateofBaden-Wu¨rttemberg.
uncertainty estimates. In International Conference on
SMacknowledgessupportfromtheNationalScienceFoun-
LearningRepresentations,2021.
dation (NSF) under the NSF CAREER Award 2047418;
NSFGrants2003237and2007719,theDepartmentofEn-
Antoran,J.,Padhy,S.,Barbano,R.,Nalisnick,E.,Janz,D.,
ergy, Office of Science under grant DE-SC0022331, as
andHerna´ndez-Lobato,J.M. Sampling-basedinference
well as gifts from Disney and Qualcomm. CN kindly ac-
for large linear models, with application to linearised
knowledges the support of EPSRC grants EP/V022636/1
Laplace. InInternationalConferenceonLearningRepre-
andEP/Y028783/1.
sentations,2023.
Arbel, J., Pitas, K., Vladimirova, M., and Fortuin, V. A
primeronBayesianneuralnetworks: reviewanddebates.
arXivpreprintarXiv:2309.16314,2023.
9PositionPaper:BayesianDeepLearningintheAgeofLarge-ScaleAI
Atzeni, M., Sachan, M., and Loukas, A. Infusing lattice Chen, T., Fox, E., and Guestrin, C. Stochastic gradient
symmetry priors in attention mechanisms for sample- HamiltonianMonteCarlo. InInternationalConference
efficient abstract geometric reasoning. arXiv preprint onMachineLearning,2014.
arXiv:2306.03175,2023.
Ciosek, K., Fortuin, V., Tomioka, R., Hofmann, K., and
Bamler, R., Salehi, F., and Mandt, S. Augmenting and Turner,R. Conservativeuncertaintyestimationbyfitting
tuningknowledgegraphembeddings. InConferenceon priornetworks. InInternationalConferenceonLearning
UncertaintyinArtificialIntelligence,2020. Representations,2020.
Band, N., Rudner, T. G. J., Feng, Q., Filos, A., Nado, Cohen,S.Bayesiananalysisinnaturallanguageprocessing.
Z., Dusenberry, M. W., Jerfel, G., Tran, D., and Gal, SpringerNature,2022.
Y. BenchmarkingBayesianDeepLearningonDiabetic
Retinopathy Detection Tasks. In Advances in Neural Cranmer,M.,Tamayo,D.,Rein,H.,Battaglia,P.,Hadden,
InformationProcessingSystems,2021. S.,Armitage,P.J.,Ho,S.,andSpergel,D.N. ABayesian
neuralnetworkpredictsthedissolutionofcompactplan-
Bayes,T. Anessaytowardssolvingaprobleminthedoc- etarysystems. ProceedingsoftheNationalAcademyof
trineofchances. PhilosophicaltransactionsoftheRoyal Sciences,118(40):e2026053118,2021.
SocietyofLondon,53:370–418,1763. BythelateRev.
Mr.Bayes,FRScommunicatedbyMr.Price,inaletter D’Angelo,F.andFortuin,V. Repulsivedeepensemblesare
toJohnCanton,AMFRS. Bayesian. AdvancesinNeuralInformationProcessing
Systems,2021.
Bhatt, U., Antora´n, J., Zhang, Y., Liao, Q. V., Sattigeri,
P.,Fogliato,R.,Melanc¸on,G.,Krishnan,R.,Stanley,J., D’Angelo, F., Fortuin, V., and Wenzel, F. On Stein
Tickoo, O., Nachman, L., Chunara, R., Srikumar, M., variational neural network ensembles. arXiv preprint
Weller,A.,andXiang,A. Uncertaintyasaformoftrans- arXiv:2106.10760,2021.
parency: Measuring, communicating, and using uncer-
Daxberger,E.,Kristiadi,A.,Immer,A.,Eschenhagen,R.,
tainty.InProceedingsofthe2021AAAI/ACMConference
Bauer, M., and Hennig, P. Laplace redux - effortless
onAI,Ethics,andSociety,AIES’21,pp.401–413.Asso-
Bayesiandeeplearning. InAdvancesinNeuralInforma-
ciationforComputingMachinery,2021.
tionProcessingSystems,2021a.
Bingham, E., Chen, J. P., Jankowiak, M., Obermeyer, F.,
Daxberger,E.,Nalisnick,E.,Allingham,J.U.,Antoran,J.,
Pradhan,N.,Karaletsos,T.,Singh,R.,Szerlip,P.,Hors-
andHerna´ndez-Lobato,J.M. Bayesiandeeplearningvia
fall,P.,andGoodman,N.D. Pyro: Deepuniversalprob-
subnetwork inference. In International Conference on
abilistic programming. Journal of Machine Learning
MachineLearning,2021b.
Research,20(28):1–6,2019.
de G. Matthews, A. G., Rowland, M., Hron, J., Turner,
Bouchiat, K., Immer, A., Ye`che, H., Ra¨tsch, G., andFor-
R.E.,andGhahramani,Z. Gaussianprocessbehaviour
tuin, V. Laplace-approximatedneuraladditivemodels:
ImprovinginterpretabilitywithBayesianinference.arXiv inwidedeepneuralnetworks. InternationalConference
preprintarXiv:2305.16905,2023.
onLearningRepresentations,2018.
Brooks,S.,Gelman,A.,Jones,G.,andMeng,X.-L. Hand- Detommaso,G.,Gasparin,A.,Donini,M.,Seeger,M.,Wil-
bookofMarkovchainMonteCarlo. CRCPress,2011. son,A.G.,andArchambeau,C. Fortuna: Alibraryfor
uncertaintyquantificationindeeplearning.arXivpreprint
Burt,D.R.,Ober,S.W.,Garriga-Alonso,A.,andvander arXiv:2302.04019,2023.
Wilk,M. Understandingvariationalinferenceinfunction-
space. InThirdSymposiumonAdvancesinApproximate Dettmers,T.,Pagnoni,A.,Holtzman,A.,andZettlemoyer,
BayesianInference,2020. L. Qlora: EfficientfinetuningofquantizedLLMs. arXiv
preprintarXiv:2305.14314,2023.
Casper,S.,Davies,X.,Shi,C.,Gilbert,T.K.,Scheurer,J.,
Rando,J.,Freedman,R.,Korbak,T.,Lindner,D.,Freire, Dold, D., Ru¨gamer, D., Sick, B., and Du¨rr, O. Semi-
P.,etal.Openproblemsandfundamentallimitationsofre- structuredsubspaceinference. InInternationalConfer-
inforcementlearningfromhumanfeedback.Transactions enceonArtificialIntelligenceandStatistics,2024.
onMachineLearningResearch,2023.
Ferreira, L., Conselice, C. J., Duncan, K., Cheng, T.-Y.,
Chang, P.G., Murphy, K.P., andJones, M. Ondiagonal Griffiths,A.,andWhitney,A. Galaxymergerratesupto
approximationstotheextendedKalmanfilterforonline z∼3usingaBayesiandeeplearningmodel: Amajor-
trainingofBayesianneuralnetworks. InContinualLife- mergerclassifierusingillustrisTNGsimulationdata. The
longLearningWorkshopatACML2022,2022. AstrophysicalJournal,895(2):115,2020.
10PositionPaper:BayesianDeepLearningintheAgeofLarge-ScaleAI
Finzi,M.,Benton,G.,andWilson,A.G. Residualpathway Gruver, N., Finzi, M., Qiu, S., and Wilson, A. G. Large
priorsforsoftequivarianceconstraints. InAdvancesin language models are zero-shot time series forecasters.
NeuralInformationProcessingSystems,2021. arXivpreprintarXiv:2310.07820,2023.
Fortuin, V., Garriga-Alonso, A., van der Wilk, M., and Gu,Y.andDunson,D.B. Bayesianpyramids: identifiable
Aitchison, L. BNNpriors: A library for Bayesian neu- multilayer discrete latent structure models for discrete
ral network inference with different prior distributions. data. Journal of the Royal Statistical Society Series B:
SoftwareImpacts,9:100079,2021. StatisticalMethodology,85(2):399–426,2023.
Fortuin, V., Garriga-Alonso, A., Ober, S. W., Wenzel, F., Guo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. On
Ra¨tsch,G.,Turner,R.E.,vanderWilk,M.,andAitchi- calibrationofmodernneuralnetworks. InInternational
son, L. Bayesian neural network priors revisited. In ConferenceonMachineLearning,2017.
InternationalConferenceonLearningRepresentations,
2022. Havasi,M.,Peharz,R.,andHerna´ndez-Lobato,J.M. Mini-
malrandomcodelearning: Gettingbitsbackfromcom-
Gal,Y.,Islam,R.,andGhahramani,Z.DeepBayesianactive
pressedmodelparameters. InInternationalConference
learningwithimagedata. InInternationalConferenceon
onLearningRepresentations,2019.
MachineLearning,2017.
He, B., Lakshminarayanan, B., andTeh, Y.W. Bayesian
Ganev,S.K.andAitchison,L. Semi-supervisedlearning
deepensemblesviatheneuraltangentkernel.InAdvances
withaprincipledlikelihoodfromagenerativemodelof
inNeuralInformationProcessingSystems,2020.
datacuration. InInternationalConferenceonLearning
Representations,2023. Hennig, P., Osborne, M. A., and Kersting, H. P. Proba-
bilistic Numerics: Computation as Machine Learning.
Garnelo,M.,Rosenbaum,D.,Maddison,C.,Ramalho,T.,
CambridgeUniversityPress,2022.
Saxton,D.,Shanahan,M.,Teh,Y.W.,Rezende,D.,and
Eslami,S.A. Conditionalneuralprocesses. InInterna-
Herna´ndez,S.andLo´pez,J.L. Uncertaintyquantification
tionalConferenceonMachineLearning,2018.
forplantdiseasedetectionusingBayesiandeeplearning.
AppliedSoftComputing,96:106597,2020.
Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B.,
Vehtari, A., and Rubin, D. B. Bayesian data analysis.
Hobbhahn, M., Kristiadi, A., andHennig, P. Fastpredic-
ChapmanandHall/CRC,3rdedition,2013.
tive uncertainty for classification with Bayesian deep
Gelman,A.,Vehtari,A.,Simpson,D.,Margossian,C.C., networks. In Conference on Uncertainty in Artificial
Carpenter,B.,Yao,Y.,Kennedy,L.,Gabry,J.,Bu¨rkner, Intelligence,2022.
P.-C.,andModra´k,M.Bayesianworkflow.arXivpreprint
Hoeting,J.A.,Madigan,D.,Raftery,A.E.,andVolinsky,
arXiv:2011.01808,2020.
C.T. Bayesianmodelaveraging. InProceedingsofthe
Ghosh, S., Yao, J., and Doshi-Velez, F. Structured varia- AAAIWorkshoponIntegratingMultipleLearnedModels,
tionallearningofbayesianneuralnetworkswithhorse- volume335,1998.
shoe priors. In International Conference on Machine
Hoeting,J.A.,Madigan,D.,Raftery,A.E.,andVolinsky,
Learning,pp.1744–1753.PMLR,2018.
C.T. Bayesianmodelaveraging: atutorial(withcom-
Gordon, J., Bruinsma, W.P., Foong, A.Y., Requeima, J., ments by m. clyde, david draper and ei george, and a
Dubois,Y.,andTurner,R.E. Convolutionalconditional rejoinderbytheauthors. Statisticalscience,14(4):382–
neuralprocesses. InInternationalConferenceonLearn- 417,1999.
ingRepresentations,2019.
Hubin,A.andStorvik,G. Combiningmodelandparameter
Griffiths, R.-R., Klarner, L., Moss, H. B., Ravuri, A., uncertaintyinbayesianneuralnetworks. arXivpreprint
Truong,S.,Stanton,S.,Tom,G.,Rankovic,B.,Du,Y., arXiv:1903.07594,2019.
Jamasb,A.,etal. GAUCHE:AlibraryforGaussianpro-
cessesinchemistry. InAdvancesinNeuralInformation Hubin,A.,Storvik,G.,andFrommlet,F. Flexiblebayesian
ProcessingSystems,2023. nonlinearmodelconfiguration. JournalofArtificialIntel-
ligenceResearch,72:901–942,2021.
Gruver, N., Stanton, S., Kirichenko, P., Finzi, M., Maf-
fettone, P., Myers, V., Delaney, E., Greenside, P., and Immer, A., Bauer, M., Fortuin, V., Ra¨tsch, G., andKhan,
Wilson, A. G. Effective surrogate models for protein M.E. Scalablemarginallikelihoodestimationformodel
designwithBayesianoptimization. InICMLWorkshop selectionindeeplearning. InInternationalConference
onComputationalBiology,2021. onMachineLearning,2021a.
11PositionPaper:BayesianDeepLearningintheAgeofLarge-ScaleAI
Immer,A.,Korzepa,M.,andBauer,M. Improvingpredic- Khan,M.E.andRue,H. TheBayesianlearningrule. Jour-
tionsofBayesianneuralnetsvialocallinearization. In nalofMachineLearningResearch,24(281):1–46,2023.
InternationalConferenceonArtificialIntelligenceand
Statistics,2021b. Khan,M.E.,Immer,A.,Abedi,E.,andKorzepa,M. Ap-
proximateinferenceturnsdeepnetworksintoGaussian
Immer, A., van der Ouderaa, T., Ra¨tsch, G., Fortuin, V.,
processes. InAdvancesinNeuralInformationProcessing
andvanderWilk,M. Invariancelearningindeepneural
Systems,2019.
networkswithdifferentiableLaplaceapproximations. Ad-
vancesinNeuralInformationProcessingSystems,2022.
Kirichenko,P.,Izmailov,P.,andWilson,A.G. Lastlayer
re-trainingissufficientforrobustnesstospuriouscorre-
Immer, A., Van Der Ouderaa, T. F., Van Der Wilk, M.,
lations. InInternationalConferenceonLearningRepre-
Ratsch,G.,andScho¨lkopf,B. Stochasticmarginallikeli-
sentations,2023.
hoodgradientsusingneuraltangentkernels. InInterna-
tionalConferenceonMachineLearning,2023.
Klarner,L.,Rudner,T.G.J.,Reutlinger,M.,Schindler,T.,
Izmailov, P., Maddox, W. J., Kirichenko, P., Garipov, T., Morris,G.M.,Deane,C.,andTeh,Y.W. DrugDiscovery
Vetrov, D., and Wilson, A. G. Subspace inference for underCovariateShiftwithDomain-InformedPriorDis-
Bayesiandeeplearning. InConferenceonUncertaintyin tributionsoverFunctions. InInternationalConference
ArtificialIntelligence,2020. onMachineLearning,2023.
Izmailov, P., Nicholson, P., Lotfi, S., and Wilson, A. G.
Kolb,C.,Mu¨ller,C.L.,Bischl,B.,andRu¨gamer,D.Smooth-
Dangers of Bayesian model averaging under covariate
ing the edges: A general framework for smooth opti-
shift. AdvancesinNeuralInformationProcessingSys-
mizationinsparseregularizationusingHadamardover-
tems,2021a.
parametrization. arXivpreprintarXiv:2307.03571,2023.
Izmailov,P.,Vikram,S.,Hoffman,M.D.,andWilson,A.
Kristiadi, A., Hein, M., and Hennig, P. Being Bayesian,
G.G. WhatareBayesianneuralnetworkposteriorsreally
evenjustabit,fixesoverconfidenceinReLUnetworks.
like? InInternationalConferenceonMachineLearning,
InInternationalConferenceonMachineLearning,2020.
2021b.
Janz, D., Hron, J., Mazur, P., Hofmann, K., Herna´ndez- Kristiadi,A.,Hein,M.,andHennig,P. Aninfinite-feature
Lobato,J.M.,andTschiatschek,S. Successoruncertain- extensionforBayesianReLUnetsthatfixestheirasymp-
ties: explorationanduncertaintyintemporaldifference toticoverconfidence. InAdvancesinNeuralInformation
learning. Advances in Neural Information Processing ProcessingSystems,2021a.
Systems,32,2019.
Kristiadi, A., Hein, M., and Hennig, P. Learnable uncer-
Jaynes, E. T. Probability theory: The logic of science. taintyunderLaplaceapproximations. InConferenceon
CambridgeUniversityPress,2003. UncertaintyinArtificialIntelligence,2021b.
Ji,Z.,Lee,N.,Frieske,R.,Yu,T.,Su,D.,Xu,Y.,Ishii,E.,
Kristiadi,A.,Hein,M.,andHennig,P. Beingabitfrequen-
Bang,Y.J.,Madotto,A.,andFung,P. Surveyofhalluci-
tistimprovesBayesianneuralnetworks. InInternational
nationinnaturallanguagegeneration. ACMComputing
ConferenceonArtificialIntelligenceandStatistics,2022.
Surveys,55(12),2023.
Kadavath,S.,Conerly,T.,Askell,A.,Henighan,T.,Drain, Kristiadi,A.,Dangel,F.,andHennig,P. Thegeometryof
D.,Perez,E.,Schiefer,N.,Hatfield-Dodds,Z.,DasSarma, neuralnets’parameterspacesunderreparametrization. In
N.,Tran-Johnson,E.,Johnston,S.,El-Showk,S.,Jones, ConferenceonNeuralInformationProcessingSystems,
A., Elhage, N., Hume, T., Chen, A., Bai, Y., Bowman, 2023.
S., Fort, S., Ganguli, D., Hernandez, D., Jacobson, J.,
Kernion,J.,Kravec,S.,Lovitt,L.,Ndousse,K.,Olsson, Krueger,D.,Huang,C.-W.,Islam,R.,Turner,R.,Lacoste,
C.,Ringer,S.,Amodei,D.,Brown,T.,Clark,J.,Joseph, A., and Courville, A. Bayesian hypernetworks. arXiv
N.,Mann,B.,McCandlish,S.,Olah,C.,andKaplan,J.
preprintarXiv:1710.04759,2017.
Languagemodels(mostly)knowwhattheyknow. arXiv
Langford,J.andShawe-Taylor,J. PAC-Bayes&margins.
preprintarXiv:2207.05221,2022.
AdvancesinNeuralInformationProcessingSystems,15,
Kapoor,S.,Maddox,W.J.,Izmailov,P.,andWilson,A.G. 2002.
On uncertainty, tempering, and data augmentation in
Bayesianclassification. AdvancesinNeuralInformation Lawrence,N.D. Variationalinferenceinprobabilisticmod-
ProcessingSystems,2022. els. PhDthesis,UniversityofCambridge,2001.
12PositionPaper:BayesianDeepLearningintheAgeofLarge-ScaleAI
Leitherer, A., Ziletti, A., andGhiringhelli, L.M. Robust deeplearningnetworksystemformultiaccessphysical
recognitionandexploratoryanalysisofcrystalstructures monitoringsystem. Sensors,19(13):3030,2019.
viaBayesiandeeplearning. NatureCommunications,12
(1):6234,2021. Mansinghka,V.K. Nativelyprobabilisticcomputation. PhD
thesis, Massachusetts Institute of Technology, Depart-
Li,A.,Boyd,A.,Smyth,P.,andMandt,S. Detectingand mentofBrainandCognitiveSciences,2009.
adaptingtoirregulardistributionshiftsinBayesianonline
learning. Advances in Neural Information Processing Margatina,K.,Barrault,L.,andAletras,N. Ontheimpor-
Systems,2021. tanceofeffectivelyadaptingpretrainedlanguagemodels
foractivelearning. InProceedingsofthe60thAnnual
Li,J.,Miao,Z.,Qiu,Q.,andZhang,R. TrainingBayesian
MeetingoftheAssociationforComputationalLinguistics.
neuralnetworkswithsparsesubspacevariationalinfer-
AssociationforComputationalLinguistics,2022.
ence. InInternationalConferenceonLearningRepresen-
tations,2024.
Margatina,K.,Schick,T.,Aletras,N.,andDwivedi-Yu,J.
Active learning principles for in-context learning with
Li, Y. L., Rudner, T. G., and Wilson, A. G. A study of
largelanguagemodels. InFindingsoftheAssociationfor
Bayesian neural network surrogates for Bayesian opti-
ComputationalLinguistics: EMNLP2023,2023.
mization. arXivpreprintarXiv:2305.20028,2023.
Lipman,Y.,Chen,R.T.,Ben-Hamu,H.,Nickel,M.,andLe, McAllister,R.,Gal,Y.,Kendall,A.,VanDerWilk,M.,Shah,
M. Flowmatchingforgenerativemodeling. InInterna- A., Cipolla, R., and Weller, A. Concrete problems for
tionalConferenceonLearningRepresentations,2022. autonomousvehiclesafety:AdvantagesofBayesiandeep
learning.InProceedingsoftheTwenty-SixthInternational
Liu,Q.andWang,D. Steinvariationalgradientdescent: A JointConferenceonArtificialIntelligence,2017.
generalpurposeBayesianinferencealgorithm. Advances
inNeuralInformationProcessingSystems,2016. Meister,J.A.,Nguyen,K.A.,Kapetanakis,S.,andLuo,Z.
Anoveldeeplearningapproachforone-stepconformal
Lopez,J.L.,Rudner,T.G.J.,andShamout,F. Informative
predictionapproximation. AnnalsofMathematicsand
priorsimprovethereliabilityofmultimodalclinicaldata
ArtificialIntelligence,pp.1–28,2023.
classification. InMachineLearningforHealthSympo-
siumFindings,2023.
Milsom, E., Anson, B., and Aitchison, L. Convolutional
deepkernelmachines. arXivpreprintarXiv:2309.09814,
Louizos,C.,Ullrich,K.,andWelling,M.Bayesiancompres-
sionfordeeplearning. AdvancesinNeuralInformation 2023.
ProcessingSystems,30,2017.
Moor,M.,Banerjee,O.,Abad,Z.S.H.,Krumholz,H.M.,
Luo, X., Nadiga, B. T., Park, J. H., Ren, Y., Xu, W., and Leskovec, J., Topol, E. J., and Rajpurkar, P. Founda-
Yoo,S. ABayesiandeeplearningapproachtonear-term tionmodelsforgeneralistmedicalartificialintelligence.
climate prediction. Journal of Advances in Modeling Nature,616(7956):259–265,2023.
EarthSystems,14(10):e2022MS003058,2022.
Mu¨ller,S.,Hollmann,N.,Arango,S.P.,Grabocka,J.,and
MacKay,D.J. Bayesianinterpolation. NeuralComputation, Hutter,F. TransformerscandoBayesianinference. In
4(3):415–447,1992. InternationalConferenceonLearningRepresentations,
2021.
MacKay,D.J. ChoiceofbasisforLaplaceapproximation.
MachineLearning,33:77–86,1998.
Mur-Labadia,L.,Martinez-Cantin,R.,andGuerrero,J.J.
Bayesiandeeplearningforaffordancesegmentationin
Maddox,W.J.,Izmailov,P.,Garipov,T.,Vetrov,D.P.,and
images. arXivpreprintarXiv:2303.00871,2023.
Wilson, A. G. A simple baseline for Bayesian uncer-
taintyindeeplearning. AdvancesinNeuralInformation
Murphy,K.P. Probabilisticmachinelearning: Advanced
ProcessingSystems,32,2019.
topics. MITPress,2023.
Mandt,S.,Hoffman,M.D.,andBlei,D.M. Stochasticgra-
dientdescentasapproximateBayesianinference.Journal Neal,R.M. Priorsforinfinitenetworks. Bayesianlearning
ofMachineLearningResearch,18(134):1–35,2017. forneuralnetworks,pp.29–53,1996.
Manogaran,G.,Shakeel,P.M.,Fouad,H.,Nam,Y.,Baskar, Nemeth,C.andFearnhead,P. StochasticgradientMarkov
S.,Chilamkurti,N.,andSundarasekar,R. WearableIoT chainMonteCarlo. JournaloftheAmericanStatistical
smart-log patch: An edge computing-based Bayesian Association,116(533):433–450,2021.
13PositionPaper:BayesianDeepLearningintheAgeofLarge-ScaleAI
Nguyen,C.V.,Li,Y.,Bui,T.D.,andTurner,R.E. Varia- Polson,N.G.andRocˇkova´,V. Posteriorconcentrationfor
tionalcontinuallearning. InInternationalConferenceon sparsedeeplearning. AdvancesinNeuralInformation
LearningRepresentations,2018. ProcessingSystems,31,2018.
Oates,C.J.andSullivan,T.J. Amodernretrospectiveon Qiu, S., Rudner, T. G. J., Kapoor, S., and Wilson, A. G.
probabilisticnumerics. StatisticsandComputing,29(6): Should we learn most likely functions or parameters?
1335–1351,2019.
InAdvancesinNeuralInformationProcessingSystems,
2023.
Ober,S.andAitchison,L. Avariationalapproximatepos-
Rainforth,T.,Foster,A.,Ivanova,D.R.,andSmith,F.B.
teriorforthedeepwishartprocess. AdvancesinNeural
ModernBayesianexperimentaldesign. arXivpreprint
InformationProcessingSystems,2021.
arXiv:2302.14545,2023.
Ober,S.W.,Anson,B.,Milsom,E.,andAitchison,L. An
Ritter,H.andKaraletsos,T. TyXe: Pyro-basedBayesian
improvedvariationalapproximateposteriorforthedeep
neuralnetsforPytorch.InProceedingsofMachineLearn-
wishartprocess. InConferenceonUncertaintyinArtifi-
ingandSystems,2022.
cialIntelligence,2023.
Ritter, H., Botev, A., and Barber, D. A scalable Laplace
Osband,I.,Wen,Z.,Asghari,S.M.,Dwaracherla,V.,Lu, approximationforneuralnetworks. InInternationalCon-
X.,Ibrahimi,M.,Lawson,D.,Hao,B.,O’Donoghue,B., ferenceonLearningRepresentations,2018.
and Van Roy, B. The neural testbed: Evaluating joint
predictions. AdvancesinNeuralInformationProcessing Ritter, H., Kukla, M., Zhang, C., and Li, Y. Sparse un-
Systems,2022. certaintyrepresentationindeeplearningwithinducing
weights. InAdvancesinNeuralInformationProcessing
Osband, I., Wen, Z., Asghari, S. M., Dwaracherla, V., Systems,2021.
Ibrahimi, M., Lu, X., and Van Roy, B. Approxi-
Robbins,H.E. Astochasticapproximationmethod. Annals
mateThompsonsamplingviaepistemicneuralnetworks.
ofMathematicalStatistics,22:400–407,1951.
InConferenceonUncertaintyinArtificialIntelligence,
2023. Rothfuss,J.,Fortuin,V.,Josifoski,M.,andKrause,A. PA-
COH:Bayes-optimalmeta-learningwithPAC-guarantees.
Papadopoulos, H., Vovk, V., and Gammerman, A. Con-
InInternationalConferenceonMachineLearning,2021.
formal prediction with neural networks. In 19th IEEE
InternationalConferenceonToolswithArtificialIntelli- Rothfuss,J.,Josifoski,M.,Fortuin,V.,andKrause,A. PAC-
gence(ICTAI2007),volume2,pp.388–395.IEEE,2007. Bayesianmeta-learning: Fromtheorytopractice. arXiv
preprintarXiv:2211.07206,2022.
Papamarkou,T. ApproximateblockedGibbssamplingfor
Bayesianneuralnetworks. StatisticsandComputing,33, Rudner,T.G.J.,Chen,Z.,Teh,Y.W.,andGal,Y. Tractable
2023. Function-SpaceVariationalInferenceinBayesianNeural
Networks.InAdvancesinNeuralInformationProcessing
Papamarkou,T.,Hinkle,J.,Young,M.T.,andWomble,D. Systems,2022a.
ChallengesinMarkovchainMonteCarloforBayesian
Rudner,T.G.J.,Smith,F.B.,Feng,Q.,Teh,Y.W.,andGal,
neural networks. Statistical Science, 37(3):425–442,
Y. Continual Learning via Sequential Function-Space
2022.
Variational Inference. In International Conference on
Parrado-Herna´ndez,E.,Ambroladze,A.,Shawe-Taylor,J., MachineLearning,2022b.
and Sun, S. PAC-Bayes bounds with data dependent
Rudner, T. G. J., Kapoor, S., Qiu, S., and Wilson, A. G.
priors. Journal of Machine Learning Research, 13(1):
Function-SpaceRegularizationinNeuralNetworks: A
3507–3531,2012.
ProbabilisticPerspective. InInternationalConferenceon
MachineLearning,2023a.
Peng,W.,Ye,Z.-S.,andChen,N. Bayesiandeep-learning-
basedhealthprognosticstowardprognosticsuncertainty. Rudner, T. G. J., Pan, X., Li, Y. L., Shwartz-Ziv, R., and
IEEETransactionsonIndustrialElectronics,67(3):2283–
Wilson, A. G. Uncertainty-aware priors for finetuning
2293,2019. pretrainedmodels. InPreprint,2023b.
Pielok, T., Bischl, B., and Ru¨gamer, D. Approximate Rudner,T.G.J.,Zhang,Y.S.,Wilson,A.G.,andKempe,J.
Bayesianinferencewithsteinfunctionalvariationalgra- MindtheGAP:Improvingrobustnesstosubpopulation
dientdescent. InInternationalConferenceonLearning shiftswithgroup-awarepriors. InInternationalConfer-
Representations,2022. enceonArtificialIntelligenceandStatistics,2024.
14PositionPaper:BayesianDeepLearningintheAgeofLarge-ScaleAI
Ru¨gamer,D. AnewPHO-rmulaforimprovedperformance Tom, G., Hickman, R. J., Zinzuwadia, A., Mohajeri, A.,
ofsemi-structurednetworks. InInternationalConference Sanchez-Lengeling,B.,andAspuru-Guzik,A. Calibra-
onMachineLearning,2023. tionandgeneralizabilityofprobabilisticmodelsonlow-
datachemicaldatasetswithDIONYSUS. DigitalDiscov-
Russo,D.J.,VanRoy,B.,Kazerouni,A.,Osband,I.,and ery,2023.
Wen,Z. AtutorialonThompsonsampling. Foundations
andTrends®inMachineLearning,11(1):1–96,2018. Tran, B.-H., Rossi, S., Milios, D., andFilippone, M. All
you need is a good functional prior for Bayesian deep
Schwo¨bel,P.,Jørgensen,M.,Ober,S.W.,andVanDerWilk, learning. JournalofMachineLearningResearch,23(1),
M. Lastlayermarginallikelihoodforinvariancelearning. 2022a.
InInternationalConferenceonArtificialIntelligenceand
Statistics,2022. Tran,D.,Liu,J.,Dusenberry,M.W.,Phan,D.,Collier,M.,
Ren,J.,Han,K.,Wang,Z.,Mariet,Z.,Hu,H.,Band,N.,
Sen,D.,Papamarkou,T.,andDunson,D. Bayesianneural Rudner,T.G.J.,Singhal,K.,Nado,Z.,vanAmersfoort,J.,
networksanddimensionalityreduction. InHandbookof Kirsch,A.,Jenatton,R.,Thain,N.,Yuan,H.,Buchanan,
Bayesian,fiducial,andfrequentistinference.Chapmann
K., Murphy, K., Sculley, D., Gal, Y., Ghahramani, Z.,
andHall/CRCPress,2024. Snoek, J., and Lakshminarayanan, B. Plex: Towards
Reliability Using Pretrained Large Model Extensions.
Sharma,M.,Rainforth,T.,Teh,Y.W.,andFortuin,V. Incor-
InICML2022WorkshoponPre-training: Perspectives,
poratingunlabelleddataintoBayesianneuralnetworks.
Pitfalls,andPathsForward,2022b.
arXivpreprintarXiv:2304.01762,2023.
van der Ouderaa, T. F., Immer, A., and van der Wilk, M.
Shi, L., Copot, C., and Vanlanduit, S. A Bayesian deep
Learning layer-wise equivariances automatically using
neuralnetworkforsafevisualservoinginhuman–robot
gradients. InAdvancesinNeuralInformationProcessing
interaction. FrontiersinRoboticsandAI,8:687031,2021.
Systems,2023.
Shwartz-Ziv,R.,Goldblum,M.,Souri,H.,Kapoor,S.,Zhu,
Vandal, T., Kodra, E., Dy, J., Ganguly, S., Nemani, R.,
C., LeCun, Y., and Wilson, A. G. Pre-train your loss:
andGanguly,A.R. Quantifyinguncertaintyindiscrete-
EasyBayesiantransferlearningwithinformativepriors.
continuousandskeweddatawithBayesiandeeplearning.
InAdvancesinNeuralInformationProcessingSystems,
InProceedingsofthe24thACMSIGKDDInternational
2022.
ConferenceonKnowledgeDiscovery&DataMining,pp.
2377–2386,2018.
Skaaret-Lund,L.,Storvik,G.,andHubin,A. Sparsifying
bayesian neural networks with latent binary variables
Vehtari,A.,Gelman,A.,Simpson,D.,Carpenter,B.,and
andnormalizingflows. arXivpreprintarXiv:2305.03395,
Bu¨rkner, P.-C. Rank-normalization, folding, andlocal-
2023.
ization: An improved R(cid:98) for assessing convergence of
MCMC. BayesianAnalysis,16(2):667–718,2021.
Soboczenski,F.,Himes,M.D.,O’Beirne,M.D.,Zorzan,
S.,Baydin,A.G.,Cobb,A.D.,Gal,Y.,Angerhausen,D.,
Villani, C. Topics in optimal transportation, volume 58.
Mascaro,M.,Arney,G.N.,etal. Bayesiandeeplearn-
AmericanMathematicalSoc.,2021.
ingforexoplanetatmosphericretrieval. arXivpreprint
arXiv:1811.03390,2018. Vladimirova, M., Verbeek, J., Mesejo, P., and Arbel, J.
Understanding Priors in Bayesian Neural Networks at
Song,Y.,Sohl-Dickstein,J.,Kingma,D.P.,Kumar,A.,Er- theUnitLevel. InInternationalConferenceonMachine
mon,S.,andPoole,B. Score-basedgenerativemodeling Learning,2019.
throughstochasticdifferentialequations. InInternational
ConferenceonLearningRepresentations,2020. Vovk, V., Gammerman, A., and Shafer, G. Algorithmic
learninginarandomworld,volume29. Springer,2005.
Strieth-Kalthoff,F.,Hao,H.,Rathore,V.,Derasp,J.,Gaudin,
T., Angello, N. H., Seifrid, M., Trushina, E., Guy, M., Wang, Y., Rudner, T.G.J., andWilson, A.G. Visualex-
Liu,J.,andetal. Delocalized,asynchronous,closed-loop planationsofimage-textrepresentationsviamulti-modal
discoveryoforganiclaseremitters. ChemRxiv,2023. informationbottleneckattribution. InAdvancesinNeural
InformationProcessingSystems,2023a.
Tatzel,L.,Wenger,J.,Schneider,F.,andHennig,P. Accel-
eratinggeneralizedlinearmodelsbytradingoffcompu- Wang,Z.,Chen,Y.,Song,Q.,andZhang,R.Enhancinglow-
tationforuncertainty. arXivpreprintarXiv:2310.20285, precisionsamplingviastochasticgradientHamiltonian
2023. MonteCarlo. arXivpreprintarXiv:2310.16320,2023b.
15PositionPaper:BayesianDeepLearningintheAgeofLarge-ScaleAI
Wasserman,L. Bayesianmodelselectionandmodelaverag- Yang,A.X.,Robeyns,M.,Milsom,E.,Anson,B.,Schoots,
ing. Journalofmathematicalpsychology,44(1):92–107, N.,andAitchison,L. Atheoryofrepresentationlearn-
2000. ing gives a deep generalisation of kernel methods. In
InternationalConferenceonMachineLearning,2023a.
Watanabe,S. AlgebraicGeometryandStatisticalLearning
Theory. CambridgeUniversityPress,USA,2009. Yang, A. X., Robeyns, M., Wang, X., and Aitchison, L.
Bayesianlow-rankadaptationforlargelanguagemodels.
Way, G.P.andGreene, C.S. Bayesiandeeplearningfor InternationalConferenceonLearningRepresentations,
single-cellanalysis. NatureMethods,15(12):1009–1010,
2024.
2018.
Yang,Y.,Li,W.,Gulliver,T.A.,andLi,S. Bayesiandeep
Wei, S. and Lau, E. Variational Bayesian neural net- learning-based probabilistic load forecasting in smart
works via resolution of singularities. arXiv preprint grids. IEEETransactionsonIndustrialInformatics,16
arXiv:2302.06035,2023.
(7):4703–4713,2019.
Welling,M.andTeh,Y.W. Bayesianlearningviastochastic Yang,Y.,Bamler,R.,andMandt,S. VariationalBayesian
gradientLangevindynamics.InInternationalConference quantization. InInternationalConferenceonMachine
onMachineLearning,2011. Learning,2020.
Wen, Z., Osband, I., Qin, C., Lu, X., Ibrahimi, M., Yang, Y., Hui, B., Yuan, H., Gong, N., and Cao, Y.
Dwaracherla,V.,Asghari,M.,andVanRoy,B. Frompre- SneakyPrompt: Evaluatingrobustnessoftext-to-image
dictionstodecisions: Theimportanceofjointpredictive generativemodels’safetyfilters. InIEEESymposiumon
distributions. arXivpreprintarXiv:2107.09224,2021. SecurityandPrivacy,2023b.
Wenger, J., Pleiss, G., Pfo¨rtner, M., Hennig, P., andCun- Yang,Y.,Mandt,S.,andTheis,L. Anintroductiontoneural
ningham,J.P. Posteriorandcomputationaluncertaintyin datacompression.FoundationsandTrends®inComputer
Gaussianprocesses. InAdvancesinNeuralInformation GraphicsandVision,15(2):113–200,2023c.
ProcessingSystems,2022.
Zhang, J., Jennings, J., Zhang, C., and Ma, C. Towards
Wenzel, F., Roth, K., Veeling, B., Swiatkowski, J., Tran, causalfoundationmodel: ondualitybetweencausalin-
L.,Mandt,S.,Snoek,J.,Salimans,T.,Jenatton,R.,and ferenceandattention. arXivpreprintarXiv:2310.00809,
Nowozin, S. HowgoodistheBayesposteriorindeep 2023.
neuralnetworksreally? InInternationalConferenceon
MachineLearning,2020. Zhang,R.,Li,C.,Zhang,J.,Chen,C.,andWilson,A.G.
CyclicalstochasticgradientMCMCforBayesiandeep
Wiese, J. G., Wimmer, L., Papamarkou, T., Bischl, B., learning. InInternationalConferenceonLearningRep-
Gu¨nnemann, S., and Ru¨gamer, D. Towards efficient resentations,2019.
MCMCsamplinginBayesianneuralnetworksbyexploit-
ingsymmetry. InJointEuropeanConferenceonMachine Zhang, R., Wilson, A. G., and De Sa, C. Low-precision
LearningandKnowledgeDiscoveryinDatabases(ECML stochasticgradientLangevindynamics. InInternational
PKDD):ResearchTrack,pp.459–474,2023. ConferenceonMachineLearning,2022.
Zhou,Z.,Yu,H.,andShi,H. Humanactivityrecognition
Wild, V. D., Ghalebikesabi, S., Sejdinovic, D., and
basedonimprovedBayesianconvolutionnetworktoan-
Knoblauch,J. Arigorouslinkbetweendeepensembles
and(variational)Bayesianmethods. InConferenceon alyzehealthcaredatausingwearableiotdevice. IEEE
NeuralInformationProcessingSystems,2023.
Access,8:86411–86418,2020.
Wilson,A.G.andIzmailov,P. Bayesiandeeplearningand
aprobabilisticperspectiveofgeneralization. Advancesin
NeuralInformationProcessingSystems,2020.
Wilson, A.G., Hu, Z., Salakhutdinov, R., andXing, E.P.
Deep kernel learning. In International Conference on
ArtificialIntelligenceandStatistics,2016.
Xie, S. M., Raghunathan, A., Liang, P., and Ma, T. An
explanationofin-contextlearningasimplicitBayesian
inference. arXivpreprintarXiv:2111.02080,2021.
16PositionPaper:BayesianDeepLearningintheAgeofLarge-ScaleAI
A.Diagnostics,MetricsandBenchmarks
Currently,thereisalackofconvergenceandperformancemetricsspecificallyfortheneedsofBDL.Developingsuchtools
canhelpidentifythegoalsinBDLaswellasassesstheirprogress. Besides, thechoiceofevaluationmetrics, datasets
andbenchmarkslackconsensusintheBDLcommunitywhichreflectsadifficultyinclearlydefiningthegoalsofBDL
in a field traditionally viewed through frequentist lens, specifically in terms of performance on test data. Many of the
generalBayesiandiagnosticandevaluationapproachesareproposedthroughBayesianworkflow(Gelmanetal.,2020).
ThisappendixdiscussesthemostrelevantapproachesforBDL.
Convergencediagnosticsinparameterspace. Theanalysisofconvergenceandsamplingefficiency(Gelmanetal.,2013;
Vehtarietal.,2021)forSG-MCMCsamplingbecomesadelicatematter,whichiscurrentlybypassedbyarathersimplistic
analysisofthesequantitiesusingsummarystatisticsofpredictivedistributions. Moregenerally,verifyingtheconvergenceof
inferencealgorithmsinthehigh-dimensionalandmultimodalsettingsofBDLmodelsisnotstraightforward. Convergence
checksdesignedforBNNsneedtobefurtherstudied.
Performancemetricsinpredictivespace. BDLandGPliteratureoftenfocusonthemeanofthepredictivedistribution,
overlookingtheanalysisofvarianceofthepredictivedistribution. Someperformancemetricsarecommonlyusedtoassess
variance levels, for example, by evaluating the log-likelihood or the entropy of predictions for test data (Rudner et al.,
2022a;2023a). However,asystematicwaytocharacterizethepredictiveuncertaintyinBDLinference(apartfrombinary
classificationproblemswhereAUROCandAUPRCarewidelyused)iscurrentlylacking(Arbeletal.,2023). Thechallenge
ofsettingmetricsfortheassessmentofepistemicandaleatoricuncertaintyslowstheprogressinBDLandcouldpotentially
beaddressedbyestablishingwidelyacceptedbenchmarksforBDLmethods.
Performancemetricsinmisspecifiedsettings. Addressingchallengesrelatedtodistributionshiftandtestdataperformance
requires the development of robust performance metrics. To establish BDL model reliability under distribution shift,
tightergeneralizationbounds, suchasthoseprovidedbythePAC-Bayesframework(Langford&Shawe-Taylor,2002;
Parrado-Herna´ndezetal.,2012), arecrucialtoobtainprobabilisticguaranteesonmodelperformance. Furthermore, in
misspecifiedsettings,evaluatingcalibrationbecomesparamount. Innovativetechniques,suchastwo-stagecalibration(Guo
etal.,2017)andconformalprediction(Papadopoulosetal.,2007)oritsBayesiancounterpart(Hobbhahnetal.,2022),
offer practical solutions by refining predicted probabilities and quantifying predictive uncertainty, respectively. These
approaches collectively contribute to a more comprehensive evaluation of model performance in scenarios where the
underlyingassumptionsmaynotalignwiththetruedatadistribution.
Probabilistictreatmentofdatasets. Probabilistictreatmentsofdataasafirst-classcitizenthatcanbereasonedaboutin
BNNsseempromising. Suchprobabilisticapproachesmayhelpcreatemorefocusedandusefuldatasetstorepresentthe
knowledgecontainedinvastdatasources,improvingtheabilitytotrainandmaintainlargemodels.
B.SoftwareUsability
ApplyingaBDLapproachtoareal-worldproblemisstillamorecomplexendeavorthanoptingforanoff-the-shelfstandard
deeplearningsolution,whichlimitsthereal-worldadoptionofBDL.Softwaredevelopmentiskeytoencouragingdeep
learningpractitionerstouseBayesianmethods. Moregenerally,thereisaneedforsoftwarethatwouldmakeiteasierfor
practitionerstotryBDLintheirprojects. TheuseofBDLmustbecomecompetitiveinhumaneffortwithstandarddeep
learning.
Someeffortshavebeenmadetodevelopsoftwarepackages,librariesorprobabilisticprogramminglanguages(PPLs)ontopof
deeplearningframeworks.bayesianize(Ritteretal.,2021),bnn priors(Fortuinetal.,2021),Laplace(Daxberger
etal.,2021a),Pyro(Binghametal.,2019)andTyXe(Ritter&Karaletsos,2022)aresoftwarespeciesbuiltonPyTorch,
TensorFlow ProbabilityisalibrarybuiltonTensorFlow,andFortuna(Detommasoetal.,2023)isalibrary
builtonJAX.Itwouldhelptomakefurtherprogresswithcontributionsfromtheprobabilisticprogrammingcommunity.
PPLs, such as Pyro, play a role in simplifying the application of probabilistic reasoning to deep learning. In fact,
abstractionsoftheprobabilistictreatmentofNNsinaPPL,suchasthoseperformedintheBDLlibraryTyXe,cansimplify
theapplicationofpriorsandinferencetechniquestoarbitraryNNs,asdemonstratedinavarietyofmodelsimplementedin
TyXe. PortingsuchideastomodernproblemsettingsinvolvingLLMsandmorebespokeprobabilisticstructureswould
enabletheuseofBDLinreal-worldproblems.
Contemporary deep learning pushes the limits of scale in all dimensions: datasets, parameter spaces, and structured
17PositionPaper:BayesianDeepLearningintheAgeofLarge-ScaleAI
function-valuedoutput. Forpointestimation,thecommunityhasbeendevelopingarray-centricprogrammingparadigms
thatallowsharding,partialevaluations,currying,andmore. BDLshouldbeabletomaptheseideastodevelopanalogous
software.
C.TopicalDevelopments
ThisappendixprovidestopicalorspecializedareasofBDLforfuturedevelopment. TheseincludeBDLforhuman-AI
interaction,lifelonganddecentralizedlearning,Bayesianreinforcementlearning(RL),anddomain-specificBDLmodels.
Human-AIinteractionandexplainableAI.EnablingAIsystemstocommunicateandexplaintheiruncertaintycanbuild
trustandimprovetheinteractionbetweenAIsystemsandhumans. Whileeffortsbythecommunityhavebeenmadeto
explainthepredictionsofDNNs,recenteffortsaimtoexplaintheuncertaintyofBDLmethods(Antoranetal.,2021;Bhatt
etal.,2021). UnderstandingwhichinputpatternsareresponsibleforhighpredictiveuncertaintycanbuildtrustinAIsystems
andcanprovideinsightsaboutinputregionswheredataissparse. Forexample,whentrainingaloandefaultpredictor,a
datascientistcanidentifypopulationsubgroups(byage,gender,orrace)underrepresentedinthetrainingdata. Collecting
moredatafromthesegroupscanleadtomoreaccuratepredictionsforawiderrangeofclients.
Lifelonganddecentralizedlearning. Acontemporaryresearchdirectionistogobeyondthe‘static’train-testframework
and focus on ‘dynamic’ problems where the test set is not known. This includes cases where predictive performance,
robustnessandsafetyareimportantandtherearerealisticconstraintsontheinfrastructure. Twosuchproblemsarelifelong
learninganddecentralizedlearning. FocusingonsuchproblemsisexpectedtoleadtoanewregimeinwhichBayesianideas
canbeusefulfordeeplearning.
EfficientexplorationinRL.RLisanareawhereBDLhasshownpotential. Asanexample,Thompsonsampling(TS)is
knowntobeacommonlyusedheuristicfordecisionmakingthat‘randomlyselectsanaction,accordingtotheprobability
thatitisoptimal’(Russoetal.,2018).TSbalancesexplorationwithexploitationandinitsexactformrequiressamplingfrom
theBayesianposterior.Inpractice,approximationsareoftenused,andrecentworkhasshownthatthequalityoftheresulting
multivariatejointpredictivedistributionovermultipletestinputsisimportantfordecision-making(Wenetal.,2021;Osband
etal.,2023). Thisisrelevant,astypicalBayesianandnon-Bayesianmethodsarecommonlyevaluatedbyassessingthe
qualityofmarginalpredictionsoverindividualtestinputs,ignoringpotentialdependencies(Osbandetal.,2022). While
deepensemblesareatypicalbaselineforcapturinguncertainty,BDLmethodsbasedonthelast-layerLaplaceapproximation
canoutperformdeepensemblesinthequalityofjointmultivariatepredictions(Antoranetal.,2023). Developingmethods
thatachievetrade-offsbetweencomputationalcostandthequalityoftheirjointmultivariatepredictionsisanareawhere
furtherresearchisneeded(Osbandetal.,2023). AnotheractiveareaofresearchattheintersectionofRLandBDLaimsto
produceaccurateposteriorapproximationsofvaluefunctions(forexample,Qfunctions)givendatafrominteractionswith
anenvironment(Janzetal.,2019). ThissettingisdifferentfromtypicalBayesiansupervisedlearningas,inthiscase,the
outputofvaluefunctionsisnotdirectlyobserved,andonlyrewardsareavailable.
Domain-specificBDLmodels. TherearemanyopportunitiestodevelopBayesianmethodsincombinationwithdeep
learning models that are tailored for specific domains, taking into account the characteristics of the data and the tasks
involved. Thiscaninvolveexploringhierarchicalmodels, transferlearning, ormeta-learningapproaches. Anexample
is molecular property prediction, where many different datasets are available, but each of them has limited available
data(Klarneretal.,2023). Thereisscopetocombinedeeplearningmodelsthatlearnmolecularfeaturerepresentations
withBayesianmethodsthatreceivethoserepresentationsasinputs. Thelattermethodscancaptureuncertaintyandmake
predictionsindata-limitedsettingsforeachindividualtask,whilethedeeplearningfeaturesaresharedacrosstasks.
18