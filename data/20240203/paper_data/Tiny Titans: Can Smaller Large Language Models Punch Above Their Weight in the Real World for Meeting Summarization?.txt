Tiny Titans: Can Smaller Large Language Models Punch Above Their
Weight in the Real World for Meeting Summarization?
Xue-YongFu∗,MdTahmidRahmanLaskar∗
ElenaKhasanova,ChengChen,ShashiBhushanTN
DialpadInc.
Vancouver,BC,Canada
{xue-yong,tahmid.rahman,elena.khasanova,cchen,sbhushan}@dialpad.com
Abstract However,despitetheeffectivenessofLLMsin
summarization,deployingLLMsintherealworld
LargeLanguageModels(LLMs)havedemon-
togeneratemeetingsummarieswouldalsoleadto
stratedimpressivecapabilitiestosolveawide
anincreaseinproductioncosts. Whilefine-tuning
range of tasks without being explicitly fine-
smallerlanguagemodels(Raffeletal.,2020),such
tunedontask-specificdatasets. However,de-
ployingLLMsintherealworldisnottrivial,as asBART(Lewisetal.,2020),Pegasus(Zhangetal.,
itrequiressubstantialcomputingresources. In 2020),etc. ledtostate-of-the-artresultsacrossvar-
thispaper,weinvestigatewhethersmaller,com- ioussummarizationdatasets,thesemodelsrequire
pactLLMs1areagoodalternativetothecom-
largeannotateddatasetsformodeltraining,which
parativelyLargerLLMs2toaddresssignificant
areoftendifficulttoobtaininreal-worldbusiness
costsassociatedwithutilizingLLMsinthereal scenarios. Moreover,thesesmallerlanguagemod-
world.Inthisregard,westudythemeetingsum-
elsalsodonothaveinstruction-followingcapabil-
marization task in a real-world industrial en-
ities (Zhang et al., 2023). Thus, they cannot be
vironmentandconductextensiveexperiments
by comparing the performance of fine-tuned trained to properly follow specific instructions if
compactLLMs(e.g.,FLAN-T5,TinyLLaMA, thereisachangeinuserrequirements.
LiteLLaMA)withzero-shotlargerLLMs(e.g., GPT-4 (OpenAI, 2023) is an LLM proposed
LLaMA-2, GPT-3.5, PaLM-2). We observe by OpenAI which is widely considered the best-
thatmostsmallerLLMs,evenafterfine-tuning,
performingLLMcurrentlyavailable(Changetal.,
fail to outperform larger zero-shot LLMs in
2023). GPT-4 generated responses are also used
meetingsummarizationdatasets. However, a
to fine-tune various LLMs that are significantly
notableexceptionisFLAN-T5(780Mparam-
eters), which performs on par or even better smaller in size in comparison to it (Peng et al.,
than many zero-shot Larger LLMs (from 7B 2023). SinceusingtheGPT-4APIsignificantlyin-
toabove70Bparameters),whilebeingsignifi- creasestheAPIusagecost(Laskaretal.,2023b),it
cantlysmaller. ThismakescompactLLMslike isoftennotpracticaltouseinreal-worldscenarios.
FLAN-T5asuitablecost-efficientsolutionfor
In this regard, this paper studies whether com-
real-worldindustrialdeployment.
pact/smallerLLMscanbefine-tunedinawaythat
canmimictheperformanceofGPT-4,whilealso
1 Introduction
significantlyreducingthedeploymentcostofusing
The instruction following capabilities have made LLMs in production for meeting summarization.
itpossibleforLLMstoachieveimpressiveperfor- Morespecifically,thispaperaimstoprovideacom-
manceinzero-shotscenarios(Laskaretal.,2023a; prehensive analysis of various smaller and larger
Qinetal.,2023;Bangetal.,2023),whichhasalso LLMs,whichincludeslargerLLMslikeGPT-3.5
led to an increase in using LLMs to solve real- (i.e.,ChatGPT3),PaLM-2(Google,2023),LLaMA-
worldproblems. Forinstance,intaskslikemeeting 2(Touvronetal.,2023b),aswellassmallerLLMs
summarization, LLMs have been widely utilized likeFLAN-T5(Chungetal.,2022),TinyLLaMA
in recent times due to their impressive zero-shot (Zhangetal.,2024),etc.
performance(Laskaretal.,2023b). Ourexperimentalresultsshowthatmostsmaller
LLMs, even after fine-tuning, fail to outperform
*EqualContributions.SortedbytheLastName.
1LLMsthathavelessthan2Bparametersarereferredto largerzero-shotLLMsinmeetingsummarization
asCompactLLMsinthiswork. datasets. However, a notable exception is a fine-
2LLMsthathaveatleast7Bparametersarereferredtoas
LargerLLMsinthiswork. 3https://openai.com/blog/chatgpt
4202
beF
1
]LC.sc[
1v14800.2042:viXratuned FLAN-T5-Large, which achieves perfor- thebest-performingLLMintermsofvariouseval-
manceonparwithmuchlargerLLMs(from7Bto uationbenchmarks. However,theAPIcostofus-
morethan70B)usedinzero-shotsettings,whilebe- ingGPT-4issignificantlyhigherthanofanyother
ingsignificantlysmaller. ThismakessmallerLLMs LLMs(Laskaretal.,2023b). Whilefine-tunedver-
like FLAN-T5 a suitable cost-efficient LLM for sionsoflessexpensiveclosed-sourceLLMscould
real-worlddeployment. Ourextensiveexperiments reach performance comparable to GPT-4, using
wouldgiveinsightsintothecost-effectiveutiliza- fine-tuned versions of these LLMs for inference
tion of LLMs for summarizing business meeting significantlyincreasestheAPIcost4. Sincethese
transcripts. Below,wesummarizeourmajorcon- closed-source LLMs are only available through
tributionsinthispaper: APIs,theyposepotentialprivacyrisks.
(1) We conduct an extensive evaluation of To mitigate the above issues, various open-
smallerLLMsandcomparetheirperformancewith sourceLLMshavebeenproposed(Touvronetal.,
larger LLMs in several meeting summarization 2023a,b;Jiangetal.,2023,2024). Someofthema-
datasets to address several limitations of using joradvantagesofusingopen-sourceLLMsare: (i)
LLMsintherealworld. theyareavailableforin-housedeployment,(ii)they
(2) To ensure a fair evaluation and address the canbefine-tunedtoachieveperformancecompara-
possibilityofdatacontamination,weutilize(i)one bletolargerclosed-sourceLLMs,andfinally,(iii)
real-worldAutomaticSpeechRecognition(ASR)- theinferencecostofusingbothzero-shotandfine-
generatedtranscriptiondatafromreal-worldbusi- tuned versions are the same. Thus, open-source
nessmeetings,and(ii)constructedanewversionof LLMs could be a good alternative that addresses
theQMSUM(Zhongetal.,2021)datasetwherethe thelimitationsofclosed-sourceLLMs.
referencesummariesarere-generatedtokeepthem However,deploymentoftheopen-sourceLLMs
similar to our production requirement (this also in a way that ensures customer satisfaction, i.e.,
helpsusavoidthepossibilityofdatacontamination high accuracy with low latency, would require
inLLM-generatedresponses). expensive computing resources such as powerful
(3)Finally,wedemonstratetheadvantageofde- GPUs with large memory capacity. In addition,
ployingsmallerLLMsforreal-worldusagebased fine-tuning larger LLMs also requires scarce and
on the analysis of performance (accuracy and la- costlycomputingresourceswhichmaynotbeavail-
tency),inferencecost,andcomputationalresource able in many industries. While various optimiza-
requirements. tiontechniques(Wanetal.,2023)likelow-bitquan-
tization(Frantaretal.,2022;Dettmersetal.,2023),
2 RelatedWork parameter-efficient fine-tuning (Hu et al., 2021),
etc. have been proposed recently to address the
Fine-tuninglanguagemodels(Lewisetal.,2020;
computational limitations, they often come with
Zhang et al., 2020; Raffel et al., 2020) based on
other issues, such as a drop in accuracy and an
thetransformerarchitecture(Vaswanietal.,2017)
increaseinlatency.
hasledtostate-of-the-artperformanceinvarious
Inthispaper,weaimtoaddresstheseissuesby
summarizationdatasets. Sincethesetransformer-
studyingwhetherwecanfine-tunesmallerLLMs
based language models require domain-specific
with instruction-following capabilities to mimic
fine-tuning for best results, obtaining in-domain
the performance of larger LLMs such as GPT-4
labeled data in real-world settings is not trivial.
whileensuringlowlatencywithminimizedinfer-
However,thenotablezero-shotabilitiesofLLMs
encecost.
in summarization (Laskar et al., 2023b) have at-
tractedattentionfortheirpotentialuseinpractical 3 OurMethodology
summarizationsystemswherein-domainlabeled
Theobjectiveofthisresearchistostudywhether
datasetsarenotavailable.
instruction-followingLLMsthataresmallerinsize
While zero-shot LLMs have demonstrated im-
can be effectively utilized in a real-world system
pressive performance in tasks that lack large an-
formeetingsummarizationtoensureperformance
notated datasets (Laskar et al., 2023a; Qin et al.,
comparable to the state-of-the-art larger LLMs
2023;Bangetal.,2023;Jahanetal.,2023),utiliz-
ingLLMsintherealworldalsohasseverallimita-
4https://openai.com/blog/
tions. Forinstance,GPT-4iscurrentlyregardedas gpt-3-5-turbo-fine-tuning-and-api-updateswhileminimizingtheinferencecost. Forthispur- 4.1.1 LargerZero-ShotLLMs
pose, we select LLMs that have fewer than 2B
GPT-3.5: It is an autoregressive LLM that lever-
parametersasthetargetedcompactLLMsforper-
agesreinforcementlearningfromhumanfeedback
formanceanalysis. Moreover,inreal-worldmeet-
(RLHF)mechanism. Itisthefirstbackbonemodel
ingsummarizationscenarios,usersmayhavedif-
behindChatGPTandobtainsimpressivezero-shot
ferent requirements for the LLMs. For instance,
performance across various tasks (Laskar et al.,
someusersmayprioritizemeetingsummariesthat
2023a). Weusethegpt-3.5-turbo-0613modelwith
are detailed and comprehensive, whereas others thedefaultparametersfromOpenAI5.
maypreferthemeetingsummariestobeshortand
PaLM-2: PaLM-2 is an LLM (Google, 2023)
concise. In such cases, the instruction following
developedbyGoogle. Itleveragesthemixtureof
capability is important for the LLMs that would
objectives technique (Google, 2023) and signifi-
be deployed in production such that they can ful-
cantlyoutperformstheoriginalPaLM(Chowdhery
fillvariationsinuserdemands. Therefore,inthis
et al., 2022) model. We use the text-bison@002
paper,wealsoevaluatetheperformanceofLLMs model in Google’s VertexAI6 with the default pa-
basedonadiversesetofinstructionstogenerate(i)
rametersforPaLM-2.
LongSummary,(ii)MediumLengthSummary,and
LLaMA-2: LLaMA-2 (Touvron et al., 2023b)
(iii)ShortSummary. WefollowtheworkofLaskar
isanopen-sourceLLMdevelopedbyMeta. One
etal.(2023b)forpromptconstructionandusetheir
majoradvantageofLLaMA-2overthepreviously
SummarizationviaTruncationapproachforeach
mentioned LLMs is that it is open-sourced and
typeofinstruction. Belowaretheexamplesofthe
available for both research and commercial pur-
promptsforeachcase.
poses. Inthispaper,weusetherespectiveChatver-
Long: Generatealonganddescriptivesummary sionsofLLaMA-2forallofitsvariations: 7B,13B,
ofthefollowingconversation. and 70B from HuggingFace7 (Wolf et al., 2020)
Medium: Generateasummaryofthefollowing withthedefaultparametersforinference.
conversation. Mixtral-8x-7B:TheMixtral8x7B(Jiangetal.,
Short: Generateaveryshortandconcisesum- 2024)isaSparseMixtureofExperts(SMoE)lan-
maryofthefollowingconversation. guage model which has the same architecture as
Mistral7B(Jiangetal.,2023),butwiththediffer-
encethateachlayeriscomposedof8feedforward
4 Experiments
blocks or experts. This architectural change has
made it possible for each token to have access to
In this section, we first present our models along
47B parameters while using only 13B active pa-
withtheirimplementationdetails. Next,wedemon-
rametersduringinference. Weuseitforzero-shot
stratethedatasetsweusedforevaluation. Finally,
evaluationwithitsdefaultparameters.
wedemonstrateourexperimentalfindings.
4.1.2 SmallerFine-TunedLLMs
4.1 Models FLAN-T5: FLAN-T5 (Chung et al., 2022) is an
extension of the T5 (Raffel et al., 2020) model.
WeusethreecompactLLMsthathavelessthan2B
The T5 model treats each task as a sequence-
parameters and compare their performance with
to-sequence problem. While the architecture of
various larger LLMs (having at least 7B parame-
FLAN-T5 is similar to the original T5 model, it
ters). In the case of larger LLMs, some of them
leverages instruction fine-tuning instead of tradi-
are closed-source (e.g., GPT-3.5, PaLM-2, etc.).
tionalfine-tuning. Weuseits80Mparametersmall,
When we use these closed-source LLMs, we use
250Mparameterbase,and780Mparameterlarge
theirrespectiveAPIs. Open-sourcelargerLLMsas
versions from HuggingFace8 in our experiments
wellasallofthecompactLLMsareimplemented
with the learning rate set to 2e − 5. We run 10
usingtheHuggingFacelibrary(Wolfetal.,2020).
To ensure fair evaluation, all experiments for the 5https://platform.openai.com/docs/models/
open-source LLMs were conducted on the same 6https://cloud.google.com/vertex-ai/docs/
generative-ai/model-reference/text
computingplatformwith8NVIDIAA100(80GB)
7https://huggingface.co/meta-llama
GPUs. Below, we describe the models that we
8https://huggingface.co/docs/transformers/
studyinthiswork. model_doc/flan-t5In-DomainDataset QMSUM-IDataset (ii)TheQMSUM Filtered dataset: Weusethefil-
Type Train/Test Train/Test teredversion(Laskaretal.,2023b)oftheQMSUM
No.ofSamples 1360/157 486/111 dataset(Zhongetal.,2021)togeneratethemeeting
Avg.WordsPerTranscript 600/620 8947/9461
Avg.WordsPerSummary(Overall) 88/87 333/335 summaries. Since this dataset is not instruction-
Avg.WordsPerSummary(Long) 122/122 532/523
Avg.WordsPerSummary(Medium) 76/77 303/307 focused, we regenerate the reference summaries
Avg.WordsPerSummary(Short) 60/61 170/173
usingGPT-4withthreetypesofinstructions: Long,
Table1: EvaluationDatasetStatistics. Medium, and Short. Due to the variation in sum-
maryinstructions,ourinstruction(I)focusedver-
sionofQMSUM,denotedasQMSUM-I,contains
epochsforFLAN-T5-Largeand20epochsforBase
3 times more instances than the original filtered
andSmall.
version.
TinyLLama: TinyLlama(Zhangetal.,2024)is
acompact1.1Bparameterlanguagemodelthatis 4.3 ResultsandDiscussions
builtonthearchitectureofLlama-2(Touvronetal.,
Forperformanceevaluation,weuseROUGE-1,2,
2023b). Itispre-trainedonaround1trilliontokens
L (R-1, R-2, R-L) (Lin, 2004) as our evaluation
and leverages various techniques (e.g. FlashAt-
metrics. Below,wepresentourfindings.
tention (Dao et al., 2022; Dao, 2023)) to achieve
bettercomputationalefficiency. Wefine-tuneitfor 4.3.1 PerformanceonBenchmarkDatasets
10epochswiththelearningrateof1e−5. Weshowtheresultsforbothzero-shotLLMsand
LiteLLama: LiteLLaMA9 isa460Mparameter fine-tuned compact LLMs in Table 2. Below, we
LLMthatisalsodevelopedbasedonthearchitec- summarizeourobservations:
ture of LLaMA-2 and trained over 1T tokens on (i)Wefindthatinbothdatasets,FLAN-T5-Large
partoftheRedPajama10 datasets. Wefine-tuneit is the best-performing fine-tuned smaller LLM.
for20epochswiththelearningrateof2e−5. WhereasMixtral-8x7Bisthebest-performingzero-
shotmodelamongthelargerLLMs.
4.2 Datasets
(ii)WefindthattheROUGEscoresofallmodels
While one of our objectives is to build an LLM- arequitelowerintheQMSUM-Idatasetincompar-
based meeting summarization system that has isontoourin-domaindataset. Thisisexpectedin
instruction-followingcapabilitiesforreal-worldus- thecaseofthefine-tunedmodelssincethesizeof
age,therearenomeetingsummarizationdatasets thetrainingsetintheQMSUM-Idatasetismuch
currentlyavailablehavingdifferentgoldreference smallerthanourIn-Domaindataset.
summariescorrespondingtodifferentinstructions (iii)Inzero-shotsettings,wefindthatgenerally,
suchasvaryingsummarylengthsorformats. Thus, theperformanceofGPT-3.5andPaLM-2arecom-
toevaluatetheperformanceofvariousLLMs,we parabletoMixtral. However, LLaMA-2-70Bnot
constructedtwodatasets: (i)onedatasetisbased onlyfailstooutperformtheselargermodels,italso
onourproprietaryin-domainbusinessconversation fails to outperform its smaller variations in both
transcripts,and(ii)theotherleveragesanacademic datasetsinseveralscenarios.
dataset. Below,wedescribethesedatasets(alsosee (iv)Inthecaseofthefine-tunedLLMs,wefind
Table1formoredetails). thatexceptFLAN-T5-Large,thelargerfine-tuned
modelsperformmuchbetterthansmallerones. For
(i) In-Domain dataset: This is a dataset con-
instance, TinyLLaMA-1.1B usually outperforms
structedfromreal-worldbusinessmeetings. Since
LLMs that are smaller in size than it. However,
GPT-4isfoundtobethebestperformingLLMin
itfailstooutperformFLAN-T5-Largewhichhas
a wide range of tasks including meeting summa-
about300Mfewerparameters.
rization(Laskaretal.,2023b),alongsideitsimpres-
(v) In the case of FLAN-T5 models, we find
sivecapabilityasanannotator(Pengetal.,2023),
thattheFLAN-T5-Large-780Msignificantlyout-
weuseittogeneratethereferencesummariesde-
pendingontheLong,Medium,andShortsummary performsitssmallervariants: 80Mand250M.
instructions. (vi)WhileFLAN-T5-Large-780Machievesthe
bestresultinourIn-Domaindataset(incomparison
9https://huggingface.co/ahxt/
to both larger zero-shot LLMs as well as smaller
LiteLlama-460M-1T
fine-tuned LLMs), it could not achieve the best
10https://huggingface.co/datasets/
togethercomputer/RedPajama-Data-1T performanceintheQMSUM-IdatasetsinceitfailsIn-DomainDataset QMSUM-IDataset
Models ROUGE-1 ROUGE-2 ROUGE-L ROUGE-1 ROUGE-2 ROUGE-L
GPT-3.5(Zero-Shot) 49.55 24.61 36.12 38.63 13.17 21.83
PaLM-2-text-bison@002(Zero-Shot) 48.32 23.61 35.59 39.76 12.29 21.14
LLaMA-2-7B(Zero-Shot) 47.37 20.41 30.93 35.67 10.14 18.57
LLaMA-2-13B(Zero-Shot) 47.07 21.37 31.58 32.93 9.69 18.06
LLaMA-2-70B(Zero-Shot) 46.55 20.42 32.02 33.85 9.50 18.23
Mixtral-8x7B(Zero-Shot) 51.99 25.76 36.86 40.70 13.29 21.96
TinyLLaMA-1.1B(Fine-Tuned) 50.17 22.38 33.66 23.97 6.06 16.59
LiteLLaMA-460M(Fine-Tuned) 42.64 15.31 26.95 16.66 3.80 11.43
FLAN-T5-Small-80M(Fine-Tuned) 21.19 8.13 16.74 20.18 4.49 16.1
FLAN-T5-Base-250M(Fine-Tuned) 34.44 14.36 25.33 30.41 9.45 20.24
FLAN-T5-Large-780M(Fine-Tuned) 56.14 29.42 41.11 34.03 11.31 20.92
Table2: PerformanceofLLMsontheIn-DomainandQMSUM-Idatasets.
to outperform much larger zero-shot LLMs like In-Domain QMSUM-I
GPT-3.5,PaLM-2,andMixtral-8x7b(eventhough
Model R-1 R-2 R-L R-1 R-2 R-L
itsperformanceisonparorbetterthanLLaMA-2
FLAN-T5-Large 56.14 29.42 41.11 34.03 11.31 20.92
modelsinvariousmetrics). LLaMA-2-7B 57.09 30.42 41.68 42.77 13.93 22.16
(vii) As an explanation of the performance of LLaMA-2-13B 58.92 32.70 44.04 43.86 14.39 22.58
FLAN-T5-Large,itshouldbenotedthatweusethe
Table3: CaseStudy:Fine-TuningSmallerandLargerLLMs.
defaultcontextlengthof2048tokensforFLAN-T5
sinceourobjectiveistobuildanefficientsumma-
rizationmodelfordeploymentinaspecificindus-
experimentalresultsinTable3andfindthatfine-
try. Since the average transcript length in our in-
tuningledtoLLaMA-2models(both13Band7B)
domaindatasetisabout600words,mostpartsof
outperforming FLAN-T5-Large in both datasets,
thetranscriptinourin-domaindatasetcanbecov-
withtheimprovementinQMSUM-Iisbyalarge
eredwithinthecontextwindowofFLAN-T5mod-
margin. The larger difference in performance in
els. However,thisdefaultcontextlengthisabout
QMSUMcanbeattributedtothelongertranscripts
5 times lower than the average transcript length
in QMSUM-I where the longer sequence length
inQMSUM-I,whichcouldbethepossiblereason
(contextlengthof4ktokens)inLLaMA-2models
behind its comparatively poorer performance on
couldbemoresuitablethanthecontextlengthof
QMSUM-I.Thisindicatesthatindatasetsthathave
2048tokensinFLAN-T5-Large. Nonetheless,the
smallercontextlengths,FLAN-T5-Largecouldbe
improvementsforfine-tunedLLaMA-2modelsin
veryuseful. Nonetheless,tofurtherimproveperfor-
ourIn-Domaindatasetarequitenarrow.
manceindatasetsthathavelargermeetinglengths
whileensuringlimitedcomputationalusage,other
(ii)CaseStudyonInstructionVariations: Here,
approachessuchasSummarizationviaChapteriza-
we study the performance of some LLMs in
tion(Laskaretal.,2023b)canbeinvestigated.
terms of the variations in instructions. For the
4.3.2 CaseStudies casestudy,weusethebest-performingFLAN-T5-
In this section, we conduct some case studies to Large and compare it with two zero-shot larger
further investigate the performance of the best- LLMs, one API-based: GPT-3.5, and one open-
performingsmallerfine-tunedLLM:theFLAN-T5- source: LLaMA-2-7B11. We find that on our In-
Largemodel. Below,wedemonstrateourfindings: Domain dataset, FLAN-T5-Large performs bet-
ter in Medium summaries, whereas GPT-3.5 and
(i) Case Study on Fine-Tuning Performance: LLaMA-2-7B are better in Short and Long sum-
Since FLAN-T5 performed on par or even better maries, respectively. In QMSUM-I, we find that
thanthezero-shotLLaMA-2modelsinourprevi- theperformanceinMediumsummariesisthebest
ousexperiment,inthissection,weconductacase forallLLMs.
studytocompareitsperformancewiththeLLaMA-
2-7BandLLaMA-2-13bmodelsthatarefine-tuned
11WeselectLLaMA-2-7Bsinceitisthesmallestoneamong
for3epochswithlearningrate2e−5. Weshowour allzero-shotLLMs,makingitmoresuitablefordeployment.Figure1: AverageROUGEscoresbasedontheinstructiontypesforFine-Tuned(FT)andZero-Shot(ZS)LLMs.
In-Domain QMSUM-I is0.00025$per1Kcharactersand0.0002$per1K
outputcharacters. Approximately,1tokeniscon-
Model F C FC F C FC
sideredas4characters. Thus,thecostforPaLM-2
FLAN-T5-Large-FT 4.7 4.6 4.4 3.1 2.8 3.4
GPT-3.5-ZS 5.0 3.9 4.5 4.1 3.8 3.9 is 0.0010$ per 1K input tokens and 0.0008$ per
LLaMA-2-7B-ZS 4.8 3.5 3.3 3.8 3.4 3.9 1Koutputtokens,makingitslightlycheaperthan
GPT-3.5. Regarding LLaMA-2-7B, it requires at
Table 4: Human Evaluation Results in terms of Fluency
least a machine with 1 NVIDIA L4 GPU (24GB
(F),Coherence(C),andFactualConsistency(FC).Here,‘FT’
denotes‘Fine-Tuned’,‘ZS’denotes‘Zero-Shot’. VRAM),whiletheLLaMA-2-13Bmodelrequires
2L4GPUs(48GBVRAM).Onthecontrary, the
significantlysmallerFLAN-T5-Largeconsumesno
4.3.3 HumanEvaluationResults
morethan14GBVRAM.Thus,itcanberunona
To provide more insights on LLM performance,
muchcheaperGPU.
we conduct a human evaluation to rate the LLM-
InferenceSpeed: Wealsomeasuretheinference
generatedsummariesonascaleof1to5intermsof
speed of different LLMs in a machine with 1 L4
Fluency,Coherence,andFactualConsistency. We
GPU. Based on our evaluation in our In-Domain
comparethebest-performingsmallerLLM:FLAN-
dataset, we find that, on average, LLaMA-2-7B
T5-Large with two zero-shot baselines: GPT-3.5
takes10.4secondspertranscript,whilethesmaller
andLLaMA-2-7B.FromtheresultsinTable4,we
LLM, FLAN-T5-Large, only takes an average of
find that similar to the performance in terms of
3.2seconds.
ROUGE scores, all LLMs generally achieve bet-
ter performance on our In-Domain dataset than
6 Conclusion
theQMSUM-Idataset. Wealsofindthatonaver-
age,theperformanceofFLAN-T5-Largeisbetter
Inthispaper,ourextensivestudyinvolvingvarious
thanGPT-3.5andLLaMA-2-7BonourIn-Domain
LLMs led to several key insights on building an
dataset. MuchlongermeetingsintheQMSUM-I
efficient meeting summarization system for real-
datasetcouldbethereasonbehindFLAN-T5-Large
worldusage. WhilemostlargerLLMsusuallyout-
performingpoorlyonthisdataset.
perform their smaller counterparts, we find that
FLAN-T5-Largeisanexceptioninthisregard. On
5 UsingLLMsinReal-WorldSystems
ourIn-Domaindataset,withonly780Mparameters,
To deploy LLMs in the real world, we study the FLAN-T5-Largenotonlyoutperformslargerzero-
followingaspects: cost/GPU andinferencespeed. shotLLMs,butalsoitachievescomparableperfor-
Cost/GPU: As of the time of writing this pa- mance with larger fine-tuned LLMs. This makes
per, the pricing12 in OpenAI for the GPT series FLAN-T5-Large more suitable for real-world us-
models are as follows: the 4K context version of age,especiallyinscenarioswherethemeetingsare
GPT-3.5 that we use costs 0.0015$ per 1K input nottoolong. SincetheperformanceofFLAN-T5-
tokens and 0.002$ per 1K output tokens. Mean- Large is still quite below in comparison to other
while,forPaLM-2,thepricing13 inGoogleCloud largerLLMsonQMSUM-Idatasetthathaslonger
meetings, future work should investigate the per-
12https://openai.com/pricing,accessed:01/25/2024.
formanceofFLAN-T5byapplyingvariouschap-
13https://cloud.google.com/vertex-ai/pricing,ac-
cessed:01/25/2024. terizationtechniques(Laskaretal.,2023b).Limitations summaryisrequiredtobegeneratedbasedononly
the given transcript. Meanwhile, the LLM that
One of the limitations of this work is that only
would be used in production for summarization
threetypesofinstructionswereutilized. Thus,in
will only do inference but will not be re-trained
thefuture,LLMsshouldbeevaluatedacrossmore
on live meeting transcripts. Only the users of a
instructions.
particularmeetingwillhaveaccesstothesummary.
AnotherlimitationofthisworkisthattheGPT-
Thus,informationfromanyothermeetingswillnot
4generatedsummarieswereutilizedasreference
berevealedtotheusers.
summariesinsteadofhumanannotations. Nonethe-
less, one of the major focuses of this work is to HumanEvaluation: Additionalcompensations
ensure the efficient development of a real-world werenotrequiredforthehumanevaluationsince
meeting summarization system. Since there is a itwasconductedbyin-housefull-timeemployees
lack of in-domain annotated datasets, we investi- havingexpertiseincomputationallinguistics.
gatetheperformanceofdifferentLLMstomimic
theperformanceofGPT-4andsoGPT-4generated
References
responses are utilized as the gold reference sum-
maries. However,futureworkshouldevaluatethe YejinBang,SamuelCahyawijaya,NayeonLee,Wen-
quality of GPT-4 generated summaries based on liangDai,DanSu,BryanWilie,HolyLovenia,Ziwei
Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan
humanevaluation.
Xu,andPascaleFung.2023. Amultitask,multilin-
Anotherlimitationthatshouldbepointedoutis
gual,multimodalevaluationofchatgptonreasoning,
thattheperformanceofLLMsthatwereevaluated hallucination,andinteractivity.
was based on truncating the transcript to the first
Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,
N tokensthatcanbecoveredbythemaximumse-
Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi,
quence length of the respective LLM. While this
CunxiangWang,YidongWang,etal.2023. Asur-
is done since the motivation of this work was to veyonevaluationoflargelanguagemodels. arXiv
buildanefficientsummarizationsystemthatmay preprintarXiv:2307.03109.
reducetheproductioncostinareal-worldindustrial
AakankshaChowdhery,SharanNarang,JacobDevlin,
environment(notethatourin-domaindatasetalso Maarten Bosma, Gaurav Mishra, Adam Roberts,
hasshortermeetings),futureworkshouldinvesti- Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, et al. 2022. Palm: Scaling
gatetheperformanceofsmallerLLMsbyapplying
language modeling with pathways. arXiv preprint
variouschapterizationtechniques.
arXiv:2204.02311.
Finally, studying the effects of the size of the
datasets used for fine-tuning smaller LLMs were HyungWonChung,LeHou,ShayneLongpre,Barret
Zoph,YiTay,WilliamFedus,YunxuanLi,Xuezhi
leftoutofthescopeofthisworkandwillneedto
Wang,MostafaDehghani,SiddharthaBrahma,etal.
beconsideredinfutureresearch.
2022. Scalinginstruction-finetunedlanguagemodels.
arXivpreprintarXiv:2210.11416.
EthicsStatement
TriDao.2023. Flashattention-2: Fasterattentionwith
License: We maintained the licensing require- better parallelism and work partitioning. arXiv
mentsaccordinglywhileusingdifferenttoolsfrom preprintarXiv:2307.08691.
theproviders(e.g.,OpenAI,Google,Meta,Mistral,
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and
HuggingFace). Christopher Ré. 2022. Flashattention: Fast and
memory-efficientexactattentionwithio-awareness.
Privacy: Toprotectuserprivacy,sensitivedata
AdvancesinNeuralInformationProcessingSystems,
such as personally identifiable information (e.g., 35:16344–16359.
creditcardnumber,phonenumber,personnames)
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and
were removed while constructing the In-Domain
LukeZettlemoyer.2023. Qlora: Efficientfinetuning
datasets. ofquantizedllms. arXivpreprintarXiv:2305.14314.
IntendedUse: Notethatourmodelisintended Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and
to provide business organizations with a quick Dan Alistarh. 2022. Gptq: Accurate post-training
quantizationforgenerativepre-trainedtransformers.
overviewofthemeetings. Whilepoorsummariza-
arXivpreprintarXiv:2210.17323.
tion quality may lead to a bad user experience, it
should not lead to any ethical concern since the Google.2023. Palm2technicalreport. GooleAI.Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan ColinRaffel,NoamShazeer,AdamRoberts,Katherine
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Lee,SharanNarang,MichaelMatena,YanqiZhou,
and Weizhu Chen. 2021. Lora: Low-rank adap- WeiLi,andPeterJLiu.2020. Exploringthelimits
tation of large language models. arXiv preprint oftransferlearningwithaunifiedtext-to-texttrans-
arXiv:2106.09685. former. TheJournalofMachineLearningResearch,
21(1):5485–5551.
Israt Jahan, Md Tahmid Rahman Laskar, Chun Peng,
and Jimmy Huang. 2023. A comprehensive eval- HugoTouvron,ThibautLavril,GautierIzacard,Xavier
uation of large language models on benchmark Martinet,Marie-AnneLachaux,TimothéeLacroix,
biomedical text processing tasks. arXiv preprint BaptisteRozière,NamanGoyal,EricHambro,Faisal
arXiv:2310.04270. Azhar, et al. 2023a. Llama: Open and effi-
cient foundation language models. arXiv preprint
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men- arXiv:2302.13971.
sch,ChrisBamford,DevendraSinghChaplot,Diego
delasCasas,FlorianBressand,GiannaLengyel,Guil- Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
laumeLample,LucileSaulnier,etal.2023. Mistral bert, Amjad Almahairi, Yasmine Babaei, Nikolay
7b. arXivpreprintarXiv:2310.06825. Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
Bhosale, et al. 2023b. Llama 2: Open founda-
Albert Q Jiang, Alexandre Sablayrolles, Antoine tion and fine-tuned chat models. arXiv preprint
Roux,ArthurMensch,BlancheSavary,ChrisBam- arXiv:2307.09288.
ford,DevendraSinghChaplot,DiegodelasCasas,
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Emma Bou Hanna, Florian Bressand, et al. 2024.
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Mixtralofexperts. arXivpreprintarXiv:2401.04088.
Kaiser,andIlliaPolosukhin.2017. Attentionisall
Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur youneed. InAdvancesinNeuralInformationPro-
Rahman, MdAmranHossenBhuiyan, ShafiqJoty, cessingSystems30: AnnualConferenceonNeural
andJimmyHuang.2023a. Asystematicstudyand InformationProcessingSystems2017,December4-9,
comprehensiveevaluationofChatGPTonbenchmark 2017,LongBeach,CA,USA,pages5998–6008.
datasets. In Findings of the Association for Com-
Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam,
putational Linguistics: ACL 2023, pages 431–469,
YuZheng,ZhongnanQu,ShenYan,YiZhu,Quanlu
Toronto,Canada.AssociationforComputationalLin-
Zhang,MosharafChowdhury,etal.2023. Efficient
guistics.
large language models: A survey. arXiv preprint
arXiv:2312.03863.
MdTahmidRahmanLaskar,Xue-YongFu,ChengChen,
andShashiBhushanTN.2023b. Buildingreal-world
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
meetingsummarizationsystemsusinglargelanguage
Chaumond,ClementDelangue,AnthonyMoi,Pier-
models: A practical perspective. In Proceedings
ricCistac,TimRault,RémiLouf,MorganFuntowicz,
of the 2023 Conference on Empirical Methods in
et al. 2020. Transformers: State-of-the-art natural
NaturalLanguageProcessing: IndustryTrack,pages
languageprocessing. InProceedingsofthe2020con-
343–352,Singapore.AssociationforComputational
ference on empirical methods in natural language
Linguistics.
processing: systemdemonstrations,pages38–45.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan JingqingZhang,YaoZhao,MohammadSaleh,andPe-
Ghazvininejad,AbdelrahmanMohamed,OmerLevy, terLiu.2020. Pegasus: Pre-trainingwithextracted
Veselin Stoyanov, and Luke Zettlemoyer. 2020. gap-sentencesforabstractivesummarization. InIn-
BART:Denoisingsequence-to-sequencepre-training ternationalConferenceonMachineLearning,pages
fornaturallanguagegeneration,translation,andcom- 11328–11339.PMLR.
prehension. InProceedingsofthe58thAnnualMeet-
ingoftheAssociationforComputationalLinguistics, Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and
pages7871–7880,Online.AssociationforComputa- Wei Lu. 2024. Tinyllama: An open-source small
tionalLinguistics. languagemodel. arXivpreprintarXiv:2401.02385.
ShengyuZhang,LinfengDong,XiaoyaLi,SenZhang,
Chin-YewLin.2004. Rouge: Apackageforautomatic
evaluation of summaries. In Text summarization XiaofeiSun,ShuheWang,JiweiLi,RunyiHu,Tian-
branchesout,pages74–81. weiZhang,FeiWu,etal.2023. Instructiontuning
forlargelanguagemodels: Asurvey. arXivpreprint
OpenAI.2023. Gpt-4technicalreport. arXiv:2308.10792.
BaolinPeng,ChunyuanLi,PengchengHe,MichelGal- MingZhong,DaYin,TaoYu,AhmadZaidi,Mutethia
ley,andJianfengGao.2023. Instructiontuningwith Mutuma,RahulJha,AhmedHassan,AsliCelikyil-
gpt-4. arXivpreprintarXiv:2304.03277. maz,YangLiu,XipengQiu,etal.2021. Qmsum: A
newbenchmarkforquery-basedmulti-domainmeet-
ChengweiQin,AstonZhang,ZhuoshengZhang,Jiaao ingsummarization. InProceedingsofthe2021Con-
Chen,MichihiroYasunaga,andDiyiYang.2023. Is ferenceoftheNorthAmericanChapteroftheAsso-
chatgptageneral-purposenaturallanguageprocess- ciationforComputationalLinguistics: HumanLan-
ingtasksolver? arXivpreprintarXiv:2302.06476. guageTechnologies,pages5905–5921.