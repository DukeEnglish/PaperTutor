SymbolicAI: A framework for logic-based approaches
combining generative models and solvers
Marius–ConstantinDinu∗† ClaudiuLeoveanu–Condrei‡ MarkusHolzleitner†
WernerZellinger§ SeppHochreiter†
Abstract
WeintroduceSymbolicAI,aversatileandmodularframeworkemployingalogic-basedapproachto
conceptlearningandflowmanagementingenerativeprocesses. SymbolicAIenablestheseamless
integration of generative models with a diverse range of solvers by treating large language models
(LLMs) as semantic parsers that execute tasks based on both natural and formal language instruc-
tions,thusbridgingthegapbetweensymbolicreasoningandgenerativeAI.Weleverageprobabilistic
programming principles to tackle complex tasks, and utilize differentiable and classical program-
ming paradigms with their respective strengths. The framework introduces a set of polymorphic,
compositional, and self-referential operations for data stream manipulation, aligning LLM outputs
with user objectives. As a result, we can transition between the capabilities of various foundation
models endowed with zero- and few-shot learning capabilities and specialized, fine-tuned models
orsolversproficientinaddressingspecificproblems. Inturn,theframeworkfacilitatesthecreation
andevaluationofexplainablecomputationalgraphs. Weconcludebyintroducingaqualitymeasure
and its empirical score for evaluating these computational graphs, and propose a benchmark that
comparesvariousstate-of-the-artLLMsacrossasetofcomplexworkflows. Werefertotheempirical
scoreasthe”VectorEmbeddingforRelationalTrajectoryEvaluationthroughCross-similarity”,or
VERTEXscoreforshort. Theframeworkcodebase1andbenchmark2arelinkedbelow.
Abstraction Foundation Models
Neuro-Symbolic AI Spectrum
Modeling / Coding Prompting / Fine-Tuning
Software-Engineering Machine Learning
Programming / Learning
Implementation Specialist Models
Figure1: Ourneuro-symbolicframeworkenablesaseamlesstransitionbetweensymbolicanddifferentiableprogram-
ming, each with distinct dynamics and strengths. Differentiable programming provides access to foundational and
specialistmodels. Classicalprogramming,ontheotherhand,shiftsbetweenabstractionandimplementation,focusing
onhigh-levelconceptsbeforedelvingintothedetailsofimplementation.
∗ExtensityAI,ViennaandAIAustria,Vienna—Correspondingauthoremails: dinu@ml.jku.at,office@extensity.ai
†ELLISUnitLinzandLITAILab,InstituteforMachineLearning,JohannesKeplerUniversity,Linz
‡AmazonDevices,Timisoara–workdoneoutsideofAmazon
,
§JohannRadonInstituteforComputationalandAppliedMathematics,AustrianAcademyofSciences,Vienna
1SymbolicAIframeworkreleasedonJanuary20th,2023,onGitHub: https://github.com/ExtensityAI/symbolicai.
2EvaluationbenchmarkreleasedonFebruary1st,2024,onGitHub: https://github.com/ExtensityAI/benchmark.
1
4202
beF
1
]GL.sc[
1v45800.2042:viXra1 Introduction
The recent surge in generative AI, particularly involving large language models (LLMs), has demonstrated their
wide-rangingapplicabilityacrossvariousdomains(Badita,2022;Degrave,2022). Thesemodelshaveenhancedthe
functionality of tools for search-based interactions (Microsoft, 2023; YouWrite, 2022; Writesonic, 2022), program
synthesis(Jainetal.,2021;Romera-Paredesetal.,2023;Keyetal.,2023),chat-basedinteractions(ReplikaAI,2016;
OpenAI, 2022; Google, 2023), and many more. Moreover, language-based approaches have facilitated connections
betweendifferentmodalities,enablingtext-to-image(Rameshetal.,2021;Sahariaetal.,2022),text-to-video(Singer
etal.,2022),text-to-3D(Pooleetal.,2022),text-to-audio(Oordetal.,2016;Wangetal.,2017),andtext-to-code(Wang
etal.,2021b;Luetal.,2021;Lietal.,2022b)transformations,tonameafew. Therefore,bytrainingonvastquantities
ofunlabelledtextualdata,LLMshavebeenshowntonotonlystorefactualknowledge(Petronietal.,2019;Kassner
et al., 2020) and approximate users’ intentions to some extent (Andreas, 2022), but also to unlock deep specialist
capabilities through innovative prompting techniques (Nori et al., 2023). Yet, these applications merely scratch the
surfaceofthetransformationthatlanguage-basedinteractionsareexpectedtobringtohuman-computerinteractions
inboththenearanddistantfuture.
Inpart, instruction-basedfine-tuningofLLMsthroughreinforcementlearningfromhumanfeedback(Ouyangetal.,
2022; Li et al., 2023) or direct preference optimization (Rafailov et al., 2023) has shown promising results dealing
with value misalignment issues (Bradley Knox & Stone, 2008; MacGlashan et al., 2017; Christiano et al., 2017;
Ibarz et al., 2018; Goyal et al., 2022), unlocking new possibilities for chain of thoughts (Wei et al., 2022b), tree of
thoughts (Yao et al., 2023a), and graph of thoughts interactions (Besta et al., 2023). However, recent research also
highlights the limitations of LLMs in functional linguistic competence despite their proficiency in formal linguistic
competence (Mahowald et al., 2023). Whereas formal linguistic competence encompasses the ability to understand
andgeneratelanguage,functionallinguisticcompetencepertainstotheapplicationoflanguageinreal-worldcontexts,
suchasconveyingsensoryinputorrecallinginformationfrommemory. Examplesoffunctionallinguisticcompetence
include implicatures (Ruis et al., 2022) and contextual language comprehension beyond the statistical manifestation
ofdatadistributions(Bransford&Johnson,1972). Consequently,operatingLLMsthroughapurelyinference-based
approach confines their capabilities within their provided context window, severely limiting their horizon. This
resultsindeficienciesforsituationalmodeling,non-adaptabilitythroughcontextualchanges,andshort-termproblem-
solving,amongstothercapabilities. However,simplyincreasingthecontextlengthmaynotyieldgreatercapabilities,as
demonstratedbytheobservedU-shapedperformancecurve(Liuetal.,2023)whereLLMsexcelwhenusinginformation
atthebeginningorendoftheinputcontext,butstrugglewithinformationlocatedinthemiddle,especiallyascontext
increases. These challenges are actively being researched, with novel approaches such as Hyena (Poli et al., 2023),
RWKV(Bo,2021),GateLoop(Katsch,2023),andMamba(Gu&Dao,2023)surfacing. Meanwhile,there-emergence
of interest in retrieval-augmented generative approaches (Li et al., 2022a) offers an alternative by circumventing the
autoregressivenatureofthewidely-usedTransformerarchitecture(Vaswanietal.,2017),enablingcontextenrichment
withlateralinformation. Inparallel,effortshavefocusedondevelopingtool-basedapproaches(Schicketal.,2023)or
templateframeworks(Chase,2023)toextendlargeLLMs’capabilitiesandenableabroaderspectrumofapplications.
However,theseeffortsonlypartiallycapturethevastpotentialinherentinleveragingLLMsassemanticparsers.
Inlightoftheseconsiderations,weintroduceSymbolicAI,acompositionalneuro-symbolic(NeSy)frameworkableto
represent and manipulate compositional, multi-modal, and self-referential structures (Schmidhuber, 2007; Fernando
et al., 2023). SymbolicAI augments the generative process of LLMs with functional zero- and few-shot learning
operations and enables the creation of versatile applications through in-context learning (Wei et al., 2022a). These
operationsguidethegenerativeprocessandfacilitateamodulardesignwithawiderangeofexistingsolvers,including
formal language engines for mathematical expression evaluation, theorem provers, knowledge bases, and search en-
ginesforinformationretrieval. Itexposesthesesolversasbuildingblocksforconstructingcompositionalfunctionsas
computationalgraphs,andfacilitatesthedevelopmentofanextensibletoolkitthatbridgesclassicalanddifferentiable
programming paradigms, aiming to create domain-invariant problem solvers. In designing the architecture of Sym-
bolicAI,wedrewinspirationfromthebodyofevidencethatsuggeststhehumanbrainpossessesaselectivelanguage
processing module (Deniz et al., 2019; Hu et al., 2022; Menenti et al., 2011; Scott et al., 2016; Macsweeney, 2002;
Fedorenkoetal.,2010;Regevetal.,2013),priorresearchoncognitivearchitectures(Newell&Simon,1956;Newell
etal.,1957;Newell&Simon,1972;Newell,1990;Laird,2022),andthesignificanceoflanguageonthestructureof
semanticmapsinthehumanbrain(Huthetal.,2016). Weconsiderlanguageasacentralprocessingmodule,distinct
fromothercognitiveprocessessuchasreasoningormemory(Paischeretal.,2022,2023),thatdefinesasteppingstone
towardsbroadAIsystems(seeSectionB).
Lastly, alongside the framework, we introduce a benchmark and derive a quality measure and its empirical score to
addresstheevaluationofmulti-stepNeSygenerativeprocesses. SeeSection6formoredetails.
2Insummary,wehighlightthefollowingkeycontributionsofthiswork:
• WeintroduceSymbolicAI,alogic-basedframeworkforconceptlearningandflowmanagementingenerative
processes,enablingseamlessintegrationwithawiderangeoffoundationmodelsandsolvers.
• WecombinethebroadapplicabilityofLLMsassemanticparserswithsymbolicexpressionsbyleveraginga
modularprobabilisticprogrammingparadigm,facilitatingthecreationofcomplexcomputationalgraphs.
• Weintroduceaqualitymeasureanditsempiricalscorealongsideabenchmarkforcomparingstate-of-the-art
LLMsacrossawiderangeoftasks.
2 RelatedWork
Symbolic Methods The field of symbolic AI has its foundations in the works of the Logic Theorist (LT) (Newell
& Simon, 1956) and the General Problem Solver (GPS) (Newell et al., 1957). These programs represented the first
stepstowardsautomatedreasoningandproblem-solvingusingsymbolicrepresentations. Despitetheiradvancements,
both faced challenges in dealing with the complexity of real-world problems, particularly due to the combinatorial
nature of the solution space. To address these limitations, the Soar (Laird et al., 1987) cognitive architecture was
developed, advancing the notion that intelligent behavior results from goal-oriented search through a problem space
(Newell&Simon,1972;McCarthyetal.,2006),witheachstepconsistingofselectingandapplyingoperators. Soar
introducedcomponentslikereinforcementlearning,impasses,substates,andchunkingtoenhanceitsproblem-solving
capabilities. Italsodemonstratedtheimportanceoflearningfromexperiencestoadaptandimproveperformanceover
time. However,Santoroetal.(2022)emphasizesthesubjectivityofsymbolsandsuggeststhathuman-likesymbolic
fluencycoulddevelopinmachinesthroughlearningalgorithmsimmersedinsocio-culturalcontexts. Thisperspective,
anchored in the notion that symbols are triadic and their meaning emerges from consensus, seeks to move away
fromtraditionalsymbolicAImethodologiestowardsAIthatadaptivelylearnsmeaningandbehaviorsfromhuman-like
experiences. Thegoalistocultivatemachinesthatdemonstratesymbolicbehaviorsacrossaspectrumofcompetencies,
potentiallymirroringtheevolutionaryandsociallearningprocessesobservedinhumans. Lastly,symbolicAIstruggles
with real-world data’s unpredictability and variability. These challenges have led to the employment of statistical
learningmethodologies,likedeeplearning(Alometal.,2018),whicharemoreadeptatmanagingnoiseanduncertain
informationthroughvector-valuedrepresentations.
Sub-SymbolicMethods Thesub-symbolicframework,rootedinneuralnetworkparadigms,beganwithpioneering
works such as the perceptron (McCulloch & Pitts, 1943), with the first hardware implementation quickly following
(Rosenblatt, 1958). The foundational notion of distributed processing (Rumelhart et al., 1986) was later bolstered
andfurtherexpandedbydemonstratingthatmultilayerfeedforwardnetworkswithasinglehiddenlayercanserveas
universalapproximatorsforanyBorelmeasurablefunction,givensufficienthiddenunits(Horniketal.,1989). Fast-
forward, contemporary frameworks achieve a significant leap with the introduction of the Transformer architecture
(Vaswani et al., 2017), which underpins most of today’s LLMs. These LLMs demonstrate exceptional capabilities
inin-contextlearning,amethodpopularizedbythelikesofGPT-3(Brownetal.,2020),wheremodelsimprovetask
performancethroughnaturallanguageinstructionandexamplesprovideddirectlyintheinputprompt. Whilein-context
learningbypassestheneedforexplicitretraining,itdemandsmeticulouspromptdesigntosteermodelstowardsdesired
behaviors. Despitetheirversatility,currentLLMsfacechallengessuchasfallaciousreasoningandthegenerationof
erroneouscontent,commonlyreferredtoashallucinations(Jones&Steinhardt,2022). Theselimitationshighlightthe
importanceofintegratingcomplementarysymbolicmethodstovalidateandguidethegenerativeprocessesofLLMs,
ensuringmoreaccurateandreliableoutputs.
Neuro-Symbolic Methods To overcome the limitations of each individual method, NeSy approaches meld the
statistical inference strengths of deep neural architectures with the generalization and explainability of symbolic
systems(Besoldetal.,2017;Yuetal.,2023;Hamiltonetal.,2022;Garcezetal.,2015;d’AvilaGarcezetal.,2019;
d’Avila Garcez & Lamb, 2020; Lamb et al., 2020). Some approaches focus on different strategies for integrating
learningandreasoningprocesses(Yuetal.,2023;Fangetal.,2024). Firstly,learningforreasoningmethodstreatthe
learningaspectasanacceleratorforreasoning,inwhichdeepneuralnetworksareemployedtoreducethesearchspace
forsymbolicsystems(Qu&Tang,2019;Silveretal.,2016,2017b,a;Schrittwieseretal.,2020). Secondly,reasoning
forlearningviewsreasoningasawaytoregularizelearning,inwhichsymbolicknowledgeactsasaguidingconstraint
that oversees machine learning tasks (Hu et al., 2016; Xu et al., 2018). Thirdly, the learning-reasoning category
enablesasymbioticrelationshipbetweenlearningandreasoning. Here,bothelementsinteractandshareinformation
to boost problem-solving capabilities (Donadello et al., 2017; Manhaeve et al., 2018; Mao et al., 2019; Ellis, 2023).
Thissynergyfurtherextendswhenconsideringgraph-basedmethods, whichcloselyalignwiththeobjectivesofour
3proposedframework. Researchinthisarea,suchasCycleGT(Guoetal.,2020)andPaper2vec(Ganguly&Pudi,2017),
exploredunsupervisedtechniquesforbridginggraphandtextrepresentations. Subsequently,graphembeddings,when
utilized within symbolic frameworks, can enhance knowledge graph reasoning tasks (Zhang et al., 2021), or more
generally,providethebedrockforlearningdomain-invariantrepresentations(Parketal.,2023). Lastly,buildingupon
the insights from Sun et al. (2022), the integration of NeSy techniques in scientific workflows promises significant
acceleration in scientific discovery. While previous work has effectively identified opportunities and challenges, we
havetakenamoreambitiousapproachbydevelopingacomprehensiveframeworkfromthegrounduptofacilitatea
widerangeofNeSyintegrations.
3 ProblemDefinition
Conventional approaches employing foundation models for inference, such as LLMs, are predominantly confined to
single-steporfew-stepexecutionsandprimarilyreliantonhand-craftedin-contextlearningpromptinstructions. This
restricted scope limits the utilization to single modalities, lacks refinement or verification, and exhibits limited tool
proficiency. We posit that the integration of NeSy engines as core computation units, realized through logic-based
methodologiescoupledwithsub-symbolicfoundationmodels,offersamoregeneral,robust,andverifiableperspective.
This approach has several advantages. Firstly, it facilitates the integration of pre-existing engineered solutions (e.g.
various classical algorithms), offloading computational complexity and bridging various modalities. Secondly, it
enablessub-symbolicgeneralizationtofocusonevidence-baseddecision-making. Thirdly,itprovidesaninterpretable
language-based control layer for explainable, autonomous systems. Central to our solution is a method to define
and measure the orchestration of interactions between symbolic and sub-symbolic systems, and the level at which
instructionsareformulatedforeffectivecontrolandtaskexecution.
4 DesignPrinciples
Inthefollowingsubsections,weelaborateonthekeydesignprinciplesunderlyingSymbolicAIandhowweguidethe
generativeprocessesofNeSyengines.
SymbolsandExpressions AspositedbyNewell&Simon(1976),symbolsareelementalcarriersofmeaningwithin
acomputationalcontext3. Thesesymbolsdefinephysicalpatternscapableofcomposingcomplexstructures,andare
central to the design and interpretation of logic and knowledge representations (Augusto, 2022). Thus, SymbolicAI
conceptualizes the notion that symbols, and the expressions they form, are reflections of the information inherent in
asystem, andserveas surrogateforthe interactionbetweenthe systemandthe problemspace. Moreover, we argue
thatrealpatterns,asDennett(1991)speaksof,canbeeffectivelyrealizedthroughtheuseofsymbolsbecausethese
symbolsactasversatileabstractionsthatcaptureandrepresenttheunderlyingstructuresanddynamicsofthesepatterns,
facilitatingtheirinterpretationandmanipulationincomputationalmodels.
Furthermore, we attribute task-specific mappings to a language-centric strategy, leveraging their inherent semantics
andabstractiontodescribethestatesandpropertiesoftheproblemathand. Thesemappingsareuniversalandmay
beusedtodefinescenedescriptions,long-horizonplanning,acousticproperties,emotionalstates,physicalconditions,
etc. Therefore, we adhere to the analogy of language representing the convex hull of the knowledge of our society,
utilizing it as a fundamental tool to define symbols. This approach allows us to map the complexities of the world
ontolanguage,wherelanguageitselfservesasacomprehensive,yetabstract,frameworkencapsulatingthediversityof
thesesymbolsandtheirmeanings. Thisperspectiveresonateswithourinnatehumantendenciestoattributeexisting
physicalobjectswithabstractconcepts,asexemplifiedbyournaturalinclinationtolinktangibleobjectstocolorsand
emotions,suchasblendingthecolor”red”with”heart”,”warm”,and”passion”.
However, this language-centric model does not inherently encompass all forms of representation, such as sensory
inputsandnon-discreteelements,requiringtheestablishmentofadditionalmappingstofullycapturethebreadthofthe
world. Thislimitationismanageable,sincewecaretoengageinoperationswithinthisabstractconceptualspace,and
thendefinecorrespondingmappingsbacktotheoriginalproblemspace. Thesearetypicallyappliedthroughfunction
approximation,asintypicalmodality-to-languageandlanguage-to-modalityusecases,wheremodalityisaplaceholder
forvariousskillsetssuchastext,image,video,audio,motion,etc.
Ultimately, this approach also anchors our work in the field of formal language theory, as we require a structured
methodtoconstructmappingsfromtheworldtolanguage. Thisgroundingsetsthefoundationforemployingformal
3Webaseourframework’snameontheaspirationalworkofNewellandSimon.
4languagestructures,suchasgrammars,tosystematicallydefineourlanguage-centricapproachtoproblem-solvingand
theassociatedtranslationofreal-worldcomplexitiesintolinguisticterms.
Formal Languages In formal language theory and linguistics, languages are structured following the Chomsky
hierarchy,whichclassifieslanguagesbythecomplexityoftheirgrammaticalstructure(Chomsky,1956). Thishierarchy,
comprisingfourtypesofgrammars(Type-3toType-0),delineatesformallanguagesbytheirgrammaticalcomplexity. A
grammarinthiscontextconsistsofterminalandnon-terminalsymbols,productionrules,andadesignatedstartsymbol,
facilitatingthegenerationofvalidstringswithinalanguage. IndevelopingSymbolicAI,wepositthatallsymbolscan
berepresentedasstrings,augmentedwithconditionalinstructionsandtypesderivedfromadomain-specificlanguage
(DSL)tailoredfordirectingNeSycomputationengines,likeLLMs(seeFigure2).
A key advancement of LLMs over previous systems lies in their ability to generalize from formal languages (Wang
etal.,2023a)andknowledgesystems,primarilyduetotheirworld-knowledgeandproficiencyinunderstandingcontext-
basedanalogies. Whilethereiscurrentlynouniversalconsensusamongexpertsregardingthepreciseclassificationof
naturallanguagewithintheChomskyhierarchy,wehaveseeninourempiricalevaluationspromisingresultsutilizing
LLMsassemanticparsers. Thisapproachcanbeviewedasemployingaformofflexible,context-sensitivegrammar,
which enables the processing of instructions and analogies with a nuanced understanding of language’s inherent
variabilityandcomplexity. Theintersectionbetweenformalandnaturallanguagesbecomesevidentwhenconsidering
how language patterns, through prompts like ”You are a helpful assistant...”, elicit structured responses, indicating
a potential underlying formal mechanism at play. This observation underlines the utility of such a grammar in our
framework, particularly within in-context learning, where it serves as an explicit schema guiding the structure of
examplesusedinfew-shotlearningscenarios. Forinstance,equating”3.1415...”with”π”or”August4,1961”with
”1961-08-04”inagivencontextdemonstratesthisprobabilistic,context-dependentinterpretationofsymbols. Sucha
systemdoesn’trigidlyadheretostandardgrammaticalrulesbutinsteadadjustsandinterpretsbasedonthepresented
context,effectivelycreatingadynamicandsituation-specificgrammar.
In-Context Learning Recently, several in-context learning methodologies evolved to enable tool usage through
LLMs (Schick et al., 2023), or refine the generative outcome of LLMs (Yang et al., 2023). This includes chain-of-
thought(CoT)prompting,amethodthatconditionsthemodeltorevealitsstep-by-stepreasoningprocess(Singhaletal.,
2023; Wei et al., 2022b). CoT prompting breaks down complex tasks into simpler, sequential steps, and helps with
interpretingLLM’soutput. Self-generatedCoT,wheremodelsareencouragedtogeneratetheirownreasoningchains
basedontrainingexamples,surpassesevenexpertlycraftedCoT(Fernandoetal.,2023). Thisobservationechoesother
reports that GPT-4 has an emergent self-improving capability through introspection, such as self-verification (Weng
etal.,2023)orself-consistency(Wangetal.,2023b). TreeofThoughts(ToT)enablesLLMstosolvecomplexproblems
byexploringmultiplereasoningpathsthroughasearchtreeofcoherenttextunits,demonstratingsignificantproblem-
solvingenhancementsintasksrequiringstrategicplanningandsearch(Yaoetal.,2023a). Ensembletechniquesfurther
enhance the robustness and accuracy of model predictions by combining several strategies to establish a consensus
(Norietal.,2023). Conceptually, wedesignedourframeworktoenableallthesetechniquesandcombinetheminto
dedicatedcomponentsandsub-processes. Figure2outlinesatypicalexpressionevaluationinourNeSypipeline.
Expressions DSL Prompt Neuro-Symbolic Engine Symbol
'AC' << 'B' 'ABC'
Figure2: IllustrationforNeSypipeline,showcasingconceptualusageofin-contextlearningmethodologies,domain-
specificlanguage(DSL)structures,andtheexpressionevaluationsthroughNeSyengines. Theexpressionshowcases
theleftshiftoperator≪andhowtheinformationofthesymbolBisincludedinthesymbolAC. Thevioletplaceholder
in the DSL Prompt represents an instruction, such as ”Insert the right-hand side value into the left-hand value in a
chronologicalorder.”Thepositionsbelowthatrepresenttask-specificfew-shotexamples.
Domain-InvariantAssociations In-contextlearningenabledLLMstobecomeversatiletasksolversbyinterpolating
withinthetrainingdistribution,totheextentthatevenpotentiallyunseentasksareaddressable(Brownetal.,2020). We
attributethistoassociationsformedwithintheinputspaceandthecapacityofTransformerarchitecturesfordefining
domain-invariantfeaturesub-spaces. Thisphenomenonhasstrikingparallelswithadvancementsinfew-shotlearning
approaches such as SubGD (Gauch et al., 2022), a method based on identifying and utilizing a low-dimensional
5subspace, learned from various tasks, that effectively acts to regularize the learning process, representing features
that are invariant across different learning tasks. Furthermore, SubGD reflects the potential of in-context learning
when combined with task-specific fine-tuning by showing that fine-tuning within a learned subspace significantly
outperforms traditional fine-tuning methods. We believe that the extent of in-context learning is not yet exhausted,
holdingconsiderablepromisewhenusealongsidewithtask-specificfine-tuningandsolvers. Todeveloplearningand
reasoningsystemscapableofgeneralproblem-solving,weadoptahybridmethodology. Thisapproachleveragesthe
in-contextgeneralizationcapabilityofLLMs, constructingsymbolicassociationsthataimtopreserveandpropagate
situationalcontext,andvalidatingsolutionswithestablishedsolvers.
Function Composition In SymbolicAI, function composition is relevant for constructing complex hierarchies and
behaviors from more basic, fundamental elements. It enables our framework to model interconnected processes,
where the output of one function seamlessly transitions into the input of another, thus creating a cohesive sequence
of operations. Through function composition, we construct computational graphs, in which intermediate symbols
representthenodesorstateswithinthesegraphs. Formally,functioncompositionisdenotedby◦,wherecombining
functions f and g yields a new function h = g ◦f, defined as h(x) = g(f(x)) For functions f : X → Y and
g :Y →Z,theircompositionresultsinafunctionmappingelementsfromdomainXtocodomainZthroughg(f(x)).
Althoughtraditionallythecodomainoftheinnerfunctionf alignswiththedomainoftheouterfunctiong,SymbolicAI
relaxesthisconstraintbyallowingforanysubsetrelationshipbetweenthesedomainsandcodomains,enhancingdata
flowflexibility. Forexample,thisrelaxedconstraintindomainandcodomainalignmentisparticularlybeneficialfor
in-contextlearning. Byleveragingfunctionalfew-shotlearning,wherefew-shotexamplesactasdynamicelementsof
thefunction’sdomain,SymbolicAIenhancesitsabilitytointerpretandrespondtodiverseinputcontexts. Forinstance,
afunctioncanclassifyauserrequestandselectanappropriateenginetoprocesstherequest. Thetargetmodalitymay
varybasedontherespectiveengine. Therefore,inSymbolicAI,theuseoffunctioncompositionleadstothecreationof
richerandmorenuancedsymbol-basedgenerativeflows,whereeachfunctionalunitiscapableofrepresentingeither
logicalordata-drivenbehaviors. Importantly,functioncompositionisnotconfinedtostrictlysymbolicrepresentations;
italsoconnectswithsub-symbolicprocesses. ThisenablesSymbolicAItohandlecomplexdatastreams,establishing
functioncompositionasacentraltenetinbridgingmultiplemodalitiesandcoordinatingavarietyoftasks.
5 Framework
Inthissection,wediscussthedesignchoicespertainingtoimplementationandthespecificsoftheframeworkwe’ve
employed. Foranextendedoverview,seeoursupplementarymaterialsSectionC.
Types and Representations Analogous to the Python object type, the base type of SymbolicAI is a symbol
representedthroughitsnameequivalentbasetypeSymbol. ASymbolobjectmarksanon-reducibleatomicunit. All
othersubtypes,suchasExpressionanditsderivatives,areanalogoustotheirmathematicalnamesakes,representing
expressions or units that can be further evaluated and simplified. These subtypes inherit from Symbol the base
attributes, primitive operators, and helper methods. Furthermore, each Symbol object contains valued and vector-
valuedrepresentations,obtainedthroughvalueandembeddingattributes. Thelatter,inparticular,serveasameans
toattributeasymbol’scurrentcontext,akintoembeddingtextandstoringitasaPyTorchtensor(Paszkeetal.,2019)or
NumPyarray(Harrisetal.,2020). WhileforanLLM,thenumericaltensorsmaylackinherentmeaning,vector-valued
representationsplayastrategicrolewhen1)compositesymbolscoalesceintomorecomplexexpressions,and2)these
embeddedtensorsbecomeamenabletoupdatesthroughgradient-basedoptimization. Thisdesignchoiceiscriticalin
thedynamiclandscapeofsymbolicinteractions,particularlyforpromisingapplications,suchasdevelopingautomated
andself-evolvingNeSysystems.
ToenabletheprocessingofsymbolsbyLLMs, weassumethateachSymbolobjectisrepresentedthroughPython’s
nativestringfunctionality,wherethe str methodactsaninterpretablestringrepresentation. Worthnotingisthat
encoding a complex object into a string sometimes precludes the object reconstitution. However, this concern does
not substantially impede our methodology: we can employ approximations or proxy representations stored by the
vector-valuedpropertytoeffectivelyre-mapobjects. Theserepresentationsareobtainedthroughrespectiveembedding
models. Therefore, we can theoretically assert that any Python object is parsable by an LLM by design. For more
details,seeoursupplementarymaterialsSectionE.
PolymorphicContext Polymorphismisacentralconceptinprogramminglanguagetheoryandprominentlyfeatured
inSymbolicAI,withsignificantimplicationsforthedesignandfunctionalityofourNeSyarchitecture. Polymorphism
referstotheabilityofdifferentobjectstobeaccessedthroughthesameinterface,orofasingleidentifiertorepresent
different types based on the context of execution. The provision of a single interface to entities of different types
6allows operations to be performed in ways specific to their derived types. We employ a polymorphic structure for
instructioncomposition. WedesignedtheSymbolobjecttocontainaglobalcontext,whichiscomposedofstaticand
dynamiccontextparts. Thestaticcontextisclassdependentanddefinedatdesigntime. Thedynamiccontextisruntime
adaptableandcanbechangedtoadheretoruntimespecificlogicandchanges. Moreover,Symbolassociatedoperations
resolveinapolymorphicmannerbeforebeingevaluatedbytheNeSyengines. SymbolicAI’sengineimplementation
containsapreparemethodtoresolveandcompiletheenginespecificrepresentationbyevaluatingtheSymbol-specific
operationsandcontext. Forexample,whenutilizingGPT-4visionasaNeSyenginebackend,wecomposethesystem
anduserlevelpromptsinthepreparestatementandresolveimageorvideorelatedURLqueriesbyparsingmetatags
suchasthe<<vision: ... :>>tagusingregularexpressions.
OperatorsandMethods InSymbolicAI,operatorsareoverloadedtofacilitatetransformationsof Symbolobjects.
Theseoperatorprimitivesemploydynamiccastingtoassuretypecompatibility,simplifyingdeclarations. Consequently,
Symbol objects can be easily manipulated through type specific attributions or symbolically evaluated by the NeSy
engine. For example, a central operation for boolean logic is measuring equality between symbols. To evaluate the
equalityofsymbols,weprimarilyadheretothetypespecificimplementation,becauseweprioritizestrictcomparisons
overprobabilisticevaluations. Iftheevaluationwasunsuccessful,wethenconsidersemanticequalitythroughtheNeSy
engine. SymbolicAI leverages decorators for compose operators and custom class methods. For more details, see
supplementarymaterialsSectionC.Uponinvokinganoperatorormethod,therespectiveprimitivefunctionevaluates
thesymbol’sspecifictypeanditsrespectiveattributes,andiffurthernecessary,resolvesanesteddecoratedfunction
thatthenreferencestheNeSyengineforanevaluation. Shouldtheevaluationfail,apredefinedfallbackimplementation
executes. Absentafallback,orifbothevaluationsfail,anerrorstateisraised. Theprocessingofanoperatororcustom
methodinvolvesapipelineconsistingofpre-andpost-processingsteps,aswellasconstraintenforcement. Constraints
coveraspectslikereturntypes, valueranges, andstructuralintegrity(e.g. JSONformattingthroughgrammar-based
verification). InFigure3wegiveanoverviewoftheentirepromptcompositionbasedontheuserinput,theSymbol
objectstructure,andtheNeSyengineevaluationpipeline.
Symbol Object [ Static Context ]
Attribute: [ Static Context ] Operation Input Prompt
[ Examples ]
Attribute: [ Dynamic Context ] Custom Method @decorator PreProcessor
[ Dynamic Context ]
User Input Args + Custom Method
[ Attachment ] Engine PostProcessor Constraints
[ Attachment ] Field: [ Operation ]
User Input
Field: [ Examples ] Output
[ Template ] < Prediction Starting Point >
Field: [ Template ]
Figure 3: The illustrated pipeline utilizes GPT-4 as a NeSy engine and maps all user input and object instances to
prompts. Description from left to right; yellow marks user data, blue marks templating and transformations, red
marksconstraints, andvioletthemodelpredictionplaceholder. TheuserargumentsandSymbolinstancevaluesare
evaluatedandtransformedaccordingtothepre-andpost-processingsteps. Acustommethodisconstructedbyusinga
referenceimplementationofdecoratorsandpassesitsuserargumentsandinstanceattributestotherespectiveengine.
A pre-processor manipulates the representation of user arguments and Symbol instance attributes before forwarding
them to the engine. For instance, in case the engine is an LLM (i.e. NeSy engine), the static and dynamic context
ofaSymbolinstancearemappedtothesystemanduserpromptsforthatrespectiveLLMusingthepreparemethod
of the engine. Other attributes, such as examples or templates, can help pre-define structures for the desired LLM
outputformat. Aftertheresultiscomputedbytheengine,itispassedtothepost-processorsandvalidatedagainstthe
constraints.
Self-Referential Structures SymbolicAI augments the generative process by enabling systems to introspect and
modify their behavior dynamically. We leverage LLMs to execute tasks based on both natural and formal language
instructions, adheringtothespecifieduserobjectivesandwithinnateself-referentialstructures. Wederivesubtypes
fromExpressionandenclosetheirfunctionalitiesintask-specificcomponents,whichwethenexposeagainthrough
templatingandthemodel-drivendesignoftheNeSyengine. Thisdesignchoiceallowsasystemtocreateandutilize
itsownsub-processdefinitions,analogoustoconceptsdiscussedinSchmidhuber(2007,2009). Concretely,weutilize
generalization properties from LLMs to interpret and formulate a set of operations that incorporate self-instructions
(Wangetal.,2022). Consequently,theoperationsholdtheflexibilitytoadapttothecontext,andderivesub-processes
that self-instruct LLMs to engage in situational modeling and context-sensitive problem-solving. Ultimately, this
7
ngiseD
tpmorPenablestheconstructionofhierarchicalcomputationalgraphsforself-referentialmeta-reasoningsystemswithoutthe
needtoexplicitlytrainingameta-learner(Kirsch&Schmidhuber,2022).
6 Evaluation
Inthissection,weconcludeourpaperwithanempiricalevaluationandpotentialareasofapplicationusingstate-of-
the-art LLMs as NeSy engine backends. For our evaluation we focus on the GPT family of models GPT-3.5 Turbo
(revision1106)andGPT-4Turbo(revision1106)(Brownetal.,2020)astheyarethemostproficientmodelstothis
date;Gemini-ProasthebestperformingmodelavailablethroughAPIfromGoogle;LlaMA213Basitdefinesagood
reference implementation for available open-source LLMs from Meta; Mistral 7B and Zephyr 7B as good baselines
for revised and fine-tuned open-source contestants respectively. The selected open-source models Mistral, Zephyr,
andLlaMA2areexpectedtohaveroughlyequivalentparametercountscomparedtoGPT-3.5TurboandGemini-Pro.
Allourexperimentsareexpectedtorequireacontextsizesmallerorequalto4096toenablethecomparisonsamong
the in-context capabilities across model architectures. For LlaMA 2 we use the chat version since it better follows
instructions. Ourevaluationfocusesonthreemainparts. First,weevaluatebasiccapabilitiesofthemodelstoverify
theirconsistencyandproficiencywithin-contextlearningtasks. Second, weevaluatethelogiccapabilityofmodels
bydefininglogicalexpressionsinmathematicalandnaturallanguageformandmeasurethecapabilityofthemodels
totranslateandevaluatelogicalstatementsacrossdomains. Lastly,weevaluatethecapabilityofmodelsinplanning,
constructing, maintaining, andexecutinghierarchicalcomputationalgraphs. Themodelsareinstructedtoextractan
executionplan,whichtheythenmustscheduleandexecute. Asignificantchallengeistheevaluationofthesemantics
ofamulti-stepgenerativeprocess, sincethena¨ıveassessmentoftasksuccessionwouldscoreallmodelstozeroand
renderthemasunusable. Therefore,wederiveaqualitymeasuretoaddresstheseissues.
Performance Measure One of the challenges in model evaluation is handling irrelevant predictions from models.
Even if models follow instructions and produce parts of the expected solution, we regularly observe that they —
especiallyopen-sourcemodels—appendacontinuationoftaskirrelevantpredictions. Suchpredictionsresultinfailure
modes when applying conditions and validations, and halt any multi-step procedure. Our solution is an evaluation
protocol that refines the performance measurement, allowing for more nuanced diagnostics and the possibility of
continuing the evaluation despite intermediate failures. To derive our quality measure, we borrow ideas from the
utilizationoftheFre´chetdistanceforgenerativeprocesses(Heuseletal.,2017).
We generate trajectories through a NeSy sequential process that creates a trajectory of distributions P over multiple
iterations of generative nodes. Each node in the process can bealignedto a reference distribution, which marks the
desired behavior. To quantify the validity of the generated trajectories, we measure the total distance between the
generatedandreferencedatadistributionalongthepathtrajectory. Wethereforeadoptacumulativemeasurecapable
oftakingintoaccounttheentiregenerativetrajectory. Intheory,thisprocesswouldentailcalculatingthepathintegral
overthelatentspacerepresentationsformodels,cumulatingtheFre´chetdistances(Dowson&Landau,1982)traversed
alongthesedistributionaltrajectories:
(cid:90) tf
D(P gen,P ref)= d(N(m t,C t),N(m w,t,C w,t))dt (1)
t0
whereD(P gen,P ref)denotestheintegraloftheFre´chetdistancesbetweentwodatadistributionsalongthegenerative
pathtrajectoryfromaninitialtimet toafinaltimet ,d(N(m ,C ),N(m ,C ))istheFre´chetdistancecalculated
0 f t t w,t w,t
ateachtimetbetweenthegeneratedmultivariatenormaldatadistributionwithmeanm andcovarianceC ,andthe
t t
referencemultivariatenormaldatadistributionwithmeanm andcovarianceC . Theresultingmeasurefollows
w,t w,t
propertiesofnormaldistributionsandisconsistentwithincreasingdisturbances.
However,thisapproachiscomputationallyintractableforlarge-scaleproblems,andrequiresaccesstolatentrepresen-
tations,which—especiallyinthecontextofLLMs—isnotalwaysgiven. Forcomputationalfeasibility,wetherefore
introduce an approximation that measures the embedding distances over the path trajectories utilizing an auxiliary
embeddingmodel, basedonpriorworkondistributionregression(Szabo´ etal.,2016). Theembeddingmodelmaps
thesymbolicrepresentationsintoaRKHS,suchthatwecanapplyakernelmeanembeddingfunctiontomeasuretheir
respectivedistances(Youetal.,2019;Dinuetal.,2023). Weassessthedistancethroughthemeanembeddingsw.r.t.
to a kernel function K(·,·) of the samples et x ∼ ν gt en ∈ P gen and et y ∼ ν rt ef ∈ P ref produced by the generated data
distributionandareferencedatadistributionrespectively. Wedenotebyµ ,µ themeanembeddingsassociatedto
et et
therespectivesamples,i.e. µ (z) = 1 (cid:80)n K(xt,z)incaseet = (xt)nx isy asampleofsizenoftherespective
mean embeddings. To compue tt xe the simn ilari i= ty1 betwei en the embedx dings oi fi t= h1 e generated and reference distributions,
8we evaluate the associated maximum mean discrepancy MMD2(µ ,µ ) (Gretton et al., 2012) and then, as before
et et
fortheFre´chetdistances,weintegrateovert: x y
(cid:90) tf
D˜(P gen,P ref)= MMD2(µ et,µ et)dt. (2)
x y
t0
Inempiricalevaluations, however, wecareaboutnormalizedvaluesforeaseofinterpretation. Wethereforeanalyze
the properties of the MMD and derive a similarity score, which follows the same statistical principles as the MMD,
andisboundbetween[0,1]. WeconcludedthatwecanuseonlytheMMDcrosstermstoevaluatethesimilarities. See
oursupplementarymaterialsSectionAformoredetails. ForourcomparisonsasreferencedinFigure5wetherefore
denote the similarities rather than distances. We then come to the following formulation and refer to our empirical
measureasthe”VectorEmbeddingforRelationalTrajectoryEvaluationthroughCross-similarity”,orVERTEX score
forshort:
s(P gen,P ref):=(cid:90) tf (cid:2) min(max(0, z1 M(cid:94) MD2(µ
et
x,µ
et
y)−z rand),1)(cid:3) dt. (3)
t0
We approximate the integral across time steps through Monte Carlo approximation. The introduced normalization
constants denote the similarities to a random sequence zrand, which functions as a baseline subtraction to recenter
ourresults, andagivenreferencescoretorescalew.r.t. toscoresobtainedfromcomparingrelatedsolutionsz. The
use of the min-max scaling ensures the final measure is bounded between [0,1]. This process reflects properties
such as Ho¨lder continuity that bounds the kernel function within certain limits, ensuring that the similarity measure
is appropriately scaled. To compute the embeddings, we utilize the embedding model all-mpnet-base-v2 (Song
etal.,2020),duetoitswidespreadavailability,anditsbalancebetweenspeedandquality. Asasimilaritymeasure,we
selectaGaussiankernelfollowingourderivationfromthesupplementarymaterialsSectionA.Inoursupplementary
implementations, wealsoexploreotherkernels, includingsomepreliminaryexperimentswithcosinesimilarity. We
also note that one can integrate Bernoulli distributed trials into our score, with 0 values representing failure modes
andvaluesof1beingsuccesses. Furthermore, ifwerelaxourdefinition, wecanintegrateothersimilaritymeasures
whichareboundbetween[0,1],whichthenreflectondomain-specificattributions,i.e. includingasimilaritymeasure
tailoredtowardscapturingthenuancesbetweentwosub-structuresofabstractsyntaxtree.
InourevaluationofFigure5weconcludewiththecumulativescore,forthefollowingbaseperformancecriteria.
Basic Capabilities We analyze the consistency and stability of models and establish baselines by addressing the
followingperformancecriteria:
• AssociativePrediction: Weevaluatethesuccessrateofmodelstofollowsimpleandcomplexinstructions
andassociationswithzero-andfew-shotexamples. Wethereforeaddresstheproficientuseofouroperators
betweenSymboltypes.
• Multi-modal Binding: We perform data transformations between multiple modalities by binding through
language-basedrepresentations,andevaluatetheirproficiencyintoolutilization,classificationandroutingof
requeststorelevantmodules.
• ProgramSynthesis: Weevaluateexecutablecodewithandwithoutincludingconceptsfromretrievalaug-
mentedgeneration,model-drivendevelopment,suchastemplatingtodirectthegenerativeflow,andexperi-
mentwithself-generatedinstructionsbycreatingself-referentialexpressions. Wenotonlyaddresssimilarity
between produced solutions, but also include the measurement of abstract syntax tree equivalence and the
successofexecutions.
Logical Components To evaluation the logic capabilities of models, we conditioned them to create a sequence of
expressions as self-contained components, and refer to higher-order predicate logic for their assessment. Based on
the underlying type theory originating from Whitehead & Russell (1925–1927), we evaluate a model’s capability in
theformofthereexistsxsuchthatxfulfillsy. Suchquantifiersdefinethestandardsemanticsoverexpressions,where
themeaningisgivenbyasemanticfunction. Asemanticfunctionmapsatermfromanabstractdefinitiontoapoint
inadomain,whichisaninterpretationoftheterm’stypeandvalue. Therefore,thesefunctionsoperateontypesand
valuesofexpressions,andrelationsthereof. Subsequently,NeSyenginescanformulateandevaluateatinferencetime
logic-basedinstructionsthroughLisp,Prolog,orMathematica(McCarthy,1959;Colmerauer&Roussel,1993;Chen
etal.,1993;Inc.,2022),orleverageSATsolverssuchasZ3(Moura&Bjørner,2008). Therefore,theevaluationofa
9naturallanguagestatementcanbeinterpretedbyanyexpertsystemwhichdefinesthecorrespondingsemanticfunctions
and process them either in a symbolic (Feigenbaum et al., 1965; Gamble et al., 1994), differentiable (Velicˇkovic´ &
Blundell,2021;Ibarzetal.,2022),orhybridmanner(Kuncickyetal.,1991).
WeevaluatehowproficientmodelsinterpretcustomDSLsanddefineexpressionstatements. TheDSLisdesignedto
expresslogicalrelationsandoperationsinastructuredformat,andsupportshuman-readableandmachine-interpretable
formulations. Thefollowingexampleillustratessuchrelationshipsbytranslatinganaturallanguagestatementintoan
expressionstatement,asfollows:
MarvinshasfourpawsandlikestomeowwhenIpetitsfur. IsMarvinsacat?
ADSLmayenforcetheusageofHAS(·),IS(·),etc. andmayconditionaLLMtoproducethefollowingexpressions:
• HasFourPaws(x): xhasfourpaws.
• LikesToMeowWhenPetted(x): xlikestomeowwhenitispetted.
• IsCat(x): xisacat.
Thesearethenusedtodefinethefollowinglogicalexpression:
∀x(cid:0)HasFourPaws(x)∧LikesToMeowWhenPetted(x)⇒IsCat(x)(cid:1)
.
Anautomatedtheoremprovercannowevaluatethisstatementforallxvaluesandassessthevalidityoftheoriginal
query. Lastly,ourevaluationusessymbolicmathematicstomanipulatealgebraicexpressions. Thisinvolvesdefining
symbols and performing operations like factorization, simplification, and algebraic manipulation. The symbols are
placeholdersforanyvalue,enablingthedefinitionofgeneralexpressionswithoutspecifyingtheirvaluesupfront.
HierarchicalComputationalGraphs Weevaluatethecapabilitiesofmodelstoorchestrateamulti-stepgenerative
processandevaluateasetoftasks. Modelsneedtodirectsub-processesandassociatecomputationalresultsfromand
to Symbol nodes, andmaintain relationshipsbetweenthesenodes. Giventhatthe fieldiscurrentlyat anearlystage
indevelopingevensequentialschedulersforLLM-basedplanningsystems,ourinitialevaluationswillbeconfinedto
sequential execution only. In Algorithm 1, we detail our evaluation protocol, which is designed not only to analyze
and score a series of instructions but also to provide a structured basis for recording these processes. We also note
thatourevaluationprotocolisgenerallyformulated, whichallowstheapplicationofnon-sequentialplanschedulers.
In Figure 4 we illustrate a step-wise evaluation of the contextual computational graph, in which the NeSy engine is
processingconditionedonthecurrentexecutioncontextandproducingthenextsymbolprediction.
3 1 Context
Nodes Prediction
... Symbol 1 Symbol 2 TapSey mSybmolb 3ol 3 ... 2
3
... Sequential Processing ... Operation
Context
Neuro-Symbolic Engine
Figure 4: We showcase a multi-step hierarchical computational graph, with each node in the graph represented by
a symbol. The edges are relations between symbols. The left-hand side illustrates how a new node (Symbol 3) is
obtainedbyevaluatinganoperationwithitsrespectivecontextonaNeSyengine. Theright-handsideillustratesthe
contextinformationwindow(yellowrectangle)andrelationshipoftheresultinggraphwithitsrespectivenodes.
7 Discussion
In this section, we address the limitations of SymbolicAI and the future directions we are focusing on. Some of
the limitations stem from the inherent constraints of current technologies and dependencies on third-party systems.
Additionally,thenuancedcomplexitiesofworkingwithgenerativemodelspresentsfurtherchallenges.
10Algorithm1VERTEXProtocol
Require: NeSyengine: V : S → S,whereS = (cid:83) Ln isthesetofallpossiblestringsformedbyconcatenating
n≥0
charactersfromacharactersetL,embeddingengineE :S →H⊂Rd,symbols{x ,x∗,y∗}⊂S,withx asthe
0 0
initialinstruction,x∗ asthepayloadresultedfromexecutingV,y∗ asthereference,and∗actingasaplaceholder
for P,T,C, capabilities C = {F ,F ,F ,...}, where each F represents a specific functional role within the
1 2 3 i
system,planP ⊂S,taskT ∈P,memorybufferM⊂S,ascoringfunctions˜:H×H→[0,1],aschedulerQ,
anaggregatorA,andscorevariables{s}∈[0,1].
Method:
1: V,E,Q,C,yP ←Init(·) ▷Initializetheengines,thescheduler,thecapabilities,andexpectedplan.
2: M←∅,A←∅ ▷Initializethememorybufferandaggregator.
3: xP ←GeneratePlan(x ,V) ▷Generateaplanstartingfromtheinitialinstruction.
0
4: Evaluate(xP,yP,E,A,s˜) ▷Embed,score,andaggregateplansimilarity.
5: P,M←UnfoldPlan(yP,M,Q) ▷Unfoldtheplanintoactionabletasksandupdateprogress.
6: whileP ≠ ∅do ▷Rununtilalltaskshavebeendequed.
7: T,yC,yT ←Schedule(M,Q) ▷Selectthenexttaskaccordingtotheschedulerandprogression.
8: F ←Identify(T,C,V) ▷Identifythetask-relatedcapabilityF usingV.
i i
9: xC,xT ←Execute(T,F ) ▷ExecuteT usingcapabilityF andassigntheresultsxC,xT.
i i
10: Evaluate(xC,yC,xT,yT,E,A,s˜) ▷Embed,score,andaggregatecapabilitysimilarity.
11: P,M←Update(T,P,M,A) ▷Updatetheplanandtaskprogression.
12: endwhile
13: s←Finalize(A) ▷Finalizeaggregationofscores.
14: returns ▷Returntheaggregatedscoreoftheplanexecution.
Algorithm 1: This algorithm demonstrates the implementation of our Vector Embedding for Relational Trajectory
EvaluationthroughCross-similarity(VERTEX)protocol. WestartbyinitializingtheNeSyengineV,theembedding
engineE,theschedulerQ,andasetofcapabilitiesC. Theinitialinstructionx isusedtogenerateaplanxP through
0
V. The plan and its expected outcome yP are embedded, and their similarity is scored and aggregated. The plan is
thenunfoldedintoactionabletasks. EachtaskT isselectedandexecutedusingtheappropriatecapabilityC,resulting
in the capability and task results xC,xT, and expected outcomes yC,yT updated in the memory buffer M. The
processcontinues,witheachtask’sresultbeingembedded,scored,andaggregateduntiltheplaniscomplete. Thefinal
aggregatedscoresisreturned,reflectingtheoveralleffectivenessoftheplanexecution.
7.1 Limitation
EmbeddingMeasure Ourempiricalmeasureislimitedbytheexpressivenessoftheembeddingmodelandhowwell
it captures the nuances in similarities between two representations. Furthermore, the obtained similarity scores are
highlynon-linearanddifficulttointerpret. Forinstance,tworepresentationsmayaddressthesametopic,suchasthe
problemdescriptionanditsrespectivesolution,however,whenmeasuringtheirsimilarityweobtainsimilarityscores
of∼ 70%. Wenormalizethisbysubtractinganinherentbaselineandrandomnesseffect,however,toensureamore
holistic and robust measurement we would need a significantly larger amount of baselines and experiments. Since
we were very limited in the availability of development resources, and some presented models are only addressable
through costly API walls. We are actively seeking investors and supporters to scale our solution and offer a more
compellingbenchmarksuiteinthefuture.
ModelCapabilities Anobviouslimitationrevolvesaroundthefixedcontextwindowsizeoftheunderlyinglanguage
models. DespitetheexpansionofthecontextwindowinnewermodelssuchasGPT-4,thefinitecontextstillrestricts
the amount of data that can be processed in a single pass. All information outside the context needs to be added
through information retrieval approaches, which come with their own challenges and limitations (Gao et al., 2023).
Thisleadstosideeffects,includinghallucination,giventhemodeldoesnotcontainthenecessaryinformationtoanswer
thepromptedinstruction,whichmakesitdifficulttomaintainlong-termstatefulnessforcomplexreasoningtasksand
computationalgraphs.
ErrorHandling Thecomplexityoferrorhandlingwhenevaluatingcomplexexpressionsthroughfunctioncomposi-
tionality,especiallybetweenmultiplemodalitiesanddifferentsolvers,isanothernotablechallenge. WhileSymbolicAI
introducesmechanismsforerroranalysisandautomatedcorrection,theseapproachesarenotinfallible. Theyareoften
limitedbythequalityandexpressivenessofthemodels,andthemodel’scapacitytounderstanddeeplynestedlogical
11GPT-4
GPT-3.5
Associations Gemini-Pro
1.0 LlaMA 2
Mistral
Benchmarks GPT-4 GPT-3.5 Gemini-Pro LlaMA2 Mistral Zephyr Random 0.8 Zephyr
0.6 Random
Associations 0.93 0.61 1.00 0.41 0.49 0.28 0.01
Modality 0.83 0.77 0.75 0.47 0.39 0.43 0.07 Graphs 0.4 Modality
Code 0.78 0.58 0.53 0.10 0.19 0.13 0.00 0.2
Logic 0.41 0.44 0.39 0.44 0.46 0.43 0.00
Graphs 0.36 0.31 0.21 0.05 0.06 0.08 0.00
Total 0.66 0.54 0.58 0.30 0.32 0.27 0.02
Logic Code
Figure5: WeevaluateGPT-3.5,GPT4,Gemini-Pro,LLaMA2-Chat13B,Mistral7BandZephyr7Bonfivebenchmark
categories: 1)AssociativePrediction(Association)2)Multi-modalBinding(Modality)3)ProgramSynthesis(Code)4)
FunctionalLogicComponents(Logic)and5)HierarchicalComputationalGraphs(Graphs). Wedenotethesimilarity
scoresuchthathigherisbetter. Thesimilarityscoreismeasuredaccordingtoareferencebaselineandnormalizedto
excludenoise.
constructs. Wealsonotethatforourevaluations,wedisabledanyremedyprotocol,suchastruncatingpromptsorretry
schema.
Generalization ThisresearchisalsolimitedbycurrentLLM’scapacityforreasoningandgeneralization. Although
progress has been made, models are still prone to hallucinations and reasoning errors, especially when dealing with
abstract,novel,orhighlycomplexproblemstatements(Marcus,2020). Furthermore,ourframework’srelianceonthe
model’sabilitytograspthesemanticsofoperationscanbeinfluencedbythetrainingdataandthemodel’sinnatebiases
andconceptualunderstanding(Mahowaldetal.,2023). WealsopointoutthattheinitialdevelopmentofSymbolicAI
startedwiththeGPTfamilyofmodels,andwemayencounterinnatebiasesinpromptdesignandexpressivenesswhen
usingotherreferencemodels. However,wealsopointoutthatpromptengineeringinstruction-basedstatementsisnota
reliabledirectionforimprovement. Weinsteadadvocateforenhancingtheresilienceofmodelsthroughfaulttolerance,
focusingontheirabilitytobetterfollowsemanticinstructions,notsyntacticidiosyncrasies. Anotherconcernishow
to assess the disentanglement of evaluations of models on downstream tasks, to avoid testing on training samples,
especiallyforclosed-sourcesolutionslikeGPT.
Interpretability and Transparency Finally, the issue of explainability and transparency in AI systems remains
challenging. WhileSymbolicAImakesstepstowardsmakingcomputationalprocessesmoreexplicitandexplainable
through symbolic manipulations, understanding the internal logic and decision-making of LLMs remains an open
problem. Thiscanhindertrustandadoptioninsensitiveapplicationswhereinterpretabilityofpredictionsisimportant.
7.2 FutureWork
ThegoalforAlgorithm1istobeutilizedbyanadvancedlearningagent. Thisagent,employingreinforcementlearning
methodologies (Ouyang et al., 2022; Li et al., 2023; Rafailov et al., 2023), could leverage our evaluation measure
in existing benchmarks (Milani et al., 2020; Swazinna et al., 2022; Schweighofer et al., 2022) as a means to obtain
reward signals to addresses a central problem in reinforcement learning, namely credit assignment (Sutton, 1984;
Arjona-Medinaetal.,2019;Holzleitneretal.,2020;Patiletal.,2020;Widrichetal.,2021;Dinuetal.,2022). Over
time,itaimstodeveloptheabilitytoautonomouslygenerateitsownplans,efficientlyscheduletasksandsubtasks,and
carefullyselectthemostsuitabletoolsforeachtask. Ourprotocollaysthegroundworkforthisagenttolearnandexpand
its base set of capabilities (Amaro et al., 2023), moving towards more sophisticated, self-referential orchestration of
multi-steptasks. We’vealreadynoticedthatresearchisshiftingtowardsthistypeofmethodology(Yuanetal.,2024).
Furthermore,inSection6we’veonlyconsideredasequentialscheduler. However,ourobjectiveistoultimatelyassess
anon-sequentialtaskexecutionmodel,allowingfordynamicinsertionandout-of-sequencetaskexecution.
127.3 BroaderImpact
WithLLMsbecomingmoreandmoreaccessible,progressrecentlymadepossiblebythevastopensourcecontributions
from Ko¨pf et al. (2023); Touvron et al. (2023); Taori et al. (2023); Xu et al. (2023); Geng et al. (2023); Biderman
et al. (2023), embedded accelerators for LLMs – or more generally NeSY engines – will be ubiquitous in future
computationplatforms,suchaswearables,smartphones,tablets,consoles,ornotebooks. Althoughcurrentexecution
cycles are slow and error-prone, we expect to see further performance gains through improved operating system
leveloptimizations,dedicatedGPU-centrichardwarerefinement,andimprovedsoftwareinteroperability. Webelieve
that modern programming paradigms should natively support probabilistic concepts and provide a boilerplate-free
set of features for constructing and evaluating generative computational graphs. This includes but is not limited
to compositional, parallelizable, and simulation-based executions with polymorphic and self-referential structures.
Current programming languages often have disjointed or makeshift solutions for these concepts in the context of
generativeprocesses. Webelieveintegralprobabilisticsupportfortheseconceptsintomodernsoftwareandhardware
will unlock new programming paradigms that can fully take advantage of generative architectures. We hope the
communitywillconsidertheseideasasessentialcomponentsofcontemporarycomputing.
We also expect to see significant progress by processing central language concepts through system-on-a-chip (SoC)
solutionsofpre-trainedmodels,withlinearprobinglayersforhot-swappableweightexchangeoftask-specificprojec-
tionsandexecutions. Awiderangeoffunctionalitiescanbethenoffloadedtoprobabilisticprogramminglanguagesto
operateondedicatedsymbolsandstreamlinethevector-valuedmappingsbetweentheconceptspaceandunderlying
problemspace,avoidingdefiningboilerplatecodetoloadandunloadnetworkweights.
Furthermore,webelievethatmanygainsinrepresentationalstabilityandconsistencymaybeobtainedthroughmulti-
modaldatatrainingandimprovedalignmentbasedonoperatorlearningorientedfunctionalitiesandworkflow-related
scoring functionalities, analogous to our introduced quality measure. Gains in representational stability also benefit
self-instruction and self-referential sub-process evaluations, which enable the dynamic creation and evaluation of
complex hierarchical computational graphs. This will enable online learning models to perform, in real-time, skill
acquisition of complex concepts with only one or few examples at inference time. We believe this will enable the
creationofautonomouslyself-evolvingcognitivearchitectures(Langleyetal.,2009;Dawid&LeCun,2023;Sumers
et al., 2023). We therefore see an inherent connection to generative design as an analogy for creating coherent and
stable”thought”computationalgraphs,andbelievethispavesthepathtowardbroadAIsystems(seeSectionB)andis
arequirementfordevelopingartificialgeneralintelligentagents.
Finally, we also wish to express our concern about recent economic trends in the deep-tech industry, where we
observeAI-relatedconcentrationofdataandresources,coupledwithatendencytowardsclosed-sourcepractices. We
strongly advocate for increased transparency and exchange of ideas to ensure a diverse and collective growth in our
socio-economiclandscape. Therefore,wepushtowardsademocraticandopen-sourceinitiative.
7.4 Conclusion
Inthiswork,weintroducedSymbolicAI,aframeworkthatunifiesgenerativemodelswithanarrayofsolvers,blending
the strengths of symbolic and sub-symbolic AI paradigms within a cohesive NeSy framework. SymbolicAI equips
researchersandpractitionerswithacomprehensivetoolkittodevelopcontextualizedandexplainableNeSyAIsystems
capable of addressing diverse challenges effectively. We also introduce a quality measure and a benchmark for
comparingandevaluatingawiderangeofcomputationaltasks. SymbolicAIprovidesabasisforfurtherresearchin
advancedprogramsynthesis,hierarchicalcomputationalgraphs,thedevelopmentofself-referentialsystems,andthe
integrationofprobabilisticmethodswithAIdesignforcreatingautonomousagents. Furthermore,ourcommitmentto
open-sourceidealsattemptstopromoteacultureofcollaborativegrowthandinnovation.
Acknowledgement
The ELLIS Unit Linz, the LIT AI Lab, the Institute for Machine Learning, are supported by the Federal State
UpperAustria. WethanktheprojectsMedicalCognitiveComputingCenter(MC3),INCONTROL-RL(FFG-881064),
PRIMAL(FFG-873979),S3AI(FFG-872172),DLforGranularFlow(FFG-871302),EPILEPSIA(FFG-892171),AIRI
FG 9-N (FWF-36284, FWF-36235), AI4GreenHeatingGrids (FFG- 899943), INTEGRATE (FFG-892418), ELISE
(H2020-ICT-2019-3ID:951847),Stars4Waters(HORIZON-CL6-2021-CLIMATE-01-01). WethankAudi.JKUDeep
LearningCenter,TGWLOGISTICSGROUPGMBH,SiliconAustriaLabs(SAL),FILLGesellschaftmbH,Anyline
GmbH, Google, ZF Friedrichshafen AG, Robert Bosch GmbH, UCB Biopharma SRL, Merck Healthcare KGaA,
Verbund AG, GLS (Univ. Waterloo), Software Competence Center Hagenberg GmbH, Borealis AG, TU¨V Austria,
FrauscherSensonic,TRUMPF,theNVIDIACorporationandAtlas.
13WeextendourappreciationtoAndreasWindischandClemensWasnerofAIAustriafortheirunwaveringsupport. Their
valuablefeedback,connections,andfacilitationofintroductionswithintheirexpansivenetworkhavebeeninstrumental
totheprogressofExtensityAI.
OurgratitudealsogoestoSergeiPereverzyev,whoseenlightenedguidanceandthoughtfulideashavebeenabeacon
forourresearchendeavors. OurthanksareequallyextendedtoGaryMarcus,whosestimulatingdiscussionssparked
numerousinnovativeideasincorporatedintoourframework.
We are equally grateful to Markus Hofmarcher, a friend and colleague whose informed counsel and stimulating
discussions have significantly sharpened various facets of our study. Additionally, our thanks are due to Fabian
PaischerandKajetanSchweighofer,whosepreliminaryworkandassistancehavebeenofenormousbenefit.
WearealsogratefultoTimScarfe,afriendwhosecommunityhasbeenahubforexhilaratingdiscussions. Hisonline
presenceandengagementhaveenrichedtheAIresearchlandscapeandbroadenedourperspectives.
Moreover, wewishtohonorthememoriesofthecherishedfamilymemberswelostin2023. Theirinfluenceinour
livesextendedbeyondpersonalbonds,andtheprinciplestheyinstilledinuscontinuetoshapeourjourney. Itiswith
great respect and affection that we acknowledge the indelible impact they have made, enabling us to persist in our
scientificpursuitswithdeterminationandintegrity.
References
M. Z. Alom, T. M. Taha, C. Yakopcic, S. Westberg, P. Sidike, M. S. Nasrin, B. C. Van Esesn, A. A. S. Awwal, and
V.K.Asari. Thehistorybeganfromalexnet: Acomprehensivesurveyondeeplearningapproaches. arXivpreprint
arXiv:1803.01164,2018.
R. E. Amaro, J.-Y. Chen, J. M. Duarte, T. E. Hutton, C. Irving, M. C. Kandes, A. Majumdar, D. Y. Mishin, M. H.
Nguyen, P. Rodriguez, F. Silva, R. S. Sinkovits, S. M. Strande, M. Tatineni, L. S. Tran, and N. Wolter. Voyager
– an innovative computational resource for artificial intelligence & machine learning applications in science and
engineering. InPracticeandExperienceinAdvancedResearchComputing,PEARC’23’,pp.278–282,NewYork,
NY,USA,2023.AssociationforComputingMachinery. ISBN9781450399852. doi: 10.1145/3569951.3597597.
J.Andreas. Languagemodelsasagentmodels. CoRR,abs/2212.01681,2022. doi: 10.48550/arXiv.2212.01681.
J.A.Arjona-Medina,M.Gillhofer,M.Widrich,T.Unterthiner,J.Brandstetter,andS.Hochreiter. RUDDER:return
decompositionfordelayedrewards. InAdvancesinNeuralInformationProcessingSystems32,pp.13566–13577,
2019.
L.M.Augusto. ComputationalLogic.Vol.1: ClassicalDeductiveComputingwithClassicalLogic. CollegePublica-
tions,London,2edition,2022.
F.Badita. 1337UseCasesforChatGPT&otherChatbotsintheAI-DrivenEra. GoogleDocs,2022.
D.M.Beazley. PythonEssentialReference. Developer’slibrary: essentialreferencesforprogrammingprofessionals.
Addison-Wesley,2009. ISBN9780672329784. URLhttps://books.google.ro/books?id=Chr1NDlUcI8C.
T.R.Besold,A.d.Garcez,S.Bader,H.Bowman,P.Domingos,P.Hitzler,K.-U.Kuehnberger,L.C.Lamb,D.Lowd,
P.M.V.Lima,L.dePenning,G.Pinkas,H.Poon,andG.Zaverucha. Neural-symboliclearningandreasoning: A
surveyandinterpretation,2017.
M.Besta,N.Blach,A.Kubicek,R.Gerstenberger,L.Gianinazzi,J.Gajda,T.Lehmann,M.Podstawski,H.Niewiadom-
ski,P.Nyczyk,andT.Hoefler. Graphofthoughts: Solvingelaborateproblemswithlargelanguagemodels. arXiv
preprintarXiv:2308.09687,2023.
S.Biderman,H.Schoelkopf,Q.Anthony,H.Bradley,K.O’Brien,E.Hallahan,M.AflahKhan,S.Purohit,S.Prashanth,
E.Raff,A.Skowron,L.Sutawika,andO.vanderWal. Pythia: Asuiteforanalyzinglargelanguagemodelsacross
trainingandscaling,2023.
PENG Bo. Blinkdl/rwkv-lm: 0.01. Technical report, Zenodo, August 2021. URL https://doi.org/10.5281/
zenodo.5196577.
W.BradleyKnoxandPeterStone. TAMER:TraininganAgentManuallyviaEvaluativeReinforcement. In20087th
IEEE International Conference on Development and Learning, pp. 292–297, Monterey, CA, August 2008. IEEE.
ISBN978-1-4244-2661-4. doi: 10.1109/DEVLRN.2008.4640845.
14J.D.BransfordandM.K.Johnson.Contextualprerequisitesforunderstanding: Someinvestigationsofcomprehension
andrecall. JournalofVerbalLearningandVerbalBehavior,11(6):717–726,1972. ISSN0022-5371.
T.Brown,B.Mann,N.Ryder,M.Subbiah,J.D.Kaplan,P.Dhariwal,A.Neelakantan,P.Shyam,G.Sastry,A.Askell,
S.Agarwal,A.Herbert-Voss,G.Krueger,T.Henighan,R.Child,A.Ramesh,D.Ziegler,J.Wu,C.Winter,C.Hesse,
M.Chen,E.Sigler,M.Litwin,S.Gray,B.Chess,J.Clark,C.Berner,S.McCandlish,A.Radford,I.Sutskever,and
D.Amodei. Languagemodelsarefew-shotlearners. InH.Larochelle, M.Ranzato, R.Hadsell, M.F.Balcan, and
H.Lin(eds.),AdvancesinNeuralInformationProcessingSystems,volume33,pp.1877–1901.CurranAssociates,
Inc.,2020.
H.Chase.LangChain.Technicalreport,LangChain,012023.URLhttps://github.com/hwchase17/langchain.
W.Chen,M.Kifer,andD.S.Warren. Hilog: Afoundationforhigher-orderlogicprogramming. TheJournalofLogic
Programming,15(3):187–230,1993. ISSN0743-1066.
N.Chomsky. Threemodelsforthedescriptionoflanguage. IRETransactionsonInformationTheory,2(3):113–124,
1956. doi: 10.1109/TIT.1956.1056813.
P.F.Christiano, J.Leike, T.Brown, M.Martic, S.Legg, andD.Amodei. Deepreinforcementlearningfromhuman
preferences. Advancesinneuralinformationprocessingsystems,30,2017.
A.ColmerauerandP.Roussel. ThebirthofProlog. InHOPL-II,1993.
A.d’AvilaGarcezandL.C.Lamb. Neurosymbolicai: The3rdwave. arXivpreprintarXiv:2012.05876,2020.
A.d’AvilaGarcez,M.Gori,L.C.Lamb,L.Serafini,M.Spranger,andS.N.Tran. Neural-symboliccomputing: An
effectivemethodologyforprincipledintegrationofmachinelearningandreasoning.JournalofAppliedLogic,2019.
A.DawidandY.LeCun. Introductiontolatentvariableenergy-basedmodels: Apathtowardsautonomousmachine
intelligence. arXivpreprintarXiv:2306.02572,2023.
J. Degrave. Building A Virtual Machine inside ChatGPT. Technical report, Engraved, 11 2022. URL https:
//www.engraved.blog/building-a-virtual-machine-inside/.
F. Deniz, A. O. Nunez-Elizalde, A. G. Huth, and J. L. Gallant. The representation of semantic information across
humancerebralcortexduringlisteningversusreadingisinvarianttostimulusmodality. JournalofNeuroscience,
39(39):7722–7736,2019. ISSN0270-6474. doi: 10.1523/JNEUROSCI.0675-19.2019.
D.C.Dennett. Realpatterns. JournalofPhilosophy,88(1):27–51,1991. doi: 10.2307/2027085.
M. Dilhara, A. Ketkar, and D. Dig. Understanding software-2.0: A study of machine learning library usage and
evolution. ACM Transactions on Software Engineering and Methodology (TOSEM), 30(4):55:1–55:42, jul 2021.
ISSN1049-331X. doi: 10.1145/3453478.
M.-C.Dinu,M.Hofmarcher,V.P.Patil,M.Dorfer,P.M.Blies,J.Brandstetter,J.A.Arjona-Medina,andS.Hochreiter.
Xaiandstrategyextractionviarewardredistribution. InA.Holzinger,R.Goebel,R.Fong,T.Moon,K.-R.Mu¨ller,
andW.Samek(eds.),xxAI-BeyondExplainableAI:InternationalWorkshop,HeldinConjunctionwithICML2020,
July 18, 2020, Vienna, Austria, Revised and Extended Papers, pp. 177–205, Cham, 2022. Springer International
Publishing. ISBN978-3-031-04083-2. doi: 10.1007/978-3-031-04083-2 10.
M.-C. Dinu, M. Holzleitner, M. Beck, H. D. Nguyen, A. Huber, H. Eghbal-zadeh, B. A. Moser, S. V. Pereverzyev,
S.Hochreiter,andW.Zellinger. Addressingparameterchoiceissuesinunsuperviseddomainadaptationbyaggre-
gation. InTheEleventhInternationalConferenceonLearningRepresentations,ICLR2023,Kigali,Rwanda,May
1-5,2023.OpenReview.net,2023.
I. Donadello, L. Serafini, and A. d’Avila Garcez. Logic tensor networks for semantic image interpretation. In
ProceedingsoftheTwenty-SixthInternationalJointConferenceonArtificialIntelligence,IJCAI-17,pp.1596–1602,
2017.
D.C.DowsonandB.V.Landau.Thefre´chetdistancebetweenmultivariatenormaldistributions.JournalofMultivariate
Analysis,12(3):450–455,1982. doi: https://doi.org/10.1016/0047-259X(82)90077-X.
Kevin Ellis. Human-like few-shot learning via bayesian reasoning over natural language. arXiv preprint
arXiv:2306.02797,2023.
15M.Fang,S.Deng,Y.Zhang,Z.Shi,L.Chen,M.Pechenizkiy,andJ.Wang. Largelanguagemodelsareneurosymbolic
reasoners. arXivpreprintarXiv:2401.09334,2024.
E. Fedorenko, P.-J. Hsieh, A. Nieto-Castanon, S. Whitfield-Gabrieli, and N. Kanwisher. New method for fMRI
investigations of language: Defining rois functionally in individual subjects. Journal of neurophysiology, 104:
1177–94,082010. doi: 10.1152/jn.00032.2010.
E.Feigenbaum,B.G.Buchanan,J.Lederberg,CarlDjerassi,andetal. Dendral,1965.
C. Fernando, D. Banarse, H. Michalewski, S. Osindero, and T. Rockta¨schel. Promptbreeder: Self-referential self-
improvementviapromptevolution. arXivpreprintarXiv:2309.16797,2023.
R.F.Gamble,G.-C.Roman,H.C.Cunningham,andW.E.Ball. Applyingformalverificationmethodstorule-based
programs. Int.J.ExpertSyst.,7(3):203–237,sep1994. ISSN0894-9077.
S. Ganguly and V. Pudi. Paper2vec: Combining graph and text information for scientific paper representation. In
JoemonJoseetal.(eds.),AdvancesinInformationRetrieval,volume10193ofLectureNotesinComputerScience.
Springer,Cham,2017. ISBN978-3-319-56607-8. doi: 10.1007/978-3-319-56608-5 30.
Y.Gao,Y.Xiong,X.Gao,K.Jia,J.Pan,Y.Bi,Y.Dai,J.Sun,andH.Wang. Retrieval-augmentedgenerationforlarge
languagemodels: Asurvey. arXivpreprintarXiv:2312.10997,2023.
A. Garcez, T. Besold, L. De Raedt, P. Fo¨ldia´k, P. Hitzler, T. Icard, K. Ku¨hnberger, L. Lamb, R. Miikkulainen, and
D.Silver. Neural-symboliclearningandreasoning: Contributionsandchallenges. InAAAIConference,2015.
M.Gauch,M.Beck,T.Adler,D.Kotsur,S.Fiel,H.Eghbal-zadeh,J.Brandstetter,J.Kofler,M.Holzleitner,W.Zellinger,
D.Klotz,S.Hochreiter,andS.Lehner. Few-ShotLearningbyDimensionalityReductioninGradientSpace. arXiv
preprintarXiv:2206.03483,2022.
X.Geng,A.Gudibande,H.Liu,E.Wallace,P.Abbeel,S.Levine,andD.Song.Koala: Adialoguemodelforacademic
research. Blogpost,April2023. URLhttps://bair.berkeley.edu/blog/2023/04/03/koala/.
Google. Gemini: Afamilyofhighlycapablemultimodalmodels. arXivpreprintarXiv:2312.11805,2023.
A.Goyal,A.Friesen,A.Banino,T.Weber,N.R.Ke,A.P.Badia,A.Guez,M.Mirza,P.C.Humphreys,K.Konyushova,
M. Valko, S. Osindero, T. Lillicrap, N. Heess, and C. Blundell. Retrieval-augmented reinforcement learning. In
K.Chaudhuri,S.Jegelka,L.Song,C.Szepesvari,G.Niu,andS.Sabato(eds.),Proceedingsofthe39thInternational
Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 7740–7765.
PMLR,17–23Jul2022.
A.Gretton,K.M.Borgwardt,M.J.Rasch,B.Scho¨lkopf,andA.Smola. Akerneltwo-sampletest. JournalofMachine
LearningResearch,13(25):723–773,2012.
A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint
arXiv:2312.00752,2023.
Q. Guo, Z. Jin, X. Qiu, W. Zhang, D. Wipf, and Z. Zhang. CycleGT: Unsupervised graph-to-text and text-to-graph
generationviacycletraining. arXivpreprintarXiv:2006.04702,2020.
K. Hamilton, A. Nayak, B. Bozˇ ic´, and L. Longo. Is neuro-symbolic AI meeting its promises in natural language
processing? astructuredreview. SemanticWeb,pp.1–42,nov2022. doi: 10.3233/sw-223228.
C.R.Harris,K.J.Millman,S.J.vanderWalt,R.Gommers,P.Virtanen,D.Cournapeau,E.Wieser,J.Taylor,S.Berg,
N.J.Smith,R.Kern,M.Picus,S.Hoyer,M.H.vanKerkwijk,M.Brett,A.Haldane,J.Ferna´ndezdelR´ıo,M.Wiebe,
P.Peterson,P.Ge´rard-Marchant,K.Sheppard,T.Reddy,W.Weckesser,H.Abbasi,C.Gohlke,andT.E.Oliphant.
ArrayprogrammingwithNumPy. Nature,585(7825):357–362,2020. doi: 10.1038/s41586-020-2649-2.
M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two time-scale update
rule converge to a local nash equilibrium. In Proceedings of the 31st International Conference on Neural Infor-
mation Processing Systems, NIPS’17, pp. 6629–6640, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN
9781510860964.
S.Hochreiter. Towardabroadai. Commun.ACM,65(4):56–57,mar2022. ISSN0001-0782.
S.HochreiterandJ.Schmidhuber. Flatminima. NeuralComput.,9(1):1–42,1997.
16M.Holzleitner,L.Gruber,J.A.Arjona-Medina,J.Brandstetter,andS.Hochreiter. Convergenceproofforactor-critic
methodsappliedtoPPOandRUDDER. arXivpreprintarXiv:2012.01399,2020.
K. Hornik, M. Tinchcombe, and H. White. Multilayer feedforward networks are universal approximators. Neural
Networks,2:359–366,1989. doi: 10.1016/0893-6080(89)90020-8.
J. Hu, H. Small, H. Kean, A. Takahashi, L. Zekelman, D. Kleinman, E. Ryan, A. Nieto-Castan˜o´n, V. Ferreira, and
E.Fedorenko. PrecisionfMRIrevealsthatthelanguage-selectivenetworksupportsbothphrase-structurebuilding
andlexicalaccessduringlanguageproduction. bioRxiv,2022. doi: 10.1101/2021.09.10.459596.
Z.Hu,X.Ma,Z.Liu,E.Hovy,andE.Xing. Harnessingdeepneuralnetworkswithlogicrules. InProceedingsofthe
54thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers), pp.2410–2420,
Berlin,Germany,August2016.AssociationforComputationalLinguistics.
A. G. Huth, W. A. de Heer, T. L. Griffiths, F. E. Theunissen, and J. L. Gallant. Natural speech reveals the semantic
mapsthattilehumancerebralcortex. Nature,532(7600):453–458,2016. doi: 10.1038/nature17637.
B. Ibarz, J. Leike, T. Pohlen, G. Irving, S. Legg, and D. Amodei. Reward learning from human preferences and
demonstrations in atari. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett
(eds.),AdvancesinNeuralInformationProcessingSystems,volume31.CurranAssociates,Inc.,2018.
B. Ibarz, V. Kurin, G. Papamakarios, K. Nikiforou, M. Abbana Bennani, R. Csorda´s, A. Dudzik, M. Bosˇnjak,
A.Vitvitskyi,Y.Rubanova,A.Deac,B.Bevilacqua,Y.Ganin,C.Blundell,andP.Velivcˇkovic´. Ageneralistneural
algorithmiclearner. InLOGIN,2022.
Wolfram Research, Inc. Mathematica, Version 13.2, 2022. URL https://www.wolfram.com/mathematica.
Champaign,IL.
G.Indiveri,B.Linares-Barranco,T.Hamilton,A.vanSchaik,R.Etienne-Cummings,T.Delbruck,S.Liu,P.Dudek,
P. HA˜ =Cfliger, S. Renaud, J. Schemmel, G. Cauwenberghs, J. Arthur, K. Hynna, F. Folowosele, S. SAA˜ GHI,
T.Serrano-Gotarredona,J.Wijekoon,Y.Wang,andK.Boahen. Neuromorphicsiliconneuroncircuits. Frontiersin
Neuroscience,5,2011. ISSN1662-453X. doi: 10.3389/fnins.2011.00073.
N.Jain,S.Vaidyanath,A.Iyer,N.Natarajan,S.Parthasarathy,S.Rajamani,andR.Sharma. Jigsaw: Largelanguage
modelsmeetprogramsynthesis. arXiv,2021.
J.Johnson,M.Douze,andH.Je´gou. Billion-scalesimilaritysearchwithGPUs. IEEETransactionsonBigData,7(3):
535–547,2019.
E. Jones and J. Steinhardt. Capturing failures of large language models via human cognitive biases. arXiv preprint
arXiv:2202.12299,2022.
A. Karpathy. Software 2.0. Medium, 2017. URL https://karpathy.medium.com/
software-2-0-a64152b37c35.
N. Kassner, B. Krojer, and H. Schu¨tze. Are Pretrained Language Models Symbolic Reasoners over Knowledge?
In R. Ferna´ndez and T. Linzen (eds.), Proceedings of the 24th Conference on Computational Natural Language
Learning,CoNLL2020,Online,November19-20,2020,pp.552–564.AssociationforComputationalLinguistics,
2020. doi: 10.18653/v1/2020.conll-1.45.
T.Katsch.Gateloop: Fullydata-controlledlinearrecurrenceforsequencemodeling.arXivpreprintarXiv:2311.01927,
2023.
D.Key,W.-D.Li,andK.Ellis. Towardtrustworthyneuralprogramsynthesis. arXivpreprintarXiv:2210.00848,2023.
G.Kim,P.Baldi,andS.McAleer. Languagemodelscansolvecomputertasks,2023.
L.KirschandJ.Schmidhuber. Eliminatingmetaoptimizationthroughself-referentialmetalearning. arXivpreprint
arXiv:2212.14392,2022.
A. Ko¨pf, Y. Kilcher, D. von Ru¨tte, S. Anagnostidis, Z.-R. Tam, K. Stevens, A. Barhoum, N. M. Duc, O. Stanley,
R. Nagyfi, S. ES, S. Suri, D. Glushkov, A. Dantuluri, A. Maguire, C. Schuhmann, H. Nguyen, and A. Mattick.
Openassistantconversations–democratizinglargelanguagemodelalignment,2023.
17D. C. Kuncicky, S. I. Hruska, and R. C. Lacher. Hybrid systems: the equivalence of rule-based expert system and
artificialneuralnetworkinference. Int.J.ExpertSyst.,4(3):281–297,jan1991. ISSN0894-9077.
E.Kıcıman,R.Ness,A.Sharma,andC.Tan. CausalReasoningandLargeLanguageModels: OpeningaNewFrontier
forCausality. arXiv,2023.
J.E.Laird. Introductiontosoar,2022.
J.E.Laird,A.Newell,andP.S.Rosenbloom. Soar: Anarchitectureforgeneralintelligence. ArtificialIntelligence,33
(1):1–64,1987. ISSN0004-3702.
L. C. Lamb, A. Garcez, M. Gori, M. Prates, P. Avelar, and M. Vardi. Graph neural networks meet neural-symbolic
computing: Asurveyandperspective. InAAAIConference,2020.
P. Langley, J. Laird, and S. Rogers. Cognitive architectures: Research issues and challenges. Cognitive Systems
Research,10:141–160,2009. doi: 10.1016/j.cogsys.2006.07.004.
H. Li, Y. Su, D. Cai, Y. Wang, and L. Liu. A survey on retrieval-augmented text generation. arXiv preprint
arXiv:2202.01110,2022a.
Y.Li, D.Choi, J.Chung, N.Kushman, J.Schrittwieser, R.Leblond, T.Eccles, J.Keeling, F.Gimeno, A.DalLago,
etal. Competition-levelcodegenerationwithalphacode. Science,378(6624):1092–1097,2022b.
Z.Li,Z.Yang,andM.Wang.Reinforcementlearningwithhumanfeedback: Learningdynamicchoicesviapessimism.
arXivpreprintarxiv:2305.18438,2023.
N.F.Liu,K.Lin,J.Hewitt,A.Paranjape,M.Bevilacqua,F.Petroni,andP.Liang. Lostinthemiddle: Howlanguage
modelsuselongcontexts. arXivpreprintarXiv:2307.03172,2023.
S.Lu,D.Guo,S.Ren,J.Huang,A.Svyatkovskiy,A.Blanco,C.Clement,D.Drain,D.Jiang,D.Tang,etal.Codexglue:
A machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664,
2021.
M. Lutz. Learning Python: Powerful Object-Oriented Programming. Animal Guide. O’Reilly Media, 2013. ISBN
9781449355715.
Q.Lyu,S.Havaldar,A.Stein,L.Zhang,D.Rao,E.Wong,M.Apidianaki,andC.Callison-Burch. Faithfulchain-of-
thoughtreasoning,2023.
J.MacGlashan,M.K.Ho,R.Loftin,B.Peng,G.Wang,D.L.Roberts,M.E.Taylor,andM.L.Littman. Interactive
LearningfromPolicy-DependentHumanFeedback.InProceedingsofthe34thInternationalConferenceonMachine
Learning,pp.2285–2294.PMLR,July2017.
M.Macsweeney. Neuralsystemsunderlyingbritishsignlanguageandaudio-visualenglishprocessinginnativeusers.
Brain,125:1583–1593,072002. doi: 10.1093/brain/awf153.
A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang,
S. Welleck, B. P. Majumder, S. Gupta, A. Yazdanbakhsh, and P. Clark. Self-refine: Iterative refinement with
self-feedback,2023.
K.Mahowald,A.A.Ivanova,I.A.Blank,N.Kanwisher,J.B.Tenenbaum,andE.Fedorenko. Dissociatinglanguage
andthoughtinlargelanguagemodels: acognitiveperspective. CoRR,abs/2301.06627,2023.
R.Manhaeve,S.Dumancic,A.Kimmig,T.Demeester,andL.DeRaedt. DeepProbLog: NeuralProbabilisticLogic
Programming. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.),
AdvancesinNeuralInformationProcessingSystems,volume31.CurranAssociates,Inc.,2018.
J. Mao, C. Gan, P. Kohli, J. B. Tenenbaum, and J. Wu. The neuro-symbolic concept learner: Interpreting scenes,
words,andsentencesfromnaturalsupervision. In7thInternationalConferenceonLearningRepresentations,ICLR
2019,2019.
G.Marcus.TheNextDecadeinAI:FourStepsTowardsRobustArtificialIntelligence.arXivpreprintarXiv:2002.06177,
2020.
18A.Martelli,A.Ravenscroft,andD.Ascher. PythonCookbook. O’ReillyMedia,2005. ISBN9780596554743. URL
https://books.google.ro/books?id=Q0s6Vgb98CQC.
J.McCarthy. Lisp: Aprogrammingsystemforsymbolicmanipulations. InPreprintsofPapersPresentedatthe14th
National Meeting of the Association for Computing Machinery, ACM ’59, pp. 1–4, New York, NY, USA, 1959.
AssociationforComputingMachinery. ISBN9781450373647. doi: 10.1145/612201.612243.
J.McCarthy,M.L.Minsky,N.Rochester,andC.E.Shannon. Aproposalforthedartmouthsummerresearchproject
onartificialintelligence,august31,1955. AImagazine,27(4):12–12,2006.
W.S.McCullochandW.Pitts. ALogicalCalculusofIdeasImmanentinNervousActivity. BulletinofMathematical
Biophysics,5:115–133,1943. doi: 10.1007/BF02478255.
L.Menenti,S.M.E.Gierhan,K.Segaert,andP.Hagoort. Sharedlanguage: Overlapandsegregationoftheneuronal
infrastructureforspeakingandlisteningrevealedbyfunctionalmri.PsychologicalScience,22(9):1173–1182,2011.
doi: 10.1177/0956797611418347. PMID:21841148.
Microsoft. Bingis yourAI-poweredcopilotfortheweb. Technical report, Microsoft, 2023. URLhttps://bing.
com/chat.
T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient Estimation of Word Representations in Vector Space.
InternationalConferenceonLearningRepresentations,2013.
S.Milani,N.Topin,B.Houghton,W.H.Guss,S.P.Mohanty,K.Nakata,O.Vinyals,andN.S.Kuno. Retrospective
analysisofthe2019minerlcompetitiononsampleefficientreinforcementlearning.InH.J.EscalanteandR.Hadsell
(eds.), Proceedings of the NeurIPS 2019 Competition and Demonstration Track, volume 123 of Proceedings of
MachineLearningResearch,pp.203–214.PMLR,Dec2020.
L.DeMouraandN.Bjørner.Z3: anefficientsmtsolver.InProceedingsoftheTheoryandPracticeofSoftware,14thIn-
ternationalConferenceonToolsandAlgorithmsfortheConstructionandAnalysisofSystems,TACAS’08/ETAPS’08,
pp.337–340,Berlin,Heidelberg,2008.Springer-Verlag. ISBN3540787992.
A.Newell. UnifiedTheoriesofCognition. HarvardUniversityPress,USA,1990. ISBN0674920996.
A.NewellandH.Simon. Thelogictheorymachine–acomplexinformationprocessingsystem. IRETransactionson
informationtheory,2(3):61–79,1956.
A.NewellandH.A.Simon. Humanproblemsolving. Prentice-Hall,pp.920,1972.
A. Newell and H. A. Simon. Computer science as empirical inquiry: symbols and search. Commun. ACM, 19(3):
113–126,mar1976. ISSN0001-0782. doi: 10.1145/360018.360022.
A.Newell,J.C.Shaw,andH.A.Simon. Empiricalexplorationsofthelogictheorymachine: acasestudyinheuristic.
IRE-AIEE-ACM’57(Western): PaperspresentedattheFebruary26-28,1957,westernjointcomputerconference:
Techniquesforreliability,pp.218–230,1957. doi: 10.1145/1455567.1455605.
H. Nori, Y. T. Lee, S. Zhang, D. Carignan, R. Edgar, N. Fusi, N. King, J. Larson, Y. Li, W. Liu, R. Luo, S. M.
McKinney,R.O.Ness,H.Poon,T.Qin,N.Usuyama,C.White,andE.Horvitz. Cangeneralistfoundationmodels
outcompetespecial-purposetuning? casestudyinmedicine. arXivpreprintarXiv:2311.16452,2023.
A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and
K.Kavukcuoglu. Wavenet: Agenerativemodelforrawaudio. arXivpreprintarXiv:1609.03499,2016.
OpenAI. Introducing ChatGPT. Technical report, OpenAI, November 2022. URL https://openai.com/blog/
chatgpt.
OpenAI. GPT-4TechnicalReport. arXiv,2023.
L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray,
J.Schulman,J.Hilton,F.Kelton,L.E.Miller,M.Simens,A.Askell,P.Welinder,P.F.Christiano,J.Leike,andR.J.
Lowe. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155,
2022.
19F.Paischer,T.Adler,V.Patil,A.Bitto-Nemling,M.Holzleitner,S.Lehner,H.Eghbal-Zadeh,andS.Hochreiter.History
compressionvialanguagemodelsinreinforcementlearning. InK.Chaudhuri,S.Jegelka,L.Song,C.Szepesvari,
G.Niu,andS.Sabato(eds.),Proceedingsofthe39thInternationalConferenceonMachineLearning,volume162
ofProceedingsofMachineLearningResearch,pp.17156–17185.PMLR,July2022.
F.Paischer,T.Adler,M.Hofmarcher,andS.Hochreiter. Semantichelm: Aninterpretablememoryforreinforcement
learning. CoRR,abs/2306.09312,2023. doi: 10.48550/arXiv.2306.09312.
N.Park,D.Chae,J.Shim,S.Kim,E.-S.Kim,andJ.Kim. Bridgingthedomaingapbyclustering-basedimage-text
graphmatching. arXivpreprintarXiv:2310.02692,2023.
A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga,
A. Desmaison, A. Ko¨pf, E. Z. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang,
J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library. arXiv preprint
arXiv:1912.01703,abs/1912.01703,2019.
V.P.Patil,M.Hofmarcher,M.-C.Dinu,M.Dorfer,P.M.Blies,J.Brandstetter,J.A.Arjona-Medina,andS.Hochreiter.
Align-RUDDER: Learning from few demonstrations by reward redistribution. arXiv preprint arXiv:2009.14108,
2020.
F. Petroni, T. Rockta¨schel, S. Riedel, P. S. H. Lewis, A. Bakhtin, Y. Wu, and A. H. Miller. Language Models
as Knowledge Bases? In K. Inui, J. Jiang, V. Ng, and X. Wan (eds.), Proceedings of the 2019 Conference
on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural
LanguageProcessing,EMNLP-IJCNLP2019,HongKong,China,November3-7,2019,pp.2463–2473.Association
forComputationalLinguistics,2019. doi: 10.18653/v1/D19-1250.
S.Pitis,M.R.Zhang,A.Wang,andJ.Ba. Boostedpromptensemblesforlargelanguagemodels,2023.
M. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus, Y. Bengio, S. Ermon, and C. Re´. Hyena hierarchy:
Towardslargerconvolutionallanguagemodels. arXivpreprintarXiv:2302.10866,2023.
B. Poole, A. Jain, J. T. Barron, and B. Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint
arXiv:2209.14988,2022.
M. Qu and J. Tang. Probabilistic logic neural networks for reasoning. In Proceedings of the 33rd International
ConferenceonNeuralInformationProcessingSystems,2019.
R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference optimization: Your
languagemodelissecretlyarewardmodel. arXivpreprintarXiv:2305.18290,2023.
A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever. Zero-shot text-to-image
generation. InInternationalConferenceonMachineLearning,pp.8821–8831.PMLR,2021.
M. Regev, C. J. Honey, E. Simony, and U. Hasson. Selective and invariant neural responses to spoken and written
narratives. Journal of Neuroscience, 33(40):15978–15988, 2013. ISSN 0270-6474. doi: 10.1523/JNEUROSCI.
1580-13.2013.
ReplikaAI. Pushing the Boundaries of AI to Talk to the Dead. Technical report,
ReplikaAI, 2016. URL https://www.bloomberg.com/news/articles/2016-10-20/
pushing-the-boundaries-of-ai-to-talk-to-the-dead.
B. Romera-Paredes, M. Barekatain, A. Novikov, et al. Mathematical discoveries from program search with large
languagemodels. Nature,2023. doi: 10.1038/s41586-023-06924-6.
F.Rosenblatt. Theperceptron: Aprobabilisticmodelforinformationstorageandorganizationinthebrain. Psycho-
logicalReview,65(6):386–408,1958. doi: 10.1037/h0042519.
L.Ruis,A.Khan,S.Biderman,S.Hooker,T.Rockta¨schel,andE.Grefenstette.Largelanguagemodelsarenotzero-shot
communicators. CoRR,abs/2210.14986,2022. doi: 10.48550/arXiv.2210.14986.
D.Rumelhart,G.Hinton,andR.Williams.Learningrepresentationsbyback-propagatingerrors.Nature,323:533–536,
1986. doi: 10.1038/323533a0.
20C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. Denton, S. K. S. Ghasemipour, B. K. Ayan, S. S. Mahdavi,
R.G.Lopes,etal. Photorealistictext-to-imagediffusionmodelswithdeeplanguageunderstanding. arXivpreprint
arXiv:2205.11487,2022.
A.Santoro,A.Lampinen,K.Mathewson,T.Lillicrap,andD.Raposo. Symbolicbehaviourinartificialintelligence.
arXivpreprintarXiv:2102.03406,2022.
T.Schick,J.Dwivedi-Yu,R.Dess`ı,R.Raileanu,M.Lomeli,L.Zettlemoyer,N.Cancedda,andT.Scialom.Toolformer:
Languagemodelscanteachthemselvestousetools,2023.
J.Schmidhuber. Go¨delmachines: Fullyself-referentialoptimaluniversalself-improvers. CognitiveTechnologies,8:
199–226,012007. doi: 10.1007/978-3-540-68677-4 7.
J. Schmidhuber. Driven by compression progress: A simple principle explains essential aspects of subjective
beauty, novelty, surprise, interestingness, attention, curiosity, creativity, art, science, music, jokes. arXiv preprint
arXiv:0812.4360,2009.
J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lockhart, D. Hassabis,
T. Graepel, T. Lillicrap, and D. Silver. Mastering atari, go, chess and shogi by planning with a learned model.
Nature,588(7839):604–609,2020. doi: 10.1038/s41586-020-03051-4.
K. Schweighofer, A. Radler, M.-C. Dinu, M. Hofmarcher, V. P. Patil, A. Bitto-Nemling, H. Eghbal-zadeh, and
S.Hochreiter. Adatasetperspectiveonofflinereinforcementlearning. InConferenceonLifelongLearningAgents,
pp.470–517.PMLR,2022.
T. Scott, J. Galle´e, and E. Fedorenko. A new fun and robust version of an fMRI localizer for the frontotemporal
languagesystem. CognitiveNeuroscience,8:1–10,072016. doi: 10.1080/17588928.2016.1201466.
N. Shinn, B. Labash, and A. Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection,
2023.
D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou,
V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. P. Lillicrap,
M.Leach,K.Kavukcuoglu,T.Graepel,andD.Hassabis. MasteringthegameofGowithdeepneuralnetworksand
treesearch. Nature,529(7587):484–489,2016. doi: 10.1038/nature16961.
D.Silver,T.Hubert,J.Schrittwieser,I.Antonoglou,M.Lai,A.Guez,M.Lanctot,L.Sifre,D.Kumaran,T.Graepel,
T. Lillicrap, K. Simonyan, and D. Hassabis. Mastering chess and shogi by self-play with a general reinforcement
learningalgorithm. arXivpreprintarXiv:1712.01815,2017a.
D. Silver, J. Schrittwieser, K. Simonyan, et al. Mastering the game of go without human knowledge. Nature, 550:
354–359,2017b. doi: 10.1038/nature24270.
U.Singer,A.Polyak,T.Hayes,X.Yin,J.An,S.Zhang,Q.Hu,H.Yang,O.Ashual,O.Gafni,etal. Make-a-video:
Text-to-videogenerationwithouttext-videodata. arXivpreprintarXiv:2209.14792,2022.
K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales, et al. Large language models encode
clinicalknowledge. Nature,620(7972):172–180,2023.
K.Song,X.Tan,T.Qin,J.Lu,andT.-Y.Liu. Mpnet: Maskedandpermutedpre-trainingforlanguageunderstanding.
InProceedingsofthe34thInternationalConferenceonNeuralInformationProcessingSystems,NIPS’20,pp.1414,
RedHook,NY,USA,2020.CurranAssociatesInc. ISBN9781713829546.
Spotify. ApproximateNearestNeighborsOhYeah. Technicalreport,Spotify,2017.
T.R.Sumers,S.Yao,K.Narasimhan,andT.L.Griffiths. Cognitivearchitecturesforlanguageagents. arXivpreprint
arXiv:2309.02427,2023.
M.Summerfield. ProgramminginPython3: ACompleteIntroductiontothePythonLanguage. Developer’slibrary.
Addison-Wesley,2010. ISBN9780321680563.
J.J.Sun,M.Tjandrasuwita,A.Sehgal,A.Solar-Lezama,S.Chaudhuri,Y.Yue,andO.Costilla-Reyes.Neurosymbolic
programmingforscience. arXivpreprintarXiv:2210.05050,2022.
21R.S.Sutton. TemporalCreditAssignmentinReinforcementLearning. PhDthesis,UniversityofMassachusetts,Dept.
ofComp.andInf.Sci.,1984.
P. Swazinna, S. Udluft, D. Hein, and T. Runkler. Comparing model-free and model-based algorithms for offline
reinforcementlearning. arXivpreprintarXiv:2201.05433,2022.
Z. Szabo´, B. K. Sriperumbudur, B. Po´czos, and A. Gretton. Learning theory for distribution regression. J. Mach.
Learn.Res.,17(1):5272–5311,Jan2016. ISSN1532-4435.
R.Taori, I.Gulrajani, T.Zhang, Y.Dubois, X.Li, C.Guestrin, P.Liang, andT.B.Hashimoto. Stanfordalpaca: An
instruction-followingllamamodel. https://github.com/tatsu-lab/stanford_alpaca,2023.
H.Touvron,T.Lavril,G.Izacard,X.Martinet,M.-A.Lachaux,T.Lacroix,B.Rozie`re,N.Goyal,E.Hambro,F.Azhar,
A.Rodriguez,A.Joulin,E.Grave,andG.Lample. Llama: Openandefficientfoundationlanguagemodels,2023.
A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,L.Kaiser,andI.Polosukhin. Attentionisall
youneed. InI.Guyon,U.VonLuxburg,S.Bengio,H.Wallach,R.Fergus,S.Vishwanathan,andR.Garnett(eds.),
AdvancesinNeuralInformationProcessingSystems,volume30.CurranAssociates,Inc.,2017.
P. Velicˇkovic´ and C. Blundell. Neural algorithmic reasoning. Patterns, 2(7):100273, 2021. ISSN 2666-3899. doi:
https://doi.org/10.1016/j.patter.2021.100273.
B.Wang,Z.Wang,X.Wang,Y.Cao,R.A.Saurous,andY.Kim. Grammarpromptingfordomain-specificlanguage
generationwithlargelanguagemodels. arXivpreprintarXiv:2305.19234,2023a.
J.Wang,X.Yi,R.Guo,H.Jin,P.Xu,S.Li,X.Wang,X.Guo,C.Li,X.Xu,etal. Milvus: Apurpose-builtvectordata
managementsystem.InProceedingsofthe2021InternationalConferenceonManagementofData,pp.2614–2627,
2021a.
X.Wang,J.Wei,D.Schuurmans,Q.Le,E.Chi,S.Narang,A.Chowdhery,andD.Zhou. Self-consistencyimproves
chainofthoughtreasoninginlanguagemodels. arXivpreprintarXiv:2203.11171,2023b.
Y. Wang, R.J. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio, et al.
Tacotron: Towardsend-to-endspeechsynthesis. arXivpreprintarXiv:1703.10135,2017.
Y.Wang,W.Wang,S.Joty,andS.C.H.Hoi. Codet5: Identifier-awareunifiedpre-trainedencoder-decodermodelsfor
codeunderstandingandgeneration. arXivpreprintarXiv:2109.00859,2021b.
Y.Wang,Y.Kordi,S.Mishra,A.Liu,N.A.Smith,D.Khashabi,andH.Hajishirzi. Self-instruct: Aligninglanguage
modelwithselfgeneratedinstructions. arXivpreprintarXiv:2212.10560,2022.
J.Wei,Y.Tay,R.Bommasani,C.Raffel,B.Zoph,S.Borgeaud,D.Yogatama,M.Bosma,D.Zhou,D.Metzler,E.H.
Chi,T.Hashimoto,O.Vinyals,P.Liang,J.Dean,andW.Fedus. Emergentabilitiesoflargelanguagemodels. arXiv
preprintarXiv:2206.07682,2022a.
J.Wei, X.Wang, D.Schuurmans, M.Bosma, B.Ichter, F.Xia, E.H.Chi, Q.V.Le, andD.Zhou. Chainofthought
prompting elicits reasoning in large language models. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho (eds.),
AdvancesinNeuralInformationProcessingSystems,2022b.
Y.Weng,M.Zhu,F.Xia,B.Li,S.He,S.Liu,B.Sun,K.Liu,andJ.Zhao. Largelanguagemodelsarebetterreasoners
withself-verification. arXivpreprintarXiv:2212.09561,2023.
A.N.WhiteheadandB.Russell. PrincipiaMathematica. CambridgeUniversityPress,1925–1927.
M.Widrich,M.Hofmarcher,V.P.Patil,A.Bitto-Nemling,andS.Hochreiter. ModernHopfieldNetworksforReturn
DecompositionforDelayedRewards. InDeepRLWorkshopNeurIPS2021,2021.
P.M.Winter,S.Eder,J.Weissenbo¨ck,C.Schwald,T.Doms,T.Vogt,S.Hochreiter,andB.Nessler. Trustedartificial
intelligence: Towardscertificationofmachinelearningapplications. arXivpreprintarXiv:2103.16910,2021.
Writesonic. ChatGPT Alternative Built With Superpowers - ChatSonic. Technical report, Chatsonic, 2022. URL
https://writesonic.com/chat.
C. Xu, D. Guo, N. Duan, and J. McAuley. Baize: An open-source chat model with parameter-efficient tuning on
self-chatdata,2023.
22Z.Xu,H.vanHasselt,andD.Silver. Meta-gradientreinforcementlearning. ArXiv,2018.
L. Yang, S. Zhang, Z. Yu, G. Bao, Y. Wang, J. Wang, R. Xu, W. Ye, X. Xie, W. Chen, and Y. Zhang. Supervised
KnowledgeMakesLargeLanguageModelsBetterIn-contextLearners. arXivpreprintarXiv:2312.15918,2023.
S.Yao,D.Yu,J.Zhao,I.Shafran,T.L.Griffiths,Y.Cao,andK.Narasimhan. Treeofthoughts: Deliberateproblem
solvingwithlargelanguagemodels. arXivpreprintarXiv:2305.10601,2023a.
S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. React: Synergizing reasoning and acting in
languagemodels. arXivpreprintarXiv:2210.03629,2023b.
J. Ye, Z. Wu, J. Feng, T. Yu, and L. Kong. Compositional exemplars for in-context learning. arXiv preprint
arXiv:2302.05698,2023.
K.You,X.Wang,M.Long,andM.Jordan.Towardsaccuratemodelselectionindeepunsuperviseddomainadaptation.
InK.ChaudhuriandR.Salakhutdinov(eds.),Proceedingsofthe36thInternationalConferenceonMachineLearning,
volume97ofProceedingsofMachineLearningResearch,pp.7124–7133.PMLR,Jun9–152019.
YouWrite. TheAISearchEngineYouControl. Technicalreport,You.com,2022. URLhttps://you.com.
D.Yu,B.Yang,D.Liu,H.Wang,andS.Pan. Asurveyonneural-symboliclearningsystems. NeuralNetworks,166:
105–126,2023. ISSN0893-6080.
W.Yuan,R.Y.Pang,K.Cho,S.Sukhbaatar,J.Xu,andJ.Weston. Self-rewardinglanguagemodels. arXivpreprint
arXiv:2401.10020,2024.
J. Zhang, B. Chen, L. Zhang, X. Ke, and H. Ding. Neural, symbolic and neural-symbolic reasoning on knowledge
graphs. AIOpen,pp.14–35,2021.
Appendix
A ConnectionbetweenFre´chetDistanceandMaximumMeanDiscrepancy
LetusconsideraGaussiankerneldefinedbytheexpression
(cid:18) ∥x−y∥2(cid:19)
K(x,y)=exp − , (4)
2σ2
whereσisthebandwidthparameterofthekerneland∥·∥denotestheEuclideannorm. UsingK,wecannowconstruct
ameasureofdistancebetweendistributions,byembeddingthemintotheReproducingKernelHilbertSpace(RKHS)
induced by K, using kernel mean embeddings. The resulting distance is called the Maximum Mean Discrepancy
(MMD).
More precisely, the MMD between two probability distributions P and Q is encoded in the RKHS through mean
embeddings,whichcanbeexpressedas
MMD2(P,Q)=∥E [ϕ(x)]−E [ϕ(y)]∥2 , (5)
x∼P y∼Q RKHS
whereϕ(·)representsthefeaturemappingtotheRKHScorrespondingtothekernelK.
Ontheotherhand,formultivariateGaussiandistributions,wecanusetheFre´chetdistanceasameasureofsimilarity,
whichisnothingbuttheassociatedWasserstein-2distance,forwhichanexplicitformulaisavailableintheGaussian
case. Theresultingexpressionisasfollows(Dowson&Landau,1982):
(cid:16) (cid:17)
d2(X 1,X 2)=∥µ 1−µ 2∥2 2+Tr C 1+C 2−2(C 1C 2)1 2 , (6)
whereX ∼N(µ ,C )andX ∼N(µ ,C ),andTr(·)indicatesthetraceofamatrix.
1 1 1 2 2 2
ToestablishanapproximationoftheFre´chetdistanceusingtheGaussiankernel,wetakeC =σ2I andC =σ2I as
1 2
identitycovariancematricesscaledbyσ2. Thisassumptionallowsustofocussolelyonthedisparitiesinmeanvectors:
d2(X ,X )≈∥µ −µ ∥2, (7)
1 2 1 2 2
settingasidetheeffectofdifferentcovariancestructures.
23Giventheseconditions,weattempttoarguethattheFre´chetdistancebehavessimilarlyasMMD:
d2(X ,X )≈∥µ −µ ∥2 ≈MMD2(P,Q), (8)
1 2 1 2 2
Heuristically,atleastforsmall∥µ −µ ∥,alsotheassociatedkernelevaluationsK(X ,X )tendtobesmall(seealso
1 2 1 2
Hochreiter & Schmidhuber (1997)), which leadsto a small MMD, if we ignore the terms associated to K(X ,X ),
1 1
K(X ,X )(whichcanceloutduetosamecovariancestructure).
2 2
InthenextsectionwewanttofurtherelaobarateontheMMDandapossiblescore,thatcanbederivedfromit.
A.1 ExtendedSimplificationoftheMMDCalculation
TounderstandthesimplificationoftheMMD,weareformallyexpressingtheMMDintermsofkernelsumsoverpairs
ofsampleswithinandacrosstwosamplesX andY:
m n
1 (cid:88)(cid:88) 2 (cid:88)(cid:88) 1 (cid:88)(cid:88)
MMD2(X,Y)= k(x ,x )− k(x ,y )+ k(y ,y ), (9)
m(m−1) i j mn i j n(n−1) i j
i j̸=i i=1j=1 i j̸=i
wheremandnarethesizesofsamplesX andY,respectively.
Empirical observations have led to the conclusion that the within-sample terms (cid:80) (cid:80) k(x ,x ) and
i j̸=i i j
(cid:80) (cid:80) k(y ,y ) cancel out the cross terms (cid:80)m (cid:80)n k(x ,y ) under certain conditions. This can be due to
i j̸=i i j i=1 j=1 i j
thefollowing:
• In high-dimensional embedding spaces, distributions of embedding vectors are often closely related and
normallydistributed.
• If the samples X and Y are drawn from distributions P and Q where their mean embeddings are nearly
orthogonalintheRKHS,itisthedissimilarityacrosssamples,ratherthanthatwithin,thatismostrelevant.
Therefore, under these specific conditions, it becomes justifiable to focus on the cross-terms, yielding the following
proposalforadistancemeasure:
m n
(cid:94) 2 (cid:88)(cid:88)
MMD2(X,Y)≈ k(x ,y ). (10)
mn i j
i=1j=1
B BroadAIandNeuro-SymbolicSystems
Our work focuses on broad artificial intelligence (AI) (Hochreiter, 2022) (see Figure 6) through the integration of
symbolicandsub-symbolicAImethodologies. BroadAIextendsbeyondrestrictedfocusonsingle-taskperformance
ofnarrowAI.InbroadAI,systemsareengineeredtohandlearangeoftaskswithahighdegreeofautonomy,utilizing
sensoryinput,accumulatedexperiences,andpreviouslydevelopedskills.
Symbolic AI Sub-Symbolic AI
Broad AI
Knowledge Adaptability Abstraction Interaction Interpretability Efficiency Robustness Reasoning
Figure6: Integrationofsymbolicandsub-symbolicAIinbroadAISystems, showcasingcentralconceptsofbroad
AI systems, including knowledge, adaptability, abstraction, interaction, interpretability, efficiency, robustness, and
reasoning(Hochreiter,2022).
Ourprimaryobjectiveistocombinethestrengthsofsymbolicandsub-symbolicapproachestoovercomeindividual
limitations. SymbolicAIischaracterizedbyitsemphasisonknowledgerepresentation,theabilitytoabstractandfor-
mulatemathematicalconcepts,andthecapacityforinteractionswithusersorothersystemsinahuman-understandable
24manner. Theseattributesensurethatwedevelopreasoning-based,interpretableAIsystemswithinnaterobustnessand
trustworthiness(Winteretal.,2021).
Both methodologies form the basis for developing new cognitive architectures, as illustrated in Figure 7. This
hybridizationproducescomputationalgraphscapableofcontext-awarelearningandreasoning,allowingAItoexecute
complextaskswithhuman-likeflexibility.
Software 3.0
Neuro-Symbolic Programming
Neuro-Symbolic System
Software 2.0
Differentiable Programming
Virtualization Apps Frameworks ...
Software 1.0 Java C# Python ...
Classical Programming JVM CLR C++ ...
Mac Win Unix Linux
Figure 7: Evolution of software paradigms: From Software 1.0’s rigid specification in classical programming to
Software2.0’sdata-drivenandobjectivefunction-focuseddifferentiableprogramming,leadingtoSoftware3.0’sNeSy
systemsthatemphasizehuman-centric,interaction-basedprogrammingwithcomputationalgraphs. Thisprogression
represents a shift from explicit task-specific programming to abstract, adaptive systems that cater to dynamic user
preferences.
Borrowing nomenclature from Karpathy (2017); Dilhara et al. (2021), we refer to the next generation of software
as Software 3.0, which consists of applications that are neither pre-determined at design time, nor learned through
statisticalinference,buttriggeredbyaninteractionwhichstimulatestherealizationofacomputationalgraphanalogous
toneuromorphiccircuits(Indiverietal.,2011),however,purelyestablishedatinferencetimeinthe”thought”process
ofaNeSysystem.
Toenablesuchsystems,werequireamorenativeintegration(seeillustrationinFigure8)ofprobabilisticprogramming
paradigms into our contemporary programming stack, and make their utilization a commodity for practitioners and
researchersalike.
25Computation Stack
Data Sources
and Neuro-Symbolic Interface [Viewport Engines]
Compute Engines
Java C# Python
Search Engine JVM CLR C++ Neuro-Symbolic Rendering Engine
Computation Engine
Database Engine Speech Engine
Mac Win Unix Linux
Symbolic Engine Human Interface
... ...
Figure 8: The illustration demonstrates the integration of Neuro-Symbolic computation within the contemporary
programming stack. Probabilistic programming paradigms are embedded natively alongside traditional languages
and environments, facilitated by interfaces to various data sources, compute engines, and human interaction tools,
streamliningtheiradoptioninpracticalandresearchapplications.
C FrameworkStructure
Primitives IntheSymbolicAIframework, atthecoreliestheconceptofPrimitivesandthedynamictypecreation
of Symbol objects, which are central to inherit types of behaviors. Primitives are pre-defined operations that act
on Symbol objects, encapsulating basic operations, such as arithmetic, logic, or casting operations, to name a few.
These operations are crucial to the framework’s versatility and form the foundation for more complex interactions
withincomputationalgraphs. Essentially,theycanbeviewedascontextualizedfunctionsthatacceptaSymbolobject,
send it to the NeSy engine for evaluation, and return one or more new objects, primarily new symbols. One of the
keyfeaturesofoperationsistheirpolymorphism, whichallowsforthemtobeappliedtovariousdatatypes, suchas
strings,integers,floats,lists,andmore,withdifferentbehaviorsdependingonthespecificobjectinstance. Toexecute
operations,weutilizetheSymbolobject’svalueattributecontainingtheoriginaldatatype. Thiswillbethensentas
a string representation to the engines to execute the needed operations. Consequently, all values are cast to a string
representation. Remember, this was our implicit assumption (see Section 4). For custom objects, it is essential to
defineasuitable str methodtocasttheobjecttoastringrepresentationwhilepreservingtheobject’ssemantics.
SymbolObjectsCreationandDynamicTyping ASymbolobjectisaversatileentitythatcanencapsulateavariety
of data types and behaviors. The creation of Symbol objects is facilitated through a metaclass, which enables the
dynamic typing of these objects to inherit behaviors from a collection of primitives. This dynamic typing system
is important for extending the functionality of Symbol objects beyond simple data containers; they contain specific
behaviorsappropriatefortheoperationstheywillperform. Forinstance,aSymbolobjectmaypossessthebehaviors
ofarithmeticcomputations,stringmanipulations,orevenlogicalcomparisons,dependingonthedefinedprimitives.
TypeInheritanceandExpressionCreation TypeinheritanceinSymbolicAIisleveragedtocreatenewexpressions,
which are specialized forms of Symbol objects designed to represent parts of a computational graph. Expressions
extend the capabilities of Symbol objects by providing a structured way to define complex functionalities that can
later be evaluated to produce new Symbol objects or modify existing ones. In SymbolicAI, an Expression object
inheritsthepropertiesof Symbolobjectswhilealsobeingabletodefineitsownuniquebehaviorthroughtheuseofa
forwardmethod,whichisanalogoustoacomputationalgraphnode’sevaluationfunction. Figure9givesanoverview
ofanexemplaryinheritancebranch. EachExpressionmustfeatureaforwardmethod,whichmustbeoverwritten
to define its behavior. The inherited call method invokes the forward method, evaluating the expression and
returningtheresult. Thisdesignpatternfacilitateslazyevaluationofexpressions,allowingforcomplexcomposition
ofexpressions.
Inherited from the Symbol class, the sym return type and static context properties establish the context in
which the current Expression operates. The static context impacts all operations of the current Expression
subclass, while the sym return type guarantees the acquisition of the desired return object type post-evaluation.
26Typically,thisreturnsthecurrenttypebutcanbeconfiguredtoreturnadifferenttype. Amorein-depthexamination
ofbothnotionswillbeprovidedinthefollowingsection.
Symbol
0..*
Expression
1
Style Compose Template ... Sequence
Figure 9: Class diagram showing the inheritance and composition relationships among Symbol, Expression, and
otherinheritedclasses. SymbolservesasabaseclassforExpressionwherealltheothertypesarederivedfrom. Other
typesmaycontainorassociatewithzeroormoreSymboltypes. Forexample,weillustratehowtheSequencederives
from Expression and the multiplicity ’0..*’ indicates that a Sequence can contain any number of Expression
instances.
UtilizingDecoratorsforOperationDefinition Decoratorsserveasabridgebetweenthedeclarativenatureofsym-
bolicoperationsandtheimperativeexecutionmodelofprogramminglanguages. Byaugmentingfunctiondefinitions
withdecorators,theframeworkcandynamicallyassignoperationstoSymbolorExpressionobjects,whicharethen
interpretedbytheunderlyingNeSyengineortraditionalsolvers.
For example, the @core.logic decorator can be used to augment a Symbol object with the capability to perform
logical and, or, or not operations contextually. Similarly, the @core.combine decorator allows the framework to
definethesemanticsofcombiningoraddingtwosymbolicvalues,regardlessoftheirunderlyingdatarepresentations.
1 # Example of using decorators to define logical operations
2 @core.logic(operator=’and’)
3 def _some_logic(self, other):
4 # implementation logic here
5 pass
Aspect-Oriented Programming The aspect-oriented programming paradigm offers a functional approach for ex-
tending or modifying the behavior of functions or methods without altering their code directly. This adheres to the
principles of modularity and separation of concerns, as it allows for the isolation of specific functionalities while
maintaining the original function’s core purpose. By wrapping the original function, decorators provide an efficient
and reusable way of adding or modifying behaviors. For instance, SymbolicAI integrates the zero- and few-shot
learningwithdefaultfallbackfunctionalitiesofpre-existingcode.
Theuseofdecoratorsbringsseveraladvantages(Beazley,2009;Martellietal.,2005;Summerfield,2010;Lutz,2013):
• Reusability: Decoratorspromotecodemodularity,enhancingcodereusabilityandcontributingtosoftware
maintainability. This advantage is particularly salient when managing a variety of operations, reducing
redundancyandsimplifyingtheintegrationofnewfunctionalities.
• Composition: Decoratorssupportfunctioncomposition,allowingdeveloperstoconstructcomplexfunction-
alitiesfrompre-existingcodeblockswithouttheneedtoexpandthecodebaseorrelyoncomplexinheritance
hierarchies.
• Adaptability: Usingdecoratorswecaneasilymodifyorextendthebehaviorofoperationswithoutchanging
theircoreimplementation. Thisflexibilityfacilitatesthegenerationofadaptiveworkflowsandreliablefallback
mechanismswhenexperimentalimplementationsdonotfulfillrequiredconstraints.
27Symbol Class and Computational Graph Elements A computational graph in the SymbolicAI framework is an
assemblyofinterconnectedSymbolobjects,eachencapsulatingaunitofdataandtheoperationsthatcanbeperformed
onit. Theexchangebetweenthesesymbolsformsahighlymodularandinterpretablesystem,capableofrepresenting
complexworkflows.
TheSymbolclassisanabstractionrepresentingdataandcontext. Itholdsnotonlythevalueitself,butmetadatathat
guidesitstransformationandinterpretation. Throughinheritanceandcompositionality,theSymbolcanbeextended
intomorecomplexexpressions,andbecomingnodesinacomputationalgraph. EachSymbolinstancecanoptionally
containareferencetoitsparentandchildren,naturallyformingadirectedgraphstructurewherethenodesaresymbols
andedgesrepresentrelationshipsbetweenasymbolanditsderivativecomputations.
The Linker class, is a metadata subclass, and tracks relationships and results, effectively annotating the graph with
executiondetails. Itkeepsrecordsofnodes’keys,allowingquickretrievalofrelatedcomputationaloutcomeswithin
thegraph,andaidsintaskssuchaserrortracinganddebugging.
A central concept in this structure is the notion of root, which points to the origin of the computational sequence.
Accessingtherootallowsbacktrackingthroughthegraph,makingitpossibletoaggregateresultsandinspecttheflow
ofcomputationthatledtothecurrentnode.
The computational graph’s structure is further enriched by properties like nodes, edges, and graph itself, which
collectively enable the comprehensive query of the computation’s topology. These properties are used internally to
enablegraphvisualizations,whichareusefulfordebuggingandanalysis.
ExpressionofaComputationalGraph Inpractice,considertheExpressionclass,whichextendsthefunctionality
of ‘Symbol‘. When composing a Sequence of Expression objects, we are effectively composing operations in a
predeterminedorder.
Forinstance,anexpressionsuchas:
1 Sequence(
2 Clean(),
3 Translate(),
4 Outline(),
5 Compose(),
6 )
representsaprocedurethatfirstcleansdata,thentranslatesit,outlinestheessentialinformation,andcomposesitinto
a finalized form. When this sequence is executed, the operations unfold in the exact order specified, with each step
receivingtheoutputofitspredecessorasinputandpassingitsresulttothesuccessor.
Generatingentireworkflows SymbolicAIallowsthecreationofentireworkflows,suchaswritingscientificpapers.
The following example defines a Paper expression that takes in a sequence of expressions which are executed in
sequentially. The Method expression contains a Source expression, which addresses the code base of the actual
methodofthepaper. TheRelatedWorkexpressioncontainsasequenceof Citeexpressionswhichareexecutedin
parallelandareusedtodefinethecontextoftherelatedworksection. Eachexpressioninthesequenceofexpressions
from Paper takes in the context of its predecessors. All expressions also link their results to a global linker object,
whichisusedaftertheexecutiontoretrieveindividualresultsfromdistinctexpressionnodes. InFigure10weshow
theresultingcomputationalgraphofthePaperexpression.
1 # define the computational graph
2 expression = Paper(
3 Method(
4 # link to original code base where the main method is defined
5 Source(file_link=’/path/to/.../file’),
6 ),
7 # gather resources and write the related work
8 RelatedWork(
9 Cite(file_link=’/path/to/.../file’),
10 Cite(file_link=’/path/to/.../file’),
11 ...
12 ),
13 # write the abstract and title
14 Abstract(),
15 Title(),
2816 )
17 # run the graph
18 paper_result = expression(’Write a scientific paper’)
19 # access linker to retreive the results from the method expression
20 method = expr.linker.find(’Method’)
21 # print result of the method expression
22 print(method)
Figure10: WeillustratethecomputationalgraphforthePaperexpression. Eachnoderepresentsaninstanceofan
expression with distinct properties and behaviors. The edges denote the reference relationship between expressions.
The blue highlighted nodes mark the main sequence of expressions, used to create the paper, such as Method,
RelatedWork,Abstract,etc.
D Installation
TheinstallationoftheSymbolicAIframeworkisstraightforwardandcanbedonethroughthePythonpackagemanager
pip. ToinstallSymbolicAI,openaterminalandexecutethefollowingcommandinyourcurrentpythonenvironment:
1 pip install symbolicai
ThiscommandwillinstallthelatestversionofSymbolicAIalongwithitscoredependencies,enablingtheintegration
of the framework into Python applications. If you intend to use the framework with local engines4, or with engines
poweredbyexternalAPIssuchasOpenAI’sAPI,additionalinstallationstepsarerequired.
4Thelocalenginesareexperimentalandarerunonyourlocalmachine. Formoredetails,refertothe”LocalNeuro-Symbolic
Engine”sectioninthedocumentation.
29D.1 EngineConfiguration
Beforethefirstrun,itisnecessarytoconfiguretherequiredmodulesandoptionallysetnecessaryAPIkeystoactivate
the respective engines. This can be done in multiple ways, but we recommend doing it through the configuration
wizardbyrunningthiscommandintheterminal:
1 symwzd
Thisstepisessentialtoregistertheenginesinternallyforsubsequentruns.
Forinstance,SymbolicAIincludesOpenAI’sGPTmodelsasNeSyengine. ToonlysetorchangeOpenAIAPIkeys,
thefollowingcommandisusedbeforestartingaSymbolicAIinstance:
1 # Linux / MacOS
2 export OPENAI_API_KEY="<OPENAI_API_KEY>"
AftersettinguptheAPIkeys,theSymbolicAIlibraryisimportedinPythonusing:
1 import symai
Formorelow-levelchanges,westoreeverythingunderthe$HOME/.symaifolder,suchasthesymai.config.json,
whichstoreseverykey,bothregisteredandnotregistered.
D.2 OptionalInstallations
The SymbolicAI framework is designed to leverage multiple engines for a variety of operations. To fully utilize
these capabilities, you may install additional dependencies or set up the optional API keys for specific engines like
WolframAlpha,SerpApi,andothers. InFigure11weconceptuallyoutlinetheconnectionbetweentheutilizationofan
LLManditsinteractwithothertoolsandsolvers. Instructionsandoperationscanbeinitiatedbyanyuser,pre-scripted
knowledgebaseorlearnedmetaagent.
Conditions &
Language Interface (Output) Symbolic Engine (WolframAlpha)
Constraints
Search Engine (Google)
Large Language Model
Speech Engine (Whisper)
...
Language Interface (Input) Operation
User / Knowledge Base / Meta-Learner
Figure 11: The SymbolicAI framework integrates a Large Language Model (LLM) with diverse tools and solvers
through a conceptual interaction stack. The framework enables operations initiated by users, knowledge bases, or
meta-learners to be processed by the LLM, which interfaces with specialized engines such as WolframAlpha and
Whisperviaconditionsandconstraints,enhancingtheAI’sproblem-solvingcapabilities.
For instructions on additional installations, including the support of optional engines, refer to the documentation
provided with the framework. This documentation will give detailed steps on installing optional dependencies and
configuringadditionalAPIkeys.
D.3 Benchmark
TorunthebenchmarkevaluationyoufirstneedtoinstalltheExtensityAI/benchmarkplugin:
1 sympkg i ExtensityAI/benchmark
30
Tool
InterfaceThen you can go to the $HOME/.symai/packages/ExtensityAI/benchmark directory and run the benchmark
through:
1 python test.py
Tocompilemodelsoptimizedforyourmachine,followtheinstructionsfromtheExtensityAI/benchmarkrepository.
E ImplementationDetails
LetusnowdefinesomeSymbolobjectsandperformsomebasicmanipulations.
E.1 FuzzyComparison
For instance, let’s consider the use of fuzzy5 comparisons. Within SymbolicAI, it enables more adaptable and
context-aware evaluations, accommodating the inherent uncertainties and variances often encountered in real-world
data.
1 import numpy
2
3 s = symai.Symbol(’3.1415...’)
4 s == numpy.pi
1 :[Output]:
2 True
E.2 DynamicCasting
ByenablingsentencesubtractionanddynamiccastingwithinSymbolicAI,weutilizethegeneralizationcapabilityof
NeSyenginestomanipulateandrefinetext-baseddata,creatingmoremeaningfulandcontextuallyrelevantoutcomes.
The integration of dynamic casting with Symbol objects in our API allows the users to perform operations between
Symbolobjectsandvariousdatatypes,suchasstrings,integers,floats,lists,etc. withoutcompromisingonreadability
orsimplicity.
1 s = symai.Symbol(’Hello my enemy’)
2 s - ’enemy’ + ’friend’
1 :[Output]:
2 <class ’symai.expressions.Symbol’>(value=Hello my friend)
E.3 Translation
In today’s increasingly interconnected world, translation between languages is fundamental, making it an essential
feature.
1 s = symai.Symbol("Welcome to our tutorial.")
2 s.translate(’German’)
1 :[Output]:
2 <class ’symai.expressions.Symbol’>(value=Willkommen zu unserem Tutorial.)
E.4 Filtering,Ranking,Extraction
Incorporating data-agnostic operations like filtering, ranking, and pattern extraction into our API allow the users to
easilymanipulateandanalyzediversedatasets.
1 s = symai.Symbol(numpy.array([1, 2, 3, 4, 5, 6, 7]))
2 s.rank(measure=’numerical’, order=’descending’)
1 :[Output]:
2 <class ’symai.expressions.Symbol’>(value=[’7’, ’6’, ’5’, ’4’, ’3’, ’2’, ’1’])
5Notrelatedtofuzzylogic,whichisatopicunderactiveconsideration.
31E.5 Implications
OneofthemainobjectivesbehinddevelopingSymbolicAIwastofacilitatereasoningcapabilitiesinconjunctionwith
the statistical inference inherent in LLMs. Consequently, we can carry out deductive reasoning operations utilizing
the Symbol objects. For instance, it is feasible to establish a series of operations with rules delineating the causal
relationshipbetweentwosymbols. Thesubsequentexampleillustratestheutilizationofthe&operatortocomputethe
logicalimplicationderivedfromtheinteractionoftwodistinctsymbols.
1 s1 = symai.Symbol(’The horn only sounds on Sundays.’)
2 s2 = symai.Symbol(’I hear the horn.’)
3 s1 & s2
1 :[Output]:
2 <class ’symai.expressions.Symbol’>(value=It is Sunday.)
Intheaboveexample,the&operatoroverloadstheandlogicaloperatorandextendsitsfunctionality. Furthermore,we
canestablishmoresophisticatedlogicaloperatorsforand,or,andxorthatcanbegroundedinformalproofs,aswell
as utilize the NeSy engine to parse data structures before evaluating the expressions. This enables the definition of
bespokeoperationsforexecutingintricateandrobustlogicaloperations,incorporatingconstraintstovalidateoutcomes
andguidethecomputationtowardsthedesiredbehavior.
E.6 Customoperations
Thefollowingexampledemonstrateshowtodefineacustom==operationbyoverridingthe eq methodandproviding
acustompromptobjectwithalistofexamples:
1 import symai
2
3 class Demo(symai.Symbol):
4 def __eq__(self, other) -> bool:
5 # define nested function
6 @symai.core.equals(examples=symai.Prompt([
7 "1 == ’ONE’ =>True",
8 "’six’ == 7 =>False",
9 "’Acht’ == ’eight’ =>True",
10 ...
11 ]))
12 def _func(_, other) -> bool: # [optional] cast return type (1. below)
13 return False # [optional] default behavior on failure (2. below)
14 return _func(self, other)
As illustrated in the example, this is also the method we used to implement basic operations in Symbol, namely by
defininglocalfunctionsthatarethendecoratedwiththerespectiveoperationdecoratorfromthesymai.core.pyfile.
Thesymai.core.pyisacollectionofpre-definedoperationdecoratorsthatcanbequicklyappliedtoanyfunction.
Weuselocallydefinedfunctionsinsteadofdirectlydecoratingthemainmethodsfortworeasons:
1. Wewanttocastreturntypesoftheoperationoutcometosymbolsorotherderivedclassesthereof.
2. WedonotnecessarilywantallofouroperationstobesenttotheNeSyengineandmightneedtoimplement
adefaultbehavior.
Thisisachievedusingthe sym return typemethod,whichcanprovidecontextualizedbehaviorbasedonthedefined
returntype. MoredetailscanbefoundintheactualSymbolclass.
InthecontextofLLMs,zero-andfew-shotlearningdomainshaveemergedasessentialtechniques(Yaoetal.,2023b;
Shinnetal.,2023;Kimetal.,2023;Weietal.,2022b;Lyuetal.,2023;Pitisetal.,2023;Madaanetal.,2023;Wang
etal.,2022;Yeetal.,2023)6toenablemodelstogeneralizefromlimitedtrainingdataandadapttonewtaskswithout
requiringextensiveretraining. Thiscapabilitytolearnandperformtaskswithminimalexamplesishighlydesirable,
asitreducestheneedforlargelabeleddatasetsandallowsforfasterdeploymentinnewapplications. Inthissection,
wedemonstratehowourSymbolicAPIincorporatesPythondecoratorstodefinecustomoperationsinthezero-and
few-shotdomains.
6Thisisbynomeansanexhaustivelist,weonlypointthereadertosomeveryinfluentialandrecentresearch.
32Considerthefollowingexample,wherewedefineacustomoperationtogeneratearandomintegerbetween0and10
usingtheSymbolicAPIandPythondecorators:
1 import symai
2
3 class Demo(symai.Symbol):
4 def __init__(self, value = ’’) -> None:
5 super().__init__(value)
6
7 @symai.core.zero_shot(prompt="Generate a random integer between 0 and 10.",
8 constraints=[
9 lambda x: x >= 0,
10 lambda x: x <= 10
11 ])
12 def get_random_int(self) -> int:
13 pass
In this example, the @symai.core.zero shot decorator is used to define a custom operation that does not require
any examples, as the prompt is expressive enough. The zero shot decorator takes in two arguments: prompt and
constraints. The prompt defines the conditioning for our desired operation behavior, while the constraints are
used to validate the computed outcome, ensuring it meets our expectations. If the constraints are not fulfilled, the
implementationwouldresort tothespecifieddefault implementationorthedefaultvalue. Ifneitheris provided, the
Symbolic API raises a ConstraintViolationException. The return type in the example is defined as int. The
resulting value from the wrapped function must be of type int because of the specific implementation of the auto-
castingrealizedthrough->. Ifthecastfails,theSymbolicAPIraisesaValueError. Ifnoreturntypeisspecified,the
returntypedefaultstoAny.
The@symai.core.few shotdecoratorisageneralizedversionof @symai.core.zero shotandisusedtodefine
customoperationsrequiringexamples. Thefunctionsignatureofthefew shotdecoratorisasfollows:
1 def few_shot(prompt: str,
2 examples: Prompt,
3 constraints: List[Callable] = [],
4 default: Any = None,
5 limit: int = 1,
6 pre_processor: Optional[List[PreProcessor]] = None,
7 post_processor: Optional[List[PostProcessor]] = None,
8 **wrp_kwargs):
Thebehaviorofthepromptandconstraintsattributesissimilartothezero shotdecorator. Theexamplesand
limit arguments are new, with examples defining a list of instructions conditioning the NeSy engine, and limit
specifyingthemaximumnumberofexamplesreturned. Thepre processorandpost processorargumentsaccept
listsofPreProcessorandPostProcessorobjects,respectively,whichareusedtoprocesstheinputbeforebeingfed
intotheNeSyengineandtheoutputbeforebeingreturnedtotheuser. Thewrp kwargsargumentpassesadditional
argumentstothewrappedmethod,streamliningthemtowardstheNeSyengine,orotherengines.
E.7 Prompting
Inthissection,wediscussthedesignofpromptsandtheirroleinshapingthebehaviorofoperations. Promptsserveas
containersforinformationnecessarytodefinespecificoperations,andthePromptclassservesasthebaseclassforall
theotherPromptclasses. Considerthefollowingexample,wherewedefineaPromptforcomparingtwovaluesusing
theNeSyengine. Init,whenthe<=operationontwoSymbolobjectswillberesolved,theNeSyengineevaluatesthem
inthecontextoftheCompareValuesprompt.
1 class CompareValues(symai.Prompt):
2 def __init__(self) -> symai.Prompt:
3 super().__init__([
4 "4 > 88 =>False",
5 "-inf < 0 =>True",
6 "inf > 0 =>True",
7 "4 > 3 =>True",
8 "1 < ’four’ =>True",
9 ...
10 ])
331 res = symai.Symbol(1) <= symai.Symbol(’one’)
1 :[Output]:
2 True
This evaluation returns True, as the fuzzy comparison operation conditions the engine to compare the two Symbol
objects based on their semantic meaning. More generally, the semantics of Symbol operations may vary depending
on the context hierarchy of the Expression class and the operations used. We used three main prompt designs:
Context-basedPrompts, OperationalPrompts, and Templates. Prompts canbe curatedeither throughinheritance or
composition. Forinstance,thestaticcontextcanbedefinedbyinheritingfromtheExpressionclassandoverwritingthe
static contextproperty,whileanOperationandTemplatepromptcanbecreatedbyprovidingaPreProcessor
tomodifytheinputdata.
Wewillnowprovideamoredetailedexplanationforeachpromptdesign:
1. Context-based Prompts are considered optional and can be defined in a static manner, either by sub-
classingtheExpressionclassandoverwritingthestatic contextproperty,oratruntimebyupdatingthe
dynamic context property or passing a payload kwargs to a method. Below is an example of using the
payloadkwargsthroughthemethodsignature:
1 # creating a query to ask if an issue was resolve or not
2 s = symai.Symbol("<some-community-conversation>")
3 q = s.query("Was the issue resolved?")
4 # write manual condition to check if the issue was resolved
5 if ’not resolved’ in q:
6 # do a new query but payload the previous query answer to the new query
7 s.query("What was the resolution?", payload=q)
8 ...
9 else:
10 pass # all good
11
Regardlessofhowthecontextisset,thecontextualizedpromptdefinesthedesiredbehaviorof Expression
operations. Forinstance,ifwewanttooperateinthecontextofaDSLwithouthavingtooverwriteeachbase
classmethod,wecanusethisapproach7.
2. OperationalPromptsdefinethebehaviorofanatomicoperationandarethereforemandatorytoexpressthe
nature of such an operation. For example, the + operation is used to add two Symbol objects together, and
the+operationpromptexplainsitsbehavior. Theexampleskwargsprovideanotheroptionalstructurethat
conditionstheNeSyenginewithasetofinstructions.
3. Template Prompts are optional and encapsulate the resulting prediction to enforce a specific format. For
example,togenerateHTMLtags,wecanuseacurated<html>...</html>template. Thistemplateenforces
the NeSy engine to begin the generation process already in the context of an HTML tags format and not
produceirrelevantdescriptionsaboutitstask.
E.8 Complexexpressions
Wewillnowattempttoobtainlogicalanswersbasedonquestionsofthekind:
• Alineparalleltoy =4x+6passesthrough(5,10). Whatisthey-coordinateoftheintercept?
• Bobhastwosons,JohnandJay. Jayhasonebrotherandfather. Thefatherhastwosons. Jay’sbrotherhasa
brotherandafather. WhoisJay’sbrother?
• Is1000biggerthan1063.472?
Tosolvethesetasks,wewouldinitiallyemployaseriesofoperationstoidentifythemostsuitableengineforhandling
thespecificrequirements. Subsequently,wewouldpreparetheinputtailoredtotheselectedengine.
1 val = "<one of the examples above>"
2
3 # First define a class that inherits from the \texttt{Expression} class
7Seemoredetailsinthisnotebook.
344 class ComplexExpression(symai.Expression):
5 # write a method that returns the causal evaluation
6 def causal_expression(self):
7 pass
8
9 # instantiate an object of the class
10 expr = ComplexExpression(val)
11 # set WolframAlpha as the main expression engine to use
12 expr.command(engines=[’symbolic’], expression_engine=’wolframalpha’)
13 # evaluate the expression
14 res = expr.causal_expression()
Theimplementationofcausal expressioncouldinprinciplelooklikethis:
1 def causal_expression(self):
2 # very which case to use ‘self.value‘ contains the input
3 if self.isinstanceof(’mathematics’):
4 # get the mathematical formula
5 formula = self.extract(’mathematical formula’)
6 # verify which problem type we have
7 if formula.isinstanceof(’linear function’):
8 # prepare for wolframalpha
9 question = self.extract(’question sentence’)
10 req = question.extract(’what is requested?’)
11 # get coordinate point / could also ask for other points
12 x = self.extract(’coordinate point (.,.)’)
13 # concatenate to the question and formula
14 query = formula | f’, point x = {x}’ | f’, solve {req}’
15 res = self.expression(query) # send prepared query to wolframalpha
16
17 elif formula.isinstanceof(’number comparison’):
18 res = formula.expression() # send directly to wolframalpha
19
20 ... # more cases
21
22 elif self.isinstanceof(’graph construction’):
23 sentences = self / ’.’ # first split into sentences
24 graph = {} # define graph
25 for s in sentences:
26 sym = symai.Symbol(s)
27 relations = sym.extract(
28 # extract and split by pipe
29 ’connected entities (e.g. A has three B => A | A: three B)’) / ’|’
30 for r in relations:
31 ... # add relations and populate graph; reading suggestion
32
33 ... # more cases
34
35 return res
The aforementioned example demonstrates the utilization of the causal expression method, which allows us to
extractinformationthatcanberesolvedeithermanuallyorthroughexternalsolvers,sayWolframAlpha. Initially,when
utilizingtheGPT-3backend,weanticipatedasignificantengineeringefforttodevelopsuchacomplexexpression,as
theGPT-3backendfrequentlystruggledwithaccurateinformationextractionandcomparisonresolution. However,we
remainedconfidentinthefield’sprogress,specificallywithfine-tunedmodelslikeChatGPTutilizingRLHF.Wewere
delightedtowitnessthesechallengesbeingfurthertackledthroughthelatestGPT-4model(OpenAI,2023).
Furthermore,itisworthhighlightingthat,givensufficientdata,wecouldrefinemethodsforinformationextractionor
knowledgegraphconstructionfromnaturallanguage,enablingmoreintricatereasoningtasks,suchasthosepreviously
mentioned. WealsodirectreaderstorecentpublicationsonText-to-Graphtranslations,especiallytheveryinfluential
CycleGT(Guoetal.,2020). Thisapproachallowsustoanswerqueriesbysimplytraversingthegraphandextracting
therequiredinformation.
Lastly, recent research (Kıcıman et al., 2023; Ellis, 2023) has demonstrated that algorithms based on GPT-3.5 and
GPT-4establishnewstate-of-the-artaccuracyonmultiplecausalbenchmarks,whilealsoexhibitinguniquecapabilities
35previouslyconsideredexclusivetohumans,suchasgeneratingcausalgraphsandidentifyingbackgroundcausalcontext
fromnaturallanguage. ThispointstothepotentialforLLMstobeusedalongsideexistingcausalmethodsasproxies
for human domain knowledge, reducing human effort in setting up causal analyses and ultimately accelerating the
widespreadadoptionofcausalmethods. Moreover,recentadvancesinLLMshaveopenednewfrontiersforresearch,
practice,andadoptionofcausalreasoning,transformingthewaycausalanalysisisconductedandbroadeningthescope
ofapplicationsforourframework.
OneofthemostprominentillustrationsofthisconceptisexhibitedbyWord2Vec(Mikolovetal.,2013). Word2Vec
generatesdenserepresentationsofwordsbytrainingashallowneuralnetworktopredictawordbasedonitsneighboring
words within a text corpus. These resulting vectors are extensively utilized in various natural language processing
applications,includingsentimentanalysis,textclassification,andclustering.
DrawingparallelswithWord2Vec,ourobjectiveistoexecutecontextualizedoperationsondifferentsymbols. However,
thekeydistinctionliesinthefactthatweoperatewithinthenaturallanguagedomain, asopposedtoavectorspace.
Consequently,thisgrantsusthecapabilitytoconductarithmeticonwords,sentences,paragraphs,andthelike,while
simultaneouslyvalidatingtheoutcomesinahuman-readableformat.
Thefollowingexample,weillustratethemethodologyforevaluatingsuchanexpressionthroughastringrepresentation:
1 s = symai.Symbol(’King - Man + Woman’)
2 s.expression()
1 :[Output]:
2 <class ’symai.expressions.Symbol’>(value=Queen)
IncontrasttotheSymbolobject,theExpressionrepresentsanon-terminalsymbol. Itallowsforfurtherevaluation
andextendstheSymbolclassbyoverwritingthe call method. Itservesasthefoundationforallotherexpressions
and possesses additional capabilities, namely to fetch data from URLs, search the internet, or open files. These
operationsareintentionallyseparatedfromSymbol,astheydonotutilizethevalueattributeoftheSymbolclass.
E.9 Composition
E.10 Sequences
Sequences offer a multitude of advantages in the realm of Expression objects, as they facilitate the creation of
more sophisticated structural configurations. By embodying the Sequence expression, multiple expressions can be
effectivelyevaluatedatruntime,thusenhancingtheflexibility,modularity,andadaptabilityoftheframework.
1 # first import all expressions
2 from symai.components import *
3 # define a sequence of expressions
4 Sequence(
5 Clean(),
6 Translate(),
7 Outline(),
8 Compose(),
9 )
E.11 Streams
As demonstrated earlier, creating contextualized prompts refines the behavior of operations in the NeSy engine.
However, this also consumes a considerable portion of the available context size. Given a limited context size,
this constraint may pose challenges. Fortunately, the Stream processing expression offers a solution by opening a
data stream and performing chunk-based operations on the input stream. Stream expressions can encapsulate other
expressions. Forinstance,chunkscanbemanagedusingaSequenceexpression,whichpermitsmultiplecompositional
operationsinasequentialmanner. TheexamplebelowillustratesthedefinitionofaStreamexpression:
1 Stream(
2 Sequence(
3 Clean(),
4 Translate(),
5 Outline(),
6 Embed()
367 )
8 )
Inthiscase,astreamisopenedandaSequenceexpressionispassed,whichcleans,translates,outlines,andembeds
theinput. Internally, thestreamoperationestimatestheavailablemodelcontextsizeandsegmentsthelengthyinput
textintosmallerchunkstransmittedtotheinnerexpression. Thereturnedobjecttypeisagenerator.
The limitation of this approach is that the resulting chunks are processed independently, lacking shared context or
informationamongthem. Toaddressthis,theClusterexpressioncanbeemployed,mergingtheindependentchunks
based on their similarity, as it illustrated in Figure 12. By merging individual chunks by clustering their contents,
Compose
Cluster 1 Cluster 2
Summarize 1 Summarize 2 Summarize 3
Data Stream
... Data Chunk 1 Data Chunk 2 Data Chunk 3 ...
Figure 12: Stream processing expression in NeSy engine, illustrating data stream segmentation into chunks, each
undergoingoperationslikecleaning,outlining,andembedding. TheClusterexpressionthenmergeschunksbasedon
similarity,allowingcontextuallyrelatedinformationtobeconsolidatedmeaningfully. Nodesummariesaregenerated
by extracting key labels from each cluster’s content, overcoming context size limitations and maintaining shared
informationamongprocessedchunks.
contextuallyrelatedinformationcanbeconsolidatedinameaningfulmanner. Additionally,theclusteredinformation
canbelabeledbystreamingthrougheachcluster’scontentandextractingthemostpertinentlabels,yieldinginterpretable
nodesummaries.
Thecompleteexampleisdepictedasfollows:
1 stream = Stream(
2 Sequence(
3 Clean(),
4 Translate(),
5 Outline(),
6 )
7 )
8
9 s = symai.Symbol(’<some long text>’)
10 res = symai.Symbol(list(stream(s)))
11 expr = Cluster()
12 expr(res)
Subsequently, this process can be recursively repeated on each summary node to construct a hierarchical clustering
structure. Aseachnoderepresentsasummarizedsubsetoftheoriginalinformation,thesummarycanfunctionasan
index. Theresultingtreecanbeutilizedtonavigateandretrievetheoriginalinformation,transformingthelargedata
stream problem into a search problem. Alternatively, vector-based similarity searches can be employed to identify
similarnodes. Forsearchingwithinavectorspace,dedicatedlibrariessuchasAnnoy(Spotify,2017),Faiss(Johnson
etal.,2019),orMilvus(Wangetal.,2021a)canbeused.
Insummary,Streamexpressionsoffertheadvantageofprocessinglargedatastreamsinamoreefficientandorganized
manner, whilealsoenablingtheintegrationwithotherexpressionslikeSequenceandClusterexpressions. These
37combinationsallowforamoreeffectiveapproachtohandlingcontextlimitations,promotingtheextractionofmeaningful
informationandimprovingtheoverallperformanceoftheframework.
E.12 Errorhandling,debugging,andexplainability
Effectiveerrorhandlinganddebuggingareessentialforensuringtherobustnessandreliabilityofanysoftwaresystem,
whileexplainabilityisessentialforunderstandingtheunderlyingbehaviorofthesystem,especiallyinthecontextof
AI-drivenapplications. Bydevelopingasystemthatisbothtransparentandinterpretable,wecangainvaluableinsights
intotheperformanceoftheNeSyenginesandidentifypotentialareasforimprovement.
E.13 Errorhandling
OneofthefundamentalaspectsoftheSymbolicAIAPIisbeingabletogeneratecode. Consequently,errorsmayarise,
andhandlingthemcontextuallybecomesvital. Inpursuitofaself-evolvingAPI,weintroducetheTryexpression,which
includesbuilt-infallbackstatementsandautomaticallyretriesexecutionafterperformingdedicatederroranalysisand
correction. Thisexpressionanalyzesboththeinputandtheerror,conditioningitselftoresolvetheerrorbymodifying
the original code8. If the fallback expression succeeds, the result is returned; otherwise, the process is repeated for
thespecifiednumberofretries. Ifthemaximumnumberofretriesisreachedwithoutresolvingtheissue,theerroris
raisedagain.
Considertheexampleofexecutingpreviouslygeneratedcodethatcontainsasyntaxerror. ByemployingtheExecute
expression, wecanevaluatethegeneratedcode, whichtakesasymbolandproceedswiththeexecution. Despitethe
initialfailure,theTryexpressionresolvesthesyntacticerror,returningthecorrectedandevaluatedcode:
1 expr = Try(expr=Execute())
2 s = symai.Symbol(’a = int("3,")’) # some code with a syntax error
3 expr(s)
1 :Output:
2 a = 3
Whilenotallerrorscanberesolvedaseasilyasthedemonstratedsyntaxerror,wecontinuetoexploremoresophisticated
errorhandlingmechanisms,includingtheuseofstreamsandclusteringtoaddresserrorsinahierarchicalandcontextual
manner.
8ThisissimilartotherecentlyreleasedAuto-GPTapplication.
38