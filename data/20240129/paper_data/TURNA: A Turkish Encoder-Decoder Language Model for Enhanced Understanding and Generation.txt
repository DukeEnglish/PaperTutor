TURNA: A Turkish Encoder-Decoder Language Model for Enhanced
Understanding and Generation
GökçeUludog˘an1 and ZeynepYirmibes¸og˘luBalal1 and FurkanAkkurt1
Meliks¸ahTürker1,2 and OnurGüngör1 and SusanÜsküdarlı1
1DepartmentofComputerEngineering,BogaziciUniversity
2 VNGRS-AI
{gokce.uludogan,furkan.akkurt,zeynep.yirmibesoglu,
meliksah.turker,onurgu,suzan.uskudarli}@bogazici.edu.tr
Abstract oftendonotperformwellintasksrequiringadeep
understandingoflanguage-specificnuances,such
The recent advances in natural language pro-
asdependencyparsingandnamedentityrecogni-
cessing have predominantly favored well-
tion(Virtanenetal.,2019;Baumann,2019;Tanvir
resourced English-centric models, resulting
in a significant gap with low-resource lan- et al., 2021) and lag behind monolingual models
guages. In this work, we introduce the lan- of the same scale (Rust et al., 2021; Nozza et al.,
guagemodelTURNA,whichisdevelopedfor 2020).
the low-resource language Turkish and is ca-
Recently,pretrainedlanguagemodelsbuiltupon
pableofbothnaturallanguageunderstanding
transformers(Vaswanietal.,2017)havedominated
and generation tasks. TURNA is pretrained
NLP.Thesemodelsvaryintermsoftheirarchitec-
withanencoder-decoderarchitecturebasedon
theunifiedframeworkUL2withadiversecor- turesandobjectives(i.e.,causallanguagemodeling
pus that we specifically curated for this pur- and denoising objectives). The architectures are
pose. We evaluated TURNA with three gen- commonlyclassifiedasencoder-only,decoder-only,
erationtasksandfiveunderstandingtasksfor orencoder-decodermodels. Encoder-onlymodels
Turkish. The results show that TURNA out-
aretypicallytrainedwithdenoisingobjectivesand
performsseveralmultilingualmodelsinboth
focusonunderstandingtasks(Devlinetal.,2019;
understandingandgenerationtasks,andcom-
Clark et al., 2020). Decoder-only models are de-
peteswithmonolingualTurkishmodelsinun-
derstandingtasks. TURNAismadeavailableat signed for generation tasks with causal language
hf.co/boun-tabi-LMG/turna. modeling(Radfordetal.,2019;Brownetal.,2020;
Touvron et al., 2023). Finally, encoder-decoder
1 Introduction
modelsdealwithNLPtasksthatrequirebothun-
Recent advances in natural language process- derstanding and generation of texts (Dong et al.,
ing(NLP)havepredominantlyproducedEnglish- 2019; Tay et al., 2023). Towards this end, the
centric models (Devlin et al., 2019; Clark et al., Text-to-TextTransformer(T5)(Raffeletal.,2020)
2020; Radford et al., 2019; Brown et al., 2020; employsanencoder-decoderarchitecturethatwas
Touvronetal.,2023;Jiangetal.,2023). English- pretrained with a denoising objective known as
centricmodelshavebenefitedfromthevastamount spancorruption. TheUnifyingLanguageLearning
oftrainingdatagatheredfromanabundanceofEn- (UL2)framework (Tayetal.,2023)proposesthe
glishresourcespresentontheweb. Assuch,these Mixture-of-Denoisers(MoD)pretrainingobjective
modelsbecomeutilizedinapplicationsandfuelan thatcombinesseveraldenoisingobjectives. Bycou-
abundanceoffurtherresearchleadingtothestate- plingtheMoDobjectivewithanencoder-decoder
of-the-artperformancesacrossvarioustasks(Tou- architecture,theyhaveachievedstate-of-the-artre-
vronetal.,2023;Jiangetal.,2023). Ontheother sultsinarangeofNLPtasks.
hand,low-resourcelanguagesfacechallengesdue For Turkish, while encoder-only models ex-
to the lack of data and limited computational re- ist(Schweter,2020),thereisaneedforlarge-scale
sources,leadingtoasignificantgapbetweenmod- pretrainedmodelsthatcanperformbothnaturallan-
elstrainedonwell-resourcedlanguagesandthose guageunderstanding(NLU)andgeneration(NLG).
focusingonlow-resourcelanguages. Multilingual ThisworkaimstodevelopsuchamodelforTurk-
modelshavebeenproposedthatattempttobridge ishthatperformswellacrossavarietyoftasks. To-
thisgap(Devlinetal.,2019;Conneauetal.,2020; wardsthisend,wefirstcompileadiverserangeof
Xueetal.,2021;Liuetal.,2020). However,they corpora for pretraining, including web data, sci-
4202
naJ
52
]LC.sc[
1v37341.1042:viXraentific articles, graduate theses, books, creative 2020). Thiscollectionincludesseveralvariations
writing,andtranscriptionsofparliamentaryspeech. of BERT (Devlin et al., 2019), DistilBERT (Sanh
Subsequently, we pretrain TURNA, which adopts et al., 2019), ConvBERTurk (Jiang et al., 2020),
anencoder-decoderarchitecturefollowingtheUL2 and ELECTRA (Clark et al., 2020), with varying
framework (Tay et al., 2023). Furthermore, we cases,vocabulary,andmodelsizes. Mostmodels
collectseveralpubliclyavailableTurkishdatasets weretrainedona35GBcorpuswith4.4Btokens,
fromavarietyoftasksforbenchmarkingpurposes. drawn from the Turkish OSCAR corpus (Abadji
Ourcontributionsareasfollows: etal.,2022),aWikipediadump,andvariousOPUS
corpora (Tiedemann, 2012). Some models, like
• Thereleaseof TURNA1,thefirstunifiedlan-
ConvBERTurk and ELECTRA, were also trained
guage model capable of both understanding
on the Turkish portion of the mC4 corpus. The
andgenerationtasksinTurkish. Thusfar,this
modelshavebeenevaluatedonvariousdownstream
model is the largest of its kind, which has
tasks, such as part-of-speech tagging, named en-
1.1B parameters and is trained on a diverse
tity recognition, and question answering. They
range of corpora consisting of 43B tokens
often outperform their multilingual counterparts,
fromvariousdomains.
mBERT (Devlin et al., 2019) and XLM-R (Con-
neauetal.,2020). However, thesemodelsareall
• The evaluation of TURNA on 13 datasets
encoder-only models, most suitable for language
across eight tasks, showing that it surpasses
understandingtasks. ThereisaclearneedforTurk-
multilingual models across many tasks and
ishmodelsthatexcelinbothlanguageunderstand-
it either outperforms or is on par with
ingandgeneration.
BERTurk(Schweter,2020)(aTurkishmono-
lingualencoder-onlymodel)inunderstanding
tasks. 2.3 Pretrainingobjectives
• Thereleaseofapubliclyavailableopensource Recently, pretrained language models based on
code for data collection, filtering2, model transformershavebeendominantintheNLPfield,
training3,andfine-tuning4. exhibiting variations in both components and ob-
jectives. Modelsthatexclusivelyemployencoders,
2 RelatedWork
typically trained with denoising objectives, are
gearedtowardunderstandingtasks,asexemplified
2.1 MultilingualLanguageModels
in works such as Devlin et al. (2019) and Clark
Multilingual language models are used for tasks
etal.(2020). Conversely,modelsthatexclusively
thatinvolvemultiplelanguagesandresourceswith
usedecodersaredesignedforgenerationtasks,em-
limiteddata. Turkish,consideredasalow-resource
ployingcausallanguagemodeling,asdemonstrated
language,ismoderatelyrepresentedinthesemod-
in various studies (Radford et al., 2019; Brown
els, such as mBERT (Devlin et al., 2019), XLM-
etal.,2020;Touvronetal.,2023). TheText-to-Text
R (Conneau et al., 2020), mBART (Liu et al.,
Transformer(T5)(Raffeletal.,2020),ontheother
2020),mT5(Xueetal.,2021),XGLM(Linetal.,
hand, employs an encoder-decoder architecture
2022), mGPT (Shliazhko et al., 2022), and mDe-
andundergoespretrainingwithadenoisingobjec-
BERTa(Heetal.,2023). However,thesemodels’
tivereferredtoasspancorruption. UniLM(Dong
scale-to-performance ratios are often not as effi-
etal.,2019)isalsoanencoder-decodermodel,but
cientasthatofmonolingualmodelsforlanguage-
pre-trainedusingunidirectional,bidirectional,and
specificapplicationswithabundantdata(Rustetal.,
sequence-to-sequence language modeling. This
2021;Nozzaetal.,2020).
canbeseenasacombinationofcausalanddenois-
2.2 TurkishLanguageModels ingobjectives. Recently,Tayetal.,2023proposed
thatvariouspretrainingobjectivescanberecastas
A series of BERT models for Turkish known as
each other. They introduced the UL2 framework
BERTurk have already been trained (Schweter,
basedonapretrainingobjectivecalledMixture-of-
1hf.co/boun-tabi-LMG/turna Denoisers (MoD), which combines different pre-
2github.com/boun-tabi-LMG/turkish-academic-
trainingparadigms. Theycompareddecoder-only
text-harvest
3github.com/boun-tabi-LMG/turna andencoder-decodermodelstrainedwiththeMoD
4github.com/boun-tabi-LMG/turkish-lm-tuner objectiveandfoundthatencoder-decodermodelsoftenperformbetter. Notably, byusingtheMoD uctsofhighereducationinTurkey,wereaccessed
objectiveandmoderatelyscalingupthemodel,they fromTurkey’sNationalThesisCenter6. Fromthis
achievedstate-of-the-artperformanceonadiverse repository,wedownloaded486,166thesesmarked
setofNLPtasksincludingunderstandingandgen- asTurkish,whichcomposesourYökTezscientific
erationtasks. corpus.
The collected documents were in PDF format.
3 Data
For text extraction, we utilized the Apache Tika
parser7. We applied a rigorous cleaning and fil-
WecompiledadiverseTurkishmonolingualdataset
tering strategy to remove undesired content like
topretrainourmodel. Ourdatasetcomprisesofa
pagenumbers,equations,tableentries,andsimilar
webcorpus,scientificcorporagatheredfromTurk-
unnecessary tokens introduced by the extraction
isharticlesandgraduatetheses,Turkishbooks,a
process,asdetailedinSectionA.1.
corpusofspeciallycollectedwritingsfromBilkent
University,andtranscriptionsofparliamentaryde- Weused99.99%ofthecleanedDergiparkdoc-
bates. Thedetailsofeachcorpusareexplainedin uments for training and the rest for validation, to
thefollowingsubsections,andthetrainingcorpora avoidover-inflationofthevalidationsetduetothe
statisticsaresummarizedinTable1. highnumberofdocuments. ForYökTez,99.999%
During data splitting, we ensured that the vali- ofthedocumentswereusedfortraining. Thefinal
dation set of each dataset contain a minimum of number of documents and the number of tokens
100Ktokens. Theresultingtrain-validationsplits afterlineanddocument-wisefilteringofourscien-
arereportedundereachsubsection. tifictrainingcorporaarelistedinTable1.
3.1 WebCorpora 3.3 BookCorpus
mC4 (Raffel et al., 2020) and OSCAR- TheBookCorpusisacompilationof5,080Turkish
2201 (Abadji et al., 2022) are two large fictionandnon-fictionbooks. WecleanedtheBook
multilingualwebcorpora. TheirTurkishsections Corpusinasimilar,butsimplerheuristic. Wefirst
contain87.7Mand10.8Mwebpages,respectively, standardizedthepunctuationandremovedinvalid
and result in 98.5M web pages when combined. characters. Theinitial100linesofeachbookhave
Although significant in quantity, web data is full beenfilteredoutiftheycontainauthor,translator,
of noise that is not considered part of the natural orpublishinginformation. Wedroppedanylinein
language,suchastitlesandkeywords. Therefore, thebookthatisallnumericorcontainsaURLor
suchcorporashouldbecleanedbeforebeingused ane-mail. Aftertheinitial70%lines,wetruncated
fortraining. TheOSCARandmC4corporaused the lines after a keyword indicating a bibliogra-
inthisworkwerecleanedbytheVNGRS-AIteam phy, notes, or a list of works of the author or the
using a set of heuristics and rules. The cleaned publishinghouse. 99.97%ofthebookswereused
version of the combined web corpus contains fortraining(5,078books),andtheremainingtwo
50.3Mpages. booksforvalidation.
3.2 ScientificCorpora
3.4 BilkentCreativeWritings
Tocreateacorpusinthescientificdomaincharac-
The Bilkent Creative Writings corpus comprises
terizedbyitsformalandinformativelanguagestyle,
8,630documentsproducedbyBilkentUniversity
wecollectedarticlesandtheseswritteninTurkish.
students while taking creative writing courses in
WedownloadedthearticlesfromDergiPark5,ama-
Turkish8. We cleaned this data like the book cor-
jor platform for Turkish academic journals. Our
pusbyremovinglinescontainingspecialkeywords
initial collection included 407,146 articles, all in
(suchasthewordforassignmentinTurkish)and
PDFformatandlabeledasTurkish. Thesearticles
truncatingafterbibliographies. 8,457ofthemwere
weresourcedfrom1,857distinctjournals,compris-
usedfortrainingandtherestwasusedforvalida-
ingadiverserangeoftopics. Thesearticlesform
tion.
ourDergiparkscientificcorpus.
In addition to articles, we also collected scien- 6tez.yok.gov.tr/UlusalTezMerkezi
tifictextsintheformoftheses. Thesetheses,prod- 7github.com/apache/tika
8github.com/selimfirat/bilkent-turkish-
5dergipark.org.tr writings-datasetCorpus Type #Docs #Tokens(B) Wikipedia dump dated September 17, 2021, us-
OSCAR&mC4 Web 50,336,214 25.33 ingtheSentencePieceimplementation11(Kudoand
Dergipark Scientific 334,429 1.78 Richardson, 2018). This tokenizer12 is provided
Yöktez Scientific 475,817 15.24
Books Literary 5,078 0.61 by the VNGRS-AI Team. The initial vocabulary
BilkentCreativeWritings CreativeText 8,457 0.01
size of 32,000 was expanded to 32,128 with the
ParlaMintTR Dialogue 1,333 0.07
additionof128sentineltokensusedbypretraining
Table1: TrainingDatasets objectives.
4.2 PretrainingObjectives
3.5 ParlaMintTR
The pretraining was performed with Mixture-of-
The ParlaMintTR corpus is assembled from the
Denoisers(MoD),consistingofseveraldenoising
CLARIN Flagship project9 and consists of the
objectives, which were shown to achieve better
TurkishportionofparliamentarydebatesinEurope
downstreamperformance(Tayetal.,2023). These
(1,335documents). Weusedtheoriginalformof
objectivesareR-denoising(regulardenoising),S-
thedebateswithoutapplyingspecialcleaningorfil-
denoising(sequentialdenoising),andX-denoising
tering. 1,333ofthedebateswereusedfortraining,
(extreme denoising), each characterized by the
andtwodebatesforvalidation.
meanlengthofthecorruptedspans,theratioofcor-
ruptedtokens,andthenumberofcorruptedspans.
4 Methodology
R-denoisingfollowsthestandardspancorruption
4.1 Model method of T5, selecting spans of 2 to 5 tokens,
coveringabout15%oftheinput. Thetaskisthen
We used an encoder-decoder Transformer
topredictthecorruptedtokensinthedecoderout-
model10 (Raffel et al., 2020) for TURNA. This
put. S-denoising, on the other hand, corrupts a
choice was based on the finding that encoder-
continuousportionfromarandompointinthein-
decodermodelssurpassdecoder-onlymodelswhen
put,accountingforapproximately25%oftheinput.
theUL2objectiveisused,asdemonstratedinTay
SimilartoR-denoising,thisobjectiveaimstopre-
etal.,2023. Furthermore,theencodercomponent
dictasinglecorruptedspan. However,itissimilar
canstillbeemployedeffectivelyforunderstanding
tostandardcausallanguagemodelinginthemodel-
taskswhencoupledwithtask-specificclassification
ingapproach. X-denoisingisdesignedasaninter-
heads,thusreducingthemodelparametersbyhalf.
polationbetweenR-denoisingandS-denoising. It
Due to our limited computational resources, we
aimstocorrupt50%oftheinputonaverage. This
opted for the Large36L configuration (Tay et al.,
isachievedthroughavaryingmixofmanyshortor
2021)forourmodel. Thisconfigurationrequires
fewerlongcorruptedspans,exposingthemodelto
only 37% of the parameters of a model configu-
bothdenoisingandcausallanguagemodeling-like
ration of comparable size, yet still outperforms
objectives. Duringpretraining,theseobjectivesare
it.
randomlyassignedtoeachinputsequence,witha
TURNAhas36encoderanddecoderlayers,each
distribution of 40% each for R- and X-denoisers
with 16 attention heads. The model’s token em-
and20%forS-denoising.
beddings are 1024 dimensional. The multi-layer
Themodeldifferentiatesbetweenthesedenois-
perceptronlayershave2816hiddendimensionsand
ers by using specific sentinel tokens at the begin-
employ Gated GeLu activations (Shazeer, 2020).
ningofsamples: [NLG]forX-denoiser,[NLU]for
Theparametersoftheinputandclassificationlay-
R-denoiser,and[S2S]forS-denoiser.
ers are not shared. These architectural choices
resultinamodelwith1.1Bparameters.
4.3 Implementationdetails
For tokenization, we used a unigram subword
tokenizer (Kudo, 2018) trained on 10GB of text
Pretraining. We pretrained TURNA for a total
of 1,740,000 steps with a batch size of 48 and a
thatconsistsofrandomsubsetsofOSCAR(Abadji
source and target sequence length of 512 using a
et al., 2022), OPUS (Zhang et al., 2020) and
singlev3-8typeTPUwiththeT5X13 library. This
9clarin.eu/parlamint
10Specifically, we used the version 1.1 of 11github.com/google/sentencepiece
the official T5 implementation described at 12github.com/vngrs-ai/vnlp/tree/main/
github.com/google-research/text-to-text-transfer- vnlp/turkish_word_embeddings
transformer/blob/main/released_checkpoints.md#t511 13github.com/google-research/t5xconfigurationresultsin TURNA beingexposedto and their corresponding values can be found in
42.7Btokensattheendofitstraining. Wedisabled Table7.
thedropoutduringpretrainingbutenableditduring We used beam decoding with a beam size of
fine-tuning. 4 and early stopping to generate predictions. For
The pretraining data is a mixture of samples summarizationandtitlegenerationtasks,wealso
fromthecollecteddatasets. Toensureafairrepre- applied a length penalty of 2 and enforced a no-
sentationofdifferentlanguagecharacteristics,we repeat n-gram size of 3 to ensure the diversity of
randomly selected samples from each dataset ac- theoutputandpreventrepetitionofsequences.
cordingtotheirproportions: WebCorpora(50%),
YökTez (25%), DergiPark (10%), Book Corpus 5 Experiments
(10%), ParlaMintTR (3%), and Bilkent Creative
Writings(2%). 5.1 Fine-tuningtasks
Baselines. Wecomparedourmodelwithmultilin- Thissectionprovidesanoverviewofdownstream
gualmodels: mT5,specificallymT5-large14 (Xue tasks used to evaluate the model. These tasks as-
et al., 2021), and mBART15 (Liu et al., 2020), sessmodelcapabilitiesacrossvariousdomains,and
as well as a monolingual encoder-only model, include both natural language understanding and
BERTurk16 (Schweter,2020),whereapplicable. generationtasks. Theunderstandingtasksinclude
textclassification,naturallanguageinference,se-
Fine-tuning. We fine-tuned the models using
mantictextualsimilarity,namedentityrecognition,
Hugging Face’s transformers library17 (Wolf
andpart-of-speechtagging. Thegenerationtasks
et al., 2020) on NVIDIA A40 GPUs. The stan-
compriseparaphrasing,summarization,andnews
dardtext-to-textformulationisusedforfine-tuning
titlegeneration.
the encoder-decoder models, i.e., TURNA, mT5
andmBART.Additionally,wefine-tunedTURNA’s
Paraphrasing. This task involves rephrasing a
encoderwithatask-specificheadforcertainunder-
giventextwhileretainingtheoriginalmeaning. It
standingtasks,referringtoitas TURNA-Encoder.
assesses the model’s understanding of semantics
The models were optimized for 10 epochs with
anditsabilitytogeneratediversetexts. Weutilized
an early stopping patience of 3 epochs. We used
two paraphrasing datasets, constructed from par-
theAdaFactoroptimizer(ShazeerandStern,2018)
allel corpora via machine translation and filtered
with a learning rate of 1×10−3 to tune TURNA
basedonsemanticsimilarity(Alkurdietal.,2022).
and mT5 models, without a scheduler. However,
These are TAT, which contains paraphrases from
ourattemptsatfine-tuningthemBARTmodelwith
Tatoeba20, and OST, which includes pairs from
the AdaFactor optimizer did not yield a satisfac-
OpenSubtitles2018(Lisonetal.,2018).
tory training loss curve. Consequently, we opted
fortheAdamWoptimizer(LoshchilovandHutter,
Summarization. Similartoparaphrasing, sum-
2017) with a learning rate of 5×10−5 and a lin-
marizationalsorephrasesatext. However,itaims
earscheduler. Thesameoptimizerandscheduler
toproduceacondensedversionthatonlyincludes
settingswereappliedforfine-tuningtheBERTurk
key information. Consequently, it imposes addi-
and TURNA-Encodermodels. Duetoourlimited
tional constraints on the model’s generative ca-
computationalresources,wecouldnotperformhy-
pabilities. For evaluation, we used two datasets:
perparameter tuning and used the recommended
TRNews(BaykaraandGüngör,2022)andtheTurk-
fine-tuning settings for AdaFactor18 and default
ishsubsetofMLSUM(Scialometal.,2020).
trainer settings19 for AdamW. For each task and
dataset, the batch size, and maximum input and News Title Generation. Generating titles for
targetlengthparameterswereindividuallyselected, newsarticlesevaluatesamodel’sabilitytocapture
14hf.co/google/mt5-large the most salient information in a concise manner
15hf.co/facebook/mbart-large-cc25 andchecksthemodel’screativityandunderstand-
16hf.co/dbmdz/bert-base-turkish-cased ingofkeyphrasesinthenewsdomain. Weusedthe
17github.com/huggingface/transformers
same two summarization datasets: TRNews and
18hf.co/docs/transformers/main_classes/
optimizer_schedules#transformers.Adafactor MLSUM.
19hf.co/docs/transformers/main_classes/
trainer#trainer 20tatoeba.orgNamed Entity Recognition. Named entity 5.2 EvaluationMetrics
recognition (NER) aims to locate named entities,
We evaluated the generation tasks with
andsubsequentlyclassifiestheseentitiesintopre-
ROUGE (Lin, 2004), BLEU (Papineni et al.,
defined categories, typically “person”, “location”
2002) and METEOR (Banerjee and Lavie, 2005)
and “organization”. We employed two datasets
metrics. Fortheunderstandingtasks,weadopted
forthistask: WikiANN(Rahimietal.,2019)and
standard classification metrics such as accuracy,
MilliyetNER(Türetal.,2003).
precision,recall,andF1. Theonlyexceptionwas
semantic textual similarity, a regression task, for
Part-of-speechTagging. Part-of-speech(POS)
whichweusedthePearsoncorrelationcoefficient
tagginginvolvescategorizingeachwordinasen-
for evaluation. For NLI and classification tasks,
tenceaccordingtoitsgrammaticalfunction. This
weighted precision, recall and F1 were reported,
task assigns a specific part of speech, such as
leavingoutaccuracyduetoitsequalitytoweighted
noun, pronoun, or verb, to every word, clarify-
recall.
ing its role within the sentence’s structure. We
usedtwoTurkishUniversalDependencies(Nivre 5.3 Results
et al., 2020) treebanks, IMST (Türk et al., 2023)
5.3.1 GenerationTasks
andBOUN(Mars¸anetal.,2023),tofine-tuneand
evaluateourmodel.
WeevaluatedTURNA’sgenerativecapabilitieson
three tasks and compared the results to mT5 and
mBART.Theresults,aspresentedinTable2,show
SemanticTextualSimilarity. Semantictextual
thatTURNAoutperformedthebaselinemodelsin
similarity(STS)teststhemodel’sabilitytocontex-
both paraphrasing and summarization, with mT5
tuallycomparetwosentencesbyproducingasim-
ranking second and mBART last. In title genera-
ilarityscore. WeusedtheSTSb-TR(BekenFikri
tion, TURNA performed the best on the TRNews
et al., 2021) dataset to fine-tune and evaluate our
dataset,followedbymBART.However,fortheML-
model.
SUMdataset,mBARToutperformedboth TURNA
andmT5.
NaturalLanguageInference. Naturallanguage
inference(NLI),alsoknownastextualentailment,
Table2: Downstreamperformanceofmodelsongener-
involvesexaminingapairofsentences,thepremise ationtasks.
andthehypothesis,todeterminetheirrelationship
as“entailment”,“contradiction”,or“neutral”. This Task Dataset Model Rouge1 Rouge2 RougeL BLEU METEOR
task tests a model’s understanding of context by mBART 76.86 61.34 75.18 48.85 72.61 OST mT5 77.49 62.15 75.87 49.66 73.61
comprehending the premise and assess if the hy- TURNA 78.43 63.58 76.81 51.47 74.79
mBART 82.77 68.68 81.31 55.57 77.34
pothesislogicallyfollowsit. Therefore,NLIalso
TAT mT5 88.76 77.75 87.51 67.80 85.58
measuresamodel’sreasoningskills. Forthistask, TURNA 90.22 80.23 88.95 71.14 87.56
mBART 41.39 27.63 35.61 19.66 32.30 weusedtheNaturalLanguageInferenceinTurkish MLSUM mT5 43.43 29.95 37.71 21.58 34.20
(NLI-TR) dataset (Budur et al., 2020) for evalua- TURNA 44.33 30.99 38.62 24.25 36.47
mBART 39.96 25.53 34.90 16.69 32.23
tion. TRNews mT5 41.46 27.47 36.60 18.31 34.48
TURNA 41.77 27.81 36.99 19.05 34.61
mBART 32.97 19.71 31.32 7.41 18.29
TextClassification. Textclassificationinvolves MLSUM mT5 32.60 19.65 30.93 7.15 17.75
TURNA 32.67 19.60 31.12 7.08 17.90
categorizingtextsintopredefinedgroupsbasedon
mBART 35.40 21.92 34.32 11.95 23.26
theircontents. Thistaskassessesthemodel’scon- TRNews mT5 34.84 21.62 33.85 11.96 22.40
TURNA 36.47 22.88 35.47 12.64 23.62
textualawarenessandrobustnessinextractingrel-
evant features from the input text, allowing it to
5.3.2 UnderstandingTasks
discernimportantpatternsandinformationcrucial
In assessing understanding tasks, we compared
for accurate classification. We used three differ-
bothencoder-decodermodelsfine-tunedwiththe
ent datasets for evaluating this task: Product Re-
views21,TTC490022 (YıldırımandYıldız,2018), standardtext-to-textformulationandencoder-only
andTweetSentiments(Amasyalietal.,2018).
models, such as TURNA-Encoder and BERTurk.
TURNAachievedresultsthatsurpassbothmT5and
21hf.co/datasets/turkish_product_reviews mBART across various tasks and datasets, as de-
22kaggle.com/savasy/ttc4900 tailedinTables3,4,and5,reportingPOStagging
gnisarhparaP
noitazirammuS
noitareneGeltiT& NER, NLI, and classification results, respec- Table 5: Downstream performance of models on text
classification.
tively. TURNA outperformed mBART and mT5
in all classification, NLI, STS, POS tagging and
Dataset Model Precision Recall F1
NERtasks,exceptfortheMilliyet(NER)dataset.
While TURNA slightly lagged behind BERTurk mBART 87.67 93.63 90.55
mT5 93.01 94.17 93.27
onsometasks,thiswasnotsurprisingasencoder-
TURNA 94.67 95.24 94.81
decodermodelsoftenstrugglewithunderstanding
BERTurk 94.90 95.44 94.70
tasks (Lewis et al., 2020; Kementchedjhieva and
TURNA-Encoder 95.57 95.92 95.67
Chalkidis,2023). However,TURNA-Encodersur-
mBART 78.23 71.81 73.08
passed BERTurkinNER,NLIandsomeclassifica-
mT5 67.52 66.74 66.80
tiontasks,andwascompetitiveinothers. Theno-
TURNA 89.15 88.11 88.16
tableexceptionwasthesemantictextualsimilarity
BERTurk 91.97 91.85 91.88
task(Table6),whereTURNA-Encodersignificantly
TURNA-Encoder 91.05 90.53 90.52
laggedbehindBERTurk. Thissuggeststhatfurther
mBART 74.07 71.85 72.25
hyperparametertuningcouldimproveperformance,
mT5 68.20 67.45 66.71
asevidencedbyanadditionalexperimentwheread- TURNA 74.58 73.78 73.94
justingthelearningrateenabledTURNA-Encoder
BERTurk 75.91 75.20 74.79
toachieveaasignificantlyhigherPearsoncorrela- TURNA-Encoder 77.08 76.82 76.76
tionscoreintheSTStask(refertoTable11inthe
Appendix).
Table6: Downstreamperformanceofmodelsonseman-
tictextualsimilarity(STS).
Table3: DownstreamperformanceofmodelsonPOS
Model Pearson
taggingandNER.
mBART 66.95
mT5 59.40
Task Dataset Model Precision Recall F1 Accuracy TURNA 78.74
mBART 88.15 87.75 87.95 87.75
BERTurk 82.60
mT5 90.90 90.74 90.82 90.74
BOUN TURNA 92.39 92.35 92.37 92.35 TURNA-Encoder 73.63
BERTurk 90.60 90.41 90.50 93.22
TURNA-Encoder 90.30 90.31 90.31 93.05
6 Conclusion
mBART 77.68 77.40 77.54 77.39
mT5 93.17 93.05 93.11 93.04
IMST TURNA 94.66 94.48 94.57 94.48 Inthisstudy,weintroduced TURNA,anewTurk-
BERTurk 94.28 94.14 94.21 95.62
ishlanguagemodelthatadoptsanencoder-decoder
TURNA-Encoder 93.34 93.27 93.31 94.91
mBART 87.62 70.67 78.23 98.11 architecture following the UL2 framework. This
mT5 84.73 71.98 77.83 98.20
Milliyet TURNA 91.36 83.28 87.13 97.91 modelwaspretrainedonabroadcorpuscovering
BERTurk 93.51 94.84 94.17 99.24 web data, scientific articles, theses, books, cre-
TURNA-Encoder 95.16 96.03 95.59 99.46
ative writing, and parliament corpora. Our com-
mBART 90.76 89.12 89.93 95.84
mT5 90.50 89.90 90.20 95.93 prehensiveevaluationsacrossthreegenerationand
WikiANN TURNA 90.48 90.20 90.34 96.18
five understanding tasks on 13 different datasets
BERTurk 89.83 90.41 90.12 96.53
TURNA-Encoder 91.08 92.01 91.54 97.08 showed that TURNA outperforms existing multi-
lingual models, mT5 and mBART, and performs
betterthanoronparwiththeTurkishencoder-only
model, BERTurk. To encourage further research
and facilitate benchmarking in Turkish NLP, we
Table4: Downstreamperformanceofmodelsonnatural
languageinference(NLI). havemadeourmodelsandtheentiresourcecode
for data collection, filtering, model training, and
Model Precision Recall F1 fine-tuningpubliclyaccessible.
mBART 86.14 86.06 86.08
mT5 83.67 83.66 83.66 Limitations
TURNA 86.20 86.19 86.19
BERTurk 86.94 86.88 86.90
TURNA,withits1.1Bparameters,excelsinavari-
TURNA-Encoder 88.28 88.30 88.28
etyofNLPtasks,surpassingsimilar-scalemultilin-
gualmodelslikemT5(1.2B)andmBART(610M)
SOP
REN
sweiveRtcudorP
0094CTT
tnemitneSteewTin both generation and understanding. However, ilaritybasedfilteringforTurkishparaphrasedataset
its efficiency, especially in understanding tasks, creation. In Proceedings of the 5th International
Conference on Natural Language and Speech
is closely matched by the smaller, encoder-only
Processing(ICNLSP2022),pages119–127,Trento,
modelBERTurk,whichhasonly110Mparameters.
Italy.AssociationforComputationalLinguistics.
This suggests that the scale-to-performance ratio
of TURNAmaynotbeasefficientasexpected. MehmetFatihAmasyali,HakanTasköprü,andKübra
Çaliskan.2018. Words,meanings,charactersinsen-
Addressing this, we modified TURNA into
timentanalysis. In2018InnovationsinIntelligent
TURNA-Encoder by removing the decoder and SystemsandApplicationsConference(ASYU),pages
addingtask-specificheads,whichimproveditsef- 1–6.IEEE.
ficiency. TURNA-Encoder, having half the pa-
SatanjeevBanerjeeandAlonLavie.2005. METEOR:
rameters of TURNA, surpassed BERTurk in most
An automatic metric for MT evaluation with im-
tasks, thereby improving its efficiency. However, provedcorrelationwithhumanjudgments. InPro-
thecomparisonwithBERTurkindicatesaneedfor ceedingsoftheACLWorkshoponIntrinsicandEx-
trinsic Evaluation Measures for Machine Transla-
additional pretraining to fully leverage TURNA’s
tionand/orSummarization,pages65–72,AnnArbor,
largerparametercount.
Michigan. Association for Computational Linguis-
Currentresearchonscalinglawsindicatesthat tics.
trainingmodelsforuptofourepochscanbebenefi-
AntoniaBaumann.2019. Multilinguallanguagemodels
cial(Tayloretal.,2022;Muennighoffetal.,2023).
fornamedentityrecognitioninGermanandEnglish.
Despitehaving1.1Bparameters, TURNA hasbeen
In Proceedings of the Student Research Workshop
trained with approximately 43B tokens, which AssociatedwithRANLP2019,pages21–27,Varna,
is roughly equivalent to one epoch. This under- Bulgaria.INCOMALtd.
trainingmightbelimitingitspotential. Therefore,
BatuhanBaykaraandTungaGüngör.2022. Abstractive
we suggest further pretraining of TURNA to en-
textsummarizationandnewlarge-scaledatasetsfor
hanceitsperformance. agglutinativelanguagesturkishandhungarian. Lan-
In our downstream evaluations, we used the guageResourcesandEvaluation,56(3):973–1007.
sameoptimizationhyperparametersacrossalltasks
Figen Beken Fikri, Kemal Oflazer, and Berrin
anddatasetsduetolimitedcomputationalresources. Yanikoglu. 2021. Semantic similarity based eval-
This approach may have influenced performance uationforabstractivenewssummarization. InPro-
as datasets carry differing sizes and tasks exhibit ceedingsofthe1stWorkshoponNaturalLanguage
Generation, Evaluation, andMetrics(GEM2021),
different difficulties. Hence, we suggest dataset
pages24–33,Online.AssociationforComputational
and task-specific hyperparameter tuning to thor-
Linguistics.
oughlydemonstratethecapabilitiesofourmodel
SamuelR.Bowman,GaborAngeli,ChristopherPotts,
indownstreamtasks.
and Christopher D. Manning. 2015. A large anno-
tatedcorpusforlearningnaturallanguageinference.
Acknowledgments
InProceedingsofthe2015ConferenceonEmpirical
MethodsinNaturalLanguageProcessing(EMNLP).
We thank the Google TPU Research Cloud pro-
AssociationforComputationalLinguistics.
gramforprovidinguswithcreditstopretrainour
modelonTPUv3-8machines. Wearegratefulto Tom Brown, Benjamin Mann, Nick Ryder, Melanie
TETAM and BOUN CMPE for providing access Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind
Neelakantan,PranavShyam,GirishSastry,Amanda
totheGPUclusterusedinfine-tuningandevalua-
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
tionexperiments. WealsothankVNGRS-AIteam
Gretchen Krueger, Tom Henighan, Rewon Child,
for providing the tokenizer and the cleaned web AdityaRamesh,DanielZiegler,JeffreyWu,Clemens
corpus. Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, ChristopherBerner, SamMcCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
References
Language models are few-shot learners. In Ad-
JulienAbadji,PedroOrtizSuarez,LaurentRomary,and vances in Neural Information Processing Systems,
BenoîtSagot.2022. TowardsaCleanerDocument- volume 33, pages 1877–1901. Curran Associates,
Oriented Multilingual Crawled Corpus. arXiv e- Inc.
prints,pagearXiv:2201.06642.
EmrahBudur,RızaÖzçelik,TungaGungor,andChristo-
Besher Alkurdi, Hasan Yunus Sarioglu, and pherPotts.2020. DataandRepresentationforTurk-
Mehmet Fatih Amasyali. 2022. Semantic sim- ishNaturalLanguageInference. InProceedingsofthe2020ConferenceonEmpiricalMethodsinNat- Zi-Hang Jiang, Weihao Yu, Daquan Zhou, Yunpeng
ural Language Processing (EMNLP), pages 8253– Chen,JiashiFeng,andShuichengYan.2020. Con-
8267, Online. Association for Computational Lin- vbert: Improvingbertwithspan-baseddynamiccon-
guistics. volution. AdvancesinNeuralInformationProcess-
ingSystems,33:12837–12848.
Daniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-
Gazpio, and Lucia Specia. 2017. SemEval-2017 YovaKementchedjhievaandIliasChalkidis.2023. An
task1: Semantictextualsimilaritymultilingualand explorationofencoder-decoderapproachestomulti-
crosslingual focused evaluation. In Proceedings label classification for legal and biomedical text.
of the 11th International Workshop on Semantic In Findings of the Association for Computational
Evaluation(SemEval-2017),pages1–14,Vancouver, Linguistics: ACL2023,pages5828–5843,Toronto,
Canada.AssociationforComputationalLinguistics. Canada.AssociationforComputationalLinguistics.
Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Taku Kudo. 2018. Subword regularization: Improv-
Christopher D. Manning. 2020. ELECTRA: pre- ingneuralnetworktranslationmodelswithmultiple
trainingtextencodersasdiscriminatorsratherthan subwordcandidates. InProceedingsofthe56thAn-
generators. In 8th International Conference on nualMeetingoftheAssociationforComputational
LearningRepresentations,ICLR2020,AddisAbaba, Linguistics(Volume1: LongPapers),pages66–75,
Ethiopia,April26-30,2020.OpenReview.net. Melbourne,Australia.AssociationforComputational
Linguistics.
AlexisConneau,KartikayKhandelwal,NamanGoyal,
TakuKudoandJohnRichardson.2018. SentencePiece:
Vishrav Chaudhary, Guillaume Wenzek, Francisco
A simple and language independent subword tok-
Guzmán, Edouard Grave, Myle Ott, Luke Zettle-
enizeranddetokenizerforneuraltextprocessing. In
moyer,andVeselinStoyanov.2020. Unsupervised
Proceedings of the 2018 Conference on Empirical
cross-lingualrepresentationlearningatscale. InPro-
Methods in Natural Language Processing: System
ceedings of the 58th Annual Meeting of the Asso-
Demonstrations, pages 66–71, Brussels, Belgium.
ciationforComputationalLinguistics,pages8440–
AssociationforComputationalLinguistics.
8451, Online. Association for Computational Lin-
guistics.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad,AbdelrahmanMohamed,OmerLevy,
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Veselin Stoyanov, and Luke Zettlemoyer. 2020.
Kristina Toutanova. 2019. BERT: Pre-training of
BART:Denoisingsequence-to-sequencepre-training
deepbidirectionaltransformersforlanguageunder-
fornaturallanguagegeneration,translation,andcom-
standing. InProceedingsofthe2019Conferenceof
prehension. InProceedingsofthe58thAnnualMeet-
theNorthAmericanChapteroftheAssociationfor
ingoftheAssociationforComputationalLinguistics,
ComputationalLinguistics: HumanLanguageTech-
pages7871–7880,Online.AssociationforComputa-
nologies,Volume1(LongandShortPapers),pages
tionalLinguistics.
4171–4186,Minneapolis,Minnesota.Associationfor
ComputationalLinguistics.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
maticevaluationofsummaries. InTextSummariza-
Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-
tionBranchesOut,pages74–81,Barcelona,Spain.
aodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,
AssociationforComputationalLinguistics.
andHsiao-WuenHon.2019. Unifiedlanguagemodel
pre-trainingfornaturallanguageunderstandingand
XiVictoriaLin,TodorMihaylov,MikelArtetxe,Tianlu
generation. Advancesinneuralinformationprocess-
Wang,ShuohuiChen,DanielSimig,MyleOtt,Na-
ingsystems,32.
manGoyal,ShrutiBhosale,JingfeiDu,Ramakanth
Pasunuru,SamShleifer,PunitSinghKoura,Vishrav
PengchengHe,JianfengGao,andWeizhuChen.2023. Chaudhary,BrianO’Horo,JeffWang,LukeZettle-
Debertav3:Improvingdebertausingelectra-stylepre- moyer,ZornitsaKozareva,MonaDiab,VeselinStoy-
trainingwithgradient-disentangledembeddingshar- anov, and Xian Li. 2022. Few-shot learning with
ing. arXivpreprintarXiv:2111.09543. multilingualgenerativelanguagemodels. InProceed-
ingsofthe2022ConferenceonEmpiricalMethods
KennethHeafield.2011. KenLM:Fasterandsmaller inNaturalLanguageProcessing,pages9019–9052,
languagemodelqueries. InProceedingsoftheSixth AbuDhabi,UnitedArabEmirates.Associationfor
WorkshoponStatisticalMachineTranslation,pages ComputationalLinguistics.
187–197,Edinburgh,Scotland.AssociationforCom-
putationalLinguistics. Pierre Lison, Jörg Tiedemann, and Milen Kouylekov.
2018. OpenSubtitles2018: Statistical rescoring of
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men- sentencealignmentsinlarge,noisyparallelcorpora.
sch,ChrisBamford,DevendraSinghChaplot,Diego InProceedingsoftheEleventhInternationalConfer-
delasCasas,FlorianBressand,GiannaLengyel,Guil- enceonLanguageResourcesandEvaluation(LREC
laumeLample,LucileSaulnier,etal.2023. Mistral 2018), Miyazaki, Japan. European Language Re-
7b. arXivpreprintarXiv:2310.06825. sourcesAssociation(ELRA).YinhanLiu,JiataoGu,NamanGoyal,XianLi,Sergey LongPapers),pages3118–3135,Online.Association
Edunov, Marjan Ghazvininejad, Mike Lewis, and forComputationalLinguistics.
LukeZettlemoyer.2020. Multilingualdenoisingpre-
training for neural machine translation. Transac- Victor Sanh, Lysandre Debut, Julien Chaumond, and
tionsoftheAssociationforComputationalLinguis- Thomas Wolf. 2019. Distilbert, a distilled version
tics,8:726–742. of bert: smaller, faster, cheaper and lighter. arXiv
preprintarXiv:1910.01108.
Ilya Loshchilov and Frank Hutter. 2017. Decou-
pled weight decay regularization. arXiv preprint
StefanSchweter.2020. Berturk-bertmodelsforturk-
arXiv:1711.05101.
ish.
Büs¸ra Mars¸an, Salih Furkan Akkurt, Utku Türk,
ThomasScialom,Paul-AlexisDray,SylvainLamprier,
FurkanAtmaca,S¸aziyeBetülÖzates¸,GözdeBerk,
Benjamin Piwowarski, and Jacopo Staiano. 2020.
SeyyitTalhaBedir,AbdullatifKöksal,BalkızÖztürk
MLSUM: The multilingual summarization corpus.
Bas¸aran,TungaGüngör,andArzucanÖzgür.2023.
InProceedingsofthe2020ConferenceonEmpirical
Udturkishboun. UniversalDependencies. Accessed:
MethodsinNaturalLanguageProcessing(EMNLP),
2023-11-14.
pages8051–8067,Online.AssociationforComputa-
NiklasMuennighoff,AlexanderMRush,BoazBarak, tionalLinguistics.
TevenLeScao,AleksandraPiktus,NouamaneTazi,
Sampo Pyysalo, Thomas Wolf, and Colin Raffel. NoamShazeer.2020. Gluvariantsimprovetransformer.
2023. Scaling data-constrained language models.
arXivpreprintarXiv:2305.16264. Noam Shazeer and Mitchell Stern. 2018. Adafactor:
Adaptivelearningrateswithsublinearmemorycost.
JoakimNivre,Marie-CatherinedeMarneffe,FilipGin- InInternationalConferenceonMachineLearning,
ter, Jan Hajicˇ, Christopher D. Manning, Sampo pages4596–4604.PMLR.
Pyysalo, Sebastian Schuster, Francis Tyers, and
Daniel Zeman. 2020. Universal Dependencies v2:
OlehShliazhko,AlenaFenogenova,MariaTikhonova,
Anevergrowingmultilingualtreebankcollection. In
VladislavMikhailov,AnastasiaKozlova,andTatiana
ProceedingsoftheTwelfthLanguageResourcesand
Shavrina.2022. mgpt: Few-shotlearnersgomultilin-
EvaluationConference,pages4034–4043,Marseille,
gual. arXivpreprintarXiv:2204.07580.
France.EuropeanLanguageResourcesAssociation.
UmutSulubacakandGüls¸enEryig˘it.2018. Implement-
DeboraNozza,FedericoBianchi,andDirkHovy.2020.
ing universal dependency, morphology, and multi-
Whatthe[mask]? makingsenseoflanguage-specific
wordexpressionannotationstandardsforturkishlan-
bertmodels. arXivpreprintarXiv:2003.02912.
guageprocessing. TurkishJournalofElectricalEn-
KishorePapineni,SalimRoukos,ToddWard,andWei gineeringandComputerSciences,26(3):1662–1672.
jingZhu.2002. Bleu: amethodforautomaticevalu-
ationofmachinetranslation. pages311–318. Hasan Tanvir, Claudia Kittask, Sandra Eiche, and
KairitSirts.2021. EstBERT:Apretrainedlanguage-
AlecRadford,JeffreyWu,RewonChild,DavidLuan, specific BERT for Estonian. In Proceedings of
DarioAmodei,IlyaSutskever,etal.2019. Language the23rdNordicConferenceonComputationalLin-
modelsareunsupervisedmultitasklearners. OpenAI guistics(NoDaLiDa),pages11–19,Reykjavik,Ice-
blog,1(8):9. land(Online).LinköpingUniversityElectronicPress,
Sweden.
ColinRaffel,NoamShazeer,AdamRoberts,Katherine
Lee,SharanNarang,MichaelMatena,YanqiZhou,
YiTay,MostafaDehghani,JinfengRao,WilliamFedus,
WeiLi,andPeterJLiu.2020. Exploringthelimits
SamiraAbnar,HyungWonChung,SharanNarang,
oftransferlearningwithaunifiedtext-to-texttrans-
DaniYogatama,AshishVaswani,andDonaldMet-
former. TheJournalofMachineLearningResearch,
zler. 2021. Scale efficiently: Insights from pre-
21(1):5485–5551.
trainingandfine-tuningtransformers. arXivpreprint
arXiv:2109.10686.
AfshinRahimi,YuanLi,andTrevorCohn.2019. Mas-
sivelymultilingualtransferforNER. InProceedings
of the 57th Annual Meeting of the Association for YiTay, MostafaDehghani, VinhQTran, XavierGar-
ComputationalLinguistics,pages151–164,Florence, cia,DaraBahri,TalSchuster,HuaixiuStevenZheng,
Neil Houlsby, and Donald Metzler. 2023. Unify-
Italy.AssociationforComputationalLinguistics.
ing language learning paradigms. arXiv preprint
PhillipRust,JonasPfeiffer,IvanVulic´,SebastianRuder, arXiv:2205.05131.
andIrynaGurevych.2021. Howgoodisyourtok-
enizer? onthemonolingualperformanceofmultilin- RossTaylor,MarcinKardas,GuillemCucurull,Thomas
gual language models. In Proceedings of the 59th Scialom,AnthonyHartshorn,ElvisSaravia,Andrew
AnnualMeetingoftheAssociationforComputational Poulton, Viktor Kerkez, and Robert Stojnic. 2022.
Linguisticsandthe11thInternationalJointConfer- Galactica:Alargelanguagemodelforscience. arXiv
ence on Natural Language Processing (Volume 1: preprintarXiv:2211.09085.Jörg Tiedemann. 2012. Parallel data, tools and inter- Savas¸YıldırımandTug˘baYıldız.2018. Acomparative
faces in OPUS. In Proceedings of the Eight Inter- analysis of text classification for turkish language.
national Conference on Language Resources and Pamukkale Univ Muh Bilim Derg, 24(5):879–886.
Evaluation(LREC’12),Istanbul,Turkey.European Doi: 10.5505/pajes.2018.15931.
LanguageResourcesAssociation(ELRA).
BiaoZhang,PhilipWilliams,IvanTitov,andRicoSen-
HugoTouvron,ThibautLavril,GautierIzacard,Xavier
nrich.2020. Improvingmassivelymultilingualneu-
Martinet,Marie-AnneLachaux,TimothéeLacroix,
ralmachinetranslationandzero-shottranslation.
Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. Llama: Open and effi-
A Appendix
cient foundation language models. arXiv preprint
arXiv:2302.13971.
A.1 CleaningProcedureforScientificCorpus
Gökhan Tür, Dilek Hakkani-Tür, and Kemal Oflazer.
Initially,wereplacedinvalidormisinterpretedchar-
2003. A statistical information extraction sys-
tem for turkish. Natural Language Engineering, acters resulting from Optical Character Recogni-
9(2):181–210. tion(OCR)errors,employingapredefineddictio-
nary. Subsequently, we omitted preliminary text
Utku Türk, S¸aziye Betül Özates¸, Büs¸ra Mars¸an,
Salih Furkan Akkurt, Çag˘rı Çöltekin, Güls¸en Ce- appearingbeforetheabstract,whichtypicallycon-
birog˘luEryig˘it,MemduhGökırmak,HünerKas¸ıkara, tainsnon-essentialinformationsuchasaffiliations
UmutSulubacak,andFrancisTyers.2023. Udturk-
andarticlemetadata. Thiswasachievedusingreg-
ishimst. UniversalDependencies. Accessed: 2023-
ular expressions tailored for this purpose. While
11-14.
thisapproachwassufficientforscientificarticles,
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
the theses posed additional challenges, including
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
sectionslikelistsoffigures,tables,andcustomary
Kaiser,andIlliaPolosukhin.2017. Attentionisall
youneed. Advancesinneuralinformationprocessing declarations. To handle these sections, we relied
systems,30. on regular expressions designed to identify and
subsequently discard specific titles and their ac-
AnttiVirtanen,JennaKanerva,RamiIlo,JouniLuoma,
JuhaniLuotolahti,TapioSalakoski,FilipGinter,and companyingcontent.
SampoPyysalo.2019. Multilingualisnotenough: In our effort to maintain the quality of the ex-
Bertforfinnish. arXivpreprintarXiv:1912.07076. tractedtextfromthePDFarticles,wealsoimple-
AdinaWilliams,NikitaNangia,andSamuelBowman. mented a line-wise filtering procedure involving
2018. A broad-coverage challenge corpus for sen- thestepsbelow:
tenceunderstandingthroughinference. InProceed-
ingsofthe2018ConferenceoftheNorthAmerican
• Text Statistics: Each line from the arti-
Chapter of the Association for Computational Lin-
cleswasanalyzedbasedonvariousstatistics.
guistics: HumanLanguageTechnologies,Volume1
(Long Papers), pages 1112–1122. Association for Theseincludedcharactercount,tokencount,
ComputationalLinguistics. numeric content, average token length, and
metricsreflectingtheprevalenceofnumbers,
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond,ClementDelangue,AnthonyMoi,Pier- specificallytheproportionofnumerictokens
ricCistac,TimRault,RemiLouf,MorganFuntow- tototaltokensandfrequencyofdigitappear-
icz,JoeDavison,SamShleifer,PatrickvonPlaten,
ances. Thisstageensuredtheremovalofnon-
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
contentelements,suchasheaders,pagenum-
Teven Le Scao, Sylvain Gugger, Mariama Drame,
QuentinLhoest,andAlexanderRush.2020. Trans- bers,andtableitems.
formers:State-of-the-artnaturallanguageprocessing.
InProceedingsofthe2020ConferenceonEmpirical • Language Identification and Correction:
Methods in Natural Language Processing: System
Giventhepotentialpresenceofnon-Turkish
Demonstrations,pages38–45,Online.Association
lineswithinthearticles,eachlinewaschecked
forComputationalLinguistics.
for its Turkish content using the langid li-
LintingXue,NoahConstant,AdamRoberts,MihirKale, brary23. In cases of potential anomalies or
RamiAl-Rfou,AdityaSiddhant,AdityaBarua,and
false detections, the surrounding lines were
ColinRaffel.2021. mT5: Amassivelymultilingual
pre-trainedtext-to-texttransformer. InProceedings examinedtocorrectsuchanomalies,ensuring
ofthe2021ConferenceoftheNorthAmericanChap- thatthemajorityofourextractedcontentisin
teroftheAssociationforComputationalLinguistics: Turkish.
HumanLanguageTechnologies,pages483–498,On-
line.AssociationforComputationalLinguistics. 23github.com/saffsd/langid.py• Content Identification: Although article pairswerethenfilteredbasedonsemanticsimilar-
metadata typically appears at the beginning ity,resultinginafilteredversionofthedatasetwith
ofthedocuments,theymayalsoappearelse- 706,488pairs.
where. To identify such elements as dates,
TAT(Alkurdietal.,2022) TATisanotherpara-
email addresses, and names, each line was
phrasingdatasetcreatedusingthesamemethodol-
checked using specific regular expressions.
ogyasOST.Theinitialparallelcorpusoriginates
Additionally,captions,identifiedbytheirdis-
from Tatoeba26. The unfiltered and filtered ver-
tinctpatterns,weredetectedandsubsequently
sions of the dataset include 265,203 and 50,423
removed.
pairs,respectively.
• IdentificationandFilteringofSpecialSec-
TR-News (Baykara and Güngör, 2022) TR-
tions: In scientific texts, certain lines—like
Newsisacollectionofnewsarticlesalongwithcor-
those in bibliographies and footnotes—may
respondingsummariesandtitlescoveringawide
notcontributeessentialcontent,ortheymay
range of topics. It is compiled from three Turk-
evendisrupttheprimarynarrative. Toaddress
ishnationalnewsoutlets: Cumhuriyet,NTV,and
this,weimplementedstrategiestodetectand
HaberTürk. Thedatasetconsistsofapproximately
subsequently omit such lines. This step en-
307Karticles,splitinto277,573train,14,610vali-
suredtheretentionofthetext’scoherenceand
dation,and15,379testdocuments.
continuity.
MLSUM (Scialom et al., 2020) MLSUM is a
• Citation Filtering: Citations, while crucial
large-scale, multilingual summarization dataset
to academic papers, can interrupt text flow,
thatincludesTurkisharticles. TheTurkishsubset
especiallywhenpreparingdataforlanguage
contains273,617articlesfromInternetHaber,fur-
modeltraining. Wethususedpatternstoiden-
therdividedinto259,277train,11,565validation,
tifyandremoveinlinecitations,guaranteeing
and12,755testdocuments.
asmoothtextualflow.
WikiANN(Rahimietal.,2019) WikiANNisa
Aftertheline-wisefilteringprocedurewascom-
multilingualnamedentityrecognitiondatasetcon-
plete,weapplieddocument-basedfilteringwiththe
taininginstancesfromWikipediaarticlesannotated
helpofaStatisticalLanguageModel(LM)trained
with tags of “location”, “person”, and “organiza-
onacompilationofMay2023TurkishWikipedia
tion”. The Turkish subset of the dataset includes
articles24. AKenLM5-gramlanguagemodelwas
40,000rows,splitinto20,000fortraining,10,000
trained (Heafield, 2011) on 6.8M sentences tok-
forvalidation,and10,000fortesting.
enized with a Turkish SentencePiece tokenizer25.
TheKenLMmodelwasthenusedtodiscarddocu- MilliyetNER(Türetal.,2003) MilliyetNERis
mentsdefinedbyseparatethresholdsfortheDergi- anamedentityrecognitiondatasetthatincludesin-
parkarticles(lessthan5%LMscore)andtheYök- stancesfromTurkishnewsarticlesannotatedwith
Teztheses(lessthan2%LMscore). Thethresholds tags of “location”, “person”, and “organization”.
havebeenselectedbynativeTurkishspeakersby Thedatasetcomprises515,123words,dividedinto
analyzingthedistributionofdocumentsandtheir atrainingsetof419,996,avalidationsetof45,532
qualities based on document-based average LM andatestsetof49,595words.
score.
UD Turkish IMST (Türk et al., 2023) The
A.2 Fine-tuningDatasets IMST-UDTreebankisaTurkishdependencytree-
bankintheformatoftheUniversalDependencies
OST(Alkurdietal.,2022) OSTisaparaphras-
(UD) framework (Sulubacak and Eryig˘it, 2018).
ingdataset,constructedbytranslatingEnglishsub-
Thetreebankwasannotatedmanuallyinaformat
titlesfromOpenSubtitles2018(Lisonetal.,2018)
other than UD, and then automatically converted
intoTurkish. Theoriginalsubtitlesandtheirtrans-
fortheUDversionv1.3tobethefirstTurkishUD
lations were preprocessed to create an unfiltered
treebank. It has since then received various up-
versionofthedatasetwith1,944,955pairs. These
dates and corrections. The latest version, v2.13,
24hf.co/datasets/musabg/wikipedia-tr has56,422tokensintotal,with36,415tokensfor
25github.com/vngrs-ai/vnlp/tree/main/
vnlp/turkish_word_embeddings 26tatoeba.orgtraining,10,257forvalidation,and9,750fortest- world. The dataset is available on Kaggle29 and
ing. HuggingFace30. TheTTC4900datawasalsodedu-
plicated before fine-tuning, and split with an 80-
UDTurkishBOUN(Mars¸anetal.,2023) The
10-10ratio,leaving3,631samplesfortraining,and
BOUN treebank is another Turkish dependency
454sampleseachfortestandvalidation.
treebank that has been a part of the UD project
since v2.7. Since then, it has received a few up- TweetSentiments(Amasyalietal.,2018) Tweet
dates with corrections. The latest version, v2.13, Sentimentsisasentimentclassificationdatasetwith
has 121,835 tokens in total, with 97,797 tokens three categories: positive, negative and neutral.
fortraining,12,023forvalidation,and12,015for Thedatasetconsistsof17,289tweetsthatcontain
testing. commentsaboutaGSMoperator,splitinto13,832
training and 3,457 test samples. Due to lack of
STSb-TR(BekenFikrietal.,2021) STSb-TR
a validation set, the training set was split with a
isderivedfromtheEnglishSemanticTextualSimi-
90-10 train-validation ratio. After deduplication,
laritybenchmark(STSb)dataset(Ceretal.,2017)
the resulting fine-tuning dataset contains 12,421
by translating the English sentences into Turkish
training,1,381validationand3,456testsamples.
using Google Translate, with no manual correc-
tions. Eachdataelementhastwosentencesanda A.3 Fine-tuningdetails
corresponding similarity score. The dataset con-
Data splits. We used predefined splits for
tains 5,749 training, 1,500 validation and 1,379
datasets, including training, validation, and test
testsamples.
sets. Fordatasetslackingbothvalidationandtest
sets,wedividedthedataintotraining,validation,
NLI-TR(Buduretal.,2020) TheNaturalLan-
andtestsetswithan80-10-10ratio. Intheabsence
guageInferenceinTurkish(NLI-TR)datasetcon-
of the validation set only, we utilized 10% of the
sists of two large-scale datasets containing pairs
original training data to generate a validation set,
of sentences labeled as “entailment”, “contradic-
whiletheremaining90%wasusedfortraining. We
tion”, or “neutral”. These sentence pairs were
usedthesameapproachfordatasetsthatlackeda
obtained by translating the widely used NLI cor-
testset. FortheNLItask,wefine-tunedourmodel
pora, made up of SNLI (Bowman et al., 2015)
on the training set referred to as NLI-TR (Budur
and MultiNLI (Williams et al., 2018). The SNLI
etal.,2020),whichisthecombinationofthetrain-
datasetincludes570Ksamples,with550Kfortrain-
ing sets of SNLI-TR and MultiNLI-TR, and we
ing,10Kforvalidation,and10Kfortesting. The
usedthealreadyexistingtestandvalidationsetsof
MultiNLI dataset contains 413K samples, with
theSNLI-TRdataset.
393K for training and 20K for validation, evenly
dividedbetweenmatchedandmismatchedpairs.
Dataset-specific parameters. Considering the
varyinglengthsofdatasetsamples,weuseddataset-
ProductReviews TheTurkishProductReviews
specificparameters. Theseparameterssetthemax-
is a sentiment classification dataset that contains
imuminputandtargetlengths,andbatchsizetofit
productreviewsfromvariousonlinesources,and
intothelargestbatch. Inordertospeedupthefine-
isavailableonHuggingFace27. Atotalof235,165
tuningprocess,weemployedbf16mixedprecision
reviewsarecategorizedaspositiveornegative. We
in the summarization and title generation experi-
deduplicatedthedatasetbeforeusage,andsplitit
ments, allowing for a larger batch size. Table 7
with an 80-10-10 train-validation-test ratio. The
showsthehyperparametersusedforfine-tuning.
resultingdatasetcontains186,806training,23,351
validationand23,351testsamples.
A.4 Mode-Switching
TTC4900 (Yıldırım and Yıldız, 2018) The In the UL2 framework, specific sentinel tokens
dataset is made available by the Kemik NLP are dedicated to different pretraining objectives,
Group28,andcontains4,900newsarticlesandtexts enablingthemodeltoadjustitsmodeforoptimal
classifiedwithoneofsevencategories: economy, taskperformance. Thisapproachisalsoappliedto
culture-arts,health,politics,sports,technologyand fine-tuningandfew-shotlearningbyusingatoken
27hf.co/datasets/turkish_product_reviews 29kaggle.com/savasy/ttc4900
28kemik.yildiz.edu.tr 30hf.co/datasets/ttc4900Table7: Dataset-specifichyperparametersforfine-tuning
Task Dataset MaxInputLength MaxTargetLength BatchSize
TRNews 768 128 4
Summarization
MLSUM 768 128 4
TRNews 256 64 8
TitleGeneration
MLSUM 256 64 8
Tatoeba 20 20 128
Paraphrasing
OpenSubtitles 20 20 128
WikiANN 60 40 64
NER
MilliyetNER 380 60 8
BOUN 90 300 8
POS
IMST 60 210 16
NLI NLI-TR 128 8 32
ProductReviews 20 4 32
Classification TTC4900 1,450 8 2
TweetSentiment 160 4 32
STS STSb-TR 140 10 32
tailoredtotheneedsofthedownstreamtask,such Table9: Comparisonofmodeswitchingmodesonthe
as [S2S] for generation tasks. This is known as textclassificationtask.
modeswitching.
Dataset Mode Precision Recall F1
We tested mode switching by fine-tuning
- 94.67 95.24 94.81
TURNA on several tasks and datasets. The re-
[NLG] 94.30 95.03 94.39
sults,detailedinTables8,9,and10,showedthat ProductReviews
[NLU] 94.45 95.10 94.60
TURNA models fine-tuned without any sentinel [S2S] 94.34 95.04 94.47
tokenscoredhighestonparaphrasingevaluations.
- 89.15 88.11 88.16
However, a separate sentinel token achieved the
[NLG] 89.50 88.33 88.39
TTC4900
bestscoresondifferentclassificationdatasets,with [NLU] 86.18 84.14 84.31
thescoresbeingremarkablyclose. Inthesemantic [S2S] 90.83 90.31 90.24
textualsimilaritytask,themodeltrainedwiththe - 74.58 73.78 73.94
[NLG]tokenperformedthebest. [NLG] 76.01 75.84 75.56
TweetSentiment
[NLU] 75.45 75.46 75.45
We found no consistent pattern in the perfor-
[S2S] 75.55 74.91 74.86
manceofdifferenttokensacrossvarioustasksand
datasets. Thissuggeststhatmode-switchingmight
notalwaysenhanceperformance,andcouldpoten- Table 10: Comparison of mode switching modes on
tiallydegradeit. semantictextualsimilarity(STS).
Table8: Comparisonofmodeswitchingmodesonthe Mode Pearson
paraphrasingtask.
- 78.74
[NLG] 79.71
Dataset Mode Rouge1 Rouge2 RougeL BLEU METEOR
[NLU] 78.45
- 78.43 63.58 76.81 51.47 74.79
[S2S] 78.30
[NLG] 76.20 61.11 74.50 46.27 73.76
OST
[NLU] 77.18 61.97 75.33 48.39 74.02
[S2S] 77.20 61.98 75.44 48.53 74.05
- 90.22 80.23 88.95 71.14 87.56 A.5 HyperparameterTuning
[NLG] 89.66 79.28 88.41 69.54 87.18
TAT [NLU] 89.08 78.53 87.90 68.33 86.82 We conducted an additional experiment on the
[S2S] 89.71 79.37 88.45 69.61 87.26 Semantic Textual Similarity task due to the lowPearson correlation score obtained by TURNA-
EncoderwhencomparedtoTURNAandBERTurk.
We fine-tuned TURNA-Encoder with different
learningratesontheregressiontask. Theresultsare
reportedinTable11. ThedifferenceinPearsoncor-
relationscoressuggestthatelaboratehyperparam-
etertuningcansignificantlyalterthedownstream
performanceofourmodel.
Table 11: Comparison of TURNA-Encoder perfor-
mancewithdifferentlearningratesonsemantictextual
similarity(STS).
LearningRate Pearson
(Default)5×10−5 73.63
5×10−4 77.13
5×10−3 −3.56
5×10−2 17.92