Evaluating User Experience and Data Quality in a Gamified
Data Collection for Appearance-Based Gaze Estimation
MINGTAOYUE∗,TOMOMISAYUDA∗,MILESPENNINGTON,andYUSUKESUGANO,
In-
stituteofIndustrialScience,TheUniversityofTokyo,Japan
Fig.1. Thispaperproposesamixed-mediagamificationsystemforcollectingtrainingdataforgazeestimation.
Appearance-basedgazeestimation,whichusesonlyaregularcameratoestimatehumangaze,isimportant
invariousapplicationfields.Whilethetechniquefacesdatabiasissues,datacollectionprotocolisoften
demanding,andcollectingdatafromawiderangeofparticipantsisdifficult.Itisanimportantchallenge
todesignopportunitiesthatallowadiverserangeofpeopletoparticipatewhileensuringthequalityofthe
trainingdata.Totacklethischallenge,weintroduceanovelgamifiedapproachforcollectingtrainingdata.In
thisgame,twoplayerscommunicatewordsviaeyegazethroughatransparentletterboard.Imagescaptured
duringgameplayserveasvaluabletrainingdataforgazeestimationmodels.Thegameisdesignedasaphysical
installationthatinvolvescommunicationbetweenplayers,anditisexpectedtoattracttheinterestofdiverse
participants.Weassessthegame’ssignificanceondataqualityanduserexperiencethroughacomparative
userstudy.
CCSConcepts:•Human-centeredcomputing→Humancomputerinteraction(HCI).
AdditionalKeyWordsandPhrases:datasets,gamification,gazeestimation
1 INTRODUCTION
Theincreasingimportanceofartificialintelligence(AI)andmachinelearning(ML)technologies
highlightstheurgentdemandfortrainingdata.High-qualitydataisvitalforbuildingbetterML
models,butcollectingsufficienttrainingdatapresentssignificantchallenges.FormostMLtasks,
trainingdataneedsdiversitytoguaranteethatmodelsoperaterobustlyandwithoutprejudice.
Nonetheless,gatheringdatafromdiverseparticipantsbringsaboutseveralobstacles,particularly
inprivacy-sensitiveareasinvolvingfacialimagesorbiometricinformation.Thedatacollectionis
oftentime-consumingandburdensome,furtherdiscouragingparticipantrecruitment.
Appearance-basedgazeestimation[12,22],theMLtaskofestimatinggazedirectionfromface
images,isaprimeexampleofthesedatacollectionchallenges.Thehumangazeisconsideredavital
sensingmodalityinvariousapplicationfieldssuchasusabilitystudies,hands-freeinput,anduser
stateestimationforcontext-awarecomputing[39].Appearance-basedgazeestimationallowsfor
eyetrackingwithonlyanordinarycamera,andthisopensupthepotentialforvariousapplications,
includinggaze-basedinteraction[2,29]andattentionmonitoring[13,52,66],particularlyinmobile
andpervasivesettings.Trainingaccurateandrobustgazeestimationmodelsrequiresdiverseface
imagesassociatedwithgazedirectionlabels.Nevertheless,thestandarddatacollectionprotocol,
whichrequiresparticipantstolookatthegazetargetforanextendedperiodasperinstructions,is
∗Bothauthorscontributedequallytothisresearch.
Authors’address:MingtaoYue;TomomiSayuda;MilesPennington;YusukeSugano,sugano@iis.u-tokyo.ac.jp,Instituteof
IndustrialScience,TheUniversityofTokyo,4-6-1Komaba,Meguro-ku,Tokyo,Japan,153-8505.
4202
naJ
52
]CH.sc[
1v59041.1042:viXra2 MingtaoYue,TomomiSayuda,MilesPennington,andYusukeSugano
extremelytedious.Moreover,theinherentsensitivityoffacialdatacausesconcernsaboutprivacy
andethicalissues.Thismakeslarge-scalevoluntaryparticipationlessappealingtothegeneral
public. Addressing these obstacles to achieve accessible and ethical data collection is a crucial
researchtask.
Thisworkintroducesanovelgamifiedapproachtomotivatebroaderinvolvementinappearance-
basedgazeestimationdatacollection.Theeffectofgamificationinboostingpeople’sengagement
has already been pointed out [25], and some previous examples exist of adding a gamification
aspect to data collection processes [7, 47]. However, gamification has not yet been applied in
appearance-basedgazeestimation.Therearetwochallengeswhenapplyinggamificationtothe
gazeestimationtask.Thefirstisdesigningagamelogictoacquireaccurategazepositionlabelsfor
trainingdata,i.e.,toensureplayerslookatthedesignatedgazetargetwhentheirfaceimageis
captured.Thesecondiscreatingopportunitiesthroughthegamefordiversepeopletoparticipatein
datacollection.Toachievethisgoal,wehavedesignedatwo-playercooperativegamethatinvolves
aphysicalinstallation.
AsillustratedinFig.1,thegameinvolvestwoplayerssittingoppositeeachotherwithatrans-
parentletterboardbetweenthem.Inspiredbyaugmentativealternativecommunicationtechniques
forpeoplewithmotordisabilities[64],itisdesignedasacooperativegamewhereplayerscom-
municatewordsusingtheirgaze.Theanswererplayerisgivenonlysomelettersofthewordas
ahint,andthequestionerplayerconveysthehiddenlettersusingtheirgaze.Faceimagesofthe
questionerplayerarecapturedwhentheylookatthetargetcharacters,whichresultsintraining
dataforappearance-basedgazeestimation.Accuratelyidentifyingsomeoneelse’spointofgaze
isachallenge,evenforhumans.However,designingthegameasaquizwhereplayersusethe
wordcontexttomakepredictionsmakesthegameexperienceenjoyableundermoderatedifficulty.
Thegameprovidesanovelwayofmeetingtheneedfordatacollectioningazeestimationwhile
offeringparticipantsanengagingandinteractiveexperience.
Inthispaper,wereporttheresultsofuserexperimentsconductedinapubliccafeteriaona
universitycampus.Weobtainedquestionnaireresponsesfrom46outof47participants,allowing
ustoperformasubjectiveassessmentoftheirgamingexperience.Furthermore,asubsetof22
participantswereaskedtoplaythegamewhileequippedwithawearableeyetracker.Usingthis
data,wequantitativelyevaluatedgazebehaviorduringthegameexperience,i.e.,howaccurately
playerslookattargetletters.Wealsoanalyzedthecharacteristicsofthedataobtainedfromthe
gamebycomparingourresultstooutputsfrompre-trainedgazeestimationmodels.Inthisway,we
validatethatourgamecanacquirehigh-qualitytrainingdatafromadiverserangeofparticipants.
Thisworkisthefirsttoproposeanddemonstratethegamificationapproachforcollectingdatain
appearance-basedgazeestimation.
2 RELATEDWORK
2.1 Appearance-basedGazeEstimationandDatasets
Conventional remote eye tracking devices, utilizing specialized hardware, have substantial re-
strictionsontherangeofheadmovementofthetargetperson[26].Incontrast,gazeestimation
techniquesusingregularcamerasandcomputervisiontechnologyhavethepotentialtoresolve
theselimitations,enablinggazemeasurementinmorenaturalsettings.Withtheadvancementof
deeplearning,appearance-basedgazeestimationhasbeenactivelystudiedinthelastdecade[12,22].
Appearance-basedgazeestimationisanapproachthatbuildsanML-basedmodelfromlarge-scale
data,andthemodelcanoutputgazedirectionfromaninputfaceimagewithoutexplicitlyextracting
imagefeaturessuchasthecenterofthepupil.Suchanapproachisknowntoberobustevenwith
low-resolution and poor-quality images [68], and there is potential for even more diverse eyeGamifiedDataCollectionforAppearance-BasedGazeEstimation 3
tracking applications [2, 52, 66]. When taking an appearance-based approach, the quality and
diversityoftrainingdataposesignificantchallenges.
Aclassicalapproachtoappearance-basedgazeestimationhasassumeddataspecificitytoan
individual[38,50,53].Recentyearshavewitnessedanincreasedinterestintrainingmodelsthatare
notperson-dependent,utilizinglarge-scaledatacollectedfromvariousindividuals.Deeplearning
modelshaveprovedhighlyeffectiveinthisendeavor,andvariousgazeestimationmodelshavebeen
proposedintheliterature[11,42,43,69].Similarlytootherimagerecognitiontasks,appearance-
basedgazeestimationalsosuffersfromoverfittingtotrainingdata,makingaccurateestimationin
testingenvironmentsthatsignificantlydifferfromthetrainingdataquitechallenging.Suchissues
cannotberesolvedsolelythroughdomainadaptationorgeneralizationapproaches[10,28,30,35,
45,58],anddiversedatasetsareindispensablebothfortheverificationofrobustnessandforthe
trainingofmorepowerfulmodels.
Severaldatasetshavebeencreatedtodatetoensurethediversityofvariousfactorssuchasthe
lightingenvironmentandheadpose[18,19,31,33,49,51,65,70].Suchdatasetscanbeconstructed
byinstructingparticipantstolookatspecifictargetpositionsandrepeatedlycapturingimages
oftheirfacesatthemomentoffixation.Forinstance,intheGaze360dataset,participantsstand
arounda360-degreecamerawhileresearchersmoveagazetargetboardmarkedwithanARmarker.
IntheETH-XGazedataset[65],participantsseatedinfrontofascreenwereinstructedtoclick
the mouse to confirm when a circle displayed as a gaze target shrinks to a dot. However, the
absolutenumberofthesestudiesisrelativelysmallcomparedtoresearchproposingmethodsfor
gazeestimation.Inevitably,thereisabiasinthelocationswheredatacollectionisperformedandin
theattributesoftheparticipants.Approachestosynthesizingtrainingdatahavealsobeenexplored,
buttheirperformanceisstillinferiortorealdata[44,60,61].Furthermore,asmentionedearlier,
appearance-basedgazeestimationrequiresfacialimages,whichintroducesfundamentaldifficulties
inrecruitingawiderangeofparticipants.Thisworkconstitutesanovelattempttoaddressthe
challengeofcollectingdiversedatathroughgamificationtoovercometheprevailingdifficultiesin
thisfield.
2.2 GamesforMLDataCollection
Gamificationreachesalargeraudiencebyalteringasubjectmattertobeexperiencedasagame[48]
andhasbeeninvestigatedsincetheearly1980s[40].Theconceptofgamificationhasbeendiscussed
acrossvariousfieldsofstudy,butitisparticularlyprominentwithinthecontextofcrowdsourc-
ing[41].Theideatypicallyrevolvesarounddistributinggamifiedtaskstouserstoutilizehuman
knowledgetosolvelargertaskssuchasproteinstructurediscovery[14,32].However,suchan
approachdoesnotalwaysalignwithMLdatacollection,whichisthefocusofourresearch.Spe-
cificinstanceswherethemainfocusiscollectingdataannotationsincludeimagelabels[55,56],
common-sensetexts[57],imagepreferences[24],andsound/musicannotations[34,54].Inmost
ofthesecases,thescenarioenvisionedisonewhereinnumerousplayersonthewebcollectively
performannotationsforthedata.Ourresearch’sobjectiveistodesignagamethatallowsforthe
simultaneousacquisitionofthedata(inourcase,faceimages)andthecorrespondingannotations
(gazedirections).
As more closely related work, Roemmele et al. proposed a game to collect training data for
motion-basedactionrecognition[47].ThepaperpresentsTriangleCharades,aweb-basedonline
gamewhereplayersanimatetrianglestodepictactionsforotherstoguess.Braggetal.alsoproposed
amobilegamecalledASLSeaBattletocollectalargecorpusoflabeledAmericansignlanguage
videosinanentertainingway[7].Playersrecordvideosofthemselvessigningtoattacksquareson
agridlabeledwithsigns,andtheiropponentviewsthevideostolabelthemandrevealifitwasa
hitontheirhiddenships.Comparedtothesecases,ourstudyhastwomajoruniquenesses.Oneis4 MingtaoYue,TomomiSayuda,MilesPennington,andYusukeSugano
Fig.2. Hardwareoverviewofthegamifieddatacollectionsystem.OursystemconsistsofA)twowebcams,
B)atransparentletterboard,C)tabletPCs,andD)customwritingboards.
thatthisstudy’stargetfordatacollectionisthetrainingdataforappearance-basedgazeestimation,
whichsignificantlydifferentiatesourresearchfromtheaboveexamples.Anotheruniqueaspectof
ourstudyisthatourgamedesignisnotpurelyanonlineapplication.Itisdesignedasamixed-media
systemthatincorporatesphysicaldevices.
Explorationofgamificationelementsisalsoseeninthefieldofcitizenscience[5,36].Regarding
sparkinginterestandunderstandingoftechnology,ourmotivationsaresimilartothoseofcitizen
science.However,theuniqueaspectofourstudythatsetsitapart,eveninthiscontext,isgaze
estimation targetingthrough a mixed-mediagame. While physical designis necessaryfor our
purposes,itisnotobviousthatthiswillworkinpractice.Thisstudyevaluatestheuserexperience
anddatacollectedthroughareal-worlduserstudy.
3 PROPOSEDSYSTEM
Inthissection,wewilldiscussthedetailsofthedesignandimplementationofourgamifieddata
collectionsystem.Fig.2showsanoverviewofourgamesystem.Oursystemconsistsofatransparent
letterboard(Fig.2B)equippedwithtwowebcamsembeddedintheframe(Fig.2A),tabletPCs
(Fig.2C),andcustomwritingboards(Fig.2D).Thewebcamsarephysicallyconnectedtoaserver
PC,andthetabletPCsestablishaconnectionwiththisserverusingalocalwirelessLANviaa
browser.Thetwoparticipantsengageinthegamewhileseatedoppositeoneanother,separated
bytheletterboard.Japanesealphabet(Hiragana)isdisplayedontheletterboard,reflectingthe
languagenativetothestudy’slocation.Thelettersareorganizedtoappearreversedwhenseen
fromtheopposingplayer’sside.Consequently,fromeitherplayer’sperspective,specificphysical
positionsontheboardcoincidewiththesameletters.
3.1 DesignProcess
As previously mentioned, the design process has two main objectives. The first objective is to
facilitaterecordingfaceimageswithasprecisegazedirectionlabelsaspossible,whichareusedfor
trainingappearance-basedgazeestimationmodels.Thesecondobjectiveistodesignagameplay
experiencethatallowsthegeneralparticipantstoengagewiththetechnologyandensuresthey
enjoytheprocess.Inotherwords,itisnotjustaboutcreatingahighlyeffectivedatacollectionGamifiedDataCollectionforAppearance-BasedGazeEstimation 5
toolbutalsoaboutproducinganapplicationthatparticipantswillfindengagingandenjoyable,
encouragingbroaderparticipation.
Theprimaryrequirementsfordatacollectionaremainlyreflectedinthedesignofthegamelogic.
Thetrainingdataforappearance-basedgazeestimationmustcontainfacialimageslinkedtotheir
ground-truthgazepositions.Thedatacollectionsystemshouldinitiallydirecttheparticipant’sgaze
towardsatargetlocationandsubsequentlycapturetheimagewhentheparticipantisverifiably
observingthegazetarget.However,merelyinstructingthegazetargetdoesnotguaranteethe
playerisaccuratelylookingatthespecifiedlocation.Toaddressthisissue,wehavedevelopedour
systemaroundacooperativegamemodeldesignedtoincentivizethequestionerplayertoconvey
theletterstotheanswererplayerprecisely.Theeffectivenessoffosteringcooperationratherthan
competitionamongplayershasbeennotedinthecontextofseriousgamesingeneral[8,23,59].
Previouscomparativestudieshavealsopointedoutthatgamesdesignedwithplayercooperation
andinterdependenceofferadvantages,suchassocialbondingandenjoyment,overthosewitha
competitivesetup[16,17,27].Inadditiontosuchbenefits,acooperativeruledesignisnecessary
forourgametoencourageplayerstoprovideaccuratedataalignedwiththegazetargetlabels.
Ifthiswereacompetitivegame,onemightbemotivatedtodeliberatelylookatdifferentletters
toinducemistakesintheopponent.Suchacooperativedesignfocusedonaccuratelyconveying
messagestootherhumanplayershasalsobeenadoptedinthepriorwork[47].
Itisalsonoteworthythatcraftingthegametoinvolveasmuchphysicalinteractionandcommuni-
cationaspossiblecaneffectivelystimulateinterestandfamiliaritywithtechnology.Thisobjectiveis
achievedbyintegratingnon-digitalcomponents,likeawritingboard,intospecificgamesegments.
Thedesign,whichnecessitatesthattwoplayersfaceeachotherphysically,satisfiestheneedfordata
collectionwhilesimultaneouslyfosteringnon-verbalinteractionbetweenparticipants.Aspointed
outinpriorresearchaimedatcertaindemographics,suchaschildrenandtheelderly[3,20,62,63],
theadvantagesofintuitiveandcommunicativephysicaldevicedesignscanbeimportantwhen
creatingtoolsaccessibletoadiverseuserbase.Thedesignofphysicallyplacingatransparentletter
boardisprimarilyarequirementforintroducingtheactofreadingtheotherplayer’sgazeintothe
game.Inaddition,weexpectittoarousetheinterestofadiverserangeofparticipantseffectively.
3.2 GameplayFlow
Figure3presentstheprocessofgameplay.Initially,thesystempromptsthequestionerwiththe
wordforthequiz.Atthisstage,afewlettersfromthesamewordaredisclosedtotheanswererto
functionashints.Inthesubsequentsecondstep,thequestionercommunicatesthehiddenletters
totheanswererusingeyegaze,notverbalcues,bylookingateachletteronatransparentboard.
Theanswererdoesnotneedtospecifytheletterbutratherestimateitsapproximatepositionby
interpretingthequestioner’sroughgazedirection.Customizedwritingboardsshowthesameletter
layoutasthetransparentboardandaidthisprocessbyallowingtheanswerertodrawacircle
aroundtheexpectedroughletterlocation.Thisstepisexecutedrepeatedlyforeachhiddenletter.
Finally,inthethirdstep,theanswererguessestheword,identifyingthemissinglettersbasedon
theestimatedpositionstoformacoherentword.Ifthewordisguessedcorrectly,theduoscoresa
point.Theplayersexchangeroles,andthegameproceedswiththenextword.Throughthegame,
thequestioner’sfaceimagesandthepositionsofthehiddenlettersaresaved.Thisisequivalent
tocapturingfaceimageswithknowngazetargetlocations,providingdatathatcanbeusedfor
trainingappearance-basedgazeestimationmodels.
Ourgameisdesignedfortwo-playerparticipation,alternatingrolesbetweenquestionersand
answerers.Thequestioner’sobjectiveistoindicateawordpresentedbythesystemtotheanswerer
bygazingsuccessivelyatlettersonatransparentboard.Sinceidentifyingtheexactletteranother
playerlooksatismorechallengingthanonemightassume,wehaveeliminatedtheneedforthe6 MingtaoYue,TomomiSayuda,MilesPennington,andYusukeSugano
Questioner
Questioner knows Look at each letter
whole word
Step 1 Step 2 Step 3
Provide Look at Estimate
word and guess and
the letter answer
the word
Answerer know some Guess the Answerer guesses the words
letters rough location based on the notes
Answerer
? ?
Fig.3. Theflowofthegameplay.Step1:Thequestionerfirstidentifiestheword,andtheanswererobserves
someclueletters.Step2:Thequestionerlooksateachletter,andtheanswererguessestheroughlocationsof
eachletter.Step3:Theanswererguessestheword.
answerertoidentifyeachletterindividually.Instead,theanswererfocusesontheentiresetof
letters,attemptingtodiscernthecompletewordtoensureitformsasensibleterm.
Additionally,weobservedthatguessingawordwithnocluesisextraordinarilydemanding.Dur-
ingtheprototypingphase,werealizedthatunveilingcertainlettersofawordashintssignificantly
mediatethedifficultylevel.Maintainingthislevel,neithertoosimplenoroverlycomplicated,is
crucialto sustainingthe players’interest inthe game. Ourstrategy exhibitsits benefits asthe
game’sdifficultycanbeadjustedbymanipulatingthenumberofletterspresentedashints.
3.3 UserInterface
Figure4presentstheflowofourimplementeduserinterface.Thegamecommencesuponboth
playershittingthestartbutton.Beforethefirstquestion,abriefgamedescriptionandnotesappear
on both screens. At first, player B, the answerer, gets a prompt indicating which letter will be
conveyedandcanverifythecluelettersdisplayedontheirscreen.Thegameentersthephaseof
conveyingletterswhentheanswerergetsreadyandpressesthenextbutton.
The questioner fixates on the location of each hidden letter on the transparent board and
pressesthecapturebutton.Afterthecountdown,thesystemcapturesanddisplaysanimageofthe
questioner’sface,combinedwithgazeestimationresultscomputedbyapre-trainedgazeestimation
model.Whennofaceisdetectedinthecapturedimage,thesystemdefaultstodisplayingatext
messageratherthanafaceimage.Thequestionerpressestheconfirmationbuttonafterconfirming
thattheirfacehasbeenaccuratelycapturedwithoutblinking.Thesystemthenpresentsthecaptured
faceimagetotheanswerer.Concurrently,theanswererendeavorstointerpretthequestioner’s
gazeandanticipatetheprobablelocationoftheletteronthewritingboard.Thegameprogresses
tothesubsequentletteroncetheanswerercompletestheirpredictionandpressesthenextbutton.
Uponthequestionercompletesalltheletters,thesystemshiftsfocustotheanswerer,awaiting
their response. Subsequently, the answerer inputs their predicted words using the on-screen
keyboard.Thecapturedfaceimagesassociatedwitheachhiddenletterarealsoexhibitedsideby
side.Thescreenalsoshowsatimerindicatingtheelapsedtimeonthetop,andtheanswermustbeGamifiedDataCollectionforAppearance-BasedGazeEstimation 7
Player A
(2) Trigger capture (3) Confirm image (6) Check answer
Capture After All letters
button countdown recorded
Switch
roles
Player B Cancel button C buo tn tofi nrmation N bue tx tot n
(1) Verify clue letters (4) Guess the location (5) Input answer
Observe player A's gaze
Next Answer
button button
Fig.4. Flowofouruserinterface.(1)Thequestionerandtheanswererconfirmthetargetletter,and(2)the
questionertriggerstheimagecapture.(3)Oncethequestionerhasapprovedthecapturedimage,(4)the
answereranticipatesandnotestheroughpositionoftheletter.(5)Whenalllettershavebeencaptured,the
answererguessesthewordandinputstheanswer.(6)Thesystemdisplaysthecorrectanswer,andplayers
switchrolesandmoveontothenextword.
providedwithinapresetduration.Asthistimelimitdrawsnear,anadditionalclueisrevealedin
theformofthefirstletter.Thesystemshowswhethertheansweriscorrectoncethetimelimit
concludesortheanswererfinalizestheanswer.Movingtothenextword,theplayersswitchroles;
thischangeisinitiatedbytheanswererpressingtheproceedbutton.Thegamingsessionendsafter
aspecificquantityofquestionwordshasbeenpresented.
3.4 ImplementationDetails
Thepositionalrelationshipbetweentheletterboardandthecameraiscalculatedbyattachinga
cameracalibrationpatterntotheboardandobservingitthroughamirror[46].Inotherwords,the
3Dpositionofeachletterinthecameracoordinatesystemisknownthroughcalibration.
ThefrontendserverwasdevelopedusingReactandNode.js,anditestablishesaconnectionto
thebackendserverthroughaWebSocket.ThebackendserveroperatesonthePythonlanguage,
utilizingtheFlaskframework.MostparameterscanbemanipulatedfromthefrontendsettingsGUI,
includingthetotalnumberofquestionwordspergame,theprescribedminimumandmaximum
lettersperword,andthequantityofhidden(question)lettersineveryword.Thebackendserver
maintainstheseparameters.
TheworddictionarycomprisesnounscuratedfromaJapaneselanguagelearners’dictionary,
andthebackendemploysthislistinashuffledsequencebasedonthesettings.Itshouldbenoted
thatspecialcharactersnotusedontheletterboard,suchasletterswithdiacritics,areexcluded
fromthequestions.Ifthequestionwordcontainsthesecharacters,theywillalwaysbedisplayed
ashints.Thebackendparametercontrolsthegame’stimelimitandthetimespanuntilextraclue
lettersarerevealed.
Thebackendserverisalsoinchargeofwebcamcontrol,andthefaceimagescapturedduring
the game are preprocessed in the backend server. After face detection [4] and facial landmark
alignment[9],thefaceimageisnormalizedfollowingthestandardprotocolinappearance-based
gazeestimation[67].Gazeestimationwasperformedusingapre-trainedmodelonthesynthetic
MPII-NVdataset[44].
4 USERSTUDY
Weconductedauserstudytocomparetheaccuracyofthegazelabelacquiredunderthegamified
andstandardsettings.Fig.5illustratestheflowofthetwostudyconditions.Thefirstgamified
setting involved experiments conducted in a public space, where we made a gamified system8 MingtaoYue,TomomiSayuda,MilesPennington,andYusukeSugano
Study Protocol Data Collection Measures
Gamified Data Collection User Survey
(w/o Eye Tracker)
Gamified
Setting Gamified Data Collection Letter Positions
(w/ Eye Tracker)
Eye Tracker Data
Target Positions
S St ea tn tid na grd Non-Gamified Data Collection
Eye Tracker Data
Fig.5. Illustrationoftheuserstudysettings.Thegamifiedsettingwasconductedinapublicspace,where
voluntaryparticipantswerevisitors.Thestandard-settingexperimentwasconductedinourlaboratory,and
participantswererecruiteddirectly.Werecordedeyetrackingdatafromgamifiedandstandardsettingsfor
labelaccuracycomparisonandaskedparticipantstocompleteacustomsurvey.
(a)Systeminstallation (b)ARmarkersandeyetrackingheadset
Fig.6. Theenvironmentofourgamifieddatacollectionstudy.(a)Weinstalledthegamesysteminthecorner
ofacafeteriaonouruniversitycampus.(b)Wecollectedeyetrackingdatafromasubsetofparticipantswhile
playingthegame.
availableforcafeteriavisitorstoexperience.Thesecondstandardsettingreflectedtheconventional
datacollectionmethodwithinacontrolledlaboratoryenvironment,whereweexplicitlyrecruited
participantsandprovidedthemwithspecificinstructions.Weemployedwearableeyetrackersto
obtainthereferencegazepositionsandinvestigatedthedifferencesingazelabelaccuracybetween
the two settings. Additionally, under the gamified setting, we evaluated user experience using
acustomsurvey.Thefollowingsubsectionsprovidedfurtherdetailsofthestudyprotocolsand
additionalmeasures.
4.1 StudyProtocols
4.1.1 GamifiedSetting. Inkeepingwiththeconceptofoursystem,wedidnotrecruitexperiment
participantsindividuallyforthegamifieddatacollection.Instead,weinstalledoursystemina
cafeteriaonouruniversitycampusforfivedaysandinvitedvolunteerstoparticipate.Thisgave
usanidealsettingtoconductourexperimentswithunconstrainedparticipants.Tothisend,we
installed the game system in the corner of the cafeteria (Fig. 6a). Even though this cafeteria is
locatedonauniversitycampus,itisopentouniversityaffiliatesandthegeneralpublic.Considering
privacyconcerns,weplacedalargescreenbehindthesystem’snon-wall-facingside,whichactedas
ashieldtopreventaccidentallycapturingbystandersinthebackground.Atthecafeteriaentrance,GamifiedDataCollectionforAppearance-BasedGazeEstimation 9
weputuppostersannouncingthatanexperimentwasbeingconducted.Likewise,weplacedsimilar
postersneartheexperimentbooth,andanotherposterwassetuptoremindindividualsthatfilming
wasunderway.Thesemeasuresensuredthatallvisitorsknewabouttheongoingexperimentand
therecordingactivities.
Weensuredthatatleasttwostaffmemberswerealwaysattheexperimentbooth.Ifaperson
showedinterestinourexperiment,theywerefirsthandedadetailedhandoutexplainingthepurpose
andscopeoftheexperimentandconductedthroughathoroughexplanationconcerningthedata
collectionprocess.Atthistime,itwasexplicitlyexplainedthatfaceimageswouldbecapturedand
thattheycouldbeusedfornon-commercialresearchasapublicdataset1.Weallowedonlythose
whoexpressedunderstandingandconsenttothedatacollectionprocessandvoluntarilysignedan
agreementformtoparticipateintheexperiment.SinceourgamerequiresJapanesevocabulary,
theparticipantswereJapanesespeakers,andtheexplanationandconfirmationofconsentwere
alsoconductedinJapanese.Theuniversity’sethicsreviewcommitteeexaminedandapprovedthe
experimentdetailsandconsentprocess.
Althoughsuchcaseswererare,individualswhocamealoneshowedinterestinparticipatingin
theexperiment.Occasionally,twostrangerswhovisitedsimultaneouslywerepairedtoparticipate
intheexperiment.Ifapartnercouldnotbefound,theparticipantswerepairedwithastaffmember
for the experience. In this case, the data obtained from the staff member were not used in the
followinganalysis.Toensurethatallparticipantscouldexperiencetherolesofquestionerand
respondent,wearrangedforeveryonetoengageintwogamerounds.Throughouttheexperiment,
westandardizedthewordlengthtofiveorsixcharacters,withthenumberofcharactersinquestion
consistentlysetatthree.Ifanyparticipantencountereddifficultiesunderstandinghowtoprogress
inthegame,ourstaffcouldprovideimmediateassistance.
Overfivedaysofsysteminstallation,wecollecteddatafrom47participants.Weassignedeach
participanttoproducedataforthreelettersintheirroleasaquestioner.However,duetoasystem
glitch, data from one participant was only recorded for two letters, resulting in a total of data
gatheredfor140letters.
4.1.2 Standard Setting. The standard setting refers to an experiment conducted in a situation
similartotheusualdatacollectionperformedinpreviousstudies.Theexperimentwasconducted
withinourlaboratoryspace,andwesecuredparticipantsbydirectlyapproachingstudentsand
researchers. Therefore, it is assumed that participants in the standard setting engaged in the
experimentwithamoreseriousattitudecomparedtothoseinthegamifiedsetting.
Weshowedawhitecanvaswiththesamephysicalsizeastheletterboardona55-inchdisplay.
The participants were instructed to look at 50 stimuli randomly shown inside the canvas. We
showedacircleofthesamesizeastheletterboarddesign(Fig.2)andaplussignatthecenterofthe
circle.Whenevertheparticipantpressedthespacekeyofakeyboard,acirclebegantoshrinktothe
center.Afterthreeseconds,thesameperiodasthecapturecountdownforthegamifiedsetting,the
shrinkingcircledisappeared,andgazedataandsceneimageswereretrievedfromtheeyetracker.
Sixteenparticipants(thirteenmaleandthreefemale)tookpartinthisnon-gamifiedstandard
datacollection.Theagesrangedfrom22to30years(𝑀 =25.5,𝑆𝐷 =2.5).
4.2 Measures
4.2.1 GazeLabelAccuracy. Underboththegamifiedandstandardsettings,weusedawearable
eyetrackertosimultaneouslycapturetheground-truthgazepositionstoevaluatetheaccuracyof
theacquiredgazelabels.Inotherwords,foreachdatapoint,weobtaintwogazedirections:one
calculatedfromthecharacterorgazetargetthattheparticipantwaslookingatduringthegame
1Thedatasetisbeingconsideredforpublicationinafuturelong-termstudyandwillnotbepublishedwiththispaper.10 MingtaoYue,TomomiSayuda,MilesPennington,andYusukeSugano
ordatacollection,andtheothermeasuredbytheeyetracker.Asitispossibleforparticipantsto
unintentionallylookatdifferentplacesduringthegameordatacollection,theirfacialimagescould
becapturedwiththeirgazedirectedelsewhere.Thisexperimentaimstoverifyhowcloselythe
gazelabelsobtainedunderthetwosettingsapproximatetheidealgroundtruthlabelscaptured
bytheeyetracker.Pleasenotethatthedistinctiveappearanceofwearableeyetrackerglassescan
introduceartifactsintothetrainingdata,andtheycannotbeusedintheactualdatacollectionfor
appearance-basedgazeestimation.
Thedistinctappearanceoftheeyetrackerframeactsasanartifactonthefacialimage,making
thecapturedfaceimagesunsuitablefortrainingdata.Forthisreason,inagamifiedsetting,eye
trackingdatawascollectedfromonlyaportionoftheparticipants,specifically22outof47people.
Eachparticipantwasinstructedtoweartheeyetrackerasthequestionerwhileplayingthegame.
WeusedthePupilInvisiblewearableeyetracker2tocollecteyetrackingdata.Beforeeachgame
session,weconductedgazeoffsetcorrectionusingthePupilcompanionapp.Weinstructedthe
participantstolookattheupperleftcornerofeachARmarkerandadjustedtheoffsetbetween
themarkercornerandgazelocation.Forthesegamesessions,thebackendserveraccessedthe
real-time API of Pupil Invisible to retrieve eye tracking data at exactly the moment when the
questioner’sfaceimagewascaptured.Furthermore,wecollectedeyetrackingdatafromall16
participantsunderthestandardsetting.Wealsoperformedasimilaroffsetcorrectionforeach
participantbeforecollectingthedata.Eightdatapointswereexcludedduetoseveredegradationof
sceneimagequality,resultingin792datapoints.
Tomaptheeyetrackingdatatotheboardcoordinatesystem,weplacedsixArUcoARmarkers[21]
on both sides of the board before the game sessions (Fig. 6b). In the standard setting, we also
displayedARmarkersatthesamepositionsonthedisplay.AfterretrievingthedatafromPupil
Invisible, AR markers were detected in the scene image using the OpenCV library [6]. If the
numberofdetectedmarkerswaslessthantwoforeachrow,thedatawashandledbymanually
annotatingmoremarkerlocations.Then,byleveragingtheintrinsicparametersofthescenecamera
providedbyPupilInvisibleandthemarkerlocationsintheboardcoordinatesystem,weobtaineda
homographymatrixtotransformtheimagecoordinatesystemintotheboardcoordinatesystem.
Inaddition,weestimatedthecameraoriginlocationusingthePnPalgorithm[37].Thisway,we
computetheangulardistancebetweenthegazeandletterpositions,assumingthatbothvectors
originatefromthecameraorigin.
4.2.2 UserExperienceandTechnologyAcceptance. Afterthegameexperience,participantsinthe
gamifiedsettingwereencouragedtoprovidefeedbackthroughanonlinequestionnaire.Manyofthe
experimentparticipantswereordinarypeoplewhohappenedtovisitthecafeteriaandhadlimited
timetorespondtothesurveyaftertheexperiment.Inadditiontogameplay,wewerealsointerested
inadditionalquestionsabouthowparticipants’perceptionsofgazeestimationandAI/MLmight
shiftfollowingtheirgameexperience.Forthesereasons,wedesignedashortcustomsurveywith
itemsspecificallyfocusedongameplayandtechnologyacceptance.Pleasenotethatthestandard
settinglacksgamingelements,andwedidnotaskforanysubjectivefeedbackinthissetting.
In the questionnaire, besides gathering information about the participant’s gender, age, and
industry,weaskedthemtorespondtothestatementslistedinTab.1onafive-pointLikertscale
rangingfrom“stronglyagree”to“stronglydisagree”.Moreover,weaskedforfeedbackinafree-
formformat,allowingparticipantstoexpresstheirthoughtsopenlyandwithoutrestriction.We
obtainedsurveyresponsesfrom46outof47participants,comprising26malesand20females.The
participants’agesrangedfrom18to79years(𝑀 =37.7,𝑆𝐷 =14.8).
2https://pupil-labs.com/products/invisible/GamifiedDataCollectionforAppearance-BasedGazeEstimation 11
Table1. Listofquantitativequestionsinoursurvey.Participantswereinstructedtoprovidefive-pointLikert
scaleratings.
Category ID Question
Q1-1 Itwasinterestingtoreadthegazeoftheotherplayer.
Q1-2 Itwaseasytoreadthegazeoftheotherplayer.
Game Q1-3 Itwasinterestingtoguessthewholeword.
Q1-4 Itwaseasytoguessthewholeword.
Q1-5 Itwasinterestingtocooperatewiththeotherplayer.
Q1-6 Iwouldliketoplaythegameagain.
Q2-1 Thegamewasagoodopportunitytolearnaboutgazeestimationtechnology.
Q2-2 IcouldunderstandhowthegamecouldleadtoresearchongazeestimationAI.
Technology
Q2-3 Icouldunderstandwhycollectingfaceimagesofvariouspeoplethroughthegameis
necessary.
Q2-4 IamnotafraidthatmyfaceimagewillbeusedforAIandMLresearch.
Gamified setting
3.81
Standard setting
2.69
0 5 10 15 20 25 30 35 65 70
Angular error (degree)
Fig.7. Angularerrordistributionsoftwodatacollectionsettings.Eachboxplotcorrespondstogamifiedand
standarddatacollection,andthehorizontalaxisshowsangularerrorevaluatedfromtheeyetrackingdata.
5 RESULTS
5.1 DataAccuracyComparedtoStandardDataCollection
Wefirstcomparetheaccuracyofgazelabelsunderthetwosettingsusingdataacquiredwiththe
eye tracker. As mentioned above, we used 65 data points collected from 22 participants in the
gamifiedsettingand792from16participantsinthestandardsetting.Pleasenotethatindividual
differencesareexpectedtoplayalargerroleinthegazelabelaccuracy.Whiletherearemoredata
pointsforthestandardsetting,thenumberofparticipantsisapproximatelyinthesameorderfor
bothsettings.
Figure 7 presents box plots illustrating the distribution of errors among participants in the
gamified and standard settings. Because of an extreme outlier, the horizontal axis is partially
omitted in the middle for better visibility. In the case of the gamified setting, represented as a
plotatthetop,themeanangularerrorwasindicatedas3.81degrees.Meanwhile,thestandard
setting at the bottom reflected a mean angular error measuring 2.69 degrees. Statistically, the
gamifiedsettinghadsignificantlyhighererrorsthanthestandardsetting(Mann-WhitneyUtest,12 MingtaoYue,TomomiSayuda,MilesPennington,andYusukeSugano
Gamified setting Standard setting
150 150
100 100
50 50
0 0
50 50
100 100
150 150
150 100 50 0 50 100 150 150 100 50 0 50 100 150
x (mm) x (mm)
Fig.8. Spatialdistributionsofmeasuredgazepositions.Thecenterofeachplotcorrespondstotheexpected
gazetargetlocationsandshowsscatteredplotsoftherelativegazepositionsmeasuredbytheeyetracker.
Eachdatapointiscolor-codedbytheparticipant’sID.
𝑈 = 30318,𝑝 = 0.02). One evident outlier in the data from the gamified setting was captured
whentheparticipantwaslookingnotattheletterbutatthetablet.Themeanangularerrorofthe
gamifiedsettingwas2.82degreesaftermanuallyeliminatingthisoutlier,whichiscomparableto
thestandardsetting.
Figure8visualizesthedistributionofgazepositionsforbothgamifiedandstandardsettings.We
plottedtherelativemeasuredgazepositionfromtheassumedtargetlocationforbothscenarios.
Eachdatapointiscolor-codedbytheparticipant’sID,andwehavealsoaddedcirclesofthesame
sizeasonthetextversionforbetterrepresentation.Thevisualizationrangeislimitedto±150mm,
butpleasenotethatoutliers(oneforgamifiedsettingandfiveforstandardsetting)extendbeyond
thisrangeasinFig.7.Whileaslightspatialbiascanbeobserved,itisapparentthattheproposed
approachhasnotresultedinanyextremeskewingofthedistribution.
5.2 AnalysisusingaPre-trainedGazeEstimationModel
Next, we investigate whether the data collected from our gamified system can contribute to
evaluatingandtrainingthegazeestimationmodels.Forthispurpose,wetrainedthestate-of-the-art
HybridGazeTRmodel[11]usingtheMPII-NVdataset.Weuseall140datapointsacquiredinthe
gamifiedsettinginthisexperiment.
Figure 9 shows box plots illustrating the distribution of estimation errors of the pre-trained
model.Theground-truthvaluesarecalculatedbasedonthelocationoftheletters,andtheplots
reflecttheangularerrorbetweenthosetruevaluesandtheestimatedgazevectors.Thefirstplot
correspondstotheerrordistributionofthepre-trainedmodel.Themeanangularerrorwas8.82
degrees,notablylargerthanexpectedinordinarydatasets.Thisindicatesthatourdatapresents
quite a challenging scenario for gaze estimation. Furthermore, the second plot corresponds to
thesubsetwearingtheeyetrackingheadset,asusedinSec.5.1.Oneshouldnotethatthisdatais
typicallymoredifficultduetotheocclusioncausedbytheeyetrackeritself.Evenconsideringthis
)mm(
y
)mm(
yGamifiedDataCollectionforAppearance-BasedGazeEstimation 13
Overall
8.82
Eye tracking subset
10.24
Overall (after fine-tuning)
7.96
0 10 20 30 40 50
Angular error (degree)
Fig.9. Angularerrordistributionsofthepre-trainedgazeestimator.Fromtoptobottom,eachboxplot
correspondstotheoveralldistributionofthepre-trainedmodel,thedistributionovertheeyetrackingsubset,
andtheoveralldistributionofthefine-tunedmodel.
Fig.10. Examplesofgazeestimationonourdata.Theredandgreenarrowsrepresenttheground-truthand
thepredictedgazedirections,respectively.
factor,theerrorismuchlargerthaninFig.7,confirmingtheexistenceofroomforimprovementin
estimationaccuracy.
Tofurtherverifywhetherthemodel’saccuracyimproveswithadditionaltrainingusingourdata,
weevaluatedtheprecisionofthemodelfine-tunedina3-foldcross-validation.Specifically,we
splittheparticipantsintothreefolds,wherethedatafromtheeyetrackingsubsetwerebalancedin
bothfine-tuningandtestsets.Werandomlychose15imagesfromthefine-tuninggrouptotrain
themodelbytenepochs.ThethirdplotinFig.9correspondstothisfine-tunedevaluation,and
themeanerrorisreducedfrom8.82to7.96.Pleasenotethatthenumberofimagesusedinthis
experimentismuchfewerthantypicallyusedforregularmodeltraining.However,theresultsshow
thatevenfine-tuningwithafewimagescanenhancethemodel’soverallaccuracy,demonstrating
thevalueofthecollectedimagesastrainingdata.
Figure10presentsexamplesofpredictionresultsfromthepre-trainedgazeestimator.Redarrows
display the ground-truth value reflecting the letter’s location, while green arrows indicate the
predicted gaze direction. Although we obtained permission to capture face images during our
study,thelowerportionofthefaceisintentionallyobscuredtoprioritizeprivacy.Oneinteresting
observationfromthedataobtainedinthisstudyisthediverserangeoffacialexpressionsexhibited
bytheparticipants.Theimagesontheleftshowexamplesofsuchfacialexpressions,including14 MingtaoYue,TomomiSayuda,MilesPennington,andYusukeSugano
Q1-1
Q1-2
Q1-3
Q1-4
Q1-5
Q1-6
Q2-1
Q2-2
Strongly disagree
Q2-3 Disagree
Neither agree nor disagree
Agree
Q2-4 Strongly agree
28 26 24 22 20 18 16 14 12 10 8 6 4 2 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46
Number of responses
Fig.11. Responsesfromthequestionnaire.EachchartcorrespondstoaquestioninTable1,asindicatedon
thehorizontalaxis.Differentcolorsrepresenteachoption,fromredfor“stronglydisagree”tobluefor“strongly
agree”.
laughingorengaginginconversation.Thesefactorsmayhavethepotentialtoinfluencetheaccuracy
ofgazeestimation.Therelationshipbetweenfacialexpressionsandgazeestimationhasnotbeen
thoroughly investigated, and our data collection methodology could shed light on this matter.
Imagesontherightsiderepresentagroupofsampleswithhighererror.Thisincludescaseswhere
estimationisfundamentallydifficult,suchaswhenaperson’seyesarealmostinvisible,andcases
closetooutlierswheretheimagewasnottakencorrectlyduringthegame.Thecasewherethe
participantblinksatthemomentofcaptureisclearlyundesirable,anditisimportanttodesigna
processinwhichtheplayerscanverifythecapturedimagesmorecorrectly.
Asignificantcorrelationwasobservedbetweentheerrorestimatedbyapre-trainedmodeland
theerrormeasuredbyaneyetracker(𝑟 =0.72,𝑝 <0.01).However,thiscorrelationwaslargelydue
totheoutlierdatamentionedearlier.Whenthisdatawasremoved,nocorrelationwasobserved
intheremainingdata(𝑟 =0.07,𝑝 =0.57).Thez-scoreoftheoutlierdatacalculatedbasedonthe
errorestimationbythepre-trainedmodelwas6.02.Thissuggeststhatevenasimplemethodcould
automaticallyeliminatethisoutlier.Amongthedatacollectedwithoutusinganeyetracker,there
weretwoinstanceswherethez-scoreexceeded3.0,correspondingtothetworightmostimagesin
thefirstrowofFig.10.
5.3 UserExperiencefromtheQuestionnaire
Thefive-pointLikertscaleratingsfromtheonlinequestionnaireareshownasstackedbarchartsin
Fig.11.TheverticalaxisshowsthequestionIDsinTable1.Thehorizontalaxisshowsthenumberof
responsesforeachquestion.Differentcolorsrepresenteachoption,fromredfor“stronglydisagree”
tobluefor“stronglyagree”.
Theupperpart(Q1)isalignedwithquestionsaboutthegameexperience.Overall,wefound
favorableevaluationsfortheenjoymentoftheexperience,asobservedinQ1-1,3,and5.Particularly,
Q1-3, about how interesting it was to guess the words, did not receive any negative feedback.
Furthermore,thequestionregardingthedesireforrepeatedgameplay(Q1-6)receivedconsistently
positiveresponses,indicatinghighusersatisfactionandreplayvalue.Questionscenteredonthe
game’sdifficultylevel(Q1-2,4)revealeddividedopinions,withpositiveandnegativeresponsesGamifiedDataCollectionforAppearance-BasedGazeEstimation 15
equallyprevalent.Thedifficultyingazereading(Q1-2)showedanearlysymmetricaldistribution
fromtherespondents,whereasthechallengelevelofguessingthewords(Q1-4)gatheredseveral
negativeresponses.AsignificantmediumpositiverelationshipexistedbetweenQ1-2andQ1-4
(Spearman’srankcorrelation,𝑟 =0.45,𝑝 <0.01).However,neitheroftheircorrelationswiththe
questionaboutinterestingness(Q1-1,3)wassignificant,andthedifficultydidnotnecessarilymean
theydidnotenjoythegame.
The lower section of the figure focuses on questions about the insights participants gained
fromthegameexperience.Thepredominantlypositiveresponsessuggestthatthegamecanfoster
valuablelearningexperiences.However,comparedwithQ2-1,aquestionaskingifthegameserved
asanopportunitytogainknowledgeabouteyetrackingtechnology,weobservedmorenegative
responses for other questions related to a deeper understanding of the game’s meanings and
intentions.
Outofallrespondents,twelveindividualsleftcommentsintheopen-endedsection,mostofwhich
werebriefduetothenatureoftheexperimentalsetting.Fivecommentsexpressedgratitudeforthe
insightsgainedfromexperience,suchas“Ilearnedalot.”Furthermore,threecommentsmentioned
somethingaboutparticipatingintheresearch,like“Iamgladtobepartofthedataanalysis.”We
alsonotedarangeofimpressionsregardingdifficultylevel.AcommentfromP32supplementsthe
trendsobservedinQ1-2andQ1-4;“WhileIcouldguesstheroughregion,confirmingthewordfrom
therewasquitedifficult.”ThisparticipantrespondedtoQ1-2aseasy(“stronglyagree”)butQ1-4
asdifficult(“disagree”).P30,incontrast,suggestedthatmakingthequestionsmorechallenging
andcomplexwouldbeacceptable,respondingaseasy(“stronglyagree”)tobothQ1-2andQ1-4.A
commentfromP21reflectedthegapbetweenthegamingexperienceandthereal-worldresearch
community,statingthat“IdonotmindmyfacialimagesbeingusedbyresearchersIknow.However,
Iwouldfeeluncomfortableiftheywereusedbyaresearcherunknowntome.”Finally,participant
P45speculatedaboutpotentialapplicationsofgaze-trackingtechnology;“Itwouldbeamazingif
feelingscouldbeunderstoodfromone’sgaze.”Thisindicatestheparticipant’sinspiredanticipation
towardsfurtheradvancementsandbroaderusesofgazeestimation.
6 DISCUSSIONS
6.1 QualityoftheCollectedData
Theexperimentsconductedusingawearableeyetrackerconfirmedthatthedatacollectedfrom
our game demonstrates a significantly larger margin of error than the data obtained through
conventionaldatacollection.However,thisdifferenceisnotnotablylargerandisstillsmallerthan
thetypicalerrorofpre-trainedgazeestimationmodelsonexistingdatasets.Uponinspectionofthe
datacollectedfromthegame,weidentifiedoneobviousoutliersignificantlyimpactingtheaverage
error.Oncethisoutlierwasmanuallyremoved,themarginoferrorshrankdramatically,rendering
thedataonparwiththoseobtainedthroughthestandarddatacollectionprocess.
Evaluatingtheperformanceofgazeestimationmodelstrainedonexistingdatasetsusingour
collecteddata,wealsoconfirmedthatsubstantialerrorsemergedwhencomparedtotheexisting
datasets.Theseerrorswerelargerthanthoseobservedthroughtheeyetrackerexperiment,sug-
gestingagapexistsbetweenthegazeestimationmodel’sperformanceandupper-boundaccuracy.
Thismeansthefaceimageswecollectedexhibitdifferentappearancecharacteristicsfromexisting
datasets,suggestingthepotentialtobringfurtherdiversitytofaceimageswhenservedastraining
data.Themodelfine-tuningexperimentsalsorevealedthatincorporatingsamplesfromourdataset
canenhancegazeestimationperformance.Thisfurtherprovesthatthelabelaccuracyisacceptable
astrainingdata.Inotherwords,fromboththeperspectivesofimagediversityandlabelaccuracy,16 MingtaoYue,TomomiSayuda,MilesPennington,andYusukeSugano
itcanbeconcludedthatthisstudy’sapproachyieldsvaluabletrainingdataforappearance-based
gazeestimation.
As supplementary results, we confirmed that removing outliers via a pre-training model is
possible to some extent. This direction can potentially become significant when dealing with
large-scaledatacollection.However,itshouldberememberedthatautomaticallyremovingoutliers
isnoteasy,andthereisapossibilityofremovingtoomuchdata.Therefore,itiscertainlydesirable
tobeabletoexcludeduringthegame.Althoughaprocessforplayerstoqualitativelyreviewtheir
capturedimageswasimplementedinoursystem,itwasprobablyinsufficientforthemtonotice
labelerrors.Beingabletoeffectivelyfilteroutoutlierswithoutdrasticallyaffectingthegameplay
experienceisanimportantfutureworkofoursystem.
6.2 UserExperience
Overallreactionstowardthegamifiedsystemwerelargelypositivebasedonquestionnaireresponses.
Participantsindicatedsustainedinterestinguessingwords(Q1-3),enjoyablecooperation(Q1-5),
andadesiretoreplay(Q1-6).Whilepromising,itmustbenotedwecannotdefinitivelystatethat
ourapproachdirectlyimprovedengagementwithoutcomparativedataonthenon-gamifiedsetting
andotherpossibledesignoptions.Still,willingnesstorepeatthegamehaspromisingimplications
forfeasibilityin,e.g.,long-termpublicinstallations.
Theparticipantsrespondedpositivelytotheirinterestinthetechnology.Wedidnotinitially
anticipatetheextentoftheirpositiveresponsetoourinquiriesabouttheirgraspofthetechnology,
buttheoverallresponsewasfavorable(Q2-1).Despitetheneedforfurtherinvestigationtoascertain
theaccuracyoftheircomprehension,itseemsweweresomewhatsuccessfulinconveyingthe
conceptofdatacollectionforappearance-basedgazeestimationthroughthegamingexperience.
Thismakesastrongcaseforusinggamifiedexperiencestosharecomplextechnologyconceptsina
moreaccessibleandengagingway.
Giventhenatureoftheexperiment,theparticipants’demographicswerebeyondourcontrol,
andthedemographicsoftheparticipantscanbeseenasthepotentialaudienceforthisgame.In
thatsense,itisnoteworthythattheagerangeoftheparticipantsinthisexperimentwasbroadly
distributed from 18 to 79 years old. Stimulating the interest and enjoyment of a diverse group
ofparticipantsisasignificantchallengefrombothdatacollectionandtechnicalcommunication
perspectives.Ourphysicalgamedesignhasbeencarriedoutwiththeacquisitionofsuchadiverse
rangeofplayersinmind,andtheparticipants’agedistributionandtheirsurveyresultsdemonstrate
aglimpseofitspotential.
Asmentionedatthebeginning,privacyisasignificantissuewhendealingwithfaceimages,
anditmustbeemphasizedagainthatthisstudy’sapproachdoesnotdirectlyaddressthisproblem.
Despiteclearexplanationsandobtainedconsentfromgameparticipantsinourstudy,afewstill
expressedconcernsabouttheirfaceimagesbeingusedinresearch(Q2-4).Thisaspectbecomes
particularlycriticalwhenthegameissetupasapublicinstallation.Itishighlypossiblethatmerely
displayingtextexplanationsisinsufficienttoensureplayersfullyunderstandtheresearchcontext
anddatausage.Amorein-depthexperiencedesignwillbenecessaryforfuturework,moving
beyondsimplyacquiringsuperficialengagement.
6.3 LimitationsandFutureWork
6.3.1 StudyConditions. Whileincludingastandardlab-baseddatacollectionsettingprovidesa
baselineforlabelaccuracy,itisindeedinsufficienttoevaluateuserexperience.Ideally,datacollection
without gameelements should be conducted withvisitors in publicspaces to demonstratethe
effectsofgamification.Whileitisunlikelythatstandarddatacollectionprotocolisenjoyablefor
participants,itcannotbedeniedthatthosewhofindthegameparticularlydifficultmightviewGamifiedDataCollectionforAppearance-BasedGazeEstimation 17
regulardatacollectionmethodsmorefavorably.Moreover,manualadministrationbyresearchers
in the gamified setting may inadvertently influence user behavior and experience, and a fully
autonomous system could reveal different characteristics. To demonstrate the significance of
gamification from the user’s perspective, conducting rigorous comparative experiments in the
futurewouldbenecessary.
Inaddition,wehavenotconductedadetailedcomparisontotesttheeffectsofourdesignchoices,
suchascooperativegamedesignandtheintroductionofphysicalelements.Itmustbenotedthat
itisnotatrivialtasktoimplementanonlineversionofthisgame,consideringthedifficultyof
interpretingandassociatingthegazedirectionofaremotepartnerthroughawebcam.However,
a fully software-based implementation could be highly beneficial from the perspective of data
collection.IncludingtheuseofcommonmeasuressuchasIMI[15]andPXI[1],adetailedanalysis
ofthegamingexperienceisalsoanimportantissueforfutureresearch.
6.3.2 Game Design. Our gamified procedure takes more time to capture a single image than
standarddatacollectionmethods.Ashighlightedinpreviousresearch[7],thiscanbeviewedas
asomewhatunavoidableoutcomeofgamification,althoughthesignificanceofefficiencyshould
not be overlooked. As discussed above, designing game experiences that can withstand long-
termpublicinstallationsisanimportantchallengetoovercomethisshortcoming.Bycarefully
addressingprivacyconcerns,itwillbenecessarytoactivelyconsiderusingthegamifiedsystem
throughexhibitionsandotherwaysofpresentation.Forexample,byintroducingavisualization
of how the performance of the gaze estimation model has improved with the participation of
playerssofar,itcouldbepossibletodevisewaystoenhanceengagementfromamorelong-term
perspective.
Consideringthewidevariationindifficultyevaluationamongplayers,itisimportanttointroduce
amechanismthatadaptivelychangesthedifficultylevel.Inthisgame,itcouldbepossibletoaddress
thisbyvaryingthenumberofhintlettersbasedontherequiredtimeandthecorrectnessofthe
previousanswer.Asamoreadvancedapproach,eyemovementsestimatedbytheappearance-based
methodmighthelppredictthecognitiveloadfortheplayer.Incorporatingeyetrackingdataintothe
gamingexperienceinvariouswaysalsoholdssignificantpotentialfromatechnicalcommunication
standpoint.
Anotherdesignchallengeinvolvestheinherentbiasineachcharacter’sappearancefrequency
whenusingregularwords.Anapproachmightbedistributingfrequently-appearingcharacters
evenly,butthiscouldmakeitharderforplayerstofindcharacters.Althoughpreparingasuitable
worddictionarymightberequired,refiningthebackendalgorithmcouldbeapotentialsolutionto
decreaseletterbias.Also,usingtechnologiessuchastransparentdisplays,whichcandynamically
changecontent,couldbeanidealsolution.Thesetechnologiesmaynotonlysolvetheissueof
characterfrequencybiasbutalsoenhancetheflexibilityandadaptabilityofthegamedesign.
6.3.3 TechnicalUnderstandingandCommunication. Althoughusersurveysresultedinpositive
feedback,onecriticaltaskistofostertherightunderstandingofMLandgazeestimationthrough
thegameexperience.Explainingtheconceptofappearance-basedgazeestimationtonon-expert
audiencesisnotalwayseasy.Consequently,graspinghowthedatacollectedfromthisgameaidsin
modeltrainingcanbechallenging.Specifically,whenusedinlong-terminstallations,itisessential
toclearlyshowplayershowtheperformanceofthegazeestimationmodelcouldbeimproved
duetodatacollectedfrompastgameexperiences.Thisisakeyareaweareconsideringforfuture
developments.Itiscrucialtoallowplayerstoseetheconcreteresultsoftheirparticipationand
theirdata’scontributiontothemodeltraining.18 MingtaoYue,TomomiSayuda,MilesPennington,andYusukeSugano
Moreover,itisessentialtoconsiderusingthisgametostimulateparticipantdiscussionsand
increasetheirengagementwiththetechnology,especiallywhenpairedwithactivitieslikework-
shops.Theshortdurationofthegameexperiencealonemaynotsufficientlycoverdeepdiscussions
aboutethicalissuesrelatedtofacialimagesandgazeestimation.However,thepotentialbenefitof
fosteringpositiveimpressionsthroughthesegameexperiencescouldwarrantcontinuedexploration
inthefuture.WebelievethestrategyofencouragingunderstandingandconversationaboutML
technologybyturningthedatacollectionprocessintoagamecouldextendbeyondgazeestimation.
ThisapproachmightalsopromotetechnicalliteracyinotherAIfieldsbyprovidinganenjoyable
platformfordialogueandlearning.
7 CONCLUSION
Thisresearchproposedanovelmethodtotransformthedatacollectionprocessforappearance-
basedgazeestimationintoanengaginggameforthegeneralpublic.Thegame,uniquelycomple-
mentedbyaphysicaldevice,isdesignedfortwoseatedplayerswhocommunicatewordsusing
their gaze through a transparent letter board. In this scenario, the facial images of the player
conveyingthewordscanberecorded.Thisallowsthecollectionoftrainingdata,pairedwiththe
gazeposition,forgazeestimation.Thesystemisintendedtostimulatetechnologicalinterestinas
manyindividualsaspossibleandisdesignedtoenablepeopletocomprehenditaccuratelyandto
participateindatacollectionwhilehavingfun.Thisdepictsablendingofinteractiveengagement
andscientificdatacollection,openingnewpossibilitiesforimplementingdatacollectionapproaches
inappliedMLresearch.
Wevalidatedtheeffectivenessoftheproposedsystembyinstallingitinaspaceaccessibleto
thepublic.Thedataaccuracywasevaluatedusingthegazepositionsobtainedfromawearable
eyetracker.Whiletheaccuracywascomparabletocustomarydatacollection,itwasobservedthat
extremeoutlierscouldoccur.Theaccuracyofgazespredictedbythepre-trainedgazeestimation
modelwasinferiortothatonstandarddatasets,suggestingthatthedatacollectedwithoursystem
is valuable for evaluation and training. The feedback collected from participants’ surveys was
generallypositive,confirmingthatweachievedourgoalofengagingpeopleindatacollectionwhile
fosteringinterestinthetechnology.Lookingforward,weplantoexploreotherusagescenarios,
includinglong-termexhibitionsorpairingwithworkshops,andconsiderexpandingourapproach
beyondgazeestimation.
ACKNOWLEDGMENTS
This work was partly supported by the UTokyo Ushioda Foundation, the Foundation for the
PromotionofIndustrialScience,andJSTCRESTGrantNumberJPMJCR19F2,Japan.
REFERENCES
[1] VeroVandenAbeele,KattaSpiel,LennartNacke,DanielJohnson,andKathrinGerling.2020. Developmentand
validationoftheplayerexperienceinventory:Ascaletomeasureplayerexperiencesattheleveloffunctionaland
psychosocialconsequences.InternationalJournalofHuman-ComputerStudies135(2020),102370.
[2] MihaiBâce,VincentBecker,ChenyangWang,andAndreasBulling.2020.Combininggazeestimationandopticalflow
forpursuitsinteraction.InACMSymposiumonEyeTrackingResearchandApplications.1–10.
[3] SaskiaBakker,ElisevandenHoven,andBerryEggen.2015.Peripheralinteraction:characteristicsandconsiderations.
PersonalandUbiquitousComputing19,1(2015),239–254.
[4] ValentinBazarevsky,YuryKartynnik,AndreyVakunov,KarthikRaveendran,andMatthiasGrundmann.2019.Blazeface:
Sub-millisecondneuralfacedetectiononmobilegpus.InProc.CVPRWorkshoponComputerVisionforAugmentedand
VirtualReality.
[5] AnneBowser,DerekHansen,YurongHe,CarolBoston,MatthewReid,LoganGunnell,andJenniferPreece.2013.
Usinggamificationtoinspirenewcitizensciencevolunteers.InProceedingsofthefirstinternationalconferenceon
gamefuldesign,research,andapplications.18–25.GamifiedDataCollectionforAppearance-BasedGazeEstimation 19
[6] GaryBradski.2000. TheopenCVlibrary. Dr.Dobb’sJournal:SoftwareToolsfortheProfessionalProgrammer25,11
(2000),120–123.
[7] DanielleBragg,NaomiCaselli,JohnWGallagher,MiriamGoldberg,CourtneyJOka,andWilliamThies.2021.ASL
SeaBattle:GamifyingSignLanguageDataCollection.InProceedingsofthe2021CHIconferenceonhumanfactorsin
computingsystems.1–13.
[8] DiegoBuchingerandMarcelodaSilvaHounsell.2018.Guidelinesfordesigningandusingcollaborative-competitive
seriousgames.Computers&Education118(2018),133–149.
[9] AdrianBulatandGeorgiosTzimiropoulos.2017.Howfararewefromsolvingthe2d&3dfacealignmentproblem?(and
adatasetof230,0003dfaciallandmarks).InProceedingsoftheIEEEinternationalconferenceoncomputervision.
1021–1030.
[10] YihuaCheng,YiweiBao,andFengLu.2022.Puregaze:Purifyinggazefeatureforgeneralizablegazeestimation.In
ProceedingsoftheAAAIConferenceonArtificialIntelligence,Vol.36.436–443.
[11] YihuaChengandFengLu.2022.Gazeestimationusingtransformer.In202226thInternationalConferenceonPattern
Recognition(ICPR).IEEE,3341–3347.
[12] YihuaCheng,HaofeiWang,YiweiBao,andFengLu.2021.Appearance-basedgazeestimationwithdeeplearning:A
reviewandbenchmark.arXivpreprintarXiv:2104.12668(2021).
[13] EunjiChong,KathaChanda,ZhefanYe,AudreySoutherland,NatanielRuiz,RebeccaMJones,AgataRozga,and
JamesMRehg.2017. Detectinggazetowardseyesinnaturalsocialinteractionsanditsuseinchildassessment.
ProceedingsoftheACMonInteractive,Mobile,WearableandUbiquitousTechnologies1,3(2017),1–20.
[14] SethCooper,FirasKhatib,AdrienTreuille,JanosBarbero,JeehyungLee,MichaelBeenen,AndrewLeaver-Fay,David
Baker,ZoranPopović,andFolditPlayers.2010.PredictingProteinStructureswithaMultiplayerOnlineGame.Nature
466,7307(Aug.2010),756–760.
[15] EdwardLDeciandRichardMRyan.1985.Intrinsicmotivationandself-determinationinhumanbehavior.Springer.
[16] AnsgarEDeppingandReganLMandryk.2017.Cooperationandinterdependence:Howmultiplayergamesincrease
socialcloseness.InProceedingsoftheAnnualSymposiumoncomputer-humaninteractioninplay.449–461.
[17] KatharinaEmmerichandMaicMasuch.2013. Helpingfriendsorfightingfoes:Theinfluenceofcollaborationand
competitiononplayerexperience.FDG1(2013),150–157.
[18] TobiasFischer,HyungJinChang,andYiannisDemiris.2018. Rt-gene:Real-timeeyegazeestimationinnatural
environments.InProceedingsoftheEuropeanconferenceoncomputervision(ECCV).334–352.
[19] KennethAlbertoFunesMora,FlorentMonay,andJean-MarcOdobez.2014.Eyediap:Adatabaseforthedevelopment
andevaluationofgazeestimationalgorithmsfromrgbandrgb-dcameras.InProceedingsofthesymposiumoneye
trackingresearchandapplications.255–258.
[20] FernandoGarcia-Sanjuan,JavierJaen,andVicenteNacher.2017. Tangibot:atangible-mediatedrobottosupport
cognitivegamesforageingpeople—ausabilitystudy.PervasiveandMobileComputing34(2017),91–105.
[21] SergioGarrido-Jurado,RafaelMuñoz-Salinas,FranciscoJoséMadrid-Cuevas,andManuelJesúsMarín-Jiménez.2014.
Automaticgenerationanddetectionofhighlyreliablefiducialmarkersunderocclusion. PatternRecognition47,6
(2014),2280–2292.
[22] ShreyaGhosh,AbhinavDhall,MunawarHayat,JarrodKnibbe,andQiangJi.2021.Automaticgazeanalysis:Asurvey
ofdeeplearningbasedapproaches.arXivpreprintarXiv:2108.05479(2021).
[23] SuparaGrudpan,JakobHauge,JannickeBaalsrudHauge,andRainerMalaka.2021.TransformingGamePremise:An
ApproachforDevelopingCooperativeSeriousGames.InGamesandLearningAlliance:10thInternationalConference,
GALA2021,LaSpezia,Italy,December1–2,2021,Proceedings10.Springer,208–219.
[24] SeverinHackerandLuisVonAhn.2009.Matchin:elicitinguserpreferenceswithanonlinegame.InProceedingsofthe
SIGCHIConferenceonHumanFactorsinComputingSystems.1207–1216.
[25] JuhoHamari,JonnaKoivisto,andHarriSarsa.2014.Doesgamificationwork?–aliteraturereviewofempiricalstudies
ongamification.In201447thHawaiiinternationalconferenceonsystemsciences.Ieee,3025–3034.
[26] DanWitznerHansenandQiangJi.2009. Intheeyeofthebeholder:Asurveyofmodelsforeyesandgaze. IEEE
transactionsonpatternanalysisandmachineintelligence32,3(2009),478–500.
[27] JohnHarris,MarkHancock,andStaceyDScott.2016.Leveragingasymmetriesinmultiplayergames:Investigating
designelementsofinterdependentplay.InProceedingsofthe2016AnnualSymposiumoncomputer-humaninteraction
inplay.350–361.
[28] YoichiroHisadome,TianyiWu,JiaweiQin,andYusukeSugano.2023. Rotation-ConstrainedCross-ViewFeature
FusionforMulti-ViewAppearance-basedGazeEstimation.arXivpreprintarXiv:2305.12704(2023).
[29] SinhHuynh,RajeshKrishnaBalan,andJeongGilKo.2021.imon:Appearance-basedgazetrackingsystemonmobile
devices.ProceedingsoftheACMonInteractive,Mobile,WearableandUbiquitousTechnologies5,4(2021),1–26.
[30] SwatiJindalandRobertoManduchi.2023.Contrastiverepresentationlearningforgazeestimation.InAnnualConference
onNeuralInformationProcessingSystems.PMLR,37–49.20 MingtaoYue,TomomiSayuda,MilesPennington,andYusukeSugano
[31] PetrKellnhofer,AdriaRecasens,SimonStent,WojciechMatusik,andAntonioTorralba.2019.Gaze360:Physically
unconstrainedgazeestimationinthewild.InProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision.
6912–6921.
[32] FirasKhatib,SethCooper,MichaelD.Tyka,KefanXu,IlyaMakedon,ZoranPopović,DavidBaker,andFolditPlayers.
2011.AlgorithmDiscoverybyProteinFoldingGamePlayers.ProceedingsoftheNationalAcademyofSciences108,47
(Nov.2011),18949–18953.
[33] KyleKrafka,AdityaKhosla,PetrKellnhofer,HariniKannan,SuchendraBhandarkar,WojciechMatusik,andAntonio
Torralba.2016. Eyetrackingforeveryone.InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition.2176–2184.
[34] EdithLMLaw,LuisVonAhn,RogerBDannenberg,andMikeCrawford.2007. TagATune:AGameforMusicand
SoundAnnotation..InISMIR,Vol.3.2.
[35] IsackLee,Jun-SeokYun,HeeHyeonKim,YoungjuNa,andSeokBongYoo.2022.LatentGaze:Cross-DomainGaze
EstimationthroughGaze-AwareAnalyticLatentCodeManipulation.InProceedingsoftheAsianConferenceonComputer
Vision.3379–3395.
[36] ChrisJ.Lintott,KevinSchawinski,AnžeSlosar,KateLand,StevenBamford,DanielThomas,M.JordanRaddick,
RobertC.Nichol,AlexSzalay,DanAndreescu,PhilMurray,andJanVandenberg.2008.GalaxyZoo:Morphologies
DerivedfromVisualInspectionofGalaxiesfromtheSloanDigitalSkySurvey.MonthlyNoticesoftheRoyalAstronomical
Society389,3(2008),1179–1189.
[37] DavidGLoweetal.1991.Fittingparameterizedthree-dimensionalmodelstoimages.IEEEtransactionsonpattern
analysisandmachineintelligence13,5(1991),441–450.
[38] FengLu,YusukeSugano,TakahiroOkabe,andYoichiSato.2014.Adaptivelinearregressionforappearance-based
gazeestimation.IEEEtransactionsonpatternanalysisandmachineintelligence36,10(2014),2033–2046.
[39] PäiviMajarantaandAndreasBulling.2014.Eyetrackingandeye-basedhuman–computerinteraction.InAdvancesin
physiologicalcomputing.Springer,39–65.
[40] ThomasW.Malone.1980.WhatMakesThingsFuntoLearn?HeuristicsforDesigningInstructionalComputerGames.
InProceedingsofthe3rdACMSIGSMALLSymposiumandtheFirstSIGPCSymposiumonSmallSystems-SIGSMALL’80.
162–169.
[41] BenediktMorschheuser,JuhoHamari,JonnaKoivisto,andAlexanderMaedche.2017. Gamifiedcrowdsourcing:
Conceptualization,literaturereview,andfutureagenda.InternationalJournalofHuman-ComputerStudies106(2017),
26–43.
[42] SeonwookPark,ShaliniDeMello,PavloMolchanov,UmarIqbal,OtmarHilliges,andJanKautz.2019. Few-shot
adaptivegazeestimation.InProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision.9368–9377.
[43] SeonwookPark,AdrianSpurr,andOtmarHilliges.2018.Deeppictorialgazeestimation.InProceedingsoftheEuropean
conferenceoncomputervision(ECCV).721–738.
[44] Jiawei Qin, Takuru Shimoyama, and Yusuke Sugano. 2022. Learning-by-Novel-View-Synthesis for Full-Face
Appearance-Based3DGazeEstimation.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
RecognitionWorkshops.4981–4991.
[45] JiaweiQin,TakuruShimoyama,XucongZhang,andYusukeSugano.2023.Domain-AdaptiveFull-FaceGazeEstimation
viaNovel-View-SynthesisandFeatureDisentanglement.arXivpreprintarXiv:2305.16140(2023).
[46] RuiRodrigues,JoaoPBarreto,andUrbanoNunes.2010. Cameraposeestimationusingimagesofplanarmirror
reflections.InProc.11thEuropeanConferenceonComputerVision.Springer,382–395.
[47] MelissaRoemmele,HaleyArcher-McClellan,andAndrewSGordon.2014.Trianglecharades:Adata-collectiongame
forrecognizingactionsinmotiontrajectories.InProceedingsofthe19thinternationalconferenceonIntelligentUser
Interfaces.209–214.
[48] KatieSeabornandDeborahIFels.2015. Gamificationintheoryandaction:Asurvey. InternationalJournalof
human-computerstudies74(2015),14–31.
[49] BrianASmith,QiYin,StevenKFeiner,andShreeKNayar.2013. Gazelocking:passiveeyecontactdetectionfor
human-objectinteraction.InProceedingsofthe26thannualACMsymposiumonUserinterfacesoftwareandtechnology.
271–280.
[50] YusukeSugano,YasuyukiMatsushita,andYoichiSato.2012.Appearance-basedgazeestimationusingvisualsaliency.
IEEEtransactionsonpatternanalysisandmachineintelligence35,2(2012),329–341.
[51] YusukeSugano,YasuyukiMatsushita,andYoichiSato.2014. Learning-by-synthesisforappearance-based3dgaze
estimation.InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.1821–1828.
[52] YusukeSugano,XucongZhang,andAndreasBulling.2016.Aggregaze:Collectiveestimationofaudienceattentionon
publicdisplays.InProceedingsofthe29thAnnualSymposiumonUserInterfaceSoftwareandTechnology.821–831.
[53] Kar-HanTan,DavidJKriegman,andNarendraAhuja.2002.Appearance-basedeyegazeestimation.InSixthIEEE
WorkshoponApplicationsofComputerVision,2002.(WACV2002).Proceedings.IEEE,191–195.GamifiedDataCollectionforAppearance-BasedGazeEstimation 21
[54] DouglasTurnbull,RuoranLiu,LukeBarrington,andGertRGLanckriet.2007.AGame-BasedApproachforCollecting
SemanticAnnotationsofMusic..InISMIR,Vol.7.535–538.
[55] LuisVonAhnandLauraDabbish.2004.Labelingimageswithacomputergame.InProceedingsoftheSIGCHIconference
onHumanfactorsincomputingsystems.319–326.
[56] LuisVonAhn,ShiryGinosar,MihirKedia,RuoranLiu,andManuelBlum.2006.Improvingaccessibilityoftheweb
withacomputergame.InProceedingsoftheSIGCHIconferenceonHumanFactorsincomputingsystems.79–82.
[57] LuisVonAhn,MihirKedia,andManuelBlum.2006.Verbosity:agameforcollectingcommon-sensefacts.InProceedings
oftheSIGCHIconferenceonHumanFactorsincomputingsystems.75–78.
[58] YaomingWang,YangzhouJiang,JinLi,BingbingNi,WenruiDai,ChenglinLi,HongkaiXiong,andTengLi.2022.
Contrastiveregressionfordomainadaptationongazeestimation.InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition.19376–19385.
[59] KeithWattandTamarahSmith.2021.Research-BasedGameDesignforSeriousGames.Simulation&Gaming52,5
(2021),601–613.
[60] ErrollWood,TadasBaltrušaitis,Louis-PhilippeMorency,PeterRobinson,andAndreasBulling.2016.A3Dmorphable
modeloftheeyeregion.InProceedingsofthe37thAnnualConferenceoftheEuropeanAssociationforComputerGraphics:
Posters.35–36.
[61] ErrollWood,TadasBaltrušaitis,Louis-PhilippeMorency,PeterRobinson,andAndreasBulling.2016. Learningan
appearance-basedgazeestimatorfromonemillionsynthesisedimages.InProceedingsoftheNinthBiennialACM
SymposiumonEyeTrackingResearch&Applications.131–138.
[62] LesleyXie,AlissaNAntle,andNimaMotamedi.2008.Aretangiblesmorefun?Comparingchildren’senjoymentand
engagementusingphysical,graphicalandtangibleuserinterfaces.InProceedingsofthe2ndinternationalconferenceon
Tangibleandembeddedinteraction.191–198.
[63] YeYuan,JanCao,RuotongWang,andSvetlanaYarosh.2021. Tabletopgamesintheageofremotecollaboration:
Designopportunitiesforasociallyconnectedgameexperience.InProceedingsofthe2021CHIConferenceonHuman
FactorsinComputingSystems.1–14.
[64] DebraJZeitlin,GaryMAbrams,andBKShah.1995.Useofaugmentative/alternativecommunicationinpatientswith
amyotrophiclateralsclerosis.JournalofNeurologicRehabilitation9,4(1995),217–220.
[65] XucongZhang,SeonwookPark,ThaboBeeler,DerekBradley,SiyuTang,andOtmarHilliges.2020.ETH-Xgaze:A
largescaledatasetforgazeestimationunderextremeheadposeandgazevariation.InProceedingsoftheEuropean
ConferenceonComputerVision.Springer,365–381.
[66] XucongZhang,YusukeSugano,andAndreasBulling.2017.Everydayeyecontactdetectionusingunsupervisedgaze
targetdiscovery.InProceedingsofthe30thannualACMsymposiumonuserinterfacesoftwareandtechnology.193–203.
[67] XucongZhang,YusukeSugano,andAndreasBulling.2018.Revisitingdatanormalizationforappearance-basedgaze
estimation.InProceedingsofthe2018ACMsymposiumoneyetrackingresearch&applications.1–9.
[68] XucongZhang,YusukeSugano,andAndreasBulling.2019.Evaluationofappearance-basedmethodsandimplications
forgaze-basedapplications.InProceedingsofthe2019CHIconferenceonhumanfactorsincomputingsystems.1–13.
[69] XucongZhang,YusukeSugano,MarioFritz,andAndreasBulling.2017. It’swrittenalloveryourface:Full-face
appearance-basedgazeestimation.InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition
workshops.51–60.
[70] XucongZhang,YusukeSugano,MarioFritz,andAndreasBulling.2017. Mpiigaze:Real-worlddatasetanddeep
appearance-basedgazeestimation.IEEEtransactionsonpatternanalysisandmachineintelligence41,1(2017),162–175.