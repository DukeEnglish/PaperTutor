Thispaperhasbeenacceptedforpublicationatthe
IEEEInternationalConferenceon3DVision(3DV),Davos,2024. ©IEEE
Range-Agnostic Multi-View Depth Estimation with Keyframe Selection
AndreaConti MatteoPoggi , ValerioCambareri StefanoMattoccia ,
† †‡ ∗ †‡
†DepartmentofComputerScienceandEngineering ∗SonyDepthsensingSolutions
‡AdvancedResearchCenteronElectronicSystem(ARCES) Brussels,Belgium
UniversityofBologna,Italy Brussels,Belgium
SourceViews ReferenceView Prediction GroundTruth
Prediction GroundTruth
Figure1. DepthEstimationand3DreconstructionwithRAMDepthonBlended[39]. Ontop: givenfiveimagesofthesamescene,
ourframeworkcanestimateaccuratedepthmapsthroughmulti-viewgeometrywithoutrequiringanyknowledgeaboutthereferenceview
depthrange.Atthebottom:thepointcloudobtainedfromthepredictionofthenetworkandtherespectiveground-truth.
Abstract highlydetailedshapereconstructiontoplacedigitalobjects
in real environments, historical preservation models works
Methods for 3D reconstruction from posed frames re-
of art digitally for further scientific analysis or public pre-
quire prior knowledge about the scene metric range, usu-
sentation, robotics and autonomous driving require depth
allytorecovermatchingcuesalongtheepipolarlinesand
estimation for navigation and planning. Active 3D sens-
narrow the search range. However, such prior might not
ing devices are typically preferred for high detail: LiDAR
bedirectlyavailableorestimatedinaccuratelyinrealsce-
(Light Imaging Detection and Ranging) and ToF (Time
narios – e.g., outdoor 3D reconstruction from video se-
of Flight) sensors can scan the scene actively with modu-
quences – therefore heavily hampering performance. In
latedlaserillumination,whilestructuredlightscannersinfer
this paper, we focus on multi-view depth estimation with-
scenestructurebyprojectingaknownpatternandcomput-
outrequiringpriorknowledgeaboutthemetricrangeofthe
ingitsdeformationonsurfaces. Comparedwithsuchtech-
scenebyproposingRAMDepth, anefficientandpurely2D
nologies, passive sensing from standard RGB cameras by
frameworkthatreversesthedepthestimationandmatching
triangulation has many advantages. Indeed, RGB cameras
stepsorder. Moreover,wedemonstratethecapabilityofour
are energy efficient, compact in size, and may operate in
frameworktoproviderichinsightsaboutthequalityofthe
various conditions. Among passive approaches, stereo vi-
viewsusedforprediction. Additionalmaterialcanbefound
sionleveragestwocalibratedcamerastorestrictthematch-
onourprojectpage.
ingproblemtoa1Dsearchspace,yetrequirestwocameras
1.Introduction inaconstrainedsetting–i.e.,beingnearlycoplanartoallow
forsimplercalibrationandrectification. Ontheotherhand,
Accurate 3D reconstruction is of profound interest in var-
asinglemonocularRGBcamerainmotionisthemostflex-
ious fields: mixed reality and 3D content creation require
4202
naJ
52
]VC.sc[
1v10441.1042:viXraible(aswellaschallenging)approach. viewstothosebeingstrictlynecessarytoestimateaccurate
Traditionally, multi-view 3D reconstruction techniques depth,althoughthislatteraspecthasneverbeenexplored.
canbeclassifiedinthefollowingbroadfamilies:voxel,sur- Prompted by the previous observations, we propose a
face evolution, patch, or depth-based [9, 12, 26, 31, 37]. novel framework that is (i) free from prior knowledge of
Despite being tackled with hand-crafted algorithms at first the depth range from which one samples hypotheses, and
[2, 8], most state-of-the-art methods leverage depth-based (ii) capable of distinguishing the most meaningful source
deep learning architectures. These frameworks process a framesamongmany.WewillshowthatourRangeAgnostic
set of source views and a reference view and yield an es- Multi-View framework (RAMDepth) enjoys the following
timated depth map for the latter. Most deep architectures properties:
tacklethistaskby(i)extractingdeepfeaturesfromtheim- • Scene Depth Range Invariance. Our approach is com-
ages,(ii)buildingacostvolumesampledovertheepipolar pletelyindependentofanyinputdepthrangeassumption
linesthroughasetofdepthhypothesesusingdifferentiable andthusapplicableeverywhereasetofimagesalongwith
homography, and (iii) predicting depth with a typically theirposeisprovided. Insteadofsamplingfeaturesalong
3D convolutional module. The depth estimation pipeline epipolar lines according to a fixed set of depth hypothe-
sketchedsofariseffectivebutaffectedbysomelimitations. sesandthenpredictingdepth,wereversethemechanism:
our framework iteratively updates a depth estimate dy-
First,accordingtostep(ii),priorknowledgeofthescene
namically moving along epipolar lines according to this
depthrangeisstrictlyrequiredtosampledepthhypotheses
lattertocomputecorrelationscores. Inthisway,fixingan
and build a meaningful cost volume [25]. Indeed, on the
apriorisetofdepthhypothesesisnotrequired.
one hand, sampling hypotheses out of an underestimated
• Keyframes Ranking. Our approach not only estimates
rangewouldmakethenetworkunabletopredictdepthval-
depth, but also provides insights about the match qual-
uesinout-of-rangeareas.Ontheotherhand,overestimating
ity of each source view and its contribution to the fi-
thedepthrangewillresultinsamplingcoarserhypotheses,
nalprediction, allowingwithinasingleinferencestepto
thusreducingthefine-grainedaccuracyofestimateddepth
rankinputsourceviewsaccordingtotheiractualmatch-
maps. Unfortunately, such knowledge cannot be straight-
ingagainstthereferenceview.
forwardlyretrievedinrealscenarios. Whenrawimagesare
provided,cameraposescanbeobtainedthroughtraditional To assess the performance of RAMDepth, we consid-
Structure-from-Motion(SfM)algorithms[24],possiblyes- ered different challenging benchmarks with heterogeneous
timating the depth range as well. However, such a range specifics. OnBlended[39]andTartanAir[34],wedemon-
mightbeerroneouslyestimatedduetoanumberofreasons strate the capability of our framework to seamlessly esti-
– e.g., untextured regions, visual occlusion, or poor field mate accurate depth in diverse scenes such as large-scale
of view (FoV) overlap. We point out that many applica- outdoorenvironments, top-viewbuildings, andindoorsce-
tionsinwhichcameraposesareknownbyothermeansexist narios. Indeed,ontheonehand,Blended[39]ischaracter-
(e.g., as often occurs in robotic applications [11]) and that izedbysignificantposechanges,occlusions,andlargeFoV
modernmobileplatformsprovideposeinformationthrough overlap. On the other hand, TartanAir [34] provides video
dedicatedinertialsensors. streamscharacterizedbysmall,unpredictableposechanges,
where the depth range of each frame can change abruptly.
Second, we argue that source frames must be carefully
Moreover, on UnrealStereo4K [30] we assess the general-
selectedtoallowproperdepthestimation, withasetofre-
ization capability of RAMDepth to video streams and the
quirements such as enough distance between optical cen-
possibility of applying it to the stereo setup. To conclude,
terstoallowmeaningfuldisplacements,aswellassufficient
wevalidateourperformanceonDTU[11],wherethedepth
cross-viewoverlaptoallowmatching. Moreover,thequal-
range is fixed. Along with this validation, we demonstrate
ityoftheviewsmustbeconsideredaswell: abruptlightor
the peculiar capabilities of our approach through specifi-
colorchanges,movingobjects,orscene-specificocclusions
cally designed experiments. Fig. 1 shows the outcome of
mustbetakenintoaccounttomaximizematches. Unfortu-
RAMDepthonBlended[39].
nately,alltheseaspectscannotbeevaluatedbysimplycon-
sideringposesimilaritysincemanyofthemrequireananal-
2.RelatedWork
ysisoftheimagesthemselves. Abetterapproachcouldbe
toapplySfMalgorithmsandanalyzethedistribution,qual- We cover the most relevant research topics related to our
ity,andamountofkeypointmatchesacrossdifferentviews, proposal, by reviewing prior frameworks for estimating
which would require additional offline processing. We ar- depth from multiple posed views. Depending on the set-
guethatdistinguishingmeaningfulmatchesfromunreliable tings they have been evaluated, we broadly classify them
oneswouldeasethedepthestimationtask–ashighlighted intotwocategories.
by prior works [15, 40] – as well as possibly reduce the Object-centric Reconstruction. This computer vision
computational overhead by limiting the number of source task aims at reconstructing a 3D model – often a pointcloud – of an arbitrarily large object by means of 2D im- ing the scene at once and requiring proper selection of the
agescapturedfrommultipleviewpoints. Thistaskassumes frames to be integrated into the TSDF volume. Moreover,
a controlled environment where the object depth range is theyallrequirethescenemetricrangetoinitializethevoxel
known and the viewpoints are object-centric, i.e., the ob- grid. On the other hand, depth-based methods solve this
jectisoftenappearingcenteredintheimagesandfullycov- task by composing multiple depth maps predicted from a
eredbytheviewpoints. Traditionalmethodsreconstructthe subset of source views of the whole scene [19]. Notably,
3Dstructurethroughimagepointstriangulationandmanu- [22]proposedmeta-dataintegrationinthecostvolumeand
allyengineeredfeatures. Thisformulationisessentiallyan a2Ddepthestimationmodule.
optimization procedure based on photometric consistency However, all these approaches work in an extremely
acrossviews,withshapeand3Dstructurepriorsbeingex- controlled (usually indoor) environment, where the scene
ploitedasregularization[2,8,9,23]. Todatedeeplearning depth range can be roughly estimated – usually, up to a
depth-based methods have taken the lead in this field, au- few meters. Such an assumption prevents a na¨ıve exten-
tomating feature extraction and matching. One of the first sion to less constrained environments, either indoor (e.g.,
approachesinthisdirectionisMVSNet[37],whichbuildsa large, industrial factories) or outdoor. In contrast, we de-
3Dcostvolumebymatchingpixelsalongtheepipolarlines. sign a lightweight 2D convolutional framework applicable
Suchvolumecontains,foreachpixelinthereferenceview, to awide range ofscenarios. We do notaim at recovering
the variance between features sampled across the different thewhole3Dreconstructionatonce,butinstead,wefocus
sourceimagesemployingdifferentiablehomography.Then, onaccuratedepthestimationsincethislattercoversawider
a3Dconvolutionalnetworkisappliedasregularization,and rangeofapplicationsonitsownaswell–andfromwhicha
finally,a(soft)arg-maxoperatorisappliedtoextractdepth, 3Dreconstructioncanbeobtainedifrequired[19,22].
lately composed to build a global point cloud. Follow-up
works mostly focused on cutting down memory require- 3.ProposedFramework
ments: [38] leverages 3D regularization with 2D recurrent
This paper proposes RAMDepth, a deep framework to
networks [5], while several improvements [4, 10, 36] fol-
tackle 3D reconstruction from multiple posed views lever-
low a multi-scale approach with coarse-to-fine inference.
aging 2D convolutional layers only, and an iterative opti-
Other extensions concern reasoning about pair-wise visi-
mizationprocedureaimedatrefininganinternaldepthmap.
bility [15, 40], deploying recurrent approaches [16, 33] or
Our design builds upon the following principle: given the
leveraging NeRF-inspired [17] optimization [3, 35]. De-
reference view, matches over an arbitrary source view can
spiteallthesemethodsbeingdesignedtopredictdepth,they
be found given their relative pose and enough visual over-
oftenfocusontheglobal3Dpointcloudinacontrolleden-
lap. Thus, provided an initial depth map, dense matching
vironment. Ourframeworkdiffersfromsuchapproachesin
costs can be computed between the reference and source
thataprioridepthhypothesesarenotassumedatallwhen
views. Suchinformationisthenfedtoa2Dlearnedmodule
computing matching scores and epipolar geometry is ex-
to properly refine the predicted depth map. This way, un-
ploitedtoiterativelyrefineestimateddepth. Moreover, we
likeanyotherframeworkthatbuildsacostvolumerelying
do not pursue a global 3D point cloud reconstruction but
on a set of a priori depth hypotheses, RAMDepth can dy-
focusmoreonfine-grainedhigh-qualitydepthperception.
namically navigate the matching space, while storing best
Environment Reconstruction. We categorize as envi- matchesasdepthvaluesintoaninnerstate. Epipolargeom-
ronmentreconstructionallthosemethodswhichseektoper- etrycomesintoplaysinceupdatingthestoreddepthvalues
form3Dreconstructiononnavigableenvironments,suchas means moving over the epipolar lines defined by pose in-
indoorscenes. Inthiscontext,volumetric-basedanddepth- formation. This approach can be thought of as reverting
based approaches have been deployed. Volumetric-based thecommonpipelinecomposedof(i)costvolumebuilding
methodsseektodirectlypredictaglobalvolumetricrepre- and (ii) depth estimation. Moreover, we point out that the
sentation of the scene at once, usually a Truncated Signed densematchingcostscomputedbyourframework,eachex-
Distance Function (TSDF). [18] backprojects rays of deep pressingtherelationshipbetweenaspecificsourceviewand
featuresinaglobalvoxelgridandthenleveragesa3Dcon- the reference one, can be regarded as a hint of the overall
volutionalarchitecturetodirectlyregresstheTSDFvolume. matchabilitybetweentheviews,thattakesintoaccountboth
[28]improvessuchapproachbymeansof3Drecurrentlay- FoV overlap and overall image quality. We will provide
ers and a coarse-to-fine approach. Further improvements quantitativeandqualitativeevidenceofthisrespectivelyin
bymeansoftransformershavebeenproposed[1,27].Other ourexperimentsandsupplementarymaterial.
approachescombinevolumetricreasoningwithdepth-based Framework Overview. Our architecture, sketched in
reconstructioniteratively[6,21].Overall,volumetric-based Fig. 2, can be decomposed into the following modules:
methodsrequirehighcomputationalandmemoryresources (i) image features encoding, (ii) correlation sampling (iii)
duetotheintensiveusageof3Dconvolutions,reconstruct- depth optimization, and (iv) output depth decoding. StepsCoCroreslta vtoiolunmmeasp
Source View
Selectionand
Corr. Sampling
source 1 SS eo leu cr tc ie o nVi ae nw d iter 1
Corr. Sampling
< . , . > … …
< . , . > Correlationmap
Source View iter 2
reference Selectionand …
Corr. Sampling
RecurrentRefinement
(shared)
source 2 Correlationsampling R Ene cfe or de en rce Depth iter N
Feature Encoders Concatenation Decoding
(shared)
Figure 2. RAMDepth Architecture Description. Our model instantiates an initial depth map and builds a pair-wise correlation table
betweenthetargetviewandeachsourceimage(indarkandlightblue). Then,deformablesamplingisiterativelyperformedoverit,and
thedepthstateisupdatedaccordingly.Finaldepthpredictionisupsampledthroughconvexupsampling.
(ii)and(iii)areperformedmultipletimesforafixednumber found. Thus, to better guide the optimization process we
ofiterations. Thus,ourmodeloutputsasequenceofdepth computecorrelationscoresinaneighborhood (u ,v )of
i i
N
maps(Ds) gettingprogressivelymoreaccurate. qi. Specifically, suchaneighborhoodispredictedbya2D
s N
Features∈ Encoding. GivenasetofviewsIi, i [0,N] convolutionalmoduleΘ,predictingZ indexoffsetscondi-
werefertoI0asthereferenceview–i.e.,theonefo∈ rwhich tionedbythereferencefeatures ˆandeachiterationhidden
F
we predict a depth map – andIi, i [1,N]as the source state s. The Z output channels are summed to the u ,v
i i
∈ H
ones. WeforwardeachviewIi toadeepconvolutionalen- coordinatestoobtainthesamplinglocations.
codertoextractlatentfeatures i RW 8 ×H 8×F,thatwillbe
F ∈
used to compute correlation scores in the next step. These
aredepictedinshadesofblueinFig. 2andsharethesame N(u i,v i)= (u i,v i)+Θ( Fˆ u0v0, Hs) z, z ∈Z (3)
weights. Moreover, exclusively for I0, we also extract a
(cid:2) (cid:3)
Thismechanismresemblesdeformableconvolutions[7]
disentangledsetoffeaturemapstoprovidemonocularcon-
textual information ˆ RW 8 ×H 8×F, depicted in green in inthatitsamplesfromadynamicneighborhood,yetitdif-
F ∈ ferssinceitdoesnotaccomplishaproperconvolutionwith
Fig. 2. DespitetheiterativenatureofRAMDepth,features
the sampled features but instead performs correlation with
areextractedonlyonce,atbootstrap.
thefeaturessampledfromanotherview. Itisworthobserv-
Correlation Sampling. Once the reference and source
ing that since Θ is conditioned with a state that changes
views have been encoded into deep latent features, at any
iteration the current depth estimate Ds for pixel q0 = at each iteration, these offsets may change at each itera-
[u ,v ,1]T – in homogeneous coordinu a0 tv e0 s – can be used tionaccordingly. Thereferenceviewcontextpotentiallyal-
0 0
to index a specific pixel qi = [u ,v ,1]T of a source view lows to adaptively sample correlation scores in a narrower
i i
Ii asdescribedinEq. 1,accordingtocameraintrinsicand or wider region depending on the ambiguity of the refer-
enceimageitself,likeinthepresenceofobjectboundaries
extrinsicparametersK ,K andE ,E .
0 i 0 i
orlow-texturedregions.
qi =K iE iE 0−1D u0v0K 0−1q0 (1) woT rkh se oc no arr se il na gti lo en sos uam rcp el vin ieg wm ae tc aha tin mis em
.
Td he is sc ir sib aed proso blef mar
Thisprocedureleveragesepipolargeometrysincechang- whenmultiplesourceviewsareavailable. Followingexist-
ing Ds means moving over the corresponding epipolar ingapproaches,correlationfeaturescouldbeextractedfrom
u0v0
line while not being bound to a priori depth hypotheses. eachsourceviewandthenfusedtogether.However,thisap-
Then,sourceviewfeaturesaresampledaccordinglytocom- proachwouldrequiredevelopingamergingmechanismin-
puteapixel-wisecorrelationmapC u0v0uivi –showninFig. dependentofthenumberofsourceviews–e.g.,simplecon-
2inshadesofblueaccordingtotheselectedsourceview catenationwouldbeunsuitableasitfixesthenumberofin-
putchannels. Manyexistingmodelscomputefeature-level
F
variancetocombinethevolumes[37]. Instead,wepropose
= 0 i (2)
Cu0v0uivi Fu0v0fFuivif to use a different source view for each update step in our
f=1
X framework,followingasimpleround-robinapproach. This
However, this correlation map does not provide useful methodology is simple and elegant since it exploits the it-
informationonthedirectioninwhichbettermatchescanbe erativenatureofourarchitecture,itdoesnotrequirehand-craftedfusingmodules,andcanbeextendedtoanyvariable Thisapproachisfasterthanemployingadecoderandyields
numberofsourceviews. Whiledifferentschedulingstrate- muchbetterresultscomparedtousinghand-craftedupsam-
giescanbeemployed,inthispaperwelimittothesimplest plingapproaches.
one and leave their in-depth study to future developments. LossFunction. RAMDepthissupervisedbycomputing
Wedelveintofurtheranalysisonthisinthesupplementary a simple L1 loss between the ground-truth depth D and
gt
material, where we show that such an approach is also in- eachestimateddepthmap,withaweightdecayγ setto0.8
varianttothesourceviewsorder.
S
KeyframesRanking. SinceRAMDepthexploitsasin-
glesourceviewateachiteration, isrelatedtoasinglespe-
L= γS −s ||D
gt
−Ds
||1
(4)
C s=1
cificsourceview, asitcontainscorrelationscoresbetween X
deepfeaturesofthesourceandreferenceviews. Suchcor-
4.ExperimentalResults
relationgrowsasthesourceviewfeaturesarecorrectlypro-
jectedoverthereferenceview,andthuscanberegardedas
Toassesstheeffectivenessofourapproachinthemostchal-
ascoreaboutmatchingquality[13]. Itisworthmentioning
lenging environments available, we perform experiments
thatsuchascoreissusceptibletotheFoVoverlapbutalso
onBlended[39], TartanAir[34], UnrealStereo4K[30]and
moving objects, blurring, or any other factor violating the
DTU [11]. These datasets cover a wide range of applica-
multi-viewgeometryassumptionsorthattheencodingpro-
tionsofinterest–e.g. outdoormulti-viewsettings,monoc-
cedureisnotrobustagainst. Thus,itisdirectlylinkedwith
ular video sequences, stereo perception, and object-centric
thecapabilityofthenetworktoexploitsuchsourceviewsto
indoor setups. Specifically, Blended [39] provides large
improveitsprediction. Accordingly,wecanrankeachview
complex aerial views of buildings characterized by high
bytakingthelastcorrelationmapcomputedforeachsource
inter-view pose displacements, while TartanAir provides
view and averaging it over the spatial dimensions. Since
outdoorandindoormonocularvideosequenceswithsmall
the network learns to perform good matches directly from
butunpredictableposechanges. Inboth,itisdifficulttode-
depthsupervision,thereisnoneedtodirectlysupervisethis
cide the depth range a priori as it is not usually constant
outputwhichisabyproductofourapproach.
within the same scene as well between scenes. On Un-
Depth Optimization. With the components defined so realStereo4K [30], we assess the generalization capability
far, RAMDepth estimates a depth map for the reference of RAMDepth and the possibility to perform stereo depth
view iteratively. At any stage s, a shallow recurrent net- perceptionseamlessly–tofurthersupportitsstrongmatch-
work – in purple in Fig. 2 – made of a Gated Recurrent ing effectiveness. Finally, DTU [11] provides interesting
Unit processes the sampled correlation scores and refer- cues about the performance in a controlled environment,
ence features ˆ together with the current hiddC en state s where the depth range can be accurately known a priori.
F H
anddepthmapDs(i.e.,comingfromthepreviousoptimiza-
Our framework consists of 5.9M parameters, the detailed
tion stage) to output an updated hidden state s+1. Then, architecture, trainingsettingandevaluationparametersare
H
twoconvolutionallayerspredictadepthupdate∆Dsyield-
reportedinthesupplementarymaterial. Inanyexperiment,
ingarefineddepthmapDs+1 :=Ds+∆Ds. Atbootstrap, we compute the mean absolute error (MAE), root mean
D0 isinitializedtozeroandthentheaforementioneditera- squarederror(RMSE),andthepercentageofpixelshaving
tiveprocessallowsforrapidlyupdatingthedepthmapstate deptherrorlargerthanagiventhreshold(>τ).
towardsafinal,accurateprediction.Atthefirstiteration,the Blended Benchmark. The Blended dataset [39] col-
correlationscores willnotbemeaningfulfordepth, thus lects 110K images from about 500 scenes, rendered from
C
thenetworklearnstoprovideamonocularinitializationfor meshes obtained through 3D reconstruction pipelines. It
D1. Otherapproachescouldconsistofeitherrandomlyini- featureslargeoverheadviewswherethescenedepthrange
tializingD0orinsertingafurthermoduletolearnaninitial-
would be hard to be properly recovered in a real use case,
ization. The former would be inaccurate if no information but also several objects closeups. Following [20], we test
aboutthedepthrangeisassumed,thelatterisequivalentto anymethodwithfiveinputimagesonthestandardtestset,
zeroinitializationyetrequiresanextracomponent. composed of 7 heterogeneous scenes. We first evaluate
Depth Decoding. Since RAMDepth iterates at a lower RAMDepth following the protocol and metrics by [20] to
resolution,afinalupsamplingofthedepthmapstotheorig- assess the accuracy of predicted depth maps. In this ex-
inal input resolution is required. Many approaches lever- periment, each method except ours exploits the reference
ageeitherbilinearupsampling[4,10,32,37,38]oradeep viewground-truthdepthrange. ResultsarecollectedinTa-
convolutional decoder. Instead, we compute a weighting ble 1 (a). Our framework consistently produces more ac-
mask with an upsampling module – in orange in Fig. 6 curate depth maps, despite not exploiting any knowledge
– fed with the latest hidden state s+1 and the reference aboutthedepthrangeofanyscene. Wealsopointouthow
viewfeatures
ˆ,thenweperformcH
onvexupsampling[29]. RAMDepth produces much better depth maps than other
FGroundTruthDepthRange UniqueDepthRange
Method
MAE RMSE >1m >2m >3m >4m >8m MAE RMSE >1m >2m >3m >4m >8m
Yaoetal.[37] 0.6168 1.5943 0.1392 0.0731 0.0457 0.0309 0.0103 2.1115 5.3122 0.3021 0.1637 0.1194 0.0964 0.0526
Yaoetal.[38] 0.7815 1.7397 0.1864 0.1007 0.0637 0.0433 0.0141 1.2568 2.6033 0.2918 0.1464 0.0933 0.0676 0.0286
Chengetal.[4] 0.3590 1.3589 0.0704 0.0378 0.0244 0.0171 0.0064 1.6489 4.1094 0.1844 0.1235 0.1046 0.0932 0.0602
Wangetal.[32] 0.3849 1.3581 0.0749 0.0386 0.0247 0.0175 0.0067 22.420 25.026 0.6721 0.5067 0.4989 0.4956 0.4761
Guetal.[10] 0.3684 1.3449 0.0714 0.0365 0.0234 0.0165 0.0062 1.8978 4.2927 0.2341 0.1427 0.1101 0.0921 0.0597
Zhangetal.[40] 0.3318 1.2396 0.0662 0.0323 0.0197 0.0133 0.0044 1.0536 2.8939 0.1682 0.0913 0.0643 0.0508 0.0285
Sayedetal.[22] 0.5921 1.4340 0.1404 0.0584 0.0308 0.0191 0.0057 0.5921 1.4340 0.1404 0.0584 0.0308 0.0191 0.0057
Maetal.[16] 2.1666 26.934 0.0752 0.0441 0.0316 0.0247 0.0138 8.2120 55.710 0.5780 0.5400 0.4960 0.3540 1.1150
RAMDepth(ours) 0.2982 1.1724 0.0645 0.0285 0.0159 0.0102 0.0033 0.2982 1.1724 0.0645 0.2849 0.0159 0.0102 0.0033
(a) (b)
Table1. BlendedBenchmark. Wereportcomparisonswithexistingmethodsundertwosettings: (a)byprovidingfullknowledgeabout
thescenedepthtoeachmethod,(b)byassumingauniquedepthrangetocoverthewholetestset. SinceRAMDepthdoesnotexploitany
knowledgeaboutsuchrange,itsaccuracyisnotaffectedbythesetup,unlikeothers.
ReferenceView Yaoetal.[37] Wangetal.[32] Ours GroundTruth
Figure3. QualitativeresultsonBlended. Ourapproachextractsconsistentandvisuallypleasantdepthmaps,notshowinganyvisible
outliersascanbeobservedincompetitormethods.
methods,whichshowfrequentartifactsasshowninFig.3.
Depth Range Analysis. We now focus on the impor-
tanceofnotdependingonpriorknowledgeaboutthescene
depthrange. Purposely,wedesignabenchmarktailoredto
studythisspecificaspectontheBlendedtestset,giventhe
wide set of heterogeneous scenes with depth ranges vary-
ingfromafewmetersuptohundreds. InTable1(b)each
competitor relying on the depth range is fed with a global
unique depth range, computed to cover the whole dataset
Figure4.KeyframesRanking.WeplotRMSEachievedbydrop-
one. To ease the task for competitor methods we perform
pinginputviewsinrandomorder(red)oraccordingwiththerank-
the following steps: (i) we normalize the extrinsic transla- inginformationprovidedbyRAMDepth(black).
tions between the reference and the source views to have
a mean value equal to 1 and compute the corresponding
toeasenumericalstability. Nonetheless,resultsreportedin
depth scaling factor, then (ii) we compute the mean depth
Table 1 (b) emphasize how the lack of precise knowledge
on the test set using the rescaled ground-truth depth and
aboutthedepthrangeofthesceneheavilypenalizesexist-
estimate an appropriate set of depth hypotheses equal for
ingmethods,whereasRAMDepthremainsunaffected.
every sample to cover the whole dataset depth range, fi-
nally(iii)thedepthpredictionsbymodelsprocessingdepth KeyframesRanking.Toassessthequalityofthesource
hypotheses are scaled back to the original metrical range. viewsrankingproducedbyRAMDepth,weperformapecu-
This procedure acts on the scene scale only, not affecting liarexperiment: foreachsampleintheBlendedtestset,we
the performance of trained networks, except for changing rank its source frames according to the method described
the set of depth hypotheses used to build cost volumes. in Section 3. Then, we progressively decrease the number
This procedure is a precaution adopted to have closer val- of source frames provided to our framework by selecting
uesforminimumandmaximumdepthinlarge-scalescenes, them either randomly or according to our ranking. Fig. 4MonocularVideoBenchmark StereoBenchmark
Method
MAE(m) RMSE(m) >1m >2m >3m >4m >8m MAE(px) RMSE(px) >1px >2px >3px >4px
Yaoetal.[37] 5.330 8.638 0.590 0.418 0.329 0.273 0.166 9.142 16.142 0.685 0.456 0.352 0.304
Yaoetal.[38] 4.077 7.106 0.550 0.376 0.278 0.216 0.118 8.963 16.057 0.663 0.425 0.320 0.270
Chengetal.[4] 6.511 9.935 0.635 0.468 0.375 0.314 0.196 7.539 16.096 0.357 0.261 0.230 0.211
Wangetal.[32] 7.883 10.84 0.637 0.495 0.413 0.359 0.244 3.485 10.462 0.240 0.160 0.129 0.112
Guetal.[10] 6.364 9.521 0.630 0.469 0.373 0.314 0.197 18.408 68.167 0.424 0.342 0.304 0.278
Zhangetal.[40] 6.287 8.949 0.602 0.454 0.373 0.319 0.208 9.899 26.357 0.319 0.256 0.226 0.206
Sayedetal.[22] 5.460 7.951 0.743 0.566 0.439 0.350 0.168 22.323 27.022 0.979 0.959 0.944 0.924
Maetal.[16] 7.344 13.74 0.645 0.474 0.372 0.306 0.187 3.832 10.156 0.268 0.196 0.161 0.137
RAMDepth(ours) 3.773 6.876 0.514 0.353 0.264 0.201 0.101 1.837 5.7930 0.157 0.099 0.076 0.063
Lipsonetal [14] - - - - - - - 1.646 4.8090 0.139 0.089 0.069 0.057
†
(a) (b)
Table2. UnrealStereo4kBenchmark. ApplicationofourandcompetitorframeworkstoUnrealStereo4keitherselectingsourceviews
frommonocularvideosequences(a)orusingrectifiedleftandrightstereocouplesastargetandsourceviews(b). Weprocessimagesat
960 544resolution.
×
Method MAE RMSE >1m >2m >3m >4m >8m Reference Prediction GroundTruth
Yaoetal.[37] 1.887 4.457 0.278 0.183 0.138 0.110 0.056
Yaoetal.[38] 2.191 4.729 0.346 0.228 0.170 0.134 0.066
Chengetal.[4] 1.461 3.860 0.216 0.141 0.106 0.084 0.043
Wangetal.[32] 2.351 4.980 0.331 0.228 0.176 0.144 0.078
Guetal.[10] 1.582 4.017 0.230 0.150 0.113 0.090 0.047
Sayedetal.[22] 1.561 3.303 0.316 0.167 0.112 0.083 0.036
Maetal.[16] 3.405 10.20 0.322 0.211 0.163 0.134 0.077
RAMDepth(ours) 1.258 3.289 0.203 0.125 0.090 0.070 0.034
Table 3. TartanAir Benchmark. Results achieved by existing
multi-viewframeworksandoursonTartanAir[34]. Ourmethod
consistentlydemonstratesbetterperformance.
shows the results of this experiment. Selecting frames ac-
cordingtoourrankingapproachyieldsanoverallerrorthat
diverges much more slowly. Despite not being the direct
goalofthispaper,thisexperimentlaysthegroundworkfor
interestingpotentialapplicationslikeautomaticallyremov-
ing blurred, out-of-view, or non-static frames from the set
Figure 5. TartanAir Qualitatives. TartanAir provides a wide
ofsourceviews,asmayhappenonvideosequences. rangeofcomplexenvironments,weprovideafewexamplesalong
UnrealStereo4k Benchmark. The UnrealStereo4K withthepredictionsbyRAMDepth.
dataset [30] provides synthetic stereo videos in different
challengingscenarios.Onthisdataset,weseektoassessthe tohighlighthowcloseoursolutiongetstoit,despitenotbe-
generalizationcapabilitiesofourarchitectureonmonocular ingtrainedexplicitlytodealwiththisspecificsetting–since
videosequencesand,peculiarly,atdealingwiththerectified Blendedisnotevenastereodataset. Thisevidencefurther
stereousecase. Thus,weusetheBlendedpre-trainedmod- supportsthegreatflexibilityofourapproach.
els without any kind of fine-tuning. Concerning the stereo TartanAir Benchmark. The TartanAir dataset [34] is
perception application, we use the right view as the refer- a large synthetic dataset composed of a wide spectrum of
enceviewandtheleftasthesourceone. Eveninthiscase, indoor, outdoor, aerial, and underwater scenarios recorded
weprovidetheground-truthdepthrangeininputtoallthe by a monocular camera, with different moving patterns of
methodsrequiringaprioridepthhypotheses. However,itis variable toughness. It also contains a few moving objects
worth mentioning that from a practical point of view, this likefishes,steam,andindustrialmachinesaswellashigh-
is an unrealistic assumption when dealing with left-right frequencydetailsliketreeleaves. Inthisscenario,thedepth
stereo pairs, yet necessary to deploy multi-view networks rangeofeachsingleviewishardtodefinesinceitcanem-
relyingondepthhypothesesinthissetting–exceptforours. bracehundredsofmetersinalandscapevieworafewme-
InTable2weleveragefiveconsecutiveframes(a)orasin- terswhenthecameramovesaroundawall,andthiscanhap-
glestereopair(b). Inbothcases,weachievesubstantialim- penwithinthesamesceneaswell. Thus,thisenvironment
provements over existing models, highlighting a dramatic isaperfectbenchmarkforRAMDepth. InTable3weshow
marginbyRAMDepthoverothersolutions. Asareference, the performance of our approach and existing multi-view
wealsoreporttheperformanceachievedby[14],astate-of- methods,whereeachcompetitorisfedwiththedepthrange
the-artstereonetworktrainedonavarietyofstereodatasets, from the ground-truth depth. Even though this is unfair toFigure6. BenchmarkonMemoryandTimeRequirements. WetesteachmodelinevaluationmodeonasingleNVIDIARTX3090in
32FPprecision,withinputsize768 576and5inputviews. Wemeasurepeakmemoryastheminimummemoryneededtorunamodel
×
inevaluation,timeinmillisecondsandRMSEonBlended[39].
2DMetrics 3DMetrics Convex Deformable
Method >1mm >2mm >3mm >4mm acc. compl. avg MAE >1m
Upsampling Sampling
Yaoetal.[37] 0.5550 0.3400 0.2680 0.2370 0.6350 0.3040 0.4695
Yaoetal.[38] 0.6300 0.4230 0.3290 0.2830 0.6620 0.3420 0.5020 Baseline 0.4673 0.1085
Chengetal.[4] 0.5060 0.3320 0.2770 0.2540 0.5510 0.2720 0.4115 Baseline+Deform. ✓ 0.4525 0.1046
Wangetal.[32] 0.4750 0.3100 0.2600 0.2360 0.4610 0.2980 0.3795 Baseline+Convex ✓ 0.3406 0.0756
Guetal.[10] 0.4800 0.3070 0.2570 0.2330 0.5280 0.2620 0.3950
Maetal.[16] 0.4126 0.2556 0.2029 0.1770 0.4966 0.2581 0.3773 Full ✓ ✓ 0.3197 0.0695
RAMDepth(ours) 0.3683 0.2439 0.2063 0.1884 0.4466 0.2775 0.3620 Full(Tuned) ✓ ✓ 0.2982 0.0645
Table4.DTUBenchmark.Resultsachievedbyothermulti-view Table5.AblationstudyonRAMDepth.Weassesstheimpactof
frameworksandoursonDTU.Eventhoughothermethodsaread- convexupsamplinganddeformablesamplingmodulesonBlended
vantagedbythefixeddepthrangeofthisdatasetourmethodisstill [39]. Eachablationhasbeenperformedwiththesamenumberof
comparableinperformance. trainingsteps,smallerthanthetotalusedtotrainourfinalmodel
(Tuned). Whenconvexupsamplingisnotappliedweusebilinear
our approach, not knowing anything about the prediction
upsamplinginstead.
range,westillexhibitthebestperformance. Weshowafew
qualitativeexamplesinFig. 5.
RTX3090. Thechoicetomeasurepeakmemoryisjustified
DTU Benchmark. DTU [11] is a dataset composed of bythefactthatthislatteristheminimummemoryrequired
small objects whose 3D structure is captured by means of whendeployingthesemodelsinarealapplicationandthus
a robotic arm and a structured light sensor. Due to these we believe it is the most significant metric in this sense.
specifics, it exhibits a really small and fixed depth range. InFig. 6wecanclearlyobservethatdespitebeingneither
In this context, methods relying on the scene depth range the fastest nor the lighter approach, our proposal provides
areadvantagedsincetheycanmakeuseofrobustandpre- agoodbalanceinmemoryusageandinferencetime,while
ciseinformationwhichlimitsoutliers,especiallyintexture- stillbeingthebestoneinperformance.
less areas. We pretrain on [39] following [20]. In Table
4 we show both 2D depth metrics and standard 3D met-
5.Conclusion
rics obtained with the same reconstruction pipeline from
[20] on [11]. Our approach is still competitive in both 3D Inthispaper,wehavepresentedRAMDepth,anovelframe-
pointcloudreconstructionanddepthestimation,despitebe- work for multi-view depth estimation completely inde-
ingdisadvantagedinthiscontext. Weprovideexamplesof pendent from scene depth range assumptions. We have
reconstructedpointcloudsinthesupplementarymaterial. demonstrateditsapplicabilitytodifferentenvironmentslike
Ablation study. We provide a simple ablation study monocular posed videos characterized by multiple views
about neighborhood sampling and depth decoding compo- with small baseline distances, stereo cameras, and multi-
nents, shown in Table 5. We perform such an experiment viewcameraswithlargeunconstrainedbaselinevalues. We
withaslightlysmallernumberoftrainingstepsonBlended havestudiedtheimplicationsofourapproach,highlighting
[39] with respect to our final tuned model, thus we report itscapabilitytointrospectonviewimportanceincorrelation
also the results of our final model for a better comparison. matching. Thislatterfeaturesoftensthedeployingissuesof
RAMDepthgreatlybenefitsfrombothofthesemodules. multi-viewframeworks,allowingforidentifyinglessmean-
Memory and Time Analysis. Finally, we provide ingful views and reducing inference time and memory re-
an analysis of the time and memory requirements of our quirements. However,futureresearchmayidentifysignifi-
method, comparedwithexistingapproachesinFig. 6. We cantlymoreeffectiveapproachesforthislatterpurpose.
measurepeakmemoryusage,runtime,andRMSEerrorus- Acknowledgment. We gratefully acknowledge Sony
ing 5 input views of size 768 576, on a single NVIDIA DepthsensingSolutionsSA/NVforfundingthisresearch.
×References [15] Xinjun Ma, Yue Gong, Qirui Wang, Jingwei Huang, Lei
Chen,andFanYu. Epp-mvsnet: Epipolar-assemblingbased
[1] Aljaz Bozic, Pablo Palafox, Justus Thies, Angela Dai, and
depth prediction for multi-view stereo. In Proceedings of
Matthias Nießner. TransformerFusion: Monocular RGB
the IEEE/CVF International Conference on Computer Vi-
scenereconstructionusingtransformers. NeurIPS,2021. 3
sion,pages5732–5740,2021. 2,3
[2] Neill DF Campbell, George Vogiatzis, Carlos Herna´ndez,
[16] Zeyu Ma, Zachary Teed, and Jia Deng. Multiview stereo
andRobertoCipolla. Usingmultiplehypothesestoimprove
withcascadedepipolarraft. InProceedingsoftheEuropean
depth-mapsformulti-viewstereo. InEuropeanConference
conferenceoncomputervision(ECCV),2022. 3,6,7,8
onComputerVision,pages766–779.Springer,2008. 2,3
[17] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
[3] DiChang, Aljazˇ Bozˇicˇ, TongZhang, QingsongYan, Ying-
JonathanTBarron,RaviRamamoorthi,andRenNg. Nerf:
cong Chen, Sabine Su¨sstrunk, and Matthias Nießner. Rc-
Representingscenesasneuralradiancefieldsforviewsyn-
mvsnet: unsupervisedmulti-viewstereowithneuralrender-
thesis. InEuropeanConferenceonComputerVision,pages
ing. In European Conference on Computer Vision, pages
405–421,2020. 3
665–680.Springer,2022. 3
[4] ShuoCheng,ZexiangXu,ShilinZhu,ZhuwenLi,LiErran [18] ZakMurez,TarrencevanAs,JamesBartolozzi,AyanSinha,
Li,RaviRamamoorthi,andHaoSu.Deepstereousingadap- VijayBadrinarayanan,andAndrewRabinovich.Atlas:End-
tivethinvolumerepresentationwithuncertaintyawareness. to-end 3D scene reconstruction from posed images. In
In Proceedings of the IEEE/CVF Conference on Computer ECCV,2020. 3
VisionandPatternRecognition,pages2524–2534,2020. 3, [19] Richard A Newcombe, Shahram Izadi, Otmar Hilliges,
5,6,7,8 DavidMolyneaux,DavidKim,AndrewJDavison,Pushmeet
[5] KyunghyunCho,BartMerrienboer,CaglarGulcehre,Fethi Kohi,JamieShotton,SteveHodges,andAndrewFitzgibbon.
Bougares,HolgerSchwenk,andY.Bengio.Learningphrase Kinectfusion: Real-time dense surface mapping and track-
representationsusingrnnencoder-decoderforstatisticalma- ing. In201110thIEEEinternationalsymposiumonmixed
chinetranslation. 2014. 3 andaugmentedreality,pages127–136.Ieee,2011. 3
[6] Jaesung Choe, Sunghoon Im, Francois Rameau, Minjun [20] MatteoPoggi,AndreaConti,andStefanoMattoccia. Multi-
Kang,andInSoKweon. VolumeFusion: Deepdepthfusion view guided multi-view stereo. In IEEE/RSJ International
for3Dscenereconstruction. InICCV,2021. 3 ConferenceonIntelligentRobotsandSystems,2022. IROS.
[7] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong 5,8
Zhang,HanHu,andYichenWei. Deformableconvolutional [21] Alexander Rich, Noah Stier, Pradeep Sen, and Tobias
networks. InProceedingsoftheIEEEInternationalConfer- Ho¨llerer. 3dvnet: Multi-viewdepthpredictionandvolumet-
enceonComputerVision(ICCV),2017. 4 ric refinement. In International Conference on 3D Vision
[8] Yasutaka Furukawa and Jean Ponce. Accurate, dense, and (3DV),2021. 3
robust multiview stereopsis. IEEE transactions on pattern
[22] Mohamed Sayed, John Gibson, Jamie Watson, Victor
analysisandmachineintelligence, 32(8):1362–1376, 2009.
Prisacariu,MichaelFirman,andCle´mentGodard.Simplere-
2,3
con: 3d reconstruction without 3d convolutions. In Pro-
[9] Silvano Galliani, Katrin Lasinger, and Konrad Schindler.
ceedings of the European Conference on Computer Vision
Massively parallel multiview stereopsis by surface normal
(ECCV),2022. 3,6,7
diffusion. InProceedingsoftheIEEEInternationalConfer-
[23] Johannes L Scho¨nberger, Enliang Zheng, Jan-Michael
enceonComputerVision,pages873–881,2015. 2,3
Frahm, and Marc Pollefeys. Pixelwise view selection for
[10] XiaodongGu,ZhiwenFan,SiyuZhu,ZuozhuoDai,Feitong
unstructuredmulti-viewstereo. InEuropeanConferenceon
Tan,andPingTan. Cascadecostvolumeforhigh-resolution
ComputerVision,pages501–518.Springer,2016. 3
multi-view stereo and stereo matching. In Proceedings of
[24] JohannesLutzScho¨nberger,EnliangZheng,MarcPollefeys,
theIEEE/CVFConferenceonComputerVisionandPattern
and Jan-Michael Frahm. Pixelwise view selection for un-
Recognition,pages2495–2504,2020. 3,5,6,7,8
structured multi-view stereo. In European Conference on
[11] RasmusJensen,AndersDahl,GeorgeVogiatzis,EngilTola,
ComputerVision(ECCV),2016. 2
andHenrikAanæs. Largescalemulti-viewstereopsiseval-
uation. In2014IEEEConferenceonComputerVisionand [25] PhilippSchro¨ppel,JanBechtold,ArtemijAmiranashvili,and
PatternRecognition,pages406–413.IEEE,2014. 2,5,8 ThomasBrox.Abenchmarkandabaselineforrobustmulti-
viewdepthestimation. InProceedingsoftheInternational
[12] ZhaoxinLi,KuanquanWang,WangmengZuo,DeyuMeng,
Conferenceon3DVision(3DV),2022. 2
andLeiZhang. Detail-preservingandcontent-awarevaria-
tionalmulti-viewstereoreconstruction. IEEETransactions [26] SudiptaN.Sinha,PhilipposMordohai,andMarcPollefeys.
onImageProcessing,25,2015. 2 Multi-viewstereoviagraphcutsonthedualofanadaptive
[13] PhilippLindenberger,Paul-EdouardSarlin,andMarcPolle- tetrahedralmesh. In2007IEEE11thInternationalConfer-
feys. LightGlue:LocalFeatureMatchingatLightSpeed. In enceonComputerVision,pages1–8,2007. 2
ICCV,2023. 5 [27] Noah Stier, Alexander Rich, Pradeep Sen, and Tobias
[14] Lahav Lipson, Zachary Teed, and Jia Deng. Raft-stereo: Ho¨llerer. Vortx: Volumetric 3d reconstruction with trans-
Multilevelrecurrentfieldtransformsforstereomatching. In formersforvoxelwiseviewselectionandfusion. InInterna-
InternationalConferenceon3DVision(3DV),2021. 7 tionalConferenceon3DVision(3DV),2021. 3[28] JiamingSun,YimingXie,LinghaoChen,XiaoweiZhou,and
Hujun Bao. NeuralRecon: Real-time coherent 3D recon-
structionfrommonocularvideo. InCVPR,2021. 3
[29] ZacharyTeedandJiaDeng. Raft: Recurrentall-pairsfield
transforms for optical flow. In Computer Vision – ECCV
2020, pages 402–419, Cham, 2020. Springer International
Publishing. 5
[30] FabioTosi,YiyiLiao,CarolinSchmitt,andAndreasGeiger.
Smd-nets: Stereomixturedensitynetworks. InConference
onComputerVisionandPatternRecognition(CVPR),2021.
2,5,7
[31] Ali Osman Ulusoy, Michael J. Black, and Andreas Geiger.
Semantic multi-view stereo: Jointly estimating objects and
voxels. InProceedingsIEEEConferenceonComputerVi-
sion and Pattern Recognition (CVPR) 2017, pages 4531–
4540,Piscataway,NJ,USA,2017.IEEE. 2
[32] FangjinhuaWang,SilvanoGalliani,ChristophVogel,Pablo
Speciale, and Marc Pollefeys. Patchmatchnet: Learned
multi-view patchmatch stereo. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pages14194–14203,2021. 5,6,7,8
[33] ShaoqianWang,BoLi,andYuchaoDai.Efficientmulti-view
stereobyiterativedynamiccostvolume. In2022IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR),pages8645–8654,2022. 3
[34] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu,
YuhengQiu,ChenWang,YafeiHu,AshishKapoor,andSe-
bastian Scherer. Tartanair: A dataset to push the limits of
visualslam. 2020. 2,5,7
[35] JunhuaXi, YifeiShi, YijieWang, YulanGuo, andKaiXu.
Raymvsnet: Learning ray-based 1d implicit fields for ac-
curatemulti-viewstereo. InProceedingsoftheIEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages8595–8605,2022. 3
[36] JiayuYang,WeiMao,JoseMAlvarez,andMiaomiaoLiu.
Costvolumepyramidbaseddepthinferenceformulti-view
stereo.InProceedingsoftheIEEE/CVFConferenceonCom-
puter Vision and Pattern Recognition, pages 4877–4886,
2020. 3
[37] YaoYao,ZixinLuo,ShiweiLi,TianFang,andLongQuan.
Mvsnet:Depthinferenceforunstructuredmulti-viewstereo.
InProceedingsoftheEuropeanConferenceonComputerVi-
sion(ECCV),pages767–783,2018. 2,3,4,5,6,7,8
[38] Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang,
and Long Quan. Recurrent mvsnet for high-resolution
multi-view stereo depth inference. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pages5525–5534,2019. 3,5,6,7,8
[39] YaoYao,ZixinLuo,ShiweiLi,JingyangZhang,YufanRen,
LeiZhou,TianFang,andLongQuan.Blendedmvs:Alarge-
scaledatasetforgeneralizedmulti-viewstereonetworks. In
ProceedingsoftheIEEE/CVFConferenceonComputerVi-
sionandPatternRecognition,pages1790–1799,2020. 1,2,
5,8
[40] Jingyang Zhang, Yao Yao, Shiwei Li, Zixin Luo, and Tian
Fang. Visibility-awaremulti-viewstereonetwork. InBritish
MachineVisionConference(BMVC),2020. 2,3,6,7Thispaperhasbeenacceptedforpublicationatthe
IEEEInternationalConferenceon3DVision(3DV),Davos,2024. ©IEEE
Range-Agnostic Multi-View Depth Estimation with Keyframe Selection
Supplementary Material
AndreaConti MatteoPoggi , ValerioCambareri StefanoMattoccia ,
† †‡ ∗ †‡
†DepartmentofComputerScienceandEngineering ∗SonyDepthsensingSolutions
‡AdvancedResearchCenteronElectronicSystem(ARCES) Brussels,Belgium
UniversityofBologna,Italy Brussels,Belgium
Thismanuscriptprovidesadditionalinsightsaboutourpaper“Range-AgnosticMulti-ViewDepthEstimationwithKeyframe
Selection”. Wecollecthereadditionalqualitativeandexperimentalmaterialaboutourmulti-viewdepthestimationproposal.
Moreover, we provide details about the network architecture and training procedure, as well as a qualitative study of our
keyframerankingapproach.
1.QualitativeResultsonBlended
WereportafewsamplescenesfromBlendedtoshowthenetworkcapabilitytoextractfinedetails. InFigure1weplot4out
of5viewsprovidedtothenetworkalongwiththepredictionandtheground-truth(darkblackrepresentsmissingvaluesin
theground-truth).
Source1 Source2 Source3 Reference Prediction GroundTruth
Figure1.QualitativeresultsonBlended.Predictionsobtainedbyusing5viewsasinput(only4areshowedforrepresentativepurpose).
1
4202
naJ
52
]VC.sc[
1v10441.1042:viXra2.QualitativeResultsonUnrealStereo4K
We evenly select a few samples from the available sequences of UnrealStereo4K and show the stereo pair, network pre-
diction, and ground-truth in Figure 2. UnrealStereo4K is a very challenging dataset containing heterogeneous indoor and
outdoorscenes. Ournetworkisnotfine-tunedonthedatasetitself–i.e.,weusethemodeltrainedonBlendedtoassessthe
generalizationcapabilitiesofourapproach.
LeftImage RightImage Prediction GroundTruth
Figure2.QualitativeresultsonUnrealStereo4KStereo.Weusethepre-trainedmodelonBlendedtoshowthecapabilityofourmethod
togeneralizeacrossdatasets.3.QualitativeResultsonTartanAir
WereportafewsamplescenesfromTartanAirtoshowthenetworkcapabilityonthiscomplexdataset. InFigure3weplot4
outof5viewsprovidedtothenetworkalongwiththepredictionandtheground-truth(darkblackrepresentsmissingvalues
intheground-truth).
Source1 Source2 Source3 Reference Prediction GroundTruth
Figure3.QualitativeresultsonTartanAir.Predictionsobtainedbyusing5viewsasinput(only4areshowedforrepresentativepurpose).
4.QualitativeResultsonDTU
Figure4.QualitativeresultsonDTU.Weshow3DreconstructionofdifferentobjectsandscenesprovidedbyDTUtoassessthecapability
ofourapproachtogenerateaccuratepointcloudreconstructionseventhoughwefocusonhighlydetaileddepthmapsestimation.InFigure4,wereportqualitativeresultsabout3DreconstructiononDTU.Wegenerateadepthmapforeachviewavailable
for a single scene, leveraging a total of 5 views for each prediction, and assemble such depth maps by applying geometric
andphotometricfilteringcommonintheliterature[3].
5.KeyframesRankingQualitativeResults
Reference Source1 Source2 Source3 Source4 Source5
Figure5. KeyframesRankingExample. Inthefirstrow,weshowascenefromBlendedcontaining5sourceviewsinrandomorder. In
thesecondrow,weshowourframeworkreordering.IfweapplyGaussianblurtotheimageswiththebestscoreandapplyagainreordering
(lastrow),ourframeworkassignstothemtheworstscorethistime.
Weprovideaqualitativeexampleoftheeffectivenessofkeyframesrankingenabledbyourframework. InFigure5,weshow
ascenefromBlendedcomposedof6frames: thefirstoneisthereferenceview,thensourceviewsfollowinrandomorder.
We extract correlation scores with the procedure described in the main paper and order views accordingly in the second
row. We can notice that higher scores are assigned to views with a higher visual overlap, e.g. the first source view in the
ordered row is the one that maximizes matches with the building highlighted in red, the street on its left, and the garden
betweenbuildings,whicharelargelyoccludedintheotherviews. Finally,inthelastrow,wetakethefirst2mostcorrelated
viewsaccordingtoourframeworkoutput, weapplyasimpleGaussianblurtosimulateout-of-focusimagesandrankonce
more. Wecanobservethatourframeworknowassignsthelowestscoretotheout-of-focusviews, althoughthesewerethe
best before. These experiments qualitatively demonstrate that our approach takes deeply into account not only the relative
position between views but also the 3D structure of the scene and the quality of matches it can recover from the available
views. Thus, our framework provides a view-centric methodology to discard poorly correlated views (e.g. out-of-focus,
blurred,withmovingobjects),whichcannotbeachievedbyreasoningonlyaboutrelativepose.
6.SourceViewsSchedulingAnalysis
Asalreadydetailedinthemainmanuscript,weapplyasimpleround-robinscheduletosamplethesourceviewusedtosample
correlationmatchesateachnetworkiteration. Thisapproachdoesnotcauseanyparticularproblem. Indeed,eventhoughthe
sourceviewischangedateachiterationthedepthstateisindependentofthelatter,thusenforcingconsistency. Toassessthat
our approach is not significantly affected by source views ordering, we perform a simple experiment: on the Blended test
set, wecomputemetricsforeachpermutationofthesourceviewsandcomputethe standarddeviationoftheperformance.
InTable1arereportedtheresultsofsuchexperiment. Theverylowvariancereportedattheverybottomconfirmshowthe
orderingweusetoiterateoverthesourceviewshasnegligibleimpactonthefinalqualityofthepredicteddepthmap.
modnaR
deredrO
)rulB(deredrOPermutation MAE RMSE >1m >2m >3m >4m >8m
N.1 0.316181 1.186300 0.069861 0.031390 0.017675 0.011238 0.003503
N.2 0.317703 1.188342 0.070145 0.031508 0.017682 0.011203 0.003491
N.3 0.318048 1.187708 0.070530 0.031465 0.017649 0.011226 0.003501
N.4 0.319271 1.187811 0.070630 0.031412 0.017647 0.011224 0.003536
N.5 0.315665 1.186850 0.069935 0.031355 0.017527 0.011122 0.003460
N.6 0.316765 1.187873 0.070035 0.031531 0.017691 0.011216 0.003522
Std. 0.001328 0.000755 0.000319 0.000069 0.000061 0.000042 0.000026
Table1.SourceViewsSchedulingAnalysis.ForeachsampleinthetestsetofBlendedweevaluateeachpermutationofthesourceviews
andcomputethestandarddeviationofthemetrics.Weuse3sourceviewstolimitthenumberofpermutations.
7.QualitativeResults–impactofthedepthrange
Wereportanexampleshowingthenegativeimpactthataninaccuratedepthrangecanproduceonthepredictionsofexisting
frameworks. Figure6shows,fromtoptobottom,fiveimagestakenfromBlended,theircorrespondingground-truthdepth,
andthepredictionsbyourframeworkandtwopriorworks[1,2]. Theselatterexposelargeartifactsinthefarthestregions
of the images, caused by the inaccurate depth range over which they operate. Indeed, as we can notice in the second row,
ground-truth depth is not provided for those regions, and thus the depth range used for computing the depth map does not
containthem–sinceitisestimateddirectlyfromground-truth. Onthecontrary,ourmodelproducescleananddetaileddepth
mapsevenintheseportionsoftheimages.
Figure6.WrongDepthRangeEffectsExampleonBlendedOnleft:fiveviewsfromtheBlended[5]sceneandtheirground-truthdepth,
followedbydepthmapsestimatedbyourframework,[2]and[4].Ourapproachdoesnotrequireanyknowledgeaboutthedepthrangeand
thusprovidesconsistentlysmootherdepthmapsontheentirescene,evenoutofthepre-definedrangewhere[2]and[4]struggle. Onthe
right:3Dreconstructionobtainedbymergingourpredictions,welimitthefloorreconstructiontobetterhighlighttheobjectdetails,despite
thefactthatourframeworkisabletoreconstructthewholearea.
8.NetworkStructureandTrainingDetails
ArchitectureDetails. Inthissection,wedescribethecorecomponentsofourframeworkindetail. InTable2,eachmodule
isdetailedintermsoflayersalongwiththeirparameters,inputs(inred),andoutputs(inblue). Inputsourceandtargetviews
sweiV
hturTdnuorG
sruO
]2[.lateuG
]4[.lategnaWare encoded through the Feature Encoder, then disentangled information is extracted from the reference view through the
ReferenceEncoder(calledcontextinTable2andaccounting128channels). Depth,hiddenstate,andreferencefeaturesare
usedtopredictsamplingoffsets,correlationscoresaresampledaccordingwiththemethodologydescribedinthemainpaper
and the recurrent block predicts a new hidden state and a ∆depth update. Finally, a shallow module predicts upsampling
weightsfromthehiddenstateandreferenceinformationandperformsconvexupsampling.
Name Layer K S In/Out Input
ResidualBlockStride2 Name Layer K S In/Out Input
conv0 Conv2D+BatchNorm2D+ReLU 3 2 In/Out input OffsetsComputation
c
d
oo
o
un
w
tv n1
s
C
C
Ro
o
eLn nv
v
U2 2D D+ +B Ba at tc ch hN No or rm m2 2D D+ +R Re eL LU
U
3
1
-
1
2
-
O
I
Onu u/Ot t/ /O Outu ut
t
c
i
dno opn wuv n0
t
s+conv1
c oo ffn sv e0
ts
C Co on nv v2 2D D+BatchNorm2D+ReLU 3
1
1
1
1 22 58 6+ /912 98+1 2/256
c cdo oen
np
vt te
h
0x st −, 1hiddens−1,
× ×
ResidualBlockStride1 RecurrentBlock
conv0 Conv2D+BatchNorm2D+ReLU 3 2 In/Out input corr0 Conv2D+ReLU 1 1 9 9/256 corrfeats
×
conv1 Conv2D+BatchNorm2D+ReLU 3 1 Out/Out conv0 corr1 Conv2D+ReLU 3 1 256/192 corr0
out ReLU FeatureEncoder&Refer- ence- EncoO du et r/Out conv1+input dd ff ee aa tt ss 10 C Co on nv v2 2D D+ +R Re eL LU
U
7
3
1
1
1 1/ 21 82 /8
64
d dfe ep at th s0s−1
conv0 Conv2D+BatchNorm2D+ReLU 7 2 3/64 image conv0 Conv2D+ReLU 3 1 192+64/128-1 dfeats1,corr1
conv1 ResidualBlockStride2 - - 64/64 conv0 context,conv0,
hidden0 ConvGRU2D (1,5) 1 128+1+128+128/128
c co on nv v2
3
R Re es si id du ua al lB Bl lo oc ck kS St tr ri id de e1
2
-
-
-
-
6 64 4/ /6 94
6
c co on nv v1
2 hiddens ConvGRU2D (5,1) 1 128/128
d hie dp dt eh ns 0−1,hiddens−1
conv4 ResidualBlockStride1 - - 96/96 conv3 conv1 Conv2D+ReLU 3 1 128/64 hiddens
conv5 ResidualBlockStride2 - - 96/128 conv4 ∆depth Conv2D+ReLU 3 1 64/1 conv1
conv6 ResidualBlockStride1 - - 128/128 conv5 ConvexUpsampling
conv7 ResidualBlockStride2 - - 96/128 conv6 conv0 Conv2D+ReLU 3 1 128+256/128+256 hiddens,context
conv8 ResidualBlockStride1 - - 128/128 conv7 upmask Conv2D 1 1 128+256/8 8 9 conv0
× ×
feats Conv2D 1 1 128/256 conv8
Table2. FrameworkModulesDescription. Wedetaileachlearnedcomponentofourframework. Eachmoduleinputsandoutputsare
showninredandblue,respectively.
TrainingDetails. WetrainourmodelonBlended,TartanAirandDTUwithAdamW,learningrate10 −4 andweightdecay
10 5. We always clip gradients with global norm 1 to stabilize the behavior of Gated Recurrent Units. On Blended, we
−
normalizerelativeposetranslation(betweenthereferenceandsourceviews)tohaveameanvalueof1forbetternumerical
stability. OnBlended,wetrainfor200Kstepsandthenfine-tunefor100Kstepswithalearningrateof10 5. OnDTU,we
−
fine-tune the 200K Blended checkpoint for 100K steps with a learning rate of 10 4. We always train with batch size 1 on
−
2RTX3090inmixedprecision. Duringtrainingandevaluation,wealwaysperform10cyclesovertheinputsourceviews,
thatis40totalstepswith4sourceviews, exceptfortheUnrealStereo4Kstereobenchmarkwhereweperform40updating
steps on the unique source view available. In all the experiments, we compute dynamic offsets in a neighborhood of size
=9 9foratotalof81samplingcoordinates.
||N|| ×
References
[1] ConnellyBarnes,EliShechtman,AdamFinkelstein,andDanBGoldman. Patchmatch: Arandomizedcorrespondencealgorithmfor
structuralimageediting. ACMTrans.Graph.,28(3):24,2009. 5
[2] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-
viewstereoandstereomatching. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
2495–2504,2020. 5
[3] RasmusJensen,AndersDahl,GeorgeVogiatzis,EngilTola,andHenrikAanæs. Largescalemulti-viewstereopsisevaluation. In2014
IEEEConferenceonComputerVisionandPatternRecognition,pages406–413.IEEE,2014. 4
[4] FangjinhuaWang,SilvanoGalliani,ChristophVogel,PabloSpeciale,andMarcPollefeys. Patchmatchnet:Learnedmulti-viewpatch-
matchstereo. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages14194–14203,2021.
5
[5] YaoYao,ZixinLuo,ShiweiLi,JingyangZhang,YufanRen,LeiZhou,TianFang,andLongQuan.Blendedmvs:Alarge-scaledataset
forgeneralizedmulti-viewstereonetworks.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pages1790–1799,2020. 5