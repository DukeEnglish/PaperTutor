Adaptive Mobile Manipulation for Articulated Objects
In the Open World
Haoyu Xiong Russell Mendonca Kenneth Shaw Deepak Pathak
CMU CMU CMU CMU
Fig.1:Open-WorldMobileManipulationSystem:Weuseafull-stackapproachtooperatearticulatedobjectssuchasreal-worlddoors,
cabinets, drawers, and refrigerators in open-ended unstructured environments.
Abstract—Deploying robots in open-ended unstructured en- for generalist robots which can perform useful tasks in
vironments such as homes has been a long-standing research open-ended unstructured environments, as opposed to being
problem. However, robots are often studied only in closed-off
restricted to controlled laboratory settings focused primarily
lab settings, and prior mobile manipulation work is restricted
on tabletop manipulation.
topick-move-place,whichisarguablyjustthetipoftheiceberg
in this area. In this paper, we introduce Open-World Mobile However, developing and deploying such robot systems
Manipulation System, a full-stack approach to tackle realistic
in the open-world with the capability of handling unseen
articulated object operation, e.g. real-world doors, cabinets,
objects is challenging for a variety of reasons, ranging from
drawers,andrefrigeratorsinopen-endedunstructuredenviron-
ments.Therobotutilizesanadaptivelearningframeworktoini- the lack of capable mobile manipulator hardware systems
tiallylearnsfromasmallsetofdatathroughbehaviorcloning, to the difficulty of operating in diverse scenarios. Con-
followedbylearningfromonlinepracticeonnovelobjectsthat sequently, most of the recent mobile manipulation results
falloutsidethetrainingdistribution.Wealsodevelopalow-cost
end up being limited to pick-move-place tasks [9]–[11],
mobile manipulation hardware platform capable of safe and
which is arguably representative of only a small fraction of
autonomous online adaptation in unstructured environments
with a cost of around 20,000 USD. In our experiments we problems in this space. Since learning for general-purpose
utilize 20 articulate objects across 4 buildings in the CMU mobile manipulation is challenging, we focus on a restricted
campus. With less than an hour of online learning for each class of problems, involving the operation of articulated
object, the system is able to increase success rate from 50% of
objects, such as doors, drawers, refrigerators, or cabinets in
BC pre-training to 95% using online adaptation. Video results
open-world environments. This is a common and essential
at https://open-world-mobilemanip.github.io/.
task encountered in everyday life, and is a long-standing
I. INTRODUCTION probleminthecommunity[12]–[18].Theprimarychallenge
is generalizing effectively across the diverse variety of such
Deploying robotic systems in unstructured environments
objects in unstructured real-world environments rather than
such as homes has been a long-standing research problem.
manipulating a single object in a constrained lab setup.
In recent years, significant progress has been made in de-
Furthermore, we also need capable hardware, as opening a
ployinglearning-basedapproaches[1]–[4]towardsthisgoal.
doornotonlyrequiresapowerfulanddexterousmanipulator,
However, this progress has been largely made independently
butthebasehastobestableenoughtobalancewhilethedoor
either in mobility or in manipulation, while a wide range of
is being opened and agile enough to walk through.
practicalrobotictasks requiredealingwithboth aspects[5]–
[8]. The joint study of mobile manipulation paves the way We take a full-stack approach to address the above
4202
naJ
52
]OR.sc[
1v30441.1042:viXrachallenges. In order to effectively manipulate objects in policy on a demonstration dataset using imitation learning.
open-worldsettings,weadoptaadaptivelearningapproach, Adaptive learning allows the robot to keep learning from
wheretherobotkeepslearningfromonlinesamplescollected self-practice data via online RL. Repeated interaction for
during interaction. Hence even if the robot encounters a autonomous learning requires capable hardware, for which
new door with a different mode of articulation, or with we propose a versatile, agile, low-cost easy to build system.
different physical parameters like weight or friction, it can Weintroducealow-costmobilemanipulationhardwareplat-
keep adapting by learning from its interactions. For such formthatoffersahighpayload,makingitcapableofrepeated
a system to be effective, it is critical to be able to learn interactionwithobjects,e.g.aheavy,spring-loadeddoor,and
efficiently,sinceitisexpensivetocollectrealworldsamples. a human-size, capable of maneuvering across various doors
The mobile manipulator we use as shown in Figure. 3 has and navigating around narrow and cluttered spaces in the
a very large number of degrees of freedom, corresponding open world. We conducted a field test of 8 novel objects
to the base as well as the arm. A conventional approach for ranging across 4 buildings on a university campus to test
the action space of the robot could be regular end-effector the effectiveness of our system, and found adaptive earning
control for the arm and SE2 control for the base to move in boosts success rate from 50% from the pre-trained policy to
the plane. While this is very expressive and can cover many 95% after adaptation.
potential behaviors for the robot to perform, we will need to
collect a very large amount of data to learn control policies
II. RELATEDWORK
inthisspace.Giventhatourfocusisonoperatingarticulated a) Adaptive Real-world Robot Learning: There has
objects, can we structure the action space so that we can get been a lot of prior work that studies how robots can acquire
away with needing fewer samples for learning? new behavior by directly using real-world interaction sam-
Consider the manner in which people typically approach ples via reinforcement learning using reward [19]–[22] and
operating articulated objects such as doors. This generally even via unsupervised exploration [23], [24]. More recently
first involves reaching towards a part of the object (such there have been approaches that use RL to fine-tune policies
as a handle) and establishing a grasp. We then execute con- that have been initialized via other sources of data - either
strainedmanipulationlikerotating,unlatching,orunhooking, using offline robot datasets [25], simulation [26] or human
where we apply arm or body movement to manipulate the video [27], [28] or a combination of these approaches
object. In addition to this high-level strategy, there are also [10]. There works do not use any demonstrations on the
lower-level decisions made at each step regarding exact test environment, and learn behavior via reinforcement as
directionofmovement,extentofperturbationandamountof opposedtoimitation.Weoperateinasimilarsetting,andfo-
force applied. Inspired by this, we use a hierarchical action cusondemonstratingRLadaptationonmobilemanipulation
spaceforourcontroller,wherethehigh-levelactionsequence systems that can be deployed in open-world environments.
follows the grasp, constrained manipulation strategy. These While prior large-scale industry efforts also investigate this
primitives are parameterized by learned low-level continu- [10], we seek to be able to learn much more efficiently with
ous values, which needs to be adapted to operate diverse fewer data samples.
articulated objects. To further bias the exploration of the b) Learning-based Mobile Manipulation Systems. : In
system towards reasonable actions and avoid unsafe actions recentyears,thesetupformobilemanipulationtasksinboth
duringonlinesampling,wecollectadatasetofexpertdemon- simulatedandreal-worldenvironmentshasbeenaprominent
strations on 12 training objects, including doors, drawers topicofresearch[5],[29]–[37].Notably,severalstudieshave
and cabinets to train an initial policy via behavior cloning. exploredthepotentialofintegratingLargeLanguageModels
While this is not very performant on new unseen doors into personalized home robots, signifying a trend towards
(getting around 50% accuracy), starting from this policy more interactive and user-friendly robotic systems [37]–
allows subsequent learning to be faster and safer. [39]. While these systems display impressive long horizon
Learning via repeated online interaction also requires capabilities using language for planning, these assume fixed
capable hardware. As shown in Figure 3, we provide a low-levelprimitivesforcontrol.Inourworkweseektolearn
simple and intuitive solution to build a mobile manipula- low-level control parameters via interaction. Furthermore,
tion hardware platform, followed by two main principles: unlike the majority of prior research which predominantly
(1) Versatility and agility - this is essential to effectively focuses on pick-move-place tasks [9], we consider operat-
operate diverse objects with different physical properties in ing articulated objects in unstructured environments, which
potentiallychallengingenvironments,forinstanceacluttered present an increased level of difficulty.
office. (2) Affordabiluty and Rapid-prototyping - Assembled c) DoorManipulation: Theresearchareaofdooropen-
with off the shelf components, the system is accessible and ing has a rich history in the robotics community [15]–[18],
can be readily be used by most research labs. [40]. A significant milestone in the domain was the DARPA
In this paper, we present Open-World Mobile Manipu- Robotics Challenge (DRC) finals in 2015. The accomplish-
lation System, a full stack approach to tackle the problem ment of the WPI-CMU team in door opening illustrated not
of mobile manipulation of realistic articulated objects in the only advances in robotic manipulation and control but also
open world. Efficient learning is enabled by a structured ac- thepotentialofhumanoidrobotstocarryoutintricatetasksin
tion space with parametric primitives, and by pretraining the real-worldenvironments[12]–[14].Nevertheless,priortotheAlgorithm 1 Adaptive Learning
Require: Grasping primitive G(.) taking parameter g
Grasp Mobile-Manipulation Env
Require: Constrained manipulation primitives M(.), taking
parameter C and c.
1: Initialize primitive classifier π φ({C i}N i=1|I)
2: Initialize conditional action policy
π (g,{c}N |I,{C}N )
θ i i=1 i i=1
Fig.2:Adaptive Learning Framework:Thepolicyoutputs 3: Collect a dataset D of expert demos
low-level parameters for the grasping primitive, and chooses {I,g,{C}N ,{c}N }
i i=1 i i=1
a sequence of manipulation primitives and their parameters. 4: Train π φ and π θ on D using Imitation Learning 2
5: for online RL iteration 1:N do
6: Given image I s, sample {C i}N i=1∼π φ(.|I s),
deep learning era, the primary impediment was the robots’ sample (g,{c}N )∼π (.|I )
i i=1 θ s
perceptioncapabilities,whichfalteredwhenconfrontedwith 7: Execute trajectory {G(g),{M(C i,c i)}N i=1},
tasks necessitating visual comprehension of complex and observe reward R
unstructured environments. Approaches using deep learning 8: Update policies π φ and π θ using RL (Eqs. 5, 4, 2)
to address vision challenges include Wang et al. [41], which 9: end for
leverages synthetic data to train keypoint representation for
the grasping pose estimation, and Qin et, al. [42], which
proposedanend-endpointcloudRLframeworkforsim2real a constrained mobile-manipulation primitives M(.), where
transfer. Another approach is to use simulation to learn primitive M(.) takes two parameters, a discrete parameter
policies, using environments such as Doorgym [43], which C and a continuous parameter c. Trajectories are executed
provides a simulation benchmark for door opening tasks. in an open-loop manner, a grasping primitive followed by a
The prospect of large-scale RL combined with sim-to-real sequence of N constrained mobile-manipulation primitives:
transfer holds great promise for generalizing to a diverse
range of doors in real-world settings [42]–[44]. However,
{I ,G(g),{M(C,c)}N ,I ,R}
one major drawback is that the system can only generalize s i i i=1 f
to the space of assets already present while training in the
where I is the initial observed image, G(g), M(C,c))
s i i
simulation. Such policies might struggle when faced with a
denotetheparameterizedgraspandconstrainedmanipulation
new unseen door with physical properties, texture or shape
primitives respectively, I is the final observed image, and r
f
different from the training distribution. Our approach can
istherewardforthetrajectory.Whilethisstructuredspaceis
keeponlearningviareal-worldsamples,andhencecanlearn
lessexpressivethanthefullactionspace,itislargeenoughto
to adapt to difficulties faced when operating new unseen
learn effective strategies for the everyday articulated objects
doors.
we encountered, covering 20 different doors, drawers, and
fridges in open-world environments. The key benefit of the
III. ADAPTIVELEARNINGFRAMEWORK
structure is that it allows us to learn from very few samples,
Inthissection,wedescribeouralgorithmicframeworkfor
using only on the order of 20-30 trajectories. We describe
trainingrobotsforadaptivemobilemanipulationofeveryday
the implementation details of the primitives in section IV-B.
articulated objects. To achieve efficient learning, we use a
structured hierarchical action space. This uses a fixed high-
B. Adaptive Learning
level action strategy and learnable low-level control param-
eters. Using this action space, we initialize our policy via Given an initial observation image I s, we use a classifier
behavior cloning (BC) with a diverse dataset of teleoperated π φ({C i}N i=1|I) to predict the a sequence of N discrete pa-
demonstrations. This provides a strong prior for exploration rameters {C i}N
i=1
for constrained mobile-manipulation, and
and decreases the likelihood of executing unsafe actions. a conditional policy network π θ(g,{c i}N i=1|I,{C i}N i=1) which
However, the initialized BC policy might not generalize produces the continuous parameters of the grasping primi-
to every unseen object that the robot might encounter due tive and a sequence of N constrained mobile-manipulation
to the large scope of variation of objects in open-world primitives. The robot executes the parameterized primitives
environments. To address this, we enable the robot to learn one by one in an open-loop manner.
from the online samples it collects to continually learn and 1) Imitation: We start by initializing our policy using
adapt. We describe the continual learning process as well as a small set of expert demonstrations via behavior cloning.
design considerations for online learning. The details of this dataset are described in section IV-
C. The imitation learning objective is to learn policy pa-
A. Action Space
rameters π that maximize the likelihood of the expert
θ,φ
For greater learning efficiency, we use a parameterized actions.Specifically,givenadatasetofimageobservationsI ,
s
primitive action space. Concretely, we assume access to a andcorrespondingactions{g,{C}N ,{c}N },theimitation
i i=1 i i=1
grasping primitive G(.) parameterized by g. We also have learning objective is:max(cid:2) logπ ({C}N |I )+logπ (g,{c}N |{C}N ,I )(cid:3) server camera arm & EE Z
φ i i=1 s θ i i=1 i i=1 s Y
φ,θ
(1)
X
2) Online RL: The central challenge we face is operating onboard
computer
new articulated objects that fall outside the behavior cloning
training data distribution. To address this, we enable the odometer
policy to keep improving using the online samples collected
by the robot. This corresponds to maximizing the expected Y
sum of rewards under the policy :
X
(cid:34) (cid:35)
T
maxE ∑r(s,a) (2)
θ,φ
πθ,φ
t=0
t t
omni-base
Since we utilize a highly structured action space as de-
Fig. 3: Mobile Manipulation Hardware Platform: Differ-
scribed previously, we can optimize this objective using a
ent components in the mobile manipulator hardware system.
fairly simple RL algorithm. Specifically we use the REIN-
Our design is low-cost and easy-to-build with off-the-shelf
FORCE objective [45]:
components
(cid:34) (cid:35)
T
∇ J(θ,φ)=E ∑∇ logπ(a|s)·r (3)
θ,φ πθ,φ θ t t t
t=0 • Versatility and agility: Everyday articulated objects like
=E (cid:2) (∇ logπ (C|I)+∇ logπ (g,c|C,I))·R(cid:3) (4) doors have a wide degree of variation of physical
πφ,θ φ φ i θ θ i i
properties, including weight, friction and resistance.
whereRistherewardprovidedattheendoftrajectoryex-
To successfully operate these, the platform must offer
ecution.Notethatweonlyhaveasingletime-steptransition,
high payload capabilities via a strong arm and base.
all actions are determined from the observed image I , and
s Additionally,wesoughttodevelopahuman-sized,agile
executed in an open-loop manner. Further details for online
platform capable of maneuvering across various real-
adaptation such as rewards, resets and safety are detailed in
world doors and navigating unstructured and narrow
section IV-D.
environments, such as cluttered office spaces.
3) OverallFinetuningObjective: Toensurethatthepolicy
• Affordability and Rapid-Prototyping: The platform is
doesn’tdeviatetoofarfromtheinitializationoftheimitation
designed to be low-cost for most robotics labs and em-
dataset,weuseaweightedobjectivewhilefinetuning,where
ploysoff-the-shelfcomponents.Thisallowsresearchers
the overall loss is :
to quickly assemble the system with ease, allowing the
L =L +α∗L (5) possibility of large-scale open-world data collection in
overall online offline
the future.
where loss on online sampled data is optimized via Eq.4
Weshowthedifferentcomponentsofthehardwaresystem
and loss on the batch of offline data is optimized via BC as
in Figure 3. Among the commercially available options,
in Eq.2. We use equal sized batches for online and offline
we found the Ranger Mini 2 from AgileX to be an ideal
data while performing the update.
choice for robot base due to its stability, omni-directional
IV. OPEN-WORLDMOBILEMANIPULATIONSYSTEMS velocitycontrol,andhighpayloadcapacity.Thesystemuses
In this section, we describe details of our full-stack ap- an xArm for manipulation, which is an effective low-cost
proach encompassing hardware, action space for efficient arm with a high payload (5kg), and is widely accessible
learning, the demonstration dataset for initialization of the for research labs. The system uses a Jetson computer to
policy and crucially details of autonomous, safe execution support real-time communication between sensors, the base,
with rewards. This enables our mobile manipulation system the arm, as well as a server that hosts large models. We
to adaptively learn in open-world environments, to manip- use a D435 Intel Realsense camera mounted on the frame to
ulate everyday articulated objects like cabinets, drawers, collectRGBDimagesasego-centricobservationsandaT265
refrigerators, and doors. Intel Realsense camera to provide visual odometry which is
criticalforresettingtherobotwhenperformingtrialsforRL.
A. Hardware Thegripperisequippedwitha3d-printedhookerandananti-
The transition from tabletop manipulation to mobile ma- slip tape to ensure a secure and stable grip. The overall cost
nipulation is challenging not only from algorithmic studies of the entire system is around 20,000 USD, making it an
but also from the perspective of hardware. In this project, affordable solution for most robotics labs.
we provide a simple and intuitive solution to build a mobile We compare key aspects of our modular platform with
manipulation the hardware platform. Specifically, our design that of other mobile manipulation platforms in Table I.This
addresses the following challenges - comparisonhighlightsadvantagesofoursystemsuchascost-Fig. 4: Articulated Objects: Visualization of the 12 training and 8 testing objects used, with location indicators corresponding to the
buildings in the map below. The training and testing objects are significantly different from each other, in terms of different visual
appearances, different modes of articulation, or different physical parameters, e.g. weight or friction.
is projected into 3d coordinates using camera calibration,
Wean
and this is the nominal grasp position. The low-level control
parameterstothegraspingprimitiveindicateanoffsetforthis
positionatwhichtograsp.Thisisbeneficialsincedepending
GHC NSH
onthetypeofhandletherobotmightneedtoreachaslightly
different position which can be learned via the low-level
Hamburg continuous valued parameters.
2) Constrained Mobile-Manipulation: We use velocity
control for the robot arm end-effector and the robot base.
With a 6dof arm and 3dof motion for the base (in the SE2
plane), we have a 9-dimensional vector -
Control: (v ,v ,v ,v ,v ,v ,V ,V ,V )
x y z yaw pitch roll x y ω
Fig. 5: Field Test on CMU Campus: The system was Where the first 6 dimensions correspond to control for the
evaluated on articulated objects from across four distinct arm, and the last three are for the base. The primitives we
buildings on the Carnegie Mellon University campus. use impose contraints on this space as follows -
Unlock: (0,0,v ,v ,0,0,0,0,0)
z yaw
Rotate: (0,0,0,v ,0,0,0,0,0)
effectiveness, reactivity, ability to support a high-payload yaw
arm, and a base with omnidirectional drive. Open: (0,0,0,0,0,0,V x,0,0)
B. Primitive Implementation
For control, the policy outputs an index corresponding to
In this subsection, we describe the implementation details
which primitive is to executed, as well as the corresponding
of our parameterized primitive action space.
low-level parameters for the motion. The low-level control
1) Grasping: Given the RGBD image of the scene ob-
command is continuous valued from -1 to 1 and executed
tainedfromtherealsensecamera,weuseoff-the-shelfvisual
for a fixed duration of time. The sign of the parameters
models [46], [47] to obtain the mask of the door and handle
dictatesthedirectionofthevelocitycontrol,eitherclockwise
given just text prompts. Furthermore, since the door is a
or counter-clockwise for unlock and rotate, and forward or
flat plane, we can estimate the surface normals of the door
backward for open.
using the corresponding mask and the depth image. This
C. Pretraining Dataset
is used to move the base close to the door and align it to
be perpendicular, and also to set the orientation angle for The articulated objects we consider in this project consist
graspingthehandle.Thecenterofthe2dmaskofthehandle of three rigid parts: a base part, a frame part, and a handle
tes
niart
tes
tsetHardware features comparison
Arm payload DoF arm omni-base footprint base max speed price
Stretch RE1 [8] 1.5kg 2 ✕ 34 cm, 33 cm 0.6 m/s 20k USD
Go1-air + WidowX 250s [36] 0.25kg 6 ✓ 59 cm, 22 cm 2.5 m/s 10k USD
Franka + Clearpath Ridgeback [48] 3kg 7 ✓ 96 cm, 80 cm 1.1 m/s 75k USD
Franka + Omron LD-60 [49] 3kg 7 ✕ 70 cm, 50 cm 1.8 m/s 50k USD
Xarm-6 + Agilex Ranger mini 2 (ours) 5kg 6 ✓ 74 cm, 50 cm 2.6 m/s 20k USD
TABLE I: Comparison of different aspects of popular hardware systems for mobile manipulation
part. This covers objects such as doors, cabinets, drawers potentiallydamagingitself,andalsoprovidenegativereward
and fridges. The base and frame are connected by either a to disincentivize such actions.
revolute joint (as in a cabinet) or a prismatic joint (as in 2) Reward Specification: In our main experiments, a
a drawer). The frame is connected to the handle by either human operator provides rewards- with +1 if the robot suc-
a revolute joint or a fixed joint. We identify four major cesfullyopensthedoors,0ifitfails,and-1ifthereisasafety
types of the articulated objects, which relate to the type violation.Thisisfeasiblesincethesystemrequiresveryfew
of handle, and the joint mechanisms. Handle articulations samples for learning. For autonomous learning however, we
commonly include levers (Type A) and knobs (Type B). For would like to remove the bottleneck of relying on humans
cases where handles are not articulated, the body-frame can to be present in the loop. We investigate using large vision
revolve about a hinge using a revolute joint (Type C), or language models as a source of reward. Specifically, we use
slide back and forth along a prismatic joint, for example, CLIP [50] to compute the similarity score between two text
drawers (Type D). While not exhaustive, this categorization prompts and the image observed after robot execution. The
covers a wide variety of everyday articulated objects a robot two prompts we use are - ”door that is closed” and ”door
system might encounter. To provide generalization benefits that is open”. We compute the similarity score of the final
inoperatingunseennovelarticulatedobjects,wefirstcollect observed image and each of these prompts and assign a
a offline demonstration dataset. We include 3 objects from reward of +1 if the image is closer to the prompt indicating
each category in the BC training dataset, collecting 10 thedoorisopen,and0intheothercase.Ifasafetyprotection
demonstrations for each object, producing a total of 120 is triggered the reward is -1.
trajectories. 3) ResetMechanism: Therobotemploysvisualodometry,
Wealsohave2held-outtestingobjectsfromeachcategory utilizing the T265 tracking camera mounted on its base,
for generalization experiments. The training and testing enabling it to navigate back to its initial position. At the
objects differ significantly in visual appearance (eg. texture, end of every episode, the robot releases its gripper, and
color), physical dynamics (eg. if spring-loaded), and actu- moves back to the original SE2 base position, and takes an
ation (e.g. the handle joint might be clockwise or counter- image of I f for computing reward. We then apply a random
clockwise). We include visualizations of all objects used in perturbationtotheSE2positionofthebasesothatthepolicy
trainandtestsetsinFig.4,alongwithwhichpartofcampus learns to be more robust. Furthermore, if the reward is 1,
they are from as visualized in Fig. 5. where the door is opened, the robot has a scripted routine to
close the door.
D. Autonomous and Safe Online Adaptation
V. RESULTS
The key challenge we face is operating with new objects We conduct an extensive field study involving 12 training
that fall outside the BC training domain. To address this, we objectsand8testingobjectsacrossfourdistinctbuildingson
developasystemcapableoffullyautonomousReinforcement the Carnegie Mellon University campus to test the efficacy
Learning (RL) online adaptation. In this subsection, we of our system. In our experiments, we seek to answer the
demonstrate the details of the autonomy and safety of our following questions:
system. 1) Canthesystemimproveperformanceonunseenobjects
1) Safety Aware Exploration: It is crucial to ensure that via online adaptation across diverse object categories?
the actions the robot takes for exploring are safe for its 2) Howdoesthiscomparetosimplyusingimitationlearn-
hardware, especially since it is interacting with objects ing on provided demonstrations?
underarticulationconstraints.Ideally,thiscouldbeaddressed 3) Can we automate providing rewards using off-the-shelf
for dynamic tasks like door opening using force control. vision-language models?
However, low-cost arms like the xarm-6 we use do not 4) How does the hardware design compare with other
support precise force sensing. For deploying our system, we platforms?
use a safety mechanism based which reads the joint current
A. Online Improvement
during online sampling. If the robot samples an action that
causes the joint current to meet its threshold, we terminate 1) DiverseObjectCategoryEvaluation: :Weevaluateour
the episode and reset the robot, to prevent the arm from approach on 4 categories of held-out articulated objects. Asiteration iteration iteration iteration
Fig. 6: Online Improvement: Comparison of our approach to the imitation policy on 4 different categories of articulated
objects,eachconsistingoftwodifferentobjects.Ouradaptiveapproachisabletoimproveinperformance,whiletheimitation
policy has limited generalization.
CLIP-reward comparison Action-ReplayComparison
KNN-open KNN-close BC-0 Adapt-GT
BC-0 Adapt-GT Adapt-CLIP
SuccessRateB1(knob) 10% 0% 30% 80%
Success Rate A1 (lever) 20% 100% 80%
SuccessRateA2(lever) 0% 0% 0% 80%
Success Rate B1 (knob) 30% 80% 80%
TABLE III: We compare the performance of our adaptation
TABLE II: In this table, we present improvements in online policies and initialized BC policies with KNN baselines.
adaptation with CLIP reward.
and learn from them, allowing the robot to learn how to
described in section IV-C, these are determined by handle operate novel unseen articulated objects.
articulation and joint mechanisms. This categorization is 2) Action-replay baseline: : There is also another very
based on types of handles, including levers (type A) and simpleapproachforutilizinga datasetofdemonstrationsfor
knobs (type B), as well as joint mechanisms including performing a task on a new object. This involves replaying
revolute (type C) and prismatic (type D) joints. We have trajectories from the closest object in the training set. This
two test objects from each category. We report continual closest object can be found using k-nearest neighbors with
adaptation performance in Fig. 6 over 5 iterations of fine- somedistancemetric.Thisapproachislikelytoperformwell
tuning using online interactions, starting from the behavior especially if the distribution gap between training and test
cloned initial policy. Each iteration of improvement consists objects is small, allowing the same actions to be effective.
of 5 policy rollouts, after which the model is updated using We run this baseline for two objects that are particularly
the loss in Equation 5. hard for behavior cloning, one each from Type A and
From Fig. 6, we see that our approach improves the B categories (lever and knob handles respectively). The
averagesuccessrateacrossallobjectsfrom50to95percent. distance metric we use to find the nearest neighbor in the
Hence, continually learning via online interaction samples trainingsetiseuclideandistanceofthetheCLIPencodingof
is able to overcome the limited generalization ability of observed images. We evaluate this baseline both in an open-
the initial behavior cloned policy. The adaptive learning loop and closed-loop manner. In the former case, only the
procedure is able to learn from trajectories that get high first observed image is used for comparison and the entire
reward, and then change its behavior to get higher reward retrieved action sequence is executed, and in the latter we
more often. In cases where the BC policy is reasonably search for the closest neighbor after every step of execution
performant, such as Type C and D objects with an average andperformthecorrespondingaction.FromTable IIIwesee
success rate of around 70 percent, RL is able to perfect the that this approach is quite ineffective, further underscoring
policy to 100 percent performance. Furthermore, RL is also the distribution gap between the training and test objects in
able to learn how to operate objects even when the initial our experiments.
policyismostlyunabletoperformthetask.Thiscanbeseen 3) AutonomousrewardviaVLMs: Weinvestigatewhether
from the Type A experiments, where the imitation learning wecanreplacethehumanoperatorwithanautomatedproce-
policy has a very low success rate of only 10 percent, and dure to provide rewards. The reward is given by computing
completelyfailstoopenoneofthetwodoors.Withcontinual the similarity score between the observed image at the end
practice, RL is able to achieve an average success of 90 of execution, and two text prompts, one of which indicate
percent. This shows that RL can explore to take actions that that the door is open, and the other that says the doors is
are potentially out of distribution from the imitation dataset, closed, as described in section IV-D.
etar
sseccusAs with the action- allowing for autonomous learning. We hope to deploy such
replay baseline, we eva- mobile manipulators to continuously learn a broader variety
lute this on two test of tasks via repeated practice.
doors, on each from the
handle and knob cate-
ACKNOWLEDGMENT
gories. From Table II, we
see that online adapta- We thank Shikhar Bahl, Tianyi Zhang, Xuxin Cheng, Sh-
tion with VLM reward agun Uppal, and Shivam Duggal for the helpful discussions.
achieves a similar perfor-
iteration
mance as using ground-
REFERENCES
truth human-labeled re- Fig. 7: Online Adaptation with
ward, with an average of CLIP reward. Adaptive learning [1] S. Bahl, A. Gupta, and D. Pathak, “Human-to-robot imitation in the
80 percent compared to using rewards from CLIP, instead wild,”RSS,2022. 1
of a human operator, showing our [2] A. Kumar, Z. Fu, D. Pathak, and J. Malik, “Rma: Rapid motor
90percent.Wealsoreport
system can operate autonomously. adaptationforleggedrobots,”arXivpreprintarXiv:2107.04034,2021.
the performance after ev- 1
eryiterationoftrainingin [3] M.Chang,T.Gervet,M.Khanna,S.Yenamandra,D.Shah,S.Y.Min,
Fig.7.Removingtheneedforahumanoperatortobepresent K.Shah,C.Paxton,S.Gupta,D.Batraetal.,“Goat:Gotoanything,”
arXivpreprintarXiv:2311.06430,2023. 1
in the learning loop opens up the possiblity for autonomous
[4] D.Shah,A.Sridhar,N.Dashora,K.Stachowicz,K.Black,N.Hirose,
training and improvement. andS.Levine,“Vint:Afoundationmodelforvisualnavigation,”2023.
1
B. Hardware Teleop Strength [5] Z. Fu, T. Z. Zhao, and C. Finn, “Mobile aloha: Learning bimanual
mobile manipulation with low-cost whole-body teleoperation,” in
arXiv,2024. 1,2
Expert teleoperation success rate [6] N.M.M.Shafiullah,A.Rai,H.Etukuru,Y.Liu,I.Misra,S.Chintala,
andL.Pinto,“Onbringingrobotshome,”2023. 1
lever B knob A [7] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn,
Stretch RE1 0/5 0/5 K.Gopalakrishnan,K.Hausman,A.Herzog,J.Hsu,J.Ibarz,B.Ichter,
Ours 5/5 5/5 A.Irpan,T.Jackson,S.Jesmonth,N.J.Joshi,R.Julian,D.Kalash-
nikov, Y. Kuang, I. Leal, K.-H. Lee, S. Levine, Y. Lu, U. Malla,
D.Manjunath,I.Mordatch,O.Nachum,C.Parada,J.Peralta,E.Perez,
TABLEIV:Humanexpertteleoperationsuccessrateusingstretch K. Pertsch, J. Quiambao, K. Rao, M. Ryoo, G. Salazar, P. Sanketi,
and our system for opening doors K.Sayed,J.Singh,S.Sontakke,A.Stone,C.Tan,H.Tran,V.Van-
houcke, S. Vega, Q. Vuong, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu,
and B. Zitkovich, “Rt-1: Robotics transformer for real-world control
In order to successfully operate various doors the robot atscale,”2023. 1
needs to be strong enough to open and move through them. [8] R.Yang,Y.Kim,A.Kembhavi,X.Wang,andK.Ehsani,“Harmonic
mobilemanipulation,”2023. 1,6
We empirically compare against a different popular mobile
[9] S. Yenamandra, A. Ramachandran, K. Yadav, A. Wang, M. Khanna,
manipulationsystem,namelytheStretchRE1(HelloRobot).
T. Gervet, T.-Y. Yang, V. Jain, A. W. Clegg, J. Turner et al.,
We test the ability of the robots to be teleoperated by a “Homerobot: Open-vocabulary mobile manipulation,” arXiv preprint
human expert to open two doors from different categories, arXiv:2306.11565,2023. 1,2
[10] A.Herzog,K.Rao,K.Hausman,Y.Lu,P.Wohlhart,M.Yan,J.Lin,
specificallyleverandknobdoors.Eachobjectwassubjected
M. G. Arenas, T. Xiao, D. Kappler et al., “Deep rl at scale: Sorting
to five trials. As shown is Table IV, the outcomes of these waste in office buildings with a fleet of mobile manipulators,” arXiv
trials revealed a significant limitation of the Stretch RE1: its preprintarXiv:2305.03270,2023. 1,2
[11] C. Sun, J. Orbik, C. Devin, B. Yang, A. Gupta, G. Berseth, and
payload capacity is inadequate for opening a real door, even
S.Levine,“Fullyautonomousreal-worldreinforcementlearningwith
when operated by an expert, while our system succeeds in applicationstomobilemanipulation,”2021. 1
all trials. [12] C.G.Atkeson,P.B.Benzun,N.Banerjee,D.Berenson,C.P.Bove,
X. Cui, M. DeDonato, R. Du, S. Feng, P. Franklin et al., “What
VI. CONCLUSION happenedatthedarparoboticschallengefinals,”TheDARPArobotics
challengefinals:Humanoidrobotstotherescue,pp.667–684,2018.
We present a full-stack system for adaptive learning in 1,2
[13] M.DeDonato,F.Polido,K.Knoedler,B.P.Babu,N.Banerjee,C.P.
open world environments to operate various articulated ob-
Bove,X.Cui,R.Du,P.Franklin,J.P.Graffetal.,“Teamwpi-cmu:
jects, such as doors, fridges, cabinets and drawers. The achievingreliablehumanoidbehaviorinthedarparoboticschallenge,”
system is able to learn from very few online samples since JournalofFieldRobotics,vol.34,no.2,pp.381–399,2017. 1,2
it uses a highly structured action space, which consists of [14] N. Banerjee, X. Long, R. Du, F. Polido, S. Feng, C. G. Atkeson,
M. Gennert, and T. Padir, “Human-supervised control of the atlas
a parametric grasp primitive, followed by a sequence of
humanoid robot for traversing doors,” in 2015 IEEE-RAS 15th In-
parametric constrained mobile manipulation primitives. The ternational Conference on Humanoid Robots (Humanoids). IEEE,
exploration space is further structured via a demonstration 2015,pp.722–729. 1,2
[15] S. Chitta, B. Cohen, and M. Likhachev, “Planning for autonomous
dataset on some training objects. Our approach is able to
dooropeningwithamobilemanipulator,”in2010IEEEInternational
improve performance from about 50 to 95 percent across ConferenceonRoboticsandAutomation. IEEE,2010,pp.1799–1806.
8 unseen objects from 4 different object categories, selected 1,2
[16] A. Jain and C. C. Kemp, “Behaviors for robust door opening and
frombuildingsacrosstheCMUcampus.Thesystemcanalso
doorwaytraversalwithaforce-sensingmobilemanipulator.” Georgia
learnusingrewardsfromVLMswithouthumanintervention, InstituteofTechnology,2008. 1,2
etar
sseccus[17] K. Nagatani and S. Yuta, “An experiment on opening-door-behavior IntelligentRobotsandSystems(IROS). IEEE,2022,pp.1647–1654.
byanautonomousmobilerobotwithamanipulator,”inProceedings 2
1995 IEEE/RSJ International Conference on Intelligent Robots and [35] Y. Zhao, Q. Gao, L. Qiu, G. Thattai, and G. S. Sukhatme, “Opend:
Systems. Human Robot Interaction and Cooperative Robots, vol. 2, A benchmark for language-driven door and drawer opening,” arXiv
1995,pp.45–50vol.2. 1,2 preprintarXiv:2212.05211,2022. 2
[18] L. Peterson, D. Austin, and D. Kragic, “High-level control of a [36] Z.Fu,X.Cheng,andD.Pathak,“Deepwhole-bodycontrol:learning
mobilemanipulatorfordooropening,”inProceedings.2000IEEE/RSJ aunifiedpolicyformanipulationandlocomotion,”inConferenceon
International Conference on Intelligent Robots and Systems (IROS RobotLearning. PMLR,2023,pp.138–149. 2,6
2000)(Cat.No.00CH37113),vol.3,2000,pp.2333–2338vol.3. 1,2 [37] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn,
[19] S.LevineandV.Koltun,“Guidedpolicysearch,”inICML,2013. 2 K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu et al., “Rt-1:
[20] S.Levine,C.Finn,T.Darrell,andP.Abbeel,“End-to-endtrainingof Robotics transformer for real-world control at scale,” arXiv preprint
deepvisuomotorpolicies,”JMLR,2016. 2 arXiv:2212.06817,2022. 2
[21] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, [38] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song,
D. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke et al., “Qt- J. Bohg, S. Rusinkiewicz, and T. Funkhouser, “Tidybot: Personal-
opt: Scalable deep reinforcement learning for vision-based robotic ized robot assistance with large language models,” arXiv preprint
manipulation,”arXivpreprintarXiv:1806.10293,2018. 2 arXiv:2305.05658,2023. 2
[22] D. Kalashnikov, J. Varley, Y. Chebotar, B. Swanson, R. Jon- [39] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David,
schkowski,C.Finn,S.Levine,andK.Hausman,“Mt-opt:Continuous C.Finn,K.Gopalakrishnan,K.Hausman,A.Herzogetal.,“Doasi
multi-task robotic reinforcement learning at scale,” arXiv preprint can,notasisay:Groundinglanguageinroboticaffordances,”arXiv
arXiv:2104.08212,2021. 2 preprintarXiv:2204.01691,2022. 2
[23] V.H.Pong,M.Dalal,S.Lin,A.Nair,S.Bahl,andS.Levine,“Skew- [40] R.Rusu,W.Meeussen,S.Chitta,andM.Beetz,“Laser-basedpercep-
fit: State-covering self-supervised reinforcement learning,” arXiv tionfordoorandhandleidentification,”072009,pp.1–8. 2
preprintarXiv:1903.03698,2019. 2 [41] J.Wang,S.Lin,C.Hu,Y.Zhu,andL.Zhu,“Learningsemantickey-
[24] R.Mendonca,S.Bahl,andD.Pathak,“Alan:Autonomouslyexploring pointrepresentationsfordooropeningmanipulation,”IEEERobotics
roboticagentsintherealworld,”inICRA,2023. 2 andAutomationLetters,vol.5,no.4,pp.6980–6987,2020. 3
[25] A. Kumar, A. Singh, F. Ebert, M. Nakamoto, Y. Yang, C. Finn, and [42] Y.Qin,B.Huang,Z.-H.Yin,H.Su,andX.Wang,“Dexpoint:Gener-
S. Levine, “Pre-training for robots: Offline rl enables learning new alizable point cloud reinforcement learning for sim-to-real dexterous
tasksfromahandfuloftrials,”arXivpreprintarXiv:2210.05178,2022. manipulation,”inConferenceonRobotLearning. PMLR,2023,pp.
2 594–605. 3
[26] L.Smith,J.C.Kew,X.B.Peng,S.Ha,J.Tan,andS.Levine,“Legged [43] Y. Urakami, A. Hodgkinson, C. Carlin, R. Leu, L. Rigazio, and
robots that keep on learning: Fine-tuning locomotion policies in the P. Abbeel, “Doorgym: A scalable door opening environment and
realworld,”2021. 2 baselineagent,”arXivpreprintarXiv:1908.01887,2019. 3
[27] R.Mendonca,S.Bahl,andD.Pathak,“Structuredworldmodelsfrom [44] A. Gupta, M. E. Shepherd, and S. Gupta, “Predicting motion plans
humanvideos,”2023. 2 for articulating everyday objects,” arXiv preprint arXiv:2303.01484,
[28] A. Kannan, K. Shaw, S. Bahl, P. Mannam, and D. Pathak, “Deft: 2023. 3
Dexterousfine-tuningforreal-worldhandpolicies,”CoRL,2023. 2 [45] R. J. Williams, “Simple statistical gradient-following algorithms for
[29] B. Wu, R. Martin-Martin, and L. Fei-Fei, “M-ember: Tackling long- connectionist reinforcement learning,” Machine learning, vol. 8, pp.
horizon mobile manipulation via factorized domain transfer,” arXiv 229–256,1992. 4
preprintarXiv:2305.13567,2023. 2 [46] X.Zhou,R.Girdhar,A.Joulin,P.Kra¨henbu¨hl,andI.Misra,“Detecting
[30] N. Yokoyama, A. W. Clegg, E. Undersander, S. Ha, D. Batra, and twenty-thousandclassesusingimage-levelsupervision,”inComputer
A.Rai,“Adaptiveskillcoordinationforroboticmobilemanipulation,” Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel,
arXivpreprintarXiv:2304.00410,2023. 2 October 23–27, 2022, Proceedings, Part IX. Springer, 2022, pp.
[31] S. Srivastava, C. Li, M. Lingelbach, R. Mart´ın-Mart´ın, F. Xia, K. E. 350–368. 5
Vainio, Z. Lian, C. Gokmen, S. Buch, K. Liu et al., “Behavior: [47] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson,
Benchmark for everyday household activities in virtual, interactive, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo et al., “Segment
and ecological environments,” in Conference on Robot Learning. anything,”arXivpreprintarXiv:2304.02643,2023. 5
PMLR,2022,pp.477–490. 2 [48] J. Kindle, F. Furrer, T. Novkovic, J. J. Chung, R. Siegwart, and
[32] M.Savva,A.Kadian,O.Maksymets,Y.Zhao,E.Wijmans,B.Jain, J. Nieto, “Whole-body control of a mobile manipulator using end-
J.Straub,J.Liu,V.Koltun,J.Maliketal.,“Habitat:Aplatformfor to-endreinforcementlearning,”2020. 6
embodiedairesearch,”inProceedingsoftheIEEE/CVFinternational [49] K.Rana,J.Haviland,S.Garg,J.Abou-Chakra,I.Reid,andN.Suen-
conferenceoncomputervision,2019,pp.9339–9347. 2 derhauf,“Sayplan:Groundinglargelanguagemodelsusing3dscene
[33] J.Wong,A.Tung,A.Kurenkov,A.Mandlekar,L.Fei-Fei,S.Savarese, graphsforscalabletaskplanning,”arXivpreprintarXiv:2307.06135,
andR.Mart´ın-Mart´ın,“Error-awareimitationlearningfromteleopera- 2023. 6
tiondataformobilemanipulation,”inConferenceonRobotLearning. [50] A.Radford,J.W.Kim,C.Hallacy,A.Ramesh,G.Goh,S.Agarwal,
PMLR,2022,pp.1367–1378. 2 G.Sastry,A.Askell,P.Mishkin,J.Clarketal.,“Learningtransferable
[34] M. Mittal, D. Hoeller, F. Farshidian, M. Hutter, and A. Garg, visual models from natural language supervision,” in International
“Articulated object interaction in unknown scenes with whole-body conferenceonmachinelearning. PMLR,2021,pp.8748–8763. 6
mobilemanipulation,”in2022IEEE/RSJInternationalConferenceon