1
Networked Multiagent Reinforcement Learning for
Peer-to-Peer Energy Trading
Chen Feng, and Andrew L. Liu
Abstract—Utilizing distributed renewable and energy storage framework ensures the feasibility of a distribution network.
resources in local distribution networks via peer-to-peer (P2P) Specifically, inspired by the algorithm in [1], we propose
energy trading has long been touted as a solution to improve
a consensus-based actor-critic algorithm for a decentralized
energysystems’resilienceandsustainability.Consumersandpro-
MARL, in which each agent in a repeated P2P auction is
sumers (those who have energy generation resources), however,
do not have the expertise to engage in repeated P2P trading, modeled as a Markov decision problem (MDP). By allowing
and the zero-marginal costs of renewables present challenges agents to exchange information and reach agreements, the
in determining fair market prices. To address these issues, we proposed framework facilitates efficient decision-making and
proposemulti-agentreinforcementlearning(MARL)frameworks
resource allocation, while mitigating the computational and
to help automate consumers’ bidding and management of their
privacychallengesassociatedwithcertaincentralizedlearning
solar PV and energy storage resources, under a specific P2P
clearing mechanism that utilizes the so-called supply-demand approaches. In addition, the shared network constraints, such
ratio. In addition, we show how the MARL frameworks can asvoltageregulation,canbelearnedthroughthedecentralized
integrate physical network constraints to realize voltage control, approach with a constraint violation incurring a (fictitious)
henceensuringphysicalfeasibilityoftheP2Penergytradingand penalty to each agent’s reward function. We consider this
paving way for real-world implementations.
development a significant step towards practical real-world
implementations of P2P energy trading.
I. Introduction
In addition to the modeling and algorithm development,
As our society strives to transition towards sustainable energy we establish the theoretical conditions for the consensus-
sources, distributed renewable resources and energy storage MARL algorithm to converge to an asymptotically stable
are increasingly seen as key components in creating resilient equilibrium. We consider this a significant advantage over a
and sustainable energy systems. Peer-to-peer (P2P) energy purelydecentralizedapproachwithoutagents’communication,
tradinginlocalenergymarketsoffersapromisingapproachto such as the one in our previous work [2], in which each agent
decentralize and optimize the allocation of distributed energy uses the proximal policy optimization (PPO) approach [3] to
resources (DERs). This model not only empowers consumers solve their own MDP and ignores multiagent interaction.
and those who can store or generate energy, known as pro- Through numerical simulations, we compare three differ-
sumers, to trade energy directly but also promotes renewable ent frameworks to implement P2P trading while considering
energy usage and reduces energy losses. network constraints: purely decentralized trading using PPO,
However, implementing P2P trading presents significant consensus-MARL, and a centralized learning and decentral-
challenges. First, consumers and prosumers lack the technical izedexecutionapproachusingthemultiagentdeepdeterminis-
expertise required to engage in repeated P2P trading and ticpolicygradient(MADDPG)algorithm[4].Ourfindingsin-
manage their energy resources efficiently. Second, renewable dicate that consensus-MARL achieves higher average rewards
energy, such as solar, often has zero marginal costs, which compared to both PPO and MADDPG.
introduces difficulties in determining fair market prices for The rest of the paper is organized as follows. Section II
energy trades: a uniform-pricing auction resembling a whole- reviews the literature on P2P energy trading and emphasizes
sale energy market would not work since the market clearing thecontributionofourwork.SectionIIIintroducesthemarket
prices would be zero most of the time. Third, while P2P clearing mechanism based on the supply-demand ratio, first
trading only entails financial transactions, the delivery of proposedin[5],whichcanavoidzero-clearingpriceswhenthe
energy occurs over the physical distribution networks. How supplyresourceshavezeromarginalcosts.SectionIVprovides
to maintain network feasibility in a local P2P trading market the detailed consensus-MARL formulation and algorithm, as
is an important yet open question. well as establishing convergence results. The specific setups
In response to these challenges, this paper introduces a and data of our simulation are described in Section V, along
decentralized framework utilizing multiagent reinforcement with numerical results. Finally, Section VI summarizes our
learning (MARL), which is designed to automate the bidding work and points out several future research directions.
processes of agents, enhancing the management efficiency of
II. Literature Review
their solar PV and energy storage resources. Additionally, the
The literature on decentralized control of energy transac-
Chen Feng is with School of Industrial Engineering, Purdue University, tions within distribution networks is extensive and diverse.
WestLafayette,IN,USA,email:feng219@purdue.edu.
Broadly speaking, there are two predominant approaches:
AndrewL.LuiswithSchoolofIndustrialEngineering,PurdueUniversity,
WestLafayette,IN,USA,email:andrewliu@purdue.edu. the distributed optimization approach and P2P bilateral trad-
4202
naJ
52
]YS.ssee[
1v74931.1042:viXra2
ing. One of the most prominent frameworks in the former In [17], a MADDPG algorithm tailored for a double-sided
approach is through the alternating direction of multiplier auction market to realize P2P energy trading is proposed,
method (ADMM). The ADMM solves centralized economic though they do not consider physical constraints. While the
dispatch/optimalpowerflowproblemsinadistributedmanner CTDE framework potentially enables more efficient learning
by sending price signals (the dual variables of supply-demand and robust outcomes, its reliance on a centralized entity for
constraints) for consumers/prosumers to independently opti- aggregating all agents’ information and coordinating their
mize their solutions, which are then sent back to the central trainingintroducesvulnerabilities.Specifically,itincreasesthe
optimizer to update the price signals (for example, [6], [7]). riskofsingle-pointfailuresandsusceptibilitytocyber-attacks.
While this iterative approach enjoys rigorous convergence To overcome the drawbacks of fully decentralized and
guaranteesunderconvexity,itspracticalapplicationencounters CTDE-based MARL, algorithms with limited communication
notable challenges: (i) As an algorithmic approach rather than withinagentshavealsobeendeveloped.Anotablecontribution
a market design, ADMM is subject to the same difficul- in this area is the consensus-based MARL algorithm designed
ties faced by a uniform-pricing-based market with all zero- fordiscrete-spaceproblems,aspresentedin[18],anditsexten-
marginal-cost resources. Specifically, it is prone to ‘bang- sion to continuous-space problems in [1]. This model allows
bang’ pricing issues – market clearing prices plummet to agentstolearnandimplementpoliciesinadecentralizedman-
zero in oversupply or spike to a price ceiling in undersupply. ner with limited communication, underpinned by a theoretical
Addressing this with additional market mechanisms, such guaranteeofconvergenceundercertainconditions.Separately,
as adding auxiliary and operating reserve markets, akin to [19]introducedaScalableActorCritic(SAC)framework,par-
wholesale markets, could overly complicate distribution-level ticularly relevant for large-scale networks where an individual
markets for energy trading. (ii) ADMM and most of the agent’s state transition is influenced only by its neighbors.
distributed optimization algorithms are designed for static op- SAC is distinguished by its reduced communication needs
timization,lackingthedynamismrequiredforreal-timeadapt- compared to the CTDE framework and offers a theoretical
ability,hencenecessitatingonlineversionsofsuchalgorithms. guarantee for its policy convergence as well. However, the
Online ADMM algorithms have been proposed in [8]. Even SAC framework is only established for discrete state space
so, network constraints are only guaranteed at the algorithm problems, and its extension and effectiveness in continuous-
convergence. How to guarantee the physical constraints in a state space applications remain unexplored.
distributed fashion within each iteration is an open question. In this work, we present several key contributions: First,
(iii) The expectation that consumers or prosumers can solve we develop a comprehensive framework for a P2P energy
optimization problems might be unrealistic due to a lack of trading market while integrating a consensus-based MARL
expertise or resources. algorithm. This framework facilitates automated control and
Theselimitationsformthebasisofourinterestinexploring bidding of DERs while allowing for decentralized learning of
P2P bilateral trading. Consequently, our literature review will voltage constraints. Second, we present sufficient conditions
be primarily focused on this alternative approach. There have requiredfortheconvergenceoftheMARLalgorithmandshow
been multiple review papers on the large and ever-growing the details of implementing this algorithm using deep neural
literatureonP2Penergytrading[9]–[13].Withintheliterature, networks.Third,weconductacomparativeanalysisofmarket
we focus on the works that explicitly consider distribution outcomesderivedfromthreedistinctMARLstrategies:anaive
networkconstraints.[14]proposesacontinuousdoubleauction decentralization approach, the MADDPG approach, and the
framework within which physical network constraints are proposedconsensus-basedMARL.Ourfindingsrevealthatthe
maintained through a sensitivity-based approach. The agents consensus-based approach notably outperforms the others in
in the auction are the so-called zero-intelligence-plus (ZIP) achieving better market outcomes.
traders,whoemploysimpleadaptivemechanismswithoutany
III. Market Clearing Mechanism
learningormemoryoftherepeatedmulti-agentinteractions.in
which they propose sensitivity-based More sophisticated than Inthissection,wefirstdescribeaclearingmechanism,referred
ZIP, a fully decentralized MARL is adopted in [15] to realize to as the supply-demand-ratio (SDR) mechanism [5], that
decentralizedvoltagecontrol,anditisextendedin[2]toform can address the two potential issues faced by a renewable-
a P2P energy trading market with physical constraints. These dominated P2P market: (i) marginal-cost-based pricing yield-
two papers implement a completely decentralized approach in ing zero market prices most of the time, and (ii) the potential
the sense that each agent ignores the multi-agent interaction bang-bang outcomes in a double-auction-based mechanism as
as well and just learns their single-agent policy through the the reserve prices for buyers (the utility rates) and the sellers
Proximal Policy Optimization (PPO) algorithm [3] to maxi- (usually the feed-in tariffs) are publicly known. The second
mize their own payoff. issue is unique to the P2P energy market, in our opinion, and
AnotherMARLapproachthathasbeenexploredisthecen- the bang-bang phenomenon is well-documented in [20] and
tralized training and decentralized execution (CTDE) frame- will be explained further later in this section.
work. The multi-agent deep deterministic policy gradient The SDR-clearing mechanism in P2P markets has been
(MADDPG) method from [4] was applied in [16] to solve detailed in our earlier work [2]. For completeness, we briefly
the autonomous voltage control problem, and a multi-agent revisit it here. Let I = 1,2,...,I represent all consumers
constrained soft actor-critic (MACSAC) algorithm inspired and prosumers, collectively termed as agents. We assume that
MADDPG is developed to perform Volt-VAR control (VVC). trading among the agents takes place in fixed rounds (such3
as hourly or every 15 minutes) and is cleared ahead of time
(such as day-ahead or hour-ahead). In each trading round t,
each agent i ∈ I submits bids (b ) to buy (if b < 0) or
i,t i,t
sell (if b > 0) energy. Agents can switch roles between
i,t
buyers and sellers across different rounds, but not within the
same round. For a round t, the sets of buyers and sellers are
denoted as B = {i : b < 0} and S = {i : b ≥ 0},
t i,t t i,t
respectively.
The SDR mechanism, originally proposed in [5], is an
approach to clear a market with all zero-marginal-cost supply
resources, which is straightforward to implement. An SDR is
defined as the ratio between total supply and demand bids at
t; that is,
(cid:30)
(cid:88) (cid:88)
SDR := b − b . (1)
t i,t i,t
Figure 1: Market clearing using SDR
i∈St i∈Bt
To ensure that SDR is well-defined, we assume B ̸= ∅ for
t t
any t, a reasonable assumption given agents’ continuous need
the special characteristics of such a market. Since uncleared
forenergy.Inrarecaseswhereeveryagentisaprosumerwith electricity demand bids have to be bought at the rate of UR,
excess energy at time t, the P2P market can be temporarily andunclearedenergysupplybidshavetobesoldatFIT,with
suspended, and surplus energy sold to the grid at a predefined the UR and FIT known to all market participants. With all
price, which does not impact our model or algorithm frame-
zero-marginal cost resources, agents would not know how to
work. Conversely, if S is ∅, indicating no excess energy for
t price their bids in a marginal-cost-based clearing mechanism,
sale at t, then SDR t =0. and the public information of the UR and the FIT naturally
Basedonitsdefinition,whenSDR
t
>1,itmeansthatthere
provides the focal points for sellers to bid at UR and buyers
isanoversupply.Inthiscase,weassumethattheexcessenergy to bid at FIT, aiming to maximize their own payoffs. This
in the P2P market is sold to a utility company, a distribution
wouldleadtoasimilarbang-bangtypemarketoutcome:when
systemoperator(DSO),oranaggregatoratapre-definedrate,
thereisoverdemand,themarketpricewouldbeUR;reversely,
generally denoted as FIT (feed-in tariff).1 If 0≤SDR <1,
t when there is oversupply, the price would be FIT. We view
indicating overdemand in t, then the unmet demand bids will
the SDR approach as a fair and reasonable alternative to
purchase energy at a pre-defined utility rate, denoted as UR.
the marginal-cost-based approach to avoid such bang-bang
Without loss of generality, we assume that FIT <UR.2
outcomes, especially for a new market in which the supply
The SDR mechanism determines the market clearing price
resources are likely much less than demand.
as follows:
TheSDRmethodofferstwokeybenefits:First,itmaintains
P :=P(SDR ) themarketclearingpricebetweenURandFIT,ensuringthat
t t
(cid:40) transactions for buyers and sellers are at least as favorable
(FIT −UR)·SDR +UR, 0≤SDR ≤1;
:= t t (2) as dealing directly with the utility. Second, it simplifies the
FIT, SDR t >1. bidding process by requiring only quantity bids. This allows
consumers with flexible loads to concentrate on adjusting
In (2), P denote the market clearing price in round t, which
t
their demand timings and prosumers to better manage their
is a piece-wise linear function with respect to the SDR , as
t
energy storage and sales strategies, avoiding the complexity
illustrated in Figure 1.
of submitting price-quantity pair bids.
While the SDR approach may be criticized for not being
based on economic theories or for being somewhat arbitrary IV. Decentralized Learning in P2P Trading with
(for example, instead of having a linear function, the pricing Consensus Updates
functioncouldbereplacedbyanydownward-shapednonlinear
functions,whichwouldleadtodifferentmarketpriceswiththe Inthissection,wefirstformulatethedecision-makingproblem
same SDR), we want to point out that from our perspectives, faced by each agent in a repeated P2P trading market as a
the simplicity and transparency of the SDR mechanism out- Markov decision process. (For the sake of brevity, we only
weigh the criticisms, and it is well-suited for a P2P energy describe a prosumer’s problem. A pure consumer’s problem
market with all-zero marginal cost resources. In our previous is the same without the supply bids.) We then introduce the
work [20], it is identified that a P2P energy market using details of the consensus-MARL algorithm.
traditional double-sided auction may not work well due to A. Markov Decision Process for a Prosumer
In our setup, we assume that each prosumer locates at one
1TheFIT rateisaratesetbyutilities/policymakers.Itcanbesetaszero
bus of the distribution network and has two resources under
andwillnotaffectthepricingmechanisminanyway.
2IfFIT >UR,itmeansthatenergyconsumerspaymorethantheutility their control: a solar photovoltaic (PV) system with a smart
rate to purchase energy from the prosumers, which is equivalent to a direct inverter, and a rechargeable battery as energy storage. Each
subsidy from energy consumers (including low-income consumers who do
prosumer also has a fixed baseload of energy consumption.
nothavethemeanstoinvestinDERs)toprosumers.Thiscannotbejustified
fromanequityperspective. At a particular time period t, whether a prosumer is a net4
energy seller or a buyer depends on their baseload versus the A = Aq ×Ae, where aq is the reactive power injected or
i i i i,t
level of PV generation and energy storage. absorbedbyagenti’ssmartinverter,andae isenergycharged
i,t
Duetothepresenceofenergystorage,eachagent’sbidding (ifae >0)toordischarged(ifae <0)fromthebattery.The
i,t i,t
and charging decisions are linked over time, which are natu- feasible action spaces of aq and ae are generically denoted
i,t i,t
rally modeled in a dynamic programming framework. Since by Aq and Ab, respectively. In the specific context of our
i i
an agent’s decisions at t only depend on its energy storage model,itissafetoassumethattheactionspaceA =Aq×Ae
i i i
level at t − 1 and the PV generation in t, a discrete-time isbounded.Inaddition,agenti’sactualbid(eitherbuyorsell)
Markov decision process (MDP) model is suitable here. In to a P2P market at time t can be written as
the following, we describe the key building blocks of each (cid:40) PV −dp −min(ae ,ei−ei,t), if ae ≥0,
age On bt’ ss erM vaD tiP onm ao nd del s.
tate variables:Agent i∈I attimestep t
b
i,t
= PVi,t −dpi,t −max(ai e,t ,−eη ic
·ηd),
othei r,t
wise,
i,t i,t i,t i,t i
is assumed to receive the system states s =(s ,...,s )∈ (3)
t 1,t I,t
S :=ΠI S ,whichistheconcatenationofthepersonalstates which is simply the net energy of PV generation minus
i=1 i
of each agent, where I is the total number of agents in the baseload demand (of real power) and charge to the battery.
system. Specifically, the personal states of agent i are defined State transition: Out of the state variables defined earlier,
as s := (dp ,dq ,e ,PV ) ∈ S , where dp and dq are the energy storage level e is the only one that is directly
i,t i,t i,t i,t i,t i i,t i,t i,t
the inflexible demand of active and reactive power of agent i, affected by an agent’s own action. The state transition for e
i,t
e is the state of charge of agent i’s energy storage system, can be written as follows:
i,t
and PV is the PV (real) power generation in t.
i,t
Among the state variables, (dp i,t,dq i,t,PV i,t) are only obser- e i,t+1 =
vationsbecausetheirtransitionsadheretounderlyingdistribu- (cid:110) (cid:104) 1 (cid:105) (cid:111)
max min e +ηcmax(ae ,0)+ min(ae ,0),e ,0 ,
tions that are independent of agents’ actions. In contrast, the i,t i i,t ηd i,t i
i
transitionsofe areinfluencedbyagents’actions,withdetails (4)
i,t
elaborated further below after agents’ actions are defined.
The assumption that each agent is required to observe the where η ic and η id are the charging and discharging efficiency
wholesystem’sstatemayappeartobestronginadecentralized ofagenti’sbattery,respectively,ande i isthebatterycapacity.
setting and is prone to criticisms and privacy concerns. Our The charging efficiency represents the ratio of the amount of
responses are threefold: first, the identity of the agents is energy effectively stored in the system to the energy input
irrelevant, and hence, from an agent’s perspective, which during the charging process; while the discharging efficiency
state variable corresponds to which specific agent is not is the ratio of the energy delivered by the system during
known. The state information can simply be labeled as Agent discharge to the energy that was initially stored in it. Their
1, 2, ..., I, without revealing the agent’s identity. Second, product, η ic × η id, yields the so-called round-trip efficiency.
the proposed algorithms are intended to be implemented on Theoutside“max”operationin(4)istoensurethattheenergy
devices that can receive information from the grid (generally level in the battery will not be negative; while the first “min”
referring to a utility or a distributed system operator) and operation in the bracket is to ensure that the battery storage
automatically participate in the repeated P2P market without capacity is not exceeded.
humanintervention(akacontrolautomation).Theinformation Reward: Each agent i’s reward function is affected by both
received by the devices should be encrypted and can only be the collective actions of the agents and the system states.
decrypted by the control-automation devices. Granted that the Specifically, agent i’s reward in time step t, denoted by r i,t,
devicescouldbecompromised,therehavebeenworksonhow is a random variable whose conditional expectation has two
to address adversarial attacks in a multi-agent reinforcement components as follows:
learning framework [21], which leads to interesting future
E[r |s ,a ]:=R (a ;a ,s )
research directions. Second, one of our main goals in this i,t t t i i,t −i,t t
work is to establish rigorous convergence results for the =R im(ae i,t;ae −i,t,s t)+Rv(a i,t;a −i,t,s t)/I.
proposed MARL algorithm. In [2], we proposed a completely (5)
decentralized framework in which each agent just uses their
In (5), the notation a refers to the collection of all other
local state information to update the policies and solve their −i,t
agents’ actions, excluding agent i’s; while ae refers to only
own MDP problem as if in a single-agent environment. The −i,t
theenergycharge/dischargedecisionsoftheotheragents.The
numerical results are surprisingly good in the sense that they
first term Rm in (5) is the energy purchase cost or sales profit
all indicate convergence towards some steady states as the i
of agent i from the P2P energy market (with the superscript
learningcontinues.However,notheoreticalresultscouldbees-
‘m’ standing for ‘market’), whose formulation is as follows:
tablishedwheneachagentonlyuseslocalinformation,without
communication or shared reward functions. Our view is that  I ×(cid:104) SDR ·P ·b + (1−SDR )·UR·b (cid:105)
tobeabletorigorouslyshowaMARLgame’sconvergenceto
 i∈Bt
(cid:16)
t t
(cid:17)
i,t t i,t
a steady state, there has to be some global information shared R im := + I i∈St × P t·b i,t , if 0≤SDR t ≤1,
am Ao cn tg iot nhe orag coen nt ts ro.
lvariables:attimet,theactionsthatagent
FIT
·b i,t, if SDR
t
>1,
i can take are represented by a vector a := (aq ,ae ) ∈ (6)
i,t i,t i,t5
where I is an indicator function such that I = 1 if equations(7)arewithinthevoltagelimitof[V ,V ]onevery
i∈Bt i∈Bt k k
i ∈ B , and 0 otherwise. Similarly, I = 1 if ∈ S and bus k = 1,...,N, then Rv = 0; if there is voltage violation
t i∈St t
0 otherwise. SDR is the supply-demand-ratio as defined in at a certain bus k, the term max(|V | − V ,V − |V |)
t k,t k k k,t
(1), which depends on all agents’ bids, and the bids further becomespositive,andRv becomesanegativenumberwithan
dependoneachagent’scharging/dischargingdecisions,aswell arbitrary positive number λ to amplify the voltage violation.
as the PV generation and the baseload, as specified in (3). P The other positive parameter in (7), M, is just to ensure that
t
in (6) is the market clearing price as defined in (2). Note that therewardRv isboundedbelow,atechnicalconditionneeded
when SDR < 1, not all demand bids will be cleared in the to ensure the convergence of the consensus MARL algorithm
t
P2P market at time t. In such instances, they will need to to be introduced in the next section.
purchase the needed energy at the utility rate, leading to the Ifthesystemofequations(7)doesnothaveasolutionwith
UR·b termwhendefiningtherewardRm inequation(6).To agivensetofagents’bids,wecansimplysetRv =−λ·N·M.
i,t i
ensure fairness and prevent any inadvertent favoritism among Wewanttoemphasizethatthevoltageviolationdoesnotresult
thedemandbids,weemployamechanismwhereeachdemand in actual financial penalties for the agents since the market
bid is partially cleared. Specifically, the clearance proportion clearinghappensbeforereal-timedelivery.Itsimplyrepresents
is exactly SDR (which represents the percentage of total feedback corresponding to agents’ bids and is used by each
t
demandbidscleared),andtheunclearedbidforeveryagentis agent to train their reinforcement learning policies.
then (1−SDR t)·b i,t. This approach of proportional scaling After market-clearing and solving for voltage magnitudes,
echoes the rules used in multi-unit double auctions in [22]. thetotalrewardforagentiattimesteptisrealized,asdefined
The second term in the reward function, Rv, is the system in (5). Note that while we do not explicitly assume to have a
voltage violation penalty at a given time t. To provide an DSO in the distribution network, an entity is still needed to
explicit form of Rv, we first write out the standard bus- solveEquation(7).Autilitycompanycanassumesucharole.
injection model for all k = 1,2,...,N (the time index t is The solution process may even be carried out by a distributed
omitted for simplicity): ledger system, such as on a blockchain.
N Objectivefunction:Ateachtimestept,agentireceivesthe
(cid:88)
p k = |V k||V j|(G kjcos(α k−α j)+B kjsin(α k−α j)), system state s t, chooese an action a i,t ∈ A i, and receives a
rewardr .Attimet,agenti’sdecision,giventhesystemstate
j=1 i,t
(cid:88)N s t ∈S
=(cid:81)I
i=1S i, is determined by the policy π θi(·|s). This
q = |V ||V |(G sin(α −α )−B cos(α −α )). policyisadensityfunctionmappingfromA to[0,∞),andit
k k j kj k j kj k j i
j=1 is parameterized by θ i. Here, θ i is an element of a generic set
(7) Θ
i
∈Rmi,wherethedimensionalitym iisdeterminedbyeach
In the above system of nonlinear equations, p
k
and q k, agent.Inchoosingm i,eachagentfacesatradeoff:itshouldbe
k = 1,...,N are input data, where p is the net real large enough to avoid underfitting, yet not so large as to risk
k
power injection (or withdrawal) at bus k (summation of the overfitting and increased training time. Let θ = [θ 1,··· ,θ I]
energy amount bid from all agents at bus k per unit time, i.e. betheconcatenationofθ i,andπ
θ(·|s)=(cid:81)I
i=1π θi(·|s)bethe
p =(cid:80) b , with b given in (3) and N denoting the set joint policy of all agents. In the following discussion of this
ofk agenti s∈ aN tk bui s k), ani d q k is the net reactk ive power flow at section, we assume that the Markov chains {s t} t≥0 induced
αbu ks fk or(i k.e. =q k 1,= ..− .,(cid:80) Ni ,∈ wN ik thdq i |V+ ka |q i b) e. iT nghe thv ear via ob ll te as gear me a|V gk n| ituan dd
e
b wy ithev aer sy taa tig oe nn at rs y’ c do isll te ric bt uiv tie onpo ρli θc .y π θ where θ ∈Θ is ergodic
and α being the phase angle at bus k. The terms G and Differentfromafullydecentralizedframework,thegoalfor
k kj
B areparametersthatrepresenttherealandimaginaryparts each agent in the consensus-update framework is to solve the
kj
of the admittance of branch k-j respectively. following optimization problem:
reaA lt aa ndgi rv ee an ctr io vu en pd ot wo ef rt ih ne jeP c2 tiP ontr /wad ii tn hg d, raf wor ala ag siv te hn es re et so uf ltth oe
f sup J(θ)=E
(cid:34)
lim
1 (cid:88)T (cid:32) 1(cid:88)I
r
(cid:33)(cid:12) (cid:12)
(cid:12)s ,π
(cid:35)
,
agents’bidding(p k,q k)N k=1,ifthesystemofequations(7)has θ∈Θ:=ΠI i=1Θi s0∼ρθ T→∞T t=0 I i=1 i,t (cid:12) (cid:12) 0 θ
a solution,3 then we can define the voltage violation penalty (9)
as follows:
N to optimize the average system long-term reward. The ob-
(cid:88) (cid:104) (cid:105)
Rv :=−λ clip max(|V |−V ,V −|V |),{0,M} . jective function resembles a cooperative game, which may
k,t k k k,t
k=1 face similar critiques regarding the need for each agent to
(8) access system-wide state variables in a decentralized setting.
A pertinent response, as previously mentioned, is the use of
In (8), V ,V represent the upper and lower limit of bus
k k
control automation: the objective function can be integrated
k’svoltagemagnitude,respectively.Letf denoteageneric
clip
into intelligent control devices, making them ‘black boxes’
‘clip’ function in the form of f = clip[x,{v ,v }],
clip min max
forend-users.Additionally,indevelopingaMARLframework
which is defined as f = x when v ≤ x ≤ v ,
clip min max
with provable convergence, some level of global information
f = v if x < v , and f = v if x > v .
clip min min clip max max
sharing among agents seems necessary. The collective objec-
Hence, if the voltage magnitudes |V | obtained from solving
k,t
tive function, as outlined in (9), is essential for decentralized
3Sufficientconditionsunderwhich(7)hasasolutionareprovidedin[23]. agents to reach a consensus.6
To ensure that the objective function in (9) is well-defined, Let ωi denote a temporary local parameter of Qˆ(·,·;ωi) for
(cid:101)t t
we have the following result. agent i, and it is updated as:
Lemma IV.1. The supreumum in (9), sup J(θ), exists and is ωi =ωi+β ·δi·∇ Qˆ(cid:0) s ,a ;ωi(cid:1) . (13)
θ (cid:101)t t ω,t t ω t t t
finite.
The local parameter of the policy π is updated as
θi,t
Proof. For each agent i=1,...,I, since the market clearing
price P t and the agent’s bid b i,t are bounded above (by their θ i,t+1 =Γi[θ i,t+β θ,t·I(cid:98) tQ,i], (14)
P reV spea cn td iveb lya ,tt te hr ey fic ra sp ta tec rit mies i) n, aa ncc ao gr ed ni tn ’g
s
reto waE rdq. fu( n2 c) tia on nd
,
R(3 im),
,
w Ah ner ee xaΓ mi pl:
e
iR sm thi e→ orthΘ oi go⊂ nalR pm roi jei cs toa
r
mpr ao pj pe ic nto gr
:
Γop i(e xra )to ≜r.
is then bounded above based on Eq. (6), for any a ∈ A := argmin∥y−x∥∀x∈Rmi where Θ is a compact convex set.
ΠI A and s ∈ S. The second term, Rv, is bounded above
i=1 i y∈Θi
by0basedonitsdefinitioninEq.(8).Hence,agenti’sreward The I(cid:98)Q,i term in (14) is given as follows:
t
r is bounded at any t, and for any a∈A and s∈S. Since
thi, et
number of agents is finite, there exists a common upper
I(cid:98) tQ,i =
(cid:90)
bound for all r i,t’s, and let us denote it as R. Then by the dπ (cid:0) ai |s (cid:1) ∇ logπ (cid:0) ai |s (cid:1) Qˆ(cid:0) s ,ai,a−i;ωi(cid:1) .
formulation in (9), J(θ) ≤ R for all θ ∈ Θ. By the well- θi,t t θi θi,t t t t t
Ai
known least-upper-bound property of real numbers, we know (15)
that sup J(θ) exists and is finite.
θ To limit communication among agents, each agent i only
B. Consensus-update Actor-critic Algroithm shares their local parameter ω (cid:101)ti with the others; no other
In this section, we introduce the consensus-update actor-critic information needs to be shared. A consensual estimate of
algorithm for MARL with continuous state and action spaces, Q θ(s,a)canthenbeestimatedthroughupdatingtheparameter
developed in [1], and apply it to solve Problem (9). First, ω ti +1 as follows:
define the global relative action-value function Q (s,a) for a
θ I
(cid:88)
given a∈A, s∈S and a policy π θ as: ω ti
+1
= ω (cid:101)tj. (16)
∞ (cid:34) I (cid:35) j=1
(cid:88) 1(cid:88)
Q (s,a):= E r −J(θ)|s =s,a =a,π .
θ I i,t 0 0 θ To apply such an algorithm to the P2P energy trading, we
t=0 i=1 illustrate the conceptual framework in Fig. 2. The detailed
(10)
algorithm is presented in Algorithm 1.
Note that since all the agents share the same J(θ), the Q In the next section, we present the conditions under which
functiondoesnotneedtohaveanagentindexandisthesame Algorithm 1 can be shown to converge to a steady state.
across the agents.
C. Convergence Results
The function Q (s,a) cannot be calculated by each agent,
θ In this section, we discuss the convergence results of Algo-
even if the global state and action information are shared,
rithm 1. For ease of notation, we drop the t index of the
since the joint policy distribution π is not known. Instead,
θ parameters θ for i∈I in the following discussions.
each agent uses Qˆ(·,·;ωi):S ×A→R, which is a class of i
functionparametrizedbyωi ∈RK,whereK isthedimension Definition IV.1. (Geometricergodicity,referredtoasuniform
of the ωi, to approximate the action-value function Q (s,a). ergodicity in [24]) A Markov chain having stationary distri-
θ
Note that unlike the parameters θ , where agents can choose bution π(·) is geometricly ergodic if
i
their own policy models (and hence different dimensions of
||Pn(x,·)−π(·)||≤Mcn, n=1,2,3,...
θ ), the dimension of ωi has to be the same across all agents
i
(and hence K does not have an agent index) to facilitate the for some 0 < c < 1 and 0 < M < ∞, where P is the
consensus update. transition kernel of the Markov chain.
The decentralized consensus-update actor-critic algorithm
The concept of geometric ergodicity specifies the rate at
fornetworkedmulti-agentsworksasfollows:atthebeginning
which a Markov chain converges to a stationary distribution
of time step t + 1, each agent i observes the global state
if it exists. In the following, we state the first assumption to
s and chooses their action a according to their own
t+1 i,t+1
ensure the consensus MARL convergence to a steady state,
policyπ (referredtoasthe‘actor’),whereθ isthepolicy
θi,t i,t
which requires the Markov chains formed by the system
parameter for agent i at time step t. Then each agent i will
state and state-action pairs to be both geometrically ergodic.
receive the joint action a = (a ,...,a ) and their
t+1 1,t+1 I,t+1
ownrewardr ,whichhelpthemupdateQˆ(·,·;ωi)(referred Granted that this assumption cannot be easily verified, it is
i,t+1 t
essential to the convergence proof.
to as the ‘critic’) and π on their own in each time step.
θi,t
Specifically, each agent first updates the temporal-difference Assumption1. Foranyi∈{1,...,I},s∈S andθ ∈Θ ,an
i i
(TD) error δi and long-term return estimate r¯i as follows: agent i’s policy, aka the conditional probability density over
t t+1
δ ti :=r i,t+1−r¯ ti +1+Qˆ(cid:0) s t+1,a t+1;ω ti(cid:1) −Qˆ(cid:0) s t,a t;ω ti(cid:1) t ahe ∈ac Atio .n As dp da itc ie onA ai ll, yi ,s πpos (i ·ti |ve s; )t ih sa at si ss u, mπ eθ di( ta oi b| es c) o> nti0 nufo or usa ll yl
(11) i i θi
differentiablewithrespecttotheparameterθ overΘ .Finally,
r¯i ←(1−β )·r¯i+β ·r (12) i i
t+1 ω,t t ω,t i,t the Markov chains {s t}
t≥0
and {(s t,a t)}
t≥0
induced by the7
Distribution
network & P2P
market
Action 1 Action 2
Agent 1’s Agent 2’s
reward & joint reward & joint
actions actions
1
2 Actions 2:
Actions 1:
Reactive power production,
Reactivepowerproduction,
charging/discharging
charging/discharging
Actor 1 Critic 1 Critic 2 Actor2
(Policy) (Value (Value (Policy)
function) function) Global state variables
Globalstatevariables
The PV power generation,
The PV power generation,
Active/reactivepowerofbase
Active/reactive power of base
load, voltage magnitude, SOC of load, voltage magnitude, SOC of
all the agents
all the agents Consensus update: take
averageoftheparameters
ofits own and neighbor’s
critic networks
Figure 2: Consensus MARL Framework for the Voltage Control with P2P Market
agents’ collective policies π are both geometrically ergodic, Remark 3. One exmaple of the action-value function approx-
θ
with the coressponding stationary distribution denoted by ρ imation is the Gaussian radial basis function (RBF):
θ
and ρ˜, respectively.
θ K
Remark 1 The first part of Assumption 1 is a standard
Qˆ(s,a;ω)=(cid:88)
ω ie−γj∥[s,a]−cj∥, (20)
assumption on the policy function. One example of such a j=1
policy is Gaussian, i.e., π (· | s) ∼ N (η (s),Σ ), where
η (s) : S → A ∈ Rn
isθi
a fully
connectedθi neurali
network
where [s,a] is the concatenation of s and a, γ
j
∈ R+ for
pθ ai rametrized byi θ and is continuously differentiable with j = 1,··· ,K, c j ∈ R|S|+|A| for j = 1,··· ,K. γ j and
i c can be chosen arbitrarily, as long as (γ ,c ) are different
respect to θ and s, and Σ ∈Rn×n is the covariance matrix. j j j
i i with different j so that the feature functions are linearly
Assumption 2. The policy parameter θ for each agent is independent.
i
updated by a local projection operator, Γi : Rmi → Θ
i
⊂
Assumption 4. The stepsizes β and β in Algorithm 1
Rmi, that projects any θ
i
onto a compact set Θ i. In addition,
satisfy
ω,t θ,t
Θ=(cid:81)I
Θ includes at least one local minimum of J(θ).
i=1 i
(cid:88) (cid:88) (cid:88)
β = β =∞, and (β2 +β2 )<∞. (21)
Remark 2 The requirement of the local projection operator ω,t θ,t ω,t θ,t
is standard in convergence analyses of many reinforcement t t t
learningalgorithms.IfΘ iisaconvexset,aqualifyingexample Also, β
θ,t
=o(β ω,t) and lim t→∞β ω,t+1·β ω− ,1
t
=1.
of Γi can be the nearest point projection on Θ ; that is,
i
An example of such stepsizes can be β = 1/t0.8 and
ω,t
Γi(θ i)=arg max ∥θ i−θ i∗∥ 2. (19) β θ,t =1/t.
θi∗∈Θi With the above assumptions, the convergence of Algorithm
Next, we make an assumption on the action-value function 1 is given below.
approximation.
Theorem IV.1. Assume that each agent’s state space S is
i
Assumption 3. For each agent i, the action-value func- compact for i = 1,...,I. Under Assumptions 1 – 4, the
tion is approximated by linear functions, i.e., Qˆ(s,a;ω) = sequences{θ i,t},(cid:8) µi t(cid:9) and(cid:8) ω ti(cid:9) generatedfromAlgorithm1
ω⊤ϕ(s,a) where ϕ(s,a) = [ϕ (s,a),··· ,ϕ (s,a)]⊤ ∈ RK satisfy the following.
1 K
is the feature associated with (s,a). The feature function, (i) Convergence of the critic step: 1lim (cid:80) µi =
I t→∞ i∈I t
ϕ : S × A → R, for k = 1,··· ,K, is bounded for any J(θ), where J(θ) is defined in (9), and lim ωi =ω
k t→∞ t θ
s∈S,a∈A.Furthermore,thefeaturefunctions{ϕ }K are for all i ∈ I, with ω being a solution of a fixed point
k k=1 θ
linearly independent, and for any u∈RK and u̸=0,u⊤ϕ is mapping. (The convergence is in the sense of almost
not a constant function over S×A. surely.)8
(ii) Convergence of the actor step: for each i ∈ I, θ
i,t
converges almost surely to a point θˆ that is a projection
i
onto the set Θ.
Proof. The general convergence result of the consensus algo-
Algorithm 1: Consensus-update actor-critic algorithm rithmisprovedin[1]undersixassumptions.OurAssumption
for voltage control with P2P energy market 1, 2, 3 and 4 directly correspond to four assumptions in
Input: Initial values of the parameters [1]. The fifth assumption [1] regarding agents’ time-varying
r¯i,ωi,ωi,θi,∀i∈{1,...,I}, the initial state s , and neighborhood is trivially true in our setting since we assume
0 0 (cid:101)0 0 0
stepsizes {β } and {β } . agents can communicate through the whole network and the
ω,t t≥0 θ,t t≥0
for each agent i=1,...,I do network topology does not change over time. Hence, the only
Makes the decision on reactive power generation remainingthingtoshowistheuniformboundednessofagents’
and battery charging/discharging reward r i,t.
a i,0 ∼π θi (·|s 0). BythedefinitionofthemarketclearingpriceP t in(2),itis
end 0 bounded between FIT and UR. Together with the compact-
The power flow equation of the distribution network is ness assumption of the state space S i (which is reasonable
solved, the P2P energy market is cleared, and the since an agent’s energy demand and battery/PV capacities are
information of joint actions a
0
=(a i,0,...,a I,0) is all bounded), the R im component in the reward function, as
sent to each agent. defined in (5), is uniformly bounded. The other term, the
for each agent i=1,...,I do voltageviolationpenaltyR iv,isuniformlyupperboundedby0
Observes the reward r . andlowerboundedby−λNM,basedonitsdefinition.Hence,
i,0
end r is uniformly bounded for all i and t, and the convergence
i,t
for each time step t=0,1,...T do results follow directly from Theorem 2 and 3 in [1].
for each agent i=1,...,I do
Remark 4. While the theorem establishes the convergence to
Observes the next global state s .
t+1
a steady state of the critic and actor steps, it does not address
Update r¯i ←(1−β )·r¯i+β ·r .
t+1 ω,t t ω,t i,t how good the system reward is upon convergence. A key
Makes the decision on reactive power
concerninthiscontextisthepotentialinadequacyofthelinear
generation and battery charging/discharging
approximation of the critic function Qˆ(s,a;ω) in Assumption
a ∼π (·|s ).
i,t+1 θi,t t+1
3. In our model, the distribution network encompasses a
end
multitude of agents, each exhibiting non-linear behaviors and
The power flow equation of the distribution
interactions that may not be adequately captured by simple
network is solved, the P2P energy market is
linear models. One natural idea is to deep neural networks
cleared, and the information of joint actions
(DNNs)instead,astheyexcelincapturingandmodelingnon-
a =(a ,...,a ) is sent to each agent.
t+1 i,t+1 I,t+1
linear relationships. Specifically, the layered architecture of
for each agent i=1,...,I do
1. Observes the reward r and update DNNs allows for the abstraction and hierarchical representa-
i,t+1
δi ←r −r¯i +Qˆ(cid:0) s ,a ;ωi(cid:1) − tion of data, enabling the network to learn rich and intricate
t i,t+1 t+1 t+1 t+1 t
Qˆ(cid:0)
s ,a
;ωi(cid:1)
.
representationsoftheenvironment.Thisisparticularlycrucial
t t t in our context, where the environment is characterized by
2. Critic step:
ωi ←ωi+β ·δi·∇ Qˆ(cid:0) s ,a ;ωi(cid:1) . a high degree of stochasticity and dynamical complexity.
3.(cid:101) At ctort step:ω,t θ i,t+t
1
←ω Γ˜i[θ it ,t+t β θt ,t·I(cid:98) tQ,i], T ofhe Thd eo ow ren msid Ie V.o 1f isus nin og loD nN geN rs a, ph po liw cae bv le er ., Cis onth sea qt ut eh ne tlyp ,ro tho ef
where Γi :Rmi →Θ
i
⊂Rmi, and
convergenceofthecriticandactorstepsremainsanunresolved
(cid:90)
I(cid:98) tQ,i = dπ θi,t(cid:0) ai |s t(cid:1) · (17) issue, meriting further investigation in future research.
Ai V. Simulation Results
∇ logπ (cid:0) ai |s (cid:1) Qˆ(cid:0) s ,ai,a−i;ωi(cid:1) ,
θi θi,t t t t t
(18) A. Test Data
We test our problem using the IEEE 13-bus test feeder. In our
4. Send ωi to the other agent in the system.
(cid:101)t simulations, we set each bus to have one prosumer, except
end
for the substation bus; hence, there are 12 prosumers (aka
for each agent i=1,...,I do
agents). The length of each time step is set to be 1 hour. The
Consensus step: ωi ←(cid:80)I ωj.
t+1 j=1(cid:101)t distributionnetwork’svoltagelimitissetto[0.96pu,1.04pu].
end The voltage violation penalty, M, is set at 104. The price
end ceiling and floor, that is, the utility rate UR and feed-in tariff
FIT, are set at 14 ¢/kWh and 5 ¢/kWh, respectively. The
capacity ofeach agent’s PV isset to be 30kW, andthe energy
storage capacity is set to be 50kWh, with the charging and
discharging efficiency being 0.95 and 0.9 for each agent. The
PV generation of the prosumers and the total base load of the9
pure consumers on each bus have fixed diurnal shapes. These decentralizedframeworkandinMADDPGistooptimizetheir
shapesrepresentthemeanvaluesatthecorrespondinghourof own long-term expected reward:
the day. The detail of these shapes can be found in [2] due (cid:34) ∞ (cid:35)
(cid:88)
to the page limit. The power flow equations (7) are solved by supJ (θ )=E γtr |s ,π , i=1,...,I, (25)
i i s0∼ρθ i i,t 0 θ
the open-source distribution system simulator OpenDSS [25]. θi
t=0
B. Deep Neural Network Implementation where γ is the discount factor of agent i. In the fully
i
As stated in Remark 4, we use DNNs instead of linear decentralized framework, each agent uses the single-agent
combinations of features to approximate the critic function PPOalgorithmfrom[27]toupdatetheirownpolicyandvalue
Qˆ. Toimplement theconsensus MARLwith DNNs, we apply function locally based on only their own state, action, and
the default architectures of the actor and critic networks of reward information.
actor-critic algorithms in RLlib [26]. To be specific, for the Since all three MARL approaches are categorized as policy
policy function, we use gradient descent methods, they update the parameter θ t+1 in
a similar manner to approximate gradient ascent of J(θ) as
(cid:32)(cid:34) (cid:35) (cid:34) (cid:35)(cid:33)
σ(1)(s) σ(3)(s) 0 follows:
π θi(·|s)∼N σθ (2i
)(s)
, θi
0 σ(4)(s)
, (22)
θ t+1 =θ
t+β∇(cid:92)
J(θ t), (26)
θi θi
(cid:92)
where σ(1)(s) represents the mean value of the charg- where β is the step size and ∇J(θ t) is a stochastic estimate
θi approximating the gradient of the performance measure. The
ing/discharging action, σ(2)(s) denotes the mean value of the
θi three MARL approaches differ in how each agent updates
smart inverter action. Similarly, σ θ(3 i)(s) and σ θ(4 i)(s) corre- ∇(cid:92) J(θ t).Tohighlightsuchdifferences,wepresentthespecific
spond to the variances of the charging/discharging action and gradient approximations for each of the three approaches
t he smart inverter action, respectively. The function σ θi(s)= below.
σ(1)(s)
In the fully decentralized approach, the gradient update
θi
 ···  : S → R4 is implemented as a DNN, parameter- uses local states and actions only. Particularly, for each i =
 
σ(4)(s) 1,...,I,
θi
izedbyθ i.Thisnetworkcomprisestwofullyconnectedlayers, ∇(cid:92) J (θ )=E [∇ logπ (a |s )Qπ(s ;a )].
each consisting of 256 neurons, and employs a tanh activate θi i i s∼ρθ,ai∼πθi θi θi i i i i i
(27)
function following each layer. Specifically,
In the MADDPG approach, each agent makes decisions
σ (s)=
θi basedononlytheirownstatevariables.However,theiraction-
θ(3)tanh(cid:16) θ(2)tanh(θ(1)s+θ(1)bias +θ(2)bias(cid:17) +θ(3)bias
,
value functions are updated based on the global states and
i i i i i i
actions of all agents by a central authority, who is assumed to
(23)
have access to the global information. Specifically, for each
where θ =
(θ(1),θ(1)bias ,θ(2),θ(2)bias ,θ(3),θ(3)bias
) are the
agent i = 1,...,I, the gradient approximation in MADDPG
i i i i i i i is
policyparameters,θ(1)isa256×|S|matrix,θ(2)isa256×256
i i (cid:92)
matrix, θ(3) is a 4×256 matrix, θ(1)bias is a |S|×1 vector, ∇ θiJ i(θ i)= (28)
θ(2)bias isi a 256×1 vector, θ(3)biasi is a 4×1 vector, and for E s∼ρθ,ai∼πθi [∇ θilogπ θi(a i |s i)Qπ i (s;a 1,...,a I)],
i i
any vector x∈ℜn, whereQπ(s;a ,...,a )isthecentralizedaction-valuefunc-
i 1 I
tiontobeupdatedbytheassumedcentralauthority.Thepolicy
 
ex1−e−x1
function,π (a |s ),ontheotherhand,usesonlylocalstates
tanh(x):=ex1 ·+ ·e ·−x1
, (24) s . This
isθi whi
at
isi
referred to as centralized training and
  i
ex1−e−x1 decentralized execution.
exn+e−xn
For the consensus framework, the gradient estimation uses
where x j, j =1,...,n, is the j-th element of x. the so-called expected policy gradient as in [1]:
A similar neural network architecture is also applied to
the approximator of the action-value function Qˆ(·,·;ωi). The ∇(cid:92) θiJ(θ)=E s∼ρθ,a−i∼πθ−iI θQ i(s,a −i), i=1,...,I, (29)
settings of stepsizes are β =1/t0.65 and β =1/t0.85.
ω,t θ,t where
C. Comaprison of Three Different MARL Algorithms
IQ(s,a )=E ∇ logπ (a |s)Qπ(s;a ,...,a ).
We compare the consensus MARL with two other algorithm θi −i ai∼πθi θi θi i i 1 I
(30)
frameworks:(i)afullydecentralizedframeworkinwhicheach
agent just solves their own reinforcement learning problem While the action-value function Qπ(s;a ,...,a ) is the
i 1 I
usingthePPOalgorithm[2],whilecompletelyignoringmulti- same as in (28), the key difference lies in the policy function.
agentinteractions,and(ii)theMADDPGapproach,originally As stated in Section IV-A and reflected in (30), each agent
proposed in [4]. i’s policy depends on all agents’ states s. The other key
Different from maximizing the system’s total long-term difference between the consensus MARL and the MADDPG
expected reward, the objective for each agent in the fully algorithm is how the action-value function Qπ is updated. In
i10
the MADDPG framework, the function is updated through This indicates that agents employing the consensus-update
centralized training through a central authority; while in the algorithmhavedevelopedmoresophisticatedtradingstrategies
consensus MARL, updates are executed via a decentralized compared to those using the fully decentralized PPO algo-
consensus process, as described in Section IV-B. rithm.Specifically,theconsensus-algorithm-inducedstrategies
involve buying more and selling less during off-peak hours,
D. Numerical Results
and conversely, selling more and buying less during peak
Figure 3 displays the convergence curves of the 30-episode
hours, when compared with the strategies produced by the
moving average of the episodic total cost in the distribution
purely decentralized algorithm.
networkforthethreedifferentMARLalgorithms.Itisevident
Figure 5 shows the gradual reduction of the system volt-
age deviation after training. It can be seen that the voltage
Figure 3: 30-episode moving average of episodic total reward
that the mean episodic total reward gradually converges to
Figure 5: System voltage deviation. The total system volt-
a high level at the end of the training in both frameworks,
age deviation (p.u.) in the n-th episode is calculated as
indicating the effective convergence of the consensus-update (cid:80)24n−1 (cid:80) [max(0,vj −v¯)+max(0,v−vj)].
framework. Furthermore, the agents in the consensus-update t=24(n−1) j:Bus t t
framework achievea higher stationaryreward levelthan those
in the fully decentralized framework, which is expected since violations of the system are all high for the three frameworks
the consensus-update framework benefits from improved ap- at the beginning. However, that of the consensus framework
proximation of the action-value function through communica- drops and converges to zero quickly at the fastest rate, and
tion.TherewardunderMADDPGishigherthanthatunderthe the other two converge to almost zero as well after half of the
fully decentralized PPO, but lower than that under consensus- training process. On the one hand, the results are remarkable
update framework. On one hand, MADDPG has the process inthesensethatvoltageviolationreductionisrealizedthrough
of centralized learning which can guarantee a better perfor- purely decentralized learning for all three frameworks, while
mance than the fully decentralized one. However, there is no the consensus approach has the best performance.
communication among agents in the process of decentralized
VI. Conclusion and Future Research
execution in MADDPG, so each agent’s decision-making is
only based on their own local states, which leads to a worse This study has developed a market and algorithmic frame-
overall performance than the consensus-update framework. work to enable energy consumers’ and prosumers’ partici-
Figure4presentsthemarketpriceoftheP2Pmarketinboth pation in local P2P energy trading. Utilizing reinforcement
frameworks in chronological order over the last three days learning algorithms, we have automated bidding for agents
afterthousandsoftrainingepisodes.Notably,themarketprice whileensuringdecentralizeddecision-making.TheSDR-based
market clearing addresses challenges with zero-marginal-cost
resources and simplifies bidding. Additionally, our MARL
framework includes voltage constraints of the physical net-
work, setting a foundation towards real-world applications.
However, this research represents just the initial phase in
the practical application of a P2P energy trading market, with
several challenges ahead. Theoretically, the scalability of the
MARLframeworkiscrucial;ifthescalabilityoftheconsensus
MARLalgorithmislimited,amean-fieldapproach,asin[28],
could be considered, where agents operate based on a mean-
field equilibrium. Practically, while the current model uses
discrete, synchronous trading rounds, future work should in-
Figure 4: Hourly prices over the last three days
vestigate continuous and asynchronous trading among agents.
in the consensus-update framework demonstrates a tendency Additionally,whileintegratingtransmissionconstraintsinto
to be higher during off-peak hours and lower during peak the current framework is straightforward (that is, they can
hours when compared to the fully decentralized framework. be added to the power flow equations (7)), the allocation of11
transmission losses raises important equity considerations that [17] D. Qiu, J. Wang, J. Wang, and G. Strbac, “Multi-agent reinforcement
must be addressed. learning for automated peer-to-peer energy trading in double-side auc-
tionmarket.,”inIJCAI,pp.2913–2920,2021.
Another critical aspect is cybersecurity. With increased [18] K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Basar, “Fully decen-
automation, the system becomes vulnerable to cyber attacks, tralized multi-agent reinforcement learning with networked agents,” in
InternationalConferenceonMachineLearning,pp.5872–5881,PMLR,
suchasmalicioususerscompromisingsmartinverterstoinject
2018.
false information into the market. In this context, the work in [19] G. Qu, A. Wierman, and N. Li, “Scalable reinforcement learning for
[21] provides valuable insights. multiagent networked systems,” Operations Research, vol. 70, no. 6,
pp.3601–3628,2022.
Last but not least, it is essential to investigate how short-
[20] Z. Zhao, C. Feng, and A. L. Liu, “Comparisons of auction designs
term P2P market dynamics influence long-term consumer through multi-agent learning in peer-to-peer energy trading,” IEEE
investment decisions, particularly regarding the adoption of TransactionsonSmartGrid,2022. Onlinefirst.
[21] M. Figura, K. C. Kosaraju, and V. Gupta, “Adversarial attacks in
solar panels and energy storage systems.
consensus-basedmulti-agentreinforcementlearning,”in2021American
ControlConference(ACC),pp.3050–3055,IEEE,2021.
References [22] P. Huang, A. Scheller-Wolf, and K. Sycara, “Design of a multi–unit
double auction e–market,” Computational Intelligence, vol. 18, no. 4,
[1] K.Zhang,Z.Yang,andT.Basar,“Networkedmulti-agentreinforcement pp.596–617,2002.
learningincontinuousspaces,”in2018IEEEconferenceondecisionand [23] B. Cui and X. A. Sun, “Solvability of power flow equations through
control(CDC),pp.2771–2776,IEEE,2018. existence and uniqueness of complex fixed point.” arXiv preprint
[2] C. Feng, A. L. Lu, and Y. Chen, “Decentralized voltage control with arXiv:1904.08855,2019.
peer-to-peerenergytradinginadistributionnetwork,”inProceedingsof [24] G.O.RobertsandJ.S.Rosenthal,“Generalstatespacemarkovchains
the56thHawaiiInternationalConferenceonSystemSciences,pp.2600 andmcmcalgorithms,”ProbabilitySurveys,vol.1,pp.20–71,2004.
–2609,2023. [25] Electric Power Research Institute, “OpenDSS.” https://www.epri.com/
pages/sa/opendss.
[3] J.Schulman,F.Wolski,P.Dhariwal,A.Radford,andO.Klimov,“Prox-
[26] The Ray Team, “Models, preprocessors, and action distribu-
imalpolicyoptimizationalgorithms,”arXivpreprintarXiv:1707.06347,
tions Ray 2.8.1.” https://docs.ray.io/en/latest/rllib/rllib-models.html#
2017.
default-model-config-settings.
[4] R.Lowe,Y.I.Wu,A.Tamar,J.Harb,O.PieterAbbeel,andI.Mordatch,
[27] J.Schulman,F.Wolski,P.Dhariwal,A.Radford,andO.Klimov,“Prox-
“Multi-agent actor-critic for mixed cooperative-competitive environ-
imalpolicyoptimizationalgorithms,”arXivpreprintarXiv:1707.06347,
ments,” Advances in neural information processing systems, vol. 30,
2017.
2017.
[28] X. Guo, A. Hu, R. Xu, and J. Zhang, “Learning mean-field games,”
[5] N. Liu, X. Yu, C. Wang, C. Li, L. Ma, and J. Lei, “Energy-sharing
Advancesinneuralinformationprocessingsystems,vol.32,2019.
model with price-based demand response for microgrids of peer-to-
peerprosumers,”IEEETransactionsonPowerSystems,vol.32,no.5,
pp.3569–3583,2017.
[6] T. Morstyn and M. D. McCulloch, “Multiclass energy management
forpeer-to-peerenergytradingdrivenbyprosumerpreferences,”IEEE
TransactionsonPowerSystems,vol.34,no.5,pp.4005–4014,2018.
[7] M.H.UllahandJ.-D.Park,“Peer-to-peerenergytradingintransactive
markets considering physical network constraints,” IEEE Transactions
onSmartGrid,vol.12,no.4,pp.3390–3403,2021.
[8] Y. Zhang, E. Dall’Anese, and M. Hong, “Online proximal-admm for
time-varying constrained convex optimization,” IEEE Transactions on
SignalandInformationProcessingoverNetworks,vol.7,pp.144–155,
2021.
[9] W. Tushar, T. K. Saha, C. Yuen, D. Smith, and H. V. Poor, “Peer-to-
peer trading in electricity networks: An overview,” IEEE Transactions
onSmartGrid,vol.11,no.4,pp.3185–3200,2020.
[10] W.Tushar,C.Yuen,T.K.Saha,T.Morstyn,A.C.Chapman,M.J.E.
Alam, S. Hanif, and H. V. Poor, “Peer-to-peer energy systems for
connected communities: A review of recent advances and emerging
challenges,”Appliedenergy,vol.282,p.116131,2021.
[11] W.Tushar,C.Yuen,H.Mohsenian-Rad,T.Saha,H.V.Poor,andK.L.
Wood,“Transformingenergynetworksviapeer-to-peerenergytrading:
The potential of game-theoretic approaches,” IEEE Signal Processing
Magazine,vol.35,no.4,pp.90–111,2018.
[12] E. A. Soto, L. B. Bosman, E. Wollega, and W. D. Leon-Salas, “Peer-
to-peer energy trading: A review of the literature,” Applied Energy,
vol.283,p.116268,2021.
[13] Y. Xia, Q. Xu, S. Li, R. Tang, and P. Du, “Reviewing the peer-
to-peer transactive energy market: Trading environment, optimization
methodology, and relevant resources,” Journal of Cleaner Production,
vol.383,p.135441,2023.
[14] J.Guerrero,A.C.Chapman,andG.Verbicˇ,“Decentralizedp2penergy
trading under network constraints in a low-voltage network,” IEEE
TransactionsonSmartGrid,vol.10,no.5,pp.5163–5173,2018.
[15] D.Biagioni,X.Zhang,D.Wald,D.Vaidhynathan,R.Chintala,J.King,
andA.S.Zamzam,“Powergridworld:Aframeworkformulti-agentrein-
forcementlearninginpowersystems,”arXivpreprintarXiv:2111.05969,
2021.
[16] S.Wang,J.Duan,D.Shi,C.Xu,H.Li,R.Diao,andZ.Wang,“Adata-
driven multi-agent autonomous voltage control framework using deep
reinforcementlearning,”IEEETransactionsonPowerSystems,vol.35,
no.6,pp.4644–4654,2020.