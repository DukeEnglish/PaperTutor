Smooth Ranking SVM via Cutting-Plane Method
Erhan CanOzcan1, Berk Go¨rgu¨lu¨2, Mustafa G.Baydogan3 and Ioannis Ch. Paschalidis1
1BostonUniversity
2McMasterUniversity
3Bogazici University
{cozcan, yannisp}@bu.edu,gorgulub@mcmaster.ca,mustafa.baydogan@boun.edu.tr
Abstract a misleadingmetricto assesstheperformanceofa classifier
[Rakotomamonjy,2004]. For example, in a scenario where
The mostpopularclassification algorithmsare de-
only7%oftheemailsarespam,themodelcaneasilyachieve
signed to maximize classification accuracy during
93%accuracywithoutpredictinganyemailasspam,although
training. However, this strategy may fail in the
suchamodelhasnopracticalusefulness.
presence of class imbalance since it is possible to
Overcomingclassimbalanceisessentialforthesuccessof
train models with high accuracy by overfitting to
classification algorithms, and various techniques have been
the majority class. On the other hand, the Area
proposedtohandleimbalanceddatasets[Leevyetal.,2018].
UndertheCurve(AUC)isawidelyusedmetricto
While some works tackle this problem at the data-
compareclassificationperformanceofdifferental-
level by using over-sampling and under-sampling methods
gorithmswhenthereisaclassimbalance,andvar-
[Khoshgoftaaretal.,2007;VanHulseetal.,2007],costsen-
ious approaches focusing on the direct optimiza-
sitive algorithm-levelalternatives that change the loss func-
tion of this metric during training have been pro-
tion during the training process have gained popularity re-
posed. Amongthem,SVM-basedformulationsare
cently [Caoetal.,2013; Wangetal.,2012]. However, cost
especially popular as this formulation allows in-
sensitivelearningsolutionsrelyonexplicitknowledgeofthe
corporating different regularization strategies eas-
misclassificationcosts,whichcanbeunknowninmostofthe
ily. In this work, we developa prototypelearning
cases[Alietal.,2013]. Therefore,theuseofthesesolutions
approachthatreliesoncutting-planemethod,simi-
canbelimitedinreal-worldproblems.
lartoRankingSVM,tomaximizeAUC.Ouralgo-
On the other hand, approachesmaximizing the Area Un-
rithmlearnssimplermodelsbyiterativelyintroduc-
der the Receiver Operating Characteristics (ROC) Curve
ing cutting planes, thus overfitting is prevented in
(AUC) during training may be better algorithm-level alter-
an unconventionalway. Furthermore, it penalizes
natives to consider when there is a class imbalance within
thechangesintheweightsateachiterationtoavoid
a dataset [Lingetal.,2003]. AUC concentrates on the
largejumpsthatmightbeobservedinthetestper-
relative prediction scores of samples [Bradley,1997], and
formance, thus facilitating a smooth learning pro-
quantifies the ranking capability of a model based on pair-
cess. Based on the experiments conducted on 73
wise comparisons. Although it is primarily a ranking re-
binaryclassificationdatasets,ourmethodyieldsthe
lated metric, its applications are extensively used in clas-
best test AUC in 25 datasets among its relevant
sification problems as well [CaldersandJaroszewicz,2007;
competitors.
NortonandUryasev,2019]. Furthermore, due to its robust-
ness to class imbalance, AUC is an important criterion to
assess the performance of a model, and it is preferred over
1 Introduction classificationaccuracy[Lingetal.,2003].
Unfortunately, AUC is neither a continuous nor a dif-
Classificationmodelsarefrequentlydeployedinvariousreal-
ferentiable function [Yanetal.,2003], which makes it dif-
world problems ranging from e-mail spam filters to medi-
ficult to optimize. In [Chang,2012], a mixed integer pro-
cal diagnostic tests. Since the main goal in classification is
gramming problem is proposed to maximize the AUC ex-
correctlycategorizingdataintopredeterminedclasses,accu-
actly. In theory, solving this problem yields the highest
racy is widely used as a target measure for evaluating the
possible training AUC. However, in this formulation, the
performance of different algorithms. However, class im-
number of binary variables grows quadratically due to pair-
balance is a common problem that exists in most of the
wise comparisons. Therefore, it takes significant amount
real-world datasets, and it can significantly affect the per-
of time to optimize these models in practice. Further-
formanceofclassifiersthatmaximizeaccuracyduringtrain-
more, it can lead to overfitting and poor generalization.
ing. Moreover,incase ofclass imbalance,accuracymaybe
After relaxing the integrality constraints, AUC can be ap-
UnderReview. proximated via a piecewise linear function. Based on this
4202
naJ
52
]GL.sc[
1v88341.1042:viXraidea, variuous AUC maximizing approaches such as Rank- learning takes place. However, since there is no general
ingSVM[Joachims,2002],RankBoost[Freundetal.,2003], recipe to follow during this process, feature selection usu-
and RankNet [Burgesetal.,2005] have been developed. In ally requires a solid domain knowledge. One recent study
thissense,theproblemformulationweproposeinthisstudy in[Ozcanetal.,2024]introducestheRanking-CGPrototype
has similarities with SVM-based models focusing on AUC algorithmthatemployscolumngeneration(CG)onavariant
maximization. ofRankingSVMasaninternalfeatureselectionmechanism,
Mostreal-worldproblemsexhibitnon-linearrelations,and and shows that column generation can be an important tool
the linear models cannot effectively represent these com- to prevent overfitting. Although there are numerous stud-
plex patterns. Kernel functions can be helpful to learn ies [AtamanandStreet,2005; DedieuandMazumder,2019;
complex decision boundaries, and the radial basis func- ZhangandZhou,2013] that employ the column generation
tion (RBF) kernel is one of the popular choices. This ideainSVM-basedmodelstoacceleratetheoptimization,the
function provides a proximity measure by scaling the Eu- benefit of using of column generation to prevent overfitting
clidean distance between data points via a shape parame- hasonlyrecentlybeingconsidered.Therefore,exploringthis
ter, which can affect the performance of the models sub- ideafurtherispromising.
stantially [FasshauerandZhang,2007]. Therefore, it is es- In this study, we propose Smooth Ranking-CG Prototype
sential to choose the right kernel for each problem via pa- to maximize AUC. Similar to Ranking-CG Prototype, this
rametertuningoperations. Ontheotherhand,theEuclidean methodgeneratescutting-planesbasedoncolumngeneration
distancecalculationcanbe consideredasa non-lineartrans- to learn prototypepoints. However, the Ranking-CGProto-
formationof the originalraw features, and utilizing the Eu- typelearnsamodelbyboundingtheL -normoftheweights
∞
clideandistancesbetweendatapointsasfeaturescanhelpthe [Ozcanetal.,2024],whichcanresultinlearningsub-optimal
modeltolearnnon-linearrelations[Duinetal.,2010]. More- models in some cases, thus degrading the overall perfor-
over, it is possible to extract features with high discrimina- mance. In ourstudy, we proposea formulation,which does
torypowerbylearningprototypepointsresidingontheorig- notrestricttheL -normoftheweights.Therefore,thisisthe
∞
inal feature space [Alvarezetal.,2022; Zhangetal.,2015; first work where the regularizationis achievedvia only col-
Biehletal.,2013]. The study in [Ozcanetal.,2024] lever- umngenerationasopposedtoRanking-CGPrototype.More-
agesthisidea,anddevisesaniterativealgorithmthatfindsthe over,inRanking-CGPrototypealgorithm,columngeneration
prototype points by solving a subproblem at each iteration. iterationsareceasedwiththehelpofaconvergenceparame-
Inthisstudy,webuildouralgorithmbasedonthisprototype ter, whichaffectsthetest performanceofthemodel. There-
learningstrategy. fore, the value of this parameter is determined via cross-
One common problem of learning methods is that they validation. On the other hand, in our formulation, the con-
can easily overfit the training data, and they fail to gen- vergenceparameterplaysa less criticalrole, whichhelpsus
eralize well to new data points [Chenetal.,2020]. This toremovetheconvergenceparameterfromouralgorithm.Fi-
issue deepens even more when the learned model is too nally, by conductingexperimentsover 73 publicly available
complex. In order to prevent overfitting, a number datasets, we show that the Smooth Ranking-CG Prototype
of techniques have been developed. State-of-the-art ap- can be a competitive approachsince it outperformsall rele-
proaches either incorporate a regularization term to the vantcompetitorsbyprovidingthehighestAUCin25datasets.
optimization problem [GhaddarandNaoum-Sawaya,2018;
Jime´nez-Corderoetal.,2021] or apply different feature 2 Notations andBackground
selection techniques to the data prior to optimization
Given a labeled dataset Z = (X,y), let (x ,y ) be the ith
[Lietal.,2017]. The first idea is well explored in i i
(feature,label)pairwherex ∈ Rd denotesad-dimensional
the context of Ranking SVMs, and some effective vari- i
featurevectorandy ∈{−1,1}denotestheclasslabel. Sup-
ants such as Ranking-SVM with the L -norm regularizer i
2 poseourgoalistolearnascorefunctionφ(·)thatmaximizes
[Rakotomamonjy,2004;Joachims,2002]andRanking-SVM
theAUCstatisticrepresentedasfollows:
with the L -norm regularizer [AtamanandStreet,2005] are
1
proposed. Theseapproachespreventcoefficientsfromgrow- {φ(x )>φ(x )}
p n
ing too much by penalizing their L 2-norm and L 1-norm in
AUC =
pP ǫPnP ǫN
, (1)
the objective function, respectively, and the impact of the φ |P||N|
regularization term on the objective function is controlled
via a constant cost parameter, C. However, as the choice where P and N respectivelycontain the indexesof positive
of the cost parameter, C, can dramatically affect the test andnegativeinstances, {·}istheindicatorfunction,and|.|
performance of the learned models, it is required to con- denotes the cardinality of a set. Due to the counting opera-
duct cross-validations during the experiments to select the tor involvedin AUC, this problem is non-convexregardless
best C. Another study in [MingandDing,2019] suggests oftheformofthechosenscorefunctionφ(·). Therefore,the
combining L and L -norm in the SVM context for better directoptimizationofthismetriccanbecomputationallyin-
1 2
regularization and classification performance. Furthermore, tractableforlargedatasets.
the studyin [Bertsimasetal.,2020] proposesa sparse SVM However,agoodapproximationcanbeobtainedbyreplac-
classifier by partitioning classes into clusters. On the other ingtheindicatorfunctionwiththefollowinghingelossfunc-
hand, feature selection refers to identifying the most rele- tion:
vant features to represent the patterns in data before model max(0, 1−(φ(x )−φ(x ))).
p n
1
1Ifthescorefunctionφ(·)isselectedasanaffinefunction,e.g., generation strategy, a popular technique relying on duality
φ(x) = wTx+b, a convex ranking approach with the L - [BertsimasandTsitsiklis,1997],forfindinganewprototype
2
normregularizerontheparametervectorwcanbeformulated pointto addtothesetQ. Therefore,columngenerationcan
asfollows: beaneffectivetooltocontrolthecomplexityofthemodel.
1
min wTw+C max 0, 1− wT(x −x ) ,
w 2 XX (cid:0) (cid:0) p n (cid:1)(cid:1)
pǫPnǫN
(2)
3 Proposed Approach
where C ≥ 0 is the regularization penalty. We define ξ
p,n
astheslackvariablethatmeasuresthemarginoferrorwhile
comparingapositiveinstancewithindexp∈P withanega-
tiveinstancewithindexn∈N. Byupperboundingtheterm
in the summation by ξ , Problem (2) can be transformed Theproblemgivenin(5)canbeseenasavariantofRanking-
p,n
intotheRankingSVMalgorithmin[Joachims,2002]: SVMwiththeL ∞-normregularizersinceconstraint(5c)re-
strictstheweightsbetween−1and1.However,boundingthe
weightvectorintoacertainregionmayresultinsub-optimal
1
min wTw+C ξ (3a) solutions,thuspoorperformanceinpractice. Hence,itisim-
w,ξ 2 X pǫPnX ǫN p,n portanttodesignaformulationthatdoesnotincludethiscon-
straint.
s.t. wT(x −x )≥1−ξ , ∀p∈P,∀n∈N, (3b)
p n p,n
ξ ≥0, ∀p∈P,∀n∈N, (3c) TheSimplexalgorithm,whichis frequentlyusedto solve
p,n
linear programming problems, searches for the optimal so-
NotethatRankingSVMfocusesonpairwisecomparisonsof
lution by jumping from one vertex to an adjacent vertex
the positive and negative instances and minimizes the total
[BertsimasandTsitsiklis,1997]. Therefore, the optimal so-
error in these pairwise comparisons. Therefore, it can learn
lution may change drastically from iteration to iteration. In
classifiersthatare robustto class imbalanceproblem. How-
our context, this is undesirable as the large changes in the
ever,problemssuchasoverfittingandthe inabilitytomodel
weightvectormayresultinfluctuationsintestAUC. There-
non-linearrelationspersistinthisformulation.
fore, removingconstraint(5c) requiresa modificationto the
To model non-linear relations, the study in
optimizationproblemsolvedateachiteration.
[Ozcanetal.,2024] proposes learning a score function
basedontheEuclideandistancetosomeprototypepoints:
Since our goal is to prevent weights from changing dra-
matically from iteration to iteration, we introduce an addi-
|Q|
φ(x):=φ(x,w,Q)= w(t)kx−q k, (4) tionalterm,whichpenalizesthechangeinweightsviaanon-
X t negativesmoothingparameterC,totheobjectivefunctionof
t=1
theproblemin(5). Supposethatw denotestheweightvec-
where x denotes the feature vector of an instance, w(t) is old
torlearnedbeforeaddinganewprototypepointtothesetQ,
the tth element of the weight vector w, Q is the set of then, fora givenQ, w ∈ R|Q|−1, andwith a smoothing
d−dimensional prototype points, and q denotes a single a old
t parameter,C,weformulatetheproblemasfollows:
point in Q, i.e., q ∈ Q, ∀t = {1,...,|Q|}. Accordingly,
t
foranygivenQ,theyfindtheweightvectorwbysolvingthe
followingproblemF(Q).
|Q|−1
F(Q): min ξ +C |w(t)−w(t) | (6a)
w,ξ X X p,n X old
min ξ (5a) p∈Pn∈N t=1
w,ξ X X p,n |Q|
p∈Pn∈N
s.t. w(t)(kx −q k−kx −q k)≥1−ξ ,
|Q| X p t n t p,n
s.t. w(t)(kx −q k−kx −q k)≥1−ξ , t=1
X p t n t p,n ∀p∈P,∀n∈N, (6b)
t=1
∀p∈P,∀n∈N, (5b) ξ p,n ≥0, ∀p∈P,∀n∈N. (6c)
−1≤w(t) ≤1, ∀t=1,...,|Q|, (5c)
ξ ≥0, ∀p∈P,∀n∈N. (5d)
p,n
AsthenumberofpointsinthesetQincreases,thelearned Byfollowingstandardreformulationproceduresforlinear
modelbecomesmorecomplex.Therefore,thelearnedmodel programs,wecanreplacetheabsolutevaluesintheobjective
islikelytooverfitwhenthesetQincludesmanypoints.How- function (6a) by an auxiliary vector s, and obtain an equiv-
ever, the optimization problem given in (5) is a linear pro- alentformulationto the problemin (6), which we denoteas
gramming problem, and it is possible to design a column F(Q,w ,C).
oldF(Q,w ,C): prototype point, q˜, we solve the following non-linear sub-
old
problembyusingagradientbasedoptimizationapproach:
|Q|−1
min ξ +C s(t) (7a)
w,ξ,s X X p,n X
p∈Pn∈N t=1
(cid:12) (cid:12)
s.t.
|Q|
w(t)(kx −q k−kx −q k)≥1−ξ ,
q˜=ar qg ∈m Rdax(cid:12)
(cid:12) (cid:12)X
pǫP
nX
ǫNπ p∗ ,n(kx p−qk−kx n−qk)(cid:12)
(cid:12)
(cid:12). (10)
X p t n t p,n (cid:12) (cid:12)
(cid:12) (cid:12)
t=1
Then, the pseudo-codeof SmoothRanking-CGPrototype
∀p∈P,∀n∈N, (7b)
isprovidedinAlgorithm1.
s(t)−w(t) ≥−w(t), ∀t=1,...,|Q|−1, (7c)
old
s(t)+w(t) ≥w(t), ∀t=1,...,|Q|−1, (7d) Algorithm1SmoothRanking-CGPrototype
old Input: Smoothingparameter(C >0),Setofallpoints(X)
s(t) ≥0, ∀t=1,...,|Q|−1, (7e) Output: Setofprototypepoints(Q),Optimalweights(w∗)
ξ p,n ≥0, ∀p∈P,∀n∈N. (7f) 1: Initializet=1,z 0 =10−6,Q=∅
Columngenerationinlinearprogrammingisbuiltupondu- 2: Randomlyselectapointq˜∈X
ality theory, and adding a new column to the primal prob- 3: repeat
lem results in adding a new cutting-plane to the dual prob- 4: Q=Q∪q˜
lem[BertsimasandTsitsiklis,1997]. Hence,wecandesigna 5: SolveF(Q,w old,C)in(7)toobtainw∗ andπ∗
mechanismtofindnewprototypepointtoaddtothesetQby 6: Defineφ(.):=φ(.,w∗,Q),andsetz t =AUC φ
solvingasub-problemthatisformulatedbasedontheinfor-
mationprovidedbydualvariables.Supposethattheproblem 7: Findq˜=argmax(cid:12) (cid:12) π p∗ ,n(kx p−qk−kx n−qk)(cid:12) (cid:12)
i rn es( p7 o) ni ds if no grm duu al lat pe rd obfo ler ma ig siv ae sn foQ ll, ow wo sl :d,andC,thenthecor- 8: Setw old
=wq∈ ∗Rd (cid:12)
(cid:12)
(cid:12)pP ǫPnP ǫN (cid:12)
(cid:12) (cid:12)
9: t=t+1
|Q| 10: until |zt−1−zt| <0.01
max π − (α(t)−β(t))w(t) (8a) zt−1
π,α,β X X p,n X old
p∈Pn∈N t=1
TheiterationsinAlgorithm1areinitiatedbyaddingaran-
s.t. π (kx −q k−kx −q k)
X X p,n p t n t domly selected point q˜from the feature space to the set Q.
p∈Pn∈N After solving the problem F(Q,w ,C) and obtaining w∗
old
−α(t)+β(t) =0, t=1,2,...,|Q|−1, (8b) andπ∗,wesolvethenon-linearsub-problem(10)tofindthe
prototypepointq˜. Finally,iterationscontinueuntilthealgo-
π (kx −q k−kx −q k)=0,
X X p,n p t n t rithmconverges.
p∈Pn∈N Unfortunately, (10) is a complex function that is neither
t=|Q|, (8c) convexnorconcave.Hence,itmighthavemanylocaloptima
and the gradient based algorithms may get stuck at one of
α(t)+β(t) ≤C, t=1,2,...,|Q|−1, (8d)
thesepoints.Techniquessuchasusingadaptivelearningrates
π p,n ≤1, ∀p∈P,∀n∈N, (8e) andintroducingamomentumtermareprovedtobehelpfulto
π ≥0, ∀p∈P,∀n∈N, (8f) avoid local optimums during the global optimum search in
p,n
practice,butitisnotpossibletoguaranteethatthealgorithm
α(t) ≥0, t=1,2,...,|Q|−1, (8g)
reachestheglobaloptimum. However,byusingaproperini-
β(t) ≥0, t=1,2,...,|Q|−1, (8h) tializationstrategy,thegradientdescentbasedalgorithmmay
convergeto betterlocaloptima,andwecan utilizeavailable
whereπ forp ∈ P,n ∈ N arethedualvariablesassoci-
p,n training data to design this initialization strategy. Suppose
atedwiththesoft-marginconstraintsgivenin(7b),andα :=
we have the dual solution, π∗, then the optimization of the
(α(1),α(2),...,α(|Q|−1)), β := (β(1),β(2),...,β(|Q|−1))
problemin(10)canbeinitiatedstartingfromqˆ∈X:
are the dual variables associated with the constraints given
in (7c) and (7d), respectively. Accordingto the duality the-
ory,itisenoughtooptimizetheprimalproblemtoretrievethe
o pp roti tm ota yl pd eu pa ol is no tlu totio thn eor prv ii mce alv per rs oa b. leM mor re eo suv le tr s, ia ndd gi en ng ea ran tie nw
g
qˆ=arg q∈m Xax(cid:12) (cid:12)
(cid:12) (cid:12)pX ∈PnX
∈Nπ p∗ ,n(kx p−qk−kx n−qk)(cid:12) (cid:12)
(cid:12)
(cid:12). (11)
the constraintin (8c). Therefore,giventhe optimalsolution (cid:12) (cid:12)
ofProblem(8),denotedbyπ∗,ifthereexistsaq ∈ Rdsuch Notice that t(cid:12) he problem in (11) is easy to solve si(cid:12) nce we
that havefinitelymanytrainingsamples. Afterfindingqˆ,theopti-
X X
π p∗ ,n(kx p−qk−kx n−qk)6=0, (9) mizationof(10)continuesforaspecificnumberofiterations
p∈Pn∈N unless convergenceis observed. The details of the gradient
thenaddingqtothesetQcanprovideanimprovementtothe basedalgorithmweemploytosolve(10)areprovidedinSec-
model as it harms the dual feasibility. In order to find new tion4.4 Experiments convergence criteria is not met in 1000 iterations, the opti-
mizationofthesub-problemisinterrupted,andthealgorithm
We conduct different set of experiments to show the bene-
continuestheiterationsbyaddingthebestprototypepointto
fits of the proposed approach. Section 4.1 provides the ex-
thesetQ.
perimental setup; Section 4.2 demonstrates that the smooth
Ranking-CG Prototype can be helpful to preventthe fluctu- 4.2 StabilizingtheProgress ofTestAUC
ationsintestAUCatconsecutiveiterations,whichRanking-
As mentioned earlier, the Ranking-CG Prototype learns a
CGPrototypemayexhibitiftheweightvectorisunbounded;
model by solving the problem given in (5), thus the model
and Section 4.3 shows the classification performance of the
weightsarerestrictedbetween−1and1. Duetothisbound,
proposedapproachwithrespecttodifferentbenchmarks.
the learned model can be sub-optimal in some cases, thus
showing poor performance. However, removing this bound
4.1 ExperimentalSetup
canbedevastatingforthetestperformanceofthemodelasit
The model is implemented in the Python programminglan- willmakethemodelless predictable. ThesmoothRanking-
guage, and the code is publicly available1. While Gurobi CG Prototype addresses this issue by penalizing the change
9.5.1 is utilized to solve linear programs, the Adam opti- oftheweightvectorinsteadofboundingtheL normofthe
∞
mizer in TensorFlow 2.10.0 is utilized to solve non-linear weightvectorasopposedtotheRanking-CGPrototype.
sub-problem. We test the performance of the proposed In order to investigate the case described above, we im-
method on binary classification problems that are pub- plementaversionofRanking-CGPrototypebyremovingthe
licly available on the UCI [DuaandGraff,2017] and KEEL bound on the weight vector, which we refer as Unbounded
[Alcala´-Fdezetal.,2011] repositories. The chosen datasets Ranking-CGPrototype;andwemonitorthechangeinthetest
vary in terms of the number of features, the number of in- AUCvaluesafteraddinganewpointtothesetQateachiter-
stances, and the severity of class imbalance, and the dataset ationfordifferentapproachesonasimpleXORproblemwith
characteristicsaresummarizedinAppendixA. somenoise.Figure1showsthescatterplotoftheexperimen-
The model proposed in this study is relevant to Ranking tal XOR problem we consider, and compares the test AUC
SVM-based models, thus we consider three Ranking SVM performance of our algorithm with the Ranking-CG Proto-
variants as benchmark algorithms. We refer to Ranking typeandtheUnboundedRanking-CGPrototypeasafunction
SVM with L , L and L regularizers as Ranking SVM, ofthenumberofiterations.
2 1 ∞
L Ranking, and L Ranking, respectively. While we uti- AccordingtoFigure1b,theUnboundedRanking-CGPro-
1 ∞
lize the Ranking SVM implementation in the dlib package totypecanbestillvulnerabletooverfittingunlesstheweight
[King,2009], we implement other versions of the Ranking vectorisbounded.Moreover,duetothefluctuationsobserved
SVM, and solve them via Gurobi. To guarantee fair com- intheRanking-CGPrototypeanditsunboundedversion,the
parisonsamongmodels,alltheaforementionedmethodsare convergenceparameterofthesealgorithmsmusttobetuned
trainedonthepairwisedissimilaritymatrixobtainedbasedon viacross-validationforareliableperformance.However,the
Euclidean distances instead of the raw features. Therefore, performance of the Smooth Ranking-CG Prototype is more
the upper limit for the number of features that any method robustacrosstheiterationssinceitallowsthelearnedweights
canconsiderisequalto the numberofpointsin the training to change if and only if the improvementin the objective is
data, except for the prototype learning approaches. Lastly, large enough. Therefore, the convergenceparameter in our
we includethe Ranking-CGPrototypemethodthatachieves approachisnotasimportantasitisintheRanking-CG,which
regularizationinternally via columngenerationas described allowsustoremovethisparameterfromouralgorithm.
in[Ozcanetal.,2024]amongthebenchmarksweconsider.
4.3 Classification Performance
The performance of the benchmark methods heavily de-
pendsonthevalueofitsparameter,whichcontrolsthemodel We compare the Smooth Ranking-CG Prototype with the
complexity, and the best value of this parameter is chosen aforementioned relevant competitors by conducting experi-
via 5-fold cross-validation for each dataset during the ex- mentsover73datasets, andthedetailedresultsareavailable
periments. To ensure a fair comparison, the number of op- inAppendixC.Accordingtotheseresults,Table1showsthe
tionsconsideredforthecomplexityparameterisequalacross numberofdatasetsinwhicheachapproachprovidesthehigh-
differentapproaches,andthedetailsofthoseparametersare esttestAUCamongallcompetitors(thefirstrow),andtheav-
availableinAppendixB. erage% offeatureswith coefficients’magnitudelargerthan
Ontheotherhand,theSmoothRanking-CGPrototypehas 0.001.
a smoothing parameter, C, which penalizes the changes in TheSmoothRanking-CGPrototypeyieldsthehighesttest
weightsateachiteration.Thisparameterisselectedbasedon AUCin25outof73datasets,whichmakesitascompetitive
a 5-foldcross-validationfrom thirteen values in the interval asitsrelevantcounterparts.Furthermore,asshowninTable1,
of[10−6,1.0](seeAppendixBforthedetails). Additionally, itutilizessignificantlyfewerfeatures,thusitcanbeauseful
duringtheoptimizationofthesub-probleminEquation(10), approachwhenitisrequiredtolearnsimplermodels.Finally,
we employ the Adam optimizer with its default parameters, toindicatethebenefitoftheproposedapproach,weevaluate
and the optimization of the sub-problem continues as long theperformanceoftheproposedalgorithmonlyin47datasets
as the changein objective ratio is greaterthan 10−5. If this wheretheRanking-CGPrototypecannotprovidethehighest
testAUC,andtheperformanceofalgorithmsoverthissubset
1https://github.com/erhancanozcan/SmoothRankingSVM. ofdatasetsisprovidedinTable2.(a)ScatterPlotofXORDataset (b)TestAUC
Figure 1: Changes in the Test AUC withthe number of iterations (each newly added point to the set Q) for the Ranking-CG Prototype,
UnboundedRanking-CGPrototypeandSmoothRanking-CGPrototypemethods.
SmoothRanking-CG Ranking-CG Ranking
L Ranking L Ranking
Prototype Prototype 1 ∞ SVM
#ofDatasetswithHighestAUC 25 26 30 24 24
%ofFeaturesUsed 2% 7% 9% 100% 73%
Table1: ThenumberofdatasetsinwhicheachmethodyieldsthehighestAUCandtheaverageofthe%offeaturesusedin73datasets. The
sumofthefirstrowisgreaterthan73sincemorethanoneapproachmayyieldthehighestAUCinsomedatasets.
SmoothRanking-CG Ranking-CG Ranking
L Ranking L Ranking
Prototype Prototype 1 ∞ SVM
#ofDatasetswithHighestAUC 14 0 18 12 14
Table2:ThenumberofdatasetsinwhicheachmethodyieldsthehighestAUCin47datasetsthattheRanking-CGPrototypefailstoprovide
thehighesttestAUC.Thesumofthefirstrowisgreaterthan47sincemorethanoneapproachmayyieldthehighestAUCinsomedatasets.
5 Conclusion bythis,onepotentialwaytoimprovetheperformanceofthe
Smooth Ranking-CG Prototype might be to design an algo-
This paper proposes a classifier maximizing AUC during rithm adjusting the value of the smoothing parameter in an
training by incorporating a column generation-based proto- adaptivewayateachiteration.
type learning strategy via cutting-planes. Although a simi-
lar approach,Ranking-CGPrototype, hasbeenproposedre- Acknowledgments
cently, we explain the problemsthat may arise in that algo-
rithm, and attempt to handle those by modifying the opti- Research was partially supported by the NSF under grants
mization problem. The modification we propose allows us CCF-2200052, DMS-1664644, and IIS-1914792, by the
toremovetheconvergenceparametercontrollingthecolumn DOEundergrantsDE-EE0009696andDE-AR-0001282,and
generation iterations. Based on the extensive experiments bytheONRundergrantN00014-19-1-2571.
conductedon73datasets,theSmoothRanking-CGPrototype
yieldsthehighesttestAUCin25datasetsamongitsrelevant References
competitorsbylearningsimplermodels.
[Alcala´-Fdezetal.,2011] Jesu´s Alcala´-Fdez, Alberto
We believe that the proposed method has two interesting
Ferna´ndez, Julia´n Luengo, Joaqu´ın Derrac, Salvador
aspects that can be explored further as future work. First,
Garc´ıa, Luciano Sa´nchez, and Francisco Herrera. Keel
similartoRanking-CGPrototype,thefirstprototypepointis
data-miningsoftwaretool: datasetrepository,integration
addedtothesetQrandomlytoinitiatetheSmoothRanking-
of algorithms and experimental analysis framework.
CG Prototype,whichmaypotentiallyhaveanimpactonthe
Journal of Multiple-Valued Logic & Soft Computing, 17,
performance of the algorithm. Second, the smoothing pa-
2011.
rameter, C, remains constant during the training of Smooth
Ranking-CGPrototype. However,theroleofthisparameter [Alietal.,2013] Aida Ali, Siti Mariyam Shamsuddin, and
is similar to learning rate in gradient-based algorithms, and AncaLRalescu. Classificationwithclassimbalanceprob-
setting the learning rate in an adaptive way is an important lem. InternationalJournalofAdvancesinSoftComputing
concept to improve the gradient-based algorithms. Inspired anditsApplications,5(3):176–204,2013.[Alvarezetal.,2022] Yanela Rodr´ıguez Alvarez, Mar´ıa space classification. In InternationalConference on Pat-
MatildeGarc´ıaLorenzo,Yaile´CaballeroMota,YaimaFil- ternRecognition,pages46–55.Springer,2010.
iberto Cabrera, Isabel M Garc´ıa Hilario´n, Daniela
[FasshauerandZhang,2007] Gregory E Fasshauer and
MachadoMontesdeOca, andRafaelBelloPe´rez. Fuzzy
Jack G Zhang. On choosing “optimal” shape param-
prototypeselection-based classifiers for imbalanceddata.
eters for rbf approximation. Numerical Algorithms,
case study. Pattern Recognition Letters, 163:183–190,
45:345–368,2007.
2022.
[Freundetal.,2003] Yoav Freund, Raj Iyer, Robert E
[AtamanandStreet,2005] KaanAtamanandWNickStreet.
Schapire, and Yoram Singer. An efficient boosting al-
Optimizingarea underthe roc curveusing rankingsvms.
gorithm for combining preferences. Journal of machine
InProceedingsofInternationalConferenceonKnowledge
learningresearch,4(Nov):933–969,2003.
DiscoveryinDataMining,2005.
[GhaddarandNaoum-Sawaya,2018] Bissan Ghaddar and
[BertsimasandTsitsiklis,1997] Dimitris Bertsimas and
JoeNaoum-Sawaya. Highdimensionaldataclassification
John N Tsitsiklis. Introduction to linear optimization,
andfeatureselectionusingsupportvectormachines.Euro-
volume6. AthenascientificBelmont,MA,1997.
peanJournalofOperationalResearch,265(3):993–1004,
[Bertsimasetal.,2020] Dimitris Bertsimas, 2018.
Michael Lingzhi Li, Ioannis Ch Paschalidis, and
[Jime´nez-Corderoetal.,2021] Asuncio´n Jime´nez-Cordero,
Taiyao Wang. Prescriptive analytics for reducing30-day
JuanMiguelMorales,andSalvadorPineda. A novelem-
hospital readmissions after general surgery. PloS one,
beddedmin-maxapproachforfeatureselectioninnonlin-
15(9):e0238118,2020.
earsupportvectormachineclassification. EuropeanJour-
[Biehletal.,2013] Michael Biehl, Barbara Hammer, and nalofOperationalResearch,293(1):24–35,2021.
Thomas Villmann. Distance measures for prototype
[Joachims,2002] ThorstenJoachims. Optimizingsearchen-
basedclassification. InInternationalWorkshoponBrain-
ginesusingclickthroughdata.InProceedingsoftheeighth
InspiredComputing,pages100–116.Springer,2013.
ACM SIGKDD international conference on Knowledge
[Bradley,1997] AndrewPBradley. Theuseoftheareaun- discoveryanddatamining,pages133–142.ACM,2002.
der the ROC curvein the evaluationof machine learning
[Khoshgoftaaretal.,2007] Taghi M Khoshgoftaar, Chris
algorithms. Patternrecognition,30(7):1145–1159,1997.
Seiffert, Jason Van Hulse, Amri Napolitano, and Andres
[Burgesetal.,2005] Chris Burges, Tal Shaked, Erin Ren- Folleco. Learning with limited minority class data. In
shaw,AriLazier,MattDeeds,NicoleHamilton,andGreg SixthInternationalConferenceonMachineLearningand
Hullender. Learning to rank using gradient descent. In Applications(ICMLA2007),pages348–353.IEEE,2007.
Proceedingsofthe22ndinternationalconferenceonMa-
[King,2009] Davis E. King. Dlib-ml: A machine learning
chinelearning,pages89–96,2005.
toolkit. JournalofMachineLearningResearch,10:1755–
[CaldersandJaroszewicz,2007] Toon Calders and Szymon 1758,2009.
Jaroszewicz.EfficientAUCoptimizationforclassification.
[Leevyetal.,2018] JoffreyLLeevy,TaghiMKhoshgoftaar,
InEuropeanconferenceonprinciplesofdataminingand
Richard A Bauder, and Naeem Seliya. A survey on ad-
knowledgediscovery,pages42–53.Springer,2007.
dressinghigh-classimbalanceinbigdata. JournalofBig
[Caoetal.,2013] Peng Cao, Dazhe Zhao, and Osmar Za- Data,5(1):1–30,2018.
iane.Anoptimizedcost-sensitivesvmforimbalanceddata
[Lietal.,2017] Jundong Li, Kewei Cheng, Suhang Wang,
learning. InPacific-Asiaconferenceonknowledgediscov-
FredMorstatter,RobertPTrevino,JiliangTang,andHuan
eryanddatamining,pages280–292.Springer,2013.
Liu. Featureselection: Adataperspective. ACMcomput-
[Chang,2012] Allison An Chang. Integer optimization ingsurveys(CSUR),50(6):1–45,2017.
methodsformachinelearning. PhDthesis,Massachusetts
[Lingetal.,2003] CharlesXLing,JinHuang,HarryZhang,
InstituteofTechnology,2012.
et al. AUC: a statistically consistent and more discrimi-
[Chenetal.,2020] RuidiChen,IoannisChPaschalidis,etal. nating measure than accuracy. In Ijcai, volume 3, pages
Distributionallyrobustlearning.FoundationsandTrends® 519–524,2003.
inOptimization,4(1-2):1–243,2020.
[MingandDing,2019] Di Ming and Chris Ding. Robust
[DedieuandMazumder,2019] Antoine Dedieu and Rahul flexible feature selection via exclusivel21 regularization.
Mazumder. Solving large-scale l1-regularized SVMs In Proceedingsof the 28th internationaljoint conference
and cousins: the surprising effectiveness of column and onartificialintelligence,pages3158–3164,2019.
constraint generation. arXiv preprint arXiv:1901.01585,
[NortonandUryasev,2019] MatthewNortonandStanUrya-
2019.
sev. Maximization of AUC and buffered AUC in binary
[DuaandGraff,2017] Dheeru Dua and Casey Graff. UCI classification. Mathematical Programming, 174(1):575–
machinelearningrepository,2017. 612,2019.
[Duinetal.,2010] Robert PW Duin, Marco Loog, Elz˙bieta [Ozcanetal.,2024] Erhan C Ozcan, Berk Go¨rgu¨lu¨, and
Pe¸kalska,andDavidMJTax. Feature-baseddissimilarity MustafaGBaydogan.Columngeneration-basedprototypelearning for optimizing area under the receiver operating
characteristiccurve.EuropeanJournalofOperationalRe-
search,314(1):297–307,2024.
[Rakotomamonjy,2004] AlainRakotomamonjy.Optimizing
areaunderROCcurvewithSVMs. InROCAI,pages71–
80,2004.
[VanHulseetal.,2007] Jason Van Hulse, Taghi M Khosh-
goftaar,andAmriNapolitano. Experimentalperspectives
onlearningfromimbalanceddata. In Proceedingsof the
24thinternationalconferenceonMachinelearning,pages
935–942,2007.
[Wangetal.,2012] Xiaoguang Wang, Hang Shao, Nathalie
Japkowicz, Stan Matwin, Xuan Liu, Alex Bourque, and
BaoNguyen. Usingsvmwithadaptivelyasymmetricmis-
classificationcostsformine-likeobjectsdetection.In2012
11th InternationalConference on Machine Learning and
Applications,volume2,pages78–82.IEEE,2012.
[Yanetal.,2003] Lian Yan, Robert H Dodier, Michael
Mozer,andRichardHWolniewicz. Optimizingclassifier
performanceviaanapproximationtothewilcoxon-mann-
whitneystatistic. InProceedingsofthe20thInternational
ConferenceonMachineLearning(ICML-03),pages848–
855,2003.
[ZhangandZhou,2013] LiZhangandWeiDaZhou. Analy-
sisofprogrammingpropertiesandtherow–columngener-
ationmethodfor1-normsupportvectormachines. Neural
Networks,48:32–43,2013.
[Zhangetal.,2015] Xueying Zhang, Qinbao Song, Guang-
tao Wang, Kaiyuan Zhang, Liang He, and Xiaolin Jia.
A dissimilarity-based imbalance data classification algo-
rithm. AppliedIntelligence,42:544–565,2015.Appendix A:DatasetCharacteristics
TableA.1:SummaryoftheDatasets
#ofTrain #ofTest ClassImbalanceRatio
Dataset #ofFeatures
Instances Instances (#ofClass1/#ofClass-1)
abalone-21 vs 8 435 146 9 0.02
abalone-3 vs 11 376 126 9 0.03
abalone9-18 548 183 9 0.06
cleveland-0 vs 4 129 44 13 0.08
dermatology-6 268 90 34 0.06
ecoli-0 vs 1 165 55 7 0.54
ecoli-0-1 vs 2-3-5 183 61 7 0.11
ecoli-0-1-3-7 vs 2-6 210 71 7 0.02
ecoli-0-1-4-7 vs 2-3-5-6 252 84 7 0.09
ecoli-0-2-3-4 vs 5 151 51 7 0.11
ecoli-0-2-6-7 vs 3-5 168 56 7 0.11
ecoli-0-3-4 vs 5 150 50 7 0.11
ecoli-0-3-4-6 vs 5 153 52 7 0.11
ecoli-0-3-4-7 vs 5-6 192 65 7 0.11
ecoli-0-6-7 vs 3-5 166 56 7 0.11
ecoli1 252 84 7 0.30
ecoli2 252 84 7 0.18
ecoli4 252 84 7 0.06
flare-F 799 267 17 0.04
glass0 160 54 9 0.49
glass-0-1-2-3 vs 4-5-6 160 54 9 0.31
glass-0-1-4-6 vs 2 153 52 9 0.09
glass-0-1-5 vs 2 129 43 9 0.11
glass-0-1-6 vs 2 144 48 9 0.10
glass-0-1-6 vs 5 138 46 9 0.05
glass-0-4 vs 5 69 23 9 0.11
glass-0-6 vs 5 81 27 9 0.09
glass1 160 54 9 0.55
glass4 160 54 9 0.06
glass5 160 54 9 0.04
glass6 160 54 9 0.16
haberman 229 77 3 0.36
ionosphere 263 88 34 0.56
iris0 112 38 4 0.50
kr-vs-k-zero vs eight 1095 365 20 0.02
led7digit-0-2-4-5-6-7-8-9 vs 1 332 111 7 0.09
lymphography-normal-fibrosis 111 37 32 0.04
new-thyroid1 161 54 5 0.19
new-thyroid2 161 54 5 0.19
page-blocks-1-3 vs 4 354 118 10 0.06
parkinsons 146 49 22 0.33
pima 576 192 8 0.54TableA.1:SummaryoftheDatasets(cont.)
#ofTrain # ofTest ClassImbalanceRatio
Dataset # ofFeatures
Instances Instances (#ofClass1/ # ofClass-1)
poker-8 vs 6 1107 370 10 0.02
poker-8-9 vs 6 1113 372 10 0.02
poker-9 vs 7 183 61 10 0.03
shuttle-6 vs 2-3 172 58 9 0.04
shuttle-c2-vs-c4 96 33 9 0.05
sonar 156 52 60 0.87
survival 229 77 3 0.36
vehicle1 634 212 18 0.34
vehicle3 634 212 18 0.33
votes 326 109 32 0.63
vowel0 741 247 13 0.10
winequality-red-3 vs 5 518 173 11 0.01
winequality-red-4 1199 400 11 0.03
winequality-red-8 vs 6 492 164 11 0.03
winequality-red-8 vs 6-7 641 214 11 0.02
winequality-white-3 vs 7 675 225 11 0.02
winequality-white-9 vs 4 126 42 11 0.03
wisconsin 512 171 9 0.54
yeast-0-2-5-6 vs 3-7-8-9 753 251 8 0.11
yeast-0-2-5-7-9 vs 3-6-8 753 251 8 0.11
yeast-0-3-5-9 vs 7-8 379 127 8 0.11
yeast-0-5-6-7-9 vs 4 396 132 8 0.11
yeast-1-2-8-9 vs 7 710 237 8 0.03
yeast-1-4-5-8 vs 7 519 174 8 0.04
yeast-2 vs 4 385 129 8 0.11
yeast-2 vs 8 361 121 8 0.04
yeast3 1113 371 8 0.12
yeast4 1113 371 8 0.03
yeast5 1113 371 8 0.03
yeast6 1113 371 8 0.02
zoo-3 75 26 16 0.05Appendix B:The Detailsofthe Tuned Parameters
TableB.1:ValuesconsideredinparametertuningforSmoothRanking-CGPrototypeandthebenchmarkmethods.
Method Parameters
SmoothRanking-CGPrototype C∈n10−6,5×10−6,10−5,5×10−5,10−4,5×10−4,10−3,5×10−3,10−2,5×10−2,10−1,5×10−1,100
o
Ranking-CGPrototype α∈n10−5,5×10−5,10−4,2.5×10−4,5×10−4,7.5×10−4,10−3,2.5×10−3,5×10−3,7.5×10−3,10−2,2.5×10−2,5×10−2
o
RankingSVM C∈n10−3,5×10−3,10−2,5×10−2,10−1,5×10−1,1,5,10−1,5×10−1,102,5×102,103,5×103
o
L1Ranking C∈n10−3,5×10−3,10−2,5×10−2,10−1,5×10−1,1,5,10−1,5×10−1,102,5×102,103,5×103
o
L∞Ranking C∈n10−3,5×10−3,10−2,5×10−2,10−1,5×10−1,1,5,10−1,5×10−1,102,5×102,103,5×103
oAppendix C:DetailedResults
TableC.1: Out-of-sampleAUCforSmoothRanking-CGPrototype,Ranking-CGPrototype,L 1Ranking,L ∞ Ranking,andRanking-SVM
for74datasets.
Dataset SmoothRanking-CGPrototype Ranking-CGPrototype L Ranking L Ranking RankingSVM
1 ∞
abalone-21vs8 0.975 1.000 0.949 0.968 0.991
abalone-3vs11 1.000 1.000 1.000 1.000 1.000
abalone9-18 0.960 0.939 0.940 0.879 0.929
cleveland-0vs4 0.935 0.984 0.967 0.967 0.959
dermatology-6 1.000 1.000 1.000 1.000 1.000
ecoli-0-1-3-7vs2-6 1.000 1.000 1.000 1.000 1.000
ecoli-0-1-4-7vs2-3-5-6 0.976 0.976 0.952 0.881 0.939
ecoli-0-1vs2-3-5 0.967 0.942 0.958 0.955 0.967
ecoli-0-2-3-4vs5 0.991 1.000 1.000 1.000 0.991
ecoli-0-2-6-7vs3-5 0.893 0.837 0.867 0.910 0.890
ecoli-0-3-4-6vs5 0.774 0.826 0.800 0.898 0.906
ecoli-0-3-4-7vs5-6 1.000 1.000 1.000 1.000 1.000
ecoli-0-3-4vs5 0.991 1.000 0.991 1.000 0.987
ecoli-0-6-7vs3-5 0.937 0.893 0.887 0.930 0.910
ecoli-0vs1 0.991 0.993 0.994 0.994 0.991
ecoli1 0.943 0.950 0.940 0.959 0.947
ecoli2 0.948 0.945 0.938 0.938 0.940
ecoli4 1.000 1.000 1.000 1.000 1.000
flare-F 0.905 0.904 0.863 0.838 0.874
glass-0-1-2-3vs4-5-6 0.955 0.977 0.953 0.931 0.968
glass-0-1-4-6vs2 0.812 0.818 0.771 0.771 0.755
glass-0-1-5vs2 0.897 0.885 0.756 0.885 0.878
glass-0-1-6vs2 0.653 0.710 0.716 0.670 0.699
glass-0-1-6vs5 0.500 0.545 0.466 0.920 0.989
glass-0-4vs5 0.643 0.976 0.714 1.000 1.000
glass-0-6vs5 0.980 1.000 1.000 1.000 1.000
glass0 0.887 0.861 0.880 0.840 0.877
glass1 0.841 0.847 0.853 0.857 0.880
glass4 0.954 0.980 0.902 0.961 0.928
glass5 1.000 1.000 1.000 1.000 1.000
glass6 0.945 0.936 0.888 0.936 0.866TableC.1: Out-of-sampleAUCforSmoothRanking-CGPrototype,Ranking-CGPrototype,L 1Ranking,L ∞ Ranking,andRanking-SVM
for74datasets.(cont.)
Dataset SmoothRanking-CGPrototype Ranking-CGPrototype L Ranking L Ranking RankingSVM
1 ∞
haberman 0.634 0.638 0.619 0.587 0.625
ionosphere 0.980 0.980 0.994 0.993 0.990
iris0 1.000 1.000 1.000 1.000 1.000
kr-vs-k-zerovseight 0.994 0.998 1.000 1.000 1.000
led7digit-0-2-4-5-6-7-8-9vs1 0.973 0.977 0.983 0.974 0.973
lymphography-normal-fibrosis 0.986 0.986 0.986 0.971 0.929
new-thyroid1 0.669 0.704 0.995 0.817 0.906
new-thyroid2 0.990 0.995 0.998 0.995 0.998
page-blocks-1-3vs4 0.986 0.569 0.568 0.569 0.777
parkinsons 0.842 0.854 0.865 0.854 0.858
pima 0.830 0.847 0.839 0.759 0.841
poker-8-9vs6 0.523 0.536 0.597 0.496 0.555
poker-8vs6 0.645 0.704 0.676 0.463 0.527
poker-9vs7 0.873 0.983 0.805 0.983 1.000
shuttle-6vs2-3 1.000 1.000 1.000 1.000 1.000
shuttle-c2-vs-c4 1.000 1.000 0.968 0.935 0.935
sonar 0.899 0.902 0.930 0.966 0.958
survival 0.659 0.697 0.683 0.657 0.690
vehicle1 0.660 0.904 0.916 0.908 0.913
vehicle3 0.748 0.802 0.853 0.864 0.852
votes 0.997 0.991 0.984 0.986 0.993
vowel0 0.998 1.000 1.000 1.000 1.000
winequality-red-3vs5 0.763 0.843 0.776 0.778 0.794
winequality-red-4 0.561 0.524 0.461 0.410 0.452
winequality-red-8vs6 0.649 0.669 0.668 0.713 0.649
winequality-red-8vs6-7 0.412 0.422 0.424 0.389 0.363
winequality-white-3vs7 0.884 0.943 0.966 0.866 0.940
winequality-white-9vs4 0.683 0.537 0.561 0.537 0.537
wisconsin 0.998 0.997 0.998 0.998 0.998
yeast-0-2-5-6vs3-7-8-9 0.851 0.830 0.817 0.740 0.822
yeast-0-2-5-7-9vs3-6-8 0.925 0.929 0.950 0.928 0.941
yeast-0-3-5-9vs7-8 0.719 0.734 0.725 0.707 0.746
yeast-0-5-6-7-9vs4 0.849 0.879 0.867 0.899 0.871
yeast-1-2-8-9vs7 0.820 0.941 0.920 0.810 0.905
yeast-1-4-5-8vs7 0.727 0.696 0.645 0.786 0.601
yeast-2vs4 0.979 0.975 0.971 0.954 0.982
yeast-2vs8 0.721 0.872 0.910 0.845 0.760
yeast3 0.959 0.955 0.965 0.936 0.972
yeast4 0.910 0.927 0.930 0.912 0.914
yeast5 0.986 0.985 0.992 0.987 0.991
yeast6 0.948 0.944 0.930 0.911 0.957
zoo-3 0.720 0.960 1.000 1.000 1.000TableC.2: Numberoffeatureswithcoefficients’magnitude≥0.001forSmoothRanking-CGPrototype,Ranking-CGPrototype,L 1Rank-
ing,L ∞Ranking,andRanking-SVMfor74datasets.
Dataset SmoothRanking-CGPrototype Ranking-CGPrototype L Ranking L Ranking RankingSVM
1 ∞
abalone-21vs8 6 13 12 435 433
abalone-3vs11 2 3 4 376 351
abalone9-18 6 7 79 548 546
cleveland-0vs4 3 29 6 129 32
dermatology-6 3 7 10 268 152
ecoli-0-1-3-7vs2-6 5 6 6 209 21
ecoli-0-1-4-7vs2-3-5-6 6 6 18 252 73
ecoli-0-1vs2-3-5 5 13 7 183 18
ecoli-0-2-3-4vs5 4 9 23 150 18
ecoli-0-2-6-7vs3-5 4 10 2 168 26
ecoli-0-3-4-6vs5 5 8 13 153 17
ecoli-0-3-4-7vs5-6 5 11 7 192 151
ecoli-0-3-4vs5 4 6 6 150 16
ecoli-0-6-7vs3-5 4 7 4 166 25
ecoli-0vs1 2 7 15 165 165
ecoli1 4 9 13 252 250
ecoli2 4 15 25 252 252
ecoli4 3 7 8 252 248
flare-F 5 6 9 799 757
glass-0-1-2-3vs4-5-6 3 13 31 159 157
glass-0-1-4-6vs2 4 8 32 153 153
glass-0-1-5vs2 10 29 7 129 129
glass-0-1-6vs2 6 8 30 144 143
glass-0-1-6vs5 3 4 2 138 135
glass-0-4vs5 2 2 6 69 68
glass-0-6vs5 3 3 13 81 80
glass0 8 33 34 160 158
glass1 7 68 52 160 158
glass4 4 7 5 159 144
glass5 5 5 8 160 148
glass6 3 7 14 160 159TableC.2: Numberoffeatureswithcoefficients’magnitude≥0.001forSmoothRanking-CGPrototype,Ranking-CGPrototype,L 1Rank-
ing,L ∞Ranking,andRanking-SVMfor74datasets.(cont.)
Dataset SmoothRanking-CGPrototype Ranking-CGPrototype L Ranking L Ranking RankingSVM
1 ∞
haberman 5 14 34 229 220
ionosphere 5 11 58 262 262
iris0 2 2 5 112 107
kr-vs-k-zerovseight 6 11 35 1095 1069
led7digit-0-2-4-5-6-7-8-9vs1 3 15 6 332 316
lymphography-normal-fibrosis 3 3 7 111 104
new-thyroid1 4 7 10 161 129
new-thyroid2 4 4 9 161 148
page-blocks-1-3vs4 4 67 11 354 0
parkinsons 9 59 61 146 132
pima 7 34 52 576 192
poker-8-9vs6 19 22 151 1098 1029
poker-8vs6 11 32 90 1090 385
poker-9vs7 3 8 5 182 172
shuttle-6vs2-3 2 2 4 172 0
shuttle-c2-vs-c4 2 2 3 96 6
sonar 5 6 87 154 156
survival 7 6 19 229 214
vehicle1 3 219 293 634 583
vehicle3 7 65 100 634 530
votes 3 6 31 326 308
vowel0 6 14 32 741 681
winequality-red-3vs5 8 9 2 518 4
winequality-red-4 3 383 129 1199 1154
winequality-red-8vs6 13 18 2 492 10
winequality-red-8vs6-7 11 70 19 641 190
winequality-white-3vs7 6 6 5 675 103
winequality-white-9vs4 3 9 3 126 50
wisconsin 3 15 11 512 0
yeast-0-2-5-6vs3-7-8-9 5 6 20 753 745
yeast-0-2-5-7-9vs3-6-8 6 32 87 753 750
yeast-0-3-5-9vs7-8 4 8 108 379 375
yeast-0-5-6-7-9vs4 4 40 26 396 395
yeast-1-2-8-9vs7 11 8 14 710 705
yeast-1-4-5-8vs7 8 6 7 519 490
yeast-2vs4 4 6 28 385 383
yeast-2vs8 4 21 40 361 359
yeast3 4 10 14 1112 1106
yeast4 5 6 12 1113 1081
yeast5 3 13 36 1113 1092
yeast6 4 6 6 1111 1087
zoo-3 3 3 11 75 70