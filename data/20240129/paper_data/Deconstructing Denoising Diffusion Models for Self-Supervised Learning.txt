Deconstructing Denoising Diffusion Models for Self-Supervised Learning
XinleiChen1 ZhuangLiu1 SainingXie2 KaimingHe1
1FAIR,Meta 2NewYorkUniversity
Abstract noise
In this study, we examine the representation learning
abilities of Denoising Diffusion Models (DDM) that were image PCA inv noised encoder decoder denoised
originallypurposedforimagegeneration. Ourphilosophy PCA
is to deconstruct a DDM, gradually transforming it into a
classical Denoising Autoencoder (DAE). This deconstruc-
tiveprocedureallowsustoexplorehowvariouscomponents
of modern DDMs influence self-supervised representation
learning. Weobservethatonlyaveryfewmoderncompo-
nents are critical for learning good representations, while
manyothersarenonessential. Ourstudyultimatelyarrives
atanapproachthatishighlysimplifiedandtoalargeextent
resemblesaclassicalDAE.Wehopeourstudywillrekindle cleanimage noised denoised
interestinafamilyofclassicalmethodswithintherealmof
Figure1. ThelatentDenoisingAutoencoder(l-DAE)architecture
modernself-supervisedlearning.
wehaveultimatelyreached,afterathoroughexplorationofdecon-
structingDenoisingDiffusionModels(DDM)[23],withthegoal
ofapproachingthe classicalDenoisingAutoencoder(DAE)[39]
1.Introduction asmuchaspossible.Here,thecleanimage(left)isprojectedonto
alatentspaceusingpatch-wisePCA,inwhichnoiseisadded(mid-
Denoising is at the core in the current trend of genera- dle). ItisthenprojectedbacktopixelsviainversePCA.Anau-
tive models in computer vision and other areas. Popularly toencoderislearnedtopredictadenoisedimage(right).Thissim-
knownasDenoisingDiffusionModels(DDM)today,these ple architecture largely resembles classical DAE (with the main
methods [36, 37, 38, 23, 29, 11] learn a Denoising Au- differencethatnoiseisaddedtothelatent)andachievescompeti-
tiveself-supervisedlearningperformance.
toencoder (DAE) [39] that removes noise of multiple lev-
els driven by a diffusion process. These methods achieve
impressive image generation quality, especially for high- day’s DDMs for generation are dominantly based on ad-
resolution, photo-realistic images [33, 32]—in fact, these ditive noise, implying that they may learn representations
generation models are so good that they appear to have withoutexplicitlymarkingunknown/knowncontent.
strongrecognitionrepresentationsforunderstandingthevi- Most recently, there has been an increasing interest
sualcontent. [40,28]ininspectingtherepresentationlearningabilityof
WhileDAEisapowerhouseoftoday’sgenerativemod- DDMs. In particular, these studies directly take off-the-
els, it was originally proposed for learning representations shelf pre-trained DDMs [23, 32, 11], which are originally
[39]fromdatainaself-supervisedmanner. Intoday’scom- purposed for generation, and evaluate their representation
munity of representation learning, the arguably most suc- qualityforrecognition. Theyreportencouragingresultsus-
cessful variants of DAEs are based on “masking noise” ingthesegeneration-oriented models. However,thesepio-
[39], such as predicting missing text in languages (e.g., neering studies obviously leave open questions: these off-
BERT[10])ormissingpatchesinimages(e.g.,MAE[21]). the-shelf models were designed for generation, not recog-
However, inconcept, thesemasking-basedvariantsremain nition; it remains largely unclear whether the representa-
significantly different from removing additive (e.g., Gaus- tioncapabilityisgainedbyadenoising-drivenprocess,ora
sian)noise: whilethemaskedtokensexplicitlyspecifyun- diffusion-drivenprocess.
known vs. known content, no clean signal is available in Inthiswork,wetakeamuchdeeperdiveintothedirec-
the task of separating additive noise. Nevertheless, to- tioninitializedbytheserecentexplorations[40,28].Instead
1
4202
naJ
52
]VC.sc[
1v40441.1042:viXraof using an off-the-shelf DDM that is generation-oriented, a model to generate high-fidelity data is indicative of its
wetrainmodelsthatarerecognition-oriented.Atthecoreof potential for learning good representations. Generative
ourphilosophyistodeconstruct aDDM,changingitstep- Adversarial Networks (GAN) [18], for example, have ig-
by-step into a classical DAE. Through this deconstructive nited broad interest in adversarial representation learning
research process, we examine every single aspect (that we [13,12]. VariationalAutoencoders(VAEs)[26],originally
can think of) of a modern DDM, with the goal of learning conceptualizedasgenerativemodelsforapproximatingdata
representations. Thisresearchprocessgainsusnewunder- distributions,haveevolvedtobecomeastandardinlearning
standingsonwhatarethecriticalcomponentsforaDAEto localizedrepresentations(“tokens”),e.g.,VQVAE[30]and
learngoodrepresentations. variants [16]. Image inpainting [2], essentially a form of
Surprisingly, we discover that the main critical compo- conditional image generation, has led to a family of mod-
nentisatokenizer[33]thatcreatesalow-dimensionallatent ernrepresentationlearningmethods,includingContextEn-
space. Interestingly, this observation is largely indepen- coder[31]andMaskedAutoencoder(MAE)[21].
dent of the specifics of the tokenizer—we explore a stan- Analogously,theoutstandinggenerativeperformanceof
dard VAE [26], a patch-wise VAE, a patch-wise AE, and DenoisingDiffusionModels(DDM)[36,37,38,23,11]has
a patch-wise PCA encoder. We discover that it is the low- drawn attention for their potential in representation learn-
dimensionallatentspace,ratherthanthetokenizerspecifics, ing. Pioneering studies [40, 28] have begun to investigate
thatenablesaDAEtoachievegoodrepresentations. this direction by evaluating existing pre-trained DDM net-
Thanks to the effectiveness of PCA, our deconstructive works. However, we note that while a model’s generation
trajectory ultimately reaches a simple architecture that is capabilitysuggestsacertainlevelofunderstanding,itdoes
highly similar to the classical DAE (Fig. 1). We project notnecessarilytranslatetorepresentationsusefulfordown-
the image onto a latent space using patch-wise PCA, add streamtasks. Ourstudydelvesdeeperintotheseissues.
noise, and then project it back by inverse PCA. Then we On the other hand, although Denoising Autoencoders
train an autoencoder to predict a denoised image. We call (DAE) [39] have laid the groundwork for autoencoding-
thisarchitecture“latentDenoisingAutoencoder”(l-DAE). basedrepresentationlearning,theirsuccesshasbeenmainly
Our deconstructive trajectory also reveals many other confined to scenarios involving masking-based corruption
intriguing properties that lie between DDM and classical (e.g., [21, 41, 17, 5]). To the best of our knowledge, lit-
DAE.Foroneexample, wediscoverthatevenusingasin- tle or no recent research has reported results on classical
glenoiselevel(i.e.,notusingthenoiseschedulingofDDM) DAEvariantswithadditiveGaussiannoise,andwebelieve
canachieveadecentresultwithourl-DAE.Theroleofus- that the underlying reason is that a simple DAE baseline
ing multiple levels of noise is analogous to a form of data (Fig.2(a))performspoorly1(e.g.,in∼20%Fig.5).
augmentation,whichcanbebeneficial,butnotanenabling
factor. With this and other observations, we argue that the 3.Background: DenoisingDiffusionModels
representation capability of DDM is mainly gained by the
denoising-drivenprocess,notadiffusion-drivenprocess. Our deconstructive research starts with a Denoising Diffu-
Finally,wecompareourresultswithpreviousbaselines. sionModel(DDM)[36,37,38,23,11].Webrieflydescribe
Ononehand,ourresultsaresubstantiallybetterthantheoff- theDDMweuse,following[11,32].
the-shelfcounterparts(followingthespiritof[40,28]): this A diffusion process starts from a clean data point (z 0)
isasexpected,becausetheseareourstartingpointofdecon- andsequentiallyaddsnoisetoit. Ataspecifiedtimestept,
struction. Ontheotherhand,ourresultsfallshortofbase- thenoiseddataz tisgivenby:
line contrastive learning methods (e.g., [7]) and masking-
z =γ z +σ ϵ (1)
basedmethods(e.g.,[21]),butthegapisreduced.Ourstudy t t 0 t
suggestsmoreroomforfurtherresearchalongthedirection
whereϵ∼N(0,I)isanoisemapsampledfromaGaussian
ofDAEandDDM.
distribution, andγ andσ definethescalingfactorsofthe
t t
signal and of the noise, respectively. By default, it is set
2.RelatedWork
γ2+σ2 =1[29,11].
t t
Inthehistoryofmachinelearningandcomputervision,the A denoising diffusion model is learned to remove the
generation of images (or other content) has been closely noise, conditioned on the time step t. Unlike the orig-
intertwined with the development of unsupervised or self- inal DAE [39] that predicts a clean input, the modern
supervisedlearning. Approachesingenerationareconcep- DDM [23, 29] often predicts the noise ϵ. Specifically, a
tuallyformsofun-/self-supervisedlearning,wheremodels
were trained without labeled data, learning to capture the 1According to the authors of MoCo [20] and MAE [21], significant
efforthasbeendevotedtoDAEbaselinesduringthedevelopmentofthose
underlyingdistributionsoftheinputdata.
works,followingthebestpracticeestablished. However,ithasnotledto
There has been a prevailing belief that the ability of meaningfulresults(<20%accuracy).
2noise
image latent noised encoder decoder denoised
noise
Tokenizer. DiT instantiated in [32] is a form of Latent
DiffusionModels(LDM)[33],whichusesaVQGANtok-
enizer[16].Specifically,thisVQGANtokenizertransforms
image noised encoder decoder denoised the 256×256×3 input image (height×width×channels)
intoa32×32×4latentmap,withastrideof8.
Startingbaseline. Bydefault,wetrainthemodelsfor400
(a)aclassicalDenoisingAutoencoders(DAE)
epochsonImageNet[9]witharesolutionof256×256pix-
noise els. ImplementationdetailsareinSec.A.
Our DiT baseline results are reported in Tab. 1 (line 1).
WithDiT-L,wereportalinearprobeaccuracyof57.5%us-
image latent noised encoder decoder denoised ingits 21Lencoder. Thegenerationquality(Fre´chetIncep-
tion Distance [22], FID-50K) of this DiT-L model is 11.6.
Thisisthestartingpointofourdestructivetrajectory.
Despitedifferencesinimplementationdetails,ourstart-
(b)amodernDenoisingDiffusionModel(DDM)onalatentspace
noise ing point conceptually follows recent studies [40, 28]
Figure2. AclassicalDAEandamodernDDM.(a)Aclassical (more specifically, DDAE [40]), which evaluate off-the-
DAEthataddsandpredictsnoiseontheimagespace.(b)State-of- shelfDDMsunderthelinearprobingprotocol.
the-artDiDmMages(e.g.n,oiLseDdM[33en]c,oDdeIrT[32])detchoadetroperadteenooisendalatent
space,wherethenoiseisaddedandpredicted. 4.DeconstructingDenoisingDiffusionModels
Our deconstruction trajectory is divided into three stages.
lossfunctioninthisformisminimized: Wefirstadaptthegeneration-focusedsettingsinDiTtobe
more oriented toward self-supervised learning (Sec. 4.1).
∥ϵ−net(z )∥2 (2)
t Next,wedeconstructandsimplifythetokenizerstepbystep
(Sec. 4.2). Finally, we attempt to reverse as many DDM-
wherenet(z )isthenetworkoutput.Thenetworkistrained
t motivateddesignsaspossible,pushingthemodelstowards
formultiplenoiselevelsgivenanoiseschedule,conditioned
a classical DAE [39] (Sec. 4.3). We summarize our learn-
onthetimestept.Inthegenerationprocess,atrainedmodel
ingsfromthisdeconstructingprocessinSec.4.4.
isiterativelyapplieduntilitreachesthecleansignalz .
0
DDMs can operate on two types of input spaces. One 4.1.ReorientingDDMforSelf-supervisedLearning
is the original pixel space [11], where the raw image x
0
is directly used as z . The other option is to build DDMs WhileaDDMisconceptuallyaformofaDAE,itwasorig-
0
on a latent space produced by a tokenizer, following [33]. inallydevelopedforthepurposeofimagegeneration.Many
SeeFig.2(b). Inthiscase,apre-trainedtokenizerf (which designsinaDDMareorientedtowardthegenerationtask.
isoftenanotherautoencoder,e.g.,VQVAE[30])isusedto Some designs are not legitimate for self-supervised learn-
maptheimagex intoitslatentz =f(x ). ing (e.g., class labels are involved); some others are not
0 0 0
necessary if visual quality is not concerned. In this sub-
Diffusion Transformer (DiT). Our study begins with
section, we reorient our DDM baseline for the purpose of
the Diffusion Transformer (DiT) [32]. We choose this
self-supervisedlearning,summarizedinTab.1.
Transformer-based DDM for several reasons: (i) Unlike
other UNet-based DDMs [11, 33], Transformer-based ar- Remove class-conditioning. A high-quality DDM is of-
chitectures can provide fairer comparisons with other self- ten trained with conditioning on class labels, which can
supervisedlearningbaselinesdrivenbyTransformers(e.g., largely improve the generation quality. But the usage of
[7, 21]); (ii) DiT has a clearer distinction between the en- class labels is simply not legitimate in the context of our
coderanddecoder,whileaUNet’sencoderanddecoderare self-supervisedlearningstudy. Asthefirststep,weremove
connectedbyskipconnectionsandmayrequireextraeffort class-conditioninginourbaseline.
onnetworksurgerywhenevaluatingtheencoder; (iii)DiT Surprisingly, removing class-conditioning substantially
trainsmuchfasterthanotherUNet-basedDDMs(see[32]) improves the linear probe accuracy from 57.5% to 62.1%
whileachievingbettergenerationquality. (Tab.1), eventhoughthegenerationqualityisgreatlyhurt
WeusetheDiT-Large(DiT-L)variant[32]asourDDM as expected (FID from 11.6 to 34.2). We hypothesize that
baseline. In DiT-L, the encoder and decoder put together directlyconditioningthemodelonclasslabelsmayreduce
havethesizeofViT-L[15](24blocks).Weevaluatetherep- the model’s demands on encoding the information related
resentation quality (linear probe accuracy) of the encoder, toclasslabels. Removingtheclass-conditioningcanforce
whichhas12blocks,referredtoas“1L”(halflarge). themodeltolearnmoresemantics.
2
3
tokenizer
tokenizeracc.(↑) FID(↓) 1.0
original
DiTbaseline 57.5 11.6
linear
+removeclass-conditioning 62.5 30.9
2
+removeVQGANperceptualloss 58.4 54.3 0.5
+removeVQGANadversarialloss 59.0 75.6
+replacenoiseschedule 63.4 93.2
Table 1. Reorienting DDM for self-supervised learning. We 0.0 0 200 400 600 800 1000
time step
beginwiththeDiT[32]baselineandevaluateitslinearprobeac-
Figure3. Noiseschedules.Theoriginalschedule[23,32],which
curacy(acc.) onImageNet. Eachlineisbasedonamodification
setsγ2=Πt (1−β )withalinearscheduleofβ,spendsmany
oftheimmediatelyprecedingline. Theentriesingray,inwhich t s=1 s
timestepsonverynoisyimages(smallγ). Instead,weuseasim-
classlabelsareused,arenotlegitimateresultsforself-supervised
pleschedulethatislinearonγ2,whichprovideslessnoisyimages.
learning.SeeSec.4.1fordescription.
Deconstruct VQGAN. In our baseline, the VQGAN to- Summary. Overall, the results in Tab. 1 reveal that self-
kenizer, presented by LDM [33] and inherited by DiT, supervisedlearningperformanceisnotcorrelatedtogen-
is trained with multiple loss terms: (i) autoencoding re- erationquality. TherepresentationcapabilityofaDDMis
construction loss; (ii) KL-divergence regularization loss notnecessarilytheoutcomeofitsgenerationcapability.
[33];2 (iii)perceptualloss[44]basedonasupervisedVGG
4.2.DeconstructingtheTokenizer
net[35]trainedforImageNetclassification;and(iv)adver-
sarialloss[18,16]withadiscriminator. Weablatethelatter
Next,wefurtherdeconstructtheVAEtokenizerbymaking
twotermsinTab.1.
substantialsimplifications. Wecomparethefollowingfour
As the perceptual loss [44] involves a supervised pre- variantsofautoencodersasthetokenizers,eachofwhichis
trainednetwork,usingtheVQGANtrainedwiththislossis asimplifiedversionoftheprecedingone:
notlegitimate. Instead,wetrainanotherVQGANtokenizer
[33]inwhichweremovetheperceptualloss. Usingthisto- • ConvolutionalVAE.Ourdeconstructionthusfarleadsus
kenizerreducesthelinearprobeaccuracysignificantlyfrom toaVAEtokenizer. Ascommonpractice[26,33],theen-
62.5%to58.4%(Tab.1),which,however,providesthefirst coderf(·)anddecoderg(·)ofthisVAEaredeepconvo-
legitimateentrythusfar. Thiscomparisonrevealsthatato- lutional(conv)neuralnetworks[27]. Thisconvolutional
kenizer trained with the perceptual loss (with class labels) VAEminimizesthefollowinglossfunction:
initselfprovidessemanticrepresentations. Wenotethatthe
∥x−g(f(x))∥2+KL[f(x)|N].
perceptual loss is not used from now on, in the remaining
partofthispaper.
Here,xistheinputimageoftheVAE.Thefirsttermisthe
WetrainthenextVQGANtokenizerthatfurtherremoves
reconstructionloss,andthesecondtermistheKullback-
the adversarial loss. It slightly increases the linear probe
Leiblerdivergence[3,16]betweenthelatentdistribution
accuracyfrom58.4%to59.0%(Tab.1). Withthis, ourto-
off(x)andaunitGaussiandistribution.
kenizer at this point is essentially a VAE, which we move
ontodeconstructinthenextsubsection. Wealsonotethat • Patch-wise VAE. Next we consider a simplified case in
removingeitherlossharmsgenerationquality. whichtheVAEencoderanddecoderarebothlinearpro-
jections, and the VAE input x is a patch. The training
Replacenoiseschedule. Inthetaskofgeneration,thegoal
processofthispatch-wiseVAEminimizesthisloss:
is to progressively turn a noise map into an image. As a
result, the original noise schedule spends many time steps
∥x−UTVx∥2+KL[Vx|N].
onverynoisyimages(Fig.3). Thisisnotnecessaryifour
modelisnotgeneration-oriented.
Here x denotes a patch flattened into a D-dimensional
We study a simpler noise schedule for the purpose of
vector. BothU andV ared×D matrixes,wheredisthe
self-supervisedlearning. Specifically,weletγ2linearlyde-
t dimension of the latent space. We set the patch size as
cayintherangeof1>γ2≥0(Fig.3). Thisallowsthemodel
t 16×16pixels,following[15].
to spend more capacity on cleaner images. This change
• Patch-wiseAE.WemakefurthersimplificationonVAE
greatly improves the linear probe accuracy from 59.0% to
byremovingtheregularizationterm:
63.4% (Tab. 1), suggesting that the original schedule fo-
cuses too much on noisier regimes. On the other hand, as
∥x−UTVx∥2.
expected,doingsofurtherhurtsthegenerationability,lead-
ingtoaFIDof93.2.
Assuch,thistokenizerisessentiallyanautoencoder(AE)
2The KL form in [33] does not perform explicit vector quantization onpatches,withtheencoderanddecoderbothbeinglin-
(VQ),interpretedas“thequantizationlayerabsorbedbythedecoder”[33]. earprojections.
465
(a)patch-wiseVAE 60
55 conv. VAE
patch-wise VAE
(b)patch-wiseAE 50 patch-wise AE
patch-wise PCA
45
8 16 32 64 128
(c)patch-wisePCA latent dim per token (log-scale)
latentdim.d 8 16 32 64 128
Figure4. Visualizationofthepatch-wisetokenizer. Eachfilter
conv.VAE(baseline) 54.5 63.4 62.8 57.0 48.1
corresponds to a row of the linear projection matrix V (d×D),
patch-wiseVAE 58.3 64.9 64.8 56.8 -
reshapedto16×16×3forvisualization.Hered=16.
patch-wiseAE 59.9 64.7 64.6 59.9 -
patch-wisePCA 56.0 63.4 65.1 60.0 53.9
• Patch-wise PCA. Finally, we consider a simpler variant Table 2. Linear probe accuracy vs. latent dimension. With a
whichperformsPrincipalComponentAnalysis(PCA)on DiTmodel,westudyfourvariantsoftokenizersforcomputingthe
thepatchspace. ItiseasytoshowthatPCAisequivalent latentspace.Wevarythedimensionalityd(pertoken)ofthelatent
toaspecialcaseofAE: space.Thetableisvisualizedbytheplotabove.Allfourvariants
oftokenizersexhibitsimilartrends,despitetheirdifferencesin
∥x−VTVx∥2. architecturesandlossfunctions.The63.4%entryof“conv.VAE”
isthesameentryasthelastlineinTab.1.
in which V satisfies VVT=I (d×d identity matrix).
The PCA bases can be simply computed by eigen-
decomposition on a large set of randomly sampled 60 63.4 65.1 60.0
patches,requiringnogradient-basedtraining. 56.0 58.5 53.0
50 53.9
40 43.7 44.7
Thanks to the simplicity of using patches, for the three
patch-wise PCA
patch-wise tokenizers, we can visualize their filters in the 30 pixel-based 23.6
patchspace(Fig.4). 20
12 48 192 768
Tab.2summarizesthelinearprobeaccuracyofDiTus- latent dim per token (log-scale)
ing these four variants of tokenizers. We show the results Figure5. Linearproberesultsofthepixel-basedtokenizer,op-
w.r.t. the latent dimension “per token”.3 We draw the fol- eratedonanimagesizeof256,128,64,and32,respectivelywith
lowingobservations. apatchsizeof16,8,4,2. The“latent”dimensionsofthesetok-
enizedspacesare768,192,48,and12pertoken. Similartoother
tokenizerswestudy, thispixel-basedtokenizerexhibitsasimilar
Latent dimension of the tokenizer is crucial for DDM to
trend:arelativelysmalldimensionofthelatentspaceisoptimal.
workwellinself-supervisedlearning.
As shown in Tab. 2, all four variants of tokenizers exhibit
isanalogoustoaformofimagepre-processing,ratherthan
similartrends,despitetheirdifferencesinarchitecturesand
a “network architecture”. The effectiveness of a PCA to-
loss functions. Interestingly, the optimal dimension is rel-
kenizer largely helps us push the modern DDM towards a
ativelylow(dis16or32), eventhoughthefulldimension
classicalDAE,aswewillshowinthenextsubsection.
perpatchismuchhigher(16×16×3=768).
Surprisingly,theconvolutionalVAEtokenizerisneither
necessary nor favorable; instead, all patch-based tokeniz- High-resolution, pixel-based DDMs are inferior for self-
ers,inwhicheachpatchisencodedindependently,perform supervisedlearning.
similarly with each other and consistently outperform the
Beforewemoveon,wereportanextraablationthatiscon-
ConvVAEvariant. Inaddition,theKLregularizationterm
sistent with the aforementioned observation. Specifically,
isunnecessary,asboththeAEandPCAvariantsworkwell.
weconsidera“na¨ıvetokenizer”thatperformsidentitymap-
To our further surprise, even the PCA tokenizer works
pingonpatchesextractedfromresizedimages. Inthiscase,
well. Unlike the VAE or AE counterparts, the PCA tok-
a“token”istheflattenvectorconsistingallpixelsofapatch.
enizer does not require gradient-based training. With pre-
InFig.5,weshowtheresultsofthis“pixel-based”tok-
computedPCAbases,theapplicationofthePCAtokenizer
enizer, operated on an image size of 256, 128, 64, and 32,
3Forpatch-wiseVAE/AE/PCA(patchstrideis16),wetreateachpatch respectively with a patch size of 16, 8, 4, 2. The “latent”
asatoken, sothelatentdimensionissimplydforeachpatch. Forthe dimensionsofthesetokenizedspacesare768,192,48,and
defaultconvolutionalVAEthathasastrideof8,theDiTimplementation
12pertoken. Inallcase,thesequencelengthoftheTrans-
[32]treatseach2×2patchonthelatentspaceasa“token”;asaresult,its
latentdimension“pertoken”shouldbemultipliedby4forcalibration. formeriskeptunchanged(256).
5acc.
patch-wisePCAbaseline 65.1 Remove input scaling. In modern DDMs (see Eq. (1)),
+predictcleandata(ratherthannoise) 62.4 the input is scaled by a factor of γ . This is not common
t
+removeinputscaling(fixγt≡1) 63.6 practiceinaclassicalDAE.Next,westudyremovinginput
+operateonimageinputwithinv.PCA 63.6
scaling,i.e.,wesetγ ≡1.Asγ isfixed,weneedtodefine
+operateonimageoutputwithinv.PCA 63.9 t t
anoisescheduledirectlyonσ . Wesimplysetσ asalinear
+predictoriginalimage 64.5 √ t t
schedule from 0 to 2. Moreover, we empirically set the
Table 3. Moving toward a classical DAE, starting from our weight in Eq. (3) as λ = 1/(1+σ2), which again puts
patch-wisePCAtokenizer. Eachlineisbasedonamodification t t
moreemphasisoncleanerdata(smallerσ ).
oftheimmediatelyprecedingline.SeeSec.4.3fordescriptions. t
After fixing γ ≡ 1, we achieve a decent accuracy of
t
63.6%(Tab.3),whichcomparesfavorablywiththevarying
Interestingly, this pixel-based tokenizer exhibits a sim- γ counterpart’s62.4%. Thissuggeststhatscalingthedata
t
ilar trend with other tokenizers we have studied, although byγ isnotnecessaryinourscenario.
t
theoptimaldimensionisshifted. Inparticular,theoptimal
OperateontheimagespacewithinversePCA.Thusfar,
dimensionisd=48, whichcorrespondstoanimagesizeof
for all entries we have explored (except Fig. 5), the model
64 with a patch size of 4. With an image size of 256 and
operatesonthelatentspaceproducedbyatokenizer(Fig.2
apatchsizeof16(d=768),thelinearprobeaccuracydrops
(b)).Ideally,wehopeourDAEcanworkdirectlyontheim-
dramaticallyto23.6%.
agespacewhilestillhavinggoodaccuracy. Withtheusage
These comparisons show that the tokenizer and the re-
ofPCA,wecanachievethisgoalbyinversePCA.
sultinglatentspacearecrucialforDDM/DAEtoworkcom-
TheideaisillustratedinFig.1. Specially,weprojectthe
petitivelyintheself-supervisedlearningscenario.Inpartic-
inputimageintothelatentspacebythePCAbases(i.e.,V),
ular,applyingaclassicalDAEwithadditiveGaussiannoise
add noise in the latent space, and project the noisy latent
onthepixelspaceleadstopoorresults.
back to the image space by the inverse PCA bases (VT).
4.3.TowardClassicalDenoisingAutoencoders Fig.1(middle,bottom)showsanexampleimagewithnoise
addedinthelatentspace.Withthisnoisyimageastheinput
Next,wegoonwithourdeconstructiontrajectoryandaim
to the network, we can apply a standard ViT network [15]
to get as close as possible to the classical DAE [39]. We
thatdirectlyoperateonimages,asifthereisnotokenizer.
attempttoremoveeverysingleaspectthatstillremainsbe-
Applyingthismodificationontheinputside(whilestill
tweenourcurrentPCA-basedDDMandtheclassicalDAE
predicting the output on the latent space) has 63.6% accu-
practice. Viathisdeconstructiveprocess,wegainbetterun-
racy (Tab. 3). Further applying it to the output side (i.e.,
derstandings on how every modern design may influence
predictingtheoutputontheimagespacewithinversePCA)
theclassicalDAE.Tab.3givestheresults,discussednext.
has63.9%accuracy.Bothresultsshowthatoperatingonthe
Predict clean data (rather than noise). While modern imagespacewithinversePCAcanachievesimilarresultsas
DDMscommonlypredictthenoiseϵ(seeEq.(2)),theclas- operatingonthelatentspace.
sicalDAEpredictsthecleandatainstead. Weexaminethis
Predict original image. While inverse PCA can produce
differencebyminimizingthefollowinglossfunction:
apredictiontargetintheimagespace,thistargetisnotthe
λ ∥z −net(z )∥2 (3) originalimage. ThisisbecausePCAisalossyencoderfor
t 0 t
any reduced dimension d. In contrast, it is a more natural
Herez isthecleandata(inthelatentspace), andnet(z ) solutiontopredicttheoriginalimagedirectly.
0 t
isthe networkprediction. λ isa t-dependentloss weight, Whenweletthenetworkpredicttheoriginalimage,the
t
introducedtobalancethecontributionofdifferentnoiselev- “noise”introducedincludestwoparts:(i)theadditiveGaus-
els[34]. Itissuggestedtosetλ = γ2/σ2 asper[34]. We siannoise,whoseintrinsicdimensionisd,and(ii)thePCA
t t t
find that setting λ = γ2 works better in our scenario. In- reconstructionerror,whoseintrinsicdimensionisD−d(D
t t
tuitively,itsimplyputsmoreweighttothelosstermsofthe is768). Weweightthelossofbothpartsdifferently.
cleanerdata(largerγ ). Formally,withthecleanoriginalimagex andnetwork
t 0
With the modification of predicting clean data (rather predictionnet(x ),wecancomputetheresiduerprojected
t
thannoise),thelinearprobeaccuracydegradesfrom65.1% ontothefullPCAspace: r ≜V(x −net(x )). HereV is
0 t
to62.4%(Tab.3). Thissuggeststhatthechoiceofthepre- theD-by-D matrixrepresentingthefullPCAbases. Then
dictiontargetinfluencestherepresentationquality. weminimizethefollowinglossfunction:
Even though we suffer from a degradation in this step,
D
wewillsticktothismodificationfromnowon,asourgoal (cid:88)
λ w r2. (4)
istomovetowardsaclassicalDAE.4 t i i
i=1
4Wehaverevisitedundoingthischangeinourfinalentry,inwhichwe
havenotobservedthisdegradation. Hereidenotesthei-thdimensionofthevectorr. Theper-
6DiT baseline 57.5
remove cls-cond 62.5
remove VQGAN perc. loss 58.4
remove VQGAN adv. loss 59.0
replace noise sched. 63.4
conv. VAE to patch-wise VAE 64.9
to patch-wise AE 64.7
to patch-wise PCA 65.1 Figure7. Visualization:pixelnoisevs.latentnoise.Left:clean
image, 256×256 pixels. Middle: Gaussian noise added to the
predict clean data 62.4
pixelspace. Right: Gaussiannoiseaddedtothelatentspacepro-
remove input scaling 63.6
ducedbythePCAtokenizer,visualizedbybackprojectiontothe
use image input 63.6 imagespaceusinginversePCA.σ=(cid:112) 1/3inbothcases.
use image output 63.9
predict original image 64.5 5.AnalysisandComparisons
56 58 60 62 64 66
linear probing accuracy Visualizinglatentnoise. Conceptually,l-DAEisaformof
Figure6. Theoveralldeconstructivetrajectoryfromamodern DAEthatlearnstoremovenoiseaddedtothelatentspace.
DDMtol-DAE,summarizingTab.1,Tab.2,andTab.3.Eachline
ThankstothesimplicityofPCA,wecaneasilyvisualizethe
isbasedonamodificationoftheimmediatelyprecedingline.
latentnoisebyinversePCA.
Fig.7comparesthenoiseaddedtopixelsvs.tothelatent.
dimensionweightw is1fori≤d,and0.1ford<i≤D. Unlike the pixel noise, the latent noise is largely indepen-
i
Intuitively, w down-weights the loss of the PCA recon- dent of the resolution of the image. With patch-wise PCA
i
structionerror. Withthisformulation, predictingtheorigi- asthetokenizer,thepatternofthelatentnoiseismainlyde-
nalimageachieves64.5%linearprobeaccuracy(Tab.3). terminedbythepatchsize.Intuitively,wemaythinkofitas
usingpatches,ratherthanpixels,toresolvetheimage. This
This variant is conceptually very simple: its input is a
behaviorresemblesMAE[21],whichmasksoutpatchesin-
noisyimagewhosenoiseisaddedinthePCAlatentspace,
steadofindividualpixels.
itspredictionistheoriginalcleanimage(Fig.1).
Denoisingresults. Fig.8showsmoreexamplesofdenois-
Singlenoiselevel. Lastly,outofcuriosity,wefurtherstudy
ing results based on l-DAE. Our method produces reason-
a variant with single-level noise. We note that multi-level
able predictions despite of the heavy noise. We note that
noise, given by noisescheduling, is a propertymotived by
thisislessofasurprise,becauseneuralnetwork-basedim-
thediffusionprocessinDDMs;itisconceptuallyunneces-
agerestoration[4,14]hasbeenanintensivelystudiedfield.
saryinaclassicalDAE.
(cid:112) Nevertheless, the visualization may help us better un-
Wefixthenoiselevelσasaconstant( 1/3).Usingthis
derstand how l-DAE may learn good representations. The
single-levelnoiseachievesdecentaccuracyof61.5%,a3%
heavynoiseaddedtothelatentspacecreatesachallenging
degradation vs. the multi-level noise counterpart (64.5%).
pretext task for the model to solve. It is nontrivial (even
Usingmultiplelevelsofnoiseisanalogoustoaformofdata
for human beings) to predict the content based on one or
augmentation in DAE: it is beneficial, but not an enabling
a few noisy patches locally; the model is forced to learn
factor. This also implies that the representation capability
higher-level, more holistic semantics to make sense of the
ofDDMismainlygainedbythedenoising-drivenprocess,
underlyingobjectsandscenes.
notadiffusion-drivenprocess.
Data augmentation. Notably, all models we present thus
As multi-level noise is useful and conceptually simple,
farhavenodataaugmentation:onlythecentercropsofim-
wekeepitinourfinalentriespresentedinthenextsection.
ages are used, with no random resizing or color jittering,
following[11,32]. Wefurtherexploreamilddataaugmen-
4.4.Summary
tation(randomresizedcrop)forourfinall-DAE:
Insum,wedeconstructamodernDDMandpushittowards
aug. centercrop randomcrop
a classical DAE (Fig. 6). We undo many of the modern
acc. 64.5 65.0
designsandconceptuallyretainonlytwodesignsinherited
frommodernDDMs: (i)alow-dimensionallatentspacein which has slight improvement. This suggests that the rep-
whichnoiseisadded;and(ii)multi-levelnoise. resentationlearningabilityofl-DAEislargelyindependent
We use the entry at the end of Tab. 3 as our final DAE of its reliance on data augmentation. A similar behavior
instantiation(illustratedinFig.1). Werefertothismethod wasobservedinMAE[21],whichsharplydiffersfromthe
as“latentDenoisingAutoencoder”,orinshort,l-DAE. behaviorofcontrastivelearningmethods(e.g.,[6]).
7Figure 8. Denoising results of l-DAE, evaluated on ImageNet validation images. This denoising problem, serving as a pretext task,
encourages the network to learn meaningful representations in a self-supervised manner. For each case, we show: (left) clean image;
(middle)noisyimagethatistheinputtothenetwork,wherethenoiseisaddedtothelatentspace;(right)denoisedoutput.
Training epochs. All our experiments thus far are based method ViT-B(86M) ViT-L(304M)
on400-epochtraining. FollowingMAE[21],wealsostudy MoCov3 76.7 77.6
MAE 68.0 75.8
trainingfor800and1600epochs:
l-DAE 66.6 75.0
epochs 400 800 1600 Table4. ComparisonswithpreviousbaselinesofMoCov3[7]
acc. 65.0 67.5 69.6 andMAE[21]. TheentriesofbothMAEandl-DAEaretrained
for1600epochsandwithrandomcropasthedataaugmentation.
As a reference, MAE [21] has a significant gain (4%) ex-
Linear probe accuracy on ImageNet is reported. In the brackets
tending from 400 to 800 epochs, and MoCo v3 [7] has arethenumberofparametersoftheencoder.
nearlynogain(0.2%)extendingfrom300to600epochs.
Modelsize.Thusfar,ourallmodelsarebasedontheDiT-L fairaspossiblebetweenMAEandl-DAE:botharetrained
variant[32],whoseencoderanddecoderareboth“ViT-1L”
2 for1600epochsandwithrandomcropasthedataaugmen-
(half depth of ViT-L). We further train models of different
tation. On the other hand, we should also note that MAE
sizes,whoseencoderisViT-BorViT-L(decoderisalways
ismoreefficientintrainingbecauseitonlyoperatesonun-
ofthesamesizeasencoder): masked patches. Nevertheless, we have largely closed the
encoder ViT-B ViT-1L ViT-L accuracygapbetweenMAEandaDAE-drivenmethod.
2
Last,weobservethatautoencoder-basedmethods(MAE
acc. 60.3 65.0 70.9
and l-DAE) still fall short in comparison with contrastive
Weobserveagoodscalingbehaviorw.r.t.modelsize: scal- learning methods under this protocol, especially when the
ingfromViT-BtoViT-Lhasalargegainof10.6%. Asim- model is small. We hope our study will draw more atten-
ilar scaling behavior is also observed in MAE [21], which tiontotheresearchonautoencoder-basedmethodsforself-
hasa7.8%gainfromViT-BtoViT-L. supervisedlearning.
Comparison with previous baselines. Finally, to have
6.Conclusion
a better sense of how different families of self-supervised
learningmethodsperform,wecomparewithpreviousbase-
We have reported that l-DAE, which largely resembles the
linesinTab.4. WeconsiderMoCov3[7],whichbelongsto
classicalDAE,canperformcompetitivelyinself-supervised
thefamilyofcontrastivelearningmethods,andMAE[21],
learning. The critical component is a low-dimensional la-
whichbelongstothefamilyofmasking-basedmethods.
tentspaceonwhichnoiseisadded. Wehopeourdiscovery
Interestingly, l-DAE performs decently in comparison
willreigniteinterestindenoising-basedmethodsinthecon-
withMAE,showingadegradationof1.4%(ViT-B)or0.8%
textoftoday’sself-supervisedlearningresearch.
(ViT-L).Wenotethatherethetrainingsettingsaremadeas
8Acknowledgement. We thank Pascal Vincent, Mike Rab- The DiT-L [32] has 24 blocks where the first 12 blocks
bat,andRossGirshickfortheirdiscussionandfeedback. arereferredtoasthe“encoder”(henceViT-1L)andtheoth-
2
ers the “decoder”. This separation of the encoder and de-
A.ImplementationDetails coderisartificial. Inthefollowingtable,weshowthelinear
probingresultsusingdifferentnumbersofblocksintheen-
DiT architecture. We follow the DiT architecture de- coder,usingthesamepre-trainedmodel:
sign [32]. The DiT architecture is similar to the original
enc.blocks 9 10 11 12 13 14 15
ViT [15], with extra modifications made for conditioning.
acc. 58.5 62.0 64.1 64.5 63.6 61.9 59.7
Each Transformer block accepts an embedding network (a
Theoptimalaccuracyisachievedwhentheencoderandde-
two-layerMLP)conditionedonthetimestept. Theoutput
coderhavethesamedepth. Thisbehaviorisdifferentfrom
of this embedding network determines the scale and bias
MAE’s[21],whoseencoderanddecoderareasymmetric.
parameters of LayerNorm [1], referred to as adaLN [32].
Slightly different from [32], we set the hidden dimension
B.Fine-tuningResults
of this MLP as 1/4 of its original dimension, which helps
reducemodelsizesandsavememory,atnoaccuracycost. Inadditiontolinearprobing,wealsoreportend-to-endfine-
Training. The original DiTs [32] are trained with a batch tuning results. We closely followed MAE’s protocol [21].
size of 256. To speed up our exploration, we increase the We use clean, 256×256-sized images as the inputs to the
batchsizeto2048.Weperformlinearlearningratewarmup encoder. Globally average pooled outputs are used as fea-
[19]for100epochsandthendecayitfollowingahalf-cycle tures for classification. The training batch size is 1024,
cosineschedule. Weuseabaselearningrateblr=1e-4[32] initial learning rate is 4×10−3, weight decay is 0.05, drop
bydefault,andsettheactuallrfollowingthelinearscaling path[24]is0.1,andtraininglengthis100epochs. Weuse
rule[19]: blr ×batch size/256. Noweightdecayisused a layer-wise learning rate decay of 0.85 (B) or 0.65 (L).
[32]. We train for 400 epochs by default. On a 256-core MixUp[43](0.8),CutMix[42](1.0),RandAug[8](9,0.5),
TPU-v3pod,trainingDiT-Ltakes12hours. andexponentialmovingaverage(0.9999)areused,similar
to[21]. Theresultsaresummarizedasbelow:
Linear probing. Our linear probing implementation fol-
method ViT-B ViT-L
lows the practice of MAE [21]. We use clean, 256×256-
MoCov3 83.2 84.1
sized images for linear probing training and evaluation.
MAE 83.6 85.9
The ViT output feature map is globally pooled by average l-DAE 83.7 84.7
pooling. It is then processed by a parameter-free Batch-
Overall, both autoencoder-based methods are better than
Norm[25]layerandalinearclassifierlayer,following[21]. MoCov3.l-DAEperformssimilarlywithMAEwithViT-B,
Thetrainingbatchsizeis16384,learningrateis6.4×10−3
butstillfallshortofMAEwithViT-L.
(cosine decay schedule), weight decay is 0, and training
length is 90 epochs. Randomly resized crop and flipping References
areusedduringtrainingandasinglecentercropisusedfor
[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.
testing. Top-1accuracyisreported.
Layernormalization. arXiv:1607.06450,2016.
While the model is conditioned on t in self-supervised
[2] MarceloBertalmio,GuillermoSapiro,VincentCaselles,and
pre-training,conditioningisnotneededintransferlearning
ColomaBallester. Imageinpainting. InSIGGRAPH,2000.
(e.g., linear probing). We fix the time step t value in our
[3] Christopher M Bishop and Nasser M Nasrabadi. Pattern
linearprobingtrainingandevaluation. Theinfluenceofdif-
recognitionandmachinelearning. Springer,2006.
ferenttvalues(outof1000timesteps)isshownasfollows:
[4] HaroldChristopherBurger, ChristianJSchuler, andStefan
fixedt 0 10 20 40 80 Harmeling. Image denoising: Can plain neural networks
w/cleaninput 64.1 64.5 64.1 63.3 62.2 competewithBM3D? InCVPR,2012.
w/noisyinput 64.2 65.0 65.0 65.0 64.5 [5] KaiChen,ZhiliLiu,LanqingHong,HangXu,ZhenguoLi,
Wenotethatthetvaluedetermines: (i)themodelweights, andDit-YanYeung. Mixedautoencoderforself-supervised
visualrepresentationlearning. InCVPR,2023.
whichareconditionedont,and(ii)thenoiseaddedintrans-
[6] TingChen,SimonKornblith,MohammadNorouzi,andGe-
fer learning, using the same level of t. Both are shown in
offreyHinton. Asimpleframeworkforcontrastivelearning
thistable. Weuset=10andcleaninputinallourexperi-
ofvisualrepresentations. InICML,2020.
ments,exceptTab.4whereweusetheoptimalsetting.
[7] Xinlei Chen, Saining Xie, and Kaiming He. An empirical
Fixing t also means that the t-dependent MLP layers,
study of training self-supervised Vision Transformers. In
whichareusedforconditioning,arenotexposedintransfer
ICCV,2021.
learning,becausetheycanbemergedgiventhefixedt. As [8] EkinDCubuk,BarretZoph,JonathonShlens,andQuocV
such, our model has the number of parameters just similar Le. Randaugment: Practical automated data augmentation
tothestandardViT[15],asreportedinTab.4. withareducedsearchspace. InCVPRWorkshops,2020.
9[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, LawrenceD Jackel. Backpropagationapplied tohandwrit-
andLiFei-Fei. ImageNet: Alarge-scalehierarchicalimage tenzipcoderecognition. Neuralcomputation,1989.
database. InCVPR,2009. [28] Soumik Mukhopadhyay, Matthew Gwilliam, Vatsal Agar-
[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina wal, Namitha Padmanabhan, Archana Swaminathan,
Toutanova. BERT:Pre-trainingofdeepbidirectionaltrans- Srinidhi Hegde, Tianyi Zhou, and Abhinav Shrivastava.
formersforlanguageunderstanding. InNAACL,2019. Diffusion models beat GANs on image classification.
[11] PrafullaDhariwalandAlexanderNichol. Diffusionmodels arXiv:2307.08702,2023.
beatGANsonimagesynthesis. InNeurIPS,2021. [29] Alexander Quinn Nichol and Prafulla Dhariwal. Improved
[12] JeffDonahueandKarenSimonyan. Largescaleadversarial denoisingdiffusionprobabilisticmodels. InICML,2021.
representationlearning. arXiv:1907.02544,2019. [30] Aaron van den Oord, Oriol Vinyals, and Koray
[13] JeffDonahue,PhilippKra¨henbu¨hl,andTrevorDarrell. Ad- Kavukcuoglu. Neural discrete representation learning.
versarialfeaturelearning. InICLR,2017. InNeurIPS,2017.
[14] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou [31] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor
Tang. Learning a deep convolutional network for image Darrell, and Alexei A Efros. Context encoders: Feature
super-resolution. InECCV,2014. learningbyinpainting. InCVPR,2016.
[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, [32] WilliamPeeblesandSainingXie. Scalablediffusionmodels
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, withTransformers. InICCV,2023.
MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl- [33] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
vainGelly,JakobUszkoreit,andNeilHoulsby. Animageis PatrickEsser,andBjo¨rnOmmer.High-resolutionimagesyn-
worth16x16words: Transformersforimagerecognitionat thesiswithlatentdiffusionmodels. InCVPR,2022.
scale. InICLR,2021. [34] TimSalimansandJonathanHo. Progressivedistillationfor
[16] PatrickEsser,RobinRombach,andBjo¨rnOmmer. Taming fastsamplingofdiffusionmodels. InICLR,2022.
transformersforhigh-resolutionimagesynthesis. InCVPR, [35] KarenSimonyanandAndrewZisserman. Verydeepconvo-
2021. lutionalnetworksforlarge-scaleimagerecognition.InICLR,
[17] Yuxin Fang, Li Dong, Hangbo Bao, Xinggang Wang, and 2015.
FuruWei. Corruptedimagemodelingforself-supervisedvi- [36] JaschaSohl-Dickstein, EricWeiss, NiruMaheswaranathan,
sualpre-training. arXiv:2202.03382,2022. and Surya Ganguli. Deep unsupervised learning using
[18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing nonequilibriumthermodynamics. InICML,2015.
Xu,DavidWarde-Farley,SherjilOzair,AaronCourville,and [37] YangSongandStefanoErmon. Generativemodelingbyes-
Yoshua Bengio. Generative adversarial nets. In NeurIPS, timatinggradientsofthedatadistribution. NeurIPS,2019.
2014.
[38] YangSong,JaschaSohl-Dickstein,DiederikPKingma,Ab-
[19] Priya Goyal, Piotr Dolla´r, Ross Girshick, Pieter Noord- hishekKumar,StefanoErmon,andBenPoole. Score-based
huis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, generative modeling through stochastic differential equa-
Yangqing Jia, and Kaiming He. Accurate, large minibatch tions. arXiv:2011.13456,2020.
SGD:TrainingImageNetin1hour.arXiv:1706.02677,2017.
[39] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
[20] KaimingHe,HaoqiFan,YuxinWu,SainingXie,andRoss Pierre-AntoineManzagol. Extractingandcomposingrobust
Girshick. Momentumcontrastforunsupervisedvisualrep- featureswithdenoisingautoencoders. InICML,2008.
resentationlearning. InCVPR,2020.
[40] WeilaiXiang,HongyuYang,DiHuang,andYunhongWang.
[21] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Denoisingdiffusionautoencodersareunifiedself-supervised
Dolla´r,andRossGirshick.Maskedautoencodersarescalable learners. InICCV,2023.
visionlearners. InCVPR,2022.
[41] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin
[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bao,ZhuliangYao,QiDai,andHanHu.SimMIM:Asimple
Bernhard Nessler, and Sepp Hochreiter. GANs trained by frameworkformaskedimagemodeling. InCVPR,2022.
atwotime-scaleupdateruleconvergetoalocalNashequi-
[42] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk
librium. InNeurIPS,2017.
Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-
[23] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffu-
larizationstrategytotrainstrongclassifierswithlocalizable
sionprobabilisticmodels. InNeurIPS,2020. features. InICCV,2019.
[24] GaoHuang,YuSun,ZhuangLiu,DanielSedra,andKilianQ
[43] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and
Weinberger.Deepnetworkswithstochasticdepth.InECCV,
DavidLopez-Paz. mixup: Beyondempiricalriskminimiza-
2016. tion. InICLR,2018.
[25] Sergey Ioffe and Christian Szegedy. Batch normalization:
[44] RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,
Acceleratingdeepnetworktrainingbyreducinginternalco-
and Oliver Wang. The unreasonable effectiveness of deep
variateshift. InICML,2015.
featuresasaperceptualmetric. InCVPR,2018.
[26] DiederikPKingmaandMaxWelling. Auto-encodingvaria-
tionalbayes. InICLR,2013.
[27] Yann LeCun, Bernhard Boser, John S Denker, Donnie
Henderson, Richard E Howard, Wayne Hubbard, and
10