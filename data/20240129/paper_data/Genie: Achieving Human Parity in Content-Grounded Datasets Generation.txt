PublishedasaconferencepaperatICLR2024
GENIE: ACHIEVING HUMAN PARITY IN
CONTENT-GROUNDED DATASETS GENERATION
AsafYehudai♢♣,BoazCarmeli♢,YosiMass♢,OfirArviv♢,NathanielMills♢,
AssafToledo♢,EyalShnarch♢,LeshemChoshen♢♡
IBMIsraelResearchLab♢,HebrewUniversityofJerusalem♣,MIT♡
{Asaf.Yehudai, leshem.choshen}@ibm.com
ABSTRACT
Thelackofhigh-qualitydataforcontent-groundedgenerationtaskshasbeeniden-
tified as a major obstacle to advancing these tasks. To address this gap, we pro-
pose Genie, a novel method for automatically generating high-quality content-
grounded data. It consists of three stages: (a) Content Preparation, (b) Genera-
tion:creatingtask-specificexamplesfromthecontent(e.g.,question-answerpairs
or summaries). (c) Filtering mechanism aiming to ensure the quality and faith-
fulnessofthegenerateddata. Weshowcasethismethodologybygeneratingthree
large-scale synthetic data, making wishes, for Long-Form Question-Answering
(LFQA),summarization,andinformationextraction. Inahumanevaluation,our
generateddatawasfoundtobenaturalandofhighquality. Furthermore,wecom-
paremodelstrainedonourdatawithmodelstrainedonhuman-writtendata–ELI5
and ASQA for LFQA and CNN-DailyMail for Summarization. We show that
ourmodelsareonparwithoroutperformingmodelstrainedonhuman-generated
dataandconsistentlyoutperformingtheminfaithfulness. Finally,weappliedour
method to create LFQA data within the medical domain and compared a model
trainedonitwithmodelstrainedonotherdomains.
1 INTRODUCTION
Content-grounded generation is needed in various tasks, such as Retrieval-Augmented Generation
(RAG), and content-based virtual assistants. In such tasks, the model is expected to generate a
response based on a given content (i.e., information). For example, answer a question given a
documentthatincludesinformationneededfortheanswer. Zhengetal.(2023)foundthosetypesof
taskstobethesecondmostcommonusecasesofLanguageModels.
Creating datasets with elaborate responses, which rely on long content, requires an expensive and
demandingmanualprocess. Thismayexplainwhysuchdatasetsarescarceevenforpopulartasks
suchasquestion-answeringgeneration. Moreover,mostexistingdatasetswerecollectedfromnoisy
availableresources,suchasnewsproviders(Hermannetal.,2015)andReddituserposts(Fanetal.,
2019). Thislackofhigh-qualitycontent-groundeddatahasbeenidentifiedasoneoftheobstacles
toadvancinglong-formQA(Stelmakhetal.,2022)anddomain-specificsummarization(Zhuetal.,
2020),amongothercontent-basedtasks.
To address this gap, we suggest Genie, Generate information & elucidate1, a method for creating
synthetictrainingdataforanydomainandanycontent-groundedtask. Weproposeathree-steppro-
cess:(a)ContentPreparation,(b)Generating,and(c)Filtering.Preparationisfairlystraightforward,
asthedatamaybenoisy;itisbesttocleanit.Thegenerationisdoneusingafew-shotpromptingap-
proachwithalargelanguagemodel(LLM).SeeanexampleinApp.D.Finally,sincethegeneration
isautomatic,wefilteritsoutputstoensuretheirfaithfulness,well-formedness,andoverallquality.
Genieoffersflexibilityandcangeneratesyntheticdatafordifferentdomainsandcontent-grounded
generation tasks. We apply it to the tasks of long-form QA (LFQA), summarization (§ 3), and
information extraction (IE) (App. C) by creating wish-QA, wish-summarization, and wish-IE. We
then show in a manual evaluation that it generates high-quality data that is natural, faithful, and
1Wewishedforacoolnameandthatiswhatwe’vegot.
1
4202
naJ
52
]LC.sc[
1v76341.1042:viXraPublishedasaconferencepaperatICLR2024
1: Content Preparation
Few-Shot Learning Prompt
Task description
Content:Example
…
Content:Example
<New Content Here>
Corpus Extracted Content LLM
Reward Faithfulness
Model Model
Task
Data
Generated Task Example
Question: What is the lowest point on dry land?
Answer: The lowest point on dry land is the
shore of the Dead Sea, shared by Israel, and
Jordan. It is 418 m (1,371 feet) below sea level.
3: Filtering 2: Generation
Figure 1: Genie’s three steps are: (1) Extract content from the source data (2) Prompt an LLM to
generatetask-specificexamplesbasedontheprovidedcontent(3)filterlow-qualityandunfaithful
examplestoensuredataquality.
lexically diverse (§4). For the task of LFQA, we compare the performance of models that were
trainedwithwish-QAgeneratedbyGenietothosetrainedwiththesameamountofdata,generated
byhumans(§5). Weshowthattheformermodelsoutperformorareonparwiththelattermodels.
Additionally,faithfulnessscoresshowthatmodelstrainedonoursyntheticdataaremorefaithfulto
thegroundingcontent.Thoseresultsshowtheoverallefficacyandfaithfulnessofourdataastraining
data compared to that of human-generated data. We replicate our success with summarization,
showcasingthegeneralityofthemethod. Wepubliclyreleaseallthreewishesdatasets.
2 AUTOMATICALLY CURATING DATASET FOR CONTENT-GROUNDED TASKS
Next,wedetailthestepsofGenieforautomaticallycuratinghigh-qualitycontent-groundeddatasets.
Figure1depictsthethreestepsinGenie: ContentPreparation,Generation,andFiltering. Forsim-
plicity, we refer to content-grounded data point, like a question-answer pair or a summary, as an
example.
2.1 CONTENTPREPARATION
In the preparation step, we obtain the grounding content. This is done in a rule-based fashion by
extractingpassagesfromtherawdocuments.Thisstepistheleastgeneralofourapproachasitrelies
onthespecificformatinwhichthedataisstored. Ifthedataalreadyexistsineasy-to-usepassages,
itcanbeusedasis. Forexample,brokenbylines,foundinatable,orconvenientlyextractedfrom
another dataset. As the general case, we describe the extraction of content passages directly from
webpages.
Implementationdetails. WecrawledWikipediapagesusingbrowseremulationtoallowdynamic
content to be retrieved. Then we pass the full HTML DOM through filters to remove noise (e,g.,
headers,footers,sidebars,etc.). Weareleftwiththemainpagecontentwhichisthentransformed
intoMarkdown, preservingthedocumentstructure(e.g.,lists, tables, links, imagereferences, arti-
cles, andsections). Fromthisstructureatableofcontentsisderivedandbasedonitwebreakthe
Markdownpageintopassages.
2PublishedasaconferencepaperatICLR2024
2.2 GENERATION
Inthegenerationstep,wepromptalargelanguagemodeltogenerateasyntheticexample. Forthat,
we use the in-context capabilities of the model. We prompt the model with four content-example
pairsfollowedbytheextractedcontentfromthecorpuswithnoexample(seethepromptinappendix
§D).TheLLMgeneratesanewexampletomatchtheextractedcontent.
Implementation details. We decode greedily, which encourages the models to produce more
grounded responses (Honovich et al., 2022b). In addition, we create two variants of the data, one
by generating examples using Falcon-40B (Penedo et al., 2023) and another by generating with
Llama-2-70B(Touvronetal.,2023). Ingeneral,theresultsusingthedifferentmodelshavesimilar
tendencies, with Llama being slightly better (see replications with Llama in an appendix §B). As
Falconispurelypre-trained,withoutadditionalstepswemainlyreportresultsrelyingonFalcon,to
showcasethatourmethodisnotdependentonfurtheralignmentandinstructionsteps.
2.3 FILTERING
Inthefilteringstep,wescoreeachcontent-examplepairforitsformat,faithfulness(i.e.,grounded),
andquality. Foreachsuchaspect,weimplementascoringfunctionandfilterlow-scoringpairs.
Format. We filter out examples where parts of the template are missing (e.g., in QA, when the
prefixes signifying the start of the question or the answer are absent). Furthermore, we filter ex-
amples that are too short (less than ten words) or too long (surpassing 1.5 times the length of the
groundingcontentforLFQA,and0.25forSummarization).
ForQA,weuse[document],[question],and[answer]asprefixesbeforeeachcorrespondingelement.
For summarization [document], [summarize], and [summary] with [summarize] representing the
specificsummarizationinstruction. Itisimportanttonotethatwedidnotfine-tunetheseprompts.
Faithfulness. Tovalidatethatthemodel-generatedexamplesaregroundedinthecontent,weadopt
anoff-the-shelffaithfulnessmetricandfilterlow-scoringexamples. Whendeployedwithtrustwor-
thydata,thiscanserveasameasureofcorrectness.
WetestfaithfulnessbymappingtheproblemintoaTextualEntailment(Daganetal.,2005)orNat-
uralLanguageInference(Bowmanetal.,2015)(NLI)problem. NLIinvolvestwoinputsentences:
a hypothesis and a premise. The objective is to determine whether the hypothesis can be inferred
from the premise, contradicts it, or is neutral with respect to it. NLI models were widely utilized
forfaithfulnessconsistencyevaluation(Honovichetal.,2021;Dzirietal.,2022),andmostsimply
bytakingthegroundingtextasthepremiseandthegeneratedexampleasthehypothesis(Maynez
etal.,2020). Here,weusethefine-tuningT5-11BNLImodelpresentedinHonovichetal.(2022a)
forassessingthegeneratedexamplefaithfulness.
Quality. Animportantaspectofourmethodologyinvolvesevaluatingthequalityofthegenerated
examples,specificallyquantifyingtheirrelevancetothecorrespondingtask. Notethatthetaskmay
be constant throughout a dataset (as is often the case of summarization) or be dependent upon an
instruction(suchasthequestioninquestionanswering). Tojudgethequalityautomaticallyweuse
arewardmodel.
Rewardmodelsaretrainedonhumanpreferencedatatogiveahighrewardforanswersthathuman
annotatorsprefer. Suchmodelscanquantifyqualityinahuman-likeway, consideringdimensions
thatarehardtoisolateandmeasureindependentlybydedicatedmetrics. Rewardmodelsareused
asqualityscoresforReinforcementLearningoptimization(Ouyangetal.,2022),andalsoserveas
reference-lessevaluationmetricsfortextgenerationtasks(Touvronetal.,2023).
Here, we use the reward model for both purposes and rely on the Open-Assistant model (Ko¨pf
et al., 2023), using the DeBERTa-v3 architecture (He et al., 2021). We filter generated examples
whosescoreisbelow0.5bytherewardmodelreward-model-deberta-v3-large-v2. 2 Wechose0.5
2https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large-v2
3PublishedasaconferencepaperatICLR2024
asathresholdbasedonexperimentation. Similarly,weuset5 xxl true nli mixture3 modeltofilter
examplesdeemedunfaithfulbyit.
3 EXPERIMENTAL SETUP
Here we describe the datasets we utilized in our content-grounded generation tasks: LFQA and
summarization(§3.1). Subsequently,weoutlinethevarioussyntheticdatasetswegenerated(§3.2),
andfinally,wediscussthemodelsemployedfortrainingandtheevaluationmetrics(§3.4)
3.1 DATASETS
ELI5. (Explain Like I’m Five) (Fan et al., 2019) comprises open-ended questions and extensive
responses authored by users within the Reddit forum of the same name. To these questions and
answers,retrieveddocumentswereaddedasgroundingcontent. Intheirmanualanalysis,theyhave
foundthatthecontentissufficienttoanswer65%ofthequestionsandhaveinformationrelevantto
92%ofthequestions. Inthiswork,weusetheKILTversionofthedataset(Petronietal.,2020).
ASQA. (Answer Summaries for Questions which are Ambiguous) (Stelmakh et al., 2022) is a
dataset that pairs ambiguous questions from the AmbigQA dataset (Min et al., 2020) with metic-
ulouslycraftedlong-formanswersgeneratedthroughcrowdsourcing. Toaddgroundingtheyhave
usedthesamemethodpresentedinELI5,butspecificallyretrieveddocumentsfromWikipedia.
NQ. (Natural Questions) (Kwiatkowski et al., 2019) is a dataset of real user questions sourced
fromtheGooglesearchengine. Itincludesquestionsandtheircorrespondingpassages(namedlong
answers) from Wikipedia which provide potential answers and contain extractive short answers.
This dataset does not have long-form answers, and here we will use only its documents for our
syntheticdatagenerationprocess§3.2andwillcompareoursyntheticquestionswiththequestions
fromNQ.
CNN-DailyMail. (Kwiatkowskietal.,2019)isadatasetcommonlyusedfortextsummarization.
ItconsistsofnewsarticlesfromCNNandtheDailyMailalongwiththeirhuman-writtensummaries.
3.2 GENERATINGSYNTHETICDATASETS
Thedatasetsdescribedabovewereusedtocreatedatasetsofsyntheticdata;
Wish-QA-NQ. To create this dataset, we draw upon NQ passages (Kwiatkowski et al., 2019),
for our synthetic data generation process. These passages are well-suited for our process because
they were originally extracted from Wikipedia pages by annotators and typically consist of well-
structuredparagraphs,eachcenteredaroundaspecifictopic.
Wish-QA ELI5/ASQA. For the creation of a dataset that mimics the conditions of ELI5 and
ASQA,whereanswerscanbederivedfrommultipledocuments,werelyonthetopthreeretrieved
passagesfromeitherofthecorrespondingcorpus. Thesepassagesareusedasthegroundingdocu-
mentsforconstructingthissyntheticdataset.
Inaddition,wemakeanewwishdatasetentirelyfromcrawleddata:
Wish-QA. standsforWikipediafromScratch4,isanoveldataweconstructedfollowingthegen-
eralapproachforcrawlingandprocessingasdetailedinSection§2.1. Itrepresentsarealisticdata
generation use case from unprocessed content. We note that the extracted passages may exhibit
noiseandlackofcoherenceandconciseness.
3https://huggingface.co/google/t5 xxl true nli mixture
4Wish-QAisalsothegeneralnameforalloursyntheticQAdatasets
4PublishedasaconferencepaperatICLR2024
3.3 MODELSFOREXTRINSICEVALUATION
IntheExtrinsicevaluation,ourgoalistocomparetheperformanceofmodelstrainedonoursynthetic
content-groundeddatawiththosetrainedondatageneratedbyhumans.Toensureafaircomparison,
wemaintainanequalnumberofexamplesfromeachdataset(10,000)andemployidenticalmodels
fortraining,usingthesamesetofhyperparameters. ThemodelsweusefortrainingareFlan-xl(Wei
etal.,2021)andllama-2-13b-Chat(Touvronetal.,2023). Thesemodelsserveasthefoundationfor
facilitating comparisons across architectural variations, including Encoder-Decoder and Decoder-
onlymodels,aswellasdifferentvariationsofinstructionfine-tuningandalignmenttraining.
3.4 EVALUATIONMETRICS
WeevaluatetheperformancewithROUGEasalexicalsimilaritymetric(Lin,2004), BERT-Score
asamodel-basedreference-basedmetric(Zhangetal.,2019b),andRewardmodelasamodel-based
reference-lessmetric. WereusetheANLIfaithfulnessmetricandrewardmentionedinthefiltering
forevaluation.Forfaithfulnessevaluation,wealsocalculatetheK-Precisionlexicalsimilaritymetric
(Adlakhaetal.,2023). Differentperformancemetrics(Post,2018;Zhangetal.,2019a, andmore)
showedsimilarresultsininitialtrials,showingreliabilityofdifferentforms(Perlitzetal.,2023).
ROUGE. Followingtheconventionalapproachofassessinggeneratedtextquality,includinglong-
formanswers(Fanetal.,2019),wereporttheROUGE-Lscore(Lin,2004).
BERTScore. (Zhangetal.,2019b)isasemanticsimilarity-basedmetricthatleveragespre-trained
language models to predict if the model response is semantically equivalent to the gold answer.
Kasaietal.(2022)haveshownBERTScoreF1iseffectiveinevaluatingmanygenerationtasks.
K-Precision. Following Adlakha et al. (2023) we report K-Precision, as it showed the highest
correlationwithhumanjudgmentsfromalllexicalmetrics. Themetricfollowstheintuition,thatin
faithfulresponsemostwordsneedtocomefromthecontent.
4 INTRINSIC EVALUATION
In this section, we perform intrinsic evaluation and validation of Wish-QA. We conduct a micro
Turing Test, presenting synthetic and human questions side by side. We show that the questions
generatedsyntheticallyaremorenaturalthanmostofthosefoundinavailabledatasets. Wealsotest
thewholeworkflowandshowthatthefilterscontributetothegenerateddataqualityandthatGenie
iscostandtime-efficientandcreatesdiversedata.
Naturalness Evaluation. To assess the naturalness of our questions, we conducted a human-
evaluation experiment. In the experiment, an expert annotator5 was provided with two questions:
one human-created and the other synthetic. Both questions were based on the same content. The
annotator’staskwastoidentifythequestiontheybelievedwashuman-written. Forthisexperiment,
wesampled100questionsfromELI5,ASQA,andNQ,alongwiththeir100syntheticcounterparts.
TheresultsinTable1(andApp.§B)indicatethatforELI5,thesyntheticquestionwasselectedas
thehuman-writtenonein72%ofthecases,forNQitwas63%,andforASQAitwas49%. These
resultssuggestthatoursyntheticquestionsaremorenaturalandhuman-likethanquestionscollected
fromsourceslikeRedditandGoogleSearchengine. Additionally, theyareindistinguishablefrom
questionswrittenbyexperts,suchasthoseintheASQAdataset. Asasidefinding,wealsofindthat
theASQAdatasetisofhigherqualitythantheothers,whichexperimentsbelowreplicate.
Multi-DimensionalQualityAssessment. Inthisassessment,weaimedtoinvestigatethequalities
ofthegenerateddataandtheimpactofthefiltrationprocesses. Wefocusedonthefollowingdimen-
sions: relevanceandclarityofthequestions,andfaithfulnessandoverallqualityoftheanswers. To
accomplishthat,werandomlyselected100questionsfromtheunfilteredandfilteredWish-QA.For
eachcontent-question-answertriplet,weaskedannotatorstoansweralistofquestionsasshownin
5Anon-author,nativeEnglishspeakerwithanMAdegree.
5PublishedasaconferencepaperatICLR2024
Table1. Thefirsttwoassessmentquestionsaimtoassesstherelevanceandclarityofthequestion.
TheclarityquestionisinspiredbythefindingsofMinetal.(2020),whichrevealedthatmorethan
halfofnaturallyoccurringfactoidquestionsareambiguous. Followingthat,weincludethreeques-
tions related to the answer quality. These questions are designed to ascertain whether the answer
adequatelyaddressesthequestionwhileremainingfaithfultotheunderlyingcontent. Lastly,weask
foranoverallqualityratingona5-levelLikertscale.
HumanassessmentresultsinTable1demonstratethatthefiltrationprocesshadasignificantimpact
ontherelevanceofthequestions.Althoughourfiltrationsetupdoesnotdirectlyassessthequestions,
we find that our faithfulness filter together with the reward filter provides an indirect signal about
therelevanceofthequestion. Wealsoobservedanimprovementinthepercentageofanswersthat
werefoundtoaddressthequestion. Faithfulnessresultsshowdecentimprovement,butthereisstill
roomforenhancement. Annotators’interviewsrevealthatdespitethepresenceofunfaithfulcases
inthedataset,theirgranularitywasoftenmoresubtle. Insomeinstances,themodeladdedmissing
piecesofinformationthatweresubsequentlyfoundtobefactuallycorrect.
We observe a slight improvement in the clarity of questions, coupled with almost all answers ad-
dressing the questions. This highlights that our answer is a single relevant response from a wide
spaceofplausibleanswers,awell-documentedphenomenoninLFQA(Krishnaetal.,2021).Lastly,
weidentifyanimprovementintheoverallscore,whichleadsustotheconclusionthatthefiltering
processsubstantiallycontributestothequalityandfaithfulnessofourdataset.
Table1: Multi-DimensionalQualityassessmentforsyntheticdatageneratedfromscratch. Results
show a large improvement in question relevance and the percentage of answers that address the
question,answersthatarefaithful,andoverallanswerscores.
QualityReviewQuestion WiFSw/ofilters WiFSw/filters
Isthequestionrelevanttothecontent? 67% 92%
Isthequestionclear? (notambiguous) 63% 67%
Doestheansweraddressthequestion? 80% 98%
Istheanswerfaithfultothecontent? 53% 76%
Gradetheoverallqualityoftheanswer 3.48 4.58
Diversity. Oursyntheticdataisbuiltontopoflarge-scalecontentthatcoversmanydifferentdis-
tincttopics.Asaresult,ourdatacontaindiverselexicons.Wecomputevocd-D(McCarthy&Jarvis,
2010)tomeasurethelexicaldiversityofourdata. Wefoundthatthelexicaldiversityofallsynthetic
data is higher than their human-generated counterparts (see Table 6). We also can see that most
responselengthsaresimilartotheonesinthehumanwritingdatasets.
Scale. With300Ksamplesoverall(fullstatisticsinApp.A),ourdatasetcollectionbalancesscale
andquality. ELI5isofasimilarsizebutnoisy,andASQAiscarefullyannotatedbutmuchsmaller.
Monetary and Time Cost. Genie is more cost-efficient and time-efficient than the traditional
approach of crowd-sourced dataset curation. The cost of API’s calls of models like the ones used
typically ranges from $0.02 to $0.04, while the cost of an expert annotator to create a question is
approximately $4.45 (Stelmakh et al., 2022). According to this rate, the 300K examples in our
syntheticdatasetwouldhavecostover$1M.Thetimeittakestogenerate10examplesislessthana
minute,i.e. muchfasterthanthetimethatitwouldtakeahumantoreadthecontext.
5 EXTRINSIC EVALUATION
Finding the synthetic data to be of high quality, we test its usefulness for improving training. We
present quantitative results from our extrinsic experiments, evaluating models trained on synthetic
andhuman-generateddataontheASQAandELI5testsets.
InTable2(andApp.§B)wepresentFlan-xlresultstrainedonhumanandsyntheticdata. Wenote
thatherebysyntheticin-domainwerefertothecasewherethetrainandtestcomefromthesame
dataset,eitherELI5orASQA.
6PublishedasaconferencepaperatICLR2024
Results indicate that synthetic data is a competitive alternative even when human-generated data
alreadyexists.Inallcases,weseesubstantialgainsfromtrainingonthesyntheticdata.Forexample,
Rouge-L almost triples from 10.5 to 28.2 for Synthetic NQ. This gain is over the already strong
multitask baseline (Flan) that trained on thousands of tasks, many of which are forms of question
answering.
Moreover,thesyntheticdataprovidesbetterorcomparableresultsinallmetricsevenforcaseswhere
trainandtestdatacomefromthesamedataset. While–forASQA–Rouge-LandBert-Scoreare
slightlylowerthanthein-domaintrainingdata,thesyntheticdataisevenbetterthanthehumandata
ontherestofthescoresonELI5.Weconcludethat,ifnohuman-generateddataexists,automatically
generatingithasthepotentialtobeasgood.
ASQA performs better on both ASQA and ELI5 test sets. This observation implies that ASQA
is, on the whole, a superior dataset compared to ELI5. This aligns with the substantial annotation
efforts invested in the creation of ASQA, in contrast to the noisy and automatically scraped ELI5
data. However,itisimportanttonotethatthismeticulouscurationhasledtoaconsiderablysmaller
dataset for ASQA, totaling approximately 6k examples including the development and test sets
(compared to 272k examples in ELI5). This emphasizes the contribution of our approach which
allowslarge-scalehigh-qualitydatageneration.
Anotherstrongsupportfortheeffectivenessofourdatagenerationapproachisexemplifiedbythe
model’s outputs being favored by the preference reward model, achieving comparable or higher
resultsthanthegoldstandardofbothdatasets.
Wish-QA seems to work well even with the noisy content. Wish-QA-NQ data outperformed the
synthetic in-domain data across all metrics. This can be due to the quality of the Wish-QA-NQ
beingfavorableorthatasignaldocumentgenerationsetupisslightlypreferable.
The performance on CNN-DailyMail, presented in Table 4, shows that Wish-summarization data
improvesuponthestrongFlan-xlbaseline,inBert-ScoreandRewardscorebutnotonROUGE-L.
Overall,thedatasetseemscomparable,attestingtotheflexibilityofthemethod.
5.1 FAITHFULNESSRESULTS
Generally,wefind(Table3)thattrainingonoursyntheticdataleadstomorecontent-faithfulmodels.
Models trained on Wish-QA-NQ and Synthetic in-domain data, and Wish-QA were more faithful
thanmodelstrainedonbothASQAandELI5databythek-Precision,andANLImetrics.Thisresult
isalignedwiththefindingofKrishnaetal.(2021)thatfindthatLFQAmodelsgeneratedanswers
thatarenotgroundedintheretrieveddocuments,andassertthatthisisoneofthehurdlesforfiled
progress.
Flan-xlachievesthehighestFaithfulnessscoresfollowedbythesyntheticdatasets. Flan’sachieve-
ment can be the result of its shorter and almost extractive answers. Taking into account that it is
alsosubstantiallyunderperforming,wededucethatthesyntheticdatasetsachievethebesttrade-off
acrossperformanceandfaithfulness.
The faithfulness results for CNN-DailyMail are consistently high. As we observed, Flan-xl tends
toproducemoreextractiveresponses. SinceCNN-DailyMailprimarilycontainsextractivesumma-
rization,it’snosurprisethatitexhibitshighfaithfulnessscores. However,themodeltrainedonour
data, which doesn’t emphasize extractiveness as a hard requirement, outperforms Flan-xl in terms
ofk-Precision,matchesitintermsofNLI,andachievesthehighestaverageleveloffaithfulness.
Insummary,ourquantitativeanalysisaffirmsthattheutilizationofsyntheticdatasubstantiallyen-
hances answer quality in both ASQA and ELI5 datasets. Our approach not only matches human-
generatedresponsesbutalsoquantitativelysurpassesthemintermsofreward, highlightingitspo-
tential for generating higher-quality answers. Additionally, our method ensures high faithfulness
andgroundinginthegeneratedresponses,settingitapartfromexistingdatasets.
6 DOMAIN ADAPTATION
We have demonstrated that our method can generate synthetic data that is as good as human-
generated data. Next, we raise the hypothesis that given a task in a target domain it may be more
7PublishedasaconferencepaperatICLR2024
Table2:PerformancecomparisonofFlan-xlmodelstrainedonhuman-generatedandsyntheticdata.
The results reveal that our synthetic data consistently outperforms or achieves comparable perfor-
mancetohuman-generateddata, asindicatedbyROUGE-LandBert-Scoremetrics. Additionally,
byrewardscore,modelstrainedonoursyntheticdataexhibitsuperiororcomparableperformance
tothegoldstandardresponses.
Test-set ASQA ELI5
TrainSet ROUGE-L Bert-Score Reward ROUGE-L Bert-ScoreF1 Reward
Flan-xl 10.5 49.7 28.8 6.2 46.7 9.2
ASQA 31.4 66.0 68.6 13.5 52.2 24.4
ELI5 18.7 58.7 37.2 13.1 51.3 11.3
WiFS 28.0 67.5 85.1 13.8 55.2 26.7
Wish-QA-NQ 28.2 64.8 80.3 13.2 54.0 30.3
Wish-QAin-domain 27.0 63.4 73.3 13.1 52.8 22.7
Gold - - 72.1 - - 30.3
Table3: Fiathfullnesperformance Comparison ofFlan-xlModels TrainedonHuman-Created and
Synthetic Data. The results demonstrate that our synthetic data consistently outperforms both
human-generateddataandgoldresponses,asindicatedbythek-Precision,andANLImetrics. Flan-
xlstandsoutwiththehighestscores,whichcanbeattributedtotheextractivenatureofitsresponses.
Test-set ASQA ELI5
TrainSet k-Precision ANLI k-Precision ANLI
Flan-xl 98.2 88.7 89.2 84.9
ASQA 67.5 55.7 52.2 34.3
ELI5 52.9 33.5 29.0 5.6
WiFS 77.9 74.9 58.5 37.9
Wish-QA-NQ 79.3 75.5 60.4 43.3
Wishin-domain 81.9 79.1 68.3 52.8
Gold 46.3 25.3 20.6 2.7
effective to generate synthetic data directly in the target domain than generating it, for the same
task, from another domain. To investigate this hypothesis, we define our test set as PubMed-QA,
whichspecificallyinvolvesthetaskofLFQAinthemedicaldomain. Accordingly, wecreatesyn-
theticquestion-answeringdataonpapersfromPubMed(Wish-QA-MED)astaskdatainthetarget
domain. WethencomparetheperformanceofmodelstrainedonWish-QA-MEDdatasetwiththose
trainedonWish-QA-NQdata,aswellaswithmodelstrainedonthehuman-createdELI5andASQA
datasets.
TheresultsinTable5demonstratethatthesyntheticdatasetoutperformsELI5andiscomparableto
orslightlybetterthanASQAinROUGE-LandBert-Score. Additionally,thereisamoresubstantial
gapintermsofrewardandfaithfulness.
Interestingly, Wish-QA-NQ and Wish-QA-MED achieve similar results, echoing the finding that
Wish-QAoutperformsotherdatasets.Thissuggeststhatout-of-domaindataholdslittledisadvantage
Table 4: Performance Comparison of Flan-xl Models Trained on Human-Created and Wish-
SummarizationData. Theresultsrevealthatoursyntheticdataachievescomparableperformanceto
human-generateddata.
Test-set CNN-DailyMail
Train-set ROUGE-L Bert-Score Reward k-Precision ANLI
Flan-xl 30.2 70.9 96.3 97.6 98.7
CNN-DailyMail 33.3 72.7 96.5 97.0 99.1
Wish-Summarization 28.6 71.3 97.5 98.2 98.7
8PublishedasaconferencepaperatICLR2024
Table 5: Performance of Flan-xl Models on PubMed test data. The results reveal that our syn-
theticdataconsistentlyoutperformsorachievescomparableperformancetohuman-generateddata
in general and faithfulness metrics. Results suggest that in-domain data don’t provide additional
improvementforcontent-groundedgeneration,butmayhelpthefaithfulnessofthemodel.
PubMed
Train-set ROUGE-L Bert-Score Reward K-Precision ANLI
Flan-xl 12.8 53.8 10.7 60.6 38.2
ASQA 20.5 61.4 37.3 77.2 60.8
ELI5 15.0 56.3 16.8 32.2 2.2
Wish-QA-MED 22.1 61.6 39.4 78.2 81.8
Wish-QA-NQ 22.0 62.9 44.5 84.2 73.1
over in-domain data and can often surpass it. One explanation may be that providing the content
with the task (e.g. QA) makes the model rely less on the training domain. Supportive evidence
is the finding of Onoe et al. (2023), who found that, in their task, update strategies lag behind the
performanceofsimplyconcatenatingthecontenttotheprompt.Thismaymeanthatthemodelrelies
onthecontentmorethanwaspreviouslythought(Neemanetal.,2023).
The faithfulness scores are inconclusive, while ANLI indicates that in-domain synthetic improves
faithfulness,thek-Precisionsaysotherwise,suggestingatleastparity.
WeconcludethatGeniecanbebeneficialincreatinghuman-leveldataformanytasksanddomains,
however, it can be that LFQA is flexible in terms of its training data domain. We leave for future
researchtocheckthisfindingandtoshowtasksordimensionsthatexhibitimprovementduetotarget
domaindataandcanbenefitfromourmethod.
7 RELATED WORK
Our work is far from the first to propose synthetic data for training or experimentation (Choshen
& Abend, 2019; Agarwal et al., 2020). Recently, generating data from a large language model to
train a smaller one was suggested as a weak form of distillation to improve smaller models (West
etal.,2022). Ourmethoddoesnotfocusondistillation. Apartfromusingastrongermodelforthe
syntheticdata, thosemethodsdifferfromoursasthelearnedmodelmimicsadiversesetofskills,
ratherthanbecominganexpertonatask.
Still, there are a few synthetic methods for specific tasks. Most notably, methods that rely on a
2-stepprocess,generation,andfiltering. Westetal.(2022)presenteda2-steppipelineforSymbolic
KnowledgeDistillation,ratherthanforcreatingcontent-groundeddata. Kimetal.(2022)applythis
method to create a social dialogue dataset. In Unnatural Instructions and Self-Instruct (Honovich
etal.,2022b;Wangetal.,2022),theyappliedthismethodforthecreationofaninstructiondataset.
Theirmethodreliesonmodelknowledgeforcontent-groundedtasks. Similarly,Bittonetal.(2023)
q2d approach uses a 2-step process for creating information-seeking dialogs. Those works share
similarmechanismswithourmethodbutdifferinthecontent-groundedaspectofourwork.
Thedialoginpaintingapproach(Daietal.,2022),sharesacommonobjectivewithours,togenerate
content-groundedquestionanswering. Theyaddquestionsbetweenthedocumentsentencestocre-
ateadialogue. Thisapproachensuresthegroundednessofthedialoguebutitcomesatthecostof
lessfluentandneutralconversation. Inourapproach,wegeneratethequestionandanswerusingthe
LLMandverifyitsgroundednessandqualitytoallowbothfaithfulnessandnaturalness.
8 DISCUSSION
OurworkintroducesGenie,anefficientandcost-effectiveautomatedapproachforcuratingcontent-
groundeddatasets. Ourmethodincorporatesanovelfilteringmechanismtoensuredataquality. We
demonstrate that our synthetic wish-QA and wish-summarization data achieves parity with expert
humandatasetsinbothintrinsicandextrinsicevaluations. Furthermore, weillustratethatourdata
9PublishedasaconferencepaperatICLR2024
surpasseshuman-writtendatasetsintermsoflexicaldiversityandfaithfulness. Wehavealsoproven
theapplicabilityofourmethodtonoisycrawleddata.
We want to emphasize the immense potential this approach holds for facilitating the development
of content-focused datasets and, consequently, generative models, minimizing the need for costly
human annotation. Therefore, our method democratizes the creation of such datasets and models,
makingthemmoreaccessibletotheentirecommunity.
REFERENCES
VaibhavAdlakha,ParishadBehnamGhader,XingHanLu,NicholasMeade,andSivaReddy. Eval-
uatingcorrectnessandfaithfulnessofinstruction-followingmodelsforquestionanswering. arXiv
preprintarXiv:2307.16877,2023.
Oshin Agarwal, Heming Ge, Siamak Shakeri, and Rami Al-Rfou. Knowledge graph based syn-
thetic corpus generation for knowledge-enhanced language model pre-training. arXiv preprint
arXiv:2010.12688,2020.
Yonatan Bitton, Shlomi Cohen-Ganor, Ido Hakimi, Yoad Lewenberg, Roee Aharoni, and Enav
Weinreb. q2d: Turning questions into dialogs to teach models how to search. arXiv preprint
arXiv:2304.14318,2023.
SamuelRBowman, GaborAngeli, ChristopherPotts, andChristopherDManning. Alargeanno-
tatedcorpusforlearningnaturallanguageinference. arXivpreprintarXiv:1508.05326,2015.
Leshem Choshen and Omri Abend. Automatically extracting challenge sets for non-local phe-
nomenainneuralmachinetranslation. ConferenceonComputationalNaturalLanguageLearn-
ing (CoNLL), pp. 291–303, November 2019. doi: 10.18653/v1/K19-1028. URL https:
//aclanthology.org/K19-1028.
Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment
challenge. InMachinelearningchallengesworkshop,pp.177–190.Springer,2005.
Zhuyun Dai, Arun Tejasvi Chaganty, Vincent Y Zhao, Aida Amini, Qazi Mamunur Rashid, Mike
Green, and Kelvin Guu. Dialog inpainting: Turning documents into dialogs. In International
ConferenceonMachineLearning,pp.4558–4586.PMLR,2022.
Nouha Dziri, Hannah Rashkin, Tal Linzen, and David Reitter. Evaluating attribution in dialogue
systems: Thebeginbenchmark. TransactionsoftheAssociationforComputationalLinguistics,
10:1066–1083,2022.
Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. Eli5:
Longformquestionanswering. arXivpreprintarXiv:1907.09190,2019.
PengchengHe,JianfengGao,andWeizhuChen. Debertav3: Improvingdebertausingelectra-style
pre-training with gradient-disentangled embedding sharing. arXiv preprint arXiv:2111.09543,
2021.
Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa
Suleyman,andPhilBlunsom. Teachingmachinestoreadandcomprehend. Advancesinneural
informationprocessingsystems,28,2015.
Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, and Omri Abend.
q2: Evaluatingfactualconsistencyinknowledge-groundeddialoguesviaquestiongenerationand
question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural
LanguageProcessing,pp.7856–7870,OnlineandPuntaCana,DominicanRepublic,November
2021.AssociationforComputationalLinguistics. doi: 10.18653/v1/2021.emnlp-main.619. URL
https://aclanthology.org/2021.emnlp-main.619.
OrHonovich, RoeeAharoni, JonathanHerzig, HagaiTaitelbaum, DoronKukliansy, VeredCohen,
Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. True: Re-evaluating
factualconsistencyevaluation. arXivpreprintarXiv:2204.04991,2022a.
10PublishedasaconferencepaperatICLR2024
Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning
languagemodelswith(almost)nohumanlabor. arXivpreprintarXiv:2212.09689,2022b.
Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Lavinia Dunagan, Jacob Morrison, Alexander
Fabbri, Yejin Choi, and Noah A. Smith. Bidimensional leaderboards: Generate and evaluate
languagehandinhand. InProceedingsofthe2022ConferenceoftheNorthAmericanChapterof
theAssociationforComputationalLinguistics: HumanLanguageTechnologies,pp.3540–3557,
Seattle,UnitedStates,July2022.AssociationforComputationalLinguistics. doi: 10.18653/v1/
2022.naacl-main.259. URLhttps://aclanthology.org/2022.naacl-main.259.
Hyunwoo Kim, Jack Hessel, Liwei Jiang, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Le Bras,
MaliheAlikhani,GunheeKim,MaartenSap,etal. Soda: Million-scaledialoguedistillationwith
socialcommonsensecontextualization. arXivpreprintarXiv:2212.10465,2022.
AndreasKo¨pf,YannicKilcher,DimitrivonRu¨tte,SotirisAnagnostidis,Zhi-RuiTam,KeithStevens,
Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richa´rd Nagyfi, et al. Openassistant
conversations–democratizinglargelanguagemodelalignment. arXivpreprintarXiv:2304.07327,
2023.
KalpeshKrishna,AurkoRoy,andMohitIyyer. Hurdlestoprogressinlong-formquestionanswer-
ing. In Proceedings of the 2021 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, pp. 4940–4957, Online, June
2021.AssociationforComputationalLinguistics. doi: 10.18653/v1/2021.naacl-main.393. URL
https://aclanthology.org/2021.naacl-main.393.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris
Alberti,DanielleEpstein,IlliaPolosukhin,JacobDevlin,KentonLee,etal. Naturalquestions: a
benchmarkforquestionansweringresearch. TransactionsoftheAssociationforComputational
Linguistics,7:453–466,2019.
Chin-YewLin. Rouge: Apackageforautomaticevaluationofsummaries. InTextsummarization
branchesout,pp.74–81,2004.
JoshuaMaynez,ShashiNarayan,BerndBohnet,andRyanMcDonald.Onfaithfulnessandfactuality
inabstractivesummarization. InProceedingsofthe58thAnnualMeetingoftheAssociationfor
Computational Linguistics, pp. 1906–1919, Online, July 2020. Association for Computational
Linguistics. doi: 10.18653/v1/2020.acl-main.173. URL https://aclanthology.org/
2020.acl-main.173.
Philip M McCarthy and Scott Jarvis. Mtld, vocd-d, and hd-d: A validation study of sophisticated
approachestolexicaldiversityassessment. Behaviorresearchmethods,42(2):381–392,2010.
Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. Ambigqa: Answering
ambiguousopen-domainquestions. arXivpreprintarXiv:2004.10645,2020.
EllaNeeman,RoeeAharoni,OrHonovich,LeshemChoshen,IdanSzpektor,andOmriAbend. Dis-
entQA:Disentanglingparametricandcontextualknowledgewithcounterfactualquestionanswer-
ing. Annual Meeting of the Association for Computational Linguistics, pp. 10056–10070, July
2023. doi: 10.18653/v1/2023.acl-long.559. URL https://aclanthology.org/2023.
acl-long.559.
YasumasaOnoe, MichaelJQZhang, ShankarPadmanabhan, GregDurrett, andEunsolChoi. Can
lms learn new entities from descriptions? challenges in propagating injected knowledge. arXiv
preprintarXiv:2305.01651,2023.
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,Chong
Zhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Traininglanguagemodelstofollow
instructions with human feedback. Advances in Neural Information Processing Systems, 35:
27730–27744,2022.
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli,
Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb
dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv
preprintarXiv:2306.01116,2023.
11PublishedasaconferencepaperatICLR2024
Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal Shnarch, Noam Slonim,
Michal Shmueli-Scheuer, and Leshem Choshen. Efficient benchmarking (of language models).
arXivpreprintarXiv:2308.11696,2023.
FabioPetroni,AleksandraPiktus,AngelaFan,PatrickLewis,MajidYazdani,NicolaDeCao,James
Thorne,YacineJernite,VladimirKarpukhin,JeanMaillard,etal. Kilt: abenchmarkforknowl-
edgeintensivelanguagetasks. arXivpreprintarXiv:2009.02252,2020.
Matt Post. A call for clarity in reporting BLEU scores. In Proceedings of the Third Con-
ference on Machine Translation: Research Papers, pp. 186–191, Brussels, Belgium, Octo-
ber 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6319. URL
https://aclanthology.org/W18-6319.
Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. Asqa: Factoid questions meet
long-formanswers. arXivpreprintarXiv:2204.06092,2022.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Niko-
layBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfounda-
tionandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
YizhongWang,YeganehKordi,SwaroopMishra,AlisaLiu,NoahASmith,DanielKhashabi,and
Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions.
arXivpreprintarXiv:2212.10560,2022.
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
AndrewMDai,andQuocVLe.Finetunedlanguagemodelsarezero-shotlearners.arXivpreprint
arXiv:2109.01652,2021.
Peter West, Chandra Bhagavatula, Jack Hessel, Jena Hwang, Liwei Jiang, Ronan Le Bras, Xim-
ingLu,SeanWelleck,andYejinChoi. Symbolicknowledgedistillation: fromgenerallanguage
models to commonsense models. In Proceedings of the 2022 Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics: Human Language Technolo-
gies, pp. 4602–4625, Seattle, United States, July 2022. Association for Computational Linguis-
tics. doi: 10.18653/v1/2022.naacl-main.341. URLhttps://aclanthology.org/2022.
naacl-main.341.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore:
Evaluating text generation with bert. ArXiv, abs/1904.09675, 2019a. URL https://api.
semanticscholar.org/CorpusID:127986044.
TianyiZhang,VarshaKishore,FelixWu,KilianQWeinberger,andYoavArtzi. Bertscore: Evaluat-
ingtextgenerationwithbert. arXivpreprintarXiv:1904.09675,2019b.
LianminZheng,Wei-LinChiang,YingSheng,TianleLi,SiyuanZhuang,ZhanghaoWu,Yonghao
Zhuang, Zhuohan Li, Zi Lin, Eric Xing, et al. Lmsys-chat-1m: A large-scale real-world llm
conversationdataset. arXivpreprintarXiv:2309.11998,2023.
ChenguangZhu,ZiyiYang,RobertGmyr,MichaelZeng,andXuedongHuang. Makeleadbiasin
yourfavor: Zero-shotabstractivenewssummarization. InInternationalConferenceonLearning
Representations,2020.
A DATASETS STATISTICS
While the amount of synthetic data is capped merely by the actual amount produced and the con-
tent available, meaning the others can generate more data, we supply the actual sizes we release.
StatisticsareshowninTable6
12PublishedasaconferencepaperatICLR2024
Table6: Datastatisticforseveraldatasetsincludingthesizeofthedata,thenumberofwordsinthe
response,andtheaveragelexicaldiversity. Overall,ourdatasetspans300K+samples,similartothe
sizeofELI5,andCNN-DailyMailwhichwerecollectedfromavailableresourcesandarenoisyby
nature. Ontheotherhand,ourdatais50timeslargerthanthecarefullyannotatedASQAdata. We
alsocanseethatmostresponselengthsaresimilartotheonesinthehumanwritingdatasets,andthe
lexicaldiversityofallsyntheticdataishigherthantheirhumanwritingcounterparts.
#samples #wordsinresponse lexicaldiversity
ELI5 272K 107.0 70.9
ASQA 6,316 73.3 65.8
CNN-DailyMail 311K 44.8 40.1
Wish 27K 65.1 54.3
Wish-QA-NQ 88K 73.4 62.3
Wish-QA-ELI5 78K 98.3 71.8
Wish-QA-ASQA 6140 87.5 67.0
Wish-Summarization 37K 66.6 76.0
Wish-QA-Med 69K 84.2 71.8
Table7: HumanassessmentforWiFSwithoutandwithfiltering, andSyntheticNQ.Resultsshow
a big improvement in question relevance and the percentage of answers that address the question,
answersthatarefaithful,andoverallanswerscores.
QualityReviewQuestion WiFSw/ofilters WiFSw/filters SyntheticNQ
Isthequestionrelevanttothecontent? 67% 92% 98%
Isthequestionclear? (notambiguous) 63% 67% 91%
Doestheansweraddressthequestion? 80% 98% 98%
Istheanswerfaithfultothecontent? 53% 76% 88%
Gradtheoverallqualityoftheanswer 3.48 4.58 4.74
B COMPLEMENTARY RESULTS.
We provide additional human experiments (Table 7) demonstrating that using clean content from
excitingdatasetscanfurtherimprovethesyntheticdataquality.
WealsoprovidetheresultsofsyntheticdatasetscreatedwiththestrongerLammamodelinTable8
andonPubMedonTable9.
We also include here results with Llama-2-13B-Chat as the base model. General results are pre-
sentedinTable11andFaithfulnessresultsareinTable10.
Table 8: Results on ASQA and ELI5 including models trained with Llama-2-70B-chat synthetic
data.
Test-set ASQA ELI5
TrainSet ROUGE-L Bert-ScoreF1 Reward ROUGE-L Bert-ScoreF1 Reward
Flan-xl 10.5 49.7 28.8 6.2 46.7 9.2
Humanin-domain 31.4 66.0 68.6 13.1 51.3 11.3
Humanoutofdomain 18.7 58.7 37.2 13.5 52.2 24.4
Wish-QA 28.0 67.5 85.1 13.8 55.2 26.7
Wish-QA-NQFalcon 28.2 64.8 80.3 13.2 54.0 30.3
Wish-QA-NQLlama 28.1 64.8 81.7 13.6 53.9 44.1
Wish-QAin-domainFalcon 27.0 63.4 73.3 13.1 52.8 22.7
Wish-QAin-domainLlama 28.0 64.6 76.4 13.7 53.6 30.5
Gold - - 72.1 - - 30.3
13PublishedasaconferencepaperatICLR2024
Table 9: Full results on PubMed dataset. Here we include results where Llama-2-70B is the data
generator.
PubMed
Train-set ROUGE-L Bert-Score Reward k-Precision ANLI
Flan-xl 12.8 53.8 10.7 60.6 38.2
ASQA 20.5 61.4 37.3 77.2 60.8
ELI5 15.0 56.3 16.8 32.2 2.2
Wish-QA-MedFalcon 22.1 61.6 39.4 78.2 81.8
Wish-QA-NQFalcon 22.0 62.9 44.5 84.2 73.1
Wish-QA-MedLlama 21.4 62.1 30.0 74.6 92.4
Wish-QA-NQLlama 21.4 62.5 51.2 80.6 83.0
Table10: FaithfulnessresultsonASQAandELI5whenLlama-2-13b-Chatisthebasemodel.
Test-set ASQA ELI5
Train-set k-Precision ANLI k-Precision ANLI
Llama-13b-chat 40.4 65.5 23.3 24.6
ASQA 77.3 73.1 48.0 37.3
ELI5 59.0 42.6 33.3 13.8
Wish-QA-NQ 81.1 76.2 58.2 45.1
Wish-QA-ASQA 79.2 73.1 51.8 40.3
Wish-QA-ELI5 77.6 75.9 52.0 46.1
Table11: ResultsonASQAandELI5whenLlama-2-13b-Chatisthebasemodel.
Test-set ASQA ELI5
Train-set RougeL Bert-Score Reward RougeL Bert-Score Reward
Llama-13b-chat 21.4 57.6 62.7 11.8 51.2 46
ASQA 29.0 65.2 91.1 12.4 53.5 66.9
ELI5 13.4 55.0 86.4 11.4 52.8 66.7
SyntheticNQ 26.3 64.4 86.9 12.5 53.5 52.6
SyntheticASQA 26.3 63.6 87.1 12.3 53.1 57.7
SyntheticELI5 26.3 63.6 89.2 12.4 53.0 57.1
14PublishedasaconferencepaperatICLR2024
Table12: YourTableCaption
Model ROUGE-L Bert-Score Reward K-Precision ANLI
Flan-xl 42.3 67.3 35.9 39.1 89.1
Databricks 54.6 77.2 70.1 56.7 89.9
Wish-IE 47.9 74.5 87.4 60.0 86.8
C INFORMATION EXTRACTION RESULTS.
Totesttheefficacyofthemethodinanothertask,weconductedanexperimentfocusingoninforma-
tionextraction(IE).Forthisexperiment,weutilizedtheinformationextractionpartoftheDatabricks
dataset,whichwedividedintotrainandtestsets(1000/50). Syntheticinformationextractiondata
wasgeneratedoverGoogle-NQpassages,requiringonlyanewsetoffew-shotexamplestodemon-
stratethetask,withoutanyothermodifications. Thetablebelow12showsthatourmodelimproved
in all metrics compared to the Flan-xl baseline, except for ANLI. Furthermore, it achieves higher
scores than the model trained on human data in Reward and K-Precision, while slightly lagging
behindinROUGE-L,Bert-Score,andANLI.Theaverageofthefaithfulnessmetricsindicatescom-
parableperformancesinthisdimensionforthemodelstrainedonhumanandsyntheticdata. These
experiments reinforce our belief that our method is general and can be readily applied to various
tasks,yieldingimprovedresultsthroughthegenerationofhigh-qualitysyntheticdata.
D PROMPT
WeprovideexamplesofthepromptsusedtogeneratedatainFig.2.
15PublishedasaconferencepaperatICLR2024
Figure2: Anillustrationofourdatagenerationprompt. Inblackisthefew-shotpromptwegivethe
model. InpinkanewQAthatthemodelgeneratedbasedonthetheprovidedcontent.
Instruction: Given the next [document], create a [question] and [answer] pair that are
grounded in the main point of the document, don't add any additional information that is
not in the document. The [question] is by an information-seeking user and the [answer] is
provided by a helping AI Agent.
[document]: Scrumptious Sweet Co. factory ..
### Response:
[question]: Where was the movie Chitty Chitty Bang Bang filmed?
[answer]: Chitty Chitty Bang Bang was filmed in a variety of locations in England and
Germany. Some ..
[document]: The series stars Eugene Levy as Johnny Rose …
### Response:
[question]: What is the plot of the show Schitt'sCreek?
[answer]: The show Schitt'sCreek is about a wealthy family who loses their fortune and is
forced to rebuild their lives in a small town. The show follows the family as they adjust to
their new life in the town and learn to appreciate the simple things in life.
[document]: 2016's countdown broke several Hottest 100 records…
### Response:
[question]: What was the most popular song on the 2016 Hottest 100?
[answer]: The most popular song on the 2016 Hottest 100 was "Never Be Like You" by
Flume. This was the first time that an electronic dance music producer topped the
countdown.
[document]: In Greek mythology, Persephone ..
### Response:
[question]: Who is Persephone in Greek mythology and what is her significance?
[answer]: In Greek mythology, Persephone, also known as Kore, is the daughter of Zeus and
Demeter. She ..
[document]: Terry, the dog that played Toto in the 1939 screen …
### Response:
[question]: What breed of dog was Toto in the Wizard of Oz?
[answer]: Terry, the dog that played Toto in the 1939 screen adaptation of The Wizard of Oz,
was a brindle Cairn terrier.
16