1
Topologies of Reasoning: Demystifying
Chains, Trees, and Graphs of Thoughts
Maciej Besta1†, Florim Memedi1, Zhenyu Zhang1, Robert Gerstenberger1, Nils Blach1,
Piotr Nyczyk2, Marcin Copik1, Grzegorz Kwas´niewski1, Ju¨rgen Mu¨ller3, Lukas Gianinazzi1,
Ales Kubicek1, Hubert Niewiadomski2, Onur Mutlu1, Torsten Hoefler1
1ETHZurich 2Cledar 3BASFSE
Abstract—Thefieldofnaturallanguageprocessing(NLP)haswitnessedsignificantprogressinrecentyears,withanotablefocuson
improvinglargelanguagemodels’(LLM)performancethroughinnovativepromptingtechniques.Amongthese,promptengineering
coupledwithstructureshasemergedasapromisingparadigm,withdesignssuchasChain-of-Thought,TreeofThoughts,orGraphof
Thoughts,inwhichtheoverallLLMreasoningisguidedbyastructuresuchasagraph.Asillustratedwithnumerousexamples,this
paradigmsignificantlyenhancestheLLM’scapabilitytosolvenumeroustasks,rangingfromlogicalormathematicalreasoningto
planningorcreativewriting.Tofacilitatetheunderstandingofthisgrowingfieldandpavethewayforfuturedevelopments,wedevisea
generalblueprintforeffectiveandefficientLLMreasoningschemes.Forthis,weconductanin-depthanalysisofthepromptexecution
pipeline,clarifyingandclearlydefiningdifferentconcepts.Wethenbuildthefirsttaxonomyofstructure-enhancedLLMreasoning
schemes.Wefocusonidentifyingfundamentalclassesofharnessedstructures,andweanalyzetherepresentationsofthese
structures,algorithmsexecutedwiththesestructures,andmanyothers.Werefertothesestructuresasreasoningtopologies,because
theirrepresentationbecomestoadegreespatial,astheyarecontainedwithintheLLMcontext.Ourstudycomparesexistingprompting
schemesusingtheproposedtaxonomy,discussinghowcertaindesignchoicesleadtodifferentpatternsinperformanceandcost.We
alsooutlinetheoreticalunderpinnings,relationshipsbetweenpromptingandotherspartsoftheLLMecosystemsuchasknowledge
bases,andtheassociatedresearchchallenges.Ourworkwillhelptoadvancefuturepromptengineeringtechniques.
IndexTerms—PromptEngineering,Prompting,PromptingTopology,Chain-of-Thought,TreeofThoughts,GraphofThoughts,Large
LanguageModels,LLMs,Vision-LanguageModels,VLMs,GenerativeAI,ChainPrompting,TreePrompting,GraphPrompting.
✦
1 INTRODUCTION Yet, despite all these advancements, state-of-the-art
schemes still exhibit numerous limitations. First, they are
Large Language Models (LLMs) have become a dominant
still limited to simple tasks such as Game of 24 – it is
tool in modern machine learning (ML). Originating from
criticaltofurtherenhancepromptingtoenablesolvingcom-
simple Natural Language Processing (NLP) tasks [34], [83],
plex multifaceted tasks. Moreover, state-of-the-art prompt-
[157],theirfar-extendingpotentialhasbeenquicklyapplied
ing schemes often entail high inference costs [10], [213].
inotherfields,suchaslogicalreasoning[51],planning[193],
Third, designing, developing, maintaining, and extending
medicine[182],andmanyothers.Sincetheprimarycommu-
theseschemesishard.Ononehand,thisisduetotherapid
nication medium with LLMs is natural language, prompt
development and enrichment of the “LLM ecosystem” that
engineering has become a new area of study that gained
mustbeseamlesslyintegratedintothepromptingpipeline.
widespread attention and importance [156], [201]. First, it
This includes retrieval-augmented generation (RAG), ac-
democratizestheaccesstoLLMsandtotheoverallgenera-
cessing the Internet, executing Python scripts, fine-tuning,
tiveAIlandscape,bybeingeasytouseandtrybyanybody.
and others. On other hand, different concepts related to
Second, it is cost-effective and does not require fine-tuning
theLLMreasoningarenotwell-defined,hinderingeffective
orpre-training,whichareexpensiveandtime-consuming.
designofnewmorepowerfulschemes.Forexample,while
Crafting LLM queries to increase both the accuracy of
many schemes rely on the notion of the LLM thought, it is
outcomes as well as cost-effectiveness in tasks such as
notclearhowitrelatestoconceptssuchasaprompt.
logical or algebraic queries is challenging. Despite contin-
To address the above issues, we first identify and crys-
uous advances in the size and cognitive power of LLMs,
tallize fundamental building blocks and concepts in the
solving such elaborate tasks with a single straightforward
general prompt execution pipeline. Then, we analyze and
prompt yields imprecise or plain incorrect results due to
clarify these blocks and concepts in the context of recent
the left-to-right, one-token-at-a-time nature of generative
schemes such as CoT, ToT, and GoT (contribution #1). Our
Transformer models [195]. Therefore, recent works focus
studyisbasedonabroadanalysisofrecentworksonLLM
on guiding LLMs towards the final solution through in-
reasoning. Then, we use the gained insights to develop a
termediate steps. Example such schemes include Chain-of-
general blueprint and a taxonomy of the LLM reasoning
Thought(CoT)[195],TreeofThoughts(ToT)[213],Graphof
schemes, focusing on how the underlying structure of rea-
Thoughts (GoT) [10], AutoGPT [160], ReAct [214], or LLM-
soningcanbeusedtofacilitatemoreefficient,effective,and
Compiler[110].Thislineofworksincreasestheperformance
productive prompting (contribution #2). For this, we ob-
oftheLLMreasoning.
serve that the reasoning process in many recent prompting
†Correspondingauthor schemes can be modeled as a graph. While the nature of
4202
naJ
52
]LC.sc[
1v59241.1042:viXra2
[Yao et al., May'23] [Besta et al., August'23]
https://github.com/princeton-nlp/tree-of-thought-llm https://github.com/spcl/graph-of-thoughts
[Wei et al., [Wang et al.,
Jan'22] March'22] [Long, May'23] [Lei et al., August'23]
https://github.com/jieyilong/tree-of-thought-puzzle-solver
Basic Input- Chain-of- Multiple CoTs (CoT-SC) Tree of Thoughts (ToT) Graph of Thoughts (GoT)
Output (IO) -Thought
(CoT)
Input Input Backtracking Refining Input
Input Input Branching out from a chain
from a chain
Output Backtracking
Legend
Thoughts:
Unscored
Positive
score Aggregating Aggregating
Negative chains thoughts
score Output Abandon a chain Output Key novelty Output
b AeD btw ae np e de e on n nd t e h thn o oc ui uge ghs hts t K I Ln Le tey Mr mn to hev d oe i ual gt ty e h: ts K ( H inb ae de ry ey n pon e en so nsdv di ne eCl g nt o y tm T cu h): l at ii np sle tha eS c he bl a ee i sc n tt i swn cg i oth re ( G n o t ih tb e n e o fe w un a uy re go t g tr h hhn a io v ettd ,i ru e n ,engC g ax h ao n ps t rT ds le b o - v b pi rS te ia orC nr s a sa ge r s) l yd: ibly I t an h lt o se u or gm sh ce t od s r i a ea r dt ee K A t t lr h oare o onby u psi g it fn nr o hao gr trv s my o e i vagl nt etr ty i o ra o p ( an ab h s tne - h(by e oaa wo g usn g g e od r hd ne t T g t e th ,ao o o tT iu n) g g: h t Output
within a chain of thoughts backtracking from it refine it)
Backtrack
Fig.1:Evolutionofreasoningtopologiesusedinpromptingschemes.
interacting with the LLM is temporal, the representation of LLM reasoning, as it can generate different thoughts from
the graph structure behind the LLM reasoning is periodi- thesameprompt.
callymergedwiththeLLMcontext,becoming–toadegree Tree of Thoughts (ToT) [133], [213] elevates the CoT
– spatial, thus forming different topologies. These topologies limitations by allowing prompt branching at any point of
can be a plain path graph (as in CoT [195]), multiple par- thechainofthoughts.Therefore,differentexplorationpaths
allel path graphs with a single root (as in CoT with Self- are not fundamentally independent, like in CoT-SC, but a
Consistency) [190], a tree (as in ToT [213]), or an arbitrary chain of thoughts can branch during the reasoning process
graph(asinGoT[10]).Wethenuseourtaxonomytosurvey toexploredifferentoptions.Asingletreenoderepresentsa
andanalyzeexistingpromptingschemes(contribution #3). partialsolution.Basedonagivennode,thethoughtgenerator
We dissect these schemes into fundamental aspects such constructs a given number k of new nodes. Then, the state
as the class of graphs (i.e., the topology) used to model evaluatorgeneratesscoresforeachsuchnewnode.Depend-
the reasoning process, the representation of this reasoning, ingontheusecase,theevaluationcouldbeconductedusing
or the encoding of the reasoning schedule. We focus on an LLM itself, or it can harness human scores. Finally, the
investigating which classes of schemes offer more perfor- schedule of extending the tree is dictated by the utilized
mance in terms of the accuracy of predictions, the latency searchalgorithm(e.g.,BFSorDFS).
ofexecution,orthecosteffectiveness(contribution #4).We Finally,GraphofThoughts(GoT)[10]enablesarbitrary
finallylistopenchallengesandpotentialfornovelresearch reasoning dependencies between generated thoughts. Sim-
directions(contribution#5). ilarly to ToT, every thought can generate multiple child
thoughts. However, each thought can also have multiple
parents, which can form an aggregation operation. GoT,
2 EVOLUTION OF REASONING TOPOLOGIES
allowing both branching (thoughts with out-degree > 1)
We first summarize the evolution of reasoning topologies; and aggregation (thoughts with in-degree > 1) operations,
seeFigure1foranoverview.Forthesakeofbrevity,wedo can express – for example – reasoning patterns resembling
not yet precisely define the used terminology, instead rely- dynamic programming, where GoT subgraphs are respon-
ingontermsbroadlyusedintheliterature.InSections3–4, sible for solving subproblems, which are then combined to
weintroduceanddiscussprecisenaming. formafinalsolution.
In the basic Input-Output (IO) prompting, the LLM
provides a final reply immediately upon receiving the 3 ESSENCE OF GENERAL PROMPT EXECUTION
user initial prompt. There are no intermediate steps in the
We first summarize general prompt execution by giving a
LLM reasoning. Chain topologies, introduced in Chain-of-
detailed overview of the prompting pipeline (Section 3.1)
ThoughtbyWeietal.[195],improveuponIOpromptingby
and then establishing a functional formulation for any
incorporating explicit intermediate “steps of reasoning” in
prompting scheme (Section 3.2). This formulation will fa-
addition to the input and output. Chain-of-Thought with
cilitateoursubsequentanalysisofreasoningtopologies.
Self-Consistency(CoT-SC)[190]improvesuponCoTbyin-
troducingseveralindependentreasoningchains,originating
3.1 BasicPromptingPipeline
fromthesameinitialinput.Then,thebestoutcomefromthe
finalthoughtsischosen,accordingtoapredefinedfunction Figure 2 summarizes the prompting pipeline. On the left
S.Thedrivingideaistoharnesstherandomnesswithinthe sideofthefigure,itshowsahigh-leveluser–AIinteraction,3
User-AI interac�on High-level overview of a promp�ng interac�on
... ... Prompt Reply
Prompt Reply Prompt Reply Prompt Reply
User
L inL fM rastructure Preprocess Add to context Reply to
Pre Pp rr oo mce ps tsed
cA od nd
te
t xo
t Context
toc o tIn hnp etu e Lxt Lt
M
Au gt eo nre eg rar �es os nive
OL uL tM
put
prP oo cs et
ss Po Ls Lt M-p r Oo
ut ch tee
p
su uss ee tr
d
Details of the i-th promp�ng interac�on
User thU e s oe nr l yp r uo sm erp it n i ps ut Reply
Prompt i
Provider 1 Preprocess Adding the "system Context 8 Reply to
pre-prompt", using tools, the user
Internet Databases Tools qc ah uve ao lc iik td yin i n eg g nf heo axr p nc l co o em i mtsp , e l Ri na tAn sGc , e ., ., . </> Pr Pep roro mc pe ts s 1ed ... </> Pre Pp roro mce ps t s ied Post-processed
LLM Output
A dd ad te ad mA ed tad de ad ta 2 Add to context </> Reply 1 </> Reply i 6 5 Post-process
Preprocessed Add to context
</> Prompt i 7 (Poten�ally) iterate before Checking
ge�ng back to the user for NSFW, ...
LLM
3 Input context Autoregressive
to the LLM genera�on LLM
4 Output
Poten�al Using an inflexible pipeline,
fine-tuning genera�ng auto-regressive output
un�l the model decides to stop
Fig.2:Overviewofageneralpromptingpipeline.
which consists of a series of exchanges of information be- f (for the post-processing of the LLM output 5), LLM
post
tween the user and the LLM infrastructure ; the user (for the auto-generative LLM execution 4), f c (for deter-
sendsprompts whiletheLLMsendsbackreplies . mining how the context is updated in stage 2 and f c′ (for
Anindividuali-thpromptinginteraction,picturedinthe determining how the context is updated in stage 6). For
main central part of Figure 2, starts with the user sending this, we observe that the i-th prompting interaction (for
a prompt p i. Before being fed to the model, the prompt i=1,...andc′ 0 ={})canbeformallydescribedas
can be preprocessed by the LLM provider 1, becoming
p s(cid:101) oi. mT eh ais ddc io tu iol nd alin mcl eu td ade aa tadd <i />n ,g cha ec“ ksy ins gtem forp thre e-p cr oo mm pp lit a” no cer p (cid:101)i =f pre(p i) (1)
withsomepolicies,enhancingthepromptquality,conduct- c i =f c(p (cid:101)i,c′ i−1) (2)
ing retrieval augmentation , or including outcomes from o i =LLMX(c i) (3)
running external tools such as Python scripts or accessing o (cid:101)i =f post(o i) (4)
the Internet . The preprocessed prompt is added to the
LLM context 2, which then is fed to the model 3 .
c′
i
=f c′(c i,o (cid:101)i) (5)
This results in the autoregressive output generation 4 . where
The model is usually frozen such that its weights do
• p i istheuserpromptinthei-thpromptinginteraction,
not change. In some considered prompting schemes, the
m poo sd t-e pl rc oa cn esa sl es dob 5e ,fi wn he- it cu hn ce od uld.T inh ve oo lvu etp ruu nt no i ini gsp
o
iot te hn roti ual gl hy • f p (Rp i.r Ae I( Gtp i )m) ,a ei ys xea i cn uvp tior ne lv gp er ao Rc se e cs t rs r ii i pn e tvg ,a alt cr ca A en sus sgf io nmr gm en ta htt e eio d In nG ta eep rn np e eli r te ,ad t ai not dno
additional neural layers (e.g., for sentiment analysis) or
usingothertools,
other forms of post-processing on the provider side, such
• p (cid:101)i isthepreprocessedversionofthei-thprompt,
o ta hs p eech r ca oe tc nik o tenin xsg t. Tf 6o hr e aN npS doF sW it t-p, isa rod sc ed e ni sn tsg e bd am co ko ur te t opm u the t eta o (cid:101) ud i sa i esta ra a< ls/ s> o, aa a rn d ed d pleo ydth 8e tor •
•
c i cni
′
it ii e ss r tat hh c ete io cc ono nn ( tat ee f xx te tt r aa fet tx et e rh ce eu xb t eie n cg ugi tn f inn c g)i ,n fg c′o of ft th he ei i- -t th hpp rr oo mm pp tt ii nn gg
. Note that o (cid:101)i could also be potentially fed back to the interaction (note that in the actual implementation, c i
modeldirectly,foradditionaliterations,beforegettingback and c′ would be referring to the same data structure);
i
totheuser 7. notethatc′ ={},
0
• o i is the output of the auto-generative execution of a
givenLLMX (e.g.,LLaMA),
3.2 FunctionalFormulation&BuildingBlocks
• f post(o i) is a post-processing transformation applied
WeformalizethebasicpromptingpipelinefromSection3.1. to o i; this may involve additional neural layers (e.g.,
Thisallowsustocrystallizeitsfundamentalbuildingblocks, for sentiment analysis), checking for compliance with
facilitating future optimizations and propelling both ef- guidelines,andothers.Whilemostexistingschemesdo
ficient and effective designs. The fundamental functional notfocusonthispart,weexpectthatinthefuture,the
building blocks are f (for the prompt preprocessing 1), post-processing transformation could also – similarly
pre4
topreprocessing–involveexecutingascript,accessing an edge from x to y indicating that y depends on x. If the
theInternet,RAG,andothers, task is to sort a sequence of numbers, and the strategy is
• o (cid:101)i isthepost-processedLLMoutputo i;o (cid:101)i =f post(o i), basedonsplittingthesequenceintosub-sequences,sorting
• f c,f c′aretransformationsthatdeterminetheexactform themindependently,andmerging,thentheinitialsequence
ofupdatingthecontext. couldbemodeledasanodex,andthesub-sequenceswould
formfurthernodesy,z,...,withedges(x,y),(x,z),...from
xtoallthenodesmodelingsub-sequences.Now,areasoning
3.3 ImplementingBuildingBlocks
topologyisagraphofthesenodesandedges.
The provided building blocks can serve as the basis for
Formally, a topology can be defined as G = (V,E),
productiveimplementationsofpromptingbaselinesondif-
where V is a set of nodes modeling thoughts, and E is
ferentarchitectures.Forexample,schedulingdifferentparts
a set of edges between these nodes, modeling reasoning
of the prompting pipeline in the cloud setting could be
dependenciesbetweenthoughts.Suchagraph-theoreticap-
done using the granularity of these blocks: a lightweight
proachtoreasonaboutchains,trees,andgraphsofthoughts
post-processing f could execute a fast function, while
post facilitates devising more efficient reasoning schemes. For
a longer and stateful RAG operation within f could be
pre example,whenaimingforminimizingthelatencyofsolving
automaticallyplacedonEC2.
a given task, one would attempt to devise a topology with
The details of f pre, LLMX, f post, and f c depend on the lowdistancebetweentheinputandtheoutputnode.
specificLLMinfrastructure.Ingeneral,theycanbeusedto
implement different parts of the generative AI ecosystem.
Forexample,mostofRAG-basedframeworkswouldimple- 4.2 SemanticRolesofThoughts&Topologies
ment RAG in f . Similarly, adding a system pre-prompt
pre Graph nodes can model different aspects of reasoning. For
canbeimplementedasapartoff .Thedetailsofhowthe
pre example, in writing tasks, some nodes model plans of
context is updated, or how some of its parts are removed
writing a paragraph, while other nodes model the actual
whentheinputlengthreachesitslimit,arespecifiedinf c.
paragraphs of text. We refer to such aspects as different
Inmanycases,itistheuserresponsibilitytospecifythe
semanticroles.Asalreadyobservedinthepromptingliter-
behavioroff pre,f post,f c;thisisthecase–forexample–with
ature [10], semantic roles can also be modeled with graph
LLaMA or when using the OpenAI API. Contrarily, when
theory, namely with heterogeneous graphs. This enables
interactingwithcommercialservicessuchasChatGPT,these
harnessingapowerfulmachineryfornovelLLMreasoning
transformations are defined and implemented on the LLM
works. For example, one could consider using some of the
infrastructureside.
heterogeneousgraphlearningmethods[139],[178],[220]in
futurepromptingapproaches.
4 ESSENCE OF REASONING TOPOLOGIES
We now crystallize different concepts in the area of rea- 4.3 FundamentalUseCasesofThoughts&Topologies
soning topologies, and relate it to the general prompting
We identify two fundamental use cases of thoughts and
pipelineandthefunctionalformulationfromSection3.
topologies: in-context examples and reasoning steps that
bring us towards a solution. In a topology of thoughts,
4.1 WhatIsaThoughtandaReasoningTopology? a node v is reachable from another node u, if there ex-
Many works use the term “thought”. Yet, its precise mean- ists a path from u to v. If a node is reachable from the
ing can differ, depending on the setting. For example, in node modeling the input task statement, we call such a
CoT,athoughtreferstoastatementwithinaparagraphthat node a solution node, and the corresponding topology is
contains a part of the reasoning process aimed at solving a solution topology. However, certain nodes may be not
the input task. We show this in the top part of Figure 3. In reachable from the input node. For example, a user may
ToT, in some tasks, such as Game of 24, a thought means provide (in their prompt) in-context examples that form
an intermediate or a final solution to the initial question. a small topology, which are not a step in the reasoning
However, in creative writing, it could be a plan of solving towards solving the input task, but merely examples. We
the input task, or a passage of text. In GoT, a thought will refer to such thoughts and topologies as thoughts and
contains a solution of the input task (or of its subtask). For topologies of in-context examples. Examples of the two
example,itcanbeasubsetofdocumentstobesummarized, use cases can be found in Figure 3. Solution thoughts and
orasequenceofnumberstobesorted. topologiesaremarkedwiththebluecolorwhilein-context
Toencompassallthesecases,wedefineathoughttobea examples are marked with the green color. Topologies of
semanticunitoftaskresolution,i.e.,astepintheprocessofsolving in-context examples do not span beyond a single prompt.
agiventask.Alltheaboveexamplesfallintothisdefinition: Solution topologies, on the other hand, can span across
a step in task resolution can be a statement, a plan, a text manypromptsandreplies.
passage, a set of documents, or a sequence of numbers. Distinguishingbetweensolutionandin-contextexample
We model thoughts with nodes; edges between nodes thoughtsandtopologiescanenablemoreeffectiveandeffi-
correspond to dependencies between these thoughts.The cient LLM reasoning schemes. For example, a graph topol-
details of these dependencies are also use case specific. For ogy has to be represented in a certain way. Now, knowing
example, when generating a paragraph of text, if a given thattopologiesofin-contextexamplesareusuallylimitedto
paragraph y is a refined version of a previous version x, a single prompt, while solution topologies usually stretch
then x and y become nodes in the topology, and there is beyond an individual prompt or LLM reply, one could use5
An example chain-based interac�on
A thought refers to a
single sentence in a paragraph
Topology is fully Prompt 1 that contains the LLM reasoning
included within towards solving the task
a single prompt Q: Roger has 5 tennis balls. He buys 2 more cans of
tennis balls. Each can has 3 tennis balls. How many
A: Roger started with 5 balls. tennis balls does he have now? A ap: pT lh ee s c oa rf ige it ne ari la ly .has 23
A: Roger started with 5 balls. 2 cans of 3 tennis balls
2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11. They used 20 to make lunch.
each is 6 tennis balls. Q: The cafeteria has 23 apples. If they used 20 to
make lunch and bought 6 more, how many apples So they had 23 - 20 = 3.
5 + 6 = 11. do they have?
They bought 6 more apples,
The answer is 11. Reply 1 so they have 3 +6 = 9.
A: The cafeteria has 23 apples originally. They used
A thought refers to a 20 to make lunch. So they had 23 - 20 = 3. They The answer is 9.
single sentence in a paragraph bought 6 more apples, so they have 3 + 6 = 9. The
that contains the LLM reasoning answer is 9.
towards solving the task Topology is fully
included within
a single reply
An example tree-based interac�on
In-context examples (and poten�ally Prompt 1 Topology spans over
other parts of the context) may include a prompt and a reply
instances of topologies. For example, Use 4 numbers and basic
her we, i tt hh ie nr oe t a hr ee r 1 in-l -e cv oe nl t t er xe te es x a an md p f lo er se .sts a tori t oh bm tae in� c 2 o 4.pera�ons (+-*/) A r t e hth f eeo r nu s e g t xoh tt Input: 4 9 10 13 { t. h.. am t o core rr n eo spd oe ns d
Input: 2 8 8 14 p sa ort lu o �f ot nhe to other possible
Input: 2 8 8 14 { t. h.. am t o core rr n eo spd oe ns d Possible next steps: next steps...}
to more in-context 2 + 8 = 10 (le�: 8 10 14) 4 + 9 = 13 10 - 4 = 6
examples...} 8 / 2 = 4 (le�: 4 8 14) (le�: 10 13 13) (le�: 6 9 13)
14 + 2 = 16 (le�: 8 8 16)
2 + 8 = 10 8 /2 = 4 {...more in-context examples...} 13 - 10 = 3
(le�: 8 10 14) (le�: 4 8 14) (le�: 3 4 9)
Input: 4 9 10 13
1 (l4 e �+ : 2 8 = 8 1 16 6) A to t h tho eu g nh et x tr e pf ae rr ts Possible next steps: aT o pp ro omlog py t asp na dn as ro ev pe lyr
of the solu�on Reply 1
4 + 9 = 13 (le�: 10 13 13)
Seman�c role of the dark shade: Topology is fully 10 - 4 = 6 (le�: 6 9 13) (13 + 10) * 13 =
provide the next part of the solu�on ain sc il nu gd le ed p w roi mth pin t 1 {.3 .. m- 1 o0 re = l i3 n e(l se .� ..}: 3 4 9) 2 1 Th3 0 e* * r e1 1 3 3 is = + n 2 1 o9 3 w9 = a y143 ... { &.. . em do gr ee s n tho ad te s
lS ige hm t a sn h� adc ero : ele v ao lf u t ah te e Prompt 2 t t imo h e po s ob e st a b sii i bn g l e2 n4 u mw bit eh r s. likely c oo thrr ee rs pp ro on md pto t-
-reply pairs
E rev aa clu ha 2te
4
i (f
s
g ui rv ee /n
li
kn eu lym /ib me prs
o
c sa sin
ble)
. im..
possible
t oh tha et re v sea rlu iea ste
10 14 6 4 17 3 { t. h.. am t o core rr n eo spd oe ns d 10 14: 10 + 14 = 24. sure rA ef t eh rso u tog h at n of numbers...}
to more in-context 6 4: 6 * 4 = 24. sure evalua�on
examples...} 17 3: 17 + 3 = 20. likely Topology is
{...more in-context examples...} fully included in
1 su0 r + e 14 = 24 1 lik7 e + ly 3 = 20 10 13 13 a single prompt
6 * 4 = 24 A thought refers Reply 2
sure to an evalua�on 13 + 13 + 10 =
(13 + 10) * 13 = 23 * 13 = 299 36; impossible
10 * 13 + 13 = 143 There is no
way to obtain 24 with these big
numbers. impossible
Topology is fully ...
included within {...more prompt-reply a single prompt pairs that evaluate other
Topology is fully series of numbers...}
ain sc il nu gd le ed p w roi mth pin t Prompt i aT o pp ro omlog py t asp na dn as ro ev pe lyr
Are you sure, that numbers
10 13 13 are impossible to use?
Input: 5 3 2 { t. h.. am t o core rr n eo spd oe ns d M (13ay +b 1e 3 t r +y 1a 0ls )o this combina�on: { t. h.. am t o core rr n eo spd oe ns d A t oo t f h t tho heu e g n sh e ot x l utr e �pf oae nrr ts
to more in-context to other possible
examples...} {...more prompts next steps...}
2 + 5 = 7 3 + 2 = 5 & replies...}
(le�: 3 7) (le�: 5 5) 13 - 9 = 4 6 * 9 = 54
(le�: 4 6) (le�: 13 54)
Prompt j
3 * 2 = 6
(le�: 5 6) A to t h tho eu g nh et x tr e pf ae rr ts Use 3 numbers and basic 13 - 6 = 7
of the solu�on arithme�c opera�ons (+-*/) (le�: 7 9)
to obtain 24.
Input: 5 3 2
...
Possiblenextsteps:
Legend & concepts 2 + 5 = 7 (le�: 3 7)
3 * 2 = 6 (le�: 5 6) Topologies vs. prompts-replies
3 + 2 = 5 (le�: 5 5)
{...more in-context examples...}
Green color indicates a topology of in-context
exam top ple os lo, i g. ye . r, ea l at to ep do tlo og ty h eth pa ot sis e dn o int pa u p t a tr at s ko .f the Input: 6 9 13 T uo sep ro 'l so pg ry o o mf pin t-context examples in
Possible next steps:
Blue color indicates
rela i a. e ts e.o , d l tu o t� opo o tn hlo et g o iy np t po h ul ao tt g t iy as, sk. Reply j S t sh uo i gl su g c� eo so u �n l od t n o b p te oo , l tfo o hg r e y e L xi Ln a M mus pe ler' ,s u p sr eo rm 's pt:
13 - 9 = 4 (le�: 4 6)
Different shades 13 - 6 = 7 (le�: 7 9) Topology of in-context examples in
indicate different {...more lines...} LLM's reply: for example, LLM's
seman�c roles of sugges�ons for in-context examples
nodes/edges
Solu�on topology in LLM's reply:
this usually is a part of the LLM
reasoning towards solving the input task
Fig.3:Overviewandexamplesofhowreasoningtopologiescorrespondtotheuser–AIpromptinginteractions.
...
...
...6
different representations for these two topology classes, in The provided blueprint and taxonomy are applicable to
ordertominimizetokenutilizationineachofthem. bothsolutionandin-contextexampletopologies.Forexam-
Both topologies can collectively be modeled also as a ple, a topology modeling an individual in-context example
graph, with multiple components corresponding to topolo- can have its own representation, schedule, etc.. However,
gies that are not connected with one another. To further for clarity, we will focus on applying the blueprint and
facilitatefutureoptimizations,onecouldpotentiallyharness taxonomymostlytosolutiontopologies.
a hypervertex model [31], in which arbitrary subgraphs can
be modeled as individual nodes called hypervertices. In
4.5.1 TopologyofReasoning
such a view, one could model each separate topology of
in-contextexamplesasahypervertex,connectedwithother
A reasoning scheme can harness different topologies for
nodes or hypervertices with hyperedges. This approach
the LLM reasoning process. Here, we distinguish chains,
could be harnessed to provide a theoretical framework for
trees, and graphs. Note that both a chain and a tree are
optimizingtheholisticperformanceofareasoningscheme,
eachaspecialcaseofaconnectedgraph:atreeisanacyclic
including its ingredients such as in-context examples, in
connected undirected graph, and a chain is a path graph
relationtoallotherschemeingredients.
(i.e., a tree where each node has at most one child); see
Part ❶ of Figure 4. Still, we treat them separately because
4.4 FunctionalFormulationofReasoningTopologies they differ in their effectiveness for different prompting
TheLLMreasoningthatharnessestopologiesisformulated tasks[10],[133],[213].Underthisview,theplainIOprompt
intheexactlysamewayasdescribedinSection3.However, canbeviewedasasinglenodegraph.
one has to take into account the fact that prompts p i, Second, we observe that these topologies can be har-
replieso (cid:101)i,andcontextc iingeneral,theyallcontainthoughts nessed within an individual prompt or a reply (single-prompt
and their dependencies. Now, the exact way in which a topology), but also across prompts or replies (multi-prompt
topology is mapped to p i, o (cid:101)i, and c i, depends on a specific topology);seePart❷ofFigure4.
prompting scheme. For example, in the i-th prompting An important aspect is the representation of the topol-
interaction of CoT, a reasoning topology T is a subset ogy, see Part ❸ of Figure 4. The representation can be
(potentially a proper subset) of either the LLM reply o (cid:101)i implicit (the nodes and edges are not specified explicitly)
(when T is a solution topology), or of the user prompt or explicit (nodes and edges are stated explicitly). Explicit
p i, as an in-context example (when T is a topology of in- representations vary and include a set of triples [17] or a
context examples). In many tree and graph based schemes, descriptionofnodesandedgesinnaturaltext.Implicitrep-
however,thismappingisnotsosimple,andatopologycan resentationdependsonascheme–forexample,itcouldbea
spanacrossmultiplepromptsandreplies.Weillustratethese textual recipe that prescribes generating the next reasoning
examplesinFigure3. steps.Finally,wealsoidentifyhowthetopologyisderived
When developing an LLM reasoning scheme that har- – for example, it can be constructed by the user or by the
nessesreasoningtopologies,oneneedstospecifythedetails LLM itself (Part ❹ of Figure 4). Specifically, derivation of a
of such mappings, but also build the representation of that multi-prompttopologycanbemanual(fixedbyuserbefore
topology,thescheduleoftraversingthetopology,andmany theLLMreasoning),automatic(decideddynamicallybythe
others.TofacilitatedevisingfutureLLMreasoningschemes, LLM), or semi-automatic (the overall reasoning structure
we now provide a blueprint that clearly defines all these is predefined before the LLM reasoning starts, but the
aspectsandhowtheycanbeinstantiated. user/LLM have some control over the structure as well
duringtheactualreasoning).
4.5 ABlueprintforLLMReasoning
We identify the following fundamental aspects of an LLM
4.5.2 ReasoningSchedule
reasoningschemethatharnessesatopology:❶topologyclass
(the structure of connections between intermediate steps The reasoning topology forms the “skeleton” for the LLM
of the LLM reasoning, Section 4.5.1), ❷ topology scope (the reasoning, effectively prescribing the algorithm for solving
mapping between the topology and prompts/replies/con- a given task. However, for a given fixed topology, many
text, Section 4.5.1), ❸ topology representation (how a given promptingschemesofferdifferentapproachesfortheexecu-
topology is represented within a prompt/reply/context, tion of the intermediate reasoning steps. For example, ToT
Section4.5.1),❹topologyderivation(howagiventopologyis
harnesses Breadth-First Search (BFS) or Depth-First Search
obtained,Section4.5.1),❺reasoningschedule(howisagiven
(DFS).Thismotivatesustointroduceanotherdimensionof
topology traversed to conduct the LLM reasoning, Sec- structure-enhancedreasoning,namelythereasoningschedule.
tion4.5.2),❻schedulerepresentation(howisagivenschedule
This schedule prescribes how the topology of reasoning is
representedwithinaprompt/thought,Section4.5.2),and❼ goingtobeprocessed(Part❺ofFigure4).
harnessedpartsoftheAIpipeline(whatpartsofthegenerative Wheneverthescheduleisspecified,itcanberepresented
AIpipeline,beyondprompting,areused,Section4.5.3).We in different ways. It can be a description in a natural lan-
picture the blueprint in Figure 4, and we analyze existing guage, a code specification, in-context examples, or others
schemeswithrespecttothisblueprint1,inTable1. (Part❻ofFigure4).Finally,aswiththetopology,thesched-
uleitselfcouldalsobedeterminedusingdifferentmethods,
1We encourage participation in this analysis. In case the reader is
forexamplebytheLLMon-the-fly,orpre-determined(e.g.,
in possession of additional information relevant for the analyzed
schemes,theauthorswouldwelcometheinput. asthefixedBFSschedule).7
Blueprint & taxonomy of a structure-enhanced reasoning scheme
Topology Schedule Extensions
5 Reasoning Schedule Harnessed Parts of the
1 Class 2 Scope 3 Representa�on 4 Deriva�on 7
Genera�ve AI Pipeline
6 Schedule Representa�on
1 Topology Class 2 Topology Scope
What is the connec�on structure of reasoning steps? Can the topology extend beyond a single prompt?
Chain Tree Graph Single-prompt Mul�-prompt
Input Input Input sc inoT gnh lt ee a i ps nt rer ou d mc wt pu i ttr /he r i ei ns p a ly oP rr o rem pp lyt caO rnn e e c ao sp onr nto a im nin gp m nt/ our de l� ep p sly le cO a rnn e e c a o sp onro ntam ininp g t a n/ osre i dnp egl ly e
Prompt or
reply
Output Output Output Prompt or
reply
3 Topology Representa�on 4 Topology Deriva�on
How is the topology structure represented? How is the topology structure derived?
0 Implicit Explicit Automa�c, semi-automa�c Manual The user sta�cally
prescribes the
1 2 3 " shTh oe u lfi dr bst e p er ne hli am ni cn ea dr y t hs ro elu e� �o mn es. < <n no od de e 0 1> > c co on nn ne ec ct ts s t to o < <n no od de e 1 4> >, , < <n no od de e 2 5> >, <node 3> bT yh e th s et r Lu Lc Mtu ore n -is th c eo -n fls yt ,r eu ic tt he ed r s rt er au sc ot nu ir ne g b se tf ao rr te s
4 5 6 7 8 9 E s aoa ulc guh m� o o ef n n t s th ese dhs o ie nu t l th d wr be oee a fe �un erh t mha en prc tse "d < <n no od de e 2 3> > c co on nn ne ec ct ts s t to o < <n no od de e 6 8> >, , < <n no od de e 7 9> > ... f tu w hll eiy t h u( a sc et eo rr m t (a sa ein� m c c i) o - ao n ur t r tp o oa l m r f� r aoa �ml cly ), ...
5 Schedule Class 6 Schedule Representa�on 7 Genera�ve AI Pipeline
How is the topology structure explored? How is the schedule represented?
What parts of the genera�ve AI pipeline are used, besides promp�ng?
DFS 0 BFS 0 deT se cx rt ipu �al on I en x- aco mn pte lex st Modali�es? Pre- Tools?
... ... training?
1 2 3 1 2 3 " BP Fr So mce ae nd n i en r "the " <T 1r >a ,v <e 4rs >e " nodes <0>, Retrieval?
Fine-
4 5 6 4 5 6 " DP Fr So mce ae nd n i en r t "he " <T 1r >a ,v <e 2rs >e , <n 3o >d "es <0>, tuning?
Fig.4:Taxonomyandblueprintforstructure-enhancedreasoning.
4.5.3 BeyondPrompting utilization.Wefinishthissectionwithacomparativeanalysis
ManyschemesgobeyondpurepromptingLLMs.Thismay andillustrationsofexampletopologyrepresentations.
includepre-training,fine-tuning,retrieval,tools,ordiffer-
ent modalities (Part ❼ of Figure 4). We also consider this 5.1 Multi-StepReasoning
aspect, as it provides insights into the integration of rea-
The concept of multi-step reasoning was first introduced
soningtopologieswithothermechanismsintheAIpipeline
through the seminal Chain-of-Thought (CoT) [195], a
beyondplainpromptinginteractions.
single-prompt scheme, which uses topologies of in-context
examples, also known as few-shot examples, to guide the
5 REASONING WITH CHAINS LLM into reasoning step-by-step before providing the final
We now proceed to investigate in more detail individual answer. Different following works augment or adapt in-
schemesthatusechaintopologies.Weanalyzetheseworks contextexamplestoelicitdifferentformsofreasoningsteps,
with respect to our blueprint and taxonomy in the top whilestillrelyingonthesingle-promptchaintopology.For
part of Table 1 (detailed descriptions of each individual instance,insteadofonlyprovidingastep-by-stepreasoning
scheme are provided in the appendix). We also illustrate chain in the examples, SelfAsk [152] expands each step in
fundamental concepts introduced in these works, namely the chain to also pose a follow-up question that is then
multi-step reasoning, zero-shot reasoning, planning & task de- answered in the subsequent step. Similarly, Program of
composition, task preprocessing, iterative refinement, and tool Thoughts (PoT) [41] uses code examples, rather than the8
Topology Reasoning AI
single-prompt multi-prompt Schedule Pipeline
Scheme Remarks
Class Rp. Dv.Class Rp.Dv. Scheme Rp.Dv.P F R T Modalities
Chain-of-Thought(CoT)[195] chain I(text) SA - - - - - - Ø Ø Ø Ø text
Zero-shot-CoT[112] chain I(text) SA - - - - - - Ø Ø Ø Ø text
SelfAsk[152] chain I(text) SA - - - - - - Ø Ø  Ø text
Plan-and-SolvePrompting[188] chain I(text) SA - - - - - - Ø Ø Ø Ø text
ProgramofThoughts(PoT)[41] chain I(text,code)SA - - - - - - Ø Ø Ø  text,code,table
Selection-Inference(SI)[51] - - - chain E M linear I M Ø  Ø Ø text
Chain-of-symbol(CoS)[89] chain I(text) SA chain E SA linear I M Ø Ø Ø Ø text
Least-to-MostPrompting[233] - - - chain E SA linear I M Ø Ø Ø Ø text
DecomposedPrompting[105] - - - chain E SA linear I M Ø Ø   text
LogiCoT[231] chain I(text) SA tree E SA linear I M Ø Ø Ø Ø text
SELF-REFINE[140] - - - chain E SA linear I M Ø Ø Ø Ø text
Reflexion[168] - - - chain E SA linear I M Ø Ø Ø Ø text
ReasoningGraphVerifier(RGV)[35] chain I(text) SA graph E SA linear I M Ø Ø Ø Ø text
Plan,VerifyandSwitch(PVS)[131] chain I(text,code)SA chain E SA linear I M Ø Ø Ø  text,code
Chameleon[136] - - - chain E SA linear I M Ø Ø   text,code
ChatCoT[45] chain I(text) SA chain E SA linear I M Ø Ø   text
Tree-of-Thought(ToT)[133] tree I(text) M tree E SA arbitrary E M Ø Ø Ø Ø text
TreeofThoughts(ToT)[213] tree I(text) M tree E SA arbitrary E M Ø Ø Ø Ø text
ThoughtDecomposition[205] tree I(text) M tree E SA beam† E SA Ø Ø Ø  text,code †stochastic
Self-ConsistencywithCoT[190] chain I(text) M tree(  )† E SA - - - Ø Ø Ø Ø text †depthone
CreswellandShanahan[50] tree I(text) M tree E SA beam E A Ø  Ø Ø text
DynamicLeast-to-MostPrompting[58] tree I(text) M tree E A bottomup E A Ø Ø  Ø text,code
AlgorithmofThoughts(AoT)[166] tree I(text) M - - - DFS,(BFS)I M Ø Ø Ø Ø text
TreeofUncertainThought(TouT)[145]tree I(text) M tree E SA BFS,DFS E M Ø Ø Ø Ø text
Tree-of-Mixed-Thought[91] tree I(text) M tree E SA DFS E A Ø Ø Ø  scenegraphs
TreeofClarifications(ToC)[106] tree(  )† I(text) M tree E SA BFS E A Ø Ø  Ø text †depthone
TreePrompting[170] - - - tree E A top-down E A Ø Ø Ø Ø text
Skeleton-of-Thought(SoT)[148] tree(  )† I(text) M tree(  )† E A parallel E A Ø Ø Ø Ø text †depthone
Branch-Solve-Merge(BSM)[162] tree(depthone)I(text) M graph(  )† E SA BFS E M Ø Ø Ø Ø text †doubletree(  )
ThoughtPropagation(TP)[218] graph(  ) arbitrary M graph(  )† E SA BFS E M Ø Ø Ø Ø text †doubletree(  )
SocraticQuestioning[154] tree(depthone)I(text) M graph(  )† E SA DFS E M Ø Ø Ø Ø multi †doubletree
GraphofThoughts(GoT)[10] graph(  ) arbitrary M graph E M arbitrary E M Ø Ø Ø Ø text
GraphofThought(GoT)[119] ? ? ? graph E (S)ADFS E ? Ø Ø Ø Ø text
Graph-of-Thought(GoT)[215] graph I(text) M chain E M linear E M Ø  Ø Ø text,image
ControlLLM[132] graph E(json) M graph E M DFS E M Ø Ø† Ø  text,image,video,audio†canbeused
CumulativeReasoning[224] graph(DAG) I(text) M graph(DAG)E SA arbitrary E M Ø Ø Ø Ø text
EverythingofThoughts(XoT)[57] graph ? L chain E M linear E M  Ø Ø Ø text
ResPrompt[99] graph I(text) M - - - - - - Ø Ø Ø Ø text
Hypergraph-of-Thought(HoT)[212] hypergraph ? M - - - - - - Ø  Ø Ø text,image
BatchPrompt[124] batch E(text) M chain E M linear E M Ø Ø Ø Ø text
MemoryInjections[163] - - - - - - - - - Ø Ø Ø Ø text
TABLE1:ComparisonofLLMreasoningschemeswithrespecttotheprovidedtaxonomy(Section4.5andFigure4).“Topology”:Detailsoftheharnessedstructure.
“Single- / Multi-prompt”:Doesagivenschemesupportsingle- / multi-prompttopology?Ifyes,whatisthesupportedClass,Representation,andDerivation?
“ReasoningSchedule”:Detailsoftheharnessedreasoningschedule,includingitsspecificScheme,Representation,andDerivation.“AIpipeline”:Doesagiven
schemeharnesspartsoftheAIpipelinebeyondprompting?Ifyes,whichones?(“P”:pre-training,“F”:fine-tuning,“R”:retrieval,‘T”:tools,“Modalities”:modalities).
Whendescribingrepresentations,weusethefollowingabbreviations:“E”:explicit,“I”:implicit.Whendescribingderivation,weusethefollowingabbreviations:“A”:
automatic,“L”:learned,“M”:manual,“SA”:semi-automatic.“  ”:fullsupport(i.e.,YES),“  ”:partially[supported],“Ø”:nosupport(i.e.,NO).
natural-language-basedexamplesasinCoT,toobtainastep- solution.PSoperatesinazero-shot,multi-stepmanner,thus
by-step generated, functional Python program that can be alsorelyingontheprevioustwoconceptsaswell.
executedtogetthefinalresult.
Planning and decomposition is also frequently used in
multi-prompt chains. The introduction of a node for speci-
5.2 Zero-ShotReasoningInstructions fyingthedetailsofthedecompositionatthestartofamulti-
prompt reasoning chain, usually not only determines the
Zero-shot reasoning instructions aim to elicit the same
chain’s depth, but also facilitates more effective reasoning
multi-step reasoning chains, but without the use of hand-
approaches in the subsequent sub-steps. This allows for a
tuned,problem-specificin-contextexamples,i.e.,theyelim-
finer-grained resolution of sub-tasks, enriching the overall
inate chain topologies forming in-context examples. Zero-
reasoning process. Here, Least-to-Most Prompting [233]
shot-CoT[112],anextensiontoCoT,achievesthisbysimply
grows a reasoning chain where decomposition of complex
prompting the LLM with one sentence, “Let’s think step by
tasksorquestionsisconductedinthefirstnodeandthesub-
step”, or using other similar statements. Along the same
tasks/sub-questions are solved in the subsequent nodes.
lines,PoTcanalsoleveragezero-shotreasoninginstruction,
Specifically, the multi-prompt chain operates by first de-
e.g., “Let’s write a Python program step by step and return the
composingtheoriginalquestionintoalistofsub-questions,
result.Firstlyweneedtodefinethevariables.”.
which are each solved in individual sub-steps with the
questions and answers of previous sub-steps included in
5.3 Planning&TaskDecomposition
the context. The chain terminates when the final answer is
Both planning and task decomposition aim to break down returnedafteransweringallsub-questionsinthelist.Then,
a task into a number of manageable sub-tasks that help Decomposed Prompting [105] is a modular framework for
reaching the final solution. Plan-and-Solve (PS) Prompt- a detailed decomposition of complex tasks. To generate a
ing[188],oneofthekeysingle-promptschemesbuildingon reasoning chain, the LLM is prompted by demonstrations
this concept, first divides the complex task into a chain of comprised of sequential question-operation triplets, which
sub-tasks and then executes these step-by-step for the final form“sub-questions”.IncontrasttoLeast-to-Mostprompt-9
ing, this allows for the recursive breakdown of questions Game of 24
intosimplersub-questionsuntiltheycanbesolveddirectly,
User Prompt
as further decomposition is a valid operation in the frame-
work.Apartfromthesetwoaboveschemes,decomposition Use numbers andbasic arithmetic operations (+ - * /) to obtain 24
isappliedinmanysimilarworks[59],[102],[217]. Q: 8 8 7 4
A: (7 - (8 / 8)) * 4 = 24
5.4 TaskPreprocessing Q: 9 6 5 3
A: (5 + 3) * (9 - 6) = 24
Theconceptoftaskpreprocessingcomprisesanytechnique
Q: 14 8 8 2
that preprocesses the context of a task by updating it or
rephrasing the task description itself, before taking any
LLM Answer
reasoning steps. For instance, the multi-prompt scheme
Selection-Inference(SI)[51]isdesignedtotacklemulti-step
Sure, here's one way to obtain 24 using the numbers 14, 8, 8, and 2
logical reasoning problems where all essential information
is already present within the input context. The key func- ((14 - 8) / 2) * 8 = 24
tionalityofSIliesinitsrecurringprocessofcontextpruning
before each reasoning step. This means that it selectively
Fig.5:GAMEOF24.AnexampleshowinganIOscheme(effectivelyanimplicit
filters the context to retain only the relevant information single-prompt single-node chain topology). A few-shot IO prompt is used,
necessary for each specific subsequent step of reasoning, leading to a single output directly providing the result with no intermediate
reasoningsteps.
ensuring that the most pertinent data is always used for
each decision-making stage. On the other hand, instead of
pruning the context, the multi-prompt scheme Chain-of- decompose tasks, verify and refine intermediate solutions,
symbol(CoS)[89],specificallydesignedforspatialplanning preprocess initial prompts, and utilize external tools, such
tasks,augmentsthecontextwithcondensedsymbolicrepre- asPythonscripts.ThismultifacetedapproachenablesLLMs
sentations, before subsequently using those as prompts for to tackle more complex problems by breaking them down
theLLMtoconductCoT-basedreasoning. into smaller, more manageable components and iteratively
refiningthesolutions.
5.5 IterativeRefinement
5.7.2 RepresentationsofTopology&Schedule
The introduction of verification enables the reasoning
We now illustrate representative prompts that show dif-
frameworks to iteratively refine the generated context and
ferences between single- and multi-prompt chain-based
intermediate results. With this strategy, the execution of
topologies (the tasks are described in detail in the prompts
chain-based reasoning is effectively extended with loops,
of the figures). We use examples based on the well-known
with conditions on how many times one can loop over a
tasks of Game of 24, Creative Writing, and Mathematical
node (based on the number of iterations or some terminal
or Logical Reasoning. For this, we illustrate an example
conditions).Theconceptisappliedindifferentworks[123],
simplest IO scheme in Figure 5 (effectively a single-node
[126],[140],[168],[198],[231].
CoT) and compare it to an implicit single-prompt few-shot
CoT in Figure 6, an implicit single-prompt zero-shot CoT in
5.6 ToolUtilization Figure 7 and to an example few-shot explicit multi-prompt
To better integrate multiple execution methods, more ef- CoT(Selection-Inference)inFigure8.
fective schemes opt to devise a plan that specifies tools
5.7.3 Performance
for handling each sub-task, before executing the reasoning
chain. Examples include AutoGPT [160], Toolformer [165], We now summarize performance patterns found in chain
Chameleon[136],ChatCot[45],PVS[131]andothers[230]. topologies. For a more detailed performance comparison,
seeAppendixE.1.
Overall,inArithmeticReasoning,CoTsignificantlyout-
5.7 Analysis&ComparisonofDesigns
performs Input-Output (IO) prompting, with notable im-
We now broadly discuss and analyze chain designs with provements in terms of accuracy on several datasets from
respecttodifferentaspectsofourblueprint. benchmarks such as GSM8K (Grade School Math) [48],
SVAMP(SimpleVariationsonArithmeticMathwordProb-
5.7.1 Topology&ItsConstruction
lem) [151], and MAWPS (MAth Word ProblemS) [113],
In single-prompt schemes, the entire reasoning process is which focus on mathematical tasks. The effectiveness of
executed within a single prompting round. This approach CoTincreaseswiththescaleoftheLLM.VariantslikeZero-
is less common for complex tasks, as it often demands shot-CoT,PoT,andschemeswithdecompositionnodes(like
sophisticated prompt engineering to encompass the entire Least-to-MostPrompting,PS+)showfurtherimprovements
reasoning pathway in one go. On the other hand, most in specific datasets. Chameleon, with its table reader tools,
chain designs employ multi-prompt schemes, where the enhancesperformanceintabularmathquestions.
reasoning process is segmented into multiple rounds of In Commonsense Reasoning, CoT excels over IO
prompting. This allows for a more nuanced and step-wise prompting with a distinct advantage in datasets like Strat-
approach to problem-solving. Key novel architectural fea- egyQA [71]. Specialized methods like SelfAsk and the
tures of chain designs include the ability to appropriately Selection-Inference framework show further improvements10
Math Reasoning
User Prompt LLM Answer
Q: Alexis is applying for a new job and bought a new set of business clothes to wear to Let D be the number of driveways Tobias shoveled.
the interview. She went to a department store with a budget of $200 and spent $30 on
a button-up shirt, $46 on suit pants, $38 on a suit coat, $11 on socks, and $18 on a
belt. She also purchased a pair of shoes, but lost the receipt for them. She has $16 left He earned $5 allowance per month for three months, so he has $5 * 3 = $15 from his
from her budget. How much did Alexis pay for the shoes? allowance.
A: Let S be the amount Alexis paid for the shoes.
He mowed 4 lawns, each earning him $15, so he made $15 * 4 = $60 from mowing
lawns.
She spent S + 30 + 46 + 38 + 11 + 18 = S + 143.
He also earned $7 for each driveway shoveled, and he shoveled D driveways, so he
made $7 * D from shoveling driveways.
She used all but $16 of her budget, so S + 143 = 200 - 16 = 184.
Intotal, he has $15 (allowance) + $60 (mowing lawns) + $7D (shoveling driveways)
Thus, Alexis paid S = 184 - 143 = 41 for the shoes. before buying the shoes.
So the answer is 41. After buying the $95 shoes, he has $15 in change.
Q: [more in-context examples...]
Therefore: $15 + $60 + $7D - $95 = $15
Q: Tobias is buying a new pair of shoes that costs $95. He has been saving up his
money each month for the past three months. He gets a $5 allowance a month. He
also mows lawns and shovels driveways. He charges $15 to mow a lawn and $7 to
shovel. After buying the shoes, he has $15 in change. If he mows 4 lawns, how many
driveways did he shovel? So,Tobias shoveled 5 driveways.
Fig.6:MATHREASONING.Anotherimplicitsingle-promptchaintopologyexample,encodedwithtext.Itshowshowfew-shotCoTsolvesamathquestionfrom
GSM8K[48].Thisresultsinanimplicitchainwhereindividualreasoningstepscanberegardedasnodes.
Creative Writing Logical Reasoning
User Prompt User Prompt LLM Answer
Write a coherent passage of 4 short paragraphs. The end sentence of each paragraph If something is quiet and cold then Selection Inference
must be: it is smart.
1. It isn't difficult to do a handstand if you just stand on your hands. All young, cold things are kind. All smart, blue things are kind.
2. It caught him off guard that space smelled of seared steak. If something is kind then it is nice. We know that Dave is smart and Dave is blue.
3. When she didn't like a guy who was trying to pick her up, she started using sign All smart, blue things are kind.
language. If something is cold then it is quiet. Dave is kind.
4. Each person who knows you has a different perception of who you are. If Anne is smart and Anne is kind
Let’s think step by step. then Anne is cold. If something is kind then it is nice.
If something is nice and blue then We know that Dave is kind.
LLM Answer it is young.
If something is nice and young
Mastering a handstand may seem daunting at first, but the key lies in simplicity. It isn't then it is cold. Dave is nice.
difficult to do a handstand if you just stand on your hands. Breaking down the complex
maneuver into a straightforward action can make the seemingly impossible, achievable. Anne is young. If something is nice and blue then it is young.
Sometimes, the most challenging tasks become manageable when approached with a Gary is nice. We know that Dave is nice and Dave is blue.
clear and uncomplicated perspective. Charlie is quiet.
Anne is blue. Dave is young.
[...]
Dave is smart.
In the vast expanse of outer space, surprises await even the most seasoned explorers. Anne is quiet. If something is nice and young then it is cold.
It caught him off guard that space smelled of seared steak. The unexpected aroma Dave is blue. We know that Dave is nice andDave is young.
challenged preconceptions, reminding us that the universe is full of mysteries that Gary is cold.
extend beyond the visual and auditory senses. The interplay of senses in unfamiliar Anne is nice. Dave is cold.
environments can reshape our understanding of the cosmos.
Does it imply that the
If something is cold then it is quiet.
statement "Dave is not quiet"
We know that Dave is cold.
Navigating social interactions often demands creative strategies, especially when faced is True?
with unwanted advances. When she didn't like a guy who was trying to pick her up, she
started using sign language. Non-verbal communication became her shield, a tool to Dave is quiet.
ward offunwelcome attention without resorting to confrontation. In these moments,
the power of expression transcends spoken words, allowing for a graceful escape from
uncomfortable situations. Fig.8:LOGICALREASONING.Anexampleexplicitmulti-promptchaintopol-
ogy, encoded with text.Itdemonstratesthemanuallyderivedchaintopology
Perceptions of identity are as diverse as the individuals who form them. Each person of Selection-Inference [51]. ”Selection” and ”Inference” indicate independent
who knows you has a different perception of who you are. It's a kaleidoscope of consecutiveprompts/thoughts.
perspectives, shaped by unique experiences and interactions. Understanding that our
multifaceted nature is reflected in the eyes of those around us can foster empathy and
open the door to a richer tapestry of human connections.
In Symbolic Reasoning, CoT demonstrates near-perfect
Fig.7:CREATIVEWRITING.Anexampleimplicitsingle-promptchaintopology, accuracy for in-context examples and substantial accuracy
encodedwithtext.ItshowstheCreativeWritingexamplefromToT[213]using
for out-of-domain cases for tasks like last letter concatena-
zero-shotCoTprompting.Thisresultsinanimplicitchainwithafirstnodefor
thequestionandonenodeforeachgeneratedparagraph. tionandcoinflippredictions.Schemeswithdecomposition
nodes outperform CoT in more complex tasks and longer
wordscenarios.
in multi-hop questions. Decomposition-based schemes like Overall, across different domains, CoT and its variants
Least-to-Most and Decomposed Prompting achieve high showaconsistenttrendofoutperformingbasicIOprompt-
accuracy in tasks requiring sequential actions or multi- ing. The integration of additional tools, tailored prompting
faceted reasoning. ChatCoT, with its retrieval and external strategies(likefew-shotorzero-shot),andtheincorporation
toolutilization,showsa20%gainonspecificdatasets. ofdecompositionandrefinementnodesenhancetheLLMs’
...11
Tree Topology Variants Graph Topology Variants
k-ary tree Tree of chains 1-Level trees Directed graphs Directed graphs Hypergraphs Thought Propaga�on
+ loops
Input Input Input Input Input Input Input
Example designs:
Skeleton-of-Thought
Binary tree Output
Input
Socra�c Ques�oning
Output Output Output Output Output
Different frameworks E Gx ra am php ole f d The osi ug gn hs t: s Input
enable using arbitrary Schemes that improved
tr ve ae rs ia, nb tu t is t ah e k -p ar re yv ta rl ee ent wsu u ip tc hho n Sa esC l h fC -a h Ci a on i n- no s- if o s-T f t- eh T no h cu o yg u , h g at h r, et Branch-Merge The only currently exis�ng
Example designs: effec t� ov pe ol ly o h ga y r vn ae rs iasi nn tg s tree Input tH hy ep He yrg pr ea rp gh ra-b pa hs -e od f- Ts hch oe um ghe t, ,
Tree-of-Thought uses a limited variant of
Tree of Thoughts hypergraphs, in which
[Creswell & Shanahan] Example designs: hyperdges span only
D Aly gn oa rm ithic m L e oa f s Tt h-t oo u-M gho ts st Output G Gr ra ap ph h -o of f -T Th ho ou ug gh ht ts triples of ver�ces
T T T Tr r r he e e oe e e u - o oo gf f hf -U C tM l Dn aic ex re ie cfir od ct m-a aT �i phn o oo T n suh s ig �o h ou t nght E C Rx eoa aTm s w op nitl ie h n gd S ee Gls f ri - ag C pn o hs n: Vsi es rt ie fin ec ry E Trx ea em Pp rl oe m d pe �si ng gns: Output C C E Rv eo u e smn r Pt yur rto ol hal mL i� nL pv gM e t o R fe Ta hs oo un gin hg ts E Hx ya pm erp gl re a pd he -s oig f-n T: hought Output
Fig.9:Variantsoftreeandgraphpromptingtopologies.
performancesignificantly. 6.2 Single-LevelTrees
Tree-based approach has also been harnessed in Skeleton-
6 REASONING WITH TREES of-Thought (SoT) [148], which effectively harnesses a tree
with a single level of depth. This scheme aiming to reduce
We next investigate in more detail individual schemes that
theend-to-endgenerationlatencyofLLMs,causedbytheir
usetreetopologies.Weanalyzetheseworkswithrespectto
inherent sequential decoding. Instead of generating one
our blueprint and taxonomy in the middle part of Table 1
long continuous answer, this scheme uses a divide-and-
(detailed descriptions of each individual scheme are pro-
conquer approach. In a first prompt, the LLM is instructed
vided in the appendix). We structure the discussion based
to generate a skeleton of the answer, i.e., a list of points
ontheharnessedtopologyvariants,namelytreesofchains,
that are independently answerable. Then, for each of these
1–level trees, and k–ary trees, see Figure 9 for details. As
points,anewpromptisissuedinparalleltoanswerjustthis
in chain schemes, we also discuss fundamental concepts
specific part of the question. As these points are processed
introducedorharnessedintheseworks.Treeschemes,most
inparallel,theoveralllatencyisreduced.
importantly, introduce exploration (i.e., generating multiple
thoughtsfromagivenone).Thepurposebehindexploration
is usually either task decomposition (which is similar to CoT, 6.3 k–AryTrees
but it differs as decompositions are not limited to a single
linear plan) or sampling (i.e., having a higher chance of Numerous schemes have harnessed mroe general k–ary
obtaining a high-quality solution). Moreover, tree schemes trees.First,theTree-of-Thought(ToT)designbyLong[133]
also introduce voting (i.e., automatic selection of the best utilizes a tree structure to decompose a problem into sub-
outcome of all the generated outputs) and they harnesses problemsandsolvethemusingseparateLLMprompts.Af-
various architectural concepts also used in chain schemes, tertheLLMsuggestspossiblenextstepsandcorresponding
suchasiterativerefinementortaskpreprocessing.Wefinish partial solutions, a checker module decides if any of these
thissectionwithacomparativeanalysisandillustrationsof solutions is valid, whether it can be selected as the final
exampletopologyrepresentations. one,orwhetheritshouldbacktracktothepreviousstep.All
issued prompts and answers are explicitly stored as a tree
structure and navigated through using a controller module.
6.1 TreesofChains
The LLM prompting is only used to generate the next
While trees as reasoning topologies have been explicitly individualsteps(i.e.,hops)inthistree,whereastheoverall
established in the works by Long [133] and Yao [213], this problemsolvingprocessiscoordinatedbythecontroller.
ideahasbeenpresentearlier.Chain-of-Thought with Self- Tree of Thoughts (ToT) by Yao et al. [213] differs from
Consistency (CoT-SC) [190] is an early scheme that har- theaboveToTapproachinusingtheLLMitselfasasolution
nesses the tree structure to a certain degree. Here, multiple evaluator with access to all generated solutions, instead of
CoTsoriginatefromthesameinitial(root)prompt,forming using a programmed or learned evaluator module. This
a“treeofchains”.Thechainprovidingthebestoutcometo allowstoratestatesindividuallyorvoteacrossintermediate
theinitialquestion,isselectedasthefinalanswer. solutionstoselectthemostpromisingonetocontinuewith12
the search. Both mentioned ToT approaches are a general- hard to find and it often depends on the specific problem
izationoftheIO,CoT,andCoT-SCpromptingschemes. to solve. Easily decomposable problems may benefit less
Other examples in this class of topologies include from more branching than complex problems. Specifically,
Thought Decomposition [205] (a multi-prompt scheme morecomplicatedproblemsprofitmorefromdecomposing
based on stochastic beam search and self-evaluation), a them into many/diverse sub-problems (e.g., this ensures
scheme by Creswell and Shanahan [50] (an extension of enough diversity for self-consistency to work better). In
the chain-based Selection-Inference [51]), Dynamic Least- contrast,aquestionthathasclearlyonlytwosub-partsdoes
to-Most Prompting [58] (an extension of least-to-most notbenefitfrommanymoresubdivisions,astheadditional
prompting with a tree-based problem decomposition and branches then can be either redundant or wrong. Single-
a dynamic external tree-based few-shot example selection), prompt approaches can perform better on some problems
Algorithm of Thoughts (AoT) [166] (a single-prompt ap- than multi-prompt approaches, while using only a single
proach that utilizes in-context examples formulated in an promptcomparedtopossiblyhundreds[166].
algorithmictree-basedfashion),TreeofUncertainThought
(TouT) [145] (an extension of ToT with local “uncertainty
7 REASONING WITH GRAPHS
scores” by incorporating the variance of multiple LLM re-
sponses into the state evaluation function), Tree-of-Mixed- Wealsoanalyzeschemesthatharnessgraphtopologies,see
Thought (TomT) [91] (a ToT-based reasoning scheme to the bottom part of Table 1 (detailed descriptions of each
answer questions on visual scene graphs), or Tree of Clar- individual scheme are provided, as for chains and trees, in
ifications (ToC) [106] (recursive prompting of an LLM to the appendix). Similarly to the tree analysis, we structure
constructatreeofdisambiguationsfortheinitialquestion). thediscussionbasedontheharnessedtopologyvariants,see
Figure 9 for details. We also discuss fundamental concepts
introduced or harnessed in these works. Graph schemes,
6.4 Analysis&ComparisonofDesigns
most importantly, introduce aggregation (i.e., being able to
We now broadly discuss and analyze tree designs with combine multiple thoughts into a single one). The purpose
respecttodifferentaspectsofourblueprint. behind aggregation is usually synergy (i.e., being able to
produceanoutcomebetterthantheindividualingredients)
6.4.1 Topology&ItsConstruction
oraneffectivecompositionofoutcomesoftasks.Graphschemes
The key novel architectural feature of tree schemes is the also use architectural concepts employed in chain or tree
explorationofathought,i.e.,theabilitytogeneratemultiple schemes, such as exploration or iterative refinement. We
newstepsbasedonagivensingleone.Thevastmajorityof conclude this section with a comparative analysis and il-
treeschemesaremulti-prompt.Mostmulti-promptschemes lustrationsofexamplerepresentationsofgraphtopologies.
use a dynamic approach to building the tree topology. The
details of how the topology is exactly shaped depend on
7.1 SpecialClassesofGraphs
the specific question. For most multi-prompt approaches,
Different schemes harness certain special classes of graphs.
theusercanadaptthetreetopologytoacertaindegree,i.e.,
Branch-Solve-Merge(BSM)[162]employsa1–leveldouble
byvaryingthebranchingfactor(i.e.,thenumberofthoughts
tree structure to first divide a problem into independently
generatedfromagivenvertex)andlimitingthedepthofthe
solvablesub-problems,andthencombinesthemintoafinal
tree.
solution.ThefirstpromptinstructstheLLMtoproposesub-
6.4.2 RepresentationsofTopology&Schedule problems, which are then solved independently. The final
prompt instructs the LLM to merge the results of the sub-
We showcase the differences between implicit vs. explicit
problemsintoasingleoutput.SocraticQuestioning[154]is
and single- vs. multi-prompt topologies using represen-
a scheme that models recursive exploration of the thought
tative examples (the tasks are described in detail in the
space using a tree structure. Hereby, the original question
promptsofthefigures).Wecontinuewiththedrivingtasks
is recursively decomposed into sub-tasks until all tasks
ofGameof24,CreatingWriting,andMathematical/Logical
can be solved with high confidence. These results are then
Reasoning. For this, we illustrate an implicit single-prompt
aggregated and propagated back up the tree to answer
tree topology elicited by AoT in Figure 10 as well as two
the original question. This results in an overall double tree
exampleexplicitmulti-prompttreetopologiesfromToTand
reasoningtopology.
CoT-SC in, respectively, Figure 11 and 12. Lastly, we show
anexampleofaparallelexecutionscheduleinFigure13for
SoT[148]. 7.2 DirectedGraphs
Some schemes embrace a general directed graph model.
6.4.3 Performance Graph of Thoughts (GoT) [10] uses a multi-prompt ap-
We now summarize performance patterns found within proach to improve the LLM problem solving performance
tree topologies. A detailed analysis can be found in Ap- by decomposing a given task into sub-tasks that form a
pendixE.2. graph. This decomposition is specified as a Graph of Opera-
Overall,increasingthebranchingfactor(i.e.,thenumber tions(GoO).TheGoOcoordinateshowtheLLMisprompted
of thoughts generated from a given vertex) often leads to and how the results are further used in the reasoning pro-
a higher diversity of outcomes, which can be beneficial cess.TheGraph of Thought[119]presentsamulti-prompt
for accuracy, but it also increases #prompts, i.e., compu- approach where a graph of thought is constructed recur-
tational cost. The most advantageous branching factor is sively in a DFS manner by starting at the question node that13
Game of 24
User Prompt LLM Answer: Implicit Tree Structure
Use numbers and basic arithmetic operations Trying a promising first operation:
(+ - * /) to obtain 24.
[in-context examples]
User: 1. 8 + 2: (14, 10, 8) Trying another promising first operation:
14 8 8 2 2. 8 / 2: (14, 8, 4)
Assistant:
LLM Answer
- 14 + 10: (24, 8) ... - 10 / 8: fractional - 14 + 8: (22, 4) - 14 - 8: (6, 4)
Trying a promising first operation:
1. 8 + 2: (14, 10, 8)
- 14 +10: (24, 8) 32, 16, 192, 3 ...
- 14 - 10: (8, 4) 12, 8, 32, 2 32 16 192 3 10 2 24 = 6 * 4 -> found it!
- 14 * 10: (140, 8) 148, 132, 1120, fractional
- 14 / 10: (8, 1.4) fractional
- 14 + 8: (22, 10) 32, 12, 220, fractional
- 14 - 8: (10, 6) 16, 4, 60, fractional The implicit tree structure is Step bystep reasoning in a CoT Backtracking the solution:
- 14 * 8: (112, 10) 122, 102, 1120, fractional created on the fly and traversed fashion is used for backtacking Step 1:
- 14 / 8: fractional in a DFS manner. Nodes are the solution. 8 / 2 = 4
- 10 + 8: (18, 14) 32, 4, 252, fractional constructed from left to right.
- 10 - 8: (14, 2) 16, 12, 28, 7
- 10 * 8: (80, 14) 94, 66, big, fractional
Step 2:
- 10 / 8: fractional
14 - 8 = 6
Trying another promising first operation:
2. 8 / 2: (14, 8, 4)
- 14 +8: (22, 4) 26, 18, 88, fractional
- 14 - 8: (6, 4) 10, 2, 24 = 6 * 4 -> found it! Step 3:
Backtracking the solution: 6 * 4 = 24
Step 1:
8 / 2 = 4
Step 2: Considering these steps: 24 = 6 * 4 = (14 - 8) * 4 = (14 - 8) * (8 / 2) = 24.
14 - 8 = 6
Step 3:
6 * 4 = 24
Considering these steps: 24 = 6 * 4 = (14 - 8) * 4
Answer: (14 - 8) * (8 / 2) = 24.
= (14 - 8) * (8 / 2) = 24.
Answer: (14 - 8) * (8 / 2) = 24.
Fig.10:GAMEOF24.Anexampleimplicitsingle-prompttreetopology,encodedwithtext.ItdemonstratesaGameof24DFSin-contextexamplefromAoT[166].The
leftviewshowstheuserpromptandthesingletextualanswerfromtheLLM.Therightviewshowstheimplicittreestructurethatisexploredduringthegeneration
oftheLLManswer.Wemarktextcorrespondingtoimplicitnodesasbold.
Creative Writing Math Reasoning
User User
Write a coherent passage of 4 short paragraphs. The end sentence of each paragraph [few-shot examples]
must be: 1. It isn't difficult to do a handstand if you just stand... 2. … 3. … 4. ... Q: Tobias is buying a new pair of shoes that costs $95. He has been saving up his money
each month […]?
Plan 1 Plan 2 Answer 1 Answer 2 Answer 3
...
1. Introduce and explain the 1.Introductiontoanunusual Let D be the number of Let D be the number of Let M be the number of
techn hi aq nu de s o taf nd do .ing a hs ae nlf d-h se talp n dbo ao sk a, m me en tati po hn oin rg f oa r Plan 2 is chosen. dri Hv ee w ha ay s s b T eo eb nia ss a s vh ino gv e ul ped. dr Hiv ee ew aa ry ns e dTo $b 5ia as l ls oh wo av ne cle ed. lawn bs e T to hb eia ns u m mo bw es r oa fnd D
2. … 3. … 4. ... embracing challenges. […] […] […]
2.…3.…4....
So the answer is 1. So the answer is 5. So the answer is 5.
...
Select final answer based on
Passage Passage Passage 2 is chosen. majority vote: So the answer is 5.
1 2
Fig. 12: MATH REASONING. An example using explicit multi-prompt tree
Fig.11:CREATIVEWRITING.Anexampleexplicitmulti-prompttreetopology, topology,encodedwithtext.Givenamathreasoningtask,CoT-SC[190]isused
encoded with text,fromtheTreeofThoughts(ToT)scheme[213]forcreative togeneratemultipleanswersandpickafinalonebasedonmajorityvote.Eachof
writing.Giventhetaskofwritingacoherentpassageoffourparagraphsending thegeneratedanswerscontainsmultipleCoTreasoningsteps,depictedhereina
ingivensentences,firstmultipleplans(nodes)aregeneratedandthenranked. singlenode.
Inanextstep,thebestplanisusedtogeneratemultiplepossiblepassagesas
outputs.Finally,thebestrankedpassageistheoutputoftheToTreasoning.
the initial question and passed again through the model
to predict an answer. Other schemes in this class include
represents the question to be answered by the LLM. From
Cumulative Reasoning [224], Everything of Thoughts
this node, possible reasoning paths are generated by the
(XoT)[57],ControlLLM[132],andResPrompt[99].
LLM.Foreachpath,newnodes,i.e.,intermediatereasoning
steps,aregeneratedbytheLLMandarethenusedtogrow
7.3 Hypergraphs
the graph. Graph-of-Thought [215] describes a two-stage
framework to answer multi-modal questions, i.e., textual Finally, we also consider a hypergraph, which generalizes
questions accompanied by images. In the first stage, the a graph by enabling edges to connect arbitrary subsets
model generates natural language rationales based on the of nodes instead of being links between just two nodes.
inputtext,whichprovideadditionalcontextandknowledge We include hypergraphs in the taxonomy, because prelimi-
to support answering the given question. This rationale nary works already harness them for multi-modal prompt-
generation is learned as part of the overall model pipeline. ing [212]. Here, Hypergraph-of-Thought (HoT) [212] is a
In the second stage, these rationales are then appended to multi-modalreasoningparadigmmodelingthethoughtpro-14
Question Answering
Skeleton Prompt User Input Point-Expanding Prompt
You are an organizer responsible for only giving Q: What are the most effective strategies for conflict You are responsible for continuing the writing of
the skeleton (not the full content) for answering resolution in the workplace? one and only one point in the overall answer to
the question. Provide the skeleton in a list of the following question.
points (numbered 1., 2., 3., etc.) to answer the Q: [...]
question. Instead of writing a full sentence, The skeleton of the answer is
each skeleton point should be very short with 1. Active listening
only 3~5 words. Generally, the skeleton should 1. Active Listening 1. Active listening involves fully 2. Identify issues
have 3~10 points. Now, please provide the 2. Identify issues concentrating on ... 3. Compromise
skeleton for the following question. 3. Compromise Continue and only continue the writing of point
2. Identify issues. Look into the 1. Write it **very shortly** in 1~2 sentence and
Q: [...] root causes of ... do not continue with other points!
Skeleton:
3. Compromise. Look for a
middle ground ...
1 2 3. .
.
A I Cdc oet mniv t pe if
r
y oL mi is st s ie su en ei sng F t eoi xr ps at an, nsa dw s eek dre t il ne h t e po aqn ru an e lo ls ed t lie ,o anis n. dg T e h tn e he n er , fia e nt ae acd lh ac p no o sn wit na eti rn o i ifn
s
t g dh ea e
r
l s ii vks et e dlo e f bt op yno i in st s 1 on. A ..c .tive listening involves fully concentrating
concatenating them.
Fig. 13: An example explicit multi-prompt tree topology, encoded with text. It demonstrates the automatically derived tree topology of Skeleton-of-Thought
(SoT)[148]wheretheindividualpointsareexpandedinparallel.
cess as a hypergraph. First, a graph-of-thoughts as in [215] Figure14showsapromptingexampleusingCumulative
is constructed. Then a textual hypergraph is constructed, Reasoning [224] for the Game of 24 with an explicit multi-
sharingthesamenodes.Thehyperedgesarethendefinedas prompt graph topology. Figure 15 shows an in-context ex-
node triples, e.g., ”(Lionel Messi, place of birth, Rosario)”. ample of ResPrompt [99] with a multi-step math question,
Additionally,avisualhypergraph-of-thoughtisconstructed where the topology is single-prompt and implicit. Here, an
byperformingk-meansclusteringonimagepatches,where (implicit)edgecanbeformedbyrepeatingthesametoken,
aclustercorrespondstoahyperedge.Bothhypergraphsare for example “earned from his allowance”, which implies a
thenencodedandcombinedtoperformgraphlearning. connection between step 2 and 4 where each step indi-
cates a node. Figure 16 illustrates an in-context example
7.4 Analysis&ComparisonofDesigns where Cumulative Reasoning [224] is applied to the same
We now broadly discuss and analyze graph designs with math problem with user-specified number of intermediate
respecttodifferentaspectsofourblueprint. nodesbeforereachingthefinalsolutionnode.Here,implicit
vertices are formed using numbered positions 1 and 2 on
7.4.1 Topology&ItsConstruction the list, and edges connect points 1 and 2 to point 3. In
Firstly, the considered schemes exhibit a blend of single- contrasttotheimplicitrepresentationoftopology,Figure17
and multi-prompt aspects, allowing for a high degree of shows an example from ControlLLM [132] to represent the
flexibility and control over the prompting process. This topologyexplicitlywithaJSONformat,butalsointhesingle-
is evident in the diverse approaches taken by different prompt setting. Finally, Figure 18 depicts a multi-prompt
schemes such as GoT, ControlLLM, and Cumulative Rea- example from Branch-Solve-Merge (BSM) [162] for story
soning, each offering unique ways of constructing and generation.
utilizing graphs for problem-solving. Secondly, the user’s
controloverthetopologyofthegraphissignificantinmost 7.4.3 Performance
schemes, enabling customization of the reasoning process The considered works universally show improvements
basedonspecificneeds,suchassettingbranchingfactorsor in effectiveness of graph-based prompting schemes over
definingthedepthofthegraph.Thirdly,theroleoftheLLM chains and trees across various tasks, suggesting a promis-
inthesegraph-basedschemesismultifaceted,involvingthe ingdirectionforfutureresearchandapplicationinthefield
generation, evaluation, and modification of nodes within ofAIandmachinelearning.
the graph, as well as determining the conclusion of the
reasoningprocess.Lastly,thereisanotablevariationinthe
8 CHAINS VS. TREES VS. GRAPHS OF THOUGHTS
degree of user and LLM influence on the topology across
different schemes, with some allowing direct user control, We also broadly discuss tradeoffs, commonalities, and dif-
while others rely on predefined heuristics or the LLM’s ferences between the three fundamental classes of topolo-
decision-makingcapabilities. gies:chains,trees,andgraphs.
Thenoveltyinchain-basedpromptingliesinintroducing
7.4.2 RepresentationsofTopology&Schedule explicit intermediate LLM thoughts between the input and
Wenowillustratearepresentativesetofpromptsthatshow theoutput.ThislinearsequenceofthoughtsguidestheLLM
different aspects of graph-based prompting topologies, fo- in a step-by-step manner towards the solution, enhancing
cusingonhowtherespectivegraphtopologiesareencoded theclarityandtraceabilityofthereasoningprocess.Beyond
inside the prompts (the tasks are described in detail in this, tree-based schemes bring the possibility to explore
the prompts of the figures). We illustrate ResPrompt and several next-step variants at each juncture, allowing the
Cumulative Reasoning (as example implicit single-prompt LLM to evaluate multiple pathways and select the most
representations),ControlLLM(asanexampleexplicitsingle- promisingone.Thisbranchingstructurefacilitatesabroader
promptrepresentation),andBranch-Solve-Merge(asanex- exploration of potential solutions. Graph-based schemes,
amplemulti-promptrepresentation). however, represent the most complex structure, offering an15
Game of 24
Propose Prompt
Suppose you are one of the greatest AI scientists, logicians and mathematicians. You are User Input
very good at basic arithmetic operations. Use numbers and basic arithmetic operations ...
14, 8, 8, 2
(+ - * /) to obtain 24 with input numbers. In each step, You are only allowed to
randomly choose arbitrary TWO of the input numbers to obtain a new number
using arbitrary one basic arithmetic operation (AVOID duplicating with
forbidden steps). Your calculation process must be correct.
...
[in-context examples] 12, 8, 8 4, 8, 14
Input: 14, 8, 8, 2
Forbidden Steps : [...]
Next Step:
14, 4 4, 6
14 - 2 = 12
Remaining Numbers:
12, 8, 8 (14 - 8) * (8 / 2) = 24
Verifier Prompts
Validate Step
Suppose you are one of the greatest AI scientists, logicians and mathematicians. You are
Reporter Prompt
very good at basic arithmetic operations. Use numbers and basic arithmetic operations
(+ - * /) to obtain 24 with input numbers. Evaluate if given intermediate step is Suppose you are one of the greatest AI scientists, logicians and
correct and only use two existing numbers. mathematicians. You are very good at basic arithmetic operations.
[in-context examples] Use numbers and basic arithmetic operations (+ - * /) to obtain 24
with input numbers. You need to combine the given
Input: 14, 8, 8, 2 intermediate steps step by step into a complete expression.
Intermediate step: 14 - 2 = 12 [in-context examples]
The intermediate step is valid. Input: 14, 8, 8, 2
Judgment: Intermediate steps:
Valid 8 / 2 = 4 (left 4, 8, 14)
14 - 8 = 6 (left 4, 6)
Verify Step 6 * 4 = 24
Suppose you are one of the greatest AI scientists, logicians and mathematicians. You are Draft:
very good at basic arithmetic operations. Use numbers and basic arithmetic operations
(+ - * /) to obtain 24 with input numbers. Evaluate if given numbers can reach 24 Because 6 * 4 = 24, while 6 = 14 - 8. So (14 - 8) * 4 = 24.
(sure/likely/impossible) Because (14 - 8) * 4 = 24, while 4 = 8 / 2. So (14 - 8) * (8 / 2) = 24.
[in-context examples] Output:
(14 - 8) * (8 / 2) = 24
Input: 12, 8, 8
Draft:
Starting with the user input node, one valid node is selected at random in each iteration.
(12 - 8) * 8 = 32 Each edge in the graph represents multiple interactions with the LLM, depicted on the
12 + 8 + 8 = 28 left. First a next step is proposed. Then this step is validated and verified, judging if this
8 / 8 + 12 = 13 can lead to the desired solution or if it is a dead end (dashed nodes). Opposed to ToT,
I cannot obtain 24 now, and numbers are not within a reasonable range these invalid steps are kept track of and are used in the next propose prompt as
Output: "forbidden steps", so directly influencing the next node (dashed arrows). If the validation
impossible output is "sure", the Reporter is invoked to summarise the steps and derive the final
result as shown in the reporter prompt.
(12, 8, 8) is added to forbidden steps.
Fig.14:GAMEOF24.Anexampleexplicitmulti-promptgraphtopology,encodedwithtextusingCumulativeReasoning[224].
Math Reasoning
User Prompt LLM Answer: Implicit Tree Structure
[in-context examples] 1. He has $15 left after buying the shoes that cost $95, which means he had $95 + $15 = $110 in total
Q: Tobias is buying a new pair of shoes that costs $95. before the purchase.
He has been saving up his money each month […]?
LLM Answer 2. Next, we calculate the amount he earned from his allowance. He has been saving for three
months, getting $5 each month, [...]
1. He has $15 left after buying the shoes that cost $95, which
means he had $95 + $15 = $110 in total before the purchase.
2. Next, we calculate the amount he earned from his 3. Then we know he mowed 4 lawns and he charges $15 per lawn, so from
allowance. He has been saving for three months, getting $5 mowing lawns, he earned 4 * $15 = $60.
each month, [...]
3. Then we know he mowed 4 lawns and he charges $15 per
lawn, so from mowing lawns, he earned 4 * $15 = $60. 4. Let us sum up the money he earned from his allowance and from mowing lawns: $15
(allowance) + $60 (mowing) = $75
4. Let us sum up the money he earned from his allowance and
from mowing lawns: $15 (allowance) + $60 (mowing) = $75.
5. Now, let us subtract the total amount of money earned from 5. Now, let us subtract the total amount of money earned from allowance and mowing from his total
allowance and mowing from his total amount of money before amount of money before buying the shoes to [...].
buying the shoes to [...].
6. As he charges $7 to shovel a driveway, we divide the total
earned from shoveling by the charge per driveway to find out 6. As he charges $7 to shovel a driveway, we divide the total earned from shoveling by the charge per
the number of driveways he shoveled: $35 / $7 = 5 driveways. driveway to find out the number of driveways he shoveled: $35 / $7 = 5 driveways.
Fig.15: MATHREASONING. An example implicit single-prompt graph topology, encoded with text.Itshowsanin-contextexampleofamathquestionfrom
ResPrompt[99],representingalinearsequenceofsixconnectednodeswithtwoimplicitedgesofthegraphtopology,markedwithtwodifferentcolors(redandblue),
togetherwiththeircorrespondingnodes.16
Math Reasoning
Propose Prompt User Input
Suppose you are one of the greatest AI scientists, logicians and mathematicians. Let us Premises:
think step by step. Please use First-Order Logic (FOL) to deduce a "Proposition" 1. Tobias is buying a new pair of shoes that costs $95.
from two given "Premises". Please make sure that the "Proposition" is logically […]
correct. Please make sure that the "Proposition" is not a duplicate of the Question: How many driveways did he shovel?
"Premises". Please make sure your reasoning is directly deduced from the
"Premises" and "Propositions" other than introducing unsourced common
knowledge and unsourced information by common sense reasoning.
[in-context examples]
He had $75 from saving
Premises: [...] He had $110 in total the last three months and
Question: [...] before buying the shoes. mowing lawns.
He had $110 in total before buying the shoes.
Verifier Prompts befH oe re h ba ud y $ in9 g5 ti hn e t o shta ol e s. ... sH he o ve ea lr inn ged d $ ri3 v5 e wfr ao ym s.
Validate Step
Suppose you are one of the greatest AI scientists, logicians and mathematicians. Let us
think step by step. Please determine whether the "Proposition" is directly
deduced from the "Premises" with certainty other than introducing unsourced
information by common sense reasoning, reply with True or False. We know that he earned $35 from shoveling, and
[in-context examples] he charges $7 for shoveling. Therefore, the
number of driveways he shoveled is $35 / $7 = 5.
Premises: [...]
Proposition: He had $95 in total before buying the shoes.
Judgment:
False
Verify Step Reporter Prompt
Suppose you are one of the greatest AI scientists, logicians and mathematicians. Let us
think step by step. Please use First-Order Logic (FOL) to determine whether the Suppose you are one of the greatest AI scientists, logicians and mathematicians. Let
deduction of two given "Premises" to a "Proposition" is valid or not, reply with us think step by step. Read and analyze the "Premises" first, then using First-
True or False. Order Logic (FOL) to answer the "Question". Please make sure your
[in-context examples] reasoning is directly deduced from the "Premises" and "Propositions" other
than introducing unsourced common knowledge and unsourced
Premises: [...] information by common sense reasoning.
Proposition: He had $95 in total before buying the shoes. [in-context examples]
Premises:
Judgment: 1. Tobias is buying a new pair of shoes that costs $95.
False 2. He has been saving up his money each month
[…]
The proposition is not added to the valid node list. Question: How many driveways did he shovel?
Generated Propositions:
1. He had $110 in total before buying the shoes.
2. He had $75 from saving the last three months and mowing lawns.
Each premiseis a valid node at the begining. In each iteration five valid nodes are selected at 3. Using 1 and 2, we know that he earned $35 from shoveling driveways.
random to generate a proposition. Each edge in the graph represents multiple interactions with Reasoning:
the LLM, depicted on the left. Each proposition is validated and verified before adding it to the
valid node list, judging if this is deducted from valid nodes and verified based on FOL or if it is a We know that he earned $35 from shoveling, and he charges $7 for shoveling. Therefore,
dead end (dashed nodes). After having a predefined number of propositions, the Reporter is the number of driveways he shoveled is $35 / $7 = 5.
invoked to answer the question with all valid nodes.
Fig.16:MATHREASONING.Anexampleimplicitsingle-promptgraphtopology,encodedwithtext,forthesamequestionasinFigure15,butusingCumulative
Reasoning[224].Itillustratesthreeintermediatepropositionnodeswithdirectconnectionsfromthefirsttwonodestothethirdone.Thereasoningpartindicatesthe
finalsolutionnode,withdirectconnectionsfromthethirdpropositionandinputnodes.
arbitrary reasoning framework. They enable the aggrega- abilitytoexploreawiderrangeofpotentialtransformations
tion of various reasoning steps into a synergistic solution, at each step, going beyond the linear refinement seen in
allowing for non-linear and multifaceted problem-solving chains. Tree-based schemes, with their branching paths,
approaches.Thisdiversityinstructuresreflectsthevarying offermultipleavenuesforsolutionexplorationateachdeci-
needs and complexities of tasks that LLMs are expected to sion point, leading to a more comprehensive search for the
handle. optimaloutcome.Graph-basedschemestakethisastepfur-
In terms of cost-effectiveness, chain-based prompting therbyallowingforanevenmorediversesetofconnections
schemes generally outperform their tree-based and graph- and interactions between different steps in the reasoning
basedcounterparts.Thisefficiencylargelystemsfromtheir process. This flexibility enables these schemes to handle
single-prompt nature, which requires fewer computational morecomplexandmultifacetedtaskseffectively,leadingto
resources and less processing time. Tree and graph-based higher-quality outcomes in scenarios where simple linear
schemes, on the other hand, often involve multiple rounds reasoningisinsufficient.
ofpromptingtoexplorethevariousbranchesorconnections
in their respective structures, naturally incurring higher
costs. However, this is not an inherent limitation of these
schemes. Future developments in tree and graph-based 9 DESIGN ARCHITECTURES
prompting should aim to better encode the tree and graph
structureswithinasingleprompt.Suchadvancementscould We also analyze design tradeoffs. We consider the follow-
potentially combine the cost-effectiveness of single-prompt ingaspects:designarchitecture(whatthearchitecture-level
schemeswiththeenhancedqualityandcomplexitymanage- decomposition of different schemes into interacting mod-
mentofferedbytreeandgraph-basedstructures. ules is), productivity & programmability (how well differ-
Whenitcomestothequalityofoutcomes,tree-basedand ent schemes support efforts such as new extensions), and
graph-based prompting schemes typically surpass chain- scalability as well as parallelizability (whether considered
basedones.Thissuperiorperformanceisattributedtotheir schemescanbeparallelizedandwhethertheyscalewell).17
JSON erator, an evaluator, a halter, and a controller module. The
generator prompts the LLM to produce further reasoning
User Task
steps given the current reasoning step and some context.
Please dub the video.mp4 with a suitable background music and then generate a HTML
The evaluator rates the current reasoning step, depending
web page to introduce this video.
on the implementation this rating can depend on other
LLM Task Decomposition Answer states, the path to the initial question node, or some other
[{ context. The halter determines if a suitable solution has
"description": "Dub the video with a background music",
"task": [ "video-editing"], been found and how it should be reported or if the search
"id": 1,
"args": { should continue. The controller module coordinates the
"type": "video",
"value": "video.mp4" }, other modules as well as the construction and exploration
"returns": {
"type": "video", ofthetree/graph.
"value": "<GEN>-1" }
},
9.2 Productivity&Programmability
ToT by Yao et al. [213], Tree Prompting [170] and SoT [148]
{
provide implementations that can be used directly for cus-
"description": "Generate a HTML page to introduce the video",
"task": [ "HTML-generation"], tom tasks; ToT [213] is the only tree approach that pro-
"id": 2,
"dep": 1, vides an easily usable API to solve custom problems in
"args": {
"type": "video", a multi-prompt fashion. ToT by Long [133], Thought De-
"value": "<GEN>-1" },
composition [205] and ToC [106] provide implementations
"returns": {
"type": "HTML", to reproduce their results, but can not easily be used for
"value": "<GEN>-2" }
}] other tasks. CoT-SC [190] is a paradigm and can easily be
implemented when needed, AoT [166] is a single-prompt
Fig.17: JSON. An example explicit single-prompt graph topology, encoded scheme and does not provide an implementation but lists
with JSON,basedontheControlLLMscheme[132]fortaskdecomposition.It
in-context examples. The remaining approaches [50], [58],
showstwonodesdescribingdecomposedsubtasksforsolvingagiventask.The
”dep”fieldreferstodependenttasks,showingthereisadirectedgefromthefirst [91],[145]havenocodepublishedatthistime.
node(task1)tothesecond.
In graph designs, Socratic Questioning [154] and Graph
ofThoughts(GoT)[10]provideimplementationsthatcanbe
Creative Writing
useddirectlyforcustomtasks.CumulativeReasoning[224]
User provides an implementation to reproduce their results, but
shirt, dunk, bang, soap, court, float, swimmer, coat, sleigh, groom cannot easily be used for other tasks. Thought Propaga-
tion (TP) [218], Branch-Solve-Merge (BSM) [162], Control-
Branch 1 Branch 2
... LLM [132] and ResPrompt [99] do not provide implemen-
shirt, dun ck o, ub ra tng, soap, a [ ws it no tr ey r wto ep di dc i] ng float s, l es iw ghim , m gre or o, m coat, tation but list a set of prompting examples. Among these,
adventure
ResPrompt [99] is a paradigm can be easily implemented
Story 1 Story 2 when needed. The remaining approaches [57], [119], [212],
they were greeted by a swimmer, … The [215]havenocodepublishedatthistime.
a loud bang echoed …, dunking his shirt
groom, a skilled float operator, …
in a bucket of soap and water, and then dressed in matching red coats, rode on a Overall, GoT [10] offers the most general API for ad-
using it to clean the court. …
colorful sleigh, …
dressing custom tasks in a multi-prompt fashion. It of-
Generated Story fers a principled way to design a prompting structure,
through thought transformations, i.e., ways to transform
a loud bang echoed through the church, … dunking his shirt in a bucket of soap and water,
and then using it to clean the court. … Meanwhile, outside, the groom, a skilled float GoT nodes into new outcomes. Example GoT API rou-
operator, … , rode on a colorful sleigh, … a swimmer, ..., black coat, gracefully …
tines, which implement thought transformations include
– for example Generate(prompt, k=4) (prompt the LLM to
Fig.18: CREATIVEWRITING. An example multi-prompt graph topology, en-
codedwithtext,fromtheBranch-Solve-Mergescheme[162]forstorygeneration produce k answers to a given prompt), Generate(prompt,
withbranch,solve,andmergeprompts.Givenalistofconceptsasinput,the k=1)+Repeat(k=4) (generate 4 context isolated responses
branch module generates three child nodes: two groups of concepts and one
of the LLM), Aggregate(thought1, thought2) (combine two
topicnode.Thesolvemodulethencreatestwostorynodesbasedoneachgroup
ofconceptsandthetopic.Finally,thesetwostorynodesaremergedintothefinal thoughts), KeepBest(N=1) (query the LLM to return N best
solutionnode.
results), Improve(thought) (query the LLM to improve the
result using information from another thought, e.g., input
9.1 DesignArchitecture list and currently sorted list → try to fix incorrectly sorted
elements).
Here,weanalyzehowtheoverallpromptingscheme,com-
biningtheLLMandsomeadditionallogic,decomposesinto
9.3 Scalability&Parallelizability
different modules. The module-level design architecture
is explicitly discussed to a certain degree by some tree- Only a few works address scalability and parallelizability
based approaches, such as [50], [91], [133], [145], [213] as improvementsofLLMs.SkeletonofThought[148]usesthe
well as graph-based approaches such as [10], [119], [154], ToT approach: the prompt query is a tree of depth one and
[162], [218], [224]. Detailed module-level architectures are all leaves can be processed in parallel. Batch Prompt [124]
presentedbyLong[133],Yaoetal.[213],andBestaetal[10]. improvestheperformancebybatchingdifferentdatapoints
The general architecture of considered schemes can be into a single prompt. On the other hand, Lookahead De-
summarized as consisting of the following modules: a gen- coding [65] is focused on speculatively guessing tokens to18
speed up the answer generation. We now discuss in more LLMs are able to generate a coherent chain of thoughts.
detailhowtheseconceptsarerealized. Their model generalizes Jiang’s latent space theory of lan-
Skeleton of Thought [148] tackles the problem of high guagemodels[98]tochainsofthoughts.Inthelatentspace
inference latency in LLMs by addressing the sequential theory, thoughts convey a hidden intention. Because the
decodingproblem(generatinganswertokensoneatatime). intentionisnotdirectlyobserved–onlytheutteredthought
They proposed a new approach where a skeleton prompt – there is a potential for ambiguity. This ambiguity ρ can
leads the LLM to first generate a succinct list ofpoints that be quantified probabilistically for the language as a whole.
form the skeleton of its future answer. Then, each point in Tutonovetal.extendJiang’smodelbyaddinganadditional
theskeletonisdecodedinparallel,usingbatchingwhenever hidden context variable upon which intentions are condi-
possible. The method does not require changes to LLMs tioned. This context c defines a family of tasks which each
and can be applied to off-the-shelf models. with reported defineasetofcoherentreasoningsteps.Moreover,Tutonov
speedups of up to 2.69×. However, this method ignores et al. generalize the one-shot generation of thoughts into
dependencies between points in the skeleton answer, and a chain of thoughts with an associated chain of intentions.
the authors propose replacing the tree with a graph model Every intention is conditioned on the context c and the
asfuturework. previousintentions.Inturn,thei-ththoughtinthechainis
LookaheadDecoding[65]extendstheideaofspeculative conditionedonthei-thintention.Theirmainresultisasfol-
decoding where a sequence of future tokens is predicted, lows:conditionedontheinputandasequenceofN example
and the LLM later verifies each one of them in parallel. chainofthoughtsgeneratedfromacontextc,theprobability
Thismethodrepresentsthesequentialgenerationofachain that an LLM assigns to a chain of thoughts approaches the
of responses as a non-linear system of equations and then trueprobabilityofthechaingiventheinputandthehidden
solves it with the Jacobi iteration method. In each iteration contextcwithconvergencespeedρN.Thismeansthat,with
step, at least one guessed token - the next one - is verified the appropriate examples, LLMs can generate a chain of
and matched successfully. Furthermore, the method caches thoughtsthatisarbitrarilyclosetothetrueoutput.
n-grams generated for each token by inspecting N prior Bestaetal.[10]discusstradeoffsbetweenlatency(num-
Jacobi iterations, allowing it to decode multiple tokens on ber of steps to reach the final thought) and volume, which
apositivematch.Byconductingthelookaheadandverifica- theydefine–foragiventhoughtt–asthenumberofpreceding
tioninparallel,authorsachievedecodingspeedupbyupto LLM thoughts that could have impacted thought t. Formally,
2.25×,atthecostofanexponentialincreaseinGPUFLOPs. it is the number of thoughts in the topology from which
BatchPrompt [124] is a scheme that batches similar there exists a path to thought t. They assume a time of
questions into a single prompt to reduce the overhead of O(1) for each reasoning step, fix the total cost for each
taskdescriptionandfew-shotexamplescomparedtosingle- topology in their analysis to Θ(n) and also make certain
questionprompting.However,naivebatchingdecreasesthe assumptions for each topology. For a single chain both the
overall LLM performance, and the final accuracy depends latency and the volume have a high value of N, which for
on the position of each single prompt within the batch, multiple chains (k independent chains) is reduced by k for
whichmaybechallengingtotune.BatchPromptavoidsthis both metrics (N/k). In their tree analysis they assume a
problembyperformingmajorityvotingonmultiplerounds complete k-ary tree, which has a low latency of ⌈log N⌉,
k
of prompting with different permutations of the ques- but the volume is similarly low (O(log N)). For graphs,
k
tion, allowing for competitive results compared to single- they assume two complete k-ary trees, where the first tree,
questionprompting.Byusinglargebatcheswithfewvoting representing the division into sub-tasks, is joined at the
rounds, the scheme can process queries with significantly leaf level with another “reversed” k-ary tree (including its
fewercallstotheLLM. edges), representing the aggregation into the final solution.
They conclude that double-tree graphs provide the best
tradeoffwithalatencyoflog N andavolumeofN.
10 FOUNDATIONS & THEORY k
Several works, while not providing any theoretical un-
There are a few preliminary works that attempt to provide derpinning for structured-enhanced prompting, investigate
foundationsforstructuredprompting. the capabilities for CoT to conduct formal analyses. This
Madaan and Yazdanbakhsh [141] focus on dissecting includes GSM8K [48], ProofWriter [179], FOLIO [82], Sim-
CoT into three basic components, namely symbols (se- pleLogic[221],andPrOntoQA[164].
quences of tokens used as the basis of the LLM reasoning, Finally, there have been several works that investigate
for example numbers to be sorted), patterns (the structure thetheoreticalunderpinningofgeneralin-contextlearning.
withinpromptthatreinforcestaskunderstanding,forexam- This includes Xie et al. [204], Wies et al. [200], Hahn and
pletheorderofnumberstobesorted),andtext(anytokens Goyal[80],andJiang[98].However,astheydonotfocuson
thatareneitherasymbolnorapattern).Here,“patterns”is thetopologiesofprompting,detailsareoutsidethefocusof
effectively a certain single-prompt topology that enhances ourwork.
theLLMreasoning.Theauthorsdiscoverthat,forexample,
patternsarerelevantforbettertaskunderstanding,andthey
11 RESEARCH OPPORTUNITIES
form a symbiotic relationship with text: the latter help to
createmoreusefulpatternswhiletheformerenabletheLLM We now review future research directions in structure-
togeneratetextthathelpsintasksolving. enhancedprompting.
Tutunov et al. [185] use probabilistic graphical models Exploring New Topology Classes An intriguing re-
to provide a theoretical understanding of how and why lated approach is the study of novel topology classes, such19
as hypergraphs, in prompting schemes. The existing HoT harnessed,amongothers,generaldirectedgraphs,utilizing
scheme [212] primarily uses triples, barely scratching the specializedonescouldleadtobetterreasoningroutines.One
surfaceofwhathypergraphscanofferintermsoftheircon- couldharness,forexample,structuresbehindlow-diameter
nectivity structure, where hyperdges can connect arbitrary networks [19], [20], [96], [107], [108], [116], [117], [118] for
subsetsofvertices.Futureresearchcoulddelveintohowhy- potential gains in theoretical properties of LLM reasoning,
pergraphscanaccommodatemorecomplexrelationshipsin suchaslowerlatency.
datasuchasmotifs[18],densesubgraphs[30],cliques[25], Integration with Graph Algorithms and Paradigms
[74],[174],andothers,potentiallyleadingtobreakthroughs Integrating graph-related algorithms and paradigms could
inLLM’sunderstandingandreasoningcapabilities. offer more powerful representations and schedules in LLM
ExplicitRepresentationsinSingle-PromptSettingsThe prompting. This integration might involve the use of ad-
explicit representation of prompting topologies has been vanced graph algorithms to optimize the structure and
largely unexplored, especially in scenarios involving single flowofthepromptingprocess[11],[13],potentiallyleading
prompts.Researchinthisareacouldfocusonhowdifferent to more accurate and efficient outcomes. Research could
representations,suchasAdjacencyLists,AdjacencyMatrix, explore how such different graph paradigms [26] can be
or numerous others [29], can be employed to enhance the adaptedtotheuniquerequirementsofLLMprompting.
efficiency and effectiveness of LLMs. This exploration is DiversifyingModalitiesinPromptingDifferentmodal-
vital, considering the impact of different data structures on ities in prompting, such as visual, auditory, or kinesthetic,
the processing and interpretation of information by LLMs. are currently underexplored. Research in this area could
The challenge lies in integrating these complex representa- involve developing multi-modal prompting systems that
tionsintoasingle-promptformatwithoutcompromisingthe can understand and respond to inputs in various forms
simplicityandaccessibilityofthemodels. while harnessing the advantages of graphs or trees of
Automatic Derivation of Tree and Graph Topologies thoughts.Thisdiversificationcouldleadtomoreinteractive
The majority of current tree and graph topologies in LLM and inclusive LLM systems that cater to a wider range of
prompting are created manually or semi-automatically. A usersandusecases.
promisingresearchdirectionisthedevelopmentofmethod- EnhancingRetrievalinPromptingRetrievalinprompt-
ologies for the automatic derivation of these topologies. ing is another area that has received certain attention, with
Automatingthisprocesscouldsignificantlyreducethetime various recent schemes [6], [7], [40], [68], [100], [106], [191],
and effort required to configure LLMs for specific tasks, [199],[219].Improvingretrievalmechanismscouldenhance
makingthemmoreaccessibleandefficient.Thisautomation the LLM’s ability to access and utilize relevant informa-
might involve leveraging machine learning algorithms to tion more efficiently. This might involve developing more
identifyandimplementthemosteffectivetopologiesbased sophisticated algorithms for data retrieval or integrating
onthenatureofthetaskandthedatainvolved. external databases and knowledge bases [4], [5], [15], [16],
Advancements in Single-Prompt Schemes While there [53]toexpandthescopeanddepthoftheLLM’sresponses.
are a few examples of single-prompt schemes, such as Parallel Design in Prompting The aspect of parallel
the initial Chain-of-Thought (CoT), there’s much room for designinruntimepromptingisanareathatremainsmostly
improvement.Futureresearchcouldfocusonhowtoencap- unaddressed, and only a few schemes such as Skeleton-
sulatemorecomplexanddetailedtreeandgraphstructures of-Thought address this challenge. Building upon these
within a single prompt, because it could reduce computa- attempts could significantly enhance the speed and effi-
tionalcostsandsimplifyuserinteractionwithLLMs(asone ciencyofLLMs.Researchcouldfocusondevelopingmodels
does not need multiple prompting interactions to build a that can simultaneously process multiple components of
given topology). The challenge lies in balancing the rich- a prompt or handle various tasks in parallel, or the ap-
ness of the prompt with the need to maintain clarity and propriate mapping to massively parallel architectures [19],
avoid overwhelming the model. Here, one could harness a [28], [73], [95], thereby reducing latency and improving
recent line of works related to encoding graph structures user experience. One could also investigate effective inte-
withinaprompt,suchasGPT4Graph[79],GraphText[229], gration of prompting with distributed-memory infrastruc-
GraphGPT [181], LLMs-as-Predictors [44], and others [36], ture and paradigms, such as remote direct memory access
[61], [92], [94], [127], [155], [186], [187], [216], [222], [223], (RDMA)[21],[55],[56],[70]orserverlessprocessing[49].
[227]. IntegratingStructure-EnhancedPromptingwithGraph
Investigating New Scheduling Approaches Most cur- NeuralNetworksApotentialareaofexplorationistheinte-
rent structure-enhanced prompting schemes rely on stan- gration of structure-enhanced prompting with Graph Neu-
dardschedulingalgorithmslikeBreadth-FirstSearch(BFS), ralNetworks(GNNs)[22],[27],[37],[111],[203],[225],[234]
Depth-FirstSearch(DFS),oraremanuallydesigned.Explor- and other mechanisms for Graph Machine Learning [12],
ingnewschedulingtechniquescouldleadtomoreefficient [33], [81]. GNNs, known for their proficiency in handling
and effective processing of prompts. This could include relational data and capturing dependencies in graph struc-
adaptive scheduling algorithms that adjust their approach tures, could augment the capabilities of LLMs in process-
basedonthenatureofthetaskortheresponsesoftheLLM, ing complex, structured prompts. By embedding prompt-
potentially enhancing the model’s performance in complex ing structures into graph-based representations, GNNs can
reasoningtasks. provide a more nuanced and context-aware interpretation
Investigating Novel Graph ClassesAninterestingidea of the prompts, potentially leading to richer and more
is to explore graph classes for more effective and more effi- accurate responses. Furthermore, leveraging graph-related
cient reasoning topologies. While current approaches have embeddings [12], [75] can enhance the LLMs’ ability to20
capture the subtleties in the relationships and hierarchies (andotherstructures)asmechanismsenablingstructuredLLM
present in the prompts. Finally, harnessing heterogeneous reasoning[93].
GNNs[139],[178],[220]forintegrationwiththeconceptof GeneraloverviewsofusinggraphstogetherwithLLMs
different semantic roles of vertices could be an interesting have been outlines by Li et al. [122] and Zhang et al. [226].
direction. Panetal.[150]verybrieflymentionsomeofthemethodsfor
Integrating Structure-Enhanced Prompting with Com- integrating graphs with LLMs. Zong et al. briefly mention
plex System ArchitecturesAnessentialandemergingarea graph-relatedfusionintheirworkonself-supervisedmulti-
of research is the integration of prompting capabilities into modal learning [236]. Yang et al. [208] analyze data-centric
the environment of complex existing system architectures, graphlearning.
such as graph representation learning systems [63], [121], Next, various works propose to enhance general gen-
[189],graphdatabases[14],[17],[23],orrelationaldatabases. erative models with knowledge graphs (KGs). The focus
This integration aims to facilitate direct and nuanced in- of these works is usually to use KGs in order to enhance
teractions with complex data structures stored in these theLLManswers,forexamplebygroundingknowledgein
systems.ByembeddingLLMsintotheseenvironments,the generalmodelstoreduceeffectssuchashallucinations[90],
promptingprocesscanleveragetheinherentorganizational [149], [196], [209], [210]. Example schemes include Knowl-
andrelationalcapabilitiesofthesedatabases.Thisapproach edgeGraphPrompting(KGP)[192],GraphNeuralPrompt-
would allow LLMs to access, interpret, and manipulate ing (GNP) [183], Think-on-Graph (ToG) [176], Knowledge
largeandintricatedatasetsmoreefficientlyandaccurately. Solver(KSL)[62],KnowledGPT[191],andothers[32],[138].
Forinstance,integratingwithgraphdatabasescouldenable Zhu et al. [235] discuss how LLMs can be used for en-
LLMs to naturally understand and utilize the connections hancingKGconstructionandtasks.Wenetal.[197]present
and relationships within data, while relational databases MindMap, a framework to perform reasoning on KG data.
could provide a structured and queryable data format that Pertinent triples from a KG are retrieved and the LLM is
complementstheLLMs’linguisticcapabilities. prompted to answer a question based on these triples and
Hardware AccelerationUnderstandingenergyandper- showthereasoningprocessbygeneratinga“mindmap”in
formancebottlenecksandmitigatingthemwithspecialized theformofatextualreasoningtree.
techniques such as processing-in-memory [1], [24], [72], Several works discuss graph foundation models [66],
[146], [147], [167], FPGAs [28], [54], [144], or even quan- [129]. These works discuss how to – in analogy to LLMs –
tum devices [9] will likely be increasingly important. Such devise general models pre-trained on massive amounts of
advances can also enable much more scalable models and graph data, that could then be fine-tuned for more specific
modelexecutionunderstringentconditions. applications,andusedtogetherwithpromptingforanswer-
ingvariousgraph-relatedtasks.
Recents schemes illustrate how to run analytics
12 RELATED WORK
on graphs using prompting [101]; examples include
We also broadly discuss related analyses, taxonomies, and GPT4Graph [79], GraphText [229], GraphGPT [181], LLMs-
surveys. as-Predictors[44],andothers[36],[61],[92],[94],[127],[155],
[186],[187],[216],[222],[223],[227].
Finally, several works [177], [202] describe techniques
12.1 GeneralPromptEngineering
for graph prompt learning, which is a class of approaches
There exist several detailed overviews of general prompt for enhanced prompting with graph pre-trained models.
engineering. The main difference is that we provide the This class of schemes is orthogonal to our work, because it
firsttaxonomyandanalysisofstructure-enhancedprompting focuses on prompting for graph-oriented tasks, and it does
methods, in which we focus on the topology of the LLM nottargettheLLMstructureofreasoning.
reasoning. Wang et al. [194] provide an overview of inter-
active NLP, in which they outline interactions of LLMs
13 CONCLUSION
and humans, knowledge bases, models/tools, and envi-
ronments. Gu et al. [77] provide a systematic survey of In conclusion, the rise of Large Language Models (LLMs)
promptengineeringonvision-languagefoundationmodels. has revolutionized machine learning, extending their ap-
Liu et al. [130] describe in great detail general prompting plicationsbeyondNaturalLanguageProcessing(NLP)into
and describe this area through the “pre-train, prompt, and diversefieldslikemedicine,logicalreasoning,andplanning.
predict”paradigmofbuildingapromptingscheme.Qiaoet Prompt engineering has emerged as a crucial area, democ-
al.[156]overviewpromptingschemesrelatedtoreasoning. ratizing access to LLMs and offering a cost-effective alter-
Chen et al. [38] review the potential of prompting with nativetofine-tuningandpre-training.However,challenges
LLMs.Finally,Zhangetal.[228]andChuetal.[47]review arise in optimizing LLM queries for complex tasks due to
CoTandtheassociatedpromptingschemes. theinherentlimitationsofgenerativeTransformermodels.
This paper addresses these challenges by introducing
a blueprint and an accompanying taxonomy of prompting
12.2 Graph-RelatedGenerativeAI
schemes,focusingontheunderlyingstructureofreasoning.
TherehavealsobeennumerousworksongraphsandLLMs. We propose to model a general prompting scheme as a
The key difference is that these works focus mostly on graph topology, where different classes of graphs, such
using graph structures as input data (at pre-training, fine- as k–ary trees or directed graphs, can be used to reflect
tuning, or prompting stage). We instead focus on graphs the structure and gain insights into different prompting21
schemes.Thetaxonomyisthenusedtosurveyandanalyze [8] J.Austin,A.Odena,M.Nye,M.Bosma,H.Michalewski,D.Do-
existing designs, dissecting them into fundamental aspects han, E. Jiang, C. Cai, M. Terry, Q. Le, and C. Sutton. Program
SynthesiswithLargeLanguageModels,2021. arXiv:2108.07732.
such as the representation of the reasoning topology, the
[9] K.Bertels,A.Sarkar,A.Krol,R.Budhrani,J.Samadi,E.Geoffroy,
derivationofthetopology,ortheencodingofthereasoning J.Matos,R.Abreu,G.Gielen,andI.Ashraf.QuantumAccelerator
schedule.Thistaxonomyformsablueprintthatcanbeused Stack:AResearchRoadmap,2021. arXiv:2102.02035.
tofacilitatedesigningmoreeffectivepromptingschemes. [10] M.Besta,N.Blach,A.Kubicek,R.Gerstenberger,L.Gianinazzi,
J.Gajda,T.Lehmann,M.Podstawski,H.Niewiadomski,P.Ny-
We also conduct an analysis of structure-enhanced
czyk, and T. Hoefler. Graph of Thoughts: Solving Elaborate
prompting methods in terms of their accuracy and quality Problems with Large Language Models. In Proceedings of the
of outcomes, latency, and cost-effectiveness. Our investi- Thirty-Eigth AAAI Conference on Artificial Intelligence, AAAI ’24,
Vancouver,Canada,Feb2024.AAAIPress.
gation results in different insights into the tradeoffs be-
[11] M.Besta,A.Carigiet,K.Janda,Z.Vonarburg-Shmaria,L.Giani-
tweenpromptingschemes,whichfacilitateselectingthebest
nazzi,andT.Hoefler.High-PerformanceParallelGraphColoring
methodforagivenbudgetorworkloadtarget. withStrongGuaranteesonWork,Depth,andQuality.InProceed-
Furthermore,weinvestigatethepreliminaryworksinto ingsoftheInternationalConferenceforHighPerformanceComputing,
Networking,StorageandAnalysis,SC’20,Atlanta,Georgia,2020.
foundations of structured-enhancing prompting, parallel
IEEEPress.
and scalable designs, and productivity and programmabil- [12] M. Besta, A. C. Catarino, L. Gianinazzi, N. Blach, P. Nyczyk,
ity. We also provide valuable insights into open challenges H.Niewiadomski,andT.Hoefler. HOT:Higher-OrderDynamic
and potential research directions, navigating the path for GraphRepresentationLearningwithEfficientTransformers. In
Proceedings of the Learning on Graphs Conference, LOG ’23, Nov
futureresearchavenuesintomoreadvancedprompting.
2023.
[13] M.Besta,M.Fischer,T.Ben-Nun,D.Stanojevic,J.DeFineLicht,
and T. Hoefler. Substream-Centric Maximum Matchings on
ACKNOWLEDGEMENTS FPGA. ACMTrans.ReconfigurableTechnol.Syst.,13(2),Apr2020.
[14] M. Besta, M. Fischer, V. Kalavri, M. Kapralov, and T. Hoefler.
We thank Hussein Harake, Colin McMurtrie, Mark Klein, An- PracticeofStreamingProcessingofDynamicGraphs:Concepts,
Models,andSystems.IEEETransactionsonParallelandDistributed
gelo Mangili, and the whole CSCS team granting access to
Systems,34(6):1860–1876,Jun2023.
the Ault and Daint machines, and for their excellent technical [15] M. Besta, R. Gerstenberger, N. Blach, M. Fischer, and
support.WethankTimoSchneiderforhelpwithinfrastructure T. Hoefler. GDI: A Graph Database Interface Standard.
at SPCL. This project received funding from the European https://github.com/spcl/GDI-RMA, Nov 2023. (accessed Jan.
15,2024).
ResearchCouncil(ProjectPSAP,No.101002047),andtheEuro-
[16] M. Besta, R. Gerstenberger, M. Fischer, M. Podstawski,
peanHigh-PerformanceComputingJointUndertaking(JU)un- N. Blach, B. Egeli, G. Mitenkov, W. Chlapek, M. Michalewicz,
der grant agreement No. 955513 (MAELSTROM). This project H.Niewiadomski,J.Mu¨ller,andT.Hoefler.TheGraphDatabase
Interface: Scaling Online Transactional and Analytical Graph
was supported by the ETH Future Computing Laboratory
Workloads to Hundreds of Thousands of Cores. In Proceedings
(EFCL), financed by a donation from Huawei Technologies.
of the International Conference for High Performance Computing,
This project received funding from the European Union’s HE Networking,StorageandAnalysis,SC’23,Denver,CO,USA,2023.
researchandinnovationprogrammeunderthegrantagreement AssociationforComputingMachinery.
[17] M.Besta,R.Gerstenberger,E.Peter,M.Fischer,M.Podstawski,
No.101070141(ProjectGLACIATION).
C. Barthels, G. Alonso, and T. Hoefler. Demystifying Graph
Databases:AnalysisandTaxonomyofDataOrganization,System
Designs,andGraphQueries.ACMComput.Surv.,56(2),Sep2023.
REFERENCES [18] M. Besta, R. Grob, C. Miglioli, N. Bernold, G. Kwas´niewski,
G.Gjini,R.Kanakagiri,S.Ashkboos,L.Gianinazzi,N.Dryden,
[1] J.Ahn,S.Yoo,O.Mutlu,andK.Choi. PIM-enabledinstructions: andT.Hoefler. MotifPredictionwithGraphNeuralNetworks.
a low-overhead, locality-aware processing-in-memory architec- InProceedingsofthe28thACMSIGKDDConferenceonKnowledge
ture. InProceedingsofthe42ndAnnualInternationalSymposiumon DiscoveryandDataMining,KDD’22,page35–45,WashingtonDC,
ComputerArchitecture,ISCA’15,page336–348,Portland,Oregon, USA,2022.AssociationforComputingMachinery.
2015.AssociationforComputingMachinery. [19] M. Besta, S. M. Hassan, S. Yalamanchili, R. Ausavarungnirun,
[2] A. Amini, S. Gabriel, P. Lin, R. Koncel-Kedziorski, Y. Choi, O.Mutlu,andT.Hoefler. SlimNoC:ALow-DiameterOn-Chip
and H. Hajishirzi. MathQA: Towards Interpretable Math Network Topology for High Energy Efficiency and Scalability.
WordProblemSolvingwithOperation-BasedFormalisms,,2019. SIGPLANNot.,53(2):43–55,Mar2018.
arXiv:1905.13319. [20] M.BestaandT.Hoefler. Slimfly:acosteffectivelow-diameter
[3] G.Angeli,M.J.JohnsonPremkumar,andC.D.Manning. Lever- networktopology. InProceedingsoftheInternationalConferencefor
agingLinguisticStructureForOpenDomainInformationExtrac- HighPerformanceComputing,Networking,StorageandAnalysis,SC
tion. In C. Zong and M. Strube, editors, Proceedings of the 53rd ’14,page348–359,NewOrleans,Louisana,2014.IEEEPress.
AnnualMeetingoftheAssociationforComputationalLinguisticsand [21] M.BestaandT.Hoefler. ActiveAccess:AMechanismforHigh-
the7thInternationalJointConferenceonNaturalLanguageProcessing PerformanceDistributedData-CentricComputations.InProceed-
(Volume1:LongPapers),ACL-IJCNLP’15,pages344–354,Beijing, ingsofthe29thACMonInternationalConferenceonSupercomputing,
China,Jul2015.AssociationforComputationalLinguistics. ICS ’15, page 155–164, Newport Beach, California, USA, 2015.
[4] R.AnglesandC.Gutierrez. SurveyofGraphDatabaseModels. AssociationforComputingMachinery.
ACMComput.Surv.,40(1),Feb2008. [22] M.BestaandT.Hoefler. ParallelandDistributedGraphNeural
[5] R.AnglesandC.Gutierrez.AnIntroductiontoGraphDataMan- Networks:AnIn-DepthConcurrencyAnalysis.IEEETransactions
agement. InG.H.L.Fletcher,J.Hidders,andJ.L.Larriba-Pey, onPatternAnalysisandMachineIntelligence,2023.
editors, Graph Data Management, Fundamental Issues and Recent [23] M.Besta,P.Iff,F.Scheidl,K.Osawa,N.Dryden,M.Podstawski,
Developments, Data-Centric Systems and Applications, pages 1– T.Chen,andT.Hoefler.NeuralGraphDatabases.InProceedingsof
32.Springer,2018. theFirstLearningonGraphsConference,volume198ofProceedings
[6] D.Arora,A.Kini,S.R.Chowdhury,N.Natarajan,G.Sinha,and ofMachineLearningResearch,pages31:1–31:38.PMLR,2022.
A.Sharma. GAR-meets-RAGParadigmforZero-ShotInforma- [24] M.Besta,R.Kanakagiri,G.Kwas´niewski,R.Ausavarungnirun,
tionRetrieval,2023. arXiv:2310.20158. J. Bera´nek, K. Kanellopoulos, K. Janda, Z. Vonarburg-Shmaria,
[7] A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi. Self- L. Gianinazzi, I. Stefan, J. G. Luna, J. Golinowski, M. Copik,
RAG:LearningtoRetrieve,Generate,andCritiquethroughSelf- L. Kapp-Schwoerer, S. Di Girolamo, N. Blach, M. Konieczny,
Reflection,2023. arXiv:2310.11511. O.Mutlu,andT.Hoefler.SISA:Set-CentricInstructionSetArchi-22
tectureforGraphMiningonProcessing-in-MemorySystems. In M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings,
Proceedingsofthe54thAnnualIEEE/ACMInternationalSymposium M.Plappert,F.Chantzis,E.Barnes,A.Herbert-Voss,W.H.Guss,
on Microarchitecture, MICRO ’21, page 282–297. Association for A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji,
ComputingMachinery,2021. S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam,
[25] M. Besta, C. Miglioli, P. S. Labini, J. Teˇtek, P. Iff, R. Kanaka- V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,
giri, S. Ashkboos, K. Janda, M. Podstawski, G. Kwas´niewski, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei,
N. Gleinig, F. Vella, O. Mutlu, and T. Hoefler. ProbGraph: S.McCandlish,I.Sutskever,andW.Zaremba. EvaluatingLarge
High-PerformanceandHigh-AccuracyGraphMiningwithProb- LanguageModelsTrainedonCode,2021. arXiv:2107.03374.
abilistic Set Representations. In Proceedings of the International [40] W.Chen,H.Hu,X.Chen,P.Verga,andW.W.Cohen. MuRAG:
ConferenceonHighPerformanceComputing,Networking,Storageand MultimodalRetrieval-AugmentedGeneratorforOpenQuestion
Analysis,SC’22,Dallas,Texas,2022.IEEEPress. AnsweringoverImagesandText,2022. arXiv:2210.02928.
[26] M.Besta,M.Podstawski,L.Groner,E.Solomonik,andT.Hoefler. [41] W. Chen, X. Ma, X. Wang, and W. W. Cohen. Program of
ToPushorToPull:OnReducingCommunicationandSynchro- ThoughtsPrompting:DisentanglingComputationfromReason-
nization in Graph Computations. In Proceedings of the 26th In- ing for Numerical Reasoning Tasks. Transactions on Machine
ternationalSymposiumonHigh-PerformanceParallelandDistributed LearningResearch,Nov2023.
Computing,HPDC’17,page93–104,Washington,DC,USA,2017. [42] Z. Chen, W. Chen, C. Smiley, S. Shah, I. Borova, D. Langdon,
AssociationforComputingMachinery. R.Moussa,M.Beane,T.-H.Huang,B.Routledge,andW.Y.Wang.
[27] M. Besta, P. Renc, R. Gerstenberger, P. Sylos Labini, A. Ziogas, FinQA:ADatasetofNumericalReasoningoverFinancialData.
T. Chen, L. Gianinazzi, F. Scheidl, K. Szenes, A. Carigiet, P. Iff, In M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, editors,
G.Kwas´niewski,R.Kanakagiri,C.Ge,S.Jaeger,J.Was,F.Vella, Proceedingsofthe2021ConferenceonEmpiricalMethodsinNatural
and T. Hoefler. High-Performance and Programmable Atten- LanguageProcessing,EMNLP’21,pages3697–3711,PuntaCana,
tionalGraphNeuralNetworkswithGlobalTensorFormulations. Dominican Republic, Nov 2021. Association for Computational
In Proceedings of the International Conference for High Performance Linguistics.
Computing,Networking,StorageandAnalysis,SC’23,Denver,CO, [43] Z. Chen, S. Li, C. Smiley, Z. Ma, S. Shah, and W. Y. Wang.
USA,2023.AssociationforComputingMachinery. ConvFinQA: Exploring the Chain of Numerical Reasoning in
[28] M.Besta,D.Stanojevic,J.DeFineLicht,T.Ben-Nun,andT.Hoe- Conversational Finance Question Answering. In Y. Goldberg,
fler.GraphProcessingonFPGAs:Taxonomy,Survey,Challenges, Z.Kozareva,andY.Zhang,editors,Proceedingsofthe2022Confer-
2019. arXiv:1903.06697. enceonEmpiricalMethodsinNaturalLanguageProcessing,EMNLP
[29] M.Besta,D.Stanojevic,T.Zivic,J.Singh,M.Hoerold,andT.Hoe- ’22, pages 6279–6292, Abu Dhabi, United Arab Emirates, Dec
fler.Log(Graph):Anear-OptimalHigh-PerformanceGraphRep- 2022.AssociationforComputationalLinguistics.
resentation. In Proceedings of the 27th International Conference [44] Z. Chen, H. Mao, H. Li, W. Jin, H. Wen, X. Wei, S. Wang,
on Parallel Architectures and Compilation Techniques, PACT ’18, D. Yin, W. Fan, H. Liu, and J. Tang. Exploring the Potential of
Limassol,Cyprus,2018.AssociationforComputingMachinery. Large Language Models (LLMs) in Learning on Graphs, 2023.
[30] M. Besta, Z. Vonarburg-Shmaria, Y. Schaffner, L. Schwarz, arXiv:2307.03393.
G. Kwas´niewski, L. Gianinazzi, J. Beranek, K. Janda, T. Holen- [45] Z. Chen, K. Zhou, B. Zhang, Z. Gong, W. X. Zhao, and J.-R.
stein,S.Leisinger,P.Tatkowski,E.Ozdemir,A.Balla,M.Copik, Wen. ChatCoT: Tool-Augmented Chain-of-Thought Reasoning
P.Lindenberger,M.Konieczny,O.Mutlu,andT.Hoefler. Graph- onChat-basedLargeLanguageModels. InH.Bouamor,J.Pino,
MineSuite: Enabling High-Performance and Programmable and K. Bali, editors, Findings of the Association for Computational
GraphMiningAlgorithmswithSetAlgebra. Proc.VLDBEndow., Linguistics: EMNLP 2023, pages 14777–14790, Singapore, Dec
14(11):1922–1935,Jul2021. 2023.AssociationforComputationalLinguistics.
[31] A.Bonifati,G.Fletcher,H.Voigt,andN.Yakovets. DataModels. [46] W.-L.Chiang,Z.Li,Z.Lin,Y.Sheng,Z.Wu,H.Zhang,L.Zheng,
In Querying Graphs, Synthesis Lectures on Data Management S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing.
(SDLM),pages3–14.SpringerInternationalPublishing,2018. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%
[32] R.Brate,M.-H.Dang,F.Hoppe,Y.He,A.Meron˜o-Pen˜uela,and ChatGPTQuality. https://lmsys.org/blog/2023-03-30-vicuna/,
V. Sadashivaiah. Improving Language Model Predictions via Mar2023. (accessedDec.15,2023).
PromptsEnrichedwithKnowledgeGraphs. InProceedingsofthe [47] Z.Chu,J.Chen,Q.Chen,W.Yu,T.He,H.Wang,W.Peng,M.Liu,
WorkshoponDeepLearningforKnowledgeGraphs,DL4KG@ISWC B. Qin, and T. Liu. A Survey of Chain of Thought Reasoning:
’22,Oct2022. Advances,FrontiersandFuture,2023. arXiv:2309.15402.
[33] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Van- [48] K.Cobbe,V.Kosaraju,M.Bavarian,M.Chen,H.Jun,L.Kaiser,
dergheynst. GeometricDeepLearning:GoingbeyondEuclidean M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and
data. IEEESignalProcessingMagazine,34(4):18–42,2017. J. Schulman. Training Verifiers to Solve Math Word Problems,
[34] T.Brown,B.Mann,N.Ryder,M.Subbiah,J.D.Kaplan,P.Dhari- 2021. arXiv:2110.14168.
wal,A.Neelakantan,P.Shyam,G.Sastry,A.Askell,S.Agarwal, [49] M.Copik,G.Kwas´niewski,M.Besta,M.Podstawski,andT.Hoe-
A.Herbert-Voss,G.Krueger,T.Henighan,R.Child,A.Ramesh, fler. SeBS: A Serverless Benchmark Suite for Function-as-a-
D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, ServiceComputing. InProceedingsofthe22ndInternationalMid-
M.Litwin,S.Gray,B.Chess,J.Clark,C.Berner,S.McCandlish, dleware Conference, Middleware ’21, page 64–78, Que´bec City,
A.Radford,I.Sutskever,andD.Amodei. LanguageModelsare Canada,2021.AssociationforComputingMachinery.
Few-Shot Learners. In H. Larochelle, M. Ranzato, R. Hadsell, [50] A.CreswellandM.Shanahan. FaithfulReasoningUsingLarge
M. Balcan, and H. Lin, editors, Advances in Neural Information LanguageModels,2022. arXiv:2208.14271.
Processing Systems (NeurIPS ’20), volume 33, pages 1877–1901. [51] A. Creswell, M. Shanahan, and I. Higgins. Selection-Inference:
CurranAssociates,2020. ExploitingLargeLanguageModelsforInterpretableLogicalRea-
[35] L. Cao. Enhancing Reasoning Capabilities of Large Lan- soning,2022. arXiv:2205.09712.
guage Models: A Graph-Based Verification Approach, 2023. [52] B. Dalvi, P. Jansen, O. Tafjord, Z. Xie, H. Smith, L. Pi-
arXiv:2308.09267. patanangkura,andP.Clark.ExplainingAnswerswithEntailment
[36] Z. Chai, T. Zhang, L. Wu, K. Han, X. Hu, X. Huang, and Trees. In M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih,
Y.Yang. GraphLLM:BoostingGraphReasoningAbilityofLarge editors, Proceedings of the 2021 Conference on Empirical Methods
LanguageModel,2023. arXiv:2310.05845. in Natural Language Processing, EMNLP ’21, pages 7358–7370,
[37] I. Chami, S. Abu-El-Haija, B. Perozzi, C. Re´, and K. Murphy. Punta Cana, Dominican Republic, Nov 2021. Association for
Machine Learning on Graphs: A Model and Comprehensive ComputationalLinguistics.
Taxonomy,2020. arXiv:2005.03675. [53] A.Davoudian,L.Chen,andM.Liu. ASurveyonNoSQLStores.
[38] B. Chen, Z. Zhang, N. Langrene´, and S. Zhu. Unleashing the ACMComput.Surv.,51(2),Apr2018.
potential of prompt engineering in Large Language Models: a [54] J.DeFineLicht,M.Besta,S.Meierhans,andT.Hoefler. Trans-
comprehensivereview,2023. arXiv:2310.14735. formationsofHigh-LevelSynthesisCodesforHigh-Performance
[39] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, Computing. IEEETransactionsonParallelandDistributedSystems,
J.Kaplan,H.Edwards,Y.Burda,N.Joseph,G.Brockman,A.Ray, 32(5):1014–1029,May2020.
R.Puri,G.Krueger,M.Petrov,H.Khlaaf,G.Sastry,P.Mishkin, [55] S.DiGirolamo,D.DeSensi,K.Taranov,M.Malesevic,M.Besta,
B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, T. Schneider, S. Kistler, and T. Hoefler. Building Blocks for23
Network-Accelerated Distributed File Systems. In Proceedings [74] L. Gianinazzi, M. Besta, Y. Schaffner, and T. Hoefler. Parallel
of the International Conference on High Performance Computing, Algorithms for Finding Large Cliques in Sparse Graphs. In
Networking,StorageandAnalysis,SC’22,Dallas,Texas,2022.IEEE Proceedingsofthe33rdACMSymposiumonParallelisminAlgorithms
Press. andArchitectures,SPAA’21,page243–253.AssociationforCom-
[56] S.DiGirolamo,K.Taranov,A.Kurth,M.Schaffner,T.Schneider, putingMachinery,2021.
J. Bera´nek, M. Besta, L. Benini, D. Roweth, and T. Hoefler. [75] L. Gianinazzi, M. Fries, N. Dryden, T. Ben-Nun, M. Besta, and
Network-Accelerated Non-Contiguous Memory Transfers. In T.Hoefler. LearningCombinatorialNodeLabelingAlgorithms,
ProceedingsoftheInternationalConferenceforHighPerformanceCom- 2021. arXiv:2106.03594.
puting,Networking,StorageandAnalysis,SC’19,Denver,Colorado, [76] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh.
2019.AssociationforComputingMachinery. MakingtheVinVQAMatter:ElevatingtheRoleofImageUnder-
[57] R. Ding, C. Zhang, L. Wang, Y. Xu, M. Ma, W. Zhang, S. Qin, standinginVisualQuestionAnswering,2016. arXiv:1612.00837.
S. Rajmohan, Q. Lin, and D. Zhang. Everything of Thoughts: [77] J. Gu, Z. Han, S. Chen, A. Beirami, B. He, G. Zhang, R. Liao,
Defying the Law of Penrose Triangle for Thought Generation, Y. Qin, V. Tresp, and P. Torr. A Systematic Survey of Prompt
2023. arXiv:2311.04254. Engineering on Vision-Language Foundation Models, 2023.
[58] A.Drozdov,N.Scha¨rli,E.Akyu¨rek,N.Scales,X.Song,X.Chen, arXiv:2307.12980.
O.Bousquet,andD.Zhou.CompositionalSemanticParsingwith [78] J. Gu, E. Stefani, Q. Wu, J. Thomason, and X. Wang. Vision-
LargeLanguageModels,2022. arXiv:2209.15003. and-Language Navigation: A Survey of Tasks, Methods, and
[59] D.Dua,S.Gupta,S.Singh,andM.Gardner. SuccessivePrompt- FutureDirections. InS.Muresan,P.Nakov,andA.Villavicencio,
ing for Decomposing Complex Questions. In Y. Goldberg, editors, Proceedings of the 60th Annual Meeting of the Association
Z.Kozareva,andY.Zhang,editors,Proceedingsofthe2022Confer- for Computational Linguistics (Volume 1: Long Papers), ACL ’22,
enceonEmpiricalMethodsinNaturalLanguageProcessing,EMNLP pages 7606–7623, Dublin, Ireland, May 2022. Association for
’22, pages 1251–1265, Abu Dhabi, United Arab Emirates, Dec ComputationalLinguistics.
2022.AssociationforComputationalLinguistics. [79] J. Guo, L. Du, and H. Liu. GPT4Graph: Can Large Language
[60] D.Dua,Y.Wang,P.Dasigi,G.Stanovsky,S.Singh,andM.Gard- ModelsUnderstandGraphStructuredData?AnEmpiricalEval-
ner. DROP: A Reading Comprehension Benchmark Requiring uationandBenchmarking,2023. arXiv:2305.15066.
Discrete Reasoning Over Paragraphs. In J. Burstein, C. Doran, [80] M. Hahn and N. Goyal. A Theory of Emergent In-Context
and T. Solorio, editors, Proceedings of the 2019 Conference of the LearningasImplicitStructureInduction,2023. arXiv:2303.07971.
North American Chapter of the Association for Computational Lin- [81] W.L.Hamilton,R.Ying,andJ.Leskovec. RepresentationLearn-
guistics: Human Language Technologies, Volume 1 (Long and Short ingonGraphs:MethodsandApplications.BulletinoftheTechnical
Papers),NAACL’19,pages2368–2378,Minneapolis,Minnesota, CommitteeonDataEngineering,40(3):52–74,Sept.2017.
Jun2019.AssociationforComputationalLinguistics. [82] S. Han, H. Schoelkopf, Y. Zhao, Z. Qi, M. Riddell, L. Benson,
[61] B.Fatemi,J.Halcrow,andB.Perozzi.TalklikeaGraph:Encoding L. Sun, E. Zubova, Y. Qiao, M. Burtell, D. Peng, J. Fan, Y. Liu,
GraphsforLargeLanguageModels,2023. arXiv:2310.04560. B. Wong, M. Sailor, A. Ni, L. Nan, J. Kasai, T. Yu, R. Zhang,
[62] C.Feng,X.Zhang,andZ.Fei.KnowledgeSolver:TeachingLLMs S. Joty, A. R. Fabbri, W. Kryscinski, X. V. Lin, C. Xiong, and
toSearchforDomainKnowledgefromKnowledgeGraphs,2023. D.Radev.FOLIO:NaturalLanguageReasoningwithFirst-Order
arXiv:2309.03118. Logic,2022. arXiv:2209.00840.
[63] M. Fey and J. E. Lenssen. Fast Graph Representation Learning [83] M. Hartmann and D. Sonntag. A survey on improving NLP
with PyTorch Geometric. In Proceedings of the Representation modelswithhumanexplanations.InJ.Andreas,K.Narasimhan,
Learning on Graphs and Manifolds Workshop, RLGM@ICLR ’19, andA.Nematzadeh,editors,ProceedingsoftheFirstWorkshopon
2019. LearningwithNaturalLanguageSupervision,pages40–47,Dublin,
[64] Y.FreundandR.E.Schapire. ExperimentswithaNewBoosting Ireland,May2022.AssociationforComputationalLinguistics.
Algorithm. InProceedingsoftheThirteenthInternationalConference [84] J.He-Yueya,G.Poesia,R.E.Wang,andN.D.Goodman.Solving
on International Conference on Machine Learning, ICML ’96, page Math Word Problems by Combining Language Models With
148–156,Bari,Italy,1996.MorganKaufmann. SymbolicSolvers,2023. arXiv:2304.09102.
[65] Y. Fu, P. Bailis, I. Stoica, and H. Zhang. Breaking the Sequen- [85] D.Hendrycks,C.Burns,S.Basart,A.Zou,M.Mazeika,D.Song,
tial Dependency of LLM Inference Using Lookahead Decod- andJ.Steinhardt. MeasuringMassiveMultitaskLanguageUn-
ing. https://lmsys.org/blog/2023-11-21-lookahead-decoding/, derstanding. In Proceedings of the International Conference on
Nov2023. (accessedDec.23,2023). LearningRepresentations,ICLR’21,2021.
[66] M. Galkin, X. Yuan, H. Mostafa, J. Tang, and Z. Zhu. To- [86] D.Hendrycks,C.Burns,S.Kadavath,A.Arora,S.Basart,E.Tang,
wards Foundation Models for Knowledge Graph Reasoning, D. Song, and J. Steinhardt. Measuring Mathematical Problem
2023. arXiv:2310.04562. SolvingWiththeMATHDataset. InProceedingsoftheThirty-fifth
[67] L.Gao,A.Madaan,S.Zhou,U.Alon,P.Liu,Y.Yang,J.Callan, Conference on Neural Information Processing Systems: Datasets and
and G. Neubig. PAL: Program-aided Language Models, 2022. BenchmarksTrack,NeurIPS’21,2021.
arXiv:2211.10435. [87] X.Ho,A.-K.DuongNguyen,S.Sugawara,andA.Aizawa. Con-
[68] Y.Gao,Y.Xiong,X.Gao,K.Jia,J.Pan,Y.Bi,Y.Dai,J.Sun,and structingAMulti-hopQADatasetforComprehensiveEvaluation
H.Wang. Retrieval-AugmentedGenerationforLargeLanguage of Reasoning Steps. In D. Scott, N. Bel, and C. Zong, editors,
Models:ASurvey,2023. arXiv:2312.10997. Proceedings of the 28th International Conference on Computational
[69] N.Garcia,C.Ye,Z.Liu,Q.Hu,M.Otani,C.Chu,Y.Nakashima, Linguistics,COLING’20,pages6609–6625,Barcelona,Spain,Dec
andT.Mitamura. ADatasetandBaselinesforVisualQuestion 2020.InternationalCommitteeonComputationalLinguistics.
AnsweringonArt.InA.BartoliandA.Fusiello,editors,Computer [88] M.J.Hosseini,H.Hajishirzi,O.Etzioni,andN.Kushman.Learn-
Vision–ECCV2020Workshops,volume12536ofLectureNotesin ingtoSolveArithmeticWordProblemswithVerbCategorization.
ComputerScience,pages92–108.Springer,Jan2020. InA.Moschitti,B.Pang,andW.Daelemans,editors,Proceedings
[70] R. Gerstenberger, M. Besta, and T. Hoefler. Enabling Highly- of the 2014 Conference on Empirical Methods in Natural Language
ScalableRemoteMemoryAccessProgrammingwithMPI-3One Processing, EMNLP ’14, pages 523–533, Doha, Qatar, Oct 2014.
Sided. In Proceedings of the International Conference on High AssociationforComputationalLinguistics.
PerformanceComputing,Networking,StorageandAnalysis,SC’13, [89] H. Hu, H. Lu, H. Zhang, W. Lam, and Y. Zhang. Chain-of-
Denver,Colorado,2013.AssociationforComputingMachinery. SymbolPrompting ElicitsPlanninginLargeLangaugeModels,
[71] M.Geva,D.Khashabi,E.Segal,T.Khot,D.Roth,andJ.Berant. 2023. arXiv:2305.10276.
DidAristotleUseaLaptop?AQuestionAnsweringBenchmark [90] L. Hu, Z. Liu, Z. Zhao, L. Hou, L. Nie, and J. Li. A Survey
withImplicitReasoningStrategies. TransactionsoftheAssociation of Knowledge Enhanced Pre-Trained Language Models. IEEE
forComputationalLinguistics,9:346–361,2021. TransactionsonKnowledgeandDataEngineering,pages1–19,2023.
[72] S.Ghose,A.Boroumand,J.S.Kim,J.Go´mez-Luna,andO.Mutlu. [91] P.Hu,J.Qi,X.Li,H.Li,X.Wang,B.Quan,R.Wang,andY.Zhou.
Processing-in-Memory: A Workload-driven Perspective. IBM Tree-of-Mixed-Thought:CombiningFastandSlowThinkingfor
JournalofResearchandDevelopment,63(6):3:1–3:19,Nov2019. Multi-hopVisualReasoning,2023. arXiv:2308.09658.
[73] L.Gianinazzi,T.Ben-Nun,M.Besta,S.Ashkboos,Y.Baumann, [92] Y.Hu,Z.Zhang,andL.Zhao. BeyondText:ADeepDiveinto
P.Luczynski,andT.Hoefler. Thespatialcomputer:Amodelfor LargeLanguageModels’AbilityonUnderstandingGraphData,
energy-efficientparallelcomputation,2022. arXiv:2205.04934. 2023. arXiv:2310.04944.24
[93] J. Huang and K. C.-C. Chang. Towards Reasoning in Large [109] N.KimandT.Linzen. COGS:ACompositionalGeneralization
LanguageModels:ASurvey. InA.Rogers,J.Boyd-Graber,and Challenge Based on Semantic Interpretation. In B. Webber,
N.Okazaki,editors,FindingsoftheAssociationforComputational T.Cohn,Y.He,andY.Liu,editors,Proceedingsofthe2020Confer-
Linguistics: ACL 2023, pages 1049–1065, Toronto, Canada, Jul enceonEmpiricalMethodsinNaturalLanguageProcessing,EMNLP
2023.AssociationforComputationalLinguistics. ’20,pages9087–9105.AssociationforComputationalLinguistics,
[94] J. Huang, X. Zhang, Q. Mei, and J. Ma. Can LLMs Effectively Nov2020.
Leverage Graph Structural Information: When and Why, 2023. [110] S.Kim,S.Moon,R.Tabrizi,N.Lee,M.W.Mahoney,K.Keutzer,
arXiv:2309.16595. andA.Gholami.AnLLMCompilerforParallelFunctionCalling,
[95] P.Iff,M.Besta,M.Cavalcante,T.Fischer,L.Benini,andT.Hoefler. 2023. arXiv:2312.04511.
HexaMesh:ScalingtoHundredsofChipletswithanOptimized [111] T.N.KipfandM.Welling. Semi-SupervisedClassificationwith
ChipletArrangement.InProceedingsofthe60thACM/IEEEDesign GraphConvolutionalNetwork. InProceedingsofthe5thInterna-
Automation Conference, DAC ’23, pages 1–6, San Francisco, CA, tionalConferenceonLearningRepresentations,ICLR’17,Apr2017.
USA,Jul2023. [112] T.Kojima,S.S.Gu,M.Reid,Y.Matsuo,andY.Iwasawa. Large
[96] P.Iff,M.Besta,M.Cavalcante,T.Fischer,L.Benini,andT.Hoe- LanguageModelsareZero-ShotReasoners. InS.Koyejo,S.Mo-
fler. Sparse Hamming Graph: A Customizable Network-on- hamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors,
ChipTopology. InProceedingsofthe202360thACM/IEEEDesign Advances in Neural Information Processing Systems (NeurIPS ’22),
Automation Conference, DAC ’23, pages 1–6, San Francisco, CA, volume35,pages22199–22213.CurranAssociates,2022.
USA,Jul2023. [113] R. Koncel-Kedziorski, S. Roy, A. Amini, N. Kushman, and
[97] JesseDodgeandAndreeaGaneandXiangZhangandAntoine H. Hajishirzi. MAWPS: A Math Word Problem Repository. In
Bordes and Sumit Chopra and Alexander Miller and Arthur K.Knight,A.Nenkova,andO.Rambow,editors,Proceedingsof
Szlam and Jason Weston. Evaluating Prerequisite Qualities for the 2016 Conference of the North American Chapter of the Associ-
LearningEnd-to-EndDialogSystems,2016. arXiv:1511.06931. ation for Computational Linguistics: Human Language Technologies,
NAACL-HLT ’16, pages 1152–1157, San Diego, California, Jun
[98] H.Jiang. ALatentSpaceTheoryforEmergentAbilitiesinLarge
2016.AssociationforComputationalLinguistics.
LanguageModels,2023. arXiv:2304.09960.
[114] D. Kumar, V. Gupta, S. Sharma, and S. Zhang. Realistic Data
[99] S.Jiang,Z.Shakeri,A.Chan,M.Sanjabi,H.Firooz,Y.Xia,B.Aky-
AugmentationFrameworkforEnhancingTabularReasoning. In
ildiz, Y. Sun, J. Li, Q. Wang, and A. Celikyilmaz. Resprompt:
Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Findings of
ResidualConnectionPromptingAdvancesMulti-StepReasoning
theAssociationforComputationalLinguistics:EMNLP2022,pages
inLargeLanguageModels,2023. arXiv:2310.04743.
4411–4429,AbuDhabi,UnitedArabEmirates,Dec2022.Associ-
[100] Z. Jiang, F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang,
ationforComputationalLinguistics.
J. Callan, and G. Neubig. Active Retrieval Augmented Gener-
[115] B. Lake and M. Baroni. Generalization without Systematicity:
ation. In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings
OntheCompositionalSkillsofSequence-to-SequenceRecurrent
of the 2023 Conference on Empirical Methods in Natural Language
Networks. In J. Dy and A. Krause, editors, Proceedings of the
Processing, EMNLP ’23, pages 7969–7992, Singapore, Dec 2023.
35thInternationalConferenceonInternationalConferenceonMachine
AssociationforComputationalLinguistics.
Learning(ICML’18),volume80ofProceedingsofMachineLearning
[101] B. Jin, G. Liu, C. Han, M. Jiang, H. Ji, and J. Han. Large Research,pages2873–2882.PMLR,Jul2018.
Language Models on Graphs: A Comprehensive Survey, 2023.
[116] K.Lakhotia,M.Besta,L.Monroe,K.Isham,P.Iff,T.Hoefler,and
arXiv:2312.02783.
F. Petrini. PolarFly: a cost-effective and flexible low-diameter
[102] J.Jung,L.Qin,S.Welleck,F.Brahman,C.Bhagavatula,R.LeBras, topology. In Proceedings of the International Conference on High
andY.Choi.MaieuticPrompting:LogicallyConsistentReasoning PerformanceComputing,Networking,StorageandAnalysis,SC’22,
withRecursiveExplanations. InY.Goldberg,Z.Kozareva,and Dallas,Texas,2022.IEEEPress.
Y.Zhang,editors,Proceedingsofthe2022ConferenceonEmpirical [117] K. Lakhotia, K. Isham, L. Monroe, M. Besta, T. Hoefler, and
MethodsinNaturalLanguageProcessing,EMNLP’22,pages1266– F.Petrini.In-networkAllreducewithMultipleSpanningTreeson
1279, Abu Dhabi, United Arab Emirates, Dec 2022. Association PolarFly. InProceedingsofthe35thACMSymposiumonParallelism
forComputationalLinguistics. inAlgorithmsandArchitectures,SPAA’23,page165–176,Orlando,
[103] D. Keysers, N. Scha¨rli, N. Scales, H. Buisman, D. Furrer, FL,USA,2023.AssociationforComputingMachinery.
S.Kashubin,N.Momchev,D.Sinopalnikov,L.Stafiniak,T.Tihon, [118] K.Lakhotia,L.Monroe,K.Isham,M.Besta,N.Blach,T.Hoefler,
D.Tsarkov,X.Wang,M.vanZee,andO.Bousquet. Measuring and F. Petrini. PolarStar: Expanding the Scalability Horizon of
CompositionalGeneralization:AComprehensiveMethodonRe- Diameter-3Networks,2023. arXiv:2302.07217.
alisticData. InProceedingsoftheEigthInternationalConferenceon [119] B.Lei,p.-H.Lin,C.Liao,andC.Ding. BoostingLogicalReason-
LearningRepresentations,ICLR’20,Apr2020. inginLargeLanguageModelsthroughaNewFramework:The
[104] T. Khot, K. Richardson, D. Khashabi, and A. Sabharwal. Hey GraphofThought,2023. arXiv:2308.08614.
AI, Can You Solve Complex Tasks by Talking to Agents? In [120] Leo Breiman, Jerome Friedman, Charles J. Stone, R.A. Olshen.
S.Muresan,P.Nakov,andA.Villavicencio,editors,Findingsofthe ClassificationandRegressionTrees. ChapmanandHall,1984.
Association for Computational Linguistics: ACL 2022, pages 1808– [121] S. Li, Y. Zhao, R. Varma, O. Salpekar, P. Noordhuis, T. Li,
1823,Dublin,Ireland,May2022.AssociationforComputational A. Paszke, J. Smith, B. Vaughan, P. Damania, and S. Chintala.
Linguistics. PyTorch Distributed: Experiences on Accelerating Data Parallel
[105] T.Khot,H.Trivedi,M.Finlayson,Y.Fu,K.Richardson,P.Clark, Training. Proc.VLDBEndow.,13(12):3005–3018,Aug2020.
and A. Sabharwal. Decomposed Prompting: A Modular Ap- [122] Y. Li, Z. Li, P. Wang, J. Li, X. Sun, H. Cheng, and J. X. Yu. A
proachforSolvingComplexTasks. InProceedingsoftheEleventh Survey of Graph Meets Large Language Model: Progress and
InternationalConferenceonLearningRepresentations,ICLR’23,May FutureDirections,2023. arXiv:2311.12399.
2023. [123] Y.Li,Z.Lin,S.Zhang,Q.Fu,B.Chen,J.-G.Lou,andW.Chen.
[106] G.Kim,S.Kim,B.Jeon,J.Park,andJ.Kang.TreeofClarifications: Making Language Models Better Reasoners with Step-Aware
Answering Ambiguous Questions with Retrieval-Augmented Verifier. InA.Rogers,J.Boyd-Graber,andN.Okazaki,editors,
Large Language Models. In H. Bouamor, J. Pino, and K. Bali, Proceedingsofthe61stAnnualMeetingoftheAssociationforCompu-
editors,Proceedingsofthe2023ConferenceonEmpiricalMethodsin tationalLinguistics(Volume1:LongPapers),ACL’23,pages5315–
NaturalLanguageProcessing,EMNLP’23,pages996–1009,Singa- 5333,Toronto,Canada,Jul2023.AssociationforComputational
pore,Dec2023.AssociationforComputationalLinguistics. Linguistics.
[107] J. Kim, W. J. Dally, and D. Abts. Flattened Butterfly: A Cost- [124] J. Lin, M. Diesendruck, L. Du, and R. Abraham. BatchPrompt:
efficient Topology for High-radix Networks. In Proceedings of Accomplishmorewithless,2023. arXiv:2309.00384.
the34thAnnualInternationalSymposiumonComputerArchitecture, [125] W. Ling, D. Yogatama, C. Dyer, and P. Blunsom. Program In-
ISCA’07,page126–137,SanDiego,California,USA,2007.Asso- ductionbyRationaleGeneration:LearningtoSolveandExplain
ciationforComputingMachinery. AlgebraicWordProblems. InR.BarzilayandM.-Y.Kan,editors,
[108] J. Kim, W. J. Dally, S. Scott, and D. Abts. Technology-Driven, Proceedingsofthe55thAnnualMeetingoftheAssociationforCom-
Highly-Scalable Dragonfly Topology. In Proceedings of the 35th putationalLinguistics(Volume1:LongPapers),ACL’17,pages158–
Annual International Symposium on Computer Architecture, ISCA 167,Vancouver,Canada,Jul2017.AssociationforComputational
’08,page77–88,Beijing,China,2008.IEEEComputerSociety. Linguistics.25
[126] Z. Ling, Y. Fang, X. Li, Z. Huang, M. Lee, R. Memisevic, and [143] R. Mirzaee and P. Kordjamshidi. Transfer Learning with Syn-
H.Su.DeductiveVerificationofChain-of-ThoughtReasoning.In thetic Corpora for Spatial Role Labeling and Reasoning. In
Advances in Neural Information Processing Systems (NeurIPS ’23), Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Proceedings of
volume36.CurranAssociates,2023. the2022ConferenceonEmpiricalMethodsinNaturalLanguagePro-
[127] C. Liu and B. Wu. Evaluating Large Language Models on cessing,EMNLP’22,pages6148–6165,AbuDhabi,UnitedArab
Graphs:PerformanceInsightsandComparativeAnalysis,2023. Emirates,Dec2022.AssociationforComputationalLinguistics.
arXiv:2308.11224. [144] S.Mittal.AsurveyofFPGA-basedacceleratorsforconvolutional
[128] J.Liu,L.Cui,H.Liu,D.Huang,Y.Wang,andY.Zhang.LogiQA: neuralnetworks. NeuralComputingandApplications,32(4):1109–
AChallengeDatasetforMachineReadingComprehensionwith 1139,Feb2020.
LogicalReasoning.InC.Bessiere,editor,ProceedingsoftheTwenty- [145] S. Mo and M. Xin. Tree of Uncertain Thoughts Reasoning for
NinthInternationalJointConferenceonArtificialIntelligence,IJCAI LargeLanguageModels,2023. arXiv:2309.07694.
’20,pages3622–3628.InternationalJointConferencesonArtificial [146] O. Mutlu, S. Ghose, J. Go´mez-Luna, and R. Ausavarungnirun.
IntelligenceOrganization,Jul2020. A Modern Primer on Processing in Memory. In M. M. S. Aly
[129] J.Liu,C.Yang,Z.Lu,J.Chen,Y.Li,M.Zhang,T.Bai,Y.Fang, andA.Chattopadhyay,editors,EmergingComputing:FromDevices
L.Sun,P.S.Yu,andC.Shi. TowardsGraphFoundationModels: to Systems - Looking Beyond Moore and Von Neumann, Computer
ASurveyandBeyond,2023. arXiv:2310.11829. ArchitectureandDesignMethodologies(CADM),pages171–243.
[130] P.Liu,W.Yuan,J.Fu,Z.Jiang,H.Hayashi,andG.Neubig. Pre- SpringerNatureSingapore,2023.
Train, Prompt, and Predict: A Systematic Survey of Prompting [147] O. Mutlu, S. Ghose, J. Go´mez-Luna, and R. Ausavarungnirun.
MethodsinNaturalLanguageProcessing. ACMComput.Surv., Processing Data Where It Makes Sense: Enabling In-Memory
55(9),Jan2023. Computation. Microprocessors and Microsystems, 67:28–41, Jun
2019.
[131] T.Liu,Q.Guo,Y.Yang,X.Hu,Y.Zhang,X.Qiu,andZ.Zhang.
Plan,VerifyandSwitch:IntegratedReasoningwithDiverseX-of- [148] X. Ning, Z. Lin, Z. Zhou, H. Yang, and Y. Wang. Skeleton-of-
Thoughts.InH.Bouamor,J.Pino,andK.Bali,editors,Proceedings Thought: Large Language Models Can Do Parallel Decoding,
of the 2023 Conference on Empirical Methods in Natural Language 2023. arXiv:2307.15337.
Processing, EMNLP ’23, pages 2807–2822, Singapore, Dec 2023. [149] S.Pan,L.Luo,Y.Wang,C.Chen,J.Wang,andX.Wu. Unifying
AssociationforComputationalLinguistics. Large Language Models and Knowledge Graphs: A Roadmap,
2023. arXiv:2306.08302.
[132] Z. Liu, Z. Lai, Z. Gao, E. Cui, X. Zhu, L. Lu, Q. Chen, Y. Qiao,
[150] S.Pan,Y.Zheng,andY.Liu.IntegratingGraphswithLargeLan-
J.Dai,andW.Wang. ControlLLM:AugmentLanguageModels
guageModels:MethodsandProspects,2023. arXiv:2310.05499.
withToolsbySearchingonGraphs,2023. arXiv:2310.17796.
[151] A. Patel, S. Bhattamishra, and N. Goyal. Are NLP Mod-
[133] J.Long. LargeLanguageModelGuidedTree-of-Thought,2023.
els really able to Solve Simple Math Word Problems? In
arXiv:2305.08291.
K. Toutanova, A. Rumshisky, L. Zettlemoyer, D. Hakkani-Tur,
[134] R. Long, P. Pasupat, and P. Liang. Simpler Context-Dependent
I.Beltagy,S.Bethard,R.Cotterell,T.Chakraborty,andY.Zhou,
LogicalFormsviaModelProjections. InK.ErkandN.A.Smith,
editors, Proceedings of the 2021 Conference of the North American
editors, Proceedings of the 54th Annual Meeting of the Association
Chapter of the Association for Computational Linguistics: Human
for Computational Linguistics (Volume 1: Long Papers), ACL ’16,
LanguageTechnologies,NAACL’21,pages2080–2094.Association
pages 1456–1465, Berlin, Germany, Aug 2016. Association for
forComputationalLinguistics,Jun2021.
ComputationalLinguistics.
[152] O.Press,M.Zhang,S.Min,L.Schmidt,N.Smith,andM.Lewis.
[135] P.Lu,S.Mishra,T.Xia,L.Qiu,K.-W.Chang,S.-C.Zhu,O.Tafjord,
Measuring and Narrowing the Compositionality Gap in Lan-
P.Clark,andA.Kalyan. LearntoExplain:MultimodalReason-
guage Models. In H. Bouamor, J. Pino, and K. Bali, editors,
ing via Thought Chains for Science Question Answering. In
Findings of the Association for Computational Linguistics: EMNLP
S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and
2023, pages 5687–5711, Singapore, Dec 2023. Association for
A.Oh,editors,AdvancesinNeuralInformationProcessingSystems
ComputationalLinguistics.
(NeurIPS ’22), volume 35, pages 2507–2521. Curran Associates,
[153] R. Puri, D. S. Kung, G. Janssen, W. Zhang, G. Domeniconi,
2022.
V.Zolotov,J.Dolby,J.Chen,M.Choudhury,L.Decker,V.Thost,
[136] P.Lu,B.Peng,H.Cheng,M.Galley,K.-W.Chang,Y.N.Wu,S.-
L.Buratti,S.Pujar,S.Ramji,U.Finkler,S.Malaika,andF.Reiss.
C. Zhu, and J. Gao. Chameleon: Plug-and-Play Compositional
CodeNet: A Large-Scale AI for Code Dataset for Learning a
ReasoningwithLargeLanguageModels. InAdvancesinNeural
DiversityofCodingTasks,2021. arXiv:2105.12655.
Information Processing Systems (NeurIPS ’23), volume 36. Curran
[154] J. Qi, Z. Xu, Y. Shen, M. Liu, D. Jin, Q. Wang, and L. Huang.
Associates,2023.
TheArtofSOCRATICQUESTIONING:RecursiveThinkingwith
[137] P. Lu, L. Qiu, K.-W. Chang, Y. N. Wu, S.-C. Zhu, T. Rajpuro- LargeLanguageModels,2023. arXiv:2305.14999.
hit, P. Clark, and A. Kalyan. Dynamic Prompt Learning via [155] C.Qian,H.Tang,Z.Yang,H.Liang,andY.Liu. CanLargeLan-
Policy Gradient for Semi-structured Mathematical Reasoning. guage Models Empower Molecular Property Prediction?, 2023.
In Proceedings of the Eleventh International Conference on Learning arXiv:2307.07443.
Representations,ICLR’23,May2023.
[156] S. Qiao, Y. Ou, N. Zhang, X. Chen, Y. Yao, S. Deng, C. Tan,
[138] L. Luo, Y.-F. Li, G. Haffari, and S. Pan. Reasoning on Graphs: F. Huang, and H. Chen. Reasoning with Language Model
Faithful and Interpretable Large Language Model Reasoning, Prompting: A Survey. In A. Rogers, J. Boyd-Graber, and
2023. arXiv:2310.01061. N.Okazaki,editors,Proceedingsofthe61stAnnualMeetingofthe
[139] A.Ma,X.Wang,J.Li,C.Wang,T.Xiao,Y.Liu,H.Cheng,J.Wang, Association for Computational Linguistics (Volume 1: Long Papers),
Y. Li, Y. Chang, J. Li, D. Wang, Y. Jiang, L. Su, G. Xin, S. Gu, pages 5368–5393, Toronto, Canada, Jul 2023. Association for
Z. Li, B. Liu, D. Xu, and Q. Ma. Single-cell biological network ComputationalLinguistics.
inferenceusingaheterogeneousgraphtransformer. NatureCom- [157] A.Radford,J.Wu,R.Child,D.Luan,D.Amodei,andI.Sutskever.
munications,14(1),2023. LanguageModelsareUnsupervisedMultitaskLearners.https://
[140] A.Madaan,N.Tandon,P.Gupta,S.Hallinan,L.Gao,S.Wiegr- openai.com/research/better-language-models, 2019. (accessed:
effe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, S. Gupta, Dec.4,2023).
B.P.Majumder,K.Hermann,S.Welleck,A.Yazdanbakhsh,and [158] C.Raffel,N.Shazeer,A.Roberts,K.Lee,S.Narang,M.Matena,
P. Clark. Self-Refine: Iterative Refinement with Self-Feedback, Y. Zhou, W. Li, and P. J. Liu. Exploring the Limits of Trans-
2023. arXiv:2303.17651. fer Learning with a Unified Text-to-Text Transformer, 2023.
[141] A. Madaan and A. Yazdanbakhsh. Text and Patterns: For arXiv:1910.10683.
Effective Chain of Thought, It Takes Two to Tango, 2022. [159] A.Rasouli,I.Kotseruba,T.Kunic,andJ.Tsotsos. PIE:ALarge-
arXiv:2209.07686. Scale Dataset and Models for Pedestrian Intention Estimation
[142] C. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. Bethard, and and Trajectory Prediction. In Proceedings of the 2019 IEEE/CVF
D. McClosky. The Stanford CoreNLP Natural Language Pro- InternationalConferenceonComputerVision,ICCV’19,pages6261–
cessingToolkit. InK.BontchevaandJ.Zhu,editors,Proceedings 6270,2019.
of 52nd Annual Meeting of the Association for Computational Lin- [160] T. B. Richards. AutoGPT: build & use AI agents - Github.
guistics:SystemDemonstrations,ACL’14,pages55–60,Baltimore, https://github.com/Significant-Gravitas/AutoGPT, Mar 2023.
Maryland,Jun2014.AssociationforComputationalLinguistics. (accessedJan.23,2024).26
[161] S.RoyandD.Roth. SolvingGeneralArithmeticWordProblems. guage. InC.Zong,F.Xia,W.Li,andR.Navigli,editors,Findings
InL.Ma`rquez,C.Callison-Burch,andJ.Su,editors,Proceedings oftheAssociationforComputationalLinguistics:ACL-IJCNLP2021,
of the 2015 Conference on Empirical Methods in Natural Language pages3621–3634.AssociationforComputationalLinguistics,Aug
Processing, EMNLP ’15, pages 1743–1752, Lisbon, Portugal, Sep 2021.
2015.AssociationforComputationalLinguistics. [180] A. Talmor, J. Herzig, N. Lourie, and J. Berant. Common-
[162] S. Saha, O. Levy, A. Celikyilmaz, M. Bansal, J. Weston, and senseQA:AQuestionAnsweringChallengeTargetingCommon-
X. Li. Branch-Solve-Merge Improves Large Language Model senseKnowledge.InJ.Burstein,C.Doran,andT.Solorio,editors,
EvaluationandGeneration,2023. arXiv:2310.15123. Proceedings of the 2019 Conference of the North American Chapter
[163] M. Sakarvadia, A. Ajith, A. Khan, D. Grzenda, N. Hudson, of the Association for Computational Linguistics: Human Language
A.Bauer,K.Chard,andI.Foster. MemoryInjections:Correcting Technologies,Volume1(LongandShortPapers),NAACL’19,pages
Multi-HopReasoningFailuresduringInferenceinTransformer- 4149–4158, Minneapolis, Minnesota, Jun 2019. Association for
BasedLanguageModels,2023. arXiv:2309.05605. ComputationalLinguistics.
[164] A. Saparov and H. He. Language Models Are Greedy Rea- [181] J. Tang, Y. Yang, W. Wei, L. Shi, L. Su, S. Cheng, D. Yin, and
soners: A Systematic Formal Analysis of Chain-of-Thought. In C. Huang. GraphGPT: Graph Instruction Tuning for Large
Proceedings of the Eleventh International Conference on Learning LanguageModels,2023. arXiv:2310.13023.
Representations,ICLR’23,May2023. [182] A.J.Thirunavukarasu,D.S.J.Ting,K.Elangovan,L.Gutierrez,
[165] T. Schick, J. Dwivedi-Yu, R. Dess`ı, R. Raileanu, M. Lomeli, T.F.Tan,andD.S.W.Ting. Largelanguagemodelsinmedicine.
L. Zettlemoyer, N. Cancedda, and T. Scialom. Toolformer: NatureMedicine,29(8):1930–1940,Jul2023.
Language Models Can Teach Themselves to Use Tools, 2023. [183] Y. Tian, H. Song, Z. Wang, H. Wang, Z. Hu, F. Wang, N. V.
arXiv:2302.04761. Chawla, and P. Xu. Graph Neural Prompting with Large Lan-
[166] B. Sel, A. Al-Tawaha, V. Khattar, L. Wang, R. Jia, and M. Jin. guageModels,2023. arXiv:2309.15427.
AlgorithmofThoughts:EnhancingExplorationofIdeasinLarge [184] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal.
LanguageModels,2023. arXiv:2308.10379. MuSiQue:MultihopQuestionsviaSingle-hopQuestionCompo-
[167] V.Seshadri,D.Lee,T.Mullins,H.Hassan,A.Boroumand,J.Kim, sition. TransactionsoftheAssociationforComputationalLinguistics,
M. A. Kozuch, O. Mutlu, P. B. Gibbons, and T. C. Mowry. 10:539–554,2022.
Ambit:In-memoryacceleratorforbulkbitwiseoperationsusing [185] R.Tutunov,A.Grosnit,J.Ziomek,J.Wang,andH.Bou-Ammar.
commodityDRAMtechnology. InProceedingsofthe50thAnnual Why Can Large Language Models Generate Correct Chain-of-
IEEE/ACM International Symposium on Microarchitecture, MICRO Thoughts?,2023. arXiv:2310.13571.
’17,page273–287,Cambridge,Massachusetts,2017.Association [186] H.Wang,S.Feng,T.He,Z.Tan,X.Han,andY.Tsvetkov. Can
forComputingMachinery. LanguageModelsSolveGraphProblemsinNaturalLanguage?
[168] N.Shinn,F.Cassano,A.Gopinath,K.R.Narasimhan,andS.Yao. InAdvancesinNeuralInformationProcessingSystems(NeurIPS’23),
Reflexion:Languageagentswithverbalreinforcementlearning. volume36.CurranAssociates,2023.
InAdvancesinNeuralInformationProcessingSystems(NeurIPS’23), [187] H.Wang,Y.Gao,X.Zheng,P.Zhang,H.Chen,andJ.Bu. Graph
volume36.CurranAssociates,2023. NeuralArchitectureSearchwithGPT-4,2023. arXiv:2310.01436.
[169] M. Shridhar, X. Yuan, M.-A. Coˆte´, Y. Bisk, A. Trischler, and [188] L. Wang, W. Xu, Y. Lan, Z. Hu, Y. Lan, R. K.-W. Lee, and E.-P.
M. Hausknecht. ALFWorld: Aligning Text and Embodied En- Lim. Plan-and-SolvePrompting:ImprovingZero-ShotChain-of-
vironmentsforInteractiveLearning. InProceedingsoftheInterna- Thought Reasoning by Large Language Models. In A. Rogers,
tionalConferenceonLearningRepresentations,ICLR’21,2021. J. Boyd-Graber, and N. Okazaki, editors, Proceedings of the 61st
[170] C.Singh,J.Morris,A.Rush,J.Gao,andY.Deng.TreePrompting: Annual Meeting of the Association for Computational Linguistics
EfficientTaskAdaptationwithoutFine-Tuning. InH.Bouamor, (Volume 1: Long Papers), pages 2609–2634, Toronto, Canada, Jul
J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference 2023.AssociationforComputationalLinguistics.
onEmpiricalMethodsinNaturalLanguageProcessing,EMNLP’23, [189] M.Wang,D.Zheng,Z.Ye,Q.Gan,M.Li,X.Song,J.Zhou,C.Ma,
pages6253–6267,Singapore,Dec2023.AssociationforComputa- L.Yu,Y.Gai,T.Xiao,T.He,G.Karypis,J.Li,andZ.Zhang.Deep
tionalLinguistics. GraphLibrary:AGraph-Centric,Highly-PerformantPackagefor
[171] C.Singh,J.X.Morris,J.Aneja,A.M.Rush,andJ.Gao.Explaining GraphNeuralNetworks,2020. arXiv:1909.01315.
PatternsinDatawithLanguageModelsviaInterpretableAuto- [190] X.Wang,J.Wei,D.Schuurmans,Q.Le,E.Chi,andD.Zhou.Self-
prompting,2022. arXiv:2210.01848. ConsistencyImprovesChainofThoughtReasoninginLanguage
[172] A.Srivastavaetal.BeyondtheImitationGame:Quantifyingand Models. InProceedingsoftheEleventhInternationalConferenceon
extrapolatingthecapabilitiesoflanguagemodels.Transactionson LearningRepresentations,ICLR’23,May2023.
MachineLearningResearch,2023. [191] X. Wang, Q. Yang, Y. Qiu, J. Liang, Q. He, Z. Gu, Y. Xiao, and
[173] I. Stelmakh, Y. Luan, B. Dhingra, and M.-W. Chang. W. Wang. KnowledGPT: Enhancing Large Language Models
ASQA: Factoid Questions Meet Long-Form Answers, 2022. with Retrieval and Storage Access on Knowledge Bases, 2023.
arXiv:2204.06092. arXiv:2308.11761.
[174] A. Strausz, F. Vella, S. Di Girolamo, M. Besta, and T. Hoefler. [192] Y. Wang, N. Lipka, R. A. Rossi, A. Siu, R. Zhang, and T. Derr.
AsynchronousDistributed-MemoryTriangleCountingandLCC KnowledgeGraphPromptingforMulti-DocumentQuestionAn-
withRMACaching.InProceedingsoftheIEEEInternationalParallel swering,2023. arXiv:2308.11730.
andDistributedProcessingSymposium,IPDPS’22,pages291–301, [193] Z.Wang,S.Cai,G.Chen,A.Liu,X.Ma,andY.Liang. Describe,
Lyon,France,Jun2022. Explain,PlanandSelect:InteractivePlanningwithLLMsEnables
[175] A. Suhr, M. Lewis, J. Yeh, and Y. Artzi. A Corpus of Natural Open-WorldMulti-TaskAgents.InAdvancesinNeuralInformation
Language for Visual Reasoning. In R. Barzilay and M.-Y. Kan, ProcessingSystems(NeurIPS’23),volume36.CurranAssociates,
editors, Proceedings of the 55th Annual Meeting of the Association 2023.
for Computational Linguistics (Volume 2: Short Papers), ACL ’17, [194] Z.Wang,G.Zhang,K.Yang,N.Shi,W.Zhou,S.Hao,G.Xiong,
pages 217–223, Vancouver, Canada, Jul 2017. Association for Y.Li,M.Y.Sim,X.Chen,Q.Zhu,Z.Yang,A.Nik,Q.Liu,C.Lin,
ComputationalLinguistics. S. Wang, R. Liu, W. Chen, K. Xu, D. Liu, Y. Guo, and J. Fu.
[176] J. Sun, C. Xu, L. Tang, S. Wang, C. Lin, Y. Gong, H.-Y. Shum, InteractiveNaturalLanguageProcessing,2023.arXiv:2305.13246.
and J. Guo. Think-on-Graph: Deep and Responsible Reason- [195] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and
ing of Large Language Model on Knowledge Graph, 2023. D.Zhou.Chain-of-ThoughtPromptingElicitsReasoninginLarge
arXiv:2307.07697. LanguageModels,2022. arXiv:2201.11903.
[177] X. Sun, J. Zhang, X. Wu, H. Cheng, Y. Xiong, and J. Li. Graph [196] X.Wei,S.Wang,D.Zhang,P.Bhatia,andA.Arnold.Knowledge
Prompt Learning: A Comprehensive Survey and Beyond, 2023. EnhancedPretrainedLanguageModels:ACompreshensiveSur-
arXiv:2311.16534. vey,2021. arXiv:2110.08455.
[178] Y. Sun and J. Han. Mining Heterogeneous Information Networks: [197] Y. Wen, Z. Wang, and J. Sun. MindMap: Knowledge Graph
PrinciplesandMethodologies. SynthesisLecturesonDataMining PromptingSparksGraphofThoughtsinLargeLanguageModels,
and Knowledge Discovery (SLDMKD). Springer International 2023. arXiv:2308.09729.
Publishing,2012. [198] Y.Weng,M.Zhu,F.Xia,B.Li,S.He,S.Liu,B.Sun,K.Liu,and
[179] O. Tafjord, B. Dalvi, and P. Clark. ProofWriter: Generating Im- J.Zhao. LargeLanguageModelsareBetterReasonerswithSelf-
plications,Proofs,andAbductiveStatementsoverNaturalLan- Verification.InH.Bouamor,J.Pino,andK.Bali,editors,Findings27
oftheAssociationforComputationalLinguistics:EMNLP2023,pages [218] J.Yu,R.He,andR.Ying. ThoughtPropagation:AnAnalogical
2550–2575,Singapore,Dec2023.AssociationforComputational ApproachtoComplexReasoningwithLargeLanguageModels,
Linguistics. 2023. arXiv:2310.03965.
[199] J.WestonandS.Sukhbaatar. System2Attention(issomething [219] L.Zeit-Altpeter. ForwardLookingActiveRetrievalAugmented
youmightneedtoo),2023. arXiv:2311.11829. Generation. In Proceedings of the AI Summer School at Uni Jena
[200] N. Wies, Y. Levine, and A. Shashua. The Learnability of In- (Poster),2023.
ContextLearning,2023. arXiv:2303.07895. [220] C. Zhang, D. Song, C. Huang, A. Swami, and N. V. Chawla.
[201] T.Wu,E.Jiang,A.Donsbach,J.Gray,A.Molina,M.Terry,and HeterogeneousGraphNeuralNetwork. InProceedingsofthe25th
C. J. Cai. PromptChainer: Chaining Large Language Model ACMSIGKDDInternationalConferenceonKnowledgeDiscovery&
Prompts through Visual Programming. In Extended Abstracts of DataMining,KDD’19,page793–803,Anchorage,AK,USA,2019.
theConferenceonHumanFactorsinComputingSystems,CHIEA’22, AssociationforComputingMachinery.
New Orleans, LA, USA, May 2022. Association for Computing [221] H. Zhang, L. H. Li, T. Meng, K.-W. Chang, and G. V. den
Machinery. Broeck. OntheParadoxofLearningtoReasonfromData,2022.
[202] X.Wu,K.Zhou,M.Sun,X.Wang,andN.Liu.ASurveyofGraph arXiv:2205.11502.
PromptingMethods:Techniques,Applications,andChallenges, [222] J. Zhang. Graph-ToolFormer: To Empower LLMs with Graph
2023. arXiv:2303.07275. Reasoning Ability via Prompt Augmented by ChatGPT, 2023.
[203] Z.Wu,S.Pan,F.Chen,G.Long,C.Zhang,andP.S.Yu. ACom- arXiv:2304.11116.
prehensiveSurveyonGraphNeuralNetworks.IEEETransactions [223] Y.Zhang,Z.Chen,W.Zhang,andH.Chen. MakingLargeLan-
onNeuralNetworksandLearningSystems,32(1):4–24,2021. guageModelsPerformBetterinKnowledgeGraphCompletion,
[204] S. M. Xie, A. Raghunathan, P. Liang, and T. Ma. An Expla- 2023. arXiv:2310.06671.
nation of In-context Learning as Implicit Bayesian Inference. [224] Y. Zhang, J. Yang, Y. Yuan, and A. C.-C. Yao. Cumulative
In Proceedings of the Tenth International Conference on Learning ReasoningwithLargeLanguageModels,2023.arXiv:2308.04371.
Representations,ICLR’22,Apr2022. [225] Z. Zhang, P. Cui, and W. Zhu. Deep Learning on Graphs: A
[205] Y. Xie, K. Kawaguchi, Y. Zhao, X. Zhao, M.-Y. Kan, J. He, and Survey. IEEE Transactions on Knowledge and Data Engineering,
Q.Xie. Self-EvaluationGuidedBeamSearchforReasoning. In 34(1):249–270,Jan2022.
Advances in Neural Information Processing Systems (NeurIPS ’23), [226] Z. Zhang, H. Li, Z. Zhang, Y. Qin, X. Wang, and W. Zhu.
volume36.CurranAssociates,2023. Graph Meets LLMs: Towards Large Graph Models, 2023.
[206] B. Xu, Q. Wang, Z. Mao, Y. Lyu, Q. She, and Y. Zhang. arXiv:2308.14522.
kNN Prompting: Beyond-Context Learning with Calibration- [227] Z.Zhang,X.Wang,Z.Zhang,H.Li,Y.Qin,S.Wu,andW.Zhu.
Free Nearest Neighbor Inference. In Proceedings of the Eleventh LLM4DyG: Can Large Language Models Solve Problems on
InternationalConferenceonLearningRepresentations,ICLR’23,May DynamicGraphs?,2023. arXiv:2310.17110.
2023. [228] Z. Zhang, Y. Yao, A. Zhang, X. Tang, X. Ma, Z. He, Y. Wang,
[207] C.Xu,Q.Sun,K.Zheng,X.Geng,P.Zhao,J.Feng,C.Tao,and M.Gerstein,R.Wang,G.Liu,andH.Zhao. IgnitingLanguage
D. Jiang. WizardLM: Empowering Large Language Models to Intelligence: The Hitchhiker’s Guide From Chain-of-Thought
FollowComplexInstructions,2023. arXiv:2304.12244. ReasoningtoLanguageAgents,2023. arXiv:2311.11797.
[229] J.Zhao,L.Zhuo,Y.Shen,M.Qu,K.Liu,M.Bronstein,Z.Zhu,
[208] C. Yang, D. Bo, J. Liu, Y. Peng, B. Chen, H. Dai, A. Sun, Y. Yu,
and J. Tang. GraphText: Graph Reasoning in Text Space, 2023.
Y. Xiao, Q. Zhang, C. Wang, Y. Guo, and C. Shi. Data-centric
arXiv:2310.01089.
GraphLearning:ASurvey,2023. arXiv:2310.04987.
[230] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min,
[209] J.Yang,G.Xiao,Y.Shen,W.Jiang,X.Hu,Y.Zhang,andJ.Peng.
B.Zhang,J.Zhang,Z.Dong,Y.Du,C.Yang,Y.Chen,Z.Chen,
A Survey of Knowledge Enhanced Pre-trained Models, 2021.
J. Jiang, R. Ren, Y. Li, X. Tang, Z. Liu, P. Liu, J.-Y. Nie,
arXiv:2110.00269.
and J.-R. Wen. A Survey of Large Language Models, 2023.
[210] L. Yang, H. Chen, Z. Li, X. Ding, and X. Wu. Chat-
arXiv:2303.18223.
GPT is not Enough: Enhancing Large Language Models with
[231] X.Zhao,M.Li,W.Lu,C.Weber,J.H.Lee,K.Chu,andS.Wermter.
Knowledge Graphs for Fact-aware Language Modeling, 2023.
EnhancingZero-ShotChain-of-ThoughtReasoninginLargeLan-
arXiv:2306.11489.
guageModelsthroughLogic,2023. arXiv:2309.13339.
[211] Z.Yang,P.Qi,S.Zhang,Y.Bengio,W.Cohen,R.Salakhutdinov,
[232] L.Zheng,W.-L.Chiang,Y.Sheng,S.Zhuang,Z.Wu,Y.Zhuang,
andC.D.Manning. HotpotQA:ADatasetforDiverse,Explain-
Z.Lin,Z.Li,D.Li,E.Xing,H.Zhang,J.E.Gonzalez,andI.Stoica.
able Multi-hop Question Answering. In E. Riloff, D. Chiang,
JudgingLLM-as-a-JudgewithMT-BenchandChatbotArena. In
J. Hockenmaier, and J. Tsujii, editors, Proceedings of the 2018
Proceedings of the Thirty-seventh Conference on Neural Information
Conference on Empirical Methods in Natural Language Processing,
Processing Systems: Datasets and Benchmarks Track, NeurIPS ’23,
EMNLP’18,pages2369–2380,Brussels,Belgium,Nov2018.As-
2023.
sociationforComputationalLinguistics.
[233] D.Zhou,N.Scha¨rli,L.Hou,J.Wei,N.Scales,X.Wang,D.Schu-
[212] F.Yao,C.Tian,J.Liu,Z.Zhang,Q.Liu,L.Jin,S.Li,X.Li,and
urmans,C.Cui,O.Bousquet,Q.V.Le,andE.H.Chi. Least-to-
X. Sun. Thinking Like an Expert: Multimodal Hypergraph-of-
MostPromptingEnablesComplexReasoninginLargeLanguage
Thought (HoT) Reasoning to boost Foundation Modals, 2023.
Models. InProceedingsoftheEleventhInternationalConferenceon
arXiv:2308.06207.
LearningRepresentations,ICLR’23,May2023.
[213] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and
[234] J.Zhou,G.Cui,S.Hu,Z.Zhang,C.Yang,Z.Liu,L.Wang,C.Li,
K.R.Narasimhan.TreeofThoughts:DeliberateProblemSolving
andM.Sun. Graphneuralnetworks:Areviewofmethodsand
withLargeLanguageModels. InAdvancesinNeuralInformation
applications. AIOpen,1:57–81,2020.
ProcessingSystems(NeurIPS’23),volume36.CurranAssociates,
[235] Y.Zhu,X.Wang,J.Chen,S.Qiao,Y.Ou,Y.Yao,S.Deng,H.Chen,
2023.
and N. Zhang. LLMs for Knowledge Graph Construction and
[214] S.Yao,J.Zhao,D.Yu,N.Du,I.Shafran,K.R.Narasimhan,and
Reasoning:RecentCapabilitiesandFutureOpportunities,2023.
Y.Cao. ReAct:SynergizingReasoningandActinginLanguage
arXiv:2305.13168.
Models. InProceedingsoftheEleventhInternationalConferenceon
[236] Y. Zong, O. Mac Aodha, and T. Hospedales. Self-Supervised
LearningRepresentations,ICLR’23,May2023.
MultimodalLearning:ASurvey,2023. arXiv:2304.01008.
[215] Y.Yao,Z.Li,andH.Zhao. BeyondChain-of-Thought,Effective
Graph-of-Thought Reasoning in Large Language Models, 2023.
arXiv:2305.16582.
[216] R.Ye,C.Zhang,R.Wang,S.Xu,andY.Zhang.NaturalLanguage
APPENDIX A
isAllaGraphNeeds,2023. arXiv:2308.07134.
[217] Y.Ye,B.Hui,M.Yang,B.Li,F.Huang,andY.Li.LargeLanguage DETAILED DESCRIPTIONS OF CHAIN SCHEMES
ModelsAreVersatileDecomposers:DecomposingEvidenceand
Questions for Table-Based Reasoning. In Proceedings of the 46th Westudypromptingschemesthatusechaintopologies.We
InternationalACM SIGIRConferenceon ResearchandDevelopment
list them and determine their relationship to our blueprint
inInformationRetrieval,SIGIR’23,page174–184,Taipei,Taiwan,
2023. inTable1.28
A.1 Single-PromptSchemes Chain-of-symbol (CoS) [89] is a multi-prompt scheme
utilizing two prompts for solving spatial planning tasks
Chain-based reasoning can be conducted within a single
in natural language. Prompted in a zero-shot manner, the
prompt, potentially containing multiple in-context exam-
LLM initially generates CoT-like in-context exemplars and
ples.Itisusedinbothtextualandvisualreasoningtasks.
modifies them by replacing the spatial relationships with
In the seminal Chain-of-Thought (CoT) [195] scheme,
symbols.Forexample,thenavigationaltask“StartatbankA.
in-context examples consisting of intermediate reasoning
Therearetwostoresonthemap,storeBandstoreG.Theroadfrom
stepsforagivencomplexquestionareintroducedtoprompt
bankAtostoreBgoesthroughbankC,houseH,andcinemaFto
the LLM to generate a similar multi-step reasoning chain.
store B, totaling 600 meters.” is succinctly converted into the
ThischainisusedtoimprovethequalityoftheLLManswer
symbolsequence“bankA/bankC/houseH/cinemaF/store
comparedtobasicIOprompting.
B, (200 + 100 + 100 + 200 = 600).”. Exemplars simplified in
Zero-shot-CoT was proposed by Kojima et al. [112]. It
thiswayarethenprovidedaspromptsforLLMstoconduct
comes with multi-step reasoning without in-context exam-
CoT-basedreasoning.
ples, by simply prompting the LLM with one sentence,
“Let’s think step by step”, or using other similar statements.
A.2.1 Multi-PromptChainswithDecomposition
This zero-shot prompting improves upon other zero-shot
methodsintermsoftheaccuracyofreasoningoutcomes. Theintroductionofanodefordecompositionatthestartof
SelfAsk [152] is a single-prompt scheme similar to CoT areasoningchaincannotonlydeterminethechain’sdepth,
whichaugmentsthein-contextexampleswithintermediate but also facilitate more detailed reasoning approaches in
questions. Instead of only providing a step-by-step rea- the subsequent sub-steps. This allows for a finer-grained
soning chain in the examples, each step in this chain is resolutionofsub-questions,enrichingtheoverallreasoning
expandedtoposeafollow-upquestionandthenanswerit, process. Apart from the schemes described below, similar
e.g.,“Superconductivitywasdiscoveredin1911.”isaugmented strategiesareappliedinworks[59],[102],[217].
to“Followup:Whenwassuperconductivitydiscovered?Interme- Least-to-MostPrompting[233]growsareasoningchain,
diateanswer:Superconductivitywasdiscoveredin1911”. where the decomposition of complex tasks is conducted in
Plan-and-Solve (PS) Prompting [188] builds a chain thefirstnodeandthesub-tasksaresolvedinthesubsequent
based on a devised plan, harnessing Zero-shot CoT. It first nodes. Prompted with few-shot examples showcasing the
divides the complex task into a list of sub-tasks and then decomposition of questions into lists of sub-questions, the
executes the stepwise plan for the solution. PS can be LLMfirstlygeneratesasequenceofsub-questionsgivenan
extended to PS+ by adding instructions in the prompt to originalquestion.Thenthesolvingprocessstartsbyprovid-
requesttheLLMtoextractvariablesandexplicitlycalculate ing the LLM with example solutions for simple questions.
theintermediatevalues.PS+hasnoimpactonthelengthof Subsequently during each sub-step, the LLM generates the
thechain. sub-solution, given the question for this step and the list
Program of Thoughts (PoT) [41] is a single-prompt ofpreviouslysolvedquestionswiththegeneratedanswers.
scheme similar to CoT which generates code to solve a Thechainterminateswhenthefinalanswerisreturnedafter
question.Ratherthanthenatural-language-basedexamples solvingallsub-problemsinthelist.
used in CoT, the LLM is prompted by code exemplars, Decomposed Prompting [105] is a modular framework
whereeachstatementreferstooneintermediatestepinthe for a detailed decomposition of complex tasks. To generate
reasoning chain. Zero-shot prompting can also be applied, areasoningchain,theLLMispromptedbydemonstrations
e.g., “Let’s write a Python program step by step and return the comprised of sequential question-operation triplets, which
result. Firstly we need to define the variables.”. The final result form “sub-questions”. The growth of the chain follows the
isobtainedbyexecutingthegeneratedcode. autoregressivepatternofCoT,whileeachsub-stepcaptures
the input and passes the output through the contextual
environment. Under the limited scope of the tasks, the
A.2 Multi-PromptSchemes typesofoperationfunctionsarefixed,enablingtheLLMto
conductin-contextlearningforoperationsbeforeinference.
Elicitedbymultipleroundsofprompting,theLLMreason-
ingprocesscanbeachainconsistingofseveralmessages.
A.2.2 Multi-PromptChainswithVerification
Selection-Inference (SI)[51]aimstoaddressmulti-step
logical reasoning problems where all necessary informa- The introduction of verification enables the reasoning
tion is provided within the input context. Each reasoning frameworkstoiterativelyrefinethegeneratedcontext.With
step in SI involves two distinct sub-tasks: selection and thisstrategy,theexecutionofchain-basedreasoningiseffec-
inference. Firstly, the selection sub-task is responsible for tively extended with loops, with conditions on how many
identifying and selecting relevant information needed for times one can loop over a node (based on the number
the subsequent reasoning step, effectively pruning the con- of iterations or some terminal conditions). Apart from the
text.Followingthis,theinferencesub-taskgeneratesanew schemes described below, similar strategies are applied in
intermediate piece of information, thereby performing the works[123],[126],[198].
actual reasoning step. This newly generated information LogiCoT [231] is a zero-shot, multi-prompt framework
then becomes available for use in subsequent reasoning thatleverages iterative verificationtominimizecumulative
steps for the selection sub-task. The length of the chain is errors in the reasoning chain. After generating a reasoning
bounded to a fixed number of steps, and the prompts for chainthroughfew-shotCoT,thisframeworksystematically
bothsub-tasksincluderelevantfew-shotexamples. navigates the reasoning chain with zero-shot prompts to29
pinpoint the first instance where the original reasoning de- of each current step. The nodes of the chain are executed
viated.Itpreservestheaccuratepartofthereasoningchain sequentiallyuntilthefinalresultisreturned.
up to the erroneous node and then prompts the LLM to There are also tool-based chain schemes with no plan-
diagnose and correct this specific misstep, thereby forming ningmodule;theydynamicallyselectthereasoningmethod.
an alternative reasoning chain. This process repeats until a ChatCoT [45] realizes tool-based reasoning chains
fullyverifiedreasoningchainisestablished,witheachnode through multi-round conversations with the LLM. Initially,
passingtheverification. the LLM is shown how to decompose CoT reasoning as a
SELF-REFINE [140] adopts a distinct approach to itera- multi-turn conversation, in order to learn problem-solving
tive refinement compared to LogiCoT. Instead of focusing in a step-by-step, autoregressive conversational way. Addi-
on pinpointing and correcting errors within a reasoning tionally, the knowledge of tools and relevant examples are
chain, it initially generates a complete output and then included in the context to elicit their selection and correct
enters a cyclical process, alternating between generating usage. During inference, the chain proceeds and unfolds
specificfeedbackwithconcreteactionsforenhancementand withoutaprematureplanuntiltheproblemissolved,while
applyingthisfeedbacktorefinetheoutput. the LLM simultaneously selects and executes appropriate
Similar to SELF-REFINE, Reflexion [168] employs the tools during the multi-turn conversation. The conversation
concept of iterative refinement through generating and ap- is constrained to a maximum number of turns, thereby
plying feedback, but introduces a modular approach to the constrainingthemaximumreasoningdepth,whileensuring
process, segmenting the refinement into specialized stages. allpreviousturnsarekeptwithinthecontextwindow.
It starts with an initial output by the LLM, followed by
an evaluation through task-specific grading functions, and
APPENDIX B
thengeneratesinformativefeedback,whichissubsequently
DETAILED DESCRIPTIONS OF TREE SCHEMES
applied. This reasoning chain continues looping until the
evaluationstageconfirmsthattheoutputiscorrect. Wenextanalyzeworksthatharnesstreesastheprompting
Reasoning Graph Verifier (RGV) [35] is a framework topology. We structure the discussion based on the har-
that addresses arithmetic questions. Their approach is a nessedtopologyvariants,seeFigure9.
variantofCoT-SC,inwhichtheyemployatrainableverifier
moduletodecideonthebestCoTsolution. B.1 TreesofChains
Whileharnessingtreesaspromptingtopologieshasbeenex-
A.2.3 Multi-PromptChainswithExternalTools plicitlyestablishedintheworksbyLong[133]andYao[213],
To better integrate multiple execution methods, some this idea has been present earlier. Specifically, Chain-of-
schemesopttodeviseaplanthatspecifiestoolsforhandling Thought with Self-Consistency (CoT-SC) [190] is an early
eachsub-task,beforeexecutingthereasoningchain. schemethatharnessesthetreestructuretoacertaindegree.
Plan,VerifyandSwitch(PVS)[131]iterativelyengages Here, multiple CoTs originate from the same initial (root)
prompt,forminga“treeofchains”.Thechainprovidingthe
in planning, verification, and switching among different
best outcome to the initial question, is selected as the final
methods until the acquisition of the final answer. Initially,
answer.
the LLM is prompted with definitions of three chain-based
reasoningmethods,whichincludeCoT,PoT,andamethod
using the solution of linear equations to represent the rea- B.2 Single-LevelTrees
soningprocess(namedEoT).Then,atthefirststageofeach
Skeleton-of-Thought (SoT) [148] is a prompting scheme
iteration, the LLM selects a reasoning method according
aiming to reduce the end-to-end generation latency of
to the input question and subsequently follows in-context
LLMs,causedbytheirinherentsequentialdecoding.Instead
examples on the application of the method to complete a
ofgeneratingonelongcontinuousanswer,thisschemeuses
reasoning chain accordingly. Then, the verification module
adivide-and-conquerapproach.Inafirstprompt,theLLM
acquiresconditionalinformationandtheintermediatevari-
is instructed to generate a skeleton of the answer, i.e., a
ables from the sub-steps of the chain and executes calcula-
list of points that are independently answerable. Then, for
tions, validating the correctness of the final answer. If the
each of these points, a new prompt is issued in parallel to
verificationfails,thenextiterationisactivated,inwhichthe
answerjustthisspecificpartofthequestion.Asthesepoints
LLM selects a new method from the non-selected ones and
are processed in parallel, the overall latency is reduced.
generates a new chain without memories of the previous
One can additionally include a prompt at the beginning
reasoning process. The planning module facilitates the dy-
which lets the LLM decide on solving the question via
namicswitchofreasoningmethods,therebyintegratingthe
independent points, or – whenever reasonable – by using
strengthsofdifferentparadigms.
asinglecontinuousanswer.
Chameleon [136] is a reasoning framework that pro-
vides the LLM with access to various external tools, such
B.3 k–AryTrees
as programming interpreters and table readers. Given the
question, tool descriptions, and tool usage examples, the First, the Tree-of-Thought (ToT) design by Long [133]
LLMgeneratesaplanforthereasoningchain,inwhicheach utilizes a tree structure to decompose a problem into sub-
nodecorrespondstoatoolusedinthesub-step.Duringthe problemsandsolvethemusingseparateLLMprompts.Af-
executionofthechain,theoutputofpreviousstepstogether tertheLLMsuggestspossiblenextstepsandcorresponding
withthecachedtoolexecutionresultsaretakenastheinput partial solutions, a checker module decides if any of these30
solutions is valid, whether it can be selected as the final B.3.2 Post-ToTSchemes
one,orwhetheritshouldbacktracktothepreviousstep.All Different schemes have extended the initial ToT designs.
issued prompts and answers are explicitly stored as a tree Algorithm of Thoughts (AoT) [166] is a single-prompt
structure and navigated through using a controller module. approach that utilizes in-context examples formulated in
The LLM prompting is only used to generate the next an algorithmic fashion. Instead of providing step-by-step
individualsteps(i.e.,hops)inthistree,whereastheoverall examples as in CoT prompting, AoT harnesses algorithmic
problemsolvingprocessiscoordinatedbythecontroller. reasoning steps in the examples, that explore the solution
Tree of Thoughts (ToT) by Yao et al. [213] differs from space(tree)witheitherDFSorBFS.AsopposedtoToT,AoT
theaboveToTapproachinusingtheLLMitselfasasolution usesonlyasingleprompt.
evaluator with access to all generated solutions, instead of Tree of Uncertain Thought (TouT) [145] extends ToT
using a programmed or learned evaluator module. This with local “uncertainty scores” by incorporating the vari-
allowstoratestatesindividuallyorvoteacrossintermediate ance of multiple LLM responses into the state evaluation
solutionstoselectthemostpromisingonetocontinuewith function.
the search. Both mentioned ToT approaches are a general- Tree-of-Mixed-Thought (TomT) [91] uses ToT-based
izationoftheIO,CoT,andCoT-SCpromptingschemes. promptingtoanswerquestionsonvisualscenegraphs,e.g.,
“Does the red chair have the same number of legs as the table
in front of it?”. The LLM is tasked to generate Python code
that solves the question using a set of provided functions
B.3.1 Pre-ToTSchemes
to extract and reason on the data of the scene graph. This
prompting scheme uses a DFS variant of ToT where each
Thought Decomposition [205] is a multi-prompt scheme
individual node generates up to s consecutive reasoning
based on stochastic beam search and self-evaluation. At
each level (reasoning step), n new intermediate nodes are steps, i.e. lines of code, opposed to the original ToT where
generatedforeachofthe(usually)kinputnodes.Eachnode s = 1. Evaluating the correctness of the reasoning steps
ofthissetofnknodesisevaluatedandthenthesetispruned is done as in ToT for each node, where one checks if the
downtok outputnodesviasamplingwithstochasticbeam generated code so far can (1) run correctly, and (2) the
interactions with the scene graph are consistent with the
search.Forboththegenerationandtheevaluationprompts,
existingelementsinthescene.
few-shot examples are used with a focus on textual or
Tree of Clarifications (ToC) [106] enables to answer
program-aided reasoning, depending on the dataset used
ambiguous questions by first retrieving pertinent external
forevaluation.
informationandthenrecursivelypromptinganLLMtocon-
Creswell and Shanahan [50] describe an extension of
structatreeofdisambiguationsfortheinitialquestion.This
the chain-based Selection-Inference [51], which is used to
tree is explored using BFS and after no more ambiguations
answermultiple-choicequestionsbychainingtogethercor-
exist,alongformanswerisgenerated,bycombiningallthe
rect reasoning steps over a predefined context, i.e., a set
previouslyexplorednodes.
of statements (the provided statements are sufficient to
Tree Prompting [170] is a high-level approach that
derivetheanswertothequestion).Thespaceofallpossible
proposes the training of a binary decision tree to clas-
reasoning chains forms a tree rooted at the initial question.
sify text during inference. Several mechanisms to construct
Each node represents a reasoning step that is derived by
the prompts that make up the nodes of the decision tree
(1) selecting a subset of statements from the context and
during learning are proposed: prompts based on few-shot
then (2) inferring a new statement, which is then added to
examples, human curated instruction prompts, dynamic
the context. For both steps, the authors use separately fine-
prompts constructed with discrete prompt search methods
tunedLMs.Thistreeisexploredusingbeamsearch,where
like iPrompt [171], and kNN prompting [206], which con-
another fine-tuned LM assesses the value of the current
structs new nodes based on the nearest neighbors. kNN
node, and the search ends as soon as a halter-LM decides
prompting can result in multiple prompts per decision tree
that the question can be answered with the current context
nodewiththeothermechanismsusingonlyasingleprompt.
andthenanswersit.
Additional improvements can be made by employing tree
Dynamic Least-to-Most Prompting [58] extends least- ensemblessuchasrandomforests[120]orgradient-boosted
to-most prompting with a tree-based problem decomposi- trees[64].Duringinferencethenumberofpromptsisbound
tion and a dynamic external tree-based few-shot example by the depth of the decision tree, where as thetraining can
selection.Thegoalistoturnnaturallanguagequestionsinto beresourceintensive.
formalrepresentationssuchasSPARQLqueries.Initiallythe
input question is decomposed into sub-problems using a
B.4 Analysis&ComparisonofDesigns
series of prompts. This process yields an arbitrary decision
treefortheinputincontrasttoachainintraditionalleast-to- We now broadly discuss and analyze tree designs with
most prompting. This decision tree is used to dynamically respect to different aspects of our blueprint. A detailed
select few-shot examples by externally matching that tree analysiscanbefoundinAppendixE.2.
against a precomputed decision tree for examples. In the
final step, the decision tree is linearized into a sequence B.4.1 Topology&ItsConstruction
ofsub-problemswithincreasingcomplexitywhicharethen Thekeynovelarchitecturalfeatureoftreeschemesisexplo-
translated by using prompts enhanced with the respective ration of a step, i.e., the ability to generate multiple new
examplesselectedinthesecondstep. steps based on a given single one. The vast majority of31
treeschemesaremulti-prompt.Mostmulti-promptschemes are not expert-level, they can be refined in the next step.
use a dynamic approach to building the tree topology. The Inthelaststep,theLLMispromptedtocomeupwithnew
detailsofhowthetopologyisexactlyshapeddependonthe solutionsfortheinitialproblembasedonthesolutionsfrom
specific question. For most multi-prompt approaches, the therelatedproblems.Additionally,theLLMispromptedto
usercanadaptthetreetopologytoacertaindegree,i.e.,by derivehigh-levelplanstosolvetheinputproblemusingthe
varying the branching factor and limiting the depth of the solutions of the related problems. This process can also be
tree. extended to recursively generate further related problems,
which yields in the general form a double tree structure
B.4.2 Performance withdepthk.
Increasing the branching factor often leads to more di- Socratic Questioning [154] is a prompting scheme that
versity of outcomes, which can be beneficial for accuracy, models recursive exploration of the thought space using a
but also increases the number of prompts, i.e., compu- tree structure. Hereby, the original question is recursively
tational cost. The most advantageous branching factor is decomposed into sub-tasks until all tasks can be solved
hard to find and often depends on the specific problem with high confidence. These results are then aggregated
to solve. Easily decomposable problems may benefit less and propagated back up the tree to answer the original
from more branching than complex problems. Specifically, question. This results in an overall double tree reasoning
morecomplicatedproblemsprofitmorefromdecomposing topology.Additionally,theapproachalsoprovidesamodel
them into many/diverse sub-problems (e.g., this ensures togenerateanimagecaptionrelatedtothetextpromptand
enough diversity for self-consistency to work better). In thusallowsmulti-modalreasoning.
contrast,aquestionthathasclearlyonlytwosub-partsdoes
C.2 DirectedGraphs
notbenefitfrommanymoresubdivisions,astheadditional
branches can either be redundant or wrong. Single-prompt Someschemesembraceageneraldirectedgraphmodel.
approaches can perform better on some problems than Graph of Thoughts (GoT) [10] uses a multi-prompt
multi-promptapproaches,whileusingonlyasingleprompt approach to improve the LLM problem solving perfor-
comparedtopossiblyhundreds. mance by decomposing a given task into sub-tasks that
form a graph. This decomposition is specified as a Graph
of Operations (GoO). The GoO coordinates how the LLM is
APPENDIX C
promptedandhowtheresults,whichformaseparategraph
DETAILED DESCRIPTIONS OF GRAPH SCHEMES called Graph Reasoning State (GRS), are further used in the
We finally describe schemes with topologies beyond trees reasoningprocess.
or chains. We observe that they can be further grouped Graph of Thought [119] presents a multi-prompt ap-
intosubclasses,basedontheharnessedclassofgraphs.We proach where a graph is constructed recursively in a DFS
picturedifferenttypesofgraph-basedschemesinFigure9. manner by starting at the question node that represents the
question to be answered by the LLM. From this node,
possible reasoning paths are generated by the LLM. For
C.1 SpecialClassesofGraphs
each path, new nodes, i.e., intermediate reasoning steps,
Differentschemesharnesscertainclassesofgraphs. are generated by the LLM and are then used to grow the
Branch-Solve-Merge (BSM) [162] employs a 1–level graph.Tolimitthesizeofthegraph,theschemeusesadepth
doubletreestructuretofirstdivideaproblemintoindepen- limit and requires a set of condition nodes to be provided at
dentlysolvablesub-problems,andthencombinethemintoa the start. These nodes represent axioms for the reasoning
finalsolution.ThefirstpromptinstructstheLLMtopropose processthatforminitialnodesinthereasoningpaths.After
sub-problems, which are then solved independently. The the graph construction, a path from the condition nodes
final prompt instructs the LLM to merge the results of the to the question node is searched, and a checker module
sub-problemsintoasingleoutput. validateseachreasoningstepalongthispath.Thischecking
Thought Propagation (TP) [218] employs a multi–level isdonebyaseriesofcallstotheLLMandonlypasseswith
doubletreestructureforpromptingtheLLM.Thisapproach apositivescore,ifallcallsagreeonthevalidityofthestep.
followstheideathatmulti-stepproblemsareeasiertosolve If no valid path is found, the graph is updated to enable
with access to example solutions of similar problems. This valid reasoning paths. In the first step of the update, all
is done in three steps. In the first step, given an input nodesonlydependingonconditionnodes(viaavalidstep)
problem the LLM is prompted to propose a set of related areaddedtotheconditionset.Then,newnodesandedges
problems. The main idea here is that solving these related areaddedtotheexistinggraphinthesameDFSmanneras
problemscanthenbeusedasreferenceexamplesforsolving whencreatingthegraphfromscratch.Thesegraphupdates
the input problem instead of reasoning from scratch. Ad- arerepeateduntilavalidpathtothequestionnodeisfound
ditionally, solving similar problems can lead to high-level andthereforeasolutiontotheinitialquestion.
plans for the input problem and allows TP to rectify errors Graph-of-Thought [215] describes a two-stage frame-
during planning. The LLM is prompted using in-context work to answer multi-modal questions, i.e., textual ques-
examples to generate related problems for both situations. tions accompanied by images. In the first stage, the model
Inthesecondstep,theLLMispromptedtosolvetheinput generates natural language rationales based on the input
problem as well as the related problems. For this prompt, text, which provide additional context and knowledge to
existing prompting techniques, e.g., CoT, ToT, etc., can be support answering the given question. This rationale gen-
used. Even though the solutions to the related problems eration is learned as part of the overall model pipeline.32
In the second stage, these rationales are then appended to toreviseandinfersolutionsbasedonthesegraphs.First,the
the initial question and passed again through the model to model has to be trained in advance and can be applied to
predictananswer.Thepredictionconsistsofmultiplesteps. solve problems with clear intermediate steps and solution
First,subject-verb-objecttriplesareextractedfromtheinput states, e.g., Game of 24 or 2x2 pocket cube. This model is
questionusingtheStanfordsOpenIEsystem[3].Usingthese thenusedtoinferagraphofthoughtwherepathsleadingto
triples, a Graph-of-Thought is constructed by first convert- asolvedstateoftheproblemareconvertedtotextandfedto
ing each triple to a 3–node path and then merging similar anLLMasassistingknowledgetosolvetheinitialquestion.
nodes using the Stanford CoreNLP system [142]. Next, the TheLLMisinstructedtoreviewthestepsandrefinethemif
inputtextandtheaccompanyingimageareencodedusinga necessary, hereby iteratively refining the solution graph by
Transformerencoderandavisionencoder,respectively.The applyingthefirststagetoreplaceincorrectnodes.
Graph-of-Thought is encoded using a GAN. The resulting ResPrompt [99] is a single-prompt approach extending
featuresfromtextandimagearethencombinedusingcross- CoT by augmenting the few-shot examples with residual
attention and passed through a gated fusion layer before links, i.e., previously derived results. If a current reason-
passingthemtothefinalTransformerdecoderthatpredicts ing step depends on previous results, these results are
therationalesinthefirststageandtheanswersinthesecond referenced verbatim in parentheses, effectively forming an
stage. The scheme uses the pre-trained T5 [158] model and arbitrary reasoning graph. Later stages can explicitly rely
fine-tunesitfortherationalegeneration. onmultiplepreviousresultsasopposedtoasingleprevious
ControlLLM[132]isaframeworkthatallowsanswering result.
multi-modal queries, e.g. understanding and generating
images,videos,andaudio.First,theLLMispromptedtode- C.3 Hypergraphs
compose the query into predefined sub-tasks, e.g., ”image-
Finally, we also consider a hypergraph, which generalizes
processing”or”video-generation”.Hereby,theLLMalsoin-
a graph by enabling edges to connect arbitrary subsets
fersappropriateinputandoutputtypes,e.g.,textorimage,
of nodes instead of being links between just two nodes.
fromapredefinedsetforeachsub-task.Inanextstep,aso
We include hypergraphs in the taxonomy, because prelimi-
calledToolGraphisconstructedbyusingthegeneratedsub-
nary works already harness them for multi-modal prompt-
tasks and the input resources, e.g. video or text, as nodes.
ing[212].
Connectionsbetweenthenodesaredrawnforallcompatible
Hypergraph-of-Thought (HoT) [212] is a multi-modal
input/output types. Nodes corresponding to sub-tasks or
reasoning paradigm modeling the thought process as a
resources are called tool nodes or resource nodes respectively.
hypergraph. First, a graph-of-thoughts as in [215] is con-
Inanextstep,thegraphissearchedinaDFSmanneruntil
structed.Thenatextualhypergraphisconstructed,sharing
a solution, i.e., path from input to output resource node,
the same nodes. The hyperedges are then defined as node
is found. The authors evaluated different heuristics for the
triples, e.g., ”(Lionel Messi, place of birth, Rosario)”. Addi-
DFS,i.e.,greedysearch,beamsearch,adaptivebeamsearch
tionally, a visual hypergraph-of-thought is constructed by
and exhaustive search. The graph traversal is guided by
performing k-means clustering on image patches, where a
the LLM and in each step the LLM is prompted to rate the
cluster corresponds to a hyperedge. Both hypergraphs are
relevanceoftheconnectedtoolnodesforsolvingtheoverall
thenencodedandcombinedtoperformgraphlearning.
task, this rating is then used as the heuristic. In a last step,
thefoundsolution,i.e.,pathfrominputtooutputinthetool
C.4 Analysis&ComparisonofDesigns
graph, is executed to retrieve the requested output. Finally,
the LLM is prompted to summarize the result for the user. We now broadly discuss and analyze graph designs with
The overall approach is similar to GoT [10], with the main respect to different aspects of our blueprint. A detailed
difference being that each node can access and use a tool analysiscanbefoundinAppendixE.3.
andthegraphtraversalusesaLLM-basedheuristic.
Cumulative Reasoning [224] describes a paradigm to C.4.1 Topology&ItsConstruction
solve multi-step problems by iteratively constructing a di- Firstly, the considered schemes exhibit a blend of single-
rected acyclic graph (DAG). A proposer module suggest a and multi-prompt aspects, allowing for a high degree of
next deduction step based on any previous steps, a verifier flexibility and control over the prompting process. This
module evaluates the proposed step and finally, a reporter is evident in the diverse approaches taken by different
module checks if a valid solution has been reached and schemes such as GoT, ControlLLM, and Cumulative Rea-
otherwise a next step is proposed. In this approach, one soning, each offering unique ways of constructing and
manually writes problem specific prompts that follow this utilizing graphs for problem-solving. Secondly, the user’s
paradigm.Onealsomanuallyprovidestheimplementation controloverthetopologyofthegraphissignificantinmost
onhowtheindividualmodulesinteract,i.e.,howtheDAG schemes, enabling customization of the reasoning process
is formed. Conceptually, this approach is an extension of basedonspecificneeds,suchassettingbranchingfactorsor
ToT as it allows each step to use all previously derived definingthedepthofthegraph.Thirdly,theroleoftheLLM
results,meaningdifferentsubtreescanbeconnectedtoform inthesegraph-basedschemesismultifaceted,involvingthe
aDAG. generation, evaluation, and modification of nodes within
EverythingofThoughts(XoT)[57]isatwo-stageframe- the graph, as well as determining the conclusion of the
workutilizingareinforcementlearningmodeltrainedusing reasoningprocess.Lastly,thereisanotablevariationinthe
MonteCarloTreeSearchonspecificproblemsetstogenerate degree of user and LLM influence on the topology across
graphsofthoughtsaswellasiterativelypromptingtheLLM different schemes, with some allowing direct user control,33
while others rely on predefined heuristics or the LLM’s for Wikipedia and Wikidata) [87], ProofWriter [179], En-
decision-makingcapabilities. tailmentBank [52]. SocialQA [172] focuses on daily social
events.
C.4.2 Performance Logical reasoning tasks aim to test the ability to under-
The considered works universally show improvements stand contextually cause-and-effect relationships. CauseEf-
in effectiveness of graph-based prompting schemes over fect[172]tasksthemodelwithdeterminingthecausalevent
chains and trees across various tasks, suggesting a promis- givendescriptionsoftwoevents.
ingdirectionforfutureresearchandapplicationinthefield
ofAIandmachinelearning. D.3 SymbolicReasoningandOtherDomains
Thereareseveraltasksinvolvingmanipulationsofsymbols
APPENDIX D in different domains. For Last Letter Concatenation [195],
the model concatenates the last the letters of two given
BENCHMARKS
words into an abbreviation. Coin Clipping [195] tasks the
In this section, we introduce benchmarks designed to test model with determining, whether the head side of a coin
the reasoning capabilities of LLMs. These benchmarks will facesupafteragroupof”flip/notflip”commands.
be instrumental in the performance analysis discussion in Inthedomainofspatialplanningtasks,BrickWorld[89]
thefollowingsection. asks the model to acquire a specific brick after sequen-
tially grabbing group of bricks in 2D/3D scenarios. SCAN
D.1 ArithmeticReasoning (Simplified version of the CommAI Navigation tasks) [115]
consists of sets of compositional navigation commands
Arithmeticreasoningtasks,alsocalledmathwordproblems,
pairedwiththecorrespondingactionsequences.Othertext-
focusonnarrative-basedmathematicalquestionswherethe
based spatial tasks are also popular, such as NLVR-based
model must extract and solve numerical equations from
Manipulation[175]andSPARTUN[143]
a given text-based scenario. In the datasets, each item
Semantic parsing datasets focus on the ability of
is presented as question-answering (QA) pairs, in which
the models to generalize from knowledge about com-
the answers are formatted as direct answers or multiple
ponents. Examples include CFQ (Compositional Freebase
choice. Frequently used datasets include GSM8K (Grade
Questions) [103], COGS (COmpositional Generalization
School Math) [48], SVAMP (Simple Variations on Arith-
Challenge based on Semantic Interpretation) [109], and
metic Math word Problems) [151], MAWPS (MAth Word
Alchemy[134].
ProblemS)[113],MultiArith[161],AddSub[88],DROP(Dis-
Other benchmarks are intended for special domains,
crete Reasoning Over the content of Paragraphs) [60] and
such as coding (CodeNet [153]) or pedestrian traffic be-
AQUA[125].
havior (PIE (Pedestrian Intention Estimation) [159]) for au-
Based on these benchmarks, datasets enhanced for
tonomousdriving.
morecomprehensivetextualunderstandingandmorecom-
Benchmarks that target multiple domains like MT-
plex calculations were developed. GSM-hard [67] extends
Bench [232] and MMLU (Massive Multitask Language Un-
GSM8K by introducing calculations of much larger num-
derstanding) [85] measure how broad the knowledge of a
bers. AQUA-RAT [125] requires more detailed rationales
model is. Sometimes just multiple areas of a single domain
in answering than AQUA. MathQA [2] selects the QAs
aretargeted,forexamplewithScienceQA[135].ASQA(An-
fromAQUAandprovidesoperationprograms.MATH[86]
swerSummariesforQuestionswhichareAmbiguous)[173]
is built on the problem sets from math competitions and
focuses on ambiquous questions in the context of long-
includes LATEX representations. ALGEBRA [84] focuses on
formquestionanswering.SeveralbenchmarkssuchasALF-
thealgebraicdomain.
World[169]andVQA-V2(VisualQuestionAnswering)[76]
Researchers take specific domains into consideration.
look into visual tasks, while others target logical reasoning
FinQA (Financial Question Answering) [42] involves the
such as LogiQA [128] and FOLIO [82]. AutoTNLI [114]
analysis and interpretation of financial data, reports, and
consistsofcounterfactualentity-basedtables.
tables.ConvFinQA[43]presentsthefinancialquestionsina
Other evaluation methods also include puzzles, likethe
conversational manner. To test the comprehension of struc-
Game of 24 (given four numbers, find the combination of
tureddata,TabMWP[137]collectsmathwordproblemsthat
thosenumbersandthefourbasicmathematicaloperations,
involvetabulardata.
so that the end result is 24), the 8-Puzzle (3x3 board with
a missing piece, goal is to slide the pieces to their target
D.2 CommonsenseandLogicalReasoning
location)or2x2pocketcube(smallerversionoftheRubik’s
In commonsense reasoning, benchmarks test the abili- cube).
ties to understand text and to generate responses that
align with human-like commonsense knowledge. Follow- APPENDIX E
ing that principal, group of datasets composed of multi- DETAILED ANALYSES
hop questions are constructed, including HotPotQA [211],
E.1 Chain
StrategyQA [71], MuSiQue [184], Bamboogle [152], and
CommaQA-E (Communicating with Agents for QA) [104]. E.1.1 PerformanceAnalysis
Apart from the complexity of the questions, more QA Chain-structuredpromptingmethodshavebeenappliedto
datasets incorporating stepwise explanations for the an- solve questions in diverse fields. We conduct a qualitative
swers are built, such as 2WikiMultiHopQA (2Wiki stands comparisonofthesemethodsindifferentdomains.34
E.1.1.1 Arithmetic Reasoning: IO prompting ex- CommaQA-E [104] dataset and a 30% improvement on
hibits very low performance on mathematical tasks. Com- 2WikiMultiHopQA and Musique, which stem from the de-
paredwithIOprompting,CoTattainsanaccuracyofaround signedretrievalfunction.Withtheretrievalandcalculation
60% (40% higher) on GSM8K [48], 80% (10% higher) on tools, ChatCoT obtains a 20% gain on the HotPotQA [211]
SVAMP [151] and 90% (14% higher) on MAWPS [113]. The datasetcomparedwithCoT.
accuracy of the rationales significantly increases with the Refinement improves incorrect thoughts, leading to
scale of LLMs. Zero-shot-CoT does not outperform CoT higher reasoning abilities of LLMs. Reflexion’s success rate
on arithmetic tasks, but obtains a 60% gain in accuracy on HotPotQA progressively improves with the accumula-
onMultiArith[161]overIOprompting,simultaneouslyex- tion of refinement iterations and surpasses 70% after six
ceedingIOpromptingonGSM8K,AQUA[69]andSVAMP attempts, while CoT maintains a steady success rate of
tasks. Compared with CoT, PoT with few-shot examples around 30%. LogiCoT also exhibits enhancement in rea-
improves on GSM8K and AQUA by more than 8% and soning tasks related with contextual understanding and
4% on SVAMP. For the financial datasets FinQA [42] and causality inference, achieving an improvement of around
ConvFinQA [43], few-shot PoT reaches a roughly 20% im- 10%ontheSocialQA[172]andCauseEffect[172]datasets.
provementoverCoT.IncomparisonwithCoT,schemesthat E.1.1.3 SymbolicReasoning: Inthetaskofconcate-
employdecompositiondemonstrateamodestadvantageon nating the last letters of words, CoT achieves almost 100%
math reasoning tasks. Least-to-Most Prompting increases accuracyoncasesthatwerepartofthein-contextexemplars
the accuracy on DROP [60] by 10% because of the strong and around 70% accuracy on out-of-domain (OOD) cases,
decomposability of the problems in the dataset. Although while IO prompting shows nearly no accuracy on both
Least-to-Most Prompting only shows a slight average im- cases. For coin flipping, IO prompting with a 540B LLM
provement on GSM8K, it significantly enhances accuracy achieves nearly 100% on in-domain cases, but is inferior to
(by5%)forproblemswithinGSM8Kthatrequiremorethan CoT(50%vs90%)onOODcases.Theproblem-solvingrate
5 inference steps to solve. PS+ prompting yields at least of Zero-shot-CoT is lower than CoT, but higher than IO on
5% improvement on MultiArith, AQUA, AddSub [88], and coin flip and last letter concatenation tasks. In the domain
SVAMP, but only a slight gain on GSM8K due to the high of spatial reasoning, CoS exhibites an increase of accuracy
complexity and low decomposability of the problems in ranging from 2% to 40% on Brick World, NLVR-based Ma-
GSM8K. Schemes incorporating reflection nodes show lim- nipulation[175],NaturalLanguageNavigation[78]andthe
itedadvancementinmath-relatedtasks,yetexhibitpromis- spatial QA dataset SPARTUN [143], which completely ex-
ing capabilities in contextual and textual comprehension. ceeds the performance of CoT while using a lower number
Tested on all math-related benchmarks mentioned above, of tokens. The use of decomposition enables multi-prompt
PVS exhibits a 5.5% improvement on average. Moreover, chain schemes to outperform CoT on OOD cases of the
PVSshowsanimprovementofaccuracylargerthan10%on lastletterconcatenationtask,wherethelengthofthetested
ALGEBRA [84] and GSM-hard [67] that contain algebraic words exceeds those in the exemplars. The accuracy of the
calculations with large numbers, due to the integration of LLM gradually decreases from 94% to 74% as word length
PoT andEoT methods.With theplug-in ofthe tablereader increases from 4 to 12. In contrast, CoT’s accuracy drops
tools, Chameleon increases the accuracy of answering by from 70% to 32%. The accuracy of the zero-shot method
8% over CoT in the math questions represented in tabular PS+ is still 10% higher than few-shot CoT’s. Moreover,
contextontheTabMWP[137]dataset. in the more complex task of concatenating the kth letter,
E.1.1.2 Commonsense Reasoning: CoT outper- DecomposedPromptingshowshigheraccuracy(morethan
forms IO prompting in terms of accuracy (75.6% vs 69.4%) 96%) than CoT (around 70%) and Least-to-Most (around
on the StrategyQA [71] dataset, and performs 20% better 12%).
on sports understanding tasks than experienced humans. E.1.1.4 Reasoning in Special Domains: The per-
Zero-shot-CoTdoesnotprovideperformancegainsoncom- formance evaluation of the schemes in special domains is
monsense reasoning benchmarks over IO prompting. The only selectively and qualitatively detailed here, due to the
evaluation of SelfAsk focuses on multi-hop questions that non-uniform, artificially designed metrics utilized in the
require composing multiple facts to solve and improves assessment. Refinement schemes perform well in the code-
over CoT by smaller margins on 2WikiMultiHopQA [87] relatedtasks.SELF-REFINEsuccessfullyincreasesreadabil-
andMuSiQue[184],butby11%onBamboogle[152]without ityofcodeontheCodeNet[153]dataset,andobtainshigher
heavytokenusage.TestedonbAbIQA[97]andProofWriter speedup than the IO prompting methods on code opti-
OWA[179]datasets,theSIframeworkenabled,onaverage, mization with the PIE [159] dataset. Meanwhile Reflexion
a 7B LLM (58.75% accuracy) to outperform a 280B LLM achieveshighaccuracyincodegenerationonMBPP[8]and
(44.03%) within the scope of CoT frameworks. Taking CoT HumanEval[39].
as the baseline, schemes with decomposition show varying
levelsofperformanceoncommonsensereasoningtasks.The
E.1.2 RepresentationsofTopology&Schedule
problems in SCAN [115] require the conversion of single
textual commands into action sequences, which suits the Listing1showsanexamplechaintopologyfrom[89].
schemesthatincorporatedecompositionnodes.Here,Least-
to-Mostachieves99.7%accuracycomparedtojust16.2%for
E.2 Tree
CoT.WhilePS+showslimitedimprovementonStrategyQA
due to the simple guidance from zero-shot prompting, De- We now broadly discuss and analyze tree designs with
composed Prompting attains a 20% improvement on the respecttodifferentaspectsofourblueprint.35
tree by generating k consecutive reasoning steps for each
User:
Question:Thereareasetofbricks.BrickEisontopofbrickA.Nowwehave node instead of using one node per reasoning step as in
togetaspecificbrick.Thebricksmustbegrabbedfromtoptobottom,and
ifthelowerbrickistobegrabbed,theupperbrickmustberemovedfirst. the ordinary ToT [213]. ToC [106] limits the tree size by
HowtogetbrickA? an upper node limit as well as setting a maximum depth.
Answer:Letusthinkstepbystep:
The user can choose a branching factor k which is used
LLM: to dynamically select k-shot in-context examples for the
1.TogetbrickA,wefindEisontopofA.
2.WefindEisonthetop. generation prompt such that the appropriate number of
3.WeneedtoremovebrickEfirst,asitisontopofbrickA.
4.BrickAisnowaccessibleandcanbegrabbed. child nodes are generated. Finally, in SoT [148] the depth–
SowegettheresultasE,A.
1 tree topology is only influenced by the response of the
Listing 1: An example implicit single-prompt chain topology, encoded with LLM (i.e., how the LLM determines the number of child
text.ItshowstheCoTexampleofaBrickWorldproblemfrom[89]withalinear
nodes).Theusercanonlyindirectlyinfluencethistopology
sequenceoffourconnectednodesfollowedbythefinalsolutionnode.
bypossiblyalteringthenodegenerationpromptitself.
E.2.1.2 Single-Prompt Schemes: Only AoT [166]
exclusively uses a single-prompt tree topology. Here, in-
E.2.1 Topology&ItsConstruction context examples are designed such that the reasoning
followsatreestructure,i.e.,theexamplesshowalgorithmic
E.2.1.1 Multi-Prompt Schemes: The vast majority reasoning steps such as trying out different solutions with
of schemes are multi-prompt [50], [58], [91], [106], [133], DFS on the solution space, to make the LLM search for the
[145], [148], [170], [190], [205], [213]. In all these schemes, solution in a similar tree-structured fashion. Thus, while
thetopologyistoacertaindegreedynamicallyconstructed in-context examples are provided by the user as guidance,
by the LLM (within the set boundaries of the user and the the topology for solving the input question is constructed
defined approach). The role of the LLMs can be summarised dynamicallyon-the-flysolelybytheLLM.
as (1) generating new child nodes, (2) evaluating given E.2.1.3 Beyond Single- and Multi-Prompt: In all
nodes and (3) deciding when we reached a final solution the above schemes, the root node holds the initial ques-
andreportingtheresults.Then,theusertoalargedegreealso tion and remaining nodes represent intermediate or final
controlstheconstructionprocess.InToTbyLong[133],theuser solutions.Contrarily,TreePrompting[170]buildsitstopology
providesasteplimitaswellasacheckermodule(rule-based duringaninitiallearningphase,bylearningthestructureofa
or as a DNN) that decides if a reasoning step is valid or if binarydecisiontreethroughtrainingsamples,withthegoal
backtrackingtoapreviousnodeisnecessary.ToTbyYaoet of text classification. During inference, the tree structure is
al. [213] allows the user to choose the number of samples fixedandindependentofthespecificinput.Theusercanin-
to generate at each node from which the most promising directly influence the tree topology by manually providing
b candidates are kept for BFS, here b defines the branching theprompt-candidatesusedforlearningthebinarydecision
factorofthetree.WhenusingDFS,theuserprovidesavalue treeatthetrainingstage.
threshold, so when a node evaluates to a lower score than
the threshold, one backtracks to the parent and continues
E.2.2 Performance
from there. For both presented exploration schemes (BFS
and DFS) the user provides an upper bound on the total We perform a qualitative performance evaluation of differ-
number of nodes. Thought Decomposition [205] explores enttreeschemes.Weconsiderwhatproblemsordatasetsthe
the tree using stochastic beam search. Hereby, the tree is approachisevaluatedon,howthequalityofanswersscales
constructed level by level and, in each level, k candidates withrespecttothenumberofprompts(ortokens),andwhat
are kept (the beam size) and n new nodes are generated the trade-offs in quality are with respect to different tree
for each candidate. The randomness in the stochastic beam topologiesorreasoningschedules.
search is controlled by a user-defined parameter, similarly, E.2.2.1 Considered Problems & Datasets: ToT by
the user sets the temperature for generating new samples Long[133],ToTbyYaoetal.[213],AoT[166],andTouT[145]
fromtheLLM.Thetotalnumberofstepsinthesearch,i.e., evaluatetheirperformanceonproblemsthatcanbesimply
the tree depth, is capped at 16. In CoT-SC [190], the user decomposed in a tree-structured way, such as 5x5 Sudoku
simply provides the number of CoT samples that should puzzlesinToTbyLong,andGameof24or5x5Crossword
be generated, resulting in a depth–1 tree. Creswell and puzzle for the other three approaches. Thought Decom-
Shanahan [50] use beam search to explore the tree, letting position [205] and CoT-SC [190] perform their evaluation
theuserchoosethenumberofcandidatenodestogenerate on arithmetic reasoning, e.g, GSM8K [48], symbolic rea-
oneachlevel,thebeamsizeaswellasthemaximumdepth soning, e.g., object counting or last letter concatenation, as
of the tree. In Dynamic Least-to-Most Prompting [58], the well as on commonsense reasoning tasks such as Com-
user has no direct influence on the structure of the tree monsenseQA [180] and StrategyQA [71]. The scheme of
topology.Thetopologyisentirelybasedontheresultsofthe Creswell and Shanahan [50] assumes access to an exist-
LLMsdecompositionoftheinput.TouT[145]hasthesame ing context to reason on and evaluate their approach on
user parameters as ToT by Yao et al. [213], but additionally ProofWriter [179] and EntailmentBankQA [52]. Dynamic
uses an uncertainty threshold for DFS to backtrack from Least-to-Most Prompting [58] is evaluated on semantic
nodes where the uncertainty gets too large. In TomT [91], parsing datasets (CFQ [103] and COGS [109]), TomT [91]
theusercaninfluencethetreetopologybylimitingthetotal createanewsyntheticvisualquestionansweringdatasetfor
number of nodes, setting a branching factor and selecting evaluationbycombiningtwoexistingdatasets,ToC[106]is
a block size k which indirectly limits the depth of the evaluated on a long-form QA dataset (ASQA [173]), Tree36
Prompting [170] uses 13 text classification datasets and end-to-endlatencyspeedupsacrossdifferenttaskcategories
SoT[148]isevaluatedondiversequestionsfrom[46],[207]. and LLMs. The authors compare the two step decoding
E.2.2.2 Accuracy vs. #prompts, topology variant, processofSoT,i.e.,creatingshortbulletpointstyleanswers
& reasoning schedule: Long [133] does not discuss per- which are expanded in the second step, against “normal
formance evaluation and the scheme is executed until a decoding”, where a continuous answer is generated in one
solutionisfoundoramaximumof100promptsareissued. go. There are a varying number of bullet points generated
Yaoetal.[213]evaluatetheirapproachonGameof24using with the average being 9 bullet points. The authors as-
BFSonatreeofdepth3withabranchfactorof5andscoring sess the answer quality with the help of two LLM judges
eachnode3times,onCreativeWritingusingatreeofdepth (FastChatandLLMZoo)usingdifferentmetrics.SoTreports
2,branchfactorof5andselectingthebestnodeineachlevel ahightokenoverheadofroughly60xto90x,whichcanbe
byscoring5times,andfinallyonsolving5×5Crosswords optimizeddowntoroughly30x.
with a ToT of depth 5 to 10 using DFS, a branch factor of
5andatmost100DFSsteps.ThoughtDecomposition[205]
E.3 Graph
performs beam search on a ToT with branching factor 16
andbeamsize5,essentiallygenerating5·16=80samplesat We now broadly discuss and analyze graph designs with
eachtreelevel,whilethedepthislimitedto16.Theauthors respecttodifferentaspectsofourblueprint.
find out that increasing the branching factor leads to more
E.3.1 Topology&ItsConstruction
diversity and thus improves performance when using ma-
jorityvotingonthelastlevel.Thisincreasecomesatthecost Mostgraphschemescombinetoacertaindegreesingle-and
ofusingmoretokens.InCoT-SC[190],theauthorsevaluate multi-prompt aspects. GoT by Besta et al. [10] requires a
the number of reasoning chains to sample, i.e., using a tree problem specific graph of operations for solving a problem
of depth one with a branching factor of 1 up to 40 while in a multi-prompting fashion. One has complete freedom
reportingtheaverageover10runs.Samplingmoreoutputs overtheindividualpromptstotheLLMandcanincorporate
improves accuracy, but also increases computational cost; arbitrarysingle-promptingschemes.GoTbyLeietal.[119]
most gains are realized after sampling 5 to 10 CoT-paths dynamicallyconstructsthegraph,atthetimeofthiswriting
in practice. The approach of Creswell an Shanahan [50] no code or prompt templates have been published, so it is
assumesaccesstoacontext,i.e.,asetofstatements,toreason not clear to what extend single-prompting is used. GoT by
on. The individual LMs (selection, inference, two different Yao et al. [215] uses a NLP approach to construct a graph
halter modules, and correctness prediction of a step) need of thought and uses an LM as part of the AI-pipeline to
to be fine-tuned separately in advance, which makes this learn how to use these graphs of thought for predicting an
approach very specific and incurs a setup cost. Dynamic answer.ControlLLM[132]issimilartoGoT[10]inthesense
Least-to-Most Prompting [58] comes with no explicit dis- that all nodes in the Tool Graph correspond to nodes in the
cussionaboutthenumberoftokensused,butthenumberof GoT with access to tools. Cumulative Reasoning [224] is a
promptsvarieswiththedecompositionofthespecificinput conceptual extension of ToT with access to all previously
phraseandsodoesthenumberofin-contextexemplars.The derivedresults,i.e.,nodes,resultinginaDAG.XoT[57]uses
authorsclaimaspeedupofoverafactor2comparedtoCoT- a problem-specific pre-trained RL-model to create a graph
SC prompting as only a single result is generated and no ofthoughtandusestheLLMtoreviewandsummarizethe
majorityvotingisnecessaryasinCoT-SC.AoT[166]usesa results.
singlepromptfortheGameof24(reporting9%outoftoken In graph schemes, the user has significant control over
errors)andtwopromptsforthe5x5Crosswordpuzzle.The the topology and its creation. In BSM [162], the topology is
authors claim that ToT uses many more prompts, i.e., over a double tree with one intermediate level. The user can
100 rsp. 200, for the two problem setups. Hereby, AoT is set either the number of branches, or its upper bound by
reported to perform better than ToT for the Game of 24. specifying the number of sub-problems to generate inside
Theauthorsshowthattheimpactofmoreexplorationsteps a single prompt. In Thought Propagation [218], the user
insidetheexamplesleadstolongerpromptgenerations,i.e., specifies the branching factor in a single prompt to gener-
slowerinferenceforthesamenumberofgames.TouT[145] ate analogous problems, and can predefine the number of
sample 20 LLM responses per ToT node to compute lo- levels of the double tree to grow the topology. The user in
cal variances and claim improved performance to ToT for Socratic Questioning [154] defines the branching factor of
both Game of 24 and 5x5 Crossword puzzles and further the topology by specifying its value, lower limit, or upper
increasing or decreasing the number of samples leads to limit within a single prompt. Furthermore, the user can
degrading performance. In TomT [91], the DFS is limited limit thegrowth of the topologyby defining themaximum
to 30 steps with their ToT-One-Stop and the reference ToT tree depth. In Graph of Thoughts (GoT) [10], the user
implementation using a branching factor of 3. The authors defines the complete topology of reasoning across multiple
report improved accuracy for ToT-One-Stop compared to prompts for a given use case, including, but not limited
ToTwhileusingapproximatelyhalfofthenumberofsteps. to, the branching factor, the maximum depth of tree, and
ToC [106] claims comparable performance to Cot-SC with the number of child nodes to keep. The topology in Graph
using less than 20 LLM calls. According to the published of Thought (GoT) [119] is influenced by a user-defined
results, Tree Prompting [170] enables smaller models to depth limit. In Cumulative Reasoning [224], the user can
outperformlargermodelsandtheaccuracycanbeimproved influence the total number of nodes as the proposer (LLM)
at the cost of more LLM calls; the number of LLM calls for iterativelygeneratesanewnodeuntilauser-specifiedlimit
the experiments is limited to 40. Finally, SoT [148] reports is reached. In ControlLLM [132], the user does not define37
the number of nodes during the task decomposition stage and CoT-SC (sampling 5 times). BSM makes LLaMA-2-
forsolvingagiventask.However,ausermayhaveimplicit 70B-chat competitive with zero-shot GPT-4 for the turn-1
influence on the number of nodes via prompting, e.g., by questions. BSM has similar computational requirements as
addingthephrase“Parseoutasfewtasksaspossible”.The CoT-SC,butyieldshigherscores.Additionally,BSMbenefits
user in Everything of Thoughts (XoT) [57] can indirectly smallermodels(LLaMA-2-7B),whereCoT-SCisineffective.
influence the topology by defining the number of times The benefits of increasing the branching factor seems to be
the LLM is prompted to review and refine the solution saturatedat4,butdependsonthespecificproblem.TP[218]
graph. In ResPrompt [99], the user simply provides in- for shortest-path reasoning makes significant performance
context examples which directly dictate the basis of the gains over all the baselines (IO, CoT, ToT) for all evaluated
topology. Differing from previous schemes, users in two- models,i.e.,PaLM-2,GPT-3.5andGPT-4.Theauthorsreport
stage frameworks such as Graph-of-Thought (GoT) [215] thatthereisonlyamarginaldifferencebetween1-shotand
and Hypergraph-of-Thought (HoT) [212] do not influence 5-shot prompting without further exploration. The perfor-
the topology as the (hyper)graph-of-thought is constructed mance can directly be influenced by the number of layers
bypredefinedheuristics. usedforTP.Hereby,1-layerTPhascompetitiveperformance
Similar to prompting with trees, the LLM may also influ- to ToT with a similar amount of tokens used. Socratic
encethetopologyofpromptingwithgraphswithinuser-defined Questioning[154]iscomparedagainstIO,CoT,CoT-SCand
boundaries. The role of the LLM can be summarized as (1) ToTprompting.Theauthorsreport 3-4%absolutegainover
generatingnewchildnodes,(2)evaluatinggivennodes,(3) othermethods.ThereportedToTaccuracyisrelativelylow
deleting or backtracking from a new node (e.g., based on compared to the other baselines. GoT by Besta et al. [10]
evaluationresults),and(4)determiningwhenthereasoning significantly improves upon IO, CoT, and ToT on all four
processshouldbeconcluded. tasksusingGPT-3.5.TheaccuracyenhancementofGoTover
thebest-performingbaseline(ToT)isachievedwithloweror
E.3.2 Performance comparablenumberoftokens.Theperformanceadvantages
E.3.2.1 Considered Problems & Datasets: ofGoToverthebaselinesincreasewiththeproblemsize.
BSM [162] evaluates its performance on answering
questions from multiple areas by using MT-Bench [232].
TP [218] performs shortest-path reasoning, creative writing
and LLM-Agent Planning on ALFWorld [169]. Socratic
Questioning [154] evaluates their method on both, text-
only datasets (e.g. MMLU [85] and MATH [86]), logical
reasoning questions (LogiQA [128]), as well as multi-
modal visual QA datasets such as VQA-V2 [76] and two
others. GoT by Besta et al. [10] performs integer sorting
with duplicates, set intersections, keyword counting and
document merging. GoT by Lei et al. [119] measures its
performance on the Game of 24, solving higher-degree
polynomial equations as well as deriving formulas for
recursive sequences. GoT by Yao et al. [215] evaluates on
the arithmetic reasoning benchmark GSM8K [48] and on
ScienceQA [135], which contains science questions with
images. ControlLLM [132] provides its own benchmark
containing tasks for image editing and perception, visual
QA and the usage of over 20 different common tools.
Cumulative Reasoning [224] evaluates its approach on
FOLIO [82], AutoTNLI [114], Game of 24, as well as
mathematical reasoning on the MATH [86] dataset.
XoT [57] evaluates on Game of 24, 8-Puzzle and on a
2x2 pocket cube. ResPrompt [99] uses multiple math
reasoning benchmarks for evaluation, i.e., GSM8K [48],
AQUA-RAT [125], MathQA [2] and SVAMP [151], as
well as the sequential reasoning benchmark SCONE-
Alchemy[134]andcommonsensereasoningquestionsfrom
StrategyQA [71]. HoT [212] evaluates their approach on
ScienceQA[135].
In summary, there is no clear set of benchmarks that
is used consistently with all approaches, but most include
mathreasoningproblemsorsomecommonreasoningtasks
suchasGameof24.
E.3.2.2 Accuracyvs.#prompts,topologyvariant,&
reasoningschedule: BSM[162]usesabranchingfactorof5
for its experiments and outperforms zero-shot prompting