Multimodal Pathway: Improve Transformers with Irrelevant Data from Other
Modalities
YiyuanZhang1 XiaohanDing2 KaixiongGong1
YixiaoGe2 YingShan2 XiangyuYue1*
1TheChineseUniversityofHongKong 2TencentAILab
yiyuanzhang.ai@gmail.com, xiaohding@gmail.com, xyyue@ie.cuhk.edu.hk
https://ailab-cvc.github.io/M2PT/
Abstract effective on multimodal data, e.g., CLIP [43] uses image-
textpairstoachievesuperiorperformanceinimagerecogni-
Weproposetoimprovetransformersofaspecificmodal- tion. Transformers’successinmultiplemodalitiesdemon-
itywithirrelevantdatafromothermodalities,e.g.,improve strates their abilities to universally establish sequence-to-
anImageNetmodelwithaudioorpointclouddatasets. We sequencemodeling,giventheinputsequences(i.e.,tokens)
would like to highlight that the data samples of the target which can be seen as the universal embeddings of data of
modalityareirrelevanttotheothermodalities,whichdistin- multiplemodalities[4,20,24,55,58,66]. Forbrevity,we
guishesourmethodfromotherworksutilizingpaired(e.g., refertosuchabilityastheuniversalmodelingability.
CLIP) or interleaved data of different modalities. We pro-
WewouldliketonotethatCLIP[43]representsthesig-
pose a methodology named Multimodal Pathway - given a
nificantsuccessofamethodologythatimprovesamodel’s
targetmodalityandatransformerdesignedforit,weusean
performance on a certain modality (i.e., image) with the
auxiliary transformer trained with data of another modal-
helpofdatafromanothermodality(i.e., text), butthelim-
ity and construct pathways to connect components of the
itation is also apparent - the data samples from the two
twomodelssothatdataofthetargetmodalitycanbepro-
modalities must be relevant (e.g., paired, in this case).
cessedbybothmodels. Inthisway,weutilizetheuniversal
This limitation seems so inevitable that it hardly attracts
sequence-to-sequence modeling abilities of transformers
research interest from the literature. Taking another two
obtained from two modalities. As a concrete implementa-
modalities,imageandaudio,asanexample,wemayexpect
tion,weuseamodality-specifictokenizerandtask-specific
that training with image-audio pairs may help the model
headasusualbututilizethetransformerblocksoftheaux-
recognizeimages(ifwebuildadatasetwithenoughimage-
iliary model via a proposed method named Cross-Modal
audio pairs and re-design the model to use the audio la-
Re-parameterization, which exploits the auxiliary weights
bels as the supervision, just like CLIP does with image-
without any inference costs. On the image, point cloud,
textpairs),butitseemshardtobelievethatapureaudio
video, and audio recognition tasks, we observe significant
dataset would improve a model’s performance on Ima-
and consistent performance improvements with irrelevant
geNet classification without any relevance between the
datafromothermodalities. Thecodeandmodelsareavail-
audioandimagesamples.
able at https://github.com/AILab-CVC/M2PT.
Inthispaper,weproposetoimproveatransformer’sper-
formance on a certain modality even with irrelevant data
fromanothermodality, asshowninFigure1. Themotiva-
1.Introduction tionisthatwecanseeatrainingprocessonacertainmodal-
ityasconvertingthedataofthemodalitytosequences(i.e.,
tokens) and establishing sequence-to-sequence mappings
Transformers[19,22,50,51]arewidelyadoptedinvar-
with the transformer blocks. For a specific modality, we
ioustasksacrossmodalities,suchastextclassification[10],
reckonthatthetrainedmodelhasknowledgeencodedinthe
map construction [65], object detection [4], point cloud
sequence-to-sequence modeling that can facilitate another
analysis [66], and audio spectrogram recognition [24].
modelingprocesswhoseinputsequencesareobtainedfrom
Apartfromnumerousunimodaltasks,transformersarealso
another modality. In other words, apart from the obvious
*CorrespondingAuthor modality-specific knowledge acquired through training on
1
4202
naJ
52
]VC.sc[
1v50441.1042:viXra(Train and test model on the same or a similar task) (Transfer a model to another task of the same modality)
classification
1K train set 21K train set detection segmentation
Single
Modality
valset
image video
Multiple
Modalities
LAION-400M
LAION-2B
LAION-5B point cloud audio
(Paired or interleaved multi-modal data (Irrelevant datasets of multiple modalities)
e.g., CLIP, Flamingo, BLIP) (ours)
Relevant data Irrelevant Data
Figure1. Comparedtotheknownparadigmswhichusewell-alignedmultimodaldata,wefocusonscenarioswherethedatasamplesare
frommultiplemodalitiesbutirrelevant,whichisanopenproblemintheliterature.
aspecificmodality,weseekthemodality-complementary but their main bodies (i.e., transformer blocks) may have
knowledge of sequence-to-sequence modeling in trans- the same structure. 1 For a target model and an auxiliary
formersandwillshowthatitdoesexist. model with the same structure as the main bodies, a layer
inthemainbodyoftheformershouldhaveacounterpartin
However, given a target modality, it seems difficult to
the latter. For example, the counterpart of the Query layer
designthe modeltoutilize some irrelevantdataof another
inthe9thblockofthetargetmodel,whichisthe9thQuery
modality because the data samples of different modalities
layer in the auxiliary model, should exist, and they play a
(e.g., image and audio) may vary significantly in the se-
similar role in the two models. Considering this, we build
mantics, data format, preprocessing, and it seems hardly
theconnectionsbetweenthetwomodelsbyaugmentingev-
possible to design a reasonable objective function since
erylinearlayerinthetransformerblocksofthetargetmodel
there is no relevance between any two samples. In this
with its counterpart in the auxiliary model. In such a con-
paper, we solve this problem by not directly mixing train-
ceptual design, we let the two layers take the same inputs
ingdataoftwomodalitiesbutseeingamodeltrainedona
andadduptheiroutputs,asshowninFigure2(middle).
specific unimodal dataset as a proxy of the corresponding
However, considering the budget on compute and la-
modality and using the model instead. Specifically, given
tency,wedesireanimplementationoftheMultimodalPath-
a target modality and an auxiliary modality, we propose a
way that realizes the pathways and makes good use of the
frameworknamedMultimodalPathwaytoimprovetheper-
auxiliarymodelbutbringsonlymarginaltrainingcostand
formance on the target modality by using two transform-
completelynoinferencecost, comparedtoaregularmodel
ers respectively trained with the unimodal data of the two
trainedonthetargetmodality. Wenotethattheconceptual
modalities. We construct pathways across the components
structuredescribedabovecanbeequivalentlyimplemented
of the target and auxiliary models to exploit the modality-
by a re-parameterization method, which equivalently con-
complementaryknowledgeencodedinthelattertohelpthe
vertstheconnectionsbetweenmodelstructures(i.e.,linear
former. Note pathway is an abstract concept that may re-
layers)intoconnectionsbetweenthetwomodels’weights.
fer to any connection between the two models. We name
Specifically, we construct a pathway for each target linear
the model as Multimodal Pathway Transformer (M2PT)
layer by adding the corresponding weights of its counter-
forbrevity.
This paper proposes a simple yet effective implementa- 1Exceptfortransformers,arecentwork,UniRepLKNet[18],reveals
tion of M2PT, where the key is the concrete implementa- ConvNets also effectively handle embeddings extracted from different
tionofpathwaysthatconnectthetwomodels. Asdiscussed modalitieswiththesamearchitecture(akintotransformersuniversallytok-
enizingandprocessingdataofmultiplemodalities),achievingstate-of-the-
above, thankstotheuniversalmodelingability, transform-
artperformancesintasksincludingglobalweatherforecastingandaudio
ers on different modalities may have different tokenizers, recognition.
2Multimodal Pathways Transformer (M2PT) Conceptual Structure = Implemented Structure with Cross-Modal Re-parameterization
+ +
FC FC’ FC structure
Tokenizer Transformer Blocks Head
+ equivalent
FC FC’ FC FC FC’ FC
Norm equ=ivalent Norm
+ +
parameterization
Auxiliary Modality Proj Proj’ Proj
Pathways
Attention Attention
Target Modality Q Q’ K K’ V V’ Q K V formulation
=
Norm Norm
Tokens Tokens
Figure 2. (Left) Framework of Multimodal Pathway Transformer (M2PT). We use point cloud and image modalities as an example.
Commonpracticeswithtransformersfollowthesamepipeline: using1)tokenizerstoconverttheinputdatatosequences,2)transformer
blockstoprocessthesequences,and3)headstodecodethesequences. Weupgradethesequence-to-sequencemodelingbyestablishing
pathways between the components of different modalities so processing the tokens of a specific modality can utilize the transformer
blockstrainedwithanothermodality.(Middle)ConceptualdesignofM2PT,wherethepathwaysareimplementedbylettingalinearlayer
(includingtheQuery/Key/Value/projectionlayersintheattentionblockandthoseintheFFNblock)inthetargetmodelcooperatewithits
counterpartintheauxiliarymodel. (Right)Cross-ModalRe-parameterizationefficientlyrealizesM2PTbyre-parameterizingtheweights
ofthetargetmodelwiththoseoftheauxiliarymodel,introduceingmarginaltrainingcostsandcompletelynoinferencecosts.
structure and number of parameters of the resultant model
Target
IN-1K K400 PartNet Audioset willbeidenticaltoaregularmodel.
Auxiliary Top-1 Acc. Top-1 Acc. mIoU Top-1 Acc.
Weexperimentedwiththeimage,video,pointcloud,and
+0.9% +3.8% +0.8% audiomodalities.Figure3showstherelativeimprovements
M2PTconsistentlybringsamongfourmodalities. Suchre-
sultsrevealthatthemodality-complementaryknowledgeof
+0.4% +5.7% +0.6% sequence-to-sequencemodelingintransformersdoesexist.
Asanearlyexploration, ourempiricalstudiesconfirmthat
suchimprovementsarenotsolelyduetothemoreparame-
+0.7% +1.0% +0.8%
ters,andsuggestthatsuchmodality-complementaryknowl-
edge may be related to the ability to generally process hi-
+0.4% +1.0% +1.5% erarchical representations. Abstraction hierarchy exists in
multiple modalities with concepts ranging from low-level
Figure3.ConsistentimprovementsbroughtbyM2PTacrosseach
to high-level, which may explain the universality of the
pairoffourmodalities-image,video,pointcloud,andaudio.The
learnedknowledge. Inotherwords,asatransformerisbe-
metrics are ImageNet-1K accuracy, Kinetics-400 accuracy, Part-
NetmIoU,andAudioSetaccuracy,respectively.Thenumbersrep- ing trained with images, it learns both (ability A) how to
resentthepercentageofimprovementofM2PTmodelsrelativeto understand images and (ability B) how to generally trans-
theperformanceofbaselinemodelsthatarepretrainedwithMAE- form the tokens from the lower-level patterns to a higher
stylemethods[30,31,41,68]onthefourmodalities,respectively. level without assuming they originally come from images.
Meanwhile,asanothertransformerisbeingpretrainedwith
audio data, it learns both a different “ability A” for audio
part in the trained auxiliary model scaled by a learnable
andasimilar“abilityB”,sothatitcanhelptheaforemen-
multiplierthatindicatesthestrengthofthepathway,sothat
tionedtransformerinimagerecognition.
themethodisnamedCross-ModalRe-parameterization. A
Insummary,ourcontributionsareasfollows:
significant strength of re-parameterization is that the extra
trainingcostsaremarginal(i.e.,there-parameterizedmodel • We propose Multimodal Pathway, which is a framework
will have the same number of linear layers as the original toimprovetransformersviaexploitingmodelstrainedon
model, and each linear layer merely needs to compute the othermodalities.
sum of two weight matrices before projecting the inputs) • We propose an inference-cost-free implementation of
and we can merge the weights after training so that the Multimodal Pathway, which is named Cross-Modal Re-
3parameterization. putationofeachre-parameterizedlayerintheforwardcom-
• Multimodal Pathway represents an early exploration in putationaddsuptwoweightmatrices,
this direction, which offers a novel perspective. We re-
alizesignificantandconsistentimprovementsinfourrep- 3.Method
resentative modalities, which demonstrates the potential
ofourmethodasapromisingapproach.
3.1.ArchitecturalDesign
2.RelatedWork
We design a transformer for a specific modality as three
Unimodal pretraining. The evolution of unimodal pre- modules - the modality-specific tokenizer, the modality-
trainingparadigmshastransitionedfromsupervisedtoself- agnostic transformer blocks, and the modality-specific
supervisedparadigms.Forinstance,Devlinetal.[10]intro- head. We assume the dimension of tokens is D, which is
duced the mask-reconstruction paradigm and achieved re- a pre-defined architectural hyper-parameter, and describe
markableoutcomes. Atthattime,visualpretraininglargely how to tokenize the input data of multiple modalities into
emphasized contrastive learning [5, 7, 29]. Subsequently, D-dimensionaltokens.
leveraging the vast amounts of unlabeled data, the BERT Image tokenizer. We represent an image by x ∈
I
paradigmgainedtractionandpioneerslikeMAE[30]suc- RH×W×C,where(H,W)specifiestheimage’sresolution,
cessfully applied it to visual pretraining, while others [24, and C is the number of channels. Following ViT [20], we
41,49,64]extendedthisparadigmtoareaslikepointcloud, useanS×S patchembeddinglayer,whichisimplemented
audio,andvideoperception. with an S×S convolutional layer with a stride of S, to
WeuseMAE-styleunimodalpretrainingmethodstoob- projectimagesintoD-dimensionaltokens. Thatis
tain the weights on each modality for simplicity. We do
notusesupervisedpretrainingbecausewewouldliketoen- x I ∈RH×W×C →x′ I ∈RH SW 2 ×D. (1)
sure that two unimodal datasets are completely irrelevant
byavoidingusinglabels,consideringthatthelabelsoftwo Video tokenizer. Analogous to 2D images, we use video
datasetsmaysomehowoverlap. patchesasthebasicunitsforlearningvideorepresentations.
Multimodal pretraining. Existing multimodal learning GivenanN-framevideox ∈ RN×H×W×C,similartoim-
methods require paired [53–56] or interleaved data [1]. ages,weuseanS×S embeddinglayersothat
In either case, the data samples of different modalities
are well-aligned (i.e., strongly related). A recent study x V ∈RN×H×W×C →x′ V ∈RN SH 2W×D. (2)
highlighted a main trend in the literature - existing mul-
FollowingViT[20],weuseS =14bydefault.
timodal pretraining methods are overly dependent on the
Pointcloudtokenizer. GivenapointcloudX = {x }P
well-aligned multimodal sample pairs/tuples [60]. For in- i i=1
comprising P points, each point x is defined as x =
stance,VideoBERT[46]andCBT[45]utilizewell-aligned i i
(p ,f ), where p ∈ R3 denotes the 3D coordinates and
video and speech data; many models are pretrained with i i i
f ∈ Rc encodestheattributes, e.g., color, viewpoint, nor-
large-scale image-text data, e.g., VisualBERT [34], VL- i
mal, etc. We use the Farthest Point Sampling to sample a
BERT [44], ViLBERT [39], LXMERT [48], UNITER [8],
representative skeleton from the original points at a fixed
LLaVa [37], OneLLM [27], EVA-CLIP [47], GVT [52],
sampling ratio of 1/4, then K-Nearest Neighbor method
VL-GPT[69].
to group proximate points. Then we model the geomet-
Nowadays, using the weakly-aligned or unpaired/u-
naligned multimodal data as the pretraining corpora re- ric relevance by constructing an adjacency matrix RP 4×P 4
between each pair of groups, which is then projected into
mainsunderstudied[60]. Thisworkrepresentsanearlyex-
D-dimensionaltokens. Thatis
plorationinthisdirection,whichservestofillthisgapinthe
fi Se trld u. ctural Re-parameterization is a methodology that x P ∈RP×(3+c) →x′ P ∈RP 4×P 4 →x′ P′ ∈RP 4×D. (3)
constructs extra structures (e.g., convolutional layers) dur- Audio spectrogram tokenizer. Let T and F be the num-
ing training and converts the trained structures via trans- bersoftimeframesandfrequencybins,weusex ∈RT×F
A
formingtheparameters[3,11–18]. Aprimarydrawbackof torepresentasample. Analogousto2Dimages,weseean
StructuralRe-parameterizationisthattheconstructedlayers audio sample as a single-channel image and use a similar
mustparticipateinthecomputationswiththeinputs,result- embeddinglayersothat
inginsignificantextratrainingcosts.
In contrast, Cross-Modal Re-parameterization is a sim- x
A
∈RT×F →x′
A
∈RT SF 2×D. (4)
ple re-parameterization method that is more efficient than
StructuralRe-parameterization.Specifically,theextracom- InourAudioSetexperiments,wehaveT=F=128,S=16.
4Transformerblocks. Weadoptthestructuraldesignofthe the layer computes the equivalent weight matrix and then
transformerblocksinVisionTransformer(ViT)[20],where usesittoprojecttheinput,whichis
eachtransformerblockcomprisesaself-attentionblockand
a Feed-Forward Network (FFN) block. The linear layers y =x(W +λW′). (9)
includetheQuery/Key/Value/projectionlayersintheatten-
After training, we merge the parameters by computing
tion block and two layers in the FFN block. For fairness
Wˆ =W+λW′andsaveitonly.Forinference,wesimply
and reproducibility, we use the same architectural hyper-
constructaregularlinearlayerandloadWˆ .
parameters (e.g., dimension of tokens, number of blocks,
Insummary, toconstructanduseanM2PTwithCross-
and number of heads) as ViT-Base for every M2PT model
ModalRe-parameterization,we
oneverymodality.
• Construct the tokenizer and head according to the target
3.2.Cross-ModalRe-parameterization modality.
• Construct the transformer blocks with Cross-Modal Re-
ForanM2PTmodelonaspecificmodality, weuseCross-
parameterization. For each linear layer, except for the
Modal Re-parameterization in the transformer blocks to
originalweightmatrix, weaddanextratrainableweight
utilize another model’s weights trained on another modal-
matrixandinitializeitwiththecorrespondingonefroma
ity. Specifically, let θ be an arbitrary trainable parame-
transformer trained on the auxiliary modality, and add a
ter of a layer in the transformer, x be the input, and y be
trainablescalarparameterinitializedwith0.
the output, we use f to denote the operation so that y =
• Train the re-parameterized cross-modal model just like
f(x;θ).WithCross-ModalRe-parameterization,wesimply
wetrainaregularmodel.
re-parameterizethelayerwithparametersofitscounterpart
• Aftertraining,convertthetrainedmodelandsavethecon-
inanothermodalthatistrainedonanothermodality. Letθ′
vertedoneforinference.
betheparameterofthecounterpart,theoperationbecomes
4.Experiments
y =f(x;θ+λθ′). (5)
WerefertoλastheCross-ModalScaleandθ′astheCross-
Modal Parameter. After training, we merge the model by 4.1.Setup
computing and saving θˆ = θ+λθ′ so that the model will
Datasets. For image recognition, we evaluate the mod-
nolongerhaveextraparametersandtheinferencecostsand
els’performanceonthreerepresentativeimagedatasets. 1)
modelsizewillbeidenticaltoaregularmodel.
ImageNet-1K[9]isthemostwidelyadoptedbenchmarkfor
WithCross-ModalRe-parameterization,weequivalently
visual perception tasks, which contains nearly 1.3 million
realizetheproposedM2PTtransformerblockwithmarginal
images of 1000 categories. 2) MSCOCO 2017 [36] is a
trainingcostsandcompletelynoinferencecosts. Foralin-
commonbenchmarkforobjectdetection. M2PTistrained
earlayerwhoseparametersformamatrixW ∈RDin×Dout
onthetrainsetandevaluatedonthevalsetwithMask
and the inputs and outputs are matrices x ∈ RB×Din and
RCNN [28]. 3) ADE-20K [67] is used for semantic seg-
y ∈ RB×Dout. We omit the bias term for brevity and the
mentationexperimentswithUperNet[57]andweadoptthe
originaloperationisformulatedby
single-scale evaluation setting. For point cloud, we evalu-
atetheperformanceofM2PTonShapeNetPart[61],which
y =xW . (6)
contains16,880modelsand16categories.Foraudiorecog-
nition,followingAudioMAE[31],weutilizetheAudioSet-
As described in the conceptual structure depicted in Fig-
2k [23] dataset. For video, we experiment on the action
ure2,thelinearlayeranditscounterparttakethesamein-
recognitiondataset,Kinetics-400[32],whichcontains240k
put. Theoutputwillbe
trainingvideosand20kvalidationvideosfrom400classes.
y =xW +λ(xW′). (7) Experimental details. For a pair of target modality
and auxiliary modality, we obtain the auxiliary model
by self-supervised training on a dataset of the auxiliary
Note
xW +λ(xW′)=x(W +λW′), (8) modality. Specifically, the auxiliary image model is pre-
trained with MAE [30] on ImageNet-1K [9], the auxil-
so that the two layers can be equivalently implemented by iary point cloud model is pretrained with Point-MAE [41]
asinglelayerthathasatrainablescalarλandanadditional on ShapeNet [6], the auxiliary audio model is pretrained
trainablematrixwhichisinitializedwiththecounterpartin with AudioMAE [31] on AudioSet-2M [23], the auxiliary
theauxiliarymodel.Boththeoriginalweightmatrixandthe videomodelispretrainedwithVideoMAE[49]onKinetics-
additionalonearetrainable. Ateachforwardcomputation, 700[32]. Forfairnessandreproducibility,weusetheirof-
5Table 1. Experimental results on image recognition tasks. On ImageNet, we report the results with the linear layers in transformer
blocksfinetuned(tuneacc)orfixed(fixacc).∗:resultsarereportedbyrunningtheofficialcode.ThearchitectureofeverymodelisViT-B.
Therelativeimprovementsoverthebaselinesareshowningreen.
ImageNet MSCOCO ADE20K
Method
tuneacc(%) fixacc(%) AP (%) AP (%) mIOU(%)
box mask
Pretrainedsetting
SemMAE[33] 83.4 65.0 - - 46.3
MFF[38] 83.6 67.0 48.1 43.1 47.9
MAE∗[30] 83.3 65.6 47.3 42.4 46.1
M2PT-Video(Ours) 83.6↑0.4% 67.1↑2.3% - - -
M2PT-Audio(Ours) 83.7↑0.4% 67.3↑2.6% - - -
M2PT-Point(Ours) 83.9↑0.7% 67.8↑3.4% 50.0↑5.7% 44.0↑3.8% 47.9↑3.9%
From-scratchsetting
ViT[20] 76.5 14.5 46.2 40.5 39.7
M2PT-Point(Ours) 81.9↑7.1% 19.5↑34.5% 48.9↑5.8% 42.2↑4.2% 42.5↑7.1%
ficial code for pretraining. We do not use supervised pre- mentsunderthepretrainedsetting,wherethetargetweights
training because we would like to eliminate the effects of are initialized with a ViT pretrained with MAE on Ima-
labels in the pretraining datasets so that we can ensure the geNet, and the auxiliary weights are from the models pre-
irrelevanceofthedatasamples, consideringthatthelabels trained on video, audio, and point datasets, respectively.
oftwodatasetsmaysomehowoverlap. Intermsoftheini- Such three models, which are labeled as M2PT-Video,
tializationofthetargetmodel,weadopttwosettings.1)The M2PT-Audio, andM2PT-Point, respectively, andthebase-
targetmodel(i.e.,theparametersdenotedbyW inEq.9)is line(theoriginalMAE-pretrainedViT)aretrainedonIma-
initializedwiththeaforementionedweightspretrainedwith geNetwiththefinetuningconfigurationsoriginallyadopted
theself-supervisedmethodsonthetargetmodality.Wefine- by MAE [30], and the resultant accuracies are reported in
tunetheM2PTmodelwiththedefaultfinetuningconfigura- the“tuneacc”columninTable1.Thenwetransferthebest-
tions described by the corresponding pretraining methods. performing model, which is M2PT-Point, to COCO object
The baseline model is also initialized with the pretrained detection and ADE20K semantic segmentation tasks. The
weightsandfine-tunedwithidenticalconfigurationssothat improvements are significant: the ImageNet accuracy im-
thissettingisreferredtoasthepretrainedsettingforbrevity. provesfrom83.3to83.9,theCOCOboxAPimprovesfrom
2)Thetargetmodelisrandomlyinitializedasusual,andwe 47.3 to 50.0, and the ADE20K mIoU improves from 46.1
use the widely adopted training configurations to train the to 47.9, so the relative improvements are 0.7%, 5.7%, and
M2PT model. The baseline model is trained from scratch 3.9%,respectively.
withidenticalconfigurationsforfaircomparisonssothatthe
Apart from finetuning the target and auxiliary weights,
settingisreferredtoasthefrom-scratchsettingforbrevity.
we test another setting where the parameters of linear
Inotherwords,theM2PTandbaselinemodelbothhaveno
weightsintransformerblocksarefixed,andonlytheCross-
weightspretrainedonthetargetmodalityunderthissetting.
ModalScalestogetherwiththeclassifieraretrainable. The
Metrics. We report the performance of M2PT mod-
accuracies are reported in the “fix acc” column. Natu-
els on various datasets, including top-1 accuracy for
rally, under this setting, the baseline should be the MAE-
ImageNet-1K,AudioSet,Kinetics-400,mIoUforADE20K,
pretrained ViT where only the classifier is trainable. Im-
ShapeNetPart and PartNet, and box/mask AP for MS
pressively,therelativeimprovementbecomesmoresignifi-
COCO.Tofairlyassesstheperformanceimprovementsover
cant(65.6→67.8sothattherelativeimprovementis3.4%),
thebaselinesinmultiplemetrics,wereporttherelativeper-
demonstratingthattheweightsobtainedfromtheauxiliary
centage of improvement in Table 1, 2, 3, 4. For exam-
modalityworkonanothermodality,eveniftheweightsare
ple,comparedtotheMAE-pretrainedmodel,theADE20K
fixed. WewouldliketonoteMAEisapowerfulpretraining
mIoU improves from 46.1 to 47.9 so that the relative im-
method,anditischallengingtogainfurtherimprovements
provementis(47.9−46.1)/(46.1)=3.9%(Table1).
on top of MAE. Some insightful recent methods [33, 38]
improvedMAEbutourresultsaremoresignificant.
4.2.MainResults
On the other hand, under the from-scratch setting, the
baselineisaViTtrainedfromscratch,andthetargetweights
Image recognition. We first conduct a group of experi- of M2PT are also randomly initialized. The accuracy is
6Table2. Experimentalresultsonpointclouddatasets. Were- Table3. ExperimentalresultsonAudioSet-2k. Therelativeim-
porttheclassmIoU(mIoU )andinstancemIoU onShapeNet- provementsoverthebaselinesareshowningreen.
C I
Part and mIoU on PartNet. The relative improvements over the
Method Model Top-1Acc.(%)
baselinesareshowningreen.
Pretrainedsetting
ShapeNetPart PartNet PSLA[25] CNN+Trans 31.9
Method
mIoUC(%) mIoUI(%) mIoU(%) AST[24] ViT-B 34.7
Pretrainedsetting SSAST[26] ViT-B 31.0
PointNet++[42] 81.9 85.1 42.5
AudioMAE[31] ViT-B 35.3
Point-BERT[62] 84.1 85.6 -
Point-MLP[40]. 84.6 86.1 48.1
M2PT-Point ViT-B 35.6↑0.8%
M2PT-Video ViT-B 35.5↑0.6%
Point-MAE[62] 84.2 86.1 47.4
M2PT-Image ViT-B 35.6↑0.8%
M2PT-Video 85.6↑1.7% 87.5↑1.6% 50.1↑5.7%
From-scratchsetting
M2PT-Image 85.6↑1.7% 87.5↑1.6% 49.2↑3.8%
N/A ViT-B 11.0
M2PT-Audio 85.6↑1.7% 87.5↑1.6% 48.1↑1.5%
M2PT-Point ViT-B 11.4↑3.6%
From-scratchsetting
N/A 50.2 68.4 -
M2PT-Video 50.8↑1.2% 68.8↑0.6% - Table4.ExperimentalresultsonKinetics-400.Therelativeim-
provementsoverthebaselinesareshowningreen
Method Model Top-1Acc.(%)
drastically improved from 81.9 to 76.5 so the relative im-
SlowFast-101[21] ResNet-101 79.8
provementis7.1%,suggestingtheauxiliaryweightssignif- MViTv2-B[35] ViT-B 81.2
icantlyfacilitatethetrainingprocess. Intuitively,theCross- TimeSFormer[2] ViT-B 80.7
Modal Scales are initialized with 0 but will soon become
VideoMAE[49] ViT-B 81.5
non-zeroasthetrainingproceedssothemodelwillbegrad-
M2PT-Point ViT-B 82.3↑1.0%
ually influenced by the auxiliary weights and benefit from M2PT-Image ViT-B 82.2↑0.9%
the modality-complementary knowledge. When we trans- M2PT-Audio ViT-B 82.3↑1.0%
fer such two models to COCO and ADE20K, we observe
consistentimprovementsintheboxAPandmIoU.
3D point cloud understanding. Table 2 presents the ex- MAE[49],M2PToutperformsbyatleast+0.8top-1accu-
perimental results on ShapeNetPart and PartNet datasets, racy(82.3vs. 81.5),whichrevealsthatthetemporalaware-
where we compare M2PT with existing point cloud pre- nessforvideounderstandingcanalsobeenhancedwithir-
training methods such as Point-BERT [41] and Point- relevantdatafromothermodalities.
MAE [62]. M2PT consistently improves the class mIoU
4.3.AblationStudies
from84.2to85.6andinstancemIoUfrom86.1to87.5on
ShapeNetPart and raises the mIoU from 47.4 to 50.1 on As shown in Table 5, we evaluate the design choices of
PartNet. Under the from-scratch setting, we also observe M2PTseparatelythroughagroupofablationstudiesunder
consistentimprovements. thepretrainedsettingonImageNetandtheauxiliarymodal-
Audio recognition. For the pretrained setting, the tar- ityisthepointcloud. Wemakethefollowingobservations.
get weights are initialized with an AudioMAE-pretrained 1)ApplyingCross-ModalRe-parameterizationtoevery
model. AsshowninTable3,wecompareM2PTwithexist- linearlayerdeliversthebestperformance. Ineachtrans-
ingcompetitivemethodsincludingSSAST[26],AST[24], former block, we may choose to apply our method to any
and AudioMAE [31]. M2PT improves the top-1 accuracy of the Query/Key/Value/projection layers in the attention
by 0.8% relatively on the Audioset balanced split, demon- block and the two linear layers in the FFN. Table 5 shows
strating that M2PT is also effective in audio recognition. changing any one of the layers brings improvements, and
Underthefrom-scratchsetting,M2PTbringsoutarelative thebestresultisachievedbychangingthemall.
improvementof3.6%. 2) Cross-Modal Scale should be initialized with 0. By
Video understanding. For the experiments on Kinetics- default, we initialize the Cross-Modal Scale λ with 0 for
400,weadoptonlythepretrainedsettingbecauseitisnota everylayer. Weobservethatinitializingittoahighervalue
common practice to train a model from scratch on a video degrades the performance, suggesting that the initial state
dataset, which would deliver inferior performance. We oftheM2PTshouldbeidenticaltothetargetweights(i.e.,
use the Video-MAE-pretrained ViT to initialize the target theweightspretrainedwithMAE,inthiscase).
weights. Naturally,thebaselineshouldbetheVideoMAE- 3) Cross-Modal Scale should be learnable. Fixing the
pretrained model directly finetuned on Kinetics-400. Ta- Cross-Modal Scale degrades the performance, suggesting
ble 4 shows that compared with previous works including it is important to let the model learn how to combine the
SlowFast[21],MViTv2[35],TimeSFormer[2],andVideo- targetweightsandthecorrespondingauxiliaryweights.
7Table5.AblationstudiesondesignchoicesofM2PTincludingthelayerstore-parameterizeandconfigurationsofCross-ModalScaleλ.
ThetargetdatasetisImageNet-1Kandtheauxiliarymodalityispointcloud.
Components Cross-ModalScale
Top-1accuracy(%)
Attn QKV Attn Proj FFN 1st FFN 2nd Init. Trainable
✔ 0 ✔ 83.4
✔ 0 ✔ 83.6
✔ 0 ✔ 83.6
✔ 0 ✔ 83.7
✔ ✔ ✔ ✔ 0 ✔ 83.9
✔ ✔ ✔ ✔ 10−2 ✘ 83.5
✔ ✔ ✔ ✔ 10−2 ✔ 83.6
✔ ✔ ✔ ✔ 10−4 ✔ 83.6
✔ ✔ ✔ ✔ 10−6 ✔ 83.7
Table 6. ImageNet accuracy with changed order of auxiliary beharmedifthelow-to-highcorrespondenceisinterrupted,
weightsorfewerpretrainingepochs. suggesting that such knowledge may help understand gen-
eralhierarchicalconceptsregardlessofthemodality.
Orderofauxweights Epochspretrained Top-1acc 2)Weinvestigateiftheimprovementsaremerelydueto
Normal 20 83.55 more trainable parameters or a better initialization by
Normal 220 83.69
verifying if a better pretraining process brings the afore-
Normal 300 83.93
mentioned knowledge of higher quality. We experiment
Reversed 300 83.61
using not well-trained weights as the auxiliary weights.
Specifically, the default auxiliary weights are obtained
4.4.EmpiricalDiscussions
througha300-epochself-supervisedpretrainingprocesson
4.4.1 OntheModality-ComplementaryKnowledge point cloud data, but we alternatively use the checkpoints
savedatthe20thand220thepoch,respectively,astheaux-
The observed improvements on multiple modalities have iliaryweights. Notsurprisingly,weobservethattheperfor-
shown that the auxiliary transformer has learned some mancedegradesto83.55%and83.69%,respectively,which
knowledge that is able to transfer to the target modality. isstillhigherthanthebaseline. Thisphenomenonsuggests
Wecontinuetoinvestigatethepropertiesofsuchmodality- thattheimprovementsbroughtbytheauxiliaryweightscan-
complementary knowledge through two groups of experi- notbesimplyexplainedthattheweightstrainedonanother
ments(Table6). modalitymerelyofferaninitializationhardlybetterthanthe
1) We investigate if such knowledge is related to the random initialization or the model merely benefits from a
abilitytogenerallyprocesshierarchicalrepresentations. larger number of trainable parameters (if so, training the
Abstraction hierarchy exists in multiple modalities with checkpointatthe20thepochto300epochswouldnotbring
concepts ranging from low-level to high-level, which may observableeventualimprovementsonthetargetmodality).
explain the transferability of the learned knowledge. For
example,intheimageandpointcloudmodalities,thishier-
4.4.2 DiscussionontheDataScale
archymayincludetextures(inimages)orindividualpoints
(inpointclouds), objectparts, andwholeobjects. Consid- 1) From small-scale data to large-scale data. Previous
eringthattheconceptuallevelatransformerblockworkson works such as Image2Point [59] and Point-CLIP [63] fol-
isdeterminedbyitsdepth,wedesignanexperimentbyre- lowacommonconsensusthatthemodalityowningalarger
vertingtheorderoftheauxiliaryweights. Specifically,the data scale could be utilized to benefit the other modality
counterpartofthefirsttargetblockshouldbethefirstaux- owning a smaller one. Therefore, Image2Point introduces
iliaryblock,whoseweightsareconnectedviaCross-Modal image-pretrainedmodelstodata-insufficient3Dperception
Re-parameterization,whichisobvious. Underthereverse- tasks. Differently, M2PT sets up a brand new methodol-
order setting, since the transformer has 12 blocks, we let ogy and breaks the former consensus - we discover that
thei-thblockconnectwiththe(13−i)-thblocksothatthe even though the data scale of point clouds is limited, such
target-auxiliarycorrespondenceisinterrupted. Weobserve datastillbringsoutimpressiveimprovementstotheimage,
that doing so decreases the accuracy to 83.61%, which is video, and audio perception tasks. Impressively, the pre-
0.32% lower than the normal M2PT. In summary, we ob- trainingdataofthelattermodalitiesislargerinmagnitude
serve that modality-complementary knowledge in the aux- thanthatofthepointcloud,butthepointclouddatamakesa
iliary transformer can transfer to another modality but can difference. 2)Fromlarge-scaledatatosmall-scaledata.
8Ontheotherhand,theeffectivenessofM2PThighlightsthat ofvisualrepresentations. arXivpreprintarXiv:2002.05709,
for3Dvisionresearchandotherareasthatlacklarge-scale 2020. 4
dataforpretraining,M2PTintroducesapromisingdirection [8] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,
toleverageirrelevantlarge-scaledatafromothermodalities. FaisalAhmed,ZheGan,YuCheng,andJingjingLiu.Uniter:
Universal image-text representation learning. In European
conference on computer vision, pages 104–120. Springer,
5.ConclusionandLimitation
2020. 4
[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
Thispaperexploresthefeasibilityandadvantagesofim- andLiFei-Fei. Imagenet: Alarge-scalehierarchicalimage
database. InCVPR,pages248–255.Ieee,2009. 5
proving a transformer’s performance on a specific modal-
[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
itywithirrelevantdatafromothermodalities. Wepropose
Toutanova. BERT:Pre-trainingofdeepbidirectionaltrans-
a general framework named Multimodal Pathway and a
formersforlanguageunderstanding. InNAACL-HLT,2019.
concrete inference-cost-free implementation named Cross-
1,4
Modal Re-parameterization. Multimodal Pathway repre-
[11] X Ding, H Chen, X Zhang, K Huang, J Han, and G Ding.
sents an early exploration in this direction, which offers a Re-parameterizingyouroptimizersratherthanarchitectures.
novelperspective. Werealizesignificantandconsistentim- arxiv2022. arXivpreprintarXiv:2205.15242. 4
provements on four representative modalities, demonstrat- [12] Xiaohan Ding, Yuchen Guo, Guiguang Ding, and Jungong
ingthepotentialofourmethodasapromisingapproach. Han.Acnet:Strengtheningthekernelskeletonsforpowerful
Theprimarylimitationisthatthetheorybehindtheim- cnn via asymmetric convolution blocks. In Proceedings of
provements remains to be revealed. Apart from empirical theIEEE/CVFinternationalconferenceoncomputervision,
explanations,webelievefurtherinvestigations(e.g.,amath- pages1911–1920,2019.
[13] XiaohanDing,TianxiangHao,JianchaoTan,JiLiu,Jungong
ematically provable bound) will be useful, which may re-
Han, Yuchen Guo, and Guiguang Ding. Resrep: Lossless
quireadeeperunderstandingoftheblackboxofdeepneu-
cnnpruningviadecouplingrememberingandforgetting. In
ralnetworks.
ProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pages4510–4520,2021.
References
[14] XiaohanDing,XiangyuZhang,JungongHan,andGuiguang
Ding. Diverse branch block: Building a convolution as an
[1] Jean-BaptisteAlayrac, JeffDonahue, PaulineLuc, Antoine
inception-like unit. In Proceedings of the IEEE/CVF Con-
Miech,IainBarr,YanaHasson,KarelLenc,ArthurMensch,
ferenceonComputerVisionandPatternRecognition,pages
Katie Millican, Malcolm Reynolds, et al. Flamingo: a vi-
10886–10895,2021.
sual language model for few-shot learning. arXiv preprint
[15] XiaohanDing,XiangyuZhang,NingningMa,JungongHan,
arXiv:2204.14198,2022. 4
Guiguang Ding, and Jian Sun. Repvgg: Making vgg-style
[2] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is
convnetsgreatagain. InProceedingsoftheIEEE/CVFcon-
space-time attention all you need for video understanding?
ference on computer vision and pattern recognition, pages
InICML,page4,2021. 7
13733–13742,2021.
[3] ZhichengCai,XiaohanDing,QiuShen,andXunCao. Ref-
[16] Xiaohan Ding, Honghao Chen, Xiangyu Zhang, Jungong
conv:Re-parameterizedrefocusingconvolutionforpowerful
Han, and Guiguang Ding. Repmlpnet: Hierarchical vi-
convnets. arXivpreprintarXiv:2310.10563,2023. 4
sionmlpwithre-parameterizedlocality. InProceedingsof
[4] NicolasCarion,FranciscoMassa,GabrielSynnaeve,Nicolas theIEEE/CVFConferenceonComputerVisionandPattern
Usunier,AlexanderKirillov,andSergeyZagoruyko.End-to- Recognition,pages578–587,2022.
endobjectdetectionwithtransformers.InComputerVision–
[17] XiaohanDing,XiangyuZhang,JungongHan,andGuiguang
ECCV2020: 16thEuropeanConference,Glasgow,UK,Au-
Ding. Scaling up your kernels to 31x31: Revisiting large
gust 23–28, 2020, Proceedings, Part I 16, pages 213–229. kerneldesignincnns. InProceedingsoftheIEEE/CVFcon-
Springer,2020. 1 ference on computer vision and pattern recognition, pages
[5] MathildeCaron,HugoTouvron,IshanMisra,Herve´ Je´gou, 11963–11975,2022.
JulienMairal,PiotrBojanowski,andArmandJoulin.Emerg- [18] Xiaohan Ding, Yiyuan Zhang, Yixiao Ge, Sijie Zhao, Lin
ingpropertiesinself-supervisedvisiontransformers.InPro- Song,XiangyuYue,andYingShan. Unireplknet: Auniver-
ceedingsoftheIEEE/CVFinternationalconferenceoncom- sal perception large-kernel convnet for audio, video, point
putervision,pages9650–9660,2021. 4 cloud, time-series and image recognition. arXiv preprint
[6] AngelXChang,ThomasFunkhouser,LeonidasGuibas,Pat arXiv:2311.15599,2023. 2,4
Hanrahan,QixingHuang,ZimoLi,SilvioSavarese,Mano- [19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
lis Savva, Shuran Song, Hao Su, et al. Shapenet: An Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
information-rich 3d model repository. arXiv:1512.03012, MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl-
2015. 5 vainGelly,JakobUszkoreit,andNeilHoulsby. Animageis
[7] TingChen,SimonKornblith,MohammadNorouzi,andGe- worth16x16words: Transformersforimagerecognitionat
offreyHinton. Asimpleframeworkforcontrastivelearning scale. ICLR,2021. 1
9[20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, [33] GangLi,HeliangZheng,DaqingLiu,ChaoyueWang,Bing
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Su,andChangwenZheng.Semmae:Semantic-guidedmask-
MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl- ingforlearningmaskedautoencoders. AdvancesinNeural
vainGelly,JakobUszkoreit,andNeilHoulsby. Animageis InformationProcessingSystems,35:14290–14302,2022. 6
worth16x16words: Transformersforimagerecognitionat [34] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh,
scale. InICLR,2021. 1,4,5,6 and Kai-Wei Chang. Visualbert: A simple and perfor-
[21] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and mant baseline for vision and language. arXiv preprint
Kaiming He. Slowfast networks for video recognition. In arXiv:1908.03557,2019. 4
Proceedings of the IEEE/CVF international conference on [35] Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Man-
computervision,pages6202–6211,2019. 7 galam, Bo Xiong, Jitendra Malik, and Christoph Feichten-
hofer. Mvitv2: Improvedmultiscalevisiontransformersfor
[22] Chongjian Ge, Xiaohan Ding, Zhan Tong, Li Yuan, Jian-
classificationanddetection.InProceedingsoftheIEEE/CVF
gliu Wang, Yibing Song, and Ping Luo. Advancing vi-
Conference on Computer Vision and Pattern Recognition,
siontransformerswithgroup-mixattention. arXivpreprint
pages4804–4814,2022. 7
arXiv:2311.15157,2023. 1
[36] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James
[23] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren
Hays, Pietro Perona, Deva Ramanan, Piotr Dolla´r, and
Jansen,WadeLawrence,RChanningMoore,ManojPlakal,
C.LawrenceZitnick. Microsoftcoco: Commonobjectsin
and Marvin Ritter. Audio set: An ontology and human-
context. InECCV,2014. 5
labeled dataset for audio events. In 2017 IEEE interna-
[37] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.
tionalconferenceonacoustics,speechandsignalprocessing
Visualinstructiontuning. arXivpreprintarXiv:2304.08485,
(ICASSP),pages776–780.IEEE,2017. 5
2023. 4
[24] Yuan Gong, Yu-An Chung, and James Glass. Ast: Audio
[38] Yuan Liu, Songyang Zhang, Jiacheng Chen, Zhaohui Yu,
spectrogramtransformer. arXivpreprintarXiv:2104.01778,
Kai Chen, and Dahua Lin. Improving pixel-based mim
2021. 1,4,7
by reducing wasted modeling capability. arXiv preprint
[25] YuanGong,Yu-AnChung,andJamesGlass. Psla: Improv- arXiv:2308.00261,2023. 6
ingaudiotaggingwithpretraining, sampling, labeling, and [39] JiasenLu,DhruvBatra,DeviParikh,andStefanLee.Vilbert:
aggregation.IEEE/ACMTransactionsonAudio,Speech,and Pretraining task-agnostic visiolinguistic representations for
LanguageProcessing,29:3292–3306,2021. 7 vision-and-languagetasks. Advancesinneuralinformation
[26] Yuan Gong, Cheng-I Lai, Yu-An Chung, and James Glass. processingsystems,32,2019. 4
Ssast: Self-supervised audio spectrogram transformer. In [40] XuMa,CanQin,HaoxuanYou,HaoxiRan,andYunFu.Re-
Proceedings of the AAAI Conference on Artificial Intelli- thinkingnetworkdesignandlocalgeometryinpointcloud:
gence,pages10699–10709,2022. 7 Asimpleresidualmlpframework. ICLR,2022. 7
[27] Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, [41] Yatian Pang, Wenxiao Wang, Francis EH Tay, Wei Liu,
Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, and Xi- Yonghong Tian, and Li Yuan. Masked autoencoders
angyuYue. Onellm: Oneframeworktoalignallmodalities for point cloud self-supervised learning. arXiv preprint
withlanguage. arXivpreprintarXiv:2312.03700,2023. 4 arXiv:2203.06604,2022. 3,4,5,7
[42] CharlesRQi,LiYi,HaoSu,andLeonidasJGuibas. Point-
[28] KaimingHe,GeorgiaGkioxari,PiotrDolla´r,andRossGir-
net++: Deephierarchicalfeaturelearningonpointsetsina
shick. Maskr-cnn. InICCV,pages2961–2969,2017. 5
metricspace. InNeurIPS,2017. 7
[29] KaimingHe,HaoqiFan,YuxinWu,SainingXie,andRoss
[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Girshick. Momentumcontrastforunsupervisedvisualrep-
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
resentationlearning. InProceedingsoftheIEEE/CVFcon-
AmandaAskell,PamelaMishkin,JackClark,etal. Learn-
ference on computer vision and pattern recognition, pages
ingtransferablevisualmodelsfromnaturallanguagesuper-
9729–9738,2020. 4
vision. In International Conference on Machine Learning,
[30] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
pages8748–8763.PMLR,2021. 1
Dolla´r,andRossGirshick.Maskedautoencodersarescalable
[44] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu
visionlearners.InProceedingsoftheIEEE/CVFConference
Wei,andJifengDai. Vl-bert:Pre-trainingofgenericvisual-
onComputerVisionandPatternRecognition,pages16000–
linguisticrepresentations. arXivpreprintarXiv:1908.08530,
16009,2022. 3,4,5,6
2019. 4
[31] Po-Yao Huang, Hu Xu, Juncheng Li, Alexei Baevski, [45] Chen Sun, Fabien Baradel, Kevin Murphy, and Cordelia
Michael Auli, Wojciech Galuba, Florian Metze, and Schmid. Learning video representations using contrastive
Christoph Feichtenhofer. Masked autoencoders that listen. bidirectionaltransformer. arXivpreprintarXiv:1906.05743,
arXivpreprintarXiv:2207.06405,2022. 3,5,7 2019. 4
[32] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, [46] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy,
Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, and Cordelia Schmid. Videobert: A joint model for video
TimGreen,TrevorBack,PaulNatsev,etal.Thekineticshu- and language representation learning. In Proceedings of
manactionvideodataset. arXivpreprintarXiv:1705.06950, theIEEE/CVFinternationalconferenceoncomputervision,
2017. 5 pages7464–7473,2019. 4
10[47] QuanSun,YuxinFang,LedellWu,XinlongWang,andYue LeonidasGuibas,etal. Ascalableactiveframeworkforre-
Cao.Eva-clip:Improvedtrainingtechniquesforclipatscale. gionannotationin3dshapecollections. ACMTOG,35(6):
arXivpreprintarXiv:2303.15389,2023. 4 210,2016. 5
[48] Hao Tan and Mohit Bansal. Lxmert: Learning cross- [62] Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie
modalityencoderrepresentationsfromtransformers. arXiv Zhou,andJiwenLu. Point-bert:Pre-training3dpointcloud
preprintarXiv:1908.07490,2019. 4 transformerswithmaskedpointmodeling. InCVPR,2022.
[49] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. 7
Videomae: Masked autoencoders are data-efficient learn- [63] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xu-
ers for self-supervised video pre-training. arXiv preprint peng Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng
arXiv:2203.12602,2022. 4,5,7 Li. Pointclip: Point cloud understanding by clip. In Pro-
[50] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco ceedingsoftheIEEE/CVFConferenceonComputerVision
Massa,AlexandreSablayrolles,andHerve´ Je´gou. Training andPatternRecognition,pages8552–8562,2022. 8
data-efficient image transformers & distillation through at- [64] YiyuanZhang,KaixiongGong,KaipengZhang,Hongsheng
tention. pages10347–10357.PMLR,2021. 1 Li, Yu Qiao, Wanli Ouyang, and Xiangyu Yue. Meta-
[51] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko- transformer: Aunifiedframeworkformultimodallearning.
reit,LlionJones,AidanNGomez,ŁukaszKaiser,andIllia arXivpreprintarXiv:2307.10802,2023. 4
Polosukhin. Attentionisallyouneed. Advancesinneural [65] Zhixin Zhang, Yiyuan Zhang, Xiaohan Ding, Fusheng Jin,
informationprocessingsystems,30,2017. 1 and Xiangyu Yue. Online vectorized hd map construction
[52] GuangzhiWang,YixiaoGe,XiaohanDing,MohanKankan- usinggeometry. arXivpreprintarXiv:2312.03341,2023. 1
halli, and Ying Shan. What makes for good visual to-
[66] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and
kenizers for large language models? arXiv preprint
VladlenKoltun. Pointtransformer. InICCV,pages16259–
arXiv:2305.12223,2023. 4
16268,2021. 1
[53] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,
[67] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela
Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and
Barriuso, and Antonio Torralba. Scene parsing through
HongxiaYang. Unifyingarchitectures,tasks,andmodalities
ade20kdataset. InProceedingsoftheIEEEconferenceon
throughasimplesequence-to-sequencelearningframework.
computer vision and pattern recognition, pages 633–641,
arXivpreprintarXiv:2202.03052,2022. 4
2017. 5
[54] Wenhui Wang, Hangbo Bao, Li Dong, and Furu Wei.
[68] Jinxing Zhou, Jianyuan Wang, Jiayi Zhang, Weixuan Sun,
Vlmo:Unifiedvision-languagepre-trainingwithmixture-of-
Jing Zhang, Stan Birchfield, Dan Guo, Lingpeng Kong,
modality-experts. arXivpreprintarXiv:2111.02358,2021.
MengWang,andYiranZhong. Audio–visualsegmentation.
[55] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhil-
In Computer Vision–ECCV 2022: 17th European Confer-
iang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mo-
ence, Tel Aviv, Israel, October 23–27, 2022, Proceedings,
hammed,SakshamSinghal,SubhojitSom,etal. Imageasa
PartXXXVII,pages386–403.Springer,2022. 3
foreignlanguage: Beitpretrainingforallvisionandvision-
[69] Jinguo Zhu, Xiaohan Ding, Yixiao Ge, Yuying Ge, Sijie
languagetasks. arXivpreprintarXiv:2208.10442,2022. 1
Zhao, Hengshuang Zhao, Xiaohua Wang, and Ying Shan.
[56] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia
Vl-gpt: Agenerativepre-trainedtransformerforvisionand
Tsvetkov,andYuanCao. Simvlm: Simplevisuallanguage
language understanding and generation. arXiv preprint
model pretraining with weak supervision. arXiv preprint
arXiv:2312.09251,2023. 4
arXiv:2108.10904,2021. 4
[57] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and
JianSun. Unifiedperceptualparsingforsceneunderstand-
ing. InECCV,pages418–434,2018. 5
[58] EnzeXie,WenhaiWang,ZhidingYu,AnimaAnandkumar,
Jose M Alvarez, and Ping Luo. Segformer: Simple and
efficient design for semantic segmentation with transform-
ers. Advances in Neural Information Processing Systems,
34:12077–12090,2021. 1
[59] Chenfeng Xu, Shijia Yang, Tomer Galanti, Bichen Wu,
Xiangyu Yue, Bohan Zhai, Wei Zhan, Peter Vajda, Kurt
Keutzer,andMasayoshiTomizuka. Image2point: 3dpoint-
cloud understanding with 2d image pretrained models. In
EuropeanConferenceonComputerVision,pages638–656.
Springer,2022. 8
[60] Peng Xu, Xiatian Zhu, and David A Clifton. Multimodal
learningwithtransformers:Asurvey. IEEETransactionson
PatternAnalysisandMachineIntelligence,2023. 4
[61] Li Yi, Vladimir G Kim, Duygu Ceylan, I Shen, Mengyan
Yan, Hao Su, ARCewu Lu, Qixing Huang, Alla Sheffer,
11