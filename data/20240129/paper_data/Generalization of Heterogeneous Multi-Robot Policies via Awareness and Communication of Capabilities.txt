Generalization of Heterogeneous Multi-Robot Policies
via Awareness and Communication of Capabilities†
PierceHowell1∗,MaxRudolph2∗,RezaTorbati1,KevinFu1,HarishRavichandar1
1GeorgiaInstituteofTechnology,2UniversityofTexasatAustin
Email: pierce.howell@gatech.edu
Abstract: Recent advances in multi-agent reinforcement learning (MARL) are
enablingimpressivecoordinationinheterogeneousmulti-robotteams. However,
existingapproachesoftenoverlookthechallengeofgeneralizinglearnedpolicies
toteamsofnewcompositions,sizes,androbots. Whilesuchgeneralizationmight
notbeimportantinteamsofvirtualagentsthatcanretrainpolicieson-demand,it
ispivotalinmulti-robotsystemsthataredeployedinthereal-worldandmustread-
ilyadapttoinevitablechanges. Assuch,multi-robotpoliciesmustremainrobust
toteamchanges–anabilitywecalladaptiveteaming.Inthiswork,weinvestigate
ifawarenessandcommunicationofrobotcapabilitiescanprovidesuchgeneral-
ization by conducting detailed experiments involving an established multi-robot
testbed. Wedemonstratethatshareddecentralizedpolicies,thatenablerobotsto
bebothawareofandcommunicatetheircapabilities,canachieveadaptiveteam-
ingbyimplicitlycapturingthefundamentalrelationshipbetweencollectivecapa-
bilities and effective coordination. Videos of trained policies can be viewed at
https://sites.google.com/view/cap-comm.
Keywords: Heterogeneity,Multi-RobotTeaming,Generalization
1 Introduction
Heterogeneous robot teams have the potential to address complex real-world challenges that arise
in a wide range of domains, such as precision agriculture, defense, warehouse automation, supply
chainoptimization,andenvironmentalmonitoring.However,akeyhurdleinrealizingsuchpotential
isthechallengeofensuringeffectivecommunication,coordination,andcontrol.
Existingapproachestoaddressthechallengesofmulti-robotsystemscanbecrudelycategorizedinto
twogroups.First,classicalapproachesusewell-understoodcontrollerswithsimplelocalinteraction
rules,givingrisetocomplexglobalemergentbehavior[1]. Indeed,suchcontrollershaveprovenex-
traordinarilyusefulindiversedomains. However,designingthemrequiresbothsignificanttechnical
expertiseandconsiderabledomainknowledge. Second,recentlearning-basedapproachesalleviate
the need for expertise and domain knowledge by leveraging advances in learning frameworks and
computationalresources. Learninghasbeensuccessfulinmanydomains,suchasvideogames[2],
autonomousdriving[3],disasterresponse[4],andmanufacturing[5].
However,learningapproachesarenotwithouttheirfairshareoflimitations.First,themajorityofex-
istingmethodsfocusonhomogeneousteamsand,assuch,cannothandleheterogeneousmulti-robot
teams. Second,andmoreimportantly,evenexistingmethodsdesignedforheterogeneousteamsare
oftensolelyconcernedwiththechallengeoflearningtocoordinateagiventeam,entirelyignoring
thechallengeofgeneralizingthelearnedbehaviortonewteams. Giventhepotentiallyprohibitive
costofretrainingcoordinationpoliciesafterdeploymentinreal-worldsettings,itisimperativethat
multi-robotpoliciesgeneralizelearnedbehaviorstoinevitablechangestotheteam.
In this work, we focus on the challenge of generalizing multi-robot policies to team changes. In
particular, wefocusongeneralizationoftrainedpoliciestoteamsofnewcompositions, sizes, and
∗EqualContribution.†ThisworkwassupportedinpartbytheArmyResearchLabunderGrantsW911NF-
17-2-0181andW911NF-20-2-0036
7thConferenceonRobotLearning(CoRL2023),Atlanta,USA.
4202
naJ
32
]OR.sc[
1v72131.1042:viXraFigure 1: We investigate the role of capability awareness and communication in generalizing decentralized
heterogeneousmulti-robotcoordinationpoliciestoteamsofnewcomposition,size,androbots.
robotsthatarenotencounteredintraining(seeFig. 1). Werefertosuchgeneralizationasadaptive
teaming, wherein the learning policy can readily handle changes to the team without additional
training. To this end, we need policies that can reason about how a group of diverse robots can
collectivelyachieveacommongoal,withoutassigningrigidspecializedrolestoindividualrobots.
Weinvestigatetheroleofrobotcapabilitiesingeneralizationtonewteams. Ourkeyinsightisthat
adaptive teaming requires the understanding of how a team’s diverse capabilities combine to dic-
tate the behavior of individual robots. For instance, consider an autonomous heterogeneous team
respondingtomultipleconcurrentwildfires. Effectivecoordinationinsuchsituationsrequiresrea-
soningabouttheopportunitiesandconstraintsintroducedbytherobots’individualandrelativeca-
pabilities,suchasspeed,watercapacity,andbatteryrange. Ingeneral,robotsmustlearnhowtheir
individualcapabilitiesrelatetothoseofotherstodeterminetheirroleinachievingsharedobjectives.
We develop a policy architecture that can explicitly reason about robot capabilities when select-
ing actions. Our architecture has four key properties: i) capability awareness: our design enables
actionstobeconditionedoncontinuouscapabilitiesinadditiontoobservations,ii)capabilitycom-
munication: we leverage graph networks to learn how robots must communicate their capabilities
iii) robot-agnostic: we utilize parameter sharing and learn policies that are not tied to individual
robots, andiv)decentralized: ourtrainedpoliciescanbedeployedinadecentralizedmanner. To-
gether, these four properties provide the potential to generalize to new teams. One can view this
designasanextensionofagentidentificationtechniques[6]tothemetricspaceofcapabilities. As
such, capabilities do not merely serve to distinguish between agents during training to enable be-
havioralheterogeneity[7],butalsotoprovideamoregeneralmeanstoencodehowindividualand
relativecapabilitiesinfluencecollectivebehavior.
Weevaluatetheutilityofcapabilityawarenessandcommunicationintwoheterogeneousmulti-robot
tasksinsimandreal. Ourresultsrevealthatbothawarenessandcommunicationofcapabilitiescan
enableadaptiveteaming,outperformingpoliciesthatlackeitheroneorbothofthesefeaturesinterms
ofaveragereturnsandtask-specificmetrics.Further,capability-basedpoliciesachievesuperiorzero-
shotgeneralizationthanexistingagentidentification-basedtechniques,whileensuringcomparable
performanceonthetrainingset.
2 RelatedWork
Learning for multi-robot teams: Recent advances in deep learning are providing promising ap-
proaches that circumvent the challenges associated with classical control of multi-robot systems.
Multi-agentreinforcementlearning(MARL),inparticular,hasbeenshowntobecapableofsolving
awidevarietyoftasks,includingsimpletasksinthemulti-agentparticleenvironments(MPE)[8],
complex tasks under partial observability [9], coordinating an arbitrary number of agents in video
games [2], and effective predictive modeling of multi-agent systems [10]. These approaches are
2drivenbypopularMARLalgorithmslikeQMIX[11],MADDPG[8],andMAPPO[12]–nontrivial
extensionsoftheirsingleagentcounterpartsDQN[13],DDPG[14],andPPO[15],respectively.We
adopt a PPO-based learning framework given its proven benefits despite its simplicity [12]. Cen-
tralizedtraining,decentralizedexecution(CTDE)isacommonlyusedframeworkinwhichdecen-
tralizedagentslearntotakeactionsbasedonlocalobservationswhileacentralizedcriticprovides
feedbackbasedonglobalinformation[16,17]. WeusetheCTDEparadigmasitlendsitselfnatu-
rally to multi-robot teams since observation and communication are often restricted. However, its
importanttonotethatourapproachisagnostictothespecificlearningalgorithm.
Learningforheterogeneousteams: ManyMARLalgorithmswereoriginallydesignedforusein
homogeneous multi-agent teams. However, truly homogeneous multi-robot teams are rare except
becauseofmanufacturingdifferences,wearandtear,ortaskrequirements. Mostreal-worldmulti-
robot problems such as search & rescue, agriculture, and surveillance require a diverse set of ca-
pabilitiesaggregatedfromheterogeneousrobots[18–20]. WhilemanyMARLapproachesconsider
heterogeneity, they either tend to focus on differences in behavior exhibited by physically identi-
calrobots[21], oridenticalbehaviorexhibitedbyphysically-differentrobots[22,23]. Acommon
strategy used to elicit heterogeneous behavior from shared models is referred to as agent identifi-
cationorbehavioraltyping,inwhichtheagents’observationsareappendedwithanagent-specific
index[24,25]. Whilethesemethodshavebeenshowntobehighlyeffective, recentinvestigations
haverevealedissueswithscalability[26],androbustnesstoobservationnoise[7]. Whilecapability-
awarenessissimilarinspirittoexistingidentification-basedtechniques,itdoesnotrequireassigning
indicestoindividualrobotsandcanthusgeneralizetoteamswithnewrobots. Further,mostexisting
methodsdonotsimultaneouslyhandleteamswithphysicalandbehavioraldifferences. According
to a recent heterogeneous multi-robot taxonomy [7], our work falls under the category consisting
of physically-different robots that differ in behavior, but share the same objective. Two recent ap-
proaches belong to this same category [4, 7]. However, one is limited to a discrete set of robot
types[4]andtheotherlearneddecentralizedrobot-specificpoliciesthatcannothandletheaddition
ofnewrobotsandmightnotgeneralizetonewcompositions[7].
Generalization: For applications to real-world multi-robot systems, it is essential to consider the
generalizationcapabilitiesoflearnedcontrolpolicies. Inourformulation,therearetwoaxesofgen-
eralization in heterogeneous multi-robot teams: combinatorial generalization (new team sizes and
newcompositionsofthesamerobots)andindividualcapabilitygeneralization(newrobots). Prior
worksreliantonfeedforwardorrecurrentnetworkstendtobelimitedtoteamsofstaticsize[8,27].
Combinatorial generalization for homogeneous teams can be achieved with graph network-based
policies [27, 28]. However, existing methods tend to struggle with generalization in the presence
of heterogeneity [29]. While methods that employ agent identification [24, 25] might be able to
achieve combinatorial generalization by reusing the IDs from training, it is unclear how they can
handle new robots. In stark contrast, capability-based policies are robot-agnostic and can take the
capabilitiesofthenewrobotasaninputfeaturetodetermineitsactions.
3 CapabilityAwarenessandCommunicationforAdaptiveTeaming
In this section, we first model heterogeneous teams and then introduce policy architectures that
enablecapabilityawarenessandcommunication,alongwiththeassociatedtrainingpipeline.
3.1 ModelingHeterogeneousMulti-RobotTeams
WemodelteamsofN heterogeneousrobotsasagraphG = (V,E), whereeachnodev ∈ V isa
i
robot, and each edge e = (v ,v ) ∈ E is a communication link. We use z to denote the obser-
ij i j i
vationsoftheithrobot, whichincludesitscapabilitiesanditssensorreadingsoftheenvironment.
We assume that the robots’ heterogeneity can be captured by their capabilities. We represent the
capabilities of the ith robot by a real-valued vector c ∈ C ⊆ RC, where C is the C-dimensional
i +
spaceofallcapabilitiesoftherobots. Anexampleofamulti-dimensionalcapabilityisavectorwith
elements representing payload, speed, and sensing radius. When robot i does not possess the kth
capability,thekthelementofc issettozero.
i
33.2 ProblemDescription
Weareinterestedinlearningadecentralizedcontrolpolicythatcani)effectivelycoordinateateam
of heterogeneous robots to achieve the task objectives, and ii) generalize readily to teams of both
novel compositions and novel robots that are not encountered during training. Our problem can
be viewed as a multi-agent reinforcement learning problem that can be formalized as a decentral-
ized partially-observable Markov Decision Process (Dec-POMDP) [30]. We expand on the Dec-
POMDPformulationtoincorporatethecapabilitiesofheterogeneousrobotsandarriveatthetuple
⟨D,S,{A },{Z },C,T,R,O⟩ where D is the set of N robots, S is the set of global states, {A }
i i i
isasetofactionspacesacrossallrobots, {Z }isasetofjointobservationsacrossallrobots, C is
i
the multi-dimensional space of capabilities, R is the global reward function, and T and O are the
joint state transition and observation models, respectively. Our objective is to learn decentralized
action policies that control each robot to maximize the expected return
E[(cid:80)Th
r ] over the task
t=0 t
horizonT . Thedecentralizedpolicyoftheithrobotπ(a |o )definestheprobabilitythatRoboti
h i (cid:101)i
takesActiona givenitseffectiveobservationo . Theeffectiveobservationo oftheithrobotisa
i (cid:101)i (cid:101)i
functionofbothitsindividualobservationandthatofothersinitsneighborhood.
3.3 PolicyArchitecture
To enable capability awareness and communication in multi-robot coordination policies, we de-
signed a policy architecture that leverages graph convolutional networks (GCNs) since a plethora
of recent approaches attest to their ability to learn effective communication protocols and enable
decentralizeddecision-makinginmulti-agentteams[27]. Further,operationsarelocaltonodesand
canthereforegeneralizetographsofanytopology(i.e.,permutationinvariance[31]). Weillustrate
our architecture in Fig. 1 and explain its components below. Specific details including the exact
architectureandhyperparametersweusedcanbefoundinAppendixE.
Capabilityawareness: Wearguethattheheterogeneityofrobotscanhaveasignificantimpacton
howtheteammustcoordinatetoachieveatask. Specifically,individualandcollectivecapabilities
canaffecttherolestherobotsplaywithintheteam. Forinstance,consideraheterogeneousmobile
robot team responding to wildfire incidents at multiple locations. To effectively respond, a robot
withintheteammustaccountforitsspeedandwatercapacity. Therefore,itisnecessarythatrobots
areawareoftheircapabilities.Toenablesuchawareness,weappendeachrobot’scapabilityvectorc
i
toitsobservationsbeforepassingthemalongasnodefeaturestothegraphnetwork.Thisinformation
willhelpeachrobotconditionitsactionsnotjustonobservations,butalsoonitscapabilities.
Capability communication: In addition to awareness, communicating capability information can
enableateamtoreasonabouthowitscollectivecapabilitiesimpacttaskperformance.Revisitingour
wildfire example, robots in the response team can effectively coordinate their efforts by implicitly
anddynamicallytakingonrolesbasedontheirrelativespeedandwatercapacity. Butsuchcomplex
decision making is only possible if robots communicate with each other about their capabilities.
Each node in our GCN-based policy receives capability information along with the corresponding
robot’s local observations so the learned communication protocol can help the team communicate
andeffectivelybuildrepresentationsoftheircollectivecapabilities.
Note that our policy is robot-agnostic and learns the implicit and interconnected relationships be-
tweentheobservations,capabilities,andactionsofallrobotsintheteam.Further,aswedemonstrate
inourexperiments, capabilityawarenessandcommunicationenablesgeneralizationtoteamswith
newrobotcompositions, sizes, andeventoentirelynewrobotsaslongastheircapabilitiesbelong
tothesamespaceofcapabilitiesC.
3.4 TrainingProcedure
We utilize parameter sharing to train a single action policy that is shared by all of the robots. Pa-
rametersharingisknowntoimprovelearningefficiencybylimitingthenumberofparametersthat
must be learned. More importantly, parameter sharing is required for our problem so policies can
transfer to new robots without the need for training new policies or assigning robots to already
4trainedpolicies. Additionally,webelievethatparametersharingservesasecondaryroleinlearning
generalizablestrategiesforefficientgeneralization. Sharingparametersenablesthepolicytolearn
generalizedcoordinationstrategiesthat, whenconditionedonrobotcapabilitiesandlocalobserva-
tions,canbeadaptedtospecificrobotsandcontexts.
We employ a centralized training, decentralized execution (CTDE) paradigm to train the action
policy. We apply an actor-critic model, and train using proximal policy optimization (PPO). The
actor-critic model is composed of a decentralized actor network (i.e., shared action policy) that
mapsrobotsobservationstocontrolactions,andacentralizedcriticnetwork[12],whichestimates
the value of the team’s current state based on centralized information about the environment and
robotsaggregatedfromindividualobservations. Finally,wetrainedourpoliciesonmultipleteams
untiltheyconverged,withtheteamschangingevery10episodestostabilizetraining.
4 ExperimentalDesign
Weconducteddetailedexperimentstoevaluatehowcapabilityawarenessandcommunicationimpact
generalizationto: i)newteamsizesandcompositions,andii)newrobotswithunseencapabilities.
Environments: Wedesignedtwoheterogeneousmulti-robottasksforexperimentation:
• Heterogeneous Material Transport (HMT):Ateamofrobotswithdifferentmaterialcar-
ryingcapacitiesforlumberandconcrete(denotedbyc ∈ R2 fortheithrobot)musttransport
i
materialsfromlumberandconcretedepotstoaconstructionsitetofulfillapre-specifiedquota
while minimizing over-provision. We implemented this environment as a Multi-Particle Envi-
ronment(MPE)[32]andleveragetheinfrastructureofEPyMARL[33].
• Heterogeneous Sensor Network (HSN):Arobotteammustformasinglefully-connected
sensornetworkwhilemaximizingthecollectivecoveragearea.Theithrobot’scapabilityc ∈R
i
correspondstoitssensingradius. WeimplementedthisenvironmentusingtheMARBLER[34]
frameworkwhichenableshardwareexperimentationintheRobotarium[35],awell-established
multi-robottestbed.
Policy architectures: In order to systematically examine the impact of capability awareness and
communication,weconsiderthefollowingpolicyarchitectures:
• ID(MLP):RobotID-basedMLP
• ID(GNN):RobotID-basedGNN
• CA(MLP):Capability-awareMLP
• CA(GNN):Capability-awareGNNwithoutcommunicationofcapabilities,
• CA+CC(GNN):Bothcapabilityawarenessandcommunication.
TheID-basedbaselinesstandinforSOTAapproachesthatemploybehavioraltypingtohandlehet-
erogeneousteams[24,25], and, assuch, questiontheneedforcapabilities. TheMLPbasedbase-
lineshelpusinvestigatetheneedforcommunication. Finally,theCA(GNN)enablescommunication
ofobservationsbutdoesnotdoesnotcommunicatecapabilityinformation.
Metrics: Forbothenvironments,wecomparetheabovepoliciesusingAverageReturn: theaverage
joint reward received over the task horizon (higher is better). Additionally, we use environment-
specific metrics. In HMT, we terminate the episodes when the quotas for both materials are met.
Therefore, we consider Average Steps taken to meet the quota (lower is better). For HSN, we
consider Pairwise Overlap: sum of pairwise overlapping area of robots’ coverage areas (lower is
better).
Training: For each environment, we used five teams with four robots each during training. We
selected the training teams to ensure diverse compositions and degree of heterogeneity. For HSN,
wesampledrobots’sensingradiusfromtheuniformdistributionU(0.2,0.6). ForHMT,wesampled
robots’ lumber and concrete carrying capacities from the uniform distribution U(0,1.0). We also
assignedeachrobotaone-hotIDtotrainID-basedpolicies. Wetrainedeachpolicywith3random
seeds. Weresampledrobotteamsevery10episodestostabilizetraining.
5(a)HMT:Averagereturn (b)HMT:Averagesteps (c)HSN:Averagereturn (d)HSN:Overlaparea
Figure2: Whenevaluatedonteamsseenduringtraining,capability-awarepoliciesperformedcomparablyto
ID-basedpoliciesintermsofbothaveragereturn(higherisbetter)andtask-specificmetrics(lowerisbetter).
5 Results
Below,wereporti)performanceonthetrainingteam,ii)zero-shotgeneralizationtonewteams,and
iii)zero-shotgeneralizationtonewrobotswithunseenvaluesofcapabilitiesforeachenvironment.
5.1 HeterogeneousMaterialTransport
WefirstfocusontheHeterogeneousMaterialTransport(HMT)environment.
Performanceontrainingset: Toensureconsideringcapabilitiesdoesnotnegativelyimpacttrain-
ing,wefirstevaluatetrainedpoliciesonthetrainingsetintermsofaveragereturnandaveragesteps
(seeFig.2). Wefindthatallpoliciesresultedincomparableaveragereturns(Fig.2(a)). However,
theaveragenumberofstepsperepisodebettercapturesperformanceinHMT,sinceepisodestermi-
nate early when the quota is filled. Compared to agent-ID policies, capability-based policies took
fewerstepsperepisode(Fig.2(b))toachievecomparablerewards,suggestingthatreasoningabout
capabilitiescanimprovetaskefficiencyperformance.
Zero-shot generalization to new team compositions and sizes: We next evaluated how trained
policies generalize to team compositions and sizes not encountered during training. Fig. 3 shows
plotsquantifyingperformanceonnewcompositionswithteamsizesof3,4,and5robots. Toensure
a fair comparison, we evaluated all policies on the same set of 100 teams by randomly sampling
novel combinations of robots from the training set. We evaluated each policy on each test team
across10episodesperseed.Giventhatthisevaluationinvolvednonewindividualrobots,wereused
eachrobot’sIDfromthetrainingsettofacilitatetheevaluationofID-basedpolicies.
We find that all capability-aware methods outperformed ID-based methods in return and task-
specificmetrics. Thisislikelyduetocapability-awaremethods’abilitytocapturetherelationship
between robots’ carrying capacities and the material quota. In contrast, ID-based methods must
learntoimplicitlyreasonabouthowmuchmaterialeachrobotcancarry. Interestingly,CA(MLP)re-
sultedinfewerstepsperepisode(lowestmeanandvariance)acrossallteamsizes,andoutperformed
boththeothercapability-basedandcommunication-enabledbaselines: CA(GNN)andCA+CC(GNN).
This suggests that mere awareness of capabilities is sufficient to perform well in the HMT environ-
ment. Indeed,communicationisnotasessentialinthistaskasrobotscandirectlyobserverelevant
information(e.g.materialdemands)andimplicitlycoordinateaslongastheyareawareoftheirown
capabilities. It might be possible to further improve performance by learning to better communi-
cate, butthatwouldbesignificantlymorechallengingsincethetaskcanbemostlysolvedwithout
communication.
Zero-shotgeneralizationtonewrobots: InFig. 4, weshowthepolicies’abilitytogeneralizeto
teamscomposed ofnew robots. Sincethe robots’capabilities inthisevaluation aredifferent from
those of the robots in the training set, we could not evaluate agent-ID methods since there is no
trivialwaytoassignIDstothenewrobots. Theseresultsclearlydemonstratethatreasoningabout
capabilitiescanenablegeneralizationtoteamswithentirelynewrobots. Further,weagainseethat
capabilityawarenesswithoutcommunicationissufficienttogeneralizeintheHMTenvironment.
6(a)3Robots (b)4Robots (c)5Robots
Figure3:WhengeneralizingtonewteamcompositionsandsizesinHMT,capability-basedpoliciesconsistently
outperformedID-basedpoliciesintermsofaveragestepstakentomeetthequota(lowerisbetter).
(a)3robots (b)4robots (c)5robots
Figure 4: When generalizing to new robots with unseen values for capabilities in HMT, policies that are
onlyawareofcapabilities(CA(MLP)andCA(GNN))outperformedpoliciesthatalsocommunicatedcapabilities
(CA+CC(GNN))intermsofaveragenumberofstepstakentotransporttherequiredmaterial(lowerisbetter).
Additional results: We provide additional results for HMT by reporting more task-specific metrics
and evaluations on significantly larger teams in Appendix A. The results on task-specific metrics
further support the claim that capability-aware methods generalize better than ID-based methods.
Wealsofindthatthesebenefitsextendtoteamsconsistingof8,10,and15robots.Takentogether,the
aboveresultssuggestthatreasoningaboutcapabilities(ratherthanassignedIDs)improvesadaptive
teaming,likelyduetotheabilitytomapcapabilitiestoimplicitroles.
5.2 HeterogeneousSensorNetwork
Below,wediscussresultsontheHeterogeneousSensorNetwork(HSN)environment.
Performanceontrainingset: InFig. 2(c)and(d),wereporttheperformanceoftrainedpolicies
in HSN on teams in the training set in terms of average return and pairwise overlap. All policies
except CA(MLP) performed comparably and were able to effectively learn to maximize expected
returns, achieve a fully connected sensor network, and minimize the pairwise overlap in coverage
area. Further, CA+CC(GNN) and ID(GNN) perform similarly but marginally better than the other
baselines. CA(MLP)’ssuboptimalperformanceindicatesthatcapabilityawarenessinisolationwith-
out any communication hurts performance in the HSN task. Indeed, while it is possible to achieve
goodperformanceinHMTwithoutcommunicating,HSNrequiresrobotstoeffectivelycommunicate
andreasonabouttheirneighbors’sensingradiiinordertoformeffectivenetworks.
Takentogether,theseresultssuggestthatcapabilityawarenessandcommunicationcanleadtoeffec-
tivetraininginheterogeneousteams. ID-basedmethodsareabletoperformatasimilarlevel. This
istobeexpectedgiventhatweconductedtheseevaluationsonthetrainingsetandIDsaresufficient
toimplicitlyassignrolesandcoordinateheterogeneousrobotswithinknownteams.
Zero-shot generalization to new team compositions and sizes: In Fig. 5, we report the perfor-
manceofthetrainingpoliciesinHSNwhenevaluatedonteamsofdifferentcompositionsandsizes.
We found that CA+CC(GNN) achieved the best average returns (highest mean and lowest variance)
across all team sizes. Both ID-based methods (ID(MLP) and ID(GNN)) resulted in lower returns
comparedtoallthreecapability-awarenessbaselines. Notethatthisisinstarkcontrasttotheresults
for the training set in Fig. 2, demonstrating that IDs alone might help train heterogeneous teams
7buttendtogeneralizepoorlytonewteamcompositionsandsizes. ThisislikelybecauseID-based
policiesfailtoreasonaboutrobotheterogeneity,andinsteadoverfittherelationshipsbetweenrobot
IDsandbehaviorinthetrainingset. Further, CA+CC(GNN)inparticularconsistentlyoutperformed
allotherpoliciesacrossmetricsandvariations,suggestingthatbothcapabilityawarenessandcom-
municationarenecessarytoenablegeneralizationinHSN.
Zero-shotgeneralizationtonewrobots: Weevaluatedthetrainedpolicies’abilitytogeneralizeto
teamsofdifferentsizeswhicharecomposedofentirelynewrobotswhosesensingradiiaredifferent
fromthoseencounteredintraining. SimilartoHMT,wecannotevaluateID-basedpoliciesonteams
withnewrobotssincethereisnoobviouswaytoassignIDstothenewrobots. InFig.6,wereport
theperformanceofallthreecapability-basedpoliciesintermsofaveragereturn. BothGNN-based
policies (CA(GNN) and CA+CC(GNN)) considerably outperform the CA(MLP) policy, underscoring
theimportanceofcommunicationingeneralizationtoteamswithnewrobots. However,wealsosee
thatcommunicationofobservationsaloneisinsufficient,asevidencedbythefactthatCA+CC(GNN)
(whichcommunicatesbothobservationsandcapabilities)consistentlyoutperformsCA(GNN)(which
onlycommunicatesobservations).
Real-robotdemonstrations: WealsodeployedthetrainedpoliciesofCA+CC(GNN),CA(MLP),and
CA(GNN)onthephysicalRobotarium(seeSectionB.1forfurtherdetailsandsnapshots). Overall,
wefindthatthebenefitsreportedaboveextendtophysicalrobotteams. WefindthatCA+CC(GNN)
and CA(GNN) policies generalize to physical robots and successfully build a sensor network while
minimizing sensing overlap for teams of 3 and 4 robots. The CA(MLP) policy resulted in signif-
icantly worse performance, where robots’ executed paths provoked significant engagement of the
Robotarium’sbarriercertificatesduetopotentialcollisions.
Additional results: We provide additional results for HSN by reporting more task-specific metrics
in Appendix B. Much like the results for HMT, the results on task-specific metrics further support
theclaimthatcapability-awaremethodsshowsuperioradaptiveteamingabilitycomparedwithID-
basedmethods. Theadditionalresultsalsosupportourclaiminthissectionthatcommunicationof
capabilitiesisessentialforsuccessonthistask.
6 Limitations
While our framework could reason about many different capabilities simultaneously, our experi-
ments only involved variations in 1-D and 2-D capabilities. We also only consider generalization
tonewvaluesforcapabilities; wedonotconsidergeneralizationtonewtypesofcapabilities. Ad-
ditionally, our work only considers the representation of robot’s capabilities that we can quantify.
Handlingimplicitcapabilitiesandcommunicationthereofmaybenefitfromadditionalmeta-learning
mechanisms, uncovering a more general relationship between robots’ learned behaviors and capa-
bilities. Further, wedonotconsiderhigh-levelplanningandtask-allocationandrelysolelyonthe
learning framework to perform implicit assignments to sub-tasks within the macro task. Future
work can investigate appropriate abstractions and interfaces for considering both learning-based
low-level policies and efficient algorithms for higher-level coordination. Lastly, we only consid-
(a)3Robots (b)4Robots (c)5Robots
Figure5:WhengeneralizingtonewteamcompositionsandsizesinHSN,capability-basedpoliciesconsistently
outperformedID-basedbaselinesintermsofaveragereturn(higherisbetter). Further,combiningawareness
andcommunicationofcapabilitiesresultedinthebestgeneralizationperformance.
8(a)3Robots (b)4Robots (c)5Robots
Figure6: WhengeneralizingtoteamscomprisedofnewrobotsinHSN,combiningawarenessandcommuni-
cationofcapabilities(CA+CC(GNN))achieveshigheraveragereturnsthanbaselinesthataremerelyawareof
capabilities,irrespectiveofwhethertheycommunicateobservations(CA(GNN))ornot(CA(MLP)).
eredfully-connectedcommunicationgraphsinourevaluationsforsimplicity.Whilegraphnetworks
areknowntoeffectivelysharelocalobservationsforglobalstateestimationsinpartially-connected
teams[27,36],itisunclearifsuchabilitywilltranslatetothecommunicationofcapabilities.
7 Conclusion
We investigated the utility of awareness and communication of robot capabilities in the general-
izationofheterogeneousmulti-robotpoliciestonewteams. Wedevelopedagraphnetwork-based
decentralizedpolicyarchitecturebasedonparametersharingthatenablesrobotstoreasonaboutand
communicatetheirobservationsandcapabilitiestoachieveadaptiveteaming. Ourdetailedexperi-
mentsinvolvingtwoheterogeneousmulti-robottasksunambiguouslyillustratetheimportanceand
theneedforreasoningaboutcapabilitiesasopposedtoagentIDs.
9References
[1] J. Corte´s and M. Egerstedt. Coordinated control of multi-robot systems: A survey. SICE
JournalofControl,Measurement,andSystemIntegration,2017.
[2] B. Ellis, S. Moalla, M. Samvelyan, M. Sun, A. Mahajan, J. N. Foerster, and S. Whiteson.
Smacv2: Animprovedbenchmarkforcooperativemulti-agentreinforcementlearning,2022.
[3] S. Shalev-Shwartz, S. Shammah, and A. Shashua. Safe, multi-agent, reinforcement learning
forautonomousdriving,2016.
[4] E. Seraj, Z. Wang, R. Paleja, D. Martin, M. Sklar, A. Patel, and M. Gombolay. Learning
efficientdiversecommunicationforcooperativeheterogeneousteaming. InProceedingsofthe
21stInternationalConferenceonAutonomousAgentsandMultiagentSystems,2022.
[5] Z. Wang, C. Liu, and M. Gombolay. Heterogeneous graph attention networks for scalable
multi-robotschedulingwithtemporospatialconstraints. AutonomousRobots,2022.
[6] C. Li, T. Wang, C. Wu, Q. Zhao, J. Yang, and C. Zhang. Celebrating Diversity in Shared
Multi-AgentReinforcementLearning,2021.
[7] M. Bettini, A. Shankar, and A. Prorok. Heterogeneous multi-robot reinforcement learning,
2023.
[8] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch. Multi-agent actor-critic for
mixedcooperative-competitiveenvironments. NeuralInformationProcessingSystems(NIPS),
2017.
[9] S. Omidshafiei, J. Pazis, C. Amato, J. P. How, and J. Vian. Deep decentralized multi-task
multi-agentreinforcementlearningunderpartialobservability. InInternationalConferenceon
MachineLearning,2017.
[10] Y.Hoshen. Vain: Attentionalmulti-agentpredictivemodeling. InAdvancesinNeuralInfor-
mationProcessingSystems,2017.
[11] T. Rashid, M. Samvelyan, C. S. de Witt, G. Farquhar, J. Foerster, and S. Whiteson. Qmix:
Monotonicvaluefunctionfactorisationfordeepmulti-agentreinforcementlearning,2018.
[12] C.Yu,A.Velu,E.Vinitsky,Y.Wang,A.M.Bayen,andY.Wu. Thesurprisingeffectiveness
ofMAPPOincooperative,multi-agentgames. CoRR,2021.
[13] V.Mnih,K.Kavukcuoglu,D.Silver,A.Graves,I.Antonoglou,D.Wierstra,andM.Riedmiller.
Playingatariwithdeepreinforcementlearning,2013.
[14] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra.
Continuouscontrolwithdeepreinforcementlearning,2019.
[15] J.Schulman,F.Wolski,P.Dhariwal,A.Radford,andO.Klimov.Proximalpolicyoptimization
algorithms,2017.
[16] V.EgorovandA.Shpilman. Scalablemulti-agentmodel-basedreinforcementlearning,2022.
[17] J.Foerster,G.Farquhar,T.Afouras,N.Nardelli,andS.Whiteson. Counterfactualmulti-agent
policygradients,2017.
[18] T.Dang,M.Tranzatto,S.Khattak,F.Mascarich,K.Alexis,andM.Hutter. Graph-basedsub-
terraneanexplorationpathplanningusingaerialandleggedrobots. JournalofFieldRobotics,
2020.
10[19] T.Dang,M.Tranzatto,S.Khattak,F.Mascarich,K.Alexis,andM.Hutter. Graph-basedsub-
terraneanexplorationpathplanningusingaerialandleggedrobots. JournalofFieldRobotics,
2020.
[20] G.Gil,D.E.Casagrande,L.P.Corte´s,andR.Verschae.Whythelowadoptionofroboticsinthe
farms? challengesfortheestablishmentofcommercialagriculturalrobots. SmartAgricultural
Technology,2023.
[21] C.Li,T.Wang,C.Wu,Q.Zhao,J.Yang,andC.Zhang. Celebratingdiversityinsharedmulti-
agentreinforcementlearning. AdvancesinNeuralInformationProcessingSystems,2021.
[22] J.K.Terry,N.Grammel,S.Son,andB.Black. Parametersharingforheterogeneousagentsin
multi-agentreinforcementlearning,2022.
[23] C.Wakilpoor,P.J.Martin,C.Rebhuhn,andA.Vu. Heterogeneousmulti-agentreinforcement
learningforunknownenvironmentmapping. arXivpreprintarXiv:2010.02663,2020.
[24] J.Foerster,I.A.Assael,N.DeFreitas,andS.Whiteson. Learningtocommunicatewithdeep
multi-agentreinforcementlearning.Advancesinneuralinformationprocessingsystems,2016.
[25] J. K. Gupta, M. Egorov, and M. Kochenderfer. Cooperative multi-agent control using deep
reinforcementlearning. InAutonomousAgentsandMultiagentSystems: AAMAS2017Work-
shops,BestPapers,Sa˜oPaulo,Brazil,May8-12,2017,RevisedSelectedPapers16.Springer,
2017.
[26] F. Christianos, G. Papoudakis, M. A. Rahman, and S. V. Albrecht. Scaling multi-agent rein-
forcementlearningwithselectiveparametersharing. InInternationalConferenceonMachine
Learning.PMLR,2021.
[27] A. Agarwal, S. Kumar, and K. Sycara. Learning transferable cooperative behavior in multi-
agentteams,2019.
[28] Q.Li,W.Lin,Z.Liu,andA.Prorok. Message-awaregraphattentionnetworksforlarge-scale
multi-robotpathplanning,2021.
[29] A. Mahajan, M. Samvelyan, T. Gupta, B. Ellis, M. Sun, T. Rockta¨schel, and S. Whiteson.
Generalizationincooperativemulti-agentsystems,2022.
[30] F.A.OliehoekandC.Amato. AConciseIntroductiontoDecentralizedPOMDPs. Springer-
BriefsinIntelligentSystems,2016.
[31] A.Khan,E.Tolstaya,A.Ribeiro,andV.Kumar. Graphpolicygradientsforlargescalerobot
control,2019.
[32] I. Mordatch and P. Abbeel. Emergence of grounded compositional language in multi-agent
populations. CoRR,2017. URLhttp://arxiv.org/abs/1703.04908.
[33] G. Papoudakis, F. Christianos, L. Scha¨fer, and S. V. Albrecht. Benchmarking multi-agent
deep reinforcement learning algorithms in cooperative tasks. In Proceedings of the Neural
InformationProcessingSystemsTrackonDatasetsandBenchmarks(NeurIPS),2021.
[34] R.Torbati,S.Lohiya,S.Singh,M.S.Nigam,andH.Ravichandar. Marbler:Anopenplatform
forstandardizedevaluationofmulti-robotreinforcementlearningalgorithms,2023.
[35] S. Wilson, P. Glotfelter, L. Wang, S. Mayya, G. Notomista, M. Mote, and M. Egerstedt.
Therobotarium: Globallyimpactfulopportunities,challenges,andlessonslearnedinremote-
access,distributedcontrolofmultirobotsystems. IEEEControlSystemsMagazine,2020.
[36] Q. Li, F. Gama, A. Ribeiro, and A. Prorok. Graph neural networks for decentralized multi-
robotpathplanning. CoRR,2019.
[37] J.Schulman,F.Wolski,P.Dhariwal,A.Radford,andO.Klimov.ProximalPolicyOptimization
Algorithms. 2017.
11A HeterogeneousMaterialTransport(HMT)AdditionalResults
ThissectionprovidesadditionalresultsfortheHMTenvironment.Specifically,weprovideadditional
task-specific metrics for the generalization experiments, and new generalization results for robot
teamsofsignificantlylargersizes(i.e. teamsizesof8,10,and15robots).
Thetask-specificmetricsdefinedbelowevaluatetherateatwhicheachpolicycontributestofulfilling
thetotalquota,andtheindividualquotasforlumberandconcrete:
• %ofepisodesbywhichthetotalquotawasfilled.
• %oflumberquotaremaining.
• %ofconcretequotaremaining.
For both generalization to new teams (see Fig. 7) and new robots (see Fig. 8), capability-aware
methodsfilledthetotalquotainfewerepisodestepscomparedtotheID-basedmethods,whilegen-
erallybetterpreventingover-provisioningofbothlumberandconcrete. Thisresultfurthersupports
ourclaimthatcapabilityawarenessimprovesgeneralizationperformance.ObservingFig.9,wefind
thatthesebenefitsofcapability-basedpoliciesextendtoconsiderablylargerteamsizes.
(a)3robots (b)4robots (c)5robots
Figure7:PolicieswithcapabilityawarenessoutperformagentIDmethodsatmeetingthematerialquotawitha
minimalnumberofstepswhengeneralizingtonewteamcompositions.Capability-awarenessmethodswithout
communicationofcapabilities(i.e.CA(MLP)&CA(GNN))outperformmethodswithcapabilitycommunication
forthistask.
B HeterogeneousSensorNetwork(HSN)AdditionalResults
Thissectionprovidesadditionalresultsoni)trainingperformanceandii)task-specificmetrics.
Thetask-specificmetricsfortheHSNenvironmentarethefollowing:
12(a)3robots (b)4robots (c)5robots
Figure8:Policieswithoutcommunicationofcapability-awareness(i.e. CA(MLP)andCA(GNN))outperformed
thepolicywithcommunicationofcapabilities(CA+CC(GNN)ontask-specificmetricswhengeneralizingtonew
robotswithcapabilitiesnotseenduringtraining.
• Pairwiseoverlap: Thesumofpairwiseoverlapincoverageareaamongrobots(lowerisbetter).
• %offullyconnectedteams(byepisodestep):Percentageofteamsthatmanagedtoformasensor
networkthatconnectsalloftherobots(higherisbetter).
InFig. 12,wereportthetrainingperformance(i.e. trainingteamsonly)foreachpolicy. Thetrain-
ingcurvesuggestthatcapability-awareandID-basedmethodsperformcomparablyduringlearning.
Notably,thecommunicationmodelsID(GNN)andCA+CC(GNN)convergefasterandachievehigher
overallreturnsthanothermethods. Thisresultsuggeststhatcommunicationbetweenrobotssignifi-
cantlyassistsinlearningcollaborativebehavior.
Capability-awarepoliciesagaindemonstratesuperiorperformancewhengeneralizingtonewteams
(seeFig. 10)andnewrobots(seeFig. 11)ontask-specificmetrics,highlightingtheimportanceof
capabilityawarenessforgeneralizationtorobotteamswithnewrobots,teamsizes,andteamcom-
positions. Notably, CA+CC(GNN) results in significantly lower pairwise overlap for robot teams of
size 3 and 4 robots, and marginally lower pairwise overlap for robot teams of size 5, compared to
ID-based methods. This suggests the communication-enabled policy effectively learns to commu-
nicatecapabilitiesandthatsuchcommunicationofcapabilitiesisessentialforgeneralizationinthis
task.
B.1 ImagesofRobotariumExperiments
In this section, we present visual representations derived from actual robot demonstrations of the
trained capability-aware communication policy. Videos of the robot demonstrations can be found
at: https://sites.google.com/view/cap-comm.
13S
(a)8robots (b)10robots (c)15robots
Figure9:Experimentsevaluatingthegeneralizationofpoliciestosignificantlylargerteams(size8,10,and15)
comparedtothetrainingteamsize(size4). PolicieswithcapabilityawarenessoutperformagentIDmethods
atmeetingthetotalmaterialquotawithaminimalnumberofstepswhengeneralizingtonew,largeteamcom-
positions. Capability-awarenessmethodswithoutcommunicationofcapabilities(i.e. CA(MLP)&CA(GNN))
outperformmethodswithcapabilitycommunicationforthistask.
14(a)3Robots (b)4Robots (c)5Robots
Figure10: Capability-basedpolicyarchitecturesconsistentlyoutperformID-basedbaselinesbothintermsof
averagereturnandtaskperformancemetricswhengeneralizingtonewteamcompositionsandsizes. Further,
combiningawarenessandcommunicationofcapabilitiesresultsinthebestgeneralizationperformance.
(a)3Robots (b)4Robots (c)5Robots
Figure11:Policyarchitecturethatcombinesawarenessandcommunication(CA+CC(GNN))ofcapabilitiesout-
performsbothotherbaselines(CA(GNN)intermsof%fullyconnectedandCA(MLP)intermsofaveragereturn
andpairwiseoverlap)whengeneralizingtoteamscomprisedofnewrobots.
15Figure 12: For the HSN environment, capability-aware policies perform comparably to ID-based policies in
termsoftrainingefficiency(first)andintermsoftask-specificmetricswhenevaluatingthetrainedpolicyon
thetrainingset.
(a)Beginningofepisode(3robots). (b)Endofepisode(3robots).
(c)Beginningofepisode(4robots). (d)Endofepisode(4robots).
Figure13: DemonstrationsofCA+CC(GNN)policydeployedtorealrobotteamsintheRobotariumtestbedfor
theHSNtask. Seehttps://sites.google.com/view/cap-commforvideosofdeploymenttotheRobotar-
ium.
16C EnvironmentSpecifications
C.1 HeterogeneousMaterialTransport
Thissectiondescribesadditionaldetailsabouttheheterogeneousmaterialtransport(HMT)environ-
ment.
Figure14:IntheHeterogeneousMaterialTransport(HMT)environment,eachagent’scolorisamixtureofblue
andred,whichrepresentsitsbiastowardsitscarryingcapacityforeitherlumber(red)orconcrete(blue). The
objectiveoftheteamistofillthelumberandconcretequotaattheconstructionsitewithoutdeliveringexcess.
ThelumberandconcretequotalimitfortheHMTenvironmentarerandomlyinitializedtoaninteger
valuebetween(0.5×n agents)and(2.0×n agents).
Robots have five available actions: they can move left, right, up, down, or stop. At the beginning
ofeachepisode,alloftherobotsbeginatarandompositionintheconstructionsitezone(seeFig.
14). Theobservationspaceforarobotisthecombinationoftherobot’sstateandtheenvironment’s
state: specifically it is composed of the robot’s position, velocity, amount of lumber and concrete
it’scarrying,anditsdistancetoeachdepot,thetotallumberquota,thetotalconcretequota,thetotal
amount of lumber delivered, and the total amount of concrete delivered. Robots’ observations do
not contain state information about other robots. Finally, we append the robot’s unique ID for the
ID baseline methods and the robot’s maximum lumber and concrete carrying capacity for the CA
methodstotherobots’observations.
ThetotalrewardforeachrobotinHMTiscomputedbysummingtheindividualrewardsofeachrobot.
Robotsarerewardedwhentheymakeprogressinmeetingthelumberandconcretequotasandare
penalized when they exceed the quota. If a robot enters the lumber depot or concrete depot, and
therobotisempty(i.e. notloadedwithanylumberorconcrete),andthequotahasyettobefilled,
thentherobotisrewardedwithpickuprewardof0.25. Iftherobotisloadedwithmaterial,thenthe
robotisrewardedorpenalizedwhenitdropsoffthematerialattheconstructionsite. Specifically,
whenarobotdeliversamaterialandthequotaforthatmaterialhasyettobefilled,thentherobotis
rewardedwithapositivedropoffrewardof0.75. However,iftherobotdeliversamaterialandgoes
over the quota, then the robot is penalized with a negative surplus penalty reward proportional to
the amount of surplus: −0.10×surplusmaterial. Finally, robots received a small time penalty of
−0.005foreachepisodestepinwhichthetotalquotaisnotfilled;thispromotestherobotstofinish
thetaskasquicklyaspossible.
17C.2 HeterogeneousSensorNetwork
(a)OuragentsrunninginsimulationusingtheMAR-
BLERframework. (b)OuragentsrunninginthephysicalRobotarium.
Thissectiondescribesadditionaldetailsabouttheheterogeneoussensornetwork(HSN)environment.
Robotshavefiveavailableactions: theycanmoveleft,right,up,down,orstop. Afterselectingan
action, the robots move in their selected direction for slightly less than a second before selecting
a new action. The robots start at random locations least 30cm apart from each other, move at
∼21cm/second,andutilizebarriercertificates[35]thattakeseffectat17cmawaytoensuretheydo
notcollidewhenrunninginthephysicalRobotarium.
The reward from the heterogeneous sensor network environment is a shared reward. We describe
therewardbelow:
D(i,j)=||p(i)−p(j)||−(c +c )
i j
(cid:26)
−0.9∗|D(i,j)|+0.05, ifD(i,j)<0
r(i,j)=
−1.1∗|D(i,j)|−0.05, otherwise
N
(cid:88)
R= r(i,j)
i<j
whereiandj arerobots,p(i)isthepositionofroboti,c isthe(capability)sensingradiusofrobot
i
i, and R is the cumulative team reward shared by all the robots. The above reward is designed
to reward the team when robots connect their sensing regions while minimizing overlap so as to
maximizethetotalsensingarea.
D TrainingandEvaluationSpecifications
Thissectiondescribesthedesignofthetrainingteamsandthesamplingofevaluationteamsforboth
environments. To learn generalized coordination behavior, the training teams were required to be
diverseintermsofcompositionandcapturetheunderlyingdistributionofrobotcapabilities.
D.1 HeterogeneousMaterialTransport
TrainingTeamNumber (concrete capacity,lumber capacity)
1 (0.9,0.1),(0.7,0.3),(1.0,0.0),(0.0,1.0)
2 (0.9,0.1),(0.7,0.3),(0.0,1.0),(0.2,0.8)
3 (0.8,0.2),(0.3,0.7),(0.4,0.6),(0.7,0.3)
4 (1.0,0.0),(0.0,1.0),(0.1,0.9),(0.3,0.7)
5 (0.6,0.4),(0.3,0.7),(0.7,0.3),(0.0,1.0)
Table1:TrainingteamsusedfortheHMTtask(fiveteamsoffourrobotseach).
18D.2 HeterogeneousSensorNetwork
To design these training teams, we first binned robot capabilities into small, medium, and large
sensingradiiwithbinranges[0.2m,0.33m],[0.33m,0.46m],and[0.46m,0.60m]respectively. We
then generated all possible combinations with replacements for teams composed of four robots of
small, medium, andlargerobotsforatotalof15teams. Eachrobotassignedtooneofthebins
small,medium,andlargehaditscapability(i.e. sensingradius)uniformlysampledwithinthebin
range. Thisresultedin15totalteams,forwhichwehand-selected5sufficientlydiverseteamstobe
thetrainingteams. TheresultingtrainingteamsaregiveninTable2.
TrainingTeamNumber RobotSensingRadii(m)
1 (0.2191),(0.2946),(0.2608),(0.3668)
2 (0.2746),(0.2746),(0.5824),(0.5756)
3 (0.3178),(0.3467),(0.5317),(0.6073)
4 (0.2007),(0.5722),(0.5153),(0.4622)
5 (0.4487),(0.5526),(0.5826),(0.58343)
Table2:TrainingteamsusedfortheHSNtask(fiveteamsoffourrobotseach).
Theevaluationrobotteamsweresampleddifferentlyforthedifferentexperimentalevaluationsper-
formed. In the training evaluation experiment, the teams were the same as the training teams in
Table 2. Teams for the generalization experiment to new team compositions, but not new robots,
weresampledrandomlyfromthe20robotsfromthetrainingteams(withreplacement). Eachrobot
from the pool of 20 robots was sampled with equal probability. In contrast, teams for the gener-
alizationexperimenttonewrobotsweregeneratedbyrandomlysamplingnewrobots, whereeach
robot’ssensingradiuswassampledfromauniformdistributionindependentlyU(0.2m,0.6m). For
the two generalization experiments, 100 total teams were sampled. Each algorithm was evaluated
onthesamesetofsampledteamsbyfixingthepseudorandomnumbergenerator’sseed.
We first focus on the training curves and subsequent evaluations conducted on the training set,
withoutconsideringgeneralization. Thegoalofthisexperimentistoensureintroducingcapabilities
does not negatively impact training. The learning curves in terms of average return are shown in
Fig. 12. Allmodelsachievedconvergencewithin20millionenvironmentsteps,withID(GNN)and
CA+CC(GNN)exhibitedboththefastestconvergenceandthehighestreturns. Theseresultssuggest
that communication of individual robot features, whether based on IDs or capabilities, improves
learningefficiencyandperformanceforheterogeneouscoordination.
E PolicyDetails
E.1 GraphNeuralNetworks
Weemployagraphconvolutionalnetwork(GCN)architectureforthedecentralizedpolicyπ ,which
i
enablesrobotstocommunicateforcoordinationaccordingtotherobotcommunicationgraphG.
AGCNiscomposedofLlayersofgraphconvolutions,followedbynon-linearity. Inthiswork,we
considerasinglegraphconvolutionlayerappliedtonodeiisgivenby
 
h( il) =σ (cid:88) ϕ θ(h( jl−1))
j∈N(i)∪i
whereh(l−1) ∈RF isthenodefeatureofnodej,N(i)={j|(v ,v )∈E}areallnodesjconnected
j i j
toi,ϕ isnodefeaturetransformationfunctionwithparametersθ,σ isanon-linearity(e.g. Relu),
θ
andhl ∈RGistheoutputnodefeature.
i
E.2 PolicyArchitectures
EachofthegraphneuralnetworksintheGNN-basedpolicyarchitecturesevaluatedarecomposedof
aninputencodernetwork,amessagepassingnetwork,andanactionoutputnetwork. Theencoder
19networkisa2-layerMLPwithhiddendimensionsofsize64. Forthemessagepassingnetwork, a
singlegraph convolutionlayer composedof 2-layerMLPs withReLU non-linearactivations. The
actionoutputnetworkisadditionallya2-layerMLPwithhiddendimensionsofsize64.Thelearning
rateis0.005.
MLP(ID)/MLP(CA): The MLP architectures compose of a 4-layer multi-layer perceptron with 64
hiddenunitsateachlayerandReLUnon-linearities.
CA(GNN)/CA+CC(GNN)/ID(GNN):Eachofthegraphneuralnetworkscomposeofaninput“encoder”
network, amessagepassingnetwork, andanactionoutputnetwork. Theencodernetworkandthe
action output network are multi-layer perceptrons with hidden layers of size 64, ReLU non-linear
activations, and with one and two hidden layers respectively. The message passing network is a
graphconvolutionlayerwhereinthelineartransformationofnodefeatures(i.e.observations)isdone
bya2-layerMLPwithReLUnon-linearactivationsand64dimensionalhiddenunits,followedbya
summationofthetransformedneighboringnodefeatures. Theouptutnodefeaturesaconcatenated
withtheoutputfeaturefromtheencodernetwork. Thisconcatenatedfeaturesistheinputtothetwo
out action network. The CA(GNN) network doesn’t communicate the robot’s capabilities with the
graphconvolutionlayers. Rather,thecapabilitiesareappendedtotheoutputoftheencodernetwork
andoutputofnodefeaturesofthegraphconvolutionlayerjustbeforethetheactionnetwork. Thus,
thetheactionnetworkistheonlypartofthismodelthatisconditionedonrobotcapabilities.
E.3 PolicyTrainingHyperparameters
Wedetailthehyperparametersusedtotraineachofthepoliciesusingproximalpolicyoptimization
(PPO)[37]inTable3.
Hyperparameter Value
ActionSelection(Training) soft action selection
ActionSelection(Testing) hard action selection
CriticNetworkUpdateInterval 200steps
LearningRate 0.0005
EntropyCoefficient 0.01
Epochs 4
Clip 0.2
QFunctionSteps 5
BufferLength 64
Numberoftrainingsteps 40×106(HMT),20×106(HSN)
Table3:HyperparametersusedtotraineachofthepolicieswithPPO.
20