[
    {
        "title": "Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities",
        "authors": "Yiyuan ZhangXiaohan DingKaixiong GongYixiao GeYing ShanXiangyu Yue",
        "links": "http://arxiv.org/abs/2401.14405v1",
        "entry_id": "http://arxiv.org/abs/2401.14405v1",
        "pdf_url": "http://arxiv.org/pdf/2401.14405v1",
        "summary": "We propose to improve transformers of a specific modality with irrelevant\ndata from other modalities, e.g., improve an ImageNet model with audio or point\ncloud datasets. We would like to highlight that the data samples of the target\nmodality are irrelevant to the other modalities, which distinguishes our method\nfrom other works utilizing paired (e.g., CLIP) or interleaved data of different\nmodalities. We propose a methodology named Multimodal Pathway - given a target\nmodality and a transformer designed for it, we use an auxiliary transformer\ntrained with data of another modality and construct pathways to connect\ncomponents of the two models so that data of the target modality can be\nprocessed by both models. In this way, we utilize the universal\nsequence-to-sequence modeling abilities of transformers obtained from two\nmodalities. As a concrete implementation, we use a modality-specific tokenizer\nand task-specific head as usual but utilize the transformer blocks of the\nauxiliary model via a proposed method named Cross-Modal Re-parameterization,\nwhich exploits the auxiliary weights without any inference costs. On the image,\npoint cloud, video, and audio recognition tasks, we observe significant and\nconsistent performance improvements with irrelevant data from other modalities.\nThe code and models are available at https://github.com/AILab-CVC/M2PT.",
        "updated": "2024-01-25 18:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.14405v1"
    },
    {
        "title": "Deconstructing Denoising Diffusion Models for Self-Supervised Learning",
        "authors": "Xinlei ChenZhuang LiuSaining XieKaiming He",
        "links": "http://arxiv.org/abs/2401.14404v1",
        "entry_id": "http://arxiv.org/abs/2401.14404v1",
        "pdf_url": "http://arxiv.org/pdf/2401.14404v1",
        "summary": "In this study, we examine the representation learning abilities of Denoising\nDiffusion Models (DDM) that were originally purposed for image generation. Our\nphilosophy is to deconstruct a DDM, gradually transforming it into a classical\nDenoising Autoencoder (DAE). This deconstructive procedure allows us to explore\nhow various components of modern DDMs influence self-supervised representation\nlearning. We observe that only a very few modern components are critical for\nlearning good representations, while many others are nonessential. Our study\nultimately arrives at an approach that is highly simplified and to a large\nextent resembles a classical DAE. We hope our study will rekindle interest in a\nfamily of classical methods within the realm of modern self-supervised\nlearning.",
        "updated": "2024-01-25 18:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.14404v1"
    },
    {
        "title": "Adaptive Mobile Manipulation for Articulated Objects In the Open World",
        "authors": "Haoyu XiongRussell MendoncaKenneth ShawDeepak Pathak",
        "links": "http://arxiv.org/abs/2401.14403v1",
        "entry_id": "http://arxiv.org/abs/2401.14403v1",
        "pdf_url": "http://arxiv.org/pdf/2401.14403v1",
        "summary": "Deploying robots in open-ended unstructured environments such as homes has\nbeen a long-standing research problem. However, robots are often studied only\nin closed-off lab settings, and prior mobile manipulation work is restricted to\npick-move-place, which is arguably just the tip of the iceberg in this area. In\nthis paper, we introduce Open-World Mobile Manipulation System, a full-stack\napproach to tackle realistic articulated object operation, e.g. real-world\ndoors, cabinets, drawers, and refrigerators in open-ended unstructured\nenvironments. The robot utilizes an adaptive learning framework to initially\nlearns from a small set of data through behavior cloning, followed by learning\nfrom online practice on novel objects that fall outside the training\ndistribution. We also develop a low-cost mobile manipulation hardware platform\ncapable of safe and autonomous online adaptation in unstructured environments\nwith a cost of around 20,000 USD. In our experiments we utilize 20 articulate\nobjects across 4 buildings in the CMU campus. With less than an hour of online\nlearning for each object, the system is able to increase success rate from 50%\nof BC pre-training to 95% using online adaptation. Video results at\nhttps://open-world-mobilemanip.github.io/",
        "updated": "2024-01-25 18:59:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.14403v1"
    },
    {
        "title": "pix2gestalt: Amodal Segmentation by Synthesizing Wholes",
        "authors": "Ege OzgurogluRuoshi LiuDídac SurísDian ChenAchal DavePavel TokmakovCarl Vondrick",
        "links": "http://arxiv.org/abs/2401.14398v1",
        "entry_id": "http://arxiv.org/abs/2401.14398v1",
        "pdf_url": "http://arxiv.org/pdf/2401.14398v1",
        "summary": "We introduce pix2gestalt, a framework for zero-shot amodal segmentation,\nwhich learns to estimate the shape and appearance of whole objects that are\nonly partially visible behind occlusions. By capitalizing on large-scale\ndiffusion models and transferring their representations to this task, we learn\na conditional diffusion model for reconstructing whole objects in challenging\nzero-shot cases, including examples that break natural and physical priors,\nsuch as art. As training data, we use a synthetically curated dataset\ncontaining occluded objects paired with their whole counterparts. Experiments\nshow that our approach outperforms supervised baselines on established\nbenchmarks. Our model can furthermore be used to significantly improve the\nperformance of existing object recognition and 3D reconstruction methods in the\npresence of occlusions.",
        "updated": "2024-01-25 18:57:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.14398v1"
    },
    {
        "title": "Smooth Ranking SVM via Cutting-Plane Method",
        "authors": "Erhan Can OzcanBerk GörgülüMustafa G. BaydoganIoannis Ch. Paschalidis",
        "links": "http://arxiv.org/abs/2401.14388v1",
        "entry_id": "http://arxiv.org/abs/2401.14388v1",
        "pdf_url": "http://arxiv.org/pdf/2401.14388v1",
        "summary": "The most popular classification algorithms are designed to maximize\nclassification accuracy during training. However, this strategy may fail in the\npresence of class imbalance since it is possible to train models with high\naccuracy by overfitting to the majority class. On the other hand, the Area\nUnder the Curve (AUC) is a widely used metric to compare classification\nperformance of different algorithms when there is a class imbalance, and\nvarious approaches focusing on the direct optimization of this metric during\ntraining have been proposed. Among them, SVM-based formulations are especially\npopular as this formulation allows incorporating different regularization\nstrategies easily. In this work, we develop a prototype learning approach that\nrelies on cutting-plane method, similar to Ranking SVM, to maximize AUC. Our\nalgorithm learns simpler models by iteratively introducing cutting planes, thus\noverfitting is prevented in an unconventional way. Furthermore, it penalizes\nthe changes in the weights at each iteration to avoid large jumps that might be\nobserved in the test performance, thus facilitating a smooth learning process.\nBased on the experiments conducted on 73 binary classification datasets, our\nmethod yields the best test AUC in 25 datasets among its relevant competitors.",
        "updated": "2024-01-25 18:47:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.14388v1"
    }
]