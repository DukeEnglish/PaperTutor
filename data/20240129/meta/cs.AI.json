[
    {
        "title": "Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities",
        "authors": "Yiyuan ZhangXiaohan DingKaixiong GongYixiao GeYing ShanXiangyu Yue",
        "links": "http://arxiv.org/abs/2401.14405v1",
        "entry_id": "http://arxiv.org/abs/2401.14405v1",
        "pdf_url": "http://arxiv.org/pdf/2401.14405v1",
        "summary": "We propose to improve transformers of a specific modality with irrelevant\ndata from other modalities, e.g., improve an ImageNet model with audio or point\ncloud datasets. We would like to highlight that the data samples of the target\nmodality are irrelevant to the other modalities, which distinguishes our method\nfrom other works utilizing paired (e.g., CLIP) or interleaved data of different\nmodalities. We propose a methodology named Multimodal Pathway - given a target\nmodality and a transformer designed for it, we use an auxiliary transformer\ntrained with data of another modality and construct pathways to connect\ncomponents of the two models so that data of the target modality can be\nprocessed by both models. In this way, we utilize the universal\nsequence-to-sequence modeling abilities of transformers obtained from two\nmodalities. As a concrete implementation, we use a modality-specific tokenizer\nand task-specific head as usual but utilize the transformer blocks of the\nauxiliary model via a proposed method named Cross-Modal Re-parameterization,\nwhich exploits the auxiliary weights without any inference costs. On the image,\npoint cloud, video, and audio recognition tasks, we observe significant and\nconsistent performance improvements with irrelevant data from other modalities.\nThe code and models are available at https://github.com/AILab-CVC/M2PT.",
        "updated": "2024-01-25 18:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.14405v1"
    },
    {
        "title": "Adaptive Mobile Manipulation for Articulated Objects In the Open World",
        "authors": "Haoyu XiongRussell MendoncaKenneth ShawDeepak Pathak",
        "links": "http://arxiv.org/abs/2401.14403v1",
        "entry_id": "http://arxiv.org/abs/2401.14403v1",
        "pdf_url": "http://arxiv.org/pdf/2401.14403v1",
        "summary": "Deploying robots in open-ended unstructured environments such as homes has\nbeen a long-standing research problem. However, robots are often studied only\nin closed-off lab settings, and prior mobile manipulation work is restricted to\npick-move-place, which is arguably just the tip of the iceberg in this area. In\nthis paper, we introduce Open-World Mobile Manipulation System, a full-stack\napproach to tackle realistic articulated object operation, e.g. real-world\ndoors, cabinets, drawers, and refrigerators in open-ended unstructured\nenvironments. The robot utilizes an adaptive learning framework to initially\nlearns from a small set of data through behavior cloning, followed by learning\nfrom online practice on novel objects that fall outside the training\ndistribution. We also develop a low-cost mobile manipulation hardware platform\ncapable of safe and autonomous online adaptation in unstructured environments\nwith a cost of around 20,000 USD. In our experiments we utilize 20 articulate\nobjects across 4 buildings in the CMU campus. With less than an hour of online\nlearning for each object, the system is able to increase success rate from 50%\nof BC pre-training to 95% using online adaptation. Video results at\nhttps://open-world-mobilemanip.github.io/",
        "updated": "2024-01-25 18:59:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.14403v1"
    },
    {
        "title": "TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation",
        "authors": "Gökçe UludoğanZeynep Yirmibeşoğlu BalalFurkan AkkurtMelikşah TürkerOnur GüngörSusan Üsküdarlı",
        "links": "http://arxiv.org/abs/2401.14373v1",
        "entry_id": "http://arxiv.org/abs/2401.14373v1",
        "pdf_url": "http://arxiv.org/pdf/2401.14373v1",
        "summary": "The recent advances in natural language processing have predominantly favored\nwell-resourced English-centric models, resulting in a significant gap with\nlow-resource languages. In this work, we introduce the language model TURNA,\nwhich is developed for the low-resource language Turkish and is capable of both\nnatural language understanding and generation tasks. TURNA is pretrained with\nan encoder-decoder architecture based on the unified framework UL2 with a\ndiverse corpus that we specifically curated for this purpose. We evaluated\nTURNA with three generation tasks and five understanding tasks for Turkish. The\nresults show that TURNA outperforms several multilingual models in both\nunderstanding and generation tasks, and competes with monolingual Turkish\nmodels in understanding tasks. TURNA is made available at\nhttps://huggingface.co/boun-tabi-LMG/TURNA .",
        "updated": "2024-01-25 18:24:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.14373v1"
    },
    {
        "title": "Efficient Optimisation of Physical Reservoir Computers using only a Delayed Input",
        "authors": "Enrico PiccoLina JaurigueKathy LüdgeSerge Massar",
        "links": "http://arxiv.org/abs/2401.14371v1",
        "entry_id": "http://arxiv.org/abs/2401.14371v1",
        "pdf_url": "http://arxiv.org/pdf/2401.14371v1",
        "summary": "We present an experimental validation of a recently proposed optimization\ntechnique for reservoir computing, using an optoelectronic setup. Reservoir\ncomputing is a robust framework for signal processing applications, and the\ndevelopment of efficient optimization approaches remains a key challenge. The\ntechnique we address leverages solely a delayed version of the input signal to\nidentify the optimal operational region of the reservoir, simplifying the\ntraditionally time-consuming task of hyperparameter tuning. We verify the\neffectiveness of this approach on different benchmark tasks and reservoir\noperating conditions.",
        "updated": "2024-01-25 18:20:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.14371v1"
    },
    {
        "title": "Genie: Achieving Human Parity in Content-Grounded Datasets Generation",
        "authors": "Asaf YehudaiBoaz CarmeliYosi MassOfir ArvivNathaniel MillsAssaf ToledoEyal ShnarchLeshem Choshen",
        "links": "http://arxiv.org/abs/2401.14367v1",
        "entry_id": "http://arxiv.org/abs/2401.14367v1",
        "pdf_url": "http://arxiv.org/pdf/2401.14367v1",
        "summary": "The lack of high-quality data for content-grounded generation tasks has been\nidentified as a major obstacle to advancing these tasks. To address this gap,\nwe propose Genie, a novel method for automatically generating high-quality\ncontent-grounded data. It consists of three stages: (a) Content Preparation,\n(b) Generation: creating task-specific examples from the content (e.g.,\nquestion-answer pairs or summaries). (c) Filtering mechanism aiming to ensure\nthe quality and faithfulness of the generated data. We showcase this\nmethodology by generating three large-scale synthetic data, making wishes, for\nLong-Form Question-Answering (LFQA), summarization, and information extraction.\nIn a human evaluation, our generated data was found to be natural and of high\nquality. Furthermore, we compare models trained on our data with models trained\non human-written data -- ELI5 and ASQA for LFQA and CNN-DailyMail for\nSummarization. We show that our models are on par with or outperforming models\ntrained on human-generated data and consistently outperforming them in\nfaithfulness. Finally, we applied our method to create LFQA data within the\nmedical domain and compared a model trained on it with models trained on other\ndomains.",
        "updated": "2024-01-25 18:14:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.14367v1"
    }
]