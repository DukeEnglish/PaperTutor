Differentiable Logic Programming for Distant
Supervision
AkihiroTakemuraa,* andKatsumiInouea
aNationalInstituteofInformatics,Tokyo,Japan
ORCID(AkihiroTakemura): https://orcid.org/0000-0003-4130-8311,ORCID(KatsumiInoue):
https://orcid.org/0000-0002-2717-9122
Abstract. We introduce a new method for integrating neural [2,28].However,thesemethodshaveissuessuchasnotbeingable
networks with logic programming in Neural-Symbolic AI (NeSy), todirectlyhandlelogicalconstraintsandnotbeingapplicabletoneu-
aimedatlearningwithdistantsupervision,inwhichdirectlabelsare ralnetworklearningasis.Thus,inthispaper,weproposeamethod
unavailable.Unlikepriormethods,ourapproachdoesnotdependon thatenableslearninginneuralnetworksforNeSytasksusinglogical
symbolicsolversforreasoningaboutmissinglabels.Instead,itevalu- programsthatincludeconstraints.
ateslogicalimplicationsandconstraintsinadifferentiablemannerby Distant supervision is a method of generating labeled data for
embeddingbothneuralnetworkoutputsandlogicprogramsintoma- learningusingrules,externaldata,orknowledgebasesandwaspro-
trices. This method facilitates more efficient learning under distant posed by Mintz et al. [21] as a method to train classifiers for rela-
supervision. We evaluated our approach against existing methods tionextractionbasedoninformationfromknowledgebases.InNeSy,
whilemaintainingaconstantvolumeoftrainingdata.Thefindings taskswherelabelinformationisprovidedthroughsymbolicreason-
indicatethatourmethodnotonlymatchesorexceedstheaccuracy ingarecommonlyused,withMNISTAddition[19]beingarepre-
ofothermethodsacrossvarioustasksbutalsospeedsupthelearn- sentativetask.Inthistask,pairsofhandwrittendigitsareinput,and
ingprocess.Theseresultshighlightthepotentialofourapproachto thegoalistolearntheclassificationofhandwrittendigitswiththe
enhancebothaccuracyandlearningefficiencyinNeSyapplications. sumofthedigitsprovidedasthelabel(e.g., + =5).Unlikethe
usualMNISTclassification,inMNISTAddition,thelabelsarenot
givenforeachimageindividually.Inthiscase,therelationshipbe-
1 Introduction
tweenthesumgivenasthelabelandthedigitscorrespondingtothe
Neural-Symbolic AI (NeSy) [15, 16] is a field of research aimed imagesisexpectedtobeprovidedthroughsymbolicreasoning.
atcombiningneuralnetworkswithsymbolicreasoning.Whiledeep In this paper, we propose a novel architecture for NeSy systems
learningiscapableoflearningcomplexrepresentationsfrominput- [19,33]thatintegratesdifferentiablelogicprogramming[2,28]and
outputpairs,itrequiresalargeamountoftrainingdataandstruggles neuralnetworks.Thispapermakesthefollowingcontributions:
withtasksthatrequirelogicalreasoning.Ontheotherhand,learning
with symbolic reasoning can be done with small amounts of data,
butitissensitivetonoiseandunabletohandlenon-symbolicdata.In 1. We propose a novel architecture that integrates neural networks
NeSy,itiscrucialtocombinetherolesofneuralnetworksandsym- with logic programming through a differentiable approach. This
bolicreasoninginawaythatleveragestheirrespectivestrengths. methodfacilitatesthedirectevaluationoflogicalimplicationsand
TherearevariousmethodsforimplementingNeSy,includingas- constraintsusingdifferentiableoperations,thusenablingeffective
sociatingthecontinuous-valuedparametersofneuralnetworks(NN) learning under distant supervision without relying on symbolic
with logical language and using the results of logical reasoning as solversforreasoningaboutmissinglabels.
thevalueofthelossfunction(e.g.,semanticloss[32]).Alsoknown 2. Wedemonstratethroughexperiments withaconstant volumeof
are methods that combine symbolic solvers with neural networks, trainingdatathatourproposedmethodnotonlymatchesbut,in
e.g.,DeepProbLog[19]usesProblogforprobabilisticlogicprogram- somecases,exceedstheaccuracyofexistingapproachesthatuti-
ming, and NeurASP [33] uses clingo for answer set programming lizesymbolicsolvers.Moreover,weachievedasignificantreduc-
(ASP).Thesemethodsthatinternallycallsolversoftenencapsulate tioninthetrainingtimeforneuralnetworks,highlightingsubstan-
computationallyexpensiveproblemssuchasweightedmodelcount- tialgainsincomputationalefficiency.
ingorenumeratingstablemodelsineachiterationduringlearning.
Alternativemethodshavebeenproposedthatembedinferencetra-
The structure of this paper is as follows. After the preliminaries
ditionally done by symbolic reasoning solvers into vector spaces
inSection2,Section3introducesthelogicprogrammingsemantics
andperformsymbolicreasoningusinglinearalgebra[24].Onesuch
invectorspaces.Section4presentsourproposedmethodforusing
methodembedslogicprogramsintovectorspacesanddesignsappro-
differentiablelogicprogrammingfordistantsupervision.Section5
priatelossfunctionsbasedonthesemanticsofnon-monotonicrea-
presentstheresultsofexperimentsandcomparisontothestateofthe
soningtocomputetheresultsofreasoninginadifferentiablemanner
artNeSymethods.Section6coverstherelatedworksintheliterature.
∗CorrespondingAuthor.Email:atakemura@nii.ac.jp Finally,Section7presentstheconclusion.
4202
guA
22
]IA.sc[
1v19521.8042:viXra2 Preliminaries vectorsaredenotedusingboldlowercaseletters(v).Theelementin
thei-throwandj-thcolumnofamatrixisdenotedbyM ,andthe
ij
AnormallogicprogramP isasetofrulesoftheform:
i-thelementofavectorisdenotedbyv .Thesliceofthei-throwofa
i
matrixisdenotedbyM ,andthesliceofthej-thcolumnisdenoted
A←A ∧···∧A ∧¬A ∧···∧¬A (1) i:
1 m m+1 n byM .Variablesaredenotedbyuppercaseletters,andconstants
:j
andpredicatesaredenotedbylowercaseletters;e.g.,insum(L),L
whereAandA (n ≥ m ≥ 0)areatoms.Inthispaper,theterms
i
isavariableandsum/1isapredicatewitharity1.
‘normallogicprogram’,‘logicprogram’,and‘program’areusedin-
terchangeably.Anatomisapredicatewithsomearity,e.g.,p(X,Y),
wherevariablesarerepresentedbyuppercasecharacters,andpred- 3.1 EmbeddingNormalLogicPrograms
icatesandconstantsarerepresentedbylowercasecharacters.Alit-
GivenagroundnormallogicprogramP,weintroducetwomatrices
eraliseitheranatomp,oritsnegation¬p.TheatomAin(1)isthe
that jointly represent the program. The program matrix represents
headand{A ,...,A }isthebodyofarule.ForeachruleR ofthe
1 n i
form(1),definehead(R )=A,body+(R )={A ,...,A }and thebodiesoftherulesintheprogram,andtheheadmatrixrepresents
i i 1 m
body−(R )={A ,...,A }. their disjunctions. This is an alternative formulation to the embed-
i m+1 n
dingapproachdescribedbySakamaetal.[24].
The Herbrand universe of a logic program P is the set of all
groundtermsinthelanguageofP,i.e.,termscomposedoffunction
Definition1(ProgramMatrix). LetP beagroundnormallogicpro-
symbolsandconstantsthatappearinP.TheHerbrandbaseB is
P gramwithRrulesandthesizeofitsHerbrandbasebe|B | = N.
P
thesetofatomsthatcanbeformedfromtherelationsoftheprogram Then P is represented by a binary matrix Q ∈ {0,1}R×2N such
and terms in the Herbrand universe. We assume that the Herbrand
thati-throwcorrespondstothebodyofthei-thruleR :Q =1if
i ij
baseB P ofaprogramtobelexicographicallyordered. a ∈ body+(R ),Q = 1ifa ∈ body−(R ),andQ = 0
j i i(N+j) j i ij
Arulewithanemptybodyisafact.AprogramP isdefiniteifno
otherwise.
ruleinP containsnegationasfailure.Aprogram,arule,oranatom
isgroundifitisvariablefree.AprogramP issemanticallyidentified Definition2(HeadMatrix). LetD∈{0,1}(N×R)betheheadma-
withitsgroundinstantiation,ground(P),bysubstitutingvariables trixassociatedwithP.ThentheelementD =1iftheheadofrule
ji
inP byelementsofitsHerbranduniverseineverypossibleway. R (1≤i≤R)isa (1≤j ≤N),and0otherwise.
i j
An interpretation I ⊆ B satisfies a rule R of the form (1) if
P i
body+(R i) ⊆ I andbody−(R i)∩I = ∅implyA ∈ I.Aninter- Example1. ConsiderthefollowingprogramP 1with3rules:
pretation that satisfies every rule in a program P is a model of the
(R )a←c∧¬b (R )a←a (R )b←¬a (3)
program.Amodelofaprogramissupportedifforeachatomp∈I, 1 2 3
thereexistsagroundrulesuchthatI satisfiesitsbody[1].Amodel
P isencodedintoapairofmatrices(Q,D):
1
M isminimalifthereisnomodelJ ofP suchthatJ ⊂M.Adefi-
niteprogramhasauniqueminimalmodel,whichistheleastmodel. a b c ¬a ¬b ¬c R R R
1 2 3
GivenanormallogicprogramP andaninterpretationI,thereduct    
R 0 0 1 0 1 0 a 1 1 0
PI,whichisagrounddefiniteprogram,isconstructedasfollows:a 1
groundruleA ← L ,...,L isinPI iffthereisagroundruleof Q= R 2 1 0 0 0 0 0 D= b  0 0 1  (4)
1 m R 0 0 0 1 0 0 c 0 0 0
theform(1)suchthatbody−(R )∩I =∅.IftheleastmodelofPI 3
i
is identical to I, then I is a stable model of P [14]. For a definite Qrepresentsthebodiesoftherules,whicharetheconjunctions
program,thestablemodelcoincideswiththeleastmodel.Astable oftheliteralsappearinginthebodies.Forexample,Q represents
1:
modelisalwayssupported,buttheconversedoesnotholdingeneral. thebodyofR ,(c∧¬b).Drepresentsthedisjunctionsofthebodies
1
SupportedmodelscanbecomputedasthemodelsofClark’scom- oftherulessharingthesamehead.Forexample,D representsthe
1:
pletion[5].Letheads(P,a)bethesetofrulesinP whoseheadis disjunctionbody(R )∨body(R )=(c∧¬b)∨a.Together,Qand
1 2
a.ThecompletionofP,denotedcomp(P),isthesetofclauses DrepresentthelogicprogramP.
(cid:95)
a↔ body(R ) (2)
i 3.2 EvaluatingEmbeddedNormalLogicPrograms
Ri∈heads(P,a)
Weconsidertheconjunctionappearinginthebodiesoftherulesas
foralla∈B .Amodelofcomp(P)isasupportedmodelofP [1].
P the negation of disjunctions of negated literals using De Morgan’s
LetI ⊆ B beaninterpretationofP.Therelation|=isdefined
P law,i.e.,L ∧···∧L = ¬(¬L ∨···∨¬L ).Thismeansthat
1 n 1 n
asfollows:foraruleR oftheform(1),IsatisfiesR ifhead(R )∩
i i i whenevaluatingthebodyofarule,insteadofcheckingwhetherall
I ̸= ∅ whenever body(R ) ⊆ I, and denoted as I |= R ; for a
i i literals hold (as in [28]), we can count the number of false literals
programP,I satisfiesP ifI |= R forallR ∈ P;foraformula
i i andcheckwhetherthecountexceeds1.Tothisend,weintroducea
F = F ∨···∨F (k ≥ 0),I |= F iffthereisaF (k ≥ i ≥ 1)
1 k i piecewiselinearfunctionmin (x) = min(x,1) = ReLU(1−x),
1
suchthatI |=F ,i.e.,theemptydisjunctionisfalse.Letcomp(R )
i p whichgives1forx ≥ 1.Thisfunctionisalmosteverywherediffer-
denotethecompletedrule(p↔body(R )∨···∨body(R ))for
p1 pj entiable(exceptatx=1),whichallowsgradient-basedoptimization
theatomp,thenp∈IiffI |=comp(R ).
p tobeappliedeffectively.
Toevaluatenormallogicprogramsinvectorspaces,weintroduce
3 Semantics thevectorizedcounterpartsofinterpretationandmodel.
In this section, we consider the semantics of ground normal logic Definition3(InterpretationVector). LetP beagroundnormallogic
programs in vector spaces. First, we introduce the necessary nota- program.AninterpretationI ⊆B P isrepresentedbyabinaryvector
tions. Matrices are denoted using bold uppercase letters (M), and v = (v 1,...,v N)⊺ ∈ ZN where each element v i(1 ≤ i ≤ N)represents the truth value of the proposition a i such that v i = 1 (cid:18) (cid:16) (cid:0) (cid:1)(cid:17)(cid:19)
Thevectorh = min D 1−min Q(1−w) servesas
if a ∈ I, otherwise v = 0. We assume propositional variables 1 1
i i
sharethecommonindexsuchthatv icorrespondstoa i,andwewrite theheadvector,whichisanindicatorvectorrepresentingtrueatoms
idx(v i)=a i. followingtheevaluationofrulebodiesinthelogicprogram.Thiswill
beusedlatertodefinethelossfunctioninSection4.1.4.Wenotethat
Definition 4 (Complementary Interpretation Vector). The comple- while this paper computes supported models as the models of the
mentaryinterpretationvectorw ∈ Z2N isabinaryvector,whichis completionformula,thiscanbeviewedasanalternativeformulation
aconcatenationoftheinterpretationvectorv anditscomplement: of the methods based on the immediate consequence operator T
P
w=[v;1 N −v]. invectorspaces[24,2,28].ConsidertheequationinProposition1:
givendist(v,hI) = 0,wehavev = hI,whichcorrespondstothe
Proposition1. (EmbeddingModelsofNormalLogicPrograms)Let conditionT (I)=I[20].
P
P = (Q,D) be an embedding of a ground normal logic program
P,dist(·,·)beadistancefunctioninametricspace,vbeaninter-
3.3 EmbeddingandEvaluatingConstraints
pretationvectorrepresentingI ⊆B ,andwbeitscomplementary
P
interpretationvector.Then,foraninterpretationvectorv,
Aconstraintisarulewithanemptyhead,e.g.,←a∧brepresentsa
I |=comp(P)iffdist(cid:18) v,min (cid:18) D(cid:16) 1−min (cid:0) Q(1−w)(cid:1)(cid:17)(cid:19)(cid:19) =0 c co on ns st tr ra ai in nt tsw ah re er re ua lea sn id nb am pru os gt rn ao mt ,b wot eh eb me btr eu de ts hi em mul it nan toeo au cs oly n. sS tri an ic ne t
1 1
matrixCinthesamemannerastheprogrammatrixP.Notethatwe
donotrequiretheheadmatrixbecauseconstraintshaveemptyheads.
Proof. (Sketch;fullproofintheAppendix.)Arowsliceofthepro-
grammatrixQ i:correspondstothebodyofaruleR i,sothematrix- Definition 5 (Constraint Matrix). Let C = {C 1,...,C k} = {←
vectorproductsQ i:wandQ i:(1−w)computesthenumberoftrue B 1,...,← B k} be the set of constraints in a program P with
and false literals in I, respectively. The conjunctions can be com- |B | = N. Then the matrix corresponding to the constraints is
P
puted as the negation of disjunctions of negated literals using De C ∈ {0,1}(k×2N) such that i-th row corresponds to the body of
Mo Lr eg ta {n’ Rs ala iw }, =i.e {., a1 i− ←m Bin j1 ,( .Q ..i ,:( a1 i− ←w B)) t}.
bethesetofrulesthat
t ah je ∈i-t bh odco yn −s (tr Ca ii )nt aC ndi: CC ijij == 01 oi tf ha ej rw∈ isb eo .dy+(C i),C i(N+j) =1if
sharethesameheadatoma ,whereB denotetherulebodies,and
i j
B ∨···∨B bethedisjunctionoftherulebodies.Byconstruction To evaluate the constraints, we check whether the bodies of the
j t
of the head matrix D, D (1−min (Q(1−w))) computes the constraintrulesareinI:givenaconstraintR ,ifbody(R )⊆Ithen
i: 1 i i
number of true rule bodies that share the same head. Thus, h = theconstraintisviolated;otherwiseitissatisfied.
(cid:18) (cid:16) (cid:0) (cid:1)(cid:17)(cid:19) i
min D 1−min Q(1−w) = 1ifthereisatleastone
1 i: 1 Proposition2. (EvaluatingConstraints)LetCbeanembeddingof
rule body that is true in I and in the disjunction B ∨ ··· ∨ B , constraintsC,dist(·,·)beadistancefunctioninametricspace,v
j t
and0otherwise.Thencomputingh
i
correspondstotheevaluation beaninterpretationvectorrepresentingI ⊆B P,andwbeitscom-
ofI |=comp(R ).Thiscanbegeneralizedtotheentirematrix. plementaryinterpretationvector.Then,foraninterpretationvector
LethI =
minai(cid:18)
D(cid:16) 1−min (cid:0) Q(1−w)(cid:1)(cid:17)(cid:19) ,thenthesecond v,itholdsthatI ̸|=Ciffdist(1,min 1(C(1−w)))=0.
1 1
Proof. Proved similarly to Proposition 1. Let cI = min (C(1−
partoftheiffrelationissimplifiedtodist(v,hI))=0. 1
w)).Considerthei-thconstraintC andthecorrespondingrowslice
i
• IfI |=comp(P),thendist(v,hI)=0. C i:.Theexistenceofatleastonefalseliteralinthebodyiscomputed
bycI =min (C (1−w)),wherecI =1ifthereisafalseliteral
S thu ap tp io sse truI e|=
in
Ico ,m sop( hR Iai =), t 1h .en Ott hh ee rr we ii ss e,at wl he ea nst won ee hr au vl ee Ibod ̸|=y andci I
i
= 0o1 theri w: ise,i.e.,whencI
i
=i 0,thebodyissatisfiedand
i
comp(R ),hI =0.Therefore,itholdsthathI =v ,andsince theconstraintisviolated.
theindexa ii isari bitrary,wehavev=hI,i.e.,dii st(v,hi I)=0. SupposeI ̸|=C i,thenthereisatleastonefalseliteralinthebody
i
• Ifdist(v,hI)=0,thenI |=comp(P). ofC i,somin 1(C i:(1−w))) = 1.RepeatthisforallC i ∈ C,we
Considerdist(v,hI) = 0.ForhI = 1,thereisatleastonerule obtaina1-vector,whichmeansthereisatleastonefalseliteralinthe
i
body that is true in I, and for hI = 0, there is no rule body bodiesofallconstraints.Bydefinition,dist(1,1)=0.Theconverse
i
that is true in I. Since we have v = hI, for v = hI = 1, canbeprovedsimilarly.
i i i i
(cid:87)
a ↔ body(R ) is satisfied and denote I |=
i Rj∈heads(P,ai) j
comp(R ai), and for v i = hI i = 0 denote I ̸|= comp(R ai). Example3. Considertheconstraint←a∧b.Thenwehave:
Sincetheindexiisarbitrary,weconcludeI |=comp(P).
a b c ¬a ¬b ¬c
(cid:0) (cid:1)
C=C 1 1 0 0 0 0 (5)
1
Example2. (Example1contd.)ConsidertheprogramP andcor- Weexaminetwoscenarios:inthefirst,theconstraintisviolated,and
1
responding matrices Q and D from Example 2. This program has in the second, it is not. Let cI = min 1(C(1−w)) and dist(·,·)
2 supported models {{a},{b}}. Let v{a} = (1 0 0)⊤ represent denotetheEuclideandistance,then,forthefollowingcases,
the interpretation {a}, and w be its complementary interpretation
• v{a,b} =(110):weobtainc{a,b} =(0),anddist(1,c{a,b})=
vector.WehavehI = (1 0 0)⊤ = v,andtaketheEuclideandis-
(cid:113) 1,soweconcludethattheconstraintisviolated.
tance:dist(v,hI)= (cid:80)3 i=1(v i−hI i)=0.Therefore,according • v{a} =(100):weobtainc{a} =(1),anddist(1,c{a,b})=0,
toProposition1,I |=comp(P ). soweconcludeI ̸|=C.
1For later use in the loss function (Section 4.1.4), we define the 4.1.2 Constraints
constraintviolationvectorc′asc′ =1−c=1−min (C(1−w)).
1
Intuitively,thismodificationturnscintoanindicatorvector,where
eachelementc′ =1meansthatthei-thconstraintisviolated. InthecaseofMNISTAddition,constraintscanberepresentedwith
i
smallernumberofrulescomparedtotheimplicationrules,e.g.,
4 LearningwithDifferentiableLogicProgram
←label(i ,i ,0)∧¬obs(i ,0,i ,0).
1 2 1 2
←label(i ,i ,1)∧¬obs(i ,0,i ,1)∧¬obs(i ,1,i ,0).
Inthissection,weshowhowtheaforementioneddifferentiablelogic 1 2 1 2 1 2
program semantics can be used to train neural networks. Although ...
ourmethodsupportsbothimplicationandconstraintrules,itisnot ←label(i ,i ,18)∧¬obs(i ,9,i ,9). (7)
1 2 1 2
alwaysnecessarytousebothofthemforlearning,asweshallshow
later in the experimental section. Specifically, for the NeSy tasks
Intuitively,onecanreadthefirstruleas“whenthelabelis0,both
westudied,usingexclusivelyeitheroneofimplicationorconstraint
ofthedigitsmustbe0”.Thisessentiallyamountstoenumeratingall
rulesisenoughtoachievecompetitiveaccuracy.Ontheotherhand,
possiblecombinationsofdigitsthatsumtothelabelandaddingthem
in NeurASP [33] for example, the observation atoms are typically
asnegativeliteralstotherulebodies.Preparingconstraintsforeach
givenasintegrityconstraintsinASPrulestocomputestablemodels,
labelresultsinaconstraintmatrixC∈{0,1}(19×238).
andimplicationrulesaredefinedsimilarlytoours.Consequently,we
included a combination of both implication rules and constrains in
ourexperimentstoprovideathoroughevaluation.
4.1.3 HandlingNeuralNetworkOutputs
4.1 Example:MNISTAddition
Here,theoutputsoftheneuralnetworkcombinedwithfactsevident
TheMNISTdigitadditionproblem[19]isasimpledistantsupervi-
fromtheproblemandlabelsaretreatedasacontinuousinterpreta-
sion task that is commonly used in neural-symbolic literature. The
tion.Factsevidentfromtheproblemreferto,forexample,thethree
inputconsistsoftwoimages,andtheoutputistheirsum(e.g., ,
digits(d ,d ,d )intheApply2x2taskorthegivendigitnumberin
1 2 3
,9).Thegoalistolearndigitclassificationfromthesumofdig-
theMembertask.ThedetailsoftheApply2x2andMembertaskswill
its,ratherthanfromimageswithindividuallylabeleddigits,asinthe
beexplainedlaterintheexperimentsection.Thelabelinformationis
usualMNISTdigitclassification.Forbrevity,weshallfocusonthe
alsoincorporatedintothiscontinuous-valuedinterpretationvector.
singledigitadditionsinthissection.
Wefirstintroduceneuralpredicates[19],whichactasaninterface
Definition 6 (Continuous-valued Interpretation Vector). Let x ∈
between the neural and logic programming parts. More concretely,
[0,1]N betheoutputpassedthroughthelastlayeroftheneuralnet-
a neural predicate is a predicate which can take references to ten-
work. Let f ∈ {0,1}N represent facts from the problem that are
sorizedobjectsasarguments.Forexample,inMNSITAddition,we
notdependentonNN’soutput,andlb ∈ {0,1}N representthela-
definetwoneuralpredicates,obs/4andlabel/3.obs(i ,D ,i ,D )
1 1 2 2 bel information available from the instance. If the number of ele-
representsasituationwhereimagesi andi wereclassifiedasdigits
1 2 ments in x,f or lb is less than N, pad appropriately with zeros.
D andD rangingfrom0to9,respectively.label(i ,i ,S)repre-
1 2 1 2 The continuous-valued interpretation vector z is computed as fol-
sentsthetwoimagesandtheirsumSrangingfrom0to18.Thus,we
lows:z=x+f +lb.
obtain100obsand19labelneuralatoms.
InMNISTAddition,factsarenotavailablefromtheproblemset-
4.1.1 Implicationrules tings,soweonlyfocusontheneuralnetworkoutputsandlabels.In
thistask,theinputstothe(convolutional)neuralnetworkaretwoim-
Ingeneral,weexpectthelabelatomstoappearintheheadsofthe ages(i ,i ).AfterpassingthroughtheSoftmaxactivation,weobtain
1 2
implicationrulessothatwecancomparetheoutputofthelogicpro- two output vectors x ,x ∈ [0,1]10 as (probabilistic) outputs. To
1 2
gramming component with the label using a loss function such as mapthesevectorsto100obsneuralatoms,wecomputetheirCarte-
binarycrossentropy.ForasingledigitMNISTAddition,thelabelis sianproductandobtainx ∈ [0,1]100:x = x ×x .Dependingon
1 2
theintegervaluesbetween0and18representedbythelabel/3pred- theproblem,thedimensionofthecontinuous-valuedinterpretation
icate.Asfortheindividualrules,weenumeratethepossiblecombi- vector may be different for implication and constraints. In MNIST
nationsofdigitsthatsumtothegivenlabel,e.g., Addition,forimplicationrulesitisnotnecessarytohavelabelinthe
interpretationvector,astheyaretheheadsoftherules.Ontheother
label(i 1,i 2,0)←obs(i 1,0,i 2,0). hand,itisnecessarytoincludelabelintheinterpretationvectorfor
label(i ,i ,1)←obs(i ,0,i ,1). evaluatingconstraints,astheyarepresentintherulebodies.Itisim-
1 2 1 2
perative that the indexing remain consistent across all vectors and
label(i ,i ,1)←obs(i ,1,i ,0).
1 2 1 2 matrices;otherwiseweriskneuralnetworkoutputsbeingmappedto
... incorrectneuralatoms.
label(i ,i ,18)←obs(i ,9,i ,9). (6) ThelearningpipelineisshowninFigure1.Firstly,theinputim-
1 2 1 2
agesareclassifiedusingtheCNN,andweassociateitsprobabilistic
Inthisway,100ruleswithlabel/3intheheadscanbeinstantiated, output with neural atoms obs/4. Then, using the neural atoms and
whichresultsintheembeddedprogramP impl. =(Q impl.,D impl.) label information, we obtain the logical loss which can be used to
whereQ impl. ∈{0,1}(100×200)andD impl. ∈{0,1}(19×100). traintheCNNwithgradientbackpropagation.5 Experiments
Implication Embed Pimpl.
label(i1,i2,0) :- obs(i1,0,i2,0).
Implication matrix
label(i1,i2,1) :- obs(i1,0,i2,1). ... 5.1 TaskDescription
Inputs
0 0 obs(i1,0,i2,0)
+ CNN 1 ...1
...
o ...bs(i1,0,i2,1) L =o Lg ii mca pl lL
.
+o s Ls
cons.
WestudiedthelearningperformanceonthefollowingNeSytasks.
9 9 obs(i1,9,i2,9)
= Prob. Neural Gradient
10Label Output atoms Backpropagation MNISTAddition[19]
C :- o lan bs elt (r i1a ,i in 2,t 0s ), not obs(i1,0,i2,0). Embed C Constraint matrix TheinputconsistsoftwoMNISTimagesofdigits(i 1,i 2),andthe
:- label(i1,i2,1), not obs(i1,0,i2,1), not obs(i1,1,i2,0). outputistheirsum(e.g., , ,9).Thegoalistolearnimageclas-
Figure1. ThelearningpipelineforMNISTAddition sification from the sum of digits, rather than from images with in-
4.1.4 LossFunction
dividuallylabeleddigits,asintheusualMNISTclassification.This
paperdealswithtwotypes:single-digitadditions(twoimages),and
two-digitadditions(fourimages).
Usingtheembeddedprogram,continuous-valuedinterpretationvec-
torandlabelinformation,thelossfunctionisdefinedasfollows:
ADD2x2[13]
(cid:18) (cid:16) (cid:0) (cid:1)(cid:17)(cid:19)
h=min D 1−min Q(1−w ) (8)
1 1 z TheinputisfourMNISTimagesofdigits(i ,i ,i ,i )arranged
11 12 21 22
c′ =1−min (C(1−w )) (9) ina2x2grid,andtheoutputisfoursums(s 1,s 2,s 3,s 4)calculated
1 z
fromeachrowandcolumnofthegrid(e.g., , , , ,1,8,3,6).
L=L +L =BCE(h,lb)+BCE(c′,0) (10)
impl. cons. ThegoalistolearntheclassificationproblemofMNISTimagesfrom
thefoursumsprovidedaslabels.
whereBCE standsforbinarycross-entropy,lbcorrespondstothe
labelvector,and0isazerovectorwiththesamedimensionasc′.
Notethatw hereisthecomplementaryinterpretationvectorbased APPLY2x2[13]
z
onthecontinuous-valuedinterpretationvectorzwhichcontainsneu-
ralatoms,factsandlabels:w
z
=[z;1
N
−z].Whenthecontinuous Theinputconsistsofthreenumbers(d 1,d 2,d 3)andfourhandwrit-
valuedinterpretationvectorisabinaryone,hcorrespondstothein- ten operator images (o 1,o 2,o 3,o 4) arranged in a 2x2 grid, with
terpretationI.IfallatomsthataresupposedtobetrueinI aretrue, theoutputbeingtheresults(r 1,r 2,r 3,r 4)ofoperationsperformed
thentheBCElosswillbe0.c′correspondstoanindicatorvectorfor along each row and column of the grid. The operators are one of
theviolatedconstraints.Thus,c′shouldbeall0whenallconstraints {+,−,×}. For example, the result for the first row is calculated
aresatisfied.CombiningtheaforementionedBCE’s,thelossfunction as r 1 = (d 1 op 11 d 2) op 12 d 3 (e.g., 1,2,4, , , , ,
willbe0iffallimpliedneuralatomsaretrue,andallconstraintsare 6,−4,−2,12).Thegoalistolearntheclassificationofhandwritten
satisfied. operatorsfromthefourresultsandthreenumbersgivenaslabels.
The difference between programs (6) and (7) lies in their struc-
ture: implication rules may contain label neural atoms in the head,
whereasconstraintsalwayscontainlabelneuralatomsinthebody. MEMBER(n)[30]
Implication rules present a more straightforward approach for rep-
resentingpartialinformationintheformoflogicalrules,especially Forn=3,theinputconsistsofthreeimages(i 1,i 2,i 3)andonenum-
in scenarios where employing intermediate predicates is necessary ber (d 1), with the output being a boolean indicating whether d 1 is
orenumeratingconstraintsistime-consuming.Inthecontextofthe included in the three images (e.g., , , , 4,0). For n=5, the
NeSytasksexaminedinthisstudy,weobservedaconsistentpattern inputincludesfiveimages(i 1,...,i 5).Thegoalistolearntheclas-
where implication rules are typically comprised of straightforward sificationproblemofMNISTimagesfromthenumbersprovidedas
forward inference rules, and constraints are formed from a fewer labels.Thispaperdealswithtwotypes:n=3andn=5.
number of rules. Each of these constraints contains a label neural
atomalongsideaseriesofnegatedobservationatomsintheirbody.
Basedonthedefinitionofthecombinedlossfunction,itisclear 5.2 ImplementationandExperimentalSetup
that only one is necessary for accomplishing the MNIST Addition
task.ThefirstBCEisessentiallythesameastheonefora19-label The methods introduced in the previous section were implemented
multiclass classification task (labels spanning from 0 to 18), while in PyTorch. The convolutional neural network (CNN) used in the
the second BCE corresponds to a multiclass classification with an experimentsisthesameasthosein[19]and[33].Theexperimental
all-0label. environmentisanAMDRyzen7950X(16c/32t),128GBRAM,and
Theevaluationofimplicationrulesensuresthatifthecorrectla- anNVIDIAA4000(16GB),withsettingstoutilizetheGPUasmuch
bel’s corresponding atom is derived as the head, L becomes aspossible.Thenumberoftrainingdatawas30,000and15,000for
impl.
0.Similarly,ifallconstraintsaresatisfied,thenL becomes0. Addition 1 and 2, respectively, and 10,000 for other tasks. Unless
cons.
Sinceboththeevaluationofimplicationrulesandconstraintsarede- otherwise noted, the number of epochs and batch size for all tasks
finedina(almost-everywhere)differentiablemanner,itispossibleto weresetto1,andAdamwasusedwithalearningrateof0.001.Each
traintheneuralnetworkusingthislossfunctionthroughbackpropa- experimentconsistedof5repeatedtrials,andtheaverageisreported.
gation. Thetimeoutwassetto30minutespertrial.5.3 Results Table3. Learningtimesofeachmethod.Thenumbersinparenthesesindi-
caTteimtime(esoeuct)s(30min). Comparisons Ours
Table1showsthedimensionsoftheprogrammatrixP,headmatrix
DPL DSL NASP I C I+C
D,andconstraintmatrixC.Intermsofthenumberofrulesinre- Addition1 470.7 20.8 84.7 31.4 31.4 35.9
spectiveprograms,theconstraintmatrixisusuallysmallerthanthe Addition2 1120 33.6 283.5 83.5 19.9 87.4
implication matrix, because each constraint rule (row) is often an Add2x2 T/O(5) 35.8 131.1 16.9 16.2 18.0
Apply2x2 323.3 127.7 25.5 154.4 228.5 359.5
enumerationofconditionsforthetargetatom,whereasintheimpli-
Member3 1782(3) 398.4 191.9 10.3 10.3 11.9
cationmatrix,multiplerules(rows)cansharethesamehead.Therate Member5 T/O(5) T/O(5) T/O(5) 11.6 11.7 13.4
ofgrowthoftheprogrammatrixishighlytaskdependent;forexam-
ple,addingtwodigitstotheMNISTAdditiontaskresultinadding
Addition1 Addition2
10,000elementstothematrix,whereasaddingtwodigitstotheset
100
intheMembertaskresultsinaddingafewthousandelements.
80
TPabl(eim1p.licDatiimone)nsionsofprDogr(ahmeamd)atricesC(constraint) 60 I NASP
Addition1 100×200 19×100 19×238 40 C DPL
Addition2 10000×20000 199×10000 199×20398 20
I+C DSL
Add2x2 400×800 76×400 76×952
Apply2x2 11979×2680 10597×11979 10597×23874
0 20000 0 10000
Member3 40×60 20×40 40×100
Member5 60×100 20×60 60×140 Add2x2 Apply2x2
100
Theaccuracyandtrainingtimearereportedintables2and3,re- 80
spectively.Inthetables,Iindicatestrainingusingonlyimplication 60
rules,Cindicatestrainingusingonlyconstraints,andI+Cindicates 40
trainingusingbothimplicationrulesandconstraints.DPL,DSLand 20
NASPareabbreviationsforDeepProbLog[19],DeepStochLog[31]
andNeurASP[33],respectively. 0 5000 10000 0 5000 10000
Table 2 shows the average accuracy of MNIST digit classifica-
Member3 Member5
tion and math operator classification. It can be seen that while the
100
proposedmethodachievedcomparableaccuracytothecomparison
80
methods in MNIST Addition and Add2x2, it significantly outper-
60
formedthecomparisonmethodsinApply2x2.However,inMember
40
3,theaccuracywaslower.Comparingtheuseofimplicationrules,
20
constraints, or both, there was no significant difference except for
Member(n),whereusingeitherimplicationrulesorconstraintsalone
0 5000 10000 0 5000 10000
tended to result in higher accuracy than using both. In particular,
iteration iteration
forMember5tasks,combiningimplicationrulesandconstraintsled
Figure 2 sFhoigwusret2h.e tTeessttaaccccuurraaccyy(%d)udriunrgingtrtarianininingg..1 We see that
tosignificantlyworseperformancecomparedtoindividualapplica-
the addition tasks (MNIST Addition and Add2x2) can be handled
tions.Thissuggeststhatwemightneedtomodifythecombination
equallywellbyourmethod,NeurASPandDeepStochLog.Thedif-
lossorconsiderintroducingaschedulerforimprovingtrainingper-
ference is more pronounced for Apply2x2 and Member3, and in
formance.
Apply2x2, we observe that DeepStochLog plateaus quickly while
Table 2. Accuracy on digit and operator classification. The numbers in NeurASP fluctuates around 80%. In Member 3, NeurASP leads in
parAenctchuersaecsyi(n%di)catetimeoutCso(3m0pmariinso).ns Ours termsofaccuracyupto5,000iterations,othermethodscatchupafter
DPL DSL NASP I C I+C 9,000iterations.Finally,itisinterestingthattheimplicationruleonly
Addition1 97.8 95.8 97.7 97.7 97.5 97.4
methodperformswellinMember5comparedagainstconstraintonly
Addition2 97.7 97.8 97.8 97.5 97.9 97.8
orthecombinationofboth.Thissuggeststhattheremightbetasks
Add2x2 T/O(5) 98.0 97.5 97.7 97.6 97.9
Apply2x2 87.8 87.8 80.9 99.5 99.4 99.4 that can be learned better by certain types or combinations of pro-
Member3 92.3(3) 92.9 91.7 87.8 87.0 84.6 grams,althoughitisdifficulttoknowbeforehandwhichonewould
Member5 T/O(5) T/O(5) T/O(5) 86.3 86.4 69.5 performthebest.
Table3showstheaveragetrainingtimesofeachmethod.Except
6 RelatedWork
forApply2x2,itisevidentthattheproposedmethodcanlearnfaster
thanexistingmethods.EspeciallyinMember5,wherecomparison
Xuetal.[32]introducedthesemanticlossthatleveragessentential
methodstimedout,theproposedmethodprocessed10,000training
decision diagrams for efficient loss calculation, enabling effective
instancesinabout13seconds,showingasignificantspeeddifference.
learning in both supervised and semi-supervised settings. LTN [4]
ThelongtrainingtimeforApply2x2canbeattributedtotherather
and LNN [22] embed first-order logic formulas using fuzzy logic
naive implementation used in this experiment. In our implementa-
withinneuralnetworkarchitectures.Ourapproachissimilartothe
tion,Apply2x2requiredamorefine-grainedcontroloverindexing,
semanticloss,inthesensethatweevaluatetheneuraloutputusinga
necessitatingtheuseoflessefficient’for-loops’toavoidneuralnet-
work outputs being mapped to incorrect neural atoms. In contrast,
1 Thisisfromadifferentsetofexperiments,assuch,theresultspresented
inothertasks,themoreefficientvectorizedoperationswereusedto inthisfiguredonotnecessarilymatchthoseinTable2andTable3.The
computetheCartesianproduct. timeoutwassetto2hourspertrial.
)%(
ycarucca
po/tigiddifferentiablelossfunctiontotraintheneuralnetworkbybackprop- therewritingofsameheadrules,forexample.Themethodpresented
agation.Ontheotherhand,ourapproachdoesnotrequireweighted inthispaperdoesnotminimizethelossfunctiondirectly,andtheloss
modelcounting,nordirectembeddingoflogicaloperations,andthe isreducedbyupdatingtheparametersoftheneuralnetworktopre-
logic programming semantics is kept separate from the perception dict the correct intermediate labels in a distant supervision setting.
neuralnetwork. To the best of our knowledge, no other implementation exists that
Paralleltodirectintegrationstrategies,significantworkhasbeen utilizesthedifferentiablelogicprogrammingsemanticsfortraining
conducedoncouplingneuraloutputswithsymbolicsolvers.Forin- neuralnetworksinneural-symbolicsettings.
stance,DeepProbLog[19],DeepStochLog[31],NeurASP[33],and
NeuroLog [30] facilitate inference by integrating the neural out-
7 Conclusion
puts as probabilities or weighted models within a symbolic solver.
While these approaches can utilize logical constraints and back- Weproposedamethodtoassistthelearningofneuralnetworkswith
ground knowledge represented by logical programs, the computa- logic programs and verified its effectiveness in NeSy tasks. This
tional cost of symbolic reasoning can become a bottleneck during method is based on a differentiable logic programming semantics,
learning.Distinctfromtheaforementionedcouplingapproaches,the where continuous-valued interpretation vectors contain outputs of
focusshiftsmoretowardsreasoningratherthansimultaneouslearn- neuralnetworks,andevaluationofimplicationrulesandconstraints
inginneural-symbolicsystemsproposedbyEiteretal.[8]andEm- are incorporated into a loss function, enabling the learning of neu-
bed2Sym[3].InthepipelineproposedbyEiteretal.[8],theneural ralnetworksunderthedistantsupervisionsettings.Theexperimental
networkistrainedseparately,andpredictionsthatpassapredefined resultsshowedthatitispossibletoachievecomparableaccuracyto
confidencethresholdarethentranslatedintologicprogramsforrea- thosebasedonsymbolicsolvers,andwithanexception,theproposed
soningwithASP. methodcompletedtheneuralnetworktrainingmuchfasterthanthe
Various neural-symbolic approaches have been developed for existingmethods.
symbolicrulelearning,withvaryingdegreeofintegrationbetween Thefindingsofthisstudydemonstratetheeffectivenessoftheap-
logicalreasoningandneuralcomputation.NeuralTheoremProvers proachbasedonthedifferentiablelogicprogrammingsemanticsfor
[23] and ∂ILP [9] allow end-to-end differentiable learning of ex- enabling high-accuracy and fast learning in NeSy. Future work in-
planatory rules from data. Similarly, frameworks like NeuroLog cludesapplyingtheproposedmethodtomorecomplexNeSytasks.
[30], NSIL [6], and the Apperception Engine [10] integrate sym- Additionally,amoredetailedanalysisofthebalancebetweencon-
bolic solvers to enhance the reasoning processes, utilizing the out- straintsandimplicationrulesisnecessary.Inthetasksaddressedin
putfromneuralnetworksrepresentedaslogicalconstructs.Addition- thisstudy,therewaslittledifferencebetweenusingconstraintsand
ally,αILP[27]andDFORL[12]extendthecapabilitiesofinductive implicationrules,butthismightvarybytask.Anotherlimitationis
logic programming (ILP) by making the learning process differen- that the semantics as presented currently is only valid for crisp 0-
tiable. In particular, αILP is designed for visual scene understand- 1 interpretations, so other continuous-valued interpretations do not
ing,whileDFORLfocusesonrelationaldata,demonstratingthever- necessarilyhavenaturalmeaningsassociatedwiththem.Tothisend,
satilityofdifferentiableILPinhandlingdiversedatatypes.InNeu- itmightbeinterestingtoseekconnectionsbetweenthedifferentiable
ralLogicMachines[7],thearchitectureoftheneuralnetworkitself logicprogrammingsemanticsandfuzzylogic.
is designed to mimic logical reasoning processes, thereby learning
toapproximatesymbolicrules.Incontrasttothesemethods,which
Acknowledgements
predominantly focus on enhancing the logic programming aspects
withinneuralframeworks,ourapproachfocusesmoreonenhancing
This work has been supported by JST CREST JPMJCR22D3 and
thetrainingofneuralnetworksthemselves,andweleavetheinduc-
JSPSKAKENHIJP21H04905.
tivelearningoflogicprogramsforfuturework.
Recent advancements have explored various methods for repre-
sentinglogicprogrammingsemanticsinvectorspaces,eachwithdis- References
tinct embedding strategies and computational techniques. Notably,
[1] K.R.Apt,H.A.Blair,andA.Walker. Towardsatheoryofdeclar-
Sakamaetal.[24]proposedamethodtoembedlogicprogramsinto ativeknowledge. InFoundationsofDeductiveDatabasesandLogic
vectorspaces,andcomputelogicprogrammingsemanticsusinglin- Programming,pages89–148.Elsevier,1988.
ear algebra. More specifically, their method allows for the compu- [2] Y.Aspis,K.Broda,A.Russo,andJ.Lobo. StableandSupportedSe-
manticsinContinuousVectorSpaces. InProceedingsofthe18thIn-
tation of stable models as fixpoints through repeated tensor mul-
ternationalConferenceonPrinciplesofKnowledgeRepresentationand
tiplications. However, a significant limitation from the standpoint Reasoning,pages59–68,2020.doi:10.24963/kr.2020/7.
of neural-symbolic integration is the non-differentiability of their [3] Y.Aspis,K.Broda,J.Lobo,andA.Russo. Embed2Sym-Scalable
Neuro-SymbolicReasoningviaClusteredEmbeddings.InProceedings
method,whichcomplicatesdirectintegrationwithneuralnetworks.
ofthe19thInternationalConferenceonPrinciplesofKnowledgeRep-
Othermatrix-basedapproachesbySatoetal.[25]andTakemuraand resentationandReasoning,2022.
Inoue[28]usebinaryprogrammatricestorepresentlogicprograms. [4] S.Badreddine,A.d’AvilaGarcez,L.Serafini,andM.Spranger. Logic
While Sato et al. [25]’s method is non-differentiable, the one pro- TensorNetworks.ArtificialIntelligence,303:103649,Feb.2022.ISSN
0004-3702.doi:10.1016/j.artint.2021.103649.
posedbyTakemuraandInoue[28]isbasedonadifferentiableloss
[5] K.L.Clark.NegationasFailure.InLogicandDataBases,pages293–
function,wheretheinterpretationvectoristreatedastheinputtothis 322.SpringerUS,Boston,MA,1978. ISBN978-1-4684-3384-5. doi:
lossfunctionandthelossitselfisdirectlyminimizedbyupdatingthe 10.1007/978-1-4684-3384-5_11.
[6] D.Cunnington,M.Law,J.Lobo,andA.Russo.Neuro-SymbolicLearn-
interpretation vector. Similarly, Aspis et al. [2] proposed a method
ingofAnswerSetProgramsfromRawData. InProceedingsofthe
basedonaroot-findingalgorithm,whichpresentsyetanothercom- Thirty-SecondInternationalJointConferenceonArtificialIntelligence,
putationalstrategyinthisdomain.Incontrasttotheseexistingmeth- pages3586–3596.ijcai.org,2023.doi:10.24963/IJCAI.2023/399.
ods,ourapproachcomputessupportedmodelsinvectorspaceswith- [7] H.Dong,J.Mao,T.Lin,C.Wang,L.Li,andD.Zhou. NeuralLogic
Machines. InInternationalConferenceonLearningRepresentations
outimposingrestrictiveconditionsonprogramstructuresthatrequire
(ICLR),page22,2019.[8] T.Eiter,N.Higuera,J.Oetsch,andM.Pritz. ANeuro-SymbolicASP NeuralStochasticLogicProgramming. InThirty-SixthAAAIConfer-
PipelineforVisualQuestionAnswering.TheoryPract.Log.Program., enceonArtificialIntelligence,AAAI2022,pages10090–10100.AAAI
22(5):739–754,2022.doi:10.1017/S1471068422000229. Press,2022.doi:10.1609/AAAI.V36I9.21248.
[9] R.EvansandE.Grefenstette. LearningExplanatoryRulesfromNoisy [32] J.Xu,Z.Zhang,T.Friedman,Y.Liang,andG.V.denBroeck. Ase-
Data. JournalofArtificialIntelligenceResearch,61:1–64,Jan.2018. manticlossfunctionfordeeplearningwithsymbolicknowledge. In
ISSN1076-9757.doi:10.1613/jair.5714. Proceedingsofthe35thInternationalConferenceonMachineLearn-
[10] R.Evans,M.Bosnjak,L.Buesing,K.Ellis,D.P.Reichert,P.Kohli,and ing,ICML2018,volume80,pages5498–5507,2018.
M.J.Sergot. Makingsenseofrawinput. ArtificialIntelligence,299: [33] Z.Yang,A.Ishay,andJ.Lee. NeurASP:EmbracingNeuralNetworks
103521,2021.doi:10.1016/j.artint.2021.103521. into Answer Set Programming. In Proceedings of the Twenty-Ninth
[11] P.Ferraris,J.Lee,andV.Lifschitz. AgeneralizationoftheLin-Zhao InternationalJointConferenceonArtificialIntelligence,pages1755–
theorem. AnnalsofMathematicsandArtificialIntelligence,47(1):79– 1762,July2020. ISBN978-0-9992411-6-5. doi:10.24963/ijcai.2020/
101,June2006.ISSN1573-7470.doi:10.1007/s10472-006-9025-2. 243.
[12] K.Gao,K.Inoue,Y.Cao,andH.Wang. Adifferentiablefirst-order
rule learner for inductive logic programming. Artificial Intelligence,
331:104108,June2024. ISSN0004-3702. doi:10.1016/j.artint.2024.
104108.
[13] A.L.Gaunt,M.Brockschmidt,N.Kushman,andD.Tarlow. Differen-
tiableProgramswithNeuralLibraries. InProceedingsofthe34thIn-
ternationalConferenceonMachineLearning,pages1213–1222,July
2017.
[14] M.GelfondandV.Lifschitz.Thestablemodelsemanticsforlogicpro-
gramming.InICLP/SLP,volume88,pages1070–1080,1988.
[15] P.HitzlerandM.K.Sarker,editors. Neuro-SymbolicArtificialIntelli-
gence:TheStateoftheArt,volume342ofFrontiersinArtificialIntel-
ligenceandApplications. IOSPress,2022. ISBN978-1-64368-244-0.
doi:10.3233/FAIA342.
[16] P.Hitzler,M.K.Sarker,andA.Eberhart,editors.CompendiumofNeu-
rosymbolicArtificialIntelligence,volume369ofFrontiersinArtificial
IntelligenceandApplications. IOSPress,2023. ISBN978-1-64368-
406-2.doi:10.3233/FAIA369.
[17] Y.Lecun,L.Bottou,Y.Bengio,andP.Haffner. Gradient-basedlearn-
ingappliedtodocumentrecognition.ProceedingsoftheIEEE,86(11):
2278–2324,Nov.1998.ISSN1558-2256.doi:10.1109/5.726791.
[18] F.LinandY.Zhao.ASSAT:Computinganswersetsofalogicprogram
by SAT solvers. Artificial Intelligence, 157(1):115–137, Aug. 2004.
ISSN0004-3702.doi:10.1016/j.artint.2004.04.004.
[19] R.Manhaeve,S.Dumancic,A.Kimmig,T.Demeester,andL.DeRaedt.
DeepProbLog:NeuralProbabilisticLogicProgramming. InAdvances
inNeuralInformationProcessingSystems31,pages3749–3759,2018.
[20] W. Marek and V. Subrahmanian. The relationship between stable,
supported,defaultandautoepistemicsemanticsforgenerallogicpro-
grams. Theoretical Computer Science, 103(2):365–386, Sept. 1992.
ISSN03043975.doi:10.1016/0304-3975(92)90019-C.
[21] M.Mintz,S.Bills,R.Snow,andD.Jurafsky. Distantsupervisionfor
relation extraction without labeled data. In Proceedings of the Joint
Conferenceofthe47thAnnualMeetingoftheACLandthe4thInterna-
tionalJointConferenceonNaturalLanguageProcessingoftheAFNLP,
pages1003–1011,Aug.2009.
[22] R.Riegel,A.Gray,F.Luus,N.Khan,N.Makondo,I.Y.Akhalwaya,
H. Qian, R. Fagin, F. Barahona, U. Sharma, S. Ikbal, H. Karanam,
S.Neelam,A.Likhyani,andS.Srivastava. LogicalNeuralNetworks.
arXiv preprint: arXiv:2006.13155, June 2020. doi: 10.48550/arXiv.
2006.13155.
[23] T.RocktäschelandS.Riedel. End-to-enddifferentiableproving. In
AdvancesinNeuralInformationProcessingSystems,pages3788–3800,
2017.
[24] C.Sakama,K.Inoue,andT.Sato.Logicprogrammingintensorspaces.
AnnalsofMathematicsandArtificialIntelligence,89:1133–1153,Aug.
2021.ISSN1573-7470.doi:10.1007/s10472-021-09767-x.
[25] T.Sato,C.Sakama,andK.Inoue. From3-valuedSemanticstoSup-
portedModelComputationforLogicProgramsinVectorSpaces. In
12th International Conference on Agents and Artificial Intelligence,
pages758–765,Sept.2020.ISBN978-989-758-395-7.
[26] T.Sato,A.Takemura,andK.Inoue. Towardsend-to-endASPcompu-
tation.arXivpreprint:arXiv:2306.06821,June2023.
[27] H. Shindo, V. Pfanschilling, D. S. Dhami, and K. Kersting. αILP:
Thinkingvisualscenesasdifferentiablelogicprograms. Mach.Learn.,
112(5):1465–1497,2023.doi:10.1007/S10994-023-06320-1.
[28] A.TakemuraandK.Inoue. Gradient-BasedSupportedModelCom-
putationinVectorSpaces. InLogicProgrammingandNonmonotonic
Reasoning, pages 336–349, 2022. ISBN 978-3-031-15707-3. doi:
10.1007/978-3-031-15707-3_26.
[29] M.Thoma.TheHASYv2dataset.CoRR,abs/1701.08380,2017.
[30] E.Tsamoura,T.Hospedales,andL.Michael. Neural-SymbolicInte-
gration:ACompositionalPerspective. ProceedingsoftheAAAICon-
ferenceonArtificialIntelligence,35(6):5051–5060,May2021. ISSN
2374-3468.
[31] T.Winters,G.Marra,R.Manhaeve,andL.D.Raedt. DeepStochLog:A ExperimentalDetails themselves,i.e.,thesumsinadditiontasksorthebinarymembership
labelsinmembershiptasks.
A.1 Datasets
TaskTable5. NAecucruarlacbyas(e%lin)ereTsuimltse(s)
For tasks involving MNIST digits, specifically MNIST Addition 1
Addition1 89.3 22.3
and 2, Member 3 and 5, and Add2x2, we used the MNIST dataset Addition2 1.0 12.6
fromLecunetal.[17].ThemathoperatorimagesinApply2x2task Add2x2 66.7 19.2
comes from the HASY dataset by Thoma [29], and we only used Apply2x2 0.1 24.3
Member3 26.9 8.7
theaddition,subtractionandmultiplicationoperators.Wethengen-
Member5 40.6 9.5
eratedrandomcombinationsofimagesforeachtaskandusedthem
astrainingdata.
B ProofofProposition1
A.2 ComputationalEnvironment
Proposition3. (EmbeddingModelsofNormalLogicPrograms)Let
TheexperimentalenvironmentisadesktopcomputerwithanAMD P = (Q,D) be an embedding of a ground normal logic program
Ryzen 7950X (16c/32t), 128GB RAM, and an NVIDIA A4000 P,dist(·,·)beadistancefunctioninametricspace,vbeaninter-
(16GB),withsettingstoutilizetheGPUasmuchaspossible. pretationvectorrepresentingI ⊆B ,andwbeitscomplementary
P
interpretationvector.Then,forasuitableinterpretationvectorv,
A.3 NeuralNetwork (cid:18) (cid:18) (cid:16) (cid:0) (cid:1)(cid:17)(cid:19)(cid:19)
I |=comp(P)iffdist v,min D 1−min Q(1−w) =0
1 1
A.3.1 NeuralNetworkArchitectures
Proof. Firstly,weshowthat1−min (Q(1−w))correspondstothe
1
TaskTable4. NNeeutrwalonrketworkarchitecturesuNseeduirnalthBeaseexlpineeriments
evaluationofconjunctionsintherulebodies.Weonlyshowthecase
Addition1 Conv2D(6,5), Max- Identical to the network
forasinglerowsliceoftheprogrammatrix,butbyconstructionitcan
Pool2D(2,2), ReLU, on the left, except the
betriviallyappliedtotheentirematrix.Byconstruction,asliceofthe
Conv2D(16,5), Max- lasttwolayersLinear(19),
Pool2D(2,2), ReLU, LogSoftmax programmatrixQ i:correspondstothebodyofaruleR i.Then,the
Linear(120), ReLU, matrix-vectorproductQ wcomputesthenumberoftrueliteralsin
i:
Linear(84), ReLu, Lin- I.WhenallliteralsinthebodyofR aretrue,thebodyistrueinI,
i
ear(10),Softmax (cid:80)
and |Q | = Q w.Theconjunctionscanalsobecomputedas
Addition2 IdenticaltoAddition1 Identical to Addition 1, j ij i:
exceptthelasttwolayers thenegationofdisjunctionsofnegatedliterals,usingDeMorgan’s
Linear(199),LogSoftmax law,i.e.,L 1∧···∧L n = ¬(¬L 1∨···∨¬L m).Thus,wecount
Add2x2 IdenticaltoAddition1 Identical to Addition 1, thenumberoffalseliteralsinthebodyofR ,byk = Q (1−w).
i i:
exceptthelasttwolayers
If k ≥ 1, then there is at least one literal that is false in I, and if
Linear(19),LogSoftmax
k = 0thenthereisnoliteralthatisfalseinI.Theexistenceofat
Apply2x2 Identical to Addition 1, Identical to Addition 1,
except the penultimate exceptthelasttwolayers leastonefalseliteraliscomputedbymin 1(Q i:(1−w)),whichis1
layerLinear(3) Linear(1200), LogSoft- ifthereisafalseliteraland0otherwise.Thiscorrespondstotheeval-
max uationofdisjunctionsofnegatedliterals.Nowweobtainthecomple-
Member3 IdenticaltoAddition1 Identical to Addition 1,
mentbysubtractingfroma1-vector,i.e.,1−min (Q (1−w)),
exceptthelasttwolayers 1 i:
Linear(1),Sigmoid which corresponds to the negation of the disjunctions. Therefore,
Member5 IdenticaltoAddition1 Identical to Addition 1, 1−min 1(Q i:(1−w)) corresponds to the evaluation of I |= R i
exceptthelasttwolayers invectorspaces.
Linear(1),Sigmoid (cid:18) (cid:16) (cid:0) (cid:1)(cid:17)(cid:19)
Secondly,weshowthatmin D 1−min Q(1−w) corre-
1 1
Table4summarizestheneuralnetworkarchitecturesusedinthe spondstotheevaluationofdisjunctionsoftherulebodiesthatshare
experiments.Conv2D(o,k)isa2-dimensionalconvolutionlayerwith the same head atoms. Let {Rai} = {a
i
← B j,...,a
i
← B t}
o output channels and a kernel size of k. MaxPool2D(k,s) is a 2- betheorderedsetofrulesthatsharethesameheadatoma ,where
i
dimentionalMaxPoollayerwithakernelsizeofk,andastrideofs. B denote the rule bodies, and B ∨···∨B be the disjunction
j j t
Lin(n)isafullyconnectedlayerofoutputsizen. of the rule bodies. We assume the starting index j is appropri-
ately set such that j corresponds to the first row of submatrix cor-
responding to {Rai} in the program matrix Q. Using the afore-
A.3.2 NeuralBaselines
mentioned result, we can count the number of true rule bodies as:
WereporttheresultsoftheneuralbaselinesinTable5.Theexper- d = (cid:80)t(1−min (Q (1−w))). By construction of the head
i j 1 j:
imentalsettingsareidenticaltotheonespresentedinthemaintext, matrixD,wecanreplacethesummationbymatrixmultiplication:
i.e.,thesamenumberoftrainingdata,optimizersettings(Adamwith d = D (1 − min (Q(1 − w))). For an atom a , d ≥ 1 if
i i: 1 i i
learningrateof0.001),numberofepochs(1),batchsize(1),timeout there is at least one rule body that is true in I, and d = 0 other-
i
(30 mins), and number trials (5 per experiment). The architectures (cid:18) (cid:16) (cid:0) (cid:1)(cid:17)(cid:19)
wise. Thus, h = min D 1−min Q(1−w) = 1 if
usedinthisexperimentareshowninthe“NeuralBaseline“column i 1 i: 1
ofTable4. thereisatleastonerulebodythatistrueinI andinthedisjunction
Notethattheaccuracyfiguresinthistablearenot directlycom- B j ∨···∨B t,and0otherwise.Computingh i alsocorrespondsto
p ma ar ia nbl te ext to at rh ee thr ees au cl cts uri an cyth fie gm ura ei sn fote rx tt h. eT dh ie gir te as nu dlts opre ep rao tr ot red clain sst ih fie
-
t hh Iee =va mlu ia ntio (cid:18)n Dof (cid:16)I 1|= −c mom inp (cid:0)( QR a (i 1). −Le wt )u (cid:1)s (cid:17)in (cid:19)tr .o Td hu ec ne ta hc eo el lu em mn env te sc hto Ir
cations,whereastheonesreportedherearetheaccuracyforthelabels 1 1 iare 1 if there is at least one rule body for a i that is true in I, and SinceESLo isdefinedforeachloop,theremaybeanexponential
(cid:18) (cid:16) (cid:0) (cid:1)(cid:17)(cid:19) numberofexternalsupportmatrices.
0otherwise.Therefore,min D 1−min Q(1−w) corre-
1 1
spondstotheevaluationofdisjunctionsoftherulebodiesthatshare Example4. ConsiderP ,whichhasasingleloopL={{a}}.Then
1
thesameheadatoms. according to the previous definitions construct LP1 ∈ {0,1}1×3,
Nowwearereadytoprovetheif-and-only-ifrelationinthepropo- andESL1 ∈{0,1}3×3asfollows:
sition.LethI =min (cid:18) D(cid:16) 1−min (cid:0) Q(1−w)(cid:1)(cid:17)(cid:19) ,thenthesec-
1 1 R R R
1 2 3
ondpartissimplifiedtodist(v,hI))=0. a b c a 1 0 0 
• IfI |=comp(P),thendist(v,hI)=0. LP1 =L 1 (cid:0) 1 0 0(cid:1) ESL1 = b  0 0 0  (11)
c 0 0 0
Suppose I |= comp(R ), then there is at least one rule body
ai
that is true in I, so hI = 1. Otherwise, when we have I ̸|=
i
comp(R ),hI =0.Therefore,itholdsthathI =v ,andsince C.2 EvaluatingEmbeddedLoopFormula
ai i i i
thiscanbetriviallyappliedtotheentirevectorusingtheindexi,
wehavev=hI,i.e.,dist(v,hI)=0. Weshallbeusingtheconjunctiveloopformuladefinition.Recallthe
• Ifdist(v,hI)=i 0,thenI |=comp(P). d (cid:87)efinitionoftheconjunctiveloopformula:LF(L)∧ =((cid:86) a∈La)→
Considerdist(v,hI) = 0.ForhI i = 1,thereisatleastonerule ( B∈E (cid:86)BP(L)BF( (cid:87)B)).Thiscanthenbetranslatedintothefollow-
body that is true in I, and for hI
i
= 0, there is no rule body ing;¬( a∈La)∨( B∈EBP(L)BF(B)).Thefirsttermcanbetriv-
that is true in I. Since we have v = hI, for v = hI = 1, ially evaluated by a simple matrix-vector multiplication; given an
i i i i
a
i
↔ (cid:87) Rj∈heads(P,ai)body(R j) is satisfied and denote I |= cin ot mer pp ure teta st ti ho en nv ue mct bo er rv ofre lop ore pse an toti mng sta hn atin at re er fp ar le st eat ii non I.I,L o(1−v)
comp(R ), and for v = hI = 0 denote I ̸|= comp(R ).
ai i i ai The second term requires that there must be at least one exter-
Sincetheindexiisarbitrary,weconcludeI |=comp(P).
nal support rule body that is satisfied in I. We start by evaluating
therulebodiesinasimilarmannertotheevaluationofimplication
rulesasinProposition3.GivenanembeddingQofaprogramanda
C LoopFormulaandStableModelSemantics complementaryinterpretationvectorw,weobtainaindicatorvector
z ∈ {0,1}R×1 where z = 1 indicates the rule body is satisfied:
i
Thecontentofthefollowingsectioniscloselyrelatedtoastudyby z=1−min (Q(1−w)).Then,wecombinezwiththeloopma-
1
Satoetal.[26],whichaddressesthecomputationofstablemodelsin trixandtheexternalsupportmatrix:L ESLoz.Intuitively,ESLoz
o
vectorspaces. isequivalenttocheckingwhetheratleastoneoftheexternalsupport
bodiesholdsforeacha ∈ L ,andmultiplicationwithL amounts
i o o
C.1 EmbeddingLoopFormula tocountingthenumberoftrueexternalsupportbodies.
ForaloopL ∈L,wethencombinethetwotermsinthedisjunc-
o
TheLin-Zhaotheorem[18]showshowtoturnanormallogicpro-
tion:
gram into a set of propositional formulas that describe the stable
u =min
(cid:0)
L (1−v)+L
ESLoz(cid:1)
(12)
models of the program. A set ∅ ⊂ L ⊆ B is a loop of a logic o 1 o o
P
program if it induces a non-trivial strongly connected subgraph of Notethatu oisascalarandu o =1indicatestheloopformulaforL o
thepositivedependencygraphofP.Asubgraphissaidtobenon- issatisfied.Wethenapplythistoallloopsintheprogram,andcount
trivial if each pair of atoms in L is connected by at least one path thenumberofunsatisfiedloopformula:
ofnon-zerolength.ThesetofallloopsofP isdenotedbyloop(P).
Thenumberofloopsmaybeexponentialin|B P|,andtheprogram
L
=(cid:88)O
(1−u ) (13)
P is tight if loop(P) = ∅. For L ⊆ B , the external support is loop o
P
given by ES(L) = {r ∈ P|head(r),body+(r)∩L = ∅}. The o=1
Loopformulacanbeconstructedtoexcludeloopswithoutexternal Thistermwillbe0ifftheloopformulaissatisfied.
support.ThedisjunctiveloopformulaisLF(L)∨ = ((cid:87) a) →
(cid:87) a∈L
( BF(B)).AccordingtoFerrarisetal.[11],itisequiv-
alenB t∈ tE oB tP he(L c) onjunctive loop formula LF(L)∧ = ((cid:86) a) → C.3 StableModelSemanticsinVectorSpaces
(cid:87) a∈L
( BF(B)). The loop formula enforces all atoms in L TheLin-ZhaotheoremessentiallystatesthatIisastablemodelofa
B∈EBP(L)
tobefalsewheneverLisnotexternallysupported. programiffI |=comp(P)∪LF wherecomp(P)isthecompletion
We introduce two types of matrices; a loop matrix which corre- and LF is the loop formula. Thus, combining Proposition 1 and
spondstotheenumerationofloopsL,andaexternalsupportmatrix the aforementioned embedded loop formula evaluation, we can
whichisanindicatormatrixfortheexternalsupport. construct the following loss function which is 0 iff when v (and
correspondingI)isastablemodelofP.
Definition7. (Loopmatrix)Lettheo-thloopbedenotedbyL ∈
o
L(1≤o≤O)whereOisthetotalnumberofloopsintheprogram.
Then, L is embedded into the matrix L by stacking a binary row
L =L +L
vectorL ,whereL =1ifidx(i)∈L andL =0otherwise. SM comp. loop
o,: o,i o o,i (cid:18) (cid:18) (cid:16) (cid:0) (cid:1)(cid:17)(cid:19)(cid:19)
Definition8. (ExternalSupportmatrix)LetES(L o)betheexter- =dist v,min 1 D 1−min 1 Q(1−w)
nalsupportfortheloopL .Then,defineabinarymatrixESLo ∈
{0,1}N×RwhereESL i,jo =o 1ifthej-thruleistheexternalsupport +(cid:88)O (cid:18)
1−min
(cid:0)
L (1−v)+L
ESLoz(cid:1)(cid:19)
(14)
1 o o
fortheatoma ∈L,andESLo =0otherwise.
i i,j o=1Whilethisfunctionisalmosteverywheredifferentiableandwecan
indeedusegradient-basedmethodstosearchforstablemodels,itis
stillanopenquestionwhetherthiscontinuousmethodismoreeffi-
cientthanthediscretemethodcommonlyusedinexistingsolvers.