Distributed quasi-Newton robust estimation under
differential privacy
Chuhan Wang1, Lixing Zhu1 and Xuehu Zhu2∗
1 Department of Statistics, Beijing Normal University at Zhuhai, China
2 School of Mathematics, Xi’an Jiaotong University, Xi’An, China
Abstract
For distributed computing with Byzantine machines under Privacy Protection
(PP) constraints, this paper develops a robust PP distributed quasi-Newton estima-
tion, which only requires the node machines to transmit five vectors to the central
processor with high asymptotic relative efficiency. Compared with the gradient de-
scent strategy which requires more rounds of transmission and the Newton iteration
strategy which requires the entire Hessian matrix to be transmitted, the novel quasi-
Newton iteration has advantages in reducing privacy budgeting and transmission
cost. Moreover, our PP algorithm does not depend on the boundedness of gradients
and second-order derivatives. When gradients and second-order derivatives follow
sub-exponential distributions, we offer a mechanism that can ensure PP with a suf-
ficiently high probability. Furthermore, this novel estimator can achieve the optimal
convergence rate and the asymptotic normality. The numerical studies on synthetic
and real data sets evaluate the performance of the proposed algorithm.
KEY WORDS: M-estimation; Distributed inference; Differential privacy; Quasi-Newton;
Byzantine-robustness
∗Lixing Zhu is the corresponding author. Email: lzhu@bnu.edu.cn. The coauthors’ names are in
alphabetical order. The research described herewith was supported by a grant (NSFC12131006) from the
National Natural Scientific Foundation of China. ???
1
4202
guA
22
]LM.tats[
1v35321.8042:viXra1 Introduction
In recent years, with the decreasing cost of data collection and storage, massive data has
become a common concern in statistical analysis. However, the data utilized for training
models is often sensitive and possessed by private individuals or companies. Although
data analysts can make the most accurate statistical inferences by directly obtaining raw
data from data owners, this may put individuals at risk of leaking sensitive information.
Accordingly, it is necessary to construct information transmission methods that fulfill the
requirements of data analysis while ensuring stringent privacy protection for sensitive data.
In practical problems, massive personal data sets often come from multiple sources.
For privacy protection purposes, data analysts cannot obtain all data from these sources,
while only certain statistics for analysis. This can be seen as a distributed computing
framework: the node machines are responsible for storing data and transmitting privacy-
preserving statistics to the central processor, and the central processor is responsible for
computing and providing necessary information feedback to the node machines. We are
motivatedtodevelopadistributedcomputingmethodthatprotectsindividualprivacywith
high probability and makes statistical inferences as accurately as possible.
Besides, due to machine crashes, interference during data transmission or other reasons,
some node machines may behave abnormally, or even exhibit Byzantine failures—arbitrary
and potentially adversarial behavior. To handle this situation, we propose the robust
algorithm such that when a small number of machines encounter anomalies or transmission
problems, the final estimation results still have good statistical properties. This paper
mainly focuses on developing robust M-estimation methods under privacy protection in a
distributed framework.
21.1 Problem formulation
A general M-estimation problem is formulated as follows:
(cid:90)
θ∗ = argminE {f(X,θ)} = argmin f(X,θ)dF(x), (1.1)
X
θ∈Θ θ∈Θ X
where X = (X ,X ,··· ,X )⊤ is a p-dimensional random vector in the sample space X ⊂
1 2 q
Rp, and θ = (θ ,θ ,··· ,θ )⊤ is a p-dimensional vector in the parameter space Θ ∈ Rp.
1 2 p
Moreover, f is a convex loss function, and F(x) represents the cumulative distribution
function of X.
In a distributed setting, we assume there are totally N = n(m + 1) i.i.d. samples
{X ,X ,··· ,X } evenly stored on m+1 different machines {I ,I ,I ,···I }, including
1 2 N 0 1 2 m
one central processor I and m node machines {I ,I ,···I }, and each machine has n
0 1 2 m
samples. Some distributed M-estimation methods have been in the literature. For example,
Jordan et al. (2019) transforms parameter estimation into a gradient descent problem, fully
utilizing the gradients of all data to achieve the optimal convergence rate in a statistical
sense. Besides, Huang and Huo (2019) reduces the rounds of communication by passing
the Hessian matrix, Fan et al. (2023) considers a distributed M-estimation method with
penalty terms, and Wang and Wang (2023) studies the M-estimation problem when data
on different machines has heterogeneity.
For robust estimation in a distributed framework, we usually refer to machines that
provideabnormaldataasByzantinemachines, andtheresultingerrorsarecalledByzantine
failures (Lamport et al., 1982). Concretely speaking, Byzantine machines may behave
completely arbitrary and send unreliable information to the central processor. Here we
assumetheproportionofByzantinemachinestobeα ,anduseB ⊂ {1,··· ,m}torepresent
n
the index set of the Byzantine machines.
The key to solve Byzantine failures is adopting robust estimation strategies. The meth-
ods in the literature include median estimation, trimmed mean estimation (Yin et al., 2018,
2019),geometricmedianestimation(Chenetal.,2017)anditerativefilteringestimation(Su
3and Xu, 2019), and among these methods, median and trimmed mean estimation are the
most commonly used. However, these two methods have low asymptotic relative efficiency.
For example, compared with the mean estimation of normal samples, the asymptotic rel-
ative efficiency of the median estimation is only 0.637, and the trimmed mean estimation
is 1 − β, where β ≥ 2α is the proportion of truncated samples. Therefore, we try to
n
construct robust statistics with higher asymptotic relative efficiency.
In terms of privacy protection issues, the currently recognized privacy protection mech-
anisms mainly include the (ε,δ)-differential privacy protection (Wang et al., 2024; Cai
et al., 2024) and the f-differential privacy protection (Dong et al., 2022; Awan and Vadhan,
2023). Compared to the f-differential privacy protection, the differential privacy mech-
anism is simpler and easier to implement, and it is widely used in practical problems.
There are three main mechanisms for differential privacy protection by adding noise to
statistics: the Laplace mechanism, the Gaussian mechanism, and the exponential mecha-
nism. Laplace and Gaussian mechanisms are simpler then the exponential mechanism, and
Gaussian mechanism has better performances under the f-differential privacy protection
framework.
1.2 Contributions
To address Byzantine failures and privacy protection issues for distributed M-estimation,
while minimizing communication costs between machines and ensuring the effectiveness
of the algorithm, we adopt the ideas of quasi-Newton iteration and composite quantile
estimation (Goldfarb (1970) and Zou and Yuan (2008)), and achieve differential privacy
protection by Gaussian mechanism to the transmitted statistics. The proposed method has
the following features.
(1) For general Newton iteration (see, e.g., Huang and Huo (2019), when transmitting
the Hessian matrix, it needs to add the noise to every element of the p×p Hessian matrix
for privacy protection. When the dimension p is large, the privacy budget required by
the Hessian matrix is much greater than that required by the gradient. In other words,
4to achieve the same level of privacy protection as transmitting a p-dimensional vector, a
greateramountofnoisemustbeaddedtotheHessianmatrix. Thiscausestheaccuracyloss
of the final estimator. However, we will see that the quasi-Newton method only requires
the transmission of a p-dimensional vector, with the noise added to its p coordinates. As a
result, the privacy budget of each iteration is close to the gradient descent, and the quasi-
Newton algorithm requires less vector transmission. The proposed robust quasi-Newton
algorithm can reach the optimal convergence rate after transmitting five vectors and it-
erating two rounds. It is worth noting that the privacy budget will also increase linearly
with the times of vector transmission, therefore, under the same estimation accuracy, the
quasi-Newton method usually has less privacy budget than the Newton iteration. More-
over, in the second iteration, the quasi-Newton method avoids the difficulty of recomputing
the inverse matrix, which is particularly beneficial when p is large.
(2) In terms of robust estimation strategy, inspired from the idea of composite quantile
estimations widely used to reduce variance in (Zou and Yuan, 2008; Lin et al., 2019; Hou
et al., 2023), we suggest a Distributed Composite Quantile (DCQ) estimator to handle the
statistics used for iteration, including the gradients, and the Hessian matrices. As long
as the estimators computed by node machines converge to the normal distribution, we
can apply the DCQ method to these local estimators for the final robust estimation. The
main advantage of the DCQ estimations is that they have much higher asymptotic relative
efficiency than the median estimation. Specifically, for normal samples, the asymptotic
relative efficiency of the median estimator relative to the mean estimator is 0.653, whereas
the DCQ estimator can reach 0.955.
(3) We use the Gaussian mechanism for the differential privacy protection instead of
the Laplace mechanism. The Gaussian mechanism can maintain these limiting normal
distributions with a slightly larger variance, satisfying the requirements of the DCQ esti-
mations. In contrast, Laplace noise disrupts the normality of the original local estimators,
which may slow down the theoretical convergence rates of the estimators. Besides, the
Gaussian mechanism can also achieve optimal f-differential privacy protection, see Dong
5et al. (2022) for details.
(4) In previous privacy-preserving strategies for parameter estimation, boundedness
assumptions are often made to ensure bounded sensitivity. However, when dealing with
statistics such as the gradients being generally unbounded, such assumptions significantly
increase the limitations of the methods. The typical Gaussian distribution is also not
applicable under boundedness assumptions. A more representative approach involves con-
trolling the tail probability of statistics using sub-Gaussian or sub-exponential distribution
assumptions. This allows the definition of sensitivity such that differential privacy pro-
tection can be achieved with a sufficiently high probability. Cai et al. (2024) provides an
illustrative example in the context of principal component analysis. Our research applies
thisapproachprimarilytoM-estimators, gradientestimators, andotherusefulestimatorsin
distributed iterative procedures, thereby relaxing the boundedness constraints for statistics
in distributed optimization algorithms with privacy protection.
1.3 Organization
This paper is organized as follows. Section 2 introduces the notations and the basic prin-
ciple of the Gaussian privacy protection mechanism. Section 3 presents the Distributed
Composite Quantile (DCQ) estimators. Section 4 develops a differentially private robust
distributed quasi-Newton estimator and outlines its limiting properties. Section 5 discusses
the simulation results and real data analysis. Section 6 includes some further discussions.
Section 7 details the regularity assumptions. All proofs are provided in Supplementary
Materials.
62 Preliminaries
2.1 Notations
(1) For any vector v = (v ,v ,...,v )⊤, define ∥v∥ = ((cid:80)p v2)1/2 and v⊗2 = vv⊤. For
1 2 p l=1 l
two vectors u ∈ Rp and v ∈ Rp, let ⟨u,v⟩ = (cid:80)p u v .
l=1 l l
(2) For a matrix M, denote ∥M∥ = sup ∥a⊤M∥ as the spectral norm, which is
∥a∥=1
the largest eigenvalue of M when M is a square matrix. Denote Tr(M) as the trace of M,
and let M represent the row vector of the l-th row of M. Write the largest and smallest
l·
eigenvalues of M as λ (M) and λ (M), respectively. Let diag(M) be the vector of
max min
entries on the diagonal of M.
(3) Define B(v,r) = {w ∈ Rp : |w−v| ≤ r}. Let I(·) be the indicator function.
(4) Write F−1(y) as the smallest x ∈ R satisfying F(x) ≥ y. Let N(0,1) denote the
standard normal distribution with the distribution function Ψ(x) = P(N(0,1) ≤ x) and
the density function ψ(x) = √1 exp(−x2).
2π 2
(5) Use ∇ and ∇2 to represent the gradient and Hessian operators. For f(θ) =
f(θ ,θ ,...,θ ), ∇f denotes the partial derivative with respect to θ , and ∇2f rep-
1 2 p l l l1l2
resents the second-order derivatives of θ and θ . Moreover, ∇f and ∇2f refer to the
l1 l2
vector with the l-th entry ∇f and the matrix with the (l ,l )-entry ∇2f , respectively..
l 1 2 l1l2
(6) For any positive integer N, denote [N] as the index set {1,2,...,N} and [N] as
0
the index set {0,1,2,...,N}. For two sets A and A , denote A \A as the set of elements
1 2 1 2
belonging to A but not to A .
1 2
(7) For X ,X ,...,X , let med{X ,j ∈ [m]} be the median of {X ,X ,...,X }. For
1 2 m j 1 2 m
m vectors a = (a ,a ,...,a )⊤, j ∈ [m], define med{a ,j ∈ [m]} = (med{a ,j ∈
j j,1 j,2 j,p j j,1
[m]},...,med{a ,j ∈ [m]})⊤, where med{a ,j ∈ [m]} is the median of {a ,j ∈ [m]},
j,p j,l j,l
l ∈ [p].
(8)ForM-estimation,writethem+1machinesasI ,I ,I ,...,I ,whereI actsasthe
0 1 2 m 0
central processor, and the others are node machines. For any random variable X, let E (·)
X
represent the expectation over X. For j ∈ [m] , define F (θ) = 1 (cid:80) f (X ,θ). Let θˆ =
0 j n i∈Ij i j
7(θˆ ,...,θˆ ) = argmin F (θ). DefinethegloballossfunctionF(θ) = 1 (cid:80)m F (θ) =
j1 jp θ∈Θ j m+1 j=0 j
1 (cid:80)(m+1)nf(X ,θ) and F (θ) = E{f(X,θ)}.
(m+1)n i=1 i µ
2.2 A brief review of privacy framework
In a distributed framework, allowing the central processor to access all original data on
each node machine is unrealistic and poses privacy risks. Differential privacy mechanisms
are typically used to safeguard privacy, ensuring that each node machine protects its local
data and communicates only privatized information to the central processor for further
analysis.
Differential Privacy (DP) was first conceptually proposed by Dwork et al. (2006). If
two data sets X and X′ differ by only one datum, we say X and X′ are a pair of adjacent
data set. A randomized algorithm M : Xn → Rp is called (ε,δ)-differentially private if for
every pair adjacent data sets X,X′ ∈ Xn that differ by one individual datum and every
S ∈ Rp,
P(M(X) ∈ S) ≤ eε ·P(M(X′) ∈ S)+δ,
where the probability P is only induced by the randomness of M.
To provide a randomized algorithm that satisfies DP, we need to define ℓ sensitivity.
2
The ℓ -sensitivity of M : Xn → Rp is defined as ∆ = sup ∥M(X) − M(X′)∥, where
2 X,X′
X and X′ are a pair of adjacent data sets. Then the following Gaussian mechanism can
achieve (ε,δ)-DP.
Lemma 2.1. (Dwork et al. (2014)) Given a function M : Xn → Rp with the ℓ -sensitivity
2
√
∆ and a dataset X ⊂ Xn, assume σ ≥ 2log(1.25/δ)∆ . The following Gaussian mechanism
ε
yields (ε,δ)-DP:
G(X,σ) := M(X)+b, b ∼ N(0,σ2I ),
p
8where I is a p×p identity matrix.
p
The Gaussian mechanism above requires a finite sensitivity value. However, in many
cases, the sensitivity under strict definition is unbounded (e.g. M is the average of i.i.d.
standard normal distribution random vectors) and then the Gaussian mechanism fails to
hold. To solve this problem, most existing studies designed some bounded conditions, for
example, Zhou and Tang (2020) and Wang et al. (2024) assumed that the L norm of
2
the first derivative of the loss functions in their algorithms are bounded. However, if we
assume sub-exponential (sub-Gaussian) distributions for i.i.d. random variables, then for
their average, by adjusting the noise variance, the Gaussian mechanism can achieve (ε,δ)
privacy protection with a sufficiently high probability. We will discuss this issue in detail
in Section 4.
3 Distributed composite quantile estimation
In a distributed framework, for given n, let {Y(n),j ∈ [m]} be m i.i.d. observations that are
j
usually summary statistics of a random variable Y(n). When Y(n) converges to a symmetric
random variable Y as n → ∞, we can use Yˆ(n) = med{Y(n),j ∈ [m]} to estimate E(Y(n)),
med j
which is robust but has low asymptotic relative efficiency. Let g(·) and G(·) denote the
density function and the distribution function of Y − E(Y), respectively. To reduce the
variance of
Yˆ(n),
we modify
Yˆ(n)
by using the idea of composite quantile estimation (Wang
med med
et al. (2023)): for a given integer K,
(cid:104) (cid:105)
σˆ (cid:80)K (cid:80)m I(Y(n) ≤ Yˆ(n) +σˆ ∆ )−κ
Yˆ(n) =Yˆ(n)
−
Y(n) k=1 j=1 j med Y(n) k k
, (3.1)
cq med m(cid:80)K
g(∆ )
k=1 k
where κ = k , ∆ = G−1(κ ) and σˆ2 is the variance estimator of Y(n). Compared
k K+1 k k Y(n)
to Yˆ(n), Yˆ(n) utilizes more information about the empirical distribution of Y(n). We will
med cq
define the estimator with the form in (3.1) as the Distributed Composite Quantile (DCQ)
estimator. The following Theorem 3.1 guarantees the convergence rate of
Yˆ(n).
cq
9Theorem 3.1. (Convergence rate of the DCQ estimator) Let α be the proportion of Byzan-
n
tine machines and Yˆ(n) be defined in (3.1). Suppose that {Y(n),j ∈ [m]} are i.i.d random
cq j
variables with
(cid:12) (cid:16) (cid:17) (cid:12)
(cid:12)P (Y(n) −µ )/σ < x −G(x)(cid:12) = O(ρ ),
(cid:12) j Y Y(n) (cid:12) n
where µ is a constant and σ2 = Var(Y(n)). Moreover, assume G(·) satisfy Assumption
Y Y(n)
7.15 in Section 7 and σˆ −σ = O (η ). Then
Y(n) Y(n) p n
(cid:32) (cid:114) (cid:33)
1 logn (α +ρ +η )logn
Yˆ(n) −µ = O √ + + n n n +α +ρ +η .
cq Y p m m m n n n
√ √
When logn = o( m) and α +ρ +η = o(1/ m), we have
n n n
√
m(Yˆ(n)
−µ )/σ
→d
N(0,1), (3.2)
cq Y cq
where σ2 = D σ2 with D = (cid:80)K (cid:80)K [min{κ ,κ }/{(cid:80)K ψ(∆ )}2].
cq K Y(n) K k1=1 k2=1 k1 k2 k=1 k
Remark 3.1. Equation (3.2) in Theorem 3.1 shows that for m local estimators computed
by the node machines converging to a certain distribution, if α , ρ and η satisfy some rate
n n n
conditions, then the DCQ estimator of these local estimators can converge to normal distri-
√
bution with the rate m. For example, let {X ,i ∈ [n],j ∈ [m] } be i.i.d. random variables
j,i 0
with mean µ , Y(n) = √1 (cid:80)n (X − 1 (cid:80)n X ), σˆ = 1 (cid:80)n (X − 1 (cid:80)n X )2,
X j n i=1 j,i n i=1 j,i Y(n) n i=1 0,i n i=1 0,i
α
n
= √1
m
and logn = o(m), then ρ
n
= η
n
= √1
n
and Yˆ c( qn) − µ
X
= O p(√1 m). In this ex-
ample, the DCQ estimator degenerates into the VRMOM estimator proposed by Tu et al.
(2021). Furthermore, the DCQ estimation can be applied to more statistics, including the
M-estimation.
104 Differentially Private Robust Distributed Quasi-
Newton Estimator
InSubsection4.1,wepresentthecomputationofthedifferentiallyprivaterobustdistributed
quasi-Newton estimator step by step. To maintain content coherence, we defer the discus-
sion of the differential privacy guarantee to Subsection 4.2. Furthermore, Subsection 4.3
examines the scenario in which the central processor may act as a Byzantine machine.
4.1 Quasi-Newton algorithm
The quasi-Newton method does not compute the Hessian matrix directly. Instead, it ap-
proximates the Hessian or its inverse, while retaining certain properties of the Hessian that
provide essential information for determining the gradient descent direction. Depending on
how the Hessian matrix is approximated, common quasi-Newton methods are categorized
into three types: Symmetric Rank-One update (SR1) Davidon (1991), Broyden-Fletcher-
Goldfarb-Shanno update (BFGS) Goldfarb (1970), and Davidon-Fletcher-Powell update
(DFP) Fletcher and Powell (1963). Compared to SR1, both BFGS and DFP ensure the
positive definiteness of the Hessian matrix estimation. There is a dual relationship between
DFP and BFGS; however, BFGS often performs better than DFP in practice. Several
studies have applied BFGS to distributed computing without considering robustness and
privacy protection, such as Chen et al. (2014), Eisen et al. (2017), and Wu et al. (2023).
Thus, we recommend BFGS for quasi-Newton iteration. Notably, the computational cost
of determining a general Hessian inverse matrix is O(p3). However, by approximating the
Hessianinverseduringtheiterationwithoutdirectlyinvertingthematrix, thequasi-Newton
method reduces the computational complexity to O(p2) (Wu et al. (2023)).
We then briefly review the BFGS algorithm in a distributed framework. Let θˆ be
(t)
the parameter estimator after t iterations. ∇F(θˆ ) and ∇2F(θˆ ) are the corresponding
(t) (t)
11derivative and Hessian matrix respectively. According to Taylor’s expansion,
∇F(θˆ )−∇F(θˆ ) ≈ ∇2F(θˆ )(θˆ −θˆ ).
(t+1) (t) (t+1) (t+1) (t)
For the j-th machine, assume we already have H(t) ≈ {∇2F(θˆ )}−1. Then
j (t)
H(t+1){∇F(θˆ )−∇F(θˆ )} ≈ θˆ −θˆ . (4.1)
j (t+1) (t) (t+1) (t)
Given H(t), H(t+1) is updated to be
j j
H(t+1) = H(t) +α v(t){v(t)}⊤ +β u(t){u(t)}⊤
j j j j j j j j
for some coefficient α ,β ∈ R and v(t),u(t) ∈ Rp. Note that when v(t) = ··· = v(t) =
j j 0 m
θˆ −θˆ , u(t) = H(t){∇F(θˆ )−∇F(θˆ )}, α = ··· = α = [{v(t)}⊤{∇F(θˆ )−
(t+1) (t) j j (t+1) (t) 0 m 0 (t+1)
∇F(θˆ )}]−1 and β = −[{u(t)}⊤{∇F(θˆ )−∇F(θˆ )}]−1, the left-hand term and the
(t) j j (t+1) (t)
right-hand term in (4.1) are equal for all j ∈ [m] .
0
Thus, through a simple organization, we update H(t+1) as:
j
H(t+1) = (V(t))⊤H(t)V(t) +ρ(t)(θˆ −θˆ )(θˆ −θˆ )⊤,
j j (t+1) (t) (t+1) (t)
where ρ(t) = [(θˆ −θˆ )⊤{∇F(θˆ )−∇F(θˆ )}]−1 and V(t) = I −ρ(t){∇F(θˆ )−
(t+1) (t) (t+1) (t) p (t+1)
∇F(θˆ )}(θˆ − θˆ )⊤. It is worth noting that both ρ(t) and V(t) are unrelated to j.
(t) (t+1) (t)
Therefore, the main goal of distributed computing is to merge the information provided
by H(t), or more specifically, to merge the product of (V(t))⊤H(t)V(t) and the gradient for
j j
j ∈ [m] , which are p-dimensional vectors.
0
In the following subsections 4.1.1-4.1.3, we discuss how to gain the robust initial estima-
tor and update it by the robust quasi-Newton algorithm for distributed computing under
privacy protection constraints.
124.1.1 The construction of an initial estimator
Based on the construction of the DCQ estimator, we consider how to obtain a robust
parameter estimator that can achieve privacy protection without iteration. Before infor-
mation communication, every machine can compute the local parameter estimators based
on their own data, that is, θˆ = argmin 1 (cid:80) f(X ,θ) for j ∈ [m] . To achieve
j θ∈Θ n i∈Ij i 0
(ε,δ)-DP, every machine needs to add a Gaussian noise to θˆ ,
j
θˆ = θˆ +b(1) = (θˆDP,··· ,θˆDP)⊤ with b(1) i. ∼i.d. N(0,s2I ) (4.2)
j,DP j j j1 jp j 1 p
and then sends θˆ to the central processor.
j,DP
The following lemma guarantees that the estimator in (4.2) satisfying (ε,δ)-DP has a
normal weak limit:
Lemma 4.1. Suppose that Assumptions 7.1-7.3 and 7.5-7.10 hold. Let
θˆDP
be the l-th
jl
entry of θˆ defined in (4.2). For j ∈/ B, there exists a positive constant C such that
j,DP a
(cid:12) (cid:32) √ (cid:33) (cid:12)
(cid:12) n(θˆDP −θ∗) (cid:12) (cid:18) plogn(cid:19)
sup (cid:12)P jl l ≤ u −Ψ(u)(cid:12) ≤ C √ . (4.3)
(cid:12) {σ2(θ∗)+ns2}1/2 (cid:12) a n
−∞<u<∞(cid:12) l 1 (cid:12)
According toTheorem 3.1, to compute theDCQ estimatorofθˆ , weneed toestimate
j,DP
√ √
the variance of nθˆDP. Recall θˆ = med{θˆ ,j ∈ [m] }. The variance of nθˆDP
jl med,DP j,DP 0 jl
can be estimated by σˆ2 := σˆ2(θˆ )+ns2, where
bl l med,DP 1
(cid:16) (cid:17)
σˆ2(θˆ ),··· ,σˆ2(θˆ )
1 med,DP p med,DP
(cid:18)
1 (cid:88)
= diag {∇2F (θˆ )}−1 {∇f(X ,θˆ )−∇F (θˆ )}⊗2
0 med,DP i med,DP 0 med,DP
n
i∈I0
(cid:19)
{∇2F (θˆ )}−1 .
0 med,DP
Lemma 4.2 shows that σˆ (θˆ ) is a consistent estimator of σ (θ∗), and ns2 is the
l med,DP l 1
increase for variance caused by Gaussian noise. Since this estimation only uses θˆ
med,DP
and the sample on the central processor, this variance computation process does not add
13any additional information transmission.
Lemma 4.2. Under Assumptions 7.1-7.10 and 7.13-7.14, for any l ∈ [p],
√
|σˆ (θˆ )−σ (θ∗)| = O (p/ n),
l med,DP l p
where(σ2(θ∗),··· ,σ2(θ∗))arethediagonalelementsofΣ(θ∗) = {∇2F (θ∗)}−1E{∇f(X,θ∗)⊗2}
1 p µ
{∇2F (θ∗)}−1.
µ
Then we can use the DCQ estimator of
{θˆDP,j
∈ [m] } as the l-th entry of the initial
jl 0
estimator. Let θˆ = (θˆDP,K,··· ,θˆDP,K)⊤, where
cq,DP cq,1 cq,p
(cid:104) √ (cid:105)
σˆ (θˆ )(cid:80)K (cid:80)m I(θˆDP ≤ θˆDP +σˆ (θˆ )Ψ−1(κ )/ n)−κ
θˆDP,K =θˆDP
−
bl med,DP k=1 j=1 √jl med,l bl med,DP k k
cq,l med,l
m
n(cid:80)K
ψ(Ψ−1(κ ))
k=1 k
(4.4)
and κ = k . The following theorem shows its convergence rate.
k K+1
Theorem 4.1. Suppose Assumptions 7.1-7.3, 7.5-7.10 and 7.14 hold. If α = O(1/logn),
n
log3n/m = o(1) and p2log2n/n = o(1), then
√ √
(cid:18)
α p p
p3/2logn(cid:19)
∥θˆ −θ∗∥ = O √n + √ + . (4.5)
cq,DP p
n mn n
When p is relatively large compared to n (e.g. p2log3n = n), the last term p3/2logn/n
in (4.5) may converge to zero slowly. We can improve the convergence rate through further
iterations.
4.1.2 One-stage estimator
In this subsection, we discuss how to use gradients and Hessian matrices to update the
initial estimator in the first iteration. After computing θˆ , the central processor sends
cq,DP
θˆ to the node machines, and then the node machines compute ∇F (θˆ ) by using
cq,DP j cq,DP
14their own samples. To achieve (ε,δ)-DP, the node machines send
∇FDP(θˆ ) = ∇F (θˆ )+b(2) with b(2) ∼ N(0,s2I ) (4.6)
j cq,DP j cq,DP j j 2 p
tothecentralprocessorI . Thenthecentralprocessorusestheselocalgradientstocompute
0
their DCQ estimator ∇(cid:92) FDP(θˆ ) and sends it back to the node machines. After that,
cq cq,DP
the node machines are responsible for using their own data to compute the estimator of the
inverse of the Hessian matrix, {∇2F (θˆ )}−1, and to add Gaussian noise to the vector
j cq,DP
{∇2F (θˆ )}−1∇(cid:92) FDP(θˆ ), that is
j cq,DP cq cq,DP
h(1) = {∇2F (θˆ )}−1∇(cid:92) FDP(θˆ )+b(3) with b(3) ∼ N(0,s2 I ). (4.7)
j j cq,DP cq cq,DP j j 3,j p
The node machines send h(1) to the central processor. Finally, the central processor com-
j
putes the DCQ estimator of h(1), which is written as H , the one-stage estimator can be
j 1
defined as:
θˆ = θˆ −H . (4.8)
os,DP cq,DP 1
Remark 4.1. When p is large, the computational complexity to compute {∇2F (θˆ )}−1
j cq,DP
is O(p3). To reduce computational burden, we can also use the quasi-Newton method to es-
timate {∇2F (θˆ )}−1, and the details can be found in Algorithm 1 in Wu et al. (2023).
j cq,DP
Since this estimation is performed on a single machine and does not involve robust estima-
tion and privacy protection, we will not discuss it further.
Lemmas4.5and5.1inSupplementaryMaterialsshowthat∇FDP(θˆ )andh(1) have
jl cq,DP j
normal weak limits. To derive the DCQ estimator, we still need to estimate their variance.
Similar to Subsection 4.1.1, the variance of their entries can be estimated by using the
data on the central processor. Write ∇FDP(θ) as (∇FDP(θˆ ),··· ,∇FDP(θˆ ))⊤,
j j1 cq,DP jp cq,DP
15√
the variance of n∇FDP(θˆ ) can be estimated by
jl cq,DP
1 (cid:88) 1 (cid:88)
{∇f (X ,θˆ )− ∇f (X ,θˆ )}2 +ns2.
n l i cq,DP n l i cq,DP 2
i∈I0 i∈I0
For h(1), since
j
{∇2F (θˆ )}−1∇(cid:92) FDP(θˆ )
j cq,DP cq cq,DP
={∇2F (θˆ )}−1{∇2F (θˆ )}{∇2F (θˆ )}−1∇(cid:92) FDP(θˆ ), (4.9)
j cq,DP j cq,DP j cq,DP cq cq,DP
√
the variance of nh(1) can be estimated as:
jl
1 (cid:88) [{∇2F (θˆ )}−1{∇2f(X ,θˆ )}{∇2F (θˆ )}−1∇(cid:92) FDP(θˆ )]2
n 0 cq,DP l· i cq,DP 0 cq,DP cq cq,DP
i∈I0
(cid:34) (cid:35)2
− 1 (cid:88) {∇2F (θˆ )}−1{∇2f(X ,θˆ )}{∇2F (θˆ )}−1∇(cid:92) FDP(θˆ )
n 0 cq,DP l· i cq,DP 0 cq,DP cq cq,DP
i∈I0
+ns2 , (4.10)
3,0
where {∇2F (θˆ )}−1 denotes the l-th row of the matrix {∇2F (θˆ )}−1.
0 cq,DP l· 0 cq,DP
Theorem 4.2. Suppose Assumptions 7.1-7.3, 7.5 and 7.8-7.14 hold. When p2log2n/n =
o(1), log3n/m = o(1) and α = O(1/logn), it holds that
n
√
(cid:18)
α p
(cid:114)
p
p3log2n(cid:19)
∥θˆ −θ∗∥ = O √n + + . (4.11)
os,DP p n mn n2
Remark 4.2. The difference between Theorem 4.1 and Theorem 4.2 is that the conver-
gence rate p3/2logn/n is increased to p3log2n/n2. However, under the rate assumptions
(cid:112)
p2log2n/n = o(1) and mlogn = o(n), this rate is still slower than p/(mn). In order to
obtain the optimal convergence rate, we need to further iterate the estimator in subsection
4.1.3.
164.1.3 Quasi-Newton estimator
Below we explain how to use the quasi-Newton algorithm for the second iteration. Firstly,
the central processor transfers θˆ back to the node machines, which compute the local
os,DP
gradient ∇F (θˆ ) and send
j os,DP
∇FDP(θˆ ,θˆ ) := ∇F (θˆ )−∇F (θˆ )+b(4), (4.12)
j os,DP cq,DP j os,DP j cq,DP j
where b(4) ∼ N(0,s2I ), back to the central processor. Secondly, the central processor
j 4 p
computes DCQ estimators using the values of ∇FDP(θˆ ,θˆ ) and ∇FDP(θˆ )+
j os,DP cq,DP j cq,DP
∇FDP(θˆ ,θˆ ) , for the convenience, we denote them as ∇(cid:92) FDP(θˆ ,θˆ ) and
j os,DP cq,DP cq os,DP cq,DP
∇(cid:92) FDP(θˆ ) respectively. Thirdly, the central processor transmits them to the node
cq os,DP
machines, and the node machines use quasi-Newton method to update the previously com-
puted matrix {∇2F (θˆ )}−1. Concretely speaking, the standard BFGS quasi-Newton
j cq,DP
method achieves the inverse of Hessian matrix updating by computing
{∇(cid:91) 2F (θˆ )}−1 := (V(1))⊤{∇2F (θˆ )}−1V(1) +ρ(1)(θˆ −θˆ )⊗2, (4.13)
j os,DP j cq,DP os,DP cq,DP
where
ρ(1) = [(θˆ −θˆ )⊤{∇(cid:92) FDP(θˆ ,θˆ )}]−1
os,DP cq,DP cq os,DP cq,DP
and
V(1) = I −ρ(1)∇(cid:92) FDP(θˆ ,θˆ )(θˆ −θˆ )⊤.
p cq os,DP cq,DP os,DP cq,DP
Then, the node machine passes
h(2) := {∇(cid:91) 2F (θˆ )}−1∇(cid:92) FDP(θˆ )+b(5) with b(5) ∼ N(0,s2 I ) (4.14)
j j os,DP cq os,DP j j 5,j p
to the central processor, which computes the DCQ estimator H of h(2) and ultimately
2 j
computes the final estimator θˆ = θˆ −H .
qN,DP os,DP 2
It is worth mentioning that ρ(1) and V(1) are the same on different machines, and
17the only term in (4.13) that varies with machines is {∇2F (θˆ )}−1. Therefore, node
j cq,DP
machines only need to compute
h(3) := (V(1))⊤{∇2F (θˆ )}−1V(1)∇(cid:92) FDP(θˆ )+b(5), (4.15)
j j cq,DP cq os,DP j
and the central processor can compute the term ρ(1)(θˆ − θˆ )⊗2∇(cid:92) FDP(θˆ ).
os,DP cq,DP cq os,DP
Thesetwoalgorithmsaboveareessentiallythesame. Writeh(3) as(h(3),··· ,h(3))⊤. Similar
j j1 jp
√
to equation (4.10), the variance of nh(3) can be estimated by
jl
1 (cid:88) [(V(1))⊤{∇2F (θˆ )}−1∇2f(X ,θˆ ){∇2F (θˆ )}−1V(1)∇(cid:92) FDP(θˆ )]2
n l· 0 cq,DP i cq,DP 0 cq,DP cq os,DP
i∈I0
(cid:34) (cid:35)2
− 1 (cid:88) (V(1))⊤{∇2F (θˆ )}−1∇2f(X ,θˆ ){∇2F (θˆ )}−1V(1)∇(cid:92) FDP(θˆ )
n l· 0 cq,DP i cq,DP 0 cq,DP cq os,DP
i∈I0
+ns2 . (4.16)
5,0
ThecomputationproceduresinSubsection4.1.3canbesummarizedbelowinAlgorithm
1.
Theorem 4.3. Suppose Assumptions 7.1-7.4, 7.5 and 7.8-7.14 hold. When p2log2n/n =
o(1), log3n/m = o(1) and α = O(1/logn),
n
√
(cid:18) (cid:114) (cid:19)
α p p
∥θˆ −θ∗∥ = O √n + . (4.17)
qN,DP p
n mn
√
Moreover, if α = o (1/ pm) and p9mlog6n/n5 = o (1), we have that for any constant
n p p
vector v satisfying ∥v∥ = 1,
√
mn⟨θˆ −θ∗,v⟩
qN,DP d
→ N(0,1),
v⊤ΣK(θ∗)v
cq
where ΣK(θ∗) = {∇2F (θ∗)}−1V (θ∗){∇2F (θ∗)}−1, V (θ∗) is a p × p matrix with
cq µ g,vr µ g,vr
the (l ,l )-th entry (cid:80)K (cid:80)K (κl1,l2 −κ κ )σ (θ∗)σ (θ∗)/{(cid:80)K ψ(∆ )}2 and
1 2 k1=1 k2=1 k1,k2,K k1 k2 gr,l1 gr,l2 k=1 k
σ2 (θ∗) is the variance of ∇f (X,θ∗).
gr,l l
18Algorithm 1 Robust distributed quasi-Newton estimator with privacy protection.
Input: The data set {X ,X ,...,X } which is evenly distributed on m + 1 machines
1 2 N
{I ,I ,I ,...,I }withthelocalsamplesizenandthecentralprocessorI . Apositive
0 1 2 m 0
integer K used for DCQ estimator.
1: Each machine computes a local M-estimator θˆ , j ∈ [m] and sends it to I .
j,DP 0 0
2: (Initial estimator) I computes θˆ by (4.4). Then I transmits θˆ to each
0 cq,DP 0 cq,DP
machine I ,j ∈ [m] .
j 0
3: Each machine sends ∇FDP(θˆ ) to I .
j cq,DP 0
4: I computes ∇(cid:92) FDP(θˆ ) , and then sends ∇(cid:92) FDP(θˆ ) to the local machines.
0 cq cq,DP cq cq,DP
5: Every machine sends h(1) computed by (4.7) to I .
j 0
6: (One-stage estimator) I computes the DCQ estimator H based on h(1). Then I
0 1 j 0
updates the parameter estimator by θˆ = θˆ − H and sends it to the node
os,DP cq,DP 1
machines.
7: The node machines send ∇FDP(θˆ ,θˆ ) defined by (4.12) to I , and then I
j os,DP cq,DP 0 0
sends the corresponding DCQ estimator ∇(cid:92) FDP(θˆ ,θˆ ) back to the node ma-
cq os,DP cq,DP
chines.
8: (Quasi-Newton estimator) Each machine sends h(3) computed by (4.15) to I , and
j 0
I computes H , which is the DCQ estimator of h(2) defined in (4.14), and then derive
0 2 j
the estimator θˆ = θˆ −H .
qN,DP os,DP 2
Output: The final estimator θˆ .
qN,DP
Remark 4.3. The quantity κl1,l2 in Theorem 4.3 is defined as follows: let (ξ ,ξ ) follow
k1,k2,K l1 l2
a mean-zero bivariate normal distribution with Var(ξ ) = Var(ξ ) = 1 and Cov(ξ ,ξ ) =
l1 l2 l1 l2
σ l1,l2(θ∗) . Then κl1,l2 = P(ξ ≤ ∆ ,ξ ≤ ∆ ). It is easy to compute that when
σ l1(θ∗)σ l2(θ∗) k1,k2,K l1 k1 l2 k2
K ≥ 5, each diagonal element of V (θ∗) in Theorem 4.3 is smaller than 1.1 times that of
g,vr
E[{∇f(X,θ∗)}⊗2], soΣK(θ∗)isveryclosetoΣ(θ∗) = {∇2F (θ∗)}−1V (θ∗){∇2F (θ∗)}−1,
cq µ g,vr µ
which is the covariance matrix of the M-estimator.
4.2 Guarantee of differential privacy protection
We first introduce privacy protection methods for the mean of random variables that obey
sub-Gaussian (sub-exponential) distributions. In fact, we have the following two conclu-
sions:
Lemma 4.3. Assume the data set X = {X ,··· ,X } is made up of i.i.d. vectors sampled
1 n
from the mean-zero sub-Gaussian distribution with parameter ν, that is, for any t ∈ R and
19l ∈ [p], E{exp(tX )} ≤ exp(t2ν2/2), where X is the l-th entry of X. Let M(X) =
l l
√
1 (cid:80)n X . If we choose ∆ = 2γ plogn, then the Gaussian mechinism in Lemma 2.1 is
n i=1 i n
(ε,δ)-DP with probability as least 1−2pn−γ2/ν2.
Lemma 4.4. Assume the data set X = {X ,··· ,X } is made up of i.i.d. vectors sampled
1 n
from the mean-zero sub-exponential distribution with parameter (ν,α), that is, for any
−α−1 < t < α−1, E{exp(tX )} ≤ exp(t2ν2/2), where X is the l-th entry of X. Let
l l
√
M(X) = 1 (cid:80)n X . If we choose ∆ = 2γ plogn , then the Gaussian mechinism in Lemma
n i=1 i n
2.1 is (ε,δ)-DP with probability as least 1−2pmax{n−γ2(logn)/ν2,n−γ/α}.
By selecting suitable γ, Lemmas 4.3 and 4.4 can guarantee that (ε,δ)-DP holds with
a large probability. In practical problems, we can add additional trimming procedure to
the data set, so that the Gaussian mechanism can ensure (ε,δ)-differential privacy strictly.
However, how to trim data is not the focus of this paper, and we do not further discuss
specific details. According to the different distributions of random variables, the privacy
budget of the exponential distribution may be larger than that of the Gaussian distribution
√
with “ logn”, as shown in Lemmas 4.3 and 4.4.
By controlling the eigenvalues of the Hessian matrix, Lemmas 4.3 and 4.4 can be ex-
tended to general M-estimators to derive the following theorem.
Theorem4.4. SupposeAssumptions7.1-7.3and7.5hold. Letθˆ= argmin 1 (cid:80)n f(X ,θ)
θ∈Θ n i=1 i
and θˆ = θˆ+b with b ∼ N(0,s2I ). Define ∆ = (cid:112) 2log(1/δ)/ε. Then for any positive
DP p
√
constants γ and γ , when s = 2.02γ plogn∆/(λ n), θˆ is (ε,δ)-DP with the probability
0 s DP
at least 1−2pmax{n−γ2(logn)/ν g2,n−γ/αg}−o(n−γ0).
Remark 4.4. The sub-exponential Assumption 7.5 can be enhanced to sub-Gaussian As-
√
sumption7.4, whichcanreducethestandarddeviation“s”ofthenoisefrom2.02γ plogn∆/(λ n)
s
√
to 2.02γ plogn∆/(λ n). The details can be found in Lemma 19 in Supplementary Mate-
s
rials.
We have discussed the variance of Gaussian noise, including values from s to s .
1 5,j
Under the sub-exponential assumptions of the gradients and the Hessian matrices, we have
20the following Theorem 4.5. Similar to Theorem 4.4, when the gradients and the Hessian
matrices obey sub-Gaussian distribution, the variance of the Gaussian noise can be reduced
√
by a factor “ logn”. The details can be found in Lemma 39 in Supplementary Materials.
Theorem 4.5. Suppose Assumptions 7.1-7.3, 7.5 and 7.8 hold and j ∈/ B. Let ∆ =
(cid:112)
2log(1/δ)/ε. Then for any positive constant γ , we have the following conclusions:
0
√
(1) When s = 2.02γ plogn∆/(λ n), θˆ defined in (4.2) satisfies (ε,δ)-DP with
1 1 s j,DP
the probability at least 1−2pmax{n−γ 12(logn)/ν g2,n−γ1/αg}−o(n−γ0).
√
(2) When s = 2γ plogn∆/n, the gradient ∇FDP(θˆ ) defined in (4.6) achieves
2 2 j cq,DP
(ε,δ)-DP with the probability at least 1−2pmax{n−γ 22(logn)/ν g2,n−γ2/αg}−o(n−γ0).
(3) When s = 2.02γ √ plogn∥{∇2F (θˆ )}−1∇(cid:92) FDP(θˆ )∥∆/(λ n), h(1) defined
3,j 3 j cq,DP cq cq,DP s j
in (4.7) is (ε,δ)-DP with the probability at least 1−2pmax{n−γ 32(logn)/ν h2,n−γ3/α h}−o(n−γ0).
√
(4) When s = 2γ plogn∥θˆ − θˆ ∥∆/n, ∇FDP(θˆ ,θˆ ) defined in
4 4 os,DP cq,DP j os,DP cq,DP
(4.12) is (ε,δ)-DP with the probability at least 1−2pmax{n−γ 42(logn)/ν h2,n−γ4/α h}−o(n−γ0).
(5)Whens = 2.02γ √ plogn∥V(1){∇2F (θˆ )}−1∥∥{∇2F (θˆ )}−1V(1)∇(cid:92) FDP(θˆ )∥∆/n,
5,j 5 j cq,DP j cq,DP cq os,DP
h(3) definedin(4.15)is(ε,δ)-DPwithprobabilityatleast1−2pmax{n−γ 52(logn)/ν h2,n−γ5/α h}−
j
o(n−γ0).
Remark 4.5. Although we use the same notation in Lemma 4.5, ε and δ from (1) to (5) are
not necessarily equal. According to Dwork et al. (2006), the composition of k queries with
(ε ,δ )-differential privacy guarantees(i = 1,2,··· ,k), achieves at least
((cid:80)k
ε
,(cid:80)k
δ )-
i i i=1 i i=1 i
differential privacy. We can allocate the total privacy budget in five steps based on actual
issues. When all ε and δ are the same, the above Algorithm achieves (5ε,5δ)-differential
privacy with probability at least
p
total
= 1−2pmmax{n−γ 12(logn)/ν g2,n−γ1/αg}
−2pmmax{n−γ 22(logn)/ν g2,n−γ2/αg}−2pmmax{n−γ 32(logn)/ν h2,n−γ3/α h}
−2pmmax{n−γ 42(logn)/ν h2,n−γ4/α h}−2pmmax{n−γ 52(logn)/ν h2,n−γ5/α h}−o(mn−γ0).
By adjusting the variance of the Gaussian noise in Theorem 4.5, the constants γ to γ can
1 5
21be large enough, which can guarantee the probability p is close enough to 1. Moreover,
total
with probability at least p , the total privacy budget has a tighter upper bound according
total
to Corollary 4.1 with k = 5, which is mainly useful when ε is small.
According to Theorem 3.2 in Kairouz et al. (2015), we have the following corollary:
Corollary 4.1. Let s to s be defined as Theorem 4.5. For any ε > 0, δ ∈ [0,1] and
1 5,j
˜
any slack parameter δ ∈ [0,1], the class of (ε,δ)-differentially private mechanisms satisfy
(ε˜
,1−(1−δ)k(1−δ˜
))-differential privacy under k-fold adaptive composition, where
δ˜
  (eε −1)kε (cid:118) (cid:117) (cid:117) (cid:32) √ kε2(cid:33) (eε −1)kε (cid:115) (cid:18) 1(cid:19) 
ε˜ = min kε, +ε(cid:116)2klog e+ , +ε 2klog .
δ˜

eε +1 δ˜ eε +1 δ˜

Remark 4.6. In our research, we use the Gaussian mechanism instead of the Laplace mech-
anism to protect privacy. This choice is primarily due to the need to leverage the asymptotic
normality of the local estimator when applying the DCQ estimator. When the perturbation
term added for privacy protection follows a normal distribution, the local estimator, after
the addition of the perturbation term, also adheres to a normal distribution according to the
convolution formula. This approach is simpler than handling the combination of Laplace
and normal distributions.
4.3 The cases with unreliable central processor
In previous discussions, we always assumed that the central processor was not a Byzantine
machine and had its own sample data. However, in practical scenarios, the central proces-
sor may only perform computational tasks without storing data, or its data may exhibit
significant heterogeneity compared to that of other machines. In these situations, using the
samples from the central processor to estimate the variance of local estimators would be
unreliable, leading to a decrease in the effectiveness of the DCQ estimator. Since median
estimation does not require variance information, we can use the median instead of the
DCQ estimator for iteration.
22However, for crucial statistics such as gradients, which significantly affect the final
estimator, we prefer more precise estimation methods than the median. We may use
node machines to compute the variance used for the DCQ estimator, but we need to ensure
privacyprotectionwhentransmittingvariancestothecentralprocessor. Here, weshowhow
to calculate ∇(cid:92) FDP(θˆ ) as an example. The details of privacy protection for variances
cq cq,DP
used for DCQ estimator are provided in Theorem 4.6.
Theorem 4.6. Assume X ,··· ,X are i.i.d. random variables sampled from the mean-
1 n
zero sub-Gaussian distribution with parameter ν, that is, for any t ∈ R, E{exp(tX)} ≤
exp(t2ν2/2). Let M({X ,··· ,X }) = 1 (cid:80)n (X −1 (cid:80)n X )2. If we choose ∆ = 4γlogn+1
1 n n i=1 i n i=1 i n
with γ ≥ 1, then the Gaussian mechinism in Lemma 2.1 is (ε,δ)-DP with probability as
least 1−8n−γ/ν2.
Replace all the DCQ estimators in Algorithm 1 with the corresponding median estima-
tors except for ∇(cid:92) FDP(θˆ ). To achieve privacy protection for a p-dimensional vector, we
cq cq
need to replace (ε,δ) in Lemma 4.6 with (ε/p,δ/p). Before estimating the DCQ estimator
∇(cid:92) FDP(θˆ ), we need to estimate the variance of √ n∇FDP(θˆ ) by using the data on
cq cq 1 cq,DP
each node machine. Let the j-th node machine compute
1 (cid:88) 1 (cid:88)
σˆ2 := {∇f (X ,θˆ )− ∇f (X ,θˆ )}2,
g,jl n l i cq,DP n l i cq,DP
i∈Ij i∈Ij
√
andsendσˆ2 +b(6) tothecentralprocessor, whereb(6) ∼ N(0,s2)withs = 2γ p(4logn+
g,jl jl jl 6 6 6
(cid:112)
1) log(1.25p/δ)/(nε). The central processor computes σˆ2 = med{σˆ2 +s2,j ∈ [m]},
g,med,l g,jl 6
and this variance transmission achieves (ε,δ)-DP with probability at least 1 − 8pn−γ6/ν2.
Then ∇(cid:92) FDP(θˆ ) can be computed by (3.1).
cq cq
5 Numerical Studies
In this section, we construct some experiments on synthetic data and the MNIST real data
set to examine the finite sample performance of the proposed methods.
235.1 Synthetic Data
In this subsection, we perform logistic and Poisson regressions to evaluate the effectiveness
and robustness of the proposed method. Throughout the simulations, we set K in (3.1)
used for the DCQ estimator as 10 and design the proportion α of Byzantine machines
to be α = 0 and α = 10%, representing normal and Byzantine settings, respectively. In
Byzantine settings, we generate the Byzantine machines by scaling attacks based on −3
times normal value. That is to say, the statistics transmitted by the node machines to the
central processor are −3 times the correct statistics. To demonstrate the effectiveness of
the two iterations in our quasi-Newton algorithm, we compute the Mean of Root Squared
Errors (MRSE) of θˆ , θˆ , and θˆ separately over 100 simulations.
vr,DP os,DP qN,DP
In Byzantine settings, we generate the Byzantine machines by scaling attacks based on
−3 times normal value. That is to say, the statistics transmitted by the node machines to
the central processor are −3 times the correct statistics. To demonstrate the effectiveness
ofthe two iterationsinour quasi-Newtonalgorithm, we compute theMean ofRoot Squared
Errors (MRSE) of θˆ , θˆ and θˆ separately over 100 simulations.
vr,DP os,DP qN,DP
Experiment 1: Logistic regression model. Consider the following model:
exp(X⊤θ∗)
Y ∼ Bernoulli(p ) with p = , (5.1)
0 0 1+exp(X⊤θ∗)
where Y ∈ {0,1} is a binary response variable, X ∈ Rp follows the multivariate normal
distribution N(0,Σ ) with Σ being a symmetric Toeplitz matrix with the (i,j)-entry
T T
(0.6)|i−j|, i,j ∈ [q], θ∗ ∈ Rp is the target parameter and θ∗ = p−1/2(1/2,1/2··· ,1/2)⊤.
We set the dimension of X to be p = 10 and p = 20 respectively. For some simple
computations and Monte Carlo estimates, we take γ = γ = γ = γ = γ = 2.
1 2 3 4 5
To illustrate the impact of privacy budget on the accuracy of the parameter estimator,
weconsiderthealgorithmswhichachieve(ε,0.05)-DP,whereεtakes4,6,8,10,12,14,16,18,
20,30,40 and 50 respectively. Since our algorithm needs to transfer 5 vectors totally, we
add Gaussian noise to achieve (ε/5,0.01)-DP for each vector.
24Figures 1 and 2 show the line graphs to show the variation of MRSE against ε, where
the total sample size is N = mn = 2000000 with the number of machines m to be 500
and 1000, respectively. The dash line, dot line, and dot-dash line correspond to three
estimators, θˆ , θˆ and θˆ , respectively. To better illustrate the impact of noise
vr,DP os,DP qN,DP
on parameter estimation accuracy, we draw a solid line which corresponds to the quasi-
Newton estimator without privacy protection. It can be seen that the MRSE of θˆ
vr,DP
is much larger than θˆ and θˆ , and the MRSE of θˆ is slightly smaller than
os,DP qN,DP qN,DP
θˆ . When there exists 10% Byzantine machines, the effect of two iterations in our
os,DP
quasi-Newton algorithm is more pronounced. When the privacy budget ε exceeds 20, the
MRSE curve begins to be flat and the MRSE curve of θˆ is very close to the black
qN,DP
line. Figures 1 and 2 show that taking ε as 20 to 30 may be a good choice that balances
privacy protection and estimation accuracy.
Figure 1: Logistic regression: ε varies from 4 to 50, δ = 0.05, p = 10, m = 500 or 1000,
n = 4000 or 2000, total sample size N = 2000000, Byzantine machine proportion α = 0 or
10%.
Next, we consider the variation of the MRSE against the number m of machines. Con-
sider the sample size n = 1,000 on each node machine and the number m of machines
varies from 500 to 5,000. The results for p = 10 and 20 are shown in Figure 3. The MRSE
gradually decreases with the increase of m, but slows down the decreasing rate significantly
25Figure 2: Logistic regression: ε varies from 4 to 50, δ = 0.05, p = 20, m = 500 or 1000,
n = 4000 or 2000, total sample size N = 2000000, Byzantine machine proportion α = 0 or
10%.
after m exceeds 2000.
Figure 3: Logistic regression: m varies from 500 to 5000, p = 10 or 20, n = 1000, Byzantine
machine proportion α = 0 or 10%, ε = 30, δ = 0.05.
Experiment 2: Poisson regression model. Consider the Poisson regression model:
Y ∼ Poisson(λ) where λ = exp(X⊤θ),
26where θ = p−1/2(1/2,1/2,...,1/2)⊤ is the target parameter, and X is a p-dimensional
independent vector generated by a truncated normal distribution. Concretely speaking, we
generate X , for i ∈ [N], from the normal distribution N(0,Σ ) with Σ = (0.6)|i−j|, and
i T T
regenerate X if |X⊤θ| > 1. In fact, more than 90% of X satisfy |X⊤θ∗| ≤ 1, so this
i i i i
distribution is close to the general normal distribution.
Figures 4 and 5 respectively show the variation of the MRSE associated with p = 10
and p = 20, where ε = 4,6,8,10,12,14,16,18,20,30,40,50 and δ = 0.05, and Figure 6
displays the variation of the MRSE against the number m of machines.
Figure 4: Poisson regression: ε varies from 4 to 50, δ = 0.05, p = 10, m = 500 or 1000,
n = 4000 or 2000, total sample size N = 2000000, Byzantine machine proportion α = 0 or
10%.
The following conclusions can be drawn from Figures 4 to 6. First, for fixed values
of (m,n) and (ε,δ), as the parameter dimension p and the Byzantine machine proportion
α increase, the effect of two rounds of iterations in our quasi-Newton algorithm becomes
more pronounced, with the first iteration (θˆ to θˆ ) showing a significantly greater
vr,DP os,DP
impact than the second iteration (θˆ to θˆ ). Additionally, when p and (m,n) are
os,DP qN,DP
fixed, the MRSE decreases significantly as ε increases, but when ε exceeds 30, the MRSE
closely approaches the quasi-Newton estimator without privacy protection (represented by
the solid line in Figure 5). Finally, when p, n, and (ε,δ) are fixed, the MRSE gradually
27Figure 5: Poisson regression: ε varies from 4 to 50, δ = 0.05, p = 20, m = 500 or 1000,
n = 4000 or 2000, total sample size N = 2000000, Byzantine machine proportion α = 0 or
10%.
Figure 6: Poisson regression: m varies from 500 to 5000, p = 10 or 20, n = 1000, Byzantine
machine proportion α = 0 or 10%, ε = 30, δ = 0.05.
decreases with an increase in m, although the rate of decrease slows when m exceeds 2,000.
5.2 A real data example
The data set is from the NIST (National Institute of Standards and Technology) database,
accessible from http://yann.lecun.com/exdb/mnist/. It contains 60,000 images and
28their labels, and each image is a monochrome depiction of a handwritten digit from 0 to
9, displayed against a black background with white digits, and the pixel values span from
0 to 255.
We focus on the digits 6, 8, and 9, which are recognized as difficult to differentiate.
Our objective is to train three logistic classifiers to distinguish between the digit pairs 6
and 8, 6 and 9, and 8 and 9, respectively. Initially, we exclude variables where 75% of
the observations are zero. We then standardize the samples and apply the Lasso-logistic
regression method described in Friedman et al. (2010) to refine the remaining variables.
Finally, we select 6, 5, and 8 significant variables for the classifiers of 8 and 9, 6 and 8, and
6 and 9, respectively.
Weselect11,760samplesforeachclassifierandevenlydistributethemacross10, 15, and
20 machines. For the cases with 10 and 15 machines, we assign 1 Byzantine machine, while
for 20 machines, we assign 2 Byzantine machines. The Byzantine machines send values to
the central processor that are 3 times the true values. First, we use the training dataset to
estimate the logistic regression parameters through the quasi-Newton algorithm, and then
we assess the estimation accuracy on the test dataset. Take γ = γ = γ = γ = γ = 0.5.
1 2 3 4 5
The prediction accuracy rates on the test dataset are presented in Table 1. To illustrate the
impact of privacy protection mechanisms on estimation performance, we also provide the
prediction accuracy on the test set without the distributed setting and privacy protection
in the “Global” column.
Table 1 shows that when ε exceeds 20, the prediction accuracy rate is very close to
the case where the total train dataset is used to estimate the parameters regardless of the
distributed nature of obtained data and privacy protection requirement. If the total sample
size remains unchanged, increasing the number of machines decreases prediction accuracy,
while the influence of Byzantine machines is less significant. Additionally, the larger the
number of variables used in the logistic model, the greater the required privacy budget. In
our example, the regression model for digits 6 and 9, which uses 5 variables, maintains high
prediction accuracy on the test set when ε is equal to 5. In contrast, the regression model
29Table 1: Prediction accuracy rates of “6”, “8” and “9” in MNIST using logistic regression.
In Byzantine cases, the number of Byzantine machines are respectively 1 for m = 10 and
15, and 2 for m = 20.
m=10, n=1176 m=15, n=784 m=20, n=588
8 and 9 Normal Byzantine Normal Byzantine Normal Byzantine Global
ε = 5 79.04% 78.06% 57.86% 68.16% 60.88% 64.84%
ε = 10 83.21% 82.84% 82.51% 82.84% 82.60% 80.33%
83.91%
ε = 20 83.70% 83.49% 83.74% 83.62% 83.65% 83.43%
ε = 30 83.87% 83.82% 83.89% 83.87% 83.77% 83.68%
6 and 9 Normal Byzantine Normal Byzantine Normal Byzantine
ε = 5 88.33% 88.08% 87.86% 88.12% 87.91% 87.60%
ε = 10 88.20% 88.26% 88.25% 88.21% 88.20% 88.23%
88.21%
ε = 20 88.30% 88.24% 88.17% 88.17% 88.19% 88.17%
ε = 30 88.28% 88.26% 88.15% 88.15% 88.18% 88.19%
6 and 8 Normal Byzantine Normal Byzantine Normal Byzantine
ε = 5 86.49% 85.76% 83.84% 82.04% 81.31% 78.51%
ε = 10 86.68% 86.84% 86.86% 86.57% 86.64% 86.76%
86.75%
ε = 20 86.73% 86.74% 86.72% 86.72% 86.73% 86.71%
ε = 30 86.72% 86.72% 86.67% 86.71% 86.70% 86.75%
for digits 8 and 9, which utilizes 8 variables, exhibits a decrease in prediction accuracy as
ε decreases from 20 to 10, with a pronounced decline occurring when ε is further reduced
to 5.
6 Discussions
Thisresearchexploresrobustestimationproceduresforproblemsinvolvingdistributeddata
and Byzantine machines while ensuring data privacy. Reducing the amount of information
transmission, and thereby lowering transmission costs, is a key concern. In the context
of privacy protection, minimizing information transmission is even more crucial, as pri-
vacy budgets increase linearly with repeated queries. Our estimation strategy employs the
quasi-Newton algorithm, which leverages Hessian matrix information to perform gradient
descent, thus reducing the number of information transmission rounds while maintaining
the same data transmission volume per iteration as the gradient descent method. Al-
though this study focuses on robust estimation using DCQ estimators, the proposed robust
quasi-Newton privacy-preserving algorithm is also applicable to other distributed robust
30estimation strategies, such as those proposed by Yin et al. (2018, 2019), Chen et al. (2017),
or Su and Xu (2019).
Several further research directions merit investigation. First, many studies, such as
Lee et al. (2017) and Battey et al. (2018), focus on distributed M-estimation with sparse
structures, and the proposed robust, private distributed quasi-Newton estimation method
can be applied to this problem. Additionally, exploring the combination of other privacy
protection mechanisms, such as f-differential privacy proposed by Dong et al. (2022), with
distributed quasi-Newton methods is a worthwhile endeavor. Moreover, many references
consider semi-supervised M-estimation, such as Azriel et al. (2021) and Song et al. (2024),
and it would be of great interest to extend our method to the semi-supervised distributed
computing problem. Finally, the privacy-preserving DCQ estimators used for iteration
leverage information from other individuals, resembling the concept of transfer learning.
Thus, extending our method to transfer learning with privacy-preserving DCQ estimators,
as in Li et al. (2022), is another promising research direction.
7 Assumptions
Assumption 7.1. (Parameter space) The parameter space Θ ⊂ Rp is a compact convex
set, and θ∗ is an interior point in Θ. The ℓ -radius D = max ∥θ −θ∗∥ is bounded.
2 θ∈Θ 2
Assumption 7.2. (Convexity) The loss function f(x,θ) is convex with respect to θ ∈ Θ
for all x.
Assumption 7.3. (Bounded eigenvalue for Hessian matrix) The loss function f(x,θ)
is twice differentiable, and there exists two positive constants Λ and Λ such that Λ ≤
s l s
Λ (∇2F (θ∗)) ≤ Λ (∇2F (θ∗)) ≤ Λ .
min µ max µ l
Assumption 7.4. (Sub-Gaussian for the gradient) There exists a positive constant v such
g
that for any t ∈ R, l ∈ [p] and θ ∈ B(θ∗,ζ),
(cid:18) (cid:19)
1
E(exp{t|∇f (X,θ∗)−∇F (θ∗)|}) ≤ exp v2t2 .
l µl 2 g
31Assumption 7.5. (Sub-exponential for the gradient) There exist two positive constants ξ
g
and v such that for any 0 < t < ξ−1, l ∈ [p] and θ ∈ B(θ∗,ζ),
g g
(cid:18) (cid:19)
1
E(exp{t|∇f (X,θ∗)−∇F (θ∗)|}) ≤ exp v2t2 .
l µl 2 g
Assumption 7.6. (Bounded eigenvalue) There exist two positive constants Λ′ and Λ′ such
s l
that
Λ′ ≤ ∥E[{∇f(X,θ∗)}⊗2]∥ ≤ Λ′.
s l
Assumption 7.7. (Sub-Gaussian for the Hessian matrix) There exists two positive con-
stants ξ and v such that for any t ∈ R and θ ∈ B(θ∗,ζ),
h h
(cid:18) (cid:19)
1
E(cid:0)
exp[ta⊤{∇2f(X,θ∗)−∇2F
(θ∗)}a](cid:1)
≤ exp v2t2 ,
µ 2 h
where a can be any constant vector satisfying ∥a∥ = 1.
Assumption 7.8. (Sub-exponential for the Hessian matrix) There exists two positive con-
stants ξ and v such that for any 0 < t < ξ−1 and θ ∈ B(θ∗,ζ),
h h h
(cid:18) (cid:19)
1
E(cid:0)
exp[ta⊤{∇2f(X,θ∗)−∇2F
(θ∗)}a](cid:1)
≤ exp v2t2 ,
µ 2 h
where a can be any constant vector satisfying ∥a∥ = 1.
Assumption 7.9. (Sub-exponential for the inner product of vectors in the Hessian matrix)
Let [∇2F (θ∗)]−1 be the l-th row of [∇2F (θ∗)]−1 and ∇2f(X,θ) be the l-th column of
µ l· µ ·l
∇2f(X,θ). There exists two positive constants t and δ such that for any l ,l ∈ [p], if
1 2
θ ∈ B(θ∗,ζ),
E(cid:2)
exp(t|⟨[∇2F (θ∗)]−1,∇2f(X,θ)
⟩|)(cid:3)
≤ 2.
µ l1· ·l2
Assumption 7.10. (Moment ratio restriction 1) For l ∈ [p], there exists a positive constant
32R such that
m1
E(cid:2)
|⟨[∇2F
(θ∗)]−1,∇f(X,θ∗)⟩|3(cid:3)
µ l· ≤ R .
E(cid:2)
⟨[∇2F
(θ∗)]−1,∇f(X,θ∗)⟩2(cid:3) m1
µ l·
Assumption 7.11. (Moment ratio restriction 2) For any l ∈ [p], there exist two positive
constants R and ζ such that for any θ ∈ B(θ∗,ζ),
m2
E[|∇f (X,θ)−∇F (θ)|3]
l µl
≤ R .
E[{∇f (X,θ)−∇F (θ)}2] m2
l µl
Assumption 7.12. (Moment ratio restriction 3) For any l ,l ∈ [p], there exist two positive
1 2
constants R and ζ such that for any θ ∈ B(θ∗,ζ)
m3
E[|∇2f (X,θ)−∇2F (θ)|3]
l1l2 µl1l2
≤ R .
E[{∇2f (X,θ)−∇2F (θ)}2] m3
l1l2 µl1l2
Assumption 7.13. (Sub-exponential for the gradient) There exist two positive constants
ξ and v such that for any 0 < t < ξ−1 and l ∈ [p],
0 0 0
(cid:20) (cid:26) t|∇f (X,θ)−∇f (X,θ∗)−∇F (θ)+∇F (θ∗)|(cid:27)(cid:21) (cid:18) 1 (cid:19)
E exp l l µl µl ≤ exp v2t2 .
∥θ −θ∗∥ 2 0
Assumption 7.14. (Smooth) There exist two positive constants C and ζ such that for
H
arbitrary θ ,θ ∈ B(θ∗,ζ) ⊂ Θ, ∥∇2F (θ )−∇2F (θ )∥ ≤ C ∥θ −θ ∥.
1 2 µ 1 µ 2 H 1 2
Assumption 7.15. For the distribution function G(·), let C and C′ be two positive con-
g g
stants. We suppose that
(1) G−1(1/2) = 0;
(2) For any x ,x , |G(x )−G(x )| ≤ C |x −x |;
1 2 1 2 g 1 2
(3) There exists a positive constant ζ < 1/2 such that for any 1/2−ζ < x < 1/2+ζ ,
0 0 0
|G−1(x )−G−1(x )| ≤ C′|x −x |.
1 2 g 1 2
Remark 7.1. Assumptions 7.1 is common in classical statistical analysis of M-estimators
(e.g., Van der Vaart (2000)). The convex assumption 7.2 can be found in Fan et al.
33(2023) and Chen et al. (2022). Assumption 7.3 ensures the strong local convexity of the
loss function, and similar assumptions can be found in Zhang et al. (2013), Jordan et al.
(2019), and Tu et al. (2021).
Assumption 7.4 (Assumption 7.5) requires each entry of the gradient to follow a sub-
Gaussian (sub-exponential) distribution, which is equivalent to that in Yin et al. (2019) and
Tu et al. (2021). The assumptions of Gaussian and exponential distribution have an impact
of “logn” on the variance of the Gaussian noise term in privacy protection. Assumption
7.6 requires that the covariance matrix of the gradient at the true value of the parameter has
bounded eigenvalues, which is a regularity condition on studying the asymptotic properties
of M-estimators (see, e.g., Tu et al. (2021)).
Assumptions 7.7 and 7.8 demand that the quadratic form of the second derivative of the
loss function at the true value of the parameter also obeys a sub-Gaussian (sub-exponential)
distribution. Similar to Assumption 7.6, under the sub-Gaussian assumption, the noise
variance can be reduced by “logn”, and we omit further discussion. Assumption 7.9 requires
that the inner product between the row vector about the inverse of the Hessian matrix and the
second partial derivative of the loss function follows a sub-exponential distribution. Since
the non-diagonal entries of the inverse of a Hessian matrix are small in many cases, this
assumption is not strong.
Assumptions 7.10-7.12 necessitate the third-order moment of some random variables to
be controlled by a constant multiple of their second-order moment, a condition frequently
employed when utilizing the Berry-Esseen theorem to establish the asymptotic normality
(Tu et al. (2021)). When a random variable exhibits pseudo-independence or adheres to an
elliptical distribution (see, e.g., Cui et al. (2018)), its third-order moment can be controlled
by a constant multiple of its second-order moment. Assumptions 7.13 and 7.14 represent
two smoothness assumptions, analogous to Assumptions C and D in Tu et al. (2021).
Assumption 7.15 guarantee that G(·) is the distribution function of a centralized random
variable and satisfying some Lipschitz conditions. Many common limit distributions satisfy
Assumption 7.15 after centralization, such as the normal distribution and the chi-square
34distribution.
Throughout all assumptions, we do not explicitly assume the dimensions p of the pa-
rameter θ or q of X, with the requirements for p and q contained implicitly within the
functions relative to the loss function and its derivatives.
References
Awan, J.andVadhan, S.(2023). Canonicalnoisedistributionsandprivatehypothesistests.
The Annals of Statistics, 51(2):547–572.
Azriel, D., Brown, L. D., Sklar, M., Berk, R., Buja, A., and Zhao, L. (2021).
Semi-supervised linear regression. Journal of the American Statistical Association,
117(540):2238–2251.
Battey, H., Fan, J., Liu, H., Lu, J., and Zhu, Z. (2018). Distributed testing and estimation
under sparse high dimensional models. The Annals of Statistics, 46(3):1352–1382.
Cai, T., Xia, D., and Zha, M. (2024). Optimal differentially private pca and estimation for
spiked covariance matrices. arXiv preprint arXiv:2401.03820.
Chen, W., Wang, Z., and Zhou, J. (2014). Large-scale l-bfgs using mapreduce. Advances
in Neural Information Processing Systems, (27).
Chen, X., Liu, W., and Zhang, Y. (2022). First-order newton-type estimator for distributed
estimationandinference. Journal of the American Statistical Association,117(540):1858–
1874.
Chen, Y., Su, L., and Xu, J. (2017). Distributed statistical machine learning in adversarial
settings: Byzantine gradient descent. Proceedings of the ACM on Measurement and
Analysis of Computing Systems, 1(2):1–25.
Cui, H., Guo, W., and Zhong, W. (2018). Test for high-dimensional regression coefficients
using refitted cross-validation variance estimation. The Annals of Statistics, 46:958–988.
35Davidon, W. C. (1991). Variable metric method for minimization. SIAM Journal on
Optimization, 1:1–17.
Dong, J., Roth, A., and Su, W. (2022). Gaussian differential privacy. Journal of the Royal
Statistical Society Series B: Statistical Methodology, 84(1):3–37.
Dwork, C., McSherry, F., Nissim, K., and Smith, A. (2006). Calibrating noise to sensitivity
in private data analysis. In Theory of Cryptography: Third Theory of Cryptography
Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3, pages
265–284. Springer.
Dwork, C., Roth, A., et al. (2014). The algorithmic foundations of differential privacy.
Foundations and Trends® in Theoretical Computer Science, 9(3–4):211–407.
Eisen, M., Mokhtari, A., and Ribeiro, A. (2017). Decentralized quasi-newton methods.
IEEE Transactions on Signal Processing, 65(10):2613–2628.
Fan, J., Guo, Y., and Wang, K. (2023). Communication-efficient accurate statistical esti-
mation. Journal of the American Statistical Association, 118(542):1000–1010.
Fletcher, R. and Powell, M. (1963). A rapidly convergent descent method for minimization.
The Computer Journal, 6:163–168.
Friedman, J. H., Hastie, T., and Tibshirani, R. (2010). Regularization paths for generalized
linear models via coordinate descent. Journal of Statistical Software, 33(1):1–22.
Goldfarb, D. (1970). A family of variable-metric methods derived by variational means.
Mathematics of Computation, 24:23–26.
Hou, Z., Ma, W., and Wang, L. (2023). Sparse and debiased lasso estimation and infer-
ence for high-dimensional composite quantile regression with distributed data. TEST,
32(4):1230–1250.
Huang, C. and Huo, X. (2019). A distributed one-step estimator. Mathematical Program-
ming, 174:41–76.
36Jordan, M. I., Lee, J. D., and Yang, Y. (2019). Communication-efficient distributed statis-
tical inference. Journal of the American Statistical Association, 114:668–681.
Kairouz, P., Oh, S., and Viswanath, P. (2015). The composition theorem for differential
privacy. International Conference on Machine Learning, 37:1376–1385.
Lamport, L., Shostak, R., and Pease, M. (1982). The byzantine generals problem. ACM
Transactions on Programming Languages and Systems, 4:382–401.
Lee, J. D., Liu, Q., Sun, Y., and Taylor, J. E. (2017). Communication-efficient sparse
regression. The Journal of Machine Learning Research, 18(1):115–144.
Li,S.,Cai,T.T.,andLi,H.(2022). Transferlearningforhigh-dimensionallinearregression:
Prediction, estimation and minimax optimality. Journal of the Royal Statistical Society
Series B: Statistical Methodology, 84(1):149–173.
Lin, L., Li, F., Wang, K., and Zhu, L. (2019). Composite estimation. Statistica Sinica,
29(3):1367–1393.
Song, S., Lin, Y., and Zhou, Y. (2024). A general m-estimation theory in semi-supervised
framework. Journal of the American Statistical Association, 119(546):1065–1075.
Su, L. and Xu, J. (2019). Securing distributed gradient descent in high dimensional sta-
tistical learning. Proceedings of the ACM on Measurement and Analysis of Computing
Systems, 3:1–41.
Tu, J., Liu, W., Mao, X., and Chen, X. (2021). Variance reduced median-of-means es-
timator for Byzantine-robust distributed inference. The Journal of Machine Learning
Research, 22(84):3780–3846.
Van der Vaart, A. W. (2000). Asymptotic statistics, volume 3. Cambridge university press.
Wang, C., Zhu, X., and Zhu, L. (2023). Byzantine-robust distributed one-step estimation.
arXiv preprint arXiv:2307.07767.
37Wang, P., Lei, Y., Ying, Y., and Zhou, D. (2024). Differentially private stochastic gradient
descent with low-noise. Neurocomputing, 585:127557.
Wang, R. and Wang, Q. (2023). A robust fusion-extraction procedure with summary
statistics in the presence of biased sources. Biometrika, 110(4):1023–1040.
Wu, S., Huang, D., and Wang, H. (2023). Quasi-newton updating for large-scale dis-
tributed learning. Journal of the Royal Statistical Society Series B: Statistical Method-
ology, 85(4):1326–1354.
Yin, D., Chen, Y., Ramchandran, K., and Bartlett, P. L. (2018). Byzantine-robust dis-
tributed learning: Towards optimal statistical rates. International Conference on Ma-
chine Learning, 80:5650–5659.
Yin, D., Chen, Y., Ramchandran, K., and Bartlett, P. L. (2019). Defending against sad-
dle point attack in byzantine-robust distributed learning. International Conference on
Machine Learning, 97:7074–7084.
Zhang, Y., Duchi, J.C., andWainwright, M.J.(2013). Communication-efficientalgorithms
for statistical optimization. The Journal of Machine Learning Research, 14:3321–3363.
Zhou, Y. and Tang, S. (2020). Differentially private distributed learning. INFORMS
Journal on Computing, 32(3):779–789.
Zou, H. and Yuan, M. (2008). Composite quantile regression and the oracle model selection
theory. The Annals of Statistics, 36:1108–1126.
38