Jamba-1.5:
Hybrid Transformer-Mamba Models at Scale
JambaTeam
Abstract
WepresentJamba-1.5,newinstruction-tunedlargelanguagemodelsbasedonour
Jamba architecture. Jamba is a hybrid Transformer-Mamba mixture of experts
architecture, providing high throughput and low memory usage across context
lengths, while retaining the same or better quality as Transformer models. We
release two model sizes: Jamba-1.5-Large, with 94B active parameters, and
Jamba-1.5-Mini, with 12B active parameters. Both models are fine-tuned for
a variety of conversational and instruction-following capabilties, and have an
effectivecontextlengthof256Ktokens,thelargestamongstopen-weightmodels.
Tosupportcost-effectiveinference,weintroduceExpertsInt8,anovelquantization
techniquethatallowsfittingJamba-1.5-Largeonamachinewith880GBGPUs
whenprocessing256K-tokencontextswithoutlossofquality. Whenevaluatedon
abatteryofacademicandchatbotbenchmarks,Jambamodelsachieveexcellent
results while providing high throughput and outperforming other open-weight
modelsonlong-contextbenchmarks. Themodelweightsforbothsizesarepublicly
available under the Jamba Open Model License and we release ExpertsInt8 as
opensource.
Models: https://huggingface.co/ai21labs
1 Introduction
ThispaperintroducesJamba-1.5,twonewlargelanguagemodelsbasedonourJambaarchitecture
[24],whichareavailableforpublicuse. Jamba-1.5-Miniisanupdatedandinstruction-tunedversion
ofourearlierJambarelease[24]. Likeitssmallersibling,Jamba-1.5-Largeisahybridarchitecture
thatmixesTransformer[36]andMamba[13]layers,withamixture-of-experts(MoE)module[8,34].
SincetheintroductionofJamba,similareffortshaveconfirmedthebenefitsofcombiningTransformer
andstate-space-modelsatascaleofupto8Bparameters[6,37]. Jamba-1.5-Largedemonstratesthe
benefitsofthisarchitectureatamuchlargerscale. Ithas94Bactiveparameters,outofatotalof398B
parameters. Evenatthislargesize,themodelcanfitonasinglemachinewith880GBGPUswhen
processingacontextof256Ktokens,thankstotheefficiencyoftheJambaarchitectureinadditionto
anovelquantizationtechniquewehavedeveloped,asdescribedinSection3.1.
BothJamba-1.5-MiniandJamba-1.5-Largeareinstruction-tunedmodels,havingundergonepost-
trainingtoprovidethemwithvariouscapabilities. Ourevaluationsacrossawiderangeofbenchmarks
showthattheyperformcomparablytomodelsattheirsize,whileofferingtheefficiencybenefitsof
theJambaarchitecture. Inparticular,Jamba-1.5modelsshineatlong-contextevaluations,making
themtheonlymodelswithaneffectivelengthof256KontheRULERbenchmark,whileoffering10x
reductioninKVcachememoryaswellassuperiorthroughputandlatency.
4202
guA
22
]LC.sc[
1v07521.8042:viXraWemaketheJamba-1.5modelsavailableundertheJambaOpenModelLicense:
https://www.ai21.com/licenses/jamba-open-model-license.
Themodelsarepubliclyavailable:
Jamba-1.5-Mini: https://huggingface.co/ai21labs/AI21-Jamba-1.5-Mini
Jamba-1.5-Large: https://huggingface.co/ai21labs/AI21-Jamba-1.5-Large
2 ModelArchitecture
Jamba-1.5-LargeisbasedonJamba[24],ourhybriddecoderarchitecturethatmixesTransformer
layers[36]withMambalayers[13],astate-spacemodel(SSM)[14,15],inadditiontoamixture-of-
experts(MoE)module[8,34]. See[24]foradetaileddescriptionofthisarchitecture.
DuringourworkonJamba[24],wefoundthatthecombinationofTransformer,Mamba,andMoE
elementsfacilitatesbalancingdesiderataofthroughput,memoryusage,andquality. Jamba-1.5-Large
demonstratesthisflexibilityatalargerscale.
Jamba-1.5-Large follows the same Jamba structure but with a larger capacity. It has 94B active
parametersand398Btotalparameters. Ithas9blocks,witheachblockhavingthefollowingspecs:
• l=8layersineachblock.
• a:m=1:7ratioofattention-to-Mambalayers. Thisratiowasfoundoptimalinourwork
onJamba[24]andsimilarratioswasalsoconfirmedassuccessfulinfollow-upwork[6,37].
• MoEisusedinsteadofasingleMLPeverye=2layers. Therearen=16experts,andwe
selectthetopK =2ateachtoken.
• Thehiddenstatedimensionalityis8192.
• Thenumberofattentionqueryheadsis64andthenumberofKVheadsis8.
Table1comparestheJamba-1.5modelstopubliclyavailablemodelsofsimilarsizes. Jamba-1.5-Mini
hasasimilarnumberofactiveparametersasMixtral8x7B,whileJamba-1.5-Large’sactiveparameter
countisbetweenLLaMA-3.1-70BandMistral-Large-2. Atthesametime,bothourJambamodels
haveamuchsmallerKVcachememoryusage(at256Ktokens)comparedtoallothermodels,with
roughlyanorderofmagnitudereductioncomparedtotheirrespectivecounterparts.
Withthesesettings,andourspecializedquantization(Section3.1),Jamba-1.5-Largecanbeservedon
asinglemachinewith880GBGPUswithcontextlengthsupto256Ktokens.
Availableparams Activeparams KVcache(256Kcontext,16bit)
Mistral 7.2B 7.2B 32GB
Mixtral8x7B 46.7B 12.9B 32GB
LLaMA-3.18B 8B 8B 32GB
Mixtral8x22B 141B 39B 56GB
Mistral-Large-2 123B 123B 88GB
LLaMA-3.170B 70B 70B 80GB
LLaMA-3.1405B 405B 405B 252GB
Jamba-1.5-Mini 52B 12B 4GB
Jamba-1.5-Large 398B 94B 9GB
Table1: ComparisonofJamba-1.5-Mini,Jamba-1.5-Largeandrecentopenmodelsintermsoftotal
availableparameters,activeparameters,andKVcachememoryonlongcontexts. Jamba-1.5-Mini
andJamba-1.5-LargeprovidesubstantialreductionsintheKVcachememoryrequirements.
2Forthisrelease,weexperimentedalsowithMamba-2[6],afasterandimprovedversionofMamba,
whichwasreportedtooutperformMambaandTransformersseparately. However,asFigure1shows,
wefoundthatinahybridarchitecture,theMamba-1-AttentioncombinationworksbetterthanMamba-
2-Attention, so we use Mamba-1 in Jamba-1.5-Large. (We also found the hybrid architecture to
outperformpureMamba-2.) WehypothesizethisisbecausesomeoftheadvantagesofMamba-2
overMamba-1,inparticulartheabilitytouseamuchlargerstatesize,arelesssignificantwhenwe
havefullattentionlayersinterleavedbetweentheMambalayers,astheycanpoolinformationfrom
theentirecontext.
(a)350M.
(b)1.3B.
Figure 1: Comparison of Mamba-1, Mamba-2, Mamba-1-Attention, and Mamba-2-Attention on
modelstrainedfor100Btokens. WhileMamba-2outperformsMamba-1withoutattention,thehybrid
Mamba-1-Attentionperformsbetter.
3 ServingConsiderationsandImprovements
WeshareafewinsightsandimprovementswehaveintroducedtoallowforefficientservingofJamba
modelsatalargescale.
3.1 ExpertsInt8Quantization
TosupportefficientservingofJamba-1.5-Large,wedevelopedanewquantizationtechnique,which
wedubExpertsInt8. Weobservethatover85%ofthemodelweightsareintheMoElayers,andover
90%areinMoEorMLPlayers. Wewishtoquantizetheseweightswhilestillenjoyingthebenefitsof
fastBF16kernels. Todoso,wequantizetheMoEandMLPweightstoINT8,savetheminINT8,and
dequnatizethembacktoBF16beforetheactualcomputation. Importantly,thedequantizationstep
happensdirectlyinsidethefused_moekernelinvLLM[18]. Inthisway,thedequantizationprocess
addsnegligibleoverhead,andevenleadstoimprovedlatencyoverBF16.1 Wehavecontributedour
modifiedfused_moekerneltovLLM.2
1Weattributethistothethekerneloperatingonrelativelysmallblocksofweightsandactivations,whichit
movesfromGPUHBMtoSRAMpriortoperformingthecomputations.Inourimplementation,theweights
movefromHBMtoSRAMwhentheyareinint8,soittakeslesstimeastheirmemoryfootprintiscutbyhalf.
2Pullrequesthere:https://github.com/vllm-project/vllm/pull/7415
3OurExpertsInt8methodhasseveraladvantages. First,itisfast;quantizationonlytakesafewseconds
atmodelloading. Second,unlikemostothertechniquesinvLLM,itdoesnotrelyoncalibration,
which can take hours or days and can be unstable. Third, we can still use BF16 to hold large
activations. Fourth,itisavailabletouseonA100GPUs,unlikeFP8,whichisonlyavailableonH100.
Finally,ourquantizationmatchesFP8inlatency,whilesurpassingotherquantizationtechniques,
withoutalossinquality.
Figure2comparesthelatencywithdifferentquantizationtechniquesusingJamba-1.5-Mini,Jamba-
1.5-Large, and two Mixtral models (8x78B and 8x22B). On H100 GPUs, ExpertsInt8 matches
the latency of FP8. On A100, where FP8 is unavailable, ExpertsInt8 is an attractive technique,
outperformingGPTQ[9]byalargemargin. TogetherwiththeadvtanagesofExpertsInt8explained
above,thismakesitanattractivequantizationtechniqueforservinglargeMoEmodels.
ExpertsInt8 FP8 GPTQ None ExpertsInt8 GPTQ None ExpertsInt8 FP8
15 50 20
40 15 10 30 10
20
5 5
10
0
0 0 10 20 30 0 0 10 20 30 40 50 60 70 0 10 20 30 40 50 60 70
Batch size Batch size Batch size
(a)Jamba,2xH100 (b)Jamba,2xA100 (c)Jamba-1.5-Large,8xH100
ExpertsInt8 FP8 None ExpertsInt8 FP8 None
6 6
5 5
4 4
3 3
2 2
1 1
0
0 10 20 30 0 0 10 20 30
Batch size Batch size
(d)Mixtral-8x7B,2xH100 (e)Mixtral-8x22B,8xH100
Figure2: Comparisonofdifferentquantizationtechniques,showingend-to-endlatencywith1024-
tokencontextand128-tokendecoding. ExpertsInt8performssimilartoFP8,whilebeingfastand
simpletoapplyandstillallowingBF16activations,aswellasapplicabletoA100GPUs,whereFP8
isunavailable.
3.2 ActivationLoss
Duringpre-training,wefoundthatcertainactivations,namelyoutputsofspecificexpertsaswellas
thetheoutputofthelastMambalayers,weregraduallyincreasinginmagnitudeforcertaininput
tokens, eventually reaching values as high as 4×106. Although we did not find this to hurt the
pre-trainingitself,whichwasdoneinBF16precision,themagnitudeoftheactivationscouldcause
numericalissuesduringinferenceassomequantizationlibrariessupportonlyFP16precisionfor
activations,whichhasamaximumrangeof64K.
Toalleviatetheseconcerns,weaddedan“ActivationLoss”term,proportionaltothemean-square
ofactivationsintheforwardpass,withaconfigurableαfactor,whichpenalizeslargeractivation
values. Wefoundviaexperimentationthatthisauxilarylosshasnoaffectonthetrainingevenwithα
valuesuptoatleast10−3. ForJamba-1.5-Large,weusedα=10−5whichwasenoughtoreducethe
activationstoanacceptablerange(2K-3Kmax). Moreover,addingthisauxilarylossreducedthe
activationsalmostinstantly,allowingittobeaddedonlytowardstheendofthetrainingwithoutany
affectontrainingspeedandquality.
Tovalidatethisapproach,weranourfullevaluationsuiteonthemodelusingFP16activationsand
obtainedthesameresultsastheBF16evaluationswithoutanynans/overflows.
4
)t/s( ycnetaL
)t/s(
ycnetaL
)t/s( ycnetaL
)t/s(
ycnetaL
)t/s( ycnetaL4 ThroughputandLatencyAnalysis
ThankstothehybridJambaarchitecture,ourJamba-1.5modelsprovideexcellentthroughputand
latency. Figures3and4showthisforJamba-1.5-MiniandJamba-1.5-Large,respectively. Asshown
inthefigures,ourmodelsobtainmuchbetterlatencyandthroughputthansimilarly-sizedmodels.
Theiradvantageshinesatlongcontexts,withsubstantialgaps. Importantly,Jamba-1.5-Largeruns
efficientlyevenatlongcontexts,wherethelargeLLaMA3-405Bcannotrunonthesamehardware.
Latency at different context lengths Throughput at different context lengths
Jamba 1.5 Mini Llama 3.1 8B Mixtral-8x7B Mistral Nemo 12B Jamba 1.5 Mini Llama 3.1 8B Mixtral-8x7B Mistral Nemo 12B
100 80
75 60
50
40
25
20
0
4096 8192 16384 32768 65536 131072 262144
0
0 50000 100000 150000 200000 250000
Total context Total context
(a)Jamba-1.5-Mini,end-to-endlatency. (b)Jamba-1.5-Mini,outputtokensthroughput.
Figure3: ComparisonofJamba-1.5-Miniwithothermodelsintermsoflatencyandthroughout. All
measurementsweredoneon2xA10080GBGPUs,withbatchsize1,andoutputlength512tokens.
Jamba-1.5-Miniexhibitsbetterlatency,especiallyatlargecontexts,withonlyaslightreductionin
outputtokensthroughput.
Latency at different context lengths Throughput at different context lengths
Jamba 1.5 Large Llama 3.1 70B Mistral Large 2 Llama 3.1 405B Jamba 1.5 Large Llama 3.1 70B Mistral Large 2 Llama 3.1 405B
200 50
40
150
30
100
20
50 10
0 0
4096 8192 16384 32768 65536 131072262144 0 50000 100000 150000 200000 250000
Total context Total context
(a)Jamba-1.5-Large,end-to-endlatency. (b)Jamba-1.5-Large,outputtokensthroughput.
Figure4: ComparisonofJamba-1.5-Largewithothermodelsintermsoflatencyandthroughout. All
measurementsweredoneon8xA10080GBGPUs,withbatchsize1,andoutputlength512tokens.
Jamba-1.5-Largeexhibitsbetterlatency,especiallyatlargecontexts,withonlyaslightreductionin
outputtokensthroughput. TheLLaMA-3.1-405Bresultstruncateat64Kbecausethemodelistoo
largetofitcontextlengthsgreaterthan≈100K tokenson880GBGPUs.
5 Training
5.1 TrainingInfrastructureandData
Jamba-1.5-LargewastrainedonNVIDIAH100GPUsusingourin-houseproprietaryframework,
whichincludesFSDP,tensorparallelism,sequenceparallelism,andexpertparallelism. Forthelatter
wehaveadaptedMegaBlocks[10].
5.2 TrainingStages
Themodelwastrainedinthreestages. Duringpre-training,itwasfirsttrainedonanin-housedataset
lastupdatedinMarch2024.Ourpre-trainingdatasetisamixtureofpubliclyavailablewebdocuments,
code,booksandscientificarticles. Ourpre-processingpipelineincludesparsing,qualityfilters,and
deduplication. Tomakethebestuseofpubliclyavailabledata,wedevelopedourownin-houseparser,
andusedittoextracttextandformatting. Theexactdatamixturewasdeterminedthroughvarious
5
)t/s(
ycnetaL
)t/s(
ycnetaL
)s/t(
tuphguorhT
)s/t(
tuphguorhTablations. Thisstageincludedmultilingualdatawithemphasisonthefollowinglanguages: English,
Spanish,French,Portueguse,Italian,Dutch,German,Arabic,andHebrew. Itwasthentrainedfora
shortphaseofmid-trainingwithahighproportionoflongdocumentstoemphasizeitslong-range
capabilities. Finally,themodelwentthroughpost-training,describedinthenextsection.
5.3 Post-training
Ourapproachtopost-trainingaimstoachievetwoobjectivessimultaneously: (i)providethemodel
with various skills and conversational capabilities; (ii) retain capabilities from pre-training and
especiallythelong-contextcapabilitiesfrommid-training. Thesetwoobjectivesarepartlyconflicting,
sincemostoftheavailablepost-trainingdatasetsconsistofrelativelyshortexamples.
Giventheseconsiderations, ourpost-trainingprocessinvolvessupervisedfine-tuning[32,39]on
high-qualityconversationaldata,skill-specificdata,andlong-contextdata. Mixingthesedifferent
typesofdataaimstoretainlong-contextcapabilitiesandacquiredesiredskills. Asshowninthe
evaluationsbelow,wefindthatourmodelsperformverywellinlong-contextevaluations.
Whenperformingsupervisedfine-tuning, wemakeheavyuseofsyntheticdata, asiscommonin
recentfoundationmodels[7]andreflectingourapproachforconstructingstructureddataforbuilding
compound AI systems [20]. We developed multiple different data synthesis pipelines, targeting
differentmodelcapabilities. Allpipelinesapplythefollowingpattern:(i)Sampleorgenerateprompts
inatargetdistribution;(ii)Generateresponsesfromlanguagemodels;(iii)Filterorrankresponses
byqualityaccordingtoautomaticvalidationandscoring;and(iv)Post-edittoremoveartifactsandfit
desiredformatting. Weusedifferentmodels,prompting,sampling,filteringandeditingfordifferent
datapipelinesthatcomposethefinaldatamixes.
Wepickedourfinaltrainingrecipes(datamixandhyperparameters)basedonabatteryofmostly
internalautomaticmetrics. BothJamba-1.5modelsarefine-tunedwiththesamecontroltokensand
formattingtemplate,whichweprovideasapartofourreleaseasaHF-compatibletokenizerandchat
template;seethemodelcardfordetails.
Wegiveseveralnotableexamplesofsyntheticdatageneration:
Table-basedQA. Wegeneratetabulardataandaccompanyingquestion-answerpairs,asdemon-
stratedinourworkontableunderstanding[20]. Wethenconvertthetablesintonaturallanguage
paragraphsusingalanguagemodel. Ourgeneratedtrainingexamplesincludeextraction,aggregation,
andattributiontasksvis-a-vistextcorrespondingtospecificrowsorcolumnsinagiventable.
DocumentQA. Givenadocument,wepromptalanguagemodeltogeneratequestion-answerpairs,
forbothsingleandmultipleparagraphs. Wesometimesembedtheseexampleswithinlongercontext
byaddingsimilartexts,toencouragelong-contextunderstandingwithattribution.
Tooluse. Weusetheopen-sourceGlaivefunction-callingdataset[1]asastartingpoint,filtered
withvariousheuristicsandvalidationsontheoutputschemas. Tosupportparallelfunctioncalling,
wefirstgeneratemultiplevalidparameterassignmentsforeachfunctioninGlaive. Next,wesample
subsetsofthesevalidparameterassignments,forthesamefunctionandacrossdifferentfunctions,to
generateuserrequestscorrespondingtothesetoffunctioncalls.Finally,wepromptafunction-calling
languagemodeltorespondtothesegenerateduserrequestsandretaineonlyresponseswherethe
functioncallsmatchedtheoriginalparameterassignments.
Steerability. Wedefinedasetofinstructionsthatcanbeeasilyvalidatedandsynthesizedprompts
thatincludeagenericdocument-draftingtaskwithoneormoreconstraintsaddedtoit. Wegenerated
completionsforthesepromptsfromalanguagemodelandusedrejectionsamplingbasedonthevali-
dationsofourfine-grainedinstructionsplusageneral-purposerewardmodel. Tosupportinstructions
insystemmessages, wechosemultiplepromptsofthiskindthatshareafine-grainedinstruction
instanceandreformattedthesepromptsintoamulti-turnconversation,withtheinstructionmovedto
thesystemmessage.
5.4 SomeObservations
WeshareafewobservationsfromthedevelopmentofJamba-1.5. Whilethesearenotfullyexplored,
wehopetheywouldinspirethecommunitytolookfurtherintotheseissues.
6First,whileweincludedonlyaverysmallfractionofnon-englishdata,forafewlanguagesandonly
forspecificskillsinthepost-trainingphase,ourJamba-1.5modelsperformquitewellinmultiple
languages. Wedidincludemultilingualdatainthepre-trainingphase,asmentionedabove. Thus
we speculate that the models are able to use the learned knowledge from that phase when being
post-trainedmostlyinEnglish.
Second,ourefficientJambaarchitecturelowersthecostoffine-tuningonlongcontexts,allowingus
toexperimentmorewithagivenbudget. Thuswecouldexperimentwithmultipledifferenttraining
recipesatthepost-trainingstage.
Finally,whilepreferencetuningalgorithmslikePPO[33]orDPO[29]improvealignmentbetween
modeloutputsandhumanintent,wefoundthatthecombinationofcarefulsyntheticdatageneration,
datafiltering,andsupervisedfine-tuningiscrucialforobtainingastrongpost-trainedmodel.
6 Evaluation
Whilewebelievebenchmarksareonlypartlycorrelatedwithsuccessofrealapplicationsanduser
satisfaction, we report results on key public benchmarks. First, we report results on standard
academicbenchmarks. Then,weevaluatethemodelonchatbotbenchmarks. Finally,weevaluate
Jamba-1.5-Largeonseverallong-contextevaluationsandamultilingualevaluation.
Wecomparewithrecentopen-weightmodelsofthesamesizerange: LLaMA-3.170BandMistral-
Large-2-123B when comparing with Jamba-1.5-Large; LLaMA-3.1-8B and Gemma-2-9B when
comparingwithJamba-1.5-Mini.
6.1 AcademicBenchmarks
Wereportresultswithawiderangeofstandardacademicbenchmarks: MMLU[16],MMLU-Pro
[38], GPQA [31], ARC-Challence [5], BBH [35], and HumanEval [4]. We also evaluate on the
IFEvalinstructionfollowingdataset[42]andtheBFCLv1functioncallingdataset[40]. Finally,we
reportsafetyevaluationsonRealToxicity[12]andTruthfulQA[26].
Table2comparesJamba-1.5-Largetoseveralpubliclyavailablemodelsatsimilarsizes. Allresults
areeithertakenfromofficialsourcesorevaluatedbyus,asindicatedinthetable.3 Weobservethatthe
Jamba-1.5modelsperformsimilarlytorecentstate-of-the-artpubliclyavailablemodelsonstandard
academicbenchmarks,includingknowledge,reasoning,instructionfollowingandfunctioncalling
capabilities. Wealsoobservesimilarsafetymetricsasthosereportedintheliterature. Wereferto
Section7formoreinformationaboutourgeneralapproachforsafetyandalignmentofmodels.
Importantly,theJamba-1.5modelsachievetheseresultswhileprovidingmuchbetterthroughputand
latency,asdiscussedabove.
Jamba-1.5 LLaMA-3.1 Gemma-2 Jamba-1.5 LLaMA-3.1 Mistral-L-2
Benchmark Metric Mini 8B 9B Large 70B 123B
MMLU 5-shot 69.7 69.4 71.3 80.0 83.6 82.5†
MMLUPro 5-shot 39.8 38.0⋄ 39.0⋄ 48.3 53.0⋄ 54.2†
GPQA 0-shot 32.3 27.0⋄ 36.0⋄ 36.9 36.0⋄ 40.7†
ARC-C 0-shot 85.7 83.4 68.4 93.0 94.8 65.0†
BBH 3-shot 53.4 51.0⋄ 60.0⋄ 65.5 69 70.8†
HumanEval pass@1 62.8 72.6 40.2 71.3 80.5 92
GSM8K 5-shot 75.8 75.2/83.7⋆ 68.6 87.0 71.5/94.2⋆ 91.0†
IFEval 0-shot 75.8 80.4 74.3 81.5 87.5 87.8†
BFCL 0-shot 80.7 76.1 -‡ 85.5 84.8 85.1†
RealToxicity avgtox 8.1 - 8.2 6.7 - -
TruthfulQA 0-shot 54.1 51.5† 50.2 58.3 60.7† 50.4†
Table2: Jamba-1.5modelsobtainsimilarperformancetosimilarlysizedmodelswhileenjoyinga
betterthroughputandlatency. † evaluationrunbyus. ⋄ reportedintheHuggingFaceOpenLLM
leaderboard. ‡Lackingfunctioncallingcapabilities. ⋆Strict/flexibleevaluation.
3Intwocaseswefailedtoobtaingoodresults:Mistral-Large-2failstoobtaingoodscoresonARC-Cdespite
multipleattempts.LLaMA-3.1modelsperformpoorlyonGSM8Kwiththestandardstrictevaluationmode,so
wealsoreportforthemaflexibleevaluation,whichallowshigherresults.
76.2 ChatBotEvaluations
In this section we evaluate the Jamba-1.5 models on two chatbot scenarios: Arena-Hard [22], a
setof500challenginguserqueriesthatusesGPT4-Turboasajudge,andWildBench[25],which
usesGPT4-Turboasajudgewithalengthbiasmitigation. AsTable3shows,Jamba-1.5models
obtainexcellentreusltsintheseevaluations,withJamba-1.5-LargesurpassingLLaMA-3.170B,but
somewhattrailingbehindMistral-Large-2123B,whichhasabout30%moreactiveparameters.
Jamba-1.5 LLaMA-3.1 Gemma-2 Jamba-1.5 LLaMA-3.1 Mistral-L-2
Benchmark Mini 8B 9B Large 70B 123B
Arena-Hard 46.1 21.3 43.2† 65.4 55.7 70.4
Wild-Bench 42.4 33.6† 42.7 48.5 49.8† 56.3†
Table3: ComparisonofJamba-1.5modelstosimilarlysizedmodelsonchatbotbenchmarks. Jamba-
1.5modelsobtainsimilarperformancewithbetterthroughputandlatency. †evaluationrunbyus.
6.3 Long-ContextEvaluations
Thereleasedmodelhandlescontextlengthsofupto256Ktokens. Inthissection,weevaluateiton
syntheticandnaturalisticbenchmarksthattestitslong-contextcapabilities.
6.3.1 RULER
We evaluate on the RULER benchmark, a set of 13 synthetic tasks aimed to assess long-context
capabilitiesoflanguagemodels. RULERincludes8variantsofneedle-in-a-haystackretrievaltasks
[17, 21, 27, 28], including multiple ‘needles’ [2]. It also has one variable tracking task where a
chainofvariablebindingsshouldbereturned,twoaggregationtaskswhereoneneedstoreturnthe
mostcommonwords,andtwoquestion-answeringtasks,whereparagraphscotraininganswersfrom
naturalisticdatasets[30,41]areinsertedintorandomparagraphstosimulatelongcontexts.
The results are shown in Table 4. Among all publicly available and proprietary models, Jamba-
1.5-MiniandJamba-1.5-Largearetheonlyoneswithaconfirmedeffectivelengthof256Ktokens.
Gemini-proreportsgoodresultsupto128KontheoriginalRULERpaper. However,wewereunable
toreproducetheseresultsdespitemucheffort. WeexaminedGemini-progenerationsandnoticed
themodeloftenfailstoanswerorgeneratesarefusal. SincetheofficialRULERresultsarefroma
previewversion,wehypothesizethatGemini-prohadsinceundergonethroughupdatesthathavehurt
itsperformacneonRULER.
Claimed Effective
Length Length 4k 8k 16k 32k 64k 128k 256k Avg.
Jamba-1.5-Large 256K 256K 96.7 96.6 96.4 96.0 95.4 95.1 93.9 95.7
Jamba-1.5-Mini 256K 256K 95.7 95.2 94.7 93.8 92.7 89.8 86.1 92.6
Gemini-1.5-pro 1M >128K 96.7 95.8 96 95.9 95.9 94.4 65.1† 91.4
GPT-4-1106-preview 128K 64K 96.6 96.3 95.2 93.2 87 81.2 - 91.6
LLaMA3.170B 128K 64K 96.5 95.8 95.4 94.8 88.4 66.6 - 89.6
Qwen272B 128K 32K 96.9 96.1 94.9 94.1 79.8 53.7 - 85.9
Command-R+ 128K 32K 95.6 95.2 94.2 92 84.3 63.1 - 87.4
LLaMA3.18B 128K 32K 95.5 93.8 91.6 87.4 84.7 77 - 88.3
Command-R 128K 32K 93.8 93.3 92.4 89.5 84.9 76 - 88.3
MistralLarge2 128K 32K 96.2 96.1 95.1 93 78.8 23.7 - 80.5
Mixtral8x22B 64K 32K 95.6 94.9 93.4 90.9 84.7 31.7 - 81.9
Yi34B 200K 32K 93.3 92.2 91.3 87.5 83.2 77.3 - 87.5
Phi3mini3.8B 128K 32K 92.2 91.5 90.7 87.5 80.6 66.7 - 84.8
Phi3medium14B 128K 32K 93.3 93.2 91.1 86.8 78.6 46.1 - 81.5
Mixtral8x7B 32K 32K 94.9 92.1 92.5 85.9 72.4 44.5 - 80.4
MistralNemo12B 128K 16K 87.8 87.2 87.7 69 46.8 19 - 66.2
DBRX 32K 8K 95.1 93.8 83.6 63.1 2.4 0 - 56.3
Table4: ComparisonofJamba-1.5modelswithotherpubliclyavailableandproprietarymodelson
theRULERbenchmark. ResultsforothermodelsarefromtheRULERGithub. †Evaluationrunby
us. Jamba-1.5modelsaretheonlyoneswithaconfirmedeffectivelengthof256Ktokens.
86.3.2 Infinite-BENCH
Nextweevaluateon∞BENCH, adatasetdesignedtoevaluatelong-contextabilitiesoflanguage
models,withanaveragelengthof100Ktokens. WefocusontwoEnglishtasksonunderstandinglong
novels: questionanswering(EN.QA)andmultiple-choicequestionanswering(EN.MC).AsTable5
shows,Jamba-1.5modelsperformverywellinthiscase,outperformingsimilarlysizedLLaMA-3.1
andMistral-Large-2models. (WedonotreportresultswithGemma-29Bduetoitsshortcontext
windowof8K.)
Jamba-1.5 LLaMA-3.1 Gemma-2 Jamba-1.5 LLaMA-3.1 Mistral-L-2
Benchmark Mini 8B 9B Large 70B 123B
EN.MC 76.9 65.1 - 80.4 78.2 36.9†
EN.QA 40.6 27.1 - 34.9 36.7 -
Table5: Jamba-1.5modelsoutperformsimilarlysizedLLaMA-3andMistral-Large-2modelsin
long-contextevaluations. †evaluationrunbyus.
6.4 Multilingualcapabilities
WeperformabasicevaluationofJamba-1.5abilitiesinnon-Englishlangauges. Inparticular,we
report results on the multilingual MMLU dataset [19] as distributed through the LM Evaluation
Harness[11]. Table6showstheresults,whereJamba-1.5-Miniperformssimilarlyorbetterthanits
pointsofcomparison. Jamba-1.5-Largeisslightlybehinditscomparablemodels,butstillexhibits
goodmultilingualcapabilities.
Spanish Portuguese French German Arabic Italian Dutch Avg
Jamba-1.5-Mini 66.3 66.7 65.9 63.8 57.3 65.1 65.0 64.30
LLaMA-3.1-8B 59.5 59.1 59.5 57.2 46.9 58.4 57.2 56.83
Gemma-9B 66.0 59.9 66.7 64.3 55.9 65.8 64.8 63.34
Jamba-1.5-Large 75.5 75.5 75.8 73.9 67.1 75.2 74.6 73.94
LLaMA-3.1-70B 79.5 79.4 79.1 78.4 70.4 79.1 78.4 77.76
Mistral-Large-2 78.7 78.4 78.4 77.4 65.9 78.3 76.2 76.19
Table6: ComparisonofJamba-1.5withothermodelsonthemultilingualMMLUdataset.
7 AlignmentandSafetyConsiderations
Ourapproachtoalignmentofourmodelsisdrivenbycreatingtransparencybetweenmodelbehavior
andcustomerexpectations. Ourmodelsdefaulttoabusinesscodeofconductbasedonourpartici-
pationinindustrystandardsbodies,thinktanksanddirectexperiencewithourcustomers. Wesee
thisasanongoingandevolvingcollaboration. Inaddition,companieshavemultiplewaystocontrol
model conduct to reflect their individual values and cultures such as additional training and fine
tuning,systemmessagesandpromptengineering. Overall,ourAIcodeofconductisbasedonthe
followingobjectives:
• Alignmodelbehaviorandoutputwithcompanyvaluesandnormativebusinessdecorum.
• Clearlystatetenetsofintendedbehaviorsuchthaterrors/bugsareeasilydiscerned.
• CollaboratewithCustomersandmapbehaviortotheirbestpractices.
• Continuouslygatherfeedbacktomonitorandactivelyimprovebehavior.
InlinewithourroleinanOECDtaskforcetodevelopamonitoringmechanismforapplyingtheG7
HiroshimaCodeofConductforOrganisationsDevelopingAdvancedAISystems,wehaveorganized
ourmodelalignmentworkwiththeOECDvalues-basedAIprinciples:4 inclusivegrowth,sustainable
developmentandwell-being;human-centeredvaluesandfairness;transparencyandexplainability;
robustness,securityandsafety;andaccountability.
4https://oecd.ai/en/ai-principles
9Foreachofthefirstfourprincipleswehavedetailedbehavioralexpectationsortenetsandexamples
thatcanbeusedtotrain/alignandtestforcompliance. Theprincipleofaccountabilityisfocusedon
AI21’sroleintakingresponsibilityforthebehaviorofthemodels. Wesubmitthatthisaccountability
is demonstrated primarily through transparency and engagement with customers, regulators and
independent3rd-parties. OurengagementwithOECD,Stanford’sHELM[23]andFMTI[3]and
documentslikethisdemonstratethiscommitment,aswellasourhighrankingontheFMTI(2ndas
ofMay2024).
In total, we have created 60 tenets that map to the OECD principles. These tenets are stated as
directivesofbehaviorforourmodelstoavoid. Thefulllistwillbemadepubliclyavailable.
8 Conclusion
We have presented Jamba-1.5-Large and Jamba-1.5-Mini, two new large-scale models based on
theJambahybridTransformer-Mambaarchitecture. Bothmodelsachieveexcellentperformancein
academicbenchmarks,chatbotevaluations,andlong-contextevaluations,whileofferingimproved
latencyandthroughput,especiallyforlongcontexts. Wereleasethemodelweightsforusebythe
communityinhopesthatothersbuildonthistechnology.
10Contributions
Pre-andPost-Training Data
AlanArazi BenAviram
BarakLenz* DorZimberg
ChenAlmagor IdoBlass
DanPadnos* OhadLeshno
DanielGissin* RomGilad
DanielJannai TomBenGal
DorMuhlgay
EddenMGerber
Evaluation
ErezSafahi
ClaraFridman
GalCohen
JulieFadlon
GalShachaf
MariaRozman
HofitBata
NaamaGidron
InbalMagar
Ro’iBelson
ItayDalmedigos
TalNess
JhonathanOsin*
MatanDanos
MichaelGokhman Project&ProductManagement
NirRatner OrDagan*
NoamGat RoiCohen
NoamRozen ShakedMeirom*
OmerAntverg TalDelbari
OmriAbend YoavShoham
OpherLieber*
OritCohavi
RazAlon
ShakedMeirom
TomBraude
UriyaPumerantz
YonatanBelinkov
YuvalGloberson
YuvalPelegLevy
Serving&Infrastructure
AmirBergman
AvshalomManevich
BarakPeleg
EladDolev
EranKrakovsky
ErezSchwartz
HaimRozenblum
MorZusman
OdedFried
RomanGlozman
ShaharLev
TomerAsida
YehoshuaCohen
*Projectleads
11References
[1] GlaiveAI. Glaivefunctioncallingv2. https://huggingface.co/datasets/glaiveai/
glaive-function-calling-v2.
[2] SimranArora,SabriEyuboglu,AmanTimalsina,IsysJohnson,MichaelPoli,JamesZou,Atri
Rudra,andChristopherRe. Zoology: Measuringandimprovingrecallinefficientlanguage
models. InTheTwelfthInternationalConferenceonLearningRepresentations.
[3] Rishi Bommasani, Kevin Klyman, Sayash Kapoor, Shayne Longpre, Betty Xiong, Nestor
Maslej,andPercyLiang. Thefoundationmodeltransparencyindexv1.1: May2024. 2024.
[4] MarkChen,JerryTworek,HeewooJun,QimingYuan,HenriquePondedeOliveiraPinto,Jared
Kaplan,HarriEdwards,YuriBurda,NicholasJoseph,GregBrockman,etal. Evaluatinglarge
languagemodelstrainedoncode. arXivpreprintarXiv:2107.03374,2021.
[5] PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,AshishSabharwal,CarissaSchoenick,
andOyvindTafjord. Thinkyouhavesolvedquestionanswering? tryARC,theAI2reasoning
challenge. arXivpreprintarXiv:1803.05457,2018.
[6] TriDaoandAlbertGu. TransformersareSSMs: Generalizedmodelsandefficientalgorithms
throughstructuredstatespaceduality. ArXiv,abs/2405.21060,2024.
[7] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,
AieshaLetman,AkhilMathur,AlanSchelten,AmyYang,AngelaFan,etal. Thellama3herd
ofmodels. arXivpreprintarXiv:2407.21783,2024.
[8] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion
parametermodelswithsimpleandefficientsparsity. JournalofMachineLearningResearch,
23(120):1–39,2022.
[9] EliasFrantar,SalehAshkboos,TorstenHoefler,andDanAlistarh.OPTQ:Accuratequantization
forgenerativepre-trainedtransformers. InTheEleventhInternationalConferenceonLearning
Representations,2023.
[10] TrevorGale,DeepakNarayanan,CliffYoung,andMateiZaharia. MegaBlocks:EfficientSparse
TrainingwithMixture-of-Experts. ProceedingsofMachineLearningandSystems,5,2023.
[11] LeoGao,JonathanTow,BaberAbbasi,StellaBiderman,SidBlack,AnthonyDiPofi,Charles
Foster,LaurenceGolding,JeffreyHsu,AlainLeNoac’h,HaonanLi,KyleMcDonell,Niklas
Muennighoff,ChrisOciepa,JasonPhang,LariaReynolds,HaileySchoelkopf,AviyaSkowron,
LintangSutawika,EricTang,AnishThite,BenWang,KevinWang,andAndyZou.Aframework
forfew-shotlanguagemodelevaluation,072024.
[12] SamuelGehman,SuchinGururangan,MaartenSap,YejinChoi,andNoahASmith. Realtox-
icityprompts: Evaluating neural toxic degeneration in language models. In Findings of the
AssociationforComputationalLinguistics: EMNLP2020,pages3356–3369,2020.
[13] AlbertGuandTriDao. Mamba: Linear-timesequencemodelingwithselectivestatespaces.
arXivpreprintarXiv:2312.00752,2023.
[14] AlbertGu,KaranGoel,andChristopherRe.Efficientlymodelinglongsequenceswithstructured
statespaces. InInternationalConferenceonLearningRepresentations,2021.
[15] AlbertGu,IsysJohnson,KaranGoel,KhaledSaab,TriDao,AtriRudra,andChristopherRé.
Combiningrecurrent,convolutional,andcontinuous-timemodelswithlinearstatespacelayers.
Advancesinneuralinformationprocessingsystems,34:572–585,2021.
[16] DanHendrycks,CollinBurns,StevenBasart,AndyZou,MantasMazeika,DawnSong,and
Jacob Steinhardt. Measuring massive multitask language understanding. In International
ConferenceonLearningRepresentations,2020.
[17] Greg Kamradt. Needle in a haystack - pressure testing llms. https://github.com/
gkamradt/LLMTest_NeedleInAHaystack/,2023.
12[18] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu,
JosephE.Gonzalez,HaoZhang,andIonStoica. Efficientmemorymanagementforlargelan-
guagemodelservingwithpagedattention. InProceedingsoftheACMSIGOPS29thSymposium
onOperatingSystemsPrinciples,2023.
[19] VietLai, ChienNguyen, NghiaNgo, ThuatNguyen, FranckDernoncourt, RyanRossi, and
ThienNguyen. Okapi: Instruction-tunedlargelanguagemodelsinmultiplelanguageswith
reinforcement learning from human feedback. In Yansong Feng and Els Lefever, editors,
Proceedings of the 2023 Conference on Empirical Methods in Natural Language Process-
ing: System Demonstrations, pages 318–327, Singapore, December 2023. Association for
ComputationalLinguistics.
[20] BarakLenz,RazAlong,NoamRozen,OmriAbend,YonatanBelinkov,KevinLeyton-Brown,
and Yoav Shoham. Structured data as a key element of ai systems: A test case on table
understanding. InCompoundAISystemsWorkshop,2025.
[21] DachengLi,RulinShao,AnzeXie,YingSheng,LianminZheng,JosephGonzalez,IonStoica,
XuezheMa,andHaoZhang. Howlongcancontextlengthofopen-sourcellmstrulypromise?
InNeurIPS2023WorkshoponInstructionTuningandInstructionFollowing,2023.
[22] TianleLi*,Wei-LinChiang*,EvanFrick,LisaDunlap,BanghuaZhu,JosephE.Gonzalez,and
IonStoica. Fromlivedatatohigh-qualitybenchmarks: Thearena-hardpipeline,April2024.
[23] PercyLiang,RishiBommasani,TonyLee,DimitrisTsipras,DilaraSoylu,MichihiroYasunaga,
YianZhang, DeepakNarayanan, YuhuaiWu, AnanyaKumar, BenjaminNewman, Binhang
Yuan,BobbyYan,CeZhang,ChristianCosgrove,ChristopherD.Manning,ChristopherR’e,
Diana Acosta-Navas, Drew A. Hudson, E. Zelikman, Esin Durmus, Faisal Ladhak, Frieda
Rong,HongyuRen,HuaxiuYao,JueWang,KeshavSanthanam,LaurelJ.Orr,LuciaZheng,
MertYuksekgonul,MiracSuzgun,NathanS.Kim,NeelGuha,NiladriS.Chatterji,O.Khattab,
PeterHenderson,QianHuang,RyanChi,SangMichaelXie,ShibaniSanturkar,SuryaGanguli,
Tatsunori Hashimoto, Thomas F. Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang,
Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation of language
models. AnnalsoftheNewYorkAcademyofSciences,1525:140–146,2023.
[24] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez
Safahi, Shaked Haim Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz
Alon,TomerAsida,AmirBergman,RomanGlozman,MichaelGokhman,AvshalomManevich,
NirRatner,NoamRozen,ErezShwartz,MorZusman,andYoavShoham. Jamba: Ahybrid
transformer-mambalanguagemodel. ArXiv,abs/2403.19887,2024.
[25] Bill Yuchen Lin, Yuntian Deng, Khyathi Chandu, Faeze Brahman, Abhilasha Ravichander,
ValentinaPyatkin,NouhaDziri,RonanJosephLeBras,andYejinChoi. Wildbench: Bench-
marking llms with challenging tasks from real users in the wild. ArXiv, abs/2406.04770,
2024.
[26] StephanieLin, JacobHilton, andOwainEvans. Truthfulqa: Measuringhowmodelsmimic
humanfalsehoods. InProceedingsofthe60thAnnualMeetingoftheAssociationforComputa-
tionalLinguistics(Volume1: LongPapers),pages3214–3252,2022.
[27] NelsonF.Liu,KevinLin,JohnHewitt,AshwinParanjape,MicheleBevilacqua,FabioPetroni,
andPercyLiang. Lostinthemiddle: Howlanguagemodelsuselongcontexts. Transactionsof
theAssociationforComputationalLinguistics,12:157–173,2023.
[28] AmirkeivanMohtashamiandMartinJaggi. Random-accessinfinitecontextlengthfortrans-
formers. InThirty-seventhConferenceonNeuralInformationProcessingSystems,2023.
[29] RafaelRafailov,ArchitSharma,EricMitchell,ChristopherDManning,StefanoErmon,and
ChelseaFinn. Directpreferenceoptimization: Yourlanguagemodelissecretlyarewardmodel.
AdvancesinNeuralInformationProcessingSystems,36,2024.
[30] PranavRajpurkar,RobinJia,andPercyLiang. Knowwhatyoudon’tknow:Unanswerableques-
tionsforsquad.InProceedingsofthe56thAnnualMeetingoftheAssociationforComputational
Linguistics(Volume2: ShortPapers),pages784–789,2018.
13[31] DavidRein,BettyLiHou,AsaCooperStickland,JacksonPetty,RichardYuanzhePang,Julien
Dirani,JulianMichael,andSamuelRBowman. GPQA:Agraduate-levelGoogle-proofQ&A
benchmark. arXivpreprintarXiv:2311.12022,2023.
[32] VictorSanh,AlbertWebson,ColinRaffel,StephenBach,LintangSutawika,ZaidAlyafeai,
AntoineChaffin,ArnaudStiegler,ArunRaja,MananDey,etal. Multitaskpromptedtraining
enableszero-shottaskgeneralization. InInternationalConferenceonLearningRepresentations.
[33] JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. Proximal
policyoptimizationalgorithms. arXivpreprintarXiv:1707.06347,2017.
[34] NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,AndyDavis,QuocLe,GeoffreyHinton,
andJeffDean. Outrageouslylargeneuralnetworks: Thesparsely-gatedmixture-of-experts
layer. InInternationalConferenceonLearningRepresentations,2017.
[35] MiracSuzgun,NathanScales,NathanaelSchärli,SebastianGehrmann,YiTay,HyungWon
Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging BIG-
Benchtasksandwhetherchain-of-thoughtcansolvethem. InFindingsoftheAssociationfor
ComputationalLinguistics: ACL2023,pages13003–13051,2023.
[36] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,
ŁukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. Advancesinneuralinformation
processingsystems,30,2017.
[37] RogerWaleffe,WonminByeon,DuncanRiach,BrandonNorick,VijayAnandKorthikanti,Tri
Dao,AlbertGu,AliHatamizadeh,SudhakarSingh,DeepakNarayanan,GarvitKulshreshtha,
Vartika Singh, Jared Casper, Jan Kautz, Mohammad Shoeybi, and Bryan Catanzaro. An
empiricalstudyofmamba-basedlanguagemodels. ArXiv,abs/2406.07887,2024.
[38] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo,
WeimingRen,AaranArulraj,XuanHe,ZiyanJiang,etal. MMLU-Pro: Amorerobustand
challengingmulti-tasklanguageunderstandingbenchmark. arXivpreprintarXiv:2406.01574,
2024.
[39] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan
Du,AndrewMDai,andQuocVLe. Finetunedlanguagemodelsarezero-shotlearners. In
InternationalConferenceonLearningRepresentations.
[40] FanjiaYan,HuanzhiMao,CharlieCheng-JieJi,TianjunZhang,ShishirG.Patil,IonStoica,
andJosephE.Gonzalez. Berkeleyfunctioncallingleaderboard. 2024.
[41] ZhilinYang,PengQi,SaizhengZhang,YoshuaBengio,WilliamW.Cohen,RuslanSalakhut-
dinov,andChristopherD.Manning. Hotpotqa: Adatasetfordiverse,explainablemulti-hop
questionanswering. InConferenceonEmpiricalMethodsinNaturalLanguageProcessing,
2018.
[42] JeffreyZhou,TianjianLu,SwaroopMishra,SiddharthaBrahma,SujoyBasu,YiLuan,Denny
Zhou,andLeHou. Instruction-followingevaluationforlargelanguagemodels. arXivpreprint
arXiv:2311.07911,2023.
14