MuMA-ToM: Multi-modal Multi-Agent Theory of Mind
HaojunShi1*,SuyuYe1*,XinyuFang1,ChuanyangJin1,LaylaIsik1,Yen-LingKuo2,
TianminShu1
1JohnsHopkinsUniversity,2UniversityofVirginia
{hshi33,sye10,xfang21,cjin33,lisik,tianmin.shu}@jhu.edu,ylkuo@virginia.edu
Hamlin,Wynn,andBloom2007).Crucially,understanding
socialinteractionsgoesbeyondactionrecognition.Weoften
Abstract needtoreasonaboutwhypeopleinteractwithoneanotherin
acertainmanner.Wecanachievethisbyinferringpeople’s
Understandingpeople’ssocialinteractionsincomplexreal-
mentalstatesaswellashowtheyreasonaboutoneanother’s
world scenarios often relies on intricate mental reasoning.
mental states, i.e., multi-agent Theory of Mind (ToM) rea-
To truly understand how and why people interact with one
soning. For instance, if Alice puts away a book on Bob’s
another,wemustinfertheunderlyingmentalstatesthatgive
desk, she may be trying to clean up or hide the book, de- rise to the social interactions, i.e., Theory of Mind reason-
pending on both her social goal (helping or hindering) and
inginmulti-agentinteractions.Additionally,socialinterac-
where she believes Bob wants the book (belief of other’s
tionsareoftenmulti-modal–wecanwatchpeople’sactions,
goal). As an observer, it may be difficult to disambiguate
heartheirconversations,and/orreadabouttheirpastbehav-
betweenthesescenarios.However,ifwehadheardBobask-
iors.ForAIsystemstosuccessfullyandsafelyinteractwith
ingAlicewherethebookwasbefore,wewouldconfidently
peopleinreal-worldenvironments,theyalsoneedtounder-
infer that Alice wanted to hinder Bob. Such multi-modal,
standpeople’smentalstatesaswellastheirinferencesabout
multi-agentTheoryofMindabilitiesarenotonlycrucialfor
eachother’smentalstatesbasedonmulti-modalinformation
humansbutalsoforAIsystemsthataredeployedinhuman
abouttheirinteractions.Forthis,weintroduceMuMA-ToM,
livingenvironments,suchasassistiverobots.Withoutaro-
a Multi-modal Multi-Agent Theory of Mind benchmark.
bustunderstandingofpeople’smentalstatesincomplexso-
MuMA-ToMisthefirstmulti-modalTheoryofMindbench-
cialinteractions,AIsystemsmaycausedetrimentalerrorsin
mark that evaluates mental reasoning in embodied multi-
theirinteractionswithpeople.
agent interactions. In MuMA-ToM, we provide video and
textdescriptionsofpeople’smulti-modalbehaviorinrealis- Despitetherecentadvancesinevaluatingandengineering
tic household environments. Based on the context, we then machine Theory of Mind, prior works have not adequately
askquestionsaboutpeople’sgoals,beliefs,andbeliefsabout addressed the challenge of Theory of Mind reasoning in
others’ goals. We validated MuMA-ToM in a human ex- multi-modal social interactions. First, common Theory of
perimentandprovidedahumanbaseline.Wealsoproposed Mindbenchmarks(Gordon2016b;Gandhietal.2021;Shu
a novel multi-modal, multi-agent ToM model, LIMP (Lan- et al. 2021; Kosinski 2023; Jin et al. 2024) have only fo-
guage model-based Inverse Multi-agent Planning). Our ex- cusedonindividuals’mentalstates.However,thereareother
perimentalresultsshowthatLIMPsignificantlyoutperforms important aspects of multi-agent mental reasoning, includ-
state-of-the-art methods, including large multi-modal mod- ing social goals (e.g., helping, hindering) and beliefs about
els(e.g.,GPT-4o,Gemini-1.5Pro)andarecentmulti-modal others’goals.Second,therehasnotbeenamulti-modalso-
ToMmodel,BIP-ALM.1 cial interaction dataset designed for systematic Theory of
Mindreasoningevaluation.Theonlypriormulti-modalThe-
Introduction ory of Mind dataset is MMToM-QA, which solely focuses
onsingle-agentactivities.Text-onlybenchmarkssuchasHi-
Humans live in a social world; we not only engage in so-
TOM (Wu et al. 2023) feature multi-agent events, but lack
cialinteractionsourselvesbutcanalsounderstandotherpeo-
visual inputs. Thus, it remains unclear how we can evalu-
ple’ssocialinteractions.StudiesinDevelopmentalPsychol-
atethemulti-modalmulti-agentTheoryofMindcapacityin
ogy have indicated that the ability to understand different
machinelearningmodels.
kindsofsocialinteractionsdevelopsearlyandisoneofthe
To address these challenges, we introduce a new The-
basesformoresophisticatedsocialskillsdevelopedlaterin
oryofMindbenchmark,MuMA-ToM(Multi-modalMulti-
life(Denhametal.2003;Wellman,Cross,andWatson2001;
Agent Theory of Mind benchmark). MuMA-ToM includes
*denotesequalcontribution a large set of question-answering trials. As summarized in
1Codeanddataareavailableat:https://github.com/SCAI-JHU/ Figure1,questionsinMuMA-ToMareorganizedintothree
MuMA-ToM categories:(1)beliefinference,(2)socialgoalinference,and
4202
guA
22
]IA.sc[
1v47521.8042:viXraBelief Inference Social Goal Inference Belief of Goal Inference
wD ho e ry eo it su h ? k en bo ew er tcI h os efa f lw e ive i it nt o ga bn ro lt e oh i me n H tha ev e m y ao gu az s inee en ? caI bbt ei’ ns d ei rn t o it onh m e the The events in the text occur first, followed by the video.
Text: David walked to a book and grabbed it. He then walked to the living room,
headed to the bedroom, and finally reached the desk there, placing the book on the
desk.
A B John walks to the living room A B Kevin walks towards the cabinet
...
CJ tao bh ln e sees the beer on the coffee D J co oh ffn e eg r ta ab bs le the beer on the C Kevin opens the cabinet DK gre av bin b ic nl go s ae ns y c tha ib ni gnet without A Sarah walks to the desk B Sarah picks up the book CS roa ora mh walks to the living DS coar ffa eh e p tl aa bc le es the book on the
Question: If Mary has been trying to hinder John Question: If Jessica knows what is inside the Question: Which of the following statements is MOST likely?
from achieving his goal, when giving information, cabinet in the bedroom, which of the following is A) Sarah believed that David placed the book at his desired location: she moved the
where does she LEAST likely believe the beer was MOST likely? book to the coffee table to help David.
located? A) Jessica is trying to help Kevin B) Sarah believed that David wanted to place the book on the coffee table: she
A) Coffee table in the living room B) Jessica is trying to hinder Kevin intentionally moved the book to hinder David.
B) Kitchen cabinet C) Jessica is indifferent towards Kevin’s goals C) Sarah believed that David wanted to place the book on the coffee table: she
C) Fridge moved the book to help David.
Figure1:Examplequestionsforeachquestiontype.Weprovidekeyframesforthevideoineachexample.Theconversations
in the chat bubbles are provided as subtitles and shown as part of the multi-modal inputs when viewing the video. Note that
the captions on the bottom of the frames are for illustrative purposes only and are not shown in the videos. The checkmarks
indicatethecorrectanswers.Weprovidethevideosandtextfortheexamplesinthesupplementarymaterial.
(3) belief of goal inference. In each trial, there is a multi- theclassicSally-Annetest(Baron-Cohen,Leslie,andFrith
agent event in a household environment depicted by video 1985)forfalsebeliefsandhigher-orderbeliefs(Le,Boureau,
andtext.AsshowninFigure1,insometrials,textmayshow and Nickel 2019; He et al. 2023; Xu et al. 2024; Soubki
aconversationbetweentwoagents;inothertrials,textmay et al. 2024). There have also been multi-agent benchmarks
describeapartofaneventthatisnotdepictedinthevideo. that focus on a single agent’s beliefs & intentions in com-
Based on the multi-modal inputs, there will be a question plex conversations or interactions (Kim et al. 2023; Chen
aboutagents’mentalstates.Weevaluatedbothhumansand etal.2024a;Chanetal.2024;Sabouretal.2024).Inthese
state-of-the-artmulti-modalmodelsonMuMA-ToM.While benchmarks,otheragentsareusuallypresenttoaddcontext
humanscanachievenear-perfectperformance,baselinesall or complexity, but there are no questions about inter-agent
fail to robustly infer the mental states based on the multi- relationships.Priorworksontestingsocialrelationshipun-
modalcontext. derstanding(Netanyahuetal.2021a;Lietal.2024a)relyon
To bridge the gap between human ToM and machine simpleanimations,whichlacktherealismofembodiedhu-
ToM, we propose a novel multi-modal multi-agent Theory maninteractions.MostexistingToMbenchmarkshaveonly
of Mind method – LIMP (Language model-based Inverse eithertextorvideo.TheonlyexceptionisMMToM-QA(Jin
Multi-modal Planning). Inspired by a recent method, BIP- et al. 2024), which has single-agent activities depicted in
ALM,proposedby(Jinetal.2024),LIMPincorporateslan- video and text. Our MuMA-ToM benchmark features two
guage models as components for inverse planning. Unlike agents interacting in an embodied household environment,
BIP-ALM, LIMP (1) introduces multi-agent planning with withbothtextandvideoasmulti-modalinputs,andincludes
two-level reasoning, (2) eliminates the need for manually questionsthattesttheagents’socialintentionsandtheirrea-
definedsymbolicrepresentationsforabettergenerality,and soningabouteachother’smentalstates.
(3) can leverage any pretrained LLMs whereas BIP-ALM
requires LLMs finetuned on symbolic representations. Ex-
Multi-Modal Benchmarks. Given the recent advances
perimentalresultsdemonstratethatLIMPsignificantlyout-
in LLMs, there has been increasing interest in develop-
performsbaselines.
ing multi-modal QA benchmarks. Most of these bench-
In sum, our contribution includes (1) the first bench-
marks focus on models’ ability to fuse information from
mark on multi-modal multi-agent Theory of Mind reason-
multiple modalities, where answers are directly retriev-
ing, (2) a human experiment validating the benchmark and
able without complex reasoning (Li et al. 2023b; Sanders
providing a human baseline, (3) a systematic evaluation of
et al. 2023; Li et al. 2023a; Ying et al. 2024a; Tang et al.
state-of-the-art large multi-modal models (LMMs), and (4)
2024;Pandyaetal.2024).Arecentbenchmark,Perception
a novel multi-modal multi-agent ToM method combing in-
Test (Patraucean et al. 2024), evaluates physical reasoning
versemulti-agentplanningandlanguagemodels.
such as predicting world states and explaining counterfac-
tual facts. But it differs from ToM reasoning. Pipelines for
RelatedWorks
generating multi-modal datasets, SEED-story (Yang et al.
TheoryofMindBenchmarks. Single-agentToMbench- 2024)andTaskMeAnything(Zhangetal.2024),alsodonot
marks(Gordon2016b;Gandhietal.2021;Shuetal.2021; evaluate ToM reasoning. MMToM-QA (Jin et al. 2024), a
Kosinski2023;Jinetal.2024)haveextensivelytestedcon- recent multi-modal ToM benchmark, evaluates ToM with
cepts like belief, goal, preferences, constraints, and ratio- multi-modal inputs about single-agent behaviors. Unlike
nality. Multi-agent benchmarks are typically builtbased on MMToM-QA,ourbenchmarkincludesmulti-agentinterac-tions and evaluates models’ understanding of mental state variable of one of the agents. Among the three options, we
reasoninginmulti-modalsocialinteractions. makesurethatthereisalwaysoneoptionthatisclearlythe
mostlikelytobecorrect.
Machine Theory of Mind. Traditional approaches to
One of the challenges in designing these three types of
Theory of Mind reasoning fall into two categories: end-
questionsisthatgivenaninteraction,multiplecombinations
to-end training (Rabinowitz et al. 2018; Han and Gmy-
ofthesementalvariablescouldbeequallypossible.Forin-
trasiewicz2019)andBayesianInversePlanning(Bakeretal.
stance,ifweseethatAlice’sactionspreventBobfromreach-
2017; Zhi-Xuan et al. 2020; Stacy et al. 2024). There have
ing his goal, it could be because Alice is hindering Bob,
been works on neural amortized inference that combine
knowing Bob’s true intent; or she may try to help Bob but
these two methods for efficient and robust ToM inference
has a false belief of Bob’s goal and ends up accidentally
in visual domains (Jha et al. 2024; Puig et al. 2023). Re-
hinderingBob.Toaddressthechallengeoflargehypothesis
cently, LLMs demonstrated some ToM reasoning capabili-
space,wealwaysaskaquestionaboutamentalvariablecon-
ties(Kosinski2023;Bubecketal.2023),buttheirToMrea-
ditionedonexplicitlyprovidedassumptionsabouttheother
soning is still brittle (Verma, Bhambri, and Kambhampati
twomentalvariables.Forinstance,asshownintheexample
2024; Amirizaniani et al. 2024; Ullman 2023; Sclar et al.
questionofthebeliefinferenceinFigure1,thegoalofJohn
2023a; Ivanova et al. 2024). Approaches using prompt en-
can be inferred from his question about where the beer is
gineering have been proposed to enhance the ToM capac-
(thegoalisgettingbeer),andMaryshouldbeawareofthis
ities in LLMs for text-based QAs (Wilf et al. 2023; Sclar
assheansweredJohn’squestion;thesocialgoalofMaryis
et al. 2023b). Jin et al. (2024) proposed, BIP-ALM, for
unclear,thereforethequestionprovidesahypotheticalsocial
multi-modal ToM. BIP-ALM first extracts and fuses sym-
goal,hindering,asthecondition.Theremainingmentalvari-
bolicrepresentationsfrommulti-modalinputsandthencom-
ableofMary,herbeliefofthephysicalstatecanbeclearly
bines a language model and Bayesian inverse planning to
inferredgivenhersocialgoalandherbeliefofJohn’sgoal.
conduct ToM reasoning based on the symbolic representa-
Weexplainthedesignofeachquestiontypeasfollows.
tions. While achieving promising results on MMToM-QA,
Belief Inference. These questions focus on inferring a
BIP-ALMlacksmulti-agentreasoningcapacityandrequires
person’s belief about the physical state based on their ut-
finetuning a language model on hand-designed symbols.
terance and social goal. The person may have a true belief
Our LIMP model builds on BIP-ALM and introduces key
or false belief about the location of the object, which can
improvements including multi-agent planning and general,
be inferred when we constrain their social goal to be help-
domain-invariantrepresentations.
ingorhindering.IntheexampledepictedinFigure1,John
MuMA-ToMBenchmark asks Mary where he can find the beer. Mary suggests the
coffee table, which turns out to be the correct location, as
GeneralStructure
John successfully finds the beer there. This could be inter-
Thebenchmark consistsof225 multi-modalsocial interac- pretedintwoways:(1)MaryhelpsJohn,genuinelybeliev-
tionsbetweentwoagents.Thereare900multi-choiceques- ingthebeerisonthecoffeetable,or(2)Maryaccidentally
tions based on these social interactions. Each question de- helps John while intending to mislead him, mistakenly be-
pictsasocialinteractioninvideoandtextjointly.Asshown lievingthatthebeerisn’tonthecoffeetable.Toanswercor-
in Figure 1, the text may show a conversation between the rectly,amodelneedstounderstand:(1)MaryknowsJohn’s
agentsorapartoftheevent,andthevideoshowsthecom- goal (from their conversation), (2) John follows Mary’s di-
plementarypartoftheevent.Giventhemulti-modalinputs, rections (from their conversation and his actions afterward
the questions are designed to assess the understanding of in the video), and (3) John achieves his goal by following
agents’mentalstatesduringtheseinteractions,probingthree Mary’sdirections(asshowninthevideo).Webalancetrue
mainconcepts:(1)beliefs,(2)socialgoals,and(3)beliefsof andfalsebeliefsintheground-truthanswers.
others’goals.Eachconcepthas300questions.Wealsocre- SocialGoalInference.Inthesequestions,weaskabouta
atedatrainingsetconsistingof1,030videosannotatedwith person’ssocialgoal.Specifically,weconsiderhelping,hin-
theagents’actionsandgoals.Thetrainingsetdoesnotpro- dering, or acting independently as the three possible social
vide example questions. It is intended for a model to learn goalcategories,whicharealsothecommonsocialgoaltypes
abouttypicalmulti-agenthouseholdactivities. in physically grounded social interaction reasoning studied
by prior works in cognitive science (Hamlin, Wynn, and
QuestionTypes
Bloom2007;Ullmanetal.2009;Shuetal.2020;Malikand
As identified in prior works in cognitive science (Ullman Isik 2023). The example in Figure 1 shows an interaction
etal.2009;Shuetal.2020)andmulti-agentplanning(Gmy- similar tothe onein theexample forbelief inference ques-
trasiewicz and Doshi 2005; Tejwani et al. 2021), there are tions. In this particular example, Jessica misleads Kevin to
three mental variables that are crucial to ToM reasoning in the cabinet where there is no magazine inside. In the ques-
multi-agent interactions: an agent’s belief of the physical tion,weassumethatJessicadoesindeedknowthetruestate,
state, its social goal, and its belief of other agents’ goals. andtherefore,oneshouldinferthatJessicaistryingtohin-
Therefore,wedesignthreetypesofquestionsinourbench- derKevin.Toachievethiscorrectinference,amodelneeds
markcorrespondingtothethreementalvariables:beliefin- to focus on (1) how Jessica infers Kevin’s goal (from the
ference, social goal inference, and belief of goal inference. conversation), (2) how Kevin searches the room after the
Eachtypeofquestionasksaboutthecorrespondingmental conversation(fromboththeconversationandthevideofol-lowing the conversation), and (3) whether Kevin can find method proposed by Ying et al. (2024b)—Goal-Oriented
his goal object at the location suggested by Jessica (from Mental Alignment (GOMA)—-to generate action plans as
the video). We balance cooperative and adversarial behav- well as verbal communication. GOMA combines hierar-
iorsfortheground-truthanswers. chical planning, goal inference, and large language models
Belief of Goal Inference. Belief of goal inference asks (LLMs) to generate multi-modal interactions between em-
a model of how one person thinks about another person’s bodied agents. Prior work (Puig et al. 2020) has demon-
goalgiventhecontext.Ineachoptionforaquestionofthis stratedthatactivitiessynthesizedinVirtualHomeindeedre-
type,wealwayspairthebeliefofanotherperson’sgoalwith semblereal-worldhumanactivities.Weprovidemoredetails
the corresponding social goal to minimize ambiguity. For ontheproceduralgenerationinthesupplementarymaterial.
instance,intheinteractionfortheexamplequestionofbelief
of goal inference in Figure 1, Sarah moves the book to the OurModel
coffeetableafterDavidplacesitonthedesk.However,itis
Formulation
unclear whether Sarah is aware that David places the book
To model social interactions between two agents, i and j,
thereandwhetherSarahthinksthatDavidwantstokeepthe
andtherecursivementalreasoningbetweenthem,weadopt
book on the desk. If Sarah were trying to help David, as
an Interactive Partially Observable Markov Decision Pro-
assumedinthecorrectoption,shewouldhavebelievedthat
cesses (I-POMDP) formulation (Gmytrasiewicz and Doshi
David wanted the book on the coffee table instead. In this
2005). We define st as the state, at and at as agents’ ac-
case, as a third-person observer, we may not be certain of i j
David’s true intent, but we can still infer Sarah’s belief of tions, and ut and ut as agents’ utterances at time t. Each
i j
David’s goal given that her social goal is helping him. For agent maintains its own beliefs bt and bt, as well as goals
i j
thistype, halfof thequestions havea truebelief ofgoal as g andg .Tocapturerecursivereasoning,wedefineinterac-
i j
the correct answer, and the other half have a false belief of tivestatesfortheagents,denotedasis andis atlevelℓ.
i,ℓ j,ℓ
goalasthecorrectanswer. Fromtheperspectiveofagenti,itsinteractivestateateach
level is defined as follows (we consider the first two levels
Multi-modalInformation inthiswork):
In MMToM-QA, the only prior multi-modal ToM QA • Level0:is =s
i,0
benchmark,eachmodalitycoveredallaspectsofthehuman
• Level1:is = (s,b ,g )(whereb isadistribution
activityandenvironment,makingitdifficulttodiscernwhat i,1 j,0 j j,0
overagentj’slevel0interactivestate,is )
information could be extracted from one modality but not j,0
• ...
the other. Our benchmark aims to provide clearly separate
informationaccessibleonlythroughonemodality,allowing Giventhebeliefofinteractivestateb(is ),anagent’saction
i,1
ustounderstandpreciselyhowamodelneedstofusemulti- policywillbeπ(a |is ,g ),anditsutterancepolicywillbe
i i,1 i
modalinputstoanswereachquestion. π(u |is ,g ).
i i,1 i
As illustrated in Figure 1, there are two main ways in
whichmulti-modalinformationmustbeintegrated.First,if Overview
thereareconversationsbetweentwoagents,themodelmust PreviousworksonInverseMulti-agentPlanning(IMP)(Ull-
understand the exchanged information and how it impacts manetal.2009;Netanyahuetal.2021a)havedemonstrated
each person’s mental state, including any changes in their thatIMPcanrobustlyinferagents’mentalstatesinsocialin-
beliefs about each other. The model must also observe ac- teractions.However,thesemethodsrelyonmanuallycrafted
tionsandoutcomes,connectingthemtotheconversationto plannersandarelimitedtosimplevisualscenarios,suchas
reason further about mental states. Note that conversations 2D grid worlds. Jin et al. (2024) introduced the BIP-ALM
canoccuratanypointinthevideo.Second,forinteractions model, which leverages language models for inverse plan-
withoutverbalcommunication,weprovidepartoftheevent ning to achieve single-agent Theory of Mind reasoning in
in text and the remaining part in video. Specifically, we ei- complex, realistic settings. Inspired by BIP-ALM, we pro-
ther describe the first half in text and show the second part poseanovelmethod,Languagemodel-basedInverseMulti-
invideoorshowthefirstpartinvideoanddescribethesec- agentPlanning(LIMP),tocombineIMPandlanguagemod-
ondhalfintext.Thesetwodesignsarerandomlysampledto els for robust multi-agent Theory of Mind reasoning based
describeinteractionsjointlyinvideoandtext. onmulti-modalinputs.
As illustrated in Figure 2, LIMP consists of three key
ProceduralGeneration
components: multi-modal information fusion, hypothesis
We use a multi-agent household simulator, VirtualHome parsing, and inverse multi-agent planning. Compared to
(Puig et al. 2018a, 2020), to procedurally synthesize social BIP-ALM,ourapproachoffersseveralimprovements.First,
interactions between two agents. For each interaction, we whileBIP-ALMislimitedtosingle-agentscenarios,LIMP
sample an environment and goals for the agents. We con- identifies three mental variables crucial to understanding
siderthreegeneralsocialscenarios:anagentistryingtohelp multi-agent interactions—belief, social goal, and belief of
another agent, an agent is trying to hinder another agent, goal. We then implement multi-agent planning based on
and two agents are acting independently. Agents only have these variables to reason about multi-modal social interac-
partial observations and do not know each others’ goals. tions.Second,BIP-ALMreliesonhand-designedsymbolic
Theycanoptionallytalktoeachother.Weleveragearecent representationsandrequiresfinetuninglanguagemodelsonRGB Video Action
Detection Fused Information Prob. of
Action and Utterance Sequence Options
VLM Actions & Multimodal Kevin Sarah
Utterances Fusion ... Grabs milk from Inverse A) 0.7
kitchen counter Multi-Agent B) 0.1
Walks to the
Text Text Parsing LLM kitchen Puts milk in fridge Planning C) 0.2
“Where is the “The milk is in the
[...] Sarah takes the milk from Actions & milk?” fridge”
the kitchen counter and LLM Utterances Initial state:
moves it to the fridge [...] The milk is on the kitchen counter
Question
Mental Variable Hypotheses
Which of the following is MOST likely?
A) Sarah is trying to help Kevin LLM Hypothesis A: ..., help, ...
Hypothesis B: ..., hinder, ...
B) Sarah is trying to hinder Kevin
Hypothesis C: ..., indifferent, ...
C) Sarah is indifferent towards Kevin’s goals Hypothesis Parsing
Figure2:OverviewofLIMP.LIMPhasthreecomponents:(1)themulti-modalinformationfusionmoduleextractsandfuses
informationfromvisionandtext;(2)thehypothesisparsingmodulegenerateshypotheticalvaluesforthethreementalvariables
giventhequestionandthefusedinformation;and(3)theinversemulti-agentplanningmoduleassessestheprobabilitiesofeach
optiongiventhehypotheticalmentalvariablesandthemulti-modalagentbehaviordescribedinthefusedinformation.
RGB Videos Raw action Raw output: Fused Information ingthismethod,thereconstructedinitialstatewillonlycon-
extraction Jo eh nn t ea rn sd t hJ ee s ks itic ca h eb no .th Action & Utterance sequence siderobjectsrelevanttohumanactionsandutterances.This
John ask Jessica Kevin Sarah
VLM in jdja uu ib ici cco a ee tu e, it na s l s no t ih ddca a e Jt t e fi to rs h in s dei gco r ea ef . is “We hn ete rer k isit jc uh ice en ?” ie “ nIn sft o ie dur e nk fdi rt ic j duh gie c en e ” s ci um rap cl yifi oe fs tt hh ee ic no fen rt ee nx ct ea .n Gd ic va en nc tho ens ie nq itu iae ln stl ty ateim ap nr do tv he et ah ce tia oc n-
Then John walks to walk to fridge walk to table
f gr ri ad bg se , s o op me en s o bit j ea cn td . grab juice stand at the table sequences,wecaninferthestateateachstep.
Text Initial state:
Actions & Juice inside fridge There is often missing information in the visual percep-
jJ uo ich en ” , a as nk ds JJ ee ss ss ii cc aa , a “ nW sh we er re s i “s I Utterances tionresults.Forinstance,asshowninFigure3,theVLMdid
found juice inside fridge”. LLM LLM Fill missing information and
Then Jessica walks to table reconstruct initial state notrecognizetheobjectthepersongrabbedandproducedan
and stands there Text parsing Multimodal fusion
ambiguousaction–“grabssomeobject.”Thisisalsocom-
Figure3:Illustrationofthemulti-modalinformationfusion montopeople,astheobjectpickedupbythepersonisof-
inLIMP.Itfillsinmissinginformationbasedonthecontext ten occluded. However, we can still infer that the object is
andrecoverstheinitialstatefromagents’actions. likelyjuicebasedonthecontextprovidedinthetext.Toem-
ulatesuchability,weleverageanLLMtofuseinformation
these representations. In contrast, LIMP uses natural lan- extractedfromvideoandtext,whichinferstheinformation
guage to represent states, actions, and utterances, eliminat- missingfromvisualperceptionbasedonthecomplementary
ing the need for finetuning and enhancing generalizability informationdescribedinthetext.
across domains. Finally, LIMP’s multi-modal information
In this work, we use Gemini 1.5 Pro for the VLM and
fusion module can fill in missing information from visual
GPT-4ofortheLLMastheyproducethebestresults.
perception using contextual cues from text or action se-
quences(Figure3),acapabilityabsentinBIP-ALM.Wedis-
cussthedetailsofeachcomponentintheremainingsection.
HypothesisParsing
Multi-modalInformationFusion
To answer the question about a person’s mental state in a
We use a vision-language model (VLM) to extract the ac-
social interaction, LIMP will parse relevant hypotheses of
tions and utterances of each person depicted in the video.
allmentalvariablesofthatperson(agenti)–beliefofstate
Giventext,weuseanLLMtoextracttheactionsandutter-
b(s), social goal g , and belief of other agent’s goal b(g ).
ances of each person. We then fuse the extracted informa- i j
Forthis,wepromptGPT-4owiththeinitialstateandques-
tion to form the initial state and the complete sequences of
tion text to generate a reasonable hypothesis of the three
actionsandutterancesusinganLLMasfollows.
mentalvariablesforeachoption,H =⟨b(s),g ,b(g )⟩.
Unlike MMToM-QA, our benchmark does not provide i j
a text description of the full state, as such descriptions are
rarelyprovidedinreal-worldapplications.However,asob-
InverseMulti-AgentPlanning
jects may be occluded or too small to detect even for hu-
mans,inferringthestatedirectlyfromtheRGBvideoscould
bedifficult.Instead,wepromptanLLMwiththeinferredac- Given the fused information from multi-modal inputs and
tionsandutterancesofbothagentstoinferthepartoftheini- the parsed hypotheses, inverse multi-agent planning con-
tialstaterelevanttotheactivity.Forexample,ifAlicegrabs ducts Bayesian inference over a person’s mental state by
a carrot from the fridge, and moves it to the kitchen table, evaluating the likelihood of actions and utterances given
wecaninferthatthecarrotwasoriginallyinthefridge.Us- each hypothesis. Following the I-POMDP formulation, we100%
Time
Step 0 ....... Step t-1 Step t
Actions enter livingroom a ...0 j ..: .t .-1 close livingroom entera kt j itchen y c 80%
U Att ce oo tr f if a
o
jj n nc se entN ero kn ie
tchen
au . .. .. .00 ij. .. .:: . .tt . .-- 11
put
poc tNa ab o ti onn e ie nt
fridge
w“ aH la kv p te oo wy tu aao at ttj iu ro
d
s ? se ” te an
b
a
le
a r u c c
A
246 000 %%%
of i
Utterance None u ..0 .i ..: .t .-1 None “I foundu at i potato 0%
of i inside fridge”
Initial state Hypothesis of i IMP inference at t
milk onS 0 kitchen ..., hiH nder, ... Action LL of i π(a t i | a 0 i : t -,1 u 0 i : t -,1 a 0 j : t -,1 u 0 j : t -,1 s 0 , H ) = 0.99
table... Utterance LL of i π(u t i | a 0 i : t -,1 u 0 i :t -,1 a 0 j : t -,1 u 0 j : t -,1 s0 , H ) = 0.01
Figure5:HumanandmodelperformanceonMuMA-T oM.
Figure 4: Illustration for inverse multi-agent planning. We
estimate the action and utterance likelihood of agent i at
Method Belief Social Belief All
each step t given the past actions and utterances of both
Goal ofGoal
agentsfromstep0tostept−1,theinitialstates0,andthe
Human 98.9 94.4 87.1 93.5
hypothesisH.LLinthefigurestandsforlikelihood. Gemini1.5Flash 53.9 33.0 41.4 42.7
Gemini1.5Pro 78.9 43.9 46.9 56.4
definethisprobabilisticinferenceasfollows: Llava1.613B 70.2 43.2 17.9 43.7
Llava1.634B 93.6 37.2 27.5 52.8
P(H |a0:T,u0:T,a0:T,u0:T,s0) GPT-4o 67.9 39.6 44.4 50.6
i i j j
InternVL28B 62.2 44.6 45.1 50.6
(cid:89)T InternVL226B 59.3 44.9 35.5 46.6
∝P(H) π(at i |a0 i:t−1,u0 i:t−1,a0 j:t−1,u0 j:t−1,s0,H) VideoLlama27B 70.1 45.6 37.7 51.1
t=1 BIP-ALM 41.2 34.1 30.6 33.9
T LIMP 93.4 67.7 68.7 76.6
(cid:89)
· π(ut |a0:t−1,u0:t−1,a0:t−1,u0:t−1,s0,H), (1)
i i i j j Table1:Humanandmodelperformancefordifferentques-
t=1 tiontypesaswellasforallquestions.
where the action policy and the utterance policy can be es-
timated by the log probabilities of the prompt completion
the benchmark. Each question received responses from 3
by a language model for each time step t. Note that in the
participants. The experiment was approved by an institu-
standard policy definitions in I-POMDP, we need agent i’s
tionalreviewboard.
beliefofagentj’sbeliefofthestateateachstep.This,how-
ever,isdifficulttoexplicitlyestimate.Instead,inthiswork,
Baselines
weconsiderpastactionsandutterancesofallagentsaspart
of the condition of the policies to avoid the explicit belief Weevaluatedourbenchmarkonstate-of-the-artLMMs.For
of belief inference. We prompt an LLM with the hypothe- models capable of processing video input, the entire video
sis,theinitialstate,andthepreviousactionsandutterances was provided. For models without video input capabili-
ofbothagentstoestimatetheactionandutterancepolicies. ties,weuniformlysampleoneframeevery20framesfrom
In this way, we do not need to implement domain-specific thevideoepisodeasinput.WeevaluatedGPT-4o(OpenAI
planning,whichcanbeextremelychallengingandslowfor 2023), Llava 1.6 (Liu et al. 2023), Gemini 1.5 (Reid et al.
multi-agentinteractionswithbothphysicalactionsandver- 2024), InternVL2 (Chen et al. 2023) and VideoLlama 2
bal communication. In this work, we use GPT-4o for the (Chengetal.2024).Weevaluatedthelatestversionofeach
LLM. We find that GPT-4o can accurately estimate the ac- LMM at the time of submission. For LIMP, we use Gem-
tionandutterancepoliciesbasedonthegivencondition. ini 1.5 Pro as the VLM and GPT-4o as the LLM. Finally,
Figure4illustrateshowIMPevaluatestheactionandut- we evaluated BIP-ALM with finetuned Llama 2 (Jin et al.
terancelikelihoodatonetimestep.Giventhecondition,the 2024), the best-performing model on a prior multi-modal
LLMestimatesthatitislikelythatagentiwilltaketheob- ToMbenchmark,MMToM-QA.Moredetailsoftheexperi-
served action (“walk towards table”) but is unlikely to say mentsareprovidedinthesupplementarymaterial.
“Ifoundapotatoinsidefridge”asitisinconsistentwiththe
Results
socialgoalofhinderingagentj(agentihadjustputapotato
inthefridgebeforetheconversation). WereportthehumanandmodelperformanceinFigure5and
Table 3. Human participants achieved almost perfect accu-
Experiments racyacrossallquestions,with98.9%ofthecorrectanswers
having majority agreement. The overall performance aver-
HumanExperiment
aged across individual participants is 93.5%. The slightly
We recruited 18 participants (mean age = 36.0; 10 female) lowerperformanceonsocialgoalinference(94.4%)andbe-
fromProlifictoanswer90questionsrandomlysampledfrom lief of goal inference (87.1%) indicates these questions aremorechallengingandrequiregreaterfocus. Why does LIMP outperform the best LMMs? LIMP
All LMM baselines performed poorly on MuMA-ToM, overcomesthetwoaforementionedweaknessesofLMMs–
indicating a substantial gap between machine and human the inability to recognize multi-modal behavior under vari-
ToM. The best-performing LMM baseline is Gemini 1.5 ous social goals and the sensitivity to noisy visual percep-
Pro,butitsoverallaccuracyisonly56.4%.Amongthethree tion.First,whileLLMsstrugglewithdirectToMreasoning,
questiontypes,beliefinferenceistheeasiestforLMMs.In theyexcelattheforwardgenerationofmulti-modalbehav-
particular,Llava34Bachievedthehighestaccuracyforbe- iorgivenmentalstates.Forexample,itismuchharderforan
lief inference. However, all LMMs struggle with the more LLMtocorrectlyinferwhetheranagentishinderinganother
challenging social goal inference and belief of goal infer- bylyingthanitisforthemodeltogeneratealiebasedonthe
encequestions,whicharenotevaluatedbypriorToMbench- agent’s belief and social goal. Such multi-modal behavior
marks.Thebest-performingLMMmodeloverallisGemini generation ability enables LIMP to estimate the action and
1.5Pro,withanaccuracyof56.4%.Notably,BIP-ALMhad utterancelikelihood,identifyingthekeyactionsand/orutter-
anaccuracyof33.9%,indicatingitsinabilitytounderstand ancesthatrevealthetruementalstateofanagent(asshown
multi-agentinteractions.ThisisbecauseBIP-ALMonlyre- inFigure4).Second,whenaVLMfailstorecognizetheex-
lies on single-agent goals and beliefs for inverse planning act object that the agent is interacting with, LIMP can fill
but does not consider social goals and beliefs of others’ inthismissinginformationwithcontextfromthetextinput.
goals. Additionally, BIP-ALM assumes certain symbolic We evaluated the action accuracy by using semantic simi-
representations, which also limits its generality. Our LIMP larityandfoundthatthisapproachincreasesinferredaction
model significantly outperforms all state-of-the-art models accuracyfrom54.4%to86.6%.Asaresult,LIMPisableto
onourbenchmark,withanoverallaccuracyof76.6%.Criti- performinferenceonmuchmoreaccurateinformation.
cally,byusingGPT-4otoestimateactionandutterancelike- How general is LIMP? Prior inverse planning models,
lihood for inverse multi-agent planning, LIMP can achieve includingBIP-ALM,allrequirehandcraftedrepresentations
muchbetterperformancethanGPT-4oitself.Thereisstilla for specific domains. LIMP, however, represents all infor-
gapbetweenthebestmodelperformanceandhumanperfor- mationusingnaturallanguage,whichenablesthedirectuse
mance,highlightingtheneedforfurtherstudies. ofanypretrainedLLMsandVLMswithoutdomain-specific
WefinetunedVideoLlama2onavideoinstructiondataset knowledge or finetuning. By utilizing powerful pretrained
created from our training set. The finetuned model did VLMs for visual perception, LIMP can directly recognize
not perform better, suggesting that common finetuning ap- actions from RGB videos in an open-ended way, without
proaches for LMMs do not improve their ToM capacities. specifying target action labels for a domain. LIMP also
WealsoevaluatedtheperformanceofLMMswithchain-of- leverages an LLM to use contextual clues from the text to
shotprompting(Kojimaetal.2022)andfoundnoimprove- fill in missing information from visual perception, provid-
ment(Table2inthesupplementarymaterial). ing a general method for multi-modal information fusion.
OnecanalsoeasilyupgradeLIMPbyplugginginanystate-
Discussion of-the-artVLMsandLLMs.
Why do LMMs perform poorly? There are two sources WhatarethelimitationsofLIMP?Hallucinationscre-
of systematic errors for LMMs. First, LMMs struggle with ated by the VLM can cause significant errors in LIMP. For
understanding multi-modal behavior in complex social sit- example, an agent may only open and close the fridge, but
uations,oftenfailingtodistinguishbetweendeliberatehin- the VLM may mistakenly think that the agent also grabs
dering and failed attempts to help due to incorrect beliefs. something from the fridge. Such hallucinations in action
Most models can solve belief inference tasks where help- recognitioncannotbecorrectedbythetexturalcontext.As
ing is the assumed social goal. However, they consistently a result, LIMP will incorrectly interpret the agent’s behav-
struggleinscenarioswherehinderingistheassumedsocial ior. Additionally, LIMP does not explicitly infer an agent’s
goal(e.g.,”IfMaryistryingtohinderJack,wheredoesshe beliefofanotheragent’sbelief.ItinsteadpromptsanLLM
least likely believe...?). Except for Llava 1.6 34B (87.2%) with past actions and utterances to implicitly infer that,
andGemini1.5Pro(62.2%),allmodelsperformatorbelow whichcanbecomecostlyforlongerevents.LIMPalsodoes
thelevelofrandomguessinginthesecases,withIntern-VL notperformrecursivereasoningformorethantwolevels.
226B(18.6%)performingmuchworsethanrandomchance. Whatarethelimitationsofourbenchmark?Thesce-
Thefailuretounderstandadversarialbehaviorsisevenmore nariosinourbenchmarkarecurrentlylimitedtointeractions
prominent in social goal inference and belief of goal infer- between two agents in household settings, where there are
ence. For instance, if Sarah tells John there is milk in the three social goals: helping, hindering, and acting indepen-
fridge,butthereisn’tanymilk,andSarahknowsthis,sheis dently. Moreover, the current benchmark has synthetic hu-
hinderingJohn.Second,LMMsoftenfailtocorrectlyinter- manactivities.Thesesyntheticactivitiesarerealisticasver-
pretvisualinputs,suchaswhenanobjectistoosmalloris ifiedinpriorwork(Puigetal.2020)andenablelarge-scale
occluded when the agent is picking it up, leading to incor- testing. However sim-to-real evaluation could be valuable
rectconclusionsabouttheagent’sactions.Whilehumansare forfuturestudies.
abletousecontextualcluestoinferwhattheobjectmightbe,
Conclusion
VLMs struggle with this task. These errors in recognizing
crucialactionslikelycontributesignificantlytotheiroverall We present the first multi-modal Theory of Mind bench-
poorperformanceonourbenchmark. markformulti-agentinteractionsincomplexembodiedset-tings. We have systematically evaluated humans and state- Gandhi, K.; Fra¨nken, J.-P.; Gerstenberg, T.; and Goodman,
of-the-artLMMsonourbenchmark.Wehavealsoproposed N.2024. Understandingsocialreasoninginlanguagemod-
a novel multi-modal ToM model that outperforms all base- elswithlanguagemodels. AdvancesinNeuralInformation
lineswhilemaintaininggenerality.Infuturework,weintend ProcessingSystems,36.
to incorporate more complex real-world scenarios beyond Gandhi, K.; Stojnic, G.; Lake, B. M.; and Dillon, M. R.
household environments and introduce multi-modal social 2021. Baby Intuitions Benchmark (BIB): Discerning the
interactionsinvolvingmorethantwoagents.Wealsoplanto goals,preferences,andactionsofothers. AdvancesinNeu-
create a test set with real-world videos for ToM evaluation ralInformationProcessingSystems,34:9963–9976.
inreal-worldscenarios.
Gmytrasiewicz,P.J.;andDoshi,P.2005. AFrameworkfor
SequentialPlanninginMulti-AgentSettings. JournalofAr-
References tificialIntelligenceResearch,24:49–79.
Amirizaniani, M.; Martin, E.; Sivachenko, M.; Mashhadi, Gordon,A.2016a. Commonsenseinterpretationoftriangle
A.; and Shah, C. 2024. Do LLMs Exhibit Human-Like behavior. InProceedingsoftheaaaiconferenceonartificial
Reasoning?EvaluatingTheoryofMindinLLMsforOpen- intelligence,volume30.
EndedResponses. arXivpreprintarXiv:2406.05659. Gordon,A.S.2016b.CommonsenseInterpretationofTrian-
gleBehavior. InAAAIConferenceonArtificialIntelligence.
Baker, C. L.; Jara-Ettinger, J.; Saxe, R.; and Tenenbaum,
J. B. 2017. Rational quantitative attribution of beliefs, de- Hamlin,J.K.;Wynn,K.;andBloom,P.2007. Socialevalu-
sires and percepts in human mentalizing. Nature Human ationbypreverbalinfants. Nature,450(7169):557–559.
Behaviour,1(4):1–10. Han,Y.;andGmytrasiewicz,P.2019.IPOMDP-Net:ADeep
NeuralNetworkforPartiallyObservableMulti-AgentPlan-
Baron-Cohen, S.; Leslie, A. M.; and Frith, U. 1985. Does
ningUsing InteractivePOMDPs. Proceedingsof theAAAI
theautisticchildhavea“theoryofmind”?Cognition,21(1):
ConferenceonArtificialIntelligence,33(01):6062–6069.
37–46.
He, Y.; Wu, Y.; Jia, Y.; Mihalcea, R.; Chen, Y.; and Deng,
Bubeck, S.; Chandrasekaran, V.; Eldan, R.; Gehrke, J.;
N.2023. Hi-tom:Abenchmarkforevaluatinghigher-order
Horvitz,E.;Kamar,E.;Lee,P.;Lee,Y.T.;Li,Y.;Lundberg,
theory of mind reasoning in large language models. arXiv
S.;etal.2023.Sparksofartificialgeneralintelligence:Early
preprintarXiv:2310.16755.
experimentswithgpt-4. arXivpreprintarXiv:2303.12712.
Hou, G.; Zhang, W.; Shen, Y.; Wu, L.; and Lu, W. 2024.
Chan, C.; Jiayang, C.; Yim, Y.; Deng, Z.; Fan, W.; Li, H.; TimeToM: Temporal Space is the Key to Unlocking the
Liu, X.; Zhang, H.; Wang, W.; and Song, Y. 2024. Nego- Door of Large Language Models’ Theory-of-Mind. arXiv
tiationToM: A Benchmark for Stress-testing Machine The- preprintarXiv:2407.01455.
ory of Mind on Negotiation Surrounding. arXiv preprint
Ivanova, A. A.; Sathe, A.; Lipkin, B.; Kumar, U.; Rad-
arXiv:2404.13627.
kani, S.; Clark, T. H.; Kauf, C.; Hu, J.; Pramod, R. T.;
Chen, Z.; Wu, J.; Wang, W.; Su, W.; Chen, G.; Xing, S.; Grand, G.; Paulun, V.; Ryskina, M.; Akyu¨rek, E.; Wilcox,
Zhong,M.;Zhang,Q.;Zhu,X.;Lu,L.;Li,B.;Luo,P.;Lu, E.; Rashid, N.; Choshen, L.; Levy, R.; Fedorenko, E.;
T.; Qiao, Y.; and Dai, J. 2023. InternVL: Scaling up Vi- Tenenbaum, J.; and Andreas, J. 2024. Elements of World
sion Foundation Models and Aligning for Generic Visual- Knowledge (EWOK): A cognition-inspired framework for
LinguisticTasks. arXivpreprintarXiv:2312.14238. evaluating basic world knowledge in language models.
arXiv:2405.09605.
Chen, Z.; Wu, J.; Zhou, J.; Wen, B.; Bi, G.; Jiang, G.;
Cao,Y.;Hu,M.;Lai,Y.;Xiong,Z.;andHuang,M.2024a. Jha,K.;Le,T.A.;Jin,C.;Kuo,Y.-L.;Tenenbaum,J.B.;and
ToMBench: Benchmarking Theory of Mind in Large Lan- Shu,T.2024.NeuralAmortizedInferenceforNestedMulti-
guageModels. arXiv:2402.15052. AgentReasoning. ProceedingsoftheAAAI Conferenceon
ArtificialIntelligence,38(1):530–537.
Chen,Z.;Wu,J.;Zhou,J.;Wen,B.;Bi,G.;Jiang,G.;Cao,
Jin, C.; Wu, Y.; Cao, J.; Xiang, J.; Kuo, Y.-L.; Hu, Z.; Ull-
Y.; Hu, M.; Lai, Y.; Xiong, Z.; et al. 2024b. ToMBench:
man, T.;Torralba, A.; Tenenbaum, J.B.; and Shu, T. 2024.
BenchmarkingTheoryofMindinLargeLanguageModels.
MMMToM-QA:MultimodalTheoryofMindQuestionAn-
arXivpreprintarXiv:2402.15052.
swering. 62ndAnnualMeetingoftheAssociationforCom-
Cheng, Z.; Leng, S.; Zhang, H.; Xin, Y.; Li, X.; Chen, putationalLinguistics(ACL).
G.; Zhu, Y.; Zhang, W.; Luo, Z.; Zhao, D.; and Bing, L.
Kim, H.; Sclar, M.; Zhou, X.; Bras, R. L.; Kim, G.; Choi,
2024. VideoLLaMA 2: Advancing Spatial-Temporal Mod-
Y.;andSap,M.2023. FANToM:ABenchmarkforStress-
eling and Audio Understanding in Video-LLMs. arXiv
testing Machine Theory of Mind in Interactions. arXiv
preprintarXiv:2406.07476.
preprintarXiv:2310.15421.
Cohen, M. 2021. Exploring RoBERTa’s theory of mind Kojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y.; and Iwasawa,
throughtextualentailment. Y.2022. LargeLanguageModelsareZero-ShotReasoners.
Denham, S. A.; Blair, K. A.; DeMulder, E.; Levitas, J.; ArXiv,abs/2205.11916.
Sawyer, K.; Auerbach-Major, S.; and Queenan, P. 2003. Kosinski, M. 2023. Theory of Mind May Have Sponta-
Preschool emotional competence: Pathway to social com- neouslyEmergedinLargeLanguageModels.arXivpreprint
petence? ChildDevelopment,74(1):238–256. arXiv:2302.02083.Le, M.; Boureau, Y.-L.; and Nickel, M. 2019. Revisiting Puig, X.; Shu, T.; Li, S.; Wang, Z.; Tenenbaum, J. B.; Fi-
the evaluation of theory of mind through question answer- dler, S.; and Torralba, A. 2020. Watch-And-Help: A Chal-
ing. In Proceedings of the 2019 Conference on Empirical lenge for Social Perception and Human-AI Collaboration.
MethodsinNaturalLanguageProcessingandthe9thInter- arXiv:2010.09890.
nationalJointConferenceonNaturalLanguageProcessing
Puig, X.; Shu, T.; Tenenbaum, J. B.; and Torralba, A.
(EMNLP-IJCNLP),5872–5877.
2023. NOPA: Neurally-guided Online Probabilistic As-
Li, B.; Wang, R.; Wang, G.; Ge, Y.; Ge, Y.; and sistance for Building Socially Intelligent Home Assistants.
Shan, Y. 2023a. Seed-bench: Benchmarking multimodal arXivpreprintarXiv:2301.05223.
llms with generative comprehension. arXiv preprint
Rabinowitz, N. C.; Perbet, F.; Song, H. F.; Zhang, C.; Es-
arXiv:2307.16125.
lami,S.M.A.;andBotvinick,M.2018. MachineTheoryof
Li, L.; Yin, Y.; Li, S.; Chen, L.; Wang, P.; Ren, S.; Li, M.; Mind. arXiv:1802.07740.
Yang, Y.; Xu, J.; Sun, X.; Kong, L.; and Liu, Q. 2023b.
Reid, M.; Savinov, N.; Teplyashin, D.; Lepikhin, D.; Lilli-
M3IT: A Large-Scale Dataset towards Multi-Modal Multi-
crap,T.;Alayrac,J.-b.;Soricut,R.;Lazaridou,A.;Firat,O.;
lingualInstructionTuning. arXiv:2306.04387.
Schrittwieser, J.; et al. 2024. Gemini 1.5: Unlocking mul-
Li,W.;Yasuda,S.C.;Dillon,M.R.;andLake,B.2024a.An timodalunderstandingacrossmillionsoftokensofcontext.
Infant-CognitionInspiredMachineBenchmarkforIdentify- arXivpreprintarXiv:2403.05530.
ing Agency, Affiliation, Belief, and Intention. In Proceed-
Sabour,S.;Liu,S.;Zhang,Z.;Liu,J.M.;Zhou,J.;Sunaryo,
ingsoftheAnnualMeetingoftheCognitiveScienceSociety,
A. S.; Li, J.; Lee, T. M. C.; Mihalcea, R.; and Huang, M.
volume46.
2024. EmoBench:EvaluatingtheEmotionalIntelligenceof
Li,W.;Yasuda,S.C.;Dillon,M.R.;andLake,B.M.2024b. LargeLanguageModels. arXiv:2402.12071.
AnInfant-CognitionInspiredMachineBenchmarkforIden-
Sanders, K.; Etter, D.; Kriz, R.; and Van Durme, B. 2023.
tifyingAgency,Affiliation,Belief,andIntention.
MultiVENT: Multilingual Videos of Events with Aligned
Liu,H.;Li,C.;Wu,Q.;andLee,Y.J.2023. VisualInstruc- NaturalText. arXivpreprintarXiv:2307.03153.
tionTuning.
Sclar, M.; Kumar, S.; West, P.; Suhr, A.; Choi, Y.; and
Malik, M.;and Isik,L. 2023. Relational visual representa- Tsvetkov,Y.2023a. MindingLanguageModels’(Lackof)
tionsunderliehumansocialinteractionrecognition. Nature Theory of Mind: A Plug-and-Play Multi-Character Belief
Communications,14(1):7317. Tracker. arXiv:2306.00924.
Netanyahu, A.; Shu, T.; Katz, B.; Barbu, A.; and Tenen- Sclar, M.; Kumar, S.; West, P.; Suhr, A.; Choi, Y.; and
baum,J.B.2021a. Phase:Physically-groundedabstractso- Tsvetkov, Y. 2023b. Minding Language Models’ (Lack
cial events for machine social perception. In Proceedings of) Theory of Mind: A Plug-and-Play Multi-Character Be-
oftheaaaiconferenceonartificialintelligence,volume35, lief Tracker. In Proceedings of the 61st Annual Meeting of
845–853. the Association for Computational Linguistics (Volume 1:
Netanyahu, A.; Shu, T.; Katz, B.; Barbu, A.; and Tenen- LongPapers),13960–13980.Toronto,Canada:Association
baum,J.B.2021b. PHASE:PHysically-groundedAbstract forComputationalLinguistics.
SocialEventsforMachineSocialPerception. In35thAAAI Sclar, M.; Kumar, S.; West, P.; Suhr, A.; Choi, Y.; and
ConferenceonArtificialIntelligence(AAAI). Tsvetkov,Y.2023c. MindingLanguageModels’(Lackof)
OpenAI. 2023. GPT-4 Technical Report. ArXiv, Theory of Mind: A Plug-and-Play Multi-Character Belief
abs/2303.08774. Tracker. arXiv:2306.00924.
Pandya, P.; Talwarr, A. S.; Gupta, V.; Kataria, T.; Shu, T.; Bhandwaldar, A.; Gan, C.; Smith, K.; Liu, S.;
Gupta, V.; and Roth, D. 2024. NTSEBENCH: Cogni- Gutfreund, D.; Spelke, E.; Tenenbaum, J.; and Ullman, T.
tive Reasoning Benchmark for Vision Language Models. 2021. Agent: A benchmark for core psychological rea-
arXiv:2407.10380. soning. InInternationalConferenceonMachineLearning,
9614–9625.PMLR.
Patraucean, V.; Smaira, L.; Gupta, A.; Recasens, A.; Mar-
keeva,L.;Banarse,D.;Koppula,S.;Malinowski,M.;Yang, Shu, T.; Kryven, M.; Ullman, T. D.; and Tenenbaum, J.
Y.; Doersch, C.; et al. 2024. Perception test: A diagnostic 2020.AdventuresinFlatland:PerceivingSocialInteractions
benchmarkformultimodalvideomodels. AdvancesinNeu- UnderPhysicalDynamics. InCogSci.
ralInformationProcessingSystems,36.
Soubki, A.; Murzaku, J.; Jordehi, A. Y.; Zeng, P.;
Puig, X.; Ra, K.; Boben, M.; Li, J.; Wang, T.; Fidler, S.; Markowska, M.; Mirroshandel, S. A.; and Rambow, O.
andTorralba,A.2018a. Virtualhome:Simulatinghousehold 2024. Views Are My Own, But Also Yours: Benchmark-
activities via programs. In Proceedings of the IEEE Con- ingTheoryofMindusingCommonGround. arXivpreprint
ferenceonComputerVisionandPatternRecognition,8494– arXiv:2403.02451.
8502.
Stacy,S.;Gong,S.;Parab,A.;Zhao,M.;Jiang,K.;andGao,
Puig,X.;Ra,K.;Boben,M.;Li,J.;Wang,T.;Fidler,S.;and T.2024. ABayesiantheoryofmindapproachtomodeling
Torralba, A. 2018b. VirtualHome: Simulating Household cooperationandcommunication.WileyInterdisciplinaryRe-
ActivitiesviaPrograms. arXiv:1806.07011. views:ComputationalStatistics,16(1):e1631.Tang, J.; Liu, Q.; Ye, Y.; Lu, J.; Wei, S.; Lin, C.; Li, W.; Zhi-Xuan,T.;Mann,J.;Silver,T.;Tenenbaum,J.;andMans-
Mahmood, M. F. F. B.; Feng, H.; Zhao, Z.; et al. 2024. inghka,V.2020. Onlinebayesiangoalinferenceforbound-
MTVQA: Benchmarking Multilingual Text-Centric Visual edlyrationalplanningagents. Advancesinneuralinforma-
QuestionAnswering. arXivpreprintarXiv:2405.11985. tionprocessingsystems,33:19238–19250.
Tejwani, R.; Kuo, Y.-L.; Shu, T.; Katz, B.; and Barbu, A.
2021. Socialinteractionsasrecursivemdps. InConference
onRobotLearning,949–958.PMLR.
Ullman, T. 2023. Large Language Models Fail on Trivial
AlterationstoTheory-of-MindTasks. arXiv:2302.08399.
Ullman,T.;Baker,C.;Macindoe,O.;Evans,O.;Goodman,
N.;andTenenbaum,J.2009.Helporhinder:Bayesianmod-
elsofsocialgoalinference. Advancesinneuralinformation
processingsystems,22.
Verma,M.;Bhambri,S.;andKambhampati,S.2024. The-
oryofMindabilitiesofLargeLanguageModelsinHuman-
Robot Interaction: An Illusion? In Companion of the 2024
ACM/IEEE International Conference on Human-Robot In-
teraction,36–45.
Wellman, H. M.; Cross, D.; and Watson, J. 2001. Meta-
analysis of theory-of-mind development: The truth about
falsebelief. ChildDevelopment,72(3):655–684.
Wilf, A.; Lee, S. S.; Liang, P. P.; and Morency, L.-P. 2023.
ThinkTwice:Perspective-TakingImprovesLargeLanguage
Models’Theory-of-MindCapabilities. arXiv:2311.10227.
Wu, Y.; He, Y.; Jia, Y.; Mihalcea, R.; Chen, Y.; and Deng,
N. 2023. Hi-ToM: A Benchmark for Evaluating Higher-
OrderTheoryofMindReasoninginLargeLanguageMod-
els. In Bouamor, H.; Pino, J.; and Bali, K., eds., Findings
of the Association for Computational Linguistics: EMNLP
2023, 10691–10706. Singapore: Association for Computa-
tionalLinguistics.
Xu, H.; Zhao, R.; Zhu, L.; Du, J.; and He, Y. 2024. Open-
ToM:AComprehensiveBenchmarkforEvaluatingTheory-
of-MindReasoningCapabilitiesofLargeLanguageModels.
arXivpreprintarXiv:2402.06044.
Yang, S.; Ge, Y.; Li, Y.; Chen, Y.; Ge, Y.; Shan, Y.; and
Chen, Y. 2024. SEED-Story: Multimodal Long Story
Generation with Large Language Model. arXiv preprint
arXiv:2407.08683.
Ying, K.; Meng, F.; Wang, J.; Li, Z.; Lin, H.; Yang, Y.;
Zhang, H.; Zhang, W.; Lin, Y.; Liu, S.; et al. 2024a. Mmt-
bench: A comprehensive multimodal benchmark for evalu-
ating large vision-language models towards multitask agi.
arXivpreprintarXiv:2404.16006.
Ying,L.;Jha,K.;Aarya,S.;Tenenbaum,J.B.;Torralba,A.;
andShu,T.2024b. GOMA:ProactiveEmbodiedCoopera-
tive Communication via Goal-Oriented Mental Alignment.
arXiv:2403.11075.
Zhang, H.; Li, X.; and Bing, L. 2023. Video-LLaMA: An
Instruction-tuned Audio-Visual Language Model for Video
Understanding. arXivpreprintarXiv:2306.02858.
Zhang, J.; Huang, W.; Ma, Z.; Michel, O.; He, D.; Gupta,
T.; Ma, W.-C.; Farhadi, A.; Kembhavi, A.; and Krishna, R.
2024.TaskMeAnything.arXivpreprintarXiv:2406.11775.Appendix
ComparisonofToMBenchmarks only accepts textual input, we adapted it to our dataset by
adding Gemini 1.5 Pro’s visual extraction results after the
Table 2 provides a comparison between our MuMA-ToM textual input as input for SimToM and tested it with GPT-
benchmark and prior ToM benchmarks, highlighting key 4oservingastheprimarylanguagemodel.SimToM,which
featuressuchasthesizeofthetestset,inputmodalities,and analyzestheperspectiveofeachagenttoassistthelanguage
evaluation metrics. Our benchmark stands out as the only model,achievedthehighestaccuracyinbelief-of-goalques-
benchmark with multi-modal inputs and multi-agent inter- tions among all the baselines tested. This suggests that a
actions.Itsimultaneouslyevaluatesmulti-agentsocialinter- multi-stepapproachcanimprovealanguagemodel’scapac-
actions with belief, goal, and belief of other agents’ goals, ityforToMreasoning.However,theoverallaccuracyisstill
aswellastheabilitytoinfermentalstatesfrommulti-modal below50%.
inputs.
LIMP w/ Llama 3.1 8B for Inverse Multi-agent Plan-
MuMA-ToMBenchmarkDetails ning Solving ToM problems with language models usu-
ally requires some form of finetuning or few-shot prompt-
ingtoequipthemodelwithdomain-specificknowledge.In
MoreQuantitativeResults
contrast, LIMP leverages the forward planning capabilities
The results of all experiments conducted in our study are of language models to address the inverse planning prob-
showninTable3. lemwithoutanyfinetuningoradditionaldomainknowledge.
BeyondtestingverylargemodelslikeGPT-4o,wealsoex-
Chain of Thought Prompting. We evaluate state-of-the-
plored the potential of smaller models, such as Llama 3.1
artmodels’performanceonourdatasetwithzero-shotchain
8B, as an inverse planner for LIMP. However, the results
ofthought(CoT)prompting,asintroducedby(Kojimaetal.
indicate that smaller models lack the ability to effectively
2022).Weaddthephrase“Let’sthinkstepbystep”afterthe
functionasinverseplannersformulti-agentactions.Acloser
questionpromptbutbeforethelistofoptions.
qualitativeexaminationofLlama8B’sfailurepatternsshows
For all models tested, using CoT prompting showed no
thatthemodelisunabletounderstandtheconceptofhinder-
significant improvement in performance. In fact, for many
ing, which leads to poor performance across all questions
models, using CoT prompting caused a decrease in perfor-
relatedtohindering.
mance.WhilethereareinstanceswhereCoTledtosomeim-
provement,suchasinbeliefinferenceforInternVL226B, QualitativeResults
the overall impact effect was negligible on more challeng-
We provide two examples where Gemini 1.5 Pro, the
ingsocialgoalandbeliefofgoalinferencequestions.These
best-performing LMM on the MuMA-ToM benchmark,
results further highlight the current limitations of state-of-
fails while LIMP succeeds, highlighting the challenges
the-art LMMs. Even with CoT guidance, they struggle to
state-of-the-art LMMs face on our benchmark. We also
effectivelyunderstandsocialinteractions.
provideanexamplewherehallucinationsleadtoLIMPalso
Finetuned Baseline We finetuned the VideoLlama 2 7B failingtosolvetheproblem.
modelonourtrainingsetforactioncaptioningtasksfollow-
ing(Zhang,Li,andBing2023),usingtwoA100GPUsfor Example of Gemini’s due to failure to understand di-
1 epoch, with a learning rate of 2e-5 and a batch size of 4. versesocialinteractions
The performance of the model was lower after finetuning,
suggesting that the model may have inherent limitations in
Do you know It’s in the
ToMreasoningoractionrecognition.Weexperimentedwith where it sh ?e beer fridge
finetuningforupto3epochsandfoundthatextendingfine-
tuningbeyondoneepochleadstoover-fitting,andthemodel
wasunabletoanswerthequestionswithA,B,orC.
AdvancedPromptingforToM. Recentworkshavelever-
A B Michael opens the fridge
aged language models to tackle ToM problems through
multi-step reasoning approaches (Wilf et al. 2023; Sclar
etal.2023c;Houetal.2024).Amongthesetext-onlymod-
els,wechosetoevaluateSimToM,asthecodefortheother
models was either unavailable or required extensive mod-
ifications to integrate with our benchmark. Since SimToM
C Michael closes the fridge without grabbing anything D Michael finds the beer on the coffeetable
first exampleBenchmark Agent Testedconcepts Size Modality Communication Generation Evaluation
number
Triangle Single SocialInteraction 100 Text No Hand-designed Multiplechoice
COPA (Gor- agent Q&A
don2016a)
ToMi (Le, Multi First&SecondOr- 400 Text No Templates Multiplechoice
Boureau, and agents derbelief Q&A
Nickel2019)
Phase (Ne- Multi Goals and Social 500 Video No Procedural Multiplechoice
tanyahu et al. agents relationships Generation recognition
2021b)
Agent (Shu Single Goal Preferences, 960 Video No Procedural Surpriseratting
etal.2021) agent Action Efficiency, Generation
Unobserved Con-
straints, and Cost-
RewardTrade-offs
Epistemic rea- Multi KnowledgeandBe- 2000 Text No Templates True or false
soning (Cohen agents lief judgements
2021)
BIB (Gandhi Single Goal Preferences, 5000 Video No Procedural Surpriserating
etal.2021) & Multi rational actions, Generation
agents constraints
Adv-CSFB Single Falsebelief 183 Text No Hand-designed Multiplechoice
(Kosinski agent filling in the
2023) blanks
Hi-ToM (He Multi High-orderbeliefs 600 Text Yes Procedural Multiplechoice
etal.2023) agents Generation Q&A
FANToM Multi Belief & informa- 4807 Text Yes Procedural Question an-
(Kim et al. agents tiontracking Generation swering
2023)
BigToM Single Belief 5000 Text No Proceduralgen- Question an-
(Gandhi et al. agent eration swering
2024)
MMTOM-QA Single Belief&Goal 600 Text & No Proceduralgen- Multiplechoice
(Jin et al. agent Video eration Q&A
2024)
TomBench Multi Emotion,desire,in- 5330 Text Yes Proceduralgen- Multiplechoice
(Chen et al. agents tention,knowledge, eration Q&A
2024b) belief, non-literal
communication
OpenToM (Xu Multi Second-order be- 696 Text No Proceduralgen- Question an-
etal.2024) agents lief,attitude eration swering
Negotiation Multi Belief, desire, in- 13800 Text Yes Proceduralgen- Question an-
ToM (Chan agents tention eration swering
etal.2024)
Infant Cog- Multi False belief, social 2000 Video No Proceduralgen- Surpriserating
nition Bench- agents goal eration
mark(Lietal.
2024b)
Common-ToM Multi Highorderbelief 2104 Text Yes Proceduralgen- True of false
(Soubki et al. agents eration judgements
2024)
EmoBench Multi Complex emotions, 200 Text Yes Hand-designed Multiplechoice
(Sabour et al. agents personal beliefs Q&A
2024) & experiences,
emotional cues,
perspectivetaking
Our MuMA- Multi Belief, social goal 900 Text & Yes Proceduralgen- Multiplechoice
ToM bench- agents andbeliefofother’s Video eration Q&A
mark goal
Table2:ComparisonbetweenMuMA-ToMandpriorToMBenchmarksMethod BeliefInference SocialGoalInference BeliefofGoalInference All
Llava1.634B 93.6 37.2 27.5 52.8
Llava1.634BCoT 93.2 46.1 19.4 52.9
Llava1.613B 70.2 43.2 17.9 43.7
Llava1.613BCoT 64.9 41.6 25.3 43.9
Gemini1.5Flash 53.9 33.0 41.4 42.7
Gemini1.5FlashCoT 56.7 35.6 41.4 43.6
Gemini1.5Pro 78.9 43.9 46.9 56.4
Gemini1.5ProCoT 79.8 42.6 41.1 54.5
GPT-4o 67.9 39.6 44.4 50.6
GPT-4oCoT 62.2 33.6 39.8 45.2
InternVL28B 62.2 44.6 45.1 50.6
InternVL28BCoT 57.7 44.9 43.5 48.7
InternVL226B 59.3 44.9 35.5 46.6
InternVL226BCoT 64.1 44.9 36.1 48.4
VideoLlama27B 70.1 45.6 37.7 51.1
VideoLlama27BCoT 51.8 42.9 34.9 42.8
VideoLlama27B(finetuned) 42.7 35.7 34.3 37.3
SimToM 54.6 43.5 44.8 47.6
LIMPwithLlama3.18B 35.8 23.4 37.7 33.0
BIP-ALM 41.2 34.1 30.6 33.9
LIMPwithGPT-4o 93.4 67.7 68.7 76.6
Table 3: All experiment results: For models that accept video input, the full videos were provided. For models that do not,
uniformlysampledframes(every20frames)wereusedinstead.SinceSimToMisatext-basedmodel,weprovideditwiththe
actionrecognitionoutputsfromGemini1.5Pro.
stop him, this suggests that she is
Question:Giventheaboveinteraction,assumingthatJes-
okay with him finding the beer.
sicaknowswhatisinsidethefridge,whichofthefollow-
Let’s look at why the other answers are
ingstatementsisLEASTlikely?
incorrect:
A) When giving information, Jessica was indifferent to-
A) When giving information, Jessica was
wardsMichael’sgoals.
B) When giving information, Jessica has been trying to indifferent towards Michael’s goals -
preventMichaelfromfindingthebeer. This could be true as Jessica doesn’t
C) When giving information, Jessica has been trying to explicitly state that she wants him
helpMichaellocatethebeer. to find the beer nor does she show any
CorrectAnswer:C disappointment when he finds it.
GeminiAnswer:B C) When giving information, Jessica has
been trying to help Michael locate the
The correct answer to this question is ‘helping’ because beer - This is the most likely answer.
it is assumed that Jessica knows there is no beer in the Jessica provided the location of the
fridge, yet still directs Michael there, suggesting deliberate beer to Michael after he asked, making
misleading,sohelpingisleastlikely.However,Geminifails it easier for him to find it.
tocorrectlyunderstandtheconceptofhindering.Itassumes
that as long as the agent provides information when asked,
itistryingtohelp,regardlessofwhethertheinformationis
accurate.SinceGeminicannotinterprettheactofproviding
misleading information as hindering, it fails to answer this
question correctly. Gemini’s explanation for its answer is
shownbelow.
Gemini’sanswerandexplanation:
The correct answer is B) When giving
information, Jessica has been trying to
prevent Michael from finding the beer.
Reason: We can see in the video,
after Michael asks where the beer is,
Jessica goes to the living room and
waits there. When Michael goes to the
kitchen, Jessica makes no attempt toExample of Gemini’s failure due to visual recognition
failure ExampleofLIMP’sfailureduetohallucination
It’s in the Where is the Do you know It’s in the
microwave carrot? where the wine bathroom
is? cabinet
A B Michael opens the microwave
A B Michael opens the bathroom cabinet
C Michael closes microwave without grabbing anything D Michael finds the carrot inside the stove C Michael closes bathroom cabinet without grabbing anything D Michael finds the wine on coffeetable
Question: Given the above interaction, assuming
third example
Question: Given the above interaction, assuming that Sasreacohndknows what is inside the bathroom cab-
that Mary knows what is inside the microwave, inet, wexhaimcphle of the following statements is LEAST
whichofthefollowingstatementsisMOSTlikely? likely?
A) When giving information, Mary was indifferent A) When giving information, Sarah was indifferent
towardsMichael’sgoals towardsMichael’sgoals
B) When giving information, Mary has been trying B)Whengivinginformation,Sarahhasbeentrying
tohelpMichaellocatethecarrot topreventMichaelfromfindingthewine
C) When giving information, Mary has been trying C)Whengivinginformation,Sarahhasbeentrying
topreventMichaelfromfindingthecarrot tohelpMichaellocatethewine
CorrectAnswer:C CorrectAnswer:C
GeminiAnswer:B LIMP’sanswer:B
The question is similar to the previous two, with Sarah
Gemini’sanswerandexplanation: providingmisleadinginformation.Gemini1.5Proheremis-
The answer is B) When giving takenly extracts Michael grabbing the wine from the bath-
information, Mary has been trying roomcabinetwhileMichaelactuallydoesnot.Astheinitial
to help Michael locate the carrot. state retrieval process in LIMP relies on the action, LIMP
Reasoning: The dialogue clearly shows will mistakenly identify the location of wine as inside the
that Michael is seeking information bathroomcabinetandmistakenlyinterpretSarah’smislead-
about the carrot’s location. Mary ingwordsashelping.
responded directly, indicating she
understood his request and provided BenchmarkStatistics
effective information. Even though There are 225 interactive scenarios in our MuMA-ToM
Michael’s further seek of carrot is benchmark, 150 of which have language communication,
not shown, this suggests she was trying and75ofwhichdonothavecommunication.Theepisodes
to assist him. There’s no evidence to inthebenchmarkaregeneratedtobefactuallycorrect,con-
support options A or C. Mary’s direct cise,andhumanreadable.Eachinteractivescenariohappens
and relevant response implies she’s not in one of the four apartment with 10+ containers, 10+ sur-
being indifferent or obstructive but faces,and300+objectsintotal.Thereare17relevantobjects
rather cooperative. fortheagents’goalintotal,distributedamong11initiallo-
cations. Figure 6 shows the distribution of objects’ initial
Thecorrectanswertothisquestionis‘hindering’because location.
it is assumed that Mary knows there is no carrot in the Figure 7 shows the distribution of text and video length
microwave, yet still directs Michael there, suggesting de- over all the scenarios. On average, the videos have 364.8
liberate misleading. Gemini fails to recognize this because frames(approximately36secondslong),andthetextinputs
it cannot identify the carrot in the video. Without this contain136tokens(manyofwhicharejustconversations).
recognition, it is unable to verify whether Michael actually Therelativelyshortcontextlengthreducestheneedforthe
foundthecarrotinthemicrowaveandmistakenlyinterprets modeltoretrievevalidinformationfromalargecontext,al-
Mary’sinstructionsasanattempttohelp. lowingustofocusontestingmodels’ToMcapabilitywith-
outlong-contexttracking.Figure6:Objects’initiallocationsinMuMA-ToM.
AvailableData makeinferences,thisinformationcanbehelpfulfortesting
models’capabilityofsolvingToMproblemswithsomead-
We also provide depth images, instance segmentation, ditionalinformation:forexample,ground-truthactionsand
ground-truthactions,states,andcameradataforourbench- objectlocations(frominstancesegmentation).
mark in addition to RGB videos and text. Even though our
LIMP model does not rely on any of this information toFigure7:MuMA-ToMcontextlengthdistribution.Thetextsandvideosaredesignedtobeasconciseaspossible,allowingus
tofocusontestingmodels’ToMcapabilitywithoutlong-contexttracking.Thevideosarerenderedat10framespersecond.
ProceduralGenerationDetails weuseareshownbelow.
Figure8summarizestheproceduralgenerationprocess.We
Promptforaddingvarietyforinquiry
follow a recent paper GOMA (Ying et al. 2024b) to gener-
Objective: Generate natural language from a lan-
ateactions&utterancesequence,usethevirtualhome(Puig
guagetemplate.
et al. 2018b) 3D simulator to generate humanoid actions
UserInput:Questionswithabasictemplatedformat
within a realistic household environment and use GPT-4o
intheformof”WhereisX?WhereisY?”
togeneratetextsandquestions.
Instructions:Convertthisquestionintonaturalcon-
Step1inFigure8showstheaction&utterancesequence
versational language. Make it seem like everyday
generationprocess.Weusefourdifferentapartmentsasthe
conversation.Iftheuserasksaboutmultipleobjects,
baseenvironmentfortwoagents’interactions,samplingob-
combinetheobjectsintoasinglequestion.
jects to different containers & surfaces within the apart-
ment to generate a distinctive environment for each inter-
activescenario.Twoagents’initiallocation(roomlocation),
Promptforaddingvarietytoresponse
physical goal (finding or rearranging an object), initial be-
Objective:Generatenaturallanguagefromlanguage
lief(ground-truthbelief,falsebelief,oruniformbelief),and
template.
social intentions (help, hinder, independent) are also sam-
User Input: The locations of an object with a basic
pled. For interactive scenarios without language, we sam-
templatedformat,withentriesseparatedby;.Forin-
pledtheenvironmentandagents’goalinawaythatensures
stance, apple on table 121 livingroom; apple inside
two agents’ are aiming to put the same object to different
fridge 240 kitchen; apple null; banana on counter
locationsandthereisonlyoneobjectofthattypeintheen-
101 kitchen; banana null means that there is an ap-
vironment.Inthisway,agentswillhavetorearrangetheob-
ple on the table in the livingroom, an apple inside
jectaftertheotheragenthasplacedtheobject.Afterward,a
thefridgeinthekitchen,andthelocationofthethird
Monte Carlo Tree Search (MCTS) planner is used to com-
appleisunknown.Thereisabananaonthekitchen
pute the action sequence for each agent. The utterance is
counter, and the location of the other banana is un-
computed separately: for each step, if the two agents are
known.
in the same room and the first agent is uncertain about its
Instructions:Convertthisstatementintonaturalcon-
goal object’s location (entropy of its belief probability dis-
versational language. If the multiple locations are
tribution exceeds a threshold), the first agent will send an
providedforthesameobject,andsomeofthemare
inquiry. Upon receiving the inquiry, the second agent will
null,ignorethenullobjectsinthefinaldescription.
answerbasedonitssocialintention(provideacontradictory
Intheaboveexample,ignoreapplenullandbanana
answer with its belief when trying to hinder), and the first
null since the locations of the other apples and ba-
agent will update its belief accordingly. As agents’ beliefs
nanaareknown.
donotnecessarilymatchtheground-truthstate,thecombi-
nation of intention with the ground-truth environment state
iscomplicated:forinstance,providingfalseinformationcan Step2inFigure8showsthevisualgenerationprocedure.
be interpreted as trying to help but failing due to mistaken After generating raw action & utterance sequence, we use
beliefordeliberatelytryingtohinder.Aftertheoriginalut- the Virtualhome simulator to render RGB, depth, and in-
teranceisgenerated,weuseGPT-4otoaddvarietyandim- stance segmentation frames as well as supporting data like
provethequalityoflanguagecommunication.Theprompts cameradataorscenegraphs.Then,rawframesarecombined1 2
Apartment: 2
Initial state: book
on sofa, ...
Virtualhome
Simulator
Initial
sampler
Action & Utterance sequence
Observation Actions &
Agent 1: walk into kitchen, ask
Utterances
about milk, open fridge, ...
Video synthesis
Agent 1: goal: milk on table Agent 2: walk into kitchen,
social goal: none inform milk in fridge, grab
Planner
Agent 2: goal: mug on table mug
Social goal: help
RGB video, Text, Questions
3
GPT-4o for episode
Question template description generation
Given that [name] knows
location of [object name], Episode descriptions
which is MOST likely? Michael and Jessica GPT-4o for
A) ... walks into kitchen, ... question
B) ...
generation
C) ...
Figure8:OverviewoftheProceduralgenerationprocess.Thismethodensuresthattheepisodesandgroundtruthanswersare
factuallycorrect,whilemaintainingrealisticconversationsandscenarios.
together into a video. For interactive scenarios with lan-
User Input: A list of actions by each agent, Verbal
guage, agent names’, avatars and communication are over-
communicationbetweentheagents.
laidascaptionsonthevideoframes.
Structure: Actions: A list of actions taken by agent
0andagent1,Language:Verbalcommunicationbe-
tweentheagentsinalistformat.
Step3inFigure8showsthetextgenerationprocedurefor
Instructions:
inputtextandquestions.Withtwoagents’actionsandutter-
1. Synchronization guidelines: Synchronize actions
ancesequence,wepromptGPT-4otogenerateadescription
and language, the first entry in the ”language” list
of the two agents’ actions in a story-like way while main-
corresponds to the first action step, the second en-
taining chronological sequence. Portions of these descrip-
try in the ”language” list corresponds to the sec-
tionsarethenusedastextualinputforthequestions:forlan-
ond action and so on. If a language entry is null,
guage scenarios, the conversation serves as the text input;
thereisnocommunicationatthattimestep.Synchro-
forno-languagescenarios,oneagent’sactionisprovidedas
nizedescriptionsofactionsandlanguagestrictlyby
thetextinputwhiletheotheragent’sactionisshowninthe
timesteps.
video.WethenpromptGPT-4otogeneratequestionsbased
2. Agent names: Choose from a predefined list of
onourpre-designedtemplatesandthefulldescriptions.Es-
commonnames.
sentially,GPT-4ofillsintheblanksofthequestiontemplates
3. Description guidelines: Describe the actions and
usingtheinformationfromthecompletedescription.
languageofbothagentstogether,stepbystep.Avoid
adjectives and excessive descriptions. Do not skip
anyactionorlanguagesteps.
4.Afterestablishingthetimeline,makethedescrip-
tionshorter,moreconciseandflowalotlikeastory.
Donotskipanyactions
Prompt for generating episode descriptions 5. Place more emphasis on the events immediately
Objective: Create a description of a two-agent followingthelanguageconversation(ifany)
interactionscenariobasedontheprovidedlanguage 6.Whendescribingaactioninvolvedwithgrabbing
template. objects,makesuretoalsoincludetheoriginalplace
oftheobject
[Exampleinputandgeneratedscenario]Promptforgeneratingquestiontemplates container or picking up an item. If you cannot de-
Objective: Generate questions from the language cipher the location that [agent name] grabs from,
templatebyfillingintheblank. make your best guess based on all the context in
Userinput:Adescriptionoftheepisode. thevideo.Ifyoucannoteffectivelyidentifytheob-
Expectedoutput:Questionsfollowingthistemplated ject, just leave it as g¨rab some objectw¨ithout trying
format, filling in the blanks, denoted by [] where toguesstheexactone.
necessary.
Template:
Question: Given the above interaction, assuming
that[secondagent’sname]knowswhatis[inside
TextParsing&Multi-modalFusion
on a location that second agent mentioned in the
communication], which of the following statement
isMOSTlikely?
Forprocessingtextualinformation,wedirectlyuseGPT-4o
A)Whengivinginformation,[secondagent’sname]
toparsetheactionsandutterancesofeachagentseparately,
has been trying to help [first agent’s name] locate
in chronological order. Then, this parsed text information,
[theobjectthatsecondagentmentioned]
alongwiththerawvisualoutputsfromtextinputaswellas
B)Whengivinginformation,[secondagent’sname]
rawvisualoutputsfromGemini,isprovidedtoGPT-4ofor
hasbeentryingtoprevent[firstagent’sname]from
informationfusion.
finding[theobjectthatsecondagentmentioned]
C)Whengivinginformation,[secondagent’sname] A key step in our multi-modal fusion process we use is
wasindifferenttowards[firstagent’sname]’sgoals filling in missing information from the visual output based
Answer: B) When giving information, [second on the context. In the prompt given to Gemini, we instruct
agent’s name] has been trying to prevent [first the model to leave blanks for exact object names, as accu-
agent’s name] from finding [the object that second ratelyrecognizingsmallorobscuredobjectsisoftenimpos-
agentmentioned] sibleandcouldleadtounreasonableresults.Therawvisual
output, along with text input that provides necessary con-
text,isthenusedbyGPT-4otofillintheseblankswiththe
correctobjectnamesmentionedinthecontext.Thismethod
LIMPImplementationDetails
reduces the model’s reliance on recognizing small objects
directly,andtakesamorehuman-likeapproachtotheprob-
VisualPerception
lem.
A previous multi-modal ToM model, BIP-ALM(Jin et al. Anotherimportantstepinthemulti-modalfusionprocess
2024), relies on instance segmentation to extract ground is initial state retrieval. The initial state of the environment
truth object information and spacial relationships from vi- iscrucialfortheplanningprocess,astheagents’beliefsare
sual inputs. However, this level of detailed information is based on the initial state instead of the changed state, un-
notavailabletootherbaselinesorhumanparticipants,andis lesstheyobserveotheragentmovingthingsarounddirectly.
rarely available in real-world datasets. In RGB videos, de- Sincewedonotuseinstancesegmentation,itischallenging
tecting the exact object is challenging for both models and forthemodeltodirectlyidentifyobjectlocationsorgenerate
humans, as the objects might be small or obscured from scenegraphsfromvisualinput.Instead,weusetheagents’
view. To maintain generality, we use Gemini 1.5 Pro(Reid actionstoinfertheinitialstateoftheenvironment.Thisre-
et al. 2024) as our visual perception model. We upload the duces uncertainty for the model and allows it to focus on
RGB videos to the Gemini web version in google AI stu- relevant objects to the interaction while ignoring unrelated
dio without any extra information about possible locations ones.
andobjects,andaskedittoextracttheactionandutterance
Thepromptsweusefortextparsingandmulti-modalfu-
sequencesoftheagents.Thepromptweuseisshownbelow.
sionareshownbelow.
PromptforGemini1.5Provisualextraction
Task: You will watch a video depicting two agents Promptfortextparsing
performing some actions. Your goal is to infer and You will read a piece of text describing actions of
describe the actions in chronological order. For somenumberofpeoplewithdistinctivenames.You
[agent name], provide details about his/her actions, willalsohaveaname,whichisthenameoftheper-
including what objects she handled, where she ob- son whom you should pay attention to. Summarize
tainedthemfrom,andwheresheplacedthem.For- the person’s actions and utterance separately in a
mulateallactionsintoasingleline.Donotinclude chronologicalorder.Onlyincludetheactionsandut-
any newline characters. Note that an agent moving terancedirectlytakenbythepersoninthetext,and
theirarmprobablyindicatesopeninga excludeanyout the exact location of the object. The prompt is shown
previousactionsmentionedindirectly.Ifyoucannot
below.
find either utterance or actions of the person in the
text, leave the corresponding section blank. When
reading words like ”it”, replace it with inferred ob- Promptforlatentvariableextraction
ject or location to make actions clearer. Do not in- You will read a question about agents’ mind and
cludeagent’scommunicationaspartofit.Organize ideas, and the initial state of the environment from
your answer in this form: Actions: [”action one”, whichagents’areinteractingin.Agents’knowledge
”action two”, ”action three”, ...] ... Utterance: [”ut- & belief are about this initial state, but not neces-
teranceone”,”utterancetwo”,”utterancethree”,...] sarily changed state after some actions. For each
... choice, extract one set of second person’s belief
Text:textinput (make sure to turn it into some statement about the
Name:nameoftheagent environment state), second person’s social goal to-
wardfirstperon’sactions(help,hinderorsomesim-
ilar words of indepedent), and second person’s be-
lieved first person’s physical goal (some arrange-
Promptforerrorrecovery
ment of objects). Organize the answer in this way:
You will read some text describing a person’s
A: Belief: contents; Social goal: contents; Believed
action.Thenameofthepersonisgiven.Summarize
Goal: contents. B: Belief: contents; Social goal:
andreorganizetheperson’sactions.Possibleactions
contents; Believed Goal: contents. C: Belief: con-
include walk towards somewhere, grab something
tents;Socialgoal:contents;BelievedGoal:contents.
from somewhere, open some container, close
Do not include any other information or extra con-
some container, put something somewhere. Only
tents. Make sure your answer follow the format re-
summarizetheseactionsandtheirsynonymsinthis
quirement,use”;”toseparatevariableswithineach
formandabandonmismatchactions.Omitperson’s
choiceandendresponsewith”.”.Separatecontents
name.Whenmentioninglocationname,trytoinfer
of”A”,”B”and”C”with”.”
room the location is inside and include it in the
Initialstate:
action Check objects mentioned in the Additional
Question:
Information section. Replace any object mentioned
Actions:
in action with the object appeared in that section
Formulate your final answer in the following form.
Actions:[”action1”,”action2”,....]
InverseMulti-agentPlanningwithGPT-4o
Inputtext:rawoutputofGemini Unlike open-source models, GPT-4o does not provide the
Additionalinformation:context logprobabilityforanygivencompletion,sotheexactprob-
Person’sname:agentname abilityoftheutteranceoractioncannotbecalculated.How-
ever, GPT-4o does offer the log probabilities for the top
5 responses it generates. To address this, we implement a
Promptforinitialstateretrieval methodthatasksGPT-4otoassessthelikelihoodofagiven
You will read one or two person’s actions in a list utteranceoractionandrestrictsitsmostlikelyresponsesto
like form. From the actions taken, extract the ini- twochoices:A)Likely,orB)Unlikely.Wethencalculatethe
tial state of the environment before any people act. probabilityofthecompletionbyusingthelogprobabilityof
Checkeachgrabactionorsynonyms.Describeitin thetoken’A’.Thepromptisshownbelow:
theform”Thereisa[objectgrabbed][on/insidelo-
cationofgrabbing].Onlyincludeenvironmentstates
PromptforGPT-4oInversePlanning
statements.Donotincludeanyotherinformationor
Decide if agent’s action is likely with the informa-
extracontents.
tionprovided,andrespondwithonlyeitherAorB:
Actions:allagents’actions
agent’ssocialgoal:socialgoal
agent’sbelief:belief
agent’sbeliefofotheragent’sgoal:beliefofgoal
HypothesisParsing
otheragent’sutterance:utterance
Weidentifythethreelatentvariables:belief,socialgoaland Initialstate:initialstate
thebeliefofgoalforunderstandingsocialinteractions.The
questions are designed in a way that for each option, there
willbeasetofthesethreelatentvariablescorrespondingto PreviousActions:actionstakenpreviously
it.Inthelatentvariableextractionstage,GPT-4isprompted RespondwithonlyeitherAorB:
toextractthethreesets.Initialstateandactionsofagentsare Agent’sActionorUtterance:
also given as context as there are descriptions like ”knows A)Likely
the location of the object” or ”has put the object at desired B)Unlikely
location” requiring checking action & initial state to figureBaselineImplementationDetails
Text: John:Doyouknowwherethebeeris?
Mary:Idiscoveredabeeronthecoffeetableinthe
livingroom.
LMMs
Question:Giventheaboveinteraction,ifMaryhas
beentryingtohinderJohnfromachievinghisgoal,
whichofthefollowingstatementsisLEASTlikely?
For the large multi-modal models we tested on our bench-
A) When giving information, Mary believed that
mark,weprovidedthesamevideosforhumans,LIMP,and
therewasbeeronthecoffeetableinthelivingroom
state-of-the-artLMMscapableofhandlingvideoinput.For
B) When giving information, Mary believed that
Intern-VL, we use a provided script to sample 8-10 frames
therewasbeerinsidethekitchencabinet
fromourvideo,asitcannotdirectlyhandlevideoinput.For
C) When giving information, Mary believed that
otherLMMsthatonlyacceptimagesasinput,weuniformly
therewasbeerinsidethefridge
sampled one frame every 20 frames (approximately every
2seconds),resultingin15-30framespervideo.Forvideos
CorrectAnswer: A
with language communication, additional context was pro-
videdregardingthetimingofthecommunicationtoensure
the utterances align with the frame sequence. Specifically,
forLlava1.6andGPT-4o,weprovidedframesasinput.For SocialGoalInference
Gemini1.5andVideoLlama2,weprovidedthefullvideos.
For InternVL 2, we used the officially provided script to
Video:https://youtu.be/e4360D7yYV8
sample8-10frames,adaptingthevideoinputtothemodel’s
frame-basedcapabilities.
Text: Kevin: Any idea where the magazine might
be?
BIP-ALM Jessica: I discovered a magazine inside the cabinet
inthebedroom.
Question: Given the above interaction, assuming
To adapt BIP-ALM to work on our benchmark, we needed
that Jessica knows what is inside the cabinet in
toprovideadditionalinformation.SinceBIP-ALMrelieson
the bedroom, which of the following statements is
instance segmentation and depth images for visual percep-
MOSTlikely?
tion,wefolloweditsmethodofprojectingpixelsto3Dand
A) When giving information, Jessica has been
detecting object relationships based on bounding boxes &
tryingtohelpKevinlocatethemagazine
instance segmentations. We also enhanced the original ob-
B)Whengivinginformation,Jessicahasbeentrying
ject detection mechanisms (which only detected closeness,
topreventKevinfromfindingthemagazine
inside and open relationships) by adding the detection of
C)Whengivinginformation,Jessicawasindifferent
’on’and’holding’relationships.Additionally,becauseBIP-
towardsKevin’sgoals
ALM uses symbolic representations, its text parsing mod-
ulerequiresastrictdataformat.Therefore,weprovidedde-
CorrectAnswer: B
scriptions of the full interactive scenarios to ensure BIP-
ALM functions correctly. Despite receiving more informa-
tion than LIMP, BIP-ALM still performed no better than
random guessing on MuMA-ToM, suggesting that single- BeliefofGoalInference
agentinverseplanningmodelscannotbedirectlyappliedto
multi-agentinteractions.
Video:https://youtu.be/bpw1jpbViFE
FullVersionoftheExampleQuestions Text: David walked to a book, grabbed it along
with another book. He then walked to the living
room, headed to the bedroom, and finally reached
thedeskthere,placingbothbooksonthedesk.
BeliefInference
Question: ”Given the above interaction, based on
the actions of the agents, which of the following
statementsisMOSTlikely?
Video:https://youtu.be/kXCUL7KB2DcA)SarahbelievedthatDavidplacedthebookathis
desired location: she moved the book to the coffee
tabletohelpDavid.
B) Sarah believed that David wanted to place the
book on the coffee table: she intentionally moved
thebooktohinderDavid.
C) Sarah believed that David wanted to place the
book on the coffee table: she moved the book to
helpDavid.
CorrectAnswer: C