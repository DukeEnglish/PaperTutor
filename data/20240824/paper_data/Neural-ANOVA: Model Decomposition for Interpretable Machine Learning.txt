Neural-ANOVA: Model Decomposition for Interpretable Machine Learning
SteffenLimmer,SteffenUdluft,ClemensOtte
1SiemensAG,Technology,Munich,Germany
steffen.limmer@siemens.com
Abstract The functional analysis of variance (ANOVA) decompo-
sition addresses these challenges by separating interaction
The analysis of variance (ANOVA) decomposition offers a
effects in order to gain deeper insights into the effects and
systematic method to understand the interaction effects that dependencies between input variables and output variable,
contributetoaspecificdecisionoutput.Inthispaperweintro-
owingtoitsabilitytodecomposecomplexrelationshipsinto
duceNeural-ANOVA,anapproachtodecomposeneuralnet-
works into glassbox models using the ANOVA decomposi- lower-ordereffects.TheANOVAdecompositionhasproven
tion.Ourapproachformulatesalearningproblem,whichen- valuable in various industrial domains such as modeling of
ablesrapidandclosed-formevaluationofintegralsoversub- batteries (Adachi et al. 2023) and fluid flows (Yang et al.
spacesthatappearinthecalculationoftheANOVAdecompo- 2012).
sition.Finally,weconductnumericalexperimentstoillustrate A primary challenge in computing the ANOVA decom-
theadvantagesofenhancedinterpretabilityandmodelvalida-
positionarisesfromtheneedtoevaluatehigher-dimensional
tionbyadecompositionofthelearnedinteractioneffects.
integrals over subspaces of the input domain. Often, this
problem is addressed by numerical approximation tech-
Introduction niquesorbyrestrictingtheapproximationspacetorandom-
Deploying machine learning models for regression or con- forests (Hutter, Hoos, and Leyton-Brown 2014) or spline
troltasksinindustrialsettingsoftenentailsmeetingspecific functions (Potts and Schmischke 2021), for which efficient
certificationrequirements.Theserequirementscanvaryde- integration techniques are available. However, each of the
pendingontheapplicationdomainandthecriticalityofthe latter introduces an error due to approximation, model bias
task,andmayultimatelydeterminewhetheraparticularma- oradmitslimitedexpressivityforagiventask.
chinelearningmodelcanbeused.Ensuringcompliancemay In this study, we introduce a novel method for applying
involve testing the model against a series of cases curated the ANOVA decomposition based on standard neural net-
bydomainexpertsorconductingcomprehensiveevaluations works, resulting in models that are more interpretable and
underadverseoperatingconditionstoconfirmthatthemodel suitableforindustrialmachinelearningapplications.Were-
accuratelycapturesexpectedinteractioneffects. fertothesemodelsasNeural-ANOVAmodels.Ourkeycon-
In addition to certification, machine learning models in- tributionsareasfollows:
tended for industrial use must often satisfy robustness and
explainability criteria. A challenge in this context may be
1. We introduce a novel learning formulation that enables
handling missing data, which can arise from various issues
rapid and closed-form evaluation of integrals over sub-
such as sensor failures, preprocessing errors, connectivity
spacesappearingintheANOVAdecompositionofneural
problems, calibration faults, or data corruption during stor-
networks.
age. Addressing missing or corrupted data is particularly
problematic for industrial machine learning models operat- 2. We demonstrate that Neural-ANOVA models include
ingatshortcycletimes(e.g.,lessthan1ms).Insuchcases, Generalized Additive Models (GAMs) as a special
advanced imputation techniques can be too slow, and sim- case, showing comparable performance across various
plermethodslikemeanormedianimputationmaynotpro- datasets. Our proposed framework supports diverse ac-
videthenecessaryperformance. tivation functions and layer sizes, utilizing only nested
Anothercriticalchallengeinvolvesensuringtransparency automaticdifferentiationandthesumofevaluations.
and providing explanations for the decision-making pro-
cessesofAIsystems.Techniquescollectivelyreferredtoas 3. Through extensive evaluations on various regression
ExplainableAI(XAI)aimtomitigatethe”blackbox”nature tasks, encompassing both synthetic test functions and
ofmodelslikeneuralnetworksbyelucidatingthedependen- real-world industrial datasets, we show that Neural-
cies that lead to specific decisions. Achieving XAI is espe- ANOVAmodelscanoutperformGAMsbyincorporating
ciallycrucialforcontrolsystemsorneuralprocessmodels, appropriatehigher-orderinteractions.
wherecomprehendingthedecisionsisessential.
4202
guA
22
]LM.tats[
1v91321.8042:viXra1.0
0.8
00 .. 46 f(x,x) 12
0.2
0.0
0.0 1.0
0.2 0.8
0.4 0.6
x1 0.6 0.8 0.20.4 x2
1.0 0.0
Figure 1: Neural-ANOVA decomposition. The original data is approximated by the mixed derivative of a neural network
(NN).Aclosed-formANOVAdecompositionisobtainedbydecomposingthetrainedNNintolower-dimensionalsubnetworks
NN (x ).Thesesubnetworksarederivedthroughclosed-formevaluationofintegralsoversubspaces.
S S
RelatedWork ANOVADecomposition
GeneralizedandNeuralAdditiveModels ThefunctionalANOVAdecomposition(HoeffdingandRob-
Generalized Additive Models (GAMs) (Hastie 2017) are a bins 1948; Sobol 2001; Hooker 2004) is a statistical tech-
powerfulandversatileapproachformachinelearningprob- nique for the dimension-wise decomposition of a square-
lems. They extend Generalized Linear Models (GLMs) by integrable function f : K R into a sum of lower-
X →
incorporatingnon-linearrelationshipsbetweenfeaturesand dimensionalfunctionsf accordingto
S
the target variable through flexible shape functions. GAMs (cid:88)
f(x)= f (x ). (3)
areapplicabletobothregressionandclassificationtasksand
S S
have been successfully used in various domains such as
S⊆K
healthcareorfinance(Hegselmannetal.2020;Berg2007). Here,eachfunctionf onlydependsonasubsetofvariables
AkeyadvantageofGAMsistheirinterpretability,which indexed by the set S and the sum ranges over all 2K
S ⊆ K
stemsfromtheirstructureofunivariateinteractions subsetsof := 1,...,K .
K { }
K A specific construction and algorithm was proposed in
(cid:88)
f(x)=f 0+ f k(x k), (1) (Hooker 2004; Kuo et al. 2010), necessitating the compu-
i=k tationofseveralmultidimensionalintegralsoftheform
orincludingalsobivariateinteractions
(cid:90)
K K K (cid:88)
(cid:88) (cid:88)(cid:88) f (x )= f(x)dx f (x ), (4)
f(x)=f + f (x )+ f (x ,x ). (2)
0 k k kl k l S S K−|S| K\S − ⊊ U U
k=1 k=1l<k X U S
The influence of each feature on the prediction can be wherefirsttermrepresentsanintegraloverasubsetofvari-
comprehensively understood by visualizing its correspond- ables, while the second term subtracts all proper subsets in
ing shape functions. Various methods are available for fit- amannersimilartobackfitting.Theresultingcomputational
tingGeneralizedAdditiveModels(GAMs).Onetraditional algorithmisdetailedinAlg.1.Usingthisapproach,onecan
methodisbackfitting(BreimanandFriedman1985),which demonstrate that all terms f are orthogonal with respect
iterativelyupdatesthecomponentsofthemodelbysequen- to the inner product f,g
=S(cid:82)
f(x) g(x) dx. Addition-
⟨ ⟩ ·
tially refitting them. Another common approach involves ally,thisconstructionexhibitsthefavorablepropertythatthe
spline-based regression (Wahba 1990). More recently, sev- functionalvariance
eral machine learning approaches have been proposed that (cid:90) (cid:18)(cid:90) (cid:19)2
leverageconventionalgradientdescentalgorithms.Notably, σ2 = f2(x)dx fdx (5)
Neural Additive Models (NAMs) (Agarwal et al. 2021), −
use neural networks to represent the shape functions and can be decomposed into the sum of individual component
are trained using standard stochastic gradient descent tech- variances
niques. However, the authors note some considerations (cid:90)
(cid:88) (cid:88)
when using NAMs. Computationally, the optimization pro- σ2 = σ2 = f2(x )dx . (6)
cesscanbechallenginganddemandscarefulselectionofhy- S S S S S S
perparametersandapplicationofregularizationtechniques. Furthermore, it can be shown that the decomposition is
Furthermore, choosing the appropriate representation for minimalinthesensethatnounnecessarytermsarebeingin-
shapefunctionsiscrucialtoavoidoverfittingorunderfitting troducedinthedecomposition.Toillustratethisminimality,
thedata,necessitatingcarefulconsiderationandexperimen- considerafunctionf(x ,x )=2x wheretheANOVAde-
1 2 1
tation. compositionensuresthatnounfavorablenon-minimaltermssuch as f(x ,x ) = x x +(x +x ) are introduced
(Kuoetal.21 0102
).
1 − 2 1 2 Input:f ∈L2([0,1]K)
Theminimalitypropertyalsoallowstodefinemeaningful Output:functions f ,variances σ 2
(cid:82) { S}S⊆K { S}S⊆K
dimensionalities for a function. For instance, one such di- f := f(x)dx;σ :=0;
K
mensioncanbedescribedasthesuperpositiondimensionof ∅ X ∅
for , = )do
afunction,definedas S ⊆K S ̸ (cid:82)∅ (cid:80)
f (x ):= f(x)dx f (x );
f(x)=(cid:88) f (x ), (7) σS 2 :=S(cid:82) fX2K (− x|S| )dx ; K\S− U⊊ S U U
|S|≤ds S S
end
S X|S| S S S
wherethevariancedecomposesaccordingto
(cid:88)
σ2 =σ2. (8)
σ2 :=(cid:82) XKf2(x)dx −(cid:0)(cid:82) XKf(x)dx(cid:1)2 ≡(cid:80) S⊆Kσ S2;
|S|≤ds S Algorithm1:ANOVAdecompositionoff proposedby
(Kuoetal.2010).
Inotherwords,ifafunctionf hasaneffectivesuperposition
dimension d , it implies that interactions involve no more
s
thand variables.Furthermore,ifafunctionhasaneffective
s
and partial fractions. Closed-form solutions for general an-
superpositiondimensionof1,itindicatestheexistenceofan
tiderivatives, i.e., indefinite integrals, are limited to a small
idealregressorintheformofaGeneralizedAdditiveModel
class of functions and often involve complex algorithms
(GAM).
such as the Risch algorithm (Risch 1969). Numerical inte-
Thetruncationdimensionisanothermeaningfulquantity
grationmethods,includingRiemannsums,quadratures,and
thatissaidtoholdwithdimensiond ifthereexistsasetof
t
MonteCarlomethods(Owen2023),arecommonlyusedin
truncationvariables with =d suchthat
t
T |K\T| practice.Thesemethodstypicallyrequireatradeoffbetween
(cid:88)
f(x)= f (x ), (9) thenumberofsamplesandaccuracy.
S⊆K\T S S Neuralnetworks,beinguniversalfunctionapproximators,
with can also be utilized for analytical integration within the
(cid:88) σ2 =σ2. (10) framework of automatic integration (Lindell, Martel, and
Wetzstein 2021). This technique involves training a neural
S
S⊆K\T
network to approximate the antiderivative so that integrals
Usingthetruncationdimension,wecanidentifysetsofrel-
can be obtained by evaluating the trained network at the
evant andirrelevant variables.Additionally, wecan use the
boundary points of the integration domain. The approach
truncated sum (9) to approximate the function if the vari-
relies on taking derivatives of the neural network, applied
ables in the set are unavailable, e.g., due to sensor cor-
T repeatedlytoallinputcoordinatesandsubsequentlyusedto
ruption or processing errors. However, in such scenarios,
fit the training data. Using this method enables the compu-
weshouldnotexpectaperfectapproximation,meaningthe
equalitiesin(9,10)willnothold.
tationofanydefiniteD-dimensionalintegralusing2D eval-
uations of a neural network. It has inspired a range of ap-
Various methods for numerically approximating the
plications, such as neural radiance fields (Gao et al. 2022),
ANOVA decomposition have been introduced in the lit-
tomography(Ru¨ckertetal.2022),pathlossprediction(Lim-
erature. These methods include approaches based on ran-
mer,Alba,andMichailow2023)andneuralpointprocesses
dom forests (Hutter, Hoos, and Leyton-Brown 2014) and
(ZhouandYu2024).
orthonormal systems utilizing polynomial or Fourier ba-
sis functions (Potts and Schmischke 2021). Each approach
NeuralANOVADecomposition
incorporates different model-specific approximation tech-
niques for evaluating the integral (4). The effectiveness of Inthissection,wepresentourmaincontribution,whichpro-
theseapproximationschemescanbeconstrainedbytheex- vides a rapid and closed-form evaluation of integrals over
pressivityofthechosenmodelorthemaximumorderofin- subspacesofthetypegivenby(4)intheANOVAdecompo-
teractions that can be efficiently included in the numerical sition,utilizingneuralnetworks.
approximationprocess.
Moreover,integratinganumericalapproximationscheme BivariateExample
into the training loop of a machine learning model is chal- We begin by demonstrating the fundamental process of
lenging. This difficulty arises from the need to balance the automatic integration using a sample bivariate function,
number of required evaluations with the acceptable level f(x ,x ), to emphasize the differences in the training ap-
1 2
of approximation error. For example, (Owen 2023) report proach. Conventional neural network training typically in-
needing approximately ten thousand function evaluations volvesminimizingalossfunctionoftheform
t do ima ec nh si ie ov ne ala sn etta ic nc ge up sta inb gle qua ap sp ir -o Mx oim nta eti Con arle orr io nr tegin rata ionfi .ve- r(θ)=(cid:88) ϕ(cid:16) f(x(i),x(i)) NN(θ,x(i),x(i))(cid:17) , (11)
1 2 − 1 2
i
AutomaticIntegration
where ϕ denotes an appropriate loss function, such as the
Analytical integration is generally considered more chal- absoluteerrororsquarederror.
lengingthandifferentiation.Variousstrategiesforexactin- In the proposed method, we aim to fit samples of a
tegration include variable substitution, integration by parts, given function f(x ,x ) while simultaneously calculating
1 2integrals over the input domain. The work (Lindell, Mar- 3-dimensionalcaseby
tel,andWetzstein2021)suggeststraininganeuralnetwork
NN(θ,x 1,x 2) by differentiating the network with respect (cid:90) u1 f(x 1,x 2,x 3)dx 1 = d d NN(x)(cid:12) (cid:12) (cid:12) (20)
t po ara tl il alit ds ei rn ip vu att ivc eo .o Trd hi ena trte as in, is np gec pi rfi oc ca el sly sie nv va ol lu va et sin mg ii nts imm izix ined
g
l1 dx 2dx 3 (cid:12) x1∈(l1,u1)
andintegralsoverasubsetoftwovariables(e.g.x ,x )can
alossfunctiondefinedas 2 3
beobtainedby
r(θ)=(cid:88) ϕ(cid:16) f(x(i),x(i)) d d NN(θ,x(i),x(i))(cid:17) . (cid:90) (cid:12)
i 1 2 − dx 1dx 2 1 2 f(x 1,x 2,x 3)dx 2dx 3 = d NN(x)(cid:12) (cid:12) .
dx (cid:12)
To ensure computational efficiency, the
t( e1 r2 m) 1 x2,x3∈(l2,u2) ×(l3 (,u 23 1)
)
d d NN(θ,x ,x ) can be compiled just-in-time
dx1dx2 1 2 SummaryofAlgorithm
and evaluated during the training process using standard
techniquesinautomaticdifferentiation. We now present the main result of this paper: a compu-
After successful optimization, the optimized neural net- tational algorithm designed to train a neural network, de-
workparameters,denotedasθ⋆,areobtained.Integralscan notedasNN,whichallowsforaclosed-formdecomposition
then be computed by evaluating the neural network at the into lower-dimensional subnetworks NN . This method is
cornerpointsoftheintegrationdomain,[l ,u ] [l ,u ]ac- termedNeural-ANOVAandsummarizediSnAlg.2.
1 1 2 2
×
cordingto In Alg. 2, the following steps are performed in order to
calculate the required integrals of the ANOVA decomposi-
(cid:90) u1(cid:90) u2
f(x ,x )dx dx (13) tion as a signed sum of multiple evaluations of the trained
1 2 1 2
model and the functional transformation of differentiation.
l1 l2
=NN(θ,l ,l ) NN(θ,u ,l ) (14) First, the model is trained using the loss function specified
1 2 1 2
− in(16)wherethemodelisdifferentiatedw.r.t.allinputvari-
NN(θ,l ,u )+NN(θ,u ,u )
− 1 2 1 2 ables. Second, we compute the integral over the subspace
(cid:12)
:=NN(θ,x 1,x 2)(cid:12)
x1,x2∈(l1,u1)
×(l2,u2). (15) spannedbythevariablesx
Sc
(cf.(4))accordingto
(cid:90) (cid:12)
High-dimensionalGeneralization
I (x )= NN(x)dx c =
d |S|NN(x)(cid:12)
(cid:12)
Next,wepresentthegeneralizationofautomaticintegration S S Sc S dx
S
(cid:12) xSc ∈(0,1)|Sc|
t 1o fc oa rlc au fl ua nte cth ioig nh ce or- mdi pm rie sn ins gio Knal inin pt ue tgr fa el as tuth rea st aa np dpe oa nr ein taA rglg et. =(cid:88) (cid:18)
(
1)sd |S|NN(x)(cid:19)
. (22)
output, i.e., f : RK R. To this end, the neural network xSc ∈(0,1)|Sc| − dx S
NN (x):RK Ri→ strainedusingtheloss Here,thesignexponentsdenotesthemultiplicityoflower
θ
→ boundsintheevaluatedexpressionand c := thecom-
S K\S
plementof overthefullindexset .Inotherwords,in(22)
r(θ)=(cid:88) ϕ(cid:0) f(x(i)) d d NN(θ,x(i))(cid:1) . (16) thetrainedS modelisfirstdifferentiaK tedw.r.t.thevariablesin
i − dx 1 ···dx K theactive-set andthenevaluatedatthe2 |Sc |cornerpoints
of the inactiveS -set c that are to be integrated over so that
Then,wecanestablishthefollowingrelationbetween(i) S
theresultisafunctionofonlythevariablesx .Lastly,the
the trained neural network, (ii) the general anti-derivative
Neural-ANOVA component NN is obtainedSby using the
(integral) and (iii) the definite anti-derivative (integral) by
integral calculated in (22) and sSubtracting the components
usingthefundamentaltheoremofcalculus(Mutze2004)ac-
ofallpropersubsets
cordingto
(cid:90)
(cid:88)
NN (x )= NN(x)dx c NN (x ). (23)
f(x)= dd
x
···dxd NN(x) (17) S S Sc S − U⊊
S
U U
1 K The complete resulting algorithm to obtain Neural-
⇕ ANOVA is provided in Alg. 2 where all the neural network
(cid:90)
terms can be calculated fast and in closed-form at runtime
f(x)dx=NN(x) (18)
andthevariancesσ2,σ2 canbeobtainedofflinebystandard
numericalmethodssucShasMonteCarloapproximation.
⇕ Oneapproachtocalculatethemixedpartialderivativein
(cid:90) u
f(x)dx= (cid:88) ( 1)sNN(x), (19) (16)supportedbystandardautomaticdifferentiationframe-
l − works is to apply nested differentiation. While the imple-
x ∈(l1,u1) ×···×(lK,uK)
mentation of this approach is straight forward e.g. in the
wheresdenotesthemultiplicityoflowerboundsintheeval- automatic differentiation framework JAX (Bradbury et al.
uatedexpression. 2018),itrequirestraversingtheoriginalcomputationgraph
Using this relation, we can verify that integration over a multipletimeswhichmayresultinredundantcomputations
singlevariables(e.g.x )canbeobtainedforinstanceinthe aswasnotedin(Hoffmann2016;Bettencourt,Johnson,and
1Input:Sampledfunctionf(x) ([0,1]K) S {1} {2} {3} {1,2} {1,3} {2,3}
2
∈L
Output:Nets NN ,variances σ 2 Ref. 0.314 0.442 0.0 0.0 0.244 0.0
Obtainθ ∗byt{ raininS g}S f⊆ (xK )
≈
d dK xNN θ{ (xS )}S⊆K Est. 0.305 0.439 6e−9 1e−10 0.256 8e−9
NN :=(cid:80) ( 1)sNN (x) Table 1: True sensitivities and numerical estimates calcu-
∅ x ∈(0,1)K − θ latedusingN-ANOVAfortheIshigamifunction.
σ :=0
∅
for , = do
S ⊆K S ̸ ∅
NN (x ):= d|S| NN(x) 0.2 0.2
(cid:80)S S
NN
d (x xS
)σ2
:=|xS (cid:82)c ∈(0,1)K N− N|S 2|
(x )dx
0.1 0.1
− U⊊ S U U S [0,1]|S| S S S 0.0 0.0
end 0.1 0.1
− −
σ2 :=(cid:82) (dK NN)2(x)dx NN2 (cid:80) σ2 −0.2 −0.2
[0,1]K dx − ∅ ≡ S⊆K S 0.0 0.2 0.4 x10.6 0.8 1.0 0.0 0.2 0.4 x20.6 0.8 1.0
Algorithm 2: Neural-ANOVA decomposition of f,
(a) (b)
adaptedfrom(Kuoetal.2010).
0.4
Duvenaud2019).Wehighlightthatalsomoresophisticated 0.2 0.3
m coe mth po ad res dfo tor tc ha elc nu ela st ti en dg at ph pe rom acix he sd ucp har ati sal Tad ye lr oiv ra st eiv re iesex ai ps -t −−00 00..
.0
.1
21
NN1,3 000 ... 012
proximation, which was shown to admit favorable runtime 0.00 1.00
0.25 0.75
properties for calculating higher-order derivatives (Betten- 0.50 0.50
x1 0.75 1.00 0.000.25 x3 S
court,Johnson,andDuvenaud2019).Inthispaper,wechose
(c) (d)
toretainthenestedapproachapproachasweobservesatis-
factory runtime and numerical stability up to moderate di-
Figure2:(a-c)PlotsofNN (x )for = 1 , 2 , 1,3 ,
mensionK ≤10.
and(d)sensitivitiesσ
fortSheISshigamS ifun{ cti} on{
.
} { }
S
NumericalExample
This section presents a concise numerical example for a
Finally, we can evaluate and illustrate the decomposed
commontest-functionfromsensitivityanalysis,namelythe
function (cf. Fig. 2) and obtain sensitivities using a Monte
3-dimensionalIshigami-function
CarloestimateaccordingtoAlg.2.WeseeinTab.1thatthe
f(x)=sin(x )+asin2(x )+bx4sin(x ), (24) sensitivities match well with their closed form expressions
1 2 3 1
(SobolandLevitan1999).
witha=7,b=0.1.
We normalize the input and output domain and present Experiments
thegenerateddataforx =0inFig.1.Thelossfunctionfor
3
This section presents the results of numerical experiments
trainingtheneuralnetworkisdefinedas
performed on simulation functions from the field of sensi-
(cid:88)(cid:16) d3 (cid:17)2 tivityanalysisandreal-worldindustrialapplications.
r(θ)= f(x ,x ,x ) NN(x ,x ,x ) .
1 2 3 − dx dx dx 1 2 3 Forsensitivityanalysis,weusesampleddataofthesim-
1 2 3
i ulation functions Ishigami, OTL Circuit and Piston from
(25)
the UQTestFuns-library (Wicaksono and Hecht 2023). The
Next,wefindthetermsoftheNeural-ANOVAdecomposi- training,validationandtestingdataisgeneratedbyevaluat-
tionusingthetrainednetworkaccordingto ingthefunctionusingthedefaultsamplingdistributionsand
min-max scaling of input and output domain to [0,1]. The
NN =NN(u ,u ,u ) NN(u ,l ,u ) (26)
∅ 1 2 3 − 1 2 3 primaryobjectiveoftheseexperimentsistoevaluatetheex-
NN(l 1,u 2,u 3)+NN(l 1,l 2,u 3) NN(u 1,u 2,l 3) pressivepowerandgeneralizationcapabilitiesofmixedpar-
− −
+NN(u ,l ,l )+NN(l ,u ,l ) NN(l ,l ,l ) tial derivative networks with different activation functions.
1 2 3 1 2 3 1 2 3
− Thestudyalsoexaminesfunctionpropertiessuchassuper-
(cid:12) positionandtruncationdimensions.
d (cid:12)
NN 1(x 1)= NN(x 1,x 2,x 3)(cid:12) NN As industrial datasets we consider Airfoil Self-Noise
dx 1 (cid:12) x2,x3∈(l2,u2) ×(l3,u3)− (27)∅ ( CA oS mN p) r, esC so ivm eb Sin tre ed ngC thyc (l Ce CP Sow
)
der atP asla en tst (( AC sC uP n) cia on nd
,
C No ewnc mre at ne
et al. 2007). We provide an overview of the considered
(cid:12)
d d (cid:12) datasetsinTab.2.Inallcasesthedataissplitinto60/20/20
NN 1,2(x 1,x 2)=
dx dx
NN(x 1,x 2,x 3)(cid:12)
(cid:12) ratiofortraining,validation,and,testing,respectively.
1 2 x3∈(l3,u3)
We use JAX (Bradbury et al. 2018) to implement
NN NN 1(x 1) NN 2(x 2) (28) both Neural-ANOVA and an MLP baseline. The MLP
− ∅− −
1NN 2NN
2σ/2σ
S
∅ 1 }{ 2 }{ 3 }{ 2,1 }
{
3,1 }
{
3,2 }
{serves as benchmark for evaluating the expressive power tecture where the error level is slightly higher for the N-
and noise robustness of different architectures. We ex- ANOVAnetworks.Notably,N-ANOVAmodelsutilizingthe
periment with the following standard architectures: (i) reluactivationfunctiondemonstrateasignificantlossinex-
3 layers with 32 neurons and sigmoid activation, and pressivepowerwhensubjectedtodifferentiationandtherep
ablations with (ii) 8,16,32,48 hidden neurons and activation shows promising robustness to higher noise lev-
{ }
sigmoid,relu,swish,rep activation where rep denotes els.
{ }
the rectified polynomial function. The default architecture
serves as the model for mixed partial derivative training in Dataset features samples
the N-ANOVA approach (16) for the simulation functions
Ishigami(ISH) 3 10000
ISH, CIR and PST and the MLP approach on all datasets.
OTLCircuit(CIR) 6 10000
For the ASN, CCP, and CCS datasets, we observe, simi-
Piston(PST) 7 10000
lar to (Agarwal et al. 2021), the necessity of regularization
due to the limited number of data points. For N-ANOVA, AirfoilSelf-Noise(ASN) 5 1503
CombinedCyclePowerPlant(CCP) 4 9568
ourempiricalfindingsindicatethatatwo-layerarchitecture
ConcreteCompressiveStrength(CCS) 8 1030
with rep activation, 16 neurons and ℓ -weight regulariza-
2
tionprovidessatisfactoryresultsforASNandCCP,butled
Table2:Datasetoverview.
to a small number of divergent runs, which were excluded
fromtheanalysis.Thisissuecouldpotentiallybemitigated
through more advanced hyperparameter tuning or by em-
ployingmethodsbasedoncloningthetrainedbaselineMLP. MLP N-ANOVA NAM
We also report results for Neural Additive Models
trainingtime 121.3 857.6 −∗
(NAMs),whichconsistofthreelayerswith32neuronsand
parameters 1345 1345 11544
reluactivation,followingtheJAXimplementation1tomain-
tain consistency with the experimental setup. In Tab. 3, we
Table 3: Comparison of avg. training time in seconds and
present a comparison of training time between N-ANOVA
numberoftrainableparametersforthePistondataset.
and a standard MLP and compare the model sizes of all
(*excludedduetodifferenttrainingframework)
threeapproachesintermsoftrainableparameters.Thiscon-
firms that the N-ANOVA model has an identical parameter
counttotheMLPandtheNAMarchitecturetypicallycon-
tainsmoreparametersduetotheKindependentfeaturenet- T {1} {2} {3} {1,2} {1,3} {2,3}
works. Tab. 4 shows the impact of truncating variables on
N-ANOVA 0.09 0.14 0.05 0.17 0.09 0.15
\T
the Ishigami dataset to analyze the effect of reducing input MLP 0.09 0.24 0.06 0.26 0.09 0.25
\T
variables,suchasinscenarioswithmissingvalues.
InTab.5,wereporttherootMSE(RMSE)andstandard Table4:ComparisonoftruncatingvariablesforN-ANOVA
erroronthetestset,basedon10runswithdifferentrandom using truncated sum (9) and MLP using replacement by
seeds.Thetrainingsutilizevalidationearlystoppingandare meanonIshigamidataset(RMSEover10seeds).
obtained using the adam and bfgs optimizers. We find that
MLP and N-ANOVA (all interactions) as well as NAM
and N-ANOVA (univ∞ariate interactions) perform similarly Conclusion
1
onthesimulationfunctions.Fordatasetswithasmallsample In this paper, we present an efficient method for comput-
count,NAMsdemonstrateslightlysuperiorgeneralizationin ingthefunctionalANOVAdecompositionusingneuralnet-
theunivariatesetting.Thisperformancecanbematchedby works to quantify learned interaction effects across multi-
N-ANOVA 2wherebivariateinteractionsareincluded.How- pledatasets.Wederiveanovellearningproblemfocusedon
ever,N-ANOVAshowsperformancedeteriorationforsmall computingintegralsoversubspacesessentialtotheANOVA
samplesizes,specificallyfor1030samplesinthelargestdi- decompositionanddemonstratehowthisalgorithmcande-
mension K = 8 of the CCS dataset. These results suggest compose a network by fitting the mixed partial derivative
the potential for developing mixed partial derivative archi- to the training data. Our approach is empirically validated
tecturesthatgeneralizebetterinfutureresearch. onvarioustestfunctionsfromuncertaintyquantificationand
FortheAirfoildataset,wealsodepicttheshapefunctions real-worldindustrialdatasets,confirmingtheaccuracyofthe
oftheN-ANOVAapproachandtheestimatedsensitivitiesin functional decomposition. We also show that the Neural-
Fig.3whereweseethatthemodelassignsastrongimpact ANOVAapproachcanspecializetoobtainageneralizedad-
toasmallnumberofinteractions. ditive model. The method provides a principled way to an-
Finally, Fig. 4 illustrates ablation studies on the stabil- alyzeinteractioneffects,offeringdeeperinsightsintotrain-
ityofdifferentmodelsandundervaryinglevelsofadditive ing results and the implications of using a specific trained
noise. The results indicate that the mixed partial derivative model, allowing domain experts to certify particular use
networks within the N-ANOVA framework exhibit similar cases.Furtherresearchmayaddressmoretayloredarchitec-
scaling and robustness behavior to a standard MLP archi- tures that maintain higher expressive power or generaliza-
tionunderdifferentiation.Ourimplementationwillbemade
1https://github.com/Habush/nam jax availablewiththepaper.MLP N-ANOVA N-ANOVA N-ANOVA N-ANOVA N-ANOVA NAM
∞ 4 3 2 1
1.7E-04 1.2E-04 1.2E-04 1.2E-04 1.2E-04 5.06E-02 5.08E-02
ISH
±0.5E-04 ±0.2E-04 ±0.2E-04 ±0.2E-04 ±0.2E-04 ±0.04E-02 ±0.05E-02
5.8E-05 1.3E-04 1.1E-04 1.0E-04 1.1E-04 1.59E-02 1.61E-02
CIR
±1.5E-05 ±0.3E-04 ±0.2E-04 ±0.2E-04 ±0.2E-04 ±0.01E-02 ±0.02E-02
5.2E-05 1.65E-04 2.52E-04 2.96E-03 1.62E-02 3.94E-02 3.86E-02
PST
±0.8E-05 ±0.2E-04 ±0.07E-04 ±0.05E-03 ±0.03E-02 ±0.04E-02 ±0.06E-02
4.4E-02 9.0E-02 9.0E-02 1.00E-01 1.19E-01 1.67E-01 1.23E-01
ASN
±0.2E-02 ±0.3E-02 ±0.3E-02 ±0.09E-01 ±0.07E-01 ±0.07E-01 ±0.02E-01
5.33E-02 5.73E-02 5.73E-02 5.74E-02 5.77E-02 5.95E-02 5.68E-02
CCP
±0.08E-02 ±0.05E-02 ±0.05E-02 ±0.05E-02 ±0.05E-02 ±0.06E-02 ±0.06E-02
7.4E-02 1.03E-01 1.03E-01 1.04E-01 1.06E-01 1.51E-01 7.1E-02
CCS
±0.2E-02 ±0.06E-01 ±0.06E-01 ±0.06E-01 ±0.06E-01 ±0.2E-01 ±0.2E-02
Table5: PerformancecomparisonofproposedN-ANOVA withvaryingsuperpositiondimensiond ,NeuralAdditiveModel
ds s
(NAM) (Agarwal et al. 2021) (i.e., d = 1) and Multi-Layer Perceptron (MLP) (d = K). The error is shown as RMSE on
s s
holdoutset(lowerisbetter).
1.0 1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0 0.0
−0.5 −0.5 −0.5 −0.5 −0.5
−1.0
0.0 0.2 0.4 0.6 0.8 1.0
−1.0
0.0 0.2 0.4 0.6 0.8 1.0
−1.0
0.0 0.2 0.4 0.6 0.8 1.0
−1.0
0.0 0.2 0.4 0.6 0.8 1.0
−1.0
0.0 0.2 0.4 0.6 0.8 1.0
x0 x1 x2 x3 x4
(a) (b) (c) (d) (e)
1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0
0.8 0.5 0.8 0.5 0.8 0.5 0.8 0.5 0.20
0.6 0.0 0.6 0.0 0.6 0.0 0.6 0.0 0.15 0.4 0.4 0.4 0.4 0.10
0.2 −0.5 0.2 −0.5 0.2 −0.5 0.2 −0.5 0.05
0.0 0.0 0.5 1.0 −1.0 0.0 0.0 0.5 1.0 −1.0 0.0 0.0 0.5 1.0 −1.0 0.0 0.0 0.5 1.0 −1.0 0.00
x0 x0 x0 x1
S
(f) (g) (h) (i) (j)
Figure3:Plotsof(a)-(e)NN (x ),(f)-(i)NN (x ),(j)sensitivitiesσ fortheAirfoildataset.
k k
S S S
activation swish model activation swish model activation swish model activation swish model
relu rep mlp relu rep mlp relu rep mlp relu rep mlp
sigmoid n-anova sigmoid n-anova sigmoid n-anova sigmoid n-anova
10−1 10−1 10−1 10−1
10−2 10−2
10−3 10−3 10−3 10−3
10−4 10−4
10−5
10 20 30 40 50
10−5
10 20 30 40 50 10−7 10−5 10−3 10−1 10−7 10−5 10−3 10−1
numneurons numneurons noisevariance noisevariance
(a) (b) (c) (d)
Figure4:Ablationon(a)-(b)trainingandtestingerrorforvaryingnumberofhiddenlayerneuronsand(c)-(d)forvaryinglevel
ofadditivetrainingnoiseandvaryingactivationfunctionsonthePistondataset.
0NN
1x
esmrniart
1,0NN
1NN
2x
esmrtset
2,0NN
2NN
4x
esmrniart
4,0NN
3NN
4x
esmrtset
4,1NN
4NN
2σ/S2σ
∅}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},,,,,1234234344234344344434444401234{{{{{,,,,,,,,,,,,,,,,,,,,,,,,,,00001112231112232233223333{{{{{{{{{{,,,,,,,,,,,,,,,,0000001112111222{{{{{{{{{{,,,,,,000011{{{{{,0{References IEEE33rdInternationalWorkshoponMachineLearningfor
SignalProcessing(MLSP),1–5.IEEE.
Adachi, M.; Kuhn, Y.; Horstmann, B.; Latz, A.; Osborne,
M.A.;andHowey,D.A.2023. Bayesianmodelselectionof Lindell, D.; Martel, J.; and Wetzstein, G. 2021. Autoint:
lithium-ionbatterymodelsviaBayesianquadrature. IFAC- Automatic integration for fast neural volume rendering. In
PapersOnLine,56(2):10521–10526. ProceedingsoftheIEEE/CVFConferenceonComputerVi-
sionandPatternRecognition,14556–14565.
Agarwal,R.;Melnick,L.;Frosst,N.;Zhang,X.;Lengerich,
B.; Caruana, R.; and Hinton, G. E. 2021. Neural additive Mutze,U.2004. TheFundamentalTheoremofCalculusin
models:interpretablemachinelearningwithneuralnets. In Rˆn. Technicalreport,UniversityofTexas.
Proceedingsofthe35thInternationalConferenceonNeural Owen,A.2023. PracticalQuasi-MonteCarlo. Draftavail-
InformationProcessingSystems. able at https://artowen. su. domains/mc/practicalqmc. pdf.
Asuncion,A.;Newman,D.;etal.2007. UCImachinelearn- Accessed14thAugust.
ingrepository. Potts,D.;andSchmischke,M.2021. Interpretableapproxi-
Berg, D. 2007. Bankruptcy prediction by generalized ad- mationofhigh-dimensionaldata. SIAMJournalonMathe-
ditive models. Applied Stochastic Models in Business and maticsofDataScience,3(4):1301–1323.
Industry,23(2):129–143. Risch, R. H. 1969. The problem of integration in finite
Bettencourt, J.; Johnson, M. J.; and Duvenaud, D. 2019. terms. TransactionsoftheAmericanMathematicalSociety,
Taylor-mode automatic differentiation for higher-order 139:167–189.
derivatives in JAX. In Program Transformations for ML Ru¨ckert,D.;Wang,Y.;Li,R.;Idoughi,R.;andHeidrich,W.
WorkshopatNeurIPS2019. 2022. Neat: Neural adaptive tomography. ACM Transac-
Bradbury,J.;Frostig,R.;Hawkins,P.;Johnson,M.J.;Leary, tionsonGraphics(TOG),41(4):1–13.
C.; Maclaurin, D.; Necula, G.; Paszke, A.; VanderPlas, J.; Sobol, I. M. 2001. Global sensitivity indices for nonlin-
Wanderman-Milne,S.;andZhang,Q.2018. JAX:compos- ear mathematical models and their Monte Carlo estimates.
abletransformationsofPython+NumPyprograms. Mathematics and computers in simulation, 55(1-3): 271–
280.
Breiman, L.; and Friedman, J. H. 1985. Estimating opti-
maltransformationsformultipleregressionandcorrelation. Sobol, I. M.; and Levitan, Y. L. 1999. On the use of vari-
Journal of the American statistical Association, 80(391): ance reducing multipliers in Monte Carlo computations of
580–598. a global sensitivity index. Computer Physics Communica-
tions,117(1):52–61.
Gao, K.; Gao, Y.; He, H.; Lu, D.; Xu, L.; and Li, J. 2022.
NeRF:Neuralradiancefieldin3Dvision,acomprehensive Wahba, G. 1990. Spline Models for Observational Data.
review. arXivpreprintarXiv:2210.00379. CBMS-NSF Regional Conference Series in Applied Math-
ematics. Society for Industrial and Applied Mathematics.
Hastie,T.J.2017. Generalizedadditivemodels. InStatisti-
ISBN9780898712445.
calmodelsinS,249–307.Routledge.
Wicaksono, D.; and Hecht, M. 2023. UQTestFuns: A
Hegselmann,S.;Volkert,T.;Ohlenburg,H.;Gottschalk,A.;
Python3libraryofuncertaintyquantification(UQ)testfunc-
Dugas, M.; and Ertmer, C. 2020. An evaluation of the
tions. JournalofOpenSourceSoftware,8(90):5671.
doctor-interpretability of generalized additive models with
interactions. In Machine Learning for Healthcare Confer- Yang, X.; Choi, M.; Lin, G.; and Karniadakis, G. E. 2012.
ence,46–79.PMLR. AdaptiveANOVAdecompositionofstochasticincompress-
ible and compressible flows. Journal of Computational
Hoeffding,W.;andRobbins,H.1948. Thecentrallimitthe-
Physics,231(4):1587–1614.
orem for dependent random variables. Duke Mathematical
Journal,15(3):773. Zhou, Z.; and Yu, R. 2024. Automatic integration for spa-
tiotemporalneuralpointprocesses. AdvancesinNeuralIn-
Hoffmann, P. H. 2016. A hitchhiker’s guide to automatic
formationProcessingSystems,36.
differentiation. NumericalAlgorithms,72(3):775–811.
Hooker, G. 2004. Discovering additive structure in black
box functions. In Proceedings of the tenth ACM SIGKDD
internationalconferenceonKnowledgediscoveryanddata
mining,575–580.
Hutter, F.; Hoos, H.; and Leyton-Brown, K. 2014. An ef-
ficient approach for assessing hyperparameter importance.
InInternationalconferenceonmachinelearning,754–762.
PMLR.
Kuo, F.; Sloan, I.; Wasilkowski, G.; and Woz´niakowski, H.
2010. Ondecompositionsofmultivariatefunctions. Mathe-
maticsofcomputation,79(270):953–966.
Limmer,S.;Alba,A.M.;andMichailow,N.2023. Physics-
informed neural networks for pathloss prediction. In 2023