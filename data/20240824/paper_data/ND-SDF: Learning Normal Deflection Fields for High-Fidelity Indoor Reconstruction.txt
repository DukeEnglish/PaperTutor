ND-SDF: LEARNING NORMAL DEFLECTION FIELDS
FOR HIGH-FIDELITY INDOOR RECONSTRUCTION
ZiyuTang1,WeicaiYe1,2,(cid:66),YifanWang2,DiHuang2,HujunBao1,TongHe2,(cid:66),GuofengZhang1
1StateKeyLabofCAD&CG,ZhejiangUniversity,2ShanghaiAILaboratory
https://zju3dv.github.io/nd-sdf/
Figure1:WepresentND-SDF,aframeworkforhigh-fidelity3Dindoorsurfacereconstructionfrom
multi-views. ShownaboveisanextractedmeshfromScanNet++.
ABSTRACT
Neural implicit reconstruction via volume rendering has demonstrated its effec-
tivenessinrecoveringdense3Dsurfaces. However,itisnon-trivialtosimultane-
ouslyrecovermeticulousgeometryandpreservesmoothnessacrossregionswith
differingcharacteristics.Toaddressthisissue,previousmethodstypicallyemploy
geometric priors, which are often constrained by the performance of the prior
models. Inthispaper, weproposeND-SDF,whichlearnsaNormalDdeflection
field to represent the angular deviation between the scene normal and the prior
normal. Unlike previous methods that uniformly apply geometric priors on all
samples,introducingsignificantbiasinaccuracy,ourproposednormaldeflection
fielddynamicallylearnsandadaptstheutilizationofsamplesbasedontheirspe-
cificcharacteristics,therebyimprovingboththeaccuracyandeffectivenessofthe
model. Our method not only obtains smooth weakly textured regions such as
wallsandfloorsbutalsopreservesthegeometricdetailsofcomplexstructures. In
addition, weintroduceanovel raysamplingstrategybasedonthe deflectionan-
gletofacilitatetheunbiasedrenderingprocess,whichsignificantlyimprovesthe
qualityandaccuracyofintricatesurfaces,especiallyonthinstructures.Consistent
improvementsonvariouschallengingdatasetsdemonstratethesuperiorityofour
method.
1
4202
guA
22
]VC.sc[
1v89521.8042:viXra1 INTRODUCTION
3DsurfacereconstructionChenetal.(2024);Yeetal.(2024b;a);Wangetal.(2024);Yeetal.(2022;
2023b);Liuetal.(2021);Lietal.(2020)aimstorecoverwatertight,dense3Dgeometryfrommulti-
viewimagesHartley&Zisserman(2003),representingasignificantresearchareaincomputervision
andgraphics. Therecoveredsurfaceshaveproveninvaluableformanydownstreamtasks,including
roboticnavigation,AR/VR,andsmartcities.
Recently, coordinate-based networks Mildenhall et al. (2021); Barron et al. (2022); Mu¨ller et al.
(2022); Martin-Brualla et al. (2021); Zhang et al. (2020); Ye et al. (2023a); Huang et al. (2024);
Mingetal.(2022)aredrawingincreasingattention,duetotheirremarkableperformanceinthetask
ofnovelviewsynthesis.Inspiredbytheimplicitrepresentationofthescene,manysubsequentworks
introducedSDFParketal.(2019)oroccupancyLietal.(2022)toparameterizethe3Dgeometry.
However, recovering high-fidelity surfaces remains challenging, as relying solely on color images
forsupervisionoftenresultsinanunderconstrainedproblem,especiallyforregionsliketextureless
wallsandceilings.PreviousmethodsYuetal.(2022b);Wangetal.(2022)haveattemptedtomitigate
thisissuebyincorporatingauxiliarysupervision. Forinstance,MonoSDFemploysmonocularcues
fromapretrainedmodelaspseudo-groundtruthtosupervisethemodel,whichpartiallyalleviatesthe
problemintexturelessareas. However,thelargeerrorsduetodomaingapsinmonocularcuesand
theinconsistenciesinview-dependentpriorguidanceoftenleadtovisibledetaillossanderroneous
surfaces,asdepictedinFigure3oftheexperimentalsection.
Noting that prior models are highly accurate in simpler regions like floors and walls but struggle
withcomplexgeometries,weintroduceND-SDFforhigh-fidelityindoorreconstruction(Figure1).
Our core innovation is the construction of a deflection field that adaptively learns the geometric
deviations between the actual scene geometry and the prior geometry derived from normal priors.
Theinherentdeviationisdefinedastheangulardifferencebetweenthetruescenenormalsandthe
priornormals. Byaligningthedeflectedscenenormalswiththenormalcues,ourmodelcanadap-
tively learn the deviation, which encourages accurate recovery of intricate structures maintaining
finedetailswithoutbeingmisledbyerroneouspriors.
In addition, we propose a novel adaptive deflection angle prior loss that leverages prior normals
fordifferentialsupervisionofhighandlow-frequencyareas,achievinganoptimalbalancebetween
smoothnessanddetail. Ourobservationsindicatethatlargeangledeviationsareprimarilyconcen-
trated in thin and fine-grained structures. Building on this insight, we introduce deflection-angle
guided optimization to proactively facilitate the recovery of detailed structures. Furthermore, we
addressasignificantbiasissueandproposedeflectionangle-guidedunbiasedrenderingtoimprove
the reconstruction of small or thin structures, as shown in Figure 1. In summary, we present the
followingcontributions:
• Weproposeanovelsceneattributefield,namednormaldeflectionfield,whichadaptively
learns the deviations between the scene and normal priors. This method aids us in dis-
tinguishing between detailed and textureless areas. Therefore, we can restore more fine
structureswhileensuringthesmoothnessandintegrityofthescene.
• Empiricallyassumingthatpriormodelsincurlargererrorsincomplexareas,weemploythe
deflection angle to discriminate between high and low-frequency regions. Consequently,
weproposeanoveladaptivedeflectionanglepriorlossthatdynamicallyadjuststheutiliza-
tionofdistinctcues,therebyachievingabalancebetweencomplexstructuresandsmooth
surfaces. Furthermore,weutilizethedeflectionangletoguideraysamplingandphotomet-
ricoptimization,facilitatingtherestorationoffiner-grainedstructures.
• To address the inherent bias issue of neural surface rendering, we integrate the unbiasing
method Zhang et al. (2023) to implement an adaptive, unbiased rendering strategy. This
approachfacilitatestherecoveryofextremelythinstructureswithoutcompromisingscene
fidelity. Our method outperforms previous approaches significantly in indoor reconstruc-
tionevaluations,andthissuperiorityisvalidatedthroughextensiveablationexperiments.
22 RELATED WORK
Neural Surface Representation of 3D scenes Recently, the representation of 3D scenes using
neural fields has gained popularity due to their expressiveness and simplicity. NeRF-type ap-
proaches Mildenhall et al. (2021); Barron et al. (2022); Mu¨ller et al. (2022); Fridovich-Keil et al.
(2022);Sunetal.(2022)haveproposedencodingcoordinate-baseddensityandappearanceofscenes
byutilizingsimplemultilayerperceptrons(MLPs)andexplicitstructuressuchasvoxelgrids,result-
ing in photorealistic novel view synthesis. However, these approaches fail to accurately recover
surfaces due to the lack of constraints on density. To address this issue, subsequent works, such
as VolSDF Yariv et al. (2021) and Neus Wang et al. (2021), implicitly represent signed distance
functions (SDFs) and employ a SDF-density conversion function to encourage precise surface re-
construction. Other methods Li et al. (2023); Rosu & Behnke (2023); Wang et al. (2023); Yariv
et al. (2023); Fu et al. (2022); Zhang et al. (2023) have also been proposed, introducing different
representationsandoptimizationtechniquestofurtherenhancereconstructionqualityandefficiency.
Nevertheless,theseaforementionedmethodsstruggletohandleindoorsceneswithalargenumberof
low-frequencyareas(e.g.,wallsandfloors),asphotometriclossbecomesunreliableinsuchregions.
Neural reconstruction in Indoor Environments Due to the complex layouts of indoor scenes,
additional auxiliary data are required for reasonable reconstruction. Manhattan-SDF Guo et al.
(2022)employstheManhattanassumptionandsemanticpriorstojointlyregularizetexturelessre-
gions such as walls and floors. HelixSurf Liang et al. (2023) achieves intertwined regularization
iterativelybycombiningneuralimplicitsurfacelearningwithPatchMatch-basedmulti-viewstereo
(MVS) Barnes et al. (2009) robustly and efficiently. Other works Yu et al. (2022b); Wang et al.
(2022)proposeutilizingmonocularpriorsfrompretrainedmodelstoachievesmoothandcomplete
reconstruction. However, directly applying normal or depth priors Yu et al. (2022b) may lead to
undesirable reconstruction results due to the unreliable nature of these priors. NeuRis Wang et al.
(2022)filtersoutunreliablepriorsbasedontheassumptionthatareaswithrich2Dvisualfeatures
aremoreerror-prone. H2O-SDFParketal.(2024)simplyutilizesprioruncertaintytore-weightthe
normalpriorloss. Nevertheless,bothofthesemethodsexhibitweakgeneralizationcapabilities. For
example, the visual feature hypothesis Wang et al. (2022) struggles to differentiate between rich-
texturedplanarareas. Similarly,prior-guidedre-weightingParketal.(2024)isfurtherconstrained
bythedomaingapbetweenthepriormodelandthescene. Ontheotherhand,DebSDFXiaoetal.
(2024)effectivelydesignsanuncertaintyfieldbasedonaprobabilisticmodeltoguidethelossfunc-
tion,leadingtomorerobustandaccuratereconstructions. Incontrasttomethodswithweakergen-
eralization capabilities and those relying on probabilistic models, our proposed method, ND-SDF,
learnsaNormalDeflectionfieldthatenablesthedynamicadaptationofpriorsbasedontheirchar-
acteristics. ThisfielddirectlymodelsgeometricallymeaningfulSO(3)residuals,resultinginhighly
detailedsurfacereconstructionswhilemaintainingsmoothnessandrobustness.
3 METHOD
The primary objective of our Normal Deviation Signed Distance Function (ND-SDF) is to recon-
structdensesurfacesfromcalibratedmulti-viewimages,withaprimaryfocusonindoorsceneswith
establishedpriors. Toachievethis,weintroduceanovelcomponentknownasthenormaldeflection
field. This field is designed to quantify and learn the deviations between actual scene geometries
andtheircorrespondingnormalpriors.Byintegratingthisfield,ourmethodcanadaptivelysupervise
bothhighandlow-frequencyregionsinthescene.Thisadaptivesupervisioniscrucialforpreserving
finedetailswhileensuringoverallsurfacesmoothness.
TheoperationalframeworkofourapproachisdepictedinFigure2.Insubsequentsections,wedelve
intoacomprehensivediscussionontheimplementationofthenormaldeflectionfield. Additionally,
weexplorevariousstrategiesthatleveragethisfieldtoenhancethefidelityofsurfacedetails,thereby
promotingamoreaccuratesurfacereconstruction.
3.1 PRELIMINARIES
VolumeRenderingNeRFassumesthatarayr(t)=o+tvisemittedfromviewpointoindirection
v, where t denotes the distance from the viewpoint. N points are sampled on the ray, i.e. x =
i
3Figure2: Overviewofourmethod. Weutilizemulti-resolutionhashgridsγ asscenerepresenta-
L
tion. ThecoreofND-SDFisthenormaldeflectionfield. Werepresentdeflectionwithquaternions,
whicharepredictedbythedeflectionnetwork(denotedasf ). Wealignthedeflectednormalswith
d
thepriornormalstolearnthedeviationbetweenthesceneandthepriors.Todistinctlysupervisehigh
andlow-frequencyareas,weemployanadaptivedeflectionanglepriorloss,ensuringbothsmooth-
nessanddetail. Furthermore,weutilizethedeflectionangle∆θ todistinguishcomplexstructures,
enablingangle-guidedsamplingandcolorlosstofacilitateintricatesurfacedetails. Lastly,wecom-
binetheunbiasedrenderingmethodZhangetal.(2023)toensurethegenerationofextremelythin
structuresindoors.
o+t v,i ∈ {1,2...N}. Giveneachpoint’svolumedensityσ andcolorc ,thecoloroftheraycan
i i i
besynthesizedusingvolumerenderingtechniquesas:
N i−1
Cˆ(r)=(cid:88)
T α c , α =1−exp(−σ δ ),T =
(cid:89)
(1−α ), (1)
i i i i i i i j
i=1 j=1
whereαistheopacityofi-thraysegment,T denotesthetransmittancerate,δ =t −t denotes
i i i−1
distancebetweenadjacentsamples.
A color loss supervising the rendered color and ground-truth color is utilized to optimize the net-
work:
L = (cid:88) ∥Cˆ(r)−C(r)∥ , (2)
color 1
r∈R
whereRisthesampledrayset.
SDF-inducedVolumeRenderingTheriseofneuralsurfacereconstructionisattributedtomethods
like VolSDF and Neus, which propose specific SDF to volume density representation, allowing
optimizationwithvolumerendering. Thefinalsurface,denotedasS,isequivalenttothezerolevel-
setoftheimplicitdistancefield,i.e.,S ={x∈R3|s(x)=0},wheres(x)istheSDFvalue.
We follow VolSDF, employing the Laplace Cumulative Distribution Function(CDF) to model the
relationshipbetweenSDFandvolumetricdensity:
 (cid:16) (cid:17)
1  1 exp −s(x) ifs(x)≤0
2β β
σ(x)= βΨ β(−s(x))=
 1 − 1
exp(cid:16) s(x)(cid:17)
ifs(x)>0
, (3)
β 2β β
where Ψ denotes the Laplace CDF, β denotes the variance. The depth Dˆ(r) and normal Nˆ(r) of
β
thesurfaceintersectionpointissynthesizedusingvolumerenderingtechnology:
N N
Dˆ(r)=(cid:88)
T α t
,Nˆ(r)=(cid:88)
T α n , (4)
i i i i i i
i=1 i=1
where n is the analytical gradient of the SDF network. Following MonoSDF Yu et al. (2022b),
i
we utilize monocular depth and normal generated by pretrained model Eftekhar et al. (2021) to
supervisetherendereddepthandnormal:
L =(cid:80) ∥wDˆ(r)+q−D(r)∥2
L
=d (cid:80)epth ∥Nˆr (∈ rR
)−N(r)∥
+(cid:13) (cid:13)1−Nˆ(r)T N(r)(cid:13)
(cid:13)
, (5)
normal r∈R 1 (cid:13) (cid:13)
1
4wherethetwocoefficients(w,q)obtainedbyleastsquarealgorithmsareutilizedtoalignthescale
betweenmonoculardepthandrendereddepth,i.e.,wDˆ +q ≈D.
WeencodescenegeometryusingInstant-NGPMu¨lleretal.(2022)γ andashallowMLPf ,thatis,
L g
(s(x)∈R3,z(x)∈R256)=f (x,PE(x),γ (x)),wherez(x)denotesthelatentgeometryfeature.
g L
Also,weutilizetheeikonaltermGroppetal.(2020);Yarivetal.(2020)toregularizetheshapeof
SDFin3Dspace:
N
L = 1 (cid:88) (∥∇s(x )∥ −1)2, (6)
eik N i 2
i=1
FollowingNeuralangeloLietal.(2023),wefurtheremploynumericalgradientstoenhancesurface
geometricconsistencyandusecurvaturelossforsmoothness. Thedetaileddefinitionsofthemare
providedintheappendix.
3.2 NORMALDEFLECTIONFIELD
Asignificantchallengeinindoor3Dreconstructionisachievingabalancebetweenthesmoothness
of surfaces and the intricacy of complex structures. Traditional approaches relying solely on pho-
tometriclosshaveproveninadequateforaccuratelycapturingsmoothareas,particularlyintexture-
deficientregions. Thesemethodsoftennecessitateauxiliarydata,suchasnormalpriors,toenhance
the reconstruction quality in textureless areas. However, the uniform application of normal priors
across different scene types can impair the recovery of complex structures. This is primarily due
to the variable reliability of these priors in diverse regions, where they may not accurately reflect
the underlying geometry. To address this issue, we propose the development of a Normal Deflec-
tion field. This field is designed to dynamically represent the deviation between the actual scene
normalsandtheprovidednormalpriors. Bydoingso,iteffectivelycircumventsthepotentialmis-
guidancefrominconsistentpriors, therebyenablingamorereliableandnuancedreconstructionof
bothsmoothandcomplexindoorstructures.
We choose quaternion as the deflection form, which is a lightweight rotation representation. The
quaternioncanbeparameterizedbyasingleMLPf :
d
q =f (x ,v ,n ,z ), (7)
i d i i i i
whereq =(q0,q1,q2,q3)isthedeflectionquaternion(normalizeddefault)ofsampledx . Quater-
i i i i i i
nionat thesurface intersection pointis synthesizedusingvolume renderingtechnology, similar to
howNeRFsynthesizescolors:
N
(cid:88)
Q(r)= T α q , (8)
i i i
i=1
WedeflecttherenderednormalusingthecomposeddeflectionquaternionQ(r):
Nˆd (r)=Q(r)⊗Nˆ(r)⊗Q−1(r), (9)
whereNˆd denotesthedeflectedrenderednormal,Q−1 denotestheinverseofQ,alsoknownasthe
conjugate,and⊗isaquaternionmultiplicationoperation. Sinceaquaternioncanberepresentedin
trigonometricform, i.e. Q = cosθ +sinθ(u1i,u2j,u3k), thisoperationis torotatetherendered
2 2
normalaroundthequaternionaxisu=(u1,u2,u3)byθdegrees.
The deviation between the scene and priors is learned by minimizing the difference between the
deflectedrenderednormalandthepriornormal. Thus,wedefinethedeflectednormalloss:
Ld =
(cid:88) ∥Nˆd(r)−N(r)∥ +(cid:13) (cid:13)1−Nˆd(r)TN(r)(cid:13)
(cid:13) , (10)
normal 1 (cid:13) (cid:13)
1
r∈R
3.3 ADAPTIVEDEFLECTIONANGLEPRIORLOSS
Thelearneddeviationisdefinedasthedeflectionangle(∆θ)oftherenderednormal:
∆θ
=arccos(Nˆ(r)·Nˆd
(r)), (11)
5where∆θ ∈[0,π],Empirically,thedeviationbetweenthesceneandpriorsincreasesasthestructure
becomes more complex. It leads to an intuitive conclusion that the deflection angle is small in
smoothareasandlargeinhigh-frequencyareas. Theadaptivedeflectionanglenormalpriorlossis
proposedtodynamicallyadjusttheutilizationofdifferingpriorsbasedontheircharacteristics:
Lad = (cid:80) gd(∆θ)Ld (Nˆd (r),N(r))+
normal r∈R normal , (12)
g(∆θ)L (Nˆ(r),N(r))
normal
wheregdandgaremodulationfunctionsthatadjusttheweightofthedeflectedandoriginalnormal
losstermbasedonthedeflectionangle. Wesimilarlydefineanadaptivedepthpriorloss:
Lad = (cid:88) g(∆θ)L (Dˆ(r),D(r)). (13)
depth depth
r∈R
As∆θincreases,gd(∆θ)increases,andg(θ)decreases,sinceweshouldlessapplypriorsincomplex
areas indicated by large deflection angles. The combined adaptive normal and depth prior loss is
referredtoastheadaptivedeflectionanglepriorloss(seeappendixfordetaileddefinitions).
3.4 DEFLECTIONANGLEGUIDEDOPTIMIZATION
Throughtheutilizationoftheproposedadaptivepriorloss,wedynamicallyadjusttheutilizationof
various priors, significantly enhancing the quality of reconstruction. However, this alone is insuf-
ficientforgeneratingmorecomplexstructures. Fundamentally,merelylearningthedeviationdoes
notendowthecapabilitytoreconstructadditionaldetails. Recognizingthatlargedeflectionangles
indicatecomplexareas,weintroducethreedeflectionangleguidedoptimizationmethodsdesigned
tofacilitatetherecoveryofthinnerandmorefine-grainedstructures.
Deflectionangleguidedsampling. Texturelessareas,suchaswalls,constituteasignificantpor-
tion of indoor scenes. These areas converge rapidly by utilizing priors. To prevent continuous
oversamplingofwell-learnedsmoothregions,weproactivelysamplemoreraysincomplexareasto
capture finer details. The sampling process is guided by the deflection angles (see supplementary
fordetails).
Deflectionangleguidedphotometricloss. Tofurtherpromotetherecoveryofdetails,weimpose
additionalphotometriclossoncomplexareasindicatedbythedeflectionangles. Were-weightthe
originalcolorlossguidedbythedeflectionangle,resultinginthere-weightedcolorloss:
Ld = (cid:88) w (∆θ(r))∥Cˆ(r)−C(r)∥ , (14)
color color 1
r∈R
where ∆θ(r) denotes the deflection angle of sampled ray r, and w is the re-weight function
color
guidedby∆θ.
Deflectionangleguidedunbiasedrendering. Accuratelyreconstructingthinstructures,suchas
chairlegs,remainschallenging.ThislimitationarisesfrominherentbiasissuesinSDF-inducedvol-
umerendering,manifestingintwoways:(1)AccordingtoTUVRZhangetal.(2023),thederivative
oftherenderingweight ∂w(t) isinfluencedbyvariousangledifferencesbetweenraydirectionsand
∂t
scenenormals. Thisresultsinanon-maximumvalueofwatthesurfaceintersectionpoint,reducing
surface quality. (2) For rays passing close to the object, the Laplace CDF assigns high density to
points near the surface of the ray, leading to incorrect rendering depths and normals. This issue
causes thin structures to gradually disappear during training. Therefore, it is crucial to apply an
unbiasedrenderingmethodduringreconstruction.
FollowingTUVRZhangetal.(2023),wetransformSDFtodensityusinganunbiasedfunction:
(cid:32) (cid:33)
−f (r(t ))
σ(r(t i))=αΨ β (cid:12)
(cid:12)f
g′g
(r(t
i)i )(cid:12)
(cid:12)
, (15)
wherer(t )=o+t vrepresentsasampledpointontheray,andf isthegeometrynetwork,which
i i g
predictstheSDFvalue. Thistransformationeffectivelyalleviatesbiasissues. However,weobserve
thatsimplyapplyingitresultsinthesurfacefailingtoconverge. Hence,weonlypartiallyapplyitto
thinstructuresindicatedbythelearneddeflectionangles.
6Method Acc↓ Comp↓ Pre↑ Recall↑ Chamfer↓ F-score↑
COLMAPScho¨nbergeretal.(2016) 0.047 0.235 71.1 44.1 0.141 53.7
UnisurfOechsleetal.(2021) 0.554 0.164 21.2 36.2 0.359 26.7
VolSDFYarivetal.(2021) 0.414 0.120 32.1 39.4 0.267 34.6
NeusWangetal.(2021) 0.179 0.208 31.3 27.5 0.194 29.1
NeuRISWangetal.(2022) 0.050 0.049 71.7 66.9 0.050 69.2
MonoSDFYuetal.(2022b) 0.035 0.048 79.9 68.1 0.042 73.3
HelixSurfLiangetal.(2023) 0.038 0.044 78.6 72.7 0.042 75.5
Ours 0.032 0.044 84.9 73.5 0.038 78.6
Table1: QuantitativeresultsontheScanNetdataset. Weachievestate-of-the-artperformance.
Method Auditorium Ballroom Courtroom Museum Mean
NeuRISWangetal.(2022) 0.79 0.84 2.78 1.34 1.13
MonoSDFYuetal.(2022b) 3.17 3.70 13.75 5.68 6.58
Ours 4.37 6.45 16.28 10.30 9.35
Table2: F-scoreontheTanksandTemplesdataset.
3.5 OPTIMIZATION
Theoveralllossfunctionisdefinedas:
L=Ld +λ L +λ L +λ Lad +λ Lad , (16)
color 1 eik 2 curv 3 normal 4 depth
Theweightsλ ...λ areutilizedtobalancetheimportanceoftheselossterms.
1 4
4 EXPERIMENT
Datasets We conducted experiments on four indoor datasets: ScanNet Dai et al. (2017),
ReplicaStraubetal.(2019),TanksandTemplesKnapitschetal.(2017),andScanNet++Yeshwanth
et al. (2023). ScanNet contains 1513 indoor scenes captured with an iPad, processed using the
BundleFusion algorithm to obtain camera poses and surface reconstruction. Replica is a synthetic
datasetcomprising18indoorscenes. Eachscenefeaturesdensegeometryandhighdynamicrange
textures. TanksandTemplesisalarge-scale3Dreconstructiondatasetincludinghigh-resolutionout-
doorandindoorenvironments. WefollowedthesplitsfromMonoSDFandappliedthesameeval-
uation settings. We also conduct experiments on ScanNet++, which includes 460 indoor scenes
captured using laser scanners These scenes offer high-quality dense reconstructions and images.
Fortesting,weselectedsixscenesfromScanNet++.
Implementation details Our method was implemented using PyTorch Paszke et al. (2019). The
image resolution for all scenes is 384×384. We obtained normal and depth cues using Omnidata.
Multi-resolutionhashgridswereutilizedforscenerepresentation. Boththegeometrynetworkand
color network consisted of two layers, each with 256 nodes. Our network was optimized using
AdamW Loshchilov & Hutter (2017) optimizer with a learning rate of 1e-3. The weights for loss
terms were: λ = 0.05, λ = 0.0005, λ = 0.025, λ = 0.05. Upon adequate initialization of
1 2 3 4
the deflection field, we initiated deflection angle guided sampling, photometric optimization, and
unbiased rendering. All experiments were conducted on an NVIDIA TESLA A100 PCIe 40GB,
witheachiterationsampling4×1024rays, totaling128,000trainingsteps. Thetotaltrainingtime
wasabout12hours. Ourhashencodingresolutionspannedfrom25to211across16levels,withthe
initialactivationlevelsetto8andactivationstepsto2000.
Metrics In line with prior research, we employ six standard metrics to evaluate the reconstructed
meshes: Accuracy,Completeness,ChamferDistance,Precision,Recall,andF-score. Additionally,
normalconsistencyisutilizedforevaluatingtheReplicadataset.
Baselines We compare with the following methods: (1) Classic MVS COLMAP Schonberger &
Frahm(2016);Scho¨nbergeretal.(2016);(2)NeuralimplicitmethodsincludingVolSDFandNeus;
(3)MethodsutilizingauxiliarydataincludingMonoSDFandNeuRIS;(4)Otherindoorreconstruc-
tionmethodslikeHelixSurfLiangetal.(2023).
7Figure 3: Qualitive results on ScanNet. Compared with previous state-of-the-art works, our
method produces more thin and fine-grained structures, such as the chair legs and detailed table
layouts.
4.1 COMPARISONS
ScanNet:InFigure3,wepresentqualitativeresultsandinTable1,quantitativeresultsaredisplayed.
Thesequantitativeresultsareaveragedovertheselectedfourscenes. ND-SDFsurpassesprevious
best-performingworks,achievingstate-of-the-artperformance. Specifically,weachievethehighest
F-score,whichservesasacredibleindicatorreflectingreconstructionaccuracybyconsideringboth
AccuracyandCompleteness.WhencomparedwithHelixSurfandMonoSDF(asshowninFigure3),
ourapproachaccuratelycapturesthinstructuressuchaschairlegsandlamprings. Thisunderscores
thesuperiorityofourproposeddeflectionmethods,whichsignificantlyenhancetherecoveryofthin
andfine-grainedstructuresindoors.
Method Acc↓ Comp↓ Pre↑ Recall↑ Chamfer↓ F-score↑
VolSDFYarivetal.(2021) 0.070 0.102 0.405 0.339 0.086 0.368
Baked-AngeloYuetal.(2022a) 0.152 0.039 0.543 0.718 0.095 0.614
MonoSDFYuetal.(2022b) 0.057 0.032 0.651 0.703 0.044 0.675
Ours 0.056 0.024 0.667 0.785 0.040 0.721
Table 3: Quantitative results on the ScanNet++ dataset. We select 6 scenes and calculate the
averagemetricsofthem.
Method NormalC.↑ Chamfer↓ F-score↑
UnisurfOechsleetal.(2021) 90.96 4.93 78.99
MonoSDFYuetal.(2022b) 92.11 2.94 86.18
Ours 92.85 2.57 91.6
Table4: QuantitativeresultsofReplicadataset.
Replica: Following MonoSDF Yu et al. (2022b), we used their pre-processed masks to filter out
abnormalpriors. WereportedquantitativeresultsinTable4. OurapproachachievesthehighestF-
scoreandthelowestChamferdistance,substantiallysurpassingpreviousworks. Seesupplementary
forqualitativeresults.
Tanksandtemples: WealsoconductedexperimentsontheadvancedsplitofT&T,whichprovides
several challenging large-scale indoor scenes. Quantitative results are provided in Table 2, where
ND-SDF outperforms previous methods that utilize priors. Additional qualitative results can be
foundinthesupplementarymaterial,ourmethodrecoveredsubstantialdetailedstructures.
ScanNet++: ScanNet++comprisesnumerouscomplexindoorscenes,eachofferinghigh-resolution
GT mesh and RGB data. Since no public results are available for ScanNet++, we trained some
baselines using our settings, including VolSDF Yariv et al. (2021), MonoSDF Yu et al. (2022b),
andBaked-AngeloYuetal.(2022a). Amongthese,onlyMonoSDFutilizedmonocularpriors. Our
8Method OmnidataEftekharetal.(2021) WizardFuetal.(2024)
Base 0.700 0.661
Ours 0.750 0.730
Table5: F-scoreofDifferentPriorsModels..
Method Base ND AP DO DU — Acc↓ Comp↓ Prec↑ Recall↑ Chamfer↓ F-score↑
Base ✓ × × × × — 0.046 0.046 0.576 0.640 0.046 0.606
ModelA ✓ ✓ × × × — 0.044 0.042 0.601 0.666 0.043 0.632
ModelB ✓ ✓ ✓ × × — 0.038 0.032 0.637 0.726 0.035 0.679
ModelC ✓ ✓ ✓ ✓ × — 0.040 0.030 0.631 0.740 0.035 0.681
Ours ✓ ✓ ✓ ✓ ✓ — 0.038 0.031 0.648 0.728 0.034 0.686
Table 6: Quantitative Results of Ablation for Different Modules. ‘Base’ can be considered as
MonoSDF+Neuralangelo. ‘ND’denotesapplyingthedeflectionfieldandthedeflectednormalloss
term Ld . ‘AP’ denotes the two adaptive deflection angle prior loss terms including Lad
normal normal
and Lad . ‘DO’ denotes the two deflection angle guided optimizations, namely sampling and
depth
photometricloss. ‘DU’denotesthedeflectionangleguidedunbiasedrendering.
quantitativeresultsaresummarizedinTable3,demonstratingstate-of-the-artperformanceacrossall
metrics. Foradditionalvisualizationresults,pleaserefertothesupplementarymaterial.
Figure4: VisualizationResultsoftheModelsafterAblation.
4.2 ABLATIONSTUDY
4.2.1 ABLATIONOFDIFFERENTMODULES
The learned deviations play a crucial role in locating high-frequency regions. Consequently, we
employdeflectionanglesforsampling, photometricoptimization, andunbiasedrenderingtofacil-
itate the recovery of thin and fine-grained structures. We conducted detailed ablation experiments
toassesstheeffectivenessofthesemodules,resultinginfivepost-ablationmodels: Base,ModelA,
ModelB,ModelC,andOurs. Forquantitativeresultsandcomprehensivedefinitions,pleasereferto
Table6. Figure4visuallyillustrateshowtheproposedcomponentsenhanceoverallperformance.
9AnalysisofNormalDeflectionFieldTheBasemodelstrictlyadherestomonocularcues,leading
to significant detail loss due to misguided priors in complex areas. In ModelA, we introduced the
deflection field and applied the deflected normal loss to learn deviations. The improvement in the
F-scoreto0.632(asshowninTable6)confirmsthesuperiorityofthedeviationlearningapproach.
However, we observed wrinkles (as depicted in Figure 4) in textureless regions like walls. This
issuearisesfromthelackofconstraintsondeflection,resultingininefficientutilizationofpriorsin
smoothareas. InModelB,introducinganadaptivedeflectionanglepriorlosssignificantlyenhanced
reconstructionquality,balancingbothsmoothnessanddetail.
Analysis of deflection angle guided optimization While ModelB substantially improves recon-
struction quality compared to the Base, it still struggles with finer structures. In ModelC, we in-
troduceddeflectionangleguidedsamplingandcolorlosstoemphasizesurfacedetails. Finally,we
incorporateddeflectionangleguidedunbiasedrenderingtofacilitatetherecoveryofthinstructures.
Figure 4 demonstrates that Ours recovers a wide range of complex and fine structures, including
skull models, wires, and iron swabs, surpassing the Base model. As indicated in Table 6, Ours
achievedthehighestf-score,provingtheeffectivenessoftheproposedmodulesforrestoringhigh-
fidelitysurfaces.
4.2.2 ABLATIONOFDIFFERENTPRIORMODELS
Ourmethodlearnsdeviationsbetweenthesceneandnormalpriorspredictedbyaspecificpretrained
model. Itisessentialtoexploreitseffectivenessacrossdifferentcues. Toachievethis,weemployed
thecurrentlystate-of-the-artmethodGeowizardFuetal.(2024)togeneratebothdepthandnormal
cues. Thequantitativeresults,assummarizedinTable5,demonstratethatourmethodsignificantly
enhancessurfacequalityregardlessofthecuesused. Thisexperimentdemonstratesthesuperiority
ofourmethod. Itinvariablyimprovesreconstructionaccuracyinscenarioswherepriorknowledge
isused.
5 CONCLUSION
WehavepresentedND-SDF,anovelapproachthatlearnsdeviationsbetweenthesceneandnormal
priorsforhigh-fidelityindoorsurfacereconstruction. Weproposeanadaptivedeflectionangleprior
loss to dynamically supervise areas with varying characteristics. By identifying high-frequency
regionsbasedondeflectionangles,weemployangleguidedoptimizationtogeneratethinandfine-
grained structures. Our method recovers a substantial amount of complex structures, as demon-
strated by extensive qualitative and quantitative results that confirm its superiority. ND-SDF effi-
ciently adapts to unreliable priors in challenging areas, significantly addressing the issue of detail
losscausedbylargepriorerrorsforregionswithfinestructures.
LimitationND-SDFisconfinedtoreconstructionscenariosinvolvingpriors.Ourapproachexhibits
limitedcapabilityinhandlingareaswithlesscoverage. Duetoinherentambiguitiesinobservations,
thedeflectionfieldmaylearnincorrectstructuresandsettleintolocaloptima. Wekindlyreferthe
reader to the supplementary material for more analysis and results. In the future, we may explore
multi-viewconsistencyconstraintstoenhancereconstructionquality.
10REFERENCES
Connelly Barnes, Eli Shechtman, Adam Finkelstein, and Dan B Goldman. Patchmatch: A ran-
domized correspondence algorithm for structural image editing. ACM Trans. Graph., 28(3):24,
2009.
JonathanTBarron,BenMildenhall,DorVerbin,PratulPSrinivasan,andPeterHedman. Mip-nerf
360:Unboundedanti-aliasedneuralradiancefields. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pp.5470–5479,2022.
Danpeng Chen, Hai Li, Weicai Ye, Yifan Wang, Weijian Xie, Shangjin Zhai, Nan Wang, Haomin
Liu, Hujun Bao, and Guofeng Zhang. Pgsr: Planar-based gaussian splatting for efficient and
high-fidelitysurfacereconstruction. arXivpreprintarXiv:2406.06521,2024.
Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias
Nießner. Scannet: Richly-annotated3dreconstructionsofindoorscenes. InProceedingsofthe
IEEEconferenceoncomputervisionandpatternrecognition,pp.5828–5839,2017.
Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. Omnidata: A scalable pipeline
formakingmulti-taskmid-levelvisiondatasetsfrom3dscans. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision,pp.10786–10796,2021.
Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo
Kanazawa. Plenoxels:Radiancefieldswithoutneuralnetworks. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pp.5501–5510,2022.
Qiancheng Fu, Qingshan Xu, Yew Soon Ong, and Wenbing Tao. Geo-neus: Geometry-consistent
neuralimplicitsurfaceslearningformulti-viewreconstruction. AdvancesinNeuralInformation
ProcessingSystems,35:3403–3416,2022.
Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and
XiaoxiaoLong. Geowizard: Unleashingthediffusionpriorsfor3dgeometryestimationfroma
singleimage. arXivpreprintarXiv:2403.12013,2024.
AmosGropp, LiorYariv, NivHaim, MatanAtzmon, andYaronLipman. Implicitgeometricregu-
larizationforlearningshapes. arXivpreprintarXiv:2002.10099,2020.
Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei
Zhou. Neural3dscenereconstructionwiththemanhattan-worldassumption. InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.5511–5520,2022.
Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge
universitypress,2003.
Chenxi Huang, Yuenan Hou, Weicai Ye, Di Huang, Xiaoshui Huang, Binbin Lin, Deng Cai, and
WanliOuyang.Nerf-det++:Incorporatingsemanticcuesandperspective-awaredepthsupervision
forindoormulti-view3ddetection. arXivpreprintarXiv:2402.14464,2024.
ArnoKnapitsch,JaesikPark,Qian-YiZhou,andVladlenKoltun.Tanksandtemples:Benchmarking
large-scalescenereconstruction. ACMTransactionsonGraphics(ToG),36(4):1–13,2017.
HaiLi,WeicaiYe,GuofengZhang,SanyuanZhang,andHujunBao. Saliencyguidedsubdivision
forsingle-viewmeshreconstruction. In2020InternationalConferenceon3DVision(3DV),pp.
1098–1107.IEEE,2020.
HaiLi,XingruiYang,HongjiaZhai,YuqianLiu,HujunBao,andGuofengZhang. Vox-surf: Voxel-
basedimplicitsurfacerepresentation. IEEETransactionsonVisualizationandComputerGraph-
ics,2022.
ZhaoshuoLi,ThomasMu¨ller,AlexEvans,RussellHTaylor,MathiasUnberath,Ming-YuLiu,and
Chen-Hsuan Lin. Neuralangelo: High-fidelity neural surface reconstruction. In Proceedings of
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.8456–8465,2023.
11Zhihao Liang, Zhangjin Huang, Changxing Ding, and Kui Jia. Helixsurf: A robust and efficient
neuralimplicitsurfacelearningofindoorsceneswithiterativeintertwinedregularization. InPro-
ceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.13165–
13174,2023.
Xiangyu Liu, Weicai Ye, Chaoran Tian, Zhaopeng Cui, Hujun Bao, and Guofeng Zhang. Cox-
graph: multi-robot collaborative, globally consistent, online dense reconstruction system. In
2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 8722–
8728.IEEE,2021.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101,2017.
RicardoMartin-Brualla,NohaRadwan,MehdiSMSajjadi,JonathanTBarron,AlexeyDosovitskiy,
andDanielDuckworth. Nerfinthewild: Neuralradiancefieldsforunconstrainedphotocollec-
tions. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pp.7210–7219,2021.
BenMildenhall,PratulPSrinivasan,MatthewTancik,JonathanTBarron,RaviRamamoorthi,and
RenNg. Nerf:Representingscenesasneuralradiancefieldsforviewsynthesis. Communications
oftheACM,65(1):99–106,2021.
YuhangMing,WeicaiYe,andAndrewCalway.idf-slam:End-to-endrgb-dslamwithneuralimplicit
mappinganddeepfeaturetracking. arXivpreprintarXiv:2209.07919,2022.
ThomasMu¨ller,AlexEvans,ChristophSchied,andAlexanderKeller. Instantneuralgraphicsprim-
itiveswithamultiresolutionhashencoding. ACMtransactionsongraphics(TOG),41(4):1–15,
2022.
Michael Oechsle, SongyouPeng, and Andreas Geiger. Unisurf: Unifying neural implicit surfaces
andradiancefieldsformulti-viewreconstruction. InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision,pp.5589–5599,2021.
Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove.
Deepsdf:Learningcontinuoussigneddistancefunctionsforshaperepresentation. InProceedings
oftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pp.165–174,2019.
Minyoung Park, Mirae Do, YeonJae Shin, Jaeseok Yoo, Jongkwang Hong, Joongrock Kim, and
ChulLee. H2o-sdf: Two-phaselearningfor3dindoorreconstructionusingobjectsurfacefields.
arXivpreprintarXiv:2402.08138,2024.
AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,Trevor
Killeen,ZemingLin,NataliaGimelshein,LucaAntiga,etal. Pytorch: Animperativestyle,high-
performancedeeplearninglibrary. Advancesinneuralinformationprocessingsystems,32,2019.
RaduAlexandruRosuandSvenBehnke. Permutosdf: Fastmulti-viewreconstructionwithimplicit
surfacesusingpermutohedrallattices. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pp.8466–8475,2023.
JohannesLSchonbergerandJan-MichaelFrahm. Structure-from-motionrevisited. InProceedings
oftheIEEEconferenceoncomputervisionandpatternrecognition,pp.4104–4113,2016.
JohannesLScho¨nberger,EnliangZheng,Jan-MichaelFrahm,andMarcPollefeys. Pixelwiseview
selection for unstructured multi-view stereo. In Computer Vision–ECCV 2016: 14th European
Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14, pp.
501–518.Springer,2016.
JulianStraub,ThomasWhelan,LingniMa,YufanChen,ErikWijmans,SimonGreen,JakobJEngel,
RaulMur-Artal,CarlRen,ShobhitVerma,etal. Thereplicadataset: Adigitalreplicaofindoor
spaces. arXivpreprintarXiv:1906.05797,2019.
Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast con-
vergence for radiance fields reconstruction. In Proceedings of the IEEE/CVF Conference on
ComputerVisionandPatternRecognition,pp.5459–5469,2022.
12Jiepeng Wang, Peng Wang, Xiaoxiao Long, Christian Theobalt, Taku Komura, Lingjie Liu, and
WenpingWang.Neuris:Neuralreconstructionofindoorscenesusingnormalpriors.InEuropean
ConferenceonComputerVision,pp.139–155.Springer,2022.
PengWang, LingjieLiu, YuanLiu, ChristianTheobalt, TakuKomura, andWenpingWang. Neus:
Learning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv
preprintarXiv:2106.10689,2021.
YifanWang,DiHuang,WeicaiYe,GuofengZhang,WanliOuyang,andTongHe.Neurodin:Atwo-
stageframeworkforhigh-fidelityneuralsurfacereconstruction.arXivpreprintarXiv:2408.10178,
2024.
YimingWang,QinHan,MarcHabermann,KostasDaniilidis,ChristianTheobalt,andLingjieLiu.
Neus2: Fastlearningofneuralimplicitsurfacesformulti-viewreconstruction. InProceedingsof
theIEEE/CVFInternationalConferenceonComputerVision,pp.3295–3306,2023.
Yuting Xiao, Jingwei Xu, Zehao Yu, and Shenghua Gao. Debsdf: Delving into the details and
biasofneuralindoorscenereconstruction. IEEETransactionsonPatternAnalysisandMachine
Intelligence(TPAMI),2024.
LiorYariv,YoniKasten,DrorMoran,MeiravGalun,MatanAtzmon,BasriRonen,andYaronLip-
man. Multiview neural surface reconstruction by disentangling geometry and appearance. Ad-
vancesinNeuralInformationProcessingSystems,33:2492–2502,2020.
LiorYariv,JiataoGu,YoniKasten,andYaronLipman.Volumerenderingofneuralimplicitsurfaces.
AdvancesinNeuralInformationProcessingSystems,34:4805–4815,2021.
Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin, Pratul P Srinivasan, Richard Szeliski,
JonathanTBarron,andBenMildenhall. Bakedsdf: Meshingneuralsdfsforreal-timeviewsyn-
thesis. InACMSIGGRAPH2023ConferenceProceedings,pp.1–9,2023.
Weicai Ye, Xingyuan Yu, Xinyue Lan, Yuhang Ming, Jinyu Li, Hujun Bao, Zhaopeng Cui, and
Guofeng Zhang. Deflowslam: Self-supervised scene motion decomposition for dynamic dense
slam. arXivpreprintarXiv:2207.08794,2022.
WeicaiYe,ShuoChen,ChongBao,HujunBao,MarcPollefeys,ZhaopengCui,andGuofengZhang.
IntrinsicNeRF:LearningIntrinsicNeuralRadianceFieldsforEditableNovelViewSynthesis. In
ProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,2023a.
WeicaiYe, XinyueLan, ShuoChen, YuhangMing, XingyuanYu, HujunBao, ZhaopengCui, and
GuofengZhang. Pvo: Panopticvisualodometry. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition(CVPR),pp.9579–9589,June2023b.
WeicaiYe,XinyuChen,RuohaoZhan,DiHuang,XiaoshuiHuang,HaoyiZhu,HujunBao,Wanli
Ouyang,TongHe,andGuofengZhang. Datap-sfm:Dynamic-awaretrackinganypointforrobust
densestructurefrommotioninthewild. arxivpreprint,2024a.
WeicaiYe,HaoLi,YuanyuanGao,YalunDai,JunyiChen,NanqingDong,DingwenZhang,Hujun
Bao, Wanli Ouyang, Yu Qiao, Tong He, and Guofeng Zhang. Fedsurfgs: Scalable 3d surface
gaussiansplattingwithfederatedlearningforlargescenereconstruction. arxivpreprint,2024b.
Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: A high-
fidelity dataset of 3d indoor scenes. In Proceedings of the IEEE/CVF International Conference
onComputerVision,pp.12–22,2023.
Zehao Yu, Anpei Chen, Bozidar Antic, Songyou Peng Peng, Apratim Bhattacharyya, Michael
Niemeyer, SiyuTang, TorstenSattler, andAndreasGeiger. Sdfstudio: Aunifiedframeworkfor
surfacereconstruction,2022a.
ZehaoYu, SongyouPeng, MichaelNiemeyer, TorstenSattler, andAndreasGeiger. Monosdf: Ex-
ploringmonoculargeometriccuesforneuralimplicitsurfacereconstruction. Advancesinneural
informationprocessingsystems,35:25018–25032,2022b.
13KaiZhang,GernotRiegler,NoahSnavely,andVladlenKoltun. Nerf++: Analyzingandimproving
neuralradiancefields. arXivpreprintarXiv:2010.07492,2020.
Yongqiang Zhang, Zhipeng Hu, Haoqian Wu, Minda Zhao, Lincheng Li, Zhengxia Zou, and
Changjie Fan. Towards unbiased volume rendering of neural implicit surfaces with geometry
priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion,pp.4359–4368,2023.
14APPENDIX / SUPPLEMENTAL MATERIAL
Figure5:ArchitectureofND-SDF.Weutlizethemulti-resolutionhashgridsγ asthescenerepre-
L
sentation.Thegeometrynetworkf ,colornetworkf ,anddeflectionnetworkf areallconstructed
g c d
usingsimplemultilayerperceptrons(MLPs).
A IMPLEMENTATION DETAILS
A.1 ARCHITECTURE
When considering the inputs to the deflection network, two scenarios are contemplated: (1) No
deflection is required when the prior is entirely accurate. In this case, the quaternion axis aligns
withthescenenormals, therebycloselyassociatingdeflectionwiththescenegeometry. (2)Given
that the prior model is view-dependent, the quaternion for alignment with the prior must also be
view-dependent. Consequently,wedefine:
q=f (x,v,n,z), (17)
d
where q = (q ,q ,q ,q ) represents the deflection quaternion. As detailed in Section 3.2, the
0 1 2 3
quaternioncanbeexpressedintrigonometricformasq=cos(θ/2)+sin(θ/2)(cid:0) u1i+u2j+u3k(cid:1)
.
Withinthedeflectionnetwork,therotationaxisisinitializedalongthex-axis,i.e.,u=(1,0,0),and
therotationangleθ/2issettoπ/2. Bydefault,thenetworknormalizestheoutputquaternions.
We observe that the structure of the deflection network mirrors that of the color network Yariv
et al. (2020), denoted as c = f (x,v,n,z). However, considering that the color network solely
c
models scene-related radiance, while the deflection network encapsulates a greater extent of the
priordeviation,weseparatethesetwonetworks.
A.2 PROGRESSIVEWARM-UP
Intheearlystagesoftraining,deflectionfieldswouldintroducenegativeeffectssuchasnoise.Before
theroughformationofscenegeometry, thesedeflectionquaternionsarerandomanderroneous. If
unconstrained,deflectingthetruenormalsduringthisinitialphasecanleadtoreconstructionerrors,
suchassurfaceprotrusions.
15Method F-score
w/owarm-up 0.667
wwarm-up 0.686
Table7: AblationofWarm-UpStrategy. Thewarm-upstrategycanbetterfacilitatetheinitializa-
tionofthedeflectionfield,resultinginimprovedsurfacereconstructionquality.
Figure6: VisualizationoftheDeflectionAngle. Wevisualizetheanglemapandtheanglemask
usingthresholdsof5°,15°,and25°.
Toaddressthisissue,weproposeagradualactivationstrategyfordeflectionfields,allowingthemto
becomeeffectiveasthesurfacetakesshape. Specifically,wedesigna”quatanneal”policytoenable
the deflection effect progressively. Given the surface normals synthesized by volume rendering,
denoted as N(r), and the quaternion Q(r) = (Q ,Q ,Q ,Q ), we first decompose Q(r) into its
0 1 2 3
trigonometricrepresentation:
θ =arccosQ
u2 = [Q 1,Q 2,Q 3]0 . (18)
sinθ
2
Here,urepresentstherotationaxiscorrespondingtothequaternionQ(r),andθistherotationangle.
Weutilizethescenenormalstowarmupthedeflectionaxis,startingwithzerorotation:
θi 2ter =prog q× θ
2 . (19)
u =prog ×u+(1−prog )×Nˆ(r)
iter q q
Here, the parameter prog linearly increases from 0 to 1 during the training process within the
q
process interval [0,anneal quat end]. Once the training progress reaches anneal quat end, the
warm-upphaseconcludes,andprog becomesequalto1.
q
Thewarm-updeflectionquaternioniscomputedas:
θ θ
Q (r)=cos iter +sin iter ×u . (20)
iter 2 2 iter
During the initial training stages, the deflection angles are close to zero, enabling us to leverage
priorknowledgetolearnsmoothregionsandapproximatethescenegeometry. Weconsiderittobe
adequatelyinitializedasthewarm-upphaseconcludes.Inthisprocess,thedeflectionfieldgradually
adapts tomore complexstructures anddeviations from thenormal priors. Thequantitative results
withandwithoutprogressivewarm-uparepresentedinTable7.
A.3 MOREDETAILS
A.3.1 ADAPTIVEDEFLECTIONANGLEPRIORLOSS
InSection3.3,weintroducetwomodulationfunctions,namelygd andg(Eq.12),whichadjustthe
weightofthedeflectedandoriginalnormallosstermsbasedonthedeflectionangle.
Here provide the specific form for both functions. For convenience, we employ a shifted logistic
functiontodefinethem:
16gd(θ)= 1
(1+e−s0(θ−θ0)) . (21)
g(θ)=1−gd(θ)
Here, theparameters controlsthesteepnessofgd, andθ denotestheoffsetterm. Inourexperi-
0 0
ments,wesets =12.5,θ = π forallscenes.
0 0 12
As illustrated in Figure 6, we consistently observe that a deviation angle less than 5° indicates a
simpleflatregion,whereasadeviationanglegreaterthan15°largelysuggestsacomplexstructural
region.
Additionally,wevisualizethemodulationfunctiongd(∆θ)inFigure7. Forregionswithadeviation
anglelessthan5°,weassignweightsgreaterthan0.9totheoriginalnormallosstermtoeffectively
utilizethenormalpriorsinsmoothregions.
Figure7: ModulationFunctiongd(∆θ)definedinEq.21.
A.3.2 DEFLECTIONANGLEGUIDEDOPTIMIZATION
InSection3.4,weintroducethreedeflectionangleguidedoptimizationmethods,includingraysam-
pling,photometricoptimization,andunbiasedrendering.
Toachieveraysamplingguidedbythedeflectionangle,wedynamicallymaintainadeflectionangle
map (∆θ¯) per image during the training process. In the sampling phase, we first calculate the
per-pixel sampling probability based on the deflection angle map. Then use the probability map
forinversesampling,meaningthatareaswithgreaterdeflectionmagnitudetendtohavemorerays
sampled.
Weemployascaledandshiftedlogisticfunctiontocalculatetheper-pixelsamplingprobability:
t
p(r )=1+ 1 , (22)
i (1+e−s1(∆θ¯(r i)−θ 1))
where∆θ¯representsthedeflectionanglemap,andp(r )∈[1,t +1],withthesteepnesscontrolled
i 1
bys . Wethennormalizetheprobabilitymapforinversesamplingusingthefollowingequation:
1
p(r )
p(r )= i . (23)
i (cid:80) p(r )
i i
For the angle guided color loss, we similarly employ a scaled and shifted logistic function to re-
weighttheoriginalcolorlosstermguidedbythedeflectionangle(Eq.14):
t
w (∆θ(r))=1+ 2 . (24)
color (1+e−s2(∆θ(r)−θ 2))
17Regarding the angle guided unbiased rendering, we define a bias confidence using the deflection
angle. Anothershiftedlogisticfunctionisemployedtocomputethebiasconfidence:
1
cfd(r)= . (25)
(1+e−s3(∆θ¯(r)−θ 3))
Thisleadstodeflectionangleguidedunbiasing:
(cid:32) (cid:33)
−f (r(t ))
σ(r(t i))=αΨ β cfd(r)(cid:12)
(cid:12)f
g′(r(tg i))(cid:12) (cid:12)+i
1−cfd(r)
. (26)
Inpractice,weset: (s =25,θ = π,t =4),(s =25,θ = π,t =2),(s =25,θ = π).
1 1 12 1 2 2 12 2 3 3 18
A.3.3 NUMERICALGRADIENT
FollowingNeuralangeloLietal.(2023),weemploynumericalgradientsw.r.tmulti-resolutionhash
gridstoenhancereconstructionquality.Thenumericalgradientiscomputedasfollows(considering
onlytheSDFvalueoutputoff ):
g
f (γ (x +ϵ ))−f (γ (x −ϵ ))
∇ f (x )= g L i x g L i x . (27)
x g i 2ϵ
Here,ϵ = [ϵ,0,0]signifiestheincrementalstepsizealongthex-axis. Themethodforcalculating
x
thenumericalgradientalongtheyorz-axisremainsthesame.
WealsoutilizethecurvaturelossLietal.(2023)tofurtherencouragethesmoothnessoftherecon-
structedsurfaces. Thislosstermisdefinedas:
N
L
curv
= N1 (cid:88)(cid:12) (cid:12)∇2f g(x i)(cid:12) (cid:12). (28)
i=1
B EXTENSIVE ABLATION STUDIES
B.1 ABLATIONOFPARTIALUNBIASEDRENDERING
InSection3.5,weintroduceanunbiasedfunction.Wepartiallyapplythistechniquetothinstructures
indicated by the learned deflection angles, without doing so may result in an inability to converge
andadecreaseinreconstructionquality.
We attribute this issue to the inconsistency in perspective inherent in the transform function. Ac-
cordingtoTUVRZhangetal.(2023):
f′(r(t ))=∇f (o+t v)·v =n ·v, (29)
g i g i i
wherevrepresentstheraydirectionandn denotesthenormalatpointx (alsor(t )). Wefindthat
i i i
∂fg(r(ti)) isequivalenttothecosineofrayandnormal.
∂t
Consideranyotherrayinspace,denotedasr′(t) = o′+tv′. Assumingthatthisrayalsointersects
atx andsamplesthesamepointsuchthatx = o′+t v′ = x ,thevolumetricdensityofx under
i j j i j
thetransformfunctionisgivenby:
(cid:32) (cid:33)
−f (r′(t ))
σ(r′(t j))=αΨ β (cid:12) (cid:12)f g′g (r′(t jj ))(cid:12) (cid:12) , (30)
(cid:12) (cid:12) (cid:12) (cid:12)
wheref g(r(t i)) = f g(r(t j))becausex
i
= x
j
andn
i
= n j,but(cid:12)f g′(r(t i))(cid:12) ̸= (cid:12)f g′(r′(t j))(cid:12)because
n ·v ̸= n ·v′. The inconsistency in perspective leads to ambiguity in the volume density at the
i j
samesamplingpoint,causingcomplexindoorsurfacestofailtoconverge. Therefore,toreducethis
18Method F-score
w/opartial 0.662
wpartial 0.686
Table 8: Ablation of Partial Unbiased Rendering. We observed a decrease in reconstruction
qualityafteromittingpartialunbiasedrendering.
Method F-score
w/oLd 0.664
depth
wLd 0.686
depth
Table9:Ablationofadaptivedepthpriorloss. Weobservedadecreaseinreconstructionaccuracy
afteromittingtheadaptivedepthloss.
ambiguity,wechoosetoapplythetransformfunctiononlytofineregionsindicatedbythedeflection
angle.
ThequantitativeablationresultsforpartialunbiasedrenderingarepresentedinTable8, wherethe
improvedF-scoreunderscoresthesignificanceofthisstrategy.
Figure8: VisualcomparisonsofpartialunbiasedrenderingonScanNet.
In Figure 8, qualitative comparisons on ScanNet are presented. As indicated by the blue box, the
reconstructed surfaces exhibit wrinkles and irregularities without the application of the proposed
partial unbiased rendering strategy and fail to capture the fine structural details of the chair legs
highlighted in the red box. After applying the strategy, the overall surface becomes precise and
smooth. Thesevisualresultsdemonstratethesuperiorityoftheproposedpartialunbiasedrendering
method.
B.2 ABLATIONOFADAPTIVEMONOCULARDEPTHLOSS
Notably, we use the same method to adjust depth supervision (Eq. 13) based on deflection angle.
This simplified treatment rests on a strong foundation for two reasons. First, both the depth and
normalcuesoriginatefromthesamepretrainedmodelEftekharetal.(2021)andshareacommon
inductive bias. Second, monocular depth lacks scale and cannot provide accurate supervision. In
implicitsurfacereconstructionwithpriors,depthcuesareprimarilyutilizedtofacilitateconvergence
and accelerate surface formation. Automatically and continuously masking high-frequency areas
basedonthedeflectionanglehasnoadverseimpactontherecoveryofdetails.
WepresentthequantitativeablationresultsinTable9. Theimprovedperformanceunderscoresthe
importanceofutilizingtheadaptivedeflectionangledepthpriorloss.
19036bce3393 0e75f3c4d9
Method
Acc↓ Comp↓ Pre↑ Recall↑ Chamfer↓ F-score↑ Acc↓ Comp↓ Pre↑ Recall↑ Chamfer↓ F-score↑
VolSDF 0.053 0.098 0.475 0.372 0.0755 0.417 0.053 0.078 0.395 0.363 0.065 0.378
Baked-Angelo 0.168 0.023 0.52 0.78 0.0955 0.624 0.121 0.053 0.525 0.719 0.087 0.607
MonoSDF 0.037 0.037 0.606 0.656 0.037 0.630 0.064 0.027 0.574 0.705 0.046 0.633
ND-SDF 0.037 0.031 0.648 0.728 0.034 0.686 0.054 0.018 0.662 0.864 0.036 0.750
108ec0b806 7f4d173c9c
Method
Acc↓ Comp↓ Pre↑ Recall↑ Chamfer↓ F-score↑ Acc↓ Comp↓ Pre↑ Recall↑ Chamfer↓ F-score↑
VolSDF 0.049 0.097 0.475 0.372 0.073 0.417 0.097 0.075 0.457 0.436 0.086 0.446
Baked-Angelo 0.092 0.050 0.498 0.624 0.071 0.554 0.420 0.024 0.457 0.799 0.222 0.582
MonoSDF 0.041 0.045 0.575 0.576 0.043 0.576 0.138 0.022 0.708 0.813 0.080 0.757
ND-SDF 0.047 0.040 0.591 0.646 0.044 0.617 0.112 0.015 0.721 0.888 0.064 0.796
ab11145646 e050c15a8d
Method
Acc↓ Comp↓ Pre↑ Recall↑ Chamfer↓ F-score↑ Acc↓ Comp↓ Pre↑ Recall↑ Chamfer↓ F-score↑
VolSDF 0.089 0.164 0.321 0.226 0.126 0.265 0.077 0.101 0.307 0.262 0.089 0.283
Baked-Angelo 0.076 0.038 0.575 0.701 0.057 0.632 0.034 0.043 0.685 0.687 0.038 0.686
MonoSDF 0.036 0.035 0.695 0.700 0.036 0.697 0.027 0.024 0.747 0.770 0.026 0.758
ND-SDF 0.054 0.024 0.657 0.786 0.039 0.716 0.036 0.020 0.723 0.798 0.028 0.759
Table 10: Quantitative results of the 6 selected scenes from ScanNet++. Our method achieves
state-of-the-artperformance.
C MORE RESULTS
C.1 REPLICA
WepresentadditionalvisualizationresultsonReplicainFigure9.
C.2 SCANNET++
WepresentdetailedquantitativeresultsoftheselectedsixscenesfromScanNet++inTable10and
providemorevisualizationresultsinFigure10.Comparedtothebaselines,ourmethodyieldssignif-
icantlyimprovedreconstructionsbothvisuallyandquantitatively. Notably,ourmethodcontributes
substantiallytocapturingthinandfine-grainedstructures.
C.3 TANKSANDTEMPLES
We present additional qualitative comparisons with previous state-of-the-art baselines, including
MonoSDF Yu et al. (2022b) and NeuRIS Yu et al. (2022b), as depicted in Figure 11. Notably,
ND-SDFachievesaccuratedensesurfacesrepletewithextensivedetails.
AsillustratedinFigure12andFigure13,wefurtherextractedthereconstructedmeshataresolution
of 2048. The mesh, with this resolution, demonstrates that our method significantly enhances the
generationofsmallandcomplexstructures,eveninchallenginglarge-scaleindoorscenes. Thehash
grid for all experiments remains at the default settings, with resolutions spanning from 25 to 211
across16levels,andthesizeofhashmapstayingat219.
C.4 MOREINFORMATIONABOUTDATASET
ThequalitativeresultsonScanNetinthemaincontentdemonstratethatourmethodcancapturefine
details, such as the chair legs, which are even missed by the ground truth (GT). This highlights a
knownissueregardingthereliabilityofScanNetGTmeshes. TheScanNetdataset, constructedin
2017,isbasedonRGB-DdatacapturedbyaniPadwithanadditionalstructuresensor. Theimages
haverelativelylowresolutionandsufferfrommotionblurandlightingvariations.Inaddition,dueto
thelimitationsofthebundle-fusionalgorithm,theGTsurfaceslackfinestructures,whichcanlead
toadecreaseinevaluationperformanceintheseareas. Thisalsoexplainswhywetendtoconduct
quantitativeablationstudiesonScanNet++,whichprovidesmorereliableevaluationresults.
20Figure9: VisualizationresultsonReplicadataset.
D SOCIETAL IMPACT
Our method contributes to high-fidelity indoor surface reconstruction. On the positive side, pre-
cise indoor surface reconstruction can revolutionize fields such as architecture, virtual reality, and
robotics. Itenablesarchitectstovisualizeandoptimizedesigns,enhancesimmersiveexperiencesin
virtualenvironments, andaidsinnavigationforautonomousrobots. However, therearealsochal-
lenges. Misuse of this technology could invade privacy, as detailed indoor reconstructions might
reveal sensitive information. Striking a balance between innovation and responsible deployment
willbecrucialformaximizingthepositiveimpactwhileminimizingpotentialharm.
21Figure 10: Qualitative Comparison on ScanNet++. We compare ND-SDF with several base-
linemethods,includingVolSDF,Baked-Angelo,andMonoSDF.Notably,onlyMonoSDFleveraged
monocularpriors. Ourmethodfacilitatesbothsmoothnessanddetails,whileMonoSDFlosessub-
stantialfinestructures,andBaked-Angelocannothandletexturelesssmoothregionssuchasfloors.
22Figure 11: Qualitative Comparison on Tanks & Temples. We compare ND-SDF with previous
state-of-the-artindoorimplicitreconstructionmethods.
23Figure 12: Qualitative Comparison of Different extraction resolutions on Tanks & Temples.
WeadditionallyextracttherecoveredmeshfromtheimplicitSDFataresolutionof2048.
24Figure13: QualitativeResultsoftheextractedmeshataresolutionof2048onTanks&Tem-
ples.
25