[
    {
        "title": "Controllable Text Generation for Large Language Models: A Survey",
        "authors": "Xun LiangHanyu WangYezhaohui WangShichao SongJiawei YangSimin NiuJie HuDan LiuShunyu YaoFeiyu XiongZhiyu Li",
        "links": "http://arxiv.org/abs/2408.12599v1",
        "entry_id": "http://arxiv.org/abs/2408.12599v1",
        "pdf_url": "http://arxiv.org/pdf/2408.12599v1",
        "summary": "In Natural Language Processing (NLP), Large Language Models (LLMs) have\ndemonstrated high text generation quality. However, in real-world applications,\nLLMs must meet increasingly complex requirements. Beyond avoiding misleading or\ninappropriate content, LLMs are also expected to cater to specific user needs,\nsuch as imitating particular writing styles or generating text with poetic\nrichness. These varied demands have driven the development of Controllable Text\nGeneration (CTG) techniques, which ensure that outputs adhere to predefined\ncontrol conditions--such as safety, sentiment, thematic consistency, and\nlinguistic style--while maintaining high standards of helpfulness, fluency, and\ndiversity.\n  This paper systematically reviews the latest advancements in CTG for LLMs,\noffering a comprehensive definition of its core concepts and clarifying the\nrequirements for control conditions and text quality. We categorize CTG tasks\ninto two primary types: content control and attribute control. The key methods\nare discussed, including model retraining, fine-tuning, reinforcement learning,\nprompt engineering, latent space manipulation, and decoding-time intervention.\nWe analyze each method's characteristics, advantages, and limitations,\nproviding nuanced insights for achieving generation control. Additionally, we\nreview CTG evaluation methods, summarize its applications across domains, and\naddress key challenges in current research, including reduced fluency and\npracticality. We also propose several appeals, such as placing greater emphasis\non real-world applications in future research. This paper aims to offer\nvaluable guidance to researchers and developers in the field. Our reference\nlist and Chinese version are open-sourced at\nhttps://github.com/IAAR-Shanghai/CTGSurvey.",
        "updated": "2024-08-22 17:59:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.12599v1"
    },
    {
        "title": "RuleAlign: Making Large Language Models Better Physicians with Diagnostic Rule Alignment",
        "authors": "Xiaohan WangXiaoyan YangYuqi ZhuYue ShenJian WangPeng WeiLei LiangJinjie GuHuajun ChenNingyu Zhang",
        "links": "http://arxiv.org/abs/2408.12579v1",
        "entry_id": "http://arxiv.org/abs/2408.12579v1",
        "pdf_url": "http://arxiv.org/pdf/2408.12579v1",
        "summary": "Large Language Models (LLMs) like GPT-4, MedPaLM-2, and Med-Gemini achieve\nperformance competitively with human experts across various medical benchmarks.\nHowever, they still face challenges in making professional diagnoses akin to\nphysicians, particularly in efficiently gathering patient information and\nreasoning the final diagnosis. To this end, we introduce the RuleAlign\nframework, designed to align LLMs with specific diagnostic rules. We develop a\nmedical dialogue dataset comprising rule-based communications between patients\nand physicians and design an alignment learning approach through preference\nlearning. Experimental results demonstrate the effectiveness of the proposed\napproach. We hope that our work can serve as an inspiration for exploring the\npotential of LLMs as AI physicians.",
        "updated": "2024-08-22 17:44:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.12579v1"
    },
    {
        "title": "MuMA-ToM: Multi-modal Multi-Agent Theory of Mind",
        "authors": "Haojun ShiSuyu YeXinyu FangChuanyang JinLayla IsikYen-Ling KuoTianmin Shu",
        "links": "http://arxiv.org/abs/2408.12574v1",
        "entry_id": "http://arxiv.org/abs/2408.12574v1",
        "pdf_url": "http://arxiv.org/pdf/2408.12574v1",
        "summary": "Understanding people's social interactions in complex real-world scenarios\noften relies on intricate mental reasoning. To truly understand how and why\npeople interact with one another, we must infer the underlying mental states\nthat give rise to the social interactions, i.e., Theory of Mind reasoning in\nmulti-agent interactions. Additionally, social interactions are often\nmulti-modal -- we can watch people's actions, hear their conversations, and/or\nread about their past behaviors. For AI systems to successfully and safely\ninteract with people in real-world environments, they also need to understand\npeople's mental states as well as their inferences about each other's mental\nstates based on multi-modal information about their interactions. For this, we\nintroduce MuMA-ToM, a Multi-modal Multi-Agent Theory of Mind benchmark.\nMuMA-ToM is the first multi-modal Theory of Mind benchmark that evaluates\nmental reasoning in embodied multi-agent interactions. In MuMA-ToM, we provide\nvideo and text descriptions of people's multi-modal behavior in realistic\nhousehold environments. Based on the context, we then ask questions about\npeople's goals, beliefs, and beliefs about others' goals. We validated MuMA-ToM\nin a human experiment and provided a human baseline. We also proposed a novel\nmulti-modal, multi-agent ToM model, LIMP (Language model-based Inverse\nMulti-agent Planning). Our experimental results show that LIMP significantly\noutperforms state-of-the-art methods, including large multi-modal models (e.g.,\nGPT-4o, Gemini-1.5 Pro) and a recent multi-modal ToM model, BIP-ALM.",
        "updated": "2024-08-22 17:41:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.12574v1"
    },
    {
        "title": "Jamba-1.5: Hybrid Transformer-Mamba Models at Scale",
        "authors": "Jamba TeamBarak LenzAlan AraziAmir BergmanAvshalom ManevichBarak PelegBen AviramChen AlmagorClara FridmanDan PadnosDaniel GissinDaniel JannaiDor MuhlgayDor ZimbergEdden M GerberElad DolevEran KrakovskyErez SafahiErez SchwartzGal CohenGal ShachafHaim RozenblumHofit BataIdo BlassInbal MagarItay DalmedigosJhonathan OsinJulie FadlonMaria RozmanMatan DanosMichael GokhmanMor ZusmanNaama GidronNir RatnerNoam GatNoam RozenOded FriedOhad LeshnoOmer AntvergOmri AbendOpher LieberOr DaganOrit CohaviRaz AlonRo'i BelsonRoi CohenRom GiladRoman GlozmanShahar LevShaked MeiromTal DelbariTal NessTomer AsidaTom Ben GalTom BraudeUriya PumerantzYehoshua CohenYonatan BelinkovYuval GlobersonYuval Peleg LevyYoav Shoham",
        "links": "http://arxiv.org/abs/2408.12570v1",
        "entry_id": "http://arxiv.org/abs/2408.12570v1",
        "pdf_url": "http://arxiv.org/pdf/2408.12570v1",
        "summary": "We present Jamba-1.5, new instruction-tuned large language models based on\nour Jamba architecture. Jamba is a hybrid Transformer-Mamba mixture of experts\narchitecture, providing high throughput and low memory usage across context\nlengths, while retaining the same or better quality as Transformer models. We\nrelease two model sizes: Jamba-1.5-Large, with 94B active parameters, and\nJamba-1.5-Mini, with 12B active parameters. Both models are fine-tuned for a\nvariety of conversational and instruction-following capabilties, and have an\neffective context length of 256K tokens, the largest amongst open-weight\nmodels. To support cost-effective inference, we introduce ExpertsInt8, a novel\nquantization technique that allows fitting Jamba-1.5-Large on a machine with 8\n80GB GPUs when processing 256K-token contexts without loss of quality. When\nevaluated on a battery of academic and chatbot benchmarks, Jamba-1.5 models\nachieve excellent results while providing high throughput and outperforming\nother open-weight models on long-context benchmarks. The model weights for both\nsizes are publicly available under the Jamba Open Model License and we release\nExpertsInt8 as open source.",
        "updated": "2024-08-22 17:38:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.12570v1"
    },
    {
        "title": "Towards Evaluating and Building Versatile Large Language Models for Medicine",
        "authors": "Chaoyi WuPengcheng QiuJinxin LiuHongfei GuNa LiYa ZhangYanfeng WangWeidi Xie",
        "links": "http://arxiv.org/abs/2408.12547v1",
        "entry_id": "http://arxiv.org/abs/2408.12547v1",
        "pdf_url": "http://arxiv.org/pdf/2408.12547v1",
        "summary": "In this study, we present MedS-Bench, a comprehensive benchmark designed to\nevaluate the performance of large language models (LLMs) in clinical contexts.\nUnlike existing benchmarks that focus on multiple-choice question answering,\nMedS-Bench spans 11 high-level clinical tasks, including clinical report\nsummarization, treatment recommendations, diagnosis, named entity recognition,\nand medical concept explanation, among others. We evaluated six leading LLMs,\ne.g., MEDITRON, Mistral, InternLM 2, Llama 3, GPT-4, and Claude-3.5 using\nfew-shot prompting, and found that even the most sophisticated models struggle\nwith these complex tasks. To address these limitations, we developed MedS-Ins,\na large-scale instruction tuning dataset for medicine. MedS-Ins comprises 58\nmedically oriented language corpora, totaling 13.5 million samples across 122\ntasks. To demonstrate the dataset's utility, we conducted a proof-of-concept\nexperiment by performing instruction tuning on a lightweight, open-source\nmedical language model. The resulting model, MMedIns-Llama 3, significantly\noutperformed existing models across nearly all clinical tasks. To promote\nfurther advancements in the application of LLMs to clinical challenges, we have\nmade the MedS-Ins dataset fully accessible and invite the research community to\ncontribute to its expansion.Additionally, we have launched a dynamic\nleaderboard for MedS-Bench, which we plan to regularly update the test set to\ntrack progress and enhance the adaptation of general LLMs to the medical\ndomain. Leaderboard: https://henrychur.github.io/MedS-Bench/. Github:\nhttps://github.com/MAGIC-AI4Med/MedS-Ins.",
        "updated": "2024-08-22 17:01:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.12547v1"
    }
]