[
    {
        "title": "Factor Adjusted Spectral Clustering for Mixture Models",
        "authors": "Shange TangSoham JanaJianqing Fan",
        "links": "http://arxiv.org/abs/2408.12564v1",
        "entry_id": "http://arxiv.org/abs/2408.12564v1",
        "pdf_url": "http://arxiv.org/pdf/2408.12564v1",
        "summary": "This paper studies a factor modeling-based approach for clustering\nhigh-dimensional data generated from a mixture of strongly correlated\nvariables. Statistical modeling with correlated structures pervades modern\napplications in economics, finance, genomics, wireless sensing, etc., with\nfactor modeling being one of the popular techniques for explaining the common\ndependence. Standard techniques for clustering high-dimensional data, e.g.,\nnaive spectral clustering, often fail to yield insightful results as their\nperformances heavily depend on the mixture components having a weakly\ncorrelated structure. To address the clustering problem in the presence of a\nlatent factor model, we propose the Factor Adjusted Spectral Clustering (FASC)\nalgorithm, which uses an additional data denoising step via eliminating the\nfactor component to cope with the data dependency. We prove this method\nachieves an exponentially low mislabeling rate, with respect to the signal to\nnoise ratio under a general set of assumptions. Our assumption bridges many\nclassical factor models in the literature, such as the pervasive factor model,\nthe weak factor model, and the sparse factor model. The FASC algorithm is also\ncomputationally efficient, requiring only near-linear sample complexity with\nrespect to the data dimension. We also show the applicability of the FASC\nalgorithm with real data experiments and numerical studies, and establish that\nFASC provides significant results in many cases where traditional spectral\nclustering fails.",
        "updated": "2024-08-22 17:31:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.12564v1"
    },
    {
        "title": "Distributed quasi-Newton robust estimation under differential privacy",
        "authors": "Chuhan WangLixing ZhuXuehu Zhu",
        "links": "http://arxiv.org/abs/2408.12353v1",
        "entry_id": "http://arxiv.org/abs/2408.12353v1",
        "pdf_url": "http://arxiv.org/pdf/2408.12353v1",
        "summary": "For distributed computing with Byzantine machines under Privacy Protection\n(PP) constraints, this paper develops a robust PP distributed quasi-Newton\nestimation, which only requires the node machines to transmit five vectors to\nthe central processor with high asymptotic relative efficiency. Compared with\nthe gradient descent strategy which requires more rounds of transmission and\nthe Newton iteration strategy which requires the entire Hessian matrix to be\ntransmitted, the novel quasi-Newton iteration has advantages in reducing\nprivacy budgeting and transmission cost. Moreover, our PP algorithm does not\ndepend on the boundedness of gradients and second-order derivatives. When\ngradients and second-order derivatives follow sub-exponential distributions, we\noffer a mechanism that can ensure PP with a sufficiently high probability.\nFurthermore, this novel estimator can achieve the optimal convergence rate and\nthe asymptotic normality. The numerical studies on synthetic and real data sets\nevaluate the performance of the proposed algorithm.",
        "updated": "2024-08-22 12:51:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.12353v1"
    },
    {
        "title": "Simplifying Random Forests' Probabilistic Forecasts",
        "authors": "Nils KosterFabian Krüger",
        "links": "http://arxiv.org/abs/2408.12332v1",
        "entry_id": "http://arxiv.org/abs/2408.12332v1",
        "pdf_url": "http://arxiv.org/pdf/2408.12332v1",
        "summary": "Since their introduction by Breiman, Random Forests (RFs) have proven to be\nuseful for both classification and regression tasks. The RF prediction of a\npreviously unseen observation can be represented as a weighted sum of all\ntraining sample observations. This nearest-neighbor-type representation is\nuseful, among other things, for constructing forecast distributions\n(Meinshausen, 2006). In this paper, we consider simplifying RF-based forecast\ndistributions by sparsifying them. That is, we focus on a small subset of\nnearest neighbors while setting the remaining weights to zero. This\nsparsification step greatly improves the interpretability of RF predictions. It\ncan be applied to any forecasting task without re-training existing RF models.\nIn empirical experiments, we document that the simplified predictions can be\nsimilar to or exceed the original ones in terms of forecasting performance. We\nexplore the statistical sources of this finding via a stylized analytical model\nof RFs. The model suggests that simplification is particularly promising if the\nunknown true forecast distribution contains many small weights that are\nestimated imprecisely.",
        "updated": "2024-08-22 12:20:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.12332v1"
    },
    {
        "title": "Neural-ANOVA: Model Decomposition for Interpretable Machine Learning",
        "authors": "Steffen LimmerSteffen UdluftClemens Otte",
        "links": "http://arxiv.org/abs/2408.12319v1",
        "entry_id": "http://arxiv.org/abs/2408.12319v1",
        "pdf_url": "http://arxiv.org/pdf/2408.12319v1",
        "summary": "The analysis of variance (ANOVA) decomposition offers a systematic method to\nunderstand the interaction effects that contribute to a specific decision\noutput. In this paper we introduce Neural-ANOVA, an approach to decompose\nneural networks into glassbox models using the ANOVA decomposition. Our\napproach formulates a learning problem, which enables rapid and closed-form\nevaluation of integrals over subspaces that appear in the calculation of the\nANOVA decomposition. Finally, we conduct numerical experiments to illustrate\nthe advantages of enhanced interpretability and model validation by a\ndecomposition of the learned interaction effects.",
        "updated": "2024-08-22 11:55:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.12319v1"
    },
    {
        "title": "Demystifying Functional Random Forests: Novel Explainability Tools for Model Transparency in High-Dimensional Spaces",
        "authors": "Fabrizio MaturoAnnamaria Porreca",
        "links": "http://arxiv.org/abs/2408.12288v1",
        "entry_id": "http://arxiv.org/abs/2408.12288v1",
        "pdf_url": "http://arxiv.org/pdf/2408.12288v1",
        "summary": "The advent of big data has raised significant challenges in analysing\nhigh-dimensional datasets across various domains such as medicine, ecology, and\neconomics. Functional Data Analysis (FDA) has proven to be a robust framework\nfor addressing these challenges, enabling the transformation of\nhigh-dimensional data into functional forms that capture intricate temporal and\nspatial patterns. However, despite advancements in functional classification\nmethods and very high performance demonstrated by combining FDA and ensemble\nmethods, a critical gap persists in the literature concerning the transparency\nand interpretability of black-box models, e.g. Functional Random Forests (FRF).\nIn response to this need, this paper introduces a novel suite of explainability\ntools to illuminate the inner mechanisms of FRF. We propose using Functional\nPartial Dependence Plots (FPDPs), Functional Principal Component (FPC)\nProbability Heatmaps, various model-specific and model-agnostic FPCs'\nimportance metrics, and the FPC Internal-External Importance and Explained\nVariance Bubble Plot. These tools collectively enhance the transparency of FRF\nmodels by providing a detailed analysis of how individual FPCs contribute to\nmodel predictions. By applying these methods to an ECG dataset, we demonstrate\nthe effectiveness of these tools in revealing critical patterns and improving\nthe explainability of FRF.",
        "updated": "2024-08-22 10:52:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.12288v1"
    }
]