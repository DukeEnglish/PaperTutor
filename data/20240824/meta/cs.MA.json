[
    {
        "title": "MEDCO: Medical Education Copilots Based on A Multi-Agent Framework",
        "authors": "Hao WeiJianing QiuHaibao YuWu Yuan",
        "links": "http://arxiv.org/abs/2408.12496v1",
        "entry_id": "http://arxiv.org/abs/2408.12496v1",
        "pdf_url": "http://arxiv.org/pdf/2408.12496v1",
        "summary": "Large language models (LLMs) have had a significant impact on diverse\nresearch domains, including medicine and healthcare. However, the potential of\nLLMs as copilots in medical education remains underexplored. Current\nAI-assisted educational tools are limited by their solitary learning approach\nand inability to simulate the multi-disciplinary and interactive nature of\nactual medical training. To address these limitations, we propose MEDCO\n(Medical EDucation COpilots), a novel multi-agent-based copilot system\nspecially developed to emulate real-world medical training environments. MEDCO\nincorporates three primary agents: an agentic patient, an expert doctor, and a\nradiologist, facilitating a multi-modal and interactive learning environment.\nOur framework emphasizes the learning of proficient question-asking skills,\nmulti-disciplinary collaboration, and peer discussions between students. Our\nexperiments show that simulated virtual students who underwent training with\nMEDCO not only achieved substantial performance enhancements comparable to\nthose of advanced models, but also demonstrated human-like learning behaviors\nand improvements, coupled with an increase in the number of learning samples.\nThis work contributes to medical education by introducing a copilot that\nimplements an interactive and collaborative learning approach. It also provides\nvaluable insights into the effectiveness of AI-integrated training paradigms.",
        "updated": "2024-08-22 15:41:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.12496v1"
    },
    {
        "title": "Balancing Act: Prioritization Strategies for LLM-Designed Restless Bandit Rewards",
        "authors": "Shresth VermaNiclas BoehmerLingkai KongMilind Tambe",
        "links": "http://arxiv.org/abs/2408.12112v1",
        "entry_id": "http://arxiv.org/abs/2408.12112v1",
        "pdf_url": "http://arxiv.org/pdf/2408.12112v1",
        "summary": "LLMs are increasingly used to design reward functions based on human\npreferences in Reinforcement Learning (RL). We focus on LLM-designed rewards\nfor Restless Multi-Armed Bandits, a framework for allocating limited resources\namong agents. In applications such as public health, this approach empowers\ngrassroots health workers to tailor automated allocation decisions to community\nneeds. In the presence of multiple agents, altering the reward function based\non human preferences can impact subpopulations very differently, leading to\ncomplex tradeoffs and a multi-objective resource allocation problem. We are the\nfirst to present a principled method termed Social Choice Language Model for\ndealing with these tradeoffs for LLM-designed rewards for multiagent planners\nin general and restless bandits in particular. The novel part of our model is a\ntransparent and configurable selection component, called an adjudicator,\nexternal to the LLM that controls complex tradeoffs via a user-selected social\nwelfare function. Our experiments demonstrate that our model reliably selects\nmore effective, aligned, and balanced reward functions compared to purely\nLLM-based approaches.",
        "updated": "2024-08-22 03:54:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.12112v1"
    },
    {
        "title": "Empirical Equilibria in Agent-based Economic systems with Learning agents",
        "authors": "Kshama DwarakanathSvitlana VyetrenkoTucker Balch",
        "links": "http://arxiv.org/abs/2408.12038v1",
        "entry_id": "http://arxiv.org/abs/2408.12038v1",
        "pdf_url": "http://arxiv.org/pdf/2408.12038v1",
        "summary": "We present an agent-based simulator for economic systems with heterogeneous\nhouseholds, firms, central bank, and government agents. These agents interact\nto define production, consumption, and monetary flow. Each agent type has\ndistinct objectives, such as households seeking utility from consumption and\nthe central bank targeting inflation and production. We define this multi-agent\neconomic system using an OpenAI Gym-style environment, enabling agents to\noptimize their objectives through reinforcement learning. Standard multi-agent\nreinforcement learning (MARL) schemes, like independent learning, enable agents\nto learn concurrently but do not address whether the resulting strategies are\nat equilibrium. This study integrates the Policy Space Response Oracle (PSRO)\nalgorithm, which has shown superior performance over independent MARL in games\nwith homogeneous agents, with economic agent-based modeling. We use PSRO to\ndevelop agent policies approximating Nash equilibria of the empirical economic\ngame, thereby linking to economic equilibria. Our results demonstrate that PSRO\nstrategies achieve lower regret values than independent MARL strategies in our\neconomic system with four agent types. This work aims to bridge artificial\nintelligence, economics, and empirical game theory towards future research.",
        "updated": "2024-08-21 23:47:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.12038v1"
    },
    {
        "title": "VIRIS: Simulating indoor airborne transmission combining architectural design and people movement",
        "authors": "Yidan XueWassim JabiThomas E. WoolleyKaterina Kaouri",
        "links": "http://arxiv.org/abs/2408.11772v1",
        "entry_id": "http://arxiv.org/abs/2408.11772v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11772v1",
        "summary": "A Viral Infection Risk Indoor Simulator (VIRIS) has been developed to quickly\nassess and compare mitigations for airborne disease spread. This agent-based\nsimulator combines people movement in an indoor space, viral transmission\nmodelling and detailed architectural design, and it is powered by topologicpy,\nan open-source Python library. VIRIS generates very fast predictions of the\nviral concentration and the spatiotemporal infection risk for individuals as\nthey move through a given space. The simulator is validated with data from a\ncourtroom superspreader event. A sensitivity study for unknown parameter values\nis also performed. We compare several non-pharmaceutical interventions (NPIs)\nissued in UK government guidance, for two indoor settings: a care home and a\nsupermarket. Additionally, we have developed the user-friendly VIRIS web app\nthat allows quick exploration of diverse scenarios of interest and\nvisualisation, allowing policymakers, architects and space managers to easily\ndesign or assess infection risk in an indoor space.",
        "updated": "2024-08-21 16:54:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11772v1"
    },
    {
        "title": "Bayesian Optimization Framework for Efficient Fleet Design in Autonomous Multi-Robot Exploration",
        "authors": "David Molina ConchaJiping LiHaoran YinKyeonghyeon ParkHyun-Rok LeeTaesik LeeDhruv SirohiChi-Guhn Lee",
        "links": "http://arxiv.org/abs/2408.11751v1",
        "entry_id": "http://arxiv.org/abs/2408.11751v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11751v1",
        "summary": "This study addresses the challenge of fleet design optimization in the\ncontext of heterogeneous multi-robot fleets, aiming to obtain feasible designs\nthat balance performance and costs. In the domain of autonomous multi-robot\nexploration, reinforcement learning agents play a central role, offering\nadaptability to complex terrains and facilitating collaboration among robots.\nHowever, modifying the fleet composition results in changes in the learned\nbehavior, and training multi-robot systems using multi-agent reinforcement\nlearning is expensive. Therefore, an exhaustive evaluation of each potential\nfleet design is infeasible. To tackle these hurdles, we introduce Bayesian\nOptimization for Fleet Design (BOFD), a framework leveraging multi-objective\nBayesian Optimization to explore fleets on the Pareto front of performance and\ncost while accounting for uncertainty in the design space. Moreover, we\nestablish a sub-linear bound for cumulative regret, supporting BOFD's\nrobustness and efficacy. Extensive benchmark experiments in synthetic and\nsimulated environments demonstrate the superiority of our framework over\nstate-of-the-art methods, achieving efficient fleet designs with minimal fleet\nevaluations.",
        "updated": "2024-08-21 16:22:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11751v1"
    }
]