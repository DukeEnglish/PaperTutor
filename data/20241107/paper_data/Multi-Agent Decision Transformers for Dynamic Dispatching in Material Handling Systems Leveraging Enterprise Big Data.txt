MULTI-AGENT DECISION TRANSFORMERS FOR DYNAMIC
DISPATCHING IN MATERIAL HANDLING SYSTEMS LEVERAGING
ENTERPRISE BIG DATA
XianYeowLee HaiyanWang DaisukeKatsumata
xian.lee@hal.hitachi.com haiyan.wang@hal.hitachi.com daisuke.katsumata@hal.hitachi.com
IndustrialA.I.Lab. IndustrialA.I.Lab. JRAutomationCollaborationProject
HitachiAmericaLtd. HitachiAmericaLtd. HitachiAmericaLtd.
TakaharuMatsui ChetanGupta
takaharu.matsui@hal.hitachi.com chetan.gupta@hal.hitachi.com
JRAutomationCollaborationProject IndustrialA.I.Lab.
HitachiAmericaLtd. HitachiAmericaLtd.
ABSTRACT
Dynamicdispatchingrulesthatallocateresourcestotasksinreal-timeplayacriticalroleinensuring
efficientoperationsofmanyautomatedmaterialhandlingsystemsacrossindustries. Traditionally,the
dispatchingrulesdeployedaretypicallytheresultofmanuallycraftedheuristicsbasedondomain
experts’knowledge. Generatingtheserulesistime-consumingandoftensub-optimal. Asenterprises
increasinglyaccumulatevastamountsofoperationaldata,thereissignificantpotentialtoleverage
thisbigdatatoenhancetheperformanceofautomatedsystems. Onepromisingapproachistouse
Decision Transformers, which can be trained on existing enterprise data to learn better dynamic
dispatchingrulesforimprovingsystemthroughput. Inthiswork,westudytheapplicationofDecision
Transformersasdynamicdispatchingpolicieswithinanactualmulti-agentmaterialhandlingsystem
andidentifyscenarioswhereenterprisescaneffectivelyleverageDecisionTransformersonexisting
bigdatatogainbusinessvalue. OurempiricalresultsdemonstratethatDecisionTransformerscan
improvethematerialhandlingsystem’sthroughputbyaconsiderableamountwhentheheuristic
originallyusedintheenterprisedataexhibitsmoderateperformanceandinvolvesnorandomness.
Whentheoriginalheuristichasstrongperformance,DecisionTransformerscanstillimprovethe
throughputbutwithasmallerimprovementmargin. However,whentheoriginalheuristicscontainan
elementofrandomnessorwhentheperformanceofthedatasetisbelowacertainthreshold,Decision
Transformers fail to outperform the original heuristic. These results highlight both the potential
andlimitationsofDecisionTransformersasdispatchingpoliciesforautomatedindustrialmaterial
handlingsystems.
1 Introduction
Dynamic dispatching, the process of dispatching resources in real-time in response to system conditions, plays a
criticalroleinensuringsmoothandefficientoperationsinmanyindustrialapplications. Subsequently,thistranslatesto
additionalbusinessvalue,suchascostsavingsandincreasedcustomersatisfaction. Oneareawherethedeploymentof
dynamicdispatchinghasalargeimpactisinmaterialhandlingsystems,wheregoodsaretypicallytransportedbetween
multiplepointsundertheconstraintoflimitedresources.
Traditionally, dynamic dispatching in these systems is often deployed using heuristic rules manually designed via
a trial-and-error process or by a domain expert. Dynamic dispatching and scheduling are also applied in a wide
variety of fields, not just material handling systems, and there have been multiple works that attempt to generate
dispatchingrulesviadomainknowledgeandheuristicsDhurasevicandJakobovic[2018],Brankeetal.[2015]. These
4202
voN
4
]IA.sc[
1v48520.1142:viXraAPREPRINT
approachesaretime-consuming,andtheavailabilityofdomainexpertsisusuallynotguaranteed. Anotherapproachto
deployingdynamicdispatchingstrategiesistoemployoptimization-basedmethodsQinetal.[2021],Zhangetal.[2023].
Nevertheless,amajorlimitationofthiscategoryofapproachesisthattheycanbetime-consumingandcomputationally
inefficienttocomputeasolutionwheneveradispatchdecisionisrequired.
Morerecently,data-drivenmethodssuchasmachinelearning(ML)andreinforcementlearning(RL)-basedapproaches
hasalsobeenproposedPrioreetal.[2014],Dingetal.[2023]. WhileML-basedapproachesfocusesonimitatingthe
performanceofstaticdatasets,RL-baseddynamicdispatchingpoliciesexploreagivenenvironmentduringtrainingto
discoverbetterpoliciesKangetal.[2019],Jeongetal.[2021],Zengetal.[2023],Leeetal.[2024]. Assuch,RL-based
approaches avoid the lengthy trial-and-error process of designing heuristics and circumvent the need to solve an
optimizationeverytimedispatchingisneeded. However,thetrainingprocessremainsthemainbottleneckindeploying
RL-basedpolicies. ItisoftennotfeasibletotrainRLpoliciesonactualsystemsduetosafetyconsiderationsGuetal.
[2022]andduetothelengthytrainingdurationrequiredbecauseofthelargenumberofinteractionstypicallynecessary
totrainagoodpolicy. Subsequently,mostRLpoliciesaretrainedonsimulators,whichprovideasafeplaceforRL
policiestoexploreandenabletrainingbeyondreal-timespeed. Nevertheless,developingasimulatorisalsooftencostly,
andRLpoliciestrainedinsimulatorenvironmentsoftensufferfromthesim-to-realgapZhaoetal.[2020],Salvatoetal.
[2021].
ToaddressthecomplexityoftrainingonlineRL-basedalgorithms,numerousworksintheliteraturehaveadvocatedthe
ideaofofflineRL:thatis,howtomaximallyextractanoptimalpolicygivenastaticdatasetPrudencioetal.[2023],
Levineetal.[2020]. Withtheproliferationofbigdatainindustrialsettings,vastamountsofhistoricaloperationaldata
areoftenavailable,providingarichresourcefortrainingmoreeffectiveandrobustpoliciesandpotentiallyleading
toimprovedsystemperformance. InthecontextofusingRLfordynamicdispatchinginmaterialhandlingsystems,
thisisacompellingparadigm,asadatasetcouldbecollectedbasedonhistoricalreal-timedatausingasub-optimal
dispatchingpolicy,whichcouldthenbeusedtotrainabetterRL-basedpolicy.
Unfortunately,offline-RLmethodsareknowntobeover-optimisticintermsofvalueestimationKumaretal.[2020]
andhighvarianceofreward/gradientestimationLevineetal.[2020],whichcouldresultindestabilizedpolicytraining.
Inresponse,manyeffortsinthecommunityhavebeendirectedatdevelopingbetteroffline-RLmethodstomitigate
theseissueswhilestillleveragingthebenefitsofhistoricaldataandRL-basedapproachesLevineetal.[2020],Agarwal
etal.[2020],Prudencioetal.[2023].
Recently,Chenetal.[2021]proposedtoreformulatetheofflineRLproblemintoasequencemodelingproblemby
leveragingthepowerfulmodelingcapabilitiesofatransformerarchitectureVaswani[2017]. TheyproposedDecision
Transformers,whichhavebeenshowntobeastrongalternativetoexistingofflineRLalgorithmswithoutexplicitly
learningavaluefunctionBhargavaetal.[2023],andhasbeenappliedinmultipleareasofworkYuanetal.[2024].
AsmostexistingworkofDecisionTransformersaremainlyfocusedonbenchmarkproblems,inthiswork,weare
motivatedtostudyifDecisionTransformerscanalsobeappliedeffectivelytocomplex,real-worldindustrialproblems
duetotheirabilitytoleverageexistingenterprisedata,simplicityintermsofimplementationandpotentialtobetrained
inalargescale,parallelizedsetting. Specifically,weinvestigatethefeasibilityofusingDecisionTransformersasafirst
stepindevelopingdata-drivendynamicdispatchingstrategiesinamulti-agentsettingtomaximizethethroughputofa
materialhandlingsystem. Ourcontributionsarefocusedonansweringthefollowingquestions:
• CanDecisionTransformerstrainonexistingenterpriseoperationaldatatodiscovermoreeffectivepolicies
withinacomplexreal-worldmaterialhandlingsystemwheredispatchingdecisionsarerequiredatdifferent
pointsofthesystemasynchronously,i.e.,giventhemulti-agentasynchronoussetting,canindependentDecision
Transformers“stitch”lowerrewardtrajectoriesfromsub-optimalheuristicstoachievehigherrewardsduring
testing?
• PreviousworkbyPasteretal.[2022]hasshownthatDecisionTransformersperformsbadlywhentrained
onstochasticenvironmentsandweaimtostudyhowareindependentDecisionTransformersdeployedin
multi-agentsettingsaffectedbyenvironmentalanddatastochasticity?
• How do datasets generated from different heuristic qualities affect the final performance of the Decision
Transformers?
2 Background
2.1 MaterialDispatchingSimulator
Inthissection,wedescribeaninstantiationofanactualmaterialhandlingsystemusedinthisstudy. Specifically,we
studyamaterialhandlingsystemwithaconveyorbeltcommonlyusedinwarehousefacilities. Toexplainthesystem,a
2APREPRINT
Figure1: Asimplifiedillustrationofthematerialhandlingsystemweusedinthiscasestudy.
simplifiedversionofthesystemlayoutisshowninFigure1. Ingeneral,thematerialhandlingsystemconsistsoftwo
processes: shippingandreceiving;andthreetypesofprocesspoints: incoming,storageandoutgoing. Inthereceiving
process,goodsenterthematerialhandlingsystemviatheincomingprocesspoints(redcircles). Thesegoodsareplaced
onemptypalletsontheconveyorbeltanddynamicallydispatchedtooneofthemanypossiblestoragepoints(white
circles). Simultaneously,intheshippingprocess,goodsareretrievedfromthestoragepoints,placedonemptypallets,
andsenttotheoutgoingpoints(bluecircles)tosatisfydownstreamtasks,suchasfulfillingacustomerorder. Acommon
KeyPerformanceIndex(KPI)foroptimizingthesesystemsistomaximizethethroughputoftheentiresystem;inthis
case,thecombinedthroughputofgoodsenteringthestoragepointsandthegoodsbeingsenttotheoutgoingpoints,
wherethroughputisthedefinedastheamountofmaterialoritemspassingthroughaprocessoveraspecifiedperiodof
time. Intheshippingprocess,thestoragepointswheregoodsareretrievedandtheoutgoingpointswherethegoods
aresentarespecifiedbythedownstreamdemandrequests. However,inthereceivingprocess,whichstoragepointthe
goodsshouldbedispatchedtowhenitarrivesatanincomingpointshouldbecarefullydeterminedinreal-timeinorder
tooptimizetheKPI.Thedispatchingdecisionsaretraditionallymadebyheuristicsthatareeithercraftedbydomain
expertsorbuiltthroughatrial-and-errorprocess.
Thesesystemstypicallycomewithmultipleconstraints,makingthedynamicdispatchingdecisionsatincomingpoints
complex. Oneofthemainbottlenecksofoptimizingthethroughputofthissystemisthatallthepointprocessessharea
commonresource: thepalletscirculatingthesystemontheconveyor. Eachpointprocessalsohasadesignatedbuffer,
whichonlyallowsacertainnumberofincomingpalletstowaitinaqueue,andanyadditionalpalletsdispatchedtothe
pointprocessarecirculatedaroundtheconveyorbeltuntilthebufferhasavacantspace. Additionally,thereisanaspect
ofstochasticitypresentinthesystemintermsofthegoodsbeingretrievedfromthestoragethatisdependentonthe
demandoftheoutgoingpoints. Together,theseconstraintsgiverisetonumerouscomplexities,suchascongestionat
processpointsiftoomanypalletsaredispatchedtothesameprocesspointsoridlingprocesspointswhenthereare
insufficientemptypalletsfortheprocesspointstoloadandunloadgoodsontothepallets,thusnegativelyimpacting
thethroughputofthesystem. Hence,deployingagooddynamicdispatchingpolicyisimperativetomaximizingthe
system’sthroughput. Inthissystem,weconsiderdynamicdispatchingpoliciesthatcanbedeployedateachofthe
incomingpoints,wherethedecisionspaceofeachpolicyconsistofallthestoragepoints.
2.2 DecisionTransformers
DecisionTransformers,introducedbyChenetal.Chenetal.[2021],reformulatetheofflinereinforcementlearning
(RL)problemintoasequencemodelingproblem,leveragingtheauto-regressivearchitectureofGPT-2Radfordetal.
[2019]. Themodeltakesasinputasequence,oflengthk,ofpaststates{s ,...,s },actions{a ,...,a },and
t−k t t−k t−1
returns-to-go{R ,...,R },whereR =
(cid:80)T
r ,representingfuturecumulativerewards. Ateachtimestept,
t−k t t t′=t t′
theinputtuplex = (s ,a ,R )isencodedintoasequenceX = [x ,...,x ],whichisprocessedbyaseriesof
t t t−1 t t 0 t
transformationtoproducehiddenstatesH =Transformer(X ). Themodelpredictsthenextactionaˆ byapplyinga
t t t
lineartransformationandsoftmaxfunctiontothefinalhiddenstateh ,sothataˆ =softmax(Wh +b),whereW and
t t t
barelearnedparameters. Themodelistrainedusingasupervisedlossfunction,conventionallythemeansquarederror
forcontinuousactions:
(cid:34) T (cid:35)
L =E (cid:88) ∥a −aˆ ∥2
supervised (s,a,R)∼D t t
t=0
3APREPRINT
orthecross-entropylossfordiscreteactions:
(cid:34) T (cid:35)
(cid:88)
L =E −log(pˆ[a ])
supervised (s,a,R)∼D t t
t=0
wherea isthetrueactionattimet,aˆ isthepredictedcontinuousaction,pˆ isthepredictedprobabilitydistributionfor
t t t
discreteactions,andpˆ[a ]isthepredictedprobabilityforthetruediscreteactiona .
t t t
Duringdeployment,itisconditionedontheinitialstates anddesiredcumulativerewardR topredictthefirstaction,
0 0
thenautoregressivelygeneratessubsequentactions,eachconditionedontheupdatedsequenceofpaststates,actions,
andremainingrewards,withR =R −r . ThismethodofferssignificantadvantagesovertraditionalRLmethods,
t+1 t t
particularlyinofflinesettingswherethemodellearnsfromstaticdatasetswithoutexploration,andhasdemonstrated
competitiveperformanceonseveralbenchmarks. Forfurtherdetails,wereferreaderstotheoriginalpaperChenetal.
[2021].
Figure 2: Overview of our proposed framework of using multiple Decision Transformers as dynamic dispatching
policiesinamulti-agentsetting.
3 FormulationandMethodology
3.1 Formulation
ThissectionpresentstheformulationthatallowsustoapplyDecisionTransformersasdynamicdispatchingpolicies.
Formally,theproblemofdynamicdispatchinginmaterialhandlingsystemscanbedefinedasselectingasequenceof
decisionsateachincomingpointsuchthatthemaximumthroughputofthesystemismaximized. This,inturn,couldbe
abstractedintoanRLformulation,whereanRL-basedpolicyobservesthestateofthesystemandselectsanaction,
representingthedispatchingdecision,tomaximizeaspecificreward,inthiscase,thetotalthroughput.Withthedynamic
dispatchingproblemcastasanRLproblem,wecanthenfollowtheconventionalroutineofcollecting/generatinga
staticdatasetconsistingoftuplesof(state,action,reward,done)andtrainoff-the-shelfofflineRLalgorithmsonit. In
thisstudy,wefocusonDecisionTransformersduetotheirpowerfulcapabilitiesinattendingtosequentialdataand
relativeeaseofimplementation.
Anotherdesignchoicewemadeinthisformulationisthechoiceofrepresentation. Astherearemultipleincoming
pointswheredispatchingoccurs,wefaceachoiceofrepresentingthedispatchingpolicyusingasinglecentralized
DecisionTransformerversusmultipledecentralizedDecisionTransformers. Sincedispatchingattheincomingpoints
occursasynchronously,wehypothesizethatrepresentingthedispatchingpoliciesinamulti-agentparadigmwillbe
morebeneficialforseveralreasons. ThefirstreasonisthatifweuseacentralizedDecisionTransformer,intuitively,the
modelwouldhavetolearnnotjusttomodelthejointdistributionsofstatesandactionsbutalsotomodeltheprobability
ofaneventoccurringatacertainincomingpoint,whichissignificantlymorechallenging. Additionally,acentralized
DecisionTransformerlimitstheapproach’sscalability,asaddingincomingpointstothesystem’slayoutwouldrequire
retraining the model to account for the change in action space. In contrast, using multiple Decision Transformers
simplifiesthemodelingtaskandpotentiallyallowsustore-usethesametrainedmodelfornewincomingpointssince
4APREPRINT
conveyorlayoutsareoftenreplicatedinmaterialhandlingsystems. Nevertheless,usingamulti-agentparadigmalso
hasitsdrawbackssincethatinherentlyrequirestrainingmultiplemodelsandpotentiallyintroducestheproblemof
coordinationbetweendifferentmodels. Figure2providesanoverviewoftheframeworkinthispaper,whichrepresents
eachincomingpoint’sdynamicdispatchingpolicyusingaseparateDecisionTransformerinamulti-agentsetting.
3.2 EnvironmentandHeuristicsDetails
Duetoconfidentialityrestrictionspreventingtheuseofactualenterprisedataforpublication,wedevelopedanin-house
high-fidelitysimulatorthatutilizesdatadistributionsobtainedfromactualenterprisedatatoreplicatetheactualmaterial
handling system operations and generate the data for training Decision Transformers. We implemented multiple
heuristicsthathavebeentestedontheactualsysteminthesimulatorandverifiedthatthesimulator’sspecificationsand
properties,suchastheprocessingtimesandthroughputmatchtheactualsystem. Aftervalidatingthatthesimulatoris
calibratedtotheactualsystemfaithfully,weusedthesimulatortogeneratedatafortrainingtheDecisionTransformers.
Althoughweusedsimulateddatainthiswork,weemphasizethattheframeworkremainsdirectlyapplicabletoactual
data. Tocollectthesimulationdatainausableform,wedefinethesimulator’sstateasallthesensorinformationthatis
availableinreal-timeandusedbytheexistingheuristics: 1)Numberofpalletsheadingtoeachstoragepoint,2)Number
ofpalletsatconveyor’sjunctiongoingintoeachdownstreamdirection,3)Inventorylevelateachstorage,whichleads
toa44-dimensionalstatevector. Theactionspaceisdefinedasthedispatchingdecisionsandtherewardisthetotal
throughputofthesystem. Thefulldetailsoftheactualsystem’slayoutareshowninTable1
Table1: MaterialHandlingSystemSpecifications
Specification Value
NumberofLoops 3
NumberofIncomingPoints 4
NumberofStoragePoints 20
NumberofOutgoingPoints 6
NumberofJunctionPoints 4
IncomingPointsProcessingTime 5sec
StoragePointsProcessingTime 10sec
OutgoingPointsProcessingTime 6sec
JunctionPointsProcessingTime 0.5sec
BuffersizeforIncomingPoints 4
BuffersizeforStoragePoints 8
BuffersizeforOutgoingPoints 10
NumberofPallets 500
Simulationresolution 0.1sec/step
Followingtheconventionofthebaselinespresentedin Leeetal.[2024],weimplementedfourheuristicswithdifferent
skilllevels,denotedas’Low’,’Medium’,’High’,basedontheobservedperformance,and’Random’. Wefirstdefine
severalauxiliaryvariablesandfunctionsthatareemployedacrosstheseheuristics. Eachstoragepointinthesystem
isassociatedwithaspecific"loop"intheconveyorsystem. Foranyincoming-storagepointpair,wecategorizethe
storagepointsaseitherbeinginthesameloopastheincomingpoint(S )orindifferentloops(S ). Wealsodefine
same other
S =S ∪S asthesetofallstoragepoints. LetIn(s)denotethenumberofincomingpalletsandOut(s)denote
all same other
thenumberofoutgoingpalletsatstoragepoints. Additionally,wedefineX andX asthenumberofpallets
same other
assignedtostoragepointsinthesameloopandotherloops,respectively,foragivenincomingpoint. Inthesecond
heuristic,weintroduceacostfunctioncost(L ),whichisdefinedas:
i
X −X
cost(L )= same min +C
i X −X Lj
max min
whereL denotesaspecificloopi,X andX representthemaximumandminimumnumberofpalletsacrossall
i max min
loops,andC isahyperparameterthatreflectsthedistancecostbetweenloopsiandj.
Lj
Thethreeheuristicsaresummarizedasfollows: Thefirstheuristicrandomlyassignspalletstostoragepointswithinthe
sameloop(S ),basedonproximity. Thesecondheuristicselectsstoragepointsbasedonacombinationofbuffer
same
occupancy, distance, andpallet flow (inbound/outbound), usingthe costfunctionto prioritizeloops. Intuitively, it
selectsstoragepointsbyfilteringbasedonexpectedbufferoccupancy,thecostfunction,andthenchoosingthestorage
pointwiththefewestincomingpallets. Thethirdheuristicextendsthesecondheuristicbyaddinglogictohandleloop
5APREPRINT
congestion,filteringbypalletdistributionacrossloopsandexpectedbufferoccupancybeforefinallyselectingastorage
pointbasedonthesmallestdifferencebetweenoutgoingandincomingpallets. Thealgorithmscorrespondingtothese
heuristics,referredtoasLow,Medium,andHigh,aredetailedinAlgorithms1,2,and3,respectively. Theseheuristics
weredevelopedthroughexpert’sknowledgeandfine-tunedusingsimulationmodelstooptimizeperformance. Finally,
wealsoimplementeda’Random’heuristic,whichisasimplebaselinethatdispatchesthepalletstorandomstorage
points.
Fordispatchingdecisionsatjunctions,aheuristicdirectsemptypalletstowardsconveyorloopswiththefewestpallets.
Algorithm1:Heuristic1(Low)
Input: Environment,NumberofEpisodesN,EpisodeHorizonT
Result: Actiondecisionsa ateachtimestep
t
forEpisode=1toN do
fort=1toT do
Observestates andeventindicatorI fromEnvironment;
t t
ifI isTruethen
t
S←S // Get set of storage points within the same loop
sameloop
a ∼S// Sample and dispatch to random storage point
t
else
Skipa // Non-event transition
t
end
end
end
Algorithm2:Heuristic2(Medium)
Input: Environment,NumberofEpisodesN,EpisodeHorizonT
Input: ParameterC
1
Result: Actiondecisionsa ateachtimestep
t
forEpisode=1toN do
fort=1toT do
Observestates andeventindicatorI fromEnvironment;
t t
ifI isTruethen
t
S←S // Get set of all storage points
all
S←{s∈S|In(s)≤C }// Get storage points with fewer than C incoming
1 1
pallets
S←minCost(S)// Get storage points in loop with minimum cost
if|S|=1then
a ←s∈S// If only one storage point in set, select it
t
else
a
t
←argmin s∈S(In(s))// Select storage with smallest number of incoming
pallets
end
else
Skipa // Non-event transition
t
end
end
end
3.3 DataGeneration
Foreachheuristic,weran4000one-hoursimulations,correspondingto4000episodes. Aone-hoursimulationtime
periodwasselectedbecauseitrepresentsatime-framewithsufficienteventsoccurringandisalsoacommonlyused
time-frameusedforcomparingthroughputintheindustry. Specifically,ateachincomingprocesspoint,wecollected
thedataintheformof(state,action,reward,done)wheneveranevent,i.e.,adispatchingdecisionisneeded,occurs
at the specific point. Since the events occur asynchronously for each incoming point, the total number of events
6APREPRINT
Algorithm3:Heuristic3(High)
Input: Environment,NumberofEpisodesN,EpisodeHorizonT
Input: ParametersC ,C ,C
1 2 3
Result: Actiondecisionsa ateachtimestep
t
forEpisode=1toN do
fort=1toT do
Observestates andeventindicatorI fromEnvironment;
t t
ifI isTruethen
t
X ←Numberofpalletsassignedtostoragepointsinthesameloop;
same
X ←Numberofpalletsassignedtostoragepointsinotherloops;
other
ifX <C andX <C then
same 1 other 2
S←S // Get set of all storage points
all
else
ifX <C andX >C then
same 1 other 2
S←S // Get set of storage points in the same loop
same
else
ifX >C andX <C then
same 1 other 2
S←S // Get set of storage points in other loops
other
else
S←S // Get set of all storage points
all
end
end
end
S←{s∈S|In(s)≤C }// Get storage points with fewer than C incoming
3 3
pallets
ifS\{S }≠ ∅then
others
S←S\{S }// Remove storage points that belong to other loops
others
end
if|S|=1then
a ←s∈S// If only one storage point in the set, select it
t
else
a
t
←argmin s∈S(Out(s)−In(s))// Select storage point with minimum out-in
difference
end
else
Skipa // Non-event transition
t
end
end
end
ateachincomingpointisdifferentforeachepisode. Nevertheless, weobservedthatthenumberofeventsateach
incomingpointfallswithintherangeof500to550eventsineachepisode. Insummary,wecuratedfourdifferent
datasetscorrespondingtofourdifferentheuristics,eachconsistingof4000trajectoriesforeachincomingpoint,yielding
approximatelyadatasetof2milliontransitionstotraineachDecisionTransformerforeachdispatchingpoint. All
simulationswereperformedona24-coreIntel(R)Core(TM)i9-10920XCPU.
3.4 Training
WetrainedaDecisionTransformerforeachdispatchingpointinthematerialhandlingsystemoneachgenerateddataset
andtrainedeachmodelfor50epochsbasedontheobservedconvergenceandplateauingofthelossfunction. Sincethe
dynamicdispatchinginthisproblemnecessitatesdiscreteactions,wetrainedthemodelsusingastandardcross-entropy
loss. OurimplementationisbasedonamodificationoftheimplementationbyBeechingandSimonini[2022]. All
trainingwasperformedonasingleNVIDIARTX4090GPUandthehyperparametersweusedinourexperimentsare
defaultimplementationparametersandareshowninTable2
7APREPRINT
Hyperparameters
Maxpastsequencelength(k) 20
Optimizer AdamW
Batchsize 256
Warmupratio 0.1
Max. grad. norm 0.25
Learningrate 1e-4
Weightdecay 1e-4
Numberoftrainepochs 50
Table2: DecisionTransformerhyperparameters
4 ResultsandDiscussion
4.1 Results
Inthissection,wepresentempiricalresultsofourexperimentsandanswerthequestionsweposed.
(a) (b)
Figure3:(a)DistributionofthroughputbetweenheuristicsofdifferentskilllevelsandDecisionTransformerstrainedon
thedatageneratedbytheheuristics. (b)AdditionalcomparisonofthethroughputdistributionsofDecisionTransformers
trainedondatageneratedbyadeterministiclow-skillheuristic(SLL)andondatageneratedbyastochastichigh-skill
onlineRLpolicy.
CanDecisionTransformersbeappliedtodynamicdispatchingproblemsinamulti-agentasynchronoussetting,
andcanthey“stitch”lowerrewardtrajectoriesfrompotentiallysub-optimalheuristicstoachievehighrewards
duringtesting?
Toanswerthequestionabove,wecomparethetotalthroughputoftheDecisionTransformersasdispatchingpolicies
withthecorrespondingheuristics’performanceusedtogeneratethetrainingdataacross50episodesand5random
seeds. Duringtesting,weusedthemedianthroughputachievedbytheheuristicsasthedesiredreturnratherthanusing
anarbitraryreturn. Figure3(a),presentstheresultsofthecomparison. Weobservethatforexperimentswherethe
DecisionTransformersweretrainedondatacollectedunderthe’Medium’and’High’heuristics,DecisionTransformer
policiesoutperformtheheuristics’originalperformance. Conversely,forDecisionTransformersthatweretrainedusing
datageneratedfrom’Random’and’Low’heuristics,theDecisionTransformersseverelyunder-performtheheuristics.
We hypothesize that the Decision Transformers trained on the ’Low’ and ’Random’ data performed poorly due to
twopossiblereasons: 1.) First,the’Random’and’Low’heuristicswerebothrulesthatcontainaninherentaspectof
randomnessinthedecision-making. Incontrast,the’Medium’and’High’heuristicswerefullydeterministicrules.
Hence,onepossibilityisthatDecisionTransformersdonotperformwellwhentrainedondatageneratedwithsome
8APREPRINT
Figure4: DistributionofthroughputofDecisionTransformersofdifferentskilllevelsconditionedondifferentspecified
throughput. ’DT-R’,’DT-L’,’DT-M’,’DT-H’denotemodelsthataretrainedonthe’Random’,’Low’,’Medium’and
’High’datasetsrespectively.
aspectofinherentrandomness. 2)Secondly,anotherpossibilityisthereisacertainthresholdofperformancethatneeds
tobeexhibitedbythetrajectoriesoftheheuristicsfortheDecisionTransformertostitchtogethertogeneratebetter
trajectories. SinceDecisionTransformerisanofflinemethodandisinherentlydata-dependent,thiscouldalsobea
possibilitythatexplainsthepoorperformanceoftheDecisionTransformertrainedonthe’Low’heuristic’sdata.
Toverifyourhypothesis,weconductedtwoadditionalexperiments. Wedesignedafourthheuristicwitharelatively
lowperformancewhilekeepingthelogicfullydeterministic. Specifically,wedesignedtheheuristictoselectstorage
points in the same loop as the incoming points and send the pallets to the storage points with the least number of
palletsthatareassignedtothem. Thisissimilartothe’Low’heuristic,exceptthatinsteadofassigningtoarandom
storage point, we assign it to the least busy storage point. We denote this heuristic as ’SLL’, representing “Same
Loop-Least". ThisexperimentistostudytheimpactofdatawithlowperformanceonthetrainingoftheDecision
Transformer. Additionally,wealsoconductedanotherexperimentwherewetrainedanonlineRLalgorithminthe
simulatorusingamulti-agentPPOalgorithmYuetal.[2022]. Subsequently,weusethisonlinepolicytogeneratea
staticdatasetfortrainingDecisionTransformersfollowingthesameproceduresabove. Thepurposeofthisexperiment
istodetermineifrandomnessaffecttheperformanceofthetrainedDecisionTransformer,sincetheonlineRLmethod
isabletoexploreandcollectmoretrajectoriestoachieveahigherperformance,whiletheactiongeneratedbytheRL
algorithmissampledfromalearneddistribution. Wedenotethismethodas’RL’.Wereferreadersinterestedinthe
detailsofthisapproachtothefollowingworkYuetal.[2022].
Figure3(b),showstheresultsoftheadditionalexperiments,withtheresultsofthe’Low’and’Random’heuristics
shownforadditionalreference. Comparingthe’SLL’withthe’Low’heuristic,weobservethatthe’SLL’heuristic
performanceisslightlylowerthanthe’Low’heuristic. However,despitetheheuristicbeingdeterministic,weseethat
theDecisionTransformertrainedonthe’SLL’data(DT-SLL)stillperformsworsethantheheuristic. Thisconfirmsthe
hypothesisthatevenintheabsenceofrandomness,datawithhighperformancetrajectoriesisneededtotrainaDecision
Transformerthatoutperformstheheuristic. Ontheotherhand,wecanseethatanRLagenttrainedinanonlinefashion
isabletoachieveahighthroughputvalue,similartotheperformanceofthe’High’heuristic. Nonetheless,despite
thedatageneratedbytheRLpolicyhavinghighperformance,theDecisionTransformertrainedonthosedataalso
significantlyunderperformstheheuristic’sthroughputvalue. Thishighlightsanotherlimitationthatthedatagenerated
needstobedeterministicaswell,confirmingourearlierhypothesis.
Basedonthetheresultspresented,weanecdotallyconcludethatinthiscase,DecisionTransformerscan’stitch’multiple
rewardtrajectoriestoemithighrewardtrajectories,subjecttothefactthatthedatathesemodelsaretrainedonwere
notgeneratedwithsomeaspectofinherentrandomnessandtherewardofthetrajectorieshavetobeoveracertain
threshold,ratherthantrajectorieswithanyarbitraryperformance. Table3tabulatesthesummarystatisticsofthebox
plotsshowninFigure3.
9APREPRINT
Experiments Min 1stQ Median 3rdQ Max
Random 4213 4301 4349 4380 4479
Low 4040 4118 4150 4183 4317
Medium 3970 4128 4180 4221 4405
High 4357 4505 4552 4589 4738
SSL 3624 3731 3765 3805 3897
RL 4340 4594 4642 4700 4845
DT-Random 2480 2946 3142 3283 3750
DT-Low 2233 2568 2777 3201 3471
DT-Medium 4246 4376 4432 4473 4571
DT-High 4510 4574 4599 4620 4670
DT-SSL 1965 2133 2214 2255 2355
DT-RL 2027 2087 2121 2143 2225
Table3: Numericalstatisticsofbox-plotsshowninresults.
Additionally,despitetheDecisionTransformerstrainedondataderivedfromdeterministichigh-performingheuristics
outperformingtheircorrespondingheuristics,wenotethattheachievedtotalthroughputdoesnotmatchthespecified
desired total throughput. To further investigate the behaviors of these Decision Transformers, we conducted five
additionalexperimentsbyconditioningthemodelsbasedonarangeofvaluescenteredaroundthemedianthroughput
(specifically,median±100,200). Figure4showstheexperimentresults. Empirically,weobservethatregardlessofthe
typesofheuristicsusedtogeneratethetrainingdata,theDecisionTransformersshowednoclearcorrelationbetween
thespecifieddesiredrewardsandactualrewards.
AreDecisionTransformersaffectedbydatawithinherentstochasticity?
Figure5: UMAPprojectionoftheuniquestatevisitationsgeneratedbythesixheuristicsofdifferentskilllevelin(a)
the1stand2nddimensionand(b)the2ndand3rddimension.
Asalludedtointhepreviousparagraph,wehavedemonstratedthatDecisionTransformersareaffectedbystochasticity
intermsofactiondatageneratedbytheheuristic. However,wealsohighlightthatthestatetransitionsgeneratedin
ourdatasetarealsostochastic,duetorandomnessintermsofthegoodsbeingretrievedfromthestoragepoints. As
such,basedonourobservationsinFigure3,wecanonlyanecdotallyconfirmthatavanillaimplementationofDecision
Transformersislessaffectedbystatestochasticitythanactionstochasticity. Nevertheless,Pasteretal.[2022]hasalso
demonstratedthatstochasticityinstatescouldcontributetothelackofalignmentbetweenthespecifiedandachieved
rewardsandthatabetterrewardcouldbeachievedbyclusteringthetrajectoriesandconditioningthemodelsonthe
averageclusterrewardsduringtraining. Weintendtoinvestigatethisdirectioninfutureworks.
HowdoesdataqualityaffecttheperformanceofDecisionTransformer?
10APREPRINT
(a) (b)
(c) (d)
Figure6: Comparisonofthroughputforheuristics,DecisionTransformerstrainedonasingledatasetandDecision
Transformerstrainedondatasetsgeneratedfrommultipleheuristics,conditionedonarangeofdesiredrewards. The
postfix’L’,’M’,’H’denotesthespecificheuristicfromwhichtherangeofdesiredrewardswasdetermined. (a)’LM’
denotesdatasetwithtrajectoriesfromthe’Low,Medium’data,(b)’LH’denotesdatasetwithtrajectoriesfromthe’Low,
High’data,(c)’MH’denotesdatasetwithtrajectoriesfromthe’Medium,High’data,(d)’LMH’denotesdatasetwith
trajectoriesfromthe’Low,Medium,High’data.
Inthissection,weconductastudyintohowthequalityofdataaffectsthefinalperformanceofDecisionTransformers.
Asafirststep,weconductedadimensionalityreductionofstatesinourdatasetstovisualizetheuniquestatevisitations
inourdatasets. Toachievethat,weremovedallduplicatesofstatesinthecollecteddatasetsandprojectedthemintoa
3-dimensionalspaceusingUMAPMcInnesetal.[2018]. Figure5(a)and(b)showtheprojectionofhigh-dimensional
statesintothe1st-2ndand2nd-3rddimensions,respectively,withthecolorscorrespondingtodatageneratedwiththe
sixdifferentheuristics. Weobservedthatthestatevisitationsofthedatageneratedbythe’Random,’’Medium,’and
’High’formarelativelytightclusterinthefirsttwodimensions. Incontrast, the’Low’and’SLL’dataformstwo
relativelyseparateclustersthatisdistantfromtherestofthedata. Finally,thedatageneratedbythehigh-performing
onlineRLapproachalsoformsafourthdistinctclusterofstatevisitations. Asimilarobservationcanbealsobemadein
the2nd-3rddimensionofUMAPprojectionasseeninFigure5(b). Themaintakeawayofthesefiguresisthatdifferent
11APREPRINT
heuristicsemitstatevisitationsthatcoverdifferentregionsofthestatespaceandtrainingDecisionTransformerson
datageneratedbyasingleheuristicsmaybesub-optimalsincethecoverageofstatesvisitedduetoanysingle,manually
craftedheuristicscouldpotentiallybelimited.
Next, we study the effect of combining datasets with different skill levels on the performance of the Decision
Transformers. Specifically,wecuratedmultipleadditionaldatasetswith12,000trajectorieseach,denotedas’LMH,’
’LH,’’LM,’and’MH.’The’LMH’dataismadeupof4000trajectoriessampledfromthe’Low,’’Medium’and’High’
datasetseach,whilethe’LH,’’LM,’and’MH’dataismadeupof6000trajectoriessampledfromthe’Low-High,’
’Low-Medium’and’Medium-High’datarespectively. Were-trainedtheDecisionTransformersbasedonthesedataand
conditionedthemodelsbasedonarangeofthroughputvaluesachievedbythe’Low,’’Medium,’and’High’heuristics,
respectively. Figure6presentstheresultsoftheexperiments. Oneapparentobservationisthattheinclusionofdata
generatedbydeterministic,highperformingheuristicssignificantlyoutweighstheeffectofdatageneratedbyheuristics
withrandomnessorheuristicswithlowperformance. ThiscanbeseeninFigures6(a),(b),and(d),wherethemodels
trainedwithacombinationofdatasampledfromthe’Medium’or’High’and’Low’datasetssignificantlyimprove
overthemodelstrainedonjustthe’Low’dataset. WealsoobservethattheDecisionTransformerstendtobebiased
towardstheperformanceofthe’High’dataset. Forexample,inFigures6(b)and(d),theDecisionTransformersstillfail
toachievethespecifiedrewardsbelongingtotheregionofthe’Low’heuristicsdespiteincludinganequalamountof
datafromthe’Low’datasetintothecombineddatasets. Furthermore,fromFigure6(b),weobservethattheDecision
Transformeralsofailedtoachievethespecifiedrewardsofa’Medium’heuristic,despitehavingtrainedon’Low’and
’High’dataandhavingseenawidercoverageofstatevisitationsasshowninFigure5(a). Insummary,weconcludethat
inourapplication,DecisionTransformerstendtobebiasedtowardshigh-performingdata,generallyfailtointerpolate
betweenthedatasetstoachieveunseentargetreturns,andtheirperformancedoesnotseemtoimprovewithawider
qualityofdata. However,thismightbespecifictoourapplicationandmaybeattributedtooneormorefeatures,such
asbeinginamulti-agentsettingandthepresenceofstochasticityinthestateandactiondata.
4.2 Discussions
Inthissection, wediscusstheimplicationsofthisworkfromanindustrypointofview. Historicaldataownedby
enterprisesintheindustrialsectorholdssubstantialpotentialfortrainingDecisionTransformersfordecision-making
systemsinanofflinemanner. OurstudyhighlightsthatwhileDecisionTransformersholdpromise,theireffectiveness
issignificantlyinfluencedbytherandomnessintheunderlyingdatasetandsub-optimaldatacanseverelylimittheir
effectiveness. Tothisend,wepositthatfiltering/rankingtechniquesandstatisticalmethodscouldpotentiallybeapplied
tothedatasettoobtainasubsetofhigher-performingdataandtodetectthepresenceofrandomnessintheabsenceof
domainexpertsknowledgeoftheenterprisedata. Thisunderscorestheimportanceforindustriestonotonlygather
largevolumesofdatabutalsoservesasaguidelinetodeterminewhentrainingDecisionTransformerswillresultin
effectivedynamicdispatchingstrategies.
OneofthemainbenefitsofDecisionTransformersoranyofflineRLapproachingeneralfordecision-makingproblems
istheabilitytocircumventtherequirementofdevelopingahigh-fidelitysimulatorortrainingtheagentonanactual
system. However,tovalidatetheeffectivenessofthesemethodsinactualindustrialapplications,asimulatorisstill
oftenrequiredtotestandrefinethepoliciesasitisoftentoocostlyandriskytodirectlydeployadispatchingpolicy
onthesystem. Despitethis,enterpriseswithextensivehistoricaldatamaystillfindthismethodbeneficial. Froman
implementationperspective,itissimplertoleveragehistoricaldatatotrainDecisionTransformerstoachievemarginal
improvementsduetolesshyper-parametertuningrequired,providedthatthedatameetstherequiredconditions,as
comparedtoanonlineRLapproach. Nonetheless,deploymentofDecisionTransformer-baseddispatchingpolicies
ontoanactualsystemisalsosusceptibletothechallengesthatarecommontomostdata-drivenmachinelearning
methods,suchasintegrationwithexistinginfrastructure,distributiondriftsandtheoccurrenceofedgecases. While
thesechallengesarenon-trivial,webelievethatthey’renotspecifictoDecisionTransformersordynamicdispatching
problemsandcouldpotentiallybeovercomebysolutionssuchasameticuloussystemintegrationplans,real-timedata
monitoringplatformsandcarefuldesignofrulesforexceptionhandling,e.g.,datatransformationpipelinesthatensure
inputandoutputdatalieswithinanacceptablerange.
OurfindingshighlightedotherlimitationsofthecurrentimplementationofDecisionTransformers,particularlythe
lackofastrongcorrelationbetweenthespecifiedtargetreturnandtheactualreturnachieved. Thispointstotheneed
forfurtherexperimentationtoisolatethecauseoftheseissuesaswellasastudyintomoresophisticatedversionsof
DecisionTransformers,suchasthoseusingrewardpredictionmechanisms,toaddresstheseshortcomings. Webelieve
thatfuturestudiesfocusingonhowexistingdatacanbebetterutilizedoraugmentedwithdomainknowledgetoenhance
thecapabilitiesofDecisionTransformers,wouldbebeneficialtoimprovetheirapplicationinindustrialautomation
settings.
12APREPRINT
5 Conclusion
WeshowthatDecisionTransformershavethepotentialtobedeployedasdynamicdispatchingpoliciesinmaterial
handlingsystemsundertheassumptionthatexistingenterprisedatacanbetransformedintotherequiredformatand
thattheunderlyingmethodgeneratingthedataisnotinherentlyrandomandhasagoodinitialperformance. Duetothe
relativelysimpleimplementationofDecisionTransformers,thishasthepotentialtodemocratizethedeploymentof
thesemodelsinactualindustrialsystemstoimprovebusinessKPIssuchassystemthroughputwithouttheneedtotrain
onlineRL-basedmethods. Futureworkswillfocusoncomplementaryimprovementssuchasmethodsthatcouldbetter
incorporateprobabilisticdataintothetrainingprocess,bettercommunicationstrategiesbetweenthemultipleagents,
perhapsthroughacentralizedcriticandalsohyperparametertuningtoimprovetheexistingimplementationofDecision
Transformers. Additionally,benchmarkingtheperformanceofDecisionTransformersagainstotherstate-of-the-art
offlineRLmethodsandtestingthegeneralizationcapabilityofmulti-agentDecisionTransformerstootherdynamic
dispatchingsystemsarealsoimportantavenuesofstudy.
References
Marko Dhurasevic and Domagoj Jakobovic. A survey of dispatching rules for the dynamic unrelated machines
environment. ExpertSystemswithApplications,113:555–569,2018.
JürgenBranke,SuNguyen,ChristophWPickardt,andMengjieZhang. Automateddesignofproductionscheduling
heuristics: Areview. IEEETransactionsonEvolutionaryComputation,20(1):110–124,2015.
Wei Qin, Zilong Zhuang, Yaoming Zhou, and Yinbin Sun. Dynamic dispatching for interbay automated material
handlingwithlottargetingusingimprovedparallelmultiple-objectivegeneticalgorithm. Computers&Operations
Research,131:105264,2021.
FangfangZhang,YiMei,SuNguyen,andMengjieZhang. Surveyongeneticprogrammingandmachinelearning
techniquesforheuristicdesigninjobshopscheduling. IEEETransactionsonEvolutionaryComputation,2023.
PaoloPriore,AlbertoGómez,RaúlPino,andRafaelRosillo. Dynamicschedulingofmanufacturingsystemsusing
machinelearning: Anupdatedreview. AiEdam,28(1):83–97,2014.
Jiepin Ding, Mingsong Chen, Ting Wang, Junlong Zhou, Xin Fu, and Keqin Li. A survey of ai-enabled dynamic
manufacturingscheduling: Fromdirectedheuristicstoautonomouslearning. ACMComputingSurveys,55(14s):
1–36,2023.
YounkookKang,SungwonLyu,JeeyungKim,BongjoonPark,andSungzoonCho. Dynamicvehicletrafficcontrol
usingdeepreinforcementlearninginautomatedmaterialhandlingsystem. InProceedingsoftheAAAIConference
onArtificialIntelligence,volume33,pages9949–9950,2019.
YongkukJeong,TarunKumarAgrawal,ErikFlores-García,andMagnusWiktorsson. Areinforcementlearningmodel
formaterialhandlingtaskassignmentandrouteplanningindynamicproductionlogisticsenvironment. Procedia
CIRP,104:1807–1812,2021.
HaoZeng,QiongWu,KunpengHan,JunyingHe,andHaoyuanHu. Adeepreinforcementlearningapproachforonline
parcelassignment. InProceedingsofthe2023InternationalConferenceonAutonomousAgentsandMultiagent
Systems,pages1961–1968,2023.
XianYeowLee,HaiyanWang,DaisukeKatsumata,TakaharuMatsui,andChetanGupta. Multi-agentreinforcement
learningfordynamicdispatchinginmaterialhandlingsystems. arXivpreprintarXiv:2409.18435,2024.
ShangdingGu,LongYang,YaliDu,GuangChen,FlorianWalter,JunWang,YaodongYang,andAloisKnoll. Areview
ofsafereinforcementlearning: Methods,theoryandapplications. arXivpreprintarXiv:2205.10330,2022.
WenshuaiZhao,JorgePeñaQueralta,andTomiWesterlund. Sim-to-realtransferindeepreinforcementlearningfor
robotics: asurvey. In2020IEEEsymposiumseriesoncomputationalintelligence(SSCI),pages737–744.IEEE,
2020.
EricaSalvato,GianfrancoFenu,EricMedvet,andFeliceAndreaPellegrino. Crossingtherealitygap: Asurveyon
sim-to-realtransferabilityofrobotcontrollersinreinforcementlearning. IEEEAccess,9:153171–153187,2021.
RafaelFigueiredoPrudencio,MarcosROAMaximo,andEstherLunaColombini. Asurveyonofflinereinforcement
learning: Taxonomy,review,andopenproblems. IEEETransactionsonNeuralNetworksandLearningSystems,
2023.
SergeyLevine,AviralKumar,GeorgeTucker,andJustinFu. Offlinereinforcementlearning: Tutorial,review,and
perspectivesonopenproblems. arXivpreprintarXiv:2005.01643,2020.
13APREPRINT
AviralKumar,AurickZhou,GeorgeTucker,andSergeyLevine. Conservativeq-learningforofflinereinforcement
learning. AdvancesinNeuralInformationProcessingSystems,33:1179–1191,2020.
RishabhAgarwal,DaleSchuurmans,andMohammadNorouzi. Anoptimisticperspectiveonofflinereinforcement
learning. InInternationalconferenceonmachinelearning,pages104–114.PMLR,2020.
LiliChen,KevinLu,AravindRajeswaran,KiminLee,AdityaGrover,MishaLaskin,PieterAbbeel,AravindSrinivas,
andIgorMordatch. Decisiontransformer: Reinforcementlearningviasequencemodeling. Advancesinneural
informationprocessingsystems,34:15084–15097,2021.
AVaswani. Attentionisallyouneed. AdvancesinNeuralInformationProcessingSystems,2017.
PrajjwalBhargava,RohanChitnis,AlborzGeramifard,ShagunSodhani,andAmyZhang. Sequencemodelingisa
robustcontenderforofflinereinforcementlearning. arXivpreprintarXiv:2305.14550,2023.
WeilinYuan,JiaxingChen,ShaofeiChen,DaweiFeng,ZhenzhenHu,PengLi,andWeiweiZhao. Transformerin
reinforcementlearningfordecision-making:asurvey. FrontiersofInformationTechnology&ElectronicEngineering,
25(6):763–790,2024.
KeiranPaster,SheilaMcIlraith,andJimmyBa. Youcan’tcountonluck: Whydecisiontransformersandrvsfailin
stochasticenvironments. Advancesinneuralinformationprocessingsystems,35:38966–38979,2022.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are
unsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
EdwardBeechingandThomasSimonini. TrainyourfirstDecisionTransformer. Huggingface,2022. URLhttps:
//huggingface.co/blog/train-decision-transformers. Accessedon26thApril2024.
Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising
effectivenessofppoincooperativemulti-agentgames. AdvancesinNeuralInformationProcessingSystems,35:
24611–24624,2022.
Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for
dimensionreduction. arXivpreprintarXiv:1802.03426,2018.
14