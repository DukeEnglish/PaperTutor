Classification Done Right for Vision-Language
Pre-Training
ZilongHuang QinghaoYe BingyiKang JiashiFeng HaoqiFan
ByteDanceResearch
Code&Models: x-cls/superclass
Abstract
WeintroduceSuperClass,asupersimpleclassificationmethodforvision-language
pre-trainingonimage-textdata. UnlikeitscontrastivecounterpartCLIP [57]who
contrast with a text encoder, SuperClass directly utilizes tokenized raw text as
supervised classification labels, without the need for additional text filtering or
selection. Duetotheabsenceofthetextencodingascontrastivetarget,SuperClass
doesnotrequireatextencoderanddoesnotneedtomaintainalargebatchsize
as CLIP [57] does. SuperClass demonstrated superior performance on various
downstreamtasks,includingclassiccomputervisionbenchmarksandvisionlan-
guagedownstreamtasks. WefurtherexploredthescalingbehaviorofSuperClass
onmodelsize,traininglength,ordatasize,andreportedencouragingresultsand
comparisonstoCLIP.
1 Introduction
Pretrainingmethodologies[35,57,51,60]thatdirectlyharnessweb-scaleimage-textdatasethave
transformedthefieldofcomputervisioninrecentyears. Amongthem,contrastivelanguageimage
pretraining(CLIP)[57]hasgainedescaladingpopularityandbecomepredominantduetothefollow-
ingreasons. First,itservesastheindustry-standardpre-trainedmodelthatfacilitateszero-shotvisual
recognition[50,52]andfinetuningondownstreamtasks[19,17]. Second, properscalingbehav-
iors[12]areobservedsuchthatCLIPcanconsistentlybenefitfromlargermodelsandbiggerdatato
someextent. Third,itoffersstrongcross-modalabilitiesasitisinherentlydesignedtounderstandand
connectinformationacrosstextandimages. Therefore,CLIP-stylemodelsarethedefaultchoices
formostmodernVisualLanguageModels [47,2,1],whichconnectavisionbackbonewithadeep
languagemodel[69,13].
Despiteitssuccess,CLIPnecessitatesverylargebatchsizesfortraining—typicallyover64,000—to
achieveoptimalperformance,alongwithsubstantialcomputationalresourcesfortextencoding. This
highcomputationaldemandlimitsaccessibilityforresearcherswithlimitedresourcesandengineering
expertise. Inourwork,weaimtoaddresstheheavycomputationalburdenbyreplacingcontrastive
methodologywithasimplerclassificationapproach,eliminatestheneedforlargecontrastivebatch
sizes,andtextencoders.
Inthiswork,werevisittheclassificationmethodforpretrainingonlarge-scaletext-imagepairs. Some
previousworks [54,31,39,27,51]attempttotacklethisbyemployingbag-of-wordsclassification
in a weak supervised learning manner. However, most of these studies have been conducted on
asmallscale,andthereisnoevidencedemonstratingtheirscalabilityintermsofdataandmodel
size. Incontrast,ourmethoddemonstratestheperformanceofSuperClassonascalecomparable
to CLIP [57], achieving favorable model performance with 13 billion seen samples on 1 billion
uniquetext-imagepairs. Someotherconcurrentefforts[31]havealsoattemptedtoreplacecontrastive
learningwithclassification. However,theyrelyheavilyonpreprocessingthetextmodality,using
Preprint.
4202
voN
6
]VC.sc[
2v31330.1142:viXraInfoNCE text tokens
image text image
image text image
tokens tokens tokens
a) CLIP b) SuperClass
Figure1: (left)CLIPusestwoseparateTransformerencoderstoextractvectorrepresentationsfrom
image-textpairs. Thetextencoderoperatesonasubword-leveltokenizer. (right)Theproposedbag
ofsubwordsclassificationbothonlyusethesingleTransformerencoder.
bag-of-wordsandotherhand-craftedrulestoconverttextintosemi-labels. Somecommonpractices
includefiltering,wordsegmentation,lemmatization,andtheremovalofnumbersandstopwordsto
createauniquevocabularyofcleanwords. Wefoundthepreprocessingofteneliminateslong-tailed
wordsorstopwordsthatcontainvaluableinformationforrepresentationlearning(seeSec.4.4). In
contrast,SuperClasssimplyutilizesrawwordtokensassupervisionsignalswithoutrequiringany
hand-craftedpreprocessing: nofilteringorremovalofstopwords. HenceSuperClasspreservesall
informationfromtheoriginaltextdescriptionsassupervisionsignal.
WeproposedaSuper-simple-Classificationapproach(SuperClass)thatsimplytrainstoclassifyraw
texttokensandscalesasgoodasCLIP.AsshowninFigure1,similartoCLIP,SuperClassdirectly
operateontexttokenswithanymanualtextfiltering. Ourcomprehensiveempiricalstudyshowsthat
evenwithouttheneedforatextencoder,classificationmethodscanachieveperformancecomparable
tothecontrastiveapproachintermsofbothmodelcapabilitiesanddatascalability. Wedemonstrate
thatSuperClassisacompetitivealternativetoitscontrastivecounterpartonbothimageclassification
andvision&languagetasks.PretrainedonthesameDatacomp-1B[21]datasetswithanequalnumber
ofseensamples,SuperClassdominantlyoutperformsitscontrastivecounterpartsacrossvariousof
visiononlyandvision&languagescenarios. WefurtherexplorethescalingbehaviorofSuperClass
concerningmodelsizeandnumberofseensamples. Experimentssuggestthatclassification-based
methods can exhibit competitive or even superior scaling behavior compared to their contrastive
counterparts. We hope our method, experiments and analysis can encourage future potentials of
classification-basedmethodsasthefoundationalvision-languagepretrainingmethods.
2 RelatedWork
Withthegrowingavailabilityoflarge-scale,web-sourcedimage-textdatasets[57,65,6,68,21,63,
4,62],newmethodshaveemergedtoleveragethisdataassupervisionfortrainingdeeprepresenta-
tions. Theseapproachestypicallyinvolveoneofthreestrategies: usingtextasclassificationlabels,
implementingimage-textcontrastivelearning,ortreatingtextasautoregressivetargets.
Textasclassificationlabels. Theexplorationofimage-textdataformodeltraininghasdeeproots,
with early work like Image-to-Word[54] over two decades ago aiming to enhance content-based
imageretrieval. Thisstudypioneeredeffortstotrainmodelstopredictnounsandadjectivesintext
documentslinkedtoimages.Buildingontheseearlyideas,subsequentresearchhassoughttoimprove
dataefficiency[68,43],modeleffectiveness[31,27],andvocabularyexpansion[27,78,51,39]. With
the recent develop of network architecture, Tag2Text [27] and RAM [78] have employed Vision
Transformers(ViT)[18]asvisionbackbones, extractingnounsfromtheCC12Mdataset[6]and,
throughacombinationofrulesandmanualselecting,arrivedat6,449wordstouseasclassification
categories. Similarly,CatLIP[51]hasfilteredout"goldlabels"fromtheCC3M[65]andDatacomp-
1B [21] datasets based on certain rules, and then trained visual models using even larger-scale
image-textpairdatasets.
2Unlikepreviousclassificationmethodsthatrelyoncomplexrulesormanualfilteringtocurate"gold
labels"forclassificationvocabularies,ourapproacheliminatestheneedforsuchfiltering. Instead,
wedirectlyleveragetexttokensasclassificationcategories,preservingvaluabletextualinformation
thatmightotherwisebediscarded.
Image-textcontrastivelearning. Large-scalecontrastivevision-languagepretraininggainedtrac-
tionwiththeintroductionofCLIP[57]andALIGN[30]. Sincethen,numerousapproacheshave
focusedonenhancingCLIP’sperformance[76,45,44,11,7,74]. Forinstance,SigLIP[76]reduces
thecomputationalloadofCLIP’ssoftmax-basedcontrastivelossbyemployingasigmoidlossfor
localpairwisesimilaritycalculations. LiT[77]adoptspretrainedvisionandlanguagebackboneswith
contrastivetraining,whileothermethods[45,44,19]aimtoenhancetrainingefficiencyinimage-text
pretraining. InternVL[11]furtherinnovatesbyintegratingalargelanguagemodelasthetextencoder
withinCLIP.
Inourapproach,wechallengethenecessityofanadditionalbackbonetoencodetextinformationfor
contrastivelearning. Instead,wedirectlyusetexttokeninputasthesupervisorysignal,eliminating
theneedfortextencodingandavoidingthecomputationaloverheadoflargecontrastiveoperations.
Thisstreamlinedsetupachievesperformancecomparabletodual-backbonemethods.
Textasautoregressivetargets. Variousrecentstudies[15,61,38,32,41,26]havedelvedinto
employing image captioning for model pretraining. SimVLM [71] has innovated this field by
pioneeringthepretrainingofamultimodalencoder-decoderthatfusesvisionandlanguageatanearly
stage,leveragingahybridarchitectureforapplicationssuchasvisualquestionanswering(VQA).
CapPa[70]demonstratesthatasimpleencoder-decodersetupcanefficientlypretrainvisionencoders
solelythroughcaptioning. Furthermore,recentlystudies[75,42,37]combinecontrastivelearning
withcaptioningobjectives,occasionallyincorporatinganadditionaltextencoder.
Inthiswork,werevisittheclassification-basedapproachusinglarge-scalevisual-languagedatasets.
Unlike the image captioning methods mentioned earlier, our classification method integrates the
textcaptioningdecoderwithinthevisionencoder,allowingasinglevisionencodertoconnectboth
modalities. Experiments demonstrate that SuperClass achieves competitive, and often superior,
performanceacrossvariousdownstreamtasks.
3 Asimpleclassificationapproachtopretrainvisionencoders
In this section, we present our proposed approach, SuperClass, which employs a classification-
basedpretrainingmethodusingtextsupervision. Webeginbyoutliningthegeneralframeworkof
SuperClass. Next,weexplainhowtextisconvertedintocategorylabelswithouttheneedtoselect
"goldlabels",allowingalltexttosupervisethetrainingoftheimageencoder. Finally,weillustrate
ourchoiceoflossdesignamongvariousclassificationlosses. Additionally,recognizingthediffering
significanceanddiscriminativepowerofeachword,weincorporatedinversedocumentfrequencyas
classweightsinthelossdesign.
Overview. Weaimtoestablishapretrainingmethodbasedonimageclassificationthatmatches
CLIP in simplicity, scalability, and efficiency. To achieve this, we follow the standard protocol
byutilizingVisionTransformer(ViT)backbonesasvisionencoders,followedbyaglobalaverage
poolinglayerandalinearlayerastheclassificationheadtooutputthelogitvectorx. Thesupervision
targetsarederivedfromthetextassociatedwiththeimage,andtheclassificationlossiscomputed
usingthetext-derivedclassificationlabelsandthepredictedlogits.
TextsasLabels. WedirectlyusetokenizedtextasK-hotlabels,whereKisthenumberoftokens
inthegivensentences. Morespecifically,foragivenimage-textdatasetD ={(I ,T )|i∈[1,N]}
i i
withN pairsofimagesI andtextcaptionsT,wedifferfrompreviousclassification-basedmethods
bydirectlyusinganexistingsubword-leveltokenizer,suchastheoneusedinCLIPorBERT,witha
vocabularysizeV. ThistokenizerinputsthetextT andobtainthesetCofcorrespondingsubword
IDs, which serves as the classification labels. The label in the set C satisfies {c ∈ [1,V]}. The
classificationlabelsCwillbeconvertedintoK-hotvectory∈RV,wherey =1whencinthesetC,
c
otherwisey =0.
c
3Comparedtopreviousmethods,ourapproachdoesnotrequireanypreprocessingormanualthreshold
setting,makingitstraightforward. Atthesametime,italsoavoidstheout-of-vocabularyissuethat
mightbeencounteredbypreviousapproaches.
ClassificationLoss. Asignificantbodyofresearchhasfocusedonmulti-labelclassificationloss.
However,itisimportanttoemphasizethatourprimarygoalistopretrainvisionencodersratherthan
prioritizemulti-labelclassificationaccuracy. Inamulti-labelscenario,aSoftmaxlossisappliedin
SuperClassbyrepresentinglabelsinaprobabilisticmanner,whereyˆ isanormalizedweightedlabel.
c
(cid:88)V exc
ℓ =− yˆ log (1)
ce
c=1
c (cid:80) c′ex c′
Weevaluatedseveralmulti-labelclassificationlosses,includingSoftmaxloss,BCEloss,softmargin
loss, ASL [59], and two-way loss [33]. Surprisingly, the simple Softmax loss yielded the best
pretrainingresults. Thismaybeduetothefactthatexistingmulti-labelclassificationlossesassume
thatlabelsarepreciseandexhaustive,aimingtooptimizethemarginbetweenpositiveandnegative
classes. However,theinherentnoiseinimage-textdataandthelimitationsoftextinfullycapturing
animage’scontentmeanthatnotallobjectsinanimagearealwaysreferencedintheaccompanying
text.
Inverse Document Frequency. Within the subword vocabulary, not all categories contribute
semanticallyequally, asdifferentwordscarryvaryingamountsofinformation. Additionally, the
subword dictionary contains many words unrelated to visual content that frequently appear in
sentences,whichdonotprovideeffectivesupervisoryinformation. Therefore,wordswithhigher
information content should be given greater weight during training. To achieve this, we employ
InverseDocumentFrequency(IDF)asameasureofinformationsignificance. Thefewerthenumber
ofsamplescontainingaspecificword,thestrongeritsabilitytodifferentiatebetweensamples. We
usetheIDFstatisticofeachcategory(subword)astheweightforthecorrespondingclassification
label,assigningdifferentweightsw totheclassificationlabelsc.
c
(cid:18) (cid:19)
w y |D|
yˆ = c c , w =log (2)
c (cid:80) w y c 1+df(c)
c′ c′ c′
where |D| denotes the total number of image-text pairs, df(c) is the document frequency (df) of
subwordc,inotherwords,it’sthecountoftextscontainingsubwordc. Forgreatereaseofuse,we
haveimplementedanonlineIDFstatisticthatiscomputedduringthetrainingprocess,eliminatingthe
needforpre-trainingofflinestatistics. Thisapproachenhancestheuser-friendlinessandportabilityof
ourmethod.
4 Experiment
4.1 Experimentsetup
Weuseastandardsubsetofthedatacompdataset[21]forpre-training,whichcontainsabout1.3B
image-textpairs. Abatchsizeof16kand90kareadoptedforourclassificationmodelsandCLIP
models. Intheablationsection,allexperimentsareconductedwithabatchsizeof16k. Tomakea
faircomparsionwiththeCLIP,weuse90kbatchsize,adopttheAdamWwithacosineschedule,and
setthesamelearningrateanddecayasCLIP.
4.2 Evaluationprotocols
Inordertobetterhighlighttheeffectivenessofpretrainingmethod,weconcentrateontheproperties
ofthefrozenrepresentations.
Linear probing We evaluate the classification accuracy when using the full ImageNet-1k [14]
trainingsettolearnadenseprojectionlayerandfrozentheparametersofbackbone. Wefollowthe
linearprobingtrainingrecipefromMAE[24].
4Table1: ComparisonoftheLinearprobingtop-1accuracyonImageNet-1Kdataset.
ViT-Base ViT-Large
Method PreTrainingdata
#SeenSamples Top-1(%) #SeenSamples Top-1(%)
contrastiveorclusteringbased
MoCov3[10] IN1K 400M 76.7 400M 77.6
DINO[5] IN1K 512M 78.2 - -
iBOT[80] IN22K 400M 79.5 256M 81.0
DINOv2[55] LVD-142M - - 2B 84.5
reconstructionbased
BEiT[3] D250M+IN22K 1B 56.7 1B 73.5
SimMIM[73] IN1K 1B 56.7 - -
CAE[8] D250M 2B 70.4 2B 78.1
MAE[24] IN1K 2B 68.0 2B 75.8
vision-languagepretrainingbased
OpenaiCLIP[57] WIT-400M 13B 78.5 13B 82.7
Cappa[70] WebLI-1B - - 9B 83.0
OpenCLIP[29] Datacomp-1B - - 13B 83.9
SuperClass Datacomp-1B 1B 78.7 1B 82.6
SuperClass Datacomp-1B 13B 80.2 13B 85.0
Table2: Performanceoffrozenvisualrepresen- Table 3: Zero-shot Top-1 acc. and CIDEr
tations on different classification datasets. 10- are tested on ImageNet-1k dataset and COCO
shotlinearevaluationaccuracyonthepre-logit captions, respectively. The zero-shot accuracy
representation. *resultsfromthepaper. ofSuperClassisobtainedafterlock-imagetun-
ing[77].
Method ImageNet Pets Cars
Case Backbone 0-shot CIDEr
MAE 44.0±0.1 57.7±0.2 32.5±0.1
DINOv2 77.0±0.1 94.2±0.1 76.8±0.2 OpenaiCLIP ViT-L/14 75.3 113.5
CapPa* 70.6±0.2 92.6±0.5 92.2±0.2 OpenCLIP ViT-L/14 79.2 -
OpenCLIP 75.6±0.1 92.2±0.6 92.7±0.3 CLIP,reimpl. ViT-L/16 79.0 112.6
SuperClass 77.2±0.1 94.6±0.1 92.6±0.1 SuperClass ViT-L/16 79.7 113.0
10-shotclassification FollowingthesettingofCappa[70],weperform10-shotclassificationon
ImageNet-1k[14],Pets[56]andCars[34]. Foreachdatasetandmodel,werun3times,andreport
themeanresultsandvariance.
Locked-image Tuning Locked-image Tuning (LiT) [77] employs contrastive training to align
lockedimageandunlockedtextmodels. Generally,LiTisanefficientwaytoequipanypretrained
visionbackbonewithzero-shotclassificationandretrievalcapabilities. Wefollowthesetupfrom
LiT[77]andassessthezero-shotclassificationaccuracyonImageNet-1k[14].
Collaboratingwithlanguagemodels Motivatedbyrecentworks[47,11,1,67,2,71]combining
pretrainedvisionbackbones[57,19,76]andlanguagemodels[69,13],weinvestigatetheamenability
of the learned representations to interface with a text decoder. Here, we evaluate two ways to
collaboratewithlargelanguagemodels. 1)followingClipCap[53],wefrozenbothpretrainedimage
encoder and pretrained 12-layer GPT2 decoder [58], only train an adapter to connect the image
encoder and language model to perform image captioning on COCO captions [9]. 2) following
LLaVA[47]setup,wetrainandfinetuneaprojectionlayerandapretrainedlargelanguagemodels,
Vicuna-7B[13]tosolvedownstreamtasks,includingVQAv2(val)[22],GQA[28],VizWiz(val)[23],
T-VQA(val)[66],SQA(img)[79],MMBench(en)[48],MME[20],POPE[46],MMMU[25]and
SEEDBench[40].
4.3 Mainresults
Comparisonwithdifferenttypesofpretrainingmethods InTable1,wecomparethemodels
trained by SuperClass with the different types of pretraining methods, including contrastive or
clusteringbasedmethods[10,5,80,55],reconstructionbased[3,73,8,24],andvision-language
5pretrainingbasedmethods[57,70,21]. Ingeneral,theproposedmethodachievesbestperformance
amongthesepretrainingmethods.
ComparedtothecurrentSOTAself-supervisedmodelDINOv2[55],ourmethodachievesa0.5%
higher accuracy in IN-1K linear probing (85.0 vs 84.5) without a bunch of bells and whistles.
AlthoughSuperClasshasseenmoresamples,itoperatesasasimplerclassificationframeworkand
doesnotemployMultiCrop,MaskedImageModeling,orContrastivelearning,asDINOv2does.
Althoughcomparingaself-supervisedlearningmethodtoa(weakly)supervisedlearningapproach
is a system-level comparison, we still observe that a simple classification pretraining approach
demonstratessuperiorperformanceacrossmanyclassybenchmarksthatshowninTable2.
ComparedtothecontrastivecounterpartsCLIP[57],ourmethodachieveshigherlinearprobingtop-1
accuracy on ImageNet-1K dataset with ViT-Base (80.2vs 78.5) and ViT-Large(85.0 vs 82.7) as
backbone. Forafaircomparison,wefurthermakeacomparisonwithOpenCLIP[29]whichtrains
aViT-Largemodelwithabatchsize90kbasedonDatacomp-1Bdataset. Ourmethodconsistently
outperformsOpenCLIPbyalargemargin(85.0vs83.9). InTable2,ourmethodsurpassesOpenCLIP
onIN-1KandPetsbyclearmarginswithimprovementsof1.6and2.2points,whilebeingcomparable
withOpenCLIPonCars(92.6v.s92.7).
FurthercomparisonwithCLIP InTable3,wecomparethemodelstrainedbySuperClasswith
thecurrentlywidelyusedCLIPmodels,includingzero-shotclassification,andCOCOcaptioning.
Toverifytheeffectivenessofthepretrainingmethod,weadaptthestandardViT[18]structureas
thevisualbackboneandaddedaclassificationheadontopofit. Weusetheopen-sourceDatacomp-
1B[21]datasetandencounter13Bsamplesinthetrainingprocess.
For a better comparison with the CLIP models, we select three types: OpenAI’s CLIP ViT-L/14
trained on the internal WiT-400M data, and Laion’s CLIP ViT-L/14 trained on the open-source
Datacom-1Bdataset. Thecheckpointsofthesetwomodelshavebeenopen-sourced. Thecheckpint
ofLaion’sopenCLIPisdownloadedfromHugginfaceHub1. Forafaircomparison,wetrainedthe
ViT-L/16modelwithabatchsize90kbasedontheourcodebaseandDatacomp-1Bdataset.
WithLockimageTuning[77], thetrainedclassificationmodelalsogainstheabilityofzero-shot
classification. Ourmethodachieves79.7%Top-1zero-shotaccuracyonImageNet-1kdatatsetwhich
ismuchbetterthanOpenAICLIPViT-L/14andOpenCLIPViT-L/14. Althoughmaybetheyarenot
directlycomparable,thisdoreflectthatthevisionmodeltrainedbytheproposedSuperClassiswith
strongvisualperceptioncapabilities.
Combiningthefrozenvisionencoderwithafrozenpretrained12-layerGPT-2decoder[58]viaa
trainedadapter,themodelsaretrainedonCOCOcaptions[9]andCIDErsocresarereported. We
observethattheCIDErsocresofourmethodareslightlybelowOpenAI’sCLIP,whichmaybedueto
theuseofdifferentdatasets;OpenAI’sCLIPutilizesaninternaldataset,WiT-400M.However,our
approachoutperformsourimplementedCLIPmodelwiththesamesettings.
Overall,themodelstrainedbyproposedSuperClassdemonstratedmarginallyimprovedaccuracy
inbothclassificationcapabilitiesandthevision&languagetaskwhencomparedtothecontrastive
pretrainedCLIPmodels.
Largemulti-modalmodels Manylargemulti-modalmodelsintegratepre-trainedvisionbackbones
withlargelanguagemodels. Weexplorehowamenablethelearnedrepresentationsaretointerfacing
withatextdecoder.FollowingtheLLaVAsetup[47],wecombinefrozenCLIPmodelsandSuperClass
modelswiththepretrainedVicuna-7B[13]andperformdownstreamtasks.InFigure2,weshowsome
resultsofvision&languagedownstreamtasks. TheresultsdemonstratethatSuperClassmodelscould
achievebetterperformancethanCLIPmodelsonthemajorityofdatasets. Itisworthmentioning
that,incomparisontoCLIPmodels,SuperClassmodelsexhibitsignificantlybetterperformanceon
VQAv2[22],T-VQA[66],andMMBench[48],whichpertaintoOCRandfine-grainedrecognition
tasks,respectively. Inaddition,theoverallaccuracymeasurementonVizWiz[23]arenotstabledue
toasignificantportionofquestionsbeinglabeledasunanswerable. Toensurethecompletenessof
ourfindings,westillpresenttheresultsonthisdataset.
Duetospacelimitations,detailednumericalresultsareprovidedintheappendix. Additionally,you
canfindmoreexperimentalresultsintheappendix.
1Laion’sCLIPhttps://huggingface.co/laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K.
6ImageNet 0-shot ImageNet Linear Probing VQAv2 T-VQA
70 80 30
70
65 75
68 25
60 70
66 20
55 65
64
15
50 60
S B L S B L S B L S B L
FLOPs FLOPs FLOPs FLOPs
ImageNet 0-shot ImageNet Linear Probing VQAv2 T-VQA
75 35
82 72
70 70 30
65 78 68 25
60 74 66 20
55
64 15
70
50
10
128M 512M 1.28B 128M 512M 1.28B 128M 512M 1.28B 128M 512M 1.28B
Training Samples Seen Training Samples Seen Training Samples Seen Training Samples Seen
CLIP SuperClass
Figure2: Zero-shotclassificationaccuracyandlinearprobingaccuracyonImageNet-1kdataset(left
twocolumns);PerformanceofVQAv2andT-VQAwithLLaVAtrainingrecipe(righttwocolumns).
Toprow: Wecomparetheperformanceofvisionbackbones—ViT-S/16,B/16,andL/16—pretrained
via classification and contrastive methods with the same batch size of 16k and 512 million seen
samples,focusingontheirsizeandcomputationalcost. SuperClassdemonstratesbetterscalingon
zero-shotclassificationandVQAv2,T-VQAtasks. Bottomrow: ComparingSuperClassandCLIP,
performance increases with more training examples, mirroring the effects of model scaling. All
methodsaretrainedthesamebatchsizeof16kandViT-L/16asbackbone.
Modelscalingresults InthetoprowofFigure2,weshowcasetheperformanceacrossclassification
and vision & language tasks for varying model scales. For a fair comparison, both CLIP and
SuperClassmodelsundergotrainingwithidenticalsettings,whichincludeabatchsizeof16kand512
millionseensamples. AsshowninFigure2,withthemodelscalingup,weobserveacorresponding
enhancementinperformance,whetheritisforclassificationtasksorthedownstreamtasksassociated
withLLaVA.Generallyspeaking,withthesamemodelsize,modelspre-trainedusingSuperClass
exhibit superior precision compared to those trained with CLIP. SuperClass demonstrates better
scalingonzero-shotclassificationandVQAv2,T-VQAtasks.
DataScalingresults InthebottomrowofFigure2,weshowcasetheperformanceacrossclassi-
ficationandvision&languagetasksforvaryingseensamples. Forafaircomparison,bothCLIP
andSuperClassmodelsundergotrainingwithidenticalsettings,whichincludeabatchsizeof16k
andViT-L/16asbackbone. Figure2illustratesthatasthenumberofseensamplesgrows,thereis
anoticeableimprovementinperformanceforbothclassificationanddownstreamtaskslinkedto
LLaVA.Typically,modelspre-trainedwithSuperClassoutperformthosetrainedwithCLIPinterms
ofaccuracywhengiventhesameamountofseensamples. SuperClassexhibitsthesameorslightly
betterscalingbehaviorcomparedtoCLIPondownstreamtasks. Inaddition,SuperClassdoesnot
requireatextencoder,itoffersbetterefficiencyintrainingcomparedtoCLIP.
4.4 Ablations
ToverifytherationalityoftheSuperClass,weconductextensiveablationexperimentsthatpretrainon
datacomp-1B[21]andevaluateonseveraldownstreamtaskswithdifferentsettingsforSuperClass.
Word-level tokenizer vs. Subword-level tokenizer Table 4 presents the results of word-level
tokenizerandsubword-leveltokenizeronservaldaownstreamtasks. Weusethetokeinzerinopenai
CLIPasoursubword-leveltokenizer.Wecompareitwiththeword-leveltokenizerusedinCatLIP[51],
whichcarefullyselectedapproximately40,000"goldlabels"fromthedatacomp-1Bdataset. Aside
fromthetokenizerbeingdifferent,allmodelsaretrainedunderthesamesettings.
ForViT-S/16,word-leveltokenizerachievesbetterclassificationaccuracythansubword-leveltok-
enizer. Apossiblereasonisthatwhenthemodelcapacityislimited,thefilteredcleansupervisory
7Table 4: Word tokenizer vs. Subword tokenizer. The performance of classification and LLaVA
downstreamtaskswithdifferenttokenizers. SuperClassuseasubword-leveltokenizertomaptext
intocategorylabels. Allmodelsaretrainedinthesamesettingswithabatchsize16kand512Mseen
samples.
Classification Vision&LanguageDownstreamTasks
Tokenizer ViT LP ZS
Word S/16 68.4 53.2 65.28 55.38 43.12 15.84 65.83 49.14 1228 80.32 36.6 50.54
Subword S/16 67.9 52.8 65.54 55.95 46.23 16.46 65.64 48.53 1306 81.02 33.2 50.43
Word B/16 76.1 61.4 67.72 57.12 46.20 20.98 65.79 54.63 1296 81.88 34.4 53.38
Subword B/16 76.0 61.7 68.34 57.43 41.79 24.41 65.54 54.03 1324 82.28 36.9 52.88
Word L/16 80.3 68.2 69.47 57.36 51.94 23.87 65.00 56.27 1335 83.28 35.4 53.64
Subword L/16 80.5 69.0 70.40 58.16 51.48 29.83 67.72 59.45 1373 84.04 36.3 53.74
Table5:TheperformanceofclassificationandLLaVAdownstreamtaskswithdifferentsubword-level
tokenizers. Allmodelsaretrainedinthesamesettingswithabatchsize16k,512Mseensamplesand
ViT-L/16asBackbone.
Classification Vision&LanguageDownstreamTasks
Tokenizer Vocab LP ZS
OpenaiCLIP 49,152 80.5 69.0 70.40 58.16 51.48 29.83 67.72 59.45 1373 84.04 36.3 53.74
WordPiece 32,000 80.5 68.5 69.95 57.76 49.07 29.33 65.99 56.18 1375 83.37 35.1 54.05
SentencePiece32,000 80.2 67.8 69.52 57.95 49.20 28.56 65.05 57.47 1301 82.16 34.8 53.52
informationmaybemoreconducivetomodelconvergence. However,withtheincreasingsizeof
the model, subword-level tokenizer gradually outperforms the word-level tokenizer, whether in
classificationtasksorvision&languagetasks.
Regardlessofmodelsize,onmostvision&languagetasks,subword-leveltokenizertendstoperform
betterthanword-leveltokenizer. Thereasonmaybethatsubword-leveltokenizerretainasubstantial
amountoflanguage-relatedinformation,althoughitmaynotbehighlyrelevanttovisualinformation.
Thismakesthefeaturesofthemodelstrainedwiththesubword-leveltokenizermorereadilyintegrated
withlargelanguagemodels.
Overall,usingasubword-leveltokenizerexhibitsbetterscalingbehaviorandismoresuitableforuse
inlargemulti-modalmodels.
Differentsubword-leveltokenizers Table5presentstheresultsonclassificationtasksandLLaVA
downstreamtaskswithdifferentsubword-leveltokenizers. Here,wecomparethecharacter-based
bytepairencodingtokenizer[64]usedinCLIP[57],theWordPiece[72]tokenizerusedinBERT[16]
and the SentencePiece [36] tokenizer used in LLama [69], they are all subword-level tokenizers.
ThetokenizerusedinopenaiCLIPobtainsbestperformanceontheclassificationtaskandLLaVA
downstreamtasks. Finally,wechoosethetokenizerusedinCLIP[57]forthetrainingofSuperClass
models.
Classification loss Table 6 represents different classification loss on ImageNet-1k dataset. We
selectedseveralofthemostcommonlyusedmulti-labelclassificationlossesforexperimentation.
Softmaxlossisoftenusedinsingle-labelclassificationtasks. Itispossibleapplyasoftmaxloss
in a multi-label scenario through describing labels in a probabilistic way. BCE loss is a binary
cross-entropy(BCE)lossandisoftenusedinmulti-labelclassificationtasksasbaseline. Asymmetric
Loss(ASLloss)aimprovedBCElosstoaddresspositive-negativeimbalance. Softmarginlossisa
8
2vAQV
2vAQV
AQG
AQG
ziWziV
ziWziV
AQV-T
AQV-T
AQS
AQS
hcneBMM
hcneBMM
EMM
EMM
EPOP
EPOP
UMMM
UMMM
DEES
DEESTable6: Theperformanceonclassificationtaskswithdifferentclassificationlosses. Allmodelsare
trainedinthesamesettingswithabatchsize16k,512MseensamplesandViT-B/16asBackbone.
Loss&Acc. Softmax BCE ASL SoftMargin Two-way
Linearprob 75.6 73.6 73.8 73.5 74.8
Zero-shot 60.8 58.5 58.7 58.1 59.7
Table7: TheeffectofIDFweightinthelossandremovingstopwords.
Classification Vision&LanguageDownstreamTasks
Tokenizer LP ZS
SuperClass 76.0 61.7 68.34 57.43 41.79 24.41 65.54 54.03 1324 82.28 36.9 52.88
w/oIDF 75.6 60.8 68.08 57.27 47.60 23.73 65.44 54.55 1310 82.58 34.6 52.53
rmStopwords 75.7 61.0 68.29 57.41 47.67 24.12 65.34 53.86 1343 82.50 33.8 53.12
margin-basedlossformulti-labelclassificationtasks. Two-waylossisthecurrentstate-of-the-art
(SOTA)formulti-labelclassificationtasks.
Surprisingly,thesimplestsoftmaxlossoutperformsallothermulti-labelclassificationlossesbya
largemargin. Webelievethatexistingmulti-labelclassificationlossesoperateundertheassumption
thatlabelsarebothaccurateandcomplete, aimingtooptimizetheclassificationmarginbetween
positiveandnegativeclasses. However,inreality,image-textdatacontainsconsiderablenoise,and
asingletextpassagecannotpossiblycaptureallthecontentsofanimage. Consequently, certain
categoricalobjectspresentintheimagemaynotbementionedintheassociatedtext. Inthecontextof
image-textpretraining,howtodesignabetterlossfunctionremainsaquestionworthyofexploration.
IDFasclassweights Consideringthattheimportanceofeachcategory(subword)inthevocabulary
isnotequalandtheinformationtheycarryvaries,weuseIDFasclassweights. TheTable7repsents
theresultsofwithandwithoutIDFasclassweights.SuperClasswithoutIDFexperiencedanoticeable
decreaseinaccuracyonclassificationtasks,thechangeinprecisiononLLaVAtasksisnotsignificant.
Removing stopwords? Stop words are commonly used in Text Mining and Natural Language
Processing (NLP) to eliminate words that are so widely used that they carry very little useful
information. Inthepreviousclassificationmethods,thestopwordsareremoved. Thestopwordsare
downloadfromNLTK[49]. However,theresultsinTable7showsthatthekeepingstopwordscould
helpthevisionencodertogainbetterperformanceonclassificationtasks.
5 LimitationandConclusion
We have conducted a thorough comparison of vision encoders pre-trained with contrastive and
classificationobjectivesanddeterminedthatmodelspre-trainedwithclassificationsurpassCLIP
models in both classification and vision & language tasks. Additionally, our approach does not
requireatextencoder,whichleadstohighertrainingefficiencythanthatofCLIP.Furthermore,our
findingssuggestthatclassificationasapre-trainingtaskmayhavebeneficialscalingpropertiesas
modelanddatasizesincrease,andweencouragefutureresearchtodelveintothispossibility.
Whileitdeliversimpressiveresultsonvariousdownstreamtasks,itcompletelyignorewordorderand
objectrelationships,whichimpliesthatwearelosingimportantsupervisoryinformation. Addressing
thiswillbethedirectionofourfutureresearchefforts.
Tosumup,wehavedemonstratedthatstraightforwardimageclassificationcanserveasaneffective
pre-training strategy for vision backbones derived from image-text data. Our aim is to stimulate
subsequentstudiestopaymoreattentiontothebenefitsofclassificationasapre-trainingtaskfor
visionencoders.
9
2vAQV
AQG
ziWziV AQV-T
AQS
hcneBMM
EMM EPOP
UMMM
DEESReferences
[1] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,KarelLenc,
ArthurMensch,KatieMillican,MalcolmReynolds,RomanRing,ElizaRutherford,SerkanCabi,Tengda
Han,ZhitaoGong,SinaSamangooei,MarianneMonteiro,JacobMenick,SebastianBorgeaud,Andrew
Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,
AndrewZisserman,andKarenSimonyan. Flamingo:Avisuallanguagemodelforfew-shotlearning. In
NeurIPS,2022.
[2] JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,XiaodongDeng,YangFan,WenbinGe,YuHan,
FeiHuang,etal. Qwentechnicalreport. arXivpreprintarXiv:2309.16609,2023.
[3] HangboBao,LiDong,SonghaoPiao,andFuruWei. Beit:Bertpre-trainingofimagetransformers. arXiv
preprintarXiv:2106.08254,2021.
[4] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim.
Coyo-700m:Image-textpairdataset. https://github.com/kakaobrain/coyo-dataset,2022.
[5] MathildeCaron,HugoTouvron,IshanMisra,HervéJégou,JulienMairal,PiotrBojanowski,andArmand
Joulin. Emergingpropertiesinself-supervisedvisiontransformers. InProceedingsoftheInternational
ConferenceonComputerVision(ICCV),2021.
[6] SoravitChangpinyo,PiyushSharma,NanDing,andRaduSoricut. Conceptual12M:Pushingweb-scale
image-textpre-trainingtorecognizelong-tailvisualconcepts. InCVPR,2021.
[7] JiennegChen,QihangYu,XiaohuiShen,AlanYuille,andLiang-ChiehChen. Vitamin:Designingscalable
visionmodelsinthevision-languageera. arXivpreprintarXiv:2404.02132,2024.
[8] XiaokangChen,MingyuDing,XiaodiWang,YingXin,ShentongMo,YunhaoWang,ShuminHan,Ping
Luo,GangZeng,andJingdongWang. Contextautoencoderforself-supervisedrepresentationlearning.
InternationalJournalofComputerVision,132(1):208–223,2024.
[9] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and
C.LawrenceZitnick. MicrosoftCOCOCaptions:Datacollectionandevaluationserver. arXivpreprint
arXiv:1504.00325,2015.
[10] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision
transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages
9640–9649,2021.
[11] ZheChen,JiannanWu,WenhaiWang,WeijieSu,GuoChen,SenXing,MuyanZhong,QinglongZhang,
XizhouZhu,LeweiLu,BinLi,PingLuo,TongLu,YuQiao,andJifengDai. Internvl:Scalingupvision
foundationmodelsandaligningforgenericvisual-linguistictasks. arXivpreprintarXiv:2312.14238,2023.
[12] MehdiCherti,RomainBeaumont,RossWightman,MitchellWortsman,GabrielIlharco,CadeGordon,
Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive
language-imagelearning. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages2818–2829,2023.
[13] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,Siyuan
Zhuang,YonghaoZhuang,JosephEGonzalez,etal. Vicuna:Anopen-sourcechatbotimpressinggpt-4
with90%*chatgptquality,march2023. URLhttps://lmsys.org/blog/2023-03-30-vicuna,3(5),2023.
[14] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.Imagenet:Alarge-scalehierarchical
imagedatabase. In2009IEEEconferenceoncomputervisionandpatternrecognition,pages248–255.
Ieee,2009.
[15] KaranDesaiandJustinJohnson. VirTex: Learningvisualrepresentationsfromtextualannotations. In
CVPR,2021.
[16] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert:Pre-trainingofdeepbidirec-
tionaltransformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,2018.
[17] XiaoyiDong,JianminBao,TingZhang,DongdongChen,ShuyangGu,WeimingZhang,LuYuan,Dong
Chen,FangWen,andNenghaiYu. Clipitselfisastrongfine-tuner:Achieving85.7%and88.0%top-1
accuracywithvit-bandvit-lonimagenet. arXivpreprintarXiv:2212.06138,2022.
[18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,etal.Animageisworth
16x16words:Transformersforimagerecognitionatscale. arXivpreprintarXiv:2010.11929,2020.
[19] YuxinFang,WenWang,BinhuiXie,QuanSun,LedellWu,XinggangWang,TiejunHuang,XinlongWang,
andYueCao. Eva:Exploringthelimitsofmaskedvisualrepresentationlearningatscale. InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages19358–19369,2023.
10[20] ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,ZhenyuQiu,WeiLin,
JinruiYang,XiawuZheng,etal. Mme: Acomprehensiveevaluationbenchmarkformultimodallarge
languagemodels. arXivpreprintarXiv:2306.13394,2023.
[21] SamirYitzhakGadre,GabrielIlharco,AlexFang,JonathanHayase,GeorgiosSmyrnis,ThaoNguyen,
RyanMarten,MitchellWortsman,DhrubaGhosh,JieyuZhang,etal. Datacomp: Insearchofthenext
generationofmultimodaldatasets. arXivpreprintarXiv:2304.14108,2023.
[22] YashGoyal,TejasKhot,DouglasSummers-Stay,DhruvBatra,andDeviParikh. MakingtheVinVQA
matter: Elevating the role of image understanding in Visual Question Answering. In Conference on
ComputerVisionandPatternRecognition(CVPR),2017.
[23] DannaGurari,QingLi,AbigaleJStangl,AnhongGuo,ChiLin,KristenGrauman,JieboLuo,andJeffreyP
Bigham. Vizwizgrandchallenge:Answeringvisualquestionsfromblindpeople. InProceedingsofthe
IEEEconferenceoncomputervisionandpatternrecognition,pages3608–3617,2018.
[24] KaimingHe,XinleiChen,SainingXie,YanghaoLi,PiotrDollár,andRossGirshick.Maskedautoencoders
arescalablevisionlearners. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
recognition,pages16000–16009,2022.
[25] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob
Steinhardt. Measuringmassivemultitasklanguageunderstanding. arXivpreprintarXiv:2009.03300,2020.
[26] XiaoweiHu, ZheGan, JianfengWang, ZhengyuanYang, ZichengLiu, YumaoLu, andLijuanWang.
Scalingupvision-languagepre-trainingforimagecaptioning. InCVPR,pages17980–17989,2022.
[27] Xinyu Huang, Youcai Zhang, Jinyu Ma, Weiwei Tian, Rui Feng, Yuejie Zhang, Yaqian Li, Yandong
Guo, and Lei Zhang. Tag2text: Guiding vision-language model via image tagging. arXiv preprint
arXiv:2303.05657,2023.
[28] DrewAHudsonandChristopherDManning. Gqa: Anewdatasetforreal-worldvisualreasoningand
compositionalquestionanswering. InProceedingsoftheIEEE/CVFconferenceoncomputervisionand
patternrecognition,pages6700–6709,2019.
[29] GabrielIlharco,MitchellWortsman,RossWightman,CadeGordon,NicholasCarlini,RohanTaori,Achal
Dave,VaishaalShankar,HongseokNamkoong,JohnMiller,HannanehHajishirzi,AliFarhadi,andLudwig
Schmidt. Openclip. Ingithub.com.Zenodo,July2021.
[30] ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh,HieuPham,QuocV.Le,Yun-HsuanSung,
ZhenLi,andTomDuerig. Scalingupvisualandvision-languagerepresentationlearningwithnoisytext
supervision. InICML,2021.
[31] ArmandJoulin,LaurensVanDerMaaten,AllanJabri,andNicolasVasilache.Learningvisualfeaturesfrom
largeweaklysuperviseddata. InComputerVision–ECCV2016:14thEuropeanConference,Amsterdam,
TheNetherlands,October11–14,2016,Proceedings,PartVII14,pages67–84.Springer,2016.
[32] GeewookKim,TeakgyuHong,MoonbinYim,JeongYeonNam,JinyoungPark,JinyeongYim,Won-
seokHwang,SangdooYun,DongyoonHan,andSeunghyunPark. OCR-Freedocumentunderstanding
transformer. InECCV,pages498–517,2022.
[33] TakumiKobayashi. Two-waymulti-labelloss. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pages7476–7485,2023.
[34] JonathanKrause,MichaelStark,JiaDeng,andLiFei-Fei. 3dobjectrepresentationsforfine-grained
categorization. InProceedingsoftheIEEEinternationalconferenceoncomputervisionworkshops,pages
554–561,2013.
[35] AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton. Imagenetclassificationwithdeepconvolutional
neuralnetworks. Advancesinneuralinformationprocessingsystems,25,2012.
[36] TakuKudoandJohnRichardson. Sentencepiece:Asimpleandlanguageindependentsubwordtokenizer
anddetokenizerforneuraltextprocessing. arXivpreprintarXiv:1808.06226,2018.
[37] WeichengKuo,AJPiergiovanni,DahunKim,XiyangLuo,BenCaine,WeiLi,AbhijitOgale,Luowei
Zhou,AndrewDai,ZhifengChen,etal. Mammut:Asimplearchitectureforjointlearningformultimodal
tasks. arXiv:2303.16839,2023.
[38] KentonLee,MandarJoshi,IuliaTurc,HexiangHu,FangyuLiu,JulianEisenschlos,UrvashiKhandelwal,
PeterShaw,Ming-WeiChang,andKristinaToutanova. Pix2struct:Screenshotparsingaspretrainingfor
visuallanguageunderstanding. arXiv:2210.03347,2022.
[39] AngLi,AllanJabri,ArmandJoulin,andLaurensVanDerMaaten. Learningvisualn-gramsfromweb
data. InICCV,pages4183–4192,2017.
[40] BohaoLi,RuiWang,GuangzhiWang,YuyingGe,YixiaoGe,andYingShan. Seed-bench:Benchmarking
multimodalllmswithgenerativecomprehension. arXivpreprintarXiv:2307.16125,2023.
11[41] GangLiandYangLi. Spotlight:MobileUIunderstandingusingvision-languagemodelswithafocus. In
ICLR,2023.
[42] JunnanLi,DongxuLi,CaimingXiong,andStevenHoi. BLIP:Bootstrappinglanguage-imagepre-training
forunifiedvision-languageunderstandingandgeneration. InICML,pages12888–12900,2022.
[43] WenLi,LiminWang,WeiLi,EirikurAgustsson,andLucVanGool. Webvisiondatabase:Visuallearning
andunderstandingfromwebdata. arXivpreprintarXiv:1708.02862,2017.
[44] XianhangLi,ZeyuWang,andCihangXie. Aninversescalinglawforcliptraining. AdvancesinNeural
InformationProcessingSystems,36,2024.
[45] YanghaoLi,HaoqiFan,RonghangHu,ChristophFeichtenhofer,andKaimingHe.Scalinglanguage-image
pre-trainingviamasking. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages23390–23400,2023.
[46] YifanLi,YifanDu,KunZhou,JinpengWang,WayneXinZhao,andJirongWen. Evaluatingobject
hallucinationinlargevision-languagemodels. ArXiv,abs/2305.10355,2023.
[47] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning,2023.
[48] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,SongyangZhang,WangboZhao,YikeYuan,Jiaqi
Wang,ConghuiHe,ZiweiLiu,etal. Mmbench:Isyourmulti-modalmodelanall-aroundplayer? arXiv
preprintarXiv:2307.06281,2023.
[49] EdwardLoperandStevenBird. Nltk:Thenaturallanguagetoolkit. arXivpreprintcs/0205028,2002.
[50] TimoLüddeckeandAlexanderEcker. Imagesegmentationusingtextandimageprompts. InProceedings
oftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages7086–7096,2022.
[51] SachinMehta,MaxwellHorton,FartashFaghri,MohammadHosseinSekhavat,MahyarNajibi,Mehrdad
Farajtabar,OncelTuzel,andMohammadRastegari. Catlip:Clip-levelvisualrecognitionaccuracywith2.7
xfasterpre-trainingonweb-scaleimage-textdata. arXivpreprintarXiv:2404.15653,2024.
[52] MMinderer,AGritsenko,AStone,MNeumann,DWeissenborn,ADosovitskiy,AMahendran,AArnab,
MDehghani,ZShen,etal. Simpleopen-vocabularyobjectdetectionwithvisiontransformers.arxiv2022.
arXivpreprintarXiv:2205.06230,2,2022.
[53] RonMokady,AmirHertz,andAmitHBermano.Clipcap:Clipprefixforimagecaptioning.arXivpreprint
arXiv:2111.09734,2021.
[54] YasuhideMori,HironobuTakahashi,andRyuichiOka. Image-to-wordtransformationbasedondividing
andvectorquantizingimageswithwords.InFirstinternationalworkshoponmultimediaintelligentstorage
andretrievalmanagement,volume2.Citeseer,1999.
[55] MaximeOquab,TimothéeDarcet,ThéoMoutakanni,HuyVo,MarcSzafraniec,VasilKhalidov,Pierre
Fernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal. Dinov2:Learningrobustvisual
featureswithoutsupervision. arXivpreprintarXiv:2304.07193,2023.
[56] OmkarMParkhi,AndreaVedaldi,AndrewZisserman,andCVJawahar. Catsanddogs. In2012IEEE
conferenceoncomputervisionandpatternrecognition,pages3498–3505.IEEE,2012.
[57] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,GretchenKrueger,andIlyaSutskever. Learning
transferablevisualmodelsfromnaturallanguagesupervision. InICML,2021.
[58] AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal. Language
modelsareunsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
[59] TalRidnik,EmanuelBen-Baruch,NadavZamir,AsafNoy,ItamarFriedman,MatanProtter,andLihiZelnik-
Manor. Asymmetriclossformulti-labelclassification. InProceedingsoftheIEEE/CVFinternational
conferenceoncomputervision,pages82–91,2021.
[60] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-resolution
imagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,pages10684–10695,2022.
[61] MertBulentSariyildiz, JulienPerez, andDianeLarlus. Learningvisualrepresentationswithcaption
annotations. InECCV,pages153–170,2020.
[62] ChristophSchuhmann,RomainBeaumont,RichardVencu,CadeGordon,RossWightman,MehdiCherti,
TheoCoombes,AarushKatta,ClaytonMullis,MitchellWortsman,etal. Laion-5b:Anopenlarge-scale
dataset for training next generation image-text models. Advances in Neural Information Processing
Systems,35:25278–25294,2022.
[63] ChristophSchuhmann,RichardVencu,RomainBeaumont,RobertKaczmarczyk,ClaytonMullis,Aarush
Katta,TheoCoombes,JeniaJitsev,andAranKomatsuzaki. Laion-400m:Opendatasetofclip-filtered400
millionimage-textpairs. arXivpreprintarXiv:2111.02114,2021.
12[64] Rico Sennrich, BarryHaddow, andAlexandra Birch. Neuralmachine translationof rarewords with
subwordunits. arXivpreprintarXiv:1508.07909,2015.
[65] PiyushSharma, NanDing, SebastianGoodman, andRaduSoricut. Conceptualcaptions: Acleaned,
hypernymed,imagealt-textdatasetforautomaticimagecaptioning. InACL,2018.
[66] AmanpreetSingh,VivekNatarajan,MeetShah,YuJiang,XinleiChen,DhruvBatra,DeviParikh,and
MarcusRohrbach. Towardsvqamodelsthatcanread. InProceedingsoftheIEEE/CVFconferenceon
computervisionandpatternrecognition,pages8317–8326,2019.
[67] QuanSun,QiyingYu,YufengCui,FanZhang,XiaosongZhang,YuezeWang,HongchengGao,Jingjing
Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. arXiv preprint
arXiv:2307.05222,2023.
[68] BartThomee,DavidA.Shamma,GeraldFriedland,BenjaminElizalde,KarlNi,DouglasPoland,Damian
Borth,andLi-JiaLi. YFCC100M:thenewdatainmultimediaresearch. Commun.ACM,59(2):64–73,
2016.
[69] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov, SoumyaBatra, PrajjwalBhargava, ShrutiBhosale, etal. Llama2: Openfoundationand
fine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
[70] MichaelTschannen,ManojKumar,AndreasSteiner,XiaohuaZhai,NeilHoulsby,andLucasBeyer. Image
captionersarescalablevisionlearnerstoo. AdvancesinNeuralInformationProcessingSystems,36,2024.
[71] ZiruiWang,JiahuiYu,AdamsWeiYu,ZihangDai,YuliaTsvetkov,andYuanCao. SimVLM:Simple
visuallanguagemodelpretrainingwithweaksupervision. InICLR,2022.
[72] YonghuiWu,MikeSchuster,ZhifengChen,QuocVLe,MohammadNorouzi,WolfgangMacherey,Maxim
Krikun,YuanCao,QinGao,KlausMacherey,etal. Google’sneuralmachinetranslationsystem:Bridging
thegapbetweenhumanandmachinetranslation. arXivpreprintarXiv:1609.08144,2016.
[73] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu.
Simmim:Asimpleframeworkformaskedimagemodeling. InProceedingsoftheIEEE/CVFconference
oncomputervisionandpatternrecognition,pages9653–9663,2022.
[74] JiaruiXu,ShaliniDeMello,SifeiLiu,WonminByeon,ThomasBreuel,JanKautz,andXiaolongWang.
Groupvit: Semantic segmentation emerges from text supervision. In Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages18134–18144,2022.
[75] JiahuiYu,ZiruiWang,VijayVasudevan,LeggYeung,MojtabaSeyedhosseini,andYonghuiWu. Coca:
Contrastivecaptionersareimage-textfoundationmodels. Trans.MachineLearningResearch,2022.
[76] XiaohuaZhai,BasilMustafa,AlexanderKolesnikov,andLucasBeyer. Sigmoidlossforlanguageimage
pre-training. arXiv:2303.15343,2023.
[77] XiaohuaZhai,XiaoWang,BasilMustafa,AndreasSteiner,DanielKeysers,AlexanderKolesnikov,and
LucasBeyer. Lit: Zero-shottransferwithlocked-imagetexttuning. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages18123–18133,2022.
[78] YoucaiZhang,XinyuHuang,JinyuMa,ZhaoyangLi,ZhaochuanLuo,YanchunXie,YuzhuoQin,Tong
Luo,YaqianLi,ShilongLiu,etal. Recognizeanything:Astrongimagetaggingmodel. arXivpreprint
arXiv:2306.03514,2023.
[79] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal
chain-of-thoughtreasoninginlanguagemodels. arXivpreprintarXiv:2302.00923,2023.
[80] JinghaoZhou,ChenWei,HuiyuWang,WeiShen,CihangXie,AlanYuille,andTaoKong. ibot:Image
bertpre-trainingwithonlinetokenizer. InternationalConferenceonLearningRepresentations(ICLR),
2022.
13Table8: TheperformanceofclassificationandLLaVAdownstreamtaskswithdifferentseensamples.
"LP"meanslinearprobingand"ZS"meanszero-shotclassification,thesetwoaretestedonImageNet-
1Kdataset. Theresultsofvision&languagedownstreamtasksareobtainedbycombinethefrozen
visionmodelsandVicuna-7B[13],followingthesettingsinLLaVA[47].
Classification Vision&LanguageDownstreamTasks
Method Data LP ZS
CLIP 128M 69.4 48.8 63.20 53.98 45.80 13.27 64.70 46.56 1216 78.20 35.1 47.84
SuperClass 128M 71.1 51.2 64.07 54.96 49.21 13.33 65.44 49.14 1241 80.01 35.3 49.08
CLIP 512M 79.7 65.9 68.13 57.32 44.92 22.21 64.45 51.54 1299 82.48 35.3 53.06
SuperClass 512M 80.5 69.0 70.40 58.16 51.48 29.83 67.72 59.45 1373 84.04 36.3 53.74
CLIP 1.28B 81.9 71.4 70.33 58.95 46.71 27.97 64.65 55.49 1351 83.37 35.7 55.09
SuperClass 1.28B 82.6 73.6 71.85 59.09 51.70 34.37 65.94 59.70 1392 84.41 36.8 55.51
Table9: TheperformanceofclassificationandLLaVAdownstreamtaskswithdifferentmodelsizes.
Classification Vision&LanguageDownstreamTasks
Method ViT LP ZS
CLIP S/16 63.6 52.0 64.48 55.26 44.16 15.98 65.84 47.33 1227 80.49 35.8 49.72
SuperClass S/16 67.9 52.8 65.54 55.95 46.23 16.46 65.64 48.53 1306 81.02 33.2 50.43
CLIP B/16 75.5 59.8 66.11 56.28 49.17 19.10 64.30 48.62 1289 81.47 35.9 50.76
SuperClass B/16 76.0 61.7 68.34 57.43 41.79 24.41 65.54 54.03 1324 82.28 36.9 52.88
CLIP L/16 79.7 65.9 68.13 57.32 44.92 22.21 64.45 51.54 1299 82.48 35.3 53.06
SuperClass L/16 80.5 69.0 70.40 58.16 51.48 29.83 67.72 59.45 1373 84.04 36.3 53.74
A Appendix/supplementalmaterial
BroaderImpacts
Thisworkpresentsanewapproachtotrainvisionmodels,whichcanbeappliedforimagerecognition
and other vision tasks. The approach demonstrates higher efficiency than the popular one in the
community,whichcanreducethecomputationalcostandthepowercostforcomputervisionmodel
training.
DataScalingresults InTable8,weshowcasetheperformanceacrossclassificationandvision&
languagetasksforvaryingseensamples. Forafaircomparison,bothCLIPandSuperClassmodels
undergotrainingwithidenticalsettings,whichincludeabatchsizeof16kandViT-L/16asbackbone.
Figure2illustratesthatasthenumberofseensamplesgrows, thereisanoticeableimprovement
inperformanceforbothclassificationanddownstreamtaskslinkedtoLLaVA.Typically,models
pre-trainedwithSuperClassoutperformthosetrainedwithCLIPintermsofaccuracywhengiven
thesameamountofseensamples. SuperClassexhibitsthesameorslightlybetterscalingbehavior
comparedtoCLIPondownstreamtasks. Inaddition,SuperClassdoesnotrequireatextencoder,it
offersbetterefficiencyintrainingcomparedtoCLIP.
Modelscalingresults InTable9,weshowcasetheperformanceacrossclassificationandvision&
languagetasksforvaryingmodelscales. Forafaircomparison,bothCLIPandSuperClassmodels
undergo training with identical settings, which include a batch size of 16k and 512 million seen
samples.
14
2vAQV
2vAQV
AQG
AQG
ziWziV
ziWziV
AQV-T
AQV-T
AQS
AQS
hcneBMM
hcneBMM
EMM
EMM
EPOP
EPOP
UMMM
UMMM
DEES
DEESTable10: Performanceoffrozenvisualrepresentationstrainedviaimageclassification(SuperClass)
andconstrastively(CLIP).Linearprobingandzero-shotclassificationarebothtestedonImageNet-1k
dataset. CaptioningisconductedonCOCOcaptionsandCIDErisreportedinthetable. Thezero-shot
accuracyofSuperClassisobtainedafterlock-imagetuning[77].
Method Backbone Data SeenSamples Zero-shot LinearProbing
CLIP RN-50 Datacomp-1B 1.28B 60.73 70.28
SuperClass RN-50 Datacomp-1B 1.28B 62.81 71.92
CLIP ConvNext-tiny Datacomp-1B 1.28B 59.94 70.35
SuperClass ConvNext-tiny Datacomp-1B 1.28B 62.85 72.33
Table11: Theperformanceofvision&languagedownstreamtaskswithdifferentpretrainedmodels.
Method VQAv2 GQA VizWiz T-VQA SciQA MME MMB PoPE MMMU
OpenCLIP 74.54 61.03 50.47 38.16 67.33 1434/269 60.73 85.52 35.9
MAE 63.50 54.58 50.22 11.55 54.75 1175/343 42.44 80.69 35.7
DINOv2 73.32 61.87 49.15 14.08 64.90 1336/297 57.90 86.24 35.3
SuperClass 75.24 60.96 54.33 39.20 66.09 1371/322 63.14 85.69 36.0
As shown in Figure 2, with the model scaling up, we observe a corresponding enhancement in
performance,whetheritisforclassificationtasksorthedownstreamtasksassociatedwithLLaVA.
Generallyspeaking,withthesamemodelsize,modelspre-trainedusingSuperClassexhibitsuperior
precisioncomparedtothosetrainedwithCLIP.SuperClassdemonstratesbetterscalingonzero-shot
classificationandVQAv2,T-VQAtasks.
Superclasswithdifferentmodeltypes Toevaluatetherobustnessofourproposedmethodacross
differentmodeltypes,weselectedtworepresentativeconvolution-basednetworks: ResNet50and
ConvNext-Tiny. We compare SuperClass against CLIP for ImageNet zero-shot (LiT) and linear
probingclassification,asshowninTable10. Allexperimentswereconductedwithabatchsizeof
16kand1.28Bseensamples. WeobservethatSuperClasssurpassesCLIPinallsettingsbyaclear
margin,rangingfrom1.64to2.91. TheseresultsdemonstratethatthesuperiorityofSuperClassover
CLIPisrobustacrossdifferentmodelarchitectures.
VLMdownstreamtaskswithdifferenttypesofpretrainingmodels FollowingtheLLaVAsetup,
wecombinefrozenCLIPmodels,self-supervisedmodels,andSuperClassmodelswiththepre-trained
Vicuna-7Bandperformdownstreamtasks. TheexperimentalresultsinTable11demonstratethatthe
proposedmethodcouldachievebetterthanself-supervisedViTpre-trainingmethods,likeDINOv2,
andweakly-supervisedmethods,likeCLIP.
Comparisonwithotherclassificationbasedpretrainingmodels Wehaveincludedthecompari-
sonwithotherclassification-basedmethods,likeCatLIP[51]inthesubsectionWord-leveltokenizer
vs. Subword-level tokenizer. The word-level tokenizer is used in CatLIP [51], which carefully
selectedapproximately40,000"goldlabels"fromthedatacomp-1Bdataset. Asidefromthetokenizer
beingdifferent,allmodelsaretrainedunderthesamesettings. TheresultsofTable4showthatwith
theincreasingsizeofthemodel,thesubword-leveltokenizergraduallyoutperformstheword-level
tokenizer,whetherinclassificationtasksorvision&languagetasks. Wealsoprovidetheresults
offinetuningonImageNet-1kinTable12. UsingthesamedatasetDatacom-1Bfortraining, the
samebackboneViT-L/16asbackbone,thesamenumber13Billionoftrainingsamplesseen,the
SuperClasscouldachievebetterperformancethanCatLIP(87.8vs86.5).
Table12: ComparisonoftheFine-tuningtop-1accuracyonImageNet-1Kdataset. *numberfromthe
paper.
Method PretrainingData ImageNet-1kFine-tuning
OpenCLIPViT-L/14 Datacomp-1B 87.4
CatLIPViT-L/16* Datacomp-1B 86.5
SuperclassViT-L/16 Datacomp-1B 87.8
15