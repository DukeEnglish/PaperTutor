SPACE: 3D Spatial Co-operation and Exploration Framework for
Robust Mapping and Coverage with Multi-Robot Systems
Sai Krishna Ghanta Ramviyas Parasuraman
Abstract—Inindoorenvironments,multi-robotvisual(RGB-
Dynamic Robot
D) mapping and exploration hold immense potential for ap-
plication in domains such as domestic service and logistics,
where deploying multiple robots in the same environment can Ghosting Trail Effect
significantlyenhanceefficiency.However,therearetwoprimary
Observer Robot
challenges: (1) the ”ghosting trail” effect, which occurs due to
overlappingviewsofrobotsimpactingtheaccuracyandquality
of point cloud reconstruction, and (2) the oversight of visual
reconstructions in selecting the most effective frontiers for
exploration.Giventhesechallengesareinterrelated,weaddress
themtogetherbyproposinganewsemi-distributedframework
(SPACE) for spatial cooperation in indoor environments that
enables enhanced coverage and 3D mapping. SPACE leverages
geometric techniques, including ”mutual awareness” and a (a) (b)
”dynamicrobotfilter,”toovercomespatialmappingconstraints.
Additionally, we introduce a novel spatial frontier detection
system and map merger, integrated with an adaptive frontier
assignerforoptimalcoveragebalancingtheexplorationandre-
construction objectives. In extensive ROS-Gazebo simulations,
SPACE demonstrated superior performance over state-of-the-
art approaches in both exploration and mapping metrics.
(c) (d)
I. INTRODUCTION
Fig. 1: The Ghosting Trail Problem: Formation of ghosting regions &
Multi-Robot Exploration (MRE) is pivotal in advancing
poor quality spatial maps due to inter-robot visibility during mapping and
robotics research due to its ability to enhance environmen-
exploration.(a),(b)and(c),(d)representsexploredandcomposite(where
tal awareness over extended periods, enabling applications eachcolorrepresentsalocalmapbyeachrobot)3DmapusingRRTwith
such as environmental monitoring, patrolling [1], search and Kimera-Multi[9]andSPACEwithRTABMap[10],respectively.
rescue[2],andintelligenttransportation[3].Thecoreobjec-
Compared to collaborative LIDAR-based exploration and
tive of MRE is to synergistically improve the autonomous
large-scalevisual(RGB-Dcamera-based)explorationframe-
navigation and mapping capabilities of coordinated robots,
works, the exploration of visual MRE in indoor environ-
optimizing spatial comprehension, cost-effectiveness, travel
ments remains underexplored. This is particularly important
time, and energy consumption. Recent advancements have
because robot interactions and overlapping views in indoor
seen the development of more efficient and resilient MRE
spaces introduce unique dynamic challenges. State-of-the-
algorithms and systems [4], integrating diverse objectives,
art (SOTA) SLAM approaches, such as RTAB-Map [10],
sensor modalities, and communication frameworks [5].
Kimera-Multi [9], ORB-SLAM3 [11], Swarm-SLAM [12],
Frontier-basedMREmethods[5][6]havereceivedsignif-
and CORB-SLAM [3], have demonstrated and improved
icant attention for their capacity to accelerate exploration by
mapping and data communication efficiency in indoor and
identifying and prioritizing frontiers that offer maximum in-
outdoor environments using improved use of RGB-D data,
formation gain. However, recent efforts have predominantly
butinter-robotvisibility(appearance)inmapreconstructions
focused on building efficient 2D grid maps, optimizing cost,
is typically neglected. These methods often underperform
and reducing travel time. These works often fall short when
in co-located multi-robot environments, suffering from a
appliedtotheconstructionofvisual3Dmaps.Manyexisting
”ghosting trail” effect due to overlapping robot exploration
approaches [7], [8] rely on computer vision-based frontier
paths (see Fig. 1 for an illustration). This ghosting signifi-
detection in 2D grid maps, which are not well-suited for the
cantly degrades 3D map quality, particularly during critical
complexities of 3D spatial reconstruction. This highlights a
stages like map merging and frontier identification, resulting
critical gap in the literature, particularly in extracting spatial
in poor spatial mapping and suboptimal exploration. These
frontiers and reconstructions.
resulting 3D maps can produce unexpected outcomes when
SchoolofComputing,UniversityofGeorgia,Athens,GA30602,USA. used in higher-level semantic classification, clustering, nav-
Authoremails:{sai.krishna;ramviyas}@uga.edu igation, and decision-making algorithms [13]–[16].
This work is supported by the Army Research Laboratory and was
To address this gap, this paper presents a semi-
accomplished under Cooperative Agreement Number W911NF-17-2-0181
(DCISTCRA). decentralized framework for multi-robot 3D spatial explo-
4202
voN
4
]OR.sc[
1v42520.1142:viXraration and mapping, termed SPACE. Our approach enhances often sensitive to noise. The feature-based map merging
the accuracy of generating dense 3D metric mesh models providesafuzzygridmapandhighlydependsontherobots’
by introducing mutual awareness, enabling robots to op- initial positions.
erate collaboratively in shared environments. In addition, Most MRE approaches rely on the grid map for frontier
we propose a bi-variate spatial frontier detection method, detection, goal assignment, and navigation. The Visual Si-
a dynamic robot filter, and a coherent spatial map merger multaneousLocalizationandMapping(VSLAM)approaches
for superior multi-robot mapping. We further introduce an [10], [24] aid the MRE with spatial mapping and are effec-
adaptive frontier assigner that optimizes spatial information tive for single-robot exploration in static environments. The
gain and the quality of dense metric map construction and semantic VSLAMs [25] are introduced to address the is-
achieves an optimal exploration, improving the 3D recon- sues,primarilytheghostingeffect,indynamicenvironments
struction accuracy and spatial coverage performance. caused by humans. Semantic VSLAMs eliminate dynamic
The core novelties and contributions of this paper are features by detecting humans using semantic segmentation
• The introduction of geometric-based mutual awareness or object detection. However, they are ineffective for multi-
and dynamic robot filter methods to address the spatial robotenvironmentsbecauseoftherapiddynamicmovements
constraints in visual mapping, significantly improving in robots and the difficulty in training the recognition algo-
3D multi-robot mapping in indoor environments. rithms for all kinds of robots, unlike humans.
• A novel MRE approach, leveraging frontier importance Contrary to the existing works, SPACE addresses and
balancing the exploration and 3D reconstruction objec- overcomesthechallengesassociatedwithMREinvisual3D
tives and an adaptive exploration validator to optimize maps. SPACE maximizes the efficiency of the 3D map with
exploration efficiency and coverage. a novel robot dynamic filter and map merging algorithm.
The exploration strategy is designed to consider the 3D
We demonstrate the effectiveness of SPACE in comprehen-
informationgain,whichbalancestheunexploredandweakly
sivesimulationexperimentsandvalidateagainststate-of-the-
reconstructed frontiers for effective spatial exploration.
art multi-robot mapping methods such as RTAB-MAP [10],
Kimera-Multi [9] and exploration approaches such as RRT
III. PROPOSEDMETHODOLOGY
[17], DRL [18], and SEAL [19]. Finally, we open-source
SPACE1 as a ROS package to facilitate its adoption and Problem Setting: Let there be n robots denoted by the set
R = {r ,r ,...,r }, each with known initial position in a
further development by the broader robotics community. A 1 2 n
global frame. Each robot r maps the environments, creating
videosupplementdemonstratingtheapproachinsimulations i
and real-world robots is available2. a spatial map P i. All these spatial maps {P 1,P 2,...,P n}
are merged to create a global exploration map P∗. The
II. RELATEDWORK frontiers F identified in P∗ are categorized into two sets:
unexplored F and weakly explored F based on their densi-
Frontier-based exploration is widely used in MRE, where u w
tiesandvariances,respectively.Thefrontierassignerassigns
the robot works towards maximizing its exploration by
afrontier f withmaximumrevenuevalueinU(r,{F ,F }∈
moving toward the unexplored areas on the map. Existing a i u w
F) to each robot r. To minimize the latency within the re-
works [20] [21] aim greedily to push robots either to the i
sources,thestreamingupdatesofthelocalandmergedmaps
closestfrontierormostuncertainregionformaximizationof
arecomputedandcommunicatedbyedge/centralprocessing.
coverage.Rapidly-exploringRandomTrees(RRTs)[17]have
The SPACE is a semi-distributed framework with cen-
significantly performed in planning multi-robot exploration
tralized map merging, frontier detection, and frontier as-
schemes for fast and efficient exploration. However, they
signment modules, as shown in Fig. 2. It consists of on-
sufferfromsuboptimalsolutionsduetothestochasticnature
board (distributed) processes such as Visual SLAM, mutual
of RRTs. Deep Reinforcement Learning (DRL) approaches
awareness, dynamic robot filtering, frontier validation, and
have been proposed to enhance multi-robot exploration. In
path planning. Moreover, SPACE utilizes translated frontiers
[18], DRL is integrated with Voronoi-based cooperative ex-
F′ ,F′ within2DgridmapM′
forpathplanningandnaviga-
plorationandwaslimitedbytrainingchallengestodistribute u w
tion. SPACE can be integrated with existing visual SLAMs
the coverage for each robot. SEAL [19], a recent Gaussian
(RTABMap [10] and Kimera-Multi [9]) for localization and
Processes-basedinformationfusionframework,isintroduced
RGB-Dmapping.Though2DgridmapM′
canbeextracted
to maximize the efficiency of exploration and localization.
fromthevisualSLAMs,existing2DSLAMapproachessuch
However, this approach fails to tackle non-linearized hulls
asgmapping[26]andSLAMtoolbox[27]canbeintegrated.
with their convex hull optimization. In CQLite [5], an
enhanced Q-learning was introduced to solve the problems
A. Mutual Awareness Module
of revisiting areas that are already explored by the robots.
The proposed mutual awareness is a geometric-based
Nonetheless, the existing approaches rely on contour-based
approach to determine whether the other robots are within
and feature-based techniques for frontier detection [22] and
the Field-of-View (FoV) of the observer robot. Moreover,
map merging [23]. These frontier detection algorithms are
this approach provides an ability to recognize the presence
1https://github.com/herolab-uga/SPACE-MAP of other robots within their FoV & avoid explicit visual
2https://youtu.be/EE0velFrJgI mapping. This method performs pairwise iteration with allRobot r
n
Central/Edge Processing
Distributed Processing Robot r
2
Robot r
Path Planner and 1
Motion Control Goals Spatial Map Merger* Grid Map Merger*
Local Controller Local
Odometry
Goal Validator*
Global & Maps
Adaptive Frontier
Odometry Frontier Detection*
Localization Assigner*
Mutual Global
Global
Visual SLAM Awareness* Odometry
Odometry,
(RTABMAP, Goals
Dynamic Robot Merged Frontier Translator
Kimera-Multi, etc.) Dynamic
Filter* Map,&
Features
Goals
Fig. 2: Overview of the proposed methodology. The Blue-shaded components are Mapping-related modules, and the Orange-shaded components are
Exploration-realatedmodules.Markedwithanasterisk(*)arenovelelementsintroducedinthispaperformulti-robotexploration.
the robots in the environment. Consider an observer and a Given a robot observer and a target, the objective is to
target in a global frame, with their respective pose p = determine the image coordinates (u,v,d) on the observer’s
o
(x ,y ,ψ ),p = (x,y,ψ). We solve the problem on the camera at which the target appears. This involves translating
o o o t t t t
plane, ignoring the z component, roll, and pitch in the pose. thetarget’s positionfrom theworld coordinatesystemto the
Firstly,wedetectwhetheranyotherrobotiswithintheRGB- camera’simageplanethroughaseriesoftransformations.Let
D mapping range before FoV calculation. This effective the observer’s camera height from the ground be denoted by
detectionofrobotproximitybythesensorisvalidandfurther h . First, we calculate the relative position of the target with
o
processedonlyif0≤∥p −p∥≤γ,whereγ isthemaximum respecttotheobserverina3Dcoordinatesystemfroma2D
o t
range of the RGB-D camera. If any target robot is within coordinate system with h . Later, we calculate the relative
o
proximity, we calculate the angle θ between the observer position vector to get the position in the observer’s frame as
and target. To ensure the angles are in between [−π,π], we
p =R(−ψ ).(p [0],h ,∥p ∥)T, (2)
perform angle normalization of observer yaw ψ and θ as obs-frame obs rel-2D obs rel-2D
where R(.) is the rotation matrix constructed to account
θ =((θ+π) mod2π)−π,
norm (1) for the observer’s yaw ψ in the plane, and p rel-2D =p t−
ψ norm=((ψ obs+π) mod2π)−π. p
o
refers to the position of relative position in 3D co-
ordinate system p . The p in the coordinates
The normalized angular difference between the observer’s rel-3D obs-frame
of the target relative to the observer, rotated and adjusted
orientation and the vector pointing to the target is com-
for height, giving the observer’s perspective in 3D. After
puted by ∆θ = (θ −ψ +π) mod2π−π. Consid-
norm norm
the estimation of position of target in observer’s frame, the
ering robots as circular objects maximizes precision in
positionisprojectedtotheimageplanewiththeintrinsicand
awareness. The target robot with its largest dimension as the
(cid:16) (cid:17) extrinsic camera parameters. The intrinsic camera matrix,
radiusisrepresentedasangularsizeα=arctan R ,as
∥po−pt∥ which transforms 3D camera coordinates into 2D image
viewed from the observer. The visibility from the observer’s
coordinates, is represented as K. The extrinsic parameters
camera can then be estimated by comparing the angle ∆θ
M are encapsulated by the rotation matrix R and the
ext ext
to the effective half FoV and angular size of the robot.
translation vector T , which align the camera with respect
ext
The target robot is within the observer robot’s FoV if:
to a global reference frame. The matrix product of camera
|∆θ|+ c ≤ FoVcam−α, where FoV is the horizontal
FoV
of∥p tho− ept c∥ amera,2
R is the robot’s
radca im
us, and c is
parameterswiththehomogeneouscoordinatesofthetarget’s
∥po−pt∥ position in the observer’s frame yields normalized image
the dynamic buffer to balance the latency with distance. coordinates as shown:
 
B. Dynamic Robot Filter (DRF) Module  u 0
(cid:20) (cid:21)
WhenthetargetrobotiswithintheFoV,theobserverrobot v=K M ext; 0  · p obs-frame (3)
 0  1
continuously maps the target robot, leading to a ghosting d
1
traileffectinthemap.TheproposedDRFeliminatesthedy-
namic features of the target robot while mapping. The DRF Moreover, a 3D bounding box with dimension c ,
∥po−pt∥
estimates the target robot’s position in the image coordinate where c is an upscaling constant, is introduced centering
frame. Converting a world coordinate system to an image (u,v,d) to extract the region of target robot within the ob-
coordinate system is often challenging. It involves precise serverframe.ADBSCANclusteringalgorithmsegmentsthe
utilization of both intrinsic & extrinsic camera parameters, robot within the bounding box and eliminates the dynamic
and real-world camera pose to translate the coordinates. features within the cluster.C. Coherent Spatial Map Merger Module the variance represents the weakly constructed regions. The
unexplored points U are identified by densities below a
The objective of spatial map merger is to find a set
of rigid transformations {T i}n
i=1
that align all point clouds thresholdδ d,andweaklyconstructedindicesW byvariances
{P}n into a unified, coherent global coordinate space. exceeding the threshold δ v, as shown in Alg. 1.
i i=1
This approach aims to minimize the global alignment error
Algorithm 1 3D Frontier Detection in Point Cloud
across transformations of point clouds simultaneously to
calculate the merged map P∗ with maximum alignment: Require: P∗, ρ, r s, δ d, δ v
Ensure: F ,F
u w
min ∑ Λ i,j·∥T iP i−T jP j∥2. (4) 1: P∗← Downsample(P, ρ)
{Ti} (i,j)∈E 2: Initialize arrays: ρ, σ2
Here, T
i
and T
j
are the transformations applied to point 3: for each point p i∈P d∗ do
cloudsP
i
andP
j
respectively,E representssetofalledges 4: ρ(p i) ←|{p j∈P d∗:∥p i−p j∥≤ρ}|
in the interconnected pose graph consisting point clouds 5: N ρ(p i) ←{p j∈P d∗:∥p i−p j∥≤ρ}
{P i}n i=1 as nodes. Each edge, for instance P i to P j, in 6: p← |Nρ1 (pi)|∑ pj∈Nρ(pi)p j
ap no dse ing fr oap rmh ai ts ioq nua mn ati tfi rie cd esus Λin i,g j.tr Ta hn esfo trr am na st fi oo rn mam tia ot nric mes atT ri i, xj 87 :: endσ f2 o( rp i) ← |Nρ1 (pi)|∑ pj∈Nρ(pi)∥p j−p∥2
T i,j aligns P i to P j, and the information matrix Λ i,j 9: return F u ←{i:ρ(p i)<δ d}, F w ←{i:σ2(p i)>δ v}
quantifies the certainty of this alignment. This pose graph
isresponsibleforallthesimultaneoustransformationsofthe
point clouds. The existing SOTA approaches, Kimera-Multi E. Adaptive Frontier Assignment Module
[9],donotconsiderthenon-linearitiesintransformationesti-
This module is responsible for the mathematical and
mation,whichresultsinnoiseandsub-optimalsolutions.The
operational dynamics of the frontier assignment within the
Levenberg-Marquardt algorithm [28] is utilized to solve this
SPACE exploration framework. We categorized the entire
non-linear least squares problem. This method interpolates
mechanism into two parallel threads:
between the Gauss-Newton algorithm and gradient descent,
3DInformationGain:Itdescribesthepossibilityofgaining
offeringarobustapproachfordealingwithnon-linearitiesin
newinformationthroughfrontierexploration.Thesetofvox-
the iterative transformation estimation:
elsV within the region around the frontier f are considered
f
T(k+1)=T(k)−(JTΛJ+µI)−1JTΛr, (5) in estimation of possible information gain. The information
gain I (f) for a frontier f is the average non-probabilistic
where J is the Jacobian matrix of the residuals r with g
Shannon entropy [29]across its constituentpoints, as shown
respect to the transformations. Λ is a block-diagonal matrix
in the equation below:
consisting of all information matrices Λ , and µ is the
i,j
damping factor that adjusts the algorithm’s convergence  1
behavior. The iterations continue until the alignment error
−
|V f|
v∑ ∈Vfρ(v)log(ρ(v)) if f ∈F
u
function reaches its global minimum. I g(f)=
1
(7)
D. Bi-variate Spatial Frontier Detection Module
−
|V f|
v∑ ∈Vfσ2(v)log(σ2(v)) if f ∈F
w
In recent works, frontiers in an environment are typically
Frontier Importance:Thefrontieridentifiercategorizesthe
delineated by contours along the edges in grid maps. How-
frontiers into F and F . Initially, exploration prioritizes the
ever, these frontiers do not account for partially constructed u w
unexplored regions, but over time, the importance shifts
& unexplored regions in a 3D space. We propose a novel
towards weakly constructed regions. We proposed an im-
approach for identifying two-classes of frontiers: 1.) weakly
portant function that considers the class of frontier, time of
explored regions 2.) unexplored regions within dense-metric
exploration, and distance to frontiers. The distance factor D
3D maps as shown in Fig. 3. These frontiers are detected
represents the average Euclidean distance from the centroid
within the merged map P∗, where each point p ∈P∗ is
i c at l of all robots to the nearest unexplored frontier,
a vector in R3. We define a voxel grid with side length ρ c
relative to the average distance to weakly explored frontiers
to obtain downsampled point cloud, P∗. The density ρ(p)
d i (D= ∑f∈Fu∥lc−f∥ ). The time factor T(t) measures elapsed
at a point p i is calculated based on the number of neighbor ∑f∈Fw∥lc−f∥
timerelativetothestartoftheexplorationphase.Thefrontier
points p within a specified radius r :
j s
importance I(f) is expressed as:
t
ρ(p)=|{p ∈P∗:∥p −p∥≤r }| (6)
i j d j i s  e−λT(t)×D
The variance σ2(p i) for each point p
i
in a point cloud is 1+e−λ(T(t)−ξ) if f ∈F u,
c oo fm nep iu gt he bd ort io ngca pl oc iu nl ta ste
N
ρth (e
p
is )p wat ii ta hl inv ta hr eia rb ai dli it uy sa ρm .Ton hg edth ee nss ite yt I t(f)=
1−
e−λT(t)×D
if f ∈F w,
(8)
ρ(p) and variance σ2(p) are calculated efficiently using a
1+e−λ(T(t)−ξ)
i i
KD-Treetoavoidabrute-forcesearchthroughallpoints.The whereλ isascalingparameterandξ isathresholdparameter
density of a point presents the unexplored region, whereas that determines the transition in the sigmoid function. The(a) (b) (c)
Fig. 3: (Left) Contour-based Frontier Detection [22] (Center) Bi-Variate Spatial Frontier Detection of SPACE (Right) Grid
map with Translated Spatial Frontiers.
revenue function of a frontier with respect to a robot r at for visual SLAM within each robot, generating 2D & 3D
i
position p is calculated with 3D information gain, frontier submaps using data from the camera sensor. The 2D sub-
i
importanceandheuristicdistancehbetweenp andtranslated maps, utilized for path planning, are merged by multirobot-
i
frontier p f′ with κ is a scaling constant, as shown below. map-merge [31] without considering the initial positions of
robots. The ROS move-base package [32] enables the robots
I(f)·I (f)
U(r,f′ )= t g , (9) to navigate toward their designated goals while avoiding ob-
i
κ×h(p i,p f′) stacles.ThispackageincorporatestheA*algorithmforlong-
range path planning and the Dynamic Window Approach
′
The maximum revenue value within U(r,F ) ∈
i (DWA)[33]forreal-timeobstacleevasion.Theglobalframe
′ ′ ′
{U(r,f ),U(r,f )...U(r,f )} is assigned to the robot r.
i 1 i 2 i m i referstoareferenceframefixedatapointintheworld,while
F. Adaptive Exploration Goal Validator Module thelocalframepertainstoaspecificrobot’sreferenceframe.
Generally,certainrobots,especiallyUGVs,arenotcapable We conducted experiments in two indoor simulation envi-
of mapping the entire environment due to their 2D mobility ronments:AWSHouse[34](70m2 area)andAWSBookstore
andheight,resultinginanincreaseininvalidfrontiers(which [35] (100m2 area). Robots are mounted with Kinect-V2
cannot be explored). To tackle uncertain or invalid frontiers, RGB-D camera with horizontal FoV =84.1°and maxi-
cam
whichsignificantlyextendthedurationofrobotexplorations, mum range γ = 5m for gathering the spatial information.
we introduce the adaptive exploration validator, which esti- Evaluations focused primarily on three distinct setups to
mates the exploration duration effectively. To facilitate this, test the system’s effectiveness and adaptability: three robots
we employ the A* path planning algorithm to determine navigating the house, three robots in the bookstore, and six
′
the shortest path P between the 2D translated frontier f robots operating within the bookstore. Robots had a max.
and the robot’s current position p. The exploration time linear speed of 0.5m/s and a turning rate of πrad/s.
4
τ required to traverse the sequence of points on the path
P={p ,p ,...,p }(forafixedtimehorizonτ)iscalculated Thegroundtruthmaps,forexperimentalanalysis,aregen-
1 2 τ
as follows: eratedwith theGazeboMap Generator [36].For evaluation,
theexploredandground-truthmeshesaresampleduniformly
t e(p,p f′)=t e(p,p 1)+t e(p 1,p 2)+...+t e(p n,p f′) (10) with 103points/m2 as in [9].
The Adaptive Exploration Validator monitors the explo- Performance Metrics: The metrics below are used to eval-
ration time for the last β points within the n-point estimated uate mapping and exploration objectives comprehensively.
trajectory. If the robot exploration time is greater than the
t (R ,R ), then the frontier f is considered as invalid.
e β f
• Bounding Volume (BV): The volume of the 3D map
G. Algorithmic Complexity created by the ghosting trail effect.
The overall time complexity of the Mutual Awareness is • Total Coverage (2D/3D): The % of a map in the 2D
O(n ×n),andisdependentonthenumberofrobots(n <n) (grid)or3D(pointcloud)thathasbeenmappedrelative
p p
withinproximity.ThetimecomplexityofDRF,SpatialFron- to the respective ground truth. The 3D total coverage is
tierDetection,3Dinformationgain,andFrontierImportance calculated after the removal of the ghosting region.
are O(1), O(n2), O(n), and O(n×m), respectively. Here, n • OverlapPercentage(2D/3D):Thepercentageofamap
is the number of robots, and m is the number of frontiers. in the 2D (grid) or 3D (point cloud) that has been
covered more than once during the mapping process.
IV. EXPERIMENTALRESULTSANDANALYSIS
• Mapping Time: To assess efficiency, the total duration
We implemented the approach in the ROS-Gazebo frame- (s)forexplorationaveragedacrossthetrialsisreported.
work and Turtlebot-3 Waffle Robots. SPACE is employed • Total Distance: The cumulative distance (m) (i.e., path
with the ROS RTAB-Map package [30] & Kimera-Multi [9] length) traveled by all the robots during the trial.AWS House World AWS Bookstore World
Fig.4:Performanceofthespatialmappinginmulti-robotexploration.Here,weusedtheSPACEexplorationapproachdescribedinSec.IIIinallmapping
variants for a fair comparison. The top row subplots show the 3D reconstruction accuracy in the three scenarios tested ((Left): AWS House World (3
Robots),(Middle):AWSBookstoreWorld(3Robots),and(Right):AWSBookstoreWorld(6Robots)).Inthebottomrow,the(Left)plotshowsthedetailed
robot-wiseperformancevariations,and(Right)plotshowstheimpactoftheghostingtraileffect(boundingvolumeofthereconstructioninaccuracies)by
increasingtherobotdensityinagivenarea.SPACE-RTABMapisinvarianttotheghostingeffectwithanincreaseindensity.
TABLE I: Performance comparison of SPACE with other base mapping
methods in the AWS Bookstore world with 3 Robots (Note: SPACE’s
explorationapproachisusedforallthecomparedmethods).Bestvaluesare
inboldfaceandasterisk(∗).TheresultshighlightthebenefitsofourDRF
layerinimproving3Dreconstructionaccuracyanditseffectinexploration.
Metrics RTAB-Map Kimera- SPACE w/SPACE w/
Multi RTABMap Kimera-Multi
MappingTime(s) 359±51 338±64 321±21∗ 324±23
TotalDistance(m) 247±18 253±12 142±8∗ 152±6 Fig. 5: Performance Analysis of Exploration Strategies with respect to
2DCoverage(%) 93±2 95±4 100±3 100±2∗ TotalSpatialCoverageintestedenvironments(Left)AWSHouseWorld(3
3DCoverage(%) 87±4 90±6 91±3 94±6∗ robots)(Right)AWSBookstoreWorld(3robots).
2DOverlap(%) 42±8 43±3 35±1∗ 37±4
3DOverlap(%) 41±1 47±8 38±2∗ 40±2
3D-MapRMSE(%) 2.01 2.76 0.12∗ 0.14 increases, the RMSE tends to be more unstable throughout
BV(%) 9.1 8.86 0.01∗ 0.015 the exploration due to the increase in the frequency of the
ghosting effect. Moreover, while SPACE shows a slightly
lower RMSE (-24.8%) compared to individual robot-wise
A. Mapping Performance map comparisons, it outperforms in the RMSE of merged
The performance of the spatial mapping during and after maps, demonstrating superior performance when integrating
MRE in different scenarios are depicted in Table I and spatial maps from multiple robots. Moreover, the experi-
Fig. 4. We tested the SPACE framework integrated with ments with isolated environments depicted an exponential
recent VSLAMs such as RTAB-Map [10] and Kimera-Multi relationship between the number of robots and the bounding
[9], in addition to standalone evaluations of RTAB-Map volume of the ghosting region. This implies the advantages
and Kimera-Multi, to facilitate a clear comparison. The of the SPACE are amplified in a denser robot deployment.
SPACE-integrated VSLAMs are equipped with our mutual
B. Exploration Performance
awareness, dynamic robot filter, and map merging modules,
whereas standalone approaches are equipped only with the TheSPACEmulti-robotexplorationapproachiscompared
SPACE exploration approach and ICP map merging as in withtherecentarchitecturesRRT[17],DRL[18],andSEAL
Kimera-Multi [37]. Unlike the RMSE trend observed with [19] in the AWS House and AWS Bookstore worlds. The
RTAB-Map and Kimera-Multi, which fluctuated, the RMSE results are presented in Table II and an analysis of the
oftheSPACE,whenintegratedwiththesesystems,exhibited efficiency of the exploration approaches with respect to the
a more consistent and stable decrease throughout explo- spatial coverage is depicted in Fig. 5. The SPACE surpasses
ration. This is because of the generation of ghosting regions theothermethods,achievingmaximumcoverageinlesstime.
throughout the exploration in the standalone systems. Moreover, SPACE without DRF performed comparably to
The overall performance of the SPACE with both RTAB- SEAL, and both configurations significantly outperformed
MapandKimera-Multiis25timeshigherthanthestandalone the RRT and DRL methods.
RTAB-Map and Kimera-Multi. As the number of robots The spatial explored maps across the AWS House andTABLE II: PerformancecomparisonofSPACEwithotherexplorationmethods.Note:RTAB-Mapisusedasthe3Dmappinglayerforallmethods.
Evaluationparameters ThreerobotsinAWSHouseworld ThreerobotsinAWSBookstoreworld SixrobotsinAWSBookstoreworld
RRT[17]DRL[18]SEAL[19] SPACE RRT[17]DRL[18]SEAL[19] SPACE RRT[17]DRL[18]SEAL[19] SPACE
MappingTime(s) 608±52 466±67 457±34∗ 463±27 347±32 324±21 324±18 321±17∗ 212±18 267±29 197±11∗ 199±9
TotalDistance(m) 192±11 104±19 156±11 154±9∗ 278±26 235±9 142±8∗ 151±18 223±12 196±17 141±18 138±6∗
Total2DCoverage(%) 87±4 91±3 90±2 97±3∗ 90±5 93±2 99±1 100±3∗ 93±4 94±5∗ 93±1 93±2
Total3DCoverage(%) 79±3 88±2 92±2 95±1∗ 82±2 88±4 91±3 98±1∗ 82±4 92±1 91±3 94±3∗
2DOverlapPercentage(%) 51±5 46±6 24±2∗ 27±2 57±8 51±9 37±5 35±1∗ 47±7 39±8 24±8∗ 26±2
3DOverlapPercentage(%) 68±1 67±6 65±5 51±4∗ 42±7 49±1 38±2 28±6∗ 40±13 37±12 41±2 19±3∗
3D-MapRMSE 3.71 3.78 3.68 0.08∗ 2.55 2.88 2.79 0.12∗ 4.10 3.95 3.15 0.07∗
BoundingVolume(%) 10.1 9.95 9.78 0.02∗ 8.1 7.76 9.64 0.01∗ 20.5 21.7 19.5 0.52∗
RRT DRL SEAL SPACE
Fig. 6: The3DMergedandLocalReconstructionMapswithtrajectoriesofeachrobotfromexperimentsintheAWSHouseandwith3robots.
RRT DRL SEAL SPACE
Fig. 7: The3DMergedandLocalReconstructionMapswithtrajectoriesofeachrobotfromexperimentsintheAWSBookstoreandwith3robots.
AWS Bookstore worlds with benchmark exploration algo- SPACE-Map is improved by 5% compared to the existing
rithms are shown in Fig. 6, 7. The SPACE consistently architectures. Moreover, we observed a reduction of an
outperformedtheRRTandDRLinkeyperformancemetrics, average of 26 meters in travel distance compared to the
achieving broader coverage in less time and distance, with bestinRRT,DRL,andSEALintheconductedexperiments.
anaverage5.1%and14.3%increasein2Dand3Dmapping The grid map overlap percentage is reduced by 18% in
coverage, respectively. Although the mapping time is 1% average across all the scenarios. Moreover, the 3D overlap
higher than the DRL in three robot house worlds, the total percentage outperformed RRT, DRL and SEAL by 20% in
path is reduced by almost 8%. The overall mapping time of all the scenarios. On average, the RMSE of the SPACE 3Dmap is approximately 97.85% lower when compared to the [14] D. De Gregorio and L. Di Stefano, “Skimap: An efficient mapping
3D maps explored using RRT, DRL, and SEAL methods. frameworkforrobotnavigation,”in2017IEEEInternationalConfer-
ence on Robotics and Automation (ICRA). IEEE, 2017, pp. 2569–
Moreover,theeffectoftheboundingvolumeoftheghosting
2576.
trail is negligible in SPACE. [15] T. Ran, L. Yuan, J. Zhang, D. Tang, and L. He, “Rs-slam: A robust
semanticslamindynamicenvironmentsbasedonrgb-dsensor,”IEEE
V. CONCLUSION SensorsJournal,vol.21,no.18,pp.20657–20664,2021.
[16] S. S. Kannan, W. Jo, R. Parasuraman, and B.-C. Min, “Material
We introduced SPACE, a multi-robot spatial exploration mapping in unknown environments using tapping sound,” in 2020
pipelineoptimizedforindoorenvironments,whererobotsare IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems
colocated during exploration. Our semi-distributed approach (IROS). IEEE,2020,pp.4855–4861.
[17] L.Zhang,Z.Lin,J.Wang,andB.He,“Rapidly-exploringrandomtrees
maximizes the efficiency of dense metric 3D mesh models multi-robotmapexplorationunderoptimizationframework,”Robotics
and their utility for exploration, accounting for the complex andAutonomousSystems,vol.131,p.103565,2020.
spatial constraints within indoor environments. Moreover, [18] J. Hu, H. Niu, J. Carrasco, B. Lennox, and F. Arvin, “Voronoi-
based multi-robot autonomous exploration in unknown environments
SPACE surpassed in exploration and mapping performances via deep reinforcement learning,” IEEE Transactions on Vehicular
comparedtootherbenchmarkexplorationstrategiesinexten- Technology,vol.69,no.12,pp.14413–14423,2020.
sivesimulationexperiments.SPACEoffersfastspatialexplo- [19] E. Latif and R. Parasuraman, “Seal: Simultaneous exploration and
localizationformulti-robotsystems,”in2023IEEE/RSJInternational
ration, making it beneficial for indoor robot applications. ConferenceonIntelligentRobotsandSystems(IROS). IEEE,2023,
pp.5358–5365.
REFERENCES [20] B.Yamauchi,“Afrontier-basedapproachforautonomousexploration,”
inProceedings1997IEEEInternationalSymposiumonComputational
[1] M. V. Espina, R. Grech, D. De Jager, P. Remagnino, L. Iocchi,
IntelligenceinRoboticsandAutomationCIRA’97.’TowardsNewCom-
L. Marchetti, D. Nardi, D. Monekosso, M. Nicolescu, and C. King,
putationalPrinciplesforRoboticsandAutomation’. IEEE,1997,pp.
“Multi-robot teams for environmental monitoring,” Innovations in
146–151.
Defence Support Systems–3: Intelligent Paradigms in Security, pp.
[21] ——,“Frontier-basedexplorationusingmultiplerobots,”inProceed-
183–209,2011.
ings of the second international conference on Autonomous agents,
[2] A. Romero, C. Delgado, L. Zanzi, R. Sua´rez, and X. Costa-Pe´rez,
1998,pp.47–53.
“Cellular-enabled collaborative robots planning and operations for
[22] M.KeidarandG.A.Kaminka,“Efficientfrontierdetectionforrobot
search-and-rescuescenarios,”in2024IEEEInternationalConference
exploration,”TheInternationalJournalofRoboticsResearch,vol.33,
onRoboticsandAutomation(ICRA),2024,pp.5942–5948.
no.2,pp.215–236,2014.
[3] F. Li, S. Yang, X. Yi, and X. Yang, “Corb-slam: a collaborative
[23] J.Ho¨rner,“Map-mergingformulti-robotsystem,”2016.
visual slam system for multiple robots,” in Collaborative Comput-
[24] R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos, “Orb-slam: a
ing: Networking, Applications and Worksharing: 13th International
versatileandaccuratemonocularslamsystem,”IEEEtransactionson
Conference,CollaborateCom2017,Edinburgh,UK,December11–13,
robotics,vol.31,no.5,pp.1147–1163,2015.
2017,Proceedings13. Springer,2018,pp.480–490.
[25] G. S. Krishna, K. Supriya, and S. Baidya, “3ds-slam: A 3d object
[4] R. Almadhoun, T. Taha, L. Seneviratne, and Y. Zweiri, “A survey
detectionbasedsemanticslamtowardsdynamicindoorenvironments,”
on multi-robot coverage path planning for model reconstruction and
arXivpreprintarXiv:2310.06385,2023.
mapping,”SNAppliedSciences,vol.1,pp.1–24,2019.
[26] G. Grisetti, C. Stachniss, and W. Burgard, “Improved techniques for
[5] E. Latif and R. Parasuraman, “Communication-efficient multi-robot grid mapping with rao-blackwellized particle filters,” IEEE transac-
exploration using coverage-biased distributed q-learning,” IEEE tionsonRobotics,vol.23,no.1,pp.34–46,2007.
RoboticsandAutomationLetters,2024.
[27] S.MacenskiandI.Jambrecic,“Slamtoolbox:Slamforthedynamic
[6] A. Batinovic, T. Petrovic, A. Ivanovic, F. Petric, and S. Bogdan, world,” Journal of Open Source Software, vol. 6, no. 61, p. 2783,
“A multi-resolution frontier-based planner for autonomous 3d explo-
2021.
ration,”IEEERoboticsandAutomationLetters,vol.6,no.3,pp.4528–
[28] J.J.More´,“Thelevenberg-marquardtalgorithm:implementationand
4535,2021. theory,”inNumericalanalysis:proceedingsofthebiennialConference
[7] B.P.L.Lau,B.J.Y.Ong,L.K.Y.Loh,R.Liu,C.Yuen,G.S.Soh, heldatDundee,June28–July1,1977. Springer,2006,pp.105–116.
andU.-X.Tan,“Multi-agv’stemporalmemory-basedrrtexplorationin
[29] L.J.Wuest,B.G.Nickerson,andR.A.Mureika,“Informationentropy
unknownenvironment,”IEEERoboticsandAutomationLetters,vol.7,
ofnon-probabilisticprocesses,”Geographicalanalysis,vol.35,no.3,
no.4,pp.9256–9263,2022.
pp.215–248,2003.
[8] H. Umari and S. Mukhopadhyay, “Autonomous robotic exploration
[30] Introlab, “Rtab-map’s ros package,” https://github.com/introlab/
based on multiple rapidly-exploring randomized trees,” in 2017
rtabmapros,2024.
IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems
[31] J.Horner,“Rospackagesformultirobotexploration,”https://github.
(IROS),2017,pp.1396–1402.
com/hrnr/m-explore,2016.
[9] Y. Tian, Y. Chang, F. H. Arias, C. Nieto-Granda, J. P. How, and
[32] ROS-Planning, “Ros navigation stack,” https://github.com/
L.Carlone,“Kimera-multi:Robust,distributed,densemetric-semantic
ros-planning/navigation,2023.
slamformulti-robotsystems,”IEEETransactionsonRobotics,vol.38,
[33] D.Fox,W.Burgard,andS.Thrun,“Thedynamicwindowapproachto
no.4,2022. collisionavoidance,”IEEERobotics&AutomationMagazine,vol.4,
[10] M. Labbe´ and F. Michaud, “Rtab-map as an open-source lidar and
no.1,pp.23–33,1997.
visual simultaneous localization and mapping library for large-scale
[34] AWS-Robotics, “A house world with multiple rooms and furni-
and long-term online operation,” Journal of field robotics, vol. 36,
ture for aws robomaker and gazebo simulations,” https://github.com/
no.2,pp.416–446,2019.
aws-robotics/aws-robomaker-small-house-world,2020.
[11] C. Campos, R. Elvira, J. J. G. Rodr´ıguez, J. M. Montiel, and
[35] ——, “A bookstore world with shelving and tables for aws
J.D.Tardo´s,“Orb-slam3:Anaccurateopen-sourcelibraryforvisual,
robomaker and gazebo simulations,” https://github.com/aws-robotics/
visual–inertial,andmultimapslam,”IEEETransactionsonRobotics,
aws-robomaker-bookstore-world,2020.
vol.37,no.6,pp.1874–1890,2021.
[36] Arshadlab,“Ros2gazeboworld2d/3dmapgenerator,”https://github.
[12] P.-Y. Lajoie and G. Beltrame, “Swarm-slam: Sparse decentralized
com/arshadlab/gazebomapcreator,2023.
collaborative simultaneous localization and mapping framework for
[37] P.J.BeslandN.D.McKay,“Methodforregistrationof3-dshapes,”
multi-robotsystems,”IEEERoboticsandAutomationLetters,vol.9,
inSensorfusionIV:controlparadigmsanddatastructures,vol.1611.
no.1,pp.475–482,2023.
Spie,1992,pp.586–606.
[13] S. K. Ravipati, E. Latif, R. Parasuraman, and S. M. Bhandarkar,
“Object-orientedmaterialclassificationand3dclusteringforimproved
semantic perception and mapping in mobile robots,” arXiv preprint
arXiv:2407.06077,2024.