VERITAS: A Unified Approach to Reliability Evaluation
RajkumarRamamurthy* MeghanaArakkalRajeev⋆ OliverMolenschot
JamesZou NazneenRajani
CollinearAI
team@collinear.ai
Abstract raisedincreasingconcernsaboutthesafetyandre-
liabilityofLLMapplications(Xuetal.,2024). We
Large language models (LLMs) often fail to notethat,although,hallucinationsareundesiredfor
synthesize information from their context to knowledgeintensivetasks,thesamequirkisactu-
generate an accurate response. This renders
allydesirableincreativetaskssuchasstory-telling,
themunreliableinknowledgeintensivesettings
image-generation,proseorpoetrygeneration,and
wherereliabilityoftheoutputiskey. Acritical
brainstorming.
componentforreliableLLMsistheintegration
Hallucinations are particularly common in
ofarobustfact-checkingsystemthatcandetect
hallucinations across various formats. While closed-book settings, where the knowledge en-
severalopen-accessfact-checkingmodelsare coded in the model’s weights and the LLM has
available, their functionality is often limited torecallitduringgeneration. Inthissetting,there
to specific tasks, such as grounded question-
isnorelevantsourcematerialorcontextprovidedto
answeringorentailmentverification,andthey
theLLMandithastosolelyrelyontheknowledge
performlesseffectivelyinconversationalset-
embeddedinitsweightduringthepre-trainingand
tings. Ontheotherhand,closed-accessmodels
post-trainingstages. Kadavathetal.(2022)show
likeGPT-4andClaudeoffergreaterflexibility
acrossdifferentcontexts,includinggrounded thatforsuchclosed-booksettings,self-evaluation,
dialogueverification,butarehinderedbyhigh wherein a LLM evaluates the validity of its own
costsandlatency. Inthiswork,weintroduce claimsandpredictingwhetheritcancorrectlyan-
VERITAS,afamilyofhallucinationdetection swer a user question, works well in the multiple-
modelsdesignedtooperateflexiblyacrossdi-
choiceandtrue/falsetasksettings.
verse contexts while minimizing latency and
On the other hand, in open-book settings, the
costs. VERITAS achieves state-of-the-art re-
LLM has access to relevant source materials ei-
sults considering average performance on all
ther provided directly in context or as part of a
majorhallucinationdetectionbenchmarks,with
10% increase in average performance when compoundsystemthatusesRetrieval-Augmented
comparedtosimilar-sizedmodelsandgetclose Generation(RAG).RAG(Guuetal.,2020;Lewis
totheperformanceofGPT4turbowithLLM- etal.,2020;Izacardetal.,2024)isawidelyused
as-a-judgesetting.
approachforbuildinguserfacingNLPapplications,
suchassystemsforgroundedquestionanswering
1 Introduction
(QA),fact-checking,summarizationandcustomer
support. LLMsareoftenunreliableinsuchopen-
Large language models (LLMs) (Brown et al.,
booksettings,andgenerateresponsesthatcontra-
2020)havemaderemarkablestridesinknowledge
dictsinformationprovidedinitscontext,especially
intensivetaskssuchassearch,questionanswering,
whentheknowledgedomainoftheinformationin
and natural language understanding. These mod-
contextisoutofdistribution(Shusteretal.,2021;
els, trained on vast amounts of data, possess the
Magesh et al., 2024). This severely hinders the
abilitytogeneratecoherentandcontextuallyrele-
application of these powerful models on private
vanttext. However,theyalsoexhibitaconcerning
knowledgestoresoncomplexdatainvolvingmulti-
issue: theirgeneratedcontentoftenincludesplau-
stepreasoning.
sible but factually incorrect information. These
Manydifferentreasonshavebeenproposedfor
incorrect outputs, known as hallucinations, have
why LLMs exhibit such quirky behavior includ-
*Equalcontribution ingdefectivetrainingdataandbenchmarks(Dziri
4202
voN
5
]LC.sc[
1v00330.1142:viXraetal.,2022a),biasintrainingdata(McKennaetal., 2 RelatedWork
2023),post-trainingonnewknowledge(Gekhman
ReliabilityEvaluation Therehasbeenaslewof
et al., 2024), and knowledge cutoff (Vu et al.,
workonLLMevaluationandthereisanentireline
2023). To address the problem of reliability in
ofresearchthatfocusesonreliabilityorhallucina-
LLMs, a number of benchmarks to evaluate fac-
tionevaluation. Thehallucinationevaluationtask
tuality have been created in diverse task formats
istopredictwhetherornottheLLMsresponseis
such as NLI, QA, and dialog. Humans or LLMs
consistentwithagivencontext. Thecontextisusu-
intheLLM-as-a-Judgesettingsareprimarilyused
ally in the form of a long-form source document
for evaluation on these benchmarks. It leverages
or k retrieved documents from a RAG retriever.
thepowerofLLMstoperformtheroleofJudges,
The task setup for this kind of evaluation is like
whichcanprovidejudgementsoncontentquality,
theopen-bookexamwhereintheLLMgainsnew
coherence, and alignment (Vu et al., 2024; Kim
knowledge through the context. In contrast, the
etal.,2024). Lynx(Ravietal.,2024)andBeSpoke-
closed-bookexamevaluatestheabilityofaLLM
MiniCheck (Tang et al., 2024a) are examples of
toretrieveknowledgeaccuratelyfromitsweights
LLM-as-a-judgetrainedtojudgehallucinationsfor
baked-induringpre-trainingorpost-training.
QAandNLItasksrespectively. Thesemodelsdo
Thehallucinationevaluatorsbelongtotwobroad
nogeneralizetotasksbeyondwhattheyaretrained
classes of models: 1. classifier models (encoder
on. Consequently, there is no single model that
transformers) and 2. generative models (decoder
worksacrossdifferenttaskformatsandondiverse
transformers).
domaindatasets.
ARES (Saad-Falcon et al., 2024) is a suite of
AunifiedapproachtoevaluatingLLMreliability
classifier models based on DeBERTa-v3- Large
wouldnotonlyenablebetterunderstandingofgaps
finetunedwithsyntheticdataconstructedbyboot-
incapabilitiesofLLMs,butalsotodevelopmore
strapping few-shot demonstrations. Belyi et al.
robust post-training techniques including RLAIF
(2024)proposedasimilarDeBERTa-basedpropri-
(reinforcement learning with AI feedback) (Lee
etary classifier model, Luna, finetuned on propri-
etal.,2024)whereinfactualresponsesarechosen
etary datasets across different industry segments.
over non-factual ones. As a result, the ability of
Minicheck-DBTA is also a DeBERTA–v3-Large
the AI judge to distinguish between factual and
finetunedforfact-checkingbytrainingonacombi-
non-factual responses by cross-checking against
nationofsyntheticandasubsetofadversarialNLI
retrieved documents is critical for this alignment
dataset(Tangetal.,2024b).
process (Tian et al., 2023; Lin et al., 2024). Our
Generativehallucinationevaluatorsincludethe
contributionsarethree-fold:
LLM-as-a-Judgemodels,thatarenotspecialized
forreliability,suchasJudgeLM(Zhuetal.,2023),
• We unify the hallucination detection prob- Prometheus(Kimetal.,2024),andothers(Zheng
lem and propose a multi-task setup that in- et al., 2023; Gao et al., 2023). Lynx (Ravi et al.,
cludesNLI,QA,anddialog. OurVERITAS 2024) is a hallucination detector LLM based on
judgeisstate-of-the-artonLLM-AggreFact, theLlama-3-Instructmodelsin8Band70B,fine-
Halubench,HalluDial,andtwoproprietaryen- tuned on 2400 training examples sampled from
terprisedatasets,reachingtheperformanceof FinanceBench (Islam et al., 2023), DROP (Dua
GPT4Turbo. et al., 2019), CovidQA (Möller et al., 2020) and
PubMedQA(Jinetal.,2019)whereinhalfofthose
• We curate VERITAS Collection, high qual- examples are perturbed to construct hallucinated
ity,diversetrainingdatasetcoveringdifferent answers.
formats for training hallucination detection Apartfromtheabovetwocategoriesofevalua-
models. tors,yetanotherlineofworkfocusesonheuristic-
based consistency checks (Agrawal et al., 2024;
Manakuletal.,2023;Guerreiroetal.,2023).
• WepresentVERITASBench,aunifiedbench-
mark for evaluating LLMs on reliability
Benchmarks and Metrics HaluEval (Li et al.,
in open-book settings across 18 different
2023) is a collection of 35k samples consisting
datasetscoveringthreedifferenttasks.
of QA and dialog tasks created using a two-step
processoffilteringandannotationsforhallucina-Document
The Sandlot is a 1993 American coming-of-age baseball film which tells
the story of a group of young baseball players during the summer of 1962.
Task 1 NLI Label 1
Claim/SUMMARY RATIONALE
It is not a story set in the winter of 1973. The claim is supported by the document since the
claim states that the story is not set in winter which
is correct according to the document which states
that story took place in summer.
Task 2 QA
Question: What is the time frame of the story in 
Sandlot film?
 Label 1
Answer: The story is not set in the winter of 1973. veritas
RATIONALE
The answer aligns with the document, as it correctly
states the story is not set in winter, but rather

in summer.
Task 3 Dialogue
User: I was rewatching The Sandlot and wondered .
—when is the story set? 
Assistant: It takes place in the summer of 1962, Label 1
which gives it that nostalgic feel. 
User: That fits! I didn’t remember any mention of RATIONALE
another time period, though.  The assistant's response is accurate, as it correctly
Assistant: Exactly, it's all about that 1960s summer clarifies that the story does not take place in
vibe. Definitely not set in a later time, like the winter, but in summer, which is consistent with
winter of 1973! the document.
Figure1: VERITASmodelsprovideaunifiedinterfaceforhallucinationdetectionusingamulti-taskapproach
comprising of three tasks. 1) NLI task in which the claim or the summary of the given document is checked/
verified 2) Grounded QA in which the answer is assessed for factuality 3) Grounded Dialogue in which the
assistantresponsesareverified. Inallthesetasks,evaluationisperformedbasedonthegivendocument. Label1
indicatesnohallucination(contentisfactuallyconsistent),whileLabel0denotesfactualinconsistencies.
tions. Ontheotherhand,HaluBench(Ravietal., vance. Other benchmarks for evaluating knowl-
2024) is a a QA hallucination evaluation bench- edge synthesis and factuality in LLMs include
markof15ksamplesthatconsistsofcontextalong the KILT (Petroni et al., 2021), TruthfulQA (Lin
with a question-answer pair and whether the an- et al., 2022), BEGIN (Dziri et al., 2022b) and
swercontainshallucinationornot. HalluDial(Luo FreshQA(Vuetal.,2023).
etal.,2024)isabenchmarkfocusedondialoghal-
3 VERITASDataCollection
lucinationdetectionwithabout4kdialogswithan
averageof4.5turns. Thebenchmarkcomprisesof
Addressingthelimitationsofcurrentopensource
conversationsacrosssevencategories: game,food,
models,ourprimarygoalistodesignamodelthat
music, culture, health, animal, and color. LLM-
is capable of flexibly handling various input for-
AggreFact(Tangetal.,2024b)isabenchmarkcon-
mats. To achieve this, we aim to curate a diverse
sistingoftenpubliclyavailabledatasetsannotated
trainingdatathatcomprisesoftasksliketextualen-
byhumansforfactcheckingconsistency.
tailment,summarization,questionansweringand
EXAMisametricthatestimatesnumberofques- groundeddialogueverification.
tionsansweredcorrectly,forasetofqueries,bya
3.1 TextualEntailment
QA system based on the LLM response (Sander
andDietz,2021). ARESproposesascoreforRAG For textual entailment, we primarily source data
system ranking based on its judge models using from the ANLI dataset (Nie et al., 2020), which
prediction-poweredinference(PPI)(Angelopoulos consists of adversarially constructed entailment
et al., 2023) that leverages the human annotated data points. Each data point includes a premise,
samplesforcomputingconfidenceintervals(Saad- a hypothesis, and a label indicating one of three
Falcon et al., 2024). RAGAS (Es et al., 2023) relationships: entailment, contradiction, or neu-
usedLLMstogeneratestatementsformaquestion- tral. Tofocusonmoreinformativesignals,weuse
answer pair computes three evaluation metrics, data from the second iteration of ANLI, exclud-
faithfulness, answer relevance, and context rele- ing neutral instances to emphasize clearer learn-ingpatterns. Additionally,weincorporateexisting Format Train Dev
datasets for hallucination detection, particularly
Minicheck (Tang et al., 2024a), which provides En Esp En Esp
document-claim pairs with a focus on sentence- NLI 50967 30897 3986 542
Summarization 17056 - 3416 -
level claims. Both datasets are reformatted into
QA 26433 3807 1739 596
a unified structure of (document, claim, label) to Dialogue 25878 2432 1666 279
ensureconsistency. Total 120334 37136 10807 1417
3.2 GroundedQuestionAnswering Table1: VERITASTrainingcollection: Overviewof
curateddatasetwithcountsofdifferentsplits,andsub-
We repurpose existing reading comprehension
splits.
datasets consisting of passage, question, and an-
swertuples,eachemphasizingdifferentaspectsof
evaluation. domains and styles. The datasets are selected to
representdifferenttypesofsourcedocuments.
• DROP (Dua et al., 2019): This dataset con-
The different datasets used are: BillSum (Ko-
tainspassages,questions,andanswers,where
rnilova and Eidelman, 2019) containing US
verifying the correctness of an answer often
congressional and California state bills and
requires complex numerical reasoning. The
their summaries; SAMSum (Gliwa et al., 2019)
taskinvolvesextractingnumericalfactsfrom
comprising messenger-like conversations with
various parts of the passage and performing
abstractive summaries; BigPatent (Sharma et al.,
operations like addition, counting, and sort-
2019)with1.3millionU.S.patentdocumentsand
ing.
theirhuman-writtensummaries;andMulti-News
• NewsQA (Trischler et al., 2016): Built on (Fabbrietal.,2019),whichfeatureshumanwritten
CNNnewsarticles,thismachinecomprehen- summariesofnewsarticlesfromnewser.com.
sion dataset provides question-answer pairs. We sample equally from these datasets. forming
It poses challenges as it requires reasoning 12k datapoints to form our summarization task.
beyondsimplewordmatchingorentailment. Additionally, we augment the summarization
dataset with negative examples by generating
• TriviaQA (Joshi et al., 2017): This dataset
factually unsupported summaries, as all original
consistsofquestion-answerpairsgroundedin
summariesinthesedatasetsareinherentlyfactually
Wikipedia and web articles written by trivia
supported.
enthusiasts. Wespecificallyconsidersamples
groundedinwebarticles,astheirunstructured
Additionally, to bring the same domain di-
naturehelpsindetectinghallucinationsinreal-
versitytotheNLItaskandtomaximizetheutility
worldsettings.
ofthisdata,wealsoreformulatethesummarization
instances as NLI tasks by treating summaries as
• SearchQA (Dunn et al., 2017): Designed
claims,therebyenrichingourNLItask.
to simulate an end-to-end QA system, this
dataset consists of question-answer pairs
3.3.1 Generationofhallucinationsamples
groundedinnoisydocuments,whichmayin-
Since the aforementioned datasets are primarily
cludeirrelevantinformation. SimilartoTrivi-
question-answerdatasets,theydonotnaturallyin-
aQA, this setup closely resembles a typical
clude answers that are unfaithful or incorrect rel-
retrieval-augmented generation (RAG) sys-
ative to the context. To address this, we gener-
tem.
ate incorrect answers using Llama 70B Instruct,
Additionally,wecurateavalidationsplitconsist- promptingittoproduceunfaithfulanswersbased
ing of samples from DROP, TextBookQA (Kem- onthegivencontext.
bhavi et al., 2017), and RACE (Lai et al., 2017), Wegeneratediversehallucinationtypesfollow-
ensuringevaluationonoutofdomaindatasets. ingthetaxonomyproposedbyMishraetal.(2024),
whichincludes: Entityerrors,whereanincorrect
3.3 Summarization
entityaltersthefactualityofastatement;Relation
For the summarization component, we curate a errors,involvingincorrectsemanticrelationships
diverse collection of datasets that span multiple likeverbsorprepositions;Sentenceerrors,wherethe entire statement contradicts the evidence; In- 4 VERITAS
vented errors, containing fabricated information
Wefine-tunetwoclassesoftransformermodelsby
notfoundinthecontext;Subjectiveerrors,based
leveragingVERITASData: encoder-basedclassi-
onpersonalopinionsratherthanfacts;andUnveri-
fieranddecoder-basedgenerativemodels.
fiableerrors,wheretheanswercannotbevalidated
by the given evidence. This variety ensures we
4.1 Classifiermodels
generatediversefactualerrorsforeffectivehalluci-
nationdetection. Fortheclassifiermodel,wefine-tuneDeBERT-v3-
large(Heetal.,2023)asthisbackbonehasdemon-
strated strong performance in claim verification
3.4 GroundedDialogueVerification
tasksinpreviousworks(Tangetal.,2024a;Belyi
Totrainmodelsthatcandetectfactualerrorsincon- etal.,2024). Tounifytheinputtextformat,wecon-
versational settings, we aim to include grounded verteachdatapointinto(document,conversation)
dialogue verification task. In this task, there is structureapplicableforallformatsincludingNLI,
aconversationbetweenuserandassistant, where QAanddialogue. ForQA,theclaimisformatted
theresponsesbyassistantareevaluatedagainsta asasingle-turndialoguebetweentheuserandthe
reference document (e.g., RAG context). Since assistant. InthecaseofNLI,theclaimispresented
no publicly available datasets contain document- astheassistantresponse. PleaserefertoAppendix
conversation-labeltriplesforthispurpose,wegen- Aforexactinputformatting. Weusethestandard
erate this dataset synthetically. To achieve this, cross-entropy loss for training and fine-tune the
weconvertthecuratedQAsamplesintodialogue modelfor2epochswithaconstantlearningrateof
by prompting Llama 70B Instruct to transform 1e−6,awarmupratio0.1,batchsizeof4,weight
question-answerpairsintomulti-turnconversations decayof0.001. Hereafter,werefertothisresulting
betweenauserandanassistant,ensuringthatthe classifierasVERITASDeBERTa.
factualinformationintheanswersispreserved.
4.2 Generativemodels
3.5 Spanishdata Next,weconsideredgenerativetransformermod-
els,astheyareincreasinglyappliedtotaskssuchas
WetranslateddatafromEnglishtoSpanishtoim-
LLM-as-judges,includinghallucinationdetection.
provethejudge’sproficiencyinSpanish. Thistrans-
Forthis, wefine-tunevariantsofLLaMA3.23B
lationprocessincludeddatapointsinNLI,QA,and
and LLaMA 3.1 8B Instruct models. Given the
Dialogue formats, with 37k for training and 1.5k
strong instruction-following capabilities of these
for validation. The LLama3.1-8B Instruct model
models, we adopt a multi-task setup without the
wasusedfortranslatingthesedatapoints.
needforstrictunifiedformatting. Instead,weuse
task-specificinstructiontemplates,tailoredforeach
3.6 RationaleGenerationandDataCleansing
taskwhilemaintainingconsistencyacrossthetem-
Inordertotrainmodelsthatcanlearnfromreason- plates by varying only the entity being assessed.
ingpaths,wegeneraterationalesforthecollected This flexibility ensures that the models can han-
dataset. Theserationalesprovideaclearexplana- dle different types of inputs effectively without
tionofthelogicbehindeachresponse,allowingthe compromisingonperformance. Fordetailedinput
modeltounderstandnotonlythecorrectlabelbut formattingandinstructiontemplates,pleaserefer
alsothereasoningthatsupportsit. Beyondsimple toAppendixB.
explanations,rationalesactascriticallearningsig- Wefine-tunethesemodelsusingteacherforcing,
nalsforthemodels. Togeneratetheserationales, where the models generate JSON-formatted out-
wepromptGPT-4otooutputboththerationaleand putsthatincludeboththerationaleandthelabelfor
the label for each data point multiple times. If theentiretrainingsplitofVERITASData,whose
theoutputlabelsconsistentlycontradicttheground train and dev compositions are shown in Table 1.
truthlabel,wediscardthedatapointduetothisin- Eachexampleisstructuredinachat-basedformat,
consistency. Whenthelabelsalignwiththeground with the input appearing as a user query and the
truth,weretaintherationalewhichcorresponding correspondinggroundtruthlabelandrationalein
to the correct prediction. This approach helps to theassistant’sresponse. Toreducecomputational
ensurehigh-qualityrationalesandaccuratelabels. costs, we adopt QLoRA (Dettmers et al., 2024),Format Dataset Count Question Answering For the question-
NLI LLMAggreFact 29320 answering format of evaluation, we include two
HaluBenchPubMedQA 1000 key benchmarks: HaluEval (Li et al., 2023) and
HaluBenchFinanceBench 1000 HaluBench (Ravi et al., 2024). In HaluEval, we
QA
HaluEvalQA 20000
focus solely on the QA-type samples. Similarly
HaluEvalDialog 20000
Dialogue for HaluBench, we incorporate datasets such as
HalluDial 10000
PubMedQA(Jinetal.,2019),FinanceBench(Islam
RealEstateQA 108
Enterprise etal.,2023),whichprovidedomain-specificchal-
PersonaChatbotDialog 92
lengesforQA.Notably, weexcludeanysamples
Total - 81520
from the DROP dataset, as some of VERITAS
Table2: VERITASBenchComprehensivebenchmark training data is derived from it. Additionally, we
forhallucinationdetectionconsistingofthreedifferent excludesplitsofRAGTruth (Wuetal.,2023)and
evaluationformats
HaluEvalareexcludedastheyarealreadyincluded
inunderothersplits.
fine-tuning the models in a 4-bit format. We set
Grounded Dialogue Verification We consider
thelow-rankadapterrankto64,specificallytarget-
thedialoguesplitfromHaluEvalandincludeHal-
ingkeytransformercomponentssuchasprojection
luDial(Luoetal.,2024),oneofthemostextensive
layers,embeddings,andthefinallanguagemodel
benchmarksforhallucinationdetectionindialogue
layers. Trainingisperformedwithan8192-token
contexts. ItistobenotedthattestsplitofHalluDial
sequencelengthandalearningrateof5e−6,along
isnotpubliclyavailable,soweuserandomsubset
withawarm-upratioof0.1,forasingleepoch. We
containing10ksamplesfromtrainsplit.
applygradientaccumulationovertwostepswitha
batchsizeof2. Theresultingmodelsarereferred Enterprise Additionally,weincludetwopropri-
toasVERITAS3BandVERITAS8B. etaryenterprisedatasetsfromdomainsofpersona
chatbots and real estate QA system covering dia-
5 VERITASBench
logueandQAformatsrespectively.
Existinghallucinationbenchmarkshaveprimarily
6 Results
focusedonspecifictypesofevaluation. Forexam-
ple,LLMAggreFact(Tangetal.,2024a)centersex-
We benchmarked the VERITAS models on the
clusivelyonclaimverification,whilebenchmarks
VERITASBenchbycomparingthemagainstsev-
likeHaluBench(Ravietal.,2024)targetquestion-
eral baselines, including current state-of-the-art
answering(QA)tasks. Althoughsomebenchmarks
classifier and generative models. In the genera-
forgroundeddialogueverificationexist(Luoetal.,
tivemodelscategory,weselectedGPT-4Turboas
2024),theyarerarelyutilizedinthedevelopment
the closed-source baseline due to its competitive
offact-checkingmodels. Thisgapmotivatedusto
performanceacrossvariousLLM-as-judgebench-
design a comprehensive benchmark that incorpo-
marks. Additionally,weincludedLynx8B(Ravi
ratesthreeevaluationformats: claimverification,
et al., 2024) and Bespoke MiniCheck 7B (Tang
questionanswering,anddialogueverification,pro-
et al., 2024a), both of which have shown impres-
vidingamoreholisticassessmentofmodelperfor-
sive results on QA and NLI evaluations respec-
mance. Theexactcompositionofdifferenttasksin
tively. Forclassifiermodels,weprimarilyconsid-
VERITASBenchisdepictedinTable2.
ered Minicheck DeBERTa, known for its strong
Claim Verification For the claim verification performance in claim verification tasks. As eval-
task, we leverage LLMAggreFact (Tang et al., uationmetrics,weusebalancedaccuracyforLL-
2024a), as it contains a wide array of subtasks MAggreFactandforallothers,wereportaccuracy
within. Itprovidesadiversesetofclaimsthatcover asthemainevaluationmetric.
multiplescenarios,includingclaimsderivedfrom
6.1 MainResults
model-generated summaries, LLM responses to
searchqueriesandclaimssourcedfromWikipedia. VERITASmodelsgeneralizeacrossallformats
Thisdiversityensuresacomprehensiveevaluation Table 3 presents the key results from VERITAS
ofmodelsforclaimverificationacrossvariouscon- Bench. MinicheckDeBERTaperformswellinNLI
textsandcontenttypes. butfallsbehindinQAanddialoguetasks,primarilyModel NLI QA Dialog Average
LLMAggreFact PubMedQA FinanceBench HaluEvalQA HalluDial HaluEvalDialog
GPT4Turbo 76.1 86.8 73.9 85.6 76.0 69.2 77.9
Classifiers
MinicheckDeBERTa(440M) 73.1 59.7 50.0 63.8 52.0 50.5 58.2
VERITASDeBERTa(440M) 73.0 75.5 56.7 84.6 73.2 65.9 71.5
Generative
Lynx8B 69.3 88.6 75.4 87.2 66.7 68.2 75.9
BespokeMiniCheck7B 77.4 75.2 58.1 87.2 64.6 55.3 69.6
VERITAS3B 73.2 79.9 65.0 83.7 74.0 67.7 73.9
VERITAS8B 74.0 83.9 68.2 84.9 73.6 74.5 76.5
Table3: VERITASBenchResults: MainresultscomparingVERITASmodelstoexistingopenandclosed-access
modelsacrossthethreetaskformats,NLI,QA,anddialog. WhileLynx,BespokeMiniCheck7BexcelatQAand
NLItasksrespectively,theydonotgeneralizeonconversationalsettings. Ontheotherhand,VERITASmodels
performwellonallformats.
Model LLMAggreFact
AggreFact-CNN AggreFact-XSum TofuEval-MediaS TofuEval-MeetB Wice Reveal ClaimVerify FactCheck-GPT ExpertQA Lfqa RAGTruth Average
GPT4Turbo 65.3 73.7 72.4 82.0 76.8 88.7 69.3 80.0 60.6 83.3 85.5 76.1
Classifiers
MinicheckDeBERTa 64.2 71.0 69.3 72.7 69.4 87.3 75.6 73.0 58.9 83.9 78.8 73.1
VERITASDeBERTa 54.6 73.5 72.7 77.8 74.8 84.2 70.7 74.3 59.2 85.9 75.5 73.0
Generative
Lynx8B 59.3 64.7 70.7 77.6 65.4 74.7 69.7 67.9 58.6 77.9 76.1 69.3
BespokeMiniCheck7B 65.5 77.8 76.0 78.3 83.0 88.0 75.3 77.7 59.2 86.7 84.0 77.4
VERITAS3B 52.3 71.7 72.3 74.8 72.5 88.1 74.7 78.3 59.4 86.5 75.0 73.2
VERITAS8B 53.9 71.8 74.0 78.3 74.9 87.8 71.5 79.3 60.0 83.7 79.3 74.0
Table4: LLMAggreFact: ResultscomparingVERITASmodelstoexistingopenandclosed-accessmodelson
subsplitsofLLMAggreFact
Model RealEstate(QA) PersonaChatbots(Dialog) Average
English Spanish
GPT4Turbo 66.1 56.1 90.2 70.8
Baselines
Lynx8B 64.7 55.6 80.4 66.9
BespokeMiniCheck7B 65.7 50.5 55.4 57.2
MinicheckDeBERTa 73.8 57.0 62.0 64.3
VERITASDeBERTa 77.4 70.8 91.3 79.8
Veritas3B 72.7 73.7 90.2 78.9
Veritas8B 83.2 72.2 90.2 81.9
Table5: ResultscomparingVeritasmodelstoexistingopenandclosed-accessmodelsonEnterpriseproprietary
datasetsinthedomainsofpersonachatbotsandrealestate
becauseitisexclusivelytrainedonclaimverifica- isnotpubliclyknown. OurVERITAS8B,trained
tion,makingitlesseffectiveforhandlingQAand onamorediversedataset,performscompetitively,
conversation-basedinputs. Incontrast,VERITAS nearlyclosingthegapwithGPT-4.
DeBERTa,trainedusingamulti-taskapproach,sur-
passesMinicheckinbothQAanddialogueformats. Encoder models are natural fact checkers
Amonggenerativemodels,Lynx8BandBespoke DeBERTa-basedmodels,despitehavingrelatively
MiniCheck 7B excel in QA but struggle with di- fewer parameters (440M), perform exceptionally
aloguetasks. Lynx’sstrongQAperformancecan wellonbenchmarkssuchasLLMAggreFact(see
beattributedtoitstrainingdata,whichispartially Table 4). Interestingly, generative models such
sourcedfromPubMedQA.Similarly,trainingdata as LLama 3B, even when trained with reasoning
of Bespoke MiniCheck 7B largely involves NLI traces,couldnotmatchtheperformanceofVERI-
samples,alongsideproprietarydatawhoseformat TASDeBERTa. Therecouldbeseveralreasonsfor
this. First, encoder models excel in entailmentbased tasks, making them naturally well-suited extensivereports,afewoftheminLLMAggreFact
forclaimverification. Second,traininggenerative benchmark. Currently,wemitigatethisbysplitting
models to predict a final label is inherently chal- documents into smaller chunks and aggregating
lenging. While rationales are helpful, the actual theresults,followingpriorapproaches(Tangetal.,
labelappearsattheend,whichmightleadtofocus 2024a). However,thissolutionissuboptimal.
moreonmimickingthestyleofrationalesinstead
Backbones Thebackbonearchitecturesusedin
oflearningthecorrectoutput. Apotentialsolution
VERITAS, such as LLama and DeBERTa, may
to this issue could be augmenting the data with
not be fully optimized for the specialized task of
examplesthatomitrationales(Wangetal.,2024),
reliabilityassessment. Exploringalternativearchi-
allowingthemodeltobetterlearnhowtoproduce
tectures,likeFlan-T5,couldleadtoperformance
accuratelabels.
gains(Tangetal.,2024a),especiallyinourmulti-
On enterprise data As show in Table 5, the tasksetups. Themulti-taskinstructionfine-tuning
VERITAS models, particularly VERITAS 8B, ofFlan-T5(Chungetal.,2024)makesitapromis-
demonstrateexceptionalperformanceacrossboth ingcandidateforfuturework.
realestateQAandpersonachatbotdialoguetasks.
SimilarlyVERITAS3Balsoperformsstronglysur-
References
passingmuchlargermodelssuchasLynx8Band
Minicheck7B.Bothmodelssurpassallbaselines Ayush Agrawal, Mirac Suzgun, Lester Mackey, and
andnotablyVERITAS8BoutperformsevenGPT- AdamKalai.2024. Dolanguagemodelsknowwhen
4Turbohighlightingitseffectivenessinhandling they’rehallucinatingreferences? InFindingsofthe
Association for Computational Linguistics: EACL
multi-lingualanddiverseformats,showcasingits
2024,pages912–928,St.Julian’s,Malta.Association
adaptabilityforentreprise-levelapplications.
forComputationalLinguistics.
7 Conclusion Anastasios N. Angelopoulos, Stephen Bates, Clara
Fannjiang, Michael I. Jordan, and Tijana Zrnic.
VERITASisaunifiedapproachforjudgingrelia- 2023. Prediction-powered inference. Preprint,
bilityofLLMsthatemploysamulti-tasktraining arXiv:2301.09633.
setupacrossNLI,QA,anddialog. Ourapproach
MashaBelyi,RobertFriel,ShuaiShao,andAtindriyo
outperforms existing open-access models while
Sanyal.2024. Luna:Anevaluationfoundationmodel
maintaining competitive performance to GPT4 to catch language model hallucinations with high
turbowithblazingfastinference(∼100millisec- accuracyandlowcost. Preprint,arXiv:2406.00975.
ondslatency)andlowcosts. Thegenerativejudges
TomB.Brown,BenjaminMann,NickRyder,Melanie
canlearnandself-improvefromrationalewithout Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
anyadditionaltrainingdata. Neelakantan,PranavShyam,GirishSastry,Amanda
VERITASbenchisaunifiedbenchmarkforeval- Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
uating hallucination across 18 different datasets
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
fromdifferentdomains. OurresultsontheVERI-
Clemens Winter, Christopher Hesse, Mark Chen,
TASbenchmarkconfirmthatencodermodelssuch EricSigler,MateuszLitwin,ScottGray,Benjamin
asDeBERTaarenaturalfactcheckersandperform Chess, Jack Clark, Christopher Berner, Sam Mc-
Candlish, AlecRadford, IlyaSutskever, andDario
competitivelywithmodelsthatare16xtheirsize.
Amodei.2020. Languagemodelsarefew-shotlearn-
The VERITAS collection and benchmark lay the
ers. Preprint,arXiv:2005.14165.
groundworkforrobustpost-trainingtechniquesof
LLMsthatrelyonAIfeedbackforaligningLLMs HyungWonChung,LeHou,ShayneLongpre,Barret
Zoph,YiTay,WilliamFedus,YunxuanLi,Xuezhi
tomakethemmoretruthful.
Wang,MostafaDehghani,SiddharthaBrahma,etal.
2024. Scalinginstruction-finetunedlanguagemodels.
Limitations
JournalofMachineLearningResearch,25(70):1–53.
DocumentLength Onekeylimitationofourap-
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and
proach is the document length constraint in gen- LukeZettlemoyer.2024. Qlora: Efficientfinetuning
erative models. Despite training with a sequence ofquantizedllms. AdvancesinNeuralInformation
ProcessingSystems,36.
lengthof8192tokens,handlingmuchlongerdoc-
uments remain a challenge. This may limit per-
DheeruDua,YizhongWang,PradeepDasigi,Gabriel
formanceontasksrequiringfull-lengtharticlesor Stanovsky,SameerSingh,andMattGardner.2019.Drop: Areadingcomprehensionbenchmarkrequir- Pranab Islam, Anand Kannappan, Douwe Kiela, Re-
ing discrete reasoning over paragraphs. Preprint, beccaQian,NinoScherrer,andBertieVidgen.2023.
arXiv:1903.00161. Financebench: Anewbenchmarkforfinancialques-
tionanswering. Preprint,arXiv:2311.11944.
MatthewDunn,LeventSagun,MikeHiggins,VUgur
Guney, Volkan Cirik, and Kyunghyun Cho. 2017. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas
Searchqa: A new q&a dataset augmented with Hosseini,FabioPetroni,TimoSchick,JaneDwivedi-
context from a search engine. arXiv preprint Yu,ArmandJoulin,SebastianRiedel,andEdouard
arXiv:1704.05179. Grave.2024. Atlas: few-shotlearningwithretrieval
augmentedlanguagemodels. J.Mach.Learn.Res.,
NouhaDziri,SivanMilton,MoYu,OsmarZaiane,and 24(1).
SivaReddy.2022a. Ontheoriginofhallucinations
in conversational models: Is it the datasets or the QiaoJin,BhuwanDhingra,ZhengpingLiu,WilliamW.
models? InProceedingsofthe2022Conferenceof Cohen, and Xinghua Lu. 2019. Pubmedqa: A
theNorthAmericanChapteroftheAssociationfor datasetforbiomedicalresearchquestionanswering.
ComputationalLinguistics: HumanLanguageTech- Preprint,arXiv:1909.06146.
nologies, pages 5271–5285, Seattle, United States.
AssociationforComputationalLinguistics. MandarJoshi,EunsolChoi,DanielSWeld,andLuke
Zettlemoyer.2017. Triviaqa: Alargescaledistantly
NouhaDziri,HannahRashkin,TalLinzen,andDavid supervisedchallengedatasetforreadingcomprehen-
Reitter.2022b. EvaluatingAttributioninDialogue sion. arXivpreprintarXiv:1705.03551.
Systems: The BEGIN Benchmark. Transactions
of the Association for Computational Linguistics, SauravKadavath,TomConerly,AmandaAskell,Tom
10:1066–1083. Henighan, Dawn Drain, Ethan Perez, Nicholas
Schiefer,ZacHatfield-Dodds,NovaDasSarma,Eli
Shahul Es, Jithin James, Luis Espinosa-Anke, and
Tran-Johnson, Scott Johnston, Sheer El-Showk,
Steven Schockaert. 2023. Ragas: Automated eval-
Andy Jones, Nelson Elhage, Tristan Hume, Anna
uationofretrievalaugmentedgeneration. Preprint,
Chen, Yuntao Bai, Sam Bowman, Stanislav Fort,
arXiv:2309.15217.
Deep Ganguli, Danny Hernandez, Josh Jacobson,
JacksonKernion,ShaunaKravec,LianeLovitt,Ka-
AlexanderR.Fabbri,IreneLi,TianweiShe,SuyiLi,and
malNdousse,CatherineOlsson,SamRinger,Dario
DragomirR.Radev.2019. Multi-news: alarge-scale
Amodei,TomBrown,JackClark,NicholasJoseph,
multi-documentsummarizationdatasetandabstrac-
BenMann,SamMcCandlish,ChrisOlah,andJared
tivehierarchicalmodel. CoRR,abs/1906.01749.
Kaplan.2022. Languagemodels(mostly)knowwhat
theyknow. Preprint,arXiv:2207.05221.
Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony
Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vin-
AniruddhaKembhavi,MinjoonSeo,DustinSchwenk,
centY.Zhao,NiLao,HongraeLee,Da-ChengJuan,
Jonghyun Choi, Ali Farhadi, and Hannaneh Ha-
and Kelvin Guu. 2023. Rarr: Researching and re-
jishirzi.2017. Areyousmarterthanasixthgrader?
vising what language models say, using language
textbookquestionansweringformultimodalmachine
models. Preprint,arXiv:2210.08726.
comprehension. 2017 IEEE Conference on Com-
puterVisionandPatternRecognition(CVPR),pages
ZorikGekhman,GalYona,RoeeAharoni,MatanEyal,
5376–5384.
AmirFeder,RoiReichart,andJonathanHerzig.2024.
Doesfine-tuningllmsonnewknowledgeencourage
Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang,
hallucinations? Preprint,arXiv:2405.05904.
Shayne Longpre, Hwaran Lee, Sangdoo Yun,
BogdanGliwa,IwonaMochol,MaciejBiesek,andAlek- SeongjinShin,SungdongKim,JamesThorne,and
sander Wawer. 2019. Samsum corpus: A human- Minjoon Seo. 2024. Prometheus: Inducing fine-
annotated dialogue dataset for abstractive summa- grained evaluation capability in language models.
rization. CoRR,abs/1911.12237. Preprint,arXiv:2310.08491.
NunoM.Guerreiro,ElenaVoita,andAndréF.T.Mar- AnastassiaKornilovaandVladEidelman.2019. Bill-
tins. 2023. Looking for a needle in a haystack: A sum: AcorpusforautomaticsummarizationofUS
comprehensivestudyofhallucinationsinneuralma- legislation. CoRR,abs/1910.00523.
chinetranslation. Preprint,arXiv:2208.05309.
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,
KelvinGuu,KentonLee,ZoraTung,PanupongPasu- andEduardHovy.2017. Race: Large-scalereading
pat,andMing-WeiChang.2020. Realm: Retrieval- comprehension dataset from examinations. arXiv
augmented language model pre-training. Preprint, preprintarXiv:1704.04683.
arXiv:2002.08909.
Harrison Lee, Samrat Phatale, Hassan Mansoor, Kel-
PengchengHe,JianfengGao,andWeizhuChen.2023. lieRenLu,ThomasMesnard,JohanFerret,Colton
Debertav3:Improvingdebertausingelectra-stylepre- Bishop, Ethan Hall, Victor Carbune, and Abhinav
trainingwithgradient-disentangledembeddingshar- Rastogi.2024. RLAIF:Scalingreinforcementlearn-
ing. Preprint,arXiv:2111.09543. ingfromhumanfeedbackwithAIfeedback.PatrickLewis,EthanPerez,AleksandraPiktus,Fabio YixinNie,AdinaWilliams,EmilyDinan,MohitBansal,
Petroni,VladimirKarpukhin,NamanGoyal,Hein- JasonWeston,andDouweKiela.2020. Adversarial
richKüttler, MikeLewis, Wen-tauYih, TimRock- NLI:Anewbenchmarkfornaturallanguageunder-
täschel, Sebastian Riedel, and Douwe Kiela. 2020. standing. InProceedingsofthe58thAnnualMeet-
Retrieval-augmented generation for knowledge- ingoftheAssociationforComputationalLinguistics,
intensivenlptasks. InProceedingsofthe34thInter- pages4885–4901,Online.AssociationforComputa-
nationalConferenceonNeuralInformationProcess- tionalLinguistics.
ingSystems,NIPS’20,RedHook,NY,USA.Curran
AssociatesInc. FabioPetroni,AleksandraPiktus,AngelaFan,Patrick
Lewis,MajidYazdani,NicolaDeCao,JamesThorne,
JunyiLi,XiaoxueCheng,XinZhao,Jian-YunNie,and YacineJernite,VladimirKarpukhin,JeanMaillard,
Ji-Rong Wen. 2023. HaluEval: A large-scale hal- VassilisPlachouras,TimRocktäschel,andSebastian
lucinationevaluationbenchmarkforlargelanguage Riedel. 2021. KILT: a benchmark for knowledge
models. InProceedingsofthe2023Conferenceon intensivelanguagetasks. InProceedingsofthe2021
EmpiricalMethodsinNaturalLanguageProcessing, Conference of the North American Chapter of the
pages6449–6464,Singapore.AssociationforCom- AssociationforComputationalLinguistics: Human
putationalLinguistics. LanguageTechnologies,pages2523–2544,Online.
AssociationforComputationalLinguistics.
Sheng-Chieh Lin, Luyu Gao, Barlas Oguz, Wenhan
Xiong, Jimmy Lin, Wen tau Yih, and Xilun Chen. SelvanSunithaRavi,BartoszMielczarek,AnandKan-
2024. Flame: Factuality-awarealignmentforlarge nappan, Douwe Kiela, and Rebecca Qian. 2024.
languagemodels. Preprint,arXiv:2405.01525. Lynx: An open source hallucination evaluation
model. Preprint,arXiv:2407.08488.
StephanieLin,JacobHilton,andOwainEvans.2022.
TruthfulQA:Measuringhowmodelsmimichuman JonSaad-Falcon,OmarKhattab,ChristopherPotts,and
falsehoods. InProceedingsofthe60thAnnualMeet- Matei Zaharia. 2024. Ares: An automated evalua-
ingoftheAssociationforComputationalLinguistics tionframeworkforretrieval-augmentedgeneration
(Volume1: LongPapers),pages3214–3252,Dublin, systems. Preprint,arXiv:2311.09476.
Ireland.AssociationforComputationalLinguistics.
DavidPSanderandLauraDietz.2021. Exam: Howto
evaluateretrieve-and-generatesystemsforuserswho
Wen Luo, Tianshu Shen, Wei Li, Guangyue Peng,
donot(yet)knowwhattheywant.
RichengXuan,HoufengWang,andXiYang.2024.
Halludial: A large-scale benchmark for automatic
Eva Sharma, Chen Li, and Lu Wang. 2019. BIG-
dialogue-level hallucination evaluation. Preprint,
PATENT: A large-scale dataset for abstractive and
arXiv:2406.07070.
coherentsummarization. CoRR,abs/1906.03741.
VarunMagesh,FaizSurani,MatthewDahl,MiracSuz-
KurtShuster,SpencerPoff,MoyaChen,DouweKiela,
gun, Christopher D. Manning, and Daniel E. Ho.
and Jason Weston. 2021. Retrieval augmentation
2024. Hallucination-free? assessing the reliabil-
reduces hallucination in conversation. In Findings
ity of leading ai legal research tools. Preprint,
of the Association for Computational Linguistics:
arXiv:2405.20362.
EMNLP 2021, pages 3784–3803, Punta Cana, Do-
minican Republic. Association for Computational
PotsaweeManakul,AdianLiusie,andMarkJ.F.Gales.
Linguistics.
2023. Selfcheckgpt: Zero-resource black-box hal-
lucination detection for generative large language
LiyanTang,PhilippeLaban,andGregDurrett.2024a.
models. Preprint,arXiv:2303.08896.
Minicheck:Efficientfact-checkingofllmsonground-
ingdocuments. arXivpreprintarXiv:2404.10774.
Nick McKenna, Tianyi Li, Liang Cheng, Moham-
madJavadHosseini,MarkJohnson,andMarkSteed-
LiyanTang,PhilippeLaban,andGregDurrett.2024b.
man. 2023. Sources of hallucination by large lan-
Minicheck:Efficientfact-checkingofllmsonground-
guagemodelsoninferencetasks. InThe2023Con-
ingdocuments. Preprint,arXiv:2404.10774.
ferenceonEmpiricalMethodsinNaturalLanguage
Processing. Katherine Tian, Eric Mitchell, Huaxiu Yao, Christo-
pher D. Manning, and Chelsea Finn. 2023. Fine-
Abhika Mishra, Akari Asai, Vidhisha Balachandran, tuning language models for factuality. Preprint,
YizhongWang,GrahamNeubig,YuliaTsvetkov,and arXiv:2311.08401.
Hannaneh Hajishirzi. 2024. Fine-grained halluci-
nation detection and editing for language models. AdamTrischler,TongWang,XingdiYuan,JustinHarris,
Preprint,arXiv:2401.06855. Alessandro Sordoni, Philip Bachman, and Kaheer
Suleman.2016. Newsqa: Amachinecomprehension
Timo Möller, Anthony Reina, Raghavan Jayakumar, dataset. arXivpreprintarXiv:1611.09830.
andMaltePietsch.2020. COVID-QA:Aquestion
answeringdatasetforCOVID-19. InProceedingsof TuVu,MohitIyyer,XuezhiWang,NoahConstant,Jerry
the1stWorkshoponNLPforCOVID-19atACL2020, Wei,JasonWei,ChrisTar,Yun-HsuanSung,Denny
Online.AssociationforComputationalLinguistics. Zhou,QuocLe,andThangLuong.2023. Freshllms:Refreshinglargelanguagemodelswithsearchengine
augmentation. Preprint,arXiv:2310.03214.
Tu Vu, Kalpesh Krishna, Salaheddin Alzubi, Chris
Tar, Manaal Faruqui, and Yun-Hsuan Sung. 2024.
Foundational autoraters: Taming large language
models for better automatic evaluation. Preprint,
arXiv:2407.10817.
PeifengWang,AustinXu,YilunZhou,CaimingXiong,
andShafiqJoty.2024. Directjudgementpreference
optimization. arXivpreprintarXiv:2409.14664.
Yuanhao Wu, Juno Zhu, Siliang Xu, Kashun Shum,
ChengNiu,RandyZhong,JuntongSong,andTong
Zhang.2023. Ragtruth: Ahallucinationcorpusfor
developingtrustworthyretrieval-augmentedlanguage
models. arXivpreprintarXiv:2401.00396.
ZiweiXu,SanjayJain,andMohanKankanhalli.2024.
Hallucinationisinevitable: Aninnatelimitationof
largelanguagemodels. Preprint,arXiv:2401.11817.
LianminZheng,Wei-LinChiang,YingSheng,Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
ZhuohanLi,DachengLi,EricP.Xing,HaoZhang,
Joseph E. Gonzalez, and Ion Stoica. 2023. Judg-
ingllm-as-a-judgewithmt-benchandchatbotarena.
Preprint,arXiv:2306.05685.
Lianghui Zhu, Xinggang Wang, and Xinlong Wang.
2023. Judgelm: Fine-tunedlargelanguagemodels
arescalablejudges. Preprint,arXiv:2310.17631.A VERITASDeBERTaInputFormat
DeBERTaInputFormatForNLI
# Context:
{{ document }}
# Claim:
assistant: {{ claim }}
DeBERTaInputFormatForQA
# Context:
{{ document }}
# Claim:
user: {{ question }}
assistant: {{ answer }}
DeBERTaInputFormatForDialogue
# Context:
{{ document }}
# Claim:
{% for message in conversation %}
{{ message.role }}: {{ message.content }}
{% endfor %}
DeBERTaInputFormatForSummaryVerification
# Context:
{{ document }}
# Summary:
assistant: {{ summary }}B VERITASLLamaInstructionFormat
LLamaInputFormatForNLI
You will classify whether the claim is supported by the given document or not.
Follow these steps:\n\n
1. Assess the claim against the document \n
2. Classify it is 0 (not supported) or 1 (supported) \n
3. Provide Rationale: Explain your classification decision with a brief
rationale.\n
4. Finally, output the results as a JSON object with the fields \"rationale\"
and \"output\" where \"output\" contains the classification (0 or 1)
# Document:\n
{{ document }}\n
# Claim:\n
{{ claim }}\n
Now, please output the following as a JSON object:\n
{
"rationale": <verbal feedback> (str datatype),\n
"output": <classification score (0 or 1)> (int datatype),\n
}
LLamaInputFormatForQA
You will classify whether the answer is supported by the given document or
not.
Follow these steps:\n\n
1. Assess the answer against the document \n
2. Classify it is 0 (not supported) or 1 (supported) \n
3. Provide Rationale: Explain your classification decision with a brief
rationale.\n
4. Finally, output the results as a JSON object with the fields \"rationale\"
and \"output\" where \"output\" contains the classification (0 or 1)
# Document:\n
{{ document }}\n
# Question:\n
{{ question }}\n
# Answer:\n
{{ answer }}\n
Now, please output the following as a JSON object:\n
{
"rationale": <verbal feedback> (str datatype),\n
"output": <classification score (0 or 1)> (int datatype),\n
}LLamaInputFormatForDialogue
You will classify whether the last assistant response in the conversation is
supported by the given document or not.
Follow these steps:\n\n
1. Assess the last assistant response in the conversation against the
document \n
2. Classify it is 0 (not supported) or 1 (supported) \n
3. Provide Rationale: Explain your classification decision with a brief
rationale.\n
4. Finally, output the results as a JSON object with the fields \"rationale\"
and \"output\" where \"output\" contains the classification (0 or 1)
# Document:\n
{{ document }}\n
# Conversation:\n
{% for message in conversation %}
{{ message.role }}: {{ message.content }}
{% endfor %}
Now, please output the following as a JSON object:\n
{
"rationale": <verbal feedback> (str datatype),\n
"output": <classification score (0 or 1)> (int datatype),\n
}
LLamaInputFormatForSummaryVerification
You will classify whether the given summary is supported by the given
document or not.
Follow these steps:\n\n
1. Assess the summary against the document \n
2. Classify it is 0 (not supported) or 1 (supported) \n
3. Provide Rationale: Explain your classification decision with a brief
rationale.\n
4. Finally, output the results as a JSON object with the fields \"rationale\"
and \"output\" where \"output\" contains the classification (0 or 1)
# Document:\n
{{ document }}\n
# Summary:\n
{{ summary }}\n
Now, please output the following as a JSON object:\n
{
"rationale": <verbal feedback> (str datatype),\n
"output": <classification score (0 or 1)> (int datatype),\n
}