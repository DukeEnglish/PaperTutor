1
Autonomous Decision Making for UAV
Cooperative Pursuit-Evasion Game
with Reinforcement Learning
Yang Zhao, Zidong Nie, Kangsheng Dong, Qinghua Huang, Xuelong Li
Abstract—The application of intelligent decision-making in modeling and solving pursuit-evasion game scenarios [1]–
unmannedaerialvehicle(UAV)isincreasing,andwiththedevel- [4], optimization theory to model pursuit-evasion game as
opmentofUAV1v1pursuit-evasiongame,multi-UAVcooperative
a multi-objective decision optimization problem [3], [4], and
game has emerged as a new challenge. This paper proposes a
utilizing artificial intelligence decision technology with self-
deepreinforcementlearning-basedmodelfordecision-makingin
multi-roleUAVcooperativepursuit-evasiongame,toaddressthe learning capabilities [5]. The game theory-based approach
challenge of enabling UAV to autonomously make decisions in is limited by its myopic focus on short-term advantages
complex game environments. In order to enhance the training in UAV game, and the difficulty of accurately modeling
efficiencyofthereinforcementlearningalgorithminUAVpursuit-
complex pursuit-evasion game scenarios. The computational
evasiongameenvironmentthathashigh-dimensionalstate-action
performance of the pursuit-evasion game decision method,
space,thispaperproposesmulti-environmentasynchronousdou-
bledeepQ-networkwithpriorityexperiencereplayalgorithmto based on optimization theory, often fails to satisfy the real-
effectively train the UAV’s game policy. Furthermore, aiming time requirements of pursuit-evasion game decision-making
to improve cooperation ability and task completion efficiency, and is primarily employed for offline research aimed at
as well as minimize the cost of UAVs in the pursuit-evasion
optimizing pursuit-evasion game policies. While the pursuit-
game, this paper focuses on the allocation of roles and targets
evasion game situation exhibits significant diversity and the
within multi-UAV environment. The cooperative game decision
model with varying numbers of UAVs are obtained by assigning artificially generated rules are incapable of encompassing all
diverse tasks and roles to the UAVs in different scenarios. conceivable scenarios. Consequently, while the method may
The simulation results demonstrate that the proposed method appear straightforward, it necessitates a substantial workload
enables autonomous decision-making of the UAVs in pursuit-
and falls short in terms of both robustness and accuracy. The
evasion game scenarios and exhibits significant capabilities in
emergence of deep learning technology has led to signifi-
cooperation.
cantadvancementsinvariousdomains[6]–[9].Reinforcement
Index Terms—unmanned aerial vehicle, pursuit-evasion game,
learning(RL),anartificialintelligencetechniqueforintelligent
deepreinforcementlearning,multi-rolecooperation,autonomous
decision-making, has merged with deep learning. In recent
decision-making.
years, deep reinforcement learning has emerged as one of the
most successful methodologies in the field of artificial intel-
I. INTRODUCTION
ligence, with widespread applications in intelligent decision-
THE capability of unmanned aerial vehicle (UAV) contin- making, control and so on [10]–[15], and also plays a crucial
ues to enhance through the utilization of advanced flight role in intelligent pursuit-evasion game. By establishing a
control,payload,power,andothertechnologies.Thisprogress decision-making framework to govern the agent-environment
servesasanewdrivingforceforitstechnologicaldevelopment interaction and formulating a rational reward function, deep
and facilitates the rapid generation of UAV’s game capability reinforcementlearningempowerstheUAVagenttoeffectively
inhighlychallengingenvironments.ThedevelopmentofUAV acquire knowledge and make informed decisions in pursuit-
equipment technology exhibits characteristics such as net- evasion game scenarios. This not only enhances confront
working, decentralization, cost-effectiveness, and intelligence. effectiveness but also bolsters survival capabilities, thereby
With advancements in sensor technology, airborne comput- attracting considerable attention from researchers in the field
ing power, and communication capabilities of weapons and of intelligent pursuit-evasion game. In order to enhance the
equipment, the performance of UAV will witness further en- efficiency of reinforcement learning algorithms in exploring
hancements. Consequently, these low-cost and mass-produced the policy space, Zhang et al. [16] proposed a heuristic Q-
UAV will find wider application across various scenarios. networkapproachthatincorporatesexpertknowledgetoguide
Equippedwithautonomousdecision-makingcapabilities,UAV the search process. This method utilizes the heuristic Q-
can significantly contribute to areas including reconnaissance network technique to train neural network models for solving
missions, manned-unmanned cooperation, as well as pursuit- the over-the-horizon pursuit-evasion game maneuver decision
evasion game. problem. Yang et al. [17] Proposed a approach that presents a
At present, the research on UAV pursuit-evasion game decision-making method for autonomous maneuvers of UAV
primarily concentrates on 1v1 UAV game and multi-UAV in pursuit-evasion game, utilizing the DDPG algorithm. This
cooperative game. In the field of 1v1 UAV pursuit-evasion method effectively filters out numerous invalid action values
game, there are three traditional method: game theory for using optimized pursuit-evasion game maneuver action val-
4202
voN
5
]IA.sc[
1v38920.1142:viXra2
ues generated by the optimization algorithm. Furthermore, it
incorporates the optimized action into the replay buffer as
an initial sample, thereby enhancing both game effectiveness
andsurvivabilityoftheDDPGalgorithmduringUAVpursuit-
evasion game.
The increasing complexity of the UAV application environ-
mentandthegrowingdiversityoftaskshaveposedchallenges
forasingleUAVtoeffectivelyhandlevariousapplicationsce-
narios. With the development of deep reinforcement learning
technology in the multi-agent field [18]–[20], the cooperative
technology of multiple UAVs has emerged as an imperative
solution and a significant developmental trend. Based on the Fig.1. 3-DOFUAVparticlemodel.
1v1 UAV game, researchers have devoted their efforts to
studying multi-UAV cooperative pursuit-evasion game. The
iii) We established multi-role UAV cooperative pursuit-
decision-making problem of cooperative multi-target attack in
evasion game framework and validated its effectiveness in
pursuit-evasiongamewasinvestigatedbyLuoetal.[21],who
scenarios involving 2v1, 2v2, and 3v2, yielding favorable
proposed a heuristic adaptive genetic algorithm to effectively
outcomes.
exploretheoptimalsolutionformissiletargetassignment.The
The subsequent sections of the paper are structured in
proposed approach by Wang et al. [22] employed the clonal
the following manner. Section 2 presents the UAV dynamics
selection algorithm to establish a multi-step UAV dynamic
model and offers a comprehensive exposition of the pursuit-
weapon-target assignment game model, based on the double
evasion game system. Section 3 presents the maneuvering
matrixgameNashequilibriumpointsolutionmethod,resulting
decision algorithms employed by the opposing sides. Section
in a more precise Nash equilibrium solution. Furthermore, the
4 presents the components involved in constructing rein-
technologyofdeepreinforcementlearningalsofindsextensive
forcement learning models. Section 5 presents the training
applicationsinthedomainofmulti-agentsystems.Zhangetal.
and testing of the model, which are demonstrated through
[23]successfullyimplementedcommunicationbetweenUAVs
simulation analysis. And the paper concludes with Section
through bidirectional recurrent neural networks, integrating
6, presenting a comprehensive summary encompassing the
targetallocationandpursuit-evasiongamesituationassessment
entirety of the study.
togeneratecooperativetacticalmaneuverstrategiesthatmerge
formation tactical objectives with each UAV’s reinforcement
II. DESCRIPTIONOFUAVPURSUIT-EVASION
learningobjective.Lietal.[24]proposedamulti-agentdouble
GAMESYSTEM
soft actor-critic algorithm, which employs a distributed exe-
A. UAV Dynamics Model
cution framework based on decentralized partially observable
Markov decision process and centralized training. It considers The UAV dynamics model serves as the fundamental basis
themulti-UAVcooperativepursuit-evasiongameproblemasa for comprehending the pursuit-evasion environment of UAV.
completecooperativegameinordertoachieveeffectivecollab- This study aims to investigate the intelligent decision-making
oration among multiple UAVs. The aforementioned methods capabilitiesofUAVinsuchenvironments.Consequently,when
considerthecommunicationandcollaborationamongmultiple establishingtheUAVmodel,itisabstractedasaparticlemodel
UAVs to effectively accomplish cooperative pursuit-evasion and employs a 3 degree of freedom (3-DOF) control mode
game missions. However, they regard the UAV formation as [25].
a whole, with only cooperation rather than detailed division In the inertial coordinate system, the state variables of the
of labor, focusing on winning the game while overlooking the 3-DOF equation for UAV consist of [x,y,z,v,γ,ψ], where
costofUAV’sduringsuchgames.Theseapproachesmaylead (x,y,z) represents the positional information of UAV in the
to even if the UAV formation gain the eventual triumph, but inertialcoordinatesystem,visascalardenotingthevelocityof
individual UAVs may be exposed to potential encirclement, UAV,γ andψarerespectivelyindicativeofthepitchangleand
causing losses. yaw angle of UAV, signifying its direction of motion. Where,
This paper proposes a deep reinforcement learning-based γ is defined as the angle between⃗v, the velocity vector of the
cooperativegamemethodformulti-roleformationofUAVsto UAV, and the x-o-y plane of the inertial coordinate system.
effectively address this issue, wherein each UAV is assigned ψ is defined as the angle between v⃗′ and the y-axis, while
distinct roles in the pursuit-evasion game to optimize victory v⃗′ is the projection of ⃗v onto the x-o-y plane of the inertial
rate and minimize the cost of game. The main contributions coordinate system.
of this paper are as follows: The control variable of UAV can be represented by three
i) The proposed algorithm, MEADDQN, enhances the effi- parameters:[n ,n ,ϕ]. Where, n represents the overload in
x z x
ciency of data collection for interactive training in reinforce- the direction of UAV velocity, which is used to control the
ment learning and improves sample efficiency through PER; acceleration and deceleration. The variable n represents the
z
ii)WedesignedrewardshapingfortwodifferentUAVroles vertical axis overload of the UAV body, while ϕ denotes the
and conducted training, enabling them to proficiently perform roll angle of the velocity vector, and they control the change
pursuit and bait tasks respectively. of velocity direction collectively. The intelligent algorithm3
judged to successfully intercept the target when α < 5◦,
(cid:13) (cid:13) U
α <90◦, and d=(cid:13)P⃗(cid:13)<800m.
T (cid:13) (cid:13)
III. MANEUVERPOLICYMODELINGFORUAV
PURSUIT-EVASIONGAME
In the UAV pursuit-evasion game scenario described in
thisstudy,theUAVformationsrepresentingopposingfactions
are denoted as red and blue correspondingly. The red team
policy network model in this paper is trained using the deep
reinforcement learning algorithm to guide the red UAVs in
makingmaneuverdecisionsduringcooperativepursuit-evasion
game tasks within the red-blue formation. The blue UAVs
realize their maneuver decision through the matrix game
Fig.2. JudgmentstandardofInterception.
algorithm, serving as an adversary to evaluate the efficacy of
the deep reinforcement learning algorithm.
utilizes these three control variables to determine the ma-
neuvering mode of the UAV, thereby enabling it to execute A. Red Team — Multi-Environment Asynchronous Double
intricate aerial maneuvers and accomplish the pursuit-evasion DeepQ-networkAlgorithmwithPrioritizedExperienceReplay
game missions. Affected by these three control parameters,
The field of reinforcement learning is dedicated to max-
the changes in UAV’s speed, roll angle and yaw angle are as
imizing agent’s cumulative reward within a complex and
follows:
 uncertainenvironment.Theagentimprovesitsactionselection
ν˙ =g(n −sinγ)
 x byperceivingtheenvironmentalstateandreceivingrewarding
γ˙ = vg(n zcosϕ−cosγ) (1) feedback, thus obtaining the maximum return. Reinforcement
ψ˙ = gnzsinϕ learning problems are usually modeled utilizing Markov de-
vcosγ
cision processes (MDP). MDP is a mathematical framework
Furthermore, in the inertial coordinate system, the UAV
that models the decision-making process of an agent in an
coordinates exhibit the following variations:
uncertainenvironment.Itcapturesthenotionthatfuturestates
 are determined solely by the current state and actions taken,
x˙ =vcosγsinψ
 without any dependence on past states. MDP can be rep-
y˙ =vcosγcosψ (2) resented using a quad-tuple:⟨S,A,P,R⟩. The state space is
z˙ =vsinγ denoted as S, the action space as A, the environment state
transitionprobabilityasP,andtherewardfunctionS×A→R
quantifies the amount of feedback that agent can receive for
B. Judgment Standard of Interception in Pursuit-Evasion
executing an action in the current state.
Game
The advantages and disadvantages of the confrontation in R(s,a)=E[r |s =s,a =a] (3)
t+1 t t
the pursuit-evasion game environment are conveyed via the
The primary objective of reinforcement learning algorithms
relativesituationalinformationoftheUAVs.Itisexpectedthat
istooptimizestrategiesthroughinteractivetrialanderror,with
UAV can achieve intelligent decision-making to secure more
the ultimate goal of maximizing the returns.
advantageous firing positions during pursuit-evasion game.
The coverage of a UAV’s firepower typically forms a frontal T−t
(cid:88)
cone,therebyenablingthedeterminationofUAV’sadvantages maximize G t = γkR t+k (4)
and disadvantages in a pursuit-evasion game environment k=0
based on the UAV’s orientation. The pursuit-evasion game Where, G is the returns, which is the cumulative discount
t
environment discussed in this paper does not encompass the reward after time t, and γ is a discount factor that satisfies
simulationofUAVfirepower.Therefore,inordertoeffectively 0 ≤ γ ≤ 1. The aforementioned definition can be intuitively
neutralize enemy aircraft, the UAV must autonomously ma- comprehendedasanagentfocusingmoreonnear-termrewards
neuver and strategically position itself behind the target UAV, than on rewards that are further away.
ensuring a tail chase to target is executed within UAV’s fire- Reinforcement learning algorithm optimizes a policy π,
power range. This study imposes a strict numerical constraint π:S → A is the mapping function of the agent from state
on tracking interception in pursuit-evasion games. to action. Via maintaining an action value function Q (s,a),
π
As shown in Fig. 2, the vector P⃗ represents the relative thevalue-basedreinforcementlearningalgorithmsevaluatethe
position of the UAV and the target, the antenna train angle benefit of selecting action a in state s when agent’s policy π
α corresponds to the angle between the velocity vector v is determined.
U U
of the UAV and the relative position vector P⃗, and the aspect
∞
angle α refers to the angle between the velocity vector v (cid:88)
T T Q (s,a)=E [ γkR |S =s,A =a] (5)
of the target and the relative position vector P⃗. The UAV is π π t+k+1 t t
k=04
Fig.3. ReinforcementlearningalgorithmframeworkforUAVpursuit-evasiongame.
The action-value function is updated iteratively according the transition with higher priority for training with a higher
to the Bellman Equation (6), and Q∗(s,a) = max Q (s,a) probability during sampling.
π π
is obtained through constantly approximation. Based on
δ =R +γV(S )−V(S ) (8)
this, agent can obtain the optimal policy π∗(a|s) = t t t+1 t
argmax Q∗(s,a).
a∈A(s) This section proposes a Multi-Environment Asynchronous
Double Deep Q-Network (MEADDQN) algorithm, which
serves as a further optimization of the aforementioned al-
Q (s,a)=E [R +γQ (S ,A )|S =s,A =a] gorithm through the introduction of multi-environment asyn-
π π t+1 π t+1 t+1 t t
(6) chronous experience collection, with the objective of expe-
Mnih et al. [11] utilized Q(s,a;θ) function to approximate diting the training process and enhancing the efficiency of
the optimal Q∗(s,a) function and employed a deep neural acquiring pursuit-evasion game interaction data. As shown in
network to solve for Q(s,a;θ), which forms the fundamental Fig. 3, MEADDQN concurrently generates multiple pursuit-
concept of the Deep Q-Network (DQN) algorithm [26], [27]. evasion game environments with identical tasks in parallel
In order to improve the efficiency and stability of the algo- threads,eachenvironmentbeinginitializeddifferently.Agents
rithm,atargetnetworkQ(s,a;θ′)isadded,whichparticipates operateasynchronouslyandinteractwithdistinctenvironments
inthetrainingofthepolicynetworkandreplicatesthecurrent while adhering to the same policy network. All interaction
parameters of the policy network at regular intervals. The data collected from these environments is consolidated into a
targetnetworkinthetrainingprocessintroducesacertaintime unified replay buffer that supports PER, enabling sampling
delay to decouple the value estimation of adjacent moments, of data from the buffer during training. To enhance the
thereby mitigating the impact of unstable fluctuations in data algorithm’s robustness and explore strategic possibilities, this
transmission during each iteration. In Double DQN [28] algo- paper introduces different action noise to the UAV agents in
rithm, the loss function of the neural network is diverse interactive environments. In environments with higher
levels of action noise, agents will engage in more audacious
L(θ)=[Q(s ,a ;θ)−r +γQ(s ,a ;θ′)]2 (7) exploration, whereas in environments with lower levels of
t t t t+1 max
action noise, agents will leverage their acquired experience to
identifythemostrewardingdecisionwithinthecurrentpolicy.
The solution can be obtained by gradient descent method.
Double DQN is an off-policy reinforcement learning algo-
B. Blue Team — Matrix Game Algorithm
rithm that can utilize a distinct policy for data acquisition,
which differs from the current update policy. The interactive This paper improves the UAV matrix game algorithm [30]
datawillbestoredinthereplaybufferastheformoftransition for multi-UAV. In the multi-UAV matrix game algorithm, the
transition (s ,a ,r ,s ) and trained using the time series blueUAVconstructsab×r-dimensionalmatrixG foreachred
t t t t+1
difference method, thereby effectively enhancing data utiliza- UAV, where b represents the number of available actions for
tion. Prioritized experience replay (PER) [29] is a method for theblueUAVandr representsthenumberofavailableactions
sampling interactive data. When storing each transition in the fortheredUAV.ThevalueofG inthismatrixrepresentsthe
ij
replaybuffer,PERassignsdifferentprioritiestoeachtransition reward score for the blue side, based on the current pursuit-
basedontheabsolutevalueofitsTD-Error|δ |(8),andselects evasiongamesituation,assumingthatthebluesidetakeaction
t5
i and the red side take action j, following a similar approach
as reinforcement learning’s reward function to ensure fairness
in subsequent adversarial games. After obtaining these payoff
matrices, further processing is carried out:
i)Findtherowminimumofeachrowofeachpayoffmatrix
and sum according to the row index i;
ii) Select the maximum value from these minimum value
sums;
iii) The action a corresponding to the maximum value’s
i
row index is the optimal maneuver of the blue side.
The row minimum represents the lowest return for blue
Fig.4. AutonomousdecisionactionspaceforUAV.
UAV, assuming that red UAVs’ policy can always minimize
blue UAV’s return, while the maximum value of these mini-
mum values ensures that blue UAV will receive the highest B. Action Space
possible return even if red UAVs always choose actions
The present study utilizes the body axis direction overload,
unfavorable to blue UAV.
longitudinal overload, and roll angle as control variables to
establisha3-DOFUAVparticlecontrolmodel.Thismodelfa-
IV. REINFORCEMENTLEARNINGELEMENTIN cilitatesmoreaccuratesimulationofrealisticflighttrajectories.
MULTI-ROLEUAVPURSUIT-EVASIONGAME The present section introduces a 15-dimensional discrete ac-
The reinforcement learning problem is typically described tionspacespecificallytailoredfortheDDQNalgorithminthe
using the MDP 4-tuple model ⟨S,A,P,R⟩, where the state context of reinforcement learning with discrete control [31].
transition probability P is determined by the environment This customized discrete action space aims to accommodate
itself and does not necessitate explicit modeling in model- the three control variables (n ,n ,ϕ), which is shown as Fig.
x z
freereinforcementlearningalgorithms.Therefore,thissection 4.
will provide a detailed description of modeling the state
space,actionspace,andrewardfunctionforthereinforcement
C. Reward Function for Multi-role UAV
learning task of multi-role UAVs cooperation.
The reward function design in the task of UAV cooperative
pursuit-evasiongameaimstostrategicallyguidethevictoryof
A. State Space
thepursuit-evasiongame.Thusthefinaloutcomer which
final
The state space in this paper is designed to encompass all
signifies the victory or defeat of pursuit-evasion game, can be
the state information of both UAVs, as well as variables that
utilizeddirectlyasarewardsignal.However,theagentisonly
canexpresstherelativeinformationbetweenthetwoopposing
provided with r at the end of each episode, necessitating
final
sides. This comprehensive representation serves as input for
a prolonged waiting period to ascertain the correctness of its
the policy network, enabling it to make informed decisions in
actions. Moreover, identifying advantageous action paths in
the current confrontational scenario. UAV’s state information
environments with sparse reward poses a formidable chal-
can be characterized by its position, pitch angle, and yaw
lenge for the agent. This section enhances the efficiency of
angle. Furthermore, the variables illustrated in Fig. 2 can also
reinforcement learning algorithms through the utilization of
depict the relative information during pursuit-evasion game.
dense reward shaping for the pursuit-evasion game task. The
The efficacy of reinforcement learning training is ensured by
effective allocation of tasks is critical strategies for enhancing
representingthestateofUAVinthispaperasa13-dimensional
the win rate of pursuit-evasion game and minimizing opera-
variable:
tional losses. The UAV entities in the pursuit-evasion game
(z ,v ,γ ,ψ ,z ,v ,γ ,ψ ,α ,α ,d,γ ,ψ ) (9) environment are assigned the following roles: one type is
U U U U T T T T U T P P
designated for target attack and pursuit, called pursuit UAV,
The first four quantities represent the attributes of the UAV while the other type, called bait UAV, functions as a bait
itself: z U denotes the altitude, v U is a scalar that represents to draw enemy fire and create one-on-one or even multi-
speed, γ U signifies the pitch angle, and ψ U indicates the yaw on-one scenarios for other UAVs. This section tailors the
angle.Thesubsequentfourquantitiesdepictthecharacteristics dense rewards in distinct manners for these two categories of
of the target UAV: z T refers to its altitude, v T denotes its UAVs, ensuring their ability to successfully accomplish their
speed, γ T represents its pitch angle, and ψ T indicates its respective tasks.
yaw angle. The remaining five quantities are employed to 1) Pursuit UAV Reward Shaping:
represent the relative information between the two drones, ⃝1Angle advantage reward:
whereα ,α anddareintroducedinSectionIIasindicators
U T
α +α
forantennatrainangle,aspectangleanddistancerespectively. r =1− U T (10)
Additionally,asfortherelativepositionvectorP⃗,itsnumerical p 2π
magnitude is indicated by d, and the orientation of P⃗ can be Theangleadvantagerewardaimsforthatα andα should
U T
represented in a similar manner to the pitch and yaw angles beminimized,whichalignswiththeanglerequirementsofthe
of the UAV point model. judgment standard of interception.6
⃝2Distance advantage reward:
(cid:32) (cid:33)
abs(∥P⃗∥−d )
r =exp − opt (11)
d d
0
The distance advantage reward is designed to guide the
UAV to reach the objective distance to the target. d in (11)
opt
represents the objective distance, set to d =800m, and d
opt 0
is a distance constant parameter.
⃝3Velocity advantage reward:
r =
− v→
U
·P⃗
(12) Fig.5. Initializationofenvironmentsituation.
v V ∥P⃗∥
max
Thevelocityadvantagerewardisdirectlyproportionaltothe V. EXPERIMENT
projection of the UAV’s velocity vector v onto the relative
U
positionvectorP⃗.Therangeofthevelocityadvantagereward
In this study, pursuit UAV and bait UAV were trained
is constrained to [-1,1] by v max, which aligns with the range independently and subsequently integrated within the multi-
of the other two rewards. UAV environment. Both agent types use the same policy
To sum up, combining the collision and out-of-bound networkarchitecturecomprisingthreehiddenlayerswith512,
penaltytermr ,thepursuitUAV’sdenserewardr design 1024, and 512 neurons respectively, facilitating the policy
punish t
is as follows: transformation in a multi-UAV environment. In reinforcement
learning training, the discount factor is γ = 0.95, the replay
buffer capacity is 100000, the batch size is 1024, and the

r final, intercept or be intercepted activation function is ReLU.
r = r , collision or out of bound To faithfully replicate actual UAV flight conditions, no
t punish
w r +w r +w r , otherwise horizontal constraints are enforced within the airspace where
1 p 2 d 3 v
(13) pursuit-evasiongametakesplacewhileonlythez-axisbound-
w , w , and w are the weights of each reward. ary is set as 1000m<z <13000m in the inertial coordinate
1 2 3
2) Bait UAV Reward Shaping: system. In the designated airspace, a specified number of red
andblueUAVsaredeployedforpursuit-evasiongamebytheir
⃝1Angle advantage reward:
own maneuver policy. The outcome is determined based on
(cid:18) (cid:19)
abs(α −α ) the cost of the game between the two sides’ UAVs. Once a
r =2∗exp − T opt −1 (14)
p α UAV is successfully intercepted by the opponent, we consider
0
it destroyed and remove it from the pursuit-evasion game
(14) represents that bait UAV needs to maintain an aspect environment. The team that successfully intercepts all of the
angletohaveasufficientdecoyeffectonthetargetUAV,where opponent first shall emerge as the victor. The initialization
α opt is bait UAV’s objective aspect angle, and α 0 is an angle process of the pursuit-evasion game environment involves
constant parameter. fixing the position of the red UAVs and establishing a spatial
⃝2Distance advantage reward: coordinate system with a red UAV as its origin. Then the
(cid:32) (cid:33) blueUAVsshouldbeinitializedrandomlywithinarectangular
abs(∥P⃗∥−d )
r =exp − opt (15) space centered on the red UAV, which is limited 20000m in
d d 0 length, 20000m in width, and 6000m in height, as shown in
Fig. 5, to ensure diversity in the initial position. To avoid
The distance advantage reward utilizes the identical calcu- the issue of decision steps becoming excessively short in
lation methodology as the pursuit UAV, wherein the objective subsequentroundsduetoinitialdistancesbeingtooclose,this
distance is designated as d opt = 1500m. This ensures the studyimplementedaninitialvacuumzonewithinarectangular
safety of UAV while concurrently generating a decoy effect regionmeasuring4000metersinlength,4000metersinwidth,
against the target. and6000metersinheight,asshowninFig.5.TheblueUAVs
The bait UAV does not necessarily require a consistent ve- will not be initialized in this area, thereby ensuring a certain
locity advantage, but rather should be strategically positioned distance is maintained between both sides during the initial
to allure the target. Therefore, considering the penalty term phaseoftheconfrontation.Asfortheinitializationoftheflight
r punish for collision and out-of-bound situations, the dense attitudeofUAVs,thisstudyassumesthatbothUAVsstartwith
reward r t for the bait UAV is designed as follows: ahorizontalflightposition.TheredUAVs’initialyawangleis
set to ψ =0, while the yaw angle of the blue team drone is
 U
r , be intercepted
 final randomly initialized. In this way, the simulated confrontation
r t = r punish, collision or out of bound (16) allowsforvariousinitialposturesbetweenthetwosideUAVs,
w r +w r , otherwise thereby simulating states of advantage and disadvantage.
1 p 2 d7
A. Basic Training of Pursuit UAV and Bait UAV.
In the initial stages of training, the policy network of the
red team is initialized randomly. However, this can lead to
consistent failures for the red team and even difficulties in
obtaining positive reward signals when directly confronted
with a blue team that possesses a higher level of intelligent
decision-making capability. Consequently, not only does this
(a)
situationimpacttrainingefficiencybutitalsoresultsinanex-
ceedinglyslowconvergencespeed.Tosolvethisproblem,this
study proposes three basic training methods for the red team
policy network. These basic training methods are intended to
enable the red team’s policy network to learn the maneuver
model of the drone and the basic logic of pursuit-evasion
game. Through a progressive sequence from simplicity to
complexity,thesethreetrainingmethodsenabletheredteam’s
policy network to gradually assimilate knowledge pertaining (b)
to UAV pursuit-evasion game. This not only enhances the
intelligenceleveloftheUAVagentbutalsoestablishesasolid
groundwork for competing against highly intelligent decision-
making adversaries. The three basic training scenarios are as
follows: ⃝1 against straight-line maneuver target; ⃝2 against
circling maneuver target; ⃝3 against random maneuver target.
First, pursuit UAV’s policy network undergoes sequential
basic training using MEADDQN with PER for three basic
(c)
sessions, each consisting of 200000 steps. Finally, all these
basictrainingsareamalgamated,withonerandomlyinitialized Fig. 7. Bait UAV basic training reward curve and simulation. (a) against
ineachroundforatotaltrainingdurationof600000steps.Fig. straight-linemaneuvertarget.(b)againstcirclingmaneuvertarget.(c)against
randommaneuvertarget.
6 demonstrate the reward curves of three basic trainings and
the pursuit-evasion game simulation after training completion
respectively. It can be observed that with increasing com-
plexity of the training scenario, there is a slight degradation
in convergence. However, the pursuit UAV still effectively
accomplishes its task.
Next, perform the same basic trainings for bait UAV, the
(a) results of which are shown in Fig. 7. Bait UAV is effectively
maintaining a advantageous position ahead of the target,
ensuring both safety and attractiveness.
B. Against Matrix Game Algorithm Training.
After successfully completing the three basic training ses-
sions, it can be inferred that the red UAV’s policy network
possesses a rudimentary comprehension of the 3DOF particle
model of drones and the fundamental principles of pursuit-
(b)
evasion game, and exhibits certain game capabilities. Conse-
quently,itcangamewithblueUAVthatemploysmatrixgame
algorithm and possesses intelligent decision-making ability.
Fig.8showsthetrainingrewardcurvesofthepursuitUAVand
bait UAV in a pursuit-evasion game against UAV controlled
by matrix game algorithm.
Additionally, Fig. 9 presents a comparison of MEADDQN
withPERtootherreinforcementlearningalgorithmsusingthe
(c) training of the pursuit UAV as an example.
Fig. 10 demonstrates the track simulation results of the
Fig.6. PursuitUAVbasictrainingrewardcurveandsimulation.(a)against
straight-linemaneuvertarget.(b)againstcirclingmaneuvertarget.(c)against pursuit-evasion game of two different roles against matrix
randommaneuvertarget game algorithm, and gives real-time reward curves. As Fig.8
TABLEI
ROLEANDTARGETALLOCATIONMETHOD
2v1 TheRedTeamhasanumericaladvantageanddoesnot
needtoworryaboutencirclement,sosetbothUAVsto
pursuitmode.
2v2 Periodically, for each blue UAV, calculate the reward
score for all red UAVs acting as pursuit UAVs in the
currentscenario,takingthehighergroupasthepursuit
groupandtheothergroupasthebaitgroup.Oncethe
pursuitgroupcompletesitstask,switchtoa2v1mode.
3v2 Periodically, for each blue UAV, calculate the reward
(a) (b)
score for all red UAVs acting as pursuit UAVs in the
Fig.8. (a)PursuitUAVpursuit-evasiongametrainingrewardcurve.(b)Bait current scenario, taking the highest 2v1 group as the
UAVpursuit-evasiongametrainingrewardcurve. pursuit group and the other group as the bait group.
Oncethepursuitgroupcompletesitstask,thescenario
transitionstoa3v1modewiththreepursuitUAVs.
C. Multi-Role UAV Cooperative Pursuit-Evasion Game.
AftertrainingpursuitUAVandbaitUAV,twodifferentroles
of UAV, this study proposed a Multi-role UAV cooperative
pursuit-escape game method, which also uses matrix game
algorithmastheopponentforpursuit-escapegame.According
totheMulti-UAVpursuit-escapegameenvironmentintroduced
in Section II, in the Multi-role UAV cooperative pursuit-
escape game, the red UAVs are initialized with different roles
and execute different strategies, while the blue UAVs are
controlled by matrix game algorithm. Once the red UAV has
determined its own role, it must also establish a clear target,
whether it be pursuit or bait. The roles and targets of each
UAV can be assigned based on the current game situation.
In order to enhance the efficiency of the game, this study
Fig.9. Comparisonofreinforcementlearning algorithmsinpursuit-evasion
prioritizes allocation methods that can quickly achieve tail-
game.
biting scenarios, enabling pursuit UAV to quickly eliminate
Blue Team members. Bait UAV are responsible for creating
10(a)shows,thePursuitUAVcanperformthepursuit-evasion one-on-one or multi-on-one scenarios to enable pursuit UAV
game task very well, consistently maintaining a dominant to avoid potential encirclement.
position throughout the pursuit process. In the dogfight where Fig. 11 shows the training and testing procedure of multi-
both sides gain similar rewards, the pursuit UAV can adjust role UAV cooperative pursuit-evasion game. In training pro-
to secure the dominant position, thereby widening the reward cedure, the pursuit and bait policies, which have been trained
gap and ultimately intercepting the blue UAV. In the test, the in the 1v1 UAV game, are concurrently applied in the multi-
interception reward is set to r = 2. The target of the UAV environment and iterative trained through independent
final
bait UAV consistently receives positive reward feedback in reinforcement learning. In each iteration, only one agent in
Fig. 10(b), because the bait UAV in the trajectory simulation the environment is designated as the training status, while
always stays in a position that appears to be advantageous to the policies of the remaining agents are held constant. This
itstarget,Moreover,thespecialrewardcalculationmethodfor approach effectively mitigates environmental fluctuations and
the bait UAV enables it to receive a high reward under these enhances training stability. And as MEADDQN is an offline
circumstances. reinforcement learning algorithm, the interaction data from
(a) (b)
Fig.10. UAVpursuit-evasiongamesimulationtracksandreal-timerewardcurves.(a)PursuitUAVsimulationresults.(b)BaitUAVsimulationresults9
Fig.11. Multi-roleUAVcooperativepursuit-evasiongameframework.
(a) (b) (c)
Fig.12. Multi-UAVcooperative pursuit-evasion gamesimulationtracksandreal-timerewardcurves.(a)2v1 simulationresults.(b)2v2simulationresults.
(c)3v2simulationresults.
agents using the same policy as the agent being trained can UAV,successfullytrackingandinterceptionthetargetabout70
also be utilized for training and updating the policy network, seconds after the 2v1 game begin. Fig. 12(b) depicts a game
thereby enhancing sample efficiency. During the training pro- scenarioinvolvingtworedUAVs,whereinoneisdesignatedas
cess, only the UAVs’ target are allocated, while their roles the pursuit UAV and the other as the bait UAV. The bait UAV
remain unchanged after initialization. In the test, the roles sacrifices itself to strategically create a favorable situation for
and targets of the UAVs are allocated by the role and target thepursuitUAV.(TherewardcurvesofsimulationtestsinFig.
allocation system every 10 decision steps or after a UAV is 12usetherewardcalculationmethodologyofthepursuitUAV
intercepted.Thestudyconductedexperimentsin2v1,2v2,and to show the UAVs’ advantage and disadvantage in a pursuit-
3v2 settings, implementing corresponding methods for role evasiongame.)Fig.12(c)illustratesthatatthebeginningofthe
and target allocation as presented in Table I. the simulation 3v2 game, two red UAVs function as pursuit UAVs while one
results are illustrated in Fig. 12. redUAVservesasabaitUAV.Thetargethasbeensuccessfully
Fig.12(a)showsthattworedUAVsplaytheroleofpursuit intercepted by the pursuit group approximately 30 seconds10
TABLEII [5] T. H. Teng, A. H. Tan, Y. S. Tan and A. Yeo, “Self-organizing neural
WINRATEOFPURSUIT-EVASIONGAMEIN100TESTEPISODES(1MIN) networksforlearningaircombatmaneuvers,”inThe2012International
JointConferenceonNeuralNetworks,Brisbane,QLD,Australia,pp.1-8,
win standoff lose 2012.
2v1 26% 74% 0% [6] Y. Zhao and X. Li, “Spectral Clustering With Adaptive Neighbors for
2v2 11% 89% 0% DeepLearning,”inIEEETransactionsonNeuralNetworksandLearning
3v2 18% 82% 0% Systems,vol.34,no.4,pp.2068-2078,April2023.
[7] X.Li,T.WeiandY.Zhao,“DeepSpectralClusteringWithConstrained
LaplacianRank,”inIEEETransactionsonNeuralNetworksandLearning
TABLEIII Systems,vol.35,no.5,pp.7102-7113,May2024.
WINRATEOFPURSUIT-EVASIONGAMEIN100TESTEPISODES(3MIN) [8] Y. Zhao and X. Li, “Deep Spectral Clustering With Regularized Linear
Embedding for Hyperspectral Image Clustering,” in IEEE Transactions
win standoff lose onGeoscienceandRemoteSensing,vol.61,pp.1-11,2023.
2v1 54% 46% 0% [9] B.ZhaoandX.Li,“Edge-AwareNetworkforFlow-BasedVideoFrame
2v2 27% 73% 0% Interpolation,” in IEEE Transactions on Neural Networks and Learning
3v2 35% 65% 0% Systems,vol.35,no.1,pp.1401-1408,Jan.2024.
[10] L. C. Garaffa, M. Basso, A. A. Konzen and E. P. de Freitas, “Rein-
forcement learning for mobile robotics exploration: A survey,” in IEEE
TABLEIV TransactionsonNeuralNetworksandLearningSystems,vol.34,no.8,
WINRATEOFPURSUIT-EVASIONGAMEIN100TESTEPISODES(5MIN) pp.3796-3810,Aug.2023.
[11] B. Lian, W. Xue, F. L. Lewis and T. Chai, “Inverse reinforcement
win standoff lose learning for adversarial apprentice games,” in IEEE Transactions on
2v1 89% 11% 0% Neural Networks and Learning Systems, vol. 34, no. 8, pp. 4596-4609,
2v2 65% 35% 0% Aug.2023.
3v2 76% 24% 0% [12] Z.Zhao,Q.Wang,andX.Li,“Deepreinforcementlearningbasedlane
detectionandlocalization,”Neurocomputing,vol.413,pp.328-338,2020.
[13] K. Zhang, Y. Li, J. Wang, E. Cambria and X. Li, “Real-Time Video
Emotion Recognition Based on Reinforcement Learning and Domain
into the game. At this moment, the bait UAV transitions into Knowledge,” in IEEE Transactions on Circuits and Systems for Video
pursuit mode and efficiently accomplishes the interception of Technology,vol.32,no.3,pp.1034-1047,March2022.
[14] Y. Liu, Y. Pang, R. Jin, Y. Hou and X. Li, “Reinforcement Learning
the target. Table II, III, and IV shows the winning rates of
and Transformer for Fast Magnetic Resonance Imaging Scan,” in IEEE
multi-UAV pursuit-evasion game over 100 test episodes with Transactions on Emerging Topics in Computational Intelligence, vol. 8,
each episode lasting 1 minute, 3 minutes, and 5 minutes. no.3,pp.2310-2323,June2024.
[15] D. Jia, T. Li, Y. Zhao, X. Zhang, Z. Wang, “Empty nodes affect con-
ditionalcooperationunderreinforcementlearning,”AppliedMathematics
VI. CONCLUSION andComputation,vol.413,2022.126658,
[16] X. Zhang, G. Liu, C. Yang, J. Wu, “Research on Air Confrontation
ThispaperproposestheMEADDQNwithPERalgorithmto ManeuverDecision-MakingMethodBasedonReinforcementLearning,”
Electronics,vol.7,no.11,2018.
address the problem of multi-UAV pursuit-evasion game. By
[17] Q. Yang, Y. Zhu, J. Zhang, S. Qiao and J. Liu, “UAV air combat
assigning distinct tasks to UAVs in the pursuit-evasion game autonomousmaneuverdecisionbasedonDDPGalgorithm,”in2019IEEE
environment, We trained two types of UAVs with different 15th International Conference on Control and Automation, Edinburgh,
UK,pp.37-42,2019.
policies, enabling them to collaboratively solve the multi-
[18] X.Yao,C.Wen,Y.WangandX.Tan,“SMIX(λ):Enhancingcentralized
dronepursuit-evasiongameproblemthroughcollaborationand value functions for cooperative multiagent reinforcement learning,” in
task allocation. The method proposed in this paper enhances IEEE Transactions on Neural Networks and Learning Systems, vol. 34,
no.1,pp.52-63,Jan.2023.
the mission efficiency and cooperation ability of multi-UAV
[19] Y.Jin,S.Wei,J.YuanandX.Zhang,“Hierarchicalandstablemultiagent
pursuit-evasion game. The future will witness further explo- reinforcement learning for cooperative navigation control,” in IEEE
ration of multi-UAV cooperation scenarios, with the aim of Transactions on Neural Networks and Learning Systems, vol. 34, no.
1,pp.90-103,Jan.2023.
proposing universally applicable methodology for role and
[20] X. Xu, R. Li, Z. Zhao and H. Zhang, “Stigmergic independent rein-
target allocation. In addition, the challenge of autonomous forcement learning for multiagent collaboration,” in IEEE Transactions
decision-making in the presence of incomplete information, onNeuralNetworksandLearningSystems,vol.33,no.9,pp.4285-4299,
Sept.2022.
particularly when the UAV lacks a comprehensive perception
[21] D. Luo, C. Shen, B. Wang and W. Wu, “Air combat decision-making
or encounters communication obstacles, constitutes a primary for cooperative multiple target attack using heuristic adaptive genetic
focus for our forthcoming studies. algorithm,”in2005InternationalConferenceonMachineLearningand
Cybernetics,Guangzhou,China,pp.473-478,2005.
[22] Y.Wang,W.ZhangandY.Li,“Anefficientclonalselectionalgorithm
REFERENCES tosolvedynamicweapon-targetassignmentgamemodelinUAVcooper-
ative aerial combat,” in 2016 35th Chinese Control Conference (CCC),
[1] G. Xu, S. Wei, and H. Zhang, “Application of situation function in air Chengdu,China,pp.9578-9581,2016.
combat differential games,” in Proc. 36th Chin. Control Conf., Dalian, [23] J. Zhang, Q. Yang, G. Shi, Y. Lu, and Y. Wu, “UAV cooperative air
China,pp.5865-5870,2017. combatmaneuverdecisionbasedonmulti-agentreinforcementlearning,”
[2] H. Park, B. Y. Lee, M. J. Tahk, and D. W. Yoo, “Differential game in Journal of Systems Engineering and Electronics, vol. 32, no. 6, pp.
based air combat maneuver generation using scoring function matrix,” 1421-1438,Dec.2021.
InternationalJournalofAeronauticalandSpaceSciences,vol.17,no.2, [24] S. Li, Y. Wang, Y. Zhou, Y. Jia, H. Shi, F. Yang, and C. Zhang,
pp.204–213,2016. “Multi-UAV cooperative air combat decision-making based on multi-
[3] S.Zhang,Y.Zhou,Z.Li,andW.Pan,“Greywolfoptimizerforunmanned agentdouble-softactor-critic,”Aerospace,vol.10,no.7,2023.
combataerialvehiclepathplanning,”AdvancesinEngineeringSoftware, [25] Y.Wang,C.Huang,C.Tang,“Researchonunmannedcombataerialve-
vol.99,pp.121-136,2016. hiclerobustmaneuveringdecisionunderincompletetargetinformation,”
[4] C.Huang,K.Dong,H.Huang,S.TangandZ.Zhang,“Autonomousair AdvancesinMechanicalEngineering,vol.8,no.10,2016.
combatmaneuverdecisionusingBayesianinferenceandmovinghorizon [26] V. Mnih, K. Kavukcuoglu,D. Silver, A. Graves, I. Antonoglou, D.
optimization,” in Journal of Systems Engineering and Electronics, vol. Wierstra, and M. Riedmiller, “Playing atari with deep reinforcement
29,no.1,pp.86-97,Feb.2018. learning,”2013,arXiv:1312.5602.11
[27] V.Mnih,K.Kavukcuoglu,D.Silver,etal.,“Human-levelcontrolthrough
deep reinforcement learning,” Nature, vol. 518, no. 7540, pp. 529–533,
Feb.2015,https://doi.org/10.1038/nature14236.
[28] H. van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning
with double Q-learning,” in Proceedings of the AAAI Conference on
ArtificialIntelligence,vol.30,no.1,2016.
[29] T.Schaul,J.Quan,I.Antonoglou,andD.Silver,“Prioritizedexperience
replay,”2015,arXiv:1511.05952.
[30] F. Austin, G. Carbone, M. Falco, H. Hinz, and M. Lewis, “Game
theory for automated maneuvering during air-to-air combat,” Journal of
Guidance,Control,andDynamics,vol.13,no.6,pp.1143-1149,1990.
[31] Q.Yang,J.Zhang,G.Shi,J.HuandY.Wu,“ManeuverDecisionofUAV
inShort-RangeAirCombatBasedonDeepReinforcementLearning,”in
IEEEAccess,vol.8,pp.363-378,2020.