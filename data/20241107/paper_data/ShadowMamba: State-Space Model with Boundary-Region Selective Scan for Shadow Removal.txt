JOURNALOFLATEXCLASSFILES 1
ShadowMamba: State-Space Model with
Boundary-Region Selective Scan for Shadow
Removal
Xiujin Zhu , Chee-Onn Chow , Senior Member, IEEE, and Joon Huang Chuah , Senior Member, IEEE
Abstract—Image shadow removal is a typical low-level vi- requires separate modeling of the shadow regions, making it
sion problem, where the presence of shadows leads to abrupt a challenging task.
changes in brightness in certain regions, affecting the accuracy
Traditional shadow removal methods are primarily divided
of upstream tasks. Current shadow removal methods still face
into illumination transfer methods [13], [14] and shadow
challenges such as residual boundary artifacts, and capturing
featureinformationatshadowboundariesiscrucialforremoving region relighting methods [15], [16]. These approaches rely
shadows and eliminating residual boundary artifacts. Recently, on physical modeling of the shadow itself and are effective
Mamba has achieved remarkable success in computer vision for single shadow types, but they often perform poorly in
by globally modeling long-sequence information with linear
complex background scenarios. In recent years, deep learn-
complexity. However, when applied to image shadow removal,
ing methods have gradually replaced traditional approaches,
the original Mamba scanning method overlooks the semantic
continuity of shadow boundaries as well as the continuity of leading to the emergence of numerous shadow removal meth-
semantics within the same region. Based on the unique charac- ods [17]–[21] based on CNN and transformer architectures.
teristics of shadow images, this paper proposes a novel selective These methods model shadow images based on brightness
scanningmethodcalledboundary-regionselectivescanning.This
information [22], [23], shadow boundary information [24],
methodscansboundaryregions,shadowregions,andnon-shadow
[25], or regional information [18], [20], [21], utilizing deep
regions independently, bringing pixels of the same region type
closer together in the long sequence, especially focusing on the networksforshadowremoval.Amongthese,shadowboundary
local information at the boundaries, which is crucial for shadow informationisaneffectivefeatureforshadowremoval.Dueto
removal.Thismethodcombineswithglobalscanningandchannel thehighlycomplextransitionofintensityandcoloratshadow
scanningtojointlyaccomplishtheshadowremoval.Wenameour
boundaries, removing shadow pixels at the boundary is more
modelShadowMamba,thefirstMamba-basedmodelforshadow
challenging than removing pixels within the shadowed area.
removal. Extensive experimental results show that our method
outperforms current state-of-the-art models across most metrics Therefore, helping the model understand shadow boundary
onmultipledatasets.ThecodeforShadowMambaisavailableat information and local features at the boundary is crucial for
(Code will be released upon acceptance). shadow removal.
IndexTerms—Mamba,Scanningmechanism,Shadowremoval, Most current approaches [17], [26], [27] post-process the
Shadow boundary shadow boundaries. While this can lead to some perfor-
mance improvements, balancing shadow removal with detail
preservation at shadow boundary pixels is challenging when
I. INTRODUCTION
relying solely on smoothing constraints, thereby limiting its
SHADOWS are cast when objects block light, making effectiveness. Some methods [24], [25] directly model the
it inevitable that they are captured in images during shadowboundary,withonerepresentativesupervisedapproach
acquisition. The presence of shadows not only causes the beingBA-ShadowNet[24].Itusesashadowboundarymaskto
image to lose certain information but also affects the accu- crop the boundary pixels from the original image and applies
racy of downstream tasks, such as object detection [1], [2], supervised constraints directly to these pixels, fusing the
instance segmentation [3], [4], and image classification [5], boundary information with the information from the shadow
[6]. Therefore, image shadow removal is a crucial task. removal branch. Although this method effectively improves
Shadow removal is a typical low-level vision problem, but the removal of boundary artifacts, it processes boundary
it differs from other low-level vision tasks like image super- pixels separately by cropping them out, which completely
resolution [7], [8], low-light image enhancement [9], [10], discards the semantic relationships between boundary pixels
and image deraining [11] or dehazing [12], which involve and shadow or non-shadow regions, affecting the model’s
processing the entire image, shadow removal focuses only understanding of brightness variation between regions. More-
on the damaged regions within the image. As a result, it over, the CNN architecture used in this method cannot model
all boundary pixels globally, resulting in suboptimal results.
ManuscriptreceivedXXX;acceptedXXX.DateofpublicationXXX;date Based on this idea, we envision developing a model that can
ofcurrentversionXXX.(Correspondingauthor:Chee-OnnChow).
effectively model all boundary pixels while preserving the
The authors are with the Department of Electrical Engineering,
Faculty of Engineering, Universiti Malaya, Malaysia. (E-mail: relationships among boundary regions, shadow regions, and
s2121087@siswa.um.edu.my,cochow@um.edu.my,jhchuah@um.edu.my). non-shadow regions, thereby improving the performance of
boundary-based shadow removal methods.
4202
voN
5
]VC.sc[
1v06230.1142:viXraJOURNALOFLATEXCLASSFILES 2
Recently, an improved structured state-space sequence
modelnamedMamba[28]hasbeenintroduced.Itiscapableof
modeling long-sequence relationships with linear complexity
and introduces a novel form of attention through the selective
scanning mechanism, outperforming transformer models in
many tasks [29], [30]. Originally, Mamba was designed for
sequence modeling of one-dimensional (1D) data, employing
1D causal convolutions to process the input. In the field of
computervision,itiscrucialtofullyconsidertherelationships
between pixels in the up, down, left, and right directions,
ensuring that the scanning sequence is spatially continuous (a)Localscan[34] (b)Boundary-regionscan(Ours)
and enhancing semantic continuity. Therefore, many meth-
Fig. 1: The difference between the proposed boundary-region
ods improve the processing performance of two-dimensional
scanning and local scanning. Both the purple window and
(2D) image data by modifying the scanning paths of the
the orange window belong to the boundary region of the
selective scanning mechanism. Vim [31] and Vmamba [29]
shadow, and they have a strong semantic correlation, so
are among the first to apply Mamba to the visual field,
their distance in the long sequence should be close. Whether
designingbidirectionalscanningandhorizontal-verticalcross-
scanning horizontally or vertically, our proposed boundary-
scanning mechanisms. Many Mamba-based visual models
region scanning reduces their distance in the long sequence.
have been proposed based on this, such as Plain-Mamba
[32], Mamba-ND [33], and LocalMamba [34]. These models
have significantly improve Mamba’s performance in image is also applied in our proposed method, in which another
tasks by adjusting the scanning order and increasing semantic branch is employed for global scanning to capture the global
relevance. Based on the previous assumption, if the scanning information of the image and the relationships between the
order in Mamba’s selective scanning mechanism can follow original pixels. Fig.1 provides a detailed explanation of the
theshadowboundariesasmuchaspossible,orbringthepixels differencesbetweentheboundary-regionscanningmechanism
at the shadow boundaries closer in the long sequence, while and the local scanning mechanism.
retaining the information and connections from both shadow The main contributions of this paper are as follows:
and non-shadow regions, it could enhance the performance of
• We introduce the state-space sequence model Mamba to
shadow removal.
image shadow removal for the first time and design a
Most of the existing Mamba-based methods perform se-
U-Net-based model called ShadowMamba. It consists of
quential scanning of the entire image. Although LocalMamba
several Dual-branch Selective Scanning Blocks (DSSB),
[34]captureslocalinformationusingwindows,itstillscansall
with the two branches respectively capturing the global
windows sequentially. For shadow removal, while sequential
information of shadow images and the local information
scanningcanalsobeusedforshadowremoval,itdoesnottake
of boundaries and regions.
intoaccountthespecificcharacteristicsofshadowimages,thus
• We design a Boundary-Region Selective Scanning Mod-
failingtoachievepromisingperformance.Sequentialscanning
ule (BRSSM) that uses the shadow mask to classify the
ignores shadow boundary information, causing the semantic
divided windows into shadow regions, boundary regions,
relationships of pixels at shadow boundaries to be disrupted.
and non-shadow regions. It scans these three types of
Furthermore, sequential scanning causes pixels within the
windows sequentially to ensure the semantic continuity
same region to be separated by pixels from another region,
ofpixelsofthesametype,whichhelpscaptureboundary
affectingthecorrelationbetweenpixelswithinthesameregion
information useful for the shadow removal.
and resulting in longer distances between them in the long
• ShadowMamba outperforms state-of-the-art methods on
sequence,whichhinderstheabilitytoeffectivelycapturetheir
most metrics of the SRD, ISTD, and ISTD+ datasets,
semantic relationships.
particularly demonstrating remarkable effectiveness in
Based on this, this paper designs a new boundary-region
handling soft shadows. This novel architecture has the
selective scanning mechanism by leveraging the character-
potential to challenge the dominance of transformer
istics of shadow images. It is inspired by LocalMamba’s
architectures in the field of shadow removal, offering
local scanning mode [34] and introduces an additional mask
researchers a fresh perspective.
input to divide the image into several windows. According
to the rules designed by this mechanism, the windows are
II. RELATEDWORK
classified into shadow regions, boundary regions, and non-
In this section, we introduce the related work on shadow
shadowregions,andthenrearrangedintoanewsequence.The
removal in images, the relevant work on Mamba, and its
pixel information in the three regions is scanned sequentially
applications in the field of image restoration.
usinghorizontalandverticalcross-scanning.Thissignificantly
enhances the semantic relevance among pixels within the
A. Image Shadow Removal
same type of region, allowing for better capture of detailed
information at the shadow boundaries, thereby improving Currently, the work on shadow removal, apart from a
shadow removal performance. Additionally, global scanning few traditional methods based on physical modeling, mainlyJOURNALOFLATEXCLASSFILES 3
relies on deep learning approaches, primarily based on CNN modeling global information with linear complexity. Through
and transformer architectures, with a few models utilizing hardware-aware algorithms, it enhances runtime speed and
sequential architectures such as RNN and LSTM. surpasses the performance of transformers in most tasks.
TherearevariousCNN-basedmethodsforshadowremoval. The emergence of Mamba seems to have broken the
DHAN [35] enhances the receptive field by improving the monopolyoftransformers,leadingtoasurgeofMamba-based
network structure, achieving better shadow removal results. work across various fields, including text, images, videos,
SP+M+I-Net [17], EMDN [22] and PBID [23] use a linear il- and multimodal applications. The original scanning method
luminationmodel,utilizingCNNstoestimatetheillumination of Mamba is designed for 1D data, and when applied to other
parameters of shadow regions. AEFNet [27] achieves shadow domains,itoverlooksthecontinuitybetweenadjacentpixelsor
removal from the perspective of auto exposure fusion. CA- frames.Therefore,modifyingitsscanningmethodhasbecome
Net [18] and SG-ShadowNet [36] transfer feature information a major research focus. To address the inherent non-causal
from non-shadow regions to shadow regions, enabling the natureofvisualdata,Vim[31]proposesbidirectionalscanning
transformation of illumination information or style transfer and incorporates positional information embedding. Vmamba
for shadow removal. Inpaint4shadow [19] models the shadow [29] introduces a cross-scanning module to achieve 1D selec-
removal problem as a fusion of image inpainting and shadow tive scanning withina 2D image space,incorporating a global
removal, achieving excellent performance by fine-tuning the receptivefield.Plain-Mamba[32]adoptsacontinuouszig-zag
image inpainting network. BM-Net [37] uses shadow gener- scanning pattern, ensuring that each visual token is always
ation to assist the shadow removal process, guiding it with adjacent to the previous scanned token. It also incorporates
invariant image information. BA-ShadowNet [24] leverages learnable parameters into the B matrix of the state equation
boundary information for shadow removal, demonstrating the to achieve directional awareness. Mamba-ND [33] proposes
effectiveness of boundary information in shadow removal. bidirectional cross-scanning and a multi-head mechanism,
Theattentionmechanismoftransformersexcelsatmodeling emphasizing that a key element for Mamba’s adaptation to
the relationships between shadow and non-shadow regions to multi-dimensional data is the design of sequence permuta-
achieve the transfer of illumination information. SpA-former tions. LocalMamba [34] performs local scanning within each
[38] and TSRformer [39] enhance the receptive field by window and then sequentially scanned across the windows,
modifying the network structure for direct shadow removal. significantly enhancing the model’s ability to capture detailed
CRFormer [20] and Shadowformer [21] combine the char- local features. It then used a spatial channel attention module
acteristics of transformers and the shadow removal, utilizing to select the most suitable scanning combinations, filtering
a cross-region attention mechanism to compute a correction out redundant information. Videomamba [43] and Vivim [44]
matrix that adjusts self-attention weights, thereby indirectly propose three-dimensional (3D) bidirectional selective scan-
achievingthetransferofinformationfromnon-shadowregions ning and spatiotemporal selective scanning when processing
to shadow regions. 3D data, enhancing model performance by strengthening the
In addition, there are generative models based on the afore- semantic relevance between frames.
mentioned architectures. Recently, diffusion-based shadow
removal methods have gradually replaced GAN-based ap- C. Applications of Mamba in Image Restoration
proaches. LFG-Diffusion [40] obtains latent features by mini-
In the field of image restoration, many tasks have also
mizing the difference between the feature spaces of shadowed
adopted Mamba models and achieved outstanding results.
and non-shadowed images, as latent features contain more
VmambaIR [45] proposes omnidirectional selective scanning,
useful information compared to explicit features. The unsu-
capable of modeling the information flow from different
pervised model BCDiff [25] models the shadow, non-shadow,
feature dimensions. MambaIR [30] introduces local enhance-
and boundary regions, and utilizes the underlying reflectance
ment and channel attention mechanisms to improve Mamba
intheshadowregiontomaintainstructuralconsistencyduring
for image restoration. Pan-Mamba [46] employs a gating
diffusion sampling.
mechanism for multi-input feature fusion, focusing on the
pansharpening of remote sensing images. Retinexmamba [47]
B. State Space Models divides the overall architecture into an illumination estimator
and a damage restorer for low-light image enhancement.
Recently,state-spacemodels(SSMs)havegainedsignificant
FreqMamba[48]utilizeslocalscanningtoscanwaveletpacket
attention in the field of deep learning, emerging as a strong
decomposed frequency band images, employed the cross-
competitor to CNNs and transformers. SSM originates from
scanningmechanismtoscantheoriginalimage,andcombined
modern control theory and is a model that describes the state
Fouriertransformtoprocessglobalinput,ultimatelyproducing
representation of sequences at each time step and predicts
rain-free images. These studies have highlighted Mamba’s
their next state. Since the original SSM theory deals with
potential and importance in advancing the field of image
continuous functions, Hippo [41] discretizes it and enabled
restoration.
parallelization for training and inference. S4 [42] transforms
the Hippo matrix into a normal matrix and a low-rank matrix,
improving computational efficiency. Mamba [28] builds on
III. METHODOLOGY
previous work by incorporating a selective scanning mech- In this section, the preliminary knowledge related to state-
anism to implement a novel attention mechanism, capable of space models is first introduced. Next, the overall architectureJOURNALOFLATEXCLASSFILES 4
ofShadowMambaispresented,whichconsistsofseveraldual- F ∈RC×H×W from the input. Subsequently, these shallow
in
branchselectivescanningblocks(DSSB)inaU-Netstructure. features are fed into a U-Net structure composed of several
The DSSB captures global information and boundary region DSSBs, with the decoded output being F ∈ RC×H×W.
out
information through two separate paths and performs channel Finally,alinearmappinglayerisappliedtoobtaintheresidual
modeling on the merged features to enhance the model’s I ∈R3×H×W betweentheoutputimageandtheinputimage.
r
understandingcapability.Finally,adetailedexplanationofthe
boundary-region selective scanning mechanism is provided. C. Dual-Branch Selective Scanning Block (DSSB)
This mechanism uses a mask to distinguish the regions to The DSSB is the core component of ShadowMamba, and
whichthewindowsbelong,thenrearrangestheoriginalimage it consists of two parts: the feature extraction part and the
sequence based on the classification and performs cross- feature selection part. The feature extraction part is com-
scanning on the sequence. posed of the Global State-Space Module (GSSM), Boundary-
Region State-Space Module (BRSSM), and Channel State-
A. Preliminaries Space Module (CSSM), which are responsible for extracting
global information, boundary region information, and channel
SSM is a mathematical model used to describe dynamic
information, respectively. The feature selection part regulates
systems,utilizingstate-spacevariablestorepresenttheinternal
the flow of information through the Efficient Feed-Forward
state of the system. Mathematically, it is typically represented
Network(EFFN),whichprocesseshiddenlayerfeaturesusing
by both the state equation and the output equation:
deep convolution and gating mechanisms.
(cid:40) h˙(t)=Ah(t)+Bx(t) 1) GSSM: TheGSSMunfoldspixelsdirectlyandperforms
(1) cross-scanning, capturing global information from multiple
y(t)=Ch(t)+Dx(t)
directions within the shadow image. It acquires useful infor-
where h˙(t) represents the time derivative of the state vector mation fromthe perspectiveof theentire shadow imagewhile
preservingtheoriginalpixelrelationships.Intermsofspecific
h(t), which is the state at the next time step, matrices A and
structuralsettings,theconfigurationofthecoremoduleVSSB
B define the relationship between the state h(t) and the input
x(t) with the next state h˙(t), while matrices C and D define in Vmamba [29] is adopted. The input passes through an
initial linear embedding layer, and the output is divided into
therelationshipbetweenthestateh(t)andtheinputx(t)with
two information streams. One stream goes through a 3 × 3
the output y(t).
depthwise convolution layer and a Silu activation function
The above process is for continuous functions, and in deep
beforeenteringthecoreSSM.TheoutputofSSMisprocessed
learning applications, it must be discretized. For the input
through a layer normalization layer and then added to the
signal, zero-order hold is used [42], and combined with the
output of the other information stream, which has passed
time scale parameter ∆, the continuous parameters A and B
are converted to discrete parameters A¯ and B¯, the discrete through the Silu activation. This combination produces the
final output.
form is represented as follows:
2) BRSSM: TheBRSSMincorporatestheboundary-region
h˙(t)=A¯h(t)+B¯x(t) selective scanning mechanism we designed. It reorders pixels
y(t)=Ch(t)+Dx(t)
from different regions using a window classification method
(2) and then performs a local scan [34] on these windows. This
 BA ¯¯ = =e ∆∆ AA
−1(e∆A−I)·∆B
m asec ch loa sn eis am
s
pen os su sir be ls et wha itt hp oi ux tel bs eiw ngith si en pat rh ae tes dam bye pre ixg eio lsn frs ota my
other regions, thereby enhancing their semantic relevance.
Mamba [28] has designed a selective scanning mechanism
Additionally,thelocalscanmethodbettercapturesthedetailed
that stands out in the SSM framework. This mechanism can
information at boundaries, thereby improving the shadow
dynamically adjust the B and C matrices based on different
removalperformance.Inthenextsubsection,adetailedexpla-
inputs, allowing it to automatically select important features.
nationoftheboundary-regionselectivescanningmechanismis
It achieves a novel attention mechanism that maintains linear
provided.Intermsofspecificstructuralsettings,itisthesame
computational complexity while having a global receptive
as GSSM. After merging the output features of GSSM and
field.
BRSSM, they are sent to CSSM for the next step of channel
scanning.
B. Overall Architecture
3) CSSM: ThemergedfeaturesofGSSMandBRSSMpro-
The U-Net architecture [49] is chosen to design the Shad- duces numerous channels with different feature information,
owMamba because it can capture global features at different where some channels may carry more boundaries information
scales. More importantly, different scales can obtain different and lighting variations than others that are crucial for image
combinations of region classifications, enabling the extraction shadowremoval.CSSMutilizesselectivescanningforchannel
of useful boundary and region information. Fig.2 shows the modeling, enhancing the correlation between channels. This
overall architecture of ShadowMamba. allows dynamic adjustment and control of channel features,
Specifically, given a shadow input I ∈ R3×H×W and ensuringthatthemodelcanoptimizeitsprocessingstrategyin
s
its corresponding shadow mask M ∈ R1×H×W, an over- realtimebasedonchangesintheinputinformation,highlight-
lappedembeddingisfirstappliedtoobtaintheshallowfeature ing and reinforcing the most useful channel features. In termsJOURNALOFLATEXCLASSFILES 5
Fig.2:ThearchitectureoftheproposedShadowMamba.Theshadowimageandtheshadowmaskarejointlyinput,andshallow
featuresareextractedthroughconvolutionoperations.ThesefeaturesarethenfedintoaU-Netarchitecture,whichiscomposed
of several of our proposed DSSBs. Within the DSSB, there are two paths: the GSSM path captures global information through
cross scan, and The BRSSM path reorders the windows based on the mask to capture the boundary and region information
of the shadow image. After reshaping the features from both paths, the CSSM models the channel information. Finally, the
EFFN module uses convolution operations and a gating mechanism to achieve selective control over the information.
of specific structure, a pooling operation is first applied to Incontrast,thereisasignificantluminancedifferencebetween
the features of all channels, followed by scanning the features shadow and non-shadow regions, which can be understood as
in both forward and backward directions along the channel weaker semantic relevance.
dimension. Finally, the information from channel modeling If the original Mamba [28] or VMamba [29] scanning
and spatial modeling is fused using a residual approach. methods are followed, during the scanning process, pixels of
4) EFFN: Similar to [45], the EFFN structure uses 1x1 thesameregiontypeareofteninterruptedbypixelsfromother
convolution to map features into a high-dimensional space, regions,leadingtothedispersionofshadowboundaryorother
then processes the hidden layer features using depthwise regionpixelsthroughoutthelongsequence.Ensuringsemantic
convolution and a gating mechanism. Finally, 1x1 convolu- continuity between pixels of the same region is crucial for
tion is employed to map the features back to their original shadow removal, especially in boundary regions.
dimensions.Bycontrollingtheflowofinformation,EFFNcan Inspired by LocalMamba [34], the boundary-region selec-
highlight important features and suppress irrelevant ones. tivescanningmechanismadoptswindowsasthebasicunitfor
sorting. This not only allows the shadow mask to differentiate
D. Boundary-region selective scanning mechanism between shadow and non-shadow regions but also classifies
the boundary regions. After that, a local scan is performed
The boundary-region selective scanning mechanism is the
on the reordered windows. This method ensures that pixels
core of our proposed method, specifically designed to cater to
of the same type are closer together in the long sequence,
imageshadowremoval.Whenunfoldinga2Dimageintoa1D
thereby enhancing their respective semantic relevance. Addi-
sequence, the arrangement of the sequence is very important,
tionally,itimprovesthemodel’sunderstandingoflocaldetails,
a fact that has been demonstrated in much research. Although
especially boundary details, thus enhancing shadow removal
Mamba can perform global modeling on long sequences, if
performance. Fig.3 illustrates the working principle of the
semantically related pixels are far apart in a long sequence,
boundary-region selective scanning mechanism.
it can negatively impact the performance. In other words, the
Specifically,theinputimageanditscorrespondingmaskare
closer semantically related pixels are in a long sequence, the
first divided into windows of the same size. Then, the shadow
more beneficial it is for modeling in the state-space model.
mask windows are classified according to the following rules:
Shadow boundaries are crucial information in shadow re-
moval, so it is necessary to ensure that the semantics of the

pixels at the shadow boundaries remain as continuous as pos- 0, if P(W)={0},
sible, or that their distances in the long sequence are as close f(W)= 1, if P(W)={0,1}, (3)
aspossible.Luminanceisthemostimportantfactorinshadow 2, if P(W)={1}.
removal. In shadow images, the pixels within the shadow
region are semanticallyrelated due to theirsimilar luminance. P(W) represents the set of pixel values in window W.JOURNALOFLATEXCLASSFILES 6
(a) (b) (c) (d)
Fig. 3: The diagram illustrates the principle of the boundary-region selective scanning mechanism. (a) Uses the shadow mask
todividethewindowedregionsanddeterminewhethertheybelongtotheshadowarea,non-shadowarea,orboundaryarea.(b)
Aftersuccessfullyclassifyingthevariouswindowsofthemask,itisappliedtotheoriginalimage.(c)Eachcategoryofwindow
is scanned sequentially, performing horizontal and vertical scans respectively. (d) Visualization of the scanning sequence (In
the actual model, the window size is 8×8, this is just a rough demonstration.)
When P(W) = {0}, all pixels in the window are 0, which IV. EXPERIMENTS
belongs to category 0, indicating that the window is part of A. Implementation Details
the non-shadow region. When P(W) = {0,1}, the window
1) Network: The proposed ShadowMamba was imple-
contains both 0 and 1 (255), which belongs to category 1,
mented in PyTorch with a network width of 32 channels.
indicating that the window is part of the boundary region.
In its U-Net structure, the number of DSSB modules at
When P(W)={1}, all elements in the window are 1 (255),
each layer is [2, 3, 3, 4, 3, 3, 2]. We set the batch size
whichbelongstocategory2,indicatingthatthewindowispart
to 1 and the window size for the boundary-region selective
of the shadow region. Fig.3a also illustrates our classification
scanning mechanism to 8. Training was performed using an
process. Classify all windows and apply this classification
RTX4080GPU,withdataaugmentationtechniquesincluding
information to the shadow image, as shown in Fig.3b. Rear-
horizontalandverticalrotationsandMixupaugmentation[51].
range the windows according to their categories and sequen-
The AdamW optimizer was used to update the learnable
tially scan the non-shadow region windows, boundary region
parameters, with an initial learning rate of 2 × 10−4. The
windows, and shadow region windows, as shown in Fig.3c.
learningratewasgraduallyreducedto1×10−6 usingacosine
This mechanism employs four scanning methods: horizontal,
annealing strategy. More experimental settings can be found
vertical, reverse horizontal, and reverse vertical. The scanning
in supplementary.
methodisthesameforboththewindowinteriorsandbetween
2) Datasets: During the experimental phase, three com-
the windows, allowing for a more comprehensive capture
monly used datasets in the field of shadow removal were
of boundary information in all directions. Fig.3d provides
utilized: SRD [52], ISTD [53], and ISTD+ [54]. The SRD
a visual simulation of the scanning sequence. It is clear
dataset consists of 2,680 training and 408 testing pairs of
that the distances between windows of the same type in the
shadow and shadow-free images, but no masks are provided.
long sequence are close, and this arrangement enhances the
Similar to other methods [21], [40], we used the predicted
semantic relevance between similar pixel blocks. In addition,
masks provided by DHAN [35] for training and testing. The
thismechanismdoesnothandleshadowboundariesseparately
ISTD dataset includes 1,330 training and 540 testing triplets
likeBA-ShadowNet[24].Instead,itretainsthecontentofboth
(shadow image, mask, and shadow-free image). Due to the
shadow and non-shadow regions, allowing them to undergo
inconsistent lighting between the shadow and shadow-free
long-sequencemodeling.Thismaintainsacertainlevelofcor-
images in ISTD, ISTD+ was adjusted using image processing
relationandenhancesthemodel’sunderstandingofbrightness
algorithms to address this issue. ISTD+ contains the same
variations between regions.
number of triplets as ISTD.
3) EvaluationMetrics: Similartopreviouswork[17],[23],
E. Loss Function theRootMeanAbsoluteError(RMAE)intheLabcolorspace
is used as an evaluation metric. Additionally, Peak Signal-
OnlyasingleCharbonnierloss[50]isusedinourproposed
to-Noise Ratio (PSNR) and Structural Similarity (SSIM) are
method to maintain pixel consistency, as shown in the follow-
employed to evaluate image performance in the RGB space.
ing formula:
Note that a lower RMAE indicates better performance, while
(cid:112) higher PSNR and SSIM values represent better results.
L(y,yˆ)= (y−yˆ)2+ϵ2 (4)
where y is the ground truth shadow-free image, yˆ is the B. Comparison with State-of-the-Art Methods
predicted image output. ϵ is a small constant added to avoid The proposed ShadowMamba is compared with the most
numerical instability. popular state-of-the-art shadow removal models, includingJOURNALOFLATEXCLASSFILES 7
TABLE I: The quantitative results of shadow removal using our models and recent methods on SRD datasets. (Bold indicates
first place, and blue indicates second place.)
AllImage(ALL) ShadowRegion(S) Non-ShadowRegion(NS)
Model
RMAE PSNR SSIM RMAE PSNR SSIM RMAE PSNR SSIM
CANet[18] 6.54 27.40 0.883 8.70 32.32 0.966 5.72 30.03 0.936
EMDN[22] 7.59 24.13 0.777 10.35 29.49 0.937 6.53 26.60 0.876
PBID[23] 4.86 30.46 0.949 8.47 33.02 0.976 3.48 35.36 0.982
SG-ShadowNet[36] 4.43 31.24 0.953 7.69 33.79 0.979 3.18 36.29 0.983
BM-Net[37] 4.40 31.80 0.957 7.14 35.15 0.982 3.35 36.18 0.983
Inpaint4shadow[19] 4.04 33.11 0.959 6.27 36.28 0.982 3.19 37.20 0.984
Shadowformer-L[21] 4.04 32.90 0.958 5.90 36.91 0.989 3.44 36.22 0.989
LFG-Diffusion[40] 4.52 31.71 0.954 6.96 35.24 0.979 3.59 35.82 0.981
ShadowMamba(Ours) 3.87 33.63 0.965 5.81 37.29 0.986 3.13 37.52 0.985
TABLE II: The quantitative results of shadow removal using our models and recent methods on ISTD datasets.
AllImage(ALL) ShadowRegion(S) Non-ShadowRegion(NS)
Model
RMAE PSNR SSIM RMAE PSNR SSIM RMAE PSNR SSIM
SP+M+I-Net[17] 7.96 25.01 0.948 10.84 32.89 0.986 7.40 26.11 0.965
CANet[18] 5.56 27.39 0.854 7.45 34.90 0.977 5.19 28.70 0.885
DHAN[35] 5.65 29.11 0.954 7.49 35.53 0.988 5.30 31.05 0.971
AEFNet[27] 5.88 27.19 0.846 7.74 34.71 0.975 5.52 28.61 0.880
CRFormer[20] 6.07 - - 7.32 - - 5.82 - -
EMDN[22] 5.22 29.98 0.944 7.78 36.27 0.986 4.72 31.85 0.965
BM-Net[37] 5.02 30.28 0.959 7.34 35.61 0.988 4.57 32.80 0.976
Shadowformer-S[21] 4.27 31.81 0.967 6.16 37.99 0.990 3.90 33.89 0.980
ShadowMamba(Ours) 4.23 31.79 0.960 5.95 38.01 0.990 3.90 33.78 0.974
TABLE III: The quantitative results of shadow removal using
SP+M+I-Net [17], DHAN [35], AEFNet [27], CANet [18],
our models and recent methods on ISTD+ datasets.
EMDN [22], PBID [23], SG-ShadowNet [36], BM-Net [37],
Inpaint4shadow [19], Shadowformer (ISTD, ISTD+ dataset)
RMAE
Model
[21], and LFG-Diffusion [40]. These models are all open- ALL S NS
source, but due to inconsistencies in evaluation methods in AEFNet[27] 4.2 6.6 3.8
these studies, such as differences in evaluation metrics or SG-ShadowNet[36] 3.4 5.9 2.9
input sizes, the fairness of comparisons can be affected. A PBID[23] 3.3 6.5 2.7
uniform evaluation script is used to compare the result sets BM-Net[37] 3.0 5.6 2.5
BA-ShadowNet[24] 3.0 5.9 2.4
of these open-source models and ShadowMamba, with input
CRFormer[20] 3.3 5.9 2.7
sizesconsistentlysetto256×256toensuremaximumfairness.
Inpaint4shadow[19] 3.4 5.9 2.9
Additionally, there are some non-open-source models that are
Shadowformer-L[21] 2.8 5.2 2.4
highly relevant to our work, for which we directly used the
ShadowMamba(Ours) 2.8 5.8 2.3
resultsreportedintheirpapers,includingBA-ShadowNet[24],
CRformer [20] and Shadowformer (SRD dataset) [21].
1) Quantitativemeasure: TABLEI,II&IIIshowthequan- ods often ignore the penumbra and handle the entire shadow
titativeresultsonthetestingsetsoverSRD,ISTD,andISTD+, regionuniformlywhendealingwithsoftshadows,whichlimits
respectively. It can be seen that our method demonstrates their performance. In complex backgrounds, the similarity in
performanceadvantagesonbothsoftshadowandhardshadow texture between shadow and non-shadow regions decreases,
datasets. and in such scenarios, regional illumination transfer methods
On the SRD dataset [52], our method achieves signifi- like Shadowformer [21] cannot fully utilize their advantages.
cant performance improvements, surpassing previous state- In contrast, boundary-based methods are often more effective
of-the-art models such as Shadowformer-Large [21] and In- in these cases, as they are not affected by complex back-
paint4Shadow [19], as well as the latest LFG-Diffusion [40] grounds and rely solely on boundary information.
method. The SRD dataset has more complex backgrounds On the ISTD dataset [53], ShadowMamba achieves sig-
and contains soft shadow images, making it particularly suit- nificant performance improvements over previous methods.
able for ShadowMamba’s boundary-region selective scanning Theboundary-regionscanningmechanismleveragesboundary
mechanism. By using windows as the basic unit, the mecha- priorstoenhancethemodel’sunderstandingoflocaldetailsin
nism can fully capture boundary information at various scales shadow boundaries, effectively eliminating boundary artifacts
for soft shadows with wide boundaries, thereby automatically and resulting in improved performance. Compared to CNN-
distinguishing the brightness variations between penumbra, based shadow removal methods [17]–[19], [22], [23], [27],
umbra, and non-shadow regions. Non-boundary-based meth- [35]–[37], which inherently lack global modeling capabilitiesJOURNALOFLATEXCLASSFILES 8
Shadow image EMDN [22] PBID [23] SG [36] BM-Net [37] LFG [40] Ours Ground truth
Fig. 4: Qualitative comparison of ShadowMamba with other existing methods in SRD.
TABLE IV: Ablation Study for Investigating the Components
andstruggletocapturethelong-distancerelationshipsbetween
of ShadowMamba on the ISTD Dataset using RMAE.
shadow and non-shadow regions in the image, our model fo-
cusesonboundarieswhilealsoenablingglobalmodeling,thus
RMAE
GSSM BRSSM CSSM EFFN
maintaining the connection between shadow and non-shadow ALL S NS
regions. Compared to transformer-based methods such as 1 ✓ 6.83 7.94 6.62
CRformer[20]andShadowformer-Small[21],ShadowMamba 2 ✓ ✓ 4.80 6.62 4.37
achieves superior performance on most metrics. Notably, 3 ✓ ✓ 6.42 7.78 6.33
ShadowMamba maintains linear complexity while achieving 4 ✓ ✓ ✓ 4.63 6.59 4.25
5 ✓ ✓ ✓ ✓ 4.23 5.95 3.90
globalmodeling,offeringanadvantageovertransformer-based
methods when processing high-resolution images.
On the ISTD+ dataset [54], compared to the boundary-
based shadow removal model BA-ShadowNet [24], which while Fig.6 presents the visual results of different variant
is most similar to our approach, we achieve significant models. The GSSM branch is the cornerstone of the DSSB
improvements across all three metrics. This indicates that, block, ensuring the lower bound of the model’s performance.
compared to directly cropping boundary region pixels for It utilizes cross-scanning to capture global information from
supervision,retaininginformationfrombothshadowandnon- multiple directions within shadow images, leveraging the
shadow regions while globally modeling the boundary region globalreceptivefieldtoobtainusefulinformationfromtheper-
enables the model to better understand brightness variations spectiveoftheentireshadowimage.TheBRSSMbranchisthe
betweenregions,therebymoreeffectivelybalancingbrightness key component of the DSSB block. Its internal region bound-
restoration in shadow areas and removing boundary artifacts. ary selective scanning not only captures local information of
2) Qualitative measure: Tofurtherdemonstratetheadvan- shadowimagesbutalsoenhancesthemodel’sunderstandingof
tagesofShadowMambaovercompetingmethods,visualresult localdetailssuchasshadowboundariesandregions.Asshown
comparisonsontheSRDandISTDdatasetsareshowninFig.4 in the table, the addition of the BRSSM branch significantly
and Fig.5. On the SRD dataset, the proposed method’s visual improves the shadow removal effect. CSSM is the refined
advantages are quite evident. In all three rows of images, it is component of the DSSB block. It integrates and optimizes
clear that the results of ShadowMamba show fewer residual the feature channels of global information and boundary re-
artifacts, with more consistent brightness and more refined gion information, further enhancing the model’s performance.
handling of soft shadow boundaries. On the ISTD dataset, EFFN plays a crucial role in managing the information flow
the proposed method shows fewer shadow boundary artifacts across different levels of the pipeline, and adding EFFN has
compared to other methods, with minimal changes to the improved the model’s performance.
content of non-shadow regions. As shown in the first-row 2) The effectiveness of region boundary scanning: The
images, the proposed method better restores the original color
boundary-region selective scanning mechanism classifies win-
of the person’s clothing.
dowsintothreecategories:boundaryregions,shadowregions,
and non-shadow regions, and then scans each category se-
C. Ablation Study quentially. To validate the effectiveness of this approach in
WeconductedablationstudiesondifferentvariantsofShad- the shadow removal task, two variant models were designed
owMamba using the ISTD dataset to verify the effectiveness to replace the BRSSM module while keeping the other struc-
of each module’s design. tures unchanged. TABLE V presents the results of these two
1) The effectiveness of each component: TABLE IV variants. The first variant model, called mask scan, directly
shows the impact of each component on model performance, divides the image into two regions using a mask and employsJOURNALOFLATEXCLASSFILES 9
Shadow image CANet [18] AEFNet [27] EMDN [22] BM-Net [37] Shadowformer [21] Ours Ground truth
Fig. 5: Qualitative comparison of ShadowMamba with other existing methods in ISTD.
(a)Shadowimage(b)Detectedmask (c)Noise (d)Output
Input 1 2 3 4 5
Fig. 7: Noise in the Detected Shadow Mask.
Fig.6:Visualexamplesoftheresults,thezoom-inregions,and
the zoom-in error maps for ablation study, including shadow
input, and results of five ablation experiments corresponding
not provide shadow masks, it is usually necessary to use
to the No. in TABLE IV
shadow detection models to obtain mask information, such as
TABLEV:AblationStudyofDifferentScanningTypesonthe in the case of SRD [52]. In these situations, noise inevitably
ISTD Dataset using RMAE. appears in the masks, especially in complex shadow scenes,
as shown in Fig.7. If regions are classified based on the
ScanningTypes RMAE ideal conditions mentioned above, it may lead to inaccurate
ALL S NS
region partitioning. Although the noise has some impacts, the
Maskscan 6.24 7.68 6.06
best results are still achieved on the SRD dataset. The main
Localscan 6.13 8.17 5.74
reason is that despite the noise, the distances between most
Boundary-regionscan 4.23 5.95 3.90
windows of the same region type in the long sequence are
alreadyclose,whichisbeneficialforboundary-regionselective
basic cross-scanning to sequentially scan these two regions scanning.Moreover,inscenarioswithsignificantnoise,CSSM
without utilizing local scanning. While this method maintains emphasizes the channel information from the global scan,
semantic continuity within regions, it does not leverage the while EFFN controls the flow of more precise boundary
crucial boundary prior information, resulting in a lack of region scanning information, demonstrating the advantage of
understanding of local details, especially at boundaries. From our dual-branch approach for shadow removal tasks. Finally,
the results, it can be seen that this method slightly improves we attempted to classify the windows by setting a threshold,
performance compared to only using global scanning. The such as designating a window as a shadow region if it
secondvariantmodeldirectlyappliessequentiallocalscanning contains95%shadowpixels,butthisdidnotleadtosignificant
[34]totheentireimagewithoutclassifyingwindows,asshown improvements.
inFig.1a.Althoughthisscanningmethodcaptureslocaldetails 2) ShadowMamba’s advantage for high-resolution im-
of the image, it does not ensure the semantic consistency ages: Currently, shadow removal methods based on trans-
of pixels within regions of the same type, particularly those former architectures have become mainstream, thanks to the
near boundaries, resulting in no significant improvement in ability of the self-attention mechanism to capture correlations
performance.Clearly,incorporatingtheboundary-regionselec- between shadow and non-shadow regions, thereby facilitating
tive scanning mechanism significantly improves performance. illumination information transfer. Although transformers can
This enhancement is due to the closer proximity of boundary performglobalmodeling,theysufferfromhighcomputational
pixels in the long sequence and the effective capture of local complexity. Swin-transformer alleviates this problem to some
boundary details at multiple scales. extentthroughthewindowmechanism,butatthecostoflosing
the global receptive field, which impacts performance. As a
result,bothCRFormer[20]andShadowformer[21],whichare
D. Discussion
based on Vision transformer [55] and Swin-transformer [56],
1) The impact of mask noise: The boundary-region selec- respectively,facesimilarissues.Withtheincreasingresolution
tive scanning mechanism is applied to datasets that provide of images, the computational cost of such models grows ex-
accurate shadow masks. However, for some datasets that do ponentially. Our proposed ShadowMamba, however, achievesJOURNALOFLATEXCLASSFILES 10
global modeling with linear complexity, which provides a [12] Y. Song, Z. He, H. Qian, and X. Du, “Vision transformers for single
significant advantage when handling high-resolution shadow imagedehazing,”IEEETransactionsonImageProcessing,vol.32,pp.
1927–1941,2023.
images.
[13] R. Guo, Q. Dai, and D. Hoiem, “Paired regions for shadow detection
and removal,” IEEE transactions on pattern analysis and machine
V. CONCLUSION intelligence,vol.35,no.12,pp.2956–2967,2012.
[14] L. Zhang, Q. Zhang, and C. Xiao, “Shadow remover: Image shadow
ThispaperpresentsaMamba-basedshadowremovalmodel removalbasedonilluminationrecoveringoptimization,”IEEETransac-
called ShadowMamba, which is the first model to apply tionsonImageProcessing,vol.24,no.11,pp.4623–4636,2015.
[15] G. D. Finlayson, M. S. Drew, and C. Lu, “Entropy minimization for
Mamba for image shadow removal. Considering the char-
shadow removal,” International Journal of Computer Vision, vol. 85,
acteristics of shadows, we design a boundary-region selec- no.1,pp.35–57,2009.
tive scanning mechanism. This mechanism categorizes and [16] H.GongandD.Cosker,“Interactiveshadowremovalandgroundtruth
forvariablescenecategories,”inBMVC2014-ProceedingsoftheBritish
rearranges multiple divided windows of the image, enhanc-
MachineVisionConference2014,2014.
ing the semantic coherence of pixels within similar regions. [17] H.LeandD.Samaras,“Physics-basedshadowimagedecompositionfor
Meanwhile, the window mechanism captures local bound- shadowremoval,”IEEETransactionsonPatternAnalysisandMachine
Intelligence,vol.44,no.12,pp.9088–9101,2021.
ary details across multiple scales, helping the model better
[18] Z. Chen, C. Long, L. Zhang, and C. Xiao, “Canet: A context-aware
understand boundary semantics and thereby improve over- network for shadow removal,” in Proceedings of the IEEE/CVF inter-
all performance. Compared to previous boundary modeling- nationalconferenceoncomputervision,2021,pp.4743–4752.
[19] X. Li, Q. Guo, R. Abdelfattah, D. Lin, W. Feng, I. Tsang, and
based shadow removal methods, our approach enables global
S.Wang,“Leveraginginpaintingforsingle-imageshadowremoval,”in
modeling of all boundary pixels while preserving correlations Proceedings of the IEEE/CVF International Conference on Computer
with shadow and non-shadow regions, thus better balancing Vision,2023,pp.13055–13064.
[20] J.Wan,H.Yin,Z.Wu,X.Wu,Z.Liu,andS.Wang,“Crformer:Across-
shadow region brightness restoration and boundary artifact
regiontransformerforshadowremoval,”ImageandVisionComputing,
removal. Furthermore, our method may also be extended to p.105273,2024.
other image processing tasks that require mask-assisted input, [21] L. Guo, S. Huang, D. Liu, H. Cheng, and B. Wen, “Shadowformer:
Global context helps shadow removal,” in Proceedings of the AAAI
such as image matting and watermark removal.
ConferenceonArtificialIntelligence,vol.37,no.1,2023,pp.710–718.
[22] Y. Zhu, Z. Xiao, Y. Fang, X. Fu, Z. Xiong, and Z.-J. Zha, “Efficient
REFERENCES model-drivennetworkforshadowremoval,”inProceedingsoftheAAAI
conferenceonartificialintelligence,vol.36,no.3,2022,pp.3635–3643.
[1] Y.Zhao,W.Lv,S.Xu,J.Wei,G.Wang,Q.Dang,Y.Liu,andJ.Chen, [23] T. Einy, E. Immer, G. Vered, and S. Avidan, “Physics based image
“Detrsbeatyolosonreal-timeobjectdetection,”inProceedingsofthe deshadowingusinglocallinearmodel,”inProceedingsoftheIEEE/CVF
IEEE/CVF Conference on Computer Vision and Pattern Recognition, conferenceoncomputervisionandpatternrecognition,2022,pp.3012–
2024,pp.16965–16974. 3020.
[2] S. Chen, P. Sun, Y. Song, and P. Luo, “Diffusiondet: Diffusion model [24] K. Niu, Y. Liu, E. Wu, and G. Xing, “A boundary-aware network for
for object detection,” in Proceedings of the IEEE/CVF international shadowremoval,”IEEETransactionsonMultimedia,vol.25,pp.6782–
conferenceoncomputervision,2023,pp.19830–19843. 6793,2022.
[3] J.Jain,J.Li,M.T.Chiu,A.Hassani,N.Orlov,andH.Shi,“Oneformer: [25] L. Guo, C. Wang, W. Yang, Y. Wang, and B. Wen, “Boundary-aware
Onetransformertoruleuniversalimagesegmentation,”inProceedings divideandconquer:Adiffusion-basedsolutionforunsupervisedshadow
oftheIEEE/CVFConferenceonComputerVisionandPatternRecogni- removal,”inProceedingsoftheIEEE/CVFInternationalConferenceon
tion,2023,pp.2989–2998. ComputerVision,2023,pp.13045–13054.
[4] A.Hatamizadeh,Y.Tang,V.Nath,D.Yang,A.Myronenko,B.Land- [26] Z.Liu,H.Yin,X.Wu,Z.Wu,Y.Mi,andS.Wang,“Fromshadowgen-
man,H.R.Roth,andD.Xu,“Unetr:Transformersfor3dmedicalimage erationtoshadowremoval,”inProceedingsoftheIEEE/CVFconference
segmentation,” in Proceedings of the IEEE/CVF winter conference on oncomputervisionandpatternrecognition,2021,pp.4927–4936.
applicationsofcomputervision,2022,pp.574–584. [27] L. Fu, C. Zhou, Q. Guo, F. Juefei-Xu, H. Yu, W. Feng, Y. Liu, and
[5] C.-F.R.Chen,Q.Fan,andR.Panda,“Crossvit:Cross-attentionmulti- S. Wang, “Auto-exposure fusion for single-image shadow removal,”
scalevisiontransformerforimageclassification,”inProceedingsofthe in Proceedings of the IEEE/CVF conference on computer vision and
IEEE/CVFinternationalconferenceoncomputervision,2021,pp.357– patternrecognition,2021,pp.10571–10580.
366. [28] A. Gu and T. Dao, “Mamba: Linear-time sequence modeling with
[6] H.Touvron,P.Bojanowski,M.Caron,M.Cord,A.El-Nouby,E.Grave, selectivestatespaces,”arXivpreprintarXiv:2312.00752,2023.
G. Izacard, A. Joulin, G. Synnaeve, J. Verbeek et al., “Resmlp: Feed- [29] Y. Liu, Y. Tian, Y. Zhao, H. Yu, L. Xie, Y. Wang, Q. Ye, and Y. Liu,
forward networks for image classification with data-efficient training,” “Vmamba: Visual state space model,” ArXiv, vol. abs/2401.10166,
IEEEtransactionsonpatternanalysisandmachineintelligence,vol.45, 2024. [Online]. Available: https://api.semanticscholar.org/CorpusID:
no.4,pp.5314–5321,2022. 267035250
[7] Z. Lu, J. Li, H. Liu, C. Huang, L. Zhang, and T. Zeng, “Transformer [30] H. Guo, J. Li, T. Dai, Z. Ouyang, X. Ren, and S.-T. Xia, “Mambair:
for single image super-resolution,” in Proceedings of the IEEE/CVF Asimplebaselineforimagerestorationwithstate-spacemodel,”arXiv
conferenceoncomputervisionandpatternrecognition,2022,pp.457– preprintarXiv:2402.15648,2024.
466. [31] L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, and X. Wang, “Vision
[8] X. Chu, L. Chen, and W. Yu, “Nafssr: Stereo image super-resolution mamba:Efficientvisualrepresentationlearningwithbidirectionalstate
usingnafnet,”inProceedingsoftheIEEE/CVFconferenceoncomputer spacemodel,”arXivpreprintarXiv:2401.09417,2024.
visionandpatternrecognition,2022,pp.1239–1248. [32] C.Yang,Z.Chen,M.Espinosa,L.Ericsson,Z.Wang,J.Liu,andE.J.
[9] W.Wu,J.Weng,P.Zhang,X.Wang,W.Yang,andJ.Jiang,“Uretinex- Crowley, “Plainmamba: Improving non-hierarchical mamba in visual
net:Retinex-baseddeepunfoldingnetworkforlow-lightimageenhance- recognition,”arXivpreprintarXiv:2403.17695,2024.
ment,”inProceedingsoftheIEEE/CVFconferenceoncomputervision [33] S. Li, H. Singh, and A. Grover, “Mamba-nd: Selective state space
andpatternrecognition,2022,pp.5901–5910. modelingformulti-dimensionaldata,”arXivpreprintarXiv:2402.05892,
[10] Y.Cai,H.Bian,J.Lin,H.Wang,R.Timofte,andY.Zhang,“Retinex- 2024.
former: One-stage retinex-based transformer for low-light image en- [34] T.Huang,X.Pei,S.You,F.Wang,C.Qian,andC.Xu,“Localmamba:
hancement,”inProceedingsoftheIEEE/CVFInternationalConference Visualstatespacemodelwithwindowedselectivescan,”arXivpreprint
onComputerVision,2023,pp.12504–12513. arXiv:2403.09338,2024.
[11] X. Chen, H. Li, M. Li, and J. Pan, “Learning a sparse transformer [35] X. Cun, C.-M. Pun, and C. Shi, “Towards ghost-free shadow removal
networkforeffectiveimagederaining,”inProceedingsoftheIEEE/CVF viadualhierarchicalaggregationnetworkandshadowmattinggan,”in
Conference on Computer Vision and Pattern Recognition, 2023, pp. ProceedingsoftheAAAIConferenceonArtificialIntelligence,vol.34,
5896–5905. no.07,2020,pp.10680–10687.JOURNALOFLATEXCLASSFILES 11
[36] J. Wan, H. Yin, Z. Wu, X. Wu, Y. Liu, and S. Wang, “Style- Xiujin Zhu received his B.E. degree from Guilin
guidedshadowremoval,”inEuropeanConferenceonComputerVision. UniversityofAerospaceTechnology,Guilin,China,
Springer,2022,pp.361–378. in2018,andhisM.S.degreefromtheUniversityof
[37] Y. Zhu, J. Huang, X. Fu, F. Zhao, Q. Sun, and Z.-J. Zha, “Bijective Jinan,Jinan,China,in2021.Heiscurrentlypursuing
mappingnetworkforshadowremoval,”inProceedingsoftheIEEE/CVF a Ph.D. degree in the Department of Electrical
Conference on Computer Vision and Pattern Recognition, 2022, pp. Engineering, University of Malaya. His current re-
5627–5636. searchinterestsincludecomputervisionandpattern
[38] X. Zhang, Y. Zhao, C. Gu, C. Lu, and S. Zhu, “Spa-former: An recognition.
effective and lightweight transformer for image shadow removal,” in
2023 International Joint Conference on Neural Networks (IJCNN).
IEEE,2023,pp.1–8.
[39] H.-E. Chang, C.-H. Hsieh, H.-H. Yang, I. Chen, Y.-C. Chen, Y.-C.
Chiang,Z.-K.Huang,W.-T.Chen,S.-Y.Kuoetal.,“Tsrformer:Trans-
former based two-stage refinement for single image shadow removal,”
in Proceedings of the IEEE/CVF Conference on Computer Vision and
PatternRecognition,2023,pp.1436–1446.
[40] K.Mei,L.Figueroa,Z.Lin,Z.Ding,S.Cohen,andV.M.Patel,“Latent
feature-guideddiffusionmodelsforshadowremoval,”inProceedingsof
theIEEE/CVFWinterConferenceonApplicationsofComputerVision,
2024,pp.4313–4322.
[41] A. Gu, T. Dao, S. Ermon, A. Rudra, and C. Re´, “Hippo: Recurrent
memory with optimal polynomial projections,” Advances in neural
informationprocessingsystems,vol.33,pp.1474–1487,2020.
[42] A.Gu,K.Goel,andC.Re´,“Efficientlymodelinglongsequenceswith
structuredstatespaces,”arXivpreprintarXiv:2111.00396,2021.
[43] K. Li, X. Li, Y. Wang, Y. He, Y. Wang, L. Wang, and Y. Qiao, Chee-Onn Chow (SeniorMember,IEEE)received
“Videomamba: State space model for efficient video understanding,” hisDoctorateofEngineeringfromtheTokaiUniver-
arXivpreprintarXiv:2403.06977,2024. sity,Japan.Heiscurrentlyanassociateprofessorat
[44] Y.Yang,Z.Xing,andL.Zhu,“Vivim:avideovisionmambaformedical theDepartmentofElectricalEngineering,University
videoobjectsegmentation,”arXivpreprintarXiv:2401.14168,2024. ofMalaya.HeisleadingtheMobileComputingRe-
[45] Y.Shi,B.Xia,X.Jin,X.Wang,T.Zhao,X.Xia,X.Xiao,andW.Yang, searchGroup,whichfocusesonresearchrelatedto
“Vmambair: Visual state space model for image restoration,” arXiv wirelesscommunications,communicationnetworks,
preprintarXiv:2403.11423,2024. multimediaapplicationsanddataanalytics.Heisa
[46] X. He, K. Cao, K. Yan, R. Li, C. Xie, J. Zhang, and M. Zhou, seniormemberofIEEEandaregisteredProfessional
“Pan-mamba: Effective pan-sharpening with state space model,” arXiv Engineer(BoardofEngineersMalaysia).
preprintarXiv:2402.12192,2024.
[47] J. Bai, Y. Yin, and Q. He, “Retinexmamba: Retinex-based mamba for
low-lightimageenhancement,”arXivpreprintarXiv:2405.03349,2024.
[48] Z. Zou, H. Yu, J. Huang, and F. Zhao, “Freqmamba: Viewing mamba
fromafrequencyperspectiveforimagederaining,”inACMMultimedia
2024,2024.
[49] O.Ronneberger,P.Fischer,andT.Brox,“U-net:Convolutionalnetworks
for biomedicalimage segmentation,” in Medical imagecomputing and
computer-assisted intervention–MICCAI 2015: 18th international con-
ference,Munich,Germany,October5-9,2015,proceedings,partIII18.
Springer,2015,pp.234–241.
[50] P.Charbonnier,L.Blanc-Feraud,G.Aubert,andM.Barlaud,“Twode-
terministichalf-quadraticregularizationalgorithmsforcomputedimag-
ing,”inProceedingsof1stinternationalconferenceonimageprocessing,
vol.2. IEEE,1994,pp.168–172.
[51] H.Zhang,“mixup:Beyondempiricalriskminimization,”arXivpreprint
Joon Huang Chuah (Senior Member, IEEE) re-
arXiv:1710.09412,2017.
ceivedtheB.Eng.(Hons.)degreefromtheUniversiti
[52] L.Qu,J.Tian,S.He,Y.Tang,andR.W.Lau,“Deshadownet:Amulti-
TeknologiMalaysia,theM.Eng.degreefromtheNa-
contextembeddingdeepnetworkforshadowremoval,”inProceedings
tionalUniversityofSingapore,andtheM.Phil.and
of the IEEE conference on computer vision and pattern recognition,
Ph.D. degrees from the University of Cambridge.
2017,pp.4067–4075.
HeiscurrentlyHeadofVIPResearchGroupatthe
[53] J.Wang,X.Li,andJ.Yang,“Stackedconditionalgenerativeadversarial
DepartmentofElectricalEngineering,Universityof
networks for jointly learning shadow detection and shadow removal,”
Malaya. He was the Honorary Treasurer of IEEE
inProceedingsoftheIEEEconferenceoncomputervisionandpattern
Computational Intelligence Society (CIS) Malaysia
recognition,2018,pp.1788–1797.
ChapterandtheHonorarySecretaryofIEEECoun-
[54] H.LeandD.Samaras,“Shadowremovalviashadowimagedecompo-
cil on RFID Malaysia Chapter. He is the Vice
sition,” in Proceedings of the IEEE/CVF International Conference on
Chairman of the Institution of Engineering and Technology (IET) Malaysia
ComputerVision,2019,pp.8578–8587.
Network. He is a Fellow and the Honorary Secretary of the Institution
[55] A. Dosovitskiy, “An image is worth 16x16 words: Transformers for
of Engineers, Malaysia (IEM). His main research interests include image
imagerecognitionatscale,”arXivpreprintarXiv:2010.11929,2020.
processing, computational intelligence, IC design, and scanning electron
[56] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and
microscopy.
B.Guo,“Swintransformer:Hierarchicalvisiontransformerusingshifted
windows,”inProceedingsoftheIEEE/CVFinternationalconferenceon
computervision,2021,pp.10012–10022.