robotics
Perspective
The Future of Intelligent Healthcare: A Systematic Analysis and
Discussion on the Integration and Impact of Robots Using Large
Language Models for Healthcare
SourenPashangpour1,* andGoldieNejat1,2,3,*
1 AutonomousSystemsandBiomechatronicsLaboratory(ASBLab),DepartmentofMechanicalandIndustrial
Engineering,UniversityofToronto,Toronto,ONM5S3G8,Canada
2 KITE,TorontoRehabilitationInstitute,UniversityHealthNewtork(UHN),Toronto,ONM5G2A2,Canada
3 RotmanResearchInstitute,BaycrestHealthSciences,NorthYork,ONM6A2E1,Canada
* Correspondence:souren.pashangpour@mail.utoronto.ca(S.P.);nejat@mie.utoronto.ca(G.N.)
Abstract: Thepotentialuseoflargelanguagemodels(LLMs)inhealthcareroboticscanhelpad-
dressthesignificantdemandputonhealthcaresystemsaroundtheworldwithrespecttoanaging
demographicandashortageofhealthcareprofessionals. EventhoughLLMshavealreadybeen
integratedintomedicinetoassistbothcliniciansandpatients,theintegrationofLLMswithinhealth-
carerobotshasnotyetbeenexploredforclinicalsettings.Inthisperspectivepaper,weinvestigate
thegroundbreakingdevelopmentsinroboticsandLLMstouniquelyidentifytheneededsystem
requirementsfordesigninghealth-specificLLM-basedrobotsintermsofmulti-modalcommunication
throughhuman–robotinteractions(HRIs),semanticreasoning,andtaskplanning.Furthermore,we
discusstheethicalissues,openchallenges,andpotentialfutureresearchdirectionsforthisemerging
innovativefield.
Keywords: large language models; healthcare robotics; multi-modal communication; semantic
reasoning;taskplanning
Citation:Pashangpour,S.;Nejat,G.
TheFutureofIntelligentHealthcare: 1. Introduction
ASystematicAnalysisandDiscussion
Inhealthcare,theneedfornewtechnologytomaintainthequalityandefficiencyof
ontheIntegrationandImpactof
careisparamount. Thisdemandhasbeenamplifiedbyanincreaseintheoverallolder
RobotsUsingLargeLanguageModels
populationoftheworld. Namely,by2050,22%oftheglobalpopulationisexpectedtobe
forHealthcare.Robotics2024,13,112.
over65yearsold[1].Thisdemographicshiftleadstoarisingprevalenceofchronicdiseases
https://doi.org/10.3390/
suchasdementia,diabetes,andheartdisease,whichrequirecontinuousmonitoringand
robotics13080112
long-termmanagement,furtherstraininghealthcareresources[2,3]. In2022,Canadaalone
AcademicEditor:Po-YenChen
had143,695jobvacanciesforhealthcareprofessionals[4]. IntheU.S.,itispredictedthatby
Received:30May2024 2026therewillbeashortageofupto3.2millionhealthcareworkers[5],highlightingthe
Revised:6July2024 staggeringworkforceshortage[5]. Furthermore,thevastamountsofhealthdatagenerated
Accepted:18July2024 inthissectorrequireefficientmanagementandusetoimprovepatientoutcomesandreduce
Published:23July2024 healthcarecosts,ataskwellsuitedforgenerativeAIanddeeplearningmodels[6–8]. For
example,generativeAImodels,suchaslargelanguagemodels(LLMs),canusethelarge
EHR(ElectronicHealthRecord)datasetsfortrainingtolearnpatternsbetweensymptoms,
diagnoses,andrecommendationsinordertohelpinhealthcarewiththemanagementof
Copyright: © 2024 by the authors.
dataandtheretrievalofinformation,aswellasdecision-makingprocesses[9].
Licensee MDPI, Basel, Switzerland.
The need for new technologies in healthcare is multi-faceted, using the following:
Thisarticleisanopenaccessarticle
(1)classicalanddeeplearningmethodstofacilitatemedicalimaginganalysis[10],(2)deep
distributed under the terms and
neuralnetworks(DNNs)toperformautomateddiseasedetectionandprediction[11],and
conditionsoftheCreativeCommons
(3)LLMsforclinicaldecision-makingandteleconsultation[12]. Inparticular,LLMshave
Attribution(CCBY)license(https://
alreadybeenintegratedintomedicinetoassistwithclinicalnote-takingbyrewritingand
creativecommons.org/licenses/by/
summarizingclinicians’notesforclarityandhavealsobeenleveragedthroughchatbotsto
4.0/).
Robotics2024,13,112.https://doi.org/10.3390/robotics13080112 https://www.mdpi.com/journal/robotics
4202
voN
5
]OR.sc[
1v78230.1142:viXraRobotics2024,13,112 2of43
assistpatientswithremindersandgeneralquestionsaboutmedications[12]. Furthermore,
LLMshavealsobeenusedtopersonalizetreatmentplansforindividualpatientsbyassisting
withEHRdataextraction[13]. TheuseofdeeplearningandLLMshavemanybenefitsand
advancementsinthedeliveryofhealthcare.
Inadditiontotheaforementionedsoftwaretechnologies,innovationsinroboticshave
allowedtheirintegrationintohealthcare,includingthefollowing: (1)surgicalrobotsbeing
usedtoperformvariousminimallyinvasivesurgicalproceduresforbrain,spine,andneck
surgeries,whereaccuracyandreliabilityareparamount[14,15],(2)rehabilitationrobots
in the form of robotic assistive wheelchairs [16], prostheses [17], and robotic arms and
exoskeletonsforlowerandupperlimbrehab[18,19],(3)mobilemedicationdeliveryrobots
utilizedasautomatedmedicationcarts[20],and(4)humanoidrobotsmonitoringpatient
vitalsigns[21]. Theneedforrobotstoassistinhealthcarewasespeciallyevidentduringthe
COVID-19pandemic,wherethehealthcaresectorturnedtoroboticsinresponsetopublic
healthemergenciesforvarioustaskstominimizeperson-to-personcontactandthespread
ofthevirus[22]. ThisincludedautonomousdisinfectingrobotsusingUV-Cirradiation
forsurfacedecontaminationinhospitals[23]andservicerobotsusedtofacilitatesocial
distancingmeasuresinhospitalsandlong-termcarehomesbyconductinginitialscreening
ofCOVID-19symptomsanddetectingfacemasks[23–26]. Socialrobotshavealsobeen
usedtoprovidecompanionshiptoolderadultsinhealthcareenvironments(i.e.,long-term
homes)byengaginginconversations,suchassharingstoriesortellingjokes,thuscreating
amorepersonalandcomfortingpresencebyinteractingwithusers[27,28]. Ingeneral,the
integrationofrobotsintohealthcareaimstoenhancepatientexperiencesandoutcomes,
supportskillaugmentation,andimprovetheoverallqualityofcarewhilereducingthe
workloadofcareproviders[29].
DespitetheuseofbothhealthcarerobotsandLLMsinhealthcare,theintegrationand
deploymentofthesetwotechnologiesstillremainsunexplored. Totheauthors’knowledge,
therehaveonlybeenthreeinstanceswhererobotsandLLMshavebeenapplieddirectly
forhealthcareapplications: twoinstancesinvolvingsocialrobots[30,31]andoneinstance
involvingasurgicalrobot[32]. However, thispotentialmarriageofroboticsandLLMs
forhealthcarepresentsanuntappedopportunity,asthecombinationofarobot’sphysical
capabilitieswiththeunderstandingandgenerativeabilitiesofLLMshasthepotentialto
provideperson-centeredcare,aswellasstreamlineoperationalworkflows(e.g.,logistical
tasks),andreducetheworkloadofhealthcareprofessionals. Thisintegrationwillresultin
theutilizationofvasthealthcaredatatofurtherrefinediagnostic,therapeutic,andpredictive
healthcareservices. ThepotentialofhealthcarerobotsusingLLMstoprovidesuchservices
underlinestheurgentneedforresearchanddevelopmentinthisemergingarea.
Inthispaper,wepresentthefirstinvestigationintotheemergingfieldofhealthcare
robotsusingLLMs.Namely,weexploretheinnovativedevelopmentswhichaimtoaddress
thechallengeofenhancingthequalityandefficiencyofpatientcareduringatimewherethe
demographicisshiftingtowardsanolderpopulationandthereisashortageofhealthcare
professionals. Ourobjectiveistoidentifytheneededsystemrequirementsformulti-modal
communicationthroughhuman–robotinteractions(HRIs),systematicreasoning,andtask
planningfordesigninghealth-specificLLM-basedroboticsolutions. Wewillalsodiscuss
theethicalissuesassociatedwiththepotentialdevelopmentandutilizationofhealthcare
robotsleveragingLLMsandtheopenchallengesandpotentialfutureresearchdirections
forthisfield.
2. LargeLanguageModels(LLMs)forHealthcare
TheversatilityofgenerativeAIisapparentinitsabilitytobetrainedonanarrayof
datatypesfromtextual(i.e.,EHRs)[33]andvisualcontent(i.e.,medicalimagingdata)[34]
togeneticsequences[29]inordertolearnandcapturetheunderlyingpatternsanddis-
tributions from such data. This makes it especially valuable for healthcare tasks that
requireadaptabilityandcontinuouslearning[30]. LLMsrepresentasignificantadvance-
mentingenerativeAI.Theyprimarilyutilizethetransformerarchitecture[35]andcontainRobotics2024,13,112 3of43
keyfeaturesincludingmulti-headattentionforparallelprocessing,positionalencodings
forsequenceawareness,layernormalization,andfeedforwardnetworksfordatarefine-
ment[36]. Theencoder–decoderstructureinmanyLLMsfacilitatescomplextaskssuchas
languagetranslationandcontentgeneration[37]. Thetransformerarchitectureprovides
LLMswiththepotentialtobeinstrumentalinhealthcareroboticdevelopmentduetothe
proficiencyingeneratinghuman-liketext[38],understandingofthelexicalsemanticsof
thephysicalworld[36,39],andmakingdecisionsregardingappropriaterobotbehaviorsto
implement[40].
Table 1 presents an overview of five specialized LLMs designed for healthcare:
(1) BiomedBERT [41], (2) MED-PaLM 2 [42], (3) DRG-LLaMA [43], (4) GPT2-BioPt [44],
and (5) Clinical-T5 [45]. It also consists of seven general LLMs which have been incor-
porated into robotic tasks for HRIs, semantic reasoning, and planning: (1) GPT-3 [46],
(2)GPT-3.5[46],(3)GPT-4[47],(4)T5[48],(5)DialogLED[49],(6)PaLM[50],and(7)the
Stanford Alpaca [51]. Furthermore, Table 1 includes two popular general-use LLMs:
(1) BERT [52] and (2) PaLM 2 [53]. To date, the specialized LLMs have not yet been
deployed in robotics for healthcare applications. However, they have the potential to
be applied to tasks such as diagnostic assistance, patient data analysis, and treatment
andprocedurerecommendations. ThesespecializedLLMsarecreatedbyfine-tuningthe
modelparametersoftheirrespectivebasemodels,namely,BERT[52],PaLM[50],PaLM
2[53],LLaMA2[54],GPT2[55],andT5[48],onhealthcaredatasetssuchasMedQA[56],
MedMCQA [57], and PubMedQA [58]. It is important to note that these base models
are considered foundational models, which can be either open-source or closed-source.
Open-sourcemodelsarethosewhoseunderlyingcodeandtrainingproceduresarefreely
availableforuse,modification,anddistributionbyanyone,facilitatingtransparencyand
collaborationinmodeldevelopment[59]. Incontrast,closed-sourcemodelsareproprietary,
withtheircode,methodologies,anddataoftenkeptconfidentialbytheorganizationthat
developedthem,limitingaccessandmodificationbyexternalentities[60]. Thisdistinction
iscrucial,asitinfluencesthebreadthofapplicationandcustomizationpotentialofeach
modelinhealthcaresettings. Moreover,thesebasemodelsarebuiltfromthegroundup,
havinguniquearchitectures,andtheyaretrainedonexpertlycurateddatasetstogivethem
generalknowledgeabouttheworld[61]. Thetrainingdatasizeisoftenreportedingiga-
bytes,whilethecontextlength(promptlength)isreportedastokenswhichrepresentthe
variablenumbersofcharactersdependingonthetokenizationprocessused. Thesizeand
thesourceofthetrainingdata(i.e.,chatforums,scholarlyjournals)arewhatgivesbreadth
(generalknowledge)tothemodel,whilethecontextlengthdictatesthesizeoftheinputto
themodelandconstitutesthecontextforwhatthemodelgenerates[62,63]. Thegeneral
LLMsreportedinTable1arethemodelsthatwillbediscussedinthefollowingsectionsin
thispaperwithrespecttotheircurrentuseinroboticsandtheirpotentialimplicationsfor
healthcare. WewillpresenttwodesignstudiesinSection8showcasinghowthesethree
typesofLLMscanbepotentiallyusedforhealthcareroboticsapplications.
Thereareseveralcommonpromptingtechniquesthatareessentialfortheinteraction
betweenroboticsystemsandLLMswhichcanalsobeextendedtohealthcaresettings. The
promptsserveastheconduitthroughwhichqueriesortasksarecommunicatedtoLLMs
throughagents(peopleorrobots),withthemodelgeneratingresponsesbasedonthegiven
promptstructure. OutputsfromLLMsareprobabilistic,leadingto‘promptengineering’,
wheredifferentpromptstructuresareusedtobiastheoutputoftheLLMtowardsade-
sired outcome [64]. Moreover, the context window of an LLM refers to the contiguous
sequenceoftokensconsideredbytheLLMinaforwardpass(i.e.,generationtime)[65].
Thiswindow,typicallylimitedbymemoryconstraints,determinestheamountofpreceding
andsucceedingtextdatathemodelutilizestomaintaincoherenceandaccuracyintasks
suchaswordprediction[66]. Thisisevidentinfew-shotprompting,whereinput–output
pairsprovidedtothemodelfillitscontextwindowwithconsistentstructures,reducing
variationsintheinputtext[67,68]. Thisuniformityinthecontextwindowinfluencesthe
attentionmechanismtofocusmorenarrowly,enhancingtherelevanceofcertainpartsofRobotics2024,13,112 4of43
the input tothe generated output [69,70]. Therefore, the effectiveness of apromptand,
consequently,theutilityofthemodel’sresponsearehighlycontingentupontheprompt’s
design. Apromptingmethodiscriticalforachievingspecificobjectives. Table2describes
keypromptingstrategies,detailingtheiroperationalframeworks,exemplaryapplications
wehaveidentifiedwithinhealthcare,andtheirrespectiveadvantagesandlimitations. It
is worth noting that prompts can be multi-modal, including text, images, or audio. In
orderforLLMstorespondtoavarietyofinformationalinputs[71],thenon-textualcompo-
nentsareconvertedintotextualdescriptionsthroughpreprocessingmodelssuchasCLIP
whichalignsimagesandtextinajointembeddingspace[72]orSCANREFwhichaligns
pointcloudsandsentencesinajointembeddingspace[73]. Thismulti-modalintegration
broadensthescopeofLLMapplications,enhancingtheiradaptabilityandeffectivenessin
complexhealthcareenvironmentswherediversedatatypesareprevalent,suchasmedical
imagingdata,recordedconversationsbetweenpatientsandhealthcareprofessionals,and
dedicatory images such as maps of healthcare environments or medication labels. The
promptingtechniquespresentedinTable2integrateLLMsintorobotics,asdiscussedin
theliteratureinthesubsequentsections. TheexamplepromptsprovidedbyusinTable2
highlighttheirpotentialapplicationsinhealthcare.
Table1.LLMswithpotentialapplicationsinhealthcare.
Foundational PapersThatUse
ModelName ParameterSize ContextLength TypeofTrainingData
Model theModel
MedQA,MedMCQA,
MED-PaLM-2[42] 540B 8196 HealthSearchQA,LiveQA,and PaLM2[53]
MedicationQA
236,192MIMIC-IVdischarge
DRG-LLaMA[43] 7B,13B,70B 4096 LLaMa2[54]
summaries
PorTuguese-2withbiomedical
GPT2-BioPT[44] 124M,770M, 1024 GPT-2[55]
literature[44]
Variable,
220M,770M,3 Approximately2milliontextualnotes
Clinical-T5[45] memory- T5[48]
B,11B fromMIMIC-III
constrained
BREATHEcontainingresearcharticles
andabstractsfromdifferentsources
BiomedBERT[41] 110M,340M 512 (BMJ,arXiv,medRxiv,bioRxiv, BERT[52]
CORD-19,SpringerNature,NCBI,
JAMA,andBioASQ)
BookCorpus(800Mwords)and
BERT[52] 110M,340M 512 NA
EnglishWikipedia(2500Mwords)
60M,220M,770 750GBofColossalCleanCrawled
T5[48] VariableLength NA [31,74,75]
M,3B,11B Corpus(C4)
GPT-3[46] 175B 4096 Notprovided NA [76–80]
GPT-3.5[46] 175B 4096 Notprovided NA [31,32,81–84]
GPT-4[47] 1.8T 128,000 Notprovided NA [74,85,86]
Socialmediaconversations
(multilingual):50%;
filteredwebpages(multilingual):27%;
PaLM[50] 8B,62B,540B 2048 books(English):13%; NA [87]
GitHub(code):5%;
Wikipedia(multilingual):4%;
news(English):1%
Not
PaLM2[53] Notavailable Notavailable NA
available
Amixofpubliclyavailableonline
StanfordAlpaca[51] 7B 4096 dataandsyntheticdatageneratedby NA [88]
GPT-3
Books,EnglishWikipedia,realnews,
DialogLED[49] 41M 4096 NA [31]
andstoriesRobotics2024,13,112 5of43
Table2.PromptingtechniquesforLLMsforhealthcare.
PapersThatUse
PromptingMethod PromptDescription ExamplePrompt Advantages Disadvantages
Method
Maynotelicitdetailed
Straightforward,clear,
‘Whataretheprimary ornuancedresponses;
Directquestioningabout andeasytounderstand.
Directquestioning symptomsofType2 limitedtotheuser’s
atopicofinterest. Effectiveforfactual
diabetes?’ knowledgetoaskthe
inquiries.
rightquestions.
Breaksdowncomplex
processesinto
‘TodeterminetheBody
Aproblemispresented, understandablesteps;
MassIndex(BMI),first Canbetime-consuming;
followedbya usefulforteachingand
Chainofthought dividetheweightin requiresaccurateinitial [75,86,87]
step-by-stepreasoning clarification.This
kilogramsbytheheight logictobeeffective.
processtosolveit. approachcanhelpthe
inmeterssquared.’
modelincomplex
problem-solvingtasks.
Providinglittletono Responsesmaylack
context(zero-shot)to ‘Describetheprocessof Teststhemodel’sability contextorspecificity;
Zero-shot guidetheLLMonhow cellularrespirationin torespondbasedonits dependentonthe [31,74,75,81,84,85]
torespondorwhat humancells.’ pre-trainedknowledge. model’sexisting
formattofollow. knowledge.
‘[Example1:‘Anapple
isafruitthatcanhelp Providescontext
Givingafewexamples Thequalityofthe
withdigestion’.] throughexamples;
(few-shot)toguidethe responsedependsonthe
Few-shotlearning [Example2:‘Atreadmill improvestheaccuracy [31,32,76]
LLMonhowtorespond qualityoftheexamples
isadeviceusedfor andrelevanceof
orwhatformattofollow. provided.
physicalexercise’.] responses.
Whatisanultrasound?’
Incorporatingemotional ‘It’scrucialformy
Overexaggerationin
cuestopromptsand/or family’swell-being.Can Resultinmoreengaging
emotionalstimuliand
EmotionPrompt[89] askingtheLLMto youprovideadviceon andlessgenericLLM [82,83,86]
indicationtoexcessive
emphasizeemotion maintainingabalanced outputs.
gestures.
stimulusinitsoutput. dietforhearthealth?’
Incorporatingmorethan
justtextintheprompts,
likeimagesordata,
‘HereisanMRIimageof Incorporatesdifferent RequiresLLMmodels
especiallyinmodelsthat
aknee.Canyouexplain datatypesforamore capableofprocessing
canprocessmulti-modal
Multi-modalprompting thecommoninjuries holisticunderstanding; andinterpreting [77,85]
inputs.Thisisusefulfor
indicatedbythistypeof usefulfordiagnostics multipledatamodes
tasksthatrequire
scan?’ andtreatmentplanning. effectively.
interpretationacross
differenttypesof
information.
‘Yourroleistogenerate
roboticplansinaX
embodiedrobotcapable
Combinationof of<primitiveactions: EnablesLLMsfor
previousprompting moveTO(location,grasp integrationwithrobotic
Requiresexpert
methodswiththe (Object),scan()> systems.EnablesLLMs
programmingto
additionofprimitive Thecurrentstateofthe tobeusedtogenerate
Task-oriented integrateintoan
robotactionsand robotis${state}, roboticplanstakinginto [32,76,78–80,84,86–88]
prompting autonomoussystem.
feedbackfromtherobot generateroboticplans considerationthe
Limitedbycontext
anditsoperating bygeneratingpythonic abilitiesofarobotand
lengthofLLMmodels.
environment,aswell codewiththeuseof theconditionsinthe
userrequest. primitiveaction environment.
functions.
Theuserisrequestioned
${userRequest).’
3. Human–RobotInteraction(HRI)andCommunication
ThefieldofHRIforhealthcareisfocusedonthedevelopmentofappropriatedesign
and implementationstrategies for robots to assistwith differenttasks for a widerange
of users from healthcare professionals [23] to patients [89]. Therefore, HRI approaches
requireroboticsystemstounderstandandadapttotheneedsofusers. ThetypesofHRIs
used in healthcare applications include the following: (1) teleoperation, e.g., through
graphical user interfaces for socially assistive robots to provide therapy and cognitive
interactions[90,91];(2)socialinteractionsusingnaturalcommunicationmodes,forexample,
forcompanionship[92]; and(3)theuseofmechanicalinterfacesused, forexample, for
surgical robots [93]. Surgical robots require mechanical interfaces (joysticks, switches,
etc.) whichcanresultinhighcognitiveworkloadsforsurgeons[94]. Ontheotherhand,
socialHRIutilizesnaturallanguageprocessing(NLP)torecognizeuserverbalrequestsRobotics2024,13,112 6of43
andcommandsinordertoprovideremindersandtoengageinconversations[95]. These
requestsandcommandsaredependentspecificallyonpronunciationandwordchoices,
whichcanpotentiallyresultinincorrectcommandcallsbeingdetected[96].
Ingeneral,itisimportantthatinteractivehealthcarerobotshaveintelligentcommu-
nicationabilitiesusingmultiplemodalitiessuchasspokennaturallanguage,gaze,facial
expressions,andillustrativegesturesinordertobeabletorecognizetheintentofauser
andeffectivelyconveytheirownintentusingthesemodes. Currenthealthcarerobotsare
capableofbothsingle-andmulti-modalinteractions;however,theystillhavenotyetbeen
widely adopted in hospitals, clinics, and/or long-term care homes due mainly to their
linguisticlimitations,whichcanleadtocriticalproceduralmistakes(misinterpretationof
userrequests)and/orfrustrationandlackoftrustinthetechnology[97].
LLMscanaddresstheaforementionedchallengesinordertoimproveuserexperiences
andtheintenttousethetechnology[98]. TheabilityofLLMstogeneratedynamicmulti-
modalcommunicationanddetectandutilizeemotionalcuesthroughtheuseofemotional
promptingtechniquessuchasEmotionPrompt[99]choreographedbyanLLMwillresultin
naturalHRIsthatwillbesimilartohuman–humaninteractions[100]. Theintroductionof
LLMsinhealthcarerobotswillaimtoimproveHRIviathecohesionbetweenmultiplecom-
municationmodes;however,todate,theuseofLLMsinHRIhasmainlyonlyconsidereda
singlemode[101].
3.1. Single-ModalCommunication
Single-modalrobotcommunicationinhealthcareapplicationshasmainlyconsisted
ofeithertextualorverbalinformationexchange[102,103]. Aprimarychallengeforsuch
single-mode communication is ensuring that a robot does not become monotonous or
repetitive,asthiscanreduceuserengagementandtrustandalsonegativelyimpactthe
adoptionofhealthcarerobots[104].
In[76],GPT-3wasintegratedintothe‘Mini’smallcharacter-likesocialrobottoprovide
companionship to older adults with mild cognitive impairment. The robot engaged in
cognitivestimulationgamesandgeneralconversationswiththeolderadults. TheBabbage
andDavinciversionsofGPT-3wereusedtogenerateuser-adaptedsemanticdescriptions
andtoparaphraseprewrittentextsinSpanish. Thisintegrationwastailoredtoachieve
asingleconsistentmodeofcommunication,enhancingtherobot’scapabilitiesinnatural
andadaptivedialogues. Theresearchhighlightsthepotentialofstreamlinedadaptable
interactionsinsocialroboticsmadepossiblebythecapabilitiesofGPT-3tocreatetailored
responses. The use of GPT-3, despite requiring a translation step from English, was
justified by its high performance and adaptability to the specific requirements of the
applicationcomparedtoothermodelsincludingT5multilingual[105],PEGASUS[106],
andBERT2BERT[107].
In[32],anaturallanguageinterfaceusingGPT-3.5wasimplementedintoadaVinci
surgical robot [108] to provide a user-friendly interface for surgeons. The aim was to
minimizethecognitiveloadofsurgeonsandimproveefficiency. Asurgeoninputsaverbal
commandusingamicrophone,whichisthenpreprocessedusinganoff-the-shelftext-to-
speechmodelandpromptedtoGPT-3.5. Thispromptalsoincludesadictionaryofpossible
actionsthatthedaVincirobotcanperform. GPT-3.5isaskedtomatchthenaturallanguage
inputtoarobotactionthroughgeneratinganoutputbasedonthecontentsofitscontext
window. The specific actions for the robot to execute include camera position settings,
videoandpicturerecording,andfindingandtrackingsurgicaltoolsbasedonthesurgeon’s
command. A Robot Operating System (ROS) [109] node structure was used to directly
provideexecutioncommandstothedaVincirobot. Thesystemusabilitywastestedina
laboratorywhere275naturallanguagecommandsweregiventothesystem. Itwasable
tocorrectlyidentifyandexecutetheintendedrobotactionwithasuccessrateof94.2%. A
timedelayexistedbetweenthecommandrequestandtheexecutionoftheroboticaction,
whichwasattributedtocomputationtimetorespondoftheGPT-3.5model.Robotics2024,13,112 7of43
In[31],GPT-3.5-Turbowasusedtoidentifytheintentofindividualsinmulti-party
conversationsbetweenthesocialrobotARI[110]andpatientsandtheircompanionsin
amemoryclinicinahospital. ARIprovideddirectionsandrespondedtovisitor/patient
questions. Namely,adatasetofmulti-partyinteractionswasobtainedandannotatedfor
the intention and goal of each speaker. This dataset was created using a Wizard of Oz
procedurewheretheoperatorwouldchoosetheresponseoftheARIrobot. TheT5-large,
DialogLED,andGPT-3.5-TurboLLMsweretestedonthesetasksusingthedataset. The
modelswereevaluatedusingzero-shotandfew-shotapproachesduetothelimiteddataset
ofpatients. TheANOVAandTukeyHSDstatisticalsignificancetestswereusedtoevaluate
themodelperformance. Namely,theANOVAtestdeterminedasignificantdifferencein
thepercentageofcorrectannotationsforintentslotrecognitionbetweenGPT-3.5-Turboand
bothT5-LargeandDialogLED.TheTukeyHSDvalidatedthatGPT-3.5-Turbosignificantly
outperformedallothermodelsintermsofexactmatchandpartialcorrectnessofannotations
infew-shotsettings. ItwasfoundthatGPT-3.5-Turbousedinafew-shotsettingwitha
‘reasoning’-styleprompt,wherethereasoningforthedesiredoutputwasexplained,had
thehighestrecognitionandtrackingcorrectness. Namely,itwascorrectin69.57%ofintent
recognitionand62.3%ofgoaltracking. However,allmodelswerepronetohallucination
whenthepromptwasphrasedasastory. Thiscanbeanissueinhealthcaresettings,as
incorrectinformationcanbeprovidedtopatientsandhealthcareprofessionals.
In[81],theChatGPTwebinterfacewasusedforspeakerdiarizationofHRIsbetween
peopleandthesocialrobotFurhat[111],whichhashuman-likeexpressionsandconversa-
tionalcapabilities,toinvestigatethecapabilitiesofLLMsforimprovingHRIexperience.
Namely,diarizationconceptssuchas‘who’spokeand‘when’theyspokewereexploredin
multi-partyhumaninteractions. Therobotwouldbeabletoaddressspecificusersingroup
interactionsandallowforalessresource-intensivesystemwhichiscapableofdiarization.
ThedevelopedsystemusedChatGPTinzero-shotsettingstodetermineifthemodelcould
identifydifferentspeakerswithinatranscribedconversationbasedonthelinguisticpat-
ternsofeachparticipantinvolvedintheconversation. Atestdatasetwascreatedbyusing
thewhispermodelofOpenAI[112]totranscribeavideowhereadiscussionbetweentwo
hiringmanagers,acandidate,andaFurhatrobottookplace. Themodelachieved77%in
exactmatches,0.88insentencelevelannotationaccuracy,0.92inword-levelannotation
accuracy, and 0.18 in Jaccard similarity, demonstrating that large language models can
be used for diarization. Currently, the time required to generate speaker labels is too
slowforreal-worldimplementation. However,ChatGPTintegratedintohealthcarerobots
fordiarizationtaskshasthepotentialtoreducetheadministrativeburdenoncaregivers
by assuming the role of an assistant and generating structured clinical notes based on
caregiver–patientinteractions. Furthermore,itsattentionheadmechanismcanidentifythe
caregiverandthepatientbyanalyzingthelinguisticpatternsofeachandsubsequently
documentingtheinteractionintodiagnosis,symptom,andtreatmentsections.
3.2. Multi-ModalCommunication
InHRI,multi-modalcommunicationisfarmoreengagingandeffectiveinbuilding
trust[113]whencomparedtosingle-modalcommunication,providingamoreinteractive
andnuancedpatientexperiencethroughtheuseofgestures(e.g.,animatedspeech)and
bodyposesandvaryingspeechintonations[114,115]. Multi-modalHRIemulateshuman
interactionpatternsclosely[116],andLLMscangenerateemblems(non-verbalgesturesor
bodylanguagethathavespecificmeanings)forarobottodisplayinazero-shotapproach.
LLMsarealsoadaptabletovariationsinuserinteractionsduetoword-levelannotation
accuracyandtheirattentionheadmechanismswhichdynamicallyincreasetheimportance
ofrelevantpartsoftheinputtext. Thesemechanismsallowacontextualunderstandingof
userinteractionswhichisrequiredwhilegeneratinganappropriatespeechresponseand
emblem. TheabilityofLLMstogeneratecontextuallyaccurateresponsesandemblems
inHRIwithouttheneedfortrainingeverypossibleinteractionmakesLLMssuitableforRobotics2024,13,112 8of43
facilitatingHRIinhealthcaresettings,especiallyconsideringthesignificantvariancein
demographicsandthediversenatureofinteractionsencounteredinsuchsettings.
LLM frameworks applied in multi-modal HRI have the potential to make HRI in
healthcaresettingsmoreengaging. Forexample,byaligningnon-linguisticcommandsto
naturallanguageandgeneratingdynamicresponsestouserqueries,theycanpotentially
providemorenaturalcommunicationduringrobotic-guidedtherapysessions,breaking
downlanguagebarriersandprovidingemotionalsupport. Forexample,in[82],GPT-3.5
wasusedtoalignnon-linguisticcommunicationcueswiththenaturallanguageresponses
ofanyrobotcapableofmulti-modalcommunication. TheEmpatheticSocialRobotDesign
Frameworkutilizedconsistedofthefollowingmodules: Speech,Action,FacialExpression,
andEmotion(SAFE),alongsidetheuserrequest. ForSpeech,GPT-3.5consideredseven
typesofspeech,from‘highandfastspeech’to‘slowspeechinneutraltones’. Actionen-
compassedsevengestures,suchasturningtheheadtowardsthespeaker,nodding,shaking
thehead,interlockinghandsonthetable,andeyecontact. FacialExpressionsincludedten
optionsincludingfrown,lightsmile,pout,noexpression,brightsmile,raisedeyebrows,
grin,loweredeyebrows,jawdrop,andwidenedeyes. ForEmotion,theframeworkpro-
videdGPT-3.5withtenemotionalstatesfortherobottodisplay: joy,liveliness,sadness,
surprise,anger,worry,calmness,indifference,absenceofemotion,anddisgust. Inauser
studyanalogoustotheTuringtest,GPT-3.5waspromptedwithaspecificproblemaccording
totheSAFEpromptstructuring,suchas‘Iamtoonervousfortheupcominginternshipinterview’.
Itwasalsoprovidedwithanexampleofhowitshouldrespond. Theresponsegenerated
by the GPT-3.5 was then compared to a response of a human presented with the same
problem. Theaveragealignmentscoreforspeech,action,facialexpression,andemotion
was26%,10%,31%,32%,and25%,respectively. Sucharobotsystemcanbepotentially
usefulforReminiscence/RehabilitationInteractiveTherapyandActivities[117]forthose
livingwithdementia. Asocialrobotcanengageolderadultsinreminiscenceactivitiessuch
asmusic,TVshows,andmovies. Itcaninterpretusers’non-verbalresponsesandadaptits
interactionstosuittheiremotionalstates,providingcognitiveengagementandfostering
emotionalwell-being.
In [83], the text-davinci-003 model of GPT-3.5 was used to generate the dynamic
responsesoftheFurhatrobottovisitorquestionsinregardstonewsandresearchbeing
conductedattheNationalRobotariumintheUK.TheintegrationofGPT-3.5intoFurhatwas
toenablehuman-likespeechandcontextuallyaccurategestureswhilecreatingconsistency
betweenthesemodesofcommunication. TheFurhatSDKprovidedthefollowing: (1)an
automatic speech recognition (ASR) module to transcribe speech to text, (2) a natural
languageunderstanding(NLU)moduletoidentifyuserintent, (3)adialoguemanager
(DiaL)tomaintainconversationalflow,and(4)naturallanguagegeneration(NLG)using
GPT-3.5. More specifically, the NLGmodule generated responses basedon engineered
promptscontainingtheuserrequestfromtheNLUmodule,Furhat’spersonality,andpast
dialoguehistoriesfromtheDiaLmodule. TheresponsesgeneratedbytheNLGmodule
hadanassociatedemblemwhichwasparsedbytheFurhatSDKandpresentedbytherobot
usingbothverbalandnon-verbalcommunication.
3.3. SummaryandOutlook
TheintegrationofLLMframeworksforHRIintohealthcarerobotscanpotentially
improvethecohesivenessandengagementofinteractionsbetweenhealthcarerobotsand
patients, visitors, and stakeholders. LLMs have been embedded into social robots to
improveHRIby(1)providingnon-repetitivesingle-mode(verbal)[76]andmulti-modal
communication,wherethelatterconsistsofembeddingnon-verbalcommunicationinto
theverbalresponsesofhealthcarerobots(i.e.,gestures,eyecontact,andfacialexpressions)
tousers[82,83],and(2)identifyingthelinguisticpatternsoftheuser[31,81]. Moreover,
LLMshavethepotentialtoprovideamoreefficientinteractioninterfacethroughtheuseof
naturallanguagetocontrolsurgicalrobotssuchasthedaVincirobot[32].Robotics2024,13,112 9of43
The use of LLMs for multi-modal communication can augment robots in order to
extendtheiruseinhealthcareenvironmentsbyaccommodatingnon-verbalcommunication
with individuals living with cognitive impairments, autism spectrum disorder, and/or
learningdisabilities,whereverbalcommunicationaloneisnotalwaysfeasible.Bidirectional
multi-modal communication can be used when a healthcare robot needs to effectively
convey and recognize various non-verbal cues such as gestures/body language, facial
expressions,andvocalintonations. Forexample,ahealthcarerobotshouldrecognizewhen
apatientisstressedorupsetusingthesenon-verbalcuesandrespondwithanappropriate
emotionalbehavior(i.e.,concerned)ratherthanbeingcheerfulorhappy,therebyaligning
itsresponseswiththepatient’semotionalstates.
4. SemanticReasoning
Significantamountsofinformationmustbeprocessedinhealthcare,includingimages,
data,andtext,inordertominimizeerrorsandimprovetheefficiencyofpersonalizedcare
techniques [118]. The semantic reasoning of healthcare information requires experts to
identifyrelationshipsbetweendifferentfactorssuchasgeneticpredispositions,lifestyle
choices, environmental exposures, and/or social determinants of health which may all
influencethehealthofanindividual[119,120]. Therefore,semanticreasoningencompasses
theunderstandingofmeanings, concepts, andrelationshipsbetweendataandmedical
knowledge. Inparticular,ontologiesandknowledgegraphscreatedfrompatientEHRs
are currently used to interpolate how various symptoms, diseases, and treatments are
interrelatedandinfluenceoneanotherinordertomakepredictionsforclinicaldecision-
makingandpatientcare[121].
COVID-19increasedthedemandfortelehealthby367%inadultsaged55–65andby
406%foradultsaged65yearsandolder[122]. However,frameworkssuchastheeCoach
personalized referral program to help people stay active and achieve physical activity
goals[123]andtheBabylonChatbotwhichprovideshealthcareconsolationsthrougha
mobileapp[124]bothrequirecomplexontologiesandaccesstolargeamountsofcontextual
informationtogeneratepersonalizedrecommendations. Therefore,itisnotonlyamatter
of creating healthcare-focused datasets and ontologies to train deep learning and NLP
modelstobeabletoprovidepredictionsandinference;thereneedstoalsobeaneffective
approach to creating such inferences in order to (1) facilitate the seamless exchange of
semanticreasoningframeworksamonghealthcareinstitutions[125]and(2)simplifythe
creationofframeworksthatcaneffectivelycaptureandconveythecomplexsemanticsof
medicaldatasets,terminologies,andenvironments(hospitals,clinics,etc.)[126].
Traditional robotic semantic reasoning frameworks consist of three core compo-
nents [127]: (1) knowledge resources (raw data), (2) computational frameworks (math-
ematical models), and (3) world representations (scene/environment representations).
Knowledge resources include the data from which semantic knowledge is extracted
(i.e.,EHRs,clinicaltrialresults). ThesedataareusedtotraintheLLMs. Computational
frameworks consist of models such as transformers (LLMs) [128], probabilistic models
(Bayesian networks) [129], or deep learning models (long short-term memory (LSTM)
networks) used to capture temporal dependencies in data [130]. These computational
frameworksarethemodelsthatencodetherelationshipsbetweenconcepts[131]. They
thenusetheencodedknowledgetoperforminference[127]. Worldrepresentationsare
usedbyrobotstomodeltheirsurroundingenvironmentsandtheirownbehaviors[132].
InthecaseofLLMs,theworldrepresentationprovidesanLLMwithascenedescription
ofarobot’sworkingenvironmentinthecontextwindowandthereby‘groundstheLLM’
to its current environment [20]. The aforementioned core components allow robots to
perceive,understand,andgeneralizesemanticknowledgeinordertoimproveperformance
inreal-worldtasks. TheincorporationofLLMsforsemanticreasoninginhealthcarerobots
offersmultipleadvantages. Namely,healthcarerobotsneedsemanticreasoningtoidentify
relationshipsbetweenatask,anenvironment,andausercommand. Semanticreasoning
canbeused(1)whengeneratingobjectmanipulationplansforsurgicaltools,equipment,Robotics2024,13,112 10of43
instrumentation,andmedicalsupplies,(2)tonavigatetospecificareasandregionsinthe
environmentsuchastheORand/orpatientrooms,(3)tointeractwithpeople(toprovide
assistiveHRI)andotherrobots,and(4)tocompletetaskmanagementfunctionalitiesor
specifictasksandservices(patienteducation,guidance,informatics).
Todate,onlyahandfulofrobotshavebeenincorporatedwithLLMsforthepurpose
of semantic reasoning. For example, in [77], the semi-humanoid robot NICOL (Neuro-
InspiredCOLlaborator)usedGPT-3toreasonaboutmulti-modalinputs(sound,haptics,
visuals,weight,texture)inordertoimproveroboticperceptioninobjectmanipulationto
helpdifferentiatebetweenvisuallysimilarobjects. Namely,GPT-3wasusedtoperform
interactivemulti-modalperceptionandrobotbehaviorexplanations. TheMATCHA(multi-
modal environmental chatting) prompting technique provided specific action prompts
to GPT-3 which included stored information about the robot’s environment. Namely,
these action prompts included descriptions of the actions, as well as an example of an
expectedresponse. GPT-3providedinstructionstotherobottodeterminethetargetblock
byknockingonandweighingtheblocksonebyone. Therobotthenreportedthefindings
(stored information) of each step back to GPT-3. This process was repeated until GPT-
3 was able to determine which was the correct block with high accuracy (>90%). The
systemhasbeentestedonlyinsimulationbypromptingNICOLtopickupaspecificblock
madeofmetaloutofthreeblocks,wherethecharacteristicsoftheblockswereprovided
rather than perceived by the robot. Even though this task has not been implemented
directlyinahealthcaresetting,thepotentialofMATCHAcanbeexploredparticularlyfor
improvingoperationalefficiencyanddecision-makingforpatientcareintermsofmulti-
modalinformationgatheringandreasoningforrobotmanipulationtaskssuchasretrieving
andhandingover(1)medicalsuppliesonshelvesinasupplyroomor(2)surgicaltoolson
atableintheOR.
In [87], the ‘SayCan’ method integrated the PaLM 540 B parameter model, PALM-
SayCan,intoamobilemanipulator[133]forsemanticreasoninginthecontextofhuman-
centeredenvironments(i.e.,akitcheninanoffice). Themanipulatorprovidedthepercep-
tionandmanipulationcapabilities,whilePaLMprovidedhigh-levelsemanticknowledge
about tasks to promote successful task completion. The system was trained on a mock
kitchen and tested in an office kitchen environment. SayCan is not only prompt-based
butalsousesatemporaldifferencereinforcementlearning(RL)approach. Thisapproach
learnstherewardsofeachaction(completingtheobjectivevs. courseofaction)andthere-
foreprioritizestheexecutableactionsbasedontherobot’scurrentenvironment. PaLM
540Bisprovidedwithpromptsthatincludetherobot’scapabilitiesandtheirdescriptions
andtheactionstherobotcantakeexpressedasafunction(e.g.,‘goto()’). PaLM540Bis
givenatask,suchasfindobjectX,andaskedtogeneratemultiplepossibleactionspaired
withthelikelihoodofeachprediction. Theactionprobabilitiesarethenmultipliedbythe
probabilityofsuccess(acquiredthroughthetemporaldifferencemethodinRL).Theaction
functionwiththehighestprobabilitywhichistheoutputoftheLLMisthenexecutedby
therobot’slow-levelcontrolsystem. Thisprocessisrepeatedforallsubsequentstepsin
thegeneratedplan. Theapproachwasbenchmarkedbyobtainingtheplansuccessrate
andtheexecutionsuccessrateincompletingatasksuchasbringingabagofricechips
from a drawer. The SayCan development improved task execution in human-centered
environments by leveraging semantic reasoning. The potential healthcare applications
of‘SayCan’canbeextendedtomobilesocialrobotsdirectingpatients/visitorstospecific
rooms or departments in a hospital and/or mobile manipulators fetching or localizing
essentialmedicalsupplies.
In [85], the LLM-Grounder, an open-vocabulary zero-shot LLM-based 3D visual
grounding pipeline, was introduced. LLM-Grounder integrated GPT-4 and a robotic
simulatedagenttoreasonaboutthesemanticrelationshipsbetweenhigh-levelcommands
givenbyauser(i.e.,findthegreymonitorontopofthesmallercurveddesk)andthework-
ingenvironment(simulatedoffice)forobjectlocalizationtasks. GPT-4wasusedtobreak
downnaturallanguagecommandsintotheirsemanticconstituents,thetargetandaland-Robotics2024,13,112 11of43
mark. TheabilityofGPT-4to(1)identifythelandmarkandthetargetand(2)differentiate
betweenthetwoprovidesanefficientmethodtogeneratefromhigh-levelusercommands
torobotactionstoperforminordertocompleteagoal. Namely,OpenScene[134],whichis
a3Dvisualgroundingmethodthatusesthetransformerarchitecturetogenerate3Dscene
layoutsbasedontextualdescriptions,wasused. GPT-4providedOpenScenewith(1)the
targetnameanditsattribute(monitor,lightgrey)and(2)thelandmarknameandrelation
(curveddesk,small). OpenScenereturnedtoGPT-4therespectiveboundingboxvolumes
anddistances. GPT-4wasthenusedtoreasonaboutthesizeofobjectsandtheirrelative
locationtothelandmark,decidingonwhichfoundtargetandlandmarkpair(s)hasthe
highestlikelihoodtobecorrect. Therefore,GPT-4isusedtoprovideanefficientmethodfor
robotstounderstandthesemanticsofhigh-levelusercommandsandforthemtoacton
thesecommands.LLM-GrounderwasevaluatedontheScanRefer[73]benchmark,whichis
astandarddatasetforassessing3Dvision–languagegroundingcapabilities. TheScanRefer
datasetconsistsofscenesrangingfromwildlifetohomeenvironments,whereeachpoint
cloudandimagehasatextdescription[73]. LLM-Grounderdemonstratedstate-of-the-art
performancewhenusedforzero-shotopen-vocabularygrounding,excellingincomplex
language(increased#ofnounsincommand)queryunderstandingoverapproachesthat
donotuseLLMsandrelysolelyonCLIP.Theabilityofarobottocorrectlyunderstanduser
commandsinthecontextoflocatingobjectsinhuman-centeredenvironmentsiscrucial
forthesuccessfulimplementationofhealthcarerobots. Arobotdeployedinahealthcare
environmentmustbeabletoidentifythecorrecttarget(outofmany)incaseswhereitems
arenoteasilydistinguishable,suchasidentifyingaspecificmedicationonamedication
cartwherelabelsarenotvisible. Thiscanonlybeachievedifthehealthcarerobotisableto
understandthepillbottledescriptionandsemanticrelationshiptothelandmarkspecified
bytheuserinnaturallanguage.
In [74], the ‘Lang2LTL’ method integrated the SPOT quad-pedal robot [135] with
GPT-4andtheT5basetoprovideSPOTwiththesemanticreasoningcapabilitiesrequired
tounderstandandactonuserspeechcommandsinthecontextofnavigatingenvironments
rangingfromofficestocitystreets. Lang2LTLusedtheseLLMstobreakdownnavigational
commands(i.e.,‘GotothestoreonMainStreetbutonlyaftervisitingthebank’)toLinear
Temporal Logic (LTL) in the form of sequential objectives, such as (1) go to the bank
and(2)gotothestoreonMainStreetafter. Usercommandsintheformofnaturallanguage
weregiventoGPT-4toidentifythereferringexpressions(RFs)suchasthestoreonMain
Streetorthebank. AfteridentifyingtheRFs,theRFsweregroundedtoknownphysical
locations(retrievedfromadatabase)bybeingcomparedtoknownpropositionembeddings
whicharelocationdescription/coordinatepairingsusingcosinesimilarity. Thegrounded
RFswereusedasaninputintothefine-tunedT5basemodeltogenerateLTLformulas.
Thefine-tunedT5modelwasthenusedto(1)generalizetheinitialinputcommand(go
to‘a’butonlyaftervisiting‘b’)and(2)generatetheLTLformularequiredbytheplanner
tofacilitatenavigationviatherobot’slow-levelcontroller. Lang2LTLwastestedonSPOT
in an indoor environment consisting of bookshelves, desks, couches, elevators, tables,
etc. SPOTsuccessfullygrounded52commands,including40satisfiable(executable)and
12 unsatisfiable ones (unsafe to execute). The potential applications of ‘Lang2LTL’ in
healthcarearerobotdeliverytaskswithinclinicalsettings. Namely,theaddedabilityto
identifyunsaferoboticactionexecutionscanaidarobotplanner. Thiscanbeachievedby
identifyingthesemanticimportanceofusercommandsandtheorderofoperations,such
asretrieveawalkingaidforapatient,thenvisitthepatient’sroomtoprovidethemwith
theaid.
SummaryandOutlook
Ingeneral,theintegrationofLLMframeworksforsemanticreasoningintohealthcare
roboticshasthepotentialtoimproverobotautonomyincomplexanddynamichealthcare
environments. Currently,LLM-basedsemanticreasoninghasincreasedrobotautonomy,
as the transformer architecture of LLMs semantically reasons about the entire input atRobotics2024,13,112 12of43
once,facilitatingfasterandmoreaccuratedecision-makingincomparisontotraditional
sequential processing models such as Recurrent Neural Networks (RNNs) [39]. This
approachallowsrobotstounderstandcomplexrelationshipsamongobjectcharacteristics,
roboticactionoutcomes,thespatialimportanceoftargetobjectsversuslandmarks,and
user-imposedconstraints. HealthcarerobotscanuseframeworkssuchasMATCHA[77]
forautonomouscapabilitiessuchasthedetectionandmanipulationofmedicalsupplies
based on characteristics such as weight and texture. Additional applications include
robotsaugmentedwithSayCan[87],LLM-Grounder[85],orLang2LTL[74]frameworksfor
(1)navigatingefficientlywithinhealthcarefacilitiesbyunderstandingspatiallayoutsand
identifyingpotentialunsafesurfaces(e.g.,wetfloors),(2)executingpatient-specifictasksby
interpretingnaturallanguagecommandsgivenbycareproviderssuchasretrievingavital
signsmonitorcartbeforevisitingapatientintheirroom,or(3)increasingthesurgicalteam’s
situationalawarenessbyalertingthemtoequipmentneedsandpreemptivelymanaging
robotictoolpositions,thusensuringsustainedoperationalfocusandefficiency.
5. Planning
An existing challenge for healthcare robotics is autonomously planning safe and
effectivebehaviorsinrealtimeforhealthcaretasks. Thesetaskscaninclude(1)navigating
throughcomplexhospitalenvironmentstofindhealthcareprofessionalsandpatientsandto
escortvisitors,(2)managinganddeliveringmedicationsandmedicalsupplies,(3)assisting
insurgerybytoolhandoversandsupportingprecisetoolmovements,and(4)facilitating
bothphysicalandcognitiverehabilitationwithdifferentusergroups. Eachofthesetasks
requiresarobottoperceiveandinterpretitssurroundingsandmakereal-timedecisions
thatensuresafetyandtaskeffectiveness. Forexample,surgicalrobotsoperatingwithinsoft
tissuesandnavigatingcurvedpathsfaceaparticularlyuncertainanddynamicenvironment
duetothecomplexandvariablenatureofhumananatomy. Softtissuescanshiftordeform
during procedures, altering expected pathways and requiring real-time adjustments in
therobot’smovements. Additionally,theinherentvariabilityinpatientanatomymeans
thatpre-plannedpathsmaynotalwaysapplyprecisely,necessitatingcontinuoussensory
feedbackandadaptivecontrolstrategiestoaccuratelyguidesurgicaltoolswithoutcausing
unintendeddamage. Thisenvironmentdemandshighlevelsofprecisionandadaptability
fromsurgicalrobotstoensuresafetyandeffectivenessintheiroperations.
Ingeneral,healthcarerobotictaskplansneedtobeadaptabletodifferentsituationsand
people. Currently,themajorityofsurgicalrobotsareteleoperatedbyasurgeon[136,137].
An autonomous surgical robot needs to adjust to changes in patient anatomy in real
time[138]. Presently,thelackofreal-time3Dsensingisasignificantconstraint[139]. This
constraintpreventssurgicalrobotsfrom(1)operatinginrealisticconditions(i.e.,lighting
changes, occlusions) and (2) operating on non-planar surfaces [139]. While minimally
invasive methods such as the multi-camera CARET system proposed in [140] attempt
to enhance the surgical field of view without making additional incisions, they do rely
on complex intra-camera tracking to maintain an expanded view. Namely, this system
determinesthecorrespondencebetweendifferentcamerasattheinitializationstageand
updates the expanded view frequently when there is enough overlap between views.
However, thiscanstillleadtoinaccuratemosaickingresultsduetoerroraccumulation
over time [141]. Although the visual field provided by instruments like laparoscopes
is not always optimal, surgeons are able to effectively conduct surgeries due to their
continuouslearningandabilitytointerpretsurgicalsituationsandsemanticallyreason
aboutthecurrentstateoftheoperation. IntegratingLLMswithsurgicalrobotswillallow
forthisdecision-makingprocessbysemanticallyreasoningaboutthecurrentstateofthe
surgeryandgeneratingrobotactionplansfromthelimitedinformationabouttheoperating
environment provided by imaging sensors such as laparoscopes and the multi-camera
CARETsystem.
Existingroboticplanningframeworksthathaveincorporatedautonomyhavemainly
usedheuristicandDLmodels. Heuristicmethods, suchas(1)geneticalgorithms[142],Robotics2024,13,112 13of43
(2)GreedyBest-FirstSearch[143],and(3)SimulatedAnnealing[144],aimtoidentifythe
sequence of robot actions for task planning. Additionally, DL methods, such as (1) the
pathplanningandcollisionnetwork(PPCNet)[145],(2)DRLmodels[146],and(3)LSTM
networks[147],learnfromtrainingexamplesandepisodesinordertoautonomouslyplan
roboticactionsandgeneralizetoreal-worldsituations. Theunpredictablenatureofhealth-
careenvironments,withsuddenmedicalemergenciesandchangingpatientconditions,
requiresarobottoautonomouslyreacttoneworquicklychangingscenarios. Heuristic
modelsarebasedonasetofrulesandtendtoyieldsatisfactoryratherthanoptimalso-
lutions [148]. Furthermore, the efficacy of DL models in generalizing to unseen data is
significantlyinfluencedbythediversityandsizeoftrainingdatasets[149,150]. Ensuring
thatroboticplannerscanadapttonewandvaryingconditionsbeyondtheirtrainingdatais
crucialfortheireffectivedeploymentinhealthcaresettings. Consequently,thereisaneed
forhealthcareroboticsplanningmethodsthatcanmanageextensivedatasetsencompassing
immenseamountsofhigh-qualitydatadescribinghuman-centeredscenes,medicaland
generalknowledge,andvariousmedicalprocedurestoenablerobotstodetermineoptimal
plansforcomplextasks. Theseincluderobot-guidedsurgeryforpreciseoperationalassis-
tance,real-timediagnosticanalysisduringvariousmedicalprocedurestoinformdecisions,
androbot-ledinterventionsandrehabilitationthatadjusttreatmentplansbasedonongoing
patientevaluationsacrossdifferenthealthcaresettings.
LLMscaninterpretcomplexinstructionsandpatientdata,facilitatingrobotstomake
informed decisions in real time. Unlike heuristic models that rely on predefined rules,
LLMstakeadvantageofattentionheadmechanismswhichaddbiastowardsgenerating
outputsrelatedtothecontentsofthecontextwindowwhichincludeinformationfromthe
environment[151]. Thecontentscanbeupdatedinrealtimetocontainthecurrentstateof
boththerobotandthehealthcareenvironment,therebyhavingtheLLMgeneraterobotic
plansspecifictothesestates.Furthermore,boththeabilityofLLMstodynamicallyaddbias
whengeneratingaplanandtosemanticallyreasonallowsLLMstoadaptalreadygenerated
roboticplansbysupplementingthemodelinputwithstatechangesanditerativelyrefining
thesequenceofplannedrobotactionstoensuresuccessfulexecution. LLMsmimichuman
planningincontrasttotheaforementionedDLmethods. Namely,thelatterlacktheability
toiterativelyandefficientlyupdatespecificportionsofroboticplanstoaddressrun-time
issuessuchasequipmentfailuresinsurgicalroomsorvariationsintreatmentresponses
duringrehabilitationsessions[152]. Healthcarerobotsneedtheadaptablecapabilitiesof
LLMsforplanningtoenhancetheireffectivenessindiverseanddynamicenvironments,
suchas(1)emergencyroomswhereconditionscanchangerapidly,(2)ORsthatrequire
precision under varying circumstances, and (3) rehabilitation settings where patient re-
sponsescanbeunpredictableandvaried. Therefore,usingLLMsinhealthcarerobotics
planningcanprovide(1)recoveryincasesofsuddenenvironmentalchanges,forexample,
aninfluxofpatientsduetoanoutbreakwheretherobothastoprioritizewhichpatients
itshouldassistfirst,(2)roboticplanssuitableforexecutionaroundvulnerablepeopleby
inferring about social norms, for example, when escorting frail patients to their rooms,
(3)fine-tunedmotionplanstofacilitaterehabilitationforstokesurvivorsandphysically
impairedpatients,and(4)dynamicallygeneratedroboticplansthatcanbeinterpretedby
non-expertroboticists(healthcareprofessionals),forexample,insurgicalroboticswhere
thesurgeonmustanticipatetherobot’snextplannedmotion. Furthermore,theintegration
ofVision–LanguageModels(VLMs)canprovidetheutilizationofthevisuallyanchored
featuresofVLMstoassistwithtaskplanninginhealthcarerobotics[153]. Namely,VLMs
can aid in grounding in the physical world for healthcare robotic applications by inte-
gratingvisualandtextualdataintoajointembeddingspace,whichLLMslackontheir
own,enablingrobotstounderstandphysicalcontextsandrespondmoreappropriatelyto
environmentalcues.
Todate,ahandfulofrobotshaveusedLLMsfortaskplanninginhuman-centered
environments. For example, in [88], the LLM-BRAIn method consisted of the Stanford
Alpaca7BparameterLLMinordertogeneratebehaviortrees(BTs)foridentifyingandRobotics2024,13,112 14of43
retrievingobjectsformobilemanipulatorrobots. BTsprovideamodularbehaviorstructure
consistingofnodes(robotexecutionsteps)thatcanbeeasilyscalable. Initially,GPT-3was
usedtocreateasynthetictrainingdatasetwhereeachexampleisanXMLfilecontaininga
randomlygeneratedBTforamobilemanipulatorrobotandadescriptionoftherobot’s
movementandobjectmanipulationsteps. Thesyntheticdatasetwasusedtofine-tunethe
AlpacamodelparametersforgeneratingrobotBTs. TogenerateBTsbasedonuserrequests,
thefine-tunedAlpacamodelwaspromptedwithalistofrobotactionstoexecute(e.g.,take
object,scanarea)andtheuserrequest(e.g.,ifobjectisvisible,movetowardsit,takeit,and
processit). AROS2BTinterpreter[149]tooktheoutputofthefine-tunedStanfordAlpaca
model(XMLformat)andgeneratedexecutableactionsfortherobot. TheBRAInmethod
includedtherobot’sfunctionalityatallplanningstepsinanunderstandableformatfor
theuser. ToincorporatetheLLM-BRAInmethodintohealthcarerobots,thegeneratedBTs
usedtotrainLLM-BRAInshouldincludehealthcare-relatedtasksinsteadofthegeneral
localizationandretrievalofobjects,thusallowinghealthcareprofessionalstounderstanda
healthcarerobot’sintentionsandactionsandpotentiallystoppingtheexecutionofataskif
thereareanyconcerns,ensuringsafeinteractionswhenprovidingpatientcare.
In[84],GPT-3.5wasusedtoguideaFrankaEmikaPandarobotarmoperatingina
mockkitchentofacilitatethehandoverofdirtykitchenutensils(i.e.,fork,spoon,knife)toa
humanwashingthedishes. Thezero-shotcapabilitiesofGPT-3.5inplanningcollaborative
robotictasksthatalignwithhumansocialnormswereinvestigated. GPT-3.5wasprompted
withexamplesfromtheMANNERS-DB[154]datasetwhichcontainsHRIscenarioswhere
appropriate robot behaviors are represented. GPT-3.5 was then tested for its ability to
(1) establish/maintain human trust while guiding the robot to hand off sharp objects
(i.e.,knives)and(2)predictculturalandsocialnormsusing(a)theTrust-Transfer[155,156]
which contains 189 instances of driving and household tasks where participants rated
their trust in the robot completing the task on a seven-point Likert scale and (b) the
SocialIQA[157]whichcontains1954testingexamples,eachcontainingacontent,question,
threepossibleanswers,andagroundtruth,wherearobotcanbetestedonitscommonsense
reasoning. Inaphysicalexperimentconsistingoftherobotandauser,GPT-3.5facilitated
robot-assistedutensilwashingwhileallowingforuserintervention. Successandfailure
inhandoverswerereportedwithonly28.1%ofparticipantstrustingtherobotwithknife
handovers. GPT-3.5canimprovepatientandcareproviderHRIbyutilizingin-context
learningtorefinerobotictaskplans,ensuringtheyalignwithuserperceptionsthrough
analyzingpastfailuresandapplyingnecessarycorrections. Thepotentialapplicationsin
healthcare-relatedtaskscaninclude(1)deliveringmedications/suppliesbynavigatingbusy
hospitalcorridors(e.g.,givingwaytohumansandincorporatingothersocialetiquettes)
and(2)autonomouslycleaningpatientroomswhiletheyarestillintheroom(e.g.,keeping
asafedistance,smoothnavigation).
In[78],theProgPromptmethodintegratedGPT-3intotheFrankaEmikaPandama-
nipulator robot [158] to facilitate object manipulation in a physical mock kitchen and
simulatedVirtualHome[159]. Thepromptswerebasedonobjectmanipulationandsorting,
forexample,‘sortfruitsontheplate,andsortbottlesinthebox’. Intheimplementationofthe
ProgPromptmethod,twoinstancesofGPT-3wereusedtogenerateandrefinerobottask
plans. ThefirstinstanceofGPT-3wasgivenapromptcontainingrobotprimitiveactions,a
listofavailableobjectsintheenvironment(whichweredynamicallyidentifiedthroughan
open-vocabularyobjectdetectionmodelinreal-worldapplicationsorpredefinedinsimu-
lations),andexampleplansillustratingthedesiredtaskstructureandoutcomes. GPT-3
thengeneratedaninitialplanwithnaturallanguagecommentsdetailingeachstepand
logicalassertions(e.g.,‘putthebananaontheplate’)toprovideamethodforerrorrecovery
incaseofexecutionfailures. ThesecondGPT-3instanceiteratedovertheinitialplanwith
afocusonreasoningaboutthecurrentsemanticstateoftheenvironmentandthetaskat
hand. Theplanwasmodifiedbyaddingorremovingstepstosuccessfullyachievethegoal
providedinthefirstinstance. Forexample,ifthetaskinvolvedmicrowavingsalmon,the
secondinstanceconsideredthestateofrelevantobjectsandactions(suchaswhethertheRobotics2024,13,112 15of43
microwavedoorisopenortherobotisholdingthesalmon)todeterminethenextlogical
stepintheplan. Thisiterativeprocessallowedfordynamicadaptationandrefinement
of the plan, ensuring it was contextually appropriate and executable within the given
environment. TheProgPromptframeworkwastestedintheVirtualHomeenvironment
usingGPT-3togeneratetaskplansacross10tasks,suchas‘putsalmoninthefridge’or‘put
bananaonplate’. TheProgPromptmethodexcelledinsimpletasks. However,itwasless
effectiveincomplexsequentialtasks,suchas‘putthebananaontheplateandthepearinthe
bowl,sortthefruitsontheplateandthebottlesinthebox’. TheProgPromptmethodintegrated
with GPT-3 can potentially be used in healthcare robotics to enable robotic medication
dispensingtaskssuchaspreciselydispensingintomeasuringcups,usinglogicassertions
andsemanticreasoningbasedonthetextualdescriptionofthescenetodeterminewhen
thecupisfull, thuseliminatingtheneedforspecializedhardwaresuchasprescription
dispensingsystems[160]andalsorelyingonvisiontocontrolaroboticmanipulatorfor
accuratedispensation.
In[79],theRoboGPTframeworkutilizedthreeinstancesofGPT-3intheFrankaEmika
manipulatorrobot[158]toimproveroboticmanipulationtaskssuchaswordspellingusing
letterblocks,movingletterblocks,binpackingandpyramidstacking,andhousebuilding
usingcubes. ThefirstinstanceofGPT-3wasadecisionbot,whichgeneratedasequence
of robot actions based on a task-oriented prompt. The prompt provided detailed back-
groundinformation,including(1)therobot’scapabilities,(2)adescriptionofthescene,and
(3)guidelinesonthemodel’sresponseformat.ItalsoincludedrobotAPIcommandssuchas
‘envs.pickObject(object_name)’andthecoordinatesofobjectswithinthescene. Thedecision
botiteratedonthegeneratedsequenceofrobotactionstoresolveanyrun-timeerrorstoen-
suretherobotcanperformeachstepoftheplan. Theevaluationbot(secondinstance)was
deployedtoverifythatthesequenceofrobotactionsgeneratedbythedecisionbotaligned
withthetaskrequirements,effectivelyservingasunitteststoconfirmthecorrectnessof
eachaction. Lastly,thecorrectorbotwasgiventhegeneratedplanandtheresultsfrom
theevaluationbotinordertoidentifythereasonforthefailureoftheplan. Theprompt
ofthecorrectorbotincludedbackgroundinformation,describingitsrolespecificallyasa
‘codecorrector’,andthecoordinatesoftheobjectsinthescene. Thepromptalsooutlined
astructuredanalysisprocess,whichincludedthefollowingsteps: guessingtheintended
spatialrelationshipsbetweenobjectsfrominputcodes,determiningactualspatialrelation-
shipsfromfinalobjectstates,andanalyzingdiscrepanciestosuggestpossiblereasonsfor
anyfailures. Theevaluationbotvalidatedthedecisionbot’splanbyverifyingthecorrect
stackingofcubes. ItusedrobotAPIfunctionssuchasthe‘self.checkOnTop(object_1_name,
object_2_name)’methodtoconfirmthattheobjectswereaccuratelyplacedaccordingtothe
taskrequirementsandobjectattributesprovided. Thiscollaborativesystemenabledprecise
taskexecutionthroughtheiterativerefinementandevaluationofthegeneratedplan. After
ataskwassuccessfullycompletedusingtheplan,theplanandtheoutcomeoftheplan
(i.e.,successorfailure)servedasademonstrationexampleusedtotrainaDRLmodelto
performthesametasks,therebyimprovingtheefficiencyofgeneratedrobotactionplans
by reducing therelianceon thedecision, evaluation, and corrector bots. The RoboGPT
frameworkcanbeadvantageousforhealthcareroboticsintermsofassistingcareproviders
with repetitive and non-repetitive tasks. For many repetitive tasks, such as organizing
medicalequipmentorretrievingsurgicaltoolsintheOR,theframeworkcanenablethe
robottorelyontheDRLmodelforrobottaskexecutioninsteadofgeneratingrepetitive
plansusingLLMs,thusimprovingefficiency. Asfornon-repetitivetasks,suchastriaging,
thethreebotscanprovideahealthcarerobotwiththeversatilityneededtoadapttochanges
intheenvironmentandtheconstraintsofthetaskandpatients.
In [80], GPT-3 was used as the robotic planner for the ‘Toyota HSD’ service robot
operatinginahomeenvironment. Therobotfulfilleduserrequestswithrespecttogeneral-
purposeservicetasks,suchas‘pickuptheapplefromthebookcaseandputitonthestoragetable’.
AtaskinstantiationmodulewasusedtoexecuterobotactionsplannedbyGPT-3,suchas
theprimitiverobotactions‘move_to()’,‘grasp()’,‘pass_to()’,and‘visual_question_answering()’.Robotics2024,13,112 16of43
Aninferencemodulewasusedtoextractinformationfromtheenvironment. Theinference
moduleconsistedof(1)GoogleCloudspeechrecognition[161]totranscribeusercommands,
(2)objectdetectionusingYOLOv7[162]torecognizehouseholdobjectsin2Dimages,(3)a
visualquestionanswering(VQA)modeltoscantheenvironmentbasedonRGBimages
with the purpose of locating a specific object, (4) the open-vocabulary object detection
model Detic [163] to obtain textual descriptions of the environment to provide context
toGPT-3,and(5)theEZPOSEhumanposeestimationmodel[164]toidentifypeoplefor
humanguidancethroughposeestimation. OnceGPT-3ispromptedwithatask-oriented
prompt containing the user’s request (using speech–text to transcribe user requests), it
generates a sequence of primitive robot action functional calls using the user-specified
targetsasarguments,forexample,the‘locatethefruitsinthediningroom’argumenttothe
VQAmodewouldbe‘wherearethefruits?’. Furthermore,thelocationsoftheargumentsin
thegeneratedplanarecheckedagainstanobjectlocationdatabasebeforebeingexecuted.
Ifthelocationisfound,theplanisexecutedorelsetheinferencemoduleisqueriedwith
imagesoftheenvironmentusingpointcloudandRGBinformation. Then,GPT-3isdirectly
questionedaboutthepossiblelocationsoftheobjectinthecontextofthecurrentscene
description. Thisframeworkwastestedattherobocup@homeJapanopencompetition,
anditwonfirstplace. Therobotplannercanbeextendedtohealthcareenvironmentsto
generate robot action plans based on all the available information from the scene. The
frameworkdevelopedcanbeclassifiedasaVLM,asRGBandpointclouddataareusedin
conjunctionwithanLLMtogenerateplanswhichconsiderallfeaturesoftheenvironment
(i.e., people, objects, obstacles). In healthcare settings, this integration can effectively
providepatient/visitornavigationguidancethroughsocialnavigation,enablinghealthcare
robotstoautonomouslygenerateandupdatenavigationpathsinrealtimewhileadapting
todynamicchangessuchasincreasingcrowdsornewemergencysituations. Theserobots
caninteractusingnaturallanguage,adjusttheirnavigationpaceandroutebasedonverbal
feedbacktoensuretheuser’scomfort,andcatertospecificneedssuchasslowermovement
forelderlypatientsorquickaccessforemergencies. Thisapproachcanenhancetheoverall
efficiencyanduserexperienceinnavigatinglargehospitalenvironments.
In[86],GPT-4wasintegratedintotheAlter3androidrobot[165]fortheself-planning
oftherobot’sphysicalactionsinordertoadaptitsownposetouserspeechrequestsin
applications which include ‘Take a selfie’, ‘Pretending to be a ghost’, and ‘I was enjoying a
moviewhileeatingpopcorninthetheater,whenIsuddenlyrealizedthatIwasactuallyeatingthe
popcornofthepersonnexttome’. Twoframeworksweredeveloped. Thefirstframework
consisted of task-oriented prompting to plan the limb movements of the Alter3 robot
and used EmotionPrompt to ensure that there are expressive gestures included in the
generatedmovementplan. Forexample,GPT-4wasinstructedtogenerateahigh-level
planofhowAlter3shouldchoreographitslimbsinresponseto,e.g.,‘drinksometea’. The
task-oriented prompt used the high-level plan to generate a sequence of robot actions
correspondingtoeachlimboftherobottorealizetheplannedexpressionsandmovements,
wheretheemotionalpromptplacedemphasisonexaggeratingtheemotionalandfacial
expressionsassociatedwiththeuserrequest. Namely,thetask-orientedpromptgenerated
a sequence of function calls that were executed by the control system of Alter3, where
eachfunctioncallcontrolsoneofAlter3’s43motionaxes. Eachlimb(eyebrows,shoulders,
indexfinger,etc.) hasmultipleaxes. Theeffectivenessofthisapproachinarticulatingan
androidrobotwasexploredinauserstudy.Inthestudy,participantsinteractedwithAlter3,
whereGPT-4generatedanactionplanbasedoninputprompts. Theparticipantsfailedto
distinguishbetweenGPT-4-generatedandrobotic-expert-programmedmovements. The
secondframeworkintroducedaclosed-loopsystemthatbuildsontothefirstframeworkby
storinggeneratedsequencesofrobotactionsinaJSONdatabasetobeusedagainforsimilar
requests. Forexample,iftherobothasbeenaskedto‘takeaselfie’,theplanwhicharticulates
therobot’slimbswillonlyneedtobegeneratedonce,andanysubsequentrequestto‘takea
selfie’willusethealreadygeneratedplanstoredintheJSONdatabase,thereforereducing
plangenerationtimeandimprovingresponsiveness. Additionally,thisframeworkwasRobotics2024,13,112 17of43
furtherexpandedtoincorporatea‘SocialBrain’mechanism,employingmultipleinstances
ofGPT-4,eachwithadistinctrole,tomimichumancollaborationinproblem-solving. A
potentialhealthcareapplicationforGPT-4inroboticplanningistoautonomouslygenerate
detailed rehabilitation plans that can be reviewed by a physiotherapist before they are
executed,therebykeepingthetherapistinvolved. Thegeneratedplanscanbeupdatedin
realtimebasedoninputsfrompatientswithrespecttotheircomfortandpainlevelsduring
arehabilitationsession. Thisdynamicadjustmenthelpsinprovidingpersonalizedcare,
andtheinitialplansserveasaclearcommunicationtoolforhealthcareprofessionalsto
understandandoverseethetreatmentprotocol.
In[75],theKnowledgeBotframeworkusedtheT5LLMasthebackboneforarobot
actionplannerusedinconjunctionwithaconversationalembodiedagentintheAlexaPrize
SimBotChallengeenvironment[166]. AT5-Largewasusedtotrainanobjectgenerator
to generate a list of objects of interest based on a user-provided task description, such
as‘faucet,houseplant,cup’for‘watertheplant’. Then,eachobjectwasconcatenatedwith
the task description and used by the encoder–decoder-based planner (T5 backbone) to
generatestep-by-steprobotactionstoexecute. Avariationofthisprocedurewasdeveloped
inordertoemulatehumancognitiveprocessesintaskplanningandexecutiontoensure
thateachstepisinformedbytheoveralltaskcontextandtheprogressmade. Inorderto
generatestep-by-steproboticactionplans,thetaskanditsgeneratedstepsarepassedto
theobjectgeneratorandanencoderinparallel. Then,theoutputfromtheencodergoes
tothe(1)decoderand(2)ispairedwiththegeneratedlistofobjectstobeanalyzedbythe
attentionmechanismwhichaddsbiastotheobjectrelevanttothegenerationstep. Finally,
theoutputfromtheattentionmechanismisalsoprovidedtothedecoder,wherethenext
step of the plan to complete a task is generated. The generated step is concatenated to
the task description (initial input), where the process is repeated until a complete plan
isgenerated. IntheAlexaArenaenvironment[167],theKnowledgeBotframeworkwas
usedtogeneraterobotactionsforanembodiedAIingamingsessions. Theframework
wasevaluatedbasedontheGoalCompletionmetric,whichmeasuresthefractionofgame
sessions successfully completed. The success rate on unseen scenarios achieved in the
AlexaArenaenvironmentwas13.6%ontaskssuchas‘pickupthemilkinthefridgeandplace
itonthetable’whichhada1.72%improvementovertheAlexaPrizeTeamBaseline. The
potentialapplicationsoftheKnowledgeBotframeworkinhealthcarecouldinvolveusing
amodelwithdomain-specificknowledgesuchasMED-PaLM2[41]actingastheobject
generator, wheretheLLMinterpretspatientconditionsorstaffinstructionstogenerate
lists of relevant objects and actions such as specific medications, medical devices, and
proceduralstepswhicharethenusedbytheLLM(i.e.,GPT-4)-basedplannertogenerateto
planandexecuteprecisecontext-awareassistancetasks.
SummaryandOutlook
LLMsusedforrobotplanningpurposesareabletocreatehigh-levelplansbygenerat-
ingasequenceofatomicactionsbasedontextualdescriptionsoftherobot’senvironment
provided to the model in the input prompt. Task plans have been used mainly for ob-
ject handling tasks, including object (1) localization and retrieval [80,88], (2) handover
to humans [84], and (3) sorting [78,79]. Extending these abilities to healthcare robotics
throughintegratingLLMsandVLMsintoplanningframeworkscanpotentiallyimprove
theefficacyofhealthcarerobotsbyenablingreal-timedecision-makingincomplexand
unpredictablemedicalenvironments. Thesemodelscanautonomouslygenerateandmod-
ify task sequences in response to dynamic conditions such as emergency interventions
or sudden changes in a patient’s health status. Furthermore, LLMs can be used with
surgicalrobotstogeneratehuman-understandableplansandprovideremindersandcues
based on the current state of the OR (i.e., understanding non-verbal communication of
surgeonsorORnurses)toincreasethesituationalawarenessoftheoperatingteam. This
functionality allows surgeons and physicians to stay informed and involved, ensuring
thatplansareverifiedandapprovedbeforeexecutioninordertomaintainhighlevelsofRobotics2024,13,112 18of43
transparencyduringprocedures. Forrehabilitationrobots,LLMsfacilitatethecustomiza-
tionoftherapeuticexercisesinrealtimeandagainbygeneratinghuman-understandable
planssothatthetherapistiskeptinformed. Theycanalsoadapttreatmentstoimprovethe
recoveryprogressofpatients. Moreover,thestrengthofLLMsingeneratinghigh-leveltask
plansenablesthemtoserveasframeworksforotherdeeplearningmodels,suchasDRL
systems,toexecutemoredetailedlow-levelcommandsforthecontrolofrobotarms,end
effectors,andmobileplatforms. Thislayeredapproachtotaskplanningensuresthatwhile
LLMshandlebroadstrategicdecisions,thefinertacticalaspectsofrobotcontrolarerefined
throughcontinuouslearningmodels,ensuringtheprecisionexecutionofcomplextasks.
Thisintegrationnotonlymaximizestheeffectivenessofinterventionsbutalsoensuresthat
roboticoperationsareadaptedtotheimmediateneedsofthehealthcaresetting.
6. EthicalConsiderationsofRoboticsUsingLLMsinHealthcare
Ethicsisacriticalaspectofhealthcare,encompassingcaregiver–patientinteractions
andtheuseoftechnologytoenhancepatientcare[168].Ethicalconsiderationsfortheuseof
healthcarerobotsembeddedwithLLMsneedtobeaddressedforthewidespreadadoption
ofthisemergingtechnologyinordertosupportpatients,healthcareprofessionals,andad-
vancementsinthisarea. Theethicalconsiderationsincludeaccountability,humanizingcare,
andprivacy[169]. Aholisticapproachisessential,ensuringpatientautonomyovertheir
bodyandmedicalinformationwiththeexpectationofimprovingtheirhealth. Maintaining
theprivacyandprotectionofpersonalhealthinformationisparamount,emphasizingthe
importance of informed consent for data usage and increased efforts to stop data com-
mercializationwhileincreasingtransparencyinhowthedataareused[170]. Regardless
oftheroboticcaredeliverymethod,maintainingqualityandequitableaccessacrossall
demographicsisimportant.
TherehavebeenextensiveseparatereviewsoftheethicsoftheuseofLLMsinhealth-
care[171–180]andtheuseofrobotsinhealthcare[181–191]. Althoughthereexistframe-
workssuchasEthicallyAlignedDesignfromIEEE[192],TheTorontoDeclaration: Protect-
ingtherighttoequalityandnon-discriminationinmachinelearningsystems[193],andthe
AIUniversalGuidelines[194]whichprovideguidelinesonhowtheethicsofsocietyshould
beconsideredduringthedesignphase,theydonotprovideenoughinsightintoexactly
howregulationsshouldbeestablishedforgenerativeAImodels. Moreover,althoughthe
currentgapsinregulatoryframeworksforAIhavebeenidentifiedandcountriessuchas
theUnitedStatesandChinahavestartedtheprocessofestablishingregulatoryframeworks
fortheuseofAIinhealthcare,theseframeworksarenotapplicabletogenerativeAIas
thetechnologyisstillevolvinganditisdifficulttodevelopaframeworkwhichcoversall
thepotentialimpactsofgenerativeAI[195]. Therefore,currentguidelinesandresearch
do not yet address the problems that arise from combining generative AI (LLMs) and
healthcarerobots.
ThissectionaimstointroduceadiscussionontheuseofLLMframeworksforhealth-
carerobotsforthefacilitationof(1)multi-modalcommunication,(2)semanticreasoning
abouthealthcareenvironmentsandpatientandcareproviderneeds,and(3)generating
andexecutingsaferobotactionplansaroundvulnerablepeople,includingfrailandcogni-
tivelyimpairedindividuals. Wediscussthesemainpointsastheypertaintoaccountability,
humanizingcare, andprivacy. Namely, wediscussthesethreeethicalconcernsasthey
directlyrelatetotherobotmulti-modalcommunication,semanticreasoning,androbotic
taskplanningtopicsdiscussedinthispaper.
6.1. Accountability
Accountability in healthcare robotics centers on identifying who is responsible for
system errors and adverse events [169]. To date, there has been no consensus on how
theaccountabilityofhealthcarerobotsembeddedwithLLMsshouldbeconsideredand
whoshouldbeaccountable. LLMsaretrainedonlargedatasetscontaininghumanlan-
guage,andtherefore,LLMframeworksusedinroboticsformulti-modalcommunicationRobotics2024,13,112 19of43
are often perceived as proficient in understanding human language and kinesics [151].
However,LLMframeworksarenotsensitivetochangesinsentencestructure,wordchoice,
or grammar when identifying user requests [196], thereby promoting the illusion that
theyunderstandhumancommunication. AlthoughLLMsappeartounderstandhuman
languagethrough(1)generatinggrammaticallycorrecttextand(2)propositionalreasoning
aboutsentencessuchas‘Thedoctortreatedthechildwiththefever’or‘Thenurseexaminedthe
patientwiththeburn’,itisamirageanddoesnotmeanthattheLLMactuallyunderstands
theunderlyingthoughtandintentbeingconveyedthroughlanguage;rather,itisusing
probabilitiestopredictthenextplausiblewordinthesentence[197]. Therefore,therobot
doesnotunderstandthemedicaladviceitisprovidingorthemodesthroughwhichitis
providingit(i.e.,gestures,bodypostureused). Itcanberiskyifhealthcareprofessionals
becomeover-reliantonLLM-augmentedhealthcarerobotstodeliver,forexample,patient
education. Thisarisesfromthefactthattherobotwillbeabletogeneratecoherentand
convincing(butincorrect)advicewhichmaymisleadpatients,resultingininjuryorthe
worseningofthepatients’healthconditions.
Assemanticreasoningfocusesonobtainingnewknowledgeandassociationsfrom
existingknowledge,itcanhelphealthcarerobotswithnumerousclinicalreasoningtasks
from diagnosis to therapy design. However, it is important that healthcare providers
understandthattheLLM-augmentedhealthcarerobotscannotunderstandthedatawhich
theyhaveembeddedandaremerelygenerating/identifyingpatternswhichwerepresent
duringtraining[173]. Therefore,aclinicianshouldnotbaseapatient’sdiagnosisononlya
pre-screeninginteractionthepatienthadwithanLLM-augmentedhealthcarerobot. While
LLMslikeGPT-4achieveperformanceinthe75thpercentileontheMedicalKnowledge
Self-AssessmentProgram[47],theyarenotqualifiedtofunctionlikephysicians. Physicians
shouldavoidover-relyingonLLM-augmentedhealthcarerobotsforinterpretingpatient
EHRs. Thedivisionofaccountabilityamongstakeholdersremainsambiguousduetothe
lackoflegalprecedentsinthisdomain.
Forhealthcarerobotplanning,itisimportanttodistinguishbetweenafeasibleandan
optimalplan[198],especiallywererobotswillbeoperatingincloseproximitytovulnerable
individuals. Anoptimalplanforahealthcarerobotassistingwithpatientrehabilitation
is one that accounts for the ability of a patient to perform the movements planned by
therobot. However,evenifanLLM-augmentedhealthcarerobotisintelligent,ithasno
conceptualunderstandingofthehumancondition,asAIisnotsentient[199],andtherefore,
thetherapistshouldoverseetheplangeneratedbythehealthcarerobotandensurethatit
issafeforapatienttoperformwiththerobot. Secondly,healthcarerobotswhichgenerate
plansforreal-timeexecutionsuchassurgicalrobotsshouldalwayskeepthesurgeonaware
ofthegeneratedplanbeforeexecution[200]. Forexample,thereshouldbefeedbackfrom
thesurgicalroboteitherintheformofagraphicalinterfaceoraudiblecueswhichinform
thesurgicalteamoftherobot’snextplannedaction. Thiswillensurethatthesurgicalteam
canefficientlymaintaintheirsituationalawareness,remainincontrol,andbeaccountable
topreventnear-misseventsanderrorsduringoperation[200].
6.2. HumanizingCare
Humanizing care in the context of healthcare robotics using LLM frameworks in-
volvesensuringthatintelligentrobotsimprovethecompassionelementsofcare,whichare
integrity,excellence,compassion,altruism,respect,empathy,andservice[201,202].
Multi-modalcommunicationfacilitatedbyanLLMframeworkusespromptengineer-
ingtoaligntheembeddingspaceofanLLMtothecontextofaconversation[203],aprocess
whichisprobabilisticanddoesnotalwaysproducerepeatableresults[204]. Therefore,it
cannotbestatedwithcertaintythatanLLMmodelwillalwaysrespondappropriatelyto
patientsduetotheprobabilisticnatureofpromptengineering. Moreover,promptsengi-
neeredtofacilitatemulti-modalcommunicationareanalogoustoemotionprompting[99],
whichcanresultin(1)theoverexaggerationofnon-verbalcuesand(2)thegenerationof
oversimplified language as a result of the emphasis on emotional responses which canRobotics2024,13,112 20of43
omitpertinentinformation. Bothofthesecanleadtopatientinfantilization,especiallyin
repeatedinteractionsbetweenvulnerabledemographicsandLLM-augmentedhealthcare
robots[205]. Theinfantilizationofpatientsandlackofcontroloverahealthcarerobot’s
responsetopatientsviolatescompassion,respect,andempathicaspectsofwhatitmeans
toprovidecompassionatecare. Additionally,theuseofLLM-augmentedhealthcarerobots
hasthepotentialtoworsenthedisparityinequalaccesstohealthcarewhilealsoincreasing
the gap in the standard of care between demographics. For example, GPT-4 supports
26differentlanguages,andwhentestedonLLMbenchmarkssuchasMassiveMultitask
LanguageUnderstanding(MMLU)[206],itsthree-shotaccuracyrangesfrom85.5%(En-
glish) to 62% (Telugu) [47]. Therefore, an LLM-augmented healthcare robot developed
usingGPT-4asthebackbonecandifferinthe(1)qualityofcareand(2)accesstocarein
casesofunsupportedlanguagesanddialects,ultimatelycreatinginequalitiesinhealthcare
environments.
ThedatausedtotrainfoundationalLLMs(i.e.,GPT-4,GPT-3,PaLM2)consistofdata
availablefromtheinternet[207]. However,notalldemographicshaveequalopportunities
tocontributetothisdataduetoavarietyofreasons(access,awareness)[208]. Therefore,
healthcare-specializedmodelssuchasMED-PaLM2[42]derivedfromfine-tuningfoun-
dationalmodels(PaLM2)onmedicalinformationwillalsobeunrepresentativeofhealth
conditionsprevalentinminoritydemographics[209]. Asaresultoftheseskewedtraining
data,anLLM’sembeddingspacewillnothavetheknowledgeneededtoreasonbasedon
patientinformationsuchas(1)culturalbackgroundand(2)patienthealthconditionsin
ordertoprovideculturallyappropriatemedicaladvicesuchassuggestingdietarychoices.
Thisneedstobeaddressedsothathealthcarerobotsdonotpotentiallyexacerbateequality
gapsbetweenminorityandmajoritydemographicgroups.
6.3. Privacy
ThecombinationofLLMframeworksandhealthcarerobotsamplifiesdataprivacy
risks. Traditionally,LLMsrequireausertocreateapromptandinitiateaninteraction[210].
However,healthcarerobotsaugmentedwithLLMframeworksaremobileandcontinuously
use environmental stimuli as input into their models for the purpose of multi-modal
communication,semanticreasoning,androbotactionplanning. Theseincludevideosof
patientsandaudiosofconversationsinanenvironment. Thismobilityallowstherobots
to pick up information from various locations (i.e., waiting and patient rooms, triage
stations, etc.), thereby increasing the likelihood of processing confidential information
and contributing to potentially exposing this information. Furthermore, during direct
communicationwithpatients,healthcarerobotsshouldnotdiscloseconfidentialpatient
informationinamyriadofscenarios. Forexample,ifamalicioususerqueriesahealthcare
robotaboutarecentpatient,duetoinformationretainedinthecontextwindowofanLLM,
therobotinadvertentlydisclosessensitivepatientdetails[211]. Moreover,LLM-augmented
healthcarerobots,lackingsentience,alsohavealimitedunderstandingofprivacynuances,
which can lead to the inadvertent disclosure of sensitive medical information about a
patienttheyareinteractingwith[212].
Ifhealthcarerobotsuseaclosed-sourceLLM(i.e.,GPT-4)forsemanticreasoning,they
arealsoprovidingaccesstotheirdeveloperstousetheinteractionhistorytoimprovetheir
models[213,214]. Thiscanleadtoprivacyissues,asonceanLLMistrainedoninteraction
histories,itincorporatesthesedataintothemodel’sweights,effectivelyembeddingthe
learnedinformationwithinthetransformerarchitecture[215]. Consequently,whenused
forsemanticreasoningaboutpatientinformation,ifthemodelencountersaninputthatis
similartoapreviousinteraction(e.g.,similarpatientdescriptions),itislikelytoreference
the part of the embedding space where this previous interaction was stored. This can
potentiallyleadtothereuseorinadvertentdisclosureofspecificdetailsfromthoseprior
interactionsinitsoutput[216]. Thismethodofextractingmodeltrainingdataisreferredto
asamodelinversionattack[217]. Inamodelinversionattack,anadversaryusesprompt
engineering to extract sensitive training data details embedded in the model’s weightsRobotics2024,13,112 21of43
and revealed through its outputs [217,218]. Furthermore, a model inversion attack can
alsobeusedtorecoverpreviouspromptsprovidedtothemodel[219],whichinthecase
ofhealthcaremayincludethepersonallyidentifiableinformationofapatientorvisitor
whohaspreviouslyinteractedwiththehealthcarerobot. Byanalyzingtheseresponses,
the attacker can infer and reconstruct aspects of the original data, especially when the
inputsmimicthetraining. AmodelinversionattacktargetedtowardsanLLM-augmented
healthcarerobotcanresultinabreachoftheconfidentialityofpatientsandshouldbea
majorprivacyconcernthatneedstobeaddressed.
SummaryandEthicalOutlook
Thissectionhasoutlinedkeyethicalissuesconcerningaccountability,humanizing
care, and privacy related to the potential incorporation of LLM-embedded healthcare
robotsinhealthcaresettings. Theseareimportantconcernsfordevelopers,researchers,and
healthcareprofessionalstoconsider. Itisalsoimportanttoconductlong-termstudieswith
suchtechnologiestoexploretheirimpactdirectlyonthedeliveryofhealthcare.Suchstudies
wouldneedtospecificallyconsidertheworkloadandburdenofcarestaff,patientoutcomes,
andmanagementoftasks. Inparticular,thesestudieswillhelptobetterunderstandhow
healthcarerobotsusingLLMswilladdvaluetoclinicalpracticesandpatientinteractions
overtimewhileautonomouslyaugmentingpatientcare. Furthermore,theywillbecrucial
in identifying training and deployment strategies to ensure the ethical, effective, and
responsibleuseofthisemergingtechnology.
7. OpenChallengesandFutureResearchDirectionsinHealthcareRobotsUsingLLMs
ThepotentialuseofLLMframeworksinhealthcareroboticscanenhancerobotintel-
ligencebygeneratinganaturallanguageofsemanticknowledge,promotingautonomy
throughtaskplanning,andenhancingHRIcapabilitiesthroughmulti-modalcommuni-
cation. In this section, we discuss the open technical research challenges and potential
researchdirectionsofthisemergingfield.
7.1. OpenResearchChallenges
Therearethreemainresearchchallengesthatneedtobeaddressedbeforehealthcare
robotsaugmentedwithLLMframeworkscanbewidelyadoptedinreal-worldcareenvi-
ronments: (1)theslowresponsespeedofLLMsinreal-timehealthcareroboticsinteractions,
(2) open- versus closed-source embedded LLMs, and (3) generalizability for healthcare
robotics. Thesechallengesaredescribedinthefollowing:
1. Slow response speed: The first technical challenge is due to the time required
for an LLM to generate an appropriate output such as a plan or action. For example,
the time required to generate a robot plan using remotely hosted LLMs such as GPT
seriesmodels[46,47]hasbeenfoundtotakeanywherefrom36.89to220.58sdepending
on the task complexity [220], while the time required by locally hosted models such as
LLaMa[221]canrangefrom73to234s[222]. Ingeneral,theresponsetimeincreasesasthe
totalnumberoftokensperqueryincrease,thereforelimitingthehorizonofrobotaction
plansandcommunication[223]. However, healthcarerobotshavereal-timeconstraints
and need to be able to generate robot action plans and/or communication behavior in
real time and adapt to user and environmental changes. Failure to generate a plan in
real time can result in obsolete action plans and/or task incompletion. In comparison,
real-time robot plan generation using classical methods such as the Hierarchical Task
Network(HTN)[224]orAnswerSetPlanning(ASP)[225],suchasClingo4[226],iscapable
ofreal-timeplangeneration. Forexample,HTNcantakeapproximately2–17sforobject
localizationandretrievaltasks[224]. InordertoimprovethetimeperformanceofLLM-
augmentedhealthcarerobots,weneedtofurtherexplore(1)theuseofpromptengineering
for use in healthcare robots to update the context embedding using as few tokens as
possibletoreducecomputationaloverhead[227];(2)optimizingthecontextwindowusing
‘attentionsinks’topreservetheKeyandValuestatesoftheinitialtokens,ensuringthatRobotics2024,13,112 22of43
the initial instructions to the LLM model are not discarded [228], and thus, additional
tokensarenotneededtore-alignahealthcarerobot;and(3)contextcachingthroughstoring
Key–Value activations from previously processed tokens and referencing these cached
activationsduringinferenceratherthanrecomputingthemforeachnewtoken[229],which
will reduce the response generation time [229]. These methods aim to reduce memory
bandwidth,memoryusage,andcomputationinanattempttoincreasetheresponsespeeds
ofthemodels.
2. Open-versusclosed-sourcemodels: Bothopen-sourceorclosed-sourcemodelscan
beconsideredintheselectionofLLMsforhealthcarerobots. Closed-sourcemodels,such
asGPT-4[47],typicallyoutperformopen-sourcealternativesacrossbenchmarkssuchas
MMLU[206]andHellaSwag[230],whichhaveadirectcorrelationwithamodel’sability
tobeusedformulti-modalcommunication, semanticreasoning, androbotactionplan-
ning[231]. However,thesemodelshavedrawbacks,notably,alackoftransparencyand
restrictedcontroloverdatausage,raisingprivacyandcomplianceissueswithstandards
such as HIPAA [232], GDPR [233], and PIPEDA [234]. On the other hand, open-source
modelssuchasPaLM2[53],LLaMa2[54],GPT-2[55],T5[48],andBERT[52]offerfull
transparencybymakingtheirarchitectureandcodepubliclyaccessible,allowinghospitals
tohostthesemodelsontheirlocalserversandsavetheinteractiondatalocallytoensure
controloverthedata. Despitethesebenefits,open-sourcemodelsoftenunderperformcom-
paredtoclosed-sourcemodels[235]. Thisdiscrepancyisaresultoflimitedresearchcapital,
ultimatelyleadingtolowerqualitytrainingdatasetsandalackofcomputingresourcesto
trainbiggerandmorecapablemodels.Improvingopen-sourceLLMsforhealthcarerobotics
involvesenrichingtrainingdatawithspecificdetailssuchasmedicalprocedureexecution
andpatientdataprocessing. Furthermore,addingdatasetsonroboticpathplanningin
healthcareenvironments,proceduralcompliance,staffinteractionprotocols,andpatient
safetycansignificantlyenhanceamodel’srelevanceandeffectivenessinhealthcaresettings,
leadingtomoreaccurateandcompliantoutcomes. Moreover,collaborativedevelopments
betweenhealthcareinstitutesshouldbeencouragedtosharecostsandexpertise. These
steps,whilerequiringasignificantinvestmentoftime,money,andexpertise,arecriticalfor
optimizingopen-sourceLLMsforhealthcareapplications.
3. Generalizabilityforhealthcarerobotics: LLMframeworksneedtobeadaptable
acrossdifferenttypesofhealthcarerobotswithoutbeingrestrictedtospecificrobotmodels.
Hospitalsseeklong-terminvestmentswhichoftensurpassadecade[236],andtherefore,
LLMframeworksthatdonotnecessitateredevelopmentforeachnewroboticsystemshould
beprioritized. Tofuture-proofanLLMframeworkforhealthcarerobots,theLLMshould
implement a modular architecture designed with a general application programming
interface, similar to the ROS [109] framework for robotics. This architecture should be
designedtofacilitateabridgebetweentheLLMframeworkandtherobot’sperceptionand
controlsystems,whichmanageactuatorcontrolandsensordatacollection. Themodular-
basedapproachshouldutilizecurrentcommunicationprotocolstoprovidetheLLMwith
insights into a robot’s capabilities and allow the robot to receive high-level commands
ornaturallanguagescriptstobeusedinHRI.Theintegrationshouldalsoprovidecare
providerswithauser-friendlyGUItoreviewandmakeefficientchangestopromptsused
by the LLM in the background for robot behavior control using natural language and
therebynotoverwhelminghealthcarestaffwithtechnicalcomplexities.
7.2. FutureResearchDirections
The aim of the emerging field of healthcare robots with embedded LLMs is to de-
velopintelligenthealthcarerobotscapableof(1)adaptingtoandfunctioninginvarying
environmentsfromemergencyandurgentcaredepartmentstosurgeryandacutecareto
rehabilitationcentersandlong-termcarefacilities;(2)manipulating,fetching,anddeliv-
eringawiderangeofobjectsincludingmedicalinstrumentsandtools,medications,lab
specimens, soft goods such as gauze and bandages, food and nutritional supplies, and
personalcareitemslikeblanketsandpillows;and(3)interactingwithdiversepeoplefromRobotics2024,13,112 23of43
surgeons,doctors, andnursestopatientswithvariousconditionsandfamilymembers.
Withrespecttothelatter,incorporatingtheperspectivesofhealthcareprofessionalsand
patientusersinthedeploymentofhealthcareroboticsusingLLMsiscrucialtobridging
researchwithreal-worldhealthcareapplications. Inparticular,co-designanduser-centered
designapproachescandirectlyincludetheinsightsandexperiencesoftheseindividualsin
thetechnologydevelopmentprocesswhilemaintainingtransparencyindecision-making
in order to closely align such technology with user needs and preferences. Clear and
understandableexplanationsofroboticactionsandbehaviorstouserswillfurtherbuild
trust,enhancingthereliabilityandsafetyessentialforwidespreadadoption[237].
Regulatorycomplianceframeworksshouldbeconsideredtoensurethatregulations,
guidelines, and/or legislation are met when incorporating LLM-embedded healthcare
robotsinhealthcaresettingstoensuresafetyandsecurityintheiruse. Inparticular,the
needforsuchframeworksexists;however,theframeworksthemselveshavenotyetbeen
designed,andtherearenouniversalstandards[238–240]. Establishingstringentguidelines
ondataprivacy,transparency,theexplainabilityofdecisionsmadebyhealthcarerobots,
and the protocols for the human oversight of robot actions will not only promote legal
andethicalusebutalsoimprovetheintegrationofthesetechnologiesintothehealthcare
sector. Thisimprovementwillbearesultofclearandenforceableguidelineswhichhelp
healthcareorganizationsnavigatelegalandethicalcomplexities,thusfosteringtrustamong
stakeholders and patients [241], thereby improving patient care while minimizing the
dehumanizationofpatientsand,thereby,fosteringtrustamongusersandstakeholders.
To date, existing robots have yet to generalize to such a wide range of tasks, envi-
ronments, and HRI scenarios. However, we believe that healthcare robots augmented
with LLMs can provide (1) effective HRI interfaces through intuitive natural language
communication to enable smoother interactions across various healthcare settings and
(2)versatilityingeneratinghigh-levelrobotactionplansandsemanticallyreasoningabout
amyriadofpossiblescenarioswhichcouldtakeplaceinORs,patientwards,andoutpatient
clinicsandalsotohandleawiderangeoftasksfromsurgicalassistancetopatientcareand
administrativeduties. Futuretechnicalresearchdirectionscanincludetheincorporationof
VLMswithLLMsinhealthcarerobotstoincreaseperceptioncapabilities,theuseofmulti-
lingualLLMstoallowforapplicationwithdiverseusers,theincorporationofautomated
promptingtohandlevaryinghealthcarescenarios,andthedevelopmentofcustomLLMs
forhealthcarerobots. Weselectedthesefutureresearchdirectionsastheyareunexplored
research avenues for LLM architectures, in particular in terms of their applicability to
healthcareapplications. Thesefutureresearchdirectionsaredescribedinthefollowing:
1. IncorporationofVision–LanguageModels(VLMs): VLMsarelanguagemodels
whichco-embedimageandtextdata. Theyaretypicallytrainedonextensivedatasets,such
asMS-COCO[242]andVisualGenome[243],composedofimagesrangingfromnatural
scenerytocommonobjectsalongsidetheircorrespondingtextualdescriptions. VLMsexcel
atrecognizingandnarrativelydescribingvisualcontentasaresultoftheimage–textjoint
embeddingspace[244]. However,inhealthcare,theconventionalrelianceonRGBimages
for VLM inputs is an ethical concern due to the potential breach of patient and visitor
confidentiality[245]. Toaddressthis,healthcareapplicationsmaypreferentiallyusepoint
clouddata,whichcapturethree-dimensionalspatialinformationbyrepresentingscenesor
objectsasacollectionofverticesinacoordinatesystem[246].AdaptingVLMstoworkwith
pointcloudsinvolvesretrainingthemodelsusingdatasetswhichincludelabeled3Dspatial
data and generating a co-embedding space between point clouds and texts [247]. This
adaptationnotonlyhelpsmitigateprivacyconcernsbutalsoexpandstheutilityofVLMs
inhealthcare, offeringanewdimensionofdata. Forexample, thetransitionfromRGB
topointcloudscanimprovemulti-modalcommunicationandpatientintentrecognition
through the analysis of 3D point clouds of human body poses [248], which can in turn
providetheLLMwithmoreinformationtobeusedduringsemanticreasoningtogenerate
planscognizantofthepatientpsychologicalstatewhengeneratingplanstoassistpatients.Robotics2024,13,112 24of43
2. Leveraging multilingual LLMs for diverse populations: Closed-source LLMs
such as GPT-4 [47] contain multilingual capabilities. For example, GPT-4 [47] supports
27languagesrangingfromEnglishtoUrdu[47]. However,open-sourcemodelssuchas
LLaMa2[54]areonlytrainedinEnglish[249],anditisacommunityprojectthatresearchers
needtoundertaketoexpandthelistofsupportedlanguages[250]. Collecting/refininga
multilingualtrainingdatasetandtraininga70Bparametermodelisresource-intensiveand
difficulttocarryoutbyresearchers.Furthermore,thereisavarietyofdemographicspresent
inhealthcaresettings. LLM-augmentedhealthcarerobotsshouldbecapableofinteracting
withalldemographicswithequalproficiencytomaintainthecompassionateelementsof
care. Therefore,itisworthwhiletoinvestigatemethodssuchastransferlearning[251,252]
whichcanincreasetheefficiencyoftrainingLLMstosupportnewlanguages. Forexample,
in[251],itwasrevealedthatapproximately1%ofthetotalmodelparametersofLLaMa2
correspondstolinguisticcompetencewhichrepresentsanLLM’sknowledgeofgrammatical
rulesandpatterns[253]. Therefore,byholdingtheweightsofthemodelconstantinthe
specific regions of the embedding space of the LLM during further training (where a
regionencodesthelinguisticknowledgeofaparticularlanguage),modelsnotonlyretain
previouslyacquiredlanguagesmoreeffectivelybutalsodemonstrateaheightenedcapacity
forrapidadaptationtonewlinguisticenvironments.
3. Incorporating automated prompting to handle various healthcare scenarios:
Healthcaresettingsprovidechallengingbutrealisticreal-worldenvironments.Forexample,
for healthcare robots to provide multi-modal communication, prompt engineering can
be used to adapt robot assistive responses based on the age, health conditions, and/or
cognitive or physical disabilities of patients. The advantage of prompt engineering in
healthcareroboticsformulti-modalcommunicationisthatitfacilitatescustomizedcontext-
sensitiveinteractionstailoredtoindividualpatientprofiles. Namely,byusingpromptsto
aligntheLLMtobetterrepresenttheuser,themodelisdirectedintotherelevantembedding
spaceregionbasedonthecurrentcontext.Thereby,themodeldynamicallymodifiesrobotic
responsesbasedonspecificpatientdatasuchasage,healthconditions,andcognitiveor
physicalabilities.Automatedpromptinginhealthcaresettingscanimprovetheefficiencyof
promptengineeringthroughtechniquessuchaspromptcompressionwhichaimstoextract
onlytheessentialinformationfrompromptsusingknowledgedistillationtoteachsimpler
modelstomimicmorecomplexoneswithshorterinputs,encodingtoreducepromptsinto
compactvectorrepresentations[254]. Thistherebycondensescomplexmedicaldatainto
actionableprompts. Promptoptimizationfurtherrefinestheseinteractionsusinggradient-
basedoptimization,selectingthemosteffectiveformat[254]. Thisdynamicmodification
ofroboticresponsesensuresmulti-modalcommunicationthatisnotonlyresponsiveand
context-sensitivebutalsoadaptstodiverseclinicalscenariostoenhancetheoverallquality
ofcareinreal-worldhealthcareenvironments.
4. CustomizedLLMsforhealthcarerobotics: Inhealthcareapplications,technologies
mustnotonlyperformoptimallybutalsoadheretostrictethicalstandardssuchasthose
outlinedbyHIPAA[231]. TheadoptionofLLMsinhealthcaresettingsrequiresthepoten-
tialdesignofcustomLLMmodelsthatconsiderthetransparencyofhowandwhatdataare
usedandthecontrolofthedataandmaintainhighperformanceingeneratingcohesiveand
appropriateoutputsforrobotmulti-modalcommunication,semanticreasoning,andtask
planning. Forexample,OpenAIhascustomGPTs[255],whichallowuserstofine-tuneGPT
modelsbasedontheirowndataandnothavetosharetheirinteractionswiththedevelopers
oftheGPT(i.e.,OpenAI).However,usingthesecustomGPTsdoesnotallowhealthcare
organizationstohavefullcontroloverthemodelbehavior,asthemodelisdesignedandits
behaviorisfine-tunedusinghumanreinforcementlearningbythedevelopers[238]and
thedataarestoredonremoteserverslimitingthecontroloverdatahandlingprotocolsby
hospitals. Furthermore,currentmethodsusedforremovingpersonallyidentifiableinfor-
mationfromLLMmodelssuchasmanualdatascrubbing[256]andretrainingmodelswith
sanitizeddatasets[257]canbecostlyandtime-consuming[256,258]. Inparticular,manual
datascrubbingrequiresextensivehumanoversighttoidentifyandremovepersonallyiden-Robotics2024,13,112 25of43
tifiableinformation[259],whichislabor-intensiveandpronetoerrors. Namely,datasets
usedfortrainingLLMssuchasCommonCrawlcontainpetabytesofdata[260]. Therefore,
itisnotpossibletohavecompletehumanoversightoverthedatacleaningprocess. Hence,
thisiswhyheuristicmethodsareusedtofindandreplacewordsintandemwithhuman
oversight[48]. Theprocessofretrainingmodelswithsanitizeddatarequiressignificant
computational resources and also involves lengthy cycles of validation where humans
interactwiththemodeltoensuresatisfactoryresponsesandtheintegrityoftheperfor-
manceofamodelsuchasGPT-4[238],whichisoftenreferredtoasreinforcementlearning
fromhumanfeedback(RLHF)[261]. Therefore,thereisaneedtodevelopnewefficient
differentialprivacystrategies[262]whichcanminimizetherisksofdatacontaminationand
leakageofpersonallyidentifiableinformation(PII)duringtheinitialtrainingofanLLM
model. Namely,in[263],itwasnotedthatusingdifferentialprivacyguidelinessuchas
addingnoisetothetrainingdataresultsina10%increaseinaccuracyinthePIIinferenceof
theGPT-2model.However,addingexcessivenoiseanddatascrubbingusingNamedEntity
Recognition[264]tofurtherlimittheinferenceofPIIsignificantlydegradestheutilityofthe
models. Therefore,itisworthwhiletoinvestigatenewmethodstoscrubPIIandprevent
theleakageofPII.Forexample,in[265],aLLaMA7Bmodelwasfine-tunedonuser–LLM
interactionswithdifferentialprivacyusingDP-Adam[266],andthen,thefine-tunedmodel
wasusedtogenerateasyntheticdataset. Thesyntheticdatasetwasthenresampledusinga
DPhistogramtoalignthedistributionofthesyntheticdatasetwiththerealdataset. The
resampleddatasetwasusedtotrainasubsequentmodel. Thismethodshowedpromise
byproducingan8.6%relativeimprovementinperformancecomparedtousingtheinitial
dataset. TheaforementionedmodelsattempttominimizetherisksofleakedPII;however,
theiraccuracymaynotmeetregulationssuchastheEURightToBeForgottenregulation
whichdictatestheremovalofthepersonaldataofusers[267]. Therefore,researchisstill
neededtodeterminehowsuchapproachescanbeusedforcleaningdatasetsusedtotrain
LLMswhicharestructurallysimilarlytowebcrawls(i.e.,massivetextualcorpuses).
8. DesignofPotentialHealthcareApplicationsofLLM-EmbeddedHealthcareRobots
Inthissection,wefurtherexploreindetailspecificpotentialhealthcareapplication
designsforLLM-basedhealthcarerobotsintermsofmulti-modalcommunicationduring
assistiveHRIandsemanticreasoningandrobottaskplanningforroboticsurgery.
8.1. Design1: Multi-ModalCommunicationforaSociallyAssistiveRobot
Scenario: A socially assistive robot can be used for Reminiscence/Rehabilitation
Interactive Therapy [268] with individuals living with dementia. These reminiscence
activitiescanincludetherecallofpastevents,includinglisteningandsingingtofavorite
songs,watchinganddiscussingfavoriteTVshowsormovies,anddiscussingsignificant
historicalevents. Therobotshouldbecapableofinterpretingtheverbalandnon-verbal
responsesoftheusersandadaptingitsinteractionstotheiremotionalstatestopromote
engagementandemotionalwell-being.
Tofacilitatetherapy,asociallyassistiverobotwouldrequireeitherageneratedgeneral
knowledgebasecontainingdetailsabouthistoricalevents,movies,TVshows,andmusic
ortheabilitytosearchthewebforthisinformation. However,therobotwouldrequire
dedicatedsearchalgorithmstoefficientlyfindrelevantinformationfortheassistiveHRI
scenarios. Withtheuseofdatasets,therobotisrestrictedtoonlythelimitedinformation
availableinadataset,whilewebsearchesfocusonkeywordswithinawebpageandthen
convert all text within that webpage into speech. LLMs can be used to address these
challenges. Theydonotrequireadatasetforonlineknowledgeretrieval(onlyfortraining
purposes), allowing them to obtain new information on the fly from the web or from
additionaldatasetsknownasvectordatabasesasneeded. Thelatterisknownasretrieval-
augmentedgeneration(RAG)[269]. Furthermore,traditionalwebsearchesdonotanalyze
or summarize the content on webpages, whereas LLMs generate responses containing
newcontentwhicharebasedonpatternsandassociationstheyhavelearnedandthatareRobotics 2024, 13, x FOR PEER REVIEW 29 of 45
The Converse module is used to facilitate multi-modal HRI and uses GPT-4v for emo-
tion recognition and response generation to user requests. The Converse module will con-
catenate the therapist’s verbal prompt to Prompt 3 to be used with GPT-4v. Prompt 3 out-
lines GPT-4v’s role and includes RGB images of the user obtained from the Sensor module,
for example, ‘you are embedded within a socially assistive robot to provide Reminiscence/Reha-
bilitation Interactive Therapy to older adults with dementia. You will also be provided with images
of the scene, if you detect the user to have negative emotions, start reminiscence therapy based on
this information [patient information]’. The user request will also be concatenated to this
prompt to enable GPT-4v to generate a contextually accurate textual response. The textual
response from GPT-4v is then concatenated to Prompt 4 to be used with the third instance
of GPT-4. The third instance of GPT-4 is also within the Converse module and is used to
embed atomic actions (non-verbal elements) into the textual response generated by GPT-
Robotics2024,13,112 26of43
4v. Namely, Prompt 4 defines the atomic actions (gestures, facial expressions, body poses)
and shows examples of how they will be integrated into the response of GPT-4v (verbal
response to the user). This prompt will result in GPT-4 generating the output of the Con-
availabletothemonnewwebsites,increasingflexibilityandadaptabilitytonewtopics
verse module containing the robot’s verbal response to the user, embedded with atomic
andactivities[39]. Inparticular,weconsidertheutilizationofGPT-4[47]forPersonalized
acRteiomnisn itsoc epnrcoeviTdhee rraopbyo.t Beemloowti,ownaelo iunttleirnaectthioendse asnigdn toof aallpoowte tnhteia rlofbraomt teow foarckiliftoartea msoucilatil-ly
maosdsiaslt iHveRrIo. botthatcouldleverageGPT-4tofacilitateReminiscenceTherapy,Figure1.
FiFgiugurer e1.1 L.LLMLM-e-membebdeddedde dfrfarmameweworokr karacrhchitietcetcuturer efofor rsosocicailallyly asassissitsitvive erorobboot tinin ddeseisgignn 11. .
Dynamiccontentretrieval: GPT-4canimprovetheefficiencyofasociallyassistive
robot in providing reminiscence activities by conducting web searches and generating
abstractivesummaries,ratherthanprovidingverbatiminformationfoundonwebpages,
astraditionalwebsearchesdo. Morespecifically,GPT-4canretrieveinformationsuchas
historicalevents,queuemoviesandTVshows,andplaymusicthroughAPIcallstovideo
sharingplatformssuchasYouTubeandSoundCloud. Theretrievedinformationcanbe
transferredintorobotspeech,andthemediacanbeplayedthrougharobot’sdisplayscreen
andspeakerstoprovideReminiscenceTherapy. Tofacilitatedynamiccontentretrievalthat
ispersonalizedtoeachuser,atherapistcanprovideaninitialspokenlanguageprompt
which provides information about the user’s interests and past hobbies to GPT-4. This
informationisstoredwithinGPT-4’scontextwindow,andfutureuserrequestsformedia
willretrieverelevantcontentbasedontheuser’sinterestwithouttheneedtoprovidethe
specificnamesofsongs,movies,orTVshows.
Emotionrecognition: GPT-4v[270],whichisthemulti-modalvariantofGPT-4,can
detecttheemotionsoftheuserfromRGBimagestakenbythesociallyassistiverobot’sRobotics2024,13,112 27of43
onboardcamera. GPT-4vcandetectthevisualfeaturesofapersonandusethesedetected
featurestoinfluencefuturetextualresponses. Forexample,whenpromptedwithimages
ofapersonshowingclearsignsofdistress,GPT-4vengagesinempatheticconversations.
The capability of GPT-4v to identify the emotional states of a user enables the socially
assistive robot to adapt its verbal and non-verbal response during assistive HRI to the
user. Forexample,therobotcanchangediscussiontopics,simplifyaconversation,play
upliftingmusic,orcontinuetoconverseaboutatopicwhichisobservedtobeeffective.
Thisfunctionalityenablestherobottointeractusingmultiplemodesofcommunication
andtoalsounderstandmultiplemodesofcommunication,therebyfacilitatingbidirectional
multi-modalHRIwithouttheneedforextensivepre-programmingandcomplexdesigns,
which is a current barrier to the adoption of socially assistive robots for use with older
adultsdiagnosedwithdementia[271].
Multi-modalinteractions: GPT-4cangeneratespeechandincorporatepredefined
atomicactionsforthesociallyassistiverobotusingfew-shotlearning. Theseatomicactions
canincludegestures,bodylanguage,andfacialexpressions. Namely,atextualresponseis
generatedbasedonanaturallanguagepromptfromatherapistwhichincludescontextual
informationabouttheusersuchasinterestsandthecognitiveimpairmentleveloftheolder
adultandatranscriptionoftheuserrequestprovidedtoGPT-4bythespeech-to-textservice
usedbytherobot. Theresponseisthenconveyedverballytoauserusingtext-to-speech
softwaresuchastheGoogleCloudtext-to-speechservice[161]. Tofacilitatenon-verbal
communication,aseparatepromptforGPT-4iscreatedcontaining(1)thenon-verbalatomic
actions, (2) instructions on how to embed the atomic actions into the generated textual
response resulting from the first prompt, and (3) the generated textual response itself.
Theresultinggeneratedtextwillincludethetextualresponsewithnon-verbalelements
(atomicactions)embeddedwithin. Specifically,theattentionheadmechanismsofGPT-4
applyabiastoeachtoken(word)ofthesecondinputprompt,thereforeprioritizingatomic
actionsthataremostrelevanttothecurrentinteractionstatethatisstoredinthecontext
windowofthemodel. Theinteractionstaterepresentsallinteractionsthathaveoccurred,
includingallinputstoGPT-4,andthegeneratedoutputsofGPT-4(i.e.,atomicactions,user
requests,responses). Consequently,theactionswhichareembeddedwithinthetextual
response of GPT-4 are the non-verbal elements most appropriate for the current state
oftheinteraction,ensuringamoreseamlessandcontextuallyappropriatemulti-modal
communicationexperience.
Frameworkintegration: ThreeinstancesofGPT-4arecreated,andeachinstancerefers
toadistinctconfigurationofthemodelhavingadistinctrole,distinctpromptsasoutlined
inTable3,andadistinctcontext. ThefirstinstanceofGPT-4isusedtodeterminetheuser
intent,thesecondisusedforthedynamicretrievalofrequiredmediafromtheweb,and
thethirdisusedtoembedatomicactions(non-verbalelements)intothegeneratedtextual
responses of GPT-4v. Additionally, one instance of GPT-4v is also utilized for emotion
recognition. AllimplementationsaresetupusingtheOpenAIAPI[272]whichallowsfor
accesstotheGPT-4andGPT-4vmodelsthroughHTTPSrequests.
ThefirstinstanceofGPT-4isusedintheDecisionmoduleintheframework(Figure1)
to determine whether the user is seeking entertainment such as TV shows, movies, or
musicoriftheyarelookingtodiscusshistoricaleventsorengageingeneralreminiscence
conversations. The Decision module uses Prompt 1 and prompts this instance of GPT-4
with ‘Determine the intent of the user request, does the user seek entertainment? If yes return
“entertain(user_request)”oriftheuserisseekinghistoricaleventsandconversationreturn“con-
verse(user_request)”’andconcatenatesthetranscriptionoftheuserrequest(providedby
thespeech-to-textservice)totheprompt. GPT-4willgenerateatextualresponsewhich
iseithertheentertain(user_request)functionortheconverse(user_request)function. Theap-
propriatefunctionthatisspecifiedintheresponseofGPT-4willbeexecuted,utilizingthe
correspondingEntertainorConversemodule. ThesecondinstanceofGPT-4isembedded
intotheEntertainmoduleoftherobotforthepurposeofdynamicallyretrievingcontent
basedontheuser’sinterestswhicharedefinedbythenaturallanguageprompttoGPT-4Robotics2024,13,112 28of43
by the user’s therapist. Additionally, Prompt 2 will be used to provide the model with
examplesonhowtosearchandobtainmediafrom,forexample,theYouTubeAPI[273]
basedontheuser’srequest. TheEntertainmodulewillgenerateatextualresponsewhich
isAPIcallstoretrieveandplaybackmediausingtherobot’sonboardscreencontrolled
throughitslow-levelcontroller.
TheConversemoduleisusedtofacilitatemulti-modalHRIandusesGPT-4vforemo-
tion recognition and response generation to user requests. The Converse module will
concatenate the therapist’s verbal prompt to Prompt 3 to be used with GPT-4v. Prompt
3outlinesGPT-4v’sroleandincludesRGBimagesoftheuserobtainedfromtheSensor
module, for example, ‘you are embedded within a socially assistive robot to provide Reminis-
cence/RehabilitationInteractiveTherapytoolderadultswithdementia. Youwillalsobeprovided
withimagesofthescene,ifyoudetecttheusertohavenegativeemotions,startreminiscencetherapy
basedonthisinformation[patientinformation]’. Theuserrequestwillalsobeconcatenated
tothisprompttoenableGPT-4vtogenerateacontextuallyaccuratetextualresponse. The
textualresponsefromGPT-4visthenconcatenatedtoPrompt4tobeusedwiththethird
instanceofGPT-4. ThethirdinstanceofGPT-4isalsowithintheConversemoduleandis
usedtoembedatomicactions(non-verbalelements)intothetextualresponsegeneratedby
GPT-4v. Namely,Prompt4definestheatomicactions(gestures,facialexpressions,body
poses)andshowsexamplesofhowtheywillbeintegratedintotheresponseofGPT-4v
(verbalresponsetotheuser). ThispromptwillresultinGPT-4generatingtheoutputof
theConversemodulecontainingtherobot’sverbalresponsetotheuser,embeddedwith
atomicactionstoproviderobotemotionalinteractionsandtoallowtherobottofacilitate
multi-modalHRI.
Table3.Promptsfordesign1.
Determinetheintentoftheuserrequest,doestheuserseekentertainment?Ifyesreturn“entertain(user_request)”
Prompt1
oriftheuserisseekinghistoricaleventsandconversationreturn“converse(user_request)
YouareapartoftheEntertainmodulewithinasociallyassistiverobot.Yourroleistoaccessandprovide
entertainmentbasedonthepreferencesandrequestsoftheuser.Giventhetextualtranscriptionoftheuser’s
spokenrequest,usethefollowingsequenceoffunctioncallstoguideyourresponse.
Example1:
UserRequest:‘Iwanttowatchadocumentaryaboutspace’.
APICall:searchYouTube(‘documentaryaboutspace’)
FunctionCalls:
1.video_id=fetchVideoID(‘documentaryaboutspace’)
2.video_path=saveVideo(video_id)
Prompt2 3.playMedia(video_path)
Example2:
UserRequest:‘Playsomeclassicalmusic’.
APICall:searchYouTube(‘classicalmusicplaylist’)
FunctionCalls:
1.video_id=fetchVideoID(‘classicalmusicplaylist’)
2.video_path=saveVideo(video_id)
3.playMedia(video_path)
Basedontheuser’scurrentrequest,followthesestepstoretrievethevideoID,saveit,andthenplaythemedia.
UsetheappropriateAPIcallstosearchYouTubeandhandletheresponseseffectively.
youareembeddedwithinasociallyassistiverobottoprovideReminiscence/RehabilitationInteractiveTherapyto
Prompt3 olderadultswithdementia.Youwillalsobeprovidedwithimagesofthescene,ifyoudetecttheusertohave
negativeemotions,startreminiscencetherapybasedonthisinformation[patientinformation]
Ourgoalistointegratenon-verbalcommunicationintothetext-basedscriptthatthesociallyassistiverobotwill
usetorespondtoolderadultswithdementia.Therobot’sscriptshouldincludeatomicactionstoperformspecific
gestures,bodymovementsandfacialexpressions,improvingitsinteractionsandprovidingamorecomforting
presence.Thesearetheatomicactions:“
Yes:nodsheaddownwards;Explain:movesbothhandsinfrontofrobotandthenapartfromeachother;
Prompt4 Confident:robottiltshipbackwardsandstandswithawidestance;”
youneedtotakethis<script>presentationandmatch/re-writeittoincludetheappropriategesturesandbody
movementsembeddedwithinthetext.Hereisanexample:
r“ˆstart(animations/Stand/Gestures/Explain)Welcometoafascinatingjourneyintotherealmofrobotic
learning!”
r”Justlikehumans,robotscanlearnandevolve.ˆstop(animations/Stand/Gestures/Confident)Robotics 2024, 13, x FOR PEER REVIEW 34 of 45
Robotics2024,13,112 29of43
You are tasked with two functions: (1) Documenting the surgical procedure as a list surrounded by
8.2. Design2: SemanticReasoningandPlanning
*(documentation)*. Here is an example: “*(1. Start of procedure, 2. Beginning on the rise of nose, …)*”,
and (2) if there is faiSlucreen ianr ihoa:nAdosvuergr iocfa slurorgbioctanl teoeodlss ttoo rae scuogrgneiozen,, gidraebn,tiafny dthhea insdsuoevs eirn stpheec tiofiocl shuarngdicoa-l
toolsduringanoperation. Namely,therobotneedstofirstsemanticallyreasonaboutthe
Prompt 4 ver process based on surgeon feedback, and generate recommendations for PaLM-E to consider in gen-
currentstateofthesurgicaloperationtoidentifythetoolrequiredbythesurgeon,before
erating the new plan. The suggestion to PaLM-E should be surrounded by &(suggestion)&, and should
generatingaplantolocalize,retrieve,andhandoverthetooltothesurgeon. Below,we
include the step(s) of the plan which have resulted in errors and the associated target object(s) and loca-
outlinethedesignofapotentialframeworkforasurgicalrobotthatcouldleverageLLMs
tion(s) based on the corresponding images of the OR and feedback from a surgeon
forbothsemanticreasoningandtaskplanning,Figure2.
FFiigguurree 22.. L LL LM M- -e em mb be ed dd de ed
d
f fr ra am me ew wo orr kk aa rr cc hh ii tt ee cc tt uu rr ee ff oo rr ss uu rr gg ii cc aa ll rr oo bb oo tt ii nn dd ee ss ii gg nn 22 ..
AuthoTrh Ceoinmtrpibleumtioennst:a tCioonncoepftsueamlizaantitoicn,r eSa.Ps.o annidn gGa.Nn.d; mpelathnondionlgogfyo,r Ss.Pu.r ganicda lGr.Nob.;o vtasliidsacthioanl,-
lSe.Pn.g ainndg ,Ga.sNs.e; mfoarmntailc arneaalsyosnisi,n Sg.Pa. laonnde Gre.Nqu.; irinevse(s1t)igaatkionno, wS.lPe.d agneds oGu.Nrc.;e rsepsoeucrificecs,t oGs.Nu.r;g dicaatal
pcurroacteiodnu, rSe.sP,. wanhdi cGh.Nco.;n wtariintisngc—lasosr-igleinvaell dkrnaoftw plreedpgareastiuocnh, Sa.sP.‘ baonnde Gsa.Nw.s; awrerittiynpgi—carlleyviueswed afnodr
ceudtittiinngg, bSo.Pn.e sa’nbdu Gt.aNls.;o viinsustaalnizcaeti-olenv, eSl.Pk.n aonwd lGed.Ng.e; ssuupcehrvaissitohne, iGn.tNen.;d perdojeucste aodfmainstisetrrnaatilosna, wG.fNo.r;
funding acquisition, G.N. All authors have read and agreed to the published version of the manu-
cuttingthroughthesternum,and(2)acomputationalframeworksuchasanLLMwhichcan
script.
processandcomparethetoolscurrentlyavailableagainstworldrepresentationsacquired
tFhurnoduinggh: vAeGrbEa-WlfEeLeLd bInacc.k, Cfraonmaditahne Fsruarigltyeo Nneotwrobrakg (-CoFf-Nw),o Crdasnandeau Rraelsenaertcwh oCrhkaimrso (dCeRlCs)s purcoh-
agrsaOmp, NenaStucreanl eSc[i1e3n4c]eso arnDd eEtnicgi[n1e6e3r]i.ngH Roewseeavrcehr, Cthouenicmil polfe Cmaennadtaat i(oNnSEaRnCd),d aensdig NnSoEfReCa HcheRoof
tChReEaAfTorEe pmroegnrtaiomn.e drequirementsspecificallyforsurgicaloperationsislabor-intensiveand
non-trivialandrequirescross-domainexpertise. LLMscanaddressthesechallenges,as
theyprovideanall-in-onesolution. Inparticular,LLMshaveaknowledgesourcewhichRobotics2024,13,112 30of43
isintheirembeddingspacethatcanbetrainedspecificallyformedicaluse,suchasMed-
PaLM2[42]presentedinSection2. Furthermore,theimplementationofanLLMintoa
surgical robot for semantic reasoning is beneficial, as an LLM also includes a computa-
tionalframework,asprovidedbythetransformerarchitecture,therebystreamliningthe
implementation. Lastly,theworldrepresentationscanbeprovidedthroughpromptswhich
depictordescribethesurgicaloperationineitherimagesortext. Therefore,anLLMhas
thepotentialtobeincorporatedintosurgicalrobotsthatcanbeusedtointelligentlyassist
surgeonsduringsurgicaloperations.
Moreover, we consider the use of an LLM for surgical robot task planning, as it
provides the following advantages: (1) generated surgical plans are human-readable,
(2)surgicalstaffcaninputchangestotheplanviaverbalcommandstothesurgicalrobot,
withoutmuchaddedcomplexity,(3)theLLMforsurgicalrobottaskplanninginteracts
directlywithsemanticreasoningmodulesvianaturallanguageprompts,furtherreduc-
ing complexity, and (4) unlike deep learning models such as PPCNet [145] or heuristic
modelssuchthegeneticalgorithms[142], whichrequireretrainingorredesignfornew
environments since they often fail to generalize, LLMs such as PaLM-E [274] are not
environment-specific,allowingforbetteradaptationtounencounteredsettings.
WeconsidertheuseofMed-PaLM2andPaLM-E,whicharemulti-modalLLMs,to
provideasurgicalrobotwiththefollowingcapabilities:
Automateddocumentationandreporting: Med-PaLM2canautomaticallydocument
eachstepofthesurgicalprocess,generatingdetailedreportsthatincludethecurrentstate
ofthesurgicalprocedure,toolusage,andanyencounteredmedicalcomplications. This
is achieved by prompting Med-PaLM 2 with (1) a role, (2) RGB images of the surgical
operationprovidedbycamerasintheOR,(3)backgroundinformationaboutthesurgical
operation taking place, and (4) the transcription of verbal communication taking place
withintheOR.Med-PaLM2willthengenerateaproceduralreportofthesurgicaloper-
ationdocumentingtheactionsofthesurgicalteamintheorderthattheyoccurred. This
functionalityaidsinpost-operativereviewandqualitycontrol,providingabreakdown
ofthesurgeryforrecordkeepingandensuringthatallsurgicalactionsaretraceableand
transparent. Thiscanfacilitateeasierfollow-upandassessmentbymedicalprofessionals,
withouttheneedforoverburdeningsurgicalstaffwithdocumentationwritingtasks.
Human-understandableplangeneration: PaLM-Eisspecificallydesignedtobeused
forlonghorizonplanninginembodiedrobotsandsupportsmulti-modalpromptswhich
canincludetext,images,andstateestimations[274]. Moreover,PaLM-Ecangenerateplans
inhuman-understandableformatssuchasbehaviortrees[275]andcantakeintoaccount
feedbackfromsurgeonswhengeneratingplans. Thisfunctionalityiscrucialforsurgical
robotsfortworeasons: (1)itprovidestherobotwitharecoverymethodincaseswherethe
robotfailstolocalizethesurgicaltool,and(2)therecoverymethodutilizesverbalfeedback
fromtheORstaff,providinganintuitivewayforhumanstoguidetherobot. Thisdirect
communicationminimizesdelaysbyenablingtherobottoimmediatelynavigatetothe
correctlocationfortoolretrieval. Itavoidstheinefficienciesoftrial-and-errorsearchesor
relianceonexternaldatabasestosearchforthetool,whichnotonlyslowdowntheprocess
butalsodonotupdatethedatabaseinrealtimeiftoolshavebeenmisplaced,potentially
leadingtoafailedrobotstate.
Toolselectionandhandover: AsurgicalrobotcanleverageMed-PaLM2tointerpret
thecurrentstateofasurgicalprocedureinrealtimebasedonverbalcommandsfromsur-
geonsandRGBimagesprovidedbycamerasintheOR.Moreover,Med-PaLM2augmented
withaRAGmoduletoprovidesurgery-specificinformationcaninferwhichsurgicaltool
isneededbasedonapromptcontainingthecurrentstepofthesurgeryfrom(1)themost
recentactiondescribedinthesurgicalreport,(2)RGBimagesoftheoperation,and(3)the
transcriptionofverbalcommunicationbetweensurgicalstaffintheOR.Oncethesurgical
toolisidentifiedbyMED-PaLM2,theresultcanbeusedasaninputtoPaLM-Etodynami-
callygeneratearobotplanintheformofabehaviortreetolocalize,grasp,andhandover
thesurgicaltooltoasurgeon. Furthermore,byautomatingboththeprocessofpromptingRobotics2024,13,112 31of43
Med-PaLM 2 and PaLM-E, the surgical robot can infer which tool a surgeon may need
beforethesurgeonexplicitlyrequests. Theabilityofasurgicalrobottoinferandretrieve
thesurgicaltoolsinatimelymannercanpossiblyreducethecognitiveloadofthesurgical
teamandprovideashort-termsolutiontoproblemssuchasshortagesofORstaff[276].
Frameworkintegration:ThreeinstancesofMed-PaLM2aresetupthroughtheGoogle
CloudAPIplatform[277],eachwithadistinctrole,distinctpromptsasoutlinedinTable4,
and a distinct context. The first instance of Med-PaLM 2 is used to generate a detailed
surgical procedure, providing a step-by-step breakdown of the surgery, including the
necessarysurgicaltools. Thesecondinstanceisusedtoextractkeyinformationfromthe
detailedsurgicalprocedure,forexample,identifyingthesurgicaltoolrequiredforeach
stepofsurgery. Thethirdinstanceisusedtogenerateatextualresponsewhichdocuments
the entirety of the surgical operation and provides feedback to PaLM-E. Furthermore,
oneinstanceofPaLM-Eisutilizedtogeneratesurgicalrobotplanstolocalize,grasp,and
handoversurgicaltools. PaLM-EgeneratesabehaviortreeinXMLformatwhichisthen
utilizedbythesurgicalrobot’slow-levelcontrollertofacilitatethehandoverofsurgical
tools. ThefirsttwoinstancesofMed-PaLM2areusedforsemanticreasoning,whilethe
thirdinstanceofMed-PaLM2andtheonlyinstanceofPaLM-Eareusedforplanningas
showninFigure2.
ThefirstinstanceofMed-PaLM2isusedintheKnowledgemodule(1)todetermine
ambiguitiesintheresultsfromtheQueryDBmodule,avectordatabaseofcommonsurgical
procedures, and(2)togenerateatextualresponsetoaddresstheambiguities. Thefirst
instanceofMed-PaLM2usesPrompt1,whichisasfollows: ‘Arethereanyambiguitiesin
the retrieved data with respect to the [surgical operation]?, your response should address the
ambiguities or any missing tools or procedural steps relevant to the ongoing surgical operation
[surgicaloperation],onlyincludethesurgicalprocedureandtherequiredtools,donotprovide
explanationsforhowtheambiguitiesareidentified’. Thegeneratedresponsefromthisinstance
ofMed-PaLM2isconcatenatedtoPrompt2tobeusedwiththesecondinstanceofMed-
PaLM2. ThesecondinstanceisalsowithintheKnowledgemodule,anditsroleistoidentify
thetargetobjects(surgicaltools)requiredbyasurgeonforeachstepofthesurgeryand
togenerateapromptforthePlanningmodule. Prompt2isasfollows: ‘analyzethecurrent
surgical procedure details. For each step, identify the required surgical tool and its location as
observedintheaccompanyingimages. Generatearesponseformattedasathree-partentryforeach
step,delineatedbycolons. Theformatshouldbe: procedurestepnumber,nameofthesurgicaltool,
andthetool’slocation. Forexample,‘Step1: BoneSaw: ToolCart’.
TheresponsefromthesecondinstanceofMed-PaLM2isconcatenatedtoPrompt3
whichistobeusedwithPaLM-EinthePlannermodule. Prompt3includesRGBimages
fromtheOR,theoutputoftheKnowledgemodule,thedescriptionoftheavailableatomic
actionsthesurgicalrobotcanperform,andanexampleofageneratedplaninXMLformat.
These atomic actions are graspTool(tool) and navigateTo(location). Prompt 3 also instructs
PaLM-EtogeneratetheplanasabehaviortreeinXMLformatwhichisutilizedbythe
surgicalrobot’slow-levelcontrollertofacilitatethehandoverofsurgicaltoolstoasurgeon.
ThethirdinstanceofMed-PaLM2iswithintheEvaluationmodule. TheroleofMed-
PaLM 2 here is to (1) document the surgery for post-operative review and (2) generate
feedbackforthePlannermoduleonchangesthatshouldbemadebasedonasurgeon’s
feedbackinresponsetoafailuretohandoversurgicaltoolstothesurgeon. Prompt4is
usedtoinstructMed-PaLM2with‘youaretaskedwithtwofunctions: (1)Documentingthe
surgicalprocedureasalistsurroundedby*(documentation)*. Hereisanexample: “*(1. Startof
procedure,2. Beginningontheriseofnose,...)*”,and(2)ifthereisfailureinhandoverofsurgical
toolstoasurgeon,identifytheissuesinthetoolhandoverprocessbasedonsurgeonfeedback,and
generaterecommendationsforPaLM-Etoconsideringeneratingthenewplan. Thesuggestionto
PaLM-Eshouldbesurroundedby&(suggestion)&,andshouldincludethestep(s)oftheplanwhich
haveresultedinerrorsandtheassociatedtargetobject(s)andlocation(s)basedonthecorresponding
imagesoftheORandfeedbackfromasurgeon’. TheoutputfromtheEvaluationmoduleisthenRobotics2024,13,112 32of43
passedtothePlannermodule,wherethecyclerepeatsuntilthereissuccessfulhandoverof
thesurgicaltool.
Table4.Promptsfordesign2.
Arethereanyambiguitiesintheretrieveddatawithrespecttothe[surgicaloperation]?,yourresponseshould
addresstheambiguitiesoranymissingtoolsorproceduralstepsrelevanttotheongoingsurgicaloperation
Prompt1.
[surgicaloperation],onlyincludethesurgicalprocedureandtherequiredtools,donotprovideexplanationsfor
howtheambiguitiesareidentified.
Analyzethecurrentsurgicalproceduredetails.Foreachstep,identifytherequiredsurgicaltoolanditslocationas
observedintheaccompanyingimages.Generatearesponseformattedasathree-partentryforeachstep,
Prompt2
delineatedbycolons.Theformatshouldbe:procedurestepnumber,nameofthesurgicaltool,andthetool’s
location.Forexample,‘Step1:BoneSaw:ToolCart.’
yourroleistomanageandfacilitatetheretrievalofsurgicaltoolsthroughgeneratingbehaviortreeswrittenin
XMLformat.Youaredesignedtointerpretsurgeoncommandsandfeedback.
FunctionalCapabilities:
graspTool(tool):Graspsaspecifiedsurgicaltoolnecessaryfortheprocedure.
releaseTool(tool):Releasesthecurrentlyheldtoolbackintothetooltray.
navigateTo(location):Movestherobot’sarmstoaspecifiedlocationwithinthesurgicalfield.
reportFailure():Logsanerrorandsignalsforhumanassistanceifataskcannotbecompleted.
Example:
<BehaviorTree>
<Sequencename=“ToolRetrievalforSurgeryPreparation”>
<Actionfunction=“navigateTo(‘ToolCart’)”/>
<Actionfunction=“graspTool(‘Scalpel’)”onFailure=“reportFailure”/>
<Actionfunction=“navigateTo(‘SurgicalTable’)”onFailure=“reportFailure”/>
<Actionfunction=“releaseTool(‘Scalpel’)”onFailure=“reportFailure”/>
<Actionfunction=“navigateTo(‘ToolCart’)”onFailure=“reportFailure”/>
Prompt3
<Actionfunction=“graspTool(‘Scissors’)”onFailure=“reportFailure”/>
<Actionfunction=“navigateTo(‘SurgicalTable’)”onFailure=“reportFailure”/>
<Actionfunction=“releaseTool(‘Scissors’)”onFailure=“reportFailure”/>
<Actionfunction=“navigateTo(‘ToolCart’)”onFailure=“reportFailure”/>
<Actionfunction=“graspTool(‘SutureKit’)”onFailure=“reportFailure”/>
<Actionfunction=“navigateTo(‘SurgicalTable’)”onFailure=“reportFailure”/>
<Actionfunction=“releaseTool(‘SutureKit’)”onFailure=“reportFailure”/>
</Sequence>
</BehaviorTree>
<SubTree>
<Actionname=“reportFailure”>
<Logmessage=“STUCK:Assistancerequired.”/>
<Signalfunction=“requestHelp”/>
</Action>
</SubTree>
Youaretaskedwithtwofunctions:(1)Documentingthesurgicalprocedureasalistsurroundedby
*(documentation)*.Hereisanexample:“*(1.Startofprocedure,2.Beginningontheriseofnose,...)*”,and(2)if
thereisfailureinhandoverofsurgicaltoolstoasurgeon,identifytheissuesinthetoolhandoverprocessbasedon
Prompt4 surgeonfeedback,andgeneraterecommendationsforPaLM-Etoconsideringeneratingthenewplan.The
suggestiontoPaLM-Eshouldbesurroundedby&(suggestion)&,andshouldincludethestep(s)oftheplanwhich
haveresultedinerrorsandtheassociatedtargetobject(s)andlocation(s)basedonthecorrespondingimagesof
theORandfeedbackfromasurgeon
AuthorContributions:Conceptualization,S.P.andG.N.;methodology,S.P.andG.N.;validation,S.P.
andG.N.;formalanalysis,S.P.andG.N.;investigation,S.P.andG.N.;resources,G.N.;datacuration,
S.P.andG.N.;writing—originaldraftpreparation,S.P.andG.N.;writing—reviewandediting,S.P.
and G.N.; visualization, S.P. and G.N.; supervision, G.N.; project administration, G.N.; funding
acquisition,G.N.Allauthorshavereadandagreedtothepublishedversionofthemanuscript.
Funding:AGE-WELLInc.,CanadianFrailtyNetwork(CFN),CanadaResearchChairs(CRC)pro-
gram,NaturalSciencesandEngineeringResearchCouncilofCanada(NSERC),andNSERCHeRo
CREATEprogram.
DataAvailabilityStatement:Nonewdatawerecreatedoranalyzedinthisstudy.Datasharingis
notapplicabletothisarticle.Robotics2024,13,112 33of43
Acknowledgments:TheauthorswouldliketothankandacknowledgetheassistanceofClaraNaini
inhelpingtofindandorganizesomeofthescholarlypapersonLLMsforrobotsusedinthesections
ofthemanuscript.
ConflictsofInterest: Theauthorsdeclarenoconflictsofinterest. Thefundershadnoroleinthe
design of this study; in the collection, analyses, or interpretation of data; in the writing of this
manuscript;orinthedecisiontopublishtheresults.
References
1. WorldHealthOrganization.AgeingandHealth.Availableonline:https://www.who.int/news-room/fact-sheets/detail/ageing-
and-health(accessedon3January2024).
2. Hornstein,J.ChronicDiseasesinAmerica|CDC.Availableonline:https://www.cdc.gov/chronicdisease/resources/infographic/
chronic-diseases.htm(accessedon19January2024).
3. Hacker,K.A.COVID-19andChronicDisease:TheImpactNowandintheFuture.Prev.Chronic.Dis.2021,18,E62.[CrossRef]
[PubMed]
4. Express Entry Targeted Occupations: How Many Healthcare Workers Does Canada Need?|CIC News. Available online:
https://www.cicnews.com/2023/10/express-entry-targeted-occupations-how-many-healthcare-workers-does-canada-need-
1040056.html(accessedon19January2024).
5. FactSheet:StrengtheningtheHealthCareWorkforce|AHA.Availableonline:https://www.aha.org/fact-sheets/2021-05-26-fact-
sheet-strengthening-health-care-workforce(accessedon25June2024).
6. TulaneUniversity.BigDatainHealthCareandPatientOutcomes.Availableonline:https://publichealth.tulane.edu/blog/big-
data-in-healthcare/(accessedon19January2024).
7. Gibson, K. The Impact of Health Informatics on Patient Outcomes. Available online: https://graduate.northeastern.edu/
resources/impact-of-healthcare-informatics-on-patient-outcomes/(accessedon19January2024).
8. Northeastern University Graduate Programs. Using Data Analytics to Predict Outcomes in Healthcare. Available online:
https://journal.ahima.org/page/using-data-analytics-to-predict-outcomes-in-healthcare(accessedon19January2024).
9. Yu, P.; Xu, H.; Hu, X.; Deng, C. Leveraging Generative AI and Large Language Models: A Comprehensive Roadmap for
HealthcareIntegration.Healthcare2023,11,2776.[CrossRef][PubMed]
10. Shen,D.;Wu,G.;Suk,H.-I.DeepLearninginMedicalImageAnalysis.Annu.Rev.Biomed.Eng.2017,19,221–248.[CrossRef]
[PubMed]
11. Park,D.J.;Park,M.W.;Lee,H.;Kim,Y.-J.;Kim,Y.;Park,Y.H.DevelopmentofMachineLearningModelforDiagnosticDisease
PredictionBasedonLaboratoryTests.Sci.Rep.2021,11,7567.[CrossRef][PubMed]
12. Webster,P.SixWaysLargeLanguageModelsAreChangingHealthcare.Nat.Med.2023,29,2969–2971.[CrossRef][PubMed]
13. Benary,M.; Wang,X.D.; Schmidt,M.; Soll,D.; Hilfenhaus,G.; Nassir,M.; Sigler,C.; Knödler,M.; Keller,U.; Beule,D.; etal.
Leveraging Large Language Models for Decision Support in Personalized Oncology. JAMA Netw. Open 2023, 6, e2343689.
[CrossRef][PubMed]
14. UCDavisHealthMinimallyInvasiveandRoboticSurgery|ComprehensiveSurgicalServices|UCDavisHealth.Availableonline:
https://health.ucdavis.edu/surgicalservices/minimally_invasive_surgery.html(accessedon25January2024).
15. RoboticSurgery:Robot-AssistedSurgery,Advantages,Disadvantages.Availableonline:https://my.clevelandclinic.org/health/
treatments/22178-robotic-surgery(accessedon19January2024).
16. Sivakanthan,S.;Candiotti,J.L.;Sundaram,A.S.;Duvall,J.A.;Sergeant,J.J.G.;Cooper,R.;Satpute,S.;Turner,R.L.;Cooper,R.A.
Mini-Review:RoboticWheelchairTaxonomyandReadiness.Neurosci.Lett.2022,772,136482.[CrossRef][PubMed]
17. Fanciullacci,C.;McKinney,Z.;Monaco,V.;Milandri,G.;Davalli,A.;Sacchetti,R.;Laffranchi,M.;DeMichieli,L.;Baldoni,A.;
Mazzoni,A.;etal.SurveyofTransfemoralAmputeeExperienceandPrioritiesfortheUser-CenteredDesignofPoweredRobotic
TransfemoralProstheses.J.Neuroeng.Rehabil.2021,18,168.[CrossRef][PubMed]
18. MIT-ManusRobotAidsPhysicalTherapyofStrokeVictims.Availableonline:https://news.mit.edu/2000/manus-0607(accessed
on20January2024).
19. Maciejasz,P.;Eschweiler,J.;Gerlach-Hahn,K.;Jansen-Troy,A.;Leonhardt,S.ASurveyonRoboticDevicesforUpperLimb
Rehabilitation.J.Neuroeng.Rehabil.2014,11,3.[CrossRef][PubMed]
20. Teng,R.;Ding,Y.;See,K.C.UseofRobotsinCriticalCare:SystematicReview.J.Med.InternetRes.2022,24,e33380.[CrossRef]
[PubMed]
21. Abdullahi,U.;Muhammad,B.;Masari,A.;Bugaje,A.ARemote-OperatedHumanoidRobotBasedPatientMonitoringSystem.
IREJ.2023,7,17–22.
22. Gonzalez,C.ServiceRobotsUsedforMedicalCareandDeliveries—ASME.Availableonline:https://www.asme.org/topics-
resources/content/are-service-bots-the-new-future-post-covid-19(accessedon3January2024).
23. Sarker,S.;Jamal,L.;Ahmed,S.F.;Irtisam,N.RoboticsandArtificialIntelligenceinHealthcareduringCOVID-19Pandemic:A
SystematicReview.Robot.Auton.Syst.2021,146,103902.[CrossRef][PubMed]
24. HowRobotsBecameEssentialWorkersintheCOVID-19Response—IEEESpectrum.Availableonline:https://spectrum.ieee.
org/how-robots-became-essential-workers-in-the-covid19-response(accessedon20January2024).Robotics2024,13,112 34of43
25. TheCleverUseofRobotsduringCOVID-19—EHLInsights|Business.Availableonline:https://hospitalityinsights.ehl.edu/
robots-during-covid-19(accessedon20January2024).
26. Getson,C.;Nejat,G.TheAdoptionofSociallyAssistiveRobotsforLong-TermCare:DuringCOVID-19andinaPost-Pandemic
Society.Healthc.Manag.Forum2022,35,301–309.[CrossRef][PubMed]
27. Henschel,A.;Laban,G.;Cross,E.S.WhatMakesaRobotSocial?AReviewofSocialRobotsfromScienceFictiontoaHomeor
HospitalNearYou.Curr.Robot.Rep.2021,2,9–19.[CrossRef][PubMed]
28. Kim,J.;Kim,S.;Kim,S.;Lee,E.;Heo,Y.;Hwang,C.-Y.;Choi,Y.-Y.;Kong,H.-J.;Ryu,H.;Lee,H.CompanionRobotsforOlder
Adults:Rodgers’EvolutionaryConceptAnalysisApproach.Intell.Serv.Robot.2021,14,729–739.[CrossRef][PubMed]
29. Denecke,K.;Baudoin,C.R.AReviewofArtificialIntelligenceandRoboticsinTransformedHealthEcosystems.Front.Med.2022,
9,795957.[CrossRef]
30. Sevilla-Salcedo, J.; Fernádez-Rodicio, E.; Martín-Galván, L.; Castro-González, Á.; Castillo, J.C.; Salichs, M.A. Using Large
LanguageModelstoShapeSocialRobots’Speech.Int.J.Interact.Multimed.Artif.Intell.2023,8,6.[CrossRef]
31. Addlesee,A.;Siein´ska,W.;Gunson,N.;Garcia,D.H.;Dondrup,C.;Lemon,O.Multi-PartyGoalTrackingwithLLMs:Comparing
Pre-Training,Fine-Tuning,andPromptEngineering2023. InProceedingsofthe24thAnnualMeetingoftheSpecialInterest
GrouponDiscourseandDialogue,Prague,Czechia,11–15September2023.
32. Pandya,A.ChatGPT-EnableddaVinciSurgicalRobotPrototype:AdvancementsandLimitations.Robotics2023,12,97.[CrossRef]
33. Elgedawy, R.; Srinivasan, S.; Danciu, I. Dynamic Q&A of Clinical Documents with Large Language Models. arXiv 2024,
arXiv:2401.10733.
34. Hu,M.;Pan,S.;Li,Y.;Yang,X.AdvancingMedicalImagingwithLanguageModels:Ajourneyfromn-gramstochatgpt.arXiv
2023,arXiv:2304.04920.
35. AComprehensiveOverviewofLargeLanguageModels.Availableonline:https://ar5iv.labs.arxiv.org/html/2307.06435(accessed
on8March2024).
36. Lin,T.;Wang,Y.;Liu,X.;Qiu,X.ASurveyofTransformers.AIOpen2022,3,111–132.[CrossRef]
37. Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder. Available online: https:
//ar5iv.labs.arxiv.org/html/2304.04052(accessedon8March2024).
38. King,J.;Baffour,P.;Crossley,S.;Holbrook,R.;Demkin,M.LLM—DetectAIGeneratedText.Availableonline:https://kaggle.
com/competitions/llm-detect-ai-generated-text(accessedon8March2024).
39. Vaswani,A.;Shazeer,N.;Parmar,N.;Uszkoreit,J.;Jones,L.;Gomez,A.N.;Kaiser,Ł.;Polosukhin,I.Attentionisallyouneed.
InProceedingsofthe31stInternationalConferenceonNeuralInformationProcessingSystems,LongBeach,CA,USA,4–9
December2017;2017;pp.6000–6010.
40. Burns,K.;Jain,A.;Go,K.;Xia,F.;Stark,M.;Schaal,S.;Hausman,K.GeneratingRobotPolicyCodeforHigh-Precisionand
Contact-RichManipulationTasks.arXiv2023,arXiv:2404.06645.
41. Gu,Y.;Tinn,R.;Cheng,H.;Lucas,M.;Usuyama,N.;Liu,X.;Naumann,T.;Gao,J.;Poon,H.Domain-SpecificLanguageModel
PretrainingforBiomedicalNaturalLanguageProcessing.Availableonline:https://arxiv.org/abs/2007.15779v6(accessedon
8March2024).
42. Gupta,A.;Waldron,A.SharingGoogle’sMed-PaLM2MedicalLargeLanguageModel,orLLM.Availableonline:https://cloud.
google.com/blog/topics/healthcare-life-sciences/sharing-google-med-palm-2-medical-large-language-model(accessedon
20January2024).
43. Wang,H.;Gao,C.;Dantona,C.;Hull,B.;Sun,J.DRG-LLaMA:TuningLLaMAModeltoPredictDiagnosis-RelatedGroupfor
HospitalizedPatients.NpjDigit.Med.2024,7,1–9.[CrossRef][PubMed]
44. Schneider, E.T.R.; deSouza, J.V.A.; Gumiel, Y.B.; Moro, C.; Paraiso, E.C.AGPT-2LanguageModelforBiomedicalTextsin
Portuguese.InProceedingsofthe2021IEEE34thInternationalSymposiumonComputer-BasedMedicalSystems(CBMS),Aveiro,
Portugal,7–9June2021;pp.474–479.
45. Lehman,E.;Johnson,A.Clinical-T5:LargeLanguageModelsBuiltUsingMIMICClinicalText.PhysioNet2023.[CrossRef]
46. Brown,T.B.;Mann,B.;Ryder,N.;Subbiah,M.;Kaplan,J.;Dhariwal,P.;Neelakantan,A.;Shyam,P.;Sastry,G.;Askell,A.;etal.
LanguageModelsAreFew-ShotLearners.InProceedingsofthe34thInternationalConferenceonNeuralInformationProcessing
Systems,Vancouver,BC,Canada,6–12December2020;p.159.
47. OpenAI;Achiam,J.;Adler,S.;Agarwal,S.;Ahmad,L.;Akkaya,I.;Aleman,F.L.;Almeida,D.;Altenschmidt,J.;Altman,S.;etal.
GPT-4TechnicalReport.arXiv2023,arXiv:2303.08774.
48. Raffel,C.;Shazeer,N.;Roberts,A.;Lee,K.;Narang,S.;Matena,M.;Zhou,Y.;Li,W.;Liu,P.J.ExploringtheLimitsofTransfer
LearningwithaUnifiedText-to-TextTransformer.J.Mach.Learn.Res.2020,21,1–67.
49. Beltagy,I.;Peters,M.E.;Cohan,A.Longformer:TheLong-DocumentTransformer.arXiv2020,arXiv:2004.05150.
50. Chowdhery,A.;Narang,S.;Devlin,J.;Bosma,M.;Mishra,G.;Roberts,A.;Barham,P.;Chung,H.W.;Sutton,C.;Gehrmann,S.;
etal.PaLM:ScalingLanguageModelingwithPathways.J.Mach.Learn.Res.2023,24,1–113.
51. Taori,R.;Gulrajani,I.;Zhang,T.;Dubois,Y.;Li,X.;Guestrin,C.;Liang,P.;Hashimoto,T.Alpaca:AStrong,ReplicableInstruction-
Following Mode; Stanford Center for Research on Foundation Models: Stanford, CA, USA, 2023. Available online: https:
//crfm.stanford.edu/2023/03/13/alpaca.html(accessedon8March2024).Robotics2024,13,112 35of43
52. Devlin,J.;Chang,M.-W.;Lee,K.;Toutanova,K.BERT:Pre-TrainingofDeepBidirectionalTransformersforLanguageUnderstand-
ing.InProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:
HumanLanguageTechnologies,Minneapolis,MN,USA,2–7June2019;pp.4171–4186.
53. Anil,R.;Dai,A.M.;Firat,O.;Johnson,M.;Lepikhin,D.;Passos,A.;Shakeri,S.;Taropa,E.;Bailey,P.;Chen,Z.;etal. PaLM2
TechnicalReport.arXiv2023,arXiv:2305.10403.
54. Touvron,H.;Martin,L.;Stone,K.R.;Albert,P.;Almahairi,A.;Babaei,Y.;Bashlykov,N.;Batra,S.;Bhargava,P.;Bhosale,S.;etal.
Llama2:OpenFoundationandFine-TunedChatModels.arXiv2020,arXiv:2307.09288.
55. Radford,A.; Wu,J.; Child,R.; Luan,D.; Amodei,D.; Sutskever,I.LanguageModelsAreUnsupervisedMultitaskLearners.
Availableonline:https://openai.com/index/better-language-models/(accessedon26June2024).
56. Jin,D.;Pan,E.;Oufattole,N.;Weng,W.-H.;Fang,H.;Szolovits,P.WhatDiseaseDoesThisPatientHave?ALarge-ScaleOpen
DomainQuestionAnsweringDatasetfromMedicalExams.Appl.Sci.2020,11,6421.[CrossRef]
57. Pal,A.;Umapathi,L.K.;Sankarasubbu,M.MedMCQA:ALarge-ScaleMulti-SubjectMulti-ChoiceDatasetforMedicalDomain
QuestionAnswering.InProceedingsoftheMachineLearningResearch(PMLR),ACMConferenceonHealth,Inference,and
Learning(CHIL),Virtual,7April2022;Volume174,pp.248–260.
58. Jin, Q.; Dhingra, B.; Liu, Z.; Cohen, W.W.; Lu, X.PubMedQA:ADatasetforBiomedicalResearchQuestionAnswering. In
Proceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJoint
ConferenceonNaturalLanguageProcessing,HongKong,China,3–7November2019;pp.2567–2577.
59. Tozzi,C.;Zittrain,J.Introduction. InForFunandProfit: AHistoryoftheFreeandOpenSourceSoftwareRevolution;MITPress:
Cambridge,MA,USA,2017.Availableonline:https://ieeexplore.ieee.org/document/8047084(accessedon29May2024).
60. Spirling,A.WhyOpen-SourceGenerativeAIModelsAreanEthicalWayForwardforScience.Nature2023,616,413.[CrossRef]
[PubMed]
61. Bommasani,R.;Hudson,D.A.;Adeli,E.;Altman,R.;Arora,S.;vonArx,S.;Bernstein,M.S.;Bohg,J.;Bosselut,A.;Brunskill,E.;
etal.OntheOpportunitiesandRisksofFoundationModels.arXiv2022,arXiv:2108.07258.
62. Self-InfluenceGuidedDataReweightingforLanguageModelPre-Training.Availableonline:https://ar5iv.labs.arxiv.org/html/
2311.00913(accessedon8March2024).
63. Solaiman, I.; Dennison, C. Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets. In
Proceedingsofthe35thInternationalConferenceonNeuralInformationProcessingSystems,Sydney,Australia,6–14December
2021;p.448.
64. PromptDesignandEngineering:IntroductionandAdvancedMethods.Availableonline:https://ar5iv.labs.arxiv.org/html/2401
.14423(accessedon8March2024).
65. Ratner,N.;Levine,Y.;Belinkov,Y.;Ram,O.;Magar,I.;Abend,O.;Karpas,E.;Shashua,A.;Leyton-Brown,K.;Shoham,Y.Parallel
ContextWindowsforLargeLanguageModels.InProceedingsofthe61stAnnualMeetingoftheAssociationforComputational
Linguistics,Toronto,ON,Canada,9–14July2023;pp.6383–6402.
66. Chen,H.;Pasunuru,R.;Weston,J.;Celikyilmaz,A.WalkingDowntheMemoryMaze:BeyondContextLimitthroughInteractive
Reading.arXiv2023,arXiv:2310.05029.
67. Fairness-GuidedFew-ShotPromptingforLargeLanguageModels.Availableonline:https://ar5iv.labs.arxiv.org/html/2303.132
17(accessedon8March2024).
68. Skill-BasedFew-ShotSelectionforIn-ContextLearning.Availableonline:https://ar5iv.labs.arxiv.org/html/2305.14210(accessed
on8March2024).
69. ExtendingContextWindowofLargeLanguageModelsviaPositionInterpolation.Availableonline:https://ar5iv.labs.arxiv.org/
html/2306.15595(accessedon8March2024).
70. ParallelContextWindowsImproveIn-ContextLearningofLargeLanguageModels.Availableonline:https://ar5iv.labs.arxiv.
org/html/2212.10947(accessedon8March2024).
71. MM-LLMs:RecentAdvancesinMultiModalLargeLanguageModels.Availableonline:https://ar5iv.labs.arxiv.org/html/2401.1
3601(accessedon8March2024).
72. Radford,A.;Kim,J.W.;Hallacy,C.;Ramesh,A.;Goh,G.;Agarwal,S.;Sastry,G.;Askell,A.;Mishkin,P.;Clark,J.;etal.Learning
TransferableVisualModelsFromNaturalLanguageSupervision.InProceedingsofthe38thInternationalConferenceonMachine
Learning,ICML2021,VirtualEvent,18–24July2021;Volume139,pp.8748–8763.
73. Chen,D.;Chang,A.;Nießner,M.ScanRefer:3DObjectLocalizationinRGB-DScansUsingNaturalLanguage.InProceedingsof
theComputerVision—ECCV2020,Glasgow,UK,12November2020;pp.202–221.
74. Liu,J.X.;Yang,Z.;Idrees,I.;Liang,S.;Schornstein,B.;Tellex,S.;Shah,A.GroundingComplexNaturalLanguageCommands
forTemporalTasksinUnseenEnvironments.InProceedingsofthe7thConferenceonRobotLearning,Atlanta,GA,USA,6–9
November2023.
75. Liu,M.;Shen,Y.;Yao,B.M.;Wang,S.;Qi,J.;Xu,Z.;Huang,L.KnowledgeBot:ImprovingAssistiveRobotforTaskCompletionand
LiveInteractionviaNeuro-SymbolicReasoning.InProceedingsoftheAlexaPrizeSimBotChallenge,VirtualEvent,6April2023.
76. Salichs, M.A.; Castro-González, Á.; Salichs, E.; Fernández-Rodicio, E.; Maroto-Gómez, M.; Gamboa-Montero, J.J.;
Marques-Villarroya,S.; Castillo, J.C.; Alonso-Martín, F.; Malfaz, M. Mini: A New Social Robot for the Elderly. Int. J.
Soc.Robot.2020,12,1231–1249.[CrossRef]Robotics2024,13,112 36of43
77. Zhao,X.;Li,M.;Weber,C.;Hafez,M.B.;Wermter,S.ChatwiththeEnvironment:InteractiveMultimodalPerceptionUsingLarge
LanguageModels. InProceedingsofthe2023IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems(IROS),
Detroit,MI,USA,1October2023;pp.3590–3596.
78. Singh, I.; Blukis, V.; Mousavian, A.; Goyal, A.; Xu, D.; Tremblay, J.; Fox, D.; Thomason, J.; Garg, A.ProgPrompt: Program
GenerationforSituatedRobotTaskPlanningUsingLargeLanguageModels.Auton.Robots2023,47,999–1012.[CrossRef]
79. Jin,Y.;Li,D.;A,Y.;Shi,J.;Hao,P.;Sun,F.;Zhang,J.;Fang,B.RobotGPT:RobotManipulationLearningfromChatGPT.IEEERobot.
Autom.Lett.2023,9,2543–2550.[CrossRef]
80. Obinata, Y.; Kanazawa, N.; Kawaharazuka, K.; Yanokura, I.; Kim, S.; Okada, K.; Inaba, M.FoundationModelBasedOpen
VocabularyTaskPlanningandExecutiveSystemforGeneralPurposeServiceRobots.arXiv2023,arXiv:2308.03357.
81. Murali,P.;Steenstra,I.;Yun,H.S.;Shamekhi,A.;Bickmore,T.ImprovingMultipartyInteractionswithaRobotUsingLarge
LanguageModels. InProceedingsoftheExtendedAbstractsofthe2023CHIConferenceonHumanFactorsinComputing
Systems,Hamburg,Germany,19April2023;pp.1–8.
82. Paiva,A.;Leite,I.;Boukricha,H.;Wachsmuth,I.EmpathyinVirtualAgentsandRobots:ASurvey.ACMTrans.Interact.Intell.
Syst.2017,7,11.[CrossRef]
83. Cherakara,N.;Varghese,F.;Shabana,S.;Nelson,N.;Karukayil,A.;Kulothungan,R.;Farhan,M.A.;Nesset,B.;Moujahid,M.;
Dinkar,T.;etal.FurChat:AnEmbodiedConversationalAgentUsingLLMs,CombiningOpenandClosed-DomainDialogue
withFacialExpressions.InProceedingsofthe24thAnnualMeetingoftheSpecialInterestGrouponDiscourseandDialogue,
Prague,Czechia,11–15September2023;pp.588–592.
84. Zhang, B.; Soh, H. Large Language Models as Zero-Shot Human Models for Human-Robot Interaction. arXiv 2023,
arXiv:2303.03548.
85. Yang,J.;Chen,X.;Qian,S.;Madaan,N.;Iyengar,M.;Fouhey,D.F.;Chai,J.LLM-Grounder:Open-Vocabulary3DVisualGrounding
withLargeLanguageModelasanAgent.arXiv2023,arXiv:2309.12311.
86. Yoshida,T.;Masumori,A.;Ikegami,T.FromTexttoMotion: GroundingGPT-4inaHumanoidRobot“Alter3”. arXiv2023,
arXiv:2312.06571.
87. Ahn,M.;Brohan,A.;Brown,N.;Chebotar,Y.;Cortes,O.;David,B.;Finn,C.;Fu,C.;Gopalakrishnan,K.;Hausman,K.;etal.Do
AsICan,NotAsISay:GroundingLanguageinRoboticAffordances.arXiv2022,arXiv:2204.01691.
88. Lykov,A.;Tsetserukou,D.LLM-BRAIn:AI-DrivenFastGenerationofRobotBehaviourTreeBasedonLargeLanguageModel.
arXiv2023,arXiv:2305.19352.
89. Kubota,A.;Cruz-Sandoval,D.;Kim,S.;Twamley,E.W.;Riek,L.D.CognitivelyAssistiveRobotsatHome:HRIDesignPatterns
forTranslationalScience. InProceedingsofthe202217thACM/IEEEInternationalConferenceonHuman-RobotInteraction
(HRI),Sapporo,Hokkaido,Japan,7–10March2022;pp.53–62.
90. Elbeleidy,S.;Rosen,D.;Liu,D.;Shick,A.;Williams,T.AnalyzingTeleoperationInterfaceUsageofRobotsinTherapyforChildren
withAutism.InProceedingsoftheACMInteractionDesignandChildrenConference,Athens,Greece,18May2021;pp.112–118.
91. Louie,W.-Y.G.;Nejat,G.ASocialRobotLearningtoFacilitateanAssistiveGroup-BasedActivityfromNon-ExpertCaregivers.
Int.J.Soc.Robot.2020,12,1159–1176.[CrossRef]
92. Mishra,D.;Romero,G.A.;Pande,A.;NachenahalliBhuthegowda,B.;Chaskopoulos,D.;Shrestha,B.AnExplorationofthe
PepperRobot’sCapabilities:UnveilingItsPotential.Appl.Sci.2024,14,110.[CrossRef]
93. Anderson,P.L.;Lathrop,R.A.;Herrell,S.D.;Webster,R.J.ComparingaMechanicalAnalogueWiththeDaVinciUserInterface:
SuturingatChallengingAngles.IEEERobot.Autom.Lett.2016,1,1060–1065.[CrossRef][PubMed]
94. Muradore,R.;Fiorini,P.;Akgun,G.;Barkana,D.E.;Bonfe,M.;Boriero,F.;Caprara,A.;DeRossi,G.;Dodi,R.;Elle,O.J.;etal.
DevelopmentofaCognitiveRoboticSystemforSimpleSurgicalTasks.Int.J.Adv.Robot.Syst.2015,12,37.[CrossRef]
95. Łukasik,S.;Tobis,S.;Suwalska,J.;Łojko,D.;Napierała,M.;Proch,M.;Neumann-Podczaska,A.;Suwalska,A.TheRoleofSocially
AssistiveRobotsintheCareofOlderPeople:ToAssistinCognitiveTraining,toRemindortoAccompany?Sustainability2021,13,
10394.[CrossRef]
96. NaturalLanguageRobotProgramming:NLPIntegratedwithAutonomousRoboticGrasping.Availableonline:https://ar5iv.
labs.arxiv.org/html/2304.02993(accessedon8March2024).
97. Papadopoulos, I.; Koulouglioti, C.; Lazzarino, R.; Ali, S.EnablersandBarrierstotheImplementationofSociallyAssistive
HumanoidRobotsinHealthandSocialCare:ASystematicReview.BMJOpen2020,10,e033096.[CrossRef]
98. Kim,C.Y.;Lee,C.P.;Mutlu,B.UnderstandingLarge-LanguageModel(LLM)-PoweredHuman-RobotInteraction.InProceedings
ofthe2024ACM/IEEEInternationalConferenceonHuman-RobotInteraction(HRI′24),Boulder,CO,USA,11–14March2024;
pp.371–380.
99. EmotionIsAllYouNeed?-BoostingChatGPTPerformancewithEmotionalStimulus-FlowGPT.Availableonline:https://flowgpt.
com/blog/emoGPT(accessedon30January2024).
100. Mishra,C.;Verdonschot,R.;Hagoort,P.;Skantze,G.Real-TimeEmotionGenerationinHuman-RobotDialogueUsingLarge
LanguageModels.Front.Robot.AI2023,10,1271610.[CrossRef][PubMed]
101. Wang,C.;Hasler,S.;Tanneberg,D.;Ocker,F.;Joublin,F.;Ceravola,A.;Deigmoeller,J.;Gienger,M.LaMI:LargeLanguageModels
forMulti-ModalHuman-RobotInteraction.InProceedingsoftheExtendedAbstractsofthe2024CHIConferenceonHuman
FactorsinComputingSystems,Honolulu,HI,USA,11–16May2024;p.218.Robotics2024,13,112 37of43
102. Townsend,D.;MajidiRad,A.TrustinHuman-RobotInteractionWithinHealthcareServices:AReviewStudy.InProceedingsof
theVolume7:46thMechanismsandRoboticsConference(MR),St.Louis,MI,USA,14August2022;p.V007T07A030.
103. Abdi,J.;Al-Hindawi,A.;Ng,T.;Vizcaychipi,M.P.ScopingReviewontheUseofSociallyAssistiveRobotTechnologyinElderly
Care.BMJOpen2018,8,e018815.[CrossRef][PubMed]
104. Abbott,R.;Orr,N.;McGill,P.;Whear,R.;Bethel,A.;Garside,R.;Stein,K.;Thompson-Coon,J.HowDo“Robopets”Impactthe
HealthandWell-beingofResidentsinCareHomes?ASystematicReviewofQualitativeandQuantitativeEvidence.Int.J.Older
PeopleNurs.2019,14,e12239.[CrossRef][PubMed]
105. Xue,L.;Constant,N.;Roberts,A.;Kale,M.;Al-Rfou,R.;Siddhant,A.;Barua,A.;Raffel,C.mT5: AMassivelyMultilingual
Pre-TrainedText-to-TextTransformer.InProceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociation
forComputationalLinguistics:HumanLanguageTechnologies,Online,6–11June2021;pp.483–498.
106. Zhang,J.;Zhao,Y.;Saleh,M.;Liu,P.J.PEGASUS:Pre-TrainingwithExtractedGap-SentencesforAbstractiveSummarization.
arXiv2020.[CrossRef]
107. Cañete,J.;Chaperon,G.;Fuentes,R.;Ho,J.-H.;Kang,H.;Pérez,J.SpanishPre-TrainedBERTModelandEvaluationData.arXiv
2023,arXiv:2308.02976.
108. DaVinciRoboticSurgicalSystems|Intuitive.Availableonline:https://www.intuitive.com/en-us/products-and-services/da-
vinci(accessedon9March2024).
109. ROS:Home.Availableonline:https://www.ros.org/(accessedon21September2023).
110. Palrobot ARI—The Social and Collaborative Robot. Available online: https://pal-robotics.com/robots/ari/ (accessed on
9March2024).
111. TheWorld’sMostAdvancedSocialRobot.Availableonline:https://furhatrobotics.com/(accessedon9March2024).
112. Radford, A.; Kim, J.W.; Xu, T.; Brockman, G.; McLeavey, C.; Sutskever, I. Robust speech recognition via large-scale weak
supervision. InProceedingsofthe40thInternationalConferenceonMachineLearning,Honolulu,HI,USA,23–29July2023;
p.1182.
113. Su,H.;Qi,W.;Chen,J.;Yang,C.;Sandoval,J.;Laribi,M.A.RecentAdvancementsinMultimodalHuman–RobotInteraction.Front.
Neurorobot.2023,17,1084000.[CrossRef][PubMed]
114. Saunderson,S.;Nejat,G.HowRobotsInfluenceHumans: ASurveyofNonverbalCommunicationinSocialHuman–Robot
Interaction.Int.J.Soc.Robot.2019,11,575–608.[CrossRef]
115. Maurtua,I.;Fernández,I.;Tellaeche,A.;Kildal,J.;Susperregi,L.;Ibarguren,A.;Sierra,B.NaturalMultimodalCommunication
forHuman–RobotCollaboration.Int.J.Adv.Robot.Syst.2017,14,1729881417716043.[CrossRef]
116. Schreiter, T.; Morillo-Mendez, L.; Chadalavada, R.T.; Rudenko, A.; Billing, E.; Magnusson, M.; Arras, K.O.; Lilienthal, A.J.
AdvantagesofMultimodalversusVerbal-OnlyRobot-to-HumanCommunicationwithanAnthropomorphicRoboticMockDriver.
InProceedingsofthe202332ndIEEEInternationalConferenceonRobotandHumanInteractiveCommunication(RO-MAN),
Busan,RepublicofKorea,28–31August2023;pp.293–300.
117. RITA–Reminiscence Interactive Therapy and Activities—mPower. Available online: https://mpowerhealth.eu/impact/
reducing-the-digital-divide-connecting-and-empowering/rita-reminiscence-interactive-therapy-and-activities/(accessedon
27May2024).
118. RolesandChallengesofSemanticIntelligenceinHealthcareCognitiveComputing;Carbonaro,A.;Tiwari,S.;Ortiz-Rodriguez,F.;Janev,
V.(Eds.)StudiesontheSemanticWeb/Ssw;IOSPress:Amsterdam,TheNetherlands,2023;ISBN978-1-64368-460-4.
119. NationalLibraryofMedicineTheSemanticNetwork.Availableonline:https://www.nlm.nih.gov/research/umls/new_users/
online_learning/OVR_003.html(accessedon29January2024).
120. Zhang,H.;Hu,H.;Diller,M.;Hogan,W.R.;Prosperi,M.;Guo,Y.;Bian,J.SemanticStandardsofExternalExposomeData.Environ.
Res.2021,197,111185.[CrossRef][PubMed]
121. Aldughayfiq,B.;Ashfaq,F.;Jhanjhi,N.Z.;Humayun,M.CapturingSemanticRelationshipsinElectronicHealthRecordsUsing
KnowledgeGraphs:AnImplementationUsingMIMICIIIDatasetandGraphDB.Healthcare2023,11,1762.[CrossRef][PubMed]
122. Busso,M.;Gonzalez,M.P.;Scartascini,C.OntheDemandforTelemedicine:EvidencefromtheCOVID-19Pandemic.HealthEcon.
2022,31,1491–1505.[CrossRef][PubMed]
123. Chatterjee,A.;Prinz,A.;Riegler,M.A.;Meena,Y.K.AnAutomaticandPersonalizedRecommendationModellinginActivity
eCoachingwithDeepLearningandOntology.Sci.Rep.2023,13,10182.[CrossRef][PubMed]
124. Barisevicˇius,G.;Coste,M.;Geleta,D.;Juric,D.;Khodadadi,M.;Stoilos,G.;Zaihrayeu,I.SupportingDigitalHealthcareServices
UsingSemanticWebTechnologies.InProceedingsofthe17thInternationalSemanticWebConference,Monterey,CA,USA,8–12
October2018;pp.291–306.
125. Yu,W.D.;Jonnalagadda,S.R.SemanticWebandMininginHealthcare.InProceedingsoftheHEALTHCOM20068thInternational
Conferenceone-HealthNetworking,ApplicationsandServices,NewDelhi,India,17–19August2006;pp.198–201.
126. Kara,N.;Dragoi,O.A.ReasoningwithContextualDatainTelehealthApplications.InProceedingsoftheThirdIEEEInternational
ConferenceonWirelessandMobileComputing,NetworkingandCommunications(WiMob2007),WhitePlains,NY,USA,8–10
October2007;p.69.
127. Liu,W.; Daruna,A.; Patel,M.; Ramachandruni,K.; Chernova,S.ASurveyofSemanticReasoningFrameworksforRobotic
Systems.Robot.Auton.Syst.2023,159,104294.[CrossRef]Robotics2024,13,112 38of43
128. Tang,X.;Zheng,Z.;Li,J.;Meng,F.;Zhu,S.-C.;Liang,Y.;Zhang,M.LargeLanguageModelsAreIn-ContextSemanticReasoners
RatherthanSymbolicReasoners.arXiv2023,arXiv:2305.14825.
129. Wen,Y.;Zhang,Y.;Huang,L.;Zhou,C.;Xiao,C.;Zhang,F.;Peng,X.;Zhan,W.;Sui,Z.SemanticModellingofShipBehaviorin
HarborBasedonOntologyandDynamicBayesianNetwork.ISPRSInt.J.Geo-Inf.2019,8,107.[CrossRef]
130. Zheng,W.;Liu,X.;Ni,X.;Yin,L.;Yang,B.ImprovingVisualReasoningThroughSemanticRepresentation.IEEEAccess2021,9,
91476–91486.[CrossRef]
131. Pise, A.A.; Vadapalli, H.; Sanders, I. Relational Reasoning Using Neural Networks: A Survey. Int. J. Uncertain. Fuzziness
Knowl.-BasedSyst.2021,29,237–258.[CrossRef]
132. Li,K.;Hopkins,A.K.;Bau,D.;Viégas,F.;Pfister,H.;Wattenberg,M.EmergentWorldRepresentations: ExploringaSequence
ModelTrainedonaSyntheticTask.arXiv2023,arXiv:2210.13382.
133. Availableonline:https://everydayrobots.com(accessedon23February2024).
134. Peng,S.;Genova,K.;Jiang,C.;Tagliasacchi,A.;Pollefeys,M.;Funkhouser,T.OpenScene:3DSceneUnderstandingwithOpen
Vocabularies.InProceedingsofthe2023IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),Vancouver,
BC,Canada,17–24June2023;pp.815–824.
135. BostonDynamicsSpot|BostonDynamics. Availableonline: https://bostondynamics.com/products/spot/(accessedon24
February2024).
136. Avgousti,S.;Christoforou,E.G.;Panayides,A.S.;Voskarides,S.;Novales,C.;Nouaille,L.;Pattichis,C.S.;Vieyres,P.Medical
TeleroboticSystems:CurrentStatusandFutureTrends.Biomed.Eng.OnLine2016,15,96.[CrossRef][PubMed]
137. Yang,G.;Lv,H.;Zhang,Z.;Yang,L.;Deng,J.;You,S.;Du,J.;Yang,H.KeepHealthcareWorkersSafe:ApplicationofTeleoperated
RobotinIsolationWardforCOVID-19PreventionandControl.Chin.J.Mech.Eng.2020,33,47.[CrossRef]
138. Battaglia,E.;Boehm,J.;Zheng,Y.;Jamieson,A.R.;Gahan,J.;Fey,A.M.RethinkingAutonomousSurgery:FocusingonEnhance-
mentOverAutonomy.Eur.Urol.Focus2021,7,696–705.[CrossRef][PubMed]
139. Leonard,S.;Wu,K.L.;Kim,Y.;Krieger,A.;Kim,P.C.W.SmartTissueAnastomosisRobot(STAR):AVision-GuidedRobotics
SystemforLaparoscopicSuturing.IEEETrans.Biomed.Engineering2014,61,1305–1317.Availableonline:https://ieeexplore.ieee.
org/document/6720152(accessedon27May2024).[CrossRef][PubMed]
140. Takada,C.;Suzuki,T.;Afifi,A.;Nakaguchi,T.HybridTrackingandMatchingAlgorithmforMosaickingMultipleSurgicalViews.
InComputer-AssistedandRoboticEndoscopy; Peters,T.,Yang,G.-Z.,Navab,N.,Mori,K.,Luo,X.,Reichl,T.,McLeod,J.,Eds.;
SpringerInternationalPublishing:Athens,Greece,17October2017;pp.24–35.
141. Afifi,A.;Takada,C.;Yoshimura,Y.;Nakaguchi,T.Real-TimeExpandedField-of-ViewforMinimallyInvasiveSurgeryUsing
Multi-CameraVisualSimultaneousLocalizationandMapping.Sensors2021,21,2106.[CrossRef][PubMed]
142. Lamini,C.;Benhlima,S.;Elbekri,A.GeneticAlgorithmBasedApproachforAutonomousMobileRobotPathPlanning.Procedia
Comput.Sci.2018,127,180–189.[CrossRef]
143. Xiang,D.;Lin,H.;Ouyang,J.;Huang,D.CombinedImprovedA*andGreedyAlgorithmforPathPlanningofMulti-Objective
MobileRobot|ScientificReports.Sci.Rep.2022,12,13273.Availableonline:https://www.nature.com/articles/s41598-022-17684
-0(accessedon10March2024).[CrossRef][PubMed]
144. deSalesGuerraTsuzuki,M.;deCastroMartins,T.;Takase,F.K.RobotPathPlanningUsingSimulatedAnnealing—ScienceDirect.
IFACProc. Vol. 2006,39,175–180. Availableonline: https://www.sciencedirect.com/science/article/pii/S1474667015358250
(accessedon10March2024).[CrossRef]
145. End-to-EndDeepLearning-BasedFrameworkforPathPlanningandCollisionChecking:BinPickingApplication. Available
online:https://ar5iv.labs.arxiv.org/html/2304.00119(accessedon3March2024).
146. Quinones-Ramirez,M.;Rios-Martinez,J.;Uc-Cetina,V.RobotPathPlanningUsingDeepReinforcementLearning.arXiv2023,
arXiv:2302.09120.
147. Nicola,F.;Fujimoto,Y.;Oboe,R.ALSTMNeuralNetworkappliedtoMobileRobotsPathPlanning.InProceedingsoftheIEEE
InternationalConferenceonIndustrialInformatics(INDIN),Porto,Portugal,18–20July2018;pp.349–354.
148. Hjeij,M.;Vilks,A.ABriefHistoryofHeuristics: HowDidResearchonHeuristicsEvolve? Humanit. Soc. Sci. Commun. 2023,
10,64.[CrossRef]
149. Kawaguchi,K.;Kaelbling,L.;Bengio,Y.GeneralizationinDeepLearning.InMathematicalAspectsofDeepLearning;Cambridge
UniversityPress:Cambridge,UK,2022.
150. OntheGeneralizationMysteryinDeepLearning.Availableonline:https://ar5iv.labs.arxiv.org/html/2203.10036(accessedon
10March2024).
151. UnderstandingLLMs:AComprehensiveOverviewfromTrainingtoInference.Availableonline:https://ar5iv.labs.arxiv.org/
html/2401.02038(accessedon3May2024).
152. ADaPT:As-NeededDecompositionandPlanningwithLanguageModels.Availableonline:https://arxiv.org/abs/2311.05772
(accessedon10March2024).
153. Wang,J.;Wu,Z.;Li,Y.;Jiang,H.;Shu,P.;Shi,E.;Hu,H.;Ma,C.;Liu,Y.;Wang,X.;etal.LargeLanguageModelsforRobotics:
Opportunities,Challenges,andPerspectives.Availableonline:https://arxiv.org/abs/2401.04334v1(accessedon3March2024).
154. Tjomsland,J.;Kalkan,S.;Gunes,H.MindYourManners!ADatasetandAContinualLearningApproachforAssessingSocial
AppropriatenessofRobotActions.Front.Robot.AI2022,9,669420.[CrossRef][PubMed]Robotics2024,13,112 39of43
155. Soh,H.;Pan,S.;Min,C.;Hsu,D.TheTransferofHumanTrustinRobotCapabilitiesacrossTasks.InProceedingsoftheRobotics:
ScienceandSystemsXIV;Robotics:ScienceandSystemsFoundation,Pittsburgh,PA,USA,26June2018.
156. Soh,H.;Xie,Y.;Chen,M.;Hsu,D.Multi-TaskTrustTransferforHuman-RobotInteraction.SageJ.2020,39,233–249.[CrossRef]
157. Sap,M.;Rashkin,H.;Chen,D.;LeBras,R.;Choi,Y.SocialIQA:CommonsenseReasoningaboutSocialInteractions.arXiv2019,
arXiv:1904.09728.
158. FRANKARESEARCH3.Availableonline:https://franka.de/(accessedon10March2024).
159. Puig,X.;Ra,K.;Boben,M.;Li,J.;Wang,T.;Fidler,S.;Torralba,A.VirtualHome:SimulatingHouseholdActivitiesViaPrograms.
InProceedingsofthe2018IEEE/CVFConferenceonComputerVisionandPatternRecognition,SaltLakeCity,UT,USA,18–23
June2018;pp.8494–8502.
160. Renfrow,J.NewRobotfromPilloHealth,Black+DeckerOffersin-HomeMonitoring,MedicationDispensing|FierceHealthcare.
Available online: https://www.fiercehealthcare.com/tech/new-robot-offers-home-monitoring-and-medication-dispensing
(accessedon3May2024).
161. Speech-to-TextAI:SpeechRecognitionandTranscription|GoogleCloud.Availableonline:https://cloud.google.com/speech-to-
text(accessedon9March2024).
162. Redmon,J.;Divvala,S.;Girshick,R.;Farhadi,A.YouOnlyLookOnce:Unified,Real-TimeObjectDetection.InProceedingsofthe
2016IEEEConferenceonComputerVisionandPatternRecognition(CVPR),LasVegas,NV,USA,27–30June2016;pp.779–788.
163. Zhou,X.;Girdhar,R.;Joulin,A.;Krähenbühl,P.;Misra,I.DetectingTwenty-ThousandClassesUsingImage-LevelSupervision.In
ProceedingsoftheComputerVision–ECCV2022,TelAviv,Israel,23–27October2022;pp.350–368.
164. Tobeta,M.;Sawada,Y.;Zheng,Z.;Takamuku,S.;Natori,N.E2Pose:FullyConvolutionalNetworksforEnd-to-EndMulti-Person
PoseEstimation.InProceedingsofthe2022IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems(IROS),Kyoto,
Japan,23–27October2022;pp.532–537.
165. Machine,A.AndroidAlter3.Availableonline:http://alternativemachine.co.jp/en/project/alter3/(accessedon4March2024).
166. Shi,H.;Ball,L.;Thattai,G.;Zhang,D.;Hu,L.;Gao,Q.;Shakiah,S.;Gao,X.;Padmakumar,A.;Yang,B.;etal.Alexa,Playwith
Robot:IntroducingtheFirstAlexaPrizeSimBotChallengeonEmbodiedAI.arXiv2023,arXiv:2308.05221.
167. Gao,Q.;Thattai,G.;Shakiah,S.;Gao,X.;Pansare,S.;Sharma,V.;Sukhatme,G.;Shi,H.;Yang,B.;Zheng,D.;etal.AlexaArena:A
User-CentricInteractivePlatformforEmbodiedAI.arXiv2023.[CrossRef]
168. EthicsofCareinTechnology-mediatedHealthcarePractices:AScopingReview-Ramvi-2023-ScandinavianJournalofCaring
Sciences-WileyOnlineLibrary.Availableonline:https://onlinelibrary.wiley.com/doi/full/10.1111/scs.13186(accessedon10
March2024).
169. EthicalImplicationsofAIandRoboticsinHealthcare:AReview-PMC.Availableonline:https://www.ncbi.nlm.nih.gov/pmc/
articles/PMC10727550/(accessedon10March2024).
170. TheValueandImportanceofHealthInformationPrivacy-BeyondtheHIPAAPrivacyRule-NCBIBookshelf.Availableonline:
https://www.ncbi.nlm.nih.gov/books/NBK9579/(accessedon10March2024).
171. Harrer,S.AttentionIsNotAllYouNeed:TheComplicatedCaseofEthicallyUsingLargeLanguageModelsinHealthcareand
Medicine.eBioMedicine2023,90,104512.[CrossRef][PubMed]
172. Singhal,K.;Azizi,S.;Tu,T.;Mahdavi,S.S.;Wei,J.;Chung,H.W.;Scales,N.;Tanwani,A.;Cole-Lewis,H.;Pfohl,S.;etal.Large
LanguageModelsEncodeClinicalKnowledge.Nature2023,620,172–180.[CrossRef][PubMed]
173. Bender,E.M.;Gebru,T.;McMillan-Major,A.;Shmitchell,S.OntheDangersofStochasticParrots:CanLanguageModelsBeToo
Big?InProceedingsofthe2021ACMConferenceonFairness,Accountability,andTransparency,VirtualEvent,3March2021;
pp.610–623.
174. Lareyre,F.;Raffort,J.EthicalConcernsRegardingtheUseofLargeLanguageModelsinHealthcare.EJVESVasc.Forum2023,61,
1.[CrossRef][PubMed]
175. Li,H.;Moon,J.T.;Purkayastha,S.;Celi,L.A.;Trivedi,H.;Gichoya,J.W.EthicsofLargeLanguageModelsinMedicineandMedical
Research.LancetDigit.Health2023,5,e333–e335.[CrossRef][PubMed]
176. Jeyaraman,M.;Balaji,S.;Jeyaraman,N.;Yadav,S.UnravelingtheEthicalEnigma:ArtificialIntelligenceinHealthcare.Cureus
2023,15,e43262.[CrossRef][PubMed]
177. Wang,C.;Liu,S.;Yang,H.;Guo,J.;Wu,Y.;Liu,J.EthicalConsiderationsofUsingChatGPTinHealthCare.J.Med.InternetRes.
2023,25,e48009.[CrossRef][PubMed]
178. Stahl,B.C.;Eke,D.TheEthicsofChatGPT–ExploringtheEthicalIssuesofanEmergingTechnology.Int.J.Inf.Manag.2024,74,
102700.[CrossRef]
179. Oniani, D.; Hilsman, J.; Peng, Y.; Poropatich, R.K.; Pamplin, J.C.; Legault, G.L.; Wang, Y.AdoptingandExpandingEthical
PrinciplesforGenerativeArtificialIntelligencefromMilitarytoHealthcare.NpjDigit.Med.2023,6,225.[CrossRef][PubMed]
180. Clusmann,J.;Kolbinger,F.R.;Muti,H.S.;Carrero,Z.I.;Eckardt,J.-N.;Laleh,N.G.;Löffler,C.M.L.;Schwarzkopf,S.-C.;Unger,M.;
Veldhuizen,G.P.;etal.TheFutureLandscapeofLargeLanguageModelsinMedicine.Commun.Med.2023,3,141.[CrossRef]
[PubMed]
181. Murphy, K.; Di Ruggiero, E.; Upshur, R.; Willison, D.J.; Malhotra, N.; Cai, J.C.; Malhotra, N.; Lui, V.; Gibson, J. Artificial
IntelligenceforGoodHealth:AScopingReviewoftheEthicsLiterature.BMCMed.Ethics2021,22,14.[CrossRef][PubMed]
182. Sharkey,A.;Sharkey,N.GrannyandtheRobots:EthicalIssuesinRobotCarefortheElderly.EthicsInf.Technol.2012,14,27–40.
[CrossRef]Robotics2024,13,112 40of43
183. Siqueira-Batista,R.;Souza,C.R.;Maia,P.M.;Siqueira,S.L.ROBOTICSURGERY:BIOETHICALASPECTS.ABCDArq.Bras.Cir.
Dig.SãoPaulo2016,29,287–290.[CrossRef][PubMed]
184. O’Brolcháin,F.RobotsandPeoplewithDementia:UnintendedConsequencesandMoralHazard.Nurs.Ethics2019,26,962–972.
[CrossRef][PubMed]
185. HouseofLords.AIintheUK:Ready,WillingandAble.2017.Availableonline:https://publications.parliament.uk/pa/ld20171
9/ldselect/ldai/100/100.pdf(accessedon10March2024).
186. Decker,M.CaregivingRobotsandEthicalReflection:ThePerspectiveofInterdisciplinaryTechnologyAssessment.AISoc.2008,
22,315–330.[CrossRef]
187. Coeckelbergh,M.;Pop,C.;Simut,R.;Peca,A.;Pintea,S.;David,D.;Vanderborght,B.ASurveyofExpectationsAbouttheRoleof
RobotsinRobot-AssistedTherapyforChildrenwithASD:EthicalAcceptability,Trust,Sociability,Appearance,andAttachment.
Sci.Eng.Ethics2016,22,47–65.[CrossRef]
188. Feil-Seifer,D.;Mataric´,M.J.SociallyAssistiveRobotics.IEEERobot.Autom.Mag.2011,18,24–31.[CrossRef]
189. Luxton,D.D.RecommendationsfortheEthicalUseandDesignofArtificialIntelligentCareProviders.Artif.Intell.Med.2014,62,
1–10.[CrossRef][PubMed]
190. Nielsen,S.;Langensiepen,S.;Madi,M.;Elissen,M.;Stephan,A.;Meyer,G.ImplementingEthicalAspectsintheDevelopmentof
aRoboticSystemforNursingCare:AQualitativeApproach.BMCNurs.2022,21,180.[CrossRef]
191. Yasuhara,Y.ExpectationsandEthicalDilemmasConcerningHealthcareCommunicationRobotsinHealthcareSettings:ANurse’s
Perspective.InInformationSystems-IntelligentInformationProcessingSystems,NaturalLanguageProcessing,AffectiveComputingand
ArtificialIntelligence,andanAttempttoBuildaConversationalNursingRobot;IntechOpen:London,UK,2021;ISBN978-1-83962-360-8.
192. Chatila,R.; Havens,J.C.TheIEEEGlobalInitiativeonEthicsofAutonomousandIntelligentSystems. InRoboticsandWell-
Being;AldinhasFerreira,M.I.,SilvaSequeira,J.,SinghVirk,G.,Tokhi,M.O.,Kadar,E.,Eds.;IntelligentSystems,Controland
Automation:ScienceandEngineering;SpringerInternationalPublishing:Cham,Switzerland,2019;Volume95,pp.11–16.ISBN
978-3-030-12523-3.
193. The Toronto Declaration. Available online: https://www.torontodeclaration.org/declaration-text/english/ (accessed on
26June2024).
194. AIUniversalGuidelines–Thepublicvoice.Org.Availableonline:https://thepublicvoice.org/ai-universal-guidelines/(accessed
on26June2024).
195. Chakraborty,A.;Karhade,M.GlobalAIGovernanceinHealthcare:ACross-JurisdictionalRegulatoryAnalysis.Availableonline:
https://arxiv.org/abs/2406.08695v1(accessedon24June2024).
196. Birhane,A.;Kasirzadeh,A.;Leslie,D.;Wachter,S.ScienceintheAgeofLargeLanguageModels.Nat.Rev.Phys.2023,5,277–280.
[CrossRef]
197. Browning, J.; LeCun, Y. Language, Common Sense, and the Winograd Schema Challenge. Artif. Intell. 2023, 325, 104031.
[CrossRef]
198. Abdulsaheb,J.A.;Kadhim,D.J.ClassicalandHeuristicApproachesforMobileRobotPathPlanning:ASurvey.Robotics2023,12,
93.[CrossRef]
199. Müller,V.C.EthicsofArtificialIntelligenceandRobotics.InTheStanfordEncyclopediaofPhilosophy;Zalta,E.N.,Nodelman,U.,
Eds.;MetaphysicsResearchLab,StanfordUniversity:Stanford,CA,USA,2023.
200. Clanahan,J.M.;Awad,M.M.HowDoesRobotic-AssistedSurgeryChangeORSafetyCulture?AMAJ.Ethics2023,25,615–623.
[CrossRef]
201. Ullah,E.;Parwani,A.;Baig,M.M.;Singh,R.ChallengesandBarriersofUsingLargeLanguageModels(LLM)SuchasChatGPT
forDiagnosticMedicinewithaFocusonDigitalPathology–ARecentScopingReview. Diagn. Pathol. 2024,19,43. [CrossRef]
[PubMed]
202. Lown,B.A.;Rosen,J.;Marttila,J.AnAgendaForImprovingCompassionateCare:ASurveyShowsAboutHalfOfPatientsSay
SuchCareIsMissing.HealthAff.2011,30,1772–1778.[CrossRef]
203. shanepeckham Getting Started with LLM Prompt Engineering. Available online: https://learn.microsoft.com/en-us/ai/
playbook/technology-guidance/generative-ai/working-with-llms/prompt-engineering(accessedon28May2024).
204. Wang,L.;Chen,X.;Deng,X.;Wen,H.;You,M.;Liu,W.;Li,Q.;Li,J.PromptEngineeringinConsistencyandReliabilitywiththe
Evidence-BasedGuidelineforLLMs.NpjDigit.Med.2024,7,1–9.[CrossRef][PubMed]
205. Marson,S.M.;Powell,R.M.GoffmanandtheInfantilizationofElderlyPersons:ATheoryinDevelopment.J.Sociol.Soc.Welf.
2014,41,143–158.[CrossRef]
206. Hendrycks,D.;Burns,C.;Basart,S.;Zou,A.;Mazeika,M.;Song,D.;Steinhardt,J.MeasuringMassiveMultitaskLanguage
Understanding.InProceedingsoftheInternationalConferenceonLearningRepresentations,VirtualEvent,3–7May2021.
207. Naveed,H.;Khan,A.U.;Qiu,S.;Saqib,M.;Anwar,S.;Usman,M.;Akhtar,N.;Barnes,N.;Mian,A.AComprehensiveOverview
ofLargeLanguageModels.arXiv2024,arXiv:2307.06435.
208. Busselle,R.;Reagan,J.;Pinkleton,B.;Jackson,K.FactorsAffectingInternetUseinaSaturated-AccessPopulation.Telemat.Inform.
1999,16,45–58.[CrossRef]
209. Ali, M.R.; Lawson, C.A.; Wood, A.M.; Khunti, K.AddressingEthnicandGlobalHealthInequalitiesintheEraofArtificial
IntelligenceHealthcareModels:ACallforResponsibleImplementation.J.R.Soc.Med.2023,116,260–262.[CrossRef][PubMed]Robotics2024,13,112 41of43
210. JohnmaedaPromptEngineeringwithSemanticKernel.Availableonline:https://learn.microsoft.com/en-us/semantic-kernel/
prompts/(accessedon28May2024).
211. Das, B.C.; Amini, M.H.; Wu, Y. Security and Privacy Challenges of Large Language Models: A Survey. arXiv 2024,
arXiv:2402.00888.
212. Mireshghallah, N.; Kim, H.; Zhou, X.; Tsvetkov, Y.; Sap, M.; Shokri, R.; Choi, Y.CanLLMsKeepaSecret? TestingPrivacy
ImplicationsofLanguageModelsviaContextualIntegrityTheory.InProceedingsoftheTheTwelfthInternationalConferenceon
LearningRepresentations,Vienna,Austria,7–11May2024.
213. OpenAIPrivacyPolicy.Availableonline:https://openai.com/policies/privacy-policy/(accessedon28May2024).
214. OpenAIHowYourDataIsUsedtoImproveModelPerformance|OpenAIHelpCenter.Availableonline:https://help.openai.
com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance(accessedon13May2024).
215. Liu,L.;Ning,L.USER-LLM:EfficientLLMContextualizationwithUserEmbeddings.Availableonline:http://research.google/
blog/user-llm-efficient-llm-contextualization-with-user-embeddings/(accessedon28May2024).
216. Staab,R.;Vero,M.;Balunovi’c,M.;Vechev,M.T.BeyondMemorization:ViolatingPrivacyViaInferencewithLargeLanguage
Models.InProceedingsoftheTheTwelfthInternationalConferenceonLearningRepresentations,Vienna,Austria,7–11May2024.
217. Chen,Y.;Lent,H.;Bjerva,J.TextEmbeddingInversionSecurityforMultilingualLanguageModels.arXiv2024,arXiv:2401.12192.
218. Zhang,Y.;Jia,R.;Pei,H.;Wang,W.;Li,B.;Song,D.TheSecretRevealer: GenerativeModel-InversionAttacksAgainstDeep
NeuralNetworks. InProceedingsofthe2020IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),
VirtualEvent,13–19June2020;pp.250–258.
219. Morris,J.X.; Zhao,W.; Chiu,J.T.; Shmatikov,V.; Rush,A.M.LanguageModelInversion. InProceedingsoftheTheTwelfth
InternationalConferenceonLearningRepresentations,Vienna,Austria,7May2024.
220. Liu,Y.;Palmieri,L.;Koch,S.;Georgievski,I.;Aiello,M.DELTA:DecomposedEfficientLong-TermRobotTaskPlanningUsing
LargeLanguageModels.arXiv2024,arXiv:2404.03275.
221. Touvron,H.;Lavril,T.;Izacard,G.;Martinet,X.;Lachaux,M.-A.;Lacroix,T.;Rozière,B.;Goyal,N.;Hambro,E.;Azhar,F.;etal.
LLaMA:OpenandEfficientFoundationLanguageModels.arXiv2023,arXiv:2302.13971.
222. BTGenBot:BehaviorTreeGenerationforRoboticTaskswithLightweightLLMs.Availableonline:https://ar5iv.labs.arxiv.org/
html/2403.12761(accessedon14May2024).
223. OpenAIOpenAIPlatform.Availableonline:https://platform.openai.com(accessedon8March2024).
224. Montreuil,V.;Clodic,A.;Ransan,M.;Alami,R.PlanningHumanCenteredRobotActivities.InProceedingsofthe2007IEEE
InternationalConferenceonSystems,ManandCybernetics,Montreal,QC,Canada,7–10October2007;pp.2618–2623.
225. Son,T.C.;Pontelli,E.;Balduccini,M.;Schaub,T.AnswerSetPlanning:ASurvey.TheoryPract.Log.Program.2023,23,226–298.
[CrossRef]
226. Gebser,M.;Kaminski,R.;Kaufmann,B.;Schaub,T.Clingo=ASP+Control:PreliminaryReport.arXiv2014,arXiv:1405.3694.
227. Wang,J.;Shi,E.;Yu,S.;Wu,Z.;Ma,C.;Dai,H.;Yang,Q.;Kang,Y.;Wu,J.;Hu,H.;etal. PromptEngineeringforHealthcare:
MethodologiesandApplications.arXiv2024,arXiv:2304.14670.
228. Xiao,G.;Tian,Y.;Chen,B.;Han,S.;Lewis,M.EfficientStreamingLanguageModelswithAttentionSinks.InProceedingsofthe
InternationalConferenceonLearningRepresentations,Vienna,Austria,7–11May2024.
229. Hooper,C.;Kim,S.;Mohammadzadeh,H.;Mahoney,M.W.;Shao,Y.S.;Keutzer,K.;Gholami,A.KVQuant:Towards10Million
ContextLengthLLMInferencewithKVCacheQuantization.arXiv2024,arXiv:2401.18079.
230. Papers with Code—HellaSwag Benchmark (Sentence Completion). Available online: https://paperswithcode.com/sota/
sentence-completion-on-hellaswag(accessedon28May2024).
231. Minaee,S.;Mikolov,T.;Nikzad,N.;Chenaghlu,M.A.;Socher,R.;Amatriain,X.;Gao,J.LargeLanguageModels:ASurvey.arXiv2024.
[CrossRef]
232. Rights(OCR),O.forC.SummaryoftheHIPAAPrivacyRule.Availableonline:https://www.hhs.gov/hipaa/for-professionals/
privacy/laws-regulations/index.html(accessedon20May2024).
233. GeneralDataProtectionRegulation(GDPR)–LegalText.Availableonline:https://gdpr-info.eu/(accessedon20May2024).
234. Canada, O. of the P.C. of PIPEDA Requirements in Brief. Available online: https://www.priv.gc.ca/en/privacy-topics/
privacy-laws-in-canada/the-personal-information-protection-and-electronic-documents-act-pipeda/pipeda_brief/(accessedon
20May2024).
235. Han,X.;You,Q.;Liu,Y.;Chen,W.;Zheng,H.;Mrini,K.;Lin,X.;Wang,Y.;Zhai,B.;Yuan,J.InfiMM-Eval:ComplexOpen-Ended
ReasoningEvaluationforMulti-ModalLargeLanguageModels.arXiv2023,arXiv:2311.11567.
236. Seo,G.;Park,S.;Lee,M.HowtoCalculatetheLifeCycleofHigh-RiskMedicalDevicesforPatientSafety.Front.PublicHealth
2022,10,989320.[CrossRef]
237. Javaid, M.; Estivill-Castro, V. Explanations from a Robotic Partner Build Trust on the Robot’s Decisions for Collaborative
Human-HumanoidInteraction.Robotics2021,10,51.[CrossRef]
238. HowShouldAISystemsBehave,andWhoShouldDecide?Availableonline:https://openai.com/index/how-should-ai-systems-
behave/(accessedon24June2024).
239. Altman, S.; Brockman, G.; Sutskever, I. Governance of Superintelligence. Available online: https://openai.com/index/
governance-of-superintelligence/(accessedon24June2024).Robotics2024,13,112 42of43
240. Leike,J.;Sutskever,I.IntroducingSuperalignment.Availableonline:https://openai.com/index/introducing-superalignment/
(accessedon24June2024).
241. Raval,V.;Shah,S.ThePracticalAspect:PrivacyCompliance—APathtoIncreaseTrustinTechnology.ISACA2020,6,15–19.
242. Lin,T.-Y.;Maire,M.;Belongie,S.;Bourdev,L.;Girshick,R.;Hays,J.;Perona,P.;Ramanan,D.;Zitnick,C.L.;Dollár,P.Microsoft
COCO:CommonObjectsinContext.InProceedingsoftheEuropeanConferenceonComputerVision,Zurich,Switzerland,6–12
September2014;pp.740–755.
243. Krishna,R.;Zhu,Y.;Groth,O.;Johnson,J.;Hata,K.;Kravitz,J.;Chen,S.;Kalantidis,Y.;Li,L.-J.;Shamma,D.A.;etal. Visual
Genome:ConnectingLanguageandVisionUsingCrowdsourcedDenseImageAnnotations.Int.J.Comput.Vis.2017,123,32–73.
[CrossRef]
244. Su,Y.;Lan,T.;Li,H.;Xu,J.;Wang,Y.;Cai,D.PandaGPT:OneModelToInstruction-FollowThemAll.arXiv2023,arXiv:2305.16355.
245. SunnybrookHospitalPatientandVisitorRecordingPolicy-SunnybrookHospital. Availableonline: https://sunnybrook.ca/
content/?page=privacy-recording-policy(accessedon28May2024).
246. Bello,S.A.;Yu,S.;Wang,C.Review:DeepLearningon3DPointClouds.Remote.Sens.2020,12,1729.[CrossRef]
247. Xu,R.;Wang,X.;Wang,T.;Chen,Y.;Pang,J.;Lin,D.PointLLM:EmpoweringLargeLanguageModelstoUnderstandPoint
Clouds.arXiv2023,arXiv:abs/2308.16911.
248. Robinson, F.; Nejat, G.ADeepLearningHumanActivityRecognitionFrameworkforSociallyAssistiveRobotstoSupport
ReablementofOlderAdults. InProceedingsofthe2023IEEEInternationalConferenceonRoboticsandAutomation(ICRA),
London,UK,29May–2June2023;IEEE:London,UK,2023;pp.6160–6167.
249. MetaMetaLlama2.Availableonline:https://llama.meta.com/llama2/(accessedon28May2024).
250. Colossal-AI One Half-Day of Training Using a Few Hundred Dollars Yields Similar Results to Mainstream Large Models,
Open-SourceandCommercial-FreeDomain-SpecificLLMSolution.Availableonline:https://hpc-ai.com/blog/one-half-day-of-
training-using-a-few-hundred-dollars-yields-similar-results-to-mainstream-large-models-open-source-and-commercial-free-
domain-specific-llm-solution(accessedon28May2024).
251. Zhang, Z.; Zhao, J.; Zhang, Q.; Gui, T.; Huang, X. Unveiling Linguistic Regions in Large Language Models. arXiv 2024,
arXiv:2402.14700.
252. Wen-Yi,A.;Mimno,D.HyperpolyglotLLMs:Cross-LingualInterpretabilityinTokenEmbeddings.arXiv;2023.
253. Mahowald,K.;Ivanova,A.A.;Blank,I.A.;Kanwisher,N.;Tenenbaum,J.B.;Fedorenko,E.DissociatingLanguageandThoughtin
LargeLanguageModels.TrendsCogn.Sci.2024,28,517–540.[CrossRef][PubMed]
254. Chang,K.;Xu,S.;Wang,C.;Luo,Y.;Xiao,T.;Zhu,J.EfficientPromptingMethodsforLargeLanguageModels:ASurvey.arXiv
2024,arXiv:2404.01077.
255. OpenAIIntroducingGPTs.Availableonline:https://openai.com/index/introducing-gpts/(accessedon24June2024).
256. Choi,J.;Yun,J.;Jin,K.;Kim,Y.Multi-News+: Cost-EfficientDatasetCleansingviaLLM-BasedDataAnnotation. arXiv2024,
arXiv:2404.09682.
257. Ishibashi,Y.;Shimodaira,H.KnowledgeSanitizationofLargeLanguageModels.arXiv2024,arXiv:2309.11852.
258. Faraboschi,P.;Giles,E.;Hotard,J.;Owczarek,K.;Wheeler,A.ReducingtheBarrierstoEntryforFoundationModelTraining.
arXiv2024,arXiv:2404.08811.
259. Guo,M.; Wang,Y.; Yang,Q.; Li,R.; Zhao,Y.; Li,C.; Zhu,M.; Cui,Y.; Jiang,X.; Sheng,S.; etal. NormalWorkflowandKey
StrategiesforDataCleaningTowardReal-WorldData:Viewpoint.Interact.J.Med.Res.2023,12,e44310.[CrossRef]
260. CommonCrawl—Overview.Availableonline:https://commoncrawl.org/overview(accessedon24June2024).
261. Chaudhari,S.;Aggarwal,P.;Murahari,V.;Rajpurohit,T.;Kalyan,A.;Narasimhan,K.;Deshpande,A.;daSilva,B.C.RLHF
Deciphered:ACriticalAnalysisofReinforcementLearningfromHumanFeedbackforLLMs.arXiv2024,arXiv:2404.08555.
262. Dwork, C.; McSherry, F.; Nissim, K.; Smith, A.CalibratingNoisetoSensitivityinPrivateDataAnalysis. InTheoryofCryp-
tography,ProceedingsoftheThirdTheoryofCryptographyConference,TCC2006;NewYork,NY,USA,4–7March2006,Springer:
Berlin/Heidelberg,Germany,2006;pp.265–284.
263. Lukas, N.; Salem, A.; Sim, R.; Tople, S.; Wutschitz, L.; Zanella-Béguelin, S. Analyzing Leakage of Personally Identifiable
InformationinLanguageModels.InProceedingsofthe2023IEEESymposiumonSecurityandPrivacy(SP),LosAlamitos,CA,
USA,22–24May2023;pp.346–363.
264. Lample,G.;Ballesteros,M.;Subramanian,S.;Kawakami,K.;Dyer,C.NeuralArchitecturesforNamedEntityRecognition.In
Proceedingsofthe2016ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:Human
LanguageTechnologies,SanDiego,CA,USA,12–17June2016;2016;pp.260–270.
265. Yu,D.;Kairouz,P.;Oh,S.;Xu,Z.Privacy-PreservingInstructionsforAligningLargeLanguageModels.InProceedingsofthe41st
InternationalConferenceonMachineLearning,Vienna,Austria,21–27July2024.
266. Li,X.;Tramèr,F.;Liang,P.;Hashimoto,T.LargeLanguageModelsCanBeStrongDifferentiallyPrivateLearners.InProceedings
oftheInternationalConferenceonLearningRepresentations,VirtualEvent,25–29April2022.
267. EverythingYouNeedtoKnowaboutthe“RighttoBeForgotten”. Availableonline: https://gdpr.eu/right-to-be-forgotten/
(accessedon24June2024).
268. Charity,F.H.RITA(Reminiscence/Rehabilitation&InteractiveTherapyActivities).Availableonline:https://www.nhsfife.org/
fife-health-charity/what-weve-funded/rita-reminiscencerehabilitation-interactive-therapy-activities/(accessedon3July2024).Robotics2024,13,112 43of43
269. Gao,Y.;Xiong,Y.;Gao,X.;Jia,K.;Pan,J.;Bi,Y.;Dai,Y.;Sun,J.;Wang,M.;Wang,H.Retrieval-AugmentedGenerationforLarge
LanguageModels:ASurvey.arXiv2024,arXiv:2312.10997.
270. OpenAI GPT-4V(Ision) System Card. Available online: https://openai.com/index/gpt-4v-system-card/ (accessed on
4July2024).
271. Koh,W.Q.;Felding,S.A.;Budak,K.B.;Toomey,E.;Casey,D.BarriersandFacilitatorstotheImplementationofSocialRobotsfor
OlderAdultsandPeoplewithDementia:AScopingReview.BMCGeriatr.2021,21,351.[CrossRef]
272. OpenAIAPI.Availableonline:https://openai.com/index/openai-api/(accessedon4July2024).
273. YouTubeDataAPI.Availableonline:https://developers.google.com/youtube/v3(accessedon4July2024).
274. Stockton,T.OrganizationsFearOntario’sInvestmenttoReduceSurgicalWaitTimesWillEndangerPatientsBecauseofNursing
Shortage.CapitalCurrent.2021.Availableonline:https://capitalcurrent.ca/organizations-fear-ontarios-investment-to-reduce-
surgical-wait-times-will-endanger-patients-because-of-nursing-shortage/(accessedon4July2024).
275. Driess,D.;Xia,F.;Sajjadi,M.S.M.;Lynch,C.;Chowdhery,A.;Ichter,B.;Wahid,A.;Tompson,J.;Vuong,Q.;Yu,T.;etal.PaLM-E:
AnEmbodiedMultimodalLanguageModel.InProceedingsofthe40thInternationalConferenceonMachineLearning,Honolulu,
HI,USA,23–29July2023;p.340.
276. Iovino,M.;Scukins,E.;Styrud,J.;Ögren,P.;Smith,C.ASurveyofBehaviorTreesinRoboticsandAI.Robot.Auton.Syst.2022,
154,104096.[CrossRef]
277. CloudComputingServices.Availableonline:https://cloud.google.com/(accessedon5July2024).
Disclaimer/Publisher’sNote: Thestatements, opinionsanddatacontainedinallpublicationsaresolelythoseoftheindividual
author(s)andcontributor(s)andnotofMDPIand/ortheeditor(s).MDPIand/ortheeditor(s)disclaimresponsibilityforanyinjuryto
peopleorpropertyresultingfromanyideas,methods,instructionsorproductsreferredtointhecontent.