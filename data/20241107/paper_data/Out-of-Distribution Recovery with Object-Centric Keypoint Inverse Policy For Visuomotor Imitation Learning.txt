Out-of-Distribution Recovery with Object-Centric
Keypoint Inverse Policy For Visuomotor Imitation
Learning
GeorgeJiayuanGao, TianyuLi, NadiaFigueroa
UniversityofPennsylvania
{gegao,tianyuli,nadiafig}@seas.upenn.edu
Abstract:Weproposeanobject-centricrecoverypolicyframeworktoaddressthe
challengesofout-of-distribution(OOD)scenariosinvisuomotorpolicylearning.
Previousbehaviorcloning(BC)methodsrelyheavilyonalargeamountoflabeled
data coverage, failing in unfamiliar spatial states. Without relying on extra data
collection, our approach learns a recovery policy constructed by an inverse pol-
icyinferredfromobjectkeypointmanifoldgradientintheoriginaltrainingdata.
The recovery policy serves as a simple add-on to any base visuomotor BC pol-
icy, agnostic to a specific method, guiding the system back towards the training
distribution to ensure task success even in OOD situations. We demonstrate the
effectiveness of our object-centric framework in both simulation and real robot
experiments, achievinganimprovementof77.7%overthebasepolicyinOOD.
ProjectWebsite: https://sites.google.com/view/ocr-penn
Keywords: VisuomotorPolicy,OODRecovery,Object-Centric
Figure 1: Object-Centric Recovery (OCR) on Bottle Pick and Place Task. The base visuomotor policy
(Right),trainedonthebottle’sinitialposewithinthegreen-shadedregionofthetable,exhibitslimitedgener-
alizationwhenthebottleisinitializedinthered-shadedregion,whichisconsideredout-of-distribution(OOD).
(Left) showed the recovery policy using our OCR framework to recover from the red-shaded OOD region,
returningthesystemtoaregionofhighconfidenceforthebasevisuomotorpolicy,whereitresumescontrol.
1 Introduction
Robot learning has achieved significant success in deploying Imitation Learning (IL) methods on
real-worldroboticsystems[1]. OnewidelystudiedapproachwithinILisBehaviorCloning(BC),
whichhasbeenexploredextensivelyinrecentwork[2,3,4,5,6,7]. BCmethodsenablelearning
controlpoliciesdirectlyfromdemonstrationswithouttheneedforexplicitenvironmentalmodeling,
makingtheprocessrelativelystraightforward. However,despiteproducingpromisingresults,BCis
well-knownforitssusceptibilitytothecovariateshiftproblem[1]. Thisissuearisesbecausetradi-
tionalBCapproachesdependheavilyonlargequantitiesoflabeleddata, whichareoftenobtained
through labor-intensive methods such as teleoperation or kinesthetic teaching. Consequently, BC
may struggle to perform reliably in out-of-distribution (OOD) scenarios, where data is sparse or
8thConferenceonRobotLearning(CoRL2024),Munich,Germany.
4202
voN
6
]OR.sc[
2v49230.1142:viXranoisy—reflectingabroaderchallengefacedinsupervisedlearning. Addressingthisissuetypically
requires either returning to laborious data collection or utilizing corrective mechanisms, such as
guidancefromhumanoperatorsorreinforcementlearning(RL)agents[8,9,10,11],bothofwhich
imposeadditionaldeploymenteffortsonroboticsystems.
To enjoy the benefits of strong performing BC policies in distribution (ID) settings while not re-
quiringthehumaneffortofcollectingmoredataorthecomputeeffortofrunninganRLstepwhen
OOD, in this work, we propose a recovery policy framework that brings the system back to the
training distribution to ensure task success even when OOD. In particular, we focus on the key
challengesofvisuomotorpolicylearningbyintegratingarecoverypolicyconstructedfromthegra-
dientofthetrainingdatamanifoldwithabasevisuomotorBCpolicy(e.g.,adiffusionpolicy[2]).
Inspiredbythe“BacktotheManifold”approach[12]therecoverypolicyguidesthesystembackto-
wardsthetrainingmanifold,atwhichpointthebasepolicyresumescontrol. However,unlike[12],
which focuses on recovering from OOD scenarios related to the robot’s state, our approach takes
anobject-centricperspective,specificallyaddressingOODsituationsfortask-relevantobjectstates.
Webelievethisobject-centricapproachsignificantlyenhancestheOODrecoverycapabilitiesofvi-
suomotorpolicies,leadingtomorerobustlearningforobjectmanipulationtasks. Furthermore,our
recoveryframeworkisdesignedtobeagnostictothechoiceofbasepolicy,allowingittobeseam-
lesslyintegratedwithvariousBCimplementations. Thisflexibilitymakesourmethodadaptablefor
futuredevelopmentsinimitationlearning(IL).Inthispaper,wemaketheassumptionthatwehave
access to relevant object models. Also, we mainly consider the OOD cases in which the relevant
objectentersunfamiliarspatialregions.
Therestofthepaperisorganizedasfollows: Section2presentsanoverviewoftheexistingworks.
Section3describestheproblemformulation. Section4presentstheobject-centricrecoverypolicy
framework in detail, including its construction of the training data manifold and the keypoint in-
versepolicy. Insection5,wedemonstratetheeffectivenessofourapproachonseveralbenchmarks,
including both simulation and real robot experiments, showing that our recovery policy improves
the performance when entering unfamiliar states. We also show that our method has the desired
propertyforlifelonglearningofvisuomotorpolicies,improvingtheperformanceofOODwhilenot
diminishing the in-distribution performance. Section 6 discusses the limitations and future direc-
tions.
2 RelatedWork
When deploying to the real world, vision-based IL could easily be initialized or moved to OOD
situations,possiblyduetobiasindatacollectionandcompoundingerrors. DeployingBCmethods
OODcouldleadtounknownbehaviorinthelow-dataregion. Toaddressthis,awell-knownfamily
ofapproachesisDataAggregation,whichgathersextradatafromexpertpolicies(usuallyprovided
by humans) through online interaction [13, 8, 10, 11]. However, performing such an online data
collection procedure is an additional burden to the human when building a system. Our method
tries to avoid additional online interaction and cumbersome data collection by squeezing as much
informationfromtheexistingtrainingdata.TheOODproblemhasreceivedmuchattentionfromthe
offlineRLcommunity. OfflineRLsufferslesscompoundingerrorthanBCmethodsasitoptimizes
forlong-termoutcomes[14]. However,itcouldstillstrugglewithdistributionshiftslikeextrapola-
tionerrorsduetolimiteddata. TotackletheOODproblem,methodslike[15,16,17]trytopenalize
actionsthatarefarfromthedata. Moreover,severalworks[18,19]alsoproposetorecoverbackto
thetrainingdataregion,whichindirectlysharesasimilarideaasourwork.TheparadigmofBC+RL
hasalsobeenapopularchoiceforaddressingtheOODproblem[20,9,21]. Ourapproachtakeson
asimilardirectionfortrainingarecoverypolicy,butinstead,weuseobject-centricBCastheadd-on
forthebaseBCpolicy.Closelyrelatedtoourworkis[12],whichintroducesavision-basedOODre-
coverypolicyby1)learninganequivariantmapthatencodesvisualobservationsintoalatentspace
whosegradientcorrespondstorobotend-effectorpositions,and2)followingagradientlearnedbya
MixtureDensityNetwork(MDN)[22].Ratherthanrecoveringtherobotaction,ourworkfocuseson
recoveringtask-relevantobjectsandinducingtherobotaction. Whendealingwithobjects,itcould
2bemoregeneraltoutilizeobject-centricrepresentation. Manyworks[6,23,24,25,26]demonstrate
thesuccessofobject-centricrepresentationpolicylearning. Inourwork,weusekeypointsderiving
from pose estimation [27] as the object representation for the recovery policy. There are existing
worksonusingkeypointastherepresentationforpolicylearning[28,29]. However, itisdifficult
for these methods to provide consistent 3D keypoints on the object across training and inference,
whichourmethodheavilyrelieson. Wewouldliketoaddressthisforfuturework. Recentworks
alsoexploreusingLargeLanguageModelstodeterminefailureandperformrecovery[30,31].
3 RecoveryProblemFormulation
Distribution shift for learning models is commonly quantified by measuring the Kullback-Leibler
(KL) divergence between the distribution of observations during training and the distribution of
observationsencounteredattesttime [32,33,34]. Thisdivergencereflectshowmuchthetest-time
observationsdeviatefromthoseseenduringtraining,providingametrictomeasureifascenariowe
encounteredisout-of-distribution.Formally,iftheprobabilitydistributionofthetrainingandtesting
observations is P(O) and Q(O) respectively, with O representing the set of observations, we say
thatthetestingobservationwillbeconsideredout-of-distribution(OOD)if,
D (P(O)∥Q(O))>ϵ (1)
KL
isassertedtobetruewithsomethresholdϵ>0.
We formulate our visuomotor policy interaction with the environment as a Partially Observable
Markov Decision Model (POMDP) [35]. We describe this POMDP by the tuple (S,A,O,T,E),
where s ∈ S is the set of environmental states, which are directly observable, a ∈ A is the set of
robotactions,ando ∈ O isthesetofvisualobservations. ThetransitionfunctionT : S×A → S
dictates how the unobservable state changes when robot actions are performed, and the emission
functionE :S →Oisasurjectivefunctionthatdeterminesthevisualobservationsgivenstates.
Giventhisformulation,wecanreformulatetheKLDivergenceOODmetricasfollows,
D (P(E(S))∥Q(E(S)))>ϵ. (2)
KL
Hence,fundamentally,givenanobservation-levelout-of-distributionscenario,iftheenvironmental
state variables are recovered back into the training distribution, the observations will also be re-
coveredbackintodistribution. However, therecoveryofallstatevariablesisdifficulttotackleall
at once under the imitation learning framework, which typically has access to only task-relevant
demonstrations. Therefore,forthiswork,wespecificallyfocusontherecoveryoftask-relevantob-
jectsinmanipulationtasks. Unlikepreviousdataaggregationorreinforcementlearningapproaches,
weaimforourrecoveryframeworktoexclusivelyleveragethetrainingdemonstrationsofthebase
policyandnotrequireanyadditionalpolicy-relateddatacollection.
4 Method
We present our approach in augmenting a base policy trained via Behavior Cloning (BC) by in-
corporatinganobject-centricrecoverystrategy, whichenablestask-relevantobjectstoreturntoits
EuclideantrainingmanifoldwherethebaseBCpolicyfunctionsatitsbest. Forourwork,wewill
assumetask-relevantobjectsinthescenearerigidandnon-deformable.
Toachievethis,weintroducetheObject-CentricRecovery(OCR)framework,asillustratedinFig-
ure 2. We first explicitly model the distribution of objects keypoints in the training dataset with a
GaussianMixtureModel(GMM)[36]p(ρ(i)|θ ) = ∑︁M λ N (ρ(i)|µ ,Σ )whereρ(i)
k,t k m=1 k,m θk k,t k,m k,m k,t
are keypoints in the dataset and θ = {(λ ,µ ,Σ )}M parameters of the GMM (Sec-
k k,m k,m k,m m=1
tion4.2.1,4.2.2). Attesttime,weevaluatethegradient∇p(ρtest|θ )toobtaintheobject-recovery
k k
vectors,whichweusetoplanforanobject-recoverytrajectoryζL (Section4.2.4). Wethentrans-
rec
late this trajectory into robot actions via a Keypoint Inverse Policy π that is trained using the
inv
basedataset(Section4.2.3). Lastly, Section4.2.5describeshowthebasepolicyinteractswiththe
recoverypolicytobecometheOCRjointpolicy.
3Training Time
🤖 a) Base Policy
Video
BC
Dataset
Keypoint Object Manifold Keypoint Inverse Joint Policy
Generation Estimation Policy
Off-The-Shelf 6D Density-Activated
Pose Estimator Switch
b) Recovery Action
Image Recovery-Gradient Recovery-
Estimation Keypoint
Input Planner
Inference Time
Figure 2: Object Centric Recovery (OCR) Framework. The OCR Framework augments a base policy
π , trainedviaBC,byreturningtask-relevantobjectstotheirtrainingmanifold, wherethebasepolicytakes
b
over. First,wemodelthedistributionofobjectkeypointsinthetrainingdatausingaGaussianMixtureModel
(GMM).Attesttime,wecomputethegradientoftheGMMtoderiveobject-recoveryvectors,whichareused
toplanarecoverytrajectory. ThistrajectoryisthenconvertedintorobotactionsthroughaKeypointInverse
Policyπ ,trainedsolelyonthebasedataset. Finally,thebasepolicyandtherecoverypolicyarecombined
inv
intoajointpolicy,allowingseamlessinteractionbetweenrecoveryandtaskexecution.
4.1 BaseBCPolicy
Ourformulationconsidersagenericvisuomotorpolicythatoutputsfutureactionsbasedonpastvi-
sualobservationsasthebaseBCpolicy. Weconsidersuchaliberalformulationtodemonstratethat
ourframeworkcanworkalongsideanyvariationsofBCpolicy. Formally,wedefineatypicalvisuo-
motorpolicytrainingdatasetasD ={d(i)}N ,whereeachepisoded(i) ={(o(i),a(i),p(i))}T
b b i=1 b t t t t=1
consists of the observations o(i), robot actions a(i), and robot proprioception a(i) at time step t.
t t t
Then,undertheimitationlearningframework,abasevisuomotorpolicyπ thatisparameterizedby
b
ϕ islearningbyoptimizingthefollowingbehaviorcloningobjective:
b
π∗ =argminE [L(π (o,p),a)] (3)
b
θb
(o,a,p)∼Db b
WherethelossfunctionListypicallyCross-EntropyLossorMean-SquaredError.
4.2 Object-CentricRecoveryPolicy
4.2.1 KeypointGeneration
We choose to use artificial object keypoints to represent object poses for studying object-centric
recovery,askeypointsallowustotightlycouplethepositionandorientationoftheobject,facilitating
amoreaccurateestimationofitsdistributionduringtraining.
WeconsiderthesamevisuomotorpolicytrainingdatasetformulationD = {d(i)}N ,whereeach
b b i=1
episoded(i) ={(o(i),a(i),p(i))}T consistsoftheobservationso(i),robotactionsa(i),androbot
b t t t t=1 t t
proprioceptiona(i) attimestept. Toextractobjectposesfromthesevisuomotordatasets, weem-
t
ployoff-the-shelfobjectposeestimators[37,38,27]totransformeachobservationframeo(i) into
t
the object pose T(i) . Next, we define an arbitrary set of keypoints P = {p }n , where each
obj,t k k=1
p ∈ Rd. For each keypoint k at time step t in demonstration i, we compute the transformed
k
keypoints ρ(i) = h−1(T(i) h(p )), where h represents the function that converts points into ho-
k,t obj,t k
mogeneous coordinates. The transformed keypoint ϱ(i) = {ρ(i)}n then serves as the keypoint
t k,t k=1
representation of the object’s current pose. Thus, using D , we create a new dataset that will be
b
used for recovery D = {d(i)}N , where each episode d(i) = {(ϱ(i),T(i) ,a(i),p(i))}T
rec rec i=1 rec t obj,t t t t=1
consistsofthekeypoints,objectposes,robotactions,androbotproprioceptionateachtimestep.
44.2.2 ObjectManifoldEstimation
Toestimatethemanifoldoftheobjectdistributioninthetrainingdataset,wefitaGaussianMixture
Model (GMM) [36] on each keypoint using its positions across every time step in every demon-
{︂ }︂N
stration. Specifically,givendatasetD = {(ρ(i))}T consistingofoneobjectkeypointk
kp,k k,t t=1
i=1
acrossalltimesteptineverydemonstrationi,wemodeltheprobabilityofeachρ(i) asaweighted
k,t
sumofM Gaussiandistributions:
M
p(ρ(i)|θ )= ∑︂ λ N (ρ(i)|µ ,Σ ), (4)
k,t k k,m θk k,t k,m k,m
m=1
where λ is the mixing coefficient of keypoint k for the m-th Gaussian, N(ρ(i)|µ ,Σ ) is
k,m k,t k,m k,m
the Gaussian probability density function of keypoint k for the m-th component with mean µ
k,m
and covariance Σ , and θ = {(λ ,µ ,Σ )}M are the parameters of the model that
k,m k k,m k,m k,m m=1
estimatesthedistributionofkeypointk.TofitthisGMM,weusedExpectation-Maximization[36]to
maximizethelikelihoodestimationofthemodelonthedata.ComputingaGMMforallnkeypoints
would result in parameters Θ = {θ }n that collectively estimate the probability distribution of
k k=1
thekeypointsoftheobject.
4.2.3 KeypointInversePolicy
To facilitate object manipulation for recovery, we propose the use of an Keypoint Inverse Pol-
icy π , which is designed to translate a sequence of object-keypoint trajectory along with the
inv
robot’s current state, into the corresponding robot actions necessary to execute those object mo-
tionseffectively. Formally, ifwedefineK tobethesetofobjectkeypointsobserved, andP ⊆ S
to be the set of robot proprioception states, π : KL × P → AL, where L is the observa-
inv
tion length. We already have access to the dataset of object keypoints, pose, and action tuples
D = {{(ϱ(i),T(i) ,a(i),p(i))}T }N thatweobtainedinSection4.2.1, whichwecoulduti-
rec t obj,t t t t=1 i=1
lizetodirectlytrainπ withthelearningobjective:
inv
[︂ (︂ (︂ )︂ )︂]︂
π i∗ nv =argm θinin
v
E(︂ {ϱ( ti)}j t=+ jL,{a( ti)}j t=+ jL,p( ji))︂ ∼Drec L π inv {ϱ( ti)} tj =+ jL,p( ji) ,{a( ti)}j t=+ jL (5)
where sequences of length L are pulled from the keypoint and action datasets to form {ϱ(i)}j+L
t t=j
and {a(i)}j+L, and the initial proprioception of the sequence p(i). However, by training this way
t t=j j
directly, we will still run into the same issue of distribution shift, having no keypoints-to-action
coverageontheOODregionstogenerateproperlyusefulmanipulationoutputs.Toalleviatethis,we
proposetheuseoftheinitialobjectposeT(i) to“zero-out”thedatasequence.Specifically,instead
obj,t
ofusingtheoriginalsequence,weusetheinitialobjectposeofeachsequenceT(i) tomodifythe
obj,j
sequence into {(T(i) )−1ϱ(i)}j+L, {(T(i) )−1a(i)}j+L, and (T(i) )−1p(i). For simplicity, we
obj,j t t=j obj,j t t=j obj,j j
willnamethesequantitiesϱL ,aL ,p respectively. Hence,thelearningobjectivebecomes:
zero zero zero
π∗ =argminE [︁ L(︁ π (︁ ϱL ,p )︁ ,aL )︁]︁ (6)
inv
θinv
(ϱL zero,aL zero,pzero)∼Drec inv zero zero zero
Inotherwords,thekeypointinversepolicyonlyneedstolearntooutputrobotactionsfromobject
keypoint trajectories that initialize from the identity frame. At test time, to carry out an object
motion,weinputthedesiredkeypointtrajectoryinthecurrentobjectframe,andthepolicyoutputs
thecorrespondingrobotactioninthatsameframe. Toexecutetherobotaction,wethentransform
the action from the object frame to the robot frame. This way, regardless of the object and robot
end-effector’s true pose in Euclidean space, OOD or ID, as long as we have access to the current
objectpose,wecanoutputrobotactionsthatareusefulformanipulation. Inaddition,wecanthink
of this as a way of “compressing” the input domain of the keypoint inverse policy based on the
informationavailabletous(objectpose),makingthelearningproblemextremelydataefficient.
4.2.4 DesiredObject-RecoveryMotion
Object-RecoveryVectors. Attesttime,weobtaintheexplicitcurrentposeoftherecoveryobject
via pose estimation and generate keypoints using the same methodology as described in Section
53.2.1,obtainingϱtest ={ρtest}n . Foreachkeypoint,webuildacomputationgraphoftheproba-
k k=1
bilitydensityfunction(pdf)oftheGMMwithparametersθ tooutputtheprobabilitydensityηtest
k k
with respect to ρtest. With this, we used automatic differentiation to output the gradient vector
k
δtest = ∇p(ρtest|θ ). However, the norm of this gradient vector ∥δtest∥ is strictly non-negative
k k k k
andincreasesasρtestapproachesregionsofincreasinglyhigherdensityparameterizedbytheGMM
k
withparametersθ ,whichisincontrastwithhowwewantrecoverytotakeplace-toapproachre-
k
coveryfasterwhentheobjectisfurtheraway,andslowerwhentheobjectiscloser. Tosolvethis,we
modifythemagnitudeofδtest byamonotonicallydecreasingfunction,theparameterizednegative
k
ϕ−x
exponentialfunction,whichwedefineasq(x)=e η . Thus,themodifiedrecoverygradientis
δtest
δmod =q(∥δtest∥) k (7)
k k ∥δtest∥
k
Sincetherecoverygradientmightdifferforeachkeypointk,wewillusethemeanrecoverygradient
asourfinalobjectrecovery,givenbyδ
rec
=∑︁n
k=1
δ km nod . Similarly,wetakethemeanofdensityto
gainasinglevalueforthecurrentobjectdensityη
rec
=∑︁n
k=1
η kt nest . Hence,ateachtimestepattest
time,weoutputtheobjectrecoverytuple(δ ,η ).
rec rec
RecoveryKeypointPlanner. Fromtheobjectrecoveryvectorδ ,wecangenerateanaiverecov-
rec
erykeypointtrajectorylikeso:
[︁ {tαδ +ρtest}n ]︁L (8)
rec k k=1 t=1
where α is a scaling hyperparameter that we can tune at test time to optimize for the trajectory
step size. However, this formulation does not take into account the feasibility of executing such a
trajectory, which is paramount in ensuring the quality of the recovery. To this end, we propose a
heuristic planner, using the distance of the position between the robot end-effector and the object
pose as a heuristic for how much “delay” is added to the object trajectory before it starts moving,
thusprovidingtherobotwithenoughtimetoapproachtheobjectformanipulation. Specifically,we
will define a maximum and a minimum distance where the object can be effectively manipulated
which we denote as d and d , which can be tuned easily at test time. Then, we simply fit a
max min
linearfunctionbetweenpoints(d ,L)and(d ,0),andcliptherangebetween[L,0]. Formally,
min max
if we denote the norm between the position of the end-effector and object as d , then the delay
pos
function is written as df(d ) = min(max(0,⌊ −L ∗(d −d )+L⌉)L). Thus, our
pos dmax−dmin pos min
proposedkeypointrecoverytrajectoryisexpressedas:
ζL =[︁ {max(0,t−df(d ))αδ +ρtest}n ]︁L (9)
rec pos rec k k=1 t=1
4.2.5 FinalJointPolicy
We join our base policy and recovery action as a Joint Policy through a density activation switch.
Specifically,aftercomputingthemeankeypointdensityη ,weuseatunablehyperparameterϵ
rec rec
todefinethethresholdfordistinguishingbetweenout-of-distributionandin-distributionscenarios.
IfthescenarioisclassifiedasOOD,therecoverypipelineisactivated;otherwise,ifclassifiedasID,
thebasepolicywillproceedwiththestandardBCprocess. Formally,ourjointpolicyis:
π =I π (o ,p )+I π (ζL ,p ) (10)
joint {ηrec≥ϵrec} b t t {ηrec<ϵrec} inv rec t
WealsodescribeourJointPolicyalgorithmicallyinAlgorithm1inAppendixA.
5 Evaluation
We first evaluated the Object-Centric Recovery framework’s performance using two simulation
tasks, covering both prehensile and non-prehensile manipulation. These tasks were selected to
demonstrate the framework’s versatility and robustness in handling a range of manipulation sce-
narios. Additionally, we evaluated a prehensile task in a real-world robot setting to showcase
theframework’seffectivenessinpracticalapplications. Giventheabsenceofexistingmethodsfor
object-centricrecoveryinvisuomotorpolicies, tothebestofourknowledge, webenchmarkedour
results against the out-of-distribution performance of the base BC policy. To ensure consistency
acrossalltasks,weemployedthevision-inputU-Net-baseddiffusionpolicy[2]asourbasepolicy,
6andthelow-dimensionaldiffusionpolicywasutilized
as the architecture for our keypoint inverse policy.
Our results show that, compared to the base pol-
icy, the Object-Centric Recovery framework consis-
tently achieved a high task-completion rate in object
OODscenarios,withanaveragesuccessrateof81.0%
acrossthethreeevaluatedtasks,whichisanimprove-
mentof77.7%overthebasepolicyinOOD.
Figure 3: (Left) shows the Push-T Task’s ID
In addition, we show that for a life-long continuous and OOD regions divided by a dashed line.
learning scenario, we can employ the OCR frame- (Right)showstheSquareTask’sIDandOOD
work to collect recovery demonstrations that can be regionsdrawnoutbythegreenandredregions
respectively.
augmentedalongsidetheoriginalbasepolicytraining
datasettoimbuethebasepolicywiththeabilitytore-
BasePolicy JointPolicy(Ours)
cover at a high success rate without diminishing its ID OOD OOD
performanceintheoriginalIDregions.
Push-T 0.90 0.10 0.93
Square 0.87 0.00 0.80
5.1 ExperimentalSetups
Experiment 1) Non-Prehensile, Sim, Push-T Table1:Simulatedtasksuccessrateofthebase
policyvs. jointpolicyinOODscenarios,with
Task[39]: This2Dsimulatedtaskinvolvespushinga
IDscenariobaselineasthebasepolicy.
T-shapedblock(gray)towardafixedtargetusingacir-
cularend-effectoragent(blue). Ateachreset,boththe
initialposeoftheTblockandtheinitialpositionofthe
end-effector are randomized. The task is particularly
challengingduetotherequirementfordiscontinuous,
non-linearend-effectoractions. TohighlighttheOCR
framework’s capability to handle such complexity,
we provided demonstrations initialized exclusively
ontheleft sideofthescreen, asshownbythedashed
line separating the screen in Figure 3. This setup
designates the left side as in-distribution (ID) and
Figure4: (Right)showsourFrankarecovering
the right side as out-of-distribution. For PushT, we
fromOOD(red-shaded)toID(green-shaded)in
recorded100demonstrationsintheIDregion.
theBottleTask.
Experiment 2) Prehensile, Sim, Robomimic Square
BasePolicy JointPolicy(Ours)
Task [40]: This simulated task requires the robot to ID OOD OOD
pickupanotchedsquare-shapedobjectwithaholein
Bottle 0.60 0.00 0.70
the middle, transport it, and drop it through a fixed
square peg. The initial pose of the square object is Table2: Realtasksuccessrateofthebasepol-
randomized within the SE(2) space on the table, and icyvs. jointpolicyinOODscenarios,withID
theinitialpositionoftheend-effectorisalsorandom- scenariobaselinethebasepolicy.
ized. WeusedRobomimic’sPHdataset,whichis200
OrgBasePolicy AugBasePolicy
demonstrationsinitializedexclusivelyontherightside ID OOD ID OOD
ofthetable,asshownbythered-shadedregioninFig-
Push-T 0.90 0.10 0.97 0.80
ure 3. Hence, the left side of the table, or the green-
shadedregionisconsideredin-distribution Table3: Push-Ttasksuccessrateoftheorigi-
nalbasepolicyvs.augmentedbasepolicy(with
Experiment3)Prehensile,Real,BottleTask:Theob- OCRgenerateddata)inIDandOODscenarios.
jectiveofthebottletaskisfortherobottograspayel-
low bottle, transport it, and place it onto an elevated red plate. The initial pose of the bottle is
randomizedwithintheSE(2)spaceonthetable, andtheinitialpositionoftheend-effectorisalso
randomized. As illustrated in Figure 4, the green-shaded region represents the demonstrated (in-
distribution, ID) region, while the red-shaded area indicates the OOD region. We provided 115
demonstrationswithinthegreen-shadedregionforpolicytraining. WeusedaFrankaPandarobotas
thepolicyagent,andPolymetis[41]asthecontrollerinterface. Forour6Dobjectposeestimation,
weusedFoundationPose[27],andGrounded-SAM[42]toobtaintheobjectmask.
7Experiment4)ContinuousLearningonPush-T:TodemonstratetheOCRframework’seffective-
nessinlifelongcontinuouslearning,weagainappliedObject-CentricRecoverytoOOD-initialized
Push-Tscenarios. Thistime,however,thepolicyrolledoutactionsuntilthepredefinedϵ thresh-
rec
old for reaching ID was met, while recording observations and proprioceptions. From 100 OOD
initializations,wecollectedthedatasetD ,whichweappendedtoD directlytoresumetraining
aug b
onthebasepolicycheckpoint. Wethenevaluatedtheresultingaugmentedbasepolicyinboththe
originalIDandOODscenarios.
5.2 ExperimentalResults&Analysis
1)Push-T.ShowninTable1,thePush-TbasepolicygeneralizedtotheOODregionobjectinitial-
ization in only 10% of cases, mainly due to random initialization placing the end-effector on the
right, allowing it to accidentally push the block back into the in-distribution region. In contrast,
with the OCR framework, recovery actions guide the system to a feasible interaction region (e.g.,
positioningontherighttopushtheblockleft)andgentlyreturntheblocktotheIDregion, where
the base policy completes the task. Across 30 random OOD initializations, the OCR framework
achieveda93%tasksuccessrate,significantlyoutperformingthebasepolicy.
2) Square. In OOD object initialization scenarios, the Square base policy consistently attempted
to move the end-effector toward the direction of the object but failed to reach it in all cases. In
contrast, as shown in Table 1, the OCR framework successfully manipulated the square object for
recovery,achievingan80%tasksuccessrateinOODscenarios,asubstantialimprovementoverthe
basepolicy.
3) Bottle. In OOD object initialization scenarios, the Bottle base policy failed similarly to the
Squarebasepolicy, asbothtasksinvolveprehensilemanipulation. However, asshowninTable2,
theOCRframeworkeffectivelyhandledtherecoveryforbothtasks,achievingan70%tasksuccess
rateinOODscenarios,significantlyoutperformingthebasepolicy. Interestingly,weobservedthat
the OCR joint policy’s OOD success rate is significantly higher than the base policy’s ID success
rateforthebottletask. WehypothesizethatthiscanbeattributedtotheOCRframework’sabilityto
moveobjectstowardregionsofhightrainingdensityregardlessofwheretheobjectsareinitialized.
4)ContinuousLearningonPush-T.Byresumingtrainingonthebasepolicyusingboththeoriginal
datasetD andtheaugmenteddatasetD -autonomouslycollectedviatheOCRframework-we
b aug
enabled the base policy to recover independently, achieving 80% task completion in the original
OOD regions, as shown in Table 3,. This improvement did not come at the cost of performance
intheoriginalIDscenarios; infact, theaugmentedpolicyshowedenhancedperformanceinIDas
well, improving from 90% to 97%. We hypothesize this is due to the OCR’s augmented dataset
consistently providing robot actions that move the object toward regions of higher density, even
on the ID side. In other words, OCR demonstrations likely offer actions that converge the object
to the base policy, complementing it rather than replacing it. We believe this showcases the OCR
framework’sabilitytoprovidevaluabledataforcontinuouslearning.
6 Conclusion
Inthiswork,weproposedtheObject-CentricRecoverypolicyframeworkdesignedtoaddressout-
of-distribution challenges in visuomotor policy learning, by recovering task-relevant objects into
distribution without requiring additional data collection. When our framework was tested against
variousmanipulationtasks,itdemonstratedconsiderableimprovementinperformanceinOODre-
gions. Furthermore, the framework’s capacity for continuous learning highlights its potential to
autonomously enhance policy behavior over time. However, a key limitation of our approach is
therelianceonexplicitobjectposes, whichrestrictsitsapplicabilitytoarticulatedanddeformable
objects. We aim to extend our framework by incorporating more flexible scene representations to
recoverfromabroaderrangeofOODscenarios. Despitethislimitation,webelieveourframework
representsasteptowardsimprovingtherobustnessofvisuomotorpoliciesinreal-worldsettings.
8Acknowledgments
We thank Shubhodeep Shiv Aditya for assistance in setting up the real robot vision pipeline, and
Dr. DineshJayaramanforthevaluablediscussions. Wealsoappreciatethethoughtfulfeedbackand
commentsprovidedbythereviewers.
References
[1] T. Osa, J. Pajarinen, G. Neumann, J. A. Bagnell, P. Abbeel, J. Peters, et al. An algorithmic
perspectiveonimitationlearning. FoundationsandTrends®inRobotics,7(1-2):1–179,2018.
[2] C. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. Burchfiel, and S. Song. Diffusion policy:
Visuomotorpolicylearningviaactiondiffusion. arXivpreprintarXiv:2303.04137,2023.
[3] J. Pari, N. Muhammad, S. P. Arunachalam, and L. Pinto. The surprising effectiveness of
representationlearningforvisualimitation. arXivpreprintarXiv:2112.01511,2021.
[4] T.Z.Zhao, V.Kumar, S.Levine, andC.Finn. Learningfine-grainedbimanualmanipulation
withlow-costhardware. arXivpreprintarXiv:2304.13705,2023.
[5] Z.Fu, T.Z.Zhao, andC.Finn. Mobilealoha: Learningbimanualmobilemanipulationwith
low-costwhole-bodyteleoperation. arXivpreprintarXiv:2401.02117,2024.
[6] Y.Zhu,A.Joshi,P.Stone,andY.Zhu. Viola:Imitationlearningforvision-basedmanipulation
with object proposal priors. In Conference on Robot Learning, pages 1199–1210. PMLR,
2023.
[7] L.X.Shi,A.Sharma,T.Z.Zhao,andC.Finn. Waypoint-basedimitationlearningforrobotic
manipulation. arXivpreprintarXiv:2307.14326,2023.
[8] S.Ross,G.Gordon,andD.Bagnell. Areductionofimitationlearningandstructuredpredic-
tion to no-regret online learning. In Proceedings of the fourteenth international conference
onartificialintelligenceandstatistics,pages627–635.JMLRWorkshopandConferencePro-
ceedings,2011.
[9] S. Haldar, J. Pari, A. Rai, and L. Pinto. Teach a robot to fish: Versatile imitation from one
minuteofdemonstrations. arXivpreprintarXiv:2303.01497,2023.
[10] M. Kelly, C. Sidrane, K. Driggs-Campbell, and M. J. Kochenderfer. Hg-dagger: Interactive
imitation learning with human experts. In 2019 International Conference on Robotics and
Automation(ICRA),pages8077–8083.IEEE,2019.
[11] J.Spencer,S.Choudhury,M.Barnes,M.Schmittle,M.Chiang,P.Ramadge,andS.Srinivasa.
Learningfrominterventions. InRobotics: ScienceandSystems(RSS),2020.
[12] A.Reichlin, G.L.Marchetti, H.Yin, A.Ghadirzadeh, andD.Kragic. Backtothemanifold:
Recovering from out-of-distribution states. In 2022 IEEE/RSJ International Conference on
IntelligentRobotsandSystems(IROS),pages8660–8666.IEEE,2022.
[13] S.RossandD.Bagnell. Efficientreductionsforimitationlearning. InProceedingsofthethir-
teenthinternationalconferenceonartificialintelligenceandstatistics,pages661–668.JMLR
WorkshopandConferenceProceedings,2010.
[14] S.Levine, A.Kumar, G.Tucker, andJ.Fu. Offlinereinforcementlearning: Tutorial, review,
andperspectivesonopenproblems. arXivpreprintarXiv:2005.01643,2020.
[15] A.Kumar, A.Zhou, G.Tucker, andS.Levine. Conservativeq-learningforofflinereinforce-
mentlearning. AdvancesinNeuralInformationProcessingSystems,33:1179–1191,2020.
9[16] T.Yu,A.Kumar,R.Rafailov,A.Rajeswaran,S.Levine,andC.Finn. Combo: Conservative
offlinemodel-basedpolicyoptimization. Advancesinneuralinformationprocessingsystems,
34:28954–28967,2021.
[17] I.Kostrikov, A.Nair, andS.Levine. Offlinereinforcementlearningwithimplicitq-learning.
arXivpreprintarXiv:2110.06169,2021.
[18] K.Jiang,J.-y.Yao,andX.Tan. Recoveringfromout-of-samplestatesviainversedynamicsin
offlinereinforcementlearning. AdvancesinNeuralInformationProcessingSystems,36,2024.
[19] H. Zhang, J. Shao, Y. Jiang, S. He, G. Zhang, and X. Ji. State deviation correction for of-
flinereinforcementlearning. InProceedingsoftheAAAIconferenceonartificialintelligence,
volume36,pages9022–9030,2022.
[20] S.FujimotoandS.S.Gu. Aminimalistapproachtoofflinereinforcementlearning. Advances
inneuralinformationprocessingsystems,34:20132–20145,2021.
[21] L.Ankile,A.Simeonov,I.Shenfeld,M.Torne,andP.Agrawal. Fromimitationtorefinement–
residualrlforprecisevisualassembly. arXivpreprintarXiv:2407.16677,2024.
[22] C.M.Bishop. Mixturedensitynetworks. 1994.
[23] Y. Chen, C. Wang, Y. Yang, and K. Liu. Object-centric dexterous manipulation from hu-
man motion data. In 8th Annual Conference on Robot Learning, 2024. URL https:
//openreview.net/forum?id=KAzku0Uyh1.
[24] J. Gao, Z. Tao, N. Jaquier, and T. Asfour. K-vil: Keypoints-based visual imitation learning.
IEEETransactionsonRobotics,2023.
[25] P. Mandikal and K. Grauman. Learning dexterous grasping with object-centric visual affor-
dances. In 2021 IEEE international conference on robotics and automation (ICRA), pages
6169–6176.IEEE,2021.
[26] C.Devin,P.Abbeel,T.Darrell,andS.Levine. Deepobject-centricrepresentationsforgener-
alizablerobotlearning. In2018IEEEInternationalConferenceonRoboticsandAutomation
(ICRA),pages7111–7118.IEEE,2018.
[27] B.Wen,W.Yang,J.Kautz,andS.Birchfield. Foundationpose:Unified6dposeestimationand
tracking of novel objects. In Proceedings of the IEEE/CVF Conference on Computer Vision
andPatternRecognition,pages17868–17879,2024.
[28] C.Wen,X.Lin,J.So,K.Chen,Q.Dou,Y.Gao,andP.Abbeel. Any-pointtrajectorymodeling
forpolicylearning. arXivpreprintarXiv:2401.00025,2023.
[29] M.Xu,Z.Xu,Y.Xu,C.Chi,G.Wetzstein,M.Veloso,andS.Song. Flowasthecross-domain
manipulationinterface. arXivpreprintarXiv:2407.15208,2024.
[30] Y.Dai, J.Lee, N.Fazeli, andJ.Chai. Racer: Richlanguage-guidedfailurerecoverypolicies
forimitationlearning. arXivpreprintarXiv:2409.14674,2024.
[31] C. Cornelio and M. Diab. Recover: A neuro-symbolic framework for failure detection and
recovery. arXivpreprintarXiv:2404.00756,2024.
[32] P.Li, Z.Li, H.Zhang, andJ.Bian. Onthegeneralizationpropertiesofdiffusionmodels. In
Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https:
//openreview.net/forum?id=hCUG1MCFk5.
[33] D.Hendrycks,N.Mu,E.D.Cubuk,B.Zoph,J.Gilmer,andB.Lakshminarayanan. AugMix:
A simple data processing method to improve robustness and uncertainty. Proceedings of the
InternationalConferenceonLearningRepresentations(ICLR),2020.
10[34] M. Federici, R. Tomioka, and P. Forre´. An information-theoretic approach to distribution
shifts. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in
Neural Information Processing Systems, 2021. URL https://openreview.net/forum?
id=GrZmKDYCp6H.
[35] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra. Planning and acting in partially ob-
servable stochastic domains. Artificial Intelligence, 101(1):99–134, 1998. ISSN 0004-3702.
doi:https://doi.org/10.1016/S0004-3702(98)00023-X.URLhttps://www.sciencedirect.
com/science/article/pii/S000437029800023X.
[36] C. M. Bishop and N. M. Nasrabadi. Pattern recognition and machine learning, volume 4.
Springer,2006.
[37] Y.Hai,R.Song,J.Li,andY.Hu.Shape-constraintrecurrentflowfor6dobjectposeestimation.
In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages
4831–4840,2023. doi:10.1109/CVPR52729.2023.00468.
[38] P.Ausserlechner,D.Haberger,S.Thalhammer,J.-B.Weibel,andM.Vincze. Zs6d: Zero-shot
6dobjectposeestimationusingvisiontransformers. In2024IEEEInternationalConference
on Robotics and Automation (ICRA), pages 463–469, 2024. doi:10.1109/ICRA57147.2024.
10611464.
[39] P. R. Florence, C. Lynch, A. Zeng, O. Ramirez, A. Wahid, L. Downs, A. S. Wong, J. Lee,
I.Mordatch,andJ.Tompson. Implicitbehavioralcloning. ArXiv,abs/2109.00137,2021. URL
https://api.semanticscholar.org/CorpusID:237346088.
[40] A. Mandlekar, D. Xu, J. Wong, S. Nasiriany, C. Wang, R. Kulkarni, L. Fei-Fei, S. Savarese,
Y. Zhu, and R. Mart´ın-Mart´ın. What matters in learning from offline human demonstrations
forrobotmanipulation. InarXivpreprintarXiv:2108.03298,2021.
[41] Y. Lin, A. S. Wang, G. Sutanto, A. Rai, and F. Meier. Polymetis. https://
facebookresearch.github.io/fairo/polymetis/,2021.
[42] T.Ren,S.Liu,A.Zeng,J.Lin,K.Li,H.Cao,J.Chen,X.Huang,Y.Chen,F.Yan,Z.Zeng,
H. Zhang, F. Li, J. Yang, H. Li, Q. Jiang, and L. Zhang. Grounded sam: Assembling open-
worldmodelsfordiversevisualtasks,2024.
11A FinalJointPolicyAlgorithm
Algorithm1JointPolicyAlgorithm
1: Initializeπ b,π inv,GMMswithparametersΘ
2: whileTasknotdonedo
3: Collectobservationo t,proprioceptionp t. ComputeobjectposeT obj,t,keypointsϱ t.
4: Evaluatemeankeypointrecoveryvectorδ recandmeankeypointdensityη rec.
5: ifη rec <ϵ recthen
6: ComputekeypointrecoverytrajectoryζL
rec
7: ComputerecoveryactiontrajectoryaL
out
=π inv(ζ rL ec,p t)
8: else
9: ComputebaseactiontrajectoryaL
out
=π b(o t,p t)
10: endif
11: forainaL do
out
12: Executeactiona
13: endfor
14: endwhile
12