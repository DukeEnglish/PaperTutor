Causal Responsibility Attribution for Human-AI Collaboration
YahangQi YAHAQI@ETHZ.CH
ETHZu¨rich
BernhardScho¨lkopf BS@TUEBINGEN.MPG.DE
MPITu¨bingen
ZhijingJin ZJIN@CS.TORONTO.EDU
UniversityofToronto
Abstract
AsArtificialIntelligence(AI)systemsincreasinglyinfluencedecision-makingacrossvariousfields,
theneedtoattributeresponsibilityforundesirableoutcomeshasbecomeessential,thoughcompli-
cated by the complex interplay between humans and AI. Existing attribution methods based on
actualcausalityandShapleyvaluestendtodisproportionatelyblameagentswhocontributemore
toanoutcomeandrelyonreal-worldmeasuresofblameworthinessthatmaymisalignwithrespon-
sibleAIstandards.ThispaperpresentsacausalframeworkusingStructuralCausalModels(SCMs)
tosystematicallyattributeresponsibilityinhuman-AIsystems,measuringoverallblameworthiness
while employing counterfactual reasoning to account for agents’ expected epistemic levels. Two
casestudiesillustratetheframework’sadaptabilityindiversehuman-AIcollaborationscenarios.1
Keywords:ResponsibilityAttribution,CausalInference,Human-AICollaboration,Decision-Making
1. Introduction
As Artificial Intelligence (AI) systems increasingly influence decision-making in critical sectors
suchashealthcare(Buddetal.,2021),finance(Cohenetal.,2023),andautonomousdriving(Badue
et al., 2021), the need to clearly define and attribute responsibility when outcomes are undesirable
orfailuresoccurbecomescrucial(SantonideSioandVandenHoven,2018). TheintegrationofAI
complicatestraditionalaccountabilitymechanismsduetotheshareddecision-makingresponsibili-
tiesbetweenhumansandalgorithms. Human-AIsystemsposeuniquechallengesforresponsibility
attribution, giventheirinteractive, complex,andhigh-stakesnature(Amershietal.,2019). Onone
hand,theabilityofhumanstooverrideormodifyAI-drivendecisionsintroducesanelementofun-
predictabilityinoutcomes. Ontheotherhand,AIdecisionsoftenrelyonhugedatasetsorcomplex
modelsthatlackfulltransparency,makingitdifficultforhumancounterpartstofullyunderstandor
anticipateAIbehaviour.
Toaddressthesechallenges, variousmethodshavebeenproposedforresponsibilityattribution
in multi-agent settings. Chockler and Halpern (2004) and Halpern and Kleiman-Weiner (2018)
introduced frameworks to quantify blameworthiness based on causal relationships. This definition
was further extended to multi-agent settings by attributing blameworthiness through the Shapley
value,asdemonstratedbyFriedenbergandHalpern(2019). Additionally,responsibilityattribution
hasbeenstudiedindecentralized,partiallyobservableMarkovdecisionprocessesusingtheconcept
ofactualcausality(Triantafyllouetal.,2022). Theseapproachesrelyonactualcausality(Halpern,
2016)tomeasurethedegreeofblameworthinessamongagents.
1.Ourcodeanddataareathttps://github.com/yahang-qi/Causal-Attr-Human-AI.git.
Preprint. Underreview. 1
4202
voN
5
]IA.sc[
1v57230.1142:viXraQISCHO¨LKOPFJIN
However,inhuman-AIcollaboration,usingactualcausalityandtheShapleyvaluepresentslim-
itations. One issue is that agents who contribute more to a task are assigned a higher degree of
blame,whichmaynotalignwithintuitivenotionsofresponsibilityincollaborativesettings(Kumar
etal.,2020). Furthermore,theseframeworksdefineblameworthinessbasedonthereal-worldprob-
abilitymeasure. FortrustworthyandreliableAI,itisessentialtomeasureblameworthinessagainst
the epistemic standards that AI systems should uphold, rather than solely on actual causal effects
(BostromandYudkowsky,2018).
ThispaperpresentsacausalframeworkbasedonStructuralCausalModels(SCM;Pearl,2009)
for systematically attributing responsibility in human-AI systems. The framework is designed to
be adaptable across diverse human-AI collaboration scenarios, ensuring broad applicability and
relevance. First, the paper defines a measure of blameworthiness for different actions and their as-
sociated outcomes, building on the foundations established in Halpern’s framework (Halpern and
Kleiman-Weiner, 2018). Using counterfactual reasoning, the framework then attributes responsi-
bility for specific undesirable outcomes to the relevant parties, taking into account the epistemic
leveleachpartyshouldpossess. Finally,twocasestudiesareprovidedtodemonstratethepractical
applicationofthisresponsibilityattributionframework.
2. RelatedWork
2.1. ResponsibilityAttribution
The need for judging moral responsibility arises both in ethics and in law. However, these notions
are notoriously difficult to define carefully. One famous example is the trolley-problem (Thom-
son, 1984), which is a moral dilemma in which one must decide whether to pull a lever to divert
a runaway trolley onto a track where it will kill one person, rather than allowing it to continue
on its current track, where it will kill five. To formally define moral responsibility, there is gen-
eralagreementthatadefinitionofmoralresponsibilitywillrequireintegratingcausality(Chockler
and Halpern, 2004), intention (Cushman, 2015), knowledge (Malle et al., 2014). To formally de-
fine these, structural causal models provide a powerful tool (Halpern and Pearl, 2005). Using this
framework, Halpern and Kleiman-Weiner (2018) defined the degree of blameworthiness in terms
of actual causality. Furthermore, the degree of blameworthiness can be discounted by the cost of
the action. This notation of blameworthiness is further generalised into multi-agent setting by as-
cribingblameworthinesstothegroupofagentsrelativetoanepistemicstateofpotentialoutcomes
(Friedenberg and Halpern, 2019). The degree of blameworthiness for individuals can be attributed
using the Shapley value, a notion from cooperative game theory (Shapley, 1953). However, when
usingShapleyvalueforresponsibilityattribution,agentswhichcontributesmoretothetaskmayend
upbeingblamedmore,whichmaynotalignwithintuitivenotionsofresponsibilityincollaborative
settings(Kumaretal.,2020).
In the regime of AI, Franklin et al. (2022) outlined a causal framework of responsibility attri-
butionwhichintegratesninefactors: causality,role,knowledge,objectiveforeseeability,capability,
intent, desire, autonomy, and character. The causal relationship between these nice factors and the
responsibilityisextensivelydiscussed. However,noexplicitmathematicalformulationisgiven. Re-
gardingmulti-agentsetting,Triantafyllouetal.(2022)proposedawaytoattributetheresponsibility
indecentralizedpartiallyobservableMarkovdecisionprocesses. However,theseframeworksdonot
accountforagents’expectedepistemiclevels,whichisnothelpfultobuildreliableandtrustworthy
AIsystems(BostromandYudkowsky,2018).
2CAUSALRESPONSIBILITYATTRIBUTIONFORHUMAN-AICOLLABORATION
2.2. Human-AICollaboration
The integration of AI in critical fields like healthcare (Budd et al., 2021), finance (Cohen et al.,
2023), and autonomous driving (Badue et al., 2021) has driven the development of structured
human-AIcollaborationframeworks(Wangetal.,2020). Theseframeworksgenerallyfitintothree
modes,whichdefinethebalanceofhumanandAIroles: AI-centric,Human-centric,andSymbiotic
(Fragiadakisetal.,2024).
IntheAI-centricmode,AIsystemstakethelead,performingtaskswithminimalhumaninput.
This mode focuses on maximizing computational efficiency, where AI operates autonomously in
contextslikeagenticsystems(Shavitetal.,2023)orcomplexdatapredictions,suchasproteinstruc-
tures (Jumper et al., 2021). Human-centric frameworks, often called human-in-the-loop, maintain
human oversight as central, employing AI as an auxiliary tool to manage data-heavy or repetitive
tasks. This mode supports fields where human judgment is crucial, like diagnostics in healthcare
(Chaddad et al., 2023) and assisted driving (Badue et al., 2021). The Symbiotic mode promotes
a balanced partnership, with human and AI systems sharing decision-making and complement-
ing each other’s strengths. This setup enables mutual feedback and close collaboration, ideal for
tasksrequiringbothAI’scomputationalpowerandhumanintuition,suchascreativeco-production
(RezwanaandMaher,2023).
As these systems become more prevalent, they underscore the need for responsibility attribu-
tion frameworks that address the complexity of human-AI interactions and evaluate responsibility
against the epistemic standards expected from trustworthy AI systems.epistemic standards neces-
saryforreliableandtrustworthyAIsystems..
3. Preliminaries
This section introduces the fundamental concepts and mathematical tools utilised in our study,
specificallyfocusingonStructuralCausalModels(SCMs),interventionalcounterfactual,andback-
trackingcounterfactual.
3.1. StructuralCausalModels(SCMs)
SCMs (Pearl, 2009) offer a structured framework for modelling and analysing causal relationships
between variables. We introduce SCMs below, using notation adapted from Bongers et al. (2021)
andPetersetal.(2017).
Definition1(StructuralCausalModel(Bongersetal.,2021)) A structural causal model (SCM)
isatupleM = ⟨I,J,X,E,f,P(E)⟩,where
• I andJ aredisjointfiniteindexsetofendogenousandexogenousvariables,respectively.
(cid:81) (cid:81)
• ThedomainsX = X andE = E areproductsofstandardBorelspace.
i∈I i j∈J i
• TheexogenousdistributionP(E) = (cid:78) P(E )istheproductofprobabilitydistributions.
j∈J j
• Thecausalmechanismf : X ×E → X isameasurablefunction.
ThesolutionsofanSCMintermsofrandomvariablesaredefineduptoalmostsureequality.
3QISCHO¨LKOPFJIN
Definition2(SolutiontoanSCM(Bongersetal.,2021)) Apair(X,E)ofrandomvariablesX :
Ω → X,E : Ω → E,whereΩisaprobabilityspace,isasolutionoftheSCMM = ⟨I,J,X,E,f,P(E)⟩
if
• PE = P ,thatis,thedistributionofEisequaltoP ,and
E E
• thestructuralequationsaresatisfied,thatis,
X = f(X,E)a.s.. (3.1)
Forconvenience,wecallarandomvariableXasolutionofMifthereexistsarandomvariableE
suchthat(X,E)formsasolutionofM.
In this paper, we restrict our attention to acyclic SCMs, where there is a total ordering of the en-
dogenous variables. In this setting, given a setting e for the exogenous variables, there is a unique
solutionx forallthestructuralequations.
e
3.2. Counterfactual
Counterfactualisusedtoreasonaboutwhatwouldhappentoasystemifweweretointerveneand
changesomeofitsvariables(Petersetal.,2017). InthecontextofanSCM,aninterventionthatsets
avariableX toxisdenotedasdo(X = x). TheeffectofthisinterventiononanothervariableY is
computedbymodifyingthestructuralequationsintheSCMandpropagatingtheeffects.
Definition3(Counterfactuals) ConsideranSCMM = ⟨I,J,X,E,f,P(E)⟩anditssolutionX.
Givensomeobservationsx ∈ X,wedefineacounterfactualSCMM′ = ⟨I,J,X,E,f,P(E|X =
x)⟩byreplacingthedistributionofnoisevariableswiththedistributionconditionedonobservation
X = x,denotedasP(E|X = x).
4. ACausalFrameworkofResponsibilityAttribution
This section presents the formal definitions and mathematical formulations for undesirable out-
comesandthedegreeofblameworthinessofanactionrelativetoanalternativeaction,bothofwhich
arefundamentaltoourcausalresponsibilityattributionframework. Webeginbydefiningthesekey
terms, followed by a formal definition of the overall degree of blameworthiness in a human-AI
decision-making system. Finally, we focus on specific outcomes and demonstrate how to attribute
blameworthiness to different parties based on their epistemic levels, aligned with responsible AI
standardsandrepresentedbyaprobabilitymeasureoverpotentialoutcomes..
4.1. DegreeofBlameworthiness
ConsiderasystemwhosecausalmechanismisdefinedbytheSCMM = ⟨I,J,X,E,f,P(E)⟩. In
this system, an agent takes action and may affect the probability of occurrence of some unwanted
outcomeϕ,whichisdefinedas
Definition4(OutcomeinanSCM) ConsideranSCMM = ⟨I,J,X,E,f,P(E)⟩withasolution
X. An outcome ϕ is defined as a measurable function ϕ : X → {0,1}, where ϕ = 1 if and only
if X takes a specified value or falls within a designated subset of X. That is, the outcome ϕ = 1
4CAUSALRESPONSIBILITYATTRIBUTIONFORHUMAN-AICOLLABORATION
Figure1: SCMofanDecision-MakingSystem.
”happens”whenXsatisfiescertainconditionsandϕ = 0otherwise. Itsprobabilityisthendefined
inthefollowing:
P(ϕ = 1) = P({e ∈ E : ϕ(x = 1)}), (4.1)
e
wherex isthesolutionundercausalsettinge.
e
Indecision-makingsystems,anunwantedoutcomecouldbefailureinmakingcorrectdecision.
As shown in Figure 1, a decision Y made from available information X, with f being decision-
makingpolicy. TheoptimaldecisionY∗ ismadeusingoptimalpolicyf∗. Anactionamodifiesthe
decision-making system M by modifying its decision-making policy, denoted as fa. Such action
may or may not increase the probability of unwanted outcome ϕ, when compared with another
actiona′.
Definition5(Blameworthiness) Consider a decision-making system represented by SCM M =
⟨I,J,X,E,f,P(E)⟩withasolutionX,consideractionaandreferenceactiona′,actionaissaid
tobeblameworthyforoutcomeϕwhencomparedwiththereferenceactiona′ whenitincreasesthe
probabilityofϕ:
(cid:110) (cid:111)
δ(a,a′) = max 0, P(ϕ = 1|Ma)−P(ϕ = 1|Ma′ ) , (4.2)
whereMa = ⟨I,J,X,E,fa,P(E)⟩andMa′ = ⟨I,J,X,E,fa′,P(E)⟩aredecision-makingsys-
temmodifiedviaactionaanda′,respectively.
Wecandeterminetheextenttowhichagentperformingactionaaffectstheprobabilityofoutcome
ϕ. This effect is compared with another possible action a′, which is different from a. However, in
real-world scenarios, a decision-making system is often modified by an action a because this can
reducethecostorimprovetheefficiencyofthesystem. WeuserandomvariableC tomeasurethe
costofadecisionC : X → [0,∞). Asaresult,thedegreeofblameworthinessisdiscountedbythe
improvementinefficiency.
Definition6(DiscountedBlameworthiness) Consider a decision-making system represented by
SCMM = ⟨I,J,X,E,f,P(E)⟩modifiedbyactionaanda′. Thediscounteddegreeofblamewor-
thinessisdefinedby
DB(a,a′) = γ(a,a′)δ(a,a′), (4.3)
where γ(a,a′) = τ(E[C|Ma],E[C|Ma′]) is the discount factor, E[C|Ma] and E[C|Ma′] are the
expectedcostofmakingadecisionindecision-makingsystemMmodifiedbyaanda′,respectively,
τ : (0,∞)×(0,∞) → (0,1]computesthediscountfactorbasedontheexpectedcostofmakinga
decision.
5QISCHO¨LKOPFJIN
model confident
final
decision
information model human
in unsure review
Figure2: Human-in-the-loop(HITL)decision-makingsystem.
4.2. Human-in-the-LoopDecision-MakingSystems
Thehuman-in-the-loopdecision-makingsystem,illustratedbyFigure2,canbeunderstoodthrough
a causal perspective, where both an AI model and human intervention shape the final decision.
The process starts with an input variable, X, representing the data fed into the system. This data
influences the AI model’s output, M, which includes a confidence measure. When the model is
confident,itsdecisionflowsdirectlytothefinaldecisionoutput,Y,bypassinghumanreview. How-
ever, if the model is uncertain, it passes the decision to a human reviewer, represented by H, who
considersboththeinitialdataX andthemodel’soutputM beforemakingajudgment. Thefinalde-
cision,Y,isthuscausallydeterminedbyeithertheAImodelalone(ifconfident)orbythecombined
influenceofthemodelandhumaninput(ifthemodelisunsure).
In this framework, two main causal pathways affect the final decision: a direct path where the
model’s confident output directly influences Y, and a human-intervention path where the human
review shapes Y when the model is uncertain. In a decision system M = ⟨I,J,X,E,f,P(E)⟩.
Thedecision-makingpolicyf canbemodifiedbyactions,whichincludedeployinghuman-in-the-
loop policy, denoted as a, and deploying human-only policy, denoted as a′. Deploying human-in-
the-loop policy can increase the efficiency of the decision-making system, at the price of reducing
performance. The undesired outcome in such system can be making a wrong decision, denoted as
ϕ. UsingDefinition6,thedegreeofblameworthinesscanbemeasureby
DB(a,a′) = γ(a,a′)δ(a,a′), (4.4)
(cid:110) (cid:111)
whereδ(a,a′) = max 0, P(ϕ = 1|Ma)−P(ϕ = 1|Ma′) . Here,δ(a,a′)measurestheextentof
deployingHITLpolicyfa willincreasetheprobabilityofmakingwrongdecisions,comparedwith
usinghuman-onlypolicyfa′
.
4.3. AttributionofResponsibilityforaSpecificUndesiredOutcome
Definition 4.4 provides a measure of the overall degree of blameworthiness for an undesired out-
come. However, we also aim to attribute responsibility for specific undesired outcomes. For a
givenoutcomeϕ ,responsibilityisattributedbyassessingwhetheritcouldhavebeenavoidedifan
0
agenthadtakenanalternativeaction. Inahuman-AIcollaborationsetting,thisanalysisisnotbased
on real-world probability measures but on the epistemic level the AI agent is expected to main-
tain to align with responsible AI standards, represented through an adjusted probability measure.
In human-only decision systems, responsibility attribution is straightforward as only one agent is
involved. Thus, our focus here is on analysing decision-making systems under the HITL policy,
whereresponsibilityattributionbecomesmorecomplex.
6CAUSALRESPONSIBILITYATTRIBUTIONFORHUMAN-AICOLLABORATION
Definition7(Inevitableandavoidableoutcomes) Consideradecision-makingsystemrepresented
bytheSCMM = ⟨I,J,X,E,f,P(E)⟩, wherethedecision-makingpolicyismodifiedbyactiona
under the HITL policy and by action a′ under the human-only policy. For a specific undesirable
outcomeϕ = 1,itcanbecategorizedasfollows:
0
• Inevitable outcomes: The event ϕ = 1 also occurs in the human-only decision-making
0
systemMa′.
• Avoidableoutcomes: Theeventϕ = 1wouldnotoccurinthehuman-onlydecision-making
0
systemMa′.
Furthermore,intheHITLdecision-makingsystem,theinevitableoutcomescanbecategorizedas
• Flaggedoutcomes: eventψ = 1,and
0
• Unflaggedoutcomes: eventψ = 0,
0
whereϕ is1whentheAIhasrequestedinterventionfromhumanand0ifnot.
0
Inthisdefinition,inevitableoutcomesarethosebeyondthedecision-makingcapabilitiesofboth
humanandAIagents,whileavoidableoutcomesareundesiredoutcomesthatcouldhavebeenpre-
ventedhadtheAIrequestedhumanintervention. Inevitableoutcomescanbefurthercategorizedas
flagged or unflagged, a distinction that may initially seem redundant, as counterfactual reasoning
implies the outcome remains unchanged regardless of whether the AI flagged the case. However,
this classification is crucial for accurately identifying responsible parties and attributing account-
ability,aligningwithresponsibleAIstandards.
Definition8(Responsibilityattributionfordifferenttypesofoutcomes) Consideradecision-
makingsystemrepresentedbySCMM = ⟨I,J,X,E,f,P(E)⟩,itsdecision-makingpolicyismod-
ified by action a under the HITL policy, and is modified by action a′ under the human-only policy.
Foraspecificundesiredoutcomeϕ = 1,responsibilityisattributedasfollows:
0
• Inevitableoutcomes:
– Flagged: Responsibilitylieswiththehumanwhomadethedecision.
– Unflagged: ResponsibilityisattributedtoboththeAImakingthedecisionandtheparty
responsiblefordesigningtheflaggingmechanism.
• Avoidableoutcomes: ResponsibilityisassignedtoboththeAIthatmadethedecisionandthe
partywhodesignedtheflaggingmechanism.
For avoidable outcomes, responsibility is shared between the AI that made the decision and
the party who designed the flagging mechanism, as these outcomes could have been prevented if
the AI had flagged the case or if the flagging mechanism had been more effective. For inevitable
outcomes,thehumanultimatelybearsresponsibility,asthesystem’slimitationsareconstrainedby
humancapabilities. However,itmayseemcounterintuitivefortheAItoalsobearresponsibilityfor
unflagged inevitable outcomes, since the act of flagging by the AI has no causal effect on the final
outcome. Thisresponsibilityisnotmeasuredbythereal-worldprobabilitymeasureP(E),butrather
by a probability measure aligned with responsibleAI standards, which assumes that the AI should
always consider the potential benefit of requesting human intervention to reduce the likelihood of
undesirableoutcomes.
7QISCHO¨LKOPFJIN
5. CaseStudy1: EssayGradingwithLargeLanguageModels
5.1. Background
The recent advancements in Large Language Models (LLMs) have enabled their deployment in
a variety of educational applications, including automated essay grading (Mizumoto and Eguchi,
2023). LLMstrainedonextensivetextcorporademonstratearemarkableabilitytoevaluatewritten
content, oftenproducingassessmentscomparabletothoseofhumangraders. However, challenges
arisewhenthesemodelsencounteratypicalornuancedwritingstyles,culturalreferences,orspecific
linguistic constructs outside of their training distribution, which may lead to inaccurate grading or
biasedevaluations(Taoetal.,2024).
Integrating LLMs into essay grading systems offers significant potential for efficiency, espe-
ciallyinhandlinglargevolumesofstudentessays. Nevertheless,suchintegrationrisksintroducing
variability in grading quality, particularly in cases where LLMs lack sufficient understanding of
complex or context-dependent language use. In this case study, we demonstrate how to assess the
degreeofblameworthinessusingtheframeworkproposedinSection4.
5.2. Implementation
In this case study, we evaluate a human-AI collaboration system for essay grading, in which hu-
man and AI agents work together to score student submissions. We utilize the ASAP-AES dataset
(Hamner et al., 2012), specifically focusing on Set-1, which includes persuasive, narrative, or ex-
positoryessayswrittenby8th-gradestudentsonthesocietalimpactsofcomputeruse. Inresponse
to a prompt, students write a persuasive letter to a local newspaper, arguing whether computers
benefit or harm society, with the goal of convincing readers. Essays are graded on a rubric from
1 to 6, based on criteria such as content development, organization, detail, fluency, and audience
awareness. Eachessayreceivestwoindependentscores,andwhenthesescoresareclose(adjacent),
they are summed to determine the final score. For significantly different scores (non-adjacent), an
expertadjudicatorprovidesaresolvedscore.
To test the human-AI collaboration framework, we replaced one human assessor with GPT-4
(Achiam et al., 2023). The model was provided with the grading rubric and prompted to assign a
score based on the essay content. The final grade was calculated by summing GPT-4’s score with
thatofthehumanassessor,asillustratedinFigure3.
human assessor
essay final grade
AI assessor
Figure3: Human-AIcollaborationsystemforessaygrading.
8CAUSALRESPONSIBILITYATTRIBUTIONFORHUMAN-AICOLLABORATION
5.3. Results
Using the framework from Section 4, we assess the degree of blameworthiness in the human-AI
collaborativegradingsystembycomparingtheAI-enhancedscoreswithgroundtruthscoresthrough
the quadratic weighted kappa (QWK) metric (Cohen, 1968). QWK is a widely used measure of
agreementbetweenratersineducationalassessment,particularlysuitedforordinalscoringtaskslike
essaygrading. Thismetricpenalizeslargerdiscrepanciesbetweenscoresmoreheavily,providinga
nuancedviewofratingconsistency. QWKvaluesrangefrom-1(indicatingcompletedisagreement)
to 1 (perfect agreement), with 0 implying no agreement beyond chance. The human-AI grading
system achieved a QWK of 0.478, yielding an overall degree of blameworthiness of 1−0.478 =
0.522.
ThisanalysishighlightsthatwhileintegratingLLMsinhuman-AIgradingsystemscanimprove
efficiency,italsorisksintroducinggradinginconsistencies. Bysystematicallyattributingresponsi-
bility,ourframeworkprovesvaluableinassessingthesesystems’reliabilityandaccountability.
6. CaseStudy2: Human-in-the-LoopPneumoniaDetectionUsingChestX-ray
Images
6.1. Background
RecentadvancesinAIhavesignificantlyenhancedautomatedmedicaldiagnosissystems,achieving
performance levels increasingly comparable to human clinicians (Zhou et al., 2023; Komorowski
et al., 2018). However, these models struggle with out-of-distribution cases, where limitations in
generalizationcanleadtodiagnosticerrors,especiallyonsamplesoutsidethemodel’strainingdata.
IntegratingAIintomedicaldiagnosticsoffersefficiencygainsthroughrapidprocessingoflarge
datasetsbutmayintroducenewrisks,potentiallyimpactingoverallsystemperformance. Toaddress
theserisks,quantifyingresponsibilityforerrorsbetweenhumanandAIagentsbecomescrucial. In
thiscasestudy,weillustratehowtoassessblameworthinessforundesirableoutcomes. Usingachest
X-raydatasetof5,863imageslabelledasPneumoniaorNormal(Kermanyetal.,2018),wedeploy
ahuman-in-the-loopdiagnosticsystemtodetectpneumoniabasedonchestX-rays
6.2. Implementation
Inthiscasestudy,weexamineahuman-in-the-loop(HITL)diagnosticsystemfordetectingpneumo-
nia from chest X-ray images. For reproducibility and to simulate different diagnostic capabilities,
weusetwodistinctAImodelstorepresenttheroleswithinthesystem: astrongermodel,ResNet-18
(He et al., 2016), mimicking the diagnostic capabilities of a human doctor, and a weaker model, a
standardConvolutionalNeuralNetwork(CNN;Fukushima,1980),representingtheAIcomponent
ofthesystem.
As shown in Figure 4, the HITL system operates as follows: each chest X-ray image is first
processedbytheweakerAImodel,i.e.,CNNinthiscase,whichassessesitsconfidenceinmakinga
diagnosis,whichismeasuredbytheprobabilitypofdetectingpneumoniaandpre-definedthreshold
landu,where0 ≤ l < u ≤ 1. Ifthemodelisconfidentinitsprediction(p > uorp < l),itproceeds
to make a decision autonomously. However, if the model detects high uncertainty (l ≤ p ≤ u), it
flagsthecaseandrequestshumanintervention,simulatingacollaborativedecision-makingprocess.
This setup allows us to investigate cases where the AI’s autonomy may lead to errors, as well as
scenarioswherehumanoversightisinvolvedinthedecision.
9QISCHO¨LKOPFJIN
model confident
final
diagnosis
image model doctor
in unsure review
Figure4: Human-in-the-loop(HITL)diagnosticsystemfordetectingpneumoniafromchestX-ray
images.
UsingtheframeworkfromSection4, wequantifythedegreeofblameworthinessfortheHITL
system by comparing the weaker AI model’s decisions with a hypothetical human-only system,
representedbyResNet-18.
6.3. Results
ApplyingtheframeworktotheHITLpneumoniadiagnosticsystem,wemeasuretheoveralldegree
of blameworthiness and count for each category of undesired outcomes. The performance of the
diagnosis system is measure using F1-score, which is a metric in classification tasks to evaluate a
model’s accuracy, especially in cases where the data is imbalanced. It is defined as the harmonic
meanofprecisionandrecall:
Precision·Recall
F1-score = 2· , (6.1)
Precision+Recall
where
TP TP
Precision = , Recall = . (6.2)
TP+FP TP+FN
Following the deployment of a human-in-the-loop (HITL) policy, the diagnostic system’s F1-
scoredecreasedfrom0.896to0.831,withthisdropindicatingthedegreeofblameworthinessasso-
ciatedwithimplementingtheHITLsystem.
For inevitable outcomes, errors persist despite human oversight, revealing cases where both
human and AI agents struggle with challenging, out-of-distribution data. Specifically, 2 inevitable
errors were flagged by the AI for human intervention, while 83 inevitable errors were not flagged,
suggesting overconfidence in the AI’s predictions. A well-calibrated AI model and an improved
flaggingmechanismcouldreduceresponsibilityforinevitablecasesthatarenotflagged.
Amongthe73avoidableerrors,responsibilityfallsprimarilyontheAIandthoseresponsiblefor
designingtheflaggingmechanism,astheseerrorscouldhavebeenpreventediftheAIhadflagged
them for human intervention. Here, the human is not blameworthy, since these errors were due to
theAI’sfailuretorequestintervention.
Overall,thisanalysisrevealsthat,whiletheHITLapproachcanimproveefficiency,itintroduces
risks when the AI is overconfident in its predictions and the flagging mechanism lacks precision.
Bypreciselydefiningresponsibilityforspecificoutcomes,thisstudydemonstratestheutilityofour
frameworkinevaluatingaccountabilityforbothAIandhumanagentswithinHITLdiagnosticsys-
10CAUSALRESPONSIBILITYATTRIBUTIONFORHUMAN-AICOLLABORATION
tems. Thesefindingsunderscoretheneedforfurtherrefinementsinmodelcalibrationandflagging
mechanismstooptimizehuman-AIcollaborationinhigh-stakesapplications.
7. Conclusion
This paper presented a structured causal responsibility attribution framework designed for human-
AIcollaborationdecision-makingsystems,whereAImodelsandhumanagentscollaboratetomake
critical decisions. As AI integration continues to expand in sectors such as healthcare and educa-
tion,theneedforpreciseresponsibilityattributionbecomesincreasinglyimportant,particularlyfor
undesiredoutcomesthatcanimpacttrust,accountability,andethicalAIdeployment.
The formalization of outcomes and blameworthiness in our framework provides a rigorous ap-
proach to assess the impact of an agent’s actions on undesired outcomes, enabling a systematic
evaluation of responsibility. By defining an outcome as a measurable function within an SCM,
we quantify blameworthiness by comparing the probabilities of outcomes under different actions.
We also introduced a discounted blameworthiness measure, which adjusts for the efficiency im-
provements that an action might bring, reflecting a balanced view of accountability where both
performanceandcostareconsidered.
Our framework distinguishes between inevitable and avoidable outcomes, with avoidable out-
comes further divided into flagged and unflagged categories. This classification enables precise
responsibility attribution: inevitable outcomes are attributed to limitations inherent to both AI and
humanagents, while avoidableoutcomesspecifyresponsibilitybasedon whetherhumaninterven-
tion was flagged or bypassed. Specifically, responsibility for avoidable errors may lie with the AI,
the human agent, or the designers of the flagging mechanism, depending on whether intervention
was requested. This approach addresses key issues in existing methods, such as Shapley values,
which tend to disproportionately blame agents contributing more to an outcome and rely on real-
worldmeasuresofblameworthinessthatmaynotalignwithresponsibleAIstandards.
Using this approach, we demonstrated the framework’s applicability through two case studies:
ASAP-AES essay grading and pneumonia detection from chest X-ray images. These examples
illustrate how our framework systematically assesses causal responsibility for each agent within
HITLsystems,offeringinsightstoinformfutureimprovementsinAIdesignandhuman-AIcollab-
orationstrategies. However, thisstudydoesnotaddressmeasuringblameworthinessforsequential
decisionsthatalignwithresponsibleAIstandards,whichremainsadirectionforfutureresearch.
Our causal responsibility attribution framework represents a step forward in accountability for
human-AI collaboration systems, promoting transparent and ethical AI deployment in high-stakes
settings. By integrating causal reasoning with aligned with responsible AI standards, this work
pavesthewayforresponsiblehuman-AIcollaboration,layingafoundationfortrustinincreasingly
autonomoussystems.
References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man,DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technical
report. arXivpreprintarXiv:2303.08774,2023.
Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Col-
lisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, et al. Guidelines for human-ai
11QISCHO¨LKOPFJIN
interaction. In Proceedings of the 2019 chi conference on human factors in computing systems,
pages1–13,2019.
ClaudineBadue,RaˆnikGuidolini,RaphaelVivacquaCarneiro,PedroAzevedo,ViniciusBCardoso,
AvelinoForechi,LuanJesus,RodrigoBerriel,ThiagoMPaixao,FilipeMutz,etal. Self-driving
cars: Asurvey. Expertsystemswithapplications,165:113816,2021.
StephanBongers, PatrickForre´, JonasPeters, andJorisMMooij. Foundationsofstructuralcausal
modelswithcyclesandlatentvariables. TheAnnalsofStatistics,49(5):2885–2915,2021.
NickBostromandEliezerYudkowsky. Theethicsofartificialintelligence. InArtificialintelligence
safetyandsecurity,pages57–69.ChapmanandHall/CRC,2018.
SamuelBudd,EmmaCRobinson,andBernhardKainz. Asurveyonactivelearningandhuman-in-
the-loopdeeplearningformedicalimageanalysis. Medicalimageanalysis,71:102062,2021.
AhmadChaddad,JihaoPeng,JianXu,andAhmedBouridane. Surveyofexplainableaitechniques
inhealthcare. Sensors,23(2):634,2023.
Hana Chockler and Joseph Y Halpern. Responsibility and blame: A structural-model approach.
JournalofArtificialIntelligenceResearch,22:93–115,2004.
Jacob Cohen. Weighted kappa: Nominal scale agreement provision for scaled disagreement or
partialcredit. Psychologicalbulletin,70(4):213,1968.
SamuelNCohen,ChristophReisinger,andShengWang. Arbitrage-freeneural-sdemarketmodels.
AppliedMathematicalFinance,30(1):1–46,2023.
Fiery Cushman. Deconstructing intent to reconstruct morality. Current Opinion in Psychology, 6:
97–103,2015.
GeorgeFragiadakis,ChristosDiou,GeorgeKousiouris,andMaraNikolaidou. Evaluatinghuman-ai
collaboration: Areviewandmethodologicalframework. arXivpreprintarXiv:2407.19098,2024.
Matija Franklin, Hal Ashton, Edmond Awad, and David Lagnado. Causal framework of artificial
autonomous agent responsibility. In Proceedings of the 2022 AAAI/ACM Conference on AI,
Ethics,andSociety,pages276–284,2022.
MeirFriedenbergandJosephYHalpern. Blameworthinessinmulti-agentsettings. InProceedings
oftheAAAIConferenceonArtificialIntelligence,volume33,pages525–532,2019.
Kunihiko Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of
patternrecognitionunaffectedbyshiftinposition. Biologicalcybernetics,36(4):193–202,1980.
Joseph Halpern and Max Kleiman-Weiner. Towards formal definitions of blameworthiness, inten-
tion, and moral responsibility. In Proceedings of the AAAI conference on artificial intelligence,
volume32,2018.
JosephYHalpern. Actualcausality. MiTPress,2016.
12CAUSALRESPONSIBILITYATTRIBUTIONFORHUMAN-AICOLLABORATION
Joseph Y Halpern and Judea Pearl. Causes and explanations: A structural-model approach. part i:
Causes. TheBritishjournalforthephilosophyofscience,2005.
BenHamner,JaisonMorgan, lynnvandev, MarkShermis, andTomVanderArk. Thehewlettfoun-
dation: Automated essay scoring. https://kaggle.com/competitions/asap-aes,
2012. Kaggle.
KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimagerecog-
nition. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages
770–778,2016.
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,
KathrynTunyasuvunakool,RussBates,AugustinZˇ´ıdek,AnnaPotapenko,etal. Highlyaccurate
proteinstructurepredictionwithalphafold. nature,596(7873):583–589,2021.
Daniel Kermany, Kang Zhang, Michael Goldbaum, et al. Labeled optical coherence tomography
(oct)andchestx-rayimagesforclassification. Mendeleydata,2(2):651,2018.
Matthieu Komorowski, Leo A Celi, Omar Badawi, Anthony C Gordon, and A Aldo Faisal. The
artificial intelligence clinician learns optimal treatment strategies for sepsis in intensive care.
Naturemedicine,24(11):1716–1720,2018.
I Elizabeth Kumar, Suresh Venkatasubramanian, Carlos Scheidegger, and Sorelle Friedler. Prob-
lems with shapley-value-based explanations as feature importance measures. In International
conferenceonmachinelearning,pages5491–5500.PMLR,2020.
Bertram F Malle, Steve Guglielmo, and Andrew E Monroe. A theory of blame. Psychological
Inquiry,25(2):147–186,2014.
Atsushi Mizumoto and Masaki Eguchi. Exploring the potential of using an ai language model for
automatedessayscoring. ResearchMethodsinAppliedLinguistics,2(2):100050,2023.
JudeaPearl. Causality. Cambridgeuniversitypress,2009.
JonasPeters,DominikJanzing,andBernhardScho¨lkopf. Elementsofcausalinference: foundations
andlearningalgorithms. TheMITPress,2017.
Jeba Rezwana and Mary Lou Maher. Designing creative ai partners with cofi: A framework for
modeling interaction in human-ai co-creative systems. ACM Transactions on Computer-Human
Interaction,30(5):1–28,2023.
Filippo Santoni de Sio and Jeroen Van den Hoven. Meaningful human control over autonomous
systems: Aphilosophicalaccount. FrontiersinRoboticsandAI,5:15,2018.
LloydSShapley. Avalueforn-persongames. ContributiontotheTheoryofGames,2,1953.
Yonadav Shavit, Sandhini Agarwal, Miles Brundage, Steven Adler, Cullen O’Keefe, Rosie Camp-
bell, Teddy Lee, Pamela Mishkin, Tyna Eloundou, Alan Hickey, et al. Practices for governing
agenticaisystems. ResearchPaper,OpenAI,December,2023.
13QISCHO¨LKOPFJIN
YanTao,OlgaViberg,RyanSBaker,andRene´ FKizilcec. Culturalbiasandculturalalignmentof
largelanguagemodels. PNASnexus,3(9):pgae346,2024.
JudithJarvisThomson. Thetrolleyproblem. YaleLJ,94:1395,1984.
Stelios Triantafyllou, Adish Singla, and Goran Radanovic. Actual causality and responsibility at-
tribution in decentralized partially observable markov decision processes. In Proceedings of the
2022AAAI/ACMConferenceonAI,Ethics,andSociety,pages739–752,2022.
Dakuo Wang, Elizabeth Churchill, Pattie Maes, Xiangmin Fan, Ben Shneiderman, Yuanchun Shi,
andQianyingWang. Fromhuman-humancollaborationtohuman-aicollaboration: Designingai
systems that can work together with people. In Extended abstracts of the 2020 CHI conference
onhumanfactorsincomputingsystems,pages1–6,2020.
YukunZhou,MarkAChia,SiegfriedKWagner,MuratSAyhan,DominicJWilliamson,RobbertR
Struyven,TimingLiu,MouchengXu,MateoGLozano,PeterWoodward-Court,etal. Afounda-
tionmodelforgeneralizablediseasedetectionfromretinalimages. Nature,622(7981):156–163,
2023.
14