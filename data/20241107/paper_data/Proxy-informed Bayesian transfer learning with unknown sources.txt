Proxy-informed Bayesian transfer learning with unknown sources
Sabina J. Sloman∗1, Julien Martinelli2, and Samuel Kaski1,3
1Department of Computer Science, University of Manchester, Manchester, UK
2Inserm Bordeaux Population Health, Vaccine Research Institute, Universit´e de Bordeaux,
Inria Bordeaux Sud-ouest, France
3Department of Computer Science, Aalto University, Helsinki, Finland
November 6, 2024
Abstract
Generalizationoutsidethescopeofone’strainingdatarequiresleveragingpriorknowledgeaboutthe
effectsthattransfer,andtheeffectsthatdon’t,betweendifferentdatasources. Bayesiantransferlearning
isaprincipledparadigmforspecifyingthisknowledge,andrefiningitonthebasisofdatafromthesource
(training) and target (prediction) tasks. We address the challenging transfer learning setting where the
learner(i)cannotfine-tuneinthetargettask,and(ii)doesnotknowwhichsourcedatapointscorrespond
to the same task (i.e., the data sources are unknown). We propose a proxy-informed robust method for
probabilistictransferlearning(PROMPT),whichprovidesaposteriorpredictiveestimatetailoredtothe
structure of the target task, without requiring the learner have access to any outcome information from
the target task. Instead, PROMPT relies on the availability of proxy information. PROMPT uses the
same proxy information for two purposes: (i) estimation of effects specific to the target task, and (ii)
construction of a robust reweighting of the source data for estimation of effects that transfer between
tasks. We provide theoretical results on the effect of this reweighting on the risk of negative transfer,
and demonstrate application of PROMPT in two synthetic settings.
∗Correspondencetosabina.sloman@manchester.ac.uk.
1
4202
voN
5
]GL.sc[
1v36230.1142:viXra1 INTRODUCTION
The paradigm of transfer (multi-task) learning takes, often sparse, data from one or multiple source tasks
and uses it to predict outcomes in a different but related target task. Consider, for instance, trying to
predict the effectiveness of a treatment for a new patient on the basis of observational data. Inevitably, the
measured effects of the treatment in the source data are affected by a myriad of unobserved confounders,
suchasthequalityoftreatmentinagivenclinicalsettingorthepatient’sadherencetoatreatmentregimen.
Predictioninthis, andmanyother, settingsrequireslearningbothshared parameters(treatmenteffect)and
task parameters (the quality of treatment in this patient’s local clinic; this patient’s adherence). Bayesian
learning incorporates prior information into the learning process in a principled way, and so is a natural
paradigm for such tasks, especially when the available data is sparse.
We relax two components of many common transfer learning settings: First, while many formulations
provide the learner with the opportunity to fine-tune in the target task (e.g., Grant et al. 2018; Yoon et al.
2018; Patacchiola et al. 2020), ours does not. This is inspired by settings where fine-tuning is infeasible or
maycauseundueharm(e.g.,atreatmentthatcanonlybeeffectivelyadministeredonce). Withoutadditional
outcome information from the target task, the Bayesian learner has no information on the basis of which to
update their prior beliefs about the target task parameter.
Second, while many formulations assume the availability of some prior knowledge about the source data,
such as the number of distinct tasks represented in the source data and which data points correspond to
the same task (e.g., Grant et al. 2018; Yoon et al. 2018; Gordon et al. 2019; Patacchiola et al. 2020), we
operateinthesettingof unknown data sourceswherenosuchpriorinformationisavailable. Thissetting
is inspired by transfer learning tasks characterized by unobserved confounders, whose value and influence
on the source data cannot be directly measured. To the Bayesian learner who relies on prior information,
the lack of prior knowledge is a fundamental road block to making reliable inferences. As we show later,
substituting prior knowledge with a misspecified, even apparently uninformative, prior can lead to biased
inference of the shared parameter.
To overcome these challenges, we introduce a proxy-informed robust method for probabilistic transfer
learning(PROMPT).Toaddressthechallengeinestimatingthetargettaskparameterintroducedbythelack
of outcome information in the target task, PROMPT leverages proxy information, or auxiliary information
about the target task. For example, while one cannot directly measure the quality of treatment in a given
clinical setting, those familiar with the clinic may be able to provide information about the types of health
outcomes that are representative of that clinic.
To address the challenge in estimating the shared parameter introduced by the lack of knowledge about
thedatasources,PROMPTleverageslikelihood weighting,amethodusedtoenhancerobustnesstopotential
misspecification,andconstructstheweightsinapurelydata-dependentway(i.e.,withoutadditionaloutcome
or proxy information) from a relevance function that captures the “relevance” of an arbitrary data point to
the target task.
2 PRELIMINARIES
Notation. Vectors and matrices are denoted by bold lowercase letters. a is the entry in the ith row and
i,j
jth column of a. Sets are denoted by calligraphic font (A), and A
i
is the ith element of A. aI, where I
is a set, is the subvector formed from selecting the elements of a at the indices in I. Random variables
are denoted by bold capital letters (A), and the notation for probability distributions is subscripted by the
correspondingrandomvariable(P ). Forinstance,AistherandomvariablewithdomainA andprobability
A
distribution P .
A
Bayesian transfer learning is a general Bayesian framework for leveraging data from source task(s) to
make predictions in a somewhat unrelated target task (Suder et al., 2023).
We consider a standard setting where the tasks are characterized by both shared and task parameters.
Thelearnerhasavailabletothemsourcedatad=[(x ,y ),...,(x ,y )]composedofobservations,orpairs
1 1 n n
of covariatesx ∈X andoutcomesy ∈Y1. Thecovariatesareconsideredfixed, whiletheoutcomesare
i i
1Thesourcedatamatrixcanequivalentlybewrittend tomakeexplicitthatitiscomposedofallnpastobservations.
(1:n)
2considered random variables that depend on the covariates. This induces stochasticity in the source data
itself, which can take values d∈D. We write the random variable characterizing the source data as D.
Thenatureofthedependencebetweenxandycharacterizesatask: Outcomesfromthesametaskdepend
on covariates in the same way. Each observation d is generated in the context of a particular, possibly
i
non-unique, task. In particular, the conditional probability of each outcome y given the corresponding
i
covariates x depends both on shared parameters θ ∈ T, which are the same for each task, and task
i
parameters ψ ∈ S, which differ between tasks2. Given a (θ,ψ ), the learner can evaluate p(d |θ,ψ ) =
i i i i
p(y |x ,θ,ψ )p(x )=p(y |x ,θ,ψ ),wherethelastequalityfollowsfromthefactthatx isconsideredfixed
i i i i i i i i
(i.e., p(x )=1). As is typical in such formulations, we assume a single, data-generating value of each of the
i
shared and task parameters, which we denote θ⋆ and ψ⋆, respectively. We assume that data within tasks
are conditionally independently distributed.
Definition 2.1 (Task). A task is a specification of the conditional probability P for any x . It depends
Yi|xi i
on the value of a shared parameter θ⋆ and task parameter ψ⋆. The value θ⋆ is assumed to be shared
i
across all tasks, and so each task is uniquely characterized by the value of ψ⋆.
i
At deployment, the learner encounters an (n+1)th task with target covariates x , which will induce
n+1
an outcome y . Their goal is to predict y on the basis of x and d, which requires identification of
n+1 n+1 n+1
the target data-generating process, i.e., of the shared parameter θ⋆ and task parameter ψ⋆ .
n+1
The setting is visualized in Figure 1a. Throughout, we implicitly depend on the following assumption:
Assumption 2.2. Figure 1a faithfully captures all dependencies.
Notice that Assumption 2.2 implies that the value of ψ⋆ does not depend on θ⋆.
n+1
(cid:0) (cid:1)
The Bayesian learner assigns to values θ,ψ a prior distribution, and so treats the parameters as
n+1
(cid:0) (cid:1)
random variables Θ and Ψ with distribution P . For a given value θ,ψ , the likelihood L of
n+1 Θ,Ψn+1 n+1
the source data d is
L(d,θ,ψ )=p(d|θ,ψ )
n+1 n+1
=L(d,θ) (1)
where the second line follows because the target task parameter does not affect any of the source data.
An assumption that has been leveraged in other Bayesian transfer learning approaches is that the task
identitiesareknown. Inoursettingof unknown data sources,thelearnerisunawareofhowmanyunique
source tasks there are, or of which data come from the same, or similar, source tasks.
Negative transfer refers to the phenomenon that learning from source data can hurt performance in a
targettask(Wangetal.,2019;Slomanetal.,2024). Wedistinguishbetweenthecasesofpositiveandnegative
transfer. Informally,theBayesianlearnerexperiencespositivetransferwhentheirposteriorplacesahigher
probability on the shared parameter θ⋆ than their prior; otherwise, they experience negative transfer. A
formal definition is given in Section 4. (Notice that the presence of positive transfer is a property of the
shared parameter; the target task parameter is assumed to not affect the source data, and so only the
estimate of the shared parameter will “transfer”.)
Whenreferringtothematrixofallnpastobservations,weusuallyomitthesubscript(1:n),i.e.,writed≡d .
(1:n)
2Asinthecaseofthesourcedatad,thesourcetaskparameterscanequivalentlybewrittenψ . Whenreferringtothe
(1:n)
sourcetaskparameters,weusuallyomitthesubscript(1:n),i.e.,writeψ≡ψ .
(1:n)
3Classic Bayesian inference combines the likelihood of data with the prior to construct a posterior
P as follows:
Θ,Ψn+1|d
L(d,θ,ψ ) p(θ,ψ )
p(θ,ψ |d)= n+1 n+1
n+1 E (cid:2) L(d,θ,ψ )(cid:3)
θ∼PΘ,Ψ n+1
(cid:18) (cid:19)
L(d,θ) p(θ)
= p(ψ )
E [L(d,θ)] n+1
θ∼PΘ
(cid:18)E
[L(d,θ,ψ)]
p(θ)(cid:19)
= ψ∼PΨ p(ψ )
E [L(d,θ,ψ)] n+1
θ∼PΘ,Ψ
=p(θ|d) p(ψ ) (2)
n+1
where the second line follows from Assumption 2.2 and Equation (1), and the third line makes explicit that
the likelihood of the source data for the shared parameter marginalizes across the source task parameters.
The classic Bayesian approach is limited in two senses: (i) no information is gained about ψ (the
n+1
probability of a value ψ is unchanged from the prior to the posterior), and (ii) in order to specify a
n+1
likelihood for θ, the learner is required to have access to a prior P . When data sources are unknown, no
Ψ
prior knowledge is available on which the learner can base their choice of prior. As we show informally in
Section 3.2 and formally in Section 4, misspecification of this prior leads to the risk of negative transfer.
PROMPT addresses limitation (i) with the introduction of proxy information (Section 3.1). To address
limitation (ii), PROMPT leverages generalizations of Bayesian inference developed for settings where a
well-specified likelihood function is potentially unavailable.
Likelihood weighting is a technique whereby the learner modifies the likelihood to depend on a vector
of weights η (Gru¨nwald, 2011):
Lη(d,θ,ψ)=p(d,θ,ψ)η. (3)
If η <1, the effect is to “flatten” the likelihood of the ith data point, i.e., to place more of the learner’s
i
faith in the prior than the data. Many applications consider η = η ∀i,j. When some η ̸= η , it can be
i j i j
seen as increasing the influence of some data points relative to others, or intervening on the distribution of
data the model uses for learning.
Likelihood weighting has been applied for purposes that range from potential model misspecification
(Gru¨nwald, 2011; Miller and Dunson, 2019; Dewaskar et al., 2023), potential conflation of transferable and
task-specificeffects(IbrahimandChen,2000;Ibrahimetal.,2011,2014;Suderetal.,2023), modelselection
(Ibrahim et al., 2014), and increased efficiency of MCMC samplers (Schuster and Klebanov, 2021).
3 PROXY-INFORMED ROBUST METHOD FOR PROBABILIS-
TIC TRANSFER LEARNING
Our proposed proxy-informed robust method for probabilistic transfer learning (PROMPT) addresses the
setting of transfer learning with unknown data sources by using feedback from a proxy information source
in two ways: First, this proxy information is used to form a posterior on the target task parameter ψ .
n+1
Then, reweighting techniques are used to construct a likelihood for θ that is adapted for each possible value
of ψ which, when combined with the posterior over ψ , provides a robust posterior estimate for θ. Our
n+1
algorithm performs the reweighting in a constant number of steps (see Section 3.2 and Algorithm 1). Once
thereweightingisperformed, thecalculationoftheposteriorissimilartootherBayesianinferencemethods,
and so in many cases our method increases computational overhead by only this constant amount.
Figure 1 illustrates the intuition behind PROMPT. Sections 3.1 to 3.3 detail Step 1 (Figure 1b), Step 2
(Figure 1c), and Step 3 (Figure 1d) of PROMPT. The entire procedure is summarized in Algorithm 1.
4θ ψ (1:n) ψ n+1 ψ n+1
z z
d (1:n) d n+1
(a) Assumed dependencies. (b)Step1: zfacilitatesunbiasedidentificationofψ .
n+1
θ ψ n+1 θ ψ n+1
d (1:n) d n+1
(c) Step 2: The estimate of ψ facilitates unbiased (d) Step 3: The estimates of θ and ψ facilitate
n+1 n+1
identification of θ by providing a reweighting of the unbiased identification of d .
n+1
source data to appear “as if” it came from ψ . The
n+1
pseudo-dependence is indicated by the blue dotted line.
Figure 1: Assumed dependencies between shared parameters θ, task parameters ψ, source data d, target
data d , and proxy information z. Figure 1a shows all assumed dependencies. Figures 1b to 1d show the
n+1
dependencies leveraged at Steps 1–3 of PROMPT, respectively. Unfilled nodes denote unobserved variables,
grey nodes denote observed variables, and blue nodes denote unobserved variables for which a posterior
estimate has been obtained at the corresponding step of the algorithm. Edges denote causal dependence.
Algorithm 1 Proxy-informed RObust Method for Probabilistic Transfer learning (PROMPT)
Input: Sourcedatad,proxyinformationz,prioroverthetargetdata-generatingprocessP ,relevance
Θ,Ψn+1
function R, and number of iterations for refinement of the relevance function T
Output: Proxy-informed posterior predictive distribution PR
Yn+1|xn+1,d,z
1: Compute P Ψn+1|z (Equation (4)) ▷ Step 1
2: if R depends on P Θ then ▷ Refinement of R
3: P(cid:100)R Θ ←P Θ
4: for t∈1:T do
5: Evaluate R using P(cid:100)R Θ
6: Compute LR(d,θ) (Equation (5))
7: Compute P(cid:100)R Θ using P ΘR ,Ψn+1|d,z
8: end for
9: end if
10: Evaluate R using P(cid:100)R Θ
11: Compute LR(d,θ) (Equation (5)) ▷ Step 2
12: Compute PR (Definition 3.4) ▷ Step 3
Yn+1|xn+1,d,z
3.1 Step 1: Learning task parameters via proxies
To learn the target task parameters, we assume the learner has access to a source of proxy information
thatdependsonthevalueofψ⋆ (Figure1a). Wedenotetheproxyinformationz∈Z. Inordertoleverage
n+1
the proxy information to learn ψ⋆ , we require that the learner know something about how z depends on
n+1
ψ⋆ :
n+1
5Assumption 3.1 (p(z|ψ ) is known). The learner has a model for the likelihood of proxy information z
n+1
given ψ .
n+1
CombinedwiththepriorP ,thisinducesadistributionoverz. Wedenotethecorrespondingrandom
Ψn+1
variable Z.
Under Assumption 3.1, the learner can compute the posterior probability of a value ψ as
n+1
(cid:0) (cid:1) (cid:0) (cid:1)
p z|ψ p ψ
p(cid:0) ψ |z(cid:1) = n+1 n+1 . (4)
n+1 E (cid:2) p(cid:0) z|ψ′ (cid:1)(cid:3)
ψ′ n+1
n+1
3.2 Step 2: Learning shared parameters via likelihood weighting
Step 1 of PROMPT provides the learner with a posterior for the target task parameter P . With
Ψn+1|z
an unbiased estimate of the posterior probability of ψ , constructing a posterior over the entire data-
n+1
generating process requires estimating the conditional probability of the source data given θ and ψ . If
n+1
allthesourcedatasharedthesametaskparameterψ⋆ , inferencewouldbestraightforward, asthelearner
n+1
could simply leverage what they know about the conditional probability p(d|θ,ψ ). However, because
n+1
the source data do not depend on ψ , computing the likelihood of a value θ requires marginalizing over
n+1
prior information about the source task parameters. As we discussed in Section 2, in the case of unknown
data sources, this prior information is unavailable. Before explaining our approach, we briefly motivate why
the absence of this prior information can lead to negative transfer. We provide a more formal treatment of
this claim in Section 4.
The problem: example. To motivate the need for data reweighting to avoid negative transfer, consider
estimation of the linear model y ∼ N (θ⋆x +ψ⋆x ,s) and s = 1. Let’s say that for some k ∈ 1 : n,
i i,1 i i,2
ψ⋆ =−4 and ψ⋆ =4. If the learner knew the data sources, they could compute p(cid:0) θ|d,ψ⋆ (cid:1) by
(1:k) (k+1:n+1) n+1
includingonlythek+1−nth datapointsintheestimateofthesharedparameter. Combinedwithknowledge
of ψ⋆, this results in the likelihood L(d,θ)∝(cid:81)n i=k+1e−(yi−θxi,1 2−4xi,2)2 .
Since the data sources are unknown, the classic Bayesian learner may assign a seemingly uninformative
prior to the task parameters, e.g., ψ ∼N (0,σ) for each i∈1:n and some very large σ. Their estimate of
i
the likelihood is then L(d,θ)∝(cid:81)n
i=1e−(yi 2− σ2θ xx
2
ii ,, 21)2
, which does not account for the effect of x
·,2
on the mean
of the distribution. This misspecification may bias the learner’s inference.
The solution: informal. The challenge arises because the learner requires a model for p(cid:0) d|θ,ψ⋆ (cid:1) .
n+1
Using the source data, which does not depend on ψ⋆ , as-is provides no information. As is highlighted by
n+1
this toy example, using an arbitrary prior can induce negative transfer.
In an ideal world, the learner could intervene on the source data and set ψ⋆ =... =ψ⋆ =ψ⋆ . While
1 n n+1
this is infeasible, the learner can manipulate the source data to resemble data from ψ without directly
n+1
intervening on the outcomes: Namely, they can resample the data in a way that points that are relevant to
the target task are overweighted.
The solution: formal. To effectively reweight the data, we turn to likelihood weighting. To construct
the weights, we define the concept of relevance. We first introduce notation for the “pseudo-intervention”
ofsettingthetaskparameterofadatapointtoavalueψ. Whenψ referstothevalueofavariableonwhich
a source data point d does not depend, we write the probability of observing d if the ith task parameter
i i
had been “set” to ψ as p(d |θ,ψ =ψ).
i i
Definition 3.2 (Relevance (R (ψ))). The relevance of the ith data point to a task characterized by ψ
i
is computed by a relevance function R, is written R (ψ), and is a measure positively correlated with
i
p(d |θ⋆,ψ =ψ), in expectation with respect to P .
i i D,Ψ
6(cid:0) (cid:1)
PROMPT estimates p d|θ,ψ using the relevance-weighted (r-weighted) likelihood:
n+1
n
(cid:89)
Lψ n+1(d,θ)= p(d i,θ,ψ
i
=ψ n+1)Ri(ψ n+1). (5)
i=1
Definingtherelevancefunction. Definition3.2requiresthatR (ψ)positivelycorrelatewithp(d |θ⋆,ψ =ψ).
i i i
However,θ⋆isunknowntothelearner—otherwise,theywouldn’tneedtoperforminferenceinthefirstplace.
Inmanyinstances,thelearnerdoesnotneedadditionalknowledgeaboutθ⋆,andmayconstructtherelevance
function in a way that depends only on their prior P , for instance, as R (ψ)=E [(d |θ,ψ =ψ)]. To
Θ i θ∼Θ i i
correct for the bias in the prior, we propose an iterative procedure. Namely, for each of T iterations, the
learner can evaluate R using a “proposed” distribution P(cid:98)R, compute the r-weighted likelihood in Equa-
Θ
tion (5), estimate a new proposed distribution, and reevaluate R. This is outlined in Line 2–Line 9 of
Algorithm 1. In Appendix B, we details one such iterative procedure in the context of the synthetic exam-
ples in Section 6.
3.3 Step 3: Computing the proxy-informed posterior predictive distribution
Wecannowdefinether-weighted posteriorandr-weighted posterior predictive distributionwhich
are used to make predictions in the target task.
Definition 3.3 (Relevance-weighted(r-weighted)posteriordistribution(PR )). Ther-weightedpos-
Θ,Ψn+1|d,z
terior distribution PR is the parameter distribution with density
Θ,Ψn+1|d,z
pR(θ,ψ |d,z)=
Lψ n+1(d,θ) p(cid:0) z|ψ n+1(cid:1) p(cid:0) θ,ψ n+1(cid:1)
.
n+1 E
θ,ψ n+1∼PΘ,Ψn+1
(cid:2) Lψ n+1(d,θ) p(cid:0) z|ψ n+1(cid:1)(cid:3)
Definition 3.4 (Relevance-weighted (r-weighted) posterior predictive distribution (PR )). The
Yn+1|xn+1,d,z
r-weighted posterior predictive distribution PR is the predictive distribution with density
Yn+1|xn+1,d,z
pR(y |x ,d,z)= E (cid:2) p(y |x ,θ,ψ )(cid:3) .
n+1 n+1 n+1 n+1 n+1
θ,ψ ∼PR
n+1 Θ,Ψn+1|d,z
4 THEORETICAL RESULTS
We would like to assess whether PROMPT helps the learner better identify the target data-generating
process(cid:0) θ⋆,ψ⋆ (cid:1) . Todothisweleverageaninformation-theoreticmeasure: Theinformation gain (IG),
n+1
or degree to which a Bayesian learner has “gained information” about parameter value θ, is the expected
log ratio of the posterior to prior odds of θ. The IG has been used in contexts like experimental design
(Rainforth et al., 2024) and model selection (Oladyshkin and Nowak, 2019).
In our analysis, we will assume the learner specifies a, likely misspecified, prior over ψ, P . The proba-
Ψ
bility of the data using a classic likelihood is
L(d,θ)= E [L(d,θ,ψ)]. (6)
ψ∼PΨ
Weconsiderboththeinformationgainoftheproxy-informedBayesianlearnerwhousesthispriortocompute
thelikelihood(i.e.,usesaclassiclikelihood),andtheinformationgainoftheproxy-informedBayesianlearner
who instead uses an r-weighted likelihood. Because we are interested in the learner’s information gains
under the data-generating process, we define the information gain measures as an expectation across the
truedistribution ofthesource data. To reducenotationalclutter, we use D⋆ torefer tothe randomvariable
D|θ⋆,ψ⋆, which follows the distribution of source data under the true data-generating parameters (θ⋆,ψ⋆)
(which are unavailable to the learner).
Definitions4.1and4.2givetheinformationgainwithrespecttothetargetdata-generatingprocessforthe
learner using a classic and r-weighted likelihood, respectively. Derivations of the formulas in the definitions
are provided in Appendix A.
7Definition 4.1 (Information gain with a classic likelihood (IG)). The information gained about the target
data-generating process with a classic likelihood is
(cid:34) (cid:32) p(cid:0) θ⋆,ψ⋆ |d(cid:1)(cid:33)(cid:35)
IG(cid:0) θ⋆,ψ⋆ (cid:1) = E log n+1
n+1
d,z∼PD⋆,Z
p(cid:0) θ⋆,ψ⋆ n+1(cid:1)
(cid:20) (cid:18) L(d,θ⋆) (cid:19)(cid:21) (cid:34) (cid:32) p(cid:0) z|ψ⋆ (cid:1)(cid:33)(cid:35)
= E log + E log n+1
d∼PD⋆ E θ∼PΘ[L(d,θ)] z∼PZ p(z)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
IG(θ⋆)
IG(ψ⋆ )
n+1
and
(cid:20) (cid:18) p(θ⋆|d,z)(cid:19)(cid:21)
IG(θ⋆)= E log
p(θ⋆)
d,z∼PD⋆,Z
(cid:20) (cid:18) L(d,θ⋆) (cid:19)(cid:21)
= E log
E [L(d,θ)]
d∼PD⋆ θ∼PΘ
is the information gained about the shared parameter θ⋆.
Definition 4.2 (Information gain with an r-weighted likelihood (IGR)). The information gained about the
target data-generating process with an r-weighted likelihood is
(cid:34) (cid:32) pR(cid:0) θ⋆,ψ⋆ |d,z(cid:1)(cid:33)(cid:35)
IGR(cid:0) θ⋆,ψ⋆ (cid:1) = E log n+1
n+1
d,z∼PD⋆,Z
p(cid:0) θ⋆,ψ⋆ n+1(cid:1)
(cid:34) (cid:32) (cid:33)(cid:35)
= E log
Lψ⋆ n+1(d,θ⋆)
+IG(cid:0) ψ⋆ (cid:1)
d∼PD⋆
E
θ,ψ n+1∼PΘ,Ψn+1
(cid:2) Lψ n+1(d,θ)(cid:3) n+1
(cid:124) (cid:123)(cid:122) (cid:125)
IGR(θ⋆|ψ⋆ )
n+1
and
(cid:20) (cid:18) pR(θ⋆|d,z)(cid:19)(cid:21)
IGR(θ⋆)= E log
p(θ⋆)
d,z∼PD⋆,Z
= E
 log

E
ψ
n+1∼PΨn+1|z(cid:2) L (cid:104)ψ n+1(d,θ⋆)(cid:3) (cid:105) 

d,z∼PD⋆,Z E
θ,ψ′ n+1∼PΘ,Ψn+1
Lψ′ n+1(d,θ)
is the information gained about the shared parameter θ⋆.
Definitions 4.1 and 4.2 show that the approaches differ only in the information gain they provide the
learner about θ⋆, and so we restrict our analysis to IG(θ⋆), for the learner with the classic likelihood, and
IGR(θ⋆), for the learner with the r-weighted likelihood.
Using these measures, we can now provide a formal definition of positive and negative transfer. It can
be verified that Definitions 4.1 and 4.2 imply the condition given in the informal definition in Section 2.
Definition 4.3 (Positive and negative transfer). The Bayesian learner with a classic (resp. r-weighted)
likelihood experiences positive transfer when IG(θ⋆) (resp. IGR(θ⋆)) is positive; otherwise, they experience
negative transfer.
Our goal in this section is to understand the effect of r-weighting on the risk of negative transfer. We
providethreeresults: Theorem4.5showsthatmisspecificationoftheclassiclikelihood,whichcanstemfrom
misspecificationofthepriorP ,canresultinnegativetransfer. Theorem4.9showsthatananalogousresult
Ψ
holds for the r-weighted likelihood, where the misspecification can be interpreted as the degree to which the
relevancefunctioncorrectsforamismatchbetweenthesourceandtargettasks. Proposition4.10decomposes
themeasureofmisspecificationderivedforther-weightedcase,andshowsthatitisanegativefunctionofthe
fidelityoftherelevancefunction,i.e.,thedegreetowhichitcorrelateswithp(cid:0) d |θ⋆,ψ =ψ (cid:1) . Overall,our
i i n+1
results imply that r-weighting with sufficiently high-fidelity relevance functions reduces the risk of negative
transfer.
The proofs of all results are deferred to Appendix A.
8Negativetransferwithaclassiclikelihood. Wefirstdefineatermthat,asTheorem4.5shows,provides
an intuition for the source of negative transfer.
Definition 4.4 (Misspecification of the classic likelihood (∆)). The degree to which the classic likelihood is
(cid:0) (cid:1)
misspecified is ∆≡D P || P where D is the Kullback-Leibler divergence measure.
KL D⋆ D|θ⋆ KL
The degree of likelihood misspecification is closely related to the specification of the prior over task
parameters (Sloman et al., 2024). To see this, notice that the density for P implicitly marginalizes over
D|θ⋆
the learner’s prior P as shown in Equation (6).
Ψ
Theorem 4.5 restates results from Sloman et al. (2024) in the context of our setting to show that ∆ is
responsible for negative transfer. It relies on the following assumption:
Assumption A.3 (informal). The likelihood L(d,θ) is “smooth enough” in a neighborhood of θ⋆. The
formal condition is given in Appendix A.3.
Theorem 4.5 (Negative transfer with a classic likelihood). Under Assumption A.3 in Appendix A.3,
IG(θ⋆)≤A(B−∆) (7)
where A and B are constants that do not depend on ∆.
Negative transfer with an r-weighted likelihood. We now define a concept analogous to ∆ for the
r-weighted case.
Definition 4.6 (Misspecification of the r-weighted likelihood (∆R)). The degree to which the r-weighted
(cid:104) (cid:16) (cid:17)(cid:105)
likelihood is misspecified is ∆R ≡ E D P || P where P is the dis-
ψ n+1∼PΨn+1 KL D⋆ DR(ψn+1)|θ⋆ DR(ψn+1)
tribution of data resulting from viewing R (ψ ) replicates of each d .
i n+1 i
Inther-weightedcase, themisspecificationstemsfromthefailureofthepseudo-replicationtocorrectfor
a mismatch in the source and target tasks.
Theorem4.9givesaresultanalogoustoTheorem4.5forther-weightedcase. Itdependsonthefollowing
assumptions:
Assumption 4.7 (P is well-specified). The prior P is well-specified in the sense that p(z) =
(cid:2)
ψ
n+(cid:3)1
Ψn+1
E p(z|ψ ) .
ψ n+1∼PΨn+1 n+1
Assumption 4.8 (Lψ n+1 is bounded). The r-weighted likelihood Lψ n+1(d,θ) is bounded from both below
and above: ∃a,b∈R+ such that ∀d∈D,θ ∈T,ψ
n+1
∈S, a≤Lψ n+1(d,θ)≤b.
Assumption A.6 (informal). The proxy is sufficiently informative in the sense that the “variability” of
Ψ |z is smaller than the “variability” of Ψ by a “large enough” margin. The formal condition is
n+1 n+1
given in Appendix A.4.
Assumption A.7 (informal). The likelihood L(d,θ) is “smooth enough” in a neighborhood of θ⋆ and the
estimated relevances are not “too large”. The formal condition is given in Appendix A.4.
Theorem 4.9 (Negative transfer with an r-weighted likelihood). Under Assumptions 4.7 and 4.8 and As-
sumptions A.6 and A.7 in Appendix A.4,
IGR(θ⋆)≤A(cid:0) C−∆R(cid:1)
where A and C are constants that do not depend on ∆R.
Theorem4.9impliesthatbyanalyzingtheeffectofRon∆R,wegaininsightsintotheeffectofproperties
of the relevance function on the risk of negative transfer.
9The effect of r-weighting on negative transfer. Proposition4.10analyzestheeffectofRon∆R, and
shows that the role of R in mitigating negative transfer depends on the fidelity of the relevance function:
Definition A.8 (informal). ρR is a measure of the fidelity of the relevance function, i.e., the extent of
the correlation of R (ψ ) with p(d |θ⋆,ψ = ψ ), in expectation with respect to P . The formal
i n+1 i i n+1 D,Ψn+1
definition is given in Appendix A.5.
Proposition 4.10 provides a decomposition of ∆R that makes transparent its dependence on ρR.
Proposition 4.10 (Negative transfer is reduced by high-fidelity relevance functions). ∆R is a negative
function of ρR. In particular,
∆R =E(cid:2) ESS(cid:0) d,ψ (cid:1) f(cid:0) d,ψ (cid:1)(cid:3) −nρR+D
n+1 n+1
where ESS(d,ψ) ≡
(cid:80)n
R (ψ) is the effective sample size induced by the relevance function R evaluated
i=1 i
on the sample d and task parameter ψ, the expectation is taken with respect to P , and the function
D⋆,Ψn+1
f and constant D do not depend on R.
5 RELATED WORK
Probabilistic meta-learning. Probabilistic meta-learning (Gordon et al., 2019) is a paradigm in which
a meta-learner simultaneously learns a transferable parameter value and a distribution over task parameter
values. Unlike PROMPT, this framework assumes the data sources are known in the sense that each data
point can be indexed by its task. This distinction also sets us apart from other Bayesian meta-learning
approaches(Grantetal.,2018;Yoonetal.,2018;Patacchiolaetal.,2020). Moreover,theaimofprobabilistic
meta-learning is to learn a distribution over task parameters. While this facilitates good performance on
average across tasks, the goal of PROMPT is to provide a posterior predictive distribution adapted to each
new data point and task.
Using domain similarity for domain adaptation. Manyexistingtheoreticalboundsfordomainadap-
tation rely on the similarity between source and target tasks (Redko et al., 2022). Some approaches to
domain adaptation use similarity of covariates in the target and source tasks to weight source data during
training (Plank and van Noord, 2011; Ponomareva and Thelwall, 2012; Remus, 2012; Ruder and Plank,
2017), or importance sampling techniques (Quin˜onero-Candela et al., 2009). While this can be effective in
cases of pure covariate shift, our formulation models differences between tasks as differences in the map
between covariates and outcomes, and cannot be detected on the basis of covariate information alone.
Proximal causal learning. Proximal causal learning is a paradigm that uses proxy information to learn
causal effects (Kuroki and Pearl, 2014; Tchetgen Tchetgen et al., 2020; Alabdulmohsin et al., 2023; Tsai
et al., 2024). Our setting corresponds closely to the multi-domain adaptation setting of Tsai et al. (2024).
We differ in that (i) we assume data sources are unknown, while they assume data can be indexed by its
task; and (ii) we assume the presence of both shared and task parameters, while they do not distinguish
between these. While our method for estimating the task parameter also leverages proxy methods, we differ
in our usage of reweighting methods to estimate the shared parameter, which facilitates robust estimation
without requiring additional proxy information.
Human-in-the-loop learning. In many applications, domain experts are a viable source of proxy infor-
mation, and so our work can be tied to human-in-the-loop machine learning (Wu et al., 2022). Like us,
some human-in-the-loop methods leverage expert feedback in a Bayesian framework. For example, Nahal
etal.(2024)useexpertfeedbackforlearninginout-of-distributionsettings, whileSundinetal.(2018)query
experts about the relevance of a given feature for outcome prediction.
1035
No source tasks like target task
25% source tasks like target task
30
50% source tasks like target task
75% source tasks like target task
25
All source tasks like target task
20
15
10
5
0
None Mild Extreme
Degree of multicollinearity
Figure 2: Advantage of learning with an r-weighted likelihood in the linear regression setting. Each box
in the plot shows the interquartile region (boxes) and outliers (points) of IGR(θ⋆) − IG(θ⋆) across 50
simulations where ψ⋆ , d and z are independently re-generated. Values on the x-axis correspond to three
n+1
distinct levels of multicollinearity between covariates.
6 SYNTHETIC EXAMPLES
WeheredemonstrateapplicationofPROMPTintwosyntheticsettings. Additionaldetailsofbothexamples
are provided in Appendix B.
Linear regression. We first apply PROMPT to estimation of a Bayesian linear regression setting model.
This is intended as a simplification of similar modeling paradigms used in clinical prediction tasks (Gunn-
Sandell et al., 2023), like our motivating example of treatment effect estimation. More specifically, the
synthetic data in this example are generated according to the model
y ∼N (θ⋆x +ψ⋆x ,σ)
i i,1 i i,2
where σ =1.
Figure 2 shows how the information gain achieved by PROMPT compares with the information gain
achievedbytheclassicBayesianlearnerasafunctionoftheriskofnegativetransferandtherepresentativeness
of the target task in the distribution of source tasks.
To induce the risk of negative transfer, we manipulated the degree of multicollinearity between x
(·,1)
and x : More multicollinearity makes θ⋆ and ψ⋆ harder to separately identify, so we interpret this
(·,2) n+1
as a higher risk of negative transfer. We also varied the distribution of source tasks. When p% of tasks
resemble the target task, 1−p% of tasks are set to a value that is well-represented by the classic Bayesian
11learner’s prior over task parameters. In this sense, the results in Figure 2 are a somewhat conservative test
of PROMPT. We varied the value of p% among the values indicated in the legend of Figure 2.
When there is no multicollinearity, the classic learner is not at risk of negative transfer, and performs on
parwiththelearnerwithanr-weightedlikelihood. Whenallsourcetasksarewell-representedinthelearner’s
prior (blue box), the classic learner’s prior is well-specified, and so they perform on par with the learner
with an r-weighted likelihood. When there is a risk of negative transfer, the information gain provided by
PROMPTisgenerallyhigheranddoesnotappeartobesensitivetotherepresentativenessofthetargettask
in the distribution of source tasks.
Gaussian process regression. We next demonstrate application of PROMPT in a Gaussian Process
(GP) regression setting with a composite kernel. In particular, data were generated according to the model
y ∼GP(0,k(x ,x))
i i
wherek(x ,x)=RBF (x ,x)+RBF (x ,x)andRBF istheradialbasisfunctionwithlengthscalel3. Each
i θ i ψ i l
i
simulation consisted of 80 trajectories drawn from a GP of this form. As discussed in Sloman et al. (2024),
this setting poses a risk of negative transfer because the transferable and task parameter act in combination
to determine the smoothness of the sampled functions: Smoothness, or lack thereof, in the source data can
be attributed to either the target or task parameter.
Figure 3 shows how the information gain achieved by PROMPT compares with the information gain
achieved by the classic Bayesian learner as a function of the value of θ⋆ and the representativeness of the
target task in the distribution of source tasks. As in the linear regression example, the relative performance
of PROMPT does not appear to depend on the representativeness of the target task in the distribution of
sourcetasks. Itdoesappeartodependonthevalueofθ⋆: PROMPT’srelativeabilitytorecoverlargevalues
ofθ⋆ islowerbutfarmorevariablethanitsrelativeabilitytorecoversmallvaluesofθ⋆. Thereasonforthis
is a direction for future investigation.
Afterpreliminarysimulationsappearedtoshowasensitivitytothevalueofsomesimulationparameters,
we varied these parameters across all simulations. The results shown in Figure 3 are collapsed across all
values of these parameters. Appendix B describes the interpretation of and values used for these additional
simulation parameters, and provides additional results showing the relative performance of PROMPT as a
function of the values of each of these additional parameters.
7 DISCUSSION
We presented PROMPT, a novel framework for Bayesian transfer learning. Our setting relaxes two as-
sumptions that are common in other transfer learning settings: We assume neither that the learner has the
opportunity to collect additional outcome information from (i.e., to fine-tune in) the target task, nor that
the learner has any prior knowledge about the nature of the source data. The framework of PROMPT can
accommodate a variety of forms of proxy information and relevance functions. Our work provides theoreti-
cal tools to analyze and synthetic proof-of-concepts of the importance of a key component of the proposed
method. The development of more refined algorithms within, and the application to real-world settings of,
this framework are promising avenues for future work.
Acknowledgments
The authors thank Ayush Bharti and Sammie Katt for helpful feedback on an initial draft. This work was
supported by the Research Council of Finland Flagship programme: Finnish Center for Artificial Intelli-
gence FCAI. SJS and SK were supported by the UKRI Turing AI World-Leading Researcher Fellowship,
[EP/W002973/1]. This work used the Computational Shared Facility at The University of Manchester.
3Thekernelwasadditionallyrenormalizedtohaveanamplitudeof1.
1210 tasks like target task
20 tasks like target task
40
30 tasks like target task
40 tasks like target task
30
50 tasks like target task
60 tasks like target task
20 70 tasks like target task
10
0
−10
Left tail Mode Right tail
Figure 3: Advantage of learning with an r-weighted likelihood in the GP regression setting. Each box in
the plot shows the interquartile region (boxes) and outliers (points) of IGR(θ⋆)−IG(θ⋆) across 5 levels of
outcome/proxy information trade-off × 6 levels of covariate resolution × 4 values of T (number of iterations
for refinement of the relevance function) × 50 simulations where ψ⋆ , d and z are independently re-
n+1
generated. Values on the x-axis correspond to the location of θ⋆ in terms of the learner’s prior P .
Θ
References
Ibrahim Alabdulmohsin, Nicole Chiou, Alexander D’Amour, Arthur Gretton, Sanmi Koyejo, Matt J. Kus-
ner, Stephen R. Pfohl, Olawale Salaudeen, Jessica Schrouff, and Katherine Tsai. Adapting to latent
subgroupshiftsviaconceptsandproxies. InProceedings of the 26th International Conference on Artificial
Intelligence and Statistics (AISTATS 2023), 2023.
Miheer Dewaskar, Christopher Tosh, Jeremias Knoblauch, and David B. Dunson. Robustifying likelihoods
by optimistically re-weighting data, 2023. Accessed via https://arxiv.org/abs/2303.10525.
Jonathan Gordon, John Bronskill, Matthias Bauer, Sebastian Nowozin, and Richard E. Turner. Meta-
learning probabilistic inference for prediction. In The Seventh International Conference on Learning
Representations (ICLR 2019), 2019.
Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths. Recasting gradient-based
meta-learning as hierarchical bayes, 2018. Accessed via https://arxiv.org/abs/1801.08930.
Peter Gru¨nwald. Safe learning: bridging the gap between bayes, mdl and statistical learning theory via
empirical convexity. In 24th Annual Conference on Learning Theory, 2011.
13Lauren B. Gunn-Sandell, Edward J. Bedrick, Jacob L. Hutchins, Aaron A. Berg, Alexander M. Kaizer, and
NicholeE.Carlson. Apracticalguidetoadoptingbayesiananalysesinclinicalresearch. JournalofClinical
and Translational Studies, 8, 2023.
Joseph G. Ibrahim and Ming-Hui Chen. Power prior distributions for regression models. Statistical Science,
15(1), 2000.
Joseph G. Ibrahim, Ming-Hui Chen, and Debajyoti Sinha. On optimality properties of the power prior.
Journal of the American Statistical Association, 98, 2011.
Joseph G. Ibrahim, Ming-Hui Chen, Yeongjin Gwon, and Fang Chen. The power prior: theory and applica-
tions. Statistics in Medicine, 34, 2014.
Manabu Kuroki and Judea Pearl. Measurement bias and effect restoration in causal inference. Biometrika,
2014.
Jeffrey W. Miller and David B. Dunson. Robust bayesian inference via coarsening. Journal of the American
Statistical Association, 114(527):1113–1125, 2019.
Yasmine Nahal, Janosch Menke, Julien Martinelli, Markus Heinonen, Mikhail Kabeshov, Jon Paul Janet,
Eva Nittinger, Ola Engkvist, and Samuel Kaski. Human-in-the-loop active learning for goal-oriented
molecule generation, 2024. Accessed via https://chemrxiv.org/engage/chemrxiv/article-details/
66b37e5b01103d79c5095361.
SergeyOladyshkinandWolfgangNowak. Theconnectionbetweenbayesianinferenceandinformationtheory
for model selection, information gain and experimental design. Entropy, 21, 2019.
Massimiliano Patacchiola, Jack Turner, Elliot J. Crowley, Michael O’Boyle, and Amos Storkey. Bayesian
meta-learning for the few-shot setting via deep kernels. In 34th Conference on Neural Information Pro-
cessing Systems (NeurIPS 2020), 2020.
Barbara Plank and Gertjan van Noord. Effective measures of domain similarity for parsing. In Proceedings
of the 49th Annual Meeting of the Association for Computational Linguistics, 2011.
Natalia Ponomareva and Mike Thelwall. Biographies or blenders: Which resource is best for cross-domain
sentiment analysis? In Computational Linguistics and Intelligent Text Processing (CICLing 2012), 2012.
Joaquin Quin˜onero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D. Lawrence. Dataset Shift
in Machine Learning. MIT Press, 2009.
Tom Rainforth, Adam Foster, Desi R. Ivanova, and Freddie Bickford Smith. Modern bayesian experimental
design. Statistical Science, 39(1), 2024.
IevgenRedko,EmilieMorvant,AmauryHabrard,andMarcSebban. Asurveyondomainadaptationtheory:
Learning bounds and theoretical guarantees, 2022. Accessed via https://arxiv.org/pdf/2004.11829.
RobertRemus. Domainadaptationusingdomainsimilarity-anddomaincomplexity-basedinstanceselection
for cross-domain sentiment analysis. In IEEE 12th International Conference on Data Mining Workshops,
2012.
SebastianRuderandBarbaraPlank. Learningtoselectdatafortransferlearningwithbayesianoptimization.
In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, 2017.
Ingmar Schuster and Ilja Klebanov. Markov chain importance sampling—a highly efficient estimator for
MCMC. Journal of Computational and Graphical Statistics, 30:260–268, 2021.
Sabina J. Sloman, Ayush Bharti, Julien Martinelli, and Samuel Kaski. Bayesian active learning in the pres-
enceofnuisanceparameters. InProceedingsofthe40th ConferenceonUncertaintyinArtificialIntelligence
(UAI 2024), 2024.
14Piotr M. Suder, Jason Xu, and David B. Dunson. Bayesian transfer learning, 2023. Accessed via https:
//arxiv.org/abs/2312.13484.
Iiris Sundin, Tomi Peltola, Luana Micallef, Homayun Afrabandpey, Marta Soare, Muntasir Mamun Ma-
jumder, Pedram Daee, Chen He, Baris Serim, Aki Havulinna, Caroline Heckman, Giulio Jacucci, Pekka
Marttinen,andSamuelKaski. Improvinggenomics-basedpredictionsforprecisionmedicinethroughactive
elicitation of expert knowledge. Bioinformatics, 2018.
Eric J Tchetgen Tchetgen, Andrew Ying, Yifan Cui, Xu Shi, and Wang Miao. An introduction to proximal
causal learning, 2020. Accessed via https://arxiv.org/abs/2009.10982.
Katherine Tsai, Stephen R Pfohl, Olawale Salaudeen, Nicole Chiou, Matt Kusner, Alexander D’Amour,
Sanmi Koyejo, and Arthur Gretton. Proxy methods for domain adaptation. In International Conference
on Artificial Intelligence and Statistics, pages 3961–3969. PMLR, 2024.
Zirui Wang, Zihang Dai, Barnabas Poczos, and Jaime Carbonell. Characterizing and avoiding negative
transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), 2019.
Xingjiao Wu, Luwei Xiao, Yixuan Sun, Junhang Zhang, Tianlong Ma, and Liang He. A survey of human-
in-the-loop for machine learning. Future Generation Computer Systems, 135, 2022.
Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn. Bayesian
model-agnostic meta-learning. In 32nd Conference on Neural Information Processing Systems (NeurIPS
2018), 2018.
15Appendix
The appendix is organized as follows:
• In Appendix A, we provide proofs of all our mathematical results.
• In Appendix B, we provide details of the synthetic examples presented in Section 6.
A Mathematical Details
A.1 Definitions
• H(P) is the entropy of distribution P with density p:
H(P)=− E [log(p(x))]
x∼P
• H(P || Q) is the cross-entropy from distribution P to distribution Q with density q:
H(P || Q)=− E [log(q(x))]
x∼P
• D (P || Q) is the Kullback-Leibler divergence from distribution P with density p, to distribution Q
KL
with density q:
(cid:20) (cid:21)
p(x)
D (P || Q)= E log
KL x∼P q(x)
A.2 Derivation of formulas in Definitions 4.1 and 4.2
Information gain with a classic likelihood (Definition 4.1). The information gain with a classic
likelihood can be written as
(cid:20) (cid:18)p(θ⋆,ψ⋆ |d)(cid:19)(cid:21)
IG(cid:0) θ⋆,ψ⋆ (cid:1) = E log n+1
n+1 p(θ⋆,ψ⋆ )
d,z∼PD⋆,Z n+1
(cid:20) (cid:18) p(θ⋆|d)(cid:19)(cid:21) (cid:20) (cid:18)p(ψ⋆ |z)(cid:19)(cid:21)
= E log + E log n+1 (Assumption 2.2)
d∼PD⋆ p(θ⋆) z∼PZ p(ψ⋆ n+1)
(cid:20) (cid:18) L(d,θ⋆) (cid:19)(cid:21) (cid:20) (cid:18)p(z|ψ⋆ )(cid:19)(cid:21)
= E log + E log n+1 (Equations (2) and (4)) (8)
d∼PD⋆ E θ∼PΘ[L(d,θ)] z∼PZ p(z)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
IG(θ⋆) IG(ψ⋆ )
n+1
Information gain with an r-weighted likelihood (Definition 4.2). The information gain with an
r-weighted likelihood can be written as
(cid:20) (cid:18)pR(θ⋆,ψ⋆ |d,z)(cid:19)(cid:21)
IGR(cid:0) θ⋆,ψ⋆ (cid:1) = E log n+1
n+1 p(θ⋆,ψ⋆ )
d,z∼PD⋆,Z n+1
(cid:20) (cid:18)pR(θ⋆|d,ψ⋆ )(cid:19)(cid:21) (cid:20) (cid:18)p(ψ⋆ |z)(cid:19)(cid:21)
= E log n+1 + E log n+1
d∼PD⋆ p(θ⋆) z∼PZ p(ψ⋆ n+1)
= E
(cid:34) log(cid:32) Lψ⋆ n+1(d,θ⋆) (cid:33)(cid:35)
+ E
(cid:20) log(cid:18)p(z|ψ⋆ n+1)(cid:19)(cid:21)
(9)
d∼PD⋆ E θ,ψ∼PΘ,Ψn+1 (cid:2) Lψ n+1(d,θ)(cid:3) z (cid:124)∼PZ
(cid:123)(cid:122)
p(z)
(cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)
IGR(θ⋆|ψ⋆ )
IG(ψ⋆ n+1)
n+1
where the last line follows from Assumption 2.2, Definition 3.3, and Equation (4).
16We derive IGR(θ⋆), the information gained about θ⋆, as follows:
(cid:20) (cid:18) pR(θ⋆|d,z)(cid:19)(cid:21)
IGR(θ⋆)= E log
p(θ⋆)
d,z∼PD⋆,Z
(cid:34) (cid:32)E (cid:2) pR(θ⋆|d,ψ )(cid:3)(cid:33)(cid:35)
IGR(θ⋆)= E log ψ n+1∼PΨn+1|z n+1
p(θ⋆)
d,z∼PD⋆,Z
IGR(θ⋆)= E
(cid:34) log(cid:32) E
ψ
n+1∼PΨn+1|z(cid:2) Lψ n+1(d,θ⋆)(cid:3) (cid:33)(cid:35)
(10)
d,z∼PD⋆,Z
E
θ,ψ n+1∼PΘ,Ψn+1
(cid:2) Lψ n+1(d,θ)(cid:3)
A.3 Proof of Theorem 4.5
The proof follows the proof of Proposition 4.1 and Theorem 4.5 of Sloman et al. (2024). It depends on the
following definitions:
Definition A.1 (ϵ-neighborhood of θ (N (θ)) (Definition 4.2 of Sloman et al. (2024))). N (θ) ≡ {θ′ ∈
ϵ ϵ
T | d(θ,θ′)<ϵ}, where d is a suitable distance measure, is the ϵ-neighborhood of θ.
Definition A.2 (PA (modificationofDefinition4.3ofSlomanetal.(2024))). PA refers to the distribution
Θ Θ
of Θ obtained by restricting the support of the learner’s prior to the set A, under which
p(θ)
A
p (θ)≡ (cid:82)
p(θ) dθ
A
for any θ ∈A.
Assumption A.3 (Smoothnessinparameterspace(Assumption4.4ofSlomanetal.(2024))). There exists
some ϵ>0 such that
(cid:20) (cid:18) (cid:19)(cid:21)
E log E [L(d,θ)]
d∼PD⋆ θ∼PΘ
(cid:34)(cid:32) (cid:33) (cid:32) (cid:33) (cid:32) (cid:33)(cid:35)
(cid:90) (cid:90)
≥ E p(θ) dθ log(L(d,θ⋆))+ p(θ) dθ log E [L(d,θ)]
d∼PD⋆ Nϵ(θ⋆) T\Nϵ(θ⋆) θ∼P ΘT\Nϵ(θ⋆)
(cid:16) (cid:17) (cid:16) (cid:17)
(cid:82) (cid:82)
where p(θ) dθ and p(θ) dθ are the probability that a value θ is inside and outside the
Nϵ(θ⋆) T\Nϵ(θ⋆)
ϵ-neighborhood of θ⋆, respectively.
Remark on Assumption A.3. Assumption A.3 holds when Θ is a discrete random variable (in which
casetheϵ-neighborhoodofθ⋆ canbedefinedas{θ⋆}andtoexcludeallotherparametervalues). WhenΘisa
continuous random variable, Assumption A.3 is essentially a smoothness condition: For likelihoods that are
sufficiently smooth around θ⋆, we can expect it to hold for ϵ→0. To see this, notice that Jensen’s inequality
implies that
(cid:20) (cid:18) (cid:19)(cid:21)
E log E [L(d,θ)]
d∼PD⋆ θ∼PΘ
(cid:34)(cid:32) (cid:33) (cid:32) (cid:33) (cid:32) (cid:33) (cid:32) (cid:33)(cid:35)
(cid:90) (cid:90)
≥ E p(θ) dθ log E [L(d,θ)] + p(θ) dθ log E [L(d,θ)] .
d∼PD⋆ Nϵ(θ⋆) θ∼P ΘNϵ(θ⋆) T\Nϵ(θ⋆) θ∼P ΘT\Nϵ(θ⋆)
Assumption A.3 holds when E θ∼PNϵ(θ⋆)[p(d|θ)]≈p(d|θ⋆) and the approximation is tight enough that it does
Θ
not close the Jensen gap.
17Taking P D|θ∈T\Nϵ(θ⋆) to be the source data distribution conditioned on the event that the shared pa-
rameter is not in the ϵ-neighborhood of θ⋆, we obtain
(cid:20) (cid:18) (cid:19)(cid:21)
IG(θ⋆)= E log(L(d,θ⋆))−log E [L(d,θ)]
d∼PD⋆ θ∼PΘ
(cid:34) (cid:32) (cid:33)
(cid:90)
≤E log(L(d,θ⋆))− p(θ) dθ log(L(d,θ⋆))−
d∼PD⋆
Nϵ(θ⋆)
(cid:32) (cid:33) (cid:32) (cid:33)(cid:35)
(cid:90)
p(θ) dθ log E [L(d,θ)] (Assumption A.3)
T\Nϵ(θ⋆) θ∼P ΘT\Nϵ(θ⋆)
(cid:34)(cid:32) (cid:33)(cid:32) (cid:32) (cid:33)(cid:33)(cid:35)
(cid:90)
= E p(θ) dθ log(L(d,θ⋆))−log E [L(d,θ)]
d∼PD⋆ T\Nϵ(θ⋆) θ∼P ΘT\Nϵ(θ⋆)
(cid:32) (cid:33)
(cid:90)
(cid:0) (cid:0) (cid:1) (cid:0) (cid:1)(cid:1)
= p(θ) dθ H P D⋆ || P D|θ∈T\Nϵ(θ⋆) −H P D⋆ || P D|θ⋆
T\Nϵ(θ⋆)
(cid:32) (cid:33)
(cid:90)
(cid:0) (cid:0) (cid:1) (cid:0) (cid:1)(cid:1)
= p(θ) dθ H(P D⋆)+D KL P D⋆ || P D|θ∈T\Nϵ(θ⋆) −H(P D⋆)−D KL P D⋆ || P D|θ⋆
T\Nϵ(θ⋆)
(cid:32) (cid:33)
(cid:90)
(cid:0) (cid:0) (cid:1) (cid:0) (cid:1)(cid:1)
= p(θ) dθ D KL P D⋆ || P D|θ∈T\Nϵ(θ⋆) −D KL P D⋆ || P D|θ⋆ (11)
T\Nϵ(θ⋆)
(cid:16) (cid:82) (cid:17) (cid:0) (cid:1)
as stated in the theorem for A= T\Nϵ(θ⋆)p(θ) dθ and B =D KL P D⋆ || P D|θ∈T\Nϵ(θ⋆) .
A.4 Proof of Theorem 4.9
The proof of Theorem 4.9 uses the following lemma:
LemmaA.4. DefineJ (cid:0) log;P Ψn+1|z(cid:1) ≡log(cid:16) E
ψ
n+1∼PΨn+1|z(cid:2) Lψ n+1(d,θ⋆)(cid:3)(cid:17) −E
ψ
n+1∼PΨn+1|z(cid:2) log(cid:0) Lψ n+1(d,θ⋆)(cid:1)(cid:3)
and J (cid:0) log;P Ψn+1(cid:1) ≡ log(cid:16) E
θ,ψ n+1∼PΘ,Ψn+1
(cid:2) Lψ n+1(d,θ)(cid:3)(cid:17) −E
ψ n+1∼PΨn+1
(cid:2) log(cid:0) E θ∼PΘ(cid:2) Lψ n+1(d,θ)(cid:1)(cid:3)(cid:3) .
(cid:2) (cid:0) (cid:1)(cid:3)
UnderAssumption4.8andAssumptionA.6(statedformallyintheproofofthelemma),E J log;P ≥
(cid:2) (cid:0) (cid:1)(cid:3)
d∼PD⋆ Θ,Ψn+1
E J log;P .
d,z∼PD⋆,Z Ψn+1|z
Proof of Lemma A.4. The lemma leverages a result known as H¨older’s defect (Steele, 2004; Becker, 2012):
Theorem A.5 (H¨older’s defect (restated from Steele 20044) ). If f : [a,b]→R is twice differentiable and
if we have the bounds
0≤m≤f′′(x)≤M for all x∈[a,b],
then for a distribution P over [a,b], there exists a real value µ∈[m,M] for which one has the formula
(cid:18) (cid:19)
1
E [f(x)]−f E [x] = µVar [x]
x∼P x∼P 2 x∼P
(cid:124) (cid:123)(cid:122) (cid:125)
J(−f;P)
(cid:104) (cid:105)
for Var [x]≡E (x−E [x])2 .
x∼P x∼P x∼P
andO Vu ar rg ψo na +l 1i ∼s Pto Ψnu +s 1e (cid:2)H E¨o θl ∼d Θer’ (cid:2)s Ld ψe nfe +c 1t (t do ,r θe )l (cid:3)a (cid:3)t ,e rJ es(cid:0) pl eo cg t; ivP eΨ lyn .+1 W|z(cid:1) ea fin rd stJ v(cid:0) el ro ig fy;P tΨ han t+1 t(cid:1) heto cV oa nr dψ itn i+ o1 n∼ sPΨ ren q+ u1| iz re(cid:2) dLψ fon r+1(d,θ⋆)(cid:3)
H¨older’s defect formula to apply are met. For both applications of the result, f is the negative of the log
Jfun (cid:0)c loti go ;n P. ΨI nn +1a (cid:1)p ,p flic ta at ki eo sn at so inJ pu(cid:0) l to vg a; lP
uΨ esn o+ f1|
Ez(cid:1) θ, ∼Pf Θt (cid:2)a Lke ψs n+a 1s (i dn ,p θu )t
(cid:3)
.values of Lψ n+1(d,θ⋆). In application to
4Steele(2004)statestheresultintermsofdiscretesums;weheremodifiedthestatementoftheresultsoitcanbeinterpreted
forcontinuousrandomvariables.
18• f : [a,b] → R: Assumption 4.8 ensures that inputs in both cases are bounded from both below and
above.
• f is twice differentiable: The second derivative of f evaluated at x is f′′(x)= 1 .
x2
• 0≤m≤f′′(x)≤M: Assumption 4.8 ensures this for m= 1 and M = 1 .
b2 a2
H¨older’s defect then implies the following:
J (cid:0) log;P Ψn+1|z(cid:1) = 1 2µ 1(d,z)Var
ψ
n+1∼PΨn+1|z(cid:2) Lψ n+1(d,θ⋆)(cid:3) (12)
for a scalar µ that depends on d and z, and
1
(cid:20) (cid:21)
J (cid:0) log;P Ψn+1(cid:1) = 1 2µ 2(d,z)Var
ψ n+1∼PΨn+1
θ∼E Θ(cid:2) Lψ n+1(d,θ)(cid:3) (13)
for a scalar µ that depends on d and z.
2
We can now formally state Assumption A.6:
Assumption A.6 (Sufficiently informative proxy). The proxy is sufficiently informative in the sense that
the following condition holds on the relative variance of Ψ |z and Ψ :
n+1 n+1
d,z∼E PD⋆,Z(cid:20) µ 2(d,z)Var
ψ n+1∼PΨn+1
(cid:20) θ∼E Θ(cid:2) Lψ n+1(d,θ)(cid:3)(cid:21)(cid:21) − d,z∼E PD⋆,Z(cid:104) µ 1(d,z)Var
ψ
n+1∼PΨn+1|z(cid:2) Lψ n+1(d,θ⋆)(cid:3)(cid:105) ≥0.
Direct substitution of the condition in Assumption A.6 into Equations (12) and (13) completes the
proof.
In addition to Lemma A.4, the proof of Theorem 4.9 uses the following assumption, which is a variant
of Assumption A.3 for the reweighted case:
Assumption A.7 (Smoothness in parameter space). There exists some ϵ>0 such that
(cid:20) (cid:18) (cid:19)(cid:21)
E log E (cid:2) Lψ n+1(d,θ)(cid:3)
d,ψ n+1∼PD⋆,Ψn+1 θ∼PΘ
(cid:34)(cid:32) (cid:33) (cid:32) (cid:33) (cid:32) (cid:33)(cid:35)
(cid:90) (cid:90)
≥ E p(θ) dθ log(cid:0) Lψ n+1(d,θ⋆)(cid:1) + p(θ) dθ log E (cid:2) Lψ n+1(d,θ)(cid:3)
d,ψ n+1∼PD⋆,Ψn+1 Nϵ(θ⋆) T\Nϵ(θ⋆) θ∼P ΘT\Nϵ(θ⋆)
(cid:16) (cid:17) (cid:16) (cid:17)
(cid:82) (cid:82)
where p(θ) dθ and p(θ) dθ are the probability that a value θ is inside and outside the
Nϵ(θ⋆) T\Nϵ(θ⋆)
ϵ-neighborhood of θ⋆, respectively.
Remark on Assumption A.7. In addition to the smoothness condition on the likelihood imposed by
Assumption A.3, Assumption A.7 additionally imposes what is essentially a ceiling on the outputs of the
relevance function. As discussed in Section 2, weights <1 “flatten”, or smooth out, the likelihood function;
weights >1 “sharpen” it, and may cause violation of Assumption A.7 even in cases where Assumption A.3
is met. The relevance functions used in our simulations (Section 6) output weights <1.
19We obtain
IGR(θ⋆)= E
 log

E
ψ
n+1∼PΨn+1|z(cid:2) L (cid:104)ψ n+1(d,θ⋆)(cid:3) (cid:105) 

d,z∼PD⋆,Z E
θ,ψ′ n+1∼PΘ,Ψn+1
Lψ′ n+1(d,θ)
(cid:34) (cid:35) (cid:34) (cid:20) (cid:18) (cid:19)(cid:21)(cid:35)
(Lemma A.4) ≤ E E (cid:2) log(cid:0) Lψ n+1(d,θ⋆)(cid:1)(cid:3) − E E log E (cid:2) Lψ n+1(d,θ)(cid:3)
d,z∼PD⋆,Z ψ n+1∼PΨn+1|z d∼PD⋆ ψ n+1∼PΨn+1 θ∼PΘ
(cid:34) (cid:35) (cid:34) (cid:20) (cid:18) (cid:19)(cid:21)(cid:35)
(Assumption 4.7) = E E (cid:2) log(cid:0) Lψ n+1(d,θ⋆)(cid:1)(cid:3) − E E log E (cid:2) Lψ n+1(d,θ)(cid:3)
d∼PD⋆ ψ n+1∼PΨn+1 d∼PD⋆ ψ n+1∼PΨn+1 θ∼PΘ
(cid:20) (cid:18) (cid:19)(cid:21)
= E log(cid:0) Lψ n+1(d,θ⋆)(cid:1) −log E (cid:2) Lψ n+1(d,θ)(cid:3)
d,ψ n+1∼PD⋆,Ψn+1 θ∼PΘ
(cid:34) (cid:32) (cid:33)
(cid:90)
(Assumption A.7)≤E
d,ψ n+1∼PD⋆,Ψn+1
log(cid:0) Lψ n+1(d,θ⋆)(cid:1) − Nϵ(θ⋆)p(θ) dθ log(cid:0) Lψ n+1(d,θ⋆)(cid:1) −
(cid:32) (cid:33) (cid:32) (cid:33)(cid:35)
(cid:90)
p(θ) dθ log E (cid:2) Lψ n+1(d,θ)(cid:3)
T\Nϵ(θ⋆) θ∼P ΘT\Nϵ(θ⋆)
(cid:34) (cid:34)(cid:32) (cid:33)(cid:32) (cid:32) (cid:33)(cid:33)(cid:35)(cid:35)
(cid:90)
= E E p(θ) dθ log(cid:0) Lψ n+1(d,θ⋆)(cid:1) −log E (cid:2) Lψ n+1(d,θ)(cid:3)
ψ n+1∼PΨn+1 d∼PD⋆ T\Nϵ(θ⋆) θ∼P ΘT\Nϵ(θ⋆)
(cid:32) (cid:33)
(cid:90) (cid:104) (cid:16) (cid:17) (cid:16) (cid:17)(cid:105)
= p(θ) dθ E H P || P −H P || P
T\Nϵ(θ⋆) ψ n+1∼PΨn+1
D⋆ DR(ψn+1)|θ∈T\Nϵ(θ⋆) D⋆ DR(ψn+1)|θ⋆
(cid:32) (cid:33)
(cid:90) (cid:104) (cid:16) (cid:17)
= p(θ) dθ E H(P )+D P || P
T\Nϵ(θ⋆)
ψ n+1∼PΨn+1 D⋆ KL D⋆ DR(ψn+1)|θ∈T\Nϵ(θ⋆)
(cid:16) (cid:17)(cid:105)
−H(P )−D P || P
D⋆ KL D⋆ DR(ψn+1)|θ⋆
(cid:32) (cid:33)
(cid:90) (cid:104) (cid:16) (cid:17) (cid:16) (cid:17)(cid:105)
= p(θ) dθ E D P || P −D P || P
T\Nϵ(θ⋆) ψ n+1∼PΨn+1
KL D⋆ DR(ψn+1)|θ∈T\Nϵ(θ⋆) KL D⋆ DR(ψn+1)|θ⋆
(14)
(cid:16) (cid:17) (cid:104) (cid:16) (cid:17)(cid:105)
asstatedinthetheoremforA= (cid:82) p(θ) dθ andC =E D P || P .
T\Nϵ(θ⋆) ψ n+1∼PΨn+1 KL D⋆ DR(ψn+1)|θ∈T\Nϵ(θ⋆)
A.5 Proof of Proposition 4.10
The proof depends on the following definition:
Definition A.8 (Fidelity of the relevance function (ρR)). ρR is a measure of the fidelity of the relevance
function. More specifically, it is:
(cid:34) n (cid:32) n (cid:33)
1 (cid:88) 1 (cid:88)
ρR ≡E R (ψ )− R (ψ )
d,ψ n+1∼PD⋆,Ψn+1 n i n+1 n i n+1
i=1 i=1
(cid:32) n (cid:33)(cid:35)
log(cid:0) p(cid:0) d |θ⋆,ψ =ψ (cid:1)(cid:1) − 1 (cid:88) log(cid:0) p(cid:0) d |θ⋆,ψ =ψ (cid:1)(cid:1) ,
i i n+1 n i i n+1
i=1
i.e., is the covariance of R (ψ ) and log(cid:0) p(cid:0) d |θ⋆,ψ =ψ (cid:1)(cid:1) with respect to a uniform distribution
i n+1 i i n+1
over the source data, in expectation over P .
D⋆,Ψn+1
20∆R can be rewritten as
(cid:20) (cid:20) (cid:18) L(d,θ⋆,ψ⋆)(cid:19)(cid:21)(cid:21)
∆R = E E log
ψ n+1∼PΨn+1 d∼PD⋆
Lψ n+1(θ⋆)
=− E (cid:2) log(cid:0) Lψ n+1(θ⋆)(cid:1)(cid:3) +H(P D⋆)
d,ψ n+1∼PD⋆,Ψn+1
(cid:34) n (cid:35)
=− E (cid:88) R (ψ )log(cid:0) p(cid:0) d |θ⋆,ψ =ψ (cid:1)(cid:1) +H(P )
i n+1 i i n+1 D⋆
d,ψ n+1∼PD⋆,Ψn+1
i=1
(cid:34)(cid:32) n (cid:33)(cid:32) n (cid:33)(cid:35)
= E (cid:88) R (ψ ) −(cid:88) log(cid:0) p(cid:0) d |θ⋆,ψ =ψ (cid:1)(cid:1) −nρR+H(P ) (15)
i n+1 i i n+1 D⋆
d,ψ n+1∼PD⋆,Ψn+1
i=1 i=1
as stated in the proposition for f(d,ψ)=−(cid:80)n log(p(d |θ⋆,ψ =ψ)) and D =H(P ).
i=1 i i D⋆
B Details of Synthetic Examples
We here report the details of the synthetic examples described in Section 6. We used the same methods,
described here, to evaluate the relevance function and generate synthetic proxy information. Appendix B.1
gives additional details specific to the linear regression example, and Appendix B.2 gives additional details
specific to the GP regression example and additional results showing the relative performance of PROMPT
as a function of the values of each of several simulation parameters.
Relevance function. We computed R (ψ) ∝ p(d |ψ) where the constant of proportionality was the
i i
probability a distribution with the same variance would assign to its mode. The relevance weights were
iteratively computed as described in Section 3.2.
More specifically, we first computed p(d|ψ) using the learner’s prior over the shared parameter, i.e.,
p(d|ψ)=E [p(d|θ,ψ)]. Using the calculated relevance weights, we computed the r-weighted posterior
θ∼PΘ
PR . WethenapproximatedP(cid:98)R asaGaussianusingamoment-matchingapproximationtosamples
Θ,Ψn+1|d,z Θ
from the r-weighted posterior. We then recomputed the weights using P(cid:98)R to evaluate the expectation in
Θ
p(d|ψ), i.e., p(d|ψ)≈E [p(d|θ,ψ)], and recomputed the r-weighted posterior.
θ∼P(cid:98)ΘR
Inthelinearregressionsetting,ineachsimulation,werepeatedthisthreetimesbeforeultimatelydefining
the relevance function as an expectation over the distribution P(cid:98)R obtained at the final iteration. In the
Θ
GP regression setting, we varied the number of iterations used for refinement of the relevance function (see
Appendix B.2).
Proxy information. In many settings, feedback from domain experts is a viable source of proxy infor-
mation. While domain experts may not be able to articulate precise knowledge of the target task, they can
often provide intuitive assessments (Kahneman and Klein, 2009), such as the degree to which an outcome is
representative of a given situation (Tversky and Kahneman, 1974). Inspired by this work, we intended the
synthetic proxy information source in both settings to represent a domain expert, who is presented with a
hypothetical outcome and asked the degree to which it is representative of the target task.
Proxy values were generated as z ∼Binomial(cid:0) N,p(d |ψ⋆ )(cid:1) . This has one interpretation as simulated
i i n+1
responsesonaLikertscale,i.e.,toasituationwhereadomainexpertisaskedtojudgetherepresentativeness
of a data point to the target environment on a scale of 0−N. In all simulations, N =7.
B.1 Linear regression
All simulations were run using only a CPU. In all simulations, the value of the transferable parameter
θ⋆ =−1. The prior P =N (cid:0) [0,0]⊤,diag([1,1])(cid:1) for all i∈1:n+1.
Θ,Ψi
To generate source data, we first specified a particular level of multicollinearity ρ. A higher degree of
multicollinearity makes θ⋆ and ψ⋆ harder to separately identify, so we interpret this as a higher risk of
n+1
negative transfer. We varied ρ among 0 (no multicollinearity), 1 (mild multicollinearity), and 2 (extreme
multicollinearity).
21For a given value ρ we sampled values x′ ∼ N (ρ,.25), and then constructed values x ∼ N (x′,.25)
(·,1)
(cid:16) (cid:17)
and values x ∼ N
−ρ2
,.25 . We created 100 such data points. Twenty-five of these data points were
(·,2) x′
used to create proxy information (i.e., used to generate values z as described above), and 75 were used as
i
outcome information on the basis of which to estimate the transferable parameter.
B.2 Gaussian Process regression
EachsimulationwasrunonasingleNvidiaA100GPU.Thesetofsimulationsrununderonesetofsimulation
parameters did not complete successfully. The priors were P =Lognormal(1,1) and P =Gamma(3,.8)
Θ Ψi
for all i∈1:n+1. We varied the following simulation parameters:
• Trade-off between outcome and proxy information: Ineachsimulation,someofthe80trajecto-
ries were retained to create synthetic proxy information as described above, and the rest were used for
estimation of the target parameter. This simulation parameter, which took values in {.1,.2,.3,.4,.5},
controlled the percentage of trajectories retained to create synthetic proxy information.
• Covariate resolution: Trajectories were evaluated on a grid of evenly-spaced values x ranging from
0 to 1. This parameter, which took values in {5,10,20,30,40,50}, controlled the resolution and size of
that grid.
• Number of iterations for refinement of the relevance function: This parameter, which took
values in {0,1,2,3}, controlled the number of iterations used for refinement of the relevance function
as described above.
• Source domain distribution: This parameter, which took values in {10,20,30,40,50,60,70}, con-
trolledthenumberoftrajectoriesinthesourcedatageneratedbythetargettask. Whenmtrajectories
are generated by the target task, 80−m tasks have a task parameter sampled at random from the
learner’s prior. In this sense, the results are a somewhat conservative test of PROMPT.
• Value of θ⋆: We set θ⋆ to either 1 (left tail of P ), e (mode of P ), or 6 (right tail of P ).
Θ Θ Θ
Additional results. Figure 4 shows how the relative performance of PROMPT depends on the value
of each of the simulation parameters listed above. Figure 4b shows that PROMPT’s advantage is more
pronouncedforhighercovariateresolutions. Whenthecovariateresolutionislow,thecovariatesarerelatively
farapartandsoallobservationswillberelativelyuncorrelatedregardlessofthevalueofthesharedandtask
parameters. We speculate that the result that in these cases the classic learner performs on par with the
learnerwithanr-weightedlikelihoodreflectsthatinbothcasesobservationsprovidelittleinformationabout
the smoothness of the underlying function, and so both methods gain equally little information about θ⋆.
References
RobertA.Becker. Thevariancedrainandjensen’sinequality,2012. Accessedviahttps://caepr.indiana.
edu/RePEc/inu/caeprp/caepr2012-004.pdf.
Daniel Kahneman and Gary Klein. Conditions for intuitive expertise: A failure to disagree. American
Psychologist, 64(6), 2009.
Sabina J. Sloman, Ayush Bharti, Julien Martinelli, and Samuel Kaski. Bayesian active learning in the pres-
enceofnuisanceparameters. InProceedingsofthe40th ConferenceonUncertaintyinArtificialIntelligence
(UAI 2024), 2024.
J. Michael Steele. The Cauchy-Schwarz Master Class. Cambridge University Press, 2004.
Amos Tversky and Daniel Kahneman. Judgment under uncertainty: Heuristics and biases. Science, 185,
1974.
2240 40 40
30 30 30
20 20 20
10 10 10
0 0 0
−10 −10 −10
0.1 0.2 0.3 0.4 0.5 5 10 15 20 25 30 0 1 2 3
Outcome/proxy information trade-off Covariate resolution
(a) Trade-off between outcome and (b) Covariate resolution. The advan- (c) Number of iterations for refine-
proxy information. tage is more pronounced for higher ment of the relevance function T.
covariate resolutions. See interpreta-
tion in the main text.
40 40
30 30
20 20
10 10
0 0
−10 −10
10 20 30 40 50 60 70 1 2.72 4.44 6.15
Target task cluster size
(d) Source domain distribution. (e) Value of θ⋆. The advantage is
more pronounced for lower values of
θ⋆. See discussion in Section 6.
Figure 4: Advantage of learning with an r-weighted likelihood in the GP regression setting as a function of
the simulation parameter indicated in the subfigure caption. Each box in the plot shows the interquartile
region(boxes)andoutliers(points)ofIGR(θ⋆)−IG(θ⋆)acrossallvaluesofallothersimulationparameters
× 50 simulations.
23