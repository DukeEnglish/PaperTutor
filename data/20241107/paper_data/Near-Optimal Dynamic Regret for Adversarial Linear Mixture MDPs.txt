Near-Optimal Dynamic Regret for Adversarial
Linear Mixture MDPs
Long-FeiLi,PengZhao,Zhi-HuaZhou
NationalKeyLaboratoryforNovelSoftwareTechnology,NanjingUniversity,China
SchoolofArtificialIntelligence,NanjingUniversity,China
{lilf, zhaop, zhouzh}@lamda.nju.edu.cn
Abstract
WestudyepisodiclinearmixtureMDPswiththeunknowntransitionandadver-
sarialrewardsunderfull-informationfeedback,employingdynamicregretasthe
performancemeasure. Westartwithin-depthanalysesofthestrengthsandlimita-
tionsofthetwomostpopularmethods:occupancy-measure-basedandpolicy-based
methods. Weobservethatwhiletheoccupancy-measure-basedmethodiseffective
inaddressingnon-stationaryenvironments,itencountersdifficultieswiththeun-
knowntransition. Incontrast,thepolicy-basedmethodcandealwiththeunknown
transitioneffectivelybutfaceschallengesinhandlingnon-stationaryenvironments.
Buildingonthis,weproposeanovelalgorithmthatcombinesthebenefitsofboth
methods. Specifically,itemploys(i)anoccupancy-measure-basedglobaloptimiza-
tionwithatwo-layerstructuretohandlenon-stationaryenvironments;and(ii)a
policy-basedvariance-awarevalue-targetedregressiontotackletheunknowntran-
sition. Webridgethesetwopartsbyanovelconversion. Ouralgorithmenjoysan
√
O(cid:101)(d H3K+(cid:112) HK(H +P¯ K))dynamicregret,wheredisthefeaturedimension,
H istheepisodelength,K isthenumberofepisodes,P¯ isthenon-stationarity
K
measure. Weshowitisminimaxoptimaluptologarithmicfactorsbyestablishing
amatchinglowerbound. Tothebestofourknowledge,thisisthefirstworkthat
achievesnear-optimaldynamicregretforadversariallinearmixtureMDPswith
theunknowntransitionwithoutpriorknowledgeofthenon-stationaritymeasure.
1 Introduction
ReinforcementLearning(RL)studiestheproblemwherealearnerinteractswiththeenvironments
andaimstomaximizethecumulativereward[SuttonandBarto,2018],whichhasachievedsignificant
successingames[Silveretal.,2016],robotics[Koberetal.,2013],largelanguagemodel[Ouyang
etal.,2022]andsoon. TheinteractionisusuallymodeledasMarkovDecisionProcesses(MDPs).
ResearchonMDPscanbebroadlydividedintotwolinesbasedontherewardgenerationmechanism.
Thefirstlineofwork[Jakschetal.,2010,Azaretal.,2013,2017,Heetal.,2021]considersthe
stochasticMDPswheretherewardissampledfromafixeddistribution. Inmanyreal-worldscenarios,
however,theassumptionoffixedrewarddistributionsmaynothold,asrewardscanvaryovertime.
ThismotivatesthestudyonadversarialMDPs[Even-Daretal.,2009,Yuetal.,2009,Ziminand
Neu,2013,Jinetal.,2020a],whererewardsmightchangeinanadversarialmanner. Toaddressthe
challengesoflarge-scaleMDPs,recentstudieshaveextendedthesetwoframeworkstoincorporate
functionapproximation,allowingRLalgorithmstohandlelargestateandactionspaces. Twopopular
modelsarelinearmixtureMDPs [Ayoubetal.,2020]andlinearMDPs[Jinetal.,2020b].
Inthiswork,wefocusonlinearmixtureMDPswithadversarialrewards,unknowntransitionand
full-informationfeedback. Thoughsignificantadvanceshavebeenachievedforthissetting[Caietal.,
2020,Heetal.,2022],theychoosestaticregretastheperformancemeasure,whichbenchmarksthe
38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024).
4202
voN
5
]GL.sc[
1v70130.1142:viXraTable1: Comparisonsofdynamicregretguaranteeswithpreviousworksstudyingadversariallinear
mixtureMDPswiththeunknowntransitionandfull-informationfeedback. Here,disthefeature
mappingdimension,H istheepisodelength,K isthenumberofepisodes,P andP¯ aretwokinds
K K
ofnon-stationaritymeasuredefinedin(3)satisfyingP¯ ≤HP [Zhaoetal.,2022,Lemma6].
K K
Reference DynamicRegret P orP¯
K K
Zhongetal.[2021]
O(cid:101)(cid:0) dH7/4K3/4+H2K2/3P1/3(cid:1)
Known
√ K
Lietal.[2023]
O(cid:101)(cid:0)
d H3K
√+H2(cid:112)
(K+P K)(1+P
K)(cid:1)
Known
Lietal.[2024b] O(cid:101)(cid:0) dHS K+(cid:112) HK(H +P¯ K)(cid:1) Unknown
√
UpperBound(Theorem1) O(cid:101)(cid:0) d H3K+(cid:112) HK(H +P¯ K)(cid:1) Unknown
√
LowerBound(Theorem2) Ω(cid:0) d H3K+(cid:112) HK(H +P¯ )(cid:1) /
K
learner’spoliciesπ ,...,π againstthebest-fixedpolicyinhindsight,namely,
1 K
K K
(cid:88) (cid:88)
Reg =max Vπ (s )− Vπk(s ), (1)
K k,1 k,1 k,1 k,1
π∈Π
k=1 k=1
where Vπ (s ) is the expected cumulative reward of policy π starting from initial state s at
k,1 k,1 k,1
episodekandΠisthepolicyset. WhilestaticregretisanaturalchoiceforonlineMDPs,thebest-
fixedpolicymayperformpoorlywhentherewardschangeadversarially. Tothisend,anenhanced
measurecalleddynamicregretisproposedintheliterature[Zhaoetal.,2022,Lietal.,2023],which
benchmarksthelearner’spoliciesagainstasequenceofchangingpolicies. Thismeasureisdefinedas
K K
D-Reg (πc )=(cid:88) Vπ kc (s )−(cid:88) Vπk(s ), (2)
K 1:K k,1 k,1 k,1 k,1
k=1 k=1
whereπc,...,πc isanysequenceofpoliciesinthepolicysetthatcanbechosenwithcomplete
1 K
foreknowledge of online reward functions. The dynamic regret in (2) is a stronger notation as it
recoversthestaticregretin(1)directlybysettingπc ∈argmax (cid:80)K Vπ (s ). Anideal
1:K π∈Π k=1 k,1 k,1
dynamicregretboundshouldscalewithacertainvariationquantityofcomparedpoliciesdenotedby
P (πc,...,πc )orP¯ (πc,...,πc )thatcanreflectthedegreeofenvironmentalnon-stationarity.
K 1 K K 1 K
Whiletheflexibilityofdynamicregretmakesitwell-suitedforadversarialsettings,italsopresents
significantchallenges. ThedynamicregretoftabularMDPswithfull-informationfeedbackhasbeen
thoroughlystudiedbyZhaoetal.[2022]andLietal.[2024b],whoachievedoptimaldependence
onK andP¯ fortheknownandunknowntransitionsettings,respectively,withoutrequiringprior
K
knowledgeofthenon-stationaritymeasure. However,thedynamicregretofadversariallinearmixture
MDPsisstillunderstudied. Withthepriorknowledgeofthenon-stationaritymeasure,Zhongetal.
[2021]proposedapolicyoptimizationalgorithmwiththerestartstrategy[Zhaoetal.,2020],achieving
aresultwithsuboptimaldependenceinH,K andP . Later,Lietal.[2023]significantlyimproved
K
theirresultsbydesigninganalgorithmwiththeoptimaldynamicregretinK andP ,thoughthe
K
dependenceonH remainssuboptimal. Forthemorechallengingscenarioswherenon-stationarityis
unknown,Lietal.[2023]madeaninitialsolutionbyintroducingatwo-layerpolicyoptimization
algorithm,albeitwithanadditionalterminthedynamicregretinvolvingtheswitchingnumberofthe
bestbase-learner. Toaddressthislimitation,Lietal.[2024b]developedanoccupancy-measure-based
algorithmwithtwo-layerstructurethatachievesoptimaldynamicregretinK andP¯ . However,their
K
resultincursapolynomialdependenceonthestatespacesizeS,whichisstatisticallyundesirable.
Inthiswork,weproposeanalgorithmthatachievesthenear-optimaldynamicregretind,H,K and
P¯ simultaneouslyforadversariallinearmixtureMDPswiththeunknowntransition,withoutprior
K
knowledgeofthenon-stationaritymeasure. Webeginwithin-depthanalysesofthestrengthsand
limitationsoftwomostpopularmethods: occupancy-measure-basedandpolicy-basedmethods. We
findthatwhiletheoccupancy-measure-basedmethodiseffectiveinaddressingthenon-stationary
environments,itencounterdifficultieswiththeunknowntransition. Incontrast,thepolicy-based
method can deal with the unknown transition effectively but faces challenges in handling non-
stationaryenvironments. Tothisend,weproposeanovelalgorithmthatcombinesthebenefitsofboth
2methods. Specifically,ouralgorithmemploys(i)anoccupancy-measure-basedglobaloptimization
with a two-layer framework to handle the non-stationary environments; and (ii) a policy-based
variance-awarevalue-targetedregressiontohandletheunknowntransition.Webridgethesetwoparts
√
throughanovelconversion. WeshowouralgorithmachievesanO(cid:101)(d H3K+(cid:112) HK(H +P¯ K))
dynamicregretandproveitisminimaxoptimaluptologarithmicfactorsbyestablishingamatching
lowerbound. Table1presentsthecomparisonbetweenourresultandpreviousworks. Ourresult
surpassesallpreviousresults,eventhosethatrequirepriorknowledgeofthenon-stationaritymeasure.
WenoteasimilarcombinationwasfirstlyemployedinJietal.[2024],butfordistinctlydifferent
purposes. Intheirwork,theoccupancy-measure-basedcomponentisusedtoprovideahorizon-free
(independentofhorizonlengthH)staticregretwhereasourobjectiveistoaddressnon-stationary
environments. One limitation of this hybrid approach is the computational complexity, which is
dominatedbytheoccupancy-measure-basedcomponentandthusexpensivecomparedtopolicy-based
method. Thisissueistheinherentchallengeforoccupancy-measure-basedmethodandalsoappears
in several studies [Zhao et al., 2023, Ji et al., 2024]. Nevertheless, our analyses suggest that the
occupancy-measure-basedmethodoffersuniqueadvantagesinhandlingnon-stationaryenvironments.
Investigatingwhethersimilarresultscanbeattainedbyothercomputationallyefficientmethodsisan
importantfuturework. Webelieveourworkrepresentsasignificantstepforward,asitisreasonable
toprioritizeachievingstatisticaloptimalitybeforefocusingoncomputationalefficiency.
Organization. WereviewtherelatedworkinSection2andformulatesthesetupinSection3. We
analyzethechallengesandintroduceouralgorithminSection4andpresentthedynamicregretin
Section5. Section6concludesthepaper. Duetothepagelimits,wedeferallproofstotheappendices.
Notations. Wedenoteby[n]theset{1,...,n}anddefine[x] =min{max{x,a},b}. Forvector
[a,b] √
x∈Rdandpositivesemi-definitematrixΣ∈Rd×d,define∥x∥ = x⊤Σx. Forpoliciesπandπ′,
Σ
define∥π−π′∥
1,∞
=max s∥π(·|s)−π′(·|s)∥ 1. ThenotationO(cid:101)(·)hidesallpolylogarithmicfactors.
2 RelatedWork
Inthissection,wereviewrelatedworksonthedynamicregretofMDPsinnon-stationaryenviron-
ments. Thestudiescanbedividedintotwolines: non-stationarystochasticMDPsandnon-stationary
adversarialMDPs. Thesetwocategoriesaddressdistinctchallengesandarestudiedseparately.
Non-stationaryStochasticMDPs. Non-stationarystochasticMDPsaddressscenarioswheretran-
sitionsandrewardsarestochasticallygeneratedfromvaryingdistributions. Thenon-stationarity
measureistypicallydefinedasthetotalvariationofthetransitionsorrewardsovertime. Forinfinite-
horizonMDPs,theseminalworkofJakschetal.[2010]investigatesthepiecewisestationarysetting
whereboththetransitionsandrewardsaresubjecttochangesatspecifictimeandremainfixedin
between. Ortneretal.[2019]furtheradvancethefieldbyallowingforchangesateverystep. Subse-
quently,Cheungetal.[2020]introducetheBandit-over-RLalgorithm,whichaddressesthelimitations
ofearlierworksbyeliminatingtheneedforpriorknowledgeaboutthenon-stationarity. Additional
advancementshavebeenmadeinepisodicnon-stationaryMDPs[Maoetal.,2021,Dominguesetal.,
2021]andepisodicnon-stationarylinearMDPs[TouatiandVincent,2020,Zhouetal.,2022]. A
breakthroughistheblack-boxmethodbyWeiandLuo[2021],whichcantransformanyalgorithm
withtheoptimalstaticregretundercertainconditions,intoanotheronethatachievesoptimaldynamic
regretwithoutpriorknowledgeofthenon-stationarity. However,thismethodisinapplicableinadver-
sarialsettings. Thelimitationarisesfromitsdependenceonanoptimisticestimator,constructedviaa
UpperConfidenceBound(UCB)-basedalgorithmforenvironmentalchangedetection,atechnique
thatperformswellinstochasticenvironmentsbutislesseffectiveinadversarialscenarios.
Non-stationary Adversarial MDPs. Non-stationary adversarial MDPs consider settings where
therewardsaregeneratedinanadversarialmanner. Thenon-stationaritymeasureisdefinedasthe
variationofarbitrarychangingcomparedpolicies,allowingthepolicytoadapttonon-stationary
environmentsimplicitly. Anillustrativedifferencebetweennon-stationarystochasticandadversarial
MDPsisthat,insomecases,eveniftherewardsandtransitionschangeovertime,theoptimalpolicy
mayremainfixed. ThedynamicregretoftabularMDPswithfull-informationfeedbackhasbeen
thoroughlystudiedbyZhaoetal.[2022]andLietal.[2024b],whoachievedoptimaldependence
onK andP¯ fortheknownandunknowntransitionsettingsrespectively,withoutrequiringprior
K
knowledgeofthenon-stationaritymeasure. However,thedynamicregretofadversariallinearmixture
3MDPsisstillunderstudied. Withthepriorknowledgeaboutthenon-stationaritymeasure,Zhongetal.
[2021]proposedapolicyoptimizationalgorithmwithrestartstrategy[Zhaoetal.,2020],achievinga
resultwithsuboptimaldependenceinH,K andP . Later,Lietal.[2023]significantlyimproved
K
theirresultsbydesigninganalgorithmwithoptimaldynamicregretintermsofK andP ,though
K
thedependenceonH remainssuboptimal. Forthemorechallengingscenarioswherenon-stationarity
isunknown,Lietal.[2023]madeaninitialsolutionbyintroducingatwo-layerpolicyoptimization
algorithm,albeitwithanadditionalterminthedynamicregretinvolvingtheswitchingnumberofthe
bestbase-learner. Toaddressthislimitation,Lietal.[2024b]developedanoccupancy-measure-based
algorithm with two-layer structure [Zhang et al., 2018, Yan et al., 2023, Zhao et al., 2024] that
achievesoptimaldynamicregretinK andP¯ . However,theirdynamicregretincursapolynomial
K
dependenceonthestatespacesizeS,whichisundesirable. Inthiswork,weproposeanalgorithm
thatachievesnear-optimaldynamicregretind,H,K andP¯ simultaneouslyforadversariallinear
K
mixtureMDPswiththeunknowntransition,withoutpriorknowledgeofthenon-stationaritymeasure.
3 ProblemSetup
WefocusonepisodicMDPswiththeunknowntransitionandadversarialrewardfunctionsinthe
full-informationfeedbacksetting. Weintroducetheproblemformulationinthefollowing.
Inhomogeneous,EpisodicAdversarialMDPs. Wedenoteaninhomogeneous,episodicadversarial
MDPbyatupleM = {S,A,H,{P } ,{r } }, whereS isthestatespacewith
h h∈[H] k,h k∈[K],h∈[H]
cardinality|S|=S,Aistheactionspacewithcardinality|A|=A,H isthelengthofeachepisode,
P (·|·,·):S×A×S →[0,1]isthetransitionwithP (s′|s,a)denotingtheprobabilityoftransiting
h h
tostates′giventhestatesandactionaatstageh,andr :S×A→[0,1]istherewardfunction
k,h
for episode k at stage h chosen by the adversary. A policy π = {π }H is a collection of h
h h=1
functions,whereeachπ :S →∆(A)mapsastatestoadistributionoveractionspaceA. Forany
h
(s,a)∈S×A,thestate-actionvaluefunctionQπ (s,a)andvaluefunctionVπ (s)aredefinedas:
k,h k,h
(cid:34) (cid:88)H (cid:12) (cid:35)
Qπ (s,a)=E r (s ,a )(cid:12)s =s,a =a , Vπ (s)=E [Qπ (s,a)],
k,h π k,h′ h′ h′ (cid:12) h h k,h a∼πh(·|s) k,h
h′=h
wheretheexpectationistakenovertherandomnessofπ andP. ForanyfunctionV : S → R,we
define[P hV](s,a)=E
s′∼P
h(·|s,a)[V(s′)]and[V hV](s)=[P hV2](s,a)−([P hV](s,a))2.
Theinteractionprotocolisgivenasfollows. Atthebeginningofepisodek,theenvironmentchooses
therewardfunctions{r } anddecidestheinitialstates ,wheretherewardfunctionmay
k,h h∈[H] k,1
bechoseninanadversarialmanner. Simultaneously,thelearnerdecidesapolicyπ ={π } .
k k,h h∈[H]
Starting from the initial state s , the learner chooses an action a ∼ π (·|s ), obtains the
k,1 k,h k,h k,h
rewardr (s ,a ),andtransitstothenextstates ∼ P (·|s ,a )forh ∈ [H]. After
k,h k,h k,h k,h+1 h k,h k,h
the episode k ends, the learner observes the entire reward function {r } . The goal of the
k,h h∈[H]
learneristominimizethedynamicregretin(2). DenotebyT =KH thetotalsteps.
LinearMixtureMDPs. WefocusonlinearmixtureMDPs,whichwasintroducedbyAyoubetal.
[2020]andhasbeenstudiedbysubsequentworks[Caietal.,2020,Zhouetal.,2021,Lietal.,2024c].
Definition1(LinearMixtureMDPs). AnMDPM ={S,A,H,{P } ,{r } }is
h h∈[H] k,h k∈[K],h∈[H]
calledaninhomogeneous,episodeB-boundedlinearmixtureMDP,ifthereexistaknownfeature
mappingϕ(s′|s,a):S×A×S →Rdandanunknownvectorθ∗ ∈Rdwith∥θ∗∥ ≤B,∀h∈[H],
h h 2
suchthat(i)P (s′|s,a)=ϕ(s′|s,a)⊤θ∗,(ii)∥ϕ (s,a)∥ ≜∥(cid:80) ϕ(s′|s,a)V(s′)∥ ≤1forany
h h V 2 s′∈S 2
(s,a)∈S×AandanyboundedfunctionV :S →[0,1].
OccupancyMeasure. Weintroducetheconceptofoccupancymeasure[Altman,1998,Ziminand
Neu,2013]. GivenapolicyπandatransitionP,theoccupancymeasureqisdefinedastheprobability
ofvisitingstate-action-statetriple(s,a,s′)undertransitionPandpolicyπ,thatis,
qP,π(s,a,s′)=Pr[s =s,a =a,s =s′|P,π].
h h h h+1
Avalidoccupancymeasureq ={q }H satisfiesthefollowingproperties. First,eachstageisvisited
exactly once and thus ∀h ∈ [H],h (cid:80)h=1 (cid:80) (cid:80) q (s,a,s′) = 1. Second, the probability
s∈S a∈A s′∈S h
ofenteringastatewhencomingfromthepreviousstageequalstotheprobabilityofleavingfrom
thatstatetothenextstage,i.e.,∀s ∈ S,(cid:80) (cid:80) q (s,a,s′) = 1{s = s }and∀h ∈ [2,H],
a∈A s′∈S 1 1
4(cid:80) (cid:80) q (s,a,s′)=(cid:80) (cid:80) q (s′′,a,s). Foranyoccupancymeasureqsatisfying
a∈A s′∈S h s′′∈S a∈A h−1
theabovetwoproperties,itinducesatransitionPq ={Pq}H andapolicyπq ={πq}suchthat
h h=1 h
q (s,a,s′) (cid:80) q (s,a,s′)
Pq(s′|s,a)= h ,πq(a|s)= s′ h ,∀(s,a,s′,h)∈S×A×S×[H].
h (cid:80) q (s,a,s′′) h (cid:80) q (s,a′,s′)
s′′ h a′,s′ h
Wedenoteby∆thesetofalloccupancymeasuressatisfyingtheabovetwoproperties.Foratransition
P,denoteby∆(P)∈∆thesetofoccupancymeasureswhoseinducedtransitionPq isexactlyP. For
acollectionoftransitionsP,denoteby∆(P) ∈ ∆thesetofoccupancymeasureswhoseinduced
transitionPq isinthetransitionsetP. Weuseq
k
=qP,πk,q kc =qP,π kc tosimplifythenotation.
Non-stationaritymeasure. Thenon-stationaritymeasureaimstoquantifythenon-stationarityof
theenvironments. Weintroducetwokindsofnon-stationaritymeasureswidelyusedintheliterature:
K H K H
P =(cid:88)(cid:88) ∥πc −πc ∥ , P¯ =(cid:88)(cid:88) ∥qc −qc ∥ . (3)
K k,h k−1,h 1,∞ K k,h k−1,h 1
k=2h=1 k=2h=1
Theyquantifythedifferencebetweenthecomparedpoliciesandthecomparedoccupancymeasures,
respectively. Zhaoetal.[2022,Lemma6]showitholdsthatP¯ ≤ HP . Thus,wefocusonthe
K K
P¯ -typeupperboundinthisworkasitimpliesanupperboundintermsofHP directly.
K K
4 TheProposedAlgorithm
In this section, we first analyze the strengths and limitations of two most popular methods for
adversarialMDPs. Thenweproposeanovelalgorithmthatcombinesthebenefitsofbothapproaches.
4.1 AnalysisofTwoPopularMethods
Occupancy-measure-basedandpolicy-basedmethodsarethetwomostpopularapproachesforsolving
adversarialMDPs. Bothmethodshavebeenextensivelystudiedintheliteratureandshowntoenjoy
favorablestaticregretguarantees[ZiminandNeu,2013,Caietal.,2020]. However,whenitcomes
todynamicregret,bothmethodsfacesignificantchallenges. Weintroducethedetailsbelow.
4.1.1 FrameworkI:Occupancy-measure-basedMethod
Thefirstlineofwork[ZiminandNeu,2013,RosenbergandMansour,2019,Jinetal.,2020a]em-
ployedtheoccupancy-measure-basedmethodforadversarialMDPs. Thismethodusetheoccupancy
measureasaproxyforthepolicy,optimizingovertheoccupancymeasureratherthanthepolicydi-
rectly. Whilethevaluefunctionisnotconvexinthepolicyspace,itbecomesconvexintheoccupancy
measurespace,makingthisapproachtheoreticallymoreattractivecomparedtopolicy-basedmethod.
However,thisshiftintroducesnewchallengesfordynamicregretanalysis,asdiscussedbelow.
Using the concept of the occupancy measure, the dynamic regret in (2) can be rewritten as
D-Reg (πc ) = (cid:80)K Vπ kc (s )−(cid:80)K Vπk(s ) = (cid:80)K ⟨qc −q ,r ⟩. By this conver-
K 1:K k=1 k,1 k,1 k=1 k,1 k,1 k=1 k k k
sion,theonlineMDPproblemisreducedtothestandardonlinelinearoptimizationproblemover
theoccupancymeasureset∆(P)inducedbythetruetransitionP. However,thetruetransitionPis
unknownandthusthedecisionset∆(P)isinaccessible. Toaddressthisissue,ageneralideaisto
constructaconfidencesetP atepisodekthatcontainsthetruetransitionwithhighprobabilitythen
k
replacethedecisionsetby∆(P ). Thenthedynamicregretcanbedecomposedinto:
k
K K K
(cid:88) (cid:88) (cid:88)
D-Reg (πc )= ⟨qc −q ,r ⟩= ⟨qc −qˆ ,r ⟩+ ⟨qˆ −q ,r ⟩, (4)
K 1:K k k k k k k k k k
k=1 k=1 k=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
D-Regret-OLO approximation-error
whereqˆ isanoccupancymeasureinthedecisionset∆(P ). Thefirsttermisthedynamicregret
k k
ofonlinelinearoptimizationoverthedecisionset∆(P ),whichhasbeenexploredintheknown
k
transitionsetting[Zhaoetal.,2022]. Thistermcanbewellcontrolledbyatwo-layerstructure,as
demonstratedintheonlinelearningliterature[Zhangetal.,2018,Yanetal.,2023,Zhaoetal.,2024].
5ThesecondtermreflectstheapproximationerrorintroducedbyusingtheconfidencesetP asa
k
surrogateforthetruetransitionP,whichconstitutestheprimarychallengeofthisapproach.
KeyDifficulties. Tocontroltheapproximationerrorin(4),previousworks[RosenbergandMansour,
2019,Jinetal.,2020a]proposedtoboundtheterm(cid:80)K
∥qˆ −q ∥ ,leveragingHölder’sinequality:
k=1 k k 1
⟨qˆ −q ,r ⟩ ≤ ∥qˆ −q ∥ ∥r ∥ andr ∈ [0,1]SA. Whilethisapproachiseffectivefortabular
k k k k k 1 k ∞ k
MDPs,itfailstoexploittheinherentstructureoflinearmixtureMDPs. Althoughthetransitionkernel
P exhibit a linear structure, the occupancy measure is not linear and retains a complex recursive
form,ashighlightedinZhaoetal.[2023]. Consequently,theregretboundresultingfromthismethod
dependsonthestatespacesizeS,whichisundesirableforlinearmixtureMDPs.
Remark1. Occupancy-measure-basedmethodoptimizesaglobalobjectthatencodestheentire
policyacrossallstates,whichsacrificessomecomputationalefficiencytobetterhandlenon-stationary
environments. Thankstotheirglobaloptimizationproperty,thesemethodsofferfavorabledynamic
regretguarantees. However,theystruggletoaddresstheunknowntransitioninlinearmixtureMDPs
andsufferfromanundesirabledependenceonthestatespacesizeS inthedynamicregretbound.
4.1.2 FrameworkII:Policy-basedMethod
Policy-basedmethoddirectlyoptimizesthepolicy,makingiteasiertoimplementandcomputationally
moreefficientthanoccupancy-measure-basedmethod. Moreimportantly,itshowsadvantagesin
handlingunknowntransitions,asitonlyrequiresestimatingthevaluefunction,bypassingtheneed
toestimatethetransitionkernelexplicitly. However,duetotheirinherentlocal-searchnature,this
methodfaceschallengesinadaptingtonon-stationaryenvironmentsandprovidingfavorabledynamic
regretguarantees. Weoutlinethekeychallengesbelow.
Bytheperformancedifferencelemma[KakadeandLangford,2002,Caietal.,2020],thedynamic
regretin(2)canberewrittenasthefollowingformulation:
K H
D-Reg K(π 1c :K)=(cid:88)(cid:88) E
π
kc(cid:2) ⟨Qπ k,k h(s h,·),π kc ,h(·|s h)−π k,h(·|s h)⟩(cid:3) , (5)
k=1h=1
wheretheexpectationistakenoverthechangingpolicysequenceπc,...,πc . Foreachstates ,the
1 K h
term(cid:80)K (cid:80)H ⟨Qπ (s ,·),πc (·|s )−π (·|s )⟩isexactlytheregretofaA-armedbandit
k=1 h=1 k,h h k,h h k,h h
problem,withQπk (s ,·)beingthe“rewardvector”. Thus,itindicatesthedynamicregretofonline
k,h h
MDPscanbewrittenastheweightedaverageofMABdynamicregretoverallstates, wherethe
weightforeachstateisits(unknownandtime-varying)probabilityofbeingvisitedbyπc,...,πc .
1 K
Sincethetransitionisunknown,thepolicy-basedmethodneedtoestimatethevaluefunctionQπk at
k,h
eachstate. BythedefinitionoflinearmixtureMDPsinDefinition1,foranyV (·),itholdsthat
k,h
[P V ](s,a)=⟨ϕ (s,a),θ∗⟩. Therefore,learningtheunderlyingtransitionparametersθ∗
h k,h+1 Vk,h+1 h h
canberegardedassolvinga“linearbandit”problemwherethecontextisϕ (s ,a )and
Vk,h+1 k,h k,h
thenoiseisV (s ,a )−[P V ](s ,a ). Thisobservationenablestheapplicationof
k,h+1 k,h k,h h k,h+1 k,h k,h
“linearbandits”techniquestolearnthevaluefunctioneffectivelyforthepolicy-basedmethod.
Thekeychallengeforthepolicy-basedmethodliesinhandlingthenon-stationarityofenvironment.
EvenwithaccurateestimatesofQπk ateachstate,optimizingthedynamicregretremainsdifficult.
k,h
Tooptimize(5),Lietal.[2023]proposedrunningtheExp3algorithm[Aueretal.,2002]ateachstate
andemployingthefixed-sharetechnique[HerbsterandWarmuth,2001,Cesa-Bianchietal.,2012],
forcinguniformexplorationtodealwithnon-stationaryenvironments. Theyupdatepoliciesby
π′ (·|s)∝π (·|s)exp(cid:0) η·Qπk−1 (s,·)(cid:1) , π (·|s)=(1−γ)π′ (·|s)+γπu(·|s),
k,h k−1,h k−1,h k,h k,h
whereηisthestepsize,πu(·|s)isuniformdistribution,andγisthefixed-shareparameter.Theyshow
itensuresD-Reg K(π 1c :K)≤O(cid:101)(ηKH3+H(1+P K)/η). Toachieveafavorabledynamicregret,the
(cid:112)
stepsizeηneedstobesetasη ≈ (1+P )/(KH2),whichisimpracticalasthenon-stationarity
K
measureP isunknown. Thestandardapproachtoaddressthisissueintheonlinelearningliterature
K
istoadopttheonlineensembleframework[Zhaoetal.,2024]. However,thisapproachencounters
challengesinonlineMDPsasthedynamicregretin(5)involvestheexpectationofchangingpolicies,
whichdoesnotappearinthestandardonlinelearningsetting. Weelaborateonthisissuebelow.
Specifically,thestandardprocedureofthetwo-layerframeworkisasfollows. First,weconstructa
stepsizepoolH ={η ,...,η }(N =O(logK))todiscretizetherangeoftheoptimalstepsize.
1 N
6Subsequently,multiplebase-learnersB ,...,B aremaintained,witheachassociatedwithastep
1 N
sizeη ∈H. Finally,ameta-algorithmisemployedtotracktheunknownbestbase-learner. Then,the
i
dynamicregretcanbedecomposedintotwoparts: (i)thedynamicregretofthebestbase-learner;and
(ii)theregretofthemeta-algorithmtotrackthebest-learner. Formally,
K (cid:34) H (cid:35) K (cid:34) H (cid:35)
(cid:88) E π kc (cid:88) ⟨Qπ k,k h(s h,·),π kc ,h(·|s h)−π ki ,h(·|s h)⟩ +(cid:88) E π kc (cid:88) ⟨Qπ k,k h(s h,·),π ki ,h(·|s h)−π k,h(·|s h)⟩ .
k=1 h=1 k=1 h=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
base-regret meta-regret
KeyDifficulties. Thoughthebase-regretaboveinvolvestheexpectationofchangingpolicies,itcan
beeffectivelycontrolledasthedecompositionholdsforanybase-learnerB ,allowingustoselectthe
i
onewiththeoptimalstepsizeforanalysis. However,controllingthemeta-regretremainschallenging
asitstillinvolvestheexpectationofchangingpoliciesandtheoptimaltuningishindered.
Remark 2. By solely optimizing the policy at the visited states, the computational complexity
of the policy-based method is independent of the state number S, offering a notable advantage.
However,duetothenon-stationaryenvironments,morespecifically,changingweightsofdifferent
states, only caring about the visited states without considering their importance is not enough to
achievefavorabledynamicregret. Thus,thelocal-searchpropertyenhancescomputationalefficiency
butposesdifficultiesinhandlingnon-stationaryenvironments. Thisrevealsasignificanttrade-off
betweencomputationalefficiencyandtheabilitytomanagenon-stationaryenvironments.
4.2 OurMethod: ANovelCombination
BytheanalysisinSection4.1,weobservethattheoccupancy-measure-basedmethodisproficientin
addressingnon-stationaryenvironmentsbutshowslimitedcompatibilitywithunknowntransitions. In
contrast,thepolicy-basedmethodcandealwithunknowntransitionsefficientlybutfaceschallenges
inhandlingnon-stationaryenvironments.Tothisend,weproposeanewalgorithmnamedOccupancy-
measure-basedOptimizationwithPolicy-basedEstimation(OOPE),whichcombinesthebenefits
ofbothmethods. Atahighlevel,OOPEalgorithmconsistsoftwocomponents: (i)anoccupancy-
measure-basedglobaloptimizationwithatwo-layerframeworktodealwiththenon-stationarityof
environments;and(ii)apolicy-basedvalue-targetedregressiontohandletheunknowntransition.
Webridgethetwocomponentsthroughanovelanalysisthatconvertstheoccupancy-measure-based
approximationerrorintothepolicy-basedestimationerror. Weelaborateonthedetailsbelow.
4.2.1 Occupancy-measure-basedGlobalOptimization
Theoccupancy-measure-basedoptimizationfollowsLietal.[2024b],usingonlinemirrordescentfor
updatingtheoccupancymeasureandatwo-layerstructuretomanagenon-stationaryenvironments.
WefirstconstructastepsizepoolH={η ,...,η }todiscretizetherangeoftheoptimalstepsize,
1 N
thenmaintainmultiplebase-learnersB ,...,B ,eachofwhichisassociatedwithastepsizeη ∈H.
1 N i
Finally, we use a meta-algorithm to track the best base-learner. At each episode k, we construct
aconfidencesetC suchthatθ∗ ∈ C withhighprobability. Thedetailsoftheconstructionof
k h k,h
confidencesetwillbeintroducedlater. Then,thebase-learnerB updatestheoccupancymeasureby
i
qˆi = argmax η ⟨q,r ⟩−D (q∥qˆi ), (6)
k i k ψ k−1
q∈∆(Ck,α)
whereD (q∥q′)=(cid:80) q(s,a,s′)ln q(s,a,s′) istheKL-divergenceandthedecisionsetissetas
ψ s,a,s′ q′(s,a,s′)
∆(C ,α)={q ∈[α,1]S2A,∀h∈[H]|qsatisfiesconstraints(C1)and(C2)}where
k h
(C1):(cid:88) q (s,a,s′)=1{s=s },∀s∈S; (cid:88) q (cid:0) s,a,s′(cid:1) =(cid:88) q (cid:0) s′,a,s(cid:1) ∀(s,h)∈S×[2,H];
1 k,1 h h−1
a,s′ a,s′ a,s′
q (s,a,·)
(C2):∀(s,a,h)∈S×A×[H],∃θ∈C k,h, (cid:80) h q (s,a,s′) =⟨ϕ(·|s,a),θ⟩.
s′ h
Themeta-algorithmupdatesweightsby
pi ∝pi exp(ε⟨qˆi ,r ⟩), (7)
k k−1 k−1 k−1
whereε > 0isthelearningrate,⟨qˆi ,r ⟩evaluatestheperformanceofthebase-learnerB at
k−1 k−1 i
episodek−1. Thefinaloccupancymeasureisgivenbyqˆ =(cid:80)N piqˆi andthelearnerplaysthe
k i=1 k k
policyπqˆk. Algorithm1summarizesthedetails. Weshowitenjoysthefollowingguarantee.
7Algorithm1OOPE Algorithm2CompConfSet
Input: stepsizepoolH,learningrateε,and Input: Policyπ ,trajectoryU ,andrewardr .
k k k
clippingparameterα. 1: forh=H,H −1,··· ,1do
1: Setqˆi (s,a,s′)=1/(SAS),∀i∈[N]. 2: SetQ k,h(·,·)andV k,h(·)asin(9).
1,h
2: Setpi
1
=1/N,∀i∈[N]. 3: Σ(cid:98)k+1,h ←Σ(cid:98)k,h+σ¯ k− ,2 hϕ k,h,0ϕ⊤ k,h,0.
3: fork =1toK do 4: (cid:98)b k+1,h ←(cid:98)b k,h+σ¯ k− ,2 hϕ k,h,0V k,h+1(s k,h+1).
54
6
7::
:
:
R R
C
Plee
o
acc
m
yee pii pvv
u
oee
t le
ipqˆ ckki
qˆ
ykfb πry =o km( =(cid:80)6) m
πN
if e =r qˆto k1am .- pa
i
klB g qˆi o kir .f io thr mi∈ by[N (7] ). . 5
6
7:
:
:
Σ
(cid:101)b
θ(cid:98)(cid:101) kkk +++ 111
,,
h, hh ←←←
Σ(cid:101)
(cid:98)bΣ(cid:101)
k
−
k,k
+h
1,h
1+
,+
h(cid:98)bϕ
kϕ
k
+,k
h
1,h
, ,1
h, V1 .ϕ
k2
,⊤ k h, +h, 11.
(s k,h+1).
8: Observerewardr k andtrajectoryU k. 8: θ(cid:101)k+1,h ←Σ(cid:101)− k+1 1,h(cid:101)b k+1,h.
9: C k+1 ←CompConfSet(π k,U k,r k). 9: ComputeconfidencesetC k+1by(10).
10: endfor 10: endfor
Lemma1. Supposeθ∗ ∈C ,∀k ∈[K],h∈[H]. Settheclippingparameterα=1/T2,thestep
h k,h
sizepoolasH={η =2i−1(cid:112) K−1log(S2A/H)|i∈[N]},whereN =⌈1log(1+ 4KlogT )⌉+
i 2 log(S2A/H)
(cid:112)
1,andthelearningrateε= (logN)/(HT). Algorithm1ensuresthefollowingguarantee:
(cid:88)K
⟨qc −qˆ ,r
⟩≤O(cid:16)(cid:113)
T(Hlog(S2A)+P¯
logT)(cid:17)
.
k k k K
k=1
Remark3. Bytheoccupancy-measure-basedoptimizationwithatwo-layerstructure,wecanhandle
thefirsttermin(4)well.
Itremainstoboundtheapproximation-errorterm(cid:80)K
⟨qˆ −q ,r ⟩,
k=1 k k k
whicharisesfromemployingtheconfidencesetC asasurrogatefortruetransitionparameterθ∗.
k
ImplementationDetailsofAlgorithm1. Themaincomputationcomplexityarisesfromtheonline
mirrordescentstepof(6)inLine4. Thisstepcanbedividedintointoanunconstrainedoptimization
problemandaprojectionproblem. Theunconstrainedoptimizationproblemcanbesolvedbythe
closed-formsolutionandthemaincomputationalcostliesintheprojectionstep. Jietal.[2024]show
thatthoughsuchaprojectioncannotbeformulatedasalinearprogram,theycanbeefficientlysolved
bytheDysktra’salgorithmasthedecisionsetisanintersectionofconvexsetsofexplicitlinearor
quadraticforms. WereferthereaderstoAppendixDofJietal.[2024]formoredetails.
4.2.2 OccupancyMeasuretoPolicyConversion
AsdiscussedinSection4.1.1,previousworks[RosenbergandMansour,2019,Jinetal.,2020a,Li
etal.,2024b]proposetocontroltheapproximationerrorbyboundingtheterm(cid:80)K
∥qˆ −q ∥ .
k=1 k k 1
However, though the transition P admits a linear structure, the occupancy measure does not and
retainsacomplexrecursiveform,whichintroducesanundesireddependenceonthestatenumberS
inthefinalregret. Totakeadvantageofstrengthofpolicy-basedmethodinintegratingwithlinear
functionapproximation,weproposetolearnvaluefunctionsasawholeinsteadofdirectlycontrolling
theoccupancymeasurediscrepancies. Thisstrategydivergesfromtraditionalmethodsthatbound
⟨qˆ −q ,r ⟩bythetransitiondiscrepancies(cid:80)H (cid:80) ∥P (·|s,a)−P¯ (·|s,a)∥ ,whereP¯ is
k k k h=1 s,a h k,h 1 k
theestimatedtransitioninepisodek. Instead,weopttoconstrain⟨qˆ −q ,r ⟩throughthevalue
k k k
difference,whicheffectivelyintegratesrewardinformation. Weintroducethedetailsbelow.
DenotebyV(cid:98)k,1(s k,1)=(cid:80) h,s,a,s′qˆ k,h(s,a,s′)r k,h(s,a)theexpectedrewardgiventheoccupancy
measureqˆ . Then,theapproximationerrorcanberewrittenas
k,h
K K K
(cid:88) (cid:88)(cid:16) (cid:17) (cid:88)(cid:16) (cid:17)
⟨qˆ k−q k,r k⟩= V(cid:98)k,1(s k,1)−V k,1(s k,1) + V k,1(s k,1)−V kπ ,1k(s k,1) , (8)
k=1 k=1 k=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
occupancy-policy-gap estimation-error
whereV isanintermediatevaluewedefinelater. Ourkeyideaisbuildinganoptimisticestimator
k,1
V toensurethefirsttermisnon-positivewhilecontrollingthesecondterm. Thisbypassestheneed
k,1
toboundoccupancymeasurediscrepancies,allowingustofocussolelyonthevalueestimationerror.
84.2.3 Policy-basedValue-targetedRegression
ItremainstobuildthevaluefunctiontoensureV(cid:98)k,h ≤V k,h. Akeyobservationisthattheoccupancy
measureqˆ inducesanewMDPwhosetransitionliesintheconfidencesetC withhighprobability.
k k
Thus,itsufficestoensurethatV isanoverestimateofthetruevaluefunctionintheconfidenceset.
k,h
By the definition of linear mixture MDPs, for any V (·), it holds that [P V ](s,a) =
k,h h k,h+1
⟨ϕ (s,a),θ∗⟩. Thus,wecomputetheoptimisticestimationasfollows:
Vk,h+1 h
(cid:104) (cid:105)
Q (·,·)= r (·,·)+ max⟨θ,ϕ (·,·)⟩ , V (·)=E [Q (·,a)]. (9)
k,h k,h
θ∈Ck,h
Vk,h+1
[0,H]
k,h a∼πk,h(·|·) k,h
whereC istheconfidenceset. Weintroducethedetailsofconstructingtheconfidencesetbelow.
k,h
FollowingrecentadvancesinlinearmixtureMDPs[Zhouetal.,2021,Heetal.,2022],weestimate
theparameterθ∗ bytheweightedridgeregressiontoutilizethevarianceinformation. Denoteby
h
ϕ
k,h,0
=ϕ Vk,h+1(s k,h,a k,h)andϕ
k,h,1
=ϕ
V k2
,h+1(s k,h,a k,h). Weconstructtheestimatorθ(cid:98)k,has
θ(cid:98)k,h
=argmink (cid:88)−1 [⟨ϕ j,h,0,θ⟩− σ¯V 2j,h+1(s j,h+1)]2
+λ∥θ∥2 2,
θ∈Rd j=1 j,h
whereσ¯2 istheupperconfidenceboundofthevariance[V V ](s ,a )andissetasσ¯2 =
j,h h j,h+1 j,h j,h k,h
max{H2/d,[V¯ V ](s ,a )+E },where[V¯ V ](s ,a )isanestimateforthe
k,h k,h+1 k,h k,h k,h k,h k,h+1 k,h k,h
varianceofvaluefunctionV underthetransitionP (·|s ,a ),andE isthebonustermtoguar-
k,h+1 h k k k,h
anteethetruevariance[V V ](s ,a )isupperboundedby[V¯ V ](s ,a )+E
k,h k,h+1 k,h k,h k,h k,h+1 k,h k,h k,h
withhighprobability. Bydefinition,wehave[V V ](s ,a )=⟨ϕ ,θ∗⟩−[⟨ϕ ,θ∗⟩]2.
h k,h+1 k,h k,h k,h,1 h k,h,0 h
Thus,weset[V¯ k,hV k,h+1](s k,h,a k,h)=[⟨ϕ k,h,1,θ(cid:101)k,h⟩] [0,H2]−[⟨ϕ k,h,0,θ(cid:98)k,h⟩]2 [0,H],whereθ(cid:101)k,h is
usedtoestimatethesecond-ordermomentandisconstructedas:
k−1
(cid:88)
θ(cid:101)k,h =argmin [⟨ϕ j,h,1,θ⟩−V j2 ,h+1(s j,h+1)]2+λ∥θ∥2 2.
θ∈Rd
j=1
Theconfidencesetfortheparameterθ∗ isconstructedas
h
C
k,h
=(cid:8) θ |∥Σ(cid:98)1 k/ ,h2(θ−θ(cid:98)k,h)∥
2
≤β(cid:98)k(cid:9) . (10)
whereΣ(cid:98)k,hisacovariancematrix,andβ(cid:98)k isaradiusoftheconfidenceset.
ThealgorithmissummarizedinAlgorithm2. Weshowtheapproximationerrorisboundedasbelow.
Lemma2. Settheparametersasfollows:
√ √
β(cid:98)k
=8(cid:112)
dlog(1+k/λ)log(4k2H/δ)+4
dlog(cid:0) 4k2H/δ(cid:1)
+ λB.
√ √
β¯ =8d(cid:112) log(1+k/λ)log(4k2H/δ)+4 dlog(cid:0) 4k2H/δ(cid:1) + λB,
k
√
β(cid:101)k
=8H2(cid:112) dlog(1+kH4/(dλ))log(4k2H/δ)+4H2log(cid:0) 4k2H/δ(cid:1)
+ λB.
(cid:110) (cid:13) (cid:13) (cid:111) (cid:110) (cid:13) (cid:13) (cid:111)
E
k,h
=min H2,2Hβ¯ k(cid:13) (cid:13)Σ(cid:98)− k,1 h/2ϕ k,h,0(cid:13)
(cid:13)
+min H2,β(cid:101)k(cid:13) (cid:13)Σ(cid:101)− k,1 h/2ϕ k,h,1(cid:13)
(cid:13)
.
2 2
Algorithm2ensureswithprobabilityatleast1−δ,itholdsthat
(cid:88)K (cid:16)√ √ (cid:17)
⟨qˆ k−q k,r k⟩≤O(cid:101) d2H3K+ dH4K .
k=1
√
Remark4. ThisboundcanbefurthersimplifiedasO(cid:101)( d2H3K)whend ≥ H,whichisamild
assumption. Followingpreviouswork[Zhouetal.,2021],weonlydiscussthecased≥H below.
5 TheoreticalGuarantees
Inthissection,wepresentthedynamicregretupperboundandthelowerboundforthisproblem.
95.1 DynamicRegretUpperBoundandLowerBound
Thedynamicregretupperboundofouralgorithmisguaranteedbythefollowingtheorem.
Theorem1. SettheparametersasinLemma1andLemma2. Algorithm1withAlgorithm2asthe
subroutineensureswithprobabilityatleast1−δ,thedynamicregretisupperboundedby
(cid:16)√ (cid:113) (cid:17)
D-Reg K(π 1c :K)≤O(cid:101) d2H3K+ HK(H +P¯ K) .
√
Remark5.
ComparedwiththedynamicregretofO(cid:101)(cid:0) d2H3K+H2(cid:112)
(K+P K)(1+P
K)(cid:1)
for
knownnon-stationaritymeasurecasesinLietal.[2023],ourboundhasabetterdependenceonH.
√
ComparedwiththedynamicregretofO(cid:101)(cid:0) dHS K+(cid:112) HK(H +P¯ K)(cid:1) forunknownnon-stationarity
measurecasesinLietal.[2024b],ourboundremovesthedependenceonthestatenumberS.
Then,weestablishthedynamicregretlowerboundforthisproblem.
Theorem2. SupposeB ≥2,d≥4,H ≥3,K ≥(d−1)2H/2,foranyalgorithmandanyconstant
Γ∈[0,2KH],thereexistsanadversarialinhomogeneouslinearmixtureMDPandapolicysequence
√
πc,...,πc suchthatP¯ ≤ΓandD-Reg (πc )≥Ω( d2H3K+(cid:112) HK(H +Γ)).
1 K K K 1:K
Remark6. CombiningTheorem1andTheorem2,ouralgorithmachievestheminimaxoptimal
dynamicregretintermsofd,H,K andP¯ simultaneouslyuptologarithmicfactors.
K
5.2 ProofOverview
WeprovidetheproofsketchofTheorem1. Thedetailedproofcanbefoundintheappendixes.
ProofSketch(ofTheorem1). Wecandecomposethedynamicregretasthefollowingfourterms:
K K K K
(cid:88) (cid:88) (cid:88)(cid:16) (cid:17) (cid:88)(cid:16) (cid:17)
D-Reg K(π 1c :K)= ⟨q kc −qˆ ki,r k⟩+ ⟨qˆ ki −qˆ k,r k⟩+ V(cid:98)k,1−V
k,1
+ V k,1−V kπ ,1k .
k=1 k=1 k=1 k=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
base-regret meta-regret occupancy-policy-gap estimation-error
•base-regret. BytheanalysisofOMD,itcanbeupperboundedbyO(cid:101)(cid:0) η iKH +(H +P¯ K)/η i(cid:1) .
Choosingthebase-learnerwiththebeststepsizeleadstoanupperboundofO(cid:101)((cid:112) KH(H +P¯ K)).
•meta-regret. Thistermisthestaticregretoftheexpert-trackingproblem. Asourmeta-algorithmis
√
theHedgealgorithm,bythestandardanalysis,thistermcanbeupperboundedbyO(cid:101)(H K).
•occupancy-policy-gap. AsthevaluefunctionsareoptimisticestimatorsovertheconfidencesetC ,
k
thevaluegapbetweentheoccupancymeasureandthepolicyisguaranteedtobenon-positive.
•estimation-error. LetϵQ =Q −Qπk andϵV =V −Vπk,thendefinepolicynoiseM =
k,h k,h k,h k,h k,h k,h k,h,1
E [ϵQ (s ,a)]−ϵQ (s ,a ), transition noise M = [P (ϵV )](s ,a )−
a∼πk(·|sk,h) k,h k,h k,h k,h k,h k,h,2 h k,h k,h k,h
ϵV (s )andbonusι =Q −(r +P V ). Theestimationerrorcanbedecomposed
k,h k,h+1 k,h k,h k,h h k,h+1
intotransitionnoises,policynoises,andbonuses,i.e.,(cid:80)K (cid:80)H
(M +M +ι ). The
√ k=1 h=1 k,h,1 k,h,2 k,h
transitionandpolicynoisescanbebou √ndedbyO(cid:101)( √KH3)usingAzuma-Hoeffding’sinequalities.
ThebonustermcanbeboundedbyO(cid:101)( d2H3K+ dH4K)accordingtoLemma2. ■
6 ConclusionandFutureWork
Inthiswork,westudythedynamicregretofadversariallinearmixtureMDPswiththeunknown
transition. Weobservetheoccupancy-measure-basedmethodiseffectiveinaddressingnon-stationary
environmentsbutstruggleswithunknowntransitions. Incontrast,thepolicy-basedmethodcandeal
withunknowntransitionseffectivelybutfaceschallengesinhandlingnon-stationaryenvironments.
Tothisend,weproposeanewalgorithmthatcombinesthebenefitsofbothmethods,achievingan
√
O(cid:101)( d2H3K+(cid:112) HK(H +P¯ K))dynamicregretwithoutpriorknowledgeofthenon-stationarity
measure. Weshowitisoptimaluptologarithmicfactorsbyestablishingamatchinglowerbound.
Currently,weachievethisresultbyemployingahybridmethod.Exploringwhethersimilarresultscan
beattainedusingcomputationallymoreefficientmethodsisanimportantfuturework. Furthermore,
extendingourresultstootherMDPclasses,suchasgeneralizedlinearfunctionapproximation[Wang
etal.,2021]andmultinomiallogitfunctionapproximation[Lietal.,2024a],isaninterestingdirection.
10Acknowledgments
ThisresearchwassupportedbyNationalScienceandTechnologyMajorProject(2022ZD0114800)
andNSFC(62361146852,61921006).
References
YasinAbbasi-Yadkori,DávidPál,andCsabaSzepesvári. Improvedalgorithmsforlinearstochastic
bandits. InAdvancesinNeuralInformationProcessingSystems24(NIPS),pages2312–2320,
2011.
EitanAltman. ConstrainedMarkovDecisionProcesses. Routledge,1998.
PeterAuer,NicoloCesa-Bianchi,YoavFreund,andRobertESchapire.Thenonstochasticmultiarmed
banditproblem. SIAMJournalonComputing,32(1):48–77,2002.
AlexAyoub,ZeyuJia,CsabaSzepesvári,MengdiWang,andLinYang. Model-basedreinforcement
learningwithvalue-targetedregression. InProceedingsofthe37thInternationalConferenceon
MachineLearning(ICML),pages463–474,2020.
MohammadGheshlaghiAzar,RémiMunos,andHilbertJ.Kappen. MinimaxPACboundsonthe
samplecomplexityofreinforcementlearningwithagenerativemodel. MachineLearning,91(3):
325–349,2013.
MohammadGheshlaghiAzar,IanOsband,andRémiMunos. Minimaxregretboundsforreinforce-
mentlearning. InProceedingsofthe34thInternationalConferenceonMachineLearning(ICML),
pages263–272,2017.
QiCai,ZhuoranYang,ChiJin,andZhaoranWang. Provablyefficientexplorationinpolicyoptimiza-
tion. InProceedingsofthe37thInternationalConferenceonMachineLearning(ICML),pages
1283–1294,2020.
NicoloCesa-BianchiandGáborLugosi. Prediction,Learning,andGames. CambridgeUniversity
Press,2006.
NicolòCesa-Bianchi,PierreGaillard,GáborLugosi,andGillesStoltz. Mirrordescentmeetsfixed
share(andfeelsnoregret). InAdvancesinNeuralInformationProcessingSystems25(NIPS),
pages989–997,2012.
WangChiCheung,DavidSimchi-Levi,andRuihaoZhu. Reinforcementlearningfornon-stationary
Markovdecisionprocesses:Theblessingof(more)optimism.Proceedingsofthe37thInternational
ConferenceonMachineLearning(ICML),pages1843–1854,2020.
OmarDarwicheDomingues,PierreMénard,MatteoPirotta,EmilieKaufmann,andMichalValko. A
kernel-basedapproachtonon-stationaryreinforcementlearninginmetricspaces. InProceedings
ofthe24thInternationalConferenceonArtificialIntelligenceandStatistics(AISTATS),pages
3538–3546,2021.
EyalEven-Dar,Sham.M.Kakade,andYishayMansour. OnlineMarkovdecisionprocesses. Mathe-
maticsofOperationsResearch,pages726–736,2009.
YoavFreundandRobertE.Schapire. Adecision-theoreticgeneralizationofon-linelearningandan
applicationtoboosting. JournalofComputerandSystemSciences,55(1):119–139,1997.
JiafanHe,DongruoZhou,andQuanquanGu. Nearlyminimaxoptimalreinforcementlearningfor
discountedMDPs. InAdvancesinNeuralInformationProcessingSystems34(NeurIPS),pages
22288–22300,2021.
Jiafan He, Dongruo Zhou, and Quanquan Gu. Near-optimal policy optimization algorithms for
learningadversariallinearmixtureMDPs. InProceedingsofthe25thInternationalConferenceon
ArtificialIntelligenceandStatistics(AISTATS),pages4259–4280,2022.
Mark Herbster and Manfred K. Warmuth. Tracking the best expert. Machine Learning, 32(2):
151–178,1998.
11MarkHerbsterandManfredK.Warmuth. Trackingthebestlinearpredictor. JournalofMachine
LearningResearch,1:281–309,2001.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. JournalofMachineLearningResearch,pages1563–1600,2010.
KaixuanJi,QingyueZhao,JiafanHe,WeitongZhang,andQuanquanGu.Horizon-freereinforcement
learninginadversariallinearmixtureMDPs. InProceedingsofthe12thInternationalConference
onLearningRepresentations(ICLR),2024.
Chi Jin, Zeyuan Allen-Zhu, Sébastien Bubeck, and Michael I. Jordan. Is Q-learning provably
efficient? InAdvancesinNeuralInformationProcessingSystems31(NeurIPS),pages4868–4878,
2018.
ChiJin,TianchengJin,HaipengLuo,SuvritSra,andTianchengYu. LearningadversarialMarkov
decision processes with bandit feedback and unknown transition. In Proceedings of the 37th
InternationalConferenceonMachineLearning(ICML),pages4860–4869,2020a.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I. Jordan. Provably efficient reinforcement
learningwithlinearfunctionapproximation. InProceedingsofthe33rdConferenceonLearning
Theory(COLT),pages2137–2143,2020b.
ShamM.KakadeandJohnLangford. Approximatelyoptimalapproximatereinforcementlearning. In
Proceedingsofthe19thInternationalConferenceonMachineLearning(ICML),pages267–274,
2002.
Jens Kober, J. Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey.
InternationalJournalofRoboticsResearch,32(11):1238–1274,2013.
Long-FeiLi,PengZhao,andZhi-HuaZhou. DynamicregretofadversariallinearmixtureMDPs. In
AdvancesinNeuralInformationProcessingSystems36(NeurIPS),pages60685–60711,2023.
Long-FeiLi,Yu-JieZhang,PengZhao,andZhi-HuaZhou. Provablyefficientreinforcementlearning
withmultinomiallogitfunctionapproximation. InAdvancesinNeuralInformationProcessing
Systems37(NeurIPS),pagetoappear,2024a.
Long-FeiLi,PengZhao,andZhi-HuaZhou. DynamicregretofadversarialMDPswithunknown
transition andlinear function approximation. In Proceedings ofthe 38th AAAI Conference on
ArtificialIntelligence(AAAI),pages13572–1358,2024b.
Long-Fei Li, Peng Zhao, and Zhi-Hua Zhou. Improved algorithm for adversarial linear mixture
MDPswithbanditfeedbackandunknowntransition. InProceedingsofthe27thInternational
ConferenceonArtificialIntelligenceandStatistics(AISTATS),pages3061–3069,2024c.
WeichaoMao,KaiqingZhang,RuihaoZhu,DavidSimchi-Levi,andTamerBasar. Near-optimal
model-freereinforcementlearninginnon-stationaryepisodicMDPs. InProceedingsofthe38th
InternationalConferenceonMachineLearning(ICML),pages7447–7458,2021.
RonaldOrtner,PratikGajane,andPeterAuer. Variationalregretboundsforreinforcementlearning.
InProceedingsofthe35thConferenceonUncertaintyinArtificialIntelligence(UAI),pages81–90,
2019.
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,Chong
Zhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Traininglanguagemodelstofollow
instructions with human feedback. Advances in Neural Information Processing Systems 35
(NeurIPS),pages27730–27744,2022.
AvivRosenbergandYishayMansour. OnlineconvexoptimizationinadversarialMarkovdecision
processes. InProceedingsofthe36thInternationalConferenceonMachineLearning(ICML),
pages5478–5486,2019.
12DavidSilver,AjaHuang,ChrisJ.Maddison,ArthurGuez,LaurentSifre,GeorgevandenDriess-
che,JulianSchrittwieser,IoannisAntonoglou,VedavyasPanneershelvam,MarcLanctot,Sander
Dieleman,DominikGrewe,JohnNham,NalKalchbrenner,IlyaSutskever,TimothyP.Lillicrap,
MadeleineLeach,KorayKavukcuoglu,ThoreGraepel,andDemisHassabis. Masteringthegame
ofGowithdeepneuralnetworksandtreesearch. Nature,pages484–489,2016.
RichardSSuttonandAndrewGBarto. ReinforcementLearning: AnIntroduction. MITPress,2018.
Ahmed Touati and Pascal Vincent. Efficient learning in non-stationary linear Markov decision
processes. ArXivpreprint,arXiv:2010.12870,2020.
YiningWang,RuosongWang,SimonShaoleiDu,andAkshayKrishnamurthy. Optimisminrein-
forcement learning with generalized linear function approximation. In Proceedings of the 9th
InternationalConferenceonLearningRepresentations(ICLR),2021.
Chen-YuWeiandHaipengLuo. Non-stationaryreinforcementlearningwithoutpriorknowledge: An
optimalblack-boxapproach. InProceedingsofthe34thConferenceonLearningTheory(COLT),
pages4300–4354,2021.
Yu-HuYan,PengZhao,andZhi-HuaZhou. Universalonlinelearningwithgradientvariations: A
multi-layeronlineensembleapproach. InAdvancesinNeuralInformationProcessingSystems36
(NeurIPS),pages37682–37715,2023.
JiaYuanYu,ShieMannor,andNahumShimkin. Markovdecisionprocesseswitharbitraryreward
processes. MathematicsofOperationsResearch,pages737–757,2009.
LijunZhang,ShiyinLu,andZhi-HuaZhou. Adaptiveonlinelearningindynamicenvironments. In
AdvancesinNeuralInformationProcessingSystems31(NeurIPS),pages1330–1340,2018.
CanzheZhao,RuofengYang,BaoxiangWang,andShuaiLi. Learningadversariallinearmixture
Markovdecisionprocesseswithbanditfeedbackandunknowntransition. InProceedingsofthe
11thInternationalConferenceonLearningRepresentations(ICLR),2023.
Peng Zhao, Lijun Zhang, Yuan Jiang, and Zhi-Hua Zhou. A simple approach for non-stationary
linearbandits. InProceedingsofthe23rdInternationalConferenceonArtificialIntelligenceand
Statistics(AISTATS),pages746–755,2020.
PengZhao,Long-FeiLi,andZhi-HuaZhou. DynamicregretofonlineMarkovdecisionprocesses.
InProceedingsofthe39thInternationalConferenceonMachineLearning(ICML),pages26865–
26894,2022.
PengZhao,Yu-JieZhang,LijunZhang,andZhi-HuaZhou.Adaptivityandnon-stationarity:Problem-
dependentdynamicregretforonlineconvexoptimization. JournalofMachineLearningResearch,
25(98):1–52,2024.
HanZhong,ZhuoranYang,ZhaoranWang,andCsabaSzepesvári. Optimisticpolicyoptimizationis
provablyefficientinnon-stationaryMDPs. ArXivpreprint,arXiv:2110.08984,2021.
DongruoZhou,QuanquanGu,andCsabaSzepesvári.Nearlyminimaxoptimalreinforcementlearning
forlinearmixtureMarkovdecisionprocesses. InProceedingsofthe34thConferenceonLearning
Theory(COLT),pages4532–4576,2021.
HuozhiZhou,JinglinChen,LavR.Varshney,andAshishJagmohan. Nonstationaryreinforcement
learningwithlinearfunctionapproximation. TransitiononMachineLearningResearch,2022.
ISSN2835-8856.
AlexanderZiminandGergelyNeu. OnlinelearninginepisodicMarkoviandecisionprocessesby
relativeentropypolicysearch. InAdvancesinNeuralInformationProcessingSystems26(NIPS),
pages1583–1591,2013.
13A ProofofLemma1
Proof. Withoutlossofgenerality,weassumethatallstatesarereachablewithpositiveprobability
under the uniform policy πu(a|s) = 1/A,∀s ∈ S,a ∈ A. Otherwise, we can simply remove
the unreachable states from the state space. Assume K is large enough such that the occupancy
measureofqP,πu ∈∆(P,1/T). Wedefineu =(1− 1)qc + 1qP,πu ∈∆(P,1/T2). Then,wecan
k T k T
decomposethefirsttermas
K K K
(cid:88) (cid:88) (cid:88)
⟨qc −qˆ ,r ⟩= ⟨qc −u ,r ⟩+ ⟨u −qˆ ,r ⟩
k k k k k k k k k
k=1 k=1 k=1
K K
= 1 (cid:88) ⟨qc −qP,πu ,r ⟩+(cid:88) ⟨u −qˆ ,r ⟩
T k k k k k
k=1 k=1
K K
≤ 1 (cid:88) ∥qc −qP,πu ∥ ∥r ∥ +(cid:88) ⟨u −qˆ ,r ⟩
T k 1 k ∞ k k k
k=1 k=1
K
(cid:88)
≤2+ ⟨u −qˆ ,r ⟩
k k k
k=1
K K
(cid:88) (cid:88)
=2+ ⟨u −qˆi,r ⟩+ ⟨qˆi −qˆ ,r ⟩, (11)
k k k k k k
k=1 k=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
base-regret meta-regret
whichthefirstinequalityfollowsfromHolder’sinequality,thesecondholdsby∥qc−qP,πu∥ ≤2H,
k 1
andthelastholdsforanyi∈[N]. Next,weboundbase-regretandmeta-regretseparately.
Upperboundofbase-regret. Sincethetruetransitionparameterθ∗iscontainedintheconfidence
set C by condition, we ensure that u ∈ ∆(P,1/T2) ∈ ∆(C ,1/T2). By the update rule of qˆi
k k k k
in(6),weensurethatqˆi ∈∆(C ,1/T2),∀i∈[K]. Theupdaterulein(6)canberewrittenas:
k k
q¯i = argmaxη ⟨q,r ⟩−D (q∥qˆi), qˆi = argmin D (q∥q¯i ),
k+1 i k ψ k k+1 ψ k+1
q∈RHSAS q∈∆(Ck,α)
orequivalently,
q¯i (s,a,s)=qˆi (s,a,s)exp(η r (s,a)), qˆi = argmin D (q∥q¯i ).
k+1,h k,h i k k+1 ψ k+1
q∈∆(Ck,α)
Bythethree-pointidentityinLemma9,wehave
η ⟨u −qˆi,r ⟩=D (u ∥qˆi)−D (u ∥q¯i )+D (qˆi∥q¯i )
i k k k ψ k k ψ k k+1 ψ k k+1
≤D (u ∥qˆi)−D (u ∥qˆi )+D (qˆi∥q¯i ), (12)
ψ k k ψ k k+1 ψ k k+1
wheretheinequalityisduetothePythagoreantheorem.
Forthelasttermof(12),summingoverk,wehave
K
(cid:88)
D (qˆi∥q¯i )
ψ k k+1
k=1
K H
=(cid:88)(cid:88) (cid:88) qˆi(s,a,s′)(cid:0)
−η r (s,a)−1+exp(η r
(s,a))(cid:1)
k i k i k
k=1h=1s,a,s′
K H
(cid:88)(cid:88) (cid:88)
≤η2 qˆi(s,a,s′)r (s,a)2
i k k
k=1h=1s,a,s′
≤η2HK, (13)
i
wherethefirstinequalityisduetothefactex−x−1≤x2forx∈[0,1].
14Forthefirsttwotermsof(12),bythedefinitionofKL-divergence,wehave
K
(cid:88)(cid:0) D (u ∥qˆi)−D (u ∥qˆi )(cid:1)
ψ k k ψ k k+1
k=1
K
=D (u ∥qˆi)+(cid:88)(cid:0) D (u ∥qˆi)−D (u ∥qˆi)(cid:1)
ψ 1 1 ψ k k ψ k−1 k
k=2
=D (u
∥qˆi)+(cid:88)K (cid:88)H (cid:88) (cid:32)
u
(s,a,s′)logu k,h(s,a,s′)
−u
(s,a,s′)logu k−1,h(s,a,s′)(cid:33)
ψ 1 1 k,h qˆi (s,a,s′) k−1,h qˆi (s,a,s′)
k=2h=1s,a,s′ k,h k,h
K H
(cid:88)(cid:88) (cid:88) 1
=D (u ∥qˆi)+ψ(u )−ψ(u )+ (u (s,a,s′)−u (s,a,s′))log
ψ 1 1 K 1 k,h k−1,h qˆi (s,a,s′)
k=2h=1s,a,s′ k,h
K H
(cid:88)(cid:88) (cid:88)
≤ D (u ∥qˆi)+ψ(u )−ψ(u )+2logT |u (s,a,s′)−u (s,a,s′)|
ψ 1 1 K 1 k,h k−1,h
(cid:124) (cid:123)(cid:122) (cid:125) k=2h=1s,a,s′
I1
(cid:124) (cid:123)(cid:122) (cid:125)
I2
wheretheinequalityholdsbyqˆi ∈∆(P ,1/T2). ItremainstoboundI andI separately.
k k 1 2
FortermI ,sinceqˆi minimizeψover∆(P ,1/T2),wehave⟨∇ψ(qˆi),u −qˆi⟩≥0,thus,
1 1 1 1 1 1
H
(cid:88) (cid:88) 1
I ≤ψ(u )−ψ(qˆi)≤ qˆi (s,a,s′) ≤Hlog(S2A). (14)
1 K 1 1,h qˆi (s,a,s′)
h=1s,a,s′ 1,h
FortermI ,forany(s,a),wehave
2
(cid:88) (cid:88)
|u (s,a,s′)−u (s,a,s′)|= |u (s,a)P (s′|s,a)−u (s,a,s′)P (s′|s,a)|
k,h k−1,h k,h h k−1,h h
s′ s′
(cid:88)
= |u (s,a)−u (s,a)|P (s′|s,a)
k,h k−1,h h
s′
=|u (s,a)−u (s,a)|.
k,h k−1,h
Thus,wehave
K H
I =(cid:88)(cid:88)(cid:88) |u (s,a)−u (s,a)|=(1− 1 )∥qc −qc ∥ =(1− 1 )P¯ ≤P¯ .
2 k,h k−1,h T k k−1 1 T K K
k=2h=1 s,a
(15)
Combining(12),(13),(14)and(15),wehave
K
base-regret≤ 1 (cid:88)(cid:0) D (u ∥qˆi)−D (u ∥qˆi )+D (qˆi∥q¯i )(cid:1)
η ψ k k ψ k k+1 ψ k k+1
i
k=1
1
≤η T + (Hlog(S2A)+2P¯ logT).
i η K
i
It is easy to verify that the optimal step size is η∗ = (cid:112) (Hlog(S2A)+2P¯ logT)/T. Since
K
0≤P¯ ≤2T,weensurethat
K
(cid:114) (cid:114)
Hlog(S2A) Hlog(S2A)+4T logT
≤η∗ ≤ .
T T
(cid:112)
By the construction of the step size pool H = {η = 2i−1 K−1log(S2A) | i ∈ [N]} with
i
N =1+⌈1log(1+ 4KlogT )⌉,weknowthatthestepsizethereinismonotonicallyincreasingwith
2 log(S2A)
(cid:114) (cid:114)
Hlog(S2A) Hlog(S2A)+4T logT
η = ,η ≥ .
1 T N T
15Thus,weensurethereexistsanindexi∗suchthatη ≤η∗ ≤2η =η .Sincethedecomposition
i∗ i∗ i∗+1
in(11)holdsforanyB ,wechooseB toanalyzetheregretbound. Bythedefinitionofη∗,wehave
i i∗
1
base-regret≤η T + (Hlog(S2A)+2P¯ logT)
i∗ η K
i∗
2
≤η∗T + (Hlog(S2A)+2P¯ logT)
η∗ K
(cid:113)
=3 T(Hlog(S2A)+2P¯ logT), (16)
K
wherethelastequalityholdsbysubstitutingη∗ =(cid:112) (Hlog(S2A)+2P¯ logT)/T.
K
Upperboundofmeta-regret. Denotebyhi =⟨qˆi,r ⟩∈[0,H],wehave
k k k
K N K
(cid:88) (cid:88) (cid:88)
meta-regret= ⟨qˆi − piqˆi,r ⟩= ⟨e −p ,h ⟩,
k k k k i k k
k=1 i=1 k=1
wheree isthei-thstandardbasisvectorinRN. ThisisastandardPredictionwithExpertAdvice
i
(PEA)problemandouralgorithmisthewell-knownHedgealgorithm[FreundandSchapire,1997,
HerbsterandWarmuth,1998]. BythestandardanalysisofHedge[Cesa-BianchiandLugosi,2006,
Theorem2.2],wehave
logN (cid:112)
meta-regret≤ +εH2K = HT logN, (17)
ε
(cid:112)
wheretheequalityholdsbysettingε= (logN/HT).
Combining(11),(16)and(17),wehave
(cid:88)K
⟨qc −qˆ ,r
⟩≤3(cid:113)
T(Hlog(S2A)+2P¯ logT)+2.
k k k K
k=1
Thisfinishestheproof. ■
B ProofofLemma2
In this section, we first provide the main proof of Lemma 2, and then present the proofs of the
auxiliarylemmasusedinthemainproof.
B.1 MainProof
Proof. ToproveLemma2,wefirstintroducethefollowinglemmawhichshowsthetrueparameter
θ∗ iscontainedintheconfidencesetC withhighprobabilitybysuchconfiguration.
h k,h
Lemma 3 (Zhou et al. [2021, Lemma 5]). Let C be defined in (10) and set parameters as in
k,h
Lemma2. Then,wehaveθ∗ ∈C forallh∈[H]andk ∈[K]withprobabilityatleast1−3δ.
h k,h
DenotebyE theeventwhenLemma3holds,thenPr(E)≥1−3δ. WearereadytoproveLemma2.
First,wecanrewritetheterm(cid:80)K
⟨qˆ −q ,r ⟩as
k=1 k k k
K K
(cid:88) (cid:88)
⟨qˆ k−q k,r k⟩= V(cid:98)k,1(s k,1)−V kπ ,1k(s k,1)
k=1 k=1
K K
(cid:88)(cid:16) (cid:17) (cid:88)(cid:16) (cid:17)
= V(cid:98)k,1(s k,1)−V k,1(s k,1) + V k,1(s k,1)−V kπ ,1k(s k,1) , (18)
k=1 k=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
occupancy-policy-gap estimation-error
Next,weboundthesetwotermsseparately.
Upperboundofoccupancy-iteration-gap. Thistermisthegapbetweenthevaluefunctioncomputed
bytheoccupancymeasure{qˆ }K andtheoptimisticvaluefunctioncomputedbybackwarditeration
k k=1
definedin(9). SimilartoJietal.[2024,Lemma6.1],weshowthistermisnon-positive.
16Lemma 4. For any episode k ∈ [K], define V(cid:98)k,1(s k,1) = (cid:80) h,s,a,s′qˆ k,h(s,a,s′)r k,h(s,a) and
V k,1(s k,1)thevaluefunctioncomputedin(9),ontheeventE,itholdsthatV(cid:98)k,1(s k,1)≤V k,1(s k,1).
Upperboundofestimation-error. First,wepresentthefollowinglemmawhichshowsthisterm
canbedecomposedintothreemajorterms,transitionnoise,policynoiseandthesumofbonuses.
Lemma5. Forallk ∈[K],h∈[H],itholdsthat
H
(cid:88) (cid:16) (cid:17)
V k,h(s k,h)−V kπ ,hk(s k,h)= M k,h′,1+M k,h′,2−ι k,h′(s k,h′,a k,h′) ,
h′=h
with
M =E [Q (s ,a)−Qπk (s ,a)]−(Q (s ,a )−Qπk (s ,a )),
k,h,1 a∼πk(·|sk,h) k,h k,h k,h k,h k,h k,h k,h k,h k,h k,h
M =[P (V −Vπk )](s ,a )−(V (s )−Vπk (s ))
k,h,2 h k,h+1 k,h+1 k,h k,h k,h+1 k,h+1 k,h+1 k,h+1
ι (·,·)=Q (·,·)−(r (·,·)+P V (·,·)).
k,h k,h k,h h k,h+1
NoteM isthenoisefromthestochasticpolicyandM isthenoisefromthestatetransition,
k,h,1 k,h,2
LetM =M +M ,∀k ∈[K],h∈[H],wedefinetwofollowinghighprobabilityevents:
k,h k,h,1 k,h,2
(cid:26) K H (cid:114) (cid:27) (cid:26) K H (cid:114) (cid:27)
(cid:88) (cid:88) H (cid:88)(cid:88) 1
E = ∀h∈[H], M ≤4 H3Klog ,E = M ≤ 8H3Klog .
1 k,h′ δ 2 k,h,2 δ
k=1h′=h k=1h=1
AccordingtotheAzuma-Hoeffdinginequality, wehavePr(E ) ≥ 1−δ andPr(E ) ≥ 1−δ. It
1 2
remainstoboundthemodelpredictionerrorι .
k,h
Next,weshowthepredictionerrordependsonthewidthoftheconfidencesetandthecumulative
estimatevariancebythefollowinglemma.
Lemma6. Definepredictionerrorι =Q −(r +P V ),ontheeventE,itholdsthat
k,h k,h k,h h k,h+1
(cid:118)
K H (cid:117) K H
(cid:88)(cid:88) (cid:117)(cid:88)(cid:88) (cid:112)
ι k,h(s k,h,a k,h)≤2β(cid:98)K(cid:116) σ¯ k2
,h
2Hdlog(1+K/λ).
k=1h=1 k=1h=1
Here,σ¯2 istheestimatedvariance,forthetotaltruevariance(cid:80)K (cid:80)H [V Vπk ](s ,ak),we
k,h k=1 h=1 h h+1 k,h h
introducethehighprobabilityeventE :
3
(cid:26) K H (cid:27)
E = (cid:88)(cid:88) [V Vπk ](s ,ak)≤3(HK+H3log(1/δ)) .
3 h h+1 k,h h
k=1h=1
LemmaC.5inJinetal.[2018]suggeststhatPr(E )≥1−δ. BasedontheeventsE ∩E ∩E ∩E ,
3 1 2 3
wehavethefollowinglemmawhichboundstheestimatedvarianceofthevaluefunction.
Lemma7(Heetal.[2022,Lemma6.5]). OntheeventsE ∩E ∩E ∩E ,itholdsthat
1 2 3
(cid:88)K (cid:88)H 2H3K (cid:18) 4K2H(cid:19) (cid:18) KH4(cid:19)
σ¯2 ≤ +179H2K+(165d3H4+2062d2H5)log2 log2 1+ .
k,h d δ λ
k=1h=1
CombiningLemma5,Lemma6andLemma7,wecanboundtheestimation-errorasfollows.
Lemma8. OntheeventsE ∩E ∩E ∩E ,foranyh∈[H],itholdsthat
1 2 3
K K √ √
(cid:88) V k,h(s k,h)−(cid:88) V kπ ,hk(s k,h)≤O(cid:101)(cid:0) dH4K+ d2H3K(cid:1) .
k=1 k=1
Finally,wefinishtheproofofLemma2bycombiningLemma4andLemma8. ■
17B.2 ProofsofAuxiliaryLemmas
Inthissection,weprooftheauxiliarylemmasusedintheproofofLemma2inAppendixB.1.
B.2.1 ProofofLemma4
Proof. TheproofissimilartothatofJietal.[2024,Lemma6.1]. Theonlydifferenceisthatqˆ is
k
aweightedcombinationratherthanasingleoccupancymeasureinthedecisionset. Fork ∈ [K],
the occupancy measure qˆ is given by qˆ = (cid:80)N piqˆi. Since qˆi ∈ ∆(P ,α), we ensure that
k k i=1 k k k k
qˆ ∈∆(P ,α). Thus,foroccupancymeasureqˆ ,thereexistθ¯suchthat
k k k
q (s,a,s′)
P (cid:98)k,h(s′|s,a)=
(cid:80)
k, qh
(s,a,s′)
=⟨ϕ(s′|s,a),θ¯ k,h(s,a)⟩,∀(s,a,h)∈S×A×[H].
s′ k,h
ItiseasytoverifythattheupdateruleV(cid:98)k,1(s k,1) = (cid:80) h,s,a,a′qˆ k,h(s,a,s′)r k,h(s,a)computedby
theoccupancymeasureisthesameasthefollowingbackwarditeration:
Q(cid:98)k,h(s,a)=r h(s,a)+⟨ϕ V(cid:98)k,h+1(s′|s,a),θ¯ k,h(s,a)⟩,
V(cid:98)k,h(s)=E a∼πk(·|s)Q(cid:98)k,h(s,a), V(cid:98)k,H+1(s)=0.
Then, we can prove this lemma by induction. The conclusion trivially holds for n = H + 1.
Supposethestatementholdsforn=h+1,weproveitforn=h. Forany(s,a)∈S ×A,since
Q(cid:98)k,h(s,a)≤H,soifQ k,h(s,a)=H,thenitholdsdirectly. Otherwise,wehave
Q k,h(s,a)−Q(cid:98)k,h(s,a)
=⟨ϕ Vk,h+1(s,a),θ(cid:98)k,h⟩+β(cid:98)∥ϕ Vk,h+1(s,a)∥
Σ(cid:98)− k,1
h
−⟨ϕ V(cid:98)k,h+1(s′|s,a),θ¯ k,h(s,a)⟩
≥⟨ϕ Vk,h+1(s,a),θ(cid:98)k,h⟩+β(cid:98)∥ϕ Vk,h+1(s,a)∥
Σ(cid:98)− k,1
h
−⟨ϕ Vk,h+1(s′|s,a),θ¯ k,h(s,a)⟩
=⟨ϕ Vk,h+1(s,a),θ(cid:98)k,h−θ¯ k,h(s,a)⟩+β(cid:98)∥ϕ Vk,h+1(s,a)∥
Σ(cid:98)− k,1
h
≥β(cid:98)∥ϕ Vk,h+1(s,a)∥
Σ(cid:98)− k,1
h
−∥θ(cid:98)k,h−θ¯ k,h(s,a)∥ Σ(cid:98)k,h∥ϕ Vk,h+1(s,a)∥
Σ(cid:98)− k,1
h
≥0,
where the first inequality holds by the inductive hypothesis, the second holds due to Holder’s
inequality,andthelastholdsduetoθ¯ (s,a)∈C . Byinduction,wefinishtheproof. ■
k,h k,h
B.2.2 ProofofLemma5
Proof. BythedefinitionV (s )=E [Q (s ,a)],wehave
k,h k,h a∼πk(·|sk,h) k,h k,h
V (s )−Vπk(s )
k,h k,h k,h k,h
=E (cid:2) Q (s ,a)−Qπk (s ,a)(cid:3)
a∼πk(·|sk,h) k,h k,h k,h k,h
=E (cid:2) Q (s ,a)−Qπk (s ,a)(cid:3) −(cid:0) Q (s ,a )−Qπk (s ,a )(cid:1)
a∼πk(·|sk,h) k,h k,h k,h k,h k,h k,h k,h k,h k,h k,h
+(cid:0) Q (s ,a )−Qπk (s ,a )(cid:1)
k,h k,h k,h k,h k,h k,h
=E (cid:2) Q (s ,a)−Qπk (s ,a)(cid:3) −(cid:0) Q (s ,a )−Qπk (s ,a )(cid:1)
a∼πk(·|sk,h) k,h k,h k,h k,h k,h k,h k,h k,h k,h k,h
+[P (V −Vπk )](s ,a )+ι (s ,a )
h k,h+1 k,h+1 k,h k,h k,h k,h k,h
= E (cid:2) Q (s ,a)−Qπk (s ,a)(cid:3) −(cid:0) Q (s ,a )−Qπk (s ,a )(cid:1)
a∼πk(·|sk,h) k,h k,h k,h k,h k,h k,h k,h k,h k,h k,h
(cid:124) (cid:123)(cid:122) (cid:125)
≜Mk,h,1
+[P (V −Vπk )](s ,a )−(cid:0) V (s )−Vπk (s )(cid:1)
h k,h+1 k,h+1 k,h k,h k,h+1 k,h+1 k,h+1 k,h+1
(cid:124) (cid:123)(cid:122) (cid:125)
≜Mk,h,2
+(cid:0) V (s )−Vπk (s )(cid:1) +ι (s ,a )
k,h+1 k,h+1 k,h+1 k,h+1 k,h k,h k,h
wherethethirdequalityholdsbythefactQ =r +P V +ι andQπk =r +P Vπk .
k,h k,h h k,h+1 k,h k,h k,h h k,h+1
SumminguptheaboveequationfromhtoH recursivelyfinishestheproof. ■
18B.2.3 ProofofLemma6
Proof. Bythedefinitionofι =Q −(r +P V ),wehave
k,h k,h k,h h k,h+1
ι (s,a)
k,h
=Q (s,a)−(r +P V )(s,a)
k,h k,h h k,h+1
=r k,h(s,a)+(cid:10) θ(cid:98)k,h,ϕ Vk,h+1(s,a)(cid:11) +β(cid:98)k(cid:13) (cid:13)Σ(cid:98)− k,1 h/2ϕ Vk,h+1(s,a)(cid:13) (cid:13) 2−(r k,h+P hV k,h+1)(s,a)
=(cid:10) θ(cid:98)k,h−θ h∗,ϕ Vk,h+1(s,a)(cid:11) +β(cid:98)k(cid:13) (cid:13)Σ(cid:98)− k,1 h/2ϕ Vk,h+1(s,a)(cid:13) (cid:13)
2
≤2β(cid:98)k(cid:13) (cid:13)Σ(cid:98)− k,1 h/2ϕ Vk,h+1(s,a)(cid:13) (cid:13) 2,
wherethefirstinequalityholdsbytheconfigurationofQ in(9),thesecondinequalityholdsby
k,h
thedefinitionoflinearmixtureMDPsuchthat[P V ](s,a) = ⟨ϕ (s,a),θ∗⟩andthelast
h k,h+1 Vk,h+1 h
inequalityholdsbytheconstructionoftheconfidencesetinLemma3.
Then,wehave
K H
(cid:88)(cid:88)
ι (s ,a )
k,h k,h k,h
k=1h=1
K H
≤ (cid:88)(cid:88) 2min{β(cid:98)k(cid:13) (cid:13)Σ(cid:98)− k,1 h/2ϕ Vk,h+1(s k,h,a k,h)(cid:13) (cid:13) 2,H}
k=1h=1
K H
≤ (cid:88)(cid:88) 2β(cid:98)kσ¯ k,hmin(cid:8)(cid:13) (cid:13)Σ(cid:98)− k,1 h/2ϕ Vk,h+1(s k,h,a k,h)/σ¯ k,h(cid:13) (cid:13) 2,1(cid:9)
k=1h=1
(cid:118) (cid:118)
≤2β(cid:98)K(cid:117) (cid:117) (cid:116)(cid:88)K (cid:88)H σ¯ k2 ,h(cid:117) (cid:117) (cid:116)(cid:88)K (cid:88)H min(cid:110)(cid:13) (cid:13) (cid:13)Σ(cid:98)− k,1 h/2ϕ Vk,h+1(s k,h,a k,h)/σ¯ k,h(cid:13) (cid:13)
(cid:13)
2,1(cid:111)
k=1h=1 k=1h=1
(cid:118)
(cid:117) K H
(cid:117)(cid:88)(cid:88) (cid:112)
≤2β(cid:98)K(cid:116) σ¯ k2
,h
2Hdlog(1+K/λ),
k=1h=1
√ √
wherethefirstinequalityholdsbyQ
k,h
∈[0,H],thesecondholdsby2β(cid:98)kσ¯
k,h
≥ dH/ d=H,
thethirdinequalityisbyCauchy-Schwarzinequalityandthelastinequalityholdsbytheelliptical
potentiallemmainLemma10. Thisfinishestheproof. ■
B.2.4 ProofofLemma8
Proof. TheproofcanbeobtainedbycombiningLemma5,6and7. Specifically,wehave
K K
(cid:88) (cid:88)
V (s )− Vπk(s )
k,h k,h k,h k,h
k=1 k=1
(cid:118)
(cid:117) K H (cid:114) (cid:114)
(cid:117)(cid:88)(cid:88) (cid:112) H 1
≤2β(cid:98)K(cid:116) σ¯ k2
,h
2Hdlog(1+K/λ)+4 H3Klog
δ
+ 8H3Klog
δ
k=1h=1
√ √
(cid:0) (cid:1)
≤O(cid:101) dH4K+ d2H3K .
wherethefirstinequalityholdsbyLemma5andLemma6andthelastinequalityholdsbyLemma7.
Thisfinishestheproof. ■
C ProofofTheorem1
Proof. CombiningLemma1andLemma2,wehave
(cid:16)(cid:113) (cid:17) (cid:16)√ √ (cid:17)
D-Reg K(π 1c :K)≤O T(Hlog(S2A)+P¯ KlogT) +O(cid:101) dH4K+ d2H3K .
Thisfinishestheproof. ■
19D ProofofTheorem2
Proof. OurproofissimilartothatofLietal.[2023, Theorem4]. Atahighlevel, weprovethis
lowerboundbynotingthatoptimizingthedynamicregretoflinearmixtureMDPsisharderthan(i)
optimizingthestaticregretoflinearmixtureMDPswiththeunknowntransition,(ii)optimizingthe
dynamicregretoflinearmixtureMDPswiththeknowntransition. Thus,wecanconsiderthelower
boundofthesetwoproblemsseparatelyandcombinethemtoobtainthelowerboundofthedynamic
regretoflinearmixtureMDPswiththeunknowntransition.
First,weconsiderthelowerboundofthestaticregretofadversariallinearmixtureMDPswiththe
unknowntransition. FromlowerboundinHeetal.[2022,Theorem5.3],sincethedynamicregret
recoversthestaticregretbychoosingthebest-fixedpolicy,wehavethefollowinglowerboundfor
dynamicregretinthiscase:
√
D-Reg (πc )≥Ω( d2H3K). (19)
K 1:K
Then,weconsiderthelowerboundofthedynamicregretofadversariallinearmixtureMDPswith
theknowntransition.ZiminandNeu[2013]showthelowerboundofthestaticregretforadversarial
(cid:112)
episodicloop-freeMDPwithknowntransitionisΩ(H Klog(SA)). WenotethatthoughourMDP
modelisdifferentfromtheepisodicloop-freeMDP,wecantreatourMDPmodeltotheepisodic
(cid:112)
loop-freeMDPwithanexpandedstatespaceS′ = S ×[H]. Thus,Ω(H Klog(SA))isalsoa
lowerboundofthestaticregretfortheMDPinourwork. Weconsiderthefollowingtwocases:
Case1: Γ ≤ 2H. Inthiscase,wecandirectlyutilizethelowerboundofstaticregretasalower
boundofdynamicregret,i.e.,
(cid:112)
D-Reg (πc )≥Ω(H Klog(SA)). (20)
K 1:K
Case2:Γ>2H. Withoutlossofgenerality,weassumeL=⌈Γ/2H⌉dividesK andsplitthewhole
episodesintoLpiecesequally. Next,weconstructaspecialpolicysequencesuchthatthepolicy
sequenceisfixedwithineachpieceandonlychangesinthesplitpoint. Sincethesequencechanges
atmostL−1≤Γ/2H timesandtheoccupancymeasuredifferenceateachchangepointisatmost
2H,thetotalpathlengthinK episodesdoesnotexceedΓ. Asaresult,wehave
(cid:112) (cid:112) (cid:112)
D-Reg (πc )≥LH K/Llog(SA)=H KLlog(SA)≥Ω( KHΓlog(SA)). (21)
K 1:K
Combining(20)and(21),wehavethefollowinglowerboundforthedynamicregretofadversarial
linearmixtureMDPswiththeknowntransitionkernel,
D-Reg (πc )≥Ω(cid:0) max{H(cid:112) Klog(SA),(cid:112) KHΓlog(SA)}(cid:1)
K 1:K
(cid:112)
≥Ω( KH(H +Γ)log(SA)), (22)
wherethelastinequalityholdsbymax{a,b}≥(a+b)/2.
Combining two lower bounds (19) and (22), we have the lower bound of the dynamic regret of
adversariallinearmixtureMDPswiththeunknowntransitionkernel,
√
D-Reg (πc )≥Ω(cid:0) max{ d2H3K,(cid:112) KH(H +Γ)log(SA)}(cid:1)
K 1:K
√
(cid:0) (cid:112) (cid:1)
≥Ω d2H3K+ KH(H +Γ)log(SA) .
Thisfinishestheproof. ■
E SupportingLemmas
Inthissection,weintroducethesupportinglemmasusedintheproofs.
Lemma9(Three-pointidentity). LetX beaclosedandconvexset. Foranyx∈X andy,z ∈intX,
itholdsthat
D (x,y)+D (y,z)−D (x,z)=⟨∇ψ(z)−∇ψ(y),x−y⟩.
ψ ψ ψ
Lemma 10 (Abbasi-Yadkori et al. [2011, Lemma 11]). Let {x }∞ be a sequence in Rd space,
t t=1
V =λIanddefineV =V +(cid:80)t x x⊤. If∥x ∥ ≤L,∀i∈Z ,thenforeacht∈Z ,
0 t 0 i=1 i i i 2 + +
(cid:88)t (cid:110) (cid:111) (cid:18) dλ+tL2(cid:19)
min 1,∥x ∥ ≤2dlog .
i V i− −1 1 dλ
i=1
20