GRAPH-BASED SEMI-SUPERVISED SEGREGATED LIPSCHITZ
LEARNING
FARID BOZORGNIA, YASSINE BELKHEIRI, ABDERRAHIM ELMOATAZ
Abstract. Thispaperpresentsanapproachtosemi-supervisedlearningfortheclassificationof
datausingtheLipschitzLearningongraphs. Wedevelopagraph-basedsemi-supervisedlearning
frameworkthatleveragesthepropertiesoftheinfinityLaplaciantopropagatelabelsinadataset
where only a few samples are labeled. By extending the theory of spatial segregation from the
LaplaceoperatortotheinfinityLaplaceoperator,bothincontinuumanddiscretesettings,our
approach provides a robust method for dealing with class imbalance, a common challenge in
machine learning. Experimental validation on several benchmark datasets demonstrates that
our method not only improves classification accuracy compared to existing methods but also
ensures efficient label propagation in scenarios with limited labeled data.
1. Introduction
In recent years, nonlinear Partial Differential Equations (PDEs) on graphs have attracted in-
creasing interest due to their natural emergence in various applications in mathematics, physics,
biology, economics, and data science. For example, they are relevant in fields such as Inter-
net and road networks, social networks, population dynamics, image processing, and machine
learning. Indeed, a large amount of complex and irregular data is generated daily from various
sources, including the internet, images, point clouds, 3D meshes, and biological networks. These
datasets can be directly represented or modeled as graphs or functions defined on graphs.
Consequently, intensive research aims to develop new methods for processing and analyzing
data defined on graphs and adapt classical signal and image processing methods and concepts
to graphs.
Recently, several works have focused on the study of PDEs on graphs and their local or non-
local continuous limits in the Euclidean domain. Among the significant contributions, we can
mention the works of teams such as those of A. Bertozzi [7, 34, 46], Y. VanGuennip [10, 50, 51],
S. Osher [35, 48], and J. Mazon [43–45], who have proposed to adapt several types of continuous
PDEs and variational models to graphs. Other research aims to transpose various continuous
PDEs to graphs, such as p-Laplacian equations [24, 30, 36], infinity Laplacian [30], ”game”
p-Laplacian [28], Hamilton-Jacobi equations with or without diffusion [14, 31, 33], related to
certain stochastic games [26, 29], mean curvature flow equation, or p-biharmonic equation [33].
In this paper, utilizing the characteristic of Infinity Laplacian in propagating, we propose a
novel graph-based semi-supervised learning method aimed at classifying large volumes of unla-
beled data, particularly in scenarios involving imbalanced datasets and limited labeled samples.
Key words and phrases. Graph-Based, Semi-supervised learning, Infinity Laplacian, Lipschitz Learning.
1
4202
voN
5
]GL.sc[
1v37230.1142:viXra2 FARIDBOZORGNIA,YASSINEBELKHEIRI,ABDERRAHIMELMOATAZ
Our approach exploits the geometric and topological characteristics of the unlabeled data, inte-
grating these properties to enhance the development and performance of various algorithms.
2. Mathematical preliminaries on Infinity Laplacian
In this section, we provide an overview of the infinity Laplacian. The infinity Laplacian
equation lies at the crossroads of several mathematical disciplines and is applied in various
fields, including optimal transportation, game theory, image processing, computer vision, and
surfacereconstruction. ItwasfirstexploredbyGunnarAronsson[2],who,motivatedbyclassical
analysis, sought to develop Lipschitz extensions of functions. Over the past decade, researchers
in PDEs have made significant strides in establishing the existence, uniqueness, and regularity
of solutions to this equation. For detailed discussions on the uniqueness of Lipschitz extensions
and the theory of absolute minimizers, see [4, 20]. For recent advancements in the numerical
approximation of eigenvalues and eigenfunctions of the Infinity Laplace operator, we refer to [8].
Additionally,connectionshaveemergedlinkingthisequation(andthep-Laplacian)tocontinuous
values in Tug-of-War games. For further insights and applications related to these equations,
refer to [3, 4, 19, 21, 37, 38, 41, 42] and the associated references.
2.1. Local continuous infinity Laplacian. The infinity Laplacian operator is defined as fol-
lows:
(cid:88)d ∂u ∂u ∂2u
∆ u = (∇u)TD2u∇u = . (1)
∞
∂x ∂x ∂x ∂x
i j i j
i,j=1
The operator in (1) can be normalized by 1 , e.g., cf. [5].
|∇u|2
A function u is said to be infinity harmonic if it solves the homogeneous infinity Laplacian
equation
∆ u = 0. (2)
∞
intheviscositysense. Asmentionedearlierthisequationcanbederivedasthelimitofasequence
of p-Laplacian equations
∆ u = div(|∇u|p−2∇u) = 0,
p
under certain boundary conditions for p → ∞.
In [11], the author examines the convergence rate of solutions to the p-Laplace equation as
they approach solutions of the infinity-Laplace equation. Specifically, the study establishes a
convergence rate for solutions to the p-Laplace equation as p tends to infinity. The main results
−1
indicatethatthisconvergenceoccursatarateproportionaltop 4 ingeneralcases,andimproves
−1
to a faster rate of p 2 under certain conditions, such as the presence of a positive gradient.
The infinity Laplace equation is closely connected to the problem of finding the Absolute
Minimal Lipschitz Extension (AMLE) [2, 4]. This framework aims to identify a continuous
real-valued function that maintains the smallest possible Lipschitz constant on any open set
compactly contained within Ω. This perspective is essential for developing numerical approxi-
mation schemes for solving the infinity Laplace equation (2).
We consider two metric spaces (X,dX) and (Y,dY), which have the isometric extension
property.SEGREGATED LIPSCHITZ LEARNING FOR CLASSIFICATION 3
Definition 2.1. Let Ω be a subset of X and f : Ω → Y be a Lipschitz function. If g extends f
and Lip(g,X) = Lip(f,Ω) then we say that g is a minimal Lipschitz extension (MLE) of f.
A function u ∈ W1,∞(Ω) is called absolutely minimizing Lipschitz extension of a Lipschitz
function g if u| = g and
∂Ω
∥∇u∥ ≤ ∥∇v∥ ,
L∞(Ω′) L∞(Ω′)
for all open sets Ω′ ⊂ Ω and all v such that u−v ∈ W1,∞(Ω′). The connection between an
0
AMLE and the infinity Laplacian is such that a function u ∈ Lip(Ω) is an AMLE of a Lipschitz
function g : ∂Ω → R if and only if u is a viscosity solution of the infinity Laplacian equation
with u = g on ∂Ω.
Implementing the infinity Laplacian and related equations often involves significant mathe-
maticalchallenges, suchasthelackofregularityanddifficultiesinestablishingtheexistenceand
uniqueness of solutions. From a numerical perspective, these equations can behave quite differ-
ently from their linear counterparts, leading to interesting and complex theoretical problems.
To illustrate the motivation for using the nonlinear Infinity Laplacian operator in semi-
supervised learning tasks, let us examine the following example. Let Ω = [−1,1] × [−1,1],
and let u be the solution of the problem:
(cid:26)
∆u = 0 in Ω\{(0,0)},
(3)
u(0,0) = 1, u = 0 on ∂Ω.
Next, wereplacetheLaplaceoperatorwiththeInfinityLaplacianandsolvethecorresponding
problem. The results are illustrated in Figure 1. The figure highlights that the standard Lapla-
cian is degenerate in this setting, which leads to less effective label propagation. In contrast,
the Infinity Laplacian offers better propagation.
TodemonstratehowtheInfinityLaplacianhandlesimbalanceddatamoreeffectively, wesolve
theproblemwiththeInfinityLaplacianusing10interiorpointswithavalueof+1andthemiddle
point assigned a value of -1. Figure 2 shows the resulting surface of the solution across the grid,
indicating how the Infinity Laplacian distributes the labels more efficiently in such scenarios.
We would like to highlight the results from [24], where p-Laplacian regularization for semi-
supervised learning (SSL) on large, geometric random graphs is studied. The key problem
addressed in this work is how to utilize a small set of labeled points on a graph to predict
the labels of the remaining unlabeled points by leveraging the smoothness properties of the
data. The authors study the asymptotic behavior of the regularization technique as the number
of unlabeled points grows indefinitely. When p ≤ d, the solution becomes degenerate and
discontinuous, essentially leading to spikes and poor predictions. For p ≥ d+1, the solution
becomes smooth and well-behaved, making it ideal for prediction tasks. For small values of
p (closer to 2), the function estimate is highly sensitive to the underlying data distribution.
This means the estimate is informed by the density of the unlabeled data, which can improve
predictions if the data aligns with the cluster assumption. However, it risks overfitting or
discontinuities, especially in higher dimensions. For large values of p (approaching infinity), the
estimate tends to be a smooth, Lipschitz continuous solution that is independent of the data
distribution. This happens because large values of p lead to the Absolute Minimal Lipschitz
Extension (AMLE). While this solution is smooth, it can ignore valuable information from the
unlabeled data.4 FARIDBOZORGNIA,YASSINEBELKHEIRI,ABDERRAHIMELMOATAZ
Figure 1. Thefirstpictureontop-leftshowstheinitialvalue,therightindicates
the solution of Problem 3 and the last picture is the solution of infinity Laplacian
with the same boundary condition.
Figure 2. .SEGREGATED LIPSCHITZ LEARNING FOR CLASSIFICATION 5
2.2. Non-local continuous infinity Laplacian. Chambolle et al. [16] introduced a H¨older-
type infinity Laplacian, which can be viewed as a non-local variant of the traditional infinity
Laplacian. Their approach focuses on minimizing the following non-local functional:
(cid:90) |u(x )−u(x )|p
i j
dx dx , for α ∈ [0,1] (4)
|x −x |αp i j
Ω×Ω i j
The corresponding Euler-Lagrange equation for this functional is given by
(cid:90) |u(x )−u(x )|p−1sign(u(x )−u(x ))
i j i j
dx = 0. (5)
|x −x |α |x −x |α j
Ω i j i j
As p tends to infinity, this equation formally converges to a non-linear, non-local equation
L(u) = 0 on Ω with
(cid:18) (cid:19) (cid:18) (cid:19)
u(x )−u(x ) u(x )−u(x )
j i j i
L(u) = max + min , for x ∈ Ω. (6)
xj∈Ω,xj̸=xi |x j −x i|α xj∈Ω,xj̸=xi |x j −x i|α i
This operator L(u) is referred to as the H¨older infinity Laplacian.
3. Calculus on Graphs
In fully supervised learning, a model is trained using labeled data to learn a generalized
pattern. In semi-supervised learning, we use both labeled and unlabeled data, taking advantage
of the large amount of unlabeled samples. This approach is especially beneficial in situations
where labeled data is scarce. The lack of labeled data can limit a model’s ability to learn and
generalize effectively. However, by incorporating a larger set of unlabeled samples, the model
can uncover additional insights from the data, enhancing performance on tasks with few labeled
examples.
Graph-based semi-supervised learning methods are a common and important class of semi-
supervised learning techniques. These methods represent data as connectivity graphs, capturing
the entire dataset’s structure, including labeled and unlabeled samples. For a comprehensive
review of existing methods, refer to [17].
Let the data set consist of n samples X = {x ,x ,··· ,x } ⊂ Rd, we assume there is a subset
1 2 n
of the nodes Γ = {x ,··· ,x } ⊂ X that their labels are given, where m ≪ n. In graph-based
1 m
semi-supervised learning, we aim to extend labels to the rest of the vertices {x ,··· ,x }.
m+1 n
A common approach in semi-supervised learning involves utilizing unlabeled data by con-
structing a graph across the dataset. Given a set of samples X, which includes a few labeled
samples (without their labels Y) and a large amount of unlabeled data, we build a K-Nearest
Neighbor (K-NN) graph over X.
The next step involves creating an adjacency/weight matrix, denoted by W, over the con-
structed K-NN graph over data X. This matrix captures the similarities between pairs of data
points. If the data set consists of n samples then the weight matrix W is an n×n symmetric
matrix, where the element w represents the similarity between two samples x and x . The
ij i j6 FARIDBOZORGNIA,YASSINEBELKHEIRI,ABDERRAHIMELMOATAZ
similarity is always non-negative and should be large when x and x are close together spa-
i j
tially, and small (or zero), when x and x are far apart. The degree of a vertex x is given by
i j i
(cid:80)
d(x ) = w .
i ij
j∼i
Here, we review the definitions of p-Laplace and Infinity Laplacian on the graph given in
[23, 25, 29, 30, 32]. The weighted p-Laplace operator of a function u ∈ H(V) is defined by
(cid:88) p
∆ u(x ) = w2|u(x )−u(x )|p−2(u(x )−u(x )).
w,p i ij i j i j
j∼i
The p-Laplace learning for large values of p > d,( where d is the dimension of ambient space)
[24] involves solving the nonlinear graph p-Laplacian equation:
(cid:26)
∆ u(x ) = 0 x ∈ X \Γ,
w,p i i (7)
u(x ) = y x ∈ Γ.
i i i
Note by the Sobolev embedding Theorem for p > d the H¨older continuity of solution allows to
define the boundary values.
The L norm of the gradient for u ∈ H(V) at a vertex of the graph is defined by
p
∥∇ wu(x i)∥
p
=
[(cid:88)
w
ip
2 j|u(x i)−u(x
j)|p]p1
, 0 < p < +∞.
j∼i
By considering the upwind method to discretize the gradient on the graph we have
(cid:88) p
∥∇±u(x )∥p = w2((u(x )−u(x ))±)p, 0 < p < +∞.
w i p ij i j
j∼i
For the L -norm we have
∞
∥∇±u(x )∥ = max (cid:0)√ w (u(x )−u(x ))±(cid:1) .
w i ∞ ij i j
j∼i
The Infinity Laplacian is defined by
∆ u(x ) := 1 (cid:2) ∥∇−u(x )∥ −∥∇+u(x )∥ (cid:3) .
w,∞ i 2 w i ∞ w i ∞
3.1. Infinity Laplace equation. To solve the infinity Laplacian on a graph, we use two meth-
ods. The first is an approximation of the infinity Laplacian using the Minimal Lipschitz Exten-
sion, and the second is solving the evolutionary infinity Laplace equation [27].
3.1.1. Minimal Lipshitz extension. As in the previous section, for given a graph G = (V,E)
with a set of vertices V = {x ,x ,...,x } and a weight matrix W representing edge weights (or
1 2 n
similarities), the Infinity Laplacian denoted by ∆ u or ∆ u at a vertex x is defined as:
w,∞ ∞ i
(cid:20) (cid:21)
1 (cid:0)√ (cid:1) (cid:0)√ (cid:1)
∆ u(x ) = max w max(u(x )−u(x ),0) +max w max(u(x )−u(x ),0) ,
w,∞ i ij i j ij j i
2 j∼i j∼i
where j ∼ i indicates that x is a neighbor of x , and w is the weight (or similarity) between
j i ij
vertices x and x .
i j
To solve the Infinity Laplace equation ∆ u = 0 on a graph, we aim to find a function
w,∞
u : V → R that satisfies the equation at each vertex, subject to given boundary conditions (e.g.,
fixed values of u at certain labeled vertices).SEGREGATED LIPSCHITZ LEARNING FOR CLASSIFICATION 7
Define the graph structure G, including the vertices V, edges E, and the weight matrix W.
PartitionthevertexsetV intolabeledverticesΓ(wherelabelsareknown)andunlabeledvertices
V \Γ (where labels are to be predicted). Assign known labels to the vertices in Γ. Initialize
values for u(x ) at the unlabeled vertices x ∈ V \Γ. This could be random, zero, or based on
i i
some heuristic (such as averaging over neighbors).
Use an iterative scheme to update the values of u(x ) for the unlabeled vertices. A common
i
approach is to use fixed-point iteration or Gauss-Seidel iteration, where the update rule at each
iteration k+1 is given by minimizing the discrete Lipschitz constant at each vertex.
One defines a discrete Lipschitz constant L(u ) of u in x for i ∈ {1,...,n} as
i i
|u −u |
i j
L(u ) = max ,
i
j∈Ni d ij
where d = ∥x −x ∥ distance between node x and x . In [49][Theorem 4.3.1] it was proved
ij i j i j
that the minimizer of this discrete Lipschitz constant with respect to u is given by
i
d u +d u
u∗ = argmin L(u ) = is r ir s ,
i ui i d +d
ir is
where the indices r,s ∈ N are chosen such that
i
(cid:26) (cid:27)
|u −u |
k l
(r,s) ∈ arg max . (8)
k,l∈Ni d ik +d il
Thus to update the value of u(x ) we use the following iterative scheme:
i
(k) (k)
d u +d u
∗(k+1) is r ir s
u = . (9)
i d +d
ir is
We repeat the iterative update process until the changes in u(x ) between successive iterations
i
fall below a predefined tolerance level.
3.1.2. Infinity Laplacian [27]. In this part, we present an alternative approach to solve the
following problem:
(cid:26)
∆ u(x ) = 0 x ∈ V \Γ,
w,∞ i (10)
u(x ) = g(x ) x ∈ Γ.
i i i
To solve (10) iteratively, we make use of the associated evolution equation problem:
 ∂
 u(x ,t) = ∆ u(x ,t) x ∈ X \Γ,
 i w,∞ i i
∂t
(11)
u(x ,t) = g(x ) x ∈ Γ,
 i i i
 u(x ,t = 0) = u (x ) x ∈ X.
i 0 i i
Then, using an explicit forward Euler time discretization:
∂u un+1(x )−un(x )
i i
(x ,t) =
i
∂t ∆t
with un(x ) = u(x ,n∆t), we have the following iterations:
i i
 un+1(x ) = un(x )+∆t∆ un(x ) x ∈ X \Γ,
 i i w,∞ i i
un+1(x ) = g(x ) x ∈ Γ, (12)
i i i
 u0(x ) = u (x ) x X.
i 0 i i8 FARIDBOZORGNIA,YASSINEBELKHEIRI,ABDERRAHIMELMOATAZ
4. Related Works on Graph-Based Semi-Supervised Learning
Early work on classification using graph-based semi-supervised learning traces back to Zhu
et al. [1, 53] and Belkin et al. [6]. They transformed learning tasks on sets of vectors into
graph-based problems by identifying vectors as vertices and constructing graphs that capture
the relationships among data points. However, one limitation of this approach, as noted by
Nadler et al. [47], is that when the number of nodes increases while the number of labeled
samples remains fixed, almost all values of the Laplacian minimizer converge to the mean of the
labels on the unlabeled samples.
In the Laplace learning algorithm [47, 53], the labels are extended by finding the solution
u : X → Rk for the following problem:
(cid:26) Lu(x ) = 0, x ∈ X \Γ,
i i (13)
u = g, on Γ,
Here the unnormalized graph Laplacian L of a function v ∈ ℓ2(X) is defined as:
(cid:88)
Lv(x ) := w (v(x )−v(x )),
i ij i j
y∈X
where w represents the similarity between data points x and x Let u = (u ,...,u ) be a
ij i j 1 k
solution of (13). The label of node x ∈ X \Γ is determined by:
i
arg maxu (x ).
j i
j∈{1,...,k}
In [15], a scheme called Poisson Learning was proposed. This method modifies equation (13)
by replacing the zero values on the right-hand side with the label values of the training points,
thus solving the Poisson equation on the graph. The method extends labels from a discrete set
{x : i = 1,...,l} to the rest of the graph’s nodes by solving the following system:
i
(cid:26) Lu(x ) = y −y 1 ≤ i ≤ l,
i i (14)
Lu(x ) = 0 l+1 ≤ i ≤ n,
i
with the additional condition:
n
(cid:88)
d(x )u(x ) = 0,
i i
i=1
where y = 1 (cid:80)l y is the average label vector.
l i=1 i
In [13], the game-theoretic p-Laplacian for semi-supervised learning on graphs was studied. It
wasshownthattheapproachiswell-posedinthelimitoffinitelabeleddataandinfiniteunlabeled
data. The continuum limit of graph-based semi-supervised learning using the game-theoretic
p-Laplacian converges to a weighted version of the continuous p-Laplace equation. Additionally,
thestudydemonstratedthatsolutionstothegraphp-LaplaceequationareapproximatelyH¨older
continuous with high probability.
In[14]theconsistencyofLipschitzlearningongraphsinthelimitofinfiniteunlabeleddataand
finite labeled data has been studied. It has been shown in the case of a random geometric graph
with kernel-based weights, that Lipschitz learning is well-posed in this limit but insensitive to
the distribution of unlabeled data. Furthermore, on a random geometric graph with self-tuningSEGREGATED LIPSCHITZ LEARNING FOR CLASSIFICATION 9
weights, Lipschitz learning becomes highly sensitive to the distribution of the unlabeled data.
In both cases, the results stem from showing that the sequence of learned functions converges
to the viscosity solution of an Infinity Laplace-type equation and analyzing the structure of the
limiting equation.
The work [12] provides a thorough analysis of Lipschitz learning, particularly examining the
convergenceratesofsolutionstothegraphinfinityLaplaceequationtowardsthecontinuumcase
asthegraphdensityincreases. Thisworkissignificantforgraph-basedsemi-supervisedlearning,
where labels are propagated from a small labeled set to a larger unlabeled dataset by solving
an equation on the graph resembling the infinity Laplacian. Their results also demonstrate that
even in sparse graphs—such as those commonly used in semi-supervised learning—convergence
to continuum-based infinity Laplace solutions (or absolutely minimizing Lipschitz extensions) is
achievable under general assumptions. They use a comparison with distance functions on the
graph, which allows convergence rates even at low connectivity thresholds, making the approach
relevant to practical graph applications.
For algorithms related to Lipschitz learning, see [40]. A graph-based semi-supervised learning
approach using the theory of spatial segregation of competitive systems is discussed in [9].
5. Infinity Segregation
We consider the minimization of the following functional for large values of p:

(cid:90) k
  

minJ(u) := p1 (cid:88) |∇u i|pdx,
   Ω i=1
subject to: (15)

  u i ≥ 0, and u i·u j = 0 in Ω,



u = g on ∂Ω.
i i
First, we study a simpler case where the number of components is k = 2. Let the pair (u ,u )
1 2
be the minimizer. It is easy to check that u = u −u satisfies the following equation:
1 2
(cid:26)
−∆ u = 0 in Ω,
p (16)
u = g −g on Γ.
1 2
This result follows from the fact that
|∇u|p = |∇u |p+|∇u |p.
1 2
Additionally, we have u = max(u,0) and u = max(−u,0). Next, as p → ∞ in 16 we obtain:
1 2
(cid:26)
−∆ u = 0 in Ω,
∞ (17)
u = g −g on Γ.
1 2
This implies that for the binary classification we are solving the infinity Laplace equation
with boundary labels g ,g ∈ {±1}. The support of u corresponds to the first class, and the
1 2 1
support of u corresponds to the second class.
210 FARIDBOZORGNIA,YASSINEBELKHEIRI,ABDERRAHIMELMOATAZ
In the rest, we assume that k > 2. We define
(cid:88)
uˆ = u − u .
i i j
j̸=i
Lemma 5.1. Let U = (u ,...,u ) be the minimizer and Ω ,...,Ω be the corresponding sup-
1 k 1 k
ports then the following differential inequalities hold in Ω
(1) −∆ u ≤ 0
p i
(2) −∆ uˆ ≥ 0
p i
Proof. We follow the proof in [18] and we argue by contradiction, assuming the existence of an
index j such that the following inequality holds in a distributional sense
−∆ u > 0
p j
This means that
(cid:90)
|∇u |p−2·∇u ∇ϕ > 0, ∀ϕ ∈ C∞(Ω). (18)
j j 0
Ω
As a result, these exists ϕ ≥ 0, ϕ ∈ C∞(Ω) such that
0
(cid:90)
|∇u |p−2·∇u ·∇ϕdx > 0.
j j
Ω
Let ξ > 0 be very small and consider
(cid:40)
u i ̸= j
i
V =
i [u −ξϕ]+i = j.
j
From definition of v we have v ·v = 0 whenever i ̸= j, and v = ϕ on the boundary of Ω.
i i j i i
(cid:90)
1
J(V)−J(U) = (|∇(u −ξϕ)+|p−|∇u |p)dx
j j
p¯
Ω
(cid:90)
1
≤ ( |∇(u −ξϕ)|p−|∇u |p)dx
j j
p
Ω
(cid:90)
≤ −ξ |∇u |p−2∇u ·∇ϕdx+o(ξ) ≤ 0.
j j
Ω
Choosing ξ sufficiently small, we obtain
J(V)−J(U) ≤ 0.
By assumption, U is the minimizer so J(U) < J(v) is a contradiction with the above inequality.
Therefore,
(cid:90)
|∇u |p−2∇u ·∇ϕdx ≤ 0.
j j
Ω
The argument follows the same as the previous part to prove the second part. Assume there
i such that
−∆ uˆ ≤ 0,
p iSEGREGATED LIPSCHITZ LEARNING FOR CLASSIFICATION 11
multiply by test function ϕ ≥ 0 with compact support at Ω . Then
i
(cid:90)
|∇uˆ |p−2∇uˆ ·∇ϕdx ≥ 0.
i i
Ω
Then since u have disjoint supports, from above we get
i
(cid:90)
|∇u |p−2∇u ·∇ϕdx ≤ 0.
i i
Ω
This contradicts the fact that u is sub p harmonic.
i
□
Remark 5.2. Note that in Lemma 5.1 we proved that the differential inequalities hold in the
weak sense, However, the weak and viscosity are equivalent.
Next, we pass to the limit as p tends to infinity and we obtain that the minimizer for large p
satisfies the following differential inequalities
(1) −∆ u ≤ 0
∞ i
(2) −∆ uˆ ≥ 0
∞ i
6. Schemes
This section presents our algorithms for semi-supervised learning.
6.1. Infinity Learning Schemes. Our first method is called Infinity Laplace Learning as fol-
lows. Weaimtoextendlabelsfromadiscreteset{x : i = 1,...,l}torestofnodes{x ,x ,··· ,x }.
i l+1 l+2 n
Let k denote the number of classes. To indicate that a labeled sample x belongs to the jth class
i
we write u(x ) = e where {e ,e ,··· ,e } are standard basis for Rk. Let u = (u ,··· ,u ) We
i j 1 2 k 1 k
solve the following system :
(cid:26)
∆ u(x ) = 0 l+1 ≤ i ≤ n,
∞ i (19)
u(x ) = y ∈ {e ,e ,···e } 1 ≤ i ≤ l,
i i 1 2 k
Let u = (u ,··· ,u ) be a solution of (19), the label of node x ∈ X\Γ is dictated by
1 k i
arg maxu (x ).
j i
j∈{1,···,k}
The System (19) is uncoupled and the existence and uniqueness of solution u , i = 1,2,···k,
i
follows from [14] and u is characterized by the fact that it is an absolutely minimal Lipschitz
i
extension of labeling y .
i
6.2. Infinity Segregated Learning Schemes. By Lemma (5.1) the minimizer satisfies the
following differential inequalities
(1) −∆ u ≤ 0,
∞ i
(2) −∆ uˆ ≥ 0.
∞ i
Our scheme is based on a segregated system given by Lemma 5.1. The Lemma 5.1 states that
each u is infinity harmonic in its support, consequently sub-infinity harmonic in the domain,
i
and uˆ is supper infinity harmonic. By using (9) to solve infinity Laplacian operator and the
i12 FARIDBOZORGNIA,YASSINEBELKHEIRI,ABDERRAHIMELMOATAZ
facts that u (x) ≥ 0 and u (x)·u (x) = 0 and definition of uˆ we obtain the following iterative
i i j i
scheme
 (cid:32) (cid:33)
 u(m+1) (x ) = max u∗(m)(x )− (cid:80) u∗(m)(x ), 0 , x ∈ X\Γ,
  1 i 1 i p i i
  p̸=1
  (cid:32) (cid:33)

  u(m+1) (x ) = max u∗(m)(x )− (cid:80) u∗(m)(x ), 0 , x ∈ X\Γ,
  2 i 2 i p i i
p̸=2
(20)
............

  (cid:32) (cid:33)

    u( km+1) (x i) = max u k∗(m)(x i)− (cid:80) u∗ p(m)(x i), 0 , x i ∈ X\Γ,
  p̸=k


u (x ) = ϕ (x), x ∈ Γ, for all i = 1,2,...,k.
i i i i
7. Implementation
Considering the results mentioned earlier about p-Laplace regularization for large p, which
can lead to solutions that ignore the distribution of the dataset, employing a Siamese Neural
Network (SNN) offers a more data-sensitive approach.
7.1. SNN[39]. We use SNN to construct graphs over datasets that can be employed to model
relationships between data points based on their similarity in a learned feature space. In this
setup, the Siamese network helps in learning a suitable similarity measure between data points,
which can then be used to create edges in a graph. The primary role of the SNN is to generate
meaningful embeddings for the data points by learning a similarity function.
Here we explain the process: Start with a dataset, where each data point can be represented
as a vector (e.g., an image, a feature vector, or any other data type). Next pass pairs of data
points through the SNN. The network processes both points and outputs embeddings for each.
Since both branches of the network are identical and share the same weights, the embeddings
reflect the relative similarity of the input pairs.
After the SNN generates embeddings for each pair of data points, we use a distance function
(like Distance, cosine similarity, etc.) to measure how similar or dissimilar they are.
• Distance Metric: For two data points x and x , the output embeddings from the
i j
Siamese network f(x ) and f(x ) are compared using a distance function like:
i j
f(x )·f(x )
i j
Cosine similarity[52]: cosine similarity(x ,x ) =
i j
∥f(x )∥∥f(x )∥
i j
cosine similarity(x ,x )+1
i j
Adjusted similarity: sim(x ,x ) =
i j
2
1
Adjusted distance: d(x ,x ) = −1
i j
similarity(x ,x )
i j
Withthesimilarityscoresornearestneighborsidentified,agraphG = (V,E)canbeconstructed:
By training on pairs of data points, the SNN creates embeddings in a feature space where
the similarity between data points is directly informed by their relationships in the dataset.
Therefore,usinganSNNhelpsensurethatthegraph-basedmodelsretainimportantinformationSEGREGATED LIPSCHITZ LEARNING FOR CLASSIFICATION 13
about the dataset’s inherent structure. This makes SNN an ideal choice for constructing graphs
that accurately reflect the data relationships in semi-supervised learning tasks.
7.2. Results on Different Datasets. In this subsection, we implement our scheme on some
known data sets. For visualization,
Example 7.1. In our first example, we consider a balanced Two-Moon dataset consisting of
2,000 points with a noise level of 0.15. As the noise increases, the class boundaries become
more overlapping. Table 1 presents a comparison of the average accuracy between the Infinity
Laplacian (InfL), Infinity Segregated Learning (InfSL), and the Poisson scheme, for different
numbers of initial labels per class.
Table 1. Two moon, balanced case
Average overall accuracy over 100 trials for two moon
number of labels per class 1 2 3 4 5
InfSL .9745 .9830 .9811 .9856 .9861
InfL .8365 .8538 .8775 .9063 .9319
Poisson .7809 .8523 .8769 .9228 .9322
The results in Table 1 demonstrate the performance comparison of three algorithms—our
proposed InfSL, InfL, and Poisson Learning—on the balanced Two-Moon dataset. InfSL con-
sistently achieves the highest accuracy across all configurations. With just one label per class,
InfSL achieves an accuracy of 97.45%, improving to 98.61% with five labels per class. This
demonstrates the strength of InfSL in extracting relevant information from minimal labeled data.
Example 7.2. WeimplementourschemetotheFour-Moonsdatasetcontaining2000pointsand
a noise level of 0.15, with overlapping regions that increase the difficulty for algorithms to classify
the points correctly. Figures 3-5 show the classification results on the Four-Moons dataset using
three methods: InfSL, InfL, and Poisson Learning. Each figure represents a different experiment
with varying numbers of initial labels per class.
Figure 3. Classification results on 4 Moons with 1 label per class for InfSL,
InfL, and Poisson Learning.14 FARIDBOZORGNIA,YASSINEBELKHEIRI,ABDERRAHIMELMOATAZ
Figure 4. Classification results on 4 Moons with 3 labels per class for InfSL,
InfL, and Poisson Learning.
Figure 5. Classification results on 4 Moons with 5 labels per class for InfSL,
InfL, and Poisson Learning.
In Table 2, we present a comparison of the performance of our proposed method, InfSL, along
with InfL and Poisson Learning, on the Four-Moons dataset. The table displays the average of
accuracy for various numbers of initial labels per class.
Table 2. Average accuracy results for the 4 Moons dataset
Average overall accuracy over 100 trials for 4 Moons
Number of labels per class 1 2 3 4 5
InfSL .8215 .8855 .9278 .9367 .9441
InfL .6562 .7368 .7740 .8226 .8519
Poisson .6657 .7403 .7894 .8059 .8171
The results presented in Table 2 demonstrate the effectiveness of our proposed method, InfSL,
on the more challenging Four Moons dataset. InfSL consistently outperforms both InfL and
Poisson Learning across all label configurations. With only one label per class, InfSL achieves
an accuracy of 82.15%, significantly surpassing both InfL (65.62%) and Poisson Learning
(66.57%). As the number of labels per class increases, InfSL continues to outperform, reaching
an accuracy of 94.41% with five labels per class.SEGREGATED LIPSCHITZ LEARNING FOR CLASSIFICATION 15
These results demonstrate the strength of InfSL in handling complex, non-linear datasets like
4 Moons, particularly in scenarios with limited labeled data. The performance gap between InfSL
and the other methods grows as the classification task becomes more challenging, emphasizing
the robustness of InfSL for tasks requiring minimal supervision.
Example 7.3. We evaluated our proposed method on 10 imbalanced benchmark datasets from
the KEEL repository, which vary in both the level of class imbalance and sample size [22], as
summarized in Table 3. In Table. 3 the column denoted by n stands for the total number of
samples, p is the number of features, and IR indicates the ratio between majority and Minority
class samples in each data set. To evaluate the InfSl Algorithm, we use the metrics F -Score,
1
Table 3. Information of selected imbalanced benchmark datasets
Dataset IR p n %Minority
ecoli1 3.36 7 336 22.93
ecoli2 5.46 7 336 15.47
ecoli3 8.6 7 336 10.41
shuttle-c0-vs-c4 13.87 9 1829 6.72
glass6 6.38 9 214 13.55
new-thyroid1 5.14 5 215 16.28
new-thyroid2 5.14 5 215 16.28
page-blocks0 8.79 10 5472 10.21
segment0 6.02 19 2308 14.24
vehicle0 3.25 18 846 23.52
vowel0 9.98 13 988 9.10
Recall, Accuracy, and Precision for each class. To ensure consistency for all experiments, the
data set is first shuffled for each benchmark. Subsequently, 1 percent of the samples are randomly
chosen in accordance with the dataset’s IR as the labeled samples. This process is independently
repeated 100 times, then the averages of the aforementioned metrics are computed.
Table 4 outlines the detailed performance metrics, highlighting consistent improvements in
accuracy, F1-score, recall, and precision across the imbalanced datasets. The results demonstrate
that our method is particularly effective in handling extreme class imbalances, as seen with the
”shuttle-c0-vs-c4” dataset.
Example 7.4. Next, we evaluated the performance of our method and compared it with InfL
and the Poisson scheme using the well-known MNIST dataset. Table 5 presents the average
accuracy results over 100 trials, with varying numbers of labeled points per class (ranging from
2 to 10).
Table 5 provides a comparison of the performance of InfSL, InfL, and Poisson Learning on
the MNIST dataset. The rows display the average accuracy achieved for each method, while the
columns indicate the number of labeled samples per class, ranging from 2 to 10.
InfL and Poisson Learning start with lower accuracies, particularly with 2 labeled samples
per class, but gradually improve as more labeled samples are provided. By 10 labeled samples
per class, Poisson Learning slightly surpasses InfL, though both remain below the performance
of InfSL. This demonstrates the advantage of the graph refinement process in InfSL, which
effectively captures the underlying structure of the data with minimal supervision.16 FARIDBOZORGNIA,YASSINEBELKHEIRI,ABDERRAHIMELMOATAZ
Table 4. Information of selected imbalanced benchmark datasets
Algorithm InfSL
Dataset Accuracy F1 min F1 maj Recall min Recall maj Precision min Precision maj
ecoli1 0.8836 0.6974 0.9274 0.8352 0.8996 0.6287 0.9593
ecoli2 0.9352 0.8168 0.9583 0.8518 0.9644 0.8115 0.9579
ecoli3 0.9556 0.7918 0.9749 0.8379 0.9743 0.7757 0.9765
shuttle-c0-vs-c4 0.9895 0.904 0.9944 1.0 0.989 0.8433 1.0
glass6 0.9693 0.9377 0.978 0.9492 0.994 0.9607 0.9706
new-thyroid1 0.9908 0.9725 0.9945 0.947 0.9999 0.9994 0.9891
new-thyroid2 0.9862 0.9593 0.9917 0.9218 1.0 1.0 0.9835
page-blocks0 0.966 0.7991 0.9814 0.9805 0.965 0.6806 0.9985
segment0 0.9652 0.8489 0.9803 0.9962 0.962 0.758 0.9996
vehicle0 0.9905 0.9801 0.9938 0.9709 0.9967 0.9894 0.9909
vowel0 0.981 0.8809 0.9897 1.0 0.9796 0.7916 1.0
Table 5. Accuracy Results for the MNIST Dataset
Average overall accuracy over 100 trials for MNIST Dataset
numberoflabelsperclass 2 3 4 5 6 7 8 9 10
InfSL .994606 .994663 .994672 .994678 .994677 .994682 .994689 .994690 .994690
InfL .797549 .850493 .880603 .899981 .913188 .924895 .928351 .934989 .939326
Poisson .781556 .849058 .885421 .908756 .915682 .92546 .929523 .936487 .940658
Example 7.5. Next, we evaluated the performance of our method and compared it with Infinity
Learning (InfL) and the Poisson scheme using a real-world dataset collected in collaboration
with medical professionals. This dataset comprises a total of 452 images, including 205 images
of Koilocytotic—cells that exhibit specific morphological changes indicative of cervical cancer
and 247 images of normal cells. Figure 8 shows representative examples of the two classes in
the dataset, with 9 images for each class. The inclusion of these diverse images allows for a
comprehensiveevaluationofourmethod’sabilitytodifferentiatebetweenKoilocytoticandnormal
cells, which is critical for effective cervical cancer screening.
Table 6 provides a comparison of the performance of InfSL, InfL, and Poisson Learning on
the real dataset. The rows display the average accuracy achieved for each method, while the
columns indicate the number of labeled samples per class, ranging from 2 to 7.
Table 6. Accuracy Results for the Real Dataset
Average overall accuracy over 100 trials for the Real Dataset
number of labels per class 2 3 4 5 6 7
InfSL .7435 .8044 .8707 .8955 .9188 .9295
InfL .7501 .8330 .8840 .9100 .9438 .9503
Poisson .7128 .7815 .8041 .8088 .8138 .8227
These results highlight the effectiveness of InfSL in medical image classification tasks, par-
ticularly for identifying Koilocytotic associated with cervical cancer. While InfL consistentlySEGREGATED LIPSCHITZ LEARNING FOR CLASSIFICATION 17
Figure 6. (a) Normal Figure 7. (b) Koilocytotic
Figure 8. Representativeexamplesofcellsfromthenormal(a)andkoilocytotic
(b) classes in the real dataset.
outperforms InfSL, the latter still demonstrates strong capabilities, especially as the amount of
labeled data increases. This underscores the potential of InfSL in practical applications for med-
ical diagnostics, providing valuable support for detecting critical conditions like cervical cancer.
8. Conclusion
In this work, we have developed an efficient and robust graph-based semi-supervised learning
framework leveraging the Infinity Laplacian operator. By extending spatial segregation theory
tothegraph-basedInfinityLaplacian,theproposedInfinitySegregatedLearning(InfSL)method
demonstratesexceptionalperformance,particularlyinhandlingimbalanceddatasetsandscenar-
ios with sparse labeling. We Compared various datasets to establish InfSL’s effectiveness over
existing methods like Poisson Learning and standard Infinity Laplace schemes, highlighting its
superior classification accuracy and resilience to class imbalance. This study advances semi-
supervised learning by offering a novel method that effectively balances label propagation with
dataset structure. It is highly suitable for real-world applications in fields requiring minimal
labeled data, such as medical diagnostics and image processing.
References
[1] R.AndoandT.Zhang. Learningongraphwithlaplacianregularization. Advances in neural
information processing systems, 19, 2006.
[2] G. Aronsson. Minimization problems for the functional sup f(x,f(x),f′(x)). Arkiv f¨or
x
Matematik, 6(4):409–431, 1966.
[3] G. Aronsson. Extension of functions satisfying lipschitz conditions. Arkiv f¨or matematik,
6(6):551–561, 1967.
[4] G. Aronsson, M. Crandall, and P. Juutinen. A tour of the theory of absolutely minimizing
functions. Bulletin of the American mathematical society, 41(4):439–505, 2004.
[5] E. Barron, L. Evans, and R. Jensen. The infinity laplacian, aronsson’s equation and their
generalizations. Transactions of the American Mathematical Society, 360(1):77–101, 2008.18 FARIDBOZORGNIA,YASSINEBELKHEIRI,ABDERRAHIMELMOATAZ
[6] M. Belkin and P. Niyogi. Using manifold structure for partially labeled classification. Ad-
vances in Neural Information Processing Systems, 2002.
[7] A. L. Bertozzi and A. Flenner. Diffuse interface models on graphs for classification of high
dimensional data. Multiscale Modeling & Simulation, 10(3):1090–1118, 2012.
[8] F. Bozorgnia, L. Bungert, and D. Tenbrinck. The infinity laplacian eigenvalue problem:
Reformulation and a numerical scheme. Journal for Scientific Computing, 98(40), 2024.
[9] F. Bozorgnia, M. Fotouhi, A. Arakelyan, and A. Elmoataz. Graph based semisupervised
learning using spatial segregation theory. Journal of Computational Science, 74, 2023.
[10] J. Budd, Y. van Gennip, and J. Latz. Classification and image processing with a
semi-discrete scheme for fidelity forced allen–cahn on graphs. GAMM-Mitteilungen,
44(1):e202100004, 2021.
[11] L. Bungert. The convergence rate of p-harmonic to infinity-harmonic functions. Commu-
nications in Partial Differential Equations, 48(10-12):1323–1393, 2024.
[12] L. Bungert, J. Calder, and T. Roith. Uniform convergence rates for lipschitz learning on
graphs. IMA Journal of Numerical Analysis, 2023.
[13] J. Calder. The game theoretic p-laplacian and semi-supervised learning with few labels.
Nonlinearity, 2018.
[14] J. Calder. Consistency of lipschitz learning with infinite unlabeled data and finite labeled
data. SIAM Journal on Mathematics of Data Science, 1(4):780–812, 2019.
[15] J. Calder, B. Cook, M. Thorpe, and D. Slepcev. Poisson learning: Graph based semi-
supervisedlearningatverylowlabelrates. Proceedings of the 37th International Conference
on Machine Learning, 2020.
[16] A. Chambolle, E. Lindgren, and R. Monneau. A h¨older infinity laplacian. ESAIM: Control,
Optimisation and Calculus of Variations, 18(3):799–835, 2012.
[17] Y. Chong, Y. Ding, Q. Yan, and S. Pan. Graph-based semi-supervised learning: A review.
Neurocomputing, 408:216–230, 2020.
[18] M. Conti, S. Terracini, and G. Verzini. A variational problem for the spatial segregation of
reaction-diffusion systems. Indiana University Mathematics Journal, 54(3):779–815, 2005.
[19] M. G. Crandall. A visit with the ∞-laplace equation. Calculus of variations and nonlinear
partial differential equations, 1927:75–122, 2008.
[20] M. G. Crandall, L. C. Evans, and R. F. Gariepy. Optimal lipschitz extensions and the
infinity laplacian. Calculus of Variations and Partial Differential Equations, 13(2):123–139,
2001.
[21] M. G. Crandall, G. Gunnarsson, and P. Wang. Uniqueness of ∞-harmonic functions and
the eikonal equation. Communications in Partial Differential Equations, 32(10):1587–1615,
2007.
[22] J. Derrac, S. Garcia, L. Sanchez, and F. Herrera. Keel data-mining software tool: Data set
repository, integration of algorithms and experimental analysis framework. J. Mult. Valued
Logic Soft Comput, 17:255–287, 2015.
[23] X. Desquesnes, A. Elmoataz, and O. L´ezoray. Eikonal equation adaptation on weighted
graphs: fast geometric diffusion process for local and non-local image and data processing.
Journal of Mathematical Imaging and Vision, 46(2):238–257, 2013.
[24] A. El Alaoui, X. Cheng, A. Ramdas, M. J. Wainwright, and M. I. Jordan. Asymptotic
behavior of ℓ -based laplacian regularization in semi-supervised learning. In Conference on
pSEGREGATED LIPSCHITZ LEARNING FOR CLASSIFICATION 19
Learning Theory, pages 879–906. PMLR, 2016.
[25] I. El Bouchairi, J. M. Fadili, and A. Elmoataz. Continuum limit of p-laplacian evolution
problems on graphs: Lq graphons and sparse graphs. ESAIM: Mathematical Modelling and
Numerical Analysis, 57(3):1795–1838, 2023.
[26] A. Elmoataz and P. Buyssens. On the connection between tug-of-war games and nonlocal
pdes on graphs. Comptes Rendus. M´ecanique, 345(3):177–183, 2017.
[27] A.Elmoataz, X.Desquesnes, Z.Lakhdari, andO.L´ezoray. Nonlocalinfinitylaplacianequa-
tion on graphs with applications in image processing and machine learning. Mathematics
and Computers in Simulation, 102:153–163, 2014.
[28] A. Elmoataz, X. Desquesnes, and M. Toutain. On the game p-laplacian on weighted graphs
with applications in image processing and data clustering. European Journal of Applied
Mathematics, 28(6):922–948, 2017.
[29] A. Elmoataz, F. Lozes, and M. Toutain. Nonlocal pdes on graphs: From tug-of-war games
to unified interpolation on images and point clouds. Journal of Mathematical Imaging and
Vision, 57(3):381–401, 2017.
[30] A. Elmoataz, M. Toutain, and D. Tenbrinck. On the p-laplacian and ∞-laplacian on
graphs with applications in image and data processing. SIAM Journal on Imaging Sci-
ences, 8(4):2412–2451, 2015.
[31] H. Ennaji, N. Igbida, and V. T. Nguyen. Augmented Lagrangian methods for degenerate
Hamilton-Jacobi equations. Calc. Var. Partial Differ. Equ., 60(6):28, 2021. Id/No 238.
[32] H. Ennaji, Y. Qu´eau, and A. Elmoataz. Tug of war games and pdes on graphs with
applications in image and high dimensional data processing. Scientific Reports, 13(1):6045,
2023.
[33] J.Fadili,N.Forcadel,T.TuyenNguyen,andR.Zantout. Limitsandconsistencyofnonlocal
and graph approximations to the eikonal equation. IMA Journal of Numerical Analysis,
43(6):3685–3728, 2023.
[34] C. Garcia-Cardona, E. Merkurjev, A. L. Bertozzi, A. Flenner, and A. G. Percus. Multiclass
data segmentation using diffuse interface methods on graphs. IEEE transactions on pattern
analysis and machine intelligence, 36(8):1600–1613, 2014.
[35] G. Gilboa and S. Osher. Nonlocal operators with applications to image processing. Multi-
scale Modeling & Simulation, 7(3):1005–1028, 2009.
[36] Y. Hafiene, J. Fadili, and A. Elmoataz. Nonlocal p-laplacian evolution problems on graphs.
SIAM Journal on Numerical Analysis, 56(2):1064–1090, 2018.
[37] P. Juutinen and B. Kawohl. On the evolution governed by the infinity laplacian. Mathe-
matische Annalen, 335:819–851, 2006.
[38] P. Juutinen, P. Lindqvist, and J. J. Manfredi. The ∞-eigenvalue problem. Archive for
rational mechanics and analysis, 148(2):89–105, 1999.
[39] G. Koch, R. Zemel, R. Salakhutdinov, et al. Siamese neural networks for one-shot image
recognition. In ICML deep learning workshop, volume 2, pages 1–30. Lille, 2015.
[40] R. Kyng, A. Rao, S. Sachdeva, and D. A. Spielman. Algorithms for lipschitz learning on
graphs. In Conference on Learning Theory, pages 1190–1223. PMLR, 2015.
[41] J. J. Manfredi, M. Parviainen, and J. D. Rossi. Dynamic programming principle for tug-of-
wargameswithnoise. ESAIM: Control, Optimisation and Calculus of Variations, 18(1):81–
90, 2012.20 FARIDBOZORGNIA,YASSINEBELKHEIRI,ABDERRAHIMELMOATAZ
[42] J. J. Manfredi, M. Parviainen, and J. D. Rossi. On the definition and properties of p-
harmonious functions. Annali della Scuola Normale Superiore di Pisa-Classe di Scienze,
11(2):215–241, 2012.
[43] J.M.Maz´on,M.Solera,andJ.Toledo. Gradientflowsinmetricrandomwalkspaces. SeMA
Journal, 79(1):3–35, 2022.
[44] J. M. Maz´on, M. Solera, and J. Toledo. Variational and diffusion problems in random walk
spaces. Springer, 2023.
[45] J. M. Maz´on and J. Toledo. Cahn–hilliard equations on random walk spaces. Analysis and
Applications, 21(04):959–1000, 2023.
[46] E. Merkurjev, T. Kostic, and A. L. Bertozzi. An mbo scheme on graphs for classification
and image processing. SIAM Journal on Imaging Sciences, 6(4):1903–1930, 2013.
[47] B. Nadler, N. Srebro, and X. Zhou. Semi-supervised learning with the graph laplacian:
The limit of infinite unlabelled data. Advances in neural information processing systems,
22:1330–1338, 2009.
[48] S. Osher and J. Shen. Digitized pde method for data restoration. In Handbook of analytic
computational methods in applied Mathematics, pages 751–771. Chapman and Hall/CRC,
2019.
[49] T. V. Phan. Extensions Lipschitziennes minimales. PhD thesis, Rennes, INSA, 2015.
[50] Y.vanGennip. Anmboschemeforminimizingthegraphohta–kawasakifunctional. Journal
of Nonlinear Science, 30(5):2325–2373, 2020.
[51] Y. Van Gennip, N. Guillen, B. Osting, and A. L. Bertozzi. Mean curvature, threshold
dynamics, and phase field theory on finite graphs. Milan Journal of Mathematics, 82:3–65,
2014.
[52] P.Xia,L.Zhang,andF.Li. Learningsimilaritywithcosinesimilarityensemble. Information
Sciences, 307:39–52, 2015.
[53] X. Zhu, Z. Ghahramani, and J. D. Lafferty. Semi-supervised learning using gaussian fields
and harmonic functions. In Proceedings of the 20th International conference on Machine
learning (ICML-03), pages 912–919, 2003.
CAMGSD, Department of Mathematics, Instituto Superior Te´cnico, Lisbon, Portugal
Email address: faridb.bozorgnia@tecnico.ulisboa.pt
University of Caen Normandy, GREYC UMR CNRS 6072, France
Email address: yassine.belkheiri@unicaen.fr
University of Caen Normandy, GREYC UMR CNRS 6072, France
Email address: abderrahim.elmoataz@unicaen.fr