Preprint
DIFFLM: CONTROLLABLE SYNTHETIC DATA GENER-
ATION VIA DIFFUSION LANGUAGE MODELS
YingZhou1,2‚àó,XinyaoWang3‚àó,YuleiNiu3,YaojieShen1,2,LexinTang3,FanChen3,
BenHe1,2‚Ä†,LeSun2,LongyinWen3‚Ä†
1UniversityofChineseAcademyofSciences,Beijing,China
2InstituteofSoftware,ChineseAcademyofSciences,Beijing,China
3ByteDanceInc.,SanJose,USA
zhouying20@mails.ucas.ac.cn
ABSTRACT
Recent advancements in large language models (LLMs) have significantly en-
hanced their knowledge and generative capabilities, leading to a surge of inter-
est in leveraging LLMs for high-quality data synthesis. However, synthetic data
generation via prompting LLMs remains challenging due to LLMs‚Äô limited un-
derstandingoftargetdatadistributionsandthecomplexityofpromptengineering,
especially for structured formatted data. To address these issues, we introduce
DiffLM,acontrollabledatasynthesisframeworkbasedonvariationalautoencoder
(VAE),whichfurther(1)leveragesdiffusionmodelstoreservemoreinformation
oforiginaldistributionandformatstructureinthelearnedlatentdistributionand
(2)decouplesthelearningoftargetdistributionknowledgefromtheLLM‚Äôsgen-
erativeobjectivesviaaplug-and-playlatentfeatureinjectionmodule. Asweob-
servedsignificantdiscrepanciesbetweentheVAE‚Äôslatentrepresentationsandthe
realdatadistribution,thelatentdiffusionmoduleisintroducedintoourframework
to learn a fully expressive latent distribution. Evaluations on seven real-world
datasetswithstructuredformatteddata(i.e.,Tabular,CodeandTooldata)demon-
stratethatDiffLMgenerateshigh-qualitydata,withperformanceondownstream
taskssurpassingthatofrealdataby2%‚Äì7%incertaincases. Thedataandcode
willbepubliclyavailableuponcompletionofinternalreview.
1 INTRODUCTION
Data Synthesis has become an indispensable technique in current machine learning research, en-
abling rapid generation and modification of datasets (Bauer et al., 2024), allowing researchers to
experimentwithvariousscenariosandmodelarchitectureswithouttheextensiveprocessesassoci-
ated with real-world data collection. Meanwhile, with the rapid advancements in large language
models(LLMs),recentresearchinnaturallanguageprocessing(NLP)hasincreasinglyfocusedon
leveragingLLMsforsyntheticdatageneration. Earlyeffortsattemptedtofine-tuneLLMstoalign
withrealdatadistributions(Keskaretal.,2019;Anaby-Tavoretal.,2020;Borisovetal.,2023). As
thein-contextlearningcapabilitiesofLLMshaveimproved,somestudieshaveexploredzero-shot
orfew-shotpromptingofLLMstogeneratesyntheticdata(Yeetal.,2022a;Weietal.,2024).
Despite the progress achieved, generating high-quality synthetic textual data using LLMs remains
challenging,particularlyforstructureddata(Josifoskietal.,2023;Lietal.,2022).First,LLMsoften
lackaglobalunderstandingofthetargetdatadistributionwhengeneratingsyntheticdata.Evenafter
fine-tuning, it is difficult to inject information about complex and varied distributions into current
LLMarchitectures,oftenresultinginoutputswithlowdiversityandinstancesofdatacopying(Wu
et al., 2024; Yu et al., 2023). Moreover, existing LLM-based synthetic data generation methods
typicallyinvolvecomplexpipelinesandpost-processingmechanisms,suchaspromptengineering,
multi-agent frameworks, and iterative sampling (Dekoninck et al., 2024; Wu et al., 2024). These
complexities hinder the rapid adaptation of LLMs to new tasks, limiting their utility in dynamic
‚àóEqualcontribution.
‚Ä†Correspondingauthor.
1
4202
voN
5
]GL.sc[
1v05230.1142:viXraPreprint
research and industrial scenario. Concurrently, the remarkable performance of variational autoen-
coders(VAEs)anddiffusionmodelsinimagesynthesistasks(Betkeretal.,2023;Rombachetal.,
2022)hasspurredinterestinadaptingthesetechniquestoothermodalities(Borisovetal.,2023;Li
etal.,2022;Gongetal.,2023). Althoughsomeworkshaveintroducedlatentspacesintolanguage
modelsforsimpletaskslikestyletransferortopicgeneration(Yang&Klein,2021;Lietal.,2022),
ourpreliminaryexperimentsindicatethatdirectlyapplyingthelatentdistributionslearnedbyVAEs
oftenresultsinoutputsthatareunrelatedtotherealdata. Thischallengesthedirectapplicationof
thesemethodsinmorecomplexscenariosforsyntheticdatageneration.
Toaddressthesechallenges,weproposeDiffLM,anovelframeworkthatleveragesaplug-and-play
latentspacetoprovidedatadistributioninformationforLLMsduringdatageneration. First,tode-
couplethelearningofrealdatadistributionsfromtheLLM‚Äôstrainingobjectives,wedevelopalatent
spaceusingaVAEtocaptureexternalinformation,mappingsamplesfromtherealdatasettolatent
vectors. However, we observed that simply sampling from a Gaussian distribution obtained from
naiveVAEthatcannotgeneraterealisticresults. Toovercomethepoorqualityofdatageneratedby
samplingfromVAE,weemployalatentdiffusionmethodthatlinearlyaddsnoisetothelatentspace
overtime. Adenoisingnetworkisthentrainedtolearnthesenoisesinthereverseprocess,reducing
efficiencylossindatasynthesisduetosamplingfailures.Finally,wedesignasoftpromptingmethod
toinjectlatentfeaturesintotheLLMdecodingprocess,resultingincontrollable,high-qualitysyn-
theticdata.Weevaluateourmethodonsevenreal-worldstructuredformatteddatasets,rangingfrom
simple table synthesis to more complex code and tool synthesis tasks. Experiments demonstrate
that DiffLM can generate realistic results, and ablation studies confirm the effectiveness of each
componentinourproposedmethod. Thecontributionsofthispaperarethreefold:
‚Ä¢ DecouplingDataDistributionLearning: WeproposedanewVAE-basedLLMframeworkfor
datasysthesis,whichdecouplesthelearningofrealdatadistributioninformationfromthetraining
objectivesoftheLLMbyintroducingtheasmallprojectionnetwork.
‚Ä¢ High-QualitySyntheticDataGeneration:Basedonourobservations,themeticulouslydesigned
VAEanddiffusionstructureseffectivelymodelthedistributionofrealdata,enablingthegenera-
tionofhigh-qualitysyntheticdata. Inalltasks,thequalityofthegenerateddataiscomparableto
orevensurpassesthatoftherealdata.
‚Ä¢ Comprehensive Evaluation: We validate the high quality of data generated by DiffLM across
threedistinctscenariosandsevendatasets,underscoringitsrobustnessandadaptabilityinadvanc-
ingsyntheticdatagenerationfornaturallanguageprocessing.
2 RELATED WORKS
LargeLanguageModelsinDataSynthesis. Therecentadvancementinthegenerativecapabil-
itiesofLLMshasmotivatednumerousexploratoryworksaimingtoleveragethesemodelsfordata
augmentationinareassuchastextclassification(Yeetal.,2022a;Lietal.,2023),informationex-
traction(Tangetal.,2023;Josifoskietal.,2023),andtabulardatageneration(Borisovetal.,2023;
Xuetal.,2024). AcomprehensivesurveyconductedbyLongetal.(2024)proposesaprompt-based
generic workflow for synthetic data generation, curation, and evaluation. And multiple advanced
workshaveattemptedtofine-tunelanguagemodelsfordatasynthesisinrecentyears(Anaby-Tavor
etal.,2020;Kumaretal.,2020;Dinhetal.,2022;Borisovetal.,2023;Xuetal.,2024).Specifically,
these methods involve fine-tuning LLMs on a small amount of gold data for language modeling,
followed by the use of various sampling methods to generate data. However, a major challenge
remainsinensuringthatsyntheticdataaccuratelyreflectsreal-worlddistributions. Veselovskyetal.
(2023) has shown that LLM-generated data can sometimes diverge from actual data distributions,
leading to unfaithful representations that may hinder model training. Some studies have explored
dataselection(Purietal.,2020)ordataaugmentation(Yeetal.,2022b)toaddressthisdistribution
gap,butthereremainssignificantroomforimprovement.
LatentVariableModelsinTextGeneration. Latentvariablemodelshavemadesignificantad-
vancesincomputervisioninrecentyears(Yuetal.,2022;Guetal.,2022;Luoetal.,2023;Gulrajani
et al., 2017), achieving high-quality generation results, flexibility and effectiveness, as well as ro-
bustness to noise perturbations. In particular, latent diffusion models, such as DALL-E (Betker
etal.,2023)andStableDiffusion(Rombachetal.,2022),operatetheirdiffusionprocessesinala-
2Preprint
Encoding Process
ùêøùëúùë†ùë†&‚Äô=ùê∑()(ùëû*ùëßùë• ‚à•ùëù(ùëß)) Latent Space
{
age: 38,
w e o sed co xcu :r uc k Mpac t al aia to lis eons ,: n: P :H r Fi Sv a-g ra mrte a i, d n , g -fishing, üî• EnL coM d er ¬µ + ùúé ‚Ä¶ ‚Ä¶ Dz if0 fusion Foz rwt ard Procz en ss
native-country: United-States,
income: <=50K ùëûùëß"ùëß"#$ =ùí©(ùëß"; 1‚àíùõΩ" ùëß"#$,ùõΩ"Œô)
}
‚Ä¶
ùíõùüé=ùê∏ùëõùëêùëúùëëùëíùëü(ùíô)
‚Ä¶
{ DenoisingMLP
age: 38,
ùêª"#$%&$=ùêºùëõùëóùëíùëêùë°ùëúùëü(ùíõùüé)
workclass: Private, ‚ùÑ ‚Ä¶
e od ccu uc pat ai to ion n: :H FS a-g rmra id n, g -fishing, DeL cL oM d er FL ea at te un rt e ‚Ä¶ ‚Ä¶ DiN stro ibrm uta iol n
sex: Male, Injector
native-country: United-States,
income: <=50K
} ùêøùëúùë†ùë†+,-=ùîº.!ùëßùë•(logùëù!ùë•ùëß)
Latent Denoising Backward Process
Synthesizing Process Distribution p!ùëß"ùëß"#$ =ùí©(ùëß"#$;ùúá!ùëß",ùë°,Œï!z%,t)
Figure 1: Overview of our DiffLM. The trainable lanaguage model (LM) works as VAE encoder
whilethefixedLLMdecoderservesasVAEdecoder. Wefurther(1)introducedaDiffusionmodule
tolearnthelatentspace,and(2)employalatentfeatureinjectorwithsoftpromptingtoalignlatent
vectorspacewithLLMdecoder.
tent space rather than directly in data space, enabling a near-optimal balance between generation
qualityandcomputationalefficiency. Intextgeneration,severalworks(Wisemanetal.,2018;Ding
& Gimpel, 2019; Li et al., 2022; Gu et al., 2023; Borisov et al., 2023) have attempted to combine
latentspaceswithlanguagemodelstoaccomplishtaskssuchassentencerepresentation, textstyle
transfer,anddatasetaugmentation.Additionally,somestudieshaveexploredtheuseofdiffusionfor
plug-and-playcontrollablegeneration(Lietal.,2022;Gongetal.,2023),aimingtosteertheoutputs
ofpre-trainedlanguagemodelusingauxiliarymodules. Whiletheseworksshareasimilarperspec-
tive with ours, we tackle a more challenging scenario of structured data synthesis and thoroughly
investigatemultiplemethodsoflatentknowledgeinjection. Tothebestofourknowledge,ourwork
isthefirsttocombineVAEsanddiffusionmodelswithLLMsforhigh-qualitydatasynthesis.
3 METHODOLOGY
Figure1illustratesthemainpipelineofourproposedDiffLM.First, wedefineanencodertomap
discrete data into a continuous latent space (Section 3.2). Second, although text features are ex-
tracted and compressed, vanilla latent embeddings in VAEs often lead to decoding failures due to
underutilizedoremptyregionsinthelatentspace. Toaddressthisissue,weadoptalatentdiffusion
model(Section3.3). Finally,toincorporatethepriorknowledgeintothedecodingstageofLLMs,
weproposeanovelsoftpromptinjectionmethodtosteerthedecodingprocess(Section3.4).
3.1 PROBLEMFORMULATION
WebeginbydefiningDasaknownsmallsetofreal-worlddistributiondata,whereeachelementx
representsarealsample.WedefineGasthesyntheticdatagenerator,whichlearnsthedistributionof
Dandgeneratesasetofsyntheticsamples,D ,ensuringthatthemodeldoesnotsimplymemorize
syn
and reproduce the same real samples, meaning D‚à©D = ‚àÖ. It should be noted that we focus
syn
on the task of unconditional data synthesising using LLMs, where G generates synthetic samples
independentlyofanyadditionalcontext,i.e.,withoutusingexplicitprompttext.
3.2 VAE-BASEDREPRESENTATIONLEARNING
Feature Encoding: In standard VAEs, an encoder is typically employed to map input data into a
latentspace. Givenstructuredtextdatas ,weutilizealearnableTransformer-basedpre-trainedlan-
i
guagemodel(Vaswanietal.,2017;Devlinetal.,2019;Raffeletal.,2020)toobtaintherepresenta-
3Preprint
tionvectorx ‚ààRd√ó2,whichcanbesplitintothemeanandvariance.Usingthere-parameterization
i
trick(Kingma&Welling,2014),wethenobtainthelatentfeaturez ‚ààRd:
z =¬µ+œÉ‚äôœµ, (1)
where¬µandœÉ arethemeanandstandarddeviationoutputbytheencoder,andœµissampledfroma
standardnormaldistributionN(0,I).
LLMDecoding: Aftergenerating thelatent featurez, we employafrozen-parameter LLMto re-
constructtheinputtextsinacausallanguagemodelingmanner. TherationaleforfreezingtheLLM
parameters is to avoid retraining and to preserve its general knowledge and capabilities. Conse-
quently, aligning the two different modalities, whereas the latent space and the LLM input space,
presentsasignificantchallenge.Toaddressthis,weproposeanovellatentfeatureinjectorusingsoft
promptinganddesignacorrespondinginjectornetwork;specificdetailsareprovidedinSection3.4.
VAE Training Objective: The VAE model is typically trained using the Evidence Lower Bound
(ELBO)lossfunction.Followingpreviouswork(Burgessetal.,2018),weadopttheŒ≤-VAEtraining
strategy,whichintroducesaweightingparameterŒ≤tocontrolthecontributionoftheKLdivergence
lossinthetotallossfunction.Specifically,whenŒ≤ =0,themodelreducestoastandardautoencoder.
ForŒ≤ >0,theKLconstraintencourageslearningasmootherlatentspace:
ELBO =L ‚àíŒ≤L , (2)
Œ≤ rec kl
L =E (cid:0) logp (x|z)(cid:1) , (3)
rec qœï(z|x) Œ∏
(cid:0) (cid:1)
L =D q (z|x)‚à•p(z) , (4)
kl KL œï
wherep (x|z)isthelanguagemodelingreconstructionlikelihood,q (z|x)istheapproximatepos-
Œ∏ œï
terior, and p(z) is the prior over the latent space, i.e., Gaussian distribution. In our model design,
considering the denoising network of latent diffusion, we adopt an decreasing Œ≤ adjustment strat-
egy. We initially set a larger Œ≤ weight to enforce a strong regularization on the latent space. As
the reconstruction loss convergence slows, we decrease the Œ≤ value to allow the model to focus
moreonreconstructionaccuracy. Additionally,weemployanearlystoppingmechanismtoprevent
overfitting.
3.3 LATENTSPACEDENOISING
Although VAE can learns latent space representations of data, directly sampling from the prior
distribution p(z) often exhibit low quality generated samples. In our preliminary experiments, we
observedthatdirectlyutilizingthelatentfeatureslearnedbytheVAEfrequentlyproducestextthat
is unrelated to the target data distribution. This issue arises due to the discrepancy between the
encoder‚Äôs learned posterior distribution q (z|x) and the prior p(z). To address this problem, we
œï
introduceadiffusionmodelinthelatentspacetomoreaccuratelymodelthetruedistributionofthe
latentfeatures. InspiredbyZhangetal.(2024),weextractthelatentvectorsz ‚ààZ fromthetrained
VAE for each data point x ‚àà D . Starting from the initial latent vector z , we progressively
train 0
addnoiseovertimefollowingalinearscheduletogetz . Duringthereversediffusionprocess,we
t
employastandardcontinuousdenoisingnetworktorecoverz (Songetal.,2021). Forthetraining
0
objective,weoptimizethediffusionmodelthroughdenoisingscorematching(Karrasetal.,2022):
z =z +œÉ(t)œµ,œµ‚ààN(0,I), (5)
t 0
(cid:112)
dz =‚àíœÉÀô(t)œÉ(t)‚àá logp(z )dt+ 2œÉÀô(t)œÉ(t)dœâ , (6)
t zt t t
L =E ‚à•œµ (z ,t)‚àíœµ‚à•2, (7)
diff t‚àºp(t),z0‚àºp(z0),œµ‚àºN(0,I) Œ∏ t
InforwardprocessEq.5,z isthelatentvariableattimet,andœÉ(t)isatime-dependentnoisescale
t
function.AsforbackwardprocessEq.6,œÉÀô(t)standsforthetimederivativeofœÉ(t),and‚àá logp(z )
zt t
is the gradient of the log probability density with respect to z , also known as the score function,
t
and dœâ is an increment of the Wiener process (standard Brownian motion). For diffusion model
t
traininglossEq.7,œµ (z ,t)istheneuralnetworkthatpredictsthenoiseœµgivenz andt.Thedetailed
Œ∏ t t
descriptionfordiffusionmodelcouldbefoundinAppendixA.1.
3.4 LATENTFEATUREINJECTION
Afterconstructingalatentspacethatcapturesthetruedatadistribution,twochallengesremain: 1)
AligninglatentspacewithLLM‚Äôsinputspace. HowcanthedecodingLLMprocessthelatentvector
4Preprint
Table 1: Performance of downstream tasks using generated tabular data. We evaluate the quality
from:performanceinmachinelearningefficiency(MLE)task,andcolumn-wisedistributiondensity
estimation (œÅ) task. ‚Üë,‚Üì indicate that higher (or lower) metrics correspond to better performance.
Boldface indicates DiffLM surpasses the SoTA model based on language models. Red Boldface
denotesDiffLMexceedstheMLEperformanceachievedusingrealdata.
Adult Default Magic Shoppers Beijing
Method
MLE‚Üë œÅ‚Üì MLE‚Üë œÅ‚Üì MLE‚Üë œÅ‚Üì MLE‚Üë œÅ‚Üì MLE‚Üì œÅ‚Üì
Real 0.927 - 0.770 - 0.946 - 0.926 - 0.423 -
SMOTE 0.899 1.60 0.741 1.48 0.934 0.91 0.911 2.68 0.593 1.85
CTGAN 0.886 16.84 0.696 16.83 0.855 9.810 0.875 21.15 0.902 21.39
TVAE 0.878 14.22 0.724 10.17 0.887 8.250 0.871 24.51 0.770 19.16
GOGGLE 0.778 16.97 0.584 17.02 0.654 1.900 0.658 22.33 1.090 16.93
CoDi 0.871 21.38 0.525 15.77 0.932 11.56 0.865 31.84 0.818 16.94
TabSyn 0.915 0.58 0.764 0.85 0.938 0.88 0.920 1.43 0.582 1.12
GReaT 0.913 12.12 0.755 19.94 0.888 16.16 0.902 14.51 0.653 8.25
DiffLM 0.894 9.16 0.793 9.33 0.910 7.04 0.912 14.43 0.717 6.05
z to steer a powerful language model for realistic data generation? 2) Seamless Integration with
LLMKnowledge: HowcanweintegrateexternalinformationwithoutdisruptingtheLLM‚Äôsinternal
knowledge? Motivated by adapter training methods in LLM fine-tuning (Lester et al., 2021; Li &
Liang, 2021; Houlsby et al., 2019; Liu et al., 2023a), we consider the soft prompt latent injection
approach to incorporate z into LLM decoding without training the model weights. Specifically,
afterobtainingthelatentrepresentationz,weuseanupperMLPtomapitintoksoftprompttoken
embeddings, denoted as H ‚àà Rk√ód. These soft embeddings serve as a steering vector, which
latent
is concatenated before the <BOS> token to assist the LLM in decoding. The detailed process is
illustrated in Figure 4. We also conduct ablation experiments in Section 5.1 with the other two
injection methods proposed by Li et al. (2020), which validated that our methods obtain the best
reconstructionlossanddownstreamtaskperformance.
4 EXPERIMENTS
Inthissection,weevaluatethegenerationqualityoftheDiffLMmethodonmultiplepublicbench-
marks across three tasks: 1) Tabular Data Generation: We compare DiffLM with SoTA tabular
generation algorithms, demonstrating its strong capability in structured data generation. 2) Code
Generation:DiffLMshowcasestheabilitytointegratestructureddatapriorswithitsinternalknowl-
edge. Theresultsonsyntheticdataareevenbetterthanrealones. 3)ToolGeneration: DiffLMcan
quicklyadapttocomplexfunctioncallscenarios,highlightingitsflexibilityandadaptability.
4.1 TABULARDATAGENERATION
Benchmarking. We selected five publicly available datasets for evaluation, encompassing both
classificationandregressiontasks: Adult,Beijing,Default,Magic,andShoppers. Thepropertiesof
datasetsarepresentedinTable5. Toassessthequalityofsyntheticdata,weemployedtwoperspec-
tives: 1)Low-orderstatisticalmetrics,wherewequantifiedcolumn-wisedensityestimationusing
theKolmogorov-SmirnovTestfornumericalcolumnsandtheTotalVariationDistanceforcategor-
ical columns; 2) Downstream task performance, where we measured the predictive accuracy on
testdataofclassifiersorregressorstrainedonthegenerateddata.
Baselines. We selected a comprehensive set of classic and SoTA tabular data generation models
with diverse architectures for comparison. First, we consider the performance on real data as the
upper bound for evaluation. Secondly, we included the classic method, synthetic minority over-
sampling technique (SMOTE) (Chawla et al., 2002), which generates new synthetic data patterns
by performing linear interpolation between minority class samples and their k nearest neighbors.
Additionally, for neural network-based tabular generation algorithms, we considered six baseline
5Preprint
Table 2: pass@k scores on HumanEval and MBPP. We follow Chen et al. (2021) for estimating
pass@k, where n > k solutions are generated per problem with p = 0.95 and a temperature of
0.2tocalculatethesuccessratewithzero-shotlearning. BoldfaceindicatesthatDiffLMsurpasses
the performance achieved with real data. Red Boldface indicates that DiffLM surpasses the base
model‚Äôsperformance.
HumanEval MBPP
Model Size
pass@1 pass@10 pass@100 pass@1 pass@10 pass@100
GPT-4 - 67.00 - - - - -
7B 33.50 59.60 85.90 41.40‚àó 66.70‚àó 82.50‚àó
CodeLLaMA
34B 48.80 76.80 93.00 55.00‚àó 76.20‚àó 86.60‚àó
7B 27.79 41.22 56.37 37.31 52.02 59.65
Mistral-Base
12B 10.12‚Ä† 20.91‚Ä† 28.93‚Ä† 43.38 61.44 69.09
7B 36.09 52.95 64.18 38.45 50.77 59.17
Mistral-Instruct
12B 7.08‚Ä† 12.43‚Ä† 16.14‚Ä† 52.20 63.61 69.02
7B 28.58 42.24 54.24 27.15 42.21 48.14
Mistral-Real-Code
12B 36.97 52.04 60.95 34.79 45.49 50.22
7B 35.37 47.36 54.38 32.70 41.65 47.39
Mistral-DiffLM-Code
12B 42.24 56.02 61.97 44.42 52.35 55.70
* Theseresultsareevaluatedundera3-shotsetting.
‚Ä† The vanilla Mistral-Nemo 12B models fail to pass the HumanEval benchmark, resulting in a lower
score.Wehaveconductedmultipleevaluationsandreporttheaverageperformance.
models across different architectures: 1) GAN-based models: CTGAN (Xu et al., 2019); 2) VAE-
based models: TVAE (Xu et al., 2019), GOGGLE (Liu et al., 2023b); 3) Diffusion-based models:
Codi(Leeetal.,2023),TabSyn(Zhangetal.,2024);4)LLM-based: GReaT(Borisovetal.,2023),
which attempts to fine-tune a GPT-2 (Radford et al., 2019) for table synthesis. It is worth noting
that we compare with the current strongest generative models not to merely outperform them in
tabulargenerationbuttodemonstratethatourflexibleDiffLMarchitecturecanachievecomparable
performancewhileofferingadditionaladvantages.
Evaluation. Table 1 presents the quality assessment results of the generated data. For different
tabulardatasets,wetrainaXGBoostclassifieroraregressorusingthesyntheticdatatopredictthe
labelcolumnvalues,usingAUCandRMSEtoevaluatetheaccuracy,respectively. Fromtheresults,
DiffLMoutperformsthecurrentlanguage-model-basedSoTA(GReaTmodel)onmostdatasets.No-
tably, on the Default dataset, the prediction accuracy using DiffLM‚Äôs synthetic data surpasses that
obtainedbytrainingonrealdata. ThissuggeststhatDiffLM‚Äôsapproachofintegratingtherealdata
distribution with its own learned knowledge can provide richer information for downstream tasks
whilepreservingtheoriginaldatastructure. Inotherwords,thesyntheticdatageneratedbyDiffLM
contains additional knowledge compared to real data, which is challenging to achieve with previ-
ous methods. Moreover, our generated results achieve performance comparable to prior methods
incolumn-wisedistributiondensityestimation. AlthoughtheTabSynmethodattainssuperiorper-
formance on several datasets, it should be noted that our approach focuses on general, pluggable
generationcontrolforlargelanguagemodel,ratherthantrainingdatasynthesismodelsfromscratch
for specific domains. Despite this, in tabular data generation, our method‚Äôs performance is on par
withthesedomain-specificmethods.
4.2 CODEGENERATION
Benchmarking. Inthecodegenerationscenario,tosimplifytheproblem,wefocusonPythoncode
and use the Flytech1 dataset as real data, which contains 24,813 unique real user queries and the
corresponding Python code fulfilling those requests. We discard the user queries and use only the
codetotrainDiffLM.Aftergeneratingsyntheticcodedata,wecontinuepre-trainingtheMistral7B
v0.3 base model (Jiang et al., 2023) using a smaller learning rate, i.e., 1e-5, in a causal language
1https://huggingface.co/datasets/flytech/python-codes-25k
6Preprint
6000 Real Tools
DiffLM Tools
5000 Table 3: Win rate of DiffLM
generateddata.GPT-4performs
4000
preference scoring on all real
tools and synthetic tools within
3000
the same category, considering
aspects like comprehensiveness
2000
anddiversity.
1000
Rate%
0
0 1 2 3 4 5 6 7 8 9 10 DiffLMWin 28.3
Score
Equal 6.8
Figure2: GPT-4evaluationscoresfortoolsfromtheToolBench RealWin 64.9
dataset and tools generated by DiffLM. The evaluation prompt
considersaspectssuchasclarity,specificity,completeness,con-
sistency,andapplicability.
modelingobjective. Wethenbenchmarkthetrainedmodeloncodegenerationtasks,selectingtwo
mainstreambenchmarks: HumanEval(Chenetal.,2021)andMBPP(Austinetal.,2021). Tobetter
understand the performance changes of the base model, we also experiment with base models of
differentsizes,i.e.,MistralNemowith12Bparameters.
Baselines. We include baselines from recent code models. First, we consider the CodeL-
LaMA(Rozie`reetal.,2023)series,whichuseapproximately600Btokenstocontinuepre-training
theLLaMA-2(Touvronetal.,2023)basemodel, injectingstrongcodecapabilitiesthroughmulti-
task learning. Additionally, we compare with the Mistral base model (Jiang et al., 2023) and its
instruction-tunedvariants,thelattercouldrepresentingtheupperboundofcodecapabilitiesforthis
architecture.
Evaluation. WereportthecodegenerationcapabilitiesinTable2. Specifically,Mistral-Real-Code
andMistral-DiffLM-Codedenotemodelsthatwerefurtherpre-trainedonrealdataandsyntheticdata
generatedbyDiffLM,respectively.The7BmodelsarebasedonMistral-0.3-Base,andthe12Bmod-
elsarebasedonMistral-Nemo-Base. Bothmodelsweretrainedfor3epochsonthesameamount
of data using identical hyperparameters, effectively serving as a controlled experiment where the
datasourceistheonlyvariable. Theresultsindicatethatsimplycontinuingtopre-traintheMistral
modelwithasmallamountofcodedataleadstoinconsistentimpactsoncodegenerationcapabil-
ities. Specifically,Mistral-Real-CodeshowsaslightimprovementonHumanEvalbutasignificant
declineonMBPP.However,usingoursyntheticdatatocontinuepre-trainingthebasemodelyields
betterresultsthanusingrealdata. Forinstance,Mistral-DiffLM-Code-7B,achieveda7percentage
point improvement over the base model, even outperforming the Code Llama 7B model that was
trained with more extensive data. In summary, in the code generation scenario, we focus on the
differing impacts of real data and synthetic data, further demonstrating that DiffLM can generate
syntheticdatathatisevenmoreeffectivethanrealdatainenhancingdownstreamtaskperformance.
4.3 TOOLGENERATION
Evaluation. To address more complex structured data generation scenarios, we further conduct a
toolsynthesistask. Specifically,weselecttheToolBench(Qinetal.,2024)datasetasabenchmark
for comparison, which is constructed based on the RapidAPI2 platform by crawling APIs created
byrealusersandsynthesizingrelateddialogueSFTdatausingGPT3.Weusetheitstoolsettotrain
DiffLM and then sample an equal number of tools for comparison. We assess the usability of the
generated tools from two perspectives: 1) Single-Tool Quality: We use GPT-4 as an annotator to
score the real and synthetic data across multiple dimensions on a scale from 0 to 10, where the
2https://rapidapi.com/hub
3https://chat.openai.com
7
ycneuqerFPreprint
KL-Divergence Loss LM Reconstruction Loss
1.8
Embedding Injection 5.0 Embedding Injection
1.6 Memory Injection Memory Injection
Prompt Injection Prompt Injection
1.4
Cyclical Œ≤ 2.0 Cyclical Œ≤
1.2
1.0 1.0
0.8
0.5
0.6
0.4
0.2
0.2
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Epoch Epoch
Figure3: ModellosscurvesunderdifferentlatentfeatureinjectionmethodsanddifferentŒ≤ adjust-
ment strategies. The left is the KL-divergence loss trends, and the right is the language modeling
reconstructionlossonalogarithmicscale. InthecyclicalŒ≤ strategy,Œ≤ increaseslinearlyfrom0to
0.2. TheothermethodsemployadecreasingŒ≤,startingfromamaximumvalueof0.1anddecreas-
ingtoaminimumof0.001. OurproposedinjectionandŒ≤ adjustmentstrategyachievesthelowest
reconstructionloss.
‚Ä¶ ‚Ä¶ ‚Ä¶
‚Ä¶ ‚Ä¶
Diffusion W P ‚Ä¶ W m ‚Ä¶ W e ‚Ä¶
‚Ä¶ Model ‚Ä¶
‚Ä¶ ‚Ä¶ ‚Ä¶
‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶
Normal Latent 1. Prompt Injection 2. Memory Injection 3. Embedding Injection
Distribution Distribution
Figure4: Finaldatasynthesisprocess. Thecomparisonofdifferentlatentfeatureinjectionmethods
isshowningreydashedbox.MemoryInjectionintroducesthelatentfeaturesaspastkey-value(KV)
memoriesintoeachattentionlayeroftheLLM.EmbeddingInjectiondirectlyaddsthelatentfeatures
tothetokenembeddings.
results are illustrated in Figure 2. 2) Category-Level Preference: We collect all tools within the
samecategoryanduseGPT-4toperformpreferencescoringbetweenrealtoolsandsynthetictools,
as presented in Table 3. The specific evaluation prompts are provided in the appendix B.2. From
theresults,DiffLM‚Äôssyntheticdataachieveshigherscoresinthesingle-toolscoringtask,indicating
thatleveragingtheinternalknowledgeandgenerativecapabilitiesofLLMsallowsustocreatetool
descriptions and input/output parameter definitions of higher textual quality. Additionally, in the
category-level preference evaluation, nearly 1/3 of the tool types surpass or are on par with real
dataintermsofdiversityandusability. SinceDiffLMcansampleandgeneratetoolsindefinitelyto
increasecoverage,webelievethereisroomforfurtherimprovementinthismetric.
5 ANALYSIS
5.1 ABLATIONSTUDY
The effect of adaptive Œ≤ adjustment. As described in Section 3.2, we use a decreasing Œ≤ ad-
justment strategy to train the VAE latent space. Here, we compare this with another method that
uses a cyclical schedule to anneal Œ≤ (Fu et al., 2019), evaluating both the loss decline curves and
downstream task performance to demonstrate the effectiveness of our decreasing strategy. Firstly,
asshowninFigure3,theKL-divergencelosstrendsunderdecreasingŒ≤ exhibitapatternwherethe
loss first increases, then decreases, and then increases again. This indicates that during the early
stagesofVAEtraining,DiffLMusesalargerŒ≤ tofocusonthedivergencebetweentheembedding
distributionandthestandardGaussian. Thishelpsquicklylearnastandardlatentspacetostabilize
thetrainingoftheLLMmodule. Subsequently,whenthereconstructionlossreachesabottleneck,
itgraduallyreducestheweightoftheKL-divergenceloss. Atthispoint,thetrainingobjectiveshifts
8
LK
ssoL
‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶
ceR
ssoL
‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶Preprint
Real Codi TabSyn GReaT DiffLM
1.00
0.75
0.50
0.25
0.00
0 2 4 0 2 4 0 2 4 0 2 4 0 2 4
DCR on Beijing
Real Codi TabSyn GReaT DiffLM
1.00
0.75
0.50
0.25
0.00
0 5 10 0 5 10 0 5 10 0 5 10 0 5 10
DCR on Default
Figure5: DCRresultsoftherealtestdata, Codi, TabSyn, GReaT,andDiffLMontheBeijingand
Defaultdatasets. DiffLMexhibitsaDCRdistributionsimilartothecurrentSoTAmethod,TabSyn.
towardsobtainingadecoderwithstrongergenerativecapabilities. Asaresult,theKLlossgradually
increasesandeventuallystabilizesatafairlylowvalue. Fromtheresults,ourdecreasingŒ≤ method
achieves the lowest reconstruction loss. Additionally, by introducing the latent diffusion process,
we addressthe issueof distributiondiscrepancy. Therefore, as shownin Table4, comparedto the
cyclicalmethod,thedecreasingŒ≤ strategyusedinthispaperresultsinstrongergenerativeability.
Theeffectoflatentfeatureinjection. Wealsocompare
ourproposedsoftpromptlatentfeatureinjectionmethod Table 4: The results of MLE and œÅ
with previously explored methods such as KV memory under different latent feature injections
injectionandinputembeddinginjection(Lietal.,2020); andŒ≤ adjustmentsonAdultdataset.
implementationdetailsareillustratedinFigure4. Specif-
ically, the loss convergence on the validation dataset for
Models MLE‚Üë œÅ ‚Üì
different injection methods are shown in Figure 3. The
inputembeddingmethodleadstosuboptimaltrainingre- DiffLM-CycleŒ≤ 0.872 16.79
sults,wherethereconstructionlossceasestodecreaseaf-
DiffLM-Embed - -
terreachingaround3.6. Thisindicatesthatsuchasimple
DiffLM-Memory 0.875 17.05
injectionmethodstrugglestoeffectivelyconveycomplex
realdistributioninformationtotheLLMdecoder. Mean- DiffLM-Prompt 0.894 9.16
while, the soft prompt method slightly outperforms KV
memory in terms of reconstruction loss. However, as
showninTable4,ondownstreamtaskperformanceusingtheAdultdataset,ourproposedsoftprompt
approachachieveshigher(2%)classificationaccuracyandbettercolumndensity.
5.2 TRAININGDATAPLAGIARISM
Datacopyingisasignificantchallengeforoverfittedgenerativemodelsinpracticalapplications. To
verifythatthedatageneratedbyDiffLMisnotmerelycopiedfromthetrainingdata, wecompute
the Distance to Closest Record (DCR) metric. Specifically, for each row in the tabular data, we
representthecategoricalcolumnsusingone-hotvectorsandperformmin-maxnormalizationonthe
numerical columns. We then define DCR as the minimum L1-distance between a synthetic data
pointandeachtrainingsamplepoint:
DCR(x )= min L (x ,x ). (8)
syn 1 syn real
xreal‚ààDtrain
The DCR distribution is shown in Figure 5. We observe that the LLM-based GReaT generates
results that differ significantly from the training data, indicating that vanilla fine-tuning struggles
toadaptLLMstorealdatadistributionsandgeneratehigh-qualityresults. DiffLMdemonstratesa
DCRdistributionsimilartothatoftheSoTAmethodTabSynonbothdatasets.Thisfurtherindicates
thatourproposedgeneral-purposedatasynthesisframeworkcanachieveperformance onparwith
domain-specificmodelsonspecifictasks.
9
oitaR
oitaRPreprint
Adult Default Magic
Label Label Label
<=50K 0 g
>50K 1 h
Shoppers Beijing Toolbench
Label
1.00-23.00
23.00-52.00
Label 52.00-92.00
False 92.00-157.00
True 157.00-858.00
Figure6: Thet-SNEvisualizationofthelatentspaceobtainedbyencodingevaluationdata. DiffLM
implicitlylearnsclusteringrelationshipsamongdifferenttypesofdata.
5.3 VISUALIZATION
Figure 6 presents 2D t-SNE visualizations of the latent space for multiple datasets, including four
categorical tabular datasets, one numerical tabular dataset, and one tool dataset. We use DiffLM
trainedonthecorrespondingdatasetstoencodetheirvalidationsets,obtaininglatentfeatures. Itcan
beobservedthatdataofthesameclassencodedbyDiffLMexhibitclusteringcharacteristicsinthe
latent space, as seen in the Adult and Magic. Notably, in the numerical dataset Beijing, different
targetvaluesdisplayacleartransitionaldistribution: theupperpartofthe2Dspacecorrespondsto
datawithlargertargetvalues,i.e.,157to858,whilethelowerpartcorrespondstodatawithsmaller
target values, i.e., 1 to 23. These results demonstrate that DiffLM‚Äôs latent space learning strategy
caneffectivelycapturetherealdatadistribution.
6 CONCLUSION
Inthispaper,weintroduceDiffLM,anovelframeworkdesignedtoenhanceLLM‚Äôsunderstanding
ofreal-worlddatadistributionsinsyntheticdatagenerationtasks. DiffLMleveragesaVAEtomap
real data into a latent space, which is then injected into the decoding process of LLM, enabling
end-to-endtrainingthroughcausallanguagemodelingobjective. Additionally,weincorporateadif-
fusionprocesstofurtherrefinethelearningofthelatentdistribution,mitigatingthesamplingfailures
causedbylatentspacediscrepancies. Toflexiblyandnon-intrusivelycontrolthestructureandqual-
ityofthegenerateddata, DiffLMintegratesrealdatainformationwithLLMs‚Äôinternalknowledge
by freezing the LLM parameters and using the latent features as plug-in modules. Experimental
resultsdemonstratethatDiffLMproduceshighlyrobustandconsistentoutputs. Inalldatasets,the
performanceofdownstreammodelstrainedonthegenerateddataiscomparabletoorevensurpasses
thatofmodelstrainedonrealdata.
REFERENCES
AteretAnaby-Tavor,BoazCarmeli,EstherGoldbraich,AmirKantor,GeorgeKour,SegevShlomov,
NaamaTepper,andNaamaZwerdling. Donothaveenoughdata? deeplearningtotherescue! In
AAAI,pp.7383‚Äì7390.AAAIPress,2020.
JacobAustin,AugustusOdena,MaxwellI.Nye,MaartenBosma,HenrykMichalewski,DavidDo-
han,EllenJiang,CarrieJ.Cai,MichaelTerry,QuocV.Le,andCharlesSutton.Programsynthesis
withlargelanguagemodels. CoRR,abs/2108.07732,2021.
10Preprint
Andre¬¥ Bauer,SimonTrapp,MichaelStenger,RobertLeppich,SamuelKounev,MarkLeznik,Kyle
Chard, and Ian T. Foster. Comprehensive exploration of synthetic data generation: A survey.
CoRR,abs/2401.02524,2024.
JamesBetker,GabrielGoh,LiJing,TimBrooks,JianfengWang,LinjieLi,LongOuyang,Juntang
Zhuang,JoyceLee,YufeiGuo,etal. Improvingimagegenerationwithbettercaptions. Computer
Science.https://cdn.openai.com/papers/dall-e-3.pdf,2(3):8,2023.
VadimBorisov,KathrinSe√üler,TobiasLeemann,MartinPawelczyk,andGjergjiKasneci.Language
modelsarerealistictabulardatagenerators. InICLR.OpenReview.net,2023.
ChristopherP.Burgess,IrinaHiggins,ArkaPal,Lo¬®ƒ±cMatthey,NickWatters,GuillaumeDesjardins,
andAlexanderLerchner. UnderstandingdisentanglinginŒ≤-vae. CoRR,abs/1804.03599,2018.
Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall, and W. Philip Kegelmeyer. SMOTE:
syntheticminorityover-samplingtechnique. J.Artif.Intell.Res.,16:321‚Äì357,2002.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde¬¥ de Oliveira Pinto, Jared
Kaplan,HarriEdwards,YuriBurda,andetal. Evaluatinglargelanguagemodelstrainedoncode.
CoRR,abs/2107.03374,2021.
JasperDekoninck,MarcFischer,LucaBeurer-Kellner,andMartinT.Vechev. Controlledtextgen-
erationvialanguagemodelarithmetic. InICLR.OpenReview.net,2024.
JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. BERT:pre-trainingofdeep
bidirectionaltransformersforlanguageunderstanding. InNAACL-HLT(1),pp.4171‚Äì4186.As-
sociationforComputationalLinguistics,2019.
XiaoanDingandKevinGimpel. Latent-variablegenerativemodelsfordata-efficienttextclassifica-
tion. InEMNLP/IJCNLP(1),pp.507‚Äì517.AssociationforComputationalLinguistics,2019.
TuanDinh,YuchenZeng,RuisuZhang,ZiqianLin,MichaelGira,ShashankRajput,Jy-yongSohn,
DimitrisS.Papailiopoulos, andKangwookLee. LIFT:language-interfacedfine-tuningfornon-
languagemachinelearningtasks. InNeurIPS,2022.
HaoFu,ChunyuanLi,XiaodongLiu,JianfengGao,AsliCelikyilmaz,andLawrenceCarin.Cyclical
annealingschedule:AsimpleapproachtomitigatingKLvanishing.InNAACL-HLT(1),pp.240‚Äì
250.AssociationforComputationalLinguistics,2019.
ShansanGong,MukaiLi,JiangtaoFeng,ZhiyongWu,andLingpengKong. Diffuseq: Sequenceto
sequencetextgenerationwithdiffusionmodels. InICLR.OpenReview.net,2023.
Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and
BainingGuo.Vectorquantizeddiffusionmodelfortext-to-imagesynthesis.InCVPR,pp.10686‚Äì
10696.IEEE,2022.
YuxuanGu,XiaochengFeng,SichengMa,LingyuanZhang,HengGong,WeihongZhong,andBing
Qin. Controllabletextgenerationviaprobabilitydensityestimationinthelatentspace. InACL
(1),pp.12590‚Äì12616.AssociationforComputationalLinguistics,2023.
IshaanGulrajani,KundanKumar,FarukAhmed,AdrienAliTa¬®ƒ±ga,FrancescoVisin,DavidVa¬¥zquez,
andAaronC.Courville. Pixelvae: Alatentvariablemodelfornaturalimages. InICLR(Poster).
OpenReview.net,2017.
NeilHoulsby,AndreiGiurgiu,StanislawJastrzebski,BrunaMorrone,QuentindeLaroussilhe,An-
drea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for
NLP.InICML,volume97ofProceedingsofMachineLearningResearch,pp.2790‚Äì2799.PMLR,
2019.
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chap-
lot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, and et al. Mistral 7b. CoRR,
abs/2310.06825,2023.
11Preprint
Martin Josifoski, Marija Sakota, Maxime Peyrard, and Robert West. Exploiting asymmetry for
synthetic training data generation: Synthie and the case of information extraction. In EMNLP,
pp.1555‚Äì1574.AssociationforComputationalLinguistics,2023.
TeroKarras,MiikaAittala,TimoAila,andSamuliLaine. Elucidatingthedesignspaceofdiffusion-
basedgenerativemodels. InNeurIPS,2022.
Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher.
CTRL: A conditional transformer language model for controllable generation. CoRR,
abs/1909.05858,2019.
DiederikP.KingmaandMaxWelling. Auto-encodingvariationalbayes. InICLR,2014.
Varun Kumar, Ashutosh Choudhary, and Eunah Cho. Data augmentation using pre-trained trans-
formermodels. CoRR,abs/2003.02245,2020.
ChaejeongLee,JayoungKim,andNoseongPark. Codi: Co-evolvingcontrastivediffusionmodels
for mixed-type tabular synthesis. In ICML, volume 202 of Proceedings of Machine Learning
Research,pp.18940‚Äì18956.PMLR,2023.
BrianLester,RamiAl-Rfou,andNoahConstant. Thepowerofscaleforparameter-efficientprompt
tuning. InEMNLP(1),pp.3045‚Äì3059.AssociationforComputationalLinguistics,2021.
ChunyuanLi, XiangGao, YuanLi, BaolinPeng, XiujunLi, YizheZhang, andJianfengGao. Op-
timus: Organizing sentences via pre-trained modeling of a latent space. In EMNLP (1), pp.
4678‚Äì4699.AssociationforComputationalLinguistics,2020.
XiangLisaLiandPercyLiang. Prefix-tuning: Optimizingcontinuouspromptsforgeneration. In
ACL/IJCNLP(1),pp.4582‚Äì4597.AssociationforComputationalLinguistics,2021.
Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori B. Hashimoto.
Diffusion-lmimprovescontrollabletextgeneration. InNeurIPS,2022.
ZhuoyanLi,HangxiaoZhu,ZhuoranLu,andMingYin. Syntheticdatagenerationwithlargelan-
guage models for text classification: Potential and limitations. In EMNLP, pp. 10443‚Äì10461.
AssociationforComputationalLinguistics,2023.
HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. InNeurIPS,
2023a.
TennisonLiu,ZhaozhiQian,JeroenBerrevoets,andMihaelavanderSchaar. GOGGLE:generative
modellingfortabulardatabylearningrelationalstructure. InICLR.OpenReview.net,2023b.
Lin Long, Rui Wang, Ruixuan Xiao, Junbo Zhao, Xiao Ding, Gang Chen, and Haobo Wang. On
llms-drivensyntheticdatageneration,curation,andevaluation: Asurvey. InACL(Findings),pp.
11065‚Äì11082.AssociationforComputationalLinguistics,2024.
SimianLuo,YiqinTan,LongboHuang,JianLi,andHangZhao. Latentconsistencymodels: Syn-
thesizinghigh-resolutionimageswithfew-stepinference. CoRR,abs/2310.04378,2023.
Raul Puri, Ryan Spring, Mohammad Shoeybi, Mostofa Patwary, and Bryan Catanzaro. Training
questionansweringmodelsfromsyntheticdata. InEMNLP(1),pp.5811‚Äì5826.Associationfor
ComputationalLinguistics,2020.
YujiaQin,ShihaoLiang,YiningYe,KunlunZhu,LanYan,YaxiLu,YankaiLin,XinCong,Xiangru
Tang,BillQian,SihanZhao,LaurenHong,RunchuTian,RuobingXie,JieZhou,MarkGerstein,
DahaiLi,ZhiyuanLiu,andMaosongSun. Toolllm: Facilitatinglargelanguagemodelstomaster
16000+real-worldapis. InICLR.OpenReview.net,2024.
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
modelsareunsupervisedmultitasklearners. 2019.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,Yanqi
Zhou,WeiLi,andPeterJ.Liu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-text
transformer. J.Mach.Learn.Res.,21:140:1‚Äì140:67,2020.
12Preprint
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo¬®rn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels.InCVPR,pp.10674‚Äì10685.IEEE,2022.
BaptisteRozie`re,JonasGehring,FabianGloeckle,StenSootla,ItaiGat,XiaoqingEllenTan,Yossi
Adi,JingyuLiu,andetal.Codellama:Openfoundationmodelsforcode.CoRR,abs/2308.12950,
2023.
Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and
BenPoole. Score-basedgenerativemodelingthroughstochasticdifferentialequations. InICLR.
OpenReview.net,2021.
RuixiangTang,XiaotianHan,XiaoqianJiang,andXiaHu. Doessyntheticdatagenerationofllms
helpclinicaltextmining? CoRR,abs/2303.04360,2023.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Niko-
layBashlykov,SoumyaBatra,andetal. Llama2: Openfoundationandfine-tunedchatmodels.
CoRR,abs/2307.09288,2023.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
LukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. InNIPS,pp.5998‚Äì6008,2017.
VeniaminVeselovsky,ManoelHortaRibeiro,AkhilArora,MartinJosifoski,AshtonAnderson,and
Robert West. Generating faithful synthetic data with large language models: A case study in
computationalsocialscience. CoRR,abs/2305.15041,2023.
JerryWei,ChengrunYang,XinyingSong,YifengLu,NathanHu,DustinTran,DaiyiPeng,Ruibo
Liu, Da Huang, Cosmo Du, and Quoc V. Le. Long-form factuality in large language models.
CoRR,abs/2403.18802,2024.
Sam Wiseman, Stuart M. Shieber, and Alexander M. Rush. Learning neural templates for text
generation. InEMNLP,pp.3174‚Äì3187.AssociationforComputationalLinguistics,2018.
SiyuanWu,YueHuang,ChujieGao,DongpingChen,QihuiZhang,YaoWan,TianyiZhou,Xian-
gliangZhang, JianfengGao, ChaoweiXiao, andLichaoSun. Unigen: Aunifiedframeworkfor
textualdatasetgenerationusinglargelanguagemodels. CoRR,abs/2406.18966,2024.
LeiXu,MariaSkoularidou,AlfredoCuesta-Infante,andKalyanVeeramachaneni. Modelingtabular
datausingconditionalGAN. InNeurIPS,pp.7333‚Äì7343,2019.
Shengzhe Xu, Cho-Ting Lee, Mandar Sharma, Raquib Bin Yousuf, Nikhil Muralidhar, and
Naren Ramakrishnan. Are llms naturally good at synthetic tabular data generation? CoRR,
abs/2406.14541,2024.
Kevin Yang and Dan Klein. FUDGE: controlled text generation with future discriminators. In
NAACL-HLT,pp.3511‚Äì3535.AssociationforComputationalLinguistics,2021.
JiachengYe,JiahuiGao,QintongLi,HangXu,JiangtaoFeng,ZhiyongWu,TaoYu,andLingpeng
Kong.Zerogen:Efficientzero-shotlearningviadatasetgeneration.InEMNLP,pp.11653‚Äì11669.
AssociationforComputationalLinguistics,2022a.
JiachengYe,JiahuiGao,ZhiyongWu,JiangtaoFeng,TaoYu,andLingpengKong.Progen:Progres-
sivezero-shotdatasetgenerationviain-contextfeedback. InEMNLP(Findings),pp.3671‚Äì3683.
AssociationforComputationalLinguistics,2022b.
JiahuiYu,YuanzhongXu,JingYuKoh,ThangLuong,GunjanBaid,ZiruiWang,VijayVasudevan,
AlexanderKu,YinfeiYang,BurcuKaragolAyan,BenHutchinson,WeiHan,ZaranaParekh,Xin
Li,HanZhang,JasonBaldridge,andYonghuiWu.Scalingautoregressivemodelsforcontent-rich
text-to-imagegeneration. Trans.Mach.Learn.Res.,2022,2022.
Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander J. Ratner, Ranjay Krishna, Jiaming
Shen, and Chao Zhang. Large language model as attributed training data generator: A tale of
diversityandbias. InNeurIPS,2023.
Hengrui Zhang, Jiani Zhang, Zhengyuan Shen, Balasubramaniam Srinivasan, Xiao Qin, Christos
Faloutsos,HuzefaRangwala,andGeorgeKarypis. Mixed-typetabulardatasynthesiswithscore-
baseddiffusioninlatentspace. InICLR.OpenReview.net,2024.
13Preprint
A DETAILS ON MODEL DESIGN
A.1 DIFFUSIONPROCESS
In this section, we will introduce the general process of latent diffusion models. Latent Diffusion
Models (LDMs) are a class of diffusion probabilistic models that operate in the latent space of
an autoencoder rather than directly on the high-dimensional data space. By performing diffusion
inacompressedlatentrepresentation, LDMssignificantlyreducecomputationalcomplexitywhile
maintaininghighfidelityindatageneration. AnLDMconsistsoftwoprimarycomponents:
1. Autoencoder: Encodesinputdatax intoalatentrepresentationz = E(x )anddecodes
0 0 0
latentvariablesbacktodataspacexÀÜ =D(z).
2. DiffusionModel: Definesadiffusionprocessonthelatentvariables{z }T .
t t=0
Itshouldbenotedthatthevariableusedhereisindependentwithmaintext.
Forward Process (Diffusion). The forward diffusion process in latent space progressively adds
Gaussian noise to the latent representation over T timesteps. Starting from the initial latent code
z =E(x ),obtainedbyencodingthedatax ,theforwardprocessisdefinedas:
0 0 0
(cid:112)
q(z |z )=N(z ; 1‚àíŒ≤ z ,Œ≤ I), (9)
t t‚àí1 t t t‚àí1 t
whereŒ≤ ‚àà(0,1)isapredefinedvarianceschedulethatcontrolstheamountofnoiseaddedateach
t
stept,andN denotesaGaussiandistribution. Byrecursivelyapplyingthisprocess,wecanexpress
z directlyintermsofz :
t 0
‚àö
q(z |z )=N(z ; Œ±¬Ø z ,(1‚àíŒ±¬Ø )I), (10)
t 0 t t 0 t
where Œ± = 1‚àíŒ≤ and Œ±¬Ø =
(cid:81)t
Œ± . This formulation allows efficient sampling of z at any
t t t s=1 s t
arbitrarytimesteptwithoutiteratingthroughallprevioussteps. Inthispaper,weadopttheVariance
‚àö (cid:113)
Explodingdefinedperturbationkernels,whereassettings = 1‚àíŒ≤ andœÉ = Œ≤t . Also,we
t t t 1‚àíŒ≤t
sets =1todirectlyaddnoisetothedataratherthanweightedmixing,convertEq.10to:
t
q(z |z )=N(z ;0,œÉ2I) (11)
t 0 t t
ReverseProcess(Denoising). Thereversediffusionprocessaimstorecoverz fromanoisyla-
0
tent variable z ‚àº N(0,I). It is parameterized by a neural network œµ , which predicts the noise
t Œ∏
componentateachtimestep:
p (z |z )=N(z ;¬µ (z ,t),Œ£ (z ,t)). (12)
Œ∏ t‚àí1 t t‚àí1 Œ∏ t Œ∏ t
Typically,themodelpredictsthemean¬µ whilethecovarianceŒ£ isfixedorsimplified. Bylever-
Œ∏ Œ∏
aging the properties of the forward process, the mean can be parameterized to predict the original
noiseœµaddedduringtheforwarddiffusion:
(cid:18) (cid:19)
1 Œ≤
¬µ (z ,t)= ‚àö z ‚àí ‚àö t œµ (z ,t) . (13)
Œ∏ t Œ± t 1‚àíŒ±¬Ø Œ∏ t
t t
Thisformulationenablesthemodeltodenoisez stepbystep,ultimatelyreconstructingz .
t 0
LearningObjective. ThetrainingobjectiveforLDMsfocusesonminimizingthedifferencebe-
tween the true noise œµ added during the forward process and the noise predicted by the model œµ .
Œ∏
Thesimplifiedlossfunctionis:
(cid:104) (cid:105)
L =E ‚à•œµ‚àíœµ (z ,t)‚à•2 , (14)
latent x0,œµ,t Œ∏ t
wherez issampledas:
t
‚àö ‚àö
z = Œ±¬Ø z + 1‚àíŒ±¬Ø œµ, œµ‚àºN(0,I). (15)
t t 0 t
Thisobjectiveencouragesthemodeltolearntheconditionaldistributionp (z |z )byaccurately
Œ∏ t‚àí1 t
predictingthenoisecomponentateachtimestep.
14Preprint
Noise Scheduling. The noise schedule {Œ≤ }T plays a critical role in the diffusion process. It
t t=1
dictateshowquicklynoiseisaddedintheforwardprocessand,consequently,affectsthedifficulty
ofthereversedenoisingtask. CommonstrategiesforsettingŒ≤ includelinear,cosine,andquadratic
t
schedules. We use use linear noise schedule, i.e., the perturbation kernel œÉ(t) = t. As it is an
effectiveschedule, ensuringthatthedataissufficientlydiffusedbytimestept, whilestillallowing
themodeltolearnmeaningfulreversetransitions.
B DETAILS ON EXPERIMENTAL SETUP
B.1 TABULARDATAGENERATION
Table 5: Details of tabular dataset. For each dataset, #Num stands for the number of numerical
columns,and#Catstandsforthenumberofcategoricalcolumns.
Datasets #Num #Cat #Train #Validation #Test DownstreamTask
Adult1 6 9 29,304 3,257 16,281 BinaryClassification
Beijing2 7 5 35,059 4,382 4,383 BinaryClassification
Default3 14 11 24,000 3,000 3,000 BinaryClassification
Magic4 10 1 15,216 1,902 1,902 BinaryClassification
Shoppers5 10 8 9,864 1,233 1,233 Regression
1 https://archive.ics.uci.edu/dataset/2/adult
2 https://archive.ics.uci.edu/dataset/381/beijing+pm2+5+data
3 https://archive.ics.uci.edu/dataset/350/default+of+credit+
card+clients
4 https://archive.ics.uci.edu/dataset/159/magic+gamma+
telescope
5 https://archive.ics.uci.edu/dataset/468/online+shoppers+
purchasing+intention+dataset
B.2 TOOLJUDGEMENTPROMPTS
We present the evaluation prompts used for assessing tool generation quality in Figure 7 and Fig-
ure8.
B.3 INSTRUCTIONSFORREPRODUCTION
Inthissection,wepresenttheexperimentaldetailsofDiffLM,includingdatapreprocessing,training
hyperparametersettings,anddatapost-processingfilteringmethods.
DataPreprocessing. Real-worldNLPdatasetsoftenexhibitinherentstructures,suchasthecon-
text, question, and answer in machine reading comprehension tasks, or key-value pairs in tabular
generationtasks. InDiffLM,weconvertallstructureddataintoJSONformat. Forinstance,tabular
datainaCSVfileistransformedintolinesofJSON,andtoolsfromToolBenchareabstractedinto
JSON structures comprising tool name, tool description, api name, and api description. For code
data,weusetherawcodedirectlywithoutanypreprocessingasinputforDiffLMtraining.
HyperparameterSettings.
‚Ä¢ VAEEncoder: bert-base-uncased
‚Ä¢ VAEDecoder: mistralai/Mistral-7B-Instruct-v0.3
‚Ä¢ SoftPromptTokensk: 64
‚Ä¢ SoftPromptEmbeddingDimensiond: 4096
‚Ä¢ Œ≤ =0.1
max
‚Ä¢ Œ≤ =0.001
min
‚Ä¢ DiffusionNoiseDimension: 4096
15Preprint
Given a API, evaluate it and assign a score from 0 to 10,
with 10 being the highest quality and 0 being the lowest.
Consider the aspects listed below when evaluating the
API. Provide your reasoning in "reason" and include the "
score" in JSON format.
Evaluation Aspects:
1. Clarity and Completeness of the Tool Description: Does the
tool_description clearly and thoroughly explain the
purpose and functionalities of the tool?
2. Specificity and Accuracy of the API Name and Description:
Is the api_name descriptive and appropriate? Does the
api_description accurately and specifically describe what
the API does?
3. Parameter Definition and Completeness: Are the parameters
well-defined, including types, properties, and required
fields? Do they cover all necessary inputs for the API to
function effectively?
4. Consistency Between Tool and API Descriptions: Is there a
logical connection between the tool_description and the
api_description? Do they complement each other to provide
a full understanding of the API‚Äôs capabilities?
5. Ease of Integration and Use: Based on the provided
information, how easy would it be for a developer to
integrate and use the API? Are there any missing details
that could hinder implementation?
6. Overall Usefulness and Applicability: Considering
potential use cases, how valuable is the API? Does it
meet the needs of its intended audience?
Instructions:
- For the API, analyze it based on the evaluation aspects.
- Summarize your findings and reasoning in a clear and
concise manner in "reason".
- Assign a final score between 0 and 10, reflecting the
overall quality of the API in "score" field.
- Present the output in JSON format.
API:
{api_data}
Now, provide your answer.
Figure7: Evaluationpromptforsingle-toolquality. UsedbyGPT-4withtemperature=1.0.
GenerationFiltering. ForinputsinJSONformat,weemploycolumnnamestofilterthegenerated
outputs. A generated result is considered valid only if it contains all required columns. For code
generation tasks involving plain text, we do not apply any filtering. We utilize the same filtering
criteriaacrossallbaselinemodels.
C SYNTHETIC DATA GENERATED BY DIFFLM
In Table 6, we compare the real test data of the Adult dataset with the generated outputs from
GReaT and DiffLM. As discussed in Section 5.1, DiffLM produces more diverse samples that
16Preprint
Given two sets of tools under the same category, you need to
determine better_set by following these rules:
1. Comprehensiveness of Covered Functions: Evaluate which set
covers more relevant and essential functions within the
category.
2. Accuracy of Tool Descriptions: Check if the tool
descriptions are clear, precise, and accurately reflect
each tool‚Äôs functionality.
3. Difficulty of Calling the Tools: Assess the complexity
involved in using the tools, considering the inputs and
outputs required.
4. Overall Quality Assessment: Consider any additional
factors that may impact the overall quality of the tool
sets.
Set A:
{tool_set_a}
Set B:
{tool_set_b}
If one set is better based on the above criteria, indicate
better_set as "A" or "B". If both sets are of similar
quality, indicate better_set as "equal".
Now, provide your reasoning in "reason" and indicate "
better_set" ("A" or "B" or "equal") in JSON format.
Figure8: Evaluationpromptforcategory-levelperference. UsedbyGPT-4withtemperature=1.0.
morecloselyalignwiththerealdatadistribution. Specifically, forcolumnslikeworkclassand
native-country,theoutputsgeneratedbytheGReaTmodelarerelativelyhomogeneous.
Table6: Comparisonofrealsamplesandsyntheticdata.
Methods age workclass education occupation race sex native-country income
40 Private Some-college Machine-op-inspct Asian-Pac-Islander Female Japan >50K
38 Private HS-grad Other-service White Female Canada <=50K
Real 59 Private HS-grad Craft-repair White Male England >50K
29 Self-emp-not-inc Assoc-voc Adm-clerical White Male United-States <=50K
26 Private Assoc-acdm Prof-specialty White Female Canada <=50K
27 Private Bachelors Prof-specialty White Male United-States <=50K
22 Private HS-grad Craft-repair Black Male United-States <=50K
GReaT 41 Private HS-grad Sales Black Male United-States <=50K
35 Private HS-grad Adm-clerical White Female United-States <=50K
54 Private Doctorate Prof-specialty Asian-Pac-Islander Male India >50K
34 Private Some-college Craft-repair White Male Canada <=50K
53 Local-gov Some-college Other-service White Female Canada <=50K
DiffLM 23 Private Bachelors Adm-clerical White Male England <=50K
24 ? Some-college ? Asian-Pacific-Islander Male Canada <=50K
32 Local-gov Bachelors Adm-clerical Asian-Pac-Islander Male India >50K
17