Correlating Variational Autoencoders Natively For
Multi-View Imputation
EllaS.C.Orme MarinaEvangelou
DepartmentofMathematics DepartmentofMathematics
ImperialCollegeLondon ImperialCollegeLondon
ella.orme18@imperial.ac.uk m.evangelou@imperial.ac.uk
UlrichPaquet
AfricanInstituteforMathematicalSciences,SouthAfrica
ulrich@aims.ac.za
Abstract
Multi-viewdatafromthesamesourceoftenexhibitcorrelation. Thisismirroredin
correlationbetweenthelatentspacesofseparatevariationalautoencoders(VAEs)
trainedoneachdata-view. Amulti-viewVAEapproachisproposedthatincorpo-
ratesajointpriorwithanon-zerocorrelationstructurebetweenthelatentspacesof
theVAEs. Byenforcingsuchcorrelationstructure,morestronglycorrelatedlatent
spacesareuncovered. Usingconditionaldistributionstomovebetweentheselatent
spaces,missingviewscanbeimputedandusedfordownstreamanalysis. Learning
thiscorrelationstructureinvolvesmaintainingvalidityofthepriordistribution,as
wellasasuccessfulparameterizationthatallowsend-to-endlearning.
1 Introduction
Datafrommultiplesourcesdescribingthesamesubjectsarisesinawealthofsettings. Thiscanbe
clinicalinformationofpatientsalongsidegeneticinformationandscandata. Datasetsconsistingof
multipleviewsarereferredtoasmulti-viewormulti-modaldata. Thereareinstanceswherenotall
viewsarealwaysavailableforeveryrealisation. Forexample,apatientmaymissanappointment
ormachinerymayfalter,resultinginnoreadingforaspecificview. Missingdataresultsinsmaller
usabledatasetsandreducedstatisticalpowerwithmanymethodsonlyapplicabletofulldatasets
[1]andcanresultinreducedperformanceindownstreamanalysis[2]. Thismanuscriptpresentsa
multi-viewimputationapproach,whereitsaimistoimputetherealisationsofamissingviewby
incorporatingtheinformationlearntfromtheotherview.
Theproposedmulti-viewimputationmethod,namedJointPriorVariationalAutoencoder(JPVAE),is
basedonvariationalautoencoders(VAEs)[3],withtheviewsconnectedsolelythroughajointprior
ontheVAEs’latentembeddings. Standardautoencodersseektoencodealatentrepresentationof
data,andfromthisencodingreconstructtheoriginaldataviaadecoder. Multi-viewapproachesallow
thelatentrepresentationofamissingviewtobeobtained,fromwhichthereconstructioncanbeused
asanimputationofthemissingview. Severalmulti-viewimputationapproachesexistintheliterature
based on autoencoders [4]. However, as variational autoencoders learn a continuous embedding,
they provide better interpolation of the latent space than standard autoencoders, making them a
moresuitableapproachforanimputationmethod. TheproposedjointpriorinJPVAEincorporatesa
non-zerocorrelationstructurethatisfoundtoincreasetheobservedcorrelationbetweenviewsinthe
latentspaces. Thisallowsforsuccessfulmovementbetweenlatentspaces,improvingtheabilityto
imputemissingviews.
Preprint.
4202
voN
5
]LM.tats[
1v79030.1142:viXraVariousapproacheshavebeenproposedextendingVAEstothemultiviewcase,includingthoseby
[5–8],withmissingdataimputationincludedasafeatureofthesemethods. Mostrecently,proposed
methods differ via their method of approximating a joint posterior. Daunhawer et al. [9] discuss
theundesirableupperboundthesemethodsputonalowerlimitofthelog-likelihoodusedasthe
objectivefunctioninVAEs. Incontrasttoexistingapproaches, whichlargelyassumesomejoint
posteriorand/orsharedlatentspace,ourworkhasthenoveltythatisbasedonajointpriorbetween
thelatentvariables. Tothebestofourknowledge,thisisthefirstattemptmadetocorrelatethelatent
spacesofVAEsnativelythroughajointprior.
JPVAEismostsimilarinstructuretotheprivateversionofDeepVariationalCanonicalCorrelation
Analysis(VCCA-private),amulti-viewVAEapproachbyWangetal.[10]. Howevertheirmodel
containsbothprivateandsharedvariablesinthelatentspaceanddoesn’thaveasuitableapproachfor
imputingmissingviews. JPVAEcanalsobeseenasanapplicationoftheideasfromself-supervised
learningtechniquessuchasBarlowTwins[11],wheretheembeddingsfromnoisyversionsofthe
sameinputaredriventobehighlycorrelatedviaalossfunction.
Figure1illustratesanapplicationoftheproposedapproachJPVAE,onimputingview2ofthedata
thatcorrespondstothebottomhalfofMNISTdigitsusingthetophalfofthedigitsdata(view1).
Throughtheproposedapproach,reconstructionsofmissingviewscanbeobtained. Thisisachieved
throughtheconditionaldistributionbetweenthetwolatentvariables,asillustratedinFigure2. By
incorporating a joint prior with a non-zero cross-correlation structure, we observe better quality
resultsinimputingview2. Furtherrealisationsofimageimputation,includingimputationofview1
fromview2,canbefoundinAppendixA.5.
(a)Nocorrelationlearnt,butestimatedem- (b)Correlationlearntnatively
piricallyaftertraining
Figure1: ImputationofthebottomhalfofMNISTdigits(view2ofthedata)usingthetophalfofthe
image(view1)onaJPVAEmodeltrainedwith(a)independentpriors(completelyseparateVAEs)
and(b)ajointpriorwithlearntcorrelationstructurebetweenlatentspaces. Thecrossentropyloss
betweentruebottomhalfofimageandimputationis109.1in(a)and93.04in(b). Aclassifiertrained
ontheconcatenationofthereconstructionofview1andimputationofview2achievesaveragetesting
accuracyof79.92%(1.0)in(a)and87.45%(0.27)in(b). SeeAppendixA.5forfurtherdetails.
1.1 Contributions
A novel multi-view imputation approach based on variational autoencoders, named JPVAE, is
proposedthatincontrasttoexistingworkassumesajointpriorbetweenthelatentvariables.1 Asthe
multipleviewsarecorrelated,thegeneratedlatentspacesarefoundalsotobecorrelated. JPVAE
1ThecodeforimplementationofJPVAEandthenumericalexperimentscontainedinthismanuscriptcanbe
foundathttps://github.com/eso28599/JPVAE.
2Figure 2: Workflow toobtain reconstruction x˜ and imputation x˜ from input x only, onpre-
1 2|1 1
trainedJPVAEnetwork. Trapeziumsrepresentencoders/decodersandrectanglesrepresentvectors.
Thestructuresforview1and2areshowninblueandorangerespectively. Theexpectationstepis
showninred.
takes advantage of this correlation and whilst marginals for each VAE are assumed to follow a
standardnormaldistribution,ajointpriorisassumedwithanon-zerocrosscorrelation. Theimposed
correlationstructureislearntnativelybyforcingastrongercorrelationinthelatentspace. Section
2 presents the proposed method and related theorems that showcase the validity of the proposed
method. Aspartofourwork,wepresentandprovetheoremsthatenabletheparameterizationof
differentiable positive definite matrices that allow end-to-end learning. Through our conducted
experimentspresentedinSection3,learningthecorrelationstructureleadstoimprovedimputation
ability,lowerreconstructionlossandbetterpredictivelikelihood. ImputedviewsfromJPVAEcanbe
usedfordownstreamtasks,withimprovedclassificationaccuracydemonstrated. Lastly,betterVAE
modelsarelearntwithJPVAEpreventingposteriorcollapse,acommonproblemobservedwithVAEs
[12].
1.2 Notation
Throughoutthispapermatricesaredenotedbycapitallettersandcolumnvectorsaredenotedby
lower case letters, both emboldened. X = {x }n with vector x ∈ Rc represents an entire
i i=1 i
dataset in Rn×c, with x an individual realisation. A diagonal matrix with vector a along the
i
diagonalisrepresentedbydiag(a). AblockdiagonalmatrixwithmatricesA alongthediagonalis
i
representedbybdiag(A ). Verticalconcatenationofcolumnvectorsaandbisdenotedby(a;b)
i
i.e. (a;b)=(aT;bT)T. TheeigenvaluesandsingularvaluesofmatrixAaredenotedbyλ (A)and
i
σ (A)respectively,indexedbysubscriptiinorderofdecreasingmagnitude. Idenotesanidentity
i
matrixofrelevantdimension.
2 Multi-viewvariationalautoenconderwithajointprior
Thissectionpresentsthenovelmulti-viewVAEapproachJPVAEwhereeachviewhasaseparate
associatedVAE.Beforediscussingtheproposedapproachindetail,themainconceptsofVAEsare
introduced. Subsequentlythesingle-viewVAEisextendedtothemulti-viewsettingandthejoint
priorispresented. ThedifferentcomponentsofJPVAEarepresentedincludingtheoreticalworkon
matrixparameterizationwhichenablesend-to-endlearning.
2.1 Variationalautoencoder
AstandardVAEseekstoencodedataX inaprobabilisticlatentspaceanddecodefromthisspaceto
reconstructeddataX˜ [3]. ThegoalisforX˜ tobeascloseaspossibletoX. Asthelatentspaceisa
probabilisticembeddingratherthanoneobtainedbyadeterministicmapping,ithastheadvantagethat
3itcanbeexploredfully,includingthesamplingofnewrealisationsfromthesamedistributionasX˜.
TheserealisationsareassumedtobetakenfromthesameunderlyingdistributionasX,p (·)=p(·|θ)
θ
whereθisthesetofparametersdefiningthedistribution.
VAEsareatypeofvariationalBayesianmethodthatseektofindalowerboundforthismarginal
probability,p (X),throughaBayesianframework. Thelikelihoodofx∈Rcgivenlatentvariable
θ
z ∈ Rd isdenotedbyp (·|z)andthepriorforthelatentvariables,usuallytakentobeastandard
θ
normal,isdenotedbyp (z). Theencoderanddecoderareneuralnetworksthatseektolearnthe
θ
posterior distribution of z given x, p (·|x), and the likelihood of x given z respectively, given
θ
theassumedprioroverthelatentspace. Inordertomaketheposteriorlearnable,anapproximate
posteriordistributionisusedthatusuallyisamultivariatenormaldistribution. Theapproximation
p (·|x) ≈ q (·|x) is made where ϕ denotes the parameters of the probabilistic encoder, q (·|x).
θ ϕ ϕ
Theencodermapsaninputxtoameanvectorµ(x)∈Rdandtothelogofthevectorofvariances,
σ2(x)∈Rd. Asampleisdrawnfromtheapproximateposteriordistributionandthisisusedasan
inputtotheneuralnetworkwhichactsasthedecoder,D . Asϕappearswithinthedistributionofthe
θ
latentvariables,derivativescannotsimplybetakeninsidetheexpectationterm.Differentiabilityofthe
lossfunctionisrequiredfortheloss-drivenparameterupdatestepsandthereforeareparamaterization
trickneedstobeimplemented[3].
Insteadofdrawingzdirectly,ϵ∼N(0,I)isdrawnandz =µ(x)+diag(σ2(x))ϵisdetermined.
Thedistributionoverwhichwetakeexpectationisnowindependentoftheparametersforwhichwe
takederivatives,andderivativeupdatestepsmaynowbeimplemented. Theneuralnetworkdecoder
thenseekstoreconstructxfromthelatentvariablez.
Amaximumlikelihoodprincipleisthenappliedtothemarginallikelihoodofthedatap (X)toobtain
θ
estimatesfortheparameterswithintheencodersanddecoders. Asthelikelihoodisintractablealower
boundonthelog-likelihood,knownastheEvidenceLowerBound(ELBO)isinsteadmaximised,
givenby:
L(θ,ϕ)=E [ln(p (x|z))]−D (q (·|x)||p (·)) (1)
z∼qϕ(·|x) θ KL ϕ θ
whereD (r|s)denotestheKullback-Leibler(KL)divergencebetweendistributionsrands[13].
KL
E (·) denotes the expectation with respect to the conditional distribution of z given x.
z∼qϕ(·|x)
MaximisingtheELBOisequivalenttofindingabalancebetweenanapproximateposteriorthatis
closetothepriorandputtingweightonthelatentvariablespacethatmaximisesthelikelihoodofthe
datagiventhesesamevariables,ln(p (x|z)). WithouttheKLterm,thedeltafunctionisreturnedas
θ
theapproximateposteriorandtheautoencoderisrecovered.
With this VAE formulation the KL term often becomes very small or ‘vanishes’ leading to the
posteriorequallingtheprior(posteriorcollapse). ThisisreferredtoasKLvanishingandleadsto
adecoderthatislargelyindependentofthelatentvariables. See[14]forfurtherdiscussionofthis
problem. Tocombatthis,weimplementKLannealing–aprocedurewhereaweightβ isintroduced
ontheKLtermandgraduallyincreased,typicallyfrom0,aslearningoccurs[15]. Theobjective
functionbecomes:
L (θ,ϕ)=E [ln(p (x|z))]−βD (q (·|x)||p (·)). (2)
β z∼qϕ(·|x) θ KL ϕ θ
It is now assumed two data views with shared row dimension are present and represented by
X = {X 1,X 2} with X
i
∈ RN×ci. For brevity, the sample subscript is dropped and x
i
∈ Rci
refers toan individual realisationfrom view i. Here N is thenumber ofrowsin bothviews and
c is the number of columns in view i = 1,2. This shared row dimension implies the views are
i
paired,withrowjrepresentingthesameindividual/sampleinbothviews. Thisallowsformeaningful
correlationinthelatentspace. ThenotationintroducedinthissectionforsingleviewVAEsisused
inthefollowingsectionswheremulti-viewVAEsarepresented. Subscriptsareusedtodenotethe
relevantviewse.g. theencoderanddecoderinviewiarerepresentedbyE andD respectively.
iϕi iθi
2.2 Jointpriorvariationalautoencoder
Asthedifferentviewsinamulti-viewdatasetarefromacommonsource,correlationexistsbetween
theviews. Thistranslatestocorrelationbetweenthelatentspacesofeachviewfromindependently
trainedVAEs. JPVAEtakesadvantageofthiscorrelation,enforcingtherelationshipbetweenthe
twoviewsviaajointprioronthelatentvariables,asillustratedinFigure3. Enforcingtheproposed
correlationstructurebetweenthelatentspacesensureswecanmovefromtheoriginalspacewhere
4Figure3: Workflowtoobtainreconstructionsx˜ andx˜ fromrealisationsx andx usingalearnt
1 2 1 2
JPVAE structure. Trapeziums present the encoders and decoders of the two views. Vectors are
presentedbytherectangles. Thestructuresforview1and2areshowninblueandorangerespectively.
Theprioronthelatentvariablesisshowninred.
data are highly correlated in a non-linear fashion, to a space where the correlation is linear, and
backtothereconstructedspacethathasanon-linearcorrelation. Themarginalprioronthelatent
variablescorrespondingtoeachviewisastandardnormal,aswiththetraditionalVAE.However,a
cross-covariancematrixC isassumedbetweenthelatentvariablesz andz . HavingseparateVAE
1 2
structuresforeachviewassumesconditionalindependenceinbothdirections: (a)giventhedatathe
latentvariablesareindependent,and(b)giventhelatentvariablesthedataareindependent. This
allowstheuniquefeaturesofeachviewtobeencodedanddecoded.
Asthelatentspacesarelinearlycorrelated,itispossibletomovebetweenthemviatheconditional
distribution, as illustrated in Figure 2. This allows for imputation of missing views, obtaining a
reconstruction of x solely from realisation x (and vice versa). If a joint prior is not assumed,
2 1
separateVAEsaretrainedoneachviewandthereisnocorrelationenforcedbetweenlatentspaces.
Whilstsomecorrelationexistsbetweenthelatentspaces,itisnotasstrong,andthereforemaynot
produceasaccuratereproductions,asillustratedinthenumericalexperimentsofSection3.
2.3 Objectivefunction
Byassumingthatlatentvariablesforeachviewareindependentgiventhedata,theapproximated
posteriordistributedcanbeexpressedas:
q (z|x)=q ((z ,z )|(x ,x ))=q (z |x )q (z |x ). (3)
ϕ (ϕ1,ϕ2) 1 2 1 2 ϕ1 1 1 ϕ2 2 2
Theindividualapproximateposteriorsq (·|x )aremultivariateGaussianswithmeanandcovari-
ϕi i
ance matrix determined by the output of E . For input x , these are µ (x ) and diag(σ2(x ))
iϕi i i i i i
respectively. Astheposteriorsareassumedtobeindependent,thejointdistributionismultivariate
Gaussianwithmean(µ (x );µ (x )).Thecovariancematrixisrepresentedbybdiag(Σ (x ))with
1 1 2 2 i i
Σ (x )=diag(σ2(x )). Similarly,byassumingthedataisindependentgiventhelatentvariables,
i i i i
thelikelihoodfunctioncanbeexpressedas:
p (x|z)=p ((x ,x )|(z ,z ))=p (x |z )p (x |z ). (4)
θ (θ1,θ2) 1 2 1 2 θ1 1 1 θ2 2 2
This is equivalent to having separate encoders and decoders for each view but with a joint prior.
Assuming independence means the expectation term in the ELBO can be separated into terms
correspondingtotheseparateviews:
E [lnp (x|z)]=E [lnp (x |z )]+E [lnp (x |z )].
z∼qϕ(·|x) θ z1∼qϕ1(·|x1) θ1 1 1 z2∼qϕ2(·|x2) θ2 2 2
5Theobjectivefunctionthereforebecomes:
L (θ ,ϕ ,θ ,ϕ )=E [lnp (x |z )]+E [lnp (x |z )] (5)
β 1 1 2 2 z1∼qϕ1(·|x) θ1 1 1 z1∼qϕ2(·|x) θ2 2 2
−βD (q (·|x )q (·|x )∥p (·)).
KL ϕ1 1 ϕ2 2 θ
2.4 Jointprior
Thereareassumedtoben latentvariablesintheVAEforviewi,representedbytherandomvector
i
z
i
∈Rni. Withoutlossofgeneralityitisthatassumedn
1
≤n 2. Thejointpriordistributionofthe
random vector z = (z 1,z 2) ∈ Rn1+n2 is assumed to be a multivariate normal with mean 0 and
covariancematrix,Σ :
C
(cid:18) (cid:19)
I CT
Σ = 1 (6)
C C I
2
whereC ∈Rn2×n1 isthecross-covariancematrixencapsulatingtherelationshipbetweenthetwo
latentspaces. I isthen ×n identitymatrix. AsΣ isacovariancematrix,itneedstosatisfy
i i i C
twoconditions: symmetryandpositivesemi-definiteness. Withthedefinedstructureitisclearly
symmetricandsoitissufficienttorequireΣ tobeapositivesemi-definitematrixtoensureitisa
C
well-definedcovariancematrix. ForacovariancematrixwithstructureasdefinedinEq. 6,itisuseful
tonotethatΣ ispositivedefiniteifandonlyifI −CTC (orI −CCT)ispositivedefinite
C 1 2
[16]. Additionally,as(a)arealsymmetricmatrixispositive(semi)definiteifandonlyifallofits
eigenvaluesarepositive(non-negative)[17,p.51]and(b)amatrixisinvertibleifandonlyifallofits
eigenvaluesarenon-zero,covariancematrixΣ (andI −CTC/I −CCT)ispositivedefiniteif
C 1 2
andonlyifitisinvertible.
2.5 Kullback–Leiblerdivergenceterm
Asdiscussedearlier,forcalculatingtheELBOfortheproposedprior,theKLdivergencebetween
q (·|x )q (·|x )andtheprioron(z ,z ),p ,isrequired. ThefollowinggeneralresultonKL
ϕ1 1 ϕ2 2 1 2 C
divergencebetweenmultivariateGaussiansisappliedinthissetting.
Betweentwok-dimensionalmultivariateGaussiansrandswithrespectivemeansµ andµ and
r s
respectivecovariancematricesΣ andΣ ,theKLdivergenceisgivenby[18]:
r s
(cid:20) (cid:21)
D (s||r)= 1 ln|Σ r| −k+(µ −µ )T Σ−1(µ −µ )+tr(cid:8) Σ−1Σ (cid:9) . (7)
KL 2 |Σ | s r r s r r s
s
AsthisassumestheexistenceofΣ−1andΣ−1,bothcovariancematricesmustbepositivedefinite.
r s
ByutilisingtheresultofEq. 7withtheprioroverz =(z ,z )setasr =p ,andtheapproximate
1 2 C
posterior distribution as s = q (·|x )q (·|x ), the KL term of Eq. 2 is obtained. Explicitly,
ϕ1 1 ϕ2 2
p =N(0,Σ )andq (·|x )q (·|x )=N((µ (x ),µ (x )),Σ )withΣ =bdiag(Σ ). For
C C ϕ1 1 ϕ2 2 1 1 2 2 q q i
theutilisationofEq. 7,itisassumedthatΣ ispositivedefiniteandthevariancesoftheapproximate
C
posteriorarepositive.
Usingtheblockmatrixinverseresultfrom[19,p. 18](statedinAppendixA.1.1)theinverseofΣ
C
isgivenby:
(cid:18) (cid:19)
D −D CT
Σ−1 = 1 1 (8)
C −D C D
2 2
whereD = (I −CTC)−1 andD = (I −CCT)−1. Theseinversesareguaranteedtoexist,
1 1 2 2
giventheassumptionofpositivedefinitenessonΣ . UsingEq. 8andpropertiesofthetrace,we
C
obtain:
(cid:26)(cid:18) (cid:19)(cid:27)
tr(cid:8) Σ−1Σ (cid:9) =tr D 1Σ 1 −D 1CTΣ 2 =tr{D Σ }+tr{D Σ }.
C q −D CΣ D Σ 1 1 2 2
2 1 2 2
Additionally, as Σ is a block matrix, |Σ | = |I −CTC| = 1/|D | = 1/|D | [20, p. 114].
C C 1 1 2
Similarly. asΣ isadiagonalmatrixwehave|Σ |=|Σ ||Σ |. Writingµ =µ (x )andnoting
q q 1 2 i i i
6thatµ =0,weobtain:
p
D (q (·|x )q (·|x )||p )= 1(cid:2) µTD µ −ln|Σ |−n +tr{D Σ }(cid:3) (9)
KL ϕ1 1 ϕ2 2 C 2 1 1 1 1 1 1 1
+ 1(cid:2) µTD µ −ln|Σ |−n +tr{D Σ }(cid:3)
2 2 2 2 2 2 2 2
1(cid:104) (cid:105)
− ln|D |+µTD CTµ +µTD Cµ .
2 1 1 1 2 2 2 1
2.6 Matrixparameterization
ThematrixC isunknownandthereforemustbeeitherchosenaprioriorbeoptimised. Inourwork,
C isupdatedalongwithallotherparametersateachupdatestep,usingtheELBOfunctionasthe
lossfunction. Bydoingthis,twochallengeswerefacedandaddressed. Firstly,C mustbeupdatedin
suchawaythatΣ isavalidcovariancematrix. Secondly,adifferentiableparameterizationofC is
C
neededtoallowforend-to-endlearning. Conditionsforvalidityoftheupdatestepareoutlinedinthe
followingtheorems.
Theorem2.1. Σ definedasinEq. 6ispositivesemi-definiteifandonlyifallsingularvaluesofC
C
areboundedby1.
Proof. Duetopreviousremarks,itisequivalenttoshowthatI −CTC ispositivesemi-definiteif
1
andonlyifthesingularvaluesofC areboundedby1. Further,thisisequivalenttotheshowingthat
eigenvaluesofI −CTC arenon-negativeifandonlyiftheeigenvaluesofC areboundedby1.
1
TheeigenvaluesofCTC aregivenby{σ2(C)}n1 [21]meaningtheeigenvaluesof−CTC are
k k=1
given by {−σ2(C)}n1 . Applying Weyl’s Theorem [19, p. 242] (stated in Appendix A.1.2) to
k k=1
I −CTC implies:
1
λ (I )+λ (−CTC)≤λ (I −CTC)≤λ (I )+λ (−CTC) (10)
1 1 n1+1−k k 1 n1 1 n1+1−k
=⇒ 1≤λ (I −CTC)−λ (−CTC)≤1
k 1 n1+1−k
which combined with the relations above gives λ (I − CTC) = 1 + λ (−CTC) =
k 1 n1+1−k
1−σ2 (C). ThereforealleigenvaluesofI −CTC arenon-negativeifandonlyifallsingular
n1+1−k 1
valuesofC areboundedby1.
Iftheinequalityonσ (C)isreplacedbyastrictinequalitythenΣ isguaranteedtobepositive
1 C
definite. This is clear as all eigenvalues of I −CTC are now positive which ensures positive
1
definiteness. ThisguaranteestheapplicabilityofEq. 9. Afurtherrestriction,outlinedinTheorem
2.2,canbemadewhichenablesadifferentparameterizationofC whichassumesascaledorthogonal
relationshipbetweentheviews.
Theorem2.2. IfC =αC˜ ∈Rn2×n1 where|α|≤1andC˜ isasemi-orthogonalmatrix(C˜T C˜ =I 1)
thenΣ ispositivesemi-definite.
C
Proof. GiventheassumedstructureonC,wehavethefollowing:
I −CTC =I −α2C˜T C˜ =(1−α2)I . (11)
1 1 1
AsI ispositivesemidefinite,if|α|≤1then,sotooisI −CTC.
1 1
Alternatively,thiscouldbeseenasacorollarytoTheorem2.1-duetothesemi-orthogonalityof
C˜, all singular values of C˜ are equal to 1 which means σ (C) = α with |α| ≤ 1. Again, if the
i
conditiononαisreplacedwithastrictinequality(|α|<1))thisguaranteesapplicabilityofEq. 9. A
simplificationofEq. 9assumingthisorthogonalityconditioncanbefoundinAppendixA.2.
ToensureapplicabilityofEq. 9,wecaneitherrequire(a)C = αC˜ withC˜ anorthogonalmatrix
and |α| < 1 or (b) σ (C) < 1. The latter requires a matrix factorisation of C which includes
1
singular values. The most obvious approach is to use a singular value decomposition (SVD) of
7C i.e. C = USVT where U and V are orthogonal matrices and S is the matrix of singular
values i.e. S = diag(σ (C)). These singular values can be parameterized by σ (C) = (1−
k k
exp(−σ )/(1+exp(−σ ))forσ ∈R. TofullyparameterizeC withthesingularvalueconstraint,
k k k
a parameterization of orthogonal matrices U and V is needed. Similarly, a parameterization of
orthogonalC˜ isneededforoption(a). Thefollowingassumesn =n =n.
1 2
AsdiscussedbyShepardetal.[22],therearefourpopularparameterizationsoforthogonalmatrices.
Allmethodsintheirpreliminaryformparameterizeatmostasubsetoftheorthogonalmatrices(at
maximumthosewithdeterminant+1or-1). Theseparameterizationsmustthereforebeextendedto
maptotheentireorthogonalmatrixspace. Asitistheonlyone-to-onemapping,therationalCayley
transformhasbeenchosenforusewithinJPVAE.Initsoriginalform,theCayleytransformtellsus
thatforallorthogonalmatriceswithnoeigenvaluesequalto1,thereexistsauniqueskew-symmetric
matrixA∈Rm×msuchthatO =(I+A)(I−A)−1[22].Toextendtheparameterizationtothefull
spaceoforthogonalmatrices,anextramatrixconsistingof1sand−1salongthediagonalisneeded
[23,24]. LetJ = diag([1 ;−1 ])wherer = floor(m/(1+exp(−s)))fors ∈ R. ThenO =
m−r r
J(I+A)(I−A)−1isamappingontothespaceoforthogonalmatrices.C˜ isthereforeparameterized
infullby(n−1)2/2+1parameters: ({a ∈R:i<j for i,j ∈{1,··· ,n}},{s∈R}).
ij
2.7 Imputation
Oncethemodelhasbeenlearnedonthetrainingdata,missingviewscanbeimputed. Assumedatais
availableforviewj,butnotviewi,givenx wecanimputethemissingvalueofx . Datax isfed
j i j
intotheencoderforviewj,E ,andlatentvariablesaresampledgivingz =a. Anestimateofz
jϕ j i
canbeobtainedusingtheconditionaldistributionofz givenz . Thisisfedthroughdecoderi,D ,
i j iθ
toproduceanestimateofx givenx ,x˜ .
i j i|j
The joint marginal of z and z is assumed to be a multivariate normal with mean [µ ;µ ] and
1 2 1 2
covariancematrixΣ. Themaximumlikelihoodestimatorforthemeanandcovarianceareobtained
anddenotedby[µˆ ;µˆ ]andΣˆ respectively. Asanysubsetofvariablesfromamultivariatenormal
1 2
conditionedonaknownsecondsubsetofvariablesalsofollowsamultivariatenormaldistribution,
theconditionaldistributioncanbefoundexplicitly. Fori̸=j,thedistributionofz givenz =ais
i j
observedis:
z |z =a∼N(cid:16) µˆ +Σˆ Σˆ−1 (a−µˆ ),Σˆ −Σˆ Σˆ−1 Σˆ (cid:17) (12)
i j i ij jj j ii ij jj ji
whereΣˆ
kl
∈Rnk×nl isthesubmatrixofΣˆ correspondingtothevariablesassociatedwithviewkand
viewl. TheconditionalmeanE(z |z =a)=µˆ +Σˆ Σˆ−1 (a−µˆ )isthenusedasanestimateof
i j i ij jj j
thelatentvariablesinlatentspaceiandfedintoD toobtainx˜ ,theimputedvalueofx givenx .
iθ i|j i j
3 Numericalexperiments
ThroughaseriesofexperimentstheperformanceofJPVAEisexploredforbothimputationpurposes
aswellasfordownstreamanalyseslikeclassification. AsJPVAEenablesimputationofmissing
views(X˜ ),thisabilityisinvestigatedalongsidereconstructionofviews(X˜ ).
i|j i
A multi-view dataset was created from the binary version of the popular MNIST dataset, which
consistsofhandwrittendigitsfrom0to9[25]. Foreachimage,thetophalfwastakenasview1and
thebottomhalfwastakenasview2. ThisdatasetofhalvedimagesisreferredtoashvdMNIST.The
hvdMNISTdatasethasthedesirablepropertyofhavingastrongcorrelationbetweenviews. The
dataset contains 50,000 training images and 10,000 test images. Experiments are repeated with
5 different random seeds and the average and standard deviation reported. Details on the model
architectureandtrainingdetailscanbefoundinAppendicesA.3andA.4respectively.
TwovariantsofJPVAEareexplored,whereineachonethevalidityofΣ isenforcedviadifferent
C
ways. The first variant imposes a bound on singular values (such that σ (C) < 1). The second
1
variantenforcesascaledorthogonalitywhereCCT =CTC =α2I,withthevalueofαsetas0.95.
ThetwovariantsofJPVAEarecomparedwiththeC = 0case,whichcorrespondstocompletely
disjointVAEsforeachview.
Withoutexplicitlylearningacorrelationstructure,correlationispresentbetweenthetwogenerated
latentspaces. Byincorporatingajointpriorwithanon-zerocross-correlationaspresentedinEq. 6
8intothelossfunction,JPVAEincreasesthecorrelationbetweenviewsinthelatentspaceasshownin
Figure4. Table1illustratestheimprovementintheabilitytoreconstructview2givenview1and
viceversawhencorrelationstructureislearnt. Enforcingtheorthogonalityrestrictionimprovesthe
performanceofJPVAEcomparedwithsimplyapplyingthesingularvaluebound.
Figure4: Empiricalcross-correlationbetweenview1andview2inthelatentspaces. Theleftplot
representsempiricalcross-correlationforC = 0andtherightshowsthesameforC learntwith
theorthogonalityrestrictionimposed. TheFrobeniusnormofthesematricesare1.703and3.448
respectively.
Table 1: Average reconstruction losses across the entire dataset. Best results in bold, standard
deviationinbrackets.
Reconstruction Imputation
X˜ X˜ X˜ X˜
1 2 1|2 2|1
C =0 24.64(0.37) 25.56(0.24) 114.1(2.3) 127.5(3.2)
σ (C)<1 24.08(0.44) 25.02(0.21) 106.6(1.4) 117.4(4.1)
1
CCT =CTC =α2I 23.41(0.29) 23.98(0.31) 97.25(1.9) 106.6(1.9)
Theimprovedperformanceofthemethodisenabledbythelearntcorrelationthatpreventsposterior
collapse. Following the work by [26], the phenomenon of posterior collapse is indicated by the
percentage of active units (AU). The activity of unit u in the latent space is measured by A =
Cov
(cid:0)E [u](cid:1)
. Aunitisconsideredactive,i.e.
tonothavesufferedfromposteriorcollapsu
e,if
x u∼q(u|x)
A ≥10−2. AhigherpercentageofAUsarepreservedwhenC islearnt,withthebestcasescenario
u
observedwiththeorthogonalityconstraint(Table2). Wangetal.[12]provedthatposteriorcollapse
isequivalenttolatentvariablenon-identifiability. Thisindicatesthatbyenforcingtheorthogonality
restriction,wemakethelatentvariablespaceidentifiable.
Table2: Thepercentage(%)ofactiveunits(AU)acrosslatentspaces(z ,z ),outofatotal40. Best
1 2
resultsinbold,standarddeviationinbrackets.
AU
C =0 61(1.4)
σ (C)<1 66(1.4)
1
CCT =CTC =α2I 98.5(2.2)
Theimputedviews,X˜ andX˜ ,canbeusedfordownstreamtasks. Ifnotallviewsareavailable
1|2 2|1
foranindividual,thisallowstechniquesrequiringallviewstobeapplied. Asanillustrationofthe
performanceoftheimputeddataindownstreamtasks,abasicmulti-layerperceptronclassifierwas
9trainedandtestedondifferentcombinationsofreconstructedandimputedviews. Theperformance
ofaclassifiertrainedonthereconstructeddataindicatesthatwhenC islearnt,therelevantsignal
remainspresentinboththereconstructedandtheimputeddata. Theperformanceonimputeddatais
greatlyimprovedbylearningC,andinparticularbyenforcingtheorthogonalityconstraint(Table3;
resultswithstandarddeviationcanbefoundinFigure9.).
Table3: Resultsfor(Y,Z)representclassificationaccuracy%formodeltrainedonthetraining
splitofY andtestedonthetestsplitofZ. Accuraciesfor(X ,X )and(X ,X )withstandard
1 1 2 2
deviationinbracketsare93.59%(0.25)and90.83%(0.23)respectively. Bestresultsinbold. For
clarity,resultswithstandarddeviationreportedcanbefoundinFigure9.
View1 View2
(X˜ ,X˜ )(X˜ ,X˜ )(X˜ ,X˜ ) (X˜ ,X˜ )(X˜ ,X˜ )(X˜ ,X˜ )
1 1 1 1|2 1|2 1|2 2 2 2 2|1 2|1 2|1
C =0 89.07 47.21 77.22 85.88 51.83 78.81
σ (C)<1 89.67 64.50 80.22 86.46 67.06 82.38
1
CCT =CTC =α2I 90.31 77.22 83.54 87.80 76.55 86.30
Notonlydoeslearningcorrelationstructureimprovetheabilitytoimputedata,thereconstruction
lossandclassificationaccuracyforreconstructeddatax isimprovedcomparedwiththosescores
i
seenwhentrainingeachviewonseparateVAEs(Table1). Thismaybeduetotheincreaseduseof
thelatentspace,asevidencedbythehigherpercentageofactiveunits. Whilstaclassifiertrained
ontheimputeddatafromC =0demonstratestheretentionofsignal,thelowaccuracyseenfora
classifiertrainedonreconstructeddataandhighreconstructionlossindicatesthatitdoesnotretain
thespecificsignatureoftheinput. Forexample,takingthetophalfofadigit‘2’astheinput,itmay
correctlyreconstructthebottomhalfofarealisationofthedigit‘2’butnotcorrectlyreconstructthe
specificrealisation(asseeninFigure1a).
Usingthejointpriorweseethattheviewwithstrongersignal(view1)isbolsteringtheclassification
oftheviewwithweakersignal(view2). Whilstclassificationaccuracyonx ishigherthanthat
1
onx ,theaccuracyontheimputeddataX˜ seesasmallerdrop,andisgreaterthanthatofX˜ .
2 2|1 1|2
Notably,theaccuracyonimputeddata(X˜ ,X˜ )iscomparabletothaton(X˜ ,X˜ )whilstthe
2|1 2|1 2 2
sameforview1experiencesadropinperformance.
4 Conclusions
Anovelmulti-viewVAEapproachhasbeenproposedthatnativelystrengthensthecorrelationbetween
latent spaces via a joint prior. This is the first time that a connection between multi-view VAEs
ismadethroughajointpriorratherthanajointposterior,ashaspreviouslybeenimplementedin
theliterature. Theoreticalguaranteesandparameterizationsarepresentedthatallowforend-to-end
learning. By simultaneously preventing posterior collapse, JPVAE returns superior models and
demonstratesapromisingabilitytoimputemissingdatasuitablefordownstreamtasks.
10References
[1] Javier E. Flores, Daniel M. Claborne, Zachary D. Weller, Bobbie-Jo M. Webb-Robertson,
Katrina M. Waters, and Lisa M. Bramer. Missing data in multi-omics integration: Recent
advancesthroughartificialintelligence. FrontiersinArtificialIntelligence,6,2023. ISSN2624-
8212. doi: 10.3389/frai.2023.1098308. URLhttps://www.frontiersin.org/journals/
artificial-intelligence/articles/10.3389/frai.2023.1098308.
[2] DongdongLin,JigangZhang,JingyaoLi,ChaoXu,Hong-Wendeng,andYu-PingWang. An
integrative imputation method based on multi-omics datasets. BMC Bioinformatics, 17, 06
2016. doi: 10.1186/s12859-016-1122-6.
[3] DiederikP.KingmaandMaxWelling. Auto-encodingvariationalbayes. In2ndInternational
ConferenceonLearningRepresentations,2014. URLhttp://arxiv.org/abs/1312.6114.
[4] MengSong,JonathanGreenbaum,JosephLuttrell,WeihuaZhou,ChongWu,HuiShen,Ping
Gong,ChaoyangZhang,andHong-WenDeng. Areviewofintegrativeimputationformulti-
omicsdatasets. FrontiersinGenetics,11,2020.
[5] ThomasSutter,ImantDaunhawer,andJuliaVogt. Multimodalgenerativelearningutilizing
jensen-shannon-divergence. Advancesinneuralinformationprocessingsystems,33:6100–6110,
2020.
[6] YugeShi,N.Siddharth,BrooksPaige,andPhilipH.S.Torr. Variationalmixture-of-experts
autoencodersformulti-modaldeepgenerativemodels. InProceedingsofthe33rdInternational
ConferenceonNeuralInformationProcessingSystems,2019.
[7] MikeWuandNoahGoodman. Multimodalgenerativemodelsforscalableweakly-supervised
learning.InProceedingsofthe32ndInternationalConferenceonNeuralInformationProcessing
Systems,page5580–5590,2018.
[8] Noah Cohen Kalafut, Xiang Huang, and Daifeng Wang. Joint variational autoencoders for
multimodalimputationandembedding. NatureMachineIntelligence,5:631–642,2023.
[9] ImantDaunhawer,ThomasM.Sutter,KieranChin-Cheong,EmanuelePalumbo,andJuliaE
Vogt. On the limitations of multimodal VAEs. In International Conference on Learning
Representations,2022. URLhttps://openreview.net/forum?id=w-CPUXXrAj.
[10] Weiran Wang, Xinchen Yan, Honglak Lee, and Karen Livescu. Deep variational canonical
correlationanalysis. arXivpreprintarXiv:1610.03454,2016.
[11] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-
supervisedlearningviaredundancyreduction. In38thInternationalConferenceonMachine
Learning,ICML2021,pages12310–12320.MLResearchPress,2021.
[12] YixinWang,DavidBlei,andJohnPCunningham. Posteriorcollapseandlatentvariablenon-
identifiability.InM.Ranzato,A.Beygelzimer,Y.Dauphin,P.S.Liang,andJ.WortmanVaughan,
editors,AdvancesinNeuralInformationProcessingSystems,volume34,pages5443–5455,
2021.
[13] S.KullbackandR.A.Leibler. OnInformationandSufficiency. TheAnnalsofMathematical
Statistics,22(1):79–86,1951. doi: 10.1214/aoms/1177729694.
[14] HaoFu,ChunyuanLi,XiaodongLiu,JianfengGao,AsliCelikyilmaz,andLawrenceCarin.
Cyclicalannealingschedule: AsimpleapproachtomitigatingKLvanishing. InProceedings
ofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforComputational
Linguistics: HumanLanguageTechnologies,Volume1(LongandShortPapers),pages240–
250. Association for Computational Linguistics, 2019. doi: 10.18653/v1/N19-1021. URL
https://aclanthology.org/N19-1021.
[15] SamuelR.Bowman,LukeVilnis,OriolVinyals,AndrewM.Dai,RafalJózefowicz,andSamy
Bengio. Generating sentences from a continuous space. In Conference on Computational
NaturalLanguageLearning,2015.
11[16] StephenBoydandLievenVandenberghe. Convexoptimization,chapterMathematicalback-
ground,page650. CambridgeUniversityPress,2004.
[17] K.B.PetersenandM.S.Pedersen. Thematrixcookbook, Nov2012. URLhttp://www2.
compute.dtu.dk/pubdb/pubs/3274-full.html. Version20121115.
[18] L.Pardo. StatisticalInferenceBasedonDivergenceMeasures. Statistics:ASeriesofTextbooks
andMonographs.CRCPress,2018. ISBN9781420034813.
[19] RogerA.HornandCharlesR.Johnson. MatrixAnalysis. CambridgeUniversityPress,2edition,
2012.
[20] K.M.AbadirandJ.R.Magnus. MatrixAlgebra. EconometricExercises.CambridgeUniversity
Press,2005. ISBN9781139443647.
[21] JamesW.Demmel. AppliedNumericalLinearAlgebra,chapterLinearLeastSquaresProblems,
pages 110–111. Society for Industrial and Applied Mathematics, 1997. doi: 10.1137/1.
9781611971446.ch3.
[22] RonShepard,ScottR.Brozell,andGergelyGidofalvi. Therepresentationandparametrization
oforthogonalmatrices. TheJournalofPhysicalChemistryA,119(28):7924–7939,072015.
[23] W.L.Ferrar. Algebra: aTextbookofDeterminants,MatricesandAlgebraicForms. Oxford
UniversityPress,1960.URLhttps://archive.org/details/algebra032104mbp/page/
n3/mode/2up.
[24] A.I.KhuriandI.J.Good. Theparameterizationoforthogonalmatrices: areviewmainlyfor
statisticians. SouthAfricanStatisticalJournal,23,1989.
[25] Y.Lecun,L.Bottou,Y.Bengio,andP.Haffner. Gradient-basedlearningappliedtodocument
recognition. ProceedingsoftheIEEE,86(11):2278–2324,1998. doi: 10.1109/5.726791.
[26] YuriBurda,RogerBakerGrosse,andRuslanSalakhutdinov.Importanceweightedautoencoders.
ICLR,2015.
[27] DiederikP.KingmaandJimmyBa. Adam: Amethodforstochasticoptimization,2017. URL
https://arxiv.org/abs/1412.6980.
A Appendix/supplementalmaterial
TheAppendixcontainsadditionalresultsandfigurestosupplementthemainbodyoftext.
TheoreticalresultsusedwithinthemanuscriptarepresentedinSectionA.1. Thisisfollowedby
asimplificationoftheKLtermwhenorthogonalityconstraintsareassumedwhichisoutlinedin
SectionA.2. Themodelarchitectureandtrainingdetailsimplementedinthenumericalexperiments
are detailed in Sections A.4 and A.3 respectively. Lastly, additional results from the conducted
numericalexperimentscanbefoundinSectionA.5. Allresultsthroughoutthismanuscriptaregiven
to4significantfigures,withstandarddeviationsreportedto2significantfigures.
A.1 Theoreticalresults
Theoreticalresultsutilisedwithinthepaperarenowpresented.
A.1.1 Blockmatrixinverse
Theinverseofablockmatrix,whichisneededinSection2.5todeterminetheinverseofΣ inorder
C
toderivetheKLterm,isnowoutlined.
ForamatrixAwithblockmatrixstructure
(cid:18) (cid:19)
A A
11 12 (13)
A A
21 22
thefollowingresultholds[19,p. 18]:
12LemmaA.1. Assumingallrelevantinversesexist,theinverseofAasdefinedinEq. 13isgivenby:
(cid:18) (A −A A−1A )−1 A−1A (A A−1A −A )−1(cid:19)
11 12 22 21 11 12 21 11 12 22 . (14)
A−1A (A A−1A −A )−1 (A −A A−1A )−1
22 21 12 22 21 11 22 21 11 12
ApplyingthiswithA =I ,A =CT andA =C givestheresultinthetext.
ii i 12 21
A.1.2 Weyl’sinequality
The following inequality is used within the proof of Theorem 2.1 and concerns the sequence of
eigenvalues of matrices. In contrast to the rest of the paper, the eigenvalues are indexed in non-
increasing order, not non-increasing order of magnitude. For matrix M these are denoted by
{λˆ (M)}n .
j j=1
Weyl’sinequality[19,p. 242,Corrollary4.3.15]says:
LemmaA.2. LetA,B ∈Rn×nbesymmetricmatrices. Thefollowinginequalityholds
λˆ (A)+λˆ (B)≤λˆ (A+B)≤λˆ (A)+λˆ (B), j =1,...,n (15)
j 1 j j n
Notice that λ (I ) = λˆ (I ) = 1 for all j,k, and that as −CTC is negative semi-definite all
k 1 j 1
eigenvaluesarenon-positive. Thismeansλˆ (−CTC)=λ (−CTC). Applyingthislemmawith
j n−j
A=−CTC andB =I givesEq. 10.
1
A.2 KLtermsimplification
UndertheorthogonalityassumptiononC,theKLtermderivedinEq.9canbesimplified.Specifically,
if CCT = CTC = α2I for some α ∈ (0,1) then D = D = γI where I = I = I and
1 2 1 2
γ =1/(1−α2). ThereforeEq. 9reducesto:
D (q (·|x )q (·|x )||p )= 1(cid:2) γµTµ −ln|Σ |−n+γtr{Σ }(cid:3) (16)
KL ϕ1 1 ϕ2 1 C 2 1 1 1 1
+ 1(cid:2) γµTµ −ln|Σ |−n+γtr{Σ }(cid:3)
2 2 2 2 2
− 1(cid:2) nln|γ|+2γµTCµ (cid:3) .
2 1 2
A.3 Modelarchitecture
AllencodersanddecodersfortheJPVAE,aswellastheneuralnetworkusedforclassification,consist
oftwodenselayerswith512unitseach. Thelatentdistributionlayersconsistsoftwo20unitdense
layerswhicheachparameterizethemeanandthelogofthevariancesof20normalrandomvariables.
ThedecodersoutputlayerparameterizesthedistributionofaBernouillirandomvariableforeach
pixel. Theclassifieroutputlayerparameterizesthecategoricaldistributionwith10outcomes.All
activationfunctionswereReLu.
A.4 Trainingdetails
The JPVAE used an Adam optimiser [27] with learning rate 0.001, trained for 30 epochs with a
batchsizeof32andusedbinarycrossentropyasthereconstructionerror. ThecyclicalKLannealing
scheduleasintroducedby[14]isimplemented,withM =30.
Theclassifieristrainedfor50epochswithabatchsizeof32,stepsizeof0.01,exceptforonoriginal
datawhereitistrainedfor15epochstopreventoverfitting. Crossentropylossisusedastheloss
function.
13A.5 Additionalresults
Additionalresultsarepresentedinthissection.
Figure 5 illustrates examples of reconstructing view 2 from view 1, with and without learning
correlationnatively,foramodeltrainedwithadifferentrandomseedtothatdisplayedinFigure1.
Figures6and7illustrateexamplesofreconstructingview1fromview2,againwithandwithout
learningcorrelationnatively,formodelstrainedwithdifferentrandomseeds. AsinFigure1,the
imputationofthemissingviewobtainedwhencorrelationofthelatentspacesislearntnativelyisof
higherqualitythanwhennocorrelationislearnt. Toquantitivelyevaluatethissuperiorperformance,
asimplemulti-layerperceptronclassifieristrainedtheconcatenationofthereconstructedviews. Itis
thentestedonacombinationofthereconstructedandimputedviews. Explicitly,fordatasetswhere
e.g. view2isimputedfromview1,theclassifieristrainedon(thetrainingsplitof)[X˜ ;X˜ ]and
1 2
testedon(thetestingsplitof)[X˜ ;X˜ ]. Resultsofthisclassificationtaskforvarioustestdatasets
1 2|1
aredisplayedinFigure8. Resultsfortheclassificationtaskdescribedinthemainbodyofthetext,
andpresentedinTable3,areillustratedinFigure9withstandarddeviationincorporatedviaerror
bars.
(a)Nocorrelationlearnt,butestimatedem- (b)Correlationlearntnatively
piricallyaftertraining
Figure5: AdditionalrealisationofanimputationofthebottomhalfofMNISTdigits(view2of
thedata)usingthetophalfoftheimage(view1)onaJPVAEmodeltrainedwith(a)independent
priors(completelyseparateVAEs)and(b)ajointpriorwithlearntcorrelationstructurebetweenlatent
spaces. Thecrossentropylossbetweentruebottomhalfofimageandimputationis111.9in(a)and
101.5in(b).
14(a)Nocorrelationlearnt,butestimatedem- (b)Correlationlearntnatively
piricallyaftertraining
Figure6: ImputationofthetophalfofMNISTdigits(view1ofthedata)usingthebottomhalfofthe
image(view2)onaJPVAEmodeltrainedwith(a)independentpriors(completelyseparateVAEs)
and(b)ajointpriorwithlearntcorrelationstructurebetweenlatentspaces. Thecrossentropyloss
betweentruetophalfofimageandimputationis117.8in(a)and100.2in(b).
(a)Nocorrelationlearnt,butestimatedem- (b)Correlationlearntnatively
piricallyaftertraining
Figure 7: Additional realisation of an imputation of the top half of MNIST digits (view 1 of the
data)usingthebottomhalfoftheimage(view2)onaJPVAEmodeltrainedwith(a)independent
priors (completely separate VAEs) and (b) a joint prior with learnt correlation structure between
latentspaces. Thecrossentropylossbetweentruetophalfofimageandimputationis114.3in(a)
and104.1in(b).
15Figure8: Resultsfor[Y ;Y ]representclassificationaccuracy%formodeltrainedonthetraining
1 2
split of [X˜ ;X˜ ] and tested on the test split of [Y ;Y ] (the column wise concatenation of Y
1 2 1 2 1
andY ). Accuracyfor[X ;X ]withstandarddeviationinbracketsis98.04%(0.074). Errorbars
2 1 2
present+/-onestandarddeviation.
Figure9: Resultsfor(Y,Z)representclassificationaccuracy%formodeltrainedonthetraining
splitofY andtestedonthetestsplitofZ. Accuraciesfor(X ,X )and(X ,X )withstandard
1 1 2 2
deviationinbracketsare93.59%(0.25)and90.83%(0.23)respectively. Errorbarspresent+/-one
standarddeviation.
16