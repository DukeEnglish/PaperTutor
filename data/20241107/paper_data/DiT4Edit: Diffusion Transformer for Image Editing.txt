DiT4Edit: Diffusion Transformer for Image Editing
KunyuFeng1*,YueMa2*,BingyuanWang3*,ChenyangQi2,HaozheChen1,
QifengChen2†,ZeyuWang3†
1PekingUniversity
2TheHongKongUniversityofScienceandTechnology
3HongKongUniversityofScienceandTechnology(Guangzhou)
Abstract approaches often utilize DDIM (Song, Meng, and Ermon
2020) inversion to obtain latent maps and then control the
DespiterecentadvancesinUNet-basedimageediting,meth-
attentionmechanismindiffusionmodelsforrealimageedit-
odsforshape-awareobjecteditinginhigh-resolutionimages
ing (Mokady et al. 2023) (Cao et al. 2023). However, the
are still lacking. Compared to UNet, Diffusion Transform-
consistencyoftheeditedimagesdependsheavilyonthein-
ers(DiT)demonstratesuperiorcapabilitiestoeffectivelycap-
vertibility of the DDIM inversion process. Although some
turethelong-rangedependenciesamongpatches,leadingto
higher-quality image generation. In this paper, we propose efforts have focused on optimizing this inversion (Ju et al.
DiT4Edit*,thefirstDiffusionTransformer-basedimageedit- 2024)(Dongetal.2023)forbetterresults,theeditingframe-
ingframework.Specifically,DiT4EditusestheDPM-Solver workstillreliesontoomanytimesteps(e.g.,50steps).
inversion algorithm to obtain the inverted latents, reducing In addition, current research on image editing tasks
thenumberofstepscomparedtotheDDIMinversionalgo- mainly uses the UNet-based diffusion model struc-
rithmcommonlyusedinUNet-basedframeworks.Addition- ture (Rombach et al. 2022), making the final editing re-
ally,wedesignunifiedattentioncontrolandpatchesmerging,
sults heavily bounded by the generative capacity of UNet.
tailored for transformer computation streams. This integra-
Although the attention mechanism in UNet is also derived
tionallowsourframeworktogeneratehigher-qualityedited
fromthetransformer,DiT(PeeblesandXie2023)basedon
images faster. Our design leverages the advantages of DiT,
pure transformers offers a global attention calculation be-
enablingittosurpassUNetstructuresinimageediting,espe-
ciallyinhigh-resolutionandarbitrary-sizeimages.Extensive tween patches, allowing them to capture broader and more
experimentsdemonstratethestrongperformanceofDiT4Edit detailed features compared to the UNet with convolution
acrossvariouseditingscenarios,highlightingthepotentialof blocks, leading to higher-quality images. In addition, evi-
DiffusionTransformersinsupportingimageediting. dencefromDiTdemonstratesthattransformer-baseddiffu-
sion models offer better scalability and outperform UNet-
basedmodelsinlarge-scaleexperiments.
1 Introduction
To address these challenges, we explore image editing
Recentadvancesindiffusionmodelshavewitnessedimpres- tasks using the diffusion transformer architecture and pro-
siveprogressintext-drivenvisualgeneration.Thedevelop- videavaluableempiricalbaselineforfutureresearch.First,
ment of these text-to-image (T2I) models, e.g., Stable Dif- weaimtoleveragesolversthatrequirefewerinversionsteps
fusion(SD)(Rombachetal.2022),DALL·E3(Betkeretal. to reduce our inference time while maintaining the image
2023), and PixArt (Chen et al. 2023), has led to significant quality of the results. Specifically, we employ an inversion
impacts on numerous downstream applications (Ma et al. algorithm based on a high-order DPM-Solver (Lu et al.
2024a) (Ma et al. 2024b) (Wang et al. 2024), with image 2023) to obtain better latent maps with fewer timesteps.
editing as one of the most challenging tasks. Given a syn- We then implement a unified attention control scheme for
thetic or real input image, image editing algorithms aim to text-guidedimageeditingwhilepreservingbackgroundde-
add,remove,orreplaceentireobjectsorobjectattributesac- tails. Third, to mitigate the increased computational com-
cordingtotheuser’sintent. plexity of transformers compared to UNet, we use patches
Aprimarychallengeintext-drivenimageeditingismain- merging to accelerate computation. By integrating these
taining the consistency between the source and target im- keycomponents,weintroduceDiT4Edit,thefirstdiffusion
ages. Earlier approaches (Choi et al. 2021) (Kawar et al. transformer-basededitingframeworktoourknowledge.Ex-
2023a)(Zhangetal.2023b)oftenreliedonfine-tuningdif- perimentsdemonstratethatourframeworkachievessuperior
fusion models to address this issue. However, these meth- editingresultswithfewerinferencestepsandoffersdistinct
odstypicallyrequireconsiderabletimeandcomputationre- advantagesovertraditionalUNet-basedmethods.
sources, which limits their practical applicability. Recent
*Theseauthorscontributedequally.
†CorrespondingAuthor.
*Projectpage:https://github.com/fkyyyy/DiT4Edit
4202
voN
5
]VC.sc[
1v68230.1142:viXraA photo of a woman.…wearing glasses. A swan is swimming. A boat. A snow mountain. … in spring.
A Blue Volkswagen … black Jeep,
in the forest. front view.
A horse in the field. …on the beach. A anime girl with long yellow hair. …short black hair.
Figure1:VisualresultsofDiT4Edit.OurmethodisthefirstDiT-basedimageeditingframework,whichiscapableofhandling
imagesofvarioussizes:fromsmall(512×512)tolarge(1024×1024),andevenarbitrarydimensions(upto1024×2048).
Insummary,ourcontributionsareasfollows: 2019) and time-lapse synthesis by Nam et al. (Nam et al.
2019)advancedthefieldtowardsmorecompleximagesyn-
• Based on the advantages of transformer-based diffusion
thesis tasks. These methods provided richer context aware-
models in image editing, we introduce DiT4Edit, the
ness and temporal dimension control. The introduction of
first tuning-free image editing framework using Diffu-
DenoisingDiffusionProbabilisticModels(DDPMs)byHo
sionTransformers(DiT).
etal.(Ho,Jain,andAbbeel2020)andDALL·EbyRamesh
• To adapt to the computing mechanism of transformer-
et al. (Ramesh et al. 2021) marked significant leaps for-
baseddenoising,wefirstproposeaunifiedattentioncon-
ward, achieving breakthroughs in image quality, controlla-
trol mechanism to achieve image editing. Then, we in-
bility, and diversity. Designs and applications of diffusion-
troduce the DPM-Solver inversion and patches merging
basedmethodscanbeclassifiedbytaskssuchascontrollable
strategytoreduceinferencetime.
generation, stylization, and quality improvement. Nichol et
• Extensive qualitative and quantitative results demon-
al.(Nicholetal.2021)exploredtext-conditionalimagegen-
strate the superior performance of DiT4Edit in object
eration using diffusion models, finding classifier-free guid-
editing, style editing, and shape-aware editing for var-
ancetoproducerealisticimagescloselyalignedwithhuman
ious image sizes, including 512 × 512, 1024 × 1024,
expectations. Stylization techniques, such as InST (Zhang
1024×2048.
et al. 2023a) and DiffStyler (Huang et al. 2024), and qual-
ity enhancement tools like eDiff-I (Balaji et al. 2022) and
2 RelatedWork
ScaleCrafter (He et al. 2023) push the limits of image
2.1 Text-to-ImageGeneration resolution and fidelity. Moreover, these T2I models have
Prior to the emergence of diffusion models, the success- also driven the development of a series of video genera-
ful application of Transformer architecture (Vaswani et al. tion applications (Ma et al. 2022a) (Ma et al. 2023) (Ma
2017) to image recognition with Vision Transformer (ViT) et al. 2022b) (Ma et al. 2024c) (Chen et al. 2024). Specif-
byDosovitskiyetal.(Dosovitskiyetal.2020),andforhigh- ically, Chen et al. (Chen et al. 2023) present PIXART-α,
resolution image synthesis with Taming Transformers by a Transformer-based diffusion model that significantly re-
Esser et al. (Esser, Rombach, and Ommer 2021), demon- ducestrainingtimeandcostswhilemaintainingcompetitive
stratedtheimmensepotentialofTransformersinvisualgen- imagequality.
erative tasks. Subsequent methods like SPADE (Park et al.2.2 InteractiveImageEditing the model optimizes a denoising UNet ϵ by removing ar-
θ
tificialnoise,conditionedonbothtextpromptembeddingy
Image editing encompasses scenarios like iterative gen-
andcurrentimagesamplez ,whichisanoisysampleofz
eration, collaborative creation, and image inpainting. Re- t 0
atstept∈[0,T]:
search has focused on decoupling high-level concepts and
low-level styles within deep latent structures to improve minE ∥ϵ−ϵ (z ,t,y)∥2, (1)
diffusion-based models’ performance in tasks such as con- θ z0,ϵ∼N(0,I),t θ t 2
tentediting(detailcontrol)(Kawaretal.2023b),styletrans-
Aftertraining,itiscapableofconvertingrandomnoiseϵto
fer (Brack et al. 2022), and textual inversion (Gal et al.
animagesamplezbythelearneddenoisingprocess.
2022).Diffusion-basedmethodshaverevolutionizedcontent
DPM-Solver Sampling. During the inversion stage in
editing,offeringenhancedphotorealismandgreatercontrol
Diffusionprobabilisticmodels(DPMs),acleanimagex is
in text-guided synthesis and manipulation of images. Inno- 0
graduallyaddedwithGaussiannoiseandturnedintoanoisy
vationslikeILVR(Choietal.2021)andControlNet(Zhang,
samplex :
Rao,andAgrawala2023)enableprecisecontroloverimage t
attributes, while methods like GLIDE (Nichol et al. 2021) q(x |x )=N(x |α x ,σ2I), (2)
t 0 t t 0 t
and InstructPix2Pix (Brooks, Holynski, and Efros 2023)
showcase sophistication in content editing and adaptive re- where
α2
t is the signal-to-noise ratio (SNR), which is a
σ2
sponse to textual prompts. Hertz et al. (Hertz et al. 2022) strictly dt ecreasing function of t (Lu et al. 2023). Through
introduced a framework for image editing through textual solvingthediffusionODE,theDPMsamplingcanbefaster
prompts, highlighting the role of cross-attention layers in thanothermethods:
mapping text to image layout, enabling precise local and
dx g2(t) α g2(t)
global edits. Kumari et al. (Kumari et al. 2023) proposed t =(f(t)+ )x − t x (x ,t), (3)
Custom Diffusion, a method for incorporating user-defined dt 2σ2 t 2σ2 θ t
t t
conceptsintotext-to-imagediffusionmodels,allowingrapid
where x ∼ N(0,α2,I), and f(t) = dlogαt, g(t) =
adaptationtonewconcepts.Brooksetal.(Brooks,Holynski, T (cid:101) dt
andEfros2023)presentedInstructPix2Pix,amodelthatin- dα2 t − 2dlogαtα2 (Lu et al. 2023). It is shown in the pre-
dt dt t
terpretshuman-writteninstructionstoeditimagesaccurately viousworks (Luetal.2022) (ZhangandChen2022)that
withoutneedingper-examplefine-tuning.Finally,Parmaret ODE solver using the exponential integrator exhibits faster
al. (Parmar et al. 2023) addressed content preservation in convergencecomparedtotraditionalsolversduringsolving
image-to-image translation with pix2pix-zero, ensuring in- theEq.3.Bysettingthevalueofx ,thesolutionx ofEq.3
s t
putimagecontentremainsintactwhilefacilitatingsuperior canbecalculatedby:
performance in both real and synthetic image editing with-
outadditionaltraining.Althoughtherehavebeensignificant x = α tx −α (cid:90) λt e−λx (xˆ ,λ)dλ, (4)
advancementsinimageeditingusingdiffusionmodels,these t α s t θ λ
s λs
existing attempts of image editing are still bounded by the
wheretheλ =log(α /σ )isadecreasingfunctionoftwith
pretrained generative prior of UNet. Compared with UNet- t t t
the inversion function t (·), and recent research demon-
based diffusion model, DiT is more scalable and has more λ
stratesthattheDPM-Solvercansampletherealisticimages
succinctarchitecturewhileDiT’sapplicationinimageedit-
in10–20steps.
ingisstillunder-explored.
3.2 DiffusionModelArchitecture
3 Methodology
PIXART-α. Compared to the UNet structure, Diffusion
Ourproposedframeworkaimstoachievehigh-qualityimage
Transformers (DiT) (Peebles and Xie 2023) exhibits supe-
editing for various sizes based on a diffusion transformer.
rior scaling properties, generating images of higher quality
Our method is the first editing strategy based on a pre-
anddemonstratingbetterperformance.
trained text-to-image transformer-based diffusion model,
PIXART-α is a Transformer-based text-to-image (T2I)
e.g., PIXART-α (Chen et al. 2023). With our approach,
diffusion model that consists of three main compo-
users can achieve better editing results compared to exist-
nents: Cross-Attention layer, AdaLN-single, and Re-
ing UNet-based methods by providing a target prompt. In
parameterization. (Chen et al. 2023) Researchers have
this section, we first introduce the latent diffusion models
trainedthisT2Idiffusionmodelwiththreesophisticatedde-
and DPM inversion. Then we illustrate the superiority of
signs:decompositiontrainingstrategies,efficientT2Itrans-
transformer-baseddenoisinginimageeditingtasks.Finally,
former and high-informative data. Many experimental re-
wediscusstheimplementationdetailsofoureditingmethod.
sults demonstrate that PIXART-α performs better in im-
agequality,artistry,andsemanticcontrol.Comparedtoad-
3.1 Preliminaries:LatentDiffusionModels
vanced T2I SOTA models, PIXART-α has faster training
Latent Diffusion Models. The Latent Diffusion Model speed, lower inference cost, and superior comprehensive
(LDM) (Rombach et al. 2022) proposes an image genera- performance. In this paper, we use PIXART-α as the base-
tionmethodwithadenoisingprocessinthelatentspaceZ. lineforourproposedimageeditingmethod.
Specifically, it uses an encoder E to encode a pixel image The reason for using a transformer as the denoising
x into low-resolution latents z = E(x). During training, model. Compared to the UNet structure, the transformerA camel sitting in the
T5
desert.
Source Prompt
DPM-
Scale Self Cross Scale
Solver++ Shift Attention Attention Shift FFN Scale
Inversion
Diffusion Transformer
Input Image 𝒛𝑺 Reconstruction
𝑻 Patches
Unified Attention
Merging
Control
Target Prompt
A horse sitting in the
tfihS elacS tfihS elacS NFF elacS
desert. S Sc ha ifle t AttS ee nl tf ion AtC tero ns tis on S Sc ha ifle t FFN Scale
𝒛∗
T5 𝑻 Edited Image
Figure 2: Overview of the DiT4Edit framework. During the image editing process, our inversion algorithm generates high-
qualitylatentmaps,andthefinaleditedimageisachievedthroughunifiedattentioncontrol.
incorporates a global attention mechanism, allowing the sampler(Song,Meng,andErmon2021)asfollows:
model to focus on a broader range within the image. This σ
enhancedscalabilityenablestransformerstogeneratehigh- x ti = σ ti x ti−1 −α ti(e−hi −1)x θ(x ti−1,t i−1), (6)
qualityimagesatlargesizes(e.g.,greaterthan512×512), ti−1
and even at arbitrary sizes. The editing results of our DiT- Inpracticalapplications,itiscommontosetk =2,enabling
basededitingframeworkforlarge-sizedimagesaredemon- arapidinferenceandminimizingdiscretizationerrors.This
stratedinFigures1and2.Theserepresenteditingtasksnot DPM-SolvernamedDPM-Solver++(2M)(Luetal.2023):
previously addressed by UNet-based frameworks. There- σ
fore, we adopted a transformer-based denoising model for x ti = σ ti x ti−1 −α ti(e−hi −1) ·[
our editing framework, leveraging the transformer’s capa- ti−1 (7)
1 1
bilitiestotacklethesemorecomplexeditingchallenges.
[(1+ )x (x ,t )− x (x ,t )],
2r θ ti−1 i−1 2r θ ti−2 i−2
i i
3.3 DiffusionTransformer-basedImageEditing where 2M means this solver is a second-order multistep
solver.
Inthissection,weintroducethecomponentsofourproposed
However, during the inversion stage for the high-order
DiT4Edit.AsshowninFigure2,basedonapretraineddif-
samplers such as DPM-Solver++(2M), to obtain the inver-
fusiontransformer,thepipelineofourimageeditingframe-
sion result x in current timestep t , we need to approxi-
workisasfollows. ti i
mate the values in prior timesteps like {t ,t ,...} for
DPM-Solver inversion. As we discussed earlier, using i−2 i−3
theestimatedandtheanalyticallycomputedtermsinEq.5:
a high-order DPM-Solver (e.g. DPM-Solver++), can effec-
t t ai e tv ge tr il a my l eti em tr tmp ir −o (cid:82) 1v λ ,λe s utt seh i− ne gλs xa thθm edp λ Tli an i yng loes q rp ue eae xtd pi. o anT nso E ioqa np .p 4 ar , to g λx ivi tm ie −na 1t ,te h aet nh x de ti ti − hn e1- σ ti nk (cid:88)− =1 0x( θn)(x λti−1,λ ti−1)(cid:90) λλ tit −i
1
eλ(λ− nλ t !i−1)n dλ,
(8)
DPM-Solver++canobtainaexactsolutionvalueattimet :
i A recent work (Hong et al. 2024) introduced a strategy via
k−1 the backward Euler method to get the high-order term ap-
x = σ ti x +σ (cid:88) x(n)(x ,λ ) proximationinEq.8asfollows:
ti σ ti−1 ti−1 ti n=0(cid:124)θ λt (cid:123)i− (cid:122)1 ti−1 (cid:125) z (yˆ ,t )−z (yˆ ,t )
(cid:90) λti eλ(λ−λ ti−1)n
dλ+O(e hst kim +a 1te )d
,
(5) d′
i
=z θ(zˆ ti−1,t i−1)+ θ ti−1 i−1
2r i
θ ti−2 i−2 (9,
)
n! i
λti−1
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) om(cid:123) i(cid:122)
tted
(cid:125) where z
θ
is the denosing model, {yˆ ti−1,yˆ ti−2,...} is a set
analticallycomputed of value calculated by xˆ through the DDIM inversion to
ti
Especially when k = 1, the Eq. 5 is equivalent to DDIM estimate the (xˆ ti−1,xˆ ti−2) in Eq. 8, and r i = λ λti t− i−1− λλ tit −i− 12.A standing horse.
Figure 3: Visualization of the Query features in the self-attention layers of the PixArt-α. Features in the deeper layers (right
side)areobservedtocapturethesemanticlayoutmoreeffectivelythanthoseintheshallowlayers(leftside).
Thenwecangetainversionlatentzˆ incurrenttimestep However, MasaCtrl may still encounter some failure
ti−1
by: cases,whichcanbecausedbyitsuseofQ throughoutthe
tar
entire editing process, as mentioned in a recent work (Xu
zˆ =zˆ −ρ(z′ −zˆ ), (10) et al. 2024). To address this issue, we determine when to
ti−1 ti−1 ti ti
where z′ ti = σσ tit −i 1zˆ ti−1 −α ti(e−hi −1)d′ i. In DiT4Edit, adoptQ srcbysettingathresholdS forthenumberofsteps:
weutilizetheDPM-Solver++inversionstrategytoobtainan
(cid:26)
inversionlatentfrominputimagex 0fortheeditingtask.Ad-
MutualEdit=
Attention{Q src,K src,V src}, ift>S
ditionally, this technique was not used in previously UNet- Attention{Q ,K ,V }, otherwise
tar src src
basedimageeditingmethods.Furthermore,weobservethat (11)
wecanstillobtainagoodinversionlatentmapwithoutusing
DDIMinversiontocalculatethevaluesofyˆ.
Theunifiedcontrolofattentionmechanism.Inthepre-
viousworkPrompttoPrompt(P2P)(Hertzetal.2022),re-
searchersdemonstratethatthecrossattentionlayerscontain
rich semantic information from prompt texts. This finding I. A input feature map II. Assign the patches into III. Calculate the similarity between two
can edit images through replace the cross attention maps divided into patches. two roughly identical parts. parts and merge the most similar patches.
between the source image and target image during the dif-
fusion process. Specifically, the two commonly used text
guided cross attention control strategies are cross attention
replaceandcrossattentionrefine.Thesetwomethodsensure
the seamless flow of information from the target prompt to V. Unmerge the patches into original state. IV. Calculate attention Using the
merged patches.
the source prompt, thereby guiding the latent map towards
thedesireddirection. Figure4:Thecalculationofpatchesmerging.
Differentfromthecrossattention,theself-attentionmech-
anism in diffusion transformer is utilized to guide the for-
Patches merging. To enhance the inference speed, in-
mation of image layout, a feature that cannot be accom-
spired by token merging (Bolya et al. 2023), we embed
plishedbythecross-attentionmechanism.AsshowninFig-
patches merging into the denoising model. This approach
ure3,theobjectandlayoutinformationfromthepromptare
is motivated by the observation that the number of patches
not fully captured in the query vectors of the transformer’s
involvedinattentioncalculationswithinthetransformerar-
shallow layers but are well-represented in the deeper lay-
chitectureissignificantlygreaterthanthatinUNet.Thecal-
ers. Moreover, with an increasing number of transformer
culation flow is shown in Figure 4. For a feature map, we
layers, the query vectors’ ability to capture object details
first compute the similarity between each patch and merge
becomes clearer and more specific. This suggests that the
themostsimilaronestoreducethenumberofpatchespro-
transformer’s global attention mechanism is more effective
cessed by the attention mechanism. After attention calcu-
atcapturinglong-rangeobjectinformation,makingDiTpar-
lation, we unmerge the patches to maintain the original in-
ticularlyadvantageousforlarge-scaledeformationandedit-
put size for the next layer in the model. By incorporating
ingofextensiveimages.Thisobservationsuggeststhatnon-
patches merging into our framework, we aim to streamline
rigid editing of images can be achieved by controlling self
theprocessandimproveoverallefficiency,withoutaltering
attention mechanism. In MasaCtrl (Cao et al. 2023), re-
thefundamentaloperationsofeachlayer.
searchersintroducedthemutualselfattentioncontrolmech-
anism.Tobespecific,intheearlystepsofdiffusion,thefea-
4 Experiments
tureintheeditingstepsQ ,K ,andV willbeusedin
tar tar tar
4.1 ImplementationDetails
selfattentioncalculationtogenerateanimagelayoutcloser
to the target prompt, while in the later stages, the feature Foreditingtasksinvolvingimageswithascaleof512×512
inthereconstructionsteps––K andV willbeusedto andlargersizesupto1024×2048,weusepre-trainedmod-
src src
guidethegenerationofthetargetimagelayoutclosertothe elsPixArt-α−XL−512×512versionforthesmallerscale
originalimage. andPixArt-α−XL−1024×1024−MSversionforthelargerInput Ours InfEdit Instruct-P2P MasaCtrl SDEdit P2P-Zero PnPInversion(Direct)PnPInversion(DDIM)
A camel sitting on the desert. A horse…
A cat skateboarding in Time Square. A dog riding bicycle…
Input Ours
InfEdit Instruct-P2P MasaCtrl SDEdit P2P-Zero PnPInversion(Direct) PnPInversion(DDIM)
A scene of mountain,reflected in the lake. …withaboat.
Figure5:Comparativeexperimentwith512×512,1024×1024,andhigh-resolution,non-typicalaspect-ratioimagesagainst
thebaseline.DiT4Editachievessatisfactoryconsistencyineditingresults.
Metrics FID PSNR CLIP InferenceTime(s)
Models Structure 512×512 1024×1024 1024×2048 512×512 1024×1024 1024×2048 512×512 1024×1024 1024×2048 512×512
SDEdit(Mengetal.2021) 93.25 88.56 143.87 21.54 20.76 16.35 22.41 21.39 20.73 15.62
IP2P(Brooks,Holynski,andEfros2023) 88.63 98.73 103.52 20.36 19.73 17.13 20.64 19.36 21.53 12.43
Pix2Pix-Zero(Parmaretal.2023) 92.31 101.32 158.29 20.6 16.27 23.58 22.12 17.85 20.39 31.45
UNet-based
MasaCtrl(Caoetal.2023) 110.75 176.15 236.49 17.35 20.51 16.92 23.51 21.96 15.23 19.76
InfEdit(Xuetal.2023) 86.28 87.42 98.75 21.54 22.36 21.74 24.46 23.74 22.16 5.08
PnPInversion(Juetal.2024) 84.73 85.33 110.73 21.71 23.42 24.59 21.71 20.76 19.35 30.48
Ours DiT-basd 72.36 62.45 75.43 22.85 29.75 27.46 25.39 26.97 25.66 5.15
Table 1: Quantitative comparison results. We compare our model with six prior works: Pix2Pix-Zero (Parmar et al. 2023),
PnPInversion(Juetal.2024),SDEdit(Mengetal.2021),IP2P(Brooks,Holynski,andEfros2023),MasaCtrl(Caoetal.2023),
andInfEdit(Xuetal.2023),allimplementedusingofficialopensourcecode.
scale(Chenetal.2023).Weconducteditingonbothrealand IP2P (Brooks, Holynski, and Efros 2023), MasaCtrl (Cao
generated images. For the real image input, we use DPM- et al. 2023), and InfEdit (Xu et al. 2023), all implemented
Solverinversiontogetthelatentnoisemap.Weconfigured usingofficialopensourcecode.
the DPM-Solver with 30 steps, the classifier-free guidance
of 4.5, and a patches merging ratio of 0.8. All experiments AsshowninFigure5,wecompareourmethodon512×
wereconductedonanNVIDIATeslaA100GPU. 512 and 1024 × 1024 images. The first row of the Fig-
ure 5 demonstrates our framework has the ability to gen-
erateeditedimagesthatremainconsistentwiththeoriginal
4.2 QualitativeComparison
contentwheneditingreal512×512images,whereasexist-
Weevaluatethequalitativeperformancedifferencesbetween ing methods often alter the background or target details of
our proposed DiT4Edit editing framework and six prior the originalimage. Furthermore,the secondand third rows
baselines,includingPix2Pix-Zero(Parmaretal.2023),Pn- of Figure 5 illustrate our experiments with large-scale im-
PInversion (Ju et al. 2024), SDEdit (Meng et al. 2021), agesandarbitrarilysizedimages—tasksthatpreviousUNet-basedmethodsstruggledtoaddress.Theresultsindicatethat Input Image w/o merging w/ merging
our proposed framework effectively handles style and ob-
jectshapemodificationsinlargerimages.Incontrast,some
state-of-the-artUNet-basedmethods,despitebeingcapable
of performing editing tasks, frequently result in significant
alterations and damage to the background and object loca-
tions in the source image. Additionally, due to the limita-
tions of the UNet structure, these methods typically gener- A black swan swimming. A white swan…
atetargetimagesonlyatasizeof512×512.Thesefindings
emphasizethesubstantialpotentialoftransformer-baseddif-
fusionmodelsinlarge-scaleimageediting.Wealsoperform
theuserstudyforcomprehensivecomparison.Thedetailsof
userstudycanbefoundinsupplementarymaterial.
4.3 QuantitativeComparison Batman in Eiffel Tower. Iron Man…
For quantitative evaluation, we used three indicators: Figure 6: Ablation study on patches merging. The results
Fre´chetInceptionDistance(FID)(Heuseletal.2017),Peak indicatethatthismoduledoesnotimpactthefinalqualityof
Signal-to-Noise Ratio (PSNR), and CLIP to evaluate the imageediting.
performance differences between our model and SOTA in
imagegenerationquality,backgroundpreservation,andtext
alignment. We compared images at threesizes: 512×512, Input Image DPM-Solver DDIM
(30 step) (30 step)
1024×1024,and1024×2048,withresultsdetailedinTa-
ble1.WeperformtheperformanceswithPix2Pix-Zero,Pn-
PInversion, SDEdit, IP2P, MasaCtrl, and InfEdit. It should
be noted that, since no DiT-based editing framework pre-
viously existed, all our comparison baselines are based on
the UNet architecture. The experimental results show that
A light brown bear sitting on the ground. …standing
our proposed DiT4Edit editing strategy outperforms SOTA
methodsinimagegenerationquality,backgroundpreserva-
tion, and text alignment. Due to the global attention capa-
bilitiesoftheintegratedtransformerstructure,theDiT4Edit
frameworkexhibitsstrongrobustnessacrosseditingtasksof
various sizes. The generated images not only show higher
quality but also offer better control over the background
and details, resulting in greater consistency with the origi- A man standing in front of a mountain. …sitting
nal image. Particularly for editing large or arbitrarily sized
Figure7:AblationstudyonDPM-Solverinversion.Experi-
images,DiT4Editdemonstratessignificantadvantagesover
mentshavedemonstratedthattheDPMsolverachievessu-
other methods, showcasing the powerful scaling ability of
perioreditingresultscomparedtoDDIM,withfewerinfer-
thetransformerarchitecture.Meanwhile,oureditingframe-
encesteps.
workhasashorterinferencetime,comparabletotheinver-
sionfreeeditingmethod(InfEdit).
4.4 AblationStudy egyallowsforthegenerationofsuperiorlatentmaps,result-
inginbettereditingoutcomeswithinfewersteps.
We conduct a series of ablation studies to demonstrate the
effectiveness of the DPM-Solver inversion and patches
merging. The results of our ablation experiments on ImageSize PatchesMerging Speed(s)
patchesmergingarepresentedinFigure4andTable2.Im- (cid:33) 6.01
512×512
plementing patches merging led to a notable reduction in (cid:37) 9.13
the editing time for large-sized images while maintaining (cid:33) 34.27
1024×1024
editingqualitycomparabletothatachievedwithoutpatches (cid:37) 41.52
merging. This indicates that patches merging can signifi- 1024×2048 (cid:33) 99.39
(cid:37) 122.41
cantly enhance the overall performance of image editing
frameworks. Furthermore, the ablation experiment results
Table 2: Ablation study on patches merging. The result
for DPM-Solver and DDIM are illustrated in Figure 7.
demonstratesthatthistechniqueacceleratesmodelinference
When comparing the two methods with the same number
speed, especially for large-sized image editing, without af-
ofinferencesteps(T = 30),DPMSolverconsistentlyout-
fectingthequalityofthefinalimagegeneration.
performed DDIM in terms of image editing quality. This
demonstratesthatouruseoftheDPM-Solverinversionstrat-5 DiscussionandConclusion Training of Diffusion Transformer for Photorealistic Text-
to-ImageSynthesis. arXivpreprintarXiv:2310.00426.
Conclusion.WeintroduceDiT4Edit,thefirstimage-editing
framework based on a diffusion transformer. Unlike previ- Chen, Q.; Ma, Y.; Wang, H.; Yuan, J.; Zhao, W.; Tian,
ousUNet-basedframeworks,DiT4Editofferssuperioredit- Q.; Wang, H.; Min, S.; Chen, Q.; and Liu, W. 2024.
ingqualityandsupportsimagesofvarioussizes.Leveraging Follow-Your-Canvas: Higher-Resolution Video Outpaint-
DPM Solver inversion, a unified attention control mecha- ing with Extensive Content Generation. arXiv preprint
nism, and patch merging, DiT4Edit outperforms the UNet arXiv:2409.01055.
structure in editing tasks for images sized 512 × 512 and Choi, J.; Kim, S.; Jeong, Y.; Gwon, Y.; and Yoon, S. 2021.
1024×1024. Notably, DiT4Edit can handle images of ar- Ilvr: Conditioning method for denoising diffusion proba-
bitrary sizes, such as 1024 × 2048, showcasing the trans- bilisticmodels. arXivpreprintarXiv:2108.02938.
former’sadvantagesinglobalattentionandscalability.Our
Dong, W.; Xue, S.; Duan, X.; and Han, S. 2023. Prompt
researchcansetabaselineforDiT-basedimageeditingand
Tuning Inversion forText-driven Image Editing Using Dif-
help further explore the potential of transformer structures fusion Models. In Proceedings of the IEEE/CVF Interna-
ingenerativeAI. tionalConferenceonComputerVision(ICCV),7430–7440.
Limitation. In our experiment, we observed that the T5-
Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,
tokenizeroccasionallyencountersissueswithwordsegmen-
D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;
tation,whichcanleadtofailuresinthefinaleditingprocess.
Heigold,G.;Gelly,S.;etal.2020. Animageisworth16x16
Additionally,ourmodelmightexperiencecolorinconsisten-
words: Transformers for image recognition at scale. arXiv
ciescomparedtotheoriginalimage.Furthereditingfailures
preprintarXiv:2010.11929.
areprovidedinthesupplementarymaterials.
Potential social impact. Advances in image editing mod- Esser,P.;Rombach,R.;andOmmer,B.2021. Tamingtrans-
elsopendoorsforartisticinnovationbutalsopresentrisks. formersforhigh-resolutionimagesynthesis.InProceedings
These include challenges in assessing image authenticity oftheIEEE/CVFconferenceoncomputervisionandpattern
and potential privacy breaches from unauthorized edits. recognition,12873–12883.
Clearstandardsandregulationsareneededtoensurerespon- Gal, R.; Alaluf, Y.; Atzmon, Y.; Patashnik, O.; Bermano,
sibleuseandmitigatetheserisks.Futuremodeldevelopment A. H.; Chechik, G.; and Cohen-Or, D. 2022. An image is
willprioritizeaddressingtheseconcerns. worthoneword:Personalizingtext-to-imagegenerationus-
ingtextualinversion. arXivpreprintarXiv:2208.01618.
References He, Y.; Yang, S.; Chen, H.; Cun, X.; Xia, M.; Zhang, Y.;
Wang,X.;He,R.;Chen,Q.;andShan,Y.2023.Scalecrafter:
Balaji,Y.;Nah,S.;Huang,X.;Vahdat,A.;Song,J.;Zhang,
Tuning-free higher-resolution visual generation with diffu-
Q.; Kreis, K.; Aittala, M.; Aila, T.; Laine, S.; et al. 2022.
sion models. In The Twelfth International Conference on
ediff-i:Text-to-imagediffusionmodelswithanensembleof
LearningRepresentations.
expertdenoisers. arXivpreprintarXiv:2211.01324.
Hertz, A.; Mokady, R.; Tenenbaum, J.; Aberman, K.;
Betker, J.; Goh, G.; Jing, L.; Brooks, T.; Wang, J.; Li, L.;
Pritch, Y.; and Cohen-Or, D. 2022. Prompt-to-prompt im-
Ouyang, L.; Zhuang, J.; Lee, J.; Guo, Y.; et al. 2023. Im-
age editing with cross attention control. arXiv preprint
proving image generation with better captions. Computer
arXiv:2208.01626.
Science.https://cdn.openai.com/papers/dall-e-3.pdf,2(3):
8. Heusel,M.;Ramsauer,H.;Unterthiner,T.;Nessler,B.;and
Hochreiter, S. 2017. GANs Trained by a Two Time-Scale
Bolya,D.;Fu,C.-Y.;Dai,X.;Zhang,P.;Feichtenhofer,C.;
Update Rule Converge to a Local Nash Equilibrium. In
andHoffman,J.2023.TokenMerging:YourViTButFaster.
Guyon, I.; Luxburg, U. V.; Bengio, S.; Wallach, H.; Fer-
InTheEleventhInternationalConferenceonLearningRep-
gus,R.;Vishwanathan,S.;andGarnett,R.,eds.,Advancesin
resentations.
NeuralInformationProcessingSystems,volume30.Curran
Brack, M.; Schramowski, P.; Friedrich, F.; Hintersdorf, D.; Associates,Inc.
andKersting,K.2022. Thestableartist:Steeringsemantics
Ho, J.; Jain, A.; and Abbeel, P. 2020. Denoising diffusion
indiffusionlatentspace. arXivpreprintarXiv:2212.06013.
probabilistic models. Advances in neural information pro-
Brooks, T.; Holynski, A.; and Efros, A. A. 2023. Instruct- cessingsystems,33:6840–6851.
pix2pix: Learning to follow image editing instructions. In Hong, S.; Lee, K.; Jeon, S. Y.; Bae, H.; and Chun, S. Y.
ProceedingsoftheIEEE/CVFConferenceonComputerVi- 2024. On Exact Inversion of DPM-Solvers. In Proceed-
sionandPatternRecognition,18392–18402. ingsoftheIEEE/CVFConferenceonComputerVisionand
Cao, M.; Wang, X.; Qi, Z.; Shan, Y.; Qie, X.; and Zheng, PatternRecognition(CVPR),7069–7078.
Y.2023. Masactrl:Tuning-freemutualself-attentioncontrol Huang, N.; Zhang, Y.; Tang, F.; Ma, C.; Huang, H.; Dong,
for consistent image synthesis and editing. In Proceedings W.; and Xu, C. 2024. Diffstyler: Controllable dual diffu-
oftheIEEE/CVFInternationalConferenceonComputerVi- sion for text-driven image stylization. IEEE Transactions
sion,22560–22570. onNeuralNetworksandLearningSystems.
Chen, J.; Yu, J.; Ge, C.; Yao, L.; Xie, E.; Wu, Y.; Wang, Ju, X.; Zeng, A.; Bian, Y.; Liu, S.; and Xu, Q. 2024. Pnp
Z.; Kwok, J.; Luo, P.; Lu, H.; et al. 2023. PixArt-α: Fast inversion: Boosting diffusion-based editing with 3 lines ofcode. InTheTwelfthInternationalConferenceonLearning Nam, S.; Ma, C.; Chai, M.; Brendel, W.; Xu, N.; and Kim,
Representations. S. J. 2019. End-to-end time-lapse video synthesis from a
Kawar,B.;Zada,S.;Lang,O.;Tov,O.;Chang,H.;Dekel,T.; singleoutdoorimage.InProceedingsoftheIEEE/CVFCon-
Mosseri, I.; and Irani, M. 2023a. Imagic: Text-Based Real ferenceonComputerVisionandPatternRecognition,1409–
Image Editing With Diffusion Models. In Proceedings of 1418.
theIEEE/CVFConferenceonComputerVisionandPattern Nichol, A.; Dhariwal, P.; Ramesh, A.; Shyam, P.; Mishkin,
Recognition(CVPR),6007–6017. P.;McGrew,B.;Sutskever,I.;andChen,M.2021.Glide:To-
wardsphotorealisticimagegenerationandeditingwithtext-
Kawar, B.; Zada, S.; Lang, O.; Tov, O.; Chang, H.; Dekel,
guideddiffusionmodels. arXivpreprintarXiv:2112.10741.
T.; Mosseri, I.; and Irani, M. 2023b. Imagic: Text-based
realimageeditingwithdiffusionmodels. InProceedingsof Park,T.;Liu,M.-Y.;Wang,T.-C.;andZhu,J.-Y.2019. Se-
theIEEE/CVFConferenceonComputerVisionandPattern mantic image synthesis with spatially-adaptive normaliza-
Recognition,6007–6017. tion. In Proceedings of the IEEE/CVF conference on com-
putervisionandpatternrecognition,2337–2346.
Kumari,N.;Zhang,B.;Zhang,R.;Shechtman,E.;andZhu,
J.-Y. 2023. Multi-concept customization of text-to-image Parmar,G.;KumarSingh,K.;Zhang,R.;Li,Y.;Lu,J.;and
diffusion. In Proceedings of the IEEE/CVF Conference on Zhu, J.-Y. 2023. Zero-shot image-to-image translation. In
ComputerVisionandPatternRecognition,1931–1941. ACMSIGGRAPH2023ConferenceProceedings,1–11.
Peebles, W.; and Xie, S. 2023. Scalable Diffusion Models
Lu,C.;Zhou,Y.;Bao,F.;Chen,J.;Li,C.;andZhu,J.2022.
with Transformers. In Proceedings of the IEEE/CVF In-
Dpm-solver: A fast ode solver for diffusion probabilistic
ternationalConferenceonComputerVision(ICCV),4195–
model sampling in around 10 steps. Advances in Neural
4205.
InformationProcessingSystems,35:5775–5787.
Ramesh, A.; Pavlov, M.; Goh, G.; Gray, S.; Voss, C.; Rad-
Lu,C.;Zhou,Y.;Bao,F.;Chen,J.;Li,C.;andZhu,J.2023.
ford,A.;Chen,M.;andSutskever,I.2021.Zero-shottext-to-
DPM-Solver++:FastSolverforGuidedSamplingofDiffu-
image generation. In International conference on machine
sionProbabilisticModels. arXiv:2211.01095.
learning,8821–8831.Pmlr.
Ma, Y.; Cun, X.; He, Y.; Qi, C.; Wang, X.; Shan, Y.; Li,
Rombach,R.;Blattmann,A.;Lorenz,D.;Esser,P.;andOm-
X.; and Chen, Q. 2023. MagicStick: Controllable Video
mer, B. 2022. High-resolution image synthesis with latent
EditingviaControlHandleTransformations. arXivpreprint
diffusionmodels. InProceedingsoftheIEEE/CVFconfer-
arXiv:2312.03047.
ence on computer vision and pattern recognition, 10684–
Ma, Y.; He, Y.; Cun, X.; Wang, X.; Chen, S.; Li, X.; and 10695.
Chen, Q. 2024a. Follow your pose: Pose-guided text-to-
Song,J.;Meng,C.;andErmon,S.2020. DenoisingDiffu-
videogenerationusingpose-freevideos. InProceedingsof
sionImplicitModels. CoRR,abs/2010.02502.
the AAAI Conference on Artificial Intelligence, volume 38,
Song,J.;Meng,C.;andErmon,S.2021. DenoisingDiffu-
4117–4125.
sionImplicitModels.InInternationalConferenceonLearn-
Ma,Y.;He,Y.;Wang,H.;Wang,A.;Qi,C.;Cai,C.;Li,X.; ingRepresentations.
Li, Z.; Shum, H.-Y.; Liu, W.; et al. 2024b. Follow-Your-
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
Click: Open-domain Regional Image Animation via Short
L.;Gomez,A.N.;Kaiser,Ł.;andPolosukhin,I.2017. At-
Prompts. arXivpreprintarXiv:2403.08268.
tentionisallyouneed. Advancesinneuralinformationpro-
Ma,Y.;Liu,H.;Wang,H.;Pan,H.;He,Y.;Yuan,J.;Zeng, cessingsystems,30.
A.;Cai,C.;Shum,H.-Y.;Liu,W.;etal.2024c.Follow-Your-
Wang, J.; Ma, Y.; Guo, J.; Xiao, Y.; Huang, G.; and Li,
Emoji: Fine-Controllable and Expressive Freestyle Portrait
X. 2024. COVE: Unleashing the Diffusion Feature Cor-
Animation. arXivpreprintarXiv:2406.01900.
respondence for Consistent Video Editing. arXiv preprint
Ma, Y.; Wang, Y.; Wu, Y.; Lyu, Z.; Chen, S.; Li, X.; and arXiv:2406.08850.
Qiao, Y. 2022a. Visual knowledge graph for human action Xu, S.; Huang, Y.; Pan, J.; Ma, Z.; and Chai, J. 2023.
reasoninginvideos. InProceedingsofthe30thACMInter- Inversion-free image editing with natural language. arXiv
nationalConferenceonMultimedia,4132–4141. preprintarXiv:2312.04965.
Ma,Y.;Yang,T.;Shan,Y.;andLi,X.2022b. Simvtp:Sim- Xu, S.; Huang, Y.; Pan, J.; Ma, Z.; and Chai, J. 2024.
plevideotextpre-trainingwithmaskedautoencoders. arXiv Inversion-Free Image Editing with Language-Guided Dif-
preprintarXiv:2212.03490. fusion Models. In Proceedings of the IEEE/CVF Confer-
Meng, C.; He, Y.; Song, Y.; Song, J.; Wu, J.; Zhu, J.-Y.; enceonComputerVisionandPatternRecognition(CVPR),
and Ermon, S. 2021. Sdedit: Guided image synthesis and 9452–9461.
editingwithstochasticdifferentialequations. arXivpreprint Zhang,L.;Rao,A.;andAgrawala,M.2023. Addingcondi-
arXiv:2108.01073. tionalcontroltotext-to-imagediffusionmodels.InProceed-
Mokady,R.;Hertz,A.;Aberman,K.;Pritch,Y.;andCohen- ings of the IEEE/CVF International Conference on Com-
Or, D. 2023. NULL-Text Inversion for Editing Real Im- puterVision,3836–3847.
ages Using Guided Diffusion Models. In Proceedings of Zhang, Q.; and Chen, Y. 2022. Fast sampling of diffu-
theIEEE/CVFConferenceonComputerVisionandPattern sion models with exponential integrator. arXiv preprint
Recognition(CVPR),6038–6047. arXiv:2204.13902.Zhang, Y.; Huang, N.; Tang, F.; Huang, H.; Ma, C.; Dong,
W.; and Xu, C. 2023a. Inversion-based style transfer with
diffusionmodels. InProceedingsoftheIEEE/CVFconfer-
ence on computer vision and pattern recognition, 10146–
10156.
Zhang, Z.; Han, L.; Ghosh, A.; Metaxas, D. N.; and Ren,
J.2023b. SINE:SINgleImageEditingWithText-to-Image
DiffusionModels. InProceedingsoftheIEEE/CVFConfer-
enceonComputerVisionandPatternRecognition(CVPR),
6027–6037.