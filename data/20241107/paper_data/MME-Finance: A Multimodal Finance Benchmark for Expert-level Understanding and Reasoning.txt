Preprint. UnderReview.
MME-FINANCE: A MULTIMODAL FINANCE BENCH-
MARK FOR EXPERT-LEVEL UNDERSTANDING AND
REASONING
ZiliangGan1,YuLu1,DongZhang1,HaohanLi1,CheLiu2,JianLiu3,JiLiu1,HaipangWu1,
ChaoyouFu4,ZenglinXu5,RongjunchenZhang1,YongDai1
1HiThinkResearch,2ImperialCollegeLondon,3Beihang&4Nanjing&5FudanUniversity
ABSTRACT
Inrecentyears,multimodalbenchmarksforgeneraldomainshaveguidedtherapid
developmentofmultimodalmodelsongeneraltasks. However,thefinancialfield
hasitspeculiarities. Itfeaturesuniquegraphicalimages(e.g.,candlestickcharts,
technicalindicatorcharts)andpossessesawealthofspecializedfinancialknowl-
edge(e.g.,futures,turnoverrate). Therefore,benchmarksfromgeneralfieldsof-
tenfailtomeasuretheperformanceofmultimodalmodelsinthefinancialdomain,
and thus cannot effectively guide the rapid development of large financial mod-
els. To promote the development of large financial multimodal models, we pro-
poseMME-Finance,anbilingualopen-endedandpracticalusage-orientedVisual
Question Answering (VQA) benchmark. The characteristics of our benchmark
are finance and expertise, which include constructing charts that reflect the ac-
tual usage needs of users (e.g., computer screenshots and mobile photography),
creatingquestionsaccordingtothepreferencesinfinancialdomaininquiries,and
annotatingquestionsbyexpertswith10+yearsofexperienceinthefinancialin-
dustry. Additionally, we have developed a custom-designed financial evaluation
system in which visual information is first introduced in the multi-modal eval-
uation process. Extensive experimental evaluations of 19 mainstream MLLMs
areconductedtotesttheirperception,reasoning,andcognitioncapabilities. The
results indicate that models performing well on general benchmarks cannot do
wellonMME-Finance;forinstance,thetop-performingopen-sourceandclosed-
source models obtain 65.69 (Qwen2VL-72B) and 63.18 (GPT-4o), respectively.
Theirperformanceisparticularlypoorincategoriesmostrelevanttofinance,such
as candlestick charts and technical indicator charts. In addition, we propose a
Chineseversion, whichhelpscompareperformanceofMLLMsunderaChinese
context. Therefore,wehopetoopen-sourceourbenchmarktofosterthedevelop-
ment of multimodal models in the financial domain. The code and data will be
releasedathttps://hithink-research.github.io/MME-Finance.
1 INTRODUCTION
Multimodal Large Language Models (MLLMs) [45], which equip the Large Language Mod-
els(LLMs)[12,32,34,39]withthecapabilityofvisualunderstanding,haveexperiencedarevolu-
tionary advancement recently. Works including Flamingo [3], LLaVA [27], CogVLM [41], Gem-
ini [37], and GPT-4o [31] have demonstrated intriguing capability to solve complex multimodal
recognitionandreasoningtasks. Areasonableandobjectivebenchmarkisofenormoussignificance
inthesuccessofMLLMs,whichnotonlyhelpsabettercomparisonoftheperformancesofMLLMs
butalsoprovidesvaluableguidanceformodeloptimizationandreal-worldapplications.
Earlyworksofmultimodalbenchmarks,suchasCOCOCaption[8],GQA[19],andFlickr30k[47],
have served as foundational resources for evaluating MLLMs. However, these benchmarks are
task-specific, limiting the scope for fine-grained analysis of MLLMs’ capabilities. More recent
efforts, includingMMEseries[13,14,51], MMBench[28], andMM-Vet[48], haveshiftedfocus
1
4202
voN
5
]VC.sc[
1v41330.1142:viXraPreprint. UnderReview.
towardsgeneralmultimodaltasks. Thesebenchmarkscomprehensivelyevaluatediversecapabilities
of MLLMs, such as perception and reasoning, through a broader range of tasks. Alongside these
general-purposebenchmarks,domain-specificbenchmarksarerapidlyemerging.Forinstance,inthe
medicalfield,benchmarkslikeGMAI-MMBench[7]andAsclepius[42]havebeendeveloped,while
in the autonomous driving domain, NuScenes-QA [33] and DriveLM-DATA [36] are advancing
research. These benchmarks significantly accelerated the progress of MLLMs within respective
industries.
Inthefinancialfield, understandingchartspresentsmoreuniquechallenges. (1)Jargon: Financial
chartsarefilledwithtechnicaltermssuchas“bullish”,“bearish”,“supportlevels”,and“resistance
levels”,whichmaybehardtograsp.(2)Complexity:Financialchartsoftencontainavastamountof
dataandinformation,suchastheopen,close,high,andlowpricesonacandlestickchart,alongwith
varioustechnicalindicatorsandoscillators. (3)DiversityofChartTypes: Therearemultipletypes
of charts in the financial domain, such as line charts, bar charts, and candlestick charts, each with
itsspecificusecasesandinterpretationmethods. (4)DataDensity: Financialchartsmayincludea
large number of data points, making it more difficult to identify trends and patterns. Therefore, it
is challenging to comprehensively and professionally evaluate the financial capability of MLLMs.
BenchmarkslikeFINANCEBENCH[20]andCFBenchmark[21]arefocusingontheevaluationof
LLMs. Tothebestofourknowledge, thereisnomultimodalbenchmarkinthefinancialarea, and
a significant dearth of Chinese multimodal benchmarks. Hence, a bilingual financial multimodal
benchmarkisurgentforpromotingthedevelopmentofMLLMs.
To break this gap, we propose MME-Finance, a bilingual financial multimodal benchmark for
MLLMs. Weconductextensiveresearchonreal-worldfinancialapplicationscenariosandselect6
commontypesoffinancialcharts,includingcandlestickcharts,technicalindicatorcharts,statistical
charts,tables,documents,andmixedcharts. Basedontheseimagesandtheactualusageofusersin
financialscenarios,wedesignahierarchicalseriesofopen-endedQuestionAnswering(QA)tasks,
rangingfromgeneralvisualperceptionlikeOpticalCharacterRecognition(OCR)taskstocomplex
cognitive tasks such as providing investment advice. To ensure the quality of MME-Finance, we
carefullydesigntheannotationpipelineandinviteexpertswith10+yearsofexperienceinthefinan-
cialindustrytoconductdetailedverificationoftheanswers. LLMsandMLLMsareemployedfor
automated evaluation in MME-Finance. Considering the challenges of evaluating financial open-
ended questions, we meticulously design the evaluation process and first introduce visual infor-
mation to boost the evaluation performance. The effectiveness of our evaluation method has been
validatedthroughhumanconsistencyexperiments.ExtensiveexperimentsindicateexistingMLLMs
remain inadequate in meeting the requirements of financial tasks, where the best open-source and
closed-sourcemodelshavescoredunsatisfactorily,withonly65.69%(Qwen2VL-72B)and63.18%
(GPT-4o),respectively. Particularly,therearethreepointsworthyofourattention: Thefirstpointis
thatmodelsencounterdifficultyinsometasks,especiallyspatialawarenessandestimatednumerical
calculation. Thesecondpointisthattheperformancerelatedtostockchartsisnotgood(e.g.,can-
dlestickchartsandtechnicalindicatorcharts),andthelastisthatMLLMsgenerallyperformpoorly
in questions about mobile photography, which however is a relatively high-frequency use case in
financialQA.Wesummarizeourmajorcontributionsasfollows:
• WeproposeMME-Finance,anovelbilingualmultimodalbenchmarkspecificallydesigned
toevaluatethecapabilitiesofMLLMsinthefinancialdomain. Itcomprises1,171English
and 1,103 Chinese questions, covering diverse financial image types and various multi-
modalcapabilities,andprovidingacomprehensiveevaluationofMLLMs’performancein
thefinancialdomain.
• Weintroduceanevaluationapproachofopen-endedquestionsinthefinancialdomain. By
designing appropriate prompts for corresponding tasks and exploring evaluation methods
firstlycombinedwithimageinformation,weproposeanovelevaluationstrategythathasa
highconsistencywithhumans.ThestrategycanserveasareferenceforevaluatingMLLMs
forotherworks.
• Weconductextensiveevaluationon19MLLMsbasedonMME-Finance,revealingcritical
insightsaboutthestrengthsandshortcomingsofthecurrentMLLMsinfinancialapplica-
tions. Theinsightsgainedfromthisstudyprovideafoundationforfutureresearch,guiding
the development of more robust MLLMs capable of meeting the demands of complex fi-
nancialtasks.
2Preprint. UnderReview.
2 RELATED WORK
2.1 MLLMS
Recent advancements in LLMs [5, 10, 32, 34, 39] have catalyzed significant breakthroughs in
MLLMs. Utilizingpre-trainedLLMsallowsresearcherstocircumventtheresource-intensivepro-
cessoftrainingmodelsfromscratch,therebymarkedlyreducingcomputationalcosts.Byharnessing
thecognitivecapabilitiesofLLMs,MLLMsareadeptataddressingdiversemultimodalchallenges.
To facilitate alignment between different modalities, researchers have proposed several effective
connectors. Models such as BLIP-2 [24], Mini-GPT4 [52], Video-LLaMA [50], and X-LLM [6]
employQ-Formerforthealignmentofvisualandtextualfeatures,whiletheLLaVAseries[25,27]
andVITA[15]exploitMultiLayerPerceptrons(MLPs)forthispurpose. Additionally,Flamingo[3]
andCogVLM[41]incorporatesupplementarymodulestoenhanceinteraction andfusionbetween
visual and textual elements. Closed-source MLLMs, such as Gemini [37], GPT-4V [30], GPT-
4o[31],andClaude3.5Sonnet[11],demonstrateexceptionalcapabilitiesinvisualunderstanding.
WhiletheseMLLMsdemonstrateexcellentperformanceinstandardmultimodaltaskssuchasim-
age captioning [40] and Visual Question Answering (VQA) [4], their performance in specialized
domains, particularly finance, remains relatively unexplored. Financial images generally present
diversecontentandnecessitatesspecializedknowledgeforinterpretation,posingasubstantialchal-
lengeforMLLMs.
2.2 MULTIMODALBENCHMARKS
MLLMshavedemonstratedexceptionalcapabilitiesacrossvariouscomplextasks.Objectiveandac-
curatequantificationofthesecapabilitiesisessentialforinformingfuturedevelopmenttrajectories,
makingtheestablishmentofcomprehensivebenchmarkssignificantforadvancingMLLMs. Tradi-
tionalmultimodalbenchmarkstypicallyfocusonsingletasks,forinstance,COCOCaption[8]and
Flickr30k[47]addresscaptioning,whileGQA[19],VQAv2[16],andVizWiz[17]pertaintoVQA.
Other benchmarks assess specific capabilities, such as TextCaps [35] and Tap [43] for scene text
understanding,andVCR[49]forcommonsensereasoning. Subsequentbenchmarkshaveexpanded
in both data volume and task categories. The MME benchmark [13] proposes a comprehensive
assessment across 14 perception and cognition tasks, while MMBench [28] constructs over 3,000
multiple-choice image question pairs encompassing 20 abilities. SEED-Bench [22] and SEED-
Bench-2[23]furtherscalethesamplesizesto19,000and24,371QApairsfromdiversescenarios,
respectively. Collectively,thesebenchmarksprovidethoroughevaluationsofMLLMs’capacitiesto
tacklegeneralmultimodalchallenges.
However, theperformanceevaluationofMLLMsinspecificdomainsremainsunderexplored, par-
ticularly in finance. Existing benchmarks like FINANCEBENCH [20] and CFBenchmark primar-
ily[21]assessLLMsratherthanMLLMs.
3 MME-FINANCE
In this section, we introduce MME-Finance by first elaborating on the design philosophy of the
benchmarkinSection3.1, followedbyadetaileddescriptionofthedatacollectioninSection3.2,
dataannotationinSection3.3,andthestatisticsinSection3.4.Finally,weexpoundontheevaluation
methodofMME-FinanceinSection3.5.
3.1 HIERARCHICALABILITYLEVELSOFMME-FINANCE
TheabilitiesofMLLMscanbedividedintothreecategories: visualunderstanding,logicalreason-
ing, and complex cognition. MME-Finance references these categories and organizes a three-tier
ability structure. Specifically, we define perceptual ability as the low-level capacity for extracting
andinterpretingvisualinformationfromimages. Thisfoundationalabilitysupportsotheradvanced
capabilities. To evaluate the perceptual ability, MME-Finance employs tasks such as image cap-
tioning, OCR, entity recognition, and spatial awareness. As a middle-level ability, reasoning en-
compassesfinancial-relatednumericalreasoning. MME-Financeevaluatesthisabilitythroughtasks
3Preprint. UnderReview.
involving both estimated and accurate numerical calculations. Cognition is considered as a high-
levelability,whichrequiresintegratingperceptualandreasoningskillswithdomain-specificfinan-
cial knowledge to generate reasonable answers. The corresponding tasks, typically complex and
requiringexpert-levelfinancialinsight,includereasonexplanation,riskwarning,investmentadvice,
andfinancialknowledgeQA.Itshouldbenotedthatsomecognitivetasksareinsufficienttoanswer
basedsolelyonimageinformation. Forsuchtasks,MME-Financeprovidesadditionalbackground
information retrieved via web searches, supplementing the images and questions. These tasks re-
quire MLLMs to synthesize both image content and background information to derive the correct
answers. Additionally,toassessthecapabilitytohandlehallucinationsinMLLMs,MME-Finance
includesthenotapplicabletask,whichmeanstheanswerisnotapplicableforthequestion.
Figure1: DatacollectionpipelineofMME-Finance.
3.2 DATACOLLECTION
In MME-Finance, we collect financial images from various mainstream platforms. Figure 1 illus-
tratesthedatacollectionpipeline. First,weidentifyrelevantfinancialpagesonacomputeranduse
screenshot tools to capture the appropriate areas. Then, we use mobile devices to photograph the
corresponding sections. Next, we search for the same content on mobile applications and capture
screenshotsusingsmartphones. Theinclusionofdiverseimagestyles,includingcomputerscreen-
shots, mobilephotographs, andverticalandhorizontalmobilescreenshots, isintendedtosimulate
real-world application scenarios. MME-Finance categorizes the collected images into six types:
candlestickcharts,technicalindicatorcharts,statisticalcharts,tables,documents,andmixedcharts,
whereamixedchartincludesatleasttwoofothertypes. Theseimagescoverabroadspectrumof
financial scenarios, enabling MME-Finance to evaluate MLLMs’ ability to address challenges in
thisdomaincomprehensively.
3.3 QAGENERATION
Togeneratehigh-qualityQApairsforMME-Finance,eachQApairunderwentatleasttwostagesof
manualevaluation. Figure2illustratestheQAgenerationpipeline. Wefirstdesignseveralquestion
examples for each task. Then we utilize GPT-4o to generate candidate questions for every image
based on the example questions. We meticulously review the questions and correct inappropriate
ones. In the answer generation stage, we also use GPT-4o to generate preliminary answers based
on questions and images. We check all the answers manually and correct the wrong ones. The
complexsubjectivequestionsareevaluatedbyapanelofthreefinanceresearchers, eachwithover
10yearsofexperience. Thereferenceanswerisconfirmedwhenthereviewersreachaconsensus.
Afterthisprocess,financialexpertsconductanin-depthexaminationandrefinement. Thequalityof
MME-Financeissignificantlyenhancedthroughthemanualreviewmechanism.
3.4 STATISTICS
4Preprint. UnderReview.
Figure2: AnnotationgenerationpipelineofMME-Finance.
(a) (b)
Figure3: Distributionofdifferent(a)typesand(b)stylesofimages.
As shown in Table 1, English MME-Finance Table1: Statisticofthenumberofsamplesindif-
contains 1,171 image-question-answer pairs ferentcapabilitiesandtasks.
spanning 11 distinct tasks, categorized into
3 ability levels as detailed in Section 3.1. Statistic Number
In addition, MME-Finance incorporates ques- Perception 734
tions aimed at evaluating hallucinations [46] -ImageCaption 164
of MLLMs. The number of samples per task -OCR 178
variesfrom18to229,withthe“SpatialAware- -EntityRecognition 163
ness”taskcontainingthemostand“ReasonEx- -SpatialAwareness 229
planation” the fewest. Figure 3(a) illustrates Reasoning 175
the distribution of the 6 image types, where -AccurateNumericalCalculation 133
statistical charts account for the main propor- -EstimatedNumericalCalculation 42
tion, while mixed charts are the least. Fig- Cognition 240
ure 3(b) displays the distribution of 4 image -RiskWarning 22
styles. Computer screenshots and mobile pho- -InvestmentAdvice 53
tographs constitute similar proportions, repre- -ReasonExplanation 18
senting 47.3% and 40.5% of the total, respec- -FinancialQuestionAnswer 147
tively. Vertical and horizontal mobile screen- Hallucination 22
shotscontainapproximatelysamplesizes. The -NotApplicable 22
statisticresultsofChineseversionareshownin
theappendix.
3.5 EVALUATIONMETHOD
MME-Finance’sQAformatisintentionallyopen-endedtoreflectthecomplexityofreal-worldfinan-
cialscenarios. However,evaluatingopen-endedresponsespresentsgreaterchallengescomparedto
multiple-choicequestions. ToaccuratelyevaluatethecapabilitiesofMLLMs,wedesignacompre-
hensiveevaluationprocesstailoredtothecharacteristicsofourbenchmark. AsshowninFigure4,
duringtheinferencephase,promptsarecraftedtoconstraintheoutputformatsofMLLMs,thereby
facilitating a more standardized evaluation. Drawing inspiration from the evaluation methodology
usedinMM-Vet[48], weemployanLLM-basedevaluationsystemtocomparemodelpredictions
5Preprint. UnderReview.
withthegroundtruthandtoassignascore. Thescoringsystemisdividedintosixlevels, ranging
from 0 (completely incorrect) to 5 (fully correct), with the overall score being the average across
allsamples. Giventhediversityinresponseformatsacrossdifferenttasks,wedeveloptask-specific
evaluationpromptstoensureaccurateassessments. Additionally,afew-shotapproachisemployed
todefinescoringmetricsusingin-contextexamples,whichaidsthemodelinproducingmoreaccu-
rateevaluationscores. OurexperimentalresultsdemonstratethattheLLM-basedevaluator,particu-
larlyGPT-4o,achievesthehighestconsistencywithhumanevaluators.
Figure4: InferenceandevaluationpipelineofMME-Finance. Wefirstinputtheimageandquestion
prompt into the MLLMs. Then we feed the image and evaluation prompt into GPT-4o to obtain
scores. Thequestionandevaluationpromptsarealldesignedindividuallyforeachtaskcategory.
4 EXPERIMENT
In this section, we introduce the experimental setup for evaluating MLLMs firstly in Section 4.1,
followed by an exhibition and analysis of the experimental results. The main result analysis is
presented in Section 4.2, followed by a detailed analysis of the ability dimension in Section 4.2.1
andimagetypeandstyledimensioninSection4.2.2. Finally,Section4.3elaboratesonouranalysis
ofLLMasanevaluator.
4.1 EXPERIMENTALSETUP
We utilize MME-Finance to evaluate two types of MLLMs, (1) Open-Source MLLMs includ-
ing CogVLM2 [18], Qwen2-VL [38], MiniCPM-V 2.6 [44], Phi3-Vision [1], Phi3.5-Vision [1],
LLaMA3.2 [29], LLaVA-NEXT [26], YiVL [2], and InternVL2 [9]; (2) Proprietary MLLMs in-
cludingGPT-4o, GPT-4omini[31]. TheinferencepromptsarethesameforallMLLMsforafair
comparison, andazero-shotsettingisadopted. Wefilltheprompttemplatewithimage, question,
groundtruth,andresponsefromanMLLM,andtakethefilledpromptintoanLLM-basedevaluator
for generating a score range from 0 to 5 for one sample. The scores are multiplied by 20% to be
normalized.
4.2 MAINRESULTS
Table2showstheresultsofvariousMLLMsonEnglishMME-Financefromtheviewofeachtask.
Performance across the MLLMs varies significantly, with many models exhibiting low accuracy,
highlighting the challenging nature of the MME-Finance benchmark. Among the evaluated mod-
els,Qwen2VL-72Bachievesthebestoverallperformancewith65.69%accuracy,excellinginmost
tasks, particularlyOCRandANC.ProprietaryMLLM,i.e., GPT-4o, rankssecondoverallbutsur-
passesQwen2VL-72Binallcognition-relatedtasks. ThissuggeststhatGPT-4o’ssuperiorlanguage
processingcapabilitiesgiveitanadvantageintasksrequiringcomplexreasoning. Additionally,our
findings support the observation from MMBench [28] that the size of the language model has a
6Preprint. UnderReview.
Table2: EvaluationresultsonEnglishMME-Financeforalltasks. Abbreviationsadopted: ICfor
ImageCaption;ERforEntityRecognition;SAforSpatialAwareness;FQAforFinancialQuestion
Answer; ANC for Accurate Numerical Calculation; ENC for Estimated Numerical Calculation;
RW for Risking Warning; IA for Investment Advice; RE for Reason Explaination; NA for Not
Applicable. The first, the second, and the third highest values are highlighted by orange, blue,
and green backgrounds. Allnumbersaredenotedin%withthemaxvalueof100%.
Perception Reasoning Cognition
Model Overall NA
IC OCR ER SA ANC ENC RW IA RE FQA
OpensourceMLLMs
Yi-VL-34B 17.57 29.39 1.46 3.93 8.73 5.56 11.43 42.73 35.09 58.89 47.48 36.36
CogVLM2-19B 46.32 67.32 61.24 35.83 16.59 44.51 33.33 59.09 52.83 31.11 58.64 93.64
InternVL2-2B 37.42 59.63 46.97 21.23 18.52 28.27 19.05 59.09 50.94 60.00 51.70 33.63
InternVL2-4B 47.69 67.44 58.88 33.74 18.95 55.49 30.48 68.18 54.34 64.44 60.95 59.09
InternVL2-8B 53.58 71.71 68.43 38.28 25.33 62.86 37.14 72.73 60.75 76.67 63.13 61.82
InternVL2-76B 61.62 83.17 77.64 47.60 30.31 70.08 41.90 75.45 66.42 76.67 72.24 79.09
LLaMA3.2-11B 42.51 62.44 39.10 32.02 14.50 55.79 37.14 60.00 50.57 68.89 57.55 61.82
LLaMA3.2-90B 48.76 64.27 46.74 41.27 25.85 55.64 22.86 63.64 61.13 64.44 65.58 81.82
LLaVA-NEXT-7B 28.18 58.41 22.81 14.85 11.09 7.07 10.00 45.45 47.55 12.22 54.97 55.45
LLaVA-NEXT-13B 31.37 62.68 25.39 22.58 10.31 12.63 9.05 47.27 40.00 12.22 59.46 78.18
MiniCPM2.6 51.65 71.22 63.71 37.67 24.37 55.64 21.43 72.73 58.87 66.67 66.80 77.27
Phi3-Vision 46.69 69.88 57.64 28.34 18.08 47.52 34.76 65.45 58.11 68.89 57.41 100.0
Phi3.5-Vision 38.99 67.56 33.03 18.90 20.52 32.33 19.52 67.27 55.85 72.22 54.42 93.64
Qwen2VL-2B 44.42 62.07 66.07 28.47 20.09 44.36 23.33 53.63 44.53 58.89 53.47 68.18
Qwen2VL-7B 44.44 62.19 64.49 26.50 19.04 45.56 27.62 57.27 48.30 58.89 54.97 68.18
Qwen2VL-72B 65.69 82.56 87.52 55.46 27.16 83.76 40.95 78.18 65.66 77.78 75.37 90.91
ProprietaryMLLMs
GPT-4o-05-13 42.85 71.34 28.09 28.22 19.65 31.73 36.19 76.36 62.26 75.56 71.43 81.82
GPT-4o-mini 57.34 79.15 68.99 40.25 24.72 63.31 43.81 73.64 64.53 77.78 73.20 100.0
GPT-4o 63.18 83.66 79.21 49.81 27.07 71.88 44.76 84.54 70.57 80.00 76.87 93.64
significantimpactonperformance. Forinstance,largermodelsinthesameseries,suchasLLaVA-
NEXT-13BcomparedtoLLaVA-NEXT-7B,consistentlydemonstratebetterresults.
4.2.1 ABILITYDIMENSIONALANALYSIS
Perception.The“Perception”abilityencompassesfourtasks:ImageCaptioning(IC),OpticalChar-
acter Recognition (OCR), Entity Recognition (ER), and Spatial Awareness (SA), all of which pri-
marilyfocusonvisualunderstanding. MLLMstendtoperformrelativelywellinICandOCRtasks,
suggestingthatcurrentmodelsexhibitsatisfactorygeneralvisualperceptioncapabilities. However,
theSAtaskprovestobethemostchallenging,withanhighestaccuracyofonly30.31%. Thisdiffi-
cultylikelystemsfromtheneedforfine-grainedperceptualabilitiesintheSAtask. Forexample,as
showninFigure5,thetaskrequiresidentifyingthehighestMovingAverage(MA)line.SeveralMA
linesarecloselypositionedintheimage, makingitdifficultforthemodelstodistinguishbetween
them. ThissuggeststhatalthoughMLLMsexhibitcompetenceingeneralvisualtasks,thereremains
substantialroomforimprovementinmorefine-grainedvisualperception.
Reasoning. The“Reasoning”abilityconsistsoftwotasks: AccurateNumericalCalculation(ANC)
andEstimatedNumericalCalculation(ENC),bothofwhichfocusonmathematicalandlogicalrea-
soning. Among these, the ENC task is significantly more challenging. This difficulty arises from
the inherent complexity of estimating reasonable numerical values, a ability that current MLLMs
strugglewith. AsshowninTable2,thebest-performingmodel,i.e.,Qwen2VL-72B,achievesonly
40.95% accuracy on the ENC task, which is much lower compared to 83.76% on the ANC task.
ThisdiscrepancyhighlightsthecontinueddifficultyMLLMsfaceinhandlingestimation-basedrea-
soningproblems. AsdepictedinFigure6,theexactnumericalvaluesarenotexplicitlypresentedin
7Preprint. UnderReview.
Figure5: HardexampleinSAtaskofperceptioncapability.
Table3: EvaluationresultsonEnglishMME-Financefordifferenttypesandstylesofimages. Ab-
breviations adopted: Candle. for Candlestick chart; Tech. for Technical indicator chart; Stat. for
Statistical chart; Tab. for Table; Doc. for Document; Mixed for Mixed chart; CS for Computer
Screenshot; MPforMobilePhotograph; VSforVerticalScreenshotonMobile; HSforHorizontal
ScreenshotonMobile.Thefirst,thesecond,andthethirdhighestvaluesarehighlightedby orange,
blue,and green backgrounds. Allnumbersaredenotedin%withthemaxvalueof100%.
Model Candle. Tech. Stat. Tab. Doc. Mixed CS MP VS HS
OpensourceMLLMs
Yi-VL-34B 23.64 16.36 18.76 15.42 14.89 32.38 19.42 14.39 26.06 16.62
CogVLM2-19B 39.44 35.57 52.30 50.38 45.76 57.14 47.33 44.22 49.70 49.09
InternVL2-2B 30.35 33.18 38.62 40.00 38.49 58.10 40.36 34.73 35.45 34.55
InternVL2-4B 35.38 38.98 51.48 54.66 47.77 63.81 50.87 44.85 43.64 45.71
InternVL2-8B 42.38 45.00 60.41 57.79 52.59 67.62 56.39 51.56 48.79 49.87
InternVL2-76B 55.52 47.50 63.02 70.84 63.09 67.62 62.78 61.73 54.54 58.70
LLaMA3.2-11B 35.24 31.59 47.63 50.92 39.42 48.57 45.16 39.07 38.79 47.79
LLaMA3.2-90B 40.56 40.11 51.20 58.17 45.83 64.76 50.14 46.33 46.06 56.10
LLaVA-NEXT-7B 29.65 23.52 28.80 28.32 28.34 44.76 28.45 26.08 32.73 35.32
LLaVA-NEXT-13B 27.27 26.36 33.68 32.14 32.95 39.05 32.67 29.20 30.91 35.84
MiniCPM2.6 45.03 45.00 54.23 58.63 49.42 59.05 52.09 50.51 45.45 60.78
Phi3-Vision 37.62 40.00 49.48 49.54 48.71 62.86 49.75 43.08 40.30 52.21
Phi3.5-Vision 32.73 30.45 46.25 38.24 39.21 59.05 44.73 32.28 41.52 36.88
Qwen2VL-2B 38.74 40.80 46.60 46.26 44.68 57.14 45.13 43.71 38.79 48.57
Qwen2VL-7B 39.72 41.70 46.60 46.11 44.03 54.29 44.73 44.09 36.97 50.91
Qwen2VL-72B 60.12 60.11 65.15 71.73 66.04 74.24 67.65 62.78 68.48 67.01
ProprietaryMLLMs
GPT-4o-05-13 44.62 32.84 52.99 37.18 41.65 60.95 46.43 37.26 47.27 47.79
GPT-4o-mini 51.89 50.91 63.37 60.46 54.10 68.57 59.71 53.45 62.12 60.00
GPT-4o 58.32 55.68 67.84 68.55 59.71 73.33 65.67 58.31 69.09 70.13
theimage,requiringthemodeltoinferthesevaluesbasedoncontextualclues,suchasspatialrela-
tionships. Hence,theinabilitytoreasonablyestimatesuchvaluesalsoremainsacriticallimitation
ofcurrentMLLMs.
Cognition. The“Cognition”task,consistingofRiskWarning(RW),InvestmentAdvice(IA),Rea-
son Explanation (RE), and Financial Knowledge QA (FQA), assesses the ability of MLLMs to
make complex financial decisions. Due to the inherently subjective nature of these questions, the
performance variance among different models is smaller compared to other tasks. This suggests
thatcurrentMLLMsdemonstrateabasiccompetenceinfinancialreasoning. GPT-4oachievesthe
highest overall score across those 4 tasks, indicating its superior capability in handling financially
complexandsubjectivedecision-makingscenarios.
8Preprint. UnderReview.
Figure6: HardexampleinENCtaskofreasoningcapability.
Hallucination Problem. The Not Applicable (NA) task is specifically designed to assess the hal-
lucination of MLLMs. For this task, we develop an inference prompt that explicitly informs the
modelsthattheycanrespondwith“NotApplicable”iftheydeterminethatnosuitableanswerisat-
tainable. AsshowninTable2,modelssuchasCogVLM2-19B,Phi3.5-Vision,Qwen2VL-72B,and
GPT-4odemonstrateastrongabilitytodiscernwhetheraquestionisanswerable,therebymitigating
thehallucinationissue. Incontrast,modelslikeYi-VL-34B,InternVL2-2B,andLLaVA-NEXT-7B
exhibitseverehallucinationproblems. SincethepromptremindsMLLMsofthe“NotApplicable”
optionintheNAtask,thetaskdifficultyissomewhatreduced. Toexplorehallucinationissuesmore
comprehensively, we modified the prompt to allow “Not Applicable” response across all types of
tasks.ThisledtoariseinfalsenegativesinmostMLLMs,wheremodelsincorrectlymarkquestions
with valid answers as unanswerable, suggesting that hallucination remains a significant challenge
formanyMLLMs. Thecorrespondingexperimentalresultsaredetailedintheappendix.
4.2.2 IMAGETYPEANDSTYLEDIMENSIONALANALYSIS
Table3presentstheperformanceofvariousMLLMsfromtheperspectiveofimagetypesandstyles.
Notably,mostmodelsexhibitpoorperformanceoncandlestickchartsandtechnicalindicatorcharts.
This can be attributed to the specialized nature of these charts, which demand domain-specific
knowledge that current MLLMs struggle to interpret. Regarding image styles, MLLMs exhibit
suboptimalperformancewhenappliedtomobilephotographs,primarilyattributedtothelowerres-
olutionofimagescapturedbyphones,whichhampersthevisibilityofcrucialdetails. Furthermore,
manyofthesephotosaretakenatobliqueangles,leadingtoincompleteorextraneousvisualinfor-
mation. Giventheprevalenceofsuchimageinreal-worldapplications,itisimperativetoenhance
theproficiencyofMLLMsinordertoeffectivelyprocessandanalyzemobilephotographs.
4.3 ANALYSISOFEVALUATORS
For a fair comparison of the tested MLLMs, we conduct extensive experiments to assess the ef-
fectivenessofvariousevaluators. First,weselect100samplesandgeneratecorrespondingoutputs
fromMiniCPM2.6. Toensureobjectivity,eachsampleisscoredbythreeexperiencedexperts. The
finalscoreforeachsampleisdeterminedbymode(e.g.,3forscoresof2,3,and3)ormean(e.g.,
2 for scores of 1, 2, and 3). If the score variance exceeds 2, the sample will undergo further re-
viewtodetermineafinalscore. Then,Spearman’srankcorrelationcoefficientandaverageabsolute
differences are calculated to indicate the evaluators’ performance. As shown in Table 4, GPT-4o
with image input achieves the highest Spearman’s rank correlation coefficient and the lowest av-
erageabsolutedifference,indicatingsuperiorperformance. GPT-4-Turboalsodemonstratesstrong
performance,significantlyoutperformingGPT-3.5-Turboando1-preview. Amongtheopen-source
evaluators,Qwen2VL-72BdemonstratesperformancecomparabletoGPT-4o,substantiallysurpass-
ingCogVLM2andMiniCPM2.6.Therefore,Qwen2VL-72Bcanserveasacost-effectivealternative
toGPT-4o. Furthermore,weobservethatmostevaluatorsperformbetterwithadditionalimagein-
puts. Wearguethatthisimprovementisduetotheimagesprovideevaluatorswithadditionalinfor-
mation. Wefurtherdividethequestionsintotwocategories:subjectiveandobjective. FromTable5,
9Preprint. UnderReview.
Table 4: The comparison of Spearman’s rank correlation coefficient (Sp.) and average absolute
differences (∆) between the evaluation scores of various evaluators and human-annotated scores.
Larger Sp. and smaller ∆ represent a better agreement with human evaluation, indicating a better
evaluator. Abbreviationsadopted: Cog. forCogVLM2-19B;Mini. forMiniCPM2.6;Qwen72Bfor
QwenVL2-72B.Pic. representsaddingimageasinputwhenevaluating.
Model GPT-3.5Turbo GPT-4Turbo o1-preview GPT-4o(Pic.) Cog.(Pic.) Mini.(Pic.) Qwen72B(Pic.)
Sp.(↑) 0.498 0.711 0.592 0.720(0.738) 0.049(0.027) 0.048(0.162) 0.688(0.678)
∆ (↓) 1.39 0.93 1.14 0.90(0.84) 2.18(2.27) 2.07(1.88) 1.02(1.01)
Table 5: The Spearman’s rank correlation coefficient (Sp.) and average absolute differences (∆)
between the evaluation scores of GPT-4o and human-annotated scores. Larger Sp. and smaller ∆
represent a better agreement with human evaluation, indicating a better evaluator. Objective and
Subjective denote objective and subjective questions. w and w/o represent evaluating with and
withoutimage.
Objective Subjective
w w/o w w/o
Sp. ∆ Sp. ∆ Sp. ∆ Sp. ∆
0.835 0.57 0.849 0.60 0.515 1.23 0.471 1.3
itcanbeseenthattheGPT-4oevaluatorexhibitshigherconsistencyinscoringobjectivequestions,
andtheinclusionofimagesnotablyimprovesaccuracyinevaluatingsubjectivequestions.
5 CONCLUSION
Inthispaper,wehaveintroducedMME-Finance,apioneeringefforttoestablishabilingualmulti-
modalbenchmarktailoredtoevaluatethecapabilitiesofMLLMswithinthespecializedfinancialdo-
main. Byencompassingadiverserangeoffinancialopen-endedquestions,MME-Financepresents
challengesthatspanfrombasicvisualunderstandingtocomplexfinancialreasoningandexpert-level
decision-making. Moreover, a novel evaluation strategy is proposed to ensure accurate evaluation
of the MLLMs. Our detailed evaluation of 19 MLLMs shows that both open-source and propri-
etary models have significant limitations in handling complex financial questions. MME-Finance
canserveasacriticaltooltoguidethedevelopmentofMLLMcapabilitiesinthefinancialdomain.
Infuturework,weplantoexpandthedatasizeofMME-Financeandintegratemulti-turndialogue
scenariosformorecomprehensiveevaluation.
REFERENCES
[1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah,
Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3
technical report: A highly capable language model locally on your phone. arXiv preprint
arXiv:2404.14219,2024.
[2] 01.AI,:,AlexYoung,BeiChen,ChaoLi,ChengenHuang,GeZhang,GuanweiZhang,Heng
Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn
Yue,SenbinYang,ShimingYang,TaoYu,WenXie,WenhaoHuang,XiaohuiHu,XiaoyiRen,
Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu,
ZhiyuanLiu,andZonghongDai. Yi: Openfoundationmodelsby01.ai,2024.
[3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,
KarelLenc,ArthurMensch,KatherineMillican,MalcolmReynolds,etal. Flamingo: avisual
languagemodelforfew-shotlearning.Advancesinneuralinformationprocessingsystems,35:
23716–23736,2022.
10Preprint. UnderReview.
[4] StanislawAntol,AishwaryaAgrawal,JiasenLu,MargaretMitchell,DhruvBatra,CLawrence
Zitnick,andDeviParikh. Vqa: Visualquestionanswering. InProceedingsoftheIEEEinter-
nationalconferenceoncomputervision,pp.2425–2433,2015.
[5] Tom B Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165,
2020.
[6] FeilongChen,MinglunHan,HaozhiZhao,QingyangZhang,JingShi,ShuangXu,andBoXu.
X-llm: Bootstrappingadvancedlargelanguagemodelsbytreatingmulti-modalitiesasforeign
languages. arXivpreprintarXiv:2305.04160,2023.
[7] Pengcheng Chen, Jin Ye, Guoan Wang, Yanjun Li, Zhongying Deng, Wei Li, Tianbin Li,
Haodong Duan, Ziyan Huang, Yanzhou Su, et al. Gmai-mmbench: A comprehensive multi-
modal evaluation benchmark towards general medical ai. arXiv preprint arXiv:2408.03361,
2024.
[8] XinleiChen,HaoFang,Tsung-YiLin,RamakrishnaVedantam,SaurabhGupta,PiotrDolla´r,
andCLawrenceZitnick.Microsoftcococaptions:Datacollectionandevaluationserver.arXiv
preprintarXiv:1504.00325,2015.
[9] ZheChen,WeiyunWang,HaoTian,ShenglongYe,ZhangweiGao,ErfeiCui,WenwenTong,
Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to
commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821,
2024.
[10] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,
SiyuanZhuang,YonghaoZhuang,JosephEGonzalez,etal. Vicuna: Anopen-sourcechatbot
impressinggpt-4with90%*chatgptquality. Seehttps://vicuna.lmsys.org(accessed14April
2023),2(3):6,2023.
[11] Claude. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/
claude-3-5-sonnet.
[12] Yong Dai, Duyu Tang, Liangxin Liu, Minghuan Tan, Cong Zhou, Jingquan Wang, Zhangyin
Feng,FanZhang,XueyuHu,andShumingShi. Onemodel,multiplemodalities: Asparsely
activatedapproachfortext,sound,image,videoandcode. arXivpreprintarXiv:2205.06126,
2022.
[13] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Yang Jin-
rui, Xiawu Zheng, Ke Li, and Xing Sun. Mme: A comprehensive evaluation benchmark for
multimodallargelanguagemodels. arXivpreprintarXiv:2306.13394,2023.
[14] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang,
Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever com-
prehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint
arXiv:2405.21075,2024.
[15] ChaoyouFu,HaojiaLin,ZuweiLong,YunhangShen,MengZhao,YifanZhang,XiongWang,
DiYin,LongMa,XiawuZheng,etal.Vita:Towardsopen-sourceinteractiveomnimultimodal
llm. arXivpreprintarXiv:2408.05211,2024.
[16] YashGoyal,TejasKhot,DouglasSummers-Stay,DhruvBatra,andDeviParikh. Makingthe
v in vqa matter: Elevating the role of image understanding in visual question answering. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6904–
6913,2017.
[17] DannaGurari,QingLi,AbigaleJStangl,AnhongGuo,ChiLin,KristenGrauman,JieboLuo,
andJeffreyPBigham.Vizwizgrandchallenge:Answeringvisualquestionsfromblindpeople.
InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pp.3608–
3617,2018.
[18] WenyiHong,WeihanWang,MingDing,WenmengYu,QingsongLv,YanWang,YeanCheng,
Shiyu Huang, Junhui Ji, Zhao Xue, et al. Cogvlm2: Visual language models for image and
videounderstanding. arXivpreprintarXiv:2408.16500,2024.
11Preprint. UnderReview.
[19] DrewAHudsonandChristopherDManning. Gqa: Anewdatasetforreal-worldvisualrea-
soningandcompositionalquestionanswering. InProceedingsoftheIEEE/CVFconferenceon
computervisionandpatternrecognition,pp.6700–6709,2019.
[20] Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie
Vidgen. Financebench: A new benchmark for financial question answering. arXiv preprint
arXiv:2311.11944,2023.
[21] Yang Lei, Jiangtong Li, Ming Jiang, Junjie Hu, Dawei Cheng, Zhijun Ding, and Changjun
Jiang. Cfbenchmark: Chinesefinancialassistantbenchmarkforlargelanguagemodel. arXiv
preprintarXiv:2311.05812,2023.
[22] Bohao Li, , Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Bench-
markingmultimodal llmswithgenerative comprehension. arXivpreprint arXiv:2307.16125,
2023.
[23] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhao, and Ying
Shan. Seed-bench-2: Benchmarking multimodal large language models. arXiv preprint
arXiv:2311.17092,2023.
[24] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-
image pre-training with frozen image encoders and large language models. In International
conferenceonmachinelearning,pp.19730–19742.PMLR,2023.
[25] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual
instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and
PatternRecognition,pp.26296–26306,2024.
[26] HaotianLiu,ChunyuanLi,YuhengLi,BoLi,YuanhanZhang,ShengShen,andYongJaeLee.
Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. URL https:
//llava-vl.github.io/blog/2024-01-30-llava-next/.
[27] HaotianLiu, ChunyuanLi, QingyangWu, andYongJaeLee. Visualinstructiontuning. Ad-
vancesinneuralinformationprocessingsystems,36,2024.
[28] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,SongyangZhang,WangboZhao,YikeYuan,
JiaqiWang,ConghuiHe,ZiweiLiu,etal.Mmbench:Isyourmulti-modalmodelanall-around
player? arXivpreprintarXiv:2307.06281,2023.
[29] Meta. Llama3.2, 2024. URL https://www.llama.com/docs/
model-cards-and-prompt-formats/llama3_2.
[30] Open-AI. Gpt-4v(ision)systemcard,2023.
[31] Open-AI. Gpt-4o,2024. URLhttps://openai.com/index/hello-gpt-4o/.
[32] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,
ChongZhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal.Traininglanguagemodelsto
followinstructionswithhumanfeedback. Advancesinneuralinformationprocessingsystems,
35:27730–27744,2022.
[33] Tianwen Qian, Jingjing Chen, Linhai Zhuo, Yang Jiao, and Yu-Gang Jiang. Nuscenes-qa:
A multi-modal visual question answering benchmark for autonomous driving scenario. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 4542–4550,
2024.
[34] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Languagemodelsareunsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
[35] OleksiiSidorov,RonghangHu,MarcusRohrbach,andAmanpreetSingh. Textcaps: adataset
for image captioning with reading comprehension. In Computer Vision–ECCV 2020: 16th
EuropeanConference, Glasgow, UK,August23–28, 2020, Proceedings, PartII16, pp.742–
758.Springer,2020.
12Preprint. UnderReview.
[36] Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Ping
Luo,AndreasGeiger,andHongyangLi. Drivelm: Drivingwithgraphvisualquestionanswer-
ing. arXivpreprintarXiv:2312.14150,2023.
[37] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui
Yu,RaduSoricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamilyof
highlycapablemultimodalmodels. arXivpreprintarXiv:2312.11805,2023.
[38] Qwenteam. Qwen2-vl. 2024.
[39] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Tim-
othe´eLacroix,BaptisteRozie`re,NamanGoyal,EricHambro,FaisalAzhar,etal.Llama:Open
andefficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.
[40] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neu-
ral image caption generator. In Proceedings of the IEEE conference on computer vision and
patternrecognition,pp.3156–3164,2015.
[41] WeihanWang,QingsongLv,WenmengYu,WenyiHong,JiQi,YanWang,JunhuiJi,Zhuoyi
Yang, LeiZhao, XixuanSong, etal. Cogvlm: Visualexpertforpretrainedlanguagemodels.
arXivpreprintarXiv:2311.03079,2023.
[42] Wenxuan Wang, Yihang Su, Jingyuan Huan, Jie Liu, Wenting Chen, Yudi Zhang, Cheng-Yi
Li, Kao-Jung Chang, Xiaohan Xin, Linlin Shen, et al. Asclepius: A spectrum evaluation
benchmarkformedicalmulti-modallargelanguagemodels. arXivpreprintarXiv:2402.11217,
2024.
[43] Zhengyuan Yang, Yijuan Lu, Jianfeng Wang, Xi Yin, Dinei Florencio, Lijuan Wang, Cha
Zhang,LeiZhang,andJieboLuo. Tap: Text-awarepre-trainingfortext-vqaandtext-caption.
InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pp.
8751–8761,2021.
[44] YuanYao,TianyuYu,AoZhang,ChongyiWang,JunboCui,HongjiZhu,TianchiCai,Haoyu
Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: A gpt-4v level mllm on your phone. arXiv
preprintarXiv:2408.01800,2024.
[45] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A
surveyonmultimodallargelanguagemodels. arXivpreprintarXiv:2306.13549,2023.
[46] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen,
Ke Li, Xing Sun, and Enhong Chen. Woodpecker: Hallucination correction for multimodal
largelanguagemodels. arXivpreprintarXiv:2310.16045,2023.
[47] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions
to visual denotations: New similarity metrics for semantic inference over event descriptions.
TransactionsoftheAssociationforComputationalLinguistics,2:67–78,2014.
[48] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao
Wang,andLijuanWang. Mm-vet: Evaluatinglargemultimodalmodelsforintegratedcapabil-
ities. arXivpreprintarXiv:2308.02490,2023.
[49] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition:
Visual commonsense reasoning. In Proceedings of the IEEE/CVF conference on computer
visionandpatternrecognition,pp.6720–6731,2019.
[50] HangZhang, Xin Li, andLidongBing. Video-llama: Aninstruction-tunedaudio-visual lan-
guagemodelforvideounderstanding. arXivpreprintarXiv:2306.02858,2023.
[51] Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu,
FengLi,KunWang,QingsongWen,ZhangZhang,etal. Mme-realworld: Couldyourmulti-
modalllmchallengehigh-resolutionreal-worldscenariosthataredifficultforhumans? arXiv
preprintarXiv:2408.13257,2024.
[52] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En-
hancingvision-languageunderstandingwithadvancedlargelanguagemodels. arXivpreprint
arXiv:2304.10592,2023.
13Preprint. UnderReview.
A APPENDIX
Statement:Thispaperislimitedtoacademicresearch,andtheOpenAI’sproductsuseddonotviolate
thecompany’scommercialregulations.
Inthisappendix,weprovidefurtherdetailsregardingtheproposedMME-Finance. A.1introduces
thestatisticaloverviewandtheexperimentalresultsoftheChineseversionofMME-Finance. A.2
presentsexperimentalresultswiththeprompttoallow“NotApplicable”responseacrossalltypes
oftasks. A.3providessomesamplesdemonstratingthedifficultyofrecognizingmobilephotosand
thehallucinationproblemsofMLLMs. A.4includesthedetailedinferenceandevaluationprompts.
A.5exhibitsexampleforeachtask.
A.1 CHINESEVERSIONOFMME-FINANCE.
(a) (b)
Figure7:Distributionofdifferent(a)typesand(b)stylesofimagesoftheChineseversionofMME-
Finance.
As shown in Table 6, Chinese MME-Finance Table6: Statisticofthenumberofsamplesindif-
contains 1,103 image-question-answer pairs, ferentcapabilitiesandtasksoftheChineseversion
andhasthesametaskcategoriesastheEnglish ofMME-Finance.
version. Thenumberofsamplespertaskvaries
from13to182,withthe“OCR”taskcontaining Statistic Number
themostand“ReasonExplaination”thefewest. Perception 640
Thedistributionofthe6imagetypesand4im- -ImageCaption 144
agestylesareshownintheFigure7(a)andFig- -OCR 182
ure7(b),respectively.StatisticsChartshavethe -EntityRecognition 148
largest proportion, while mixed charts are the -SpatialAwareness 166
least. As for the styles, photography has the Reasoning 158
largestnumber. -AccurateNumericalCalculation 126
-EstimatedNumericalCalculation 32
Table7showstheresultsofvariousopensource
Cognition 285
MLLMs on Chinese MME-Finance in every
-RiskWarning 37
task with Qwen2VL-72B evaluator. Among
-InvestmentAdvice 91
thesemodels,Qwen2VL-72Bachievesthebest
-ReasonExplanation 13
overallperformancewith73.35%accuracy,ex-
-FinancialQuestionAnswer 144
celling in most tasks. The performance of
Hallucination 20
Qwen2VL-7B is slightly lower, and the third
-NotApplicable 20
is InternVL2-76B. The law of larger language
modelhasabetterperformanceisalsoapplica-
ble for the InternVL and the Qwen2VL series.
TheevaluationresultswithQwen2VL-72BevaluatoronChineseMME-Financefordifferenttypes
andstylesofimagesareshowninTable8. Qwen2VL-72Bachievesthebestresultsinallcategories,
followedbyQwen2VL-7BandInternVL2-76B.Itisobviouslythatthesemodelshavelowestaccu-
racyinthetypeofcandlestickchart. Asthestylesofimages,themobilephotographismostdifficult
formostMLLMs.
A.2 RESULTSOFNAPROMPTFORALLTASKS.
Table9andTable10showstheperformanceofMLLMswiththeprompttoallow“NotApplicable”
response across all types of tasks. It is clear that most models have a lower performance in the
14Preprint. UnderReview.
Table 7: Evaluation results on the Chinese MME-Finance for all tasks. Abbreviations and color
settingsarethesameasbefore. Allnumbersaredenotedin%withthemaxvalueof100%.
Perception Reasoning Cognition
Model Overall NA
IC OCR ER SA ANC ENC RW IA RE FQA
OpensourceMLLMs
Yi-VL-34B 23.50 43.89 0.66 9.86 4.94 23.97 18.13 20.00 28.79 60.00 51.81 100.0
CogVLM2-19B 35.32 55.69 41.10 37.84 16.02 39.37 29.38 8.11 31.43 26.15 28.47 85.00
InternVL2-2B 39.76 60.83 54.51 33.24 16.02 28.25 19.38 32.43 44.40 55.38 44.86 50.00
InternVL2-4B 45.78 67.22 60.22 43.51 19.28 51.43 38.75 31.89 46.59 63.08 36.94 47.00
InternVL2-8B 58.44 73.47 76.92 55.14 25.18 52.84 42.50 53.51 61.32 76.92 67.78 60.00
InternVL2-76B 62.63 73.47 75.71 61.35 38.43 64.13 53.13 58.38 63.08 75.38 67.36 45.00
LLaVA-NEXT-7B 21.45 50.69 8.35 12.16 9.28 16.03 13.13 12.43 28.35 46.15 25.14 90.00
LLaVA-NEXT-13B 19.87 49.58 8.68 12.30 13.01 14.60 9.38 8.11 24.84 13.85 17.64 90.00
MiniCPM2.6 38.60 53.47 64.29 45.27 23.98 18.41 27.50 32.43 36.70 35.38 27.92 14.00
Phi3-Vision 31.91 57.92 32.31 40.68 16.02 29.05 23.13 22.70 32.31 43.08 14.31 75.00
Phi3.5-Vision 30.12 55.97 19.45 20.27 23.85 20.48 24.38 28.65 41.98 41.54 26.94 100.0
Qwen2VL-2B 49.12 65.97 63.41 48.38 24.94 39.05 36.88 36.76 46.37 56.92 51.53 100.0
Qwen2VL-7B 64.91 73.61 84.95 64.05 34.34 69.68 58.13 55.14 59.34 67.69 65.97 95.00
Qwen2VL-72B 73.35 79.58 89.67 73.24 55.90 73.81 73.13 69.19 65.05 76.92 74.17 60.00
Table 8: Evaluation results on Chinese MME-Finance for different types and styles of images.
Abbreviations and color settings are the same as before. All numbers are denoted in % with the
maxvalueof100%.
Model Candle. Tech. Stat. Tab. Doc. Mixed CS MP VS HS
OpensourceMLLMs
Yi-VL-34B 26.00 21.66 22.87 23.55 23.63 24.52 26.30 20.49 25.09 22.22
CogVLM2-19B 38.86 33.59 31.19 37.74 37.48 37.48 37.11 32.50 36.77 39.17
InternVL2-2B 36.29 35.80 39.01 37.90 48.22 32.26 43.70 36.92 39.63 34.72
InternVL2-4B 34.43 44.20 46.93 48.31 51.56 37.10 51.90 40.80 43.72 45.56
InternVL2-8B 49.71 55.69 56.44 59.60 66.44 53.23 61.80 55.31 62.11 50.00
InternVL2-76B 55.86 64.75 61.39 62.74 67.56 53.87 65.55 57.28 69.44 63.61
LLaVA-NEXT-7B 29.57 20.88 20.00 16.21 25.33 13.55 22.89 19.42 23.23 21.67
LLaVA-NEXT-13B 24.14 21.66 21.39 16.37 19.56 15.48 20.33 19.29 19.88 20.83
MiniCPM2.6 36.57 36.69 37.92 40.08 44.81 18.06 38.58 35.80 47.33 36.67
Phi3-Vision 31.71 35.47 29.21 31.45 34.30 22.26 36.45 27.32 31.55 34.72
Phi3.5-Vision 30.29 35.03 27.92 25.48 33.33 27.10 32.09 27.54 29.81 35.28
Qwen2VL-2B 40.71 42.10 49.11 49.68 59.41 41.61 49.95 48.88 51.80 39.72
Qwen2VL-7B 55.71 60.55 69.21 66.13 68.37 64.52 69.29 62.41 62.73 59.72
Qwen2VL-72B 64.14 71.71 77.52 75.65 75.26 67.74 76.35 69.96 74.53 74.17
setting,whichmeansthehallucinationproblemisquitecommoninMLLMs.Althoughsomemodels
haveahighrecallofthe“NotApplicable”question,theiroverallaccuracyislow. Itshowsthatthese
modelstendtoanswer“NotApplicable”forsomeunsurequestions.
A.3 HARDEXAMPLES
In this section, we present some hard examples about the difficulty of mobile photos and halluci-
nationproblemsofMLLMs. AsshowninFigure8,thetwoquestionshavesimilarcontent. When
feedingthetwoimagesintothesamemodel,theresponsesaredifferent. Forthepicturetakenwith
a mobile phone, the model mistakenly identifies decimal points as commas and the letter B as the
number8. Andthemodelaccuratelyidentifiescorrespondingelementsinthecomputerscreenshot.
15Preprint. UnderReview.
Table9: EvaluationresultsonMME-Financewiththeprompttoallow“NotApplicable”response
acrossalltypesoftasks. Abbreviationsandcolorsettingsarethesameasbefore. Allnumbersare
denotedin%withthemaxvalueof100%.
Perception Reasoning Cognition
Model Overall NA
IC OCR ER SA ANC ENC RW IA RE FQA
OpensourceMLLMs
CogVLM2-19B 31.24 36.22 42.02 26.99 7.95 34.14 19.52 13.64 38.11 32.22 48.44 70.91
InternVL2-2B 32.16 61.22 32.81 12.76 4.72 32.78 21.43 54.55 47.92 63.33 50.20 50.00
InternVL2-4B 45.93 68.05 54.27 28.22 18.69 54.74 32.38 68.18 50.19 68.89 59.46 59.09
InternVL2-8B 50.59 70.00 60.11 33.99 23.84 62.56 40.00 76.36 60.75 75.56 58.37 55.45
LLaVA-NEXT-7B 20.10 58.05 6.18 2.70 1.31 6.62 3.33 43.64 37.74 16.67 43.27 70.00
MiniCPM2.6 48.37 69.63 62.81 34.85 21.31 54.89 28.57 50.00 40.38 45.56 62.31 80.00
Phi3-Vision 37.06 69.51 58.43 29.57 11.88 22.11 3.81 7.27 16.60 41.11 47.76 98.18
Phi3.5-Vision 28.69 66.83 33.03 13.87 8.12 17.14 6.19 2.73 15.47 22.22 45.58 96.36
Qwen2VL-7B 32.40 62.32 40.00 8.10 7.07 41.80 22.86 33.64 40.00 10.00 40.14 100.0
Qwen2VL-2B 32.47 62.32 40.45 9.08 6.72 41.80 25.24 31.82 36.23 13.33 40.14 100.0
Qwen2VL-72B 62.97 80.49 83.26 50.43 25.50 78.95 46.67 73.64 68.30 73.33 73.06 86.36
ProprietaryMLLMs
GPT-4o-5-13 42.12 72.07 26.74 26.99 19.56 29.17 38.57 70.90 63.77 76.67 71.02 72.72
GPT-4o-mini 41.32 63.54 58.54 28.34 14.32 24.06 6.19 52.73 30.19 63.33 68.57 100.0
GPT-4o 61.35 83.66 78.54 46.38 28.73 71.43 40.00 80.00 65.66 77.78 70.88 80.00
Table 10: Evaluation results on MME-Finance for different types and formats of images with the
prompttoallow“NotApplicable”responseacrossalltypesoftasks. Abbreviationsandcolorset-
tingsarethesameasbefore. Allnumbersaredenotedin%withthemaxvalueof100%.
Model Candle. Tech. Stat. Tab. Doc. Mixed CS MP VS HS
OpensourceMLLMs
CogVLM2-19B 25.31 19.89 33.13 32.52 37.53 39.05 31.70 29.66 33.33 35.84
InternVL2-2B 25.03 25.57 36.64 32.37 33.88 53.33 36.21 28.82 25.76 29.09
InternVL2-4B 30.91 34.43 54.35 45.47 52.37 56.19 49.68 43.63 39.39 38.70
InternVL2-8B 39.02 38.18 55.27 50.72 58.08 69.52 54.98 47.13 49.70 41.04
LLaVA-NEXT-7B 16.08 18.75 22.82 19.31 19.93 33.33 22.42 16.71 22.12 22.60
MiniCPM2.6 39.86 44.66 51.53 48.85 52.23 38.10 47.26 48.48 48.48 55.58
Phi3-Vision 24.62 28.18 42.82 40.00 42.16 12.38 38.66 35.86 29.70 39.22
Phi3.5-Vision 20.98 22.95 30.23 30.29 34.36 10.48 32.92 22.95 31.82 30.91
Qwen2VL-2B 22.80 24.32 35.34 37.63 33.20 48.57 33.86 30.42 29.39 36.62
Qwen2VL-7B 23.92 23.86 35.57 37.91 33.06 43.81 33.54 31.18 29.39 35.32
Qwen2VL-72B 52.31 56.14 69.16 64.32 64.67 74.29 65.78 59.41 63.94 63.90
ProprietaryMLLMs
GPT-4o-5-13 39.86 33.86 53.13 36.72 40.43 63.81 45.67 36.58 49.39 44.41
GPT-4o-mini 26.15 26.02 49.90 46.87 43.88 50.48 45.99 36.50 35.76 42.08
GPT-4o 52.45 51.93 67.70 68.79 57.12 76.19 64.77 55.82 66.67 66.23
ThisindicatesthattheperceptionofmobilephonephotosisachallengeforsomeMLLMs. Figure9
illustrates a example of the hallucination problem. GPT-4o cannot recognize the initial increase
trend,whileQwen2VL-72Btotallyunabletoperceivetrends.
A.4 INFERENCEANDEVALUATIONPROMPT.
Figure10andFigure11showsthedetailedinferencepromptandevaluationprompt.
16Preprint. UnderReview.
Figure8:Comparisonofthedifficultyofrecognizingcomputerscreenshotversusphotostakenwith
amobilephone.
Figure9: ThedisplayofhallucinationproblemsofMLLMs.
A.5 DEFINITIONANDEXAMPLEFOREACHTASK.
Inthissection,weprovideadetaileddefinitionofeachtaskandpresentcorrespondingexamplesto
helpreaderlearnaboutthesetasks.
Perception
1. ImageCaption: Generateatextualdescriptionthataccuratelyrepresentsthecontent,con-
text,andsignificantelementsofanimage.
2. OCR:Recognitionoftext,numberintheimage.
3. Entity Recognition: Recognition and understanding of visual elements(such as color,
shape)ornamedentityintheimage.
4. SpatialAwareness: Understandthepositionandspatialrelationshipoftheelementsinthe
image.
Reasoning
1. Accurate Numerical Calculation: Perform accurate numerical calculation or numerical
comparisonbasedonthenumberintheimage.
2. Estimated Numerical Calculation: Obtain approximate values based on relevant
clues(suchasspatiallocation)andperformnumericalcalculation.
Cognition
17Preprint. UnderReview.
Figure10: Inferenceprompt.
18Preprint. UnderReview.
Figure11: Evaluationprompt.
19Preprint. UnderReview.
1. RiskWarning: Giveaninvestmentriskdescriptionbasedontheinformationintheimage
(andbackgroundinformation).
2. InvestmentAdvice:Giveaninvestmentadvicebasedontheinformationintheimage(and
backgroundinformation).
3. ExplainReason: Giveanreasonforthe phenomenonindicatedinthequestionbasedon
theinformationintheimage(andbackgroundinformation).
4. Financial Question Answer: Answer objective financial questions based on the scored
generalfinancialknowledge.
20Preprint. UnderReview.
Figure12: ImageCaption.
Figure13: OCR.
21Preprint. UnderReview.
Figure14: EntityRecognition.
Figure15: SpatialAwareness.
Figure16: AccurateNumericalCalculation.
22Preprint. UnderReview.
Figure17: EstimatedNumericalCalculation.
Figure18: RiskWarning.
23Preprint. UnderReview.
Figure19: InvestmentAdvice.
Figure20: ReasonExplanation.
24Preprint. UnderReview.
Figure21: FinancialQuestionAnswer.
Figure22: NotApplicable.
25