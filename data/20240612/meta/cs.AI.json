[
    {
        "title": "Commonsense-T2I Challenge: Can Text-to-Image Generation Models Understand Commonsense?",
        "authors": "Xingyu FuMuyu HeYujie LuWilliam Yang WangDan Roth",
        "links": "http://arxiv.org/abs/2406.07546v1",
        "entry_id": "http://arxiv.org/abs/2406.07546v1",
        "pdf_url": "http://arxiv.org/pdf/2406.07546v1",
        "summary": "We present a novel task and benchmark for evaluating the ability of\ntext-to-image(T2I) generation models to produce images that fit commonsense in\nreal life, which we call Commonsense-T2I. Given two adversarial text prompts\ncontaining an identical set of action words with minor differences, such as \"a\nlightbulb without electricity\" v.s. \"a lightbulb with electricity\", we evaluate\nwhether T2I models can conduct visual-commonsense reasoning, e.g. produce\nimages that fit \"the lightbulb is unlit\" vs. \"the lightbulb is lit\"\ncorrespondingly. Commonsense-T2I presents an adversarial challenge, providing\npairwise text prompts along with expected outputs. The dataset is carefully\nhand-curated by experts and annotated with fine-grained labels, such as\ncommonsense type and likelihood of the expected outputs, to assist analyzing\nmodel behavior. We benchmark a variety of state-of-the-art (sota) T2I models\nand surprisingly find that, there is still a large gap between image synthesis\nand real life photos--even the DALL-E 3 model could only achieve 48.92% on\nCommonsense-T2I, and the stable diffusion XL model only achieves 24.92%\naccuracy. Our experiments show that GPT-enriched prompts cannot solve this\nchallenge, and we include a detailed analysis about possible reasons for such\ndeficiency. We aim for Commonsense-T2I to serve as a high-quality evaluation\nbenchmark for T2I commonsense checking, fostering advancements in real life\nimage generation.",
        "updated": "2024-06-11 17:59:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.07546v1"
    },
    {
        "title": "Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena",
        "authors": "Aidar MyrzakhanSondos Mahmoud BsharatZhiqiang Shen",
        "links": "http://arxiv.org/abs/2406.07545v1",
        "entry_id": "http://arxiv.org/abs/2406.07545v1",
        "pdf_url": "http://arxiv.org/pdf/2406.07545v1",
        "summary": "Multiple-choice questions (MCQ) are frequently used to assess large language\nmodels (LLMs). Typically, an LLM is given a question and selects the answer\ndeemed most probable after adjustments for factors like length. Unfortunately,\nLLMs may inherently favor certain answer choice IDs, such as A/B/C/D, due to\ninherent biases of priori unbalanced probabilities, influencing the prediction\nof answers based on these IDs. Previous research has introduced methods to\nreduce this ''selection bias'' by simply permutating options on a few test\nsamples and applying to new ones. Another problem of MCQ is the lottery ticket\nchoice by ''random guessing''. The LLM does not learn particular knowledge, but\nthe option is guessed correctly. This situation is especially serious for those\nsmall-scale LLMs. To address them, a more thorough approach involves shifting\nfrom MCQ to open-style questions, which can fundamentally eliminate selection\nbias and random guessing issues. However, transitioning causes its own set of\nchallenges in (1) identifying suitable open-style questions and (2) validating\nthe correctness of LLM open-style responses against human-annotated\nground-truths. This work aims to tackle these significant difficulties, and\nestablish a new LLM evaluation benchmark through entirely open-style questions.\nConsequently, we introduce the Open-LLM-Leaderboard to track various LLMs'\nperformance and reflect true capability of them, such as GPT-4o/4/3.5, Claude\n3, Gemini, etc. Our code and dataset are available at\nhttps://github.com/VILA-Lab/Open-LLM-Leaderboard.",
        "updated": "2024-06-11 17:59:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.07545v1"
    },
    {
        "title": "Situational Awareness Matters in 3D Vision Language Reasoning",
        "authors": "Yunze ManLiang-Yan GuiYu-Xiong Wang",
        "links": "http://arxiv.org/abs/2406.07544v1",
        "entry_id": "http://arxiv.org/abs/2406.07544v1",
        "pdf_url": "http://arxiv.org/pdf/2406.07544v1",
        "summary": "Being able to carry out complicated vision language reasoning tasks in 3D\nspace represents a significant milestone in developing household robots and\nhuman-centered embodied AI. In this work, we demonstrate that a critical and\ndistinct challenge in 3D vision language reasoning is situational awareness,\nwhich incorporates two key components: (1) The autonomous agent grounds its\nself-location based on a language prompt. (2) The agent answers open-ended\nquestions from the perspective of its calculated position. To address this\nchallenge, we introduce SIG3D, an end-to-end Situation-Grounded model for 3D\nvision language reasoning. We tokenize the 3D scene into sparse voxel\nrepresentation and propose a language-grounded situation estimator, followed by\na situated question answering module. Experiments on the SQA3D and ScanQA\ndatasets show that SIG3D outperforms state-of-the-art models in situation\nestimation and question answering by a large margin (e.g., an enhancement of\nover 30% on situation estimation accuracy). Subsequent analysis corroborates\nour architectural design choices, explores the distinct functions of visual and\ntextual tokens, and highlights the importance of situational awareness in the\ndomain of 3D question answering.",
        "updated": "2024-06-11 17:59:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.07544v1"
    },
    {
        "title": "Cognitive Insights Across Languages: Enhancing Multimodal Interview Analysis",
        "authors": "David Ortiz-PerezJose Garcia-RodriguezDavid Tomás",
        "links": "http://arxiv.org/abs/2406.07542v1",
        "entry_id": "http://arxiv.org/abs/2406.07542v1",
        "pdf_url": "http://arxiv.org/pdf/2406.07542v1",
        "summary": "Cognitive decline is a natural process that occurs as individuals age. Early\ndiagnosis of anomalous decline is crucial for initiating professional treatment\nthat can enhance the quality of life of those affected. To address this issue,\nwe propose a multimodal model capable of predicting Mild Cognitive Impairment\nand cognitive scores. The TAUKADIAL dataset is used to conduct the evaluation,\nwhich comprises audio recordings of clinical interviews. The proposed model\ndemonstrates the ability to transcribe and differentiate between languages used\nin the interviews. Subsequently, the model extracts audio and text features,\ncombining them into a multimodal architecture to achieve robust and generalized\nresults. Our approach involves in-depth research to implement various features\nobtained from the proposed modalities.",
        "updated": "2024-06-11 17:59:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.07542v1"
    },
    {
        "title": "Simple and Effective Masked Diffusion Language Models",
        "authors": "Subham Sekhar SahooMarianne ArriolaYair SchiffAaron GokaslanEdgar MarroquinJustin T ChiuAlexander RushVolodymyr Kuleshov",
        "links": "http://arxiv.org/abs/2406.07524v1",
        "entry_id": "http://arxiv.org/abs/2406.07524v1",
        "pdf_url": "http://arxiv.org/pdf/2406.07524v1",
        "summary": "While diffusion models excel at generating high-quality images, prior work\nreports a significant performance gap between diffusion and autoregressive (AR)\nmethods in language modeling. In this work, we show that simple masked discrete\ndiffusion is more performant than previously thought. We apply an effective\ntraining recipe that improves the performance of masked diffusion models and\nderive a simplified, Rao-Blackwellized objective that results in additional\nimprovements. Our objective has a simple form -- it is a mixture of classical\nmasked language modeling losses -- and can be used to train encoder-only\nlanguage models that admit efficient samplers, including ones that can generate\narbitrary lengths of text semi-autoregressively like a traditional language\nmodel. On language modeling benchmarks, a range of masked diffusion models\ntrained with modern engineering practices achieves a new state-of-the-art among\ndiffusion models, and approaches AR perplexity. We release our code at:\nhttps://github.com/kuleshov-group/mdlm",
        "updated": "2024-06-11 17:51:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.07524v1"
    }
]