[
    {
        "title": "Commonsense-T2I Challenge: Can Text-to-Image Generation Models Understand Commonsense?",
        "authors": "Xingyu FuMuyu HeYujie LuWilliam Yang WangDan Roth",
        "links": "http://arxiv.org/abs/2406.07546v1",
        "entry_id": "http://arxiv.org/abs/2406.07546v1",
        "pdf_url": "http://arxiv.org/pdf/2406.07546v1",
        "summary": "We present a novel task and benchmark for evaluating the ability of\ntext-to-image(T2I) generation models to produce images that fit commonsense in\nreal life, which we call Commonsense-T2I. Given two adversarial text prompts\ncontaining an identical set of action words with minor differences, such as \"a\nlightbulb without electricity\" v.s. \"a lightbulb with electricity\", we evaluate\nwhether T2I models can conduct visual-commonsense reasoning, e.g. produce\nimages that fit \"the lightbulb is unlit\" vs. \"the lightbulb is lit\"\ncorrespondingly. Commonsense-T2I presents an adversarial challenge, providing\npairwise text prompts along with expected outputs. The dataset is carefully\nhand-curated by experts and annotated with fine-grained labels, such as\ncommonsense type and likelihood of the expected outputs, to assist analyzing\nmodel behavior. We benchmark a variety of state-of-the-art (sota) T2I models\nand surprisingly find that, there is still a large gap between image synthesis\nand real life photos--even the DALL-E 3 model could only achieve 48.92% on\nCommonsense-T2I, and the stable diffusion XL model only achieves 24.92%\naccuracy. Our experiments show that GPT-enriched prompts cannot solve this\nchallenge, and we include a detailed analysis about possible reasons for such\ndeficiency. We aim for Commonsense-T2I to serve as a high-quality evaluation\nbenchmark for T2I commonsense checking, fostering advancements in real life\nimage generation.",
        "updated": "2024-06-11 17:59:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.07546v1"
    },
    {
        "title": "Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena",
        "authors": "Aidar MyrzakhanSondos Mahmoud BsharatZhiqiang Shen",
        "links": "http://arxiv.org/abs/2406.07545v1",
        "entry_id": "http://arxiv.org/abs/2406.07545v1",
        "pdf_url": "http://arxiv.org/pdf/2406.07545v1",
        "summary": "Multiple-choice questions (MCQ) are frequently used to assess large language\nmodels (LLMs). Typically, an LLM is given a question and selects the answer\ndeemed most probable after adjustments for factors like length. Unfortunately,\nLLMs may inherently favor certain answer choice IDs, such as A/B/C/D, due to\ninherent biases of priori unbalanced probabilities, influencing the prediction\nof answers based on these IDs. Previous research has introduced methods to\nreduce this ''selection bias'' by simply permutating options on a few test\nsamples and applying to new ones. Another problem of MCQ is the lottery ticket\nchoice by ''random guessing''. The LLM does not learn particular knowledge, but\nthe option is guessed correctly. This situation is especially serious for those\nsmall-scale LLMs. To address them, a more thorough approach involves shifting\nfrom MCQ to open-style questions, which can fundamentally eliminate selection\nbias and random guessing issues. However, transitioning causes its own set of\nchallenges in (1) identifying suitable open-style questions and (2) validating\nthe correctness of LLM open-style responses against human-annotated\nground-truths. This work aims to tackle these significant difficulties, and\nestablish a new LLM evaluation benchmark through entirely open-style questions.\nConsequently, we introduce the Open-LLM-Leaderboard to track various LLMs'\nperformance and reflect true capability of them, such as GPT-4o/4/3.5, Claude\n3, Gemini, etc. Our code and dataset are available at\nhttps://github.com/VILA-Lab/Open-LLM-Leaderboard.",
        "updated": "2024-06-11 17:59:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.07545v1"
    },
    {
        "title": "Situational Awareness Matters in 3D Vision Language Reasoning",
        "authors": "Yunze ManLiang-Yan GuiYu-Xiong Wang",
        "links": "http://arxiv.org/abs/2406.07544v1",
        "entry_id": "http://arxiv.org/abs/2406.07544v1",
        "pdf_url": "http://arxiv.org/pdf/2406.07544v1",
        "summary": "Being able to carry out complicated vision language reasoning tasks in 3D\nspace represents a significant milestone in developing household robots and\nhuman-centered embodied AI. In this work, we demonstrate that a critical and\ndistinct challenge in 3D vision language reasoning is situational awareness,\nwhich incorporates two key components: (1) The autonomous agent grounds its\nself-location based on a language prompt. (2) The agent answers open-ended\nquestions from the perspective of its calculated position. To address this\nchallenge, we introduce SIG3D, an end-to-end Situation-Grounded model for 3D\nvision language reasoning. We tokenize the 3D scene into sparse voxel\nrepresentation and propose a language-grounded situation estimator, followed by\na situated question answering module. Experiments on the SQA3D and ScanQA\ndatasets show that SIG3D outperforms state-of-the-art models in situation\nestimation and question answering by a large margin (e.g., an enhancement of\nover 30% on situation estimation accuracy). Subsequent analysis corroborates\nour architectural design choices, explores the distinct functions of visual and\ntextual tokens, and highlights the importance of situational awareness in the\ndomain of 3D question answering.",
        "updated": "2024-06-11 17:59:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.07544v1"
    },
    {
        "title": "Simple and Effective Masked Diffusion Language Models",
        "authors": "Subham Sekhar SahooMarianne ArriolaYair SchiffAaron GokaslanEdgar MarroquinJustin T ChiuAlexander RushVolodymyr Kuleshov",
        "links": "http://arxiv.org/abs/2406.07524v1",
        "entry_id": "http://arxiv.org/abs/2406.07524v1",
        "pdf_url": "http://arxiv.org/pdf/2406.07524v1",
        "summary": "While diffusion models excel at generating high-quality images, prior work\nreports a significant performance gap between diffusion and autoregressive (AR)\nmethods in language modeling. In this work, we show that simple masked discrete\ndiffusion is more performant than previously thought. We apply an effective\ntraining recipe that improves the performance of masked diffusion models and\nderive a simplified, Rao-Blackwellized objective that results in additional\nimprovements. Our objective has a simple form -- it is a mixture of classical\nmasked language modeling losses -- and can be used to train encoder-only\nlanguage models that admit efficient samplers, including ones that can generate\narbitrary lengths of text semi-autoregressively like a traditional language\nmodel. On language modeling benchmarks, a range of masked diffusion models\ntrained with modern engineering practices achieves a new state-of-the-art among\ndiffusion models, and approaches AR perplexity. We release our code at:\nhttps://github.com/kuleshov-group/mdlm",
        "updated": "2024-06-11 17:51:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.07524v1"
    },
    {
        "title": "Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling",
        "authors": "Liliang RenYang LiuYadong LuYelong ShenChen LiangWeizhu Chen",
        "links": "http://arxiv.org/abs/2406.07522v1",
        "entry_id": "http://arxiv.org/abs/2406.07522v1",
        "pdf_url": "http://arxiv.org/pdf/2406.07522v1",
        "summary": "Efficiently modeling sequences with infinite context length has been a\nlong-standing problem. Past works suffer from either the quadratic computation\ncomplexity or the limited extrapolation ability on length generalization. In\nthis work, we present Samba, a simple hybrid architecture that layer-wise\ncombines Mamba, a selective State Space Model (SSM), with Sliding Window\nAttention (SWA). Samba selectively compresses a given sequence into recurrent\nhidden states while still maintaining the ability to precisely recall memories\nwith the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T\ntraining tokens and show that Samba substantially outperforms the\nstate-of-the-art models based on pure attention or SSMs on a wide range of\nbenchmarks. When trained on 4K length sequences, Samba can be efficiently\nextrapolated to 256K context length with perfect memory recall and show\nimproved token predictions up to 1M context length. As a linear-time sequence\nmodel, Samba enjoys a 3.73x higher throughput compared to Transformers with\ngrouped-query attention when processing user prompts of 128K length, and 3.64x\nspeedup when generating 64K tokens with unlimited streaming. A sample\nimplementation of Samba is publicly available in\nhttps://github.com/microsoft/Samba.",
        "updated": "2024-06-11 17:50:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.07522v1"
    }
]