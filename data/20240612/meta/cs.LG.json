[
    {
        "title": "Image and Video Tokenization with Binary Spherical Quantization",
        "authors": "Yue ZhaoYuanjun XiongPhilipp Krähenbühl",
        "links": "http://arxiv.org/abs/2406.07548v1",
        "entry_id": "http://arxiv.org/abs/2406.07548v1",
        "pdf_url": "http://arxiv.org/pdf/2406.07548v1",
        "summary": "We propose a new transformer-based image and video tokenizer with Binary\nSpherical Quantization (BSQ). BSQ projects the high-dimensional visual\nembedding to a lower-dimensional hypersphere and then applies binary\nquantization. BSQ is (1) parameter-efficient without an explicit codebook, (2)\nscalable to arbitrary token dimensions, and (3) compact: compressing visual\ndata by up to 100$\\times$ with minimal distortion. Our tokenizer uses a\ntransformer encoder and decoder with simple block-wise causal masking to\nsupport variable-length videos as input. The resulting BSQ-ViT achieves\nstate-of-the-art visual reconstruction quality on image and video\nreconstruction benchmarks with 2.4$\\times$ throughput compared to the best\nprior methods. Furthermore, by learning an autoregressive prior for adaptive\narithmetic coding, BSQ-ViT achieves comparable results on video compression\nwith state-of-the-art video compression standards. BSQ-ViT also enables masked\nlanguage models to achieve competitive image synthesis quality to GAN- and\ndiffusion-based methods.",
        "updated": "2024-06-11 17:59:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.07548v1"
    },
    {
        "title": "Situational Awareness Matters in 3D Vision Language Reasoning",
        "authors": "Yunze ManLiang-Yan GuiYu-Xiong Wang",
        "links": "http://arxiv.org/abs/2406.07544v1",
        "entry_id": "http://arxiv.org/abs/2406.07544v1",
        "pdf_url": "http://arxiv.org/pdf/2406.07544v1",
        "summary": "Being able to carry out complicated vision language reasoning tasks in 3D\nspace represents a significant milestone in developing household robots and\nhuman-centered embodied AI. In this work, we demonstrate that a critical and\ndistinct challenge in 3D vision language reasoning is situational awareness,\nwhich incorporates two key components: (1) The autonomous agent grounds its\nself-location based on a language prompt. (2) The agent answers open-ended\nquestions from the perspective of its calculated position. To address this\nchallenge, we introduce SIG3D, an end-to-end Situation-Grounded model for 3D\nvision language reasoning. We tokenize the 3D scene into sparse voxel\nrepresentation and propose a language-grounded situation estimator, followed by\na situated question answering module. Experiments on the SQA3D and ScanQA\ndatasets show that SIG3D outperforms state-of-the-art models in situation\nestimation and question answering by a large margin (e.g., an enhancement of\nover 30% on situation estimation accuracy). Subsequent analysis corroborates\nour architectural design choices, explores the distinct functions of visual and\ntextual tokens, and highlights the importance of situational awareness in the\ndomain of 3D question answering.",
        "updated": "2024-06-11 17:59:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.07544v1"
    },
    {
        "title": "Cognitive Insights Across Languages: Enhancing Multimodal Interview Analysis",
        "authors": "David Ortiz-PerezJose Garcia-RodriguezDavid Tomás",
        "links": "http://arxiv.org/abs/2406.07542v1",
        "entry_id": "http://arxiv.org/abs/2406.07542v1",
        "pdf_url": "http://arxiv.org/pdf/2406.07542v1",
        "summary": "Cognitive decline is a natural process that occurs as individuals age. Early\ndiagnosis of anomalous decline is crucial for initiating professional treatment\nthat can enhance the quality of life of those affected. To address this issue,\nwe propose a multimodal model capable of predicting Mild Cognitive Impairment\nand cognitive scores. The TAUKADIAL dataset is used to conduct the evaluation,\nwhich comprises audio recordings of clinical interviews. The proposed model\ndemonstrates the ability to transcribe and differentiate between languages used\nin the interviews. Subsequently, the model extracts audio and text features,\ncombining them into a multimodal architecture to achieve robust and generalized\nresults. Our approach involves in-depth research to implement various features\nobtained from the proposed modalities.",
        "updated": "2024-06-11 17:59:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.07542v1"
    },
    {
        "title": "CDSA: Conservative Denoising Score-based Algorithm for Offline Reinforcement Learning",
        "authors": "Zeyuan LiuKai YangXiu Li",
        "links": "http://arxiv.org/abs/2406.07541v1",
        "entry_id": "http://arxiv.org/abs/2406.07541v1",
        "pdf_url": "http://arxiv.org/pdf/2406.07541v1",
        "summary": "Distribution shift is a major obstacle in offline reinforcement learning,\nwhich necessitates minimizing the discrepancy between the learned policy and\nthe behavior policy to avoid overestimating rare or unseen actions. Previous\nconservative offline RL algorithms struggle to generalize to unseen actions,\ndespite their success in learning good in-distribution policy. In contrast, we\npropose to use the gradient fields of the dataset density generated from a\npre-trained offline RL algorithm to adjust the original actions. We decouple\nthe conservatism constraints from the policy, thus can benefit wide offline RL\nalgorithms. As a consequence, we propose the Conservative Denoising Score-based\nAlgorithm (CDSA) which utilizes the denoising score-based model to model the\ngradient of the dataset density, rather than the dataset density itself, and\nfacilitates a more accurate and efficient method to adjust the action generated\nby the pre-trained policy in a deterministic and continuous MDP environment. In\nexperiments, we show that our approach significantly improves the performance\nof baseline algorithms in D4RL datasets, and demonstrate the generalizability\nand plug-and-play capability of our model across different pre-trained offline\nRL policy in different tasks. We also validate that the agent exhibits greater\nrisk aversion after employing our method while showcasing its ability to\ngeneralize effectively across diverse tasks.",
        "updated": "2024-06-11 17:59:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.07541v1"
    },
    {
        "title": "Ctrl-X: Controlling Structure and Appearance for Text-To-Image Generation Without Guidance",
        "authors": "Kuan Heng LinSicheng MoBen KlingherFangzhou MuBolei Zhou",
        "links": "http://arxiv.org/abs/2406.07540v1",
        "entry_id": "http://arxiv.org/abs/2406.07540v1",
        "pdf_url": "http://arxiv.org/pdf/2406.07540v1",
        "summary": "Recent controllable generation approaches such as FreeControl and Diffusion\nSelf-guidance bring fine-grained spatial and appearance control to\ntext-to-image (T2I) diffusion models without training auxiliary modules.\nHowever, these methods optimize the latent embedding for each type of score\nfunction with longer diffusion steps, making the generation process\ntime-consuming and limiting their flexibility and use. This work presents\nCtrl-X, a simple framework for T2I diffusion controlling structure and\nappearance without additional training or guidance. Ctrl-X designs feed-forward\nstructure control to enable the structure alignment with a structure image and\nsemantic-aware appearance transfer to facilitate the appearance transfer from a\nuser-input image. Extensive qualitative and quantitative experiments illustrate\nthe superior performance of Ctrl-X on various condition inputs and model\ncheckpoints. In particular, Ctrl-X supports novel structure and appearance\ncontrol with arbitrary condition images of any modality, exhibits superior\nimage quality and appearance transfer compared to existing works, and provides\ninstant plug-and-play functionality to any T2I and text-to-video (T2V)\ndiffusion model. See our project page for an overview of the results:\nhttps://genforce.github.io/ctrl-x",
        "updated": "2024-06-11 17:59:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.07540v1"
    }
]