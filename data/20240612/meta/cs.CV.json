[
    {
        "title": "An Image is Worth 32 Tokens for Reconstruction and Generation",
        "authors": "Qihang YuMark WeberXueqing DengXiaohui ShenDaniel CremersLiang-Chieh Chen",
        "links": "http://arxiv.org/abs/2406.07550v1",
        "entry_id": "http://arxiv.org/abs/2406.07550v1",
        "pdf_url": "http://arxiv.org/pdf/2406.07550v1",
        "summary": "Recent advancements in generative models have highlighted the crucial role of\nimage tokenization in the efficient synthesis of high-resolution images.\nTokenization, which transforms images into latent representations, reduces\ncomputational demands compared to directly processing pixels and enhances the\neffectiveness and efficiency of the generation process. Prior methods, such as\nVQGAN, typically utilize 2D latent grids with fixed downsampling factors.\nHowever, these 2D tokenizations face challenges in managing the inherent\nredundancies present in images, where adjacent regions frequently display\nsimilarities. To overcome this issue, we introduce Transformer-based\n1-Dimensional Tokenizer (TiTok), an innovative approach that tokenizes images\ninto 1D latent sequences. TiTok provides a more compact latent representation,\nyielding substantially more efficient and effective representations than\nconventional techniques. For example, a 256 x 256 x 3 image can be reduced to\njust 32 discrete tokens, a significant reduction from the 256 or 1024 tokens\nobtained by prior methods. Despite its compact nature, TiTok achieves\ncompetitive performance to state-of-the-art approaches. Specifically, using the\nsame generator framework, TiTok attains 1.97 gFID, outperforming MaskGIT\nbaseline significantly by 4.21 at ImageNet 256 x 256 benchmark. The advantages\nof TiTok become even more significant when it comes to higher resolution. At\nImageNet 512 x 512 benchmark, TiTok not only outperforms state-of-the-art\ndiffusion model DiT-XL/2 (gFID 2.74 vs. 3.04), but also reduces the image\ntokens by 64x, leading to 410x faster generation process. Our best-performing\nvariant can significantly surpasses DiT-XL/2 (gFID 2.13 vs. 3.04) while still\ngenerating high-quality samples 74x faster.",
        "updated": "2024-06-11 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.07550v1"
    },
    {
        "title": "Blur-aware Spatio-temporal Sparse Transformer for Video Deblurring",
        "authors": "Huicong ZhangHaozhe XieHongxun Yao",
        "links": "http://arxiv.org/abs/2406.07551v1",
        "entry_id": "http://arxiv.org/abs/2406.07551v1",
        "pdf_url": "http://arxiv.org/pdf/2406.07551v1",
        "summary": "Video deblurring relies on leveraging information from other frames in the\nvideo sequence to restore the blurred regions in the current frame. Mainstream\napproaches employ bidirectional feature propagation, spatio-temporal\ntransformers, or a combination of both to extract information from the video\nsequence. However, limitations in memory and computational resources\nconstraints the temporal window length of the spatio-temporal transformer,\npreventing the extraction of longer temporal contextual information from the\nvideo sequence. Additionally, bidirectional feature propagation is highly\nsensitive to inaccurate optical flow in blurry frames, leading to error\naccumulation during the propagation process. To address these issues, we\npropose \\textbf{BSSTNet}, \\textbf{B}lur-aware \\textbf{S}patio-temporal\n\\textbf{S}parse \\textbf{T}ransformer Network. It introduces the blur map, which\nconverts the originally dense attention into a sparse form, enabling a more\nextensive utilization of information throughout the entire video sequence.\nSpecifically, BSSTNet (1) uses a longer temporal window in the transformer,\nleveraging information from more distant frames to restore the blurry pixels in\nthe current frame. (2) introduces bidirectional feature propagation guided by\nblur maps, which reduces error accumulation caused by the blur frame. The\nexperimental results demonstrate the proposed BSSTNet outperforms the\nstate-of-the-art methods on the GoPro and DVD datasets.",
        "updated": "2024-06-11 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.07551v1"
    },
    {
        "title": "Image and Video Tokenization with Binary Spherical Quantization",
        "authors": "Yue ZhaoYuanjun XiongPhilipp Krähenbühl",
        "links": "http://arxiv.org/abs/2406.07548v1",
        "entry_id": "http://arxiv.org/abs/2406.07548v1",
        "pdf_url": "http://arxiv.org/pdf/2406.07548v1",
        "summary": "We propose a new transformer-based image and video tokenizer with Binary\nSpherical Quantization (BSQ). BSQ projects the high-dimensional visual\nembedding to a lower-dimensional hypersphere and then applies binary\nquantization. BSQ is (1) parameter-efficient without an explicit codebook, (2)\nscalable to arbitrary token dimensions, and (3) compact: compressing visual\ndata by up to 100$\\times$ with minimal distortion. Our tokenizer uses a\ntransformer encoder and decoder with simple block-wise causal masking to\nsupport variable-length videos as input. The resulting BSQ-ViT achieves\nstate-of-the-art visual reconstruction quality on image and video\nreconstruction benchmarks with 2.4$\\times$ throughput compared to the best\nprior methods. Furthermore, by learning an autoregressive prior for adaptive\narithmetic coding, BSQ-ViT achieves comparable results on video compression\nwith state-of-the-art video compression standards. BSQ-ViT also enables masked\nlanguage models to achieve competitive image synthesis quality to GAN- and\ndiffusion-based methods.",
        "updated": "2024-06-11 17:59:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.07548v1"
    },
    {
        "title": "Zero-shot Image Editing with Reference Imitation",
        "authors": "Xi ChenYutong FengMengting ChenYiyang WangShilong ZhangYu LiuYujun ShenHengshuang Zhao",
        "links": "http://arxiv.org/abs/2406.07547v1",
        "entry_id": "http://arxiv.org/abs/2406.07547v1",
        "pdf_url": "http://arxiv.org/pdf/2406.07547v1",
        "summary": "Image editing serves as a practical yet challenging task considering the\ndiverse demands from users, where one of the hardest parts is to precisely\ndescribe how the edited image should look like. In this work, we present a new\nform of editing, termed imitative editing, to help users exercise their\ncreativity more conveniently. Concretely, to edit an image region of interest,\nusers are free to directly draw inspiration from some in-the-wild references\n(e.g., some relative pictures come across online), without having to cope with\nthe fit between the reference and the source. Such a design requires the system\nto automatically figure out what to expect from the reference to perform the\nediting. For this purpose, we propose a generative training framework, dubbed\nMimicBrush, which randomly selects two frames from a video clip, masks some\nregions of one frame, and learns to recover the masked regions using the\ninformation from the other frame. That way, our model, developed from a\ndiffusion prior, is able to capture the semantic correspondence between\nseparate images in a self-supervised manner. We experimentally show the\neffectiveness of our method under various test cases as well as its superiority\nover existing alternatives. We also construct a benchmark to facilitate further\nresearch.",
        "updated": "2024-06-11 17:59:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.07547v1"
    },
    {
        "title": "Commonsense-T2I Challenge: Can Text-to-Image Generation Models Understand Commonsense?",
        "authors": "Xingyu FuMuyu HeYujie LuWilliam Yang WangDan Roth",
        "links": "http://arxiv.org/abs/2406.07546v1",
        "entry_id": "http://arxiv.org/abs/2406.07546v1",
        "pdf_url": "http://arxiv.org/pdf/2406.07546v1",
        "summary": "We present a novel task and benchmark for evaluating the ability of\ntext-to-image(T2I) generation models to produce images that fit commonsense in\nreal life, which we call Commonsense-T2I. Given two adversarial text prompts\ncontaining an identical set of action words with minor differences, such as \"a\nlightbulb without electricity\" v.s. \"a lightbulb with electricity\", we evaluate\nwhether T2I models can conduct visual-commonsense reasoning, e.g. produce\nimages that fit \"the lightbulb is unlit\" vs. \"the lightbulb is lit\"\ncorrespondingly. Commonsense-T2I presents an adversarial challenge, providing\npairwise text prompts along with expected outputs. The dataset is carefully\nhand-curated by experts and annotated with fine-grained labels, such as\ncommonsense type and likelihood of the expected outputs, to assist analyzing\nmodel behavior. We benchmark a variety of state-of-the-art (sota) T2I models\nand surprisingly find that, there is still a large gap between image synthesis\nand real life photos--even the DALL-E 3 model could only achieve 48.92% on\nCommonsense-T2I, and the stable diffusion XL model only achieves 24.92%\naccuracy. Our experiments show that GPT-enriched prompts cannot solve this\nchallenge, and we include a detailed analysis about possible reasons for such\ndeficiency. We aim for Commonsense-T2I to serve as a high-quality evaluation\nbenchmark for T2I commonsense checking, fostering advancements in real life\nimage generation.",
        "updated": "2024-06-11 17:59:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.07546v1"
    }
]