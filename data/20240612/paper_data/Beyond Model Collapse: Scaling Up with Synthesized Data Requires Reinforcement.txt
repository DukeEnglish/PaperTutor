Beyond Model Collapse: Scaling Up with Synthesized
Data Requires Reinforcement
YunzhenFeng∗†§ ElvisDohmatob∗§ PuYang∗¶ FrancoisCharton§ JuliaKempe†‡§
§MetaFAIR
†CenterforDataScience,NewYorkUniversity
‡CourantInstitueofMathematicalSciences,NewYorkUniversity
¶SchoolofMathematicalSciences,PekingUniversity
yf2231@nyu.edu
Abstract
Synthesizeddatafromgenerativemodelsisincreasinglyconsideredasanalterna-
tivetohuman-annotateddataforfine-tuningLargeLanguageModels. Thisraises
concernsaboutmodelcollapse: adropinperformanceofmodelsfine-tunedon
generated data. Considering that it is easier for both humans and machines to
tellbetweengoodandbadexamplesthantogeneratehigh-qualitysamples, we
investigate the use of feedback on synthesized data to prevent model collapse.
WederivetheoreticalconditionsunderwhichaGaussianmixtureclassification
modelcanachieveasymptoticallyoptimalperformancewhentrainedonfeedback-
augmentedsynthesizeddata,andprovidesupportingsimulationsforfiniteregimes.
We illustrate our theoretical predictions on two practical problems: computing
matrixeigenvalueswithtransformersandnewssummarizationwithlargelanguage
models, which both undergo model collapse when trained on model-generated
data. We show that training from feedback-augmented synthesized data, either
bypruningincorrectpredictionsorbyselectingthebestofseveralguesses,can
preventmodelcollapse,validatingpopularapproacheslikeRLHF.
1 Introduction
Asgenerativemodelsforlanguage(Touvronetal.,2023;Achiametal.,2023), images(Ramesh
etal.,2021;Rombachetal.,2022),andvideo(OpenAI,2024)achievehuman-levelperformance,
asignificantfractionofthetrainingdataforfuturemodelswillbegeneratedbypreviousmodels.
ChatGPTalonegenerates0.1%ofthetokenscurrentlyproducedbyhumans(Altman,2024). Thereis
anincreasinguseofAI-synthesizeddataindiversedomainssuchascoding(Haluptzoketal.,2022)
andmathematics(Trinhetal.,2024)andstronglanguagemodelsaretautedasapossiblereplacement
forexpensivehumanannotators.
This gradual replacement of human-written corpora by machine-generated tokens gives rise to a
numberofconcerns,notablytheriskof“modelcollapse”Shumailovetal.(2023),whereiterated
trainingonsynthesizeddatabringsadropinmodelperformance,and,ultimately,“dumbermodels”.
Thisphenomenonwasobservedempirically(Hatayaetal.,2023;Mart´ınezetal.,2023a,b;Bohacek
andFarid,2023;Brieschetal.,2023;Guoetal.,2023)anddescribedtheoretically(Alemohammad
etal.,2023;Bertrandetal.,2023;Dohmatobetal.,2024a). Itsmainconsequenceisthebreakingof
*EqualContributions.
Preprint.Underreview.
4202
nuJ
11
]GL.sc[
1v51570.6042:viXraVerifier Ground Truth Ground Truth
Final Model
HHuummaann Human Reinforcement Trained Model ReinforcP er mun ee nr t
Model Reinforcement (Generator) Generator
Generator
Generator Good Model
Without Reinforcement
(Model Collapse)
Selected Synthesized Data
Without Number of Synthesized Data Original Data
Reinforcement Discarded Synthesized Data
(a) Weproposeusingaverifiertoselectgenerated (b) In theory, we consider a Gaussian mixture
synthesizeddata.Humanandmodelreinforcement model with a linear generator and linear pruner.
canenhanceperformanceandpreventmodelcol- Theprunerreinforcessynthesizeddatathroughse-
lapse,asopposedtothedegradationobservedwith- lection,resultinginimprovedperformance.
outreinforcement.
Figure1: Illustrativefiguresforourproposal(a)andforthetheoreticalandsimulationsettings(b).
knownscalinglaws(Dohmatobetal.,2024b): asdatabecomesmoresynthetic,largertrainingsetsdo
notenhanceperformance.
Meanwhile,wearewitnessingthemassiveuseofReinforcementLearningwithHumanFeedback
(RLHF)(Ouyangetal.,2022)anditsvariants,whichleverageshumanfeedbackandannotationto
trainmodelswellpasttheirperformanceonscrapedinternetdata. Astheperformanceoflanguage
models improves, their use as feedback generators, replacing human annotators, is increasingly
considered. Thisleadsustoaskthetimelyquestion:
Canfeedback,fromhumansormachines,improvesynthesizeddata,tothepointthatitcanbe
usedtotrainnewmodelswithoutfearofcollapse?
Tostudythisquestion,wefirstprovideanalytical(affirmative)resultsinatheoreticalsetting,wherewe
considerGaussianmixtures(andgeneralizations)inthehigh-dimensionallimitwithlinearmodelsas
classifiers.Weallowapossiblynoisyverifier(e.g.humanororacle)toselect(orprune)generateddata.
Wedemonstratethatasthenumberofsynthesizeddatapointsapproachesinfinity,themodeltrained
onselecteddatacanachieveoptimalresults,onparwithtrainingontheoriginaldata. Specifically,we
identifyasharpphasetransition: fromzeroaccuracyduetoerrorsinthesynthesizeddataandverifier,
to optimal accuracy. We conduct simulations on synthesized data to explore how the generator
andverifieraffectperformanceandscalingratesinfiniteregimes. Alsohere,ourresultsshowthat
oraclesupervisionconsistentlyyieldsnear-optimalresultscomparedtousingoriginallabels. Since
discerninghigh-qualitydatathroughhumansupervisionissimplerandmorecost-effectivethandirect
humanlabeling,thisprovidesstrongevidencefortheefficacyofhuman-in-the-loopsupervision.
Wenextexaminerealisticsettingstoillustrateourtheorythroughtwolarge-scaleexperiments: 1)
trainingtransformersonanarithmetictask(matrixeigenvalueprediction),usingadistancemetric
tothegroundtruthforpruninglargevolumesofsynthesizeddata,and2)newssummarizationwith
largelanguagemodels(Llama2)andlimitedsynthesizeddata.
Moreover, our experiments with the arithmetic task give us the ability to easily generate several
candidatesolutionsperprompt, akintoRLHFsettings. Weshowthatwhenusingreinforcement
(ranking with respect to the ground truth and picking the best), we achieve strong performance
improvement;however,curiouslybutasexpected,pickingjustthebestsolutionfromthegenerated
pool based on perplexity does not lead to enhanced performance. This indicates that the model
itselflackstheinherentcapabilitytoselectthesesuperiorpredictionsbasedonperplexityaloneand
supportsourhypothesisthatscalingupwithsynthesizeddatarequiresreinforcement. Inbothsettings,
reliancesolelyongenerateddataresultsinpoorerperformancecomparedtousingtheoriginaldataset,
evenwithincreaseddatavolume,indicatingmodelcollapse. Conversely,withoraclesupervision,we
achieveanimprovedsynthesizeddatasetthatsurpassestheoriginal,withperformanceimprovingas
moredataisadded.
Wesummarizeourcontributionasfollows:
• Weprovidetheoreticalanalysistopreciselycharacterizewhendataselectionleadstooptimal
performanceinthehigh-dimensionallimitwithunlimitedaccesstosynthesizeddata(Section3).
2
dezisehtnyS
ataD
ecnamrofrePSimulationresultsonsynthesizeddataextendthistothefinite-dataregimetoshowthatoracle
(human)selectioncanmatchtrainingwithoriginallabels(Section4).
• Wegeneralizetheseobservationstoempiricalsettingswithanarithmetictaskusingtransformers
andnewssummarizationusingLLaMA-2. Modelcollapseisobserved,whileoracleselection
preventsitandimprovestheselecteddatasetbeyondthesynthesizedgenerator(Section5).
Crucially, note that all that is needed to re-attain model performance akin to training on clean
originaldataistheabilitytodistinguishhigh-qualityfromlowqualitylabels;arguably,ataskmuch
simplerthanannotatingthelabels. Thus,togobeyondmodelcollapseandcontinuescalingupwith
synthesizeddata,reinforcementisallyouneed!.
2 RelatedWork
Wehereonlylistworksofdirectrelevancetoours,withanextensivereferencelistinAppendixA.
ModelCollapse. Withtheadvancementofgenerativemodels,synthesizeddatageneratedbythese
models has become increasingly prevalent online, mixing irreversibly into our training corpora.
Recentstudieshavehighlightedthepotentialfordramaticdeteriorationindownstreammodels,aphe-
nomenonknownas“modelcollapse”(Shumailovetal.,2023). Empiricalstudieshavedemonstrated
thisissueinvarioussettings(Hatayaetal.,2023;Mart´ınezetal.,2023a,b;BohacekandFarid,2023;
Brieschetal.,2023). Synthesizeddatasetshavebeenshowntoreducediversity(Padmakumarand
He,2024;Guoetal.,2023)andcausedistributionaldistortions(LeBrunetal.,2021). Theoretical
analysesalsoexaminetheeffectsofiterativetrainingonself-generateddata(Alemohammadetal.,
2023;Bertrandetal.,2023;Dohmatobetal.,2024a;Seddiketal.,2024). Notably,Dohmatobetal.
(2024b)warnsthatmodelcollapsesignifiesabreakinthescalinglaw,whereincreasingsynthesized
datavolumedoesnotenhanceperformanceaseffectivelyasscalingwithhuman-generateddata. In
thiswork,weaimtoapplyselectiontechniquestolargesynthesizeddatasetstosurpassthequalityof
theoriginaldatathattrainedthegenerator.
Synthesized Data with Selection. Empirical studies have demonstrated that applying selection
techniquestosynthesizeddatacansignificantlyenhanceperformance,particularlyinthedomains
ofcodeandmathematicswheregoodverifiersforcorrectnessexist. Forinstance,Haluptzoketal.
(2022)generatesynthesizedcodedataandfilteroutincorrectinstances. Trinhetal.(2024)leveragea
symbolicdeductionengineasaverifiertosamplecorrectsolutionsforOlympiadegeometryproblems.
Oraclereinforcementandabundantsynthesizedinputleadtonear-optimalperformance. Whena
verifierdoesnotexist,Lietal.(2022)useahigh-qualitydatasettotrainaverifiertoselectdatafor
self-labeling. Additionally,somestudiesachievedataselectionbycarefullychoosingpromptswith
highqualityandgooddiversity,employingheuristicverifiers: instructiontuning(Wangetal.,2023),
codegeneration(Weietal.,2023),andimagesynthesis(Hemmatetal.,2023;Azizietal.,2023).
TaxonomyofBenefitsofSynthesizedData. Abundantempiricalstudiesdemonstratethebenefitsof
synthesizeddatainvarioussettings;notleastinthecasewhenthedownstreambeneficiaryneedsto
performaslightlydifferenttaskthanthedatageneratingmodeland/orwhenthegeneratingmodelis
muchstrongerthantheconsumingone. InAppendixA.1,weprovideataxonomyoutliningwhenand
howsynthesizeddataisbeneficial.Specifically,weidentifyfourkeycomponents:promptengineering,
knowledgefromadvancedmodels,distributionalshiftandtargetedcuration,andexternalverifiers.
Mostempiricalstudiescanbecategorizedbasedononeormoreofthesecomponents. Ourworkalso
employsexternalverifierstopreventmodelcollapse.
DataSelection. Dataselectionisawell-establishedpreprocessingtechniquetypicallyusedfor(origi-
nal)datawithhumanlabels. SeeAppendixA.3. Ourworkconcernsreinforcementonsynthesized
data.
3 TheoreticalInsights
Here,webeginbytheoreticallycharacterizingunderwhatconditionsdataselectionwithreinforce-
mentcanleadtoimprovements,forafamilyofhigh-dimensionaldatadistributions. Notethatwe
will model the reinforcement process as a pruning strategy over synthesized data. Crucially, we
willnotnecessarilyassumethatthepruningstrategyhasaccesstothegroundtruth;rather,wewill
formulateourtheoryinsufficientgeneralitytoallowfor“intermediate”pruners,whichcanbeviewed
3asreinforcementfromadifferent(oreventhesame)model. Afullexpositionofourgeneraltheoryis
providedinAppendixD;foreaseofexposition,wespecializeheretoGaussianMixtures.
3.1 Setting
DataDistribution. WewillconsiderdistributionsP overRd×{0,1}withcertainhighdimensional
concentration properties of a general form (Condition D.1). A special case are binary Gaussian
Mixtures: featureshaveconditionaldistributiongivenbyx|y ∼N(µ ,Σ),whereµ =(2y−1)µ,
y y
forsomeµ ∈ Rd andΣisapositive-definitematrixwithE∥x∥2 = ∥µ∥2+trΣ = 1. Forfurther
2
easeofexpositionwewillonlyconsiderbalanceddistributions
P(y =1)=P(y =0)=1/2, for(x,y)∼P.
SynthesizedData. LetD ={(x ,y ),...,(x ,y )}beadatasetofN iidpairsfromthetrue
N 1 1 N N
distributionP andletD′ ={(x ,y′),...,(x ,y′ )}bethesynthesizeddatageneratedfromthe
N 1 1 N N
samedistribution,butwherelabely′ (insteadofy )hasbeengeneratedbyanAImodel.
i i
Downstream Model and Pruning. We will model our data selection (whether with or without
feedback)viaapruningstrategyq =(q ,...,q )whereq isabitwhichindicateswhethertheith
1 N i
trainingexamplefromD′ hassurvivedpruning. Forthedownstreammodelsweconsiderthefamily:
N
1
P(y =1|x,w)=yˆ:=σ(x⊤w)∈(0,1), σ(z):=
1+e−z
parametrizedbyavectorofweightsw ∈Rdandsigmoidnon-linearityσ. Letw beobtainedvia
(cid:98)N
logisticregressionfittedonD′ withridgeregularizationparameterλ> 0. Thus,wˆistheunique
N
minimizerofthefollowingobjectivefunction
N
1 (cid:88) λ
L(w):= q ℓ(σ(x⊤w),y′)+ ∥w∥2, (1)
N i i i 2
i=1
whereℓisthebinarycross-entropy. Thecorrespondingdownstreamclassifierisf(cid:98)N =f w(cid:98)N,where
the notation f refers to the linear classifier induced by a weights vector w ∈ Rd, i.e f (x) =
w w
(sign(x⊤w)+1)/2.
TestAccuracy. Thetestaccuracyofthedownstreammodelf(cid:98)N isdefinedby
acc(f(cid:98)N):=P(f(cid:98)N(x)=f Bayes(x)), forarandomtestpoint(x,y)∼P,
wheref (z):=E[y|x=z]istheBayes-optimalclassifier.Inparticular,notethatacc(f )=
Bayes Bayes
100%byconstruction. Thequantityacc(f(cid:98)N)willbethemainobjectofouranalysis,andwewillbe
interestedinhowitdependsonerrorsinthegeneratorP andthechoiceofpruningstrategyq,inthe
infinite-samplelimitN →∞.
3.2 PruningStrategy
Weconsiderawideclassofparametrizedpruningstrategiesq,whichwetermRLHF-Pruning(see
AppendixD).Theysatisfythefollowingreasonableproperty:
Assumption3.1(IndependentSelection). Thebitsq ,...,q ∈{0,1}areindependent. Thus,in
1 N
particular,whetheranytrainingexamplee :=(x ,y′)∈D′ survivespruningornotisindependent
i i i N
ofwhathappenstotheotherexamplese .
j̸=i
Weshalldenotebyp ∈ [0,1),theprobabilitythatthelabely′ ofasynthesizedexample(x,y′)is
differentfromthetruelabely,i.e
p:=P(y′ ̸=y). (2)
Notethatpdoesnotdependentontheexampleindexi,duetotheiidassumption.
Our RLHF-pruning family (refer to Appendix D for details) is described by four parameters
(ϕ ,ϕ ,ψ ,ψ ),definedasfollows
0 1 01 10
ϕ =P(q =1|y′ =k,y =k), ψ =P(q =1|y′ =ℓ,y =k). (3)
k kℓ
4For simplicity of exposition, we will focus on symmetric pruning strategies, ϕ = ϕ = ϕ and
1 0
ψ =ψ =ψ.Assumption3.1impliesthatforanyclasslabelsk,ℓ∈{0,1},therandomvariables
01 10
(z ) definedbyz =1[y =k,y′ =ℓ,q =1]areiidwithBernoulli(p )distribution,with
ikℓ i∈[N] ikℓ i i i kℓ
p =(1−p)ϕ /2, andp =pψ /2ifk ̸=ℓ. (4)
kk k kℓ k
Inthissectionwefocusonaspecialcaseofsupervisedpruningstrategiesqoftheform
SupervisedPruning: q =1[y′(x⊤w )>0], (5)
i i i prune
forsomew ∈Rd. Thispruningstrategyfiltersoutallexamplesonwhichthereisdisagreement
prune
ontheassignedlabel. InAppendixDweshowhowwecanmapthisto(ϕ,ψ)-pruning.
Letusprovidetwomorenotableexamplesof(symmetric)pruningstrategies.
NoPruning. Thecase(ϕ,ψ)=(1,1)correspondstonopruning,i.eusingtheentiretrainingdataset.
OraclePruning. Thecase(ϕ,ψ)=(1,0). Thepruningstrategyonlykeepsindicescorresponding
toexamplesinthedatasetwhichhavecorrectlabel(allcorruptedlabelsarediscarded).
3.3 PerformanceofModelsTrainedwithPruning: InsightsfromInfinite-SampleRegime
Thefollowingisourmaintheoreticalresult(seeTheoremD.3forfullstatement). Itcharacterizestest
accuracyacc(f(cid:98)N)ofthedownstreammodelonpruneddataasafunctionofp(thelabeldisagreement)
andtheparameters(ϕ,ψ)ofthepruner,inthetheoreticallimitofinfinitetrainingdata(N →∞).
Theorem3.2(SimplifiedversionofTheoremD.3). LetAssumption3.1beinorder. Fixp,ϕ,ψand
definethebreakdownpointp ∈(0,1)byp :=1/(1+ψ/ϕ). Forthefamilyofdatadistributions
⋆ ⋆
obeyingConditionD.1(includingtheGaussianmixture),foradownstreammodelf(cid:98)N trainedon
datafromageneratorwitherrorratep,prunedwithanRLHF-typestrategywithparameters(ϕ,ψ),
inthelimitN →∞itholdsa.sthat:
(i)Ifp<p ⋆thenacc(f(cid:98)N)=100%.
(ii) If p > p
⋆
then acc(f(cid:98)N) = 0%. The pruner is overwhelmed by so many inaccuracies in the
synthesizeddata,andthedownstreammodellearnstheexactoppositeofthetrueclasslabels.
Thus, there is a sharp phase-transition around the
corruption level p := 1/(1 + ψ/ϕ): as p is in-
⋆
creased past level p ⋆, the downstream model f(cid:98)N
abruptlyswitchesfrombeingperfectlyaccurate,to
perfectlyinaccurate! Theproof(seeAppendixD.7
forasketch)explicitlycomputesempiricaltestaccu-
racyintermsofN
:=(cid:80)N
z ,whichfollowa
kℓ i=1 ikℓ
binomialdistribution,boundingthegaptothepopu-
lationaccuracy,andusingconcentrationofmeasure
typetechniques. Notethatthesharptransitionisdue Figure 2: Empirical Confirmation of The-
totheinfinite-sampleregime, wherewecanavoid orem3.2. Comparingthebreakdownpoints
finite-samplecorrections. ofdifferentgeneratorsandprunersofdiffer-
entstrengths. Synthesizeddataisgenerated
SeeFigure2(andFigure5inAppendixD)foran fromalinearmodelw withclassification
gen
empiricalillustrationofthetheorem. error rate p = θ /π ∈ [0,1]. Refer to
gen
Remark3.3. Notethatthe100%accuracyachiev- Equations (6), and recall that the triplet of
able in Theorem 3.2 is idealized, and is expected angles (θ gen,θ prune,θ) maps to parameters
to only hold in the infinite sample regime (with a (ϕ,ψ) for an RLHF-type pruner. The data
possiblylargebutfixedinputdimension). is pruned with another linear model w prune
whichhasclassificationerrorθ /π. Bro-
prune
kenlinescorrespondtothepredictionofThe-
3.4 SomeConsequencesofTheorem3.2
orem 3.2, while solid points correspond to
WenowpresentsomeillustriousapplicationsofThe- experiments. Notice the sharp phase transi-
oremD.3.Theseexamplesareempiricallyconfirmed tionswherethemodelsuddenlyswitchesfrom
inFigure2(seealsoFigure5inAppendixD). perfectaccuracytoworse-than-chance,aphe-
nomenonpredicatedbythetheorem.
5NoPruning. Here,wehaveψ/ϕ=1andsothedownstreammodelachieves100%accuracyfor
allvaluesofcorruptionparameterpuptothebreakdownpointp =1/2predictedbyTheoremD.3.
⋆
OraclePruning. Forthisscenario,ψ/ϕ = 0andsoTheoremD.3predictsthatthedownstream
modelf(cid:98)N achieves100%accuracyforallvaluesofcorruptionparameterpuptothebreakdownpoint
p =1. Thisisperhapsnotsosurprisinginhindsight. Thepointisthatevenformoderatelylarge
⋆
valuesofψ/ϕ,thebreakdownpointp canstillbequitecloseto1.
⋆
SupervisedPruning. ConsiderisotropicGaussianmixturedatawithmeans±µ,andapruning
strategyasinEq.(5). Theparameters(ϕ,ψ)onlydependontheanglesθ ,θ ,θ ∈[0,π]given
gen prune
by
θ :=∠(w ,µ), θ :=∠(w ,µ),
gen gen prune prune
(6)
θ :=∠(w ,w ).
prune gen
Thisisbecause,thep ’sdefinedin(4)nowcorrespondtoorthantprobabilitiesforatrivariatenormal
kℓ
distribution,withcorrelationcoefficientsgivenbytheseangles(seealsoFigure2,andAppendixD.6
forthecalculation).
DecouplingtheGeneratorandVerifier. Althoughthegeneratorandverifierarecoupledtogether
insupervisedpruning,therearesomeintuitionsthathelpusdecouplethem: (1)abettergenerator
alwaysimprovesperformance,(2)whentheverifierispoor,suchasincasesofnopruningorrandom
pruning,wehavealowbreakdownpointandrequireagoodgeneratortoachievegoodperformance,
and(3)whentheverifierissufficientlygood,closetoanoracle,thebreakdownpointishigh,andany
non-degenerategeneratorissufficient.
4 SimulationsonSynthesizedData
Thetheoreticalresultsarebasedonthebest-casescenarioofhavingunlimitedaccesstosynthesized
data and operating in the high-dimensional limit. In this framework, the generator and verifier’s
impactonperformanceisreflectedinbinaryoutcomes: 100%or0%. Inthissection,wepresent
simulationresultsinfiniteregimestodemonstratethepracticalimplicationsofthetheory,specifically
howthegeneratorandverifieraffectperformanceandscalingrateswithrespecttothenumberof
synthesizeddatapoints.
4.1 Setting
Followingthetheoreticalsetting,weconsiderthesameGaussianmixtureandlinearmodelsforthe
generatorandselector. Letw beafixedunitvectorinRd. ThedistributionP is
∗ orig
x|y ∼N(yτw ,I /d), fory ∈[−1,+1].
∗ d
Here,τ isapositivescalarthatcontrolstheoverlap.
SynthesizedDataGeneration WeinitiallysampleN datafromthedistributionP astheorigi-
0 orig
naldataset,D ,whichisusedtotrainalinearmodelwˆusingordinaryleastsquares. Subsequently,
orig
weusew =wˆtogenerateN synthesizeddatapointswithsigmoid,constitutingthedatasetD .
gen 1 gen
Reinforcement Thedataisselectedwithvariousw inEquation(5)fromw tow whereθis
prune ∗ θ
theanglebetweenw andw . n′isthenumberofdatapointsselected.
θ ∗
4.2 LessonsLearned
In Figure 3, we run several simulations with different N values. A larger N corresponds to a
0 0
better generator trained with more original data. The synthesized data is selected using verifiers
rangingfromoracle-levelaccuracytovariouslevelsoferrorswithθ = π and π. Alarger
prune 12 6
θ correspondstoalowerqualityverifier. The“random”linedenotesrandomlyselectingn′data
prune
pointsfromthesynthesizedset,while“clean”denotestrainingthemodelwithn′datapointsfrom
theoriginaldistribution. Wehavethefollowingobservations:
6Oracle Data Weak Supervision prune= /12 Weak Supervision prune= /6 Oracle Selection Random Selection
Number of Training Data: N0=100 Number of Training Data: N0=300 Number of Training Data: N0=900
101 101 101
102 102 102
103 103 103
104 104 104
101 102 103 104 105 101 102 103 104 105 101 102 103 104 105
Number of Synthesized Data n0 Number of Synthesized Data n0 Number of Synthesized Data n0
Figure3: Simulationsonthescalingwithrespecttothenumberofselecteddata,n′. τ =0.15,N =
1
106. TheBayesoptimalclassifierachievesapproximately94%accuracyonthisdistribution. The
y-axisdenotestherelativeerror,i.e.,theaccuracyrelativetotheoptimalaccuracy.
OracleSupervisionMatchesTrainingwithOracleLabels. Theoracleachievesthebestperfor-
mance,matchingtrainingwithcleandataacrossallsettingsandattainingBayesoptimalaccuracy,as
predictedbytheory.
Weaksupervision.Weaksupervisionresultsinpoorerperformance,reflectingthedecayingthreshold
pointsoutlinedintheory. Surprisingly,thereisasweetspotwhenusingweaksupervision,afterwhich
theselectederroneouspointsdegradeaccuracy. Whenthegeneratorissufficientlyaccurate,using
weaksupervisionmayharmperformanceduetotheselectionofincorrectdatapoints.
ConnectiontotheTheory. Thesimulationsillustratethetheoryinpracticalscenariosandalignwith
theinterpretationinSection3.4. Oraclesupervisionleadstooptimalperformance,andgenerally,
improvingthegeneratorandverifierenhancesperformance. However,inalaterexperimentinSection
5.3,wewillobservethatslightlyincreasingtheverifier’saccuracy,whilechangingitscorrelationto
thegeneratordrastically,canevenhurtperformance.
5 Experiments
Inboththetheoreticalresultsandthesimulations,weexamineaclassificationcasewhereoracle
supervisioncanselect“100%correct”synthesizeddata. Beyondthesesettings,thesynthesizeddata
canhaveacontinuousspectrumregardingitsdistancetothegroundtruthandtheselecteddatamay
onlybecorrecttosomeextent. Inthissense,weassesstheimpactofreinforcementmethodsandthe
generatorintwoexperiments: (1)trainingatransformertopredicteigenvaluesofamatrixand(2)
fine-tuningLlama-2-7Bonanewssummarizationtask.
5.1 UnderstandingtheQualityofSynthesizedData
Setting. We use the example of solving arithmetic tasks with a transformer, which offers an
interpretablesettingtounderstandthegenerationqualitysincewehaveaclearmetricforerrorand
an attainable ground truth. Specifically, we leverage the problems described in Charton (2022).
Transformers(Vaswanietal.,2017)aretrainedtopredictthefiveeigenvaluesof5×5symmetric
realmatricesfrominputsconsistingofthe25realentries. Alltraining,test,andsynthesizeddata
are generated by sampling matrices with independent entries from U[−10,10]. A prediction is
consideredcorrectiftherelativeerrorintheL1normisbelowacertaintoleranceτ. Thesynthesized
datageneratoristrainedonalimitedsampleofabout200,000exampleswithAdamfor65epochs
beforeoverfitting. DetailsonthetokenizerandoptimizationcanbefoundinAppendixB.
SelectionisCrucial. InTable1,wereporttheaccuracy(onatestset)ofgeneratedpredictionsusing
greedydecodingandbeamsearchofvarioussizes.Ontherightside,onlythebestbeamsolution—the
mostconfidentsolutionbythemodel—isevaluated. Increasingthenumberofbeamsdoesnotleadto
anincreaseinaccuracy,indicatingthatself-selectiononthepredictiondoesnotresultinimproved
predictions. However,ontheleftside,weevaluatealltop-kcandidatesinthebeamkwithrespectto
thegroundtruth,andthebestoneiscountedtowardstheaccuracy. Whengoingfromgreedy(beam
1)tobeam50,theaccuracyimprovesfrom66.9%to90.4%withτ =2%. Therefore,weconclude
thatwhilethemodeldemonstratesthepotentialtogenerateimprovedsolutions,itlackstheinherent
capabilitytoautonomouslyselectsuperiorpredictions. Tocuratebettersynthesizeddata,external
supervisioniscrucial.
7
rorrE
evitaleR
rorrE
evitaleR
rorrE
evitaleR5.2 TransformerforMath
Wenowmovetogeneratingsynthesizeddatawiththisproblem. Werandomlycollectmoreprompts
(matrices) and use the generator to label them. We introduce a verifier that serves as the oracle
supervision,measuringthedistancebetweenmodelpredictionsandthecorrectsolutions. Thedatais
selectedwiththisverifierviatwomethods:
• DataSelection:Arandomsetofmatricesiscreated,andthegeneratorcomputestheeigenvalues
usinggreedydecoding. Onlydatawiththecorrectpredictions(withinatoleranceofτ =2%,
accordingtotheverifier)areretained.
• LabelSelection: Arandomsetofmatricesiscreated, andthegeneratorpredictsk possible
solutionsusingbeamsearch(inthetop-kgenerationpool).Theverifierselectsthebestprediction,
whichisthenusedforthetrainingdata. Weexperimentwithbeamsizesof5,10,25,35,and50.
Overall,sevensynthesizeddatasets,eachcontainingonemillionexamples,arecreated: oneusing
DataSelection,fiveusingLabelSelectionwithvariousbeamsizes,andonewithoutanyselection.
IntheDataSelectionsetting,approximatelytwo-thirdsofthedataareretainedwithatoleranceof
τ = 2%. Usingthesedatasets,thetransformeristrainedfromscratchandevaluatedwithgreedy
decoding. TheaccuracyofthetrainedmodelsisreportedinTable2,showingthebestrunacrossfive
seeds. Theseresultsarecomparedwiththe‘SynthesizedGenerator’rowfromTable1toassessthe
performanceofthegeneratorandasanindicatorofgenerationquality. Weobservethefollowing:
Modelcollapseisobserved. Comparing‘Greedywithoutselection’and‘SynthesizedGenerator’,
trainingwithitsownsynthesizeddataleadstoadegradationinperformance,evenwithfivetimes
moresynthesizeddatathantheamountusedtotrainthegenerator. Modelcollapsehappens.
Supervisiongoesbeyondmodelcollapse. Whenweleveragereinforcement,bothdataselection
andlabelselectionshowconsiderableimprovementcomparedtousingthegenerateddatawithout
selection. Accordingtotheory,theeffectiveψ/ϕismuchlower. Alltheselectionresultssurpassthe
synthesizedgenerator,indicatingthatwecanimproveupontheoriginaldatawithoraclereinforcement
andmitigatemodelcollapse. Additionally,increasingthenumberofbeamsconsistentlyenhances
performance,asthequalityoftheselectedsynthesizeddatacontinuestoimprove.
HowFarCanWeGowithSynthesizedDataandVerifier? Weexaminedataselectionwith30
times more data and with a stricter verifier tolerance of 1% to investigate the best performance
achievable. AsshowninTable3,usingmoredataandastrongerselectionimprovesperformance. A
strongerselectioncorrespondstobetteroracle-basedverifier,aligningwiththetheoryandsimulation.
5.3 LLMsforNewsSummarization
WenowproceedtoempiricalevaluationsusingLlama-2-7B(Touvronetal.,2023)andLlama-3-8B
(Meta, 2024). Our experiments utilize the English summarization subset of the XLSUM dataset
(Hasanetal.,2021),whichincludes307,000trainingsamplesand11,500testsamples. Eachsample
inthisdatasetpairsanewsarticlewithaprofessionallyannotatedsummary.
Forourexperiments,Llama-2isfine-tunedon12.5%ofthetrainingdata. Thisfine-tunedmodel
servesasthegeneratorforcreatingsummariesacrosstheentiretrainingset,formingoursynthesized
dataset. Thisapproachallowsustoaugmenttheamountofdataavailableinthesynthesizeddataset.
Table2: PerformanceofModelsTrainedonVari-
Table1: TheGenerator’sAccuraciesforDifferent
ousSynthesizedData.Themodelsareevaluatedus-
BeamSizes. Left: allsolutionsinbeamareevaluated
inggreedydecoding.”SynthesizedGenerator”refers
andthebestiscalculated,selectionwithoracle.Right:
to the assessed performance of the generator as an
onlythebeamsolutionwiththesmallestperplexityis
indicatorongenerationquality.
evaluated,sameasself-selection.
Toleranceτ
Verify Verify
2% 1% 0.5%
allbeams thebestbeam
DataSelection2% 72.1 20.2 2.3
Toleranceτ 2% 1% 0.5% 2% 1% 0.5%
Beam50 84.0 33.4 4.9
Beam50 90.4 60.4 22.9 65.9 19.2 2.4
Beam25 79.9 28.7 4.1
Beam35 89.2 56.9 19.8 66.0 19.2 2.4 LabelSelection
Beam10 73.9 22.7 2.9
Beam25 88.0 53.2 16.8 66.1 19.3 2.4
Beam5 69.1 19.0 2.3
Beam10 83.7 43.1 10.5 66.2 19.5 2.5
Beam5 79.3 34.9 7.1 66.5 19.7 2.4 Greedyw/oselection 60.5 14.5 1.7
Greedy 66.9 20.2 2.4 66.9 20.2 2.4
SynthesizedGenerator 66.9 20.2 2.4
8Table3: PerformanceofModelsTrainedwithMoreDataandaStrongerVerifier.Synthesizeddatacurated
withoraclepruningat1%or2%tolerance.Trainfromscratchandevaluateusinggreedydecoding.
Toleranceτ
Method DataSize 2% 1% 0.5%
DataSelection2% 1M 72.1 20.2 2.3
DataSelection2% 12M 80.1 26.3 3.4
DataSelection1% 12M 95.1 50.1 8.6
All the finetuning is with full parameter tuning to better capture the scaling law as suggested in
Zhangetal.(2024). Throughoutallphasesofevaluationandgeneration,weemploygreedydecoding.
Giventhatnewssummarizationisalow-entropytask,greedydecodingischosentoensurequality
generation. Consistentwithcommonpractice,fine-tuningislimitedtoasingleepoch. Themodel’s
performanceisassessedusingtheRouge-1metric(Lin,2004)betweenitsgeneratedsummariesand
thetargetnewssummaries.
Inlinewithourtheory,weconsiderthreesettings:(1)SelectionwithOracle:WecalculatetheRouge
score between the generated summary and the ground truth summary, keeping the data with the
highestscores;(2)SelectionwithWeakSupervision: Weleverageafine-tunedLlama-3modelwith
higherperformancethanthegeneratorandkeepthedatawiththelowestperplexity;(3)Self-Selection:
We use the generator to keep the data with the lowest perplexity. We apply three selection rates:
12.5% (when the selected synthesized data is the same size as the original data used to train the
generator),25%,and50%. TheresultsareshowninFigure4andwehavethefollowingobservations:
Model Collapse. In Figure 4 Left, using the same amount of synthesized data results in worse
performancecomparedtousingtheoriginaldata,indicatingmodelcollapse(comparing‘Random
Selection’with‘Generator’). Onlywithmoredata,theRandomselectionlinesimproveandnearly
matchtheperformanceofthegenerator.
SelectionbyOracle. Employinganoracleforselectionyieldsthebestresults. Thedatasetcuratedin
thismannersurpassestheperformanceofthegeneratorinallsettings. Oracleselectionevensurpasses
themodeltrainedwith100%ofthetrainingsetandoriginallabels,withonly1/8ofthedataand
trainingcomputeinFigure4Left.
A Verifier Model with Higher Performance is Not Always Better. Self-selection surprisingly
leadstobetterperformancethanthegenerator. Wehypothesizethatittendstoselecteasy-to-learn
samples,therebyachievinggoodperformancewithfewerdatapoints. Incontrast,Llama-3results
in performance similar to random selection but worse than self-selection. This outcome aligns
withtheoreticalexpectations, wheretheeffectivenessofaweaksupervisordependsontheangle
betweenallthreevectors,asdiscussedinSection3.4. Althoughthemodelhashigherperformance,it
showslittlecorrelationwiththegenerator(θislarger),whichimpliesthatψ/ϕmightnotbebetter.
Fine-tunedLlama-2andtheoriginalLlama-2exhibitastrongcorrelationandcouldhaveanimproved
ψ/ϕ,makingtheselectionuseful. Conversely,inthelinearalgebraexperiment,improvingtheverifier
leads to better performance, likely because the verifier is still based on the oracle with different
thresholdsinsteadofbiases,resultinginaconsistentlysmallerθ andψ/ϕ. Therefore,itiscrucial
gen
tocarefullychoosethesupervisionmodelifanoracleisnotavailableinpractice. Wesuggestfirst
usingthebestmodelyouhave,andthenselectingacorrelatedmodelwithstrongperformance.
6 DiscussionandLimitation
Inthispaper,weconsideranovelproblemrelatedtosynthesizeddata: howtopreventmodelcollapse
throughdataselection. Weproposetoleveragefeedbackfromaverifiertoreinforcethesynthesized
data. We emphasize that when training new models with synthesized data, it is crucial to focus
notonlyonthequalityofthegeneratorbutalsoonhavingahigh-qualityverifiertoselectthedata.
Specifically,undertheassumptionofalinearmodelandGaussianmixtures,wetheoreticallyanalyze
theimpactofthegeneratorandverifierontheperformanceofthefinaltrainedmodel. Furthermore,
throughthreesolidexperiments,includingsimulation,mathematics,andnewssummarization,we
demonstratethatusingadecenthigh-qualityselectorfordataselectioncanpreventmodelcollapse.
Our work is of significant theoretical and practical importance in the era of large models with
increasinguseofgenerateddata.
Onelimitationofthisstudyisthatweonlyconsidereddataselectionasameansofdatacuration.
Besidesdataselection,datacurationalsoincludesmethodssuchasdataaugmentation,dataregen-
9Oracle Selection Llama3 Selection Self Selection Random Selection Generator Full Origin
Select 12.5% Select 25.0% Select 50.0%
0.34 0.34 0.34
0.33 0.33 0.33
0.32 0.32 0.32
0.31 0.31 0.31
0.30 0.30 0.30
0.29 0.29 0.29
103 104 103 104 103 104 105
Number of Synthesized Data Number of Synthesized Data Number of Synthesized Data
Figure4: Resultsofnewssummarizationexperiments: theleftfigurerepresentsmodelstrainedon
12.5%ofthedata, themiddleon25%ofthedata, andtherighton50%ofthedata. Eachfigure
includesfourcurvesillustratingdifferenttrainingscenarios: (1)selectionwithoraclereinforcement,
(2) selection with Llama-3 as a weak reinforcement, (3) self-selection by the generator, and (4)
randomselection. Additionally,twohorizontallinesareincludedforcomparison: onerepresenting
theRougescoreofthegeneratormodelandtheotherrepresentingtheRougescoreofamodeltrained
with100%datawithoriginallabels,servingastheoptimalline.
eration,andweighting. Theexplorationofgeneraldatacurationmethodstoavoidmodelcollapse
isleftforfuturework. Ourexperimentsalsodidnotconsidertheimpactofpromptengineeringon
thegenerator. Thiscansignificantlyenhancethegenerationquality, andaccordingtotheoretical
predictions,itwouldbebeneficialforsynthesizeddata.
Acknowledgements
YF and JK acknowledge support through NSF NRT training grant award 1922658. YF and PY
wouldliketothankYanzhuGuo,DiHe,ZhenyuHefordiscussionsandsuggestions. Thisworkwas
supportedinpartthroughtheNYUITHighPerformanceComputingresources,services,andstaff
expertise.
10
1-EGUOR 1-EGUOR 1-EGUORReferences
JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoniAleman,
DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technicalreport.
arXivpreprintarXiv:2303.08774,2023.
Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang,
NiklasMuennighoff,BairuHou,LiangmingPan,HaewonJeong,etal. Asurveyondataselection
forlanguagemodels. arXivpreprintarXiv:2402.16827,2024.
Sina Alemohammad, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed Imtiaz Humayun, Hossein
Babaei, DanielLeJeune, AliSiahkoohi, andRichardG.Baraniuk. Self-consuminggenerative
modelsgomad. arXivpreprintarxiv:2307.01850,2023.
ZeyuanAllen-ZhuandYuanzhiLi. Towardsunderstandingensemble,knowledgedistillationandself-
distillationindeeplearning.InTheEleventhInternationalConferenceonLearningRepresentations,
2022.
SamAltman.openainowgeneratesabout100billionwordsperday.allpeopleonearthgenerateabout
100trillionwordsperday. https://x.com/sama/status/1756089361609981993?lang=en,
2024.
Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, and David J. Fleet.
Syntheticdatafromdiffusionmodelsimprovesimagenetclassification. TransactionsonMachine
LearningResearch,2023. ISSN2835-8856.
YuntaoBai,SauravKadavath,SandipanKundu,AmandaAskell,JacksonKernion,AndyJones,Anna
Chen,AnnaGoldie,AzaliaMirhoseini,CameronMcKinnon,etal. Constitutionalai:Harmlessness
fromaifeedback. arXivpreprintarXiv:2212.08073,2022.
QuentinBertrand,AvishekJoeyBose,AlexandreDuplessis,MarcoJiralerspong,andGauthierGidel.
On the stability of iterative retraining of generative models on their own data. arXiv preprint
arxiv:2310.00429,2023.
MatyasBohacekandHanyFarid. Nepotisticallytrainedgenerative-aimodelscollapse,2023.
MartinBriesch,DominikSobania,andFranzRothlauf. Largelanguagemodelssufferfromtheirown
output: Ananalysisoftheself-consumingtrainingloop,2023.
MaxFBurg,FlorianWenzel,DominikZietlow,MaxHorn,OsamaMakansi,FrancescoLocatello,and
ChrisRussell. Imageretrievaloutperformsdiffusionmodelsondataaugmentation. Transactions
onMachineLearningResearch,2023. ISSN2835-8856.
Franc¸oisCharton. Linearalgebrawithtransformers. arXivpreprint2112.01898,2022.
RudrajitDasandSujaySanghavi. Understandingself-distillationinthepresenceoflabelnoise. In
InternationalConferenceonMachineLearning,pages7102–7140.PMLR,2023.
ElvisDohmatob,YunzhenFeng,andJuliaKempe.Modelcollapsedemystified:Thecaseofregression.
arXivpreprintarXiv:2402.07712,2024a.
ElvisDohmatob,YunzhenFeng,PuYang,FrancoisCharton,andJuliaKempe. Ataleoftails: Model
collapseasachangeofscalinglaws. arXivpreprintarXiv:2402.07043,2024b.
BinDong,JikaiHou,YipingLu,andZhihuaZhang. Distillation∼earlystopping? harvestingdark
knowledgeutilizinganisotropicinformationretrievalforoverparameterizedneuralnetwork. arXiv
preprintarXiv:1910.01255,2019.
Lisa Dunlap, Alyssa Umino, Han Zhang, Jiezhi Yang, Joseph E Gonzalez, and Trevor Darrell.
Diversifyyourvisiondatasetswithautomaticdiffusion-basedaugmentation. AdvancesinNeural
InformationProcessingSystems,36,2023.
RonenEldanandYuanzhiLi.Tinystories:Howsmallcanlanguagemodelsbeandstillspeakcoherent
english? arXivpreprintarXiv:2305.07759,2023.
11TommasoFurlanello,ZacharyLipton,MichaelTschannen,LaurentItti,andAnimaAnandkumar.
Bornagainneuralnetworks. InInternationalconferenceonmachinelearning,pages1607–1616.
PMLR,2018.
SuriyaGunasekar,YiZhang,JyotiAneja,CaioCe´sarTeodoroMendes,AllieDelGiorno,Sivakanth
Gopi,MojanJavaheripi,PieroKauffmann,GustavodeRosa,OlliSaarikivi,etal. Textbooksareall
youneed. arXivpreprintarXiv:2306.11644,2023.
Yanzhu Guo, Guokan Shang, Michalis Vazirgiannis, and Chloe´ Clavel. The curious decline of
linguisticdiversity: Traininglanguagemodelsonsynthetictext,2023.
PatrickHaluptzok,MatthewBowers,andAdamTaumanKalai. Languagemodelscanteachthem-
selvestoprogrambetter. InTheEleventhInternationalConferenceonLearningRepresentations,
2022.
TahmidHasan,AbhikBhattacharjee,Md.SaifulIslam,KaziMubasshir,Yuan-FangLi,Yong-Bin
Kang, M. Sohel Rahman, and Rifat Shahriyar. XL-sum: Large-scale multilingual abstractive
summarizationfor44languages. InFindingsoftheAssociationforComputationalLinguistics:
ACL-IJCNLP 2021, pages 4693–4703, Online, August 2021. Association for Computational
Linguistics.
RyuichiroHataya, HanBao, andHiromiArai. Willlarge-scalegenerativemodelscorruptfuture
datasets? InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision(ICCV),
pages20555–20565,October2023.
Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song Bai, and XI-
AOJUANQI. ISSYNTHETICDATAFROMGENERATIVEMODELSREADYFORIMAGE
RECOGNITION? InTheEleventhInternationalConferenceonLearningRepresentations,2023.
Reyhane Askari Hemmat, Mohammad Pezeshki, Florian Bordes, Michal Drozdzal, and Adriana
Romero-Soriano. Feedback-guideddatasynthesisforimbalancedclassification. arXivpreprint
arXiv:2310.00158,2023.
GeoffreyHinton,OriolVinyals,andJeffDean. Distillingtheknowledgeinaneuralnetwork. arXiv
preprintarXiv:1503.02531,2015.
ShamMKakade,KarthikSridharan,andAmbujTewari. Onthecomplexityoflinearprediction: Risk
bounds,marginbounds,andregularization. Advancesinneuralinformationprocessingsystems,
21,2008.
DiederikPKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. arXivpreprint
arXiv:1412.6980,2014.
AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,Tete
Xiao,SpencerWhitehead,AlexanderCBerg,Wan-YenLo,etal.Segmentanything.InProceedings
oftheIEEE/CVFInternationalConferenceonComputerVision,pages4015–4026,2023.
GermainKolossov,AndreaMontanari,andPulkitTandon. Towardsastatisticaltheoryofdataselec-
tionunderweaksupervision.InTheTwelfthInternationalConferenceonLearningRepresentations,
2024.
BenjaminLeBrun,AlessandroSordoni,andTimothyJO’Donnell.Evaluatingdistributionaldistortion
inneurallanguagemodeling. InInternationalConferenceonLearningRepresentations,2021.
HarrisonLee,SamratPhatale,HassanMansoor,KellieLu,ThomasMesnard,ColtonBishop,Victor
Carbune,andAbhinavRastogi. Rlaif: Scalingreinforcementlearningfromhumanfeedbackwith
aifeedback. arXivpreprintarXiv:2309.00267,2023.
JunnanLi,DongxuLi,CaimingXiong,andStevenHoi. Blip: Bootstrappinglanguage-imagepre-
trainingforunifiedvision-languageunderstandingandgeneration. InInternationalconferenceon
machinelearning,pages12888–12900.PMLR,2022.
Chin-YewLin. Rouge: Apackageforautomaticevaluationofsummaries. InTextsummarization
branchesout,pages74–81,2004.
12Gonzalo Mart´ınez, Lauren Watson, Pedro Reviriego, Jose´ Alberto Herna´ndez, Marc Juarez, and
RikSarkar. Combininggenerativeartificialintelligence(ai)andtheinternet: Headingtowards
evolutionordegradation? arXivpreprintarxiv: 2303.01255,2023a.
GonzaloMart´ınez,LaurenWatson,PedroReviriego,Jose´ AlbertoHerna´ndez,MarcJuarez,andRik
Sarkar. Towardsunderstandingtheinterplayofgenerativeartificialintelligenceandtheinternet.
arXivpreprintarxiv: 2306.06130,2023b.
DavidMcAllester.Simplifiedpac-bayesianmarginbounds.InLearningTheoryandKernelMachines.
SpringerBerlinHeidelberg,2003.
Meta. Introducingmetallama3: Themostcapableopenlyavailablellmtodate. https://ai.meta.
com/blog/meta-llama-3/,2024.
HosseinMobahi,MehrdadFarajtabar,andPeterBartlett. Self-distillationamplifiesregularizationin
hilbertspace. AdvancesinNeuralInformationProcessingSystems,33:3351–3361,2020.
OpenAI. Video generation models as world simulators. https://openai.com/index/
video-generation-models-as-world-simulators/,2024.
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,Chong
Zhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Traininglanguagemodelstofollow
instructionswithhumanfeedback. Advancesinneuralinformationprocessingsystems,35:27730–
27744,2022.
VishakhPadmakumarandHeHe. Doeswritingwithlanguagemodelsreducecontentdiversity? In
InternationalConferenceonLearningRepresentations(ICLR),2024.
BaolinPeng,ChunyuanLi,PengchengHe,MichelGalley,andJianfengGao. Instructiontuningwith
gpt-4. arXivpreprintarXiv:2304.03277,2023.
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark
Chen,andIlyaSutskever. Zero-shottext-to-imagegeneration. InMarinaMeilaandTongZhang,
editors,Proceedingsofthe38thInternationalConferenceonMachineLearning,volume139of
ProceedingsofMachineLearningResearch,pages8821–8831.PMLR,18–24Jul2021.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo¨rn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFConfer-
enceonComputerVisionandPatternRecognition(CVPR),pages10684–10695,June2022.
MohamedElAmineSeddik,Suei-WenChen,SoufianeHayou,PierreYoussef,andMerouaneDebbah.
Howbadistrainingonsyntheticdata? astatisticalanalysisoflanguagemodelcollapse. arXiv
preprintarXiv:2404.05090,2024.
Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning - From Theory to
Algorithms. CambridgeUniversityPress,2014.
Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Ander-
son. Thecurseofrecursion: Trainingongenerateddatamakesmodelsforget. arXivpreprint
arxiv:2305.17493,2023.
BenSorscher,RobertGeirhos,ShashankShekhar,SuryaGanguli,andAriMorcos. Beyondneural
scaling laws: beating power law scaling via data pruning. Advances in Neural Information
ProcessingSystems,35:19523–19536,2022.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfoundation
andfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and Thang Luong. Solving olympiad geometry
withouthumandemonstrations. Nature,625(7995):476–482,2024.
SoobinUm,SuhyeonLee,andJongChulYe. Don’tplayfavorites: Minorityguidancefordiffusion
models. InTheTwelfthInternationalConferenceonLearningRepresentations,2024.
13Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
LukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. InAdvancesinNeuralInformation
ProcessingSystems,pages6000–6010,2017.
YizhongWang,YeganehKordi,SwaroopMishra,AlisaLiu,NoahASmith,DanielKhashabi,and
HannanehHajishirzi. Self-instruct: Aligninglanguagemodelswithself-generatedinstructions. In
Proceedingsofthe61stAnnualMeetingoftheAssociationforComputationalLinguistics(Volume
1: LongPapers),pages13484–13508,2023.
YuxiangWei,ZheWang,JiaweiLiu,YifengDing,andLingmingZhang. Magicoder: Sourcecodeis
allyouneed. arXivpreprintarXiv:2312.02120,2023.
KevinYang,DanKlein,AsliCelikyilmaz,NanyunPeng,andYuandongTian. RLCD:Reinforcement
learningfromcontrastivedistillationforLMalignment. InTheTwelfthInternationalConference
onLearningRepresentations,2024.
WeizheYuan,RichardYuanzhePang,KyunghyunCho,SainbayarSukhbaatar,JingXu,andJason
Weston. Self-rewardinglanguagemodels. arXivpreprintarXiv:2401.10020,2024.
BiaoZhang,ZhongtaoLiu,ColinCherry,andOrhanFirat. WhenscalingmeetsLLMfinetuning: The
effectofdata,modelandfinetuningmethod. InTheTwelfthInternationalConferenceonLearning
Representations,2024.
TianyuZheng,GeZhang,TianhaoShen,XuelingLiu,BillYuchenLin,JieFu,WenhuChen,and
XiangYue. Opencodeinterpreter:Integratingcodegenerationwithexecutionandrefinement. arXiv
preprintarXiv:2402.14658,2024.
14A MoreWorksonSynthesizedData
A.1 TaxonomyforSynthesizedData
Contrarytothephenomenonofmodelcollapse,synthesizeddatahasbeenshowntoimproveper-
formance in numerous empirical studies. We now provide a taxonomy outlining when and how
synthesizeddataisbeneficial. Specifically,weidentifyfourkeycomponents: promptengineering
●,knowledgefromadvancedmodels▲,distributionalshiftandtargetedcuration■,andexternal
verifiers✦.Mostempiricalstudiescanbecategorizedbasedononeormoreofthesecomponents.
Weuse●▲■and✦todenotethecomponentseachreferenceleverages.
CodeandMath. Haluptzoketal.(2022)✦generatesynthesizeddataforcodesanduseaverifier
to filter and show that the model can ”self-improve” with its own synthesized data. Gunasekar
etal.(2023)●■filterhigh-qualitydatafromthewebandpromptGPT-3.5withaspeciallycurated
promptsetcoveringbothqualityanddiversity. Weietal.(2023)●leverageadiverseandlargeset
ofopen-sourcecodesnippetstocuratecodeinstructionsaspromptswithgoodcoverageandhigh
quality. Zhengetal.(2024);Trinhetal.(2024)✦leverageasymbolicdeductionengineasaverifier
totestthecorrectnessofeachbranchforsolvingOlympicgeometryproblems.
Alignment. Duringstandardfine-tunings,synthesizeddataisoftengeneratedbyastrongermodel
like GPT-4 (Peng et al., 2023) ▲. Wang et al. (2023) ●✦use a good set of prompts and inputs
withaheuristicverifiertofilteroutlow-qualityonesandmaintainhighdiversity. Baietal.(2022)
●■usethemodelitselftocritiquewhetheritsowngenerationisharmful, givenalreadyharmful
promptswithgoldstandardsfromhumans. Foralignmentwithreinforcementlearning,Ouyangetal.
(2022)●✦usehumansasverifierstocomparesynthesizeddatageneratedbythecurrentmodelwith
a good set of prompts. Some papers propose reinforcement learning with AI feedback (RLAIF)
(Leeetal.,2023)●■thatleveragesanotherLLMastheverifiertomatchhumanverification. The
verifierisastrongermodel,instruct-tunedPalm2L,whilethenetworkbeingtrainedisthePalm2
XS.However,Yangetal.(2024)●laterfoundthatusingbetterprompts(self-improve)thatdirect
harmfulorharmlessresponsescansurpassRLAIF.Yuanetal.(2024)●achievesurprisingresults
withiterativefine-tuningandgeneratinggoodpromptswithin-contextlearning.
Knowledgedistillation. Mostpapersintheknowledgedistillationareainvolveusingabettermodel
to distill for general performance or specific tasks, with ●, ▲, and ■involved from case to case.
Oneexampleisthetinystorycases(EldanandLi,2023)●▲,whereGPT-4ispromptedtogenerate
storiesforfour-year-oldsthatareusedtotrainGPT-Neowithgoodperformance.
ImageDomain.Kirillovetal.(2023)andLietal.(2022)■useadistributionalshiftfromhigh-quality
tolow-qualitydatatolabelandcurateavastamountofunlabeleddata. Specifically,Lietal.(2022)
alsotrainsaverifiertofiltershigh-qualitydata. (Umetal.,2024)▲■specificallycurateminority
groups with a diffusion model to enhance performance. He et al. (2023); Dunlap et al. (2023)
▲■generatesynthesizeddatathataidsinclassificationtasksbytailoringthesynthesizeddatato
matchthemodel’slearningobjectives. Azizietal.(2023);Hemmatetal.(2023)▲■employguided
curation(withsupervision)tocuratedatafromdiffusionmodels. Burgetal.(2023)findthatwhile
synthesized data from a diffusion model helps improve downstream tasks, such as classification,
usingthepre-trainingdataofthediffusionmodelalonegivesevenstrongerperformance.
A.2 KnowledgeDistillationwithSoftLabels
Relatedtosynthesizeddata,thereisalonghistoryofusingsynthesizedlabelsinimageclassifica-
tions. Inthedomainsofself-distillationandknowledgedistillation(Hintonetal.,2015;Furlanello
etal.,2018),datawithsoftlabelsgeneratedfromtheteachermodelcansignificantlyimprovethe
performanceofthestudentmodel. Thesesoftlabelsconveyadditionalinsights—referredtoas’dark
knowledge’—thathavebeentheoreticallylinkedtospecificadvantageousadaptations. Theseinclude
implicitbiasesthatmitigateoverfitting(Mobahietal.,2020),mimicryofearlystopping(Dongetal.,
2019)forimprovedoptimizationunderlabelnoise(DasandSanghavi,2023),andadjustmentsto
accommodatespecificdatastructures(Allen-ZhuandLi,2022). Weonlyconsidersynthesizeddata
withfixedlabelsasinthecurrentpracticeofLLMsanddiffusionmodels.
15A.3 DataSelection
ComprehensivesurveysondataselectionforlanguagemodelscanbefoundinAlbalaketal.(2024),
along with theoretical studies on selection in high-dimensional settings (Sorscher et al., 2022;
Kolossovetal.,2024). Specifically,Kolossovetal.(2024)alsoexploretheuseofsurrogatemodels
for producing labels during selection, followed by curation of the original labels. In our study,
selectionisappliedtosynthesizeddatawhereoriginallabelsarenotavailable,resultingindistinct
phenomenacomparedtotheseapproachesonoriginaldata.
B PredictingtheEigenvalues
We leverage the code base provided by Charton (2022) at https://github.com/
facebookresearch/LAWTunderthelicenseCCBY-NC4.0.
InputandTokenization. Transformersaretrainedtopredicttheeigenvaluesof5×5symmetric
realmatrices. Modelinputsaresequencesof25realentries,roundedtothreesignificantdigits,and
tokenizedastripletsofsigns(+or-),mantissas(from0to999)andpoweroftenexponents(from
E-100toE100). Forinstance,the2×2matrix,
(cid:18) (cid:19)
2.3 0.6035
0.6035 −3.141
will be encoded as the sequence of 12 tokens: + 23 E-1 + 604 E-3 + 604 E-3 - 314 E-2.
Modeloutputsarevectorsof5realeigenvalues,roundedtothreesignificantdigits,andtokenizedas
before,astripletsofsign,mantissaandexponent(theP1000encodingfrom(Charton,2022)).
ModelandOptimization. Wetrainsequence-to-sequencetransformers(Vaswanietal.,2017),with
4layersintheencoder,andoneinthedecoder,512dimensionsand8attentionheads,tominimizea
cross-entropyloss,usingtheAdamoptimizer(KingmaandBa,2014),withafixedlearningrateof
5·10−5,afteraninitiallinearwarm-upphaseoverthefirst10,000optimizationsteps. Fortheresults
inTable3,themodelistrainedwith12Msamplesfor400epochsbeforeoverfitting.
Evalution. Model accuracies are measured on a held-out test set of examples not seen at train-
ing. Model predictions are evaluated by decoding the output sequence as a vector of 5 real
numbers (p ,p ,p ,p ,p ), and assessing that a prediction p(p ,p ,p ,p ,p ) of eigenvalues
1 2 3 4 5 1 2 3 4 5
v(v ,v ,v ,v ,v )iscorrectiftherelativeerrorinL1normsisbelowsometoleranceτ,i.e. if
1 2 3 4 5
5 5
(cid:88) (cid:88)
|v −p |<τ |v |.
i i i
i=1 i=1
Weusetolerancesτ of5,2,1and0.5%.
B.1 FinetuningModelswithSynthesizedData
Inallpreviousexperiments,thedatageneratedfromthegenerator(usingbeamorrejectsampling)
wereusedtotrainanewmodel. Inthissection,weconsiderusingdatageneratedfromthegenerator
tofinetunemodelspre-trainedonasmallsampleofgroundtruthdata. Weconsiderfourcases:
• Fine-tuningthegenerator(ModelA).
• Fine-tuningamodelpre-trainedtothesameaccuracyasthegenerator(62%,ModelB).
• Fine-tuningamodelpre-trainedtohigheraccuracy(93%,ModelC).
• Fine-tuningamodelpre-trainedtolowaccuracy(4%,ModelD).
Table 4 compares accuracy of the four fine-tuning cases to that of a model trained from scratch.
Fine-tuningonlyachiuevesbetterperformancewhenthepre-teainedmodelachievedhigheraccuracy
thanmodelA.Inallothercases,fine-tuningbringsnoimprovement. Notethatfine-tuningmodelA
onitsowngenerateddataachievestheworstresult,aclearcaseofmodelcollapse.
16Table4: Performanceofmodelsfine-tunedon1Mexamplesgeneratedbythegenerator.τ =2%
ModelA(66%) ModelB(62%) ModelC(93%) ModelD(4%) Fromscratch
Rejection 61.8 72.9 82.1 66.3 72.1
Beam50 74.1 82.6 87.3 78.3 84.0
Beam35 72.7 81.3 86.8 76.8 80.4
Beam25 71.3 79.8 84.4 73.3 79.9
Beam10 67.5 75.1 83.5 68.0 73.9
Beam5 64.9 70.8 80.1 65.6 69.1
Beam1 61.6 62.1 75.6 55.8 60.5
B.2 ComputationalResources
WeleverageaV100GPUwith32GBofmemoryforallexperimentsinvolvinglinearalgebra. The
trainingtimerangesfrom1to5days,dependingonthedatasizeandthenumberofepochs. For
resultsinTable3,ittakes5daystotrainon12milliondatapointsfor400epochs.
C NewsSummarization
WeleveragetheXLSUMdataset(Hasanetal.,2021)athttps://huggingface.co/datasets/
csebuetnlp/xlsumunderthelicenseCC-BY-NC-SA4.0.
Datapreprocessing. Foreachdatainbothtrainingandtestdataset,itconsistsofanewsreportanda
summarization,denotedas(news,summarization). Wewriteeachdatainthefollowingform:
Article: news. Asummaryofthearticle: summarization.
Implementation details. Weleverage the officialimplementation in Huggingface 1 for training,
underthelicenseApache2.0. Specifically,fortrainingthegenerator,westartourtrainingwiththe
pre-trained llama-2, and set the learning rate to 5e-5, the learning rate scheduler as ‘cosine’, the
numberofepochsto1,thetotalbatchsizeto32,theblocksizeto1024andtheotherstothedefault
value. Forgeneratingthesynthesizeddata,weusegreedystrategytogenerateasummarizationfor
eachnewsinthetrainingset. Fortrainingbasedontheselectedsynthesizeddata,wealsostartour
trainingwiththepre-trainedllama-2,andsetthelearningrateto2e-5,thelearningratescheduler
as‘constant’andtheotherstothesame. Forevaluation, wefirstusegreedystrategytogenerate
a summarization for each news in the test set, and then calculate the Rouge-1 score between the
generatedsummarizationandthecorrespondinggroundtruth,andfinallyreporttheaverageofthe
Rouge-1scoresofalltestdata. Whencalculatingtheperplexity,weonlycalculatetheperplexityfor
thegeneratedsummary.
ComputationalResources. Allexperimentswereconductedusingadedicatedcomputationalcluster
equippedwith4NVIDIAA800GPUs,eachwith80GBofmemory. Ourtrainingandinference
processesareperformedonthecluster.
EstimatedTime. Trainingthewholedatasetforanepochtakesabout6hours. Generatingthewhole
datasettakesabout1day. Duringevaluation,weneedtofirstgenerateandcalculatetherougescore,
whichtakesaround40minutesforonecheckpoint.
D AGeneralTheoryofPruningwithReinforcement
InSection3wehavepresentedaspecialcaseofourgeneraltheory,whichwedescribehereinmore
generalityanddetail. WhilesomeofourexpositionhereoverlapswithSection3,weprefertoleave
itasacompletetextthatprovidesastand-aloneoverview.
1https://github.com/huggingface/transformers/blob/main/examples/pytorch/
language-modeling/run_clm.py
17D.1 DataDistribution
ConsideraprobabilitydistributionP overRd×{0,1}withthefollowinghigh-dimensionalproperty
ConditionD.1. GivenN ≤N(d)iidsamples(x ,y ),...,(x ,y )fromP withN ≤N(d),the
1 1 N N
followingholdestimatesw.p1−o(1)uniformlyonalli,j ∈[N],inthelimitd→∞
∥x ∥2 ≃1,
i
(cid:26)
a, ify =y ,
x⊤x ≃ i j
i j b, ify ̸=y
i j
whereb<a<1areconstants. Forsimplicityofpresentationofourresults,Wewillfurtherassume
thatb=−aorb=0.
TheabovestructuralconditionisinspiredbyanassumptioninDasandSanghavi(2023).
Forsimplicityofexposition,wewillonlyconsiderbalanceddistributions,meaningthat
P(y =1)=P(y =0)=1/2, for(x,y)∼P.
Gaussian Mixture Example. As a first example, in the case of Gaussian mixtures where the
featureshaveconditionaldistributiongivenby
x|y ∼N(µ ,Σ), (7)
y
(8)
whereµ =(2y−1)µ,forsomeµ∈Rdandpositive-definitematrixΣwithE∥x∥2 =∥µ∥2+trΣ=
y
1,wemaytake
a=∥µ∥2, b=−a. (9)
ConditionD.1thenholdsthankstoconcentration,withN(d)=eΘ(d).
D.2 TrainingData,DataPruning,andDownstreamModel
LetD ={(x ,y ),...,(x ,y )}beadatasetofN iidpairsfromthetruedistributionP andlet
N 1 1 N N
D′ ={(x ,y′),...,(x ,y′ )}aversionofthedataset(alsoiid)withlabelsy′ insteadofy . For
N 1 1 N N i i
example,thiscouldbelabelsgeneratedbyanAItryingtoreproducereal-worlddata. D′ isthedata
N
onwhichthedownstreammodelistrained.
Wewillconsiderafamilyofmodelsgivenby
P(y =1|x,w)=yˆ:=σ(x⊤w)∈(0,1),
parametrizedbyavectorofweightsw ∈Rd. Here,σisthesigmoidfunctiondefinedby
1
σ(z):= . (10)
1+e−z
Forthelossfunction,weusebinarycross-entropy(BCE),definedby
ℓ(yˆ,y)=−ylogyˆ−(1−y)log(1−yˆ). (11)
Letw beobtainedvialogisticregressionfittedonD′ withridgeregularizationparameterλ>0.
(cid:98)N N
Thus,wˆistheunique2minimizerofthefollowingobjectivefunction
N
1 (cid:88) λ
L(w):= q ℓ(σ(x⊤w),y′)+ ∥w∥2.
N i i i 2
i=1
Hereq isabitwhichindicateswhethertheithtrainingexamplehassurvivedpruning. Thenumbers
i
q =(q 1,...,q N)iscalledapruningstrategy.Thecorrespondingdownstreamclassifierisf(cid:98)N =f w(cid:98)N,
wherethenotationf referstothelinearclassifierinducedbyaweightsvectorw ∈Rd,i.e
w
(cid:26) 1, ifx⊤w >0,
f (x):= (12)
w 0, otherwise.
2Unicityisduetostrongconvexityofobjectivefunction.
18Thetestaccuracyofthedownstreammodelf(cid:98)N isdefinedby
acc(f(cid:98)N):=P(f(cid:98)N(x)=f Bayes(x)), forarandomtestpoint(x,y)∼P,
wheref (z):=E[y|x=z]istheBayes-optimalclassifier.Inparticular,notethatacc(f )=
Bayes Bayes
100%byconstruction.
Thisquantitywillbethemainobjectofouranalysis,andwewillbeinterestedinhowitdependson
thecorruptionlevelpandthechoiceofpruningstrategyq,intheinfinite-samplelimitN →∞.
Forlaterreference,wealsodefineanempiricalversion,namelytheaccuracyoff(cid:98)N evaluatedonthe
cleandatasetD ,namely
N
1
a (cid:99)cc(f(cid:98)N):= |M||{i∈M |f(cid:98)N(x i)=y i}|, (13)
whereM :={i∈[N]|q =1}collectstheindicesoftrainingsampleswhichsurvivepruningbyq.
i
D.3 AClassofParametrizedPruningStrategies
Givenhyper-parametersϕ ,ϕ ,ψ ,ψ ∈[0,1],weconsiderabroadclassofparametrizedpruning
0 1 01 10
strategies with the following property. For any class labels k,ℓ ∈ {0,1}, the random variables
(z ) definedbyz =1[y =k,y′ =ℓ,q =1]areiidwithBernoullidistributionBern(p ),
ikℓ i∈[N] ikℓ i i i kℓ
where
p =P(q =1,y′ =ℓ,y =k)
kℓ i i i
=P(q =1|y′ =ℓ,y =k)P(y′ =ℓ|y =k)P(y =k)
i i i i i i (14)
(cid:26)
ϕ (1−p)/2, ifk =ℓ,
= k
ψ p/2, else.
kℓ
andthenumbersp,ϕ andψ aredefinedby
k kℓ
p:=P(y′ ̸=y ), ϕ =P(q =1|y′ =k,y =k), ψ =P(q =1|y′ =ℓ,y =k). (15)
i i k i i i kℓ i i i
Consequently, ifN isthenumberoftrainingexamplesthathavetruelabelk, fakelabelℓ, and
kℓ
survivepruning,then
N
(cid:88)
N := z (16)
kℓ ikℓ
i=1
whichishasbinomialdistributionBin(N,p ). Asmentionedinthemaintext,forsimplicityof
kℓ
expositionweconsideredthefollowingsimplifyingassumption
ϕ =ϕ =ϕ, ψ =ψ =ψ. (17)
1 0 01 10
SuchapruningstrategywillbereferredtoasanRLHF-typestrategywithparameter(ϕ,ψ). Asusual,
RLHFstandsforReinforcementLearningwithHumanFeedback. Itcanbelikenedtothesensein
whichthetermisclassicallyused,whereoneassumesaccesstoastrongoracle(e.gahuman)who
cantellapartbugguouspredictions,butcanalsomakemistakes.
RemarkD.2. Notethattheparametrization(ϕ,ψ)and(p ,p )describethesameRLHF-type
00 11
policyviathefollowingbijectivetransformation.
p =p =(1−p)ϕ/2, p =p =pψ/2. (18)
00 11 01 10
D.4 Examples
LetuspresentsomenotableexamplesofpruningRLHF-typepruningstrategies.
NoPruning. Thecase(ϕ,ψ)=(1,1)correspondstonopruning,i.etheentiretrainingdatasetis
used.
PureRLHF. Thecase(ϕ,ψ)=(1,0). Thepruningstrategyonlykeepsindicescorrespondingto
examplesinthedatasetwhichhavecorrectlabel(allcorruptedlabelsdiscarded).
19Supervised ((Margin-Based) Pruning. Let w ∈ Rd, and consider the pruning strategy
prune
definedby
q =1[y′(x⊤w )>0].
i i i prune
Thispruningstrategysimplifyfiltersoutallexamplesonwhichitdisagreesontheassignedlabel.
D.5 PerformanceBoundsforModelsTrainedwithRLHFPruning
Figure5: EmpiricalConfirmationofTheoremD.3. Comparingthebreakdownpointsofdifferent
models. Here, the task is classifying a Gaussian mixture, with infinite training samples from
datasetsgeneratedfromamodelwithclassificationerrorratep(x-axis). Noticethesharpphrase-
transitions where the model suddenly switches from perfect accuracy to worse-than-chance, a
phenomenonpredictedbyTheoremD.3. Left. PerformanceofRLHF-typepruningstrategieswith
differentvaluesofthehyper-parameters(ϕ,ψ). Recallthatthecaseψ/ϕ = 1correspondstono
pruning, while ψ/ϕ = 0 corresponds to pure RLHF. Right. Comparing p , approximated with
up
sup{p | acc(f(cid:98)N) ≥ 90%} (computed empirically), against the analytic estimate p− ⋆(t) given in
TheoremD.3(fort=0.1). Again,theresultsareinexcellentagreementwiththepredictionsofthe
theorem.
Thefollowingisoneofourmainresults(provedinAppendixE).
TheoremD.3. SupposeConditionD.1isinorder. Fixϕ,ψ,t∈(0,1)anddefinep±(t)∈(0,1)by
⋆
1−t 1+t
p−(t):= , p+ := (19)
⋆ 1−t+(1+t)ψ/ϕ ⋆ 1+t+(1−t)ψ/ϕ
Ifp<p− ⋆(t),thenthelimitN →∞itholdsw.p1−o(1)thattheacc(f(cid:98)N)=100%foradownstream
modelf(cid:98)N trainedondatafromageneratorwitherrorratepprunedwithanRLHF-typestrategy
withparameters(ϕ,ψ).
Ontheotherhand,ifp>p+ ⋆,theninthelimitN →∞itholdsw.p1−o(1)thattheacc(f(cid:98)N)=0%
foradownstreammodelf(cid:98)N.
Thus, there is a sharp phase-transition around the corruption level p := 1/(1+ψ/ϕ): as p is
⋆
increasedpastlevelp ⋆,thedownstreammodelf(cid:98)N abruptlyswitchesfrombeingperfectlyaccurate,
toperfectlyinaccurate!
SeeFigure5foranempiricalillustrationofthetheorem.
Thethresholdsp±(t)appearingintheabovetheoremareproxiesfortheso-calledbreakdownpoints
⋆
p ≥p definedby
up down
(cid:110) (cid:12) a.s (cid:111)
p
up
=inf p∈[0,1](cid:12)acc(f(cid:98)N) → 0%inthelimitN →∞ , (20)
(cid:110) (cid:12) a.s (cid:111)
p
down
=sup p∈[0,1](cid:12)acc(f(cid:98)N) → 100%inthelimitN →∞ . (21)
TheoremD.3impliesp ≥p−(t)andp ≤p+(t)forallt∈(0,1). Consequently,
down ⋆ up ⋆
CorollaryD.4. UnderthehypothesesofTheoremD.3,itholdsthatp =p .
up down
20D.6 SomeConsequencesofTheoremD.3
We now present some illustrious applications of Theorem D.3. These examples are empirically
confirmedinFigure5.
NoPruning. Here,wehaveψ/ϕ=1andsothedownstreammodelachieves100%accuracyfor
allvaluesofcorruptionparameterpuptotheproxybreakdownpointpredictedbyTheoremD.3is
thenp− =1/2−t/2.
⋆
PureRLHF. Forthisscenario,ψ/ϕ=0andsoTheoremD.3predictsthatthedownstreammodel
f(cid:98)N achieves 100% accuracy for all values of corruption parameter p up to the breakdown point
p− =1. Thisisperhapsnotsosurprisinginhindsight. Thepointisthatevenformoderatelylarge
⋆
valuesofψ/ϕ,theproxybreakdownpointp−givenin(19)canstillbequitecloseto1.
⋆
Self-supervised(Margin-Based)Pruning. ConsiderGaussianmixturedatawithmeans±µ,and
consideramargin-basedpruningstrategyinEquation(5). Itisclearthatϕandψonlydependonall
the3anglesbetweenthesetofvectors{w ,w ,w },withw =µ.
∗ gen prune ∗
SupervisedPruning. ConsiderisotropicGaussianmixturedatawithmeans±µ,andapruning
strategyasinEq.(5). Theparameters(ϕ,ψ)onlydependontheanglesθ ,θ ,θ ∈[0,π]given
gen prune
by
θ :=∠(w ,µ), θ :=∠(w ,µ),
gen gen prune prune
(22)
θ :=∠(w ,w ).
prune gen
Conditioned on y = 1 and using E∥x∥2 = ∥µ∥2 + trΣ = 1 (and ∥µ∥ < 1), we get x ∼
2 2
N(µ, 1 I ). Wecanrewritexas:
(1−∥µ∥2)d d
2
1
x=µ+ η, (23)
(1−∥µ∥2)d
2
where η ∼ N(0,I ). For simplicity of calculation, let us further assume here that ∥w ∥2 =
d gen 2
∥w ∥2 =1.
prune 2
ϕ =P(x⊤w >0|y =1,y′ =1)
1 prune
P(x⊤w >0,y =1,x⊤w >0)
= prune gen
P(y =1,x⊤w >0)
gen
P(η⊤w >−∥µ∥ d(1−∥µ∥2)cosθ ,η⊤w >−∥µ∥ d(1−∥µ∥2)cosθ )
= prune 2 2 prune gen 2 2 gen .
P(η⊤w >−∥µ∥ d(1−∥µ∥2)cosθ )
gen 2 2 gen
Inthesecondstep,weusethedefinitionofconditionalprobabilityanduse(23)fromline2toline3.
Therandomvariablesη⊤w andη⊤w arejointlyGaussianwith:
gen prune
(cid:18) (cid:19) (cid:18) (cid:18) (cid:19)(cid:19)
η⊤w 1 cosθ
gen ∼N 0, .
η⊤w cosθ 1
prune
Let Φ be the CDF of the standard normal distribution and Φ the CDF of the bivariate normal
2
distribution,whereΦ (x,y;ρ)isdefinedas:
2
Φ (x,y;ρ):=P(X ≤x,Y ≤y)
2
(cid:18) (cid:18) (cid:19)(cid:19)
1 ρ
for(X,Y)∼N 0, . Denotec =−∥µ∥ d(1−∥µ∥2)cosθ andc =−∥µ∥ d(1−
ρ 1 1 2 2 prune 2 2
∥µ∥2)cosθ . Wehave:
2 gen
1−Φ(c )−Φ(c )+Φ (c ,c ;cosθ)
ϕ = 1 2 2 1 2 .
1 1−Φ(c )
2
Allthedistributionsaresymmetric,andwehaveϕ =ϕ =ϕ. Inthesamespirit,ψ =ψ =ψ ,
0 1 10 01
with
Φ(c )+Φ(c )−Φ (c ,c ;cosθ)
ψ = 1 2 2 1 2 .
Φ(c )
2
21D.7 SketchofProofofTheoremD.3
Theproofisbasedonthefollowingrepresentation(refertoPropositionE.2)oftheaccuracyofthe
downstreamclassifierf(cid:98)N evaluatedonthethecleantrainingdatasetD N,namely
N 1 +N 1 +N 1 +N 1
11 A<1/2 00 D<1/2 10 B>1/2 01 C>1/2
a (cid:99)cc(f(cid:98)N)=
N +N +N +N
, (24)
11 00 10 01
forsomerandomsomerandomvariablesA,B,C,D ∈(0,1)whichdependontheN ’sdefinedin
kℓ
(16).
RemarkD.5. Weonlycomputetheaccuracya (cid:99)cc(f(cid:98)N)ofthedownstreammodelf(cid:98)N evaluatedonthe
cleantrainingdatasetD . Byclassicalresultsinlearningtheory(McAllester,2003;Shalev-Shwartz
N
andBen-David,2014;Kakadeetal.,2008),weknowthatthegaptothepopulationversion(test
√
accuracy)acc(f(cid:98)N)shrinkstozeroatrateO(1/ N),andsosincetheclaiminTheoremD.3ismade
onlyinthelimitN →∞,wearegood.
Next,inPropositionE.3andPropositionE.4,necessaryandsufficientconditionsareestablishedto
ensureA,D < 1/2andB,C > 1/2,andthereforea (cid:99)cc(f(cid:98)N) = 100%. Theseconditionsaregiven
explicitly in terms of the N ’s. Finally, in Proposition E.5, concentration of measure is used to
kℓ
controltheN ’s,andpresenttheaforementionedconditionsintermsofthep ’sdefinedin(14),
kℓ kℓ
andthereforeintermsofp,ϕ,andψalone,givingcondition(19).
E ProofofTheoremD.3
Ouranalysisisbasedonnon-trivialextensionsofargumentsbyDasandSanghavi(2023). Viz,
• We allow for a pruning mechanism (aforementioned work does study pruning, just self-
distillation),and
• We use a careful asymptotic analysis to avoid solving certain complicated fixed-point
equationsdefiningtheweightsvectorw
(cid:98)N
ofthedownstreammodelf(cid:98)N.
E.1 PreliminaryComputations
Forlateruse,givenapruningstrategyq,definethefollowingobjects
I :={j ∈[N]|y =k}, (25)
k j
I′ :={j ∈[n]|y′ =ℓ}, (26)
ℓ j
M :={i∈[N]|q =1}, (27)
i
(cid:88)
N := q =|I ∩I′∩M|, (28)
kℓ i k ℓ
i∈Ik∩I ℓ′
R:=1−a>0. (29)
Thus,N isthenumberoftrainingexamplesthathavetruelabelk,fakelabelℓ,andsurvivepruning.
kℓ
Thefollowingresultwillbecrucialinthesequel.
(cid:80)
PropositionE.1. Wehavetherepresentationw = α x ,where
(cid:98) i∈M i i
A, ifi∈I ∩I′ ∩M,
−B, ifi∈I1 ∩I1
′ ∩M,
α = 1 0 (30)
i C, ifi∈I ∩I′ ∩M,

−D,
ifi∈I0 ∩I1
′ ∩M,
0 0
andA,B,C,D ≥0solvethefollowingsystemofequations
γA=σ(−(aN A−aN B+bN C−bN D)−RA),
11 10 01 00
γB =σ(aN A−aN B+bN C−bN D−RB),
11 10 01 00
(31)
γC =σ(−(bN A−bN B+aN C−aN D)−RC),
11 10 01 00
γD =σ(bN A−bN B+aN C−aN D−RD).
11 10 01 00
22Proof. Thefollowingresultisinspiredby(DasandSanghavi,2023)andtheproofissimilar. Observe
thatKKTconditions∇L(w)=0give(cid:80)N q (yˆ −y′)x +γw =0,i.e
i=1 i i i i
w
=(cid:88)N
q α x , withα :=
y i′−yˆ
i, yˆ :=σ(v ), v =x⊤w. (32)
i i i i γ i i i i
i=1
Onethencomputes
N (cid:26)
v =x⊤w
=(cid:88)
q α x⊤x =q α +
a(s−q iα i)+bt, ifi∈I 1,
i i i i i j i i a(t−q α )+bs, ifi∈I ,
i i 0
j=1 (33)
(cid:26)
as+bt+Rq α , ifi∈I ,
= i i 1
bs+at+Rq α , ifi∈I ,
i i 0
wheres≥0andt≥0aregivenby
(cid:88) (cid:88)
s:= q α , t:= q α . (34)
j j j j
j∈I1 i∈I0
Wededucethatforanyi∈M,
1−σ(as+bt+Rq α ), ifi∈I ∩I′,
−σ(as+bt+Rq αi ),i ifi∈I1 ∩I1
′,
γα =y′−σ(v )= i i 1 0 (35)
i i i 1−σ(bs+at+Rq α ), ifi∈I ∩I′,

−σ(bs+at+Rq
αi ),i ifi∈I0 ∩I1
′.
i i 0 0
Duetomonotonicityofσ,wededucetheexistenceofA,B,C,D ≥0suchthat
A, ifi∈I ∩I′ ∩M,
−B, ifi∈I1 ∩I1
′ ∩M,
α = 1 0 (36)
i C, ifi∈I ∩I′ ∩M,

−D,
ifi∈I0 ∩I1
′ ∩M.
0 0
1−γA, ifi∈I ∩I′ ∩M,
γB, ifi∈I1 ∩I1
′ ∩M,
yˆ =y′−γα = 1 0 (37)
i i i 1−γC, ifi∈I ∩I′ ∩M,

γD,
ifi∈I0 ∩I1
′ ∩M.
0 0
Furthermore,thesescalarsmustverify
γA=1−σ(as+bt+RA)=σ(−(as+bt)−RA),
γB =σ(as+bt−RB),
(38)
γC =1−σ(bs+at+RC)=σ(−(bs+at)−RC),
γD =σ(bs+at−RD).
Finally,observethat,
s=N A−N B, t=N C−N D, (39)
11 10 01 00
fromwhichweget
as+bt=a(N A−N B)+b(N C−N D)
11 10 01 00
=aN A−aN B+bN C−bN D,
11 10 01 00
bs+at=b(N A−N B)+a(N C−N D)
11 10 01 00
=bN A−bN B+aN C−aN D.
11 10 01 00
Pluggingthisinto(38)gives(31).
23E.2 AnalyticFormulaforAccuracyEvaluatedCleanTrainingData
Onecomputestheaccuracya (cid:99)cc(f(cid:98)N)ofthedownstreammodelevaluatedonthecleantrainingdataset
D as
N
1
a (cid:99)cc(f(cid:98)N)= |M|(|{i∈M |y
i
=1∧yˆ
i
>1/2ORy
i
=0∧yˆ
i
<1/2}|).
Wecanrewritethisasfollows
|M|·a (cid:99)cc(f(cid:98)N)=|{i∈M |y
i
=1∧yˆ
i
>1/2ORy
i
=0∧yˆ
i
<1/2}|
=|{i∈I ∩M |yˆ >1/2}|+|{i∈I ∩M |yˆ <1/2}|
1 i 0 i
(cid:88) (cid:88)
= 1 + 1
yˆi>1/2 yˆi<1/2
i∈I1∩M i∈I0∩M (40)
=|I ∩I′ ∩M|1 +|I ∩I′ ∩M|1
1 1 γA<1/2 1 0 γB>1/2
+|I ∩I′ ∩M|1 +|I ∩I′ ∩M|1
0 1 γC>1/2 0 0 γD<1/2
=N 1 +N 1 +N 1 +N A .
11 γA<1/2 00 γD<1/2 10 γB>1/2 01 γC>1/2
(cid:80)
Ontheotherhand,itisclearthatthesizeofthemaskis|M|= N . Puttingthingstogether
k,ℓ kℓ
givesthefollowingresultwhichshallbecrucialinthesequel.
PropositionE.2. Foranyϕ,ψ ∈[0,1],thereisasolution(A,B,C,D)ofthesystemofequations
(31)suchthat
N 1 +N 1 +N 1 +N 1
11 A<1/2 00 D<1/2 10 B>1/2 01 C>1/2
a (cid:99)cc(f(cid:98)N)=
N +N +N +N
, (41)
11 00 10 01
whereA:=γA,B =γB,C =γC,andD =γDasusual.
Thus, to attain 100% accuracy, it suffices to have A,D < 1/2 and B,C > 1/2. The proof of
TheoremD.3willbeallaboutestablishingsufficientconditionswhichensuretheseinequalities.
E.3 SufficientConditionsforPerfectAccuracy
Notethatsinceγ =Nλwithλ>0fixedandN →∞,wehaveγ →∞andsystemofequations
(31)simplifyto3
B =σ((aN A−aN B+bN C−bN D)/γ),
11 10 01 00
A=σ(−(aN A−aN B+bN C−bN D)/γ)=1−B,
11 10 01 00
(42)
D =σ((bN A−bN B+aN C−aN D)/γ),
11 10 01 00
C =σ(−(bN A−bN B+aN C−aN D)/γ)=1−D,
11 10 01 00
whereA:=γA,B =γB,C =γC,D =γDasusual,andwehaveusedtheelementaryproperty
thatσ(−z)=1−σ(z). EliminatingAandC,theaboveequationsfurthercollapseto
B =σ((aN (1−B)−aN B+bN (1−D)−bN D)/γ),
11 10 01 00
=σ((aN +bN −a(N +N )B−b(N +N )D)/γ),
11 01 11 10 01 00
(43)
D =σ((bN (1−B)−bN B+aN (1−D)−aN D)/γ)
11 10 01 00
=σ((bN +aN −b(N +N )B−a(N +N )D)/γ).
11 01 10 10 01 00
Twospecialcasesaretractable.
TheSymmetricCase: b=−a. WehaveD =1−B,andthustheequationsbecome
D =A, C =B, D =1−B,
(44)
B =σ((a(N +N )−a(N +N +N +N )B)/γ).
11 00 11 10 01 00
3ThesesimplificationsaremadepossiblebytheMeanValueTheorem.
24IfB ≤1/2,thenwemusthaveB ≥(N +N )/(N +N +N +N ),whichisimpossible
11 00 11 10 01 00
ifweimpose
N +N <N +N , (45)
10 01 11 00
i.ethenumberofbadindiceswhichsurviveissmallerthanthenumberofgoodindiceswhichsurvive
pruning.Thus,underthepreviouscondition,wemusthaveC =B >1/2andA=D =1−B <1/2.
Bysymmetryofthepreceedingargumentweknowthattheconditionisalsonecessary. Wededuce
thefollowingresult.
PropositionE.3. Supposeb=−a. Then,foranysolution(A,B,C,D)ofthesystemofequations
(31),theinequalities
C =B >1/2, D =A<1/2, (46)
holdifandonlyiffN +N <N +N .
10 01 11 00
SkewedCase: b=0. Here,wehave
B =σ(a(N −(N +N )B)/γ),
11 11 10
(47)
D =σ(a(N −(N +N )D)/γ)
01 01 00
IfB ≤1/2,thenB ≥N /(N +N ),whichisimpossibleifweimpose
11 11 10
N <N , (48)
10 11
i.ethenumberofexampleswithtruelabel1,whichareincorrectlylabelledas0inthedataset,which
survivepruningislessthanthenumberofexampleswithtruelabel1,whicharecorrectlylabelled
andsurvivepruning. WededucethatB >1/2undertheabovecondition.
Similarly,ifD ≥1/2,thenD ≤N /(N +N ),whichisimpossibleifweimpose
01 01 00
N <N , (49)
01 00
i.ethenumberofwithtruelabel0butincorrectlylabelledas1inthedataset,whichsurvivepruningis
lessthanthenumberofexampleswithtruelabel1,whicharecorrectlylabelledandsurvivepruning.
Weobtainthefollowingresult.
PropositionE.4. Supposeb=0. Then,foranysolution(A,B,C,D)of (31),wehave
C,B >1/2iffN <N , (50)
10 11
D,A<1/2iffN <N . (51)
01 00
E.4 Concentration
WeshallnowderiveconditionswhicharesufficienttoensurethehypothesisinPropositionsE.3and
E.4,namelyN <N forallk,ℓ∈{0,1}withk ̸=ℓ.Recallthatforanyk,ℓ∈{0,1},thecounter
kℓ kk
N israndomwithbinomialdistributionBin(N,p ). Now,bybasicbinomialconcentration,we
kℓ kℓ
knowthatifp,ψ ∈[0,1)andϕ∈(0,1],thenforanyfixedt∈(0,1),itholdsw.p1−o(1)that
(cid:26)
N ≤(1+t)Np , ifk ̸=ℓ,
kℓ kℓ (52)
N ≥(1−t)Np , ifk =ℓ.
kℓ kℓ
Inparticular,w.p1−o(1),itholdsthat
N ≤(1+t)Np , (53)
kℓ kℓ
N ≥(1−t)Np . (54)
kk kk
Comparingtheaboveinequalities,wededucethefollowingresult.
PropositionE.5. Ifthefollowingconditionholds
p +p 1−t 2t
01 10 < =1−ϵwithϵ:= , (55)
p +p 1+t 1+t
00 11
thenw.p1−o(1)itholdsthat
N +N <N +N . (56)
10 01 11 00
25E.5 ProofofTheoremD.3
Follows directly from putting together Propositions E.2, E.3, E.4, and E.5, and then solving the
inequality
1−t p +p 2pψ pψ
≤ 01 10 = =
1+t p +p 2(1−p)ϕ (1−p)ϕ
00 11
forp.
26