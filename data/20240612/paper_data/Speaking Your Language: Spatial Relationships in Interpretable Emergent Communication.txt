Speaking Your Language: Spatial Relationships in
Interpretable Emergent Communication
OlafLipinski1∗ AdamJ.Sobey2,1 FedericoCerutti3 TimothyJ.Norman1
1UniversityofSouthampton 2TheAlanTuringInstitute 3UniversityofBrescia
{o.lipinski,t.j.norman}@soton.ac.uk
asobey@turing.ac.uk
federico.cerutti@unibs.it
Abstract
Effective communication requires the ability to refer to specific parts of an ob-
servationinrelationtoothers. Whileemergentcommunicationliteratureshows
success in developing various language properties, no research has shown the
emergenceofsuchpositionalreferences. Thispaperdemonstrateshowagentscan
communicate about spatial relationships within their observations. The results
indicatethatagentscandevelopalanguagecapableofexpressingtherelationships
betweenpartsoftheirobservation,achievingover90%accuracywhentrainedina
referentialgamewhichrequiressuchcommunication. Usingacollocationmeasure,
wedemonstratehowtheagentscreatesuchreferences. Thisanalysissuggeststhat
agentsuseamixtureofnon-compositionalandcompositionalmessagestoconvey
spatialrelationships. Wealsoshowthattheemergentlanguageisinterpretableby
humans. Thetranslationaccuracyistestedbycommunicatingwiththereceiver
agent,wherethereceiverachievesover78%accuracyusingpartsofthislexicon,
confirmingthattheinterpretationoftheemergentlanguagewassuccessful.
1 Spatialreferencinginemergentcommunication
Emergentcommunicationallowsagentstodevelopbespokelanguagesfortheirenvironment. While
therearemanysuccessfulexamplesofefficient(Ritaetal.,2020)andcompositional(Chaabouni
et al., 2020) languages, they often lack fundamental aspects seen in human language, such as
syntax(LazaridouandBaroni,2020)orrecursion(Baroni,2020). Itisarguedthattheseaspectsof
communicationareimportanttoimprovetheefficiencyandgeneralisabilityofemergentlanguages
(Baroni,2020;BoldtandMortensen,2024;Ritaetal.,2024). However,thecurrentarchitectures,
environments,andrewardschemesareyettoexhibitsuchfundamentalproperties.
Onesuchaspectisthedevelopmentofdeixis(Ritaetal.,2024),whichhasbeendescribedasaway
ofpointingthroughlanguage. Examplesoftemporaldeixisincludewordssuchas“yesterday”or
“before,”andspatialdeixisincludewordssuchas“here”or“nextto”(Lyons,1977). Inemergent
communication,Lipinskietal.(2023)investigatehowagentsmayrefertorepeatingobservations,
whichcouldalsobeviewedfromthelinguisticperspectiveasinvestigatingtemporaldeixis. However,
whilethereareadvocatestoinvestigatehowemergentlanguagescandevelopkeyconceptsfrom
humanlanguage(Ritaetal.,2024),noworkhasdemonstratedtheemergenceofrelativereferencesto
specificlocationswithinanobservation,orspatialdeixis.
Spatialreferenceswouldbevaluableinestablishingsharedcontextbetweenagents,increasingcom-
municationefficiencybyreducingtheneedfordetaileddescriptions,andadaptability,byremoving
the need for unique references per object. For example, instead of describing a new, previously
∗Corresponding author: o.lipinski@soton.ac.uk
Preprint.Underreview.
4202
nuJ
11
]LC.sc[
1v77270.6042:viXraunseenobject,suchas“abluevasewithintricatemotifsonthetable,”onecouldsimplyusespatial
relationshipsandsay“theobjectleftoftheplate.” Spatialreferencingstreamlinescommunicationby
leveragingthesharedenvironmentasareferencepoint.Indynamicenvironmentswhereobjectsmight
changepositions,spatialreferencesenableagentstoeasilytrackandrefertoobjectswithouthaving
toupdatetheirdescriptions. Thisenhancescommunicationefficiencyandimprovesinteractionand
collaborationbetweenagents. Theseelementsmayalsohelptheevolvedlanguagebecomehuman
interpretable, allowingthedevelopmentoftrustworthyemergentcommunication(Lazaridouand
Baroni,2020;MuandGoodman,2021).
Thispaperthereforeexploreshowagentscandevelopcommunicationwithspatialreferences. While
Ritaetal.(2024)positthattheemergenceofthesereferencesmightrequirecomplexsettings,weshow
thatevenagentstrainedinamodifiedversionofthesimplereferentialgame(Lazaridouetal.,2018;
Lewis,1969)candevelopspatialreferences.2 Thisresultinglanguageisanalysedusingacollocation
measure,NormalisedPointwiseMutualInformation(NPMI)adaptedfromcomputationallinguistics.
Our choice of NPMI is motivated by its ability to measure the strength of associations between
messagepartsandtheircontext,makingitavaluabletoolforgaininginsightsintotheunderlying
structure of the emergent language. We show how the agents compose such spatial references,
providingthefirstevidenceofasyntacticstructure,andshowingthattheemergentlanguagecanbe
interpretedbyhumans.
2 Developmentofaspatialreferentialgame
Currentemergentcommunicationenvironmentshavenotproducedlanguagesincorporatingspatial
references. To address this, we develop a referential game (Lazaridou et al., 2018) environment
whereaneffectivelanguagerequirescommunicationaboutspatialrelationships. Inthereferential
game,therearetwoagents,asenderandareceiver. Thesenderobservesavectorandtransmitsits
compressedrepresentationthroughadiscretechanneltothereceiver. Thereceiverobservesasetof
vectorstogetherwiththesender’smessage. Oneofthesevectorsisthesameastheonethesenderhas
observed. Thereceiver’sgoalistocorrectlyidentifythevectorthesenderhasdescribed,amongother
vectorsreferredtoasdistractors. Thesimplicityofthereferentialgamesenablesthereductionof
extraneousfactorswhichcouldimpacttheemergenceofspatialreferences,suchastransferlearning
ofthevisionnetworkorexploringactionspacesinmorecomplexenvironments.
Inthiswork,thesender’sinputisanobservationintheformofavectoro=[o ,o ,o ,o ,o ],where
1 2 3 4 5
∀o∈{−1,0,1...59}. Thevectoroisalwayscomposedof5integers. Theobservationincludesa
−1inonlyoneposition,e.g.,o =−1foro=[x,x,−1,x,x],toindicatethetargetintegerforthe
3
receivertoidentify. orepresentsawindowintoalongersequences,whichisrandomlygenerated
usingtheintegers{0...59}withoutrepetitions. Thissequenceisvisibletothereceiver,butnotto
thesender. Asthetarget’spositioninthesequenceisunknowntothesender,ithastorelyonthe
relativepositionalinformationpresentinitsobservation,necessitatingtheuseofspatialreferencing.
Duetothewindowintothesequencebeingoflength5,itisnecessarytoshiftthewindowwhenit
approacheseitherextentofthesequence.Thewindowisthenshiftedtotheotherside,maintainingthe
sizeof5. Forexample,givenashortsequences=[7,5,2,12,10,4,3,15,16,13,14,6,9,8,11,1],
iftheselectedtargetis1, sincetherearenointegerstotherightof1thevectorowouldbeo =
[6,9,8,11,−1]whereitisshiftedtotheleftasitapproachesthisrightmostextentofthesequence.
Duetothenecessityofmaintainingthewindowsize,someobservationsprovideadditionalpositional
informationtothesenderagent. Giventhesameexamplesequences,wecancategoriseallobser-
vationsinto5types. Thebeginandbegin+1,wherethetargetintegeriseitherat,oroneafter,the
beginningofthesequence,i.e.,o=[−1,5,2,12,10]oro=[7,−1,2,12,10]. Theendandend-1,
wherethetargetintegeriseitherat,oronebefore,theendofthesequence,i.e.,o=[6,9,8,11,−1]
oro = [6,9,8,−1,1]. Themostcommoncaseisthemiddleobservation,wherethetargetinteger
is anywhere in the sequence, excluding the first, second, second to last, and last positions, e.g.,
o = [12,10,−1,3,15]. Given a window of length 5, only 4 specific target integer positions per
sequence can result in the other observations (begin, begin+1, end-1, and end). All other target
integerpositionswithinthesequencefallintothemiddlecategory,astheydonotoccupythefirst,
second,secondtolast,orlastpositions. Consequently,themajorityofthetargetintegerpositions
resultinamiddletypeobservation.
2OurcodeisavailableonAnonymousGitHub
2The sender’s output is a message defined as a vector m = [m ,m ,m ], where m ∈ {1...26}.
1 2 3
26ischosentoallowforahighdegreeofexpressivity,withtheagentsbeingabletouseover17k
differentmessages,whilealsomatchingthesizeoftheLatinalphabet,reflectingoneofthecommon
alphabetlengths. Thevectormisalwayscomposedof3integers.
The receiver’s input is an observation consisting of three vectors: the sender’s message m, the
sequence s, and the set of distractor integers together with the target integers td. The distractor
integersarerandomlygenerated,withoutrepetitions,giventhesamerangeofintegersastheoriginal
sequences,i.e.,{0...59},excludingthetargetobjectitself.
For example, given the sequence s = [7,5,2,12,10,4,3,15,16,13,14,6,9,8,11,1], and the
sender’s observation o = [4,3,−1,16,13], the vector td could be td = [7,15,11,9], with 15
beingthetargetthatthereceiverneedstoidentify. Thesendercouldproduceamessagem=[3,1,1],
whichwouldmeanthatthetargetintegerisoneaftertheinteger3. Thismessagewouldthenbe
passedtothereceiver,togetherwithsandtd. Thereceiverwouldthenhavetocorrectlyunderstand
themessagem(i.e.,thatthetargetisoneafter3)andfindtheinteger3togetherwiththefollowing
integerinthesequences. Havingidentifiedthetarget15giventhemessagemandthesequences,it
wouldoutputthecorrectpositionofthistargetinthetdvector,i.e.,2,sincetd =15.
2
3 AgentArchitecture
The agent architecture follows that of the most commonly used EGG agents (Kharitonov et al.,
2019). Thisarchitectureisusedtomaintainconsistencywiththecommonapproachesinemergent
communicationresearch(Chaabounietal.,2019,2020;Kharitonovetal.,2019;Lipinskietal.,2023;
UedaandWashio,2021),increasingthegeneralizationoftheresultspresentedinthiswork.
Thesenderagent,showninFigure1a,receivesasingleinput,thevectoro,whichispassedthrough
the first GRU of the sender. The resulting hidden state is used as the initial hidden state for the
messagegenerationGRU(Choetal.,2014). ThemessagegenerationGRUisusedtoproducethe
message, character by character, using the Gumbel-Softmax reparametrization trick (Jang et al.,
2017;Kharitonovetal.,2019;MordatchandAbbeel,2018). Thesequenceofcharacterprobabilities
generatedfromthesenderisusedtooutputthemessagem.
misinputtothereceiveragent,showninFigure1b,togetherwiththefullsequencesandthetargets
anddistractorstd. ThemessageisprocessedbythefirstreceiverGRU,whichproducesahiddenstate
usedastheinitialhiddenstatefortheGRUprocessingthesequences. Thisistheonlychangefrom
thestandardEGGarchitecture(Kharitonovetal.,2019). ThisadditionalGRUallowsthereceiver
agenttoprocesstheadditionalinputsequences,usingtheinformationcontainedwithinthemessage
m. ThegoalofthisGRUistousetheinformationprovidedbythesendertocorrectlyidentifywhich
integerfromthesequencesisthetargetinteger. ThefinalhiddenstatefromtheadditionalGRUis
multipliedwithanembeddingofthetargetsanddistractors,tooutputthereceiver’sprediction. This
predictionisintheformoftheindexofthetargetwithintd.
Followingthecommonlyusedapproach(Kharitonovetal.,2019),agentoptimisationisperformed
using the Gumbel-Softmax reparametrization (Jang et al., 2017; Mordatch and Abbeel, 2018),
allowing for direct gradient flow through the discrete channel. The agents’ loss is computed by
applyingthecrossentropyloss,usingthereceivertargetpredictionandthetruetargetlabel. The
resulting gradients are passed to the Adam optimiser and backpropagated through the network.
DetailedtraininghyperparametersareprovidedinAppendixA.
4 MessageinterpretabilityandanalysisusingNPMI
To analyse spatial references in emergent language, a way to identify their presence is essential.
In discrete emergent languages, interpretation is typically done by either using dataset labels in
naturallanguage(Dessìetal.,2021),orbyqualitativeanalysisofspecificmessages(Havrylovand
Titov, 2017). However, both of these techniques require message-meaning pairs, and so neither
wouldbeabletoidentifythepresenceofspatialreferences,asthelabelsforspatialrelationships
thattheagentsrefertowouldnotnecessarilybeavailable. Oneapproachthatcouldovercomethis
problemisemergentlanguagesegmentationusingHarris’ArticulationScheme,recentlyemployed
byUedaetal.(2023). Uedaetal.(2023)computetheconditionalentropyofeachcharacterinthe
3Sequence
Window
[1,2,-1,4,5]
Final hidden states Target and Sequence of
Full sequence
GRU 1 for each element Distractors Character
[1,2,3,4,5,6,7,8,9,10]
in the sequence [5,10,7,3] Probabilities
Initial Hidden State for GRU
Repeat until max_len reached Object GRU 2
Embedding GRU 1
GRU 2 Hidden to Vocab Linear (Combine)
Message Gumbel-Softmax
Linear
Generation
Final hidden
Objects
VE ocm ab be td od Cin hg a f rr ao cm ter Embedded Initial Hidden State s et la et mes e nfo t r i ne a thc eh
Linear sequence
Matrix Multiply Integer
Sequence of
torch.matmul Guess
Character
Probabilities
(a)Thesenderarchitecture. (b)Thereceiverarchitecture.
Figure1: Thesenderandreceiverarchitectures. Adaptedfrom(Lipinskietal.,2023).
emergentlanguage,segmentingthemessageswheretheconditionalentropyincreases. However,
evenafterlanguagesegmentation,thereisnoeasywaytointerpretthesegments,asnomethodhas
beenproposedtomapthemtospecificmeanings.
We present an approach to both segment the emergent language and map the segments to their
meanings. WeuseacollocationmeasurecalledNormalisedPointwiseMutualInformation(NPMI)
(Bouma, 2009), often used in computational linguistics (Lim and Lauw, 2024; Thielmann et al.,
2024;Yamakietal.,2023). Itisusedtodeterminewhichmessagesareusedforwhichobservations
andtoanalysehowthemessagesarecomposed,includingwhethertheyaretriviallycompositional
(Korbaketal.,2020;Perkins,2021;Steinert-Threlkeld,2020). Byapplyingacollocationmeasure
todifferentpartsofeachmessageaswellasthewholemessage,wecanaddresstheproblemsof
bothsegmentationandinterpretationofthemessagesegments. Thisapproachallowsanypartofthe
messagetocarryadifferentmeaning. Forexample,ifanemergentmessagecontainssegmentsthat
frequentlyappearincontextsinvolvingspecificintegers,NPMIcanhelpidentifythesesegmentsand
theirmeaningsbasedontheirstatisticalassociationwiththoseintegers.
NPMIisanormalisedversionofthePointwiseMutualInformation(PMI)(ChurchandHanks,1989),
whichisameasureofassociationbetweentwoevents.PMIiswidelyusedincomputationallinguistics,
tomeasuretheassociationbetweenwords(Hanetal.,2013;PapernoandBaroni,2016). Normalising
thePMImeasureresultsinitscodomainbeingdefinedbetween−1and1,with−1indicatingapurely
negativeassociation(i.e.,eventsneveroccurringtogether),0indicatingnoassociation(i.e.,events
beingindependent),and1indicatingapurelypositiveassociation(i.e.,eventsalwaysoccurring
together). NormalisedPMIisusedforconveniencewhendefiningathresholdatwhichweconsidera
messageorn-gramtocarryaspecificmeaning,asthethresholdcanbebetween0and1,insteadof
unboundednumbersinthecaseofPMI.3
Todeterminewhichpartsofeachmessageareusedforagivenmeaning,twoalgorithmsareproposed.
1. PMI Thealgorithmtomeasurenon-compositionalmonolithicmessages,mostoftenused
nc
fortargetpositionalinformation(e.g.,begin+1(Section2));and
2. PMI thealgorithmtomeasuretriviallycompositionalmessagesandtheirn-grams,usedto
c
refertodifferentintegersindifferentpositions.
Avisualrepresentationofthedifferenttypesofmessagesthatthealgorithmscanidentifyisprovided
inFigure2. ThePMI algorithmcanidentifyanynon-compositionalmessages,whilethePMI
nc c
algorithm identifies both position variant and invariant compositional messages. The positional
varianceoftheemergentlanguagemeansthatthepositionofann-graminthemessagealsocarries
3Our implementation of NPMI is not numerically stable due to probability approximation, sometimes
exceedingthe[-1,1]co-domain.Weprovidemoredetailsinthecode.
4Message Observation
Compositional Position Invariant
[5, 6, 8] [4, 2, 8] [10, 5, 6] [X, 4, -1, X, X] [X, 2, -1, X, X] [X, X, -1, 4, X]
Compositional Position Variant
[5, 6, 8] [8, 5, 6] [10, 5, 6] [X, 4, -1, X, X] [X, 8, -1, X, X] [X, X, -1, 8, X]
Non-Compositional
[5, 6, 8] [5, 6, 6] [5, 6, 2] [-1, X, X, X, X] [X, 9, -1, X, X] [X, -1, X, X, X]
Figure2: Examplesofthedifferenttypesofmessagecompositionalitythatarepossibletoidentify
usingthePMIalgorithms.
apartofitsmeaning. Inthiswork,n-gramsrefertoacontiguoussequenceofnintegersfromthe
sender’smessage. Consequently,inonemessagethereare3unigrams(m ,m ,m ),twobigrams
1 2 3
([m ,m ],[m ,m ]),andonetrigram(i.e.,thewholemessage[m ,m ,m ]).
1 2 2 3 1 2 3
Figure 2 shows that in the position invariant case, the bigram [5,6] always carries the meaning
of 4. While in the position variant case, the bigram [5,6] in position 1 of the message means 4,
but [5,6] in position 2 of the message means 8. This can also be interpreted as the position of
thebigramcontainingadditionalinformation,meaningasingle“word”couldberepresentedasa
tupleofthebigramanditspositioninthemessage,asbothcontributetoitsunderlyinginformation.
Non-compositionalmessagesaremonolithic,i.e.,thewholemessagecarriestheentiremeaning. For
example,message[5,6,8]meansthetargetisinthefirstposition,while[5,6,6]meansthetargetis
onetotherightof9,eventhoughthetwomessagessharethebigram[5,6].
ThePMI algorithm ThePMI algorithmcalculatestheNPMIpermessagebyfirstbuildinga
nc nc
dictionaryofallcountsofeachmessagebeingsent,togetherwithanobservationthatmayprovide
positionalinformation(e.g.,begin+1)orrefertoanintegerinagivenposition(e.g.,1leftofthetarget).
Thecountsofthatmessageandthecountsoftheobservation, includingtheintegerposition, are
alsocollected. Forexample,considertheobservationo=[4,−1,15,16,13]. Forthecorresponding
messagem,thecountsforeachintegerineachpositionrelativetothetargetwouldincreaseby1
(i.e.,left1[4]+=1,right1[15]+=1etc.). Thecountforthemessagesignifyingbegin+1would
alsobeincreased. Giventhesecounts,thealgorithmthenestimatestheprobabilitiesofallrespective
events(messages,positionalobservations,andintegersingivenpositions)andcalculatestheNPMI
measure.
ThePMI algorithm ThePMI algorithmfirstcreatesadictionaryofallpossiblen-grams,given
c c
themessagespace(m)andmaximummessagelength(3). Thelistofallpossiblen-gramsispruned
tocontainonlythen-gramspresentintheagents’language,avoidingunnecessarycomputationin
thelaterpartsofthealgorithm. Giventheprunedlistofn-grams,thealgorithmchecksthecontext
inwhichthen-gramshavebeenused. Theoccurrenceofeachn-gramiscounted,togetherwiththe
n-gram position in the messages and the context in which it has been sent, or the integers in the
observation. Then-grampositioninthemessageisconsideredtoaccountforthepossibleposition
varianceofthecompositionalmessages.
Considerthepreviousexample,witho=[4,−1,15,16,13]andamessagem=[11,13,5]. Forall
n-grams([11],[13],[5],[11,13],etc.)ofthemessage,allintegersarecounted,irrespectiveoftheir
positions(i.e.,counts[4]+=1,counts[15]+=1,etc.).
Giventhesecounts,thePMI algorithmestimatestheNPMImeasureforalln-gramsandallintegers
c
in the observations. These probabilities are estimated from the dataset using the count of their
respectiveoccurrencesdividedbythenumberofallobservations/messages.
OncetheNPMImeasureisobtainedforthen-gram-integerpairs,thealgorithmcalculatestheNPMI
measure for n-grams and referent positions or the positions of the integer in the observation the
messagerefersto. Forexample,givenanobservationo=[4,−1,15,16,13],ifthemessagecontains
ann-gramwhichhasbeenidentifiedasreferringtotheinteger15,therestofthemessage(i.e.,the
5Algorithm1:PMIAlgorithmBase
1 Gatherngram_counts,context_counts,joint_counts,n_grams;
2 foreachn-gramginpositionpandcontextcdo
1
3 P(g,p)=ngram_counts[g]· total n-grams;
1
4 P(c)=context_counts[c]· total contexts;
1
5 P(g,p;c)=joint_counts[(g,c)]· total n-grams;
P(g,p,c) 1
6 NPMI(g,p;c)=log 2 P(g)P(c) · −log P(g,p,c);
2
7 end
8 returnNPMI;
unigramorbigram,dependingonthelengthoftheintegern-gram)iscountedasapossiblereference
tothatposition,inthiscase,topositionright1,or1totherightofthetarget. Thisprocedurefollows
forallmessages,buildingacountforeachtimeann-gramwasusedtogetherwithapossiblen-gram
foraninteger. ThesecountsareusedtocalculatetheNPMImeasureforn-gramandpositionpairs.
ThePMI algorithmalsoaccountsforthepossiblepositioninvarianceofthen-grams,i.e.,where
c
in the message the n-gram appears. This is achieved by calculating the respective probabilities
regardlessofthepositionofthen-graminthemessage,bysummingtheindividualcountsforeach
n-gramposition.
Pseudocode WeprovideacondensedpseudocodeforbothalgorithmsinAlgorithm1. Inthecase
ofthePMI ,then-gramsinthepseudocodewouldbewholemessages,i.e.,trigrams. Thisbase
nc
pseudocode would then be duplicated, interpreting the context as either an observation that may
providepositionalinformation(e.g.,begin+1)oraninteger.
ForthePMI algorithm,onlytheunigramsandbigramswouldbeevaluated. Thebasepseudocode
c
wouldalsobeduplicated,oncefortheintegerinagivenposition,andsecondforthereferentposition.
Each would be used as the context in which to evaluate the NPMI for each n-gram. A detailed
commentedpseudocodeforboththePMI andPMI algorithmsisavailableinAlgorithm2and
nc c
Algorithm3inAppendixB,respectively.
Bothalgorithmsusetwohyperparameters: aconfidencethresholdt andtop_nt . Theconfidence
c n
thresholdreferstothevalueoftheNPMImeasureatwhichamessageorn-gramcanbeconsidered
torefertothegivenpartoftheobservationunambiguously. Toaccountforpolysemy(whereone
symbolcanhavemultiplemeanings),theagentscanuseasinglen-gramtorefertomultipleintegers.
Thisisgivenbythesecondhyperparameter,top_n,whichsetsthedegreeofthepolysemy,orthe
numberofintegerstobeconsideredforagivenn-gram.
5 Spatialreferencingexperiments
The agent pairs are trained over 16 different seeds to verify the results’ significance. All agent
pairsachieveabove98%accuracyonthereferentialtask,showingthattheagentsdevelopawayto
communicateaboutspatialrelationshipsintheirobservations. Theanalysisprovidedinthissectionis
basedonthemessagescollectedfromthetestdatasetafterthetraininghasfinished.
Thetwohyperparameters,t andt (Section4),governingtheNPMImeasurehavebeendetermined
c n
throughagridsearchtomaximisetheunderstandingoftheemergentlanguage,bymaximisingthe
translationaccuracy. Theresultsinthissectionareobtainedusingthebest-performingvaluesfor
eachofthehyperparameters.
5.1 Emergenceofnon-compositionalspatialreferences
UsingthePMI algorithm,wedetecttheemergenceofmessagestailoredtoconveythepositional
nc
informationcontainedintheobservations. AsmentionedinSection2,senderobservationswhich
requireshiftingconveyadditionalinformationaboutthepositionofthetargetwithinthesequence. In
6over90%ofagentpairs,theseobservationsareassigneduniquemessages,usedonlyforeachkindof
observation,i.e.,begin,begin+1,end-1and,end.
In20%ofrunswhichdevelopthesespecialisedmessages,thesamerepeatingcharacterisusedto
conveythemessage. Thecharactersusedfortheseobservationsarereservedonlyforthesekindsof
observations. Forexample,inoneoftherunstheagentsusecharacter11tosignifythebeginning
ofthesequence,withthecharacter11beingusedonlyintwocontexts: asthemessages[11,11,11]
tosignifybegin,orasamessage[0,11,11]tosignifybegin+1. Inothercases,charactersarefully
reservedforspecificmessages. e.g.,22isusedonlyforend,inthemessage[22,22,22].
Theemergenceofnon-compositionalreferencesusedforotherobservationsisalsodetectedusing
thePMI algorithm. Suchmessagesrefertoaspecificintegerinaspecificpositionofthesender
nc
observation, e.g., o = 10. While we allow for polysemy of the message in our analysis using
5
t_n=[1,2,3,5,10,15],weobservethehighesttranslationaccuracywitht_n=1,indicatingthat
thenon-compositionalmessagesdonothaveanyadditionalmeanings.
5.2 Emergenceofcompositionalspatialreferences
UsingthePMI algorithm,wealsodetecttheemergenceofcompositionalspatialreferencesfor
c
25%ofagentpairs. Suchmessagesarecomposedoftwoparts,apositionalreferenceandaninteger
reference. Thepositionalreferencespecifieswhereagivenintegercanbefoundintheobservation,in
relationtothemaskedtargetinteger−1. Theintegerreferencespecifieswhichintegerthepositional
referenceisreferringto. Forexample,onepairofagentshasassignedtheunigram7tomeanthat
thetargetintegeris2totherightofthegiveninteger,andthebigram[0,2]tomeantheinteger18.
Together,amessagecanbecomposed[7,0,2],whichmeansthatthetargetintegerforthereceiver
toidentifyis2totherightoftheinteger18,i.e.,o=[18,X,−1,X,X]. Thisallowsthesenderto
identifythetargetintegerexactly,giventhesequences.
5.3 Evaluatinginterpretationvalidityandaccuracy
Toensurethevalidityofourmessageanalysis,wepresenttwonullhypotheses,which,iffalsified,
wouldindicatethemappingsgeneratedbytheNPMImeasuretobecorrect.
Hypothesis1(H1) If the correlations exist, but are more complex (i.e., require non-trivial com-
positionality(Perkins,2021),orarehighlycontextdependent(Nikolaus,2023))thenthe
accuracyshouldbenearchance,or20%,whenusingtheidentifiedmappings.
Hypothesis2(H2) Ifthepositionalcomponentsofcompositionalmessagesareincorrectlyidentified,
ordonotcarryanymeaning,thentheirinclusionshouldnotincreasetheaccuracy.
GiventhemessagesidentifiedbytheNPMImethod,wetestH1andH2byusingadictionaryofall
messagessuccessfullyidentified,givenavalueofbothNPMIhyperparameterst andt . Adatasetis
n c
generatedtocontainonlytargetswhichcanbedescribedwiththemessagespresentinthedictionary.
For the non-compositional messages, the dataset is generated by selecting a message from the
dictionaryatrandom,andcreatinganobservationthatcanbedescribedwiththatmessage. Given
anon-compositionalmessagethatcorrespondstothetargetbeingontherightoftheinteger15,an
observationo=[1,15,−1,5,36]wouldbecreated. Analogously,fornon-compositionalpositional
messagessuchasbeginanobservationo=[−1,15,8,5,36]wouldbecreated.
For the compositional messages, we create the observations by randomly selecting a positional
componentandanintegercomponentfromthedictionary. Forexample,giventheunigram7meaning
thatXis2totheleftofthetarget,wecouldselectthebigram[8,14]correspondingtotheinteger
30. Theobservationcreatedcouldthenbeo=[30,8,−1,36,5]. Thedatasetcreationprocessforthe
compositionalmessagesalsochecksiftheobservationscanbedescribedgiventhetwon-gramsin
theirrequiredpositionswithinthemessage.
To test H2, a dataset is created using only the integers that can be described by the dictionaries,
randomlyselectingintegercomponentsfromthedictionary,andcreatingtherespectiveobservations.
Thisprocessalsoaccountsfortherequiredpositionsofthemessagecomponentssothatamessage
describing the observation can always be created. For example, if the unigram 9 described the
integer 11, and the bigram [5,1] described the integer 6, a corresponding observation could be
o = [11,6,−1,8,9]. Thepositionsoftheintegersintheobservationsarechosenatrandom. By
7Table1: AccuracyimprovementsusingtheNPMI-baseddictionary,±denotesthe1-sigmastandard
deviation.Non-CompositionalPositionalreferstomessagessuchasbeginorend,Non-Compositional
Integerreferstothenon-compositionalmonolithicmessagesdescribingboththepositionandthe
integer,Compositional-NPreferstomessagesonlycontainingtheidentifiedintegercomponents,and
theCompositional-Pwhichreferstomessagescontainingboththeidentifiedintegerandpositional
components.
DictType t t AverageAccuracy MaximumAccuracy
n c
Non-CompositionalPositional 1 0.9 0.90±0.03 0.94
Non-CompositionalInteger 1 0.5 0.36±0.004 0.37
Compositional-NP 1 0.5 0.22±0.02 0.28
Compositional-P 1 0.53 0.30±0.21 0.78
generatingbothcompositionaldatasetsusingastochasticprocess,wedonotassumeaspecificsyntax.
Rather, the syntax can only be identified by looking at messages which were understood by the
receiver.
Thesedatasets,togetherwiththeirrespectivedictionaries,arethenusedtoquerythereceiveragent,
testingifthemessagesareidentifiedcorrectly. Werunthistestforallofourtrainedagents,with
thedictionariesthatwereidentifiedforeachagentpair. Weprovidethedetailsofthisevaluationin
Table1.
Using just the non-compositional positional messages, we observe a significant increase in the
performanceoftheagents, comparedtorandomchanceaccuracyof20%. ThisprovesH1false,
showingthatatleastsomemessagesdonotrequirecomplexfunctionstobecomposed,orcontextual
informationto beinterpreted. As theaccuracyfor thesemessagesreaches over90% onaverage,
we argue that the NPMI method has captured almost all the information transmitted using these
messages.
AsmentionedinH2,weexaminetheimpactofthepositionalcomponentsandwhethertheycarry
theinformationtheNPMImethodhasidentified. We,therefore,separatethecompositionalanalysis
intotwoparts: Compositional-NP,wheretherearenopositionalcomponents,andCompositional-P,
whichincludespositionalcomponents. IntheCompositional-NPcase,theagentsachieveacloseto
randomaccuracy,whereas,intheCompositional-Pcase,agentsachieveaboverandomaccuracy,with
someagentpairsreachingover75%accuracy. ThisfalsifiesourH2nullhypothesis,showingthat
theNPMImethodhassuccessfullyidentifiedthepositionalinformationcontainedinthemessages,
togetherwiththeintegerinformation.
6 Discussion
HavingfalsifiedbothH1andH2,weconfirmedthevalidityofthelanguageanalysis. Toprovide
humaninterpretabilityoftheemergentlanguage,weusetheNPMImethodtocreateadictionary
providing an understanding of both the positional and compositional messages. We present an
excerptfromanexampledictionaryinTable2. Withhumaninterpretability,wecangainadeeper
understandingoftheprinciplesunderlyingtheagents’communicationprotocol.
Wepositthattheemergenceofcompositionalspatialreferencespointstoafirstemergenceofasimple
syntacticstructureinanemergentlanguage. Bothofthen-gramsinourexamplefromSection5.2,
alsoshowninTable2,areassignedspecificpositionsinthemessagebytheagents. Theunigram
7mustalwaysbeinthefirstpositionofthemessage,whilethebigram[0,2]mustalwaysbeinthe
secondposition. WecaninterpretthesemessagesasusingtheSubject-Verb-Object(SVO)word
order,withthesubjecttargetbeingimplied,theverbbeingtheunigram7representing“istwotothe
rightof”,andtheobjectbeing[0,2]representingtheobjectof“integer18”. Theemergenceofthis
structureshowsthateventhoughreferentialgameshavebeenconsideredobsoleteinrecentresearch
(Chaabounietal.,2022;Ritaetal.,2024),acarefuldesignoftheenvironmentmayyetelicitmoreof
thefundamentalpropertiesofnaturallanguage.
3t forthereferentpositionn-gramsissetto0.3
n
8We hypothesise that the emergence of non-compositional spatial references tailored to specific
observations, such as begin+1, is due to observation sparsity. Compositionality would bring no
benefitsincetheobservationswhichtheydescribeareusuallyrare,representing1-2%ofthedataset
and are monolithic, i.e., begin, begin+1, end-1, and end. We therefore argue that the emergence
of non-compositional references in these cases is advantageous, since these messages could be
furthercompressed. Withalinguisticparsimonypressure(Chaabounietal.,2019;Ritaetal.,2020)
applied,thesemessagescouldbemoreefficientattransmittingtheinformationcontainedwithinthese
observationsthancompositionalones.
Table2: Exampledictionaryoftheagents’messagesandtheirmeanings
Message Type Meaning
[11,11,11] Non-CompositionalPositional begin
[0,11,11] Non-CompositionalPositional begin+1
[10,10,10] Non-CompositionalPositional end-1
[18,18,18] Non-CompositionalPositional end
[12,16,14] Non-CompositionalInteger 15is1leftoftarget
[15,X,X] CompositionalPositional Xis2leftoftarget
[7,X,X] CompositionalPositional Xis2rightoftarget
[X,0,17] CompositionalInteger Integer1
[X,0,2] CompositionalInteger Integer18
[X,8,14] CompositionalInteger Integer30
7 Limitations
TheaccuracyfortheNon-CompositionalInteger,andCompositional-Pmessagesaveragesabout33%.
Whilestillaboverandom,showingthatsomemeaningiscapturedinnon-compositionalmessages,it
pointstotherebeingmoretobeunderstoodaboutthesemessages. Wehypothesisethismaybedue
tothehigherdegreeofmessagepragmatism,orcontextdependence(Nikolaus,2023). Ourmethod
ofmessagegeneration,usingrandomlyselectedparts,maynotbeabletocapturethecomplexityof
themessages. Forexample,thecontextinwhichtheyareusedmightbecrucialforsomen-grams,
requiringtheuseofaspecificn-graminsteadofanotherwhenreferringtocertainintegers,orwhen
specificintegersarepresentintheobservation. JustlikeinEnglish,certainverbsareonlyusedwith
certainnouns,suchas“pilotaplane”vs“pilotacar”. Whiletheword“pilot”inthebroadsense
referstooperatingavehicle,itisnotusedwithcarsspecifically. Thismayalsobethecaseforthe
emergentlanguage. Forcompositionalmessages,anadditionalissuemaybethatsomemessagesare
non-triviallycompositional,usingfunctionsapartfromsimpleconcatenationtoconveycompositional
meaning(Perkins,2021),makingthemimpossibletoanalysewiththeNPMImeasure. However,
theseissuesmaybeaddressedbyscalingtheemergentcommunicationexperimentsasthelanguages
becomemoregeneralwiththeincreasedcomplexityoftheirenvironment(Chaabounietal.,2022).
8 Conclusion
Recentworkinthefieldofemergentcommunicationhasadvocatedforbetteralignmentofemergent
languageswithnaturallanguage(BoldtandMortensen,2024;Ritaetal.,2024), suchasthrough
theinvestigationofdeixis(Ritaetal.,2024). Alignedtothisapproach,weprovideafirstreported
emergentlanguagecontainingspatialreferences(Lyons,1977),togetherwithamethodtointerpret
theagents’messagesinnaturallanguage.Weshowthatagentscanlearntocommunicateaboutspatial
relationshipswithover90%accuracy. Weidentifybothcompositionalandnon-compositionalspatial
referencing,showingthattheagentsuseamixtureofboth. Wehypothesisewhytheagentschoose
non-compositional representations of observation types which are sparse in the dataset, arguing
that this behaviour can be used to increase communicative efficiency. We show that, using the
NPMIlanguageanalysismethod,wecancreateahumaninterpretabledictionary,oftheagents’own
language. Weconfirmthatourmethodoflanguageinterpretationisaccurate,achievingover94%
accuracyforcertaindictionaries.
9AcknowledgmentsandDisclosureofFunding
ThisworkwassupportedbytheUKResearchandInnovationCentreforDoctoralTraininginMachine
IntelligenceforNano-electronicDevicesandSystems[EP/S024298/1].
TheauthorswouldliketothankLloyd’sRegisterFoundationfortheirsupport.
TheauthorsacknowledgetheuseoftheIRIDISHigh-PerformanceComputingFacility,andassociated
supportservicesattheUniversityofSouthampton,inthecompletionofthiswork.
Forthepurposeofopenaccess,theauthorshaveappliedaCC-BYpubliccopyrightlicencetoany
AuthorAcceptedManuscriptversionarisingfromthissubmission.
References
Marco Baroni. Rat big, cat eaten! Ideas for a useful deep-agent protolanguage. ArXiv preprint,
abs/2003.11922,2020.
BrendonBoldtandDavidR.Mortensen. AReviewoftheApplicationsofDeepLearning-Based
EmergentCommunication. TransactionsonMachineLearningResearch,2024.
GerlofJ.Bouma. Proc.ofgscl. InVonderFormzurBedeutung: Texteautomatischverarbeiten-
FromFormtoMeaning: ProcessingTextsAutomatically,volume30,pages31–40,2009.
Rahma Chaabouni, Eugene Kharitonov, Emmanuel Dupoux, and Marco Baroni. Anti-efficient
encodinginemergentcommunication. InProc.ofNeurIPS,pages6290–6300,2019.
RahmaChaabouni,EugeneKharitonov,DianeBouchacourt,EmmanuelDupoux,andMarcoBaroni.
Compositionalityandgeneralizationinemergentlanguages. InProc.ofACL,pages4427–4442,
2020.
RahmaChaabouni,FlorianStrub,FlorentAltché,EugeneTarassov,CorentinTallec,ElnazDavoodi,
Kory Wallace Mathewson, Olivier Tieleman, Angeliki Lazaridou, and Bilal Piot. Emergent
communicationatscale. InProc.ofICLR,2022.
KyunghyunCho,BartvanMerriënboer,CaglarGulcehre,DzmitryBahdanau,FethiBougares,Holger
Schwenk,andYoshuaBengio. LearningphraserepresentationsusingRNNencoder–decoderfor
statisticalmachinetranslation. InProc.ofEMNLP,pages1724–1734,2014.
KennethWardChurchandPatrickHanks. Wordassociationnorms,mutualinformation,andlexicog-
raphy. InProc.ofACL,pages76–83,1989.
RobertoDessì, EugeneKharitonov, andMarcoBaroni. Interpretableagentcommunicationfrom
scratch (with a generic visual processor emerging on the side). In Proc. of NeurIPS, pages
26937–26949,2021.
LushanHan,TimFinin,PaulMcNamee,AnupamJoshi,andYelenaYesha.Improvingwordsimilarity
byaugmentingPMIwithestimatesofwordpolysemy. IEEETKDE,25(6):1307–1322,2013.
Serhii Havrylov and Ivan Titov. Emergence of language with multi-agent games: Learning to
communicatewithsequencesofsymbols. InProc.ofNeurIPS,pages2149–2159,2017.
EricJang,ShixiangGu,andBenPoole. Categoricalreparameterizationwithgumbel-softmax. In
Proc.ofICLR,2017.
EugeneKharitonov,RahmaChaabouni,DianeBouchacourt,andMarcoBaroni. EGG:atoolkitfor
researchonemergenceoflanGuageingames. InProc.ofEMNLP,pages55–60,2019.
TomaszKorbak,JulianZubek,andJoannaRaczaszek-Leonardi. Measuringnon-trivialcomposition-
alityinemergentcommunication. In4thWorkshoponEmergentCommunication,NeurIPS2020,
2020.
AngelikiLazaridouandMarcoBaroni. EmergentMulti-AgentCommunicationintheDeepLearning
Era. ArXivpreprint,abs/2006.02419,2020.
10AngelikiLazaridou,KarlMoritzHermann,KarlTuyls,andStephenClark. Emergenceoflinguistic
communicationfromreferentialgameswithsymbolicandpixelinput. InProc.ofICLR,2018.
DavidKelloggLewis. Convention: APhilosophicalStudy. Wiley-Blackwell,1969.
Jia Peng Lim and Hady W. Lauw. Aligning Human and Computational Coherence Evaluations.
ComputationalLinguistics,pages1–58,2024.
OlafLipinski,AdamJ.Sobey,FedericoCerutti,andTimothyJ.Norman. It’sAboutTime: Temporal
ReferencesinEmergentCommunication. ArXivpreprint,abs/2310.06555,2023.
JohnLyons. Deixis,spaceandtime. InSemantics,volume2,pages636–724.CambridgeUniversity
Press,1977.
IgorMordatchandPieterAbbeel. Emergenceofgroundedcompositionallanguageinmulti-agent
populations. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proc. of AAAI, pages
1495–1502,2018.
JesseMuandNoahD.Goodman. Emergentcommunicationofgeneralizations. InProc.ofNeurIPS,
pages17994–18007,2021.
MitjaNikolaus. EmergentCommunicationwithConversationalRepair. InProc.ofICLR,2023.
DenisPapernoandMarcoBaroni. Squibs: Whenthewholeislessthanthesumofitsparts: How
compositionaffectsPMIvaluesindistributionalsemanticvectors. ComputationalLinguistics,42
(2):345–350,2016.
HughPerkins. Neuralnetworkscanunderstandcompositionalfunctionsthathumansdonot,inthe
contextofemergentcommunication. ArXivpreprint,abs/2103.04180,2021.
MathieuRita,RahmaChaabouni,andEmmanuelDupoux. “LazImpa”: Lazyandimpatientneural
agentslearntocommunicateefficiently. InProc.ofCoNLL,pages335–343,2020.
MathieuRita,PaulMichel,RahmaChaabouni,OlivierPietquin,EmmanuelDupoux,andFlorian
Strub. LanguageEvolutionwithDeepLearning,2024.
ShaneSteinert-Threlkeld. TowardtheEmergenceofNontrivialCompositionality. Philosophyof
Science,87(5):897–909,2020.
AntonThielmann,ArikReuter,QuentinSeifert,ElisabethBergherr,andBenjaminSäfken. Topicsin
theHaystack: EnhancingTopicQualitythroughCorpusExpansion. ComputationalLinguistics,
pages1–37,2024.
RyoUedaandKokiWashio. OntherelationshipbetweenZipf’slawofabbreviationandinterfering
noiseinemergentlanguages. InProc.ofACL,pages60–70,2021.
RyoUeda,TaigaIshii,andYusukeMiyao. OntheWordBoundariesofEmergentLanguagesBased
onHarris’sArticulationScheme. InProc.ofICLR,2023.
RyosukeYamaki,TadahiroTaniguchi,andDaichiMochihashi. HolographicCCGParsing. InProc.
ofACL,pages262–276,2023.
11A TrainingDetails
ThecomputationalresourcesneededtoreproducethisworkareshowninTable3,withthehyper-
parametersinTable4. TheTable3showsresourcesrequiredforalltrainingandevaluation. The
processorsusedwereamixtureofIntelXeonSilver4216sandAMDEPYC7502s. TheGPUsused
forthetrainingwereamixtureofNVIDIAQuadroRTX8000s,NVIDIATeslaV100s,andNVIDIA
A100s. Thesenodesusedinourexperimentswerehostedonaninternalcluster. Thedevelopment
processconsumedmorecompute,whichweestimatewouldhaveadded10CPUandGPUhours,to
accountforexperimentation.
Table3: Computeresources
Resource Value(1Run) Value(TrainingTotal) Value(Evaluation&Analysis)
Nodes 1 8 1
CPU 16cores 128cores 64cores
GPU 1 8 1
Memory 50GB 400GB 120GB
Storage 1GB 32GB 32GB
Walltime 2hours 240hours 24hours
Table4: Hyperparameters
Parameter Value
Epochs 1000
Optimizer Adam
LearningRateα 0.001
Gumbel-SoftmaxTemperature [1.0]
TrainingDatasetSize 200k
TestDatasetSize 20k
No. Distractors 4
No. Points 60
MessageLength 3
VocabularySize 26
SenderHiddenSize 64
ReceiverHiddenSize 64
B AlgorithmDescriptions
ForourpseudocodewewillbeusingthePythonassignmentsconvention,i.e.,=and←areequivalent,
andx+=1isequivalenttox ← x+1. Thealgorithmspresentedarefortop_n = 1. Toimprove
thecomputationalefficiency. theprobabilityoftheintegerappearingisstaticallydefinedas 1 for
60
top_n = 1,orinEquation(1)fortop_n > 1. Inthecaseoftop_n > 1weusetheprobabilityfor
theintegerasperEquation(1),toaccountforthepolysemy,i.e.,theprobabilityforanyoftop_n
integersoccurringintheobservation. Thelowerpartofthebinomialis4,asthereare4integersthat
canbesampledfromthe60possibleintegers,insteadof5,asweexcludethetargetinteger.
(cid:0)60(cid:1) −(cid:0)60−top_n(cid:1)
p(integers)= 4 4 (1)
(cid:0)60(cid:1)
4
Additionally,inthePMI algorithm,wespecifyaprobabilitytoequalto0.98inLine74andLine77.
c
Thisisasimplificationofthecalculationforclarityofthepseudocode. Thisprobabilityisinstead
obtainedusingthecountofagiventypeofobservation,dividedbythenumberoftotalobservations.
Thiscalculationisperformedforeachtypeofobservation,i.e.,begin,begin+1,end,end-1andmiddle.
Theprobabilityofthemiddleobservationisverycloseto1,beingonaverage0.98,whiletheother
probabilitiesareonaverage0.005. Sincetheanyobservationismostcommon,weincludeditsvalue
inthepseudocode.
12Algorithm2:ThePMI algorithm
nc
Data: O_M ; #Allobservationstogetherwithsentmessages
Data: L=len(O_M); #Totalnumberofobservationswithsentmessages
Data: S =[begin,begin+1,end−1,end]; #Listofpositionalobservations
Result: pmi [m][NPMI]
nc
1 pmi nc =dict;
2 foro,m∈O_M do
3 pmi nc[m][count]+=1; #Messageoccurrences
4 forpos∈S do
5 ifo==posthen
6 pmi nc[pos][count]+=1; #Positionalobservationscount
7 pmi nc[m][pos]+=1; #Messagesentwithpositionalobservation
8 end
9 end
10 forinteger ∈odo
11 pmi nc[m][integer_pos][integer]+=1; #Messagesentwithintegeringivenposition
12 end
13 end
14 forpos∈S do
15 posit total =pmi nc[pos][count]; #Countofpositionalobservations
16 p(pos)= posittotal; #Estimateobservationprobability
L
17 form∈pmi nc[m]do
18 m total =pmi nc[m][count]; #Totalcountofmessage
19 ms total =pmi nc[m][pos]; #Totalcountofmessagewithpositionalobs
20 p(m)= mtotal ; #Estimatemessageprobability
L
21 p(m,pos)= mstotal ; #Estimatejointprobability
L
22 h(m,pos)=−log 2(p(m,pos));
23 pmi(m,pos)=log 2( pp (m(m )p,p (pos o) s));
24 npmi(m,pos)= pmi(m,pos);
h(m,pos)
25 pmi nc[m][NPMI]=npmi(m,pos);
26 end
27 end
28 forpos∈pmi nc[m]do
29 forinteger ∈pmi nc[m][pos]do
30 p(pos)= 1 ; #Estimatedobservationprobabilityfor60integers
60
31 m total =pmi nc[m][count]; #Totalcountofmessage
32 ms total =pmi nc[m][pos][integer]; #Totalcountofmessagewithintegeringivenposition
33 p(m)= mtotal ; #Estimatemessageprobability
L
34 p(m,pos)= mstotal ; #Estimatejointprobability
L
35 h(m,pos)=−log 2(p(m,pos));
36 pmi(m,pos)=log 2( pp (m(m )p,p (pos o) s));
37 npmi(m,pos)= pmi(m,pos);
h(m,pos)
38 pmi nc[m][pos][integer][NPMI]=npmi(m,pos);
39 end
40 end
13Algorithm3:ThePMI algorithm
c
Input: t c; #Confidencevalue
Data: O_M ; #Allobservationstogetherwithsentmessages
Data: L=len(O_M); #Totalnumberofobservationswithsentmessages
Data: ngrams; #ListofallmessagengramspresentinO_M
Result: pmi [m][NPMI]
c
1 pmi c =dict;
;#Firstweidentifyngramscorrespondingtointegers.
2 forngram∈ngramsdo
3 foro,m∈O_M do
4 ifngram∈mthen
5 pmi c[ngram][count]+=1; #Totalngramoccurrences
6 pmi c[ngram][ngram_pos][count]+=1;#ngramoccurrencesincludingngramposition
7 forinteger ∈odo
8 pmi c[ngram][integer][count]+=1; #ngramsentwithintegeringivenposition
9 pmi c[ngram][ngram_pos][integer][count]+=1; #ngramingivenpositionsent
withintegeringivenposition
10 end
11 end
12 end
13 end
;#CalculateintegerNPMI.
14 forngram∈ngramsdo
;#PositionvariantNPMI.
15 forpos∈pmi c[ngram][ngram_pos]do
16 p(integer)= 1 ; #Estimatedobservationprobabilityfor60integers
60
17 integer p =max(pmi c[ngram][integer][count]);; #Findintegerwithhighest
co-ocurrencegivenposition
18 ngram pos =pmi c[ngram][ngram_pos][count];
19 p(ngram pos)= ngra Lmpos
20 p(ngram pos,integer)= pmic[ngram][ngram L_pos][integer][count];
21 h(ngram pos,integer)=−log 2(p(ngram pos,integer));
22 pmi(ngram pos,integer)=log 2( pp (n(n gg rara mm pop so )s p,i (n inte tg eger e) r));
23 npmi(ngram pos,integer)= pm h(i n(n grg ar mam pop so ,s i, nin tet ge eg re )r);
24 pmi c[ngram][ngram_pos][integer]=npmi(ngram pos,integer);
25 end
;#PositioninvariantNPMI.
26 integer =max(pmi c[ngram][integer][count]);; #Findintegerwithhighestco-ocurrence
27 p(integer)= 1 ; #Estimatedobservationprobabilityfor60integers
60
28 ngram total =pmi c[ngram][count];
29 p(ngram)= ngramtotal ; #Ifngramislength1,itcouldappear3timespermessage
L×(4−len(ngram))
30 p(ngram,integer)= pmic[ngram][integer][count];
L
31 h(ngram,integer)=−log 2(p(ngram,integer));
32 pmi(ngram,integer)=log 2( pp (n(n gg rara mm )p,i (n inte tg eger e) r));
33 npmi(ngram,integer)= pmi(ngram,integer);
h(ngram,integer)
34 pmi c[ngram][integer]=npmi(ngram,integer);
35 end
14Algorithm4:ThePMI algorithmcont.
c
;#Nowweidentifyngramscorrespondingtoreferentpositions.
36 ngram pr =dict;
;#PrunengramswithNPMIbelowc
37 forngram∈pmi cdo
38 forinteger ∈pmi c[ngram]do
39 ifpmi c[ngram][integer]<t cthen
40 delpmi c[ngram][integer];
41 end
42 forpos∈pmi c[ngram]do
43 forinteger ∈pmi c[ngram][pos]do
44 ifpmi c[ngram][pos][integer]<t cthen
45 delpmi c[ngram][pos][integer];
46 end
47 end
48 end
49 end
50 end
;#Findmessageswithintegerngrams
51 forngram∈pmi c[ngram]do
52 foro,m∈O_M do
;#Positionvariantngram
53 ifpmi c[ngram][pos]then
54 ifngram∈m[pos]then
55 new_ngram=m−ngram; #Getleftoverngram
56 pr =pos(pmi c[ngram][pos][integer],msg); #Getthepossiblereferentposition
57 ngram pr[new_ngram][pr][count]+=1; #Countleftoverngramoccurence
58 ngram pr[new_ngram][pos][pr][count]+=1; #Countleftoverngramoccurence
ingivenpositions
59 end
60 end
;#Positioninvariantngram
61 else
62 ifngram∈mthen
63 new_ngram=m−ngram; #Getleftoverngram
64 pr =pos(pmi c[ngram][integer],msg); #Getthepossiblereferentposition
65 ngram pr[new_ngram][pr][count]+=1; #Countleftoverngramoccurence
66 ngram pr[new_ngram][pos][pr][count]+=1; #Countleftoverngramoccurence
ingivenpositions
67 end
68 end
69 end
70 end
15Algorithm5:ThePMI algorithmcont.
c
;#CalculatereferentpositionNPMI.
71 forngram∈ngram pr do
72 forpr ∈ngram pr[ngram][pr]do
;#PositionvariantNPMI.
73 forpos∈ngram pr[ngram][pos][pr]do
74 p(pr)=0.98; #Estimatedobservationprobabilityforgivenposition
75 ngram pos =ngram pr[ngram][pos][pr][count];p(ngram pos)= ngra Lmpos
p(ngram ,pr)= ngrampr[ngram][pos][pr][count];
pos L
h(ngram ,pr)=−log (p(ngram ,integer));
pos 2 pos
pmi(ngram ,pr)=log ( p(ngrampos,pr) );
pos 2 p(ngrampos)p(pr)
npmi(ngram ,pr)= pmi(ngrampos,pr);
pos h(ngrampos,pr)
pmi [ngram][pos][pr]=npmi(ngram ,pr);
c pos
76 end
;#PositioninvariantNPMI.
77 p(pr)=0.98; #Estimatedobservationprobabilityforgivenposition
78 ngram=max(ngram pr[ngram][pr][count]); #Findhighestpositionalreferencecount
79 p(ngram)= ngram;
L
80 p(ngram,pr)= ngrampr[ngram][pr][count];
L
81 h(ngram,pr)=−log 2(p(ngram,integer));
82 pmi(ngram,pr)=log 2( pp (n(n gg rara mm )p,p (pr) r));
83 npmi(ngram,pr)= pmi(ngram,pr);
h(ngram,pr)
84 pmi c[ngram][pr]=npmi(ngram,pr);
85 end
86 end
16