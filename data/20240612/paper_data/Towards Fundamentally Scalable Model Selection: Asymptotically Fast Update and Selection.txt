Towards Fundamentally Scalable Model Selection:
Asymptotically Fast Update and Selection
WenxiaoWang∗ WeimingZhuang LingjuanLyu†
UniversityofMaryland SonyAI SonyAI
wwx@umd.edu weiming.zhuang@sony.com lingjuan.lv@sony.com
Abstract
The advancement of deep learning technologies is bringing new models every
day,motivatingthestudyofscalablemodelselection. Anidealmodelselection
schemeshouldminimallysupporttwooperationsefficientlyoveralargepoolof
candidatemodels: update,whichinvolveseitheraddinganewcandidatemodel
orremovinganexistingcandidatemodel,andselection,whichinvolveslocating
highlyperformingmodelsforagiventask. However,previoussolutionstomodel
selection require high computational complexity for at least one of these two
operations. Inthiswork, wetargetfundamentally(more)scalablemodelselec-
tion that supports asymptotically fast update and asymptotically fast selection
at the same time. Firstly, we define isolated model embedding, a family of
model selection schemes supporting asymptotically fast update and selection:
With respect to the number of candidate models m, the update complexity is
O(1) and the selection consists of a single sweep over m vectors in addition to
O(1)modeloperations. Isolatedmodelembeddingalsoimpliesseveraldesirable
propertiesforapplications. Secondly,wepresentStandardizedEmbedder,an
empirical realization of isolated model embedding. We assess its effectiveness
byusingittoselectrepresentationsfromapoolof100pre-trainedvisionmodels
forclassificationtasksandmeasuringtheperformancegapsbetweentheselected
modelsandthebestcandidateswithalinearprobingprotocol.Experimentssuggest
our realization is effective in selecting models with competitive performances
andhighlightisolatedmodelembeddingasapromisingdirectiontowardsmodel
selectionthatisfundamentally(more)scalable.
1 Introduction
New models are being created and becoming available at a rate beyond previous imaginations.
Hugging Face Hub, a web platform for hosting machine learning models, datasets, and demo
applications, included more than 300k pre-trained models in August 2023, when its owner, the
company named Hugging Face, obtained a $4.5 billion valuation while raising funding of $235
millionbackedbymultipleinvestors.3 Suchvastamountsofmodelswillbecomemorevaluableifwe
canidentifytheonessuitablefortasksofinterest,motivatingthestudyofscalablemodelselection.
Arguably, an ideal model selection scheme should minimally be able to support two operations
efficientlyoveralargenumberofcandidatemodels,whichareupdateandselection. Updateisan
operationtoeitheraddanewcandidatemodelortoremoveanexistingcandidatemodel,whichis
necessaryforkeepingadynamicmodelpoolthatisup-to-date. Selectionisanoperationtolocate
∗WorkdoneduringWenxiao’sinternshipatSonyAI.
†CorrespondingAuthor.
3https://www.nasdaq.com/articles/ai-startup-hugging-face-valued-at-$4.5-bln-in-latest-round-of-funding
Preprint.Underreview.
4202
nuJ
11
]GL.sc[
1v63570.6042:viXra(a) Brute-force (b) ModelEmbedding (c) IsolatedModelEmbedding
Figure1: Illustrationsfordifferentfamiliesofmodelselectionschemes. Isolatedmodelembedding
(ours)isafamilythatsupportsasymptoticallyfastupdateandselectionatthesametime.
propermodelsforagiventask,whichisthecorefunctionalityofmodelselection. Unfortunately,
existingsolutionsrequirehighcomputationalcomplexityforatleastoneofthesetwooperations.
Themostnaivesolutionformodelselectionistodirectlyexamineeverycandidatemodelonthegiven
taskforeachselectionoperation,i.e. abrute-forcesolutionasillustratedinFigure1(a). Consequently,
thecomputationalcomplexity,measuredbythenumberofmodeloperations(e.g. forward/backward
passes),islineartothenumberofcandidatemodelsforeachselectionoperation. Suchcomplexityis
prohibitivelyhighwhendealingwithalargenumberofmodelsandwithmultipledownstreamtasks
(i.e. multipleselectionoperations),evenwhenusingtransferabilitymetrics[3,33,51,34,23,2,4]as
surrogatestoreducethecomputationalcostofmodeltraining/tuning.
Modelembeddingoffersadirectiontoimproveselectioncomplexity. Risingfromthestudyoftask
similarities[55,1,27,56],Model2Vec[1]computesanembeddingvectorforeachcandidatemodel,
which can be used repeatedly across multiple selection operations. For each selection operation,
ataskembeddingwillfirstbecomputedusingthecorrespondingdownstreamdata,whichtakesa
boundedamountofmodeloperationsthatisindependentfromthenumberofcandidatemodels,and
thenbeusedtocomparewithpre-computedmodelembeddingstolocatethepromisingcandidate
models, whichtakesasinglesweepoverthemodelembeddings. Asaresult, withrespecttothe
numberofcandidatemodelsm,thenumberofmodeloperationsperselectionisreducedtoO(1)
withthecostofasingleadditionalsweepovermvectors. AnillustrationisprovidedinFigure1(b).
However,Model2Veccomputesthemodelembeddingsthroughajointoptimizationthatinvolvesall
candidatemodels,meaningthattheModel2Vecembeddingofeachmodeldependsonnotonlythe
modelitselfbutalsootheronesinthecandidatepool. Thisimplieshighcomplexityforeachupdate
operationthatislineartothenumberofcandidatemodels(assumingnomajoralgorithmicchange)as
existingmodelembeddingsaresubjecttochangewiththeadditionofanynewcandidatemodel.
Inthiswork,wetargetfundamentally(more)scalablemodelselectionthatsupportsasymptoti-
callyfastupdateandasymptoticallyfastselectionatthesametime. Specifically:
(1)Wedefineisolatedmodelembedding,afamilyofmodelselectionschemeswithasymptotically
fast update and selection. Intuitively, isolated model embedding refers to a subset of the model
embeddingfamilywherecomputingtheembeddingofeachindividualmodelisisolatedfromthe
others,asillustratedinFigure1(c).Inanotherword,theembeddingofeachmodelwillbeindependent
fromothermodelsinthecandidatepool,whichimpliesthateachupdateoperationinvolvesonlya
singlecandidatemodelasallthepreviouslycomputedmodelembeddingsstayunchanged. Thusfor
isolatedmodelembedding,withrespecttothenumberofcandidatemodelsm,theupdatecomplexity
is O(1) and the selection consists of a single sweep over m vectors in addition to O(1) model
operations. Notably,isolatedmodelembeddingalsoimpliesseveralotherdesirableproperties.
(2)WepresentStandardizedEmbedder,anempiricalrealizationofisolatedmodelembedding. The
keyintuitionofStandardizedEmbedderisstandardization,i.e. usingonepublicmodelasthebaseline
toembedalldifferentcandidatemodels,thusensuringtheindependentlylearnedembeddingvectors
conformtothesamestandardandarethereforecomparable. Weuseittoselectrepresentationsfrom
a pool of 100 pre-trained vision models for classification tasks and assess the performance gaps
betweentheselectedmodelsandthebestcandidateswithalinearprobingprotocol. Experiments
2suggest our realization is effective in selecting competitive models and highlight isolated model
embeddingasapromisingdirectiontowardsfundamentally(more)scalablemodelselection.
2 RelatedWork
Transferability metrics for model selection. Intuitively, transferability metrics are scores that
correlatewiththeperformanceofmodels/featuresafterbeingtransferredtoanewtaskandcanbe
computedwithouttraining: H-score[3]isdefinedbyincorporatingtheestimatedinter-classvariance
andtheredundancyoffeatures;LEEP[33]estimatesthedistributionoftargettasklabelconditioned
onthelabelofpre-trainedtaskstoconstructadownstreamclassifierwithouttraininganduseits
performanceasthemetric;LogME[51]estimatesthemaximumvalueofthemarginalizedlikelihood
ofthelabelgivenpre-trainedfeaturesandusesitslogarithmasthescore;GBC[34]usesclass-wise
Gaussianstoapproximatedownstreamsamplesinthepre-trainedfeaturespacesothatclassoverlaps
can be computed with Bhattacharyya coefficients to serve as a score; TransRate [23] estimates
mutualinformationbetweenfeaturesandlabelsbyresortingtocodingrate. Separateevaluationsof
transferabilitymetricsareconductedbyAgostinellietal.[2]andBolyaetal.[4].
Modelembeddingformodelselection. Model2VecisproposedjointlywithTask2VecbyAchille
etal.[1]. ThegoalofTask2Vecistoembeddifferenttasksintoasharedvectorspaceanditdoes
sobyestimatingtheFisherinformationmatrixwithrespecttoaprobenetwork. Toembedmodels,
theyfirstinitializetheembeddingofeachmodelasthesumoftheTask2VecembeddingF ofits
pre-trainedtask(whichissetto0ifthetaskisunknown)andalearnableperturbationb. Thenthey
learntheperturbationsofallmodelsjointlybypredictingthebestmodelgiventhedistancesofmodel
embeddingstothetaskembeddings,whichrequiresaccesstomultipledownstreamtasksinadvance.
3 AFamilyofModelSelectionwithAsymptoticallyFastUpdateandSelection
3.1 FormalDefinition
Isolatedmodelembedding,aswedefined,isafamilyofmodelselectionschemesconsistingof
twoparts,asillustratedinFigure1(c): (1)preprocessing,wherecandidatemodelsareconverted
intoembeddingvectorsforlateruseinselectingmodels;(2)selection,wheremodelsareselected
givendownstreamdata(i.e. datacorrespondingtothedownstreamtask)andthemodelembeddings
generatedduringpreprocessing.
Preprocessing. The preprocessing part is defined by a model embedding function of the form
P(model) → V, where ‘model’ denotes a single (candidate) model and V denotes an embed-
ding space. Intuitively, P maps a single model into its corresponding embedding vectors. Let
f ,f ,...,f beallcurrentcandidatemodelswheremisthetotalnumberofcandidatemodels. For
1 2 m
preprocessing,embeddingvectorsofallcandidatemodelsaregeneratedas{v =P(f )}m ,where
i i i=1
v istheembeddingvectorcorrespondingtothecandidatemodelf . Notably,thisdefinitionnaturally
i i
enforcesthemodelembeddingprocesstobeisolated:ThemodelembeddingfunctionPtakesasingle
candidatemodelasitsinputandisappliedindependentlytoeachcandidatemodel,whichmeansthe
computingofeachindividualembeddingisisolatedfromothermodelsinthecandidatepool. Update
operationsofisolatedmodelembeddingaredefinedwithinpreprocessing,whereaddinganewmodel
isessentiallycomputingitscorrespondingembeddingswiththemodelembeddingfunctionP and
removingacandidatemodelisremovingitscorrespondingmodelembedding.
Selection. TheselectionpartisdefinedbyataskembeddingfunctionoftheformQ(data)→V anda
selectionmetricoftheformδ :V×V →R,where‘data’denotesthedownstreamdataandV denotes
againtheembeddingspace. Thesedefinetheselectionoperationsofisolatedmodelembedding.
Foreachselectionoperation,ataskembeddingvectorv willfirstbecomputedbyapplyingthe
task
taskembeddingfunctionQtothedownstreamdataandthenbecomparedwithpre-computedmodel
embeddingstolocatepromisingcandidatemodelsbyfindingmodelembeddingsv thatmaximize
i
theselectionmetricδ(v ,v ).
task i
3Table1: Asummaryofthecomputationalcomplexityperoperationfordifferentfamiliesofmodel
selectionschemeswithrespecttothe(current)numberofcandidatemodelsm. *Selectioncomplexity
ismeasuredbythenumberofmodeloperations,e.g. forward/backwardpassesofmodels.
Family UpdateComplexity SelectionComplexity*
Brute-force O(1) O(m)
Modelembedding O(m) O(1)
Isolatedmodelembedding O(1) O(1)
3.2 AsymptoticallyFastUpdateandSelection
Inthissection, weshowhowisolatedmodelembeddingsupportsasymptoticallyfastupdateand
selectionbyanalyzingcomputationalcomplexity. Asummaryofupdatecomplexityandselection
complexityfordifferentfamiliesofmodelselectionschemesisincludedinTable1.
Updatecomplexity: Anupdateoperationeitheraddsanewcandidatemodelorremovesanexisting
one. Withisolatedmodelembedding,addinganewmodelisessentiallycomputingitscorresponding
embeddingswiththemodelembeddingfunctionP. SincethemodelembeddingfunctionP takes
asinglemodelasitsinputandhasnodependencyonthenumberofthecandidatemodelsm,the
computationalcomplexityofP (andthereforethecomplexityofaddinganewmodel)mustbeO(1)
withrespecttom. Removingacandidatemodelissimplyremovingitscorrespondingembedding
vectorsfromamaintainedlistcontainingembeddingsofallthe(current)candidatemodels,which
canalsobeO(1). Thuswithrespecttothenumberofcandidatemodelsm,thecomplexityperupdate
operationisO(1)forisolatedmodelembedding.
Selectioncomplexity: Withisolatedmodelembedding,aselectionoperationconsistsoftwosteps.
ThefirststepistousethetaskembeddingfunctionQtocomputeataskembeddingvectorv . The
task
taskembeddingfunctionQtakesthedownstreamtaskdataastheinputandandhasnodependency
on thenumber ofthe candidate models m, thus the computational complexity of Q is O(1)with
respecttom,whichalsomeansthattherecanbeatmostO(1)modeloperations. Thesecondstepis
tocomparethetaskembeddingv withthemodelembeddings{v }m bycomputingtheselection
task i i=1
metricδ(v ,v ),whichtakesasinglesweepovermmodelembeddingvectors. Asaresult,with
task i
respect to the number of candidate models m, each selection operation consists of O(1) model
operationsinadditiontoasinglesweepovermembeddingvectors. Notably,whiletheselection
complexityistechnicallystillO(m),reducingthenumberofmodeloperationsfromO(m)toO(1)
greatlyimprovesscalability,asmodeloperations(e.g. forward/backwardpasses)aretypicallyorders
ofmagnitudeslowerthanvectoroperations(e.g. innerproducts)inpractice.
3.3 OtherDesirableProperties
Isolatedmodelembeddingisnaturallydecentralizable. Modelownerscanmaketheirmodels
candidatesoffuturemodelselectionsentirelyontheirown,requiringnocollaborationwithother
modelownersandrequiringnocentralizedcoordination. GiventhemodelembeddingfunctionP,
modelownerscanindependentlyembedtheirmodelsandpublish/broadcasttheresultingembeddings
bythemselves. Afterthat,anypartycanusethetaskembeddingfunctionQandtheselectionmetric
δtoselectmodelsforitsowntasks,usingallthemodelembeddingsthatithasaccessto. Thisisa
quitesimpledecentralizedprotocolfromisolatedmodelembedding.
Embeddingvectorsareflexibleandportableinformationcarriers. Modelembeddingsaresimply
(real)vectors,whichcanbestoredandprocessedindiverseformatsthroughvariouspackages,making
itabridgeconnectingdifferentimplementationframeworks,differentownersanddifferentplatforms.
Forexample,somemodelownersmayusePyTorchfortheirmodelsandforimplementingthemodel
embeddingfunctionP whilesomemodelownersmayuseTensorFlow,butthiscreateslittledifficulty
foraselectionoperationevenifitisimplementedinneitherframeworkssincetheselectionoperation
onlyneedstorecognizethecomputedembeddingvectors. Inaddition,typicaldimensionsofmodel
embeddingsarefairlysmallandthereforethesizesarequiteportable: Forinstance,inlaterempirical
evaluations,weincorporatesettingswith512-dimensionaland768-dimensionalmodelembeddings,
whileasaninformalcomparison,eachofFigure1(a),1(b)and1(c)contains1372×1190≈1.6×106
4RGBpixels,whichisabout4.8×106dimensionswith8bitseach. Thusasingleimageofsuchhas
alreadyenoughbitstoencodetheembeddingvectorsas32-bitfloatsformorethan1000models.
Candidatemodelscanbekeptprivatethroughoutmodelselection. Modelownerscanembed
theirmodelsusingthemodelembeddingfunctionP bythemselves,andtheonlymodel-dependent
informationrequiredbyselectionoperationsisthemodelembeddings. Thus,fortheentiremodel
selectionprocess,modelownersdonotneedtoreleasetheirmodelstoanyotherparties,including
butnotlimitedtoothermodelownersanddownstreamuserswhowanttoselectmodelsfortheir
downstreamtasks. Asselectioncompletes,downstreamuserscanreachouttotheownersofselected
candidates(insteadofallmodelowners)forrequestingaccess.
Selection operations can be made invisible to model owners. Given the model embeddings,
selectionoperationscanbeperformedfullylocallybydownstreamusers. Consequently,downstream
datacanbekeptprivateandmodelownerswillhavenoknowledgeregardingtheselectionorwhether
thereisaselection,unlessthedownstreamuserschoosetonotifythem. Thiscanbeahighlyvaluable
privacyguaranteefordownstreamusers.
4 StandardizedEmbedder: ARealizationofIsolatedModelEmbedding
In this section, we present Standardized Embedder, an empirical realization of isolated model
embedding. AsdefinedinSection3andillustratedinFigure1(c),isolatedmodelembeddingcontains
twoparts: preprocessing,wheremodelembeddingvectorsarelearnedindependentlyfordifferent
candidates,andselection,whereataskembeddingvectorislearnedfromthedownstreamdataandis
usedtosearchamongmodelembeddingstoguidemodelselection. Wewillintroducesomeconcepts
astoolsinSection4.1beforewepresentthesetwopartsrespectivelyinSection4.2and4.3.
4.1 Tool: (Approximate)FunctionalityEquivalence
Firstly, we introduce notations. Let X be the input space. A feature f : X → R is defined as a
functionmappinganysamplefromtheinputspaceX toarealnumber.AsetoffeaturesF istherefore
asetoffunctions,whichcanalsobeconsideredasafunctionF :X →Rnmappinganysampletoa
vectorofndimensions,wherencanbeeitherfiniteorcountablyinfinite,dependingonwhetheror
notthesetcontainsafinitenumberoffeatures.
Definition4.1(FunctionalityEquivalence). FortwosetsF :X →RnandFˆ :X →Rmoffeatures,
theyareconsideredδ-equivalentinfunctionalityoveradistributionDoverX,ifandonlyifthere
existtwoaffinetransformationsw,b∈Rn×m×Rmandwˆ,ˆb∈Rm×n×Rnsuchthat
(cid:104) (cid:16) (cid:17)(cid:105) (cid:104) (cid:16) (cid:17)(cid:105)
E S w⊤F(x)+b,Fˆ(x) ≥1−δ and E S F(x),wˆ⊤Fˆ(x)+ˆb ≥1−δ,
x∼D cos x∼D cos
whereS (u,v)denotescosinesimilaritybetweentwovectorsuandv.
cos
Functionalityequivalencecharacterizescaseswheretwosetsoffeaturesareconsideredthesame
regarding their usability in unknown applications. Intuitively, since most (if not all) modern ar-
chitecturesofneuralnetworkshaveatleastoneaffinetransformationfollowingthefeaturelayers,
twosetsoffeaturesshouldbeconsideredequivalenteveniftheydifferbyanaffinetransformation.
Similarargumentsareintroducedby[48]tounderstanddeeprepresentations,wheretheyconsider
two representations to be equivalent when the subspaces spanned by their activation vectors are
identical. Whileinprincipleothersimilaritymetricscanbeutilizedaswell,weusecosinesimilarity
inthisworksinceitisnaturallyinvarianttofeaturescalings.
4.2 Preprocessing: IsolatedModelEmbeddingbyIdentifyingEquivalentFeatureSubsets
WithfunctionalityequivalencefromDefinition4.1,wecancharacterizetherepresentationpowers
ofanysetoffeaturesbyassociatingthemwiththeequivalentsubsetsfromapre-defined,baseline
featuresetB :X →RN (Empiricallywewilluseapublicmodelasthisbaselinefeatureset,which
willbeelaboratedinSection5). SinceanysubsetofthebaselinefeaturesetB : X → RN canbe
directlyassociatedwithabinaryvectorfrom{0,1}N (i.e. each1indicatingthepresenceofafeature
andeach0indicatinganabsence;SeeAppendixCforexamples),wesimplyusesuchvectorsasthe
embeddingsofmodels. Fortheactualimplementation,werelaxthisbinaryembeddingspacetoa
continuousone(i.e. [0,1]N). Theformaldefinitionisasfollows.
5(a) Preprocessing:isolatedmodelembedding (b) Selection:taskembedding&
selectionmetric
Figure2: AnillustrationofStandardizedEmbedder. (a)Preprocessing: Usingfeaturesofapublic
model as the baseline, a vector embedding is learned independently for each pre-trained model.
Intuitively,theembeddingsofmodelsdenotetheirapproximatelyequivalentfeaturesubsetsinthe
baselinefeatures. (b)Selection: Taskembeddingsaredefinedbysubsetsofthebaselinefeaturesthat
areimportanttocorrespondingdownstreamtasks,whichareidentifiedthroughenforcingsparsity
regularization. Modelsareselectedbycomparingthetaskembeddingwithmodelembeddingsofall
candidates,using(thecardinalityof)standardfuzzysetintersectionastheselectionmetric.
Definition4.2(VectorEmbeddingthroughEquivalentFeatureSubsets). Givenabaselinefeatureset
B : X → RN,avectorv ∈ {0,1}N isaδ-embeddingvectorofafeaturesetF : X → Rn overa
distributionDifandonlyifF and{B |v =1}areδ-equivalentinfunctionalityoverD.
i i
Givenasetoffeaturesasthebaseline,theembeddingvectorscorrespondingtoasetoffeaturesare
definedthroughDefinition4.2. Consequently,wecannowconceptuallymapeachmodel,represented
asasetoffeatures,intoavectorembeddingspacethatassociateswiththebaselinefeatures.
Inpractice,tocomputetheembeddingvectorsgivenbaselinefeatures,werelaxthebinaryembedding
spacetoacontinuousoneandreformulateitasfollows:
max min(L ,L )
tobaseline frombaseline
v,w,b,wˆ,ˆb
subjectto:L =E
(cid:2)
S
(cid:0) w⊤F(x)+b,v⊙B(x)(cid:1)(cid:3)
tobaseline x∼D cos
(cid:104) (cid:16) (cid:17)(cid:105)
L =E S F(x),wˆ⊤(v⊙B(x))+ˆb
frombaseline x∼D cos
v ∈[0,1]n,w ∈Rn×N,b∈RN,wˆ ∈RN×n,ˆb∈Rn
whereF :X →Rnisthefeaturesetthatwewanttovectorize,B :X →RN isthesetofbaseline
features, D istheunderlyingdatadistribution, v isthe(relaxed)embeddingvector, w,b,wˆ,ˆbare
parametersofaffinetransformsand⊙denotesHadamardproduct(i.e. element-wisemultiplication).
AnillustrationofthemodelembeddingprocessisincludedinFigure2(a).
Empirically,theconstraintv ∈[0,1]n isimplementedviareparameterizationthroughthesigmoid
function,i.e. v
i
=1/(1+e−v i′/τ),whereτ isaconstantknownastemperatureandweuseafixed
temperatureofτ =0.01inallexperiments. Intuitively,theoptimizationwantstofindasubsetofthe
baselinefeatures(indicatedbythemaskv)thatisδ-equivalenttoF forsmallerδ.
BothDefinition4.2andtherelaxationarestraightforward,butitisworthnotingthattheembedding
dependsonnotonlythemodel(i.e. thesetoffeatures)tobeembedded,butalsothesetofbaseline
features,andtheembeddingvectorsmaynotbeuniquebydefinitiondependingonthechoiceof
baseline features. Conceptually, what we do here is to compare the embedding distributions of
differentmodelsbydrawingasingleembeddingvectorfromeachdistribution.
4.3 Selection: TaskEmbeddingthroughFeatureSifting
Inthissection,weshowcasehowtoderiveataskembeddingvectorfromdownstreamdataandthe
selectionmetricforcomparingthetaskembeddingwithmodelembeddings. Anillustrationofthe
6processisincludedinFigure2(b): Intuitively,wederivethetaskembeddingvectorbyidentifying
subsetsofthebaselinefeaturesB : X → RN thatareimportanttothetaskofinterest,whichcan
thenbedirectlyassociatedwithbinaryvectorsfrom{0,1}N,similartohowwepreviouslyembed
modelsasvectors,andweuseameasureof(fuzzy)setsimilarityastheselectionmetric.
Formally, for a downstream task, let X be the input space, Y be the label space, we use Dˆ to
denotethedownstreamdatadistribution,whichisadistributionoverX ×Y. UsingLtodenotethe
correspondingtaskloss,identifyingimportantfeaturescanbeformulatedasfollows:
min E (cid:2) L(cid:0) w⊤(v⊙B(x))+b,y(cid:1)(cid:3) +γ||v|| subjectto:||w⊤|| =1andv ∈[0,1]n
x,y∼Dˆ 1 1
v,w,b
wherev ∈ [0,1]n istheembeddingvectorofthetasktobelearned,w,b ∈ Rn×|Y|×R|Y| jointly
denotes a prediction head associated with the task of interest, ||v|| denotes the ℓ norm of the
1 1
embeddingvector(whichfunctionsassparsityregularization),||w⊤|| denotesthematrixnormof
1
w⊤ inducedbyℓ normofvectors(i.e. ||w⊤|| = sup ||w⊤x|| /||x|| = max (cid:80) |w |)and
1 1 x̸=0 1 1 i j ij
γ issparsitylevel,ascalarhyper-parametercontrollingthestrengthofthesparsityregularization
γ||v|| . Aruleofthumbforchoosingγ issuggestedinSection5.4.
1
Selectionmetric. Giventhetaskembedding,wecompareitwithembeddingsofcandidatemodels
to identify the most similar ones to the task embedding with respect to a similarity metric—-the
correspondingmodelsaretheonestobeselected. Notably,allourembeddingvectors,including
modelembeddingsandtaskembeddings,arerelaxationsofbinaryvectorsdenotingsubsetsofthe
baselinefeatures. Thisiswellrelatedtofuzzysettheory[53,24]whereeachelementcanhavea
degreeofmembershipbetween0and1toindicatewhetheritisnot/fully/partiallyincludedinaset.
Interpretingbothmodelembeddingsandtaskembeddingsasfuzzysets,weincorporatestandard
fuzzysetintersectiontomeasurethesimilaritybetweenmodelembeddingsandtaskembeddings.
Formally,letu,v ∈[0,1]nbetwoembeddingvectors(interpretedasfuzzysets),thecardinalityof
theirstandardintersectionissimplyI
(u,v)=(cid:80)n
min(u ,v ).
standard i=1 i i
Intuitively,thetaskembeddingdenotesthesetofbaselinefeaturesusefulforthetaskofinterestand
eachmodelembeddingdenotesthesetofbaselinefeaturespossessedbythecorrespondingcandidate
model. Thusthecardinalityoftheirintersectionmeasuresthequantityoftheusefulfeaturesowned
bycandidatemodelsandthereforeprovidesguidancefordownstreamperformance.
5 Evaluation
5.1 EvaluationSetup
Forevaluations,wegather100pre-trainedmodelswithvariousarchitecturesandtrainingrecipesas
candidatesofmodelselection. SeeAppendixAforthefulllistofmodels.
Preprocessing. RecallingthatStandardizedEmbedderusesapublicmodelasthebaselinefeatures
and(approximate)functionalityequivalenceisdefinedoveranunderlyingdistributionD,weinclude
twochoicesofbaselinefeatures,apre-trainedResNet-18orapre-trainedSwinTransformer(tiny)
andweusethevalidationsetofImageNet(50000samplesintotal)astheunderlyingdistribution
D(fortheembeddingofcandidatemodels). Weusethesamehyper-parameterswhenembedding
differentcandidatemodels: WeuseSGDoptimizerwithabatchsizeof128,aninitiallearningrateof
0.1,amomentumof0.9,andaweightdecayof5e-4;Wedividethelearningratebyafactorof10
every1kstepsforsettingswith4ktrainingsteps(=10.24epochs)percandidate,andevery3ksteps
forsettingswith10ktrainingsteps(=25.6epochs)percandidate.
Selection.Weassessthequalityofrepresentationsselectedforthreedownstreambenchmarks,CIFAR-
10[25],CIFAR-100[25]andSTL-10[12],whicharenaturalobjectclassificationsbenchmarkswith
varyinggranularityandvaryingdomainshifts(comparedtoImageNetvalidationsetthatweuseas
thedistributiontoembedmodels). Tolearntaskembeddings,weuseSGDoptimizerwithabatchsize
of128,aninitiallearningrateof0.1,amomentumof0.9,andaweightdecayof5e-4for60epochs,
withthelearningrategettingdividedbyafactorof10every15epochs. Notably,theweightdecayis
disabledforthetaskembeddingtobelearnedtoavoidinterferingwiththesparsityregularization
fromSection4.3. Forcoherence,wediscussthechoiceofthesparsitylevelγ inSection5.4.
Evaluation. Toestimateperformancegapsbetweentheselectedmodelsandthebestcandidates,
we incorporate a linear probing protocol commonly used to assess the quality of representations
7Table 2: Empirical evaluations of Standardized Embedder with 100 pre-trained models as the
candidates(SeeAppendixAforthefulllist). StandardizedEmbeddersuccessfullylocatesmodels
comparabletothebestcandidatesforcorrespondingdownstreamtasks.
Downstreamaccuracyofselectedmodels
Bestcandidate Modelusedas Trainingsteps
Downstreamtask (+gapfromthebestcandidate)
(Groundtruth) baselinefeatures percandidate
bestoftop1 bestoftop3 bestoftop5
91.81% 94.57% 94.57%
4k
(3.34%) (0.58%) (0.58%)
ResNet-18
94.36% 95.12% 95.12%
10k
(0.79%) (0.03%) (0.03%)
CIFAR-10 95.15%
94.57% 94.57% 95.12%
4k
(0.58%) (0.58%) (0.03%)
Swin-T(tiny)
95.12% 95.12% 95.12%
10k
(0.03%) (0.03%) (0.03%)
81.11% 81.11% 81.11%
4k
(1.47%) (1.47%) (1.47%)
ResNet-18
80.13% 80.13% 81.57%
10k
(2.45%) (2.45%) (1.01%)
CIFAR-100 82.58%
81.11% 81.48% 81.48%
4k
(1.47%) (1.10%) (1.10%)
Swin-T(tiny)
81.48% 81.48% 81.48%
10k
(1.10%) (1.10%) (1.10%)
98.76% 98.76% 98.76%
4k
(0.47%) (0.47%) (0.47%)
ResNet-18
97.51% 98.60% 98.76%
10k
(1.72%) (0.64%) (0.47%)
STL-10 99.24%
97.69% 98.60% 98.60%
4k
(1.55%) (0.64%) (0.64%)
Swin-T(tiny)
98.60% 98.60% 98.60%
10k
(0.64%) (0.64%) (0.64%)
[8,21,49,10]. Foreverycandidatemodelandforeverydownstreamtask,alinearheadistrained
overitsfeaturestocomputethecorrespondingdownstreamaccuracy. WeuseSGDwithabatchsize
of128,aninitiallearningrateof0.1,amomentumof0.9,andaweightdecayof5e-4for60epochs,
withthelearningrategettingdividedbyafactorof10every15epochs.
5.2 ThePerformanceofStandardizedEmbedder
InTable2,weincludethequantitativeevaluationsontheperformanceofStandardizedEmbedder.
Foreachdownstreamtask,wereportinthelefthalfofthetablethedownstreamaccuracyofthebest
candidates(i.e.thegroundtruth)forreferencesandreportintherighthalfofthetablethedownstream
accuracyofmodelsselected(i.e. thetop-1/top-3/top-5candidatesaccordingtotheselectionmetric).
StandardizedEmbeddersuccessfullylocatesmodels/representationscomparabletothebestpossible
candidateswithrespecttodifferentdownstreamtasks: Whenselectingonly1modelfromthe100
candidates,theworstaccuracygapacrossallevaluatedsettingsofStandardizedEmbedder(i.e.
withdifferentbaselinefeaturesandtrainingstepspercandidate)andalldownstreamtasksevaluated
is3.34%,whichisreducedto2.45%whenselecting3modelsand1.47%whenselecting5models;
WhenusingSwinTransformer(tiny)asbaselinefeaturesand10kembeddingstepspercandidate,the
worstgapisonly1.10%evenwhenselectingonly1model.
InFigure3(a)and3(b),wepresentboththedownstreamaccuracyandthecardinalityofstandard
intersection(i.e. theselectionmetric)fordifferentdownstreamtaskswhenusingdifferentbaseline
features. MoreresultsareincludedinFigure4and5inAppendix. Weusethedashedlinetohighlight
thedownstreamaccuracyofthepublicmodelusedasbaselinefeatures. Animportantobservation
hereisthatStandardizedEmbedderisabletolocatemuchmorecompetitivemodelswhenthepublic
modelusedasbaselinefeaturesisonlysuboptimal(i.e. thepublicmodelusedasbaselinefeatures
performsconsiderablyworseonthedownstreamtaskscomparedtothebestpossiblecandidate).
5.3 OntheChoiceofBaselineFeatures
Table2suggeststhatStandardizedEmbedderperformsbetterwhenusingSwinTransformer(tiny)
asbaselinefeaturescomparedtowhenusingResNet-18. Tofurtherunderstandthis,wecompare
the selection metric of candidate models on each of the evaluated downstream tasks when using
different baseline features in Figure 3(c), with more results included in Figure 6 and Figure 7 in
Appendix,whereinallcasesthereareclustersofgreen/orangepointsinthebottomright,which
correspondstoTransformersandhybridmodels(i.e. modelswithbothconvolutionandattention)
8(a) baselinefeatures: (b) baselinefeatures: (c) Choice of baseline (d) Choosingsparsitylevel
ResNet-18 Swin-T(tiny) features
Figure3: (a,b)Downstreamaccuracy(i.e. thegroundtruth)onCIFAR-10v.s. thecardinalityof
standardintersections(i.e. theselectionmetric)whenusing4kstepspercandidate. Thedownstream
accuracyofthebaselinefeaturesarehighlightedwiththedashedline. Whenapublicmodelisonly
suboptimal,usingitasbaselinefeaturesforStandardizedEmbeddercanstilllocatemorecompetitive
models. SeeFigure4and5inAppendixformoreresultsincludingotherdownstreamsandmoresteps.
(c)Comparingthecardinalityofstandardintersections(i.e. theselectionmetric)whenusingdifferent
baselinefeatures(ResNet-18andSwin-T(tiny))with4kstepspercandidateandCIFAR-10asthe
downstreamtask. Thegreen/orangepointsinthebottomrightsuggestusingResNet-18asbaseline
featurestendtooverestimate(some)modelswithattentionscomparedtousingSwinTransformer
(tiny). SeeFigure6and7inAppendixformoreresultsincludingotherdownstreamsandmoresteps.
(d)DownstreamaccuracyonCIFAR-10ofthebaselinefeaturesResNet-18correspondingtovarying
sparsityregularizationγ. Aruleofthumbfordecidingthevalueofγ: usingthesmallestγ withat
least3%accuracydropfromtheconvergedaccuracy. SeeFigure8inAppendixformoreresults.
that are overrated when using ResNet-18 as baseline features. This is likely because the limited
receptivefieldsofConvNetspreventsthemfromcapturingsomelong-rangecorrelationsutilizedby
modelswithattention(i.e. Transformersandhybridmodels),whichsuggestsmodelswithattention
arecurrentlybetterchoicesofbaselinefeaturesthanConvNets.
ApossiblelimitationofStandardizedEmbedderistherobustnessofthechoiceofbaselinefeaturesto
primaryparadigmshifts. WhileourresultssuggeststhatusingResNet-18,aConvNetfrom2015,
remainseffectivewiththepresenceofmanymodelsproposedafter2020and/orbasedonattention,
thereisnotheoreticalguaranteeregardingitsrobustnessundermajorparadigmshiftsinthefuture.
Ifthatisthecase,thechoiceofbaselinefeaturesmightneedtobeupdatedaccordinglyuponnew
paradigms,whichcanobviouslyintroduceadditionalcomputationaloverhead.
5.4 ChoosingSparsityLevelinTaskEmbedding
InSection4.3,weintroduceascalarhyper-parameter,thesparsitylevelγ,tocontrolthestrengthof
sparsityregularizationγ∥v∥ whendefiningtheembeddingofthedownstreamtask. Herewewill
1
presentaruleofthumbthatweuseforchoosingγ empiricallyinourexperiments.
Intuitively,thesparsityregularizationworksbypenalizingtheuseofanyfeatureandthereforeonly
featuresthatarecriticalenoughforthedownstreamtaskwillbeutilized. Asthesparsitylevelγ
increases, thesubsetsoffeaturespreservedwillalsobesmaller. Informally, todeterminetheset
of features necessary for the downstream task, one can keep increasing the sparsity level γ until
thedownstreamperformancestartstodrop. InFigure3(d),weincludedownstreamaccuracyofthe
baselinefeaturescorrespondingtovaryingsparsitylevelγ topresentaruleofthumbfordeciding
thevaluesofsparsitylevel: simplyusingthesmallestγ withatleast3%accuracydropfromthe
converged accuracy (i.e. the eventual accuracy when the sparsity level keeps decreasing). More
resultscanbefoundinFigure8inAppendix. Thissingleruleisappliedtoallexperimentsandit
workswellaspreviouslypresentedinSection5.2.
6 Conclusion
We define isolated model embedding, a family of model selection schemes where the update
complexity is O(1) and the selection consists of a single sweep over m vectors plus O(1) model
operations,bothwithrespecttothenumberofcandidatemodelsm. Isolatedmodelembeddingalso
9impliesseveralotherdesirablepropertiesforapplications. WepresentStandardizedEmbedder,an
empiricalrealizationofisolatedmodelembedding. Ourexperimentswith100visionmodelssupport
itseffectivenessandhighlightisolatedmodelembeddingasapromisingdirectiontowardsmodel
selectionthatisfundamentally(more)scalable. Whileourexperimentsfocusonvisualclassifications,
the concept of isolated model embedding is modality- and task-agnostic. Thus a natural future
directionwillbetoextendthisapproachtoencompassabroaderrangeofdatatypesandtasks.
7 Acknowledgement
Wenxiao Wang would like to thank Samyadeep Basu from University of Maryland for relevant
discussionspriortoWenxiao’sinternshipatSonyAI.
References
[1] Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji,
CharlessC.Fowlkes,StefanoSoatto,andPietroPerona. Task2vec: Taskembeddingformeta-
learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision
(ICCV),October2019.
[2] Andrea Agostinelli, Michal Pándy, Jasper R. R. Uijlings, Thomas Mensink, and Vittorio
Ferrari. Howstablearetransferabilitymetricsevaluations? InShaiAvidan,GabrielJ.Bros-
tow,MoustaphaCissé,GiovanniMariaFarinella,andTalHassner,editors,ComputerVision
- ECCV 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceed-
ings, Part XXXIV, volume 13694 of Lecture Notes in Computer Science, pages 303–321.
Springer,2022. doi: 10.1007/978-3-031-19830-4\_18. URLhttps://doi.org/10.1007/
978-3-031-19830-4_18.
[3] YajieBao,YangLi,Shao-LunHuang,LinZhang,LizhongZheng,AmirZamir,andLeonidas
Guibas. Aninformation-theoreticapproachtotransferabilityintasktransferlearning. In2019
IEEEinternationalconferenceonimageprocessing(ICIP),pages2309–2313.IEEE,2019.
[4] Daniel Bolya, Rohit Mittapalli, and Judy Hoffman. Scalable diverse model selec-
tion for accessible transfer learning. In Marc’Aurelio Ranzato, Alina Beygelzimer,
Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in
Neural Information Processing Systems 34: Annual Conference on Neural Informa-
tion Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages
19301–19312, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/
a1140a3d0df1c81e24ae954d935e8926-Abstract.html.
[5] HanCai,ChuangGan,andSongHan.Efficientvit:Enhancedlinearattentionforhigh-resolution
low-computationvisualrecognition. arXivpreprintarXiv:2205.14756,2022.
[6] MathildeCaron,HugoTouvron,IshanMisra,HervéJégou,JulienMairal,PiotrBojanowski,
andArmandJoulin. Emergingpropertiesinself-supervisedvisiontransformers. InProceedings
oftheIEEE/CVFinternationalconferenceoncomputervision,pages9650–9660,2021.
[7] Chengpeng Chen, Zichao Guo, Haien Zeng, Pengfei Xiong, and Jian Dong. Repghost: A
hardware-efficientghostmoduleviare-parameterization. arXivpreprintarXiv:2211.06088,
2022.
[8] TingChen,SimonKornblith,MohammadNorouzi,andGeoffreyHinton. Asimpleframework
for contrastive learning of visual representations. In International conference on machine
learning,pages1597–1607.PMLR,2020.
[9] Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong. When vision transformers outperform
resnetswithoutpre-trainingorstrongdataaugmentations. arXivpreprintarXiv:2106.01548,
2021.
[10] XinleiChen,ZhuangLiu,SainingXie,andKaimingHe. Deconstructingdenoisingdiffusion
modelsforself-supervisedlearning. arXivpreprintarXiv:2401.14404,2024.
[11] MehdiCherti,RomainBeaumont,RossWightman,MitchellWortsman,GabrielIlharco,Cade
Gordon,ChristophSchuhmann,LudwigSchmidt,andJeniaJitsev. Reproduciblescalinglaws
for contrastive language-image learning. In Proceedings of the IEEE/CVF Conference on
ComputerVisionandPatternRecognition,pages2818–2829,2023.
10[12] AdamCoates,AndrewNg,andHonglakLee. Ananalysisofsingle-layernetworksinunsuper-
visedfeaturelearning. InProceedingsofthefourteenthinternationalconferenceonartificial
intelligenceandstatistics,pages215–223.JMLRWorkshopandConferenceProceedings,2011.
[13] ZihangDai,HanxiaoLiu,QuocVLe,andMingxingTan. Coatnet: Marryingconvolutionand
attentionforalldatasizes. Advancesinneuralinformationprocessingsystems,34:3965–3977,
2021.
[14] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
JakobUszkoreit,andNeilHoulsby. Animageisworth16x16words: Transformersforimage
recognitionatscale. CoRR,abs/2010.11929,2020. URLhttps://arxiv.org/abs/2010.
11929.
[15] KaiHan,YunheWang,QiTian,JianyuanGuo,ChunjingXu,andChangXu. Ghostnet: More
featuresfromcheapoperations. InProceedingsoftheIEEE/CVFconferenceoncomputervision
andpatternrecognition,pages1580–1589,2020.
[16] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimage
recognition. CoRR,abs/1512.03385,2015. URLhttp://arxiv.org/abs/1512.03385.
[17] KaimingHe,XinleiChen,SainingXie,YanghaoLi,PiotrDollár,andRossGirshick. Masked
autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on
computervisionandpatternrecognition,pages16000–16009,2022.
[18] TongHe,ZhiZhang,HangZhang,ZhongyueZhang,JunyuanXie,andMuLi. Bagoftricks
forimageclassificationwithconvolutionalneuralnetworks. InProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition,pages558–567,2019.
[19] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan,
WeijunWang,YukunZhu,RuomingPang,VijayVasudevan,etal. Searchingformobilenetv3.
InProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,pages1314–1324,
2019.
[20] JieHu,LiShen,andGangSun. Squeeze-and-excitationnetworks. InProceedingsoftheIEEE
conferenceoncomputervisionandpatternrecognition,pages7132–7141,2018.
[21] Tianyu Hua, Wenxiao Wang, Zihui Xue, Sucheng Ren, Yue Wang, and Hang Zhao. On
featuredecorrelationinself-supervisedlearning. InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision(ICCV),pages9598–9608,October2021.
[22] GaoHuang,ZhuangLiu,LaurensVanDerMaaten,andKilianQWeinberger.Denselyconnected
convolutionalnetworks. InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,pages4700–4708,2017.
[23] Long-KaiHuang,JunzhouHuang,YuRong,QiangYang,andYingWei. Frustratinglyeasy
transferabilityestimation.InKamalikaChaudhuri,StefanieJegelka,LeSong,CsabaSzepesvári,
GangNiu,andSivanSabato,editors,InternationalConferenceonMachineLearning,ICML
2022,17-23July2022,Baltimore,Maryland,USA,volume162ofProceedingsofMachine
Learning Research, pages 9201–9225. PMLR, 2022. URL https://proceedings.mlr.
press/v162/huang22d.html.
[24] GeorgeKlirandBoYuan. Fuzzysetsandfuzzylogic,volume4. PrenticehallNewJersey,1995.
[25] AlexKrizhevsky,GeoffreyHinton,etal. Learningmultiplelayersoffeaturesfromtinyimages.
2009.
[26] AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton. Imagenetclassificationwithdeep
convolutional neuralnetworks. In F. Pereira, C.J. Burges, L. Bottou, andK.Q. Weinberger,
editors,AdvancesinNeuralInformationProcessingSystems,volume25.CurranAssociates,
Inc.,2012. URLhttps://proceedings.neurips.cc/paper_files/paper/2012/file/
c399862d3b9d6b76c8436e924a68c45b-Paper.pdf.
[27] XinranLiu, YikunBai, YuzheLu, AndreaSoltoggio, andSoheilKolouri. Wassersteintask
embeddingformeasuringtasksimilarities,2022.
[28] XinyuLiu,HouwenPeng,NingxinZheng,YuqingYang,HanHu,andYixuanYuan.Efficientvit:
Memory efficient vision transformer with cascaded group attention. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14420–14430,
2023.
11[29] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,ZhengZhang,StephenLin,andBaining
Guo. Swintransformer: Hierarchicalvisiontransformerusingshiftedwindows. InProceedings
oftheIEEE/CVFinternationalconferenceoncomputervision,pages10012–10022,2021.
[30] ZeLiu,HanHu,YutongLin,ZhuliangYao,ZhendaXie,YixuanWei,JiaNing,YueCao,Zheng
Zhang,LiDong,FuruWei,andBainingGuo. SwintransformerV2: scalingupcapacityand
resolution. InIEEE/CVFConferenceonComputerVisionandPatternRecognition,CVPR2022,
NewOrleans,LA,USA,June18-24,2022,pages11999–12009.IEEE,2022. doi: 10.1109/
CVPR52688.2022.01170. URLhttps://doi.org/10.1109/CVPR52688.2022.01170.
[31] ZhuangLiu,HanziMao,Chao-YuanWu,ChristophFeichtenhofer,TrevorDarrell,andSaining
Xie. Aconvnetforthe2020s. InProceedingsoftheIEEE/CVFconferenceoncomputervision
andpatternrecognition,pages11976–11986,2022.
[32] NingningMa,XiangyuZhang,Hai-TaoZheng,andJianSun.ShufflenetV2:practicalguidelines
forefficientCNNarchitecturedesign. InVittorioFerrari,MartialHebert,CristianSminchisescu,
andYairWeiss,editors,ComputerVision-ECCV2018-15thEuropeanConference,Munich,
Germany,September8-14,2018,Proceedings,PartXIV,volume11218ofLectureNotesin
ComputerScience,pages122–138.Springer,2018. doi: 10.1007/978-3-030-01264-9\_8. URL
https://doi.org/10.1007/978-3-030-01264-9_8.
[33] CuongNguyen,TalHassner,MatthiasSeeger,andCedricArchambeau. Leep: Anewmeasure
toevaluatetransferabilityoflearnedrepresentations. InInternationalConferenceonMachine
Learning,pages7294–7305.PMLR,2020.
[34] MichalPándy,AndreaAgostinelli,JasperR.R.Uijlings,VittorioFerrari,andThomasMensink.
Transferabilityestimationusingbhattacharyyaclassseparability. InIEEE/CVFConference
on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June
18-24, 2022, pages 9162–9172. IEEE, 2022. doi: 10.1109/CVPR52688.2022.00896. URL
https://doi.org/10.1109/CVPR52688.2022.00896.
[35] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InInternationalconferenceonmachinelearning,
pages8748–8763.PMLR,2021.
[36] IlijaRadosavovic,RajPrateekKosaraju,RossGirshick,KaimingHe,andPiotrDollár. Design-
ingnetworkdesignspaces. InProceedingsoftheIEEE/CVFconferenceoncomputervision
andpatternrecognition,pages10428–10436,2020.
[37] MarkSandler,AndrewHoward,MenglongZhu,AndreyZhmoginov,andLiang-ChiehChen.
Mobilenetv2: Invertedresidualsandlinearbottlenecks. InProceedingsoftheIEEEconference
oncomputervisionandpatternrecognition,pages4510–4520,2018.
[38] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit,
and Lucas Beyer. How to train your vit? data, augmentation, and regularization in vision
transformers. arXivpreprintarXiv:2106.10270,2021.
[39] ChristianSzegedy,WeiLiu,YangqingJia,PierreSermanet,ScottReed,DragomirAnguelov,
DumitruErhan,VincentVanhoucke,andAndrewRabinovich. Goingdeeperwithconvolutions.
InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages1–9,
2015.
[40] ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonShlens,andZbigniewWojna. Re-
thinkingtheinceptionarchitectureforcomputervision. InProceedingsoftheIEEEconference
oncomputervisionandpatternrecognition,pages2818–2826,2016.
[41] MingxingTanandQuocLe. Efficientnet: Rethinkingmodelscalingforconvolutionalneural
networks. InInternationalconferenceonmachinelearning,pages6105–6114.PMLR,2019.
[42] MingxingTanandQuocLe. Efficientnetv2:Smallermodelsandfastertraining. InInternational
conferenceonmachinelearning,pages10096–10106.PMLR,2021.
[43] MingxingTan,BoChen,RuomingPang,VijayVasudevan,MarkSandler,AndrewHoward,and
QuocV.Le. Mnasnet: Platform-awareneuralarchitecturesearchformobile. InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),June2019.
[44] Yehui Tang, Kai Han, Jianyuan Guo, Chang Xu, Chao Xu, and Yunhe Wang. Ghostnetv2:
enhancecheapoperationwithlong-rangeattention. AdvancesinNeuralInformationProcessing
Systems,35:9969–9982,2022.
12[45] ZhengzhongTu,HosseinTalebi,HanZhang,FengYang,PeymanMilanfar,AlanBovik,and
YinxiaoLi. Maxvit: Multi-axisvisiontransformer. InEuropeanconferenceoncomputervision,
pages459–479.Springer,2022.
[46] Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff Zhu, Oncel Tuzel, and Anurag Ranjan.
Mobileone: Animprovedonemillisecondmobilebackbone. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages7907–7917,2023.
[47] JingdongWang,KeSun,TianhengCheng,BoruiJiang,ChaoruiDeng,YangZhao,DongLiu,
YadongMu,MingkuiTan,XinggangWang,etal. Deephigh-resolutionrepresentationlearning
forvisualrecognition. IEEEtransactionsonpatternanalysisandmachineintelligence,43(10):
3349–3364,2020.
[48] Liwei Wang, Lunjia Hu, Jiayuan Gu, Zhiqiang Hu, Yue Wu, Kun He, and John Hopcroft.
Towardsunderstandinglearningrepresentations: Towhatextentdodifferentneuralnetworks
learnthesamerepresentation. Advancesinneuralinformationprocessingsystems,31,2018.
[49] Wenxiao Wang and Soheil Feizi. Temporal robustness against data poisoning. CoRR,
abs/2302.03684, 2023. doi: 10.48550/ARXIV.2302.03684. URL https://doi.org/10.
48550/arXiv.2302.03684.
[50] SainingXie,RossGirshick,PiotrDollár,ZhuowenTu,andKaimingHe. Aggregatedresidual
transformationsfordeepneuralnetworks. InProceedingsoftheIEEEconferenceoncomputer
visionandpatternrecognition,pages1492–1500,2017.
[51] KaichaoYou,YongLiu,MingshengLong,andJianminWang. Logme: Practicalassessment
of pre-trained models for transfer learning. CoRR, abs/2102.11005, 2021. URL https:
//arxiv.org/abs/2102.11005.
[52] WeihaoYu,PanZhou,ShuichengYan,andXinchaoWang. Inceptionnext: Wheninception
meetsconvnext. arXivpreprintarXiv:2303.16900,2023.
[53] LotfiAZadeh. Fuzzysets. Informationandcontrol,8(3):338–353,1965.
[54] SergeyZagoruykoandNikosKomodakis. Wideresidualnetworks. CoRR,abs/1605.07146,
2016. URLhttp://arxiv.org/abs/1605.07146.
[55] AmirRZamir,AlexanderSax,WilliamShen,LeonidasJGuibas,JitendraMalik,andSilvio
Savarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE
conferenceoncomputervisionandpatternrecognition,pages3712–3722,2018.
[56] WangchunshuZhou,CanwenXu,andJulianJ.McAuley. Efficientlytunedparametersaretask
embeddings. InYoavGoldberg,ZornitsaKozareva,andYueZhang,editors,Proceedingsof
the2022ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,EMNLP2022,
AbuDhabi,UnitedArabEmirates,December7-11,2022,pages5007–5014.Associationfor
Computational Linguistics, 2022. doi: 10.18653/v1/2022.emnlp-main.334. URL https:
//doi.org/10.18653/v1/2022.emnlp-main.334.
13A FullListofCandidateModelsUsedinTheExperiments
Table 3: A full list of the 100 pre-trained models that are used as the candidate models in the
experiments.
index name(usedbythecorrespondingsource) category source index name(usedbythecorrespondingsource) category source
1 ResNet18_Weights.IMAGENET1K_V1 ConvNet torchvision 51 ShuffleNet_V2_X1_0_Weights.IMAGENET1K_V1 ConvNet torchvision
2 EfficientNet_B0_Weights.IMAGENET1K_V1 ConvNet torchvision 52 ShuffleNet_V2_X1_5_Weights.IMAGENET1K_V1 ConvNet torchvision
3 GoogLeNet_Weights.IMAGENET1K_V1 ConvNet torchvision 53 ShuffleNet_V2_X2_0_Weights.IMAGENET1K_V1 ConvNet torchvision
4 Swin_T_Weights.IMAGENET1K_V1 Transformer torchvision 54 Swin_V2_T_Weights.IMAGENET1K_V1 Transformer torchvision
5 MobileNet_V3_Large_Weights.IMAGENET1K_V1 ConvNet torchvision 55 ViT_B_32_Weights.IMAGENET1K_V1 Transformer torchvision
6 MobileNet_V3_Large_Weights.IMAGENET1K_V2 ConvNet torchvision 56 ViT_B_16_Weights.IMAGENET1K_V1 Transformer torchvision
7 MobileNet_V3_Small_Weights.IMAGENET1K_V1 ConvNet torchvision 57 ViT_B_16_Weights.IMAGENET1K_SWAG_LINEAR_V1 Transformer torchvision
8 MNASNet0_5_Weights.IMAGENET1K_V1 ConvNet torchvision 58 Wide_ResNet50_2_Weights.IMAGENET1K_V1 ConvNet torchvision
9 ShuffleNet_V2_X0_5_Weights.IMAGENET1K_V1 ConvNet torchvision 59 Wide_ResNet50_2_Weights.IMAGENET1K_V2 ConvNet torchvision
10 AlexNet_Weights.IMAGENET1K_V1 ConvNet torchvision 60 mobileone_s0 ConvNet timm
11 ConvNeXt_Tiny_Weights.IMAGENET1K_V1 ConvNet torchvision 61 mobileone_s1 ConvNet timm
12 ConvNeXt_Small_Weights.IMAGENET1K_V1 ConvNet torchvision 62 mobileone_s2 ConvNet timm
13 DenseNet121_Weights.IMAGENET1K_V1 ConvNet torchvision 63 mobileone_s3 ConvNet timm
14 DenseNet161_Weights.IMAGENET1K_V1 ConvNet torchvision 64 mobileone_s4 ConvNet timm
15 DenseNet169_Weights.IMAGENET1K_V1 ConvNet torchvision 65 inception_next_tiny.sail_in1k ConvNet timm
16 DenseNet201_Weights.IMAGENET1K_V1 ConvNet torchvision 66 inception_next_small.sail_in1k ConvNet timm
17 EfficientNet_B1_Weights.IMAGENET1K_V1 ConvNet torchvision 67 inception_next_base.sail_in1k ConvNet timm
18 EfficientNet_B2_Weights.IMAGENET1K_V1 ConvNet torchvision 68 ghostnet_100.in1k ConvNet timm
19 EfficientNet_B3_Weights.IMAGENET1K_V1 ConvNet torchvision 69 ghostnetv2_100.in1k ConvNet timm
20 EfficientNet_B4_Weights.IMAGENET1K_V1 ConvNet torchvision 70 ghostnetv2_130.in1k ConvNet timm
21 EfficientNet_V2_S_Weights.IMAGENET1K_V1 ConvNet torchvision 71 ghostnetv2_160.in1k ConvNet timm
22 Inception_V3_Weights.IMAGENET1K_V1 ConvNet torchvision 72 repghostnet_050.in1k ConvNet timm
23 MNASNet0_75_Weights.IMAGENET1K_V1 ConvNet torchvision 73 repghostnet_058.in1k ConvNet timm
24 MNASNet1_0_Weights.IMAGENET1K_V1 ConvNet torchvision 74 repghostnet_080.in1k ConvNet timm
25 MNASNet1_3_Weights.IMAGENET1K_V1 ConvNet torchvision 75 repghostnet_100.in1k ConvNet timm
26 MobileNet_V2_Weights.IMAGENET1K_V1 ConvNet torchvision 76 efficientvit_b0.r224_in1k Transformer timm
27 MobileNet_V2_Weights.IMAGENET1K_V2 ConvNet torchvision 77 efficientvit_b1.r224_in1k Transformer timm
28 RegNet_X_1_6GF_Weights.IMAGENET1K_V1 ConvNet torchvision 78 efficientvit_b2.r224_in1k Transformer timm
29 RegNet_X_1_6GF_Weights.IMAGENET1K_V2 ConvNet torchvision 79 efficientvit_b3.r224_in1k Transformer timm
30 RegNet_X_3_2GF_Weights.IMAGENET1K_V1 ConvNet torchvision 80 efficientvit_m0.r224_in1k Transformer timm
31 RegNet_X_3_2GF_Weights.IMAGENET1K_V2 ConvNet torchvision 81 efficientvit_m1.r224_in1k Transformer timm
32 RegNet_X_400MF_Weights.IMAGENET1K_V1 ConvNet torchvision 82 efficientvit_m2.r224_in1k Transformer timm
33 RegNet_X_400MF_Weights.IMAGENET1K_V2 ConvNet torchvision 83 efficientvit_m3.r224_in1k Transformer timm
34 RegNet_X_800MF_Weights.IMAGENET1K_V1 ConvNet torchvision 84 efficientvit_m4.r224_in1k Transformer timm
35 RegNet_X_800MF_Weights.IMAGENET1K_V2 ConvNet torchvision 85 efficientvit_m5.r224_in1k Transformer timm
36 RegNet_Y_1_6GF_Weights.IMAGENET1K_V1 ConvNet torchvision 86 coatnet_nano_rw_224.sw_in1k Hybrid(Conv+Attention) timm
37 RegNet_Y_1_6GF_Weights.IMAGENET1K_V2 ConvNet torchvision 87 coatnext_nano_rw_224.sw_in1k Hybrid(Conv+Attention) timm
38 RegNet_Y_3_2GF_Weights.IMAGENET1K_V1 ConvNet torchvision 88 seresnext101_32x4d.gluon_in1k ConvNet timm
39 RegNet_Y_3_2GF_Weights.IMAGENET1K_V2 ConvNet torchvision 89 vit_tiny_r_s16_p8_224.augreg_in21k Transformer timm
40 RegNet_Y_400MF_Weights.IMAGENET1K_V1 ConvNet torchvision 90 vit_small_r26_s32_224.augreg_in21k Transformer timm
41 RegNet_Y_400MF_Weights.IMAGENET1K_V2 ConvNet torchvision 91 vit_tiny_r_s16_p8_224.augreg_in21k_ft_in1k Transformer timm
42 RegNet_Y_800MF_Weights.IMAGENET1K_V1 ConvNet torchvision 92 vit_small_r26_s32_224.augreg_in21k_ft_in1k Transformer timm
43 RegNet_Y_800MF_Weights.IMAGENET1K_V2 ConvNet torchvision 93 hrnet_w18_small.gluon_in1k ConvNet timm
44 ResNeXt50_32X4D_Weights.IMAGENET1K_V1 ConvNet torchvision 94 hrnet_w18_small_v2.gluon_in1k ConvNet timm
45 ResNeXt50_32X4D_Weights.IMAGENET1K_V2 ConvNet torchvision 95 vit_small_patch16_224.dino Transformer timm
46 ResNet101_Weights.IMAGENET1K_V1 ConvNet torchvision 96 vit_base_patch16_224.mae Transformer timm
47 ResNet101_Weights.IMAGENET1K_V2 ConvNet torchvision 97 maxvit_tiny_tf_224.in1k Hybrid(Conv+Attention) timm
48 ResNet50_Weights.IMAGENET1K_V1 ConvNet torchvision 98 maxvit_tiny_rw_224.sw_in1k Hybrid(Conv+Attention) timm
49 ResNet50_Weights.IMAGENET1K_V2 ConvNet torchvision 99 vit_base_patch32_224.sam_in1k Transformer timm
50 ResNet34_Weights.IMAGENET1K_V1 ConvNet torchvision 100 vit_base_patch32_clip_224.openai_ft_in1k Transformer timm
InTable3,weincludethefulllistofpre-trainedmodelsthatareusedinourevaluations. Weinclude
asfollowsrelevantreferencesandcorrespondingmodelindicesinTable3(notethatsomepre-trained
modelscorrespondtomultiplereferences):
• ResNet[16]: 1,46,47,48,49,50,88;
• EfficientNet/EfficientNetV2[41,42]: 2,17,18,19,20,21;
• GoogLeNet[39]: 3;
• SwinTransformer/SwinTransformerV2[29,30]: 4,54;
• MobileNetV2/V3[37,19]: 5,6,7,26,27;
• MNASNet[43]: 8,23,24,25;
• ShuffleNetV2[32]: 9,51,52,53;
• AlexNet[26]: 10;
• ConvNeXt[31]: 11,12,87;
• DenseNet[22]: 13,14,15,16;
• InceptionV3[40]: 22;
• RegNet[36]: 28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43;
• ResNeXt[50]: 44,45,88;
• VisionTransformer[14]: 55,56,57,89,90,91,92,95,96,99,100;
• WideResNet[54]: 58,59;
• MobileOne[46]: 60,61,62,63,64;
• InceptionNeXt[52]: 65,66,67;
• GhostNet/GhostNetV2[15,44]: 68,69,70,71;
14• RepGhostNet[7]: 72,73,74,75;
• EfficientViT(MIT)[5]: 76,77,78,79;
• EfficientViT(MSRA)[28]: 80,81,82,83,84,85;
• CoAtNet[13]: 86,87;
• Squeeze-and-Excitation[20]: 88;
• Bag-of-Tricks[18]: 88;
• AugReg[38]: 89,90,91,92;
• HRNet[47]: 93,94;
• DINO[6]: 95;
• MaskedAutoencoder[17]: 96;
• MaxViT[45]: 97,98;
• Sharpness-awareminimizerforViT[9]: 99;
• Reproduciblescalinglaws[11]: 100;
• CLIP[35]: 100.
15B FiguresofEmpiricalEvaluations
(a) downstream:CIFAR-10 (b) downstream:CIFAR-100 (c) downstream:STL-10
baselinefeatures:ResNet-18 baselinefeatures:ResNet-18 baselinefeatures:ResNet-18
(d) downstream:CIFAR-10 (e) downstream:CIFAR-100 (f) downstream:STL-10
baselinefeatures:Swin-T(tiny) baselinefeatures:Swin-T(tiny) baselinefeatures:Swin-T(tiny)
Figure4: Downstreamaccuracy(i.e. thegroundtruth)v.s. thecardinalityofstandardintersections
(i.e. the selection metric) when using 4k steps per candidate. The downstream accuracy of the
baselinefeaturesarehighlightedwiththedashedline. Whenapublicmodelisonlysuboptimal,using
itasbaselinefeaturesforStandardizedEmbeddercanstilllocatemorecompetitivemodels.
16(a) downstream:CIFAR-10 (b) downstream:CIFAR-100 (c) downstream:STL-10
baselinefeatures:ResNet-18 baselinefeatures:ResNet-18 baselinefeatures:ResNet-18
(d) downstream:CIFAR-10 (e) downstream:CIFAR-100 (f) downstream:STL-10
baselinefeatures:Swin-T(tiny) baselinefeatures:Swin-T(tiny) baselinefeatures:Swin-T(tiny)
Figure5: Downstreamaccuracy(i.e. thegroundtruth)v.s. thecardinalityofstandardintersections
(i.e. the selection metric) when using 10k steps per candidate. The downstream accuracy of the
baselinefeaturesarehighlightedwiththedashedline. Whenapublicmodelisonlysuboptimal,using
itasbaselinefeaturesforStandardizedEmbeddercanstilllocatemorecompetitivemodels.
17(a) downstream:CIFAR-10 (b) downstream:CIFAR-100 (c) downstream:STL-10
Figure6: Comparingthecardinalityofstandardintersections(i.e. theselectionmetric)whenus-
ing different baseline features (ResNet-18 and Swin-T (tiny)) with 4k steps per candidate. The
green/orangepointsinthebottomrightsuggestusingResNet-18asbaselinefeaturestendtooveresti-
mate(some)modelswithattentionscomparedtousingSwinTransformer(tiny).
(a) downstream:CIFAR-10 (b) downstream:CIFAR-100 (c) downstream:STL-10
Figure7: Comparingthecardinalityofstandardintersections(i.e. theselectionmetric)whenus-
ing different baseline features (ResNet-18 and Swin-T (tiny)) with 10k steps per candidate. The
green/orangepointsinthebottomrightsuggestusingResNet-18asbaselinefeaturestendtooveresti-
mate(some)modelswithattentionscomparedtousingSwinTransformer(tiny).
18(a) downstream:CIFAR-10 (b) downstream:CIFAR-100 (c) downstream:STL-10
baselinefeatures:ResNet-18 baselinefeatures:ResNet-18 baselinefeatures:ResNet-18
(d) downstream:CIFAR-10 (e) downstream:CIFAR-100 (f) downstream:STL-10
baselinefeatures:Swin-T(tiny) baselinefeatures:Swin-T(tiny) baselinefeatures:Swin-T(tiny)
Figure8: Downstreamaccuracyofthebaselinefeaturescorrespondingtovaryinglevelofsparsity
regularizationγ. Aruleofthumbfordecidingthevalueofγ: usingthesmallestγ withatleast3%
accuracydropfromtheconvergedaccuracy.
C IllustrativeExampleforSection4.2
HereisanexampletoillustratehowtoassociateeverysubsetofthebaselinefeaturesetB witha
binaryvectorfrom{0,1}N. AssumingnowthebaselinefeaturesetB :X →RN containsatotalof
N =4features,b ,b ,b ,b ,whereeachofthemisafunctionfromX toR,thentherewillbeatotal
0 1 2 3
of24 =16differentsubsetsofB. Wecanassociateeachsubsetwithadistinct,4-dimensionalbinary
vector(i.e. avectorin{0,1}4)byusing1toindicatethepresenceofafeatureand0toindicatean
absenceofafeatureinthesubset. Specifically,(0,0,0,0)willdenotetheemptysubset,(0,0,1,0)
willdenote{b }and(1,0,1,1)willdenote{b ,b ,b }.
2 0 2 3
D BroaderImpacts
Whenselectingmodelslocally,asymptoticallyfastupdateandselectionindicateconsiderablyim-
provedscalability,whichisobviouslypositiveasonecanpotentiallyutilizemorecandidatemodels.
Meanwhile,thedecentralizabilityofisolatedmodelembeddingnaturallyrelatestoapotentialappli-
cation: adecentralizedmodelmarketwithadecentralizedmodelselectionsystem.
Thereare,ofcourse,positiveimpactsfromhavingsuchadecentralizedmodelmarket. Forindividual
downstreamusers,theircapabilitiescanbeextendedbyeffectivelyaccessingmorecandidatemodels,
andtheircostsformodelselectioncanbepotentiallyreducedthroughsystematicandout-of-the-box
selectionoperations. Formodelowners,theirprofitscanbeenlargedbypromotingtheirmodelsto
morepotentialusers.
However,whilethebenefitsofsuchadecentralizedmarketstemfromtheaccelerateddistribution
ofmodels,thedistributionofmaliciousbehaviors,suchasbackdoors,mayalsobefacilitatedinthe
presenceofhostileparties,whichispotentiallyanegativeimpactandanewresearchdirection.
19