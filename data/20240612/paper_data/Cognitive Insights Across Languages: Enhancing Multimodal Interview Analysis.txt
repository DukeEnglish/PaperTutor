Cognitive Insights Across Languages: Enhancing Multimodal Interview
Analysis
DavidOrtiz-Perez1,JoseGarcia-Rodriguez1,DavidToma´s2
1 DepartmentofComputerTechnology,UniversityofAlicante,Alicante,Spain
2 DepartmentofSoftwareandComputingSystems,UniversityofAlicante,Alicante,Spain
dortiz@dtic.ua.es, jgarcia@dtic.ua.es, dtomas@dlsi.ua.es
Abstract includesaudioconversationswhereelderlyindividualsdescribe
asetofimagesinbothEnglishandChineselanguage. Wealso
Cognitivedeclineisanaturalprocessthatoccursasindividuals
considertheactualimagedescriptionsinouranalysis.
age. Earlydiagnosisofanomalousdeclineiscrucialforiniti-
Insummary,wemakethefollowingcontributions:
atingprofessionaltreatmentthatcanenhancethequalityoflife
• Weproposeamultimodalarchitecturecapableofdifferentiat- of those affected. To address this issue, we propose a multi-
ingbetweenMildCognitiveImpairment(MCI)andanormal
modalmodelcapableofpredictingMildCognitiveImpairment
cognitivedeclineduetoaging. Themodeltakesactualcon-
andcognitivescores.TheTAUKADIALdatasetisusedtocon-
versations between a clinician and the elderly as input, ex-
ducttheevaluation,whichcomprisesaudiorecordingsofclini-
tractingandpost-processingtranscription,andobtainingtex-
calinterviews. Theproposedmodeldemonstratestheabilityto
tualandacousticinformationfromtheseinteractions.
transcribe and differentiate between languages used in the in-
terviews. Subsequently,themodelextractsaudioandtextfea- • Our research involves a comprehensive exploration of var-
tures,combiningthemintoamultimodalarchitecturetoachieve ious modalities approaches, and fusion strategies to effec-
robustandgeneralizedresults. Ourapproachinvolvesin-depth tivelycombinethesemodalitiesforoptimalperformancein
researchtoimplementvariousfeaturesobtainedfromthepro- thistask.
posedmodalities. • Furthermore, we emphasize the robustness and generaliza-
Index Terms: Multimodal, Cognitive Estimation, Audio Pro- tionofourmodelachievedthroughcombinationsofdiverse
cessing,NaturalLanguageProcessing modalities. Additionally, we highlight the ease of deploy-
ment,asthemodelsolelyreliesonaudioconversations.
1. Introduction Theremainderofthepaperisstructuredasfollows.InSec-
tion2,wediscusstherelatedworksanddatasetsrelevanttothis
Cognitive abilities tend to decline over time. While some
task. Section3presentstheemployeddatasetandabriefanal-
abilities tend to remain unaffected, others, such as processing ysis.Section4elaboratesonthevariousapproachesemployed.
speed, reasoning, and memory, often show signs of deteriora-
Section5introducestheexperimentationandpresentstheob-
tion[1].Althoughcognitivedeclineisacommonphenomenon,
tainedresults. Finally,inSection6,wediscusstheconclusions
someindividualsmayexperienceamorepronounceddecrease
drawnfromthiswork.
inthesefunctions,particularlythoseaffectedbydiseasessuch
asdementia[2]. However, itisimportanttonotethatnotev-
2. RelatedWorks
ery substantial decline in cognitive function indicates demen-
tia. SomeindividualsmayexhibitMildCognitiveImpairment
Thissectionpresentsinformationondatasetsusedforpredict-
(MCI), which represents a stage between normal age-related
ing dementia and other cognitive impairments. It also covers
cognitive deterioration and dementia [3, 4]. Therefore, there
researchcarriedoutonthesedatasets,withaspecificfocuson
arevariousstagesandconsiderationswithinthisspectrum.The
multimodality.
decline in cognitive abilities due to aging poses a significant
challenge in our society, as there is a trend towards an aging
2.1. Datasets
population[5].
The primary motivation behind our work focuses on the Duetothesensitivityofthistypeofdata,acquiringlargevol-
earlydetectionofthesecognitiveimpairments. Earlydetection umesposesasignificantchallenge,constitutingacrucialaspect
iscrucialasitfacilitatespromptinterventionbyprofessionals fortrainingdeeplearningarchitectures.
inthefield. Suchinterventionscangreatlybenefitpatientsby Whenpredictingdementiaorotherformsofcognitivede-
enhancingtheirqualityoflifeandmitigatingthedeclineover cline, some datasets focus on medical data, such as the OA-
time,consequentlystimulatingpositivechangesintheircogni- SIS[8]andADNI[9]datasets, whichincorporatemedicalin-
tivebehavior[6]. formationandMagneticResonanceImaging(MRI)scansofthe
Giventheaforementionedfacts,weproposeamultimodal subject’sbrains.Datasetsthatprovideinformationinvideo,au-
andmultilingualmodeldesignedfortheearlydetectionofcog- dio,ortextformatsarerelevanttoourspecifictaskandresearch.
nitiveimpairment.Themodelaimstoaccuratelytranscribeand This emphasis relies on the practicality in real-life scenarios,
distinguish between various languages in audio conversations whereinformationcanbecapturedeasilyusingastandardcam-
topredictthecognitivestateofelderlysubjects. Itdetermines eraforvideorecording.
whethertheyexhibitsignsofMCIorpossessanormalcogni- Among these datasets are DementiaBank [10], and the
tivestatefortheirage,usingbothtexttranscriptionsandaudio ADReSS challenge [11]. These collections share similarities,
data. WeexperimentovertheTAUKADIALdataset[7],which featuring audio and text transcriptions of interviews between
4202
nuJ
11
]GL.sc[
1v24570.6042:viXracliniciansandsubjects. eachlanguagevary.Furthermore,translatingfromonelanguage
toanothermaynotbeanoptimalchoice,giventheimportance
2.2. Works of detecting subtle grammatical errors and nuances in expres-
sion.
Recently,proposalsfortheautomaticdetectionofcognitiveim-
Concerning image descriptions, our research involved an-
pairments have made a breakthrough due to advancements in
alyzing transcribed samples and following protocols outlined
deeplearning.Studieshaveexploredpatients’abilitiestoartic-
bytheorganization,leadingtotheacquisitionofauthenticde-
ulateandconveytheirthoughts,particularlyinrelationtocog-
scriptionscorrespondingtotheimages. Simultaneously,audio
nitiveimpairments.
filesunderwentpreprocessingtoextractfeatures,suchasaudio
Significantcontributionsinthisfieldincludetheworksof
biomarkers,encompassingelementslikeaudiointensity,jitter,
Yuanetal. [12]andBalagopalanetal. [13]. Theyusedpre-
andshimmer,amongothers.Toaccomplishthis,theOpenDBM
trainedtransformer-basedmodels,suchasBERT[14],overthe
1librarywasemployed.ThefeaturesobtainedfromtheopenS-
ADReSSchallengetoanalyzetextfeatures.
MILE toolkit [26] were also extracted and tested but with no
Audioanalysisisanotherrelevantfactorfordiseasedetec-
promisingresults.
tion. Studies by Hajjar et al. [15] and Laguarta et al. [16]
haveleveragedaudiobiomarkerstopredictcognitivediseases,
4.2. Text-basedmodels
such as dementia. Integration of these modalities in multi-
modal approaches can be achieved through various strategies, Thepreprocessingofthedatainvolvesprovidingamongother
asdemonstratedinourpreviousworks[17]andthoseproposed features,texttranscriptions. Thesetranscriptionsrepresentthe
bySarawgietal[18]. Acurrenthottopicisthefusionstrate- actualdialoguebetweenthesubjectsandtheclinician. During
giesofdataformodelsthataccommodatediversedatatypes,as this interaction, the subject describes the image presented by
exemplifiedbyrecentworkssuchasCLIP[19]orLLaVA[20]. theclinician,providinguswiththesubject’sdescriptionofthe
image. Additionally,agenuinedescriptionoftheimageisalso
3. Dataset acquired.Withthesetwosetsofdata,weproposethefollowing
architectures,whicharetestedlater:
TheTAUKADIALdataset[7]consistsofrecordingsofelderly
Text. Theinitialproposedmodelisthetextmodel,which
individuals prompted by clinicians to describe images. These
solely utilizes the descriptions provided by the subjects. Two
recordings,whichareprovidedinaudiofiles,involvebothChi-
separatefrozenBERTmodels[14]areusedtoextractfeatures
nese and English speakers, with each participant asked to ar-
based on the language information and description obtained
ticulate descriptions for three distinct images. Notably, the
during the preprocessing step. The use of two distinct BERT
image sets differ between English and Chinese participants.
modelsisnecessaryasthereisoneforeachlanguage. Inline
For English speakers, the assigned images are “The Cookie
with the Transformer architecture [27], a transformer encoder
Theft”[21],“CatRescue”[22],and“ComingandGoing”[23].
hasbeenused.Thisencoderistrainedtoextractadditionalfea-
Meanwhile, Chinese speakers describe a father caring for his
tures and consists of two encoder layers, each with the same
baby,scenesfromatraditionalnightmarketinTaiwan,andthe
size as the BERT base model (768 dimensions and 8 heads).
dailyactivitiesinTaiwan’sparks.
The embeddings obtained from the transformer encoder serve
Thedatasetisannotatedtoindicatewhetherthesubjecthas as an input for a final Multilayer Perceptron (MLP) used for
MCI or normal cognitive decline for their age. Furthermore, prediction. The transformer layers, as well as the MLPs, are
foreachparticipant,ascoreintheformofaMini-MentalSta- differentforeachlanguageandalsovaryfortheclassification
tus Examination (MMSE) is provided. The MMSE is a com- andregressiontasks.
monlyusedclinicaltoolforassessingcognitiveimpairmentin
Similarity. The similarity model, as the text model, en-
patients[24].Itcomprisesaseriesofelevenquestions.
tailsobtainingBERTembeddingsfromthesubject’simagede-
The training set comprises 222 samples from individuals
scriptions. However,inthismodel,BERTembeddingsarealso
withMCIand165controlsamples,including74MCIsubjects
acquiredfortherealimagedescriptions. Bothsetsofembed-
and55controlparticipants.Foreachparticipant,therearethree
dings,derivedfromfrozenmodelstoensureconsistencyduring
samples corresponding to the described pictures. The dataset
training, undergo cosine similarity analysis [19] to assess the
comprises201Chineseand186Englishsamples. Thepartici-
similaritybetweenthesubject’sdescriptionandtherealimage.
pantsareagedbetween61and87,withanaverageof72years.
Theresultingcosinesimilaritymatrixissubsequentlyfedinto
Amongtheparticipants,79arefemaleand50aremale.
anMLPforprediction.
Combination. This model closely resembles the text
4. Methodology
model,withthedistinctionlyingintheembeddingsutilizedto
feedthetransformerencoders. Theseembeddingsresultfrom
In this section, we introduce the different proposed architec-
combining the subject’s descriptions with the actual descrip-
tures.
tions.Throughthisamalgamation,achievedviasummation,we
ensurethatthemodeldulyconsiderstheimagebeingdescribed.
4.1. Datapreprocessing
Combinedsimilarity.Thisensemblecloselyresemblesthe
Thisstepservesastheinitialphaseinourwork.Asthisdataset previousone.However,inthisinstance,thecombinationofem-
comprisesoriginaldatainaudiofiles,andourinterestalsolies beddingsoccursbetweenthesubject’sdescriptionandthesim-
inextractingthesemanticmeaningofinterviews,apreprocess- ilarity to the genuine description. Through this combination,
ingstephasbeenconductedtotransformaudiointotext. themodeltakesintoaccountthedifferencesbetweenthepro-
To this aim, we utilized the Whisper model [25] to tran- videddescriptionandtheactualone,alongwiththeremainder
scribeallinterviewsanddistinguishbetweenlanguages,specif- descriptions.
icallyEnglishandChinese. Thedistinctionbetweenlanguages
iscrucialinthisprocess,astheimagedescriptionsprovidedfor 1https://github.com/AiCure/open_dbmWithallthesecombinations,ouraimistoensurethecapture
σ+ρ
ofthemostrelevantinformationfromthetextualmodality. UAR= (1)
2
4.3. Audio-basedmodel Whereσrepresentsspecificityandρrepresentssensitivity.
F1metricisalsoproposedfortheclassificationtask,repre-
Inthecontextofaudiomodality,thefeaturesextractedduring sentedinEquation2,whereπstandsfortheprecision.
the preprocessing step have been employed. This involves a
straightforwardmodel,wheretheobtainedfeaturesarefedinto 2πρ TP
F = , where π= (2)
theMLP. 1 π+ρ TP +FP
Astheaudiobiomarkersdonotsignificantlydependonthe For the regression tasks, the primary metric is the Root
languageused,nodifferentiationhasbeenappliedamonglan- MeanSquaredError(RMSE),asshowninEquation3.
guagesforthismodel.
(cid:115)
(cid:80)N (yˆ −y )2
4.4. Multimodalmodel RMSE = i=1 i i (3)
N
In the case of the multimodal model, we combined the most
The coefficient of determination (R2) metric is also pro-
effective text model with the audio model. We achieved this
posedfortheregressiontask,representedinEquation4.
byusingfeaturesextractedfromtheintermediatelayersoftheir
respective MLPs and combining them through concatenation. (cid:80)N (yˆ −y )2
Thisallowsustointegrategrammaticalandsemanticfeatures R2 =1− i=1 i i (4)
fromtext-basedmodelswiththosethatmaybelostintranscrip- (cid:80)N i=1(yˆ i−y¯)2
tions.
5.2. Ablationstudy
Figure1illustratestheprimaryarchitectureofthispipeline,
where the final MLP of the multimodal model combines fea- Toevaluatetheperformanceofdifferentmodels,weconducted
turesfromboththeaudioandtextmodels. anablationstudyusingthecross-validationstrategy.Weimple-
mented the proposed metrics and calculated them for each of
5. Experiments the5foldswitheachmodeltocomparetheireffectiveness.
The results for the classification task are presented in Ta-
This section introduces the experimental configurations, met-
ble 1, where values represent the mean of the 5-fold metric
rics,andresultsobtainedfromthepreviouslyproposedmodels.
alongwiththestandarddeviation.Wealsocalculatedthemean
andstandarddeviationofallmetrics.
5.1. Experimentalsetup Analyzingtheobtainedresultsfromtheclassificationtask,
itisevidentthatthemultimodalmodel,whichamalgamatesfea-
Thissubsectionintroducestheexperimentalconfigurationand
turesfromallothers,achievesthebestoutcome,particularlyex-
setupusedthroughoutthiswork.
cellinginthecrucialmetricofUAR.Furthermore, thismodel
Cross-validation. As there is no predefined validation
also obtained the second-best result in the remaining metrics,
split, and to ensure the selection of the best and most general
underscoring its capability and robustness against other mod-
model, a cross-validation strategy has been employed with a
els.
foldsizeof5. Thisentailssplittingthedatasetinto5different
It is noteworthy that audio features do not offer as much
folds,enablingthemodeltobeevaluatedonvarioussubsetsto
informationastext-basedapproaches,surpassingonlythesimi-
ensurerobustperformance,especiallywithnon-trainabledata.
laritymodel. However, theadditionofthissimilaritytoother
Implementation details. We employed the AdamW op-
timizerwith alearningrate of10−5 formodels basedontext features has shown improvement, as evidenced by combined
models. Notably,thesimilaritymodelalonedoesnotperform
featuresandthemultimodalone,and0.01fortheaudio-based
remarkablywell.
oneswhileincorporatingaweightdecayof0.01forallofthem.
Additionally, anablationstudyontheregressiontaskwas
Fortheregressiontask,thelearningratehasbeenincreasedten
conducted. Examiningtheseresults,wecanconcludethatthe
times.
multimodal model also outperforms the others. In this case,
Themodelsunderwenttrainingwithabatchsizeof16for
themostimportantmetricistheRMSE,wherethemultimodal
100 epochs, employing an early stopping strategy with a pa-
modelperformsbest. Similarly,forthecoefficientofdetermi-
tienceof10epochs. Thecross-entropylossfunctionwasused
nation,boththemultimodalandsimilaritymodelsprovidethe
fortheclassificationtask,andthemeansquarederrorwasused
bestresults.Forthistask,themostrelevantinformationregard-
fortheregressiontask.
ing the text differs from the other task, with the similarity to
TheexperimentswereconductedonanNVIDIAGeForce
the original description being the most important part. In the
RTX4090.Allcodedevelopedforthisprojectispubliclyavail-
caseoftheaudiomodality,itslightlyimprovestheperformance
ableinourGitHubrepository2.
comparedtotheothertext-basedmodels.
Evaluationmetrics.Toassesstheperformanceofourpro-
posed models, we used the suggested metrics within the con-
5.3. Challengeresults
textoftheTAUKADIALchallenge. Giventhatthischallenge
encompasses two distinct tasks, it employs different metrics AspartoftheTAUKADIALchallenge,adesignatedtestsethas
for each. Regarding the classification task consisting of dif- been provided to evaluate the performance on entirely unseen
ferentiatingbetweenMCIandcontrolgroups,themainmetric data. Our submission to this challenge included four distinct
hasbeentheUnweightedAverageRecall(UAR),expressedin models,andtheirrespectiveperformancesaredetailedinTable
Equation1. 2. Themultimodalmodelwasidentifiedasthetop-performing
modelbasedontheablationstudyandservesasthefoundation
2https://github.com/davidorp/taukadial forourproposedmodels.CLS 101
I 1045
Final prediction
WENT 2253 Bert model
HOME 2188
Whisper SEP 102 Bert embeddings Multimodal
from subjects model
CLS 101
Language
Audio transcription & 我 2564
identification 回家 2168 B Cer ht inm eo sd eel Similarity
了 2784
SEP 102 Bert embeddings
from real
descriptions of
images
Audio
Feature Extractor Audio features
model
Figure1:Overviewofthemultimodalarchitecture.
Table1:ResultsofClassificationTaskusing5-FoldCross-Validation.
Model UAR F1 σ ρ π RMSE R2
Text 73.07±1.5 76.92±3.6 68.14±8.4 78.0±6.1 76.72±7.1 3.14±0.3 0.09±0.0
Audio 70.75±4.0 76.22±5.9 61.41±6.0 80.1±9.2 73.32±6.5 3.11±0.2 0.1±0.1
Similarity 68.57±3.3 72.52±5.1 63.67±9.8 73.47±11.8 73.44±6.0 2.95±0.4 0.2±0.1
Combination 74.58±2.2 78.47±5.5 70.44±12.6 78.72±11.4 79.44±4.3 3.14±0.3 0.09±0.0
Combined
74.82±2.5 79.43±1.4 67.98±7.3 81.67±4.3 77.65±3.6 3.14±0.3 0.08±0.0
similarity
Multimodal 75.09±1.8 78.49±3.4 70.0±12.7 80.17±10.6 78.95±8.5 2.93±0.3 0.2±0.1
To improve the robustness of our approach, we used the forms the results obtained in the validation set for the regres-
multimodalmodelfromthebestfoldofcross-validationasone sion task. All models demonstrate similar performance, with
model. Additionally,wefine-tunedthissamemodelwithdata themultimodalmodelperformingexceptionallywellinclassi-
fromtheworstfold,resultingintheMultimodalFullmodelpre- ficationandtheMeanMultimodalFullmodelstandingoutinthe
sentedinthetable. Thisprocessaimstoensurethatourmodel regressiontask.
isexposedtotheentiretrainingdatasetwhilemaintainingaval- Compared to State-Of-The-Art models, our approach
idationset. achievessuperiorresultsintheregressiontask,surpassingoth-
Inaddition,wehavepresentedtwoextramodelstocalcu- erswitha2.50RMSEscore.However,intheclassificationtask,
latethemeanpredictionsforeachsubject,consideringthateach weattainthesecond-bestperformancewitha56.18UAR.Itis
participanthasthreesamples. Thisstrategyensuresconsistent important to note that our model demonstrates greater robust-
predictionsforagivenparticipant.Themodelsmarkedwithan ness and stability. It is worth mentioning that the model that
asteriskinthetablerepresentthisapproach. outperforms ours in classification tasks exhibits significantly
poorerresultsinregressiontaskscomparedtoourmodel.
Table2:Resultsfromthechallenge’stestset
6. Conclusions
Model UAR RMSE
Multimodal(ours) 56.18 2.59 Inthiswork,wehavepresentedamultimodalmodelcapableof
MultimodalFull(ours) 53.68 2.64 predictingMCIornormalcognitivedeclineduetoaging,along
Multimodal*(ours) 55.39 2.58 with a cognitive score based on MMSE. The early detection
MultimodalFull*(ours) 52.01 2.50 of these declines is crucial for the well-being of patients who
w2v+eGeMAPs[7] 59.18 13.28 experiencethem.WehaveemployedtheTAUKADIALdataset,
ling.[7] 53.59 2.89 whichconsistsofaudiorecordingsfrominterviewsbetweena
w2v+ling[7] 50 4.39 clinicianandasubject.
eGeMAPs[7] 44.95 17 The research conducted throughout this work focuses on
w2v[7] 46.05 4.48 the combination of features derived from both text transcrip-
tionsandaudiomodalities.Consequently,wehavedevelopeda
strongandeasilydeployablemodelbycombiningmodalities.
Examining the results obtained in the challenge. The In future works, we aspire to persist in investigating this
model’sperformanceislesseffectivecomparedtothevalidation lineofresearchanddelveintoadditionalcognitiveimpairment
foldsfortheclassificationtask. Despitethis,themodeloutper- diseases,besidestheirimpactonpeople’sbehavior.7. Acknowledgements
[12] J.Yuan,Y.Bian,X.Cai,J.Huang,Z.Ye,andK.Church,“Dis-
fluenciesandfine-tuningpre-trainedlanguagemodelsfordetec-
We would like to thank “A way of making Europe” tionofalzheimer’sdisease.”inInterspeech,vol.2020,2020,pp.
European Regional Development Fund (ERDF) and 2162–6.
MCIN/AEI/10.13039/501100011033 for supporting this
[13] A.Balagopalan,B.Eyre,F.Rudzicz,andJ.Novikova,“Tobertor
work under the “CHAN-TWIN” project (grant TED2021- nottobert:comparingspeechandlanguage-basedapproachesfor
130890B-C21. HORIZON-MSCA-2021-SE-0actionnumber: alzheimer’sdiseasedetection,”arXivpreprintarXiv:2008.01551,
101086387, REMARKABLE, Rural Environmental Monitor- 2020.
ing via ultra wide-ARea networKs And distriButed federated [14] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT:
Learning. CIAICO/2022/132 Consolidated group project pre-training of deep bidirectional transformers for language
“AI4Health” funded by Valencian government and Inter- understanding,” CoRR, vol. abs/1810.04805, 2018. [Online].
national Center for Aging Research ICAR funded project Available:http://arxiv.org/abs/1810.04805
“IASISTEM”.Thisworkhasalsobeensupportedbyaregional [15] I. Hajjar, M. Okafor, J. D. Choi, E. Moore, A. Abrol,
grantforPhDstudies,CIACIF/2022/175. V. D. Calhoun, and F. C. Goldstein, “Development of digital
voicebiomarkersandassociationswithcognition,cerebrospinal
biomarkers, and neural representation in early alzheimer’s dis-
8. References
ease,” Alzheimer’s&Dementia: Diagnosis, Assessment&Dis-
easeMonitoring,vol.15,no.1,p.e12393,2023.
[1] I.J.Deary, J.Corley, A.J.Gow, S.E.Harris, L.M.Houlihan,
R. E. Marioni, L. Penke, S. B. Rafnsson, and J. M. Starr, [16] J.LaguartaandB.Subirana,“Longitudinalspeechbiomarkersfor
“Age-associated cognitive decline,” British Medical Bulletin, automatedalzheimer’sdetection,”FrontiersinComputerScience,
vol. 92, no. 1, pp. 135–152, 09 2009. [Online]. Available: vol. 3, 2021. [Online]. Available: https://www.frontiersin.org/
https://doi.org/10.1093/bmb/ldp033 articles/10.3389/fcomp.2021.624694
[2] S.Duong,T.Patel,andF.Chang,“Dementia: Whatpharmacists [17] D. Ortiz-Perez, P. Ruiz-Ponce, D. Toma´s, J. Garcia-Rodriguez,
need to know,” Canadian Pharmacists Journal / Revue des M. F. Vizcaya-Moreno, and M. Leo, “Deep learning-based de-
Pharmaciens du Canada, vol. 150, no. 2, pp. 118–129, 2017. mentia prediction using multimodal data,” Neurocomputing, p.
[Online].Available:https://doi.org/10.1177/1715163517690745 126413,2023.
[18] U.Sarawgi,W.Zulfikar,N.Soliman,andP.Maes,“Multimodal
[3] F. Portet, P. J. Ousset, P. J. Visser, G. B. Frisoni, F. Nobili,
inductivetransferlearningfordetectionofalzheimer’sdementia
P.Scheltens,B.Vellas,J.Touchon,andtheMCIWorkingGroup
anditsseverity,”2020.
of the European Consortium on Alzheimer’s Disease (EADC),
“Mild cognitive impairment (mci) in medical practice: a [19] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh,
critical review of the concept and new diagnostic procedure. S.Agarwal,G.Sastry,A.Askell,P.Mishkin,J.Clark,G.Krueger,
report of the mci working group of the european consortium and I. Sutskever, “Learning transferable visual models from
on alzheimer’s disease,” Journal of Neurology, Neurosurgery naturallanguagesupervision,”CoRR,vol.abs/2103.00020,2021.
& Psychiatry, vol. 77, no. 6, pp. 714–718, 2006. [Online]. [Online].Available:https://arxiv.org/abs/2103.00020
Available:https://jnnp.bmj.com/content/77/6/714
[20] H.Liu,C.Li,Q.Wu,andY.J.Lee,“Visualinstructiontuning,”
[4] P. Celsis, “Age-related cognitive decline, mild cognitive 2023.
impairment or preclinical alzheimer’s disease?” Annals of [21] G.H.andK.E,“Bostondiagnosticaphasiaexaminationbooklet,”
Medicine, vol. 32, no. 1, pp. 6–14, 2000, pMID: 10711572. Lea&Febiger,1983.
[Online].Available:https://doi.org/10.3109/07853890008995904
[22] L.E.NicholasandR.H.Brookshire,“Asystemforquantifying
[5] World Health Organization, “Mental health action plan 2013- the informativeness and efficiency of the connected speech of
2020,” WHO Library Cataloguing-in-Publication DataLibrary adultswithaphasia,”JournalofSpeech,Language,andHearing
Cataloguing-in-PublicationData,pp.1–44,2023. Research,vol.36,no.2,pp.338–350,1993.[Online].Available:
https://pubs.asha.org/doi/abs/10.1044/jshr.3602.338
[6] M.DiLuca,D.Nutt,W.Oertel,P.Boyer,J.Jaarsma,F.Destre-
becq, G.Esposito, andV.Quoidbach, “Towardsearlierdiagno- [23] N.Rockwell,“Goingandcoming[oiloncanvas],”1947.
sisandtreatmentofdisordersofthebrain,”BulletinoftheWorld
[24] I.Arevalo-Rodriguez,N.Smailagic,M.R.iFiguls,A.Ciapponi,
HealthOrganization,vol.96,no.5,p.298,2018.
E.Sanchez-Perez,A.Giannakou,O.L.Pedraza,X.B.Cosp,and
[7] S.Luz,S.d.l.F.Garcia,F.Haider,D.Fromm,B.MacWhinney, S.Cullum,“Mini-mentalstateexamination(mmse)forthedetec-
A.Lanzi, Y.-N.Chang, C.-J.Chou, andY.-C.Liu, “Connected tion of alzheimer’s disease and other dementias in people with
speech-basedcognitiveassessmentinchineseandenglish,”2024. mildcognitiveimpairment(mci),”CochraneDatabaseofSystem-
aticReviews,no.3,2015.
[8] D.S.Marcus,T.H.Wang,J.Parker,J.G.Csernansky,J.C.Mor-
[25] A.Radford,J.W.Kim,T.Xu,G.Brockman,C.McLeavey,and
ris, andR.L.Buckner, “Openaccessseriesofimagingstudies
I.Sutskever,“Robustspeechrecognitionvialarge-scaleweaksu-
(oasis): cross-sectionalmridatainyoung,middleaged,nonde-
pervision,”2022.
mented,anddementedolderadults,”Journalofcognitiveneuro-
science,vol.19,no.9,pp.1498–1507,2007. [26] F. Eyben, M. Wo¨llmer, and B. Schuller, “Opensmile: the mu-
nich versatile and fast open-source audio feature extractor,” in
[9] R.C.Petersen,P.S.Aisen,L.A.Beckett,M.C.Donohue,A.C.
Proceedingsofthe18thACMinternationalconferenceonMul-
Gamst,D.J.Harvey,C.R.Jack,W.J.Jagust,L.M.Shaw,A.W.
timedia,012010,pp.1459–1462.
Togaetal.,“Alzheimer’sdiseaseneuroimaginginitiative(adni):
clinicalcharacterization,”Neurology,vol.74,no.3,pp.201–209, [27] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
2010. A.N.Gomez,L.Kaiser,andI.Polosukhin,“Attentionisallyou
need,” CoRR, vol. abs/1706.03762, 2017. [Online]. Available:
[10] A. M. Lanzi, A. K. Saylor, D. Fromm, H. Liu, B. MacWhin- http://arxiv.org/abs/1706.03762
ney, and M. L. Cohen, “Dementiabank: Theoretical rationale,
protocol,andillustrativeanalyses,”AmericanJournalofSpeech-
LanguagePathology,vol.32,no.2,pp.426–438,2023.
[11] S.Luz,F.Haider,S.delaFuente,D.Fromm,andB.MacWhin-
ney, “Alzheimer’s dementia recognition through spontaneous
speech:Theadresschallenge,”2020.