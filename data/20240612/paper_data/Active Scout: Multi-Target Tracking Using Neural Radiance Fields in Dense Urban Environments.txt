Active Scout: Multi-Target Tracking Using Neural Radiance Fields in
Dense Urban Environments
Christopher D. Hsu1,2 and Pratik Chaudhari2
Abstract—Westudypursuit-evasiongamesinhighlyoccluded waypoint. Finally, (d) we provide a policy for an active target
urban environments, e.g. tall buildings in a city, where a scout that utilizes knowledge of scout’s history of observations and
(quadrotor) tracks multiple dynamic targets on the ground. moves to locations that appear occluded to the scout.
We show that we can build a neural radiance field (NeRF)
representationofthecity—online—usingRGBanddepthimages
from different vantage points. This representation is used to
calculate the information gain to both explore unknown parts
of the city and track the targets—thereby giving a completely
first-principles approach to actively tracking dynamic targets.
We demonstrate, using a custom-built simulator using Open
Street Maps data of Philadelphia and New York City, that we
can explore and locate 20 stationary targets within 300 steps.
This is slower than a greedy baseline which which does not use
active perception. But for dynamic targets that actively hide
behind occlusions, we show that our approach maintains, at
worst, a tracking error of 200m; the greedy baseline can have
T hird Person View
a tracking error as large as 600m. We observe a number of
interesting properties in the scout’s policies, e.g., it switches its
attention to track a different target periodically, as the quality
of the NeRF representation improves over time, the scout also target-0
becomes better in terms of target tracking.
I. Introduction
Consider a game of cops and robbers in which a quadrotor Virtual Particles θ
scout (cop) must search for and track robbers within a city,
see Figure 1. Robbers actively avoid the scout by hiding in
F PV: RGB Groun d Truth F PV: RGB NeRF Render
blind spots, or unknown parts of the environment. In this
paper, we ask: what is the next best view for the scout to
400
maximize its information of the targets’ locations? We focus
onthreespecificaspects:(1)Ifthescoutdoesnothaveamap 200
of the scene, how does it explore to build one on the fly? (2)
How should the scout trade off between learning about the 0
ydetect
environment and tracking targets? (3) How should targets use
scout
−200
blind spots created by occlusions to hide from the scout? target-0
target-1
The contributions of this work are as follows. (a) Neural −400 target-2
target-3
radiance fields (NeRFs) can be trained online to represent the
history of color and depth images observed; they should be −500 0 500
x (m)
used to synthesize future views given a sample of future
Fig. 1: We present a snapshot of the scout (blue) in a Philadelphia
poses. As NeRFs are not probabilistic models, we show
scene with 4 targets. Top: third person view of the scene with
that we can build an ensemble from bootstrapped data and
hovering agent beacons. The red-blue line denotes the scout’s
calculateavarianceovercoloranddepth.(b)Bayesfilterscan trajectoryhistory.Middle:Ontheleftisthescout’sfirstpersonview
represent a history of detected target locations and we show (minus the labels) where it can see a red ‘target-0’ and virtually
how to incorporate them into the NeRF representation. (c) projected ground particles θ (white dots) that help update the target
filter. On theright we show the synthesized NeRF rendering of that
The computation of mutual information provides a seamless
viewaftertraining.Bottom:2Dmapshowingthebuildingfootprints
way of integrating exploration and tracking objectives. With
(grey), the scout and its current observation of the virtual particles
a ranking of sample poses, we can select a pose and perform ydetect (blue scatter), and the targets. Each target has an associated
a dynamically feasible quadrotor trajectory to the selected filter(opacitydenotesweight).Inthissnapshot,theredtarget’sfilter
is updated due to being observed. The purple target was viewed in
1DEVCOMArmyResearchLaboratorychristopher.d.hsu.civ@army.mil the past and its uncertainty spreads over time due to the motion
2DepartmentofElectrical&SystemsEngineeringandGeneralRobotics, model. The orange and green targets have not been viewed so their
Automation,SensingandPerception(GRASP)LaboratoryattheUniversity prior is still rather uniform over the scene (orange+green=brown).
ofPennsylvania.chsu8@seas.upenn.edu,pratikac@seas.upenn.edu
4202
nuJ
11
]AM.sc[
1v13470.6042:viXra
)m(
yII. RelatedWork environment.
However, NeRFs are only one part of the equation. Here
Inspiredbyautonomousinformationgatheringproblems[1],
we seek to solve the multi-target tracking problem. Past
the prosperous line of work called active perception [2]
works have used all types of filters to localize targets but
developed the central tenet that an active perceiver should
predominantly, kalman filters and their variants [19], particle
take control actions that lead to informative observations and
filters [3, 7], and probability hypothesis density (PHD)
use this data in a tight feedback loop to select the next set
filters [8, 20]. Single agent tracking with these methods is
of controls. The problem discussed here embodies active
straightforwardbutthedifficultylieswithmulti-targettracking.
perception and is rooted in adaptive sampling for which the
Generally one would make multiple copies of the filter of
goal is to choose the best option from a set of samples
choice to track each individual target. However, in the works
that minimize prediction uncertainty or maximization of
that use the PHD filter, they forgo the assumption of data
some information gain [3], in comparison to offline non-
association, i.e. given a target the user can update the correct
adaptive where the environment is static and the plan is
associated filter. In past works, this is a fair line of work to
computed offline [4]. Whether adaptive or non-adaptive,
consider as range and bearing measurements of the targets
mutual information satisfies submodularity [5] which shows
lose vital information. In the context of this work, we employ
that selecting sequential sensors locations in a greedy fashion
a perception system, e.g. an rgbd (color and depth) camera to
has a sensing quality that is provably close to the optimal
perform measurements. We assume that with our perceptual
sensing quality. This property holds for static scenes and has
systemwecandistinguishbetweenmultipletargetsandupdate
a flavor of the multi-agent sensor problem [6]. It is unclear
the associated filter [21].
how this property holds for temporally dynamic scenes or
Finally we want to mention another popular line of work
processesofinterest.Evenso,otherworkshavefoundsuccess
inmulti-targettracking:whenyouhaveateamofagents.Past
employing a greedy (myopic control) approach to dynamic
works have employed graph neural networks [22], joint or
multi-target problems [7, 8]. On the other hand, non-myopic
decentralized estimation over the information filter [3, 9, 20],
control solutions have had success in dynamic information
or multi-agent reinforcement learning algorithms [23]. In this
gathering problems [9].
work, multi-agent teaming is not the focus. We recount the
In the aforementioned works, the environment for which
submodularity property of mutual information which states
therobotsaredeployedinaresimplisticwithsimplemeasure-
that the greedy selection of locations of subsequently added
ments models, i.e. bearing or range measurements with noise.
sensors is near optimal.
In the case of urban environments or occlusions, not much
hasbeendone,butworksgenerallyconsiderocclusionswithin
III. ProblemFormulation
the control optimization scheme as obstacles [10, 11] and
these obstacles are often not intrusive to the tracking of the TABLE I: Key quantities in the text.
targets. There is work that considers urban environments [12]
ξ scene
and they incorporate the occlusions in their particle filter
θ(i) locationoftheith targetattimet
update.However,theirsensorisstationaryandusessimplistic t
measurement models. We hypothesize that for an active
xt∈SE(3) locationofthescoutattimet
perceiver in a complex environment with large occlusions (in
xpast≡x0:t pastlocationsofthescout
xfuture≡xt+∆t futurelocationofthescout
our case due to tall buildings in a cityscape) more complex
measurement and map representations are needed to test the
y(i)=(yrgb,ydepth,y d( ei t) ect) RGB, dd ee tep ct th ioi nm sa og fes tho ef it th he tas rc ge en teand
e thffi atca acy neo uf rat lhe rs ae diw ano cr eks fi. eI ln do (u Nr eRpa Fs )t [w 1o 4r ]k is[1 w3 e], llw -se uita er dgu fe od
r
yfuture≡y t( +i)
∆t
futureobservation
H(·),I(·) Shannonentropyandmutualinformation
active perception tasks for its ability to summarize multi-
modal information, e.g. photometric and geometric, in a Table I is a summary of the the key quantities that will
consistent fashion and synthesize new views to be able to be introduced in the text that follows. Denote the location
calculate information-based objectives such as predictive of the scout (a quadrotor) by xt ∈ the special Euclidean
information [15]. Uncertainty quantification for next best group SE(3) and its dynamics by x˙ = f(xt,ut) where ut
view selection with radiance fields has also seen success on denotesthecontrolinput;wewillelaborateuponthedynamics
constrained scenes [16]. On larger scenes NeRFs have been model later. Locations of the m mobile ground targets are
productive with the caveat of well constructed adjustments θ(i) ∈R2. We will assume that the scout can localize itself,
suchastrainingonprogressivelydifferentscalesofdatawhile e.g. with GPS, i.e., xt is known perfectly. The scout receives
also expanding the NeRF concurrently [17], or decomposing RGB and depth images from the scene ξ and, when the
the scene into multiple NeRFs [18]. In this work we use a targets are not occluded, it can detect them in the images.
fixed neural network size and online data which trades off Let y t(i) =(yrgb,ydepth,y d(i e) tect) denote the observation received
high quality resolution reconstruction for a smaller memory at time t corresponding to the ith target; it consists of RGB
footprint and speed of training. In fact, the focus of this and depth images from the scout’s location and detections of
work is the use of NeRFs as a mode to balance exploration the target in these images.
and exploitation in target tracking in an unknown complex The scout searches and tracks the targets. We developedan active perception objective for such problems in [13] and advances in NeRFs [24, 25] to build our representation. We
argued that an agent performing active perception should will represent the scene ξ : x → (c,σ) as a neural radiance
maximize the mutual information that past observations field which takes as input a pose x ≡ (R,T) ∈ SE(3) and
contain about future ones. Future observations are, of course, outputs color c ∈ R3 and density σ ∈ R +. Each location x
unavailable,andthereforesuchanagentshouldhavetheability in the NeRF is parameterized using a positional encoding
to synthesize new observations, i.e., a generative model. This schemecalledamulti-resolutionhashmapwhichfeedsintoa
representation would ideally be constructed incrementally multi-layeredperceptron(MLP)with2layersand128neurons
using past observations. We set perlayer,see[25]formoredetails.WetraintheNeRFonthe
ufuture ∈argmaxI(yfuture;ypast |xfuture). (1) fly (using a few iterations of stochastic gradient descent after
φ each time-step) using RGB images yrgb and depth images
where φ ≡ p(xfuture | ypast) is the probability distribution of ydepth with ground-truth pose from the quadrotor as it flies
the next scout location over which we are optimizing, and it
through the city.
depends upon the control ufuture. Mutual information between
The power of the NeRF representation is that it can be
two random variables I(y;x) is defined as
used to synthesize images from new viewpoints that the
(cid:90) p(x,y)
I(y;x)= dydxp(x,y)log scout might not have seen before. The volume rendering
p(x)p(y)
equation lies at the heart of this capability; it is also useful
=H(y)−H(y|x),
to understand how the NeRF is trained. Assume a pinhole
where H is the Shannon entropy. It is equal to the Kullback-
model for the camera where rays emanate from the focus
Liebler (KL) divergence KL(p(y | x),p(y)) averaged over at T. A point in the distance d ∈ R from the focus along
all possible realizations of x. In our case, the mutual
this ray has orientation δRR which can be written as x(d)=
information characterizes the discrepancy between the scout’s
δRR(d,0,0)⊤+T. The additional rotation δR corresponds
future observations given its future locations and the scout’s
to all rays that lie in the field of view of the camera. The
observations given its past locations/observations. The scout
volume rendering equation samples points along this ray
takes control actions that maximize this discrepancy, i.e., it
whilequeryingtheNeRFforcolorc(x)anddensityσ(x).The
maximizes the information gain to take control actions that transmittance p(d)=exp(cid:16) −(cid:82)d σ(x(s))ds(cid:17) is the probability
provide new information about the scene and the targets. To that a ray travels for an additid o0nal distance d from the image
calculate the mutual information, the scout must also be able
image (which is at distance d ) without encountering a solid
to sample future observations yfuture given past ones (this
object. Therefore,
p(d)σ(x(d))0
is the probability that the ray
means, both how images from the scene ξ will look and
stops at d. Each rendered pixel has
where the targets might be detected in these images). We will
(cid:90) d
discuss how to do this in the following sections. color:yrgb = dsp(s)σ(x(s))c(x(s)),
d0 (2)
IV. Methodology (cid:90) d
depth:ydepth = dsσ(x(s))s.
We have three components to the observation y: images
These integrals are
implemd0ented
using quadrature [14] and
fromthestaticsceneξ anddetectionsofthedynamictargetsθ.
techniques like [24] can be used to speed up the rendering
Inthissection,wewilldiscussfirsthowtorepresentthescene
process by skipping known free space.
when it is unknown to the scout via neural radiance fields
We train the NeRF using images, depths, and their corre-
(NeRFs). Next, we will describe how we use a Bayes filter to
sponding viewpoints collected online from a simulator that
represent and maintain an estimate of target locations. Given
renders Open Street Maps data [26], see Figure 2. For each
these two representations, we will show how to calculate
image and its viewpoint, we query the MLP for the color and
the most informative next location of the scout. Roughly
density at different points along the ray. The rendered color
speaking,thequadrotorfirstsamplesfutureobservationsyfuture
and depth of each pixel are compared to their ground-truth
f mro am ximp( iy zf eu sture th| exf iu ntu fr oe r, mξ, aθ tt i) onan gd ac inal .cu Ala tte es act hhe sv teie pw , ix tfu utu pre dath tea st valuestocalculatethelossℓ=λ 1ℓrgb+λ 2ℓdepth;weusetheℓ 1
lossforbothterms.Stochasticgradientdescent(SGD)isused
the Bayes filter and trains the NeRF using new observations
to optimize the parameters of the MLP using this objective.
yt ∼p(yt |xt,ξ,θt). We will now focus on calculating
Wetunedthehyper-parametersλ ,λ suchthatthetwoterms
I(yfuture; ypast). have approximately equal magni1 tude2 during training. As the
There are a few components to this: the mutual information quadrotor flies through the scene, we expand the training
corresponding to RGB and depth images and the mutual dataset incrementally: adding new images and continuously
information corresponding to the target detections. These are performing SGD to update the NeRF. For each mini-batch,
discussed in the following sections. half the images are sampled from recent observations and
the other half are sampled uniformly randomly from past
A. NeRF representation of the scene ξ observations [27].
Ifthemapisunknown,thescoutmustfirstbuildarepresen- We seek to calculate the mutual information
tation of the scene such that we can synthesize observations I(yfuture,rgb;ypast) or I(yfuture,depth;ypast) for RGB and depth
corresponding to future states xfuture. We utilize the recentB. Bayes Filter to estimate target locations
We represent the probability distribution of the location of
the ith ground target using N particles
N
(i) (cid:88) (i) (i)
p(θ
t
|ypast)= w
t,k
δ θ˜(i)(θ
t
)
k=1 t,k
where each θ˜(i) ∈ R2. In our problem, it is convenient to
t,k
set up the particles densely on a fixed grid on the ground
and set the weights of particles inside buildings (for a known
map) to be zero. Updating the probability distribution after
eachtime-stepthereforecorrespondstoimplementingaBayes
filter (rather than a particle filter). Note that the choice of the
Bayes filter is merely for convenience; we could have also
Fig. 2: We show an example of two scenes: Philadelphia (top) and implemented a particle filter for this problem.
NYC StuyTown (bottom). For each scene we show the ground truth
view RGB (left) compared against the NeRF rendering of RGB a) Motion Model: Depend-
(middle) and depth (right). These RGB renderings have a PSNR ingupontheexperimentalsetting, 2 0.06
≈23. Also see Figure 7. targets will either be stationary, 1
0.04
or actively hide from the scout 0
images respectively. The scene ξ is a sufficient statistic
of the past observations ypast. So we need to calculate i bn uit lh de inb gl sind insp tho ets cc ir te ya .te Wd eby cot mhe
-
−1 0.02
p sc(ξ en| ey ap na dst) p(t yh fe utup rer ,o rgb ba |b ξi )lit oy r pd (is yt fr ui tub ru e,dti eo ptn
h
|o ξv )e ,r tht ehe pru on bk an bo ilw itn y p alu gt oe rt ih the mlat ft oe rr tu hs ein tg ara geD t,ĳ tk hs it sra’ iss F− i2 g.− 32
:
Targ0
et
nois2
e
dis0 t.0 r0
i-
of scene given candidate future locations of the scout. bution model.
described in the next section. In
In [13], we show that we can calculate a distribution
addition to this, we assume that the scout does not know the
over scenes using bootstrapped versions of the training
truemotionmodelofthetargets.Ifthetargetislocatedatthe
dataset to build an ensemble of NeRFs that together
origin, the scout assumes that the probability of it moving
represents p(ξ | ypast). We use two MLPs {ξ k}2
k=1
to set
to a nearby location is given by the probability distribution
it to be δ (ξ)/2 + δ (ξ)/2, where δ denotes the Dirac
ξ1 ξ2 in the adjoining figure. It is important to choose this noise
delta distribution. NeRFs are not probabilistic models and
distribution carefully.1 The dynamics update for the Bayes
therefore we cannot directly compute the likelihood of
filter corresponds to a convolution of the particle weights
the scene. However, since the integrals of (2) are just an
with the adjoining kernel, see Figure 3.
expectation, we can adjust them to calculate a variance for
c sio mlo ir la: rv ea xr( py rr eg sb s) io=
n
(cid:82) fodd 0rd ds ep p( ts h)σ v( ax r(( ys) d) ep( tc h( )x .( Is f)) w− ey ar sg sb u)2 mean td haa
t at
timb e) tM (e ta as rgu ere tm de en tet cM tioo nde s)l: wG eiv ce an na un pe dw ateob ts he erv Ba ati yo en sy fide lt te ec rt
color and depth have a Gaussian distribution then we can as p(θ|ypast)∝p(θ|y 0:t−1) p(yt |θ). We assume that when
calculate quantities like p(yfuture,rgb |ξ). the target is in the field of view of the camera, the scout can
detect it with a probability 0.95, i.e., with Bernoulli noise.
In unbounded scenes such as the city, some rays extend to
When the target is hidden due to occlusions caused by the
infinity. This is problematic for our application because the
buildings, the scout cannot see the target. We assume that the
scout may resort to exploring the sky overhead to maximize
scout can perfectly distinguish targets from each other and
theinformationgain,i.e.,thedepthinfinity,sothereisalways
therefore the probability of incorrect data association is zero.
a mismatch between the volume density σ predicted by the
NeRF and the true volume density. We use a technique c) Calculatinginformationgainfortargettracking: The
from [28] to resolve this issue. The authors argue that most posterior over the targets p(θt |ypast) is a sufficient statistic
rays in the NeRF eventually hit some solid surface. One of the past observations ypast for detection. To calculate
can therefore model the occupancy yocc along a ray as a I(yfuture,detect; ypast) we need to calculate the probability
Bernoullirandomvariablewheretherayhitsanobstaclewith distributionofthetargetgivensomecandidatefuturelocation
probability1−p(dmax)andgoesofftoinfinitywithprobability of the scout. Given the ground-truth map, the statistic p(θt |
p(dmax). For us, this is effectively an additional observation ypast), and under the assumption that the target is stationary,
from the NeRF that depends on the volume density σ. We it is straightforward to calculate p(yfuture,detect |xfuture,θt); this
can calculate p(yocc | x t+∆t) which is probability over the is shown pictorially in Figure 1 (middle left). If we do not
Bernoulli distribution and also add an additional term to the
1Considerthesituationwhenthescoutchoosesaviewpointthathasline-
mutual information objective. This term reduces the wasteful
of-sight of the particle with a high weight. If the noise distribution were
exploration that the scout performs to gain information about Gaussian(symmetricaroundtheorigin),ifthetargetmovedaway,andthe
the open sky. scout did not obtain a detection, the posterior p(θt) would spread in all
directions.Intheory,thisisnotanissue,butinpractice,suchnoiseleadstoa
largevarianceintheposterior.Thischosennoisedistributionmodelsatarget
thatstaysinthevicinityoftheoriginwithalargeprobabilitybutescapesin
thedirectionofthefourcorners(asopposedtoanarbitrarydirection).have the ground-truth map, we use the underlying voxel grid multinomialsampling;thosewithlargerIaremorelikelytobe
from the NeRF [24, 25] and the probability p(yocc |xfuture,ξ) chosen. This scheme breaks the greedy formulation that can
to trace a ray from the camera to each particle of the Bayes causethescouttobestuckinlocalminima.Scouttrajectories
filter. If any voxel along this ray has a NeRF volume density between successive waypoints xt → x
t+∆t
are calculated
aboveathreshold,theparticleisunobservablefromthatpose. using Dĳkstra’s algorithm combined with rotorpy [30] to
Whether we have the map or not, like we described above, solve an quadratic optimization problem that parameterizes
the likelihood p(yfuture,detect |xfuture,θt) is a Bernoulli random the flat outputs using a 7th order polynomial to minimize
variable with parameter 0.95. Therefore, we can calculate the the integral of the squared snap. Since the camera pitch is
information gain I(yfuture,detect; ypast) for each target.2 independentofthequadrotordynamics,welinearlyinterpolate
the pitch along this trajectory. We found it helpful to perform
C. Controlling the trajectories of the scout an additional 2π yaw rotation with some modulation of the
pitch at the end of the trajectory; this gives the scout extra
Weusethestandarddifferentiallyflat[29]dynamicalmodel
information to train the NeRF as well as a larger potential
of a quadrotor where we are able to recover all other parts of
for spotting targets.
the state andcontrol inputsjustfrom the fourflat outputs:3D
Euclideanpositionandyaw.Withaflatsystem,wecandesign
trajectories that satisfy initial and final boundary conditions V. SimulationExperiments
easily, e.g. any polynomial that fits these conditions, up
to control constraints, is a dynamically feasible trajectory. We built a simulator using Open Street Maps [26] that
Additionally, we include an independent fifth state, pitch, and can render the scene (buildings, locations of the targets
altogether, waypoints for the quadrotor are in 5-dimensions. etc.) using OpenGL. We focus on two specific maps for
At each time-step, we sample a set of putative future states our simulation experiments. The first is of Center City
xfuture in free space (straightforward with the ground-truth Philadelphia,seeFigure1,wherewesetthecoordinateorigin
map, voxel grid underlying the NeRF is used otherwise). to be at latitude: 39.9517 and longitude: -75.1671. The map
For each waypoint, we sample future observations yfuture to is normalized such that each unit in each direction is roughly
calculate mutual information. We make a key simplifying 1m. Similarly, our second map is of an apartment complex
assumption: StuyTowninNYC,seeFigure2(bottom),withthecoordinate
I(yfuture; ypast)=I(yfuture,rgb; ypast)+I(yfuture,depth; ypast) origin set to be at latitude: 40.7327 and longitude: -73.9771.
The Center City Philadelphia map has taller buildings with
m (3)
+λ(cid:88)I(y f( ui t) ure,detect; ypast). irregular heights creating more intricate occlusions than the
i=1 NYC map that has shorter buildings and more space in
Here,thefirsttermcorrespondstotheinformationgainforthe
between. We bound the altitude the scout can travel such that
scene (as if there were no targets, split between independent
in the Philadelphia scene it can only fly up to an altitude of
terms for RGB, depth, and occupancy) and the second term
150m whereas in the NYC scene it can fly up to 100m. We
correspondstoinformationgainforthetargets(asifthescene
do this because if the scout is able to fly to an unbounded
were known). Note that the probability density of the target
height, it is easy to observe the entire map from a single
locationsinourBayesfilteriscertainlyafunctionofthescene
vantage point. From this simulator, given a pose ∈SE(3) we
(e.g., observations respect occlusions, targets cannot enter
are given RGB and depth images.
buildings, etc.). This decomposition allows us to calculate
The scout has a camera with a field of view of 90 degrees
mutual information without worrying about calculating the
jointdistributionofthesceneandtargets.Thehyper-parameter
and receives RGB and depth images of size 320×320. Each
experiment begins with the scout at the origin and collecting
λ=10 enables the scout to trade-off between target tracking
observationsbyfirstincreasingthealtitudetosomedesignated
and learning the scene (which helps target tracking in the
long-term even if it forgoes near-term tracking performance). maximum; it then performs a 2π yaw rotation with some
randomperturbationstothepitchtofitaninitialmodelofthe
We calculate 10 different scout distributions φ≡p(xfuture | scene with 30 images, 1 image per step. We train the NeRF
ypast) for (1); each of these distributions is represented by on these initial images for 4,000 training iterations before the
10 particles centered around some waypoint in 3D space.
experiment begins, and train for 4,000 more iterations after
Calculating the mutual information objective is an expensive
each waypoint is reached. Targets are randomly initialized on
calculation because it involves many different queries of the
the ground plane in free space.
NeRF; depending upon the application one could use fewer
Our figure of merit is the mean squared error (MSE)
waypoints. Instead of an argmax in (1) we experimented with
between the scout’s estimate of the target (mean of the
a more stochastic policy where waypoints are chosen using
posterior in Bayes filter) and the target’s true position. To
highlighttargetsthatarecurrentlybeingobserved,weplotthe
2Thiscalculationusesasimplisticdynamicsmodelofthetarget(described
above). This is a pragmatic choice. In principle, we could use a more minimumMSEasahighopacityplotwiththecolorindicating
complicateddynamicsmodel,e.g.,thattargetshideinblindspots.Butthen which target is contributing to this value. In comparison,
calculating p(yfuture,detect | ypast) is quite difficult; it would require us to we plot the maximum MSE as a low opacity plot and the
runadifferentupdateusingaparticlefilterforeachputativescoutlocation
xfuture. corresponding target color to represent the neglected target.A. Scout Policies since we have a camera sensor that can provide good target
identification. In all our experiments, we add a new filter for
We evaluate the 3 scout policies over 2 target policies
each new target i that is found such that there is always 1
(stationaryandactive)in2scenesallwithseed88(arbitrarily
extra filter that has not been assigned a target. By always
chosen). Each experiment will begin with the scout executing
maintaining one extra filter, the scout always has a small
an initialization phase for which they will fly to their
opportunity to select a view that promotes exploration. When
maximum height and do a 2π yaw rotation scan of the scene.
a new target is found the extra filter is assigned and a new
Afterthat,thescoutwillperform40planningstepswitheach
one with a uniform prior is created. We see that in Figure 4
step having 30 control steps. The 3 methods below describe
for each method, all targets have been identified and the filter
the scout’s policies and information strategy. We will have:
for each target has converged to the true locations.
• GTmap+MAP: ground truth map + greedy follower
(MAP: Maximum A Posteriori), 600 t ta ar rg ge et t- -0 1 600 600
target-2
• GTmap+MI: ground truth map + mutual information, 400 target-3 400 400
• and NeRF+MI: NeRF + mutual information. 200 200 200
The following experiments will evaluate whether neural
0 0 0
0 200 400 0 200 400 0 200 400
radiance fields (NeRF) are a valid replacement for a ground Steps Steps Steps
truth map (GTmap) given some control policy utilizing
the map representation. In the NeRF+MI experiments, we
will train a NeRF from data collected on the fly and scout
start loc
execute MI based control from that representation, (3). In trajectory
comparison, GTmap+MI will use the ground truth map
and mutual information calculated over the Bayes filter, i.e.
Fig. 4: Top: In the Philadelphia scene, we test the three methods
I =(cid:80)m i=1I(y f( ui t) ure,detect; ypast). GTmap+MAP (left), GTmap+MI (middle), and NeRF+MI (right)
As our control baseline, we will use the ground truth map on 20 stationary targets randomly initialized within the scene (the
legend shows an example of the first 4 targets colors) and show the
plus a greedy control policy that takes control actions that
MSEplotovercontrolsteps.Bottom:Weshowthescout’strajectory
maximize a posteriori (MAP) over the Bayes filter. The MAP
(blue line) using the GTmap+MAP (left) and NeRF+MI (right) and
policy can be thought as choosing the pose that gives the the locations of the 20 targets in the 2D plot of Philadelphia.
maximum expected value over the detected target locations
given the future poses such that
2) Actively hiding targets: Targets with the active policy
ufuture ∈argmaxE θ[y f( ui t) ure,detect;ypast|xfuture], have knowledge of the scout and maintain a history of graph
φ nodesthatthescouthasseenduringonlineexecution.During
where φ≡p(xfuture |ypast).
each iteration, an active target will randomly select a graph
nodefromthelistofparticlesthescouthasnotseenandmove
B. Target Policies
there via a path found by Dĳksta’s. We observe that targets
For each of the 3 scout policies as described above, we that follow this policy will actively hide behind buildings,
evaluatethemon2differenttypesofcontrolpolices:stationary i.e. more occluded locations, locations that are harder for the
targets and active targets. Stationary targets will be our scout to see. This is due to the fact that over time as the
exploration baseline for these methods. On the other hand, scout sees more parts of the map, the parts that have not
active targets are dynamic and are deliberate in the locations been seen must be places of the map that the scout must be
they select to go to as they are aware of the scout’s actions more deliberate to be able to view. We reset this observed
and views. particles buffer at regular intervals (every 10 planning steps)
1) Stationary Targets (exploration task): In the first ex- so that targets can return to old graph nodes. This type of
periment, see Figure 4, we test the three methods against reset forces the scout to return to previously seen locations
20 stationary targets in the Philadelphia scene. We observe for the sole reason of observing the targets.
that all methods are able to localize all the targets within In Figure 5, we plot the mean squared error (MSE) of
some time. Given a uniform prior over target locations, it can the scout’s estimate of the target over control steps in the
be seen that GTmap+MAP is the quickest at finding all the Philadelphia scene (top row) and the NYC StuyTown scene
targets. MI based policies take longer to find all of the targets (bottom row). After the initialization phase (first 30 control
but they seem to be more thorough in exploring the map. For steps) in both scenes, there is a low minimum MSE as at
the observant reader, they will notice that the MSE increases least 1 target has been seen. This is indicated by the high
for some targets for some time. This happens because we still opacity plot and the color denotes which target contributes
apply the motion model to bring about some uncertainty in to that value, i.e. the red target has the smallest MSE of the
the filter. We will see later that when the targets are dynamic, 4 and is plotted. On the other hand, the orange target has
the greedy policy is suboptimal. the largest MSE and is plotted in orange with light opacity.
This experiment also shows that our method does not In both scenes and all methods, by approximately step 200,
need to know how many targets are in the scene a priori all targets have been spotted at least once. From there on
ESM ESM ESMout the targets are more actively hiding and is the cause of
error for the scout for the rest of the episode. We observe
that the NeRF+MI (right) experiments have similar trends
to that of GTmap+MI (middle) where different targets are
observed over time (varying colors of the full opacity plots)
while allowing some targets to remain undetected for some scout
target-0
time (light opacity plots). We can see with the variation in target-1
target-2
colors that over time as targets have not been observed for t sa targ rte lt o-3 c
some time they will gain in MSE, but upon observation that Fig.6:Weshowtheactivetargets’(coloredlines)trajectoriesinthe
MSE will become small and a different target will contribute NYC StuyTown map so we can observe the scout’s trajectory (blue
line) executing the GTmap+MAP (left), GTmap+MI (middle), and
to the large MSE.
NeRF+MI (right) policies. The trajectories plotted start at step 500
Next, compare the MI based policies to the MAP based (black ‘x’) until the end of the episode (circle).
policies in Figure 5. In the Philadelphia scene (top), the
comparison, MI based policies (middle and right) do a better
greedy MAP agent with the ground truth map (left), does a
jobinexploringthemapevenwhentargetsareinviewandin
poor job of observing the orange target, allowing its MSE to
the vicinity. Even so, GTmap+MI only does a single pass to
explode. After some time it does find the target and ends the
the top left of the map resulting in a large MSE in Figure 5
episode with smaller MSE. The MAP greedy scout in both
(bottom middle). NeRF+MI (right) does the best at exploring
scenes can be seen to miss a target for quite a while. We
with multiple passes into the top left portion of the map and
attribute this to the scout greedily checking locations with
thisresultsinthescoutmaintainingalowMSEfortheorange
a large posterior, i.e. places the filter indicates the target is
agent as seen in Figure 5 (bottom right).
likely to be. Since the targets move to places that are hard
to find for the scout, the greedy scout does a bad job of Finally, we take a look at the reconstruction quality of
checking hard to view areas, therefore missing hard to find the NeRF trained on the fly in Figure 7. We plot the peak
targets. signal to noise ratio (PSNR) which is a metric to describe
the reconstruction quality of the NeRF image against the
600 t ta ar rg ge et t- -0 1 600 600 ground truth image. In Figure 7, after each set of control
target-2
400 target-3 400 400 steps where the scout is collecting images, it stops to train
the NeRF for a set number of training steps. We report the
200 200 200
average PSNR of the most recent 20 images collected prior.
0 0 500 1000 0 0 500 1000 0 0 500 1000 We observe that as the scout selects new locations to travel
Steps Steps Steps
to, it expands the scene for which the NeRF must learn to
600 t ta ar rg ge et t- -0 1 600 600
target-2 reconstruct. As the dataset grows and the scene grows, it
target-3
400 400 400 becomes more difficult for the NeRF to render high quality
200 200 200 images:duetomodelcapacityordatainsufficiency.Although
this is the case, we have seen previously that the scout is
0 0 0
0 500 1000 0 500 1000 0 500 1000
Steps Steps Steps still able to do a good job in tracking the targets. We plot
Fig. 5: In these plots the 4 targets follow the active policy: targets the PSNR evaluation at step 2,000 and at step 4,000. In past
move to locations on the map that the scout has not seen. Top:
experiments, e.g. Figure 5 (right), the 4,000 step result is
In the Philadelphia scene, we compare 3 types of scout policies
(left: GTmap+MAP, middle: GTmap+MI, right: NeRF+MI). The what the scout utilized during the episode.
plots display maximum (light opacity) and minimum (full opacity)
mean squared error (MSE) for the estimated target location against 20 20
the true target position. The color plotted shows which target is
contributing to the maximum or minimum error. Bottom: Similar 10 10
to the plots in the top row, the bottom row of plots are the 3 scout
4k training steps
policies in the NYC StuyTown scene tracking active targets. 2k training steps
0 0
0 20 40 0 20 40
WewanttonoteaninterestingobservationintheNYCStuy-
Planning Step Planning Step
Town scene with respect to the orange target as it produces Fig. 7: During each planning step after a set of control steps, the
large MSE for the GTmap+MAP and the GTmap+MI scout, scout takes some time train the NeRF. We plot the average peak
see Figure 5 bottom row. Looking at Figure 6, we surmise signal to noise ratio (PSNR) between the test set (20 most recently
that adding in the photometric and geometry information, i.e. collected images) and the ground truth after 2,000 training steps
and 4,000 training steps for the Philadelphia (left) and NYC (right)
yfuture,rgb and yfuture,depth, encourages the scout to do a more
scene. A larger PNSR is better.
thorough exploration of the map that leads to finding the
orange target in comparison to just the MI or MAP from Although we quantitatively only see a small difference in
the filter. Compare the blue trajectories of the scout. On the PSNR increase from 2k to 4k steps in Figure 7, we observe
left is the NeRF+MAP scout that greedily rotates between a difference in tracking quality. See Figure 8 which is the
the estimated target locations. It seems to miss the orange tracking error for the scout that only takes 2,000 training
target as it moves to the top left most part of the map. In steps per planning step. The upper bounds of the MSE in
ESM
ESM
ESM
ESM
ESM
ESM
RNSP RNSPboth the Philadelphia (left) and NYC (right) are greater than [4] A. Singh, A. Krause, C. Guestrin, and W. J. Kaiser, “Efficient
thatofthecomparativeexperimentsshowninFigure5(right). informative sensing using multiple robots,” Journal of Artificial
For example in the Philadelphia map, this less trained scout
IntelligenceResearch,vol.34,pp.707–755,2009.
[5] A. Krause and C. Guestrin, “Submodularity and its applications in
takes longer to find the orange agent and furthermore does a optimizedinformationgathering,”ACMTrans.Intell.Syst.Technol.,
worse job at tracking. We surmise that the better the NeRF vol.2,jul2011.
[6] G.A.HollingerandG.S.Sukhatme,“Sampling-basedroboticinfor-
representationis,thebetteritsunderstandingoftheocclusions
mationgatheringalgorithms,”TheInternationalJournalofRobotics
ofthesceneare,whichresultsisallowingmutualinformation Research,vol.33,no.9,pp.1271–1287,2014.
to extract out poses that view harder to spot blind spots. [7] J.R.SpletzerandC.J.Taylor,“Dynamicsensorplanningandcontrol
foroptimallytrackingtargets,”TheInternationalJournalofRobotics
target-0 Research,vol.22,no.1,pp.7–20,2003.
600 600 target-1 [8] P.Dames,P.Tokekar,andV.Kumar,“Detecting,localizing,andtracking
target-2
target-3 anunknownnumberofmovingtargetsusingateamofmobilerobots,”
400 400
The International Journal of Robotics Research, vol. 36, no. 13-14,
200 200 pp.1540–1553,2017.
[9] B.Schlotfeldt,D.Thakur,N.Atanasov,V.Kumar,andG.J.Pappas,
0 0 “Anytime planning for decentralized multirobot active information
0 500 1000 0 500 1000
Steps Steps gathering,” IEEE Robotics and Automation Letters, vol. 3, no. 2,
Fig. 8: We plot the MSE of the targets for a scout taking only 2k pp.1025–1032,2018.
trainingstepstotraintheNeRFduringaplanningiteration.Theleft [10] K.Hausman,G.Kahn,S.Patil,J.Müller,K.Goldberg,P.Abbeel,and
is of Philadelphia and should be compared against Figure 5 (top G.S.Sukhatme,“Occlusion-awaremulti-robot3dtracking,”in2016
right) and the right is of NYC compared against Figure 5 (bottom IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems
right).
(IROS),pp.1863–1870,2016.
[11] F.Vanegas,J.Roberts,andF.Gonzalez,“Uavtrackingofmobiletarget
inoccluded,clutteredandgps-deniedenvironments,”in2018IEEE
VI. Conclusion AerospaceConference,pp.1–7,2018.
[12] C. Berry and D. J. Bucci, “Obstructed target tracking in urban
In this work we study the pursuit-evasion game in which environments,”2019.
[13] S. He, C. D. Hsu, D. Ong, Y. S. Shao, and P. Chaudhari, “Active
a scout (quadrotor) must track multiple active ground targets
perceptionusingneuralradiancefields,”2023.
in a large urban scene. We showed that even if we do not [14] B.Mildenhall,P.P.Srinivasan,M.Tancik,J.T.Barron,R.Ramamoor-
have a map, we can train a neural radiance field (NeRF) thi,andR.Ng,“Nerf:Representingscenesasneuralradiancefields
forviewsynthesis,”2020.
representation of the scene online and still perform admirably
[15] W.Bialek,I.Nemenman,andN.Tishby,“Predictability,complexity
against baselines that use the ground truth map. We saw andlearning,”2001.
that the NeRF provides a sufficient representation of past [16] W.Jiang,B.Lei,andK.Daniilidis,“Fisherrf:Activeviewselectionand
uncertaintyquantificationforradiancefieldsusingfisherinformation,”
observations and that building an ensemble of NeRFs gives
2023.
us way to calculate probabilistic information. Furthermore, [17] Y.Xiangli,L.Xu,X.Pan,N.Zhao,A.Rao,C.Theobalt,B.Dai,and
in order to track targets we use a Bayes filter to represent D. Lin, “Bungeenerf: Progressive neural radiance field for extreme
multi-scalescenerendering,”2023.
target locations and we demonstrated how the NeRFs’ voxel
[18] M. Tancik, V. Casser, X. Yan, S. Pradhan, B. Mildenhall, P. P.
grid can be used to incorporate this filter. The efficacy of Srinivasan,J.T.Barron,andH.Kretzschmar,“Block-nerf:Scalable
our method provides support that mutual information can largesceneneuralviewsynthesis,”2022.
[19] B.Charrow,“Information-theoreticactiveperceptionformulti-robot
methodically combine exploration and tracking objectives.
teams,”012015.
Our NeRF is trained with RGB and depth images collected [20] A.BanerjeeandJ.Schneider,“Decentralizedmulti-agentactivesearch
from our OpenGL based simulator that renders Open Street andtrackingwhentargetsoutnumberagents,”2024.
[21] P. Tokekar, V. Isler, and A. Franchi, “Multi-target visual tracking
Maps data. We found that reconstruction quality of the scene
with aerial robots,” in 2014 IEEE/RSJ International Conference on
is important to tracking tasks and further improvements IntelligentRobotsandSystems,pp.3067–3072,2014.
to building the NeRF for larger scenes should improve [22] M. Tzes, N. Bousias, E. Chatzipantazis, and G. J. Pappas, “Graph
neuralnetworksformulti-robotactiveinformationacquisition,”2022.
performance [17, 18]. Similarly, in the future we we would
[23] C. D. Hsu, H. Jeong, G. J. Pappas, and P. Chaudhari, “Scalable
like to relax the assumption of having ground truth depth reinforcementlearningpoliciesformulti-agentcontrol,”2021.
by utilizing monocular depth estimation models such as [24] R. Li, M. Tancik, and A. Kanazawa, “Nerfacc: A general nerf
accelerationtoolbox,”2023.
Marigold [31] or DINOv2 [32].
[25] T.Müller,A.Evans,C.Schied,andA.Keller,“Instantneuralgraphics
primitiveswithamultiresolutionhashencoding,”ACMTransactions
VII. Acknowledgements onGraphics,vol.41,p.1–15,July2022.
[26] OpenStreetMap contributors, “Planet dump retrieved from
WewouldliketothankBethanyAllik,NathanSchomer,and https://planet.osm.org .” https://www.openstreetmap.org,
2017.
Franklin Shedleski from ARL for the insightful conversations
[27] J.Yu,J.E.Low,K.Nagami,andM.Schwager,“Nerfbridge:Bringing
on this work. real-time,onlineneuralradiancefieldtrainingtorobotics,”2023.
[28] N.Sünderhauf,J.Abou-Chakra,andD.Miller,“Density-awarenerf
References ensembles:Quantifyingpredictiveuncertaintyinneuralradiancefields,”
2022.
[1] A.Wald,“SequentialTestsofStatisticalHypotheses,”TheAnnalsof [29] D.MellingerandV.Kumar,“Minimumsnaptrajectorygenerationand
MathematicalStatistics,vol.16,no.2,pp.117–186,1945. control for quadrotors,” in 2011 IEEE International Conference on
[2] R. Bajcsy, Y. Aloimonos, and J. K. Tsotsos, “Revisiting active RoboticsandAutomation,pp.2520–2525,2011.
perception,”2016. [30] S.Folk,J.Paulos,andV.Kumar,“Rotorpy:Apython-basedmultirotor
[3] G.HoffmannandC.Tomlin,“Mobilesensornetworkcontrolusing simulatorwithaerodynamicsforeducationandresearch,”arXivpreprint
mutualinformationmethodsandparticlefilters,”AutomaticControl, arXiv:2306.04485,2023.
IEEETransactionson,vol.55,pp.32–47,022010.
ESM ESM[31] B. Ke, A. Obukhov, S. Huang, N. Metzger, R. C. Daudt, and
K.Schindler,“Repurposingdiffusion-basedimagegeneratorsformonoc-
ulardepthestimation,”inProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition(CVPR),2024.
[32] M.Oquab,T.Darcet,T.Moutakanni,H.Vo,M.Szafraniec,V.Khalidov,
P.Fernandez,D.Haziza,F.Massa,A.El-Nouby,M.Assran,N.Ballas,
W. Galuba, R. Howes, P.-Y. Huang, S.-W. Li, I. Misra, M. Rabbat,
V. Sharma, G. Synnaeve, H. Xu, H. Jegou, J. Mairal, P. Labatut,
A.Joulin,andP.Bojanowski,“Dinov2:Learningrobustvisualfeatures
withoutsupervision,”2024.