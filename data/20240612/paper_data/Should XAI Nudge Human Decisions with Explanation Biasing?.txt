ICRES2024:9thInternationalConferenceonRobot
EthicsandStandards,Yokohama,Japan,29-31July2024.
https://doi.org/10.13180/icres.2024.29-31.07.id#
Should XAI Nudge Human Decisions with Explanation Biasing?
YosukeFukuchi1 andSeijiYamada2,3
1 Tokyo Metropolitan University, Tokyo, 191-0065, Japan
2 National Institute of Informatics, Tokyo, 101-8430, Japan
3 The Graduate University for Advanced Studies (SOKENDAI), Kanagawa, 240-0193, Japan
E-mail: fukuchi@tmu.ac.jp
This paper reviews our previous trials of Nudge-XAI, an approach that introduces automatic
biases into explanations from explainable AIs (XAIs) with the aim of leading users to better
decisions,anditdiscussesthebenefitsandchallenges.Nudge-XAIusesausermodelthatpredicts
the influence of providing an explanation or emphasizing it and attempts to guide users toward
AI-suggesteddecisionswithoutcoercion.Thenudgedesignisexpectedtoenhancetheautonomy
ofusers,reducetheriskassociatedwithanAImakingdecisionswithoutusers’fullagreement,and
enableuserstoavoidAIfailures.TodiscussthepotentialofNudge-XAI,thispaperreportsapost-
hocinvestigationofpreviousexperimentalresultsusingclusteranalysis.Theresultsdemonstrate
the diversity of user behavior in response to Nudge-XAI, which supports our aim of enhancing
user autonomy. However, it also highlights the challenge of users who distrust AI and falsely
makedecisionscontrarytoAIsuggestions,suggestingtheneedforpersonalizedadjustmentofthe
strengthofnudgestomakethisapproachworkmoregenerally.
Keywords: Explainable AI; XAI Nudge; Human-AI Interaction; Human-XAI Interaction; Stock-
tradingSupport.
1. Introduction
The blackbox nature of machine learning models (particularly deep learning) makes it diffi-
cultforuserstojudgewhethertheyshouldrelyonanAIdecision,whichcantriggerineffec-
tive use of AIs such as over-/under-reliance.1 However, an increasing number of explainable
AIs (XAIs) are actively being developed for this problem,2 and they demonstrate the ef-
fectiveness of presenting explanations along with AI predictions in diverse applications.3–5
The explanations include visual explanations such as saliency maps6–8 and linguistic expla-
nations.9,10 The development of large language models (LLMs) has also made it possible to
generate various post-hoc explanations that support AI predictions.11
ThispaperdiscussesthedesignofXAIsthatenablesthemtoproperlydecidewhat to or
how to explainratherthanjustthoughtlesslypresentingallpossibleexplanations.Although
having a variety of explanations could lead to multiple perspectives on AI predictions and
deepenusers’understanding,imprudentlyprovidingallavailableexplanationsdoesnotnec-
essarily contribute to better decision-making or even result in undesirable outcomes. For
example, uninterpretability of explanations,12–14 information overload,15,16 and contextual
inaccuracy17 haveharmfuleffectsonusers’cognitiveload,taskperformance,andtasktime.
Studies related to this problem include comparisons of various explanation types or
modalitiestodeterminetheireffectiveness.16,18 Thesefindingsprovideengineerswithguide-
lines for designing more appropriate and user-friendly explanations. Another approach is
developing an algorithm to automatically evaluate XAI explanations. Wiegreffe et al. pro-
pose a method of evaluating explanations generated by LLMs by predicting human ratings
of their acceptability.11
This paper specifically emphasizes the importance of the dynamics and behavioral as-
pects of human-XAI interaction. In interaction, a human evolves their understanding of a
task and AI to figure out whether the AI is trustworthy and reliable compared with them-
©CLAWAR Association Ltd
4202
nuJ
11
]CH.sc[
1v32370.6042:viXraselves.19,20 This means that what information an XAI should provide constantly changes,
and an XAI needs to be aware of these dynamics to properly communicate its reliability,
butmostpreviousstudiesfocusonlyonthestaticcharacteristicofexplanations.Automatic
evaluation of explanations is promising regarding this point because it helps XAIs dynami-
callychangewhattoexplain.However,thepreviousworkconsidersusers’perceptionsofthe
explanations, and the behavioral aspect of human-XAI collaboration, that is, how explana-
tions provided affects the performance of decision-making and how an XAI can proactively
contributetoimprovingtaskperformancebyadaptivelychangingexplanations,isneglected.
Driven by this motivation, this paper aims to deepen the discussion on our previously
proposed Nudge-XAI,21,22 which attempts to affect user decision-making by dynamically
biasing how XAI explanations are presented. Nudge-XAI algorithmically determines the
manner in which explanations are presented—deciding whether to show an explanation21
or whether to emphasize it22—using a user model that predicts user decisions for various
scenarios regarding how explanations are provided. Nudge-XAI aims to guide users to an
AI-suggested decision through biasing and without coercing them into it. The design is
inspired by libertarian paternalism,23 the idea of leading humans to make better decisions
while respecting their freedom of choice. By integrating this concept, Nudge-XAI is also
expected to reduce the risk associated with an AI making decisions without users’ full
agreement and leave room for users to avoid AI failures.
This paper presents an analysis of prior experimental findings through cluster analysis
toexplorethepotentialofNudge-XAI.Thefindingsrevealawiderangeofuserreactionsto
Nudge-XAI, aligning with our goal of improving user autonomy. Nevertheless, the analysis
alsouncoverstheissueofuserswholacktrustinAI,leadingthemtomakedecisionsagainst
the AI’s suggestions. This indicates the importance of proper personalization, paricularly,
adjusting the strength of the nudges for each user in this approach.
2. Nudge-XAI
2.1. Formalization
In this paper, Nudge-XAI refers to an XAI system that aims to strategically guide users
to a particular decision without coercing but rather indirectly influencing them by biasing
explanations.Thedesignisinspiredbylibertarianpaternalism,23 theideaofleadinghumans
to make better decisions while respecting their freedom of choice.
Our previous studies proposed two implementations of Nudge-XAI. X-Selector21 is a
method for selecting which explanations to provide, and DynEmph determines whether
to put emphasis on an explanation using a communication robot.22 They share the same
concept of predicting the effect of such biasing on user decisions and guiding users to an
AI-suggested decision by biasing.
Nudge-XAI is formalized with UserModel and π. The former is a model of a user who
makes a decision d given explanations.
u
UserModel(c,x,d)=P(d =d|c,x). (1)
u
UserModelrepresentsaprobabilitydistributionofd conditionedbyasetofexplanationsx
u
andothercontextualvariablesc={c }.ForDynEmph,letxbeasetoftuple(x ,e ),where
j i i
x isasampleofexplanation,iistheindexforit,ande isaBooleanthatrepresentswhether
i i
to emphasize x . c includes other information such as AI predictions, task status, and user
i
status.UserModelcanbeimplementedwithmachinelearninginasupervisedmannerusing
interactionlogs.Inpreviouswork,weobtainedlogsfromcrowdworkers’trialsanduseddeep
learning for implementing the model.1e6
1.100
RL
1.075 full-position
1.050
1.025
1.000
0.975
0.950
0 10 20 30 40
day
Fig.1. Performanceofπ andresultinwhichuserwasalwaystookfullposition
π infers an AI-suggested decision d .
AI
π(c,d)=P(d =d|c). (2)
AI
Theinferenceisindependentofuserdecision-making.π canbeimplementedwithreinforce-
ment learning. Figure 1 illustrates the performance of π used in our experiment compared
with the full-position result in which user was always took full position.
Nudge-XAI explores which x will minimize the expected difference between d and d
u AI
by simulating how x will affect d with UserModel. For DynEmph, the decision of whether
u
to emphasize is calculated as:
xˆ =argmin E [|UserModel(c,x,d)−π(c,d)|]. (3)
x d
IftheuserguidanceofNudge-XAIisperfect,auser’sperformanceisexpectedtomatchthe
RL result (Fig. 1).
2.2. Implementation
We evaluated X-Selector and DynEmph in a stock-trading simulator with the support of
an XAI-based decision support system. Hereafter, we explain the case of DynEmph. In the
simulation, participants were given virtual 1M JPY, checked the opening price and a price
chartforeachday,anddecidedwhethertobuy,sell,orholdtheirpositionwiththedecision
support system. Here, the support system showed its prediction of future stock prices and
explanations for the prediction. The prediction results were represented as the probability
ofbullish(over+2%),neutral(-2%–2%),andbearish(under-2%)predictions.Onthebasis
of the information, the participants input the day’s order, and the simulator immediately
transited to the next day. Let d be the amount of the position after the day’s order. This
was repeated for 45 successive virtual days.
Figure 2 shows an example of an explanation. The system has a set of natural language
explanations generated with GPT-4V24 for the three prediction labels. DynEmph decided
whether to emphasize explanations for each label. An expression of emphasis was imple-
mented with a higher voice tone, repetitive swinging of the left arm, and lighting up of the
LEDs on its eye contours.
d was inferred by a reinforcement learning model. In the simulation, d was able
AI AI
to earn 90k JPY, which was better than most of the participants’ results, whereas a user
consistently taking a full position from the beginning lost 32.5k JPY.
tnuomaFig.2. Emphasisexpressionwithcommunicationrobot
3. Data analysis
3.1. Procedure
To discuss the potential benefits and challenges of Nudge-XAI, we conducted a post-hoc
analysisonexperimentaldataforDynEmph,22 whereacommunicationrobotautomatically
selected whether to emphasize explanations.
We conducted a cluster analysis of the data to discuss user behavior in detail. Let d⃗ be
u
a vector that represents the sequence of a user u’s 45 day’s worth of decisions, that is, how
much a user bought or sold stocks. We first applied a PCAa to each u, and let us denote
the transformed result as d⃗ ′ . We chose the first four principal components for d⃗ ′ . Then,
u u
we applied a k-means clustering for d⃗ ′ , where k =4, and acquired four clusters.
u
3.2. Results
Figure 3 shows d averaged among participants for each cluster. For comparison, the black
u
dotted line shows d . The result illustrates that DynEmph allowed for the diversity for
AI
d while guiding the participants to d . We investigated the characteristics of the four
u AI
clustersandlabeledthemasAI-aligned,Delayed,Cautious,andContrarian.16,14,10,and
11 participants were assigned to them, respectively. The clusters were in ascending order of
the mean absolute differences between d and d (Figure 4). The correlation coefficients
u AI
between d and d , which is a metric of DynEmph’s successful guidance, were also in this
u AI
order (Figure 5(a)), but the order of their final performance, or the total amount of their
assets, was reversed between Delayed and Cautious (Figure 5(b)). Let us take a closer look
at each cluster in the order of their task performance.
3.2.1. AI-aligned
The AI-aligned cluster is the group of the participants whom DynEmph guided to d the
AI
best. The correlation coefficients were constantly high throughout the trial (Figure 6). As a
aPrincipalcomponentanalysis500 cluster
AI-aligned
Delayed
400 Cautious
Contrarian
d_AI
300
200
100
0
0 10 20 30 40
day
Fig.3. AveragedamountofpositionsforeachclusteranddAI.Errorbandsshowstandarderrors.
cluster
250 AI-aligned
Delayed
Cautious
200 Contrarian
150
100
50
0
Fig.4. Absoluteerrorsbetweendu anddAI
1e 5
3.0 cluster 3.5 cluster
AI-aligned AI-aligned
Delayed 3.0 Delayed
2.5 Cautious Cautious
Contrarian 2.5 Contrarian
2.0
2.0
1.5
1.5
1.0
1.0
0.5 0.5
0.0 0.0
0.6 0.4 0.2 0.0 0.2 0.4 0.6 0.8 0.94 0.96 0.98 1.00 1.02 1.04 1.06 1.08
corrcoef amount 1e6
(a) Distributionofcorrelationcoefficientsbetween (b) Distributionoffinalassets
du anddAI
Fig.5. Distributionofcorrelationcoefficientsandfinalassets
result, the participants in this cluster earned more than those in the other clusters (Figure
5(b)).TheseweresuccessfulexamplesofDynEmphguidinguserstobetterdecisionswithout
tnuoma
ytisneD
rorre_sba
ytisneD0.8
0.6
0.4
0.2
0.0
cluster
0.2 AI-aligned
Delayed
0.4 Cautious
Contrarian
0 5 10 15 20 25
day
Fig.6. Movingaverageofcorrelationcoefficientswithwindowsize20
coercion.
3.2.2. Cautious
The Cautious participants were characterized by trading the least amount of stocks among
the clusters (Figure 3). They traded in spots and did not necessarily buy stocks when d
AI
was high, which decreased the correlation coefficients, but they achieved the second-highest
profits among the clusters. This is a positive case of Nudge-XAI embracing the autonomy
of users.
3.2.3. Delayed
AlthoughDelayedshowedthesecondlargestcorrelationcoefficient,suggestingtheinfluence
of DynEmph’s guidance, this did not improve the performance (Figure 5(b)). The perfor-
mance was inferior to Cautious despite the higher correlation. Let us compare their trades
with d (Figure 3). The Delayed participants were in moderate accordance with d but
AI AI
failedtoimmediatelyfollowthesteepdecreaseofd (days10–12,16–18),resultinginlosing
AI
their assets. This implies that the strength of the nudge was not enough for them, which is
a challenge for Nudge-XAI when a user needs to make an immediate decision.
3.2.4. Contrarian
Contrarianwastheworstexampleinthistrial.Comparedwiththecorrelationcoefficientsof
the other clusters, those of Contrarian was constantly low (Figure 6). In the first half of the
trial, the values were negative, meaning that the participants in this cluster instead made
decisions contrary to d . While the existence of this cluster supports the aim of embracing
AI
users’ freedom of choice with Nudge-XAI, this did not result in better performance in this
trial (Figure 5(b)). Therefore, when viewed in hindsight, the participants in this cluster are
considered to have had an under-trust, that is, they underestimated the performance of the
decision support system and could not make use of it.
4. Discussion
4.1. Did Nudge-XAI embrace freedom of choice?
Thediversityofd amongtheclusters(Figure3)demonstratesthatcollectively,Nudge-XAI
u
did not coerce the participants into d and successfully embraced the freedom of choice
AI
rrocwhilebiasinghowexplanationwasprovided.Inaddition,Figure6showsmoderateincreases
in the correlation coefficients, particularly for Contrarian, indicating that the participants
spontaneously increased their reliance on the decision support system in the interaction.
Therefore,wecanconcludethatNudge-XAIembracedfreedomofchoice.However,presum-
ably, because the performance of d was high enough in this trial, we did not observe a
AI
massive decrease of reliance, so further experiments such as artificially decreasing the reli-
ability of d are required to investigate whether the design of Nudge-XAI can allow users
AI
to avoid AI failure by reserving freedom of choice.
4.2. Did Nudge-XAI contribute to improving task performance?
The effects differed among the clusters. AI-aligned got the largest benefit from Nudge-XAI
by obediently following the nudges, meaning that Nudge-XAI was effective for these users
under the condition of the better performance of d . The weak but positive correlation
AI
coefficients of Cautious suggest a certain influence of explanation biasing, and at least, we
can say that Nudge-XAI was not harmful to the decisions of those users. Nudge-XAI could
not contribute to improving the task performance of Delayed and Contrarian as a result. In
particular, Contrarian’s negative performance and negative correlation coefficients between
d andd indicateariskofNudge-XAIbeingpoorlytrustedandfalselyguidingsomeusers
u AI
to decisions contrary to AI suggestions.
4.3. Should XAI nudge human decisions with explanation biasing?
TheresultssuggestedseveralbenefitsofanXAIthatnudgeshumandecisionswithexplana-
tion biasing. Nudge-XAI actually contributed to improving the task performance of a part
ofusers(AI-aligned)byguidingthemtoAI-suggesteddecisions.Userscanalsomakepartial
use of Nudge-XAI by exercising their freedom of choice (Cautious). The diversity of d im-
u
plied the potential of enabling users to avoid AI failures as a form of collective intelligence.
The design is expected to reduce the risk associated with an AI making decisions without
users’ full agreement.
However, freedom of choice also caused a challenge for some users failing to make an
immediate decision when necessary. In addition, some users falsely made decisions contrary
to AI suggestions, resulting in low performance. These challenges need to be overcome to
fullyobtainthebenefitsofNudge-XAI.Asolutioncanbetoadjustthestrengthofthenudge
depending on the characteristics of the users. Nudge-XAI may be able to contribute to the
Delayed participants by strengthening the nudge depending on the urgency of decision-
making, and on the other hand, weakening the nudge may work for Contrarian so as not
to lead to false guidance. This paper focused on the sole effect of explanation biasing,
but Nudge-XAI could also integrate other communication modalities to repair trust for
Contrarians and give a stronger message to Delayed.
5. Conclusion
Thispaperconductedapost-hocanalysisofexperimentaldataforNudge-XAI,anapproach
inwhichXAIexplanationsareautomaticallybiasedtoleaduserstobetterdecisions,todis-
cuss its benefits and challenges. The results suggested that our implementation of Nudge-
XAI successfully enhanced the autonomy of user decision-making. The guidance was bene-
ficialforAI-aligneduserswhoobedientlyfollowedthenudges,butwealsofoundsomecases
in which Nudge-XAI could not trigger users to make immediate decisions. We also found
cases in which Nudge-XAI falsely guided users to decisions contrary to AI-suggested ones
because of their under-trust in the AI. This indicates the importance of properly adjusting
the strength of nudges depending on the characteristics of users.Acknowledgments
This work was supported in part by JST CREST Grant Number JPMJCR21D4 and JSPS
KAKENHI Grant Number JP24K20846.
References
1. R.ParasuramanandV.Riley,“Humansandautomation:Use,misuse,disuse,abuse,”Human
Factors, vol. 39, no. 2, pp. 230–253, 1997.
2. A. Adadi and M. Berrada, “Peeking inside the black-box: A survey on explainable artificial
intelligence (xai),” IEEE Access, vol. 6, pp. 52138–52160, 2018.
3. M. H. Lee and C. J. Chew, “Understanding the effect of counterfactual explanations on trust
and reliance on ai for human-ai collaborative clinical decision making,” Proc. ACM Hum.-
Comput. Interact., vol. 7, no. CSCW2, oct 2023.
4. D. P. Panagoulias, E. Sarmas, V. Marinakis, M. Virvou, G. A. Tsihrintzis, and H. Doukas,
“Intelligentdecisionsupportforenergymanagement:Amethodologyfortailoredexplainability
of artificial intelligence analytics,” Electronics, vol. 12, no. 21, 2023.
5. D. Das, B. Kim, and S. Chernova, “Subgoal-based explanations for unreliable intelligent deci-
sion support systems,” in Proc. of the 28th Intl. Conf. on IUI. ACM, 2023, p. 240–250.
6. Q.-s. Zhang and S.-C. Zhu, “Visual interpretability for deep learning: a survey,” Frontiers of
Information Technology & Electronic Engineering, vol. 19, no. 1, pp. 27–39, 2018.
7. B.Alsallakh,A.Hanbury,H.Hauser,S.Miksch,andA.Rauber,“Visualmethodsforanalyzing
probabilistic classification data,” IEEE TVCG, vol. 20, no. 12, pp. 1703–1712, 2014.
8. W.Samek,A.Binder,G.Montavon,S.Lapuschkin,andK.-R.Mu¨ller,“Evaluatingthevisual-
izationofwhatadeepneuralnetworkhaslearned,”IEEETNLS,vol.28,no.11,pp.2660–2673,
2016.
9. J.vanderWaa,E.Nieuwburg,A.Cremers,andM.Neerincx,“Evaluatingxai:Acomparisonof
rule-based and example-based explanations,” Artificial Intelligence, vol. 291, p. 103404, 2021.
10. S. Galhotra, R. Pradhan, and B. Salimi, “Explaining black-box algorithms using probabilistic
contrastive counterfactuals,” in Proc. of SIGMOD 2021. New York, NY, USA: ACM, 2021,
p. 577–590.
11. S.Wiegreffe,J.Hessel,S.Swayamdipta,M.Riedl,andY.Choi,“Reframinghuman-AIcollab-
oration for generating free-text explanations,” in Proc. of the 2022 Conf. of NAACL. ACL,
July 2022, pp. 632–658.
12. A. Maehigashi, Y. Fukuchi, and S. Yamada, “Modeling reliance on xai indicating its purpose
and attention,” in Proc. Annu. Meet. CogSci, vol. 45, 2023, pp. 1929–1936.
13. ——, “Empirical investigation of how robot’s pointing gesture influences trust in and accep-
tance of heatmap-based xai,” in 2023 32nd IEEE Intl. Conf. RO-MAN, 2023, pp. 2134–2139.
14. ——, “Experimental investigation of human acceptance of ai suggestions with heatmap and
pointing-based xai,” in Proc. of the 11th Intl. Conf. on HAI. ACM, 2023, p. 291–298.
15. A. N. Ferguson, M. Franklin, and D. Lagnado, “Explanations that backfire: Explainable ar-
tificial intelligence can cause information overload,” in Proc. Annu. Meet. of CogSci, vol. 44,
no. 44, 2022.
16. L.-V.Herm,“Impactofexplainableaioncognitiveload:Insightsfromanempiricalstudy,”in
31st Euro. Conf. Info. Syst., 2023, 269.
17. U.Ehsan,P.Tambwekar,L.Chan,B.Harrison,andM.O.Riedl,“Automatedrationalegener-
ation: A technique for explainable ai and its effects on human perceptions,” in Proc. 24th Int.
Conf. IUI, 2019, p. 263–274.
18. M. Naiseh, D. Al-Thani, N. Jiang, and R. Ali, “How the different explanation classes
impact trust calibration: The case of clinical decision support systems,” International
Journal of Human-Computer Studies, vol. 169, p. 102941, 2023. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S1071581922001616
19. K. Okamura and S. Yamada, “Adaptive trust calibration for human-ai collaboration,” PLOS
ONE, vol. 15, no. 2, pp. 1–20, 02 2020. [Online]. Available: https://doi.org/10.1371/journal.
pone.0229132
20. ——, “Empirical evaluations of framework for adaptive trust calibration in human-ai cooper-
ation,” IEEE Access, vol. 8, pp. 220335–220351, 2020.
21. Y. Fukuchi and S. Yamada, “User decision guidance with selective explanation presentation
from explainable-ai,” in The 2024 33rd IEEE International Conference on Robot and HumanInteractive Communication, 2024, (accepted).
22. ——, “Dynamic explanation emphasis in human-xai interaction with communication robot,”
2024. [Online]. Available: https://doi.org/10.48550/arXiv.2403.14550
23. C.R.Sunstein,Why Nudge?: The Politics of Libertarian Paternalism. YaleUniversityPress,
2014.
24. OpenAI, “Gpt-4 technical report,” 2023.