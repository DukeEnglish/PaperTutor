Blur-aware Spatio-temporal Sparse Transformer for Video Deblurring
HuicongZhang1,HaozheXie2,HongxunYao1 (cid:0)
1 HarbinInstituteofTechnology 2 S-Lab,NanyangTechnologicalUniversity
https://vilab.hit.edu.cn/projects/bsstnet
Opticalflows K/V/Q
Blurframe Blurmap
‚Ä¶ ‚Ä¶
‚Ä¶ ‚Ä¶
(a)Blur Map Generation (c)StandardSpatio-temporalTransformer (e)Standard Flow-guided Feature Alignment
400 Std. Transformer
350 Ours
300 K/V ‚Ä¶ 250
200 Top 50%
150
‚Ä¶
100 ‚Ä¶ Btm. 50%
12 24 36 48 60 Q
‚Ä¶
Temporal Window Length
(b)FLOPs Comparison (d)Blur-awareSpatio-temporalSparseTransformer (f)Blur-awareFeature Alignment
Input VRT RVRT Shift-Net+ BSSTNet(Ours) GT
(g) Visual Comparisons on the GoPro dataset
Figure1.(a)Largemotionsinopticalflowsarehighlightedinblurmaps.(b)ComparisonofFLOPsbetweenthestandardspatio-temporal
transformerandtheblur-awarespatio-temporaltransformer.(c-d)Summaryofthestandardspatio-temporaltransformerandtheblur-aware
spatio-temporaltransformer. (e-f)Summaryofthestandardflow-guidedfeaturealignmentandblur-awarefeaturealignment. (g)Inthe
visualcomparisonsontheGoProdataset,theproposedBSSTNetrestoresthesharpestframe.
Abstract tion during the propagation process. To address these is-
sues, we propose BSSTNet, Blur-aware Spatio-temporal
Video deblurring relies on leveraging information from Sparse Transformer Network. It introduces the blur map,
otherframesinthevideosequencetorestoretheblurredre- whichconvertstheoriginallydenseattentionintoasparse
gionsinthecurrentframe. Mainstreamapproachesemploy form, enabling a more extensive utilization of information
bidirectional feature propagation, spatio-temporal trans- throughouttheentirevideosequence.Specifically,BSSTNet
formers, or a combination of both to extract information (1)usesalongertemporalwindowinthetransformer,lever-
from the video sequence. However, limitations in mem- aging information from more distant frames to restore the
ory and computational resources constraints the temporal blurry pixels in the current frame. (2) introduces bidirec-
windowlengthofthespatio-temporaltransformer,prevent- tional feature propagation guided by blur maps, which re-
ing the extraction of longer temporal contextual informa- duceserroraccumulationcausedbytheblurframe.Theex-
tion from the video sequence. Additionally, bidirectional perimentalresultsdemonstratetheproposedBSSTNetout-
feature propagation is highly sensitive to inaccurate op- performs the state-of-the-art methods on the GoPro and
tical flow in blurry frames, leading to error accumula- DVDdatasets.
(cid:0)Correspondingauthor:h.yao@hit.edu.cn
4202
nuJ
11
]VC.sc[
1v15570.6042:viXra
sPOLF
Ordered
by
blur
levels
Attention
Attention
DCN
DCN
Flow-guided
Flow-guided1.Introduction formationfromtheadjacentframes.
Thecontributionsaresummarizedasfollows.
Video deblurring aims to recover clear videos from blurry
‚Ä¢ We propose a non-learnable, parameter-free method for
inputs, and it finds wide applications in many subsequent
estimating the blur map of video frames. The blur map
vision tasks, including tracking [5, 16], video stabiliza-
provides crucial prior information on motion-blurry re-
tion [15], and SLAM [8]. Therefore, it is of great interest
gions in the video, enabling sparsity in the transformer
todevelopaneffectivealgorithmtodeblurvideosforabove
anderrorcorrectionduringbidirectionalpropagation.
mentionedhigh-levelvisiontasks.
‚Ä¢ We propose BSSTNet, comprising two major compo-
Video deblurring presents a significant challenge, as it
nents: BSST and BBFP. BSST incorporates spatio-
necessitates the extraction of pertinent information from
temporalsparseattentiontoleveragedistantinformation
otherframeswithinthevideosequencetorestoretheblurry
in the video sequence while still achieving high perfor-
frame. In recent years, there have been noteworthy ad-
mance. BBFP corrects errors in the propagation process
vancements [1, 4, 11‚Äì13, 24] in addressing this challenge.
and boosts its capability to aggregate information from
Flow-guided bidirectional propagation methods [1, 4, 12,
thevideosequence.
13, 24] employ flow-guided deformable convolution and
‚Ä¢ WequantitativelyandqualitativelyevaluateBSSTNeton
flow-guidedattentionforfeaturealignment.However,inac-
the DVD and GoPro datasets. The experimental results
curateopticalflowinblurryframescausestheintroduction
indicate that BSSTNet performs favorably against state-
ofblurrypixelsduringbidirectionalpropagation. VRTand
of-the-artmethods.
RVRT[11,12]usespatio-temporalself-attentionwithtem-
poralwindowtofusetheinformationfromvideosequence.
2.RelatedWork
Due to the high memory demand of self-attention, these
approachesfrequentlyfeaturerestrictedtemporalwindows, Many methods in video deblurring have achieved impres-
limitingtheirabilitytoincorporateinformationfromdistant siveperformances. Thevideodeblurmethodscanbecate-
sectionsofthevideo. gorizedintotwocategories:
Analyzingvideosafflictedbymotionblurrevealsacor- RNN-based Methods. On the other hand, some re-
respondencebetweentheblurryregionsinthevideoandar- searchers [6, 18, 20, 23, 25, 26] are focusing on the
easwithpixeldisplacement,wherethedegreeofblurriness RNN-base methods. STRCNN [6] adopts a recurrent neu-
isdirectlyassociatedwiththemagnitudeofpixeldisplace- ral network to fuse the concatenation of multi-frame fea-
ment. Moreover, The blurry regions are typically less fre- tures.RDN[23]developsarecurrentnetworktorecurrently
quentinboththetemporalandspatialaspectsoftheblurry use features from the previous frame at multiple scales.
videos. By leveraging the sparsity of blurry regions, the IFRNN [18] adopts an iterative recurrent neural network
computation of the spatio-temporal transformer can focus (RNN) for video deblurring. STFAN [26] uses dynamic
solely on these areas, thereby extending the temporal win- filters to align consecutive frames. PVDNet [20] contains
dow to encompass longer video clips. Moreover, bidirec- a pre-trained blur-invariant flow estimator and a pixel vol-
tional feature propagation based on blurry regions enables ume module. To aggregate video frame information, ES-
the minimization of error accumulation. As shown in Fig- TRNN [25] employs a GSA module in the recurrent net-
ure 1a, the green box area represents the blurry region in work. Recently,theBiRNN-basedmethod[1,4,13,24,28]
theframe.Similarly,boththeforwardandbackwardoptical has achieved impressive deblur results through aggressive
flowsinthesamelocationarealsomaximized,indicatinga bidirectionalpropagation. BasicVSR++[1]adoptsaggres-
correlationbetweenthemotionandblurryregions. sive bidirectional propagation. Based on BasicVSR++,
By introducing blur maps, we propose BSSTNet, Blur- RNN-MBP[28]introducesthemulti-scalebidirectionalre-
awareSpatio-temporalSparseTransformerNetwork.Com- currentneuralnetworkforvideodeblurring.STDANet[24]
pared to methods based on spatio-temporal transformer, and FGST [13] employ the flow-guided attention to align
BSSTNet introduces Blur-aware Spatio-temporal Sparse andfusetheinformationofadjacentframes. However,due
Transformer (BSST) and Blur-aware Bidirectional Feature toerroraccumulation,thesemethodsdonoteffectivelyfuse
Propagation (BBFP). The proposed BSST efficiently uti- theinformationfromlong-termframes. JiandYao[4]de-
lizesalongtemporalwindowbyapplyingsparsityoninput velop a Memory-Based network, which contains a multi-
tokens in the spatio-temporal domain based on blur maps. scalebidirectionalrecurrentneuralnetworkandamemory
Thisenablestheincorporationofdistantinformationinthe branch. However, the memory branch introduces a large
video sequence while still maintaining computational ef- searchspaceofglobalattentionandineffectivealignment.
ficiency. BBFP introduces guidance from blur maps and Transformer-basedMethods. TheSpatio-temporaltrans-
checksforflowconsistencybeforehand. Thisaidsinmini- former is widely used in video deblurring [11, 12].
mizingtheintroductionofblurrypixelsduringbidirectional VRT[11]utilizesspatio-temporalself-attentionmechanism
propagation, ultimately enhancing the ability to gather in- to integrate information across video frames. Due to theBBFP (Sec 3.3) ùëÅ√ó BSST (Sec 3.4)
‚Ä¶ BFA BFA ‚Ä¶
Encoder ‚Ä¶ BFA BFA ‚Ä¶ ‚Ä¶ ‚Ä¶ Decoder
‚Ä¶ BFA BFA ‚Ä¶
Image Features Blur Maps
Flow
Estimator Forward Flow Backward Flow
Fwd. Flows Back. Flows Blur Maps
Skip Connections
Input Frames Blur Map Estimator (Sec 3.2) Output Frames
Figure2.OverviewoftheproposedBSSTNet.BSSTNetconsistsofthreemajorcomponents:BlurMapEstimation,Blur-awareBidirec-
tionalFeaturePropagation(BBFP),andBlur-awareSpatio-temporalSparseTransformer(BSST).
computationalcomplexityofself-attention,VRTemploysa 3.2.BlurMapEstimation
2-frametemporalwindowsizeandutilizesashiftedwindow
Giventheopticalflows{O }T‚àí1 and{O }T‚àí1,
mechanism for cross-window connections. However, the t+1‚Üít t=1 t‚Üít+1 t=1
theunnormalizedblurmapsBÀÜ= {BÀÜ }T canbeobtained
indirectconnectionapproachwithasmallwindowsizefails t t=1
asfollows
tofullyexploitlong-rangeinformationwithinthevideose-
quence. RVRT [12] divides the video sequence into 2-
2
frameclips,employingsmall-windowspatio-temporalself- BÀÜ =(cid:88) ((O )2+(O )2) (1)
t t‚Üít+1 i t‚Üít‚àí1 i
attention within each clip and Flow-guided biderectional
i=1
propagation and alignment between clips. However, due
to the small window constraint of spatio-temporal self- Specially, we define O 1‚Üí0 = 0 and O T‚ÜíT+1 = 0. The
attention and the error accumulation caused by the optical blur map B and sharp map A can be generated as follows
flowofblurredframesinFlow-guidedbiderectionalpropa-
gation,RVRTstillfallsshortoffullyutilizingtheinforma-
BÀÜ ‚àímin(BÀÜ)
tionfromtheentirevideosequence. B = t
t max(BÀÜ)‚àímin(BÀÜ)
A =1‚àíB (2)
t t
3.OurApproach
whereiandtindexthechannelofopticalflowsandthetime
3.1.Overview steps,respectively.
3.3.Blur-awareBidirectionalFeaturePropagation
As shown in Figure 2, the BSSTNet contains three key
components: Blur Map Estimation, Blur-aware Bidirec- WithinBBFP,bidirectionalfeaturepropagationpropagates
tionalFeaturePropagation(BBFP),andBlur-awareSpatio- the aggregated features FÀÜ in both the forward and back-
temporal Sparse Transformer (BSST). First, the forward ward directions, incorporating Blur-aware Feature Align-
andbackwardopticalflows,denotedas{O t+1‚Üít} tT =‚àí 11 and ment(BFA).BFAisdesignedtoalignfeaturesfromneigh-
{O t‚Üít+1}T t=‚àí 11,areestimatedfromthedownsampledvideo boring frames to reconstruct the current frame. As shown
sequenceXÀÜ = {XÀÜ }T . Then,BlurMapEstimationgen- in Figure 1e and Figure 1f, the standard flow-guided fea-
t t=1
eratestheblurmapsB = {B }T foreachframearegen- ture alignment aligns all pixels in the neighboring frames,
t t=1
erated based on {O }T‚àí1 and {O }T‚àí1. Next, whereasBFAselectivelyintegratesinformationfromsharp
t+1‚Üít t=1 t‚Üít+1 t=1
BBFPproducestheaggregatedfeaturesFÀÜ usingBlur-aware pixelsguidedbyblurmaps. Thispreventsthepropagation
Feature Alignment (BFA). After that, BSST generates the of blurry regions from the features of neighboring frames
refined features F from FÀÜ with the Blur-aware Sparse duringbidirectionalfeaturepropagation.
Spatio-temporal Attention (BSSA) layers. Finally, the de- BidirectionalFeaturePropagation. Assumingthecurrent
coderreconstructsthesharpvideosequenceR={R }T . timestepisthet-thstep,andthecorrespondingpropagation
t t=1
‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶
TSSB TSSB
‚Ä¶
‚Ä¶ùêé !‚Üí!#% ùêÄ !#% DCN P P
ùêÖ!$ W W(ùêÖ($ ) Conditions
!"% !"#
ùêé ùêÄ Blur Maps ùìë ùêî ùêÑ$, ùêÑùíå, ùêÑùíó Features ùìï"
!‚Üí!#$ !#$
ùêÖ!$ W W(ùêÖ($ ) ùêÖ!$"# ùêî ùêÑ$ ùêî ùêÑùíå, ùêÑùíó
!"# !"% !
Spatial SparseinQuerySpace Spatial SparseinKey/ValueSpace
C C C C
ùêîùêÑ# ùêîùêÑ!ùêÑ" ùê†!,ùê†"
Deformable Conv. Layers Conv. Layers ùêà$ ùêàùíå, ùêàùíó,ùê†ùíå, ùê†ùíó
Temporal SparseinQuerySpace Temporal Sparse in Key/Value Space
‚Ä¶ ‚Ä¶
ùêÖ!$ Conv. Layers C DCN Offsets DCN Masks ! ‚Ä¶ ‚Ä¶
Order by Blur Levels OrderbyBlurLevels
Figure3.ThedetailsofBFA.Notethat‚ÉùW,‚ÉùC,and(cid:76)
denotesthe Select Top 50% Flatten by Window Select Bottom 50% Flatten by Window
‚ÄúWarp‚Äù, ‚ÄúConcatenation‚Äù, and ‚ÄúElement-wise Add‚Äù operations, Query Tokens K/V Tokens
respectively. ‚Ä¶ ‚Ä¶ ‚Ä¶
ùêò% # Multi-head Self Attention ùêò% !, ùêò% "
branchisthej-thbranch,thegenerationofthecurrenttime
Figure4.ThedetailsofBSST.Notethat‚ÉùP denotesthe‚ÄúWindow
stepaggregatedfeatureFÀÜj canbeobtainedas
t Partition‚Äùoperation. ‚ÄúFlattenbywindow‚Äùindicatesthatqueryto-
kensareflattenedforeachquerywindow,andK/Vtokensaregen-
eratedinasimilarmanner.Multi-headSelfAttentionisalsocom-
FÀÜj t =BFA(FÀÜj t‚àí1,FÀÜj t‚àí1,FÀÜj t‚àí2, putedonthequeryandK/Vtokensgeneratedforeachwindow.
W(FÀÜj ,O ),W(FÀÜj ,O ),
t‚àí1 t‚Üít‚àí1 t‚àí2 t‚Üít‚àí2
3.4. Blur-aware Spatio-temporal Sparse Trans-
O ,O ,A ,A ) (3)
t‚Üít‚àí1 t‚Üít‚àí2 t‚àí1 t‚àí2 former
The spatio-temporal attention is commonly employed in
whereBFAandWdenotethe‚ÄúBFA‚Äùand‚ÄúBackwardWarp‚Äù
video deblurring and demonstrates remarkable perfor-
operations, resepectively. FÀÜj‚àí1 represents the feature ag-
t mance, as shown in Figure 1c. However, the standard
gregated from the t-th time step in the (j ‚àí1)-th branch.
spatio-temporalattentionmethodoftenrestrictsitstemporal
FÀÜj t‚àí1 and FÀÜj t‚àí2 are the features generated from the previ- windowsizeduetocomputationalcomplexity,therebycon-
ousandthesecondprevioustimestep. Theaforementioned straining its capability to capture information from distant
process progresses forward through the time steps until it parts of the video sequence. To overcome this limitation,
reachest = T. Thebackwardpropagationprocessmirrors weintroducetheBlur-awareSpatio-temporalSparseTrans-
theforwardpropagationprocess. former(BSST).AsillustratedinFigure1d,BSSTfiltersout
unnecessaryandredundanttokensinthespatioandtempo-
Blur-aware Feature Alignment. Different from the stan-
ral domain according to blur maps B. As shown in Fig-
dard flow-guided feature alignment [1] that aligns all pix-
ure1b,allowingBSSTtoincludealargertemporalwindow
els in neighboring frames, BFA introduces sharp maps to
while maintaining computational efficiency. The detailed
preventtheintroductionofblurrypixelsintheneighboring
frames. AsillustratedinFigure3,alongwithfeaturesFÀÜj implementationofBSSTisillustratedinFigure4.
and FÀÜj from previous time steps, the correspondingt o‚àí p1 - Given the aggregated features FÀÜ = {FÀÜ t ‚àà
t‚àí2 RH/4√óW/4√óC}T fromthelastbranchofBBFP,
tical flows O and O , and the warped features t=1
t‚Üít‚àí1 t‚Üít‚àí2
W(FÀÜj ) and W(FÀÜj ), sharp maps A and A are weemployasoftsplitoperation[14]todivideeachag-
t‚àí1 t‚àí2 t‚àí1 t‚àí2
additionally introduced. These sharp maps serve as addi- gregatedfeatureintooverlappingpatchesofsizep√ópwith
tionalconditionstogeneratetheoffsetsandmasksofthede- astrideofs. Thesplitfeaturesarethenconcatenated,gen-
formable convolution layers [3]. Moreover, the sharp map erating the patch embeddings z ‚àà RT√óM√óN√óp2C. Next,
acts as a base mask for DCN by being added to the DCN theblurmapBaredownsampledbyaveragepoolingwith
mask. This ensures that only sharp regions of features are a kernel size of p√óp and a stride of s, resulting in B‚Üì ‚àà
propagated. RT√óM√óN√óp2C.Forsimplicity,p2CisdenotedasC .After
z
‚Ä¶ ‚Ä¶
serutaeF
lacoL
serutaeF
labolG
‚Ä¶that, zisfedtothreeseparatelinearlayertransformations, where Concat denotes the ‚ÄúConcatenation‚Äù operation. If
resultinginzÀúq ‚àà RT√óM√óN√óCz, zÀúk ‚àà RT√óM√óN√óCz, and S
i,j
= 0, it indicates that the window‚Äôs position indexed
zÀúv ‚ààRT√óM√óN√óCz,whereM,N,andC
z
respectivelyde- by (i,j) in the video sequence does not encompass blurry
notethenumberofpatchesintheheightandwidthdomains, tokens. This allows us to exclude the tokens within those
and the number of channels. Subsequently, zÀúq,zÀúk,zÀúv are windows from the spatio-temporal attention mechanism.
partitionedintom√ónnon-overlappingwindows,generat- Iq ‚àà RT√ómsns√óhw√óCz, while both Ik and Iv share the
ingpartitionedfeaturesEq,Gk,Gv ‚àà RT√óm√ón√óh√ów√óCz, sizeofRT√ómsns√ó(h+hp)(w+wp)√óCz,wherem sandn srep-
wherem√ónandh√ówarethenumberandsizeofthewin- resenting the number of selected windows in m and n do-
dows, respectively. Utilizing the embedding z and incor- mains,respectively.
porating depth-wise convolution, the generation of pooled TemporalSparseinQuerySpace.Alongthetemporaldo-
globaltokensgk andgv takesplaceasfollows main,wechoosethewindowsoftheblurryregionforquery
space, ensuring that the spatio-temporal attention mecha-
gk =l (DC(z))
k nismisdedicatedtorestoringonlytheblurryregionsofthe
gv =l (DC(z)) (4) video sequence. Given the spatial sparse embedding fea-
v
turesIq,thespatio-temporalsparseembeddingyq isgener-
whereDCrepresentsdepth-wiseconvolution,andgk,gv ‚àà atedasfollows
RT√óhp√ówp√óCz. Following that, we repeat and concate-
nate gk with Gk and gv with Gv, resulting in Ek,Ev ‚àà Hq ={Iq |U ‚â•Top(Kq,U ),
t,i,j t,i,j i,j
RT√óm√ón√ó(h+hp)√ó(w+wp)√óCz. Note that for the key/value i‚àà[1,m ],j ‚àà[1,n ]}T
windows,weenlargeitswindowsizetoenhancetherecep- s s t=1
Yq =Concat(Hq) (7)
tivefieldofkey/value[14,27]. Forsimplicity,weignoreit
inthefollowingdiscussion.
whereYq ‚ààRKq√ómsns√óhw√óCz. Top(Kq,¬∑)representsthe
Spatial Sparse in Query/Key/Value Spaces. We observe
operation of finding the K -th largest element in a vector.
that the blurry regions are typically less frequent in both q
Foreachwindowlocatedatposition(i,j)inIq,withinthe
thetemporalandspatialaspectsoftheblurredvideos. Mo-
temporaldomain,weselectivelychosethetopK windows
tivated by this observation, we only choose the tokens of q
withthehighestblurlevelsfordeblurring.
blurry windows in Eq and tokens of sharp windows in
Temporal Sparse in Key/Value Spaces. In contrast to
Ek,Ev toparticipateinthecomputationofspatio-temporal
the query space, we select the sharp regions in I ,I for
attention. This ensures that the spatio-temporal attention k v
key/value spaces. Due to the high similarity in textures
mechanism focuses solely on restoring the blurred regions
between adjacent frames, we alternately choose temporal
bytheutilizationofsharpregionsinvideosequences. First,
frames with a stride of 2 in each BSST. In BSSTNet, con-
theblurmapsofwindowsU ‚àà RT√óm√ón aregeneratedby
sisting of multiple BSSTs, odd-numbered BSSTs select
downsampling B‚Üì ‚àà RT√óM√óN using max pooling. Next
frames with odd numbers, while even-numbered BSSTs
,thespatialsparsemaskofwindowsisobtainedasfollows
choose frames with even numbers, resulting in a 50% re-
ductioninthesizeofthekey/valuespace. Giventhespatial
Ô£± 1, ifU ‚â•Œ∏, sparse embedding features Ik and Iv, the spatio-temporal
Ô£¥Ô£≤ t,i,j
sparseembeddingfeaturesyk andyv aregeneratedasfol-
Q = ‚àÄt‚àà[1,T],i‚àà[1,m],j ‚àà[1,n]
t,i,j
Ô£¥Ô£≥0,
otherwise
lows
S=Clip(cid:16)(cid:88)T
Q ,
1(cid:17)
(5)
Hk ={Ik
t,i,j
|U
t,i,j
‚â•Top(K kv,1‚àíU i,j),
t
t=1 t mod 2=0,i‚àà[1,m ],j ‚àà[1,n ]}T
s s t=1
whereŒ∏,Clip,S ‚àà Rm√ón arethethresholdforconsidering yk =Concat(Hk)
relatedwindowsasblurrywindows,aclippingfunctionthat
Hv ={Iv |U ‚â•Top(K ,1‚àíU ),
set S to 1 if (cid:80)T Q > 0, and the spatial sparse mask t,i,j t,i,j kv i,j
for Eq, Ek andt E= v1 , ret spectively. Then, the spatial sparse t mod 2=0,i‚àà[1,m s],j ‚àà[1,n s]}T t=1
embedding features Iq, Ik, and Iv are generated using the yv =Concat(Hv) (8)
followingequations
whereyk,yv ‚ààRKkv√ómsns√ó(h+hp)(w+wp)√óCz.
Iq =Concat({Eq t,i,j |S i,j =1,i‚àà[1,m],j ‚àà[1,n]}T t=1) Spatio-temporal Sparse Attention. The spatio-temporal
Ik =Concat({Ek t,i,j |S i,j =1,i‚àà[1,m],j ‚àà[1,n]}T t=1) Rsp mar ss ne s√óq Ku qe hr wy √óCem z.be Sd id min ig larly yq
,
Ti hs er se ps ah ta ip oe -td emi pn oto ralYÀÜ sq pars‚àà
e
Iv =Concat({Ev |S =1,i‚àà[1,m],j ‚àà[1,n]}T )
t,i,j i,j t=1 key/value embedding yk and yv are each reshaped
(6) into YÀÜ
k
‚àà Rmsns√óKkv(h+hp)(w+wp)√óCz and YÀÜ
v
‚ààTable1.QuantitativecomparisonsontheGoProdataset.Thebestresultsarehighlightedinbold.
Method STFAN[26] STDAN[24] RNN-MBP[28] NAFNet[2] VRT[11] RVRT[12] Shift-Net+[10] BSSTNet
PSNR 28.69 32.62 33.32 33.69 34.81 34.92 35.88 35.98
SSIM 0.8610 0.9375 0.9627 0.9670 0.9724 0.9738 0.9790 0.9792
Table2.QuantitativecomparisonsontheDVDdataset.Thebestresultsarehighlightedinbold.
Method STFAN[26] ARVo[9] RNN-MBP[28] STDAN[24] VRT[11] RVRT[12] Shift-Net+[10] BSSTNet
PSNR 31.24 32.80 32.49 33.05 34.27 34.30 34.69 34.95
SSIM 0.9340 0.9352 0.9568 0.9374 0.9651 0.9655 0.9690 0.9703
Rmsns√óKkv(h+hp)(w+wp)√óCz, respectively. For each win- Table3. ThecomparisonofFLOPsandruntimeontheDVD
dow in m n , the self-attention is calculated as follows: dataset. Thetoptworesultsaremarkedinboldandunderlined.
s s
NotethatFLOPsandruntimearecomputedforasingleframewith
aresolutionof256√ó256.
(cid:32) (cid:33)
YÀÜ YÀÜT
Attention(YÀÜ ,YÀÜ ,YÀÜ )=Softmax ‚àöq k YÀÜ (9)
q k v v Method RVRT[12] Shift-Net+[10] BSSTNet
C
z
PSNR 34.30 34.69 34.95
In BSST, the multi-head self-attention is introduced to ob- SSIM 0.9655 0.9690 0.9703
taintheoutputembeddingz
s
‚ààRms√óns√óKqhw√óCz.
GFLOPs 88.8 146 133
Runtime(ms) 23 45 28
z =MSA(YÀÜ ,YÀÜ ,YÀÜ ) (10)
s q k v
where MSA is the ‚ÄúMulti head Self-Attention‚Äù function. to4√ó10‚àí4. ThenetworkisoptimizedwithL1lossusing
Afterapplyingoursparsestrategytoeliminateunnecessary Adamoptimizer[7],whereŒ≤ 1 = 0.9andŒ≤ 2 = 0.999. The
and redundant windows, we use self-attention following flow estimator in BSSTNet uses pre-trained weights from
Eq. 9 on the remaining windows to extract fused features. the official RAFT [22] release and remains fixed during
Specially,standardwindowspatio-temporalattentionisap- training. Duringtesting,T,K q,andK kv aresetto48,24,
pliedtounselected(lessblurry)windows,allowingfeatures and 24, respectively. During training, they are 24,12, and
toberestoredtotheiroriginalsize.Subsequently,thesefea- 12,respectively.Inthetrainingphase,inputimagesareran-
turesaregatheredthroughasoftcompositionoperation[14] domlycroppedintopatcheswithresolutionsof256√ó256,
to serve as the input for the next BSST. The output of the alongwiththeapplicationofrandomflippingandrotation.
finalBSSTisdenotedasF. HyperparametersTostrikeabetterbalancebetweenvideo
deblurring quality and computational efficiency, the value
4.Experiments ofŒ∏ issetto0.3. Thepatchsizepandstridez aresetto4
and2,respectively.
4.1.Datasets
4.3.MainResults
DVD.TheDVDdataset[21]comprises71videos,consist-
ing of 6,708 blurry-sharp pairs. These are divided into 61
DVD. The quantitative results on the DVD dataset are
training videos, amounting to 5,708 pairs, and 10 testing
shown in Table 2. The proposed method demonstrates su-
videoswith1,000pairs.
periorperformanceintermsofbothPSNRandSSIMcom-
GoPro. The GoPro dataset [17] consists of 3,214 pairs
pared to existing state-of-the-art methods. Specifically, in
of blurry and sharp images at a resolution of 1280√ó720.
comparisontothebest-performingstate-of-the-artmethod,
Specifically, 2,103 pairs are allocated for training, while
Shift-Net+, the proposed BSSTNet achieves an improve-
1,111pairsaredesignatedfortesting.
ment of 0.26 dB in PSNR and 0.0013 in SSIM. Examples
4.2.ImplementationDetails from the DVD dataset are presented in Figure 5a, demon-
stratingthattheproposedmethodgeneratesimageswithin-
Training Details The network is implemented with Py- creasedsharpnessandrichervisualdetails. Thishighlights
Torch[19].Thetrainingisconductedwithabatchsizeof8 therobustnessofthemethodineliminatinglargeblurindy-
on8NVIDIAA100GPUs,andtheinitiallearningrateisset namicscenes.
GoPro. In Table 1, the proposed BSSTNet shows favor-
The source code is available at https://github.com/
huicongzhang/BSSTNet able performance in terms of both PSNR and SSIM whenInput VRT RVRT
Shift-Net+ BSSTNet GT
(a)QualitativecomparisonontheDVDdataset
Input VRT RVRT
Shift-Net+ BSSTNet GT
(b)QualitativecomparisonontheGoProdataset
Figure5.QualitativecomparisonontheGoProandDVDdatasets.Notethat‚ÄúGT‚Äùstandsfor‚ÄúGroundTruth‚Äù.TheproposedBSSTNet
producesimageswithenhancedsharpnessandmoredetailedvisualscomparedtocompetingmethods.
comparedtostate-of-the-artmethodsontheGoProdataset.
BSSTNet achieves higher PSNR and SSIM values com-
paredtoShift-Net+. ThevisualresultsinFigure5bfurther
illustratethattheproposedmethodrestoresfinerimagede-
tailsandstructures.
FLOPsandRuntime. Weconductedacomparisonofthe
computational complexity (FLOPs) and runtime between
our method, RVRT, and Shift-Net+, as presented in Ta- Previous Frame Current Frame
ble3. Incontrasttothestate-of-the-artShift-Net+,ourap-
proachdemonstratesa13GFLOPsreductioninFLOPsand
achievesaspeedupof1.6times.
4.4.AblationStudy
Effectiveness of BBFP. To evaluate the effectiveness of
BBFP,weconductanexperimentbyexcludingBBFPfrom
BSSTNet. AsillustratedinTable5,theomissionofBBFP
in Exp. (b) results in a reduction of 0.21 dB in PSNR Features from BBFP Features from SFBP
and 0.0011 in SSIM. BFA plays an important role in pre-
Figure 6. Comparison of feature alignment between BBFP
venting the introduction of blurry pixels from neighboring
andStandardFlow-guidedBidirectionalPropagation(SFBP).
frames. As shown in Table 6, replacing BFA with Stan-
ComparedtoSFBP,BBFPpreventsthepropagationofblurryre-
dardFlow-guidedFeatureAlignmentresultsinadeclinein
gionsfromthefeaturesofneighboringframesduringpropagation.
performance. Tohighlighttheimprovedfeaturealignment
capabilityofBBFP,wevisualizethealignedfeaturesinFig-
ure 6, comparing them with the standard feature bidirec-Table4.ComparisonofdifferenttemporallengthsintermsofPSNR,SSIM,Runtime,Memory,andGFLOPsbetweentheStandard
Spatio-temporalTransformer(SST)andBSST.. TheresultsareevaluatedontheDVDdataset. Notethat‚ÄúTL.‚Äùand‚ÄúMem.‚Äù denote
‚ÄúTemporalLength‚ÄùandtheusedmemoryonGPU,respectively.SSTrunsoutofmemoryforatemporallengthof60.
SST BSST
TL.
PSNR SSIM Time(ms) Mem. (GB) GFLOPs PSNR SSIM Time(ms) Mem. (GB) GFLOPs
12 34.59 0.9684 470 6.35 171 34.52 0.9681 336 2.40 122
24 34.83 0.9696 925 13.10 251 34.74 0.9692 684 4.79 127
36 34.92 0.9702 1332 20.40 277 34.85 0.9697 1026 7.30 130
48 34.97 0.9704 1776 28.27 329 34.95 0.9703 1368 9.97 133
60 - - - - - 35.01 0.9706 1712 12.78 137
Table5. EffectivenessofBBFPandBSST.Thebestresultsare the spatio-temporal domain results in the loss of valuable
highlightedinbold.TheresultsareevaluatedontheDVDdataset. information in the video sequence. Moreover, our spar-
sity strategy, which involves using the top 25% of tokens,
Exp. (a) (b) (c) (d)
achievesperformancecomparabletousingalltokenswhile
BBFP ‚úì ‚úì utilizingonlyapproximately43%oftheFLOPs. Thisindi-
BSST ‚úì ‚úì catesthatoursparsitystrategyeffectivelyleveragestokens
PSNR 33.78 34.74 34.10 34.95 insharpregionswithinthevideosequence.
SSIM 0.9645 0.9692 0.9661 0.9703 Comparison of Different Temporal Length. In Table 4,
we present a comparison of the Standard Spatio-temporal
Table6.ComparisonbetweenBFAandStandardFlow-guided Transformer (SST) under different sequence lengths in
Feature Alignment (SFFA). The best results are highlighted in
terms of PSNR, SSIM, Runtime, Memory, and GFLOPs.
bold.TheresultsareevaluatedontheDVDdataset.
As the sequence length increases, the computational com-
PSNR SSIM plexityofSSTgrowsrapidly.Incontrast,BSST‚Äôscomputa-
tionalcomplexityislessaffectedbythesequencelength,al-
SFFA 34.82 0.9696
lowingBSSTtoutilizelongersequencesandboostdeblur-
BFA 34.95 0.9703
ringperformance. Specifically,whenthesequencelengthis
60,BSSTshowsamodestgaininPSNRandSSIM.Consid-
Table7. Comparisonofvarioustokensparsitystrategies. The
ering the balance between performance and computational
bestresultsarehighlightedinbold. Theresultsareevaluatedon
load, we ultimately choose 48 as the length for the input
theDVDdataset.
videosequence.
PSNR SSIM GFLOPs
Random50% 33.92 0.9651 133
5.Conclusion
100% 34.98 0.9704 329
Top25% 34.78 0.9694 127 In this paper, we present a novel approach for video de-
Top50%(Ours) 34.95 0.9703 133 blurring, named BSSTNet. Utilizing an understanding of
theconnectionbetweenpixeldisplacementandblurredre-
tional propagation (SFBP). Benefiting from the incorpora- gions in dynamic scenes, we introduce a non-learnable,
tionofblurmaps,BBFPpreventsthepropagationofblurry parameter-freetechniquetoestimatetheblurmapofvideo
regionsfromthefeaturesofneighboringframesduringthe frames by employing optical flows. By introducing Blur-
propagationprocess,resultinginsharperfeatures. aware Spatio-temporal Sparse Transformer (BSST) and
Blur-aware Bidirectional Feature Propagation (BBFP), the
Effectiveness of BSST. To evaluate the effectiveness of
proposed BSSTNet can leverage distant information from
BSST,weconductanexperimentbyexcludingBSSTfrom
thevideosequenceandminimizetheintroductionofblurry
BSSTNet. As shown in Table 5, the omission of BSST in
pixels during bidirectional propagation. Experimental re-
Exp.(c)resultsinanotabledegradationof0.85dBinPSNR
sults indicate that the proposed BSSTNet performs favor-
and 0.0042 in SSIM. To further evaluate the effectiveness
ably against state-of-the-art methods on the GoPro and
and efficiency of BSST, we compare different token spar-
DVDdatasets,whilemaintainingcomputationalefficiency.
sitystrategies. Table7demonstratesthatusingfewertoken
numbers or randomly selecting tokens will result in a sig- Acknowledgments This research was funded by
nificant decline in performance. This result suggests that the National Science and Technology Major Project
without guidance from the blur map, discarding tokens in (2021ZD0110901).References [18] Seungjun Nah, Sanghyun Son, and Kyoung Mu Lee. Re-
currentneuralnetworkswithintra-frameiterationsforvideo
[1] Kelvin C. K. Chan, Shangchen Zhou, Xiangyu Xu, and
deblurring. InIn:CVPR,2019. 2
Chen Change Loy. Basicvsr++: Improving video super-
[19] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
resolution with enhanced propagation and alignment. In
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
CVPR,2022. 2,4
Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,
[2] LiangyuChen,XiaojieChu,XiangyuZhang,andJianSun.
AndreasKo¬®pf,EdwardYang,ZacharyDeVito,MartinRai-
Simplebaselinesforimagerestoration. InECCV,2022. 6
son, AlykhanTejani, SasankChilamkurthy, BenoitSteiner,
[3] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An
Zhang,HanHu,andYichenWei. Deformableconvolutional imperativestyle,high-performancedeeplearninglibrary. In
networks. InICCV,2017. 4 NeurIPS,2019. 6
[4] BoJiandAngelaYao. Multi-scalememory-basedvideode- [20] HyeongseokSon, JunyongLee, Jonghyeop Lee, Sunghyun
blurring. InCVPR,2022. 2 Cho,andSeungyongLee. Recurrentvideodeblurringwith
[5] HailinJin,PaoloFavaro,andRobertoCipolla. Visualtrack- blur-invariantmotionestimationandpixelvolumes. TIP,40
inginthepresenceofmotionblur. InCVPR,2005. 2 (5):185:1‚Äì185:18,2021. 2
[6] TaeHyunKim, KyoungMuLee, BernhardScho¬®lkopf, and [21] Shuochen Su, Mauricio Delbracio, Jue Wang, Guillermo
MichaelHirsch. Onlinevideodeblurringviadynamictem- Sapiro, Wolfgang Heidrich, and Oliver Wang. Deep video
poralblendingnetwork. InIn:ICCV,2017. 2 deblurringforhand-heldcameras. InCVPR,2017. 6
[7] Diederik P. Kingma and Jimmy Ba. Adam: A method for [22] ZacharyTeedandJiaDeng. RAFT:recurrentall-pairsfield
stochasticoptimization. InICLR,2015. 6 transformsforopticalflow. InECCV,2020. 6
[8] HeeSeokLee, JunghyunKwon, andKyoungMuLee. Si- [23] Patrick Wieschollek, Michael Hirsch, Bernhard Scho¬®lkopf,
multaneouslocalization,mappinganddeblurring. InICCV, andHendrikP.A.Lensch.Learningblindmotiondeblurring.
2011. 2 InIn:ICCV,2017. 2
[9] Dongxu Li, Chenchen Xu, Kaihao Zhang, Xin Yu, Yiran [24] Huicong Zhang, Haozhe Xie, and Hongxun Yao. Spatio-
Zhong, Wenqi Ren, Hanna Suominen, and Hongdong Li. temporaldeformableattentionnetworkforvideodeblurring.
Arvo: Learning all-range volumetric correspondence for InECCV,2022. 2,6
videodeblurring. InIn:CVPR,2021. 6 [25] Zhihang Zhong, Ye Gao, Yinqiang Zheng, Bo Zheng, and
[10] DasongLi,XiaoyuShi,YiZhang,KaChunCheung,Simon Imari Sato. Real-world video deblurring: A benchmark
See,XiaogangWang,HongweiQin,andHongshengLi. A datasetandanefficientrecurrentneuralnetwork. IJCV,131
simple baseline for video restoration with grouped spatial- (1):284‚Äì301,2023. 2
temporalshift. InCVPR,2023. 6 [26] Shangchen Zhou, Jiawei Zhang, Jinshan Pan, Wangmeng
[11] Jingyun Liang, Jiezhang Cao, Yuchen Fan, Kai Zhang, Zuo, Haozhe Xie, and Jimmy S. J. Ren. Spatio-temporal
RakeshRanjan,YaweiLi,RaduTimofte,andLucVanGool. filter adaptive network for video deblurring. In In: ICCV,
VRT: Avideorestorationtransformer. arXiv: 2201.12288, 2019. 2,6
2022. 2,6 [27] Shangchen Zhou, Chongyi Li, Kelvin C. K. Chan, and
[12] JingyunLiang,YuchenFan,XiaoyuXiang,RakeshRanjan, ChenChangeLoy. Propainter: Improvingpropagationand
Eddy Ilg, Simon Green, Jiezhang Cao, Kai Zhang, Radu transformerforvideoinpainting. InICCV,2023. 5
Timofte, and Luc Van Gool. Recurrent video restoration [28] ChaoZhu,HangDong,JinshanPan,BoyangLiang,Yuhao
transformerwithguideddeformableattention. InNeurIPS, Huang,LeanFu,andFeiWang. Deeprecurrentneuralnet-
2022. 2,3,6 work with multi-scale bi-directional propagation for video
[13] JingLin, YuanhaoCai, XiaowanHu, HaoqianWang, You- deblurring. InIn:AAAI,2022. 2,6
liang Yan, Xueyi Zou, Henghui Ding, Yulun Zhang, Radu
Timofte,andLucVanGool.Flow-guidedsparsetransformer
forvideodeblurring. InICML,2022. 2
[14] RuiLiu,HanmingDeng,YangyiHuang,XiaoyuShi,Lewei
Lu, Wenxiu Sun, Xiaogang Wang, Jifeng Dai, and Hong-
sheng Li. Fuseformer: Fusing fine-grained information in
transformersforvideoinpainting. InICCV,2021. 4,5,6
[15] Yasuyuki Matsushita, Eyal Ofek, Weina Ge, Xiaoou Tang,
andHeung-YeungShum.Full-framevideostabilizationwith
motioninpainting. TPAMI,28(7):1150‚Äì1163,2006. 2
[16] ChristopherMeiandIanD.Reid. Modelingandgenerating
complexmotionblurforreal-timetracking. InCVPR,2008.
2
[17] SeungjunNah,TaeHyunKim,andKyoungMuLee. Deep
multi-scaleconvolutionalneuralnetworkfordynamicscene
deblurring. InCVPR,2017. 6