Open-LLM-Leaderboard: From Multi-choice to
Open-style Questions for LLMs Evaluation,
Benchmark, and Arena
AidarMyrzakhan∗,SondosMahmoudBsharat∗,andZhiqiangShen∗
VILA-Lab,MohamedbinZayedUniversityofAI
{aidar.myrzakhan,sondos.bsharat,zhiqiang.shen}@mbzuai.ac.ae
Abstract
Multiple-choice questions (MCQ) are frequently used to assess large language
models (LLMs). Typically, an LLM is given a question and selects the answer
deemedmostprobableafteradjustmentsforfactorslikelength. Unfortunately,LLMs
mayinherentlyfavorcertainanswerchoiceIDs,suchasA/B/C/D,duetoinherent
biasesofprioriunbalancedprobabilities,influencingthepredictionofanswersbased
ontheseIDs. Previousresearchhasintroducedmethodstoreducethis“selection
bias”bysimplypermutatingoptionsonafewtestsamplesandapplyingthemtonew
ones. AnotherproblemofMCQisthelotteryticketchoiceby“randomguessing”.
TheLLMdoesnotlearnparticularknowledge,buttheoptionisguessedcorrectly.
Thissituationisespeciallyseriousforthosesmall-scaleLLMs1. Toaddressthem,
amorethoroughapproachinvolvesshiftingfromMCQtoopen-stylequestions,
which can fundamentally eliminate selection bias and random guessing issues.
However,transitioningcausesitsownsetofchallengesin(1)identifyingsuitable
open-stylequestionsand(2)validatingthecorrectnessofLLMopen-styleresponses
againsthuman-annotatedground-truths. Thisworkaimstotacklethesesignificant
difficulties, and establish a new LLM evaluation benchmark through entirely
open-stylequestions. Consequently,weintroducetheOpen-LLM-Leaderboard
to track various LLMs’ performance and reflect true capability of them, such
as GPT-4o/4/3.5, Claude 3, Gemini, etc. Our code and dataset are available at
https://github.com/VILA-Lab/Open-LLM-Leaderboard.
1 Introduction
Largelanguagemodels(LLMs)areincreasinglyexcellingatvariousnaturallanguageprocessingtasks,
includingtextgeneration[11],translation[45,50],summarization[22],codegeneration[20,33],
and chatbot interaction [28]. With the rising capability, the need for a robust evaluation strategy
thatcanaccuratelyassesstheperformanceofthesemodelsisbecomingcrucialinordertoidentify
theirtrueeffectivenessandchoosethemostappropriateoneforagiventask. Commonmetricsfor
assessingLLMstodayincluderelevance,frequencyofhallucinations,accuracyinquestionanswering,
toxicity,andretrieval-specificmetrics,amongothers. Inthecontextofquestion-answeringevaluations,
priorworksusuallyinvestigatethemodel’sperformanceintermsofansweraccuracy,courtesy,and
conciseness. And multiple choice questions (MCQ) have emerged as a predominant format for
suchassessments,whereinaquestionispresentedwithseveralpossibleresponses,andthemodel
isrequiredtoselectthemostfittingchoiceID,asexemplifiedinFigure1. Lately,theMCQformat
∗Jointfirstauthor&equalcontribution.
1Forinstance,onMMLU,therandomguessingaccuracyis25%,andmostsmall-scaleLLMsobtainresults
aroundthisvalueasshownin[35,46]. Itisdifficulttodistinguishwhichmodelisbetterunderthissituation.
Preprint. Underreview.
4202
nuJ
11
]LC.sc[
1v54570.6042:viXrahasseenwidespreadapplicationinLLM-focusedcontexts,includingbenchmarks[18,44,12]that
examine LLM capabilities and automated/crowdsourcing evaluation frameworks [21, 49, 5] that
streamlinetheassessmentprocess.
However, previous studies [48, 32] have dis- Question that is suitable for
cussedthatthelackofresilienceofLLMsto open-style: Let x = 1. What is x «
changes in the positioning of options stems 3 in Python 3?
from their tendency to exhibit biased behav- Options: A. 1 B. 3 C. 8 D. 16
ior: theyoftenfavorchoosingcertainoption Answer: C
IDs(suchas“OptionA”)asresponses,aphe- Question that is not suitable for
nomenonthatisreferredtoasselectionbias. open-style: Which of the following
Moreover, it shows that selection bias exists statements is true?
widely across various LLMs and cannot be Options:
mitigated by simple prompting skills. The A. Every equivalence relation is a
underlying reason of this issue comes from partial-ordering relation.
the condition that the model is trained with B. Number of relations form A = x, y, z
aprioridistributionthatassignsmoreproba- to B= (1, 2), is 64.
bilisticchoicestospecificIDtokens. Another C. Empty relation _ is reflexive
D. Properties of a relation being
issueofMCQisthe“randomguessing”thatis
symmetric and being un-symmetric are
discussedin[35]. Specifically,smallmodels
negative of each other.
suchasthe1B-levelvariants,maystruggleto
Answer: B
achieve reliable predictions on many bench-
markslikeMMLUwhichusesfourchoicesas Figure1: ExamplesofMCQfromMMLU.
the answer candidates of the questions. Their results could resemble random choices, not truly
capturingthemodel’sactualcapabilities.
TofundamentallyeliminateselectionbiasandrandomguessinginLLMs, inthiswork, webuild
anopen-stylequestionbenchmarkforLLMevaluation. Leveragingthisbenchmark,wepresentthe
Open-LLM-Leaderboard,anewautomatedframeworkdesignedtorefinetheassessmentprocessof
LLMs. Thisframeworkfunctionsinsupplementtopriorevaluationframeworkssuchas[21,49,5]
with several advantages as presented in Sec. 4.4. However, constructing such a benchmark has
twosignificantchallenges: (1)howtodeterminetheappropriatequestionsthatcanbeeffectively
transformedfromMCQintoopen-stylequestions,and(2)howtoestablishanapproachtoaccurately
validatethecorrectnessoftheLLM’sopen-styleanswersincomparisontohuman-annotatedground-
truths,especiallyincontrasttoMCQ,whichtypicallyhavedefinedsingle-choicestandardanswers.
Forthefirstchallengeofidentifyingthemultiple-choicequestionsthataresuitableforconvertingto
open-stylequestions,wedesignanautomaticcoarse-to-fineselectingprotocolthroughcustomized
promptsandmulti-stagefilteringprocess. Specifically,inthefirststage,weusethebinaryclassification
tofilterthequestionswithhighconfidenceasthepositivepoolandothersareassignedasnegative. Our
secondstageusesasoftscoringmethod(1-10ratings)tojudgethesuitabilityofthequestionsforthe
open-stylefromthequestionsthatareclarifiedasnegativeinthefirststage. Forthesecondchallenge
ofevaluatingthecorrectnessoftheLLM’sopen-styleanswersincomparisontohuman-annotated
ground-truths,wefurtherdesignatask-specificpromptandleverageGPT-4toexamineiftheresponse
iscorrect. Tovalidatetheaccuracyoftheautomaticevaluationstrategy,werandomlysample100
resultsandmanuallychecktheautomaticevaluationresultswiththecorrespondingresponses,and
confirmthatitisreliablewithanerrorrateoflessthan5%.
Inourend-to-endassessmentoftheLLMevaluationandrankingprocess,weconductacomprehensive
analysisonthewell-recognizedLLMs,includingGPT-4o,GPT-4,ChatGPT,Claude-3Opus,Gemini-
ProandMistral-Large. OurbenchmarkingresultsindicatethatGPT-4ocurrentlyholdsthepositionas
thestrongestLLM.WefurtherprovideasmallregimeLLMleaderboardtargetingatLLMssmaller
than3B.Moreover,ourstudydemonstratesahighcorrelationbetweentherankingsproducedbyour
open-stylebenchmarkandthosederivedfromuser-basedevaluationsordirecthumanassessments.
2 RelatedWork
LargeLanguageModels(LLMs). RecentadvancementsinLLMs,suchasGPT-3[9]andGPT-
4 [28] have had a significant impact in the field of natural language processing and have found
widespreadapplicationacrossvariousdomains. Ithasindeedinitiatedakindofchainreactionwithin
2thecommunityandbeyond. AseachnewiterationofLLMsdemonstratesenhancedcapabilities,
organizationsandresearchersacrossvarioussectorsaremotivatedtodeveloptheirownmodels,such
asLLaMA[40,41],Gemini[38],andClaude[2],orfindinnovativewaystoimproveexistingLLMs
throughinstructiontuning,likeAlpaca[37],andVicuna[10].
MultipleChoiceQuestions(MCQ).IntherealmofLLMresearch,MCQhasbecomeapivotaltool
forevaluatingandenhancingthecapabilitiesofthesemodels. NotabledatasetsliketheMMLU[18],
HellaSwag[44],andARC[12]havebeeninstrumentalinthisregard. Theirdiverseassessmentof
broadknowledgeandcommonsensereasoninghelpinbenchmarkingthedepthandversatilityof
LLMsinunderstanding,reasoning,andapplyingknowledgeacrossvariousdomains. MCSB[31]
introduces a natural prompting strategy for LLMs, which presents questions and answer choices
together,allowingthemodeltoexplicitlycompareoptions.
BiasinLLMs. Selectionbias,aspecificformofbiasrelevanttotheevaluationofLLMsthrough
MCQ, has garnered attention due to its understated and widespread impact. A series of works
[49,30,48,42]haveshownthatLLMsmaydevelopapropensitytofavorcertainanswerchoices
basedontheirpositionorencoding,suchasthealphabeticalorderingofA/B/C/DinMCQ.This
phenomenoncanleadtoskewedevaluationresults,misrepresentingamodel’strueunderstandingand
reasoningcapabilities.
3 Approach
3.1 DefiningOpen-styleQuestions
Open-stylequestions,akaopen-endedquestions,requirethemodeltogenerateananswerwithout
beingconstrainedbyasetofpredeterminedchoices. InthecontextofLLMevaluation,thesequestions
aredesignedtoassessthemodel’sabilitytogeneratecoherent,relevant,andcontextuallyappropriate
responsesbasedontheinputquery. Whilemultiple-choicequestionscanefficientlyassessspecific
factualknowledgeandcomprehension,open-stylequestionsofferadeeperinsightintotheLLM’s
generativecapabilities,understandingofcontext,andabilitytoengagewithcomplextasks. Also,
open-stylequestionscanavoidtheinherentselectionbiasandrandomguessingweaknessescompared
tomultiple-choicequestions.
3.2 AutomaticOpen-styleQuestionFilteringandGeneration
Multi-stageFilteringandPostprocessingviaCoarse-to-fineProcess. Ourproposedmulti-stage
filteringapproachconsistsoffourmainstepstostreamlinetheconversion: (1)Initiallyclassifydatasets
aseitherconvertibleornon-convertible. (2)Assigneachquestionaconfidencescoretoindicatethe
likelihoodthatitcanbeframedasanopen-stylequestion. (3)Excludequestionswithconfidence
scoresbelowaspecifiedthresholdandclassifiedasnon-convertible. (4)Combinequestionsthatare
labeledasnon-convertiblebuthavehighconfidencescoreswiththoselabeledasconvertible.
Stage1: PreliminaryFilterusingBinaryClassification. ConsideringthatthestructureofMCQ
varies,convertingthemintoanopen-styleformatisnotalwayspossible,particularlybecausecertain
questionsarestronglylinkedtotheirchoices. Forinstance,questionsformulatedas“Whichoneof
thefollowingistrue”or“Allexcept”or“Whichofthese”. Suchquestionsaretypicallyunsuitablefor
conversionintoanopen-styleformatsincetheabsenceoftheoptionscouldchangethequestion’s
core,resultinginincompletequestions.
Toeffectivelyhandlethischallengeofidentifyingwhethermultiple-choicequestionsaresuitable
foropen-styleconversion,weleveragethepowerofpromptingtechniquestocreateacustomized
classificationpromptasshowninTable1. Intheprompt,weintegratedifferenttypesofquestions
from different datasets to demonstrate how an LLM may evaluate each question to be written in
anopen-styleway,eventuallyclassifyingthemasconvertible“YES”ornon-convertible“NO”.It
willdeterminewhetheraquestionprovidesaclearcontextandinformationwithoutrelyingonthe
providedoptionsornot. Inthepromptweintegratedifferenttypesofquestionsfromdifferentdatasets
todemonstratehowanLLMlikeGPT-4mayevaluateeachquestiontobewritteninanopen-style
way,eventuallyclassifyingthemasconvertible“YES”ornon-convertible“NO”.Wesetthepromptto
eliminateanyadditionalexplanations,bystatingthat“Yourresponseshouldincludeonlytheverdict
withoutanyjustificationorreasoning.” Thisguaranteesthattheanswertoeachinquiryisconveyed
conciselyas“YES”or“NO”.
3Multiple-Choice Questions Path
Multiple-choice Collect the responses Result
question datasets from LLMs in a multiple- Evaluation
collection. choice format. (accuracy)
1 Questions are first
classified as either 'YES'
(can be written in an
open style) or 'No'
(cannot be written in an
YES No
open style) using GPT4.
2
Assign a confidence
score ranging from 1
to 10 using GPT-4.
Comparative
Utilize GPT-4 to filter analysis of both
MCQs that can be 4 formats
written as an open Greater than Less than
style. threshold threshold
3
Move to 'YES'
category Remove
Collect the responses Design a prompt Result
from LLMs in an open- for an evaluation. Evaluation
style format. (accuracy)
Open-Style Questions path
Figure2: Anoverviewofadual-pathevaluationpipelineforLLMs,startingwiththecollectionof
MCQdatasets. Itbranchesintotwopaths,withtheMCQpathproceedingdirectlyfromresponse
collectiontoevaluation,whiletheopen-stylepathpassesthroughanadditionalfilteringphase. After
evaluation,bothpathsconvergeinacomparativeanalysis.
Tounderstandourinitialfilteringresults,weconductanerroranalysismanuallybyselecting100
questions in the “YES” and “NO” pools separately. In the samples classified as “YES”, we find
that only around 5% of the questions are false positive cases, verifying a low misclassification
error for the positive question selection by our filtering strategy. Conversely, within the “NO”
sample,around40%ofthequestionsareactuallysuitableforopen-stylequestionsbutmistakenly
classifiedasnegative. Thissituationoftenarisesfromquestionsthatincludephraseslike“Whichof”.
Similarly,questionsinvolvingtrue/falsestatements,sentencecompletions,orfill-in-the-blanksare
alsosometimesinappropriatelyclassifiedasnon-convertible. Thisanalysismotivatesustodevelopa
cascadedfine-grainedstagetofurtherfiltermorepositivequestionsin“NO”poolusingparticular
prompts,asdescribedinthefollowingStage2process.
Stage2: ConfidenceScoreAssignment. Asweaimtoovercometheissueofclassifyingquestionswith
specificpatternsasnon-convertible,weintroduceasecondstageoffilteringcenteredonconfidence
scoreassignment. Thisinvolvesinstructingthelargelanguagemodeltoassignaconfidencescoreon
ascalefrom1to10,reflectingthepossibilityofthequestionbeingwritteninanopen-styleformat.
Sinceasignificantnumberofquestionsareunsuitableforanopen-styleformat,categorizedas“NO”
and have a confidence score below 5, we set a confidence score threshold to be 5. Therefore,
questionsclassifiedasnon-convertiblewithaconfidencescorelowerthanthisthresholdareexcluded,
whilethoseremainingabovethethresholdandthoseinitiallyclassifiedasconvertiblearemovedinto
the“YES”categorytobeconvertedtoanopen-styleformat.
4Table1: Promptdesignfortwo-stagefilteringandpostverification.
StageOne:CoarseFilteringPrompt
"""Your task is to review a series of multiple-choice questions and evaluate their ability to
be answered without the provided answer choices. For questions that begin with an incomplete
sentence (e.g., "During swallowing, ..."), use your knowledge to attempt to complete the
sentence accurately. For direct questions that ask for specific information or identification
(e.g., "Which of the following structures is part of the small intestine?"), assess whether the
question is formulated clearly enough that an informed answer can be given without seeing the
multiple-choice options. For mathematical or analytical questions (e.g., "Find all cosets of
the subgroup 4Z of 2Z"), determine if the question provides enough context and information for a
solution to be formulated without additional options.
Pleasefollowthisformatforyourevaluation:
QUESTION: [Insert the question here] VERDICT: Respond with "YES" if the question is clear and can
be directly answered based on its content alone, or "NO" if it relies on the answer choices to be
understood or answered. Your response should include only the verdict without any justification
or reasoning."""
StageTwo:Fine-grainedFilteringPrompt
You will assign a numerical score from 1 to 10 based on how confidently it can be answered
without the choices. The scoring criteria are as follows:
1: The question is entirely dependent on its choices for an answer, making it impossible to
answer without them. Example: ‘Which of the following statements is correct?’
10: The question can be easily and confidently answered based solely on the question stem,
without any need to refer to the provided options. Example: ‘What is the first law of
thermodynamics in physics?’ Intermediate Scores:
2-4: The question stem gives very little information and is highly reliant on the choices for
context. Example: ‘Which of these is a prime number?’ 5: The question provides some context
or information, that gives a moderate possibility to answer the question. Example: ‘Which of
the following best describes the structure that collects urine in the body?’
6: The question provides a good amount of context or information, that gives a moderate
possibility to answer the question. Example: ‘Statement 1 | A factor group of a non-Abelian
group is non-Abelian. Statement 2 | If K is a normal subgroup of H and H is a normal subgroup of
G, then K is a normal subgroup of G.’
7: The question provides a good amount of context or information, that gives a high possibility
to answer the question. Example: ‘The element (4, 2) of Z_12 x Z_8 has order’
8-9: The question provides a good amount of context or information, that gives a high
possibility to answer the question. Example: ‘A "dished face" profile is often associated with’
ONLY GIVE THE VALUE BETWEEN 1-10 AS YOUR ANSWER. DO NOT INCLUDE ANY OTHER INFORMATION IN YOUR
RESPONSE
Example Format:
QUESTION: question here
VERDICT: value in [1-10] here
GPT-4PromptforVerification
"""Evaluate the answer of a AI model to a question. You will be provided with the question, the
AI model’s answer, and the correct answer. Your task is to evaluate the AI model’s response and
determine whether it is Correct or Incorrect.
Grade the AI model answers based ONLY on their factual accuracy. It is OK if the AI model
answer contains more information than the true answer, as long as it does not contain any
conflicting statements. Otherwise, it should be marked as Incorrect. Ignore differences in
punctuation and phrasing between the AI model’s answer and the true answer.
Example Format:
QUESTION: question here
STUDENT ANSWER: student’s answer here
TRUE ANSWER: true answer here
GRADE: Correct or Incorrect here
Your response should include only the verdict without any justification or reasoning."""
3.3 Open-styleQuestionAnswerEvaluation
Afterestablishingasetofconvertiblequestionsfromvariousdatasetsandobtainingtheirresponses
from several LLMs, there arises a need to evaluate these questions. Given that our ground truth
answersarebasedontheMCQformatwithdefinedanswers,itnecessitatesamethodforefficiently
andaccuratelyvalidatingthecorrectnessofresponsestoopen-stylequestions. Tothisend,wedesign
acustomizedprompt,asshowninFigure2thatutilizesthecorrectMCQanswerasthegroundtruth
5todetermineiftheopen-styleresponsesarecorrectorincorrectbytheprediction𝑦ˆ:
𝑦ˆ =LLM (prompt(𝑞,𝑎ˆ,𝑎)) (1)
e
where𝑦ˆrepresentsthepredictionandLLM istheLLMevaluator. 𝑞,𝑎ˆand𝑎representthequestion,
e
LLMgeneratedanswer,andcorrectanswerfromMCQ,respectively,andthepromptisprovidedin
Table1ofAppendix. Whiletheseopen-styleanswersareevaluatedbasedontheMCQ’sgroundtruth,
issuesofmisevaluationmightarise. Thisincludesscenarioswherearesponseisinaccuratelyclassified
ascorrectsimplybecauseitcontainscertainkeywordsalsofoundinthegroundtruth. Totacklethis
issueweincludespecificphrasesintheprompt. Thesephrases,suchas“aslongasitdoesnotcontain
anyconflictingstatements”,ensurethataresponseisnotautomaticallyclassifiedascorrectbasedon
thepresenceofakeyword,avoidingincorrectmarkingswhentheresponsecontradictsthecorrect
answer. Additionally,topreventtheexclusionofcorrectanswersthatincorporateextrainformation,
weincorporatethephrase“ItisOKiftheAImodel’sanswercontainsmoreinformationthanthetrue
answer”. Furthermore,wehighlightthatminordifferencesinpunctuationandphrasingbetweenthe
open-styleresponsesandthegroundtruthanswersshouldnotleadtotheirbeingclassifiedasincorrect.
ToseethecorrectnessoftheLLMjudgementwetaketherandomlydrawn100responsesfromall
models. Thehumanevaluationprocessforourstudywasconductedbytheauthorsthemselves. The
agreementbetweentheLLMevaluationsandthoseofahumanevaluatorwasquantitativelyassessed
usingCohen’skappa[13],whichyieldedascoreof0.83. Thissubstantialkappascore2verifiesthat
theLLM’sabilitytodeterminethecorrectnessofresponsesalignscloselywithhumanjudgment,
demonstratingstrongreliabilityinitsevaluationprocess.
4 AnOpen-styleQuestionBenchmark(OSQ-bench)
4.1 StatisticsandDistributions
Table 2 describes the basic statistics of the dataset questions that are suitable for answering in
open-styleformat. Intotal,wehaveevaluated42Kquestionsfrom9differentdatasetsandmorethan
23Kofthemareclassifiedasappropriateforopen-styleanswering.
Table2: Statisticsonopen-stylequestionsacrossdifferentdatasets.
Benchmarks #Evaluated #Open-Style AverageQuestionLength(words)
MMLU 14,042 7,784 36.6
ARC 3,428 3,118 21.1
MedMCQA 4,183 2,318 14.1
CommonsenseQA 1,221 710 13.1
Race 4,934 3,520 10.0
OpenbookQA 1,000 491 10.3
WinoGrande 1,267 1,267 19.1
HellaSwag 10,042 3,915 40.1
PIQA 1,838 696 7.1
Overall 41,955 23,839 19.05
4.2 Diversity
Ourinvestigationintothediversityofquestionswithinourbenchmarkisfoundationalforunderstanding
thelandscapeofopen-endedquestionanswering. Tocomprehensivelyassessthebreadthofquestion
diversity,wehaveconductedasystematiccategorizationofthequestiontypessourcedfromanarray
ofdistinctdatasets. Fromthetotalinitialpoolof41,955questions,werefinetheselectionto23,839
questions,ensuringthateachoneisconducivetoopen-endedresponses. Thedistributionofthose
questionsisillustratedinFigure3,whichsegmentsthedataintoseveraldomainsbasedonthecontent
ofthequestions. Thesegmentationoftheplotunderscorestheinterdisciplinarynatureofourdataset.
Itfeaturesabroadspectrumofcategoriessuchasliteratureandreadingcomprehension,commonsense
reasoning,domain-specific(medicine,STEM,andetc),andmulti-topicknowledge. Also,Table2
demonstratesthediversityofquestionlengthusedforthebenchmark.
4.3 Quality
2The Kappa score is a statistical measure of inter-rater agreement for categorical items, defined by the
equation: 𝜅= 𝑃 1𝑜 −− 𝑃𝑃 𝑒𝑒 where𝑃 𝑜istheobservedagreementand𝑃 𝑒istheexpectedagreementbychance.
6(1) GPT-4 (2) GPT-3.5
(3) Claude-3 Opus (4) Mistral-large
Figure 4: Performance comparison of various LLMs on multiple-choice (MCQ) and open-style
questions(OSQ)acrossdifferentdatasets. Thebargraphsontheleftshowthecountsofcorrectand
incorrectresponses(✗MCQvs. ✓OSQ;✓MCQvs. ✗OSQ;✓MCQvs. ✓OSQ;✗MCQvs. ✗
OSQ),whiletheradarchartsontherightillustratetheaccuracycomparisonsbetweenMCQandOSQ
foreachlanguagemodel(PinkistheMCQaccuracyandLimeGreenistheOSQaccuracy).
Social Sciences Humanities Our newly developed benchmark, curated from
Activity
Prediction
Miscellaneous
w
o
a
anni fd
o
aqe
r
bul my
ie ls
a
ir
tt
te yioc
th
to
n
oag
s
tn
gs
d
ei uz
e
nie
t
m
ead
rb
a
ad
l
n
tea edt ta
iso
ns ae
fo
ot
p
ds r,
e
e
mn
es at
-
pa
s
tn
it
u
vyd
n
els
e
d
,o
e
uau
r
nn
st
s
rt
ewb
a
sy
ne
td
rrf iio
i
cn
nc
tg
g
eu
,
ds aii
.
nn
re
edg
. -,
Situational
Reasoning
HellaSwag
MMLU STEM s qp uo en sts ie os n. sG oi rv ie gn int ah ta et at rh ee wda idta es lyets utf ir lo izm edw ah ni dch ht ih ge hs lye
recognizable within the research community, it
Langu Ca rig tie c
L
ia tA l
e
rn aR ta ue rl a ey ds Ci oi ns mg pr Ne ath ue rn als i So cinR ena cec se TechnologyARC
Mathematical
Reasoning
M ed MM edC icaQ
l
W SA peciai ltn ieC sCoo lim nG icm r ala KoO n nn op ds wee le en Cn dP
H
gsb o eI eeo r aQ Q eo lf tAk L e hAQ ci rn e aA rng ecu ei s RtS eip scPT oa het lPyP m ui sa P ar tp iSl ce ih- oC a t ooT d rW lyo tc a ne Cin eis loc a mc oA Ci r rltC e mc n l oi C ndppo v aa m mt om ol e su Kly mm o ram nt Pa nRl oi m o c sl ro nU ea e wi on R sn nnl a en l d e seRK nc ss ee a d sen eior ps egaso nnot es lw sa eo n i enl nne sid n gd ii n gng gge vf a t i
n
s
fh t oo us
ao
rel s sir tl te e
t
oo at ss s mw ph bus u eo lis alt
e
ntr th
ja
)so
o
-t e bh su roi tla im ng
yfe
tt yh lo
a
ea ft ood rh fi aol fe re nl o ul t qow ss eq
n
u’ wpru
d
eic
e
efne sa a
n
5
rs tgp sl i-t %s oi sa ip eo
t
n
nb .yrn
s
di po lls
e
T
ei coct eaeh hsi
t
de tis ha
i
ets ssv ai mg. ve
t
ii oeM
n
et
a
rg
ed
irh ro to
ze
iaa cr eo ttse hace dd o etlu
ea
av (q n csq
s
seu rsd ur
st
ii,a e
h
te
ufi
el d r s ai ieg rtut
tt
idiy aoe ao
t
bn .hn at t leo o
e
es s,
Figure 3: Diversity and distribution of used
4.4 PropertyandAdvantage
datasetsforourOSQ-bench.
AsshowninTable3,ourleaderboardexhibitsseveraladvantages: firstisthedebiasedresultscompared
totheMCQ-basedleaderboard,whichhasbeendiscussedthoroughly. Anotheradvantageisthefaster
andcheaperevaluationovercrowduser-basedleaderboards. Ourresultsandrankingscanbegenerated
automaticallywithoutanyhumanintervention.
Table3: ComparisonwithdifferentLLMleaderboards. “Biased”indicatestheselectionbias.
Leaderboard Types Diversity Biased Evaluation
HuggingfaceLeaderboard[5] MultipleChoicesQuestions High ✓ Automatically
AlpacaEvalLeaderboard[21] HumanQuestions&Feedback Low ✗ GPT-4
ChatbotArenaLeaderboard[49] HumanQuestions&Feedback Low ✗ GPT-4/Crowdusers
Open-LLM-Leaderboard(Ours) OpenStyleQuestions High ✗ GPT-4
7ARC CommonsenseQA Hellaswag
MedMCQA MMLU OpenbookQA
PIQA Race Winogrande
Figure5: PercentageofconvertibleMCQtoopenstylequestionsonvariousdatasets.
5 Experiments
5.1 Models
We generate responses from LLMs of different sizes. The large-scale LLMs: gpt-3.5-turbo,
gpt-4-1106-preview,gpt-4o[27],claude-3-opus-20240229[3],mistral-large-latest
[24],gemini-pro[16],andllama3[1]. WeusethecommercialAPIstocollectresponsesfromallof
thesemodels. Thesmall-scaleLLMs: qwen1.5[4],gemma[39],SlimPajama-DC[35],RedPajama
[25],OLMo[17],Pythia[6],TinyLlama[46],OPT[47],GPT-Neo[8],andCerebras-GPT[14]. All
ofthesmall-scalemodelresponsesarecollectedusingHuggingface[43]andlm-evaluation-harness
framework[15]with4×4090RTXGPUs.
5.2 Datasets
Wepresentabriefoverviewofuseddatasets,highlightingtheirdistinctivecharacteristicsandthe
specificaspectstheyaimtoevaluate. MMLU[18],ARC[12],andMedMCQA[29]standoutwiththeir
comprehensiverangeoftasksspanningacrossvariousdisciplines. PIQA[7],CommonsenseQA[36],
OpenBookQA[23],andHellaSwag[44]focusonthedifferentaspectsofcommonsensereasoning,
such as physical interaction, everyday concepts, and their interrelations. RACE [19] provides a
source of reading comprehension challenges. WinoGrande [34] is designed to test the model on
resolvingcoreferencesandunderstandingnuancedrelationshipsintext. Thisdatasetwithitsunique
fill-in-a-blanktasks,inherentlyalignswithopen-endedquestionformats,negatingtheneedforour
multi-stagefilteringprocess. Forotherdatasets,questionsarefilteredusinggpt-4-0125-preview
usingpromptsfromTable1. ThepromptsforbothMCQandOSQoneachdatasetareinAppendixD.
5.3 Evaluation
OurassessmentapproachforbothMCQandOSQalignswithwidelyrecognizedevaluationframeworks
and leaderboards for LLMs. The evaluation of MCQ is conducted utilizing the OpenAI Evals
framework[26]withthezero-shotsetting,whichinvolvescomparingthegeneratedresponsewith
thegroundtruthID.Incontrast,forevaluatingresponsestoopen-endedquestions,weemploythe
gpt-4-0125-previewmodeltodeterminethecorrectnessofresponsesgeneratedbyLLMsrelative
toapre-establishedgroundtruthanswerfromthedatasetusingthepromptfromTable1.
TheresultsinTable4andFigure4arebasedonfilteredquestions. Theyshowthateverymodel
experiencesasignificantdropintheaccuracyforOSQcomparedtoMCQ.Onaverage,theaccuracy
ofOSQislowerthanMCQbyabout25%forallmodels. Thisresultcancorrelatewithourconcern
8Table4: Comparisonofmultiplechoice(MCQ)andopenstylequestions(OSQ)accuracy.
Dataset GPT-4 GPT-3.5 GeminiPro Claude-3O MistralL
MCQ OSQ MCQ OSQ MCQ OSQ MCQ OSQ MCQ OSQ
MMLU 87.28 74.77 71.25 65.38 65.71 56.04 83.52 70.23 79.50 68.76
ARC 95.54 82.68 90.64 78.42 90.96 72.35 97.50 75.47 89.96 72.32
HellaSwag 90.98 24.35 63.84 29.99 69.05 25.69 96.04 20.79 81.78 24.47
WinoGrande 84.14 66.22 78.77 64.56 66.85 56.35 81.69 63.54 75.45 56.83
PIQA 96.41 61.64 84.34 54.89 83.33 47.70 97.41 59.05 83.33 61.21
CommonsenseQA 84.93 62.96 79.15 67.89 66.62 50.56 86.76 63.66 69.58 55.35
Race 92.02 67.05 84.80 60.11 87.73 61.02 93.04 66.22 89.97 70.17
MedMCQA 72.65 51.81 58.02 41.42 58.02 35.89 72.91 49.14 66.05 43.44
OpenbookQA 94.30 60.29 83.71 49.90 86.97 52.55 93.48 52.95 88.19 58.66
Average 88.69 61.31 78.28 56.95 75.03 50.91 90.26 57.89 80.42 56.80
Table5: Open-LLMLeaderboardforLarge-scaleModels. WG,CSQA,OBQA,andHSrepresent
WinoGrande,CommonsenseQA,OpenbookQA,andHellaSwagrespectively. Wedidnotinclude
HellaSwagresultsintheoverallaccuracyastheevaluationdifficultiesmentionedinSec.5.3.
Overall MMLU ARC WG PIQA CSQA Race MedMCQA OBQA HS
GPT-4o 70.15 79.09 86.31 72.22 60.34 70.28 67.87 57.85 67.21 –
GPT-4-1106-preview 65.93 74.77 82.68 66.22 61.64 62.96 67.05 51.81 60.29 24.35
Claude-3Opus 62.53 70.23 75.47 63.54 59.05 63.66 66.22 49.14 52.95 20.79
MistralLarge 60.84 68.76 72.32 56.83 61.21 55.35 70.17 43.44 58.66 24.47
GPT-3.5 60.32 65.38 78.42 64.56 54.89 67.89 60.11 41.42 49.90 29.99
Gemini1.0Pro 54.06 56.04 72.35 56.35 47.70 50.56 61.02 35.89 52.55 25.69
Llama3-70b-Instruct 52.92 59.67 67.09 57.14 43.10 55.49 58.21 41.67 40.94 –
Table6: Open-LLMLeaderboardforsmall-scalemodelregime.
Model Overall MMLU ARC WG PIQA CSQA Race MedMCQA OBQA
Qwen1.5(1.8B) 21.68 9.99 15.84 40.96 15.52 31.13 34.91 4.70 20.37
Gemma(2B) 16.66 17.52 23.93 16.10 15.09 27.46 14.32 4.57 14.26
SlimPajama-DC(1.3B) 9.60 9.22 14.95 14.76 5.32 9.01 16.19 1.68 5.70
RedPajama(1.3B) 9.00 9.21 13.50 16.97 0.86 11.41 14.35 1.86 3.87
OLMo(1.2B) 8.85 8.54 13.18 6.16 8.05 13.10 13.61 2.07 6.11
Pythia(1.4B) 8.79 9.66 14.69 11.52 4.17 9.01 12.76 3.19 5.30
TinyLlama(1.1B) 8.45 8.94 13.31 12.23 3.59 6.06 16.7 2.07 4.68
OPT(1.3B) 7.89 7.40 11.83 12.47 4.48 7.61 13.61 1.25 4.48
GPT-Neo(1.3B) 7.42 6.94 9.69 10.81 4.31 6.34 13.75 2.63 4.89
Cerebras-GPT(1.3B) 4.86 5.37 4.43 9.31 2.16 6.20 6.90 1.04 3.46
thatthemodelwill“randomlyguess”tocorrectchoicesbutitcannotanswer. Thisdiscrepancyin
performancebetweenOSQandMCQisnotnecessarilyanegativereflectionofthemodels’overall
capabilities. Instead,itcanbeviewedasatruecomparisonofthemodels’abilitiestoprocessand
understanddiversetypesofquestions.
ThemostsignificantdifferenceinmodelsbetweenOSQandMCQisobservedforClaude-3Opus,
by31%. ThedatasetwiththelargestfallbetweenMCQandOSQisHellaSwag. Thisisbecause
of the type of questions in this dataset. It asks to choose the most plausible continuation for the
scenariospresented. EvaluatingtheOSQresponsesofLLMsagainstthegroundtruthinthisdataset
presentsasignificantchallengeduetothedifferentplausiblecompletions. Itmeansthatamultitude
ofvalidandcontextuallyappropriateanswerscanexist, whichmakesitdifficulttoevaluatewith
single-choicegroundtruth. ThiscontrastswithWinoGrande,whichconsistsofquestionsthatrequire
fill-in-the-blankinsentenceswithcorrectwords. Asaresult,HellaSwagdoesnotseemwell-suited
foropen-stylequestions,andwehavechosentoomititfromourfinalleaderboard.
5.4 LeaderboardandArena
The overall ranking of models for our benchmark is represented in Table 5 and Table 6. The
performanceofGPT-4ooveralldemonstratesitsleadingedge,withanaccuracyof70.15%,which
indicatesitsrobustnessinopen-stylequestionansweringtaskscomparedtoothermodels. Itisfollowed
byGPT-4-1106-previewwith65.93%,andClaude-3Opuswith62.68%. Theseresultshighlightthe
9advancedcapabilitiesoftheGPT-4series. Mid-tiermodelslikeMistralLargeandGPT-3.5perform
wellbutarenotonparwiththetopperformers. Ontheotherhand,modelslikeGemini1.0Proand
Llama3-70b-Instructlagbehindintermsofthecapabilitiestoanswertheopen-stylequestions.
The performance evaluation of smaller-scale LLMs reveals that Qwen1.5 leads with an overall
accuracyof21.68%,significantlyoutperformingtheothermodelsinthiscategory. Gemmafollows
with16.66%,indicatingaconsiderablegapinperformancecomparedtothetopmodel. Theremaining
modelsscorebelow10.00%,highlightingtheirlimitedabilitiestoanswertheopen-stylequestions.
AlmostallofthemodelsstrugglesignificantlywithquestionsfromMedMCQAdataset,showingan
accuracybelowof5%.
6 Conclusion
We proposed Open-LLM-Leaderboard for LLM evaluation and comprehensively examined its
efficacyusingopen-stylequestionsfromninedatasetsonOSQ-bench. Differentfrompreviousworks
thatrelyonhumanevaluationorthousandsofcrowdusersonChatbotArena,wecanhaveabenchmark
forchatLLMsinafast,automatic,andcheapscheme. Ourresultsshowahighlycorrelatedlevel
ofagreementwithhumans,indicatingafoundationforanLLM-basedevaluationbenchmarkand
frameworkusingopen-stylequestions.
LimitationsandEthicsStatement
We have discussed multiple advantages of employing open-style questions over multiple-choice
questions used in prior works. However, the LLM Leaderboard, as a tool for evaluating and
benchmarkingLLMs,hasseveralcommonlimitationsitself. Firstly,theperformancemetricsused
may not fully capture the nuanced capabilities of each model, especially in areas that require an
understandingofcontext,creativity,orcommonsensereasoning. Secondly,thebenchmarkdatasets
maynotbecomprehensiveenoughtocoverallpossibledomainsandscenarios,leadingtoapotential
biastowardscertaintypesofquestionsortasks. Thirdly,duetotherapidlyevolvingnatureofthe
field, modelsmayquicklybecomeoutdated, meaningtheleaderboardmaynotalwaysreflectthe
mostcurrentstateoftheart. Sinceourbenchmarkutilizespublicdatasetsandourcorpusconsistsof
questionsandanswers,userprivacyconcernsareminimal.
References
[1] AI@Meta. Llama3modelcard. 2024.
[2] Anthropic. Modelcardandevaluationsforclaudemodels,2023.
[3] Anthropic. https://www.anthropic.com/claude,2024.
[4] JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,XiaodongDeng,YangFan,Wenbin
Ge,YuHan,FeiHuang,BinyuanHui,LuoJi,MeiLi,JunyangLin,RunjiLin,DayihengLiu,
GaoLiu,ChengqiangLu,KemingLu,JianxinMa,RuiMen,XingzhangRen,XuanchengRen,
ChuanqiTan,SinanTan,JianhongTu,PengWang,ShĳieWang,WeiWang,ShengguangWu,
BenfengXu,JinXu,AnYang,HaoYang,JianYang,ShushengYang,YangYao,BowenYu,
HongyiYuan,ZhengYuan,JianweiZhang,XingxuanZhang,YichangZhang,ZhenruZhang,
ChangZhou,JingrenZhou,XiaohuanZhou,andTianhangZhu. Qwentechnicalreport. arXiv
preprintarXiv:2309.16609,2023.
[5] EdwardBeeching,ClémentineFourrier,NathanHabib,SheonHan,NathanLambert,Nazneen
Rajani,OmarSanseviero,LewisTunstall,andThomasWolf. Openllmleaderboard. https:
//huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard,2023.
[6] StellaBiderman,HaileySchoelkopf,QuentinGregoryAnthony,HerbieBradley,KyleO’Brien,
EricHallahan,MohammadAflahKhan,ShivanshuPurohit,USVSNSaiPrashanth,Edward
Raff,etal. Pythia: Asuiteforanalyzinglargelanguagemodelsacrosstrainingandscaling. In
InternationalConferenceonMachineLearning,pages2397–2430.PMLR,2023.
[7] YonatanBisk,RowanZellers,RonanLebras,JianfengGao,andYejinChoi. Piqa: Reasoning
about physical commonsense in natural language. Proceedings of the AAAI Conference on
ArtificialIntelligence,34(05):7432–7439,Apr.2020.
10[8] SidBlack,GaoLeo,PhilWang,ConnorLeahy,andStellaBiderman. GPT-Neo: LargeScale
AutoregressiveLanguageModelingwithMesh-Tensorflow,March2021.Ifyouusethissoftware,
pleaseciteitusingthesemetadata.
[9] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,etal.
Languagemodelsarefew-shotlearners. InH.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,
andH.Lin, editors, AdvancesinNeuralInformationProcessingSystems, volume33, pages
1877–1901.CurranAssociates,Inc.,2020.
[10] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,
SiyuanZhuang,YonghaoZhuang,JosephE.Gonzalez,IonStoica,andEricP.Xing. Vicuna:
Anopen-sourcechatbotimpressinggpt-4with90%*chatgptquality,March2023.
[11] JohnChung,EceKamar,andSaleemaAmershi.Increasingdiversitywhilemaintainingaccuracy:
Textdatagenerationwithlargelanguagemodelsandhumaninterventions. InProceedingsof
the61stAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: Long
Papers),2023.
[12] PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,AshishSabharwal,CarissaSchoenick,
andOyvindTafjord. Thinkyouhavesolvedquestionanswering? tryarc, theai2reasoning
challenge. ArXiv,abs/1803.05457,2018.
[13] JacobCohen. Acoefficientofagreementfornominalscales. EducationalandPsychological
Measurement,20(1):37–46,1960.
[14] Nolan Dey, Gurpreet Gosal, Zhiming, Chen, Hemant Khachane, William Marshall, Ribhu
Pathria,MarvinTom,andJoelHestness. Cerebras-gpt: Opencompute-optimallanguagemodels
trainedonthecerebraswafer-scalecluster. arXivpreprint:2304.03208,2023.
[15] LeoGao,JonathanTow,BaberAbbasi,StellaBiderman,SidBlack,AnthonyDiPofi,Charles
Foster,LaurenceGolding,JeffreyHsu,AlainLeNoac’h,HaonanLi,KyleMcDonell,Niklas
Muennighoff,ChrisOciepa,JasonPhang,LariaReynolds,HaileySchoelkopf,AviyaSkowron,
LintangSutawika,EricTang,AnishThite,BenWang,KevinWang,andAndyZou.Aframework
forfew-shotlanguagemodelevaluation,122023.
[16] Google. https://ai.google.dev/,2023.
[17] DirkGroeneveld, IzBeltagy, PeteWalsh, AkshitaBhagia, RodneyKinney, OyvindTafjord,
Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David
Atkinson, Russell Authur, Khyathi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar,
YulingGu,JackHessel,TusharKhot,WilliamMerrill,JacobMorrison,NiklasMuennighoff,
AakankshaNaik,CrystalNam,MatthewE.Peters,ValentinaPyatkin,AbhilashaRavichander,
DustinSchwenk,SaurabhShah,WillSmith,NishantSubramani,MitchellWortsman,Pradeep
Dasigi,NathanLambert,KyleRichardson,JesseDodge,KyleLo,LucaSoldaini,NoahA.Smith,
andHannanehHajishirzi. Olmo: Acceleratingthescienceoflanguagemodels. Preprint,2024.
[18] DanHendrycks,CollinBurns,StevenBasart,AndyZou,MantasMazeika,DawnSong,and
JacobSteinhardt. Measuringmassivemultitasklanguageunderstanding. Proceedingsofthe
InternationalConferenceonLearningRepresentations(ICLR),2021.
[19] GuokunLai,QizheXie,HanxiaoLiu,YimingYang,andEduardHovy. RACE:Large-scale
ReAdingcomprehensiondatasetfromexaminations. InProceedingsofthe2017Conferenceon
EmpiricalMethodsinNaturalLanguageProcessing,2017.
[20] RaymondLi,LoubnaBenAllal,YangtianZi,NiklasMuennighoff,DenisKocetkov,Chenghao
Mou,MarcMarone,ChristopherAkiki,etal. Starcoder: maythesourcebewithyou! arXiv
preprintarXiv:23.05.061161,2023.
[21] XuechenLi,TianyiZhang,YannDubois,RohanTaori,IshaanGulrajani,CarlosGuestrin,Percy
Liang,andTatsunoriB.Hashimoto.Alpacaeval: Anautomaticevaluatorofinstruction-following
models. https://github.com/tatsu-lab/alpaca_eval,2023.
[22] YixinLiu,KejianShi,KatherineSHe,LongtianYe,AlexanderR.Fabbri,PengfeiLiu,Dragomir
Radev,andArmanCohan. Onlearningtosummarizewithlargelanguagemodelsasreferences.
arXivpreprintarXiv:2305.14239,2023.
[23] TodorMihaylov,PeterClark,TusharKhot,andAshishSabharwal. Canasuitofarmorconduct
electricity? a new dataset for open book question answering. In Proceedings of the 2018
ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,2018.
11[24] Mistral. https://chat.mistral.ai/chat,2024.
[25] MosaicML. Mpt-1b redpajama-200b. https://huggingface.co/mosaicml/
mpt-1b-redpajama-200b. Accessed: 2024-04-29.
[26] OpenAI. Openaievals. https://github.com/openai/evals.
[27] OpenAI. https://chat.openai.com/chat,2022.
[28] OpenAI. Gpt-4technicalreport. arxivpreprintarXiv:2303.08774,2024.
[29] AnkitPal,LogeshKumarUmapathi,andMalaikannanSankarasubbu. Medmcqa: Alarge-scale
multi-subjectmulti-choicedatasetformedicaldomainquestionanswering. InProceedingsof
theConferenceonHealth,Inference,andLearning,2022.
[30] PouyaPezeshkpourandEstevamHruschka. Largelanguagemodelssensitivitytotheorderof
optionsinmultiple-choicequestions. arXivpreprintarXiv:2308.11483,2023.
[31] JoshuaRobinson,ChristopherRytting,andDavidWingate. Leveraginglargelanguagemodels
formultiplechoicequestionanswering. ArXiv,abs/2210.12353,2022.
[32] JoshuaRobinson,ChristopherMichaelRytting,andDavidWingate. Leveraginglargelanguage
modelsformultiplechoicequestionanswering. arXivpreprintarXiv:2210.12353,2023.
[33] BaptisteRozière,JonasGehring,FabianGloeckle,StenSootla,ItaiGat,XiaoqingEllenTan,
YossiAdi, JingyuLiu, RomainSauvestre, TalRemez, JérémyRapin, ArtyomKozhevnikov,
IvanEvtimov,JoannaBitton,ManishBhatt,CristianCantonFerrer,AaronGrattafiori,Wenhan
Xiong,AlexandreDéfossez,JadeCopet,FaisalAzhar,HugoTouvron,LouisMartin,Nicolas
Usunier,ThomasScialom,andGabrielSynnaeve. Codellama: Openfoundationmodelsfor
code. arXivpreprintarXiv:2308.12950,2024.
[34] KeisukeSakaguchi,RonanLeBras,ChandraBhagavatula,andYejinChoi. Winogrande: an
adversarialwinogradschemachallengeatscale. Commun.ACM,64(9):99–106,aug2021.
[35] ZhiqiangShen,TianhuaTao,LiqunMa,WillieNeiswanger,JoelHestness,NataliaVassilieva,
Daria Soboleva, and Eric Xing. Slimpajama-dc: Understanding data combinations for llm
training. arXivpreprintarXiv:2309.10818,2023.
[36] AlonTalmor,JonathanHerzig,NicholasLourie,andJonathanBerant. CommonsenseQA:A
questionansweringchallengetargetingcommonsenseknowledge. InProceedingsofthe2019
ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:
HumanLanguageTechnologies,Volume1(LongandShortPapers),2019.
[37] RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,XuechenLi,CarlosGuestrin,Percy
Liang,andTatsunoriB.Hashimoto. Stanfordalpaca: Aninstruction-followingllamamodel.
https://github.com/tatsu-lab/stanford_alpaca,2023.
[38] Gemini Team. Gemini: A family of highly capable multimodal models. arXiv preprint
arXiv:2312.11805,2023.
[39] GemmaTeam. Gemma: Openmodelsbasedongeminiresearchandtechnology. arXivpreprint
arXiv:2403.08295,2024.
[40] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timothée
Lacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Openand
efficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.
[41] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Open
foundationandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
[42] PeiyiWang, LeiLi, LiangChen, ZefanCai, DaweiZhu, BinghuaiLin, YunboCao, QiLiu,
TianyuLiu,andZhifangSui. Largelanguagemodelsarenotfairevaluators. arXivpreprint
arXiv:2305.17926,2023.
[43] ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,Anthony
Moi,PierricCistac,TimRault,RémiLouf,MorganFuntowicz,andJamieBrew. Huggingface’s
transformers: State-of-the-artnaturallanguageprocessing. CoRR,abs/1910.03771,2019.
[44] RowanZellers,AriHoltzman,YonatanBisk,AliFarhadi,andYejinChoi. HellaSwag: Can
a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the
AssociationforComputationalLinguistics,2019.
12[45] BiaoZhang,BarryHaddow,andAlexandraBirch. Promptinglargelanguagemodelformachine
translation: Acasestudy. InProceedingsofthe40thInternationalConferenceonMachine
Learning,2023.
[46] PeiyuanZhang,GuangtaoZeng,TianduoWang,andWeiLu. Tinyllama: Anopen-sourcesmall
languagemodel. arXivpreprint:2401.02385,2024.
[47] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,
ChristopherDewan, MonaDiab, XianLi, XiVictoriaLin, TodorMihaylov, MyleOtt, Sam
Shleifer,KurtShuster,DanielSimig,PunitSinghKoura,AnjaliSridhar,TianluWang,andLuke
Zettlemoyer. Opt: Openpre-trainedtransformerlanguagemodels. arXivpreprint:2205.01068,
2022.
[48] ChujieZheng,HaoZhou,FandongMeng,JieZhou,andMinlieHuang. Largelanguagemodels
arenotrobustmultiplechoiceselectors. arXivpreprintarXiv:2309.03882,2024.
[49] LianminZheng,Wei-LinChiang,YingSheng,SiyuanZhuang,ZhanghaoWu,YonghaoZhuang,
ZiLin,ZhuohanLi,DachengLi,EricP.Xing,HaoZhang,JosephE.Gonzalez,andIonStoica.
Judgingllm-as-a-judgewithmt-benchandchatbotarena. arXivpreprintarXiv:2306.05685,
2023.
[50] WenhaoZhu,HongyiLiu,QingxiuDong,JingjingXu,ShujianHuang,LingpengKong,Jiajun
Chen, andLeiLi. Multilingualmachinetranslationwithlargelanguagemodels: Empirical
resultsandanalysis. arXivpreprint2304.04675,2023.
13Appendix
A ReproducibilityStatement
We will make all our filtered open-style data (MMLU, ARC, HellaSwag, WinoGrande, PIQA,
CommonsenseQA,Race,MedMCQA,andOpenbookQA)usedinourexperimentsofSec.5and
preprocessingscriptspubliclyavailable. DetaileddatastatisticsareprovidedinSec.4.1. Considering
thepotentialhighcostsassociatedwithgatheringandreproducingourLLMresponsedatafromthe
groundup,wewillmakeavailableallresponsesfromthevariousLLMsandtheircorresponding
evaluationresultstosupportandsimplifythereproducibilityofourwork. TheOpenAIAPIswe
usedincludegpt-3.5-turbo-1106,gpt-4.0-1106-preview,gpt-4o(forresponsecollection),andgpt-4.0-
0125-preview(forfilteringandpost-evaluation);Claude3: claude-3-opus-20240229;Gemini-Pro:
gemini-pro,andMistral: mistral-large-latest.
B MoreResultsonGeminiProandStage1Filtering
Figure 6: Performance comparison of Gemini Pro on multiple-choice and open-style response
questionsacrossdiversedatasets,asshownbythecountofcorrectandincorrectanswersintheleft
barchartandmodelaccuracyintherightradarchart.
C MoreSampleQuestions
Weprovidemoresamplequestionsthatarecorrectforopen-styleandincorrectformultiplechoicein
Table7,andviceversainTable8. Theseexamplesdemonstratehowthemodel’sabilityvariesin
handlingdifferentquestionformats.
D PromptsforDifferentDatasets
Thefollowingareexamplesofpromptsfordifferentdatasets. Theyallowmodelstobetterunderstand
andrespondtospecificcontextsandobjectivesineachdataset.
14YES
ARC CommonsenseQA HellaSwag NO
14.6% 46.3%
5.1%
94.9%
85.4% 53.7%
MedMCQA MMLU OpenbookQA
51.2%
58.1% 62.8%
41.9% 37.2%
48.8%
PIQA Race WinoGrande
29.6%
0.0%
64.6%
35.4%
100.0%
70.4%
Figure7: Initialfilteringresultsacrossdatasets,showcasingthepercentagesofquestionsthatare
suitable(Yes)foropen-styleformattingandthosethatarenot(No).
Table7: ExampleofanIncorrectMCQandaCorrectOSQinMMLU.
Question that is correct for OSQ and incorrect for MCQ: How many CO2 and ATP molecules
are formed during one complete turn of the tricarboxylic acid cycle (Krebs’ cycle)?
Options: A. 2CO2 and 2ATP B. 2CO2 and 16ATP C. 2CO2 and 12ATP D. 2CO2 and 1ATP
Ground Truth: D
MCQ_Answer: A
OSQ_Answer: During one complete turn of the tricarboxylic acid cycle (also known as
the Krebs’ cycle or the citric acid cycle), a series of chemical reactions occur in
the mitochondria of cells that result in the production of carbon dioxide (CO2) and
adenosine triphosphate (ATP), among other molecules.
For each acetyl-CoA molecule that enters the cycle, the following are produced:
- 2 molecules of CO2: Each acetyl-CoA contributes two carbons to the cycle, which
are eventually released as two molecules of carbon dioxide.
- 1 molecule of ATP: One molecule of ATP (or an equivalent, such as GTP, depending
on the cell type) is produced directly in the cycle through substrate-level
phosphorylation.
However, the cycle also produces reduced coenzymes, specifically NADH and FADH2,
which are used in the electron transport chain to produce additional ATP through
oxidative phosphorylation. For each acetyl-CoA that enters the cycle:
- 3 molecules of NADH are produced, which can be used to generate approximately 2.5
ATP each through the electron transport chain.
- 1 molecule of FADH2 is produced, which can be used to generate approximately 1.5
ATP through the electron transport chain.
Therefore, while only 1 ATP is produced directly in the cycle per acetyl-CoA, the
NADH and FADH2 generated can lead to the production of additional ATP in the electron
transport chain. The exact number of ATP molecules produced can vary depending on the
efficiency of the electron transport chain and the ATP synthase complex in different
organisms and cell types.
15Table8: ExampleofaCorrectMCQandanIncorrectOSQinMMLU.
Question that is correct for MCQ and incorrect for OSQ: A tree is a connected graph
with no cycles. How many nonisomorphic trees with 5 vertices exist?
Options: A. 1 B. 2 C. 3 D. 4
Ground Truth: C
MCQ_Answer: C
OSQ_Answer: To determine the number of nonisomorphic trees with 5 vertices, we can
use Cayley’s formula, which states that the number of trees on 𝑛 labeled vertices
is 𝑛𝑛−2. However, this formula gives the number of trees when vertices are labeled,
meaning that it counts different labelings of the same tree shape as distinct. Since
we are interested in nonisomorphic trees (which do not consider different labelings as
distinct), we cannot directly use Cayley’s formula.
Instead, we need to enumerate the trees by considering their possible structures. For
a tree with 5 vertices, we can have the following configurations:
1. A linear chain (path) where each vertex is connected to two others except for the
two end vertices, which are connected to only one other vertex. This is one possible
tree.
2. A "star" tree where one central vertex is connected to the other four vertices.
This is another possible tree.
3. A tree where one vertex is connected to three others, and one of those three is
connected to the fifth vertex. This structure can be seen as a "T" shape with an
extra branch on one of the endpoints. There are three nonisomorphic trees of this
type because the branch can be on any of the three endpoints.
4. A tree where two vertices are connected to a central vertex, and each of those two
vertices is connected to one other vertex (creating a structure with a central vertex,
two intermediate vertices, and two leaf vertices). This structure can be seen as a
"Y" shape. There is only one nonisomorphic tree of this type because any permutation
of the leaf vertices does not create a fundamentally different tree structure.
Adding these up, we have:
1 (linear chain) + 1 (star) + 3 (T-shaped with an extra branch) + 1 (Y-shaped) = 6
nonisomorphic trees with 5 vertices.
Table9: PromptforMMLUdataset.
MCQPrompt: The following is the multiple choice question about abstract algebra. Please
select the correct answer from the options A, B, C, D. For example, if you think the
correct answer is A, your response should be ’A’.
Question: Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18))
over Q.
A. 0
B. 4
C. 2
D. 6
Answer:
Open-StylePrompt: Answer the following question about abstract algebra.
Question: Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18))
over Q.
Answer:
16Table10: PromptforARCdataset.
MCQ Prompt: The following is the multiple choice question. Please select the correct
answer from the options A, B, C, D. For example, if you think the correct answer is A,
your response should be ’A’.
Question: Tiny organisms called plankton live in oceans. Some plankton can take energy
from the Sun and turn it into food. How are plankton most useful to the animals that
live in the ocean?
A. Plankton are colorful.
B. Plankton clean the water.
C. Plankton release oxygen.
D. Plankton reproduce quickly.
Answer:
Open-StylePrompt: Answerthefollowingquestion.
Question: Tiny organisms called plankton live in oceans. Some plankton can take energy
from the Sun and turn it into food. How are plankton most useful to the animals that
live in the ocean?
Answer:
Table11: PromptforCommonsenseQAdataset.
MCQ Prompt: The following is the multiple choice question. Please select the correct
answer from the options A, B, C, D, E. For example, if you think the correct answer is A,
your response should be ’A’.
Question: A revolving door is convenient for two direction travel, but it also serves as
a security measure at a what?
A. bank
B. library
C. department store
D. mall
E. New York
Answer:
Open-Style Prompt: You will be presented with a variety of questions that require an
understanding of everyday scenarios, human behaviors, and common sense. Your task is
to provide the best possible answer to each question based solely on your understanding
and reasoning.
Question: A revolving door is convenient for two direction travel, but it also serves as
a security measure at a what?
Answer:
Table12: PromptforMedMCQAdataset.
MCQ Prompt: The following is the multiple choice question about medicine. Please select
the correct answer from the options A, B, C, D. For example, if you think the correct
answer is A, your response should be ’A’.
Question: Modulus of elasticity means:
A. Rigidity or stiffness of the material
B. Ability to be stretched with permanent deformation
C. Ductility of a material
D. Malleability of the metal
Answer:
Open-StylePrompt: Answer the following question about medicine.
Question: Modulus of elasticity means:
Answer:
17Table13: PromptforHellaSwagdataset.
MCQ Prompt: The following is the multiple choice question. Please select the correct
answer from the options A, B, C, D. For example, if you think the correct answer is A,
your response should be ’A’.
Question: How to clean your rv windows and mirrors fast without using any spray. you
A. also have a bucket that you spray paint a window in.
B. can reach for a running water hose and clean the inside of your rv quickly.
C. get a wash cloth and you put it under the faucet to get wet and then you rinse it out
so it’s not soaking.
D. meticulously clean the window in the glass shop and then take the plastic off and
start taking the hood off.
Answer:
Open-Style Prompt: Imagine you are provided with a scenario or a partial story taken from
everyday life or a common activity. Your task is to continue this story or scenario in a
way that makes the most sense based on what typically happens in such situations. Please
complete the sentence.
Question: How to clean your rv windows and mirrors fast without using any spray. you
Answer:
Table14: PromptforOpenbookQAdataset.
MCQ Prompt: The following is the multiple choice question. Please select the correct
answer from the options A, B, C, D. For example, if you think the correct answer is A,
your response should be ’A’.
Question: what system is needed for a body to get its needed supply of the gas humans
breathe in?
A. the circulatory system
B. the digestive system
C. the school system
D. central nervous system
Answer:
Open-Style Prompt: Consider common scenarios or outcomes that fit the context of the
sentence. Attempt to logically complete the sentences based on common knowledge and
reasoning.
Question: what system is needed for a body to get its needed supply of the gas humans
breathe in?
Answer:
Table15: PromptforPIQAdataset.
MCQ Prompt: The following is the multiple choice question. Please select the correct
answer from the options A, B. For example, if you think the correct answer is A, your
response should be ’A’.
Question: How do I ready a guinea pig cage for it’s new occupants?
A. Provide the guinea pig with a cage full of a few inches of bedding made of ripped
paper strips, you will also need to supply it with a water bottle and a food dish.
B. Provide the guinea pig with a cage full of a few inches of bedding made of ripped
jeans material, you will also need to supply it with a water bottle and a food dish.
Answer:
Open-Style Prompt: Consider common scenarios or outcomes that fit the context of the
sentence. Attempt to logically complete the sentences based on common knowledge and
reasoning.
Question: How do I ready a guinea pig cage for it’s new occupants?
Answer:
18Table16: PromptforRacedataset.
MCQ Prompt: I will give you a passage with multiple-choice question. Please select the
correct answer from the options A, B, C, D. For example, if you think the correct answer
is A, your response should be ’A’.
Passage:...
Question: What did Nancy try to do before she fell over?
A. Measure the depth of the river
B. Look for a fallen tree trunk
C. Protect her cows from being drowned
D. Run away from the flooded farm
Answer:
Open-StylePrompt: I will give you passage with question. Please, answer the question.
Passage:...
Question: What did Nancy try to do before she fell over?
Answer:
Table17: PromptforWinoGrandedataset.
MCQ Prompt: The following is the multiple choice question. Please put the correct words
in place of _. Your response should include only the option without any justification or
reasoning. Please select the correct answer from the options A, B.
Question: Sarah was a much better surgeon than Maria so _ always got the easier cases.
A. Sarah
B. Maria
Answer:
Open-StylePrompt:Please put the correct words in place of _. Give only the word that fits
the sentence.
Question: Sarah was a much better surgeon than Maria so _ always got the easier cases.
Answer:
19