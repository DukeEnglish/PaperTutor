Simple and Effective
Masked Diffusion Language Models
SubhamSekharSahoo MarianneArriola YairSchiff
CornellTech,NYC,USA. CornellTech,NYC,USA. CornellTech,NYC,USA.
ssahoo@cs.cornell.edu ma2238@cornell.edu yzs2@cornell.edu
AaronGokaslan EdgarMarroquin JustinTChiu
CornellTech,NYC,USA. CornellTech,NYC,USA. CornellTech,NYC,USA.
akg87@cs.cornell.edu emm392@cornell.edu jtc257@cornell.edu
AlexanderRush VolodymyrKuleshov
CornellTech,NYC,USA. CornellTech,NYC,USA.
ar459@cornell.edu kuleshov@cornell.edu
Abstract
Whilediffusionmodelsexcelatgeneratinghigh-qualityimages,priorworkreports
asignificantperformancegapbetweendiffusionandautoregressive(AR)methods
inlanguagemodeling.Inthiswork,weshowthatsimplemaskeddiscretediffusion
is more performant than previously thought. We apply an effective training
recipe that improves the performance of masked diffusion models and derive a
simplified,Rao-Blackwellizedobjectivethatresultsinadditionalimprovements.
Our objective has a simple form—it is a mixture of classical masked language
modeling losses—and can be used to train encoder-only language models that
admitefficientsamplers,includingonesthatcangeneratearbitrarylengthsoftext
semi-autoregressivelylikeatraditionallanguagemodel. Onlanguagemodeling
benchmarks,arangeofmaskeddiffusionmodelstrainedwithmodernengineering
practicesachievesanewstate-of-the-artamongdiffusionmodels,andapproaches
ARperplexity.Wereleaseourcodeat:https://github.com/kuleshov-group/mdlm
1 Introduction
Diffusion models excel at producing realistic, high-quality images and have received significant
attentionaspotentialtoolsforgeneratingdiscretedatasuchastext[1,23,25],biologicalsequences
[2],andgraphs[45,48]. Unlikeautoregressive(AR)approaches,diffusion-basedmethodsarenot
constrained to generate data sequentially, and therefore have the potential to improve long-term
planning,controllablegeneration,andsamplingspeed.However,discretediffusionmethodsexhibita
performancegaprelativetoARmodels[1,17,19,25],especiallyinlanguagemodeling.Thestandard
measureoflanguagemodelingperformanceislog-likelihood:whencontrollingforparametercount,
priorworkreportsasizablelog-likelihoodgapbetweenARanddiffusionmodels.
In this work, we show that simple masked diffusion language modeling (MDLM) combined
witheffectivetrainingrecipesismoreperformantthanpreviouslythought[1,19]. Wedevelopa
well-engineeredMDLMimplementationthatsignificantlyimprovesdiscretediffusionlog-likelihood;
we further improve likelihood using a simple substitution-based parameterization of the reverse
diffusionprocessthatenablesderivingaRao-Blackwellizedcontinuous-timevariationallowerbound
(ELBO)withimprovedtightness.Interestingly,ourobjectivehasasimpleform:itisaweightedaverage
Preprint.Underreview.
4202
nuJ
11
]LC.sc[
1v42570.6042:viXraFigure1:(Left)Ourproposedmaskeddiffusionlanguagemodel(MDLM)istrainedusingaweighted
averageofmaskedcrossentropylosses.(TopRight)Incomparisontomaskedlanguagemodels(MLM),
MDLM’sobjectivecorrespondtoaprincipledvariationallowerbound,andsupportsgenerationvia
ancestralsampling.(BottomRight)Perplexity(PPL)onOneBillionWordsbenchmark.
ofmaskedlanguagemodeling(MLM)losses[10],andcanbeusedtoendowBERT-style,encoder-only
models with principled generation capabilities. We complement this framework with efficient
samplers—includingonesthatcangeneratesemi-autoregressivelylikeatypicallanguagemodel.
Ourmaskeddiffusionmodelsachieveanewstate-of-the-artamongdiffusionmodelsonlanguage
modelingbenchmarksandapproachtheperplexityofARmodelswithin15-25%.Surprisingly,simple
engineeringchoicessignificantlyimproveperformanceinbothourmodelsandsimplebaselinesthat
werepreviouslythoughttoperformpoorly. Ourframeworkalsoextendstonon-languagedomains,
includingbiologicalsequencemodeling. Wepre-trainDNAsequencemodelsandobservesimilar
orhigherdownstreamperformancecomparedtoclassicalBERT-styletraining,whilealsointroducing
generativecapabilitiesthatclassicalmaskedDNAlanguagemodelslack.
Contributions Wedescribe(1)asimplemaskeddiffusionlanguagemodeling(MDLM)framework
withawell-engineeredimplementationthatoutperformsallexistingdiffusionmodelsacrosslanguage
modeling benchmarks (LM1B [5], OWT [13], DNA [38]), and that significantly improves the
performanceofexistingbaselines[1,19].OurMDLMframeworkimplements(2a)asubstitution-based
parameterization(SUBS)ofthereverseunmaskingdiffusionprocess;SUBSallowsustoderive(2b)
asimple,continuous-time,Rao-Blackwellizedobjectivethatimprovestightnessandvarianceofthe
ELBO,furtherincreasingperformance.WecomplementMDLMwith(3)fastsamplersthatsupport
semi-autoregressive(SAR)generationandoutperformpreviousSARmodels.
2 Background
2.1 DiffusionModels
Diffusionmodelsaretrainedtoiterativelyundoaforwardcorruptionprocessqthattakescleandata
xdrawnfromthedatadistributionq(x)anddefineslatentvariablesz fort∈[0,1]thatrepresent
t
progressivelynoisyversionsofx[20,39,41,37].Thestandardforwardprocessforcontinuousxis
√ √
z = α ·x+ 1−α ·ϵ (1)
t t t
whereϵ∼N(0,I)and(α ) isanoiseschedule,monotonicallydecreasingint.Theparameterized
t t∈[0,1]
reversediffusionmodelp overxandz istrainedtomaximizeavariationallowerboundonlog-
θ t
likelihood(ELBO).GivenanumberofdiscretizationstepsT,definings(i)=(i−1)/T andt(i)=i/T,
andusingD [·]todenotetheKullback–Leiblerdivergence,theNegativeELBO(NELBO)equals[39]:
KL
(cid:34) T (cid:35)
(cid:88)
E −logp (x|z )+ D [q(z |z ,x)∥p (z |z )] +D [q(z |x)∥p (z )] (2)
q θ t(0) KL s(i) t(i) θ s(i) t(i) KL t(T) θ t(T)
(cid:124) (cid:123)(cid:122) (cid:125) i=1 (cid:124) (cid:123)(cid:122) (cid:125)
L recons (cid:124) (cid:123)(cid:122) (cid:125) L prior
L
diffusion
2Forbrevity,wedropifromt(i)ands(i)below;ingeneral,swilldenotethetimestepbeforet.
2.2 DiscreteDiffusionModels
Applicationsofdiffusionmodelingtodiscretedatacanbebrokenintotwobroadcategories. First
areworksthatembeddiscretestructuresincontinuousspaceandthenperformtheGaussiandiffusion
definedaboveonthesecontinuousrepresentations[6,11,17,18,22,26,42]. Morerelatedtoour
methodareworksthatdefineadiffusionprocessdirectlyondiscretestructures.D3PM[1]introducesa
frameworkwithaMarkovforwardprocessq(z |z )=Cat(z ;Q z )definedbythemultiplication
t t−1 t t t−1
ofmatricesQ overT discretetimesteps.Thisprocessinducesmarginals
t
q(z |x)=Cat(z ;Q¯ x)=Cat(z ;Q ·Q ···Q x) (3)
t t t t t t−1 1
thatrepresentthediscrete-stateformof(1).Extendingthisformalismtocontinuoustime(asin(1))
reliesoncontinuoustimeMarkovchain(CTMC)theory[4].TheCTMCframeworkinturnsleadsto
generalizationsofthescorematchingperspectiveondiffusionmodeling[40]todiscretedata[25,44].
Notably,SEDD[25]connectsscore-basedapproacheswithELBOmaximization,enablingperformant
likelihood-basedtrainingofscore-basedmodels.
3 SimpleMaskedDiffusionModels
Whilepreviousworkondiscretediffusionsupportsgeneralforwardprocesses(e.g.,generalQ in
t
D3PM),absorbingstate(i.e.,masking)diffusionconsistentlyachievesthebestperformance[1,25].
Inthiswork,insteadofsupportinggeneralnoiseprocesses,wefocusonmaskingandderivetight
Rao-BlackwellizedobjectivesthatoutperformgeneralapproachesanddonotrequireCTMCtheory.
Inthissection,wefirstdefinethediffusionprocessforacategoricalrandomvariable.LaterinSec.3.5,
weextendthisprocesstosequencescontainingmultiplesuchcategoricalvariables. Wedenoteour
overallapproachasMaskedDiffusionLanguageModels(MDLM).
Notation. WedenotescalardiscreterandomvariableswithKcategoriesas‘one-hot’columnvectors
anddefineV ∈{x∈{0,1}K :(cid:80)K x =1}asthesetofallsuchvectors. DefineCat(·;π)asthe
i=1 i
categoricaldistributionoverKclasseswithprobabilitiesgivenbyπ∈∆K,where∆K denotesthe
K-simplex. WealsoassumethattheK-thcategorycorrespondstoaspecial[MASK]tokenandlet
m∈V betheone-hotvectorforthismask,i.e.,m =1. Additionally,let1={1}K and⟨a,b⟩and
K
a⊙brespectivelydenotethedotandHadamardproductsbetweentwovectorsaandb.
3.1 InterpolatingDiscreteDiffusion
Werestrictourattentiontoforwardprocessesqthatinterpolatebetweencleandatax∈Vandatarget
distributionCat(.;π),formingadirectextensionofGaussiandiffusionin(1).Theqdefineasequence
ofincreasinglynoisylatentvariablesz ∈V,wherethetimesteptrunsfromt=0(leastnoisy)tot=1
t
(mostnoisy).Themarginalofz conditionedonxattimetis
t
q(z |x)=Cat(z ;α x+(1−α )π), (4)
t t t t
whereα ∈[0,1]isastrictlydecreasingfunctionint,withα ≈1andα ≈0;seeSuppl.D.1fordetails.
t 0 1
Thisimpliestransitionprobabilitiesq(z |z )=Cat(z ;α z +(1−α )π),whereα =α /α .
t s t t|s s t|s t|s t s
Thisindicatesthatduringeachdiffusionstepfroms→t,afraction(1−α )oftheprobabilitymass
t|s
istransferredtothepriordistributionπ.Thereverseposteriorisgivenas(seeSuppl.15fordetails):
(cid:32) (cid:33)
[α z +(1−α )1π⊤z ]⊙[α x+(1−α )π]
t|s t t|s t s s
q(z |z ,x)=Cat z ; . (5)
s t s α z⊤x+(1−α )z⊤π
t t t t
While (4) and (5) represent a special case of the more general diffusion processes proposed in
D3PM[1],weshowbelowthattheyyieldasimplifiedvariationallowerboundobjectiveandadmit
straightforwardcontinuoustimeextensions.
3.2 MaskedDiffusion
Next,wefocusonmaskingprocessesandderiveasimpleRao-Blackwellizedobjectiveforthischoice
ofq.Thisobjectiveincurslowervarianceduringtrainingandimprovestightness.
33.2.1 ForwardMaskingProcess
Inmasked(i.e.,absorbingstate)diffusion,wesetπ=m.Ateachnoisingstep,t,theinputxtransitions
toa‘masked’statemwithsomeprobability.Ifaninputtransitionstomatanytimet′,itwillremainin
thisstateforallt>t′:q(z |z =m)=Cat(z ;m).AttimeT,allinputsaremaskedwithprobability1.
t t′ t
Themarginaloftheforwardprocess(4)isgivenbyq(z |x)=α x+(1−α )m.Usingpropertiesof
t t t
themaskingprocess,theposteriorq(z |z ,x)simplifies(5);seeSuppl.A.2:
s t
(cid:40)
Cat(z ;z ) z ̸=m,
s t t
q(z |z ,x)= (cid:16) (cid:17) (6)
s t Cat z ;(1−αs)m+(αs−αt)x z =m.
s 1−αt t
3.2.2 ReverseUnmaskingProcess
Thereverseprocessinvertsthenoiseprocessdefinedbyq.Weconsiderbothafinitenumberofsteps
T,aswellasacontinuoustimemodelcorrespondingtoT→∞.Webeginwiththediscrete-timecase
forwhichthegenerativemodelisexpressedasp
(x)=(cid:82)
p (z )p (x|z
)(cid:81)T
p (z |z )dz .
θ z θ 1 θ 0 i=1 θ s t 0:T
Theoptimalformforp (z |z )matchesthetrueposteriorin(6):thisfollowsimmediatelyfromthedef-
θ s t
initionofthediffusionobjectivein(2),whichisasumoftermsoftheformD (q(z |z ,x)∥p (z |z )).
KL s t θ s t
However, (6) is conditioned on x, which we do not know. Therefore, we introduce a model
x (z ,t):V×[0,1]→∆Kthatapproximatesxwithaneuralnetwork.Wecanalsoomitexplicitdepen-
θ t
denceofx ontimet,whichsimplifiessampling,yieldinga2xinferencespeed-up(seeSuppl.D.2).
θ
3.2.3 SUBSParameterization
Thespecificparameterizationforp (z |z )thatweuseis
θ s t
(cid:40)
Cat(z ;z ), z ̸=m,
s t t
p (z |z )=q(z |z ,x=x (z ,t))= (cid:16) (cid:17) (7)
θ s t s t θ t Cat z ;(1−αs)m+(αs−αt)xθ(zt,t) . z =m,
s 1−αt t
Furthermore,weinduce2keypropertiesoftheabsorbingstatediffusionprocessintoourdenoising
model,x (z ,t): aninputtokenremainsunchangedduringreversediffusion,andthecleaninput
θ t
isnevermaskedWeimplementtheseassubstitutionstotheoutputofx (z ,t),hencewecallour
θ t
parameterizationSUBS.
ZeroMaskingProbabilities First,noticethatbydefinition,⟨x,m⟩=0.Forthisreason,wedesign
thedenoisingnetworksuchthat⟨x (z ,t),m⟩=0,i.e.,wesubstitutethelogitindexcorresponding
θ t
tothe[MASK]tokenwith−∞.
Carry-OverUnmasking Second,ifz isunmasked,thenwedesirex (z ,t)=z ,i.e.,unmasked
t θ t t
latentsare‘carriedover’.Weaccomplishthisbysubstitutingtheoutputofournetworktosimplycopy
unmaskedinputs.
InSuppl.B.1,weshowthat“ZeroMaskingProbabilities”propertysimplifiestheD3PM’sNELBO(38)
to(40),and“Carry-OverUnmasking”futhersimplifies(40)to(42)whosecontinuoustimeequivalent
isthesimplifiedNELBO(10).Table8showsthateachsimplificationleadstoanimprovedlikelihood.
3.3 Rao-BlackwellizedLikelihoodBounds
Recallfrom(2)thatthediffusiontraningobjectivehastheformL +L +L . Forthe
recons diffusion prior
simplifiedreverseprocessin(7),thediscrete-timediffusionlossforfiniteT simplifiesto(Suppl.B.1.3):
L
=(cid:88)T
E [D (q(z |z ,x)∥p (z |z
))]=(cid:88)T
E
(cid:20)α t(i)−α
s(i) log⟨x (z
),x⟩(cid:21)
.
diffusion q KL s(i) t(i) θ s(i) t(i) q 1−α θ t(i)
t(i)
i=1 i=1
(8)
Notethatthisobjectiveissimplerandmorewell-behavedthantheexpressiononewouldobtainfor
D (q(z |z ,x)∥p (z |z ))undertheparameterizationinducedbyusingp (z |z )=q(z |z ,x=
KL s t θ s t θ s t s t
x (z ,t))from(5),whichissimilartowhatisusedbyD3PM[1](seeSuppl.A.2.4):
θ t
(cid:20) (cid:21)
α −α α ⟨x (z ,t),m⟩+(1−α ) 1−α (1−α )(α ⟨x (z ,t),m⟩+(1−α ))
s tlog t θ t t + slog s t θ t t ⟨z ,m⟩. (9)
1−α (1−α )⟨x (z ,t),x⟩ 1−α (1−α )(α ⟨x (z ,t),m⟩+(1−α )) t
t t θ t t t s θ t s
4Werefertotheprocessofobtaining(8)inlieuof(9)asaformofRao-Blackwellization.Specifically,
weanalyticallycomputeexpectationssuchas⟨x (z ,t),m⟩=0inordertosimplifyobjective(9)to
θ t
obtain(8).Withoutanalyticalsimplifications,amodelmustlearnθsuchthat⟨x (z ,t),m⟩=0holds.
θ t
UnlikeinregularRao-Blackwellization,simplificationsarepossiblebecauseofmodelingchoices
forx (z ,t)(zeromaskingprobabilitiesandcarry-overunmasking).Inthatsense,ourapproachhas
θ t
similaritiestographicalmodeling,whereincorporatingconditionalindependenciesintop setscertain
θ
log-likelihoodtermstozero. However,ourapproachalsoempiricallyhelpsreducevariance,hence
werefertoitasRao-Blackwellization,somewhatabusingtheusualterminology.
3.4 Continuous-TimeLikelihoodBounds
PreviousworkshaveshownempiricallyandmathematicallythatincreasingthenumberofstepsT
yieldsatighterapproximationtotheELBO[21].Followingasimilarargument,weformancontinuous
extensionof(8)bytakingT→∞(seeSuppl.B.2),whichyieldsthefollowingNELBO,L∞ :
NELBO
(cid:90) t=1 α′
L∞ =E t log⟨x (z ,t),x⟩dt (10)
NELBO q 1−α θ t
t=0 t
Invariancetothenoiseschedule Thefunctionα isinvertibleduetothemonotonicityassumption
t
in Sec. 3.1, and so we can perform the following change of variables in (10): γ ≡ log(1−α ).
t
Thus,thediffusionlosscanbeequivalentlyexpressedasL∞ =−E (cid:82)γ=0 log⟨x (z ,γ),x⟩dγ;
NELBO q γ=−∞ θ γ
seeSuppl.D.1.1fordetails. Thisnewformulationdemonstratesthatthediffusionlossisinvariant
tothefunctionalformofα ,whichweverifyempiricallyinSuppl.D.1.
t
3.5 MaskedDiffusionLanguageModels
Next, we apply masked diffusion to language modeling over sequences x1:L of L tokens, with
xℓ denotingtheℓ-thtoken. Wemaketheassumptionthattheforwardnoisingprocessisapplied
independentlyacrossasequenceandthat,conditionedonasequenceoflatentsz1:L,thedenoising
t
processfactorizesindependentlyacrosstokens,i.e.,p (z1:L|z1:L)=(cid:81)L p (zℓ|z1:L).Thus,we
θ s t ℓ=1 θ s t
useasinglemodeltocomputexℓ(z1:L,t)foreachℓfromamaskedsequencez ,optimizing:
θ t t
(cid:90) t=1 α′ (cid:88)
L∞ =E t log⟨xℓ(z ),xℓ⟩dt (11)
NELBO q 1−α θ t
t=0 t ℓ
Interestingly,ourobjectivehasasimpleform:itistheweightedaverageofmaskedlanguagemodeling
(MLM)losses[10].Thusourworkestablishesaconnectionbetweengenerativediffusionmodelsand
encoder-onlyBERTmodels.Ourobjectiveenablesprincipledselectionofa(randomized)masking
rate,andalsoendowsBERT-stylemodelswithprincipledgenerationcapabilities,seeSec.6.
3.5.1 TrainingConsiderationsforMaskedDiffusion
Oneofthekeycontributionsofourworkisawell-engineeredimplementationofmaskeddiffusion
models.Ourexperimentsdemonstratethattheseimprovementsgreatlyboostperformanceevenfor
methodspreviouslythoughttoperformpoorly,e.g.,Austinetal.[1].Belowwebrieflysummarizethese
implementationdetails.First,wefindthattokenizationiscriticaltoperformance.Smallvocabularies,
suchasthe8kvocabularyinAustinetal.[1],resultinlonger-rangedependenciesthatdecreasethe
performanceofbothdiffusionandARmodels. Additionally,byfocusingonmaskeddiffusion,we
areabletoprovideanumericallystableimplementationoftheobjectivefunction. Namely,since
previousformulationsofdiscretediffusionwereconstructedtoaccommodateawiderangeoflimiting
distributions[1],theobjectivewasimplementedbymaterializingthefulltransitionmatricesQ¯ and
t
posteriorprobabilities.Incontrast,weevaluateD [q(z |z ,x)||p (z |z )]byexaminingonlythe
KL s t θ s t
maskedtokenindicesratherthancomparingthefulltrueandapproximateposteriordistributions.
Furthermore,wemodernizethearchitectureforthedenoisingnetworkrelativetoD3PM[1].Inlieu
oftheT5architectureusedinD3PM,weusethediffusiontransformer(DiT)introducedinPeebles
&Xie[32],whichintegratestimestepconditioningintoastandardencoder-onlytransformer[47]
andusesrotarypositionalembeddings[43]. Inaddition,weimplementalow-discrepancysampler
thatreducesthevarianceoftheELBO,similartoKingmaetal.[21]anddrawscorrelatedsamples
t ratherthanperformingi.i.d.sampling.
i
54 InferenceandSamplinginMaskedDiffusionLanguageModels
4.1 EfficientAncestralSampling
TogenerateasequenceoflengthL,thereversediffusionprocessstartswiththesequencez1:L where
t=1
zℓ =m,forallℓ∈{1,...,L}. Thenthesubsequentlatents,z1:Laregeneratedbydiscretizingthe
t=1 t
reversediffusionprocesswithsomefiniteT.Givenz1:L,weconstructz1:Lbysamplingeachtoken
t s
zℓindependentlyfromthedistributionp (zℓ|z1:L)givenin(7).
s θ s t
Notethatinthereverseprocess,unmaskedtokensremainunchanged.Thus,ifnonewtokensinz1:L
s
becomeunmasked(whichcanoccurofteninearlydenoisingstagesforlargeT),thenz1:L=z1:L.
s t
Additionallyifthedenoisingmodel,x (z1:L)isnotconditionedontime,thenwecansimplydraw
θ t
anewsamplefromp (z1:L |z1:L)usingthepreviouslycomputedandcachedvaluex (z1:L).This
θ s−1/T s θ t
meanswehaveeffectively‘skipped’overthetimesteps, savingafunctioncalltothedenoising
network.NotethatSEDD[25]doesnotsupportthiscachingbecausethedenoisingnetworkmodels
time-dependentrates,whichrequiresconditioningontime.
4.2 Semi-AutoregressiveMaskedDiffusionLanguageModels
Ourmethodalsoadmitsaneffectivesemi-autoregressive(SAR)decodingmethodthatallowsthemodel
togeneratesequencesofarbitrarylength.Letx˜1:Lrepresenttheoutputfromsamplingasequenceof
Ltokensusingthereversediffusionprocessdescribedabove.TogenerateadditionalL′<Ltokens,we
proposeagenerationalgorithminwhichthelatterL−L′tokensx˜L′:L−L′areusedasaprefixforan
additionalroundofgeneration.Giventhecarry-overunmaskingdescribedinSec.3.2.3,theseprefixto-
kenswillsimplybecopiedoverateachdecodingstep.Theremainingtokensaregeneratedasabovewith
zℓ∼p (zℓ|zL′:L+L′ )forallℓ∈{L+1,...,L+L′},withzL′:L−L′ initializedtox˜L′:L−L′ asopposed
s θ s t 1
tobeinginitializedasmaskedtokensm.Attheendofthisprocess,wehaveproducedL+L′tokens
concat[x˜1:L,x˜L+1:L+L′],whereconcat[·]denotesconcatenationalongthesequencelengthdimension.
Thisprocesscanrepeatindefinitely,withtheprefixshiftedforeverynewroundofgeneration.
5 Experiments
5.1 MaskedDiffusionLanguageModels
ExperimentalSetup WeevaluateMDLMasagenerativemodeloflanguageandasarepresentation
modelviafine-tuningondownstreamtasks.
Forlanguagemodelinglikelihoodevaluation,weconductexperimentsontwodatasets: TheOne
BillionWordsDataset(LM1B;[5])andOpenWebText(OWT;[13]).Weusethebert-base-uncased
tokenizerforOneBillionWords,andreportperplexitiesonthetestsplit.Modelshaveacontextsize
of128. ForOWT,whichdoesnothaveapre-definedsplit,wereservethelast100Kdocumentsas
aheld-outvalidationsetandreportperplexitiesonthisset.WeusetheGPT2tokenizer[35]forOWT.
Modelshaveacontextsizeof1,024.WeutilizethetransformerarchitecturefromLouetal.[25],which
augmentsthediffusiontransformer[32]withrotaryembeddings[43]. MDLMwastrainedfor1M
or10Msteps(correspondingto33B,327Btokens,respectively)onLM1Band1MstepsonOWT
(whichcorrespondsto262Btokens).ThecorrespondingARbaselinewastrainedforhalfthenumber
ofstepstoensuresimilarnumberoftokensseen(detailsinSuppl.C.2). Fullhyperparametersare
giveninSuppl.C.4.OnOWT,wetrainwithandwithouttimestepconditioning.
For representation learning, we pre-train models on the C4 dataset [36], then fine-tune and
evaluate models on the GLUE benchmark [50]. Models have a context size of 128. We use
the bert-base-uncased tokenizer for the representation learning experiments. We utilize the
MosaicBERTarchitecturefromPortesetal.[33],anextensionoftheoriginalBERTarchitecture[10].
Wepre-trainabidirectionalMosaicBERTusinganMLMobjectivefor37BtokensofC4,aswellas
acausalvariantonthesamedata. Wefurtherfine-tuneMosaicBERTmodelusingtheMDLMfor
327Mtokens,lessthan1%ofthepre-trainingdata.WeprovidethefullhyperparametersinSuppl.C.6.
LikelihoodEvaluation OnLM1B,MDLMoutperformsallpreviousdiffusionmethods(Table1).
ComparedtotheSEDDbaselinereportedbyLouetal.[25],trainedfor66Btokens,MDLM,which
6Table1:Testperplexities(PPL;↓)onLM1B.†ReportedinHeetal.[19].Bestdiffusionvalueisbolded.
Parameters PPL(↓)
Transformer-XBase[9] 0.46B 23.5
Autoregressive
OmniNet [46] 100M 21.5
T
BERT-Mouth[49] 110M ≤142.89
D3PM(absorb)[1] 70M ≤77.50
Diffusion
Diffusion-LM[22]† 80M ≤118.62
DiffusionBert[19] 110M ≤63.78
SEDD[25](33Btokens) 110M ≤32.79
Autoregressive Transformer(33Btokens) 22.32
110M
(Retrained) Transformer(327Btokens) 20.86
Diffusion MDLM(33Btokens) ≤27.04
110M
(Ours) MDLM(327Btokens) ≤23.00
Table3:Zero-shotvalidationperplexities(↓)ofmodelstrainedfor524BtokensonOWT.Allperplexi-
tiesfordiffusionmodelsareupperbounds.
PTB Wikitext LM1B Lambada AGNews Pubmed Arxiv
AR(Retrained) 82.05 25.75 51.25 51.28 52.09 49.01 41.73
SEDD(Retrained) 100.09 34.28 68.20 49.86 62.09 44.53 38.48
MDLM(Ours) 95.26 32.83 67.01 47.52 61.15 41.89 37.37
Table4:GLUEevaluationresults.Evaluationmeasures(↑)areF1scoreforQQPandMRPC,Spearman
correlationsforSTS-B,andaccuracyfortherest.ForMNLI,wereportmatch/mismatchaccuracies.
MNLI
(m/mm) QQP QNLI SST-2 COLA STS-B MRPC RTE Avg
AR 80.94/80.78 86.98 86.16 90.14 33.43 84.32 83.88 47.29 74.88
BERT 84.43/85.35 88.41 90.46 92.20 54.81 88.41 89.16 61.37 81.62
+MDLM-FT 84.76/85.07 88.49 90.30 92.20 57.69 87.48 90.53 62.09 82.06
wetrainforthesameamount,achievesa17%improvementontheperplexitybound.Finally,MDLM
getswithin14%ofanARbaselineandcontinuestoimprovewithmoretraining.Weseethesametrend
formodelstrainedonOWT,alargerdataset,showninTable2–MDLMoutperformspriordiffusion
methods,closingthegaptowardsARmodels. ResultsonOWTtimestepconditioningareinTable
12,Suppl.D.5wherewefindthatmodelstrainedwithandwithouttimeconditioningattainsimilar
perplexities.Additionally,Figure3demonstratesthereducedvarianceweachievefromourobjective,
whencomparedtopreviousmaskeddiffusionmodels,suchasSEDD[25].
Zero-ShotLikelihoodEvaluation Wealsoexploremodels’ability
Table 2: Test perplexities
togeneralizebytakingmodelstrainedonOWTandevaluatinghow
(PPL; ↓) on OWT for mod-
well they model unseen datasets. We compare the perplexities of elstrainedfor262Btokens.†
ourMDLMwithaSEDDparameterizationandanARTransformer
denotesretrainedmodels.
languagemodel. Ourzero-shotdatasetsincludethevalidationsplits
ofPennTreeBank(PTB;[28]),Wikitext[29],LM1B,Lambada[31], PPL(↓)
AGNews[51],andScientificPapers(PubmedandArxivsubsets;[7]).
FullexperimentaldetailsareavailableinSuppl.C.4. AR† 17.54
MDLMconsistentlyoutperformstheSEDDdiffusionparameterization. SEDD† ≤24.10
Insomecases,e.g.,forLambadaandScientificPapers,MDLMattains MDLM(Ours) ≤23.21
betterperplexitythanAR.Wehypothesizethatthesedatasetsarefarther
fromOWT,andthatdiffusionmodelsmaybemorerobusttoout-of-domainevaluationduetothe
unmasking-basedobjective.
7Table6:Testperplexities(PPL;↓)ofgenerativefine-tuningoftheCaduceusMLM[38]ontheHG38
referencegenome.Bestdiffusionmodelvaluesarebolded.Errorbarsindicatethedifferencebetween
themaximumandminimumvaluesacross5randomseedsusedforfine-tuning.
Params PPL(↓)
Mamba 465K 3.067±.0104
Autoregressive(Retrained)
HyenaDNA 433K 3.153±.001
Plaid 507K ≤3.240±.005
Diffusion(Retrained)
SEDD 467K ≤3.216±.003
Diffusion(Ours) MDLM 467K ≤3.199±.010
DownstreamTaskEvaluation WefindthatBERTfine-tunedwithMDLMtobeagenerativemodel
resultsinstrongperplexitieswhilepreservingperformanceondownstreamtasks.OntheC4validation
set,theARmodelattainsperplexity(PPL)of22,thepre-trainedBERTattainsaPPLupperboundof
78(evaluatedusingtheMDLMvariationalbound),andBERT+MDLM-FTattainsaPPLupperbound
of35.InTable4,wefurtherfindthatBERT+MDLMfine-tuninghasnodegradationindownstream
GLUEperformancecomparedtotheBERTinitialization.Whiletheperplexityofourmethodishigher
thantheARbaseline,thedownstreamtaskperformanceissignificantlybetter.
Semi-AutoregressiveModeling TotesttheSARdecodingalgorithmpresentedinSec.4.2, we
comparetoSSD-LM[18]adiffusionmodelthatwasdesignedtogenerateblocksoftextautoregressively.
Wegenerate200sequencesoflength2048tokensonasingle3090GPUandevaluategenerative
perplexityunderapre-trainedGPT-2[35]model.TheSSD-LMsequencesaregeneratedusingblocks
of25tokens(asimplementedintheirpre-trainedmodel)andtheMDLMsequencesaregeneratedusing
L′=512.InTable5,wefindthatinadditiontoachievingbettergenerativeperplexity,MDLMenables
∼25-30xfasterSARdecodingrelativetoSSD-LM.
5.2 MaskedDiffusionDNAModels Table5:Semi-ARgenerativeperplexity(Gen.PPL;
↓)forsequencesof2048tokens.
Wealsoexploretheuseofourgenerativeformu-
lationinconjunctionwithStructuredStateSpace Gen.PPL(↓) Sec/Seq(↓)
models[16]. Namely,webuildontherecently
SSD-LM 35.43 2473.9
proposedCaduceus[38]model,whichusesasa
MDLM(Ours) 27.18 89.3
backbonethedata-dependentSSMMambablock
[15].
ExperimentalSetup Wepre-traintheencoder-onlyCaduceus[38],whichisanMLM,ontheHG38
humanreferencegenome[8]andperformfine-tuningusingourdiffusionparameterization.Weuse
acontextlengthof1024tokensandfollowSchiffetal.[38]fortheexperimentalsetup,otherthan
learningratewhichwasreducedto1e-3.SeeSuppl.C.7forfullexperimentaldetails.Weassessboth
generativeperformanceusingperplexityanddownstreamperformanceonGenomicsBenchmarks[14]
acrosslanguagediffusionparadigmsandARmodels.
GenerativePerformance Wefine-tunetheCaduceusMLMacrossdiffusionparameterizationsand
compareperplexitiesagainstARmodels.WereportperplexityvaluesinTable6.MDLMoutperforms
allotherdiffusionlanguagemodelingschemes.
DownstreamTaskFine-tuning WeperformdownstreamevaluationwiththeGenomicsBench-
marks[14],arecentlyproposedbenchmarkwitheightregulatoryelementclassificationtasks.Asshown
inTable7,ourgenerativefine-tuningparadigmpreservesorimprovesupondownstreamperformance
fromMLMpre-training.Absorbing-statediffusionmethodsoutperformPlaidacrosstasksexceptfor
thesimplesttaskHumanvs.Worm,whereallmethodshaveroughlythesameperformance.Fortasks
wheretheinputisabiasedsubsampleofthefullgenome,weobservethatthecorrelationbetween
perplexityanddownstreamperformanceisweaker;seeSuppl.C.7.
8Table7:GenomicBenchmarks.Top-1accuracy(↑)across5-foldcross-validation(CV)forapre-trained
ARMamba,andapre-trainedCaduceusmodelfine-tunedwithdifferentdiffusionparameterizations.
Thebestvaluespertaskareboldedandthesecondbestareitalicized.Errorbarsindicatethedifference
betweenthemaximumandminimumvaluesacross5randomseedsusedforCV.
Model Mamba Caduceus Caduceus Caduceus Caduceus
Fine-TuningObjective AR MLM Plaid SEDD MDLM(ours)
(ParameterCount) (465K) (467K) (507k) (467k) (467k)
MouseEnhancers 0.763{±0.008} 0.810{±0.016} 0.745{±0.079} 0.784{±0.058} 0.795{±0.029}
Codingvs.Intergenomic 0.897{±0.004} 0.913{±0.003} 0.908{±0.003} 0.913{±0.005} 0.913{±0.003}
Humanvs.Worm 0.967{±0.002} 0.970{±0.002} 0.971{±0.001} 0.970{±0.003} 0.970{±0.003}
HumanEnhancersCohn 0.734{±0.027} 0.737{±0.001} 0.743{±0.010} 0.746{±0.015} 0.743{±0.016}
HumanEnhancerEnsembl 0.856{±0.003} 0.907{±0.000} 0.885{±0.003} 0.905{±0.006} 0.899{±0.004}
HumanRegulatory 0.861{±0.008} 0.874{±0.003} 0.868{±0.010} 0.828{±0.037} 0.868{±0.004}
HumanOCREnsembl 0.806{±0.005} 0.821{±0.000} 0.820{±0.004} 0.816{±0.008} 0.823{±0.008}
HumanNonTATAPromoters 0.926{±0.008} 0.935{±0.014} 0.935{±0l007} 0.935{±0.014} 0.940{±0.007}
5.3 AblationAnalysis
InTable8,wecanseetheeffectofourstreamlinedmaskeddiffusionimplementation.Theimprove-
mentsdescribedinSec.3.5.1allowustogreatlyreduceperplexityofpreviouslydiscountedmodels,
suchasD3PM(seethebottomrowofthistable,whichismathematicallyequivalenttotheD3PM
formulation).WhilemostworksassumedthatD3PMachievesmediocrelog-likelihoods,weshowthat
isincorrect:ourre-implementationalmostmatchesstate-of-the-artscore-basedmethods.Thisintro-
ducesanewstrongbaselinethatopensnewresearchopportunities.Additionally,inTable8,weablate
differentcomponentsofMDLM.WeobservethattheperplexityforMDLMtrainedwithadiscrete
T=1000marginallyworsensby0.1comparedtoMDLMtrainedincontinuoustime.Additionally,
removingthe“carryover”operationfromtheSUBSparameterizationincreasestheperplexityby1.5
points. However,furtherremovingthe“zeromasking”operationdoesnotleadtoanymeaningful
changeinperplexity.
WeprovidefurtherablationsforthecontinuoustimeformulationintheAppendix,showingin Table11
thatforapre-trainedmodel,atinference,increasingT yieldsbetterlikelihoods.
6 Discussion,PriorWork,andConclusion
ComparisontoD3PM Maskeddiffusionisastrict Table8:Testperplexities(PPL;↓)forMDLM
subset of D3PM [1]; setting Q = α I+(1− ablationsonLM1B.Forthediscrete-timemod-
t|s t|s
α )1m⊤ in their framework yields our forward els, we use T = 1000. Standard deviation is
t|s
diffusion.WeimproveoverD3PMinthreeways:(1) measuredover5seedsduringevaluation.
weadopttheSUBSparameterizationforp (z |z );
θ s t
(2)thisallowsustoderiveasimplifiedobjectivethat PPL(≤)
analyticallysimplifiescertainexpectationstozero;
MDLM(46) 27.04±.01
(3)weadoptwell-engineeredtrainingrecipesthat
w/oContinuoustime(42) 27.19±.07
improveperformance.Both(1)and(2)arepossible
&w/ocarry-over(40) 28.56±.15
becausewefocusonmaskinginsteadofdeveloping
&w/ozeromasking(38) 28.51±.15
ageneraldiscretediffusionframework.Surprisingly,
(3)hasthelargestcontributiontoperformance.
ComparisontoCTMC Mostimplementationsofdiffusionworkbestincontinuoustime.However,
extendingD3PMinthiswayrequirescomputingthelimitoftheproductofaninfinitenumberof
matricesQ ·Q ···Q asT→∞,whichrequiresadvancedCTMCtheory[4].Ourworkdescribes
T T−1 t
simplecontinuous-timeformulationsforthemostcommonnoiseprocesses(e.g.,maskinganduniform
π),thushelpingmakeanimportantpartoftheliteraturemoreaccessible.Ourresultsremaincompatible
withCTMC:weeffectivelyuseratematricesR =α′(1m⊤−I).
t t
ComparisontoScoreEstimation Score-basedapproachestodiffusion[40]extendtodiscretestates,
althoughtheytypicallyfurtherbuilduponadvancedCTMCtheory.Inparticular,SEDD[25]derives
andoptimizesanELBOthatisafunctionofthescoremodel,obtainingstate-of-the-artlog-likelihoods
amongdiffusionmodels.Weuseamuchsimplerapproachthatrequiresnoadvancedtheory.
9ComparisontoBERT OurworkprovidesaprincipledwayofmakingBERTgenerativewhen
trainedwithrandomizedmaskingrates.PreviousworkongeneratingfromBERTusedGibbssampling
orad-hocmethods[12,24,49].TheconnectionbetweenBERTanddiffusionwasfirstmadebyAustin
etal.[1]: theirobjectiveeffectivelyinvolvesunmasking. Heetal.[19]additionallystartstraining
fromapretrainedBERT.However,bothworksuseanobjectivethatissimilarto(9),whichisless
numericallystablethanourobjective(seeSection3.5.1).Austinetal.[1]describeintheirappendix
howtheirELBOcansimplifytoaweightedmasking(MLM)losssimilarto(8),butusingamore
complexformulafortheweights.However,theydonottrainwiththatobjective.Ourworkderivesa
simplerexpressionfortheaverageofMLMlosses,implementsit,andobtainsbetterlikelihoods.
Conclusion Inthiswork,weexploremaskeddiffusion.Withawell-engineeredimplementationthat
supportsasimplevariationalobjective,weattainstate-of-the-artdiffusionperplexitiesonlanguage
benchmarksanddemonstratehowtoefficientlyconvertBERT-styleencodersintogenerativemodels.
Givenweareworkingonlanguagemodeling,wecarryanyoftheinherentrisksandopportunitiesthat
comewiththislineofresearch.
References
[1] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg.
Structureddenoisingdiffusionmodelsindiscretestate-spaces. AdvancesinNeuralInformation
ProcessingSystems,34:17981–17993,2021.
[2] PavelAvdeyev,ChenlaiShi,YuhaoTan,KseniiaDudnyk,andJianZhou.Dirichletdiffusionscore
modelforbiologicalsequencegeneration. InInternationalConferenceonMachineLearning,pp.
1276–1301.PMLR,2023.
[3] ŽigaAvsec,VikramAgarwal,DanielVisentin,JosephRLedsam,AgnieszkaGrabska-Barwinska,
KyleRTaylor,YannisAssael,JohnJumper,PushmeetKohli,andDavidRKelley. Effectivegene
expressionpredictionfromsequencebyintegratinglong-rangeinteractions. Naturemethods,18
(10):1196–1203,2021.
[4] AndrewCampbell,JoeBenton,ValentinDeBortoli,ThomasRainforth,GeorgeDeligiannidis,
andArnaudDoucet. Acontinuoustimeframeworkfordiscretedenoisingmodels. Advancesin
NeuralInformationProcessingSystems,35:28266–28279,2022.
[5] CiprianChelba,TomasMikolov,MikeSchuster,QiGe,ThorstenBrants,PhillippKoehn,and
TonyRobinson. Onebillionwordbenchmarkformeasuringprogressinstatisticallanguage
modeling,2014.
[6] TingChen,RuixiangZhang,andGeoffreyHinton. Analogbits:Generatingdiscretedatausing
diffusionmodelswithself-conditioning. arXivpreprintarXiv:2208.04202,2022.
[7] ArmanCohan,FranckDernoncourt,DooSoonKim,TrungBui,SeokhwanKim,WalterChang,
andNazliGoharian. Adiscourse-awareattentionmodelforabstractivesummarizationoflong
documents.Proceedingsofthe2018ConferenceoftheNorthAmericanChapteroftheAssociation
forComputationalLinguistics:HumanLanguageTechnologies,Volume2(ShortPapers),2018.
doi:10.18653/v1/n18-2097. URLhttp://dx.doi.org/10.18653/v1/n18-2097.
[8] Genome Reference Consortium. Genome reference consortium human build 37 (grch37.
Database(GenBankorRefSeq),2009.
[9] ZihangDai,ZhilinYang,YimingYang,JaimeCarbonell,QuocVLe,andRuslanSalakhutdinov.
Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint
arXiv:1901.02860,2019.
[10] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert:Pre-trainingofdeep
bidirectionaltransformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,2018.
[11] SanderDieleman,LaurentSartran,ArmanRoshannai,NikolaySavinov,YaroslavGanin,PierreH
Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, et al. Continuous
diffusionforcategoricaldata. arXivpreprintarXiv:2211.15089,2022.
10[12] MarjanGhazvininejad,OmerLevy,YinhanLiu,andLukeZettlemoyer. Mask-predict:Parallel
decodingofconditionalmaskedlanguagemodels.InKentaroInui,JingJiang,VincentNg,andXi-
aojunWan(eds.),Proceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguage
Processingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-
IJCNLP),pp.6112–6121,HongKong,China,November2019.AssociationforComputational
Linguistics. doi:10.18653/v1/D19-1633. URLhttps://aclanthology.org/D19-1633.
[13] AaronGokaslan,VanyaCohen,ElliePavlick,andStefanieTellex. Openwebtextcorpus. http:
//Skylion007.github.io/OpenWebTextCorpus,2019.
[14] KatarínaGrešová,VlastimilMartinek,DavidCˇechák,PetrŠimecˇek,andPanagiotisAlexiou.
Genomic benchmarks: a collection of datasets for genomic sequence classification. BMC
GenomicData,24(1):25,2023.
[15] AlbertGuandTriDao. Mamba: Linear-timesequencemodelingwithselectivestatespaces.
arXivpreprintarXiv:2312.00752,2023.
[16] AlbertGu,KaranGoel,andChristopherRé. Efficientlymodelinglongsequenceswithstructured
statespaces. arXivpreprintarXiv:2111.00396,2021.
[17] IshaanGulrajaniandTatsunoriBHashimoto. Likelihood-baseddiffusionlanguagemodels.
AdvancesinNeuralInformationProcessingSystems,36,2024.
[18] XiaochuangHan,SachinKumar,andYuliaTsvetkov. Ssd-lm: Semi-autoregressivesimplex-
based diffusion language model for text generation and modular control. arXiv preprint
arXiv:2210.17432,2022.
[19] ZhengfuHe,TianxiangSun,KuanningWang,XuanjingHuang,andXipengQiu. Diffusion-
bert: Improvinggenerativemaskedlanguagemodelswithdiffusionmodels. arXivpreprint
arXiv:2211.15029,2022.
[20] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advances
inneuralinformationprocessingsystems,33:6840–6851,2020.
[21] DiederikKingma,TimSalimans,BenPoole,andJonathanHo. Variationaldiffusionmodels.
Advancesinneuralinformationprocessingsystems,34:21696–21707,2021.
[22] XiangLi,JohnThickstun,IshaanGulrajani,PercySLiang,andTatsunoriBHashimoto.Diffusion-
lmimprovescontrollabletextgeneration. AdvancesinNeuralInformationProcessingSystems,
35:4328–4343,2022.
[23] XuanlinLi,BrandonTrabucco,DongHukPark,MichaelLuo,ShengShen,TrevorDarrell,and
YangGao.Discoveringnon-monotonicautoregressiveorderingswithvariationalinference.arXiv
preprintarXiv:2110.15797,2021.
[24] YiLiao,XinJiang,andQunLiu. Probabilisticallymaskedlanguagemodelcapableofautoregres-
sivegenerationinarbitrarywordorder. InDanJurafsky,JoyceChai,NatalieSchluter,andJoel
Tetreault(eds.),Proceedingsofthe58thAnnualMeetingoftheAssociationforComputational
Linguistics,pp.263–274,Online,July2020.AssociationforComputationalLinguistics. doi:
10.18653/v1/2020.acl-main.24. URLhttps://aclanthology.org/2020.acl-main.24.
[25] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion language modeling by
estimatingtheratiosofthedatadistribution. arXivpreprintarXiv:2310.16834,2023.
[26] JustinLovelace,VarshaKishore,ChaoWan,EliotShekhtman,andKilianQWeinberger. Latent
diffusionforlanguagegeneration.AdvancesinNeuralInformationProcessingSystems,36,2024.
[27] Vincent Mallet and Jean-Philippe Vert. Reverse-complement equivariant networks for dna
sequences. Advancesinneuralinformationprocessingsystems,34:13511–13523,2021.
[28] MitchMarcus,BeatriceSantorini,andMaryAnnMarcinkiewicz. Buildingalargeannotated
corpusofenglish:Thepenntreebank. Computationallinguistics,19(2):313–330,1993.
11[29] StephenMerity,CaimingXiong,JamesBradbury,andRichardSocher. Pointersentinelmixture
models,2016.
[30] EricNguyen,MichaelPoli,MarjanFaizi,ArminThomas,MichaelWornow,CallumBirch-Sykes,
StefanoMassaroli,AmanPatel,ClaytonRabideau,YoshuaBengio,etal. Hyenadna:Long-range
genomicsequencemodelingatsinglenucleotideresolution. Advancesinneuralinformation
processingsystems,36,2024.
[31] DenisPaperno,GermánKruszewski,AngelikiLazaridou,NgocQuanPham,RaffaellaBernardi,
Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The LAMBADA
dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th
AnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pp.
1525–1534,Berlin,Germany,August2016.AssociationforComputationalLinguistics. URL
http://www.aclweb.org/anthology/P16-1144.
[32] WilliamPeeblesandSainingXie. Scalablediffusionmodelswithtransformers. InProceedings
oftheIEEE/CVFInternationalConferenceonComputerVision,pp.4195–4205,2023.
[33] JacobPortes,AlexTrott,SamHavens,DanielKing,AbhinavVenigalla,MoinNadeem,Nikhil
Sardana,DayaKhudia,andJonathanFrankle. Mosaicbert:Abidirectionalencoderoptimizedfor
fastpretraining,2024.
[34] OfirPress,NoahA.Smith,andMikeLewis. Trainshort,testlong:Attentionwithlinearbiases
enablesinputlengthextrapolation,2022.
[35] AlecRadford,JeffWu,RewonChild,DavidLuan,DarioAmodei,andIlyaSutskever. Language
modelsareunsupervisedmultitasklearners. 2019.
[36] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,
YanqiZhou,WeiLi,andPeterJ.Liu. Exploringthelimitsoftransferlearningwithaunified
text-to-texttransformer. J.Mach.Learn.Res.,21(1),jan2020. ISSN1532-4435.
[37] SubhamSekharSahoo,AaronGokaslan,ChrisDeSa,andVolodymyrKuleshov. Diffusion
modelswithlearnedadaptivenoise. arXivpreprintarXiv:2312.13236,2023.
[38] YairSchiff,Chia-HsiangKao,AaronGokaslan,TriDao,AlbertGu,andVolodymyrKuleshov.
Caduceus: Bi-directional equivariant long-range dna sequence modeling. arXiv preprint
arXiv:2403.03234,2024.
[39] JaschaSohl-Dickstein,EricWeiss,NiruMaheswaranathan,andSuryaGanguli. Deepunsuper-
visedlearningusingnonequilibriumthermodynamics. InInternationalconferenceonmachine
learning,pp.2256–2265.PMLR,2015.
[40] YangSongandStefanoErmon. Generativemodelingbyestimatinggradientsofthedatadistribu-
tion. Advancesinneuralinformationprocessingsystems,32,2019.
[41] YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoErmon,and
BenPoole. Score-basedgenerativemodelingthroughstochasticdifferentialequations. arXiv
preprintarXiv:2011.13456,2020.
[42] RobinStrudel,CorentinTallec,FlorentAltché,YilunDu,YaroslavGanin,ArthurMensch,Will
Grathwohl,NikolaySavinov,SanderDieleman,LaurentSifre,etal. Self-conditionedembedding
diffusionfortextgeneration. arXivpreprintarXiv:2211.04236,2022.
[43] JianlinSu,YuLu,ShengfengPan,AhmedMurtadha,BoWen,andYunfengLiu. Roformer:
Enhancedtransformerwithrotarypositionembedding. arXivpreprintarXiv:2104.09864,2021.
[44] HaoranSun,LijunYu,BoDai,DaleSchuurmans,andHanjunDai. Score-basedcontinuous-time
discretediffusionmodels. arXivpreprintarXiv:2211.16750,2022.
[45] Zhiqing Sun and Yiming Yang. Difusco: Graph-based diffusion solvers for combinatorial
optimization. AdvancesinNeuralInformationProcessingSystems,36:3706–3731,2023.
12[46] YiTay,MostafaDehghani,VamsiAribandi,JaiGupta,PhilipMPham,ZhenQin,DaraBahri,Da-
ChengJuan,andDonaldMetzler. Omninet:Omnidirectionalrepresentationsfromtransformers.
InInternationalConferenceonMachineLearning,pp.10193–10202.PMLR,2021.
[47] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,
ŁukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. Advancesinneuralinformation
processingsystems,30,2017.
[48] Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pas-
cal Frossard. Digress: Discrete denoising diffusion for graph generation. arXiv preprint
arXiv:2209.14734,2022.
[49] AlexWangandKyunghyunCho. Berthasamouth,anditmustspeak:Bertasamarkovrandom
fieldlanguagemodel. arXivpreprintarXiv:1902.04094,2019.
[50] AlexWang,AmanpreetSingh,JulianMichael,FelixHill,OmerLevy,andSamuelR.Bowman.
GLUE:Amulti-taskbenchmarkandanalysisplatformfornaturallanguageunderstanding. In
InternationalConferenceonLearningRepresentations,2019. URLhttps://openreview.
net/forum?id=rJ4km2R5t7.
[51] XiangZhang,JunboJakeZhao,andYannLeCun. Character-levelconvolutionalnetworksfor
textclassification. InNIPS,2015.
[52] Hannah Zhou, Avanti Shrikumar, and Anshul Kundaje. Towards a better understanding of
reverse-complementequivariancefordeeplearningmodelsingenomics. InMachineLearningin
ComputationalBiology,pp.1–33.PMLR,2022.
13Contents
1 Introduction 1
2 Background 2
2.1 DiffusionModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
2.2 DiscreteDiffusionModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3 SimpleMaskedDiffusionModels 3
3.1 InterpolatingDiscreteDiffusion . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3.2 MaskedDiffusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3.3 Rao-BlackwellizedLikelihoodBounds . . . . . . . . . . . . . . . . . . . . . . . . 4
3.4 Continuous-TimeLikelihoodBounds . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.5 MaskedDiffusionLanguageModels . . . . . . . . . . . . . . . . . . . . . . . . . 5
4 InferenceandSamplinginMaskedDiffusionLanguageModels 6
4.1 EfficientAncestralSampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
4.2 Semi-AutoregressiveMaskedDiffusionLanguageModels . . . . . . . . . . . . . . 6
5 Experiments 6
5.1 MaskedDiffusionLanguageModels . . . . . . . . . . . . . . . . . . . . . . . . . 6
5.2 MaskedDiffusionDNAModels. . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5.3 AblationAnalysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
6 Discussion,PriorWork,andConclusion 9
Appendices 15
AppendixA DiscretetimeELBO 15
A.1 Genericcase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
A.2 Absorbingstate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
AppendixB MDLM 19
B.1 Rao-Blackwellization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B.2 ContinousTime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
AppendixC Experimentaldetails 21
C.1 LikelihoodEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
C.2 Avg.NumberofTokensseen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
C.3 Lowdiscrepancysampler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
C.4 LanguageModeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
C.5 ZeroshotLikelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
C.6 RepresentationLearning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
14C.7 DiffusionDNAModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
AppendixD AdditionalExperiments 23
D.1 Noisescheduleparameterization . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
D.2 Fastersamplingwithcaching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
D.3 LM1Bablations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
D.4 TrainNLLcurvesonOWT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
D.5 Time-conditioningablationonOWT . . . . . . . . . . . . . . . . . . . . . . . . . 26
Appendices
AppendixA DiscretetimeELBO
Thissectionisorganizedasfollows: First,wederivetheexpressionsforthetrueposteriorandthe
approximateposteriorasoutlinedinSuppl.A.1.Wethensimplifytheseexpressionsspecificallyfor
thecaseofabsorbingstatediffusioninSuppl.A.2.Finally,wederivetheexpressionfortheELBOfor
absorbingstatediffusioninSuppl.A.2.3.
A.1 Genericcase
GiventhestatetransitionmatrixQ ,priorπ,andthelatentvariablesz andz ,wheres<t,let
t s t
Q =α I +(1−α )1π⊤. (12)
t|s t|s n t|s
A.1.1 q(z |z )
t s
Thus,themarginalsin(3)correspondtothefollowingforwardprocess:
q(z |z )=Cat(z ;Q⊤ z )
t s t t|s s
=Cat(z ;[α I +(1−α )1π⊤]⊤z )
t t|s n t|s s
=Cat(z ;α z +(1−α )π1⊤z ) ∵1⊤z =1
t t|s s t|s s s
=Cat(z ;α z +(1−α )π). (13)
t t|s s t|s
Theaboveequationindicatesthatduringeachdiffusionstepfroms→t,afraction(1−α )ofthe
t|s
probabilitymassistransferredtothepriordistributionπ.
A.1.2 q(z |z ,x)
s t
Austinetal.[1]showthattheposteriorcorrespondingto(13)isgivenasfollows:
(cid:32) (cid:33)
Q z ⊙Q⊤x
q(z |z ,x)=Cat z ; t|s t s , (14)
s t s z⊤Q⊤x
t t
whichwesimplifytothefollowing:
q(z |z ,x)
s t
(cid:32) (cid:33)
[α I +(1−α )1π⊤]z ⊙[α I +(1−α )1π⊤]⊤x
t|s n t|s t s n s
=Cat z ;
s z⊤[α I +(1−α )1π⊤]⊤x
t t n t
(cid:32) (cid:33)
[α z +(1−α )1π⊤z ]⊙[α x+(1−α )π]
t|s t t|s t s s
=Cat z ;
s z⊤[α x+(1−α )π1⊤x]
t t t
(cid:32) (cid:33)
[α z +(1−α )1π⊤z ]⊙[α x+(1−α )π]
=Cat z ; t|s t t|s t s s . ∵1⊤x=1 (15)
s α z⊤x+(1−α )z⊤π
t t t t
15A.1.3 p (z |z )
θ s t
Austinetal.[1]approximatethereverseprocessinthefollowingmanner:
(cid:32) (cid:33)
Q z ⊙Q⊤x (z ,t)
p (z |z )=q(z |z ,x=x (z ,t))=Cat z ; t|s t s θ t . (16)
θ s t s t θ t s z⊤Q⊤x (z ,t)
t t θ t
wherex (z ,t):V×[0,1]→∆Kisanapproximationforx.
θ t
A.2 Absorbingstate
Fortheabsorbingstatediffusionprocesswehaveπ=m.
A.2.1 q(z |z ,x)
s t
Since,z ∈{x,m},takesonly2valuesweconsidertheseparatecases:z =xandz =m.
t t t
Case1. Considerthecasez =xi.e.z isunmasked.From(15),wehavethefollowing:
t t
q(z |z =x,x)
s t
(cid:32) (cid:33)
[α x+(1−α )1m⊤x]⊙[α x+(1−α )m]
t|s t|s s s
=Cat z ;
s α x⊤x+(1−α )x⊤m
t t
(cid:18) [α x]⊙[α x+(1−α )m](cid:19)
=Cat z ; t|s s s ∵x⊤m=0
s α
t
(cid:18) (cid:19)
α x
=Cat z s; αt ∵x⊤m=0andα t=α t|sα
s
t
=Cat(z ;x) ∵α =α α (17)
s t t|s s
Thus,wehavethefollowing:
q(z |z =x,x)=Cat(z ;x). (18)
s t s
Case2. Considerthecasez =m.Bysubstitutingz =mandπ=min(15),q(z |z ,x)simplifies
t t s t
tothefollowing:
(cid:18)(α m+(1−α )1)⊙(α x+(1−α )m)(cid:19)
t|s t|s s s
q(z |z =m,x)=Cat
s t (1−α )
t
(cid:18)(α (1−α )m+(1−α )(1−α )m+(α −α )x)(cid:19)
t|s s t|s s s t
=Cat
(1−α )
t
(cid:18) (cid:19)
(1−α )m+(α −α )x
=Cat z ; s s t (19)
s 1−α
t
Notethattheabovecategoricaldistributionisnon-zeroforz ∈{x,m}andzeroforeveryothervalue.
s
Thenon-zerovaluesarespecifiedasfollows:
α −α
q(z =x|z =m,x)= s t (20)
s t 1−α
t
1−α
q(z =m|z =m,x)= s (21)
s t 1−α
t
CombiningCases1and2,weget:
(cid:40)
Cat(z ;z ) z ̸=m,
s t t
q(z |z ,x)= (cid:16) (cid:17) (22)
s t Cat z ;(1−αs)m+(αs−αt)x z =m.
s 1−αt t
A.2.2 p (z |z )
θ s t
Fortheabsorbingstatediffusionprocesswithπ=m,wewanttosimplifythe(16).Forthisreason,we
consider2cases:first,whenz ̸=m(case1),second,whenz ̸=m(case2).
t t
16Case1. Considerthecasewhenz ̸=m.(16)simplifiestothefollowing:
t
(cid:32) (cid:33)
Q z ⊙Q⊤x (z ,t)
p (z |z ̸=m)=Cat z ; t|s t s θ t
θ s t s z⊤Q⊤x (z ,t)
t t θ t
(cid:32) (cid:33)
Q z ⊙Q⊤x (z ,t)
=Cat z ; t|s t s θ t
s [Q z ]⊤x (z ,t)
t t θ t
(cid:32) (cid:33)
[α z ]⊙[α I +(1−α )m1⊤]x (z ,t)
t|s t s n s θ t
=Cat z ;
s [α z ]⊤x (z ,t)
t t θ t
(cid:18) [α z ]⊙[α x (z ,t)+(1−α )m⟨1,x (z ,t)⟩](cid:19)
t|s t s θ t s θ t
=Cat z ;
s α ⟨z ,x (z ,t)⟩
t t θ t
since⟨1,x (z ,t)⟩=1,wehavethefollowing:
θ t
(cid:18) [α z ]⊙[α x (z ,t)+(1−α )m](cid:19)
t|s t s θ t s
=Cat z ;
s α ⟨z ,x (z ,t)⟩
t t θ t
sincez ⊙m=0,wehavethefollowing:
t
(cid:18) (cid:19)
α z ⊙x (z ,t)
=Cat z ; t t θ t (23)
s α ⟨z ,x (z ,t)⟩
t t θ t
Case2. Considerthecasewhenz =m.(16)simplifiestothefollowing:
t
(cid:32) (cid:33)
Q m⊙Q⊤x (z ,t)
p (z |z =m)=Cat z ; t|s s θ t
θ s t s m⊤Q x (z ,t)
t θ t
(cid:32) (cid:33)
Q m⊙Q⊤x (z ,t)
=Cat z ; t|s s θ t
s [Q⊤m]⊤x (z ,t)
t θ t
(cid:32) (cid:33)
[α m+(1−α )1]⊙[α I +(1−α )m1⊤]x (z ,t)
t|s t|s s n s θ t
=Cat z ;
s [α m+(1−α )1]⊤x (z ,t)
t t θ t
(cid:18) [α m+(1−α )1]⊙[α x (z ,t)+(1−α )m⟨1,x (z ,t)⟩](cid:19)
t|s t|s s θ t s θ t
=Cat z ;
s α ⟨m,x (z ,t)⟩+(1−α )⟨1,x (z ,t)⟩
t θ t t θ t
(cid:18) [α m+(1−α )1]⊙[α x (z ,t)+(1−α )m](cid:19)
t|s t|s s θ t s
=Cat z ;
s α ⟨x (z ,t),m⟩+(1−α )
t θ t t
(cid:18) (cid:19)
α m⊙x (z ,t)+(α −α )x (z ,t)+(1−α )m
=Cat z ; t θ t s t θ t s (24)
s α ⟨x (z ,t),m⟩+(1−α )
t θ t t
Notethattheabovecategoricaldistribution,wecanobtainthevaluesforp (z =x|z =m)and
θ s t
p (z =m|z =m)whichareasfollows:
θ s t
(α −α )⟨x (z ,t),x⟩
p (z =x|z =m)= s t θ t (25)
θ s t α ⟨x (z ,t),m⟩+(1−α )
t θ t t
α ⟨x (z ,t),m⟩+(1−α )
p (z =m|z =m)= s θ t s (26)
θ s t α ⟨x (z ,t),m⟩+(1−α )
t θ t t
Asasanitycheck,wecanverifythat(25)reducesto(20),and(26)reducesto(21)ifourdenoising
networkcanreconstructxperfectly,i.e.,x (z ,t)=x.
θ t
Combining(23)and(24),wegetthefollowingexpressionforthereverseprocessparameterization:
 (cid:16) (cid:17)
p θ(z s|z t)= CC aa tt (cid:16) zz ss ;; ααα ttt m⟨z zt t ⊙⊙ ,x xx θ θθ ( (( z
z
αz t tt t, ,, t ⟨tt ) x)) ⟩
+ θ(( zα ts ,t− ),α mt) ⟩x +θ (( 1z −t, αt) t+
)(1−αs)m(cid:17) z zt t̸= =m m,
.
(27)
A.2.3 DiffusionLoss
ForagivenT,LetL =E E TD (q(z |z ,x)∥p (z |z ))denotethediffusionloss.
T t∈{1,...,T} q(zt|x) KL s t θ s t
WebreakdownthecomputationofD (q(z |z ,x)∥p (z |z ))into2cases: z =x(case1)and
KL s t θ s t t
z =m(case2).
t
17Case1. considerthecasez =x.Let’ssimplifyD (q(z |z =x,x)∥p (z |z =x)).
t KL s t θ s t
D (q(z |z =x,x)∥p (z |z =x))
KL s t θ s t
=(cid:88)
q(z |z
=x,x)logq(z s|z t=x,x)
s t p (z |z =x)
θ s t
zs
1
=log ∵q(z =x|z ,x)=1andq(z ̸=x|z ,x)=0
p (z =x|z =x) s t s t
θ s t
=log1 From(23)
=0 (28)
Case2. Considerthecasez =m.Let’ssimplifyD (q(z |z =m,x)∥p (z |z =m)).
t KL s t θ s t
D (q(z |z =m,x)∥p (z |z =m))
KL s t θ s t
=(cid:88)
q(z |z
=m,x)logq(z s|z t=m,x)
s t p (z |z =m)
θ s t
zs
=
(cid:88)
q(z |z
=m,x)logq(z s|z t=m,x)
s t p (z |z =m)
θ s t
zs∈{x,m}
q(z =x|z =m,x)
=q(z =x|z =m,x)log s t
s t p (z =x|z =m)
θ s t
(cid:124) (cid:123)(cid:122) (cid:125)
Simplifyusing(20)and(25)
q(z =m|z =m,x)
+q(z =m|z =m,x)log s t
s t p (z =m|z =m)
θ s t
(cid:124) (cid:123)(cid:122) (cid:125)
Simplifyusing(21)and(26)
α −α α ⟨x (z ,t),m⟩+(1−α )
= s tlog t θ t t
1−α (1−α )⟨x (z ,t),x⟩
t t θ t
1−α (1−α )(α ⟨x (z ,t),m⟩+(1−α ))
+ slog s t θ t t (29)
1−α (1−α )(α ⟨x (z ,t),m⟩+(1−α ))
t t s θ t s
Thus,D (q(z |z ,x)∥p (z |z ))canbewritteninthefollowingmannerwhere⟨z ,x ⟩evaluatesto1
KL s t θ s t t 0
ifz =xand⟨z ,m⟩evaluatesto1ifz =m:
t t t
D (q(z |z ,x)∥p (z |z ))
KL s t θ s t
=D (q(z |z =x,x)∥p (z |z =x))⟨z ,x ⟩+D (q(z |z =m,x)∥p (z |z =m))⟨z ,m⟩ (30)
KL s t θ s t t 0 KL s t θ s t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
=0,from(28) Givenby(29)
Thus,wederivethediffusionloss,L ,inthefollowingmanner:
T
L =E E TD (q(z |z ,x)∥p (z |z ))
T t∈{ T1, T2,...,1} q(zt|x) KL s t θ s t
(cid:20)
α −α α ⟨x (z ,t),m⟩+(1−α )
=E E T s tlog t θ t t
t∈{ T1, T2,...,1} q(zt|x) 1−α
t
(1−α t)⟨x θ(z t,t),x⟩
(cid:21)
1−α (1−α )(α ⟨x (z ,t),m⟩+(1−α ))
+ slog s t θ t t ⟨z ,m⟩ (31)
1−α (1−α )(α ⟨x (z ,t),m⟩+(1−α )) t
t t s θ t s
NotethatL is0ifz isanunmaskedtokeni.e.z =x.
T t t
A.2.4 NELBO
Austinetal.[1],Sohl-Dicksteinetal.[39]givenlatentsx ,modelα as(α ) =1− i.
1,...,T i i i∈{1,...,T} T
However,inthispaper,wedenotethelatentsasz ;andhence,theα aregivenasfollows:
t(0),...,t(T) t(i)
i
(α i) i∈{1,...,T}=1−
T
FromAustinetal.[1],Sohl-Dicksteinetal.[39].
18i
=⇒(α i) k∈{1,...,T+1}=1−
T+1
ForT+1latents
i+1
=⇒(α i) i∈{0,...,T}=1−
T+1
Offsettingtheindicesby1.
i+1
=⇒(α ) =1− Switchingthenotationsfromα toα . (32)
t(i) i∈{0,...,T} T+1 i t(i)
Consequently,fromEquation32,wederivethat
T
α = , (33)
t(0) T+1
α =0. (34)
t(T)
Thuswehavethefollowing:
(cid:18) (cid:19)
T 1
z ∼Cat(.;α x+(1−α )m)=Cat .; x+ m , (35)
t(0) t=0 t=0 T+1 T+1
q(z |x)=Cat(.;α x+(1−α )m)=Cat(.;m), (36)
t(T) t=1 t=1
p (z )=Cat(.;m) (37)
θ t(T)
TheNELBO(2)simplifiestothefollowing:
 
E q−logp θ(x|z t(0))+ L
T
+D KL[q(z t(T)|x)∥p θ(z t(T))]
(cid:124)(cid:123)(cid:122)(cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Computeusing(31) =0using(36)and(37)
(cid:34) (cid:20)
α −α α ⟨x (z ,t),m⟩+(1−α )
=E −logp (x|z )+T s tlog t θ t t
q,t θ t(0) 1−α (1−α )⟨x (z ,t),x⟩
t t θ t
(cid:21) (cid:35)
1−α (1−α )(α ⟨x (z ,t),m⟩+(1−α ))
+ slog s t θ t t ⟨z ,m⟩ (38)
1−α (1−α )(α ⟨x (z ,t),m⟩+(1−α )) t
t t s θ t s
AppendixB MDLM
InthissectionweshowhowSUBSparameterizationcansimplifythefunctionalformoftheNELBOas
definedin(38).
B.1 Rao-Blackwellization
WeemploytheRBtechniquesasdescribedinSec.3.2.3tosimplifytheNELBO(38)to(40)usingRB2,
andfurtherto(42)usingRB1.
B.1.1 ZeroMaskingProbabilities
Using“ZeroMaskingProbabilities”(RB2)fromSec.3.2.3,weset⟨x (z ,t),m⟩=0in(31)toobtain
θ t
thefollowingsimplifieddiffusionloss:
(cid:20) (cid:21)
α −α 1
LRB2=E E T s tlog ⟨z ,m⟩
T t∈{1,...,T} q(zt|x) 1−α ⟨x (z ,t),x⟩ t
t θ t
(cid:20) (cid:21)
α −α
=E E T t slog⟨x (z ,t),x⟩ ⟨z ,m⟩. (39)
t∈{1,...,T} q(zt|x) 1−α θ t t
t
ThecorrespondingRao-BlackwellizedNELBOisgivenas:
 
E −logp (x|z )+ LRB2 +D [q(z |x)∥p (z )]
q θ t(0) T  KL t(T) θ t(T)
(cid:124)(cid:123)(cid:122)(cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Computeusing(39) =0using(36)and(37)
(cid:20) (cid:20) (cid:21) (cid:21)
α −α
=E −logp (x|z )+T t slog⟨x (z ,t),x⟩ ⟨z ,m⟩ (40)
q,t θ t(0) 1−α θ t t
t
19B.1.2 CarryOverUnmasking
Noticethattheterm⟨x (z ,t),m⟩in(39),reducesthediffusionlossto0forz ̸=mi.e.z =x.Now,
θ t t t
we’dliketoshowthatwith“CarryOverUnmasking”(RB1)fromSec.3.2.3,wecandrop⟨x (z ,t),m⟩
θ t
from (39). Recall that RB1 ensures x (z ,t)=x for z =x. Thus, with RB1 parameterization
θ t t
thediffusionloss(39)reducesto0forz =xsincelog⟨x (z ,t),m⟩=0. Thisallowsustodrop
t θ t
⟨x (z ,t),m⟩in(40)toobtainthefollowingdiffusionloss:
θ t
(cid:20) (cid:21)
α −α
LRB2+RB1=E E T t slog⟨x (z ,t),x⟩ (41)
T t∈{ T1, T2,...,1} q(zt|x) 1−α
t
θ t
B.1.3 NELBO
Thus,wehavethefollowingNELBO:
 
E −logp (x|z )+ LRB2+RB1 +D [q(z |x)∥p (z )]
q θ t(0) T  KL t(T) θ t(T)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Computeusing(41) =0using(36)and(37)
(cid:20) (cid:20) (cid:21)(cid:21)
α −α
=E −logp (x|z )+T t slog⟨x (z ,t),x⟩ (42)
q,t θ t(0) 1−α θ t
t
Comparing(42)and(40). NotethatduetoRB1,logp (x|z )in(42)reducesto0everytime
θ t(0)
z =xasexplainedin(44).However,thisisnotthecasein(40),eventhoughithasafunctionally
t(0)
similarexpressionto(42).Becauseofthisreason(42)shouldleadtoabetterlikelihoodestimateand
weempiricallyverifythisinTable8.
B.2 ContinousTime
B.2.1 DiffusionLoss
To derive the continuous-time diffusion loss, L∞ , we consider the limiting case
diffusion
lim LRB2+RB1(41):
T→∞ T
L∞ = lim LRB2+RB1
diffusion T
T→∞
(cid:20) (cid:21)
α −α
=E lim T t slog⟨x (z ,t),x⟩
t∈{ T1, T2,...,1},q(zt|x) T→∞ 1−α t θ t
(cid:20) α′ (cid:21)
=E
t∼U[0,1],q(zt|x)
1−t
α
tlog⟨x θ(z t,t),x⟩ Using Tl →im ∞T(α t−α s)=α t′
(43)
B.2.2 ReconstructionLoss
Forthecontinoustimecase,from(35),wehave
(cid:18) (cid:19)
T 1
z ∼ lim Cat .; x+ m
t(0) T→∞ T+1 T+1
=⇒z ∼Cat(.;x)
t(0)
=⇒z =x (44)
t(0)
Thus,thereconstructionlossreducesto0inthefollowingmanner:
L =−logp (x|z )
recons θ t(0)
=−logp θ(x|z t(0)=x) From(44)
=−log⟨x (x,t(0)),x⟩
θ
=−log⟨x,x⟩ Dueto“copyover”x (x,t(0))=x
θ
=0. (45)
20B.2.3 NELBO
Thus,wehavethefollowingNELBO:
 
E −logp (x|z )+ L∞ +D [q(z |x)∥p (z )]
q θ t(0) diffusion  KL t(T) θ t(T)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
=0from(45) Computeusing(41) =0using(36)and(37)
(cid:20) α′ (cid:21)
=E t log⟨x (z ,t),x⟩ (46)
q,t 1−α θ t
t
AppendixC Experimentaldetails
C.1 LikelihoodEvaluation
Weuseasinglemonte-carloestimateforttoevaluatethelikelihood.Thelowdiscrepancysampler(C.3)
playsakeyroleinreducingthevarianceoftheestimateasseeninTable8.
C.2 Avg.NumberofTokensseen
Giventraining_steps, batch_size, context_length, thenumberoftokensseenbytheAR
modelisgivenas:
training_steps×batch_size×context_length. (47)
However,thisexpressiondoesn’tholdforadiffusionmodel,sinceateachtrainingstep,afractionof
theinputtokensaremaskedbeforebeingfedtothemodel.Letp betheprobabilityofatokenbeing
m
maskedatatimestept. Forthelog-linearscheduleinourexperiments,p =t. Thus,theexpected
m
numberoftokensseenbythediffusionmodelis:
E [training_steps×batch_size×context_length×p ]
t∼U[0,1] m
=training_steps×batch_size×context_length×E [p ]
t∼U[0,1] m
=training_steps×batch_size×context_length×E [t] ∵p =t
t∼U[0,1] m
=training_steps×batch_size×context_length×0.5. ∵E [t]=0.5
t∼U[0,1]
(48)
LM1B. Following[1,25,19],wetrainMDLMfor1Mtrainingstepswithabatch_size=512,and
acontextlengthof128.Like[25]weusealog-linearscheduleandhencethenumberoftokensseenby
ourmodelis≈33B(48).Similarly,MDLMtrainedfor10Msteps,saw327Btokensinexpectation.
ThecorrespondingARbaselinewastrainedfor0.5Mand5Mstepstoensureasimilarnumberoftokens
wasseen.
OWT. We train SEDD and MDLM for 1M training steps with a batch_size = 512,
context_length=1024,andlog-linearschedule.Hence,thesemodelssaw
C.3 Lowdiscrepancysampler
Toreducevarianceduringtrainingweusealow-discrepancysampler,similartothatproposedinKingma
etal.[21].Specifically,whenprocessingaminibatchofN samples,insteadofindependentlysampling
Nfromauniformdistribution,wepartitiontheunitintervalandsamplethetimestepforeachsequence
i∈{1,...,N}fromadifferentportionoftheintervalt ∼U[i−1, i ]. Thisensuresthatoursampled
i N N
timestepsaremoreevenlyspacedacrosstheinterval[0,1],reducingthevarianceoftheELBO.
C.4 LanguageModeling
Forourforwardnoiseprocess,weusealog-linearnoiseschedulesimilartoLouetal.[25].
WedetokenizetheOneBillionWordsdatasetfollowingLouetal.[25],whosecodecanbefoundhere1.
WetokenizetheOneBillionWordsdatasetwiththebert-base-uncasedtokenizer,followingHe
etal.[19].Wepadandtruncatesequencestoalengthof128.
1https://github.com/louaaron/Score-Entropy-Discrete-Diffusion/blob/main/data.py
21We tokenize OpenWebText with the GPT2 tokenizer. We do not pad or truncate sequences – we
concatenateandwrapthemtoalengthof1,024.Whenwrapping,weaddtheeostokenin-between
concatenated.Weadditionallysetthefirstandlasttokenofeverybatchtobeeos.SinceOpenWebText
doesnothaveavalidationsplit,weleavethelast100kdocsasvalidation.
Weparameterizeourautoregressivebaselines,SEDD,andMDLMwiththetransformerarchitecture
fromLouetal.[25].Weuse12layers,ahiddendimensionof768,12attentionheads,andatimestep
embeddingof128whenapplicable.Wordembeddingsarenottiedbetweentheinputandoutput.
WeusetheAdamWoptimizerwithabatchsizeof512,constantlearningratewarmupfrom0toa
learningrateof3e-4for2,500steps.Weuseaconstantlearningratefor1M,5M,or10MstepsonOne
BillionWords,and1MstepsforOpenWebText.Weuseadropoutrateof0.1.
C.5 ZeroshotLikelihood
We evaluate zeroshot likelihoods by taking the models trained on OpenWebText and evaluating
likelihoodsonthevalidationsplitsof7datasets:PennTreeBank(PTB;Marcusetal.[28]),Wikitext
[29],OneBillionWordLanguageModelBenchmark(LM1B;Chelbaetal.[5]),Lambada[31],AG
News[51],andScientificPapers(PubmedandArxivsubsets;Cohanetal.[7]). Wedetokenizethe
datasetsfollowingLouetal.[25]. FortheAGNewsandScientificPapers(PubmedandArxiv),we
applyboththeWikitextandOneBillionWordsdetokenizers.Sincethezeroshotdatasetshavedifferent
conventionsforsequencesegmentation,wewrapsequencesto1024anddonotaddeostokensin
betweensequences.
C.6 RepresentationLearning
FollowingDevlinetal.[10],weevaluateonallGLUEtasks[50],butexcludeWNLI.
Wepre-trainaMosaicBERTmodelonC4[36]for70ksteps,correspondingto36Btokens.Wepadand
truncatethedatato128tokensusingthebert-base-uncasedtokenizer.
MosaicBERT[33]hasasimilararchitecturetobert-base-uncasedandhas137Mparameters,12
layers,12attentionheads,ahiddendimensionof768,anintermediatesizeof3072,andALiBiattention
bias[34].
Forpre-training,weusethefollowinghyperparameters: Aglobalbatchsizeof4096withgradient
accumulation,alearningrateof5e-4,lineardecayto0.02xofthelearningratewithawarmupof0.06x
ofthefulltrainingduration,andthedecoupledAdamWoptimizerwith1e-5weightdecayandbetas
0.9and0.98.
Fordiffusionfine-tuningweuseAdamWwithawarmupof2,500stepsfromalearningrateof0to5e-5,
betas0.95and0.999,andbatchsize512.Wetrainfor5kstepstotal,correspondingto32Mtokens.
ForGLUEevaluation,weusetheHuggingFacescriptfoundhere2.Weusethedefaultparametersfor
alldatasets,exceptforabatchsizeof16,whichwefoundhelpedwithsmallerdatasets.Thisincludes
thedefaultof3epochsforalldatasetsandlearningrateof2e-5.
C.7 DiffusionDNAModels
Dataset Wepre-traintheCaduceusMLM[38]ontheHG38humanreferencegenome[8].Follow-
ingSchiffetal.[38],weusecharacter-/basepair-leveltokenization. Thedatasetisbasedonthe
splitsusedinAvsecetal.[3]: thetrainingsplitcomprisesof35billiontokenscoveringthehuman
genome.Thisconsistsof34,021segmentsextendedtoamaximumlengthof1,048,576(220segments).
Wemaintainaconstant220 tokensperbatch. FortheGenomicsBenchmarktasks,weuse5-fold
cross-validationwherewesplitthetrainingsetinto90/10train/validationsplits.
Architecture TheCaduceusMLMusesasabackboneabi-directionalvariantofthedata-dependent
SSMMambablockproposedinGuetal.[16]. Thisarchitectureisidealasitcontainsinductive
biasesthatpreservereversecomplement(RC)equviariance, respectingtheinherentsymmetryof
double-strandedDNAmolecules[27,38,52].
2https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification
22Trainingdetails Allmodelsarepre-trainedon10Btokens(10Ksteps)andfine-tunedonagenerative
objectiveforanadditional50Btokens(50Ksteps).Weuseaglobalbatchsizeof1024foracontext
lengthof1024tokens.Downstreamtaskfine-tuningisperformedfor16Ksteps(1Btokens).
ForperformingCaduceusMLMpre-training,wefollowSchiffetal.[38]forthemodelsizeconfigura-
tion,andhyperparameterselection.Forpre-training,weuseafixed15%maskrateasdoneinDevlin
etal.[10].Ofthe’masked’tokens,80%arereplacedwith[MASK],10%arereplacedwitharandom
tokenfromthevocabulary,and10%areleftunchanged.
Forfine-tuningallMamba-basedmodels(includingCaduceus)ondiffusionobjectives,welowerthe
learningratefrom8e-3to1e-3.Forfine-tuningHyenaDNA[30],welowerthelearningratefrom6e-4
to5e-5.SimilartoGuetal.[16],Schiffetal.[38],wefoundthatMamba-basedmodelswererobust
tohigherlearningrates.WeexcludetimestepembeddingsforallDiffusionDNAexperiments,aswe
showithasminimalimpactongenerativeperformance(seeTable12,Suppl.D.5).
Weperformdownstreamtaskfine-tuningonthefinalhiddenstateembeddingfrompre-training.We
performmeanpoolingacrossthesequencelength,whichmayvaryfrom200toapproximately2,000
bps.Wereportthemeanand±onmax/minclassificationaccuracyover5-foldcross-validation(CV)
usingdifferentrandomseeds,withearlystoppingonvalidationaccuracy. Foreachtask,wedoa
hyperparametersweepoverbatchsizeandlearningrateandreportthevaluesofthe5-foldCVforthe
bestconfiguration.
GenomicBenchmarkTaskDistributions WeuseasubsetoftheGenomicBenchmarktaskswith
anemphasisontasksfromHumandata. Thepositivesamplesforeachdatasetweregeneratedby
selectingsamplesthatwereannotated,eithercomputationallyorexperimentally,inpreviouswork
(e.genhancers,promoters,openchromatinregions(OCR))[14].Theseannotationseachcorrespond
tosubsetsofthegenomeofvaryingsizesthatmayexhibitdifferentdistributionsofDNAthanthose
observedgloballyoverthereferencegenome.Duetothis,theobserveddatasetmayhaveadifferent
distributionthanthedatausedforpre-trainingandcalculatingperplexity.Thismightinturnleadtoa
casewhereperplexityanddownstreamperformancemaynotnecessarilycorrelate.
AppendixD AdditionalExperiments
D.1 Noisescheduleparameterization
AsdescribedinSec.3.4,theELBOisinvarianttothefunctionalformofα . Todemonstratethis,
t
weevaluateMDLM,initiallytrainedusingalog-linearscheduleonOWT,byreplacingthenoise
schedulewithvariousothernoiseschedulesasmentionedbelow.Followingpriorworks[1,25,39],we
parameterizeα =e−σ(t),whereσ(t):[0,1]→R+.Variousfunctionalformsofσ(t)arelistedbelow:
t
LogLinear[1,25,39].Theloglinearscheduleisgivenas:
σ(t)=−log(1−t) (49)
CosineSquaredschedule[18].TheCosineSquaredscheduleisgivenas:
(cid:16)π (cid:17)
σ(t)=−logcos2 (1−t) (50)
2
Cosineschedule.TheCosinescheduleisgivenas:
(cid:16)π (cid:17)
σ(t)=−logcos (1−t) (51)
2
Linear.TheLinearscheduleisgivenas:
σ(t)=σ t (52)
max
whereσ isaverylargenumber.Inourexperimentswesetitto108.
max
D.1.1 ELBOInvarianve
Thefunctionα isinvertibleduetothemonotonicityassumptioninSec.3.1,andsowecanperform
t
thefollowingchangeofvariablesin(10): γ≡log(1−α ). Letf :[0,1]→R− beafunctionsuch
t
23thatγ=f(t). Notethatα goesthroughamonotonictransformationtoobtainγ;hence,γ isalso
t
monotonicintsinceα ismonotonicint.Thisimpliesthatthefunctionf isinvertible.Lett=f−1(γ).
t
Then,wecanwehavethefollowingdiffusionloss:
(cid:90) t=1 α′
L∞ =E t log⟨x (z ,t),x⟩dt
NELBO q 1−α θ t
t=0 t
(cid:90) t=1 d
=−E log⟨x (z ,t),x⟩ [log(1−α )]dt
q θ t dt t
t=0
(cid:90) t=1 d
=−E
q
log⟨x θ(z t,t),x⟩ dt[f(t)]dt Substitutingf(t)=log(1−α t)
t=0
(cid:90) γ=0
=−E
q
log⟨x θ(z f−1(γ),f−1(γ)),x⟩dγ Changeofvariablesγ≡f(t)
γ=−∞
(cid:90) γ=0
=−E log⟨x (z˜ ,f−1(γ)),x⟩dγ z˜ ≡z
q θ γ γ f−1(γ)
γ=−∞
(cid:90) γ=0
=−E log⟨x˜ (z˜ ,γ),x⟩dγ x˜ (z˜ ,γ)≡x (z˜ ,f−1(γ)) (53)
q θ γ θ γ θ γ
γ=−∞
Thisnewformulationdemonstratesthatthediffusionlossisinvarianttothefunctionalformofα .
t
InTable9wedemonstrateempiricallythatnoisescheduleswithdifferentfunctionalformsevaluateto
thesameLikelihoodwhichisconsistentwithourtheory.However,differentschedulesleadtodifferent
perdatapointvariance. Notably,thelog-linearscheduleexhibitsthelowestvarianceamongallthe
noiseschedulesconsidered.
Table9: Likelihoodinbitsperdimension(BPD)fordifferentnoiseschedulesonOWTdataset,is
reportedalongwiththemeanandvarianceassociatedwitheachnoisescheduleperdatapoint. We
empiricallyobservethatnoisescheduleswithdifferentfunctionalformsyieldthesamelikelihood,
consistentwithourtheoryinSec.3.4;however,differentschedulesresultindifferentvariances.
σ(t) Mean Varianceperdatapoint
LogLinear(49) 3.30 1.81
Cosine(51) 3.30 3.30
CosineSquared(50) 3.30 3.30
Linear(52) 3.30 7.57
D.2 Fastersamplingwithcaching
InFigure10wecomparethewallclocktimesofvariaousmethods:AR,SEDD,MDLMwithcaching,
andMDLMwithoutcachingforgenerating64samplesonasingleGPU.Whensamplinginbatches,a
changeof1tokenwouldnecessitateacalltothedenoisingmodel.Therefore,smallerbatchsizeshave
alowerlikelihoodofatokenbeingunmasked. Thismightleadonetoprefergeneratingsamplesin
smallerbatches,asopposedtousingalargerbatchsizethatfullysaturatestheGPU.Table10shows
thatgeneratingsampleswithabatchsizeof1andusingcachingistwiceasfastasgeneratingsamples
withoutcachingwhilefullyutilizingtheGPU.InFig.2,weobservethatMDLMwithoutcaching
yieldssamplesthatconsistentlygetbettergenerativeperplexitythanSEDD.ForT={5k,10k},both
SEDDandMDLMgetbettergenerativeperplexitythantheARmodel.
Table10:Wallclocktimereportedinminutestogenerate64samplesonasingleA5000GPU.
T=5k(↓) T=10k(↓)
MDLM 70.3 127.9
+caching 40.1 60.4
24Generative perplexities across sample times on OpenWebText
Method
90 MDLM w/ caching (Batch size = 1)
MDLM w/ caching (Batch size = 8)
MDLM w/o caching (Batch size = 16)
80
SEDD (Batch size = 16)
y
exit
70
AR (Batch size = 16)
pl
er
p 60
e
v
ati
50
er
n
e
G 40
30
20
0 2k 4k 6k 8k
Sampling wall clock time (s)
Figure2:Generativeperplexitiesacrosswallclocktimeforgenerating64samplesonOWTusinga
single32GBA5000GPUarecomparedbyvaryingT∈{100,500,1000,5000,10000}inthereverse
diffusionprocess.Thesamplesaregeneratedinmini-batcheswithabatchsizeof16forAR,SEDD,
andMDLMwithoutcaching,asitisthelargestbatchsizethatfitsonthisGPU.ForMDLMwith
caching,wevarythebatchsize.
D.3 LM1Bablations
Weassesstheimportanceofourcontinuous-timeframeworkbyperformingablationondiffusionsteps
T.InTable11,wecompareNLLandPPLundercontinuousanddiscreteTinMDLM.Wefindthat
NLLconsistentlydecreasesasT→∞.
Table11:DiscretevscontinuoustimeevaluationforMDLMw/otime-conditioningonOWT.MDLM
wastrainedwithT=∞.WereporttestperplexityforadiscreteT.
T PPL(≤)
∞ 23.05
10 42.18
20 30.70
50 25.77
100 24.35
200 23.66
500 23.26
1000 23.15
D.4 TrainNLLcurvesonOWT
InFigure3,weshowthatMDLMachieveslowervariancelossduringtrainingcomparedtoaprevious
diffusionlanguagemodel,SEDD.Trainingisperformedover1MstepsonOWT(whichcorrespondsto
524Btokens).
25Train Negative Log-Likelihood (NLL) on OpenWebText
Method
5.5 SEDD
AR
MDLM
5
4.5
L
NL 4
3.5
3
2.5
0.2M 0.4M 0.6M 0.8M 1M
Train steps
Figure3: Trainnegativelog-likelihood(NLL)curvesacross1Mgradientsteps(524Btokens)on
OpenWebText[13].NLLisloggedevery1Kstepswithoutvaluesmoothing.
D.5 Time-conditioningablationonOWT
InTable12,weassesstheimportanceoftimeconditioninginMDLMonOWT.Weobservethat
time-conditioninghasminimalimpactonperplexity.Trainingisperformedover1MstepsonOWT
(whichcorrespondsto524Btokens).
Table12:Ablationontime-conditioninginMDLMonOWT.
Method PPL
MDLMw/time-conditioning 23.21
MDLMw/otime-conditioning 23.05
26NeurIPSPaperChecklist
The checklist is designed to encourage best practices for responsible machine learning research,
addressingissuesofreproducibility,transparency,researchethics,andsocietalimpact.Donotremove
thechecklist:Thepapersnotincludingthechecklistwillbedeskrejected.Thechecklistshould
followthereferencesandfollowthe(optional)supplementalmaterial.ThechecklistdoesNOTcount
towardsthepagelimit.
Pleasereadthechecklistguidelinescarefullyforinformationonhowtoanswerthesequestions.For
eachquestioninthechecklist:
• Youshouldanswer[Yes],[No],or[NA].
• [NA] meanseitherthatthequestionisNotApplicableforthatparticularpaperortherelevant
informationisNotAvailable.
• Pleaseprovideashort(1–2sentence)justificationrightafteryouranswer(evenforNA).
Thechecklistanswersareanintegralpartofyourpapersubmission. Theyarevisibletothe
reviewers,areachairs,seniorareachairs,andethicsreviewers. Youwillbeaskedtoalsoincludeit
(aftereventualrevisions)withthefinalversionofyourpaper,anditsfinalversionwillbepublished
withthepaper.
Thereviewersofyourpaperwillbeaskedtousethechecklistasoneofthefactorsintheirevaluation.
While"[Yes]"isgenerallypreferableto"[No]",itisperfectlyacceptabletoanswer"[No]"provideda
properjustificationisgiven(e.g.,"errorbarsarenotreportedbecauseitwouldbetoocomputationally
expensive"or"wewereunabletofindthelicenseforthedatasetweused"). Ingeneral,answering
"[No]"or"[NA]"isnotgroundsforrejection.Whilethequestionsarephrasedinabinaryway,we
acknowledgethatthetrueanswerisoftenmorenuanced,sopleasejustuseyourbestjudgmentand
writeajustificationtoelaborate.Allsupportingevidencecanappeareitherinthemainpaperorthe
supplementalmaterial,providedinappendix.Ifyouanswer[Yes] toaquestion,inthejustification
pleasepointtothesection(s)whererelatedmaterialforthequestioncanbefound.
IMPORTANT,please:
• Deletethisinstructionblock,butkeepthesectionheading“NeurIPSpaperchecklist",
• Keepthechecklistsubsectionheadings,questions/answersandguidelinesbelow.
• Donotmodifythequestionsandonlyusetheprovidedmacrosforyouranswers.
1. Claims
Question: Dothemainclaimsmadeintheabstractandintroductionaccuratelyreflectthe
paper’scontributionsandscope?
Answer:[Yes]
Justification:Claimsareaddressed
2. Limitations
Question:Doesthepaperdiscussthelimitationsoftheworkperformedbytheauthors?
Answer:[Yes]
Justification: Our method under-performs compared to autoregressive models. We also
discussotherlimitationsinthepaper.
3. TheoryAssumptionsandProofs
Question:Foreachtheoreticalresult,doesthepaperprovidethefullsetofassumptionsanda
complete(andcorrect)proof?
Answer:[Yes]
Justification:Theyareintheproofs.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotincludetheoreticalresults.
27• Allthetheorems, formulas, andproofsinthepapershouldbenumberedandcross-
referenced.
• Allassumptionsshouldbeclearlystatedorreferencedinthestatementofanytheorems.
• Theproofscaneitherappearinthemainpaperorthesupplementalmaterial,butifthey
appearinthesupplementalmaterial,theauthorsareencouragedtoprovideashortproof
sketchtoprovideintuition.
• Inversely,anyinformalproofprovidedinthecoreofthepapershouldbecomplemented
byformalproofsprovidedinappendixorsupplementalmaterial.
• TheoremsandLemmasthattheproofreliesuponshouldbeproperlyreferenced.
4. ExperimentalResultReproducibility
Question: Doesthepaperfullydisclosealltheinformationneededtoreproducethemain
experimentalresultsofthepapertotheextentthatitaffectsthemainclaimsand/orconclusions
ofthepaper(regardlessofwhetherthecodeanddataareprovidedornot)?
Answer:[Yes]
Justification:Weprovideallhyperparametersnecesessarytoreproducetheexperimentsand
willprovidecode.
5. Openaccesstodataandcode
Question:Doesthepaperprovideopenaccesstothedataandcode,withsufficientinstructions
tofaithfullyreproducethemainexperimentalresults,asdescribedinsupplementalmaterial?
Answer:[No]
Justification: Wewillreleaseallcodeafterthepaperisaccepted. Thedatasetsarealready
public.
Guidelines:
• TheanswerNAmeansthatpaperdoesnotincludeexperimentsrequiringcode.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy)formoredetails.
• Whileweencouragethereleaseofcodeanddata,weunderstandthatthismightnotbe
possible,so“No”isanacceptableanswer. Paperscannotberejectedsimplyfornot
includingcode,unlessthisiscentraltothecontribution(e.g.,foranewopen-source
benchmark).
• Theinstructionsshouldcontaintheexactcommandandenvironmentneededtorunto
reproducetheresults. SeetheNeurIPScodeanddatasubmissionguidelines(https:
//nips.cc/public/guides/CodeSubmissionPolicy)formoredetails.
• Theauthorsshouldprovideinstructionsondataaccessandpreparation,includinghow
toaccesstherawdata,preprocesseddata,intermediatedata,andgenerateddata,etc.
• Theauthorsshouldprovidescriptstoreproduceallexperimentalresultsforthenew
proposedmethodandbaselines.Ifonlyasubsetofexperimentsarereproducible,they
shouldstatewhichonesareomittedfromthescriptandwhy.
• Atsubmissiontime, topreserveanonymity, theauthorsshouldreleaseanonymized
versions(ifapplicable).
• Providingasmuchinformationaspossibleinsupplementalmaterial(appendedtothe
paper)isrecommended,butincludingURLstodataandcodeispermitted.
6. ExperimentalSetting/Details
Question: Doesthepaperspecifyallthetrainingandtestdetails(e.g.,datasplits,hyper-
parameters, howtheywerechosen, typeofoptimizer, etc.) necessarytounderstandthe
results?
Answer:[Yes]
Justification:Weprovidedetailedhyperparametersforallexperiments.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
• Theexperimentalsettingshouldbepresentedinthecoreofthepapertoalevelofdetail
thatisnecessarytoappreciatetheresultsandmakesenseofthem.
28• Thefulldetailscanbeprovidedeitherwiththecode,inappendix,orassupplemental
material.
7. ExperimentStatisticalSignificance
Question:Doesthepaperreporterrorbarssuitablyandcorrectlydefinedorotherappropriate
informationaboutthestatisticalsignificanceoftheexperiments?
Answer:[Yes]
Justification:Manyofourtabelsincludeerrorbarsandstandarddeviations
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
• Theauthorsshouldanswer"Yes"iftheresultsareaccompaniedbyerrorbars,confidence
intervals,orstatisticalsignificancetests,atleastfortheexperimentsthatsupportthe
mainclaimsofthepaper.
• Thefactorsofvariabilitythattheerrorbarsarecapturingshouldbeclearlystated(for
example,train/testsplit,initialization,randomdrawingofsomeparameter,oroverall
runwithgivenexperimentalconditions).
• Themethodforcalculatingtheerrorbarsshouldbeexplained(closedformformula,call
toalibraryfunction,bootstrap,etc.)
• Theassumptionsmadeshouldbegiven(e.g.,Normallydistributederrors).
• Itshouldbeclearwhethertheerrorbaristhestandarddeviationorthestandarderrorof
themean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferablyreporta2-sigmaerrorbarthanstatethattheyhavea96%CI,ifthehypothesis
ofNormalityoferrorsisnotverified.
• Forasymmetricdistributions,theauthorsshouldbecarefulnottoshowintablesor
figuressymmetricerrorbarsthatwouldyieldresultsthatareoutofrange(e.g.negative
errorrates).
8. ExperimentsComputeResources
Question:Foreachexperiment,doesthepaperprovidesufficientinformationonthecomputer
resources(typeofcomputeworkers,memory,timeofexecution)neededtoreproducethe
experiments?
Answer:[Yes].
Justification: Weconductallexperimentson8x3090s,8xA6000s,8xA100s,or8xH100s.
ThelargestmodelsonOpenWebTexttake2weekstotrainon8xA100,theLM1Bmodelsonly
take2daystotrainonthesamehardware
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
• ThepapershouldindicatethetypeofcomputeworkersCPUorGPU,internalcluster,or
cloudprovider,includingrelevantmemoryandstorage.
• Thepapershouldprovidetheamountofcomputerequiredforeachoftheindividual
experimentalrunsaswellasestimatethetotalcompute.
• Thepapershoulddisclosewhetherthefullresearchprojectrequiredmorecomputethan
theexperimentsreportedinthepaper(e.g.,preliminaryorfailedexperimentsthatdidn’t
makeitintothepaper).
9. CodeOfEthics
Question: Doestheresearchconductedinthepaperconform, ineveryrespect, withthe
NeurIPSCodeofEthicshttps://neurips.cc/public/EthicsGuidelines?
Answer:[Yes]
Justification:Wefollowstandardpractices
Guidelines:
• TheanswerNAmeansthattheauthorshavenotreviewedtheNeurIPSCodeofEthics.
29• IftheauthorsanswerNo,theyshouldexplainthespecialcircumstancesthatrequirea
deviationfromtheCodeofEthics.
• Theauthorsshouldmakesuretopreserveanonymity(e.g.,ifthereisaspecialconsidera-
tionduetolawsorregulationsintheirjurisdiction).
10. BroaderImpacts
Question:Doesthepaperdiscussbothpotentialpositivesocietalimpactsandnegativesocietal
impactsoftheworkperformed?
Answer:[Yes]
Justification:Ourmodelwillallowformorecontrollabletextgenerationmodels,anddonot
increasethecapabilityofcurrentautoregressivemodels
Guidelines:
• TheanswerNAmeansthatthereisnosocietalimpactoftheworkperformed.
• IftheauthorsanswerNAorNo,theyshouldexplainwhytheirworkhasnosocietal
impactorwhythepaperdoesnotaddresssocietalimpact.
• Examplesofnegativesocietalimpactsincludepotentialmaliciousorunintendeduses
(e.g.,disinformation,generatingfakeprofiles,surveillance),fairnessconsiderations
(e.g.,deploymentoftechnologiesthatcouldmakedecisionsthatunfairlyimpactspecific
groups),privacyconsiderations,andsecurityconsiderations.
• Theconferenceexpectsthatmanypaperswillbefoundationalresearchandnottied
toparticularapplications,letalonedeployments. However,ifthereisadirectpathto
anynegativeapplications,theauthorsshouldpointitout.Forexample,itislegitimate
topointoutthatanimprovementinthequalityofgenerativemodelscouldbeusedto
generatedeepfakesfordisinformation.Ontheotherhand,itisnotneededtopointout
thatagenericalgorithmforoptimizingneuralnetworkscouldenablepeopletotrain
modelsthatgenerateDeepfakesfaster.
• Theauthorsshouldconsiderpossibleharmsthatcouldarisewhenthetechnologyisbeing
usedasintendedandfunctioningcorrectly,harmsthatcouldarisewhenthetechnologyis
beingusedasintendedbutgivesincorrectresults,andharmsfollowingfrom(intentional
orunintentional)misuseofthetechnology.
• Iftherearenegativesocietalimpacts,theauthorscouldalsodiscusspossiblemitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanismsformonitoringmisuse,mechanismstomonitorhowasystemlearnsfrom
feedbackovertime,improvingtheefficiencyandaccessibilityofML).
11. Safeguards
Question: Doesthepaperdescribesafeguardsthathavebeenputinplaceforresponsible
releaseofdataormodelsthathaveahighriskformisuse(e.g.,pre-trainedlanguagemodels,
imagegenerators,orscrapeddatasets)?
Answer:[Yes]
Justification: Thesemodelsaretrainedontrivialdatasetsandunlikelytocauseanyharm
comparedtostateoftheartlanguagemodels.
Guidelines:
• TheanswerNAmeansthatthepaperposesnosuchrisks.
• Releasedmodelsthathaveahighriskformisuseordual-useshouldbereleasedwith
necessarysafeguardstoallowforcontrolleduseofthemodel,forexamplebyrequiring
thatusersadheretousageguidelinesorrestrictionstoaccessthemodelorimplementing
safetyfilters.
• DatasetsthathavebeenscrapedfromtheInternetcouldposesafetyrisks.Theauthors
shoulddescribehowtheyavoidedreleasingunsafeimages.
• Werecognizethatprovidingeffectivesafeguardsischallenging,andmanypapersdo
notrequirethis,butweencourageauthorstotakethisintoaccountandmakeabestfaith
effort.
12. Licensesforexistingassets
30Question: Arethecreatorsororiginalownersofassets(e.g.,code,data,models),usedin
thepaper,properlycreditedandarethelicenseandtermsofuseexplicitlymentionedand
properlyrespected?
Answer:[Yes]
Justification:Allassetsarepublicallyavailableandwerespectthelicensesforallthedata.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotuseexistingassets.
• Theauthorsshouldcitetheoriginalpaperthatproducedthecodepackageordataset.
• Theauthorsshouldstatewhichversionoftheassetisusedand,ifpossible,includea
URL.
• Thenameofthelicense(e.g.,CC-BY4.0)shouldbeincludedforeachasset.
• Forscrapeddatafromaparticularsource(e.g.,website),thecopyrightandtermsof
serviceofthatsourceshouldbeprovided.
• Ifassetsarereleased,thelicense,copyrightinformation,andtermsofuseinthepack-
ageshouldbeprovided.Forpopulardatasets,paperswithcode.com/datasetshas
curatedlicensesforsomedatasets.Theirlicensingguidecanhelpdeterminethelicense
ofadataset.
• Forexistingdatasetsthatarere-packaged,boththeoriginallicenseandthelicenseofthe
derivedasset(ifithaschanged)shouldbeprovided.
• Ifthisinformationisnotavailableonline,theauthorsareencouragedtoreachouttothe
asset’screators.
13. NewAssets
Question:Arenewassetsintroducedinthepaperwelldocumentedandisthedocumentation
providedalongsidetheassets?
Answer:[NA]
Justification:Weprovidenonewassets.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotreleasenewassets.
• Researchersshouldcommunicatethedetailsofthedataset/code/modelaspartoftheir
submissions via structured templates. This includes details about training, license,
limitations,etc.
• Thepapershoulddiscusswhetherandhowconsentwasobtainedfrompeoplewhose
assetisused.
• Atsubmissiontime,remembertoanonymizeyourassets(ifapplicable).Youcaneither
createananonymizedURLorincludeananonymizedzipfile.
14. CrowdsourcingandResearchwithHumanSubjects
Question:Forcrowdsourcingexperimentsandresearchwithhumansubjects,doesthepaper
includethefulltextofinstructionsgiventoparticipantsandscreenshots,ifapplicable,aswell
asdetailsaboutcompensation(ifany)?
Answer:[NA]
Justification:[NA]
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotinvolvecrowdsourcingnorresearchwith
humansubjects.
• Includingthisinformationinthesupplementalmaterialisfine,butifthemaincontri-
butionofthepaperinvolveshumansubjects,thenasmuchdetailaspossibleshouldbe
includedinthemainpaper.
• AccordingtotheNeurIPSCodeofEthics,workersinvolvedindatacollection,curation,
orotherlaborshouldbepaidatleasttheminimumwageinthecountryofthedata
collector.
15. InstitutionalReviewBoard(IRB)ApprovalsorEquivalentforResearchwithHuman
Subjects
31Question:Doesthepaperdescribepotentialrisksincurredbystudyparticipants,whethersuch
risksweredisclosedtothesubjects,andwhetherInstitutionalReviewBoard(IRB)approvals
(oranequivalentapproval/reviewbasedontherequirementsofyourcountryorinstitution)
wereobtained?
Answer:[NA]
Justification:[NA]
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotinvolvecrowdsourcingnorresearchwith
humansubjects.
• Dependingonthecountryinwhichresearchisconducted,IRBapproval(orequivalent)
mayberequiredforanyhumansubjectsresearch. IfyouobtainedIRBapproval,you
shouldclearlystatethisinthepaper.
• Werecognizethattheproceduresforthismayvarysignificantlybetweeninstitutions
andlocations,andweexpectauthorstoadheretotheNeurIPSCodeofEthicsandthe
guidelinesfortheirinstitution.
• Forinitialsubmissions,donotincludeanyinformationthatwouldbreakanonymity(if
applicable),suchastheinstitutionconductingthereview.
32