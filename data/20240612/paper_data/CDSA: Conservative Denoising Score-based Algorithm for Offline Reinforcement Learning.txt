CDSA: Conservative Denoising Score-based Algorithm
for Offline Reinforcement Learning
ZeyuanLiua,*,KaiYanga,∗ andXiuLia,**
aTsinghuaShenzhenInternationalGraduateSchool,TsinghuaUniversity
Abstract. Distributionshiftisamajorobstacleinofflinereinforce- However,mostpreviousconservativeofflineRLalgorithmsfailed
ment learning, which necessitates minimizing the discrepancy be- tofullydisentangletheknowledgerelatedtoconservatismfromthe
tweenthelearnedpolicyandthebehaviorpolicytoavoidoveresti- algorithm’s training process. This knowledge is typically incorpo-
matingrareorunseenactions.PreviousconservativeofflineRLalgo- rated into functions such as the final policy or critics, rendering it
rithmsstruggletogeneralizetounseenactions,despitetheirsuccess inseparablefromothercomponents.Consequently,evenifthetrain-
inlearninggoodin-distributionpolicy.Incontrast,weproposetouse ingdatasetremainsunchanged,variousalgorithmsareunabletodi-
thegradientfieldsofthedatasetdensitygeneratedfromapre-trained rectly exchange their conservatism-related knowledge, particularly
offlineRLalgorithmtoadjusttheoriginalactions.Wedecouplethe ifthisknowledgeisdeemedsolelydependentonthedistributionof
conservatism constraints from the policy, thus can benefit wide of- the training dataset. To tackle this issue, we explore the possibil-
flineRLalgorithms.Asaconsequence,weproposetheConservative ityoflearningconservatism-relatedknowledgeexclusivelyfromthe
DenoisingScore-basedAlgorithm(CDSA)whichutilizesthedenois- trainingdatasettoobtainaplug-and-playdecisionadjuster.Onein-
ingscore-basedmodeltomodelthegradientofthedatasetdensity, tuitiveapproachistoleveragethedensitydistributionofeachdataset
ratherthanthedatasetdensityitself,andfacilitatesamoreaccurate to guide the agent towards states located in areas of high density
andefficientmethodtoadjusttheactiongeneratedbythepre-trained asmuchaspossible.Thiscanbeachievedbyadjustingtheactions
policy in a deterministic and continuous MDP environment. In ex- withinthedatasettosteertransitionstowardsstateswithhigherden-
periments,weshowthatourapproachsignificantlyimprovestheper- sity.Essentially,thismodificationmakestheexecutedactionssafer
formanceofbaselinealgorithmsinD4RLdatasets,anddemonstrate and more conservative. This approach enables us to utilize any of-
thegeneralizabilityandplug-and-playcapabilityofourmodelacross flineRLalgorithmasthebaselinealgorithm.Byensuringthealgo-
differentpre-trainedofflineRLpolicyindifferenttasks.Wealsoval- rithmusesthesametrainingdataset,wecaneffectivelyharnessthe
idatethattheagentexhibitsgreaterriskaversionafteremployingour acquiredplug-and-playconservatism-relatedknowledgetoenhance
methodwhileshowcasingitsabilitytogeneralizeeffectivelyacross thealgorithm’sperformance.
diversetasks. To obtain the distribution of the dataset, various methods com-
monlyutilizetheapproachofreconstructingthedensitywithinthe
trainingdataset[18,9,32,10,28].Previousstudiesinthisdomain
1 Introduction have utilizeddensity modelsto restrict orregularize thecontroller,
preventingtheagentfromselectingactionsorexploringstateswith
Reinforcementlearning(RL)algorithmshavebeendemonstratedto low likelihood in the dataset. In contrast, our approach does not
besuccessfulonarangeofchallengingtasks,fromgames[29,35] impose constraints but rather guides the agent. Specifically, during
toroboticcontrol[34].Asoneofitsbranches,offlineRLonlyuses eachstepofthedecision-makingprocess,wegeneratemultipleac-
afixedtrainingdatasetduringtraining,whichaimsatfindingeffec- tion components to supplement the original action produced by a
tive policies while avoiding online interactions with the real envi- pre-trained offline RL algorithm. This encourages the agent to se-
ronment and a range of related issues [20, 9]. However, due to the lect actions that are more prevalent in the dataset and transition to
data-dependent nature of offline RL algorithms, this pattern makes stateswithhigherlikelihoods,mitigatinginaccuraciescausedbydis-
distribution shift a major obstacle to the effectiveness of the algo- tributionshifts.Additionally,iftheagentencountersalow-likelihood
rithm [9, 25]. Offline RL algorithms are prone to producing inac- state,ourmethodprovidesguidancetonavigateawayfromregions
curatepredictionsandcatastrophicactioncommandswhenqueried lackingsufficientsupportinthetrainingdata.
outsideofthedistributionofthetrainingdata,whichleadstocatas- Combiningthesealtogether,weproposetheConservativeDenois-
trophicoutcomes.Tostrikeasuitabletrade-offbetweenlearningan ing Score-based Algorithm (CDSA), which does not intervene in
improved policy and minimizing the divergence from the behavior thetrainingprocessoftheoriginalofflineRLalgorithm.CDSAad-
policy,aimingtoavoiderrorsduetodistributionshift,previouswork justs the generated actions during testing while avoiding excessive
hasprovidedvariousperspectives,includingconstrainingthesystem interference with the original decisions. It also proactively guides
inthetrainingdatasetdistribution[18,9,26],ordevelopingadistri- the trajectory into regions of higher density in the training dataset
butionalcritictoleveragerisk-aversemeasures[27,41]. as shown in Figure 1. Our idea is similar to the approach used in
LyapunovDensityModels(LDM)[15],whichemploysstrongthe-
∗Equalcontribution. oretical constraints and model-based planning methods to keep the
∗∗CorrespondingAuthor.Email:li.xiu@sz.tsinghua.edu.cn
4202
nuJ
11
]GL.sc[
1v14570.6042:viXra(a)
(b)
Figure1. TheConservativeDenoisingScore-basedAlgorithm(CDSA)leveragesconservatism-relatedknowledgetoenhancetheperformanceofofflineRL
algorithms.Asdepictedin(a),theoriginalRLalgorithmgeneratesactionsbasedonthecurrentstatetointeractwiththeenvironment.Toaddressthedistribution
shiftproblem,weproposetogenerateauxiliaryactionsbasedonthecurrentaction-statepair,guidingtheentiretrajectorytowardshigh-densityregionsofthe
trainingdataset.Thisisillustratedin(b),whereCDSAgeneratestwoactioncomponents,utilizingconservatism-relatedknowledgeacquiredfromthetraining
dataset,tobeaddedtotheactiongeneratedfromapre-trainedpolicyπ.
agentconfinedtohigh-densityregionsofthetrainingdataset.How- drawlessonsfromtheseworksandproposetolearnagradientfield,
ever,intheLDMapproach,thedensityofearlyterminationpoints whichcansolvethebootstrappingerrorproblemmoreflexibly.
mustbemanuallylabeledtomaintainlowdensityforspecificexper-
iments(asoutlinedinsectionD.2of[15]).Thisintroducessubjective 2.2 Risk-averseRL
humanknowledgeandimposesstringentconstraintsthatrestrictthe
algorithm’sversatility.Incontrast,ourmethodologyoperateswith- In risk-aversereinforcement learning, thegoal is to optimizesome
out such limitations. Moreover, while LDM exclusively deals with riskmeasureofthereturns.Themostcommonrisk-aversemeasures
scenarioswithinthetrainingdatadistribution,ourapproachalsoin- aretheValue-at-Risk(VaR)andConditionalValue-at-Risk(CVaR)
corporates guiding the agent outside the training data distribution. [33], which use quantiles to parameterize the policy return distri-
Furthermore,itisworthnotingthatwhileLDMtrainsaDynamics bution. There are also other measures such as the Wang measure
ModelandperformsMPC,recurrentuseofthedynamicsmodelmay [44],themean-variancecriteria[30],andthecumulativeprobability
resultinaccumulatinginferenceerrors.Toaddresstheadverseeffects weighting(CPW)metric[40].However,ourgoalisnottooptimize
stemming from network uncertainty, we adopt a strategy where, in thesecriteriabuttousethemasanevaluationindicatorofthecon-
eachstep,weemploytheinversedynamicsmodelforinferenceonly servatismofalgorithms.OurworkmainlyfocusesonVaRanduses
once.Thispracticeeffectivelyminimizeserrors. itasameasuretoevaluatetherisk-averseabilityofouralgorithm.
Our experiments demonstrate that CDSA directly enhances the
performance of Offline RL baseline algorithms and can be applied 2.3 Scored-basedmodel
todifferentalgorithmswithoutanyfine-tuningornewconservatism-
Recently,score-basedgenerativemodels[36,37,42]havereceived
relatedknowledgelearning.Thebaselinealgorithms,IQL[17]and
muchattention.Themainideaofthescore-basedmodelisthatthe
POR [49], exhibit improvements of 12.7% and 5.2% respectively
probability distribution of real data is represented by score [22], a
whenenhancedwithCDSAond4rldataset.Wealsoconductedsup-
vector field, which points to the direction where the data is most
plementaryexperiments,includingtheablationstudyontheeffectof
likelytoincrease.Leveragingthelearnedscorefunctionasaprior,
auxiliaryactionsinAppendixC.
wecanperformtheLangevinMarkovchainMonteCarlosampling
[46]togeneratethedesireddatafromrandomnoise[43].Thescore-
2 RelatedWork
based model has been successful at generating data from various
2.1 OfflineRL modalitiessuchasimages[13,37],audio[16]andgraphs[31].Some
methodsintroducescore-basedmodelsintoRL[47],whichtrytouse
Offline(orbatch)RLaimstolearnapolicyusingafixeddatasetcol- thescore-basedmodeltosolvetheproblemofobjectrearrangement.
lectedbysomeunknownbehaviorpolicies[20,21].Thecriticalchal- Itonlyconsidershowtoimprovetheprobabilityofthestatesinthe
lengeinofflineRListhedistributionshift[9,18],wheretheagent original distribution and must participate in the training process of
overestimatesandpreferstheout-of-distribution(OOD)actions,with thebaselinealgorithm.Inourwork,weusethelossfromdenoising
theresultthatitperformspoorly.Currently,therearevarioussolu- scorematching[43],whichismoreconcisethantheoriginalscore-
tionstothisprobleminvolvingconstrainingthelearnedpolicytobe basedmethod.
closer to the original behavior policy [8, 18, 45], regularizing the
criticlearningtobemorepessimisticwithOODactions[17,19,50],
3 Preliminaries
adoptingmodel-basedapproaches[51,6,24,52],leveraginguncer-
taintymeasurement[3,48,2],importancesampling[39,23,10],etc. WeconsidertheMarkovDecisionProcess(MDP)setting[38]with
Inparticular,someworkslearnthedensitymodelofthetrainingdata continuousstatess∈S,continuousactionsa∈A,transitionproba-
tohelpconstraintheagentindistribution[32,28,9,18].Wemainly bilitydistributionfunctionP(·|s,a),rewardfunctionr(s,a),initialstatedistributionρanddiscountfactorγ.TheaimofRListolearna
policyπ(a|s)thatmaximizesthecumulativediscountedreturns
(cid:34) ∞ (cid:35)
π∗ =argmaxE (cid:88) γtr(s ,a ) . (1)
π t t
π
t=0
Weconsidertheofflinesettingsinourwork.Intheofflinesetting,
weonlyhaveaccesstoafixeddatasetD = {(s,a,r,s′)}consist-
ingoftrajectoriescollectedbydifferentpolicies.Theagentcannot
directlyinteractwiththerealenvironmentandwillencounterextrap-
olationerrorswhenvisitingOODstatesortakingOODactions.
The score [22] of probability density p (x) is commonly de-
data
fined as ∇ logp (x). Score matching [14] is able to estimate Figure 2. An example comparing CDSA control and common control.
x data
∇ logp (x) without training a model to estimate p (x). The CDSAcontrolsmorecloselytoareasofdistributionforactiondecisionmak-
x data data
objectiveofscorematchingmodelminimizes
iangge.ntindistribution,weneedtofinddirectionstoincreasethelog-
likelihood of the density for both state and action, and the fastest
1 E (cid:2) ∥s (x)−∇ logp (x)∥2(cid:3) . (2) directionis∇ (s,a)logp data(s,a).Whatwewanttodoistogenerate
2 pdata(x) θ x data 2 auxiliary actions based on the current action-state pair to increase
theprobabilityoftheentiretrajectorywithinthedistribution,which
Once the score function is known, the approximate samples for
is shown in Figure 1. To find an approach to achieve this goal, we
p (x) can be generated by Langevin dynamics. The Langevin
data refertothedenoisingscore-matchingmodel[13]andusenetworks
methodrecursivelycomputesthefollowing
tofindthesedirections.
√ We adapt the approach of approximating the gradient of points
x ←x +α∇ logp (x )+ 2αz , (3)
t t−1 x data t−1 t fromthedenoisingscore-matchingmodeltoofflineRL.Whilethis
model can easily learn the gradient of points since the coordinates
wherez ∼N(0,I)andαisaconstant.Thenoiseaddedtotheequa-
t
are independent, obtaining the gradient of state-action pairs in RL
tionistopreventmultipledatapointsfrommappingtothesameloca-
settingsischallengingduetothedependencybetweenstatesandac-
tionwithinthedistribution.Whenαissmallenoughandthenumber
tions.Toaddressthis,weutilizeascore-matchingmodeltolearnthe
ofiterationsislargeenough,thedistributionofx isconsideredto
t
gradientsofactionsandstatesseparately.
bethesameasp (x).The∇ logp (x)inthisformulaissub-
data x data
Firstly,forthegradientoftheaction,weconsiderlearningthegra-
stituted by s (x) since the score network is a good estimation of
θ
dientbyusinganetworkg (s,a)tofit∇ logp (s,a)whereθis
∇ logp (x). θ a data
x data
theparametersofthisnetwork.Totrainthisnetwork,weadoptthe
Supposestatesandactionsobeyanunknownprobabilitydatadis-
denoisingscore-matchingobjective[43],whichguaranteesareason-
tributionp (s,a)intheenvironment.Thedatasetconsistsofi.i.d.
data
samples {(s,a) s ∈ S,a ∈ A}N from p (s,a). Suppose we ableestimationofthescore.Forsimplicity,wedefinex=(s,a)as
i i=1 data
current state-action pair and define x˜ = (s˜,a˜) as the noised state-
haveapre-trainedofflinepolicyπ(a|s).Ourgoalistomaketheac-
action pair. The pre-specified noise distribution q (x˜|x) is used to
tionmoreconservative,inotherwords,toincreasetheprobabilities σ
perturb x and the target of the network is to learn the score of the
ofp (s,a).
data
perturbedtargetdistribution.Thelossofthenetworkis
4 Methodology
L = 1 E (cid:2) ∥g (x˜)−∇ logq (x˜ |x)∥2(cid:3) . (4)
θ 2 qσ(x˜|x)pdata(x) θ a˜ σ 2
Weuseofflinedatasetsthatconsistoftrajectoriessampledfromany
unknown policy to generate gradient fields for action correction.
We modify the original action a
o
by introducing two action cor- The optimal network satisfies g θ∗(x) = ∇ alogq σ(x) and
rection terms associated with the action and the state respectively: ∇ alogq σ(x) ≈ ∇ alogp data(x). When the loss is small enough,
a←a o+K 1∗a 1+K 2∗a 2,wherea oistheoriginalactionsam- itcanbethoughtthatg θ∗(x) ≈ ∇ alogp data(x).Whenwechoose
pledfromthepolicyπ,a anda encouragestheagenttofavorhigh the pre-specified noise distribution as the normal distribution, the
1 2
likelihoodregions,K andK2arehyperparameters.Thechallenge relationship between original data x = (s,a) and perturbed data
1
liesindesigninga anda giventheunknowngroundeddistribution x=(s˜,a˜)is
1 2
p (s,a).Wedrawinspirationfromthescorematchingmethodto
data s˜=s,a˜∼N(a,σI). (5)
tacklethisissueandproposeoursolution.Wedonotlearnanextra
criticoractornetworkbutlearngradientfieldstomaketheagentin DirectlyoptimizingEq.(4)isdifficultbecausewedonothavethe
orclosetodistribution. directaccesstothe∇ logq (x˜ |x).Thankstothehelpofdiffusion
a˜ σ
score-basedmodel,weshowinthefollowinglemmathatthelosscan
4.1 LearningtheDensityGradientFieldsfromData berewrittentoasimplerform:
Lemma1. ThelossL inEq.(4)isequivalenttothefollowingloss:
Weconsiderthesituationwheretheofflinedatasetcontainsalarge θ
fraction of trajectories, and we refer to the probability distribution 1 (cid:20)(cid:13) z(cid:13)2(cid:21)
ofstate-actionpairsinthisdatasetasp data(s,a).Ourgoalistoget L θ = 2E qσ(x˜|x)pdata(x) (cid:13) (cid:13)g θ(x+σz)+ σ(cid:13) (cid:13)
2
, (6)
ourtrajectoryasclosetothedistributionp (s,a)ofthedatasetas
data
possible so that the agent can make better decisions. To make the wherez=(0,z)andz∼N(0,I).TheproofoftheLemma1canrefertoAppendixA.1.Whenthe Algorithm1TrainingCDSAfromofflinedata
lossconverges,g (x) ≈ ∇ logp (x)canbeusedtomodifythe
θ a data 1: Input:DatasetD,iterationsT,learningrateη θ,η φ,η ϕ
actiontomakeitmoreconservative.
2: Initializeparametersθ,φ,ϕ
Secondly, for the gradient of the state, we consider learning the
3: fort=1,2,..T do
gradientbyusinganetworkh (x)toapproximate∇ logp (x).
φ s data 4: Sample(s t,a,s t+1)∼D
Theapproachemployedforlearningthegradientofstatesisanalo- 5: Samplez,z′ ∼N(0,I)
goustothatoflearningthegradientofactions,whereintheperturbed
6: CalculateL θbyEq.(6);updateθ←θ−η θ∇L θ
datacanbeexpressedas:
7: CalculateL φbyEq.(8);updateφ←φ−η φ∇L φ
s˜∼N(s,σI),a˜=a. (7) 8: CalculateL ϕbyEq.(9);updateϕ←ϕ−η ϕ∇L ϕ
9: endfor
Thelossofthisnetworkcanbeexpressedas 10: Output:networksg θ(s,a),h φ(s,a),F ϕ(s,s′)
L φ = 21 E
qσ(x˜|x)pdata(x)(cid:34)(cid:13)
(cid:13) (cid:13) (cid:13)h φ(x+σz′)+ z
σ′(cid:13)
(cid:13) (cid:13)
(cid:13)2(cid:35)
, (8) a ar ce tioo nbt aa nin ded staf tr eom spat ch ee ts hc ao tre mam xo imde il z, er dep atr ae ss ee tnt di en ng sit th ye ad ti tr he ecti co un rs rei nn t
2
state-action pair. We directly set a as ∆a and then feed ∆s into
1
wherez′ = (0,z′)andz′ ∼ N(0,I).h φ(x)hasareasonableesti- the Inversedynamics model I ϕ(·,·) to obtaina 2. a 1 and a 2 repre-
mationof∇ logp (x)whenthelossfunctionconverges. senttheactioncorrectioncomponentsobtainedfrombothactionand
s data
As mentioned earlier, the quickest way to increase p data(s,a) is stateperspectives.a 1 Weaddthesetwoitemstotheoriginalaction
through ∇ (s,a)logp data(s,a). However, since the current state re- a olinearly,whichcanbewrittenas
mains fixed, we cannot change it while we can change the action.
Thus,weinitiallyplannedtouseaforwardmodelF(s′|s,a)topre- a=a o+δa, (10)
dictthenextstatebasedonthecurrentstateandaction.Thissimple
whereδa=K g (s,a )+K I (s,s+h )andK ,K aretwo
1 θ o 2 ϕ φ 1 2
model-basedtrickcouldhelpusmeasurethedensityofthenextstate
hyperparameters.However,thisauxiliaryactioncanonlytemporarily
inthetrainingdatasetandcalculatethegradientofthedensityfunc-
andquicklyincreasetheprobabilityofthecurrentstate-actionpair
tion.However,thepredictionaccuracyofthisforwardmodeloutside
withinthedistribution,withoutconsideringfuturesituations.There-
thesupportofthetrainingdatasetcannotbeguaranteed.Therefore,
fore,drawingontheideaofgenerativemodel,Afterusingthemodel
itbecomeschallengingfortheresultinggradienttoeffectivelyguide
to obtain the corrected action a, we repeatedly put (s,a) into the
thechangesinaction.
modeltogenerateanewa,andconstantlyimprovetheprobabilityof
To address these issues, we devised a method leveraging an in-
(s,a)inthedistribution.
verse dynamic model. Our aim is to increase the density p of
data
agent states in the dataset. An intuitive approach is to utilize gra-
Algorithm2ControlwithCDSA
dient descent to identify the direction of increasing p , thus ne-
data 1: Input:EnvironmentE,pre-trainedpolicyπ(a|s),
cessitatingtheuseofgradients ofp withrespecttostatessand
data networksg (s,a),h (s,a),I (s,s′),hyperparameters
actionsa,i.e.,∆s = ∇ logp (x),∆a = ∇ logp (x).Inthis θ φ ϕ
s data a data K ,K
way,wecanadjustthesampeldactionaandencouragetheagentto 1 2
2: GetinitialStatesfromE,setdasFalse
comeclosertos+∆s.Inourexperiments,wefoundthatemploy-
3: whilenotddo
ing∆s=∇ logp (x)andintegratinganinversedynamicmodel
I
(s,s˜)toges nerateda ata
nactionasanauxiliaryactioncomponentcan
4: Sampleoriginalactiona ofromπ(a|s)
yϕ ield more stable and effective results. This method demonstrates 5: Getsafetyactioncomponenta 1 =g θ(s,a)
g asre ia tt re er lir ee sli oab ni oli nty lyc oo nm ep mar oe dd et lo -bu as si en dg p∆ res d′ ic= tion∇ ns e′ tl wog orp kd ,at aa( sF op(x po), se·) d, 6 7:
:
G ae ←tsa af oet +ya Kct 1io ∗n ac 1o +mp Kon 2e ∗nt aa 22 =I ϕ(s,s+h φ(s))
8: Rolloutaandget(s′,r,d)
totheoriginalconcept.Theefficacyoftheseauxiliaryactioncompo-
9: endwhile
nentsissubstantiatedthroughexperiments.
10: Sets←s′
Thelossoftheinversedynamicnetworktolearnthisknowledge,
whichislearnedbyimitationlearning[12,1],is:
ThecompletealgorithmispresentedinAlgorithm1and2.Algo-
L =E ∥I (s,s˜)−a∥2, (9) rithm1trainsCDSAusingtheinputdatasetandgeneratesthreenet-
ϕ s,a,s˜∼D ϕ 2
works.Onenetworkservesastheinversedynamicmodel,whilethe
whereI ϕ(s,s˜)istheinversedynamicnetworkthatlearnsactionfrom othertwonetworkscaptureinformationaboutthegradientfields.In
statestostates˜.Whentheinversedynamicnetworkiswell-trained, Algorithm2,thesenetworksareutilizedtoadjusttheagent’sactions
wecaninputtheoriginalstateandnoisedstateintothisnetworkand inthegivenenvironment.Thepre-trainedpolicyusedinAlgorithm
gettheactionwithgoodestimationtochange∆s. 2canbeobtainedfromanyRLbaselinealgorithm,andK andK
1 2
arehyperparametersthatcontrolthescopesofauxiliaryactioncom-
4.2 ControlwithCDSA ponents.
With the essential knowledge covered, we can now utilize the gra-
5 Experiments
dient fields. During the sampling process, we obtain the current
state s and an original action a o from a baseline algorithm such WepresentempiricalevaluationsofCDSAinthissection.Wefirst
as CQL, IQL, and others. From the gradient field, we extract ∆s conducted experiments on environment Risky PointMass and pro-
and ∆a, allowing us to compute a 1 = ∆a ≈ g θ(s,a o) and vided visualized results. We then demonstrate the effectiveness of
a 2 = I ϕ(s,s+∆s) ≈ I ϕ(s,s+h φ(s,a o)). Here, ∆a and ∆s CDSAinofflineD4RL[7]MuJoCoandAntMazedatasets.Wealso(a)environment (b)search (c)CQL
Figure3. ExperimentsintheRiskyPointMassenvironmentaredepictedin(a),wheretheredcirclerepresentstheriskyzone,leadingtonegativerewardsif
occupied.Theagentbeginsatthebluepoint,targetingthepurplecircle.In(b)and(c),weemploysimpleshortestpathfindingandCQLasbaselinealgorithms,
demonstratingthecorrectiveimpactofourmethod.CDSAlearnsfromanofflinedatasetgeneratedbyapretrainedCODACagent,followingtheidentical
trainingprocedureoutlinedinitsofficialrepository[27].Maroontrajectoriesillustratebaselinealgorithmresults,whileblacktrajectoriesdepictagentsequipped
withCDSA.Ourmethoddisplaystwosetsofdirectionarrows:greenindicatingbaselinealgorithmdirections,andblueindicatingconservativeauxiliaryaction
directionsfromCDSA.Thearrowlengthsignifiesactionmagnitude.WithCDSAmodifications,theagenteffectivelyavoidstheriskyregion.
Table1. Averagenormalizedscoresofalgorithms.Wechose7popularofflineRLalgorithmstoevaluatetheeffectivenessofouralgorithm.Thescoresare
takenoverthefinal20evaluationsforMuJoCoand100evaluationsforAntMaze.CDSA(IQL)andCDSA(POR)achievedthehighestscoresin12outof15
tasks.Theabbreviationsinthetablecorrespondtothefollowingmeanings:r=random,m=medium,e=expert,u=umaze,l=large,p=play,d=diverse.
Dataset One-step10%BCTD3+BCCQLCODAC IQL PORCDSA(IQL)CDSA(POR)
hopper-r 5.2 4.2 8.5 7.9 11.0 10.8 12.5 30.9±0.19 31.9±0.14
hopper-m 59.6 56.9 59.3 53.0 70.8 62.1 89.4 65.4±5.38 90.6±10.47
hopper-m-e 103.3 110.9 98.0 105.6 112.0 109.5104.0 112.0±1.54 106.5±3.21
halfcheetah-r 3.7 5.4 11.0 17.5 34.6 16.8 17.2 17.0±0.21 17.5±0.91
halfcheetah-m 48.4 42.5 48.3 47.0 46.3 48.5 48.1 49.1±0.38 48.1±1.13
halfcheetah-m-e 93.4 92.9 90.7 75.6 70.4 79.0 81.6 81.1±1.86 85.0±1.20
walker2d-r 5.6 6.7 1.6 5.1 18.7 5.9 7.6 7.4±0.20 7.9±0.17
walker2d-m 81.8 75.0 83.7 73.3 82.0 79.6 82.1 80.2±1.61 83.8±4.12
walker2d-m-e 113.0 109.0 110.1 113.8 106.0 107.2111.6 113.9±0.29 114.4±1.63
MujocoAverage 57.1 55.9 56.8 55.4 61.3 57.7 61.6 61.9±1.3 65.1±2.6
antmaze-u 64.3 62.8 78.6 74.0 52.8 76.4 88.4 89.4±5.55 93.4±2.07
antmaze-u-d 60.7 50.2 71.4 84.0 38.4 63.2 80.8 86.6±4.03 83.0±11.85
antmaze-m-p 0.3 5.4 10.6 61.2 0.0 65.4 88.2 72.2±17.05 93.8±2.05
antmaze-m-d 0.0 9.8 3.0 53.7 0.0 61.0 88.0 72.4±17.51 89.2±4.76
antmaze-l-p 0.0 0.0 0.2 15.8 1.4 38.0 64.6 53.4±6.69 69.6±12.70
antmaze-l-d 0.0 0.0 0.0 14.9 3.8 35.4 67.4 37.0±4.06 70.4±7.23
AntMazeAverage 20.9 21.4 27.3 50.6 16.1 56.6 79.6 68.5±9.1 83.23±6.8
verifythegeneralizabilityofCDSAwithdifferenttasksinthesame thathavedifferentperformances(random,medium,andexpert,etc.)
environment.Duetospaceconstraints,weplacetheablationstudy inHopper,HalfCheetah,andWalker2denvironments.TheAntMaze
intheappendix. environmentrequirestheagenttomanipulateaquadrupedrobotto
findthetargetpointinamaze.Therearedatasets(umaze,medium,
5.1 RiskyPointMass andlarge)dividedbythesizeofthemaze.Foralldatasetsweusethe
"v2"version.
Consideranantrobotwiththepurposeoffasttravellingfromaran- Inprospect,CDSAshouldbeabletoincreasetheperformanceof
dombeginningconditiontoapurplecircle.Usingredcircles,signifi- thegivenpolicy,henceincreasingthefinalscoreandVaR(valueat
cantexpensesaretriggeredwithalowchance,increasinghazards.To risk)inthesedatasets.
traintheagent,weuseanofflinedataset,whichisthereplaybuffer
Baselines. Wechoosethewell-knownofflinealgorithmsIQL[17]
fromaCODACagent.TheresultisshowninFigure3.Becausethe
andPOR[49]asourbaselinealgorithm.Inourexperiment,wetrain
baselinealgorithmincorrectlyestimatesunfamiliarscenes,itchooses
models using the IQL and POR methods, each with 5 seeds, and
tocrossdangerousareastoreachtheendpoint.UsingCDSA,wecan
thencombinethesemodelswithCDSA.Wecomparethenormalized
successfully get the agent out of dangerous areas and into familiar
scoreswithpopularRLalgorithms,includingOne-step[4],10%BC
situationstomakedecisions.
[5],TD3+BC[8],CQL[19],CODAC[27],IQLandPOR.Theex-
perimentaldetailsareincludedinAppendixB.1.
5.2 D4RLofflinetasks
Implementationdetails. WerunthebaselinealgorithmsIQLand
ToverifytheeffectivenessofCDSAinofflinescenarios,weevaluate POR with their official codes for 1M gradient steps. For CDSA,
ourapproachonD4RLMuJoCoandAntMazedatasets.TheD4RL wetraintwoscoremodelsandtheinversedynamicmodeloneach
MuJoCo benchmark consists of datasets collected by SAC agents datasetfor10,000gradientsteps.Duetospacelimit,wedeferthede-Figure4. ResultsofVaR(thenthpercentileofcumulativesortedreward).HereonlyshowstheresultsofCDSA(IQL)(thebluelines)andIQL(thegreen
lines)intheD4RLbenchmark.TheresultsofthePORandCDSA(POR)algorithmsareshowninAppendixB.1
tailedhyperparametersetupaswellasthesetupofactioncorrection toriesofhighrewardaresafeenoughandhardlycrossriskyregions.
coefficientsK ,K oneachdatasettotable3andtable2intheap- Overall, the auxiliary action components help the agent take safer
1 2
pendix.Allalgorithmsaretrainedwithrandomseeds0-4.Wereport action,loweringtheprobabilityofriskysituationsandincreasingthe
theaverageperformanceofeachmethodpost-training. cumulativerewardinalmosteverytrajectory.
Results. TheoutcomesforalldatasetsaresummarizedinTable1.
Significantly, CDSA (POR) and CDSA (IQL) exhibit superior per-
5.3 Riskytransportation
formanceacross12taskscomparedtothebaselinealgorithms,under-
scoringtheefficacyofCDSAinenhancingperformanceacrossmost
WeuseCDSAintheRiskytransportationenvironment.Thetaskof
datasets.InMuJoCOdatasets,CDSAperformsexceptionallywellin
theagentistofindapathfromthestartpointtothetargetpoint.We
thehopperrandomdataset(186.1%forIQLand155.2%forPOR),
callthistaskpathfindingtask.Thereareriskyregionssuchasmoun-
withmarginalimprovementsinmostdatasets(rangingfrom0%to
tains, rivers, and ice roads, where the agent is at risk of accidents.
25.4%, averaging 6.45%). We attribute the particular advantage of
Wegivealargenegativerewardwithasmallprobabilityinthesear-
CDSAinthehopperrandomdatasettoitspotentiallypoordataqual-
eas to indicate that the agent has an accident. Before reaching the
ityandtherelativelysimplenatureofthehoppercontrolledbythe
targetpoint,theagentreceivesanegativerewardproportionaltothe
agents.CDSA’sabilitytoincorporatesafetyactioncomponentsaids
distancebetweentheagentandthetargetpoint.
instabilizingtherobot,therebyyieldinghigherhealthyrewards(re-
WeuseCDSAtolearnsafegradientfieldsofasafedatasetwhose
wardobtainedformaintainingthehopperhealthyateachtimestep).
trajectorydoesnotcontainanyriskregionsinthisenvironment.We
In expert datasets, CDSA shows only limited enhancement, possi-
use SAC [11] as our baseline algorithms. As depicted in Figure 5,
blyduetothealreadyconservativeandhigh-qualitynatureofexpert
mostofthetrajectoriespassthroughriskyregionstoreachthetar-
datasets, where further conservatism provided by CDSA might not
getpointquickly.Afteraddingauxiliarysafetyactioncomponentsof
yieldsubstantialimprovements.Inantmazedatasets,CDSAdemon-
CDSAineachstep,theagentcanavoidallriskyregionstoreachthe
strates significant improvements across all datasets (ranging from
destination.
1.4%to40.5%,averaging11.4%),indicatingitsefficacyinenviron-
ToverifythatCDSAiseffectiveofgeneralization,wedesigntwo
mentswithsimplerdynamics.WepositthatCDSA’ssuccessacross
moretasks.Thefirsttaskistobringgoodstothetargetpoint.The
these datasets stems from the increased likelihood of action-state
agentmustreachthepointofgoodsandthengotothetargetpoint.
pairswithinthedistribution.
Inthesecondtask,weaddanairporttothisenvironment.Usingan
Risky-averseEvaluation. ToverifythatCDSAcanreducetherisk airplane,theagentcanlandnearthetargetpoint.Weonlyemploythe
ofenteringhazardouszones,wevisualizetheVaRofIQLandCDSA CDSAmodelstrainedforthepathfindingtaskanddonottrainany
(IQL) as shown in Figure 4. Compared to IQL, CDSA (IQL) im- newmodelsfortheseadditionaltasks.Figure5showsthatinthefirst
provesVaRsignificantlyinalmosteverypercentile.Wecanalsosee task,theagentfindsthegoodsontheshortestpathandtravelstothe
thatVaRincreasesmorewhenthequantileissmallersincethetrajec- target point despite the presence of risky regions. The agent tends(a) (b) (c)
(d) (e) (f)
Figure5. Theresultsofriskytransportationexperiment.(a)showsthemapoftheenvironment. isthestartpointand isthetargetpoint,thetrajectories
oftheagentarerepresentedbyseverallinesthatincreasesaturationwiththenumberofsteps. isriver, ismountain,and isice,whichareriskyregions.
(b)showsthestatesintheofflinedataset.Theblackcolorrepresents(cid:80) ap data(s,a) = 0andthewhitecolorrepresents(cid:80) ap data(s,a)isnon-zerointhe
dataset.(c)isthegradientfieldofstateslearnedfromCDSA.Weonlyshowthegradientfieldofstatessincethegradientfieldofactionsishardtopresent.(d)is
theresultsofSACandCDSA(SAC).AfteremployingCDSAtomaintaintheagentwithintheknownregion,theagentexhibitsahigherdegreeofriskaversion.
(e)isthetaskthatrequirestheagenttobringgoodstothetargetpoint, istheregionwherethegoodsareplaced.(f)showstheresultsafteraddingtheairport
tothisenvironment, istheairportarea.Inthesetwotasks,weusetheCDSAmodelslearnedfromthepathfindingtaskwithoutanyfine-tuning.Theagent
avoidsallriskyregionsafterusingCDSA.
to go to the airport and take a plane to the location near the target sameenvironment.
point and then goes to the target point in the shortest path in the While our method shows promising results, it is essential to ac-
second task. After adding auxiliary actions generated from CDSA, knowledgeitslimitations.Specifically,CDSA’seffectivenessiscon-
thetrajectoriesaremuchsafer,andtheagentcancompletelyavoid finedtoscenarioswithcontinuousactionspaces,limitingitsappli-
enteringriskyregions.ItisclearthattheCDSAmodeltrainedinthe cabilityindiscreteactionspaces.Additionally,accuratelydetermin-
sameenvironmentcanbeappliedtoothertasks,whichdemonstrates inghyperparametersK andK posesachallenge,requiringcareful
1 2
thegeneralizationabilityofCDSA. balancing to achieve optimal performance. However, setting these
hyperparameterstothesamevalueoftenyieldssatisfactoryresults,
reducingtheneedforextensivetuningefforts.Anareaforfutureex-
6 Discussion
plorationinvolvesdevelopinganautomatedmechanismforadjusting
Our work introduces the CDSA algorithm, which learns gradient K andK .
1 2
fieldsfromdataandutilizesthemtoacquireauxiliaryactions.These
auxiliary actions guide state-action pairs towards high-density re-
Acknowledgements
gions within the dataset distribution, mitigating exposure to unfa-
miliarstates.SinceCDSAfocusessolelyonlearninggradientfields
By using the ack environment to insert your (optional) acknowl-
fromdata,independentofRLbaselinealgorithms,itseamlesslyin-
edgements,youcanensurethatthetextissuppressedwheneveryou
tegrateswithvariousalgorithmssuchasCQL,IQL,andPOR.Our
use the doubleblind option. In the final version, acknowledge-
experiments in offline settings demonstrate that our method effec-
mentsmaybeincludedontheextrapageintendedforreferences.
tively navigates away from hazardous areas and makes decisions
withinfamiliarscenarioswithinthedatasetdistribution.Combining
baselinealgorithmswithCDSAleadstoimprovedperformanceon
References
D4RL datasets across various qualities. Notably, CDSA (IQL) and
CDSA(POR)exhibitsuperiorperformancein12tasks.Furthermore, [1] P.AbbeelandA.Y.Ng. Apprenticeshiplearningviainversereinforce-
employingCDSAsignificantlyenhancestheValueatRisk(VaR)of mentlearning. InProceedingsofthetwenty-firstinternationalconfer-
baselinealgorithms,underscoringourmethod’srisk-aversecapabil- enceonMachinelearning,page1,2004.
[2] G.An,S.Moon,J.-H.Kim,andH.O.Song. Uncertainty-basedoffline
ity.IntheRiskyTransportationenvironment,wevisualizeandval-
reinforcementlearningwithdiversifiedq-ensemble.Advancesinneural
idatethegeneralizabilityofCDSAacrossdifferenttaskswithinthe informationprocessingsystems,34:7436–7447,2021.[3] C. Bai, L. Wang, Z. Yang, Z. Deng, A. Garg, P. Liu, and Z. Wang. 2083–2089.IEEE,2019.
Pessimisticbootstrappingforuncertainty-drivenofflinereinforcement [29] V.Mnih,K.Kavukcuoglu,D.Silver,A.Graves,I.Antonoglou,D.Wier-
learning.arXivpreprintarXiv:2202.11566,2022. stra,andM.Riedmiller.Playingatariwithdeepreinforcementlearning.
[4] D.Brandfonbrener,W.Whitney,R.Ranganath,andJ.Bruna. Offline arXivpreprintarXiv:1312.5602,2013.
rlwithoutoff-policyevaluation. AdvancesinNeuralInformationPro- [30] H.NamkoongandJ.C.Duchi.Variance-basedregularizationwithcon-
cessingSystems,34:4933–4946,2021. vexobjectives.Advancesinneuralinformationprocessingsystems,30,
[5] L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, 2017.
P.Abbeel,A.Srinivas,andI.Mordatch. Decisiontransformer:Rein- [31] C.Niu,Y.Song,J.Song,S.Zhao,A.Grover,andS.Ermon. Permuta-
forcementlearningviasequencemodeling. Advancesinneuralinfor- tioninvariantgraphgenerationviascore-basedgenerativemodeling.In
mationprocessingsystems,34:15084–15097,2021. InternationalConferenceonArtificialIntelligenceandStatistics,pages
[6] C.Diehl,T.Sievernich,M.Krüger,F.Hoffmann,andT.Bertran. Um- 4474–4484.PMLR,2020.
brella: Uncertainty-aware model-based offline reinforcement learning [32] C.RichterandN.Roy. Safevisualnavigationviadeeplearningand
leveragingplanning.arXivpreprintarXiv:2111.11097,2021. noveltydetection.2017.
[7] J. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine. D4rl: [33] R.T.RockafellarandS.Uryasev. Conditionalvalue-at-riskforgen-
Datasetsfordeepdata-drivenreinforcementlearning. arXivpreprint erallossdistributions.Journalofbanking&finance,26(7):1443–1471,
arXiv:2004.07219,2020. 2002.
[8] S.FujimotoandS.S.Gu. Aminimalistapproachtoofflinereinforce- [34] J.Schulman,S.Levine,P.Abbeel,M.Jordan,andP.Moritz. Trustre-
mentlearning.Advancesinneuralinformationprocessingsystems,34: gionpolicyoptimization.InInternationalconferenceonmachinelearn-
20132–20145,2021. ing,pages1889–1897.PMLR,2015.
[9] S.Fujimoto,D.Meger,andD.Precup. Off-policydeepreinforcement [35] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van
learningwithoutexploration. InInternationalconferenceonmachine Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,
learning,pages2052–2062.PMLR,2019. M.Lanctot,etal. Masteringthegameofgowithdeepneuralnetworks
[10] C.GeladaandM.G.Bellemare.Off-policydeepreinforcementlearning andtreesearch.nature,529(7587):484–489,2016.
bybootstrappingthecovariateshift. InProceedingsoftheAAAICon- [36] Y.SongandS.Ermon. Generativemodelingbyestimatinggradients
ferenceonArtificialIntelligence,volume33,pages3647–3655,2019. ofthedatadistribution. AdvancesinNeuralInformationProcessing
[11] T.Haarnoja,A.Zhou,P.Abbeel,andS.Levine. Softactor-critic:Off- Systems,32,2019.
policymaximumentropydeepreinforcementlearningwithastochastic [37] Y.Song,J.Sohl-Dickstein,D.P.Kingma,A.Kumar,S.Ermon,and
actor. InInternationalconferenceonmachinelearning,pages1861– B.Poole.Score-basedgenerativemodelingthroughstochasticdifferen-
1870.PMLR,2018. tialequations.arXivpreprintarXiv:2011.13456,2020.
[12] J.HoandS.Ermon. Generativeadversarialimitationlearning. Ad- [38] R.S.Sutton,A.G.Barto,etal.Introductiontoreinforcementlearning.
vancesinneuralinformationprocessingsystems,29,2016. 1998.
[13] J.Ho,A.Jain,andP.Abbeel.Denoisingdiffusionprobabilisticmodels. [39] R.S.Sutton,A.R.Mahmood,andM.White.Anemphaticapproachto
Advances in Neural Information Processing Systems, 33:6840–6851, theproblemofoff-policytemporal-differencelearning. TheJournalof
2020. MachineLearningResearch,17(1):2603–2631,2016.
[14] A.HyvärinenandP.Dayan. Estimationofnon-normalizedstatistical [40] A.TverskyandD.Kahneman. Advancesinprospecttheory:Cumula-
modelsbyscorematching. JournalofMachineLearningResearch,6 tiverepresentationofuncertainty.JournalofRiskanduncertainty,5(4):
(4),2005. 297–323,1992.
[15] K.Kang,P.Gradu,J.Choi,M.Janner,C.Tomlin,andS.Levine. Lya- [41] N.A.Urpí,S.Curi,andA.Krause. Risk-averseofflinereinforcement
punovdensitymodels:Constrainingdistributionshiftinlearning-based learning.arXivpreprintarXiv:2102.05371,2021.
control,2022. [42] A.Vahdat,K.Kreis,andJ.Kautz. Score-basedgenerativemodelingin
[16] Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro. Dif- latentspace. AdvancesinNeuralInformationProcessingSystems,34:
fwave:Aversatilediffusionmodelforaudiosynthesis. arXivpreprint 11287–11302,2021.
arXiv:2009.09761,2020. [43] P.Vincent. Aconnectionbetweenscorematchinganddenoisingau-
[17] I.Kostrikov,A.Nair,andS.Levine.Offlinereinforcementlearningwith toencoders.Neuralcomputation,23(7):1661–1674,2011.
implicitq-learning.arXivpreprintarXiv:2110.06169,2021. [44] S.Wang.Premiumcalculationbytransformingthelayerpremiumden-
[18] A.Kumar,J.Fu,M.Soh,G.Tucker,andS.Levine. Stabilizingoff- sity.ASTINBulletin:TheJournaloftheIAA,26(1):71–92,1996.
policyq-learningviabootstrappingerrorreduction.AdvancesinNeural [45] Z. Wang, J. J. Hunt, and M. Zhou. Diffusion policies as an expres-
InformationProcessingSystems,32,2019. sive policy class for offline reinforcement learning. arXiv preprint
[19] A.Kumar,A.Zhou,G.Tucker,andS.Levine.Conservativeq-learning arXiv:2208.06193,2022.
forofflinereinforcementlearning.AdvancesinNeuralInformationPro- [46] M.WellingandY.W.Teh. Bayesianlearningviastochasticgradient
cessingSystems,33:1179–1191,2020. langevindynamics.InProceedingsofthe28thinternationalconference
[20] S.Lange,T.Gabel,andM.Riedmiller. Batchreinforcementlearning. onmachinelearning(ICML-11),pages681–688,2011.
InReinforcementlearning,pages45–73.Springer,2012. [47] M.Wu,F.Zhong,Y.Xia,andH.Dong.Targf:Learningtargetgradient
[21] S.Levine,A.Kumar,G.Tucker,andJ.Fu.Offlinereinforcementlearn- fieldforobjectrearrangement.arXivpreprintarXiv:2209.00853,2022.
ing:Tutorial,review,andperspectivesonopenproblems.arXivpreprint [48] Y.Wu,S.Zhai,N.Srivastava,J.Susskind,J.Zhang,R.Salakhutdinov,
arXiv:2005.01643,2020. andH.Goh.Uncertaintyweightedactor-criticforofflinereinforcement
[22] Q. Liu, J. Lee, and M. Jordan. A kernelized stein discrepancy for learning.arXivpreprintarXiv:2105.08140,2021.
goodness-of-fittests.InInternationalconferenceonmachinelearning, [49] H.Xu,L.Jiang,J.Li,andX.Zhan.Apolicy-guidedimitationapproach
pages276–284.PMLR,2016. forofflinereinforcementlearning. arXivpreprintarXiv:2210.08323,
[23] Y. Liu, A. Swaminathan, A. Agarwal, and E. Brunskill. Off-policy 2022.
policy gradient with state distribution correction. arXiv preprint [50] K.Yang,J.Tao,J.Lyu,andX.Li. Explorationandanti-exploration
arXiv:1904.08473,2019. with distributional random network distillation. arXiv preprint
[24] J.Lyu,X.Li,andZ.Lu. Doublecheckyourstatebeforetrustingit: arXiv:2401.09750,2024.
Confidence-aware bidirectional offline model-based imagination. In [51] T.Yu,G.Thomas,L.Yu,S.Ermon,J.Y.Zou,S.Levine,C.Finn,and
Thirty-sixth Conference on Neural Information Processing Systems, T.Ma. Mopo:Model-basedofflinepolicyoptimization. Advancesin
2022. NeuralInformationProcessingSystems,33:14129–14142,2020.
[25] J.Lyu,X.Ma,X.Li,andZ.Lu. Mildlyconservativeq-learningfor [52] J. Zhang, J. Lyu, X. Ma, J. Yan, J. Yang, L. Wan, and X. Li.
offlinereinforcementlearning. InThirty-sixthConferenceonNeural Uncertainty-driventrajectorytruncationformodel-basedofflinerein-
InformationProcessingSystems,2022. forcementlearning.ArXiv,abs/2304.04660,2023.
[26] J.Lyu,A.Gong,L.Wan,Z.Lu,andX.Li. Stateadvantageweighting
forofflineRL.InInternationalConferenceonLearningRepresentation
tinypaper,2023.URLhttps://openreview.net/forum?id=PjypHLTo29v.
[27] Y.Ma,D.Jayaraman,andO.Bastani.Conservativeofflinedistributional
reinforcement learning. Advances in Neural Information Processing
Systems,34:19235–19247,2021.
[28] R.McAllister,G.Kahn,J.Clune,andS.Levine. Robustnesstoout-
of-distribution inputs via task-aware generative uncertainty. In 2019
InternationalConferenceonRoboticsandAutomation(ICRA),pagesA Proof Table2. Hyperparametersofeachdataset
IQL POR
K1 K2 K1 K2
A.1 Proofofequivalentlossform
hopper-r 0.1 1 0.1 0.5
hopper-m 0.001 0.001 0.001 0.001
Thelossofnetworkg (x˜)is
θ hopper-m-e 0.001 0.001 0.001 0.001
halfcheetah-r 0.001 0.005 0.003 0.03
L = 1 E (cid:2) ∥g (x˜)−∇ logq (x˜ |x)∥2(cid:3) . halfcheetah-m 0.005 0.05 0.01 0.01
θ 2 qσ(x˜|x)pdata(x) θ a˜ σ 2 halfcheetah-m-e 0.01 0.01 0.003 0.03
walker2d-r 0.03 0.3 0.00 0.01
Whenthenoisedistributionq σ(x˜)obeyN(x,σI),thepartialderiva- walker2d-m 0.03 0.003 0.005 0.01
tiveoflogq (x˜)withrespecttoais walker2d-m-e 0.1 0.01 0.00 0.01
σ
antmaze-u 0.5 0.1 0.1 0.1
antmaze-u-d 0.1 0.5 0.05 0.05
∇ a˜logq σ(x˜ |x)=∇
a˜log(cid:18) √1 e−(x˜ 2− σx 2)2(cid:19) a an nt tm ma az ze e- -m m- -p
d
0 0. .1
3
0 0. .3
3
0 0. .1
05
0 0. .1
05
2πσ
antmaze-l-p 0.3 0.3 0.1 0.1
(cid:18) (x˜−x)2(cid:19) antmaze-l-d 0.1 0.1 0.03 0.03
=∇ C−
a˜ 2σ2
Table3. HyperparametersofCDSA
∂x˜(x˜−x) Name Value
=−
∂a˜ σ2
(cid:18) (cid:19) Hiddenlayersofg θ 3
=−(0,1)· (˜s−s) ,(a˜−a) Hiddendimofg θ 32,128,32
σ2 σ2 Activationfunctionofg θ LeakyReLU(0.1)
(a˜−a)
Hiddenlayersofhφ 3
=− . Architecture Hiddendimsofhφ 32,128,32
σ2 Activationfunctionofhφ LeakyReLU(0.1)
HiddenlayersofI ϕ 3
HiddendimsofI ϕ 128
Using reparameterization trick,˜a can be expressed as˜a = a+ ActivationfunctionofI ϕ LeakyReLU(0.2)
σz wherez followsthestandardnormaldistribution.x˜canbealso Optimizer Adam
representasx˜=x+(0,σz).Thelossformulacanbeconvertedinto Learningrateofg θ 3e-4
Hyperparameters
Learningrateofhφ 3e-4
L
θ
= 21 E qσ(x˜|x)pdata(x)(cid:2) ∥g θ(x˜)−∇ a˜logq σ(x˜ |x)∥2 2(cid:3) L Be aa tcrn hi sn ig zerateofI ϕ 21 5e- 63
Iterationnumber 10000
= 21 E
qσ(x˜|x)pdata(x)(cid:34)(cid:13)
(cid:13) (cid:13) (cid:13)g θ(x˜)+ (a˜ σ−
2a)(cid:13)
(cid:13) (cid:13)
(cid:13)2(cid:35)
C AblationStudy
2
= 1 E (cid:20)(cid:13) (cid:13)g (x+(0,σz))+ z(cid:13) (cid:13)2(cid:21) Wepresentanablationstudytoverifytheeffectofauxiliaryactions
2 qσ(x˜|x)pdata(x) (cid:13) θ σ(cid:13) 2 a 1anda 2.Wedenote"CDSAw/oa 1"onlyusesactiona 2tocorrect
= 1 E (cid:20)(cid:13) (cid:13)g (x+σz)+ z(cid:13) (cid:13)2(cid:21) . action a o and "CDSA w/o a 2" only uses auxiliary action a 1. The
2 qσ(x˜|x)pdata(x) (cid:13) θ σ(cid:13) 2 resultsareshowninFigure7.
Wecanseethattheresultsof"CDSAw/oa "and"CDSAw/oa "
1 2
wherez=(0,z)andz∼N(0,I). arebetterthanthebaselinealgorithm,whichprovestheeffectiveness
ofusingauxiliaryactiona anda solely.However,theperformance
1 2
ofCDSAisbetterthanthesetwoalgorithmsandthebaselinealgo-
B Experimentdetails
rithm in almost every dataset, verifying it is better to use both to-
gether.
Our experiments were performed by using the following hardware
andsoftware:
• GPUs:NVIDIAGeForceRTX3090
• Python3.10.8
• Numpy1.23.4
• Pytorch1.13.0
• Mujoco-py2.1.2.14
• Mujoco2.3.1
• D4RL1.1
B.1 D4RLexperiments
TheVaRofPORandCDSA(POR)areshowninFigure6.Theblue
linesrepresenttheresultofthebaselinealgorithmwithCDSAand
the green lines show the result of the baseline algorithm. We can
seethatCDSAcanimprovetheVaRinalmosteverydataset,espe-
ciallywhenthepercentileissmall,verifyingtherisk-averseeffectof
CDSA.Figure6. ResultsofCDSA(POR)andPOR.Figure7. Resultsofablationstudy.