Situational Awareness Matters in 3D Vision Language Reasoning
YunzeMan Liang-YanGui Yu-XiongWang
UniversityofIllinoisUrbana-Champaign
{yunzem2,lgui,yxw}@illinois.edu
Abstract <3D Scene> <Situation> “I am sitting on an armchair facingthecenterofthe
room, andthere is a brown sofa on my right.”
Beingabletocarryoutcomplicatedvisionlanguagerea-
<Question> “Describe to me: what I will see if I stand up from the
soningtasksin3Dspacerepresentsasignificantmilestone armchair and turn right?”
in developing household robots and human-centered em- Previous Models Our SIG3D
bodiedAI.Inthiswork,wedemonstratethatacriticaland
Tokenization Tokenization
distinctchallengein3Dvisionlanguagereasoningissitua-
tionalawareness,whichincorporatestwokeycomponents:
(1) The autonomous agent grounds its self-location based Q K, V
on a language prompt. (2) The agent answers open-ended Self-attention Large
questionsfromtheperspectiveofitscalculatedposition. To TM ru al nt si- fm oro md ea rl + HT ea as dk s C Hro eass d-attention TM ru al nt si- fm oro md ea rl
addressthischallenge,weintroduceSIG3D,anend-to-end Situation
Response <text> Situation <vector> Estimation Response <text>
Situation-Grounded model for 3D vision language reason-
Figure 1. Previous methods perform direct 3D vision language
ing. We tokenize the 3D scene into sparse voxel represen-
reasoningwithoutmodelingthesituationofanembodiedagentin
tation and propose a language-grounded situation estima-
the3Denvironment.Ourmethod,SIG3D,groundsthesituational
tor,followedbyasituatedquestionansweringmodule. Ex-
descriptioninthe3Dspace,andthenre-encodesthevisualtokens
periments on the SQA3D and ScanQA datasets show that
fromtheagent’sintendedperspectivebeforevision-languagefu-
SIG3Doutperformsstate-of-the-artmodelsinsituationes-
sion,resultinginamorecomprehensiveandgeneralized3Dvision
timation and question answering by a large margin (e.g., language(VL)representationandreasoningframework. Q,K,V
an enhancement of over 30% on situation estimation ac- standforquery,key,andvalue,respectively.
curacy). Subsequent analysis corroborates our architec-
turaldesignchoices,exploresthedistinctfunctionsofvisual
themselves inside the 3D world and then perceive and in-
andtextualtokens, andhighlightstheimportanceofsitua-
teract with the surrounding environment from their ego-
tionalawarenessinthedomainof3Dquestionanswering.
perspective(Figure1). Suchsituationalawarenessisacru-
The project page is available at https://yunzeman.
cial difference between 2D and 3D visual understanding,
github.io/situation3d.
and a key to achieving seamless understanding of spatial
concepts in more complex real-world environments. Sev-
eral existing methods recognize the lack of positional un-
1.Introduction
derstanding in 3D and propose new benchmarks and joint
optimizationfunctions[43],orpositionalembeddingmeth-
Humans learn knowledge efficiently through the interac-
ods[25]toenhancetheoverallreasoningperformance.
tionswiththe3Dworldandtheintegrationofmulti-modal
information,suchasverbalguidanceorinstructions. Simi- However, the lack of an explicit situation modeling
larly,introducinglanguageguidanceintothevisualcompre- andsituation-grounded3Dreasoningmethodrestrictsthem
hensiontaskcangreatlyenhancethelearningefficiencyof from obtaining a generalizable and consistent 3D vision-
models[3,41]. Nonetheless,despiteconsiderableadvance- language (VL) representation. As shown in Figure 2, the
mentsinlinguisticunderstanding[7,11,30,57]andvision- situation prediction of the state-of-the-art method [43] (in
languageintegration[3,36,53,60],currentmethodologies blue) diverges significantly from the ground truth vectors
remain deficient in accurately perceiving and rationalizing (inred)inalmostallscenesinthedataset[13]. Moreover,
within real-world 3D environments, which is largely at- ourpilotstudyinSection3alsorevealsthatsituationalun-
tributedtothelackof3Dsituationalreasoningcapabilities. derstanding, despite being very crucial in comprehending
Compared to machine learning models, humans put thecontextofquestions,onlyplaysaminorroleinthefinal
1
4202
nuJ
11
]VC.sc[
1v44570.6042:viXranificant oversight in existing research. To address this, we
introduce SIG3D, a situation-grounded 3D VL reasoning
architecture, specifically designed to fill this void. (2) We
propose an anchor-based approach to situation estimation,
whicheffectivelynarrowstheextensivesearchspacein3D
environmentsforprecisegroundingof3Dpositionsandori-
entationswithtextualdescriptions. Additionally,weinves-
I am facing a trash can, while there is a I am facing the paper towel dispenser with tigate situational alignment and visual re-encoding mech-
door on my right. a chair in my six o’clock direction.
anisms to leverage situational awareness for enhanced QA
performance. (3)Ourmodeldemonstratessuperiorperfor-
mance on two challenging datasets, SQA3D and ScanQA,
surpassing the state of the art in both situation estimation
andQAmetrics. Ablationstudieshighlighttheimportance
ofsituation-guidedencoding,revealingitsbeneficialimpact
ongeneralQAtasks.
I am facing a pool table, taking out the
white ball from the side pocket. I am I am sitting on a couch with a pillow facing
also facing a painting on the wall. another couch and the pillow is on my right.
2.RelatedWork
Figure 2. Situation estimation in existing methods [43] fails in
mostscenarios,indicatingthemissingregistrationbetweenthesit- Vision Language Models (VLMs). Early transformer-
uationaldescriptionsand3Dembeddings.Red:Groundtruth(GT) driven [58] textual and visual encoders [17, 30] have fa-
vector.Blue:Estimatedvector. cilitated great progress in recent vision language learning.
Text-imagecontrastivemodels[29,53]proposetoalignthe
questionanswering(QA)performanceofexistingmethods. featurespaceoftwomodalitieswithlarge-scalepretraining,
In this work, we propose SIG3D, a novel approach de- fuelingnumerousdownstreamtasksfromgeneralizedopen-
signedtopreciselymodelandestimateanembodiedagent’s vocabulary visual perception [21, 33, 35] to text-to-image
ego-locationandorientationfromatextualdescription,be- generation[55]. Concurrently,someworkusestextandvi-
fore performing multi-modal QA tasks from the agent’s sion encoders on separate modalities followed by feature
ego-centricperspective,asshowninFigure1. Specifically, fusion [18, 32] for multi-modal reasoning tasks. Since the
we leverage large-scale pretrained language and visual en- emergenceofLargeLanguageModels(LLMs)[7,57,68],
coderstoprocesstheinputtextand3Ddata,andfusetheto- VLMs have experienced huge improvement with the help
kenswithattentionmodulestopredictasituationalvector. of LLMs as building blocks for multi-modal learning ar-
Previous attempts to directly predict the ego-situation are chitectures. Specifically, recent work directly projects vi-
hindered by the expansive search space inherent in 3D en- sual embeddings into language-space tokens as input to
vironments. Toaddressthischallenge,were-conceptualize LLMs[39,46,69],orusethelatentbottleneckstructurefor
thetaskasananchor-basedclassification,wherevisualto- cross-modal visual decoding [3, 25, 36, 37], or treat LLM
kensareregardedasanchorpoints,andalikelihoodofpo- layersasencoderblocksforvariousvisualtasks[48].
sition together with a set of rotation parameters are con- In the domain of visual question answering (VQA) [4,
currently regressed for each visual token. After obtaining 72],recentworkhaspushedthefrontiertowardsvideoun-
thesituationestimation,weproposeasituationalalignment derstanding [14, 27, 28, 39, 61], knowledge-based under-
and a situation-guided tokens re-encoding strategy, to per- standing[20,23,40,45,56,62],andcommonsensereason-
ceive the environment from the agent’s intended perspec- ing[67]. Despitetheoutstandingperformancein2Dimage
tive. Thesestrategiesenhancethevisualtokenswithmore interpretation,mostexistingmethodslackthecapabilityto
accuratesituationalawarenessforsubsequentQAtasks. generalize to 3D scenarios. In contrast, our work studies
therepresentationofvisualinformationanditsfusionwith
Experimentsontwochallenging3Dvisualquestionan-
languageembeddingsinthe3Ddomainbytargetingonthe
swering(VQA)datasets[5,43]demonstratethesignificant
3Dsituation-guidedvisuallanguageinterpretation.
improvement in situation estimation and QA tasks of our
model. Inparticular, weimprovetheaccuracyofsituation GroundingLanguagein3DSpace. Comparedwith2D
estimation by more than 30%, and subsequent QA perfor- images,knowledgesuchasspatialrelationships,interactive
mance by up to 3%. Further qualitative and quantitative exploration, and topological analysis – which only exists
analysis verifies our design choices and highlights the sig- in the 3D world – provides additional challenges and op-
nificanceofsituationalawarenessin3Dreasoningtasks. portunitiestodevelopbetterlanguagemodelswithstronger
To sum up, our paper has the following contributions: commonsense reasoning capability grounded in the real-
(1)Werecognizethelackofsituationalawarenessasasig- world 3D scenarios. In this direction, early work seeks
2rectlyincorporatingitresultsinonlymarginalalterationsin
Encode GT situational vector in the input 47.2
theQAoutcomes. Omittingthesituationaldescriptionen-
Corrupt situational supervision with noise 47
tirelyfromtheinputresultsinaslight2%decreaseinpreci-
Remove situational description from input 45.3 sion.However,intheabsenceofthisinformation,themodel
Baseline 47.2 resorts to random guessing when determining the correct
answer,asallresponsesdependonthesituation. Thefind-
0 10 20 30 40 Acc. / %
Figure3. ResultsonvariantsoftherepresentativeSQA3Dbase- ingsfromFigures2and3collectivelyindicateadeficiency
line method [43] demonstrate that situational understanding, de- in existing methods regarding situation estimation and the
spite being indispensable in perceiving the context of questions, application of situational understanding in subsequent rea-
makesnegligiblecontributioninexistingmethods.Thismotivates soningtasks. Theseunresolvedchallengesmotivatethede-
asituation-guided3Dencodingmechanisminourmodel. velopmentofourproposedmethod.
to ground isolated objects [1, 9] or objects within more 4.Method
complex scenes [2, 8, 19, 26] using natural language de-
scriptions. Recently, with more collected 3D vision lan- An overview of our approach SIG3D is illustrated in Fig-
guage benchmarks, some work starts to explore language- ure 4. Our method begins with a set of points that repre-
guided3Dvisualinterpretationandreasoningonadiverse sent a 3D scene, accompanied by a situational description
set of datasets, including 3D scene captioning [10], open- and a question that define the overall context of the prob-
vocabularysegmentation[16,31,50],andquestionanswer- lem. We tokenize them into separate token embeddings
ing[5,15,24,64,73]. (Section4.1), andgroundthetextualdescriptioninthe3D
The success of LLMs also elicits the usage of them in scenewithavectorcomprisinglocationandorientation.We
3Dvisionlanguagereasoningfortaskdecomposition[63], find direct single vector estimation to be challenging due
datageneration, andmulti-modalfeaturefusion[25]. Mo- to the vast and complex nature of the 3D search space, so
tivated by ScanQA [5], SQA3D [43] takes the first step to we propose an anchor-based situation estimation strategy
explore the challenging 3D situational reasoning problem (Section4.2).Subsequently,were-encodethevisualtokens
by developing a situated question answering benchmark, from the perspective of situational vectors, enhancing the
andproposingthefirstjointlearningbaselineonthisbench- situationalawarenessfordownstreamreasoningtasks(Sec-
mark. Ourworkhighlightstheuniquenessandsignificance tion4.3). Thefinalizedvisualandtextualtokensarefused
ofsituationalawarenessinthe3Dvisionlanguagelearning byatransformerdecodertogeneratethefinalresponse.
paradigm, leading to notably better 3D situational ground-
4.1.VisualandTextualTokenization
ingandquestionansweringperformance.
Leveraging input scene point clouds and textual prompts,
3.PilotStudyonSituationalReasoning
our objective is to generate three distinct types of tokens:
3D visual tokens z3D ∈ RNv×Cv, situational tokens zS ∈
Despite highlighting the importance of situational under-
RNs×Cs,andquestiontokenszQ ∈ RNq×Cq. Eachtypeof
standing and reasoning, existing methods [43] fall short
tokenischaracterizedbytwoprimarycomponents: N,rep-
in providing effective situation estimation, as illustrated
resenting the number of tokens, and C, encapsulating the
inFigure2.Thissectiondelvesintoapilotstudythatexam-
feature embeddings. To tokenize and capture feature em-
inestheimpactofsituationalunderstandingondownstream
beddings for situational input and questions, we employ a
reasoningtasks.TheSQA3Dbaseline[43]incorporatessit-
sharedtexttokenizerETXTfollowingpriormethods[5,43].
uationaldescriptionsandusesgroundtruth(GT)situational
We assume that situation and question prompts are sepa-
vectors for supervision in a direct regression task. We in-
rated in the input data. If not, LLMs [7] can be used to
vestigatethreevariantsofthisbaselinetoassesstheeffectof
parsethetextualinputwithoutchangingthesemanticmean-
situationalunderstanding.Inthefirstvariant,weremovethe
ingofthesentences. However,thereisalackofconsensus
situationaldescriptionandsupervisionfromthemodel,by
onastandard3DvisualtokenizationmethodE3Dthatisap-
passinginemptysituationaltokens. Inanothervariant,we
propriate for the 3D VL reasoning task, prompting a more
corruptthesituationsupervisionbyintroducingverylarge
detailedexplorationinthefollowingparagraphs.
Gaussian noise to the GT vectors to effectively randomize
them. Finally,wetrytoencodetheGTsituationalvectorin Visual Tokenization. Given an input point cloud p ∈
theinputwithlearnablemulti-layerperceptron(MLP)lay- RN×3,mostpriormethods[5,15,43]adoptaVoteNet[52]
erstoformaGTsituationaltoken. detector to acquire object-level tokens z3D ∈ RNobj×Cobj
Figure 3 demonstrates the results of this study, reveal- as the visual representation, where N is the number of
obj
ingnegligiblechangesinperformanceacrossthesevariants. object proposals, and C is the object-level feature em-
obj
Notably, corrupting the GT situational information or di- beddings. However, we point out several problems with
3“Describe to me: what I will
<Question> see if I stand up and turn
right.”
Large
Text Output Text
Multi-modal
Tokenizer Transformer <Response>
“I am sitting on an armchair
<Situation> facing …there is a brown Situation
sofa on my right.” Interpreter
K,V
3D Situational
Encoding
3D Visual Q Multi-layer
Tokenizer Transformer
<3D Scene>
represent
3D Positional Encoding
Position Likelihood 6D Rotation
Figure4.OverviewofourSIG3Dmodel,whichincludes3Dsceneandtextencoding,anchor-basedsituationestimation,situation-guided
visualre-encoding, andmulti-modaldecodermodules. Wetokenizethe3Dsceneintovoxels, treateachtokenasananchorpoint, and
querythetexttokenstopredictatoken-levelpositionlikelihoodandrotationmatrixtolocatethesituationalvectorassociatedwiththe
textualdescription. Thenweupdatethescenetokenswithsituationalpositionencoding(PE),andfinallyperformthe3DVLreasoning
taskwithalargetransformerdecoder.
this abstraction strategy: (1) A detection-based tokeniza- 4.2.SituationEstimation
tion method tends to ignore the non-object regions in the
Given 3D visual tokens z3D and situational tokens zS, our
scene, which can be indispensable in some reasoning sce-
objectiveistoestimatethesituationalvector⃗sreferredtoby
narios(e.g.,carpetsontheground,ceiling,walls). (2)After
thesituationaldescription,whichcomprisesapositioncom-
object-levelabstraction,thevisualrepresentationlossesthe
ponentsposrepresentedbycoordinates(x,y,z),andarota-
high-level information of the scene (e.g., the shape of the
tion component srot represented by Euler angles (θ,ψ,ϕ),
living room, the corner of the kitchen). (3) A supervised
wherepitchanglesψarealwaysdefinedas0,meaningthat
detector trained from scratch can only recognize objects
situationalvectorsaredefinedtobeparallelwiththeground
within the training set (e.g., only 20 categories for Scan-
plane. Thepriormethod[43]utilizesatransformerblockto
Net[13]),meaningthatthemethoddoesnothavezero-shot
calculatethecross-attentionfeaturebetweenvisualandlan-
capabilitytoreasonaboutnovelunseenobjectsthatarein-
guagetokens,anddirectlyregressafinalsituationalvector
evitablycommoninreal-worldscenarios.
from the averaged attention map. We find such a strategy
Inlightofthese,weadoptapretrainedopen-vocabulary producingveryinaccurateestimates,asshowninFigure2,
voxel-based tokenization method from OpenScene [50]. due to the large search space in the entire 3D volume. In-
The scene is first discretized into regular small 3D voxels spiredbyrecent3Dobjectdetectionmethods[44,65,70],
andfedintoavisualencoderforfeatureextraction: wereducethesearchspacebyturningthelocalizationprob-
z3D =E3D(V(p)), (1) lemintoaclassificationproblem.
where V represents the voxelization process, and E3D is Positional Embedding and Feature Fusion. After the
voxelization and 3D encoding process, each 3D token as-
a Minkowski sparse 3D convolutional network [12]. The
sociateswitha3Dposition(x,y,z)representingthecenter
sparsenetworkispretrainedbydistillationfromCLIP[53]
of its voxel. We first provide positional information to the
embeddings of rendered multi-view 2D images, resulting
modelbygeneratinglearnablepositionalembeddings(PE)
in a feature map with better language alignment and 3D
using a two-layer perceptron for each of the N visual to-
awareness. We take the upsampled bottleneck-layer fea- v
kens,andaddlearnablepositionalembeddingstothetoken
ture embeddings from the encoder network, and compute
features z3D. We use a situation interpreter [54] to extract
the mean average over the z-axis (vertical) to project the
situational information, and ask the updated visual tokens
voxels onto the x-y plane and treat the feature grids in the
toattendtothesesituationaltokenswithseveraltransformer
resulting2DfeaturemapasourN visualtokens. Wefind
v
layerstoproducethejointfeatureembeddings.
that this bird’s-eye-view projection results in a more com-
pactrepresentationandimprovesthefinalperformance. Anchor-basedSituationEstimation. Wetreateachout-
4put token of the feature fusion module as an anchor point, situational information. This structure allows for the re-
and use it to predict a position likelihood p ∈ [0,1] and a encoding of visual tokens under the influence of situation
rotationestimation. Sinceeachtokenhasanassociated3D and question context, guiding the model to assign higher
position (x,y,z), the position likelihood p indicates how weights to situation-related and question-related visual to-
likely the situational vector locates at the center of this to- kens. The output, termed situation-guided visual tokens,
ken(voxel). Wedefineasoftgroundtruthforthisclassifi- embodiesthisre-contextualizedunderstanding.
cationtaskwithaGaussiankernel,meaningthatthecloser
a token is to the actual situational vector spos, a higher 4.4.QuestionAnsweringHead
ground truth probability p will be assigned to that token.
We follow existing methods [25] to use a large vision-
Inordertocounteractthesparsesupervisorysignalandin-
language decoder to fuse the final visual and textual to-
crease the positive supervision around the vector position,
kens and generate textual response to the input question.
weadoptthepeakenlargingtechniqueinCenterPoint[65],
We explore both auto-regressive response generation and
wherethesizeoftheGaussiankernelisincreased(meaning
classification-based answer prediction [5, 43]. For classi-
thattheσ isincreased)toallowdensersupervisionaround fication,wepredictavectorvans ∈ Rna forthecandidates
the vector position. Furthermore, we explore different ro- ofn answersinthetrainingsetfollowing[5].
a
tation representation and find that compared with quater-
nion and (sinθ,cosθ) representations, the 6D vector pro- 5.Analysisin3DVQATask
posed by [71] achieves the best performance. Hence, we
adopt a situation estimation head with MLP layers to out- We evaluated SIG3D for 3D VL reasoning on two chal-
put7-dimensionalvectorforeachofthetokens, wherethe lenging benchmarks, addressing both visually-oriented sit-
firstchannelrepresentsthepositionlikelihoodandtheother uationestimationandtextual-focusedQAtasks.Wepresent
sixchannelsrepresentthe6Drotationmatrix. Wetakethe a detailed examination of the implementation strategies
centerofthetokenwiththepeakpositionlikelihoodasour adopted, thedatasetsemployed, andthemetricsappliedin
estimated spos, and convert its corresponding 6D rotation our research. For exhaustive understanding, implementa-
vectorasourestimatedsrot. Theestimationcanbeequiv- tion, training details, and other additional information are
alentlyrepresentedasarotationmatrixRandatranslation availableinthesupplementarymaterial.
matrix T. More discussion about the architecture and de-
Datasets. We evaluate our method on SQA3D [43] and
signchoicesisinSection5.3.
ScanQA [5], two challenging indoor 3D VQA datasets.
Both datasets are derived from the ScanNet dataset [13],
4.3.Situation-guidedVisualEncoding
serving as the foundational source for their 3D scenes.
SQA3Dfeaturesover33Kquestion-answerpairsforthe3D
After obtaining the situation estimation, we investigate a
VQA task and 26K unique situational descriptions for the
betterapproachtoenhancingthegenerationofdownstream
situationestimationtask.Eachentryinthisdatasetincludes
responses, inspired by human cognitive processes. Intu-
a 3D scene point cloud, a situational description, a ques-
itively, humans typically comprehend their immediate 3D
tion, and pertinent annotations. ScanQA consists of over
environment by first interpreting their own situation in
41Kquestion-answerpairs,withoutsituationaldescriptions
space, and then discerning their surroundings from an ap-
and situational annotations. We use it to demonstrate the
propriateviewpoint. Ourmodelisdesignedtoemulatethis
generalizability of our method on general QA tasks. We
natural strategy. Using the situational vector ⃗s, we adjust
the coordinate system by repositioning the origin at spos, usethesplitsprovidedbythesedatasets.
andreorientingtheaxesaccordingtosrotsothatthenewy- Evaluation Metrics. For SQA3D, in order to compare
axis is aligned with the indicated direction. We keep the withbaselinemethods[43,48,73],weuseashallowtrans-
z-axis vertically oriented and project the situational vec- formerdecodertaskheadtoperformtheanswerclassifica-
tors onto the x-y plane. This is in line with the format of tiontask,andevaluatetheperformancewithexactmatches
the dataset [43], where situational vectors are assumed to (EM@1),whichisequivalenttoTop-1answeraccuracy.We
be parallel with the ground plane. Subsequently, we com- alsoprovideEM@1onabreakdownofquestiontypes,in-
pute a new situation-guided PE for each of the N visual cluding“What,”“Is,”“How,”“Can,”“Which,”and“Other,”
v
tokens, similar to the learnable 3D PE outlined in Sec- basedonthefirstwordinthequestionsentence. Addition-
tion 4.2. They allow the model to grasp the positional in- ally,weevaluatesituationestimationperformancewithlo-
terrelations from the perspective of the current situation. calizationaccuracyandorientationaccuracy. Inbothtasks,
These situational embeddings are added to the output em- we use accuracy within different distance or angle thresh-
beddingsofthesituationestimationmodule,whichconsists oldsasourmetrics. Forexample, “Acc@0.5m”meansac-
of blocks featuring self-attention layers for visual tokens, curacyoflocationestimationwhenpositivethresholdisset
succeeded by cross-attention layers that bridge visual and to0.5meter. ForScanQA,weperformauto-regressivean-
5QuestionBreakdown
Model Overall
What Is How Can Which Other
GPT-3[7] 39.7 46.0 40.5 45.6 36.1 38.4 41.0
ClipBERT[34] 30.2 60.1 38.7 63.3 42.5 42.7 43.3
MCAN[66] 28.9 59.7 44.1 68.3 40.7 40.5 43.4
ScanQA[5] 28.6 65.0 47.3 66.3 43.9 42.9 45.3
SQA3D[43] 33.5 66.1 42.4 69.5 43.0 46.4 47.2
Multi-CLIP[15] - - - - - - 48.0
LM4Vision[48] 34.3 67.1 48.2 68.3 48.9 45.6 48.1
3D-LLM[25] 36.5 65.6 47.2 68.8 48.0 46.3 48.1
3D-VisTA[73] 34.8 63.3 45.4 69.8 47.2 48.1 48.5
SIG3D(Ours) 35.6 67.2 48.5 71.4 49.1 45.8 52.6
Table1.OurproposedSIG3Dachievesstate-of-the-artperformanceontheSQA3Dbenchmark[43].Weperformthebeston“Is,”“How,”
and“Can”breakdowntypesofquestions,aswellastheaverageaccuracywithEM@1metric.Theresultsarereportedonthetestset.
Localization Orientation BERT [34], and MCAN [66], which are, as reported in
Model
prior work [43], baselines focused on language-only, 2D
Acc@0.5m Acc@1.0m Acc@15° Acc@30°
video,and2DimageQA,respectively. ForGPT-3,wefol-
Random 7.2 25.8 8.4 16.9
low SQA3D [43]to convert the visualinput into a caption
SQA3D[43] 9.5 29.6 8.7 16.5
SQA3D(separate) 10.3 31.4 17.1 22.8 using Scan2Cap [10] for LLMs to process. ScanQA [5]
3D-VisTA[73] 11.7 34.5 16.9 24.2 represents a 3D QA baseline that ignores the situational
SIG3D(Ours) 27.4 59.1 28.7 42.5 input. Both SQA3D [43] and Multi-CLIP [15] employ
situational descriptions and annotations for direct regres-
Table 2. Our proposed method SIG3D performs significantly
sion tasks. LM4Vision [48] utilizes LLMs as visual and
better than prior methods [43] in the situation estimation task.
textual encoders. Additionally, 3D-VisTA [73] undergoes
“Acc@0.5m” stands for localization accuracy with 0.5m thresh-
a pretraining procedure on their large-scale 3D scene-text
old. “Acc@15°”representsorientationaccuracywith15°thresh-
old. separatemeansdisablingothertaskstoletthemodelfocus dataset,ScanScribe,priortothefinetuningonthisdataset.
onsituationestimationonly.
SituationEstimation. AsshowninTable2,ourworkper-
formssignificantlybetterthanthestateoftheart[43,73]in
Model BLEU-1 BLEU-4 ROUGE METEOR CIDEr
bothlocalizationandorientationestimationtasks. For3D-
BLIP2[37] 29.7 5.9 26.6 11.3 45.7
VisTA[73], weuseapretrainedmodelandfinetuneanew
Flamingo[3] 25.6 8.4 31.1 11.3 55.0
VN+MCAN[66] 28.0 6.2 29.8 11.4 54.7 situationheadwiththeSQA3Ddatasetfollowing[43]. We
SR+MCAN[66] 26.9 7.9 30.0 11.5 55.4 alsoreportarandombaseline,inwhichwerandomlysam-
ScanQA[5] 30.2 10.1 33.3 13.1 64.9
ple position and orientation from a uniform distribution as
3D-LLM[25] 39.3 12.0 35.7 14.5 69.4
alower-boundperformance. NotethattheoriginalSQA3D
SIG3D 39.5 12.4 35.9 13.4 68.8
performs only marginally better than the random baseline,
Table 3. Performance of SIG3D on the ScanQA dataset [5] is meaningthatitdoesnotacquireanysituationalawareness,
on-parwiththestateoftheartwithlarge-scaletext-3Dpretrain- despite having the situation estimation loss. Disabling the
ing. VN and SR stand for VoteNet and ScanRefer, respectively. QA task and asking the model to exclusively focus on the
3D-LLM[73]leveragespretrained2DVLfoundationmodelsand situation estimation task results in a slight better perfor-
LLMmodels[3,7,36,37],andispretrainedonalarge-scaleheld- mance. Our method, with the anchor-based position like-
in3D-textdatasetbeforethefinetuningonScanQA. lihood estimation, results in much better understanding of
the 3D situational relationship. Our method also outper-
swer generation with large transformer decoder [25], and forms 3D-VisTA, which is pretrained on a large-scale 3D-
evaluatewithBLEU[49],ROUGE[38],METEOR[6],and text dataset, indicating that large pretraining alone is not
CIDEr[59]metrics. enoughtoaddressthesituationalawarenessproblem. Note
thatwedonotincludetherandombaselineperformancere-
5.1.SituatedQuestionAnswering portedin [43],becauseeachvalueisobtainedbygenerating
threerandomvaluesandtakingtheclosestonetotheground
Baselines. Our study involves a comparative analysis
true,andthusitdoesnotreflectatrue“random”baseline.
with a range of representative baselines on the SQA3D
dataset. Inparticular,weevaluateagainstGPT-3[7],Clip- SituatedQuestionAnswering. SIG3Doutperformsprior
6(a)NumberofVisualTokens (b)Voxelsize(inmeters) (c)Rotationrepresentation
Acc@1.0m Acc@30° EM@1 Acc@1.0m EM@1 Acc@30° EM@1
128 48.9 38.2 49.2 0.01 54.1 49.5 Quaternion 31.4 50.0
256 59.1 42.5 50.9 0.02 59.1 50.9 6Dvector 42.5 50.9
512 57.8 42.1 50.7 0.05 47.3 48.8 sinθ,cosθ 42.6 50.6
Table4. Ablationstudyvalidatesthatourvariousdesignchoicesimprovetheperformance. “Acc@1.0m,”“Acc@30°,”and“EM@1”are
accuracy(%)forlocalizationestimation,orientationestimation,andQAtasks,respectively.Oursettingsaremarkedin gray.
Acc@1.0m Acc@30° EM@1 Acc@1.0m Acc@30° EM@1
3DVisionEncoder Baseline(jointoptimization) 29.5 23.1 47.7
Text-only(novisioninput) - - 47.5 Howtoachievebettersituationestimation
VoteNet[52] 37.4 28.2 49.1 +3DPE 38.8 23.6 47.8
3DETR[47] 47.2 29.1 49.4 +6DRepresentation 38.5 27.4 47.7
OpenScene-OpenSeg[50] 57.5 41.6 50.2 +Anchor-basedEstimation 58.8 41.9 48.2
OpenScene-LSeg[50] 59.1 42.5 50.9 HowtoutilizesituationestimationforbetterQA
LanguageTokenizer/Encoder +3DSituationalPE 58.9 41.8 50.0
+VisualTokenRe-encoding 59.1 42.5 50.9
GloVe+LSTM[22,51] 44.3 30.9 48.7
OracleModel(GroundTruthSituationInformation)
SBERT-MiniLM[54] 56.1 38.6 49.4
Situationasdirectinput 100 100 47.7
SBERT-MPNet[54] 55.9 40.6 49.7
Situationasintermediateinput 100 100 53.9
SBERT-MPNet(finetune) 59.1 42.5 50.9
Table6.Ablationstudyverifiesthatourproposedmodulesleadto
Table 5. Performance of SIG3D improves with stronger visual
bettersituationestimationandbetterQAperformance.
and language encoders. We find that the open-vocabulary point
encoderandMPNet-basedsentenceBERT(SBERT)leadstothe
bestperformance. “Acc@1.0m”and“Acc@30°”standforlocal- 5.3.AblationStudyandAnalysis
ization and orientation accuracy in the situation estimation task,
respectively.“EM@1”demonstratestheexactmatchmetricinthe Vision and Language Encoders. We study the im-
QAtask. pact of different visual and textual tokenizers in Table 5.
It is observed that the open-vocabulary visual encoder
(OpenScene) outperforms detection-based encoders (such
methods in most question breakdown categories and over-
as VoteNet and 3DETR) across all metrics. This superior
all accuracy, as shown in Table 1. Our work achieves
performance of OpenScene is attributed to the limitations
leading results without large-scale pretraining (compared
of3Ddetectors,whicharetypicallytrainedonalimitedset
with 3D-VisTA) and LLMs (compared with GPT-3), indi-
ofobjectcategories,renderingthemlesseffectiveinrecog-
catingitssuperiorityinsituationalawareness. Notethatthe
nizingnovelobjectsmentionedintextualprompts. Regard-
LLMbaselineGPT-3achievesthebestperformanceonthe
inglanguageencoders,ourfindingsindicatethatastronger
“What”category,suggestingthepotentialofastrongerlan-
backbonecorrelateswithbetterperformance,primarilydue
guageencoderininterpretingcomplicatedquestions.
to its improved capability to interpret complex textual in-
puts. ThisleadstothesuggestionofintegratingLLMswith
5.2.GeneralQuestionAnsweringonScanQA
ourmethodtopotentiallyfurtherenhanceperformance, an
Baselines. We compare with 2D image VQA MCAN- avenueweintendtoexploreinfutureresearch.
based baselines [66], ScanQA [5], 3D-LLM [25] which
Situational Awareness. In Table 6 we verify the crucial
leverages large-scale pretrained 2D VLMs and LLMs as
roleofsituationalawarenessinthe3DVLtask. Firstly,we
backbone models, and 3D-VisTA [73] pretrained on their
showthat3DPE,6Drotationestimation,andanchor-based
proposedlarge-scale3D-textdataset.
positionestimationallleadtomuchbetterpositionandori-
Question Answering. As shown in Table 3, despite that entation estimation performance. We further establish that
the questions do not explicitly require situational under- situational PE and visual token re-encoding modules lead
standingtoanswerinScanQA,SIG3Dachievescomparable to better utilization of the predicted situational vector for
resultswithstate-of-the-artmethodswithoutthelarge-scale the QA task. Additionally, We design two oracle models
3D-textpretrainingandpowerful2DVLMandLLMback- under the assumption of having access to the ground truth
bone models. Our work pretrained on SQA3D [43] leads situationalvectorasinput. Theoutcomesfromthesemod-
tohigherperformanceonBLEU-1, BLEU-4, andROUGE els reveal a critical insight: the model fails to effectively
metrics,showingitsgeneralizabilityongeneral3DQAsce- interpretsituationalinformationwhenitisdirectlyincorpo-
narios. ratedintotheinputvisualembeddings. Thisunderlinesthe
7Situational Description: After an Situational Description: I am sitting Situational Description: I am sitting on Situational Description: I am leaning
exhausted day, I am lying on the bed in an armchair with a lamp behind me. a chair in between a shelf on my right on the door facing the blinds.
closest to the window with my head and a box on the other side.
on the pillows.
Question: Can I see the television Question: Which direction should I go Question: Is the picture on the shelf or Question: I need to make a call so
from my position on the bed? if I want to open the window? on the wall? where can I find the phone?
SQA3D
Answer: No Answer: Right Answer: Wall Answer: Bed
SIG3D
(Ours)
Answer: Yes Answer: Left Answer: Shelf Answer: Desk
Figure5.QualitativeresultsdemonstratesignificantimprovementofSIG3Doverpriormethods.ThefirstrowisresultsfromSQA3D[43],
andthesecondrowisresultsfromourmethod.Inthe3Dscene,red:Groundtruth(GT)vector,andblue:Estimatedvector.
necessity of the intermediate representation and encoding embodiednavigationandcommonsenseQAactivities.This
mechanism we have proposed, affirming its importance in furtherdemonstratesgreatpotentialofourmethodinthede-
achievingoptimal3DVLtaskperformance. velopmentofindoorroboticsand/orconversationalagents.
ArchitecturalDesign. Weexploredifferentarchitectural Supplementary Material. The supplementary section
design choices of our model in Table 4. We find that the offers an extensive analysis, encompassing a detailed ex-
number of visual tokens sampled from the visual feature amination of the 3D visual token activation changes pre-
embeddings affects the performance of both situation es- andpost-situationalre-encoding. Additionally,itincludesa
timation and QA tasks. Sampling fewer visual tokens in- comprehensivecollectionofpositiveandnegativesamples,
creasestheriskofmissingtheregionofsignificance,while an insightful failure case analysis, and a forward-looking
sampling more does not lead to a better performance as discussiononlimitationsandfuturework.
well. Westudythesizeofvoxelsandfind0.02mtobethe
most effective choice, as the OpenScene [50] backbone is
6.Conclusion
pretrained with the same voxel size. We also find that the
(sinθ,cosθ) and 6D vector representations perform a lot
Inthispaper,weintroduceSIG3D,asituation-awarevision
betterthanquaternionintherotationestimationtask. This
languagemodelfor3Dreasoningtasks. Weproposetorep-
isconsistentwiththefindingreportedin[71].
resent 3D scenes as feature tokens, treat tokens as anchor
pointstoestimateasituationalvectorfromatextualdescrip-
5.4.QualitativeAnalysis
tion, and use the estimated situation as guidance to align
Finally, we demonstrate some qualitative results of our andre-encodethevisualtokenstoenhancethefeaturesfor
SIG3D in Figure 5. We show the ground truth and esti- reasoningtasks. Weobserveconsistentandsignificantper-
mated situational vectors in red and blue, respectively, in formance gains on both situation estimation and question
their corresponding 3D scenes. We also print the answers answeringtasks.
witharedcrossorgreencheckmarkindicatingthecorrect- Acknowledgement. This work was supported in part by NSF Grant
ness.Itisclearthatourmethodperformssignificantlybetter 2106825, NIFA Award 2020-67021-32799, the Jump ARCHES endow-
insituationestimationtasks,resultinginvectorsveryclose ment, and the IBM-Illinois Discovery Accelerator Institute. This work
tothegroundtruthinbothpositionandorientationperspec- used NVIDIA GPUs at NCSA Delta through allocations CIS220014,
tives.Bettersituationalawarenessalsoaidsthecomplicated CIS230012,andCIS230013fromtheACCESSprogram.
8References Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay
Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier
[1] PanosAchlioptas, JudyFan, RobertHawkins, NoahGood-
Garcia,VedantMisra,KevinRobinson,LiamFedus,Denny
man,andLeonidasJGuibas. ShapeGlot:Learninglanguage
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
forshapedifferentiation. InICCV,2019. 3
Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David
[2] PanosAchlioptas,AhmedAbdelreheem,FeiXia,Mohamed
Dohan, Shivani Agrawal, Mark Omernick, Andrew M.
Elhoseiny,andLeonidasGuibas.ReferIt3D:Neurallisteners
Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat,
forfine-grained3Dobjectidentificationinreal-worldscenes.
AitorLewkowycz, EricaMoreira, RewonChild, Oleksandr
InECCV,2020. 3
Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,
[3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An- Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,
toine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur JasonWei,KathyMeier-Hellstern,DouglasEck,JeffDean,
Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Slav Petrov, and Noah Fiedel. PaLM: Scaling language
Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, modelingwithpathways. arXivpreprintarXiv:2204.02311,
Sina Samangooei, Marianne Monteiro, Jacob Menick, Se- 2022. 1
bastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sa-
[12] ChristopherChoy,JunYoungGwak,andSilvioSavarese.4D
hand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,
spatio-temporalconvNets: Minkowskiconvolutionalneural
Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.
networks. InCVPR,2019. 4,1
Flamingo: A visual language model for few-shot learning.
[13] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-
InNeurIPS,2022. 1,2,6
ber, Thomas Funkhouser, and Matthias Nießner. ScanNet:
[4] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret
Richly-annotated 3D reconstructions of indoor scenes. In
Mitchell,DhruvBatra,CLawrenceZitnick,andDeviParikh.
CVPR,2017. 1,4,5
VQA:Visualquestionanswering. InICCV,2015. 2
[14] SamyakDatta,SameerDharur,VincentCartillier,RutaDe-
[5] Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Mo-
sai,MukulKhanna,DhruvBatra,andDeviParikh. Episodic
toakiKawanabe.ScanQA:3Dquestionansweringforspatial
memoryquestionanswering. InCVPR,2022. 2
sceneunderstanding. InCVPR,2022. 2,3,5,6,7
[15] Alexandros Delitzas, Maria Parelli, Nikolas Hars, Geor-
[6] Satanjeev Banerjee and Alon Lavie. METEOR: An auto-
giosVlassis,SotiriosAnagnostidis,GregorBachmann,and
matic metric for MT evaluation with improved correlation
ThomasHofmann.Multi-CLIP:Contrastivevision-language
withhumanjudgments.InProceedingsoftheACLWorkshop
pre-training for question answering tasks in 3D scenes. In
onIntrinsicandExtrinsicEvaluationMeasuresforMachine
BMVC,2023. 3,6
Translationand/orSummarization,2005. 6
[16] Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang,
[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
Song Bai, and Xiaojuan Qi. PLA: Language-driven open-
biah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakan-
vocabulary3Dsceneunderstanding. InCVPR,2023. 3
tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-
[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
hini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl-
JeffreyWu,ClemensWinter,ChrisHesse,MarkChen,Eric
vainGelly,JakobUszkoreit,andNeilHoulsby. Animageis
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
worth16x16words: Transformersforimagerecognitionat
Clark,ChristopherBerner,SamMcCandlish,AlecRadford,
scale. InICLR,2021. 2
Ilya Sutskever, and Dario Amodei. Language models are
few-shotlearners. InNeurIPS,2020. 1,2,3,6 [18] Zi-YiDou,YichongXu,ZheGan,JianfengWang,Shuohang
Wang,LijuanWang,ChenguangZhu,PengchuanZhang,Lu
[8] DaveZhenyuChen,AngelXChang,andMatthiasNießner.
Yuan, Nanyun Peng, Zicheng Liu, and Michael Zeng. An
ScanRefer: 3D object localization in RGB-D scans using
naturallanguage. InECCV,2020. 3 empirical study of training end-to-end vision-and-language
transformers. InCVPR,2022. 2
[9] Kevin Chen, Christopher B Choy, Manolis Savva, An-
gel X Chang, Thomas Funkhouser, and Silvio Savarese. [19] Mingtao Feng, Zhen Li, Qi Li, Liang Zhang, XiangDong
Text2shape: Generating shapes from natural language by Zhang,GuangmingZhu,HuiZhang,YaonanWang,andAj-
learningjointembeddings. InACCV,2019. 3 mal Mian. Free-form description guided 3D visual graph
networkforobjectgroundinginpointcloud.InICCV,2021.
[10] ZhenyuChen,AliGholami,MatthiasNießner,andAngelX
3
Chang.Scan2Cap:Context-awaredensecaptioninginRGB-
Dscans. InCVPR,2021. 3,6 [20] Franc¸ois Garde`res, Maryam Ziaeefard, Baptiste Abeloos,
[11] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, andFreddyLecue. ConceptBERT:Concept-awarerepresen-
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul tationforvisualquestionanswering. InEMNLP,2020. 2
Barham, Hyung Won Chung, Charles Sutton, Sebas- [21] GolnazGhiasi,XiuyeGu,YinCui,andTsung-YiLin. Scal-
tian Gehrmann, Parker Schuh, Kensen Shi, Sasha ing open-vocabulary image segmentation with image-level
Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker labels. InECCV,2022. 2
Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, [22] Alex Graves and Alex Graves. Long short-term memory.
EmilyReif, NanDu, BenHutchinson, ReinerPope, James SupervisedSequenceLabellingwithRecurrentNeuralNet-
Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, works,pages37–45,2012. 7
9[23] Liangke Gui, Borui Wang, Qiuyuan Huang, Alex Haupt- fast adaptation of pretrained contrastive models for multi-
mann,YonatanBisk,andJianfengGao. KAT:Aknowledge channelvideo-languageretrieval. InCVPR,2023. 2,1
augmentedtransformerforvision-and-language. InNAACL, [40] YuanzeLin,YujiaXie,DongdongChen,YichongXu,Chen-
2022. 2 guang Zhu, and Lu Yuan. REVIVE: Regional visual rep-
[24] Yining Hong, Chunru Lin, Yilun Du, Zhenfang Chen, resentationmattersinknowledge-basedvisualquestionan-
JoshuaBTenenbaum,andChuangGan.3Dconceptlearning swering. InNeurIPS,2022. 2
andreasoningfrommulti-Viewimages. InCVPR,2023. 3 [41] Zhiqiu Lin, Samuel Yu, Zhiyi Kuang, Deepak Pathak, and
[25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Deva Ramanan. Multimodality helps unimodality: Cross-
YilunDu,ZhenfangChen,andChuangGan. 3D-LLM:In- modalfew-shotlearningwithmultimodalmodels. InCVPR,
jectingthe3Dworldintolargelanguagemodels.InNeurIPS, 2023. 1
2023. 1,2,3,5,6,7
[42] IlyaLoshchilovandFrankHutter. Decoupledweightdecay
[26] Pin-Hao Huang, Han-Hung Lee, Hwann-Tzong Chen, and regularization. arXivpreprintarXiv:1711.05101,2017. 1
Tyng-LuhLiu. Text-guidedgraphneuralnetworksforrefer-
[43] Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao
ring3Dinstancesegmentation. InAAAI,2021. 3
Liang, Song-ChunZhu, andSiyuanHuang. SQA3D:Situ-
[27] Baoxiong Jia, Yixin Chen, Siyuan Huang, Yixin Zhu, and atedquestionansweringin3Dscenes. InICLR,2023. 1,2,
Song-ChunZhu.LEMMA:Amulti-viewdatasetforlearning 3,4,5,6,7,8
multi-agentmulti-taskactivities. InECCV,2020. 2
[44] Yunze Man, Liang-Yan Gui, and Yu-Xiong Wang. BEV-
[28] BaoxiongJia,TingLei,Song-ChunZhu,andSiyuanHuang.
guided multi-modality fusion for driving perception. In
Egotaskqa:Understandinghumantasksinegocentricvideos.
CVPR,2023. 4
InNeurIPS,2022. 2
[45] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and
[29] ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh,
RoozbehMottaghi. OK-VQA:Avisualquestionanswering
HieuPham, QuocLe, Yun-HsuanSung, ZhenLi, andTom
benchmarkrequiringexternalknowledge. InCVPR,2019. 2
Duerig.Scalingupvisualandvision-languagerepresentation
[46] JackMerullo,LouisCastricato,CarstenEickhoff,andEllie
learningwithnoisytextsupervision. InICML,2021. 2
Pavlick. Linearlymappingfromimagetotextspace. ICLR,
[30] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina
2023. 2
Toutanova. BERT:Pre-trainingofdeepbidirectionaltrans-
[47] Ishan Misra, Rohit Girdhar, and Armand Joulin. An end-
formers for language understanding. In NAACL, 2019. 1,
to-endtransformermodelfor3Dobjectdetection. InICCV,
2
2021. 7
[31] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo
[48] Ziqi Pang, Ziyang Xie, Yunze Man, and Yu-Xiong Wang.
Kanazawa,andMatthewTancik. LERF:Languageembed-
Frozentransformersinlanguagemodelsareeffectivevisual
dedradiancefields. InICCV,2023. 3
encoderlayers. InICLR,2024. 2,5,6
[32] WonjaeKim,BokyungSon,andIldooKim. ViLT:Vision-
[49] KishorePapineni,SalimRoukos,ToddWard,andWei-Jing
and-languagetransformerwithoutconvolutionorregionsu-
pervision. InICML,2021. 2 Zhu. BLEU:Amethodforautomaticevaluationofmachine
translation. InACL,2002. 6
[33] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,
ChloeRolland,LauraGustafson,TeteXiao,SpencerWhite- [50] Songyou Peng, Kyle Genova, Chiyu ”Max” Jiang, An-
head, Alexander C. Berg, Wan-Yen Lo, Piotr Dolla´r, and dreaTagliasacchi,MarcPollefeys,andThomasFunkhouser.
RossB.Girshick. Segmentanything. InICCV,2023. 2 OpenScene: 3D scene understanding with open vocabular-
ies. InCVPR,2023. 3,4,7,8,1
[34] JieLei, LinjieLi, LuoweiZhou, ZheGan, TamaraLBerg,
MohitBansal,andJingjingLiu. Lessismore: Clipbertfor [51] JeffreyPennington,RichardSocher,andChristopherDMan-
video-and-languagelearningviasparsesampling. InCVPR, ning. Glove: Global vectors for word representation. In
2021. 6 EMNLP,2014. 7
[35] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen [52] Charles R Qi, Or Litany, Kaiming He, and Leonidas J
Koltun, and Rene Ranftl. Language-driven semantic seg- Guibas. Deephoughvotingfor3Dobjectdetectioninpoint
mentation. InICLR,2022. 2 clouds. InICCV,2019. 3,7
[36] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. [53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
BLIP: Bootstrapping language-image pre-training for uni- Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
fiedvision-languageunderstandingandgeneration.InICML, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
2022. 1,2,6 Krueger, and Ilya Sutskever. Learning transferable visual
[37] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. modelsfromnaturallanguagesupervision. InICML,2021.
BLIP-2: Bootstrapping language-image pre-training with 1,2,4
frozenimageencodersandlargelanguagemodels.InICML, [54] Nils Reimers and Iryna Gurevych. Sentence-BERT: Sen-
2023. 2,6,1 tence embeddings using siamese BERT-networks. In
[38] Chin-YewLin.ROUGE:Apackageforautomaticevaluation EMNLP,2019. 4,7,1
ofsummaries. InACL,2004. 6 [55] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
[39] Xudong Lin, Simran Tiwari, Shiyuan Huang, Manling Li, PatrickEsser,andBjo¨rnOmmer.High-resolutionimagesyn-
Mike Zheng Shou, Heng Ji, and Shih-Fu Chang. Towards thesiswithlatentdiffusionmodels. InCVPR,2022. 2
10[56] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, [71] YiZhou,ConnellyBarnes,JingwanLu,JimeiYang,andHao
Kenneth Marino, and Roozbeh Mottaghi. A-OKVQA: A Li. On the continuity of rotation representations in neural
benchmarkforvisualquestionansweringusingworldknowl- networks. InCVPR,2019. 5,8
edge. InECCV,2022. 2 [72] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-
[57] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Fei. Visual7W:Groundedquestionansweringinimages. In
Martinet,Marie-AnneLachaux,Timothe´eLacroix,Baptiste CVPR,2016. 2
Rozie`re,NamanGoyal,EricHambro,FaisalAzhar,Aurelien [73] ZiyuZhu,XiaojianMa,YixinChen,ZhidongDeng,Siyuan
Rodriguez, ArmandJoulin, EdouardGrave, andGuillaume Huang,andQingLi. 3D-VisTA:Pre-trainedtransformerfor
Lample. LLaMA: Open and efficient foundation language 3Dvisionandtextalignment. InICCV,2023. 3,5,6,7
models. arXivpreprintarXiv:2302.13971,2023. 1,2
[58] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko-
reit,LlionJones,AidanNGomez,ŁukaszKaiser,andIllia
Polosukhin. Attentionisallyouneed. NeurIPS,2017. 2,1
[59] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
Parikh. CIDER: Consensus-based image description eval-
uation. InCVPR,2015. 6
[60] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu,
XizhouZhu,GangZeng,PingLuo,TongLu,JieZhou,Yu
Qiao, and Jifeng Dai. VisionLLM: Large language model
is also an open-ended decoder for vision-centric tasks. In
NeurIPS,2024. 1
[61] BoWu,ShoubinYu,ZhenfangChen,JoshuaBTenenbaum,
andChuangGan.STAR:Abenchmarkforsituatedreasoning
inreal-worldvideos. InNeurIPS(DatasetsandBenchmarks
Track),2021. 2
[62] JialinWu,JiasenLu,AshishSabharwal,andRoozbehMot-
taghi. Multi-modalanswervalidationforknowledge-based
vqa. InAAAI,2022. 2
[63] JianingYang,XuweiyiChen,ShengyiQian,NikhilMadaan,
MadhavanIyengar,DavidFFouhey,andJoyceChai. LLM-
Grounder:Open-vocabulary3Dvisualgroundingwithlarge
languagemodelasanagent. InICRA,2024. 3
[64] ShuquanYe,DongdongChen,SongfangHan,andJingLiao.
3Dquestionanswering. IEEETransactionsonVisualization
andComputerGraphics,30(3):1772–1786,2024. 3
[65] TianweiYin,XingyiZhou,andPhilippKrahenbuhl. Center-
based3Dobjectdetectionandtracking. InCVPR,2021. 4,
5
[66] Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian.
Deepmodularco-attentionnetworksforvisualquestionan-
swering. InCVPR,2019. 6,7,1
[67] RowanZellers, YonatanBisk, AliFarhadi, andYejinChoi.
Fromrecognitiontocognition:Visualcommonsensereason-
ing. InCVPR,2019. 2
[68] SusanZhang,StephenRoller,NamanGoyal,MikelArtetxe,
MoyaChen,ShuohuiChen,ChristopherDewan,MonaDiab,
Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam
Shleifer,KurtShuster,DanielSimig,PunitSinghKoura,An-
jaliSridhar,TianluWang,andLukeZettlemoyer.OPT:Open
pre-trained transformer language models. arXiv preprint
arXiv:2205.01068,2022. 2
[69] YaqiZhang,DiHuang,BinLiu,ShixiangTang,YanLu,Lu
Chen,LeiBai,QiChu,NenghaiYu,andWanliOuyang.Mo-
tionGPT:FinetunedLLMsaregeneral-purposemotiongen-
erators. InAAAI,2024. 2
[70] BradyZhouandPhilippKra¨henbu¨hl.Cross-viewtransform-
ersforreal-timemap-viewsemanticsegmentation.InCVPR,
2022. 4
11Situational Awareness Matters in 3D Vision Language Reasoning
Supplementary Material
A.ImplementationDetails Situation: I am facing… behind me. Situation: I am … picture… the other side.
Question: Is the table round, square, or rectangular? Question: Is the picture on my left on the shelf or wall?
Hereweprovidemoredetailsaboutourmodel.
SQA3D: rectangular. SQA3D: wall.
Visual and Textual Encoders. We use OpenScene [50]
(the3Ddistilledvariant)asourvisualencoder,whichincor-
SIG3D: rectangular. SIG3D: shelf.
porates distilled CLIP features into a 3D Minkowski CNN
backboneoriginallydesignedforthe3Dsemanticsegmen- [Easy]: Only one table in the scene. [Hard]: Multiple pictures, shelves in the room.
tation task. We use the default 0.02m voxel size to dis- FigureA.Visualizationofcaseswheresituationpredictionbene-
cretize a point cloud into 3D voxels and disable the scal- fitsthemost.Arrowcolors:GT,SIG3D,andSQA3Dpredictions.
ingandelasticdistortionaugmentationmethodsduringthe
voxelizationprocess. The3Darchitectureisthepredefined thisdepiction.
MinkUNet18A[12].ThenumberofvisualtokensN is256. Forexample,thevisualizationinthesecondrowreveals
v
Additionally,weusetheSentence-BERT[54]MPNetvari- anotableshiftinfocus. Initially,thetokenspredominantly
antasourtexttokenizerandencoder.Weusethefixedbatch concentrate on the bed area. However, after re-encoding,
paddingstrategyandsetthelengthtobe100forbothsitu- thereisadiscernibleshiftinattentiontowardsareasclosely
ationaltokensN andquestiontokensN . Thefeatureem- alignedwiththesituationalvectorandthosedirectlyrelated
s q
bedding sizes for all three types of tokens are set to 768. to the query. Similarly, in the third row, the situational re-
The256dimensionaloutputoftheOpenScenebackboneis encodingprocessresultsinthewindowregion“ontheleft”
projectedtothe768hiddensizewitha1x1Convlayer. We receivingincreasedemphasis. Inthefourthrow, theatten-
freezetheOpenScenebackbone, andfinetuneonlythelast tioninitiallyfocusesonthevanityregion. Thenitshiftsto
layerofthetextualbackboneduringourtrainingprocess. thetoiletontheleftoftheagent,assuggestedbythesitua-
tionalvectorandthequestionprompt.Thisexperimentpro-
Fusion and Decoder Models. We use a 4-layer MCAN
vides a clear demonstration of how our method, using en-
Transformer [58, 66] as the fusion block of the visual and
hancedsituationalawareness, contributestoimprovedper-
situationaltokens.Thelearnablepositionalembeddingsand
formanceindownstreamreasoningtasksinanexplainable
situational embeddings are composed of a 2-layer MLP:
manner. Theabilityofourmodeltodynamicallyadjustfo-
first from dimension 3 to 128, and then from 128 to the
cusinresponsetosituationalcuesisakeyfactorinitsen-
target dimension. We use BLIP-2 [37] as the large multi-
hancedreasoningcapabilities.
modal tranformer for final response generation, simiar to
3D-LLM[25].
C.MoreQualitativeResults
Training Details. We train our model with the
AdamW [42] optimizer with β = 0.9, β = 0.999, and WeshowmorequalitativeresultsofourmodelinFiguresC
1 2
ϵ=1e-8. Weuseabatchsizeof16andsettheinitiallearn- andD.Visualizationencompassesadiversearrayoftasks,
ing rate to be 2e-5. The weight decay is set to 0.05, and including queries about object orientation, characteristics
wedisabletheweightdecayonthelayernormlayersandall of specific objects, the count of objects within a scene,
bias parameters following [39]. We decrease the learning andyes/noquestionsbasedoncommonsensereasoning. A
rate by 10 times after the 10-th and the 20-th epochs. We key observation from these results is that, in numerous in-
trainthemodelforatotalof50epochsonasingleNVIDIA stances, absolute precision in situation estimation is not a
A100GPU. prerequisiteforourmodeltoaccuratelydeducetheanswers
totheposedquestions. Thisfindinghighlightsthemodel’s
B.EnhancedVisionTokenActivationThrough robustnessanditscapacitytoeffectivelyhandleavarietyof
SituationalRe-encoding querytypes,evenwithlessoptimalsituationalawareness.
In Figure B, we provide an insightful visualization of the
D.PerformanceonHardCases
activation changes in 3D visual tokens z3D, before and
after undergoing our situation-guided visual re-encoding An example of our case study of easy-hard samples is
process. This visualization employs the viridis colormap, shown in Figure A. We find that simple examples in the
whereabrightertokenrepresentationindicatesahigherac- dataset allow existing models to guess the correct answer
tivationvalue. Theeffectivenessofsituationalguidancein without any 3D situational understanding. However, our
amplifying the relevance of crucial tokens is evident from method effectively improves the hard examples with com-
13D Scene for Reference Before Situational Re-encoding After Situational Re-encoding
Situational Description: I am sitting on a couch with a
pillow facing another couch and the pillow is on my right. Question: How many couches are behind me? Answer: One
Situational Description: I am walking into the room with
the bathroom vanity on my right. Question: What is hung on the nook to my right side? Answer: Ironing board
Situational Description: I am looking at my backpack
and desk is behind me. Question: How many windows to my left? Answer: One
Situational Description: I am facing a bathroom vanity,
while having a ladder on my right within reach. Question: Is the toilet seat up or down to my left? Answer: Up
FigureB.3Dvisiontokenactivationbeforeandaftersituationalre-encoding.Wecannoticethathigherweightsareassignedtoquestion
andsituation-relatedtokensafterourproposedsituationalre-encodingmechanism.
plicatedandentangledquestionsandsituations. examplenecessitatesthemodel’scomprehensionofthespa-
tial relationship between the viewer’s perspective and the
couch, followed by an additional reasoning phase focused
E.AnalysisofFailureCases
on the couch to accurately respond to the query. The sub-
WeperformafailurecaseanalysisonourmodelinFigureE. sequentexampledemandsanunderstandingoftheconcepts
Wecategorizeandvisualizethreetypesoffailurecases. of “odd” and “even,” and their application to the count of
objectsina3Denvironment.
Accurate Situation Estimation, Incorrect Question An-
swering. This scenario demonstrates that accurate situa- Inaccurate Situation Estimation, Correct Question An-
tionalunderstandingdoesnotnecessarilyguaranteecorrect swering. This category reveals that errors in situation
responses to queries. A significant proportion of failures estimation are more likely when the scene description in-
within this category can be attributed to complex question volves minor or less common objects. Furthermore, it is
prompts that demand multi-stage reasoning or the integra- observedthatthemodelmightincidentallyarriveatthecor-
tion of commonsense knowledge. For instance, the initial rect answer without fully grasping the complex situational
2Situational Description: I am sitting Situational Description: I justwalked Situational Description: I am standing Situational Description: I am picking
on the left cushion of the couch and intotheroomthroughthedoors. in front of the door and facing the file up my jacket on the chair while facing
to my right is a pillow. cabinet. the blackboard and there is a desk with
no monitor in my six o’clock direction.
Question: Which direction should I go Question: How many armchairs are Question: Is the door behind me open Question: What color is the desk
if I want to open the curtain? directly in front of me? or closed? behind me?
Answer: Left Answer: Zero Answer: Closed Answer: Brown
Situational Description: I am fixing Situational Description: I am taking out Situational Description: I am facing a Situational Description: I am looking
the cabinet with a few bags by my the white ball from the side pocket while chair and there is a printer on my right. at mirror and combing my hair with a
right foot. my beer on the table is being drunk by hairbrush that was in the bathroom
me. I am also facing a white board. vanity.
Question: If I want to take a break and Question: Can I see chair if I turn Question: If I turned directly around and Question: Is the door to my right open
take a nap, is there a bed I could sleep around? walked straight, what would I hit first? or closed?
on?
Answer: Yes Answer: Right Answer: Bed Answer: Wall
Situational Description: I am Situational Description: I amopening Situational Description: I amsitting Situational Description: I am turning
standing in the middle of the kitchen the door. on the toilet facing a bathtub. on the lamp by the chair with the chair
and the stove is on my left. on my right within reach.
Question: What is to my left that can Question: Can I see a window if I turn Question: Does the bathtub in front of me Question: If I look to my right, can I see
be used to cook food? my head rightwards? have a shower curtain? my reflection?
Answer: Stove Answer: Yes Answer: No Answer: Wall
Situational Description: I am Situational Description: I am sitting on Situational Description: I am facing Situational Description: I am opening
leaningonthedoorfacingtheblinds. the toilet facing a bathtub. the shelf closest to the armchair. the door.
Question: Where should I walk if I Question: Can I reach the sink from Question: What color is the ledge in front Question: Are there pillows on the
want to find a book to read during my where I am sitting? of me? couches?
break?
Answer: Forward Answer: Right Answer: White Answer: Wall
FigureC.Wedemonstratemoresuccessfulexamplesofourmethod.
3Situational Description: I am facing a Situational Description: I am picking Situational Description: I am looking Situational Description: I am facing the
table, while there is a chair on my up my backpack with a chair to my for a cold drink to have and there is a toilet, and the sink is behind me.
right and two chairs behind me. left within reach. cabinet on my right side.
Question: Is the room clean or messy? Question: How many chairs are behind Question: What is on top of the cabinet Question: Is the amount of bar I am
me? that is on my 1 o’clock? facing odd or even?
Answer: Messy Answer: Two Answer: TV Answer: Odd
Situational Description: I am Situational Description: I am facing the Situational Description: I am throwing Situational Description: I am standing
standing by the paper cutter on my whiteboard, and there are some chairs trash with a chair very close to me right in front of the sink, and I can see
right within reach. and a table behind me. behind me. table across the room.
Question: Is the number of chairs on Question: What is to my left that you Question: Is the shape of table behind me Question: Can I see the TV without
my left odd or even? can use to see the outside? round, square, or rectangular? moving much?
Answer: Even Answer: Windows Answer: Rectangular Answer: No
Situational Description: I am facing Situational Description: I am opening Situational Description: I am Situational Description: I just entered
the wall with large windows, and one door while there is another one standing in front of a counter, and the room, while having a desk with a
there is a trashcan behind me. adjacent on my right. there is a recycling bin to my left shelf on top of it at my right and a door
within reach. behind me.
Question: How many chairs are Question: Is the door in front of me Question: Which direction should I go Question: Where do I go to get to the
accounted for in the room total? closed or open? if I want to open the mailbox? other bed?
Answer: Eight Answer: Closed Answer: Right Answer: Right
Situational Description: I am placing Situational Description: I am facing the Situational Description: I amopening Situational Description: I am looking
garbage into a trash can, while toilet, and the sink is behind me. the door. at mirror and combing my hair with a
having a toilet to my left within reach. hairbrush that was in the bathroom
vanity.
Question: What is the color of the Question: Which direction should I go Question: Does the toilet to my right have Question: What color is the sink in front
towel behind me? if I want to exit the room? an open or closed lid? of me?
Answer: White Answer: Right Answer: Open Answer: White
FigureD.Wedemonstratemoresuccessfulexamplesofourmethod.
4(a) Situational estimation is correct, (b) Situational estimation is wrong, (c) Situational estimation is wrong,
and QA is wrong. but QA is correct. and QA is wrong.
Situational Description: After an Situational Description: I am standing Situational Description: I am standing
exhausted day, I am lying on the bed in front of the door and facing the file by backpack on my left side and the bed
closest to the window with my head cabinet. is behind me while the curtain is in my
on the pillows. six o’clock direction.
Question: What is to the right of the Question: Is the door behind me open Question: Are there more doors to my
couch in front of me? or closed? left than there are to my right?
GT Answer: Desk GT Answer: Closed GT Answer: No
Pred Answer: End table Pred Answer: Closed Pred Answer: Yes
Situational Description: Standing by Situational Description: I am facing a Situational Description: I am sitting on
the table with a tray rack to my vending machine that is next to a chair facing the table with the
immediate right. another one, while there is a trash can blackboard behind me and a chair on
behind me directly. my left within reach.
Question: Is the amount of chair on
my left side odd of even? Question: Can I see clothes where I Question: Is the amount of table I am
am standing? facing odd or even?
GT Answer: Odd GT Answer: Yes GT Answer: Odd
Pred Answer: Even Pred Answer: Yes Pred Answer: Even
FigureE.Wedemonstratethreedifferentcategoriesoffailurecases.
and multi-modal context, particularly in cases where the terpretation.
questioninvolveschoosingbetweentwooramongmultiple
givenoptions. Therefore,ablendofqualitativeandquanti-
F.LimitationsandFutureWork
tativeassessmentsiscrucialforacomprehensiveevaluation
ofthemodel’sperformance.
Selection of 3D Scenes. The SQA3D [43] and
ScanQA [5] datasets, both derived from the ScanNet [13]
BothSituationEstimationandQuestionAnsweringare dataset,exclusivelyfeatureindoorhouseholdenvironments.
Incorrect. This group contains the most challenging ex- These static scenes limit the model’s applicability to dy-
amples from the dataset, typically encompassing multiple namic tasks like manipulation and exploration. Conse-
complexities identified in the preceding categories. These quently, our current model is tailored to static household
cases present a compounded difficulty level, highlighting settings. This scalability problem is a long-standing chal-
themodel’slimitationsinscenariosthatrequireanintricate lengeforallexisting3DVLreasoningwork[5,25,43,73].
understanding of both situational context and question in- We believe that with a more scalable visual representation
5(e.g., scene graphs, sparse learnable embeddings), we can
extendourmodeltosupportlarger3Denvironmentsinthe
futurework.
More Comprehensive Visual Encoding. In our ap-
proach, the utilization of a voxel-based, open-vocabulary
3D encoder achieves much better overall performance.
Nevertheless,forspecificqueriesinvolvingcountingorref-
erencing, a detection-based encoder may yield a more ad-
vantageousvisualtokenset,owingtoitscapacitytoprovide
instance-level information pertinent to the questions. This
indicatesthepotentialbenefitsofamultifacetedvisualtok-
enizationsystemthatamalgamatesthestrengthsofvarious
encodertypes.
6