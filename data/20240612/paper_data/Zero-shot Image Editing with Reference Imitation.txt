Zero-shot Image Editing with Reference Imitation
XiChen1 YutongFeng2 MengtingChen2 YiyangWang1 ShilongZhang1
YuLiu2 YujunShen3 HengshuangZhao1
1TheUniversityofHongKong 2AlibabaGroup 3AntGroup
https://xavierchen34.github.io/MimicBrush-Page
Source Source Result Source
reference reference reference
Result Result
Source Result Source Result
reference
reference
Figure1: DiverseeditingresultsproducedbyMimicBrush,whereusersonlyneedtospecifythe
to-editregionsinthesourceimage(i.e.,whitemasks)andprovideanin-the-wildreferenceimage
illustratinghowtheregionsareexpectedafterediting. Ourmodelautomaticallycapturesthesemantic
correspondencebetweenthem,andaccomplishestheeditingwithafeedforwardnetworkexecution.
Abstract
Imageeditingservesasapracticalyetchallengingtaskconsideringthediverse
demandsfromusers,whereoneofthehardestpartsistopreciselydescribehow
theeditedimageshouldlooklike. Inthiswork,wepresentanewformofediting,
termedimitativeediting,tohelpusersexercisetheircreativitymoreconveniently.
Concretely, to edit an image region of interest, users are free to directly draw
inspirationfromsomein-the-wildreferences(e.g., somerelativepicturescome
acrossonline),withouthavingtocopewiththefitbetweenthereferenceandthe
source. Such a design requires the system to automatically figure out what to
expectfromthereferencetoperformtheediting. Forthispurpose,weproposea
generativetrainingframework,dubbedMimicBrush,whichrandomlyselectstwo
framesfromavideoclip,maskssomeregionsofoneframe,andlearnstorecoverthe
maskedregionsusingtheinformationfromtheotherframe. Thatway,ourmodel,
developedfromadiffusionprior,isabletocapturethesemanticcorrespondence
betweenseparateimagesinaself-supervisedmanner. Weexperimentallyshowthe
effectivenessofourmethodundervarioustestcasesaswellasitssuperiorityover
existingalternatives. Wealsoconstructabenchmarktofacilitatefurtherresearch.
Preprint.Underreview.
4202
nuJ
11
]VC.sc[
1v74570.6042:viXraRef.Image
TextPrompt Ref.Mask/Box Ref.Image
SourceImage SourceImage SourceImage
Model Model Model
SourceMask SourceMask SourceMask
(a)Inpainting (b)Composition (c)Ours
Figure2: Conceptualcomparisonsfordifferentpipelines. Toeditalocalregion,besidestaking
thesourceimageandsourcemask(indicatestheto-editregion),inpaintingmodelsusetextprompts
toguidethegeneration. Imagecompositionmethodstakeareferenceimagealongwithamask/box
tocropoutthespecificreferenceregion. Differently,ourpipelinesimplytakesareferenceimage,the
referenceregionsareautomaticallydiscoveredbythemodelitself.
1 Introduction
Imageeditingenablesvariousapplicationsforcreatingnovelcontent, e.g., addingnewobject(s),
modifyingattributes,ortranslatingimagestyles. Recently,poweredbythelarge-scalepre-trained
text-to-imagediffusionmodels[33,30,36],therangeofcapacityforeditingmodels[18,3,8,2,43,
15,26,12]alsoexpandssignificantly.
Theadvancededitingmethodscouldsatisfyalargevarietyofuserrequirementsformodifyingeither
afullimage[2,18]oritslocalregions[43,26,15,8,18,3].
However, itisstillchallengingforexistingeditingmodelstofittherequirementsofcomplicated
practical scenarios. For instance, as shown in Fig. 1, it is required to modify the sole of a shoe
byreferringtoanotherone,ortopasteaspecifiedpatterntoagivenmug. Suchkindofeditingis
importantforrealapplicationslikeproductdesign,charactercreation,andspecialeffects,etc.
For this kind of local editing, existing works take the source image with a binary mask as input.
AsshowninFig.2(a),inpainting[43,52]methodsre-generatethemaskedregionfollowingtext
instructions. However,itisnotfeasibletodescribethedesiredoutcomesonlywithtexts. Forexample,
inFig.1,thedesignofshoesorthecolorsofhairishardtodescribeaccuratelyintext. Composition
methods [8, 38, 37, 53] take a reference image as input, along with a mask/box representing the
referencearea,asshowninFig.2(b). Theycouldinsertan“individualobject”fromthereference
imageintothesourceimagebutstruggletodealwithlocalcomponents(likeshoesolesandhuman
hair)orlocalpatterns(likelogosandtexture).Thesemethodsrequiretocarefullyextractthereference
areafromtheimage. Nevertheless,localcomponentsareinherentlyintertwinedwiththecontext
andarehardtounderstandwhenisolatedfromthewholeobject. Besides,their[8,38,53]training
processrequiresthemaskpairstoindicatethesameobjectindifferentstates(e.g.,twovideoframes).
Object-levelmaskpairsarefeasibletoobtain,butitisdifficulttogetthepart-levelpairsatscale.
To deal with the aforementioned requirements, we propose a novel pipeline of editing, termed
imitativeediting. AsillustratedinFig.2(c),givenasourceimagewithamaskedareaforediting,
it requires only the reference image without masks. Then, imitative editing targets to fill in the
maskedareabyautomaticallyfindingandimitatingthecorrespondingpartinthereferenceimage.
Suchapipelineformulatesmoreconvenientinteractions,withoutstrictlyseparatingthereference
componentsfromthewholeimage.Besides,itreachesaharmoniousblendingreferringtotherelation
betweenthereferenceregionanditssurroundings(e.g.,thesoleandvapeoftheshoe).
To achieve imitative editing, we design a framework called MimicBrush, with dual diffusion U-
Netstotacklethesourceandreferenceimages. Morespecifically,wetrainitinaself-supervised
manner,wherewetaketwoframesfromavideotosimulatethesourceandreferenceimages. As
thevideoframescontainbothsemanticcorrespondenceandvisualvariations,MimicBrushlearns
todiscoverthereferenceregionautomaticallyandrepaintitintothesourceimagewithanatural
combinationtoitssurroundings. InMimicBrush,wesendthemaskedsourceimageintoanimitative
U-NetandthereferenceimageintoareferenceU-Net, respectively. Thentheattentionkeysand
values of the reference U-Net are injected into the imitative U-Net, which assists in completing
the masked regions. As demonstrated in Fig. 1, MimicBrush overcomes the variations between
thesourceandreferenceimagesindifferentposes,lightings,andevencategories. Thegenerated
regionhighlypreservesthedetailsofthevisualconceptsinthereferenceimage,andharmoniously
interactswiththebackgrounds. Foramorecomprehensiveevaluationoftheproposedmethod,we
alsoconstructahigh-qualitybenchmarkofimitativeediting. Thebenchmarkincludestwomaintasks,
i.e.,partcompositionandtexturetransfer. Eachtaskcoversseveralsub-tracksinspiredbypractical
applications,e.g.,fashionandproductdesign.
2ReferenceImage
ReferenceU-Net
ConcatK,V
SourceImage
ImitativeU-Net
DepthEstimation Projector
Figure3: ThetrainingprocessofMimicBrush. First,werandomlysampletwoframesfromavideo
sequenceasthereferenceandsourceimage. Thesourceimagearethenmaskedandexertedwith
dataaugmentation. Afterward,wefeedthenoisyimagelatent,mask,backgroundlatent,anddepth
latentofthesourceimageintotheimitativeU-Net. Thereferenceimageisalsoaugmentedandsent
tothereferenceU-Net. ThedualU-Netsaretrainedtorecoverthemaskedareaofsourceimage. The
attentionkeysandvaluesofreferenceU-NetareconcatenatedwiththeimitativeU-Nettoassistthe
synthesisofthemaskedregions.
2 RelatedWork
Image inpainting. Traditional image inpainting methods [21, 22, 51, 50] only leverage the
backgroundinformationtocompletethemaskedregions. Poweredbythetext-to-imagediffusion
models,recentworks[43,30,2,15,26]leveragetextpromptstoguidethegenerationofcontent
oftheeditingregions. Theseworksfullyleveragetheflexibilityofpromptstogeneratediversified
content. A potential limitation is that only using text prompts could not fully express the user’s
intentionforsomespecificdemands.
Imagecustomization. Togeneratetheimageforthegivensubjectwithhighfidelity,customization
methods optimize a new “word” or use LoRAs to learn specific concepts. However, most of the
customization methods [34, 23, 24, 11, 1, 13] tackles the full object. Besides, they require 3-5
exemplar images and rely on subject-specific fine-tuning which last half an hour. Among them,
RealFill[41]andCLiC[35]couldcustomizethelocalregionincontext. However,RealFillrequires
3-5imagesforthesamescene,andthefinetunedmodelcouldonlycompletelocalregionsofthe
trained scene. CLiC [35] could customize the local patterns to edit different objects, but it only
demonstratesinner-categorygeneralizationsandstillrequiressubject-specificfine-tuning.
Image composition. This topic explores inserting a given object into a specific location of
the background with harmonious blending. The early works involved a long pipeline of image
segmentation,pasting,andharmonization[39,9,4,14,10,5]. Diffusion-basedmethodsproposeend-
to-endsolutionsandsupporttheposevariationsofthereferenceobject. Paint-by-example[47]and
ObjectStitch[37]useaCLIPimageencodertoextracttherepresentationoftheobject. AnyDoor[8]
usesDINOv2[27]encodersandcollectstrainingsamplesfromvideos. Laterworks[53,54,28,44]
addcameraparametersortextpromptstoincreasethecontrollability. However,theymainlyfocuson
insertingthefull-object. Part-levelcompositionposeshigherdemandsformodelingtheinteraction
betweentheeditingregionandthesurroundingcontext.
3 Method
3.1 OverallPipeline
TheoverallframeworkofMimicBrushisdemonstratedinFig.3. Torealizeimitativeediting,we
designanarchitecturewithdualdiffusionmodelsandtrainitinaself-supervisedmanner.
Videodatacontainsnaturallyconsistentcontent,andalsoshowsvisualvariationssuchasdifferent
postures of the same dog. Thus, we randomly pick two frames from a video clip as the training
3
semarFoediV
…
tnemguA
gniksaM tnemguA
)W,H,4,B(tnetaLegamI
)W,H,1,B(ksaM
)W,H,4,B(tnetalGB
)W,H,4,B(tnetaLhtpeDsamples of MimicBrush. One frame serves as the source image, where we mask out some of
its regions. Another frame serves as the reference image, assisting the model to recover of the
maskedsourceimage. Throughoutthisway,MimicBrushlearnstolocatethecorrespondingvisual
information(e.g.,thedog’sface),andrepaintitintothemaskedareainthesourceimage. Toensure
therepaintedpartisharmoniouslyblendedintothesourceimage,MimicBrushalsolearnstotransfer
thevisualcontentintothesameposture,lighting,andperspective. Itisnoteworthythatsuchatraining
processisbuiltonrawvideoclipswithouttextortrackingannotations,andcanbeeasilyscaledup
withabundantvideos.
MimicBrushleveragesadualbranchofU-Nets,i.e.,imitativeandreferenceU-Net,takingthesource
and reference images as input, respectively. The two U-Nets share their keys and values in the
attention layers, and are tamed to complete the masked source image by seeking the indications
fromthereferenceimage. Wealsoexertdataaugmentationonbothimagestoincreasethevariation
betweensourceandreferenceimage. Atthesametime,adepthmapisextractedfromtheunmasked
sourceimageandthenaddedtotheimitativeU-Netasanoptionalcondition. Inthisway,during
inference,userscoulddecidewhethertoenablethedepthmapofsourceimagetopreservetheshape
oftheobjectsintheoriginalsourceimage.
3.2 ModelStructure
OurframeworkmajorlyconsistsofanimitativeU-Net,areferenceU-Net,andadepthmodel. Inthis
section,weelaborateonthedetaileddesignsofthesecomponents.
ImitativeU-Net. TheimitativeU-Netisinitializedwithastablediffusion-1.5[33]inpaintingmodel.
Ittakesatensorwith13channelsastheinput. Theimagelatent(4channels)takeschargeofthe
diffusionprocedurefromaninitialnoisetotheoutputlatentcodestepbystep. Wealsoconcatenate
abinarymask(1channel)toindicatethegenerationregionsandabackgroundlatent(4channels)
ofthemaskedsourceimage. Inaddition,weprojectthedepthmapintoa4-channeldepthlatentto
provideshapeinformation. TheoriginalU-NetalsotakestheCLIP[31]textembeddingasinput
viacross-attention. Inthiswork, wereplaceitwiththeCLIPimageembeddingextractfromthe
reference image. Following previous works [8, 49], we add a trainable projection layer after the
imageembedding. WedonotincludethispartinFig.3forthesimplicityofillustration. During
training,alltheparametersoftheimitativeU-NetandtheCLIPprojectionlayerareoptimizable.
Reference U-Net. Recently, a bunch of works [56, 17, 46, 6, 58, 45] prove the effectiveness of
leveraginganadditionalU-Nettoextractthefine-grainedfeaturesfromthereferenceimage. Inthis
work,weapplyasimilardesignandintroduceareferenceU-Net. Itisinitializedfromastandard
stablediffusion-1.5[33]. Ittakesthe4-channellatentofthereferenceimagetoextractmulti-level
features. Following[46],weinjectthereferencefeaturesintotheimitativeU-Netinthemiddleand
upperstagesbyconcatenatingitskeysandvalueswiththeimitativeU-Netasthefollowingequation.
Inthisway,theimitativeU-Netcouldleveragethecontentfromthereferenceimagetocompletethe
maskedregionsofthesourceimage.
Q ·cat(K ,K )T
Attention=softmax( i √ i r )·cat(V ,V ) (1)
i r
d
k
Depthmodel. WeleverageDepthAnything[48]topredictthedepthmapsoftheunmaskedsource
imageasashapecontrol,whichenablesMimicBrushtoconducttexturetransfer. Wefreezethedepth
modelandaddatrainableprojector,whichprojectsthepredicteddepthmap(3-channel)tothedepth
latent(4-channel). Duringtraining,wesetaprobabilityof0.5todroptheinputofthedepthmodelas
all-zeromaps. Thus,theuserscouldtaketheshapecontrolasanoptionalchoiceduringinference.
3.3 TrainingStrategy
Tofullyunleashthecross-imageimitationabilityofMimicBrush,wefurtherproposesomestrategies
to mine more suitable training samples. Considering that our goal is to conduct robust imitative
editing even cross categories, the philosophy of collecting training data could be summarized as
twofold: First,weshouldguaranteethatthecorrespondencerelationexistsbetweenthesourceand
referenceimages. Second,weexpectlargevariationsbetweenthesourceimageandthereference
image,whichisessentialfortherobustnessoffindingthevisualcorrespondence.
4SourceImage
SourceMask SourceImage
RefImage SourceMask
RefMask RefImage
TextPrompt RefMask
DINOImage SSIM
CLIPImage PSNR
CLIPText LPIPS
Inter-IDImitation w/o GT Inner-IDReconstruction w/ GT
SourceImage
SourceMask SourceImage
RefImage SourceMask
RefMask RefImage
TextPrompt RefMask
DINOImage SSIM
CLIPImage PSNR
CLIPText LPIPS
Inter-IDImitation w/o GT Inner-IDReconstruction w/ GT
Figure4: Sampleillustrationforourbenchmark. Itcoversthetaskofpartcomposition(firstrow)
andtexturetransfer(secondrow). EachtaskincludesaInter-IDandinner-IDtrack. Theannotated
dataandevaluationmetricsforeachtrackareillustratedbesidetheexemplarimages.
Dataselection. Duringtraining,wesampletwoframesfromthesamevideo. Following[7],we
useSSIM[42]asanindicatorofthesimilaritybetweenvideoframes. Wediscardtheframepairs
withtoo-bigortoo-smallsimilaritiestoguaranteethattheselectedimagepaircontainsbothsemantic
correspondenceandvisualvariations.
Dataaugmentation. Toincreasethevariationbetweenthesourceimageandthereferenceimage,we
exertstrongdataaugmentations. Besidesapplyingtheaggressivecolorjitter,rotation,resizing,and
flipping,wealsoimplementrandomprojectiontransformationtosimulatethestrongerdeformation.
Maskingstrategy. AsimplebaselineistodividethesourceimageintoN ×N gridandrandomly
maskeachgrid. However,wefindthispurelyrandommaskingtendstocausealargeportionofeasy
cases. Forexample,asthebackground(e.g.,thegrassland,thesky)occupieslargeareaswithrepeated
content/textures,learningtocompletetheseregionsdoesnotrequirethemodeltoseekguidancefrom
thereferenceimage. Tofindmorediscriminativeregions,weapplySIFT[25]matchingbetweenthe
sourceandreferenceimagesandgetaseriesofmatchingpoints. Althoughthematchingresultsare
notperfect,theyaresufficienttoassistusinconstructingbettertrainingsamples. Specifically,we
increasethemaskingpossibilityofthegridswithmatchedfeaturepoints.
Consideringthatcollectinghigh-qualityimagesismucheasierthanvideos,wealsoconstructpseudo
framesbyapplyingaugmentationsonthestaticimagesandleveragingtheobjectsegmentationresults
formaskingthesourceimage. ThesegmentationmasksalsoimprovetherobustnessofMimicBrush
tosupportmasksinmorearbitraryshapes.
Ingeneral,MimicBrushdoesnotrelyontheheavyannotationsofthetrainingdata. Itfullybenefits
fromtheconsistencyandvariationofvideodata,andalsoleveragesimagedatatoexpandthediversity,
whichmakesthetrainingpipelinemorescalable.
3.4 EvaluationBenchmark
Imitativeeditingisanoveltask, weconstructourownbenchmarktosystematicallyevaluatethe
performance. AsshowninFig.4,wedividetheapplicationsintotwotasks: partcompositionand
texturetransfer,andwesettheinter-IDandinner-IDtrackforeachtask.
Partcompositionestimatesthefunctionsofdiscoveringthesemanticcorrespondencebetweenthe
sourceandreferenceimageandcompositingthelocalparts. Theinter-IDtrackaimstocompositethe
localpartsfromdifferentinstancesorevendifferentcategories. Wecollectdatafromthefollowing
topics: fashion,animal,product,andscenario. Foreachtopic,wemanuallycollect30samplesfrom
Pexels[29]thus120samplesintotal,whereeachsamplecontainthesourceandreferenceimagepair.
Wemanuallydrawthesourcemasktodefinethecompositionrequirement. Asthegeneratedresults
donothavegroundtruth,weannotatethereferenceregionsandwritetextpromptsfortheexpected
result. Thus,wecouldfollowDreamBooth[34]tocalculatetheDINO[27]andCLIP[31]image
similaritiesbetweenthegeneratedregionandtheannotatedreferenceregion. Inaddition,wealso
reporttheCLIPtextsimilaritybetweentheeditedimageandthetextprompts.
5
noitisopmoCtraP
refsnarTerutxeT
ataD
cirteM
ataD
cirteM
ataD
cirteM
ataD
cirteMSource+Mask Reference Fireflyw/Prompt PbEw/Box AnyDoorw/Mask Ours
Figure 5: Qualitative comparisons. Noticing that other methods require additional inputs.
Firefly[32]takesthedetailedpromptsdescriptions. Besides,wemarkthespecificreferenceregions
withboxesandmasksforPaint-by-Example[47]andAnyDoor[8]. Eventhough,MimicBrushstill
demonstratesprominentadvantagesforbothfidelityandharmony.
Wealsosetaninner-IDtrack,wherewecollect30imagepairsfromDreamBooth[34],manually
maskthediscriminativeregionsofthesourceimage,andusereferenceimagestocompletethem. The
referencewouldbeanimagecontainingthesameinstanceindifferentscenarios. Thus,theunmasked
sourceimagecouldserveasthegroundtruthtocomputeSSIM[42],PSNR[16],andLPIPS[57].
Texturetransferrequiresstrictlymaintainingtheshapeofthesourceobjectsandonlytransferring
thetexture/patternofthereferenceimage. Forthistask,weenablethedepthmapasanadditional
condition. Different from the part composition that seeks the semantic correspondence, in this
task we mask the full objects thus the model could only discover correspondence between the
textures(reference)andtheshape(source). Wealsoformulateinter-IDandinner-IDtracks. The
formerinvolves30sampleswithlargedeformationsfromPexels[29],liketransferringtheleopard
textureonacapinFig.4. Thelattercontainsanadditional30examplesfromtheDreamBooth[34]
dataset. Wefollowthesamedataformatsandevaluationmetricsaspartcomposition.
4 Experiments
4.1 ImplementationDetails
Hyperparameters. Inthiswork,allexperimentsareconductedwiththeresolutionof512×512.
Fortheimageswithdifferentaspectratios,wefirstpadtheimagesasasquareandthenresizethem
to512×512. Duringtraining,weusetheAdam[19]optimizerandsetthelearningrateas1e-5.
Experimentsareconductedwithatotalbatchsizeof64on8×A100GPUs. Forthemaskingstrategy
ofthesourceimage,werandomlychoosethegridnumberN ×N from3to10. Weset75%chances
to drop the grid with SIFT-matched features and set 50% chances for other regions. We add the
referenceU-Netasclassifier-freeguidanceanddropitduringtrainingwiththeprobabilityof10%.
Duringinference,theguidancescaleis5asdefault.
Trainingdata. Wecollect100khigh-resolutionvideosfromopen-sourcedwebsiteslikePexels[29].
Tofurtherexpandthediversityoftrainingsamples,weusetheSAM[20]datasetthatcontains10
million imagesand 1 billion objectmasks. We constructpseudo frames by applyingstrong data
augmentations on the static images from SAM and leverage the object segmentation results for
maskingthesourceimage. Duringtraining,thesamplingportionsofthevideoandSAMdataare
70%versus30%asdefault.
6Table 1: Quantitative comparisons for part composition on our constructed benchmark. The
leftpartofthetablereportstheevaluationresultsontheinner-IDtrack. Therightpartestimates
theinter-IDtrack. MimicBrushdemonstratessuperiorperformanceforeachtrackwiththemost
simplifiedinteractionform. “I”or“T”denotestheimageortextsimilarity.
inner-ID inter-ID
SSIM(↑) PSNR(↑) LPIPS(↓) DINO-I(↑) CLIP-I(↑) CLIP-T(↑)
PbE[47]w/oBox 0.51 15.17 0.49 41.44 81.00 29.45
PbE[47]w/Box 0.51 16.09 0.48 42.70 81.10 29.30
AnyDoor[8]w/oMask 0.42 12.73 0.56 43.41 78.56 28.45
AnyDoor[8]w/Mask 0.44 14.09 0.50 61.30 86.08 29.39
MimicBrush 0.70 17.54 0.28 56.48 84.30 30.08
Table2: Userstudyresults. Weletannotatorsranktheresultsofdifferentmethodsfromthebestto
theworstfromthreeaspects: fidelity,harmony,andquality. Wereportboththenumberofthebest
picksandtheaveragerankforacomprehensivecomparison.
FidelityBest FidelityRank HamonyBest HamonyRank QualityBest QualityRank
PbE[47]w/Box 10.8% 2.64 29.2% 2.57 15.8% 2.59
AnyDoor[8]w/oMask 2.8% 2.46 3.0% 2.35 4.2% 2.45
AnyDoor[8]w/Mask 30.6% 2.77 22.6% 2.72 29.4% 2.71
MimicBrush 55.8% 2.11 45.2% 2.34 50.6% 2.23
4.2 ComparisonswithOtherWorks
Inthissection,wecompareMimicBrushwithothermethodsthatcouldrealizesimilarfunctions.
Noticingthatimitativeeditingisanoveltask,noexistingmethodscouldperfectlyalignourinput
formats. Thus,weallowadditionalinputsforothermethods. Forexample,wegiveadditionalmasks
orboxesforAnyDoor[8]andPaint-by-Example[47]toindicatethereferenceregions. Wealsopick
thestate-of-the-artinpaintingtoolFirefly[32]andfeeditwithdetailedtextdescriptions.
Qualitative comparison. We visualize the qualitative results in Fig. 5. Although Firefly [32]
accuratelyfollowstheinstructionsandgenerateshigh-qualityresults,itishardforthetextprompt
to capture the details of the desired outputs, especially for logos or patterns like tattoos. Paint-
by-example[47]requiresacroppedreferenceimageinwhichthereferenceregionsarecentered.
However,evenifweprovidethiskindofinput,asthismodelonlyusesasingletokentorepresentthe
reference,itcannotguaranteethefidelitybetweenthegeneratedregionandthereferenceregion
WecarefullyannotatethemasksofthereferenceregionandfedthemtoAnyDoor[8]. Itdemonstrates
strongerabilitiesforidentitypreservationbutfailstosynthesizeharmoniousblending. Weanalyze
thattherearetwomainreasons: first,somelocalpartscouldnotbewellunderstoodwhencropped
outofthecontext. Second,mostofthetrainingsamplesofAnyDoor[8]arefullobjects. Itrequires
pairedmaskannotationforthesameinstanceindifferentvideoframesfortraining. Thepairedmasks
arefeasibletocollectforfullobjectsbuthardlyfeasibleforlocalparts. However,MimicBrushgets
aroundthisproblembyleveragingthemodeltolearnthecorrespondenceitselfinthefullcontext
insteadofusingthepairedmasks. Inthisway,MimicBrushshowssignificantsuperioritiescompared
withpreviousmethodsforcompletingarbitrarypartswithafullreferenceimage.
Quantitative comparison. We also report the results on our constructed benchmark for part
compositioninTab.1. Fortheinner-IDtrackwithgroundtruth,MimicBrushshowsthedominant
performanceeventhoughwegiveadditionalconditionsforotherworks. Forinter-IDimitation,it
is more challenging to discover the correspondent reference region. MimicBrush could still get
competitive performance compared with AnyDoor [8]. We should note that the reference mask
is given to AnyDoor. Therefore, it could forcedly locate the reference regions thus taking some
advantagesfortheevaluation.However,asdemonstratedinFig.5,itstrugglestogenerateharmonious
blendingandpreservethefinedetails.
Userstudy.Consideringthemetricscouldnotfullyreflecthumanpreferencestosomeextent,wealso
organizedauserstudy. Welet10annotatorsrankthegenerationresultsofdifferentmodelsonour
benchmark(introducedin Sec.3.4). Weevaluateeachsamplefromthreeaspects: fidelity,harmony,
andquality. Fidelityconsiderstheabilitytopreservetheidentityofthereferenceregion. Harmony
estimateswhetherthegeneratedregionscouldnaturallyblendwiththebackground. Qualityregards
whetherthegeneratedregionscontainfinedetailsingoodquality. Thosethreeaspectsareevaluated
independently. ResultsarelistedinTab.2,MimicBrushearnssignificantlymorepreferences.
7Ref&Source CLIP DINOv2 U-Net Ref&Source CLIP DINOv2 U-Net
Figure6: Ablationstudyforreferencefeatureextractors. CLIPandDINOv2encoderscouldalso
achieveimitativeeditingbutlagbehindtheU-Netinpreservingthefinedetails.
Table3: Ablationstudyfordifferentreferencefeatureextractors. U-Netdemonstratesconsistent
advantagesacrossdifferentevaluationtracksandmetricscomparedwithCLIPandDINOv2.
PartComposition TextureTransfer
SSIM(↑)PSNR(↑)LPIPS(↓)DINO-I(↑)CLIP-I(↑)CLIP-T(↑)SSIM(↑)PSNR(↑)LPIPS(↓)DINO-I(↑)CLIP-I(↑)CLIP-T(↑)
CLIPEncoder 0.66 16.78 0.31 45.03 82.3 30.05 0.75 16.78 0.31 37.86 78.30 31.39
DINOv2Encoder 0.67 16.50 0.32 48.34 82.40 29.84 0.74 17.27 0.27 46.34 78.00 30.61
Ours(U-Net) 0.70 17.54 0.28 56.48 84.30 30.08 0.75 17.73 0.26 49.83 79.44 30.75
Table 4: Ablation study for training strategies. In the first block, we verify the importance of
trainingdataandaugmentation. Inthesecondblock,weexploredifferentstrategiesformaskingthe
sourceimage. Theperformanceofourfullpipelineisgivenatthebottom.
PartComposition TextureTransfer
SSIM(↑)PSNR(↑)LPIPS(↓)DINO-I(↑)CLIP-I(↑)CLIP-T(↑)SSIM(↑)PSNR(↑)LPIPS(↓)DINO-I(↑)CLIP-I(↑)CLIP-T(↑)
◦ImageDataOnly 0.67 14.95 0.33 39.68 79.90 29.12 0.70 15.10 0.31 41.30 77.80 30.72
◦WeakAug. 0.68 16.98 0.30 50.55 83.20 29.81 0.74 18.13 0.26 50.92 80.0 31.23
◦SingleBox0.50 0.66 15.97 0.32 47.41 82.10 28.97 0.72 16.24 0.31 48.52 78.10 29.30
◦MaskGid0.25 0.68 17.10 0.30 49.17 82.94 29.80 0.74 17.35 0.27 50.09 79.50 31.05
◦MaskGid0.50 0.68 16.97 0.30 50.09 82.56 29.84 0.73 17.25 0.27 48.58 81.00 30.35
◦MaskGid0.75 0.67 16.61 0.30 49.94 83.00 29.75 0.73 16.69 0.28 52.46 81.75 30.02
•MimicBrush 0.70 17.54 0.28 56.48 84.30 30.08 0.75 17.73 0.26 49.83 79.44 31.75
4.3 AblationStudies
Inthissection,weconductextensiveablationstoverifytheeffectivenessofdifferentcomponents.
Referecefeatureextractor. MimicBrushleveragesadualU-Netstructuretomodelextractorthe
features from the source and reference image respectively. Some previous works [40, 55] prove
thatthepre-traineddiffusionmodelscontainstrongpriortocapturesemanticcorrespondence. We
explorewhetheranasymmetricstructurecouldstilllearnthesemanticcorrespondenceunderour
self-supervisedtrainingpipeline. WereplacethereferenceU-NetwiththeCLIP/DINOv2image
encoderandinjectthe16×16patchtokenswithinthecross-attentionlayers. Visualcomparisons
areprovidedinFig.6,CLIPandDINOv2alsosuccessfullylocatethereferenceregion,butU-Net
illustratesclearsuperioritiesforpreservingthefinedetails. Wealsoconductquantitativeresultsin
Tab.3,whereweconcludethatCLIPandDINOv2alsoreachcompetitiveperformance. However,as
U-Netgivesmulti-levelrepresentationswithhigherresolutions,andthefeaturespaceisnaturally
alignedwiththeinitiativeU-Net,itgivesbetterresultswhenservingasareferencefeatureextractor.
Trainingstrategies. InTab.4,Wefirstverifytheeffectivenessofthe“video-based”trainingpipeline.
Whenusingthestatisticsimagesonly,theperformanceforeachtaskdropssignificantly. Itshows
thatobjectdeformationorvariationinvideosisvitalforrealizingimitativeediting. Afterward,we
removethestrongcolorjitter,resizing,andprojectiontransformation. Weobserveacleardegradation
inpartcomposition,specificallyfortheinter-IDtrack. Itverifiestheimportanceofaugmentationfor
robustsemanticcorrespondencematching.
In the secondblock, we explore the differentmasking strategies for the source imageand report
theperformanceusingasingleboxanddifferentgidratios. AsintroducedinSec.3.3,thesepurely
randommaskingstrategiescouldcausealargenumberoflow-qualitytrainingsamples. Incontrast,
weleverageSIFTmatchingtoenhancethemaskingandreachbetterperformances(bottomrow).
4.4 QualitativeAnalysis
Inthissection,wegivemorevisualexamplesanddiscussthepotentialapplications. Asdemonstrated
in Fig. 7, MimicBrush could deal with images from various topics and domains. The first row
8Source+Mask Reference Result Source+Mask Reference Result
Figure 7: Diverse applications supported by MimicBrush. Our methods could be applied
conveniently for product design, accessories wearing, editing the scene images, and refining the
flawedgenerationresultsofothermethods. MimicBrushisabletoeditmultipleregionsinonepass.
illustrates the application for product design. The second row shows some examples of jewelry-
wearing. It should be noticed that the segmentation masks for the necklace are hard to extract,
but MimicBrush gets rid of the segmentation step and directly transfers the necklace from the
referenceimagetothesourceimage. Inthethirdrow,weshowthatMimicBrushcouldalsodeal
withbackgroundsalsonatureeffects,provingitsstronggeneralizationability.
ThelastrowillustratesapracticalapplicationthatwecouldleverageMimicBrushasapost-processing
torefinethegenerationresultsofotherworks. Intheleftexample,weimprovethefidelityforthe
image generated by AnyDoor [8]. In the right example, we mark multiple to-edit regions in the
sourceimagegeneratedbyCones-2[24]andprovideaconcatenatedreferenceimagecontainingboth
objects. WeobservethatMimicBrushcouldrefinealltheto-editregionsinasinglepass.
5 Conclusion
Wepresentanovelformofimageeditingwithsimpleinteractions,calledimitativeediting. Inour
setting,usersareonlyrequiredtomarktheeditingregiononthesourceimageandprovideareference
imagethatcontainsthedesiredvisualelements. MimicBrushautomaticallyfindsthecorresponding
referenceregiontocompletethesourceimage. Toachieveimitativeediting,wetakefulladvantage
oftheconsistencyandvariationofvideosanddesignaself-supervisedtrainingpipelinethatuses
oneframetocompletethemaskedregionsofanotherframe. MimicBrushdemonstratesimpressive
performancesforvariouseditingtasksandsupportsawiderangeofapplications. Tofacilitatefuture
explorations,weconstructabenchmarktocomprehensivelyevaluateimitativeediting. Thisworkis
expectedtobringnewinspirationforthecommunitytoexploremoreadvancedtechniquesforimage
generationandediting.
Limitations&potentialimpacts.MimicBrushdemonstratesrobustperformance.However,itcould
failtolocatethereferenceregionwhentheregionistoosmallormultiplecandidatesexistinthe
referenceimage. Inthiscase,usersshouldcropthereferenceimagetozoominonthedesiredregions.
MimicBrushcoulddealwithawiderangeofimages,thusmakingitpossibletoproducesomecontent
withnegativeimpacts. Therefore,wewouldaddcensorstofilterouttheharmfulcontentwhenwere-
leasethecodeanddemo.
9References
[1] O.Avrahami,K.Aberman,O.Fried,D.Cohen-Or,andD.Lischinski. Break-a-scene:Extractingmultiple
conceptsfromasingleimage. InSIGGRAPHAsia,2023. 3
[2] T.Brooks,A.Holynski,andA.A.Efros. Instructpix2pix:Learningtofollowimageeditinginstructions.
InCVPR,2023. 2,3
[3] M.Cao,X.Wang,Z.Qi,Y.Shan,X.Qie,andY.Zheng. Masactrl: Tuning-freemutualself-attention
controlforconsistentimagesynthesisandediting. InICCV,2023. 2
[4] B.-C.ChenandA.Kae. Towardrealisticimagecompositingwithadversariallearning. InCVPR,2019. 3
[5] H.Chen,Z.Gu,Y.Li,J.Lan,C.Meng,W.Wang,andH.Li. Hierarchicaldynamicimageharmonization.
InACMMM,2022. 3
[6] M.Chen,X.Chen,Z.Zhai,C.Ju,X.Hong,J.Lan,andS.Xiao. Wear-any-way: Manipulablevirtual
try-onviasparsecorrespondencealignment. arXiv:2403.12965,2024. 4
[7] X.Chen,Z.Liu,M.Chen,Y.Feng,Y.Liu,Y.Shen,andH.Zhao. Livephoto:Realimageanimationwith
text-guidedmotioncontrol. arXiv:2312.02928,2023. 5
[8] X.Chen, L.Huang, Y.Liu, Y.Shen, D.Zhao, andH.Zhao. Anydoor: Zero-shotobject-levelimage
customization. CVPR,2024. 2,3,4,6,7,9
[9] W.Cong,J.Zhang,L.Niu,L.Liu,Z.Ling,W.Li,andL.Zhang. Dovenet:Deepimageharmonizationvia
domainverification. InCVPR,2020. 3
[10] W.Cong,X.Tao,L.Niu,J.Liang,X.Gao,Q.Sun,andL.Zhang. High-resolutionimageharmonization
viacollaborativedualtransformations. InCVPR,2022. 3
[11] R.Gal,Y.Alaluf,Y.Atzmon,O.Patashnik,A.H.Bermano,G.Chechik,andD.Cohen-Or. Animageis
worthoneword:Personalizingtext-to-imagegenerationusingtextualinversion. InICLR,2023. 3
[12] J.Gu,Y.Wang,N.Zhao,W.Xiong,Q.Liu,Z.Zhang,H.Zhang,J.Zhang,H.Jung,andX.E.Wang.
Swapanything:Enablingarbitraryobjectswappinginpersonalizedimageediting. arXivpreprint,2024. 2
[13] Y.Gu,X.Wang,J.Z.Wu,Y.Shi,Y.Chen,Z.Fan,W.Xiao,R.Zhao,S.Chang,W.Wu,etal.Mix-of-show:
Decentralizedlow-rankadaptationformulti-conceptcustomizationofdiffusionmodels. InNeurIPS,2023.
3
[14] Z.Guo,H.Zheng,Y.Jiang,Z.Gu,andB.Zheng. Intrinsicimageharmonization. InCVPR,2021. 3
[15] A.Hertz,R.Mokady,J.Tenenbaum,K.Aberman,Y.Pritch,andD.Cohen-or. Prompt-to-promptimage
editingwithcross-attentioncontrol. InICLR,2023. 2,3
[16] A.HoreandD.Ziou. Imagequalitymetrics:Psnrvs.ssim. InICPR,2010. 6
[17] L.Hu,X.Gao,P.Zhang,K.Sun,B.Zhang,andL.Bo. Animateanyone: Consistentandcontrollable
image-to-videosynthesisforcharacteranimation. CVPR,2024. 4
[18] B.Kawar,S.Zada,O.Lang,O.Tov,H.Chang,T.Dekel,I.Mosseri,andM.Irani. Imagic:Text-basedreal
imageeditingwithdiffusionmodels. InCVPR,2023. 2
[19] D.P.KingmaandJ.Ba. Adam:Amethodforstochasticoptimization. arXiv:1412.6980,2014. 6
[20] A.Kirillov,E.Mintun,N.Ravi,H.Mao,C.Rolland,L.Gustafson,T.Xiao,S.Whitehead,A.C.Berg,
W.-Y.Lo,etal. Segmentanything. InICCV,2023. 6
[21] W.Li,Z.Lin,K.Zhou,L.Qi,Y.Wang,andJ.Jia. Mat: Mask-awaretransformerforlargeholeimage
inpainting. InCVPR,2022. 3
[22] G.Liu,F.A.Reda,K.J.Shih,T.-C.Wang,A.Tao,andB.Catanzaro. Imageinpaintingforirregularholes
usingpartialconvolutions. InECCV,2018. 3
[23] Z.Liu,R.Feng,K.Zhu,Y.Zhang,K.Zheng,Y.Liu,D.Zhao,J.Zhou,andY.Cao. Cones: Concept
neuronsindiffusionmodelsforcustomizedgeneration. InICML,2023. 3
[24] Z.Liu,Y.Zhang,Y.Shen,K.Zheng,K.Zhu,R.Feng,Y.Liu,D.Zhao,J.Zhou,andY.Cao. Cones2:
Customizableimagesynthesiswithmultiplesubjects. InNeurIPS,2023. 3,9
10[25] D.G.Lowe. Objectrecognitionfromlocalscale-invariantfeatures. InICCV,1999. 5
[26] R.Mokady,A.Hertz,K.Aberman,Y.Pritch,andD.Cohen-Or. Null-textinversionforeditingrealimages
usingguideddiffusionmodels. InCVPR,2023. 2,3
[27] M.Oquab,T.Darcet,T.Moutakanni,H.Vo,M.Szafraniec,V.Khalidov,P.Fernandez,D.Haziza,F.Massa,
A.El-Nouby,etal. Dinov2:Learningrobustvisualfeatureswithoutsupervision. TMLR,2024. 3,5
[28] Y.Pan,C.Mao,Z.Jiang,Z.Han,andJ.Zhang.Locate,assign,refine:Tamingcustomizedimageinpainting
withtext-subjectguidance. arXiv:2403.19534,2024. 3
[29] Pexels. Thebestfreestockphotos, royaltyfreeimages&videossharedbycreators. https://www.
pexels.com,2024. 5,6
[30] D.Podell,Z.English,K.Lacey,A.Blattmann,T.Dockhorn,J.Müller,J.Penna,andR.Rombach. Sdxl:
Improvinglatentdiffusionmodelsforhigh-resolutionimagesynthesis. arXiv:2307.01952,2023. 2,3
[31] A.Radford,J.W.Kim,C.Hallacy,A.Ramesh,G.Goh,S.Agarwal,G.Sastry,A.Askell,P.Mishkin,
J.Clark,etal. Learningtransferablevisualmodelsfromnaturallanguagesupervision. InICML,2021. 4,5
[32] A. reseachers. Adobe firefly: Free generative ai for creatives. https://firefly.adobe.com/
generate/inpaint,2023. 6,7
[33] R.Rombach,A.Blattmann,D.Lorenz,P.Esser,andB.Ommer. High-resolutionimagesynthesiswith
latentdiffusionmodels. InCVPR,2022. 2,4
[34] N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman. Dreambooth: Fine tuning
text-to-imagediffusionmodelsforsubject-drivengeneration. InCVPR,2023. 3,5,6
[35] M.Safaee,A.Mikaeili,O.Patashnik,D.Cohen-Or,andA.Mahdavi-Amiri. Clic: Conceptlearningin
context. InCVPR,2024. 3
[36] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes,
B.KaragolAyan,T.Salimans,etal. Photorealistictext-to-imagediffusionmodelswithdeeplanguage
understanding. InNeurIPS,2022. 2
[37] Y.Song,Z.Zhang,Z.Lin,S.Cohen,B.Price,J.Zhang,S.Y.Kim,andD.Aliaga. Objectstitch:Object
compositingwithdiffusionmodel. InCVPR,2023. 2,3
[38] Y.Song,Z.Zhang,Z.Lin,S.Cohen,B.Price,J.Zhang,S.Y.Kim,H.Zhang,W.Xiong,andD.Aliaga.
Imprint:Generativeobjectcompositingbylearningidentity-preservingrepresentation. CVPR,2024. 2
[39] K.Sunkavalli,M.K.Johnson,W.Matusik,andH.Pfister.Multi-scaleimageharmonization.InSIGGRAPH,
2010. 3
[40] L.Tang,M.Jia,Q.Wang,C.P.Phoo,andB.Hariharan. Emergentcorrespondencefromimagediffusion.
InNeurIPS,2023. 8
[41] L. Tang, N. Ruiz, Q. Chu, Y. Li, A. Holynski, D. E. Jacobs, B. Hariharan, Y. Pritch, N. Wadhwa,
K.Aberman,etal. Realfill:Reference-drivengenerationforauthenticimagecompletion. SIGGRAPH,
2024. 3
[42] Z.Wang,A.C.Bovik,H.R.Sheikh,andE.P.Simoncelli. Imagequalityassessment:fromerrorvisibility
tostructuralsimilarity. TIP,2004. 5,6
[43] S.Xie,Z.Zhang,Z.Lin,T.Hinz,andK.Zhang. Smartbrush: Textandshapeguidedobjectinpainting
withdiffusionmodel. InCVPR,2023. 2,3
[44] S.Xie,Y.Zhao,Z.Xiao,K.C.Chan,Y.Li,Y.Xu,K.Zhang,andT.Hou. Dreaminpainter:Text-guided
subject-drivenimageinpaintingwithdiffusionmodels. arXiv:2312.03771,2023. 3
[45] Z. Xu, M. Chen, Z. Wang, L. Xing, Z. Zhai, N. Sang, J. Lan, S. Xiao, and C. Gao. Tunnel try-on:
Excavatingspatial-temporaltunnelsforhigh-qualityvirtualtry-oninvideos. arXiv:2404.17571,2024. 4
[46] Z. Xu, J. Zhang, J. H. Liew, H. Yan, J.-W. Liu, C. Zhang, J. Feng, and M. Z. Shou. Magicanimate:
Temporallyconsistenthumanimageanimationusingdiffusionmodel. InCVPR,2024. 4
[47] B.Yang,S.Gu,B.Zhang,T.Zhang,X.Chen,X.Sun,D.Chen,andF.Wen. Paintbyexample:Exemplar-
basedimageeditingwithdiffusionmodels. InCVPR,2023. 3,6,7
11[48] L.Yang,B.Kang,Z.Huang,X.Xu,J.Feng,andH.Zhao. Depthanything: Unleashingthepowerof
large-scaleunlabeleddata. InCVPR,2024. 4
[49] H.Ye,J.Zhang,S.Liu,X.Han,andW.Yang. Ip-adapter: Textcompatibleimagepromptadapterfor
text-to-imagediffusionmodels. arXiv:2308.06721,2023. 4
[50] J.Yu,Z.Lin,J.Yang,X.Shen,X.Lu,andT.S.Huang. Generativeimageinpaintingwithcontextual
attention. InCVPR,2018. 3
[51] J.Yu,Z.Lin,J.Yang,X.Shen,X.Lu,andT.S.Huang.Free-formimageinpaintingwithgatedconvolution.
InICCV,2019. 3
[52] T.Yu,R.Feng,R.Feng,J.Liu,X.Jin,W.Zeng,andZ.Chen. Inpaintanything:Segmentanythingmeets
imageinpainting. arXiv:2304.06790,2023. 2
[53] Z.Yuan,M.Cao,X.Wang,Z.Qi,C.Yuan,andY.Shan. Customnet:Zero-shotobjectcustomizationwith
variable-viewpointsintext-to-imagediffusionmodels. arXiv:2310.19784,2023. 2,3
[54] B.Zhang,Y.Duan,J.Lan,Y.Hong,H.Zhu,W.Wang,andL.Niu. Controlcom: Controllableimage
compositionusingdiffusionmodel. arXiv:2308.10040,2023. 3
[55] J.Zhang,C.Herrmann,J.Hur,L.P.Cabrera,V.Jampani,D.Sun,andM.-H.Yang. Ataleoftwofeatures:
Stablediffusioncomplementsdinoforzero-shotsemanticcorrespondence. InNeurIPS,2023. 8
[56] L. Zhang. Reference-only controlnet. https://github.com/Mikubill/sd-webui-controlnet/
discussions/1236,2023. 4
[57] R.Zhang,P.Isola,A.A.Efros,E.Shechtman,andO.Wang. Theunreasonableeffectivenessofdeep
featuresasaperceptualmetric. InCVPR,2018. 6
[58] S. Zhang, L. Huang, X. Chen, Y. Zhang, Z.-F. Wu, Y. Feng, W. Wang, Y. Shen, Y. Liu, and P. Luo.
Flashface:Humanimagepersonalizationwithhigh-fidelityidentitypreservation. arXiv:2403.17008,2024.
4
12