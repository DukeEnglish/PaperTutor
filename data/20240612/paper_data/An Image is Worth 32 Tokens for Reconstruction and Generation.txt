An Image is Worth 32 Tokens
for Reconstruction and Generation
QihangYu1*,MarkWeber1,2*,XueqingDeng1,XiaohuiShen1,DanielCremers2,Liang-ChiehChen1
1ByteDance 2TechnicalUniversityMunich *equalcontribution
https://yucornetto.github.io/projects/titok.html
32 tokens can work well for…
latent size Image Reconstruction Image Generation(TiTok32tokens)
and costs
32 tokens
TiTok(ours)
256 tokens
VQGAN
65536 pixels
realimage
Figure1: WeproposeTiTok,acompact1Dtokenizerleveragingregionredundancytorepresentan
imagewithonly32tokensforimagereconstructionandgeneration.
Abstract
Recent advancements in generative models have highlightedthe crucial role of
imagetokenizationintheefficientsynthesisofhigh-resolutionimages. Tokeniza-
tion,whichtransformsimagesintolatentrepresentations,reducescomputational
demandscomparedtodirectlyprocessingpixelsandenhancestheeffectiveness
andefficiencyofthegenerationprocess. Priormethods,suchasVQGAN,typically
utilize2Dlatentgridswithfixeddownsamplingfactors. However,these2Dtok-
enizationsfacechallengesinmanagingtheinherentredundanciespresentinimages,
where adjacent regions frequently display similarities. To overcome this issue,
weintroduceTransformer-based1-DimensionalTokenizer(TiTok),aninnovative
approachthattokenizesimagesinto1Dlatentsequences. TiTokprovidesamore
compactlatentrepresentation,yieldingsubstantiallymoreefficientandeffective
representationsthanconventionaltechniques. Forexample,a256×256×3image
canbereducedtojust32discretetokens,asignificantreductionfromthe256or
1024tokensobtainedbypriormethods. Despiteitscompactnature,TiTokachieves
competitive performance to state-of-the-art approaches. Specifically, using the
same generator framework, TiTok attains 1.97 gFID, outperforming MaskGIT
baselinesignificantlyby4.21atImageNet256×256benchmark. Theadvantages
ofTiTokbecomeevenmoresignificantwhenitcomestohigherresolution. At
ImageNet512×512benchmark,TiToknotonlyoutperformsstate-of-the-artdif-
fusionmodelDiT-XL/2(gFID2.74vs.3.04),butalsoreducestheimagetokens
by64×,leadingto410×fastergenerationprocess. Ourbest-performingvariant
cansignificantlysurpassesDiT-XL/2(gFID2.13vs.3.04)whilestillgenerating
high-qualitysamples74×faster.
Preprint.Underreview.
4202
nuJ
11
]VC.sc[
1v05570.6042:viXra1 Introduction
Inrecentyears, imagegenerationhasexperiencedremarkableprogress, drivenbythesignificant
advancementsinbothtransformers[19,62,66,10,67,68]anddiffusionmodels[16,55,29,49,21].
Mirroringthetrendsingenerativelanguagemodels[48,59],thearchitectureofmanycontemporary
imagegenerationmodelsincorporateastandardimagetokenizerandde-tokenizer. Thisarrayof
models utilizes tokenized image representations—ranging from continuous [34] to discrete vec-
tors[54,61,19]—toperformacriticalfunction: translatingrawpixelsintoalatentspace. Thelatent
space(e.g.,32×32)issignificantlymorecompactthantheoriginalimagespace(256×256×3). It
offersacompressedyetexpressiverepresentation,andthusnotonlyfacilitatesefficienttrainingand
inferenceofgenerativemodelsbutalsopavesthewaytoscaleupthemodelsize.
Althoughimagetokenizersachievegreatsuccessinimagegenerationworkflows,theyencountera
fundamentallimitationtiedtotheirintrinsicdesign. Thesetokenizersarebasedonanassumption
thatthelatentspaceshouldretaina2Dstructure,tomaintainadirectmappingforlocationsbetween
thelatenttokensandimagepatches. Forexample,thetop-leftlatenttokendirectlycorrespondsto
thetop-leftimagepatch. Thisrestrictsthetokenizer’sabilitytoeffectivelyleveragetheredundancy
inherentinimagestocultivateamorecompressedlatentspace.
Takingonestepback, weraisethequestion“is2Dstructurenecessaryforimagetokenization?”
Toanswerthequestion,wedrawinspirationfromseveralimageunderstandingtaskswheremodel
predictionsarebasedsolelyonhigh-levelinformationextractedfrominputimages—suchasinimage
classification[17],objectdetection[8,77],segmentation[64,71],andmulti-modallargelanguage
models[1,40]. Thesetasksdonotneedde-tokenizers,sincetheoutputstypicallymanifestinspecific
structuresotherthanimages. Inotherwords,theyoftenformatahigher-level1Dsequenceasoutput
thatcanstillcapturealltask-relevantinformation. Priorarts,suchasobjectqueries[8,64]orthe
perceiverresampler[1],encodeimagesintoa1Dsequenceofapredeterminednumberoftokens(e.g.,
64). Thesetokensfacilitatethegenerationofoutputslikeboundingboxesorcaptions. Thesuccessof
thesemethodsmotivatesustoinvestigateamorecompact1Dsequenceasimagelatentrepresentation
inthecontextofimagereconstructionandgeneration. Itisnoteworthythatthesynthesisofboth
high-levelandlow-levelinformationiscrucialforthegenerationofhigh-qualityimages,providinga
challengeforextremelycompactlatentrepresentations.
Inthiswork,weintroduceatransformer-basedframework[62,17]designedtotokenizeanimage
toa1Ddiscretesequence,whichcanlaterbedecodedbacktotheimagespaceviaade-tokenizer.
Specifically,wepresentTransformer-based1-DimensionalTokenizer(TiTok),consistingofaVision
Transformer (ViT) encoder, a ViT decoder, and a vector quantizer following the typical Vector-
Quantized(VQ)modeldesigns[19]. Inthetokenizationphase,theimageissplitandflattenedintoa
seriesofpatches,followedbyconcatenationwitha1Dsequenceoflatenttokens. Afterthefeature
encodingprocessofViTencoder,theselatenttokensbuildthelatentrepresentationoftheimage.
Subsequenttothevectorquantizationstep[61,19],theViTdecoderisutilizedtoreconstructthe
inputimagesfromthemaskedtokensequence[15,24].
BuildinguponTiTok,weconductextensiveexperimentstoprobethedynamicsof1Dimagetokeniza-
tion. Ourinvestigationstudiestheinterplaybetweenlatentspacesize,modelsize,reconstruction
fidelity,andgenerativequality. Fromthisexploration,severalcompellinginsightsemerge:
1. Increasing the number of latent tokens representing an image consistently improves the
reconstructionperformance,yetthebenefitbecomesmarginalafter128tokens. Intriguingly,
32tokensaresufficientforareasonableimagereconstruction.
2. Scalingupthetokenizermodelsizesignificantlyimprovesperformanceofbothreconstruc-
tionandgeneration,especiallywhennumberoftokensislimited(e.g.,32or64),showcasing
apromisingpathwaytowardsacompactimagerepresentationatlatentspace.
3. 1Dtokenizationbreaksthegridconstraintsinprior2Dimagetokenizers,whichnotonly
enableseachlatenttokentoreconstructregionsbeyondafixedimagegridandleadstoa
moreflexibletokenizerdesign, butalsolearnsmorehigh-levelandsemantic-richimage
information,especiallyatacompactlatentspace.
4. 1Dtokenizationexhibitssuperiorperformanceingenerativetraining,withnotonlyasignifi-
cantspeed-upforbothtrainingandinferencebutalsoacompetitiveFIDscorecomparedto
atypical2Dtokenizer,whileusingmuchfewertokens.
2TiTok-L-32 TiTok-B-64 TiTok-L-64
150xfaster TiTok-B-128
410xfaster
169xfaster
TiTok-S-128 333x
faster
13xfaster
U-ViT-L/2 DiT-XL/2
LDM-4 U-ViT-H/2
ViT-VQGAN
ADM MaskGIT U-ViT-L/4 U-ViT-H/4 DiT-XL/2
bettergenerationquality bettergenerationquality
1.78 1.69
(a)Modelcomparisononresolutionof256 (b)Modelcomparisononresolutionof512
Figure2: AspeedandqualitycomparisonofTiTokandpriorartsonImageNet256×256and
512×512generationbenchmarks. Speed-upiscomparedagainstDiT-XL/2[49]. Thesampling
speed(de-tokenizationincluded)ismeasuredwithanA100GPU.
Inlightofthesefindings,weintroducetheTiTokfamily,encompassingmodelsofvaryingmodel
sizesandlatentsizes,capableofachievinghighlycompacttokenizationwithasfewas32tokens. We
furtherconfirmthemodel’sefficacyinimagegenerationthroughtheMaskGIT[9]framework. TiTok
isdemonstratedtofacilitatestate-of-the-artperformanceinimagegeneration,whilerequiringlatent
spacesthatare8×to64×smaller,resultinginsignificantaccelerationsduringboththetrainingand
inferencephases. Italsogeneratesimageswithsimilarorhigherqualitybutupto410×fasterthan
state-of-the-artdiffusionmodelssuchasDiT[18](Fig.2).
2 RelatedWork
Image Tokenization. Images have been compressed since the early days of deep learning with
autoencoders[27,63]. Thegeneraldesignofusinganencoderthatcompresseshigh-dimensional
imagesintoalow-dimensionallatentrepresentationandthenusingadecodertoreversetheprocess,
hasproventobesuccessfulovertheyears.VariationalAutoencoders(VAEs)[34]extendtheparadigm
bylearningtomaptheinputtoadistribution. Insteadofmodelingacontinuousdistribution,VQ-
VAEs[47,53]learnadiscreterepresentationformingacategoricaldistribution. VQGAN[19]further
improves the training process by using adversarial training [23]. The transformer design of the
autoencoderisfurtherexploredinViT-VQGAN[65]andEfficient-VQGAN[7]. Orthogonaltothis,
RQ-VAE[36]andMoVQ[76]studytheeffectofusingmultiplevectorquantizationstepsperlatent
embedding,whileMAGVIT-v2[68]andFSQ[45]proposealookup-freequantization. However,
allaforementionedworkssharethesameworkflowofanimagealwaysbeingpatchwiseencoded
into a 2D grid latent representation. In this work, we explore an innovative 1D sequence latent
representationforimagereconstructionandgeneration.
Tokenization for Image Understanding. For image understanding tasks (e.g., image classifica-
tion[17],objectdetection[8,77,74],segmentation[64,70,72],andMulti-modalLargeLanguage
Models(MLLMs)[1,40,73]),itiscommontouseageneralfeatureencoderinsteadofanautoen-
codertotokenizetheimage. Specifically,manyMLLMs[40,42,58,32,22,11]usesaCLIP[51]
encodertotokenizetheimageintohighlysemantictokens,whichproveseffectiveforimagecap-
tioning[13]andVQA[2]. SomeMLLMsalsoexplorediscretetokens[32,22]or“de-tokenize”the
CLIPembeddingsbacktoimagesthroughdiffusionmodels[58,32,22]. However,duetothenature
ofCLIPmodelsthatfocusonhigh-levelinformation,thesemethodscanonlyreconstructanimage
withhigh-levelsemanticsimilarities(i.e.,thelayoutsanddetailsarenotwell-reconstructeddueto
CLIPfeatures). Therefore,ourmethodissignificantlydifferentfromtheirs,sincetheproposedTiTok
aimstoreconstructboththehigh-levelandlow-leveldetailsofanimage,sameastypicalVQ-VAE
tokenizers[34,54,19].
ImageGeneration. ImagegenerationmethodsrangefromsamplingtheVAE[34],usingGANs[23]
toDiffusionModels[16,55,29,49,21]andautoregressivemodels[60,12,47]. Priorstudiesthat
are most related to this work build on top of a learned VQ-VAE codebook to generate images.
Autoregressivetransformer[19,65,7,36],similartodecoder-onlylanguagemodels,modeleach
patchinastep-by-stepfashion, thusrequiringasmanystepsastokennumber, e.g., 256or1024.
3
deepselpmasretsaf
)teslav(dnuobrewolDIF
deepselpmasretsaf
)teslav(dnuobrewolDIFK(e.g.,32)
input latenttokens reconstruction
1 image patches latent tokens
. . . . . . 19
0 input 𝑬𝒏𝒄 VisionTransformer . . .
(a)ImageReconstruction mask tokens c 0odebook
. . . 1
1 1 1 N
𝑸𝒖𝒂𝒏𝒕
19 B Trid ai nr se fc ot rio mn ea rl 19 𝑫𝒆𝒄 VisionTransformer 1 19 . . . 0
0 0 0
. . .
latent masked predicted
tokens tokens tokens reconstruction
(b)ImageGeneration (c)TiTok Tokenization
Figure3: Illustrationofimagereconstruction(a)andgeneration(b)withtheTiTokframework
(c). TiTokcontainsanencoderEnc,aquantizerQuant,andadecoderDec. Imagepatches,along
withafew(e.g.,32)latenttokens,arepassedthroughtheVisionTransformer(ViT)encoder. The
latenttokensarethenvector-quantized. Thequantizedtokens,alongwiththemasktokens[15,24],
arefedtotheViTdecodertoreconstructtheimage.
Non-autoregressive(orbidirectional)transformers[76,68],suchasMaskGIT[9],generallypredict
morethanasingletokenperstepandthusrequiresignificantlyfewerstepstopredictacomplete
image. Apartfromthat,furtherstudieslookedintoimprovedsamplingstrategies[38,39,37]. Aswe
focusonthetokenizationstage,weapplythecommonlyusednon-autoregressivesamplingscheme
ofMaskGITtogenerateasequenceoftokensthatislaterdecodedintoanimage.
3 Method
3.1 PreliminaryBackgroundonVQ-VAE
Theimagetokenizerplaysapivotalroleinfacilitatingthegenerativemodelsbyprovidingacompact
imagerepresentationatlatentspace.Forthescopeofourdiscussion,weprimarilyfocusontheVector-
Quantized(VQ)tokenizer[61,19],givenitsbroadapplicabilityacrossvariousdomains,including
butnotlimitedtoimageandvideogeneration[19,9,55,67],large-scalepretraining[12,5,46,3,18]
andmulti-modalmodels[20,69].
AtypicalVQmodelcontainsthreekeycomponents: anencoderEnc,avectorquantizerQuant,and
adecoderDec. GivenaninputimageI ∈ RH×W×3,whereH andW denotetheimage’sheight
andwidth,theimageisinitiallyprocessedbytheencoderEncandconvertedtolatentembeddings
Z 2D =Enc(I),whereZ 2D ∈RH f×W f ×D,whichdownsamplesthespatialdimensionsbyafactorof
f. Subsequently,eachembeddingz ∈RD ismapped(viathevectorquantizerQuant)tothenearest
codec ∈RD inalearnablecodebookC∈RN×D,comprisingN codes. Formally,wehave:
i
Quant(z)=c , wherei= argmin ∥z−c ∥ . (1)
i j 2
j∈{1,2,...,N}
Duringde-tokenization,thereconstructedimageˆIisobtainedviathedecoderDecasfollows:
ˆI=Dec(Quant(Z )). (2)
2D
DespitethenumerousimprovementsoverVQ-VAE[61](e.g.,lossfunction[19],modelarchitec-
ture[65],andquantization/codebookstrategies[76,36,68]),thefundamentalworkflow(e.g.,the2D
grid-basedlatentrepresentations)haslargelyremainedunchanged.
3.2 TiTok: From2Dto1DTokenization
WhileexistingVQmodelshavedemonstratedsignificantachievements,anotablelimitationwithinthe
standardworkflowexists: thelatentrepresentationZ isoftenenvisionedasastatic2Dgrid. Sucha
2D
configurationinherentlyassumesastrictone-to-onemappingbetweenthelatentgridsandtheoriginal
image patches. This assumption limits the VQ model’s ability to fully exploit the redundancies
presentinimages,suchassimilaritiesamongadjacentpatches. Additionally,thisapproachconstrains
4
. . .
koTiT
. . .
redocne
QV
. . .
koTiT
redoced
. . .
. .theflexibilityinselectingthelatentsize,withthemostprevalentconfigurationsbeingf =4,f =8,
orf =16[55],resultingin4096,1024,or256tokensforanimageofdimensions256×256×3.
Inspiredbythesuccessof1Dsequencerepresentationsinaddressingabroadspectrumofcomputer
vision problems [8, 1, 40], we propose to use a 1D sequence, without the fixed correspondence
betweenlatentrepresentationandimagepatchesin2Dtokenization, asanefficientandeffective
latentrepresentationforimagereconstructionandgeneration.
Image Reconstruction with TiTok. To initiate our exploration, we establish a novel frame-
worknamedTransformer-based1-DimensionalTokenizer(TiTok),leveragingVisionTransformer
(ViT)[17]1totokenizeimagesinto1Dlatenttokensandsubsequentlyreconstructtheoriginalimages
fromthese1Dlatents. AsdepictedinFig.3,TiTokemploysastandardViTforboththetokenization
and de-tokenization processes (i.e., both the encoder Enc and decoder Dec are ViTs). During
tokenization,wepatchifytheimageintopatches(withapatchembeddinglayer)P ∈ RH f×W f ×D
(withpatchsizeequaltothedownsamplingfactorf andembeddingdimensionD)andconcatenate
themwithK latenttokensL∈RK×D. TheyarethenfedintotheViTencoderEnc. Intheencoder
output,weonlyretainthelatenttokensastheimage’slatentrepresentation,therebyenablingamore
compactlatentrepresentationof1DsequenceZ (withlengthK). Thisadjustmentdecouplesthe
1D
latentsizefromimage’sresolutionandallowsmoreflexibilityindesignchoices. Thatis,wehave:
Z =Enc(P⊕L), (3)
1D
where⊕denotesconcatenation,andweonlyretainthelatenttokensfromtheencoderoutput.
In thede-tokenization phase, drawing inspirationfrom [15, 5, 24], we incorporatea sequenceof
masktokensM∈RH f×W f ×D—obtainedbyreplicatingasinglemasktoken H × W times—tothe
f f
quantizedlatenttokensZ . TheimageisthenreconstructedviatheViTdecoderDecasfollows:
1D
ˆI=Dec(Quant(Z )⊕M), (4)
1D
wherethelatenttokensZ isfirstvector-quantizedbyQuantandthenconcatenatedwiththemask
1D
tokensMbeforefeedingtothedecoderDec.
Despiteitssimplicity,weemphasizethattheconceptofcompact1Dimagetokenizationremains
underexploredinexistingliterature. TheproposedTiTokthusservesasafoundationalplatformfor
exploringthepotentialsof1Dtokenizationandde-tokenizationfornaturalimages. Itisworthnoting
thatalthoughonemayflatten2Dgridlatentsintoa1Dsequence,itsignificantlydiffersfromthe
proposed1Dtokenizer,duetothefactthattheimplicit2Dgridmappingconstraintsstillpersist.
Image Generation with TiTok. Besides the image reconstruction task which the tokenizer is
trainedfor,wealsoevaluateitseffectivenessforimagegeneration,followingthetypicalpipeline[19,
9]. Specifically, we adopt MaskGIT [9] as our generation framework due to its simplicity and
effectiveness,allowingustotrainaMaskGITmodelbysimplyreplacingitsVQGANtokenizerwith
ourTiTok. WedonotmakeanyotherspecificmodificationstoMaskGIT,butforcompleteness,we
brieflydescribeitswholegenerationprocesswithTiTok.
Theimageispre-tokenizedinto1Ddiscretetokens. Ateachtrainingstep,arandomratioofthelatent
tokens are replaced with mask tokens. Then, a bidirectional transformer takes the masked token
sequenceasinput,andpredictsthecorrespondingdiscretetokenIDofthosemaskedtokens. The
inferenceprocessconsistsofmultiplesamplingsteps,whereateachstepthetransformer’sprediction
formaskedtokenswillbesampledbasedonthepredictionconfidence,whicharethenusedtoupdate
the masked images. In this way, the image is “progressively generated” from a sequence full of
masktokenstoanimagewithgeneratedtokens, whichcanlaterbede-tokenizedbackintopixel
spaces. TheMaskGITframeworkshowsasignificantspeed-upinthegenerationprocesscomparedto
auto-regressivemodels. Wereferreadersto[9]formoredetails.
3.3 Two-StageTrainingofTiTokwithProxyCodes
ExistingTrainingStrategiesforVQModels.AlthoughmostVQmodelsadheretoastraightforward
formulation, their training process is notably sensitive, and the model’s performance is heavily
influencedbytheadoptionofmoreeffectivetrainingparadigms. Forinstance,VQGAN[19]achieves
1AlthoughotherTransformer-basedarchitectures(e.g.,Swin[43])canalsobeusedtoinstantiateTiTok,we
adoptViTforitssimplicityandeffectiveness.
5asignificantimprovementinreconstructionFID(rFID)ontheImageNet[14]validationset,when
comparedtodVAEfromDALL-E[52]. Thisenhancementisattributedtoadvancementsinperceptual
loss[33,75]andadversarialloss[23]. Moreover,MaskGIT’smodernimplementationofVQGAN[9]
utilizesrefinedtrainingtechniqueswithoutarchitecturalimprovementstoboosttheperformance
further. Notably,mostoftheseimprovementsareexclusivelyappliedduringthetrainingphase(i.e.,
throughauxiliarylosses)andsignificantlyaffectthemodels’efficacy. Giventhecomplexityofthe
lossfunctions,extensivetuningofhyper-parametersinvolved,and,mostcritically,themissingof
a publicly available code-base for reference or reproduction [9, 65, 68], establishing an optimal
experimentalsetupfortheproposedTiTokpresentsasubstantialchallenge,especiallywhenthetarget
isacompact1Dtokenizationwhichwasrarelystudiedinliterature.
Two-Stage Training Comes to the Rescue. Although training TiTok with the typical Taming-
VQGAN [19] setting is feasible, we introduce a two-stage training paradigm for an improved
performance. Thetwo-stagetrainingstrategycontains“warm-up”and“decoderfine-tuning”stages.
Specifically,inthefirst“warm-up”stage,insteadofdirectlyregressingtheRGBvaluesandemploying
avarietyoflossfunctions(asinexistingmethods),weproposetotrain1DVQmodelswiththediscrete
codesgeneratedbyanoff-the-shelfMaskGIT-VQGANmodel,whichwerefertoasproxycodes. This
approachallowsustobypasstheintricatelossfunctionsandGANarchitectures,therebyconcentrating
oureffortsonoptimizingthe1Dtokenizationsettings. Importantly,thismodificationdoesnotharm
thefunctionalityofthetokenizerandquantizerwithinTiTok,whichcanstillfullyfunctionforimage
tokeniztion and de-tokenization; the main adaptation simply involves the processing of TiTok’s
de-tokenizeroutput. Specifically,thisoutput,comprisingasetofproxycodes,issubsequentlyfed
intothesameoff-the-shelfVQGANdecodertogeneratethefinalRGBoutputs. Itisnoteworthythat
theintroductionofproxycodesdiffersfromasimpledistillation[26]. Asverifiedinourexperiments,
TiTokyieldssignificantlybettergenerationperformancethanMaskGIT-VQGAN.
Afterthefirsttrainingstagewithproxycodes,weoptionallyhavethesecond“decoderfine-tuning”
stage,inspiredby[10,50],toimprovethereconstructionquality. Specifically,wekeeptheencoder
andquantizerfrozen,andonlytrainthedecodertowardspixelspacewiththetypicalVQGANtraining
recipe[19]. Weobservethatsuchatwo-stagetrainingstrategysignificantlyimprovesthetraining
stabilityandreconstructedimagequality,asshownintheexperiments.
4 ExperimentalResults
4.1 PreliminaryExperimentsof1DTokenization
BuildinguponTiTok,weexplorearangeofconfigurations,includingthemodelsizeandthenumber
oftokens,toidentifythemostefficientandeffectivesetupfora1Dimagetokenizer.Thesepreliminary
experimentsservetoprovideathoroughevaluation,seekingapracticalconfigurationofTiTok.
PreliminaryExperimentalSetup. Unlessspecifiedotherwise, wetrainallmodelswithimages
of resolution H = 256 and W = 256, using the open-source MaskGIT-VQGAN [9] to supply
proxy codes for training. The patch size for both tokenizer and de-tokenizer is established with
f =16,andthecodebookCisconfiguredtohaveN =1024entrieswitheachentryavectorwith
16channels. ForTiTokvariants,weprimarilyinvestigatethreemodelsizes—small,base,andlarge
(i.e.,TiTok-S,TiTok-B,TiTok-L)—comprising22M,86M,and307M parametersforencoderand
decoder,respectively. WealsoassesstheimpactofvaryingthenumberoflatenttokensK from16to
256. Weperformablationexperimentswithanefficientsetting(e.g.,shortertraining).
Evaluation Protocol. Evaluation is conducted across multiple metrics to thoroughly assess the
models, including both reconstruction and generation FID metrics (i.e., rFID and gFID) [25] on
theImageNetdataset. Weexaminetraining/inferencethroughputtoofferadirectcomparisonof
generativemodel’sefficiencyrelativetodifferentlatentsizes. Furthermore,giventhatthe1DVQ
modelinherentlyservesasaformofcompactimagecompression,wefurtherinvestigatethesemantic
information retained by the model through linear probing following MAE setting [24]. For the
completedetailsofthetrainingandtestingprotocols(e.g.,hyper-parameters,trainingcosts),werefer
thereadertothesupplementarymaterialSec.A.
Afterthesetup,wenowsummarizethepreliminaryexperimentalfindingsbelow.
AnImageCanbeRepresentedby32Tokens. Theredundancyinherentinimagerepresentation
iswell-acknowledged,asevidencedbythepracticeofmaskingsignificantportionsofimages(e.g.,
6TiTok-S TiTok-B TiTok-L Taming-VQGAN MaskGIT-VQGAN
25 62.5 16 6000574 13 6.3 0.0 160
60.0 14 5000 140
20 57.5 12 4000 123.1 120
15 55.0 10 3000 89.8 100 52.5 2815.2 80
8 70.9
10
50.0 2000 54.1 60
6 5 44 57 .. 05 4 1000 1268.3 824.8 555.1 33 29 7.0
.0
212 97 .. 75 24 00
1632 64 96 128 192 256 1632 64 96 128 192 256 1632 64 96 128 192 256 1632 64 96 128 192 256
number of latent tokens number of latent tokens number of latent tokens number of latent tokens
(a) IN-1K Reconstruction (b) IN-1K Linear Probe (c) IN-1K Generation (d) Sampling Speed
Figure4: PreliminaryexperimentalresultswithdifferentTiTokvariants. Weprovideacom-
prehensive exploration in (a) ImageNet-1K reconstruction. (b) ImageNet-1K linear probing. (c)
ImageNet-1Kgeneration. (d)TrainingandinferencethroughputofMaskGIT-ViTasgeneratorand
TiTokastokenizer(evaluatedonA100GPUs,inferenceincludesde-tokenizationstepwithTiTok-B).
DetailednumberscanbefoundinsupplementarymaterialSec.B.
75%inMAE[24])toexpeditethetrainingprocesswithoutnegativelyaffectingperformance. This
strategyhasbeenvalidatedacrossavarietyofcomputervisiontasksthatrelyonhigh-levelimage
features[30,41]. However,theefficacyofsuchapproachesinthecontextofimagereconstruction
andgeneration—wherebothlow-levelandhigh-leveldetailsarecrucialforcreatingrealisticrecon-
structedandgeneratedoutputs—remainsunderexplored. Consequently,inthisexperiment,weaimto
determinetheminimumnumberoftokensrequiredtoreconstructandgeneratehigh-qualityimages.
AsdepictedinFig.4a,althoughmodelperformanceprogressivelyimproveswithanincreaseinthe
numberoflatenttokens,significantenhancementsarepredominantlyobservedwhenK rangesfrom
16to128. Beyondthispoint,increasingthelatentspacesizeyieldsonlymarginalgains. Intriguingly,
wefindthatwithmerely32latenttokens,TiTok-Lachievesperformancebetterthana2DVQGAN
model[19]using256tokens. Thisobservationsuggeststhatasfewas32tokensmaysufficeasan
effectiveimagelatentrepresentation,optimizingtheutilizationofimageredundancy.
ScalingUpTokenizerEnablesMoreCompactLatentSize. Anotherintriguingobservationfrom
Fig. 4aisthatlargertokenizersfacilitatemorecompactrepresentations. Specifically,TiTok-Bwith64
latenttokensachievesperformancecomparabletoTiTok-Swith128latenttokens,whileTiTok-Lwith
32latenttokensmatchestheperformanceofTiTok-Bwith64latenttokens. Thispatternindicates
thatwitheachincrementalincreaseinTiToksize(e.g.,fromStoB,orfromBtoL),itispossibleto
reducethesizeofthelatentimagerepresentationwithoutcompromisingperformance. Thistrend
underscoresthepotentialbenefitsofscalingupthetokenizertoachieveevenmorecompactimage
representations.
SemanticsEmergeswithCompactLatentSpace. Toevaluatethelearnedimagerepresentation,we
performlinearprobingexperimentsontheimagetokenizer,asshowninFig. 4b. Specifically,we
addabatchnormalizationlayer[31]followedbyalinearlayerontopofthefrozenfeaturesfrom
TiTokencoder,withallhyper-parametersstrictlyfollowingtheMAEprotocol[24]. Wefindthatas
thesizeofthelatentrepresentationdecreases,thetokenizerincreasinglylearnssemanticallyrich
representations,asindicatedbytheimprovedlinearprobingaccuracy. Thissuggeststhatthemodel
learnshigh-levelinformationinscenariosofconstrainedrepresentationspace.
CompactLatentRepresentationImprovesGenerativeTraining. Inadditiontoreconstruction
capabilities, we assess TiTok’s effectiveness and efficiency in generative downstream tasks, as
illustratedinFig. 4candFig. 4d. Wenotethatvariantsofdifferenttokenizersizesyieldcomparable
outcomeswhenthenumberoflatenttokensissufficientlylarge(i.e.,K ≥128). However,within
thedomainofcompactlatentsizes(i.e.,K ≤64),largertokenizersnotablyenhanceperformance.
Furthermore, the adaptabilityof 1Dtokenization inTiTokfacilitates moreefficient andeffective
generativemodeltraining. Forinstance,modelvariantswithK =32,despiteinferiorreconstruction
quality, demonstratesignificantlybettergenerativeperformance, underscoringtheadvantagesof
employing a more condensed and semantically rich latent space for generative model training.
Additionally,thereductioninlatenttokensmarkedlyacceleratestrainingandinference,witha12.8×
increaseintrainingspeed(2815.2vs.219.7samples/s/gpu)anda4.5×speedupsamplingspeed
(123.1vs.27.5samples/s/gpu),whenutilizingK =32asopposedtoK =256.
7
)DIFr(
DIF
noitcurtsnocer
)%(
ycarucca
eborp
raenil
)DIFg(
DIF noitareneg
)upg/s/selpmas(
tuphguorht
gniniart
)upg/s/selpmas(
tuphguorht
ecnerefniTable1: ImageNet-1K256×256generationresultsevaluatedwithADM[16]. †:Trainedon
OpenImages [35] ‡: Trained on OpenImages, LAION-Aesthetics/-Humans [56]. P: generator’s
parameters. S:samplingsteps. T:throughputassamplespersecondsonA100withfloat32precision.
tokenizer #tokens codebooksizerFID↓ generator gFID↓ P↓ S↓ T↑
diffusion-basedgenerativemodels
Taming-VQGAN†[55] 1024 16384 1.14 LDM-8[55] 7.76 258M 200 -
VAE†[55] 4096×3 - 0.27 LDM-4[55] 3.60 400M 250 0.4
UViT-L/2[4] 3.40 287M 50 1.1
VAE[57]‡ 1024×4 - 0.62 UViT-H/2[4] 2.29 501M 50 0.6
DiT-XL/2[49] 2.27 675M 250 0.6
transformer-basedgenerativemodels
Taming-VQGAN[19] 256 1024 7.94 Taming-Transformer[19] 15.78 1.4B 256 7.5
8.71 1.4B 16.1
RQ-VAE[36] 256 16384 3.20 RQ-Transformer[36] 64
7.55 3.8B 9.7
MaskGIT-VQGAN[9] 256 1024 2.28 MaskGIT-ViT[9] 6.18 177M 8 50.5
ViT-VQGAN[65] 1024 8192 1.28 VIM-Large[65] 4.17 1.7B 1024 0.3
TiTok-L-32 32 4096 2.21 MaskGIT-ViT[9] 2.77 177M 8 101.6
TiTok-B-64 64 4096 1.70 MaskGIT-ViT[9] 2.48 177M 8 89.8
2.50 8 53.3
TiTok-S-128 128 4096 1.71 MaskGIT-UViT-L[9,4] 287M
1.97 64 7.8
4.2 MainExperiments
Basedontheobservationsabove,theproposedTiTokfamilyeffectivelytradesoffalargermodelsize
toamorecompactlatentsize. Inthissection,wemajorlyfocusonImageNetgenerationbenchmarks
againstpriorarts,andevaluateTiTokasatokenizerinthegenerativeMaskGITframework[9].
ImplementationDetails. WeprimarilyinvestigatethefollowingTiTokvariants: TiTok-S-128(i.e.,
smallmodelwith128tokens),TiTok-B-64(i.e.,basemodelwith64tokens),andTiTok-L-32(i.e.,
largemodelwith32tokens),whereeachvariantdesignedtohalvethelatentspacesizewhilescaling
upthemodelsize. Forresolution512,wedoublethelatentsizetoensuremoredetailsarekeptat
higherresolution,leadingtoTiTok-L-64andTiTok-B-128. InthefinalsettingforTiToktraining,
thecodebookisconfiguredtoN = 4096, andthetrainingdurationisextendedto1M iterations
(200epochs). Wealsoadoptthe“decoderfine-tuning”stagetofurtherenhancemodelperformance,
wheretheencoderandquantizerarekeptfrozenandthedecoderisfine-tunedfor500k iterations.
Forthetrainingofgenerativemodels,weutilizetheMaskGIT[9]frameworkwithoutanyspecific
modifications,exceptfortheadoptionofanarccosmaskingschedule[6]. Allotherparametersare
thesameasprevioussetups,andalldesignimprovementswillbeverifiedintheablationstudies.
Main Results. We summarize the results on ImageNet-1K generation benchmark of resolution
256×256and512×512inTab.1andTab.2,respectively.2
ForImageNet256×256resultsinTab.1,TiTokcanachieveasimilarlevelofreconstructionFID
(rFID) with a much smaller number of latent tokens than other VQ models. Specifically, using
merely 32 tokens, TiTok-L-32 achieves a rFID of 2.21, comparable to the well trained VQGAN
from MaskGIT [9] (rFID 2.28), while using 8× smaller latent representation size. Furthermore,
when using the same generator framework and same sampling steps, TiTok-L-32 improves over
MaskGITbyalargemargin(from6.18to2.77gFID),showcasingthebenefitsofamoreeffective
generatortrainingwithcompact1Dtokens. Whencomparedtootherdiffusion-basedgenerative
models,TiTokcanalsoachieveacompetitiveperformancewhileenjoyinganover100×speed-up
during the sampling process. Specifically, TiTok-L-32 achieves a better gFID than LDM-4 [55]
(2.77vs.3.60),whilegeneratingimagesdramaticallyfasterby254times(101.6samples/svs.0.4
samples/s). Ourbest-performingvariantTiTok-S-128outperformsstate-of-the-artdiffusionmethod
DiT-XL/2[49](gFID1.97vs.2.27),witha13×speed-up.
ForImageNet512×512resultsinTab.2,thesignificantlybetteraccuracy-costtrade-offofTiTok
persists. TiTokmaintainsareasonablygoodrFIDcomparedtoothermethods,especiallyconsidering
thatTiTokusesmuchfewertokens(i.e.,highercompressionratio). Forgeneration,allTiTokvariants
2For fairness, we mainly consider tokenizers with vanilla VQ modules. More advanced quantization
methods [68, 45] may further benefit TiTok but beyond this paper’s focus on 1D image tokenization. See
supplementarymaterialSec.Cforthecompletetable.
8Table2: ImageNet-1K512×512generationresultsevaluatedwithADM[16]. ‡:Trainedon
OpenImages,LAION-AestheticsandLAION-Humans[56]. P:generator’sparameters. S:sampling
steps. T:throughputassamplespersecondsonA100withfloat32precision.
tokenizer #tokens codebooksizerFID↓ generator gFID↓ P↓ S↓ T↑
diffusion-basedgenerativemodels
UViT-L/4[4] 4.67 287M 50 1.0
VAE[57]‡ 4096×3 - 0.19 UViT-H/4[4] 4.05 501M 50 0.6
DiT-XL/2[49] 3.04 675M250 0.1
transformer-basedgenerativemodels
MaskGIT-VQGAN[9] 1024 1024 1.97 MaskGIT-ViT[9] 7.32 177M 12 3.9
TiTok-L-64 64 4096 1.77 MaskGIT-ViT[9] 2.74 177M 8 41.0
MaskGIT-ViT[9] 2.49 177M 8 33.3
TiTok-B-128 128 4096 1.52
MaskGIT-ViT[9] 2.13 177M 64 7.4
Table3: Ablationstudyimprovedfinalmodelsformainexperiments. Weablatethetokenizer
designs,andgeneratordesignsonImageNet-1kbenchmark. Thefinalsettingsarelabeledingray.
Generationresultsarebasedontokenizerswithoutdecoderfine-tuning
(a)TiTokconfiguration. Results (b)Maskingschedulesforgenera- (c)Effectsofproxycodes
reportedinaccumulationmanner torwithTiTok-L-32
rFID↓ IS↑
TiTok-L-32 rFID↓ IS↑ schedule gFID↓ IS↑ Taming-VQGANtrainingsetting
baseline 6.59 110.3 cosine 5.17 191.8 Taming-VQGAN(2D) 7.94 -
+largercodebook 5.85 116.6 arccos 4.94 194.0 TiTok-B-64(2D) 15.58 64.2
+200epochs 5.48 117.3 linear 4.95 193.7 TiTok-B-64 5.15 120.5
+decoderfinetuning 2.21 195.5 squareroot 5.63 170.9 Two-stagetrainingwithproxycodes
TiTok-B-64 1.70 195.2
significantlyoutperformourbaselineMaskGIT[9]byalargemargin. Whencomparedwithdiffusion-
basedmodels,TiTok-L-64showsasuperiorperformancetoDiT-XL/2[49](2.74vs.3.04),while
running410×faster. Thebest-performingvariantTiTok-B-128cansignificantlyoutperformDiT-
XL/2byalargemargin(2.13vs.3.04)butalsogenerateshigh-qualitysamples74×faster. Wealso
providevisualizationresultsandanalysisinsupplementarymaterialSec.D.
4.3 AblationStudies
WereporttheablationstudiesregardingourfinalmodeldesignsinTab.3. Specifically,inTab.3a,
weablatethetokenizerdesignsonimagereconstruction. WebeginwithourbaselineTiTok-L-32
whichattains6.59rFID.EmployingalargercodebooksizeimprovestherFIDby0.74,whilefurther
increasingthetrainingiterations(from100epochsto200epochs)yieldsanother0.37improvement
ofrFID.Ontopofthat,the“decoderfine-tuning”(ourstage-2trainingstrategy)cansubstantially
improvetheoverallreconstructionperformanceto2.21rFID.
InTab.3b,weexaminetheeffectsofdifferentmaskingschedulesforMaskGITwithTiTok. Inter-
estingly,unliketheoriginalMaskGITsetting[9]whichempiricallyfoundthatthecosinemasking
schedulesignificantlyoutperformstheotherschedules,weobservethatMaskGITequippedwith
TiTok changes the preference to the arccos or linear schedules. Additionally, unlike [9] which
reportedthattherootscheduleperformsmuchworsethantheothers,weobservethatTiTokisquite
robusttodifferentmaskingschedules. WeattributetheobservationstoTiTok’sabilitytoprovidea
morecompactandmoresemanticmeaningfultokenscomparedto2DVQGAN,ascomparedtothe
cosinemaskingschedule,linearandarccosscheduleshavealowermaskingratiointheearlysteps.
Thiscoincideswiththeobservationthatmaskingratioisusuallyhigherforredundantsignals(e.g.,
75%maskingratioinimages[24])whilerelativelylowerforsemanticmeaningfulinputs(e.g.,15%
maskingratioinlanguages[15]).
We ablate the effects of training paradigm in Tab. 3c. We begin with the training setting of
Taming-VQGAN[19],whereTiTok-B-64obtains5.15rFID,outperformingtheoriginal2DTaming-
VQGAN’s7.94rFIDunderthesametrainingsetting. Wealsoshowthenecessityof1Dtokenization
bybuildinga2DvariantofTiTok-B64,wherethearchitectureremainsthesameexceptthatimage
patchesinsteadoflatenttokensareusedasimagerepresentation. Asaresult,weobservethatthe2D
variantsuffersfromamuchworseperformance(15.58vs.5.15rFID),sincethefixedcorrespondences
in 2D tokenization limited a reasonable reconstruction under compact latent space. This result
9demonstratestheeffectivenessoftheproposed1Dtokenization,especiallyatamuchmorecompact
latentsize. AlthoughTiTokcanachieveareasonablywellperformanceunderstraightforward
single-stagetraining,thereexistsaperformancegapcomparedtotheMaskGIT-VQGAN[9]
duetothemissingofastrongtrainingrecipe, ofwhichnopublicreferenceoraccessexists.
Therefore,weadoptthetwo-stagetrainingwithproxycodes,whichprovestobeeffectiveandcan
outperformtheMaskGIT-VQGAN(1.70vs.2.28rFID).Itisnoteworthythatthetwo-stagetraining
isnotthatcrucialtoobtainareasonable1Dtokenizer,andwebelievethatTiTok,withthesimple
single-stageTaming-VQGAN’strainingsetting,couldalsobenefitfromtrainingonalagrer-scale
dataset[35]asdemonstratedin[55]andweleaveitforfutureworkduetothelimitedcompute.
5 Conclusion
Inthispaper,wehaveexploredacompact1DtokenizationTiTokforreconstructingandgenerating
natural images. Unlike the existing 2D VQ models that consider the image latent space as a 2D
grid,weprovideamorecompactformulationtotokenizeanimageintoa1Dlatentsequence. The
proposedTiTokcanrepresentanimagewith8to64timesfewertokensthanthecommonlyused2D
tokenizers. Moreover,thecompact1Dtokensnotonlysignificantlyimprovethegenerationmodel’s
trainingandinferencethroughput,butalsoachieveacompetitiveFIDontheImageNetbenchmarks.
Wehopeourresearchcanshedsomelightinthedirectiontowardsmoreefficientimagerepresentation
andgenerationmodelswith1Dimagetokenization.
10References
[1] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,KarelLenc,
ArthurMensch,KatherineMillican,MalcolmReynolds,etal. Flamingo: avisuallanguagemodelfor
few-shotlearning. NeurIPS,2022.
[2] StanislawAntol,AishwaryaAgrawal,JiasenLu,MargaretMitchell,DhruvBatra,CLawrenceZitnick,
andDeviParikh. Vqa:Visualquestionanswering. InICCV,2015.
[3] YutongBai,XinyangGeng,KarttikeyaMangalam,AmirBar,AlanYuille,TrevorDarrell,JitendraMalik,
andAlexeiAEfros. Sequentialmodelingenablesscalablelearningforlargevisionmodels. InCVPR,
2024.
[4] FanBao,ShenNie,KaiwenXue,YueCao,ChongxuanLi,HangSu,andJunZhu. Allareworthwords:A
vitbackbonefordiffusionmodels. InCVPR,2023.
[5] HangboBao,LiDong,SonghaoPiao,andFuruWei. Beit: Bertpre-trainingofimagetransformers. In
ICLR,2022.
[6] VictorBesnierandMickaelChen. Apytorchreproductionofmaskedgenerativeimagetransformer. arXiv
preprintarXiv:2310.14400,2023.
[7] ShiyueCao,YueqinYin,LianghuaHuang,YuLiu,XinZhao,DeliZhao,andKaigiHuang.Efficient-vqgan:
Towardshigh-resolutionimagegenerationwithefficientvisiontransformers. InICCV,2023.
[8] NicolasCarion,FranciscoMassa,GabrielSynnaeve,NicolasUsunier,AlexanderKirillov,andSergey
Zagoruyko. End-to-endobjectdetectionwithtransformers. InECCV,2020.
[9] HuiwenChang,HanZhang,LuJiang,CeLiu,andWilliamTFreeman.Maskgit:Maskedgenerativeimage
transformer. InCVPR,2022.
[10] HuiwenChang,HanZhang,JarredBarber,AJMaschinot,JoseLezama,LuJiang,Ming-HsuanYang,
KevinMurphy,WilliamTFreeman,MichaelRubinstein,etal.Muse:Text-to-imagegenerationviamasked
generativetransformers. InICML,2023.
[11] JiennegChen,QihangYu,XiaohuiShen,AlanYuille,andLiang-ChiehChen. Vitamin:Designingscalable
visionmodelsinthevision-languageera. arXivpreprintarXiv:2404.02132,2024.
[12] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.
Generativepretrainingfrompixels. InICML,2020.
[13] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and
CLawrenceZitnick. Microsoftcococaptions: Datacollectionandevaluationserver. arXivpreprint
arXiv:1504.00325,2015.
[14] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.Imagenet:Alarge-scalehierarchical
imagedatabase. InCVPR,2009.
[15] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert:Pre-trainingofdeepbidirec-
tionaltransformersforlanguageunderstanding. InNAACL,2018.
[16] PrafullaDhariwalandAlexanderNichol. Diffusionmodelsbeatgansonimagesynthesis. NeurIPS,2021.
[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,JakobUszkoreit,and
NeilHoulsby. Animageisworth16x16words: Transformersforimagerecognitionatscale. InICLR,
2021.
[18] AlaaeldinEl-Nouby,MichalKlein,ShuangfeiZhai,MiguelAngelBautista,AlexanderToshev,Vaishaal
Shankar,JoshuaMSusskind,andArmandJoulin. Scalablepre-trainingoflargeautoregressiveimage
models. arXivpreprintarXiv:2401.08541,2024.
[19] PatrickEsser,RobinRombach,andBjornOmmer.Tamingtransformersforhigh-resolutionimagesynthesis.
InCVPR,2021.
[20] OranGafni,AdamPolyak,OronAshual,ShellySheynin,DeviParikh,andYanivTaigman. Make-a-scene:
Scene-basedtext-to-imagegenerationwithhumanpriors. InECCV,2022.
[21] ShanghuaGao,PanZhou,Ming-MingCheng,andShuichengYan. Maskeddiffusiontransformerisa
strongimagesynthesizer. InICCV,2023.
11[22] YuyingGe,SijieZhao,ZiyunZeng,YixiaoGe,ChenLi,XintaoWang,andYingShan. Makingllamasee
anddrawwithseedtokenizer. InICLR,2024.
[23] IanGoodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,SherjilOzair,Aaron
Courville,andYoshuaBengio. Generativeadversarialnets. NeurIPS,2014.
[24] KaimingHe,XinleiChen,SainingXie,YanghaoLi,PiotrDollár,andRossGirshick.Maskedautoencoders
arescalablevisionlearners. InCVPR,2022.
[25] MartinHeusel,HubertRamsauer,ThomasUnterthiner,BernhardNessler,andSeppHochreiter. Gans
trainedbyatwotime-scaleupdateruleconvergetoalocalnashequilibrium. NeurIPS,2017.
[26] GeoffreyHinton, OriolVinyals, andJeffDean. Distillingtheknowledgeinaneuralnetwork. arXiv
preprintarXiv:1503.02531,2015.
[27] GeoffreyEHintonandRuslanRSalakhutdinov. Reducingthedimensionalityofdatawithneuralnetworks.
science,313(5786),2006.
[28] JonathanHoandTimSalimans. Classifier-freediffusionguidance. arXivpreprintarXiv:2207.12598,
2022.
[29] EmielHoogeboom,JonathanHeek,andTimSalimans. simplediffusion:End-to-enddiffusionforhigh
resolutionimages. InICML,2023.
[30] ZhichengHuang,ZhaoyangZeng,BeiLiu,DongmeiFu,andJianlongFu. Pixel-bert: Aligningimage
pixelswithtextbydeepmulti-modaltransformers. arXivpreprintarXiv:2004.00849,2020.
[31] SergeyIoffeandChristianSzegedy. Batchnormalization:Acceleratingdeepnetworktrainingbyreducing
internalcovariateshift. InICML,2015.
[32] YangJin,KunXu,LiweiChen,ChaoLiao,JianchaoTan,BinChen,ChenyiLei,AnLiu,ChengruSong,
XiaoqiangLei,etal. Unifiedlanguage-visionpretrainingwithdynamicdiscretevisualtokenization. In
ICLR,2024.
[33] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and
super-resolution. InECCV,2016.
[34] DiederikPKingmaandMaxWelling. Auto-encodingvariationalbayes. InICLR,2014.
[35] AlinaKuznetsova, HassanRom, NeilAlldrin, JasperUijlings, IvanKrasin, JordiPont-Tuset, Shahab
Kamali,StefanPopov,MatteoMalloci,AlexanderKolesnikov,etal. Theopenimagesdatasetv4:Unified
imageclassification,objectdetection,andvisualrelationshipdetectionatscale. IJCV,2020.
[36] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image
generationusingresidualquantization. InCVPR,2022.
[37] DoyupLee,ChiheonKim,SaehoonKim,MinsuCho,andWOOKSHINHAN.Draft-and-revise:Effective
imagegenerationwithcontextualrq-transformer. NeurIPS,2022.
[38] JoséLezama,HuiwenChang,LuJiang,andIrfanEssa. Improvedmaskedimagegenerationwithtoken-
critic. InECCV,2022.
[39] JoséLezama,TimSalimans,LuJiang,HuiwenChang,JonathanHo,andIrfanEssa. Predictor-corrector
samplingfordiscretediffusionmodels. InICLR,2023.
[40] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi.Blip-2:Bootstrappinglanguage-imagepre-training
withfrozenimageencodersandlargelanguagemodels. InICML,2023.
[41] YanghaoLi,HaoqiFan,RonghangHu,ChristophFeichtenhofer,andKaimingHe.Scalinglanguage-image
pre-trainingviamasking. InCVPR,2023.
[42] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. NeurIPS,2023.
[43] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,ZhengZhang,StephenLin,andBainingGuo. Swin
transformer:Hierarchicalvisiontransformerusingshiftedwindows. InICCV,2021.
[44] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101,2017.
12[45] FabianMentzer,DavidMinnen,EirikurAgustsson,andMichaelTschannen. Finitescalarquantization:
VQ-VAEmadesimple. InICLR,2024.
[46] DavidMizrahi,RomanBachmann,OguzhanKar,TeresaYeo,MingfeiGao,AfshinDehghan,andAmir
Zamir. 4m:Massivelymultimodalmaskedmodeling. NeurIPS,2023.
[47] AaronvandenOord,OriolVinyals,andKorayKavukcuoglu. Neuraldiscreterepresentationlearning.
NeurIPS,2017.
[48] OpenAI. Gpt-4technicalreport. arXivpreprintarXiv:2303.08774,2023.
[49] WilliamPeeblesandSainingXie. Scalablediffusionmodelswithtransformers. InCVPR,2023.
[50] DustinPodell,ZionEnglish,KyleLacey,AndreasBlattmann,TimDockhorn,JonasMüller,JoePenna,
andRobinRombach. Sdxl:Improvinglatentdiffusionmodelsforhigh-resolutionimagesynthesis. arXiv
preprintarXiv:2307.01952,2023.
[51] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,GretchenKrueger,andIlyaSutskever. Learning
transferablevisualmodelsfromnaturallanguagesupervision. InICML,2021.
[52] AdityaRamesh,MikhailPavlov,GabrielGoh,ScottGray,ChelseaVoss,AlecRadford,MarkChen,and
IlyaSutskever. Zero-shottext-to-imagegeneration. InICML,2021.
[53] AliRazavi,AaronVandenOord,andOriolVinyals.Generatingdiversehigh-fidelityimageswithvq-vae-2.
NeurIPS,2019.
[54] JasonTylerRolfe. Discretevariationalautoencoders. InICLR,2017.
[55] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-resolution
imagesynthesiswithlatentdiffusionmodels. InCVPR,2022.
[56] ChristophSchuhmann,RomainBeaumont,RichardVencu,CadeGordon,RossWightman,MehdiCherti,
TheoCoombes,AarushKatta,ClaytonMullis,MitchellWortsman,etal. Laion-5b:Anopenlarge-scale
datasetfortrainingnextgenerationimage-textmodels. NeurIPS,2022.
[57] stabilityai,2023. URLhttps://huggingface.co/stabilityai/sd-vae-ft-ema.
[58] QuanSun,QiyingYu,YufengCui,FanZhang,XiaosongZhang,YuezeWang,HongchengGao,Jingjing
Liu,TiejunHuang,andXinlongWang. Generativepretraininginmultimodality. InICLR,2024.
[59] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov, SoumyaBatra, PrajjwalBhargava, ShrutiBhosale, etal. Llama2: Openfoundationand
fine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
[60] AaronVandenOord,NalKalchbrenner,LasseEspeholt,OriolVinyals,AlexGraves,etal. Conditional
imagegenerationwithpixelcnndecoders. NeurIPS,2016.
[61] AaronVanDenOord,OriolVinyals,etal. Neuraldiscreterepresentationlearning. NeurIPS,2017.
[62] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,Łukasz
Kaiser,andIlliaPolosukhin. Attentionisallyouneed. NeurIPS,2017.
[63] PascalVincent,HugoLarochelle,YoshuaBengio,andPierre-AntoineManzagol.Extractingandcomposing
robustfeatureswithdenoisingautoencoders. InProceedingsofthe25thinternationalconferenceon
Machinelearning,pages1096–1103,2008.
[64] HuiyuWang,YukunZhu,HartwigAdam,AlanYuille,andLiang-ChiehChen. Max-deeplab:End-to-end
panopticsegmentationwithmasktransformers. InCVPR,2021.
[65] JiahuiYu,XinLi,JingYuKoh,HanZhang,RuomingPang,JamesQin,AlexanderKu,YuanzhongXu,
JasonBaldridge,andYonghuiWu. Vector-quantizedimagemodelingwithimprovedvqgan. InICLR,
2022.
[66] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,
AlexanderKu,YinfeiYang,BurcuKaragolAyan,etal. Scalingautoregressivemodelsforcontent-rich
text-to-imagegeneration. TMLR,2022.
13[67] LijunYu,YongCheng,KihyukSohn,JoséLezama,HanZhang,HuiwenChang,AlexanderGHauptmann,
Ming-HsuanYang,YuanHao,IrfanEssa,etal. Magvit:Maskedgenerativevideotransformer. InCVPR,
2023.
[68] LijunYu,JoséLezama,NiteshBGundavarapu,LucaVersari,KihyukSohn,DavidMinnen,YongCheng,
AgrimGupta,XiuyeGu,AlexanderGHauptmann,etal. Languagemodelbeatsdiffusion–tokenizeriskey
tovisualgeneration. InICLR,2024.
[69] LiliYu,BowenShi,RamakanthPasunuru,BenjaminMuller,OlgaGolovneva,TianluWang,ArunBabu,
BinhTang,BrianKarrer,ShellySheynin,etal. Scalingautoregressivemulti-modalmodels:Pretraining
andinstructiontuning. arXivpreprintarXiv:2309.02591,2023.
[70] QihangYu,HuiyuWang,DahunKim,SiyuanQiao,MaxwellCollins,YukunZhu,HartwigAdam,Alan
Yuille,andLiang-ChiehChen. Cmt-deeplab:Clusteringmasktransformersforpanopticsegmentation. In
CVPR,2022.
[71] QihangYu,HuiyuWang,SiyuanQiao,MaxwellCollins,YukunZhu,HartwigAdam,AlanYuille,and
Liang-ChiehChen. k-meansMaskTransformer. InECCV,2022.
[72] QihangYu,JuHe,XueqingDeng,XiaohuiShen,andLiang-ChiehChen. Convolutionsdiehard:Open-
vocabularysegmentationwithsinglefrozenconvolutionalclip. NeurIPS,2023.
[73] QihangYu,XiaohuiShen,andLiang-ChiehChen. Towardsopen-endedvisualrecognitionwithlarge
languagemodel. arXivpreprintarXiv:2311.08400,2023.
[74] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-Yeung
Shum. Dino:Detrwithimproveddenoisinganchorboxesforend-to-endobjectdetection. arXivpreprint
arXiv:2203.03605,2022.
[75] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable
effectivenessofdeepfeaturesasaperceptualmetric. InCVPR,2018.
[76] ChuanxiaZheng,Tung-LongVuong,JianfeiCai,andDinhPhung. Movq:Modulatingquantizedvectors
forhigh-fidelityimagegeneration. NeurIPS,2022.
[77] XizhouZhu,WeijieSu,LeweiLu,BinLi,XiaogangWang,andJifengDai. Deformabledetr:Deformable
transformersforend-to-endobjectdetection. InICLR,2020.
14Appendix
Inthesupplementarymaterials,weprovidethefollowingadditionaldetails:
• The comprehensive training and testing hyper-parameters and training costs for TiTok
(Sec.A).
• Thedetailedresultsofthepreliminaryexperimentsreportedinmainpaper’sFig.4(Sec.B).
• Amorecomprehensivecomparisonwithmoremetricsandbaselines(Sec.C).
• Qualitativevisualizations(Sec.D).
• Limitationdiscussion(Sec.E).
• BroaderImpactsdiscussion(Sec.F).
• DatasetLicenses(Sec.G).
A TrainingandTestingProtocols
Forimagereconstruction(tokenizer)atpreliminaryexperiments, thetrainingaugmentationis
confined to random cropping and flipping, following [19]. The training regimen spans a short
schedule,featuringabatchsizeof256over500ktrainingiterations,whichcorrelatestoroughly100
epochsontheImageNetdataset. WeemploytheAdamWoptimizer[44]withaninitiallearningrate
of1×10−4 (withcosinedecay)andweightdecay1×10−4. Weonlyadoptstage-1traininghere
(i.e.,onlythe“warm-up”trainingstage). Forthemainexperiments,weadopttheimprovements
asshownintheablationstudy(Tab. 3inmainpaper),includinglongertrainingto200epochsand
decoderfine-tuning,allotherhyper-parametersremainthesame. Weusepatchsize16forallvision
transformers at resolution 256×256 and increase it to 32 for resolution 512×512 to ensure a
computationefficiency.
TiTok-LreferstousingaViT-LforTiTokencoderanddecoder,andTiTok-B,TiTok-Sreferstousing
ViT-BandViT-Srespectively. Moreover, thetokenizertrainingtakes64A100-40Gfor74hours
(TiTok-L-32),32A100-40Gfor41hours(TiTok-B-64),32A100-40Gfor50hours(TiTok-S-128),32
A100-40Gfor70hours(TiTok-B-128forresolution512),and64A100-40Gfor91hours(TiTok-L-64
forresolution512),respectively.
Forimagegeneration(generator)atpreliminaryexperiments,wemajorlybuildthetrainingand
testing protocols on top of [9]. Specifically, all images are pre-tokenized using center crop and
random flipping augmentation, and then processed by MaskGIT [9] to generate images via the
maskedimagemodelingprocedure. Duringinference,acosinemaskingscheduleisutilizedwith8
steps. Thegenerativemodelsaretrainedwithabatchsizeof2048and500kiterationstoimprove
trainingefficiency. WeuseAdamWoptimizer[44]withlearningrate2×10−4 andweightdecay
0.03. Thelearningratestartsfrom2×10−4andthendecayto1×10−5followingacosinedecaying
schedule. We apply a dropout probability of 0.1 on the class condition. The only differences of
mainexperimentsareusinganarccosmaskingscheduleasdiscussedintheablationstudy(main
paperTab.3),allotherhyper-parametersremainthesame. Wefollowpriorarts[19,9]togenerate
50ksamplesforgenerationFIDevaluation. Wealsoadoptclassifier-freeguidance[28]following
priorarts[10,68].
AtImageNet256×256,weuseguidancescale4.5,temperature9.5forTiTok-L-32,guidancescale
3.0, temperature 11.0 for TiTok-B-64, guidance scale 2.0, temperature 3.0 for TiTok-S-128. At
ImageNet512×512,weuseguidancescale2.0,temperature7.5forTiTok-L-64,guidancescale2.5,
temperature6.5forTiTok-B-128.
Thegeneratortrainingtakes32A100-40Gfor12hours(TiTok-L-32),16hours(TiTok-B-64),29
hours(TiTok-S-128),26hours(TiTok-B-128forresolution512),18hours(TiTok-L-64forresolution
512)respectively.
B DetailedResultsofPreliminaryExperiments
WesummarizethedetailedresultsforFig.4ofmainpaperinTab.4.
15Table4: Detailedresultsofpreliminaryexperimentsinmainpaper.
(a)reconstructionFID. (b)generationFID.
#token 16 32 64 96 128 192 256 #token 16 32 64 96 128 192 256
TiTok-S 25.3 16.2 9.6 6.9 5.6 4.3 3.7 TiTok-S 16.1 8.7 4.4 3.6 3.5 3.0 3.2
TiTok-B 16.8 9.7 5.9 4.7 3.8 3.2 2.9 TiTok-B 10.9 7.5 4.1 3.4 3.2 3.1 3.5
TiTok-L 13.0 6.6 4.0 3.4 3.0 2.6 2.5 TiTok-L 9.0 6.3 4.3 3.9 3.7 3.7 3.7
(c)LinearProbingaccuracy.
#token 16 32 64 96 128 192 256
TiTok-S 50.46 48.01 48.40 48.12 46.55 47.05 44.88
TiTok-B 57.70 54.79 53.92 53.72 53.59 51.75 52.11
TiTok-L 62.10 60.03 58.85 56.12 54.35 53.95 54.36
Table5: ImageNet-1K256×256generationresultsevaluatedwithADM[16]. †:Trainedon
OpenImages [35] ‡: Trained on OpenImages, LAION-Aesthetics/-Humans [56]. P: generator’s
parameters. S:samplingsteps. T:throughputassamplespersecondsonA100withfloat32precision,
measuredwithw/guidancevariantsifavailable. “guidance"referstoclassifier-freeguidance.
w/oguidance w/guidance
tokenizer rFID↓ generator P↓ S↓ T↑
gFID↓ IS↑ gFID↓ IS↑
diffusion-basedgenerativemodels
Taming-VQGAN†[55] 1.14 LDM-8[55] 15.82 78.82 7.76 209.5 258M 200 -
VAE†[55] 0.27 LDM-4[55] 10.56 103.5 3.60 247.7 400M 250 0.4
UViT-L/2[4] 9.03 111.5 3.40 219.9 287M 50 1.1
VAE[57]‡ 0.62 UViT-H/2[4] 6.60 142.5 2.29 263.9 501M 50 0.6
DiT-XL/2[49] 9.62 121.5 2.27 278.2 675M 250 0.6
transformer-basedgenerativemodels
Taming-VQGAN[19] 7.94 Taming-Transformer[19] 15.78 78.3 - - 1.4B 256 7.5
8.71 119.0 - - 1.4B 16.1
RQ-VAE[36] 3.20 RQ-Transformer[36] 64
7.55 134.0 - - 3.8B 9.7
MaskGIT-VQGAN[9] 2.28 MaskGIT-ViT[9] 6.18 182.1 - - 177M 8 50.5
ViT-VQGAN[65] 1.28 VIM-Large[65] 4.17 175.1 - - 1.7B 1024 0.3
LFQ[68] ∼0.9 MAGVIT-v2[68] 3.65 200.5 1.78 319.4 307M 64 1.1
TiTok-L-32 2.21 MaskGIT-ViT[9] 3.15 173.0 2.77 199.8177M 8 101.6
TiTok-B-64 1.70 MaskGIT-ViT[9] 3.08 192.5 2.48 214.7177M 8 89.8
4.61 166.7 2.50 278.7 8 53.3
TiTok-S-128 1.71 MaskGIT-UViT-L[9,4] 287M
4.44 168.2 1.97 281.8 64 7.8
C AdditionalResults
Wefurtherreporttheclass-conditionalgenerationresultscomparisonwithmoremetricsandbaselines
inTab.5andTab.6forImageNet256×256and512×512generationbenchmarks,respectively.
Moreover,wereportbothresultswithoutandwithclassifier-freeguidance[28]undercolumn“w/o
guidance"and“w/guidance"respectively.
AsshowninTab.5, bothTiTok-L-32andTiTok-B-64setanewstate-of-the-artperformancefor
resultswithoutclassifier-freeguidance(i.e.,w/oguidancecolumn),whilegeneratingimagesata
muchfasterpace. Specifically,TiTok-L-32achieves3.15gFID,surpassingcurrentstate-of-the-art
MAGVIT-v2[68]’sgFID3.65,whilerequiringmuchfewersamplingsteps(8vs.64)andsmaller
modelsize(177M vs.307M),leadingtoasubstantialsamplingspeed-up(92.4×faster,101.6vs.
1.1samples/sec). Additionally,whencomparedtoMaskGIT[9],whichusestheexactsamegenerator
model(i.e., MaskGIT-ViT)asoursandtheonlydifferenceisthetoeknizer, TiTok-L-32achieves
significantlyabetterperformance(3.15vs.6.18). Theimprovementdemonstratestheefficiencyand
effectivenessofthelearnedcompact1Dlatentspaceforimagerepresentation. Whenitcomesto
resolution512×512inTab.6,MaskGIT[9]requires1024tokensforimagelatentrepresentation,
whileTiTok-L-64requires16×fewer. Asaresult,whenusingthesamegenerator(i.e.,MaskGIT-
ViT),TiTok-L-64,w/oclassifier-freeguidance,notonlysignificantlyoutperformsMaskGITinterms
ofgFID(3.64vs.7.32)butalsogeneratessamplesmuchfaster. TheadvantagesofTiTokbecome
16Table6: ImageNet-1K512×512generationresultsevaluatedwithADM[16]. †:Trainedon
OpenImages [35] ‡: Trained on OpenImages, LAION-Aesthetics/-Humans [56]. P: generator’s
parameters. S:samplingsteps. T:throughputassamplespersecondsonA100withfloat32precision,
measuredwithw/guidancevariantsifavailable. “guidance"referstoclassifier-freeguidance.
w/oguidance w/guidance
tokenizer rFID↓ generator P↓ S↓ T↑
gFID↓ IS↑ gFID↓ IS↑
diffusion-basedgenerativemodels
UViT-L/4[4] 18.03 76.9 4.67 213.3 287M 50 1.0
VAE[57]‡ 0.19 UViT-H/4[4] 15.71 101.3 4.05 263.8 501M 50 0.6
DiT-XL/2[49] 12.03 105.3 3.04 240.8 675M 250 0.1
transformer-basedgenerativemodels
MaskGIT-VQGAN[9] 1.97 MaskGIT-ViT[9] 7.32 156.0 - - 177M 12 3.9
4.61 192.4 - - 307M 12 3.5
LFQ[68] 1.22 MAGVIT-v2[68]
3.07 213.1 1.91 324.3 307M 64 1.0
TiTok-L-64 1.78 MaskGIT-ViT[9] 3.64 179.8 2.74 221.1177M 8 41.0
3.91 182.0 2.49 260.4 8 33.3
TiTok-B-128 1.37 MaskGIT-ViT[9] 177M
4.17 181.0 2.13 261.2 64 7.4
evenmoresignificantwhencomparedtothediffusionmodelssuchasDiT-XL/2[49]withguidance:
TiTok-L-64notonlyshowsasuperiorperformance(2.74vs.3.04),butalsoenjoysadramatically
highergenerationthroughput(410×).
An interesting observation is that under w/o guidance case, TiTok-L-32 (for 256 resolution) and
TiTok-L-64(for512resolution)canoutperformmostothermethods,includingTiTok-S-128and
TiTok-B-128, yettheybenefitrelativelylessfromtheclassifier-freeguidanceinthew/guidance
column. WenoteitindicatesthatthegreatpotentialofTiTokatcompactlatentsizeisstillnotfully
unleashedyet,andbetteradaptationofinferencetimeimprovementsfor1Dcompacttokens,suchas
classifier-freeguidance,whichwasdesignedformethodswithmuchmoretokensandsteps,couldbe
apromisingfuturedirection.
D Visualizations
WeprovidevisualizationofthegeneratedimagesusingTiTokinFig.5andFig.6. Moreover,we
visualize the reconstruction results under different numbers of tokens and different model sizes
inFig.7,whereweobservethatthemodeltendstokeepthehigh-levellayoutorsalientobjectswhen
thelatentrepresentationsizeislimited. Besides,alargermodelsizereconstructsanimagewithmore
detailsunderacompactlatentspacesize,demonstratinganeffectivewaytowardsamorecompact
latentspace.
E Limitations
Thispaperproposesanovel1Dtokenizationmethoddesignedtoeliminatethefixedcorresponding
constraints of existing 2D tokenization methods. The 1D tokenization model is validated using
the Vector Quantization (VQ) tokenizer formulation alongside a Masked Transformer generator
framework. Despitethepromisingresults,theproposed1Dtokenizationformulationtheoretically
has the potential to generalize to other tokenizer formulations (e.g., 1D-VAE), other generation
frameworks (e.g., Diffusion Models), and beyond the image modality (e.g., video). However,
exploringtheseextensionsisbeyondthescopeofthispaperduetolimitedcomputationalresources,
andweleavetheseaspromisingdirectionsforfutureresearch.
F BroaderImpacts
Generativemodelshavenumerousapplicationswithdiversepotentialsocialimpacts. Whilethese
modelssignificantlyenhancehumancreativity,theycanalsobemisusedformisinformation,harass-
ment,andperpetuatingsocialandculturalbiases. Similartootherdeeplearningmethods,generative
modelscanbeheavilyinfluencedbydatasetbiases,leadingtothereinforcementofnegativesocial
stereotypesandviewpoints. Developingunbiasedmodelsthatensurebothrobustnessandfairness
17macaw lion jack-o'- orange daisy bubble valley
lantern
Figure5: VisualizationofgeneratedimagesfromTiTokvariantswithMaskGIT[9]. Correspond-
ingImageNetclassnamesareshownbelowtheimages.
Figure6: VisualizationofgeneratedimagesfromTiTok-L-32withMaskGIT[9]acrossrandom
ImageNetclasses.
isacriticalareaofresearch. However, addressingtheseissuesisbeyondthescopeofthispaper.
Consideringthepotentialrisks,thispaperislimitedtoclass-conditionalgenerationusingafixed,
public,andcontrolledsetofclasses.
18
23-L-koTiT
46-B-koTiT
821-S-koTiTIncreasing number of latent token (K=16, 32, 64, 128, 256)
Figure7: Visualcomparisonofreconstructionresults. Scalingmodelsizeenablesabetterimage
qualitywhileusingamorecompactlatentspacesize. ItisalsoobservedthatTiToktendstokeepthe
salientregionswhenlatentspaceislimited.
G DatasetLicenses
Thedatasetsweusedfortrainingand/ortestingTiTokaredescribedasfollows.
ImageNet-1K: WetrainandevaluateTiTokonImageNet-1Kgenerationbenchmark. Thisdataset
spans1000objectclassesandcontains1,281,167trainingimages, 50,000validationimagesand
100,000testimages. Weusethetrainingsetforourtokenizerandgeneratortraining. Thevalidation
19
Increasing
model
size
(S,
B,
L)set is used to compute reconstruction FID for evaluating tokenizers. The generation results are
evaluatedwithgenerationFIDusingpre-computedstatisticsandscriptsfromADM[16]3.
License: https://image-net.org/accessagreement
URL:https://www.image-net.org/
3https://github.com/openai/guided-diffusion/tree/main/evaluations
20