Ctrl-X: Controlling Structure and Appearance for
Text-To-Image Generation Without Guidance
KuanHengLin1* SichengMo1* BenKlingher1 FangzhouMu2 BoleiZhou1
1UniversityofCalifornia,LosAngeles 2NVIDIA
Structure Structure
Figure1: Guidance-freestructureandappearancecontrolofStableDiffusionXL(SDXL)[27]
Ctrl-Xenablestraining-freeandguidance-freezero-shotcontrolofpretrainedtext-to-imagediffusion
modelsgivenanystructureconditionsandappearanceimages.
Abstract
RecentcontrollablegenerationapproachessuchasFreeControl[24]andDiffusion
Self-guidance [7] bring fine-grained spatial and appearance control to text-to-
image(T2I)diffusionmodelswithouttrainingauxiliarymodules. However,these
methodsoptimizethelatentembeddingforeachtypeofscorefunctionwithlonger
diffusionsteps,makingthegenerationprocesstime-consumingandlimitingtheir
flexibilityanduse.ThisworkpresentsCtrl-X,asimpleframeworkforT2Idiffusion
controllingstructureandappearancewithoutadditionaltrainingorguidance.Ctrl-X
designs feed-forward structure control to enable the structure alignment with a
structureimageandsemantic-awareappearancetransfertofacilitatetheappearance
transferfromauser-inputimage.Extensivequalitativeandquantitativeexperiments
illustratethesuperiorperformanceofCtrl-Xonvariousconditioninputsandmodel
checkpoints. Inparticular,Ctrl-Xsupportsnovelstructureandappearancecontrol
witharbitraryconditionimagesofanymodality,exhibitssuperiorimagequalityand
appearancetransfercomparedtoexistingworks,andprovidesinstantplug-and-play
functionalitytoanyT2Iandtext-to-video(T2V)diffusionmodel. Seeourproject
pageforanoverviewoftheresults: https://genforce.github.io/ctrl-x.
*Equalcontribution.
Preprint.Underreview.
4202
nuJ
11
]VC.sc[
1v04570.6042:viXra
ecnaraeppA ecnaraeppA1 Introduction
Therapidadvancementoflargetext-to-image(T2I)generativemodelshasmadeitpossibletogenerate
high-qualityimageswithjustonetextprompt. However,itremainschallengingtospecifytheexact
conceptsthatcanaccuratelyreflecthumanintentsusingonlytextualdescriptions. Recentapproaches
likeControlNet[44]andIP-Adapter[43]haveenabledcontrollableimagegenerationuponpretrained
T2Idiffusionmodelsregardingstructureandappearance,respectively. Despitetheimpressiveresults
incontrollablegeneration,theseapproaches[44,25,46,20]requirefine-tuningtheentiregenerative
modelortrainingauxiliarymodulesonlargeamountsofpaireddata.
Training-free approaches [7, 24, 4] have been proposed to address the high overhead associated
withadditionaltrainingstages. Thesemethodsoptimizethelatentembeddingacrossdiffusionsteps
using specially designed score functions to achieve finer-grained control than text alone with a
processcalledguidance. Althoughtraining-freeapproachesavoidthetrainingcost,theysignificantly
increase computing time and required GPU memory in the inference stage due to the additional
backpropagationoverthediffusionnetwork. Theyalsorequiresamplingstepsthatare2–20times
longer. Furthermore, as the expected latent distribution of each time step is predefined for each
diffusionmodel,itiscriticaltotunetheguidanceweightdelicatelyforeachscorefunction;Otherwise,
thelatentmightbeout-of-distributionandleadtoartifactsandreducedimagequality.
Totackletheselimitations,wepresentCtrl-X,asimpletraining-freeandguidance-freeframework
for T2I diffusion with structure and appearance control. We name our method “Ctrl-X” because
wereformulatethecontrollablegenerationproblemby‘cutting’(and‘pasting’)twotaskstogether:
Spatialstructurepreservationandsemantic-awarestylization. Ourinsightisthatdiffusionfeature
mapscapturerichspatialstructureandhigh-levelappearancefromearlydiffusionstepssufficientfor
structureandappearancecontrolwithoutguidance. Tothisend,Ctrl-Xemploysfeatureinjectionand
spatially-awarenormalizationintheattentionlayerstofacilitatestructureandappearancealignment
with user-provided images. By being guidance-free, Ctrl-X eliminates additional optimization
overheadandsamplingsteps,resultingina40-foldincreaseininferencespeedcomparedtoguidance-
based methods. Figure 1 shows some generation results. Moreover, Ctrl-X supports arbitrary
structureconditionsbeyondnaturalimagesandcanbeappliedtoanyT2Iandeventext-to-video
(T2V)diffusionmodels. Extensivequantitativeandqualitativeexperimentsdemonstratethesuperior
imagequalityandappearancealignmentofourmethodoverpriorworks.
Wesummarizeourcontributionsasfollows:
1. WepresentCtrl-X,asimpleplug-and-playmethodthatbuildsonpretrainedtext-to-imagediffusion
models to provide disentangled and zero-shot control of structure and appearance during the
generationprocessrequiringnoadditionaltrainingorguidance.
2. Ctrl-Xpresentsthefirstuniversalguidance-freesolutionthatsupportsmultipleconditionalsignals
(structureandappearance)andmodelarchitectures(e.g.text-to-imageandtext-to-video).
3. Ourmethoddemonstratessuperiorresultsincomparisontoprevioustraining-basedandguidance-
basedbaselines(e.g.ControlNet+IP-Adapter[44,43]andFreeControl[24])intermsofcondition
alignment,text-imagealignment,andimagequality.
2 Relatedwork
Diffusionstructurecontrol. Previousspatialstructurecontrolmethodscanbecategorizedinto
twotypes(training-basedvs.training-free)basedonwhethertheyrequiretrainingonpaireddata.
Training-based structure control methods require paired condition-image data to train additional
modulesorfine-tunetheentirediffusionnetworktofacilitategenerationfromspatialconditions[44,
25,20,46,42,3,47,38,49]. Whilepixel-levelspatialcontrolcanbeachievedwiththisapproach,a
significantdrawbackisneedingalargenumberofcondition-imagepairsastrainingdata. Although
someconditiondatacanbegeneratedfrompretrainedannotators(e.g.depthandsegmentationmaps),
otherconditiondataisdifficulttoobtainfromgivenimages(e.g.3Dmesh,pointcloud),making
theseconditionschallengingtofollow. Comparedtothesetraining-basedmethods,Ctrl-Xsupports
conditionswherepaireddataischallengingtoobtain,makingitamoreflexibleandeffectivesolution.
Training-freestructurecontrolmethodstypicallyfocusonspecificconditions.Forexample,R&B[40]
facilitatesbounding-boxguidedcontrolwithregion-awareguidance,andDenseDiffusion[17]gen-
2erates images with sparse segmentation map conditions by manipulating the attention weights.
UniversalGuidance[4]employsvariouspretrainedclassifierstosupportmultipletypesofcondition
signals. FreeControl[24]analyzessemanticcorrespondenceinthesubspaceofdiffusionfeatures
andharnessesittosupportspatialcontrolfromanyvisualcondition. Whiletheseapproachesdonot
requiretrainingdata,theyusuallyneedtocomputethegradientofthelatenttoloweranauxiliary
loss,whichrequiressubstantialcomputingtimeandGPUmemory. Incontrast,Ctrl-Xrequiresno
guidanceattheinferencestageandcontrolsstructureviadirectfeatureinjections,enablingfasterand
morerobustimagegenerationwithspatialcontrol.
Diffusionappearancecontrol. Existingappearancecontrolmethodsthatbuilduponpretrained
diffusionmodelscanalsosimilarlybecategorizedintotwotypes(training-basedvs.training-free).
Training-basedappearancecontrolmethodscanbedividedintotwocategories: Thosetrainedto
handleanyimagepromptandthoseoverfittingtoasingleinstance. Thefirstcategory[44,25,43,38]
trains additional image encoders or adapters to align the generated process with the structure or
appearancefromthereferenceimage. Thesecondcategory[30,14,8,2,26,31]istypicallyapplied
tocustomizedvisualcontentcreationbyfinetuningapretrainedtext-to-imagemodelonasmallsetof
imagesorbindingspecialtokenstoeachinstance. Themainlimitationofthesemethodsisthatthe
additionaltrainingrequiredmakesthemunscalable. However,Ctrl-Xoffersascalablesolutionto
transferappearancefromanyinstancewithouttrainingdata.
Training-freeappearancecontrolmethodsgenerallyfollowtwoapproaches: Oneapproach[1,5,41]
manipulatesself-attentionfeaturesusingpixel-leveldensecorrespondencebetweenthegenerated
image and thetarget appearance, and the other[7, 24] extracts appearance embeddingsfrom the
diffusionnetworkandtransferstheappearancebyguidingthediffusionprocesstowardsthetarget
appearanceembedding. Akeylimitationoftheseapproachesisthatasingletext-controlledtarget
cannotfullycapturethedetailsofthetargetimage,andthelattermethodsrequireadditionalopti-
mizationsteps. Bycontrast,ourmethodexploitsthespatialcorrespondenceofself-attentionlayersto
achievesemantically-awareappearancetransferwithouttargetingspecificsubjects.
3 Preliminaries
Diffusionmodelsareafamilyofprobabilisticgenerativemodelscharacterizedbytwoprocesses:
TheforwardprocessiterativelyaddsGaussiannoisetoacleanimagex toobtainx fortimestep
0 t
t∼[1,T],whichcanbereparameterizedintermsofanoisescheduleα where
t
√ √
x = α x + 1−α ϵ (1)
t t 0 t
forϵ∼N(0,I);ThebackwardprocessgeneratesimagesbyiterativelydenoisinganinitialGaussian
noise x ∼ N(0,I), also known as diffusion sampling [13]. This process uses a parameterized
T
denoisingnetworkϵ conditionedonatextpromptc,whereattimesteptweobtainacleanerx
θ t−1
√
x =√ α xˆ +(cid:112) 1−α ϵ (x |t,c), xˆ := x t− 1− √α tϵ θ(x t |t,c) . (2)
t−1 t−1 0 t−1 θ t 0 α
t
Formally,ϵ (x | t,c) ≈ −σ ∇ logp (x | t,c)approximatesascorefunctionscaledbyanoise
θ t t x t t
scheduleσ thatpointstowardahighdensityofdata,i.e.,x ,atnoiselevelt[34].
t 0
Guidance. Theiterativeinferenceofdiffusionenablesustoguidethesamplingprocessonauxiliary
information. GuidancemodifiesEquation2tocomposeadditionalscorefunctionsthatpointtoward
richerandspecificallyconditioneddistributions[4,7],expressedas
ϵˆ (x |t,c)=ϵ(x |t,c)−sg(x |t,y), (3)
θ t t t
wheregisanenergyfunctionandsistheguidancestrength. Inpractice,gcanrangefromclassifier-
free guidance (where g = ϵ and y = ∅, i.e. the empty prompt) to improve image quality and
promptadherenceforT2Idiffusion[12,29],toarbitrarygradients∇ ℓ(ϵ(x |t,c)|t,y)computed
xt t
from auxiliary models or diffusion features common to guidance-based controllable generation
[4,7,24]. Thus,guidanceprovidesgreatcustomizabilityonthetypeandvarietyofconditioning
forcontrollablegeneration,asitonlyrequiresanylossthatcanbebackpropagatedtox . However,
t
thisbackpropagationrequirementoftentranslatestoslowinferencetimeandhighmemoryusage.
Moreover,asguidance-basedmethodsoftencomposemultipleenergyfunctions,tuningtheguidance
3(a)Ctrl-Xpipeline (b)Spatially-awareappearancetransfer
Figure2: OverviewofCtrl-X. (a)Ateachsamplingstept,weobtainxsandxaviatheforward
t t
diffusionprocess,thenfeedthemintotheT2Idiffusionmodeltoobtaintheirconvolutionandself-
attentionfeatures. Then,weinjectconvolutionandself-attentionfeaturesfromxsandleverageself-
t
attentioncorrespondencetotransferspatially-awareappearancestatisticsfromxatoxo. (b)Details
t t
ofourspatially-awareappearancetransfer,whereweexploitself-attentioncorrespondencebetween
xoandxatocomputeweightedfeaturestatisticsMandSappliedtoxo.
t t t
strengthsforeachgmaybefinickyandcauseissuesofrobustness. Thus,Ctrl-Xavoidsguidance
andprovidesinstantapplicabilitytolargerT2IandT2Vmodelswithminorhyperparametertuning.
DiffusionU-Netarchitecture. ManypretrainedT2Idiffusionmodelsaretext-conditionedU-Nets,
whichcontainanencoderandadecoderthatdownsampleandthenupsampletheinputx topredictϵ,
t
withlongskipconnectionsbetweenmatchingencoderanddecoderresolutions[13,29,27]. Each
encoder/decoderblockcontainsconvolutionlayers,self-attentionlayers,andcross-attentionlayers:
Thefirsttwocontrolbothstructureandappearance,andthelastinjectstextualinformation. Thus,
manytraining-freecontrollablegenerationmethodsutilizetheselayers,throughdirectmanipulation
[11,36,18,1,41]orforcomputingguidancelosses[7,24],withself-attentionmostcommonlyused:
Leth ∈R(hw)×cbethediffusionfeaturewithheighth,widthw,andchannelsizecattimestept
l,t
rightbeforeattentionlayerl. Then,theself-attentionoperationis
Q:=h WQ and K:=h WK and V:=h WV,
l,t l l,t l l,t l
(cid:18) QK⊤(cid:19) (4)
h ←AV, A:=softmax √ ,
l,t
d
whereWQ,WK,WV ∈Rc×darelineartransformationswhichproducethequeryQ,keyK,and
l l l
valueV,respectively,andsoftmaxisappliedacrossthesecond(hw)-dimension. (Generally,c=d
fordiffusionmodels.) Intuitively,theattentionmapA∈R(hw)×(hw)encodeshoweachpixelinQ
correspondstoeachinK,whichthenrearrangesandweighsV. Thiscorrespondenceisthebasisfor
Ctrl-X’sspatially-awareappearancetransfer.
4 Guidance-freestructureandappearancecontrol
Ctrl-X is a general framework for training-free, guidance-free, and zero-shot T2I diffusion with
structure and appearance control. Given a structure image Is and appearance image Ia, Ctrl-X
manipulates a pretrained T2I diffusion model ϵ to generate an output image Io that inherits the
θ
structureofIsandappearanceofIa.
Methodoverview. OurmethodisillustratedinFigure2andissummarizedasfollows: Givenclean
structureandappearancelatentsIs =xs andIa =xa,wefirstdirectlyobtainnoisedstructureand
0 0
appearancelatentsxsandxaviathediffusionforwardprocess,thenextracttheirU-Netfeaturesfrom
t t
4Figure3: Visualizingearlydiffusionfeatures. Using20real,generated,andconditionimagesof
animals,weextractStableDiffusionXL[27]featuresrightafterdecoderlayer0convolution. We
visualizethetopthreeprincipalcomponentscomputedforeachtimestepacrossallimages. t=961
to881correspondtoinferencesteps1to5oftheDDIMschedulerwith50timesteps. Weobtainx
t
bydirectlyaddingGaussiannoisetoeachcleanimagex viathediffusionforwardprocess.
0
apretrainedT2Idiffusionmodel. Whendenoisingtheoutputlatentxo,weinjectconvolutionand
t
self-attentionfeaturesfromxsandleverageself-attentioncorrespondencetotransferspatially-aware
t
appearancestatisticsfromxatoxotoachievestructureandappearancecontrol.
t t
4.1 Feed-forwardstructurecontrol
StructurecontrolofT2IdiffusionrequirestransferringstructureinformationfromIs = xs toxo,
0 t
especiallyduringearlytimesteps. Tothisend,weinitializexo =xs ∼N(0,I)andobtainxsvia
T T t
thediffusionforwardprocessinEquation1withxs andrandomlysampledϵ∼N(0,I). Inspiredby
0
theobservationwherediffusionfeaturescontainrichlayoutinformation[36,18,24],weperform
featureandself-attentioninjectionasfollows: ForU-Netlayerlanddiffusiontimestept,letfo and
l,t
fs befeatures/activationsaftertheconvolutionblockfromxoandxs,andletAo andAs bethe
l,t t t l,t l,t
attentionmapsoftheself-attentionblockfromxoandxs. Then,wereplace
t t
fo ←fs and Ao ←As . (5)
l,t l,t l,t l,t
Incontrastto[36,18,24],wedonotperforminversionandinsteaddirectlyuseforwarddiffusion
(Equation1)toobtainxs. Weobservethatxs obtainedviatheforwarddiffusionprocesscontains
t t
sufficientstructureinformationevenatveryearly/hightimesteps,asshowninFigure3. Thisalso
reducesappearanceleakagecommontoinversion-basedmethodsobservedbyFreeControl[24]. We
studyourfeed-forwardstructurecontrolmethodinSections5.1and5.3.
Weapplyfeatureinjectionforlayersl ∈Lfeat andself-attentioninjectionforlayersl ∈Lself,and
wedosofor(normalized)timestepst≤τs,whereτs ∈[0,1]isthestructurecontrolschedule.
4.2 Spatially-awareappearancetransfer
Inspiredbypriorworksthatdefineappearanceasfeaturestatistics[15,21],weconsiderappearance
transfertobeastylizationtask. T2Idiffusionself-attentiontransformsthevalueVwithattention
mapA, wherethelatterrepresentshowpixelsinQcorrespondstopixelsinK. Asobservedby
Cross-ImageAttention[1],QK⊤canrepresentthesemanticcorrespondencebetweentwoimages
whenQandKarecomputedfromfeaturesfromeach,evenwhenthetwoimagesdiffersignificantly
instructure. Thus,inspiredbyAdaAttN[21],weproposespatially-awareappearancetransfer,where
weexploitthiscorrespondencetogenerateself-attention-weightedmeanandstandarddeviationmaps
fromxa tonormalizexo: Foranyself-attentionlayerl,letho andha bediffusionfeaturesright
t t l,t l,t
beforeself-attentionforxoandxa,respectively. Then,wecomputetheattentionmap
t t
(cid:32) (cid:33)
QoKa⊤
A=softmax √ , Qo :=norm(ho )WQ and Ka :=norm(ha )WK, (6)
d l,t l l,t l
wherenormisappliedacrossspatialdimension(hw). Notably,wenormalizeho andha firstto
l,t l,t
removeappearancestatisticsandthusisolatestructuralcorrespondence. Then,wecomputethemean
andstandarddeviationmapsMandSofha weightedbyAandusethemtonormalizeho ,
l,t l,t
(cid:113)
ho ←S⊙ho +M, M:=Aha and S:= A(ha ⊙ha )−(M⊙M). (7)
l,t l,t l,t l,t l,t
5MandS,weightedbystructuralcorrespondencesbetweenIo andIa,arespatially-awarefeature
statisticsofxawhicharetransferredtoxo. Lastly,weperformlayerlself-attentiononho asnormal.
t t l,t
Weapplyappearancetransferforlayersl∈Lapp,andwedosofor(normalized)timestepst≤τa,
whereτa ∈[0,1]istheappearancecontrolschedule.
Structureandappearancecontrol. Finally,wereplaceϵ inEquation2with
θ
ϵˆ (cid:0) xo |t,c,{fs } ,{As } ,{ha } (cid:1) , (8)
θ t l,t l∈Lfeat l,t l∈Lself l,t l∈Lapp
where{fs } ,{As } ,and{ha } respectivelycorrespondtoxsfeaturesforfeature
l,t l∈Lfeat l,t l∈Lself l,t l∈Lapp t
injection,xsattentionmapsforself-attentioninjection,andxafeaturesforappearancetransfer.
t t
5 Experiments
Wepresentextensivequantitativeandqualitativeresultstodemonstratethestructurepreservationand
appearancealignmentofCtrl-XonT2Idiffusion. AppendixAcontainsmoreimplementationdetails.
5.1 T2Idiffusionwithstructureandappearancecontrol
Baselines. Fortraining-basedmethods,ControlNet[44]andT2I-Adapter[25]learnanauxiliary
modulethatinjectsaconditionimageintoapretraineddiffusionmodelforstructurealignment. We
thencombinethemwithIP-Adapter[43],atrainedmoduleforimagepromptingandthusappearance
transfer. SplicingViTFeatures[35]trainsaU-Netfromscratchpersource-appearanceimagepair
tominimizetheirDINO-ViTself-similaritydistanceandglobal[CLS]tokenloss. (Forstructure
conditionsnotsupportedbyatraining-basedbaseline,weconvertthemtocannymaps.) Forguidance-
basedmethods,FreeControl[24]enforcestructureandappearancealignmentviabackpropagated
scorefunctionscomputedfromdiffusionfeaturesubspaces. Forguidance-freemethods,Cross-Image
Attention[1]manipulatesattentionweightstotransferappearancewhilemaintainingstructure. We
runallmethodsonSDXLv1.0[27]whenpossibleandontheirdefaultbasemodelsotherwise.
Dataset. Our method supports T2I diffusion with appearance transfer and arbitrary-condition
structure control. Since no benchmarks exist for such a flexible task, we create a new dataset
comprising 256 diverse structure-appearance pairs. The structure images consist of 31% natural
images,49%ControlNet-supportedconditions(e.g.canny,depth,segmentation),and20%in-the-wild
conditions(e.g.3Dmesh,pointcloud),andtheappearanceimagesareamixofWebandgenerated
images. Weusetemplatesandhand-annotationforthestructure,appearance,andoutputtextprompts.
Evaluationmetrics. Forquantitativeevaluation,wereporttwowidely-adoptedmetrics: DINO
Self-simmeasurestheself-similaritydistance[35]betweenthestructureandoutputimageinthe
DINO-ViT[6]featurespace,wherealowerdistanceindicatesbetterstructurepreservation;DINO
CLS measuresthelossbetweentheDINO-ViTglobal[CLS]tokensoftheappearanceandoutput
image[35],wherealowerlossindicatesbetterappearancealignment.
Qualitativeresults. AsshowninFigures4and5,Ctrl-Xfaithfullypreservesstructurefromstructure
imagesrangingfromnaturalimagesandControlNet-supportedconditions(e.g.HED,segmentation)
toin-the-wildconditions(e.g.wireframe,3Dmesh)notpossibleinpriortraining-basedmethods
whileadeptlytransferringappearancefromtheappearanceimagewithsemanticcorrespondence.
Comparisontobaselines. Figure5andTable1compareCtrl-Xtothebaselines.Fortraining-based
andguidance-basedmethods,despiteT2I-Adapter[25]andFreeControl’s[24]strongerstructure
preservation (smaller DINO self-similarity distances), they generally struggle to enforce faithful
appearancetransferandyieldworseglobalCLSlosses,whichisparticularlyvisibleinFigure5row1
and3. Sincethetraining-basedmethodscombineastructurecontrolmodule(ControlNet[44]and
T2I-Adapter)withaseparately-trainedappearancetransfermoduleIP-Adapter[43],thetwomodules
sometimesexertconflictingcontrolsignalsatthecostofappearancetransfer(e.g.row1)—andfor
ControlNet, structure preservation as well. For FreeControl, its appearance score function from
extractedembeddingsmaynotsufficientlycapturemorecomplexappearancecorrespondences,which,
alongwithneedingper-imagehyperparametertuning,resultsinlowercontrastoutputsandsometimes
failedappearancetransfer(e.g.row4). Moreover,despiteSplicingViTFeatures[35]havingthebest
DINOself-similarityandCLSscoresinTable1, Figure5revealsthatitsoutputimagesareoften
blurrywhiledisplayingstructureimageappearanceleakagewithnon-naturalimages(e.g.row3,5,
and6). Itbenchmarkswellbecauseitsper-imagetrainingminimizesthesetwometricsdirectly.
6Structure Appearance Output Structure Appearance Output Structure Appearance Output
(a)
Structure Structure
Structure a realistic photo of a a painting of a tiger Structure a photo of a railway a painting of a Structure a photo of a Karate am embroidery of
bear and an avocado looking at a large during sunset railway during the man in a park a man scuba diving
in a forest white egg on a beach harsh winter in the ocean
(b)
Structure a video game pixel a photo of a Structure an oil painting of a a photo of a robot in a Structure a photo of a yellow a painting of an
art of a mansion gingerbread house warrior holding a sword Cyberpunk city sports car speeding abandoned, worn out
in space and shield in a river holding weapons in a city car in a desert
Structure a cartoon of an evil a rough sketch of a Structure a cartoon of the Grim a photo of a Stormtrooper Structure a photo of a city a photo of a river
goblin holding a kangaroo on top of Reapaer sitting on a sitting on a bench looking intersection at night, during winter,
piece of gold a mountain bench looking at his at their phone in a bird’s eye view bird's-eye view
phone futuristic city
Figure4: QualitativeresultsforT2Idiffusionstructureandappearancecontrolandconditional
generation. Ctrl-X supports a diverse variety of structure images for both (a) structure and
appearancecontrollablegenerationand(b)prompt-drivenconditionalgeneration.
7
ecnaraeppA ecnaraeppACross-Image ControlNet T2I-Adapter Splicing ViT
Structure Appearance Ctrl-X (ours) FreeControl Attention + IP-Adapter + IP-Adapter Features
Figure5: Qualitativecomparisonofstructureandappearancecontrol. Ctrl-Xdisplayscompa-
rablestructurecontrolandsuperiorappearancetransfercomparedtotraining-basedmethods. Itis
alsomorerobustthanguidance-basedandguidance-freemethodsacrossdiversestructuretypes.
Table1: Quantitativecomparisonofstructureandappearancecontrol. Ctrl-Xconsistently
outperforms both training-based and training-free methods in appearance alignment and shows
comparableorbetterstructurepreservationcomparedtotraining-basedandguidance-freemethods,
measuredbyDINOViTself-similarityandglobalCLStokenloss[35],respectively.
Naturalimage ControlNet-supported Newcondition Inference
Method Training
time(s)
Self-sim↓ DINOCLS↓ Self-sim↓ DINOCLS↓ Self-sim↓ DINOCLS↓
SplicingViTFeatures[35] ✓ 0.030 0.006 0.043 0.012 0.037 0.013 4289.20
ControlNet+IP-Adapter[44,43] ✓ 0.068 0.109 0.136 0.092 0.139 0.103 23.10
T2I-Adapter+IP-Adapter[25,43] ✓ 0.055 0.119 0.118 0.118 0.109 0.131 17.70
Cross-ImageAttention[1] ✗ 0.145 0.110 0.196 0.152 0.195 0.139 216.46
FreeControl[24] ✗ 0.058 0.132 0.101 0.119 0.089 0.139 1210.02
Ctrl-X(ours) ✗ 0.057 0.096 0.121 0.084 0.109 0.097 30.65
Guidance-freebaselineCross-ImageAttention[1],incontrast,islessrobustandmoresensitivetothe
structureimage’sappearance,astheinvertedstructurelatentscontainstrongappearanceinformation.
Thiscausesbothpoorerstructurealignmentandfrequentappearanceleakageorartifacts(e.g.row6)
fromthestructuretotheoutputimages,resultinginworseDINOself-similaritydistancesandglobal
CLSlosses. Inpractice,wefindCross-ImageAttentiontobesensitivetoitsmaskingdomainand
sometimesfailstoproduceoutputswithcrossmodalpairs(e.g.wireframestophotos).
Inferenceefficiency. Westudytheinferencetimeofourmethodcomparedtothebaselines,allwith
basemodelSDXLv1.0exceptCross-ImageAttention(SDv1.5)andSplicingViTFeatures(U-Net).
Table1reportstheaverageinferencetimeusingasingleNVIDIAA6000GPU.Ctrl-Xisslightly
slower than training-based ControlNet (1.32×) and T2I-Adapter (1.73×) with IP-Adapter, yet it
issignificantlyfasterthanper-image-trainedSplicingViT(0.0071×),guidance-basedFreeControl
8
egami
larutaN
noitidnoc
detroppus-teNlortnoC
noitidnoc
dliw-eht-nIStructure Prompt Ctrl-X (ours) FreeControl SDEdit Prompt-to-Prompt Plug-and-Play InfEdit ControlNet T2I-Adapter
a photo of a 70s
style dining room
a photo of a red
pickup truck in
front of a
mountain
a cartoon of a
wolf howling at
the moon
an embroidery
of a man scuba
diving in the
ocean
Figure6:Qualitativecomparisonofconditionalgeneration. Ctrl-Xdisplayscomparablestructure
controlandsuperiorpromptalignmenttotraining-basedmethods,anditalsohasbetterimagequality
andismorerobustthanguidance-basedand-freemethodsacrossdifferentconditions.
Appearance Appearance
Figure7: Extensiontotext-to-video(T2V)models. Ctrl-XcanbedirectlyappliedtoT2Vmodels
[9,32]forcontrollablevideostructureandappearancecontrol. MoreresultsareinAppendixC.
(0.025×),andguidance-freeCross-ImageAttention(0.14×). Ourtraining-freeandguidance-free
methodachievescomparableruntimestotraining-basedmethods,indicatingitsflexibility.
Extensiontoprompt-drivenconditionalgeneration. Ctrl-Xalsosupportsprompt-drivencondi-
tionalgeneration,whereitgeneratesanoutputimagecomplyingwiththegiventextpromptwhile
aligning with the structure from the structure image, as shown in Figures 4 and 6. Inspired by
FreeControl[24],insteadofagivenIa,Ctrl-XcanjointlygenerateIabasedonthetextpromptalong-
sideIo,whereweobtainxa viadenoisingwithEquation2fromxa withoutcontrol. Baselines,
t−1 t
qualitativeandquantitativeanalysis,andimplementationdetailsareavailableinAppendixB.
5.2 Extensiontovideodiffusionmodels
Ctrl-Xistraining-free,guidance-free,anddemonstratescompetitiveruntime. Thuswecandirectly
applyourmethodtotext-to-video(T2V)models,asseeninFigure7. Ourmethodcloselyalignsthe
structurebetweenthestructureandoutputvideoswhiletransferringtemporallyconsistentappearance
fromtheappearanceimage. AdditionalT2VresultsonmoremodelscanbefoundinAppendixC.
5.3 Ablations
Effectofcontrol. AsseeninFigure8(a),structurecontrolisresponsibleforstructurepreservation
(appearance-onlyvs.ours). Also,structurecontrolalonecannotisolatestructureinformation,display-
ingstrongstructureimageappearanceleakageandpoor-qualityoutputs(structure-onlyvs.ours),asit
merelyinjectsstructurefeatures,whichcreatesthesemanticcorrespondenceforappearancecontrol.
Appearancetransfermethod. Asweconsiderappearancetransferasastylizationtask,wecompare
ourappearancestatisticstransferwithandwithoutattentionweighting. Withoutattentionweighting
9
erutcurtS erutcurtSStructure Appearance No control Structure-only Appearance-only Ours
(a)
Structure Appearance Without attention Ours Structure Appearance Without attention Ours
(b)
Structure Appearance Inversion Ours Structure Appearance Inversion Ours
(c)
Figure8: Ablations. Westudyablationson(a)structureandappearancecontrol,(b)appearance
transfermethods,and(c)inversionvs.ourmethodforobtainingnoisystructureandoutputlatents.
Structure Appearance Output Structure Appearance Output
Figure 9: Limitations. Ctrl-X can struggle with localizing the corresponding subject in the
appearanceimagewithappearancetransferwhenthesubjectistoosmall.
(equivalent to AdaIN [15]), the normalization is global and thus cannot consider the semantic
correspondencebetweentheappearanceandoutputimages,sotheoutputslooklow-contrast.
Effectofinversion. WecompareDDIMinversionvs.forwarddiffusion(ours)toobtainxo =xs
T T
andxsinFigure8(c). Inversiondisplaysappearanceleakagefromstructureimagesinchallenging
t
conditions(left)whilebeingsimilartoourmethodinothers(right). Consideringinversioncostsand
additionalmodelinferencetime,forwarddiffusionisabetterchoiceforourmethod.
6 Conclusion
WepresentCtrl-X,atraining-freeandguidance-freeframeworkforstructureandappearancecontrol
ofanyT2IandT2Vdiffusionmodel.Ctrl-XutilizespretrainedT2Idiffusionmodelfeaturecorrespon-
dences,supportsarbitrarystructureimageconditions,workswithmultiplemodelarchitectures,and
achievescompetitivestructurepreservationandsuperiorappearancetransfercomparedtotraining-
andguidance-basedmethodswhileenjoyingthelowoverheadbenefitsofguidance-freemethods.
Limitations. AsshowninFigure9,thekeylimitationofCtrl-Xisthesemantic-awardappearance
transfermethodmayfailtocapturethetargetappearancewhentheinstanceissmallbecauseofthe
lowresolutionofthefeaturemap. Wehopeourmethodandfindingscanunveilnewpossibilitiesand
researchoncontrollablegenerationasgenerativemodelsbecomebiggerandmorecapable.
Broaderimpacts. Ctrl-Xmakescontrollablegenerationmoreaccessibleandflexiblebysupporting
multipleconditionalsignals(structureandappearance)andmodelarchitectureswithoutthecomputa-
tionaloverheadofadditionaltrainingoroptimization. However,thisaccessibilityalsomakesusing
pretrainedT2I/T2Vmodelsformaliciousapplications(e.g.deepfakes)easier,especiallysincethe
10controllabilityenablesuserstogeneratespecificimagesandraisesethicalconcernswithconsentand
creditingartistsforusingtheirworkasconditionimages. Inresponsetothesesafetyconcerns,T2I
andT2Vmodelshavebecomemoresecure. Likewise,Ctrl-Xcaninheritthesamesafeguards,andits
plug-and-playnatureallowstheopen-sourcecommunitytoscrutinizeandimproveitssafety.
References
[1] YuvalAlaluf,DanielGaribi,OrPatashnik,HadarAverbuch-Elor,andDanielCohen-Or. Cross-image
attentionforzero-shotappearancetransfer. InACMSpecialInterestGrouponComputerGraphicsand
InteractiveTechniques,2024. 3,4,5,6,8,14
[2] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. Break-a-scene:
Extractingmultipleconceptsfromasingleimage. InACMSpecialInterestGrouponComputerGraphics
andInteractiveTechniquesAsia,2023. 3
[3] OmriAvrahami,ThomasHayes,OranGafni,SonalGupta,YanivTaigman,DeviParikh,DaniLischinski,
OhadFried,andXiYin. Spatext: Spatio-textualrepresentationforcontrollableimagegeneration. In
IEEE/CVFComputerVisionandPatternRecognitionConference,2023. 2,16
[4] ArpitBansal,Hong-MinChu,AviSchwarzschild,SoumyadipSengupta,MicahGoldblum,JonasGeiping,
andTomGoldstein. Universalguidancefordiffusionmodels. InInternationalConferenceonLearning
Representations,2023. 2,3
[5] MingdengCao,XintaoWang,ZhongangQi,YingShan,XiaohuQie,andYinqiangZheng. Masactrl:
Tuning-freemutualself-attentioncontrolforconsistentimagesynthesisandediting. InInternational
ConferenceonComputerVision,2023. 3
[6] MathildeCaron,HugoTouvron,IshanMisra,HervéJégou,JulienMairal,PiotrBojanowski,andArmand
Joulin. Emerging properties in self-supervised vision transformers. In International Conference on
ComputerVision,2021. 6
[7] DaveEpstein,AllanJabri,BenPoole,AlexeiA.Efros,andAleksanderHolynski. Diffusionself-guidance
forcontrollableimagegeneration. InAdvancesinNeuralInformationProcessingSystems,2023. 1,2,3,4
[8] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel
Cohen-Or. Animageisworthoneword:Personalizingtext-to-imagegenerationusingtextualinversion. In
InternationalConferenceonLearningRepresentations,2023. 3
[9] YuweiGuo,CeyuanYang,AnyiRao,ZhengyangLiang,YaohuiWang,YuQiao,ManeeshAgrawala,
DahuaLin,andBoDai. Animatediff:Animateyourpersonalizedtext-to-imagediffusionmodelswithout
specifictuning. InInternationalConferenceonLearningRepresentations,2024. 9,16,17
[10] RanaHanocka,GalMetzer,RajaGiryes,andDanielCohen-Or. Point2mesh:aself-priorfordeformable
meshes. ACMTransactionsonGraphics,39(4),2020. 16
[11] AmirHertz,RonMokady,JayTenenbaum,KfirAberman,YaelPritch,andDanielCohen-Or. Prompt-to-
promptimageeditingwithcrossattentioncontrol.InInternationalConferenceonLearningRepresentations,
2023. 4,14,16
[12] JonathanHoandTimSalimans.Classifier-freediffusionguidance.arXivpreprintarXiv:2207.12598,2022.
3
[13] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. InAdvancesin
NeuralInformationProcessingSystems,pages6840–6851.CurranAssociates,Inc.,2020. 3,4
[14] EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,and
WeizhuChen. LoRA:Low-rankadaptationoflargelanguagemodels. InInternationalConferenceon
LearningRepresentations,2022. 3
[15] XunHuangandSergeBelongie. Arbitrarystyletransferinreal-timewithadaptiveinstancenormalization.
InInternationalConferenceonComputerVision,2017. 5,10
[16] OzgurKara,BariscanKurtkaya,HidirYesiltepe,JamesM.Rehg,andPinarYanardag. Rave:Randomized
noiseshufflingforfastandconsistentvideoeditingwithdiffusionmodels. InIEEE/CVFComputerVision
andPatternRecognitionConference,2024. 16
11[17] YunjiKim,JiyoungLee,Jin-HwaKim,Jung-WooHa,andJun-YanZhu. Densetext-to-imagegeneration
withattentionmodulation. InIEEE/CVFComputerVisionandPatternRecognitionConference,pages
7701–7711,2023. 2
[18] YunjiKim,JiyoungLee,Jin-HwaKim,Jung-WooHa,andJun-YanZhu. Densetext-to-imagegeneration
withattentionmodulation. InInternationalConferenceonComputerVision,pages7701–7711,2023. 4,5
[19] Quanyi Li, Zhenghao Peng, Lan Feng, Qihang Zhang, Zhenghai Xue, and Bolei Zhou. Metadrive:
Composingdiversedrivingscenariosforgeneralizablereinforcementlearning. IEEETransactionson
PatternAnalysisandMachineIntelligence,2022. 16
[20] YuhengLi,HaotianLiu,QingyangWu,FangzhouMu,JianweiYang,JianfengGao,ChunyuanLi,and
YongJaeLee. Gligen:Open-setgroundedtext-to-imagegeneration. InIEEE/CVFComputerVisionand
PatternRecognitionConference,2023. 2
[21] SonghuaLiu,TianweiLin,DongliangHe,FuLi,MeilingWang,XinLi,ZhengxingSun,QianLi,andErrui
Ding. Adaattn:Revisitattentionmechanisminarbitraryneuralstyletransfer. InInternationalConference
onComputerVision,pages6649–6658,2021. 5
[22] NaureenMahmood,NimaGhorbani,NikolausF.Troje,GerardPons-Moll,andMichaelJ.Black. AMASS:
Archiveofmotioncaptureassurfaceshapes. InInternationalConferenceonComputerVision,pages
5442–5451,2019. 16
[23] ChenlinMeng,YutongHe,YangSong,JiamingSong,JiajunWu,Jun-YanZhu,andStefanoErmon.Sdedit:
Guidedimagesynthesisandeditingwithstochasticdifferentialequations. InInternationalConferenceon
LearningRepresentations,2022. 14,16
[24] SichengMo,FangzhouMu,KuanHengLin,YanliLiu,BochenGuan,YinLi,andBoleiZhou.Freecontrol:
Training-free spatial control of any text-to-image diffusion model with any condition. In IEEE/CVF
ComputerVisionandPatternRecognitionConference,2024. 1,2,3,4,5,6,8,9,14,16,18
[25] ChongMou,XintaoWang,LiangbinXie,JianZhang,ZhongangQi,YingShan,andXiaohuQie. T2i-
adapter: Learningadapterstodigoutmorecontrollableabilityfortext-to-imagediffusionmodels. In
AssociationfortheAdvancementofArtificialIntelligence,2024. 2,3,6,8,14,15,16
[26] Ryan Po, Guandao Yang, Kfir Aberman, and Gordon Wetzstein. Orthogonal adaptation for modular
customizationofdiffusionmodels,2023. 3
[27] DustinPodell,ZionEnglish,KyleLacey,AndreasBlattmann,TimDockhorn,JonasMüller,JoePenna,
andRobinRombach. SDXL:Improvinglatentdiffusionmodelsforhigh-resolutionimagesynthesis. In
InternationalConferenceonLearningRepresentations,2024. 1,4,5,6,14,15,16
[28] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisualmodelsfrom
naturallanguagesupervision. InInternationalConferenceonMachineLearning,2021. 15
[29] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-resolution
imagesynthesiswithlatentdiffusionmodels. InIEEE/CVFComputerVisionandPatternRecognition
Conference,2022. 3,4,15
[30] NatanielRuiz,YuanzhenLi,VarunJampani,YaelPritch,MichaelRubinstein,andKfirAberman. Dream-
booth:Finetuningtext-to-imagediffusionmodelsforsubject-drivengeneration. InIEEE/CVFComputer
VisionandPatternRecognitionConference,2023. 3
[31] NatanielRuiz,YuanzhenLi,VarunJampani,WeiWei,TingboHou,YaelPritch,NealWadhwa,Michael
Rubinstein,andKfirAberman.Hyperdreambooth:Hypernetworksforfastpersonalizationoftext-to-image
models,2023. 3
[32] SG_161222. Realisticvisionv5.1. https://civitai.com/models/4201?modelVersionId=130072,
2023. 9,16,17
[33] JiamingSong,ChenlinMeng,andStefanoErmon. Denoisingdiffusionimplicitmodels. InInternational
ConferenceonLearningRepresentations,2021. 14
[34] YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoErmon,andBenPoole.
Score-basedgenerativemodelingthroughstochasticdifferentialequations. InInternationalConferenceon
LearningRepresentations,2021. 3
12[35] NarekTumanyan,OmerBar-Tal,ShaiBagon,andTaliDekel.SplicingViTfeaturesforsemanticappearance
transfer. InIEEE/CVFComputerVisionandPatternRecognitionConference,2022. 6,8,14
[36] NarekTumanyan,MichalGeyer,ShaiBagon,andTaliDekel. Plug-and-playdiffusionfeaturesfortext-
drivenimage-to-imagetranslation. InIEEE/CVFComputerVisionandPatternRecognitionConference,
pages1921–1930,2023. 4,5,14,15,16,18
[37] PatrickvonPlaten,SurajPatil,AntonLozhkov,PedroCuenca,NathanLambert,KashifRasul,Mishig
Davaadorj,DhruvNair,SayakPaul,WilliamBerman,YiyiXu,StevenLiu,andThomasWolf. Diffusers:
State-of-the-artdiffusionmodels. https://github.com/huggingface/diffusers,2022. 14
[38] XudongWang,TrevorDarrell,SaiSakethRambhatla,RohitGirdhar,andIshanMisra. Instancediffusion:
Instance-levelcontrolforimagegeneration,2024. 2,3
[39] YaohuiWang,XinyuanChen,XinMa,ShangchenZhou,ZiqiHuang,YiWang,CeyuanYang,Yinan
He,JiashuoYu,PeiqingYang,etal. Lavie:High-qualityvideogenerationwithcascadedlatentdiffusion
models. arXivpreprintarXiv:2309.15103,2023. 16,17
[40] JiayuXiao,HengleiLv,LiangLi,ShuhuiWang,andQingmingHuang. R&b:Regionandboundaryaware
zero-shotgroundedtext-to-imagegeneration. InInternationalConferenceonLearningRepresentations,
2024. 2
[41] SihanXu,YidongHuang,JiayiPan,ZiqiaoMa,andJoyceChai. Inversion-freeimageeditingwithnatural
language. InIEEE/CVFComputerVisionandPatternRecognitionConference,2024. 3,4,14,15,16
[42] ZhengyuanYang,JianfengWang,ZheGan,LinjieLi,KevinLin,ChenfeiWu,NanDuan,ZichengLiu,
CeLiu,MichaelZeng,etal. Reco:Region-controlledtext-to-imagegeneration. InIEEE/CVFComputer
VisionandPatternRecognitionConference,2023. 2
[43] HuYe,JunZhang,SiboLiu,XiaoHan,andWeiYang. Ip-adapter:Textcompatibleimagepromptadapter
fortext-to-imagediffusionmodels. arXivpreprintarxiv:2308.06721,2023. 2,3,6,8,14,16
[44] LvminZhang,AnyiRao,andManeeshAgrawala. Addingconditionalcontroltotext-to-imagediffusion
models. InInternationalConferenceonComputerVision,2023. 2,3,6,8,14,15,16
[45] RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang. Theunreasonableeffec-
tivenessofdeepfeaturesasaperceptualmetric. InIEEE/CVFComputerVisionandPatternRecognition
Conference,2018. 15
[46] ShihaoZhao,DongdongChen,Yen-ChunChen,JianminBao,ShaozheHao,LuYuan,andKwan-YeeK
Wong. Uni-controlnet: All-in-onecontroltotext-to-imagediffusionmodels. InAdvancesinNeural
InformationProcessingSystems,2023. 2
[47] GuangcongZheng,XianpanZhou,XueweiLi,ZhongangQi,YingShan,andXiLi. Layoutdiffusion:
Controllablediffusionmodelforlayout-to-imagegeneration. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages22490–22499,2023. 2
[48] BoleiZhou,HangZhao,XavierPuig,SanjaFidler,AdelaBarriuso,andAntonioTorralba. Sceneparsing
through ade20k dataset. In IEEE/CVF Computer Vision and Pattern Recognition Conference, pages
5122–5130,2017. 16
[49] DeweiZhou,YouLi,FanMa,XiaotingZhang,andYiYang. Migc:Multi-instancegenerationcontroller
fortext-to-imagesynthesis,2024. 2
13Table2: Comparisontopriorworks. ComparingthecapabilitiesofCtrl-Xtopriorcontrollable
generationworks. Naturalimagesandin-the-wildconditionsrefertothetypeofstructureimagethat
themethodsupportsforstructurecontrol.
Structurecontrol
Method Appearancecontrol Training-free Guidance-free
Naturalimages In-the-wildconditions
ControlNet[44](+IP-Adapter[43]) ✓ ✓
T2I-Adapter[25](+IP-Adapter[43]) ✓ ✓
SDEdit[23] ✓ ✓ ✓
Prompt2Prompt[11] ✓ ✓ ✓
Plug-and-Play[36] ✓ ✓ ✓
InfEdit[41] ✓ ✓ ✓
SplicingViTAttention[35] ✓ ✓ ✓
Cross-ImageAttention[1] ✓ ✓ ✓ ✓
FreeControl[24] ✓ ✓ ✓ ✓
Ctrl-X(ours) ✓ ✓ ✓ ✓ ✓
A Methodandimplementationdetails
We compare Ctrl-X to prior works in terms of capabilities in Table 2. Compared to baselines,
ourmethodistheonlyworkwhichsupportsappearanceandstructurecontrolwithanystructure
conditions,whilebeingtraining-freeandguidance-free.
Moredetailsonfeed-forwardstructurecontrol. Weinjectdiffusionfeaturesafterconvolution
skipconnections. Sinceweinitializexo asrandomGaussiannoise,theimagestructureafterthe
T
firstinferencesteplikelydoesnotalignwithIs,asobservedby[36]. Thus,injectingbeforeskip
connectionsresultsinweakerstructurecontrolandimageartifacts,aswearesummingfeaturesfo
t
andfswithconflictingstructureinformation.
t
Moredetailsoninference. Withclassifier-freeguidance,inspiredby[24,1],weonlycontrolthe
prompt-conditionedϵ ,‘steering’thediffusionprocessawayfromuncontrolledgenerationandthus
θ
strengtheningstructureandappearancealignment. Also,sincestructureandappearancecontrolcan
resultinout-of-distributionx afterapplyingEquation2, weapplynr stepsofself-recurrence.
t−1
Particularly,afterobtainingxo withstructureandappearancecontrol,werepeat
t−1
xo ←√ α xˆo+(cid:112) 1−α ϵˆ (x˜o |t,c,{},{},{}),
t−1 t−1 0 t−1 θ t
√
(cid:114) α (cid:114) α x˜o− 1−α ϵˆ (x˜o |t,c,{},{},{}) (9)
x˜o := t xo + 1− t ϵ and xˆo := t t θ √t
t α t−1 α 0 α
t−1 t−1 t
nrtimesfor(normalized)timestepst∈[τr,τr],whereτr,τr ∈[0,1].
0 1 0 1
Experimenthyperparameters. ForbothT2Idiffusionwithstructureandappearancecontroland
structure-onlyconditionalgeneration,weuseStableDiffusionXL(SDXL)v1.0[27]forallCtrl-X
experiments,unlessstatedotherwise. ForSDXL,wesetLfeat ={0} ,Lself ={0,1,2} ,
decoder decoder
Lapp ={1,2,3,4} ∪{2,3,4,5} ,andτs =τa =0.6. WesampleIo with50stepsof
decoder encoder
DDIMsamplingandsetη =1[33],doingself-recurrencefornr =2forτr =0.1andτr =0.5. We
0 1
implementCtrl-XwithDiffusers[37]andrunallexperimentsonasingleNVIDIAA6000GPU.
B Extensiontoprompt-drivencontrollablegeneration
Ctrl-X also supports prompt-driven conditional generation, where it generates an output image
complyingwiththegiventextpromptwhilealigningwiththestructurefromthestructureimage,
asshowninFigures4and6. InspiredbyFreeControl[24],insteadofagivenIa,Ctrl-Xcanjointly
generateIabasedonthetextpromptalongsideIo,whereweobtainxa viadenoisingwithEquation
t−1
2fromxawithoutcontrol.
t
Baselines. Fortraining-basedmethods,wetestControlNet[44]andT2I-Adapter[25].Forguidance-
basedmethods, wetestFreeControl[24], wherewegenerateanappearanceimagealongsidethe
outputimageinsteadofinvertingagivenappearanceimage. Forguidance-freemethods,SDEdit[23]
addsnoisetotheinputimageanddenoisesitwithapretraineddiffusionmodeltopreservestructure.
Prompt-to-Prompt[11]andPlug-and-Play[36]manipulatefeaturesandattentionofpretrainedT2I
14Structure Prompt Ctrl-X (ours) FreeControl SDEdit Prompt-to-Prompt Plug-and-Play InfEdit ControlNet T2I-Adapter
a photo of a
futuristic sci-fi
power plant
a photo of two
avocados
a photo of a 70s
style dining room
a photo of a
tawny horse
an embroidery
of a man scuba
diving in the
ocean
a cartoon of a
wolf howling at
the moon
a photo of a red
pickup truck in
front of a
mountain
a cartoon of an
evil goblin
holding a piece
of gold
Figure10: Fullqualitativecomparisonofconditionalgeneration. Ctrl-Xdisplayscomparable
structurecontrolandsuperiorpromptalignmenttotraining-basedmethodswithbetterimagequality.
It is also more robust than guidance-based and guidance-free methods across a wide variety of
conditiontypes. (WerunControlNet[44]andT2I-Adapter[25]onSDv1.5[29]insteadofSDXL
v1.0[27],asthelatterfrequentlygenerateslow-contrast,flatresultsforthetwomethods.)
modelsforprompt-drivenimageediting. InfEdit[41]usesthree-branchattentionmanipulationand
consistentmulti-stepsamplingforfast,consistentimageediting.
Dataset. Ourcontrollablegenerationdatasetcomprisesof175diverseimage-promptpairswiththe
same(structure)imagesasSection5.1. Itconsistsof71%ControlNet-supportedconditionsand29%
newconditions. Weusethesamehand-annotatedstructurepromptsandhand-createoutputprompts
withinspirationfromPlug-and-Play’sdatasets[36]. SeemoredetailsinAppendixD.
Evaluationmetrics. Forquantitativeevaluation,wereportthreewidely-adoptedmetrics: DINO
Self-simfromSection5.1measuresstructurepreservation;CLIPscore[28]measuresthesimilarity
between the output image and text prompt in the CLIP embedding space, where a higher score
suggests stronger image-text alignment; LPIPS distance [45] measures the appearance deviation
oftheoutputimagefromthestructureimage,whereahigherdistancesuggestslowerappearance
leakagefromthestructureimage.
Qualitativeresults. AsshowninFigures4and10,Ctrl-Xgenerateshigh-qualityimageswithgreat
structurepreservationandclosepromptalignment. Ourmethodcanextractstructureinformation
fromawiderangeconditiontypesandproducesresultsofdiversemodalitiesbasedontheprompt.
Comparisontobaselines. Figure6andTable3compareourmethodtothebaselines. Training-
basedmethodstypicallybetterpreservestructure,withlowerDINOself-similaritydistances,atthe
costofworsepromptadherence,withlowerCLIPscores. Thisisbecausethesemodulesaretrained
oncondition-outputpairswhichlimittheoutputdistributionofthebaseT2Imodel,especiallyfor
in-the-wildconditionswheretheproducedcannymapsareunusual. Ourmethod,incontrast,transfers
15
noitidnoc
detroppus-teNlortnoC
noitidnoc
dliw-eht-nITable3: Quantitativecomparisononconditionalgeneration. Ctrl-Xoutperformsalltraining-
based and guidance-free baselines in prompt alignment (CLIP score). Although many baselines
seemtobetterpreservestructurewithlowDINOself-similaritydistances,thelowdistancesmainly
comefromseverestructureimageappearanceleakage(highLPIPS),alsoshowninFigure10. Also,
thoughFreeControldisplaysbetterstructurepreservationandpromptalignment,itstillexperiences
appearanceleakagewhichresultsinpoorimagequality(Figure10).
ControlNet-supported Newcondition
Method
Self-sim↓ CLIPscore↑ LPIPS↑ Self-sim↓ CLIPscore↑ LPIPS↑
ControlNet[44] 0.126 0.298 0.657 0.092 0.302 0.507
T2I-Adapter[25] 0.096 0.303 0.504 0.068 0.302 0.415
SDEdit[23] 0.102 0.300 0.366 0.096 0.309 0.373
Prompt-to-Prompt[11] 0.100 0.276 0.370 0.097 0.287 0.357
Plug-and-Play[36] 0.056 0.282 0.272 0.050 0.292 0.301
InfEdit[41] 0.117 0.314 0.523 0.102 0.311 0.442
FreeControl[24] 0.108 0.340 0.557 0.104 0.339 0.492
Ctrl-X(ours) 0.134 0.322 0.635 0.135 0.326 0.590
appearancefromajointly-generatedappearanceimagethatutilizesthefullgenerationpowerofthe
baseT2Imodelandisneitherdomain-limitedbytrainingnorgreatlyaffectedbyhyperparameters.
Incontrast,guidance-basedandguidance-freemethodsdisplayappearanceleakagefromthestructure
image. The guidance-based FreeControl requires per-image hyperparameter tuning, resulting in
fluctuatingimagequalityandappearanceleakagewhenranwithitsdefaulthyperparameters. Thus,
evenifitdisplaysslightlyhigherpromptadherence(higherCLIPscore),theappearanceleakage
oftenproduceslower-qualityoutputimages(lowerLPIPS).Guidance-freemethods, ontheother
hand,share(inverted)latents(SDEdit,Prompt-to-Prompt,Plug-and-Play)orinjectsdiffusionfeatures
(all)withthestructureimagewithouttheappearanceregularizationwhichCtrl-X’sjointly-generated
appearance image provides. Consequently, though structure is preserved well with better DINO
self-similaritydistances,undesirablestructureimageappearanceisalsotransferredover,resulting
inworseLPIPSscores. Forexample,allguidance-basedandguidance-freebaselinesdisplaythe
magenta-blue-greencolorsofthediningroomnormalmap(row3),thecolor-patchylookofthecar
andmountainsparsemap(row7),andtheredbackgroundofthe3Dsquirrelmesh(row8).
C Additionalvideoresults
Wepresentadditionalresultsofourmethoddirectlyappliedtotext-to-video(T2V)diffusionmodels
inFigure11,namelyAnimateDiff[9]withbasemodelRealisticVisionv5.1[32]andLaVie[39].
D Datasetdetails
Allacademicdatasetswhichweusearecitedhere[3,10,43,24,48,22,19,36,16]. Wewillrelease
ourdatasetalongsidecoderelease.
Ourdatasetconsistsof1771024×1024imagesdividedinto16typesandacross7categories. We
splittheimagesintoconditionimages(67images: “cannyedgemap”,“metadrive”,“3dmesh”,“3d
humanoid”, “depth map”, “human pose image”, “point cloud”, “sketch”, “line drawing”, “HED
edgedrawing”,“normalmap”,and“segmentationmask”)andnaturalimages(110images: “photo”,
“painting”,“cartoon”and“birdseyeview”),withthethelargesttypebeing“photo”(83images). The
conditionimagesarefurtherdividedintotwogroupsinourpaper: ControlNet-supportedconditions
(“canny edge map”, “depth map”, “human pose image”, “line drawing”, “HED edge drawing”,
“normalmap”,and“segmentationmask”)andin-the-wildconditions(“metadrive”,“3Dmesh”,“3D
humanoid”,“pointcloud”,and“sketch”).Allofourimagesfallintooneofsevencategories:“animals”
(52images),“buildings”(11images),“humans”(28images),“objects”(29images),“rooms”(24
images),“scenes”(22images)and“vehicles”(11images). Abouttwothirdsoftheimagescome
fromtheWeb,whiletheremainingthirdisgeneratedusingSDXL1.0[27]orconvertedfromnatural
imagesusingControlnetAnnotatorspackagedincontrolnet-aux[44]. Foreachoftheseimages,
wehandannotatethemwithatextpromptandothermetadata(e.g.type).Then,theseimages,promtps,
andmetadataarecombinedtoformthestructureandappearancecontroldatasetandconditional
generationdataset,detailedbelow.
16Appearance
Appearance
Appearance
Appearance
Appearance
Figure11: Additionaltext-to-video(T2V)modelresults. Ctrl-XcanbedirectlyappliedtoT2V
modelsforcontrollablevideostructureandappearancecontrol,withAnimateDiff[9]withRealistic
Visionv5.1[32]andLaVie[39]hereasexamples.
17
erutcurtS
erutcurtS
erutcurtS
erutcurtS
erutcurtS
AnimateDiff
w/
Realistic
Vision
AnimateDiff
w/
Realistic
Vision
AnimateDiff
w/
Realistic
Vision
AnimateDiff
w/
Realistic
Vision
LaVieT2Idiffusionwithstructureandappearancecontroldataset. Thisdatasetconsistsof256pairs
ofimagesfromtheimagedatasetdescribedabove. Thisdatasetisusedtoevaluateourmethodand
thebaselines’abilitytogenerateimagesadheringtothestructureofaconditionornaturalimage
whilealigningtotheappearanceofasecondnaturalimage. Eachpaircontainsastructureimage
(whichmaybeaconditionornaturalimage)andanappearanceimage(whichisanaturalimage).
Thedatasetalsoincludesastructurepromptforthestructureimage(e.g.“acannyedgemapofa
horsegalloping”),anappearancepromptfortheappearanceimage(e.g.“apaintingofatawnyhorse
inafield”),andonetargetpromptfortheoutputimage(e.g.“apaintingoftawnyhorsegalloping”)
generatedbycombiningthemetadataoftheappearanceandstructurepromptsviaatemplate,witha
fewedgecaseshand-annotated. Imagepairsareconstructedfromtwoimagesfromthesamecategory
(e.g.“animals”)andthemajorityofpairsconsistofimagesofthesamesubject(e.g.“horse”),but
weinclude30pairsofcross-subjectimages(e.g.“cat”and“dog”)totestthemethods’abilityto
generalizestructureinformationacrosssubjects.
Inpractice,whenrunningCtrl-X,wesimplysettheappearanceprompttobethesameastheoutput
promptinsteadofourhand-annotatedappearanceprompt. Wefoundlittledifferencebetweenthetwo
approaches.
Conditionalgenerationdataset. Theconditionaldatasetcombinesconditionalimageswithboth
template-generatedandhand-writtenoutputprompts(inspiredbyPlug-and-Play[36]andFreeControl
[24])toevaluateourmethodandthebaselines’abilitytoconstructanimageadheringtothestructure
oftheinputimagewhilecomplyingwiththegivenprompt. Eachentryintheconditionaldataset
consistsofaconditionimagecombinedwithauniqueprompt. Wehave175suchcondition-prompt
pairsfromthesetof66conditionimagesabove.
18