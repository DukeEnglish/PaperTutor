Scaling Large-Language-Model-based Multi-Agent Collaboration
ChenQian†⋆ ZihaoXie†⋆ YifeiWang†⋆ WeiLiu⋆ YufanDang⋆
ZhuoyunDu⋆ WeizeChen⋆ ChengYang♠ ZhiyuanLiu⋆ MaosongSun⋆(cid:66)
⋆TsinghuaUniversity ♠BeijingUniversityofPostsandTelecommunications
qianc62@gmail.com sms@tsinghua.edu.cn
Abstract
Task Solution
Pioneering advancements in large language
model-powered agents have underscored the
design pattern of multi-agent collaboration,
demonstratingthatcollectiveintelligencecan
surpassthecapabilitiesofeachindividual. In-
spiredbytheneuralscalinglaw,whichposits
thatincreasingneuronsleadstoemergentabil-
ities, this study investigates whether a simi-
lar principle applies to increasing agents in
multi-agent collaboration. Technically, we
Multi-Agent Collaboration Network
propose multi-agent collaboration networks
(cid:58)(cid:58) (cid:58) (cid:58)(cid:58)
(MACNET), which utilize directed acyclic Figure1: Givenatask,multi-agentcollaborationnet-
graphstoorganizeagentsandstreamlinetheir works(MACNET)utilizedirectedacyclicgraphstoor-
interactivereasoningviatopologicalordering, ganizediverseagentsforcollaborativeinteractions,with
with solutions derived from their dialogues. thefinalsolutionderivedfromtheirdialogues.
Extensive experiments show that MACNET
consistentlyoutperformsbaselinemodels,en-
ablingeffectiveagentcollaborationacrossvar-
iousnetworktopologiesandsupportingcoop-
istheneuralscalinglawthatfostersemergentcapa-
eration among more than a thousand agents.
Notably, we observed a small-world collabo- bilities,wherewell-trainedneuralnetworksoften
rationphenomenon, wheretopologiesresem- exhibitpower-lawscalingrelationsprimarilywith
bling small-world properties achieved supe- thenumberofneurons, alongsidefactorssuchas
riorperformance. Additionally,weidentified datasetsizeandtrainingtime(Kaplanetal.,2020;
a collaborative scaling law, indicating that
Schaefferetal.,2024;Muennighoffetal.,2024).
normalizedsolutionqualityfollowsalogistic
growthpatternasscalingagents,withcollabo-
Despite this, LLMs have inherent limitations
rativeemergenceoccurringmuchearlierthan
inenclosedreasoning,particularlywhenaddress-
previouslyobservedinstancesofneuralemer-
ingcomplexsituationsthatextendbeyondtextual
gence. Thecodeanddatawillbeavailableat
boundaries (Richards, 2023). To this end, subse-
https://github.com/OpenBMB/ChatDev.
quentstudieshavesuccessfullytransformedfoun-
1 Introduction dational LLMs into versatile autonomous agents
byequippingthemwithadvancedcapabilitiessuch
In the rapidly advancing field of artificial intelli-
as tool use (Schick et al., 2023), long-context
gence, large language models (LLMs) have cat-
memory (Park et al., 2023), and procedural plan-
alyzed transformative shifts across numerous do-
ning (Wei et al., 2022b). Along this line, multi-
mains due to their remarkable linguistic capac-
agent collaboration has emerged as an effective
itytoseamlesslyintegrateextensiveworldknowl-
paradigm to integrate the specialities of different
edge (Vaswani et al., 2017; Brown et al., 2020;
agents(Parketal.,2023;Lietal.,2023a;Qianetal.,
Bubecketal.,2023). Centraltothisbreakthrough
2024b). Throughlinguisticinteraction,agentsen-
gage in instructive and responsive utterances to
†EqualContribution.
(cid:66)CorrespondingAuthor. foster high-quality collaboration, leading to final
4202
nuJ
11
]IA.sc[
1v55170.6042:viXrasolutions1 derivedfromtheirdialogues(Qianetal., overlyextendedcontexts,enablingscalablecol-
2024b,a;Chenetal.,2024b). laborationacrossnearlyanylarge-scalenetwork.
Inspiredbytheneuralscalinglaw,anaturalques-
Weconductedacomprehensivequantitativeeval-
tionarises: doesincreasingagentsinmulti-agent
uation of three prevalent topologies—chain, tree,
collaboration exhibit emergent capabilities? In-
andgraph—dividedintosixvariants,acrossmul-
vestigating the cooperative scaling law is essen-
tipleheterogeneousdownstreamscenarios. Exten-
tial for accurately estimating the relationship be-
tweencomputingresourcesandperformancetrends
siveexperimentsdemonstratethatMACNETcon-
sistentlyoutperformsallbaseline,enablingeffec-
in multi-agent systems. This understanding en-
tive agent collaboration even in fully-connected
ablestheoptimizationofresourceutilizationand
dense networks, supporting cooperation among
theminimizationofunnecessarywaste,ultimately
more than a thousand agents. Notably, we ob-
leading to more scalable, practical, and resource-
servedasmall-worldcollaborationphenomenon,
efficientagentsystems(Kaplanetal.,2020). How-
wheretopologiesresemblingsmall-worldproper-
ever,effectivemulti-agentcollaborationtranscends
tiesdemonstratedsuperiorperformance. Addition-
themereaggregationofresponsesfromdifferent
ally,weidentifiedacollaborativescalinglaw,re-
agentsthroughmajorityvoting(Chenetal.,2024a);
vealingthatnormalizedsolutionqualityfollowsa
instead,itconstitutesanorganicallyintegratedsys-
logistic growth pattern as scaling agents. Mean-
tem that requires task-oriented interactions and
while, collaborative emergence can be observed
thoughtfuldecision-making(Hopfield,1982).
occurring significantly earlier compared to previ-
In this paper, as illustrated in Figure 1, we en-
ous instances of neural emergence. We hope our
vision multiple agents as a well-organized team
findingsoffervaluableinsightsintoresourcepredic-
composedofspecializedagents,investigatingtheir
tionandoptimizationtoenhancetheefficiencyand
interdependent interactive reasoning and collec-
scalabilityofLLMsystems(Kaplanetal.,2020).
tiveintelligenceinautonomouslysolvingcomplex
problems. To further this goal, we design appro-
2 RelatedWork
priatetopologiesandeffectiveinteractionmecha-
nismsthatalignwithboththestaticorganizational Trained on vast datasets and capable of manip-
structureandthedynamicreasoningprocess. ulating billions of parameters, LLMs have be-
come pivotal in natural language processing due
• Toensuregeneralizability,wedesignthetopol-
to their seamless integration of extensive knowl-
ogyasadirectedacyclicgraphwhereeachedge
edge (Brown et al., 2020; Bubeck et al., 2023;
ismanagedbyasupervisoryinstructorissuingdi-
Vaswani et al., 2017; Radford et al., 2019; Tou-
rectionalcommands,andeachnodeissupported
vronetal.,2023;Weietal.,2022a;Shanahanetal.,
byanexecutiveassistantprovidingtailoredso-
2023;Chenetal.,2021;Brantsetal.,2007;Chen
lutions. This mechanism effectively fosters a
etal.,2021;Ouyangetal.,2022;Yangetal.,2024;
division of labor among agents through func-
Qin et al., 2023; Kaplan et al., 2020). Central to
tionaldichotomy,seamlesslyintegratingastatic
thisbreakthroughistheneuralscalinglaw,which
topologywithspecializedagentstoformamulti-
positsthatlossscalesasapower-lawwithmodel
agentcollaborationnetwork(MACNET).
size,datasetsize,andtheamountofcomputeused
• To facilitate agents’ interactive reasoning, the fortraining(Kaplanetal.,2020;Smithetal.,2022;
interactionsequenceisorchestratedviatopologi- Muennighoff et al., 2024). The principle under-
calordering,ensuringorderlyinformationtrans- scores that scaling up language models can lead
missionthroughoutthenetwork. Withinthisar- toemergentabilities—whereperformanceexperi-
rangement,eachinteractionroundinvolvestwo encesasuddenleapasthemodelscales(Weietal.,
adjacentagentsrefiningaprevioussolution,with 2022a;Schaefferetal.,2024).
onlytherefinedsolution, ratherthantheentire Despitethese,LLMshaveinherentlimitationsin
dialogue, being propagated to the next neigh- enclosedreasoning,motivatingsubsequentstudies
bors. Thismechanismstrategicallyavoidsglobal toeffectivelyequipLLMswithadvancedcapabil-
broadcastsandsignificantlyreducestheriskof ities such as role playing (Li et al., 2023a; Chan
et al., 2024), tool use (Schick et al., 2023; Qin
1Solutions can range from a multiple-choice answer to
et al., 2024), long-context memory (Park et al.,
repository-levelcodeoracoherentnarrative,amongnumerous
otherpossibilities. 2023; Wang et al., 2023), and procedural plan-CHAIN TREE
SOLUTION INSTRUCTION SOLUTION
O
O X
O “ADD GUI”
X O
X O
Chain Star Tree
GRAPH
INSTRUCTOR
ASSISTANT ASSISTANT
NODE EDGE NODE
Mesh Layered Random
Figure3: Assigndifferentagentsonnodesandedges.
Figure2: Representativetopologicalstructures.
ning(Weietal.,2022b;Yaoetal.,2023),thereby drawing on the concept of graphs—a data struc-
transformingfundamentalLLMsintoversatileau- turethatdescribesentitiesandtheirinterrelations,
tonomous agents (Richards, 2023; Shinn et al., wemodelthetopologyasadirectedacyclicgraph
2024; Zhao et al., 2024). Along this line, multi- (DAG) (Nilsson et al., 2020) to organize inter-
agent collaboration has emerged as an effective actions among collaborative agents (Qian et al.,
paradigm to integrate the specialities of different 2024a). Concretely,afeasibletopologyisdenoted
agents(Parketal.,2023;Zhouetal.,2023;Chen asG = (V,E):
etal.,2024b;Chanetal.,2024;Chenetal.,2023;
V={v |i∈I}, E={⟨v ,v ⟩|i,j∈I ∧i̸=j} (1)
Cohen et al., 2023; Li et al., 2023b; Hua et al., i i j
2023). Astraightforwardcollaborationstrategyis whereV denotesthesetofnodesindexedbyI,and
majorityvoting(Chenetal.,2024a),whereindivid- E denotesthesetofedges,witheachedgedirected
ualsremainindependent;however,moreeffective fromonenodetoanotherandnocyclesexist.
multi-agentcollaborationshouldformanintegrated Giventheimpracticalityofenumeratingallpos-
systemthatfostersinterdependentinteractionsand sible topologies, our study focus on three preva-
thoughtfuldecision-making(Lietal.,2024;Chen lenttypes—chain,tree,andgraph—furtherdivided
et al., 2024a; Piatti et al., 2024). Based on this, intosixstructures,asdepictedinFigure2. Chain
pioneeringstudieshavedichotomizedthefunction- topologies, resembling the waterfall model (Pe-
alityofagentsintotwodistinctroles: instructors, tersenetal.,2009),linearlystructuringinteractions
whoprovidedirectionalinstructions,andassistants, along agents. Tree topologies enable agents to
whorespondwithtailoredsolutions;theseagents branchout,interactinginindependentdirections;
engageininstructiveandresponsiveutterancesto further categorized into "wider" star-shaped and
fosteraninteractionchain,collaborativelyarriving "deeper"tree-shapedstructures. Graphtopologies
atfinalsolutionsderivedfromtheirdialogues(Qian support arbitrary interaction dependencies, with
etal.,2024b;Lietal.,2023a). Thisparadigmfacili- nodeshavingmultiplechildrenandparents,form-
tatesawell-orchestratedworkflowfortask-oriented ing either divergent and convergent interactions;
interactions, significantly reducing the need for furtherclassifiedintofully-connectedmeshstruc-
manualinterventionwhiledemonstratingpromis- tures,MLP-shapedlayeredstructures,andirregular
ingquality.(Chenetal.,2024b;Chanetal.,2024). randomstructures. Theserepresentativetopologies
areextensivelystudiedincomplexnetwork(Stro-
3 Multi-AgentCollaborationNetwork gatz,2001;AlbertandBarabási,2001)andLLM
agentreasoning(Liuetal.,2023;Bestaetal.,2024),
Weaimtoestablishascalableframeworkformulti-
ensuringacomprehensiveexaminationofthemost
agent collaboration, comprising two key compo-
significantandpracticalstructuresinunderstanding
nents: the design of a multi-agent collaboration
(cid:58)(cid:58) (cid:58) (cid:58) multi-agentsystems.
(cid:58)n (cid:58)e (cid:58)twork(MACNET)andcollaborativereasoning.
In the ecosystem of LLM-powered agents, a
functionaldichotomy(Lietal.,2023a)—consisting
3.1 NetworkConstruction
ofsupervisoryinstructorswhoissuedirectionalin-
Toestablishaorganationalstructureformulti-agent structionsandexecutiveassistantswhoprovidetai-
collaboration that is both efficient and scalable, loredsolutions—caneffectivelypromotedivisionInteraction
Control Flow
Data Flow
Figure4: Streamliningtheagents’reasoningprocessinvolvesaseriesofdual-agentinteractions. Thetopological
orderguidestheinteractionsequence,whiletheoriginalconnectivitygovernsthedataflow.
oflabor,stimulatefunctionalbehaviors,andfacili- each node-occupied agent a precedes its corre-
i
tateefficienttaskresolution(Qianetal.,2024b,a). sponding edge-occupied agent a , and a pre-
ij ij
Tointegratethisstrategyintothetopology,asde- cedes a , thereby guaranteeing ensuring orderly
j
picted in Figure 3, we strategically assign an in- informationtransmissionthroughoutthenetwork.
structortoeachedgeandanassistanttoeachnode: Afterestablishingtheglobalorder,asillustrated
inFigure4,weenableeachpairofedge-connected
a = ρ(v ), ∀v ∈ V
i i i adjacentagentstointeractandexchangeinforma-
(2)
a ij = ρ(⟨v i,v j⟩), ∀⟨v i,v j⟩ ∈ E tion. ForatopologyG,thedesignresultinatotal
deploymentof|V|+|E|agentsandrequire2|E|in-
whereρ(x)representstheagentizationoperationon
teractionrounds. Withineachedge,theinteraction
anelementx,achievedbyequippingafoundation
patternbetweenassistantsandinstructorsfollows
modelwithprofessionalroles(Lietal.,2023a),ex-
amulti-turninstruction-responsesequence:
ternaltools(Schicketal.,2023),andcontext-aware
(cid:0) (cid:1)
memory(Parketal.,2023);a anda denotean τ(a ,a ,a ) = τ(a ,a ),τ(a ,a )
i ij i ij j i ij ij j
assistantagentassignedtonodev iandaninstructor τ(a i,a ij) = (a
i
→ a ij, a
ij
(cid:59) a i)⟲ (4)
agentassignedtoedgev ,respectively.
This dichotomous
desi ij
gn allows agents to spe-
τ(a ij,a j) = (a
ij
→ a j, a
j
(cid:59) a ij)⟲
cializeintheirfunctions,drivingtask-orientedlan- where→symbolizestheactofinstructing,(cid:59)in-
guageinteractionsandfacilitatingefficientinforma- dicatesthecorrespondingresponding,and⟲rep-
tiontransmissionthroughtoutthenetwork. Addi- resentstheiterativenatureoftheprocess. Specifi-
tionally,the"directed"natureoftheedgesenables cally,a requestsfeedback,a offersoptimization
i ij
the orchestration of agent interactions, while the suggestions and requests further refinement, and
"acyclic"configurationpreventsinformationprop- a providestherefinedsolution. Thus,theagents
j
agationdeadlocks. associatedwithasingleedgecaneffectivelyopti-
mizeasolutioninoneiteration.
3.2 InteractiveReasoning
Delvingdeeper,thetopologicalorderingmethod-
In the process of completing complex tasks, in- icallyunfoldsagentinteractionsintoaninteraction
teractive reasoning among agents within a static sequence,outliningthecontrolflow2withinamulti-
MACNETrequiresstrategicaltraversaltoestablish agentcollaborationprocess. Concurrently,thedata
anorderlyinteractionsequence. Ourgraphtraver- flowwithinthisprocessisconsistentwiththeorig-
salstrategyadherestotheprinciplesoftopological inal dependencies connected by edges, ensuring
ordering(BondyandMurty,1976),afundamental thattheflowofinteractedinformationalignswith
algorithmingraphtheory,whichensuresthateach theinherentdependenciesoutlinedinthetopology.
nodeisvisitedonlyafterallitsdependencieshave
beentraversed(Grossetal.,2018). Formally,for 3.3 MemoryControl
a MACNET G, its topological order is a linear ar- Inamulti-agentcollaborationsystem,unrestrained
rangementofagentsa i anda ij suchthatforevery context information exchange can lead to exces-
directededge⟨v i,v j⟩ ∈ E,theorderingsatisfies: sively long contexts, ultimately limiting scalabil-
∀⟨v ,v ⟩ ∈ E, I(a ) < I(a ) < I(a ) (3) 2Notethatalthoughtheinteractionorderisunfoldedas
i j i ij j
asequenceforvisualizationpurposesonly,certainsubstruc-
whereI(x)denotestheindexofagentxinthetopo- tures(e.g.,star-structuredtopology)inherentlysupportparal-
lelprocessing,whichisessentialinenhancingthereasoning
logical sequence. This arrangement ensures that efficiencyofpracticalsystems.ity by supporting only a few agents. To address tionalgraphs,wherenodesrepresentmanually-
this,weadoptaheuristicmechanism(Qianetal., customizedfunctionsandedgesrepresentinfor-
2024b) to manage context visibility using short- mation flow, significantly surpassing the tree-
termandlong-termmemory. Short-termmemory of-thought method by optimizing node-level
capturestheintra-interactionworkingmemorydur- promptsandmodifyinggraphconnectivity.
ingeachdual-agentinteraction,ensuringcontext-
• AGENTVERSE(Chenetal.,2024b)recruitsand
awaredecision-making. Long-termmemorymain-
orchestratesateamofexpertagentsineithera
tainsinter-interactioncontextcontinuitybytrans-
horizontalorverticaltopologicalstructure,em-
mitting only the final solutions derived from dia-
ployingmulti-agentlinguisticinteractiontoau-
logues,nottheentireconversationalhistory. This
tonomouslyrefinesolutionsanddemonstrating
approachensuresthatthecontextofancestoragents
emergent performance compared to individual
remainsMarkovian,withsolutionspropagatedonly
agents, serving as both general and powerful
fromadjacentagentsratherthanfromallprevious
multi-agentframework.
dialogues. Consequently,itreducestheriskofcon-
DatasetsandMetrics Weadoptpubliclyavail-
textoverloadwhilepreservingcontextcontinuity,
ableandlogicallychallengingbenchmarkstoeval-
therebyenablingscalablemulti-agentcollaboration
uateacrossheterogeneousdownstreamscenarios.
acrossnearlyanylarge-scalenetwork.
Furthermore, an original solution propagating • MMLU(Hendrycksetal.,2020)providesacom-
throughthenetworkundergoescontinuousrefine- prehensivesetoflogicalreasoningassessments
ment,improvingitsqualityovertime. Assolutions across diverse subjects and difficulties, utiliz-
traverse the network, they either branch off at di- ingmultiple-optionquestionstomeasuregeneral
vergent nodes or aggregate at convergent nodes. worldknowledgeandlogicalinferencecapabil-
Branching is achieved through parallel propaga- ities. We assess the quality of generated solu-
tion,whilemergingfrommultiplenodes,akintoa tionsviaaccuracy,reflectingthecorrectnessof
non-linearperceptron,requiresaneffectiveaggre- responsestomultiple-choicequestions.
gationmechanism. Technically,convergentagents • HumanEval(Chenetal.,2021),awidelyrecog-
assessthestrengthsandweaknessesofeachsolu- nizedbenchmarkforfunction-levelcodegenera-
tion, synthesizing their strengths and discarding tion,desinedformeasuringbasicprogramming
weaknesses,whichresultsinastrength-aggregated skills. Weassessviapass@k,reflectingfunction
outcomefrom"non-linear"decision-making,rather correctnessacrossmultiplestandardtestcases.
thanasimplecombinationofallsolutions.
• SRDD (Qian et al., 2024b) integrates com-
4 Experiments plex textual software requirements frommajor
real-world application platforms, designed for
Baselines Weselectdifferentkindsofrepresen-
repository-level software development, includ-
tativemethodsforquantitativecomparison.
ingrequirementcomprehension,systemdesign,
• COT(Weietal.,2022b)isatechnicallygeneral and integration testing. We assess using qual-
and empirically powerful method that endows ity, acomprehensivemetricthatintegratescru-
LLMswiththeabilitytogenerateacoherentse- cialfactorsincludingcompleteness,executabil-
ries of intermediate reasoning steps, naturally ity,andconsistency.
leading to the final solution through thought- • CommonGen-Hard (Madaan et al., 2023) re-
fulthinkingandallowingreasoningabilitiesto quires models to generate coherent sentences
emerge. incorporating discrete concepts, designed to
• AUTOGPT(Richards,2023)isaversatilesingle- testsystems’advancedcommonsensereasoning,
agentsystemthatemploysmulti-stepplanning contextualunderstanding,andcreativeproblem-
andtool-augmentedreasoningtoautonomously solving. Weassessusingacomprehensivescore
decomposecomplextasksintochainedsubtasks that integrates crucial factors including gram-
and iteratively leverages external tools within mar,fluency,contextrelevance,andlogicconsis-
anenvironment-feedbackcycletoprogressively tency(Lietal.,2018;Chenetal.,2024b).
developeffectivesolutions.
ImplementationDetails Bydefault,ourmethod
• GPTSWARM (Zhuge et al., 2024) formal- utilizestheGPT-3.5-turbomodel,chosenforitsop-
izes a swarm of LLM agents as computa- timalbalanceofreasoningefficacyandefficiency.Method Paradigm MMLU HumanEval SRDD CommonGen AVG.
COT 0.3544† 0.6098† 0.7222† 0.6165 0.5757†
AUTOGPT 0.4485† 0.4809† 0.7353† 0.5972† 0.5655†
GPTSWARM 0.2368† 0.4969 0.7096† 0.6222† 0.5163†
AGENTVERSE 0.2977† 0.7256† 0.7587† 0.5399† 0.5805
MACNET-CHAIN 0.6632 0.3720 0.8056 0.5903 0.6078
MACNET-STAR 0.4456 0.5549 0.7679 0.7382 0.6267
MACNET-TREE 0.3421 0.4878 0.8044 0.7718 0.6015
MACNET-MESH 0.6825 0.5122 0.7792 0.5525 0.6316
MACNET-LAYERED 0.2780 0.4939 0.7623 0.7176 0.5629
MACNET-RANDOM 0.6877 0.5244 0.8054 0.5912 0.6522
Table1: TheoverallperformanceofLLM-drivenmethodsacrossvariousdatasets, includingbothsingle-agent
( ) and multi-agent ( ) paradigms. For each dataset, the highest scores are highlighted in bold, while the
second-highestscoresareunderlined. Adagger(†)denotesstatisticallysignificantdifferences(p≤0.05)between
thebaselineandourchain-structuredsetting.
Weenhancethediversityofperspectivesbylever- ded in foundational models, giving single agents
aging GPT-4 to generate a pool of 4,000 profiles a notable capability in these relatively "simple"
forassignment. Theseagentsareequippedtoau- tasks. AlthoughGPTSWARMself-organizesagents
tonomouslyuseexternaltools(e.g.,Pythoncompil- throughdynamicoptimizationofnodesandedges,
ers),andtheirtemperaturesdecreaselinearlyfrom it still requires extensive task-specific customiza-
1.0 to 0.0 according to topology depths. Topo- tion for all agents and their behaviors, making it
logical sorting is implemented via Kahn’s algo- challengingtoseamlesslytransfertoheterogeneous
rithm (Kahn, 1962). During agent interactions, a downstream tasks. Giventhe increasing need for
maximumofthreeroundsofutterancesisallowed. highly performant and automatic real-world sys-
Toensurefairness,allbaselinesadheretoidentical tems,itisunrealistictoexpectthatallpreparatory
hyperparametersandsettingsintheevaluation. All knowledgecanbefullypre-encodedinfoundation
codeanddatawillbepubliclyavailable. models,norcanspecificadaptationsbepre-made
for all unforeseen complex tasks. Luckily, MAC-
4.1 DoesOurMethodLeadtoSuperior NETaddressesthischallengebyautomaticallygen-
Performance? eratingvariousnetworksthroughsimplehyperpa-
rameters (e.g., topology type and scale), without
Wefirstemploythesimplesttopology—chain—as
requiringadditionalspecificadaptations,whichrep-
thedefaultsettingforourcomparativeanalysis. As
resentsamorepromisingparadigmforenhancing
showninTable1,thechain-structuremethodcon-
autonomy,scalability,andgeneralizability.
sistentlyoutperformsallbaselinemethodsacross
mostmetrics,demonstratingasignificantmarginof
improvement. TheprimaryadvantageofMACNET, Inaddition,weablateagents’profilesandtem-
comparedtoasingleagentprovidinganswersfrom perature,whichisequivalenttograph-of-thought
aspecificperspective,liesinitsfacilitationofase- method—graph-guildedreasoingthoughtsbyasin-
quentialprocesswheresolutionsarecontinuously gleagentwholacksaprofileandhasatemperature
refined. Thisenablesautonomousandincremental setto0. Wefindthatablatingthesemechanismsre-
optimization,effectivelyalleviatingpreviouslyim- sultsinsignificantperformancedegradationacross
perfectsolutionsorfalsehallucinations(Qianetal., alltopologies,withanaveragedecreaseof2.69%.
2024b,a; Chen et al., 2024b; Chan et al., 2024). Thishighlightsthesuperiorcollectiveintelligence
Moreover, we observe that COT exhibits strong overanyformofreasoningbyasingleagent,asthe
performance oncertain datasets, even surpassing lattercorrespondstoafeaturedimensionreduction
somemulti-agentmethodsinspecificcases. This ofthehigh-dimensionalmulti-agentcombination
isprimarilybecausetheunderlyingknowledgeof space,whichsolidifiesreasoningabilityduetothe
widely-researched benchmarks is largely embed- lackofflexibilitytoexploreabetterconfiguration.4.2 HowDoDifferentTopologiesPerform 0.64
AgainstEachOther? 0.63 0.6267 Divergent
Convergent
Tounderstandthetopologicalproperties,wecon- 0.62
ducted extensive experiments by altering MAC- 0.61
0.6015
NET’s topologies. The results in Table 1 demon- 0.60
0.5902
stratethatdifferenttopologiesexhibitvaryinglev- 0.59
0.5821
elsofeffectivenessfordistincttasks. Forinstance,
0.58
achaintopologyismoresuitableforsoftwarede-
0.57
velopment,whileameshtopologyexcelsinlogical Star Tree
selection. Nosingletopologyconsistentlydelivers
Figure 5: The average performance of the divergent
optimal results across all tasks. Further observa- topology(default)anditsconvergentcounterpart.
tionrevealsthattopologiesapproachingthesmall-
worldproperty(WattsandStrogatz,1998)—char-
acterizedbyasmallaveragepathlength3—tendto outperform "deeper" tree-shaped ones. This is
primarilyattributabletooursolutionpropagation
exhibitsuperiorperformance,whichwerefertoas
mechanism, which inhibits the propagation of
the"small-worldcollaborationphenomenon". Con-
excessively long contextual reasoning processes
cretely,aseachedgeinMACNETtriggersagentin-
throughouttheentirenetwork. Asaresult,deeper
teractions,thegraph’sdensitynaturallyrepresents
topologies may cause agents to lose sight of far-
theagents’interactiondensity. Empirically,higher
ther contexts, potentially leading to version roll-
interactiondensityisassociatedwithimprovedper-
back—solutionsreverttoearlierorsimilarversions.
formanceamongthethreecoarse-grainedtopolog-
icaltypes.4 Thisperformancediscrepancycanbe Thesameprincipleappliestographstructures,in
whichmeshtopologies,comparedtolayeredones,
attributed to the fact that a higher graph density
enable direct reasoning between agents through
generally correlates with a higher clustering co-
efficient5. This increase in clustering coefficient directedges,therebyimplicitlyreducingnetwork
depthandenhancingperformance.
resultsinmoreadjacentnodepairs,decreasingthe
Inadditiontothestructuralpointofview,thedi-
averagepathlength;consequently,thelikelihood
rectionalcharacteristicsofsometopologies,which
oflong-distancesolutioninvisibilityiscorrespond-
exhibitinherentasymmetry—reversingtheedges
ingly decreased. Along this reason, we also dis-
results in an entirely unequal one—motivated us
coverthatirregularrandomstructuresoutperform
to explore reverse topologies. As shown in Fig-
regular mesh structures. This advantage can be
ure 5, merely altering the symmetry topologies’
attributedtorandomedgeconnections,which,in
orientationleadstosignificantperformancedegra-
analogy to social networks, potentially link "un-
dation. Typically,divergentstructures(thosewith
acquainted" agents via a direct shortcut, making
morechildnodesthanparentnodes)significantly
them adjacent "acquaintances" and implicitly re-
outperform convergent counterparts. Intuitively,
ducing the average path length, thus resembling
solution flow smoothly diverges, allowing each
small-world properties. Meanwhile, unlike mesh
agent to propose solutions from varied perspec-
topology,whichexhibitthehighestinteractionden-
tivesconcurrently;conversely,convergingtheso-
sity,randomtopologyachieveanoptimalbalance
lutions of multiple agents at a single point poses
betweenreducedarrangementdepthandenhanced
agreaterchallenge,illustratingthecomplexityin-
reasoning efficiency, making it a more suitable
volved in integrating diverse perspectives into a
tradeoffinpractice.
cohesivestrategy.
Additionally,itisobservedthat,giventhesame
density,"wider"star-shapedtopologiesgenerally
4.3 DoesaCollaborativeScalingLawExist?
3Averagepathlength(AlbertandBarabási,2001)istheav- Recallthattheneuralscalinglawfostersemergent
eragenumberofstepsalongtheshortestpathsforallpossible capabilities (Kaplan et al., 2020; Schaeffer et al.,
pairsofnetworknodes,whichisameasureoftheefficiency
2024;Muennighoffetal.,2024),wherethesynergy
ofinformationtransportonanetwork.
4Forexample,thedenselyconnectedmeshtopologyout- among numerous neurons enables a continuous
performsthemoderatelydensetreetopology,whichinturn trend of performance improvement. To investi-
outperformsthesparselyconnectedchaintopology.
gate the collaborative scaling law—the potential
5The clustering coefficient measures how densely con-
nectedanode’sneighborsaretoeachother(Strogatz,2001). predictable relationship between agent scale and
ytilauQChain Star Tree
80 80 80
70 70 70
60 60 60
50 50 50
20 21 22 23 24 25 26 20 21 22 23 24 25 26 20 21 22 23 24 25 26
Scale Scale Scale
Mesh Layered Random
80 80 80
70 70 70
60 60 60
50 50 50
20 21 22 23 24 25 26 20 21 22 23 24 25 26 20 21 22 23 24 25 26
Scale Scale Scale
Figure6: Scalingperformanceofmulti-agentcollaborationunderdifferenttopologies.
performance,consideringtheassociatedtimeand relying on from-scratch training in latent space
economiccosts—wescaleddifferenttopologiesby via matrix operations, requires a vast scale to in-
exponentiallyincreasingthenumberofnodesfrom corporateextensiveworldknowledgeanddevelop
1(regressingtoasingle-agentmethod)to50(cor- learningcapabilities. Incontrast, agentcoordina-
respondingto1,275agentsonameshsetting). As tion,basedontheimplicitknowledgeofpretrained
shown in Figure 6, our results confirm the small- LLMs,leveragestheunderstandingandrefinement
world collaboration phenomenon, where optimal of textual information through linguistic interac-
outcomes are achieved in high-density networks. tions, often circumventing the extensive scaling
Additionally, a reverse degradation phenomenon neededbyneuronalcoordination. Combiningthese
canbealsoobserved,wherecertainconfigurations two scaling mechanisms at different levels holds
led to an overall quality reduction ranging from promiseforproducinghigher-qualityoutcomes.
2.27%to6.24%.
5 Conclusion
As the topology scales, the quality of solu-
tionsproducedbythemulti-agentsysteminitially
We have introduced MACNET, which leverages
risesrapidlybeforereachingasaturationpoint(or
DAGstostructuretheagents’cooperativetopolo-
slightlydeclining),whichcanbeapproximatedby
gies and streamline their interactive reasoning
asigmoid-shapedfunction:
through topological ordering, with solutions de-
α rived from their dialogues. Extensive experi-
f(x) = +δ (5)
1+e−β(x−γ)
mentsdemonstratethat MACNETconsistentlyout-
performs all baseline models, enabling effective
whereα,β,γ andδ beingrealnumbersspecificto agentcollaborationacrossvarioustopologies. No-
atopology. Itisimportanttoemphasizethatthisis tably, we observed a small-world collaboration
onlyanaveragecharacterizationbasedonscale;a phenomenon,wheretopologiesresemblingsmall-
moreprecisemulti-agentsystemshouldconsider world properties demonstrated superior perfor-
additionalfactors(e.g.,foundationmodels,profile, mance. Additionally, we identified a collabora-
andtoolspaces). Notably,neuralscalinglawstyp- tivescalinglaw,revealingthatnormalizedsolution
ically require a million-fold increase in neurons qualityfollowsalogisticgrowthpatternasscaling
torevealsignificanttrendsaroundascaleof1018 agents. Meanwhile, collaborativeemergencecan
to1024 (Schaefferetal.,2024). Incontrast,most be observed occurring significantly earlier com-
topologiesinMACNETexhibitperformancesatu- pared to previous instances of neural emergence.
rationaroundascaleof24to25. Thiscollaborative We hope our findings offer valuable insights into
emergenceoccursmorerapidlycomparedtoneu- resource prediction and optimization to enhance
ralemergenceandisobservableatsmallerscales. theefficiencyandscalabilityofLLMsystems.
Theunderlyingreasonisthatneuroncoordination,
ytilauQ
ytilauQ
ytilauQ
ytilauQ
ytilauQ
ytilauQReferences MarkChen,JerryTworek,HeewooJun,QimingYuan,
Henrique Ponde de Oliveira Pinto, Jared Kaplan,
Réka Albert and Albert-László Barabási. 2001. Sta- HarriEdwards,YuriBurda,NicholasJoseph,Greg
tistical Mechanics of Complex Networks. volume Brockman, et al. 2021. Evaluating Large Lan-
cond-mat/0106096. guageModelsTrainedonCode. InarXivpreprint
arXiv:2107.03374.
MaciejBesta,FlorimMemedi,ZhenyuZhang,Robert
Gerstenberger, Guangyuan Piao, Nils Blach, Piotr Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang,
Nyczyk,MarcinCopik,GrzegorzKwas´niewski,Jür- ChenfeiYuan,ChenQian,Chi-MinChan,YujiaQin,
gen Müller, Lukas Gianinazzi, Ales Kubicek, Hu- Yaxi Lu, Ruobing Xie, et al. 2024b. AgentVerse:
bertNiewiadomski,AidanO’Mahony,OnurMutlu, FacilitatingMulti-agentCollaborationandExploring
and Torsten Hoefler. 2024. Demystifying Chains, Emergent Behaviors in Agents. In The Twelfth In-
Trees, and Graphs of Thoughts. In arXiv preprint ternationalConferenceonLearningRepresentations
arXiv:2401.14295. (ICLR).
J.A.BondyandU.R.Murty.1976. GraphTheorywith RoiCohen,MayHamri,MorGeva,andAmirGlober-
Applications. InLondon: Macmillan. son.2023. LMvsLM:DetectingFactualErrorsvia
CrossExamination. InProceedingsofthe2023Con-
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. ferenceonEmpiricalMethodsinNaturalLanguage
Och,andJeffreyDean.2007. LargeLanguageMod- Processing(EMNLP),pages12621–12640.
els in Machine Translation. In Proceedings of the
2007JointConferenceonEmpiricalMethodsinNat- Jonathan L. Gross, Jay Yellen, and Mark Anderson.
uralLanguageProcessingandComputationalNat- 2018. GraphTheoryandItsApplications. InChap-
ural Language Learning (EMNLP-CoNLL), pages manandHall/CRC.
858–867.
Dan Hendrycks, Collin Burns, Steven Basart, Andy
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Zou, Mantas Mazeika, Dawn Xiaodong Song, and
Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind Jacob Steinhardt. 2020. Measuring Massive Mul-
Neelakantan,PranavShyam,GirishSastry,Amanda titask Language Understanding. In arXiv preprint
Askell, Sandhini Agarwal, Ariel Herbert-Voss, arXiv:2009.03300.
Gretchen Krueger, Tom Henighan, Rewon Child,
JJHopfield.1982. NeuralNetworksandPhysicalSys-
AdityaRamesh,DanielZiegler,JeffreyWu,Clemens
temswithEmergentCollectiveComputationalAbil-
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
ities. InProceedingsOfTheNationalAcademyOf
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Sciences(PNAS).
Clark, ChristopherBerner, SamMcCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei,
Language Models are Few-Shot Learners. In Ad-
Jianchao Ji, Yingqiang Ge, Libby Hemphill, and
vances in Neural Information Processing Systems
Yongfeng Zhang. 2023. War and Peace (WarA-
(NeurIPS),volume33,pages1877–1901.
gent): Large Language Model-based Multi-Agent
Simulation of World Wars. In arXiv preprint
Sébastien Bubeck, Varun Chandrasekaran, Ronen El-
arXiv:2311.17227.
dan,JohannesGehrke,EricHorvitz,EceKamar,Pe-
ter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
A.B.Kahn.1962. TopologicalSortingofLargeNet-
etal.2023. SparksofArtificialGeneralIntelligence:
works. CommunicationsoftheACM,5(11):558–562.
Early Experiments with GPT-4. In arXiv preprint
arXiv:2303.12712.
JaredKaplan,SamMcCandlish,TomHenighan,TomB.
Brown,BenjaminChess,RewonChild,ScottGray,
Chi-MinChan,WeizeChen,YushengSu,JianxuanYu,
AlecRadford,JeffreyWu,andDarioAmodei.2020.
WeiXue,ShanghangZhang,JieFu,andZhiyuanLiu.
ScalingLawsforNeuralLanguageModels. InarXiv
2024. Chateval: Towards Better LLM-based Eval-
preprintarXiv:2001.08361.
uatorsthroughMulti-agentDebate. InTheTwelfth
International Conference on Learning Representa- Guohao Li, Hasan Abed Al Kader Hammoud, Hani
tions(ICLR). Itani, Dmitrii Khizbullin, and Bernard Ghanem.
2023a. CAMEL:CommunicativeAgentsfor“Mind”
Dake Chen, Hanbin Wang, Yunhao Huo, Yuzhao Li, ExplorationofLargeLanguageModelSociety. In
andHaoyangZhang.2023. GameGPT:Multi-agent Thirty-seventh Conference on Neural Information
CollaborativeFrameworkforGameDevelopment. In ProcessingSystems(NeurIPS).
arXivpreprintarXiv:2310.08067.
Junyou Li, Qin Zhang, Yangbin Yu, Qiang Fu, and
LingjiaoChen,JaredQuincyDavis,BorisHanin,Peter Deheng Ye. 2024. More Agents is All You Need.
Bailis, Ion Stoica, Matei Zaharia, and James Zou. arXivpreprintarXiv:2402.05120.
2024a. AreMoreLLMCallsAllYouNeed?Towards
ScalingLawsofCompoundInferenceSystems. In YuanLi,YixuanZhang,andLichaoSun.2023b. Metaa-
arXivpreprintarXiv:2403.02419. gents: SimulatingInteractionsofHumanBehaviorsforLLM-basedTask-orientedCoordinationviaCol- ChenQian,YufanDang,JiahaoLi,WeiLiu,ZihaoXie,
laborative Generative Agents. In arXiv preprint Yifei Wang, Weize Chen, Cheng Yang, Xin Cong,
arXiv:2310.06500. XiaoyinChe,ZhiyuanLiu,andMaosongSun.2024a.
Experiential Co-Learning of Software-Developing
ZhongyangLi, XiaoDing, andTingLiu.2018. Gen- Agents. IntheAnnualMeetingoftheAssociationfor
eratingReasonableandDiversifiedStoryEndingus- ComputationalLinguistics(ACL).
ingSequencetoSequenceModelwithAdversarial
Training. IntheInternationalConferenceonCompu- ChenQian,WeiLiu,HongzhangLiu,NuoChen,Yufan
tationalLinguistics(COLING),pages1033–1043. Dang,JiahaoLi,ChengYang,WeizeChen,Yusheng
Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu,
Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and andMaosongSun.2024b. ChatDev:Communicative
Diyi Yang. 2023. Dynamic LLM-Agent Net- Agents for Software Development. In the Annual
work: An LLM-agent Collaboration Framework Meeting of the Association for Computational Lin-
with Agent Team Optimization. In arXiv preprint guistics(ACL).
arXiv:2310.02170.
YujiaQin,ShihaoLiang,YiningYe,KunlunZhu,Lan
AmanMadaan, NiketTandon,PrakharGupta,Skyler Yan,YaxiLu,YankaiLin,XinCong,XiangruTang,
Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, BillQian,etal.2024. ToolLLM:FacilitatingLarge
Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Language Models to Master 16000+ Real-World
Shashank Gupta, Bodhisattwa Prasad Majumder, APIs. In The Twelfth International Conference on
Katherine Hermann, Sean Welleck, Amir Yazdan- LearningRepresentations(ICLR).
bakhsh,andPeterClark.2023. Self-Refine: Iterative
RefinementwithSelf-Feedback. InAdvancesinNeu- Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang,
ralInformationProcessingSystems(NeurIPS). JunruWu,JiamingShen,TianqiLiu,JialuLiu,Don-
aldMetzler,XuanhuiWang,andMichaelBendersky.
Niklas Muennighoff, Alexander Rush, Boaz Barak, 2023. Large Language Models are Effective Text
Teven Le Scao, Nouamane Tazi, Aleksandra Pik- RankerswithPairwiseRankingPrompting. InarXiv
tus,SampoPyysalo,ThomasWolf,andColinRaffel. preprintarXiv:2306.17563.
2024. ScalingData-ConstrainedLanguageModels.
AlecRadford,JeffreyWu,RewonChild,DavidLuan,
InAdvancesinNeuralInformationProcessingSys-
DarioAmodei,IlyaSutskever,etal.2019. Language
tems(NeurIPS),volume36.
Models are Unsupervised Multitask Learners. In
Anton Nilsson, Carl Bonander, Ulf Strömberg, and
OpenAIblog,volume1,page9.
Jonas Björk. 2020. A Directed Acyclic Graph for
Toran Bruce Richards. 2023. AutoGPT. In
Interactions. InInternationalJournalofEpidemiol-
https://github.com/Significant-Gravitas/AutoGPT.
ogy.
RylanSchaeffer,BrandoMiranda,andSanmiKoyejo.
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,
2024. Are Emergent Abilities of Large Language
CarrollWainwright,PamelaMishkin,ChongZhang,
ModelsaMirage? InAdvancesinNeuralInforma-
SandhiniAgarwal,KatarinaSlama,AlexRay,John
tionProcessingSystems(NeurIPS),volume36.
Schulman,JacobHilton,FraserKelton,LukeMiller,
Maddie Simens, Amanda Askell, Peter Welinder, TimoSchick,JaneDwivedi-Yu,RobertoDessì,Roberta
PaulFChristiano,JanLeike,andRyanLowe.2022. Raileanu,MariaLomeli,LukeZettlemoyer,Nicola
Training Language Models to Follow Instructions Cancedda,andThomasScialom.2023. Toolformer:
with Human Feedback. In Advances in Neural In- Language Models Can Teach Themselves to Use
formationProcessingSystems(NeurIPS),volume35, Tools. InarXivpreprintarXiv:2302.04761.
pages27730–27744.CurranAssociates,Inc.
MurrayShanahan,KyleMcDonell,andLariaReynolds.
JoonSungPark,JosephO’Brien,CarrieJunCai,Mered- 2023. RolePlaywithLargeLanguageModels. In
ithRingelMorris,PercyLiang,andMichaelSBern- Nature,volume623,pages493–498.
stein. 2023. Generative Agents: Interactive Simu-
lacraofHumanBehavior. InProceedingsofthe36th Noah Shinn, Federico Cassano, Ashwin Gopinath,
AnnualACMSymposiumonUserInterfaceSoftware KarthikNarasimhan,andShunyuYao.2024. Reflex-
andTechnology(UIST),pages1–22. ion: Language Agents with Verbal Reinforcement
Learning. AdvancesinNeuralInformationProcess-
Kai Petersen, Claes Wohlin, and Dejan Baca. 2009. ingSystems,36.
The Waterfall Model in Large-Scale Development.
InProduct-FocusedSoftwareProcessImprovement, Shaden Smith, Mostofa Patwary, Brandon Norick,
pages386–400. Patrick LeGresley, Samyam Rajbhandari, Jared
Casper, Zhun Liu, Shrimai Prabhumoye, George
GiorgioPiatti,ZhijingJin,MaxKleiman-Weiner,Bern- Zerveas, Vijay Korthikanti, Elton Zhang, Rewon
hardSchölkopf,MrinmayaSachan,andRadaMihal- Child, Reza Yazdani Aminabadi, Julie Bernauer,
cea. 2024. Cooperate or Collapse: Emergence of Xia Song, Mohammad Shoeybi, Yuxiong He,
SustainabilityBehaviorsinaSocietyofLLMAgents. MichaelHouston,SaurabhTiwary,andBryanCatan-
arXivpreprintarXiv:2404.16698. zaro. 2022. Using DeepSpeed and Megatron toTrainMegatron-TuringNLG530B,ALarge-Scale Chen,WentaoZhang,NingyuZhang,HuajunChen,
Generative Language Model. In arXiv preprint PengCui,andMrinmayaSachan.2023. Agents: An
arXiv:2201.11990. Open-sourceFrameworkforAutonomousLanguage
Agents. InarXivpreprintarXiv:2309.07870.
Steven H. Strogatz. 2001. Exploring Complex Net-
works. InNature. Mingchen Zhuge, Wenyi Wang, Louis Kirsch,
Francesco Faccio, Dmitrii Khizbullin, and Jurgen
HugoTouvron,ThibautLavril,GautierIzacard,Xavier Schmidhuber.2024. LanguageAgentsasOptimiz-
Martinet,Marie-AnneLachaux,TimothéeLacroix, ableGraphs. arXivpreprintarXiv:2402.16823.
BaptisteRozière,NamanGoyal,EricHambro,Faisal
Azhar, et al. 2023. Llama: Open and Efficient
Foundation Language Models. In arXiv preprint
arXiv:2302.13971.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser,andIlliaPolosukhin.2017. AttentionisAll
YouNeed. InAdvancesinNeuralInformationPro-
cessingSystems(NeurIPS),volume30.
Haotian Wang, Xiyuan Du, Weijiang Yu, Qianglong
Chen,KunZhu,ZhengChu,LianYan,andYiGuan.
2023. Apollo’sOracle: Retrieval-AugmentedRea-
soning in Multi-Agent Debates. In arXiv preprint
arXiv:2312.04854.
DuncanJ.WattsandStevenH.Strogatz.1998. Collec-
tive Dynamics of Small-World Networks. Nature,
393:440–442.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,
Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
MaartenBosma,DennyZhou,DonaldMetzler,EdH.
Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy
Liang,JeffDean,andWilliamFedus.2022a. Emer-
gentAbilitiesofLargeLanguageModels. InTrans-
actionsonMachineLearningResearch.
JasonWei,XuezhiWang,DaleSchuurmans,Maarten
Bosma,brianichter,FeiXia,EdChi,QuocVLe,and
Denny Zhou. 2022b. Chain-of-thought Prompting
Elicits Reasoning in Large Language Models. In
AdvancesinNeuralInformationProcessingSystems
(NeurIPS),volume35,pages24824–24837.
Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao
Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen.
2024. Large Language Models as Optimizers. In
The Twelfth International Conference on Learning
Representations(ICLR).
ShunyuYao,DianYu,JeffreyZhao,IzhakShafran,Tom
Griffiths,YuanCao,andKarthikNarasimhan.2023.
TreeofThoughts: DeliberateProblemSolvingwith
Large Language Models. In Advances in Neural
InformationProcessingSystems(NeurIPS).
AndrewZhao,DanielHuang,QuentinXu,MatthieuLin,
Yong-JinLiu,andGaoHuang.2024. Expel: LLM
Agents are Experiential Learners. In Proceedings
of the AAAI Conference on Artificial Intelligence,
volume38,pages19632–19642.
Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li,
JialongWu,TiannanWang,ShiQiu,JintianZhang,
JingChen,RuipuWu,ShuaiWang,ShidingZhu,Jiyu