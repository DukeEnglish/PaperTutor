Commonsense-T2I Challenge: Can Text-to-Image Genera-
tion Models Understand Commonsense?
XingyuFu¶,∗MuyuHe¶∗,YujieLu§∗,WilliamYangWang§,DanRoth¶
UniversityofPensylvania¶,UniversityofCalifornia,SantaBarbara§
{muyuhe, xingyuf2, danroth}@seas.upenn.edu, {yujielu, wangwilliamyang}@ucsb.edu
https://zeyofu.github.io/CommonsenseT2I
DALL-E 3 Stable Diffusion XL
Openjourney v4 Playground v2.5
prompt: A lightbulb without electricity
Figure1: AnexamplepromptinCommonsense-T2IandfailurecasesfromDALL-E3(Betker
etal.,2023),StableDiffusionXL(Rombachetal.,2022),Openjourneyv4,andPlayground
v2.5(Lietal.,2024). Theexpectedoutputforthepromptis“The lightbulb is unlit”.
Abstract
Wepresentanoveltaskandbenchmarkforevaluatingtheabilityoftext-
to-image(T2I)generationmodelstoproduceimagesthatfitcommonsense
in real life, which we call Commonsense-T2I. Given two adversarial text
prompts containing an identical set of action words with minor differ-
ences, such as “a lightbulb without electricity” vs. “a lightbulb
with electricity”,weevaluatewhetherT2Imodelscanconductvisual-
commonsensereasoning,e.g.produceimagesthatfit“The lightbulb is
unlit”vs.“The lightbulb is lit”correspondingly. Commonsense-T2I
presentsanadversarialchallenge,providingpairwisetextpromptsalong
withexpectedoutputs. Thedatasetiscarefullyhand-curatedbyexperts
andannotatedwithfine-grainedlabels, suchascommonsensetypeand
likelihood of the expected outputs, to assist analyzing model behavior.
Webenchmarkavarietyofstate-of-the-art(sota)T2Imodelsandsurpris-
ingly find that, there is still a large gap between image synthesis and
reallifephotos–eventheDALL-E3modelcouldonlyachieve48.92%on
Commonsense-T2I,andthestablediffusionXLmodelonlyachieves24.92%
accuracy. OurexperimentsshowthatGPT-enrichedpromptscannotsolve
thischallenge,andweincludeadetailedanalysisaboutpossiblereasons
forsuchdeficiency. WeaimforCommonsense-T2Itoserveasahigh-quality
∗ Theseauthorscontributedequallytothiswork.
1
4202
nuJ
11
]VC.sc[
1v64570.6042:viXraevaluationbenchmarkforT2Icommonsensechecking,fosteringadvance-
mentsinreallifeimagegeneration.
1 Introduction
Recent advances in generative modeling have allowed text-to-image (T2I) synthesis to
achieve drastic performance improvements (Ramesh et al., 2021; 2022; Rombach et al.,
2022; Betker et al., 2023; Li et al., 2024). While it seems that we can realize a complete
transitionbetweenatextpromptandanimage,animageisworthathousandwords,and
it is inevitable to lose information between the transition, due to the simplicity nature
of language. Therefore, it is not possible to provide a text prompt to a T2I model that
coverseverysingledetailaboutanimage–theT2Imodelmusttrytounderstandthetext
prompt first and self-imagine a real life scenario that contains every missing piece of
detail,beforegeneratingtherequiredimages. AsshowninFigure1,thevisualdifferences
of“lightbulb”between“A lightbulb without electricity”and“A lightbulb with
electricity”areblatantlyobvious–butwillthesamestillbetrueformachines? Atleast
currentT2ImodelsBetkeretal.(2023);Rombachetal.(2022);Lietal.(2024)failtoreasonthe
commonsenseforthisexample,namely“lightbulb is unlit without electricity”.
WeobservethatmanysamplesinexistingT2Igenerationevaluationsarestraightforward
compositionofobjectsandtheirattributes,e.g.“A red car and a white sheep”(Saharia
etal.,2022;Parketal.,2021;Choetal.,2023;Fengetal.,2022). Theyfocusonunderstanding
ofobject-relatedandattribute-relatedtokensintheprompts,suchassize,color,object,and
shape,andfailtocovercomplicatedcommonsensereasoningrequiredscenarios. There-
fore,itremainsunknownwhethercurrentgenerativeAIsystemscanreachhuman-level
intelligenceandgenerateimagesthatfitcommonsenseinreallife,usingexistingevaluations.
In this paper, we specifically focus on the following question: can text-to-image mod-
els generate images that fit commonsense in real life? Motivated by this, we propose a
novel evaluation task, called Commonsense-T2I, for measuring commonsense reasoning
capabilities in generative models. As illustrated in Figure 2, Commonsense-T2I presents
ahigh-qualityexpert-curatedtestset,witheachdatasamplecontainingtwoadversarial
textprompts,theircorrespondingexpectedoutputdescriptions,likelihoodscoreforeach
expectedoutput,andcommonsensecategory. Wedesignthepromptsinapairwiseformat
thatbothpromptscontainanidenticalsetofactionwordswithminordifferences,ordered
insuchawaythattheimagesmustshownoticeabledifferencestofitcommonsenseinreal
life. ToperformwellonCommonsense-T2I,T2Imodelsmustnotonlyencodethesuperfi-
cialmeaningofeachtokenintheprompts, butalsobeabletosynthesizecommonsense
reasoningacrossthetwomodalities.
OnenaturalquestionishowtoconductcommonsensereasoningassessmentonT2Imodels.
Most metrics evaluate the models on image-text alignment (Radford et al., 2021; Hessel
etal.,2021),focusingondiverssubjectsincludingfidelity(Heuseletal.,2017;Jayasumana
etal.,2023),faithfulness(Huetal.,2023),andcompositionality(Luetal.,2023c;Chen,2023).
Noneofthemexaminescommonsensereasoningingenerativemodels. Whilesomerecent
methodsevaluatemodelsbyhumanfeedback(Luetal.,2024),theyareeffort-demanding,
donotfocusoncommonsense,andonlyincluderelativecomparisonwithoutgoldanswers.
Inthispaper, wepresentanautomaticevaluationpipelineusingourcollectedexpected
outputdescriptionsandmultimodallargelanguagemodels(LLMs)(OpenAI,2023;Team
etal.,2023),thatistestedtoalignwellwithhumanperceptionsunderourevaluationmetric.
TheprimarypurposeofCommonsense-T2Iistosupportcommonsenseprobingtasksfor
T2Imodels. WeexperimentwithavarietyofgenerativemodelsincludingStableDiffusion
models, Playground v2.5, Openjourney v4, and DALL-E 3(Rombach et al., 2022; Betker
etal.,2023;Lietal.,2024). Surprisingly, thestateoftheart(sota)DALL-Emodel3only
achieves48.92%accuracy,andallothermodelshoveraround15-30%accuracy. Ourfindings
indicate that the commonsense reasoning capabilities have not emerged in existing T2I
models. Additional experiments(§3.3) show that GPT-revised enriched prompts cannot
solvetheCommonsense-T2Ichallenge,andweincludedetailedanalysisonpossiblereasons
forsuchdeficiencyacrossalltheT2Imodels.
2P1: A balloon filled with helium in the bedroom. P2: A balloon filled with air in the bedroom.
D1: The balloon is floating. D1: The balloon stays on the floor.
DALL-E 3
Playground 2.5
Stable Diffusion XL
FiguP ar f1 e te: 2A
r
:
w
c I oo ll rn u ks .str tu rc at tio ion n w oo frk oe nr e en djo ay tin ag e f xa am mily p t lim ee fr omCPo2m:m Ao cnosnesntrsuect-ioTn2 wIo,rwkehr ewroerkP. 1,P 2arepairwise
prom D1p :t Ts ha en bd allD oo1n, D
is
f2loa ar tie nge .x pected output descrDip1t: iTohne( b§a2ll)o,oanl ostnagys woni tthhes feloloerc.ted generated
imagesfromDALL-E3,Playgroundv2.5,andStableDiffusionXL.Moreexamplesin§3.3
DALL-E 3
Insummary,ourcontributionsarethreefold.(1)Weproposeahigh-qualityexpert-annotated
benchmarkforevaluatingcommonsensereasoningrequiredintext-to-imagegeneration.
(2) We propose an automatic evaluation pipeline using multimodal LLMs for the task,
andshowthatitishighlycorrelatedwithhumanevaluation. (3)Webenchmarkawide
rangeofT2ImodelsonCommonsense-T2Iandshowthatthereisstillahugegapbetween
allcurrentmodelsandhumanlevelintelligence,alongwithdetailedanalyses. Wehope
that CommPolanysgeronusned- 2T.25I will stimulate the community to help T2I models catch up with
human-levelcommonsensereasoningcapabilities,fosteringfurtherprogressinthefield.
2 TheCommonsense-T2IBenchmark
Our goal is to faithfully evaluate whether T2I models understand commonsense. We
introduceanovelbenchmark,Commonsense-T2I,designedtoenablebothquantitativeand
Stable Diffusion XL
qualitativeevaluationofthereal-lifecommonsensereasoningcapabilitiesofT2Imodels.
We unfold this section by illustrating the overall design of Commonsense-T2I (§2.1) and
discussingitsuniquefeatures.Thenweprovideanin-depthexplanationofthedatacuration
process(§2.2). Wefurtherpresenttheevaluationmetricdesignedforthetask(§2.3).
2.1 OverviewofCommonsense-T2I
Commonsense-T2Icomprisesof150manuallycuratedexamples,whereeachexamplehasa
pairofadversarialprompts: P andP ,theircorrespondingexpectedoutputdescriptions: D
1 2 1
andD ,likelihoodscoresforeachoutputtohappen,andcommonsensecategory. Complete
2
dataexampleisinA.1. AdatasamplesatisfiestheCommonsense-T2Icriteriaifandonlyif:
• P andP havethesamesetofactionwordsbutdifferentsubjectsoradjectives.
1 2
3
swaL
lacisyhP
secitcarP
namuH• P andP havethesamesubjectsbutdifferentactionwords.
1 2
• D andD arecompletelycontrastiveandcannotco-existinoneimage.
1 2
• P willleadtoD andP willleadtoD indailylifeundercommonsense.
1 1 2 2
Category Examples Percent
Aglassofwaterfellonthefloor: spilledwater.
PhysicalLaws 32.7
Ahotcupofwaterinwinter: steamrises.
Amanatawedding: insuit,lookingcheerful.
HumanPractices 30.0
Apersoneatingaburger: eatingwithhands.
Oaktreesduringwinter: noleavesonbranches.
BiologicalLaws 11.3
Awheatfieldinspring: greenfield.
Asmallbagofpartyballoonsforsale: balloonsareflat.
DailyItems 14.0
Aphonewithadrainedbattery: darkscreen.
Apeacockattractingamate: spreadingfeathers.
AnimalBehaviors 12.0
Apenguinslidingonice: slidingonitsbelly.
Table1: Commonsenseknowledgecategoriesandpercentageofdatasamples.
2.2 DatasetCollectionProcess
The Commonsense-T2I dataset is entirely hand-curated by experts. We first decide on
the categories of commonsense knowledge required for text-to-image generation, and
thenuseGPT-4(Brownetal.,2020)tohelpgeneratemultipleexamplesrequiringvisual-
commonsenseknowledgeasinspirations,andmanuallycurateeachtestsampleofpairwise
promptsandexpectedoutputs.
CommonsenseCategory. Commonsenseknowledgeisahugeportionofhumanexperience,
encompassingknowledgeaboutthespatial,physical,social,temporal,andpsychological
aspects of typical everyday life (Liu & Singh, 2004), and widely studied in AI research
(Bosselutetal.,2019;Zellersetal.,2019). TobuildCommonsense-T2I,wemanuallyselect
fivecategoriesofknowledgethatnaturallyrequirevisualcommonsenseunderstanding,as
illustratedinTable1.
InspirationGeneration. WeprompttheGPT-4-turbomodeliterativelytogenerateamas-
sivepoolofspecificexamplesasinspirationsforeachcommonsensecategory. Specifically,
wefirstuseGPT-4togeneratenaturalsentenceexamplesgivenacommonsensecategory,e.g.
“butter melts when heating”givencategoryphysicallaws. Additionally,werequireGPT
toonlyprovideexamplesthatarevisuallysalientandeasytovisualize. Forinstance,“a
bowl of milk put outside the fridge for a few days”isabadcasebecauseitentails
anon-visualcommonsenseknowledgethatmilkwouldturnsour. Foreachexample,we
promptGPT-4tolocatethesubjectandexpectedoutput,anddiscoverspecificrealscenarios
asP ,e.g. thesubjectis“butter”,scenariois“inaheatedpan”,andexpectedoutputis“butter
1
melts in a heated pan” . Finally, we ask GPT-4 to generate one counter-example for each
real scenario with a slightly different subject or different action for the same subject, as
thetemporarycandidateforP . However,thequalityofgeneratedexamplesareoftennot
2
guaranteed,andmaynottestthedesiredcommonsenseknowledge.
ManualCuration. Preparedwiththeauto-generatedexamplesasinspirations,wemanually
createthepairwisepromptsandexpectedoutputdescriptions,andverifythecommonsense
type, add the likelihood score. For the pairwise prompts, we rewrite to have natural
sentences that do not reveal the expected outputs. For instance, we turn “an untied
air balloon”into“A balloon filled with air”asinFigure2. Also,werevisetokeep
minimumdifferencebetweenP andP ,whileleadingtocontrastingoutputspercriteria.
1 2
Fortheexpectedoutputdescriptions,werewritetobefaithfultotheprompts. Forexample,
wechangedtheGPToutput“Amirrorreflectingnothingintheroom”to“Abarelyvisibleroom”
for “A mirror in a room without light”. For the likelihood score, we rate to answer
“Howmanyoutoftengeneratedimagesdowebelieveshouldshowtheexpectedoutputs?”,
anddiscardexampleswithlikelihoodscorelowerthan7.
4Data quality control: To guarantee the quality of Commonsense-T2I, we manually go
throughallcollecteddataagainandfilteroutdatathatareambiguous.
P1: Bear eating salmon.
D1 fit (I1,D1): No
A bear is in a r iver
I1: catching a fish.
A bear is eating a whole
raw salmon fish. fit (I1,D2): No
Does the image fit Incorrect!
P2: A person eating salmon. the description?
D2 fit (I2,D1): No
A person is e a ting from
fit (I1,D1): Yes
I2: a plate. Human GPT-4V fit (I1,D2): No
Correct!
A person is eating a fit (I2,D1): No
salmon fillet. fit (I2,D2): Yes
fit (I2,D2): Yes
Figure3: TheevaluationpipelineforCommonsense-T2I.MoredetailsareinSection2.3.
2.3 EvaluationMetrics
Commonsense-T2Iisdesignedasapairwisechallengesetandweevaluateperformanceof
T2Imodelsaccordingtothefollowingcriteria: onlywhenbothofthepairwisepromptsP
1
andP havethegeneratedimages I and I matchtheexpectedoutputdescriptionsD and
2 1 2 1
D atthesametime,wecountthesampleascorrect. Specifically,
2
(cid:26)
1, ifimage I fitsthedescriptionD
fit(I D ) = i j
i j
0, otherwise
isanindicatorfunctionevaluatingwhetherthegeneratedimage I fitsthedescriptionD
i j
wherei,j∈ [1,2]. Then,thescorefordatasamplencontainingpairwisepromptsPn andPn
1 2
iscalculatedas
(cid:26) 1, if fit(In Dn)+ fit(In Dn)− fit(In Dn)− fit(In Dn) =2
score = 1 1 2 2 1 2 2 1
n
0, otherwise
whereitisonlycorrectwhenthegeneratedimagesforthepairwisepromptsarebothcorrect
atthesametime. Forinstance,iftheT2ImodelgeneratesimagesforP correctlybutimages
1
forP incorrectly,namely fit(I D ) =1and fit(I D ) =0,thenweconsiderthesample
2 1 1 2 2
asincorrect,becausethemodelfailstoconducttherequiredcommonsensereasoning. In
ourexperiments,wegeneratemultipletimesforeachdatasampleandtaketheaverage
scoreforfaircomparison. Thefinalaccuracyisthencalculatedas
1 ∑N
Accuracy = score
n
N
n=1
whereNisthetotalnumberofdatasamples. DetailscanbefoundinFigure3.
3 Experiment
In this section, we first describe the baseline T2I models and experimental setup (§3.1).
Then we present a comprehensive evaluation of both human and existing multimodal
models(§3.2). WedemonstratethatwhileT2Imodelscangeneratehigh-qualityimages,
Commonsense-T2Iischallengingforexistingmodels. Finally,weprovidedetailedanalyses
on using multimodal large language models (LLMs) as auto evaluators, whether GPT-
enrichedpromptscansolvetheproblem,possiblereasonsfordificiencyincurrentmodels,
anderroranalysisacrossdifferentT2Imodels(§3.3).
5CLIP GeminiPro GPT-4preview GPT-4Turbo GPT-4o
Model/Evaluator Human
(Radfordetal.,2021)(Teametal.,2023)(OpenAI,2023)(OpenAI,2023)(OpenAI,2023)
SD-21(Rombachetal.,2022) 24.67 21.67 15.83 21.17 21.00 18.83
SD-XL(Rombachetal.,2022) 26.00 29.67 23.50 26.67 25.17 24.92
Openjourneyv4 26.83 26.67 18.83 20.17 22.33 –
Playgroundv2.5(Lietal.,2024) 24.83 29.00 18.50 20.33 25.83 –
LCMs(Luoetal.,2023) 23.33 23.50 17.17 20.67 23.83 –
DALL-E3(Betkeretal.,2023) 40.17 45.50 43.83 45.83 48.83 48.92
DALL-E3w/orevision 34.83 36.50 33.50 35.00 37.50 34.00
Table2: MainresultsontheCommonsense-T2Ichallengeset. Thecolumnsrowshowsthe
T2Imodelsthatweevaluateon,andthefirstrowshowstheevaluatorchoices. Thebest
performancemodelundereachevaluatorisin-bold.
3.1 ExperimentalSetup
BaselineT2IModels: WeevaluateCommonsense-T2IonavarieryofT2Imodels,including
twoStableDiffusion(Rombachetal.,2022)variants: (1)StableDiffusionv2.11(SD-21),and
(2)StableDiffusionXL2(SD-XL).DevelopedupontheStableDiffusionXLmodel,(3)Play-
groundv2.5(Lietal.,2024)3isincludedasitprovideshigh-qualityimagesthatarepreferred
byhumanoverStableDiffusionXLandDALL-E3(Betkeretal.,2023)asinthepaper. We
alsoinclude(4)Openjourneyv44fromPromptHero5,whichisfinetuneduponStableDiffu-
sionv1.5usingMidjourney6images,and(6)theLatentConsistencyModels(LCMs)(Luo
etal.,2023)7,whichisdistilledfromDreamshaperv78fine-tuneofStableDiffusionv1.5.For
API-basedmodels,weevaluateontheDALL-E3model(Betkeretal.,2023). TheDALL-E3
modelbydefaultenrichesandrevisesthegiventextpromptusingGPTmodel(Brownetal.,
2020)beforegeneration,addingmoredetailstothepromptsincemoredetailedprompts
generallyresultinhigherqualityimages. Therefore,weincludetwovariantsofthemodel:
(6)DALL-E3,whichistheoriginalmodel,and(7)DALL-E3w/orevision,whichturns
offtheGPT-revisionfunction–wefollowtheOpenAIinstruction9 andaddthefollowingto
our prompts: I NEED to test how the tool works with extremely simple prompts.
DO NOT add any detail, just use it AS-IS: {prompt}.
Evaluation Protocol: We assign two experts (coauthors) for each data sample
inCommonsense-T2Iandpresenttheiraveragescoresashumanperformance. Asstatedin
Section2.3,evaluatorsareexpectedtoseparatelydeterminewhetherthecontentinimageIn
1
fitsthedescriptionDn,andalsowhetherimage In fitsthedescriptionDn,fordatasamplen.
1 2 2
Inordertoconductfaircomparisonandavoidrandomness,wegeneratetheimagesforeach
datasamplefourtimes,evaluateseparately,andtaketheaveragescoreforeachdatasample
tocalculatefinalaccuracy. Namely,score isaveragedoverfourtimesofgenerations.
n
Noticethatduetothedemandingnatureofthetask,wehaveconductedmanualevaluations
onlyonthefollowingmodels: SD-21,SD-XL,DALL-E3,andDALL-E3w/orevision.
Automatic Evaluation: Due to the effort-demanding nature of human evaluation, we
experimentwithmultimodalLLMstoconductautomaticevaluationonCommonsense-T2I.
Two models are tested in our paper: GPT-4V(ision) (OpenAI, 2023), which is known to
1https://huggingface.co/stabilityai/stable-diffusion-2-1
2https://huggingface.co/docs/diffusers/en/using-diffusers/sdxl
3https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic
4https://huggingface.co/prompthero/openjourney-v4
5https://prompthero.com/
6https://www.midjourney.com/home
7https://huggingface.co/SimianLuo/LCM_Dreamshaper_v7
8https://huggingface.co/Lykon/dreamshaper-7
9https://platform.openai.com/docs/guides/images/prompting
6CLIP GeminiPro GPT-4o Human DALL-E 3 SD XL SD v2.1
0.8
40
0.6
30 0.4
20 0.2
DALL-E 3 0.2 0.4 0.6 0.8
SD-21 SD-XL w/o revision DALL-E 3 CLIP_Similarity(P1, P2)
Figure4:Comparisonbetweenusingmulti- Figure5: IllustrationoftheCLIPembedding
modalLLMsvs.humansasevaluators. We similarity of prompts P , P against human
1 2
canseethattheevaluatorsCLIPandGem- evaluatedperformancescores.Itsuggeststhat
iniPro are both not ideal. GPT-4o, how- T2Imodelsperformbadlywhentheirtexten-
ever,consistentlyprovidessimilarevalua- codersfailtodifferentiatebetweenP andP ,
1 2
tion scores to humans and can be a good andperformwellwhenP andP arecorrectly
1 2
candidateforautomaticevaluation. embeddedtobefar.
be one of the most powerful multimodal models to date, and we evaluate using three
checkpoint models: gpt-4-vision-preview, gpt-4-turbo, and gpt-4o; GeminiPro (Team
et al., 2023), which is one of the most widely used multimodal models, and we use the
Gemini1.0ProVisionversionofit. Specifically,foreachimage I anddescriptionD,weget
fit(I D) with the following prompt: Can you tell me if the image generally fits
the descriptions "{description}"? If it generally fits the descriptions,
then return 1, otherwise, return 0. Give me number 1 or 0 only. Notice that
in several cases where the multimodal LLM fails to tell fit(I D) correctly and believes
the image fits both expected output descriptions D and D , we randomly select one
1 2
descriptionsuchthat fit(I D )+ fit(I D ) =1.
1 2
WealsoincludeCLIP(Radfordetal.,2021)(ViT-L/14)asanadditionalevaluator,considering
thatitisoneofthemostwidelyusedmultimodalencoders. Specifically,wegiveCLIPthe
generatedimage I andtwoexpectedoutputdescriptionsD andD ,andlet fit(I D ) =
1 2 1
1, fit(I D ) =0ifthecosinesimilaritybetween I andD islargerthanthatbetween I and
2 1
D ,or fit(I D ) =0, fit(I D ) =1ifviceversa.
2 1 2
3.2 MainResults
HumanEvalutionResults: AsillustratedinTable2,themeanaccuracyofopen-source
stablediffusionmodelshoverbelow25%,withtheXLmodelachievinga6.09%improve-
mentoverthev2.1model. WesurprisinglyfindthateventhesotaDALL-E3modelonly
achieves48.92%,meaningthatitfailsonthecommonsensechallengeformorethanhalf
ofthecasesinCommonsense-T2I.AfterturningofftheGPTrevisionfunctionofDALL-E3,
DALL-E3w/orevisionachieves15.92%worsethantheDALL-E3performance,suggesting
thepossibleimprovementsbroughtbyenrichedtextprompts.
Automatic Evaluation Results: Can Multimodal LLMs replace human evaluators? As
showninTable2,withthemostadvancedmodels,GPT-4oandGeminiPro,wecanachieve
automaticevaluationperformancessimilartothatofhumans,withthemaximumdifference
being5.09%and5.25%respectively. Theirperformancetrendsareconsistentwiththatof
humans,asshowninFigure4.Notably,GPT-4Vconsistentlyrateslowerthanhumans,while
GeminiProalwaysrateshigherthanhumansexceptonDALL-E3images. Webelievethat
multimodalLLMs,especiallyGPT-4V,canrepresenttheT2Imodelperformancesgenerally.
However,CLIPevaluationscoresarerelativelydivergentfromthatofhuman,andcannot
representthetrendsofT2Imodelperformenceswell.
7
erocsprompt: A lightbulb without electricity
DALL-E 3 expected: Lightbulb is unlit DALL-E 3 w/o revision
revised prompt: Visualize a clear glass lightbulb. The filament
inside is untouched and frail, but there's no glow emanating from it.
The metallic base is screwed tightly, waiting for the moment to be set
in a power source. However, currently, it's devoid of electricity.
Around it, the surroundings are dark and still, amplifying its ghostly
inert state. The lightbulb stands as a testament of technology, yet it's
symbolizing a paradoxical illustration of darkness in this instance.
Please show it prominently, taking center stage in this image.
prompt: a pen placed in a cup of water
DALL-E 3 DALL-E 3 w/o revision
expected: The submerged part of the pen appears bent
revised prompt: An image depicting a realistic scenario where a
ballpoint pen is immersed in a clear glass cup filled with water. The
cup should be resting on a flat and clean surface, and light should be
reflecting off the water and the glass, giving them a glossy look. The
pen inside the water is distorted due to the refraction of light.
Shadows from pen and the cup are visible on the surface.
prompt: an hourglass just finishing its counting
DALL-E 3 expected: Sand is mostly in the bottom part DALL-E 3 w/o revision
revised prompt: An ultra realistic image of a vintage hourglass
right at the end of its countdown, the last grain of sand dropping
from the top chamber to the bottom one. The hourglass has an
ornate design and is crafted from dark polished wood and glistening,
clear glass. The fine grains of sand within it are golden in color. It
stands on a richly varnished wooden table and behind it is a nearly
fading neutral background.
prompt: a sundae untouched for several hours
DALL-E 3 expected: The ice cream has melted into liquid form DALL-E 3 w/o revision
revised prompt: Generate a detailed and hyper realistic image of
a sundae that has been left untouched for several hours. It should
portray clear signs of melting and decomposition of its ingredients
which typically includes whipped cream, ice cream of various flavors,
sauces like chocolate and caramel, nuts, cherries, and a glass serving
dish. There should be visible signs of the condensation on the outer
surface of the dish. The background can include a table surface with
some splatters and drops of melted ice cream and ingredients.
Figure 6: Error cases of DALL-E 3. Prompts and expected outputs are in the green box.
DALL-E 3 images are generated with the revised prompts returned by DALL-E 3, and
DALL-E3w/orevisionimagesaregeneratedwiththeoriginalprompt. Thehighlighted
sentencesare(partially)correctexpectedoutputdescriptionsinrevisedprompts.
3.3 Analysis
Are T2I models limited by text embedding? Since all the Stable Diffusion (based) T2I
modelsscoreunder35%accuracyonCommonsense-T2I,weinvestigatethepossiblereason
behindthisphenomena:thesemodelsmightbebiasedbythetextembeddingoftheprompts.
Themotivationisfollows:iftheembeddingsofP andP ,whichareinputstotheT2Imodels,
1 2
areverysimilar,thentheycouldleadtheT2ImodelstogeneratesimilarimagesforP and
1
P ,whiletheexpectedoutputsshoulddifferent. WedeploytheCLIP(Radfordetal.,2021)
2
(ViT/L14)encoder,whichisthedefaulttextencoderforStableDiffusion(based)models,
toencodethepairwisepromptsP andP inCommonsense-T2I.Wecomparethesimilarity
1 2
betweenCLIPembeddingofP andP againstperformancescoreasinFigure5. Noticethat
1 2
weadoptmin-maxnormalizationtoprojecttheembeddingsimilarityvaluesinto[0,1].
8Can GPT augmented prompts solve the Commonsense-T2I problems? To answer this
question,weanalyzetheerrorcasesofDALL-E3,whichautomaticallyusesGPT-augmented
revised-prompts,andcheckwhethertherevisedpromptsincludecorrectexpectedoutputs.
As in Figure 6, we show the difference between DALL-E 3 outputs and DALL-E 3 w/o
revision outputs, along with the GPT-revised prompt used by DALL-E 3. We can see
that in the first two failed cases, the GPT revised prompts can provide the exact correct
expected output information; while in the last two cases, they provide partially correct
information. Inshort,GPTaugmentedpromptscanhelptosomeextent,withDALL-E3
achieving14.92%improvementoverDALL-E3w/orevision. However,theycannotsolve
theCommonsense-T2Ichallenge–theyeitherfailtoprovidecomprehensivecorrectdetails,
ortheT2Ipartfailstovisualizethecorrectdetails.
DodifferentT2Imodelsmakesameerrors? TheStableDiffusionbasedmodels:SD-21,SD-
XL,Playgroundv2.5,andOpenjourneyv4failonmostsamplesinCommonsense-T2I,even
for the easy ones, such as “prompt: A peacock sleeping, expected: The peacock
has its feathers closed” from “Animal Behaviors”. Additional failed examples are
showninFigure9. Meanwhile,DALL-E3,asillustratedinFigure6,oftenfailsonmore
complicatedcases,e.g.itmostlysucceedson“Animal Behaviors”and“Biological Laws”
samples and fails on uncommon situations with commonly seen objects, such as unlit
lightbulbs, distinguished candles, fully melted ice cream,...,etc.
4 RelatedWork
T2I Models and Benchmarks Text-to-image synthesis models are typically trained to
generateimagesconditionedontext. EarlystudieswidelyuseGANs(Reedetal.,2016b;a).
Starting from DALL·E (Ramesh et al., 2021), image generation models started to show
impressiveresults. Recently,diffusionmodelshaveachievedremarkablesuccessontext-
guided image generation (Nichol et al., 2021; Saharia et al., 2022; Ramesh et al., 2022;
Rombachetal.,2022;Lietal.,2024). Anotherseriesofwork(Luetal.,2023b;Zhuetal.,
2023) enables the collaboration between the LLMs and text-to-image models. Multiple
benchmarksevaluatingdifferentaspectsofT2Imodelshavebeenintroduced. Mostofthem
focus on straighforward text prompts, e.g. “a read book and a yellow vase.”, and evaluate
directvisualattributessuchascounting,color,andshape(Sahariaetal.,2022;Parketal.,
2021). Others focus on object detection and relations (Cho et al., 2023; Bakr et al., 2023),
compositionality: presenceofmultipleobjectsandtheirattributes(Bakretal.,2023;Huang
etal.,2023),andfairness(Choetal.,2023;Bakretal.,2023). Butnoneofthemevaluates
multimodalcommonsenseunderstandingofgenerativemodels.
T2IEvaluation Mostmetricsevaluatethemodelsonfidelity(Salimansetal.,2016;Heusel
etal.,2017;Jayasumanaetal.,2023),image-textalignment(Radfordetal.,2021;Hesseletal.,
2021; Li et al., 2023), Recent metrics try to use large language models (LLMs) (Lu et al.,
2023c;Zhangetal.,2023;Chen,2023),orVQA(Huetal.,2023),orhuman(Kuetal.,2023;
Luetal.,2024)forevaluation. However,thereisnocomprehensivestudyonhowwellthose
evaluationmetricsworkforcommonsenseT2Igeneration. Weproposeevaluationmetrics
specificallydesignedforourtaskandvalidatethatourproposedmetricsalignwellwith
humanperceptions.
MultimodalLargeLanguageModels WiththerecentdevelopmentofmultimodalLarge
Language Models (LLMs) (Alayrac et al., 2022; Li et al., 2023; Liu et al., 2023a; OpenAI,
2023;Liuetal.,2023b;Teametal.,2023;Zhangetal.,2023),morereasoning-relatedresearch
questionsaboutmultimodalityhavebeenstudied(Thrushetal.,2022;Marinoetal.,2019;
Fu et al., 2022; Lu et al., 2023a; Fu et al., 2023a; Gupta & Kembhavi, 2023; Sur´ıs et al.,
2023;Fuetal.,2024;2023b). Onepaper(Zhangetal.,2022)studiesvisualcommonsense
knowledgeinpretrainedmultimodalmodels. However,Thereisnocomprehensivestudies
oncommonsensereasoninginT2Imodels.
95 Conclusion
WeintroduceCommonsense-T2I,anoveltaskforevaluatingcommonsensereasoningabili-
tiesofT2Imodels. Weprovideahigh-qualityexpert-annotatedtestsetforthetaskincluding
pairwisepromptsandexpectedoutputs. OurexperimentsshowthatcurrentT2Imodels
scorebetween15-50%onourdataset,fosteringfutureresearchinthisdirection.
Limitations WewouldliketoemphasizethatthesizeofCommonsense-T2Iislimitedbythe
needtomanuallyreviseallthesamples(eachincludingfiveentries)byexperts. Wepropose
Commonsense-T2I as a high-quality expert-curated test set and believe it can serve as a
goodevaluationbenchmarkforthegoalofthispaper. Nevertheless,usingtheinspiration
generationmethodin§2.2,onecaneasilygeneratehugeamountofweak-supervisiondata.
References
Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,
KarelLenc,ArthurMensch,KatherineMillican,MalcolmReynolds,etal. Flamingo: a
visuallanguagemodelforfew-shotlearning. AdvancesinNeuralInformationProcessing
Systems,35:23716–23736,2022.
EslamMohamedBakr,PengzhanSun,XiaogianShen,FaizanFarooqKhan,LiErranLi,and
MohamedElhoseiny. Hrs-bench: Holistic,reliableandscalablebenchmarkfortext-to-
imagemodels. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,
pp.20041–20053,2023.
JamesBetker, GabrielGoh, LiJing, TimBrooks, JianfengWang, LinjieLi, LongOuyang,
JuntangZhuang,JoyceLee,YufeiGuo,etal. Improvingimagegenerationwithbetter
captions. ComputerScience.https://cdn.openai.com/papers/dall-e-3.pdf,2(3):8,2023.
AntoineBosselut,HannahRashkin,MaartenSap,ChaitanyaMalaviya,AsliCelikyilmaz,
and Yejin Choi. Comet: Commonsense transformers for automatic knowledge graph
construction. arXivpreprintarXiv:1906.05317,2019.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Languagemodelsarefew-shotlearners. Advancesinneuralinformationprocessingsystems,
33:1877–1901,2020.
YixiongChen. X-iqe: explainableimagequalityevaluationfortext-to-imagegeneration
withvisuallargelanguagemodels. arXivpreprintarXiv:2305.10843,2023.
JaeminCho,AbhayZala,andMohitBansal.Dall-eval:Probingthereasoningskillsandsocial
biasesoftext-to-imagegenerationmodels. InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision,pp.3043–3054,2023.
WeixiFeng,XuehaiHe,Tsu-JuiFu,VarunJampani,ArjunAkula,PradyumnaNarayana,
SugatoBasu,XinEricWang,andWilliamYangWang. Training-freestructureddiffusion
guidanceforcompositionaltext-to-imagesynthesis. arXivpreprintarXiv:2212.05032,2022.
XingyuFu,BenZhou,IshaanChandratreya,CarlVondrick,andDanRoth. There’satime
andplaceforreasoningbeyondtheimage. InProceedingsofthe60thAnnualMeetingofthe
AssociationforComputationalLinguistics(Volume1: LongPapers),pp.1138–1149,2022.
XingyuFu,ShengZhang,GukyeongKwon,PramudithaPerera,HenghuiZhu,YuhaoZhang,
AlexanderHanboLi,WilliamYangWang,ZhiguoWang,VittorioCastelli,PatrickNg,
DanRoth,andBingXiang. Generatethenselect: Open-endedvisualquestionanswering
guidedbyworldknowledge. InFindingsoftheAssociationforComputationalLinguistics:
ACL2023,pp.2333–2346.AssociationforComputationalLinguistics,July2023a.
XingyuFu,BenZhou,SihaoChen,MarkYatskar,andDanRoth. Interpretablebydesign
visualquestionanswering. arXivpreprintarXiv:2305.14882,2023b.
10XingyuFu,YushiHu,BangzhengLi,YuFeng,HaoyuWang,XudongLin,DanRoth,NoahA
Smith,Wei-ChiuMa,andRanjayKrishna. Blink: Multimodallargelanguagemodelscan
seebutnotperceive. arXivpreprintarXiv:2404.12390,2024.
Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual
reasoningwithouttraining. InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition(CVPR),pp.14953–14962,June2023.
JackHessel,AriHoltzman,MaxwellForbes,RonanLeBras,andYejinChoi. Clipscore: A
reference-freeevaluationmetricforimagecaptioning. arXivpreprintarXiv:2104.08718,
2021.
MartinHeusel,HubertRamsauer,ThomasUnterthiner,BernhardNessler,andSeppHochre-
iter. Ganstrainedbyatwotime-scaleupdateruleconvergetoalocalnashequilibrium.
Advancesinneuralinformationprocessingsystems,30,2017.
YushiHu,BenlinLiu,JungoKasai,YizhongWang,MariOstendorf,RanjayKrishna,and
NoahASmith. Tifa: Accurateandinterpretabletext-to-imagefaithfulnessevaluation
withquestionanswering. arXivpreprintarXiv:2303.11897,2023.
Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: A
comprehensivebenchmarkforopen-worldcompositionaltext-to-imagegeneration. arXiv
preprintarXiv:2307.06350,2023.
Sadeep Jayasumana, Srikumar Ramalingam, Andreas Veit, Daniel Glasner, Ayan
Chakrabarti, and Sanjiv Kumar. Rethinking fid: Towards a better evaluation metric
forimagegeneration. arXivpreprintarXiv:2401.09603,2023.
MaxKu,TianleLi,KaiZhang,YujieLu,XingyuFu,WenwenZhuang,andWenhuChen.
Imagenhub: Standardizingtheevaluationofconditionalimagegenerationmodels,2023.
DaiqingLi,AleksKamko,EhsanAkhgari,AliSabet,LinmiaoXu,andSuhailDoshi. Play-
groundv2.5: Threeinsightstowardsenhancingaestheticqualityintext-to-imagegenera-
tion. arXivpreprintarXiv:2402.17245,2024.
JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. Blip-2: Bootstrappinglanguage-
imagepre-trainingwithfrozenimageencodersandlargelanguagemodels. arXivpreprint
arXiv:2301.12597,2023.
HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee. Improvedbaselineswithvisual
instructiontuning,2023a.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning,
2023b.
HugoLiuandPushSingh. Conceptnet—apracticalcommonsensereasoningtool-kit. BT
technologyjournal,22(4):211–226,2004.
YujieLu,XiujunLi,WilliamYangWang,andYejinChoi. Vim: Probingmultimodallarge
languagemodelsforvisualembeddedinstructionfollowing,2023a.
Yujie Lu, Pan Lu, Zhiyu Chen, Wanrong Zhu, Xin Eric Wang, and William Yang Wang.
Multimodalproceduralplanningviadualtext-imageprompting,2023b.
Yujie Lu, Xianjun Yang, Xiujun Li, Xin Eric Wang, and William Yang Wang. Llmscore:
Unveiling the power of large language models in text-to-image synthesis evaluation.
arXivpreprintarXiv:2305.11116,2023c.
Yujie Lu, Dongfu Jiang, Wenhu Chen, William Wang, Yejin Choi, and Bill Yuchen Lin.
Wildvision arena: Benchmarking multimodal llms in the wild, February 2024. URL
https://huggingface.co/spaces/WildVision/vision-arena/.
Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency
models: Synthesizing high-resolution images with few-step inference. arXiv preprint
arXiv:2310.04378,2023.
11KennethMarino,MohammadRastegari,AliFarhadi,andRoozbehMottaghi. Ok-vqa: A
visualquestionansweringbenchmarkrequiringexternalknowledge. InConferenceon
ComputerVisionandPatternRecognition(CVPR),2019.
Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob
McGrew,IlyaSutskever,andMarkChen. Glide: Towardsphotorealisticimagegeneration
andeditingwithtext-guideddiffusionmodels. arXivpreprintarXiv:2112.10741,2021.
OpenAI. Gpt-4technicalreport,2023.
DongHukPark,SamanehAzadi,XihuiLiu,TrevorDarrell,andAnnaRohrbach.Benchmark
forcompositionaltext-to-imagesynthesis. InThirty-fifthConferenceonNeuralInformation
ProcessingSystemsDatasetsandBenchmarksTrack(Round1),2021.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini
Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferablevisualmodelsfromnaturallanguagesupervision. InInternationalconference
onmachinelearning,pp.8748–8763.PMLR,2021.
AdityaRamesh,MikhailPavlov,GabrielGoh,ScottGray,ChelseaVoss,AlecRadford,Mark
Chen,andIlyaSutskever. Zero-shottext-to-imagegeneration. InInternationalConference
onMachineLearning,pp.8821–8831.PMLR,2021.
AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,andMarkChen. Hierarchical
text-conditionalimagegenerationwithcliplatents. arXivpreprintarXiv:2204.06125,1(2):3,
2022.
ScottReed,ZeynepAkata,XinchenYan,LajanugenLogeswaran,BerntSchiele,andHonglak
Lee. Generativeadversarialtexttoimagesynthesis. InInternationalconferenceonmachine
learning,pp.1060–1069.PMLR,2016a.
ScottEReed,ZeynepAkata,SantoshMohan,SamuelTenka,BerntSchiele,andHonglak
Lee. Learningwhatandwheretodraw. Advancesinneuralinformationprocessingsystems,
29,2016b.
RobinRombach, AndreasBlattmann, DominikLorenz, PatrickEsser, andBjo¨rnOmmer.
High-resolution image synthesis with latent diffusion models. In Proceedings of the
IEEE/CVFconferenceoncomputervisionandpatternrecognition,pp.10684–10695,2022.
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,
KamyarGhasemipour,RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,etal.
Photorealistictext-to-imagediffusionmodelswithdeeplanguageunderstanding. Ad-
vancesinNeuralInformationProcessingSystems,35:36479–36494,2022.
TimSalimans,IanGoodfellow,WojciechZaremba,VickiCheung,AlecRadford,andXiChen.
Improvedtechniquesfortraininggans. Advancesinneuralinformationprocessingsystems,
29,2016.
D´ıdac Sur´ıs, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python
executionforreasoning. arXivpreprintarXiv:2303.08128,2023.
GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,Jiahui
Yu,RaduSoricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamily
ofhighlycapablemultimodalmodels. arXivpreprintarXiv:2312.11805,2023.
TristanThrush,RyanJiang,MaxBartolo,AmanpreetSingh,AdinaWilliams,DouweKiela,
andCandaceRoss. Winoground: Probingvisionandlanguagemodelsforvisio-linguistic
compositionality. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pp.5238–5248,2022.
RowanZellers,YonatanBisk,AliFarhadi,andYejinChoi. Fromrecognitiontocognition:
Visual commonsense reasoning. In Proceedings of the IEEE/CVF conference on computer
visionandpatternrecognition,pp.6720–6731,2019.
12ChenyuZhang,BenjaminVanDurme,ZhuowanLi,andEliasStengel-Eskin.Visualcommon-
senseinpretrainedunimodalandmultimodalmodels. arXivpreprintarXiv:2205.01850,
2022.
XinluZhang,YujieLu,WeizhiWang,AnYan,JunYan,LiankeQin,HengWang,XifengYan,
WilliamYangWang,andLindaRuthPetzold. Gpt-4v(ision)asageneralistevaluatorfor
vision-languagetasks,2023.
Wanrong Zhu, Xinyi Wang, Yujie Lu, Tsu-Jui Fu, Xin Eric Wang, Miguel Eckstein, and
WilliamYangWang. Collaborativegenerativeai: Integratinggpt-kforefficienteditingin
text-to-imagegeneration,2023.
A Appendix
A.1 CompleteDataExample
AcompletedatasampleinCommonsense-T2Ilooksasthefollowing:
P : A birthday cake after making a wish
1
P : A birthday cake before making a wish
2
D : The candles are extinguished
1
D : The candles on the cake are lit
2
category: Human Practices
likelihood:9
A.2 DatasetInspirationGenerationPrompts
WeillustratethepromptsweusedtogeneratethedatainspirationsfromGPT-4-turboas
following: Figure7and8.
A.3 ErrorCasesExamples
InFigure9,weillustratesomeerrorcasesbySD-XLandPlaygroundv2.5.
13Figure7: Promptsweusetogeneratedatainspirations. (1/2)
Figure8: Promptsweusetogeneratedatainspirations. (2/2)
14prompt: A person eating salmon
expected: A person is eating from a plate; A person is eating a salmon fillet
Playground 2.5 Stable Diffusion XL
prompt: A piece of butter in a heated pan
expected: The piece of butter is melting
Playground 2.5 Stable Diffusion XL
prompt: A flag on a pole on a windless day
expected: The flag is hanging down from the pole
Playground 2.5 Stable Diffusion XL
prompt: A cup of coffee with no milk
expected: The cup contains black coffee
Playground 2.5 Stable Diffusion XL
Figure 9: Error cases of SD-XL andPlayground v2.5. The prompt and expected output
descriptionareprovidedingreenboxforeachexample.
15