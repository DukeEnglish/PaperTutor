OK-Robot:
What Really Matters in Integrating Open-Knowledge
Models for Robotics
Peiqi Liu*1 Yaswanth Orru*1 Chris Paxton2 Nur Muhammad Mahi Shafiullah†1 Lerrel Pinto†1
New York University1, AI at Meta2
https://ok-robot.github.io
“OK Robot, move the Takis on the desk to the nightstand”
1.3 V.o DxerloMpa p 1 2. 1L.a Vnogxe-SlMAaMp +
(“TN aP ka irv sii g oma nt i tio htn eiP v dla een sk”) 3 2 Gr( a“T sAN a pk (na <iv s Ryi g o GaG n Bt i to Dhrn ea >P ,d sl “a e Tnp s ak k” i) s”)
Drop(<RGBD>, “nightstand”)
11. .V VooxxeellMMaapp
NNaavviiggaattiioonnPPllaann
((““TTaakkiiss oonn tthhee ddeesskk””))
1 2 1
3
2
1
2
3
3
“Move the soda can to “Move the purple “Move the white meds
the box” shampoo to the red bag” box to the trash bin”
Fig. 1: OK-Robot is an Open Knowledge robotic system, which integrates a variety of learned models trained on publicly available data, to
pick and drop objects in real-world environments. Using Open Knowledge models such as CLIP, Lang-SAM, AnyGrasp, and OWL-ViT,
OK-Robot achieves a 58.5% success rate across 10 unseen, cluttered home environments, and 82.4% on cleaner, decluttered environments.
4202
naJ
22
]OR.sc[
1v20221.1042:viXraAbstract—Remarkableprogresshasbeenmadeinrecentyears VLMs and robotics primitives, while being flexible enough to
in the fields of vision, language, and robotics. We now have incorporate newer models as they are developed by the VLM
vision models capable of recognizing objects based on language
and robotics community.
queries, navigation systems that can effectively control mobile
We present OK-Robot, an Open Knowledge Robot that
systems, and grasping models that can handle a wide range of
objects.Despitetheseadvancements,general-purposeapplications integrates state-of-the-art VLMs with powerful robotics prim-
of robotics still lag behind, even though they rely on these itives for navigation and grasping to enable pick-and-drop.
fundamental capabilities of recognition, navigation, and grasping. Here, Open Knowledge refers to learned models trained on
Inthispaper,weadoptasystems-firstapproachtodevelopanew
large, publicly available datasets. When placed in a new home
OpenKnowledge-basedroboticsframeworkcalledOK-Robot.By
environment, OK-Robot is seeded with a scan taken from an
combining Vision-Language Models (VLMs) for object detection,
navigation primitives for movement, and grasping primitives iPhone. Given this scan, dense vision-language representations
for object manipulation, OK-Robot offers a integrated solution are computed using LangSam [24] and CLIP [9] and stored
for pick-and-drop operations without requiring any training. To in a semantic memory. Then, given a language-query for an
evaluate its performance, we run OK-Robot in 10 real-world
object that has to be picked, language representations of the
home environments. The results demonstrate that OK-Robot
query is matched with semantic memory. After this, navigation
achieves a 58.5% success rate in open-ended pick-and-drop tasks,
representing a new state-of-the-art in Open Vocabulary Mobile and picking primitives are applied sequentially to move to the
Manipulation (OVMM) with nearly 1.8× the performance of desired object and pick it up. A similar process can be carried
prior work. On cleaner, uncluttered environments, OK-Robot’s out for dropping the object.
performance increases to 82%. However, the most important
To study OK-Robot, we tested it in 10 real world home
insight gained from OK-Robot is the critical role of nuanced
detailswhencombiningOpenKnowledgesystemslikeVLMswith environments. Through our experiments, we found that on a
robotic modules. never seen, natural home environment, a zero-shot deployment
of our system achieves 58.5% success on average. However,
I. INTRODUCTION this success rate is largely dependant on the “naturalness” of
Creating a general-purpose robot has been a longstanding the environment, as we show that with improving the queries,
dream of the robotics community. With the increase in data- decluttering the space, and excluding objects that are clearly
driven approaches and large robot models, impressive progress adversarial(toolarge,tootranslucent,tooslippery),thissuccess
is being made [1–4]. However, current systems are brittle, rate reaches about 82.4%. Overall, through our experiments,
closed, and fail when encountering unseen scenarios. Even we make the following observations:
the largest robotics models can often only be deployed in • Pre-trained VLMs are highly effective for open-
previously seen environments [5, 6]. The brittleness of these vocabulary navigation: Current open-vocabulary vision-
systems is further exacerbated in settings where little robotic language models such as CLIP [9] or OWL-ViT [8] offer
data is available, such as in unstructured home environments. strong performance in identifing arbitrary objects in the real
The poor generalization of robotic systems lies in stark world, and enable navigating to them in a zero-shot manner
contrasttolargevisionmodels[7–10],whichshowcapabilities (see Section II-A.)
of semantic understanding [11–13], detection [7, 8], and • Pre-trained grasping models can be directly applied to
connecting visual representations to language [9, 10, 14] mobile manipulation: Similar to VLMs, special purpose
At the same time, base robotic skills for navigation [15], robot models pre-trained on large amounts of data can be
grasping[16–19],andrearrangement[20,21]arefairlymature. appliedoutoftheboxtoapproachopen-vocabularygrasping
Hence, it is perplexing that robotic systems that combine in homes. These robot models do not require any additional
modern vision models with robot-specific primitives perform training or fine-tuning (see Section II-B.)
so poorly. To highlight the difficulty of this problem, the • How components are combined is crucial: Given the
recent NeurIPS 2023 challenge for open-vocabulary mobile pretrained models, we find that they can be combined with
manipulation (OVMM) [22] registered a success rate of 33% no training using a simple state-machine model. We also
for the winning solution [23]. find that using heuristics to counteract the robot’s physical
So what makes open-vocabulary robotics so hard? Unfortu- limitationscanleadtoabettersuccessrateintherealworld
nately, there isn’t a single challenge that makes this problem (see Section II-D.)
hard. Instead, inaccuracies in different components compound • Several challenges still remain: While, given the im-
and together results in overall drop. For example, the quality mense challenge of operating zero-shot in arbitrary homes,
of open-vocabulary retrievals of objects in homes is dependent OK-Robot improves upon prior work, by analyzing the
on the quality of query strings, navigation targets determined failuremodeswefindthattherearesignificantimprovements
from VLMs may not be reachable to the robot, and the choice that can be made on the VLMs, robot models, and robot
of different grasping models may lead to large differences in morphology,thatwilldirectlyincreaseperformanceofopen-
graspingperformance.Hence,makingprogressonthisproblem knowledge manipulation agents (see Section III-D).
requires a careful and nuanced framework that both integrates
To encourage and support future work in open-knowledge
robotics, we will share the code and modules for OK-Robot,
*Denotesequalcontributionand†denotesequaladvising.
Correspondenceto:mahi@cs.nyu.edu and are committed to supporting reproduction of our results.More information along with robot videos are available on our collected by the camera, giving us a point cloud where each
project website: https://ok-robot.github.io. point has an associated semantic vector coming from CLIP.
Then, we voxelize the point cloud to a 5 cm resolution and for
II. TECHNICALCOMPONENTSANDMETHOD
each voxel, calculate the detector-confidence weighted average
Our method, on a high level, solves the problem described for the CLIP embeddings that belong to that voxel. This voxel
by the query: “Pick up A (from B) and drop it on/in C”, map builds the base of our object memory module. Note that
where A is an object and B and C are places in a real-world the representation created this way remains static after the
environment such as homes. The system we introduce is a first scan, and cannot be adapted during the robot’s operation.
combination of three primary subsystems combined on a Hello This inability to dynamically create a map is discussed in our
Robot: Stretch. Namely, these are the open-vocabulary object limitations section (Section V).
navigation module, the open-vocabulary RGB-D grasping Querying the memory module: Our semantic object memory
module,andthedroppingheuristic.Inthissection,wedescribe gives us a static world representation represented as possibly
each of these components in more details. non-empty voxels in the world, and a semantic CLIP vector
associatedwitheachvoxel.Givenalanguagequery,weconvert
A. Open-home, open-vocabulary object navigation
it to a semantic vector using the CLIP language encoder. Then,
The first component of our method is an open-home, open- we find the top voxel where the dot product between the
vocabularyobjectnavigationmodelthatweusetomapahome encoded vector and the voxel’s semantic representation is
and subsequently navigate to any object of interest designated maximized. Since each voxel is associated with a real location
by a natural language query. in the home, this lets us find the location where a queried
Scanning the home: For open vocabulary object navigation, object is most likely to be found, similar to Figure 2(a).
we follow the approach from CLIP-Fields [27] and assume Whennecessary,weimplement“AonB”as“AnearB”.We
a pre-mapping phase where the home is “scanned” manually do so by selecting top-10 points for query A and top-50 points
using an iPhone. This manual scan simply consists of taking for query B. Then, we calculate the 10×50 pairwise euclidean
a video of the home using the Record3D app on the iPhone, distances and pick the A-point associated with the shortest (A,
which results in a sequence of posed RGB-D images. B) distance. Note that, during the object navigation phase, we
Alternatively,thiscouldbedoneautomaticallyusingfrontier- use this query only to navigate to the object approximately,
based exploration [15, 25, 26], but for speed and simplicity we and not for manipulation.
prefer the manual approach [26, 27]. We take this approach This approach gives us two advantages: our map can be as
since the frontier-based approaches tend to be slow and lower resolution than those in prior work [26, 27, 30], and
cumbering, especially for a novel space, while our “scan” take we can deal with small movements in object’s location after
lessthanoneminuteforeachroom.Oncecollected,theRGB-D building the map.
images,alongwiththecameraposeandpositions,areexported Navigating to objects in the real world: Once our navigation
to our library for map-building. model gives us a 3D location coordinate in the real world,
To ensure our semantic memory contains both the objects of we use that as a navigation target for our robot to initialize
interest as well as the navigable surface and any obstacles, the our manipulation phase. In previous works [15, 27, 31], the
recording must capture the floor surface alongside the objects navigation objective was to go and look at an object, which
and receptacles in the environment. can be done while staying at a safe distance from the object
Detecting objects:Oneachframeofthescan,werunanopen- itself. In contrast, our navigation module must place the robot
vocabulary object detector. Unlike previous works which used at an arms length so that the robot can manipulate the target
Detic [7], we chose OWL-ViT [8] as the object detector since object afterwards. Thus, our navigation method has to balance
we found it to perform better in preliminary queries. We apply the following objectives:
the detector on every frame, and extract each of the object 1) the robot needs to be close enough to the object to
bounding box, CLIP-embedding, detector confidence, and pass manipulate it,
themontotheobjectmemorymoduleofournavigationmodule. 2) the robot needs some space to move its gripper, so there
Building on top of previous work [27], we further refine needs to be a small but non-negligible space between the
the bounding boxes into object masks with Segment Anything robot and the objectm and
(SAM) [28]. Note that, in many cases, open-vocabulary object 3) the robot needs to avoid collision during manipulation,
detectors still require a set of natural language object queries and thus needs to keep its distance from all obstacles.
that they try to detect. We supply a large set of such object We use three different navigation score functions, each associ-
queries, derived from the original Scannet200 labels [29], such ated with one of our previous concerns, and evaluate them on
that the detector captures most common objects in the scene. each point of the space to find the best position to place the
Object-centric semantic memory: We use an object-centric robot.
memory similar to Clip-Fields [27] and OVMM [25] that we Let a random point be ⃗x, the closest obstacle point as ⃗x ,
obs
call the VoxelMap. The object masks are back-projected in and the target object as ⃗x . Then, we can define the following
o
real-world coordinates using the depth image and the pose three functions s ,s ,s to capture our three criterion. Then,
1 2 3Plant
Red body
Brown bag spray
Pink powder
Umbrella bottle
Brown hat Orange Brown hat
laundry bag Start point
Bed
(a) Open-vocabulary object localization (b) Open-vocabulary navigation planning
using VoxelMap using VoxelMap and heuristics weighted A*
Fig. 2: Open-vocabulary, open knowledge object localization and navigation in the real-world. We use the VoxelMap [25] for localizing
objects with natural language queries, and use an A* algorithm similar to USANet [26] for path planning.
their weighted sum s to find the ideal navigation point x⃗∗ in Grasp perception:Oncetherobotreachestheobjectusingthe
our space that minimizes s(⃗x), and the direction is towards navigationmethodoutlinedinSectionII-A,weuseapre-trained
the vector from x⃗∗ to x⃗ . grasping model or heuristic to generate a grasp for the robot.
o
We point the robot’s RGB-D head camera towards the object’s
s (⃗x)=||⃗x−⃗x ||
1 o location in space, as given to us by our semantic memory
s 2(⃗x)=40−min(||⃗x−⃗x o||,40) module,andcaptureanRGB-Dimagefromit(Figure3,column
(cid:40) 1).Webackprojectandconvertthedepthimagetoapointcloud
1/||⃗x−⃗x ||, if ||⃗x−⃗x || ≤30
s 3(⃗x)= obs obs 0 as necessary. Then, we pass this information to our grasp
0, otherwise
generation module. The grasp generation module that we use
s(⃗x)=s 1(⃗x)+8s 2(⃗x)+8s 3(⃗x) in our work is AnyGrasp [19], which generates collision free
grasps with a parallel jaw gripper in a scene given a single
To navigate to this target point safely from any other point in
RGB image and a pointcloud.
space, we follow a similar approach to [26, 32] by building
AnyGrasp provides us with possible grasps in the scene
an obstacle map from our previously captured posed RGB-D
(Figure 3 column 2) with grasp point, width, height, depth,
images. We build a 2D, 10cm×10cm grid of obstacles over
and a “graspness score”, which indicates uncalibrated model
which we navigate using the A* algorithm. To convert our
confidence in each grasp. However, such modules generally
voxel map to an obstacle map, we first set a floor and ceiling
generate all possible grasps in a scene, which we need to filter
height. Presence of occupied voxels in between them implies
using the language query.
the grid cell is occupied, while presence of neither ceiling nor
floor voxels mean that the grid cell is unexplored. We mark Filtering grasps using language queries: Once we get all
both occupied or unexplored cells as not navigable. Around proposed grasps from AnyGrasp, we filter the grasps using
each occupied point, we mark any point within a 20 cm radius LangSam [24]. We use LangSam [24] to segment the captured
as also non-navigable to account for the robot’s radius and a image and get the desired object’s mask with the language
turn radius. In our A* algorithm, we use the s function as a query (Figure 3 column 3). Then, we project all the proposed
3
heuristic on the node costs to navigate further away from any grasp points onto the image and find the grasps that fall into
obstacles, which makes our generated paths similar to ideal the object mask (Figure 3 column 4).
Voronoi paths [33] in our experiments. We pick the best grasp using a heuristic, where if the grasp
score is S and the angle between the grasp normal and floor
B. Open-vocabulary grasping in the real world
normal is θ, then the new heuristic score is S−(θ4/10). This
Unlikeopen-vocabularynavigation,forgrasping,ourmethod heuristic prioritizes grasps with the highest graspness score but
needs to physically interact with arbitrary objects in the real also a horizontally flat proposed grasp. We prefer horizontal
world, which makes this part significantly more difficult. As grasps because they are robust to small calibration errors on
a result, we opt for using a pre-trained grasping model to the robot, while vertical grasps need to be quite point-accurate
generate grasp poses in the real world, and filter that with to be successful. Being robust to hand-eye calibration errors is
language-conditioning using a modern VLM. a desired property as we transport the robot to different homesRobot view AnyGrasp proposals LangSam mask Grasp filtering Final grasp
Fig. 3: Open-vocabulary grasping in the real world. From left to right, we show the (a) robot POV image, (b) all suggested grasps from
AnyGrasp [19], (c) object mask given label from LangSam [24], (d) grasp points filtered by the mask, and (e) grasp chosen for execution.
over the course of our experiments. LangSam [24] similar to Section II-B using the drop language
Grasp execution: Once we identify the best grasp (Figure 3 query. Then, we align that segmented point cloud such that
X-axis is aligned with the way the robot is facing, Y-axis
column 5), we use a simple pre-grasp approach [34] to grasp
our intended object. Let’s assume that →− p is the grasp point is to its left and right, and the Z-axis of the point cloud is
and →− a is the approach vector given by our grasping model. aligned with the floor normal. We call this aligned pointcloud
P .Finally,wenormalizethepointcloudsothattherobot’sX-
Then, our robot gripper follows the following trajectory: a
and Y- coordinate is (0,0), and the floor plane is at z =0. On
→− →− →− →− →− →− →−
⟨p −0.2a, p −0.08a, p −0.04a, p⟩ the aligned, segmented point cloud, we consider the X- and
Y-coordinates for each point, and find the respective medians
Put simply, our method approaches the object from a pre-grasp
on each axis that we call x and y . Finally, we find a drop
position in a line with progressively smaller motions. Moving m m
height using z = 0.2+max{z | (x,y,z) ∈ P ;0 ≤ x ≤
slower as we approach the object is important since the robot max a
x ;|y −y | < 0.1} on the segmented, aligned pointcloud.
can knock over light objects otherwise. Once we reach the m m
We add a small buffer of 0.2 to the height to avoid collisions
predicted grasp point, we close the gripper in a close loop
between the robot and the drop location. Finally, we move the
fashion to make sure we can get a solid grip on the object
robot gripper above the drop point, and open the gripper to
without crushing it. Finally, after grasping the object, we lift
drop the object. While this heuristic sometimes fails to place
up the robot arm, retract it fully, and rotate the wrist to have
an object on a cluttered surface, in our experiments it performs
the object tucked over the body. This behavior maintains the
well on average.
robot footprint while ensuring the object is held securely by
the robot and doesn’t fall while navigating to the drop location.
D. Deployment in homes
C. Dropping heuristic
Once we have our navigation, pick, and drop primitive in
After picking up an object, we find and navigatte to the place, we combine them directly to create our robot method
location to drop it using the same methods as described in that can be applied in any novel home directly. For a new
Section II-A. Unlike in HomeRobot’s baseline implementa- home environment, we can “scan” the room in under a minute.
tion [25], which assumes that the drop-off location is a flat Then, it takes less than five minutes to process that into our
surface, we extend our heuristic to also cover concave objects VoxelMap. For our ablations, it takes about 50 minutes to train
such as sink, bins, boxes, and bags. First, we segment the the necessary implicit semantic fields/SDF models such as
point cloud P captured by the robot’s head camera using CLIP-Fields or USA-Net if we are using them. Once that isdone,therobotcanbeimmediatelyplacedatthebaseandstart objectssourcedfromeachhome.Asaresult,eachofthesuccess
operating. From arriving into a completely novel environment and the failure of the robot tells us something interesting about
to start operating autonomously in it, our system takes under applying open-knowledge models in robotics, which is what
10 minutes on average to complete the first pick-and-drop task. we analyze over the next sections.
In Appendix C, we provide the details of all our home
State machine model: The transition between different mod-
experiments and results from the same, and in Appendix B we
ules happens automatically, in a predefined fashion, once a
showasubsetoftheobjectsOK-Robotoperatedon.Snippets
user specifies the object to pick and where to drop it. Since
of our experiments are in Figure 1, and full videos can be seen
we do not implement error detection or correction, our state
on our project website.
machine model is a simple linear chain of steps leading from
navigating to object, to grasping, to navigating to goal, and to
B. Ablations over system components
dropping the object at the goal to finish the task.
Apart from the navigation and manipulation strategies that
Protocol for home experiments: To run our experiment in a
we used in the home experiments, we also evaluated a number
novelhome,wefirstmovetherobottoapreviouslyunobserved
of alternative semantic memory module and open vocabulary
room. There, we record the scene and create our VoxelMap.
navigation modules. We compared them by evaluating them in
Concurrently, we arbitrarily pick between 10-20 objects in
three different environment setups in our lab.
each scene that can fit in the robot gripper. These are objects
Alternate semantic navigation strategies: We evaluate the
“found” in the scene, and are not ones selected beforehand.
following semantic memory modules:
We come up with a language query for each chosen object
using GPT-4V [35] to keep the queries consistent and free of • VoxelMap [25]: VoxelMap converts every detected object
to a semantic vector and stores such info into an associated
experimenter bias. The effect of different queries for the same
object on OK-Robot is discussed in Section III-D. Then, we voxel. Occupied voxels serve as an obstacle map.
query our navigation module to filter out all the navigation • CLIP-Fields [27]: CLIP-Fields converts a sequence of
posed RGB-D images to a semantic vector field by using
failures; i.e. objects whose location could not be found by our
open-label object detectors and semantic language embed-
semantic memory module. Then, we execute pick-and-drop on
ding models. The result associates each point in the space
remaining objects sequentially, without resets between trials.
with two semantic vectors, one generated via a VLM [9],
III. EXPERIMENTS and another generated via a language model [36], which is
then embedded into a neural field [37].
We evaluate our method in two set of experiments. On
• USA-Net [26]: USA-Net generates multi-scale CLIP fea-
the first set of experiments, we evaluate between multiple
tures and embeds them in a neural field that also doubles
alternatives for each of our navigation and manipulation
as a signed distance field. As a result, a single model can
modules. These experiments give us insights about which
support both object retrieval and navigation.
modules to use and evaluate in a home environment as a
We compare them in the same three environments with a fixed
part of our method. On the next set of experiments, we took
set of queries, the results of which are shown in Figure 5.
ourrobotsto10homesandran171pick-and-dropexperiments
toempiricallyevaluatehowourmethodperformsincompletely Alternate grasping strategies:Similarly,wecomparemultiple
novelhomes,andtounderstandthefailuremodesofoursystem. grasping strategies to find out the best grasping strategy for
Through these experiments, we look to answer a series our method.
of questions regarding the capabilities and limits of current • AnyGrasp [19]: AnyGrasp is a single view RGB-D based
OpenKnowledgeroboticsystems,asembodiedbyOK-Robot. grasping model. It is trained on the GraspNet dataset which
Namely, we ask the following: contains 1B grasp labels.
1) How well can such a system tackle the challenge of pick • Open Graspness [19]: Since the AnyGrasp model is free
and drop in arbitrary homes? but not open source, we use an open licensed baseline
2) How well do alternate primitives for navigation and trained on the same dataset.
graspingcomparetotherecipepresentedhereforbuilding • Contact-GraspNet [16]: We use Contact-GraspNet as a
an Open Knowledge robotic system? prior work baseline, which is trained on the Acronym [38]
3) How well can our current systems handle unique chal- dataset. One limitation of Contact-GraspNet is that it was
lenges that make homes particularly difficult, such as trained on a fixed camera view for a tabletop setting. As a
clutter, ambiguity, and affordance challenges? result,inourapplicationwithamovingcameraandarbitrary
4) What are the failure modes of such a system and its locations, it failed to give us meaningful grasps.
individual components in real home environments? • Top-down grasp [25]: As a heuristic based baseline, we
compare with the top-down heuristic grasp provided in the
A. List of home experiments HomeRobot project.
Over the 10 home environment, OK-Robot achieved a In Figure 5, we see their comparative performance in three
58.5%successratesincompletingfullpick-and-drops.Notably, lab environments. For semantic memory modules, we see that
this is a zero-shot algorithm, and the success rate is over novel VoxelMap, used in OK-Robot and described in Sec. II-A,Fig. 4: All the success and failure cases in our home experiments, aggregated over all three cleaning phases, and broken down by mode of
failure. From left to right, we show the application of the three components of OK-Robot, and show a breakdown of the long-tail failure
modes of each of the components.
Success Manipulation failure
Semantic memory module
Navigation failure Placing failure
VoxelMap
Clip fields none 58 15 25
USA Net low 71 12 16
Grasping module
high 82 4 13
AnyGrasp
AnyGrasp 0 20 40 60 80 100
Open Source Percentage of trials
Top down
Fig. 6: Failure modes of our method in novel homes, broken down
0 20 40 60 80 100 by the failures of the three modules and the cleanup levels.
Fig. 5: Ablation experiment using different semantic memory and
grasping modules, with the bars showing average performance and
the error bars showing standard deviation over the environments. C. Impact of clutter, object ambiguity, and affordance
What makes home environments especially difficult com-
pared to lab experiments is the presence of physical clutter,
language-to-object mapping ambiguity, and hard-to-reach posi-
outperformsothersemanticmemorymodulesbyasmallmargin. tions. To gain a clear understanding of how such factors play
It also has much lower variance compared to the alternatives, into our experiments, we go through two “clean-up” processes
meaningitismorereliable.Asforgraspingmodules,AnyGrasp in each environment. During the clean-up, we pick a subset of
clearlyoutperformsothergraspingmethods,performingalmost objects that are free from ambiguity from the previous rounds,
50% better in a relative scale over the next best candidate, top- clean the clutter around objects, and generally relocated them
down grasp. However, the fact that a heuristic-based algorithm, inanaccessiblelocations.Wegothroughtwoofsuchclean-up
top-down grasp from HomeRobot [25] beats the open-source rounds at each environment, which gives us insights about
AnyGrasp baseline and Contact-GraspNet shows that building the performance gap caused by the natural difficulties of a
a truly general-purpose grasping model remains difficult. home-like environment.
level
punaelCObject is transparent, so Diagonal grasp on a
Top-down grasp on tall
pointcloud is imperfect, cylindrical object is
counter is unreachable
so grasp is imperfect unstable
Fine grasps on small Grasps on round objects Grasps on flat objects
objects are vulnerable to are unstable when not collide with env if not
Fig. 7: Samples of failed or ambiguous language queries into our calibration errors perfectly diametrical perfectly executed
semantic memory module. Since the memory module depends on Fig.8:Samplesoffailuresofourmanipulationmodule.Mostfailures
pretrained large vision language model, its performance shows stemfromusingonlyasingleRGB-Dviewtogeneratethegraspand
susceptibility to particular “incantations” similar to current LLMs. the limiting form-factor of a large two-fingered parallel jaw gripper.
WeshowacompleteanalysisofthetaskslistedsectionIII-A reasons our OK-Robot can fail is when a natural language
which failed in various stages in Figure 6. As we can see from query given by the user doesn’t retrieve the intended object
this breakdown, as we clean up the environment and remove from the semantic memory. In Figure 7 we show how some
the ambiguous objects, the navigation accuracy goes up, and queries may fail while semantically very similar but slightly
the total error rate goes down from 15% to 12% and finally modified wording of the same query might succeed.
all the way down to 4%. Similarly, as we clean up clutters Generally, this has been the case for scenes where there
from the environment, we find that the manipulation accuracy are multiple visually or semantically similar objects, as shown
also improves and the error rates decrease from 25% to 16% in the figure. There are other cases where some queries may
and finally 13%. Finally, since the drop-module is agnostic pass while other very similar queries may fail. An interactive
of the label ambiguity or manipulation difficulty arising from system that gets confirmation from the user as it retrieves an
clutter, the failure rate of the dropping primitive stays roughly object from memory would avoid such issues.
constant through the three phases of cleanup. Grasping module limitations: One potential failure mode of
our system is that our manipulation is performed by executing
D. Understanding the performance of OK-Robot
the outputs of a pre-trained model-generated grasps that are
While our method can show zero-shot generalization in predicted based on a single RGB-D image, with a model that
completely new environments, we probe OK-Robot to better wasn’t designed for the Hello Robot: Stretch gripper.
understand its failure modes. Primarily, we elaborate on how As a result, sometimes such grasps are unreliable or
our model performed in novel homes, what were the biggest unrealistic, as shown in Figure 8. There are cases where the
challenges, and discuss potential solutions to them. proposed grasp is infeasible given the robot joint limits, or
We first show a coarse-level breakdown of the failures, only is simply too far from the robot body. Development of better
considering the three high level modules of our method in heuristics will let us sample better grasps for a given object.
Figure 6. We see that generally, the leading cause of failure is In some other cases, the model generates a good grasp pose,
our manipulation failure, which intuitively is the most difficult but as the robot is executing the grasping primitive, it collides
as well. However, at a closer look, we notice a long tail of with some minor environment obstacle. Since we do not plan
failure causes, which is presented in figure 4. the grasp trajectory, and instead try to apply the same grasp
We see that the leading three cause of failures are failing trajectory in every case, some such failures are inevitable.
to retrieve the right object to navigate to from the semantic Better grasping models that generates a grasp trajectory as
memory (9.3%), getting a difficult pose from the manipulation well as a pose may solve such issues. Finally, our grasping
module(8.0%),andhardwaredifficulties(7.5%).Inthissection, module struggles with flat objects categorically, like chocolate
we go over the analysis of the failure modes presented in bars and books, since it’s difficult to grasp them off a surface
Figure 4 and discuss the most frequent cases. with a two-fingered gripper.
Natural language queries for objects: One of the primary Robot hardware limitations: While our robot of choice, aHello Robot: Stretch, is able to pick-and-drop a number of Morerecently,thereisasetofgeneral-purposemanipulation
objects, there are certain hardware limitations that determines models moving beyond just grasping [57–61]. Some of these
what the robot can and cannot manipulate. For example, the works perform general language-conditioned manipulation
robot has a 1 kg (2 lbs) payload limit when the arm is fully tasks, but are largely limited to a small set of scenes and
extended, and as such our method is unable to move objects objects. HACMan [62] demonstrates a larger range of object
like a full dish soap container. Similarly, objects that are far manipulation capabilities, focused on pushing and prodding. In
from navigable floor space, such as in the middle of a bed, or the future, such models could expand the reach of our system.
on high places, is difficult for the robot to reach because of the
reach limits of the arm. Finally, in some situations, the robot C. Open vocabulary robot systems
hardware or the RealSense camera can become miscalibrated
Many recent works have worked on language-enabled
over time, especially during continuous testing in homes. This
tasks for complex robot systems. Some examples include
miscalibration can lead to error since the manipulation module
languageconditionedpolicylearning[57,63–65],learninggoal-
requires hand-eye coordination in the robot.
conditioned value functions [3, 66], and using large language
models to generate code [67–69]. However, a fundamental
IV. RELATEDWORKS
difference remains between systems which aim to operate
A. Vision-Language models for robotic navigation on arbitrary objects in an open-vocab manner, and systems
where one can specify one among a limited number of goals
Early applications of pre-trained open-knowledge models
or options using language. Consequently, Open-Vocabulary
has been in open-vocabulary navigation. Navigating to various
Mobile Manipulation has been proposed as a key challenge for
objectsisanimportanttaskwhichhasbeenlookedatinawide
robotic manipulation [25]. There has previously been efforts to
rangeofpreviousworks[25,31,39],aswellasinthecontextof
build such a system [70, 71]. However, unlike such previous
longer pick-and-place tasks [40, 41]. However, these methods
work,wetrytobuildeverythingonanopenplatformandensure
have generally been applied to relatively small numbers of
our method can work without having to re-train anything for a
objects [42]. Recently, Objaverse [43] has shown navigation
novelhome.Recently,UniTeam[23]wonthe2023HomeRobot
to thousands of object types, for example, but much of this
OVMM Challenge [22] with a modular system doing pick-
work has been restricted to simulated or highly controlled
and-place to arbitrary objects, with a zero-shot generalization
environments.
requirement similar to ours.
Theearlyworkaddressingthisproblembuildsuponrepresen-
In parallel, recently, there have been a number of papers
tations derived from pre-trained vision language models, such
doing open-vocabulary manipulation using GPT or especially
as SemAbs [44], CLIP-Fields [27], VLMaps [45], NLMap-
GPT4 [35]. GPT4V can be included in robot task planning
SayCan [46], and later, ConceptFusion [47] and LERF [30].
frameworks and used to execute long-horizon robot tasks,
Most of these models show object localization in pre-mapped
including ones from human demonstrations [72]. Concept-
scenes, while CLIP-Fields, VLMaps, and NLMap-SayCan
Graphs [49] is a good recent example, showing complex
show integration with real robots for indoor navigation tasks.
objectsearch,planning,andpick-and-placecapabilitiestoopen-
USA-Nets [26] extends this task to include an affordance
vocabularyobjects.SayPlan[73]alsoshowshowthesecanuse
model, navigating with open-vocabulary queries while doing
usedtogetherwithascenegraphtohandleverylarge,complex
object avoidance. ViNT [48] proposes a foundation model for
environments,andmulti-steptasks;thisworkiscomplementary
robotic navigation which can be applied to vision-language
to ours, as it doesn’t handle how to implement pick and place.
navigation problems. More recently, GOAT [31] was proposed
as a modular system for “going to anything” and navigating to
V. LIMITATIONS,OPENPROBLEMSAND
any object in any environment. ConceptGraphs [49] proposed
REQUESTFORRESEARCH
an open scene representation capable of handling complex
queries using LLMs and creating a scene graph. While our method shows significant success in completely
novel home environments, it also shows many places where
B. Pretrained robot manipulation models such methods can improve. In this section, we discuss a few
of such potential improvement in the future.
Whilehumanscanfrequentlylookatobjectsandimmediately
know how to grasp it, such grasping knowledge is not easily
A. Live semantic memory and obstacle maps
accessible to robots. Over the years, there has been many
works that has focused on creating such a general robot grasp All the current semantic memory modules and obstacle map
generationmodel[1,50–55]forarbitraryobjectsandpotentially builders build a static representation of the world, without
cluttered scenes via learning methods. Our work focuses on a good way of keeping it up-to-date as the world changes.
morerecentiterationsofsuchmethods[16,19]thataretrained However, homes are dynamic environments, with many small
on large grasping datasets [18, 38]. While these models only changes over the day every day. Future research that can build
perform one task, namely grasping, they predict grasps across a dynamic semantic memory and obstacle map would unlock
a large object surface and thus enable downstream complex, potential for continuous application of such pick-and-drop
long-horizon manipulation tasks [20, 21, 56]. methods in a novel home out of the box.B. Grasp plans instead of proposals Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog,
et al. Do as I can, not as I say: Grounding language
Currently, the grasping module proposes generic grasps
in robotic affordances. Conference on Robot Learning
without taking the robot’s body and dynamics into account.
(CoRL), 2022.
Similarly, given a grasp pose, often the open loop grasping
[4] Nur Muhammad Mahi Shafiullah, Anant Rai, Haritheja
trajectory collides with environmental obstacles, which can be
Etukuru, Yiqian Liu, Ishan Misra, Soumith Chintala, and
easily improved by using a module to generate grasp plans
Lerrel Pinto. On bringing robots home, 2023.
rather than grasp poses only.
[5] Anthony Brohan, Noah Brown, Justice Carbajal, Yev-
C. Improving interactivity between robot and user
gen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana
One of the major causes of failure in our method is in Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine
navigation: where the semantic query is ambiguous and the Hsu, et al. Rt-1: Robotics transformer for real-world
intended object is not retrieved from the semantic memory. In control at scale. arXiv preprint arXiv:2212.06817, 2022.
such ambiguous cases, interaction with the user would go a [6] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen
longwaytodisambiguatethequeryandhelptherobotsucceed Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding,
more often. Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2:
Vision-language-actionmodelstransferwebknowledgeto
D. Detecting and recovering from failure
robotic control. arXiv preprint arXiv:2307.15818, 2023.
Currently, we observe a multiplicative error accumulation
[7] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp
between our modules: if any of our independent components Kra¨henbu¨hl, and Ishan Misra. Detecting twenty-thousand
fail, the entire process fails. As a result, even if our modules classes using image-level supervision. In European
each perform independently at or above 80% success rate, ConferenceonComputerVision,pages350–368.Springer,
our final success rate can still be below 60%. However, with
2022.
better error detection and retrying algorithms, we can recover
[8] Matthias Minderer, Alexey Gritsenko, Austin Stone,
from much more single-stage errors, and similarly improve our
Maxim Neumann,Dirk Weissenborn, AlexeyDosovitskiy,
overall success rate [23].
Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani,
E. Robustifying robot hardware Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf,
and Neil Houlsby. Simple open-vocabulary object detec-
While Hello Robot - Stretch [74] is an affordable and
tion with vision transformers. In European Conference
portable platform on which we can implement such an open-
on Computer Vision, pages 728–755. Springer, 2022.
home system for arbitrary homes, we also acknowledge that
[9] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
with robust hardware such methods may have vastly enhanced
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
capacity. Such robust hardware may enable us to reach high
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
and low places, and pick up heavier objects. Finally, improved
Krueger, and Ilya Sutskever. Learning transferable
robot odometry will enable us to execute much more finer
visual models from natural language supervision. In
grasps than is possible today.
International Conference on Machine Learning (ICML),
ACKNOWLEDGMENTS volume 139, pages 8748–8763, 2021.
NYU authors are supported by grants from Amazon, Honda, [10] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and
and ONR award numbers N00014-21-1-2404 and N00014-21- Roozbeh Mottaghi. Ok-vqa: A visual question answering
1-2758. NMS is supported by the Apple Scholar in AI/ML benchmark requiring external knowledge. In Proceedings
oftheIEEE/cvfconferenceoncomputervisionandpattern
Fellowship. LP is supported by the Packard Fellowship. Our
utmostgratitudegoestoourfriendsandcolleagueswhohelped recognition, pages 3195–3204, 2019.
usbyhostingourexperimentsintheirhomes.Finally,wethank [11] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-
JayVakil,SiddhantHaldar,PaulaPascualandUlyanaPiterbarg toine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur
for valuable feedback and conversations. Mensch,KatieMillican,MalcolmReynolds,RomanRing,
ElizaRutherford,SerkanCabi,TengdaHan,ZhitaoGong,
REFERENCES
Sina Samangooei, Marianne Monteiro, Jacob Menick,
[1] Lerrel Pinto and Abhinav Gupta. Supersizing self- Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh,
supervision: Learning to grasp from 50k tries and 700 SahandSharifzadeh,MikolajBinkowski,RicardoBarreira,
robot hours. ICRA, 2016. Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.
[2] SergeyLevine,PeterPastor,AlexKrizhevsky,JulianIbarz, Flamingo: a visual language model for few-shot learning,
and Deirdre Quillen. Learning hand-eye coordination for 2022.
robotic grasping with deep learning and large-scale data [12] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao
collection. The International journal of robotics research, Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su,
37(4-5):421–436, 2018. Jun Zhu, et al. Grounding DINO: Marrying DINO with
[3] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen groundedpre-trainingforopen-setobjectdetection. arXiv
Chebotar, Omar Cortes, Byron David, Chelsea Finn, preprint arXiv:2303.05499, 2023.[13] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae [25] Sriram Yenamandra, Arun Ramachandran, Karmesh Ya-
Lee. Visual instruction tuning, 2023. dav, Austin Wang, Mukul Khanna, Theophile Gervet,
[14] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Tsung-Yen Yang, Vidhi Jain, Alexander William Clegg,
Dario Amodei, and Ilya Sutskever. Language models are John Turner, et al. Homerobot: Open-vocabulary mobile
unsupervised multitask learners. OpenAI Blog, 2019. manipulation. arXiv preprint arXiv:2306.11565, 2023.
[15] Theophile Gervet, Soumith Chintala, Dhruv Batra, Jiten- [26] Benjamin Bolte, Austin Wang, Jimmy Yang, Mustafa
draMalik,andDevendraSinghChaplot. Navigatingtoob- Mukadam, Mrinal Kalakrishnan, and Chris Paxton. Usa-
jects in the real world. Science Robotics, 8(79):eadf6991, net: Unified semantic and affordance representations for
2023. robot memory. arXiv preprint arXiv:2304.12164, 2023.
[16] Martin Sundermeyer, Arsalan Mousavian, Rudolph [27] Nur Muhammad Mahi Shafiullah, Chris Paxton, Lerrel
Triebel, and Dieter Fox. Contact-graspnet: Efficient 6- Pinto, Soumith Chintala, and Arthur Szlam. Clip-fields:
dof grasp generation in cluttered scenes. In 2021 IEEE Weakly supervised semantic fields for robotic memory.
International Conference on Robotics and Automation arXiv preprint arXiv:2210.05663, 2022.
(ICRA), pages 13438–13444. IEEE, 2021. [28] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi
[17] Jeffrey Mahler, Jacky Liang, Sherdil Niyaz, Michael Mao,ChloeRolland,LauraGustafson,TeteXiao,Spencer
Laskey, Richard Doan, Xinyu Liu, Juan Aparicio Ojea, Whitehead,AlexanderC.Berg,Wan-YenLo,PiotrDollar,
and Ken Goldberg. Dex-net 2.0: Deep learning to plan and Ross Girshick. Segment anything. In ICCV, pages
robust grasps with synthetic point clouds and analytic 4015–4026, October 2023.
grasp metrics. arXiv preprint arXiv:1703.09312, 2017. [29] David Rozenberszki, Or Litany, and Angela Dai.
[18] Hao-Shu Fang, Chenxi Wang, Minghao Gou, and Cewu Language-grounded indoor 3d semantic segmentation in
Lu. Graspnet-1billion: a large-scale benchmark for the wild, 2022.
general object grasping. In Proceedings of the IEEE/CVF [30] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo
conference on computer vision and pattern recognition, Kanazawa, and Matthew Tancik. Lerf: Language embed-
pages 11444–11453, 2020. ded radiance fields. In Proceedings of the IEEE/CVF
[19] Hao-Shu Fang, Chenxi Wang, Hongjie Fang, Minghao International Conference on Computer Vision, pages
Gou, Jirong Liu, Hengxu Yan, Wenhai Liu, Yichen 19729–19739, 2023.
Xie, and Cewu Lu. Anygrasp: Robust and efficient [31] Matthew Chang, Theophile Gervet, Mukul Khanna, Sri-
grasp perception in spatial and temporal domains. IEEE ram Yenamandra, Dhruv Shah, So Yeon Min, Kavit Shah,
Transactions on Robotics, 2023. Chris Paxton, Saurabh Gupta, Dhruv Batra, et al. Goat:
[20] Ankit Goyal, Arsalan Mousavian, Chris Paxton, Yu-Wei Go to any thing. arXiv preprint arXiv:2311.06430, 2023.
Chao, Brian Okorn, Jia Deng, and Dieter Fox. Ifor: [32] Chenguang Huang, Oier Mees, Andy Zeng, and Wolfram
Iterative flow minimization for robotic object rearrange- Burgard. Audiovisuallanguagemapsforrobotnavigation.
ment. In Proceedings of the IEEE/CVF Conference on arXiv preprint arXiv:2303.07522, 2023.
Computer Vision and Pattern Recognition, pages 14787– [33] Santiago Garrido, Luis Moreno, Mohamed Abderrahim,
14797, 2022. and Fernando Martin. Path planning for mobile robot
[21] Weiyu Liu, Tucker Hermans, Sonia Chernova, and Chris navigation using voronoi diagram and fast marching. In
Paxton. Structdiffusion: Object-centric diffusion for 2006 IEEE/RSJ International Conference on Intelligent
semantic rearrangement of novel objects. arXiv preprint Robots and Systems, pages 2376–2381. IEEE, 2006.
arXiv:2211.04604, 2022. [34] Sudeep Dasari, Abhinav Gupta, and Vikash Kumar.
[22] SriramYenamandra,ArunRamachandran,MukulKhanna, Learning dexterous manipulation from exemplar object
Karmesh Yadav, Devendra Singh Chaplot, Gunjan Chh- trajectories and pre-grasps, 2023.
ablani, Alexander Clegg, Theophile Gervet, Vidhi Jain, [35] OpenAI. GPT-4 technical report, 2023.
Ruslan Partsey, Ram Ramrakhya, Andrew Szot, Tsung- [36] Nils Reimers and Iryna Gurevych. Sentence-bert: Sen-
Yen Yang, Aaron Edsinger, Charlie Kemp, Binit Shah, tence embeddings using siamese bert-networks, 2019.
ZsoltKira,DhruvBatra,RoozbehMottaghi,YonatanBisk, [37] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
and Chris Paxton. The homerobot open vocab mobile JonathanTBarron,RaviRamamoorthi,andRenNg. Nerf:
manipulation challenge. In Thirty-seventh Conference Representing scenes as neural radiance fields for view
on Neural Information Processing Systems: Competition synthesis. European Conference on Computer Vision
Track, 2023. (ECCV), 65(1):99–106, 2020.
[23] Andrew Melnik, Michael Bu¨ttner, Leon Harz, Lyon [38] Clemens Eppner, Arsalan Mousavian, and Dieter Fox.
Brown, Gora Chand Nandi, Arjun PS, Gaurav Kumar Acronym:Alarge-scalegraspdatasetbasedonsimulation.
Yadav, Rahul Kala, and Robert Haschke. Uniteam: Open In 2021 IEEE International Conference on Robotics and
vocabularymobilemanipulationchallenge. arXivpreprint Automation (ICRA), pages 6222–6227. IEEE, 2021.
arXiv:2312.08611, 2023. [39] Arsalan Mousavian, Alexander Toshev, Marek Fisˇer, Jana
[24] Luca Medeiros. Lang segment anything. https://github. Kosˇecka´, Ayzaan Wahid, and James Davidson. Visual
com/luca-medeiros/lang-segment-anything, 2023. representations for semantic target driven navigation. In2019 International Conference on Robotics and Automa- and Ken Goldberg. Dex-Net 2.0: Deep learning to plan
tion (ICRA), pages 8846–8852. IEEE, 5 2019. robust grasps with synthetic point clouds and analytic
[40] ValtsBlukis,ChrisPaxton,DieterFox,AnimeshGarg,and grasp metrics. In Robotics: Science and Systems (RSS),
Yoav Artzi. A persistent spatial semantic representation 2017.
for high-level natural language instruction execution. In [52] Jeffrey Mahler, Matthew Matl, Xinyu Liu, Albert Li,
Conference on Robot Learning, pages 706–717. PMLR, DavidGealy,andKenGoldberg. Dex-net3.0:Computing
2022. robust robot vacuum suction grasp targets in point clouds
[41] So Yeon Min, Devendra Singh Chaplot, Pradeep Raviku- using a new analytic model and deep learning, 2018.
mar, Yonatan Bisk, and Ruslan Salakhutdinov. Film: [53] Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian
Followinginstructionsinlanguagewithmodularmethods. Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen,
arXiv preprint arXiv:2110.07342, 2021. Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke,
[42] Matt Deitke, Dhruv Batra, Yonatan Bisk, Tommaso et al. QT-Opt: Scalable deep reinforcement learning
Campari, Angel X Chang, Devendra Singh Chaplot, for vision-based robotic manipulation. arXiv preprint
Changan Chen, Claudia Pe´rez D’Arpino, Kiana Ehsani, arXiv:1806.10293, 2018.
Ali Farhadi, et al. Retrospectives on the embodied ai [54] Yuzhe Qin, Rui Chen, Hao Zhu, Meng Song, Jing Xu,
workshop. arXiv preprint arXiv:2210.06849, 2022. and Hao Su. S4g: Amodal single-view single-shot se(3)
[43] MattDeitke,DustinSchwenk,JordiSalvador,LucaWeihs, grasp detection in cluttered scenes, 2019.
Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana [55] Arsalan Mousavian, Clemens Eppner, and Dieter Fox.
Ehsani,AniruddhaKembhavi,andAliFarhadi. Objaverse: 6-dof graspnet: Variational grasp generation for object
A universe of annotated 3d objects. In Proceedings of the manipulation, 2019.
IEEE/CVF Conference on Computer Vision and Pattern [56] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu,
Recognition, pages 13142–13153, 2023. J. Tremblay, and D. Fox. Progprompt: Generating
[44] Huy Ha and Shuran Song. Semantic abstraction: Open- situated robot task plans using large language models.
world 3d scene understanding from 2d vision-language In 2023 IEEE International Conference on Robotics and
models, 2022. Automation (ICRA), page 11523, 2023.
[45] Chenguang Huang, Oier Mees, Andy Zeng, and Wolfram [57] Mohit Shridhar, Lucas Manuelli, and Dieter Fox.
Burgard. Visual language maps for robot navigation. In Perceiver-Actor: A multi-task transformer for robotic
2023 IEEE International Conference on Robotics and manipulation. In CoRL, pages 785–799. PMLR, 2023.
Automation (ICRA), pages 10608–10615. IEEE, 2023. [58] Priyam Parashar, Jay Vakil, Sam Powers, and Chris
[46] Boyuan Chen, Fei Xia, Brian Ichter, Kanishka Rao, Paxton. Spatial-language attention policies for efficient
Keerthana Gopalakrishnan, Michael S. Ryoo, Austin robot learning. arXiv preprint arXiv:2304.11235, 2023.
Stone, and Daniel Kappler. Open-vocabulary queryable [59] Nur Muhammad Shafiullah, Zichen Cui, Ariuntuya Arty
scene representations for real world planning. In arXiv Altanzaya, and Lerrel Pinto. Behavior transformers:
preprint arXiv:2209.09874, 2022. Cloning k modes with one stone. Advances in neural
[47] Krishna Murthy Jatavallabhula, Alihusein Kuwajerwala, information processing systems, 35:22955–22968, 2022.
Qiao Gu, Mohd Omama, Tao Chen, Shuang Li, Ganesh [60] Zichen Jeff Cui, Yibin Wang, Nur Muhammad Mahi
Iyer, Soroush Saryazdi, Nikhil Keetha, Ayush Tewari, Shafiullah, and Lerrel Pinto. From play to policy:
et al. Conceptfusion: Open-set multimodal 3d mapping. Conditional behavior generation from uncurated robot
arXiv preprint arXiv:2302.07241, 2023. data, 2022.
[48] DhruvShah,AjaySridhar,NitishDashora,KyleStachow- [61] Theophile Gervet, Zhou Xian, Nikolaos Gkanatsios, and
icz, Kevin Black, Noriaki Hirose, and Sergey Levine. KaterinaFragkiadaki. Act3d:3dfeaturefieldtransformers
ViNT: A Foundation Model for Visual Navigation. In 7th for multi-task robotic manipulation. In Conference on
Annual Conference on Robot Learning (CoRL), 2023. Robot Learning, pages 3949–3965. PMLR, 2023.
[49] Qiao Gu, Alihusein Kuwajerwala, Sacha Morin, Kr- [62] Wenxuan Zhou, Bowen Jiang, Fan Yang, Chris Paxton,
ishna Murthy Jatavallabhula, Bipasha Sen, Aditya Agar- and David Held. Learning hybrid actor-critic maps
wal, Corban Rivera, William Paul, Kirsty Ellis, Rama for 6d non-prehensile manipulation. arXiv preprint
Chellappa, et al. Conceptgraphs: Open-vocabulary 3d arXiv:2305.03942, 2023.
scene graphs for perception and planning. arXiv preprint [63] MohitShridhar,LucasManuelli,andDieterFox. CLIPort:
arXiv:2309.16650, 2023. What and where pathways for robotic manipulation. In
[50] AbhinavGupta,AdithyavairavanMurali,DhirajPrakashc- CoRL, pages 894–906. PMLR, 2022.
hand Gandhi, and Lerrel Pinto. Robot learning in [64] Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar,
homes: Improving generalization and reducing dataset Jonathan Tompson, Sergey Levine, and Pierre Sermanet.
bias. AdvancesinNeuralInformationProcessingSystems, Learning latent plans from play. In CoRL, pages 1113–
31:9094–9104, 2018. 1132. PMLR, 2020.
[51] Jeffrey Mahler, Jacky Liang, Sherdil Niyaz, Michael [65] Corey Lynch and Pierre Sermanet. Language conditioned
Laskey, Richard Doan, Xinyu Liu, Juan Aparicio Ojea, imitation learning over unstructured data. Robotics:Science and Systems, 2021.
[66] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu
Li, Jiajun Wu, and Li Fei-Fei. VoxPoser: Composable
3D value maps for robotic manipulation with language
models. In CoRL, 2023.
[67] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol
Hausman, Brian Ichter, Pete Florence, and Andy Zeng.
CodeasPolicies:Languagemodelprogramsforembodied
control. In icra, pages 9493–9500. IEEE, 2023.
[68] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar,
ChaoweiXiao,YukeZhu,LinxiFan,andAnimaAnandku-
mar. Voyager: An open-ended embodied agent with large
languagemodels. arXivpreprintarXiv:Arxiv-2305.16291,
2023.
[69] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit
Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse
Thomason, and Animesh Garg. ProgPrompt: Generating
situated robot task plans using large language models. In
ICRA, pages 11523–11530. IEEE, 2023.
[70] Naoki Yokoyama, Alex Clegg, Joanne Truong, Eric
Undersander, Tsung-Yen Yang, Sergio Arnaud, Sehoon
Ha, Dhruv Batra, and Akshara Rai. ASC: Adaptive
skill coordination for robotic mobile manipulation. arXiv
preprint arXiv:2304.00410, 2023.
[71] Austin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrish-
nan, Kuang-Huei Lee, Quan Vuong, Paul Wohlhart, Sean
Kirmani, Brianna Zitkovich, Fei Xia, Chelsea Finn, and
Karol Hausman. Open-world object manipulation using
pre-trained vision-language model. In arXiv preprint,
2023.
[72] Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi,
Jun Takamatsu, and Katsushi Ikeuchi. Gpt-4v(ision)
for robotics: Multimodal task planning from human
demonstration. arXiv preprint arXiv:2311.12015, 2023.
[73] Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-
Chakra, Ian Reid, and Niko Suenderhauf. Sayplan:
Grounding large language models using 3d scene
graphs for scalable task planning. arXiv preprint
arXiv:2307.06135, 2023.
[74] Charles C Kemp, Aaron Edsinger, Henry M Clever, and
Blaine Matulevich. The design of stretch: A compact,
lightweight mobile manipulator for indoor human envi-
ronments. In 2022 International Conference on Robotics
and Automation (ICRA), pages 3150–3157. IEEE, 2022.APPENDIXA ’poster’, ’bed’, ’cart’, ’light switch’,
SCANNET200TEXTQUERIES ’backpack’, ’power strip’, ’baseball’,
’mustard’, ’bathroom vanity’, ’water
To detect objects in a given home environment pitcher’, ’closet’, ’couch’, ’beverage’,
using OWL-ViT, we use the Scannet200 labels. The ’toy’, ’salt’, ’plant’, ’pillow’, ’broom’,
full label set is here: [’shower head’, ’spray’, ’pepper’, ’muffins’, ’multivitamin’,
’inhaler’, ’guitar case’, ’plunger’, ’towel’, ’storage bin’, ’nightstand’,
’range hood’, ’toilet paper dispenser’, ’radiator’, ’telephone’, ’pillar’, ’tissue
’adapter’, ’soy sauce’, ’pipe’, ’bottle’, box’, ’vent’, ’hair dryer’, ’ledge’,
’door’, ’scale’, ’paper towel’, ’paper ’mirror’, ’sign’, ’plate’, ’tripod’,
towel roll’, ’stove’, ’mailbox’, ’chair’, ’kitchen cabinet’, ’column’,
’scissors’, ’tape’, ’bathroom stall’, ’water cooler’, ’plastic bag’, ’umbrella’,
’chopsticks’, ’case of water bottles’, ’doorframe’, ’paper’, ’laundry hamper’,
’hand sanitizer’, ’laptop’, ’alcohol ’food’, ’jacket’, ’closet door’, ’computer
disinfection’, ’keyboard’, ’coffee tower’, ’stairs’, ’keyboard piano’,
maker’, ’light’, ’toaster’, ’stuffed ’person’, ’table’, ’machine’, ’projector
animal’, ’divider’, ’clothes dryer’, screen’, ’shoe’].
’toilet seat cover dispenser’, ’file
cabinet’, ’curtain’, ’ironing board’,
APPENDIXB
’fire extinguisher’, ’fruit’, ’object’,
SAMPLEOBJECTSFROMOURTRIALS
’blinds’, ’container’, ’bag’, ’oven’, During our experiments, we tried to sample objects that
’body wash’, ’bucket’, ’cd case’, ’tv’, can plausibly be manipulated by the Hello Robot: Stretch
’tray’, ’bowl’, ’cabinet’, ’speaker’, gripper from the home environments. As a result, OK-Robot
’crate’, ’projector’, ’book’, ’school encountered a large variety of objects with different shapes
bag’, ’laundry detergent’, ’mattress’, and visual features. A subsample of such objects are presented
’bathtub’, ’clothes’, ’candle’, ’basket’, in the Figures 9, 10.
’glass’, ’face wash’, ’notebook’, ’purse’,
APPENDIXC
’shower’, ’power outlet’, ’trash bin’,
LISTOFHOMEEXPERIMENTS
’paper bag’, ’water dispenser’, ’package’,
A full list of experiments in homes can be found in Table I.
’bulletin board’, ’printer’, ’windowsill’,
’disinfecting wipes’, ’bookshelf’,
’recycling bin’, ’headphones’, ’dresser’,
’mouse’, ’shower gel’, ’dustpan’, ’cup’,
’storage organizer’, ’vacuum cleaner’,
’fireplace’, ’dish rack’, ’coffee kettle’,
’fire alarm’, ’plants’, ’rag’, ’can’,
’piano’, ’bathroom cabinet’, ’shelf’,
’cushion’, ’monitor’, ’fan’, ’tube’,
’box’, ’blackboard’, ’ball’, ’bicycle’,
’guitar’, ’trash can’, ’hand sanitizers’,
’paper towel dispenser’, ’whiteboard’,
’bin’, ’potted plant’, ’tennis’,
’soap dish’, ’structure’, ’calendar’,
’dumbbell’, ’fish oil’, ’paper cutter’,
’ottoman’, ’stool’, ’hand wash’, ’lamp’,
’toaster oven’, ’music stand’, ’water
bottle’, ’clock’, ’charger’, ’picture’,
’bascketball’, ’sink’, ’microwave’,
’screwdriver’, ’kitchen counter’, ’rack’,
’apple’, ’washing machine’, ’suitcase’,
’ladder’, ’ping pong ball’, ’window’,
’dishwasher’, ’storage container’,
’toilet paper holder’, ’coat rack’, ’soap
dispenser’, ’refrigerator’, ’banana’,
’counter’, ’toilet paper’, ’mug’, ’marker
pen’, ’hat’, ’aerosol’, ’luggage’,Arm smartphone holder Gray toy dragon Toy plant White shirt
Tangled earphones Playing cards Blue gloves Toy cactus
Toy grapes Medicine bottles Grey rag Blue hair oil bottle
Toothpaste White pretzel Blue body wash
Blue pretzel pack
Fig. 9: Sample objects on our home experiments, sampled from each home environment, which OK-Robot was able to pick and drop
successfully.Purple strap Yellow ginger paste packet Blue bag Steel wool
Translucent grey cup Black face wash Gold wrapped chocolate Black head band
Blue eyeglass case Flu!y headbands Yogurt drinks Lotion pump
Blue hair gel tube Brown trail mix bag White Apple bag Small hand sanitizer
Fig. 10: Sample objects on our home experiments, sampled from each home environment, which OK-Robot failed to pick up successfully.TABLE I: A list of all tasks in the home enviroments, along with their categories and success rates out of 10 trials.
Pick object Place location Result
Home 1
Cleanup level: none
silver cup white table Success
blue eye glass case chair Success
printed paper cup, coffee cup [white table] Manipulation failure
small red and white medication Chair Success
power adapter Grey Bed Success
wrapped paper Navigation failure
blue body wash study table Success
blue air spray white table Success
black face wash Manipulation failure
yellow face wash chair Success
body spray Navigation failure
small hand sanitizer Manipulation failure
blue inhaler device(window) white table Success
inhaler box(window) dust bin Success
multivitamin container Navigation failure
red towel white cloth bin (air conditioner) Success
white shirt white cloth bin (air conditioner) Success
Cleanup level: low
silver cup white table Success
blue eye glass case Navigation failure
printed paper cup, coffee cup [white table] dust bin Success
small red and white medication Chair Success
power adapter Navigation failure
blue body wash white table Success
blue air spray white table Success
yellow face wash white table Success
small hand sanitizer study table Success
blue inhaler device(window) Manipulation failure
inhaler box(window) dust bin Success
red towel white cloth bin(air conditioner) Success
white shirt white cloth bin(air conditioner) Success
Cleanup level: high
silver cup white table Success
printed paper cup, coffee cup [white table] dust bin Success
blue body wash white table Success
blue air spray white table Success
yellow face wash Manipulation failure
small hand sanitizer Manipulation failure
inhaler box(window) dust bin Success
white shirt white cloth bin(air conditioner) Success
Home 2
Cleanup level: None
fanta can dust bin Success
tennis ball small red shopping bag Success
black head band [bed] Manipulation failure
purple shampoo bottle white rack Success
toothpaste small red shopping bag Success
Continued on the next pagePick object Place location Result
orange packaging dust bin Success
green hair cream jar [white rack] Navigation failure
green detergent pack [white rack] white table Success
blue moisturizer [white rack] Navigation failure
green plastic cover Navigation failure
storage container Manipulation failure
blue hair oil bottle white rack Success
blue pretzels pack white rack Success
blue hair gel tube Manipulation failure
red bottle [white rack] brown desk Success
blue bottle [air conditioner] white cloth bin(air conditioner) Success
wallet Manipulation failure
Cleanup level: low
fanta can black trash can Success
tennis ball red target bag Success
black head band [bed] red target bag Success
purple shampoo bottle red target bag Success
toothpaste red target bag Success
orange packaging black trash can Success
green detergent pack [white rack] Manipulation failure
blue moisturizer [white rack] Navigation failure
blue hair oil bottle white rack Success
blue pretzels pack white rack Success
wallet Manipulation failure
Cleanup level: high
fanta can black trash can Success
purple shampoo bottle small red shopping bag Success
orange packaging black trash can Success
blue moisturizer [white rack] white rack Success
blue hair oil bottle Manipulation failure
blue hair gel tube dust bin Success
red bottle [white rack] target bag Placing failure
blue bottle [air conditioner] white cloth bin(air conditioner) Success
Home 3
Cleanup level: none
apple white plate Success
ice cream white and green bag Success
green lime juice bottle red basket Success
yellow packet Manipulation failure
red packet Manipulation failure
orange can card board box Success
cooking oil bottle Manipulation failure
pasta sauce Manipulation failure
orange box [stove] Manipulation failure
green bowl sink Success
washing gloves green bag [card board box] Success
small oregano bottle red basket Success
yellow noodles packet [stove] red basket Success
blue dish wash bottle card board box Success
scrubber Navigation failure
dressing salad bottle Navigation failure
Continued on the next pagePick object Place location Result
Cleanup level: low
apple white plate Success
ice cream red basket Success
green lime juice bottle red basket Success
yellow packet green bag Success
red packet Manipulation failure
orange can card board box Success
cooking oil bottle marble surface [red basket] Success
green bowl Manipulation failure
washing gloves sink Success
small oregano bottle red basket Success
yellow noodles packet [stove] Manipulation failure
blue dish wash bottle card board box Success
Cleanup level: high
apple white plate Success
ice cream red basket Success
green lime juice bottle red basket Success
orange can card board box Success
cooking oil bottle Manipulation failure
washing gloves sink Success
small oregano bottle red basket Success
yellow noodles packet [stove] red basket Success
blue dish wash bottle card board box Success
Home 4
Cleanup level: none
pepsi black chair Success
birdie cloth bin Success
black hat Navigation failure
owl like wood carving bed Success
red inhaler Manipulation failure
black sesame seeds Manipulation failure
banana Manipulation failure
loose-leaf herbal tea jar black chair Success
red pencil sharpener Navigation failure
fast-food French fries container blue shopping bag [metal drying rack] Placing failure
milk plastic storage drawer unit Success
socks[bed] Navigation failure
purple gloves Manipulation failure
target bag cloth bin Success
muffin grey bed Success
tissue paper table Success
grey eyeglass box Manipulation failure
Cleanup level: low
pepsi basket Success
birdie white drawer Success
owl like wood carving Navigation failure
red inhaler plastic storage drawer unit Success
black sesame seeds bed Success
loose-leaf herbal tea jar table Success
fast-food French fries container chair Success
Continued on the next pagePick object Place location Result
milk chair Success
purple gloves basket Success
target bag basket Placing failure
muffin table Success
tissue paper Manipulation failure
grey eyeglass box Navigation failure
Cleanup level: high
pepsi basket Success
birdie bed Success
red inhaler plastic storage drawer unit Success
black sesame seeds desk Success
banana Manipulation failure
loose-leaf herbal tea jar Manipulation failure
milk chair Success
purple gloves basket Success
target bag basket Success
muffin bed Success
Home 5
Cleanup level: none
tiger balm topical ointment Navigation failure
pink shampoo trader joes shapping bag Success
aveeno sunscreen protective lotion trader joes shapping bag Success
small yellow nozzle spray Manipulation failure
black hair care spray Manipulation failure
green hand sanitizer Manipulation failure
white hand sanitizer Navigation failure
white bowl [ketchup] black sofa chair Success
blue bowl Manipulation failure
blue sponge trader joes shapping bag Success
ketchup Manipulation failure
white salt Manipulation failure
black pepper black drawer Success
blue bottle Navigation failure
purple light bulb box trader joes shopping bag Success
white plastic bag bed Success
rag white rack Success
Cleanup level: low
pink shampoo Navigation failure
aveeno sunscreen protective lotion Manipulation failure
small yellow nozzle spray Manipulation failure
white bowl [ketchup] black sofa chair Success
blue sponge bed Success
ketchup trader joes shopping bag Success
white salt trader joes shopping bag Success
black pepper Navigation failure
blue bottle black sofa chair Success
purple light bulb box Manipulation failure
rag white rack Success
Cleanup level: high
pink shampoo trader joes shopping bag Success
Continued on the next pagePick object Place location Result
green hand sanitizer black sofa chair Success
white bowl [ketchup] Manipulation failure
blue sponge bed Success
ketchup black drawer Success
white salt white drawer Success
purple light bulb box trader joes shopping bag Success
rag black sofa chair Success
Home 6
Cleanup level: none
translucent grey cup Manipulation failure
green mouth spray box stove Success
green eyeglass container chair Success
blue bag Manipulation failure
black burn ointment box Navigation failure
white vitamin bottle Navigation failure
McDonald’s paper bag stove Success
purple medicine packaging chair Success
grey rag sink Success
sparkling water can [sink] countertop Success
gold wrapped chocolate Manipulation failure
lemon tea carton table Success
metallic golden beverage can table Success
red bottle table Success
tea milk bottle Navigation failure
nyu water bottle [sink] table Success
white hand wash Navigation failure
Cleanup level: low
translucent grey cup Navigation failure
green mouth spray box Manipulation failure
blue bag brown box Success
black burn ointment box brown box Success
McDonald’s paper bag Navigation failure
grey rag sink Success
sparkling water can [sink] chair Success
lemon tea carton stove Success
metallic golden beverage can Navigation failure
red bottle brown box Success
nyu water bottle [sink] table Success
white hand wash sink Success
Cleanup level: high
blue bag brown box Success
black burn ointment box Manipulation failure
grey rag sink Success
sparkling water can [sink] chair Success
lemon tea carton table Success
metallic golden beverage can stove Success
red bottle Navigation failure
nyu water bottle [sink] Manipulation failure
white hand wash Manipulation failure
Home 7
Continued on the next pagePick object Place location Result
Cleanup level: none
blue plastic bag roll Navigation failure
green bag basket[window] Success
toy cactus desk Success
toy van chair Success
brown medical bandage chair Success
power adapter Navigation failure
red herbal tea brown cardboard box Success
apple juice box brown cardboard box Success
paper towel blue cardboard box Success
toy bear bed blanket Success
yellow ball bed blanket Success
black pants basket[window] Success
purple water bottle desk Success
blue eyeglass case Manipulation failure
brown toy monkey Navigation failure
blue hardware box [table] blue cardboard box Success
green zandu balm container blue cardboard box Success
Cleanup level: low
green bag basket Success
toy cactus basket Success
toy van chair Success
brown medical bandage Manipulation failure
red herbal tea brown box Success
apple juice box brown box Success
paper towel basket Success
toy bear desk Success
purple water bottle desk Success
blue eyeglass case Manipulation failure
green zandu balm container blue cardboard box Success
Cleanup level: high
green bag stool [window] Success
toy cactus table Success
toy van white basket Success
red herbal tea brown cardboard box Success
apple juice box brown cardboard box Success
paper towel blue cardboard box Success
toy bear white basket Success
yellow ball bed Success
purple water bottle black tote bag Success
green zandu balm container blue cardboard box Success
Home 8
Cleanup level: none
cyan air spray brown shelf [sink] Success
blue gloves kitchen sink Success
blue peanut butter black stove Success
nutella table Success
green bag brown shelf [sink] Success
green bandage box trash can Success
green detergent kitchen sink Success
black ‘red pepper sauce’ Manipulation failure
Continued on the next pagePick object Place location Result
red bag chair Success
black bag chair Success
red spray [brown shelf] kitchen countertop Success
steel wool Manipulation failure
white aerosol trash can Success
white pretzel black stove Success
purple crisp kitchen countertop Success
plastic bowl Manipulation failure
playing card microwave Success
Cleanup level: low
cyan air apray chair Success
blue gloves sink Success
blue peanut butter Navigation failure
green bag brown shelf Success
green bandage box brown shopping bag Success
green detergent microwave Success
red bag Manipulation failure
black bag chair Success
white aerosol trash can Success
white pretzel black stove Success
purple crisp kitchen countertop Success
plastic bowl Manipulation failure
playing card microwave Success
Cleanup level: high
cyan air apray brown shelf [sink] Success
blue gloves stove Success
blue peanut butter black stove Success
green bag brown shelf [sink] Success
green bandage box microwave Success
green detergent Manipulation failure
black bag chair Success
white aerosol table Success
purple crisp chair Success
playing card microwave Success
Home 9
Cleanup level: none
toy grapes black laundry bag Success
purple strap Manipulation failure
red foggy body spray Manipulation failure
arm smartphone holder bed Success
medicine bottle Manipulation failure
yogurt beverage Navigation failure
blue shaving cream can Navigation failure
blue cup table Success
purple tape Manipulation failure
black shoe brush Navigation failure
fluffy headband Manipulation failure
black water bottle brown shopping bag Placing failure
yellow eyeglass case black chair Success
paper cup Manipulation failure
lotion pump Manipulation failure
Continued on the next pagePick object Place location Result
nasal spray Manipulation failure
plastic bag trash basket Success
Cleanup level: low
toy grapes Manipulation failure
red foggy body spray brown paper bag Success
arm smartphone holder brown paper bag Success
yogurt beverage desk Success
blue shaving cream can black bag Success
blue cup black chair Success
black shoe brush Manipulation failure
fluffy headband Navigation failure
black water bottle folded chair Success
nasal spray Navigation failure
plastic bag trash basket Success
Cleanup level: high
red foggy body spray brown paper bag Success
arm smartphone holder Manipulation failure
yogurt beverage desk Success
blue shaving cream can black bag Success
blue cup black chair Success
black water bottle white bed Success
nasal spray folded chair Success
plastic bag trash basket Success
Home 10
Cleanup level: none
grey toy dragon bed Success
purple body spray Manipulation failure
hand sanitizer shelf Success
toy plant bed [shelf] Success
brown trail mix bag Manipulation failure
hanging blue shirt cloth bin Success
white apple bag Manipulation failure
white and pink powder bottle table Success
cough syrup bottle shelf Success
tangled ear phones office chair Success
red deodrant stick[table] chair Success
black body spray chair Success
hair treatment medicine bottle Manipulation failure
green tea package chair Success
portable speaker [green tea package] office chair Success
wooden workout gripper Navigation failure
brown box Navigation failure
blue bulb adapter office chair Success
game controller office chair Success
Cleanup level: low
grey toy dragon orange bag Success
purple body spray table Success
hand sanitizer Navigation failure
toy plant bed Success
brown trail mix bag Manipulation failure
Continued on the next pagePick object Place location Result
white and pink powder bottle black chair [bed] Success
cough syrup bottle shelf [bed] Success
red deodrant stick[table] bed [rack] Success
black body spray rack [bed] Placing failure
green tea package orange bag Success
brown box black chair [bed] Success
blue bulb adapter Manipulation failure
Cleanup level: high
purple body spray orange bag Success
toy plant bed Success
white and pink powder bottle Navigation failure
cough syrup bottle shelf [bed] Success
red deodrant stick[table] Navigation failure
black body spray black chair Success
green tea package table Success
blue bulb adapter shelf Success