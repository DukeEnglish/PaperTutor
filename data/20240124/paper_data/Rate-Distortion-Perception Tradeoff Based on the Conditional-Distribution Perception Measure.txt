1
Rate-Distortion-Perception Tradeoff Based on the
Conditional-Distribution Perception Measure
Sadaf Salehkalaibar, Jun Chen, Ashish Khisti, Wei Yu
Abstract
Westudytherate-distortion-perception(RDP)tradeoffforamemorylesssourcemodelintheasymptoticlimitoflargeblock-
lengths. Our perception measure is based on a divergence between the distributions of the source and reconstruction sequences
conditionedontheencoderoutput,whichwasfirstproposedin[1],[2].Weconsiderthecasewhenthereisnosharedrandomness
between the encoder and the decoder. For the case of discrete memoryless sources we derive a single-letter characterization of
the RDP function, thus settling a problem that remains open for the marginal metric introduced in Blau and Michaeli [3] (with
nosharedrandomness).Ourachievabilityschemeisbasedonlossysourcecodingwithaposteriorreferencemapproposedin[4].
For the case of continuous valued sources under squared error distortion measure and squared quadratic Wasserstein perception
measurewealsoderiveasingle-lettercharacterizationandshowthatanoise-addingmechanismatthedecodersufficestoachieve
the optimal representation. For the case of zero perception loss, we show that our characterization interestingly coincides with
the results for the marginal metric derived in [5], [6] and again demonstrate that zero perception loss can be achieved with a
3-dB penalty in the minimum distortion. Finally we specialize our results to the case of Gaussian sources. We derive the RDP
functionforvectorGaussiansourcesandproposeawaterfillingtypesolution.WealsopartiallycharacterizetheRDPfunctionfor
a mixture of vector Gaussians.
I. INTRODUCTION
Rate-distortion-perception (RDP) tradeoff [3], a generalization of the classical rate-distortion function [7] to incorporate
distribution constraints on the reconstruction, provides a theoretical framework for a variety of deep neural compression
systems that exhibit an inherent tradeoff between reconstruction fidelity and realism [8]. In this framework, the perception
loss is measured through a suitable divergence metric between the source and reconstruction distributions, with perfect realism
corresponding to the case when the source and reconstruction distributions are identical. The work of Blau and Michaeli [3]
establishes that when distortion loss is measured using mean squared error, perfect realism can be achieved with no more than
3-dB increase in the minimum distortion. The work of Theis and Wagner [9] establishes an operational interpretation of the
rate-distortion-perception function. The special case of (scalar) Gaussian sources has been studied in [10] where it is shown
that Gaussian distributions attain the RDP function. Furthermore a natural notion of universality is established where any
representation corresponding to a boundary point on RDP tradeoff curve can be converted to another representation associated
with another boundary point. The case when there is limited or no shared randomness between the encoder and decoder has
been studied in [5], [11]–[13] (see also [6]). To our knowledge, unlike the setting with (unlimited) shared randomness, a
computablecharacterizationofRDPfunctionremainslargelyopeninthesesettings.TheextensionofRDPfunctiontothecase
when correlated side information available to either the encoder or the decoder has been studied in [14], [15]. The application
of RDP function to neural compression has been studied in e.g., [1], [2], [16]–[21] and references therein.
While the perception loss metric in prior works [8] is based on the divergence between the source and reconstruction
distributions, a different choice was proposed in [1], [2]. This work empirically observed that a perception loss metric that
measuresthedivergencebetweenthesourceandreconstructiondistributionsconditioned ontheoutputoftheencoderresultsin
higher perceptual quality in reconstructions. Intuitively, this metric forces the decoder to follow the the conditional distribution
ofthe sourcegiventhe minimummeansquare error(MMSE)reconstruction andthusintroduces adjustmentsin the finedetails
that improve the blurriness, without deviating significantly from the MMSE reconstruction. To our knowledge a theoretical
study of the RDP function when the perception measure is based on such a conditional metric has not been previously
considered.Inthisworkwepresentsuchastudyundertheassumptionthatthereisnosharedrandomnessbetweentheencoder
and decoder. We will denote this setting as conditional-distribution based perception measure while the original setting as
marginal-distribution based perception measure. Our main contributions are as follows
WecharacterizetheRDPtradeoffforfinitealphabetsources(Theorem1)andexplicitlyderivethetradeofffortheuniform
•
Bernoulli source (Theorem 2). The achievable scheme uses some recent tools developed for lossy source coding with
a posterior reference map [4]. The complete characterization of the RDP function for the conditional-distribution based
perception measure is interesting since a similar characterization of the RDP function for the marginal-distribution based
perception measure only exists for the case of zero perception loss where the source and reconstruction distributions
exactly match [5] (when there is no shared randomness).
Sadaf Salehkalaibar, Ashish Khisti and Wei Yu are with the Department of Electrical and Computer Engineering at the University of Toronto, Toronto,
M5S3G4,Canada(email:{sadafs,akhisti,weiyu}@ece.utoronto.ca),
Jun Chen is with the Department of Electrical and Computer Engineering at McMaster University, Hamilton, ON L8S 4K1, Canada (email: chen-
jun@mcmaster.ca).
4202
naJ
22
]TI.sc[
1v70221.1042:viXra2
1
E[ℓ(M)] R
n ≤
1
E[∆(Xn,Xˆn)] D
1 n ≤
nE[φ(p Xn |M(. |M),p Xˆn |M(. |M))] ≤P
Fig.1. Systemmodelwiththeperceptionmeasurebasedontheconditionaldistribution.
The RDP tradeoff is further characterized for continuous alphabet sources (Theorem 3) under squared error distortion
•
measure and squared quadratic Wasserstein perception measure. For the case of zero perception loss, it is shown that
the rate-distortion tradeoff coincides with that of the marginal-distribution based perception measure (Corollary 1). As it
has been previously observed for the latter measure [13], [22], [23], for a fixed encoder, the distortion of reconstruction
satisfying zero perception loss is exactly twice that of the MMSE representation. Furthermore, the MMSE represenation
can be transformed to representations for other operating points on the RDP tradeoff by adding noise at the decoder.
Shannon’s lower bound [7, Equation (13.159)] for the RD tradeoff is extended to the RDP setting (Theorem 4). Using
•
this lower bound, we are able to partially characterize the RDP tradeoff for a Gaussian-mixture source (Corollary 2) and
completely characterize the tradeoff for a vector Gaussian source (Corollary 3). For the latter case, a water-filling type
solution is also derived.
The rest of this paper is organized as follows. Section II introduces the system model as well as the operational and the
informational definitions of RDP function with conditional-distribution based perception measure. In Section III, we establish
a single-letter characterization of the operational RDP function by showing that it coincides with its informational version; for
the uniform Bernoulli source under Hamming distortion measure and divergence induced by Hamming cost function, a more
explicit characterization is obtained. In Section IV, we prove that the aforementioned single-letter characterization continues
to hold for square-integrable sources under squared error distortion measure and squared quadratic Wasserstein perception
measure, and matches its marginal-distribution based counterpart when the perceptual quality is required to be perfect; we also
extend the Shannon lower bound to the our setting, which yields a partial characterization of the RDP function for Gaussian
mixture sources and a complete solution for Gaussian sources. Section V includes some concluding remarks.
Notation: Entropy, differential entropy, and mutual information are denoted by H(), h(), and I(; ), repsectively. We use
· · · ·
[a:b] to represent the set of integers from a to b for any two integers a b, and use x y to represent the minimum of two
≤ ∧
real numbers x and y . The cardinality of set is written as . Let Ber(p) and (µ,Σ) denote respectively the Bernoulli
S |S| N
distribution with parameter p and the Gaussian distribution with mean µ and covariance matrix Σ. For two matrices A and
B, we write A B (A B) if A B is positive definite (semidefinite). Throughout this paper, the base of the logarithm
≻ ⪰ −
function is e.
II. PROBLEMFORMULATION
A. System Model
Let the source X(t) be a stationary and memoryless process with marginal distribution p over alphabet (see
{
}∞t=1 X
X
Fig. 1). A stochastic encoder f(n) : n maps a length-n source sequence Xn to a codeword M in a binary prefix
X → M
code according to some conditional distribution p . A stochastic decoder g(n) : n then generates a length-n
M Xn
reconsM truction sequence Xˆn based on M according to| some conditional distribution pM → . X Note that this coding system
XˆnM
induces the following joint distribution: |
p =p p p .
XnMXˆn Xn M Xn XˆnM
| |
Let ∆ : [0, ) be a distortion measure with ∆(x,xˆ) = 0 if and only if x = xˆ. Define ∆(xn,xˆn) :=
(cid:80)n ∆(x(tX ),xˆ× (t)X ) fo→ r xn,xˆ∞ n n. Let ϕ: [0, ] be a divergence with ϕ(p,p)=0 if and only if p=p, where
t=1 ∈X P×P → ∞ ′ ′
denotesthesetofprobabilitydistributions.Throughoutthispaper,wefocusonaspecialclassofdivergencesthatarisefrom
P
the theory of optimal transport. Specifically, for any two probability distributions p X˜n and p X¯n over Xn, let
n
(cid:88)
ϕ(p X˜n,p X¯n):= inf E[c(X˜(t),X¯(t))], (1)
p X˜nX¯n∈Π(p X˜n,p X¯n)
t=1
where Π(p X˜n,p X¯n) denotes theset ofcouplings of p
X˜n
and p X¯n, and c:
X×X
→[0, ∞) is acost functionwith c(x˜,x¯)=0
if and only if x˜=x¯. In this work, ϕ will serve the role of perception measure.
Proposition 1: ϕ defined in (1) has the following properties.3
1) Tensorizability:
n
(cid:88)
ϕ(p X˜n,p X¯n)
≥
ϕ(p X˜(t),p X¯(t))
t=1
and the equality holds if p X˜n
=(cid:81)n
i=1p X˜(t) and p X¯n
=(cid:81)n
i=1p X¯(t).
2) Convexity:
ϕ((1 −λ)p X˜n +λp ′ X˜n,(1 −λ)p X¯n +λp ′X¯n) ≤(1 −λ)ϕ(p X˜n,p X¯n)+λϕ(p ′ X˜n,p ′X¯n)
for λ [0,1].
∈
3) Continuity:
|ϕ(p X˜n,p X¯n) −ϕ(p Y˜n,p Y¯n) |≤nc max(d TV(p X˜n,p Y˜n)+d TV(p X¯n,p Y¯n)),
where d is the total variation distance, and c :=sup c(x,x).
TV max x,x′ ′
∈X
Proof: See Appendix A.
B. Rate-Distortion-Perception Function
Definition 1: We say rate R is achievable subject to distortion and percetion constraints D and P if for some n, there exist
encoder f(n) and decoder g(n) such that (see Fig. 1)
1
E[ℓ(M)] R,
n ≤
1
E[∆(Xn,Xˆn)] D,
n ≤
1
E[ϕ(p ( M),p ( M))] P, (2)
n Xn |M ·| Xˆn |M ·| ≤
where ℓ(M) denotes the length of M. The infimum of all such R is denoted by R (D,P), which will be referred to as the
C
operational rate-distortion-perception function for the conditional-distribution based perception measure.
To the end of characterizing R (D,P), we introduce the following informational rate-distortion-perception function:
C
R(D,P):= inf I(X;U)
p UXˆ|X
s.t. X U Xˆ form a Markov chain, (3)
↔ ↔
E[∆(X,Xˆ)] D, (4)
≤
E[ϕ(p ( U),p ( U))] P. (5)
X |U ·| Xˆ |U ·| ≤
Proposition 2: R(D,P) is convex in (D,P).
Proof: See Appendix B.
Proposition 3: If < , then there is no loss of generality in assuming that the alphabet of U, denoted by , satisfies
|X| ∞ U
+2; moreover, the infimum in the definition of R(D,P) can be attained, thus is a minimum.
|U|≤|X|
Proof: Note that < implies c < , which in light of part 3) of Proposition 1 further implies the continuity of
max
|X| ∞ ∞
ϕ(p,p)in(p,p).Therefore,wecaninvokethesupportlemma[24,p.631]toestablishthedesiredcardinalitybound.Moreover,
′ ′
the continuity of ϕ(p,p), together with the cardinality bound, implies that the feasible domain for p is compact. As a
′ UXˆ X
consequence, the objective function I(X;U), which is continuous in p , has a minimum value over|this domain.
UXˆ X
|
III. FINITEALPHABETSOURCES
We focuson finite alphabetsources in thissection. Our firstmain result showsthat the operationalrate-distortion-perception
function coincides with its informational counterpart for such sources.
Theorem 1: Assume < . For D 0 and P 0,
|X| ∞ ≥ ≥
R (D,P)=R(D,P).
C
Proof: See Appendix C. The achievability part of the proof relies on a recent development in information theory known
as lossy source coding with a posterior reference map [4].
Remark 1: The proof actually indicates that if (2) is replaced with the following stronger constraint
1
ϕ(p ( m),p ( m)) P, m ,
n Xn |M ·| Xˆn |M ·| ≤ ∈M
Theorem 1 continues to hold.4
The next result provides an explicit characterization of R (D,P) for the uniform Bernoulli source (i.e., X Ber(1)) under
C ∼ 2
Hamming distortion measure (i.e., ∆(x,xˆ) = d (x,xˆ)) and divergence induced by Hamming cost function (i.e., c(x,xˆ) =
H
d (x,xˆ)). For D 0 and P 0, let
H
≥ ≥
¯h(D,P):=(cid:40) H b(1+(D ∧P) −√1 2+(D ∧P)2 −2D ), D ∈[0, 21),
log2, D [1, ),
∈ 2 ∞
where H denotes the binary entropy function. Moreover, let ¯h be the upper concave envelope of ¯h over [0, )2.
b
∞
Theorem 2: Assume X Ber(1), ∆(x,xˆ)=d (x,xˆ), and c(x,xˆ)=d (x,xˆ). For D 0 and P 0,
∼ 2 H H ≥ ≥
R (D,P)=log2 ¯h(D,P).
C
−
Proof: See Appendix D.
Remark 2: The upper concave envelope operation is necessary as ¯h itself is not concave in (D,P). See Appendix E for
some relevant analysis.
Remark 3: Theorem 2 implies that for D 0 and P D,
≥ ≥
R (D,P) log2 ¯h(D,P)
C
≤ −
(cid:40)
log2 H (D), D [0,1),
= − b ∈ 2
0, D [1, ).
∈ 2 ∞
Thisupperboundistightbecauseitcoincideswiththerate-distortionfunctionoftheuniformBernoullisourceunderHamming
distortion measure [7, Theorem 13.3.1], which is the infimum of achievable rates when the perception constraint is ignored.
The operationalRDP functionfor themarginal-distribution basedperception measure,denoted as R (D,P), canbe defined
M
similarly by replacing (2) with
1
ϕ(p ,p ) P. (6)
n
Xn Xˆn
≤
It follows by part 2) of Proposition 1 that (2) implies (6). As a consequence, we must have
R (D,P) R (D,P). (7)
M C
≤
Different from R (D,P), a single-letter characterization of R (D,P) is unavailable except for the special case P =0, where
C M
it is known [6, Section III.B], [5, Corollary 1], [13, Equation (16)] that
R (D,0)= inf max I(X;U),I(Xˆ;U) (8)
M
p UXˆ|X { }
s.t. X U Xˆ form a Markov chain, (9)
↔ ↔
E[∆(X,Xˆ)] D, (10)
≤
p =p . (11)
Xˆ X
The difficulty in characterizing R (D,P) arises from the fact that the i.i.d. form of the reconstruction sequence Xˆn favored
M
by the perception constraint (6) (see part 1) and part 2) of Proposition 1) is not necessarily desirable from the rate perspective.
This tension disappears when P =0 as Xˆn is forced to be an i.i.d. sequence. In contrast, under constraint (2), the conditional
i.i.d. form of the source sequence Xn and the reconstruction sequence Xˆn given the codeword M is desirable from both the
perception and the rate perspectives. This explains why R (D,P) is more amenable to single-letterization as compared to
C
R (D,P).
M
In light of Theorem 2, when X Ber(1) and ∆(x,xˆ)=d (x,xˆ),
∼ 2 H
R (D,0) log2 ¯h(D,0)
C
≤ −
(cid:40)
=
log2 −H b(1 −√1 2−2D), D ∈[0, 21),
0, D [1, ).
∈ 2 ∞
Interestingly, the corresponding R (D,0) is given by [13, Theorem 7]
M
(cid:40)
R (D,0)=
log2 −H b(1 −√1 2−2D), D ∈[0, 21),
M 0, D [1, ).
∈ 2 ∞
In view of (7), we must have
R (D,0)=R (D,0) (12)
C M
for this special case. As shown in the next section (see Corollary 1), this is a general phenomenon rather than a coincidence.5
IV. CONTINOUSALPHABETSOURCES
In this section, we consider continuous alphabet sources, more specifically, the case where X := (X ,X ,...,X )T is a
1 2 L
random vector with = RL. The following result indicates that Theorem 1 continues to hold for square-integrable sources
(i.e., E[ X 2] < X ) under squared error distortion measure (i.e, ∆(x,xˆ) = x xˆ 2) and squared quadratic Wasserstein
∥ ∥ ∞ ∥ − ∥
distance (i.e., ϕ(p,p) = W2(p,p), resulting from choosing c(x,xˆ) = x xˆ 2). As D = 0 corresponds to lossless source
′ 2 ′ ∥ − ∥
coding, which is generally impossible for continuous alphabet sources (unless p has a discrete support), we focus on the
X
case D >0 throughout this section.
Theorem 3: Assume E[ X 2]< , ∆(x,xˆ)= x xˆ 2, and ϕ(p,p)=W2(p,p). For D >0 and P 0,
∥ ∥ ∞ ∥ − ∥ ′ 2 ′ ≥
R (D,P)=R(D,P). (13)
C
Moreover, in this case,
R(D,P)=R(D,P), (14)
′
where
R(D,P):= inf I(X;U )
′ ′
p U′Xˆ′|X
s.t. X U Xˆ form a Markov chain, (15)
′ ′
↔ ↔
U =E[X U ]=E[Xˆ U ] almost surely, (16)
′ ′ ′ ′
| |
E[ V 2]+E[ Vˆ 2] D, (17)
∥ ∥ ∥ ∥ ≤
E[W2(p ( U ),p ( U ))] P, (18)
2 V |U′ ·| ′ Vˆ |U′ ·| ′ ≤
with V :=X U and Vˆ =Xˆ U .
′ ′ ′
− −
Proof: See Appendix F.
Remark 4: In some scenarios, the source only takes on values from a strict subset of RL and the reconstruction is also
X
confined to . The proof of (13) is not directly applicable to such scenarios as the output of quantizer ξ may live outside
X X
(exceptforthespecialcaseP =0wherethereconstructionisforcedtohavethesamedistributionasthesource).Nevertheless,
it can be shown (see Appendix G) using a more delicate argument that (13) continues to hold in the aforementioned scenarios
(correspondingly, R(D,P) is defined with Xˆ restricted to ). On the other hand, except for the special case P =0, the proof
of (14) relies critically on the fact that Xˆ and Xˆ in theX definition of R(D,P) and R(D,P) have the freedom to take on
′ ′
values from RL.
Remark 5: The equivalent characterization in (14) suggests that it suffices to consider MMSE representation U since any
′
other optimal representation X can be generated from U through a simple noise-adding mechanism. This is closely related
′ ′
to the universality of MMSE representation observed in the setting with (unlimited) shared randomness [10].
The following result indicates that R (D,0) is always equal to R (D,0) under squared error distortion measure, and
C M
connects them to the classical rate-distortion function.
Corollary 1: Assume E[ X 2]< and ∆(x,xˆ)= x xˆ 2. For D >0,
∥ ∥ ∞ ∥ − ∥
(cid:18) (cid:19)
D
R (D,0)=R (D,0)=R , (19)
C M
2
where
(cid:18) (cid:19)
D
R := inf I(X;U¯)
2 p U¯|X
D
s.t. E[ X U¯ 2] . (20)
∥ − ∥ ≤ 2
Proof: See Appendix H.
Remark 6: The second equality in (19) is known [13, Equation (20)] (see also [22, Theorem 2], [23, Theorem 2]).
Remark 7: Note that d (x,xˆ)= x xˆ 2 for x,xˆ 0,1 . Therefore, (12) can be viewed as a special case of Corollary 1.
H
Remark 8: In the definition of R∥ (D− ),∥ we allow U¯∈{ to ha} ve the freedom to take on values from RL even if X is only
2
defined over a strict subset of RL. Otherwise, the second equality in (19) might not hold. For example, when X Ber(1)
and ∆(x,xˆ)= x xˆ 2, we have R (1,0)=R (1,0)=0; on the other hand, R(1)=0 only if U¯ is allowed to∼ be equ2 al
∥ − ∥ C 2 M 2 4
to 1, which does not belong to 0,1 . See Remark 4 for a related discussion.
2 { }
The next result extends the Shannon lower bound [7, Equation (13.159)] to the RDP setting.
Theorem 4: Assume σ2 := E[(X E[X ])2] (0, ), ℓ [1 : L], ∆(x,xˆ) = x xˆ 2, and ϕ(p,p) = W2(p,p). For
ℓ ℓ − ℓ ∈ ∞ ∈ ∥ − ∥ ′ 2 ′
D >0 and P 0,
≥
L
(cid:88)1
R (D,P) h(X) log(2πeω ),
C ℓ
≥ − 2
ℓ=16
ω
Fig.2. Water-fillingsolutionforthevectorGaussiansource.
where
(cid:40) ω σ2, D+(cid:112) (2D (D P))(D P)<2(cid:80)L σ2,
ω ℓ := σ2∧
,
ℓ D+(cid:112) (2D− (D∧ P))(D∧
P)
2(cid:80)ℓ L′=1 σℓ 2′
,
ℓ ∈[1:L], (21)
ℓ − ∧ ∧ ≥ ℓ′=1 ℓ′
with ω being the unique solution to
L (cid:112)
(cid:88) D+ (2D (D P))(D P)
(ω σ2)= − ∧ ∧ .
∧ ℓ 2
ℓ=1
Proof: See Appendix I.
The following result provides a partial characterization of R (D,P) for Gaussian mixture sources. Let
(cid:80)K
β (µ ,Σ )
be a mixture of K Gaussian distributions, (µ ,Σ ), k
[1C
:K], with β >0, k [1:K], and
(cid:80)K
β
k ==1 1.k WN
e
ak ssumk
e
N k k ∈ k ∈ k=1 k
Σ 0 and consequently λ (Σ ) > 0, k [1 : K], where λ (A) denotes the minimum eigenvalue of symmetric matrix
k min k min
≻ ∈
A.
Corollary 2: Assume X (cid:80)K β (µ ,Σ ), ∆(x,xˆ) = x xˆ 2, and ϕ(p,p) = W2(p,p). For D > 0 and P 0
∼ k=1 k N k k ∥ − ∥ ′ 2 ′ ≥
satisfying
(cid:112)
D+ (2D (D P))(D P)
− ∧ ∧ min λ (Σ ) K ,
2L ≤ { min k }k=1
we have
(cid:32) (cid:112) (cid:33)
L 2πe(D+ (2D (D P))(D P))
R (D,P)=h(X) log − ∧ ∧ .
C
− 2 2L
Proof: See Appendix J.
A complete characterization of R (D,P) can be obtained for Gaussian sources, namely, X (µ,Σ). Let Σ = ΘTΛΘ
C
∼ N
be the eigenvalue decomposition of Σ, where Θ is a unitary matrix and Λ is a diagonal matrix with the ℓ-th diagonal entry
denoted by λ , ℓ [1:L]. We assume Σ 0 and consequently λ >0, ℓ [1:L].
ℓ ℓ
∈ ≻ ∈
Corollary 3: Assume X (µ,Σ), ∆(x,xˆ)= x xˆ 2, and ϕ(p,p)=W2(p,p). For D >0 and P 0,
∼N ∥ − ∥ ′ 2 ′ ≥
L (cid:18) (cid:19)
(cid:88)1 λ
ℓ
R (D,P)= log ,
C
2 ω
ℓ
ℓ=1
where ω is defined in (21) with σ2 replaced by λ , ℓ [1:L] (see Fig. 2).
ℓ ℓ ℓ ∈
Proof: See Appendix K.
Remark 9: Note that R (D,P) degenerates to the rate-distortion function R(D) of quadratic vector Gaussian source coding
C
when P D, where R(D) is given by the conventional reverse waterfilling formula [7, Theorem 13.3.3]
≥
 L (cid:16) (cid:17)
(cid:80) 1log λℓ , D <(cid:80)L λ ,
R(D)= ℓ=12 ω ∧λℓ ℓ=1 ℓ

0, D
(cid:80)L
λ ,
≥ ℓ=1 ℓ
with ω being the unique solution to
L
(cid:88)
(ω λ )=D.
ℓ
∧
ℓ=17
It is also easy to verify that R (D,0)=R(D), which is consistent with Corollary 1.
C 2
Remark 10: Note that ω can be interpreted as the water level in the subspace associated with eigenvalue λ , ℓ [1 : L]
ℓ ℓ
∈
(see Fig. 2). Different from the conventional waterfilling formula where the water level coincides with the distortion loss in
eachsubspacewhenthedistortionconstraintisactive,thesituationismorecomplexhereduetothepresenceoftheperception
constraint. Specifically, for ℓ [1 : L], the distortion loss D and the perception loss P in the subspace associated with
ℓ ℓ
∈
eigenvalue λ are given respectively by1
ℓ
(cid:18) (cid:19)
D
:= 2D2 −2 (D D√ P(2 )D 2−P)P ω ℓ, D >P,
ℓ −
ω , D P,
ℓ
≤

(cid:18) (cid:19)2
P
:= √(2D D−P P)P −P
ω ℓ, D >P,
ℓ −
ω
, D P.
ℓ
≤
V. CONCLUSION
We have characterized the RDP tradeoff for both finite and continuous alphabet sources when the perception measure is
based on the divergence between the distributions of the source and reconstruction sequences conditioned on the encoder
output. A novel waterfilling type solution is obtained for vector Gaussian sources under squared error distortion measure
and squared quadratic Wasserstein perception measure. In contrast to the conventional reverse waterfilling solution, here the
water level depends on both distortion and perception losses. Throughout this work, we have focused on the setting when no
shared randomness is assumed between the encoder and the decoder. When shared randomness is available, the analysis of the
proposed conditional-distribution based perception measure appears significantly harder and is left for future research.
APPENDIXA
PROOFOFPROPOSITION1
Tensorizability: Note that
n
(cid:88)
ϕ(p X˜n,p X¯n)= inf E[c(X˜(t),X¯(t))]
p X˜nX¯n∈Π(p X˜n,p X¯n)
t=1
n
(cid:88)
inf E[c(X˜(t),X¯(t))] (22)
≥ t=1p X˜ iX¯(t)∈Π(p X˜(t),p X¯(t))
n
(cid:88)
= ϕ(p X˜(t),p X¯(t)),
t=1
wheretheinequalityin(22)becomesanequalitybecause(cid:81)n
t=1p
X˜(t)Xˆ(t)
∈Π(p X˜n,p X¯n)foranyp
X˜(t)X¯(t)
∈Π(p X˜(t),p X¯(t)),
t [1:n].
∈
Convexity: Note that
(1 −λ)ϕ(p X˜n,p X¯n)+λϕ(p
′
X˜n,p ′X¯n)
n n
(cid:88) (cid:88)
=(1 λ) inf E[c(X˜(t),X¯(t))]+λ inf E[c(X˜(t),X¯(t))]
− p X˜nX¯n∈Π(p X˜n,p X¯n)
t=1
p′ X˜nX¯n∈Π(p′ X˜n,p′ X¯n)
t=1
n
(cid:88)
= inf E[c(X˜(t),X¯(t))]
(1−λ)pX˜nX¯n+λp′ X˜nX¯n:
t=1
p X˜nX¯n∈Π(p X˜n,p X¯n),p′ X˜nX¯n∈Π(p′ X˜n,p X′ ¯n)
≥ϕ((1 −λ)p X˜n +λp ′ X˜n,(1 −λ)p X¯n +λp ′X¯n), (23)
where (23) is due to the fact that (1 −λ)p X˜nX¯n+λp
′ X˜nX¯n
∈Π((1 −λ)p X˜n+λp
′
X˜n,(1 −λ)p X¯n+λp ′X¯n) for any p
X˜nX¯n ∈
Π(p X˜n,p X¯n) and p
′ X˜nX¯n
∈Π(p
′
X˜n,p ′X¯n).
1ActuallyD
ℓ
andP
ℓ
arenotuniquelydefinedwhenD+(cid:112) (2D−(D∧P))(D∧P)>2(cid:80)L
ℓ=1λ
ℓ
(whichimpliesR C(D,P)=0).8
Continuity: Construct a Markov chain Y˜n ↔X˜n ↔X¯n ↔Y¯n with p
X˜nY˜n
∈Π(p X˜n,p Y˜n), p
X˜nX¯n
∈Π(p X˜n,p X¯n), and
p X¯nY¯n ∈Π(p X¯n,p Y¯n). We have
n
(cid:88)
E[c(X˜(t),X¯(t))]
t=1
n
(cid:88)
P(X˜n =Y˜n,X¯n =Y¯n) E[c(X˜(t),X¯(t))X˜n =Y˜n,X¯n =Y¯n]
≥ |
t=1
n
(cid:88)
=P(X˜n =Y˜n,X¯n =Y¯n) E[c(Y˜(t),Y¯(t))X˜n =Y˜n,X¯n =Y¯n]
|
t=1
n n
(cid:88) (cid:88)
= E[c(Y˜(t),Y¯(t))] P(X˜n =Y˜n or X¯n =Y¯n) E[c(Y˜(t),Y¯(t))X˜n =Y˜n or X¯n =Y¯n]
− ̸ ̸ | ̸ ̸
t=1 t=1
n
(cid:88)
E[c(Y˜(t),Y¯(t))] nc P(X˜n =Y˜n or X¯n =Y¯n)
max
≥ − ̸ ̸
t=1
n
(cid:88)
E[c(Y˜(t),Y¯(t))] nc (P(X˜n =Y˜n)+P(X¯n =Y¯n))
max
≥ − ̸ ̸
t=1
≥ϕ(p Y˜n,p Y¯n) −nc max(P(X˜n ̸=Y˜n)+P(X¯n ̸=Y¯n)), (24)
where (24) is due to p
Y˜nY¯n
∈Π(p Y˜n,p Y¯n). Therefore,
n
(cid:88)
inf sup E[c(X˜(t),X¯(t))]
p X˜nX¯n∈Π(p X˜n,p X¯n) pX˜nY˜n∈Π(pX˜n,pY˜n) t=1
p X¯nY¯n ∈Π(p X¯n,p Y¯n)
≥p
X˜nX¯n∈Πin (pf
X˜n,p X¯n)
pX˜nY˜n∈s Πu (pp
X˜n,pY˜n)
ϕ(p Y˜n,p Y¯n) −nc max(P(X˜n ̸=Y˜n)+P(X¯n ̸=Y¯n)). (25)
p X¯nY¯n ∈Π(p X¯n,p Y¯n)
Note that
n n
(cid:88) (cid:88)
inf sup E[c(X˜(t),X¯(t))]= inf E[c(X˜(t),X¯(t))]
p X˜nX¯n∈Π(p X˜n,p X¯n) pX˜nY˜n∈Π(pX˜n,pY˜n) t=1 p X˜nX¯n∈Π(p X˜n,p X¯n) t=1
p X¯nY¯n ∈Π(p X¯n,p Y¯n)
=ϕ(p X˜n,p X¯n) (26)
and
p
X˜nX¯n∈Πin (pf
X˜n,p X¯n)
pX˜nY˜n∈s Πu (pp
X˜n,pY˜n)
ϕ(p Y˜n,p Y¯n) −nc max(P(X˜n ̸=Y˜n)+P(X¯n ̸=Y¯n))
p X¯nY¯n ∈Π(p X¯n,p Y¯n)
=
p
X˜nX¯n∈Πin (pf
X˜n,p
X¯n)ϕ(p Y˜n,p Y¯n) −nc max(d TV(p X˜n,p Y˜n)+d TV(p X¯n,p Y¯n))
=ϕ(p Y˜n,p Y¯n) −nc max(d TV(p X˜n,p Y˜n)+d TV(p X¯n,p Y¯n)). (27)
Substituting (26) and (27) into (25) proves
ϕ(p X˜n,p X¯n) ≥ϕ(p Y˜n,p Y¯n) −nc max(d TV(p X˜n,p Y˜n)+d TV(p X¯n,p Y¯n)). (28)
By symmetry, we also have
ϕ(p Y˜n,p Y¯n) ≥ϕ(p X˜n,p X¯n) −nc max(d TV(p X˜n,p Y˜n)+d TV(p X¯n,p Y¯n)). (29)
Combining (28) and (29) yields the desired result.
APPENDIXB
PROOFOFPROPOSITION2
For any (D(j),P(j)) and ϵ>0, there exists p such that
U(j)Xˆ(j)X
|
I(X;U(j)) R(D(j),P(j))+ϵ,
≤
X U(j) Xˆ(j) form a Markov chain,
↔ ↔
E[∆(X,Xˆ(j))] D(j),
≤
E[ϕ(p ( U(j)),p ( U(j)))] P(j), j =0,1.
X |U(j) ·| Xˆ |U(j) ·| ≤9
Let Q Ber(λ) with λ [0,1]. Construct V and Xˆ such that p ( j) = p () and p ( ,j) = p ( ),
j = 0,1∼ . Further let X be∈ jointly distributed with (V,Q,Xˆ) such thV a| tQ p·| =U( pj) · withXˆ p|VQ ·|· ( ,j) =Xˆ p(j) |U(j) (·|· ),
j =0,1. Setting U :=(V,Q), it can be verified that
X |VQXˆ X |VQ X |VQ ·|· X |U(j) ·|·
I(X;U)=(1 λ)I(X;U(0))+λI(X;U(1)) (1 λ)R(D(0),P(0))+λR(D(1),P(1))+ϵ,
− ≤ −
X U Xˆ form a Markov chain,
↔ ↔
E[∆(X,Xˆ)]=(1 λ)E[∆(X,Xˆ(0))]+λE[∆(X,Xˆ(1))] (1 λ)D(0)+λD(1),
− ≤ −
E[ϕ(p ( U),p ( U))]=(1 λ)E[ϕ(p ( U(0)),p ( U(0)))]+λE[ϕ(p ( U(1)),p ( U(1)))]
X |U ·| Xˆ |U ·| − X |U(1) ·| Xˆ |U(0) ·| X |U(1) ·| Xˆ |U(1) ·|
(1 λ)P(0)+λP(1),
≤ −
which further implies
R((1 λ)D(0)+λD(1),(1 λ)P(0)+λP(1)) (1 λ)R(D(0),P(0))+λR(D(1),P(1))+ϵ.
− − ≤ −
Sending ϵ 0 proves the desired result.
→
APPENDIXC
PROOFOFTHEOREM1
Achievability: We first establish the following two technical lemmas.
Lemma 1: Let p and p denote respectively the output distribution and the backward test channnel induced by p
U XU X
and p . For any δ > 0, th| ere exists a stochastic function ψ(n) : n (n) (n) n, with (n) (n)(p ) and
(n)
U |X
(n) = , mapping Xn to Un according to some conditional
dX istri→ butiG
on
p∪B s⊆ uchU
that
G ⊆ Tδ U
UnXn
G ∩B ∅ |
1
log (n) (n) I(X;U)+δ,
n⌈ |G ∪B |⌉≤
P Un (n) δ,
{ ∈B }≤
d (p ( un),pn ( un)) δ, un (n).
TV Xn |Un ·| X |U ·| ≤ ∈G
Proof: Let p play the role of posterior reference map in the sense of [4]. By [4, Theorem 3], for every sufficiently
XU
small η > 0, there| exists a stochastic function ψ(n) : n (n) u˜n , with (n) (n)(p ) and u˜n n (n)(p ),
η U η U
X → C ∪{ } C ⊆ T ∈ U \T
mapping Xn to Un according to some conditional distribution p such that
UnXn
|
1
log (n) u˜n I(X;U)+η,
n⌈ |C ∪{ }|⌉≤
P Un =u˜n η,
{ }≤
E[d (p ( Un),pn ( Un))] η.
TV Xn |Un ·| X |U ·| ≤
Let (n) := un (n) :d (p ( un),pn ( un)) δ and n :=( (n) (n)) u˜n . We have
G { ∈C TV Xn |Un ·| X |U ·| ≤ } B C \G ∪{ }
P Un (n) =P d (p ( Un),pn ( Un))>δ or Un =u˜n
{ ∈B } { TV Xn |Un ·| X |U ·| }
P d (p ( Un),pn ( Un))>δ +η
≤ η{ TV Xn |Un ·| X |U ·| }
+η, (30)
≤ δ
where (30) is due to Markov’s inequality. Choosing η
δ2
completes the proof of Proposition 1.
≤ 1+δ
Lemma 2: For p X˜nX¯n =p X˜np X¯n and p Y˜nY¯n =p Y˜np Y¯n,
|E[∆(X˜n,X¯n)] −E[∆(Y˜n,Y¯n)] |≤2n∆ max(d TV(p X˜n,p Y˜n)+d TV(p X¯n,p Y¯n)),
where ∆ :=max ∆(x,xˆ).
max x,xˆ
∈X10
Proof: Note that
E[∆(X˜n,X¯n)]
(cid:88)
= p X˜n(x˜n)p X¯n(x¯n)∆(x˜n,x¯n)
x˜n,x¯n n
(cid:88)∈X (cid:88)
≤
p Y˜n(x˜n)p Y¯n(x¯n)∆(x˜n,x¯n)+ |p X˜n(x˜n)p X¯n(x¯n) −p Y˜n(x˜n)p Y¯n(x¯n) |∆(x˜n,x¯n)
x˜n,x¯n n x˜n,x¯n n
∈X (cid:88) ∈X
=E[∆(Y˜n,Y¯n)]+ |p X˜n(x˜n)p X¯n(x¯n) −p Y˜n(x˜n)p Y¯n(x¯n) |∆(x˜n,x¯n)
x˜n,x¯n n
∈X (cid:88)
≤E[∆(Y˜n,Y¯n)]+n∆
max
|p X˜n(x˜n)p X¯n(x¯n) −p Y˜n(x˜n)p Y¯n(x¯n)
|
x˜n,x¯n n
(cid:88)∈X
=E[∆(Y˜n,Y¯n)]+n∆
max
|p X˜n(x˜n)p X¯n(x¯n) −p Y˜n(x˜n)p X¯n(x¯n)
|
x˜n,x¯n n
(cid:88) ∈X
+n∆
max
|p Y˜n(x˜n)p X¯n(x¯n) −p Y˜n(x˜n)p Y¯n(x¯n)
|
x˜n,x¯n n
∈X (cid:88) (cid:88)
=E[∆(Y˜n,Y¯n)]+n∆
max
p X¯n(x¯n) |p X˜n(x˜n) −p Y˜n(x˜n)
|
x¯n n x˜n n
(cid:88) (cid:88)∈X ∈X
+n∆
max
p Y˜n(x˜n) |p X¯n(x¯n) −p Y¯n(x¯n)
|
x˜n x¯n n
∈X
=E[∆(Y˜n,Y¯n)]+2n∆ max(d TV(p X˜n,p Y˜n)+d TV(p X¯n,p Y¯n)). (31)
By symmetry, we also have
E[∆(Y˜n,Y¯n)] ≤E[∆(X˜n,X¯n)]+2n∆ max(d TV(p X˜n,p Y˜n)+d TV(p X¯n,p Y¯n)). (32)
Combining (31) and (32) yields the desired result.
We shall prove the achievability part of Theorem 1 by treating the following three cases separately.
1) D =0:
It is easy to verify that R(0,P) = H(X). Since E[∆(Xn,Xˆn)] = 0 implies p = p and consequently
XnM XˆnM
E[ϕ(p ( M),p ( M))]=0, the problem boils down to lossless source codin| g, for whic|h H(X) is known to
be
theX inn fi|M mu· m|
of
acXˆ hn i|eM vab·|
le rates. This proves the achievability part for case 1).
2) D >0 and P =0:
In light of Proposition 2, R(D,0) is continuous in D for D > 0. As a consequence, for any ϵ > 0, there exists ρ > 0
satisfying R(D ρ,0) R(D,0)+ϵ. By the definition of R(D ρ,0), we can find p such that I(X;U) =
− ≤ − UXˆ X
R(D ρ,0), X U Xˆ form a Markov chain, E[∆(X,Xˆ)] D ρ, and p = p| . Let p be the
condit−
ional
distrib↔
ution
s↔
pecified in Lemma 1, and set p
=≤
p
−
p
wX it|hU Xˆ |U Un |Xn
UnXˆnXn UnXn XˆnUn
| | |
p ( un)=p ( un), un (n) (n). (33)
Xˆn |Un ·| Xn |Un ·| ∈G ∪B
Construct a bijection κ: (n) (n) , where is a set of binary codewords of length log (n) (n) . Define
G ∪B →M M ⌈ |G ∪B |⌉
M :=κ(Un). For the rate, we have
1 1
E[ℓ(M)]= log (n) (n)
n n⌈ |G ∪B |⌉
I(X;U)+δ
≤
=R(D ρ,0)+δ
−
R(D,0)+ϵ+δ.
≤
As to the perception loss, it follows by (33) that p =p , which further implies
XˆnM XnM
| |
1
E[ϕ(p ( M),p ( M))]=0. (34)
n Xn |M ·| Xˆn |M ·|
It remains to analyze the distortion loss. Note that
1 1
E[∆(Xn,Xˆn)]= E[E[∆(Xn,Xˆn)Un]]
n n |
1 (cid:88) 1 (cid:88)
= p (un)E[∆(Xn,Xˆn)Un =un]+ p (un)E[∆(Xn,Xˆn)Un =un]. (35)
Un Un
n | n |
un (n) un (n)
∈G ∈B11
For un (n),
∈G
E[∆(Xn,Xˆn)Un =un]
|
n
(cid:88)
E[∆(X,Xˆ)U =u(t)]+4n∆ d (p ( un),pn ( un)) (36)
≤ | max TV Xn |Un ·| X |U ·|
t=1
n
(cid:88)
E[∆(X,Xˆ)U =u(t)]+4n∆ δ
max
≤ |
t=1
n(1+δ)E[E[∆(X,Xˆ)U]]+4n∆ δ (37)
max
≤ |
=n(1+δ)E[∆(X,Xˆ)]+4n∆ δ
max
n(1+δ)(D ρ)+4n∆ δ, (38)
max
≤ −
where (36) is due to Lemma 2, and (37) is due to the typical average lemma [24, p. 26]. For un (n),
∈B
E[∆(Xn,Xˆn)Un =un] n∆ . (39)
max
| ≤
Substituting (38) and (39) into (35) gives
1
E[∆(Xn,Xˆn)] ((1+δ)(D ρ)+4∆ δ)P(Un (n))+∆ P(Un B(n))
max max
n ≤ − ∈G ∈
(1+δ)(D ρ)+5∆ δ
max
≤ −
=D ρ+(D ρ+5∆ )δ.
max
− −
Choosing δ max ϵ, ρ ensures
≤ { D −ρ+5∆max}
1
E[ℓ(M)] R(D,0)+2ϵ, (40)
n ≤
1
E[∆(Xn,Xˆn)] D. (41)
n ≤
In view of (34), (40), (41), and the fact that ϵ>0 is arbitrary, the achievability part for case 2) is proved.
3) D >0 and P >0:
InlightofProposition2,R(D,P)iscontinuousin(D,P)forD >0andP >0.Asaconsequence,foranyϵ>0,there
existsρ>0satisfyingR(D ρ,P ρ) R(D,P)+ϵ.BythedefinitionofR(D ρ,P ρ),wecanfindp suchthat
− − ≤ − − UXˆ X
I(X;U)=R(D ρ,P ρ),X U Xˆ formaMarkovchain,E[∆(X,Xˆ)] D ρ,andE[ϕ ( U),p| ( U)]
P ρ. Let p
−
be
−
the
cond↔ itiona↔
l distribution specified in Lemma 1,
and≤
set
p− =pX |U ·|
p
Xˆ |U w·|
ith
≤
− Un |Xn UnXˆn |Xn Un |Xn Xˆn |Un
p ( un)=pn ( un), un (n),
XˆnUn ·| Xˆ U ·| ∈G
| |
p ( un)=p ( un), un (n).
Xˆn |Un ·| Xn |Un ·| ∈B
Construct a bijection κ: (n) (n) , where is a set of binary codewords of length log (n) (n) . Define
G ∪B →M M ⌈ |G ∪B |⌉
M :=κ(Un). The rate can be bounded as follows:
1 1
E[ℓ(M)]= log (n) (n)
n n⌈ |G ∪B |⌉
I(X;U)+δ
≤
=R(D ρ,P ρ)+δ
− −
R(D,P)+ϵ+δ.
≤
As to the distoriton loss, we have
1 1
E[∆(Xn,Xˆn)]= E[E[∆(Xn,Xˆn)Un]]
n n |
1 (cid:88) 1 (cid:88)
= p (un)E[∆(Xn,Xˆn)Un =un]+ p (un)E[∆(Xn,Xˆn)Un =un]. (42)
Un Un
n | n |
un (n) un (n)
∈G ∈B12
For un (n),
∈G
E[∆(Xn,Xˆn)Un =un]
|
n
(cid:88)
E[∆(X,Xˆ)U =u(t)]+2n∆ d (p ( un),pn ( un)) (43)
≤ | max TV Xn |Un ·| X |U ·|
t=1
n
(cid:88)
E[∆(X,Xˆ)U =u(t)]+2n∆ δ
max
≤ |
t=1
n(1+δ)E[E[∆(X,Xˆ)U]]+2n∆ δ (44)
max
≤ |
=n(1+δ)E[∆(X,Xˆ)]+2n∆ δ
max
n(1+δ)(D ρ)+2n∆ δ, (45)
max
≤ −
where (43) is due to Lemma 2, and (44) is due to the typical average lemma [24, p. 26]. For un (n),
∈B
E[∆(Xn,Xˆn)Un =un] n∆ . (46)
max
| ≤
Substituting (45) and (46) into (42) gives
1
E[∆(Xn,Xˆn)] ((1+δ)(D ρ)+2∆ δ)P(Un (n))+∆ P(Un B(n))
max max
n ≤ − ∈G ∈
(1+δ)(D ρ)+3∆ δ
max
≤ −
=D ρ+(D ρ+3∆ )δ.
max
− −
It remains to analyze the perception loss. Note that
1
E[ϕ(p ( M),p ( M))]
n Xn |M ·| Xˆn |M ·|
1 (cid:88) 1 (cid:88)
= p (un)ϕ(p ( un),p ( un))+ p (un)ϕ(p ( un),p ( un))
n Un Xn |Un ·| Xˆn |Un ·| n Un Xn |Un ·| Xˆn |Un ·|
un (n) un (n)
∈G ∈B
1 (cid:88)
= p (un)ϕ(p ( un),p ( un))
n Un Xn |Un ·| Xˆn |Un ·|
un (n)
∈G
1 (cid:88)
p (un)(ϕ(pn ( un),pn ( un))+nc d (p ( un),pn ( un))) (47)
≤ n Un X |U ·| Xˆ |U ·| max TV Xn |Un ·| X |U ·|
un (n)
∈G
1 (cid:88)
p (un)ϕ(pn ( un),pn ( un))+c δ
≤ n Un X |U ·| Xˆ |U ·| max
un (n)
∈G
n
1 (cid:88) (cid:88)
= p (un) ϕ(p ( u(t)),p ( u(t)))+c δ (48)
n Un X |U ·| Xˆ |U ·| max
un (n) t=1
(cid:88)∈G
p (un)(1+δ)E[p ( U),p ( U)]+c δ (49)
≤ Un X |U ·| Xˆ |U ·| max
un (n)
∈G
(1+δ)E[p ( U),p ( U)]+c δ
≤ X |U ·| Xˆ |U ·| max
(1+δ)(P ρ)+c δ
max
≤ −
=P ρ+(P ρ+c )δ,
max
− −
where (47) is due to part 3) of Proposition 1, (48) is due to part 1) of Proposition 1, and (49) is due to the typical
average lemma [24, p. 26]. Choosing δ max ϵ, ρ , ρ ensures
≤ { D −ρ+3∆max P −ρ+cmax}
1
E[ℓ(M)] R(D,P)+2ϵ,
n ≤
1
E[∆(Xn,Xˆn)] D,
n ≤
1
E[ϕ(p ( M),p ( M))] P.
n Xn |M ·| Xˆn |M ·| ≤
Since ϵ>0 is arbitrary, the achievability part for case 3) is proved.13
Converse: Let R be an achievable rate subject to distortion constraint D and perception constraint P. By Definition 1, we
can find encoder f(n) and decoder g(n) satisfying
1
E[ℓ(M)] R, (50)
n ≤
1
E[∆(Xn,Xˆn)] D, (51)
n ≤
1
E[ϕ(p ( M),p ( M))] P. (52)
n Xn |M ·| Xˆn |M ·| ≤
Let U(t):=M, t [1:n]. Moreover, let T be uniformly distributed over [1:n] and independent of (Xn,M,Xˆn). Note that
∈
1 1
E[ℓ(M)] H(M)
n ≥ n
1
I(Xn;M)
≥ n
n
1 (cid:88)
= I(X(t);M Xt 1)
−
n |
t=1
n
1 (cid:88)
= I(X(t);M,Xt 1)
−
n
t=1
n
1 (cid:88)
I(X(t);M)
≥ n
t=1
n
1 (cid:88)
= I(X(t);U(t))
n
t=1
=I(X(T);U(T)T)
|
=I(X(T);U(T),T), (53)
where (53) is due to the fact that X(T) is independent of T as the conditional distribution of X(T) given T is the same as
the marginal distribution of X(T) (which is p ). In addition, we have
X
n
1 1 (cid:88)
E[∆(Xn,Xˆn)]= E[∆(X(t),Xˆ(t))]
n n
t=1
=E[E[∆(X(T),Xˆ(T))T]]
|
=E[∆(X(T),Xˆ(T))] (54)
and
n
1 1 (cid:88)
E[ϕ(p ( M),p ( M))] E[ϕ(p ( M),p ( M))] (55)
n Xn |M ·| Xˆn |M ·| ≥ n X(t) |M ·| Xˆ(t) |M ·|
t=1
n
1 (cid:88)
= E[ϕ(p ( U(t)),p ( U(t)))]
n X(t) |U(t) ·| Xˆ(t) |U(t) ·|
t=1
=E[ϕ(p ( U(T),T),p ( U(T),T))], (56)
X(T) |U(T),T ·| Xˆ(T) |U(T),T ·|
where (55) follows by part 1) of Proposition 1. It is clear that X(T) (U(T),T) Xˆ(T) form a Markov chain. Setting
↔ ↔
U :=(U(T),T) and combining (53), (54), (56) with (50), (51), (52) completes the proof.
APPENDIXD
PROOFOFTHEOREM2
We first establish the following technical lemma.
Lemma 3: For D 0 and P 0,
≥ ≥
¯h(D,P)=maxH (a) (57)
b
a,aˆ
s.t. (1 a)aˆ+a(1 aˆ) D,
− − ≤
a aˆ P,
| − |≤
0 a 1,
≤ ≤
0 aˆ 1.
≤ ≤14
Proof: By symmetry, it suffices to consider a [0,1]. As a consequence, there is no loss of optimality in restricting
∈ 2
aˆ [0,a]. We shall treat the following cases separately.
∈
1) D [1, ):
∈ 2 ∞
It is clear that H (a) log2 and this upper bound is attained at (a,aˆ)=(1,1).
b ≤ 2 2
2) D [0,1) and P [D, ):
∈ 2 ∈ ∞
Sincea (1 a)aˆ+a(1 aˆ) D < 1,itfollowsthatH (a) H (D).Thisupperboundisattainedat(a,aˆ)=(D,0).
≤ − − ≤ 2 b ≤ b
3) D [0,1) and P [0,D):
∈ 2 ∈
Setting (1 a)aˆ+a(1 aˆ)=D and a aˆ=P gives
− − −
1+P √1+P2 2D
a= − − , (58)
2
1 P √1+P2 2D
aˆ= − − − . (59)
2
Since H (1+P √1+P2 2D) is a monotonically increasing function of (D,P) for D [0,1) and P [0,D), the
b − 2 − ∈ 2 ∈
maximum value of the optimization problem in (57) is attained at (a,aˆ) given by (58) and (59).
This completes the proof of Lemma 3.
Now we proceed to prove Theorem 2. Let p a conditional distribution that satisfies (3), (4), and (5). For any u ,
UXˆ X ∈U
|
H(X U =u)=H (P (1u)),
b XU
| | |
E[d (X,Xˆ)U =u]=(1 p (1u))p (1u)+p (1u)(1 p (1u)),
H | − X |U | Xˆ |U | X |U | − Xˆ |U |
E[d (p ( u),p ( u))]= p (1u) p (1u).
TV X |U ·| Xˆ |U ·| | X |U | − Xˆ |U | |
In light of Lemma 3,
H(X U =u) ¯h(E[d (X,Xˆ)U =u],d (p ( u),p ( u)))
| ≤ H | TV X |U ·| Xˆ |U ·|
Therefore,
I(X;U)=log2 H(X U)
− |
log2 E[¯h(E[d (X,Xˆ)U],d (p ( U),p ( U)))]
≥ − H | TV X |U ·| Xˆ |U ·|
log2 ¯h((E[d (X,Xˆ)],E[d (p ( U),p ( U))]))
≥ − H TV X |U ·| Xˆ |U ·|
log2 h(D,P), (60)
≥ −
where (60) is due to the fact that ¯h(D,P) is non-decreasing in (D,P) (as ¯h(D,P) is non-decreasing in (D,P)). This proves
R(D,P) log2 h(D,P).
≥ −
According to Carathéodory’s theorem, for any D 0 and P 0, we can write
≥ ≥
3
(cid:88)
¯h(D,P)= α(i)¯h(D(i),P(i)),
i=1
where α(i) 0, D(i) 0, P(i) 0, i [1 : 3], with (cid:80)3 α(i) = 1, (cid:80)3 α(i)D(i) = D, and (cid:80)3 α(i)P(i) = P. By
≥ ≥ ≥ ∈ i=1 i=1 i=1
Lemma 3, there exist (a(i),aˆ(i)) [0,1]2 such that H (a(i)) = ¯h(D(i),P(i)), (1 a(i))aˆ(i) +a(i)(1 aˆ(i)) D(i), and
b
a(i) aˆ(i) P(i), i [1:3]. Set∈ :=[1:6] and construct a Markov chain X − U Xˆ with − ≤
| − |≤ ∈ U ↔ ↔
α(u)
p (u)= , u [1:3],
U
2 ∈
α(u 3)
−
p (u)= , u [4:6],
U
2 ∈
p (1u)=a(u), u [1:3],
XU
| | ∈
p (1u)=1 a(u 3), u [4:6],
XU −
| | − ∈
p (1u)=aˆ(u), u [1:3],
Xˆ U | ∈
|
p (1u)=1 aˆ(u 3), u [4:6].
Xˆ U | − − ∈
|15
Note that
6
(cid:88)
p (1)= p (u)p (1u)
X U XU
| |
u=1
3 6
1 (cid:88) 1 (cid:88)
= α(u)a(u)+ α(u 3)(1 a(u 3))
− −
2 2 −
u=1 u=4
3 3
1 (cid:88) 1 (cid:88)
= α(u)a(u)+ α(u)(1 a(u))
2 2 −
u=1 u=1
3
1(cid:88)
= α(u)
2
i=1
1
= .
2
So our construction preserves the source distribution. It can be verified that
6
(cid:88)
H(X U)= p (u)H(X U =u)
U
| |
u=1
3 6
1 (cid:88) 1 (cid:88)
= α(u)H (a(u))+ α(u 3)H (1 a(u 3))
b − b −
2 2 −
u=1 u=4
3
(cid:88)
= α(u)H (a(u))
b
u=1
3
(cid:88)
= α(u)¯h(D(i),P(i))
u=1
=¯h(D,P).
Moreover,
6
(cid:88)
E[d (X,Xˆ)]= p (u)E[d (X,Xˆ)U =u]
H U H
|
u=1
3 6
1 (cid:88) 1 (cid:88)
= α(u)((1 a(u))aˆ(u)+a(u)(1 aˆ(u)))+ α(u 3)(a(u 3)(1 aˆ(u 3))+(1 a(u 3))aˆ(u 3))
− − − − −
2 − − 2 − −
u=1 u=4
3
(cid:88)
= α(u)((1 a(u))aˆ(u)+a(u)(1 aˆ(u)))
− −
u=1
3
(cid:88)
α(u)D(u)
≤
u=1
=D
and
6
(cid:88)
E[d (p ( U),p ( U))]= p (u)d (p ( u),p ( u))
TV X |U ·| Xˆ |U ·| U TV X |U ·| Xˆ |U ·|
u=1
3 6
1 (cid:88) 1 (cid:88)
= α(u) a(u) aˆ(u) + α(u 3) aˆ(u 3) a(u 3)
− − −
2 | − | 2 | − |
u=1 u=4
3
(cid:88)
= α(u) a(u) aˆ(u)
| − |
u=1
3
(cid:88)
α(u)P(u)
≤
u=1
=P.16
Therefore, we must have R(D,P)=log2 ¯h(D,P). Invoking Theorem 1 completes the proof.
−
APPENDIXE
ONTHECONCAVITYOF¯h
For D (0,1) and P (0,D),
∈ 2 ∈
(cid:18) (cid:19)
∂¯h 1 1 P +υ
(D,P)= log − ,
∂D 2υ 1+P υ
(cid:18) − (cid:19)
∂¯h υ P 1 P +υ
(D,P)= − log − .
∂P 2υ 1+P υ
−
where υ :=√1+P2 2D; moreover,
−
∂2¯h 1 (cid:18) 1 P +υ(cid:19) 1
(D,P)= log − ,
∂D2 2υ3 1+P υ − υ2(1 (υ P)2)
∂2¯h 1 2D (cid:18) 1− P +υ(cid:19) − (− υ P)2
(D,P)= − log − − ,
∂P2 − 2υ3 1+P υ − υ2(1 (υ P)2)
∂2¯h P (cid:18) 1 P +− υ(cid:19) υ− P−
(D,P)= log − − .
∂D∂P −2υ3 1+P υ − υ2(1 (υ P)2)
− − −
Evaluating the Hessian matrix of ¯h at (D,P)=(1 ϵ,ϵ) gives
2 −
(cid:32) ∂2h¯(cid:0)1 ϵ,ϵ(cid:1) ∂2h¯ (cid:0)1 ϵ,ϵ(cid:1)(cid:33)  1 +o(cid:16) 1 (cid:17) 1 +o(cid:16) 1 (cid:17)
∂ ∂D 2h¯2 (cid:0)2 1− ϵ,ϵ(cid:1) ∂ ∂D 2∂ h¯P (cid:0)12 − ϵ,ϵ(cid:1) =−2√ 12ϵ +o(cid:16) √ 1ϵ (cid:17) −√2 2ϵ +o(1√ )ϵ . (61)
∂D∂P 2 − ∂P2 2 − −√2ϵ √ϵ −
Note that
(cid:32) ∂2h¯(cid:0)1 ϵ,ϵ(cid:1) ∂2h¯ (cid:0)1 ϵ,ϵ(cid:1)(cid:33)(cid:18) a(cid:19)
ϵlim 0(a,b) ∂ ∂D 2h¯2 (cid:0)2 1− ϵ,ϵ(cid:1) ∂ ∂D 2∂ h¯P (cid:0)12 − ϵ,ϵ(cid:1) b
→ ∂D∂P 2 − ∂P2 2 −
(cid:40)
, a2+4ab<0,
= ∞
, a2+4ab>0.
−∞
Therefore, ¯h is neither concave or convex. One can also reach the same conclusion by observing that the eigvalues of the
Hessian matrix in (61) are
1 √17 (cid:18) 1 (cid:19)
− ± +o .
4√2ϵ √ϵ
APPENDIXF
PROOFOFTHEOREM3
Regarding (13), it suffices to show
R (D,P) R(D,P) (62)
C
≤
sincetheconversepartintheproofofTheorem1continuestoholdforcontinuousalphabetsources.Weshalltreatthefollowing
two cases separately.
1) D >0 and P =0:
In light of Proposition 2, R(D,0) is continuous in D for D > 0. So for any ϵ > 0, there exists ρ > 0 satisfying
R(D ρ,0) R(D,0)+ϵ. By the definition of R(D ρ,0), we can find p such that I(X;U) R(D ρ,0)+ϵ,
− ≤ − UXˆ X ≤ −
X U Xˆ form a Markov chain, E[ X Xˆ 2] D ρ, and p =p | . Let ξ be a vector quantizer that maps
eac↔ hpoin↔
tinRL
toitsnearestneighbor∥
in
−
:=
∥
1
≤
[
N− :N]L,wheX re|U
N
isX aˆ |pU
ositiveinteger.Moreover,letY :=ξ(X)
Y √N −
and Yˆ = ξ(Xˆ). Note that Y X U Xˆ Yˆ form a Markov chain and p = p . As a consequence, we
have
↔ ↔ ↔ ↔ Y |U Xˆ |U
I(Y;U) I(X;U)
≤
R(D ρ,0)+ϵ
≤ −
R(D,0)+2ϵ (63)
≤17
and
E[W2(p ( U),p ( U))]=0. (64)
2 Y |U ·| Yˆ |U ·|
Moreover,
(cid:34) (cid:35)
L
(cid:88)
E[ Y Yˆ 2]=E Y Yˆ 2
ℓ ℓ
∥ − ∥ | − |
ℓ=1
(cid:34) L (cid:18) (cid:19)2(cid:35)
(cid:88) 1
E X Xˆ +
ℓ ℓ
≤ | − | √N
ℓ=1
(cid:34) L (cid:18) (cid:19)(cid:35)
(cid:88) 2 1
=E X Xˆ 2+ X Xˆ +
ℓ ℓ ℓ ℓ
| − | √N| − | N
ℓ=1
(cid:114)
L L
E[ X Xˆ 2]+2 E[ X Xˆ 2]+
≤ ∥ − ∥ N ∥ − ∥ N
(cid:114)
L L
D ρ+2 (D ρ)+
≤ − N − N
=:D . (65)
′
Combining (63), (64), and (65) yields
R(D ,0p ) R(D,0)+2ϵ,
′ Y
| ≤
where R(D,P p ) denotes the counterpart of R(D,P) for source Y.
Y
According to T| heorem 1, there exists p =pnp p such that 1E[ℓ(M)] R(D ,0p )+ϵ, 1E[ Yn
Yˆn 2] D , and p = p .
CY on nM strYˆ un
ct p
Y M |Yn Yˆn
|
=M
p p
n
p
≤ with′ p| Y =n
p
∥ −
=
pn
∥
.
C≤ learl′
y, we
haY vn e|M
p
Yˆ =n |M
p and
conX sen qY un eM ntlYˆ ynXˆn Xn |Yn YnMYˆn Xˆn |Yˆn Xn |Yn Xˆn |Yˆn
XY XnM XˆnM
| | |
1
E[W2(p ( M),p ( M))]=0. (66)
n 2 Xn |M ·| Xˆn |M ·|
Moreover,
n L
1 1 (cid:88)(cid:88)
E[ Xn Xˆn 2]= E[X (t) Xˆ (t)2]
ℓ ℓ
n ∥ − ∥ n | − |
t=1ℓ=1
n L 4
1 (cid:88)(cid:88)(cid:88)
= P (Y(t),Yˆ(t)) E[X (t) Xˆ (t)2 (Y(t),Yˆ(t)) ], (67)
ℓ,i ℓ ℓ ℓ,i
n { ∈S } | − | | ∈S
t=1ℓ=1i=1
where
:= (y,yˆ) 2 : y =√N, yˆ =√N ,
ℓ,1 ℓ ℓ
S { ∈Y | |̸ | |̸ }
:= (y,yˆ) 2 : y =√N, yˆ =√N ,
ℓ,2 ℓ ℓ
S { ∈Y | | | | }
:= (y,yˆ) 2 : y =√N, yˆ =√N ,
ℓ,3 ℓ ℓ
S { ∈Y | | | |̸ }
:= (y,yˆ) 2 : y =√N, yˆ =√N .
ℓ,4 ℓ ℓ
S { ∈Y | |̸ | | }
For (y,yˆ) ,
ℓ,1
∈S
(cid:18) (cid:19)2
1
E[X (t) Xˆ (t)2 Y(t)=y,Yˆ(t)=yˆ] y yˆ + .
ℓ ℓ ℓ ℓ
| − | | ≤ | − | √N
Therefore,
P (Y(t),Yˆ(t)) E[X (t) Xˆ (t)2 (Y(t),Yˆ(t)) ]
ℓ,1 ℓ ℓ ℓ,1
{ ∈S } | − | | ∈S
(cid:88)
= p (y,yˆ)E[X (t) Xˆ (t)2 Y(t)=y,Yˆ(t)=yˆ]
Y(t)Yˆ(t)
|
ℓ
−
ℓ
| |
(y,yˆ) ∈Sℓ,1
(cid:18) (cid:19)2
(cid:88) 1
p (y,yˆ) y yˆ +
≤
Y(t)Yˆ(t)
|
ℓ
−
ℓ
| √N
(y,yˆ) ∈Sℓ,1
(cid:18) (cid:19)2
(cid:88) 1
p (y,yˆ) y yˆ +
≤
Y(t)Yˆ(t)
|
ℓ
−
ℓ
| √N
(y,yˆ) 2
∈Y18
2 1
=E[Y (t) Yˆ (t)2]+ E[Y (t) Yˆ (t)]+
ℓ ℓ ℓ ℓ
| − | √N | − | N
2 (cid:113) 1
E[Y (t) Yˆ (t)2]+ E[Y (t) Yˆ (t)2]+ . (68)
ℓ ℓ ℓ ℓ
≤ | − | √N | − | N
For (y,yˆ) ,
ℓ,2
∈S
E[X (t) Xˆ (t)2 Y(t)=y,Yˆ =yˆ] 2E[X2(t)+Xˆ2(t)Y(t)=y,Yˆ(t)=yˆ]
| ℓ − ℓ | | ≤ ℓ ℓ |
=2E[X2(t)Y(t)=y]+2E[Xˆ2(t)Yˆ(t)=yˆ], (69)
ℓ | ℓ |
where (69) follows by the fact that X (t) Y(t) Yˆ(t) Xˆ (t) form a Markov chain. Therefore,
ℓ ℓ
→ ↔ ↔
P (Y(t),Yˆ(t)) E[X (t) Xˆ (t)2 (Y(t),Yˆ(t)) ]
ℓ,2 ℓ ℓ ℓ,2
{ ∈S } | − | | ∈S
(cid:88)
= p (y,yˆ)E[X (t) Xˆ (t)2 Y(t)=y,Yˆ(t)=yˆ]
Y(t)Yˆ(t)
|
ℓ
−
ℓ
| |
(y,yˆ) ∈Sℓ,2
(cid:88)
2 p (y,yˆ)(E[X2(t)Y(t)=y]+E[Xˆ2(t)Yˆ(t)=yˆ])
≤ Y(t)Yˆ(t) ℓ | ℓ |
(y,yˆ) ∈Sℓ,2
(cid:88) (cid:88)
2 p (y)E[X2(t)Y(t)=y]+2 p (yˆ)E[Xˆ2(t)Yˆ(t)=yˆ]
≤ Yℓ ℓ | Yˆ(t) ℓ |
y ∈Y:yℓ∈{−√N,√N
}
yˆ ∈Y:yˆℓ∈{−√N,√N
}
(cid:88)
=4 p (y)E[X2 Y =y] (70)
Y ℓ|
y ∈Y:yℓ∈{−√N,√N
}
4δ , (71)
ℓ,N
≤
where
(cid:26) (cid:27) (cid:20) (cid:12) (cid:21)
δ ℓ,N :=P |X ℓ
|≥
N √− N1 E X ℓ2(cid:12) (cid:12) (cid:12)|X ℓ
|≥
N √− N1 ,
and (70) is due to p =p =p . For (y,yˆ) ,
Xℓ(t)Y(t) Xˆ ℓ(t)Yˆ(t) XℓY ∈Sℓ,3
E[X (t) Xˆ (t)2 Y(t)=y,Yˆ(t)=yˆ] E[(2X (t))2 Y(t)=y,Yˆ(t)=yˆ]
ℓ ℓ ℓ
| − | | ≤ | | |
=4E[X2(t)Y(t)=y], (72)
ℓ |
where (72) follows by the fact that X (t) Y(t) Yˆ(t) form a Markov chain. Therefore,
ℓ
↔ ↔
P (Y(t),Yˆ(t)) E[X (t) Xˆ (t)2 (Y(t),Yˆ(t)) ]
ℓ,3 ℓ ℓ ℓ,3
{ ∈S } | − | | ∈S
(cid:88)
= p (y,yˆ)E[X (t) Xˆ (t)2 Y(t)=y,Yˆ(t)=yˆ]
Y(t)Yˆ(t)
|
ℓ
−
ℓ
| |
(y,yˆ) ∈Sℓ,3
(cid:88)
4 p (y,yˆ)E[X2(t)Y(t)=y]
≤ Y(t)Yˆ(t) ℓ |
(y,yˆ) ∈Sℓ,3
(cid:88)
4 p (y)E[X2(t)Y(t)=y]
≤ Y(t) ℓ |
y ∈Y:yℓ∈{−√N,√N
}
(cid:88)
=4 p (y)E[X2 Y =y] (73)
Y ℓ|
y ∈Y:yℓ∈{−√N,√N
}
4δ , (74)
ℓ,N
≤
where (73) is due to p =p . By symmetry
Xℓ(t)Y(t) XℓY
P (Y(t),Yˆ(t)) E[X (t) Xˆ (t)2 (Y(t),Yˆ(t)) ] 4δ . (75)
ℓ,4 ℓ ℓ ℓ,4 ℓ,N
{ ∈S } | − | | ∈S ≤
Substituting (68), (71), (74), and (75) into (67) gives
1 1 (cid:88)n (cid:88)L (cid:18) 2 (cid:113) 1 (cid:19)
E[ Xn Xˆn 2] E[Y (t) Yˆ (t)2]+ E[Y (t) Yˆ (t)2]+ +12δ
ℓ ℓ ℓ ℓ ℓ,N
n ∥ − ∥ ≤ n | − | √N | − | N
t=1ℓ=1
(cid:114) L
1 L L (cid:88)
E[ Yn Yˆn 2]+2 E[ Yn Yˆn 2]+ +12 δ
ℓ,N
≤ n ∥ − ∥ nN ∥ − ∥ N
ℓ=1
(cid:114) L
L L (cid:88)
D +2 D + +12 δ .
≤ ′ N ′ N ℓ,N
ℓ=119
Since E[ X 2] < , it follows by the dominated convergence theorem that (cid:80)L δ tends to zero as N .
∥ ∥ (cid:113)∞ ℓ=1 ℓ,N → ∞
Moreover, D +2 LD + L converges to D ρ as N . So when N is sufficiently large, we have
′ N ′ N − →∞
1
E[ Xn Xˆn 2] D. (76)
n ∥ − ∥ ≤
Combining (66) and (76) as well as the fact that
1
E[ℓ(M)] R(D ,0p )+ϵ R(D,0)+3ϵ
′ Y
n ≤ | ≤
yields R (D,0) R(D,0)+3ϵ. Since ϵ>0 is arbitrary, this proves (62) for P =0.
C
≤
2) D >0 and P >0:
In light of Proposition 2, R(D,P) is continuous in (D,P) for D > 0 and P > 0. So for any ϵ > 0, there exists
ρ > 0 satisfying R(D ρ,P ρ) R(D,P) + ϵ. By the definition of R(D ρ,P ρ), we can find p
− − ≤ − − UXˆ X
such that I(X;U) R(D ρ,P ρ) + ϵ, X U Xˆ form a Markov chain, E[ X Xˆ 2] D ρ, an| d
E[W2(p ( U),p≤ ( U)− )] P − ρ. Let Y :=↔ ξ(X)↔ and Yˆ := ξ(Xˆ). Note that Y ∥ X − U∥ X≤ˆ Yˆ− form a
Mark2
ov
X ch|U ain· .|
As
aXˆ c|oU ns· e| quenc≤
e,
− ↔ ↔ ↔ ↔
I(Y;U) I(X;U)
≤
R(D ρ,P ρ)+ϵ
≤ − −
R(D,P)+2ϵ. (77)
≤
The derivation of (65) continues to hold here, i.e.,
(cid:114)
L L
E[ Y Yˆ 2] E[ X Xˆ 2]+2 E[ X Xˆ 2]+ (78)
∥ − ∥ ≤ ∥ − ∥ N ∥ − ∥ N
(cid:114)
L L
D ρ+2 (D ρ)+
≤ − N − N
=D . (79)
′
Since (78) is valid for any jointly distributed (X,Xˆ) and the induced (Y,Yˆ) (via quantizer ξ), it follows that
(cid:114)
L L
W2(p ( u),p ( u)) W2(p ( u),p ( u))+2 W2(p ( u),p ( u))+ , u .
2 Y |U ·| Yˆ |U ·| ≤ 2 X |U ·| Xˆ |U ·| N 2 X |U ·| Xˆ |U ·| N ∈U
Therefore, we have
E[W2(p ( U),p ( U))]
2 Y |U ·| Yˆ |U ·|
(cid:114)
L L
E[W2(p ( U),p ( U))]+2 E[W2(p ( U),p ( U))]+
≤ 2 X |U ·| Xˆ |U ·| N 2 X |U ·| Xˆ |U ·| N
(cid:114)
L L
P ρ+2 (P ρ)+
≤ − N − N
=:P . (80)
′
Combining (77), (79), and (80) yields
R(D ,P p ) R(D,P)+2ϵ.
′ ′ Y
| ≤
AccordingtoTheorem1,thereexistsp =pnp p suchthat 1E[ℓ(M)] R(D ,P p )+ϵ, 1E[ Yn
Yˆn 2] D , and E[W2(p ( U),p
YnM
(
Yˆ Un
))]
Y PM
.
C|Y on nsY tˆ rn
u|
cM
t p
n
=p
≤
p
′ w′ | itY
h p
n =∥
pn
−
and∥ let≤ Xˆn′ :=Yˆn. Not2 e thY a| tU ·| Yˆ |U ·| ≤ ′ XnYnMYˆn Xn |Yn YnMYˆn Xn |Yn X |Y
1 1
E[ Xn Xˆn 2]= E[ Xn Yˆn 2]
n ∥ − ∥ n ∥ − ∥
n L
1 (cid:88)(cid:88)
= P Y (t) =√N E[X (t) Yˆ (t)2 Y (t) =√N]
ℓ ℓ ℓ ℓ
n {| |̸ } | − | || |̸
t=1ℓ=1
n L
1 (cid:88)(cid:88)
+ P Y (t) =√N E[X (t) Yˆ (t)2 Y (t) =√N]. (81)
ℓ ℓ ℓ ℓ
n {| | } | − | || |
t=1ℓ=120
For y 1 [1 N :N 1],
ℓ ∈ √N − −
(cid:34)(cid:18)
1
(cid:19)2(cid:12)
(cid:12)
(cid:35)
E[X (t) Yˆ (t)2 Y (t)=y ] E y Yˆ (t) + (cid:12)Y (t)=y .
| ℓ − ℓ | | ℓ ℓ ≤ | ℓ − ℓ | √N (cid:12) (cid:12) ℓ ℓ
Therefore,
P Y (t) =√N E[X (t) Yˆ (t)2 Y (t) =√N]
ℓ ℓ ℓ ℓ
{| |̸ } | − | || |̸
(cid:88)
= p (y )E[X (t) Yˆ (t)2 Y (t)=y ]
Yℓ(t) ℓ
|
ℓ
−
ℓ
| |
ℓ ℓ
yℓ∈√1 N[1 −N:N −1]
(cid:88)
(cid:34)(cid:18)
1
(cid:19)2(cid:12)
(cid:12)
(cid:35)
= p (y)E y Yˆ (t) + (cid:12)Y (t)=y
Y(t) | ℓ − ℓ | √N (cid:12) (cid:12) ℓ ℓ
yℓ∈√1 N[1 −N:N −1]
(cid:88)
(cid:34)(cid:18)
1
(cid:19)2(cid:12)
(cid:12)
(cid:35)
p (y)E y Yˆ (t) + (cid:12)Y (t)=y
≤ Y(t) | ℓ − ℓ | √N (cid:12) (cid:12) ℓ ℓ
yℓ∈√1 N[ −N:N]
2 1
=E[Y (t) Yˆ (t)2]+ E[Y (t) Yˆ (t)]+
ℓ ℓ ℓ ℓ
| − | √N | − | N
2 (cid:113) 1
E[Y (t) Yˆ (t)2+ E[Y (t) Yˆ (t)2]+ . (82)
ℓ ℓ ℓ ℓ
≤ | − | √N | − | N
For y √N,√N ,
ℓ
∈{− }
E[X (t) Yˆ (t)2 Y (t)=y ] E[(3X (t))2 Y (t)=y ]
ℓ ℓ ℓ ℓ ℓ ℓ ℓ
| − | | ≤ | | |
=9E[X2(t)Y (t)=y ].
ℓ | ℓ ℓ
Therefore,
P Y (t) =√N E[X (t) Yˆ (t)2 Y (t) =√N]
ℓ ℓ ℓ ℓ
{| | } | − | || |
(cid:88)
= p (y )E[X (t) Yˆ (t)2 Y (t)=y ]
Yℓ(t) ℓ
|
ℓ
−
ℓ
| |
ℓ ℓ
yℓ∈{−√N,√N
}
(cid:88)
9 p (y )E[X2(t)Y (t)=y ]
≤ Yℓ(t) ℓ ℓ | ℓ ℓ
y √N,√N
∈{− }
9δ . (83)
ℓ,N
≤
Substituting (82) and (83) into (81) gives
1 1 (cid:88)n (cid:88)L (cid:18) 2 (cid:113) 1 (cid:19)
E[ Xn Xˆn ] E[Y (t) Yˆ (t)2+ E[Y (t) Yˆ (t)2]+ +9δ
ℓ ℓ ℓ ℓ ℓ,N
n ∥ − ∥ ≤ n | − | √N | − | N
t=1ℓ=1
(cid:114) L
1 L L (cid:88)
E[ Yn Yˆn 2]+2 E[ Yn Yˆn 2]+ +9 δ (84)
ℓ,N
≤ n ∥ − ∥ nN ∥ − ∥ N
ℓ=1
(cid:114) L
L L (cid:88)
D +2 D + +9 δ .
≤ ′ N ′ N ℓ,N
ℓ=1
Since(84)isvalidforanyjointlydistributed(Yn,Yˆn)andtheinduced(Xn,Xˆn)(viaMarkovcouplingXn Yn Yˆn
with p =pn and setting Xˆn =Yˆn), it follows that ↔ ↔
XnYn XY
| |
1
W2(p ( m),p ( m))
n 2 Xn |M ·| Xˆn |M ·|
(cid:114) L
1 L L (cid:88)
W2(p ( m),p ( m))+2 W2(p ( m),p ( m))+ +9 δ , m .
≤ n 2 Yn |M ·| Yˆn |M ·| nN 2 Yn |M ·| Yˆn |M ·| N ℓ,N ∈M
ℓ=121
Therefore, we have
1
E[W2(p ( M),p ( M))]
n 2 Xn |M ·| Xˆn |M ·|
(cid:114) L
1 L L (cid:88)
E[W2(p ( M),p ( M))]+2 E[W2(p ( M),p ( M))]+ +9 δ
≤ n 2 Yn |M ·| Yˆn |M ·| nN 2 Yn |M ·| Yˆn |M ·| N ℓ,N
ℓ=1
(cid:114) L
L L (cid:88)
P +2 P + +9 δ .
≤ ′ N ′ N ℓ,N
ℓ=1
Since E[ X 2] < , it follows by the dominated convergence theorem that (cid:80)L δ tends to zero as N .
∥ ∥ (cid:113)∞ (cid:113) ℓ=1 ℓ,N → ∞
Moreover, D +2 LD + L and P +2 LP + L converge respectively to D ρ and P ρ as N . So when
′ N ′ N ′ N ′ N − − →∞
N is sufficiently large, we have
1
E[ Xn Xˆn 2] D, (85)
n ∥ − ∥ ≤
1
E[W2(p ( M),p ( M))] P. (86)
n 2 Xn |M ·| Xˆn |M ·| ≤
Combining (85) and (86) as well as the fact that
1
E[ℓ(M)] R(D ,P p )+ϵ R(D,P)+3ϵ
′ ′ Y
n ≤ | ≤
yields R (D,0) R(D,0)+3ϵ. Since ϵ>0 is arbitrary, this proves (62) for P >0.
C
≤
Now we proceed to prove (14). For any (U,Xˆ) satisfying (3), let U := E[X U] and construct Xˆ such that p =
′ | ′ U′Vˆ X
p p and p =p . Clearly, X U Xˆ form a Markov chain, and |
U′ |X Vˆ |U′ Vˆ |U′ Xˆ −E[Xˆ |U] |U′ ↔ ′ ↔ ′
E[Xˆ U ]=U +E[Vˆ U ]
′ ′ ′ ′
| |
=U +E[Xˆ E[Xˆ U]U ]
′ ′
− | |
=U +E[Xˆ E[Xˆ U ]U ] (87)
′ ′ ′
− | |
=U almost surely,
′
where (87) is because X U U form a Markov chain. It follows by the data processing inequality [7, Theorem 2.8.1]
′
↔ ↔
that
I(X;U) I(X;U ).
′
≥
Moreover, we have
D E[ X Xˆ 2]
≥ ∥ − ∥
=E[ X E[X U] 2]+E[ E[X U] E[Xˆ U] 2]+E[ Xˆ E[Xˆ U] 2] (88)
∥ − | ∥ ∥ | − | ∥ ∥ − | ∥
E[ X E[X U] 2]+E[ Xˆ E[Xˆ U] 2]
≥ ∥ − | ∥ ∥ − | ∥
=E[ V 2]+E[ Vˆ ]
∥ ∥ ∥ ∥
and
P E[W2(p ( U),p ( U))]
≥ 2 X |U ·| Xˆ |U ·|
=E[W 22(p X −E[X |U] |U( ·|U),p Xˆ −E[Xˆ |U] |U( ·|U))]+E[ ∥E[X |U] −E[Xˆ |U] ∥2]
≥E[W 22(p X −E[X |U] |U( ·|U),p Xˆ −E[Xˆ |U] |U( ·|U))]
=E[E[W 22(p X −E[X |U] |U( ·|U),p Xˆ −E[Xˆ |U] |U( ·|U)) |U ′]]
≥E[W 22(p X −E[X |U] |U′( ·|U ′),p Xˆ −E[Xˆ |U] |U′( ·|U ′))] (89)
=E[W2(p ( U ),p ( U ))],
2 V |U′ ·| ′ Vˆ |U′ ·| ′
where(88)followsbythefactthatX E[X U],E[X U] E[Xˆ U],andXˆ E[Xˆ U]areuncorrelated(whichisaconsequence
of the Markov structure X U X−ˆ) whi| le (89) is| due− to the| Markov st− ructure| s X U U and Xˆ U U as well
′ ′
↔ ↔ ↔ ↔ ↔ ↔
as part 2) of Proposition 1. This proves R(D,P) R(D,P).
′
On the other hand, for any (U ,Xˆ ) satisfying (≥ 15) and (16),
′ ′
E[ X Xˆ 2]=E[ V 2]+E[ Vˆ2 ]
′
∥ − ∥ ∥ ∥ ∥ ∥22
and
E[W2(p ( U ),p ( U ))]=E[W2(p ( U ),p ( U ))].
2 X |U′ ·| ′ Xˆ′ |U′ ·| ′ 2 V |U′ ·| ′ Vˆ |U′ ·| ′
Therefore, we must have R(D,P) R(D,P). This completes the proof of Theorem 3.
′
≤
APPENDIXG
PROOFOF(13)FORRESTRICTED
X
The proof of (13) in Appendix F assumes that the reconstruction has the freedom to take on values from RL (so does Xˆ
in the definition of R(D,P)) even if the support of p might not fully cover RL.
X
Here we shall show that (13) continues to hold when the source and reconstruction are confined to a strict subset of RL
(correspondingly, R(D,P) is defined with Xˆ restricted to ). It suffices to show X
X
R (D,P) R(D,P) (90)
C
≤
since the converse part in the proof of Theorem 1 is also applicable here.
We first establish the following technical lemma. For any N > 0, let B(N′) := x RL : x 2 M , B(N′) := x
′
RL : x 2 >M and define a deterministic mapping ξ(N′) : (N′)as follows.{ ∈ ∥ ∥ ≤ } { ∈
∥ ∥ } X →X
1) B(N ): Let (N′) := and ξ(N′)(x):=x for all x .
′
2) X ⊆B(N ): Let X (N′) :=X ( B(N′)) r(N′) and ∈X
′
X ̸⊆ X X ∩ ∪{ }
(cid:40)
x, x
B(N′),
ξ(N′)(x):=
∈X ∩
r(N′), x B(N′),
∈X ∩
where r(N′) is a fixed point in B(N′) satisfying
X ∩
r(N′) 2 2 inf x 2.
∥ ∥ ≤
x
B(N′)∥ ∥
∈X∩
Lemma 4: For any jointly distributed X , Xˆ , and U with E[ X 2]< and E[ X Xˆ 2]< ,
′ ′ ′ ′ ′ ′
∥ ∥ ∞ ∥ − ∥ ∞
lim E[E[ ξ(N′)(X ) ξ(N′)(Xˆ ) 2 U ]]=E[E[ X Xˆ 2 U ]].
′ ′ ′ ′ ′ ′
M ∥ − ∥ | ∥ − ∥ |
→∞
Proof: Note that
E[E[ ξ(N′)(X ) ξ(N′)(Xˆ ) 2 U ]]=E[ ξ(N′)(X ) ξ(N′)(Xˆ ) 2]
′ ′ ′ ′ ′
∥ − ∥ | ∥ − ∥
and
E[E[ X Xˆ 2 U ]]=E[ X Xˆ 2].
′ ′ ′ ′ ′
∥ − ∥ | ∥ − ∥
Since
ξ(N′)(X ) ξ(N′)(Xˆ ) 2 2 ξ(N′)(X ) 2+2 ξ(N′)(Xˆ ) 2
′ ′ ′ ′
∥ − ∥ ≤ ∥ ∥ ∥ ∥
4 X 2+4 Xˆ 2
′ ′
≤ ∥ ∥ ∥ ∥
4 X 2+4 X (X Xˆ ) 2
′ ′ ′ ′
≤ ∥ ∥ ∥ − − ∥
4 X 2+4(2 X 2+2 X Xˆ 2)
′ ′ ′ ′
≤ ∥ ∥ ∥ ∥ ∥ − ∥
=12 X 2+8 X Xˆ 2
′ ′ ′
∥ ∥ ∥ − ∥
and
E[12 X 2+8 X Xˆ 2]=12E[ X 2]+8E[ X Xˆ 2]
′ ′ ′ ′ ′ ′
∥ ∥ ∥ − ∥ ∥ ∥ ∥ − ∥
< ,
∞
invoking the dominated convergence theorem yields the desired result.
Now we proceed to prove (90). It suffices to consider the case P > 0 since the case P = 0 is covered by the proof in
Appendix F. In light of Proposition 2, R(D,P) is continuous in (D,P) for D >0 and P >0. So for any ϵ>0, there exists
ρ>0satisfyingR(D ρ,P ρ) R(D,P)+ϵ.BythedefinitionofR(D ρ,P ρ),wecanfindp suchthatI(X;U)
− − ≤ − − UXˆ X ≤
R(D ρ,P ρ)+ϵ,X U Xˆ formaMarkovchain,E[ X Xˆ 2] D ρ,andE[W2(p ( U| ),p ( U))] P ρ.
Let X− (N′) :=− ξ(N′)(X)↔ and X↔ ˆ(N′) :=ξ(N′)(Xˆ). It follows ∥ by L− emm∥ a 4≤ (wi− th U being a2 conX s| tU ant· )| thatXˆ |U ·| ≤ −
′
lim E[ X(N′) Xˆ(N′) 2]=E[ X Xˆ 2].
M ∥ − ∥ ∥ − ∥
→∞23
Moreover, since every coupling of p XU and p Xˆ U induces a coupling of p X(N′)U and p Xˆ(N′)U, it follows again by Lemma
4 that | | | |
li MmsupE[W 22(p X(N′) |U( ·|U),p Xˆ(N′) |U( ·|U))] ≤E[W 22(p X |U( ·|U),p Xˆ |U( ·|U))].
→∞
Therefore, we can choose a sufficiently large M to ensure
ρ
E[ X(N′) Xˆ(N′) 2] D ,
∥ − ∥ ≤ − 2
ρ
E[W 22(p X(N′) |U( ·|U),p Xˆ(N′) |U( ·|U))] ≤P
−
2.
Let N be a postive integer such that (N′) 1 [ N : N]L. For each cell (cid:81)L [ ij ,ij+1], pick some x ( (N′)
X ⊆ √N − j=1 √N √N ∈ X ∩
(cid:81)L [ ij ,ij+1]) r(M) as its representative point2, (i , ,i ) [ N :N 1]L. Let (N′,N) denote the set that consists
j=1 √N √N \{ } 1 ··· L ∈ − − Y
of such representative points and r(M). Clearly, (N′,N) (N′) . Construct ξ(N′,N) : (N′) (N′,N) that maps
r(N′) to itself and maps any other x (N′) to thY e represe⊆ ntiX tive poi⊆ ntX of the cell that contains3X x. Let→ Y Y :=ξ(N′,N)(X(N′))
and Yˆ :=ξ(N′,N)(Xˆ(N′)). Note that∈ YX X(N′) X U Xˆ Xˆ(N′) Yˆ form a Markov chain. As a consequence,
↔ ↔ ↔ ↔ ↔ ↔
I(Y;U)
I(X(N′);U)
≤
I(X;U)
≤
R(D ρ,P ρ)+ϵ
≤ − −
R(D,P)+2ϵ. (91)
≤
Moreover, we have
(cid:34) (cid:35)
L
(cid:88)
E[ Y Yˆ 2]=E Y Yˆ 2
ℓ ℓ
∥ − ∥ | − |
ℓ=1
(cid:34) L (cid:18) (cid:19)(cid:35)
E (cid:88) X(N′) Xˆ(N′) + 2
≤ | ℓ − ℓ | √N
ℓ=1
(cid:34) L (cid:18) (cid:19)(cid:35)
=E (cid:88) X(N′) Xˆ(N′) 2+ 4 X(N′) Xˆ(N′) + 4
| ℓ − ℓ | √N| ℓ − ℓ | N
ℓ=1
(cid:114)
L 4L
E[ X(N′) Xˆ(N′) 2]+4 E[ X(N′) Xˆ(N′) 2]+ (92)
≤ ∥ − ∥ N ∥ − ∥ N
(cid:114)
ρ L(cid:16) ρ(cid:17) 4L
D +4 D +
≤ − 2 N − 2 N
=:D˜. (93)
Since (92) is valid for any jointly distributed (X(N′),Xˆ(N′)) and the induced (Y,Yˆ) (via mapping ξ(N′,N)), it follows that
(cid:114)
L 4L
W 22(p Y |U( ·|u),p Yˆ |U( ·|u)) ≤W 22(p X(N′) |U( ·|u),p Xˆ(N′) |U( ·|u))+4 NW 22(p X(N′) |U( ·|u),p Xˆ(N′) |U( ·|u))+ N , u ∈U.
Therefore, we have
(cid:114)
L 4L
E[W 22(p Y |U( ·|U),p Yˆ |U( ·|U))] ≤E[W 22(p X(N′) |U( ·|U),p Xˆ(N′) |U( ·|U))]+4 NE[W 22(p X(N′) |U( ·|U),p Xˆ(N′) |U( ·|U))]+ N
(cid:114)
ρ L(cid:16) ρ(cid:17) 4L
P +4 P +
≤ − 2 N − 2 N
=:P˜. (94)
Combining (91), (93), and (94) yields
R(D˜,P˜ p ) R(D,P)+2ϵ,
Y
| ≤
where R(D˜,P˜ p ) denotes the counterpart of R(D˜,P˜) for source Y.
Y
|
2If(X(N′)∩(cid:81)L [√ij ,i √j+1 ])\{r(M)}isempty,thenthiscellhasnorepresentativepoint.
j=1 N N
3Ifxiscontainedinmultiplecells,assignittooneofthem.Theassignmentisdoneinasystematicmannertoavoidmeasurabilityissues.24
According to Theorem 1, there exists p = pnp p such that 1E[ℓ(M)] R(D ,P p )+ϵ, 1E[ Yn
Yˆn 2] D , and E[W2(p ( U),p
(Y UnM ))Yˆ ]n
P
.Y CoM ns|Y trn uctYˆ pn |M =pn
p
≤ wit′
h
p′ | Y =pnn ∥
and
l−
et
Xˆn∥ :=≤ Yˆn.′
Note that
2 Y |U ·| Yˆ |U ·| ≤ ′ XnYnMYˆn Xn |Yn YMYˆn Xn |Yn X |Y
1 1
E[ Xn Xˆn 2]= E[ Xn Yˆn ]
n ∥ − ∥ n ∥ − ∥
n
= 1 (cid:88) P Y(t)=r(N′) E[ X(t) Yˆ(t) 2 Y(t)=r(N′)]
n { ̸ } ∥ − ∥ | ̸
t=1
n
+ 1 (cid:88) P Y(t)=r(N′) E[ X(t) Yˆ(t) 2 Y(t)=r(N′)]. (95)
n { } ∥ − ∥ |
t=1
For y (N′,N) r(N′) ,
∈Y \{ }
E[ X(t) Yˆ(t) 2 Y(t)=y]=E[ (X(t) Y(t))+(Y(t) Yˆ(t)) 2 Y(t)=y]
∥ − ∥ | ∥ − − ∥ |
(cid:113)
E[ Y(t) Yˆ(t) 2 Y(t)=y]+2 E[ X(t) Y(t) 2 Y(t) Yˆ(t) 2 Y(t)=y]
≤ ∥ − ∥ | ∥ − ∥ ∥ − ∥ |
+E[ X(t) Y(t) 2 Y(t)=y]
∥ − ∥ |
(cid:114)
L L
E[ Y(t) Yˆ(t) 2 Y(t)=y]+2 E[ Y(t) Yˆ(t) 2 Y(t)=y]+ .
≤ ∥ − ∥ | N ∥ − ∥ | N
Therefore,
P Y(t)=r(N′) E[ X(t) Yˆ(t) 2 Y(t)=r(N′)]
{ ̸ } ∥ − ∥ | ̸
(cid:88)
= p (y)E[ X(t) Yˆ(t) 2 Y(t)=y]
Y(t)
∥ − ∥ |
y (N′,N) r(N′)
∈Y \{ }
(cid:88)
p (y)E[ X(t) Yˆ(t) 2 Y(t)=y]
Y(t)
≤ ∥ − ∥ |
y (N′,N)
∈Y (cid:114)
L L
E[ Y(t) Yˆ(t) 2]+2 E[ Y(t) Yˆ(t) 2]+ . (96)
≤ ∥ − ∥ N ∥ − ∥ N
Moreover, we have
P Y(t)=r(N′) E[ X(t) Yˆ(t) 2 Y(t)=r(N′)] P Y(t)=r(N′) E[2 X(t) 2+2 Yˆ(t) 2 Y(t)=r(N′)]
{ } ∥ − ∥ | ≤ { } ∥ ∥ ∥ ∥ |
6P Y(t)=r(N′) E[ X(t) 2 Y(t)=r(N′)]
≤ { } ∥ ∥ |
=6P Y =r(N′) E[ X 2 Y =r(N′)]
{ } ∥ ∥ |
=6δ˜ N′, (97)
where
δ˜ N′ :=P X 2 >N ′ E[ X 2 X 2 >N ′].
{∥ ∥ } ∥ ∥ |∥ ∥
Substituting (96) and (97) into (95) gives
n (cid:32) (cid:114) (cid:33)
1 1 (cid:88) L L
E[ Xn Xˆn 2] E[ Y(t) Yˆ(t) 2]+2 E[ Y(t) Yˆ(t) 2]+ +6δ˜ N′
n ∥ − ∥ ≤ n ∥ − ∥ N ∥ − ∥ N
t=1
(cid:114)
1 L L
E[ Yn Yˆn 2]+2 E[ Yn Yˆn 2]+ +6δ˜
N′
(98)
≤ n ∥ − ∥ nN ∥ − ∥ N
(cid:114)
L L
D˜ +2 D˜ + +6δ˜ N′.
≤ N N
Since (98) is valid for any jointly distributed (Yn,Yˆn) and the induced (Xn,Xˆn) (via Markov coupling Xn Yn Yˆn
with p =pn and setting Xˆn =Yˆn), it follows that ↔ ↔
XnYn XY
| |
1
W2(p ( m),p ( m))
n 2 Xn |M ·| Xˆn |M ·|
(cid:114)
1 L L
≤
nW 22(p
Yn
|M( ·|m),p
Yˆn
|M( ·|m))+2 nNW 22(p
Yn
|M( ·|m),p
Yˆn
|M( ·|m))+
N
+6δ˜ N′, m ∈M.25
Therefore, we have
1
E[W2(p ( M),p ( M))]
n 2 Xn |M ·| Xˆn |M ·|
(cid:114)
1 L L
≤
nE[W 22(p Yn |M( ·|M),p Yˆn |M( ·|M))]+2 nNE[W 22(p Yn |M( ·|M),p Yˆn |M( ·|M))]+
N
+6δ˜ N′
(cid:114)
L L
P˜+2 P˜+ +6δ˜ N′.
≤ N N
Since E[ X 2] < , it follows by the dominated convergence theorem that δ˜ N′ tends to zero as N ′ . Moreover
(cid:113)∥ ∥ ∞ (cid:113) → ∞
D˜ +2 LD˜ + L ad P˜ +2 LP˜ + L converge respectively to D ρ and P ρ as N . So when N and N are
N N N N − 2 − 2 → ∞ ′
sufficiently large, we have
1
E[ Xn Xˆn 2] D, (99)
n ∥ − ∥ ≤
1
E[W2(p ( M),p ( M))] P. (100)
n 2 Xn |M ·| Xˆn |M ·| ≤
Combining (99) and (100) as well as the fact that
1
E[ℓ(M)] R(D˜,P˜ p )+ϵ R(D,P)+3ϵ
Y
n ≤ | ≤
yields R (D,P) R(D,P)+3ϵ. Since ϵ>0 is arbitrary, this proves (90) for P >0.
C
≤
APPENDIXH
PROOFOFCOROLLARY1
For any p U¯ |X satisfying (20), let U :=E[X |U¯]. It follows by the data processing inequality [7, Theorem 2.8.1] that
I(X;U) I(X;U¯).
≤
Moreover, let Xˆ be jointly distributed with (X,U) such that X U Xˆ form a Markov chain and p =p . We have
↔ ↔ Xˆ |U X |U
E[ X Xˆ 2]=E[ X U 2]+E[ Xˆ U 2]
∥ − ∥ ∥ − ∥ ∥ − ∥
=2E[ X U 2]
∥ − ∥
2E[ X U¯ 2]
≤ ∥ − ∥
D.
≤
This shows R(D,0) R(D), which, in conjunction with (13), implies
≤ 2
(cid:18) (cid:19)
D
R (D,0) R . (101)
C
≤ 2
It is known [6, Section III.B], [5, Corollary 1], [13, Equation (16)] that (8) holds for the case E[ X 2]< and ∆(x,xˆ)=
∥ ∥ ∞
x xˆ 2. For any p satisfying (9), (10), and (11), we have
∥ − ∥ UXˆ X
|
I(X;U) I(X;E[X U]), (102)
≥ |
I(Xˆ;U) I(Xˆ;E[Xˆ U]), (103)
≥ |
E[ X Xˆ 2] E[ X E[X U] 2]+E[ Xˆ E[Xˆ U] 2].
∥ − ∥ ≥ ∥ − | ∥ ∥ − | ∥
If E[ X E[X U] 2] E[ Xˆ E[Xˆ U] 2], then let U¯ :=E[X U]. Note that
∥ − | ∥ ≤ ∥ − | ∥ |
1 D
E[ X U¯ ] E[ X Xˆ 2] ,
∥ − ∥ ≤ 2 ∥ − ∥ ≤ 2
which, together with (102), implies
(cid:18) (cid:19)
D
R R (M,0). (104)
C
2 ≤
If E[ X E[X U] 2]>E[ Xˆ E[Xˆ U] 2], then let U¯ :=E[Xˆ U]. Note that
∥ − | ∥ ∥ − | ∥ |
1 D
E[ Xˆ U¯ ] E[ X Xˆ 2] ,
∥ − ∥ ≤ 2 ∥ − ∥ ≤ 2
which, together with (103) and (11), implies (104) as well.
Combining (101), (104), and (7) proves Corollary 1.26
APPENDIXI
PROOFOFTHEOREM4
We first establish the following technical lemma.
Lemma 5: For σ2 >0, ℓ [1:L], D >0, and P 0,
ℓ ∈ ≥
L
(cid:88)1
χ(D,P)= log(2πeω ),
ℓ
− 2
ℓ=1
where
L
(cid:88)1
χ(D,P):= min log(2πeγ ) (105)
ℓ
{γℓ,γˆℓ}L ℓ=1−
ℓ=1
2
s.t. 0 γ σ2, ℓ [1:L], (106)
≤ ℓ ≤ ℓ ∈
0 γˆ , ℓ [1:L], (107)
ℓ
≤ ∈
L
(cid:88)
(γ +γˆ ) D, (108)
ℓ ℓ
≤
ℓ=1
L
(cid:88) (cid:112)
(√γ γˆ )2 P. (109)
ℓ ℓ
− ≤
ℓ=1
Moreover, the minimum value of the optimization problem in (105) is attained at
γ =γ :=ω , ℓ [1:L], (110)
ℓ ℓ∗ ℓ
∈
γˆ =γˆ =αω , ℓ [1:L], (111)
ℓ ℓ∗ ℓ
∈
with

(cid:18) (cid:19)2
 D −√(2D −P)P
, D >P,
α:= D P (112)
−
0,
D P.
≤
Proof: It is easy to verify that the optimization problem in (105) is a convex program. The Lagrangian of this convex
program is given by
(cid:32) (cid:33) (cid:32) (cid:33)
L L L L L
(cid:88)1 (cid:88) (cid:88) (cid:112) (cid:88) (cid:88)
ζ := log(2πeγ )+ν (γ +γˆ ) D +ν (√γ γˆ )2 P + τ (γ σ2) τˆγˆ .
− 2 ℓ 1 ℓ ℓ − 2 ℓ − ℓ − ℓ ℓ − ℓ − ℓ ℓ
ℓ=1 ℓ=1 ℓ=1 ℓ=1 ℓ=1
Note that
(cid:18) (cid:19)
∂ζ 1 √γˆ
ℓ
= +ν +ν 1 +τ , ℓ [1:L],
1 2 ℓ
∂γ
ℓ
−2γ
ℓ
− √γ
ℓ
∈
and
(cid:18) (cid:19)
∂ζ √γ
ℓ
=ν +ν 1 τˆ, ℓ [1:L].
1 2 ℓ
∂γˆ − √γˆ − ∈
ℓ ℓ
The minimum value of the optimization problem in (105) is attained at γ ,γˆ L = γ ,γˆ L if and only if there exist
{ ℓ ℓ }ℓ=1 { ℓ∗ ℓ∗ }ℓ=1
nonnegative Lagrange multipliers ν , ν , τ , τˆ, ℓ [1:L], such that
1 2 ℓ ℓ
∈
0 γ σ2, ℓ [1:L], (113)
≤ ℓ∗ ≤ ℓ ∈
0 γˆ , ℓ [1:L], (114)
≤
ℓ∗
∈
L
(cid:88)
(γ +γˆ ) D, (115)
ℓ∗ ℓ∗
≤
ℓ=1
L
(cid:88) ((cid:112) γ (cid:112) γˆ )2 P, (116)
ℓ∗
−
ℓ∗
≤
ℓ=1
(cid:32) (cid:112) (cid:33)
1
+ν +ν 1
γˆ ℓ∗
+τ =0, ℓ [1:L], (117)
1 2 (cid:112) ℓ
− 2γ ℓ∗ − γ ℓ∗ ∈
(cid:32) (cid:112) (cid:33)
γ
ν +ν 1
ℓ∗
τˆ =0, ℓ [1:L], (118)
1 2 (cid:112) ℓ
− γˆ − ∈
ℓ∗27
(cid:32) (cid:33)
L
(cid:88)
ν (γ +γˆ ) D =0, (119)
1 ℓ∗ ℓ∗
−
ℓ=1
(cid:32) (cid:33)
L
ν (cid:88) ((cid:112) γ (cid:112) γˆ )2 P =0, (120)
2 ℓ∗
−
ℓ∗
−
ℓ=1
τ (γ σ2)=0, ℓ [1:L], (121)
ℓ ℓ∗ − ℓ ∈
τˆγˆ =0, ℓ [1:L]. (122)
ℓ ℓ
∈
Therefore, it suffices to verify that the above Karush–Kuhn–Tucker conditions are satisfied by γ ,γˆ L defined in (110)
{ ℓ∗ ℓ∗ }ℓ=1
and (111). We shall consider the following cases separately.
1) P <D and
D+(cid:112)
(2D (D P))(D
P)<2(cid:80)Lσ2
(i.e., P <D and
D+(cid:112)
(2D P)P
<2(cid:80)Lσ2):
− ∧ ∧ ℓ ℓ − ℓ ℓ
We have
γ =ω σ2, ℓ [1:L],
ℓ∗ ∧ ℓ ∈
(cid:32) (cid:112) (cid:33)2
D (2D P)P
γˆ ℓ∗ = −
D
P− (ω ∧σ ℓ2), ℓ ∈[1:L],
−
where ω is the unique solution to
L (cid:112)
(cid:88) D+ (2D P)P
(ω σ2)= − .
∧ ℓ 2
ℓ=1
Let
(cid:112)
P + (2D P)P
ν := − ,
1 (cid:112)
4ω (2D P)P
−
D P
ν 2 := (cid:112) − ,
4ω (2D P)P
−
(ω σ2) 0
τ := − ℓ ∨ , ℓ [1:L],
ℓ 2ωσ2 ∈
ℓ
τˆ :=0, ℓ [1:L].
ℓ
∈
It is clear that the Lagrange multipliers defined above are nonnegative. Moreover, the Karush–Kuhn–Tucker conditions
(113)–(122) are satisfied. In particular,
L  (cid:32) (cid:112) (cid:33)2 L
(cid:88) D (2D P)P (cid:88)
(γ ℓ∗+γˆ ℓ∗)=1+ −
D
P−  (ω ∧σ ℓ2)
ℓ=1 − ℓ=1
 (cid:32) (cid:112) (cid:33)2 (cid:112)
D (2D P)P D+ (2D P)P
=1+ − −  −
D P 2
−
=D
and
(cid:88)L ((cid:112) γ (cid:113) γˆ2)2 =(cid:32) 1 D −(cid:112) (2D −P)P(cid:33)2 (cid:88)L (ω σ2)
ℓ∗ − ℓ − D P ∧ ℓ
ℓ=1 − ℓ=1
(cid:32) (cid:112) (cid:33)2 (cid:112)
D (2D P)P D+ (2D P)P
= 1 − − −
− D P 2
−
=P.
2) P D and
D+(cid:112)
(2D (D P))(D
P)<2(cid:80)Lσ2
(i.e., P D and D
<(cid:80)Lσ2):
≥ − ∧ ∧ ℓ ℓ ≥ ℓ ℓ
We have
γ =ω σ2, ℓ [1:L],
ℓ∗ ∧ ℓ ∈
γˆ =0, ℓ [1:L],
ℓ∗
∈28
where ω is the unique solution to
L
(cid:88)
(ω σ2)=D.
∧ ℓ
ℓ=1
Let
1
ν := ,
1
2ω
ν :=0,
2
(ω σ2) 0
τ := − ℓ ∨ , ℓ [1:L],
ℓ 2ωσ2 ∈
ℓ
1
τˆ := , ℓ [1:L].
ℓ
2ω ∈
It is clear that the Lagrange multipliers defined above are nonnegative. Moreover, the Karush–Kuhn–Tucker conditions
(113)–(122) are satisfied. In particular,
L L
(cid:88) (cid:88)
(γ +γˆ )= (ω σ2)=D
ℓ∗ ℓ∗ ∧ ℓ
ℓ=1 ℓ=1
and
L L
(cid:88) ((cid:112) γ (cid:112) γˆ )2 =(cid:88) (ω σ2)=D P.
ℓ∗ − ℓ∗ ∧ ℓ ≤
ℓ=1 ℓ=1
3) P <D and
D+(cid:112)
(2D (D P))(D P)
2(cid:80)Lσ2
(i.e., P <D and
D+(cid:112)
(2D P)P
2(cid:80)Lσ2):
− ∧ ∧ ≥ ℓ ℓ − ≥ ℓ ℓ
We have
γ =σ2, ℓ [1:L],
ℓ∗ ℓ ∈
(cid:32) (cid:112) (cid:33)2
D (2D P)P
γˆ ℓ∗ = −
D
P− σ ℓ2, ℓ ∈[1:L].
−
Let
ν :=0,
1
ν :=0,
2
1
τ := , ℓ [1:L],
ℓ 2σ2 ∈
ℓ
τˆ :=0, ℓ [1:L].
ℓ
∈
It is clear that the Lagrange multipliers defined above are nonnegative. Moreover, the Karush–Kuhn–Tucker conditions
(113)–(122) are satisfied. In particular,
L  (cid:32) (cid:112) (cid:33)2 L
(cid:88) D (2D P)P (cid:88)
(γ ℓ∗+γˆ ℓ∗)=1+ −
D
P−  σ ℓ2
ℓ=1 − ℓ=1
 (cid:32) (cid:112) (cid:33)2 (cid:112)
D (2D P)P D+ (2D P)P
1+ − −  −
≤ D P 2
−
=D
and
L (cid:32) (cid:112) (cid:33)2 L
(cid:88) ((cid:112) γ (cid:112) γˆ )2 = 1 D − (2D −P)P (cid:88) σ2
ℓ∗ − ℓ∗ − D P ℓ
ℓ=1 − ℓ=1
(cid:32) (cid:112) (cid:33)2 (cid:112)
D (2D P)P D+ (2D P)P
1 − − −
≤ − D P 2
−
=P.
4) P D and
D+(cid:112)
(2D (D P))(D P)
2(cid:80)Lσ2
(i.e., P D and D
(cid:80)Lσ2):
≥ − ∧ ∧ ≥ ℓ ℓ ≥ ≥ ℓ ℓ29
We have
γ =σ2, ℓ [1:L],
ℓ∗ ℓ ∈
γˆ =0, ℓ [1:L].
ℓ∗
∈
Let
ν :=0,
1
ν :=0,
2
1
τ := , ℓ [1:L],
ℓ 2σ2 ∈
ℓ
τˆ :=0, ℓ [1:L].
ℓ
∈
It is clear that the Lagrange multipliers defined above are nonnegative. Moreover, the Karush–Kuhn–Tucker conditions
(113)–(122) are satisfied. In particular,
L L
(cid:88) (cid:88)
(γ +γˆ )= σ2 D
ℓ∗ ℓ∗ ℓ ≤
ℓ=1 ℓ=1
and
L L
(cid:88) ((cid:112) γ (cid:112) γˆ )2 =(cid:88) σ2 D P.
ℓ∗ − ℓ∗ ℓ ≤ ≤
ℓ=1 ℓ=1
This completes the proof of Lemma 5.
Now we proceed to prove Theorem 4. For any (U ,Xˆ ) satisfying (15)–(18), let γ := E[V2] and γˆ := E[Vˆ2], where V
′ ′ ℓ ℓ ℓ ℓ ℓ
and Vˆ denote the ℓ-th component of V and Vˆ, respectively, ℓ [1:L]. Clearly,
ℓ
∈
0 γ σ2, ℓ [1:L], (123)
≤ ℓ ≤ ℓ ∈
0 γˆ , ℓ [1:L]. (124)
ℓ
≤ ∈
Note that
I(X;U )=h(X) h(X U )
′ ′
− |
=h(X) h(V U )
′
− |
h(X) h(V)
≥ −
L
(cid:88)
h(X) h(V )
ℓ
≥ −
ℓ=1
L
(cid:88)1
h(X) log(2πeγ ), (125)
ℓ
≥ − 2
ℓ=1
where (125) is due to [?, Theorem 9.6.5]. Moreover, we have
D E[ V 2]+E[ Vˆ 2]
≥ ∥ ∥ ∥ ∥
L
(cid:88)
= (γ +γˆ ) (126)
ℓ ℓ
ℓ=1
and
P E[W2(p ( U ),p ( U ))]
≥ 2 V |U′ ·| ′ Vˆ |U′ ·| ′
W2(p ,p ) (127)
≥ 2 V Vˆ
L
(cid:88)
W2(p ,p ) (128)
≥ 2 Vℓ Vˆ ℓ
ℓ
L
(cid:88) (cid:112)
(√γ γˆ )2, (129)
ℓ ℓ
≥ −
ℓ=1
where (127) is due to part 2) of Proposition 1, (128) is due to part 1) of Proposition 1, and (129) is due to [25, Equation (6)].
Combining (123), (124), (125), (126), and (129) yields R(D,P) h(X)+χ(D,P). In light of Theorem 3 and Lemma 5,
′
≥
the proof is complete.30
APPENDIXJ
PROOFOFCOROLLARY2
In light of Theorem 4,
L
R (D,P) h(X) log(2πeω)
C
≥ − 2
if ω min σ2 L , where
≤ { ℓ}ℓ=1
(cid:112)
D+ (2D (D P))(D P)
ω = − ∧ ∧ .
2L
Let Σ denote the covariance matrix of X. Note that σ2, ℓ [1:L], are the diagonal entries of Σ. We have
ℓ ∈
min σ2 L λ (Σ) (130)
{ ℓ}ℓ=1 ≥ min
(cid:32) (cid:33)
K
(cid:88)
λ β Σ (131)
min k k
≥
k=1
min λ (Σ ) K ,
≥ { min k }k=1
where (130) is due to [26, Theorem 4.3.26], and (131) is due to Σ
(cid:80)K
β Σ .
⪰ k=1 k k
Now it remains to show
L
R (D,P) h(X) log(2πeω) (132)
C
≤ − 2
if ω min λ (Σ ) K . Note that ω min λ (Σ ) K ensures Σ ωI 0, k [1 : K], where I denotes the
L L≤ identi{ tym mi an trixk . W}k e= c1 an write X =U≤ +V,{ wm hein re Uk }k= (cid:80)1 K β (µk − ,Σ ⪰ ωI) and∈ V (0,ωI) are independent.
M× oreover, let Xˆ = U +Vˆ with Vˆ (0′ ,αωI) indepen′ d∼ ent ok f= (1 U k ,VN ), wk herk e− α is defined ∼ inN (112). It is clear that (15)
′ ′ ′
∼ N
and (16) are satisfied. Moreover, (17) and (18) are also satisfied since
E[ V 2]+E[ Vˆ 2]=L(1+α)ω =D
∥ ∥ ∥ ∥
and
E[W2(p ( U ),p ( U ))]=W2(p ,p )
2 V |U′ ·| ′ Vˆ |U′ ·| ′ 2 V Vˆ
=L(√ω √αω)2 (133)
−
(cid:40)
P, D >P,
=
D, D P,
≤
P,
≤
where (133) is due to [27, Proposition 7]. Note that
I(X;U )=h(X) h(X U )
′ ′
− |
=h(X) h(V)
−
L
=h(X) log(2πeω).
− 2
Therefore, when ω min λ (Σ ) K , we must have R(D,P) h(X) Llog(2πeω), which in light of Theorem 3
≤ { min k }k=1 ′ ≤ − 2
further implies (132).
APPENDIXK
PROOFOFCOROLLARY3
Let Z := ΘX. We have Z (Θµ,Λ) and consequently h(Z) = (cid:80)L 1log(2πeλ ). Since unitary transformation is
∼ N ℓ=1 2 ℓ
invertible and preserves the Euclidean norm, invoking Theorem 4 with X replaced by Z proves
L
(cid:88)1 λ
ℓ
R (D,P) log .
C
≥ 2 ω
ℓ
ℓ=1
It remains to show
L
R (D,P)
(cid:88)1
log
λ
ℓ . (134)
C
≤ 2 ω
ℓ
ℓ=131
For any γ ,γˆ L satisfying (106)–(109) with σ2 replaced by λ , ℓ [1 : L], define two diagonal matrices Γ and Γˆ with
the ℓ-th { diaℓ gonℓ } alℓ= e1 ntry being γ and γˆ , respectivℓ ely, ℓ [1 : L]ℓ . No∈ te that (106) and (107) ensures Γ 0, Γˆ 0, and
ℓ ℓ
∈ ⪰ ⪰
Λ Γ 0. We can write X =U +V, where U (0,ΘT(Λ Γ)Θ) and V (0,ΘTΓΘ) are independent. Moreover,
′ ′
let− Xˆ ⪰ =U +Vˆ with Vˆ (0,ΘTΓˆΘ) indepen∼ denN t of (U ,V)− . It is clear tha∼ t (1N 5) and (16) are satisfied. Moreover, (17)
′ ′ ′ ′
∼N
and (18) are also satisfied since
L
(cid:88)
E[ V ]+E[ Vˆ 2]= (γ +γˆ ) D
ℓ ℓ
∥ ∥ ∥ ∥ ≤
ℓ=1
and
E[W2(p ( U ),p ( U))]=W2(p ,p )
2 V |U′ ·| ′ Vˆ |U′ ·| 2 V Vˆ
L
(cid:88) (cid:112)
= (√γ γˆ )2 (135)
ℓ ℓ
−
ℓ=1
P,
≤
where (135) is due to [27, Proposition 7]. Note that
I(X;U )=h(X) h(X U )
′ ′
− |
=h(X) h(V)
−
L
(cid:88)1
=h(X) log(2πeγ ).
ℓ
− 2
ℓ=1
Therefore,wemusthaveR(P,D) h(X)+χ(D,P)withσ2 replacedbyλ ,ℓ [1:L],inthedefinitionofχ(D,P),which
in light of Lemma 5 and
T′ heorem≤
3 as well as the fact
h(X)ℓ =h(Z)=(cid:80)Lℓ 1∈
log(2πeλ ) further implies (134).
ℓ=1 2 ℓ
REFERENCES
[1] F.Mentzer,E.Agustsson,J.Ballé,D.Minnen,N.Johnston,andG.Toderici,“Neuralvideocompressionusinggansfordetailsynthesisandpropagation,”
inEuropeanConferenceonComputerVision,2022.
[2] F.Mentzer,G.Toderici,M.Tschannen,andE.Agustsson,“High-fidelitygenerativeimagecompression,”inAdvancesinNeuralInformationProcessing
Systems,2020.
[3] Y. Blau and T. Michaeli, “Rethinking lossy compression: The rate-distortion-perception tradeoff,” in International Conference on Machine Learning.
PMLR,2019,pp.675–685.
[4] T. A. Atif, M. A. Sohail, and S. S. Pradhan, “Lossy quantum source coding with a global error criterion based on a posterior reference map,” 2023.
[Online].Available:https://arxiv.org/abs/2302.00625
[5] A.B.Wagner,“Therate-distortion-perceptiontradeoff:Theroleofcommonrandomness,”arXivpreprintarXiv:2202.04147,2022.
[6] N.Saldi,T.Linder,andS.Yüksel,“Outputconstrainedlossysourcecodingwithlimitedcommonrandomness,”IEEETrans.onInfo.Theory,vol.61,
no.9,pp.4984–4998,2015.
[7] T.M.CoverandJ.A.Thomas,ElementsofInformationTheory,2ndEd. Wiley,2006.
[8] Y. Blau and T. Michaeli, “The perception-distortion tradeoff,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
2018,pp.6228–6237.
[9] L.TheisandA.Wagner,“Acodingtheoremfortherate-distortion-perceptionfunction,”inNeuralCompressionWorkshopofInternationalConference
onLearningRepresentations(ICLR),2021.
[10] G.Zhang,J.Qian,J.Chen,andA.Khisti,“Universalrate-distortion-perceptionrepresentationsforlossycompression,”inAdvancesinNeuralInformation
ProcessingSystems,2021,pp.11517–11529.
[11] R. Matsumoto, “Introducing the perception-distortion tradeoff into the rate-distortion theory of general information sources,” IEICE Comm. Express,
vol.7,no.11,pp.427–431,2018.
[12] ——, “Rate-distortion-perception tradeoff of variable-length source coding for general information sources,” IEICE Comm. Express, vol. 8, no. 2, pp.
38–42,2019.
[13] J. Chen, L. Yu, J. Wang, W. Shi, Y. Ge, and W. Tong, “On the rate-distortion-perception function,” IEEE Journal on Selected Areas in Information
Theory,vol.3,no.4,pp.664–673,2022.
[14] X.Niu,D.Gündüz,B.Bai,andW.Han,“Conditionalrate-distortion-perceptiontrade-off,”arXivpreprintarXiv:2305.09318,2023.
[15] Y.HamdiandD.Gündüz,“Therate-distortion-perceptiontrade-offwithsideinformation,”arXivpreprintarXiv:2305.13116,2023.
[16] E.Agustsson,M.Tschannen,F.Mentzer,R.Timofte,andL.VanGool,“Generativeadversarialnetworksforextremelearnedimagecompression,”in
ProceedingsoftheIEEEInternationalConferenceonComputerVision,2019,pp.221–231.
[17] J. Ballé, V. Laparra, and E. P. Simoncelli, “End-to-end optimized image compression,” in 5th International Conference on Learning Representations,
2017.
[18] L. Theis, W. Shi, A. Cunningham, and F. Huszár, “Lossy image compression with compressive autoencoders,” in 5th International Conference on
LearningRepresentations,2017.
[19] F. Mentzer, E. Agustsson, M. Tschannen, R. Timofte, and L. V. Gool, “Conditional probability models for deep image compression,” in The IEEE
ConferenceonComputerVisionandPatternRecognition(CVPR),2018.
[20] A.Golinski,R.Pourreza,Y.Yang,G.Sautiere,andT.S.Cohen,“Feedbackrecurrentautoencoderforvideocompression,”inProceedingsoftheAsian
ConferenceonComputerVision,2020.
[21] H.Liu,G.Zhang,J.Chen,andA.Khisti,“Lossycompressionwithdistributionshiftasentropyconstrainedoptimaltransport,”inInternationalConference
onLearningRepresentations(ICLR),2022.
[22] Z.Yan,F.Wen,R.Ying,C.Ma,andP.Liu,“Onperceptuallossycompression:Thecostofperceptualreconstructionandanoptimaltrainingframework,”
inInternationalConferenceonMachineLearning,2021.32
[23] L.TheisandE.Agustsson,“Ontheadvantagesofstochasticencoders,”inICLRneuralcompressionworkshop,2021.
[24] A.ElGamalandY.H.Kim,NetworkInformationTheory. CambridgeUniversityPress,2011.
[25] D.C.DowsonandB.V.Landau,“Thefréchetdistancebetweenmultivariatenormaldistributions,”J.MultivariateAnal.,vol.12,no.3,pp.450–455,
1982.
[26] R.A.HornandC.R.Johnson,MatrixAnalysis. CambridgeUniversityPress,1985.
[27] C.R.GivensandR.M.Shortt,“Aclassofwassersteinmetricsforprobability,”MichiganMath.J.,vol.31,no.2,pp.231–240,1984.