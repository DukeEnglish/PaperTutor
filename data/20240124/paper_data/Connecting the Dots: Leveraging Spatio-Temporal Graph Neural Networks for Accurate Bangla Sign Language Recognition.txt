AIforBangla2.0
Connecting the Dots: Leveraging Spatio-Temporal Graph Neural Networks for
Accurate Bangla Sign Language Recognition
HazSameenShahgir1,KhondkerSalmanSayeed1,
MdTokiTahmid1,TanjeemAzwadZaman1
Md.ZarifUlAlam1*
1DepartmentofCSE,BUET
Abstract to develop an interpreter that can translate sign languages
into text or speech, enabling deaf individuals to communi-
RecentadvancesinDeepLearningandComputerVisionhave
cate effectively with society. This necessity has made the
beensuccessfullyleveragedtoservemarginalizedcommuni-
recognition of Bangladeshi sign language (BdSL) a signif-
ties in various contexts. One such area is Sign Language - icant and challenging topic in the field of computer vision
aprimarymeansofcommunicationforthedeafcommunity.
andmachinelearning.
However,sofar,thebulkofresearcheffortsandinvestments
havegoneintoAmericanSignLanguage,andresearchactiv- While current works on sign language recognition are
ityintolow-resourcesignlanguages-especiallyBanglaSign classifiedintotwocategories-isolatedandcontinuoussign
Language - has lagged significantly. In this research paper, languagerecognition,thelackofavailabledatasetshashin-
wepresentanewword-levelBanglaSignLanguagedataset- dered research into Bangla Sign Language (BdSL) recog-
BdSL40-consistingof611videosover40words,alongwith nition. In particular, there has been no concerted effort to
twodifferentapproaches:onewitha3DConvolutionalNeu- collectword-leveldataonBdSL,whichhaslimitedtheap-
ral Network model and another with a novel Graph Neural
plicationofmachinelearninginthisarea.
Network approach for the classification of BdSL40 dataset.
Ourcontributionsaresummarizedasfollows:
This is the first study on word-level BdSL recognition, and
thedatasetwastranscribedfromIndianSignLanguage(ISL) • We present the first word-level Bangla Sign Language
usingtheBanglaSignLanguageDictionary(1997).Thepro- dataset(BdSL40),consistingof611videosover40BdSL
posedGNNmodelachievedanF1scoreof89%.Thestudy words, with 8 to 22 video clips per word. This dataset
highlightsthesignificantlexicalandsemanticsimilaritybe-
addresses the lack of available word-level datasets for
tweenBdSL,WestBengalSignLanguage,andISL,andthe
BdSL, enabling research into sign language recognition
lack of word-level datasets for BdSL in the literature. The
usingmachinelearning.
datasetandsourcecodearepubliclyavailable.
• We propose a model for the classification of BdSL40
using a 3D Convolutional Neural Network (3D-CNN)
Introduction
(Tran et al. 2015), which achieved a peak accuracy of
Sign language is a vital mode of communication for the 82.43%onan80-20splitofthedataset.Thisprovidesa
deafcommunity.Whilesignlanguagedictionariesprovidea strongbaselineforfutureresearchinthisarea.
foundationforlearningsignlanguage,thepracticalapplica- • We also propose a novel method for BdSL40 classifica-
tionofsignlanguagevariesintermsofphonological,mor- tion by extracting key points from the videos and con-
phological, grammatical, and lexical aspects 1. The diver- structing a spatiotemporal graph. We then use a Graph
sityofsignlanguagesisinfluencedbyregionalexpressions NeuralNetwork(Yu,Yin,andZhu2017)toclassifythe
andlanguagealphabets,resultinginmultiplesignlanguages, dataset, achieving a peak accuracy of 89% on an 80-20
suchasAmerican,Arabic,French,Spanish,Chinese,andIn- split of the dataset. This method provides an alternative
dian.InBangladesh,thelackofproperdevicesormethods approach to the classification of sign language recogni-
that can serve as interpreters makes it necessary for hear- tionanddemonstratesthepotentialofGNNsinthisarea.
ingindividualstolearnsignlanguagetocommunicatewith
the deaf. However, the complexity of the Bangla Sign lan-
guage,whichutilizesbothhandandbodygestures,posesa
challenge for individuals seeking to learn it. Consequently,
deafindividualshavedifficultyteachingtheirsignlanguage
tohearingindividuals,creatingadistancebetweenthedeaf
andthebroadersociety.Therefore,thereisapressingneed
*CorrespondingAuthor.
E-mail:1705010@ugrad.cse.buet.ac.bd
1https://github.com/Patchwork53/BdSL40 Dataset AI for Bangla 2.0 Honorable Mention
1
4202
naJ
22
]VC.sc[
1v01221.1042:viXraDatasetCreation dividingbythestandarddeviationof0.5foreachcolorchan-
nel.Themodelwastrainedfor120epochswithabatchsize
Owing to geographical proximity and significant cultural
of64andalearningrateof5e-5.
crossover, the sign languages of Bangladesh (BdSL), West
Bengal (WBSL), and India (ISL) bear striking similarities
SpatioTemporalGraphNeuralNetwork
withoneanother.BdSLandWBSLhavehighsemanticsim-
ilarity and are mutually comprehensible while BdSL and In this section, we describe our methodology for classify-
ISL have 75% lexical similarity but differ in their mean- ing the Bangla Sign Language 40 (BdSL40) dataset using
ing(JohnsonandJohnson2016).Forexample,thesignfor Spatio-TemporalGraphNeuralNetwork.
“Fish” in ISL is the same as for “Tortoise” in BdSL. Out Dataset Preprocessing For the preprocessing phase, we
of the three mentioned sign languages, only ISL has a siz- needtoextractthehandskeypointsdatausingapre-trained,
able word-level dataset available to the public: INCLUDE ready-to-usesolution.Mostoftheavailablemethodsrelyon
bySridharetal.(2020).WewentovertheINCLUDEdataset the use of CNNs and are trained on large amounts of data.
and consulted the Bangladesh Sign Language Dictionary Due to its fast inference speed, high accuracy, and simple
(1997) to find the Bangla meaning of the signs in the IN- use, we chose the framework MediaPipe Hands (Lugaresi
CLUDE dataset. From the 263 words in INCLUDE, we etal.2019)forourtask.Itextracts21x-y-zhandkeypoints
were able to collect 40 terms of which 28 words had the of the human body from each frame. The hand key point
same meaning and sign in both ISL and BdSL, and 12 had datawasacquiredviathePythonAPIofMediaPipeHands,
the same sign but a different meaning in BdSL. It is to be using a minimum detection confidence of 0.8 and a mini-
noted that the 40 words had the same signing motions in mum tracking confidence of 0.5 for tracking key points in
both BdSL and ISL. We noticed similarities between other consecutiveframes.
wordsbutdidnotincludetheminBdSL40sincethesigning
Spatio-Temporal Graph Construction Our proposed
motionswerenotthesame.
Spatio-Temporal Graph Neural Network is fed by a four-
Intotal,thereare611videosover40BdSLwords,with8
dimensional matrix in the shape of [N, C, T, V] where
to22videoclipsperword.
N denotes the batch size, C denotes the number of input
features(x-y-z-coordinate), T denotes the input time steps
ProposedMethodology
andV denotesthegraphvertices(joints).
Wepresentthemethodologyforour2differentapproaches Our method uses the 2-stream Adaptive Graph Convo-
inthissection. lutional Network (AGCN) proposed by Li et al. (2018).
TheadjacencymatrixforAGCNiscomposedofthreesub-
VideoResNet
matrices:
Inthissection,wedescribeourmethodologyforclassifying 1. Theinward-linksstartingfromthewristjoint,
theBanglaSignLanguage40(BdSL40)datasetusingVideo
2. Theoutwardlinkspointingintheoppositedirection,and
ResNet(Tranetal.2018).Theproposedmethodologycon-
3. Theself-linksofeachjoint.
sists of two main stages: dataset preprocessing and model
Thus, the matrix is of the shape [V, V, 3], where V is
training.
21inthiswork.TheHand-AGCNmodelusedinthiswork
Dataset Preprocessing. The BdSL40 dataset comprises isastackof7AGCNblockswithincreasingoutputfeature
611 videos. Each video has a resolution of 1080x1920 and dimensions.Aprecedingbatchnormalizationlayerisadded
a frame rate of 30 frames per second. Before training the forinputdatanormalization.Aglobalaveragepoolinglayer
model,wefirstpreprocessthedatasettoensurethatthedata (GAP) followed by a fully connected layer (FC) maps the
isinaformatsuitablefortraining.First,wesplitthedataset outputfeaturestothecorrespondingoutputclasses.
into training and testing sets with an 80-20 ratio. Then, we
HandGraphModeling TheMediaPipeHandskeypoint
extracted the frames from each video and resized them to
extractionmethodpredictsthex-y-zcoordinatesof21hand
100x100toreducethedimensionalityofthedata.
joints; four joints per finger plus an additional wrist joint.
Model Training. We trained the model using the Video Forthedefinitionoftheunderlyinggraph,eachofthejoints
ResNet architecture, which is a variant of the ResNet ar- is connected to its natural neighbor, resulting in a graph of
chitecture that is specifically designed for video data. The 21 vertices and 20 edges. These might be too few connec-
Video ResNet architecture consists of three main compo- tions forthe fine-grained handmovements. To obtain more
nents: a 3D convolutional backbone, a temporal pooling semanticinformationinthehandgraph,twotypesofaddi-
layer, and a fully connected layer. The 3D convolutional tional joints were added. The first type of added joint links
backbone extracts spatio-temporal features from the input the fingertips to the base of the right neighbor finger. The
video frames. The temporal pooling layer aggregates the secondtypeofadditionaljointlinksthefingertipstothemid-
spatio-temporal features across time to produce a fixed- dle segment of the same finger. These supplementary links
lengthfeaturevectorforeachvideoclip.Thefullyconnected help to retrieve more information about the different states
layerthenmapsthefeaturevectortotheoutputclasses. ofthehand.Thefirsttypecontainsdataaboutthehorizontal
Duringtraining,thefirst6andlast8framesofeachvideo and vertical distance of two fingers and can therefore help
wereskippedtoremoveanyredundantinformation.Frames to encode the overlapping or spreading of two fingers. The
were normalized by subtracting the mean value of 0.5 and secondtypeencodesthebendingofthefingers.
2Figure1:BdSL40exampledata:FramesextractedfromgesturelabeledStudent
Figure2:BdSL40exampledata:FramesextractedfromgesturelabeledTortoise
Figure3:Classificationpipelineofaspecificsignlanguagegesture.First,theframesareextractedfromthevideo.Thenthey
arefedintoapretrainedVideoResNetModelwhichdoestheclassificationusing3DConvolutionalNetworks
3Table1:ClassificationresultsfortheBdSL40datasetusing
2s-AGCN
Sign Precision Recall F1-Score
bad 0.922 0.969 0.945
book 0.728 0.780 0.753
Figure4:SpatioTemporalGraphConstruction brown 0.887 0.913 0.900
bed 0.925 0.925 0.925
camera 0.933 0.944 0.939
Model Training Once the spatio-temporal graphs were cheap 0.949 0.944 0.947
constructed, they were used to train a Spatio-Temporal
cow 0.933 0.863 0.897
GraphNeuralNetwork(GNN)modelforclassification.The
crane 0.446 0.944 0.606
2s-AGCNalgorithmwasusedforclassification.
deaf 0.915 0.872 0.893
The batch size was determined in a preliminary experi-
friend 0.895 0.813 0.852
mentusingmini-batchesof32,64,and128,with64result-
fulfill 0.906 0.868 0.886
inginthehighestaccuracyinthevalidationset.Thenumber
glad 0.906 0.853 0.879
of time steps T was empirically specified to be 50 and V
heavy 0.856 0.834 0.845
was set to be 21. The model was trained for 5 epochs with
i 0.930 0.901 0.915
a batch size of 64 and a learning rate of 1e-2. During the
india 0.948 0.953 0.950
experiments,itwasobservedthatafter5epochsnoneofthe
lawyer 0.989 0.924 0.956
modelsshowedanysubstantialaccuracyincrease.
life 0.903 0.914 0.909
ResultAnalysis money 0.915 0.941 0.928
more 0.937 0.886 0.911
In this study, we evaluated two methods for classifying
new 0.913 0.952 0.932
the BdSL40 dataset: Video ResNet and Spatio-Temporal
noon 0.890 0.920 0.905
GNN. The Video ResNet model achieved an accuracy of
pant 0.748 0.825 0.785
82.43%Ontheotherhand,theSpatioTemporalGNNmodel
quiet 0.902 0.934 0.918
achievedanaccuracyof89%
rich 0.898 0.840 0.868
To further analyze the performance of the two methods,
ring 0.922 0.784 0.847
we also calculated precision, recall, and F1 score for each
shirt 0.929 0.926 0.927
class. The results showed that the Spatio-Temporal GNN
shoes 0.809 0.878 0.842
methodhadhigherprecision,recall,andF1scoresformost
skirt 0.712 0.407 0.517
of the classes compared to the Video ResNet method. This
soap 0.782 0.539 0.638
suggests that the Spatio-Temporal GNN method is more
effective in distinguishing between different sign language square 0.926 0.948 0.937
gestures. straight 0.780 0.656 0.713
student 0.934 0.881 0.906
teacher 0.950 0.833 0.887
telephone 0.884 0.884 0.884
thick 0.955 0.923 0.939
time 0.605 0.721 0.658
tortoise 0.822 0.694 0.753
winter 0.891 0.936 0.913
yesterday 0.863 0.852 0.857
you 0.856 0.633 0.728
4References
Johnson,R.J.;andJohnson,J.E.2016.Distinctionbetween
west Bengal sign language and Indian sign language based
on statistical assessment. Sign Language Studies, 16(4):
473–499.
Li, R.; Wang, S.; Zhu, F.; and Huang, J. 2018. Adaptive
graphconvolutionalneuralnetworks. InProceedingsofthe
AAAIconferenceonartificialintelligence,volume32.
Lugaresi,C.;Tang,J.;Nash,H.;McClanahan,C.;Uboweja,
E.;Hays,M.;Zhang,F.;Chang,C.-L.;Yong,M.G.;Lee,J.;
etal.2019.Mediapipe:Aframeworkforbuildingperception
pipelines. arXivpreprintarXiv:1906.08172.
Sridhar, A.; Ganesan, R. G.; Kumar, P.; and Khapra, M.
2020.Include:Alargescaledatasetforindiansignlanguage
recognition. InProceedingsofthe28thACMinternational
conferenceonmultimedia,1366–1375.
Tran,D.;Bourdev,L.;Fergus,R.;Torresani,L.;andPaluri,
M. 2015. Learningspatiotemporal features with 3d convo-
lutionalnetworks. InProceedingsoftheIEEEinternational
conferenceoncomputervision,4489–4497.
Tran, D.; Wang, H.; Torresani, L.; Ray, J.; LeCun, Y.; and
Paluri, M. 2018. A closer look at spatiotemporal convolu-
tionsforactionrecognition.InProceedingsoftheIEEEcon-
ferenceonComputerVisionandPatternRecognition,6450–
6459.
Yu, B.; Yin, H.; and Zhu, Z. 2017. Spatio-temporal graph
convolutionalnetworks:Adeeplearningframeworkfortraf-
ficforecasting. arXivpreprintarXiv:1709.04875.
5