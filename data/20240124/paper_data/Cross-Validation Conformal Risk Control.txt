Cross-Validation Conformal Risk Control
Kfir M. Cohen, Sangwoo Park, Osvaldo Simeone Shlomo Shamai (Shitz)
King’s Communications, Learning, and Information Processing (KCLIP) lab Viterbi Faculty of Electrical and Computing Engineering
Centre for Intelligent Information Processing Systems (CIIPS) Technion—Israel Institute of Technology
Department of Engineering, King’s College London
Abstract—THIS PAPER IS ELIGIBLE FOR THE STUDENT
PAPER AWARD. Conformal risk control (CRC) is a recently
proposed technique that applies post-hoc to a conventional targetriskα
point predictor to provide calibration guarantees. Generalizing Dval
validation
conformalprediction(CP),withCRC,calibrationisensuredfora
set predictor that is extracted from the point predictor to control D split train riskV coB ntrol
a risk function such as the probability of miscoverage or the
false negative rate. The original CRC requires the available data Dtr threshold
set to be split between training and validation data sets. This
λ(cid:0)Dval|Dtr(cid:1)
can be problematic when data availability is limited, resulting test ΓVB(x|D)
i in
s
ii nn te rf ofi dc uie cn et dse tht ap tre isdi bc ato sr es d. oIn nt ch ri os ssp -a vp ae lir d, aa tin oo nv ,e rl aC thR eC
r
tm hae nth oo nd inpu xt predictiveset Γ λ(x|Dtr) ∅ λ Y
validation as the original CRC. The proposed cross-validation
CRC (CV-CRC) extends a version of the jackknife-minmax from
CP to CRC, allowing for the control of a broader range of risk
functions. CV-CRC is proved to offer theoretical guarantees on
the average risk of the set predictor. Furthermore, numerical targetriskα
experiments show that CV-CRC can reduce the average set size D 1 k=1,...,K
D
with respect to CRC when the available data are limited. k foldset
CV
K-
I. INTRODUCTION fold D 2 train riskcontrol
D
split ...
D
−k
D
A. Context and Motivation K
thresholdλCV(D)
One of the key requirements for the application of artificial ...
test
intelligence(AI)toolstorisk-sensitivefieldssuchashealthcare ΓCV(x|D)
and engineering is the capacity of AI algorithms to quantify inpu xt predictiveset (cid:83)K k=1Γ λ(x|D −k) ∅ λ Y
their uncertainty [1], [2]. This requires guarantees on the
adherence of the “error bars” produced by the AI model
to the true predictive uncertainty. The predictive uncertainty Fig. 1. Illustration of (top) the existing validation-based conformal risk
control(VB-CRC)[7];and(bottom)theproposedmethodcross-validation-
encompasses both the epistemic uncertainty caused by limited
basedconformalriskcontrol(CV-CRC),whichaimsatreducingthepredictive
availability of data and the aleatoric uncertainty inherent setssizesbyreusingtheavailabledataD moreefficiently.
in the randomness of data generation [3]. Without making
strong assumptions on the data generation mechanism it is
The work of Kfir M. Cohen, Sangwoo Park and Osvaldo Simeone has generallyimpossibletoprovidestrictuncertaintyquantification
beensupportedbytheEuropeanResearchCouncil(ERC)undertheEuropean
guarantees for any input, but assumption-free guarantees can
Union’sHorizon2020researchandinnovationprogramme,grantagreementNo.
725731.TheworkofOsvaldoSimeonehasalsobeensupportedbyanOpen be established on average over validation and test data [4].
Fellowship of the EPSRC with reference EP/W024101/1, by the European Conformalprediction(CP)[5],[6],anditsextensionconformal
Union’sHorizonEuropeProjectCENTRICunderGrant101096379,andby
risk control (CRC) [7], are widely established methodologies
ProjectREASON,aUKGovernmentfundedprojectundertheFutureOpen
Networks Research Challenge (FONRC) sponsored by the Department of for the evaluation of predictors with provable uncertainty
ScienceInnovationandTechnology(DSIT).TheworkofShlomoShamaihas quantification properties.
beensupportedbytheGermanResearchFoundation(DFG)viatheGerman-
Israeli Project Cooperation (DIP), under Project SH 1937/1-1. The authors To elaborate, assume access to a data set D of N pairs of
acknowledgeKing’sComputationalResearch,EngineeringandTechnology examplesconsistingofinputxandoutputy.Basedonthedata
Environment(CREATE).RetrievedJanuary18,2024,fromhttps://doi.org/10.
set D and on a class of point predictors, CP and CRC produce
18742/rnvf-m076.
KMC,SPandOSconceivedtheproject;KMCandSPdevelopedthetheory a set predictor Γ(x|D) mapping a test input x into a subset of
with the supervision and guidance of OS; KMC performed the simulation; theoutputspace.ThesizeofthesetpredictorΓ(x|D)provides
KMC, SP and OS prepared the manuscript; and SS reviewed the text and
a measure of the uncertainty of the predictor for input x [6].
contributedtothevisionofthepaper.Allauthorsdiscussedtheresultsand
contributedtothefinalmanuscript. On average over the data set D and over a test input-output
4202
naJ
22
]GL.sc[
1v47911.1042:viXrapair (x,y), we wish to guarantee the calibration condition and hence it may be referred to as VB-CRC for consistency
(cid:2) (cid:0) (cid:1)(cid:3) with the terminology applied above for CP. Accordingly, it
E ℓ y,Γ(x|DDD) ≤α, (1)
DDD,x,y∼p0(D,x,y) relies on a split of the data set into training and validation sets,
where boldface fonts denote random quantities, ℓ(·,·) is a loss resulting in inefficient predictive sets when data are limited.
measure, and α a user-specified maximum average loss level.
C. Main Contributions
In (1), under the joint distribution p (D,x,y), the examples
0
in the data set DDD and the test pair (x,y) are assumed to be Inthispaper,weintroduceanovelversionofCRCbasedon
independent identically distributed (i.i.d.), or, more generally cross-validation. The proposed CV-CRC method generalizes
exchangeable. CV-CP, supporting arbitrary bounded and monotonic risk
CRC can satisfy the requirement (1) for any user-specified functions. As we will demonstrate, the design and analysis of
target average loss level α, as long as the loss function is CV-CRC are non-trivial extensions of CV-CP, requiring new
bounded and it decreases as the predicted set grows. Examples definitions and proof techniques.
of such loss functions are the 0-1 miscoverage probability Therestofthepaperisorganizedasfollows.Sec.IIprovides
the necessary background, while CV-CRC is presented in
ℓ(y,Γ)=1(y ∈/ Γ), (2) Sec. III. Numerical experiments are reported in Sec. IV, and
Sec. V draws some conclusions. All proofs are deferred to the
which returns 1 if the true label y is not in the set Γ and
supplementary material.
0 otherwise, and the false negative rate, which returns the
fraction of true values of y that are not included in set Γ for
II. BACKGROUND
multi-label problems [7].
Consider N +1 data points
The requirement (1) can be always satisfied for such
monotonic loss functions by returning as set predictor Γ the (x[1],y[1]) , (x[2],y[2]) , ... , x[N +1],y[N +1]) (3)
entire set of possible values for the output variable y. However, (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
=z[1] =z[2] =z[N+1]
a set predictor is useful only as long as it is of moderate
average size. The motivation of this work is to construct a set over the sample space X ×Y that are drawn according to
predictor that meets (1), while producing small predictive sets an exchangeable joint distribution p 0(D,x,y) over index i=
even in the presence of a limited data set D. 1,...,N. The first N data points constitute the data set D =
{z[i] = (x[i],y[i])}N , while the last data point z[N +1] is
i=1
B. State of the Art
the test pair, which is also denoted as z = (x,y). We fix a
CP addresses the design of set predictors satisfying the loss function ℓ:Y ×2Y →R, which, given any label y ∈Y
calibration condition (1) in the special case of the miscoverage and a predictive set Γ⊆Y, returns a loss bounded as
loss(2)[5],[6],[8].ThereareseveralvariantsofCP,including
b≤ℓ(y,Γ)≤B (4)
validation-based CP (VB-CP), cross-validation-based CP (CV-
CP) [9], and full CP [5]. While full CP is considered to be for some constants B < ∞ and b ∈ {−∞}∪R. We further
impractical, requiring many rounds of retraining, VB-CP splits require that the loss is monotonic in the predictive set Γ in the
thedatasetintotrainingandvalidationdatasets,anditoperates sense that the following implication holds
over a single round of training. However, the need to devote
a separate data set for validation can significantly reduce the Γ 1 ⊆Γ 2 ⇒ ℓ(y,Γ 1)≥ℓ(y,Γ 2) for each y ∈Y. (5)
quality of the trained model, resulting in predictive sets of
Note that the 0-1 miscoverage loss (2) assumed by CP satisfies
large sizes when data are limited [9].
(4) with b=0 and B =1, and it also satisfies the implication
CV-CP reduces the computational complexity as compared
(5).
to full CP, while reducing the predicted set size as compared
For a given data set D, VB-CRC uses a two-step procedure
to VB-CP. This is done by partitioning the available data set
to satisfy the constraint (1) for some target average loss α in
into multiple folds, each acting as a validation data set for
the interval
the model trained based on leave-fold-out data. At the cost of
b ≤ α ≤ B. (6)
increasingthecomplexity,requiringasmanytrainingroundsas
the number of folds, CV-CP was shown to produce important To start, as illustrated in the top panel of Fig. 1, the available
savings in terms of prediction set sizes [10], [11], [12]. datasetDissplitintoNtrexamplesformingthetrainingsetDtr
Other extensions of CP include CP-aware training strategies andNval =N−Ntr pointsformingthevalidationsetDval with
[13], [14], prediction under distributional shifts [15], improve- D =Dtr∪Dval.InthefirststepofVB-CRC,amodelistrained
ments in the training algorithms [16], [17], novel calibration based on the training set Dtr using any arbitrary scheme. Then,
metrics [18], [19], applications to engineering problems [10], in the second step, VB-CRC determines a threshold λ ∈ R
[20],andonlineversions[21],[22]withapplications[23],[24]. by using the validation data set Dval. As explained next, the
CRC generalizes CP to address the calibration criterion (1) threshold λ dictates which labels y ∈Y are to be included in
for a wider class of risks, with the only constraints that the the prediction set Γ (x|Dtr) for any test input x as follows.
λ
risk function be bounded and monotonic in the predicted set A nonconformity (NC) score NC((x,y)|Dtr) is selected that
size [7], [22], [25], [26]. The original CRC is validation-based, evaluates the loss of the trained predictor on a pair (x,y).
2Examples of NC scores include the residual between the label fold as D ={(x [1],y [1]),...,(x [N/K],y [N/K])}, and
k k k k k
andatrainedpredictorforregressionproblemsandthelog-loss we will denote the mapping of the i-th data point z[i] to its
for classification problems [6], [27], [28]. With the given NC fold index as k[i]:{1,...,N}→{1,...,K}. Like VB-CRC,
score, the set prediction is obtained as CV-CRC operates in two steps.
(cid:110) (cid:12) (cid:111) In the first step, for any k-th fold, a model is trained using
Γ (x|Dtr)= y′ ∈Y(cid:12)NC((x,y′)|Dtr)≤λ , (7)
λ (cid:12) the leave-fold-out training set D −k = D\D k of N −N/K
samples. Accordingly, unlike VB-CRC, K training rounds are
thusincludingalllabelsy′ ∈Y withNCscoresmallerorequal
required for CV-CRC. In the second step, as we will detail,
to the threshold λ. By design, the set (7) satisfies the nesting
CV-CRC determines a threshold λ to determine which values
property
of the output y to include in the predicted set.
λ <λ ⇒ Γ (x|Dtr)⊆Γ (x|Dtr) (8) Given a threshold λ, CV-CRC produces the predictive set
1 2 λ1 λ2
for any input x and data sets Dtr. ΓCV(x|D)=(cid:110) y′ ∈Y(cid:12) (cid:12) min (cid:8) NC((x,y′)|D )(cid:9) ≤λ(cid:111) ,
λ (cid:12) −k
We define the risk as the population, or test, loss of the k∈{1,...,K}
(14)
predicted set (7) as
which includes all labels y′ ∈Y with minimum, i.e., best case,
R(λ|Dtr)=E (cid:104) ℓ(cid:0) y,Γ (x|Dtr)(cid:1)(cid:105) . (9) NC score across the K folds, that is not larger than λ.
x,y∼p0(x,y) λ
To determine the threshold λ, CV-CRC estimates the
Given the validation data set Dval ={(xval[i],yval[i])}Nval, the population risk (9) using cross-validation as
i=1
risk (9) can be estimated as (cid:18) K N/K (cid:19)
(cid:18)Nval (cid:19)
RˆCV(λ|D)= K1
+1
(cid:88) K
N
(cid:88) ℓ(cid:0) y k[j],Γ λ(cid:0) x k[j](cid:12) (cid:12)D −k(cid:1)(cid:1) +B .
Rˆval(λ|Dtr,Dval)= 1 (cid:88) ℓ(cid:0) yval[i],Γ (xval[i]|Dtr)(cid:1) +B , k=1 j=1
Nval+1 λ (15)
i=1 (10) The cross-validation-based estimate (15) can be interpreted
which is a function of the threshold λ. This corresponds to a as the conventional cross-validation loss evaluated on an
regularized empirical estimate of the risk (9) that effectively augmented data set
adds an (N +1)-th dummy validation example with maximal Daug =(cid:8) D ,D ,...,D ,D (cid:9) , (16)
1 2 K dummy
loss B. (cid:124) (cid:123)(cid:122) (cid:125)
=D
VB-CRC chooses the lowest threshold λ such that the
with the first K folds being the available data set D =
estimate (10) is no larger than the target average risk α as in
{D ,...,D }, and the additional (K+1)-th fold containing
(cid:110) (cid:12) (cid:111) 1 K
λVB(Dval|Dtr)=inf λ(cid:12)Rˆval(λ|Dtr,Dval)≤α . (11) N/K dummy points with the maximal loss of B. In a manner
(cid:12)
λ similar to VB-CRC, the addition to dummy data points acts as
With this threshold choice, as proven in [7], the set predictor aregularizerfortheestimate(15),whichisrequiredtoprovide
(7) obtained via VB-CRC, i.e., performance guarantees.
ΓVB(x|Dtr,Dval)=Γ (x|Dtr) (12) Finally, CV-CRC selects the threshold λ by imposing that
λVB(Dval|Dtr)
the cross-validation based estimate (15) of the loss is no larger
ensuresthedesiredcondition(1).Moreprecisely,thecondition than the target average loss value α as in
(1) holds for any fixed training set Dtr, i.e., we have the (cid:110) (cid:12) (cid:111)
λCV(D)=inf λ(cid:12)RˆCV(λ|D)≤α . (17)
inequality (cid:12)
λ
E (cid:2) ℓ(cid:0) y,ΓVB(x|Dtr,DDDval)(cid:1)(cid:3) ≤α. (13) CV-CRC reduces to the jackknife-minmax scheme in [9]
DDDval,x,y∼p0(Dval,x,y)
when evaluated with the miscoverage loss (2) in the special
Furthermore, in order for (13) to hold, VB-CRC only requires case of K =N folds.
the validation data Dval and test pair (x,y) to be exchangeable.
Theorem 1. Fix any bounded and monotonic loss function
III. CROSS-VALIDATIONCONFORMALRISKCONTROL ℓ(·,·) satisfying conditions (4) and (5), and any NC score
While VB-CRC reviewed in the previous section guarantees NC((x,y)|Dtr) that is permutation-invariant with respect to
the average risk condition (13), splitting the available data the ordering of the examples in the training set Dtr. For any
set into training and validation sets may potentially lead to number of folds satisfying K ≥B/(α−b)−1, the CV-CRC
inefficientsetpredictors,havinglargepredictivesetsonaverage. predictive set ΓCV (x|D) with (14) and (17) guarantees the
λCV(D)
In this section, we introduce the proposed CV-CRC scheme condition
that aims at improving the efficiency of VB-CRC [7] via cross-
E
(cid:104) ℓ(cid:0) y,ΓCV(x|DDD)(cid:1)(cid:105)
≤α. (18)
validation [9], while still guaranteeing condition (1).
DDD,x,y∼p0(D,x,y)
To start, as illustrated in the bottom panel of Fig. 1, the The theorem thus confirms that CV-CRC meets the desired
available data set D = {z[i]}N is partitioned using a fixed condition(1).Inthisregard,wenotethat,asin(1),theaverage
i=1
mapping into K folds D ={D }K of N/K-samples each, loss in (18) includes averaging over the entire data set D,
k k=1
which is assumed to be an integer. We will write each k-th unlike the condition (13) satisfied by VB-CRC. Furthermore,
3Theorem 1 requires the NC score to be permutation-invariant 0.12
with respect to the data points in the training set, which is not
the case for VB-CRC. Permutation-invariance is also needed 0.1
for CV-CP [9], as well as for full CP [5]. In practice, a
permutation-invariant NC score can be obtained by imple- 0.08
menting permutation-invariant training schemes such as full
gradient descent, in which the final trained model does not 0.06
depend on the ordering of the training data points.
0.04
IV. EXAMPLES
In this section, we numerically validate the proposed CV- 0.02
CRC using two synthetic examples. The first is a vector
regression problem, whereas the second concerns the problem 0
100 200 300 400 500
of temporal point process prediction [29], [30]. Our code is
publicly available1.
A. Vector Regression Fig. 2. Empirical risk of VB-CRC and CV-CRC for the vector regression
problem.
Inspired by the example in [9], we first investigate a
vector regression problem in which the output variable y = 50
[y ,...,y ]⊤ is m-dimensional. The joint distribution of data 40
1 m
set D and test pair (x,y) is obtained as
(cid:90) (cid:18)N+1 (cid:19) 20
(cid:89)
p (D,x,y)= p (ϕ) p (x[i])p (y[i]|x[i],ϕ) dϕ,
0 0 0 0
i=1
(19) 10
where (x[N +1]=x,y[N +1]=y) is the test example, and 8
we have the Gaussian distributions
p (x) = N(x|0,d−1I ), (20a) 4
0 d
p (y|x,ϕ) = N(y|ϕ⊤·x,β−1I ), (20b)
0 0 m
2
while p (ϕ) is a mixture of Gaussians with means determined
0 100 200 300 400 500
by an i.i.d. Bernoulli vector b as
p (ϕ)=E (cid:2) N(ϕ|µ b,γ−1I )(cid:3) . (21)
0 bi∼.i.d.Bern(0.5) 0 0 d Fig. 3. Empirical inefficiency of VB-CRC and CV-CRC for the vector
regressionproblem.
We set µ = 10, γ = 1, β = 4, d = |X| = 50, and
0 0 0
m=|Y|=30. Note that the distribution (19) is exchangeable.
Using maximum-likelihood learning, given a training data with [·] standing for the jth element of its argument for VB-
j
set Dtr, we obtain the model parameter ϕM DL tr used for the linear CRC, and
prediction model yˆ(x|Dtr) = (ϕML)⊤x as ϕML = X† Y ,
D Dtr Dtr Dtr
where (·)† denotes the pseudo-inverse, (·)⊤ denotes transpose, (cid:91)K (cid:110) (cid:12) (cid:111)
and the input and label data matrices X ∈RN×d and Y ∈ ΓC jV = y j(cid:12) (cid:12)|y j −[yˆ(x|D −k)] j|≤λCV(D)/2 (24)
D D
RN×m haveinput(xtr[i])⊤ andlabel(ytr[i])⊤ astheirithrows, k=1
respectively. for CV-CRC. The loss function used in the risk (1) is defined
The NC score is set to the maximum prediction residual as
m
across the m dimensions of the output variable y as
ℓ(y,Γ)=
1 (cid:88) 1(cid:0)
y ∈/ Γ
(cid:1)
, (25)
NC((x,y)|Dtr)=2(cid:13) (cid:13)y−yˆ(x|Dtr)(cid:13)
(cid:13) , (22)
m
j=1
j j
∞
where the infinity norm (cid:13) (cid:13)·(cid:13) (cid:13) returns the largest magnitude which evaluates the fraction of entries of vector y that are not
∞
of its input vector. This results in predictive sets (12) and (14) included in the predictive set. This loss satisfies condition (4)
with (17) in the form of Γ=Γ ×···×Γ , with × being the with b=0 and B =1. Note that CP is not applicable to this
1 m
Cartesian product and loss, since it is different from (2).
Lastly,wedefinetheinefficiencyasthesizeofthepredictive
(cid:110) (cid:12) (cid:111)
ΓV jB = y j(cid:12) (cid:12)|y j −[yˆ(x|Dtr)] j|≤λVB(Dval|Dtr)/2 (23) set evaluated as the average over all dimensions of the
predictive intervals across the m dimensions of the output
1https://github.com/kclip/cvcrc y, i.e., ineff(Γ)= m1 (cid:80)m j=1(cid:12) (cid:12)Γ j(cid:12) (cid:12).
4futureeventsy∈Rm
pasteventsx∈Rd
time 0.2
t1 t2 ... td td+1 td+2 ... td+m
0.15
predictedset
D pointprocess Γ(x|D)⊆Rm Γ1
...
setpredictor
desiredriskα 0.1
Γm
20 30 40 50 60 70
Fig.4. Temporalpointprocessprediction:Afterobservingthepastdtimes
t1,...,tn,apointprocesssetpredictoroutputspredictiveintervalsΓj(x|D)
foreachofthenextmpointswithj=1,...,m.
30
25
For target risk α = 0.1, the empirical risk and empirical
inefficiency of Nte =200 test covariate-output pairs, averaged
20
over50independentsimulations,areshowninFig.2andFig.3.
Fig.2,validatesthetheoreticalresultthatCRCschemessatisfy
15
condition (1). However, from Fig. 3, VB-CRC is observed to
have a larger inefficiency than CV-CRC, particularly in the
10
small data set size regime. Thus, CV-CRC uses data more 20 30 40 50 60 70
efficiently, with K =20 folds striking a good balance between
inefficiency and computational complexity in this regime.
Fig.5. Empiricalrisk(top)andinefficiency(bottom)ofVB-CRCandN-CV-
CRCforthetemporalpointprocesspredictionproblem.
B. Temporal Point Process Prediction
A temporal process consists of a sequence of events at
random times t ,t ,... with t < t < ... As illustrated in to lie outside the predicted intervals, i.e., α=1/6. We average
1 2 1 2
Fig. 4, given the past d events’ timings x={t ,...,t }, the over 200 independent simulations with Nte =1000 test points
1 d
goal is to output intervals Γ (x|D) for each of the following in each run.
j
m events with j =1,...,m. The loss function is defined as ThetoppanelofFig.5illustratethetestrisk(25)asfunction
in (25). of data set size N, validating that both scheme attain risks
Data and test sequences of timings are generated following lower than the desired level α. The bottom panel of the figure
a self-exciting Hawkes process [31] with intensity function shows that CV-CRC with K =N reduces the average size of
the predicted intervals.
(cid:88) (cid:16) (cid:17)
λ(t|H )=µ+ α β e−β1(t−ti)+α β e−β2(t−ti) ,
t 1 1 2 2
V. CONCLUSION
i:ti<t
In this paper, we have introduced a novel conformal risk
with µ=0.2,α =α =0.4,β =1 and β =20 [29]. The
1 2 1 2 control (CRC) scheme based on cross-validation, generalizing
predictor is a recurrent neural network that outputs a predictive
cross-validation CP to losses beyond miscoverage. The pro-
density function p(t |t ,...,t ,ϕ ) with trained parameter
i+1 1 i Dtr posedCV-CRCwasshowntoprovablycontroltheaveragerisk,
ϕ [29]. The median tˆ (t ,...,t ,ϕ ) of the predictive
Dtr i+1 1 i Dtr withexperimentsdemonstratingittobemoreefficientthanVB-
distribution is used as the point estimate for the (i+1)-th
CRC when the available data for training and calibration are
event. For i>d, estimates {tˆ}i−1 are used in lieu of the
j j=d+1 scarce. Further work may consider using the jackknife+ of [9]
correct timings in the point prediction.
instead of the jackknife-minmax for more efficient predictive
VB-CRC (12) produces intervals
sets; and extending the scheme to meta-learning [32].
(cid:110) (cid:12) (cid:111)
ΓVB = y (cid:12)|y −tˆ (Dtr)|≤γjλVB(Dval|Dtr)/2 , (26)
j j(cid:12) j d+j
where multiplication by the interval common ratio γ = 1.2
increases the interval sizes for later predictions, and for the
CV-CRC (14), we have
(cid:91)K (cid:110) (cid:12) (cid:111)
ΓCV = y (cid:12)|y −tˆ (D )|≤γjλCV(D)/2 . (27)
j j(cid:12) j d+j −k
k=1
We set the length of the observed sequence as d = 60, and
predict the next m=6 events. We allow one event on average
5REFERENCES [19] A.Perez-Lebel,M.LeMorvan,andG.Varoquaux,“BeyondCalibration:
EstimatingtheGroupingLossofModernNeuralNetworks,”inICLR
[1] D.Tran,J.Liu,M.W.Dusenberry,D.Phan,M.Collier,J.Ren,K.Han, 2023–TheEleventhInternationalConferenceonLearningRepresenta-
Z. Wang, Z. Mariet, H. Hu et al., “Plex: Towards reliability using tions,2023.
pretrained large model extensions,” arXiv preprint arXiv:2207.07411,
[20] S.ParkandO.Simeone,“QuantumConformalPredictionforReliable
2022. UncertaintyQuantificationinQuantumMachineLearning,”IEEETrans-
[2] B.Rajendran,O.Simeone,andB.M.Al-Hashimi,“Towardsefficientand actionsonQuantumEngineering,no.01,pp.1–24,nov2023.
trustworthyaithroughhardware-algorithm-communicationco-design,”
[21] I.GibbsandE.Candes,“AdaptiveConformalInferenceUnderDistribu-
arXivpreprintarXiv:2309.15942,2023.
tionShift,”AdvancesinNeuralInformationProcessingSystems,vol.34,
[3] O.Simeone,MachineLearningforEngineers. CambridgeUniversity
pp.1660–1672,2021.
Press,2022.
[22] S. Feldman, L. Ringel, S. Bates, and Y. Romano, “Achieving Risk
[4] J.LeiandL.Wasserman,“Distribution-FreePredictionbandsfornon- ControlinOnlineLearningSettings,”TransactionsonMachineLearning
parametricRegression,”JournaloftheRoyalStatisticalSocietySeries
Research, 2023.[Online]. Available: https://openreview.net/forum?id=
B:StatisticalMethodology,vol.76,no.1,pp.71–96,2014.
5Y04GWvoJu
[5] V. Vovk, A. Gammerman, and G. Shafer, Algorithmic Learning in a
[23] Y.Zhang,S.Park,andO.Simeone,“BayesianOptimizationwithFormal
RandomWorld. Springer,2005,springer,NewYork. Safety Guarantees via Online Conformal Prediction,” arXiv preprint
[6] A. N. Angelopoulos and S. Bates, “Conformal Prediction: A Gentle arXiv:2306.17815,2023.
Introduction,”FoundationsandTrends®inMachineLearning,vol.16,
[24] K. M. Cohen, S. Park, O. Simeone, P. Popovski, and S. Shamai,
no.4,pp.494–591,2023.
“Guaranteeddynamicschedulingofultra-reliablelow-latencytrafficvia
[7] A. N. Angelopoulos, S. Bates, A. Fisch, L. Lei, and T. Schuster, conformalprediction,”IEEESignalProcessingLetters,vol.30,pp.473–
“ConformalRiskControl,”arXivpreprintarXiv:2208.02814,2022.
477,2023.
[8] M.Fontana,G.Zeni,andS.Vantini,“ConformalPrediction:AUnified
[25] S.Bates,A.Angelopoulos,L.Lei,J.Malik,andM.Jordan,“Distribution-
ReviewofTheoryandNewChallenges,”Bernoulli,vol.29,no.1,pp.1
Free, Risk-Controlling Prediction Sets,” Journal of the ACM (JACM),
–23,2023.[Online].Available:https://doi.org/10.3150/21-BEJ1447
vol.68,no.6,pp.1–34,2021.
[9] R.F.Barber,E.J.Candes,A.Ramdas,andR.J.Tibshirani,“Predictive
[26] A.N.Angelopoulos,S.Bates,E.J.Candès,M.I.Jordan,andL.Lei,
InferencewiththeJackknife+,”TheAnnalsofStatistics,vol.49,no.1,
“Learn Then Test: Calibrating Predictive Algorithms to Achieve Risk
pp.486–507,2021. Control,”arXivpreprintarXiv:2110.01052,2021.
[10] K.M.Cohen,S.Park,O.Simeone,andS.ShamaiShitz,“CalibratingAI
[27] A.N.Angelopoulos,S.Bates,M.I.Jordan,andJ.Malik,“Uncertainty
ModelsforWirelessCommunicationsviaConformalPrediction,”IEEE
Sets for Image Classifiers using Conformal Prediction,” in 9th
TransactionsonMachineLearninginCommunicationsandNetworking,
International Conference on Learning Representations, ICLR 2021,
vol.1,pp.296–312,2023. VirtualEvent,Austria,May3-7,2021. OpenReview.net,2021.[Online].
[11] N. Deutschmann, M. Rigotti, and M. R. Martinez, “Adaptive Con-
Available:https://openreview.net/forum?id=eNdiU_DbM9
formal Regression with Jackknife+ Rescaled Scores,” arXiv preprint
[28] Y. Romano, E. Patterson, and E. Candes, “Conformalized Quantile
arXiv:2305.19901,2023.
Regression,”AdvancesinNeuralInformationProcessingSystems,vol.32,
[12] C.Gupta,A.K.Kuchibhotla,andA.Ramdas,“NestedConformalPredic-
2019.
tionandQuantileOut-Of-BagEnsembleMethods,”PatternRecognition,
[29] T.Omi,N.Ueda,andK.Aihara,“FullyNeuralNetworkBasedModel
vol.127,p.108496,2022. for General Temporal Point Processes,” in Proceedings of the 33rd
[13] D. Stutz, K. D. Dvijotham, A. T. Cemgil, and A. Doucet, “Learning International Conference on Neural Information Processing Systems,
OptimalConformalClassifiers,”inInternationalConferenceonLearning
2019,pp.2122–2132.
Representations,2021.
[30] M.Dubey,R.Palakkadavath,andP.Srijith,“BayesianNeuralHawkes
[14] B.-S. Einbinder, Y. Romano, M. Sesia, and Y. Zhou, “Training ProcessforEventUncertaintyPrediction,”InternationalJournalofData
Uncertainty-Aware Classifiers with Conformalized Deep Learning,” ScienceandAnalytics,pp.1–15,2023.
AdvancesinNeuralInformationProcessingSystems,vol.35,pp.22380–
[31] A.G.Hawkes,“SpectraofSomeSelf-ExcitingandMutuallyExciting
22395,2022. PointProcesses,”Biometrika,vol.58,no.1,pp.83–90,1971.
[15] R.J.Tibshirani,R.FoygelBarber,E.Candes,andA.Ramdas,“Conformal
[32] S. Park, K. M. Cohen, and O. Simeone, “Few-Shot Calibration of
Prediction under Covariate Shift,” Advances in Neural Information
Set Predictors via Meta-Learned Cross-Validation-Based Conformal
ProcessingSystems,vol.32,2019.
Prediction,” IEEE Transactions on Pattern Analysis and Machine
[16] Y. Yang and A. K. Kuchibhotla, “Finite-Sample Efficient Conformal Intelligence,no.01,pp.1–13,oct2023.
Prediction,”arXivpreprintarXiv:2104.13871,2021.
[33] A.K.Kuchibhotla,“Exchangeability,ConformalPrediction,andRank
[17] A.Kumar,S.Sarawagi,andU.Jain,“TrainableCalibrationMeasures Tests,”arXivpreprintarXiv:2005.06095,2020.
forNeuralNetworksfromKernelMeanEmbeddings,”inInternational
[34] A. Dean and J. Verducci, “Linear Transformations that Preserve Ma-
ConferenceonMachineLearning. PMLR,2018,pp.2805–2814. jorization,SchurConcavity,andExchangeability,”LinearAlgebraand
[18] M.J.Holland,“MakingLearningMoreTransparentusingConformalized ItsApplications,vol.127,pp.121–138,1990.
PerformancePrediction,”arXivpreprintarXiv:2007.04486,2020.
6Cross-Validation Conformal Risk Control:
Supplementary Material
APPENDIXA
PROOFTHATVB-CRCACHIEVESTARGETRISK
In this appendix, we prove condition (13) for VB-CRC. While this result was originally shown in [7], here we provide an
equivelant proof that is more convenient to support the proof of Theorem 1 in Appendix C. We start by bounding the VB-CRC
threshold (11) using the following steps
λVB(Dval|Dtr) =
inf(cid:40) λ(cid:12) (cid:12)
(cid:12) 1
(cid:18) (cid:88)Nval
ℓ(cid:0) yval[i],Γ (xval[i]|Dtr)(cid:1)
+B(cid:19) ≤α(cid:41)
(cid:12)Nval+1 λ
λ (cid:12)
i=1
≥
inf(cid:40) λ(cid:12) (cid:12)
(cid:12) 1
(cid:18) (cid:88)Nval
ℓ(cid:0) yval[i],Γ (xval[i]|Dtr)(cid:1) +ℓ(cid:0) y,Γ
(x|Dtr)(cid:1)(cid:19) ≤α(cid:41)
(28)
(cid:12)Nval+1 λ λ
λ (cid:12)
i=1
=: λ′(Dval,x,y|Dtr),
where the inequality in (28) follows from (4). The ground-truth risk averaged over test example (x,y) and validation set Dval
is upper bounded as
E
(cid:104) ℓ(cid:0)
y,Γ
(x|Dtr)(cid:1)(cid:105)
≤ E
(cid:104) ℓ(cid:0)
y,Γ
(x|Dtr)(cid:1)(cid:105)
(29a)
DDDval,x,y∼p0(Dval,x,y) λVB(DDDval|Dtr) DDDval,x,y∼p0(Dval,x,y) λ′(DDDval,x,y|Dtr)
≤ α, (29b)
where the first inequality (29a) follows the nesting property (8) given inequality (28). The second inequality (29a) is an
application of the following lemma, whose proof is deferred to Appendix B.
Lemma 2. Letv ,...,v berandomvariableswithanexchangeablejointdistributionsuchthattheequationP(cid:0) 1 (cid:80)M v ≤
(cid:1) 1 M M i=1 i
α =1 holds. Then, we have the inequality E [v ]≤α for all m∈{1,...,M}.
v1:M∼p0(v1:M) m
To apply Lemma 2 in (29a), we define M =Nval+1 variables by
(cid:40) ℓ(cid:0) yval[i],Γ (xval[i]|Dtr)(cid:1) i=1,...,Nval
v = λ′(DDDval,x,y|Dtr) (30)
i ℓ(cid:0) y,Γ (x|Dtr) i=Nval+1,
λ′(DDDval,x,y|Dtr)
whose empirical average is, by (28), no greater than α. Furthermore, to comply with the technical conditions of Lemma 2,
variables v need to be exchangeable. This is justified by the following lemma, which is a corollary of [33, Theorem 3] or
1:M
[34, Theorem 4].
Lemma 3. Let w ,...,w ∈ W be a collection of exchangeable random vectors, f : W → R be a fixed mapping, and
1 M
g :WM →R be a fixed mapping that is permutation-invariant, i.e., oblivious to the ordering of its M input values. Then, the
M random variables formed as v =f(w ,g(w )), ... ,v =f(w ,g(w )) are exchangeable.
1 1 1:M M M 1:M
Lemma 3 implies the exchangeability of variables (30) by defining the Nval+1 exchangeable vectors as
(cid:40)
zval[i] i=1,...,Nval
w = (31)
i (x,y) i=Nval+1;
the permutation invariant function is set as g(·)=λ′(·|Dtr); the fixed mapping is
v =f(cid:0) w =(x ,y ),g(w )(cid:1) =ℓ(y ,Γ (x |Dtr)); (32)
i i i i 1:M i g(w1:M) i
and we focus on the average risk of the last term, i.e., m=M =Nval+1. This completes the proof of (13).
APPENDIXB
PROOFOFLEMMA2
In this appendix, we prove Lemma 2. To start, define a bag u= u ,...,u of M elements u ...,u as a multiset, i.e.,
1 M 1 M
as an unordered list with allowed repetitions [5]. By definition, two(cid:72)bags u and(cid:73)v are equal if they contain the same elements,
bag
irrespective of the ordering of their identical items, which we write as u = v. One can form a bag out of a random vector
v ,...,v ∼p (v ) by discarding the order of the items. Accordingly, the distribution of the bag u is given by
1 M 0 1:M
p
(u)=P(cid:16)
v ,...,v
b =ag
u ,...,u
(cid:17)
=
(cid:88) P(cid:0)
v =u ,...,v =u
(cid:1)
, (33)
0 1 M 1 M 1 π(1) M π(M)
(cid:72) (cid:73) (cid:72) (cid:73)
π∈ΠM
7where the sum is over the set Π of all M! permutations. For example, three Bernoulli variables v ,v ,v ∼ Bern(q) with
M 1 2 3
i.i.d.
bag
parameter q ∈[0,1] can constitute four different bags. In fact, bag u = v ,v ,v equals 0,0,0 with probability (w.p.)
1 2 3
(1−q)3, 0,0,1 w.p. 3(1−q)2q, 0,1,1 w.p. 3(1−q)q2, and 1,1,1 (cid:72)w.p. q3. (cid:73) (cid:72) (cid:73)
With th(cid:72)ese defi(cid:73)nitions, we obtain(cid:72)the fol(cid:73)lowing chain of inequa(cid:72)lities (cid:73)
(cid:104) (cid:104) (cid:12) (cid:105)(cid:105)
E [v ] = E E v (cid:12) v ,...,v b =ag u (34a)
v1:M∼p0(v1:M) m u∼p0(u) v1:M∼p0(v1:M|u) m(cid:12)
(cid:72)
1 M
(cid:73)
M
(cid:104) (cid:88) (cid:105)
= E 1 u (34b)
u∼p0(u) M l
l=1
= E
(cid:104)
E
(cid:104)
1
(cid:88)M
u
(cid:12)
(cid:12)ub =ag v ,...,v
(cid:105)(cid:105)
(34c)
v1:M∼p0(v1:M) u∼p0(u|v1:M) M l(cid:12)
(cid:72)
1 M
(cid:73)
l=1
= E
(cid:104)
E
(cid:104)
1
(cid:88)M
v
(cid:12)
(cid:12)ub =ag v ,...,v
(cid:105)(cid:105)
(34d)
v1:M∼p0(v1:M) u∼p0(u|v1:M) M l(cid:12)
(cid:72)
1 M
(cid:73)
l=1
M
(cid:104) (cid:88) (cid:105)
= E 1 v (34e)
v1:M∼p0(v1:M) M l
l=1
= α. (34f)
The inequalities of (34) are justified as follows: (34a) and (34c) stem from the law of iterated expectations over all possible
bags of M items; (34b) arises from the fact that each item in the bag has an equal likelihood to be the realization of the m-th
variable v ; is again the law of iterated expectation with the reintroduction of the random vector; (34d) stems from the fact
m
that if two bags have the same items, their sum is identical; (34e) leverages the fact that the bag given its random variables is a
deterministically specified; and lastly, (34f) is by the assumption in Lemma 2. This concludes the proof of Lemma 2.
APPENDIXC
PROOFOFTHEOREM1
To prove Theorem 1, let us introduced an augmented data set, such that the last, (K+1)-th, fold Dte =D is composed
K+1
of N/K arbitrary test points
D˜ =(cid:8) D ,D ,...,D ,D (cid:9) (35)
1 2 K K+1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
=D =Dte
with the test point (x,y) included as the first point in the test set, i.e., (x,y) = (xte[1],yte[1]) = (x [1],y [1]). By
K+1 K+1
construction, all N +N/K points in the augmented data set D˜ are exchangeable and distributed according to joint distribution
p (D˜)=p (D,Dte). We denote the elements of the augmented set D˜ in (35) as
0 0
(x˜ [j],y˜ [j]) = (x [j],y [j]) for k ∈{1,...K} (36a)
k k k k
(x˜ [j],y˜ [j]) = (xte[j],yte[j]). (36b)
K+1 K+1
Note that the augmented set D˜ in (35) is different than the augmented set using dummy points Daug (16). For a pair of folds
indices k′,k ∈{1,...,K+1} with k ̸=k′, we also define the augmented leave-two-folds-out (L2O) set as the augmented set
without the two indexed folds, i.e.,
D˜ =D˜ \{D ,D }. (37)
−(k′,k) k′ k
As a special case, when one of the indices points to the (K +1)-th fold, which is the test fold, the L2O reduces to the
leave-one-out of the available data set D˜ =D . For every fold within the augmented data set D˜ (35), we evaluate
−(K+1,k) −k
the average L2O loss (37), minimized over the second fold index as
K+1 N/K
RˆCV (λ|D˜)= 1 (cid:88) K (cid:88) min (cid:110) ℓ(cid:0) y˜ [j],Γ (x˜ [j]|D˜ )(cid:1)(cid:111) . (38)
L2O K+1 N k λ k −(k,k′)
k′∈{1,...,K+1}\{k}
k=1 j=1
Finally, we define the L2O threshold as the minimal threshold value for which the estimated average L2O risk (38) is no larger
than α, i.e.,
(cid:110) (cid:12) (cid:111)
λCV (D˜)=inf λ(cid:12)RˆCV (λ|D˜)≤α . (39)
L2O (cid:12) L2O
λ
8Corollary 4. The L2O threshold λCV (D˜) in (39) is fold-permutation-invariant, i.e., for any of the (K +1)! possible fold-
L2O
permutation mappings π, we have
λCV (cid:0) D˜(cid:1) =λCV (cid:0) {D˜ }K+1(cid:1) =λCV (cid:0) {D˜ }K+1(cid:1) . (40)
L2O L2O k k=1 L2O π[k] k=1
This is due to the commutative property of the outer fold-summation and of the inner, within-fold, summation in (38).
Lemma 5. The L2O threshold λCV (D˜) in (39) lower bounds the K-CV-CRC threshold (17)
L2O
λCV (D˜)≤λCV(D). (41)
L2O
The proof of Lemma 5 is given in Appendix D.
We now define K+1 random variables v ,...,v , whose randomness stems from their dependence on the augmented
1 K+1
data set DDD˜ . Each k-th random variable v is the minimal leave-two-fold-out empirical risk averaged over the N/K examples
k
in the validation fold DDD , i.e.,
k
N/K
v = K (cid:88) min (cid:110) ℓ(cid:0) y˜ [j],Γ (x˜ [j]|DDD˜˜˜ )(cid:1)(cid:111) for k =1,...,K+1. (42)
k N k λCV(DDD˜˜˜) k −(k′,k)
k′∈{1,...,K+1}\{k} L2O
j=1
The random variables (cid:8) v ,...,v (cid:9) =(cid:8) v (λ,DDD˜˜˜),...,v (λ,DDD˜˜˜)(cid:9) are exchangeable for any fixed threshold due to the
1 K+1 1 K+1
exchangeability of the folds in the augmented data set. Therefore, by Lemma 2, we have the inequality
E (cid:2) v (λCV (DDD˜˜˜),DDD˜˜˜)(cid:3) ≤α. (43)
DDD˜˜˜∼p0(D˜) K+1 L2O
We are now ready to follow the steps
E
(cid:104) ℓ(cid:0) y,ΓCV(x|DDD)(cid:1)(cid:105)
DDD,x,y∼p0(D,x,y)
K
(cid:104) (cid:0) (cid:91) (cid:1)(cid:105)
= E ℓ y, Γ (x|DDD ) (44a)
DDD,x,y∼p0(D,x,y) λCV(DDD) −k′
k′=1
(cid:104) (cid:110) (cid:0) (cid:1)(cid:9)(cid:105)
≤ E min ℓ y,Γ (x|DDD ) (44b)
DDD,x,y∼p0(D,x,y)
k′∈{1,...,K}
λCV(DDD) −k′
= E (cid:104) min (cid:110) ℓ(cid:0) yte[1],Γ (xte[1]|DDD )(cid:1)(cid:9)(cid:105) (44c)
DDD,DDDte∼p0(D,Dte)
k′∈{1,...,K}
λCV(DDD) −k′
N/K
= E (cid:104) K (cid:88) min (cid:110) ℓ(cid:0) yte[j],Γ (xte[j]|DDD )(cid:1)(cid:111) (44d)
DDD,DDDte∼p0(D,Dte) N
k′∈{1,...,K}
λCV(DDD) −k′
j=1
N/K
= E (cid:104) min (cid:110) K (cid:88) ℓ(cid:0) y˜ [j],Γ (x˜ [j]|DDD˜˜˜ )(cid:1)(cid:111)(cid:105) (44e)
DDD˜˜˜∼p0(D˜)
k′∈{1,...,K}
N K+1 λCV(DDD) K+1 −(k′,K+1)
j=1
N/K
≤ E (cid:104) min (cid:110) K (cid:88) ℓ(cid:0) y˜ [j],Γ (x˜ [j]|DDD˜˜˜ )(cid:1)(cid:111)(cid:105) (44f)
DDD˜˜˜∼p0(D˜)
k′∈{1,...,K}
N K+1 λC L2V O(DDD˜˜˜) K+1 −(k′,K+1)
j=1
≤ α, (44g)
where (44a) is a consequence of (14), which is equivalent to ΓCV(x|D) = (cid:83)K Γ (x|D ); inequality (44b) is due to the
λ k=1 λ −k
nestingproperty(5)appliedonaparticularleft-fold-outk′ whichisasubsetoftheunionofallleft-fold-outsets;(44d)leverages
exchangeability as all test points have the same expected loss; (44e) uses the augmented data set notations (36b); inequality
(44f) is an outcome of the nesting properties (8) and (5) with inequality (41); in inequality (44g), we have used (43), alongside
Corollary 4, stating that the L2O threshold is fold-invariant. This completes the proof of Theorem 1.
9APPENDIXD
PROOFOFLEMMA5
The proof of Lemma 5 stated in Appendix C follows the steps
λCV (D˜) = inf(cid:40) λ(cid:12) (cid:12) (cid:12) 1 K (cid:88)+1 K N (cid:88)/K min (cid:110) ℓ(cid:0) y˜ [j],Γ (x˜ [j]|D˜ )(cid:1)(cid:111) ≤α(cid:41) (45a)
L2O (cid:12)K+1 N k λ k −(k,k′)
λ (cid:12) k′∈{1,...,K+1}\{k}
k=1 j=1
= inf(cid:40) λ(cid:12) (cid:12) (cid:12) 1 (cid:32) (cid:88)K K N (cid:88)/K min (cid:110) ℓ(cid:0) y˜ [j],Γ (x˜ [j]|D˜ )(cid:1)(cid:111) (45b)
(cid:12)K+1 N k λ k −(k,k′)
λ (cid:12) k′∈{1,...,K+1}\{k}
k=1 j=1
N/K (cid:33) (cid:41)
+K (cid:88) min (cid:110) ℓ(cid:0) y˜ [j],Γ (x˜ [j]|D˜ )(cid:1)(cid:111) ≤α
N K+1 λ K+1 −(K+1,k′)
k′∈{1,...,K}
j=1
≤ inf(cid:40) λ(cid:12) (cid:12) (cid:12) 1 (cid:16)(cid:88)K K N (cid:88)/K min (cid:110) ℓ(cid:0) y˜ [j],Γ (x˜ [j]|D˜ )(cid:1)(cid:111) + K N (cid:88)/K B(cid:17) ≤α(cid:41) (45c)
(cid:12)K+1 N k λ k −(k,k′) N
λ (cid:12) k′∈{1,...,K+1}\{k}
k=1 j=1 j=1
≤ inf(cid:40) λ(cid:12) (cid:12) (cid:12) 1 (cid:16)(cid:88)K K N (cid:88)/K ℓ(cid:0) y˜ [j],Γ (x˜ [j]|D˜ )(cid:1) +B(cid:17) ≤α(cid:41) (45d)
(cid:12)K+1 N k λ k −(k,K+1)
λ (cid:12)
k=1 j=1
= inf(cid:40) λ(cid:12) (cid:12) (cid:12) 1 (cid:16)(cid:88)K K N (cid:88)/K ℓ(cid:0) y [j],Γ (x [j]|D )(cid:1) +B(cid:17) ≤α(cid:41) (45e)
(cid:12)K+1 N k λ k −k
λ (cid:12)
k=1 j=1
= λCV(D), (45f)
where (45a) stems from the definition in (39); (45b) is obtained by decomposing the first sum into its first K summation terms,
and by listing the last term, the (K+1)-th, on its own; and (45f) follows the definition in (17). This completes the proof of
Lemma 5.
10