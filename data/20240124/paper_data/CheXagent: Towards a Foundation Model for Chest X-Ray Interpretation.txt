CheXagent: Towards a Foundation Model
for Chest X-Ray Interpretation
ZhihongChen1‚àó MayaVarma1‚àó Jean-BenoitDelbrouck1‚àó MagdaliniPaschali1
LouisBlankemeier1 DaveVanVeen1 JeyaMariaJoseValanarasu1 AlaaYoussef1
JosephPaulCohen1‚Ä† EduardoPontesReis1 EmilyB.Tsai1 AndrewJohnston1
CameronOlsen1 TanishqMathewAbraham2 SergiosGatidis1
AkshayS.Chaudhari1 CurtisLanglotz1
1StanfordUniversity 2StabilityAI
{zhihongc,mvarma2,jbdel,paschali,akshaysc,langlotz}@stanford.edu
Abstract
ChestX-rays(CXRs)arethemostfrequentlyperformedimagingtestinclinical
practice. Recentadvancesinthedevelopmentofvision-languagefoundationmod-
els(FMs)giverisetothepossibilityofperformingautomatedCXRinterpretation,
whichcanassistphysicianswithclinicaldecision-makingandimprovepatientout-
comes.However,developingFMsthatcanaccuratelyinterpretCXRsischallenging
due to the (1) limited availability of large-scale vision-language datasets in the
medicalimagedomain,(2)lackofvisionandlanguageencodersthatcancapture
thecomplexitiesofmedicaldata,and(3)absenceofevaluationframeworksfor
benchmarkingtheabilitiesofFMsonCXRinterpretation. Inthiswork,weaddress
thesechallengesbyfirstintroducingCheXinstruct-alarge-scaleinstruction-tuning
datasetcuratedfrom28publicly-availabledatasets. WethenpresentCheXagent-
aninstruction-tunedFMcapableofanalyzingandsummarizingCXRs. Tobuild
CheXagent,wedesignaclinicallargelanguagemodel(LLM)forparsingradiology
reports,avisionencoderforrepresentingCXRimages,andanetworktobridgethe
visionandlanguagemodalities. Finally,weintroduceCheXbench-anovelbench-
markdesignedtosystematicallyevaluateFMsacross8clinically-relevantCXR
interpretation tasks. Extensive quantitative evaluations and qualitative reviews
withfiveexpertradiologistsdemonstratethatCheXagentoutperformspreviously-
developedgeneral-andmedical-domainFMsonCheXbenchtasks. Furthermore,
inanefforttoimprovemodeltransparency,weperformafairnessevaluationacross
factors of sex, race and age to highlight potential performance disparities. Our
projectisathttps://stanford-aimi.github.io/chexagent.html.
Preprint.
4202
naJ
22
]VC.sc[
1v80221.1042:viXraCheXinstruct
Local Findings Generation
6 Million CXR - Text -
QA Triplets Q: Given the image(s), describe
"Mediastinal‚Äù.
A: The mediastinal contours are
CheXagent
notable only for tortuosity of
8 Billion Parameter the aorta.
Instruction-tuned CXR FM Open-ended VQA
Q: Where is the opacity located?
CheXbench
A: Right of the midline,
Benchmark over superior to the right hilum
8 tasks and 7 datasets
Figure1:Overviewoftheproposedpipeline:CheXinstructisacurationofdatasetsforinstruction-tuningacross
variousCXRtasks,CheXagentisourclinicalFMforCXRinterpretation,andCheXbenchisourcomprehensive
FMevaluationbenchmark.TwoexampleCXRinterpretationtasksincludelocalfindingsgenerationandopen-
endedvisualquestionanswering(VQA).
1 Introduction
Foundationmodels(FMs)haverecentlyemergedasapowerfulclassofmodelscapableofperforming
a diverse range of reasoning and comprehension tasks [9]. The rise of FMs presents a major
opportunitytore-imaginecomplexhealthcareworkflowsthatcommonlyrequireposingmulti-faceted
questions from inherently multi-modal data. One particular clinical workflow is the analysis of
medicalimagingdata.TakeforexamplechestX-ray(CXR)interpretation-themostcommonmedical
imagingstudy,with70+millionCXRsperformedannuallyintheUS[33].Here,radiologistsinterpret
hundredsofimagesdaily,translateimaginginsightsintotextualdescriptions,andsummarizeimage
findingssuccinctlytootherclinicians,whilemaximizingaccuracyandminimizingbias. Moreover,
patient-facingtasksforCXRsmightincludeansweringclarifyingquestionsaboutthesegenerated
findings.ACXRFMcapableofautomatingorincreasingtheefficiencyofthesetaskscansubstantially
improveclinicaldecision-makingaswellaspatientsatisfactionandoutcomes[67,75,96].
High-qualityCXRFMsmustbridgethegapbetweenvisionandlanguage. Mostpriorapproachesfor
buildingvision-languageFMsfocusonnaturalimagesettings,wherehigh-performingmethodsalign
imageencoderswithpretrainedlargelanguagemodels(LLMs). Suchinstruction-tunedmultimodal
LLMsdemonstratesuperiorcapabilitiesacrossarangeofperceptionandgenerationtasks. However,
developinginstruction-tunedmultimodalLLMsformedicalimagingischallengingforthefollowing
reasons:
1. Thereisalackofvision-languagemedicalimagingdatasets. Instruction-tunedmultimodal
LLMsrequirelarge-scale,diversetrainingdatasetswithdatatripletsconsistingofinstruc-
tions, images, and answers. Whereas existing approaches in the natural image domain
leveragemillionsoftrainingsamplesfromdatasetslikeLAION-5B[66],theavailability
ofsuchdataisseverelylimitedinthemedicaldomainduetopatientprivacyconcernsthat
preventdisseminationoflargecorporaofmedicaltext. Asaresult, existinginstruction-
tunedmultimodalLLMsdevelopedforCXRinterpretationaretrainedonsmalldatasets
withlimitedinstructiondiversity[74,89].
2. Existing vision and language encoders fail to capture the complexities of medical data.
MultimodalLLMsrequirepowerfulimageandtextencoders. However,CXRsrepresenta
largedomainshiftfromnaturalimagesandtext,andthus,existingpretrainedvisionand
languageencodersstrugglewithmedicalknowledgegrounding.
3. PerformingrigorousevaluationsofmultimodalLLMsinmedicineischallenging. Multi-
modalLLMsgenerateopen-endedresponses,andevaluatingfree-textresponsesforfactual
correctnessandcompletenessisarduous,particularlyforlarge-scaleevaluationsthatrequire
domainexpertise. PreviousFMsdevelopedforCXRinterpretationareprimarilyevaluated
usingvisual-question-answer(VQA)ortextgenerationtasks. Tothebestofourknowledge,
*Equalcontributions.
‚Ä†WorknotrelatedtopositionatAmazon.
2therearenoexistingbenchmarksforquantitativeandqualitativeevaluationofmultimodal
LLMsacrossdiverse,clinically-relevantCXRinterpretationtasks.
Inthiswork,weaddressthesechallengesby(i)introducingCheXinstruct,alarge-scaleinstruction-
tuningdatasetwithinstruction-image-answertriplets,(ii)developingCheXagent,aninstruction-tuned
multimodalLLMforCXRinterpretation,and(iii)curatingCheXbench,anovelbenchmarktoenable
systematiccomparisonsofFMsacross8clinically-relevantCXRinterpretationtasks. Belowwe
outlineourkeycontributions,alsosummarizedinFig.1,thatcanhelpcreatecapableandrobustCXR
FMs:
1. CheXinstruct is an instruction-tuning dataset with 6M instruction-image-answer triplets
designedtoimprovetheabilityofFMstointerpretCXRs. Wecollectinstructionsfrom34
tasksand65uniquedatasets,spanningcategoriesincludingcoarse-andfine-grainedimage
understanding,questionanswering,andtextgeneration.
2. CheXagentisaninstruction-tunedfoundationmodelwith8Bparameterscapableofanalyz-
ingimages,understandingtext,andgeneratingresponses. Ourmethodologyfordeveloping
CheXagentincludestraining(1)aclinicalLLMcapableofunderstandingradiologyreports,
(2)avisionencodercapableofreadingCXRs,and(3)anetworktobridgethevisionand
languagemodalities. Wethenperforminstruction-tuningusingdatafromCheXinstruct.
3. CheXbenchisanovelbenchmarkdesignedtorigorouslyevaluateFMsacrosstwoevaluation
axes: image perception and textual understanding. We introduce 8 tasks across 7 CXR
datasets, andweevaluateperformanceusingclose-endedmultiple-choicepredictionsas
wellasopen-endedtextgeneration.
WeuseCheXbenchtocompareCheXagentwithsixpriorgeneral-domainandmedical-domainFMs.
Acrosssixvisualtasks,theperformanceofCheXagentsurpassesgeneral-domainFMsby97.5%and
medical-domainFMsby55.7%. Acrosstwotextgenerationtasks,CheXagentprovidesmedicaltext
evaluatedviaautomatedquantitativemetricsandqualitativemetricsfromfiveexpertradiologists. We
furtherprovideanevaluationofpotentialmodelbiasandhighlightperformancedisparitiesacross
demographicfactorsofsex,raceandagetoimprovemodeltransparency.
2 RelatedWork
2.1 FoundationModels
ThesurgeinavailabledataandcomputationalresourceshasenabledthecreationofFMsthatare
versatileinaddressingawidearrayoftaskswithasinglegeneralistmodel.
Language: SignificantstridesinFMswerefirstseeninnaturallanguageprocessing(NLP)because
large-scaletextdatawaseasilyaccessibleonline. LLMslikeGPT-3[10], ChatGPT,GPT-4[54],
FLAN-T5[18],Llama-2[76,77],Mistral7B[35],andPaLM-2[17,3]excelatmultipletext-based
tasksusingonlyprompting,enablingnewpossibilities,suchasoutperforminghumanexpertsfor
clinicaltextsummarization[81].
Vision: Invision,FMslikeStableDiffusion[63],DALL-E[61,60],andImagen[64]wereproposed
forthetaskoftext-to-imagegeneration. ModelsLikeSegmentAnythingModel(SAM)[38]and
SegmentEverythingEverywhereModel(SEEM)[98]havebeendevelopedtoperformin-the-wild
segmentation. ToachieveLLM-likescalinginvision,ViT-22B[19],ascaled-upversionofViT[23]
wasintroducedasalargevisionencoder. Inthemedicaldomain,thereareseveraldatasetsdesigned
fortrainingFMs[49].
Vision-Language: Giventheinherentconnectionbetweendifferentmodalitiessuchaslanguage
andvision,inter-modalitysupervisionhasbeencriticalinthedevelopmentofmanyvision-language
foundation models (VLMs), like CLIP [59]. Several other large-scale VLMs like Flamingo [1],
Coca [94], Qwen-VL [5], BLIP [44], LLaVA [47], PaLI-X [14], and CogVLM [83] have been
introducedwithafocusondevelopingmodelspowerfulenoughtoperceiveandunderstandboth
textandimages. Toevaluatetheperformanceofthesemodels,benchmarkslikeMultimodalLarge
Language Model Evaluation (MME) [25] and SEED-Bench [42] have been introduced. These
benchmarksarecarefullycuratedusingmanuallygeneratedinstruction-answerpairstoavoidany
dataleakage.
3Stage0:TrainaclinicalLLM
TextCorpus:
üî• NextWord
Text LLM
Prediction
Stage1:Trainavisionencoder
üî• Image
Text Qformer
Captioning
Vision üî• Image-Text
Image
Encoder Contrastive
Stage2:Train a vision-language bridger
üî•
Qformer Bridger
Vision Image
Image LLM
Encoder Captioning
Stage3:InstructionTuning
üî• üî•
Qformer Bridger
Vision üî• NextWord
Image LLM
Encoder Prediction
:Frozenüî•:Trainable
(a)OverviewofCheXinstruct (b)TrainingofCheXagent
Figure2:CollectionofdatasetsandtaskscomprisingCheXinstruct(Left).Thefour-stagetrainingprocessof
CheXagent,startingfromadaptingageneralLLMforclinicaluse,throughtrainingaCXRvisionencoderanda
vision-languagebridger,tothefinalstageofinstructiontuningondiverseCXRtasks(Right).
2.2 MedicalFoundationModels
Biomedicallanguagemodels(LMs)likeBioNLP[41],BioBERT[40],PubMedBERT[29],BioGPT
[48], Med-PaLM[40]andMed-PaLM2[71]haveshownthatLMscanbefine-tunedoncurated
biomedical corpora to perform competitively on medical question answering tasks. Specifically,
Med-PaLM 2 [71] achieved high scores in questions in the style of the US Medical Licensing
Examination(USMLE)highlightingrapidprogresstowardsphysician-levelperformanceformedical
question answering. Since medicine inherently involves multiple modalities and covers a broad
range of tasks, multi-modal FMs are particularly suitable for this field. Works like LLaVA-Med
[43],Med-Flamingo[51],andMed-PaLMM[78]haveproposedgeneralistmodelsspanningacross
modalitieslikeimagingandclinicaltextfortasksinradiology,dermatology,pathology,etc. RadFM
[88]introducedafoundationmodelforradiologyforboth2Dand3Dimagingtasks. XrayGPT[74]
alignedaMedCLIP[85]visualencoderwithVicuna[16],whichwasfinetunedusingclinicaltext.
RoentGen,afine-tunedversionofStableDiffusion,allowsgeneratingCXRimagesusingradiology
reporttextprompts[13,12].
Med-PaLMM,Med-Flamingo,RadFM,andLLaVA-Medaimtoestablishgeneralistframeworksthat
empowermodelstoprocessdatafromvariousimagemodalities. Incontrast,CheXagentisdesigned
toexcelinhandlingmultipletasksrelatedtoCXRswithinasinglemodel.
2.3 MostRelatedWork
Our approach is most similar to the following works. (i) BLIP-2 [44]: our model architecture is
closelyalignedwiththatofBLIP-2,whichusesaQueryingTransformer(QFormer)tobridgethe
vision-languagemodalitygap;(ii)FLAN[86]andMultiInstruct[90]: ourdevelopmentofinstruction
tuningdatasetsderivedfromexistingannotateddatasetsisinspiredbytheseworks.
3 CheXinstruct: Instruction-TuningDataset
Motivation CheXinstructseekstocoverabroadrangeoftasksshowninFig.2atosupporttraining
CXRFMs. Thesetaskscaneither(i)improvetheabilitiesofFMstounderstandCXRsor(ii)improve
clinicaldecisionmaking.
DesignScheme CheXinstructisorganizedintofourlevels:
4Table1:ThestatisticsofCheXinstruct,wherethetotalanduniquenumbersofquestionsandimagesareshown
alongwiththeaveragenumberoftimeseachuniqueimageisreusedinthedataset(reuse).
Questions Images
Split
Total Unique Reuse Total Unique Reuse
Train 6.1M 1.9M 3.2 9.3M 1.1M 8.6
Val 203.3K 82.9K 2.5 228.9K 31.7K 7.2
Test 188.3K 72.8K 2.6 233.7K 49.5K 4.7
1. CapabilityLevel,wherewespecifytheessentialskillsandcompetenciesthataCXRFM
shouldpossess.
2. TaskLevel,whereweoutlineawidearrayofspecifictasksthatalignwitheachidentified
capability,providingaclearframeworkforwhattheFMshouldbeabletoaccomplishinthe
contextofCXRs.
3. DatasetLevel, where we identify and categorize various CXR datasets, each associated
withaparticulartask. Wefocusonensuringthatthedatasetsarerelevantandappropriately
matchedtothetaskstheyareintendedtosupport.
4. InstanceLevel,wherewedefineindividualinstanceswithineachdatasetatthemostgranular
level. Eachinstancecomprisesaninput(suchasaCXRimage)anditscorrespondinglabels,
formingthebasicunitsfortrainingandevaluatingtheFM.
Tasks CheXinstructconsistsoffivetaskcategoriesaccordingtotheircapabilities:
‚Ä¢ Coarse-grainedImageUnderstanding, which defines the overall understanding of CXRs,
e.g.,viewclassification[36],anddiseaseclassification[84,32,62,55,30,6,34,11,69].
‚Ä¢ Fine-grainedImageUnderstanding, which defines the localized understanding of CXRs,
e.g.,abnormalitydetection[53,58],abnormalitygrounding[8],andforeignobjectdetec-
tion[91].
‚Ä¢ QuestionAnswering,whichdefinestheabilitytorespondtoaquestionrelatedtoCXRs,e.g.,
close-endedvisualquestionanswering(VQA)[97,57],open-endedVQA[7,4],difference
VQA[31],andtextQA.
‚Ä¢ TextGeneration,whichdefinestheabilitytogenerateradiologyreportsections,includinga
descriptionofthefindings [21,82,56],impressiongeneration[24],findingssummariza-
tion[15],andlocalfindingsgeneration[36].
‚Ä¢ Miscellaneous: ThiscategorydefinesthemiscellaneousabilitiesthatarecriticalforaCXR
FM,e.g.,reportevaluation[93,50],andnaturallanguageexplanation[37].1
DatasetSources Withtheaforementionedtaxonomyoftasks,wecreatedCheXinstructbyeither(i)
collectingexistingpubliclyavailabledatasetsor(ii)curatingadatasetwithnewlabelsfromexisting
datasets.
Foreverydataset,wederiverelevantinformationtoformulatedistincttasks. Allthedatasetsaresplit
followingtheircorrespondingofficialsplitsifapplicable.2
Task Instruction Creation Inspired by [86], we create instructions by manually writing ten in-
structiontemplatesforeachtask.3 Eachinstructiontemplatecontainsplaceholders(e.g.,<IMAGE>,
<QUESTION>,and<OPTIONS>),whicharereplacedwithspecificvalueswhencreatinginstruction-
followinginstances.
Finally, each CheXinstruct instance is a triplet consisting of an image (or none, in the case of
non-image-basedtasks),aquestion,andananswer.
DataAnalysis WedescribetheoverallstatisticsofCheXinstruct,includingthenumberofquestions-
1AdditionaldetailsaboutthetasksarepresentedinAppendixA.
2MoredetailsofthedatasetsourcesarepresentedinAppendixB.
3Theannotatorsareresearchersinradiologyandcomputervision(CV).
5answer pairs and CXRs in Table 1. There are 6.1M question-answer pairs in CheXinstruct. Fur-
thermore,sinceimagesfromonedatasetmaybeusedacrossmultipletasks,wepresentadditional
informationtoprovideintuitionaboutthepotential‚Äúoverlap‚Äùproblemamongdifferentdatasets.4
4 CheXagent: Instruction-FollowingCXRFM
4.1 ProblemSetup
The aim of CheXagent is a model that can ‚Äúsee‚Äù images x and/or ‚Äúread‚Äù text x and generate
I T
‚Äúresponses‚Äùy. Tothisend,weintroducethreecomponentsofourmodel: avisionencoderM ,a
v
vision-languagebridgerM ,andalanguagedecoderM . Therefore,theultimateformulationofthe
b l
desiredmodelis:
y =M (M (M (x )),x ). (1)
l b v I T
4.2 TrainingCheXagent
Inthissubsection,wepresentthefourtrainingstagesofCheXagent,whichareillustratedinFigure2b.
Stage0: TrainaclinicalLLM Numerousopen-sourcebiomedicallargelanguagemodels(LLMs)
exist,suchasBioMegatron[70],GatorTron[92],BioGPT[48],BioMedLM5,andPMC-LLaMA[87].
ThesemodelsarepredominantlytrainedonPubMedCentral(PMC)articlesratherthanclinicaltexts.
Toaddressthisgap,wefocusondevelopingaclinicalLLMbyadaptingageneral-domainLLM.
OurstartingpointisMistral-7B-v0.1[35],chosenforitsdemonstratedstrongreasoningcapabilities
acrossvariousbenchmarks.
Toinfusethemodelwithcomprehensivemedicalandclinicalknowledge,weutilizefivedistincttext
sourcesfortraining: (i)PMCarticleabstracts,(ii)radiologyreportsfromMIMIC-IV,(iii)MIMIC-
IV discharge summaries, (iv) medical terms from Wikipedia, and (v) CXRs from CheXinstruct.
Importantly,forMIMIC-IVdata,wemeticulouslyexcludeanystudiesthatarepartofthevalidation
andtestsetsofMIMIC-CXRtopreventdataleakage.
Stage1: TrainavisionencoderforCXR Drawinginspirationfromtheworksof[45]and[94],
we train our vision encoder using a variety of visual pre-training objectives, namely image-text
contrastive(ITC)andimagecaptioning(IC).Ourmodelarchitecturemirrorsthatof[45]. Fortraining
purposes,weutilizedatasetscomprisingimage-textpairs,specificallyfromMIMIC-CXR,PadChest,
andBIMCV-COVID-19. PreliminarystudiesindicatethatemployingacombinationofITCandIC,
akintotheapproachin[94],yieldsenhancedperformanceinourspecificcontext.
Stage2: Trainavision-languagebridger FollowingthetrainingoftheclinicalLLMandtheCXR
visionencoder,wefocusondevelopingabridgermodel,M . Thismodelisdesignedtomapvisual
b
datatothecorrespondinglanguage(semantic)space. Duringtraining,wekeepboththeclinicalLLM,
M ,andtheCXRvisionencoder,M frozen. Thisapproachiscrucialforpreventingcatastrophic
l v
forgettingofpriorknowledgeduringtheimage-textalignmentprocess. FortrainingM ,weemploy
b
thesamedatasetsasinStage1,applyinganimagecaptioningobjectiveforlearning.
Stage3: InstructiontuningUponcompletingStage2,weobtainamulti-modalLLMtailoredfor
CXRinterpretation. Inthisstage,ourfocusshiftstotrainingthemodelonavarietyoftaskswithin
theCheXinstructframework. Priortotraining,weconsidertwokeyaspects: (i)reservingcertain
task-datasetpairsexclusivelyforevaluationpurposes,and(ii)determiningoptimaldatasetratiosto
ensurebalancedtrainingacrossdifferentcapabilities. Forthefirst,wesequesterdatasetsincluding
OpenI,SLAKE,andSIIM,whichfacilitatesamorestreamlinedevaluationprocess(asdiscussedin
¬ß5). Forthesecond,weheuristicallyestablishdatasetratiosbycarefullyassessingthequalityand
diversityofeachdataset. Thismethodleavesroomforfutureexplorationintoautomateddataset
selectionandbalancing.6 Thistrainingisconductedusinganext-wordpredictionobjective,withthe
losscomputationbeinglimitedtoanswers.
ImplementationDetails Formodelarchitecture,weuseEVA-CLIP-g[73]forthevisionencoderand
BERT[22]fortheQformer,alinearlayerforthebridger,andMistralfortheLLM.Theoptimization
4MoredetailsaboutthedataanalysisarereportedinAppendixC.
5https://huggingface.co/stanford-crfm/BioMedLM
6MoredetailsondatasetratioscanbefoundinAppendixD.
6Table2: ResultsofEvaluationAxis1ofCheXbenchfortasksassociatedwithimageperceptioncomparing
CheXagentwithgeneraldomainandmedicaldomainFMsonseveralCXRdatasets.Foreachtask,wereport
accuracy.
General-domainFMs Medical-domainFMs CheXagent
Task Dataset
BLIP-2 InstructBLIP XrayGPT MedFlamingo RadFM LLaVA-Med (Ours)
MIMIC-CXR 28.8 25.3 24.0 25.0 28.5 23.8 97.5
ViewClassification
CheXpert 38.0 34.0 33.0 39.0 37.0 30.0 96.7
SIIM 53.0 54.0 50.0 50.0 50.0 49.0 64.0
BinaryDiseaseClassification RSNA 50.0 60.0 50.0 50.0 50.0 44.0 81.0
CheXpert 51.5 53.2 51.5 48.5 55.8 47.6 76.0
OpenI 40.2 40.2 45.4 39.0 42.2 43.8 47.0
SingleDiseaseIdentification MIMIC-CXR 25.6 22.6 24.1 25.6 27.2 26.7 30.3
CheXpert 21.3 19.5 23.7 26.0 26.6 26.0 29.6
OpenI 48.5 54.4 57.7 46.1 52.8 53.9 55.6
MultiDiseaseIdentification MIMIC-CXR 30.0 25.3 39.0 14.7 22.3 28.7 55.3
CheXpert 4.3 6.1 3.9 7.1 23.6 2.1 52.1
Rad-Restruct 41.2 42.4 38.6 45.5 48.5 34.9 57.1
VisualQuestionAnswering
SLAKE 74.3 86.4 52.4 64.8 85.0 55.5 78.1
Image-TextReasoning OpenI 47.9 52.6 52.4 54.7 54.0 45.8 59.0
oftrainableparametersisstructuredacrossvariousstages: (a)theentireLLMistrainedinStage0;
(b)inStage1,wetrainboththeLoRAparametersofthevisionencoderandtheentireBERTencoder;
(c)Stage2involvestrainingthevision-languagebridger;and(d)inStage3,wefocusontraining
boththebridgerandtheentireLLM.Foroptimizationacrossallthesestages,weemploytheAdamW
optimizer,witheachstagehavingitsownhyper-parameters7.
5 CheXbench: ABenchmarkforEvaluatingFMsonCXRInterpretation
Inthissection,weintroduceCheXbench,anevaluationbenchmarkforenablingsystematiccompar-
isonsofFMsacross8clinically-relevantCXRinterpretationtasks.
5.1 BenchmarkDesign
CheXbenchisstructuredwithtwoevaluationaxes,craftedtoassesscrucialaspectsofCXRinterpreta-
tion: imageperceptionandtextualunderstanding. TheevaluationswithinCheXbenchareconducted
onaspecificsubsetoftheCheXinstructtestset.8 Inthefollowingsections,wedescribethetasks
associatedwitheachevaluationaxis.
EvaluationAxis1-ImagePerception: WefirstaimtoevaluatetheabilityofFMstounderstandthe
visualcontentofCXRs. Ourgoalsareto(1)evaluatetheabilityofFMstogeneralizetoavarietyof
datadistributionsand(2)includearangeofchallenging,clinically-relevanttasks. Tothisend,we
introduce6tasksacross7datasets;inparticular,wenotethat3datasets(SIIM,SLAKE,andOpenI)
werecompletelyheld-outfromCheXagenttrainingtoavoidanypotentialdataleakage. Inlinewith
priorbenchmarksdesignedforgeneraldomainFMs[42,26],weuseamultiple-choiceformat,where
animageandaquestionareposedtotheFMandmultipleoptionsareconsidered. Sinceopen-ended,
free-textoutputsfromFMsarechallengingtoevaluate,weinsteadcomputelog-likelihoodscores
associatedwitheachoption;theoptionwiththehighestscoreisthenselectedastheresponse. For
eachtask,wereportaccuracy.
‚Ä¢ ViewClassification(700samples): Given a CXR, the FM is tasked with identifying the
imagingview. WeperformviewclassificationontheCheXperttestsetwiththreepossible
options(AP,PA,andLateral)aswellastheMIMIC-CXRtestsetwithfourpossibleoptions
(AP,PA,Lateral,andLL).
‚Ä¢ BinaryDiseaseClassification(433samples): GivenaCXRandadiseaselabel,theFMis
taskedwithdeterminingifthediseaseispresentintheimage. Weperformbinarydisease
classificationwithtwelvediseaselabelsintheCheXperttestset,onediseaselabelinthe
RSNAdataset,andonediseaselabelintheSIIMdataset. Therearetwopossibleoptions
(yesandno).
7MoredetailsontheimplementationcanbefoundinAppendixE.
8ThisiscomprehensivelydetailedinAppendixF.
7Table3: Resultsofevaluationaxis2ofCheXbenchforthetaskoffindings Table 4: Results of eval.
generationcomparingCheXagentwithbaselinemedical-domainFMsusing axis2onfindingssumma-
variousmetrics. rization.
PrivateDataset MIMIC-CXR MIMIC-CXR
Model Size Model Size
BERT-S CheXbert-S RadGraph-S BERT-S CheXbert-S RadGraph-S Rouge-L
MedFlamingo 8B 8.5 2.7 1.7 10.4 3.2 2.2 Llama-2 7B 20.3
LLaVA-Med 8B 12.5 17.0 4.2 6.2 17.5 4.0 Vicuna 7B 21.5
RadFM 14B 35.7 12.7 5.1 45.7 17.5 10.9 FLAN-T5 11B 42.5
XrayGPT 8B 40.1 23.4 9.0 44.0 24.2 11.2 FLAN-UL2 20B 42.1
CheXagent 8B 46.6 23.7 14.6 50.4 24.9 18.6 CheXagent 8B 40.3
‚Ä¢ SingleDiseaseIdentification(864samples): GivenaCXR,theFMistaskedwithidentify-
ingthediseasepresentintheimage. Weimplementsinglediseaseidentificationwiththe
CheXperttestset(wherediseaselabelsareobtainedfromexpertradiologistannotations)and
OpenI(wherediseaselabelsareobtainedfromMedicalSubjectHeadings(MeSH)codes).
Therearefourpossibleoptionsassociatedwitheachquestion,andeachoptionincludesa
singlediseaselabel(e.g.‚Äúpneumonia").
‚Ä¢ Multi-DiseaseIdentification(1387samples): GivenaCXR,theFMistaskedwithidentify-
ingasetofmultiplediseasespresentintheimage.Weimplementmulti-diseaseclassification
usingCheXpertandOpenI.Therearefourpossibleoptionsassociatedwitheachquestion,
and each option includes a set of multiple diseases (e.g. ‚Äúpneumonia, pleural effusion,
cardiomegaly").
‚Ä¢ Visual-Question-Answering(1319samples): WeevaluateFMsacrosstwostandardVQA
benchmarks: SLAKEandRad-Restruct. SLAKEconsistsofquestionswithtwooptions
(yesandno),andRad-Restructconsistsofquestionswithbetweentwoandfouroptions.
‚Ä¢ Image-TextReasoning(380samples): GivenaCXR,theFMistaskedwithidentifyingthe
diseaseintheimage. Incontrasttothesingle-diseaseclassificationtask,thistaskemploys
hard negatives; each question is associated with two challenging options, distinguished
onlybyasinglewordindicatinglocationorseverity(e.g.‚Äúleft-sidedpleuraleffusion"vs.
‚Äúright-sidedpleuraleffusion"). Weimplementimage-textreasoningwiththeOpenIdataset.
EvaluationAxis2-TextualUnderstanding: WeadditionallyevaluatetheabilityofCheXagentand
baselineFMstogenerateandsummarizetext. Tothisend,weintroducethefollowing2tasks: we
evaluateopen-endedresponsesusingacombinationofautomatedmetrics(ROUGE-L[46],CheXbert-
Score[72],BERT-Score[95],RadGraph-Score[20],andGPT-4)andhumanexpertevaluationsfrom
fiveradiologists.
‚Ä¢ FindingsSectionGeneration:Givenanimage,theFMistaskedwithgeneratingthefindings
sectionoftheradiologyreport. Thistaskinvolvesidentifyingkeyfeaturesoftheimage,
suchasthepresenceofabnormalities. Weimplementthefindingssectiongenerationtask
withMIMIC-CXR.SinceexistingmedicalFMsareoftentrainedonMIMIC-CXR,wealso
evaluatethemodelsonaprivatedataset.
‚Ä¢ FindingsSummarization: Giventhefindingssectionofaradiologyreport,theFMistasked
withsummarizingthekeyobservationsintoaconcisestatement. Thistaskdoesnotinclude
images. WeevaluateFindingsSummarizationonMIMIC-CXR.
5.2 EvaluationResults
Inourstudy,weemployCheXbenchtocompareCheXagentagainsttwogeneral-domaininstruction-
tuned FMs, InstructBLIP and BLIP2, which achieve state-of-the-art performance in previous re-
search[42]. Additionally,wecompareCheXagentwithfourmedicalFMs: XrayGPT,MedFlamingo,
RadFM, and LLaVA-Med [74, 51, 43, 88]. This comparison aims to provide a comprehensive
understandingofCheXagent‚Äôsperformanceinrelationtobothgeneralandmedical-specificmodels.
Table2providesresultsonthesixtasksassociatedwithevaluationaxis1. CheXagentdemonstrates
superiorperformanceacrossimageperceptiontasks,achievinganaverageimprovementof97.5%
overgeneral-domainFMsandanaverageimprovementof55.7%overmedicalFMs. Weprovidea
detailedbreakdownofCheXagentperformance:
859% 66% 75% 96%
1%
2%
28% 16% 16%
13% 18% 8%
Figure3:GPT-4evaluationsdemonstratethatthereportsgeneratedbyCheXagentoutperformmedical-domain
FMsforthefindingsgenerationtaskonMIMIC-CXR.
‚Ä¢ On view classification, CheXagent achieves near perfect performance, demonstrating a
68.7point(238%)improvementovertheclosestbaselineonMIMIC-CXRanda57.7point
(148%)improvementovertheclosestbaselineonCheXpert.Themajorityofgeneral-domain
andmedical-domainFMsdemonstrateperformancenearrandom.
‚Ä¢ Onbinarydiseaseclassification,singledisease,andmulti-diseaseidentification,CheXagent
demonstratesanaverageimprovementof11.6pointsovertheclosestbaseline. Inparticular,
wenotethatCheXagentdemonstratessuperiorperformanceontheSIIMandOpenIdatasets,
whichwerecompletelyheld-outfromtrainingCheXagent;thissuggeststheabilityofour
modeltogeneralizetodiverseCXRs. Onmulti-diseaseclassificationonOpenI,XrayGPT
outperformsCheXagentby1.1points;thiscouldbeattributedtothefactthatXrayGPTwas
fine-tunedonsamplesfromOpenI,whichwouldbiasitsclassificationaccuracyonthistask.
‚Ä¢ Onvisualquestionanswering,CheXagentoutperformsbaselinesonRad-Restruct(achiev-
ing a 8.6 point improvement over the closest baseline) and is competitive with existing
approaches on SLAKE. We note that the SLAKE and Rad-Restruct datasets were com-
pletelyheld-outfromCheXagenttraining,whereasRadFMwastrainedusingsamplesfrom
SLAKE.
‚Ä¢ Onimage-textreasoning,CheXagentachievesa4.3point(7.8%)improvementoverthe
closestbaseline,MedFlamingo. Weobservethattheimage-textreasoningtask,whichwas
explicitlydesignedtoincludehardnegatives,isparticularlychallengingforallevaluated
FMs.
Tables3, 4andFigure3provideresultsonthetwotasksassociatedwithevaluationaxis2,medical
textgenerationandsummarization. WeprovideadetailedbreakdownofCheXagentperformance
below:
‚Ä¢ Onfindingssectiongeneration,CheXagentoutperformsallmedicalFMsacrossallmetrics
onboththeprivatedatasetandMIMIC-CXR.Inparticular,CheXagentachievesanaverage
improvmentof6.5pointsonRadGraphscoresand0.5pointsonCheXbertscores;thisis
notable since these metrics directly evaluate factual correctness. Figure 3 shows results
fromautomatedevaluationsusingGPT-4. For152randomlyselectedsamplesfromour
privatedataset,weprovidedGPT-4withareferencereport,thefindingssectiongenerated
byCheXagent, andthefindingssectiongeneratedbyeachofthemedicalFMbaselines;
GPT-4wasthenpromptedtoselectthereportwiththehighestaccuracy. AsshowninFigure
3,ourGPT-4evaluationsdemonstratethatCheXagentgenerateshigh-qualityreportswhen
comparedtoothermedicalFMs.
‚Ä¢ Forfindingssummarization,asshowninTable4,CheXagentoutperformsLLMsofcompa-
rablesizeontheRouge-LmetricandachievescomparableperformancetoLLMswithmore
thantwicethenumberofparameters[80,81].
6 HumanEvaluation
TocomplementthequantitativeresultspresentedinSection5.2,weconductareaderstudyinwhich
five radiologists compare text generated by CheXagent against text written by a physician. This
studyincludesthetwotextualunderstandingtasksdescribedinSection5.1: findingsgenerationand
9Which radiology report findings section...
[Completeness] ... more completely captures important information?
[Correctness] ... includes less false information?
[Conciseness] ... contains less non-important information?
Physician Physician CheXagent CheXagent
significantly slightly neither slightly significantly
-10 -5 0 5 10
Figure4:Readerstudywithfiveradiologists.Top:StudydesigncomparingthefindingssectionsofCheXagent
vs.thatofhumanexpertsfortwotasks(rows)acrossthreeattributes(columns).Bottom:Results.Compared
tohumans,CheXagentachievesparityinreportsummarization,whiletheresultsdemonstrateagapbetween
CheXagentreportgenerationandhuman-levelexpertise.
CheXagent: the right-sided chest tube has been removed. there
is no evidence of pneumothorax. there is a small right pleural
effusion. bibasilar atelectasis is present. there is no
pulmonary edema. the heart size is normal. the mediastinal
contours are normal. the hilar contours are normal. there is no
pneumothorax.
Physician: right-sided chest tube remains in place, with slight
increase in size of a small right pleural effusion, but no
visible pneumothorax. bibasilar linear atelectasis has slightly
worsened, and there is a persistent small left pleural
effusion.
Color key: Correct Error Refers to prior study
Figure5:ComparisonofCheXagent(top)againstphysician(bottom)onreportgeneration.Radiologistreports
oftenrefertopaststudies(purple),acontextnotavailabletoCheXagent.Thepresenceofthesereferencesinthe
trainingsetperhapscontributestoCheXagent‚Äôserror(red)inanotherwiseaccuratereport(green),motivating
futureworkforvision-languagemodels.
findingssummarizationonMIMIC-CXR.Forbothtasks,eachradiologistviewsthesamesetof20
randomlyselectedsamples,whichcontainthetaskinputaswellasanA/Bcomparison(CheXagent
vs. physician)ofthetaskoutputs. Radiologiststhengradetheseoutputsusingafive-pointLikert
scaleacross: completeness,correctness,andconciseness.
StudyresultsinFigure4showthatCheXagentiscomparabletophysiciansforfindingssummarization
andthattextfromhumanexpertsachieveshigherscoresintermsoffindingsgeneration. Although
CheXagentoutperformsothervision-languagemodelsinquantitativemetrics,thesefindingsindicate
thatfurtherimprovementcouldfacilitateclosingthegapbetweenCheXagentandhumanradiologists.
Wenowdiscussqualitativefindingstobetterunderstandtheopportunityforfutureimprovement
in vision-language models. As shown in Figure 5, physicians‚Äô reports typically reference past
patientstudiestotrackchangesovertime. Thus,thecapabilitiesofCheXagent,whichistrainedon
cross-sectionalreports,couldbefurtherimprovedbyincorporatinglongitudinaldataduringtraining.
Wealsoobservedthatthemodelfrequentlyproducedauniformdistancemeasurementof3.5cmin
itsgeneratedtext,irrespectiveofcontext. ThisissuearisesbecauseCheXagentisnotdesignedto
directlyestimatephysicaldistancesorotherquantitiesfromimages. Thus,addressingthislimitation
representsyetanotheravenueforadvancementinvision-languagemodels.
7 FairnessEvaluation
Recent studies [28, 68] highlight the presence of biases in AI models used in radiology, raising
concernsabouttheirequitableapplicationacrossdiversepopulations. Forfairnessevaluation,we
testCheXagentonasubsetoftheCheXpertpublictestset, annotatedbytwoexpertradiologists
toavoidlabelnoise[65],usingfrontalviewCXRsfromindividualsself-reportingasAsian,White,
or Black. In total we use 159 unique subjects labeled as ‚ÄúNo Finding" and ‚ÄúCardiomegaly". To
createabalancedtestset,weresamplewithreplacement[27]toensurebalanceddiseaseprevalence
10Figure6:EvaluationofCheXagentsubgroupperformanceoncardiomegalyclassificationinvestigatingpotential
modelbiases.F1Scoresvaryacrosssex,racialgroups,andagecategories.
andsubgrouprepresentation. Wegenerate2000bootstrapsamplestocalculatemeanandstandard
deviationofF1-scores.
Weevaluatemodelperformanceforthedetectionofcardiomegalywiththeprompt‚ÄúDoesthischest
X-raycontaincardiomegaly?"withpossibleanswers‚ÄúYes"and‚ÄúNo". Detectionofcardiomegalyis
crucialforearlydiagnosis,andtreatmentofheartconditions[2],withstudiesrevealingsignificant
disparitiesinheartdiseasedeathratesbasedondemographicfactors[79]. Ourfindings,shownin
Fig.6revealdisparities;F1-scoresarehigherformalescomparedtofemalesandvaryacrossracial
groups,withthemodelperformingbestfortheBlacksubgroupandworstfortheAsiansubgroup.
Thiscouldreflectinherentdifferencesinthepresentationofcardiomegalyacrossraces,andcould
be influenced by the limited samples of 14 Black and 30 unique Asian subjects included in the
testset. Age-wise,themodelperformsbetterforthe65+agegroupcomparedtothe0-65group,
potentiallyduetoahigherprevalenceofcardiomegalyinolderpatients,andage-relatedphysiological
differences. Theseresultsareconsistentwithexistingliterature[28]andunderscoretheneedfor
continued efforts in mitigating biases in AI models used in healthcare. An effective approach to
addressthisissueiscuratinglargerandmorediversedatasets,whichcanhelpindevelopingmodels
thataremorerepresentativeandequitableacrossdifferentpatientdemographics.
8 Conclusion
Inconclusion,ourworkrepresentsprogresstowardsautomatedCXRinterpretation. Weintroduce(i)
CheXinstruct,aninstruction-tuningdataset,(ii)CheXagent,an8B-parametervision-languageFMand
demonstrateitsabilitiesthrough(iii)CheXbench,ourbenchmarkingframeworkincluding8tasksover
7datasets. CheXagentachievesimprovementinvisualperceptionandtextgenerationtaskscompared
to general- and medical-domain LLMs and is validated by five expert radiologists. Furthermore,
ourfairnessanalysisacrosssex,race,andagecontributestotheongoingeffortstoenhancemodel
transparencyinhealthcareAI.ThereleaseofCheXinstruct,CheXagent,andCheXbenchtothepublic
domainnotonlyunderscoresourcommitmenttoadvancingmedicalAIbutalsosetsanewbenchmark
forfuturedevelopmentsinthiscriticalareaofresearch.
Acknowledgements
MV is supported by graduate fellowship awards from the Department of Defense (NDSEG) and
theKnight-HennessyScholarsprogramatStanfordUniversity. ACreceivesresearchsupportfrom
theNationalInstitutesofHealth(grants-R01HL167974,R01AR077604,R01EB002524,R01
AR079431, P41 EB027060, and contracts 75N92020C00008, 75N92020C00021); and from GE
Healthcare and Philips. Research reported in this publication was made possible in part by the
NationalInstituteofBiomedicalImagingandBioengineering(NIBIB)oftheNationalInstitutesof
Healthundercontracts75N92020C00008and75N92020C00021,andbygrant#1R18HS028955from
theAgencyforHealthResearchandQuality. ThisworkissupportedinpartbyMIDRC(TheMedical
ImagingandDataResourceCenter),madepossiblebytheNationalInstituteofBiomedicalImaging
andBioengineering(NIBIB)oftheNationalInstitutesofHealthundercontract75N92020D00021.
WeacknowledgesupportbyStabilityAIinprovidingcomputationalsupportforthiswork.
11References
[1] J.-B.Alayrac,J.Donahue,P.Luc,A.Miech,I.Barr,Y.Hasson,K.Lenc,A.Mensch,K.Millican,
M.Reynolds,etal. Flamingo: avisuallanguagemodelforfew-shotlearning. Advancesin
NeuralInformationProcessingSystems,35:23716‚Äì23736,2022. 3
[2] S.S.Alghamdi,I.Abdelaziz,M.Albadri,S.Alyanbaawi,R.Aljondi,andA.Tajaldeen. Study
ofcardiomegalyusingchestx-ray. JournalofRadiationResearchandAppliedSciences,13(1):
460‚Äì467,2020. 11
[3] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa,
P.Bailey,Z.Chen,etal. Palm2technicalreport. arXivpreprintarXiv:2305.10403,2023. 3
[4] S.Bae,D.Kyung,J.Ryu,E.Cho,G.Lee,S.Kweon,J.Oh,L.Ji,E.I.Chang,T.Kim,etal.
Ehrxqa: A multi-modal question answering dataset for electronic health records with chest
x-rayimages. arXivpreprintarXiv:2310.18652,2023. 5,19
[5] J.Bai,S.Bai,S.Yang,S.Wang,S.Tan,P.Wang,J.Lin,C.Zhou,andJ.Zhou. Qwen-vl: A
frontierlargevision-languagemodelwithversatileabilities. arXivpreprintarXiv:2308.12966,
2023. 3
[6] S.Bannur,S.Hyland,Q.Liu,F.P√©rez-Garc√≠a,M.Ilse,D.C.deCastro,B.Boecking,H.Sharma,
K. Bouzid, A. Schwaighofer, et al. Ms-cxr-t: Learning to exploit temporal structure for
biomedicalvision-languageprocessing,2023. 5,19
[7] A. Ben Abacha, S. A. Hasan, V. V. Datla, D. Demner-Fushman, and H. M√ºller. Vqa-med:
Overviewofthemedicalvisualquestionansweringtaskatimageclef2019. InProceedingsof
CLEF(ConferenceandLabsoftheEvaluationForum)2019WorkingNotes.9-12September
2019,2019. 5,19
[8] B.Boecking,N.Usuyama,S.Bannur,D.C.Castro,A.Schwaighofer,S.Hyland,M.Wetscherek,
T.Naumann,A.Nori,J.Alvarez-Valle,etal. Makingthemostoftextsemanticstoimprove
biomedicalvision‚Äìlanguageprocessing. InEuropeanconferenceoncomputervision,pages
1‚Äì21.Springer,2022. 5,19
[9] R.Bommasani, D.A.Hudson, E.Adeli, R.Altman, S.Arora, S.vonArx, M.S.Bernstein,
J.Bohg,A.Bosselut,E.Brunskill,etal. Ontheopportunitiesandrisksoffoundationmodels.
arXivpreprintarXiv:2108.07258,2021. 2
[10] T.Brown,B.Mann,N.Ryder,M.Subbiah,J.D.Kaplan,P.Dhariwal,A.Neelakantan,P.Shyam,
G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural
informationprocessingsystems,33:1877‚Äì1901,2020. 3
[11] A.Bustos,A.Pertusa,J.-M.Salinas,andM.DeLaIglesia-Vaya. Padchest: Alargechestx-ray
imagedatasetwithmulti-labelannotatedreports. Medicalimageanalysis,66:101797,2020. 5,
19
[12] P.Chambon,C.Bluethgen,J.-B.Delbrouck,R.VanderSluijs,M.Po≈Çacin,J.M.Z.Chaves,T.M.
Abraham,S.Purohit,C.P.Langlotz,andA.Chaudhari. Roentgen: Vision-languagefoundation
modelforchestx-raygeneration,2022. URLhttps://arxiv.org/abs/2211.12737. 4
[13] P. Chambon, C. Bluethgen, C. P. Langlotz, and A. Chaudhari. Adapting pretrained vision-
languagefoundationalmodelstomedicalimagingdomains,2022. 4
[14] X.Chen,J.Djolonga,P.Padlewski,B.Mustafa,S.Changpinyo,J.Wu,C.R.Ruiz,S.Goodman,
X.Wang,Y.Tay,etal. Pali-x: Onscalingupamultilingualvisionandlanguagemodel. arXiv
preprintarXiv:2305.18565,2023. 3
[15] Z.Chen,M.Varma,X.Wan,C.Langlotz,andJ.-B.Delbrouck. Towardexpandingthescopeof
radiologyreportsummarizationtomultipleanatomiesandmodalities. InA.Rogers,J.Boyd-
Graber,andN.Okazaki,editors,Proceedingsofthe61stAnnualMeetingoftheAssociationfor
ComputationalLinguistics(Volume2: ShortPapers),pages469‚Äì484,Toronto,Canada,July
2023.AssociationforComputationalLinguistics. 5,19
12[16] W.-L.Chiang,Z.Li,Z.Lin,Y.Sheng,Z.Wu,H.Zhang,L.Zheng,S.Zhuang,Y.Zhuang,J.E.
Gonzalez,etal. Vicuna: Anopen-sourcechatbotimpressinggpt-4with90%*chatgptquality.
Seehttps://vicuna.lmsys.org(accessed14April2023),2023. 4
[17] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W.
Chung,C.Sutton,S.Gehrmann,etal. Palm: Scalinglanguagemodelingwithpathways. arXiv
preprintarXiv:2204.02311,2022. 3
[18] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. De-
hghani, S. Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint
arXiv:2210.11416,2022. 3
[19] M.Dehghani,J.Djolonga,B.Mustafa,P.Padlewski,J.Heek,J.Gilmer,A.P.Steiner,M.Caron,
R.Geirhos,I.Alabdulmohsin,etal. Scalingvisiontransformersto22billionparameters. In
InternationalConferenceonMachineLearning,pages7480‚Äì7512.PMLR,2023. 3
[20] J.-B.Delbrouck,P.Chambon,C.Bluethgen,E.Tsai,O.Almusa,andC.Langlotz. Improving
thefactualcorrectnessofradiologyreportgenerationwithsemanticrewards. InY.Goldberg,
Z.Kozareva,andY.Zhang,editors,FindingsoftheAssociationforComputationalLinguistics:
EMNLP2022,pages4348‚Äì4360,AbuDhabi,UnitedArabEmirates,Dec.2022.Association
for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.319. URL https:
//aclanthology.org/2022.findings-emnlp.319. 8
[21] D. Demner-Fushman, S. Antani, M. Simpson, and G. R. Thoma. Design and development
ofamultimodalbiomedicalinformationretrievalsystem. JournalofComputingScienceand
Engineering,6(2):168‚Äì177,2012. 5,19
[22] J.Devlin,M.-W.Chang,K.Lee,andK.Toutanova. Bert: Pre-trainingofdeepbidirectional
transformers for language understanding. In Proceedings of the 2019 Conference of the
NorthAmericanChapteroftheAssociationforComputationalLinguistics: HumanLanguage
Technologies,Volume1(LongandShortPapers),pages4171‚Äì4186,2019. 6
[23] A.Dosovitskiy,L.Beyer,A.Kolesnikov,D.Weissenborn,X.Zhai,T.Unterthiner,M.Dehghani,
M.Minderer,G.Heigold,S.Gelly,etal. Animageisworth16x16words: Transformersfor
imagerecognitionatscale. arXivpreprintarXiv:2010.11929,2020. 3
[24] S.Feng,D.Azzollini,J.S.Kim,C.-K.Jin,S.P.Gordon,J.Yeoh,E.Kim,M.Han,A.Lee,
A.Patel,etal. Curationofthecandid-ptxdatasetwithfree-textreports. Radiology: Artificial
Intelligence,3(6):e210136,2021. 5,19
[25] C.Fu,P.Chen,Y.Shen,Y.Qin,M.Zhang,X.Lin,Z.Qiu,W.Lin,J.Yang,X.Zheng,etal.
Mme: Acomprehensiveevaluationbenchmarkformultimodallargelanguagemodels. arXiv
preprintarXiv:2306.13394,2023. 3
[26] L.Gao,J.Tow,S.Biderman,S.Black,A.DiPofi,C.Foster,L.Golding,J.Hsu,K.McDonell,
N.Muennighoff,J.Phang,L.Reynolds,E.Tang,A.Thite,B.Wang,K.Wang,andA.Zou. A
frameworkforfew-shotlanguagemodelevaluation,Sept.2021. URLhttps://doi.org/10.
5281/zenodo.5371628. 7
[27] B. Glocker, C. Jones, M. Bernhardt, and S. Winzeck. Algorithmic encoding of protected
characteristicsinchestx-raydiseasedetectionmodels. Ebiomedicine,89,2023. 10
[28] B.Glocker,C.Jones,M.Roschewitz,andS.Winzeck. Riskofbiasinchestradiographydeep
learningfoundationmodels. Radiology: ArtificialIntelligence,5(6):e230060,2023. 10,11
[29] Y.Gu,R.Tinn,H.Cheng,M.Lucas,N.Usuyama,X.Liu,T.Naumann,J.Gao,andH.Poon.
Domain-specificlanguagemodelpretrainingforbiomedicalnaturallanguageprocessing. ACM
TransactionsonComputingforHealthcare(HEALTH),3(1):1‚Äì23,2021. 4
[30] G.Holste,S.Wang,A.Jaiswal,Y.Yang,M.Lin,Y.Peng,andA.Wang. Cxr-lt: Multi-label
long-tailedclassificationonchestx-rays. PhysioNet,2023. 5,19
13[31] X.Hu,L.Gu,Q.An,M.Zhang,L.Liu,K.Kobayashi,T.Harada,R.M.Summers,andY.Zhu.
Expertknowledge-awareimagedifferencegraphrepresentationlearningfordifference-aware
medicalvisualquestionanswering. InProceedingsofthe29thACMSIGKDDConferenceon
KnowledgeDiscoveryandDataMining,pages4156‚Äì4165,2023. 5,19
[32] J. Irvin, P. Rajpurkar, M. Ko, Y. Yu, S. Ciurea-Ilcus, C. Chute, H. Marklund, B. Haghgoo,
R. Ball, K. Shpanskaya, et al. Chexpert: A large chest radiograph dataset with uncertainty
labelsandexpertcomparison. InProceedingsoftheAAAIconferenceonartificialintelligence,
volume33,pages590‚Äì597,2019. 5,19
[33] L. Iyeke, R. Moss, R. Hall, J. Wang, L. Sandhu, B. Appold, E. Kalontar, D. Menoudakos,
M.Ramnarine,S.P.LaVine,etal. Reducingunnecessary‚Äòadmission‚Äôchestx-rays: Aninitiative
tominimizelow-valuecare. Cureus,14(10),2022. 2
[34] S.Jaeger,S.Candemir,S.Antani,Y.-X.J.W√°ng,P.-X.Lu,andG.Thoma. Twopublicchest
x-raydatasetsforcomputer-aidedscreeningofpulmonarydiseases. Quantitativeimagingin
medicineandsurgery,4(6):475,2014. 5,19
[35] A.Q.Jiang,A.Sablayrolles,A.Mensch,C.Bamford,D.S.Chaplot,D.d.l.Casas,F.Bressand,
G.Lengyel,G.Lample,L.Saulnier,etal. Mistral7b. arXivpreprintarXiv:2310.06825,2023.
3,6
[36] A.E.Johnson,T.J.Pollard,N.R.Greenbaum,M.P.Lungren,C.-y.Deng,Y.Peng,Z.Lu,R.G.
Mark,S.J.Berkowitz,andS.Horng. Mimic-cxr-jpg,alargepubliclyavailabledatabaseof
labeledchestradiographs. arXivpreprintarXiv:1901.07042,2019. 5
[37] M.Kayser,C.Emde,O.-M.Camburu,G.Parsons,B.Papiez,andT.Lukasiewicz. Explaining
chestx-raypathologiesinnaturallanguage. InInternationalConferenceonMedicalImage
ComputingandComputer-AssistedIntervention,pages701‚Äì713.Springer,2022. 5,19
[38] A.Kirillov,E.Mintun,N.Ravi,H.Mao,C.Rolland,L.Gustafson,T.Xiao,S.Whitehead,A.C.
Berg,W.-Y.Lo,etal. Segmentanything. arXivpreprintarXiv:2304.02643,2023. 3
[39] J.J.Lau,S.Gayen,A.BenAbacha,andD.Demner-Fushman. Adatasetofclinicallygenerated
visualquestionsandanswersaboutradiologyimages. Scientificdata,5(1):1‚Äì10,2018. 19
[40] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang. Biobert: a pre-trained
biomedicallanguagerepresentationmodelforbiomedicaltextmining. Bioinformatics,36(4):
1234‚Äì1240,2020. 4
[41] P.Lewis,M.Ott,J.Du,andV.Stoyanov.Pretrainedlanguagemodelsforbiomedicalandclinical
tasks: understanding and extending the state-of-the-art. In Proceedings of the 3rd Clinical
NaturalLanguageProcessingWorkshop,pages146‚Äì157,2020. 4
[42] B.Li,R.Wang,G.Wang,Y.Ge,Y.Ge,andY.Shan. Seed-bench: Benchmarkingmultimodal
llmswithgenerativecomprehension. arXivpreprintarXiv:2307.16125,2023. 3,7,8
[43] C.Li,C.Wong,S.Zhang,N.Usuyama,H.Liu,J.Yang,T.Naumann,H.Poon,andJ.Gao.
Llava-med: Trainingalargelanguage-and-visionassistantforbiomedicineinoneday. arXiv
preprintarXiv:2306.00890,2023. 4,8
[44] J. Li, D. Li, C. Xiong, and S. Hoi. Blip: Bootstrapping language-image pre-training for
unifiedvision-languageunderstandingandgeneration. InInternationalConferenceonMachine
Learning,pages12888‚Äì12900.PMLR,2022. 3,4
[45] J.Li,D.Li,S.Savarese,andS.Hoi. Blip-2: Bootstrappinglanguage-imagepre-trainingwith
frozenimageencodersandlargelanguagemodels. arXivpreprintarXiv:2301.12597,2023. 6
[46] C.-Y.Lin. Rouge: Apackageforautomaticevaluationofsummaries. InTextsummarization
branchesout,pages74‚Äì81,2004. 8
[47] H.Liu,C.Li,Q.Wu,andY.J.Lee. Visualinstructiontuning. arXivpreprintarXiv:2304.08485,
2023. 3
14[48] R.Luo,L.Sun,Y.Xia,T.Qin,S.Zhang,H.Poon,andT.-Y.Liu. Biogpt: generativepre-trained
transformer for biomedical text generation and mining. Briefings in Bioinformatics, 23(6):
bbac409,2022. 4,6
[49] X.Mei,Z.Liu,P.M.Robson,B.Marinelli,M.Huang,A.Doshi,A.Jacobi,C.Cao,K.E.Link,
T.Yang,etal. Radimagenet: anopenradiologicdeeplearningresearchdatasetforeffective
transferlearning. Radiology: ArtificialIntelligence,4(5):e210315,2022. 3
[50] Y.Miura,Y.Zhang,E.Tsai,C.Langlotz,andD.Jurafsky. Improvingfactualcompleteness
and consistency of image-to-text radiology report generation. In Proceedings of the 2021
ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:
HumanLanguageTechnologies,pages5288‚Äì5304,2021. 5,19
[51] M. Moor, Q. Huang, S. Wu, M. Yasunaga, C. Zakka, Y. Dalmia, E. P. Reis, P. Rajpurkar,
and J. Leskovec. Med-flamingo: a multimodal medical few-shot learner. arXiv preprint
arXiv:2307.15189,2023. 4,8
[52] S.Mukherjee,A.Mitra,G.Jawahar,S.Agarwal,H.Palangi,andA.Awadallah. Orca: Progres-
sivelearningfromcomplexexplanationtracesofgpt-4. arXivpreprintarXiv:2306.02707,2023.
19
[53] H.Q.Nguyen,K.Lam,L.T.Le,H.H.Pham,D.Q.Tran,D.B.Nguyen,D.D.Le,C.M.Pham,
H.T.Tong,D.H.Dinh,etal. Vindr-cxr: Anopendatasetofchestx-rayswithradiologist‚Äôs
annotations. ScientificData,9(1):429,2022. 5,19
[54] OpenAI. Gpt-4technicalreport,2023. 3
[55] M.Pavlova,T.Tuinstra,H.Aboutalebi,A.Zhao,H.Gunraj,andA.Wong. Covidxcxr-3: a
large-scale,open-sourcebenchmarkdatasetofchestx-rayimagesforcomputer-aidedcovid-19
diagnostics. arXivpreprintarXiv:2206.03671,2022. 5,19
[56] O.Pelka,S.Koitka,J.R√ºckert,F.Nensa,andC.M.Friedrich. Radiologyobjectsincontext
(roco): amultimodalimagedataset. InIntravascularImagingandComputerAssistedStenting
andLarge-ScaleAnnotationofBiomedicalDataandExpertLabelSynthesis,pages180‚Äì189.
Springer,2018. 5,19
[57] C. Pellegrini, M. Keicher, E. √ñzsoy, and N. Navab. Rad-restruct: A novel vqa benchmark
andmethodforstructuredradiologyreporting. InInternationalConferenceonMedicalImage
ComputingandComputer-AssistedIntervention,pages409‚Äì419.Springer,2023. 5,19
[58] H.H.Pham,T.T.Tran,andH.Q.Nguyen. Vindr-pcxr: Anopen,large-scalepediatricchest
x-raydatasetforinterpretationofcommonthoracicdiseases. PhysioNet,2022. 5,19
[59] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,
P.Mishkin,J.Clark,etal.Learningtransferablevisualmodelsfromnaturallanguagesupervision.
InInternationalconferenceonmachinelearning,pages8748‚Äì8763.PMLR,2021. 3
[60] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever.
Zero-shottext-to-imagegeneration. InInternationalConferenceonMachineLearning,pages
8821‚Äì8831.PMLR,2021. 3
[61] A.Ramesh,P.Dhariwal,A.Nichol,C.Chu,andM.Chen. Hierarchicaltext-conditionalimage
generationwithcliplatents. arXivpreprintarXiv:2204.06125,1(2):3,2022. 3
[62] E.P.Reis,J.P.dePaiva,M.C.daSilva,G.A.Ribeiro,V.F.Paiva,L.Bulgarelli,H.M.Lee,P.V.
Santos,V.M.Brito,L.T.Amaral,etal. Brax,brazilianlabeledchestx-raydataset. Scientific
Data,9(1):487,2022. 5,19
[63] R.Rombach,A.Blattmann,D.Lorenz,P.Esser,andB.Ommer.High-resolutionimagesynthesis
withlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconferenceoncomputervision
andpatternrecognition,pages10684‚Äì10695,2022. 3
15[64] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gon-
tijoLopes,B.KaragolAyan,T.Salimans,etal. Photorealistictext-to-imagediffusionmodels
withdeeplanguageunderstanding. AdvancesinNeuralInformationProcessingSystems,35:
36479‚Äì36494,2022. 3
[65] A.Saporta,X.Gui,A.Agrawal,A.Pareek,S.Q.Truong,C.D.Nguyen,V.-D.Ngo,J.Seekins,
F.G.Blankenberg,A.Y.Ng,etal.Benchmarkingsaliencymethodsforchestx-rayinterpretation.
NatureMachineIntelligence,4(10):867‚Äì878,2022. 10
[66] C.Schuhmann, R.Beaumont, R.Vencu, C.Gordon, R.Wightman, M.Cherti, T.Coombes,
A.Katta,C.Mullis,M.Wortsman,etal. Laion-5b: Anopenlarge-scaledatasetfortraining
nextgenerationimage-textmodels. AdvancesinNeuralInformationProcessingSystems,35:
25278‚Äì25294,2022. 2
[67] J.C.Seah,C.H.Tang,Q.D.Buchlak,X.G.Holt,J.B.Wardman,A.Aimoldin,N.Esmaili,
H.Ahmad,H.Pham,J.F.Lambert,etal. Effectofacomprehensivedeep-learningmodelonthe
accuracyofchestx-rayinterpretationbyradiologists: aretrospective,multireadermulticase
study. TheLancetDigitalHealth,3(8):e496‚Äìe506,2021. 2
[68] L.Seyyed-Kalantari,H.Zhang,M.B.McDermott,I.Y.Chen,andM.Ghassemi.Underdiagnosis
biasofartificialintelligencealgorithmsappliedtochestradiographsinunder-servedpatient
populations. Naturemedicine,27(12):2176‚Äì2182,2021. 10
[69] G.Shih,C.C.Wu,S.S.Halabi,M.D.Kohli,L.M.Prevedello,T.S.Cook,A.Sharma,J.K.
Amorosa,V.Arteaga,M.Galperin-Aizenberg,etal.Augmentingthenationalinstitutesofhealth
chestradiographdatasetwithexpertannotationsofpossiblepneumonia. Radiology: Artificial
Intelligence,1(1):e180041,2019. 5,19
[70] H.-C.Shin,Y.Zhang,E.Bakhturina,R.Puri,M.Patwary,M.Shoeybi,andR.Mani. Biomega-
tron: Largerbiomedicaldomainlanguagemodel. InProceedingsofthe2020Conferenceon
EmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages4700‚Äì4706,2020. 6
[71] K.Singhal,T.Tu,J.Gottweis,R.Sayres,E.Wulczyn,L.Hou,K.Clark,S.Pfohl,H.Cole-Lewis,
D.Neal,etal. Towardsexpert-levelmedicalquestionansweringwithlargelanguagemodels.
arXivpreprintarXiv:2305.09617,2023. 4
[72] A.Smit,S.Jain,P.Rajpurkar,A.Pareek,A.Y.Ng,andM.Lungren. Combiningautomatic
labelersandexpertannotationsforaccurateradiologyreportlabelingusingbert. InProceedings
of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),
pages1500‚Äì1519,2020. 8
[73] Q.Sun,Y.Fang,L.Wu,X.Wang,andY.Cao. Eva-clip: Improvedtrainingtechniquesforclip
atscale. arXivpreprintarXiv:2303.15389,2023. 6
[74] O.Thawkar,A.Shaker,S.S.Mullappilly,H.Cholakkal,R.M.Anwer,S.Khan,J.Laaksonen,
andF.S.Khan. Xraygpt: Chestradiographssummarizationusingmedicalvision-language
models. arXivpreprintarXiv:2306.07971,2023. 2,4,8
[75] E.Tiu,E.Talius,P.Patel,C.P.Langlotz,A.Y.Ng,andP.Rajpurkar. Expert-leveldetection
of pathologies from unannotated chest x-ray images via self-supervised learning. Nature
BiomedicalEngineering,6(12):1399‚Äì1406,2022. 2
[76] H.Touvron,T.Lavril,G.Izacard,X.Martinet,M.-A.Lachaux,T.Lacroix,B.Rozi√®re,N.Goyal,
E.Hambro, F.Azhar, etal. Llama: Openandefficientfoundationlanguagemodels. arXiv
preprintarXiv:2302.13971,2023. 3
[77] H.Touvron,L.Martin,K.Stone,P.Albert,A.Almahairi,Y.Babaei,N.Bashlykov,S.Batra,
P.Bhargava,S.Bhosale,etal. Llama2: Openfoundationandfine-tunedchatmodels. arXiv
preprintarXiv:2307.09288,2023. 3
[78] T. Tu, S. Azizi, D. Driess, M. Schaekermann, M. Amin, P.-C. Chang, A. Carroll, C. Lau,
R.Tanno,I.Ktena,etal. Towardsgeneralistbiomedicalai. arXivpreprintarXiv:2307.14334,
2023. 4
16[79] M.VanDyke,S.Greer,E.Odom,L.Schieb,A.Vaughan,M.Kramer,andM.Casper. Heart
diseasedeathratesamongblacksandwhitesaged35years‚Äîunitedstates,1968‚Äì2015. MMWR
SurveillanceSummaries,67(5):1,2018. 11
[80] D. Van Veen, C. Van Uden, M. Attias, A. Pareek, C. Bluethgen, M. Polacin, W. Chiu, J.-
B. Delbrouck, J. Zambrano Chaves, C. Langlotz, A. Chaudhari, and J. Pauly. RadAdapt:
Radiologyreportsummarizationvialightweightdomainadaptationoflargelanguagemodels.
InD.Demner-fushman,S.Ananiadou,andK.Cohen,editors,The22ndWorkshoponBiomedical
NaturalLanguageProcessingandBioNLPSharedTasks,pages449‚Äì460,Toronto,Canada,July
2023.AssociationforComputationalLinguistics. doi: 10.18653/v1/2023.bionlp-1.42. URL
https://aclanthology.org/2023.bionlp-1.42. 9
[81] D.VanVeen,C.VanUden,L.Blankemeier,J.-B.Delbrouck,A.Aali,C.Bluethgen,A.Pareek,
M.Polacin,W.Collins,N.Ahuja,etal. Clinicaltextsummarization: Adaptinglargelanguage
modelscanoutperformhumanexperts. arXivpreprintarXiv:2309.07430,2023. 3,9
[82] M.D.L.I.Vay√°,J.M.Saborit,J.A.Montell,A.Pertusa,A.Bustos,M.Cazorla,J.Galant,
X.Barber, D.Orozco-Beltr√°n, F.Garc√≠a-Garc√≠a, etal. Bimcvcovid-19+: alargeannotated
datasetofrxandctimagesfromcovid-19patients. arXivpreprintarXiv:2006.01174,2020. 5,
19
[83] W. Wang, Q. Lv, W. Yu, W. Hong, J. Qi, Y. Wang, J. Ji, Z. Yang, L. Zhao, X. Song, et al.
Cogvlm: Visualexpertforpretrainedlanguagemodels. arXivpreprintarXiv:2311.03079,2023.
3
[84] X.Wang,Y.Peng,L.Lu,Z.Lu,M.Bagheri,andR.M.Summers. Chestx-ray8: Hospital-scale
chestx-raydatabaseandbenchmarksonweakly-supervisedclassificationandlocalizationof
commonthoraxdiseases.InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,pages2097‚Äì2106,2017. 5,19
[85] Z.Wang,Z.Wu,D.Agarwal,andJ.Sun. Medclip: Contrastivelearningfromunpairedmedical
imagesandtext. arXivpreprintarXiv:2210.10163,2022. 4
[86] J. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le.
Finetunedlanguagemodelsarezero-shotlearners. InInternationalConferenceonLearning
Representations,2021. 4,5
[87] C. Wu, W. Lin, X. Zhang, Y. Zhang, Y. Wang, and W. Xie. Pmc-llama: Towards building
open-sourcelanguagemodelsformedicine. arXivpreprintarXiv:2305.10415,2023. 6
[88] C.Wu,X.Zhang,Y.Zhang,Y.Wang,andW.Xie. Towardsgeneralistfoundationmodelfor
radiology. arXivpreprintarXiv:2308.02463,2023. 4,8
[89] S. Xu, L. Yang, C. Kelly, M. Sieniek, T. Kohlberger, M. Ma, W.-H. Weng, A. Kiraly,
S. Kazemzadeh, Z. Melamed, et al. Elixr: Towards a general purpose x-ray artificial in-
telligencesystemthroughalignmentoflargelanguagemodelsandradiologyvisionencoders.
arXivpreprintarXiv:2308.01317,2023. 2
[90] Z.Xu,Y.Shen,andL.Huang. Multiinstruct: Improvingmulti-modalzero-shotlearningvia
instructiontuning. arXivpreprintarXiv:2212.10773,2022. 4
[91] Z. Xue, S. Candemir, S. Antani, L. R. Long, S. Jaeger, D. Demner-Fushman, and G. R.
Thoma. Foreignobjectdetectioninchestx-rays. In2015IEEEinternationalconferenceon
bioinformaticsandbiomedicine(BIBM),pages956‚Äì961.IEEE,2015. 5,19
[92] X.Yang,A.Chen,N.PourNejatian,H.C.Shin,K.E.Smith,C.Parisien,C.Compas,C.Martin,
A.B.Costa,M.G.Flores,etal. Alargelanguagemodelforelectronichealthrecords. NPJ
DigitalMedicine,5(1):194,2022. 6
[93] F.Yu,M.Endo,R.Krishnan,I.Pan,A.Tsai,E.P.Reis,E.K.U.N.Fonseca,H.M.H.Lee,
Z.S.H.Abad,A.Y.Ng,etal. Evaluatingprogressinautomaticchestx-rayradiologyreport
generation. Patterns,4(9),2023. 5,19
17[94] J.Yu, Z.Wang, V.Vasudevan, L.Yeung, M.Seyedhosseini, andY.Wu. Coca: Contrastive
captionersareimage-textfoundationmodels. arXivpreprintarXiv:2205.01917,2022. 3,6
[95] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi. Bertscore: Evaluating text
generationwithbert. InInternationalConferenceonLearningRepresentations,2019. 8
[96] X. Zhang, C. Wu, Y. Zhang, W. Xie, and Y. Wang. Knowledge-enhanced visual-language
pre-trainingonchestradiologyimages. NatureCommunications,14(1):4542,2023. 2
[97] X.Zhang,C.Wu,Z.Zhao,W.Lin,Y.Zhang,Y.Wang,andW.Xie. Pmc-vqa:Visualinstruction
tuningformedicalvisualquestionanswering. arXivpreprintarXiv:2305.10415,2023. 5,19
[98] X.Zou,J.Yang,H.Zhang,F.Li,L.Li,J.Gao,andY.J.Lee. Segmenteverythingeverywhere
allatonce. arXivpreprintarXiv:2304.06718,2023. 3
18A.AdditionaldetailsonCheXinstructtasks
WereportthedetailofeachtaskinTable5,wherethreelevels(i.e.,capabilities,tasks,datasets)and
thecorrespondingtaskdescriptionsareshown.
B.AdditionaldetailsonCheXinstructdatasetsources
WecreatedCheXinstructbyeither(i)collectingexistingpubliclyavailabledatasetsor(ii)curatinga
datasetwithnewlabelsfromexistingdatasets.
We include the following publicly-available datasets: BIMCV-COVID-19 [82], BRAX [62],
CANDID-PTX [24], ChestXray14 [84], CheXpert [32], COVIDX-CXR-3 [55], CXR-LT [30],
MedVQA-2019[7],MIMIC-CXR-VQA[4],MIMIC-Diff-VQA[31],MIMIC-RRS[15],MIMIC-
NLE [37], MS-CXR [8], MS-CXR-T [6], NLM-TB [34], Object-CXR [91], OpenI [21], PadCh-
est[11],PMC-VQA[97],Rad-Restruct[57],RadNLI[50],ReXVal[93],ROCO[56],RSNA[69],
SIIM9, VinDr-CXR [53], VinDr-PCXR [58], and VQA-RAD [39]. Additionally, we use the
OpenOrca10[52]textinstructiontuningdatasettoimprovethediversityoftextinstructions.
Inordertocuratedatasetswithnewlabelsbasedonexistingdatasets, weperformtranslationon
theradiologyreportsofBIMCV-COVID-19andPadChestfromSpanishtoEnglishusingGPT-4.
Additionally,wefilterVQA-RAD,SLAKE,MedVQA-2019,andPMC-VQAtoincludeonlychest
X-rayimagesandtheircorrespondingquestion-answerpairs. FortheMIMIC-CXRdataset,weuse
GPT-4toformatitsFindingsandImpressionsectionstostructuredones(calledMIMIC-CXR-Struct),
whereeachsentenceisannotatedwithitscorrespondinganatomicalstructureandparaphrasedto
eliminatecomparisonstothepatient‚Äôspriorimages.
C.ExtendedanalysisofCheXinstruct
WereportCheXinstructstatisticsinTable6,wherethenumberofinstancesandnumberofimages
associatedwitheachdatasetareprovided.
D.CheXagentdatasetratios
InTable7,wereportthesamplingratiosofeachdatasetusedforStage-3trainingofCheXagent. The
value‚Äú0‚Äùmeansthatthecorrespondingdatasetisheldout.
E.CheXagentimplementationdetails
ThetrainingofCheXagentinvolvesfourstagesandwereportthetraininghyperparametersofeach
stageinTable8.
F.CheXbenchDesign
CheXbenchevaluatestwocomponentsofCXRinterpretation: imageperceptionandtextualunder-
standing.
Evaluation Axis 1 - Image Perception: We assess FM performance on image perception with
sixmultiple-choicetasksacrosssevendatasets. Incombination,thesetasksinclude5,083samples
spanningviewclassification,binarydiseaseclassification,singlediseaseidentification,multi-disease
identification,visual-question-answering,andimage-textreasoning.InFigure7,weprovideexamples
fromeachtask. Table9providesasummaryofalltasksincludedinEvaluationAxis1.
EvaluationAxis2-TextualUnderstanding: WeassessFMperformanceontextualunderstanding
withtwotasks: findingsgenerationandfindingssummarization. Weevaluateperformanceusing
severalautomatedmetrics: ROUGE-L,CheXbert-Score,BERT-Score,RadGraph-Score,andGPT-4.
WeprovidepromptsusedforGPT-4evaluationsinFigure8.
9https://www.kaggle.com/datasets/jesperdramsch/siim-acr-pneumothorax-segmentation-data
10https://huggingface.co/datasets/Open-Orca/OpenOrca
19View Classification Binary Disease Classification Single Disease identification
What is the view of this chest X-ray? Does this chest X-ray contain Which finding is in this chest X-ray?
A. AP pneumonia? A. lung lesion
B. PA A. no B. no finding
C. LATERAL B. yes C. cardiomegaly
D. pleural effusion
Multi-Disease Identification Visual-Question-Answering Image-Text Reasoning
W A.hi pch n efi un mdi on ng is a ,a cr ae r i dn io t mhi es gc ah le ys t X-ray? A par te i eth ne t‚Äôr se r a igb hn to lr um nga ?li ties in the Which finding is in this chest X-ray?
B. consolidation, cardiomegaly A. no A. elevated right diaphragm
C. lung lesion, support devices B. yes B. elevated left diaphragm
D. pleural effusion, support devices
Figure7:ExamplesofCheXbenchimageperceptiontasks:CheXbenchincludessixmultiple-choicetasksthat
evaluatetheabilityofFMstointerpretchestX-rays.
ThepromptforGPT-4Evaluation
Prompt:
We would like to request your feedback on the radiology reports generated by two AI
assistantsbycomparingthemtothereferencereportwrittenbyradiologists.
[ReferenceReport]
[reference]
[EndofReferenceReport]
[Assistant1]
[report1]
[Assistant1]
[Assistant2]
[report2]
[EndofAssistant2]
[Requirements]
1. Thelengthofthereportsisnotimportant.
2. Thestyleofthereportsisnotimportant.
3. Theclinicalaccuracyisimportantespeciallyforpositivefindings(i.e.,diseases).
Therefore,pleasefocusonclinicalaccuracyinsteadofthelengthandstyle.
[EndofRequirements]
Pleasecomparetheaccuracyoftheirgeneratedreports.YoushouldtellmewhetherAssistant
1is‚Äúbetterthan‚Äù,‚Äúworsethan‚Äù,or‚Äúequalto‚ÄùAssistant2. Pleasefirstcomparethegenerated
reports with the reference report to analyze which one is more in line with the given
requirements. In the last line, please output a single line containing only a single label
selectingfrom‚ÄúAssistant1isbetterthanAssistant2‚Äù,‚ÄúAssistant1isworsethanAssistant
2‚Äù,and‚ÄúAssistant1isequaltoAssistant2‚Äù.
Figure8:ThepromptforGPT-4Evaluation.
20Table5:DetailedtaskinformationofCheXinstruct,wherethreelevels(i.e.,capabilities,tasks,datasets)andthe
correspondingtaskdescriptionsareshown.
Capability Task Dataset Description
ChestX-ray14
CheXpert
MIMIC-CXR
PadChest
DiseaseClassification RSNA Givenan<IMAGE>,themodelisrequiredtodiagnoseifthe<DISEASE>exists.
Coarse-grainedImageUnderstanding C CO XV R-I LD TX-CXR-3
BRAX
NLM-TB
Temporalimageclassification MS-CXR-T Giventhe<PRIORIMAGE>and<CURRENTIMAGE>,identifytheprogression<LABEL>.
ViewClassification MIMIC-CXR Giventhe<IMAGE>,identifyits<VIEW>.
ViewMatching MIMIC-CXR Giventhe<IMAGE1>and<IMAGE2>,iftheybelongtothesamestudy.
AbnormalityDetection VV ii nn DD rr -- PC CX XR R Giventhe<IMAGE>,localizethe<REGION>ofabnormalities.
AbnormalityGrounding VV ii nn DD rr -- PC CX XR R Giventhe<IMAGE>,localizethe<REGION>of<abnormality>.
Fine-grainedImageUnderstanding
PneumothoraxSegmentation SC Ia In Mdid Giventhe<IMAGE>,segmentthe<REGION>ofpneumothorax.
RibFractureSegmentation Candid Giventhe<IMAGE>,segmentthe<REGION>ofribfractures.
ChestTubeSegmentation Candid Giventhe<IMAGE>,segmentthe<REGION>ofChestTubes.
ForeignObjectDetection Object-CXR Giventhe<IMAGE>,detectthe<REGION>ofexternalobjects.
PhraseGrounding MS-CXR Giventhe<IMAGE>and<PHRASE>,identifythe<REGION>of<PHRASE>.
GroundedCaptioning MS-CXR Giventhe<IMAGE>and<REGION>,generatea<CAPTION>.
GroundedDiagnosis MS-CXR Giventhe<IMAGE>and<REGION>,generatea<DIAGNOSIS>.
GroundedPhraseExtraction MS-CXR Giventhe<IMAGE>,its<REPORT>,and<REGION>s,extracta<PHRASE>.
PhraseExtractionandGrounding MS-CXR Giventhe<IMAGE>andits<REPORT>,extracta<PHRASE>andlocalizeits<REGION>.
ReportGeneration BPa Id MC Ch Ves -t COVID19 Giventhe<IMAGE>,generateits<REPORT>.
MIMIC-CXR
FindingsGeneration MIMIC-CXR-Struct Giventhe<IMAGE>,generateits<FINDINGS>.
OpenI
MIMIC-CXR
TextGeneration ImpressionGeneretion M OpIM enI IC-CXR-Struct Giventhe<IMAGE>,generateits<IMPRESSION>.
Candid
ProgressionFindingsGeneration MIMIC-CXR Giventhe<REFERENCEIMAGE>and<MAINIMAGE>,generateits<FINDINGS>.
ProgressionImpressionGeneration MIMIC-CXR Giventhe<REFERENCEIMAGE>and<MAINIMAGE>,generateits<IMPRESSION>.
MIMIC-CXR
FindingsSummarization OpenI Giventhe<FINDINGS>,generateits<IMPRESSION>.
MIMIC-III
CaptionGeneration ROCO Giventhe<IMAGE>,generateits<CAPTION>.
LocalFindingsGeneration MIMIC-CXR-Struct Giventhe<IMAGE>andaanatomy,generateits<FINDINGS>.
LocalImpressionGeneration MIMIC-CXR-Struct Giventhe<IMAGE>andaanatomy,generateits<IMPRESSION>.
VQA-RAD
SLAKE
Open-endedVQA M PMed CV -VQ QA A-2019 Giventhecontentofthegiven<IMAGE>,answerthe<QUESTION>.
Rad-Restruct
MIMIC-CXR-VQA
QuestionAnswering VQA-RAD
SLAKE
Close-endedVQA P RM adC -R-V esQ trA uct Giventhecontentofthegiven<IMAGE>,chooseoneoptionfromthe<OPTIONS>toanswerthe<QUESTION>.
MIMIC-CXR-VQA
DifferenceVQA MIMIC-Diff-VQA Givena<REFERENCEIMAGE>anda<MAINIMAGE>,answerthe<QUESTION>.
TextQA RadQA Given<PARAGRAPH>,answerthe<QUSETION>.
Image-TextMatching RM OIM COIC-CXR Giventhe<IMAGE>and<REPORT>,decideiftheymatch.
Miscellaneous Image-TextSelection RM OIM COIC-CXR Giventhe<IMAGE>,selectthetextthatbestmatchestheimagefrom<OPTIONS>.
ReportEvaluation ReXVal Givena<REFENCEREPORT>anda<GENERATEDREPORT>,identifythe<ERROR>.
NaturalLanguageExplanation MIMIC-NLE Gvenan<IMAGE>and<DISEASE>,generatethenaturallanguage<EXPLANATION>.
NaturalLanguageInference RadNLI Givena<PREMISEREPORT>,determinewhethera<HYPOTHESISREPORT>isentailment,contradiction,orneutral.
TemporalSentenceSimilarity MS-CXR-T Given<SENTENCE1>and<SENTENCE2>,identifytheir<SIMILARITY>intermsofdiseaseprogression.
21Table6:ThebasicstatisticsofCheXinstruct,wherethenumbersofinstancesandimagesofeachdatasetare
shown.Thedatasetsaresplitfollowingtheirofficialortraditionalways.
Task-DatasetPair Instances(train) Instances(val) Instances(test) Images(train) Images(val) Images(test)
(Open-EndedVQA)VQA-RAD 713 119 119 73 50 50
(Open-EndedVQA)SLAKE 1,093 233 220 90 19 17
(Open-EndedVQA)MedVQA-2019 78 11 1 78 11 1
(Open-EndedVQA)PMC-VQA 747 0 229 646 0 193
(Open-EndedVQA)Rad-Restruct 142,340 17,641 17,641 2,972 374 374
(Open-EndedVQA)MIMIC-CXR-VQA 259,484 63,078 11,347 127,798 8,647 500
(Close-EndedVQA)VQA-RAD 417 69 69 69 37 37
(Close-EndedVQA)SLAKE 297 59 64 90 19 17
(Close-EndedVQA)MedVQA-2019 1 0 0 1 0 0
(Close-EndedVQA)PMC-VQA 682 0 209 596 0 180
(Close-EndedVQA)Rad-Restruct 142,340 17,641 17,641 2,972 374 374
(Close-EndedVQA)MIMIC-CXR-VQA 162,577 39,376 6,952 106,209 8,586 500
(DifferenceVQA)MIMIC-Diff-VQA 160,054 1,325 2,967 160,377 1,332 2,969
(TextQA)RadQA 4,878 614 614 0 0 0
(ImageClassification)ChestXray14 78,484 11,211 22,425 78,484 11,211 22,425
(ImageClassification)CheXpert 223,414 234 668 223,414 234 668
(ImageClassification)MIMIC-CXR 212,098 1,714 3,131 348,516 2,813 4,896
(ImageClassification)PadChest 109,845 0 0 160,742 0 0
(ImageClassification)RSNA 18,678 4,003 4,003 18,678 4,003 4,003
(ImageClassification)COVIDX-CXR-3 29,986 0 400 29,986 0 400
(ImageClassification)CXR-LT 155,349 0 0 255,445 0 0
(ImageClassification)Brax 23,276 0 0 40,967 0 0
(ImageClassification)NLM-TB 800 0 0 800 0 0
(TemporalImageClassification)MS-CXR-T 985 10 50 1,903 19 96
(ViewClassification)MIMIC-CXR 353,640 2,867 4,834 353,640 2,867 4,834
(ViewMatching)MIMIC-CXR 34,174 290 320 33,797 293 349
(PhraseGrounding)MS-CXR 964 7 189 878 5 164
(GroundedCaptioning)MS-CXR 964 7 189 878 5 164
(GroundedDiagnosis)MS-CXR 964 7 189 878 5 164
(GroundedPhraseExtraction)MS-CXR 527 7 9 490 5 8
(PhraseExtractionandGrounding)MS-CXR 527 7 9 490 5 8
(AbnormalityDetection)VinDr-CXR 15,000 0 3,000 15,000 0 3,000
(AbnormalityGrounding)VinDr-CXR 30,282 0 4,022 11,685 0 1,999
(GroundedDiagnosis)VinDr-CXR 17,880 0 2,345 4,510 0 937
(AbnormalityDetection)VinDr-PCXR 7,728 0 1,397 7,728 0 1,397
(AbnormalityGrounding)VinDr-PCXR 6,648 0 1,218 4,477 0 809
(GroundedDiagnosis)VinDr-PCXR 4,788 0 851 2,496 0 469
(PneumothoraxSegmentation)Candid 8,195 0 0 8,195 0 0
(PneumothoraxSegmentation)SIIM 7,621 1,709 1,704 7,621 1,709 1,704
(RibFractureSegmentation)Candid 670 0 0 668 0 0
(ChestTubeSegmentation)Candid 2,846 0 0 2,775 0 0
(ForeignObjectDetection)Object-CXR 8,000 1,000 0 8,000 1,000 0
(ReportGeneration)PadChest 109,792 0 0 160,670 0 0
(ReportGeneration)BIMCV-COVID19 46,941 0 0 65,421 0 0
(FindingsGeneration)MIMIC-CXR 152,173 1,196 2,347 270,790 2,130 3,858
(FindingsGeneration)MIMIC-CXR-Struct 148,501 1,164 2,309 264,777 2,079 3,786
(FindingsGeneration)OpenI 0 0 3,337 0 0 6,473
(ImpressionGeneration)MIMIC-CXR 185,816 1,521 2,224 316,684 2,573 3,724
(ImpressionGeneration)MIMIC-CXR-Struct 175,152 1,430 2,141 298,108 2,429 3,584
(ImpressionGeneration)OpenI 0 0 3,820 0 0 7,418
(ImpressionGeneration)Candid 18,307 0 0 19,206 0 0
(FindingsSummarization)MIMIC-CXR 125,417 991 1,624 0 0 0
(ProgressionFindingsGeneration)MIMIC-CXR 19,107 138 483 34,354 258 851
(ProgressionImpressionGeneration)MIMIC-CXR 26,646 216 313 46,227 369 560
(FindingsSummarization)OpenI 0 0 3,419 0 0 0
(FindingsSummarization)MIMIC-III 59,320 7,413 13,057 0 0 0
(CaptionGeneration)ROCO 2,554 293 324 2,554 293 324
(LocalFindingsGeneration)MIMIC-CXR-Struct 1,059,903 8,299 16,275 264,688 2,078 3,785
(LocalImpressionGeneration)MIMIC-CXR-Struct 674,284 5,686 8,196 297,598 2,426 3,582
(Image-TextMatching)MIMIC-CXR 675,978 5,434 9,142 354,619 2,866 4,710
(Image-TextMatching)ROCO 5,108 586 648 2,554 293 324
(Image-TextSelection)MIMIC-CXR 337,989 2,717 4,571 354,619 2,866 4,710
(Image-TextSelection)ROCO 2,554 293 324 2,554 293 324
(ReportEvaluation)ReXVal 0 0 200 0 0 0
(NaturalLanguageExplanation)MIMIC-NLE 37,016 273 714 51,503 373 944
(NaturalLanguageInference)RadNLI 0 480 480 0 0 0
(TemporalSentenceSimilarity)MS-CXR-T 0 0 361 0 0 0
(NamedEntityRecognition)RadGraph 541 9 50 0 0 0
22Table7:Thesamplingratioofeachdatasetinthethird-stagetrainingofCheXagent.
Task-DatasetPair SamplingRatio
(NamedEntityRecognition)RadGraph 10.00
(AbnormalityGrounding)VinDr-CXR 1.00
(AbnormalityGrounding)VinDr-PCXR 1.00
(CaptionGeneration)ROCO 1.00
(Close-EndedVQA)PMC-VQA 1.00
(Close-EndedVQA)VQA-RAD 1.00
(ForeignObjectDetection)Object-CXR 1.00
(GroundedCaptioning)MS-CXR 1.00
(GroundedDiagnosis)MS-CXR 1.00
(GroundedDiagnosis)VinDr-CXR 1.00
(GroundedDiagnosis)VinDr-PCXR 1.00
(GroundedPhraseExtraction)MS-CXR 1.00
(ImageClassification)COVIDX-CXR-3 1.00
(ImageClassification)NLM-TB 1.00
(ImageClassification)RSNA 1.00
(ImpressionGeneration)Candid 1.00
(Open-EndedVQA)MedVQA-2019 1.00
(Open-EndedVQA)PMC-VQA 1.00
(Open-EndedVQA)VQA-RAD 1.00
(PhraseGrounding)MS-CXR 1.00
(PneumothoraxSegmentation)Candid 1.00
(ProgressionFindingsGeneration)MIMIC-CXR 1.00
(ProgressionImpressionGeneration)MIMIC-CXR 1.00
(TextQA)RadQA 1.00
(ViewMatching)MIMIC-CXR 0.50
(FindingsGeneration)MIMIC-CXR-Struct 0.40
(ImpressionGeneration)MIMIC-CXR-Struct 0.30
(NaturalLanguageExplanation)MIMIC-NLE 0.30
(ReportGeneration)BIMCV-COVID19 0.25
(FindingsSummarization)MIMIC-III 0.20
(ImageClassification)ChestXray14 0.20
(Close-EndedVQA)MIMIC-CXR-VQA 0.10
(FindingsGeneration)MIMIC-CXR 0.10
(FindingsSummarization)MIMIC-CXR 0.10
(ImageClassification)Brax 0.10
(ImageClassification)CheXpert 0.10
(ImageClassification)PadChest 0.10
(Image-TextMatching)ROCO 0.10
(Image-TextSelection)ROCO 0.10
(ImpressionGeneration)MIMIC-CXR 0.10
(ReportGeneration)PadChest 0.10
(ViewClassification)MIMIC-CXR 0.10
(ImageClassification)MIMIC-CXR 0.05
(Open-EndedVQA)MIMIC-CXR-VQA 0.05
(LocalFindingsGeneration)MIMIC-CXR-Struct 0.02
(LocalImpressionGeneration)MIMIC-CXR-Struct 0.02
(TextInstructions)OpenOrca 0.02
(DifferenceVQA)MIMIC-Diff-VQA 0.01
(ImageClassification)CXR-LT 0.01
(Image-TextMatching)MIMIC-CXR 0.01
(Image-TextSelection)MIMIC-CXR 0.01
(AbnormalityDetection)VinDr-CXR 0.00
(AbnormalityDetection)VinDr-PCXR 0.00
(ChestTubeSegmentation)Candid 0.00
(Close-EndedVQA)Rad-Restruct 0.00
(Close-EndedVQA)SLAKE 0.00
(Open-EndedVQA)Rad-Restruct 0.00
(Open-EndedVQA)SLAKE 0.00
(PhraseExtractionandGrounding)MS-CXR 0.00
(PneumothoraxSegmentation)SIIM 0.00
(RibFractureSegmentation)Candid 0.00
23Table8:TraininghyperparametersofCheXagent.
Configuration Stage0 Stage1 Stage2 Stage3
ViTinit. - EVA01-CLIP-g-14-plus ViTStage1 ViTStage2
LLMinit. Mistral-7B-v0.1 - LLMStage0 LLMStage2
Qformerinit. - BERTBase QformerStage1 QformerStage2
Imageresolution - 4482 4482 4482
ViTsequencelength - 1024 1024 1024
LLMsequencelength 2048 - 512 512
Learnablequerynumbers - 128 128 128
Globalbatchsize 2048 256 2048 512
Optimizer AdamW
Optimizerhyperparameter Œ≤ =0.9,Œ≤ =0.999,eps=1e‚àí8
1 2
Peaklearningrate 5e‚àí6 1e‚àí4 1e‚àí4 1e‚àí6
Minimumlearningrate 5e‚àí7 1e‚àí5 1e‚àí5 1e‚àí7
Learningrateschedule cosine
Weightdecay 0.05
Gradientclip 1.0
Numericalprecision bf16
DeepSpeed ZeROstage2 - - ZeROstage2
Table9:StatisticsforimageperceptiontasksinCheXbench(EvaluationAxis1).
Task Dataset Num. Samples Num. Options
MIMIC-CXR 400 4
ViewClassification
CheXpert 300 3
SIIM 100 2
BinaryDiseaseClassification RSNA 100 2
CheXpert 233 2
OpenI 500 4
SingleDiseaseIdentification MIMIC-CXR 195 4
CheXpert 169 4
OpenI 807 4
MultiDiseaseIdentification MIMIC-CXR 300 4
CheXpert 280 4
Rad-Restruct 899 2-4
VisualQuestionAnswering
SLAKE 420 2
Image-TextReasoning OpenI 380 2
24