DiFu,FaresAbawi,PhilippAllgeuer,andStefanWermter.2024.HumanImpressionofHumanoidRobotsMirroringSocialCues.InCompanionofthe2024ACM/IEEEInternational
ConferenceonHuman-RobotInteraction(HRI’24Companion),March11-14,2024,Boulder,CO,USA.https://doi.org/10.1145/3610978.3640580.
Human Impression of Humanoid Robots Mirroring Social Cues
DiFu∗ FaresAbawi∗
di.fu@uni-hamburg.de fares.abawi@uni-hamburg.de
UniversityofHamburg UniversityofHamburg
Hamburg,Germany Hamburg,Germany
PhilippAllgeuer StefanWermter
philipp.allgeuer@uni-hamburg.de stefan.wermter@uni-hamburg.de
UniversityofHamburg UniversityofHamburg
Hamburg,Germany Hamburg,Germany
A B
C D
Figure1:Aparticipantperformingthefourmirroringtasksinrandomorder:A)TheiCubrobotmirroringfacialexpressions;
B)ThePepperrobotaffectivelysignalingthroughLEDcolorchanges;C)TheiCubrobotmirroringheadmovementbasedon
aninertialmeasurementunit(IMU)readings.TheredcircleshowstheIMU;D)TheiCubrobotmirroringheadmovement
accordingtoavision-basedmodel.Theredcircleshowsthecamera.
ABSTRACT controlledoneinthemovementmirroringtask.Ourfindingssug-
Mirroringnon-verbalsocialcuessuchasaffectormovementcan gestthatdifferentroboticplatformsimpactpeople’sperceptionof
enhancehuman-humanandhuman-robotinteractionsinthereal robots’mirroringduringHRI.Thecontrolmethodalsocontributes
world.Theroboticplatformsandcontrolmethodsalsoimpactpeo- totherobot’smirroringperformance.Ourworkshedslightonthe
ple’sperceptionofhuman-robotinteraction.However,limitedstud- designandapplicationofdifferenthumanoidrobotsinthereal
ieshavecomparedrobotimitationacrossdifferentplatformsand world.
controlmethods.Ourresearchaddressesthisgapbyconducting
twoexperimentscomparingpeople’sperceptionofaffectivemirror- CCSCONCEPTS
ingbetweentheiCubandPepperrobotsandmovementmirroring
•Human-centeredcomputing→Userstudies;Interactionde-
betweenvision-basediCubcontrolandInertialMeasurementUnit
signtheory,conceptsandparadigms.
(IMU)-basediCubcontrol.WediscoveredthattheiCubrobotwas
perceivedasmorehumanlikethanthePepperrobotwhenmirroring
affect.Avision-basedcontrollediCuboutperformedtheIMU-based KEYWORDS
affectivemirroring,movementmirroring,gazeandheadmovement,
∗Bothauthorscontributedequallytothisresearch. human-robotinteraction
ThisworkislicensedunderaCreativeCommonsAttribution ACMReferenceFormat:
International4.0License.
DiFu,FaresAbawi,PhilippAllgeuer,andStefanWermter.2024.Human
ImpressionofHumanoidRobotsMirroringSocialCues.InCompanionof
HRI’24Companion,March11–14,2024,Boulder,CO,USA
the2024ACM/IEEEInternationalConferenceonHuman-RobotInteraction
©2024Copyrightheldbytheowner/author(s).
ACMISBN979-8-4007-0323-2/24/03 (HRI’24Companion),March11–14,2024,Boulder,CO,USA.ACM,NewYork,
https://doi.org/10.1145/3610978.3640580 NY,USA,5pages.https://doi.org/10.1145/3610978.3640580
4202
naJ
22
]OR.sc[
1v67021.1042:viXraHRI’24Companion,March11–14,2024,Boulder,CO,USA DiFu,FaresAbawi,PhilippAllgeuer,andStefanWermter
1 INTRODUCTION Inthisstudy,weconductedtwoexperimentswithtwohumanoids,
The mirror neuron system (MNS) in humans facilitates the un- theiCubandPepperrobots,asshowninFigure1.Thefirstexper-
derstandingofothersbysimulatingtheirbehaviorsviasensori- iment compared people’s perceptions of affective mirroring on
motorprocesses[5].Mirroring,afundamentalelementofsocial differenthumanoidrobots.Thesecondexperimentassessedthe
interaction,involvessubconsciouslyimitatinganotherindividual’s impactofvariouscontrolmethodsonthesamerobotplatformdo-
nonverbalcues,suchasgestures,expressions,andpostures[10]. ingmovementmirroring.Weevaluatedtherobots’performance
Itcanreflectanadaptiveintegrationandutilizationofsocialcues bytheirmirroringspeedandaccuracy.People’sperceptionofthe
withinthesocialcontext[22].Thismechanismoftenleadsindi- robotswasmeasuredfromfourdimensions—SociallyIntelligent,
vidualstocollaboratewiththosewhoexhibitsimilarandfamiliar Mechanical,Responsive,andHumanlike.Throughtheseinvestiga-
behaviors[7].Mirrorsystemdysfunctioncontributestodifficulties tions,ourgoalistoenhancethealignmentofroboticdesignwith
in social communication for individuals with Autism Spectrum humaninteractionpreferences.Weaimtosolvetheseissuesby
Disorders (ASD) [18]. Mirroring also plays a significant role in investigatingthefollowingresearchquestions(RQ):
human-robotsocialinteraction.Bymimickingnon-verbalsocial RQ1 Howdodifferentroboticsplatforms,specificallytheiCub
cues,humansfeelsociallyclosertotherobotandperceiveitas andPepperrobots,compareinaffectivemirroring?
moreawareoftheintentionsbehindtheirsocialbehaviors[14]. RQ2 Howdovariousroboticcontrolmethods,especiallyvision-
For robots, affective mirroring causes people to perceive the basedcontrolledandIMU-basedcontrolledmethods,impact
robotasanagentcapableofconveyinginternalstates,displaying theiCubrobot’sperformanceinmovementmirroringtasks?
socialintelligence,andexpressinghumanlikecharacteristics[3,6].
Gonsioretal.[11]investigatedtheimpactofmirroringfacialex- 2 STUDYDESIGN
pressionsonempathyandperceivedsubjectiveperformancein 2.1 AffectiveMirroringTask
interactionswiththerobotheadEDDIE[21],revealingthatadap-
Inthisexperiment,participantswereaskedtomakeeightfacial
tivemodesofrobotbehavior,wheretherobotmirroredhuman
expressions—Anger,Fear,Happiness,Disgust,Sadness,Neutral,Sur-
expressions, led to increased levels of human empathy and im-
prise,andContempt—infrontofthePepperoriCubrobots.The
provedperceivedtaskperformancecomparedtoanon-adaptive
expressionsweretobeperformedwithinoneminuteinanyorder.
mode—withoutfacialexpressionmimicry.Althoughmostprevious
Therobotmirroredparticipants’expressionseitherthroughaffec-
researchshowsconsistentfindings,fewstudiescomparepeople’s
tivesignaling—bychangingthePepperrobot’seyeandshoulder
perceptionsofaffectivemirroringondifferenthumanoidrobots.
LEDcolors[13,16]—orroboticfacialexpressions—bychangingthe
Robotsconveyemotionalsignalsinvariousways.Forinstance,the
iCubrobot’seyebrowandmouthLEDpatterns[2].Next,partici-
iCubrobotcandisplaysimplifiedfacialexpressionswithLEDlight
pantswereaskedtomatchthecolorsdisplayedonthePepperrobot
patternchanges,andthePepperrobotcanchangethecolorofthe
(depictedinthetoprowofFigure2)andfacialexpressionsonthe
shoulderandeyelidstorepresentemotions.Itmaycausepeopleto
iCubrobot(depictedinthebottomrowofFigure2)toemotioncat-
interpretthemdifferentlyforthesameexpression.
egories.Technicaldetailsforrunningtheexperimentareprovided
Movementmirroringenhancesrobots’sociabilityduringhuman-
aspartoftheWrapyfi[1]tutorialseries1.
robotinteractions,makingthemmorehumanlike,empathetic,and
Uponcompletionofthetask,participantswereaskedtoscana
sociallyintelligent[4].Twoprimarymethodsofenablingrobots
QRcodeappearingonthePepper’stabletusingtheircellphonesto
tomirrorhumanmovementsincludeIMU-basedcontrolledand
completeathree-itemquestionnaire,evaluatingtheirexperiences
vision-basedcontrolledimitations.IMU-basedcontrolledmirroring
witheitherrobot.Inbothquestionnaires,participantswereasked
usesreadingsfromanIMUattachedtoahead-mountedeyetracker
toratetheirinteractionwiththerobotsusinga5-pointLikertscale:
wornbyanactortodirectlytranslatetheirheadmovementsinto
roboticactions[9].Incontrast,vision-basedcontrolledmirroring Q1 Howprecisewastherobotinmirroringyourfacialexpres-
usesexternalcamerasandposeestimationalgorithmstointerpret sions?(1=veryimprecise,5=veryprecise)
anactor’sheadmovementsandmirrorthemthrougharobot[8]. Q2 Didtherobotmirroryourexpressionswithmajordelay?(1=
Liuetal.[17]showthatthelightweightmodelsurpassestheother nosignificantdelay,5=significantdelay)
state-of-the-artmodelsonthesamerobotdoingtheheadmovement Participantsratedtheirimpressionoftherobotsonfourdimensions
mirroring.Geminianietal.[9]findthattheMicrosoftKinect-based —SociallyIntelligent,Mechanical,Responsive,andHumanlike—using
controlledNAOrobotoutperformstheIMU-basedcontrolledNAO a5-pointLikertscale(1=notatall,5=yes,alot).
robotregardinglimbmovementmirroringintheautismtreatment.
However,comparingdifferentcontrolmethodsofrobotsondoing 2.2 GazeandHeadMovementMirroringTask
headandgazemirroringremainstobestudied. In this experiment, participants interacted with the iCub robot
Socialrobotsaredesignedtoaidpeople,butindividualshave giventwoconditions.Underthevision-basedcontrolledcondition,
beenadaptingtotherobotsinstead.Thisisduetothefactthat theiCubrobot’smovementswereactuatedbyavision-basedhead
robotsarenotalwaysdesignedwithhumanpreferencesandinterac- poseestimationmodel.Undertheinertialmeasurementunit(IMU)
tiveneeds[15,19].Researchersinroboticmirroringareconstantly controlledcondition,theorientationreadingsarrivedinsteadfrom
improvinghumanoidrobots’accuracyandtimelinessinsimulat- anIMUattachedtoawearableeyetracker.Participantsworethe
ingsocialcues.However,researchaboutsubjectiveevaluationand eyetrackerandwereaskedtolookattheiCubrobot,freelymoving
preferenceoftheroboticplatformandcontrolmethodislimited.
1https://wrapyfi.readthedocs.io/en/latest/tutorials/Multiple%20Robots.htmlHumanImpressionofHumanoidRobotsMirroringSocialCues HRI’24Companion,March11–14,2024,Boulder,CO,USA
theireyesandhead.Participantsobservedthemovementsofthe toeachdimensionofthequestionnaires.Resultsshowedthattheir
iCubrobottoevaluatetheinteraction.Technicaldetailsforrunning responseswerenormallydistributed.Inaddition,allPosthoctests
the experiment are provided as part of the Wrapyfi [1] tutorial inthisstudyusedBonferronicorrection.
series2.
ParticipantswereaskedtoratetheirinteractionwiththeiCub 3.1 AffectiveMirroring
robotusinga5-pointLikertscale:
Fortheaffectivemirroringtaskoneitherrobot,therecognition
Q1 Howprecisewastherobotinmirroringyourheadmovements? accuracyislistedinFigure2.ForthePepperrobot,participants
(1=veryimprecise,5=veryprecise) weremostaccurateinrecognizinganger(86.2%)andleastaccurate
Q2 Didtherobotmirroryourheadmovementswithmajordelay? inrecognizingfear(3.4%).FortheiCubrobot,participantswere
(1=nosignificantdelay,5=significantdelay) mostaccurateinrecognizinghappiness(100%)andleastaccurate
Q3 Didtherobotmoveitseyes?(Yes/No) inrecognizingdisgust(16.7%).
Q4 Howprecisewastherobotinmirroringyoureyemovements? Forparticipants’ratingofinteractionwiththerobots,results
(1=veryimprecise,5=veryprecise) ofpaired-samples𝑡-testsdisplayednosignificantdifferenceinpre-
Q5 Didtherobotmirroryoureyemovementswithmajordelay? cision (Q1) between the Pepper (mean ± SE = 2.79±.18) and
(1=nosignificantdelay,5=significantdelay) iCub(mean ± SE=2.90±.15)robots,(𝑡(28) = .46,𝑝 = .65).No
ParticipantsratedtheirimpressionoftheiCubrobotonfourdimen- significantdifferenceindelay(Q2)wasfoundbetweenthePepper
sions—SociallyIntelligent,Mechanical,Responsive,andHumanlike— (mean ± SE = 2.38±.18) and iCub (mean ± SE = 2.48±.20)
usinga5-pointLikertscale(1=notatall,5=yes,alot).
robots,(𝑡(28)=.52,𝑝 =.61).Forparticipants’ratingoftheimpres-
sionoftherobots,resultsofpaired-samples𝑡-testsdisplayedthat
2.3 ExperimentalSetup theiCub(mean ± SE = 2.86±.20)robotwasratedsignificantly
morehumanlikethanthePepper(mean ± SE=2.10±.16)robot,
Theparticipantswereseated80cmawayfromtheiCubrobot’s
(𝑡(28) =3.45,𝑝 < .01).Nosignificantdifferenceswerefoundfor
head,adjustingitsheighttomatchtheireyelevel.Acircularmarker
theotherthreedimensions—SociallyIntelligent,Mechanical,and
wasplacedbesidetheiCubrobottocalibratethePupilCoreeye
Responsive—betweenthetworobots(𝑝𝑠 >.05)(SeeTable1).
tracker.SituatedinfrontoftheiCubrobotwasaLogitechC920
webcamfacingtheparticipantstoperformtasksrequiringafixed
3.2 MovementMirroring
viewoftheirfaceswhiletheiCubrobotmoveditsheadandeyes.
ThePepperrobotstoodfacingtheparticipantsatanangleof45 Apaired-samples𝑡-testsshowedthatparticipantsratedthevision-
degreeswithadistanceof1.2m.ThePepperrobotdisplayedan basedcontrolledrobot(mean ± SE=3.55±.24)significantlymore
illustrationoftheongoingtaskonitstabletandcommunicatedthe precise(Q1)thantheIMU-basedcontrolledrobot(mean ± SE=
instructionsverbally.Theinteractionwasoneminutelongpertask 2.90±.19),(𝑡(26) = 2.19,𝑝 < .05).Thevision-basedcontrolled
conditionandtheconditionorderwasrandomized.Weusedthe robot(mean ± SE=2.00±.17)wasratedsignificantlylessdelayed
Wrapyfi[1]frameworkformanagingthetaskorder,transmitting (Q2)thantheIMU-basedcontrolledrobot(mean±SE=2.66±.21),
databetweenmodelsandrobotsusingvariousmiddleware,and (𝑡(26) = −3.09,𝑝 < .01).Underthevision-basedcontrolledcon-
orchestratingtheexperimentalpipeline. dition,allparticipantsobservedthattherobotmirroredtheireye
movements,whereastwodidnotundertheIMU-basedcontrolled
2.4 Participants condition(Q3).Therefore,weonlyanalyzeddatafrom27partici-
pantswhoreportedobservingeyemovementunderbothconditions.
30participants(female=7,male=22,preferrednottosay=1)
Thepaired-samples𝑡-testshowednosignificantdifferenceinthe
took part in both studies. Participants were between 24 and 41
precisionratingoftheeyemovementbetweenthevision-basedcon-
yearsofage,withameanageof28.7.Allparticipantsreported
trolledrobot(mean±SE=2.48±.19)andtheIMU-basedcontrolled
nohistoryofneurologicalconditions—seizures,epilepsy,stroke,
robot(mean±SE=2.37±.19)(𝑝 >.05)(Q4).Also,nosignificantdif-
etc.—andhadnormalorcorrected-to-normalvisionandhearing.
ferencewasfoundinthedelayratingoftheeyemovementbetween
Oneparticipant’sdatawasexcludedfromthePepperrobot’saffec-
thevision-basedcontrolledrobot(mean ± SE = 3.07±.23)and
tivemirroringexperimentbecauseofself-reportedcolorblindness.
theIMU-basedcontrolledrobot(mean ± SE=3.48±.24)(𝑝 >.05)
Another participant’s data was excluded from the iCub robot’s
(Q5).Fortheimpressionoftherobot,participantsreportedthat
movementmirroringexperimentduetotechnicalissues.Thisstudy
thevision-basedcontrollediCub(mean ± SE=3.66±.22)robot
adheredtotheprinciplesexpressedintheDeclarationofHelsinki.
wassignificantlymoreresponsivethantheIMU-controlledrobot
ParticipantssignedconsentformsapprovedbytheEthicsCommit-
(mean ± SE = 3.17±.21),(𝑡(26) = 2.39,𝑝 < .05).However,no
teeattheDepartmentofInformatics,UniversityofHamburg.
significantdifferenceswerefoundintheremainingdimensions
3 RESULTS —SociallyIntelligent,Mechanical,andHumanlike—betweenthetwo
conditions(𝑝𝑠 >.05)(resultsareshowninTable1).
We evaluated the results of both mirroring tasks, studying the
perceivedimpressionoftherobotineachseparatecondition,as
4 DISCUSSION
wellascomparingthepairedconditionswithineachrespective
task.Normalitytestswereconductedontheparticipants’answers ParticipantsassociatedtheiCubrobot’sfacialexpressionswithemo-
tionsmorethanthePepperrobot’saffectivesignalingandfound
2https://wrapyfi.readthedocs.io/en/latest/tutorials/Multiple%20Sensors.html theiCubrobotmorehumanlike.AnotherobservationrelatestoHRI’24Companion,March11–14,2024,Boulder,CO,USA DiFu,FaresAbawi,PhilippAllgeuer,andStefanWermter
Anger Fear Happiness Disgust Sadness Neutral Surprise Contempt
86.2% 3.4% 65.5% 24.1% 34.5% 37.9% 10.3% 6.9%
73.3% 46.7% 100% 16.7% 26.7% 80.0% 60.0% 20.0%
Figure2:EightemotioncategoriesmimickedonthePepper(Top)andiCub(Bottom)robotsintheformofaffectivesignaling
androboticfacialexpressions,respectively.Resultsofthehumanstudyarereportedbeloweachimageintermsoftheaverage
accuracyinmatchingeachaffectivesignalorfacialexpressiontoanemotioncategory.
5.0 5.0
Affect iCub Mov(Model) iCub
4.5 4.5
Affect Pepper Mov(IMU) iCub
4.0 4.0 *
3.5 3.5
**
3.0 3.0
2.5 2.5
2.0 2.0
1.5 1.5
1.0 1.0
Soc. Intelligent Mechanical Responsive Humanlike Soc. Intelligent Mechanical Responsive Humanlike
(a)AffectiveMirroring (b)MovementMirroring
Figure3:Participants’impressions(5-pointLikertscale)ofrobotsunderdifferentaffectiveandmovementmirroringconditions.
∗denotes.01<𝑝<.05,and∗∗.001<𝑝<.01
Table1:Impressionoftherobotsunderdifferenttaskcondi- perceivedasequallyhumanlike,implyingthatlessresponsiveness
tions(Mean±SE)
doesnotcontradicthumanlikeness.
Severallimitationscouldbeaddressedandinvestigatedinfuture
Affect Affect Mov.(Model) Mov.(IMU) research.Wecouldnotcomparemovementmirroringonthetwo
iCub Pepper iCub iCub humanoidrobots.ThisisbecausethePepperrobotisnotableto
Soc.Intelligent 2.81±.22 2.89±.21 2.65±.20 2.46±.22 roll its head or move its eyes, unlike the iCub robot. Our iCub
Mechanical 3.08±.24 2.93±.21 3.65±.24 3.85±.18
robotdoesn’thaveafullbody,hence,wecannotstudythelimb
Responsive 3.31±.21 3.19±.16 3.65±.15 3.23±.22
mirroringbetweenthetworobots.Futurestudiescouldaddress
Humanlike 2.81±.22 2.15±.16 2.46±.19 2.39±.21
theinteractioneffectbetweenaffectiveandmovementmirroring.
Moreover,researcherscouldinvestigatehowdifferenthumanoid
robotsandcontrolmethodsimpactchildrenwithASD,andwhether
theaccuracyofrecognizingdifferentaffectivesignalsconveyedby
itaffectstheirsocialfunctions[24].
eitherrobot.ParticipantscouldaccuratelyassociateAngerwiththe
colorredandHappinesswithgreenonthePepperrobot.Thisis
5 CONCLUSIONS
complementedbyfindingsassociatingexposuretodifferentcolors
withphysiologicalandpsychologicalresponses[20,23].Partici- Weinvestigatedhumanperceptionsoftwohumanoidrobotsinthe
pantsmoreaccuratelyidentifiedexpressionsofHappiness,Neutral, affectiveandmovementmirroringtasks.Ourfindingsrevealedthat
andSurpriseontheiCubrobotcomparedtothePepperrobot.This arobotdisplayingfacialexpressionslikeaniCubrobotwasper-
canbeattributedtohumansprimarilyrelyingonobservingthe ceivedasmorehumanlikethanarobotconveyingaffectivesignals
mouth and eyebrows to recognize these facial expressions [12], likeaPepperrobot.Forgazeandheadmirroring,avision-based
featuresthatthePepperrobotlacks. controlledrobotperformedbetterthananIMU-basedcontrolled
Wecomparedtwomovementmirroringmethods.Thevision- robot.Thiscouldbeattributedtolatencyinprocessingandtrans-
basedcontrolledmethodproducedsmoother,moreprecise,and mittingthefilteredIMUreadings.Insummary,weshowedthat
moreresponsivemovementsthantheIMU-basedcontrolledmethod. roboticplatformsandrobotcontrolmethodsplayedanessential
TheIMU-basedcontrolledmethodtransferstheIMUreadingsat roleinmirroringtasksduringHRI.Itmayguidefuturehumanoid
afasterrate,butthiscausesjitterymovementsduetohardware robotdesigndecisionstoalignwithhumans’needs.
limitations.ThesefindingsarealsoconsistentwithGeminiaiet
ACKNOWLEDGMENTS
al.[9]thattheIMU-basedNAOrobotismoreintrusiveandrequires
longersetuptimethantheKinect-basedNAOrobotduringthelimb TheauthorsgratefullyacknowledgepartialsupportfromtheGer-
movementmirroring.However,inourstudy,bothmethodswere manResearchFoundationDFGunderprojectCML(TRR169).
reppeP
buCiHumanImpressionofHumanoidRobotsMirroringSocialCues HRI’24Companion,March11–14,2024,Boulder,CO,USA
REFERENCES
InternationalWorkshoponRobotandHumanInteractiveCommunication(RO-
[1] FaresAbawi,PhilippAllgeuer,DiFu,andStefanWermter.2024. Wrapyfi:A MAN).IEEE,350–356. https://doi.org/10.1109/ROMAN.2011.6005294
PythonWrapperforIntegratingRobots,Sensors,andApplicationsacrossMultiple [12] MariaGuarnera,ZiraHichy,MauraCascio,StefanoCarrubba,andStefaniaL
Middleware.InACM/IEEEInternationalConferenceonHuman-RobotInteraction Buccheri.2017.Facialexpressionsandtheabilitytorecognizeemotionsfromthe
(HRI).ACM. https://doi.org/10.1145/3610977.3637471 eyesormouth:acomparisonbetweenchildrenandadults.TheJournalofGenetic
[2] MotonobuAoki,KarthikeyanKalyanasundaramBalasubramanian,DiegoTorazza, Psychology178,6(2017),309–318. https://doi.org/10.1080/00221325.2017.1361377
FrancescoRea,DoreenJirak,GiulioSandini,TakuraYanagi,AtsushiTakamatsu, [13] DavidOJohnson,RaymondHCuijpers,andDavidvanderPol.2013.Imitating
StephaneBouet,andTomohiroYamamura.2022.ANovelWire-driven3DEye- humanemotionswithartificialfacialexpressions.InternationalJournalofSocial
browDesignforCommunicationwithHumanoidRobotiCub.In2022IEEE/RSJ Robotics5(2013),503–513.
InternationalConferenceonIntelligentRobotsandSystems(IROS).IEEE,8248– [14] JamyLi,WendyJu,andCliffNass.2015.ObserverPerceptionofDominanceand
8254. MirroringBehaviorinHuman-RobotRelationships.InACM/IEEEInternational
[3] CynthiaBreazeal,KerstinDautenhahn,andTakayukiKanda.2016.Socialrobotics. ConferenceonHuman-RobotInteraction(HRI).ACM,133–140. https://doi.org/10.
SpringerHandbookofRobotics(2016),1935–1972. https://doi.org/10.1007/978-3- 1145/2696454.2696459
319-32552-1_72 [15] VelvetinaLim,MakiRooksby,andEmilySCross.2021. SocialRobotsona
[4] NicoletaBugnariu,CarolynYoung,KatelynRockenbach,RitaMPatterson,Car- GlobalStage:EstablishingaRoleforCultureDuringHuman-RobotInteraction.
olynGarver,IsuraRanatunga,MonicaBeltran,NahumTorres-Arenas,and InternationalJournalofSocialRobotics13,6(2021),1307–1333. https://doi.org/
DanPopa.2013. Human-robotinteractionasatooltoevaluateandquan- 10.1007/s12369-020-00710-4
tifymotorimitationbehaviorinchildrenwithAutismSpectrumDisorders. [16] Pei-Chun Lin, Patrick CK Hung, Ying Jiang, Carolina Padilla Velasco, and
In2013InternationalConferenceonVirtualRehabilitation(ICVR).IEEE,57–62. MarcoAntonioMartínezCano.2023. Anexperimentaldesignforfacialand
https://doi.org/10.1109/ICVR.2013.6662088 coloremotionexpressionofasocialrobot.TheJournalofSupercomputing79,2
[5] EvanWCarrandPiotrWinkielman.2014. Whenmirroringisbothsimple (2023),1980–2009.
and“smart”:howmimicrycanbeembodied,adaptive,andnon-representational. [17] XiaofengLiu,YizhouChen,JieLi,andAngeloCangelosi.2022.Real-TimeRobotic
FrontiersinHumanNeuroscience8(2014),505. https://doi.org/10.3389/fnhum. MirroredBehaviorofFacialExpressionsandHeadMotionsBasedonLightweight
2014.00505 Networks.IEEEInternetofThingsJournal10,2(2022),1401–1413. https://doi.
[6] LuisaDamiano,PaulDumouchel,andHagenLehmann.2015.Towardshuman– org/10.1109/JIOT.2022.3205123
robotaffectiveco-evolutionovercomingoppositionsinconstructingemotions [18] JellinaPrinsenandKaatAlaerts.2022.Brokenorsociallymistunedmirroringin
andempathy. InternationalJournalofSocialRobotics7(2015),7–18. https: ASD?Aninvestigationviatranscranialmagneticstimulation.AutismResearch
//doi.org/10.1007/s12369-014-0258-7 15,6(2022),1056–1067. https://doi.org/10.1002/aur.2720
[19] SelmaŠabanović.2010.RobotsinSociety,SocietyinRobots:MutualShapingof
[7] HinkeMEndedijk,MMeyer,HBekkering,AHNCillessen,andSabineHunnius.
SocietyandTechnologyasaFrameworkforSocialRobotDesign.International
2017.Neuralmirroringandsocialinteraction:Motorsysteminvolvementduring
JournalofSocialRobotics2,4(2010),439–450. https://doi.org/10.1007/s12369-
actionobservationrelatestoearlypeercooperation.DevelopmentalCognitive
010-0066-7
Neuroscience24(2017),33–41. https://doi.org/10.1016/j.dcn.2017.01.001
[20] SichaoSongandSeijiYamada.2017.ExpressingEmotionsthroughColor,Sound,
[8] MarcoFerro,AntonioPaolillo,AndreaCherubini,andMarilenaVendittelli.2019.
andVibrationwithanAppearance-ConstrainedSocialRobot.InACM/IEEEIn-
Vision-BasedNavigationofOmnidirectionalMobileRobots.IEEERoboticsand
ternationalConferenceonHuman-RobotInteraction(HRI).ACM,2–11. https:
AutomationLetters4,3(2019),2691–2698. https://doi.org/10.1109/LRA.2019.
//doi.org/10.1145/2909824.3020239
2913077
[21] StefanSosnowski,AnsgarBittermann,KoljaKühnlenz,andMartinBuss.2006.
[9] AliceGeminiani,LauraSantos,ClaudiaCasellato,AndreaFarabbi,NicolaFarella,
DesignandEvaluationofEmotion-DisplayEDDIE.InIEEE/RSJInternational
JoséSantos-Victor,IvanaOlivieri,andAlessandraPedrocchi.2019.Designand
ConferenceonIntelligentRobotsandSystems(IROS).IEEE,3113–3118. https:
validationoftwoembodiedmirroringsetupsforinteractivegameswithautis-
//doi.org/10.1109/IROS.2006.282330
ticchildrenusingtheNAOhumanoidrobot.In201941stAnnualInternational
[22] LynMVanSwol.2003. TheEffectsofNonverbalMirroringonPerceivedPer-
ConferenceoftheIEEEEngineeringinMedicineandBiologySociety(EMBC).IEEE,
suasiveness,AgreementwithanImitator,andReciprocityinaGroupDiscus-
1641–1644. https://doi.org/10.1109/EMBC.2019.8857576
sion. CommunicationResearch30,4(2003),461–480. https://doi.org/10.1177/
[10] GyörgyGergely.2018.Thesocialconstructionofthesubjectiveself:Theroleof
0093650203253318
affect-mirroring,markedness,andostensivecommunicationinself-development.
[23] LisaWilmsandDanielOberfeld.2018. Colorandemotion:effectsofhue,sat-
InDevelopmentalscienceandpsychoanalysis.Routledge,45–88. https://doi.org/
uration,andbrightness. PsychologicalResearch82,5(2018),896–914. https:
10.4324/9780429473654-4
//doi.org/10.1007/s00426-017-0880-8
[11] BarbaraGonsior,StefanSosnowski,ChristophMayer,JürgenBlume,BerndRadig,
[24] ZhiZheng,EricMYoung,AmyRSwanson,AmySWeitlauf,ZacharyEWarren,
DirkWollherr,andKoljaKühnlenz.2011. Improvingaspectsofempathyand
andNilanjanSarkar.2015.Robot-MediatedImitationSkillTrainingforChildren
subjectiveperformanceforHRIthroughmirroringfacialexpressions.InIEEE
WithAutism.IEEETransactionsonNeuralSystemsandRehabilitationEngineering
24,6(2015),682–691. https://doi.org/10.1109/TNSRE.2015.2475724