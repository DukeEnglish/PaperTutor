UNIVERSAL NEURONS IN GPT2 LANGUAGE MODELS
WesGurnee1∗ TheoHorsley2 ZifanCarlGuo1 TaraRezaeiKheirkhah1
QinyiSun1 WillHathaway1 NeelNanda† DimitrisBertsimas1†
1MIT 2UniversityofCambridge
ABSTRACT
Abasicquestionwithintheemergingfieldofmechanisticinterpretabilityisthedegreetowhichneural
networkslearnthesameunderlyingmechanisms. Inotherwords,areneuralmechanismsuniversal
acrossdifferentmodels?Inthiswork,westudytheuniversalityofindividualneuronsacrossGPT2
modelstrainedfromdifferentinitialrandomseeds,motivatedbythehypothesisthatuniversalneurons
arelikelytobeinterpretable.Inparticular,wecomputepairwisecorrelationsofneuronactivationsover
100milliontokensforeveryneuronpairacrossfivedifferentseedsandfindthat1-5%ofneuronsare
universal,thatis,pairsofneuronswhichconsistentlyactivateonthesameinputs.Wethenstudythese
universalneuronsindetail,findingthattheyusuallyhaveclearinterpretationsandtaxonomizethem
intoasmallnumberofneuronfamilies.Weconcludebystudyingpatternsinneuronweightstoestablish
severaluniversalfunctionalrolesofneuronsinsimplecircuits:deactivatingattentionheads,changing
theentropyofthenexttokendistribution,andpredictingthenexttokento(not)bewithinaparticularset.
1 Introduction
Aslargelanguagemodels(LLMs)becomemorewidelydeployedinhigh-stakessettings,ourlackofunderstandingof
whyorhowmodelsmakedecisionscreatesmanypotentialvulnerabilitiesandrisks(Bommasanietal.,2021;Hendrycks
etal.,2023;Bengioetal.,2023).Whilesomeclaimdeeplearningbasedsystemsarefundamentallyinscrutable,artificial
neuralnetworksseemunusuallyamenabletoempiricalsciencecomparedtoothercomplexsystems: theyarefully
observable,(mostly)deterministic,createdbyprocesseswecontrol,admitcompletemathematicaldescriptionsoftheir
formandfunction,canberunonanyinputwitharbitrarymodificationsmadetotheirinternals,allatlowcostandon
computationaltimescales(Olah,2021).Anadvancedscienceofinterpretabilityenablesamoreinformeddiscussionof
therisksposedbyadvancedAIsystemsandlaysfirmergroundtoengineersystemslesslikelytocauseharm(Doshi-Velez
andKim,2017;Benderetal.,2021;Weidingeretal.,2022;Ngoetal.,2023;Carlsmith,2023).
Olahetal.(2020b)proposethreespeculativeclaimsregardingtheinterpretationofartificialneuralnetworks: that
features—directionsinactivationspacerepresentingproperitesoftheinput—arethefundamentalunitofanalysis,that
featuresareconnectedintocircuitsvianetworkweights,andthatfeaturesandcircuitsareuniversalacrossmodels.That
is,analogousfeaturesandcircuitsforminadiversearrayofmodelsandthatdifferenttrainingtrajectoriesconvergeon
similarsolutions(Lietal.,2015).Takenseriously,thesehypothesessuggestastrategyfordiscoveringimportantfeatures
andcircuits: lookforthatwhichisuniversal. Thislineofreasoningmotivatesourwork,whereweleveragedifferent
notionsofuniversalitytoidentifyandstudyindividualneuronsthatrepresentfeaturesorunderliecircuits.
Beyonddiscovery,thedegreetowhichneuralmechanismsareuniversalisabasicopenquestionthatinformswhatkinds
ofinterpretabilityresearcharemostlikelytobetractableandimportant. Iftheuniversalityhypothesisislargelytrue
inpractice,wewouldexpectdetailedmechanisticanalyses(Cammarataetal.,2021;Wangetal.,2022a;Olssonetal.,
2022;Nandaetal.,2023;McDougalletal.,2023)togeneralizeacrossmodelssuchthatitmightbepossibletodevelop
aperiodictableofneuralcircuitswhichcanbeautomaticallyreferencedwheninterpretingnewmodels(Olahetal.,
2020b). Conversely,itbecomeslesssensibletodedicatesubstantialmanuallabortounderstandlow-leveldetailsof
circuitsiftheyarecompletelydifferentineverymodel,andinsteadmoreefficienttoallocateefforttoengineeringscalable
andautomatedmethodsthatcanaidinunderstandingandmonitoringhigher-levelrepresentationsofparticularinterest
(Burnsetal.,2022;Conmyetal.,2023;Billsetal.,2023;Zouetal.,2023;Brickenetal.,2023).However,eveninthecase
∗Correspondence:wesg@mit.edu;†SeniorAuthor
4202
naJ
22
]GL.sc[
1v18121.1042:viXraFigure1:UniversalneuronsinGPT2models,interpretedviatheiractivations(a-d),weights(e),andcausalinterventions
(f).(a)Neuronswhichactivateprimarilyonaspecificindividualletterandsecondarilyontokenswhichbeginwiththe
letter;(b)Neuronwhichactivatesapproximatelyifandonlyiftheprevioustokencontainsacomma;(c)Neuronswhich
activateasafunctionofabsolutetokenpositioninthecontext(shadedareadenotesstandarddeviationaroundthemean);
(d)Aneuronwhichactivatesinmedicalcontexts(e.g.pubmedabstracts)butnotinnon-medicaldistributions;(e)aneuron
whichdecreasestheprobabilityofpredictinganyintegertokensbetween1700and2050(i.e.,years);(f)Neuronswhich
changetheentropyofthenexttokendistributionwhencausallyintervened.
thatnotallfeaturesorcircuitsareuniversal,thosewhicharecommonacrossmodelsarelikelytobemorefundamental
(Bauetal.,2018;Olssonetal.,2022),andstudyingthemshouldbeprioritizedaccordingly.
Inthiswork,westudytheuniversalityofindividualneuronsacrossGPT2languagemodels(Radfordetal.,2019)trained
fromfivedifferentrandominitializations(Karamchetietal.,2021).Whileitiswellknownthatindividualneuronsareoften
polysemantic(Nguyenetal.,2016;Olahetal.,2020b;Elhageetal.,2022b;Gurneeetal.,2023)i.e.,representmultiple
unrelatedconcepts,wehypothesizedthatuniversalneuronsweremorelikelytobemonosemantic,potentiallygivingan
approximationonthenumberofindependentlymeaningfulneurons.Wechoosetostudymodelsofthesamearchitecture
trainedonthesamedatatohavethemostfavorableexperimentalconditionsformeasuringuniversalitytoestablisharough
boundfortheuniversalityoverlargerchanges.Webeginbyoperationalizingneuronuniversalityintermsofactivationcor-
relations,thatis,whetherthereexistpairsofneuronsacrossdifferentmodelswhichconsistentlyactivateonthesameinputs.
Wecomputepairwisecorrelationsofneuronactivationsover100milliontokensforeveryneuronpairacrossthedifferent
seedsandfindthatonly1-5%ofneuronspassatargetthresholdofuniversalitycomparedtorandombaselines(§4.1).We
thenstudytheseuniversalneuronsindetail,analyzingvariousstatisticalpropertiesofbothweightsandactivations(§4.2),
andfindthattheyusuallyhaveclearinterpretationsandtaxonomizethemintoasmallnumberofneuronfamilies(§4.3).
InSection5westudyamoreabstractformofuniversalityintermsofneuronweightsratherthanactivations. Thatis,
ratherthanunderstandaneuronintermsoftheinputswhichcauseittoactivate,understandaneuronintermsofthe
effectstheneuronhasonlatermodelcomponentsordirectlyonthefinalprediction.Specifically,weanalyzepatterns
inthecompositionalstructureoftheweightsandfindconsistentoutliersinhowneuronsaffectothernetworkcomponents,
constitutingverysimplecircuits. InSection5.1,weshowthereexistsalargefamilyoflatelayerneuronswhichhave
clearrolesinpredictingorsuppressingacoherentsetoftokens(e.g.,second-personpronounsorsingledigitnumbers),
wherethesuppressionneuronstypicallycomeinlaterlayersthanthepredictionneurons.Wetheninvestigateasmall
setofneuronsthatleveragethefinallayer-normoperationtomodulatetheoverallentropyofthenexttokenprediction
distribution(§5.2).Weconcludewithananalysisofneuronswhichcontroltheextenttowhichanattentionheadattends
tothefirsttoken,whichempiricallycontrolstheoutputnormofthehead,effectivelyturningaheadonoroff(§5.3).
22 RelatedWork
UniversalNeuralMechanisms Featuresandcircuitslikehigh-lowfrequencydetectors(Schubertetal.,2021a)and
curvecircuits(Cammarataetal.,2021)havebeenfoundtoreoccurinvisionmodels,withsomefeaturesevenreappearing
inbiologicalneuralnetworks(Gohetal.,2021).Inlanguagemodels,recentresearchhasfoundsimilarlyuniversalcircuits
andcomponentslikeinductionheads(Olssonetal.,2022)andsuccessorheads(Gouldetal.,2023)andthatmodelsreuse
certaincircuitcomponentstoimplementdifferenttasks(Merulloetal.,2023).Therehasalsobeenaflurryofrecentwork
onstudyingmoreabstractuniversalmechanismsinlanguagemodelslikefunctionvectors(Toddetal.,2023;Hendeletal.,
2023),variablebindingmechanisms(FengandSteinhardt,2023),andlongcontextretrieval(VariengienandWinsor,
2023). Studyinguniversalityintoymodelshasprovided“mixedevidence”ontheuniversalityhypothesis(Chughtai
etal.,2023)andshownthatmultiplealgorithmsexisttoimplementthesametasks(Zhongetal.,2023;Liaoetal.,2023).
RepresentationalSimilarity Precedingthestatementoftheuniversalityhypothesisinmechanisticinterpretability,
therehasbeensubstantialworkmeasuringrepresentationalsimilarity(Klabundeetal.,2023).Commonmethodsinclude
canonicalcorrelationanalysis-basedmeasures(Raghuetal.,2017;Morcosetal.,2018),alignment-basedmeasures
(Hamiltonetal.,2018;Dingetal.,2021;Williamsetal.,2022;Duongetal.,2023),matrix-basedmeasures(Kornblithetal.,
2019;Tangetal.,2020;Shahbazietal.,2021;Lin,2022;Boix-Adseraetal.,2022;Godfreyetal.,2023),neighborhood-
basedmeasures(HryniowskiandWong,2020;GwilliamandShrivastava,2022),topology-basedmeasures(Khrulkovand
Oseledets,2018;Barannikovetal.,2022),anddescriptivestatistics(WangandIsola,2022;Luetal.,2022;Langeetal.,
2022).Previouswork,mostlyinvisionmodels,hasyieldedmixedconclusionsonwhethernetworkswiththesamearchi-
tecturelearnsimilarrepresentations.Somestudieshavefoundthatnetworkswithdifferentinitializations“exhibitverylow
similarity”(Wangetal.,2018)and“donotconvergetoauniquebasis”(Brownetal.,2023),whileothershaveshownthat
networkslearnthesamelow-dimensionalsubspacesbutnotidenticalbasisvectors(Lietal.,2016)andthatdifferentmodels
canbelinearlystitchedtogetherwithminimallosssuggestingtheylearnsimilarrepresentations(Bansaletal.,2021).
AnalyzingIndividualNeurons Manypriorinterpretabilitystudieshaveanalyzedindividualneurons.Invisionmodels,
researchershavefoundneuronswhichactivateforspecificobjects(Bauetal.,2020),curvesatspecificorientations
(Cammarataetal.,2021),highfrequencyboundaries(Schubertetal.,2021b),multimodalconcepts(Gohetal.,2021),
aswellasforfacets(Nguyenetal.,2016)andcompositions(MuandAndreas,2020)thereof.Moreover,manyofthese
neuronsseemuniversalacrossmodelsDravidetal.(2023).Inlanguagemodels,neuronshavebeenfoundtocorrespond
tosentiment(Radfordetal.,2017;DonnellyandRoegiest,2019),knowledge(Daietal.,2021),skills(Wangetal.,
2022b),de-/re-tokenization(Elhageetal.,2022a),contexts(Gurneeetal.,2023;Billsetal.,2023),position(Voitaetal.,
2023),spaceandtime(GurneeandTegmark,2023),andmanyotherlinguisticandgrammaticalfeatures(Bauetal.,2018;
Xinetal.,2019;Dalvietal.,2019,2020;Durranietal.,2022;Sajjadetal.,2022).Moregenerally,itishypothesizedthat
neuronsinlanguagemodelsformkey-valuestores(Gevaetal.,2020)thatfacilitatenexttokenpredictionbypromoting
conceptsinthevocabularyspace(Gevaetal.,2022).However,manychallengesexistinstudyingindividualneurons,
especiallyindrawingcausalconclusions(AntvergandBelinkov,2021;Huangetal.,2023).
3 ConceptualandEmpiricalPreliminaries
3.1 Universality
NotionsofUniversality Universalitycanrefertomanydifferentnotionsofsimilarity,eachatadifferentlevelof
abstractionandwithdifferingmeasuresandmethodologies.SimilartoMarr’slevelsofanalysisinneuroscience(Hamrick
andMohamed,2020;Marr,2010),relevantnotionsofuniversalityare:computationalorfunctionaluniversalityregarding
whethera(sub)networkimplementsaparticularinput-output-behavior(e.g.,whetherthenexttokenpredictionsfortwodif-
ferentnetworksarethesame);algorithmicuniversalityregardingwhetherornotaparticularfunctionisimplementedusing
thesamecomputationalsteps(e.g.,whetheratransformertrainedtosortstringsalwayslearnsthesamesortingalgorithm);
representationaluniversality,orthedegreeofsimilarityoftheinformationcontainedwithindifferentrepresentations(Ko-
rnblithetal.,2019)(e.g.,whethereverynetworkrepresentsabsolutepositioninthecontext);andfinallyimplementation
universality,i.e.,whetherindividualmodelcomponentslearnedbydifferentmodelsimplementthesamespecializedcom-
putations(e.g.,inductionheads(Olssonetal.,2022),successorheads(Gouldetal.,2023),Frenchneurons(Gurneeetal.,
2023),interalia).Noneofthesenotionsofuniversalityareusuallybinary,andtheuniversalitybetweencomponentsorcom-
putationscanrangefrombeingformallyisomorphictosimplysharingacommonhigh-levelconceptualorstatisticalmotif.
Inthiswork,weareprimarilyconcernedwithimplementationuniversalityintheformofwhetherindividualneurons
learntospecializeandactivateforthesameinputsacrossmodels. Ifsuchuniversalneuronsdoexist,thenthisisalso
asimpleformoffunctionaluniversality,asthedistinctneuronsconstitutethefinalnodeofdistinctsubnetworkswhich
computethesameoutput.
3DimensionsofVariations Universalitymustbemeasuredoversomeindependentdimensionofvariation,thatis,some
changeinthemodel,dataor,training.Forexample,modelvariablesincluderandomseed,modelsize,hyperparameters,
andarchitecturalchanges;datavariablesincludethedatasize,ordering,anddistributionofcontents;trainingvariables
includelossfunction, optimizer, regularization, finetuning, andhyperparametersthereof. Assumingthatchanging
randomseedisthesmallestchange,thisworkprimarilyfocusesoninitializationuniversalityinanattempttobound
thedegreeofsimilarityexpectedwhenstudyinglargerchanges.
3.2 Models
Werestrictourscopetotransformer-basedauto-regressivelanguagemodels(Radfordetal.,2018)thatcurrentlypower
themostcapableAIsystems(Bubecketal.,2023). Givenaninputsequenceoftokensx=[x ,...,x ]∈X ⊆Vtfrom
1 t
thevocabularyV,alanguagemodelM:X →Y outputsaprobabilitydistributionoverthevocabularytopredictthe
nexttokeninthesequence.
WefocusonareplicationoftheGPT2seriesofmodels(Radfordetal.,2019)withsomesupportingexperimentsonthe
Pythiafamily(Bidermanetal.,2023).ForaGPT2-smallandGPT2-mediumarchitecture(see§A.3forhyperparameters)
westudyfivemodelstrainedfromdifferentrandomseeds,referredtoasGPT2-{small,medium}-[a-e](Karamcheti
etal.,2021).
AnatomyofaNeuron Ofparticularimportancetothisinvestigationisthefunctionalformoftheneuronsinthefeed
forward(alsoknownasmulti-layerperceptron(MLP))layersinthetransformer. TheoutputofanMLPlayergiven
anormalizedhiddenstatex∈Rdmodelis
MLP(x)=W σ(W x+b )+b (1)
out in in out
whereW oT ut,W in∈Rdmlp×dmodel arelearnedweightmatrices,b inandb outarelearnedbiases,andσisanelementwise
nonlinearactivationfunction. Forallmodelswestudy,σistheGeLUactivationfunctionσ(x)=xΦ(x)(Hendrycks
andGimpel,2016).Onecananalyzeanindividualneuronjintermsoftheitsactivationσ(wjx+bj)fordifferentinputs
in in
x,oritsweights—rowjofW orWT whichrespectivelydictateforwhatinputsaneuronactivatesandwhateffects
in out
ithasdownstream.
Wereferthereaderto(Elhageetal.,2021)forafulldescriptionofthetransformerarchitecture. Weemploystandard
weightpreprocessingtechniquesdescribedfurtherinA.1.
4 TheSearchforUniversalNeurons
4.1 HowUniversalareIndividualNeurons?
Experiment Inspiredbypriorworkstudyingcommonneuronsinneuralnetworks(Lietal.,2015;Bauetal.,2018;
Dravidetal.,2023),wecomputemaximumpairwisecorrelationsofneuronactivationsacrossfivedifferentmodels
GPT2-{a,b,c,d,e}tofindpairsofneuronsacrossmodelswhichactivateonthesameinputs. Inparticular,letN(a)
bethesetofneuronsinmodela.Foreachneuroni∈N(a),wecomputethePearsoncorrelation
E(cid:2) (vi−µ )(vj−µ )(cid:3)
ρa,m= i j (2)
i,j σ σ
i j
withallneuronsj∈N(m)ineverymodelm∈{b,c,d,e},whereµ andσ arethemeanandstandarddeviationofa
i i
vectorofneuronactivationsvicomputedacrossadatasetof100milliontokensfromthePiletestset(Gaoetal.,2020).
Forabaseline,wealsocomputeρ¯a,m,whereinsteadoftakingthecorrelationofρ(vi,vj),wecomputeρ(vi,(RV) )
i,j j
forarandomd ×d GaussianmatrixRandthematrixofactivationsVforallneuronsinaparticularlayerN (m).
mlp mlp ℓ
Inotherwords,wecomputethecorrelationbetweenneuronsandelementswithinarandom(approximate)rotationof
alayerofneuronstoestablishabaselinecorrelationforthecasewheretheredoesnotexistaprivilegedbasis(Elhage
etal.,2021;Brownetal.,2023)toverifytheimportanceoftheneuronbasis.
ForasetofmodelsM wedefinetheexcesscorrelationofneuroniasthedifferencebetweenthemeanmaximum
correlationacrossmodelsandthemeanmaximumbaselinecorrelationintherotatedbasis:
(cid:18) (cid:19)
1 (cid:88)
ϱ = max ρa,m− max ρ¯a,m (3)
i |M|
m∈M
j∈N(m) i,j j∈NR(m) i,j
4Figure2:SummaryofneuroncorrelationexperimentsinGPT2-medium-a.(a)Distributionofthemean(overmodelsb-e)
max(overneurons)correlation,themeanbaselinecorrelation,andthedifference(excess).(b)Themax(overmodels)
max(overneurons)correlationcomparedtothemin(overmodels)max(overneuron)correlationforeachneuron.(c)
Percentageoflayerpairswithmostsimilarneuronpairs.
Results Figure2summarizesourresults. InFigure2a,wedepicttheaverageofthemaximumneuroncorrelations
acrossmodels[b-e],theaverageofthebaselinecorrelations,andtheexcesscorrelationi.e.,theleftterm,therightterm,
andthedifferencein(3). Whilethereisnoprincipledthresholdatwhichaneuronshouldbedeemeduniversal,only
1253outofthe98304neuronsinGPT2-medium-ahaveanexcesscorrelationgreaterthan0.5.
Tounderstandifhigh(low)correlationinonemodelimplieshigh(low)correlationinallthemodels,inFigure2bwe
reportmax max ρa,mcomparedtomin max ρa,mforeveryneuroni∈N(a).Figure2bsuggeststhere
m j∈N(x) i,j m j∈N(m) i,j
isrelativelylittlevariationinthecorrelations,asthemeandifferencebetweenthemax-maxandmin-maxcorrelation
is0.049forallneuronsand0.105forneuronswithϱ>0.5.Anothernaturalhypothesisisthatneuronsspecializeinto
rolesbasedonhowdeeptheyarewithinthenetwork(assuggestedby(Olahetal.,2020b;Elhageetal.,2022a)). In
2c,foreachlayerlofmodela,wecomputethefractionofneuronsinlayerlthathavetheirmostcorrelatedneuronin
layerl′forallℓ′inmodels[b-e]. Averagingacrossthedifferentmodels,weobservesignificantdepthspecialization,
suggestingthatneuronsdoperformdepthspecificcomputations,whichweexplorefurtherin§4.3.
WerepeattheseexperimentsonGPT2-smallandPythia-160mdisplayedinFigures12and13respectively. Arather
surprisingfindingisthatwhilethepercentageofuniversalneurons(ϱ >0.5)withinGPT2-mediumandPythia-160Mare
i
quiteconsistent(1.23%and1.26%respectively),thenumberinGPT2-small-aisfarhigherat4.16%.Weofferadditional
resultsandspeculationsin§6.3.
4.2 PropertiesofUniversalNeurons
Wenowseektounderstandwhethertherearestatisticalproprietiesassociatedwithwhetheraneuronisuniversalornot,
definedashavinganexcesscorrelationϱ >0.5.ForallneuronsinGPT2-medium-a,GPT2-small-a,andPythia-160m,
i
wecomputevarioussummarystatisticsoftheirweightsandactivations.Foractivations,wecomputethemean,skew,
andkurtosisofthepre-activationdistributionover100milliontokens,aswellasthefractionofactivationsgreaterthan
zero,termedactivationsparsity.Forweights,werecordtheinputbiasb ,thecosinesimilaritybetweentheinputand
in
outputweightcos(w ,w ),theweightdecaypenalty∥w ∥2+∥w ∥2,andthekurtosisoftheneuronoutputweights
in out in 2 out 2
withtheunembeddingkurt(cos(w ,w )).
out U
InFigure3,wereportthesestatisticsforuniversalneuronsasapercentilecomparedtoallneuronswithinthesame
layer;wechoosethisnormalizationtoenablecomparisonacrossdifferentlayers,models,andmetrics(abreakdown
permetricandlayerforGPT2-medium-aisgiveninFigure14). Ourresultsshowthatuniversalneuronsdostandout
comparedtonon-universalneurons. Specifically,universalneuronstypicallyhavelargeweightnorm(implyingthey
areimportantbecausethemodelwastrainedwithweightdecay)andhavealargenegativeinputbias,resultingina
largenegativepre-activationmeanandhenceloweractivationfrequency. Furthermore,universalneuronshavevery
highpre-activationskewandkurtosis,implyingtheyusuallyhavenegativeactivation,butoccasionallyhaveverypositive
activation,proprietieswewouldexpectofmonosemanticneurons(Olahetal.,2020b;Elhageetal.,2022b;Gurneeetal.,
2023)whichonlyactivatewhenaspecificfeatureispresentintheinput.Incontrast,non-universalneuronsusuallyhave
skewapproximately0andkurtosisapproximately3,identicaltoaGaussiandistribution.Wewilldiscussthemeaning
ofhighW kurtosisin§5.1andhighcos(w ,w )in§6.
U in out
5Figure3:Propertiesofactivationsandweightsofuniversalneuronsforthreedifferentmodels,plottedasapercentile
comparedtoneuronsinthesamelayer.
4.3 UniversalNeuronFamilies
Motivatedbytheobservationthatuniversalneuronshavedistributionalstatisticssuggestiveofmonosemanticity,we
zoom-inonindividualneuronswithϱ>0.5andattempttogroupthemintoapartialtaxonimizationofneuronfamilies
(Olahetal.,2020a;Cammarataetal.,2021).Aftermanuallyinspectingmanysuchneurons,wedevelopedseveralhundred
automatedteststoclassifyneuronsusingalgorithmicallygeneratedlabelsderivedfromelementsofthevocabulary
(e.g.,whetheratokenis_all_capsorcontains_digit)andfromtheNLPpackagespaCy(Honnibaletal.,2020).
Specifically,foreachneuronwithactivationvectorv,andeachtestexplanationwhichisabinaryvectoryoveralltokens
intheinput,wecomputethereductioninvariancewhenconditionedontheexplanation:
(1−β)σ2(v|y=0)+βσ2(v|y=1)
1− (4)
σ2(v)
whereβisthefractionofpositivelabelsandσ2(·)isthevarianceofavectororsubsetthereof.Below,wequalitatively
describethemostcommonfamilies,andfindourresultsreplicatemanyfindingspreviouslydocumentedintheliterature.
UnigramNeurons Themostcommontypeofneuronwefoundwereunigramneurons,whichsimplyactivateapproxi-
matelyifandonlyifthecurrenttokenisaparticularwordorpartofaword.Theseneuronsoftenhavemanynearduplicate
neuronsactivatingforthesameunigram(Figure15)andappearpredominatelyinthefirsttwolayers(Figure16).One
subtletyisthefactthatcommonwordswilloftenhavefourdifferenttokenizationsfromdifferentcombinationsofcapitaliza-
tionandprecedingspace(e.g.,“_on”“_On”“on”and“On”).Therefore,forneuronsrespondingtoalphabeticalunigrams,
webreakdownactivationsdependingonwhethertheunigramappearsasaword,atthebeginningofaword,orinthe
middleofaword(Figure4a),andfindbothpositiveandnegativecaseswheretheduplicateneuronsrespondtotheunigram
variationsdifferently(Figures4aand15).Suchneuronsillustratethatthetoken(un)embeddingsmaynotcontainallofthe
relevanttoken-levelinformation,andthatthemodelusesneuronstocreatean“extended”embeddingofhighercapacity.
AlphabetNeurons Aparticularlyfunsubclassofunigramneuronsarealphabetneurons(Figure1a),whichactivate
moststronglyontokenscorrespondingtoanindividualletter,andsecondarilyontokenswhichbeginwiththerespective
letter.For18of26Englishlettersthereexistalphabetneuronswithϱ>0.5(Figure17),withsomelettersalsohaving
severalnearduplicateneurons.
PreviousTokenNeurons Afterfindinganexampleofoneneuronwhichseemedtoactivatepurelyasafunctionofthe
previoustoken(e.g.,ifitcontainsacomma;Figure1b),wedecidedtorerunourunigramtestswiththelabelsshiftedby
one—thatis,withthelabelgivenbytheprevioustoken.Thesetestssurfacedmanymoreprevioustokenneuronsoccurring
mostofteninlayers4-6(seeFigure18foranadditional25universalprevioustokenneurons).Suchneuronsillustratethe
manypotentiallyredundantpathsofcomputationsthatcanoccurwhichcomplicatesablationbasedinterpretabilitystudies.
PositionNeurons Inspiredbytherecentworkof(Voitaetal.,2023),wealsorunevaluationsforpositionneurons,
neurons which activate as a function of absolute position rather than token or context (Figure 1c). We follow the
procedureof(Voitaetal.,2023)(whoruntheirexperimentsonOPTmodelswithReLUactivation(Zhangetal.,2022))
6(a)Nearduplicate“on”unigramneurons (b)Syntaxneuron (c)PlaceNeurons
Figure4:AdditionalexamplesofuniversalneuronfamiliesinGPT2-medium.
bycomputingthemutualinformationbetweenactivationandcontextposition,andfindsimilarresults,withneurons
thathaveavarietyofpositionalpatternsconcentratedinlayers0-2(seeFigure19for20moreneurons).Similartothe
unigramneurons,thepresenceoftheseneuronsispotentiallyunexpectedgiventheiroutputscouldbelearneddirectly
bythepositionalembeddingatthebeginningofthemodelwithlessvarianceinactivation.
SyntaxNeurons UsingtheNLPpackagespaCy(Honnibaletal.,2020),welabelourinputdatawithpart-of-speech,
dependencyrole,andmorphologicaldata.Wefindmanyindividualneuronsthatselectivelyactivateforbasiclinguisticfea-
tureslikenegation,plurals,andverbforms(Figure4b)whicharenotconcentratedtoanypartofthenetworkandresemble
pastfindingsonlinguisticproperties(Dalvietal.,2019;Durranietal.,2022).Figure20includes25moreexamples.
SemanticNeurons Finally,wefoundalargenumberofneuronswhichactivateforsemanticfeaturescorresponding
tocoherenttopics(LimandLauw,2023),concepts(Elhageetal.,2022a),orcontexts(Gurneeetal.,2023).Suchfeatures
arenaturallymuchhardertoalgorithmicallysupervise.WeusethesubdistributionlabelfromthePiledataset(Gaoetal.,
2020)andmanuallylabeledtopicsfromanSVDbasedtopicmodelasabestattempt,butthisleavesmanyinterpretable
neuronsundiscoveredanduncategorized.InFigure4c,weshowthreeregionsneuronswhichactivatemoststronglyon
tokenscorrespondingtoplacesinCanada,Japan,orLatinAmericarespectively.Figure21depicts30additionalcontext
neuronswhichactivateonspecificsubdistributions,withmanyneuronswhichalwaysactivatefornon-englishtext.
5 UniversalFunctionalRolesofNeurons
Whilethepreviousdiscussionwasprimarilyfocusedonanalyzingtheactivationsofneurons,andbyextensionthe
featurestheyrepresent,thissectionisdedicatedtostudyingtheweightsofneuronstobetterunderstandtheirdownstream
effects. Theneuronsinthissectionareexamplesofactionmechanisms(Anthropic,2023)—modelcomponentsthat
arebetterthoughtofasimplementinganactionratherthanpurelyextractingorrepresentingafeature,analogousto
motorneuronsinneuroscience.
5.1 PredictionNeurons
Asimplebuteffectivemethodtounderstandweightsisthroughlogitattributiontechniques(Nostalgebraist,2020;Geva
etal.,2022;Daretal.,2022).Inthiscase,wecanapproximateaneuron’seffectonthefinalpredictionlogitsbysimply
computingtheproductbetweentheunembeddingmatrixandaneuronoutputweightW w andhenceinterpretthe
U out
neuronbasedonhowitpromotesconceptsinthevocabularyspace(Gevaetal.,2022).
Whenweapplyourautomatedtestsfrom§4.3onW w ratherthantheactivationsforouruniversalneurons,wefound
U out
severalgeneralpatterns(Figure5),manyindividualneuronswithextremelyclearinterpretations(Figure23),andclusters
ofneuronswhichallaffectthesametokens(Figure24). Specifically,wefindmanyexamplesofpredictionneurons
thatpositivelyincreasethepredictedprobabilityofacoherentsetoftokenswhileleavingmostothersapproximately
unchanged(Fig5a);suppressionneuronsthataresimilar,exceptdecreasetheprobabilityofagroupofrelatedtokens
(Fig5b);andpartitionneuronsthatpartitionthevocabularyintotwogroups,increasingtheprobabilityofonewhile
decreasingtheprobabilityoftheother(Fig5c).Theprediction,suppression,andpartitionmotifscanbeautomatically
detected by studying the moments of the distribution of vocabulary effects given by W w . In particular, both
U out
predictionandsuppressionneuronswillhavehighkurtosis(thefourthmoment—ameasureofhowmuchmassisin
thetailsofadistribution),butpredictionneuronswillhavepositiveskewandsuppressionneuronswillhavenegative
7Figure 5: Example prediction neurons in GPT2-medium-a. Depicts the distribution of logit effects on the output
vocabulary(W w )splitbytokenpropertyfor3differentneurons.(a)Predictionneuronincreasinglogitsofinteger
U out
tokensbetween1700and2050(i.e.years;highkurtosis),(b)Suppressionneurondecreasinglogitsfortokenscontaining
anopenparenthesis(highkurtosisandnegativeskew),and(c)Partitionneuronboostingtokensbeginningwithaspace
andsuppressingtokenswhichdonot(highvariance;note,lineary-scale).
Figure6:Summarystatisticsofcosinesimilaritybetweenneuronoutputweights(W )andtokenunembedding(W )
out U
forGPT2-medium-[a-e].(a,b)Percentilesofkurtosisandskewbylayeraveragedover[a-e].(c)Distributionofskewsfor
neuronswithkurtosisgreaterthan10inlastfourlayers.Shadedareadenotesrangeacrossallfivemodels.
skew. Partitionneuronswillshifttheprobabilityofmosttokensandhavehighvarianceinoveralllogiteffect. From
this,weseealmostalluniversalneurons(ϱ>0.5)inlaterlayersareoneofthesepredictionneuronvariants(Figure14).
To better understand the number and location of these prediction neurons, we compute the moment metrics of
cos(W ,w )forallneuronsinallfiveGPT2-mediummodels,andshowhowthesestatisticsvaryovermodeldepth
U out
inFigure6.Wefindastrikingpatternwhichisremarkablyconsistentacrossthedifferentseeds:afteraboutthehalfway
pointinthemodel,predictionneuronsbecomeincreasinglyprevalentuntiltheveryendofthenetworkwherethereis
asuddenshifttowardsamuchlargernumberofsuppressionneurons. Toensurethisisnotjustanartifactofthetied
embeddings(W =WT)intheGPT2models,wealsorunthisanalysisonfivePythiamodelsrangingfrom410Mto
E U
6.9Bparametersandfindtheresultsarelargelythesame(Figure22).
Whenstudyingtheactivationsofsuppressionneurons,wenoticedthattheyactivatefarmoreoftenwhenthenexttoken
isinfactfromthesetoftokenstheysuppress(e.g.,ayeartokenlike“1970”;Figure24).Weintuitthatthesesuppression
neuronsfirewhenitisplausiblebutnotcertainthatthenexttokenisfromtherelevantset.Combinedwiththeobservation
thatthereexistmanysuppressionandpredictionneuronsforthesametokenclass(Figure24),wetakethisasevidence
ofanensemblehypothesiswherethemodelusesmultipleneuronswithsomeindependenterrorthatcombinetoform
amorerobustandcalibratedestimateofwhetherthenexttokenisinfactayear.
Inadditiontobeingacleanexampleofanactionmechanism(Anthropic,2023),theseresultsareinterestingasthey
refineaconjecturemadeby(Gevaetal.,2022). Specifically,ratherthan“feed-forwardlayersbuildpredictionsby
promoting concepts in the vocabulary space,” we claim late feed-forward (MLP) layers build predictions by both
promotingandsuppressingconceptsinthevocabularyspace. Moreover,itsuggeststherearedifferentstagesinthe
iterativeinferencepipeline(Belroseetal.,2023;Jastrze˛bskietal.,2017),wherefirstaffirmativepredictionsaremade,
8Figure7:Summaryof(anti-)entropyneuronsinGPT2-medium-acomparedto20randomneuronsfromfinaltwolayers.
Entropyneuronshavehighweightnorm(a)withoutputweightsmostlyorthogonaltotheunembeddingmatrix(b).Fixing
theactivationtolargervaluescausesthefinallayernormscaletoincreasedramatically(c)whileleavingtherankingof
thetruenexttokenpredictionmostlyunchanged(d).Increasedlayernormscalesqueezesthelogitdistribution,causinga
largeincreaseinthepredictionentropy(e;ordecreaseforanti-entropyneuron)andanincreaseordecreaseintheloss
dependingonthemodel’sbaselinelevelofunder-orover-confidence(f).Legendappliestoallsubplots.
andthenthedistributionissharpenedormademorecalibratedbysuppressionneuronsattheveryend.Theexistence
ofsuppressionneuronsalsoshedslightonrecentobservationsofindividualneurons(Billsetal.,2023)andMLPlayers
(McGrathetal.,2023)suppressingthemaximumlikelihoodtokenandbeingamechanismforself-repair.
5.2 EntropyNeurons
Becausemodelsaretrainedwithweightdecay(ℓ regularization)wehypothesizedthatneuronswithlargeweight
2
normswouldbemoreinterestingorimportantbecausetheycomeatahighercost.Whilemostturnedouttoberelatively
uninteresting(mostlyneuronswhichactivateforthebeginningofsequencetoken),the15th largestnormneuronin
GP2-medium-a(L23.945)hadanespeciallyinterestingproperty:ithadthelowestvariancelogiteffectW w ofany
U out
neuroninthemodel;i.e.,itonlyhasatinyeffectonthelogits.Tounderstandwhyafinallayerneuron,whichcanonly
affectthefinallogitdistribution,hashighweightnormwhileperforminganapproximateno-oponthelogits,recall
thefinaldecodingformulafortheprobabilityofthenexttokengivenafinalresidualstreamvectorx
x−E[x]
p(y|x)=Softmax(W LayerNorm(x)), LayerNorm(x)= . (5)
U (cid:112)
Var[x]+ϵ
Wehypothesizethatthefunctionofthisneuronistomodulatethemodel’suncertaintyoverthenexttokenbyusing
thelayernormtosqueezethelogitdistribution,inamannerquitesimilartomanuallyincreasingthetemperaturewhen
performinginference. Tosupportthishypothesis,weperformacausalintervention,fixingtheneuroninquestionto
aparticularvalueandstudyingtheeffectcomparedto20randomneuronsfromthelasttwolayersthatarenotinthe
topdecileofnormorinthebottomdecileoflogitvariance(Figure7).Wefindthatinterveningonthisentropyneuron
indeedcausesthelayernormscaletoincreasedramatically(becauseofthelargeweightnorm)whilelargelynotaffecting
therelativeorderingofthevocabulary(becauseofthelowcomposition),havingtheeffectofincreasingoverallentropy
bydampeningthepost-layernormcomponentofxintherowspaceofW .
U
Additionally,weobservedaneuron(L22.2882)withcos(w23.945,w22.2882)=−0.886(i.e.,aneuronthatwritesin
out out
theoppositedirectionforminganantipodalpair(Elhageetal.,2022b))thatalsohashighweightnorm.Repeatingthe
interventionexperiment,wefindthisneurondecreasesthelayernormscaleanddecreasesthemeannexttokenentropy,
formingananti-entropyneuron.Theseresultssuggesttheremaybeoneormoreglobaluncertaintydirectionsthatthe
9Figure8:Summaryofattention(de-)activationneuronresultsinGPT2-medium-a. (a)Distributionofheuristicscore
h foreverypairofneuronsandheadscomparedtorandomneurondirectionsR.(b;c)pathablationseffectofneuron
n
L4.3594onheadL5.H0:ablatingpositiveactivationreducesattentiontoBOS(b)causingthenormtoincrease(c).
modelmaintainstomodulateitsoverallconfidenceinitsprediction. However,ourexperimentswithfixedactivation
valuedonotnecessarilyimplythemodelusestheseneuronstoincreasetheentropyasageneraluncertaintymechanism,
andwedidnoticecasesinwhichincreasingtheactivationoftheentropyneurondecreasedentropy,suggestingthetrue
mechanismmaybemorecomplicated.
WerepeattheseexperimentsonGPT2-small-aandfindanevenmoredramaticantipodalpairof(anti-)entropyneurons
inFigure25.Toourknowledge,thisisthefirstdocumentedmechanismforuncertaintyquantificationinlanguagemodels
andperhapsthesecondexampleofamechanisminvolvinglayernorm(Brodyetal.,2023).
5.3 AttentionDeactivationNeurons
Inautoregressivemodels,attentionheadsfrequentlyplacealloftheirattentiononthebeginningofsequence(BOS)token
(Xiaoetal.,2023). WehypothesisethatthemodelusestheattentiontotheBOStokenasakindof(de-)activationfor
thehead,wherefullyattendingtoBOSimpliestheheadisdeactivatedandhasminimaleffect.Moreover,wehypothesize
thatthereareindividualneuronswhichcontroltheextenttowhichheadsattendtoBOS.
Recalltheoutputofanattentionheado foradestinationtokendfromsourcetokenssisgivenby
d
q =W r , k =W r , S =qTk , A
=softmax(M √(S ds)
), v =W r , o =W
(cid:88)
A v
d Q d s K s ds d s ds s d h s V s d O s ds s
wherer istheresidualstreamatthesource/destinationtoken,d isthebottleneckdimensionofthehead,andM(·)
s/d h
appliesthecausalattentionmasktotheattentionscores.ThecalculationoftheattentionpatternA viaasoftmaxacross
ds
thesourcepositionsmeansthattheattentiongiventothesourcetokensbyagivendestinationtokensumstoone.
ThevectorW v isconstantforallpromptsandcontainsnosemanticinformation.Ifithasalownorm,attendingto
O BOS
BOSscalesdowntheoutputsofattendingtoothersourcepositionswhilemaintainingtheirrelativeattentionbecausethe
attentionscoresmustsumtoone.IftheBOSoutputnormisnearzero,theheadcaneffectivelyturnoffbyonlyattending
totheBOStoken.Inpractice,themedianheadinGPT-2-medium-ahasaW v withnorm19.4timessmallerthan
O BOS
theaverageforothertokens.
Wecanidentifyneuronswhichmayusethismechanismforagivenheadbyaheuristicscoreh =WT WTk
n out Q BOS
forunitnormalizedW .PositivescoressuggestsactivationoftheneuronwillincreasetheattentionplacedonBOS,
out
decreasingtheoutputnormofthehead,andtheoppositefornegativescores. Figure8ashowsthedistributionofthe
scoresforallheadsinGPT2-medium-acomparedtoaunitnormalizedGaussianmatrixR.
Foragivenneuron,wecanmeasuretheeffectofactivationontheattentiontoBOSandoutputnormofagivenhead
bypathablation(Wangetal.,2022a)oftheneuronataparticulardestinationtoken.Specifically,wecanmeasurethe
differenceinBOSattentionandnormoftheoutputoftheheadbetweentheoriginalrunandaforwardpasswherethe
contributionofaneuronisdeleted(i.e,zeropathablated)fromtheinputofaparticularheadatthecurrenttokenposition.
Weperformthisprocedureoverarandomsubsetoftokensinthesecondhalfofthecontexttoavoidspuriouseffects
stemmingfromshortcontexts.Figure8band8cdepicttheresultsofthesepathablationsforthehighestscoringneuron
inlayer4forhead0inattentionlayer5.Thisisanexampleofanattentiondeactivationneuron—increasingtheactivation
oftheneuronincreasestheattentiontoBOSreducingtheoutputnormofthehead∥o ∥.SeeFigure26for5additional
d
examplesofattention(de-)activatingneurons.
10Figure9: Activationfrequencyofneuron(fractionofactivationvaluesgreaterthanzero)versuscosinesimilarityof
neuroninputandoutputweightsforGPT2-small-a(left),GPT2-medium-a(center),andPythia-160M(right).
6 AdditionalMysteries
Weconcludeourinvestigationbycommentingonseveralmiscellaneousresultsthatwethinkareworthreportingbut
thatwedonotfullyunderstand.
6.1 CosineandActivationFrequency
Anunexpectedlystrongrelationshipweobservedisthecorrelationbetweenactivationfrequencyofaneuronandthe
cosinesimilaritybetweenitsinputandoutputweightvectorscos(w ,w )asshowninFigure9.Almostallneurons
in out
withaveryhighactivationfrequencyhaveinputandoutputweightsinalmostoppositedirections.Theseneuronsare
predominantlyinthefirstquarterofnetworkdepthandhavesmallexcesscorrelation,i.e.,theyarenotuniversalas
measuredbyactivation.Wealsofinditnoteworthythatthereappearstobeanapproximateceilingandflooronthecosine
similarityofapproximately±0.8.
6.2 DuplicationandUniversality
Whileneuronredundancyhasbeenobservedinmodelsbefore(Casperetal.,2021;Dalvietal.,2020)andlargemodels
canbeeffectivelypruned(Xiaetal.,2023),weweresurprisedbythenumberofseeminglyduplicateuniversalneurons
weobserved(e.g.,Figure15orthe105BOSneuronsweobserved). Naively,thisissurprising,asitseemswasteful
todedicatemultipleneuronstothesamefeature. Largermodelshavemorecapacityandareempiricallymuchmore
effectivesowhyhaveredundantneuronswhenyoucouldinsteadhaveoneneuronwithtwicetheoutputweightnorm?
Afewpotentialexplanationsare(1)thesemodelsweretrainedwithweightdecay,creatinganincentivetospreadout
thecomputation.(2)Dropout—however,inthesemodelsdropoutisappliedtotheoutputoftheMLPlayer,ratherthanthe
MLPactivationsthemselves.(3)Theseneuronsarevestigialremnantsthatwereusefulearlierintraining(Quirkeetal.,
2023),butarepotentiallystuckinalocalminimaandarenolongeruseful.(4)Theduplicatedneuronsareonlyactivating
thesameoncommonfeatures,butarepolysemanticwithdifferentsetsofrarerfeatures.(5)Ensembling,whereeachneuron
computesthesamefeaturebutwithsomeindependenterror,andtogetherformanensemblewithloweraverageerror.
Bymeasuringredundancyintermsofsimilarityinweights(Figure10),wefindveryfewneuronswhichareliteral
duplicates,providingmoreevidencefor(4)and(5).Basedonthemuchhigherlevelofsimilarityforuniversalneurons,
itispossiblethiseffectisrelativelysmallingeneral.
6.3 ScaleandUniversality
Asmentionedin§4,GPT2-mediumandPythia-160Mhaveaconsistentnumberofuniversalneurons(1.23%and1.26%re-
spectively),whileGPT2-small-ahasmanymore4.16%.InFigure11weshowthedistributionofmax,baseline,andexcess
correlationsforallmodels,whereweseethatGPT2-mediumandPythia-160Mhavealmostidenticaldistributionswhile
GPT2-smallisanoutlier.GPT2-smallalsohascorrespondinglygreaterweightredundancyasshowninFigure10.Oneex-
planationforthisisthenumberofuniversalneuronsdecreasesinlargermodels.Thisispotentiallyimpliedbyresultsfrom
11Figure10:Distributionofcosinesimilaritiesofmostsimilarneuronsmeasuredbyinputweights(top)andoutputweights
(bottom)forGPT2-small-a(left),GPT2-medium-a(middle),andPythia-160M(right)coloredbyuniversality(ϱ>0.5).
Figure11: Empiricaldistributionofmaxneuroncorrelationaveragedacrossmodels(left),maxbaselinecorrelation
averagedacrossmodels(middle),andthedifferencedenotedastheexcesscorrelation(right).
(Billsetal.,2023)whoobservelargermodelshavefewerneuronswhichadmithighqualitynaturallanguageinterpretations.
However,withoutadditionalexperimentsonlargermodelstrainedfromrandomseeds,thisremainsanopenquestion.
7 DiscussionandConclusion
Findings Inthiswork,weexploretheuniversalityofindividualneuronsinGPT2languagemodels,andfindthatonly
about1-5%ofneuronsareuniversalacrossmodels,constitutinganotherpieceofevidencethatindividualneuronsare
nottheappropriateunitofanalysisformostnetworkbehaviours.Nonetheless,wehaveshownthatleveraginguniversality
isaneffectiveunsupervisedapproachtoidentifyinterpretablemodelcomponentsandimportantmotifs.Inparticular,
thosefewneuronswhichareuniversalareofteninterpretable,canbegroupedintoasmallernumberofneuronfamilies,
andoftendevelopwithnearduplicateneuronsinthesamemodel. Someuniversalneuronsalsohaveclearfunctional
roles,likemodulatingthenexttokenpredictionentropy,controllingtheoutputnormofanattentionhead,andpredicting
orsuppressingelementsofthevocabularyintheprediction.Moreover,thesefunctionalneuronsoftenformantipodal
pairs,potentiallyenablingcollectionsofneuronstoensembletoimproverobustnessandcalibration.
Limitations ComparedtofrontierLLMs,westudysmallmodelsofonlyhundredsofmillionparametersandtens
ofthousandsofneuronsduetotheexpenseoftrainingmultiplelargescalelanguagemodelsfromdifferentrandom
initializations.Wealsostudyarelativelynarrowformofuniversality:neuronuniversalityoverrandomseedswithinthe
12samemodelfamily.Studyinguniversalityacrossdifferentmodelfamiliesismadedifficultbytokenizationdiscrepancies,
andstudyingmodelsacrosslargersizesisdifficultduetotheexpenseofcomputingallpairwiseneuroncorrelationsovera
sufficientlysizedtextcorpus.Additionally,manyofourinterpretationsrelyonmanualanalysisoralgorithmicsupervision
whichrestrictsthescopeandgeneralityofourmethods.Moreover,ournarrowfocusonasubsetofindividualelements
oftheneuronbasispotentiallyobscuresimportantdetailsandignoresthevastmajorityofoverallnetworkcomputation.
FutureWork Eachoftheselimitationssuggestavenuesforfuturework. Insteadofstudyingtheneuronbasis,our
experimentscouldbereplicatedonanovercompletedictionarybasisthatismorelikelytocontainthetruemodelfeatures
(Cunninghametal.,2023;Brickenetal.,2023).Motivatedbythefindingthatthemostcorrelatedneuronsoccurinsimilar
networkdepths,ourexperimentscouldbererunonlargermodelswherepairwisecorrelationsareonlycomputedbetween
adjacentlayerstoimprovescalability.Additionally,theinterpretationofcommonunitscouldbefurtherautomatedusing
LLMstoprovideexplanations(Billsetal.,2023). Finally,byuncoveringinterpretablefootholdswithintheinternals
ofthenetwork,ourfindingscanformthebasisofdeeperinvestigationsintohowthesecomponentsrespondtostimulusor
perturbation,developovertraining(Quirkeetal.,2023),andaffectdownstreamcomponentstofurtherelucidategeneral
motifsandspecificcircuitswithinlanguagemodels.
Acknowledgments
WewouldliketothankYossiGandelsman,LovisHeindrich,LuciaQuirke,forusefuldiscussionsandcommentsonour
work.WemadeextensiveuseoftheTransformerLenslibrary(Nanda,2022)andtheMITSupercloud(Reutheretal.,2018)
forourexperimentsandcomputationalresources.WGwaspartiallysupportedbyanOpenPhilanthropyearlycareergrant.
AuthorContribution
WGledtheproject,conductedmostoftheanalysis,andwrotemostofthepaper. THledtheeffortonunderstanding
attention(de-)activationneurons,andperformedthecorrespondinganalysisandwriting.ZCGassistedwithexperimental
infrastructure.ZCG,TRK,QS,andWHassistedwithneuronanalysisandwriting/editing.NNgavefrequentanddetailed
feedbackonexperimentdesignandanalysisinadditiontoeditingthepaper.DBsupportedtheprojectandeditedthepaper.
References
Anthropic(2023). Circuitsupdates-july2023. https://transformer-circuits.pub/2023/july-update/index.html.
Antverg,O.andBelinkov,Y.(2021). Onthepitfallsofanalyzingindividualneuronsinlanguagemodels. arXivpreprint
arXiv:2110.07483.
Bansal,Y.,Nakkiran,P.,andBarak,B.(2021). RevisitingModelStitchingtoCompareNeuralRepresentations.
Barannikov,S.,Trofimov,I.,Balabin,N.,andBurnaev,E.(2022). RepresentationTopologyDivergence: AMethod
forComparingNeuralNetworkRepresentations.
Bau,A.,Belinkov,Y.,Sajjad,H.,Durrani,N.,Dalvi,F.,andGlass,J.(2018). Identifyingandcontrollingimportant
neuronsinneuralmachinetranslation. arXivpreprintarXiv:1811.01157.
Bau,D.,Zhu,J.-Y.,Strobelt,H.,Lapedriza,A.,Zhou,B.,andTorralba,A.(2020). Understandingtheroleofindividual
unitsinadeepneuralnetwork. ProceedingsoftheNationalAcademyofSciences.
Belrose,N.,Furman,Z.,Smith,L.,Halawi,D.,Ostrovsky,I.,McKinney,L.,Biderman,S.,andSteinhardt,J.(2023).
Elicitinglatentpredictionsfromtransformerswiththetunedlens. arXivpreprintarXiv:2303.08112.
Bender,E.M.,Gebru,T.,McMillan-Major,A.,andShmitchell,S.(2021). Onthedangersofstochasticparrots: Can
languagemodelsbetoobig? InProceedingsofthe2021ACMconferenceonfairness,accountability,andtransparency,
pages610–623.
Bengio,Y.,Hinton,G.,Yao,A.,Song,D.,Abbeel,P.,Harari,Y.N.,Zhang,Y.-Q.,Xue,L.,Shalev-Shwartz,S.,Hadfield,
G.,etal.(2023). Managingairisksinaneraofrapidprogress. arXivpreprintarXiv:2310.17688.
Biderman,S.,Schoelkopf,H.,Anthony,Q.,Bradley,H.,O’Brien,K.,Hallahan,E.,Khan,M.A.,Purohit,S.,Prashanth,
U.S.,Raff,E.,Skowron,A.,Sutawika,L.,andvanderWal,O.(2023). Pythia:Asuiteforanalyzinglargelanguage
modelsacrosstrainingandscaling.
Bills, S., Cammarata, N., Mossing, D., Tillman, H., Gao, L., Goh, G., Sutskever, I., Leike, J., Wu,
J., and Saunders, W. (2023). Language models can explain neurons in language models. https:
//openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html.
13Boix-Adsera,E.,Lawrence,H.,Stepaniants,G.,andRigollet,P.(2022). GULP:aprediction-basedmetricbetween
representations.
Bommasani,R.,Hudson,D.A.,Adeli,E.,Altman,R.,Arora,S.,vonArx,S.,Bernstein,M.S.,Bohg,J.,Bosselut,A.,
Brunskill,E.,etal.(2021). Ontheopportunitiesandrisksoffoundationmodels. arXivpreprintarXiv:2108.07258.
Bricken, T., Templeton, A., Batson, J., Chen, B., Jermyn, A., Conerly, T., Turner, N., Anil, C., Denison, C.,
Askell, A., Lasenby, R., Wu, Y., Kravec, S., Schiefer, N., Maxwell, T., Joseph, N., Hatfield-Dodds, Z., Tamkin,
A., Nguyen, K., McLean, B., Burke, J. E., Hume, T., Carter, S., Henighan, T., and Olah, C. (2023). Towards
monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread.
https://transformer-circuits.pub/2023/monosemantic-features/index.html.
Brody,S.,Alon,U.,andYahav,E.(2023).Ontheexpressivityroleoflayernormintransformers’attention.arXivpreprint
arXiv:2305.02582.
Brown,D.,Vyas,N.,andBansal,Y.(2023). Onprivilegedandconvergentbasesinneuralnetworkrepresentations. arXiv
preprintarXiv:2307.12941.
Bubeck,S.,Chandrasekaran,V.,Eldan,R.,Gehrke,J.,Horvitz,E.,Kamar,E.,Lee,P.,Lee,Y.T.,Li,Y.,Lundberg,S.,
etal.(2023). Sparksofartificialgeneralintelligence:Earlyexperimentswithgpt-4. arXivpreprintarXiv:2303.12712.
Burns, C., Ye, H., Klein, D., andSteinhardt, J.(2022). Discoveringlatentknowledgeinlanguagemodelswithout
supervision. arXivpreprintarXiv:2212.03827.
Cammarata, N., Goh, G., Carter, S., Voss, C., Schubert, L., and Olah, C. (2021). Curve circuits. Distill.
https://distill.pub/2020/circuits/curve-circuits.
Carlsmith,J.(2023). Schemingais: Willaisfakealignmentduringtraininginordertogetpower? arXivpreprint
arXiv:2311.08379.
Casper,S.,Boix,X.,D’Amario,V.,Guo,L.,Schrimpf,M.,Vinken,K.,andKreiman,G.(2021). Frivolousunits:Wider
networksarenotreallythatwide. InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume35,pages
6921–6929.
Chughtai,B.,Chan,L.,andNanda,N.(2023). Atoymodelofuniversality:Reverseengineeringhownetworkslearn
groupoperations. InProceedingsofthe40thInternationalConferenceonMachineLearning,ICML’23.JMLR.org.
Conmy,A.,Mavor-Parker,A.N.,Lynch,A.,Heimersheim,S.,andGarriga-Alonso,A.(2023). Towardsautomated
circuitdiscoveryformechanisticinterpretability. arXivpreprintarXiv:2304.14997.
Cunningham,H.,Ewart,A.,Riggs,L.,Huben,R.,andSharkey,L.(2023). Sparseautoencodersfindhighlyinterpretable
featuresinlanguagemodels. arXivpreprintarXiv:2309.08600.
Dai,D.,Dong,L.,Hao,Y.,Sui,Z.,Chang,B.,andWei,F.(2021). Knowledgeneuronsinpretrainedtransformers. arXiv
preprintarXiv:2104.08696.
Dalvi,F.,Durrani,N.,Sajjad,H.,Belinkov,Y.,Bau,A.,andGlass,J.(2019). Whatisonegrainofsandinthedesert?
analyzingindividualneuronsindeepnlpmodels. InProceedingsoftheAAAIConferenceonArtificialIntelligence,
volume33,pages6309–6317.
Dalvi,F.,Sajjad,H.,Durrani,N.,andBelinkov,Y.(2020). Analyzingredundancyinpretrainedtransformermodels.
arXivpreprintarXiv:2004.04010.
Dar, G., Geva, M., Gupta, A., andBerant, J.(2022). Analyzingtransformersinembeddingspace. arXivpreprint
arXiv:2209.02535.
Ding,F.,Denain,J.-S.,andSteinhardt,J.(2021). GroundingRepresentationSimilaritywithStatisticalTesting.
Donnelly,J.andRoegiest,A.(2019). Oninterpretabilityandfeaturerepresentations:ananalysisofthesentimentneuron.
InAdvancesinInformationRetrieval:41stEuropeanConferenceonIRResearch,ECIR2019,Cologne,Germany,
April14–18,2019,Proceedings,PartI41,pages795–802.Springer.
Doshi-Velez,F.andKim,B.(2017). Towardsarigorousscienceofinterpretablemachinelearning. arXivpreprint
arXiv:1702.08608.
Dravid,A.,Gandelsman,Y.,Efros,A.A.,andShocher,A.(2023). Rosettaneurons: Miningthecommonunitsina
modelzoo. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages1934–1943.
Duong,L.R.,Zhou,J.,Nassar,J.,Berman,J.,Olieslagers,J.,andWilliams,A.H.(2023). Representationaldissimilarity
metricspacesforstochasticneuralnetworks.
Durrani,N.,Dalvi,F.,andSajjad,H.(2022). Linguisticcorrelationanalysis:Discoveringsalientneuronsindeepnlp
models. arXivpreprintarXiv:2206.13288.
14Elhage,N.,Hume,T.,Olsson,C.,Nanda,N.,Henighan,T.,Johnston,S.,ElShowk,S.,Joseph,N.,DasSarma,N.,Mann,
B.,Hernandez,D.,Askell,A.,Ndousse,K.,Drain,D.,Chen,A.,Bai,Y.,Ganguli,D.,Lovitt,L.,Hatfield-Dodds,
Z.,Kernion,J.,Conerly,T.,Kravec,S.,Fort,S.,Kadavath,S.,Jacobson,J.,Tran-Johnson,E.,Kaplan,J.,Clark,J.,
Brown,T.,McCandlish,S.,Amodei,D.,andOlah,C.(2022a). Softmaxlinearunits. TransformerCircuitsThread.
https://transformer-circuits.pub/2022/solu/index.html.
Elhage,N.,Hume,T.,Olsson,C.,Schiefer,N.,Henighan,T.,Kravec,S.,Hatfield-Dodds,Z.,Lasenby,R.,Drain,D.,
Chen,C.,etal.(2022b). Toymodelsofsuperposition. arXivpreprintarXiv:2209.10652.
Elhage,N.,Nanda,N.,Olsson,C.,Henighan,T.,Joseph,N.,Mann,B.,Askell,A.,Bai,Y.,Chen,A.,Conerly,T.,etal.
(2021). Amathematicalframeworkfortransformercircuits. TransformerCircuitsThread.
Feng,J.andSteinhardt,J.(2023). Howdolanguagemodelsbindentitiesincontext? arXivpreprintarXiv:2310.17191.
Gao,L.,Biderman,S.,Black,S.,Golding,L.,Hoppe,T.,Foster,C.,Phang,J.,He,H.,Thite,A.,Nabeshima,N.,etal.
(2020). Thepile:An800gbdatasetofdiversetextforlanguagemodeling. arXivpreprintarXiv:2101.00027.
Geva,M.,Caciularu,A.,Wang,K.R.,andGoldberg,Y.(2022). Transformerfeed-forwardlayersbuildpredictionsby
promotingconceptsinthevocabularyspace. arXivpreprintarXiv:2203.14680.
Geva,M.,Schuster,R.,Berant,J.,andLevy,O.(2020). Transformerfeed-forwardlayersarekey-valuememories. arXiv
preprintarXiv:2012.14913.
Godfrey,C.,Brown,D.,Emerson,T.,andKvinge,H.(2023). OntheSymmetriesofDeepLearningModelsandtheir
InternalRepresentations.
Goh,G.,Cammarata,N.,Voss,C.,Carter,S.,Petrov,M.,Schubert,L.,Radford,A.,andOlah,C.(2021). Multimodal
neuronsinartificialneuralnetworks. Distill,6(3):e30.
Gould,R.,Ong,E.,Ogden,G.,andConmy,A.(2023). Successorheads: Recurring,interpretableattentionheadsin
thewild. arXivpreprintarXiv:2312.09230.
Gurnee,W.,Nanda,N.,Pauly,M.,Harvey,K.,Troitskii,D.,andBertsimas,D.(2023). Findingneuronsinahaystack:
Casestudieswithsparseprobing. arXivpreprintarXiv:2305.01610.
Gurnee,W.andTegmark,M.(2023). Languagemodelsrepresentspaceandtime. arXivpreprintarXiv:2310.02207.
Gwilliam,M.andShrivastava,A.(2022). BeyondSupervisedvs.Unsupervised: RepresentativeBenchmarkingand
AnalysisofImageRepresentationLearning.
Hamilton, W.L., Leskovec, J., andJurafsky, D.(2018). DiachronicWordEmbeddingsRevealStatisticalLawsof
SemanticChange.
Hamrick,J.andMohamed,S.(2020). Levelsofanalysisformachinelearning. arXivpreprintarXiv:2004.05107.
Hendel, R., Geva, M., and Globerson, A. (2023). In-context learning creates task vectors. arXiv preprint
arXiv:2310.15916.
Hendrycks,D.andGimpel,K.(2016). Gaussianerrorlinearunits(gelus). arXivpreprintarXiv:1606.08415.
Hendrycks, D., Mazeika, M., and Woodside, T. (2023). An overview of catastrophic ai risks. arXiv preprint
arXiv:2306.12001.
Honnibal, M., Montani, I., VanLandeghem, S., andBoyd, A.(2020). spacy: Industrial-strengthnaturallanguage
processinginpython.
Hryniowski,A.andWong,A.(2020). Inter-layerInformationSimilarityAssessmentofDeepNeuralNetworksVia
TopologicalSimilarityandPersistenceAnalysisofDataNeighbourDynamics.
Huang,J.,Geiger,A.,D’Oosterlinck,K.,Wu,Z.,andPotts,C.(2023).Rigorouslyassessingnaturallanguageexplanations
ofneurons. arXivpreprintarXiv:2309.10312.
Jastrze˛bski,S.,Arpit,D.,Ballas,N.,Verma,V.,Che,T.,andBengio,Y.(2017). Residualconnectionsencourageiterative
inference. arXivpreprintarXiv:1710.04773.
Karamcheti,S.,Orr,L.,Bolton,J.,Zhang,T.,Goel,K.,Narayan,A.,Bommasani,R.,Narayanan,D.,Hashimoto,T.,
Jurafsky,D.,Manning,C.D.,Potts,C.,Ré,C.,andLiang,P.(2021). Mistral-ajourneytowardsreproduciblelanguage
modeltraining.
Khrulkov,V.andOseledets,I.(2018). GeometryScore:AMethodForComparingGenerativeAdversarialNetworks.
Klabunde,M.,Schumacher,T.,Strohmaier,M.,andLemmerich,F.(2023). SimilarityofNeuralNetworkModels:A
SurveyofFunctionalandRepresentationalMeasures.
Kornblith,S.,Norouzi,M.,Lee,H.,andHinton,G.(2019). Similarityofneuralnetworkrepresentationsrevisited. In
Internationalconferenceonmachinelearning,pages3519–3529.PMLR.
15Lange,R.D.,Rolnick,D.S.,andKording,K.P.(2022). Clusteringunitsinneuralnetworks:upstreamvsdownstream
information.
Li,Y.,Yosinski,J.,Clune,J.,Lipson,H.,andHopcroft,J.(2015). Convergentlearning:Dodifferentneuralnetworks
learnthesamerepresentations? arXivpreprintarXiv:1511.07543.
Li,Y.,Yosinski,J.,Clune,J.,Lipson,H.,andHopcroft,J.(2016). ConvergentLearning:Dodifferentneuralnetworks
learnthesamerepresentations?
Liao,I.,Liu,Z.,andTegmark,M.(2023). Generatinginterpretablenetworksusinghypernetworks. arXivpreprint
arXiv:2312.03051.
Lim,J.andLauw,H.(2023). Disentanglingtransformerlanguagemodelsassuperposedtopicmodels. InProceedings
ofthe2023ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages8646–8666.
Lin,B.(2022). GeometricandTopologicalInferenceforDeepRepresentationsofComplexNetworks. InCompanion
ProceedingsoftheWebConference2022,pages334–338.
Lu,Y.,Yang,W.,Zhang,Y.,Chen,Z.,Chen,J.,Xuan,Q.,Wang,Z.,andYang,X.(2022). UnderstandingtheDynamics
ofDNNsUsingGraphModularity.
Marr, D. (2010). Vision: A computational investigation into the human representation and processing of visual
information. MITpress.
McDougall,C.,Conmy,A.,Rushing,C.,McGrath,T.,andNanda,N.(2023). Copysuppression: Comprehensively
understandinganattentionhead. arXivpreprintarXiv:2310.04625.
McGrath,T.,Rahtz,M.,Kramar,J.,Mikulik,V.,andLegg,S.(2023). Thehydraeffect:Emergentself-repairinlanguage
modelcomputations. arXivpreprintarXiv:2307.15771.
Merullo,J.,Eickhoff,C.,andPavlick,E.(2023). Circuitcomponentreuseacrosstasksintransformerlanguagemodels.
arXivpreprintarXiv:2310.08744.
Morcos, A.S., Raghu, M., andBengio, S.(2018). Insightsonrepresentationalsimilarityinneuralnetworkswith
canonicalcorrelation.
Mu,J.andAndreas,J.(2020). Compositionalexplanationsofneurons. AdvancesinNeuralInformationProcessing
Systems,33:17153–17163.
Nanda,N.(2022). Transformerlens.
Nanda,N.,Chan,L.,Liberum,T.,Smith,J.,andSteinhardt,J.(2023). Progressmeasuresforgrokkingviamechanistic
interpretability. arXivpreprintarXiv:2301.05217.
Ngo,R.,Chan,L.,andMindermann,S.(2023). Thealignmentproblemfromadeeplearningperspective.
Nguyen,A.,Yosinski,J.,andClune,J.(2016). Multifacetedfeaturevisualization: Uncoveringthedifferenttypesof
featureslearnedbyeachneuronindeepneuralnetworks.
Nostalgebraist (2020). Interpreting gpt: The logit lens. https://www.alignmentforum.org/posts/
AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens.
Olah,C.(2021). Interpretabilityvsneuroscience. https://colah.github.io/notes/interp-v-neuro/.
Olah,C.,Cammarata,N.,Schubert,L.,Goh,G.,Petrov,M.,andCarter,S.(2020a). Anoverviewofearlyvisionin
inceptionv1. Distill. https://distill.pub/2020/circuits/early-vision.
Olah,C.,Cammarata,N.,Schubert,L.,Goh,G.,Petrov,M.,andCarter,S.(2020b). Zoomin:Anintroductiontocircuits.
Distill,5(3):e00024–001.
Olsson,C.,Elhage,N.,Nanda,N.,Joseph,N.,DasSarma,N.,Henighan,T.,Mann,B.,Askell,A.,Bai,Y.,Chen,A.,
Conerly,T.,Drain,D.,Ganguli,D.,Hatfield-Dodds,Z.,Hernandez,D.,Johnston,S.,Jones,A.,Kernion,J.,Lovitt,
L.,Ndousse,K.,Amodei,D.,Brown,T.,Clark,J.,Kaplan,J.,McCandlish,S.,andOlah,C.(2022). In-contextlearning
andinductionheads. TransformerCircuitsThread. https://transformer-circuits.pub/2022/in-context-learning-and-
induction-heads/index.html.
Quirke,L.,Heindrich,L.,Gurnee,W.,andNanda,N.(2023). Trainingdynamicsofcontextualn-gramsinlanguage
models. arXivpreprintarXiv:2311.00863.
Radford,A.,Jozefowicz,R.,andSutskever,I.(2017). Learningtogeneratereviewsanddiscoveringsentiment. arXiv
preprintarXiv:1704.01444.
Radford,A.,Narasimhan,K.,Salimans,T.,Sutskever,I.,etal.(2018). Improvinglanguageunderstandingbygenerative
pre-training.
16Radford,A.,Wu,J.,Child,R.,Luan,D.,Amodei,D.,Sutskever,I.,etal.(2019). Languagemodelsareunsupervised
multitasklearners. OpenAIblog,1(8):9.
Raghu,M.,Gilmer,J.,Yosinski,J.,andSohl-Dickstein,J.(2017). SVCCA:SingularVectorCanonicalCorrelation
AnalysisforDeepLearningDynamicsandInterpretability.
Reuther,A.,Kepner,J.,Byun,C.,Samsi,S.,Arcand,W.,Bestor,D.,Bergeron,B.,Gadepally,V.,Houle,M.,Hubbell,
M.,Jones,M.,Klein,A.,Milechin,L.,Mullen,J.,Prout,A.,Rosa,A.,Yee,C.,andMichaleas,P.(2018). Interactive
supercomputingon40,000coresformachinelearninganddataanalysis. In2018IEEEHighPerformanceextreme
ComputingConference(HPEC),pages1–6.IEEE.
Sajjad,H.,Durrani,N.,Dalvi,F.,Alam,F.,Khan,A.R.,andXu,J.(2022). Analyzingencodedconceptsintransformer
languagemodels. arXivpreprintarXiv:2206.13289.
Schubert, L., Voss, C., Cammarata, N., Goh, G., and Olah, C. (2021a). High-low frequency detectors. Distill.
https://distill.pub/2020/circuits/frequency-edges.
Schubert, L., Voss, C., Cammarata, N., Goh, G., and Olah, C. (2021b). High-low frequency detectors. Distill,
6(1):e00024–005.
Shahbazi,M.,Shirali,A.,Aghajan,H.,andNili,H.(2021). UsingdistanceontheRiemannianmanifoldtocompare
representationsinbrainandinmodels. NeuroImage,239:118271.
Tang,S.,Maddox,W.J.,Dickens,C.,Diethe,T.,andDamianou,A.(2020).SimilarityofNeuralNetworkswithGradients.
Todd,E.,Li,M.L.,Sharma,A.S.,Mueller,A.,Wallace,B.C.,andBau,D.(2023). Functionvectorsinlargelanguage
models. arXivpreprintarXiv:2310.15213.
Variengien,A.andWinsor,E.(2023). Lookbeforeyouleap: Auniversalemergentdecompositionofretrievaltasks
inlanguagemodels.
Voita,E.,Ferrando,J.,andNalmpantis,C.(2023). Neuronsinlargelanguagemodels:Dead,n-gram,positional. arXiv
preprintarXiv:2309.04827.
Wang,K.,Variengien,A.,Conmy,A.,Shlegeris,B.,andSteinhardt,J.(2022a). Interpretabilityinthewild: acircuit
forindirectobjectidentificationingpt-2small. arXivpreprintarXiv:2211.00593.
Wang, L., Hu, L., Gu, J., Wu, Y., Hu, Z., He, K., and Hopcroft, J. (2018). Towards Understanding Learning
Representations:ToWhatExtentDoDifferentNeuralNetworksLearntheSameRepresentation.
Wang,T.andIsola,P.(2022). UnderstandingContrastiveRepresentationLearningthroughAlignmentandUniformity
ontheHypersphere.
Wang,X.,Wen,K.,Zhang,Z.,Hou,L.,Liu,Z.,andLi,J.(2022b). Findingskillneuronsinpre-trainedtransformer-based
languagemodels. arXivpreprintarXiv:2211.07349.
Weidinger,L.,Uesato,J.,Rauh,M.,Griffin,C.,Huang,P.-S.,Mellor,J.,Glaese,A.,Cheng,M.,Balle,B.,Kasirzadeh,
A.,etal.(2022). Taxonomyofrisksposedbylanguagemodels. InProceedingsofthe2022ACMConferenceon
Fairness,Accountability,andTransparency,pages214–229.
Williams, A. H., Kunz, E., Kornblith, S., and Linderman, S. W. (2022). Generalized Shape Metrics on Neural
Representations.
Xia,M.,Gao,T.,Zeng,Z.,andChen,D.(2023). Shearedllama:Acceleratinglanguagemodelpre-trainingviastructured
pruning. arXivpreprintarXiv:2310.06694.
Xiao,G.,Tian,Y.,Chen,B.,Han,S.,andLewis,M.(2023). Efficientstreaminglanguagemodelswithattentionsinks.
arXivpreprintarXiv:2309.17453.
Xin,J.,Lin,J.,andYu,Y.(2019). Whatpartoftheneuralnetworkdoesthis? understandinglstmsbymeasuringand
dissectingneurons. InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessing
andthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages5823–5830.
Zhang,S.,Roller,S.,Goyal,N.,Artetxe,M.,Chen,M.,Chen,S.,Dewan,C.,Diab,M.,Li,X.,Lin,X.V.,etal.(2022).
Opt:Openpre-trainedtransformerlanguagemodels. arXivpreprintarXiv:2205.01068.
Zhong,Z.,Liu,Z.,Tegmark,M.,andAndreas,J.(2023). Theclockandthepizza:Twostoriesinmechanisticexplanation
ofneuralnetworks. arXivpreprintarXiv:2306.17844.
Zou,A.,Phan,L.,Chen,S.,Campbell,J.,Guo,P.,Ren,R.,Pan,A.,Yin,X.,Mazeika,M.,Dombrowski,A.-K.,etal.
(2023). Representationengineering:Atop-downapproachtoaitransparency. arXivpreprintarXiv:2310.01405.
17A AdditionalEmpiricalDetails
Allofourcodeanddataisavailableathttps://github.com/wesg52/universal-neurons.
Most of our plots in the main text (and therefore neuron indices) correspond to the HuggingFace model
stanford-crfm/arwen-gpt2-medium-x21 with our additional correlation experiments being conducted on
stanford-crfm/alias-gpt2-small-x21andEleutherAI/pythia-160m.
A.1 WeightPreprocessing
Weemployseveralstandardweightpreprocessingtechniquestosimplifycalculations(Nanda,2022).
FoldinginLayerNorm Mostlayernormimplementationsalsoincludetrainableparametersγ∈Rnandb∈Rn
x−E(x)
LayerNorm(x)= ∗γ+b. (6)
(cid:112)
Var(x)
Toaccountforthese,wecan“fold”thelayernormparametersintoW byobservingthatthelayernormparameters
in
areequivalenttoalinearlayer,andthencombinetheadjacentlinearlayers.Inparticular,wecancreateeffectiveweights
W =W diag(γ) b =b +W b (7)
eff in eff in in
Finally,wecancenterthereadingweightsbecausetheprecedinglayernormprojectsouttheallonesvector.Thuswe
cancentertheweightsW becomes
eff
W′ (i,:)=W (i,:)−W¯ (i,:)
eff eff eff
WritingWeightCentering EverytimethemodelinteractswiththeresidualstreamitappliesaLayerNormfirst.Thus
thecomponentsofW andb thatliealongtheall-onesdirectionoftheresidualstreamhavenoeffectonthemodel’s
out out
calculation.So,weagainmean-centerW andb bysubtractingthemeansofthecolumnsofW
out out out
W′ (:,i)=W (:,i)−W¯ (:,i)
out out out
UnembedCentering Additionally,sincesoftmaxistranslationinvariant,wemodifyW into
U
′
W (:,i)=W (:,i)−w
U U i
Forbothoftheses,seethetransformerlensdocumentationformoredetails.
Thepurposeofallofthesetranslationsistoremoveirrelevantcomponentsandotherparameterizationdegreesoffreedom
sothatcosinesimilaritiesandotherweightcomputationshavemean0.
A.2 CorrelationComputations
Wecomputeourcorrelationsovera100milliontokensubsetofthePiletestset(Gaoetal.,2020),tokenizedtoacontext
lengthof512tokens.Wecomputecorrelationsoveralltokensthatarenotpadding,beginning-of-sequence,ornew-line
tokens.
EfficientComputation Becausestoringneuronactivationsfortwomodelsover100Mtokenswouldbe36petabytes
ofdata,werequireastreamingalgorithm.Todoso,observethatgivenapairofneuronactivations{(x ,y ),...,(x ,y )}
1 1 n n
consistingofnpairs,thecorrelationcanbecomputedas
(cid:80)n
(x −x¯)(y −y¯)
(cid:80)
x y −nx¯y¯
ρ = i=1 i i = i i i
xy (cid:113)
(cid:80)n i=1(x
i−x¯)2(cid:113)
(cid:80)n i=1(y i−y¯)2
(cid:112)(cid:80)
ix2
i−nx¯2(cid:112)(cid:80)
iy i2−ny¯2
wherex¯,y¯arethesamplemean. Therefore,insteadofsavingallneuronactivations,wecanmaintainfourn_neuron
dimensionalvectorsandonen_neuron×n_neuronmatrixcorrespondingtotherunningneuronactivationmeansin
modelAandmodelB,arunningsumofeachneuronssquaredactivation,andarunningsumofpairwiseproducts.Atthe
endofthedataset,weperformtheappropriatearithmetictocombinetheresultsintopairwisecorrelationsforallmodels.
A.3 ModelHyperparameters
18Property GPT-2Small GPT-2Medium Pythia160M
layers 12 24 12
heads 12 16 12
d 768 1024 768
model
d 50257 50257 50304
vocab
d 3072 4096 3072
MLP
parameters 160M 410M 160M
context 1024 1024 2048
activationfunction gelu_new gelu_new gelu
posembeddings absolute absolute RoPE
rotarypercentage N/A N/A 25
precision Float-32 Float-32 Float-16
dataset OpenwebText OpenwebText Pile
p 0.1 0.1 0
dropout
Table1:Hyperparametersofmodels
B AdditionalResults
Figure12:SummaryofneuroncorrelationexperimentsinGPT2-small-a.(a)Distributionofthemean(overmodelsb-e)
max(overneurons)correlation,themeanbaselinecorrelation,andthedifference(excess).(b)Themax(overmodels)
max(overneurons)correlationcomparedtothemin(overmodels)max(overneuron)correlationforeachneuron.(c)
Percentageoflayerpairswithmostsimilarneuronpairs.
19Figure13:SummaryofneuroncorrelationexperimentsinPythia-160m.(a)Distributionofthemean(overmodelsb-e)
max(overneurons)correlation,themeanbaselinecorrelation,andthedifference(excess).(b)Themax(overmodels)
max(overneurons)correlationcomparedtothemin(overmodels)max(overneuron)correlationforeachneuron.(c)
Percentageoflayerpairswithmostsimilarneuronpairs.
20Figure14:Distributionofneuronmetricsforuniversalandnon-universalneuronsinGPT2-medium-abylayer.From
toptobottom:thekurtosisofcos(W ,w ),theskewofcos(W ,w ),cosinesimilaritybetweeninputandoutput
U out U out
weightcos(w ,w ),weightdecaypenalty∥w ∥2+∥w ∥2,activationfrequency(percentageofactivationsgreater
in out in 2 out 2
than0),thepre-activationskew,andthepre-activationkurtosis.
21Figure15:DuplicateunigramneuronsinGPT2-medium-a.Eachsubplotdepictsseveralneuronswhichactivateona
particulartoken,brokendownbywhetherthistokenexistsasastandaloneword,isthefirsttokeninamulti-tokenword,
orisanon-firsttokeninamulti-tokenword,versusallothertokens(e.g.,“an,”“an|agram,”“Gig|an|tism”).
22Figure16:UniversalunigramneuronsinGPT2-medium-a.
23Figure17:UniversalalphabetneuronsinGPT2-medium-a.
24Figure18:UniversalprevioustokenneuronsinGPT2-medium-a.
25Figure19:UniversalpositionneuronsinGPT2-small-a.
26Figure20:UniversalsyntaxneuronsinGPT2-medium-a.
27Figure21:UniversalcontextneuronsinGPT2-medium-a.
28Figure22:DistributionofvocabularycompositionstatisticsforfivedifferentPythiamodelsmeasuredoverlayers.Left
showspercentilesofcos(W ,W )kurtosis. Rightshowspercentilesofcos(W ,W )skewbrokendownby
U out U out
whetherneuronhascos(W ,W )kurtosisgreaterthanorlessthan10.
U out
29Figure23:UniversalpredictionneuronsinGPT2-medium-a.
30Figure24:PredictionneuronsforthesamefeatureinGPT2-medium-a.Leftcolumndepictslogiteffectbrokendownby
vocabularyitemperneuronandrightcolumnshowsactivationvaluebrokendownbytruenexttokenperneuron.
31Figure25:Summaryof(anti-)entropyneuronsinGPT2-small-acomparedto20randomneuronsfromfinaltwolayers.
Entropyneuronshavehighweightnorm(a)withoutputweightsmostlyorthogonaltotheunembeddingmatrix(b).When
activated,thiscausesthefinallayernormscaletoincreasedramatically(c)whileleavingtherelativeorderingoverthe
nexttokenpredictionmostlyunchanged(d).Increasedlayernormscalesqueezesthelogitdistribution,causingalarge
increaseinthepredictionentropy(e;ordecreaseforanti-entropyneuron)andanincreaseordecreaseinthelossdepending
onthemodel’sbaselinelevelofunder-orover-confidence(f).Legendappliestoallsubplots.
32Figure26: Furtherexamplesofattentionactivationanddeactivationneurons. Row1: A15H8withL14N411,Row
2: A15H8withL14N2335,Row3: A15H8withL14N1625,Row4: A20H4withL19N2509,Row5: A22H7with
L20N2114
33