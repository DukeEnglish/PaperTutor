Exploring Simple Open-Vocabulary Semantic Segmentation
ZihangLai
UniversityofOxford
zihang.lai@eng.ox.ac.uk
Abstract Figure1. S-Segresultonawebimage. Ourgoalistosegment
everything,includingfictionalcharacterslikeminions.
Open-vocabularysemanticsegmentationmodelsaimto
accuratelyassignasemanticlabeltoeachpixelinanimage
fromasetofarbitraryopen-vocabularytexts. Inorderto
learn such pixel-level alignment, current approaches typ-
ically rely on a combination of (i) image-level VL model
(e.g.CLIP),(ii)groundtruthmasks,and(iii)customgroup-
ing encoders. In this paper, we introduce S-Seg, a novel
model that can achieve surprisingly strong performance
without depending on any of the above elements. S-Seg Pseudo-mask Mask
leveragespseudo-maskandlanguagetotrainaMaskFormer, generator supervision
and can be easily trained from publicly available image-
A yellow
Language Language Semantic
textdatasets. Contrarytopriorworks,ourmodeldirectly rubber duck
model feature supervision
trainsforpixel-levelfeaturesandlanguagealignment. Once sits in water.
trained,S-Seggeneralizeswelltomultipletestingdatasets
Figure2. OurS-Segframeworkleveragespseudo-maskandlan-
without requiring fine-tuning. In addition, S-Seg has the
guagetotrainaMaskFormer.Weshowthatourmethodofdirectly
extrabenefitsofscalabilitywithdataandconsistentlyim-
trainingforpixel-levelfeatureandlanguagealignmentyieldssupe-
provementwhenaugmentedwithself-training. Webelieve
riorresults.
thatoursimpleyeteffectiveapproachwillserveasasolid
baselineforfutureresearch. Ourcodewillbereleasedat
OnecommontacticisadaptingexistingVision-Language
https://github.com/zlai0/S-Seg.
(VL) models, which are initially trained for image-level
alignment (e.g., CLIP [46]), to perform at the pixel level.
Anotherstrategyinvolvestrainingmodelsongroundtruth
1.Introduction
masksthatareannotatedforaselectnumberofseenclasses,
Open-vocabularysemanticsegmentationpresentsaunique thereby encouraging the model to extrapolate its learning
challengeasitrequiresassigningaccuratesemanticlabels tonovelunseenclasses. Furthermore, specializedmodels
toeachpixelinanimageusingarbitraryopen-vocabulary suchasGroupViT[57]andOVSegmentor[58],whichare
texts,ratherthanafixedsetofclasses. Thismeansthatthe explicitlydesignedforopen-vocabularysegmentation,are
model must be able to segment and classify any arbitrary beingexplored. Thesemodelstypicallygroupsimilarpixels
categoriesexpressedinlanguage. Achievingthisrequiresa withintheimageencoderbasedontheirfeatures,employing
robust,pixel-levelalignmentbetweenimagesandtextualde- a hierarchical approach, enhancing the model’s ability to
scriptions,whichenablesaccurateassociationofeachpixel understandtheimageatmultiplegranularities.
withthemostrelevantclassfromadynamicallyprovidedset In this paper, we report a model that can work surpris-
oftextualcategories. inglywellwithnoneoftheabovestrategies. Ourapproach,
A primary obstacle in this domain is that it is impossi- named S-Seg, is built on top of a standard MaskFormer
bletoconstructdatasetsthatprovidepixel-levelannotations model. Ourmodeldirectlytrainsforpixel-levelfeatureand
for all possible labels. This limitation often results in the languagealignment,usingneitherexistinglargeimage-level
adoptionofweakly-supervisedorsemi-supervisedlearning alignmentmodelslikeCLIP[46]nor manuallyannotated
approaches. Currentmethodstypicallyrelyonacombina- segmentationorclassificationlabels.
tionofstrategiestolearntherequiredpixel-levelalignment. Oneofthebiggestchallengeswefaceisfindingtheright
1
4202
naJ
22
]VC.sc[
1v71221.1042:viXra
riap
txet-egamI
minions
tiger
Maoia
PASCAL VOC PASCAL Context COCO
Background Sheep Cat Duck Water Cat Floor Background Cow Person Bowl Orange
Figure3.QualitativeresultsofS-Seg,evaluatedusingalldatasetclassesasqueries.Ourmodelcopeswithchallengingsituation,suchas
overlappingobjects(col.2)andsmallobjects(col.5).Ourmodelisalsocapableofhandling“stuff”categoriessuchaswaterandfloor(col.
3,4).Moreover,ourS-Seg+modelisabletocorrectsmallerrorsobservedintheS-Segmethod(col.4).Finally,intheCOCOdataset,which
featuredasignificantlyhighernumberofobjects,ourmodelisstillabletoachievehighaccuracyinitspredictions.
supervisionsinceannotatedmasksandlabelsarenotavail-
# I [n, h, w, c] - minibatch of aligned images
able. Toaddressthisissue,weproposetoleveragepseudo- # T [n, l] - minibatch of aligned texts
masksandlanguagetosuperviseMaskFormer. Ourstrategy # N - number of MaskFormer queries
# C - number of pseudo masks
involves using a pseudo-mask generator to provide class-
agnostic mask supervision by generating pseudo ground # predict mask, mask feature, and text feature
M, M_f = maskformer(I) # [n, N, H, W], [n, N, d_f]
truthmasks. Weadoptasimpledesignthatclustersimage
T_f = text_encoder(T) # [n, d_f]
representationsobtainedthroughself-supervisedrepresen-
tation learning methods like DINO [5]. Our experiments # aggregate all mask features [n, d_f]
M_f = M_f.mean(axis=1)
demonstratethatthisapproachdeliversexceptionalperfor-
mance, whichisessentialforhigh-qualitysupervision, as # generate pseudo masks [n, C, H, W]
wellasrapidprocessingspeed,whichisnecessaryforeffi- S = pseudo_mask_generator(I)
cienttraining. Inaddition,weusenoisywebtextstoprovide
# compute loss
semantic supervision. The image-text dataset contains a loss_c = contrastive_loss(M_f, T_f)
wide range of concepts and has demonstrated impressive loss_m = mask_loss(M, S)
loss = (loss_c + loss_m)/2
zero-shot classification results [46]. We utilize a straight-
forwardimage-textcontrastiveloss,whichhasproventobe Figure4.PseudocodefortrainingS-Segwithimage-textpairs.
highlyeffective. Oncetrained,ourmodelgeneralizeswell
tonewcategorieswithoutrequiringfine-tuning. datasets,highlightingthepotentialforfurtherimprovement
ofourapproach.
S-Segisasimpleandeffectivemodelthatcanbetrained
Oursimplesolutionsuggeststhattherelianceoncomplex
usingpubliclyavailableimage-textdatasets,suchasConcep-
modelsandextensivegroundtruthdatainopen-vocabulary
tualCaptions[7,48]. Thismakesiteasytoreproduceand
semanticsegmentationmaybereduced,leadingtoamore
extend for further research. The S-Seg framework is also
streamlined and accessible framework for future develop-
designedtobeflexiblewitheasilyreplaceablesubmodules.
mentsinthefield,andwehopeourexplorationcanserveas
We prioritize simplicity in our subcomponent selection to
asolidbaselineforfutureresearch.
focusonthegeneraldesignofourframework,whileremain-
ingopentomoreadvancedtechniquesthatcouldresultin
2.Relatedwork
furtherimprovements.
WeconductedathoroughevaluationofS-Segusingmul- Open-vocabularysegmentation. Theearliesteffortstoem-
tiple benchmark datasets, and we show that our method ploylanguageforimagesegmentationcanbetracedback
achieve competitive results on three widely tested bench- to Duygulu et al.’s seminal work [16], where the authors
marks(PascalVOC,PascalContext,andCOCO).Inaddition, tackledimagesegmentationbyframingitasamachinetrans-
pseudo-maskandlanguageprovidescalablesupervisionand lationproblem. Currentapproachesleverageacombination
our model consistently improves in performance as more ofstrategytolearnpixel-levelimage-textalignment.
databecameavailable. Finally,wefindaddinganadditional Adaptingimage-levelvision-languagemodels. Thefirst
self-trainingstepleadstoanevengreaterimprovementtoour strategy involves the adapting pretrained vision-language
model,withanaverageincreaseof5.5%mIoUoverthree models, originallydesignedforimage-levelalignment, to
2
tupnI
geS-S
+geS-SImage-text pair
N Masks Pseudo masks
Image
(from image-text pair)
Lmask
Pseudo-Mask
Generator
N Mask Features Lcontrast Language Language A yellow
rubber duck
Feature Model
sits in water.
Mask and Semantic supervision
Figure5.OverviewofS-Seg.AMaskFormermodelcomputesmasksandmaskfeaturesfromanimageinput.Apseudo-maskgenerator
producessegmentationmapstosupervisemaskpredictions,whileatextthatdescribestheimage,encodedbyalanguagemodeltrained
togetherwiththeMaskFormer,providessupervisionformaskfeaturesusingimage-textcontrastiveloss.
themoregranulartaskofpixel-levelalignment.Thisstrategy tention[38]forasimilarpixelgroupingprocessbasedon
is widely adopted in open-vocabulary methods [6, 18, 23, featureproximity.
24,30,31,33,41,42,47,53,58,59,61]. Theseworksvary Our model, S-Seg, can be conceptualized as a synergy
in their methods of refining image-level models for finer of these approaches. It can be viewed as a CLIP model
alignmenttasks. integratedwithaMaskFormerimageencoder,directlyopti-
MaskCLIP[61]demonstratesmodifyingtheCLIPimage mizingforpixel-levelfeatureandlanguagealignment. Al-
encodercansignificantlyenhanceitspixel-levelalignment ternatively,itresembles“ZegFormerwithpseudomaskand
capabilitieswithoutrequiringretraining. TCL[6]employs languagetraining”or“GroupVitwithaMaskFormerasthe
CLIPforinitialtext-to-imageregiongrounding, followed grouping mechanism.” Interestingly, our model relates to
bycontrastivelearningtorefinethealignmentbetweenthe eachmethodbyomittingcertaincorearchitecturalcompo-
text embedding and the grounded region. OpenSeg [18] nentsorsupervisionmethod. Evenso,ourmethodisableto
fine-tunesALIGN[28]usingagroundingloss[21]tobetter achievecompetitiveperformance.
alignwordsincaptionstosegmentationmasks. OpenSeg Unsupervised image grouping. Unsupervised image
andDiffuMask[53]alsoexploredtheuseofpseudo-masks. grouping methods are designed to segment images with-
Theprimarydistinctionliesintheirdependencyondiffer- outtheuseofmanuallylabeledsegmentationmasks. Early
entsourcesforlearning;OpenSegusesannotatedsegments unsupervisedimagegroupingmethodscanberoughlycate-
whileDiffuMaskemploysmasksgeneratedthroughdiffu- gorizedaslow-levelfeature-based[3],clustering-based[29],
sion. Incontrast,ourmethodisentirelylearnedfrompseudo and graph-based [49]. More recently, self-supervised
masks. Also,ourmaskgeneratorisentirelyself-supervised, learning-basedapproaches[11,22,26,27,50,51,60]have
whereastheirmaskgeneratorisfully-supervised. shownsuperiorperformanceinunsupervisedimagegroup-
ing.
Groundtruthmasks. Anothereffectivestrategy[14,18,
23, 24, 31, 33, 42] involves training models using ground
3.Approach
truthmasksannotatedforalimitedsetofseenclasses. By
trainingonseenannotations,modelsareencouragedtolearn Ourproposedmethod,calledS-Seg,isconceptuallysimple:
detailedfeaturesandpatternsthatarepotentiallyapplicable welearnaMaskFormermodelfrompseudo-maskandlan-
beyondthescopeofthetrainedclasses. guage. Ourmethodleveragesimage-textpairssolely,with-
Ofmostrelevance,ZegFormer[14]trainsaMaskFormer outrelyingongroundtruthmasksorlarge-scalepreatrained
by decoupling zero-shot semantic segmentation into two models. In figure 4, we provide pseudocode for the core
sub-tasks, a class-agnostic grouping task and a zero-shot implementation of training S-Seg. Figure 5 provides a
segmentclassificationtask. Ourmethodhassimilartraining schematiclayoutofourapproach.
paradigmbutwithnotabledistinctions. SimilartoGroupViT,
3.1.Problemdefinition
we train exclusively with image-text pairs and do not uti-
lizeapretrainedCLIPmodel. Notably,evenwithoutaccess Weconsidertheproblemofopen-vocabularysemanticseg-
togroundtruthmasks,labels,orCLIP,ourmethodoutper- mentation,whereweaimtolearnafunctionf thatmapsan
formsZegFormerinunseencategories,indicatingpotentially imageI andasetofcategorynamesC ={c }toasemantic
i
strongergeneralization. segmentation map S, where c can be any category name
i
Custom grouping-based encoders. The third strategy expressedasopenvocabularytexts.
employs custom-designed models specifically for open- Our approach is based on previous works [47, 57, 61],
vocabularysegmentation. GroupViT[57]groupspixelsin and we adopt their problem setting. Specifically, we use
animagehierarchicallybasedontheirattentionscoreswith a web dataset of image-text pairs (I ,T ) during training,
i i
learnablegrouptokens. OVSegmentor[58]appliesSlotAt- whereT isatextuallabelthatdescribesthecontentofthe
i
3
remroFksaMcorrespondingimageI . However,sincethetextuallabels
i
aregatheredfromtheweb,theymaybenoisyandcontain N Masks Combine output
Image
errors. We do not use any additional manual annotated N Mask Features
segmentationorclassificationlabelsduringtraining.
Duringtesting,asetofcategorynamesCisprovided,and
themodelistaskedwithassigningasemanticlabelc
i
∈C TTeTeexxxttt La Mng ou da eg le L Fa en ag tu ua reg se Classify N Classes
to each pixel in an unlabeled image. The performance of
themodelisevaluatedbasedonitsmeanIntersectionover Zero-shot classifier on mask features
Union(mIoU)withthegroundtruthlabels.
Figure6.TestingonS-Seg.Duringinference,S-Seggeneralizeto
3.2.AdaptingMaskFormer newcategoriesbyleveraginglanguagefeaturesgeneratedfroma
listofcandidateclassesintext.
Our approach builds on top of MaskFormer [10]. Here,
we begin by briefly review MaskFormer and explain the
adjustmentswemade.
DINO
TheMaskformermodeltakesanimageasinputandgen- K-Means
ViT
eratesN masksandmaskfeatures. First, theinputimage
passesthroughabackbonemodeltoproducefeaturemapsat
differentoutputresolutions. Theseimagefeaturesarethen Figure7.ThePseudo-maskgeneratorgeneratespseudo-masks
fed into a per-pixel encoder, which upsamples and aggre- tosupervisepredictedmaskduringtraining.Thismoduletakesan
gatesthemintoasetoffeaturemapswithhigherresolution. imageasitsinput,extractsitsfeaturesusingaDINOpre-trained
Meanwhile,atransformerdecoderusesN learnablequeries ViT,andthenemploysK-meansclusteringtogroupthepixelsinto
tocross-attendtothesetoffeatureswiththelowestresolu- segments.
tionandgatherglobalinformationabouteachsegment.
IntheoriginalMaskformer,alinearclassifierandsoftmax Method Sup. P.VOC↑ P.Context↑ Time(s)↓
activationwereappliedtotheoutputofthedecodertopredict
SpectralClus.[49]* none 49.2 43.2 0.543
class probabilities for a fixed list of categories. However, K-Means[29]* none 49.5 43.3 0.188
aswedonothaveafixedlistofcategories,weremovethis ImageNet[15]+[29] label 68.8 58.1 0.079
GroupViT[57] text 73.7 54.6 0.002
classifierbranchandoutputtheN rawmaskfeaturesinstead.
Inadditiontopredictingmaskfeatures,theMaskformer Pseudo-mask(Ours) self 78.8 66.3 0.002
alsopredictsN binarymasks. Topredicteachmask,adot Table1.Ourpseudo-maskgeneratorachievesexcellentoracle
product is taken between the mask embedding, generated performancewithrapidspeed,makingitanidealmasksupervi-
frommaskfeatures,andthehighresolutionper-pixelfeature. sion.Wereportamortisedrunningtimeonabatchof128samples,
Finally, N mask-feature pairs are combined to generate a simulatingtrainingtimescenario.*Weprocessdownsampledim-
semanticsegmentationmapastheoutput. ageat H 8 × W 8 resolutiontoobtainreasonablerunningtime.
3.3.S-Seg
In the testing phase (as shown in figure 6), the trained
S-SegemploysMaskFormerasitssegmentationmodel,but
MaskFormer model predicts N masks and mask features
inourweakly-supervisedlearningsetting(whereonlytexts
fromtheinputimage. Thelanguagemodeltakesasinputa
areavailable),wefacethechallengeofnothavingannotated
listofcandidatecategorynames(representedastexts)and
masksandlabels. Toovercomethis,weutilizepseudolabels
extractsasetoflanguagefeatures. Thesefeaturesarethen
andlanguagetoassupervision.
usedtoclassifythemaskfeatures. Thisprocessissimilar
Our training framework is illustrated in Figure 5. We
to the one used in CLIP [46], where the image and possi-
firstgenerateasetofsegmentationmapsusingourpseudo-
bletextinputsareencodedbytheirrespectiveencodersto
mask generator (Sec. 3.3.1) and use them as supervision
computefeatureembeddings. Thecosinesimilaritybetween
formaskprediction. Meanwhile,weusealanguagemodel
theseembeddingsiscalculatedandadjustedbyalearnable
to process input text and generate language embeddings.
temperatureparameter. Theresultingvaluesarenormalized
Theseembeddingsprovidesupervisionformaskfeaturesby
intoaclassprobabilitydistributionusingasoftmaxfunction,
leveragingimage-textcontrastiveloss(Sec.3.3.2).
and a combination module is used to takes N mask-class
Notably, unlike the supervised learning setting, where
pairstoproducethefinalsegmentationmap,similarto[10].
maskandlabelannotationsarecoupled,wedecouplemask
andsemanticsupervision. Thisenablesustoutilizepseudo- Next,wewillprovideadetaileddescriptionofthesub-
maskandlanguageastwodistinctformsofsupervision. componentsinourframework.
4
remroFksaMPseudo mask Oracle segment wecomputethemasklossbetweenpredictedmasksandtheir
correspondingpseudo-mask,utilizingacombinationofdice
loss[43]andfocalloss[35].
L =λ L +λ L (1)
mask dice dice focal focal
3.3.2 LanguageSupervision
Ourmodellearnstoclassifyopen-vocabularyconceptsfrom
languagesupervision. Totrainthemodel,weuseanimage-
textcontrastiveloss[18,46]. Specifically,weviewN mask
Figure8.Examplepseudo-masks.Ourpseudo-maskgeneratoris
featuresasrepresentationoftheinputimage,eachcapturing
capableofgeneratinghigh-qualityartificialmasks.Whenprovided
information about a different part of the image. We then
with an oracle label, these masks demonstrate a high degree of
compute a single feature that represents the entire image
overlapwiththegroundtruthannotations.
by taking the average of these mask features. To encode
3.3.1 Pseudo-MaskGenerator thetext, weuseatexttransformer[52]andselecttheem-
beddingcorrespondingtothe[EOS]token,resultingina
Inourapproach,weuseapseudo-maskgenerator(fig.7)to textual feature. Since the visual and textual features may
produceaclass-agnosticsegmentationmapfromtheinput havedifferentdimensions, weprojecteachrepresentation
image,whichsupervisesthemaskpredictionofourmodel. intoacommonembeddingspaceusing2-layerMLPs. To
Toimplementthepseudo-maskgenerator,weadoptasim- compute the image-text contrastive loss, we calculate the
plestrategythatinvolvesclusteringtokensextractedfroma cosine similarity between the image embeddings and the
self-supervisedpre-trainedViT.Specifically,weuseaDINO- textembeddingswithinthesamebatch. Followingcommon
pretrainedViTtocomputeasetoffeaturizedtokensfromthe practice[32,45,46],wedecoupletheimage-textcontrastive
inputimage.Wethenapplyaclusteringalgorithm(K-Means lossintotwoparts:
in our case) to these tokens, assigning each token a label
thatcorrespondstotheindexoftheclusteritbelongsto. We
N ⊺
reshapetheresultinglabelmapintoanimageandresizeitto L =− 1 (cid:88) log exp(x iy i/σ) (2)
theoriginalresolutiontosupervisethemaskpredictionof I→T N (cid:80)N exp(x⊺ y /σ)
i j=1 i j
oursegmentationmodel.
N ⊺
Despite its simplicity, our pseudo-mask generator
L =−
1 (cid:88)
log
exp(y ix i/σ)
(3)
achievesbothimpressiveperformance,whichiscrucialfor T→I N (cid:80)N exp(y⊺ x /σ)
i j=1 i j
high-qualitysupervision,andfastprocessingspeed,whichis
wherex andy areL2-normalizedembeddingofimage
i i
essentialforefficienttraining. Weevaluateitsperformance
and text of the i-th pair. N denotes batch size and σ is a
andcompareagainstbaselinemethods,andthequantitative
learnable temperature parameter optimized together with
resultsarepresentedinTable1,withexamplepredictions
the rest of the model. The total loss is the sum of these
visualizedin8.Ourmethodsignificantlyoutperformssimple
twolosses,L =L +L . Thislossfunction
contrastive I→T T→I
baselinessuchasK-MeansandSpectralClustering,which
promoteshighsimilarityforpositivepairsandlowsimilarity
naively cluster image pixels, while running two orders of
fornegativepairs. Thelossisminimizedwhenthepositive
magnitudefaster. WealsoobservedthatclusteringDINO
image-textpairshavethehighestsimilarity. Toincreasethe
representationoutperformsclusteringImageNetpre-trained
contrastiveefficiency,weaggregatenegativesamplesfrom
ViT representation by a significant margin. Notably, our
allnodeswhenweusedistributedtraining,enablingmore
pseudo-maskgeneratorevenoutperformsGroupViT,which
negativesamplestobecomparedagainst.
hasalreadyemployedvision-languagetraining.
Since the predicted masks are unordered, we need to
3.3.3 TrainingLoss
matchtheN predictedmaskswithK pseudogroundtruth
masks. To accomplish this, we utilize bipartite matching, Overall,maskloss(Sec.3.3.1)andimage-textcontrastive
as described in [4, 10], which assigns a pseudo-mask to loss(Sec.3.3.2)completethenecessarymaskandsemantic
eachpredictedmasksuchthattheoverallassignmentcost supervisionthatisneededtotrainourmodel. Thefinalloss
isminimalinallpossibleassignments. Sinceeachpseudo- isaweightedcombinationofthetwolosses:
mask is assigned to at most one predicted mask, N −K
L=λ L +λ L (4)
pseudo-masksareunassignedtono-object(Ø).UnlikeMask- mask mask contrastive contrastive
Former[10],wedonotpenalizetheseno-objectmasks,nor Inourexperiment,weuseλ =1.0,λ =1.0,
mask contrastive
doweuseclassificationlossasanassignmentcost. Finally, λ =1.0,λ =20.0.
dice focal
5Input CLIP MaskCLIP GroupViT Ours Ours+ Fully Sup. Annotation
Figure9.Qualitativecomparisonwithexistingmethods.CLIP[46]isprimarilydesignedforclassificationanddoesnotperformwellin
segmentation.MaskCLIP[61]adaptsCLIPforsegmentation,althoughitproducesnoisypredictionsandcannothandlebackgroundclasses.
GroupViT[57]isastrongcompetitor,butitcouldstruggleinchallengingscenarios.
3.3.4 Self-training Method P.VOC P.Context COCO 3-Avg.
B1:PseudoMask+CLIP 12.9 3.9 2.9 6.6
B2:Pseudo-maskViT 23.2 11.0 10.4 14.9
In order to enhance our results, we introduce an optional
step wherein we train a new model using the predictions S-Seg(Ours) 44.9 22.9 22.5 30.1
generatedbyourcurrentmodel.Thisprocessofself-training Table2.Simplebaselinesforopen-vocabularysemanticsegmen-
resultsinanaugmentedmodel,whichwerefertoasS-Seg+. tation.AllmodelsaretrainedonCC12M.Highervaluesarebetter.
Morespecifically,whenweevaluateonagivendataset,we Twosimplebaselinesfailtoobtainsatisfactoryresults,evenusing
generatepseudolabelsfortheunlabeledimagesinthetrain- afterusingourpseudomasksandnolesstrainingdata.
ing set. Subsequently, we employ these pseudo labels to
4.1.Simplebaselines
trainanewsegmentationmodel.
Self-trainingimprovestheaccuracybyleveragingaddi- The high quality of pseudo-masks (as shown in Figure 7)
tionaldata[56],augmentation[62],andbootstrapping[20]. mayleadonetoassumethattheprimarychallengeissimply
In our situation, self-training offers even greater benefits classifyingthesemasks,andthatthiscanbeaccomplished
sincewecantakeadvantageofadditionalinformationthat byutilizingpre-existingmethodssuchasCLIP.Totestthis
isobtainableduringtesting: unlabeledimagesandtesting assumption,wefirstdeveloptwosimplebaselines.
categories. Weshowthatthisadditionalstepimprovesour Baseline1: Pseudo-mask+CLIP.Firstly,ourpseudo
resultssignificantlyatnoextramanuallabellingcost. labelgeneratorisutilizedtoobtainpseudosegments. Then,
weiteratethroughallthemasksandapplythecurrentmask
totheoriginalimage. Next,themaskedimageisfedtoCLIP
4.Experiments forclassificationandtheresultingclasslabelisassignedto
thecorrespondingsegment.
In this section, we empirically evaluate our method and Baseline2: Pseudo-maskViT.Weintroduceanewvi-
compare to existing approaches. We show that, although sualbackbonethatdiffersfromtheregularViT.Insteadof
our method is quite simple, it performs surprisingly well poolingallimagetokensintoasinglefeature,wefirstindi-
against more complex existing methods. We evaluate the vidually pool tokens in each segment of the pseudo-mask
open-vocabularysemanticsegmentationperformanceofS- into segment features, and then pool these features into a
Seg on the validation set of three datasets: Pascal VOC visualembedding. WetrainaCLIP-likemodelfromscratch
2012[17](21classes),PascalContext[44](60classes)and usingthisvisualbackbone. Duringtesting,weclassifyeach
COCO[34](81classes). Formoreimplementationdetails, segmentfeatureandassignthelabeltothatsegment.
pleaserefertooursupplementarymaterials. The results are presented in Table 2. As we can see,
6Method OV Sup. P.VOC P.Context COCO Method OV Sup. P.VOC P.Context COCO
Linearly-probedclassificationmodels: Open-vocabularymodels(annotatedmasksrequiredfortraining):
MoCov3[9] ✗ self 34.3 21.3 - SPNet[54] ✓ mask+text 18.3 24.3 -
DINO[5] ✗ self 39.1 20.4 - ZS3Net[2] ✓ mask+text 38.3 19.4 21.1
LSeg[31] ✓ mask+text 52.3 - 27.2
Open-vocabularymodels(annotatedmasksnotrequiredfortraining): OpenSeg[18] ✓ mask+text 77.2 45.9 38.1
CLIP[46]† ✓ text 13.5 8.1 5.9 ZegFormer[14] ✓ mask+text 80.7 - -
MaskCLIP[61]† ✓ text 26.8 22.8 12.8 GKC[24] ✓ mask+text 83.2 45.2 -
ViL-Seg[36] ✓ text 34.4 16.3 16.4 ODISE[59] ✓ mask+text 85.7 84.6 65.2*
GC roL uI pP Vpy iT[4 [7 5] 7] ✓ ✓ t te ex xt t 5 52 0. .2 8 23- .7 27- .5 OD Ve SO ep g[ [2 33 3] ] ✓ ✓ m ma as sk k+ +t te ex xt t 9 91 4. .7 5 4 58 5. .8 7 - -
SegCLIP[41] ✓ text 52.6 24.7 26.5 SAN[42] ✓ mask+text 94.6 57.7 -
OVSegmentor[58] ✓ text 53.8 20.4 25.1 Open-vocabularymodels(annotatedmasksnotrequiredfortraining):
TCL[6] ✓ text 55.0 30.4 31.6 CLIP[46]† ✓ text 39.6 9.0 13.8
S-Seg(Ours) ✓ text 53.2 27.9 30.3 MaskCLIP[61]† ✓ text 49.5 25.5 23.6
S-Seg+(Ours) ✓ text 62.0 30.2 35.7 GroupViT[57]† ✓ text 77.2 23.0 37.5
TCL[6] ✓ text 83.2 33.9 -
Fully-supervisedsegmentationmodels:
DeepLabV3+†[8] ✗ GT 78.7 46.4 55.7 S-Seg(Ours) ✓ text 81.8 27.2 42.4
S-Seg+(Ours) ✓ text 84.7 31.6 53.0
MaskFormer†[10] ✗ GT 81.2 50.0 62.1
Fully-supervisedsegmentationmodels:
DeepLabV3+†[8] ✗ GT 89.9 48.5 66.9
Table3.Open-vocabularysemanticsegmentationresults(back-
groundpixelsincludedinevaluation).BenchmarkedonPascal
Table4.Open-vocabularysemanticsegmentationresults(back-
VOC(P.VOC),PascalContext(P.Context)andCOCO,following
groundpixelsexcludedinevaluation).Benchmarkedfollowing
standardevaluationprotocolsonopen-vocabularymodeltrained
withoutannotatedmasks[6,41,57,58].Ourapproachobtainsec-
standardprotocolforevaluatingopen-vocabularymodelswithanno-
tatedmasks[14,23,24].S-Segachievescompetitiveperformance
ondhighestperformanceonaverageandhasbetterresultsthan
GroupViTonalldatasets.†denotesourrecomputedresults.Higher
comparedtoearliermethodssimilartotheprevioussetting.†de-
notesourrecomputedresults.*COCOisusedfortraining.Higher
valuesarebetter.
valuesarebetter.
open-vocabularysegmentationismorecomplexthansim-
4.3.Evaluationwithoutbackground
ply grouping image into segments and then categorizing
themintoclasses,evenwhenthesegmentsareofhighqual- Wealsoevaluateourmodelontheevaluationprotocolcom-
ity. Baseline 1 employs a significantly larger pretrained monly used for evaluating open-vocabulary models with
CLIPViT/L-14modelthatwasalsotrainedonamuchlarger annotatedmasks[14,23,24],wherethebackgroundpixels
dataset,whileBaseline2istrainedusingthesamedataas areexcludedinevaluation. Wenotethatthissettingiseasier
ours. Nevertheless, bothbaselinesfailtoachievesatisfac- becausebackgroundclassismorediverseinappearanceand
toryresults,suggestingthatopen-vocabularysegmentation often requires additional processing such as thresholding.
cannotbenaivelydeconstructedinsuchways. Wehypoth- Table4showstheresults. Similartotheprevioussetting,our
esizethatamulti-tasklearningapproachthatjointlytrains S-SegandS-Seg+modelsachievecompetitiveperformance
forsegmentationandclassificationcouldyieldsignificant comparedtoearliermethods.
advantages.
4.4.Ablationstudies
4.2.Evaluationwithbackground
Self-training. We investigated the effectiveness of self-
Intable3,weevaluateourmodelandcomparewithexist- trainingforimprovingsegmentationperformance. Tothis
ingmethodonopen-vocabularysemanticsegmentationtask. end, we compared S-Seg and S-Seg+ on three datasets
Followingstandardevaluationprotocolsonopen-vocabulary and evaluated the results using Figure 10. We found that
modeltrainedwithoutannotatedmasks[6,41,57,58],wein- self-trainingconsistentlyimprovedthesegmentationperfor-
cludebackgroundpixelsinevaluationandobtainbackground mancebyasignificantmargin(+5.5%mIoUonaverage),
predictionbysettingathesholdforbackgroundclasses[57]. regardless of the data size and test dataset. These results
DespitethesimplicityofS-Seg,ourapproachachievecom- indicatethatself-trainingisareliableapproachforenhanc-
petitiveperformanceoverpreviousopen-vocabularysegmen- ingtheperformanceofS-Segandcanprovideadesirable
tationmethodsthatdoesnotrequiremaskannotations. Our complementforfurtherimprovement.
modelhassecondhighestperformanceonaverageandhas Data scalability. To evaluate the scalability of our
betterresultsthanGroupViTonalldatasets. Moreover,our method,wetrainedS-SegandS-Seg+usingthreedatasets
self-trained model, S-Seg+, provides an impressive 5.5% of increasing sizes: 12M, 15M, and 26M. The results of
mIoUimprovementoverourbasemodelS-Seg(42.6%vs the experiments are presented in Figure 12. We observed
37.1%3-avg. mIoU),suggestingtheefficacyofself-training. thatbothmodelsachievesignificantimprovementsinper-
760 mIoU. 30 mIoU. 35 mI 5o .4U %.
↑8.8%
mIoU.
↑2.3% ↑
55 28 5.4% 31
↑
mIoU. mIoU.
50 mIoU. 9.1% 26 27 4.4%
8.2% ↑ mIoU. ↑
↑ w/o self-train mIoU. w/o self-train 3.7% w/o self-train
45 24 2.6% 23 ↑
w/ self-train ↑ w/ self-train w/ self-train
10 20 30 10 20 30 10 20 30
Size of training data ( 106) Size of training data ( 106) Size of training data ( 106)
× × ×
(a)PascalVOC(+18.3%) (b)PascalContext(+14.1%) (c)COCO(+17.6%)
Figure10.Self-trainingimprovement.Weshowaveragerelativeimprovementinbracketontopoftheplot.weobservethatself-training
consistentlyleadstosignificantimprovementforS-Segacrossallofourtrainingandtestingdatasettings.
S-Seg (details) S-Seg+ (details) S-Seg (details) S-Seg+ (details)
Figure11. Visualizingeffectofself-training. Ourself-trained
S-Seg+ model demonstrates the ability to accurately predict in
regionsoverlookedbyS-Seg,asshowninthecolorfulrectangles.
Pascal VOC Pascal Context COCO 3-Average
52 27 30 37
49 25 27 34
46 23 24 31
10 20 30 10 20 30 10 20 30 10 20 30
(a)S-Seg
Pascal VOC Pascal Context COCO 3-Average
1.00
Figure13. Qualitativeresultsonwebimages. Thequeryclass
0.7650 30 34 42 nameisshowntotheright. Row1: S-Segisabletosegmentfic-
0.5507 28 31 39 tionalcharactersinananimatedscene. Row2: Despitehaving
0.2554 26 28 36 takenamudbath,thetigercanstillbeeasilyrecognizedandseg-
mented.Row3:S-Segiscapableofidentifyspecificlandmarks.
0.00
0.100 20 300.2 10 200.4 30 10 0.620 30 0.180 20 310.0
Size of training data ( ×106) objectsinchallengingcaseswherepreviousmethodshave
(b)S-Seg+
failed. Additionally,weobservedthatself-trainingcancor-
Figure12.Scalingtrainingdataprovideconsistentgaininper- rectminorerrorsinourbasemodel(asshownindetailin
formance,withorwithoutself-training.Wetrainourmodelusing fig.11). InFigure1and13,wepresentS-Seg’sperformance
differentsizesofdata:CC12M(12M),CC12M+CC3M(15M),and onwebimagesusingcustomqueryclasses. Ourmodelis
CC12M+CC3M+RedCaps(26M).Wenoteasteadyimprovement able to produce precise results for these categories. For
intheperformanceasthedatasizeincreases. morequalitativeresults,pleaserefertooursupplementary
material.
formanceacrossallthreetestingdatasetsastheamountof
dataincreased,suggestingthatourmethodscaleswellwith
5.Conclusion
largerdatasets.
To summarize, we propose S-Seg, a simple and intuitive
4.5.Visualization
framework that enables accurate and generalizable open-
ThequalitativeresultsofourmodelareillustratedinFigure3. vocabularysegmentation. Ouralgorithmdirectlytrainsfor
Our model has demonstrated its ability to handle difficult pixel-level feature and language alignment, and does not
situationssuchasoverlappingandsmallobjects. Comparing requiremanualsegmentationannotationsorextensivepre-
our results to those of existing methods, as shown in Fig- training. Wehopethatoursimpleyeteffectiveapproachwill
ure9,weobservedthatourapproachaccuratelysegments serveasasolidbaselineforfutureresearch.
8
)%(
UoIm
)%(
UoIm
avatar
tiger
MaoiReferences [17] MarkEveringham,LucVanGool,ChristopherKIWilliams,
JohnWinn,andAndrewZisserman. Thepascalvisualobject
[1] HangboBao,LiDong,SonghaoPiao,andFuruWei. Beit:
classes(voc)challenge. IJCV,2009. 6,11,12
Bert pre-training of image transformers. arXiv preprint
[18] GolnazGhiasi,XiuyeGu,YinCui,andTsung-YiLin.Scaling
arXiv:2106.08254,2021. 11
open-vocabularyimagesegmentationwithimage-levellabels.
[2] MaximeBucher,Tuan-HungVu,MatthieuCord,andPatrick
InProc.ECCV,2022. 3,5,7,13
Pe´rez. Zero-shotsemanticsegmentation. NerIPS,2019. 7
[19] Priya Goyal, Piotr Dolla´r, Ross Girshick, Pieter Noord-
[3] JohnCanny. Acomputationalapproachtoedgedetection.
huis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,
IEEEPAMI,1986. 3
Yangqing Jia, and Kaiming He. Accurate, large mini-
[4] NicolasCarion,FranciscoMassa,GabrielSynnaeve,Nicolas batch sgd: Training imagenet in 1 hour. arXiv preprint
Usunier,AlexanderKirillov,andSergeyZagoruyko. End-to- arXiv:1706.02677,2017. 11
endobjectdetectionwithtransformers. InProc.ECCV,2020.
[20] Jean-BastienGrill,FlorianStrub,FlorentAltche´,Corentin
5
Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doer-
[5] MathildeCaron,HugoTouvron,IshanMisra,Herve´Je´gou, sch,BernardoAvilaPires,ZhaohanGuo,MohammadGhesh-
JulienMairal,PiotrBojanowski,andArmandJoulin. Emerg- laghiAzar,etal. Bootstrapyourownlatent-anewapproach
ingpropertiesinself-supervisedvisiontransformers. InProc. toself-supervisedlearning. NerIPS,2020. 6
ICCV,2021. 2,7 [21] TanmayGupta,ArashVahdat,GalChechik,XiaodongYang,
[6] JunbumCha,JonghwanMun,andByungseokRoh. Learning JanKautz,andDerekHoiem.Contrastivelearningforweakly
togeneratetext-groundedmaskforopen-worldsemanticseg- supervisedphrasegrounding. InProc.ECCV,2020. 3
mentationfromonlyimage-textpairs. InProc.CVPR,2023. [22] MarkHamilton,ZhoutongZhang,BharathHariharan,Noah
3,7 Snavely, andWilliamTFreeman. Unsupervisedsemantic
[7] SoravitChangpinyo,PiyushSharma,NanDing,andRadu segmentationbydistillingfeaturecorrespondences. InProc.
Soricut. Conceptual 12m: Pushing web-scale image-text ICLR,2022. 3
pre-trainingtorecognizelong-tailvisualconcepts. InProc. [23] CongHan,YujieZhong,DengjieLi,KaiHan,andLinMa.
CVPR,2021. 2,11 Open-vocabularysemanticsegmentationwithdecoupledone-
[8] Liang-ChiehChen,YukunZhu,GeorgePapandreou,Florian passnetwork. InProc.ICCV,2023. 3,7
Schroff,andHartwigAdam. Encoder-decoderwithatrous [24] KunyangHan,YongLiu,JunHaoLiew,HenghuiDing,Jiajun
separableconvolutionforsemanticimagesegmentation. In Liu,YitongWang,YansongTang,YujiuYang,JiashiFeng,
Proc.ECCV,2018. 7,11 YaoZhao,andYunchaoWei. Globalknowledgecalibration
[9] XinleiChen,SainingXie,andKaimingHe. Anempirical forfastopen-vocabularysegmentation. InProc.ICCV,2023.
studyoftrainingself-supervisedvisiontransformers. InProc. 3,7
ICCV,2021. 7 [25] KaimingHe,XinleiChen,SainingXie,YanghaoLi,Piotr
[10] BowenCheng,AlexSchwing,andAlexanderKirillov. Per- Dolla´r,andRossGirshick. Maskedautoencodersarescalable
pixelclassificationisnotallyouneedforsemanticsegmenta- visionlearners. InProc.CVPR,2022. 11
tion. 2021. 4,5,7,11 [26] Jyh-JingHwang,StellaXYu,JianboShi,MaxwellDCollins,
[11] Jang Hyun Cho, Utkarsh Mall, Kavita Bala, and Bharath Tien-JuYang,XiaoZhang,andLiang-ChiehChen. Segsort:
Hariharan. Picie:Unsupervisedsemanticsegmentationusing Segmentationbydiscriminativesortingofsegments. InProc.
invarianceandequivarianceinclustering. InProc.CVPR, ICCV,2019. 3
2021. 3 [27] Xu Ji, Joao F Henriques, and Andrea Vedaldi. Invariant
[12] KevinClark,Minh-ThangLuong,QuocVLe,andChristo- informationclusteringforunsupervisedimageclassification
pherDManning. Electra:Pre-trainingtextencodersasdis- andsegmentation. InProc.ICCV,2019. 3
criminatorsratherthangenerators. Proc.ICLR,2020. 11 [28] ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh,
[13] KaranDesai,GauravKaul,ZubinAysola,andJustinJohnson. HieuPham,QuocLe,Yun-HsuanSung,ZhenLi,andTom
Redcaps:Web-curatedimage-textdatacreatedbythepeople, Duerig. Scalingupvisualandvision-languagerepresentation
forthepeople. arXivpreprintarXiv:2111.11431,2021. 11 learningwithnoisytextsupervision. InProc.ICML,2021. 3
[14] JianDing,NanXue,Gui-SongXia,andDengxinDai. De- [29] TapasKanungo,DavidMMount,NathanSNetanyahu,Chris-
couplingzero-shotsemanticsegmentation. InProc.CVPR, tineDPiatko,RuthSilverman,andAngelaYWu.Anefficient
2022. 3,7,13 k-meansclusteringalgorithm:Analysisandimplementation.
[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, IEEEPAMI,2002. 3,4
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, [30] LaurynasKarazija,IroLaina,AndreaVedaldi,andChristian
MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl- Rupprecht. Diffusionmodelsforzero-shotopen-vocabulary
vainGelly, etal. Animageisworth16x16words: Trans- segmentation. arXivpreprintarXiv:2306.09316,2023. 3
formersforimagerecognitionatscale. InProc.ICLR,2020. [31] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen
4 Koltun, and Rene´ Ranftl. Language-driven semantic seg-
[16] Pinar Duygulu, Kobus Barnard, Joao FG de Freitas, and mentation. InProc.ICLR,2022. 3,7,13
David A Forsyth. Object recognition as machine transla- [32] YanghaoLi,HaoqiFan,RonghangHu,ChristophFeichten-
tion: Learningalexiconforafixedimagevocabulary. In hofer,andKaimingHe. Scalinglanguage-imagepre-training
Proc.ECCV,2002. 2 viamasking. 2023. 5
9[33] FengLiang,BichenWu,XiaoliangDai,KunpengLi,Yinan ComputationalLinguistics(Volume1:LongPapers),2018. 2,
Zhao,HangZhang,PeizhaoZhang,PeterVajda,andDiana 11
Marculescu. Open-vocabularysemanticsegmentationwith [49] JianboShiandJitendraMalik. Normalizedcutsandimage
mask-adaptedclip. InProc.CVPR,2023. 3,7 segmentation. IEEEPAMI,2000. 3,4
[34] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays, [50] WouterVanGansbeke,SimonVandenhende,StamatiosGeor-
PietroPerona,DevaRamanan,PiotrDolla´r,andCLawrence goulis,MarcProesmans,andLucVanGool. Scan:Learning
Zitnick.Microsoftcoco:Commonobjectsincontext.InProc. toclassifyimageswithoutlabels. InProc.ECCV,2020. 3
ECCV,2014. 6,11,12 [51] WouterVanGansbeke,SimonVandenhende,StamatiosGeor-
[35] Tsung-YiLin,PriyaGoyal,RossGirshick,KaimingHe,and goulis,andLucVanGool. Unsupervisedsemanticsegmen-
PiotrDolla´r. Focallossfordenseobjectdetection. InProc. tationbycontrastingobjectmaskproposals. InProc.ICCV,
ICCV,2017. 5 2021. 3
[36] QuandeLiu,YoupengWen,JianhuaHan,ChunjingXu,Hang
[52] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko-
Xu,andXiaodanLiang. Open-worldsemanticsegmentation
reit,LlionJones,AidanNGomez,ŁukaszKaiser,andIllia
viacontrastingandclusteringvision-languageembedding. In Polosukhin. Attentionisallyouneed. NerIPS,2017. 5,11
Proc.ECCV,2022. 7
[53] WeijiaWu,YuzhongZhao,MikeZhengShou,HongZhou,
[37] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,Zheng
andChunhuaShen. Diffumask: Synthesizingimageswith
Zhang, StephenLin, andBainingGuo. Swintransformer:
pixel-levelannotationsforsemanticsegmentationusingdiffu-
Hierarchicalvisiontransformerusingshiftedwindows. In
sionmodels. InProc.ICCV,2023. 3
Proc.ICCV,2021. 11
[54] Yongqin Xian, Subhabrata Choudhury, Yang He, Bernt
[38] FrancescoLocatello,DirkWeissenborn,ThomasUnterthiner,
Schiele, and Zeynep Akata. Semantic projection network
Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit,
forzero-andfew-labelsemanticsegmentation. InProceed-
AlexeyDosovitskiy,andThomasKipf. Object-centriclearn-
ingsoftheIEEE/CVFConferenceonComputerVisionand
ingwithslotattention. InNerIPS,2020. 3
PatternRecognition,pages8256–8265,2019. 7
[39] IlyaLoshchilovandFrankHutter. Sgdr:Stochasticgradient
[55] TeteXiao,YingchengLiu,BoleiZhou,YuningJiang,and
descentwithwarmrestarts. Proc.ICLR,2016. 11
JianSun. Unifiedperceptualparsingforsceneunderstanding.
[40] IlyaLoshchilovandFrankHutter. Decoupledweightdecay
InProc.ECCV,2018. 11
regularization. InProc.ICCV,2019. 11
[56] QizheXie,Minh-ThangLuong,EduardHovy,andQuocV
[41] HuaishaoLuo,JunweiBao,YouzhengWu,XiaodongHe,and
Le. Self-trainingwithnoisystudentimprovesimagenetclas-
TianruiLi. Segclip:Patchaggregationwithlearnablecenters
sification. InProc.CVPR,2020. 6
foropen-vocabularysemanticsegmentation. InProc.ICML,
[57] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon,
2023. 3,7
ThomasBreuel,JanKautz,andXiaolongWang. Groupvit:
[42] FangyunWeiHanHuXiangBaiMengdeXu,ZhengZhang.
Semanticsegmentationemergesfromtextsupervision. In
Sideadapternetworkforopen-vocabularysemanticsegmen-
Proc.CVPR,2022. 1,3,4,6,7,11,12,13,16
tation. 2023. 3,7
[58] JilanXu,JunlinHou,YuejieZhang,RuiFeng,YiWang,Yu
[43] FaustoMilletari,NassirNavab,andSeyed-AhmadAhmadi.
Qiao,andWeidiXie. Learningopen-vocabularysemantic
V-net: Fullyconvolutionalneuralnetworksforvolumetric
segmentationmodelsfromnaturallanguagesupervision. In
medicalimagesegmentation. InProc.3DV,2016. 5
Proc.CVPR,2023. 1,3,7
[44] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu
[59] JiaruiXu,SifeiLiu,ArashVahdat,WonminByeon,Xiaolong
Cho, Seong-WhanLee, SanjaFidler, RaquelUrtasun, and
Wang, and Shalini De Mello. Open-vocabulary panoptic
Alan Yuille. The role of context for object detection and
segmentationwithtext-to-imagediffusionmodels. InProc.
semanticsegmentationinthewild. InProc.CVPR,2014. 6,
CVPR,2023. 3,7
11
[45] NormanMu,AlexanderKirillov,DavidWagner,andSain- [60] Andrii Zadaianchuk, Matthaeus Kleindessner, Yi Zhu,
ingXie. Slip: Self-supervisionmeetslanguage-imagepre- FrancescoLocatello, andThomasBrox. Unsupervisedse-
training. InProc.ECCV,2022. 5 manticsegmentationwithself-supervisedobject-centricrep-
[46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya resentations. InProc.ICLR,2023. 3
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, [61] ChongZhou,ChenChangeLoy,andBoDai. Extractfree
AmandaAskell,PamelaMishkin,JackClark,etal. Learning denselabelsfromclip. InProc.ECCV,2022. 3,6,7,12,13,
transferablevisualmodelsfromnaturallanguagesupervision. 16
InProc.ICML,2021. 1,2,4,5,6,7,11,12,16 [62] BarretZoph,GolnazGhiasi,Tsung-YiLin,YinCui,Hanx-
[47] KanchanaRanasinghe,BrandonMcKinzie,SachinRavi,Yin- iaoLiu,EkinDogusCubuk,andQuocLe. Rethinkingpre-
feiYang,AlexanderToshev,andJonathonShlens. Perceptual trainingandself-training. NerIPS,2020. 6
groupinginvision-languagemodels. InProc.ICCV,2023. 3,
7
[48] PiyushSharma,NanDing,SebastianGoodman,andRadu
Soricut. Conceptualcaptions: Acleaned,hypernymed,im-
agealt-textdatasetforautomaticimagecaptioning. InPro-
ceedingsofthe56thAnnualMeetingoftheAssociationfor
10A.Implementationdetails config value
optimizer AdamW[40]
A.1.S-Segexperiments baselearningrate 5e-4
weightdecay 0.05
Architecture. OurexperimentsuseMaskFormer[10]with optimizermomentum β1,β2=0.9,0.999
Swin-S[37]backboneand6-layertransformerdecoderwith batchsize 4096
learningrateschedule cosinedecay[39]
N =64queries. Thehiddenandoutputfeaturedimension
warmupepochs[19] 2
is256. ThelanguagemodelisaTransformer[52]with12 trainingepochs 30
layers,eachwithahiddendimensionof256. Thecontext
length(maximumlengthofinputtext)issetto77andthevo- Table5.S-Segsetting.
cabularysizeis49408. Weusea2-layerMLPtoprojectthe
visualandtextfeatureintoacommonembeddingspaceof config value
dimension256. WeuseDINOViT-S/8asthepretrainedViT optimizer AdamW[40]
inpseudo-maskgeneratorandgenerates8pseudo-masks. baselearningrate 1e-4
weightdecay 0.05
Training. Duringtraining,weusedthreepublicallyavail-
optimizermomentum β1,β2=0.9,0.999
abledatasets: CC3M[48],CC12M[7],andRedCaps[13], batchsize 16
containing3M,12Mand12Mimage-textpairs,respectively. learningrateschedule polynomialdecay
Duetostorageconstraint,weuseonlyfirst11Mdatasam- warmupiters[19] 1.5k
trainingiters 20k(voc),40k(ctxt),80k(coco)
plesatasmallerresolutionofwhenusingRedCapsdataset.
layer-wiselrdecay[12] 0.7
Intotal,weuseatmost26Mimage-textpairsfortraining
- this is an order of magnitude fewer data than CLIP [46]
Table6.S-Seg+setting.
and1-4MfewerthanGroupViT[57]. Thetotaldatasettakes
about2.4TBstoragespace. Table5showsourdefaulttrain-
ingsetting.Allinputimagesarerandomresizedandcropped A.3.Reimplementedbaselines
to224×224inresolution. Following[57],weextractnouns
CLIP[46].WeutilizedtheCLIPViT-B/16modelalongwith
andverbsfromrawsentencebecausethesewordsaremore
theofficialpretrainingweights. TheViTmodelincorporates
likelytodescribetheimage.
attentionalpoolinginitslastlayer,usinganadditional[CLS]
Inference. We evaluate S-Seg on the validation set of
tokentoaggregateothertokens. Wechoosetoemploythe
threedatasets: PascalVOC2012[17],PascalContext[44]
valueembeddingastherepresentationofeachtoken,asthe
andCOCO[34]. ThePascalVOCdatasetcontain1449im-
queryandkeyembeddingofthefinallayerisnotfullytrained
agesfortesting. Eachimageislabeledwith20foreground
during CLIP pretraining (only the similarity between the
classesandabackgroundclass. ThePascalContextdataset
queryembeddingofthe[CLS]tokenandthekeyembedding
contains5104testingimageswith59foregroundclassesand
ofothertokensisutilized).Finally,weleveragethelanguage
abackgroundclass. TheCOCOdatasetcontains5000im-
modeltoencodeallclassesandclassifythevisualtokens,
agesfortestingwith80foregroundclassesandanadditional
similartoCLIP’szero-shotclassificationapproach.
backgroundclass. Asin[57],wecombineallinstancesof
MaskCLIP[10]. Weusethetestingcodeandweights
thesameclasstogetsemanticsegmentationmaskforeach
provided by the authors, but re-evaluating them on the
imageinCOCO.FollowingGroupViT[57], wethreshold
commonly-usedprotocolthatincludesthebackgroundclass.
themaximumprobabilitytoobtainbackgroundprediction.
Tofurtherassesstheefficacyofourapproach,aswellasbase-
Duringinference,wesettheinputresolutionto448×448,
linemethods,weemployedtheevaluationmetricutilizedby
whichisconsistentwith[57].
MaskCLIP,whichspecificallydisregardsbackgroundpixels.
GroupViT [57]. The GroupViT project has provided
A.2.S-Seg+experiments
pre-trainedmodelsfortwoconfigurations. Withoutspecific
Self-training. Forself-trainingexperiments,weuseUper- clarification,weopttousethemodelwiththehighestaver-
Net[55]withMAE[25]pretrainedViTbackbone.Weutilize ageaccuracy,whichwastrainedonCC12M,CC15M,and
apyramid-structurednetworktomergethefeaturesobtained Redcapsdatasets. Thisparticularmodelalsocloselyaligns
fromlayer4,6,8,and12oftheViT,followingtheimple- withourmethodintermsoftrainingdata.
mentationofBEiT[1]. Weusethesamemodelthatweused Fullysupervisedmodels(DeepLabV3+[8]andMask-
toevaluateourmainresultstogeneratetrainingdatafrom Former[10]). Weleveragepubliccheckpointswhenavail-
thetrainsetoftherespectivedataset. Traininghyperparam- able. In cases where a checkpoint is not available, we re-
etersareprovidedinTable6. Following[1,25],weusea trainthemodelusingtheoriginaltraininghyperparameters
layerwiselearningratedecay[12]. Wedonotuserelative (e.g.optimizer,learningrate,momentum,andweightdecay)
positionembeddingsinourbackboneViTmodel(whichis alongwiththestandardtrainingschedule,whichvariesde-
usedby[1,25]atfine-tuningstageforextraimprovement). pendingonthedataset(40kiterationsforP.VOC,80kforP.
11S-Seg S-Seg+ 3-Average
data method
VOC Context COCO VOC Context COCO 12M 15M 26M
12M 44.9 22.9 22.5 53.1 25.5 26.2 w/oself-train 30.1 30.8 37.1
15M 45.1(+0.2) 23.8(+0.9) 27.9(+5.4) 54.2(+1.1) 29.2(+3.7) 28.0(+1.8) w/self-train 34.9 37.1 42.6
26M 53.2(+8.3) 27.9(+5.0) 30.3(+7.8) 62.0(+8.9) 30.2(+4.7) 35.7(+9.5) ∆ +4.8 +6.3 +5.5
(a)Scalingtrainingdataprovideconsistentgain:Wetrainourmodelusingdifferent (b)Self-trainingoffersconstantimprovement:Weob-
sizeofdata:12M(CC12M),15M(+CC3M),and26M(+RedCaps).Wenoteasteady servethatself-trainingconsistentlyleadstosignificantim-
improvementinthemodel’sperformanceasthedatasizeincreases. provementonperformanceacross3datasets.
Table7.Ablationsondatascalabilityandself-training.WereportmIoUevaluatedonthreedatasets.Highervaluesarebetter.
CLIP 13.2 10.4 4.4 8.0 5.9 19.4 27.0 17.5 26.0 3.1 19.6 9.0 21.5 16.8 11.2 11.7 5.2 13.1 7.6 21.1 12.2 13.5
MaskCLIP 41.3 12.8 18.7 22.5 6.7 22.8 50.7 23.4 56.8 13.6 34.1 8.1 46.3 29.5 39.9 22.7 9.5 29.5 25.1 30.8 18.2 26.8
GroupViT 79.0 37.4 29.9 33.3 33.9 64.4 60.2 62.4 76.7 16.2 68.8 28.0 75.9 62.5 64.2 51.6 38.7 63.0 37.4 44.0 38.4 50.8
S-Seg(Ours) 81.0 47.2 40.1 38.6 30.0 63.5 74.6 67.6 75.7 18.6 65.3 34.4 72.2 56.3 68.0 50.7 45.7 60.2 33.6 53.1 41.0 53.2
S-Seg+(Ours) 86.5 53.8 42.0 48.1 49.3 76.0 84.7 74.5 87.2 17.1 81.8 35.0 83.4 65.2 74.3 65.3 46.6 78.2 40.2 58.5 53.6 62.0
Table8.Per-categoryopenvocabularysemanticsegmentationperformanceover21PascalVOCclasses.Ourmethodsurpassbaseline
methodssuchasGroupViTonthePascalVOCdataset,particularlyinsegmentinglargeobjectsandcategorieswithconsistenttextures.
LVIS ImageNet-S B.3.Per-categoryresult
Method OV Sup.
(1103classes) (919classes)
CLIP[46] ✓ text 1.3 8.0
MaskCLIP[61] ✓ text 4.3 9.1 Table8presentsthemIoUresultsofourmodelsandbaseline
GroupViT[57] ✓ text 7.2 32.2 methodsonthePascalVOCdataset,whereeachclassiseval-
S-Seg(Ours) ✓ text 8.5 34.9
uatedseparately. OurmodelsoutperformGroupViTinmost
ViT-FCN1 ✗ GT 9.6 40.4 classes,andS-Seg+achievessuperiorperformanceacross
allcategories. Ourmodelsareparticularlyeffectiveatseg-
Table9. Open-vocabularysemanticsegmentationresultson
mentinglargeobjectssuchasaeroplanes,buses,andtrains,
LVISandImageNet-S.Ourmethoddemonstratescompetitiveper-
withanaverageimprovementof11.1comparedto2.5for
formanceonthesechallengingdatasetswithasignificantlylarger
allclasses. Thisimprovementcouldsuggestthatourmod-
numberofclasses.
els benefit from the pseudo-mask generator, which works
betterforlargerobjects(whichshowsa83.3%oracleperfor-
Context,and160kforCOCO).Weshowtheperformanceof mancecomparedto77.2%forotherclasses). Ontheother
DeepLabV3+inqualitativecomparisons(FullySup.). hand,ourself-trainingmodelperformsbetteroncategories
thatshareconsistenttexture,suchascats,cows,dogs,and
B.Additionalresults sheep,withanaverageimprovementof14.3comparedto8.8
forallclasses. Thisindicatesthatself-trainingcanidentify
B.1.Additionaldatasets
commonfeaturesandreducenoiseintheself-traininglabels.
We evaluate our method on two new challenging datasets
thatcontainsignificantlymoreclasses,LVIS(1103classes)
and ImageNet-S (919 classes). The results are shown in
Table 9. We observe that our model outperforms several B.4.Additionalvisualizations
existingopen-vocabularybaselinemethodsandapproaches
supervisedmodels,indicatingitsrobustnessinchallenging
Figures15and16presentmoredetailedopen-vocabulary
scenarios.
segmentationresultsinhigherresolution. Asshowninthe
B.2.Ablationresults results,ourapproachcaneffectivelysegmentobject-centric
images from [17] (fig. 15) as well as context-rich images
InTable7,weshownumericalresultscorrespondingtoFig- from[34](fig.16)accurately. Ourmethodcansegmentob-
ure 10 and 12 in the main paper. As seen from the table, jectsbasedsolelyontheircategoryname,withoutrequiring
scalingdataandself-trainingprovideconsistentgaininper- any annotations from specific target datasets during train-
formanceforourmodel. ing. Figure17and18provideadditionalcomparisonwith
1WealsotriedDeepLabV3+butfailedtoobtainsatisfactoryresults. previousmethods.
12
sdohteMVO
.GB
enalporea
elcycib drib taob elttob sub rac tac tiahc woc elbat god esroh
ekibrotom
nosrep tnalp peehs afos niart rotinom UoImMethod Algorithm VLPretrain Pretraindata Anno.masks I-Tpairs Custommodel Loss mIoU(VOC)
Adapt&Refineimage-level
OpenSeg[18] Yes(ALIGN) 1800M Yes(COCO) - Notrequired image+pixel 77.2
VLalignmentmodels
Directlytraining
ZegFormer[14] Yes(CLIP) 400M Yes(COCO) - Notrequired image+pixel 80.7
pixel&languagealignment
Adapt&Refineimage-level
MaskCLIP[61] Yes(CLIP) 400M Notrequired - Notrequired image 49.5
VLalignmentmodels
Extractsegments
GroupViT[57] Notrequired - Notrequired 30M Yes(GroupViT) image 77.2
fromlanguagealignment
Directlytraining
S-Seg(Ours) Notrequired - Notrequired 26M Notrequired image+pixel 81.8
pixel&languagealignment
Table10.ComparingS-Seg(Ours)withclosely-relatedmethods(OpenSeg[18],ZegFromer[14],MaskCLIP[61],andGroupViT[57]).
Weconductacomparativeanalysisofourmethodagainstarangeofclosely-relatedapproaches,whicharefurtherdetailedinSectionC.
similarity & similarity & GT masks similarity & Pseudo masks similarity &
disimilarity disimilarity disimilarity disimilarity
extract group predict predict
Language Language Language Language
ViT Model GroupViT Model MaskFormer Model MaskFormer Model
prompt
image text image text image class labels image text
image-text image-text image-text
pair pair pair
MaskCLIP GroupViT ZegFormer S-Seg (Ours)
Figure14.ComparingS-Seg(Ours)withclosely-relatedmethods.ThecomponentsinredarethosedifferentfromS-Seg.
C.MethodologyComparisons solutionoffersbettergeneralizationthanZegFormer,despite
not utilizing CLIP, annotated masks, or pixel-wise labels.
We present a comparative analysis of our method against
Thearchitecturalandtrainingsimilaritiesbetweenthetwo
severalclosely-relatedexemplaryapproaches. Ourmethod
methodssuggestthattheirintegrationcouldleadtoenhanced
serves as a connection among these methodologies. The
performance,ahypothesisweleaveforfutureexploration.
primarysimilaritiesanddifferencesareoutlinedinTable10,
RelationtoCLIP/MaskCLIP.Ourmethodcloselypar-
withfurtherdiscussionbelow.
allelsCLIPintheimage-textcontrastivetrainingparadigm
RelationtoOpenSeg. OpenSeg(andsimilarmethods, andcanbeseenasa”CLIPwithMaskFormerastheimage
e.g.LSeg[31])refinesimage-levelmodelslikeCLIP/ALIGN encoder,”supplementedbyanadditionalmasksupervision
by training on annotated semantic masks. The pretrained branch. Despitethesesimilarities,CLIPprimarilyaimsto
image-levelmodelprovideslanguagealignmentandutilize learnimage-levelalignment,whereasS-Segisfocusedon
groundtruthmaskforrefiningpixel-levelfeature.Incontrast, pixel-level alignment. This is evident from the fact that
S-Segtrainsdirectlyonpixelfeaturesfrompseudo-masks evenwiththeMaskCLIPadaptation,thesegmentationper-
andlearnslanguagealignmentthroughtext. Conceptually, formancesignificantlylagsbehindthatofothercompared
S-Segoffersanend-to-endalternativetoOpenSeg,withthe methods. Thishighlightstheimportanceofincorporating
addedadvantageoftrainingexclusivelyonimage-textpairs. boththeMaskFormerandmasksupervisioninS-Seg.
Ourapproachremovestheneedfortheresource-intensive Relation to GroupViT. GroupViT and S-Seg share a
VLpretrainingstep,streamlinesthelearningprocess,and similarproblemsetup,wherebothmethodsavoidCLIPpre-
reducestherelianceonextensivesuperviseddata. trainingandmanualannotations. Methodologically,S-Seg
RelationtoZegFormer. Ourmethodcanbeconceptu- resembles ”GroupViT with MaskFormer as the grouping
alizedasavariantof”ZegFormertrainedfromscratchwith model.” A key difference, however, is that GroupViT ex-
pseudo-masksandlanguage,”albeitwithnotableimplemen- tractssegmentsfromatrainedmodel,whileS-Segdirectly
tationdistinctions. Trainingwithseengroundtruthmasks predictssegmentation,supervisedbypseudo-masks. This
benefits in-domain classes, but may not extend to unseen moreexplicitformofsupervisionallowsS-Segtoleverage
classes. Interestingly,whileourmethodunderperformscom- standardsegmentationmodelslikeMaskFormermoreeffec-
paredtoZegFormeronseenclasses,itsurpassesZegFormer tivelyandoffersapotentiallysimplerpathwayforupdates
inhandlingunseenclassesanddemonstratessuperioraver- withfutureadvancementsinsegmentationmodels.
ageperformanceacrossthedataset. Thissuggeststhatour
13Input Ours Ours+
Cat Cow Car Boat Dog Person Bicycle
Figure15.AdditionalqualitativeresultsofS-Seginhigherresolution(object-centricimages).Ourmethoddemonstratesrobustness
indealingwithchallengingscenarios,suchasobjectswithunconventionalshapesandposes(row1),imageswithunusualcolorandtone
(row2),objectsofthesameclassbutwithdifferingcolors(row3),objectswiththesimilarcolorbutofdifferentclasses(row4),concealed
objects(row5),andvariousotherdifficultsituations.
14Input Ours Ours+
Person Surfboard Bird Banana Umbrella Boat Couch
Figure16. AdditionalqualitativeresultsofS-Seginhigherresolution(context-richimages). Althoughcontext-richimagespose
challengesinsegmentationduetothepresenceofanincreasednumberofsmallandclutteredobjects,ourmethodcanstillaccurately
segmenttheobjectswithprecision.
15Input CLIP MaskCLIP GroupViT Ours Ours+ Fully Sup. Annotation
Figure17.Additionalqualitativecomparisonwithexistingmethods.CLIP[46]isprimarilydesignedforclassificationanddoesnot
performwellinsegmentation.MaskCLIP[61]adaptsCLIPforsegmentation,althoughitproducesnoisypredictionsandcannothandle
backgroundclasses.GroupViT[57]isastrongcompetitor,butitcouldstruggleinchallengingscenarios.
16Input CLIP MaskCLIP GroupViT
Ours Ours+ Fully Sup. Ground Truth
Input CLIP MaskCLIP GroupViT
Ours Ours+ Fully Sup. Ground Truth
Input CLIP MaskCLIP GroupViT
Ours Ours+ Fully Sup. Ground Truth
Input CLIP MaskCLIP GroupViT
Ours Ours+ Fully Sup. Ground Truth
Figure18.Additionalqualitativecomparisonwithexistingmethods(continued).
17