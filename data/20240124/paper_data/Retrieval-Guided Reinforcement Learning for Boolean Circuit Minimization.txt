PublishedasaconferencepaperatICLR2024
RETRIEVAL-GUIDED REINFORCEMENT LEARNING FOR
BOOLEAN CIRCUIT MINIMIZATION
AnimeshBasakChowdhury∗ BenjaminTan
QualcommIncorporated Electrical&SoftwareEngineering
{abasakch}@qti.qualcomm.com UniversityofCalgary
{benjamin.tan1}@ucalgary.ca
MarcoRomanelli,RameshKarri,SiddharthGarg
ElectricalandComputerEngineering
NewYorkUniversity
{mr6582,rkarri,sg175}@nyu.edu
ABSTRACT
Logicsynthesis,apivotalstageinchipdesign,entailsoptimizingchipspecifica-
tionsencodedinhardwaredescriptionlanguageslikeVerilogintohighlyefficient
implementations using Boolean logic gates. The process involves a sequential
applicationoflogicminimizationheuristics(“synthesisrecipe"),withtheirarrange-
mentsignificantlyimpactingcrucialmetricssuchasareaanddelay. Addressingthe
challengeposedbythebroadspectrumofdesigncomplexities—fromvariations
ofpastdesigns(e.g.,addersandmultipliers)toentirelynovelconfigurations(e.g.,
innovativeprocessorinstructions)—requiresanuanced‘synthesisrecipe’guided
byhumanexpertiseandintuition. Thisstudyconductsathoroughexaminationof
learningandsearchtechniquesforlogicsynthesis,unearthingasurprisingrevela-
tion: pre-trainedagents,whenconfrontedwithentirelynoveldesigns,mayveeroff
course,detrimentallyaffectingthesearchtrajectory. WepresentABC-RL,ametic-
ulouslytunedαparameterthatadeptlyadjustsrecommendationsfrompre-trained
agentsduringthesearchprocess. Computedbasedonsimilarityscoresthrough
nearestneighborretrievalfromthetrainingdataset,ABC-RLyieldssuperiorsyn-
thesisrecipestailoredforawidearrayofhardwaredesigns. Ourfindingsshowcase
substantialenhancementsintheQuality-of-result(QoR)ofsynthesizedcircuits,
boastingimprovementsofupto24.8%comparedtostate-of-the-arttechniques.Fur-
thermore,ABC-RLachievesanimpressiveupto9xreductioninruntime(iso-QoR)
whencomparedtocurrentstate-of-the-artmethodologies.
1 INTRODUCTION
Modernchipsaredesignedusingsophisticatedelectronicdesignautomation(EDA)algorithmsthat
automatically convert logic functions expressed in a hardware description language (HDL) like
Verilogtoaphysicallayoutthatcanbemanufacturedatasemiconductorfoundry. EDAinvolvesa
sequenceofsteps,thefirstofwhichislogicsynthesis. LogicsynthesisconvertsHDLintoalow-level
“netlist” of Boolean logic gates that implement the desired function. A netlist is a graph whose
nodesarelogicgates(e.g., ANDs, NOTs, ORs)andwhoseedgesrepresentconnectionsbetween
gates. SubsequentEDAstepslikephysicaldesignthenplacegatesonanx-ysurfaceandroutewires
betweenthem. AsthefirststepintheEDAflow,anyinefficienciesinthisstep,e.g.,redundantlogic
gates,flowdownthroughouttheEDAflow. Thus,thequalityoflogicsynthesis—thearea,powerand
delayofthesynthesizednetlist—iscrucialtothequalityofthefinaldesign(Amarúetal.,2017).
As shown in Fig. 1, state-of-art logic synthesis algorithms perform a sequence of functionality-
preservingtransformations, e.g., eliminatingredundantnodes, reorderingBooleanformulas, and
streamliningnoderepresentations,toarriveatafinaloptimizednetlistYangetal.(2012);Mishchenko
∗TheworkwasdonewhenthefirstauthorwasagraduatestudentatNewYorkUniversity
1
4202
naJ
22
]GL.sc[
1v50221.1042:viXraPublishedasaconferencepaperatICLR2024
etal.(2006);Rieneretal.(2019);Yu(2020);Netoetal.(2022).Aspecificsequenceoftransformations
iscalleda“synthesisrecipe.” Typically,designersuseexperienceandintuitiontopicka“good"
synthesis recipe from the solution space of all recipes and iterate if the quality of result is poor.
Thismanualprocessiscostlyandtime-consuming,especiallyformodern,complexchips. Further,
thedesignspaceofsynthesisrecipesislarge. ABC(Brayton&Mishchenko,2010),astate-of-art
open-sourcesynthesistool,providesatoolkitof7transformations,yieldingadesignspaceof107
recipes(assuming10-steprecipes). Agrowingbodyofworkhassoughttoleveragemachinelearning
andreinforcementlearning(RL)toautomaticallyidentifyhigh-qualitysynthesisrecipes(Yuetal.,
2018;Hosnyetal.,2020;Zhuetal.,2020;Yu,2020;Netoetal.,2022;Chowdhuryetal.,2022),
showingpromisingresults.
Priorworkinthisareafallswithinoneoftwocat-
e eg to alr .i ,e 2s 0. 2T 2h )e pfi rors pt ol si en se eo ff ficw ieo nrk ts( eY au rc, h20 h2 e0 u; riN ste icto s, m ( i sni un 2o m1 ,d c, ,u i cn ol ,e u tf )a ; I In n2 1 Sum GIA n rvn aed pr- ht x4 V Vx V1.x2 V.x3.x4N Do ed 4 pe ths ::
3
Monte-Carlotreesearch(MCTS)inparticular, i inn 1p ,u int 2,cin; Cin Cout rx e1 -wx r2 itx e3 (rx w1 )
t fo orex ap gl io vr ee nth ne ets lo isl tu .ti To hn es sp ea mce eo thf os dy snt th rae is nis are pc oi lp ice ys o s a inu su 1m st ^p i i, g nu c n 2ot
^
u s ct u; inm
;
= b rf rw rwz rC f(Mho ) i =c e 7s x4V x1Vx1. Vx2.x3.x4 N Do ed 3 pe ths ::
a ing ie tin at lid zu er din fg roM mC sT crS ati cte hra foti ron as g, ib vu et nth ne eta lig se tn at ni ds a ( (i in ns 1 2s & &ig i cnn in2 c )) |o |ut = rf balax2 ncex 3 (b) 3
(in1&cin) ; x1.x2.x3.x4Nodes:
doesnotlearnfrompastdata,e.g.,repositories V 3
endmodule V V Depth:
ofpreviouslysynthesizeddesignsabundantin RTL verilog Te mc ah pn po il no ggy B Qe os Rt lengR te hc (i Lp )e = 10 x4 x1x2x3 2
chipdesigncompanies,orsourcedfrompublic
repositories (Chowdhury et al., 2021; Thakur Figure1: (Left)AhardwaredesigninVerilogisfirst
etal.,2023). transformed into an and-inverter-graph (AIG), i.e., a
netlistcontainingonlyANDandNOTgates. Thena
Toleverageexperiencefrompastdesigns,asec-
sequence of functionality-preserving transformations
ondlineofworkseekstoaugmentsearchwith
(here, picked from set {rw, rwz, ..., b }) is applied
learning. Chowdhuryetal.(2022)showthat togenerateanoptimizedAIG. Eachsuchsequenceis
apredictiveQoRmodeltrainedonpastdesigns calledasynthesisrecipe.Thesynthesisrecipewiththe
withsimulatedannealing-basedsearchcanout- bestqualityofresult(QoR)(e.g.,areaordelay)isshown
performpriorsearch-onlymethodsbyasmuch ingreen.(Right)ApplyingrwandbtoanAIGresults
as 20% in area and delay. The learned model resultsinanAIGwithfewernodesandlowerdepth.
replacestime-consumingsynthesisruns—asin-
glesynthesisruncantakearound10.9minutes 16 12
inourexperiments—withfastbutapproximate 10 14
QoRestimatesfromapre-traineddeepnetwork. 8 12
6 10
4 MCTS 8 MCTS
Motivational Observation. Given the tree- 02 MCTS+Learning 6 MCTS+Learning
4
structuredsearchspace(seeFig.2),webeginby 20 40 60 80 100 20 40 60 80 100
Iterations Iterations
buildingandevaluatingabaselinesolutionthat:
(a)square (b)cavlc
1) pre-trains an offline RL agent on a dataset
Figure2:Reductioninarea-delayproduct(greaterre-
ofpastdesigns;andthen2)performsRL-agent ductionisbetter)oversearchiterationsforapuresearch
guidedMCTSsearchoversynthesisrecipespace strategy(MCTS)andsearchaugmentedwithlearning
for new designs. Although details vary, this (MCTS+Learning).Learninganofflinepolicydoesnot
strategyhasbeensuccessfulinothertree-search helpinbothcases.
problems, mostprominentlyin AlphaGo(Silveret al.,2016)andAlphaZero Silveretal. (2017).
Interestingly,wefoundthatalthoughtheagentlearnedonpastdatahelpsslightlyonaverage,on
11outof20designs,thebaselinestrategyunderperformedsimpleMCTSsearch(see Table2for
detailedresults). Fig.2showstwoexamples: inboth,pureMCTSachievesbettersolutionsfaster
thanlearningaugmentedMCTS.Ideally,weseeksolutionsthatprovideconsistentimprovements
oversearch-onlymethodsbyleveragingpastdata.
One reason for poor performance is the large diversity of netlists in logic synthesis benchmarks.
Netlistsvarysignificantlyinsize(100-46Knodes)andfunction. TheEPFLbenchmarks,forexample,
partitionnetistsintothosethatperformarithmeticfunctions(e.g.,adders,dividers,square-roots)and
controlfunctions(e.g.,finitestatemachines,patterndetectors,etc.) becauseoflargedifferencesin
theirgraphstructures. Inpractice,whiledesignersoftenreusecomponentsfrompastdesigns,they
2
)%(
noitcudeRPublishedasaconferencepaperatICLR2024
frequentlycomeupwithnewdesignsandnovelfunctions. Fornetliststhatdiffersignificantlyfrom
thoseinthetrainingdataset,pre-trainedagentshurtbydivertingsearchtowardssuboptimalrecipes.
OverviewofApproach. WeproposeABC-RL,anewretrievalguidedRLapproachthatadaptively
tunesthecontributionofthepre-trainedpolicyagentintheonlinesearchstagedependingontheinput
netlist. ABC-RLcomputesatuningfactorα ∈ [0,1]bycomputingasimilarityscoreoftheinput
netlistanditsnearestneighborretrievedfromthetrainingdataset. Similarityiscomputedongraph
neuralnetwork(GNN)featureslearnedduringtraining. Ifthenewnetlistisidenticaltooneinthe
trainingdataset,wesetα=0,andonlythepre-trainedagentisusedtooutputthesynthesisrecipe.
Conversely, whenα = 1(i.e., thenetlistisnovel), thepre-trainedagentisignoredandABC-RL
defaultstoasearchstrategy. Real-worldnetlistslieinbetweentheseextremes;accordinglyABC-RL
modulatingtherelativecontributionsofthepre-trainedagentandpureMCTSsearchtothefinalresult.
WemakecarefularchitecturalchoicesinourimplementationofABC-RL,includingthechoiceof
netlistandsynthesisrecipeencodersandstate-spacerepresentation. TheyaredescribedinSection2.3.
AlthoughourmaincontributionisABC-RL,theMCTS+Learningbaseline(i.e.,withoutretrieval)
hasnotbeenevaluatedforlogicsynthesis. Ourevaluationshighlightitsbenefitsanddrawbacks.
SnapshotofResultsandKeyContributions. Acrossthreecommonlogicsynthesisbenchmark
suitesABC-RLconsistentlyoutperformspriorSOTAML-basedlogicsynthesissolutions,ourown
MCTS+Learningbaseline,and anMCTS+LearningsolutionforchipplacementMirhoseinietal.
(2021),adifferentEDAproblem,adaptedtologicsynthesis. ABC-RLachievesupto24.8%geo.
meanimprovementsinQoR(here,area-delayproduct)overSOTA.Atiso-QoR,ABC-RLreduces
synthesisruntimebyupto9×.
Insummary,ourkeycontributionsare:
• WeproposeABC-RL,anewretrieval-guidedRLapproachforlogicsynthesisthatlearnsfrom
past historical data, i.e., previously seen netlists, to optimize the QoR for a new netlist at test
time. Indoingso,weidentifydistributionshiftbetweentrainingandtestdataasakeyproblem
inthisdomain,andshowthatabaselinestrategythataugmentsMCTSwithapre-trainedpolicy
agentSilveretal.(2016)failstoimproveuponpureMCTSsearch.
• Toaddresstheseconcerns,weintroducenewlightweightretrievalmechanisminABC-RLthatuses
thesimilarityscorebetweenthenewtestnetlistanditsnearestneighborinthetrainingset. This
scoremodulatestherelativecontributionofpre-trainedRLagentusingamodulationparameterα,
down-weightingthelearnedpolicydependingonthenoveltyofthetestnetlist.
• WemakecarefularchitecturalchoicesforABC-RL’spolicyagent,includinganewtransformer
basedsynthesisencoder,andevaluateacrossthreeofcommonlogicsynthesisbenchmarks. ABC-
RLconsistentlyoutperformspriorSOTAMLforlogicsynthesismethodsonQoRandruntime,
asalsoarecentMLforchip-placementmethod(Mirhoseinietal.,2021)adaptedtoourproblem
setting. Ablationstudiesestablishtheimportanceofα-tuningtoABC-RL.
• Whileourfocusisonlogicsynthesis,ourlightweightretrieval-guidedapproachmightfindusein
RLproblemswhereonlineruntimeandtraining-testdistributionshiftarekeyconcerns.
WenowdescribeABC-RL,startingwithapreciseproblemformulationandbackground.
2 PROPOSED APPROACH
2.1 PROBLEMSTATEMENTANDBACKGROUND
We begin by formally defining the logic synthesis problem using ABC (Brayton & Mishchenko,
2010), the leading open-source synthesis tool, as an exemplar. As shown in Figure 1, ABC first
convertstheVerilogdescriptionofachipintoanunoptimizedAnd-Invert-Graph(AIG)G ∈ G,
0
whereGisthesetofalldirectedacyclicgraphs. TheAIGrepresentsANDgatesasnodes,wires/NOT
gatesassolid/dashededges,andimplementsthesameBooleanfunctionastheinputVerilog. Next,
ABC performs a series of functionality-preserving transformations on G . Transformations are
0
pickedfromafinitesetofM actions,A={rf,rm,...,b}. ForABC,M =7. Applyinganaction
onanAIGyieldsanewAIGasdeterminedbythesynthesisfunctionS:G×A→G. Asynthesis
recipeR∈ALisasequenceofLactionsthatareappliedtoG inorder. Givenasynthesisrecipe
0
3PublishedasaconferencepaperatICLR2024
P ={a ,a ,...,a }(a ∈A),weobtainG =S(G ,a )foralli∈[0,L−1]whereG is
0 1 L−1 i i+1 i i L
thefinaloptimizedAIG.Finally,letQoR:G →RmeasurethequalityofgraphG,forinstance,its
inversearea-delayproduct(solargerisbetter). Then,weseektosolvethisoptimizationproblem:
argmaxQoR(G ), s.t. G =S(G ,a ) ∀i∈[0,L−1]. (1)
L i+1 i i
P∈AL
WenowdiscussABC-RL,ourproposedapproachtosolvethisoptimizationproblem. Inadditionto
G ,theAIGtobesynthesized,wewillassumeaccesstoatrainingsetofAIGstoaidoptimization.
0
2.2 BASELINEMCTS-BASEDOPTIMIZATION
Thetree-structuredsolutionspaceforlogicsynthesismotivatedpriorwork(Yu,2020;Netoetal.,
2022)toadoptanMCTS-basedsearchstrategythatwebrieflyreviewhere. Astateshereisthe
currentAIGafterltransformations. Inagivenstate,anyactiona∈Acanbepickedasdescribed
above. Finally, the reward QoR(G ) is delayed to the final synthesis step. In iteration k of the
L
search,MCTSkeepstrackoftwofunctions:Qk (s,a)whichismeasurethe“goodness"ofastate
MCTS
actionpair,andUk (s,a)whichrepresentsupperconfidencetree(UCT)factorthatencourages
MCTS
explorationoflessvisitedstatesandactions. Thepolicyπk (s):
MCTS
πk (s)=argmax(cid:0) Qk (s,a)+Uk (s,a)(cid:1) . (2)
MCTS MCTS MCTS
a∈A
balancesexploitationagainstexplorationbycombiningthetwoterms. Furtherdetailsarein§A.3.
2.3 PROPOSEDABC-RLMETHODOLOGY
Wedescribeourproposedsolutionintwosteps. First,wedescribeMCTS+Learning,whichbuilds
similar principles as Silver et al. (2016) by training an reinforcement learning (RL) policy agent
onpriornetliststoguideMonteCarlotreesearch(MCTS)search,highlightinghowpriorworkis
adaptedtothelogicsynthesisproblem. Then,wedescribeourfullsolution,ABC-RL,thatusesnovel
retrieval-guidedaugmentationtosignificantlyimproveMCTS+Learning.
2.3.1 MCTS+LEARNING
As noted, we use a dataset of N training circuits to learn a policy agent π (s,a) that outputs
tr θ
the probability of taking action a in state s by approximating the pure MCTS policy on the
training set. We first describe our state-space representation and policy network architecture.
Initial AIG Past
heuristics
State-spaceandpolicynetworkarchitecture:Weencodestatesas
Node features (1x2) sequence
(0-10) asatwo-tupleoftheinputAIG,G ,andsequenceofl≤Lsynthesis
0
GCN (1x32) actions, i.e., A = {a ,a ,...,a }takensofar. Becausethetwo
l 0 1 l
BN+LeakyReLU Transformer inputsareindifferentformats,ourpolicynetworkhastwoparallel
encoder branchesthatlearnembeddingsofAIGG andpartialrecipe.
GCN (1x32) 0
For the AIG input, we employ a 3-layer graph convolutional net-
BN Sequence
embedding work(GCN)Kipf&Welling(2016)architecturetoobtainanembed-
Mean pool Max pool (1x768) dingh . WeuseLeakyRELUastheactivationfunctionandapply
G0
batchnormalizationbeforeeachlayer. (Seeappendix§B.1forde-
AIG embedding tails.) IncontrasttopriorworkthatdirectlyencodesrecipesChowd-
huryetal.(2022),weuseasimplesingleattentionlayerBERTtrans-
FC (832x256) +BN +LeakyRELU formerarchitectureDevlinetal.(2018)tocomputepartialrecipe
embeddings,h ,whichweconcatenatewithAIGembeddings. We
FC (256x256) +BN +LeakyRELU Al
makethischoicesincepartialsynthesisrecipesarevariablelength,
FC (256x7)
andtobettercapturecontextualrelationshipswithinasequenceof
Softmax actions. Ablation studies demonstrate the advantages of this ap-
proach. The final embedding is a concatenation on the AIG and
(s,)
partialsynthesisrecipeembeddings.
Figure3: Policynetworkarchi-
tecture.GCN:Graphconvolution RL-agent training: With the policy network in place, policy
network, BN: Batch normaliza- π (s,a) is learned on a training dataset of past netlists using a
θ
tion,FC:Fullyconnectedlayer
4PublishedasaconferencepaperatICLR2024
Validation
1 circuits 1 4 1 2
Winner Test
? circuit Query 3
T cr oa ri pn uin sg sG eu a4 i rd ce h S Ta a rm 3n ad ip nle BuffC ed ro a2 ll te act Query 2
nN
e3
ie
gA
a
hrg
be
oe
s
un
t
rt AUM RC OT CS
Set
+ teLM m5eC a pr .T n (S i Tn )g
,
tunM e 6C d T leS a+ rning 5 Agent Cn orN e e mie g tra ph ier ube v to es aut
4
lr
Agent retrieval threshold Generate recipe
Figure4:ABC-RLflow:Trainingtheagent(left),settingtemperatureT andthresholdδ (mid)andRecipe
th
generationatinference-time(right)
cross-entropylossbetweenthelearnedpolicyandtheMCTSpolicyoversamplespickedfroma
replaybuffer. Thelearnedpolicyisusedduringinferencetobiastheupperconfidencetree(UCTin
Eq.2)ofMCTStowardsfavorablepathsbycomputinganewU∗k (s,a)as:
MCTS
U∗k (s,a)=π (s,a)·Uk (s,a). (3)
MCTS θ MCTS
Forcompleteness,weoutlinethepseudocodeforRL-trainingintheappendix(Algorithm1).
2.4 RETRIEVAL-GUIDEDLOGICSYNTHESIS(ABC-RL)
Aswenoted,hardwaredesignsfrequentlycontainbothfamiliarandentirelynewcomponents. In
thelattercase,ourresultsindicatethatthelearnedRL-agentscansometimeshurtperformanceon
novelinputsbybiasingsearchtowardssub-optimalsynthesisrecipes. InABC-RL,weintroducea
newtermα∈[0,1]inEquation3thatstrategicallyweightsthefrompre-trainedagentscontribution,
completelyturningitoffwhenα=1(novelcircuit)anddefaultingtothebaselineapproachwhen
α=0.
(1)Similarityscorecomputation: Toquantifynoveltyofanewnetlist,G ,attest-time,wecompute
0
asimilarityscorewithrespecttoitsnearestneighborinthetrainingdatasetD ={Gtr,...,Gtr }.
tr 0 Ntr
Toavoidexpenseivenearestneighborqueriesinthegraphspace, forinstance, viasub-graphiso-
morphisms,weleveragethegraphencodings,h alreadylearnedbythepolicyagent. Specifically,
G
weoutputthesmallestcosinedistance(∆ cos(h G1,h G2) = 1− |hh GG 11 |· |h hG G2 2|)betweenthetestAIG
embeddingandallgraphsinthetrainingset: δ =min ∆ (h ,h ).
G0 i cos G0 Gt ir
(2)Tuningagent’srecommendationduringMCTS:Tomodulatethebalancebetweentheprior
learnedpolicyandpuresearch,weupdatethepriorUCTwithα∈[0,1]asfollows:
U∗k (s,a)=π (s,a)(1−α)·Uk (s,a), (4)
MCTS θ MCTS
andαiscomputedbypassingsimilarityscore,δ ,throughasigmoidfunction,α=σ (δ ),
G0 δth,T G0
definedasσ (z)= 1 withthreshold(δ )andtemperature(T)hyperparameters.
δth,T 1+e−z− Tδth th
Eq.4allowsαtosmoothlyvaryin[0,1]asintended,whilethresholdandtemperaturehyperparameters
controltheshapeofthesigmoid. Thresholdδ controlshowclosenewnetlistshavetobetothe
th
training data to be considered “novel." In general, small thresholds bias ABC-RL towards more
searchandlesslearningfrompastdata. Temperatureδ controlsthetransitionfrom“previously
th
seen"tonovel. SmalltemperaturescauseABC-RLtocreateaharderthresholdsbetweenpreviously
seenandnoveldesigns. Bothhyperparametersarechosenusingvalidationdata.
(3)Puttingitalltogether: InFig.4,wepresentanoverviewoftheABC-RL.Webeginbytraining
anRL-agentontrainingdatasetD . Then, weuseaseparateheld-outvalidationdatasettotune
tr
threshold,δ ,andtemperature,T,bycomparingwins/lossesofbaselineMCTS+Learningvs. ABC-
th
RLandperformingagrid-search.DuringinferenceonanewnetlistG ,ABC-RLretrievesthenearest
0
neighborfromthetrainingdata,computesαandperformsonlineα-guidedsearchusingweighted
recommendationsfromthepre-trainedRLagent.
3 EMPIRICAL EVALUATION
3.1 EXPERIMENTALSETUP
5PublishedasaconferencepaperatICLR2024
Split Circuits
Datasets:Weconsider
alu2,apex3,apex5,b2,c1355,c5315,c2670,c6288,prom2,frg1,i7,i8,m3,
Train threedatasetsusedby
max512,table5,adder,log2,max,multiplier,arbiter,ctrl,int2float,priority
logic synthesis com-
Valid apex7,c1908,c3540,frg2,max128,apex6,c432,c499,seq,table3,i10,sin,i2c
munity: MCNCYang
Test alu4,apex1,apex2,apex4,i9,m4,prom1,b9,c880,c7552,pair,max1024{C1-C12}, (1991), EPFL arith-
bar,div,square,sqrt{A1-A4},cavlc,mem_ctrl,router,voter{R1-R4}
metic and EPFL ran-
dom control bench-
Table1:Training,validationandtestsplitsinourexperiments.Netlistsfromeach
marks Amarú et al.
benchmarkarerepresentedineachsplit.Inthetestset,MCNCnetlistsarerelabeled
[C1-C12],EPFL-arithto[A1-A4]andEPFL-controlto[R1-R4]. (2015). MCNCbench-
markshave38netlists
rangingfrom100–8K
nodes. EPFLarithmeticbenchmarkshaveoperationslikeadditions,multiplicationsetc. andhave
1000-44Knodes. EPFLrandomcontrolbenchmarkshavefinite-statemachines,routinglogicand
otherfunctionswith100–46Knodes.
Train-testsplit: Werandomlysplitthe56totalnetlistsobtainedfromallthreebenchmarksinto23
netlistsfortraining13forvalidation(11MCNC,1EPFL-arith,1EPFL-rand)andremaining20for
test(seeTable1). Weensurethatnetlistsfromeachbenchmarkarerepresentedproportionallyin
training,validationandtestdata.
Optimization objective and metrics: We seek to identify the best L = 10 synthesis recipes.
ConsistentwithpriorworksHosnyetal.(2020);Zhuetal.(2020);Netoetal.(2022),weusearea-
delayproduct(ADP)asourQoRmetric. Areaanddelayvaluesareobtainedusinga7nmtechnology
librarypost-technologymappingofthesynthesizedAIG.Asabaseline,wecompareagainstADPof
theresyn2synthesisrecipeasisalsodoneinpriorworkNetoetal.(2022);Chowdhuryetal.(2022).
InadditiontoADPreduction,wereportruntimereductionofABC-RLatiso-QoR,i.e.,howmuch
fasterABC-RLisinreachingthebestADPachievedbycompetingmethods. Duringevaluationson
testcircuits,wegiveeachtechniqueatotalbudgetof100synthesisruns.
Trainingdetailsandhyper-parameters: TotrainourRL-agents,weuseHeinitializationHeetal.
(2015)forweightsandfollowingAndrychowiczetal.(2020),multiplyweightsofthefinallayerwith
0.01topreventbiastowardsanyoneaction. Agentsaretrainedfor50epochsusingAdamwithan
initiallearningrateof0.01. Ineachtrainingepoch,weperformMCTSonallnetlistswithanMCTS
searchbudgetK =512persynthesislevel. AfterMCTSsimulations,wesampleL×N (N is
tr tr
thenumberoftrainingcircuits)experiencesfromthereplaybuffer(size2×L×N )fortraining.
tr
Tostabilizetraining,wenormalizeQoRrewards(AppendixC.1)andclipitto[−1,+1]Mnihetal.
(2015). WesetT =100andδ =0.007basedonourvalidationdata.
th
WeperformedthetrainingonaservermachinewithoneNVIDIARTXA4000with16GBVRAM.
ThemajorbottleneckduringtrainingisthesynthesistimeforrunningABC;actualgradientupdates
arerelativelyinexpensive. FullytrainingtheRL-agenttrainingtookaround9days.
Baselinesforcomparison: WecompareABC-RLwithfivemainmethods: (1)MCTS:Search-only
MCTSNetoetal.(2022);(2)DRiLLS:RLagenttrainedviaA2Cusinghand-craftedAIGfeatures
(not on past-training data) Hosny et al. (2020) (3) Online-RL: RL agent trained online via PPO
usingGraphConvolutionalNetworksforAIGfeatureextraction(butnotonpasttrainingdata)(Zhu
et al., 2020); (4) SA+Pred: simulated annealing (SA) with QoR predictor learned from training
data Chowdhury et al. (2022); and (5) MCTS+L(earning): our own baseline MCTS+Learning
solutionusingapre-trainedRL-agent(Section2.3.1). MCTSandSA+Predarethecurrentstate-of-
the-art(SOTA)methods. DRiLLSandOnline-RLaresimilarandunder-performMCTS.Wereport
resultsversusexisitngmethodsforcompleteness. MCTS+Lisnewandhasnotbeenevaluatedon
logicsynthesis. AfinalandsixthbaselineforcomparisonisMCTS+L+FT(Mirhoseinietal.,2021)
whichisproposedforchipplacement,adifferentEDAproblem,butweadaptforlogicsynthesis.
MCTS+FTissimilartoMCTS+Lbutcontinuestofine-tuneitspre-trainedRL-agentontestinputs.
3.2 EXPERIMENTALRESULTS
3.2.1 ABC-RLVS. SOTA
InTable2,wecompareABC-RLoverSOTAmethods Hosnyetal.(2020);Zhuetal.(2020);Neto
etal.(2022);Chowdhuryetal.(2022)intermsofpercentagearea-delayproduct(ADP)reduction
6PublishedasaconferencepaperatICLR2024
20.0 12 80 22 02 14
17.5 18 12
15.0 16 16 10
12.5 14 14 8
10 57. ..0
05
11 02 11 02
8
46 M
S AA
BC
+
CT -PS
Rr Led.
8 6 2
20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100
Iterations Iterations Iterations Iterations
(a)alu4 (b)apex1 (c)c880 (d)apex4
Figure5:Area-delayproductreduction(in%)comparedtoresyn2onMCNCcircuits.
(relative to the baseline resyn2 recipe). We also report speed-ups of ABC-RL to reach the same
QoRastheSOTAmethods. Giventhelongsynthesisruntimes,speed-upatiso-QoRasanequally
importantmetric. Overall,ABC-RLachievesthelargestgeo. meanreductioninADP,reducing
ADP by 25% (smaller ADP is better) over resyn2. The next best method is our own MCTS+L
implementationwhichachieves20.7%ADPreduction,althoughitisonlyslightlybetterthanMCTS.
ABC-RLisalsoconsistentlythewinnerinallbutfournetlists,andineachofthesecasesABC-
RLfinishessecondonlymarginallybehindthewinner. Interestingly,thewinnerinthesecasesis
Online+RL,amethodthatoverallhasthepoorestperformance. Importantly,ABC-RLisconsistently
betterthanMCTS+Land MCTS.Thatis, weshowthatbothlearningonpastdataandusing
retrievalarekeytogoodperformance. Finally,ABC-RLisalsofasterthancompetingmethods
withageo. meanspeed-upof3.8×overSOTA.Wedivedeeperintospecificbenchmarksuiteto
understandwhereABC-RL’simprovementsstemfrom.
MCNCbenchmarks: ABC-RLprovideswithsubstantialimprovementsonbenchmarkssuchas
C1(apex1),C7(prom1),C9(c880),andC12(max1024). Fig.5plotstheADPreductionsover
searchiterationsforMCTS,SA+Pred,andABC-RLoverfournetlistsfromMCNC.NotefromFig.5
thatABC-RLinmostcasesachieveshigherADPreductionsearlierthancompetingmethods. Thus,
designerscanterminatesearchwhenadesiredADPisachieved. Thisresultsinrun-timespeedups
upto5.9×atiso-QoRcomparedtostandardMCTS.(seeAppendixD.1.1forcompleteresults)
Table2: Area-delayreduction(%)comparedtoresyn2: DRiLLSHosnyetal.(2020),Online-RLZhuetal.
(2020),SA+Pred.Chowdhuryetal.(2022),MCTSNetoetal.(2022),MCTS+Learning(MCTS+L)andABC-RL.
Speed-upvs.MCTS
.
ADPreduction(in%)
Methods
MCNC EPFLarith EPFLrandom Geo-
mean
C1 C2 C3 C4 C5 C6 C7 C8 C9 C10 C11 C12 A1 A2 A3 A4 R1 R2 R3 R4
DRiLLS 18.9 6.7 8.0 13.0 38.4 19.1 5.4 18.0 14.3 18.6 6.6 11.0 28.8 34.7 11.1 22.7 15.4 23.0 12.9 10.1 16.1
Online-RL 20.6 6.6 8.1 13.5 39.4 21.0 5.0 17.9 16.2 20.2 4.7 11.4 36.9 34.8 10.4 24.1 16.3 22.5 10.7 8.3 16.1
SA+Pred. 17.6 17.0 15.6 13.0 46.5 18.2 8.5 23.6 19.9 17.6 10.0 20.3 36.9 25.2 8.2 21.1 16.8 21.5 25.7 26.2 19.7
MCTS 17.1 15.9 13.1 13.0 46.9 14.9 6.5 23.2 17.7 20.5 13.1 19.7 25.4 46.0 10.7 18.7 15.9 21.6 21.6 27.1 19.8
MCTS+L 17.0 19.6 16.9 12.5 46.9 13.9 10.1 24.1 17.1 16.8 8.1 19.5 36.9 55.9 10.3 22.7 15.8 24.1 38.9 26.9 20.7
ABC-RL 19.9 19.6 16.8 15.0 46.9 19.1 12.1 24.3 21.3 21.1 13.6 21.6 36.9 56.2 14.0 23.8 19.8 30.2 38.9 30.0 25.3
Iso-QoR
1.9x 5.9x 1.8x 1.6x 1.2x 1.3x 3.2x 4.1x 2.2x 1.2x 0.9x 1.8x 3.7x 9.0x 4.2x 8.3x 5.0x 6.4x 3.1x 5.7x 3.8x
Speed-up
EPFL benchmarks: ABC-RL excels on 7 out of 8 EPFL designs, particularly demonstrating
significantimprovementsonA3(square),R1(cavlc),andR3(mem_ctrl)comparedtoprior
methodsHosnyetal.(2020);Zhuetal.(2020);Chowdhuryetal.(2022);Netoetal.(2022). Notably,
baselineMCTS+LearningperformspoorlyonA3(square),R1(cavlc),andR4(voter). In
Fig.6,weillustratehowABC-RLfine-tunesαforvariouscircuits,carefullyadjustingpre-trained
recommendationstoavoidunproductiveexplorationpaths. AcrossallEPFLbenchmarks,ABC-RL
consistently achieves superior ADP reduction compared to pure MCTS, with a geometric ADP
reduction of 28.85% over resyn2. This significantly improves QoR over standard MCTS and
SA+Pred., by 5.99% and 6.12% respectively. Moreover, ABC-RL delivers an average of 1.6×
runtimespeed-upatiso-QoRcomparedtostandardMCTSNetoetal.(2022),upto9×speed-up.
7
)%(
noitcudeRPublishedasaconferencepaperatICLR2024
16 20 30 25
14 18
12 16 28 20
10 14 26
8 12 15
0246 M
A AB
BC CCT
-
-S
R RL
L
(w/o tune)
10
68
222 024 10
5
4
20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100
Iterations Iterations Iterations Iterations
(a)square (b)cavlc (c)voter (d)c7552
Figure6:Area-delayproductreduction(in%)usingABC-RLcomparedtoMCTS+Learning.
3.2.2 BENCHMARKSPECIFICABC-RLAGENTSVS. SOTA
TofurtherexaminethebenefitsofABC-RLintermsofnetlistdiversity,wetrainthreebenchmark-
specificABC-RLagentsoneachbenchmarksuiteusing,asbefore,thetrain-vaidation-testsplitsin
Table1. Althoughtrainedoneachbenchmarkindividually,evaluationofbenchmark-specificagents
isonthefulltestdataset. Thishastwoobjectives: 1)Assesshowbenchmark-specificagentscompare
againstthebenchmark-wideagentsontheirowntestinputs,and2)StudyhowABC-RL’sbenchmark-
specific agents adapt when deployed on test inputs from other benchmarks. Benchmark-specific
agentsarereferredtoasABC-RL+X,whereXisthebenchmarkname(MCNC,ARITH,orRC).
Table3: Area-delayreduction(in%)obtainedusingbenchmarkspecificagents(MCNC,ARITHandRC).
MCTSrepresentstreesearchadoptedinNetoetal.(2022)
ABC-RL ADPreduction(in%)
agents
MCNC EPFLarith EPFLrandom Geo.
/Methods
mean
C1 C2 C3 C4 C5 C6 C7 C8 C9 C10 C11 C12 A1 A2 A3 A4 R1 R2 R3 R4
ABC-RL 19.9 19.6 16.8 15.0 46.9 19.1 12.1 24.3 21.3 21.1 13.6 21.6 36.9 56.2 14.0 23.8 19.8 30.2 38.9 30.0 25.3
+MCNC 20.9 19.2 17.5 15.0 52.5 18.1 10.9 24.1 24.7 21.1 16.8 22.0 32.9 47.9 10.9 18.7 18.2 24.0 24.5 27.1 21.3
+ARITH 18.0 16.0 17.3 13.0 46.9 20.3 8.8 23.2 20.0 24.1 13.1 20.8 36.9 55.9 12.1 25.1 16.9 21.5 21.8 27.7 21.4
+RC 19.0 18.5 17.4 12.5 46.9 16.8 11.9 23.3 22.5 20.4 13.1 20.0 36.5 53.4 10.8 18.7 17.8 22.8 26.6 27.1 21.7
MCTS 17.1 15.9 13.1 13.0 46.9 14.9 6.5 23.2 17.7 20.5 13.1 19.7 25.4 45.9 10.7 18.7 15.9 21.6 21.6 27.1 19.8
InTable3,wepresenttheperformanceofABC-RLusingbenchmark-specificagents. Notably,ABC-
RL+XagentsoftenoutperformthegeneralABC-RLagentontestinputsfromtheirownbenchmark
suites. Forexample,ABC-RL+MCNCoutperformsABC-RLon7of12benchmarks. Inreturn,the
performanceofbenchmark-specificagentsdropsontestinputsfromotherbenchmarksbecausethese
newnetlistsarenovelfortheagent. Nonetheless,ourbenchmark-specificagentsstilloutperform
theSOTAMCTSapproachingeo. meanADPreduction. Infact,ifexceptABC-RL,eachofour
benchmark-specificagentswouldstilloutperformotherSOTAmethodsincludingMCTS+L.These
resultsemphasizeABC-RL’sabilitytofine-tuneαeffectively,eveninthepresenceofasubstantial
distributiongapbetweentrainingandtestdata.
3.3 ABC-RLVS. MCTS+L+FT
Inrecentwork,Mirhoseinietal.(2021)proposedapre-trainedPPOagentforchipplacement. This
problemseekstoplaceblocksonthechipsurfacesoastoreducetotalchiparea,wire-lengthand
congestion. Althoughtheinputtochipplacementisalsoagraph,thegraphonlyencodesconnectivity
andnotfunctionality. Importantly,anactioninthissetting,e.g. movingorswappingblocks,isquick,
allowingformillionsofactionstobeexplored. Incontrast,forlogicsynthesis,actions(synthesis
steps)involveexpensivefunctionality-preservinggraph-leveltransformationsontheentiredesign
takingupto5minutesforlargerdesigns. Toadapttonewinputs,Mirhoseinietal.(2021)adopta
differentstrategy: theycontinuetofine-tune(FT)theiragentsastheyperformsearchontestinputs.
HereweaskiftheFTstrategycouldworkforABC-RLinsteadofourretrieval-guidedsolution.
Totestthis,wefine-tuneABC-RL’sbenchmark-wideagentduringonlineMCTSwithinourevaluation
budgetof100synthesisruns. Table4comparesABC-RLvs. thenewMCTS+L+FTapproach. ABC-
8
)%(
noitcudeRPublishedasaconferencepaperatICLR2024
Table4:Area-delayreduction(in%).ABC-RL−BERTisABC-RLtrainedwithnaivesynthesisencoderinstead
ofBERT.MCTS+L+FTindicateMCTS+Learningwithonlinefine-tuning.
ADPreduction(in%)
Ablation
study MCNC EPFLarith EPFLrandom Geo.
mean
C1 C2 C3 C4 C5 C6 C7 C8 C9 C10 C11 C12 A1 A2 A3 A4 R1 R2 R3 R4
ABC-RL 19.9 19.6 16.8 15.0 46.9 19.1 12.1 24.3 21.3 21.1 13.6 21.6 36.9 56.2 14.0 23.8 19.8 30.2 38.9 30.0 25.3
MCTS+L+FT 17.1 18.0 15.0 14.1 37.9 12.9 10.1 24.3 17.3 19.6 10.0 20.0 36.9 55.9 10.7 22.1 20.0 30.2 38.9 28.3 23.3
ABC-RL−BERT 17.0 16.5 14.9 13.1 44.6 16.9 10.0 23.5 16.3 18.8 10.6 19.0 36.9 51.7 9.9 20.0 15.5 23.0 28.0 26.9 21.2
RLoutperformsMCTS+L+FTonallbutonenetlist,andreducesADPby2.66%,2.40%and0.33%
onMCNC,EPFL,andrandomcontrolbenchmarks,with9.0%declineonC5(i9).
3.3.1 IMPACTOFARCHITECTURALCHOICES
WeinspecttheroleofBERT-basedrecipeencoderinABC-RLbyreplacingitwithafixedlength
(L=10)encoderwhere,usingtheapproachfrom(Chowdhuryetal.,2022),wedirectlyencodethe
synthesiscommandsinnumericalformandapplyzero-paddingforrecipelengthlessthanL. The
resultsareshowninTable4. ABC-RLreducesADPby2.51%,4.04%and4.11%onMCNC,EPFL
arithmeticandrandomcontrolbenchmarks,andupto-10.90%declineonR3(router),compared
totheversionwithoutBERT.Thisshowstheimportanceoftransformer-basedencoderinextracting
meaningfulfeaturesfromsynthesisrecipesub-sequencesforstaterepresentation.
4 RELATED WORK
Learning-basedapproachesforlogicsynthesis: Thiscanbeclassifiedintotwosub-categories: 1)
Synthesisrecipeclassification(Yuetal.,2018;Netoetal.,2019)andprediction(Chowdhuryetal.,
2021;2022)basedapproaches,and2)RL-basedapproaches(Haaswijketal.,2018;Hosnyetal.,
2020;Zhuetal.,2020). Netoetal.(2019)partitiontheoriginalgraphintosmallersub-networks
andperformsbinaryclassificationonsub-networkstopickwhichrecipesworkbest. Ontheother
hand,RL-basedsolutionsHaaswijketal.(2018);Hosnyetal.(2020);Zhuetal.(2020)useonline
RL algorithms to craft synthesis recipes, but do not leverage prior data. We show that ABC-RL
outperformsthem.
MLforEDA:MLhasbeenusedforarangeofEDAproblems Mirhoseinietal.(2021);Kurinetal.
(2020);Laietal.(2022;2023);Schmittetal.(2021);Yolcu&Póczos(2019);Vasudevanetal.(2021);
Yangetal.(2022). Closertothiswork, Mirhoseinietal.(2021)usedadeep-RLagenttooptimize
chipplacement,adifferentproblem,andusethepre-trainedagent(withonlinefine-tuning)toplace
thenewdesign. Thisleaveslimitedscopeforonlineexploration. Additionally,eachmoveoractionin
placement,i.e.,movingthex-yco-ordinatesofmodulesinthedesign,ischeapunliketime-consuming
actionsinlogicsynthesis. Thusplacementagentscanbefine-tunedwithlargeramountsoftest-time
datarelativetoABC-RLwhichhasaconstrainedonlinesearchbudget. Ourablationstudyshows
ABC-RLdefeatssearchcombinedwithfine-tunedagentforgivensynthesisbudget. Arelatedbodyof
workdevelopedgeneralrepresentationsofbooleancircuits,forinstance,DeepGateLietal.(2022);
Shietal.(2023),ConVERTSChowdhuryetal.(2023)and“functionalitymatters"Wangetal.(2022),
learnedonsignalprobabilityestimationandfunctionalityprediction,respectively. Theseembeddings
couldenhancethequalityofourGCNembeddingsandareinterestingavenuesforfuturework.
RL and search for combinatorial optimization: Fusing learning and search finds applications
acrossdiversedomainssuchasbranchingheuristics(Heetal.,2014),Goandchessplaying(Silver
etal.,2016;Schrittwieseretal.,2020),travelingsalesman(TSP)(Xing&Tu,2020),andcommon
subgraphdetection(Baietal.,2021). Eachoftheseproblemshasuniquestructure. TSPandcommon
subgraphdetectionbothhavegraphinputslikelogicsynthesisbutdonotperformtransformationson
graphs. Branchingproblemshavetree-structure,butdonotoperateongraphs. GoandChessinvolve
self-play during training and must anticipate opponents. Thus these works have each developed
specializedsolutionstailoredtotheproblemdomain,aswedowithABC-RL.Further,theseprevious
workshavenotidentifieddistributionshiftasaproblemandoperateatleastundertheassumptionthat
train-teststatedistributionsalignclosely.
9PublishedasaconferencepaperatICLR2024
RetrievalguidedReinforcementlearning: Recentworks(Goyaletal.,2022;Humphreysetal.,
2022)haveexploredthebenefitsofretrievalingame-playingRL-agents. However,theyimplement
retrieval differently: trajectories from prior episodes are retrieved and the entire trajectory is an
additonalinputtothepolicyagent. Thisalsorequiresthepolicy-agenttobeawareofretrievalduring
training. Incontrast,ourretrievalstrategyislightweight;insteadofanentiregraph/netlist,weonly
retrievethesimilarityscorefromthetrainingdatasetandthenfixα. Inaddition,wedonotneedto
incorporatetheretrievalstrategyduringtraining,enablingoff-the-shelfuseofpre-trainedRLagents.
ABC-RLalreadysignificantlyoutperformsSOTAmethodswiththisstrategy,buttheapproachmight
bebeneficialinothersettingswhereonlinecostsareseverelyconstrained.
5 CONCLUSION
WeintroduceABC-RL,anovelmethodologythatoptimizeslearningandsearchthrougharetrieval-
guidedmechanism,significantlyenhancingtheidentificationofhigh-qualitysynthesisrecipesfornew
hardwaredesigns. Specifically,tuningtheαparameteroftheRLagentduringMCTSsearchwithin
thesynthesisrecipespaceeffectivelymitigatesmisguidedsearchestowardunfavorablerewarding
trajectories,particularlywhenencounteringsufficientlynoveldesigns. Thesecoreconcepts,substan-
tiatedbyempiricalresults,underscorethepotentialofABC-RLingeneratinghigh-qualitysynthesis
recipes,therebystreamliningmoderncomplexchipdesignprocessesforenhancedefficiency.
ReproducibilityStatementForreproducibilityweprovidedetailedinformationregardingmethod-
ologies,architectures,andsettingsinSection3.1. Weattachcodebaseforreview. Postacceptanceof
ourwork,wewillpubliclyreleaseitwithdetaileduser’sinstruction.
REFERENCES
Luca Amarú, Pierre-Emmanuel Gaillardon, and Giovanni De Micheli. The epfl combinational
benchmarksuite. InProceedingsofthe24thInternationalWorkshoponLogic&Synthesis(IWLS),
numberCONF,2015.
LucaAmarú,PatrickVuillod,JiongLuo,andJanetOlson. Logicoptimizationandsynthesis: Trends
and directions in industry. In Design, Automation & Test in Europe Conference & Exhibition
(DATE),2017,pp.1303–1305.IEEE,2017.
MarcinAndrychowicz,AntonRaichuk,PiotrStan´czyk,ManuOrsini,SertanGirgin,RaphaelMarinier,
LéonardHussenot,MatthieuGeist,OlivierPietquin,MarcinMichalski,etal. Whatmattersin
on-policyreinforcementlearning? alarge-scaleempiricalstudy. arXivpreprintarXiv:2006.05990,
2020.
Yunsheng Bai, Derek Xu, Yizhou Sun, and Wei Wang. Glsearch: Maximum common subgraph
detectionvialearningtosearch. InInternationalConferenceonMachineLearning,pp.588–598.
PMLR,2021.
RobertBraytonandAlanMishchenko. ABC:AnAcademicIndustrial-StrengthVerificationTool.
InTayssirTouili,ByronCook,andPaulJackson(eds.),ComputerAidedVerification,pp.24–40,
2010.
RobertKBrayton,GaryDHachtel,CurtMcMullen,andAlbertoSangiovanni-Vincentelli. Logic
minimizationalgorithmsforVLSIsynthesis,volume2. SpringerScience&BusinessMedia,1984.
AnimeshBChowdhury,JitendraBhandari,LucaCollini,RameshKarri,BenjaminTan,andSiddharth
Garg. ConVERTS:Contrastivelylearningstructurallyinvariantnetlistrepresentations. In2023
ACM/IEEE5thWorkshoponMachineLearningforCAD(MLCAD),pp.1–6.IEEE,2023.
Animesh Basak Chowdhury, Benjamin Tan, Ramesh Karri, and Siddharth Garg. OpenABC-D:
A large-scale dataset for machine learning guided integrated circuit synthesis. arXiv preprint
arXiv:2110.11292,2021.
AnimeshBasakChowdhury,BenjaminTan,RyanCarey,TushitJain,RameshKarri,andSiddharth
Garg.Bulls-eye:Activefew-shotlearningguidedlogicsynthesis.IEEETransactionsonComputer-
AidedDesignofIntegratedCircuitsandSystems,2022.
10PublishedasaconferencepaperatICLR2024
JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingofdeep
bidirectionaltransformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,2018.
AnirudhGoyal,AbramFriesen,AndreaBanino,TheophaneWeber,NanRosemaryKe,AdriaPuig-
domenech Badia, Arthur Guez, Mehdi Mirza, Peter C Humphreys, Ksenia Konyushova, et al.
Retrieval-augmentedreinforcementlearning. InInternationalConferenceonMachineLearning,
pp.7740–7765.PMLR,2022.
WinstonHaaswijk,EdoCollins,BenoitSeguin,MathiasSoeken,FrédéricKaplan,SabineSüsstrunk,
and Giovanni De Micheli. Deep learning for logic optimization algorithms. In International
SymposiumonCircuitsandSystems(ISCAS),pp.1–4,2018.
HeHe,HalDaumeIII,andJasonMEisner. Learningtosearchinbranchandboundalgorithms.
Advancesinneuralinformationprocessingsystems,27,2014.
KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Delvingdeepintorectifiers: Surpassing
human-levelperformanceonimagenetclassification. InProceedingsoftheIEEEinternational
conferenceoncomputervision,pp.1026–1034,2015.
AbdelrahmanHosny,SoheilHashemi,MohamedShalan,andSheriefReda. DRiLLS:Deeprein-
forcementlearningforlogicsynthesis. InAsiaandSouthPacificDesignAutomationConference
(ASP-DAC),pp.581–586,2020.
PeterHumphreys,ArthurGuez,OlivierTieleman,LaurentSifre,ThéophaneWeber,andTimothy
Lillicrap. Large-scale retrieval for reinforcement learning. Advances in Neural Information
ProcessingSystems,35:20092–20104,2022.
ThomasNKipfandMaxWelling. Semi-supervisedclassificationwithgraphconvolutionalnetworks.
arXivpreprintarXiv:1609.02907,2016.
LeventeKocsisandCsabaSzepesvári. Banditbasedmonte-carloplanning. InMachineLearning:
ECML2006: 17thEuropeanConferenceonMachineLearningBerlin,Germany,September18-22,
2006Proceedings17,pp.282–293.Springer,2006.
Vitaly Kurin, Saad Godil, Shimon Whiteson, and Bryan Catanzaro. Can q-learning with graph
networkslearnageneralizablebranchingheuristicforasatsolver? AdvancesinNeuralInformation
ProcessingSystems,33:9608–9621,2020.
YaoLai,YaoMu,andPingLuo. Maskplace: Fastchipplacementviareinforcedvisualrepresentation
learning. AdvancesinNeuralInformationProcessingSystems,35:24019–24030,2022.
YaoLai,JinxinLiu,ZhentaoTang,BinWang,JianyeHao,andPingLuo. Chipformer: Transferable
chipplacementviaofflinedecisiontransformer. arXivpreprintarXiv:2306.14744,2023.
MinLi,SadafKhan,ZhengyuanShi,NaixingWang,HuangYu,andQiangXu. Deepgate: Learning
neuralrepresentationsoflogicgates. InProceedingsofthe59thACM/IEEEDesignAutomation
Conference,pp.667–672,2022.
AzaliaMirhoseini,AnnaGoldie,MustafaYazgan,JoeWenjieJiang,EbrahimSonghori,ShenWang,
Young-JoonLee,EricJohnson,OmkarPathak,AzadeNazi,etal. Agraphplacementmethodology
forfastchipdesign. Nature,594(7862):207–212,2021.
AlanMishchenko,SatrajitChatterjee,andRobertBrayton. DAG-awareaigrewriting: Afreshlookat
combinationallogicsynthesis. InDesignAutomationConference(DAC),pp.532–535,2006.
VolodymyrMnih,KorayKavukcuoglu,DavidSilver,AndreiARusu,JoelVeness,MarcGBellemare,
AlexGraves,MartinRiedmiller,AndreasKFidjeland,GeorgOstrovski,etal. Human-levelcontrol
throughdeepreinforcementlearning. nature,518(7540):529–533,2015.
Walter Lau Neto, Max Austin, Scott Temple, Luca Amaru, Xifan Tang, and Pierre-Emmanuel
Gaillardon. LSOracle: alogicsynthesisframeworkdrivenbyartificialintelligence: Invitedpaper.
InInternationalConferenceonComputer-AidedDesign(ICCAD),pp.1–6,2019.
11PublishedasaconferencepaperatICLR2024
Walter Lau Neto, Yingjie Li, Pierre-Emmanuel Gaillardon, and Cunxi Yu. Flowtune: End-to-
end automatic logic optimization exploration via domain-specific multi-armed bandit. IEEE
TransactionsonComputer-AidedDesignofIntegratedCircuitsandSystems,2022.
Heinz Riener, Eleonora Testa, Winston Haaswijk, Alan Mishchenko, Luca Amarù, Giovanni
DeMicheli,andMathiasSoeken. Scalablegenericlogicsynthesis: Oneapproachtorulethemall.
InProceedingsofthe56thAnnualDesignAutomationConference2019,pp.1–6,2019.
FrederikSchmitt,ChristopherHahn,MarkusNRabe,andBerndFinkbeiner. Neuralcircuitsynthesis
fromspecificationpatterns. AdvancesinNeuralInformationProcessingSystems,34:15408–15420,
2021.
JulianSchrittwieser,IoannisAntonoglou,ThomasHubert,KarenSimonyan,LaurentSifre,Simon
Schmitt,ArthurGuez,EdwardLockhart,DemisHassabis,ThoreGraepel,etal. Masteringatari,
go,chessandshogibyplanningwithalearnedmodel. Nature,588(7839):604–609,2020.
Zhengyuan Shi, Hongyang Pan, Sadaf Khan, Min Li, Yi Liu, Junhua Huang, Hui-Ling Zhen,
MingxuanYuan,ZhufeiChu,andQiangXu.Deepgate2:Functionality-awarecircuitrepresentation
learning. arXivpreprintarXiv:2305.16373,2023.
DavidSilver,AjaHuang,ChrisJMaddison,ArthurGuez,LaurentSifre,GeorgeVanDenDriessche,
JulianSchrittwieser,IoannisAntonoglou,VedaPanneershelvam,MarcLanctot,etal. Mastering
thegameofgowithdeepneuralnetworksandtreesearch. nature,529(7587):484–489,2016.
DavidSilver,ThomasHubert,JulianSchrittwieser,IoannisAntonoglou,MatthewLai,ArthurGuez,
MarcLanctot,LaurentSifre,DharshanKumaran,ThoreGraepel,etal. Masteringchessandshogi
byself-playwithageneralreinforcementlearningalgorithm. arXivpreprintarXiv:1712.01815,
2017.
ShailjaThakur,BaleeghAhmad,HammondPearce,BenjaminTan,BrendanDolan-Gavitt,Ramesh
Karri,andSiddharthGarg. Verigen: Alargelanguagemodelforverilogcodegeneration. arXiv
preprintarXiv:2308.00708,2023.
ShobhaVasudevan, WenjieJoe Jiang, David Bieber, Rishabh Singh, CRichard Ho, CharlesSut-
ton, et al. Learning semantic representations to verify hardware designs. Advances in Neural
InformationProcessingSystems,34:23491–23504,2021.
Ziyi Wang, Chen Bai, Zhuolun He, Guangliang Zhang, Qiang Xu, Tsung-Yi Ho, Bei Yu, and
YuHuang. Functionalitymattersinnetlistrepresentationlearning. InProceedingsofthe59th
ACM/IEEEDesignAutomationConference,pp.61–66,2022.
ZhihaoXingandShikuiTu. Agraphneuralnetworkassistedmontecarlotreesearchapproachto
travelingsalesmanproblem. IEEEAccess,8:108418–108428,2020.
SaeyangYang. Logicsynthesisandoptimizationbenchmarksuserguide: version3.0. Citeseer,1991.
WenlongYang,LingliWang,andAlanMishchenko. Lazyman’slogicsynthesis. InProceedingsof
theInternationalConferenceonComputer-AidedDesign,pp.597–604,2012.
ZhihaoYang,DongLi,YingxueffZhang,ZhanguangZhang,GuojieSong,JianyeHao,etal.Versatile
multi-stage graph neural network for circuit representation. Advances in Neural Information
ProcessingSystems,35:20313–20324,2022.
EmreYolcuandBarnabásPóczos.Learninglocalsearchheuristicsforbooleansatisfiability.Advances
inNeuralInformationProcessingSystems,32,2019.
Cunxi Yu. Flowtune: Practical multi-armed bandits in boolean optimization. In International
ConferenceOnComputerAidedDesign(ICCAD),pp.1–9,2020.
CunxiYu,HoupingXiao,andGiovanniDeMicheli. Developingsynthesisflowswithouthuman
knowledge. InDesignAutomationConference(DAC),pp.1–6,2018.
KerenZhu,MingjieLiu,HaoChen,ZhengZhao,andDavidZ.Pan. Exploringlogicoptimizations
withreinforcementlearningandgraphconvolutionalnetwork. InWorkshoponMachineLearning
forCAD(MLCAD),pp.145–150,2020.
12PublishedasaconferencepaperatICLR2024
A APPENDIX 1
A.1 LOGICSYNTHESIS
Logicsynthesistransformsahardwaredesigninregistertransferlevel(RTL)toaBooleangate-level
network,optimizesthenumberofgates/depth,andthenmapsittostandardcellsinatechnology
library Brayton et al. (1984). Well-known representations of Boolean networks include sum-of-
productform,product-of-sum,Binarydecisiondiagrams,andAIGswhichareawidelyaccepted
formatusingonlyAND(nodes)andNOTgates(dottededges). Severallogicminimizationheuristics
(discussedinSectionA.2))havebeendevelopedtoperformoptimizationonAIGgraphsbecauseofits
compactcircuitrepresentationanddirectedacyclicgraph(DAG)-basedstructuring. Theseheuristics
areappliedsequentially(“synthesisrecipe”)toperformone-passlogicoptimizationreducingthe
number of nodes and depth of AIG. The optimized network is then mapped using cells from
technologylibrarytofinallyreportarea,delayandpowerconsumption.
A.2 LOGICMINIMIZATIONHEURISTICS
WenowdescribeoptimizationheuristicsprovidedbyindustrialstrengthacademictoolABCBrayton
&Mishchenko(2010):
1. Balance(b)optimizesAIGdepthbyapplyingassociativeandcommutativelogicfunctiontree-
balancingtransformationstooptimizefordelay.
2. Rewrite (rw, rw -z) is a directed acyclic graph (DAG)-aware logic rewriting technique that
performstemplatepatternmatchingonsub-treesandencodesthemwithequivalentlogicfunctions.
3. Refactor(rf,rf-z)performsaggressivechangestothenetlistwithoutcaringaboutlogicsharing.
ItiterativelyexaminesallnodesintheAIG,listsoutthemaximumfan-out-freecones,andreplaces
themwithequivalentfunctionswhenitimprovesthecost(e.g.,reducesthenumberofnodes).
4.Re-substitution(rs,rs-z)createsnewnodesinthecircuitrepresentingintermediatefunctionalities
usingexistingnodes;andremoveredundantnodes. Re-substitutionimproveslogicsharing.
The zero-cost (-z) variants of these transformation heuristics perform structural changes to the
netlistwithoutreducingnodesordepthofAIG. However,previousempiricalresultsshowcircuit
transformationshelpfuturepassesofotherlogicminimizationheuristicsreducethenodes/depthand
achievetheminimizationobjective.
A.3 MONTECARLOTREESEARCH
WediscussindetailtheMCTSalgorithm. Duringselection,asearchtreeisbuiltfromthecurrent
statebyfollowingasearchpolicy,withtheaimofidentifyingpromisingstatesforexploration.
whereQk (s,a)denotesestimatedQvalue(discussednext)obtainedaftertakingactionafrom
MCTS
statesduringthekthiterationofMCTSsimulation. Uk (s,a)representsupperconfidencetree
MCTS
(UCT)explorationfactorofMCTSsearch.
(cid:115) log(cid:0)(cid:80)
Nk
(s,a)(cid:1)
Uk (s,a)=c a MCTS , (5)
MCTS UCT Nk (s,a)
MCTS
Nk (s,a)denotesthevisitcountoftheresultingstateaftertakingactionafromstates. c
MCTS UCT
denotesaconstantexplorationfactorKocsis&Szepesvári(2006).
Theselectionphaserepeatsuntilaleafnodeisreachedinthesearchtree. AleafnodeinMCTStree
denoteseithernochildnodeshavebeencreatedoritisaterminalstateoftheenvironment. Once
aleafnodeisreachedtheexpansionphasebeginswhereanactionispickedrandomlyanditsroll
outvalueisreturnedorR(s )isreturnedfortheterminalstates . Next,backpropagationhappens
L L
whereallparentnodesQ (s,a)valuesareupdatedaccordingtothefollowingequation.
k
Nk (s,a)
MC(cid:88)TS
Qk (s,a)= Ri (s,a)/Nk (s,a). (6)
MCTS MCTS MCTS
i=1
13PublishedasaconferencepaperatICLR2024
A.4 ABC-RLAGENTPRE-TRAININGPROCESS
AsdiscussedinSection2.3,wepre-trainanagentusingavailablepastdatatohelpwithchoosing
whichlogicminimizationheuristictoaddtothesynthesisrecipe.TheprocessisshownasAlgorithm1.
Algorithm1ABC-RL:Policyagentpre-training
1: procedureTRAINING(θ)
2: Replaybuffer(RB) ← ϕ,D train = {AIG 1,AIG 2,...,AIG n},num_epochs=N,Recipe
length=L,AIGembeddingnetwork: Λ,Recipeembeddingnetwork: R,Agentpolicyπ :=U
θ
(Uniformdistribution),MCTSiterations=K,Actionspace=A
3: forAIG i ∈D traindo
4: r ←0,depth←0
5: s←Λ(AIG i)+R(r)
6: whiledepth<Ldo
7: π MCTS =MCTS(s,π θ,K)
8: a=argmax a′∈Aπ MCTS(s,a′)
9: r ←r+a,s′ ←A(AIG i)+R(r)
10: RB ←RB(cid:83) (s,a,s′,π MCTS(s,·))
11: s←s′,depth←depth+1
12: forepochs<N do
13: θ ←θ i−α∇ θL(π MCTS,π θ)
B NETWORK ARCHITECTURE
B.1 AIGNETWORKARCHITECTURE
AIGencodinginABC:AnAIGgraphisadirectedacyclicgraphrepresentingthecircuit’sboolean
functionality. WereadinthesameAIGformatintroducedinMishchenkoetal.(2006)andcommonly
usedinliterature: nodesintheAIGrepresentANDgates,PrimaryInputs(PIs)orPrimaryOutputs
(POs). Ontheotherhand,NOTgatesarerepresentedbyedges: dashededgesrepresentNOTgates
(i.e.,theoutputoftheedgeisalogicalnegationofitsinput)andsolidedgesrepresentasimplewire
whoseoutputequalsitsinput.
GCN-basedAIGembedding: StartingwithagraphG=(V,E)thathasverticesVandedgesE,the
GCNaggregatesfeatureinformationofanodewithitsneighbors’nodeinformation.Theoutputisthen
normalizedusingBatchnormandpassedthroughanon-linearLeakyReLUactivationfunction.
Thisprocessisrepeatedforklayerstoobtaininformationforeachnodebasedoninformationfrom
itsneighboursuptoadistanceofk-hops. Agraph-levelREADOUToperationproducesagraph-level
embedding. Formally:
(cid:88)
hk−1
hk =σ(W i +b ),k ∈[1..K] (7)
u k (cid:112) (cid:112) k
N(u)× N(v)
i∈u∪N(u)
h =READOUT({hk;u∈V})
G u
Here,theembeddingfornodeu,generatedbythekth layeroftheGCN,isrepresentedbyhk.The
u
parametersW andb aretrainable,andσisanon-linearReLUactivationfunction. N(·)denotes
k k
the1-hopneighborsofanode. TheREADOUTfunctioncombinestheactivationsfromthekthlayer
ofallnodestoproducethefinaloutputbyperformingapoolingoperation.
EachnodeintheAIGreadinfromABCistranslatedtoanodeinourGCN.Fortheinitialembeddings,
h0,Weusetwo-dimensionalvectortoencodenode-levelfeatures: (1)nodetype(AND,PI,orPO)
u
and(2)numberofnegatedfan-inedgesChowdhuryetal.(2021;2022). wechoosek =3andglobal
averageandmaxpoolingconcatenatedastheREADOUToperation.
ArchitecturalchoiceofGNN:WearticulateourrationaleforutilizingasimpleGraphConvolutional
Network (GCN) architecture to encode AIGs for the generation of synthesis recipes aimed at
optimizing the area-delay product. We elucidate why this approach is effective and support our
argumentwithanexperimentthatvalidatesitsefficacy:
14PublishedasaconferencepaperatICLR2024
• Workingprincipleoflogicsynthesistransformations: Logicsynthesistransformations
ofABC(andingeneralcommerciallogicsynthesistools)includingrewrite,refactorand
re-substituteperformstransformationsatalocalsubgraphlevelsratherthanthewholeAIG
structure. Fore.g. *rewrite*performsabackwardpassfromprimaryoutputstoprimary
inputs,performk-waycutateachAIGnodeandreplacethefunctionalitywithoptimized
implementationavailableinthetruthtablelibraryofABC.Similarly,*refactor*randomly
picksafan-inconeofanintermediatenodeofAIGandreplacesitwithadifferentimplemen-
tationifitreducedthenodesofAIG.Thus,effectivenessofanysynthesistransformations
donotrequiredeeperGCNlayers;capturingneighborhoodinformationuptodepth3inour
caseworkedwelltoextractfeaturesoutofAIGwhichcanhelppredictwhichtransformation
nextcanhelpreducearea-delayproduct.
• FeatureinitializationofnodesinAIG:ThenodeinourAIGencompassestwoimportant
feature: i)Typeofnode(PrimaryInput,PrimaryOutputandInternalNode)andii)Number
ofnegatedfan-ins. Thus,ourfeatureinitializationschemetakescareofthefunctionality
eventhoughthestructureofAIGareexactlysimilarandthefunctionality. Thus,twoAIG
havingexactsamestructurebutedgetypesaredifferent(dottededgerepresentnegationand
solidedgerepresentbuffer),theinitialnodefeaturesofAIGitselfwillbevastlydifferent
and thus our 3-layer GCNs have been capable enough to distinguish between them and
generatedifferentsynthesisrecipes.
Several recent GNN-based architectures Li et al. (2022); Shi et al. (2023) have been proposed
recently to capture functionality of AIG-based hardware representations. This remains an active
explorationdirectiontofurtherenhancethebenefitsofABC-RLtodistinguishstructurallysimilar
yetsubstantiallyvaryinginfunctionalityspace.
C EXPERIMENTAL DETAILS
C.1 REWARDNORMALIZATION
Inourwork,maximizingQoRentailsfindingarecipeP whichisminimizingthearea-delayproduct
of transformed AIG graph. We consider as a baseline recipe an expert-crafted synthesis recipe
resyn2Mishchenkoetal.(2006)ontopofwhichweimproveourADP.
(cid:40)
1− ADP(S(G,P)) ADP(S(G,P))<2×ADP(S(G,P)),
R= ADP(S(G,resyn2))
−1 otherwise.
C.2 BENCHMARKCHARACTERIZATION
Wepresentthecharacterizationofcircuitsusedinourdataset. Thisdataprovidesacleanpictureon
sizeandlevelvariationacrossalltheAIGs.
15PublishedasaconferencepaperatICLR2024
Name Inputs Outputs Nodes level
alu2 10 6 401 40
alu4 10 6 735 42
apex1 45 45 2655 27
apex2 39 3 445 29
apex3 54 50 2374 21
apex4 9 19 3452 21
apex5 117 88 1280 21
apex6 135 99 659 15
apex7 49 37 221 14
b2 16 17 1814 22
b9 41 21 105 10
C432 36 7 209 42
C499 41 32 400 20
C880 60 26 327 24
C1355 41 32 504 26
C1908 31 25 414 32
C2670 233 140 717 21
C3540 50 22 1038 41
C5315 178 123 1773 38
C6288 32 32 2337 120
C7552 207 108 2074 29
frg1 28 3 126 19
frg2 143 139 1164 13
i10 257 224 2675 50
i7 199 67 904 6
i8 133 81 3310 21
i9 88 63 889 14
m3 8 16 434 14
m4 8 16 760 14
max1024 10 6 1021 20
max128 7 24 536 13
max512 9 6 743 19
pair 173 137 1500 24
prom1 9 40 7803 24
prom2 9 21 3513 22
seq 41 35 2411 29
table3 14 14 2183 24
table5 17 15 1987 26
adder 256 129 1020 255
bar 135 128 3336 12
div 128 128 44762 4470
log2 32 32 32060 444
max 512 130 2865 287
multiplier 128 128 27062 274
sin 24 25 5416 225
sqrt 128 64 24618 5058
square 62 128 18484 250
arbiter 256 129 11839 87
ctrl 7 26 174 10
cavlc 10 11 693 16
i2c 147 142 1342 20
int2float 11 7 260 16
mem_ctrl 1204 1231 46836 114
priority 128 8 978 250
router 60 30 257 54
voter 1001 1 13758 70
Table5:Benchmarkcharacterization:Primaryinputs,outputs,numberofnodesandlevelofAIGs
16PublishedasaconferencepaperatICLR2024
25.0 17.5 14 50
122 702 ... 505 11 25 .. 50 11 02
40
15.0 10.0 8 30
12.5 7.5 6 20
10.0 5.0 4
7.5 2.5 2 10
5.0
20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100
Iterations Iterations Iterations Iterations
(a)b9 (b)apex2 (c)prom1 (d)i9
20.0 15.0 22.5 25
17.5 12.5 20.0 20
15.0 10.0 17.5
12.5 7.5 15.0 15
10.0 5.0 12.5 10
57 .. 05 02 .. 05 10 7. .0
5
5
20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100
Iterations Iterations Iterations Iterations
(e)m4 (f)pair (g)max1024 (h)c7552
Figure 7: Area-delay product reduction (in %) compared to resyn2 on MCNC circuits. GREEN:
SA+Pred.Chowdhuryetal.(2022),BLUE:MCTSNetoetal.(2022),RED:ABC-RL
D RESULTS
D.1 PERFORMANCEOFABC-RLAGAINSTPRIORWORKSANDBASELINEMCTS+LEARNING
D.1.1 MCNCBENCHMARKS
Figure7plotstheADPreductionsoversearchiterationsforMCTS,SA+Pred,andABC-RL.Inm4,
ABC-RL’sagentexplorespathswithhigherrewardswhereasstandardMCTScontinuessearching
withoutfurtherimprovement. Asimilartrendisobservedforprom1demonstratingthatapre-trained
agenthelpsbiassearchtowardsbetterpartsofthesearchspace. SA+Pred.Chowdhuryetal.(2022)
alsoleveragespasthistory,butisunabletocompete(onaverage)withMCTSandABC-RLinpart
becauseSAtypicallyunderperformsMCTSontree-basedsearchspaces. AlsonotefromFigure5
thatABC-RLinmostcasesachieveshigherADPreductionsearlierthancompetingmethods(except
pair). Thisresultsinsignificantgeo. meanrun-timespeedupsof2.5×atiso-QoRcomparedto
standardMCTSonMCNCbenchmarks.
D.1.2 EPFLARITHMETICBENCHMARKS
Figure8illustratestheperformanceofABC-RLincomparisontostate-of-the-artmethods: Pure
MCTSNetoetal.(2022)andSA+PredictionChowdhuryetal.(2022). Incontrasttothescenario
where MCTS+Baseline underperforms pure MCTS (as shown in 2), here we observe that ABC-
RL effectively addresses this issue, resulting in superior ADP reduction. Remarkably, ABC-RL
achievedageometricmean5.8×iso-QoRspeed-upcomparedtoMCTSacrosstheEPFLarithmetic
benchmarks.
16 25
35 50 14
30 40 12 20
10
25 30 8 15
20 20 6 10 MCTS
15 10 4 SA+Pred.
0 2 5 ABC-RL
20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100
Iterations Iterations Iterations Iterations
(a)bar (b)div (c)square (d)sqrt
Figure8:Area-delayproductreduction(in%)comparedtoresyn2onEPFLarithmeticbenchmarks.GREEN:
SA+Pred.Chowdhuryetal.(2022),BLUE:MCTSNetoetal.(2022),RED:ABC-RL
17
)%(
noitcudeR
)%(
noitcudeR
)%(
noitcudeRPublishedasaconferencepaperatICLR2024
D.1.3 EPFLRANDOMCONTROLBENCHMARKS
20 30 40 30
18 35 28
16 25 30 26
14 25 24
12 20 20 22
10 468 11 05 11 05
05
112 680 M S AA BC + CT -PS Rr Led.
20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100
Iterations Iterations Iterations Iterations
(a)cavlc (b)mem_ctrl (c)router (d)voter
Figure9:Area-delayproductreduction(in%)comparedtoresyn2onEPFLrandomcontrolbenchmarks.On
cavlcandrouter,ABC-RLperformbetterthanMCTSwherebaselineMCTS+Learningunder-perform.
GREEN:SA+Pred.Chowdhuryetal.(2022),BLUE:MCTSNetoetal.(2022),RED:ABC-RL.
D.2 PERFORMANCEOFBENCHMARK-SPECIFICABC-RLAGENTS
ABC-RL+MCNCagent: For6outof12MCNCbenchmarks,ABC-RLguidedbytheMCNCagent
demonstratedimprovedperformancecomparedtothebenchmark-wideagent. Thissuggeststhat
thehyper-parameters(δ andT)derivedfromthevalidationdatasetledtooptimizedαvaluesfor
th
MCNCbenchmarks. However,theperformanceoftheMCNCagentwascomparativelyloweron
EPFLarithmeticandrandomcontrolbenchmarks.
ABC-RL+ARITH agent: Our EPFL arith agent resulted in better ADP reduction compared to
benchmark-wideagentonlyonA4(sqrt). Thisindicatethatbenchmark-wideagentisabletolearn
morefromdiversesetofbenchmarksresultinginbetterADPreduction. OnMCNCbenchmarks,we
observethatARITHagentperformedthebestamongstallonC6(m4)andC10(c7552)because
thesearearithmeticcircuits.
ABC-RL+RCagent: OurRCagentperformanceonEPFLrandomcontrolbenchmarksarenotthat
greatcomparedtobenchmark-wideagent. ThisisprimarilybecauseofthefactthatEPFLrandom
controlbenchmarkshavehardwaredesignsperforminguniquefunctionalityandhencelearningfrom
historydoesn’thelpmuch. But,ABC-RLensuresthatperformancedon’tdetoriatecomparedtopure
MCTS.
D.3 PERFORMANCEOFABC-RLVERSUSFINE-TUNING(MCTS+L+FT)
MCNCBenchmarks: InFig.10,wedepicttheperformancecomparisonamongMCTS+finetune
agent,ABC-RL,andpureMCTS.Remarkably,ABC-RLoutperformsMCTS+finetuneon11outof
12benchmarks,approachingMCTS+finetune’sperformanceonb9. Adetailedanalysisofcircuits
whereMCTS+finetuneperformsworsethanpureMCTS(i9, m4, pair, c880, max1024,
andc7552)revealsthatthesebelongto6outof8MCNCdesignswhereMCTS+learningperforms
suboptimallycomparedtopureMCTS.Thisobservationunderscoresthefactthatalthoughfinetuning
contributestoabettergeometricmeanoverMCTS+learning(23.3%over20.7%),itstillfallsshort
on6outof8benchmarks. Fortheremainingtwobenchmarks,alu4andapex4,MCTS+finetune
performs comparably to pure MCTS for alu4 and slightly better for apex4. Thus, ABC-RL
emergesasamoresuitablechoiceforscenarioswherefine-tuningisresource-intensive,yetweseek
aversatileagentcapableofappropriatelyguidingthesearchawayfromunfavorabletrajectories.
EPFLBenchmarks:InFig.11and12,wepresenttheperformancecomparisonwithMCTS+finetune.
Notably,fordesignsbaranddiv,MCTS+finetuneachievedequivalentADPasABC-RL,main-
taining the same iso-QoR speed-up compared to MCTS. These designs exhibited strong perfor-
mancewithbaselineMCTS+Learning,thusaligningwiththeexpectationoffavorableresultswith
MCTS+finetune. Onsquare,MCTS+finetunenearlymatchedtheADPreductionachievedbypure
MCTS.Thissuggeststhatfine-tuningcontributestopolicyimprovementfromthepre-trainedagent,
resultinginenhancedperformancecomparedtobaselineMCTS+Learning. Inthecaseofsqrt,
MCTS+finetuneapproachedtheperformanceofABC-RL.Ourfine-tuningexperimentsaffirmits
abilitytocorrectthemodelpolicy,althoughitrequiremoresamplestoconvergetowardsABC-RL
performance.
18
)%(
noitcudeRPublishedasaconferencepaperatICLR2024
12 70 .. 50 12 80 11 57 .. 05 11 24
15.0 16 12.5 10
12.5 14 10.0 8
10 57. ..0
05
11 02 257 ... 505 46 M
M
ABC
C
CT
T
-S
S R+ Lfinetune
8 2
20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100
Iterations Iterations Iterations Iterations
(a)alu4 (b)apex1 (c)apex2 (d)apex4
22 24 1122 6802 111 024 4455 0505
20 14 8 35
12 6 30
18 10 4 25
8 2 20
16 6 15
20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100
Iterations Iterations Iterations Iterations
(e)b9 (f)c880 (g)prom1 (h)i9
20 11 46 22.5 25
15 12 20.0 20
10 17.5
10 8 15.0 15
6
5 4 12.5 10
2 10.0
0 0 7.5 5
20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100
Iterations Iterations Iterations Iterations
(i)m4 (j)pair (k)max1024 (l)c7552
Figure 10: Area-delay product reduction (in %) compared to resyn2 on MCNC benchmarks. YELLOW:
MCTS+Finetune,BLUE:MCTSNetoetal.(2022),RED:ABC-RL
16 25.0
35 50 14 22.5
30 40 12 20.0 10 17.5
25 30 8 15.0
20 20 46 11 02 .. 05 M MC CT TS S+finetune
15 10 2 7.5 ABC-RL
20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100
Iterations Iterations Iterations Iterations
(a)bar (b)div (c)square (d)sqrt
Figure11:Area-delayproductreduction(in%)comparedtoresyn2onEPFLarithmeticbenchmarks.YELLOW:
MCTS+Finetune,BLUE:MCTSNetoetal.(2022),RED:ABC-RL
20 30 34 50 30
11 68 25 30 28
14 25 26
12 20 20 24
10 15 MCTS
8 15 10 22 MCTS+finetune
6 5 20 ABC-RL
4 10 0
20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100
Iterations Iterations Iterations Iterations
(a)cavlc (b)mem_ctrl (c)router (d)voter
Figure12: Area-delayproductreduction(in%)comparedtoresyn2onEPFLrandomcontrolbenchmarks.
YELLOW:MCTS+FT,BLUE:MCTSNetoetal.(2022),RED:ABC-RL
19
)%(
noitcudeR
)%(
noitcudeR
)%(
noitcudeR
)%(
noitcudeR
)%(
noitcudeRPublishedasaconferencepaperatICLR2024
D.4 ABC-RL:SIMILARITYSCORECOMPUTATIONANDNEARESTNEIGHBOURRETRIEVAL
Next,wereportnearestneighborretrievalperformanceofABC-RLwhichisakeymechanismin
settingαtotunepre-trainedagent’srecommendationduringMCTSsearch. Wereportsimilarity
scorewhichisthecosinedistancebetweentestAIGandnearestneighbourretrievedviasimilarity.
WealsoreportthetrainingcircuitwhichthetestAIGisclosestto. Basedonourvalidationdataset,
wesetT =100andδ =0.007.
th
Benchmark Designs SimilarityScore Nearestneighbour
alu4 0.000 alu2
apex1 0.001 apex3
apex2 0.002 alu2
apex4 0.000 prom2
c7552 0.006 max512
i9 0.003 prom2
MCNC
m4 0.001 max512
prom1 0.002 prom2
b9 0.005 c2670
c880 0.006 frg1
pair 0.010 m3
max1024 0.023 alu2
bar 0.002 prom2
div 0.001 log2
EPFLarith
square 0.012 alu2
sqrt 0.002 multiplier
cavlc 0.007 alu2
mem_ctrl 0.001 prom2
EPFLrandomcontrol
router 0.025 adder
voter 0.006 m3
Table6: Similarityscore(×10−2)ofnearestneighbourretrievedusingABC-RLfortestdesigns. Nearest
neighbourdenotesthetrainingdesignclosesttotest-timedesign
D.5 ABC-RL:ARCHITECTURALCHOICESFORRECIPEENCODER
ABC-RLusesBERT-basedrecipeencodertoextracttwomeaningfulinformation: 1)Contextual
relationship between current synthesis transformations and previous ones and 2) The synthesis
transformationswhichneedsmoreattentioncomparedtoothersdependingonitsposition. Fore.g.
rewriteoperationatthestartofasynthesisrecipetendtooptimizemorenumberofnodesinAIG
comparedtoitspositionlaterintherecipeYu(2020);Netoetal.(2022). Similarly,transformations
like balance are intended towards reducing delay of the design wheras transformations like
rewrite, refactor, resubareintendedtowardsareaoptimization.Thus,selectiveattention
andplacementoftheirpositionswithrespecttoothersynthesistransformationsneedstobelearned
whichmakesBERTanidealchoicetoencodesynthesisrecipe. Aspartofadditionalablationstudy,
weencodeoursynthesisrecipewithLSTMnetworkwithinputsequencelength(L=10)andapply
zero-paddingforrecipelengthlessthanL.
Table7:Area-delayreduction(in%).ABC-RL−BERTisABC-RLtrainedwithnaivesynthesisencoderinstead
ofBERT.MCTS+L+FTindicateMCTS+Learningwithonlinefine-tuning.
ADPreduction(in%)
Recipe
encoder MCNC EPFLarith EPFLrandom Geo.
mean
C1 C2 C3 C4 C5 C6 C7 C8 C9 C10 C11 C12 A1 A2 A3 A4 R1 R2 R3 R4
Naive 17.0 16.5 14.9 13.1 44.6 16.9 10.0 23.5 16.3 18.8 10.6 19.0 36.9 51.7 9.9 20.0 15.5 23.0 28.0 26.9 21.2
LSTM-based 19.5 17.8 14.9 13.3 44.5 17.8 10.3 23.5 18.8 20.1 10.6 19.6 36.9 53.4 10.9 22.4 16.8 24.3 32.3 28.7 22.6
BERT-based 19.9 19.6 16.8 15.0 46.9 19.1 12.1 24.3 21.3 21.1 13.6 21.6 36.9 56.2 14.0 23.8 19.8 30.2 38.9 30.0 25.3
20PublishedasaconferencepaperatICLR2024
D.6 RUNTIMEANALYSISOFABC-RLVERSUSSOTA
Wenowpresentwall-timecomparisonofABC-RLversusSOTAmethodsontestdesignsfor100
iterations.Wenotethatforallonlinesearchschemes,theruntimeisdominatedbythatthenumber
ofonlinesynthesisruns(typically9.5secondsperrun)asopposedtoinferencecostofthedeep
network (e.g. 11 milli-seconds for ABC-RL) . Thus, as observed in the table below, ABC-RL
runtimeiswithin1.5%ofMCTSandSA+Pred. Table8presentswall-timecomparisonofABC-RL
versusexistingSOTAmethodsfor100iterations. Overall,ABC-RLruntimeoverheadoverMCTS
andSA+Pred. (over100iterations)hasageometricmeanof1.51%and2.09%,respectively. Interms
ofwall-time,ABC-RLachieves3.75xgeo. meaniso-QoRspeed-up.
Runtime(inseconds) ABC-RLoverhead(in%) Iso-QoRspeed-up
Designs
w.r.t. MCTSwall-time
MCTS SA+Pred. ABC-RL w.r.t. MCTS w.r.t. SA+Pred.
alu4 35.6 35.3 36.0 1.12 1.98 1.89x
apex1 105.6 105.1 107.0 1.33 1.81 5.82x
apex2 20.2 20.1 20.5 1.49 1.99 1.80x
apex4 195.6 193.7 199.6 2.04 3.05 1.58x
c7552 91.4 91.2 93.2 1.97 2.19 1.16x
i9 40.2 40.3 41.2 2.49 2.23 1.30x
m4 37.1 37.3 37.9 1.89 1.41 3.11x
prom1 201.6 200.3 205.7 2.03 2.70 4.04x
b9 16.9 16.8 17.3 2.37 2.98 2.11x
c880 16.8 16.6 17.5 2.38 2.99 1.20x
pair 110.3 110.0 112.0 1.54 1.73 0.90x
max1024 82.6 82.0 84.9 2.78 3.50 1.77x
bar 192.4 191.2 196.5 2.13 2.77 3.59x
div 655.4 652.1 668.9 2.06 2.58 8.82x
square 398.6 395.0 403.1 1.12 2.05 4.17x
sqrt 448.5 444.2 455.5 1.56 2.54 8.21x
cavlc 56.4 56.1 57.2 1.42 1.96 4.95x
mem_ctrl 953.7 950.3 966.5 1.34 1.70 6.33x
router 44.2 43.9 44.8 1.36 2.05 3.08x
voter 312.5 311.0 317.9 1.73 2.22 5.57x
Geomean - - - 1.51 2.09 3.75x
Table8:Wall-timeoverheadofABC-RLoverSOTAmethodsfor100iterations(Budget:100synthesisruns).
Wereportiso-QoRwall-timespeed-upwithrespecttobaselineMCTSNetoetal.(2022).
D.7 ABC-RLPERFORMANCEONTRAININGANDVALIDATIONDESIGNS
Next, we present ABC-RL performance on training and validation circuits and compare it with
baselineMCTS.Fortrainingcircuits,ABC-RLsetsα=0indicatingthesearchwithaugmentedwith
fullrecommendationfrompre-trainedagent. Forvalidationcircuits,ABC-RLsetsαandperforms
searchwithtunedαrecommendationfrompre-trainedagent.
21PublishedasaconferencepaperatICLR2024
ADPreduction(in%)
Designs
MCTS ABC-RL
alu2 21.2 22.3
apex3 12.9 12.9
apex5 32.50 32.5
apex6 10.00 10.7
apex7 0.80 1.7
b2 20.75 22.1
c1355 34.80 35.4
c1908 14.05 15.9
c2670 7.50 10.7
c3540 20.30 22.8
c432 31.00 31.8
c499 12.80 12.8
c6288 0.28 0.6
frg1 25.80 25.8
frg2 46.20 47.1
i10 28.25 31.2
i7 37.50 37.7
i8 40.00 47.3
m3 20.10 25.0
max128 24.10 31.8
max512 14.00 16.8
prom2 18.10 19.8
seq 17.10 20.9
table3 15.95 16.1
table5 23.40 25.5
Geomean 15.39 17.84
Table9:Area-delayreductioncomparedtoresyn2onMCNCtrainingandvalidationcircuits.Wecompare
resultsofMCTSandABC-RLapproach.
ADPreduction(in%) ADPreduction(in%)
Designs Designs
MCTS ABC-RL MCTS ABC-RL
adder 18.63 18.63 arbiter 0.03 0.03
log2 9.09 11.51 ctrl 27.58 30.85
max 37.50 45.86 i2c 13.45 15.65
multiplier 9.90 12.68 int2float 8.10 8.1
sin 14.50 15.96 priority 77.53 77.5
Geomean 15.55 18.18 Geomean 5.87 6.19
Table10:Area-delayreductionoverresyn2onEPFLarithmetic(left)andrandomcontrol(right)trainingand
validationbenchmarksusingMCTSandABC-RL
22