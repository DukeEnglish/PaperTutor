RUMBoost: Gradient Boosted Random Utility Models
NicolasSalvade´a,∗,TimHillela
aDepartmentofCivil,EnvironmentalandGeomaticEngineering,UniversityCollegeLondon,GowerStreet,London,WC1E6BT,UnitedKingdom
Abstract
This paper introduces the RUMBoost model, a novel discrete choice modelling approach that combines the inter-
pretabilityandbehaviouralrobustnessofRandomUtilityModels(RUMs)withthegeneralisationandpredictiveabil-
ityofdeeplearningmethods. Weobtainthefullfunctionalformofnon-linearutilityspecificationsbyreplacingeach
linearparameterintheutilityfunctionsofaRUMwithanensembleofgradientboostedregressiontrees. Thisenables
piece-wiseconstantutilityvaluestobeimputedforallalternativesdirectlyfromthedataforanypossiblecombina-
tion of input variables. We introduce additional constraints on the ensembles to ensure three crucial features of the
utility specifications: (i) dependency of the utilities of each alternative on only the attributes of that alternative, (ii)
monotonicityofmarginalutilities,and(iii)anintrinsicallyinterpretablefunctionalform,wheretheexactresponseof
themodelisknownthroughouttheentireinputspace. Furthermore, weintroduceanoptimisation-basedsmoothing
techniquethatreplacesthepiece-wiseconstantutilityvaluesofalternativeattributeswithmonotonicpiece-wisecu-
bicsplinestoidentifynon-linearparameterswithdefinedgradient. WedemonstratethepotentialoftheRUMBoost
modelcomparedtovariousMLandRandomUtilitybenchmarkmodelsforrevealedpreferencemodechoicedatafrom
London. The results highlight both the great predictive performance and the direct interpretability of our proposed
approach. Furthermore,byanalysingthenon-linearutilityfunctions,wecanidentifycomplexbehavioursassociated
withdifferenttransportationmodeswhichwouldnothavebeenpossiblewithconventionalapproaches.Thesmoothed
attributeutilityfunctionsallowforthecalculationofvariousbehaviouralindicatorssuchastheValueofTime(VoT)
and marginal utilities. Finally, we demonstrate the flexibility of our methodology by showing how the RUMBoost
modelcanbeextendedtocomplexmodelspecifications,includingattributeinteractions,correlationwithinalternative
errorterms(NestedLogitmodel)andheterogeneitywithinthepopulation(MixedLogitmodel).
Keywords: DiscreteChoice,ModeChoice,MachineLearning,RandomUtility,EnsembleLearning
1. Introductionandliteraturereview
Discretechoicemodels(DCMs), basedonRandomUtilitytheory, havebeenusedextensivelytomodelchoices
overthelast50years(Ben-AkivaandLerman,1985;Train,2009),includingthechoiceoftravelmode. DCMshave
manydesirablequalities: mostcrucially,theirparametricformisdirectlyinterpretableandallowsfortheintegration
of expert knowledge consistent with behavioural theory. For example, with a DCM, it is possible to: (i) ensure
marginal utilities in the model are monotonic (e.g. that increasing the cost of an alternative will always decrease
its utility); (ii) restrict the utilities of each alternative to be dependent only on the attributes (or features) of that
alternative;and(iii)definearbitraryinteractionsofsocio-economiccharacteristicsandalternative-specificattributes.
These three points are essential to derive key behavioural indicators, such as elasticities and Value of Time (VoT),
used to inform transport policies and investment decisions. However, the parametric form of DCMs is not without
disadvantages. Crucially, the linear-in-parameters utility functions are relatively inflexible and must be specified in
advance by the modeller. As such, these models may fail to capture complex phenomena and non-linear effects in
human behaviour. Among DCMs, one of the best-known and most widely used models is the Multinomial Logit
model(MNL)derivedbyMcFaddenetal.(1973). Themodelassumesthattheerrortermisatype1extremevalue
∗Correspondingauthor
Emailaddresses:nicolas.salvade.22@ucl.ac.uk(NicolasSalvade´),tim.hillel@ucl.ac.uk(TimHillel)
PreprintsubmittedtoTransportationResearchPartC:EmergingTechnologies January23,2024
4202
naJ
22
]GL.sc[
1v45911.1042:viXra(EV)randomvariable,whichisindependentandidenticallydistributed(i.i.d.)acrossallalternativesandobservations.
This formulation results in a closed-form expression of the probability, allowing for easier estimation of the model
parameters. However,morecomplexmodelspecificationsexist,forexampletocapture: (i)differentdistributionsof
the error term; (ii) nesting (joint consideration) of alternatives; (iii) behavioural heterogeneity across individuals in
thepopulation;and(iv)behaviouralheterogeneityacrosssequentialchoices.
Therehavebeennumerousattemptstoapplymachinelearning(ML)probabilisticclassificationalgorithms,such
as neural networks and ensemble learning, to investigate choice behaviour. These models exhibit high predictive
performanceand,thankstotheirdata-drivennature,donotrequireanyutilityfunctionstobespecifiedinadvanceof
model estimation. However, they lack an underlying behavioural model and so it is not possible to guarantee con-
sistency of forecasts or derive behavioural indicators such as Value of Time (VoT) or willingness-to- pay from the
modelparameters. Initialapproachesforanalysingthesemodelsfromabehaviouralperspectiverelyonapproximat-
ingthepartialderivativesoftheoutputprobabilitiesofunconstrainedMLclassifiersinordertodefineelasticities(and
thereforemarginalratesofsubstitution/VoT)forvariablesofinterest. Wangetal.(2020)usethisapproachtoextract
economicindicatorsfromneuralnetworksformodechoiceproblems,whilstA´ngelMart´ın-Baosetal.(2023)extend
this to several other classification algorithms, including Gradient Boosting Decision Trees, the Random Forest, and
SupportVectorMachines. UnlikemarginalutilitiesfromaDCM,whichhaveaparametricfunctionalform,theprob-
abilityderivativesofMLclassifiersprovideonlyanumericestimateofthepointelasticitiesatobserveddatapoints.
Furthermore, as the underlying models are unconstrained, they exhibit several qualities that are inconsistent with
random utility theory, including: (i) non-monotonic elasticities, leading to unwanted behaviours, suchas a negative
VoT; and (ii) including all features for all alternatives uniformly, therefore violating the independence of irrelevant
alternatives assumption. As such, these techniques have seen limited real-world use and practitioners continue to
rely predominantly on parametric DCMs. That being said, the ability of ML models to capture complex non-linear
relationshipsaswellastheirimprovedpredictiveaccuracymakesthemanattractiveproposition.
Inresponsetotheselimitations,therehasbeenanemergenceofhybriddata-drivenutilitymodelsinrecentyears,
thatattempttocombinethebenefitsofMLandDCMs. Thesecanlargelybegroupedintotwodifferentapproaches:
1. addingadditionalconstraintstomachinelearningmodels(e.g. monotonicity,alternative-specificattributes,etc)
sothattheiroutputcanmimicDCMutilityvalues;and
2. usingdata-drivenapproachestoautomateorassistwithidentifyingsuitableparametricutilityfunctions.
TherehavebeenseveralstudieswhichattempttoincorporatekeycomponentsfromDCMsthroughmakingappro-
priateconstraintsonthestructureofMLmodels. ThisanalysisstemsfromthefactthatprobabilisticMLclassifiers,
suchasneuralnetworksandGBDTs,makeuseofthesamelogisticfunction(commonlyreferredtoassoftmaxinML)
astheMNLtogeneratechoiceprobabilitiesovereachalternative;replacingthelinear-in-parameterutilityfunctions
of the MNL with a complex network of neurons with non-linear activation functions (in the case of the neural net-
work)orensembleofregressiontrees(inthecaseofGBDTs). Assuch, withsuitableconstraintsonthemodel, the
pre-softmaxregressionvaluesforeachalternativecanbeconsideredaspseudo-utilities. Themostimportantofthese
constraintsisthatthepseudo-utilityofeachalternativeisafunctionofonlytheattributesofthatalternative(alongside
the socio-economic characteristics of the decision-maker), therefore obeying the independence of irrelevant alter-
natives assumption. Without this assumption, it is not possible to perform a utility-based behavioural analysis as
modifyingoneattributewouldaffecttheutilityvaluesofallalternatives. Wangetal.(2021)buildontheirpriorwork
ofmodellingchoiceswithDNNsbydefiningaseparatesub-networkperalternative,dependentonlyontheattributes
ofthatalternative,thusallowingformorerobustbehaviouralindicatorstobeextracted. Similarly,Mart´ın-Baosetal.
(2021)makeuseofkernelLogisticRegression(KLR)(wherethenon-linearityisachievedthroughtransformingthe
inputdatawithakernel)tolearnthedeterministicpartofeachalternativeutilitybyhavingonekernelperalternative.
Thisallowsforbehaviouralindicatorstobedefinedinthepseudo-utilityspace,ratherthantheprobabilityspace.Other
studiesinsteaduseaneuralnetworktocomplementaconventionalutilitymodel. Sifringeretal.(2020)useaCon-
volutionalNeuralNetwork(CNN)tolearnalinear-in-parameterutilityfunctionforthealternative-specificattributes
alongsideaDNNtogetsocio-economicinteractions, addingthemtothepseudo-utilityoutputinasecondstage. In
a similar approach, Wong and Farooq (2021) use a Residual Neural Network to add a non-linear, fully-connected
residualontopofthelinearpseudo-utilityateachlayerofthenetwork.
2However, these studies still lack of a key component of the interpretability and extrapolation ability of DCMs:
monotonicity of marginal pseudo-utilities. Whilst not relying on traditional deep learning models, there are several
studiesthatallowformonotonicityofmarginalutilitiesinMLmodels. KimandBansal(2023)usealatticenetwork
withinputandoutputcalibrationlayertoaccountfornon-linearitiesandpartialmonotonicity. KruegerandDaziano
(2022)replacethesumoperatorinatraditionalMixedLogitmodelwiththeChoquetintegral,accountingforattribute
interaction under monotonicity. Lastly, Aboutaleb (2022) take advantage of the sum of squares of polynomials to
learndata-drivenutilityfunctionsundermonotonicityandshapeconstraints.
Finally, some researchers retain the traditional structure of the utility function but add data-driven components
such as learned parameters or assisted utility specification. Han et al. (2022), with the TasteNet model, use a Deep
Neural Network to learn individual-specific parameters from socio-economic characteristics, constraining the sign
of the parameters with the appropriate activation function. Instead of replacing the deterministic utility by a data
driven model, Hillel et al. (2019) and Ortelli et al. (2021) use data-driven models to assist the modeller with utility
specification. Thefirstpapersuggestspotentialnon-linearinteractionsofattributesfromthegainofsplitpointsina
GBDTalgorithm. Thesecondstudyproposesanalgorithmforautomaticallyevaluatingmultiplemodelspecifications
bydrawingtheParetofrontier,whichisusefulinassessingthequalityofmodelspecificationundermultipleobjectives.
However,themodellerstillneedstopre-specifytheinputspaceandidentifynon-linearities.
The summary of state-of-the-art practices in hybrid utility models is in Table 1. The features that we consider
in the table are if the models: (i) include alternative-specific attributes; (ii) incorporate monotonicity constraints;
(iii)haveintrinsicallyinterpretablepseudo-utilities,asinDCMs;(iv)provideautomaticidentificationofnon-linearity
inthepseudo-utilities,i.ethecompletenon-lineartransformationsfromtheinputvariabletothepseudo-utilitiesare
observable; and (v) include non-i.i.d. error terms, therefore allowing for complex model specifications such as the
NestedLogitortheMixedLogitmodels; Inthispaper, weprovideanalgorithmtodirectlylearnthenon-linearfull
functionalformoftheutility,whileensuringalternative-specificattributesandmonotonicityonkeyattributes.
Table1:Summaryofstate-of-the-artpracticeinhybridutilitymodels.Across(X)meansthatthemodelimplementsthefeatureandatilde(∼)
meansthatthemodelpartlyincorporatesthefeature.Completeexplanationsonthefeaturesthatweconsiderforeachcolumnareinthetext.
Intrinsically
Alternative Automatic Non-i.i.d
Monotonicity interpretable
Papers specific idenficationof error
constraint utility
attributes non-linearities terms
function
Wangetal.(2021) X
Mart´ın-Baosetal.(2021) X
WongandFarooq(2021) X ∼ X
Sifringeretal.(2020) X ∼ X
Hanetal.(2022) X X X
KimandBansal(2023) X X ∼ ∼
KruegerandDaziano(2022) X X ∼ X
Aboutaleb(2022) X X X X
Ortellietal.(2021) X X X X
Whilsttheabovereviewevaluatestheliteratureinhybriddata-drivenutilitymodelling,itisalsoworthconsidering
themoregeneralfieldofeXplainableArtificialIntelligence(XAI),whereresearchersaimtoaddexplanationstothe
predictionsofgeneralpurposeMLalgorithms. ThemostwellknownXAItechniques,inuseacrossmultiplefields,
are: LIME (Ribeiro et al., 2016) and SHAP (Lundberg and Lee, 2017). Several researchers use them to provide
interpretability of ML classifiers in a mode choice context (Tamim Kashifi et al., 2022; Ren et al., 2023; Dahmen
etal.,2023;A´ngelMart´ın-Baosetal.,2023). Howeverthesemethodsarelocalapproximationsofthemodelrelying
ondatapointsand,therefore,havenoguaranteeofvalidityoutsideoftheinputspace. Ontheotherhand,Explainable
3BoostingMachine(EBM)fromCaruanaetal.(2015)isanexampleofinterpretableGradientBoostingDecisionTrees
algorithm. Theyachieveinterpretabilitybygrowingshallowregressiontrees. However,themodeldoesnotallowthe
modellertospecifyalternative-specificvariablesandmonotonicityisonlyappliedasapost-processingtoolonfeatures
withoutinteraction.Inaddition,afeaturecaninteractwithseveralotherfeaturessimultaneously,whichisinconsistent
withrandomutilitytheory. WealsoobservethatthemajorityofresearchonMLmodelsforchoicemodellingfocuses
onneuralnetworks.Althoughneuralnetworkshavenumerousapplicationsinmanyfieldsandshowgreatresults,other
algorithmsexhibitbetterperformanceonclassificationtaskssuchaschoicemodelling. Inparticular,GBDTmodels,
including XGBoost (Chen and Guestrin, 2016) and LightGBM (Ke et al., 2017) have demonstrated state-of-the-art
predictiveperformanceforavarietyofclassificationtasks(Hilleletal.,2018;A´ngelMart´ın-Baosetal.,2023). These
models,despitelackingofbehaviourinterpretability,consistencyandrobustness(likeotherMLmodels)showgreat
interpretabilitypotentialduetotheiradditivenature. Furthermore,existingMLapproacheshavecontinuedtorelyon
the logit formulation, with i.i.d. EV error terms. As such, they cannot capture complex behavioural phenomenons
(e.g. paneleffects,dynamicorsequentialchoicesituations,nestingofalternatives,etc).
Inthispaper,wepresentRandomUtilityModelswithBoosting(RUMBoost)whichaimstocombinethepredictive
powerofGBDTswiththeinterpretabilityandbehaviouralconsistencyofDCMs.Atahighlevel,RUMBoostreplaces
eachparameterintheutilityspecificationsoftraditionalDCMswithanensembleofregressiontrees,allowingfornon-
linearparameterstobeextracteddirectlyfromdata. Algorithmically,RUMBoostconsistsoftwoparts: (i)Gradient
BoostedUtilityValues(GBUV),whereensemblesofregressiontreesareusedtoimputepiece-wiseconstantvalues
for each parameter in each pseudo-utility specification; and (ii) Piece-wise Cubic Utility Functions (PCUF), where
monotonic piece-wise cubic splines are optimised to fit the GBUV outputs, to allow for a defined gradient for each
parameterwhereelasticitiesareneeded.
The GBUV output effectively defines piece-wise constant pseudo-utility specifications that are consistent with
a DCM, with the ability to constrain the model to have: (i) pseudo-utility functions depending only on their al-
ternative-specific attributes and socio-economic characteristics; (ii) monotonicity of marginal pseudo-utilities; and
(iii) arbitrary interactions between variables as defined by the modeller. Furthermore, the GBUV output is intrinsi-
callyinterpretable,withtheexactfunctionalformobservabletothemodelleroverthefullinputspaceofeachvariable.
Hereafter,wenolongerdistinguishbetweenutilityandpseudo-utilityaswebelievethat,withtheconstraintsafore-
mentionned,theGBUVoutputcanbeinterpretedasutilityvalues.
AstheGBUVoutputispiece-wiseconstant,itdoesnothaveadefinedgradient,withthegradientbeingeitherzero
orinfiniteatanygivenpoint. Assuch,behaviouralindicatorssuchaselasticitiesandVoTcannotbeextracteddirectly.
ThisisaddressedthroughthesecondelementofRUMBoost,PCUF.PCUFonlyneedstobeappliedforvariablesfor
whichelasticitiesareneeded(typicallythealternative-specificattributes),whichoptimisesasetofpiece-wisecubic
splines(polynomials)toobtainadefinedgradientoverthefullrangeoftheinputspace. Thisallowsthemodellerto
computebehaviouralindicatorssuchastheValueofTime(VoT).
Theflexibilityofourapproachallowsforestimatingcomplexmodelstructureswithcorrelatederrortermssuchas
theNestedLogitmodeloraccountingforpopulationheterogeneitysuchastheMixedLogitmodel. Themodelisim-
plementedwithPythonandthecodeisfreelyavailableonGitHub(https://github.com/NicoSlvd/RUMBoost).
Therestofthepaperisorganisedasfollows:Section2presentsthemethodologyofthispaper,theresultsofacase
study are presented in Section 3, Section 4 provides some model extensions, and Section 5 concludes and suggests
areasofimprovementforfuturework.
2. Methodology
2.1. Theoreticalbackground
We summarise here key theory of both RUMs and GBDTs under a unified notation in order to help the reader
understand our approach. For a more detailed background on each approach, we direct the reader to the following
texts: Ben-AkivaandLerman(1985);Train(2009);Friedman(2001);ChenandGuestrin(2016);Keetal.(2017).
2.1.1. RUMs
DiscreteChoiceModelsbasedonRandomUtilitytheoryassumethatindividualschoosethealternativethatmax-
imisestheirutility,alatentmeasureoftheattractivityofanalternative. Theutilityisusuallydefinedasfollows:
4U =V +ε (1)
in in in
whereV istheobservableutilityandε theerrortermcapturingunobservedvariablesforalternativeiandindividual
in in
n. InaDCM,V isamanuallyspecifiedlinear-in-parametersfunctionofthevariables,suchthat:
in
(cid:88)K
V =β + β x (2)
in i,0 i,k ikn
k
where K + 1 parameters associated with K variables x have to be estimated from the data. For simplicity, we
ikn
considerheretheMNLmodel,wheretheerrortermisassumedtobei.i.d,followinganextremevaluedistributionof
type1(Gumbel). Wewilllaterdemonstratemorecomplexmodelspecificationswheretheerrortermsareallowedto
becorrelatedacrossalternatives. FortheMNL,inaclassificationproblemwith J classes,theprobabilitiesaregiven
bythemulti-classlogisticfunction:
eVin
P = (3)
in (cid:80)JeVjn
j
ThequalitiesoftheMNLmodelaredirectlyderivedfromtheparametricformofV .Inparticular,itisintrinsically
in
interpretablebecausetherearenovariableinteractions.
In addition, the modeller can easily incorporate domain knowledge through alternative-specific attributes and
monotonicityofmarginalutilities.Foramulti-classclassificationproblemwithJclasses,andKexplanatoryvariables,
wedefine:
x = a ∪s (4)
i i
wherex isthesetofvariablesforalternativeiand
i
(i) |x|≤ K,i.e. thecardinalityofx shouldbesmallerorequalthanthenumberofattributesK
i i
(ii) a isthesetofalternative-specificattributesforalternativei,suchthata ∈ x, ∀ianda ∩a =∅, ∀i(cid:44) j
i i i j
(iii) sisthesetofsocio-economiccharacteristicssuchthats∈ xands∩a =∅, ∀i
i
Discretechoicemodelsalsoguaranteethemonotonicityofmarginalutilities. Apositive(resp. negative)mono-
tonicrelationshipimpliesthatanincreasingattribute x willincrease(resp. decrease)thevalueoftheutilityfunction
k
V . The signs of parameters in DCMs allow the modeller to easily verify the monotonicity of an attribute and, if
in
needed,toconstrainitwithparameterbounds.
Finally,thegradientisalwaysdefinedandeasytocompute,whichallowsforthederivationofbehaviouralindi-
cators. Among them, the Value of Time (VoT) is an important measure of the perceived cost of travelling time by
individuals. TheVoTofanalternativeiisdefinedasfollows:
∂V 1 β
VoT = i · = i,time (5)
i ∂x
i,time
∂Vi β
i,cost
∂xi,cost
whereβ andβ aretheparametersassociatedwiththetimex andcostx attributesoftheutilityfunction
i,time i,cost i,time i,cost
V.
i
2.1.2. Gradientboosting
ThefollowingnotationcombinesthatofFriedman(2001)andChenandGuestrin(2016),summarisingtheGBDT
algorithm commonly used in popular libraries such as XGBoost and LightGBM. Deep learning models, including
DNNsandGBDTusethesamemulti-classlogisticfunction(typicallyreferredtoasthesoftmaxfunction)astheMNL,
whichmeansthattheyarebasedonthesameassumptionsontheerrorterm.Inotherwords,thesemodelsfirstestimate
a latent regression value for each class which is then used to generate choice probabilities. The advantage of these
modelsoveralinear-in-parameterRUMisthattheyareinherentlynon-linear,andsocancapturecomplexrelationships
5betweentheinputfeaturesandthechoiceprobabilities. However, unlikeinRUMs, theseregressionmodels: (i)are
afunctionofallexplanatoryvariables/features(i.e. attributesofeachalternative);(ii)donotcontainanyconstraints
to represent behavioural assumptions; (iii) allow for complex feature interactions that cannot be constrained (or in
manycasesevenobserved)bythemodeller;and(iv)haveanunknownfunctionalformthatisnotobservablebythe
modeller. Formally, theMLpredictivefunction F(x)isusedtoreplacethedeterministicpartoftheutilityfunction
i
V(x). InGBDTalgorithms,thepredictivefunctionisanadditivefunctionoftheform:
i
(cid:88)
V(x)= F (x)= f (x) (6)
i i,m i,m
m
where f (x) is the output of a single regression tree. At each boosting round m, assuming J classes, J regression
i,m
treesareinducedtodirectlyminimisethefollowingobjective:
(cid:88)N (cid:88)J
L= ℓ(y ,yˆ + f (x )) (7)
in in,m−1 i,m in
n=1 i=1
where:
• N isthenumberofobservationsinthedataset;
• y =1ifthechoicei oftheindividualnisi,and0otherwise;and
in n
• yˆ isthepredictedprobabilityofclassiandobservationnwithattributesx atiterationm−1,i.e. yˆ =
in,m−1 in in,m−1
F (x ).
i,m−1 in
Thisoptimisationproblemhasnoclosed-formsolution. ItisgenerallysolvedbytakingtheTaylorsecond-order
approximationofthelossfunction:
(cid:88)N (cid:88)J
1
L= ℓ(y ,yˆ )+g f (x )+ h f (x )2 (8)
in in,m−1 in i,m in in i,m in
2
n=1 i=1
whereg = ∂ℓ(y ,yˆ )/∂yˆ andh = ∂2ℓ(y ,yˆ )/∂2yˆ arethefirstandsecondderivativeoftheloss
in in in,m−1 in,m−1 in in in,m−1 in,m−1
functionwithrespecttothepredictionforobservationnandclassi.Here,theHessianmatrixisreplacedbyadiagonal
approximationfollowingFriedman(2001),whichmeansthateachclasspredictionisassumedtobeindependent.This
approximation is crucial for computational efficiency and allows for efficient optimisation of the loss function. In
addition,wecanignoretheconstanttermℓ(y ,yˆ )fortheminimisationtask.
in in,m−1
Assumingthatwehave Lterminalnodes(i.e. thebottomnodesoftheregressiontree)resultingin Lregions L
im
foratreeofclassiatiterationm,eachobservationwillbelonguniquelytooneoftheregionL ,suchthatEquation
im
8becomes:
(cid:88)J (cid:88) (cid:88) 1 (cid:88)
L= ( g )γ + ( h )γ2 (9)
in l,im 2 in l,im
i=1 l∈Lim n∈l n∈l
with γ being the leaf value at region l for class i. Therefore, by taking the first derivative with respect to γ and
l,im
settingitto0,weobtaintheoptimalleafvalue:
(cid:80)
( g )
γ
l,im
=− ((cid:80)n∈l hin
)
(10)
n∈l in
Bysubstituting Equation10 into9, andassuminga splitpoint thatwould leadto aleft L andright L
left,im right,im
regions,weobtainthelossreductionforsplitting:
L
split
=
1 2(cid:88)J   (cid:88) ((cid:80) (cid:80)n∈lleftg hin)2
+
(cid:88) ((cid:80) (cid:80)n∈lrightg hin)2
−
(cid:88) ((cid:80) (cid:80)n∈lg hin)2 
(11)
i=1 lleft∈Lleft,im n∈lleft in lright∈Lright,im n∈lright in l∈Lim n∈l in
6Therefore,wecanchoosethesplitpointthatmaximisesthelossreduction. Foraclassificationproblem,theloss
functionisusuallydefinedas:
ℓ=y ·log(p(x )), (12)
in i in
wherep(x )=yˆ .Thisequationistypicallyreferredtoasthecross-entropylossinMLcontexts,thoughisequivalent
i in in
to the log likelihood in RUMs. As in the MNL, we can obtain the predicted probabilities of each class using the
softmaxfunction:
eVi(xin)
p(x )= , (13)
i in (cid:80)J
j=1eVj(xjn)
PluggingEquations12and13intoEquation10gives:
(cid:80)
γ l,im = J− J 1 (cid:80) xin∈ pRl, (im xy in )(− y p −i(x pin () x )) (14)
xin∈Rl,im i in in i in
where J−1 isafactoraccountingforredundancy.
J
Theutilitiesofeachensembleforeachindividualcanthenbeupdatedateachiteration:
(cid:88)L
V (x )=V (x )+ γ ·1(x ∈ L ) (15)
i,m in i,m−1 in l,im in im
l=1
where1(·)istheindicatorfunction,i.e. equals1iftheargumentistrue,0otherwise.
Monotonicity is easily implemented in regression trees. If we assume a positive (resp. negative) monotonic
relationship for an explanatory variable, a split point that partitions the data in two such that x < x is only
left right
consideredif:
γ <γ (resp.γ >γ ) (16)
left right left right
Notethat,inordertosatisfythemonotonicityoverthefulltree,thesubsequentleftandrightleafvaluesarebounded
by
(γleft+γright)
. Thesideofthetreeandthenature(positiveornegative)oftheconstraintdetermineifitisaloweror
2
upperbound.
2.2. TheRUMBoostmodel
WefirstexplainherehowweadaptthegeneralGBDTmodeltooutputGradientBoostedUtilityValues(GBUV)
toemulateparametricRUMs. WethenpresentthePiecewise-CubicUtilityFunctions(PCUF)algorithm,thatoutputs
smoothedmonotonicnon-linearparameters.
2.2.1. GradientBoostedUtilityValues(GBUV)
InRUMBoost-GBUV,wereplaceeachparameterintheutilityfunctionsofaRUMwithanensembleofregression
trees,wheretheleavesintheregressiontreesrepresentthepartialutilitycontributionforthecorrespondingvalueof
thatvariable. Thesecanthenbeaddedovereachtreeintheensembletofindthecontributionofeachvariabletothe
utility.Theoverallutilityforeachalternativecanthereforebefoundbysummingtheensemblesforeachvariableover
allvariablesintheutilityfunction. ForKparametersappliedtoKvariables,wehave:
(cid:88)Ki (cid:88)Mik
V =ASC + f (x ) (17)
in i imk ink
k m
where ASC is an Alternative-Specific Constant for alternative i and M is the number of regression trees in the
i ik
ensembleforparameterkforalternativei. Inotherwords,byorderingthesplitpointsforeachvariableinascending
order,andaddingleafvaluesfortheappropriatevalues,wecaninterpretthesumasrawutilityvaluesforeachvariable.
This allows us to impute the full functional form of the ”non-linear parameters” with piece-wise constants. In this
form, the utility function is non-linear (and non-continuous) and known over the full input space. Probabilities for
7each alternative can then be calculated with the appropriate transformation (e.g. softmax/logistic function for the
MNLmodel).
Forsimplicityofnotation,theaboveformulationshowseachparameterbeingappliedtoasinglevariable. How-
ever,itispossibletodefinearbitraryfeatureinteractionsbyallowingtheregressiontreesineachensembletospliton
multiplevariablese.g. foraninteractionoftwovariablesk andk
wewouldhave(cid:80)Mik
f (x ,x ). Thisallows
1 2 m imk ink1 ink2
themodellertospecifyanydesiredfeatureinteractionsasrequired.
WemodifythestandardGBDTboostingalgorithminordertoidentifyoptimalRUMBoost-GBUVensemblesfor
each variable (or combination of variables). For a J-class problem, we introduce J trees at each boosting iteration
duringmodeltraining,onefortheutilityfunctionofeachalternative. Ateachiteration,themodelcomputeschoice
probabilities for the previous estimates of the utility functions, derives the gradient and hessian of the log-loss, and
uses them for boosting the next set of J trees. Each regression tree is added to the ensemble for a single variable,
automaticallyselectedbythealgorithminanexhaustivesearch(forcomputationalefficiencythesearchistypically
limitedtoafinitenumberofpossiblesplitpoints)inordertominimisethelossfunction.1
Oncethemodelhasbeenfullytrained,wecanextractnormalisedASCs. WedefinethenonnormalisedASCsas:
(cid:88)Ki (cid:88)Mik
ASC′ = f (0) (18)
i imk
k m
SinceinRUMonlythedifferenceinutilitiesmatter,oneoftheASCscanbenormalisedto0. Assumingthatthe
ASCofthealternative jisnormalised,weobtainthefollowingsetofASCs:
ASC =ASC′−ASC′,∀i=1,...,J (19)
i i j
WebuildouralgorithmontopofLightGBM(Keetal.,2017),suchthateachensembleisaLightGBMBoosterob-
ject. Therefore,wecanmakeuseofthealreadyimplementedmonotonicityconstraintfeaturetoimposemonotonicity
onutilitiesasrequired.
TheRUMBoost-GBUVmodeltrainingalgorithmisdescribedformallyinAlgorithm1. Notethatthealgorithmis
independentoftheassumptionontheerrorterms. ThisallowsRUMBoost-GBUVtobeusedtoemulateanyarbitrary
model formulation for which the gradient and Hessian of the loss function is defined, including Nested Logit and
Mixed Logit models. Furthermore, as the code has been re-implemented from scratch, any ML regressor that can
satisfytheconstraintsdescribedabovecanbeusedinplaceofGBDT.
Algorithm1: RUMBoost-GBUV
x = a +s, ∀i=1,...,J
i i
Positivemonotonicsetofattributesx+ ⊆ x, ∀i=1,...,J
i i
Negativemonotonicsetofattributesx− ⊆ x, ∀i=1,...,J
i i
x+∩x− =∅, ∀i=1,...,J
i i
Specifyvariablesinteractions
V (x )=0, ∀i=1,...,J
in in
form=1toMdo
Compute p (x )
in in
fori=1toJdo
Computethefirstandsecondderivativesofthelossfunction
Choosethesplitpointthatmaximisesthelossreductionofanyvariablek
Addtheregressiontree f (x )toitscorrespondingensemble
imk ink
endfor
endfor
1NotethatforthepracticalimplementationofRUMBoost-GBUV,thereisactuallyasingleensembleperalternativeutilityfunction,witheach
treeintheensemblerestrictedtoonlysplitonthecorrespondingvariable(s).Thetreesforeachvariablearethengroupedintoaseparateensemble
oncethemodelhasbeenfullytrained.However,thisisequivalenttohavingaseparateensembleforeachvariableduringmodeltraining,whichwe
believetobeamoreintuitiveinterpretationoftheunderlyingfunctionality,andsopresentthealgorithminthatwayhere.
82.2.2. Piece-wiseCubicUtilityFunction(PCUF)
The GBUV ensembles for each parameter in Section 2.2.1 are non-continuous, and so have a gradient of either
zeroorinfiniteatanypoint.However,manybehaviouralindicatorsrequiretheutilityfunctiontohavedefinedgradient
tobecomputed. Therefore, weinterpolatetheutilityvaluesintoasmoothfunctionusingpiece-wisecubicHermite
splines. Using splines ensure that the underlying cubic polynomials have equal values and derivatives at the knots
(i.e. boundary points). Given the number of knots and their positions, only the derivative at each knot needs to
be computed, making Hermite splines very attractive for efficient computations. Using the approach introduced by
Fritsch and Carlson (1980), it is possible to guarantee monotonic splines, where the gradient is always negative or
positive (or zero) as required. The interpolation must satisfy two conflicting objectives: (i) fitting the data as well
aspossibletomaintaingoodpredictivepoweronout-of-sampledata; and(ii)beingassmoothaspossibletoobtain
relevantbehaviouralindicators.
Thefirstobjectivefavoursahighernumberofknots,whilethesecondaimsforalowernumbersothatthederiva-
tive is well defined. A natural objective function to capture the trade-off of both these objectives is the Bayesian
InformationCriterion(BIC),whichtakesthefollowingform:
BIC =−2N·L+df ·ln(N) (20)
whereListhelossfunctiondescribedinEquation12,df isthedegreeoffreedomofthemodel,andN isthenumber
ofobservations.Thefirstpartofthefunctionaimsforabetterfitofthedata,whilethesecondpartpenalisesthemodel
foritscomplexity.
RUMBoost-PCUF,therefore,hastwoparameterstotune: (i)thenumberofknots;and(ii)theirpositions. Given
asequenceofQ+1knotsa =t <t <...<t =b foranattributekwherea andb arethedomainwherethat
k 0,k 1,k Q,k k k k
attributeisdefined,theoptimalpositionsandnumbersofknotsaredeterminedbythefollowingoptimisationproblem:
min −2N·L+df ·ln(N)
tq,k
s.t. t q+1,k−t
q,k
>0 ∀q=0,...,Q,∀k
(21)
t =a ∀k
0,k k
t =b ∀k
Q,k k
Giventhenumberofknots,thereisanoptimalpositionofknotsthatminimisesthelossfunction. Therefore,the
twohyperparameterscanbetunedsequentially: thenumberofknotsisselectedfirstandtheiroptimalpositionsare
foundwithaconstrainedoptimisationsolverafterwards. However,thiscanbeacomplexoptimisationproblemifthe
numberofattributesishigh, andithasbeenshownintheliteraturethatitisacceptabletofixthepositionsofknots
over the range of the attribute (e.g., quantile) and optimise only their numbers (Hastie and Tibshirani, 1990). The
RUMBoost-PCUFalgorithmissummarisedinAlgorithm2.
Algorithm2: RUMBoost-PCUF
x = a +s, ∀i=1,...,J
i i
GradientboostedutilityvaluesV ∀i=1,...,J,k=1,...,K
ik
NumberofiterationsforhyperparametersearchN
iter
forn=1toN do
iter
fori=1toJdo
fork=1toK do
if x ∈ a then
k i
ChooseanumberofknotsQ
k
Definetheinitialpositionofknotst = Q -quantile(x ,q) ∀q=0,...,Q
q,k k k k
endif
endfor
endfor
OptimisetheknotspositiontominimisetheBIC
endfor
92.3. Codeandimplementation
WeimplementthemodelinPython,makinguseofthelibraryLightGBMfortheutilityregressionensembles(Ke
etal.,2017). Ourimplementationcreatesaregressionensembleforeachalternativeinwhichtheinputattributes(or
features) canbe specifiedindependently. We setup thelogistic/softmax functionso thatin each roundof boosting,
the trees (and split points in each tree) are selected to directly minimise the log-loss (cross-entropy loss), therefore
emulatingmaximumlikelihoodestimation(MLE).Separateensemblesforeachparameterareobtainedbyrestricting
thepossiblesetoffeatureinteractionsineachtree. Thecodemakesuseoftheexistingmonotonicconstraintsfunc-
tionalitytoguaranteemonotonicityofthemarginalutilitiesasrequired. Wehavethereforeimplementedaninterface
whichallowsthemodellertospecify:
• whichattributesshouldbeincludedineachutilityfunction
• controlattributeinteractions
• specifywhichattributesshouldhavemonotonicmarginalutilities.
Earlystoppingisusedtodeterminetheappropriatenumberoftreesineachensemble, withboostingterminated
oncethelog-lossdoesnotimproveonout-of-sampledataforagivennumberofiterations(e.g. 100). Furthermore,
we have written a script that converts model files from the popular choice modelling software Biogeme (Bierlaire,
2023) to be used directly within RUMBoost, therefore allowing modellers to easily replicate any MNL model in
RUMBoost. Thisconversionworksusingtheutilityspecificationtodefinealternative-specificattributesandcontrol
attributeinteractions,andusingboundsontheparameterstodefinemonotonicconstraint. Thecodeisfreelyavailable
onGithub(https://github.com/NicoSlvd/RUMBoost)
3. Detailedcasestudy
Weapplyourmethodologyonacasestudy,wherewebenchmarkRUMBoostagainstaMNLmodelandthreeML
classifiers: NeuralNetwork(NN),DeepNeuralNetwork(DNN)andLightGBM.Thesemodelsarere-implemented
fromA´ngelMart´ın-Baosetal.(2023)2. Inaddition,weshowthenon-linearutilityfunctionsandcomputebehavioural
indicators. Toensureafaircomparison,RUMBoostisbuiltusingthesamemodelspecificationastheMNLmodel.
3.1. Casestudyspecifications
We use the London Passenger Mode Choice (LPMC) (Hillel et al., 2018) dataset for our case study, a publicly
availabledatasetprovidingdetailsofmorethan80000tripsinLondon,alongsidetheirassociatedmodechoicedeci-
sions.ItisanaugmentedversionoftheLondonTravelDemandSurvey(LTDS)tripdiarydataset,toincludethetravel
time and cost of alternatives. The dataset contains observations from 17615 households over a three-year period,
andtherearefourpossiblealternatives: walking, cycling, publictransportanddriving. TheMNLmodel, alsoused
tocreateRUMBoost,isa62-parametermodelwithalternative-specificconstants(ASC).WhenestimatingtheMNL
model,wenormalisetheASC,thegenericattributesandthesocio-economiccharacteristicsofthewalkingalternative
tozero. ThemodelspecificationissummarisedinTable2. Lastly,theRUMBoostandMLmodelsaretrainedonthe
firsttwoyearsofthedatasetwitha5-foldcrossvalidationschemedesignedinsuchawaythattripsperformedbythe
samehouseholdmemberscannotbeindifferentfolds,toavoiddataleakage. Weincludeanearlystoppingcriterionof
100iterations,i.e. westopthetrainingiftheperformanceonthevalidationsetisnotimprovingduring100iterations.
FortheMLclassifiers,wealsoincludeahyperparametersearch. ThissearchisdonewiththepythonlibraryHyperopt
(Bergstraetal.,2013),andthesearchspaceandresultsaresummarisedinAppendix A.
3.2. RUMBoostmodelspecification
The MNL model is directly used to specify the constraints of the RUMBoost model. The alternative-specific
attributesconstraintisdirectlysatisfiedbytheMNLutilityspecification.Interactionsbetweenattributesarerestricted,
suchthateachtreecorrespondstoasingleparameter. Finally,monotonicityconstraintsareobtainedfromthebounds
10Table2:VariablesusedintheLPMCRUMBoostandMNLmodels.FortheMNLestimation,thesocio-economiccharacteristicsandgeneric
attributesarenormalisedto0forthewalkingalternative.PurposeandFueltypearedummyvariableswhereonecategoryisnormalised.The
constantsarenotincludedintheRUMBoosttraining,butarereconstructedafterwards,followingEquation19.
Walking Cycling PublicTransport Driving
Alternative-specificattributes
Constant ✓ ✓ ✓
Traveltime ✓ ✓ ✓ ✓
Accesstime ✓
Transfertime ✓
Waitingtime ✓
Num. ofPTchanges ✓
Cost ✓ ✓
Congestioncharge ✓
Socio-economiccharacteristicsandgenericattributes
Straight-linedistance ✓ ✓ ✓ ✓
Startingtime ✓ ✓ ✓ ✓
Dayoftheweek ✓ ✓ ✓ ✓
Gender ✓ ✓ ✓ ✓
Age ✓ ✓ ✓ ✓
Drivinglicense ✓ ✓ ✓ ✓
Carownership ✓ ✓ ✓ ✓
Purpose: home-basedwork ✓ ✓ ✓ ✓
Purpose: home-basededucation ✓ ✓ ✓ ✓
Purpose: home-basedother ✓ ✓ ✓ ✓
Purpose: employersbusiness ✓ ✓ ✓ ✓
Purpose: non-home-basedother ✓ ✓ ✓ ✓
Fueltype: diesel ✓ ✓ ✓ ✓
Fueltype: hybrid ✓ ✓ ✓ ✓
Fueltype: petrol ✓ ✓ ✓ ✓
Fueltype: average ✓ ✓ ✓ ✓
thatwouldbeappliedtotheMNLbetaparameters(seeTable3),andsoappliednegativelyontraveltime,headway,
costanddistance,andpositivelyoncarownershipanddrivinglicense(whenapplicable).
One big advantage of RUMBoost over the more flexible GBDT model is that the additional constraints help to
regularise the model and therefore has a lower propensity to overfit compared to the unconstrained GBDT model.
Wefindthatthemodellingresultsarelessdependentonhyperparametervalues,includingregularisationparameters.
Thus, we use the LightGBM default parameters except for the learning rate, the maximum depth of trees and the
numberoftrees. Thenumberofboostingroundsisobtainedwiththecross-validationearlystoppingcriterion,where
we average the best number of trees of each fold to obtain the final number of trees. As such, we make use of the
followingparametersforeachregressionensemble:
• learningrate: 0.1
• maxdepth: 1(followingattributeinteractionconstraints)
• minimumdataandsumofhessianinleaf: 20(default)and0.001(default)
• maximumnumberofbinsandminimumofdatainbins: 255(default)and3(default)
2Thecodeisfreelyavailableathttps://github.com/JoseAngelMartinB/prediction-behavioural-analysis-ml-travel-mode-choice
11Table3:Attributesconstrainedtomonotonicity.Traveltime,costanddistancearemonotonicnegative,i.e.anincreaseoftheseattributeswill
decreasetheutilityfunctionvalue.Carownershipanddrivinglicensearepositivemonotonicandappliedonlyonforthedrivingalternative.
Monotonicnegative Traveltime,cost,distance
Monotonicpositive Carownership*,drivinglicense*
*Onlyforthedrivingalternative
• monotoneconstraintmethod: advanced
• numberofboostingrounds(trees): 1300
3.3. ComparisonswithotherMLmodelsandMNL
TheresultsoftheRUMBoostmodelarebenchmarkedagainsttheMNLandMLmodelspresentedinSection3.1.
Wecomparethemodelswiththeircross-entropyloss(CLE)onthetestset(lowerisbetter)andtheircomputational
timepercross-validationiteration. TheresultsareshowninTable4.
Table4:BenchmarkofClassificationonLPMCDataset.ThemodelsarecomparedwiththeirCEL(negativeCross-EntropyLoss,lowerthe
better)onthetestsetandtheircomputationaltimeforoneCViteration.ThetrainingresultsofRUMBoost-PCUFaretheresultsofthe
optimisationproblemdescribedinSection2.2.2.
LPMC
Models Metrics
5foldCV Holdouttestset
CEL 0.6913 0.7085
MNL
Comp. Time[s] 242.14 -
CEL 0.6516 0.6667
NN
Comp. Time[s] 7.85 -
CEL 0.6613 0.6735
DNN
Comp. Time[s] 3.89 -
CEL 0.6381 0.6537
LightGBM
Comp. Time[s] 4.64 -
CEL 0.6570 0.6737
RUMBoost-GBUV
Comp. Time[s] 6.48 -
CEL 0.6479* 0.6730
RUMBoost-PCUF
Comp. Time[s] 712.48* -
*NotwithCV
Overall,theRUMBoostmodeloutperformstheMNLmodelonbothtrainingandtestingvalidations,whilststill
ensuringadirectlyinterpretablefunctionalform. WhilstthereisamarginalperformancesacrificeoftheRUMBoost
modelscomparedtotheunconstrainedGBDTandNNmodels,itisimportanttonotethatthefunctionalformofthe
RUMBoostutilitiesisdirectlyinterpretableoverthefullinputspace,allowingforguaranteesofbehaviouralconsis-
tency of forecasts, and in the case of RUMBoost-PCUF, extraction of behavioural indicators, which is not possible
for the GBDT and NN models. Note that further enhancements of the RUMBoost model, including the functional
effects model (see Section 4.3) further narrow this gap. Interestingly, the loss of information due to smoothing is
minimal, andtheCElossevenimprovesontheLPMCdataset, evenwithanobjectivefunctionpenalisingcomplex
models. Therefore,wededucethatthepiece-wisesplinesactasfurtherregularisationoftheRUMBoostmodelwhen
thereisasufficientamountofdataandsplittingpoints. Computationally,theGBUValgorithmshowssimilarresults
thantheMLclassifiers,whichismuchbetterthantheMNLmodel. ThePCUFalgorithmontheotherhandhasabig
computationaltime,illustratingthecomplexityoftheoptimisationproblem.
123.4. Gradientboostedutilityvalues
The primary advantage of using RUMBoost over other unconstrained ML algorithms is that non-linear utility
functionscanbeobserveddirectly. Wehaveaccesstotheensembleforeachparameterthankstothecustomtraining
function. Therefore, wecandelveintoeachtreeofeachensembletoretrieveleafvaluesandsplittingpoints. More
specifically,eachsplitpointinaregressiontreerepresentsastepintheGBUVoutputforthecorrespondingparameter.
Theutilitycontributionatpoint xisthecumulativesumofallcorrespondingleafvaluesofthetreesintheensemble.
Thesefunctionsarepresentedforthetraveltime,cost,departuretimeandageparameterinFigures1a,1b,2aand2b.
Figure1ashowstheimpactoftraveltimesonparametervaluesfortheLPMCdataset, withthepublictransport
alternative including both bus and train travel times. From the graph, it can be noted that the walking and driving
timeparametershaveaconvexshape,indicatingthattheincreaseinashorttriphasagreaterinfluenceontheutility
functionthanalongertrip. Thiscontrastswiththetwoparametervalueslinkedtopublictransport,wherebustravel
time has an approximately linear curve, and train travel time impact results in a concave parameter values. Finally,
theparametervaluesofcyclingisinitiallyconvex,thenreachesaplateaubetween0.5and1houroftraveltime,and
decreasesrapidlyafter1hour. Figure1bdepictstheconstantutilitycontributionsoftravelcostparameterfordriving
andPT.Interestingly,bothparametervaluesexhibitasimilarbehaviour. Thereisfirstasharpdecreaserepresenting
thedisutilityoftravelling,thenaplateau,andafinaldroparound2£.
Figures2aand2bshowtheconstantutilityvaluesoftheageanddeparturetimeparametersforeachalternative.
Following behavioural theory, there are no monotonicity constraints on these variables, but we still observe that
increasingagemainlyreducestheutilityofwalkingandcycling, withamostlyconvexshape. Forpublictransport,
theparametervaluesisthelowestatolderages,andpeaksaroundtheageof20. Finally,forthedrivingalternative,
theutilityishigherwithyoungerandolderages, butthelowestpointisattheageof20. Alsonotethatpassengers
are included in the driving alternatives, explaining the high values for younger and older individuals. Finally, the
departure time utilities are stable during the day. The walking and cycling utilities exhibit a sharp increase around
midnight. Thepublictransportutilityhasahighervalueonthemorning, correspondingtothemorningcommuting
time. Finally,thedrivingutilitydecreasesuntil7h,andincreasesuntiltheendofthedayafterthat.
(a)Traveltime(LPMC) (b)Cost(LPMC)
Figure1:Utilitycontributionsofa)traveltimeandb)costontheLPMCdataset,bothunderanegativemonotonicconstraint.Eachsteprepresents
asplitpointofaregressiontreeinthecorrespondingensemble.
13(a)Age(LPMC) (b)Departuretime(LPMC)
Figure2:Utilitycontributionsofa)ageandb)departuretimeontheLPMCdataset.Bothvariablesarenon-monotonic.Eachsteprepresentsa
splitpointofaregressiontreeinthecorrespondingensemble.
3.4.1. GBUVrobustness
TodemonstratetherobustnessoftheGBUVoutputs,weperformbootstrapsamplingfor100iterations. Weplot
theresultingparametervalueswiththeiraverageandshowtheresultsforthetraveltimeparametersinFigure3. To
visualisethedistributionoftheinputspace,weplotahistogramofthedatadistributionontopofeachfigure.
Figure 3 show that, even with sampling with replacement, i.e. changing the distribution of the population, the
utilityvaluesarerobust. Whiletheirvaluesmayvaryslightly,theirshapeissimilarinallutilities,especiallywhenthe
densityofobservationsishigh. Thewalkinganddrivingtraveltime(Figures3aand3d)exhibitthebestrobustness,
whilecyclingtraveltime(Figure3b)andtherailtraveltime(Figure3c)aretheoneshowingthemostscalevariability.
However,thecyclingalternativeistheleastchosenalternative,andahighnumberofindividualshavenotraveltime
forthePTalternative,whichcanexplainthisfinding. Thesebootstrappedutilitiescouldbefurtherusedtocalculate
confidenceinterval.
14(a)Walkingtraveltime (b)Cyclingtraveltime
(c)PT-Railtraveltime (d)Drivingtraveltime
Figure3:UtilitycontributionsofthetraveltimeontheLPMCdatasetwithbootstrappingfora)walking,b)cycling,c)PTandd)driving
alternative.Eachlinewithtransparencycorrespondstoabootstrapsamplingiteration.Themeanishighlighted,andthedistributionofdatais
shownontopofeachfigure.Thefiguresarecroppedat2hoursoftraveltime.
153.5. Piece-wisecubicutilityfunctions
WemakeuseoftheSciPy(Virtanenetal.,2020)implementationofmonotoniccubicsplines(FritschandCarlson,
1980; Fritsch and Butland, 1984) to smooth the GBUV outputs to produce piece-wise cubic utility functions. We
treatfindingthenumberandlocationoftheknotsasaheurisiticoptimisationproblemwhereweminimisetheBICof
themodellikelihood,followingthemethodologyintroducedinSection2.2.2. WemakeuseoftheHyperoptPython
library to identify optimal solutions. The case-study model has 15 variables for which we wish to extract marginal
utilities. EachsearchinHyperoptinvolvesselectingadifferentnumberofknots, constrainedtobeanintegervalue
betweenaminimumof3andupto8. Intotal,25searchesareconducted(i.e. 25differentcombinationsofnumbers
ofknotsforeachvariable).
Theinneroptimisationloopthenidentifiesoptimalknotlocations,givenafixednumberofknotsforeachvariable,
usingtheSLSQP(SequentialLeastSquaresQuadraticProgramming)algorithm,implementedinSciPy. Weconstrain
thefirstandlastknotstobeatthelocationofthefirstandlastobservationsforeachvariable. Fortheinitialsolution
for the remaining knots, we follow Wang et al. (2023), where we set the position of K +1 knots at the k/(K +1)th
quantile.
The optimised number of knots for each variable are shown in Table 5. The use of the BIC as the objective
function results in parsimonious solutions, where the number of splines is limited for simple transformations. The
straight-linedistanceisnotincludedforpublictransportastherewerenoregressiontreesintheparameterensemble.
ThesmoothedPCUFoutputsforthetraveltimeandcostparametersareshowninFigure4. Fromthegraph,itisclear
thatPCUFoutputhassuccessfullysmoothedtheGBUVfunctions,ratherthanoverfittingwithtoomanysplines.
Table5:OptimalnumberofknotsforPCUF.Thenumberofknotsischosenwithahyperparametersearchof25iterations
Attributes Numberofknots
Walking
Traveltime 6
Distance 6
Cycling
Traveltime 6
Distance 6
Publictransport
Railtraveltime 3
Bustraveltime 4
Accesstraveltime 4
Interchangewaitingtime 8
Interchangewalkingtime 7
Cost 3
Driving
Traveltime 5
Distance 4
Cost 3
UsingthegradientsfromthePCUFfunctions,theValueofTime(VoT)canbecomputedforthealternativesthat
includebothtraveltimeandcostparameters. Wedefinetheplotonlyintheareawherethecostderivativeisnotzero,
andwecapthemaximumvalueat100£/handminimumvalueat0.1£/h. Wealsoexcludetheflatareasatlowand
high values of both attributes. As both the travel time and cost attributes are non-linear, the VoT is unique to each
combinationoftraveltimeandcost. Assuch,itisrepresentedasa3Ddistribution,thoughnotethatthisdistribution
is homogeneous across the population. Figure 5 displays the VoT for the PT and driving alternatives on the LPMC
datasetonalogarithmic(base10)scale. Weusealogarithmicscaletobettervisualisethedifferencesamonglower
16(a)Traveltime (b)Cost
Figure4:Piece-wisemonotoniccubicsplineinterpolationofa)traveltimeandb)costontheLPMCdataset.Theknotsaredrawninblackandthe
firstandlastknotsareomittedforclarity.TheGBUVusedforinterpolationareplottedasascatterplot.
VoTs. Thevalueoftimeofrail(Figure5a)rangesfrom2to5£/hfortripslastinglessthan0.6hourandincreasesto
10to20£/hfortraveltimesof0.6to1hour. Itdecreasesagainafteronehour. Thissuggeststhatbetween0.6and1
houroftraveltime,individualsarewillingtopaymoretoreducetheirtraveltimes. Regardingthevalueoftimefor
driving(Figure5b),weobserveoveralladecreaseofthetraveltimewithincreasingtraveltime,andanincreasingof
VoTwithincreasingcost. Thelowestvalueisaround2£/hfornocostand1.3hoursoftraveltime,andthehighest
valueiscappedat100£/hfor4£costandno traveltime. Wededucethatindividualswithhighcost andlowtravel
timesarewillingtopaymoretoreducetheirtraveltime.
(a)Rail(LPMCdataset) (b)Drivingalternative(LPMCdataset)
Figure5:ValueofTime(VoT)fora)rail,b)driving.TheVoTiscappedat100£/h,anddisplayedonlywheretheutilityfunctionsderivativesare
nonzero.
In addition to the contour plot VoT, we also compute the VoT across the population. To do so, we remove
observations that have a zero travel time for the rail alternative, and exclude the 0.1% highest values (from 99.9%
to100%). Then, wecalculatetheVoTofallremainingindividualswiththeirrespectivecostsandtraveltimes. The
17resultsareshowninFigure6. ForthePTalternative(Figure6a),thedistributionofVoTpeaksbelow1£/handthen
continuously decreases until 5£/h. For driving VoT (Figure 6b), there is a sharp peak at 17.5 £/h. These values are
bothlowerthantheVoTextractedfromlinear-in-parameterRUMmodelsforthesamedataset,of8.73£/hand40£/h
respectively(seeHillel(2019),p.133),showingtheimpactofthenon-linearutilityspecification.
(a)PTalternative (b)Drivingalternative
Figure6:HistogramofthepopulationValueofTime(VoT)fora)rail,b)driving.Theobservationswithzerotraveltimes,aswellasthehighest
0.1%VoTvaluesareexcluded.Thesolidlinerepresentsthekerneldensityestimates.
4. ExtensionsofRUMBoost
In this section, we present three extensions of RUMBoost, highlighting how the approach can be generalised to
differentmodellingscenarios: a)incorporatingattributeinteractions(Section4.1);b)assumingalternativecorrelation
withintheerrorterm(Section4.2);andc)accountingforcorrelationwithintripsmadebythesameindividual(Section
4.3).
4.1. Second-orderattributeinteractions
By allowing two continuous attributes to interact, we can consider attribute interactions. In doing so, it is still
possible to interpret the ensemble output for these two attributes on a contour plot. As an example, we arbitrarily
allowageandtraveltimetointeract. Weallowforamaxdepthoftwoineachtreetoallowforfeatureinteractions
and, through early stopping, perform 680 total boosting rounds. Figure 7 shows the resulting contour plot for all
alternatives. The contour plots are only monotonic with respect to travel time, as specified in the model. For the
walkingalternative(Figure7a),longertraveltimesleadtoincreaseddisutilityforbothyoungerandolderages,while
shortertraveltimesresultinrelativelyuniformdisutility. Forcycling(Figure7b),weobserveasimilarphenomenon
forlongertraveltimesbutthedisutilityisstillpronouncedforolderindividualswithshortertraveltimes,whichaligns
with the expectation that cycling may be less feasible for older individuals. In the case of public transport (Figure
7c), disutility is more significant for younger ages and less pronounced for adults and shorter travel times. Lastly,
the disutility associated with an increase in age or travel time for the car alternative (Figure 7d) remains relatively
constant,butitismitigatedforagesbelow10andtraveltimesshorterthan0.5hour.
4.2. NestedRUMBoost
Until now, we assumed the error terms for each alternative to be distributed i.i.d., leading to the MNL model
formulation. We show that we can relax this assumption with our approach by assuming an error term correlated
withinalternatives. InRUM,thiserrortermleadstotheNestedLogit(NL)model. Weupdatetheprobabilityformula
andupdatethegradientandhessian(secondderivative)computationsaccordingly,giving:
P(i)= P(i|m)P(m) (22)
wheretheprobabilityofchoosingiknowingthenestmis:
18(a)Walkingalternative (b)Cyclingalternative
(c)Publictransportalternative (d)Drivingalternative
Figure7:Utilityfunctionintheformofacontourplotforthetraveltime(xaxis)withage(yaxis)interactionontheLPMCdatasetfora)walking,
b)cycling,c)PTandd)drivingalternative.Onlythetraveltimeissubjecttomonotonicityconstraint.
eµmVi
P(i|m)= (cid:80) , (23)
j∈meµmVj
whiletheprobabilitytochoosethenestmis:
eV˜
m
P(m)= , (24)
(cid:80)M p=1eV˜
p
where:
• V˜
m
= µ1
m
ln(cid:16)(cid:80) i∈meµmVi(cid:17)
• Mthenumberofnest
19• µ thescalingparameterofnestm
m
Wetreatµasahyperparameter, wherewesearchitsoptimalvaluewitha5-foldcrossvalidationschemeonthe
trainingtest.MoredetailsaregivenintheappendixAppendix A.WetraintheNestedRUMBoostmodelontheLPMC
dataset,andcompareitsperformanceagainstaNLmodel. Thenestisassumedtobewithinthemotorisedmodes(PT
and driving) while walking and cycling are in their own nests. The optimal value of µ found with hyperparameter
tuning is 1.16, while the one estimated with the Nested Logit model is 1.35 (see Appendix B). The difference of
flexibilityofthesetwomodelsexplainsthisdifference. TheresultsareshowninTable6.
Table6:BenchmarkofclassificationontheLPMCDatasetfortheNestedRUMBoostwiththeRUMsandRUMBoost.Themodelsarecompared
withtheirCEL(negativeCross-EntropyLoss,lowerthebetter)onthetestsetandtheircomputationaltimeforoneCViteration.
MNL NestedLogit RUMBoost-GBUV NestedRUMBoost
CE Time[s] CE Time[s] CE Time[s] CE Time[s]
5foldCV 0.6913 242.14 0.6921 1067.04 0.6570 8.9 0.6568 48.53
Holdouttestset 0.7085 - 0.7091 - 0.6737 - 0.6731 -
WhilsttheNestedLogitmodeldoesnotshowaperformanceimprovementovertheMNLmodeintermsofout-of-
samplevalidation,thenestedRUMBoostmodelmarginallyoutperformsthebaseRUMBoostmodel. Notethatthese
resultsarewithoutPCUFsmoothing,buttheNestedRUMBoostcouldbesmoothed,asinSection3.5.
4.3. FunctionalEffectRUMBoost
Lastly,weproposetheFunctionalEffectRUMBoost(FE-RUMBoost),amodelaccountingforobservationscor-
relation. This model draws some parallels with the Mixed Effect model, where the fixed effect part is RUMBoost
withoutattributeinteractionandtherandomeffectpartincludesallsocio-economiccharacteristicswithfullinterac-
tion. Bydoingso, wekeepthefullutilityinterpretabilityonthetripattributes, andwelearnanindividual-specific
constantforeachalternativewiththesecondpartofthemodel. Inotherwords,thesecondpartofthemodelisalatent
representationofeachindividualfromtheirsocio-economiccharacteristics, whichcancapturecorrelationforpanel
dataorotherobservationcorrelatedsituations. WeapplythemodelontheLPMCdatasetandwecompareitwiththe
benchmarksofSection3.3inTable7.
Table7:BenchmarkofclassificationontheLPMCDatasetfortheFE-RUMBoostwiththeMLclassifiers.Themodelsarecomparedwiththeir
CEL(negativeCross-EntropyLoss,lowerthebetter)onthetestsetandtheircomputationaltimeforoneCViteration.
LightGBM NN DNN FE-RUMBoost
CE Time[s] CE Time[s] CE Time[s] CE Time[s]
5foldCV 0.6381 4.64 0.6516 7.85 0.6613 3.89 0.6447 10.9
Holdouttestset 0.6537 - 0.6667 - 0.6735 - 0.6626 -
Thisextensionofthemodelsubstantiallyimprovesthepredictionperformanceonthetestset. Ourmodeloutper-
formsboththeNNandDNNclassifiersandnarrowsthegapvstheunconstrainedLightGBMmodel,whilekeeping
thefullinterpretabilityonkeyalternative-specificattributes. Thecomputationalcostinducedbythegreatercomplex-
ityisminimalcomparedtotheinitialRUMBoost. Again,themodelpresentedhereiswithoutsmoothingbut,because
allthetripattributesthatwerepreviouslysmoothedareinthefirstpartofthemodel, wecanapplysmoothingasin
Section3.5. Inaddition,wecanvisualisetheindividualspecificconstantperalternative. Weshowthedistributionof
theconstantsperalternativeinFigure8intheformofhistograms. Thesehistogramsshowclearlythatforcycling,PT
anddrivingthedistributionofindividual-specificconstantsarebi-modal.
Thesethree extensionsdemonstrate theRUMBoost model’sabilityto incorporateattribute interactions, account
forcorrelatederrortermswithinalternatives,andlearnanindividual-specificconstantforobservationcorrelateddata.
Theextensionscanalsobecombined,astheyareappliedtodifferentpartsofthemodel.Thisshowcasestheflexibility
andgeneralisabilityoftheRUMBoostframework.Infuturework,anycomplexchoicesituationsthatleadtoadefined
gradientandhessiancouldbeappliedtothemodel.
20Figure8:Histogramsoftheindividual-specificconstantlearntwiththeFunctionalEffectRUMBoostofa)walking,b)cycling,c)PTandd)
drivingalternative.Foreachhistogram,weplotthekerneldensityestimateasasolidline.Overall,cycling,PTanddrivingindividualconstants
exhibitabi-modaldistribution,andwalkingalternativeauni-modaldistributioncenteredatzero.
5. Conclusionandfurtherwork
ThemethodologypresentedinthispaperallowsforafullyinterpretableMLmodel(RUMBoost)basedonGBDT
and inspired by random utility models. In short, RUMBoost replaces every parameter of a RUM by an ensemble
of regression trees. By re-implementing classification for GBDT, we can provide specific attributes for alternative
utilities, control attribute interactions in the ensemble, and apply monotonic constraints on key attributes based on
domain knowledge. These constraints considerably improve the predictions of traditional RUMs and enable the
derivation of non-linear utility functions. Furthermore, we apply piece-wise monotonic cubic splines to interpolate
the utility functions and obtain a smooth utility function. We find that the smoothing acts as further regularization
and enables us to compute the Value of Time (VoT). We also show that the modularity of our approach allow for
the estimation of complex model specification such as error term accounting for correlation within alternatives or
correlation within observations. Our approach offer to observe the full functional form of the utility function with
definedgradient,justlikeinDCMs. Thekeydifferenceisthattheutilityfunctionisdirectlylearntfromthedata.
Whilstappliedheretochoicemodels,thismethodologycouldbeusedinplaceofanylinear-in-parametersmodels,
21forregression,classification,oranytaskforwhichthegradientandhessianofthecostfunctionarewelldefined. Fur-
therworkincludesapplyingthemodeltovariousproblemstodemonstratethisstatement. Furtherworkalsoincludes
applyingtheRUMBoostmodeltoothercomplexmodelspecificationssuchastheCross-NestedLogit(CNL)model.
ThePCUFalgorithmcouldbeimprovedbyapplyingB-splines, whichwouldprovideaC2 monotonicinterpolation
ofthedata,whereshapeconstraintcouldbeincorporated. Finally,theGBUVcouldbecomputeddirectlywithlinear
trees,quadratictreesorsplines,toobtaindirectlypiece-wiseutilityfunctionswithdefinedgradient.
CRediTauthorshipcontributionstatement
NicolasSalvade´:Conceptualization,Methodology,Software,Writing-OriginalDraft,Visualization.TimHillel:
Conceptualization,Methodology,Writing-ReviewandEditing,Supervision.
Declarationofcompetinginterest
None
Acknowledgements
We sincerely thank Prof. Michel Bierlaire for his guidance on the smoothing process. We also would like to
expressoursinceregratitudetoDr. Jose´ A´ngelMart´ın-BaosforhisassistancewiththebenchmarksontheLPMC.
Appendix A. Hyperparametersearch
TableA.8:HyperparametersearchandoptimalvalueforRUMBoost,NestedRUMBoost,andFE-RUMBoostontheLPMCdataset
RUMBoost NestedRUMBoost FE-RUMBoost
Numberofsearches 1 25 100
Time[s] 44.5 5378 5366
Hyperparameter Distribution Searchspace
Meanofnum iterations earlystopping 100 1300 1256 1099
bagging fraction uniform [0.5,1] - - 0.700
bagging freq choice (0,1,5,10) - - 10
feature fraction uniform [0.5,10] - - 0.867
lambda l1 loguniform [0.0001,10] - - 6.592
lambda l2 loguniform [0.0001,10] - - 1.028
learning rate fixed 0.1 - - 0.1
max bin uniform [100,500] - - 131
min data in leaf uniform [1,200] - - 27
min gain to split loguniform [0.0001,5] - - 0.800
min sum hessian in leaf loguniform [1,100] - - 1.783
num leaves uniform [2,100] - - 74
mu uniform [1,2] - 1.167 -
22TableA.9:HyperparametersearchandoptimalvalueforLightGBM,NN,andDNNontheLPMCdataset.
LightGBM NN DNN
Numberofsearches 1000 1000 1000
Time[s] 24882 28736 26343
Hyperparameter Distribution Searchspace
MeanofCVnum iterations earlystopping 100 1493 - -
bagging fraction uniform [0.5,1] 0.7204 - 0.700
bagging freq choice (0,1,5,10) 1 - 10
feature fraction uniform [0.5,10] 0.6007 - 0.867
lambda l1 loguniform [0.0001,10] 0.0242 - 6.592
lambda l2 loguniform [0.0001,10] 0.0001 - 1.028
learning rate fixed - 0.1 adaptive 0.1
max bin discreteuniform [100,500] 237 - 131
min data in leaf discreteuniform [1,200] 156 - 27
min gain to split loguniform [0.0001,5] 0.0007 - 0.800
min sum hessian in leaf loguniform [1,100] 1.4136 - 1.783
num leaves discreteuniform [2,100] 16 - 74
activation fixed - - tanh(x) relu(x)
batch size choice (128,256,512,1024) - 1024 1024
hidden layer size discreteuniform [10,500] - - 20
learning rate init uniform [0.0001,1] - 0.0907 -
solver choice [lbfgs,sgd,adam] - sgd -
depth discreteuniform [2,10] - - 2
drop choice (0.5,0.3,0.1) - - 0.3
epochs discreteuniform [50,200] - - 90
width choice (25,50,100,150,200) - - 50
Appendix B. EstimationoftheMNLmodels
TableB.10:ParameterestimatesoftheLPMCMNL.Outofthe62parameters,9arenotsignificantata95%confidenceinterval.
LPMC-MNL
Value Activebound Rob. p-value
ASC Bike -3.380 0.000 0.000
ASC Car -2.592 0.000 0.000
ASC Public Transport -1.908 0.000 0.000
B age Bike -0.004 0.000 0.032
B age Car 0.005 0.000 0.000
B age Public Transport 0.011 0.000 0.000
B car ownership Bike 0.036 0.000 0.590
B car ownership Car 0.694 0.000 0.000
B car ownership Public Transport -0.213 0.000 0.000
B con charge Car -1.147 0.000 0.000
B cost driving fuel Car 0.000 1.000 1.000
B cost transit Public Transport -0.115 0.000 0.000
B day of week Bike -0.020 0.000 0.201
B day of week Car 0.030 0.000 0.000
B day of week Public Transport -0.044 0.000 0.000
23TableB.10:ParameterestimatesoftheLPMCMNL.Outofthe62parameters,9arenotsignificantata95%confidenceinterval.
LPMC-MNL
Value Activebound Rob. p-value
B distance Bike -0.232 0.000 0.040
B distance Car 0.000 1.000 1.000
B distance Public Transport 0.000 1.000 1.000
B driving license Bike 0.678 0.000 0.000
B driving license Car 0.663 0.000 0.000
B driving license Public Transport -0.526 0.000 0.000
B dur cycling Bike -2.670 0.000 0.000
B dur driving Car -4.777 0.000 0.000
B dur pt access Public Transport -4.608 0.000 0.000
B dur pt bus Public Transport -1.911 0.000 0.000
B dur pt int waiting Public Transport -4.284 0.000 0.000
B dur pt int walking Public Transport -2.335 0.000 0.027
B dur pt rail Public Transport -1.338 0.000 0.000
B dur walking Walk -8.596 0.000 0.000
B female Bike -0.834 0.000 0.000
B female Car 0.100 0.000 0.002
B female Public Transport 0.160 0.000 0.000
B fueltype Avrg Bike -0.691 0.000 0.000
B fueltype Avrg Car -1.400 0.000 0.000
B fueltype Avrg Public Transport -0.221 0.000 0.000
B fueltype Diesel Bike -0.822 0.000 0.000
B fueltype Diesel Car -0.228 0.000 0.000
B fueltype Diesel Public Transport -0.419 0.000 0.000
B fueltype Hybrid Bike -1.000 0.000 0.000
B fueltype Hybrid Car -0.721 0.000 0.000
B fueltype Hybrid Public Transport -0.945 0.000 0.000
B fueltype Petrol Bike -0.867 0.000 0.000
B fueltype Petrol Car -0.242 0.000 0.000
B fueltype Petrol Public Transport -0.323 0.000 0.000
B pt n interchanges Public Transport -0.101 0.000 0.154
B purpose B Bike -0.029 0.000 0.775
B purpose B Car -0.043 0.000 0.543
B purpose B Public Transport -0.012 0.000 0.874
B purpose HBE Bike -1.054 0.000 0.000
B purpose HBE Car -0.756 0.000 0.000
B purpose HBE Public Transport -0.237 0.000 0.000
B purpose HBO Bike -0.773 0.000 0.000
B purpose HBO Car -0.352 0.000 0.000
B purpose HBO Public Transport -0.442 0.000 0.000
B purpose HBW Bike -0.291 0.000 0.000
B purpose HBW Car -1.062 0.000 0.000
B purpose HBW Public Transport -0.502 0.000 0.000
B purpose NHBO Bike -1.233 0.000 0.000
B purpose NHBO Car -0.379 0.000 0.000
B purpose NHBO Public Transport -0.715 0.000 0.000
B start time linear Bike 0.017 0.000 0.015
B start time linear Car 0.027 0.000 0.000
24TableB.10:ParameterestimatesoftheLPMCMNL.Outofthe62parameters,9arenotsignificantata95%confidenceinterval.
LPMC-MNL
Value Activebound Rob. p-value
B start time linear Public Transport 0.010 0.000 0.016
B traffic perc Car -2.404 0.000 0.000
TableB.11:ParameterestimatesoftheLPMCNL.Outofthe63parameters,10arenotsignificantata95%confidenceinterval.
LPMC-NL
Value Activebound Rob. p-value
ASC Bike -3.346 0.000 0.000
ASC Car -2.439 0.000 0.000
ASC Public Transport -1.969 0.000 0.000
B age Bike -0.004 0.000 0.026
B age Car 0.007 0.000 0.000
B age Public Transport 0.011 0.000 0.000
B car ownership Bike 0.062 0.000 0.351
B car ownership Car 0.628 0.000 0.000
B car ownership Public Transport -0.037 0.000 0.369
B con charge Car -0.816 0.000 0.000
B cost driving fuel Car 0.000 1.000 1.000
B cost transit Public Transport -0.077 0.000 0.000
B day of week Bike -0.023 0.000 0.155
B day of week Car 0.022 0.000 0.007
B day of week Public Transport -0.033 0.000 0.000
B distance Bike -0.225 0.000 0.042
B distance Car -0.005 0.000 0.960
B distance Public Transport 0.000 1.000 1.000
B driving license Bike 0.705 0.000 0.000
B driving license Car 0.484 0.000 0.000
B driving license Public Transport -0.396 0.000 0.000
B dur cycling Bike -1.839 0.000 0.003
B dur driving Car -3.409 0.000 0.000
B dur pt access Public Transport -3.410 0.000 0.000
B dur pt bus Public Transport -1.445 0.000 0.000
B dur pt int waiting Public Transport -3.036 0.000 0.000
B dur pt int walking Public Transport -1.876 0.000 0.017
B dur pt rail Public Transport -1.085 0.000 0.000
B dur walking Walk -8.171 0.000 0.000
B female Bike -0.831 0.000 0.000
B female Car 0.112 0.000 0.000
B female Public Transport 0.156 0.000 0.000
B fueltype Avrg Bike -0.680 0.000 0.000
B fueltype Avrg Car -1.175 0.000 0.000
B fueltype Avrg Public Transport -0.322 0.000 0.000
B fueltype Diesel Bike -0.817 0.000 0.000
B fueltype Diesel Car -0.251 0.000 0.000
25TableB.11:ParameterestimatesoftheLPMCNL.Outofthe63parameters,10arenotsignificantata95%confidenceinterval.
LPMC-NL
Value Activebound Rob. p-value
B fueltype Diesel Public Transport -0.391 0.000 0.000
B fueltype Hybrid Bike -0.985 0.000 0.000
B fueltype Hybrid Car -0.755 0.000 0.000
B fueltype Hybrid Public Transport -0.941 0.000 0.000
B fueltype Petrol Bike -0.864 0.000 0.000
B fueltype Petrol Car -0.259 0.000 0.000
B fueltype Petrol Public Transport -0.315 0.000 0.000
B pt n interchanges Public Transport -0.088 0.000 0.093
B purpose B Bike -0.032 0.000 0.748
B purpose B Car -0.066 0.000 0.337
B purpose B Public Transport -0.053 0.000 0.445
B purpose HBE Bike -1.036 0.000 0.000
B purpose HBE Car -0.665 0.000 0.000
B purpose HBE Public Transport -0.271 0.000 0.000
B purpose HBO Bike -0.771 0.000 0.000
B purpose HBO Car -0.317 0.000 0.000
B purpose HBO Public Transport -0.397 0.000 0.000
B purpose HBW Bike -0.272 0.000 0.000
B purpose HBW Car -0.976 0.000 0.000
B purpose HBW Public Transport -0.570 0.000 0.000
B purpose NHBO Bike -1.235 0.000 0.000
B purpose NHBO Car -0.415 0.000 0.000
B purpose NHBO Public Transport -0.678 0.000 0.000
B start time linear Bike 0.016 0.000 0.016
B start time linear Car 0.024 0.000 0.000
B start time linear Public Transport 0.012 0.000 0.002
B traffic perc Car -1.947 0.000 0.000
MU m 1.391 0.000 0.000
References
Aboutaleb,Y.M.,2022.Theory-constrainedData-drivenModelSelection,Specification,andEstimation:ApplicationsinDiscreteChoiceModels.
Ph.D.thesis.MassachusettsInstituteofTechnology.
Ben-Akiva,M.E.,Lerman,S.R.,1985. Discretechoiceanalysis: theoryandapplicationtotraveldemand. MITPressseriesintransportation
studies,MITPress,Cambridge,Mass.
Bergstra,J.,Yamins,D.,Cox,D.,2013. Makingascienceofmodelsearch: Hyperparameteroptimizationinhundredsofdimensionsforvision
architectures,in:Internationalconferenceonmachinelearning,PMLR.pp.115–123.
Bierlaire,M.,2023.AshortintroductiontoBiogeme.TechnicalReport.TechnicalreportTRANSP-OR230620.TransportandMobilityLaboratory,
ENAC,EPFL.
Caruana, R., Lou, Y., Gehrke, J., Koch, P., Sturm, M., Elhadad, N., 2015. Intelligiblemodelsforhealthcare: Predictingpneumoniariskand
hospital30-dayreadmission,in:Proceedingsofthe21thACMSIGKDDinternationalconferenceonknowledgediscoveryanddatamining,pp.
1721–1730.
Chen,T.,Guestrin,C.,2016. XGBoost:AScalableTreeBoostingSystem,in:Proceedingsofthe22ndACMSIGKDDInternationalConference
onKnowledgeDiscoveryandDataMining,pp.785–794. URL:http://arxiv.org/abs/1603.02754,doi:10.1145/2939672.2939785.
arXiv:1603.02754[cs].
Dahmen,V.,Weikl,S.,Bogenberger,K.,2023. Interpretablemachinelearningformodechoicemodelingon1tracking-basedrevealedpreference
data2.Victoria4,5.
Friedman,J.H.,2001.Greedyfunctionapproximation:Agradientboostingmachine.TheAnnalsofStatistics29.doi:10.1214/aos/1013203451.
26Fritsch,F.N.,Butland,J.,1984.Amethodforconstructinglocalmonotonepiecewisecubicinterpolants.SIAMjournalonscientificandstatistical
computing5,300–304.
Fritsch,F.N.,Carlson,R.E.,1980.Monotonepiecewisecubicinterpolation.SIAMJournalonNumericalAnalysis17,238–246.
Han,Y.,Pereira,F.C.,Ben-Akiva,M.,Zegras,C.,2022.Aneural-embeddeddiscretechoicemodel:Learningtasterepresentationwithstrengthened
interpretability.TransportationResearchPartB:Methodological163,166–186.
Hastie,T.J.,Tibshirani,R.J.,1990.Generalizedadditivemodels.volume43.CRCpress.
Hillel,T.,2019.Understandingtravelmodechoice:Anewapproachforcityscalesimulation.Ph.D.thesis.UniversityofCambridge.
Hillel,T.,Bierlaire,M.,Elshafie,M.,Jin,Y.,2019. Weakteachers:Assistedspecificationofdiscretechoicemodelsusingensemblelearning,in:
8thSymposiumoftheEuropeanassociationforresearchintransportation,Budapest.
Hillel,T.,Elshafie,M.Z.E.B.,Jin,Y.,2018. Recreatingpassengermodechoice-setsfortransportsimulation: AcasestudyofLondon,UK. Pro-
ceedingsoftheInstitutionofCivilEngineers-SmartInfrastructureandConstruction171,29–42.URL:https://www.icevirtuallibrary.
com/doi/10.1680/jsmic.17.00018,doi:10.1680/jsmic.17.00018.
Ke,G.,Meng,Q.,Finley,T.,Wang,T.,Chen,W.,Ma,W.,Ye,Q.,Liu,T.Y.,2017. Lightgbm: Ahighlyefficientgradientboostingdecisiontree.
Advancesinneuralinformationprocessingsystems30.
Kim,E.J.,Bansal,P.,2023.Anewflexibleandpartiallymonotonicdiscretechoicemodel.AvailableatSSRN4448172.
Krueger,R.,Daziano,R.A.,2022.StatedchoiceanalysisofpreferencesforCOVID-19vaccinesusingtheChoquetintegral.JournalofChoiceMod-
elling 45, 100385. URL: https://linkinghub.elsevier.com/retrieve/pii/S1755534522000422, doi:10.1016/j.jocm.2022.
100385.
Lundberg,S.M.,Lee,S.I.,2017.Consistentfeatureattributionfortreeensembles.arXivpreprintarXiv:1706.06060.
Mart´ın-Baos,J.A´.,Garcia-Rodenas,R.,Rodriguez-Benitez,L.,2021. Revisitingkernellogisticregressionundertherandomutilitymodelsper-
spective.aninterpretablemachine-learningapproach.TransportationLetters13,151–162.
A´ngel Mart´ın-Baos, J., Lo´pez-Go´mez, J.A., Rodriguez-Benitez, L., Hillel, T., Garc´ıa-Ro´denas, R., 2023. A prediction and behavioural
analysis of machine learning methods for modelling travel mode choice. Transportation Research Part C: Emerging Technologies 156,
104318. URL: https://www.sciencedirect.com/science/article/pii/S0968090X23003078, doi:https://doi.org/10.1016/
j.trc.2023.104318.
McFadden,D.,etal.,1973.Conditionallogitanalysisofqualitativechoicebehavior.FrontierinEconometrics,105–142.
Ortelli,N.,Hillel,T.,Pereira,F.C.,deLapparent,M.,Bierlaire,M.,2021.Assistedspecificationofdiscretechoicemodels.JournalofChoiceMod-
elling 39, 100285. URL: https://linkinghub.elsevier.com/retrieve/pii/S175553452100018X, doi:10.1016/j.jocm.2021.
100285.
Ren,Y.,Yang,M.,Chen,E.,Cheng,L.,Yuan,Y.,2023. Exploringpassengers’choiceoftransfercityinair-to-railintermodaltravelusingan
interpretableensemblemachinelearningapproach.Transportation,1–31.
Ribeiro,M.T.,Singh,S.,Guestrin,C.,2016. ”whyshoulditrustyou?”explainingthepredictionsofanyclassifier,in: Proceedingsofthe22nd
ACMSIGKDDinternationalconferenceonknowledgediscoveryanddatamining,pp.1135–1144.
Sifringer,B.,Lurkin,V.,Alahi,A.,2020.Enhancingdiscretechoicemodelswithrepresentationlearning.TransportationResearchPartB:Method-
ological140,236–261. URL:https://linkinghub.elsevier.com/retrieve/pii/S0191261520303830,doi:10.1016/j.trb.2020.
08.006.
Tamim Kashifi, M., Jamal, A., Samim Kashefi, M., Almoshaogeh, M., Masiur Rahman, S., 2022. Predicting the travel mode choice with
interpretable machine learning techniques: A comparative study. Travel Behaviour and Society 29, 279–296. URL: https://www.
sciencedirect.com/science/article/pii/S2214367X22000746,doi:https://doi.org/10.1016/j.tbs.2022.07.003.
Train,K.E.,2009.Discretechoicemethodswithsimulation.Cambridgeuniversitypress.
Virtanen,P.,Gommers,R.,Oliphant,T.E.,Haberland,M.,Reddy,T.,Cournapeau,D.,Burovski,E.,Peterson,P.,Weckesser,W.,Bright,J.,van
derWalt,S.J.,Brett,M.,Wilson,J.,Millman,K.J.,Mayorov,N.,Nelson,A.R.J.,Jones,E.,Kern,R.,Larson,E.,Carey,C.J.,Polat,˙I.,Feng,
Y.,Moore,E.W.,VanderPlas,J.,Laxalde,D.,Perktold,J.,Cimrman,R.,Henriksen,I.,Quintero,E.A.,Harris,C.R.,Archibald,A.M.,Ribeiro,
A.H.,Pedregosa,F.,vanMulbregt,P.,SciPy1.0Contributors,2020. SciPy1.0:FundamentalAlgorithmsforScientificComputinginPython.
NatureMethods17,261–272.doi:10.1038/s41592-019-0686-2.
Wang,L.,Fan,X.,Li,H.,Liu,J.S.,2023.Monotonecubicb-splines.arXivpreprintarXiv:2307.01748.
Wang,S.,Mo,B.,Zhao,J.,2021. DeepNeuralNetworksforChoiceAnalysis: ArchitecturalDesignwithAlternative-SpecificUtilityFunctions
URL:http://arxiv.org/abs/1909.07481.arXiv:1909.07481[cs,econ,q-fin,stat].
Wang, S., Wang, Q., Zhao, J., 2020. Deep neural networks for choice analysis: Extracting complete economic information for interpreta-
tion. TransportationResearchPartC:EmergingTechnologies118,102701. URL:https://linkinghub.elsevier.com/retrieve/pii/
S0968090X20306161,doi:10.1016/j.trc.2020.102701.
Wong,M.,Farooq,B.,2021. ResLogit:Aresidualneuralnetworklogitmodelfordata-drivenchoicemodelling. TransportationResearchPartC:
EmergingTechnologies126,103050. URL:https://linkinghub.elsevier.com/retrieve/pii/S0968090X21000802,doi:10.1016/
j.trc.2021.103050.
27