Mitigating Covariate Shift in Misspecified Regression
with Applications to Reinforcement Learning
Philip Amortila Tongyi Cao
University of Illinois, Urbana-Champaign University of Massachusetts, Amherst
philipa4@illinois.edu tcao@cs.umass.edu
Akshay Krishnamurthy
Microsoft Research, NYC
akshaykr@microsoft.com
Abstract
A pervasive phenomenon in machine learning applications is distribution shift, where training and
deployment conditions for a machine learning model differ. As distribution shift typically results in a
degradation in performance, much attention has been devoted to algorithmic interventions that mitigate
these detrimental effects. In this paper, we study the effect of distribution shift in the presence of model
misspecification, specifically focusing on L -misspecified regression and adversarial covariate shift, where
∞
the regression target remains fixed while the covariate distribution changes arbitrarily. We show that
empiricalriskminimization,orstandardleastsquaresregression,canresultinundesirablemisspecification
amplification wheretheerrorduetomisspecificationisamplifiedbythedensityratiobetweenthetraining
andtestingdistributions. Asourmainresult,wedevelopanewalgorithm—inspiredbyrobustoptimization
techniques—that avoids this undesirable behavior, resulting in no misspecification amplification while
still obtaining optimal statistical rates. As applications, we use this regression procedure to obtain new
guaranteesinofflineandonlinereinforcementlearningwithmisspecificationandestablishnewseparations
between previously studied structural conditions and notions of coverage.
1 Introduction
A majority of machine learning methods are developed and analyzed under the idealized setting where the
training conditions accurately reflect those at deployment. Yet, almost all practical applications exhibit
distribution shift, where these conditions differ significantly. Distribution shift can occur for a plethora of
reasons, ranging from quirks in data collection (Recht et al., 2019), to temporal drift (Gama et al., 2014;
Besbes et al., 2015), to users adapting to an ML model (Perdomo et al., 2020), and it typically results
in a degradation in model performance. Due to the prevalence of this phenomenon and the diversity of
applications where it manifests, there is a vast and ever-growing body of literature studying algorithmic
interventions to mitigate distribution shift (Quinonero-Candela et al., 2008; Sugiyama and Kawanabe, 2012).
Covariate shift is perhaps the most basic form of distribution shift. Covariate shift is pertinent to supervised
learning—wherethegoalistopredictalabelY fromcovariatesX—andpositsachangeinthedistributionover
covariateswhilekeepingthetargetpredictorfixed. Thissetup,inparticularthatthetargetdoesnotchange,is
natural in applications including neural algorithmic reasoning (Anil et al., 2022; Zhang et al., 2022; Liu et al.,
2023), reinforcement learning (Ross et al., 2011; Levine et al., 2020), and computer vision (Koh et al., 2021;
Rechtetal.,2019;Milleretal.,2021). Itiswellknownthatonecanadaptguaranteesfromstatisticallearning
tothecovariateshiftsetting; specifically, forwell-specifiedregression, aclassicaldensity-ratioargumentshows
that empirical risk minimization (ERM) is consistent under suitably well-behaved covariate shifts.
Onestipulationofthisconsistencyguaranteeisthatthemodel/hypothesisclassbewell-specified (alsoreferred
Authorslistedinalphabeticalorder.
1
4202
naJ
22
]LM.tats[
1v61221.1042:viXratoasrealizable). Althoughstatisticallearningtheoryoffersarathercompleteunderstandingofmisspecification
in the absence of covariate shift (via agnostic learning and excess risk bounds), our understanding of how
covariate shift can adversely interact with model misspecification remains fairly immature. This interaction
is the focus of the present paper.
1.1 Contributions
We study regression under adversarial covariate shift where we receive regression samples from a distribution
D but are evaluated on an arbitrary distribution D for which no prior knowledge is available; we only
train test
assume that the distributions share the same target regression function f⋆ and that the worst-case density
ratio of the covariate marginals is bounded by C ∈[1,∞) (formally defined in Section 2). As inductive bias,
∞
we have a function class F of predictors and assume L -misspecification: there exists a predictor f¯∈F that
∞
is pointwise close to f⋆, i.e., ∥f¯−f⋆∥ ≤ε . This notion is natural for the covariate shift setting because it
∞ ∞
ensures that f¯has low and comparable prediction error on both D and any D .
train test
In this setup we obtain the following results:
1. Weshowthatstandardempiricalriskminimization(ERM)isnotrobusttocovariateshiftinthepresence
ofmisspecification. Precisely,eveninthelimitofinfinitedata,ERMoverF canincursquaredprediction
error under D scaling as Ω(C ε2 ). Meanwhile the error of the L -misspecified predictor f¯is at
test ∞ ∞ ∞
most ε2 . We call this phenomenon—where the misspecification error is scaled by the density ratio
∞
coefficient (despite there being a predictor avoiding this scaling)—misspecification amplification.
2. As our main result, we give a new algorithm, called disagreement-based regression (DBR), that avoids
misspecification amplification and is therefore robust to adversarial covariate shift under misspecifica-
tion. DBR has asymptotic prediction error under D test scaling as O(ε2 ∞), with no dependence on the
density ratio coefficient C . At the same time, it has order-optimal finite sample behavior recovering
∞
standard “fast rate” guarantees for the well-specified setting, and can be extended to adapt to unknown
misspecificationlevel(asshowninAppendixA.4). Toourknowledge,thisisthefirstresultavoidingmis-
specification amplification in the adversarial covariate shift setting. Our assumptions—particularly that
no information about D is available and that F is unstructured—rule out prior approaches based on
test
densityratios(Shimodaira,2000;DuchiandNamkoong,2021)orsup-normconvergence(Schmidt-Hieber
and Zamolodtchikov, 2022); see Section 5 for further discussion.
To demonstrate the utility of disagreement-based regression, we deploy the procedure in value function
approximationsettingsinreinforcementlearning(RL),whereregressionisastandardprimitiveandmitigating
the adverse effects of distribution shift is a central challenge. Here, using DBR as a drop-in replacement for
ERM when fitting Bellman backups, we obtain the following results:
1. In the offline RL setting, we instantiate the minimax algorithm of Chen and Jiang (2019) with DBR and
show that, under L -misspecification and with coverage measured via the concentrability coefficient,
∞
misspecification amplification can be avoided when learning a near optimal policy. In contrast, prior
lower bounds imply that misspecification amplification is unavoidable when coverage is measured via
Bellman transfer coefficients (Du et al., 2020; Van Roy and Dong, 2019; Lattimore et al., 2020). Our
result therefore establishes a new separation between concentrability and Bellman transfer coefficients.
2. In the online RL setting, we instantiate the GOLF algorithm of Jin et al. (2021) with DBR and obtain
analogous results under the structural condition of coverability (building on the analysis of Xie et al.
(2023)). Taken with the above lower bounds (Du et al., 2020; Van Roy and Dong, 2019; Lattimore
et al., 2020), this separates structural conditions involving Bellman errors (e.g., Bellman rank (Jiang
et al., 2017), Bellman-eluder dimension (Jin et al., 2021), or sequential extrapolation coefficient (Xie
et al., 2023)) from coverability, which does not.
To keep the presentation concise and focused on the interaction between covariate shift and misspecification,
we focus on the simplest settings that manifest misspecification amplification. In Section 6, we discuss a
number of directions for future work, which include extensions to the core technical and algorithmic results.
22 Misspecified regression under distribution shift
We begin by introducing the formal problem setting and our assumptions. Most proofs for results in this
sectionaredeferredtoAppendixA.Therearetwojointdistributions,calledD andD ,overX×Rwhere
train test
X is a covariate space. We use P ,P and E ,E to denote the probability law and expectation
train test train test
under these distributions. We hypothesize that D and D share the same Bayes regression function, an
train test
assumption referred to as covariate shift in the literature (Shimodaira, 2000).
Assumption 2.1 (Covariate shift). For all x∈X we have
E [y |x]=E [y |x].
train test
Letf⋆ :x(cid:55)→E [y |x]denotethesharedBayesregressionfunction. Wepositthatthemarginaldistributions
train
over X are absolutely continuous with respect to a reference measure and use d and d to denote the
train test
corresponding marginal densities. We assume these are related via the following density ratio assumption.
Assumption 2.2 (Bounded density ratios). The density ratio
(cid:12) (cid:12)
C :=
sup(cid:12)
(cid:12)
d test(x) (cid:12)
(cid:12)
∞ (cid:12)d (x)(cid:12)
x∈X train
is bounded, i.e., C <∞.
∞
Note that C ≥ 1 always. Boundedness of density ratios is standard in the covariate shift literature;
∞
indeed the coefficient C appears in the classical covariate shift analyses as well as in many algorithmic
∞
interventions (Shimodaira, 2000; Sugiyama et al., 2007). Beyond satisfying these assumption, D can be
test
adaptively and adversarially chosen. In particular, no information about D , such as labeled/unlabeled
test
samples or other inductive bias, is available.
We have a dataset {(x ,y )}n of n i.i.d. labeled examples sampled from D and a function class
i i i=1 train
F ⊂(X →R) of predictors. We define the (squared) prediction errors
R (f):=E (cid:2) (f(x)−f⋆(x))2(cid:3) , and R (f):=E (cid:2) (f(x)−f⋆(x))2(cid:3) . (1)
train train test test
We seek to use the dataset to find a predictor fˆfor which R (fˆ) is small.
test
Regarding F, we make two assumptions: we assume that |F|<∞ and that F is L -misspecified.
∞
Assumption 2.3 (L -misspecification). For some ε ≥0, there exists f¯∈F with
∞ ∞
(cid:13) (cid:13)f¯−f⋆(cid:13)
(cid:13)
∞
≤ε ∞, where ∥f∥
∞
:= sup|f(x)|.
x∈X
Most prior analyses for regression under covariate shift assume that the model class F is well-specified, i.e.,
that ε = 0 so that f⋆ ∈ F. L -misspecification provides a relaxation that is natural for at least two
∞ ∞
reasons. First, it enables end-to-end learning guarantees via composition with approximation-theoretic results
for specific function classes (e.g., neural networks), where it is standard to measure approximation via the
L norm (Telgarsky, 2021). More importantly, L -misspecification is particularly apt in the covariate shift
∞ ∞
setting because it ensures that f¯has low prediction error on both D and D . Thus, there is at least
test train
one high-quality predictor whose performance is stable across distributions. In contrast, we have no such
guarantee if we, for example, measure misspecification with respect to other norms (which depend on the
distribution) or consider the agnostic setting (with no quantified misspecification assumption). Indeed, we
will see below that misspecification amplification is unavoidable in such cases.
We also make the following technical assumption.
Assumption 2.4 (Boundedness). sup ∥f∥ ≤1 and |y|≤1 almost surely under D and D .
f∈F ∞ train test
We impose Assumption 2.4 and that |F|<∞ solely to highlight the novel algorithmic and technical aspects;
we expect that relaxing these assumptions is possible.
32.1 Misspecification amplification for empirical risk minimization
WhenthereisnopriorknowledgeaboutordatafromD ,perhapsthemostnaturalalgorithmforoptimizing
test
R test(·) is empirical risk minimization (ERM) on the data from the training distribution:
n
fˆ(n) :=argmin 1 (cid:88) (f(x )−y )2.
ERM n i i
f∈F
i=1
A standard uniform convergence argument yields the classical covariate shift guarantee for ERM:
Proposition 2.1 (ERM upper bound). For any δ ∈(0,1) with probability at least 1−δ, ERM satisfies
(cid:18) (cid:19)
log(|F|/δ)
R (fˆ(n))≤O C ε2 +C .
test ERM ∞ ∞ ∞ n
The second term which scales as 1/n—the statistical term—is optimal in the generality of our setup (Ma
et al., 2023; Ge et al., 2023), the interpretation being that the effective sample size is reduced by a factor of
C due to the mismatch between D and D . The first term—the misspecification term—represents the
∞ train test
asymptotic1 test error of ERM and demonstrates a phenomenon that we call misspecification amplification,
whereby the error due to misspecification is amplified by the density ratio coefficient. This phenomenon is
simultaneously more concerning and less intuitive than the degradation of the statistical term, because it
describes an error which does not decay with larger sample sizes and because f¯∈F has R (f¯)=ε2 . Since
test ∞
F containsapredictorthatdoesnotincurmisspecificationamplification,onemighthopethatmisspecification
amplification can be avoided.
Our first main result is that misspecification amplification cannot be avoided by ERM in the worst case. The
result is proved in the asymptotic regime, where ERM is equivalent to the L 2(D train)-projection of f⋆ onto
the function class F, defined as
fˆ(∞) ∈argmin∥f −f⋆∥2 , with ∥g∥2 :=E (cid:2) g(x)2(cid:3) .
ERM L2(Dtrain) L2(Dtrain) train
f∈F
The next proposition shows that fˆ(∞) can incur misspecification amplification.
ERM
Proposition 2.2 (ERM lower bound). For all ε
∞
∈(0,1) and C
∞
∈[1,∞) such that (cid:112) C ∞·ε
∞
≤1/2, and
for all ζ > 0 sufficiently small, there exist distributions D ,D and a function class F with |F| = 2
train test
satisfying Assumption 2.1-Assumption 2.4 (with parameters ε ,C ) such that
∞ ∞
R (fˆ(∞))=C ε2 −ζ.
test ERM ∞ ∞
Combined with the optimality of the statistical term (Ma et al., 2023;
f
Ge et al., 2023), this establishes that Proposition 2.1 characterizes the bad
behavior of ERM under L ∞-misspecification and covariate shift. The
f¯
construction is based on the following insight, visualized in Figure 1. ε ∞
The fact that f¯is L -close to f⋆ guarantees that its prediction errors f⋆
∞ D
are“spreadout” acrossthedomainX. Sincef¯∈F,weknowthatfˆ(∞) test
ERM X
must satisfy ∥fˆ(∞)−f⋆∥2 ≤ε2 . Unfortunately, this property
D
ERM L2(Dtrain) ∞ train
does not guarantee that the errors of fˆ(∞) are “spread out” in a similar Figure 1: The construction used to
ERM
manner to f¯’s. Indeed, we construct a predictor f bad that concentrates prove Proposition 2.2. f bad and f¯
its errors on a region of X that is amplified by D
test
and makes up for haveequalriskunderD trainbutf
bad
thisbyhavingzeroerrorelsewhere. Bysettingtheparameterscarefully, concentrates errors onto D .
test
we can ensure that this bad predictor is chosen by ERM.
Wenotethatessentiallythesameconstructionshowsthat,undertheweakernotionofL (D )-misspecification,
2 train
amplification is unavoidable for any proper learner (which outputs a function in F). Indeed, in Figure 1, the
function class {f } is L (D )-misspecified but f has much higher error on D .
bad 2 train bad test
1Weconsidertheasymptoticregimewheren→∞withallotherquantities,likelog|F|andε∞,fixed.
4Other existing algorithms. Proposition 2.2 only pertains to ERM, and thus, one might ask whether
other algorithms can avoid misspecification amplification. Before turning to our positive results in the next
section, we briefly note that other standard algorithms (that do not require knowledge of D ) either incur
test
misspecification amplification to some degree, or have some other failure mode. This pertains to the star
algorithm (Audibert, 2007; Liang et al., 2015), other aggregation schemes (c.f., Lecué and Rigollet, 2014),
and L -regression (Knight, 2017; Yi and Neykov, 2024), as we discuss in Appendix A.2. Several methods for
∞
mitigating covariate shift can avoid misspecification amplification, but either require knowledge of D or
test
structural assumptions on F; see Section 5.
2.2 Main result: Disagreement-based regression
In this section, we provide a new algorithm that avoids misspecification amplification while requiring no
knowledge of D and recovering optimal statistical rates. To develop some intuition, observe that in the
test
construction in Figure 1, the only way for the bad predictor (f bad, in red) to be chosen by ERM and have
large errors on D is for it to have much lower error than f¯on the rest of the domain. Indeed, if we could
test
filter out the points where f ’s error is less than f¯’s, then f cannot overcome the large errors on D .
bad bad test
Stated another way, we can avoid misspecification amplification in this example if we restrict the regression
problem to the region where |f (x)−f⋆(x)|≥|f¯(x)−f⋆(x)|.
bad
Generalizing this insight to a larger function class suggests that, when considering a candidate f ∈F, we
should only measure the square loss for f on the region where |f(x)−f⋆(x)|≥|f¯(x)−f⋆(x)|. Unfortunately,
this region depends on f⋆ and f¯, both of which are unknown. Nevertheless, our approach is based on this
intuition, and we avoid the dependence on these unknown functions with two algorithmic ideas.
To eliminate the dependence on f⋆, we use the fact that |f¯(x)−f⋆(x)| ≤ ε and approximate the above
∞
region with I :={x:|f(x)−f¯(x)|≥cε }. Indeed for c≥2,
f ∞
{x:|f(x)−f¯(x)|≥cε }⊆{x:|f(x)−f⋆(x)|≥|f¯(x)−f⋆(x)|}.
∞
On the other hand, we know that |f(x)−f⋆(x)|≤(c+1)ε in the complementary region, IC. This is, up to
∞ f
the constant factor, the best pointwise guarantee we can attain, making it safe to ignore the complementary
region. This resolves the first issue of dependence on f⋆.
To address the dependence on f¯, we use that f¯∈ F and formulate a robust optimization objective that
implicitly considers all possible pairwise “disagreement regions.” Formally, the algorithm is:
n
Wτ (x):=1{|f(x)−g(x)|≥τ}, fˆ(n) ←argminmax 1 (cid:88) Wτ (x)(cid:8) (f(x)−y)2−(g(x)−y)2(cid:9) . (2)
f,g DBR f∈F g∈F n f,g
i=1
We call this algorithm disagreement-based regression (DBR) and keep the dependence on τ implicit in the
notation for the solution fˆ(n).2 There are essentially three key ingredients. First, we introduce the “filter”
DBR
Wτ torestricttheregressionproblemtothesetofpointswherethepredictionsoff andg differconsiderably,
f,g
which we call the disagreement region. This formalizes the intuition that we should only measure the square
loss for f on points where |f(x)−f¯(x)|≥cε . Second is the robust optimization approach, where for each
∞
f ∈F, we consider all possible choices g ∈F for filtering, which allows us to take g to be L -close to f⋆ in
∞
the analysis. Finally, we measure the square loss regret in the disagreement region, by subtracting off the
square loss of the comparator function g. Similar to Agarwal and Zhang (2022), this accounts for the fact
that each g ∈F yields a different regression problem, with potentially different Bayes error rates.3
As our main theorem, we show that disagreement-based regression enjoys the following guarantee.
2The name stems from the literature on disagreement-based active learning (Hanneke, 2014), where a similar “range”
computationhasappeared(Krishnamurthyetal.,2019;Fosteretal.,2018,2021). Howeverourusageisconceptuallyunrelated:
weusedisagreementforrobustnesstocovariateshift,while,inactivelearning,disagreementisusedtoreducesamplecomplexity.
3Moredirectly,theprobabilitymassoffilteredpointsP train[W fτ ,g(x)]couldvaryconsiderablyfordifferentf,g∈F.
5Theorem 2.1 (Main result for DBR). Fix δ ∈(0,1). Let F be a function class with |F|<∞ satisfying As-
sumption 2.3 and Assumption 2.4. Then with probability at least 1−δ, fˆ(n) with τ ≥3ε satisfies
DBR ∞
(cid:104) (cid:110) (cid:111) (cid:110) (cid:111)(cid:105) 160log(2|F|/δ)
E 1 |fˆ(n)(x)−f⋆(x)|≥τ +ε · (fˆ(n)(x)−f⋆(x))2−ε2 ≤ , (3)
train DBR ∞ DBR ∞ 3n
which directly implies
(cid:104)(cid:12) (cid:12) (cid:105) 160log(2|F|/δ)
P (cid:12)fˆ(n)(x)−f⋆(x)(cid:12)≥τ +ε ≤ . (4)
train (cid:12) DBR (cid:12) ∞ 3n(τ2+2τε )
∞
Before turning to a discussion of Theorem 2.1 we state two immediate corollaries. The first addresses the
adversarial covariate shift setting, bounding the risk of fˆ(n) under D .
DBR test
Corollary 2.1 (Covariate shift for DBR). Fix δ ∈ (0,1). Under Assumption 2.1–Assumption 2.4, with
probability at least 1−δ, fˆ(n) with τ =3ε satisfies
DBR ∞
(cid:18) (cid:19)
log(|F|/δ)
R (fˆ(n))≤17ε2 +O C . (5)
test DBR ∞ ∞ n
The next result shows that fˆ(n) recovers the optimal guarantee in the well-specified case, i.e., when ε =0.
DBR ∞
Corollary 2.2 (Well-specified case). Fix δ ∈(0,1). Under Assumption 2.1–Assumption 2.4 (with ε =0),
∞
with probability at least 1−δ, fˆ(n) with τ
≤O(cid:16)(cid:112) log(|F|/δ)/n(cid:17)
satisfies
DBR
(cid:18) (cid:19) (cid:18) (cid:19)
log(|F|/δ) log(|F|/δ)
R (fˆ(n))≤O and R (fˆ(n))≤O C . (6)
train DBR n test DBR ∞ n
We now turn to some remarks regarding Theorem 2.1 and the corollaries.
DBR avoids misspecification amplification Comparing Corollary 2.1 in the n→∞ limit with Proposi-
tion 2.2 highlights the main qualitative difference between DBR and ERM. DBR attains O(ε2 ) asymptotic test
∞
errorwhilethetesterrorforERMislowerboundedbyΩ(C ε2 ). Inotherwords, DBRavoidsmisspecification
∞ ∞
amplification while ERM does not. At the same time, the statistical term is identical (up to constants) to
that of ERM, enabling us to recover the optimal rate in the well-specified case.
Quantile guarantee Taking τ =O(ε ) in Eq. (3), we have that P [|fˆ(n)(x)−f⋆(x)|≥cε ]≲ 1 ,
∞ train DBR ∞ nε2
which controls the large quantiles of the prediction error. This is reminiscent of what can be achieved b∞y
applying Markov’s inequality to the guarantee for ERM in the well-specified case. In contrast, ERM only
ensures that R (fˆ(n)) = Ω(ε2 ) under misspecification, which does not imply any meaningful quantile
train ERM ∞
guarantee. One interpretation of our results is that, although such quantile guarantees are not possible for
ERM under misspecification, there is no information-theoretic obstruction. We also note that these quantile
guarantees are rather different from sup-norm convergence; see Section 5 for further discussion.
Computational efficiency DBR, as described in Eq. (2), does not appear to be computationally tractable,
primarily due to the non-smoothness and non-convexity introduced by the filter W . A natural direction for
f,g
future work is to understand the computational challenges involved in avoiding misspecification amplification.
2.2.1 Extensions
Before closing this section, we mention two extensions that we defer to Appendix A.4.
• Approximation factor. The approximation factor of 17 in Corollary 2.1 can be improved to 10 (cf.
Proposition A.1); however our approach for doing so degrades the convergence rate of the statistical
term. We do not know the optimal approximation factor for this setting or whether there is an inherent
trade-off between the statistical term and the approximation/misspecification term.
6• Adapting to unknown misspecification. Theorem 2.1 requires setting τ ≥ 3ε which can always be
∞
achievedbysettingτ sufficientlylarge. However,settingτ =O(ε )yieldsthebestguarantee,andso,we
∞
wouldliketochooseτ inadata-dependentfashiontoadapttothemisspecificationlevel. PropositionA.2
shows that this can be done while recovering essentially the same guarantee as in Theorem 2.1.
3 Proof of Theorem 2.1
This section contains the proof of Theorem 2.1—which we emphasize only requires elementary arguments—
and is not essential for understanding the main results of the paper. A reader interested in applications
of Theorem 2.1 to reinforcement learning can proceed to Section 4.
The proof of Theorem 2.1 is organized into three steps, each of which is fairly simple. It is helpful to define
empirical and population versions of the pairwise objective used by DBR:
n
(Empirical): L(cid:98)(f;g):= n1 (cid:88) W fτ ,g(x i)(cid:8) (f(x i)−y i)2−(g(x i)−y i)2(cid:9) ,
i=1
(Population): L(f;g):=E (cid:2) Wτ (x)(cid:8) (f(x)−y)2−(g(x)−y)2(cid:9)(cid:3) .
train f,g
First, we establish a certain non-negativity property of the population objective, which is the main structural
result. The second step is a uniform convergence argument to show that L(cid:98)(·;·), which appears in the
algorithm, concentrates to the population counterpart L(·;·). Finally, we study the minimizer fˆ(n) and an
DBR
L -approximation f¯and relate their objective values to establish the theorem. Details and proofs for the
∞
corollaries are deferred to Appendix A.3.
Step 1: Non-negativity. The key lemma for the analysis is the following structural property.
Lemma 3.1 (Non-negativity). With τ ≥2ε and for any f¯∈F such that ∥f¯−f⋆∥ ≤ε , we have
∞ ∞ ∞
L(f;f¯)≥(τ2−2τε )Pr[Wτ (x)]≥0.
∞ f,f¯
The proof requires only algebraic manipulations and actually reveals a stronger property: with τ ≥2ε , the
∞
randomvariableWτ (x)(cid:2) (f(x)−f⋆(x))2−(f¯(x)−f⋆(x))2(cid:3)isnon-negativealmostsurely. Bythesymmetry
f,f¯
L(f;g)=−L(g;f), the lemma also shows that any L -misspecified f¯has non-positive population objective.
∞
Step 2: Uniform convergence. Next we establish the following concentration guarantee.
Lemma 3.2 (Concentration). Fix δ ∈(0,1) and τ ≥3ε and define ε := 80log(|F|/δ). Under Assumption
∞ stat 3n
2.3, for any f¯∈F such that ∥f¯−f⋆∥ ≤ε , with probability at least 1−δ we have
∞ ∞
∀f ∈F : L(f;f¯)≤2L(cid:98)(f;f¯)+ε stat, and equivalently, L(cid:98)(f¯;f)≤ 1 2(cid:0) L(f¯;f)+ε stat(cid:1) .
TheproofisbasedonBernstein’sinequalityandimportantlyexploitsa“self-bounding” propertyofL(cid:98)(f;g)—in
particular that Var[L(cid:98)(f;f¯)]≤(12/n)L(f;f¯)—analogously to the analysis for ERM in the well-specified case.
Step 3: Analysis of fˆ(n). Let f¯∈F be any function that is L -close to f⋆ and condition on the high
DBR ∞
probability event in Lemma 3.2 holding with the choice f¯. The DBR minimizer satisfies
(i) (ii)
L(fˆ D(n BR);f¯)≤2L(cid:98)(fˆ D(n BR);f¯)+ε
stat
≤2maxL(cid:98)(fˆ D(n BR);g)+ε
stat
g∈F
(iii) (iv) (v)
≤2maxL(cid:98)(f¯;g)+ε
stat
≤ maxL(f¯;g)+2ε
stat
≤2ε stat.
g∈F g∈F
7Here inequalities (i) and (iv) are applications of Lemma 3.2, (ii) and (iii) follow from the definition of fˆ(n)
DBR
since f¯∈F, and (v) is an application of Lemma 3.1 along with the symmetry L(f;g)=−L(g;f). Eq. (3)
now follows from the fact that Wτ (x)≥1{|f(x)−f⋆(x)|≥τ +ε }. Eq. (4) follows since under the event
f,f¯ ∞
|f(x)−f⋆(x)|≥τ +ε we can lower bound (f(x)−f⋆(x))2−(f¯(x)−f⋆(x))2 ≥(τ +ε )2−ε2 .
∞ ∞ ∞
4 Applications to online and offline reinforcement learning
In this section, we deploy disagreement-based regression to obtain new results in offline and online RL
with function approximation. Algorithmically, this is achieved by using DBR as a drop-in replacement for
square loss regression in existing algorithms. We illustrate this by examining and improving the Bellman
residual minimization (a.k.a. minimax) algorithm for offline RL (Antos et al., 2008; Chen and Jiang, 2019)
(Section 4.1) and the GOLF algorithm (Jin et al., 2021) for online RL (Section 4.2). The analyses also require
minimal modifications to those of Xie and Jiang (2021) and Xie et al. (2023), respectively. To emphasize the
ease with which DBR can be applied, we adopt the formulations and much of the notation from these works.
All proofs for results in this section are deferred to Appendix B.
4.1 Offline reinforcement learning
Setup and notation. We consider a discounted Markov decision process (MDP) M =(P,R,d ,γ) over
0
states S and actions A, where P :S×A→∆(S) is the transition operator, R:S×A→[0,1] is the reward
function,d ∈∆(S)istheinitialstatedistribution,andγ ∈[0,1)isthediscountfactor. Apolicyπ :S →∆(A)
0
induces a trajectory s ,a ,r ,s ,a ,r ,... where s ∼ d , and for each h ∈ N, a ∼ π(s ), r = R(s ,a ),
0 0 0 1 1 1 0 0 h h h h h
and s ∼P(s ,a ). We use Pπ[·] and Eπ[·] to denote probability and expectation under this process. Let
h+1 h h
dπ ∈∆(S×A) denote the occupancy measure of π at time-step h, defined as dπ(s,a):=Pπ[s =s,a =a]
h h h h
and let dπ :=(1−γ)(cid:80)∞ γhdπ.
h=0 h
The value of π is denoted J(π) := Eπ(cid:2)(cid:80)∞ γhr (cid:3). Each policy π has value functions Vπ : s (cid:55)→
h=0 h
Eπ(cid:2)(cid:80)∞ γhr |s =s(cid:3) and Qπ :(s,a)(cid:55)→Eπ(cid:2)(cid:80)∞ γhr |s =s,a =a(cid:3), and it is known that there exists
h=0 h 0 h=0 h 0 0
a policy π⋆ that maximizes Vπ(s) simultaneously for all s∈S. This policy also optimizes J(·) and hence is
called the optimal policy. It is also known that the value function Q⋆ :=Qπ⋆ induces the optimal policy via
π⋆ :s(cid:55)→argmax Q⋆(s,a) and additionally satisfies Bellman’s optimality equation: Q⋆(s,a):=[TQ⋆](s,a)
a
where T is the Bellman operator, defined via Tf :(s,a)(cid:55)→E[r +γmax f(s ,a′)|s =s,a =a].
0 a′ 1 0 0
Intheofflinevaluefunctionapproximationsetting,wearegivenadatasetofntuplesD :={(s ,a ,r ,s′)}n
n i i i i i=1
generated i.i.d. from the following process: (s ,a )∼µ where µ∈∆(S,A) is the data collection distribution,
i i
r =R(s ,a ), and s′ ∼P(s ,a ). We are also given a function class F ⊂(S ×A→R), where each f ∈F
i i i i i i
induces the policy π :s(cid:55)→argmax f(s,a). Given dataset D and function class F, we seek a policy πˆ that
f a n
has small suboptimality gap: J(π⋆)−J(πˆ). We impose the following assumptions on the function class and
on the data collection distribution:
• L -misspecified realizability/completeness: There exists f¯∈ F such that ∥f¯−Tf¯∥ ≤ ε .
∞ ∞ ∞
Additionally, for any f ∈F there exists g ∈F such that ∥g−Tf∥ ≤ε .
∞ ∞
(cid:13) (cid:13)
• Concentrability: There exists a constant C ∈ [1,∞) such that max (cid:13)dπ(cid:13) ≤ C . Here
conc π∈Π(cid:13) µ (cid:13) conc
∞
Π:={π :f ∈F} is the policy class induced by F.
f
There is a large body of recent work studying various function approximation and coverage assumptions in
offline RL (c.f., Xie and Jiang, 2021). Arguably the most standard are concentrability, as we use, and exact
realizability/completeness, which is stronger than our version with misspecification. Regarding the function
approximation assumption, it is not hard to show that misspecification amplification—which in this setting
√
is defined by the suboptimality J(π⋆)−J(πˆ) scaling as Ω(ε C )—is necessary under weaker notions,
∞ conc
such as L (µ)-misspecification. Regarding coverage, as we will discuss below, the strength of the coverage
2
assumption determines whether misspecification amplification can be avoided or not.
8Algorithm and guarantee. Thealgorithmwestudyisaminormodificationtotheminimaxalgorithm(An-
tos et al., 2008; Chen and Jiang, 2019). For each function f˜∈F and each tuple (s ,a ,r ,s′) we can form a
i i i i
regression sample (s ,a ,y :=r +γmax f˜(s′,a′)) and define the predictor fˆvia the objective:
i i f˜,i i a′ i
n
fˆ:=argminmax 1 (cid:88) Wτ (s ,a )(cid:8) (f(s ,a )−y )2−(g(s ,a )−y )2(cid:9) . (7)
f∈F g∈F n f,g i i i i f,i i i f,i
i=1
HereWτ (·)isthefilterinEq. (2)withx=(s,a). Givenfˆ, weoutputπˆ :=π . Notethattheonlydifference
f,g fˆ
between this algorithm and the original minimax algorithm is the use of the filter Wτ (·) which is essential
f,g
for obtaining the following guarantee.
Theorem 4.1 (DBR for offline RL). Fix δ ∈ (0,1), assume that F is L ∞-misspecified and µ satisfies
concentrability (as defined above). Consider the algorithm defined in Eq. (7) with τ = 3ε . Then, with
∞
probability at least 1−δ we have
(cid:32) (cid:114) (cid:33)
ε 1 log(|F|/δ)
J(π⋆)−J(πˆ)≤O ∞ + C .
1−γ 1−γ conc n
The theorem is best understood via comparison to the guarantee for the standard minimax algorithm, e.g.,
Theorem 5 of Xie and Jiang (2020). Under our assumptions (L -misspecification and concentrability), these
∞
two bounds differ only in the misspecification term: our theorem scales as ε /(1−γ) while the guarantee for
√ ∞
the minimax algorithm scales as ε C /(1−γ).4 Thus, our algorithm inherits the favorable properties of
∞ conc
DBR to avoid misspecification amplification in offline RL.
This feature is notable in light of existing lower bounds for misspecified RL (Du et al., 2020; Van Roy and
Dong, 2019; Lattimore et al., 2020). Formally, these results consider linear function approximation in various
online RL models, but the constructions can be extended to offline RL with general function approximation
where coverage is measured via the Bellman transfer coefficient. This coefficient is the smallest C
transfer
such that max
π,f∈F
∥ ∥f f− −a ap px x[ [f f] ]∥ ∥2 L 22(dπ) ≤C
transfer
where apx[f]∈F is the L ∞-approximation of Tf.5 The lower
L2(µ) √
bound states that an asymptotic error of Ω(ε C ) is unavoidable.
∞ transfer
To contextualize our result with this lower bound, we identify two regimes: the “Bellman transfer regime”
whereC <∞andthe“concentrabilityregime” whereC <∞, andnotethat, sinceC ≤C ,
transfer conc transfer conc
the former is more general. In the Bellman transfer regime, misspecification amplification is unavoidable. In
the concentrability regime, Theorem 4.1 avoids misspecification amplification and is sample efficient (i.e.,
has statistical term scaling as poly(C ,log(|F|/δ), 1, 1 )). This is the first result showing that both of
conc n 1−γ
these properties are simultaneously achievable: prior results achieve sample efficiency with misspecification
amplification (e.g., Xie and Jiang, 2020), or avoid misspecification amplification with undesirable sample
complexity scaling as poly(|S|) (the latter is easily achieved under concentrability via a tabular model-based
approach). Thus, the regime determines whether misspecification amplification is avoidable or not, and, in
the regime where it is avoidable, our algorithm does so in a sample-efficient manner.
4.2 Online reinforcement learning
Setup and notation. We consider a finite horizon episodic MDP (P,R,H,s ) over state space S and
1
action space A, where H ∈ N is horizon, P := {P }H with P : S ×A → ∆(S) is the non-stationary
h h=1 h
4XieandJiang(2020)considerslightlyweakerassumptions: theymeasurebothmisspecificationandconcentrabilityviathe
L2(µ)norm. OuranalysiseasilyaccommodatesL2(µ)-concentrability,ascanbeseenfromtheproof. Ontheotherhand,as
describedinSection2.1,misspecificationamplificationisnecessaryunderL2(µ)-misspecification.
5Many Bellman transfer coefficients exist, but a standard one is the smallest C such that
transfer
max π,f∈F∥f−Tf∥2 L2(dπ)/∥f−Tf∥2
L2(µ)
≤ C transfer. This coincides with ours under exact realizability/completeness,
butwebelieveourdefinitionismoreappropriateforthemisspecifiedcasebecauseitisequivalenttofeaturecoverageunder
linear function approximation. Indeed, if F consists of linear functions in some feature map ϕ:S×A→Rd (but Tf may
notbelinearduetomisspecification)thenourdefinitioncanbeexpressedviathefeatures(asmax π,θ∈Rdθ⊤Σπθ/θ⊤Σµθ where
Σ =E (cid:2) ϕ(s,a)ϕ(s,a)⊤(cid:3) )butthestandarddefinitioncannot.
d d
9transition operator, R:={R }H with R :S×A→[0,1] is the non-stationary reward function, and s is a
h h=1 h 1
fixed starting state. A (non-stationary) policy π :={π }H is a sequence of mappings π :S →∆(A) which
h h=1 h
inducesatrajectory(s ,a ,r ,...,s ,a ,r )wherea ∼π (s ),r =R (s ,a )ands ∼P (s ,a )for
1 1 1 H H H h h h h h h h h+1 h h h
each time step. We use Pπ[·] and Eπ[·] to denote probability and expectation under this process, respectively.
Letdπ ∈∆(S×A)denotetheoccupancymeasureofπattime-steph,definedasdπ(s,a):=Pπ[s =s,a =a].
h h h h
(cid:104) (cid:105)
The value of policy π is denoted J(π) := Eπ (cid:80)H r . Each policy has value functions: Vπ : s (cid:55)→
h=1 h h
(cid:104) (cid:105) (cid:104) (cid:105)
Eπ (cid:80)H r |s =s and Qπ :(s,a)(cid:55)→Eπ (cid:80)H r |s =s,a =a and there exist an optimal policy
h′=h h′ h h h′=h h′ h h
π⋆ = {π⋆}H that maximizes Vπ simultaneously for each state s ∈ S and hence maximizes J(·). The
h h=1 h
optimal value function Q⋆ := Qπ h⋆ induces π⋆ via π⋆ : s (cid:55)→ argmax Q⋆(s,a) and satisfies Bellman’s
h h h a h
equation: Q⋆(s,a)=[T Q⋆ ](s,a) where the Bellman operator T is defined via [T f ](s,a)=R (s,a)+
h h h+1 h h h+1 h
E[max f (s ,a′)|s =s,a =a]. We assume per-episode rewards satisfy (cid:80)H r ∈[0,1].
a′ h+1 h+1 h h h=1 h
InonlineRL,weinteractwiththeMDPforT episodes,whereineachepisodeweselectapolicyπ(t) andcollect
the trajectory (s(t),a(t),r(t),...,s(t),a(t),r(t)) by taking actions a(t) =π(t)(s(t)). We measure performance via
1 1 1 H H H h h h
the cumulative regret, define as Reg :=(cid:80)T J(π⋆)−J(π(t)). We equip the learner with a value function
t=1
class F :=F ×...×F where each F ⊂S×A→[0,1]. Each f ∈F induces a policy π which, at time
1 H h f
step h takes actions via π (s )=argmax f (s ,a ). We make the following assumptions:
f,h h a h h h
• L -approximate realizability/completeness. For each h ∈ [H] there exists f¯ ∈ F such
∞ h h
that (cid:13) (cid:13)f¯ h−T hf¯ h+1(cid:13) (cid:13) ∞ ≤ ε ∞.Additionally, for each f h+1 ∈ F h+1 there exists f h ∈ F h such that
∥f −T f ∥ ≤ε .
h h h+1 ∞ ∞
(cid:13) (cid:13)
• Coverability. There exists a constant C cov ∈[1,∞) such that inf µ1,...,µH∈∆(S×A)sup π∈Π,h(cid:13) (cid:13)µdπ h h(cid:13) (cid:13)
∞
≤
C . Here Π:={π :π (s)=argmax f (s,a),f ∈F} is the policy class induced by F.
cov f f,h a h
AsinofflineRL,thereisalargebodyofrecentworkstudyingfunctionapproximationandstructuralconditions
for sample-efficient online RL (c.f., Agarwal et al., 2019; Foster and Rakhlin, 2023). It is fairly standard
to assume exact realizability and completeness, which is stronger than our version with misspecification.
Coverability is a recently proposed structural condition (Xie et al., 2023): C is known to be small in many
cov
MDP models of interest, but weaker conditions that enable sample-efficiency are known. As we will see, the
strength of the structural condition determines whether misspecification amplification can be avoided or not.
Algorithm and guarantee. The algorithm is a very minor modification to GOLF (Jin et al., 2021; Xie
et al., 2023). To condense the notation, given a sample (s(i),a(i),r(i),s(i) ) and a function f′ ∈F , define
h h h h+1 h+1
x(i) :=(s(i),a(i)) and y(i) :=r(i)+max f′(s(i) ,a′). At the beginning of episode t, define a version space
h h h f′,h h a′ h+1
(cid:40) t−1 (cid:41)
(cid:88) (cid:110) (cid:111)
F(t−1) := f ∈F :∀h∈[H]: max Wτ (x(i)) (f (x(i))−y(i) )2−(g (x(i))−y(i) )2 ≤β ,
gh∈Fhi=1
fh,gh h h h fh+1,h h h fh+1,h
where β >0 is a hyperparameter we will set below.
Then, we define the optimistic value function f(t) :=argmax f (s ,π (s )) and the induced policy
f∈F(t−1) 1 1 f,1 1
π(t) := π , collect a trajectory via π(t), and proceed to the next episode. Note that the only difference
f(t)
between this algorithm, which we call GOLF.DBR, and the version of GOLF studied by Xie et al. (2023) is that
weusethefilterWτ (·)intheconstructionoftheversionspace. GOLF.DBRenjoysthefollowingguarantee.
fh,gh
Theorem 4.2 (DBR for online RL). Fix δ ∈(0,1), and assume that F is L ∞-misspecified and µ satisfies
coverability (as defined above). Consider GOLF.DBR with τ = 3ε ∞ and β = clog(TH|F|/δ). Then, with
probability at least 1−δ, we have
(cid:16) (cid:112) (cid:17)
Reg≤O ε HT +H C T log(TH|F|/δ)log(T) .
∞ cov
Paralleling the discussion following Theorem 4.1, we emphasize two aspects of the result. The first is that it
extends Theorem 1 of Xie et al. (2023) to the misspecified setting, with no degradation of the statistical term
√
and without incurring a dependence on ε C . In other words, it avoids misspecification amplification.
∞ cov
10The second remark is that, when taken with existing lower bounds (Du et al., 2020; Van Roy and Dong, 2019;
Lattimore et al., 2020), Theorem 4.2 establishes a separation between coverability and structural parameters
defined in terms of Bellman errors, which include the Bellman-Eluder dimension (Jin et al., 2021), bilinear
rank (Du et al., 2021), and Bellman rank (Jiang et al., 2017).6 This separation is more subtle than in offline
RL, because here, as long as the state-action space is finite, one can always use a “tabular” method and
√
eliminate misspecification altogether, at the cost of poly(|S|,|A|)· T regret. To rule out this algorithm, we
restrict to sample-efficient methods: in a setting where a particular structural parameter (e.g., coverability or
Bellman rank) is bounded by d we say that an algorithm is sample-efficient if its statistical term scales as
poly(d,log(|F|/δ),H)·o(T). The lower bounds show that, when the structural parameter involves Bellman
√
errors(liketheBellmanrank), ε T dmisspecificationerrorisnecessaryforsampleefficientalgorithms.7 On
∞
theotherhand, undercoverability, wecanachievemisspecificationerrorwithnodependenceonthestructural
parameter, in a sample efficient manner.8 This establishes that whether misspecification amplification can be
avoided sample-efficiently depends on the structural properties of the MDP. To our knowledge, this is a novel
insight into the interaction between the structural and function approximation assumptions in online RL.
5 Related work
There is a vast body of work studying distribution shift broadly and covariate shift in particular. We focus on
the most closely related techniques for the covariate shift setting and refer the reader to Quinonero-Candela
et al. (2008); Sugiyama and Kawanabe (2012); Shen et al. (2021) for a more comprehensive treatment.
Reweighting and robust optimization. Perhaps the most common way to correct for covariate shift is
by reweighting each example (x,y) in the objective function by the density ratio w(x):=d (x)/d (x).
test train
This method has been studied in a long series of works (Shimodaira, 2000; Cortes et al., 2010; Cortes and
Mohri, 2014). In its simplest form it requires knowledge of D via the density ratios, so it is not directly
test
applicable to our adversarial covariate shift setting. Extensions include approaches that estimate density
ratios using unlabeled samples from D (Huang et al., 2006; Sugiyama et al., 2007; Gretton et al., 2009;
test
Yu and Szepesvári, 2012) and robust optimization approaches that employ an auxiliary hypothesis class of
distributions P containing D (Hashimoto et al., 2018; Sagawa et al., 2020; Duchi and Namkoong, 2021;
test
Agarwal and Zhang, 2022). However, these still require prior knowledge about D , in particular it is known
test
that the sample complexity of robust optimization scales with the statistical complexity of the auxiliary class
P (Duchi and Namkoong, 2021), leading to vacuous bounds in the absence of inductive bias.
Ge et al. (2023) study statistical inference under covariate shift in well- and misspecified settings. They
show that maximum likelihood estimation on D is inconsistent with misspecification, a result which is
train
conceptually similar to our lower bound for ERM. However, their construction is not L ∞-misspecified so it is
not directly comparable. Algorithmically, they use reweighting for the misspecified case, which, as mentioned,
cannot be implemented in our setting.
Sup-norm convergence and function class-specific results. Another line of work provides specialized
analyses for specific function classes of interest, such as linear (Lei et al., 2021), nonparametric (Kpotufe
and Martinet, 2018; Pathak et al., 2022; Ma et al., 2023), and some neural network (Dong and Ma, 2023a)
classes. The overarching technical approach in these works is to measure distance between distributions in
manner that captures the structure of the function class, analogously to learning-theoretic results for domain
adaptation (Ben-David et al., 2006; Mansour et al., 2009). A complementary approach is based on sup-norm
convergence which seeks to control ∥fˆ−f⋆∥ for a predictor fˆand is naturally robust to covariate shift.
∞
Sup-normconvergencehasbeenstudiedforvariousfunctionclasses(c.f.,Schmidt-HieberandZamolodtchikov,
2022; Dong and Ma, 2023b), but unfortunately is not possible in the general statistical learning setup (Dong
and Ma, 2023b). We mention sup-norm convergence primarily to contrast with our quantile guarantee in Eq.
(4), which controls the probability over x of large errors rather than the magnitude of the errors themselves
6AswithBellmantransfercoefficients,webelievethesedefinitionsshouldbeadjustedtoaccommodatemisspecification. See
Definition10inJiangetal.(2017)foranexample.
7Formally,foranyζ>0onerequiresatleastexp(d2ζ)samplestofindad1/2−ζε∞ suboptimal √policy(Lattimoreetal.,2020).
8Webelievethatmisspecificationerrorε∞HT isoptimalundercoverabilityandthatε∞HT disoptimalunderstructural
parameterslikeBellmanrank. However,itremainsopentoestablishthenecessityofthehorizonfactors.
11and which is attainable for any function class, even with misspecification. All of these works differ from ours
in that (a) they consider specific function classes and (b) they operate closer to the well-specified regime than
we do (e.g., in the nonparametric setting, one can drive the misspecification error to zero).
Related work in reinforcement learning. Our results for offline and online RL build directly on the
analyses in Xie and Jiang (2020) and Xie et al. (2023) respectively. The former contributes to a long line
of work on offline RL (Munos, 2003, 2007; Antos et al., 2008; Chen and Jiang, 2019) while the latter is
part of a series of works establishing structural conditions under which online reinforcement learning is
statisticallytractable(c.f.,Agarwaletal.,2019;FosterandRakhlin,2023). Manyoftheseworksdoaccountfor
misspecification, but the question of whether misspecification amplification can be avoided is not considered.
Results that do focus on misspecification primarily consider linear function approximation. In the simpler
offlinepolicyevaluationsetting,severalworksstudyleastsquarestemporaldifferencelearning(LSTD)(Bradtke
and Barto, 1996) with misspecification (Tsitsiklis and Van Roy, 1996; Yu and Bertsekas, 2010; Mou et al.,
2022). Recently, Amortila et al. (2023) precisely characterized the optimal misspecification amplification (i.e.,
approximation factors) achievable across a range of settings, showing that LSTD is essentially optimal in
most regimes. The exception is when the offline data distribution is supported on the entire state space,
one can employ a “tabular” model-based algorithm to incur no approximation error whatsoever, but the
sample complexity scales polynomially with |S|. Our offline RL results are conceptually similar because under
concentrability (which essentially implies full support), the standard minimax algorithm does not achieve
the optimal approximation factor. A crucial difference is that our disagreement-based variant achieves an
improved approximation factor without incurring any sample complexity overhead.
For the more challenging offline policy optimization and online RL, Du et al. (2020); Van Roy and Dong
(2019); Lattimore et al. (2020) establish conditions under which misspecification amplification is necessary.
As discussed above, combining our results with these lower bounds and their variations, reveals new tradeoffs
between coverage/structural and function approximation conditions, distinct from tradeoffs established by
prior work (Xie and Jiang, 2021; Foster et al., 2022).
6 Discussion
This paper highlights an intriguing interplay between misspecification and distribution shift, exposing the
undesirable misspecification amplification property of ERM, and proposing disagreement-based regression as a
remedy. We have shown that using disagreement-based regression in online and offline reinforcement learning
yields new technical results and reveals new tradeoffs between coverage/structural assumptions and function
approximation assumptions.
We close by mentioning several interesting avenues for future work. There are a number of directions that
pertain to the core setting of misspecified regression under covariate shift; for example, (a) extending the
analysis of DBR to infinite function classes, other loss functions, and other notions of misspecification, (b)
derivingamorecomputationallyefficientprocedure—perhapsinanoraclemodelofcomputation—thatavoids
misspecification amplification, and (c) determining the optimal achievable approximation factor. Pertaining
to reinforcement learning theory, we believe the most pressing direction is to deepen our understanding of
the relationship between coverage/structural assumptions (for offline/online RL, respectively) and function
approximationassumptions,andwebelievemisspecificationprovidesanovellenstostudythisrelationship. It
is also worthwhile to consider other applications involving distribution shift where DBR or related procedures
may reveal new conceptual insights. Finally, it would also be interesting to study empirical issues, to
understand how pervasive and problematic misspecification amplification is, develop practical interventions,
and consider applying them to distribution shift and deep reinforcement learning scenarios.
In short, there is much more to understand about the interplay between misspecification and distribution
shift, and we look forward to progress in the years to come.
Acknowledgements
We thank Adam Block for helpful feedback on a early version of the manuscript.
12References
Alekh Agarwal and Tong Zhang. Minimax regret optimization for robust machine learning under distribution
shift. In Conference on Learning Theory, 2022.
Alekh Agarwal, Nan Jiang, Sham M Kakade, and Wen Sun. Reinforcement learning: Theory and algorithms.
https://rltheorybook.github.io/, 2019. Version: January 31, 2022.
Philip Amortila, Nan Jiang, and Csaba Szepesvári. The optimal approximation factors in misspecified
off-policy value function estimation. In International Conference on Machine Learning, 2023.
CemAnil,YuhuaiWu,AndersAndreassen,AitorLewkowycz,VedantMisra,VinayRamasesh,AmbroseSlone,
Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length generalization in large language
models. Advances in Neural Information Processing Systems, 2022.
András Antos, Csaba Szepesvári, and Rémi Munos. Learning near-optimal policies with bellman-residual
minimization based fitted policy iteration and a single sample path. Machine Learning, 2008.
Jean-Yves Audibert. Progressive mixture rules are deviation suboptimal. Advances in Neural Information
Processing Systems, 2007.
Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain
adaptation. Advances in Neural Information Processing Systems, 19, 2006.
Omar Besbes, Yonatan Gur, and Assaf Zeevi. Non-stationary stochastic optimization. Operations Research,
2015.
Steven J Bradtke and Andrew G Barto. Linear least-squares algorithms for temporal difference learning.
Machine learning, 1996.
Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. In
International Conference on Machine Learning, 2019.
Corinna Cortes and Mehryar Mohri. Domain adaptation and sample bias correction theory and algorithm for
regression. Theoretical Computer Science, 2014.
Corinna Cortes, Yishay Mansour, and Mehryar Mohri. Learning bounds for importance weighting. Advances
in Neural Information Processing Systems, 2010.
Kefan Dong and Tengyu Ma. First steps toward understanding the extrapolation of nonlinear models to
unseen domains. In International Conference on Learning Representations, 2023a.
Kefan Dong and Tengyu Ma. Toward L -recovery of nonlinear functions: A polynomial sample complexity
∞
bound for gaussian random fields. In Conference on Learning Theory, 2023b.
Simon Du, Sham Kakade, Jason Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and Ruosong Wang.
Bilinear classes: A structural framework for provable generalization in RL. In International Conference on
Machine Learning, 2021.
Simon S Du, Sham M Kakade, Ruosong Wang, and Lin F Yang. Is a good representation sufficient for sample
efficient reinforcement learning? In International Conference on Learning Representations, 2020.
John C Duchi and Hongseok Namkoong. Learning models with uniform performance via distributionally
robust optimization. The Annals of Statistics, 2021.
Dylan Foster, Alekh Agarwal, Miroslav Dudík, Haipeng Luo, and Robert Schapire. Practical contextual
bandits with regression oracles. In International Conference on Machine Learning, 2018.
DylanJFosterandAlexanderRakhlin. Foundationsofreinforcementlearningandinteractivedecisionmaking.
arXiv:2312.16730, 2023.
Dylan J Foster, Alexander Rakhlin, David Simchi-Levi, and Yunzong Xu. Instance-dependent complexity
of contextual bandits and reinforcement learning: A disagreement-based perspective. In Conference on
Learning Theory, 2021.
13Dylan J Foster, Akshay Krishnamurthy, David Simchi-Levi, and Yunzong Xu. Offline reinforcement learning:
Fundamental barriers for value function approximation. In Conference on Learning Theory, 2022.
João Gama, Indre˙ Žliobaite˙, Albert Bifet, Mykola Pechenizkiy, and Abdelhamid Bouchachia. A survey on
concept drift adaptation. ACM Computing Surveys, 2014.
Jiawei Ge, Shange Tang, Jianqing Fan, Cong Ma, and Chi Jin. Maximum likelihood estimation is all you
need for well-specified covariate shift. arXiv:2311.15961, 2023.
ArthurGretton,AlexSmola,JiayuanHuang,MarcelSchmittfull,KarstenBorgwardt,andBernhardSchölkopf.
Covariate shift by kernel mean matching. Dataset Shift in Machine Learning, 2009.
Steve Hanneke. Theory of disagreement-based active learning. Foundations and Trends in Machine Learning,
2014.
Tatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness without demo-
graphics in repeated loss minimization. In International Conference on Machine Learning, 2018.
Jiayuan Huang, Arthur Gretton, Karsten Borgwardt, Bernhard Schölkopf, and Alex Smola. Correcting
sample selection bias by unlabeled data. Advances in Neural Information Processing Systems, 2006.
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contextual
decision processes with low Bellman rank are PAC-learnable. In International Conference on Machine
Learning, 2017.
Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of RL problems,
and sample-efficient algorithms. Advances in Neural Information Processing Systems, 2021.
Keith Knight. On the asymptotic distribution of the L estimator in linear regression. Technical report,
∞
University of Toronto, 2017.
Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani,
WeihuaHu,MichihiroYasunaga,RichardLanasPhillips,IrenaGao,TonyLee,EtienneDavid,IanStavness,
Wei Guo, Berton Earnshaw, Imran Haque, Sara M Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson,
Sergey Levine, Chelsea Finn, and Percy Liang. Wilds: A benchmark of in-the-wild distribution shifts. In
International Conference on Machine Learning, 2021.
Samory Kpotufe and Guillaume Martinet. Marginal singularity, and the benefits of labels in covariate-shift.
In Conference On Learning Theory, 2018.
Akshay Krishnamurthy, Alekh Agarwal, Tzu-Kuo Huang, Hal Daumé III, and John Langford. Active learning
for cost-sensitive classification. Journal of Machine Learning Research, 2019.
Tor Lattimore, Csaba Szepesvari, and Gellert Weisz. Learning with good feature representations in bandits
and in RL with a generative model. In International Conference on Machine Learning, 2020.
Guillaume Lecué and Philippe Rigollet. Optimal learning with Q-aggregation. The Annals of Statistics, 2014.
Qi Lei, Wei Hu, and Jason Lee. Near-optimal linear regression under distribution shift. In International
Conference on Machine Learning, 2021.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review,
and perspectives on open problems. arXiv:2005.01643, 2020.
Tengyuan Liang, Alexander Rakhlin, and Karthik Sridharan. Learning with square loss: Localization through
offset rademacher complexity. In Conference on Learning Theory, 2015.
Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Exposing attention
glitches with flip-flop language modeling. Advances in Neural Information Processing Systems, 2023.
Cong Ma, Reese Pathak, and Martin J Wainwright. Optimally tackling covariate shift in RKHS-based
nonparametric regression. The Annals of Statistics, 2023.
14Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and
algorithms. arXiv:0902.3430, 2009.
John P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar, Percy
Liang, Yair Carmon, and Ludwig Schmidt. Accuracy on the line: On the strong correlation between
out-of-distribution and in-distribution generalization. In International Conference on Machine Learning,
2021.
WenlongMou,AshwinPananjady,andMartinJWainwright. Optimaloracleinequalitiesforsolvingprojected
fixed-point equations. Mathematics of Operations Research, 2022.
Rémi Munos. Error bounds for approximate policy iteration. In International Conference on Machine
Learning, 2003.
Rémi Munos. Performance bounds in L -norm for approximate value iteration. SIAM Journal on Control
p
and Optimization, 2007.
ReesePathak,CongMa,andMartinWainwright.Anewsimilaritymeasureforcovariateshiftwithapplications
to nonparametric regression. In International Conference on Machine Learning, 2022.
Juan Perdomo, Tijana Zrnic, Celestine Mendler-Dünner, and Moritz Hardt. Performative prediction. In
International Conference on Machine Learning, 2020.
Joaquin Quinonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset Shift in
Machine Learning. Mit Press, 2008.
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize
to imagenet? In International Conference on Machine Learning, 2019.
Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured
prediction to no-regret online learning. In International Conference on Artificial Intelligence and Statistics,
2011.
Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural
networksforgroupshifts: Ontheimportanceofregularizationforworst-casegeneralization. InInternational
Conference on Learning Representations, 2020.
Johannes Schmidt-Hieber and Petr Zamolodtchikov. Local convergence rates of the least squares estimator
with applications to transfer learning. arXiv:2204.05003, 2022.
Zheyan Shen, Jiashuo Liu, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui. Towards out-of-
distribution generalization: A survey. arXiv:2108.13624, 2021.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood
function. Journal of Statistical Planning and Inference, 2000.
Masashi Sugiyama and Motoaki Kawanabe. Machine Learning in non-stationary environments: Introduction
to covariate shift adaptation. MIT press, 2012.
Masashi Sugiyama, Shinichi Nakajima, Hisashi Kashima, Paul Buenau, and Motoaki Kawanabe. Direct
importance estimation with model selection and its application to covariate shift adaptation. Advances in
Neural Information Processing Systems, 2007.
Matus Telgarsky. Deep learning theory lecture notes. https://mjt.cs.illinois.edu/dlt/, 2021. Version:
2021-10-27 v0.0-e7150f2d (alpha).
John Tsitsiklis and Benjamin Van Roy. Analysis of temporal-difference learning with function approximation.
Advances in Neural Information Processing Systems, 1996.
BenjaminVanRoyandShiDong. CommentsontheDu-Kakade-Wang-Yanglowerbounds. arXiv:1911.07910,
2019.
15Tengyang Xie and Nan Jiang. Q⋆ approximation schemes for batch reinforcement learning: A theoretical
comparison. In Conference on Uncertainty in Artificial Intelligence, 2020.
Tengyang Xie and Nan Jiang. Batch value-function approximation with only realizability. In International
Conference on Machine Learning, 2021.
Tengyang Xie, Dylan J Foster, Yu Bai, Nan Jiang, and Sham M Kakade. The role of coverage in online
reinforcement learning. In International Conference on Learning Representations, 2023.
Yufei Yi and Matey Neykov. Non-asymptotic bounds for the L estimator in linear regression with uniform
∞
noise. Bernoulli, 2024.
Huizhen Yu and Dimitri P Bertsekas. Error bounds for approximations from projected linear equations.
Mathematics of Operations Research, 2010.
Yaoliang Yu and Csaba Szepesvári. Analysis of kernel mean matching under covariate shift. In International
Conference on Machine Learning, 2012.
Yi Zhang, Arturs Backurs, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, and Tal Wagner. Unveiling
transformers with lego: a synthetic reasoning task. arXiv:2206.04301, 2022.
16A Proofs for Section 2
A.1 Analysis for ERM
Proposition 2.1 (ERM upper bound). For any δ ∈(0,1) with probability at least 1−δ, ERM satisfies
(cid:18) (cid:19)
log(|F|/δ)
R (fˆ(n))≤O C ε2 +C .
test ERM ∞ ∞ ∞ n
Proof of Proposition 2.1. The proof of Proposition 2.1 is fairly standard, particularly in the well-specified
casewhenε =0. OuranalysisthathandlesmisspecificationisadaptedfromtheproofofLemma16inChen
∞
and Jiang (2019). For the majority of the proof we only consider D , and we consequently omit the
train
subscript when indexing expectations, variances, and the risk functional. Define
n
1 (cid:88)
R(f):=E[(f(x)−f⋆(x))2] and R(cid:98)(f):=
n
(f(x i)−y i)2,
i=1
so that fˆ(n) := argmin R(cid:98)(f). We establish concentration on the “excess risk” functional R(cid:98)(f)−R(cid:98)(f¯).
ERM f∈F
For any f ∈F, we establish the following facts:
E[(f(x)−y)2−(f¯(x)−y)2]=E[(f(x)−f⋆(x))2−(f¯(x)−f⋆(x))2] (8)
Var[(f(x)−y)2−(f¯(x)−y)2]≤8E[(f(x)−y)2−(f¯(x)−y)2]+16ε2 . (9)
∞
Eq. (8) implies that E[R(cid:98)(f)−R(cid:98)(f¯)] = R(f)−R(f¯) as desired. Eq. (9) will enable us to achieve a fast
convergence rate. The former is derived as follows. Observe that conditional on any x we have
E[(f(x)−y)2−(f¯(x)−y)2 |x]
=E[(f(x)−y)2−(f¯(x)−f⋆(x)+f⋆(x)−y)2 |x]
=E[(f(x)−y)2−(f¯(x)−f⋆(x))2−2(f¯(x)−f⋆(x))(f⋆(x)−y)−(f⋆(x)−y)2 |x]
=E[(f(x)−y)2−(f¯(x)−f⋆(x))2−(f⋆(x)−y)2 |x]
=f(x)2−f⋆(x)2−2E [y |x](f(x)−f⋆(x))−(f¯(x)−f⋆(x))2
train
=(f(x)−f⋆(x))2−(f¯(x)−f⋆(x))2.
Eq. (9) is derived as follows.
Var[(f(x)−y)2−(f¯(x)−y)2]≤E[((f(x)−y)2−(f¯(x)−y)2)2]
=E[(f(x)−f¯(x))2(f(x)+f¯(x)−2y)2]
≤4E[(f(x)−f¯(x))2]
≤8E[(f(x)−f⋆(x))2+(f¯(x)−f⋆(x))2]
=8E[(f(x)−f⋆(x))2−(f¯(x)−f⋆(x))2+2(f¯(x)−f⋆(x)2]
≤8E[(f(x)−f⋆(x))2−(f¯(x)−f⋆(x))2]+16ε2 .
∞
Finally, we apply Eq. (8).
Now, Bernstein’s inequality and a union bound over f ∈F gives that with probability at least 1−δ
(cid:114)
(16(R(f)−R(f¯))+32ε2 )log(|F|/δ) 4log(|F|/δ)
∀f ∈F :R(f)−R(f¯)−(R(cid:98)(f)−R(cid:98)(f¯))≤ ∞ + .
n 3n
Since fˆ(n) minimizes R(cid:98)(f) we have that R(cid:98)(fˆ(n))−R(cid:98)(f¯)≤0, we can deduce that
ERM ERM
(cid:115)
(16(R(fˆ(n))−R(f¯))+32ε2 )log(|F|/δ) 4log(|F|/δ)
R(fˆ(n))−R(f¯)≤ ERM ∞ + .
ERM n 3n
17√
Using the AM-GM inequality ( ab≤a/2+b/2), the right hand side can be simplified to yield
1 28log(|F|/δ)
R(fˆ(n))−R(f¯)≤ (R(fˆ(n))−R(f¯))+ε2 + .
ERM 2 ERM ∞ 3n
Re-arranging and using that R (f¯)≤ε2 we obtain
train ∞
56log(|F|/δ)
R (fˆ(n))≤3ε2 + .
train ERM ∞ 3n
Finally we bound the risk under D via a standard importance weighting argument:
test
(cid:20) (cid:21) (cid:12) (cid:12) (cid:18) (cid:19)
R test(fˆ E(n RM))=E train dd test( (x x) )(fˆ E(n RM)(x)−f⋆(x))2 ≤ sup(cid:12) (cid:12) (cid:12)dd test( (x x) )(cid:12) (cid:12) (cid:12)· 3ε2 ∞+ 56log 3( n|F|/δ) .
train x∈X train
Note that we crucially use that (fˆ(n)(x)−f⋆(x))2 is non-negative here. This proves the proposition.
ERM
Proposition 2.2 (ERM lower bound). For all ε
∞
∈(0,1) and C
∞
∈[1,∞) such that (cid:112) C ∞·ε
∞
≤1/2, and
for all ζ > 0 sufficiently small, there exist distributions D ,D and a function class F with |F| = 2
train test
satisfying Assumption 2.1-Assumption 2.4 (with parameters ε ,C ) such that
∞ ∞
R (fˆ(∞))=C ε2 −ζ.
test ERM ∞ ∞
Proof of Proposition 2.2. Fixε ∈(0,1)andC ≥1suchthat(cid:112) C ·ε ≤1/2. Let0<ζ <(cid:112) C ·ε .
∞ ∞ ∞ ∞ ∞ ∞
Let X = [0,1] and let D be the distribution over (x,y) where x ∼ Uniform(X) and y ∼ Ber(1/2). Let
train
X(cid:101) :=[0,1/C ∞]⊂X and let D
test
be the distribution over (x,y) where x∼Uniform(X(cid:101)) an (cid:12)d y ∼B (cid:12)er(1/2).
Thesechoicesyieldf⋆(x)=1/2forallx∈X,satisfyAssumption2.1,andensurethatsup (cid:12)dtest(x)(cid:12)=C .
x∈X(cid:12)dtrain(x)(cid:12) ∞
Let F ={f¯,f } where f¯(x)=1/2+ε for all x∈X (satisfying Assumption 2.3) and f is defined as
bad ∞ bad
(cid:40)
1/2 if x∈/ X(cid:101)
f (x)= .
bad
1/2+ζ if x∈X(cid:101)
B cay lcud le afi tn ioi ntio sn h, owo sbs te hr av te thth isa it nefˆ qE(∞ R uM a) lit=
y
if sb sa ad tia ss fiel don fg ora as ny∥f ζba <d− (cid:112)f C⋆∥2 L ·2 ε(Dt .rai Hn) ow< ev(cid:13) (cid:13) ef r¯ ,− ff⋆(cid:13) (cid:13) h2 L a2 s(D latr ra gin e). poA pud lai tr ie oc nt
∞ ∞ bad
risk under D , in particular
test
R (f )=E [(f (x)−f⋆(x))2]=ζ2,
test bad test bad
which we can make arbitrarily close to C ε2 .
∞ ∞
A.2 Discussion of other algorithms
Star algorithm. Audibert’s star algorithm (Audibert, 2007; Liang et al., 2015) is a two-stage regression
procedure that achieves the fast convergence rate for non-convex classes in misspecified or agnostic regression.
Given that the construction used to prove Proposition 2.2 has a finite (and hence non-convex) function class,
one might ask whether the star algorithm can avoid misspecification amplification. We briefly sketch here
why this is not the case. In the context of the construction, where F ={f ,f¯}, the asymptotic version of
bad
the star algorithm is to compute
fˆ := argmin E [(f (x)−f⋆(x))2] where f (x)=(1−α)f (x)+αf¯(x).
star train α α bad
fα:α∈[0,1]
We claim that when ζ =(cid:112) C ·ε , the optimal choice for α is exactly 1/2. The prediction error under D
∞ ∞ test
forthischoiceis,unfortunately,exactly1/4((cid:112) C ∞+1)2ε2 ∞,whichstillmanifestsmisspecificationamplification.
18Note that, due to the simplicity of our construction, the same argument applies to other improper learning
schemes based on convexification (c.f., Lecué and Rigollet, 2014).
To see that the minimum is achieved at α=1/2, we write the optimization problem over α as
1 (cid:18) (cid:113) (cid:19)2 (cid:18) 1 (cid:19) 2α(1−α)
argmin · (1−α) C ε +αε + 1− ·(αε )2 =argminα2+(1−α)2+ .
α∈[0,1] C ∞ ∞ ∞ ∞ C ∞ ∞ α∈[0,1] (cid:112) C ∞
The derivative, w.r.t. α, of the latter is
(cid:18) (cid:19)
d α2+(1−α)2+ 2α√(1−α) (cid:32) (cid:33)
C 2 4α 2
∞ =2α−2(1−α)+ − = 2− (2α−1).
(cid:112) (cid:112) (cid:112)
dα C C C
∞ ∞ ∞
Since C >1, the second derivative is non-negative, so the optimization problem is convex. Moreover, the
∞
derivative is zero at α=1/2, showing that this is a minimizer of the optimization problem.
L regression. Given that we assume L -misspecification, and in light of the construction for Proposi-
∞ ∞
tion 2.2, it is tempting to optimize the maximal absolute deviation instead of the square loss:
fˆ(n) ←argminmax|f(x )−y |.
∞ i i
f∈F i
This procedure is known as L regression or the Chebyshev estimator and has been studied in the statistics
∞
community (Knight, 2017; Yi and Neykov, 2024). These analyses primarily consider the well-specified setting
with noise that is uniformly distributed, i.e., y = f⋆(x )+ϵ where ϵ ∼ Unif([−a,a]) for some a ≥ 0.
i i i i
We believe such analyses can extend to the L -misspecified setting to show that the procedure avoids
∞
misspecification amplification. However, strong assumptions on the noise are crucial, as L regression can be
∞
inconsistent under more general conditions.
We illustrate with a simple example. Let X ={x} be a singleton, y =Ber(1/4) and F ={f⋆ :x(cid:55)→1/4,f :
x(cid:55)→1/2} be a class with two functions. For all n sufficiently large, the dataset will contain the sample (x,1)
at which point f⋆ will have L error 3/4, while f will have error 1/2. Thus the method will be inconsistent.
∞
A.3 Analysis for DBR
We begin with the proofs of Lemma 3.1 and Lemma 3.2, thus completing steps one and two of the proof.
Then we turn to proving the corollaries.
Lemma 3.1 (Non-negativity). With τ ≥2ε and for any f¯∈F such that ∥f¯−f⋆∥ ≤ε , we have
∞ ∞ ∞
L(f;f¯)≥(τ2−2τε )Pr[Wτ (x)]≥0.
∞ f,f¯
Proof of Lemma 3.1. Following the calculation used to derive Eq. (8) we have that, conditional on any x:
E [(f(x)−y)2−(f¯(x)−y)2 |x]=(f(x)−f⋆(x))2−(f¯(x)−f⋆(x))2
train
Under the event x∈Wτ with τ ≥2ε we claim that this must be non-negative. In particular
f,f¯ ∞
|f(x)−f⋆(x)|≥|f(x)−f¯(x)|−|f¯(x)−f⋆(x)|≥τ −ε ≥ε ≥0
∞ ∞
Therefore,
(f(x)−f⋆(x))2−(f¯(x)−f⋆(x))2 ≥(τ −ε )2−ε2 ≥τ2−2τε .
∞ ∞ ∞
The right hand side is non-negative whenever τ ≥2ε.
19Lemma 3.2 (Concentration). Fix δ ∈(0,1) and τ ≥3ε and define ε := 80log(|F|/δ). Under Assumption
∞ stat 3n
2.3, for any f¯∈F such that ∥f¯−f⋆∥ ≤ε , with probability at least 1−δ we have
∞ ∞
∀f ∈F : L(f;f¯)≤2L(cid:98)(f;f¯)+ε stat, and equivalently, L(cid:98)(f¯;f)≤ 1 2(cid:0) L(f¯;f)+ε stat(cid:1) .
Proof of Lemma 3.2. TheconcentrationinequalityissimilartotheoneusedintheproofofProposition2.1.
We apply Bernstein’s inequality and a union bound to the empirical disagreement-based loss L(cid:98)(f;f¯) for
each f ∈F. To do so, we must calculate the mean, variance, and range of L(cid:98)(f;f¯). Note that by the same
calculation as in the proof of Proposition 2.1, we have that E[L(cid:98)(f;f¯)]=L(f;f¯) and that the range of each
random variable in the empirical average is 1. The variance calculation however is slightly different:
Var[Wτ (x)·{(f(x)−y)2−(f¯(x)−y)2}]≤E[Wτ (x)·{(f(x)−y)2−(f¯(x)−y)2}2]
f,f¯ f,f¯
≤E[Wτ (x)(f(x)−f¯(x))2(f(x)+f¯(x)−2y)2]
f,f¯
≤4E[Wτ (x)(f(x)−f¯(x))2].
f,f¯
Next, we consider a fixed x and define a := (f(x)−f⋆(x)) and b := (f⋆(x)−f¯(x)), so that we can write
(f(x)−f¯(x))2 =(f(x)−f⋆(x)+f⋆(x)−f¯(x))2 =(a+b)2. Now, when τ ≥3ε we have:
∞
Wτ (x)=1⇒|a|=|f(x)−f⋆(x)|≥|f(x)−f¯(x)|−ε ≥2ε .
f,f¯ ∞ ∞
Alongwiththefactthat|b|=|f¯(x)−f⋆(x)|≤ε , thisimpliesthat|b|≤|a|/2orequivalentlythatb2 ≤a2/4.
∞
Using this, we can deduce that
9a2 9a2 3a2
(a+b)2 ≤ ≤ −3b2+ =3(a2−b2).
4 4 2
Re-introducing the definitions for a and b we have that
Var[Wτ (x)·{(f(x)−y)2−(f¯(x)−y)2}]≤12L(f;f¯)
f,f¯
Now, applying Bernstein’s inequality and a union bound over all f ∈F yields that with probability 1−δ:
(cid:114)
24L(f;f¯)log(|F|/δ) 4log(|F|/δ) 1 40log(|F|/δ)
∀f ∈F :L(f;f¯)−L(cid:98)(f;f¯)≤ + ≤ L(f;f¯)+ .
n 3n 2 3n
Re-arranging proves the first statement, and the second statement follows from the symmetries L(cid:98)(f;g)=
−L(cid:98)(g;f) and L(f;g)=−L(g;f).
Corollary 2.1 (Covariate shift for DBR). Fix δ ∈ (0,1). Under Assumption 2.1–Assumption 2.4, with
probability at least 1−δ, fˆ(n) with τ =3ε satisfies
DBR ∞
(cid:18) (cid:19)
log(|F|/δ)
R (fˆ(n))≤17ε2 +O C . (5)
test DBR ∞ ∞ n
Proof of Corollary 2.1. Beginning the with risk under D and assuming that τ =3ε we can write
test ∞
R (fˆ(n))=E [(fˆ(n)(x)−f⋆(x))2]
test DBR test DBR
=E [1{|fˆ(n)(x)−f⋆(x)|<4ε }·(fˆ(n)(x)−f⋆(x))2]
test DBR ∞ DBR
+E [1{|fˆ(n)(x)−f⋆(x)|≥4ε }·(fˆ(n)(x)−f⋆(x))2]
test DBR ∞ DBR
≤16ε2 +E [1{|fˆ(n)(x)−f⋆(x)|≥4ε }·(fˆ(n)(x)−f⋆(x))2]
∞ test DBR ∞ DBR
≤17ε2 +E [1{|fˆ(n)(x)−f⋆(x)|≥4ε }·{(fˆ(n)(x)−f⋆(x))2−ε2 }].
∞ test DBR ∞ DBR ∞
Note that, due to the indicator, the quantity inside the expectation is non-negative. Therefore, via exactly
the same importance weighting argument as we used in the proof of Proposition 2.1, the latter is at most C
∞
times the quantity bounded in Eq. (3).
20Corollary 2.2 (Well-specified case). Fix δ ∈(0,1). Under Assumption 2.1–Assumption 2.4 (with ε =0),
∞
with probability at least 1−δ, fˆ(n) with τ
≤O(cid:16)(cid:112) log(|F|/δ)/n(cid:17)
satisfies
DBR
(cid:18) (cid:19) (cid:18) (cid:19)
log(|F|/δ) log(|F|/δ)
R (fˆ(n))≤O and R (fˆ(n))≤O C . (6)
train DBR n test DBR ∞ n
Proof of Corollary 2.2. Let ∆ denote the right hand side of Eq. (3). Note that in the well-specified case
where ε =0, Theorem 2.1 ensures that
∞
E [1{|fˆ(n)(x)−f⋆(x)|≥τ}·(fˆ(n)(x)−f⋆(x))2]≤∆.
train DBR DBR
√
Then, if we take τ ≤ ∆, we have
R (fˆ(n))=E [1{|fˆ(n)(x)−f⋆(x)|<τ}·(fˆ(n)(x)−f⋆(x))2]
train DBR train DBR DBR
+E [1{|fˆ(n)(x)−f⋆(x)|≥τ}·(fˆ(n)(x)−f⋆(x))2]
train DBR DBR
≤τ2+∆≤2∆.
This proves the corollary.
A.4 Extensions
In this section, we provide two results mentioned in Section 2. First we improve the approximation factor
in Corollary 2.1 from 17 to 10 albeit at the cost of a worse statistical term. Second we show how to choose τ
in a data-driven fashion to adapt to unknown misspecification level ε .
∞
Proposition A.1 (Improved approximation factor). Under Assumption 2.1–Assumption 2.4, with τ =2ε
∞
and for δ ∈(0,1), we have that, with probability at least 1−δ:
(cid:32)(cid:114) (cid:33)
log(|F|/δ)
R (fˆ(n))≤10ε2 +C ·O . (10)
test DBR ∞ ∞ n
Proof sketch. The proof is essentially identical to that of Theorem 2.1, except that we replace the
concentration statement of Lemma 3.2 with a simpler one that relies on Hoeffding’s inequality. The new
concentration statement is that for any τ ≥0 and δ ∈(0,1) with probability 1−δ we have
∀f ∈F : L(f;f¯)≤L(cid:98)(f;f¯)+ε slow,
(cid:113)
where ε := c log(|F|/δ) for some universal constant c > 0. This follows by a standard application of
slow n
Hoeffding’s inequality and a union bound, but importantly does not impose the restriction that τ ≥3ε .
∞
Now the analysis to prove Theorem 2.1 yields that for any τ ≥2ε :
∞
(cid:104) (cid:110) (cid:111)(cid:105)
E 1{|fˆ(n)(x)−f⋆(x)|≥τ +ε }· (fˆ(n)(x)−f⋆(x))2−(f¯(x)−f⋆(x))2 ≤cε .
train DBR ∞ DBR slow
Taking τ =2ε and following the derivation used to prove Corollary 2.1, we get
∞
R (fˆ(n))≤10ε2 +cε
test DBR ∞ slow
(Note that this requires the non-negativity property provided by Lemma 3.1, which we still have.)
The next result considers adapting to an unknown misspecification level.
21Proposition A.2 (Adapting to ε ). Let δ ∈ (0,1) and define S := {2i : τ ≤ 2i ≤ τ } where
∞ min max
(cid:113)
τ := 160log(|F||S|/δ) and τ :=1. Let τ⋆ :=min{τ ∈S :τ ≥3ε }. Then there is an algorithm that,
min 3n max ∞
without knowledge of ε and with probability at least 1−δ, computes fˆsatisfying
∞
(cid:104) (cid:110) (cid:111)(cid:105) 160log(2|F||S|/δ)
E 1{|fˆ(x)−f⋆(x)|≥τ⋆+ε }· (fˆ(x)−f⋆(x))2−ε2 ≤ .
train ∞ ∞ 3n
Note that when ε ≪τ , we are essentially in the realizable regime. Thus, via the proof of Corollary 2.2
∞ min
the above guarantee with τ⋆ :=τ suffices. On the other hand if ε ≥1/3 then τ⋆ is undefined, but due
min ∞
to Assumption 2.4 the guarantee in Theorem 2.1 is vacuous. Thus, the above theorem recovers essentially the
same result as Theorem 2.1, but without knowledge of ε .
∞
Proof sketch. The algorithm is as follows. We run a slight variation of disagreement based regression for
each τ ∈ S: Instead of computing the minimizer of the objective in Eq. (2) we form the version space of
near-minimizers. Specifically, define
(cid:40) n (cid:41)
∀τ ∈S : F := f ∈F :max 1 (cid:88) Wτ (x )(cid:8) (f(x )−y )2−(g(x )−y )2(cid:9) ≤ε /2 ,
τ g∈F n f,g i i i i i stat
i=1
where we define ε = 80log(|F||S|/δ). Note this is slightly inflated from the definition in the statement
stat 3n
of Lemma 3.2, which accounts for a union bound over all |S| runs of the algorithm. Next, we define
 
 (cid:92) 
τˆ:=argmin τ ∈S : F ̸=∅ ,
τ′
 
τ′∈S:τ′≥τ
and return any function in this intersection, i.e., let fˆbe any function in (cid:84) F .
τ′∈S:τ′≥τˆ τ′
For the analysis, via the analysis of Theorem 2.1 and a union bound over the |S| choices for τ, we have
∀τ ≥τ⋆ : f¯∈F and f ∈F ⇒Lτ(f;f¯)≤ε ,
τ τ stat
where Lτ(f;g) is the population objective with parameter τ. The first statement directly implies that τˆ≤τ⋆.
This in turn implies that fˆ∈ F τ⋆ and so fˆachieves the same statistical guarantee as if we ran DBR with
parameter τ⋆ (up to the additional union bound).
B Proofs for Section 4
B.1 Offline RL
Theorem 4.1 (DBR for offline RL). Fix δ ∈ (0,1), assume that F is L ∞-misspecified and µ satisfies
concentrability (as defined above). Consider the algorithm defined in Eq. (7) with τ = 3ε . Then, with
∞
probability at least 1−δ we have
(cid:32) (cid:114) (cid:33)
ε 1 log(|F|/δ)
J(π⋆)−J(πˆ)≤O ∞ + C .
1−γ 1−γ conc n
Proof of Theorem 4.1. For each “target” function f ∈F such that f ̸=f¯, let us define apx[f ]∈F
trg trg trg
to be any approximation to the Bellman backup Tf s.t. ∥apx[f ]−Tf ∥ ≤ε . Define apx[f¯]=f¯,
trg trg trg ∞ ∞
which also satisfies ∥apx[f¯]−Tf¯∥ ≤ε by assumption. Let us define the empirical and population losses
∞ ∞
for the disagreement-based regression problem with regression targets derived from f .
trg
n
(Empirical) L(cid:98)ftrg(f;g):= n1 (cid:88) W fτ ,g(s i,a i)(cid:8) (f(s i,a i)−y ftrg,i)2−(g(s i,a i)−y ftrg,i)2(cid:9) ,
i=1
(Population) L (f;g):=E (cid:2) Wτ (s,a)(cid:8) (f(s,a)−y )2−(g(s,a)−y )2(cid:9)(cid:3) .
ftrg µ f,g ftrg ftrg
22Here recall that y :=r+max f (s′,a′) is derived from the sample (s,a,r,s′). Also note that we use
ftrg a′ trg
E [·] to denote expectation with respect to the data collection policy.
µ
First, we apply Lemma 3.1 and Lemma 3.2 to each of the |F| regression problems. By approximate
completeness and the definition of apx[f ] this yields
trg
∀f trg,f ∈F : 0≤L ftrg(f;apx[f trg])≤2L(cid:98)ftrg(f;apx[f trg])+ε stat, (11)
where ε := 160log(|F|/δ). The above uniform bound holds with probability 1−δ. Note that this ε is
stat 3n stat
twice as large as the one in the proof of Theorem 2.1, which accounts for the additional union bound over all
|F| regression problems.
The main statistical guarantee for fˆis derived as follows
(i) (ii) (iii) (iv)
L fˆ(fˆ;apx[fˆ])≤2L(cid:98) fˆ(fˆ;apx[fˆ])+ε
stat
≤2maxL(cid:98) fˆ(fˆ;g)+ε
stat
≤2maxL(cid:98) f¯(f¯;g)+ε
stat
≤2ε stat.
g∈F g∈F
Here (i) is the second inequality in Eq. (11), (ii) follows since apx[fˆ]∈F, (iii) uses the optimality property
of fˆ, and (iv) uses Eq. (11) again, noting the symmetry of L f¯(·;·) and using apx[f¯]=f¯.
Since the Bayes regression function defined by targets y is Tfˆ, this yields
fˆ
(cid:104) (cid:110) (cid:111) (cid:110) (cid:111)(cid:105)
E 1 |fˆ(s,a)−apx[fˆ](s,a)|≥3ε · (fˆ(s,a)−[Tfˆ](s,a))2−(apx[fˆ](s,a)−[Tfˆ](s,a))2 ≤2ε .
µ ∞ stat
(12)
We translate this to the squared Bellman error on any other distribution ν ∈∆(X ×A) via a slightly stronger
argument than the one used to prove Corollary 2.1.
(cid:104)(cid:12) (cid:12)(cid:105) (cid:104)(cid:12) (cid:12)(cid:105)
E (cid:12)fˆ(s,a)−[Tfˆ](s,a)(cid:12) ≤ε +E (cid:12)fˆ(s,a)−apx[fˆ](s,a)(cid:12)
ν (cid:12) (cid:12) ∞ ν (cid:12) (cid:12)
(cid:104) (cid:110) (cid:111) (cid:12) (cid:12)(cid:105)
≤4ε +E 1 |fˆ(s,a)−apx[fˆ](s,a)|≥3ε ·(cid:12)fˆ(s,a)−apx[fˆ](s,a)(cid:12)
∞ ν ∞ (cid:12) (cid:12)
≤4ε +(cid:118) (cid:117) (cid:117) (cid:116)E (cid:34)(cid:18) ν(s,a)(cid:19)2(cid:35) ·(cid:115) E (cid:20) 1(cid:110) |fˆ(s,a)−apx[fˆ](s,a)|≥3ε (cid:111) ·(cid:16) fˆ(s,a)−apx[fˆ](s,a)(cid:17)2(cid:21)
∞ µ µ(s,a) µ ∞
(cid:115)
(cid:20) (cid:110) (cid:111) (cid:16) (cid:17)2(cid:21)
=4ε +∥ν/µ∥ · E 1 |fˆ(s,a)−apx[fˆ](s,a)|≥3ε · fˆ(s,a)−apx[fˆ](s,a)
∞ L2(µ) µ ∞
√
≤4ε +∥ν/µ∥ · 6ε .
∞ L2(µ) stat
The last inequality is based on the “self-bounding” argument we used to control the variance in the proof
of Lemma 3.2, which showed that under the event |fˆ(s,a)−apx[fˆ](s,a)|≥3ε :
∞
(cid:16) (cid:17)2 (cid:26)(cid:16) (cid:17)2 (cid:16) (cid:17)2(cid:27)
fˆ(s,a)−apx[fˆ](s,a) ≤3· fˆ(s,a)−[Tfˆ](s,a) − apx[fˆ](s,a)−[Tfˆ](s,a) .
Note that ∥ν/µ∥2 ≤∥ν/µ∥ since E [ν(s,a)/µ(s,a)]=E [1]=1.
L2(µ) ∞ µ ν
Finally, we appeal to the telescoping performance difference lemma (c.f., Xie and Jiang, 2020, Theorem 2),
which states that for an action-value function f,
J(π⋆)−J(π )≤
E dπ⋆[[Tf](s,a)−f(s,a)]
+
E dπf[f(s,a)−[Tf](s,a)]
,
f 1−γ 1−γ
where dπ :=(1−γ)(cid:80)∞ γhdπ. Both terms are controlled by the distribution shift argument above and the
h=0 h
concentrability coefficient, yielding the theorem.
23B.2 Online RL
Theorem 4.2 (DBR for online RL). Fix δ ∈(0,1), and assume that F is L ∞-misspecified and µ satisfies
coverability (as defined above). Consider GOLF.DBR with τ = 3ε ∞ and β = clog(TH|F|/δ). Then, with
probability at least 1−δ, we have
(cid:16) (cid:112) (cid:17)
Reg≤O ε HT +H C T log(TH|F|/δ)log(T) .
∞ cov
Proof of Theorem 4.2. TheproofmakesessentiallytwomodificationstotheproofofTheorem1ofXieetal.
(2023). The first step is a concentration argument, which is essentially a martingale version of Theorem 2.1.
The second is the distribution shift argument, which is very similar to the one we used to prove Theorem 4.1.
To keep the presentation concise, we focus on these arguments, and explain how they fit into the analysis
of Xie et al. (2023), but we do not provide a self-contained proof.
Notation. We adopt the following notation. Recall that F(t−1) is the version space used in episode t and
that f(t) ∈F(t−1) induces the policy π(t) deployed in the episode. As before, let apx[f ]∈F denote the
h+1 h
L -approximation to T f . For each episode t let
∞ h h+1
δ(t)(·):=f(t)(·)−[T f(t) ](·) and
h h h h+1
err(t)(·):=1(cid:8)(cid:12)
(cid:12)f h(t)(·)−apx[f h(t +)
1](·)(cid:12)
(cid:12)≥3ε
∞(cid:9) ·(cid:110)(cid:0)
f h(t)(·)−[T hf h(t +)
1](·)(cid:1)2 −(cid:0)
apx[f h(t +) 1](·)−[T hf h(t +)
1](·)(cid:1)2(cid:111)
.
Let d(t) =dπ(t) and define d(cid:101)(t)(x,a)=(cid:80)t−1d(i)(x,a) and µ⋆ to be the distribution that achieves the value
h h h i=1 h h
C for layer h.
cov
Concentration. By a martingale version of Theorem 2.1, we can show that with probability at least 1−δ,
for all t∈[T]:
(i) f¯∈F(t), and (ii) ∀h∈[H]: (cid:88) d(cid:101)(t)(s,a)err(t)(s,a)≤O(β), (13)
h h
s,a
where β =clog(TH|F|/δ). We do not provide a complete proof of this statement, noting that it is essentially
the same guarantee as in Eq. (12), except that (a) it is a non-stationary version with a union bound over
each time step h and episode t and (b) it uses martingale concentration (i.e., Freedman’s inequality instead
of Bernstein’s inequality). It is also worth comparing with the concentration guarantee of (Xie et al., 2023)
under exact realizability/completeness, which is that Q⋆ ∈F(t) and that (cid:80) d(cid:101)(t)(s,a)(δ(t)(s,a))2 ≤O(β).
s,a h h
Distribution shift. To bound the regret, note that
T H
Reg≤(cid:88)(cid:88) E (cid:2) δ(t)(s,a)(cid:3) .
(s,a)∼d(t) h
h
t=1h=1
For distribution shift, we must translate the above on-policy Bellman errors to the “DBR” errors on the
historical data d(cid:101)(t), which is controlled by Eq. (13). Following (Xie et al., 2023) we consider burn-in and
h
stable phases. Let
(cid:110) (cid:111)
γ h(s,a):=min t:d(cid:101)( ht)(s,a)≥C cov·µ⋆ h(s,a) ,
and decompose
T T
(cid:88) E (cid:2) δ(t)(s,a)(cid:3) =(cid:88) E (cid:2) δ(t)(s,a)1{t<γ (s,a)}(cid:3) +E (cid:2) δ(t)(s,a)1{t≥γ (s,a)}(cid:3) .
(s,a)∼d(t) h (s,a)∼d(t) h h (s,a)∼d(t) h h
h h h
t=1 t=1
24The first term is the regret incurred during the burn-in phase, which is bounded by 2C following exactly
cov
the argument of Xie et al. (2023). This contributes a total regret of 2HC .
cov
The second term is the regret incurred during the stable-phase, for which we must perform a distribution
shift argument. To condense the notation, define
δ¯(t)(·):=apx[f(t) ](·)−[T f(t) ](·), and δ˜(t)(·):=f(t)(·)−apx[f(t) ](·).
h h+1 h h+1 h h h+1
Note that, by assumption, (cid:12) (cid:12)δ¯ h(t)(s,a)(cid:12) (cid:12)≤ε ∞. Then,
T
(cid:88) E (cid:2) δ(t)(s,a)1{t>γ (s,a)}(cid:3)
d(t) h h
h
t=1
T
=(cid:88)
E
(cid:104)(cid:16) δ˜(t)(s,a)+δ¯(t)(s,a)(cid:17)
1{t>γ
(s,a)}(cid:105)
d(t) h h h
h
t=1
T
≤(cid:88) E (cid:104) δ˜(t)(s,a)1{t>γ (s,a)}(cid:105) +Tε
d(t) h h ∞
h
t=1
T
≤(cid:88) E (cid:104) 1(cid:110) |δ˜(t)(s,a)|≥3ε (cid:111) δ˜(t)(s,a)1{t>γ (s,a)}(cid:105) +4Tε
d(t) h ∞ h h ∞
h
t=1
(cid:118) (cid:118)
≤(cid:117) (cid:117) (cid:116)(cid:88)T (cid:88)(cid:0)1{t>γ h d(cid:101)( (s t), (a s) ,} ad )( ht)(s,a)(cid:1)2 ·(cid:117) (cid:117) (cid:116)(cid:88)T (cid:88) d(cid:101) h(t)(x,a)1(cid:110) |δ˜ h(t)(s,a)|≥3ε ∞(cid:111) (δ˜ h(t)(s,a))2+4Tε
∞
t=1 s,a h t=1 x,a
(cid:118) (cid:118)
≤(cid:117) (cid:117) (cid:116)(cid:88)T (cid:88)(cid:0)1{t>γ h d(cid:101)( (s t), (a s) ,} ad )( ht)(s,a)(cid:1)2 ·(cid:117) (cid:117) (cid:116)3(cid:88)T (cid:88) d(cid:101) h(t)(x,a)err( ht)(s,a)+4Tε ∞.
t=1 s,a h t=1 x,a
ThepenultimateinequalityisCauchy-Schwarzandthefinalinequalityfollowsfromtheself-boundingproperty
(cid:12) (cid:12)
thatweusedintheproofofLemma3.2andTheorem4.1. Inparticularundertheeventthat(cid:12)δ˜(t)(s,a)(cid:12)≥3ε ,
(cid:12) h (cid:12) ∞
wecanbound(δ˜(t)(s,a))2 ≤3(cid:0) (δ(t)(s,a))2−(δ¯(t)(s,a))2(cid:1). Thuswehaveconvertedfromtheon-policyBellman
h h h
error to the historical “DBR” errors, i.e., we can further bound by
(cid:118)
≤(cid:117) (cid:117) (cid:116)(cid:88)T (cid:88)(cid:0)1{t>γ h(s,a)}d h(t)(s,a)(cid:1)2 ·O(cid:16)(cid:112) βT(cid:17)
+4Tε ∞.
d(cid:101)(t)(s,a)
t=1 s,a h
Meanwhile the density ratio term is bounded by O((cid:112) C log(T)) via the analysis of Xie et al. (2023).
cov
Repeating this analysis for each time step h proves the theorem.
25