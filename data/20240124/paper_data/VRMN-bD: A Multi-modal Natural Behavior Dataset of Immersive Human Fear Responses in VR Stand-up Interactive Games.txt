VRMN-bD: A Multi-modal Natural Behavior Dataset of Immersive Human
Fear Responses in VR Stand-up Interactive Games
HeZhang* XinyangLi† YuanxiSun‡
TheFutureLaboratory AcademyofArts&Design SchoolofComputerandCyberSciences
TsinghuaUniversity TsinghuaUniversity CommunicationUniversityofChina
CollegeofInformationSciencesandTechnology
PennStateUniversity
XinyiFu§ ChristineQiu¶
TheFutureLaboratory SchoolofElectricalEngineeringandComputerScience
TsinghuaUniversity TheKTHRoyalInstituteofTechnology
JohnM.Carroll||
CollegeofInformationSciencesandTechnology
PennStateUniversity
ABSTRACT reality(VR),augmentedreality(AR)andmixedreality(MR)[49],
anincreasingnumberofcompanies,developersandconsumersare
Understandingandrecognizingemotionsareimportantandchal-
showinggreatexpectationsandenthusiasm[80].Amongthem,VR
lenging issues in the metaverse era. Understanding, identifying,
technologyandconsumer-gradeVRdevicesserveasanimportant
andpredictingfear,whichisoneofthefundamentalhumanemo-
userinterfacetoaccessthemetaverse.Meanwhile,theactofhuman-
tions,invirtualreality(VR)environmentsplaysanessentialrole
machineinteractioninVRhasalsoinspireddeveloperstoimagine
in immersive game development, scene development, and next-
morepossibilitiesforthefuture.Thestudyofaffectivecomputingis
generation virtual human-computer interaction applications. In
notonlyanimportantresearchissueinthefieldofhuman-computer
this article, we used VR horror games as a medium to analyze
interaction[62],butalsoanessentialresearchtopicforthemetaverse.
fearemotionsbycollectingmulti-modaldata(posture,audio,and
Themetaverse,asapost-realworld[57],possessesmoreplasticity
physiologicalsignals)from23players. WeusedanLSTM-based
andpossibilitythantherealworld. Furthermore,themetaverseis
modeltopredictfearwithaccuraciesof65.31%and90.47%un-
envisionednotonlyasaserviceconceptbutalsoasafuturespace
der6-levelclassification(nofearandfivedifferentlevelsoffear)
withdiversesocialities.Therefore,itisobviousthatthemetaverse
and2-levelclassification(nofearandfear),respectively. Wecon-
shouldcontainvariouscharacteristicsandemotionalexpressions.
structedamulti-modalnaturalbehaviordatasetofimmersivehuman
Amongallemotions,fearhasasignificantimpactonhumanbehav-
fearresponses(VRMN-bD)andcompareditwithexistingrelevant
ior,decision-making,mentalhealthandsociallifeasoneofthemost
advanceddatasets.Theresultsshowthatourdatasethasfewerlimi-
importantbasichumanemotions[47].Thestudyofhowtoinduce,
tationsintermsofcollectionmethod,datascaleandaudiencescope.
enhance[37],diminish,calm[30],andovercome[21]fearinvirtual
Weareuniqueandadvancedintargetingmulti-modaldatasetsof
environmentscanhelppeoplebetterdealwithfearfulfeelingsinthe
fearandbehaviorinVRstand-upinteractiveenvironments.More-
virtualworld.
over,wediscussedtheimplicationsofthisworkforcommunities
andapplications.Thedatasetandpre-trainedmodelareavailableat Atpresent,importantissuesonfearinthemetaversefrominclude
https://github.com/KindOPSTAR/VRMN-bD. datasets,perceptionsandrecognition,simulation,andevaluation.
Datasets,especiallynaturalbehaviordata,arethebasisforthecom-
IndexTerms: Human-centeredcomputing—Visualization—Visu-
putationoffear. Ahigh-qualitydatasetisessentialforstudieson
alization techniques—Treemaps; Human-centered computing—
identification[71],simulation[20],designofstimuli[36],designof
Visualization—Visualizationdesignandevaluationmethods
interactions[41]andothermachinelearningtasks[83].Perception
andrecognitionofemotionshasbeenoneofthemostchallenging
1 INTRODUCTION
issueswithinthedomainofaffectivecomputing[93]. Themulti-
Emotionisthemostpowerfulmotivationalforceinhumans,andit modalrecognitionalgorithmforfearinvirtualenvironmentshas
issignificantlycorrelatedwithperception,attention,memoryand greatapplicationpotential.First,ithasapositiveimpactonthede-
learning.Moreover,emotionswithspecificexpressiveabilitiesare velopmentofVR,wheremoreaccuraterecognitionresultsmaylead
acrucialhumantrait[19].Withtherapidriseandintensivediscus- toanenhancedsenseofrealismandimmersioninthemetaverse.
sionofthemetaverseconcept,avirtualenvironmentthatblendsthe Second, such recognition of specific emotions can help improve
physicalanddigital,drivenbytheinternet,webtechnologies,virtual human-centeredanduser-centereddesigns[45].Finally,thisrecog-
nitionmethodmightprovideausefulreferenceforidentifyingother
*e-mail:hpz5211@psu.edu emotions. Another crucial issue for the metaverse is evaluation.
†e-mail:lixinyang22@mails.tsinghua.edu.cn Theevaluationofthefearresponseisdecisivefortheenhancement
‡e-mail:syxi0120@gmail.com ofrelevantissuesinthemetaverse, suchasuserexperience[73],
§e-mail:fuxy@mail.tsinghua.edu.cn,Correspondingauthor socialsystems[92], usability[98], ethics, moralsandjustice[7].
¶e-mail:qiu-yh18@tsinghua.org.cn Infact,theidentificationofnegativeemotionssuchasfearismore
||e-mail:jmc56@psu.edu challengingthantheidentificationofpositiveemotionsbecauseof
self-concealment[48].
Inthisarticle,weaddressedthefollowingmainresearchquestions
(RQs):
4202
naJ
22
]CH.sc[
1v33121.1042:viXraRQ1 Howcanamulti-modalnonperformingfeardatasetbebuilt theyareintheenvironmentofthegame(PI)experiencingthescene
utilizingVRhorrorgames? asitishappening(PSI)[50].WithadvancesinVRtechnology,this
illusionarymechanismisachievedprimarilythroughhead-mounted
RQ2 Howcanmulti-modaldatabeusedtorecognizeusers’fears? displayscombinedwithprecisemotiontrackingsystems,allowing
theusertoexperienceaninteractive3Dvirtualenvironment[17].
ToaddresstheseRQs,weproposed(1)arigoroususerexperimen-
Asaresult,playersplayinghorrorgamesthroughVRexperiencea
talprotocoldesigntoinduceparticipants’fearutilizingVRhorror
muchstrongersenseoffearandanxietythanthoseplayinginvideo
games;(2)multi-modaldatacollection,processing,annotation,and
mode[60].Ingeneral,studiesbasedon3Denvironmentscanobtain
fusiontobuildanonperformancefeardataset;and(3)aprediction
betterresultsthan2Dstimuli[38].Further,astudybySomarathna
algorithmoffearusingmulti-modaldata.Inaddition,wealsopro-
etal.[79]showedthatthefearcouldbeeffectivelyinducedinVR
videdinsightsintotheemotionalchallengesofthemetaverse. In
environments,especiallygames.
summary,wemadethefollowingmaincontributions:
Generally,horrorgamesrequireplayerstoactivelyrespondto
threatstosurvive. Afterafearresponseisgenerated,peopletend
(1) Amulti-modalnaturalbehaviorfeardataset.
tousevariouscopingmeasurestoalleviatetheirfear.Asurveyby
(2) Anovelapproachtofearrecognition. LynchandMartinsonvideogames[52]revealedthatthestimuli
thatparticipantsmostoftenreportedtriggeringtheirfearsingames
2 RELATEDWORK weredarkness,disfiguredhumans,zombiesandtheunknown.Lin’s
proposedtheoreticalframework[50]forhowplayersreacttohorror
2.1 Fear-arousingStimulation
contentinVRcontainsthreestrategies.1)Approach(monitoring)
Scholarswhosupporttheclassificationofemotionsgenerallyagree strategies. Atthecognitivelevel, playersalwaysalertlymonitor
thathumanshavemorethanadozenbasicemotionsthatcontain theirsurroundingsforpossiblehazards,whileatthebehaviorallevel,
physiologicalelements[35,78]. Sixbasicemotionsidentifiedin theychoosetobeproactive,forexample,byadoptingin-gameskills
Ekman’sfacial-expressionresearch[74]arecommonlyaccepted: toproactivelyeliminatehazards.2)Self-helpstrategies.Whileon
anger,disgust,fear,happiness,sadnessandsurprise,ofwhichfearis thecognitivelevel,playersactivelytalktothemselvestoencour-
astronglyunpleasantnegativeemotion.Fearisdefinedasamultidi- age themselves, the behavioral level is where players choose to
mensionalresponseinwhichapersonhasanimmediateemotional expresstheirfearsbyscreamingorshouting. 3)Avoidancestrate-
reactionandsubsequentcognitiveresponsetoaperceptiblethreaten- gies. Thefirstoftwoavoidancestrategiesisphysicalandmental
ingstimulusintheenvironment.Fearoftenarisesfromthepresence disengagement,whereplayersusuallyturntheirheads,closetheir
orpresumedpresenceofdanger.Inotherwords,itisanexperience eyes,removetheirheadphonesorcrouchdowntoavoidthesounds
inducedbyastimulus[52]. orimagesthatfrightenthem,andthesecondisdenial,wherethey
Peoplearesusceptibletoexperiencingfearwhentheyperceive tellthemselvestheexperienceisnotreal[50,103].Thefindingsin
significantandpersonallyrelevantdangersinrealenvironments[46]. thesepaststudiesprovidedreferencesforgameselectionanddata
Inaddition,mediatedenvironmentscanalsoevokefearinpeople. annotation.
Cantorsuggeststhatthelinkbetweenhorrormediaandthefears
TheexplorationofhorrorcontentinVRcanbeappliedinmany
ofitsviewerscanbeexplainedbytheprincipleofstimulusgen-
ways. Virtualrealityexposuretherapy(VRET)isanincreasingly
eration[11]. Ifreal-worldstimulievokeemotions,thenthesame
commontreatmentforanxietyandspecificphobias(agoraphobia,
stimuli portrayed in media will evoke the same or less stressful
fearofdriving,claustrophobia,aviophobiaandarachnophobia)[61]
experiences[31]. TheDiagnosticandStatisticalManualofMen-
. Commercial games combining horror genres and biofeedback
talDisorders(DSM-IV)[81]frameworkclassifiesfear-provoking
technologymaybeausefulstressorforpracticingstressmanagement
factorsintofivecategories:animals(e.g.,dogsorspiders);environ-
skills[8,60].Additionally,experiencingscaresinanentertainment
ment(e.g.,fireorfloods);blood/injections/injuries(e.g.,woundsor
environmenthasgainedpopularitybythemarket. Throughanin-
needles);situationalfactors(e.g.,height,confinedspacesormore
depth study of the characteristics of horror game content in VR
specificspacessuchasadoctor’soffice);andotherfactors(objec-
gamesandwhatgameelementsinducefearinplayersandtowhat
tivelyharmlessbutdisturbingstimulisuchasdistortedfacesand
extent, thisstudyprovidesatheoreticalbasisforfearcomputing
loudnoises).Thesecategoriesserveasreferencesforourselection
researchandabetterproductdevelopmentevaluationtoolforVR
ofhorrorgames.
contentdesignersandrelatedresearchers.
Whenaperson’sfearisaroused,thefollowingphysiologicalre-
sponsesusuallyoccur:1)physiologicalchanges(e.g.,sweatypalms, 2.3 AffectiveComputingMethods
increasedheartrateortrembling),2)changesinfacialexpressions
2.3.1 BodyGesturebasedMethod
(e.g.,widenedeyes,contractedfacialmusclesorincreasedtone),3)
changesinthewayinformationisprocessed(e.g.,morelikelyto Thepossibilityofemotionrecognitionthroughbodygestureinfor-
receivesuggestedprotectivemeasures),and4)thetendencytotake mationhasbeenwidelyacknowledgedbyresearchers[91].Based
specificactions(e.g.,seekingcover,hidingorrunningaway)[42]. oncameraimages,Cuietal.[39]extractedthekeypointsofhuman
This immediate physiological response can be used as an objec- postureandusedanalgorithmtodrawasimplifiedmaptoidentify
tive measure of fear, and physiological signals collected in real theemotionalinformationforvariouspostures.Shietal.[72]used
time,suchasheartrate[68],heartratevariability[97]andconduc- theposeestimationalgorithmtoextract3Dskeletoninformation
tance[85],canallidentifyindividuals’fearstates.Thesereferences intheIEMOCAPdatabaseandproposedaself-attentionenhanced
ofresponsesinfearprovidesuggestionsfortheexperimentaldesign spatialtemporalgraphconvolutionalnetworkforemotionrecog-
ofthisstudy,especiallythechoiceofdatamodalities. nitionof3Dskeletons, whichconfirmedtheabilitytorecognize
emotionbasedon3Dskeletons.Similarly,basedonbonedetection
2.2 VRHorrorGames
technology,Tsaietal. [88]trainedtheST-GCNrecognitionmodel
TheillusionarymechanismsprovidedbyVRallowuserstoreact to effectively identify four emotional states. Sapin´ski et al. [67]
realisticallytoVRscenes,effectivelyprovidinganimmersiveexpe- proposedamethodtorecognizebasicemotionalstates. Itgener-
rience.Thefirstistheplaceillusion(PI),alsoknownas‘beingthere’ ates a model of affective action based on features inferred from
or‘presence’,whichreferstoasenseofbeinginarealplace.The thespaciallocationandtheorientationofjointswithinthetracked
secondistheplausibilityillusion(PSI),anillusionthattheeventsbe- skeleton.Itshowsthefeasibilityofautomaticemotionrecognition
ingportrayedareactuallyhappening[76].VRmakesusersbelieve fromsequencesofbodygestures,whichcanserveasanadditionalsourceofinformationinmulti-modalemotionrecognition.However, thedomainoftime,frequency,andnonlinearity.Itmadeaffective
emotionrecognitionbasedongesturesismostlyusedforspecific statepredictionpossibleusingphysiologicalsignals.Thestudyby
singlegestures,anditsperformanceisgreatlydegradedforgestures Moghimi et al. [56] demonstrated that physiological signals can
innaturalsituations[67].Bhattacharyaetal.[5]presentedanovel beusedeffectivelyforemotionrecognitionclassificationtasksina
classifiernetworkcalledSTEPtoclassifyperceivedhumanemo- gamingenvironment.
tionfromgaits,andthismodelclassifiedtheperceivedemotionof
humansintooneoffouremotions: happy, sad, angry, orneutral. 2.3.4 Multi-modalMethod
Fuetal.[22]presentedanapproachthatusedaKinectv2sensor In recent years, this multi-modal analysis approach has received
tocapturewhole-bodyposturesandrecognizehumanfearinaVR attentionfromresearchersandisconsideredapossiblefuturedirec-
environmentusingalong-andshort-termmemorymodel(LSTM). tion,whichisnoteworthy.Thereisevidencethatmulti-modaldata
Previousstudieshavedemonstratedthathumanemotionscanbe fusionforemotionrecognitionisanimportantapproachtoaddress
effectivelyrecognizedthroughskeletalposes. However,manyre- thelimitationsofeachsingle-channelanalysismentionedabove[24].
searchershaveclaimedthatcurrentresearchislimitedbythelack Sebastianetal.[70]presentedfusiontechniquesondeeplearning
ofvideoqualityandhaveexpressedconcernsaboutthelimitations modelsforimprovedemotionrecognitioninmulti-modalscenarios.
ofusingasinglemodality.Therefore,combininginformationfrom Intramodalitydynamicsforeachemotionarecapturedintheneural
othermodalitiesandbetterdatasetsmaybeeffectiveinimproving networkdesignedforthespecificmodality.
performance. Basedontheexistingbehavioralemotionrecognition,Matsuda
etal. [53]combined behavioralfeatures suchaseyemovements
2.3.2 AudiobasedMethod and head movements with audiovisual signals, collected human
Speechisoneofthemostdirectandoldestwaysforhumanstocon- voicesignalsduringtravel,andclassifiedthreecategoriesofemo-
veyemotions,andvoiceinformationhasbeenusedbyresearchers tions,positive(excited,happy/pleasedandcalm/relaxed),negative
for emotion research for more than 20 years [14]. Additionally, (sleepy/tired,bored/depressed,disappointed,dis-tressed/frustrated
voicesdisplayedbecauseoffeartendtobestrong[89],whichseems andafraid/alarmed),andneutral. Validationofbimodaldatacom-
tobeahigh-performancemodalitytoapplytoemotionrecognition. biningbehaviorandsoundforemotionrecognitioninareal-world
AnjaliBhavanetal.[6]proposedabaggedensemblecomprisingsup- setting. Kesharietal.[39]integratedfacialexpressionandupper
portvectormachineswithaGaussiankernelforSER.Itextracted bodygesturedatatoachievehigheremotionrecognitionaccuracy
Mel-frequency cepstrum coefficients (MFCCs) and spectral cen- thanusingsinglegesturedata.Insomespecialworkenvironments,
troidstorepresentemotionalspeech,followedbyawrapper-based suchastheemotiondetectionofdrivingstates,somestudieshave
feature selection method to retrieve the best feature set. Its best usedthecombinationofphysiologicalsignalsandspeechsignals
accuracyis75.69%usingtheRAVDESSdatabase,whichshowsits to extract and analyze data features for emotion classification in
superiorityoverothertechnologiessuchasAdaBoost.Itshouldbe thecorrespondingworkenvironment.Studies[1]haveshownthat
notedthatthisworkfocusesononlyacousticfeaturesratherthan multi-modaldata,similartothecombinationofspeechandphysio-
speechfeatures.Tzirakisetal.[90]proposedaconvolutionrecurrent logicalsignals,aremorehelpfultoimprovetherecognitionability
neuralnetworkstructureforspeechemotionrecognition.Thismodel ofemotionrecognitionsystemsthanasinglemodality.Inaddition,
ismadeofaConvolutionalNeuralNetwork(CNN),whichextracts Siddharthdetal.[75]showedthatmulti-modalfusiondatahasthe
featuresfromtherawsignaldataandisstackedwitha2-layerLSTM. advantageofthescalabilityandeasyfeatureextraction.
Themodelachievedthebestresultsatthetimeregardingtheconsis-
tencycorrelationcoefficientfortheRECOLAdatabase.However, 3 USEREXPERIMENTDESIGN
emotionrecognitionusingvoicealoneisstillinsufficientbecause To address our RQs, we designed a rigorous user experimental
whilevoicesmayhavedistinctivefeaturesduringstrongemotions protocoltoinduceparticipants’fearandgatherparticipants’data
(crying,screaming,anger,etc.),itisoftendifficulttodiscriminate utilizingVRhorrorgames.ToaddressRQ1,weprovideacomplete
voices expressing nearly neutral emotions [15]. In addition, as andcomprehensiveexperimentalprocedure,whichalsoincludesthe
humansbecomemoresociallyengaged,peopleareincreasinglysup- dataacquisitionscheme.ToaddressRQ2,weprovideaLSTM-based
pressingthereleaseofemotionsthroughtheirvoices[94].Therefore, predictivemodel.
voicesareworthusingasvalidinformationforidentifyinghuman
emotionsbutshouldnotbeusedastheonlytypeofinformation. 3.1 ExperimentalSituation
The experimental site consisted of three areas: the waiting area,
2.3.3 PhysiologicalSignalbasedMethod
the experimental area, and the director-interview area (see Fig-
Theapplicationofphysiologicalsignalsisconsideredaneffective ure1). Waitingarea: Thewaitingareawassetupinaseparate
emotionrecognitionmethod.Manystudiesusephysiologicalsignals room, visually and aurally separated from the experimental and
asthesignalsourceforemotionrecognition,butfewstudiesusea director-interviewareas,whereparticipantsconfirmedtheiratten-
physiologicalsignalastheonlycriterionforemotionrecognition.In danceattheexperiment,filledintheinformedconsentformand
previousstudies,therehavebeentworesearchmethodsforemotion thepretestPANAS-Xscale, worethephysiologicalsignalacqui-
recognition involving physiological signals. One approach is to sition equipment and waited for the experiment. Experimental
usethecombinationofmultiplephysiologicalsignalsasthebasis area: Theexperimentalareawasbuiltina3.5m×2.8m×2.6m
foremotionclassification,suchasthecombinationofrespiration (Length×Width×Height)semiopenspacepartitionedbycurtains
(RSP)andheartratevariation(HRV).Anotherapproachistocom- andpartitionwiththedirector-interviewarea.Experimentalpartic-
binephysiologicalsignalswithsignalsfromothermodalitiessuch ipantsexperiencedthepreinstalledVRgamesinthisarea. There
as, facialexpressionsandvoice. Ohetal.[59]usedfivephysio- werenoobstaclesinthespaceexceptfortheVRequipmentnec-
logicaldata, includingrespirationandheartrate, tooptimizethe essaryfortheexperiment. TherewerefourFilrcamerassetupin
modelbasedonCNNtoclassifyandidentify6emotionalstates. eachofthefourcornersoftheexperimentalarea,fixedatacertain
Santamaria-Granadosetal.[66]appliedadeepconvolutionalneural heightusingatripod.TheVRdeviceusedintheexperimentisthe
networkonanAMIGOSdatasetofphysiologicalsignals, which HTCVive(withaper-eyeresolutionof1080x1200,arefreshrate
contains electrocardiograms and galvanic skin responses. It cor- of90Hz,andamaximumfieldofviewofabout110◦).Interview
relatesphysiologicalsignalswiththedataofarousalandvalence area:Theinterviewareawasadjacenttotheexperimentalareabut
ofthedatasetandextractsthefeaturesofphysiologicalsignalsin isolatedfromitintermsofarealisticview;asetofPCcomputersFigure2:Examplesofgamepostersandscreenshotsoftheactual
game.Thefirstimageontheleftisthe"Richie’sPlankExperience"
poster,thesecondimageontheleftisthe"Richie’sPlankExperience"
in-gamescreenshot,thethirdimageontheleftisthe"Phasmophobia"
poster, thethirdimageontheleftisthe"Phasmophobia"in-game
Figure 1: Experimental Places. The upper left corner shows the
screenshot,thefirstimageontherightisthe"EmilyWantsToPlay"
monitorcomputerscreen,whichincludedareal-timegamescreen,
in-gamescreenshot, andthefirstimageontherightisthe"Emily
amulti-camerasystemmonitoringscreen,andaphysiologicalsignal
WantsToPlay"poster.
recordingequipmentscreen.Thelowerleftcornershowsthelayout
oftherealexperimentalplace,includingthedirector-interviewarea
andtheexperimentalarea. Therightsideshowsaplanviewofthe
fullexperimentalplace. withasinglecameraresolutionof1280x720.Aftercalibratingthe
internalandexternalparametersofthecamera,thehumanskeletal
points were extracted using OpenPose [12] and reconstructed to
obtain3Dkeypoints. OpenPosetakesRGBimagesasinputina
andmonitorsweresetupinthisareatorunthesoftwarerequired
single view camera and each key skeleton point in the image as
forvariousexperiments,andtheexperimentalhostcouldviewthe
output.OpenPoseprovides25keypoints,andTable2explainsin
situationintheexperimentalareainrealtimethroughthemonitors.
detailthe25keyjointsofthehumanbodycorrespondingtothe25
Inaddition,thisareacontainedasubareacalledtheinterviewarea
keypoints. After3Dreconstructionofskeletonpointdatausing
forinterviewingparticipantsinvariousstagesoftheexperiment.
amultiviewcamerasystem,25keypointswererepresentedby3D
3.2 VRHorrorGamesSelectionandUsage coordinates(x-,y-,z-axis)with75factorsintotal.Missingvalues
in3Dkeypointswerefurtherprocessedusinginterpolation.Figure3
Inthisexperiment,theresearcherschosethreeVRhorrorgames
showstheviewsofeachsinglecameraintheexperimentandthe
fromtheSteamplatform,Game1(Richie’sPlankExperience[87]),
3Dposeestimationresults.Participantsworewirelessmicrophones
Game 2 (Phasmophobia [23]), and Game 3 (Emily Wants To
ontheirlapelstorecordanyvocals,andtheaudiowasrecordedat
Play [34]), which were thought to stimulate apparent fear. Fig-
abitrateof128kbps,coveringtheentireVRexperimentprocess.
ure2showsaschematicdiagramofthesegames.Theresearchers
Fullgamegraphicsandgamesoundswererecordedthroughthe
chosetoexperimentwithVRhorrorgamerulesasfollows:1)the
XboxGameBar. Thephysiologicalsignalrecordingdevice(the
gameprocessisalinearscripttocontrolthegameprogress,length,
Zephyr™BioHarness™[29])continuouslyrecordedtheheartrate
andthesamevariablesasmuchaspossible; 2)neithershortnor
andrespiratoryrateatasamplingrateofonceeverytwoseconds.
long-expectedgametime,eachgameprocesseswithin5-20minutes
Thephysiologicaldatasamplingrateprovidedbythedevicemight
toavoidthefailureorfadingofemotionalarousalcausedbythe
notbeperfect,butitisusableinthiscontext[13,58].Moreover,be-
lengthofthegame;3)theoperationrulesoftheselectedgameare
causethisstudyemploysmulti-modaldata,itreducesthelimitations
simpleandcanallownovicestolearnquickly,andthegamedoesnot
ofrelyingonsinglemodalitydata. Atthisstage,wefilledinthe
containcombinationsofbuttonsandcomplexpuzzles.4)whether
missingvaluesinthe3Dskeletalpointsandalignedthedatafrom
therearenecessaryelementstostimulatespecificemotionalfear;
differentmodalitiesaccordingtothestartandendtimestamps,and
and5)basedontheplayer’soverallrating.Table1showsthemetrics
splitintothreegroupsaccordingtothedifferentgames.
foreachgamebasedontheabovecriteriaandwhethertheresearcher
providedspurioustargetsforcontrollingtheflowoftheexperiment
andtriggeringspecificeffectsatthetimeoftheexperimentforrefer-
ence.Consideringtheindividualdifferencesoftheparticipants,to
reducethevariablesthatweredifficulttocontrolinthegameexperi-
ment,theresearcherconcealedortamperedwithsomerealgoalsand
providedaconvincingfalsepurposetodrivetheparticipantstoplay
thegame[43],whichinturnmadetheplotprogressionofGame2
andGame3independentoftheplayer’sbehaviorinthegame.For
example,inGame2,playersareaskedtoexploreasmanylocations
aspossibletofindanonexistentredbeartoy,whileabrownbear
canindeedbefoundinthegame. Thisconfusingtaskissetupto
allowparticipantstoexploreasmanydarkenvironmentsaspossible
Figure3:Humanskeletalpointcalibration.FourFilrcameraviews(on
andtriggermorepotentiallyhorrificgameevents. InGame1,the
twosides)and3-Dreconstructedskeletalpointview(inthecenter).
player’sactionsbasedontherealgameobjectivesdidnotbranchthe
Thealignmentofeachviewandthereconstructed3-Dresultswere
progressofthegame,andthereforeonlytheuniqueobjectiveswere
re-layout,butwithoutmodifyingthecontent.Thenumbersinthemid
presentedtotheparticipantswithoutchangingthegameplay.
figurerepresentthecoordinatesoftheskeletalpoints,andthedetails
areshowninthefollowingTable2.
4 DATACOLLECTIONANDDATASETCONSTRUCTION
4.1 DataCollectionandPre-processing
Then,were-layouttherecordingofgamefootagewithsounds,
Ourcontributeddatasetcontainsdatafromatotalofthreemodalities: multicameraviews,3Dkeypointreconstruction,andvisualizations
posture,audioandphysiologicalsignals. FourFilrcamerasinthe ofphysiologicaldata,andmergethemintoonevideoalignedatthe
experimentalarearecordedthecompleteVRexperimentalprocess, firstframe.Meanwhile,weaddareferencelinebasedonvideotimeTable1:Theselectedgamesandtheirreferencefactors.
Game Game(task)duration TypesofFearsSimulated Player’soverallrating
No. Nameofgame (approximately) EaseofLearning (fearof) (positiverate-totaluserreviews)i Spurioustargets
Heights,Falling,
Richie’sPlank VeryPositive
1 5-10mins VeryEasy Spider,Dentists, Nofalsegoals
Experience (81%-567)
Jumpscary
Claustrophobic/Darkness,
OverwhelminglyPositive Afalsegoal
2 Phasmophobia 20mins Easy Solitude,Paranormal,
(96%-489,650) andsomefalsetips
DeathandNear-Death
Claustrophobic/Darkness,
MostlyPositive Afalsegoal
3 EmilyWantsToPlay 5-10mins VeryEasy Paranormal,Jumpscary,
(78%-1,694) andsomefalsetips
Thunderstorm,Dolls
iDatacollectiondateisSeptember18,2023
Table2:Openpose(skeleton)keypointsdata.Eachkeypointcorre-
includedtwocategories(nonfearandfear),with6levels(thisrefer-
spondstotheexampleshowninFigure3."R"meansright,and"L"
encesthestudybyFuetal.[22],wherelevel0=nonfear,andlevels
meansleft.
1-5,fearlevelfromthelowesttohighest). Amongthem,nonfear
No. 0 1 2 3 4 5 6 7 8 emotions(level0)wereautomaticallyfilledafterannotationandno
Name Nose Neck RShoulder RElbow RWrist LShoulder LElbow LWrist MidHip
No. 9 10 11 12 13 14 15 16 17 specialmanualannotationwasneeded.
Name RHip RKnee RAnkle LHip LKnee LAnkle REye LEye REar We recruited 5 annotators, and each reformatted video had at
No. 18 19 20 21 22 23 24
Name LEar LBigToe LSmallToe LHeel RBigToe RSmallToe RHeel least2annotators.Beforestartingtheannotation,atutorialsession
andasimulatedannotationusingthesamplewereconductedfor
alltheannotatorstoallowthemtofullyunderstandtheusageof
tothevisualizationsofthephysiologicaldataforfutureannotation theannotationtool.Weintroducedsomegroundtruthsaboutbody
purposes. gestures[16],screaming[69]andphysiologicalsignals[51]offear
conditioningtoannotatorsduringthesimulationannotation.Also,
4.2 DataReportofParticipant theannotatorconsiderstheparticipant’sself-reportintheannotation
process.Aftercompletingtheannotation,weusedabsolutemajority
WerecruitedthosewhowereinterestedinparticipatingintheVR
votingtoobtainthefinalannotationresults.Iftheabsolutemajority
experimentasparticipantsthroughsocialmediagroupsandnearby
systemfailed,thefinalannotationresultswereequaltothenearest
universitiesinChina,butforhealthandsafetyreasons,theeligible
wholenumberoftheaverageofallannotationlevels(lessthan1to
candidatesforthisexperimentwerecontrolledtobebetween18-30
makeupfor1).Finally,theannotationresultsweremergedwiththe
yearsoldandself-reportedtobeingoodphysicalhealth,mentallyfit,
multi-modaldatasetalignedbytimestamp.
andfreeofdiseasesormedicalhistoriessuchasheartdisease,visual
Here,wedidnotincludeparticipants’subjectivefearratingsre-
problems,andvertigo.Allparticipantswerepaidacertainamount
ported in real time in the dataset directly. Although self-reports
ofcashasthanks.Allparticipantsweregiventheoptiontostayafter
havebeenusedinnumerousstudies,therearesomeobviouslimi-
theexperimentalperiodforafreeexperiencewithVRgamesfor
tations[4,65]thatdonotfitourdataset.Again,becausecollecting
additionaltime(withnospecialrestrictions)withoutinterferingwith
continuoususerself-reportsinVRenvironments,especiallyinmo-
theexperimentorthetypicalresearchenvironment.Thisexperiment
tion,inatransientandprecisemannerposesasignificantchallenge
wasprocessedunderapprovalofuniversityIRB.Informedconsent
intheformofuserdistraction[100].
wasobtainedfromallparticipants.
Therewere23participants(P1-P23, 9malesand14females), 4.4 DataAnalysisandFeatureExtraction
aged between 18 and 28 years old (median age = 21) and with-
Toexploitdeeplearningmethodstopredictfearlevels,westructured
outsignificantcongenitaldisorders.Intotal,95%(22)participants
theseunstructureddatathroughfeatureextractionforeachmodality.
completedGame1,60%(14)participantscompletedGame2,and
Then,wesynchronizedthemaccordingtotheframeindex,which
56%(13)ofparticipantscompletedGame3. Themainreasonfor
wasthesimplestcomponentofthevideo.
participantsnotcompletingthegamewasearlywithdrawalbecause
For the video model: we adopted OpenPose to learn 25 key
ofsensoryoverload.Onepersonquitearlyduetointensediscomfort
humanskeletalpointsin3Dspatialcoordinates(d=75).Considering
causedby3Dvertigo/cybersickness,andwedidnotusedatafrom
thattheseskeletonfeatureswerelikelytocontainamassiveamount
thisparticipant. Therestallreportedearlyexitduetofeelingtoo
ofredundantinformation,wethereforetookthePCAapproachto
scary. In the cases of early withdrawal due to sensory overload,
reduce the dimension from 75 to 33 while retaining 98% of the
onepersonoccurredduringthegame(shoutedtostopatthevery
informationaskeyfeaturesforeachframe.
beginningofGame2),andalltherestendedafterthepreviousgame
Foraudioinformation:Gameaudioandmicrophoneaudiowere
interview(didnotstartthenextgame).
extracted together and converted into a digital audio signal, and
a sample of audio features is shown in Figure 4. The features
4.3 DataAnnotation
used7metricstorepresentthedata: thezero-crossingrate,spec-
WebuiltanancillarytoolforhelpingmanualannotationintheWin- tralcentroid,spectralbandwidth,spectralrolloff,chromafeatures,
dowsplatform.Thetoolallowsannotatorstoannotatemulti-modal rmseandMFCC.Thesemetricsnotonlyreflecttheaudiointerms
dataandimprovetheconsistencyandworkabilityoftheannotation offrequency, butalsoindicatehumanvoicesthroughtheMFCC
process. The tool contained the area of monitoring, the annota- specifically.Toalignwithskeletalfeatures,weprocessedtheaudio
tiontoolbar,andthelabeledrecords. Thedatawereannotatedby featuresintoframewiseinsteadofsecondwisefeaturesin26dimen-
watching video of participants’ game views, multiview physical sions,ofwhich20dimensionswereMFCCfeatures,andtookthe
movementsandthechartofphysiologicaldatacombinedwithgame averageoftheaudiofeaturestoalignwiththeimageframerate.
soundsandmicrophonesoundsbytimestamp(inmilliseconds).An- Similarly,wetransformedthephysiologicaldatatodescribethe
notators could use this annotation tool to replay video clips and heartrateandbreathingfrequencyofaparticipantperframerather
changetheannotationlevel(repetitiveratings).Theannotateddata thanperminute.Thephysiologicaldatawerealignedwiththeimageframerateusingtheaverageinterpolationofadjacentdatatoobtain 5 MULTI-MODALFEARPREDICTIONMODELING
acompletedatasetcontainingaudiofeaturesandphysiologicaldata
To address RQ2, we adopted LSTM as our base model because
basedon3Dkeypointdata.Then,weconcatenatedthefeaturesin
LSTMcanfullylearnfeaturesintemporaldynamicsandimprove
threemodalsaswellasthetargetlabelsinaccordancewiththeframe
classificationaccuracy[75]. Buildingonthisfoundation, weex-
indextocomposethecompletefeaturedata.
tendedthemodelbyapplyinganadditionalbackwardlayerofLSTM
toenablelearningintwodirections.Atthesametime,theattention
layerwasintroducedtogeneratetheattentionscorethroughwhich
framesthatcontainlessusefulinformationcouldbeidentified.Then,
we showed the predicted results based on four different models
accordingtoourexperimentaldatain6-&2-classificationtasks.
5.1 BidirectionalLSTM+AttentionModel
The goal of our model was to predict fear levels for each frame
basedonacomprehensiveintegrationofasequenceofmulti-modal
data.Thisincludesskeletalpoints,audiofeatures,andphysiological
datarecordedinthesameperiodoftime.Tothisend,weadopted
LSTMasourbasemodel. Onthisbasis,weextendedthemodel
Figure4: ExampleofDigitalAudioInformation, wherethetopleft byapplyingabackwardlayerofLSTMtolearnintwodirections.
is the audio waveform plot, the top right is the spectral csentroid Thus,themodelwouldbeabletoutilizeinformationbothfromthe
visualization,thebottomleftisthezero-crossingrate(ZCR)visualiza- pastandfuture.Inaddition,theattentionlayerwerealsoadoptedto
tion,andthebottomrightistheMel-frequencycepstralcoefficients capturethemostsignificantsemanticinformationinthesequence.
(MFCC). Theattentionmechanismoperatesbycomputingattentionscoresfor
eachframeinthesequence.
4.5 DatasetConstruction AsshowninFigure5,theentirenetworkconsistedof4layers.In
general,wenowinputasequenceoffeaturesrepresentinglsucces-
Thevideoandaudiodurationsofthedatasetwere9hours,28min-
siveframesintothenetworkandobtainedtheoutputasthefearlevel
utes and 58 seconds and contained 967079 frames in total. Our
ofthefirstframeinthesequence.Thesefeaturesencompassskeletal
datasetconsistedof61dimensionsof3-modalfeaturesand3di-
data,audiocues,andphysiologicalresponses,collectivelyproviding
mensionsoffearlevellabels. Amongthefeatures,33dimensions
acomprehensivedatasetforanalysis.Thesequencecouldprovide
wereextractedfrom3Dkeypoints,26dimensionsdescribedaudio
theinformationconcerningtheaccelerationofhumanmotionsand
characteristics,and2dimensionswererelatedtophysiologicaldata.
thechangeofaudioandphysiologicalsignals. Betweentheinput
Thelabelscontainedtheresultsof2annotatorsandtheiraverage
andoutputlayers,weestablishedtheBLSTMlayerandtheattention
score. Afterdataannotation,weobtainedaglimpseofthedistri-
bution of fear levels in the entire dataset. According to Table 4,
layer.IntheBLSTMlayer,thereweretwohiddenstateshtandhˆ tfor
bothdirectionsusingthehiddenstatefromthepreviousstepandthe
participantsbarelyshowedanyindicationsofbeingscared58.18%
input.Thisbi-directionalprocessingiscrucialforunderstandingthe
ofthetime, whichwaslabeledaslevel0. Theproportionofthe
temporalcontextoffearresponses,asitaccountsfortheprogression
remainingfearcategoriesdecreasedwithincreasinglevel.Thus,the
andregressionofemotionalstates.FollowingtheBLSTMlayer,the
difficultyofthepredictiontaskwasincreasedtoacertainextent
attentionmechanismtakescenterstage.Itcomputesattentionscores
duetotheunevendistributionofthedataset. Thisalsoledtothe
foreachframebyapplyingalearnedtransformationtotheBLSTM
attentionmechanismbeingaddedtothemodel,whichisdiscussed
outputs.Theattentionscorecalculationformulais:
inthefollowingsection.
Table3:Descriptionofourdataset.Thefirstrowindicatestwodifferent ut=tanh(statet×weight W), (1)
tasks(6-and2-classification). Thesecondrowindicatesemotion
referencelabels.Inthethirdrow,0to5arethefearlevels(level0= whereut istheintermediaterepresentationattimestept,statet
non-fear;levels1-5,fearlevelfromthelowesttohighest,or1=fear). istheoutputstateoftheLSTMattimestept,andweightw isthe
Thefourthrowshowsthenumberoffearannotateddatainthedataset learnedweightmatrixfortransformingtheLSTMoutputstate.
foreachlevel. Thefifthrowrepresentstheratiooffear-annotated
d ava eta rao gf ee ha ec ah rtle rav te el ato ndth se tato ndta al rd da dt ea vs ie at t. ioR no fow rs ths eix dt io ffee ri eg nh tt lep vre els se on ft ft eh ae
r
a t∗=ut×weight proj, (2)
a ran tn eo ata nt ded std aa nt da a. rR do dw evn iain tie onto foe rle tv he en dp ifr fee rs ee nn tt lt eh ve ela sv oe fra feg ae rr ae ns np oir ta at to er dy wherea t∗representstherawattentionscoresattimestept,ut is
theintermediaterepresentationcomputedasperEquation1,and
data. Rowstwelvetofourteenintroduceacceleration, referringto
theaverage3Dskeletalpointaccelerationanditsstandarddeviation,
weight projisanotherlearnedweightmatrixusedforprojectingthe
intermediaterepresentationontotheattentionscores.
consideringitsmovementinallthreespatialdimensions(x-,y-,andz-
Thesescoresarethennormalizedusingthesoftmaxfunction:
axes).Thisaccelerationiscalculatedfromtheindividualaccelerations
ineachofthesedimensions.Allvaluesareroundedtotwodecimal
places.
6-classification 2-classification
a ti=
∑l
i=ex 0p e( xa pt∗ (i)
a t∗ i)
, (3)
Non-fear Fear Non-fear Fear
0 1 2 3 4 5 Total 0 1 Total
Count 562681 284204 78099 31466 10202 427 967079 562681 404398 967079 wherea∗ representsthenormalizedattentionscorefortheith
Radio 58.18% 29.39% 8.08% 3.25% 1.05% 0.04% 100% 58.18% 41.82% 100% ti
Heartrate frameofsequencet.Thesescoressignifytherelativeimportanceof
Mean 94.39 97.42 97.26 98.09 104.30 92.62 94.39 97.61
Std 17.11 17.50 17.92 16.15 21.24 4.80 17.11 17.61 eachframeinthecontextoftheentiresequence.
Breathrate
Mean 15.89 16.71 17.82 17.91 18.38 16.86 15.89 17.06 ThemodelthencomputesaweightedsumoftheBLSTMoutputs,
Std 5.61 5.24 5.77 5.74 5.73 5.31 5.61 5.42
Acceleration usingthenormalizedattentionscoresasweights. Thisstepeffec-
Mean 0.28 0.09 0.09 0.09 0.09 0.12 0.28 0.09
Std 51.08 0.28 0.57 0.07 0.55 0.12 51.08 0.35 tivelyaggregatesthesequenceinformation,withahigheremphasis
onframesdeemedmorerelevantbytheattentionmechanism:Table4:ComparisonoftheApproaches.Thefirstrowindicatestwo
l differenttasks(6-and2-classification).Thesecondrowindicatesthe
O t∗=∑a tiO ti, (4) modelused. Inthisstudy,wefocusonshowingBLSTM+attention
under6-classificationtask. Thethirdrowindicatestheaccuracyof
i=0
themodel,wheretheaccuracyofBLSTM+attention(6-classification)
whereO t∗istheweightedoutputforsequencet. Thisattention- is65.31%. Thefourthrowindicatestherecallofthemodel,where
focusedapproachallowsourmodeltobemoresensitiveandprecise therecallofBLSTM+attention(6-classification)is65.31%.Thefifth
inpredictingthefearlevelassociatedwitheachframe. row indicates the F1 value of the model, where the F1 value of
Finally,weutilizedfullconnected(FC)layerstofinishtheclassifi- BLSTM+attention(6-classification)is67.46%. Inaddition, arefer-
cationjob.Meanwhile,overfittingwasacommonissueinoptimizing encetotheresultsofthe2-classificationtaskisprovidedontheright
themodel.Toaddressthechallengeofoverfitting,weincorporated sideofthetable.Allmodelsweretrained,testedandvalidatedusing
dropout as a regularization technique in the FC layers. Dropout thesamedatasetprovidedinthisexperiments.
randomlydisablesafractionofneuronsduringthetrainingprocess, 6-classificationtask 2-classificationtask
whichhelpsinpreventingthemodelfrombecomingtoodependent LSTM LSTM+attention BLSTM BLSTM+attention LSTM BLSTM+attention
Accuracy 60.22% 59.41% 61.90% 65.31% 90.47% 76.96%
onspecificfeatures,thusenhancingitsgeneralizationcapabilities. Recall 59.69% 60.20% 61.74% 65.31% 90.47% 82.65%
F1 61.34% 62.34% 63.96% 67.46% 90.47% 83.09%
thatprovidesaudiofeatures. (5)ThelargestdatasizeintheVR
gameenvironment.(6)Wearetheonlymulti-modaldatabaseother
thanGranatoetal.[27]thatprovidesmoreinteractionbehaviorsin
VRenvironments.(7)Wedonotrequireparticipantstomakeself-
reportedannotationsduringthegame,asamomentaryandprecise
manner reporting would divide the user’s attention in VR [100].
However,westillprovideself-reporteddataattheendofthegame
toassistdataannotation.
Inaddition,wemustacknowledgethattheworkhascertainlimi-
tations,whicharefurtherdiscussedinsection7.1.However,wehave
fourmainaspectstosupportHCIandrelatedcommunities[96]thus
far:(1)providesaspecificapproachtoconstructingmulti-modalsen-
timentdatasets.Thispaperpresentsadetailedconstructionprocess
Figure5:ThearchitectureofBLSTM+attentionmodel.Xt,Yt indicate
foracompletedataset,includingexperimentalsetup,datacollection,
theinputandoutputonstept ofthemodel. ht andhˆ t standforthe annotation,analysis,andvalidation.Byreferringtothealreadyvali-
hiddenstatesofforwardlayerandbackwardlayerforeachstep.Ot is
datedconstructionprocesspresentedinthispaper,futureresearchers
thecorrespondingoutputofBLSTMmodel.
can more easily construct multi-modal sentiment datasets in VR
environmentsorinotherenvironments. (2)providesanadvanced
5.2 PredictionResults multi-modalemotiondatasetinVRfeargameenvironmentsthat
containscontinuoustimeseriessamplesandrichfeatures. Future
Duringtheexperiments,werandomlysplitthedatasetintotraining
researcherscanusethisdatasettotestalgorithmperformanceorasa
(80%),validation(10%)andtest(10%)sets.Thevalidationsetwas
referenceforapplicationdevelopment.(3)confirmtheeffectiveness
usedtotunethehyperparameters,andweevaluatedthemodelon
ofpredictingfearemotioninvirtualenvironmentsbycombining
thetestset. Tooptimizethemodel,wetestedmodelsondifferent
bodyposture,audio,andphysiologicaldata.(4)Providesananno-
parameters.
tationtoolwithavisualinterface. Thistoolcaneffectivelyassist
We applied different combinations of these parameters to the
in the annotation of raw data, significantly reducing annotators’
LSTM,LSTM+attention,BLSTMandBLSTM+attention.Mean-
operationaldifficultyandannotationerrorscausedbyasynchrony.
while,theperformancewasevaluatedby3metrics:accuracy,recall
andF1score. AccordingtoTable4, wefoundthattheattention
mechanismandthebackforwadlayerinBLSTMenhancedthepre-
7 DISCUSSIONSANDINSIGHT
dictiveabilityofthenetworktoacertainextentfor6-classification Byreviewingpastresearchandcombiningcurrenttrendsintech-
task. nologyandsociety,wediscoveredthatthereisstillaseriouslack
Ingeneral,theBLSTM+attentionreachedthehighestscoreon ofdiscussionofemotionalissuesinthemetaverse. VR,asoneof
allthreemetricsforthe6-classificationtask,upto65.31%accuracy. themostlikelymetaversebuildingenvironments,wasthefocusof
Thebestresultwasobtainedwhenthelearningratewas0.0001,the futureresearchanddevelopment.Emotionwasapartofeffectively
dropoutratewas0.5,thebatchsizewas256,thesequencelengthwas enhancingtherealisticandsocialsenseofthemeta-universe.Hence,
16,andwetrainedthemodelfor50epochs.Inaddition,weperform studiesonuserpsychologyandbehaviorinvirtualscenariosarenec-
asimplerecodingofthedatasetintoa2-classificationtasktotestthe essary.Inthissection,wefirstdiscussthelimitationsofthispaper.
performanceofmodelsinbothfearandnon-fearrecognitiontasks, Then,wealsodiscussedthechallengesassociatedwithemotionsin
andtheaccuracyisupto90.47%. themetaverse.
6 DATASETADVANCEMENT 7.1 Limitations
Wecomparedthedatasetproposedinthispaperwithsevenother Our work has the following limitations. (1) We did not use
datasetsproposedinpreviousstudies(seeTable5). Comparedto EDA/GSRinourexperiments,althoughitcanbeeffectiveiniden-
previousstudies,ourdatasetisunique,andhasfollowingadvantages, tifyingemotions[2,28,51]. ThelackofmobilityofGSRdevices
because(1)ourdatasetcontainsmoreinteractivebehaviorsinVR can create barriers to player interaction and degrade the gaming
environmentsthanwatchingvideos[77,82,84,99,101]orplaying experienceinVRgamingenvironments[18]. Fortunately,weun-
2Dgames[44]. (2)Onlyweprovidethefullbodyposefeatures. derstand that there is much cutting-edge research on the use of
(3) We provide up to 6 categories of data for a specific emotion gestures[32,40],whichmakesitpossibletoabandontheuseofjoy-
(fear).(4)WearetheonlydatasetotherthanSoleymanietal.[77] stickcontrollersinthefuture.Similarly,EEGsignalsareconsideredTable5:Comparisonofdatasets:Thetableshowssevenrepresentativedatasetsintherelevantfieldsfrom2012to2022andthebasicinformation
ofourproposeddataset.
Author Soleymanietal.[77] Xueetal.[99] Granatoetal.[27] Yuetal.[101] Tabbaaetal.[84] Kuttetal.[44] Suhaimietal.[82] Ours(VRMN-bD)
Year 2012 2015 2020 2021 2021 2022 2022 2023
32participant 2games 568mins
120trials∗25
Samples 538 ∗ foreach participants∗4s 312 ≈7650mins ≈20000rows and58seconds
8delectedvideos participant (967079rows)
DataFusion LateFusion Pre-Fusion Pre-Fusion Singlechannelγ Pre-Fusion Datasetonlyγ Pre-Fusion Pre-Fusion
Subjectα 27 32 33 25 26 102 32 23
St Mim eu thla ot dion 2DVideo 360◦VRVideo and2 VD RG Gam ame eβ VRVideo 360◦VRVideo Ima2 gD eaG na dm Ae u, dio 360◦VRVideo VRGame
Active × × ✓ × × ✓ × ✓
Interactions
Continuous
C C+D C C C C C C+D
/Discrete
Audio ✓ × × × × × × ✓
✓(33features
Body × × × × × × × 3dimensionsδ)
Face ✓(20features) × × × × × × ×
Modality EE Ey Ge ✓ ✓ ✓ × ✓× ✓× ✓ × × × ✓× × ×
GSR × ✓ ✓ × ✓ ✓ × ×
HR\HRV\ECG ✓ ✓ ✓ × ✓ ✓ × ✓
BR ✓ × ✓ × × × × ✓
Acceleration × ✓ × × × ✓ ✓ ✓
screen
Other − SKT,BVP EMG − − − gyroscope
recording
Annotation
Self-report Self-report Self-report Self-report Self-report Self-report Pre-labeling Annotation
Method
Expression SBEplusAnxiety positive,neutral SEBplusCalm happy,scared,
Valence-Arousal Valence-Arousal SEBplusContempt Fear
Classification andAmusement andnegative andAnxious calm,andbored
ValenceLevel
ofClassificationε 3+3 5 5(ingreneral) 1 9 3+3+3 1 6
SBE=SevenBasicEmotions(anger,disgust,fear,happy,sad,surprise,andneutral).
αThefinalnumberofparticipants
βRacinggames:participantsdonotneedtostandupandotheractions.
γNotapplicablebecauseofreasons.
δContains98%ofinformation.
εMaximumlevel.
tobeaneffectivechannelforsensinghumanemotions,butbased sense of presence in the VR experience [95]. To best avoid the
onthelargeconflictinwearcompatibilitybetweenexistingEEG effectof3Dvertigosymptoms,weappliedstrategiestominimize
devicesandVRdevices(especiallyVRhead-mounteddisplays), theeffects, suchasselectingparticipantswhoself-reported"no"
usingbothdevicesatthesametimecanaffectthequalityofEEG 3Dvertigoforthisstudy,andwedidnotfindsignificant3Dvertigo
signalsduetoplayermovementsthatcausetheEEGtomoveor symptomsforotherparticipantsduringtheexperimentprocess.In
falloffandaffecttheVRgamingexperienceduetodisruptionof addition,wehaveaddedintervalsindifferentgamestoavoid3D
movement[33,99].Thus,althoughtheabovedeviceswerenotused vertigosymptoms[64].Weletparticipantsplayeachgameforno
intheconstructionofthedataset,amorerealisticprocessofplayer morethan20minutes,andtheshorterexperiencewasaneffective
experiencewasrecorded.Thisdataset,becauseofitsextremelyhigh measuretoavoid3Dvertigosymptoms[3]. However,itisstilla
confidencelevel,couldhelpinthedevelopmentofwearabledevices challengetoavoid3Dvertigosymptomsespeciallyforalong-term
forvirtualrealityactivitiesinthefuture. (2)Wedidnotconsider experience[86].(6)Thedemographicsoftheparticipantsmaybea
eyetrackinginourexperiments,althoughpreviousworkhasdemon- limitation. TheparticipantswereallfromtheChinesepopulation,
stratedacorrelationbetweeneyemovementsandemotion[26],and whichmightintroducebiasinthedataset,particularlyduetodif-
thepotentialforhigheraccuracyinmulti-classemotionrecognition ferentacquiredfearsandreactionsinfearstatescausedbydiverse
tasks[77,84].However,withthecurrenttechnology,eyetrackingVR culturalbackgroundsandhabits,andthiscouldlimittheabilityto
devicesaredifficulttogeneralizeintermsofcost[10,54],whichis generalizetheresultstoothergroupsofpeople. (7)Thedataset
beyondthecurrentscope.Wewillinvestigatethisworkinthefuture. proposedinthisstudyisrichininformation,yetthispaperdoesnot
Thisproblemwilllikelybesolvedastheusabilityandperformance includeallpossibleandinterestingrelatedresearch,especiallythose
ofwearabledevicesimprove.However,wirelessphysiologicalmea- provenbutinneedoffurtherexploration,suchasstudiesonspecific
surements are still a recognized limitation [55] at this time. (3) features(likeposturalacceleration)inrelationtotypesoffear,re-
Althoughalargenumberofstrategieswereusedinthisstudytotry searchontherelationshipbetweengenderandlevelsoffear[102],
toavoidindividualdifferences,thereiscurrentlynoeffectivewayto andstudiesontheimpactofsocialenvironmentandpsychological
completelyeliminateindividualdifferences.Amongthem,thealgo- factorsonbehaviorinplayingVRhorrorgames.
rithmismoredifficulttorecognizethelow-fearannotationsection.
Forthisreason,theaccuracyimprovedsubstantiallyafterre-coding
7.2 InsightsforResearchersandGameDevelopers
themulti-classification(6-classification)taskintoa2-classification
task.(4)Weunderstandthatrunningmachinelearningmodelscon- Inmachinelearning,theimportanceofhigh-qualitydatasetswas
sumessignificantcomputationalresourcesandthatourproposed obvious,especiallyinthecontextoftherecentpopularityofLLMs.
modelandparametersmaynotbeoptimalsolutions.Therefore,we Thehigh-qualityfearsentimentdatasetconstructedinthispaper
plantoopen-sourcethedatasetforfutureresearch.Inaddition,we alloweddeveloperstoskipthecomplexandtediousdatacollection
notethesurprise/shockbroughttothescientificcommunitybythe andthusstudythespecificproblemdirectly(e.g.,furthermodeling).
popularityofAItoolsbuiltonlargelanguagemodels(LLMs)since TheconstructedmodelforidentifyinghumanfearemotionsinVR
2019,especiallyChatGPT[9,63].Thetuningofmodelsandtheir environmentswouldenabledeveloperstoverifytheplayer’sfear
parametersforbetterperformancethroughAIiterationshasbeen level in the game easily and further understand the player’s fear
achieved,andthereforetheauthorswishtoemphasizetheimpor- response,whichhelpedthedesignofgameflow,hardwaredevices,
tanceofdatasetsratherthanthemodelsthemselves.(5)3Dvertigo security,etc.Inrealapplicationdevelopment,researchersordevelop-
isanimportantflawintheVRexperience[25]thatcanreducethe erscanuseouralreadytrainedmodelstounderstandplayers’fearful
emotionsandtestwhethergamesandapplicationssuccessfullyelicitfearfromusers. Furthermore,developerscanuseourresearchto analysisofskinconductance.IEEETransactionsonAffectiveCom-
designgamesandapplicationswithdifferentpacing,difficulty,or puting,13(4):2047–2057,2022.doi:10.1109/TAFFC.2022.3197842
styles.Wehopethattheprovidedmethodsforcollecting,processing, [4] A.H.Bettis,T.A.Burke,J.Nesi,andR.T.Liu.Digitaltechnologies
andapplyingtime-series-baseddata,especiallypredictivemodels, foremotion-regulationassessmentandintervention: Aconceptual
offerthepossibilitytofurtherconsidertheplayerexperiencetody- review.ClinicalPsychologicalScience,10(1):3–26,2022.
namicallyadjusttheapplicationscenario,difficultyandatmosphere. [5] U.Bhattacharya, T.Mittal, R.Chandra, T.Randhavane, A.Bera,
Inaddition,oneofthepurposesofourstudyusingVRasamediator andD.Manocha. Step: Spatialtemporalgraphconvolutionalnet-
istoconsiderthesenseofrealismandimmersion.Inotherwords, worksforemotionperceptionfromgaits.InProceedingsoftheAAAI
ConferenceonArtificialIntelligence,vol.34,pp.1342–1350,2020.
theplayer’sresponseisclosertotheactualscene. Therefore,the
[6] A.Bhavan,P.Chauhan,R.R.Shah,etal. Baggedsupportvector
predictivemodelsandtheoreticalcontributionshavearobustap-
machinesforemotionrecognitionfromspeech. Knowledge-Based
plicationinreallife,suchascreatinganarchitecturalatmosphere
Systems,184:104886,2019.
(e.g., hauntedhouses, escaperooms), therapy(overcomingfear),
[7] S.E.Bibri. Thesocialshapingofthemetaverseasanalternative
andunderstandingorscenarioreenactment(real-lifefears).
totheimaginariesofdata-drivensmartcities: Astudyinscience,
technology,andsociety.SmartCities,5(3):832–874,2022.
8 CONCLUSIONANDFUTUREWORK
[8] S.Bouchard,F.Bernier,É.Boivin,B.Morin,andG.Robillard.Us-
Emotionshaveasignificantimpactonsociallifeandhumandevel- ingbiofeedbackwhileimmersedinastressfulvideogameincreases
opment. ThedevelopmentofVRandmetaversediscussionshave theeffectivenessofstressmanagementskillsinsoldiers. PloSone,
broughtthetopicofemotionsinVRenvironmentstounprecedented 7(4):e36169,2012.
attention.Peoplewanttocreateametaversewithmorepossibilities, [9] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schul-
andemotionsinthevirtualenvironmentaddmoreopportunitiesand man, J. Tang, and W. Zaremba. Openai gym. arXiv preprint
humanitytothisnew"world".Emotionshaveasignificantimpact arXiv:1606.01540,2016.
onsociallifeandhumandevelopment. ThedevelopmentofVR [10] M.Burch,R.Haymoz,andS.Lindau.Thebenefitsanddrawbacksof
andmetaversediscussionshavebroughtunprecedentedattentionto eyetrackingforimprovingeducationalsystems.In2022Symposium
thetopicofemotionsinVRenvironments.Ourresearchaddresses onEyeTrackingResearchandApplications,pp.1–5,2022.
[11] J.CANTOR.FRIGHTREACTIONSTOMASSMEDIA.InMedia
humanfearemotionsinvirtualenvironments,providinganeffective
Effects.Routledge,thirded.,2008.
way to applymulti-modaldata toidentifyfearemotions. To ad-
[12] Z.Cao,G.HidalgoMartinez,T.Simon,S.Wei,andY.A.Sheikh.
dresstheseissues,weprovidethecompleteexperimentalprocedure,
Openpose:Realtimemulti-person2dposeestimationusingpartaffin-
whichincludesgameselection,experimentalsteps,datacollection
ityfields.IEEETransactionsonPatternAnalysisandMachineIntel-
andlabelingmethods,datasetconstruction,andpredictionmodel
ligence,2019.
training. Overall,weprovideahigh-qualitymulti-modal(videos,
[13] T.Charoensook,M.Barlow,andE.Lakshika.Heartrateandbreathing
audio,andphysiologicalsignals)immersivehumanfearresponses
variabilityforvirtualrealitygameplay. In2019IEEE7thInterna-
dataset(VRMN-bD)andafearpredictionmodelwithanaccuracyof tionalConferenceonSeriousGamesandApplicationsforHealth
upto65.31%under6-classifications,andaccuracyofupto90.47% (SeGAH),pp.1–7,2019.doi:10.1109/SeGAH.2019.8882434
under2-classificationstask.Wealsoprovideavisualannotationtool [14] L.S.Chen,T.S.Huang,T.Miyasato,andR.Nakatsu. Multimodal
formulti-modaldataasapartofcontributiontotheresearch. humanemotion/expressionrecognition.InProceedingsThirdIEEE
Inadditiontofurtheroptimizingthemodelandaddressingthe InternationalConferenceonAutomaticFaceandGestureRecognition,
previously mentioned limitations, we hope to conduct future re- pp.366–371.IEEE,1998.
searchonfearemotionsandbehaviorstrategiesinotherinteresting [15] C.Clavel,I.Vasilescu,L.Devillers,G.Richard,andT.Ehrette.Fear-
scenarios.Moreover,weaimtoextendthisresearchtoothertypes typeemotionrecognitionforfutureaudio-basedsurveillancesystems.
ofemotions,andtocompareandanalyzefearandotheremotional SpeechCommunication,50(6):487–503,2008.
experiencesinVRenvironments. Thiscouldhelpinidentifying [16] M.Coulson. Attributingemotiontostaticbodypostures:Recogni-
deeperrelationshipsbetweenvarioushumanemotions,especially tionaccuracy,confusions,andviewpointdependence. Journalof
inthecontextofthefuturemetaverse.Weplantoexpandthenum- nonverbalbehavior,28(2):117–139,2004.
[17] A.Davis,J.Murphy,D.Owens,D.Khazanchi,andI.Zigurs.Avatars,
ber and diversity of participants in future studies, increasing the
people,andvirtualworlds:Foundationsforresearchinmetaverses.
numberofparticipantsandconsideringuniquehorrorelementsin
JournaloftheAssociationforInformationSystems,10(2):1,2009.
differentculturalcontexts,aswellasthespecificunderstandingand
[18] M.E.Dawson,A.M.Schell,andD.L.Filion.TheElectrodermalSys-
responsestofearamongdifferentgroupsofpeople. Finally, the
tem,p.217–243.CambridgeHandbooksinPsychology.Cambridge
dataset,pre-trainedmodel,andmoreinformationareavailableat
UniversityPress,4ed.,2016.doi:10.1017/9781107415782.010
https://github.com/KindOPSTAR/VRMN-bD.
[19] R. J. Dolan. Emotion, cognition, and behavior. science,
298(5596):1191–1194,2002.
ACKNOWLEDGMENTS
[20] L.Downs,A.Francis,N.Koenig,B.Kinman,R.Hickman,K.Rey-
ThisworkwassupportedbytheYouthProgramofChineseMinistry mann,T.B.McHugh,andV.Vanhoucke.Googlescannedobjects:A
ofEducationHumanitiesandSocialSciencesProject(GrantNo. high-qualitydatasetof3dscannedhouseholditems.arXivpreprint
23YJCZH049). arXiv:2204.11918,2022.
[21] D.Freeman,P.Haselton,J.Freeman,B.Spanlang,S.Kishore,E.Al-
REFERENCES bery,M.Denne,P.Brown,M.Slater,andA.Nickless. Automated
[1] M.Ali,A.H.Mosa,F.A.Machot,andK.Kyamakya.Emotionrecog- psychologicaltherapyusingimmersivevirtualrealityfortreatmentof
nitioninvolvingphysiologicalandspeechsignals:Acomprehensive fearofheights:asingle-blind,parallel-group,randomisedcontrolled
review.Recentadvancesinnonlineardynamicsandsynchronization, trial.TheLancetPsychiatry,5(8):625–632,2018.
pp.287–302,2018. [22] X.Fu,C.Xue,Q.Yin,Y.Jiang,Y.Li,Y.Cai,andW.Sun.Gesture
[2] E.Babaei,B.Tag,T.Dingler,andE.Velloso. Acritiqueofelec- basedfearrecognitionusingnonperformancedatasetfromvrhorror
trodermalactivitypracticesatchi. InProceedingsofthe2021CHI games.In20219thInternationalConferenceonAffectiveComputing
ConferenceonHumanFactorsinComputingSystems,CHI’21.Asso- andIntelligentInteraction(ACII),pp.1–8.IEEE,2021.
ciationforComputingMachinery,NewYork,NY,USA,2021.doi: [23] K.Games.Phasmophobia,2020.Lastaccessed06April2022.
10.1145/3411764.3445370 [24] J.Gao,P.Li,Z.Chen,andJ.Zhang.Asurveyondeeplearningfor
[3] A.Baldini,S.Frumento,D.Menicucci,A.Gemignani,E.P.Scilingo,
multimodaldatafusion.NeuralComputation,32(5):829–864,2020.
andA.Greco.Subjectivefearinvirtualreality:Alinearmixed-effects [25] Y.Gao,A.Chen,S.Chi,G.Zhang,andA.Hao.Analysisofemotionaltendencyandsyntacticpropertiesofvrgamereviews.In2022IEEE [43] M.Kors,E.D.VanderSpek,andB.A.Schouten.Afoundationfor
ConferenceonVirtualRealityand3DUserInterfacesAbstractsand thepersuasivegameplayexperience.InFDG,2015.
Workshops(VRW),pp.648–649, 2022.doi: 10.1109/VRW55335. [44] K.Kutt,D.Dra˛z˙yk,L.Z˙uchowska,M.Szela˛z˙ek,S.Bobek,andG.J.
2022.00175 Nalepa.Biraffe2,amultimodaldatasetforemotion-basedpersonaliza-
[26] C.Geraets,S.K.Tuente,B.Lestestuiver,M.VanBeilen,S.Nijman, tioninrichaffectivegameenvironments.ScientificData,9(1):1–15,
J.Marsman,andW.Veling.Virtualrealityfacialemotionrecognition 2022.
insocialenvironments:Aneye-trackingstudy.InternetInterventions, [45] M.LaMuraandP.Lamberti.Human-machineinteractionpersonal-
25:100432,2021. ization:areviewongenderandemotionrecognitionthroughspeech
[27] M.Granato, D.Gadia, D.Maggiorini, andL.A.Ripamonti. An analysis. In2020IEEEInternationalWorkshoponMetrologyfor
empiricalstudyofplayers’emotionsinvrracinggamesbasedona Industry4.0&IoT,pp.319–323.IEEE,2020.
datasetofphysiologicaldata. MultimediaToolsandApplications, [46] P.J.Lang. Cognitioninemotion: Conceptandaction. Emotions,
79(45):33657–33686,2020. cognition,andbehavior,191:228,1984.
[28] K.Gupta,S.W.T.Chan,Y.S.Pai,N.Strachan,J.Su,A.Sumich, [47] P.J.Lang. Thecognitivepsychophysiologyofemotion: Fearand
S.Nanayakkara,andM.Billinghurst.Totalvrecall:Usingbiosignals anxiety.InAnxietyandtheanxietydisorders,pp.131–170.Routledge,
torecognizeemotionalautobiographicalmemoryinvirtualreality. 2019.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,6(2),jul [48] D.G.Larson,R.L.Chastain,W.T.Hoyt,andR.Ayzenberg. Self-
2022.doi:10.1145/3534615 concealment: Integrativereviewandworkingmodel. Journalof
[29] J.HailstoneandA.E.Kilding.Reliabilityandvalidityofthezephyr™ SocialandClinicalpsychology,34(8):705,2015.
bioharness™tomeasurerespiratoryresponsestoexercise.Measure- [49] L.-H.Lee,T.Braud,P.Zhou,L.Wang,D.Xu,Z.Lin,A.Kumar,
mentinPhysicalEducationandExerciseScience,15(4):293–300, C.Bermejo,andP.Hui.Alloneneedstoknowaboutmetaverse:A
2011. completesurveyontechnologicalsingularity,virtualecosystem,and
[30] S.R.Harris,R.L.Kemmerling,andM.M.North.Briefvirtualreality researchagenda.arXivpreprintarXiv:2110.05352,2021.
therapyforpublicspeakinganxiety. Cyberpsychology&behavior, [50] J.-H.T.Lin. Fearinvirtualreality(VR):Fearelements, coping
5(6):543–550,2002. reactions,immediateandnext-dayfrightresponsestowardasurvival
[31] K.HarrisonandJ.Cantor. TalesfromtheScreen:EnduringFright horrorzombievirtualrealitygame.ComputersinHumanBehavior,
ReactionstoScaryMedia. MediaPsychology,1(2):97–116,June 72:350–361,July2017.doi:10.1016/j.chb.2017.02.057
1999.doi:10.1207/s1532785xmep0102_1 [51] T.B.Lonsdorf,M.M.Menz,M.Andreatta,M.A.Fullana,A.Golkar,
[32] E.Hayashi,J.Lien,N.Gillian,L.Giusti,D.Weber,J.Yamanaka, J.Haaker,I.Heitland,A.Hermann,M.Kuhn,O.Kruse,etal.Don’t
L.Bedal,andI.Poupyrev. Radarnet: Efficientgesturerecognition fear‘fearconditioning’:Methodologicalconsiderationsforthedesign
techniqueutilizingaminiatureradarsensor. InProceedingsofthe andanalysisofstudiesonhumanfearacquisition,extinction,and
2021CHIConferenceonHumanFactorsinComputingSystems,CHI returnoffear.Neuroscience&BiobehavioralReviews,77:247–285,
’21.AssociationforComputingMachinery,NewYork,NY,USA, 2017.
2021.doi:10.1145/3411764.3445367 [52] T.LynchandN.Martins. NothingtoFear? AnAnalysisofCol-
[33] L.He,H.Li,T.Xue,D.Sun,S.Zhu,andG.Ding. Amiinthe lege Students’ Fear Experiences With Video Games. Journal of
theater?usabilitystudyofliveperformancebasedvirtualreality.In Broadcasting&ElectronicMedia,59(2):298–317,Apr.2015.doi:10.
Proceedingsofthe24thACMSymposiumonVirtualRealitySoftware 1080/08838151.2015.1029128
andTechnology,VRST’18.AssociationforComputingMachinery, [53] Y.Matsuda,D.Fedotov,Y.Takahashi,Y.Arakawa,K.Yasumoto,and
NewYork,NY,USA,2018.doi:10.1145/3281505.3281508 W.Minker.Emotour:Multimodalemotionrecognitionusingphysio-
[34] S.Hitchcock.Emilywantstoplay.PC,ShawnHitchcock,2015. logicalandaudio-visualfeatures. InProceedingsofthe2018ACM
[35] C.Izard. Thepsychologyofemotions.newyork,london: Plenum InternationalJointConferenceand2018InternationalSymposiumon
press,1991. PervasiveandUbiquitousComputingandWearableComputers,pp.
[36] P.Jemioło,D.Storman,B.Giz˙ycka,andA.Lige˛za. Emotionelici- 946–951,2018.
tationwithstimulidatasetsinautomaticaffectrecognitionstudies– [54] M.Meißner,J.Pfeiffer,T.Pfeiffer,andH.Oppewal. Combining
umbrellareview.InIFIPConferenceonHuman-ComputerInterac- virtualrealityandmobileeyetrackingtoprovideanaturalisticex-
tion,pp.248–269.Springer,2021. perimentalenvironmentforshopperresearch. JournalofBusiness
[37] C.Jicol,C.H.Wan,B.Doling,C.H.Illingworth,J.Yoon,C.Headey, Research,100:445–458,2019.
C.Lutteroth,M.J.Proulx,K.Petrini,andE.O’Neill. Effectsof [55] B.MeulemanandD.Rudrauf. Inductionandprofilingofstrong
emotionandagencyonpresenceinvirtualreality.InProceedingsof multi-componentialemotionsinvirtualreality. IEEETransactions
the2021CHIConferenceonHumanFactorsinComputingSystems, onAffectiveComputing,12(1):189–202,2021.doi:10.1109/TAFFC.
pp.1–13,2021. 2018.2864730
[38] I.Kakkos,G.N.Dimitrakopoulos,L.Gao,Y.Zhang,P.Qi,G.K. [56] M.Moghimi,R.Stone,andP.Rotshtein. Affectiverecognitionin
Matsopoulos,N.Thakor,A.Bezerianos,andY.Sun.Mentalworkload dynamicandinteractivevirtualenvironments.IEEETransactionson
drivesdifferentreorganizationsoffunctionalcorticalconnectivity AffectiveComputing,11(1):45–62,2020.doi:10.1109/TAFFC.2017.
between2dand3dsimulatedflightexperiments.IEEETransactions 2764896
onNeuralSystemsandRehabilitationEngineering,27(9):1704–1713, [57] S.Mystakidis.Metaverse.Encyclopedia,2(1):486–497,2022.
2019. [58] D.Nepi, A.Sbrollini, A.Agostinelli, E.Maranesi, M.Morettini,
[39] T.KeshariandS.Palaniswamy.Emotionrecognitionusingfeature- F. Di Nardo, S. Fioretti, P. Pierleoni, L. Pernini, S. Valenti, and
levelfusionoffacialexpressionsandbodygestures. In2019In- L.Burattini.Validationoftheheart-ratesignalprovidedbythezephyr
ternationalConferenceonCommunicationandElectronicsSystems bioharness3.0.In2016ComputinginCardiologyConference(CinC),
(ICCES),pp.1184–1189.IEEE,2019. pp.361–364,2016.
[40] D.Kim,K.Park,andG.Lee.Atatouch:Robustfingerpinchdetection [59] S.Oh,J.-Y.Lee,andD.K.Kim.Thedesignofcnnarchitecturesfor
foravrcontrollerusingrfreturnloss. InProceedingsofthe2021 optimalsixbasicemotionclassificationusingmultiplephysiological
CHIConferenceonHumanFactorsinComputingSystems,CHI’21. signals.Sensors,20(3):866,2020.
AssociationforComputingMachinery,NewYork,NY,USA,2021. [60] F. Pallavicini, A. Ferrari, A. Pepe, G. Garcea, A. Zanacchi, and
doi:10.1145/3411764.3445442 F.Mantovani.EffectivenessofVirtualRealitySurvivalHorrorGames
[41] A.Kirk.Datavisualisation:Ahandbookfordatadrivendesign.Sage, fortheEmotionalElicitation:PreliminaryInsightsUsingResident
2016. Evil7:Biohazard.InM.AntonaandC.Stephanidis,eds.,Universal
[42] J.M.Kivikangas. Emotiontheories,theaffectivesystem,andwhy AccessinHuman-ComputerInteraction.Virtual, Augmented, and
adigitalgamesresearchershouldcare.InEvolutionaryPsychology IntelligentEnvironments,ComputerScience,pp.87–101.Springer
andDigitalGames,pp.73–92.Routledge,2018. InternationalPublishing,Cham,2018.doi:10.1007/978-3-319-92052-8_8 [83] Z.Sun,L.Li,Y.Liu,X.Du,andL.Li.Ontheimportanceofbuilding
[61] T.D.ParsonsandA.A.Rizzo.Affectiveoutcomesofvirtualreality high-qualitytrainingdatasetsforneuralcodesearch.InProceedings
exposuretherapyforanxietyandspecificphobias:Ameta-analysis. ofthe44thInternationalConferenceonSoftwareEngineering,pp.
JournalofBehaviorTherapyandExperimentalPsychiatry,39(3):250– 1609–1620,2022.
261,Sept.2008.doi:10.1016/j.jbtep.2007.07.007 [84] L.Tabbaa,R.Searle,S.M.Bafti,M.M.Hossain,J.Intarasisrisawat,
[62] R.W.Picard.Affectivecomputingforhci.InHCI(1),pp.829–833. M.Glancy,andC.S.Ang.Vreed:Virtualrealityemotionrecognition
Citeseer,1999. datasetusingeyetracking&physiologicalmeasures.Proceedingsof
[63] A.Radford,J.Wu,R.Child,D.Luan,D.Amodei,I.Sutskever,etal. theACMonInteractive,Mobile,WearableandUbiquitousTechnolo-
Languagemodelsareunsupervisedmultitasklearners.OpenAIblog, gies,5(4):1–20,2021.
1(8):9,2019. [85] K.Tabbert,R.Stark,P.Kirsch,andD.Vaitl.Dissociationofneural
[64] A.N.RamaseriChandra,F.ElJamiy,andH.Reza. Asystematic responsesandskinconductancereactionsduringfearconditioning
surveyoncybersicknessinvirtualenvironments.Computers,11(4):51, withandwithoutawarenessofstimuluscontingencies.Neuroimage,
2022. 32(2):761–770,2006.
[65] J.Šalkevicius,R.Damaševicˇius,R.Maskeliunas,andI.Laukiene˙. [86] N. Tian, P. Lopes, and R. Boulic. A review of cybersickness in
Anxietylevelrecognitionforvirtualrealitytherapysystemusing head-mounteddisplays:raisingattentiontoindividualsusceptibility.
physiologicalsignals.Electronics,8(9):1039,2019. VirtualReality,pp.1–33,2022.
[66] L.Santamaria-Granados,M.Munoz-Organero,G.Ramirez-Gonzalez, [87] V.Toast.Richie’splankexperience.GameWebsite).AccessedJan-
E.Abdulhay,andN.Arunkumar. Usingdeepconvolutionalneural uary,1:2020,2016.
network for emotion detection on a physiological signals dataset [88] M.-F.TsaiandC.-H.Chen. Spatialtemporalvariationgraphcon-
(amigos).IEEEAccess,7:57–67,2018. volutionalnetworks(stv-gcn)forskeleton-basedemotionalaction
[67] T.Sapin´ski,D.Kamin´ska,A.Pelikant,andG.Anbarjafari.Emotion recognition.IEEEAccess,9:13870–13877,2021.
recognitionfromskeletalmovements.Entropy,21(7):646,2019. [89] N.TsuchiyaandR.Adolphs.Emotionandconsciousness.Trendsin
[68] G.Sartory,S.Rachman,andS.Grey. Aninvestigationoftherela- cognitivesciences,11(4):158–167,2007.
tionbetweenreportedfearandheartrate. BehaviourResearchand [90] P.Tzirakis,J.Zhang,andB.W.Schuller.End-to-endspeechemotion
Therapy,1977. recognitionusingdeepneuralnetworks.In2018IEEEinternational
[69] S.Scheveneels, Y.Boddez, andD.Hermans. Predictingclinical conferenceonacoustics,speechandsignalprocessing(ICASSP),pp.
outcomesviahumanfearconditioning:Anarrativereview.Behaviour 5089–5093.IEEE,2018.
ResearchandTherapy,142:103870,2021. [91] M.Vrigkas,C.Nikou,andI.A.Kakadiaris. Areviewofhuman
[70] J.Sebastian,P.Pierucci,etal.Fusiontechniquesforutterance-level activityrecognitionmethods. FrontiersinRoboticsandAI,2:28,
emotionrecognitioncombiningspeechandtranscripts.InInterspeech, 2015.
pp.51–55,2019. [92] F.-Y.Wang,R.Qin,X.Wang,andB.Hu.Metasocietiesinmetaverse:
[71] S.Shao,Z.Li,T.Zhang,C.Peng,G.Yu,X.Zhang,J.Li,andJ.Sun. Metaeconomicsandmetamanagementformetaenterprisesandmetac-
Objects365:Alarge-scale,high-qualitydatasetforobjectdetection.In ities.IEEETransactionsonComputationalSocialSystems,9(1):2–7,
ProceedingsoftheIEEE/CVFinternationalconferenceoncomputer 2022.
vision,pp.8430–8439,2019. [93] Y.Wang,W.Song,W.Tao,A.Liotta,D.Yang,X.Li,S.Gao,Y.Sun,
[72] J. Shi, C. Liu, C. T. Ishi, and H. Ishiguro. Skeleton-based emo- W.Ge,W.Zhang,etal.Asystematicreviewonaffectivecomputing:
tionrecognitionbasedontwo-streamself-attentionenhancedspatial- Emotionmodels,databases,andrecentadvances.InformationFusion,
temporalgraphconvolutionalnetwork.Sensors,21(1):205,2020. 2022.
[73] D.Shin. Theactualizationofmetaaffordances: Conceptualizing [94] H.H.Watkins.Thesilentabreaction.InternationalJournalofClinical
affordanceactualizationinthemetaversegames.ComputersinHuman andExperimentalHypnosis,28(2):101–113,1980.
Behavior,133:107292,2022. [95] S.Weech,S.Kenny,andM.Barnett-Cowan.Presenceandcybersick-
[74] M.N.Shiota.Ekman’sTheoryofBasicEmotions.InTheSAGEEncy- nessinvirtualrealityarenegativelyrelated:areview. Frontiersin
clopediaofTheoryinPsychology,pp.249–250.SAGEPublications, psychology,10:158,2019.
Inc.,ThousandOaks,,2016.doi:10.4135/9781483346274 [96] J.O.WobbrockandJ.A.Kientz.Researchcontributionsinhuman-
[75] Siddharth,T.-P.Jung,andT.J.Sejnowski.Utilizingdeeplearningto- computerinteraction.interactions,23(3):38–44,2016.
wardsmulti-modalbio-sensingandvision-basedaffectivecomputing. [97] Y.Wu,R.Gu,Q.Yang,andY.-j.Luo. Howdoamusement,anger
IEEETransactionsonAffectiveComputing,13(1):96–107,2022.doi: andfearinfluenceheartrateandheartratevariability? Frontiersin
10.1109/TAFFC.2019.2916015 neuroscience,13:1131,2019.
[76] M.Slater.Placeillusionandplausibilitycanleadtorealisticbehaviour [98] N.Xi,J.Chen,F.Gama,M.Riar,andJ.Hamari. Thechallenges
inimmersivevirtualenvironments. PhilosophicalTransactionsof ofenteringthemetaverse:Anexperimentontheeffectofextended
theRoyalSocietyB:BiologicalSciences,364(1535):3549–3557,Dec. realityonworkload.InformationSystemsFrontiers,pp.1–22,2022.
2009.doi:10.1098/rstb.2009.0138 [99] T.Xue,A.ElAli,T.Zhang,G.Ding,andP.Cesar. Ceap-360vr:A
[77] M.Soleymani,J.Lichtenauer,T.Pun,andM.Pantic.Amultimodal continuousphysiologicalandbehavioralemotionannotationdataset
databaseforaffectrecognitionandimplicittagging. IEEEtransac- for360vrvideos.IEEETransactionsonMultimedia,2021.
tionsonaffectivecomputing,3(1):42–55,2011. [100] T.Xue,A.ElAli,T.Zhang,G.Ding,andP.Cesar.Rcea-360vr:Real-
[78] R.L.Solomon.Theopponent-processtheoryofacquiredmotivation: time,continuousemotionannotationin360°vrvideosforcollecting
Thecostsofpleasureandthebenefitsofpain.AmericanPsychologist, preciseviewport-dependentgroundtruthlabels.InProceedingsofthe
35(8):691–712,1980.doi:10.1037/0003-066X.35.8.691 2021CHIConferenceonHumanFactorsinComputingSystems,CHI
[79] R. Somarathna, T. Bednarz, and G. Mohammadi. Virtual reality ’21.AssociationforComputingMachinery,NewYork,NY,USA,
foremotionelicitation–areview. IEEETransactionsonAffective 2021.doi:10.1145/3411764.3445487
Computing,pp.1–21,2022.doi:10.1109/TAFFC.2022.3181053 [101] M.Yu,S.Xiao,M.Hua,H.Wang,X.Chen,F.Tian,andY.Li.Eeg-
[80] M.Sparkes.Whatisametaverse,2021. basedemotionrecognitioninanimmersivevirtualrealityenvironment:
[81] R.L.Spitzer,M.E.Gibbon,A.E.Skodol,J.B.Williams,andM.B. Fromlocalactivitytobrainnetworkfeatures. BiomedicalSignal
First. DSM-IVcasebook: AlearningcompaniontotheDiagnostic ProcessingandControl,72:103349,2022.
andStatisticalManualofMentalDisorders. AmericanPsychiatric [102] H.Zhang,X.Li,C.Qiu,andX.Fu.Decodingfear:Exploringuser
Association,1994. experiencesinvirtualrealityhorrorgames,2023.
[82] N.S.Suhaimi,J.Mountstephens,andJ.Teo.Adatasetforemotion [103] M.ZuckermanandM.Gagne. Thecoperevised: Proposinga5-
recognitionusingvirtualrealityandeeg(der-vreeg):Emotionalstate factormodelofcopingstrategies.JournalofResearchinPersonality,
classificationusinglow-costwearablevr-eegheadsets.BigDataand 37(3):169–204,2003.
CognitiveComputing,6(1):16,2022.