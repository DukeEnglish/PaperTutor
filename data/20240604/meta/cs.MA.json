[
    {
        "title": "Fusion-PSRO: Nash Policy Fusion for Policy Space Response Oracles",
        "authors": "Jiesong LianYucong HuangMingzhi WangChengdong MaYixue HaoYing WenYaodong Yang",
        "links": "http://arxiv.org/abs/2405.21027v2",
        "entry_id": "http://arxiv.org/abs/2405.21027v2",
        "pdf_url": "http://arxiv.org/pdf/2405.21027v2",
        "summary": "A popular approach for solving zero-sum games is to maintain populations of\npolicies to approximate the Nash Equilibrium (NE). Previous studies have shown\nthat Policy Space Response Oracle (PSRO) algorithm is an effective multi-agent\nreinforcement learning framework for solving such games. However, repeatedly\ntraining new policies from scratch to approximate Best Response (BR) to\nopponents' mixed policies at each iteration is both inefficient and costly.\nWhile some PSRO variants initialize a new policy by inheriting from past BR\npolicies, this approach limits the exploration of new policies, especially\nagainst challenging opponents. To address this issue, we propose Fusion-PSRO,\nwhich employs policy fusion to initialize policies for better approximation to\nBR. By selecting high-quality base policies from meta-NE, policy fusion fuses\nthe base policies into a new policy through model averaging. This approach\nallows the initialized policies to incorporate multiple expert policies, making\nit easier to handle difficult opponents compared to inheriting from past BR\npolicies or initializing from scratch. Moreover, our method only modifies the\npolicy initialization phase, allowing its application to nearly all PSRO\nvariants without additional training overhead. Our experiments on\nnon-transitive matrix games, Leduc Poker, and the more complex Liars Dice\ndemonstrate that Fusion-PSRO enhances the performance of nearly all PSRO\nvariants, achieving lower exploitability.",
        "updated": "2024-06-03 08:43:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.21027v2"
    },
    {
        "title": "Congestion-Aware Path Re-routing Strategy for Dense Urban Airspace",
        "authors": "Sajid Ahamed M APrathyush P MenonDebasish Ghose",
        "links": "http://arxiv.org/abs/2405.20972v1",
        "entry_id": "http://arxiv.org/abs/2405.20972v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20972v1",
        "summary": "Existing UAS Traffic Management (UTM) frameworks designate preplanned flight\npaths to uncrewed aircraft systems (UAS), enabling the UAS to deliver payloads.\nHowever, with increasing delivery demand between the source-destination pairs\nin the urban airspace, UAS will likely experience considerable congestion on\nthe nominal paths. We propose a rule-based congestion mitigation strategy that\nimproves UAS safety and airspace utilization in congested traffic streams. The\nstrategy relies on nominal path information from the UTM and positional\ninformation of other UAS in the vicinity. Following the strategy, UAS opts for\nalternative local paths in the unoccupied airspace surrounding the nominal path\nand avoids congested regions. The strategy results in UAS traffic exploring and\nspreading to alternative adjacent routes on encountering congestion. The paper\npresents queuing models to estimate the expected traffic spread for varying\nstochastic delivery demand at the source, thus helping to reserve the airspace\naround the nominal path beforehand to accommodate any foreseen congestion.\nSimulations are presented to validate the queuing results in the presence of\nstatic obstacles and intersecting UAS streams.",
        "updated": "2024-05-31 16:20:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20972v1"
    },
    {
        "title": "Paying to Do Better: Games with Payments between Learning Agents",
        "authors": "Yoav KolumbusJoe HalpernÉva Tardos",
        "links": "http://arxiv.org/abs/2405.20880v1",
        "entry_id": "http://arxiv.org/abs/2405.20880v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20880v1",
        "summary": "In repeated games, such as auctions, players typically use learning\nalgorithms to choose their actions. The use of such autonomous learning agents\nhas become widespread on online platforms. In this paper, we explore the impact\nof players incorporating monetary transfers into their agents' algorithms,\naiming to incentivize behavior in their favor. Our focus is on understanding\nwhen players have incentives to make use of monetary transfers, how these\npayments affect learning dynamics, and what the implications are for welfare\nand its distribution among the players. We propose a simple game-theoretic\nmodel to capture such scenarios. Our results on general games show that in a\nbroad class of games, players benefit from letting their learning agents make\npayments to other learners during the game dynamics, and that in many cases,\nthis kind of behavior improves welfare for all players. Our results on first-\nand second-price auctions show that in equilibria of the ``payment policy\ngame,'' the agents' dynamics can reach strong collusive outcomes with low\nrevenue for the auctioneer. These results highlight a challenge for mechanism\ndesign in systems where automated learning agents can benefit from interacting\nwith their peers outside the boundaries of the mechanism.",
        "updated": "2024-05-31 14:55:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20880v1"
    },
    {
        "title": "Optimally Improving Cooperative Learning in a Social Setting",
        "authors": "Shahrzad HaddadanCheng XinJie Gao",
        "links": "http://arxiv.org/abs/2405.20808v1",
        "entry_id": "http://arxiv.org/abs/2405.20808v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20808v1",
        "summary": "We consider a cooperative learning scenario where a collection of networked\nagents with individually owned classifiers dynamically update their\npredictions, for the same classification task, through communication or\nobservations of each other's predictions. Clearly if highly influential\nvertices use erroneous classifiers, there will be a negative effect on the\naccuracy of all the agents in the network. We ask the following question: how\ncan we optimally fix the prediction of a few classifiers so as maximize the\noverall accuracy in the entire network. To this end we consider an aggregate\nand an egalitarian objective function. We show a polynomial time algorithm for\noptimizing the aggregate objective function, and show that optimizing the\negalitarian objective function is NP-hard. Furthermore, we develop\napproximation algorithms for the egalitarian improvement. The performance of\nall of our algorithms are guaranteed by mathematical analysis and backed by\nexperiments on synthetic and real data.",
        "updated": "2024-05-31 14:07:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20808v1"
    },
    {
        "title": "No-Regret Learning for Fair Multi-Agent Social Welfare Optimization",
        "authors": "Mengxiao ZhangRamiro Deo-Campo VuongHaipeng Luo",
        "links": "http://arxiv.org/abs/2405.20678v1",
        "entry_id": "http://arxiv.org/abs/2405.20678v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20678v1",
        "summary": "We consider the problem of online multi-agent Nash social welfare (NSW)\nmaximization. While previous works of Hossain et al. [2021], Jones et al.\n[2023] study similar problems in stochastic multi-agent multi-armed bandits and\nshow that $\\sqrt{T}$-regret is possible after $T$ rounds, their fairness\nmeasure is the product of all agents' rewards, instead of their NSW (that is,\ntheir geometric mean). Given the fundamental role of NSW in the fairness\nliterature, it is more than natural to ask whether no-regret fair learning with\nNSW as the objective is possible. In this work, we provide a complete answer to\nthis question in various settings. Specifically, in stochastic $N$-agent\n$K$-armed bandits, we develop an algorithm with\n$\\widetilde{\\mathcal{O}}\\left(K^{\\frac{2}{N}}T^{\\frac{N-1}{N}}\\right)$ regret\nand prove that the dependence on $T$ is tight, making it a sharp contrast to\nthe $\\sqrt{T}$-regret bounds of Hossain et al. [2021], Jones et al. [2023]. We\nthen consider a more challenging version of the problem with adversarial\nrewards. Somewhat surprisingly, despite NSW being a concave function, we prove\nthat no algorithm can achieve sublinear regret. To circumvent such negative\nresults, we further consider a setting with full-information feedback and\ndesign two algorithms with $\\sqrt{T}$-regret: the first one has no dependence\non $N$ at all and is applicable to not just NSW but a broad class of welfare\nfunctions, while the second one has better dependence on $K$ and is preferable\nwhen $N$ is small. Finally, we also show that logarithmic regret is possible\nwhenever there exists one agent who is indifferent about different arms.",
        "updated": "2024-05-31 08:21:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20678v1"
    }
]