[
    {
        "title": "Code Pretraining Improves Entity Tracking Abilities of Language Models",
        "authors": "Najoung KimSebastian SchusterShubham Toshniwal",
        "links": "http://arxiv.org/abs/2405.21068v1",
        "entry_id": "http://arxiv.org/abs/2405.21068v1",
        "pdf_url": "http://arxiv.org/pdf/2405.21068v1",
        "summary": "Recent work has provided indirect evidence that pretraining language models\non code improves the ability of models to track state changes of discourse\nentities expressed in natural language. In this work, we systematically test\nthis claim by comparing pairs of language models on their entity tracking\nperformance. Critically, the pairs consist of base models and models trained on\ntop of these base models with additional code data. We extend this analysis to\nadditionally examine the effect of math training, another highly structured\ndata type, and alignment tuning, an important step for enhancing the usability\nof models. We find clear evidence that models additionally trained on large\namounts of code outperform the base models. On the other hand, we find no\nconsistent benefit of additional math training or alignment tuning across\nvarious model families.",
        "updated": "2024-05-31 17:56:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.21068v1"
    },
    {
        "title": "Recurrent neural networks: vanishing and exploding gradients are not the end of the story",
        "authors": "Nicolas ZucchetAntonio Orvieto",
        "links": "http://arxiv.org/abs/2405.21064v1",
        "entry_id": "http://arxiv.org/abs/2405.21064v1",
        "pdf_url": "http://arxiv.org/pdf/2405.21064v1",
        "summary": "Recurrent neural networks (RNNs) notoriously struggle to learn long-term\nmemories, primarily due to vanishing and exploding gradients. The recent\nsuccess of state-space models (SSMs), a subclass of RNNs, to overcome such\ndifficulties challenges our theoretical understanding. In this paper, we delve\ninto the optimization challenges of RNNs and discover that, as the memory of a\nnetwork increases, changes in its parameters result in increasingly large\noutput variations, making gradient-based learning highly sensitive, even\nwithout exploding gradients. Our analysis further reveals the importance of the\nelement-wise recurrence design pattern combined with careful parametrizations\nin mitigating this effect. This feature is present in SSMs, as well as in other\narchitectures, such as LSTMs. Overall, our insights provide a new explanation\nfor some of the difficulties in gradient-based learning of RNNs and why some\narchitectures perform better than others.",
        "updated": "2024-05-31 17:53:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.21064v1"
    },
    {
        "title": "Neural Network Verification with Branch-and-Bound for General Nonlinearities",
        "authors": "Zhouxing ShiQirui JinZico KolterSuman JanaCho-Jui HsiehHuan Zhang",
        "links": "http://arxiv.org/abs/2405.21063v1",
        "entry_id": "http://arxiv.org/abs/2405.21063v1",
        "pdf_url": "http://arxiv.org/pdf/2405.21063v1",
        "summary": "Branch-and-bound (BaB) is among the most effective methods for neural network\n(NN) verification. However, existing works on BaB have mostly focused on NNs\nwith piecewise linear activations, especially ReLU networks. In this paper, we\ndevelop a general framework, named GenBaB, to conduct BaB for general\nnonlinearities in general computational graphs based on linear bound\npropagation. To decide which neuron to branch, we design a new branching\nheuristic which leverages linear bounds as shortcuts to efficiently estimate\nthe potential improvement after branching. To decide nontrivial branching\npoints for general nonlinear functions, we propose to optimize branching points\noffline, which can be efficiently leveraged during verification with a lookup\ntable. We demonstrate the effectiveness of our GenBaB on verifying a wide range\nof NNs, including networks with activation functions such as Sigmoid, Tanh,\nSine and GeLU, as well as networks involving multi-dimensional nonlinear\noperations such as multiplications in LSTMs and Vision Transformers. Our\nframework also allows the verification of general nonlinear computation graphs\nand enables verification applications beyond simple neural networks,\nparticularly for AC Optimal Power Flow (ACOPF). GenBaB is part of the latest\n$\\alpha,\\!\\beta$-CROWN, the winner of the 4th International Verification of\nNeural Networks Competition (VNN-COMP 2023).",
        "updated": "2024-05-31 17:51:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.21063v1"
    },
    {
        "title": "An Organic Weed Control Prototype using Directed Energy and Deep Learning",
        "authors": "Deng CaoHongbo ZhangRajveer Dhillon",
        "links": "http://arxiv.org/abs/2405.21056v1",
        "entry_id": "http://arxiv.org/abs/2405.21056v1",
        "pdf_url": "http://arxiv.org/pdf/2405.21056v1",
        "summary": "Organic weed control is a vital to improve crop yield with a sustainable\napproach. In this work, a directed energy weed control robot prototype\nspecifically designed for organic farms is proposed. The robot uses a novel\ndistributed array robot (DAR) unit for weed treatment. Soybean and corn\ndatabases are built to train deep learning neural nets to perform weed\nrecognition. The initial deep learning neural nets show a high performance in\nclassifying crops. The robot uses a patented directed energy plant eradication\nrecipe that is completely organic and UV-C free, with no chemical damage or\nphysical disturbance to the soil. The deep learning can classify 8 common weed\nspecies in a soybean field under natural environment with up to 98% accuracy.",
        "updated": "2024-05-31 17:47:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.21056v1"
    },
    {
        "title": "Grammar-Aligned Decoding",
        "authors": "Kanghee ParkJiayu WangTaylor Berg-KirkpatrickNadia PolikarpovaLoris D'Antoni",
        "links": "http://arxiv.org/abs/2405.21047v1",
        "entry_id": "http://arxiv.org/abs/2405.21047v1",
        "pdf_url": "http://arxiv.org/pdf/2405.21047v1",
        "summary": "Large Language Models (LLMs) struggle with reliably generating highly\nstructured outputs, such as program code, mathematical formulas, or well-formed\nmarkup. Constrained decoding approaches mitigate this problem by greedily\nrestricting what tokens an LLM can output at each step to guarantee that the\noutput matches a given constraint. Specifically, in grammar-constrained\ndecoding (GCD), the LLM's output must follow a given grammar. In this paper we\ndemonstrate that GCD techniques (and in general constrained decoding\ntechniques) can distort the LLM's distribution, leading to outputs that are\ngrammatical but appear with likelihoods that are not proportional to the ones\ngiven by the LLM, and so ultimately are low-quality. We call the problem of\naligning sampling with a grammar constraint, grammar-aligned decoding (GAD),\nand propose adaptive sampling with approximate expected futures (ASAp), a\ndecoding algorithm that guarantees the output to be grammatical while provably\nproducing outputs that match the conditional probability of the LLM's\ndistribution conditioned on the given grammar constraint. Our algorithm uses\nprior sample outputs to soundly overapproximate the future grammaticality of\ndifferent output prefixes. Our evaluation on code generation and structured NLP\ntasks shows how ASAp often produces outputs with higher likelihood (according\nto the LLM's distribution) than existing GCD techniques, while still enforcing\nthe desired grammatical constraints.",
        "updated": "2024-05-31 17:39:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.21047v1"
    }
]