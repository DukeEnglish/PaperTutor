[
    {
        "title": "Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis",
        "authors": "Chaoyou FuYuhan DaiYondong LuoLei LiShuhuai RenRenrui ZhangZihan WangChenyu ZhouYunhang ShenMengdan ZhangPeixian ChenYanwei LiShaohui LinSirui ZhaoKe LiTong XuXiawu ZhengEnhong ChenRongrong JiXing Sun",
        "links": "http://arxiv.org/abs/2405.21075v1",
        "entry_id": "http://arxiv.org/abs/2405.21075v1",
        "pdf_url": "http://arxiv.org/pdf/2405.21075v1",
        "summary": "In the quest for artificial general intelligence, Multi-modal Large Language\nModels (MLLMs) have emerged as a focal point in recent advancements. However,\nthe predominant focus remains on developing their capabilities in static image\nunderstanding. The potential of MLLMs in processing sequential visual data is\nstill insufficiently explored, highlighting the absence of a comprehensive,\nhigh-quality assessment of their performance. In this paper, we introduce\nVideo-MME, the first-ever full-spectrum, Multi-Modal Evaluation benchmark of\nMLLMs in Video analysis. Our work distinguishes from existing benchmarks\nthrough four key features: 1) Diversity in video types, spanning 6 primary\nvisual domains with 30 subfields to ensure broad scenario generalizability; 2)\nDuration in temporal dimension, encompassing both short-, medium-, and\nlong-term videos, ranging from 11 seconds to 1 hour, for robust contextual\ndynamics; 3) Breadth in data modalities, integrating multi-modal inputs besides\nvideo frames, including subtitles and audios, to unveil the all-round\ncapabilities of MLLMs; 4) Quality in annotations, utilizing rigorous manual\nlabeling by expert annotators to facilitate precise and reliable model\nassessment. 900 videos with a total of 256 hours are manually selected and\nannotated by repeatedly viewing all the video content, resulting in 2,700\nquestion-answer pairs. With Video-MME, we extensively evaluate various\nstate-of-the-art MLLMs, including GPT-4 series and Gemini 1.5 Pro, as well as\nopen-source image models like InternVL-Chat-V1.5 and video models like\nLLaVA-NeXT-Video. Our experiments reveal that Gemini 1.5 Pro is the\nbest-performing commercial model, significantly outperforming the open-source\nmodels. Our dataset along with these findings underscores the need for further\nimprovements in handling longer sequences and multi-modal data. Project Page:\nhttps://video-mme.github.io",
        "updated": "2024-05-31 17:59:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.21075v1"
    },
    {
        "title": "Latent Intrinsics Emerge from Training to Relight",
        "authors": "Xiao ZhangWilliam GaoSeemandhar JainMichael MaireDavid. A. ForsythAnand Bhattad",
        "links": "http://arxiv.org/abs/2405.21074v1",
        "entry_id": "http://arxiv.org/abs/2405.21074v1",
        "pdf_url": "http://arxiv.org/pdf/2405.21074v1",
        "summary": "Image relighting is the task of showing what a scene from a source image\nwould look like if illuminated differently. Inverse graphics schemes recover an\nexplicit representation of geometry and a set of chosen intrinsics, then\nrelight with some form of renderer. However error control for inverse graphics\nis difficult, and inverse graphics methods can represent only the effects of\nthe chosen intrinsics. This paper describes a relighting method that is\nentirely data-driven, where intrinsics and lighting are each represented as\nlatent variables. Our approach produces SOTA relightings of real scenes, as\nmeasured by standard metrics. We show that albedo can be recovered from our\nlatent intrinsics without using any example albedos, and that the albedos\nrecovered are competitive with SOTA methods.",
        "updated": "2024-05-31 17:59:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.21074v1"
    },
    {
        "title": "Generalization Beyond Data Imbalance: A Controlled Study on CLIP for Transferable Insights",
        "authors": "Xin WenBingchen ZhaoYilun ChenJiangmiao PangXiaojuan Qi",
        "links": "http://arxiv.org/abs/2405.21070v1",
        "entry_id": "http://arxiv.org/abs/2405.21070v1",
        "pdf_url": "http://arxiv.org/pdf/2405.21070v1",
        "summary": "Severe data imbalance naturally exists among web-scale vision-language\ndatasets. Despite this, we find CLIP pre-trained thereupon exhibits notable\nrobustness to the data imbalance compared to supervised learning, and\ndemonstrates significant effectiveness in learning generalizable\nrepresentations. With an aim to investigate the reasons behind this finding, we\nconduct controlled experiments to study various underlying factors, and reveal\nthat CLIP's pretext task forms a dynamic classification problem wherein only a\nsubset of classes is present in training. This isolates the bias from dominant\nclasses and implicitly balances the learning signal. Furthermore, the\nrobustness and discriminability of CLIP improve with more descriptive language\nsupervision, larger data scale, and broader open-world concepts, which are\ninaccessible to supervised learning. Our study not only uncovers the mechanisms\nbehind CLIP's generalizability beyond data imbalance but also provides\ntransferable insights for the research community. The findings are validated in\nboth supervised and self-supervised learning, enabling models trained on\nimbalanced data to achieve CLIP-level performance on diverse recognition tasks.\nCode will be available at: https://github.com/CVMI-Lab/clip-beyond-tail.",
        "updated": "2024-05-31 17:57:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.21070v1"
    },
    {
        "title": "Mixed Diffusion for 3D Indoor Scene Synthesis",
        "authors": "Siyi HuDiego Martin ArroyoStephanie DebatsFabian ManhardtLuca CarloneFederico Tombari",
        "links": "http://arxiv.org/abs/2405.21066v1",
        "entry_id": "http://arxiv.org/abs/2405.21066v1",
        "pdf_url": "http://arxiv.org/pdf/2405.21066v1",
        "summary": "Realistic conditional 3D scene synthesis significantly enhances and\naccelerates the creation of virtual environments, which can also provide\nextensive training data for computer vision and robotics research among other\napplications. Diffusion models have shown great performance in related\napplications, e.g., making precise arrangements of unordered sets. However,\nthese models have not been fully explored in floor-conditioned scene synthesis\nproblems. We present MiDiffusion, a novel mixed discrete-continuous diffusion\nmodel architecture, designed to synthesize plausible 3D indoor scenes from\ngiven room types, floor plans, and potentially pre-existing objects. We\nrepresent a scene layout by a 2D floor plan and a set of objects, each defined\nby its category, location, size, and orientation. Our approach uniquely\nimplements structured corruption across the mixed discrete semantic and\ncontinuous geometric domains, resulting in a better conditioned problem for the\nreverse denoising step. We evaluate our approach on the 3D-FRONT dataset. Our\nexperimental results demonstrate that MiDiffusion substantially outperforms\nstate-of-the-art autoregressive and diffusion models in floor-conditioned 3D\nscene synthesis. In addition, our models can handle partial object constraints\nvia a corruption-and-masking strategy without task specific training. We show\nMiDiffusion maintains clear advantages over existing approaches in scene\ncompletion and furniture arrangement experiments.",
        "updated": "2024-05-31 17:54:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.21066v1"
    },
    {
        "title": "Unified Directly Denoising for Both Variance Preserving and Variance Exploding Diffusion Models",
        "authors": "Jingjing WangDan ZhangFeng Luo",
        "links": "http://arxiv.org/abs/2405.21059v1",
        "entry_id": "http://arxiv.org/abs/2405.21059v1",
        "pdf_url": "http://arxiv.org/pdf/2405.21059v1",
        "summary": "Previous work has demonstrated that, in the Variance Preserving (VP)\nscenario, the nascent Directly Denoising Diffusion Models (DDDM) can generate\nhigh-quality images in one step while achieving even better performance in\nmultistep sampling. However, the Pseudo-LPIPS loss used in DDDM leads to\nconcerns about the bias in assessment. Here, we propose a unified DDDM (uDDDM)\nframework that generates images in one-step/multiple steps for both Variance\nPreserving (VP) and Variance Exploding (VE) cases. We provide theoretical\nproofs of the existence and uniqueness of the model's solution paths, as well\nas the non-intersecting property of the sampling paths. Additionally, we\npropose an adaptive Pseudo-Huber loss function to balance the convergence to\nthe true solution and the stability of convergence process.Through a\ncomprehensive evaluation, we demonstrate that uDDDMs achieve FID scores\ncomparable to the best-performing methods available for CIFAR-10 in both VP and\nVE. Specifically, uDDDM achieves one-step generation on CIFAR10 with FID of\n2.63 and 2.53 for VE and VP respectively. By extending the sampling to 1000\nsteps, we further reduce FID score to 1.71 and 1.65 for VE and VP respectively,\nsetting state-of-the-art performance in both cases.",
        "updated": "2024-05-31 17:49:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.21059v1"
    }
]