Transformers are SSMs: Generalized Models and Efficient Algorithms
Through Structured State Space Duality
TriDao∗1 andAlbertGu∗2
1DepartmentofComputerScience,PrincetonUniversity
2MachineLearningDepartment,CarnegieMellonUniversity
tri@tridao.me,agu@cs.cmu.edu
Abstract
WhileTransformershavebeenthemainarchitecturebehinddeeplearning’ssuccessinlanguagemodeling,state-space
models(SSMs)suchasMambahaverecentlybeenshowntomatchoroutperformTransformersatsmalltomediumscale.
We show that these families of models are actually quite closely related, and develop a rich framework of theoretical
connectionsbetweenSSMsandvariantsofattention,connectedthroughvariousdecompositionsofawell-studiedclass
ofstructuredsemiseparablematrices. Ourstatespaceduality(SSD)frameworkallowsustodesignanewarchitecture
(Mamba-2)whosecorelayerisanarefinementofMamba’sselectiveSSMthatis2-8×faster, whilecontinuingtobe
competitivewithTransformersonlanguagemodeling.
1 Introduction
Transformers,inparticulardecoder-onlymodels(e.g.GPT(Brownetal.2020),Llama(Touvron,Lavril,etal.2023))which
process input sequences in a causal fashion, are one of the main drivers of modern deep learning’s success. Numer-
ousapproachesattempttoapproximatethecoreattentionlayertoaddressitsefficiencyissues(Tayetal.2022),suchas
scaling quadratically in sequence length during training and requiring a cache of size linear in sequence length during
autoregressivegeneration.Inparallel,aclassofalternativesequencemodels,structuredstate-spacemodels(SSMs),have
emerged with linear scaling in sequence length during training and constant state size during generation. They show
strongperformanceonlong-rangetasks(e.g. S4(Gu,Goel,andRé2022))andrecentlymatchedorbeatTransformerson
languagemodeling(e.g.Mamba(GuandDao2023))atsmalltomoderatescale.However,thedevelopmentofSSMshave
appeareddisjointfromthecommunity’scollectiveefforttoimproveTransformers,suchasunderstandingthemtheoreti-
callyaswellasoptimizingthemonmodernhardware.Asaresult,itismoredifficulttounderstandandexperimentwith
SSMs compared to Transformers, and it remains challenging to train SSMs as efficiently as Transformers from both an
algorithmicandsystemsperspective.
OurmaingoalistodeveloparichbodyoftheoreticalconnectionsbetweenstructuredSSMsandvariantsofattention.This
willallowustotransferalgorithmicandsystemsoptimizationsoriginallydevelopedforTransformerstoSSMs,towards
thegoalofbuildingfoundationmodelsthatperformbetterthanTransformerswhilescalingmoreefficientlyinsequence
length.AmilestonecontributioninthisdirectionwastheLinearAttention(LA)framework(Katharopoulosetal.2020),
whichderivedaconnectionbetweenautoregressiveattentionandlinearRNNsbyshowingtheequivalencebetween“dual
forms”ofquadratickernelizedattentionandaparticularlinearrecurrence. Thisdualityallowsnewcapabilitiessuchas
the ability to have both efficient parallelizable training and efficient autoregressive inference. In the same spirit, this
paperprovidesmultipleviewpointsconnectinglinear-complexitySSMswithquadratic-complexityformstocombinethe
strengthsofSSMsandattention.1
∗Alphabeticalbylastname.
1Technicallyspeaking,theseconnectionsonlyrelatetocertainflavorsofattention;thetitleofthispaperisanhomagetoKatharopoulosetal.(2020)
whichfirstshowedthat“TransformersareRNNs”.
1
4202
yaM
13
]GL.sc[
1v06012.5042:viXraState Space Duality. Our framework connecting struc-
Efficient
tured SSMs and variants of attention, which we call struc-
Algorithms
tured state space duality (SSD), is made through the
abstractions of structured matrices: matrices with sub-
quadraticparametersandmultiplicationcomplexity. Wede- Sec. 6
veloptwobroadframeworksforrepresentingsequencemod-
els,oneasmatrixtransformationsandoneastensorcontrac- Structured
tions,whicheachrevealdifferentperspectivesoftheduality. Matrices
Ourtechnicalcontributionsinclude:
Semiseparable Structured Masked
• Weshowanequivalencebetweenstatespacemodelsanda
Matrices Attention (SMA)
well-studiedfamilyofstructuredmatricescalledsemisep- Sec. 3 Sec. 4
arable matrices (Section 3). This connection is at the
heart our framework, revealing new properties and algo-
State Space Sec. 5
rithms for SSMs. A central message of this paper is that Attention
Models (SSM)
differentmethodsofcomputingstatespacemodelscanbere-
State Space
framedasvariousmatrixmultiplicationalgorithmsonstruc-
turedmatrices. Duality (SSD)
• We significantly improve the theory of linear atten-
Sec. 7
tion (Katharopoulos et al. 2020). We first provide an in-
cisiveproofofitsrecurrentformthroughthelanguageof
Mamba-2
tensorcontractions,andthengeneralizeittoanewfamily
of structuredmaskedattention(SMA)(Section4). Figure 1: (Structured State-Space Duality.) This paper
fleshes out the relationship between state space models
• WeconnectSSMsandSMA,showingthattheyhavealarge
andattentionthroughthebridgeofstructuredmatrices.
intersection that are duals of each other, possessing both
SSM-like linear and attention-like quadratic forms (Sec-
tion 5). We also prove that any kernel attention method
possessingafastrecurrentformmustbeanSSM.
Beyonditsintrinsictheoreticalvalue,ourframeworkopensupabroadsetofdirectionsforunderstandingandimproving
sequencemodels.
EfficientAlgorithms. Firstandmostimportantly,ourframeworkexposesnewefficientandeasily-implementablealgo-
rithmsforcomputingSSMs(Section6).WeintroduceanewSSDalgorithm,basedonblockdecompositionsofsemisepa-
rablematrices,thattakesadvantageofboththelinearSSMrecurrenceandquadraticdualform,obtainingoptimaltradeoffs
onallmainefficiencyaxes(e.g. trainingandinferencecompute,memoryusage,andabilitytoleveragematrixmultipli-
cationunitsonmodernhardware).AdedicatedimplementationofSSDis2−8×fasterthantheoptimizedselectivescan
implementationofMamba,whilesimultaneouslyallowingformuchlargerrecurrentstatesizes(8×thesizeofMambaor
evenhigher,withminimalslowdown). SSDishighlycompetitivewithoptimizedimplementationsofsoftmaxattention
(FlashAttention-2(Dao2024)),crossingoveratsequencelength2Kand6×fasteratsequencelength16K.
Architecture Design. One major obstacle to adopting new architectures such as SSMs is the ecosystem tailored to
Transformers,suchashardware-efficientoptimizationandparallelismtechniquesforlarge-scaletraining.Ourframework
allowsusingestablishedconventionsandtechniquesforattentiontobuildavocabularyofarchitecturedesignchoicesfor
SSMs, andfurtherimprovethem(Section7). Forexample, weintroducetheanalogofheadsfrommulti-headattention
(MHA)toSSMs. WeshowthattheMambaarchitectureisamulti-inputSSM(MIS)thatturnsouttobeanalogousto
multi-valueattention(MVA),andcompareothervariantsofMambawithdifferentheadstructures.
WealsousetheseideastomakeslightmodificationstotheMambablock, whichallowstensorparallelismtobeimple-
mented(e.g. inthestyleofMegatron(Shoeybietal.2019)). Themainideasincludeintroducinggrouped-valueattention
(GVA)headstructure,andmovingalldata-dependentprojectionstooccurinparallelatthebeginningoftheblock.
The combination of the modified parallel Mamba block, together with using SSD as the inner SSM layer, results in the
Mamba-2architecture. WeinvestigateChinchillascalinglawsforMamba-2inthesamesettingasMamba,findingthat
itParetodominatesMambaandTransformer++inbothperplexityandwall-clocktime.Weadditionallytrainafamilyof
2Mamba-2 models at varying sizes on the Pile, showing that it matches or outperforms Mamba and open source Trans-
formers on standard downstream evaluations. For example, Mamba-2 with 2.7B parameters trained on 300B tokens on
thePileoutperformsMamba-2.8B,Pythia-2.8BandevenPythia-6.9Btrainedonthesamedataset.
SystemsOptimizations. TheSSDframeworkconnectsSSMsandTransformers,allowingustoleveragearichbodyof
workonsystemsoptimizationsdevelopedforTransformers(Section8).
• For example, Tensor Parallelism (TP) is an important model parallelism technique to train large Transformer models
bysplittingeachlayeracrossGPUsonthesamenode. WedesignMamba-2tobeTP-friendly,reducingthenumberof
synchronizationpointperblockbyhalf.
• Forverylongsequenceswhoseactivationsdonotfitononedevice, sequenceparallelismhasbeendevelopedforthe
attentionblocks. WedescribehowtotrainSSMsingeneralandMamba-2inparticularwithsequenceparallelism,by
passingtherecurrentstatesbetweendevices.
• Forfinetuningwithexamplesofdifferentlengths,forbestefficiency,Transformerrequiressophisticatedtechniquesto
remove padding tokens and perform attention on variable length sequences. We show how Mamba-2 can be trained
withvariablesequencelengthsefficiently,requiringnopaddingtokens.
Section9empiricallyvalidatesMamba-2onlanguagemodeling,trainingefficiency,andadifficultmulti-queryassociative
recalltask(Arora,Eyuboglu,Zhang,etal.2024). Finally,inSection10,weprovideanextendedrelatedworkanddiscuss
potentialresearchdirectionsopenedupbyourframework.
Modelcodeandpre-trainedcheckpointsareopen-sourcedathttps://github.com/state-spaces/mamba.
2 Background and Overview
2.1 StructuredStateSpaceModels
Structuredstatespacesequencemodels(S4)arearecentclassofsequencemodelsfordeeplearningthatarebroadlyrelated
to RNNs, CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a
1-dimensionalsequence𝑥 ∈RT ↦→𝑦 ∈RTthroughanimplicitlatentstateℎ ∈R(T,N).
AgeneraldiscreteformofstructuredSSMstakestheformofequation(1).
ℎ
𝑡
=𝐴ℎ 𝑡−1+𝐵𝑥
𝑡
(1a) ℎ
𝑡
=𝐴 𝑡ℎ 𝑡−1+𝐵 𝑡𝑥
𝑡
(2a)
𝑦
𝑡
=𝐶⊤ℎ
𝑡
(1b) 𝑦
𝑡
=𝐶 𝑡⊤ℎ
𝑡
(2b)
where𝐴 ∈ R(N,N),𝐵 ∈ R(N,1),𝐶 ∈ R(N,1). Structured SSMs are so named because the𝐴 matrix controlling the temporal
dynamicsmustbestructuredinordertocomputethissequence-to-sequencetransformationefficientlyenoughtobeused
indeepneuralnetworks.Theoriginalstructuresintroducedwerediagonalpluslow-rank(DPLR)(Gu,Goel,andRé2022)
anddiagonal(Gu,Gupta,etal.2022; Gupta,Gu,andBerant2022; J.T.Smith,Warrington,andLinderman2023),which
remainsthemostpopularstructure.
In this work, we use the term state space model (SSM) to refer to structured SSMs. There are many flavors of such
SSMs, with deep ties to several major paradigms of neural sequence models such as continuous-time, recurrent, and
convolutionalmodels(Gu, Johnson, Goel, etal.2021). Weprovideabriefoverviewbelow, andrefertopriorworkfor
morecontextanddetails(Gu2023;GuandDao2023).
Continuous-timeModels. TheoriginalstructuredSSMsoriginatedascontinuous-timemapsonfunctions𝑥(𝑡) ∈R↦→
𝑦(𝑡) ∈ R,ratherthanoperatingdirectlyonsequences. Inthecontinuous-timeperspective,inequation(1a)thematrices
(𝐴,𝐵) arenotdirectlylearnedbutgeneratedfromunderlyingparameters (𝐴˚,𝐵˚ ),alongwithaparameterizedstepsizeΔ.
The“continuousparameters”(Δ,𝐴˚,𝐵˚ )areconvertedto“discreteparameters”(𝐴,𝐵)throughfixedformulas𝐴= 𝑓 𝐴(Δ,𝐴˚ )
and𝐵 = 𝑓 𝐵(Δ,𝐵˚ ),wherethepair(𝑓 𝐴,𝑓 𝐵)iscalledadiscretizationrule.
Remark1. Whileourmainmodelsadoptthesameparameterizationanddiscretizationstepaspriorwork(seeGuandDao
(2023) for details), for simplifying exposition and notation we omit it in the rest of this paper. We note that prior work on
3structuredSSMsreferredtothecontinuousparameters (𝐴˚,𝐵˚ ) anddiscreteparameters (𝐴,𝐵) as (𝐴,𝐵) and (𝐴¯,𝐵¯ ) instead;we
havechangednotationtosimplifythepresentationandfocusdirectlyonthediscreteparameters,whichgovernthemainSSM
recurrence.
RecurrentModels. Equations(1)and(2)taketheformofarecurrencewhichislinearinitsinput𝑥. StructuredSSMs
canthereforebeviewedastypesofrecurrentneuralnetworks(RNNs),wherethelinearityendowsthemwithadditional
properties and allows them to avoid the sequential computation of traditional RNNs. Conversely, despite this simplifi-
cation,SSMsarestillfullyexpressiveassequencetransformations(inthesenseofuniversalapproximation)(Kaul2020;
Orvietoetal.2023;ShidaWangandXue2023).
ConvolutionalModels. WhentheSSM’sdynamicsareconstantthroughtimeasinequation(1), themodeliscalled
lineartime-invariant(LTI).Inthiscase,theyareequivalenttoconvolutions. Thus,SSMscanalsobeviewedastypes
ofCNNs,butwhere(i)theconvolutionkernelsareimplicitlyparameterizedthroughtheSSMparameters(𝐴,𝐵,𝐶)and(ii)
theconvolutionkernelsaregenerallyglobalinsteadoflocal. Conversely, throughclassicalsignalprocessingtheoryall
sufficientlywell-behavedconvolutionscanberepresentedasSSMs.
Commonly, previousLTISSMswouldusetheconvolutionalmodeforefficientparallelizabletraining(wherethewhole
inputsequenceisseenaheadoftime),andswitchedintorecurrentmode(1)forefficientautoregressiveinference(where
theinputsareseenonestepatatime).
SelectiveStateSpaceModels. Theform(2)wherethe parameters (𝐴,𝐵,𝐶) canalsovaryintime wasintroducedin
MambaastheselectiveSSM.ComparedtothestandardLTIformulation(1),thismodelcanselectivelychoosetofocus
onorignoreinputsateverytimestep. ItwasshowntoperformmuchbetterthanLTISSMsoninformation-densedata
such as language, especially as its state size N increases allowing for more information capacity. However, it can only
be computed in recurrent instead of convolutional mode, and requires a careful hardware-aware implementation to be
efficient. Evenso,itisstilllessefficientthanhardware-friendlymodelssuchasCNNsandTransformersbecauseitdoes
notleveragematrixmultiplicationunits,whichmodernacceleratorssuchasGPUsandTPUsarespecializedfor.
Whiletime-invariantSSMsarecloselyrelatedtocontinuous,recurrent,andconvolutionalsequencemodels,theyarenot
directlyrelatedtoattention. Inthispaper,weshowadeeperrelationshipbetweenselectiveSSMsandattention,anduse
ittosignificantlyimprovethetrainingspeedofSSMswhilesimultaneouslyallowingformuchlargerstatesizesN.
StructuredSSMsasSequenceTransformations.
Definition2.1. Weusethetermsequencetransformationtorefertoaparameterizedmaponsequences𝑌 = 𝑓 𝜃(𝑋)where
𝑋,𝑌 ∈ R(T,P) and𝜃 isanarbitrarycollectionofparameters. Trepresentsthesequenceor timeaxis;subscriptsindexintothe
firstdimension,e.g.𝑋 𝑡,𝑌 𝑡 ∈RP .
Sequence transformations (e.g. SSMs, or self-attention) are the cornerstone of deep sequence models, where they are
incorporated into neural network architectures (e.g. Transformers). The SSM in (1) or (2) is a sequence transformation
withP=1;itcanbegeneralizedtoP>1bysimplybroadcastingacrossthisdimension(inotherwords,viewingtheinput
as P independent sequences and applying the SSM to each). One can think of P as a head dimension, which we will
elaborateoninSection7.
Definition 2.2. We define the SSM operator SSM(𝐴,𝐵,𝐶) = SSM(𝐴 0:𝑇,𝐵 0:𝑇,𝐶 0:𝑇) as the sequence transformation𝑋 ∈
R(T,P) ↦→𝑌 ∈R(T,P) definedbyequation(2).
InSSMs,theNdimensionisafreeparametercalledthestatesizeorstatedimension.Wealsocallitthestateexpansion
factor,becauseitexpandsthesizeoftheinput/outputbyafactorof𝑁,withimplicationsforthecomputationalefficiency
ofthesemodels.
Finally,weremarkthatmanytypesofsequencetransformations,suchasattention,canberepresentedasasinglematrix
multiplicationacrossthesequencedimension.
Definition2.3. Wecallasequencetransformation𝑌 = 𝑓 𝜃(𝑋) amatrixtransformationifitcanbewrittenintheform
𝑌 =𝑀 𝜃𝑋 where𝑀 isamatrixdependingontheparameters𝜃. Weidentifythesequencetransformationwiththematrix𝑀,
andoftendropthedependenceon𝜃 whenclearfromcontext.
42.2 Attention
Attentionbroadlyreferstoatypeofcomputationthatassignsscorestoeverypairofpositionsinasequence, allowing
eachelementto“attend”totherest.Byfarthemostcommonandimportantvariantofattentionissoftmaxself-attention,
whichcanbedefinedas
𝑌 =softmax(𝑄𝐾⊤)·𝑉
for𝑄,𝐾,𝑉 ∈ R(T,P). Themechanismofpairwisecomparisons(inducedbymaterializing𝑄𝐾⊤)leadstothecharacteristic
quadratictrainingcostofattention.
Manyvariantsofattentionhavebeenproposed,butallsharetheunderlyingcoreoftheseattentionscores,withvarious
approximations(Tayetal.2022).Themostimportantvariantforthisworkislinearattention(Katharopoulosetal.2020).
Roughlyspeaking,thisfamilyofmethodsdropsthesoftmaxbyfoldingitintoakernelfeaturemap,andusesassociativity
ofmatrixmultiplicationtorewrite (𝑄𝐾⊤) ·𝑉 = 𝑄 · (𝐾⊤𝑉). Moreover, intheimportantcaseofcausal(autoregressive)
attention,theyshowthatwhenthecausalmaskisincorporatedintotheleft-handsideas (𝐿◦𝑄𝐾⊤) ·𝑉,where𝐿 isthe
lower-triangular1’smatrix,thentheright-handsidecanbeexpandedasarecurrence.Severalrecentandconcurrentworks
suchasRetNet(Y.Sunetal.2023)andGateLoop(Katsch2023)strengthenthistomoregeneralformsof𝐿(Section10).In
thiswork,ourformulationofstructuredmaskedattentionwillstronglygeneralizetheseideas.
2.3 StructuredMatrices
General matrices 𝑀 ∈ R(T,T) require T2 parameters to represent and 𝑂(T2) time to perform basic operations such as
matrix-vectormultiplication.Structuredmatricesarethosethat
(i) canberepresentedinsubquadratic(ideallylinear)parametersthroughacompressedrepresentation,and
(ii) havefastalgorithms(mostimportantlymatrixmultiplication)byoperatingdirectlyonthiscompressedrepresen-
tation.
Perhapsthemostcanonicalfamiliesofstructuredmatricesaresparseandlow-rankmatrices.However,thereexistmany
other families, such as Toeplitz, Cauchy, Vandermonde, and butterfly matrices, which have all been used in machine
learningforefficientmodels(Dao,Gu,etal.2019;D.Fuetal.2024;Gu,Gupta,etal.2022;Thomasetal.2018).Structured
matrices are a powerful abstraction for efficient representations and algorithms. In this work, we will show that SSMs
areequivalenttoanotherclassofstructuredmatricesthathavenotpreviouslybeenusedindeeplearning,andusethis
connectiontoderiveefficientmethodsandalgorithms.
2.4 Overview: StructuredStateSpaceDuality
WhilethispaperdevelopsamuchricherframeworkofconnectionsbetweenSSMs,attention,andstructuredmatrices,we
provideabriefsummaryofthemainmethod,whichisactuallyquiteself-containedandsimplealgorithmically.
Recurrent(Linear)Form. Thestatespacedual(SSD)layercanbedefinedasaspecialcaseoftheselectiveSSM(2).
The standard computation of an SSM as a recurrence (or parallel scan) can be applied, which has linear complexity in
sequencelength.ComparedtotheversionusedinMamba,SSDhastwominordifferences:
• Thestructureon𝐴isfurthersimplifiedfromdiagonaltoscalartimesidentitystructure.Each𝐴
𝑡
canalsobeidentified
withjustascalarinthiscase.
• WeusealargerheaddimensionP,comparedtoP = 1usedinMamba. TypicallyP = {64,128} ischosenwhichis
similartoconventionsformodernTransformers.
Compared to the original selective SSM, these changes can be viewed as slightly decreasing the expressive power in
return for significant training efficiency improvements. In particular, our new algorithms will allow the use of matrix
multiplicationunitsonmodernaccelerators.
5Dual(Quadratic)Form. ThedualformofSSDisaquadraticcomputationcloselyrelatedtoattention,definedas
(cid:40)
(𝐿◦𝑄𝐾⊤)·𝑉 𝐿 𝑖𝑗 = 𝑎 𝑖 ×···×𝑎 𝑗+1 𝑖 ≥ 𝑗
0 𝑖 < 𝑗
where𝑎
𝑖
areinput-dependentscalarsboundedin [0,1].
Comparedtostandardsoftmaxattention,therearetwomaindifferences
• Thesoftmaxisdropped.
• Theattentionmatrixismultipliedelementwise-wisebyanadditionalmaskmatrix𝐿.
Both of these changes can be viewed as addressing problems in vanilla attention. For example, the softmax has been
recently observed to cause problems in attention scores, such as the “attention sink” phenomenon (Darcet et al. 2024;
Xiaoetal.2024). Moreimportantly, themaskmatrix𝐿 canbeviewedasreplacingtheheuristicpositionalembeddings
ofTransformerswithadifferentdata-dependentpositionalmaskthatcontrolshowmuchinformationistransferedacross
time.
Morebroadly,thisformisaninstanceofourstructuredmaskedattentiongeneralizationoflinearattention,definedin
Section4.
MatrixFormandSSDAlgorithm. ThevariousformsofSSDareconnectedthroughaunifiedmatrixrepresentation,
byshowingthatSSMshaveamatrixtransformationform𝑌 =𝑀𝑋 foramatrix𝑀 𝜃 ∈R(T,T) thatdependson𝜃 = (𝐴,𝐵,𝐶).
Inparticular,thedualformofSSDisequivalenttonaive(quadratic-time)multiplicationbythematrix𝑀,andtherecurrent
formisaparticularefficient(linear-time)algorithmthatleveragesthestructurein𝑀.
Goingbeyondthese,any algorithmformultiplicationby𝑀 canbeapplied. Ourproposedhardware-efficientSSDalgo-
rithm(Section6)isanewstructuredmatrixmultiplicationmethodthatinvolvesblockdecompositionsof𝑀,whichobtains
better efficiency tradeoffs than either the pure linear or quadratic forms. It is relatively simple and easy-to-implement
comparedtogeneralselectiveSSMs(GuandDao2023); Listing1providesacompleteimplementationinafewlinesof
code.
Figure1providesasimpleroadmapoftherelationshipsbetweentheconceptspresentedinthispaper.
2.5 Notation
Throughoutthispaper,wepreferusingprecisenotationthatcanbemappedtocode.
MatricesandVectors. Wegenerallyuselowercasetodenotevectors(i.e.tensorswithasingleaxis)anduppercaseto
denotematrices(i.e.tensorswithmorethanoneaxes). Wedonotboldmatricesinthiswork. Sometimes,ifamatrixis
tiedorrepeatedalongoneaxis(andhencecanalsobeviewedasavector),wemayuseeitherupperorlowercaseforit.2
·denotesscalarormatrixmultiplicationwhile◦denotesHadamard(elementwise)multiplication.
Indexing. WeusePython-styleindexing,e.g.𝑖 : 𝑗 referstotherange(𝑖,𝑖+1,...,𝑗−1)when𝑖 < 𝑗 and(𝑖,𝑖−1,...,𝑗+1)
when𝑖 > 𝑗. For example, for any symbol𝑣 we let𝑣 𝑗:𝑖 for 𝑗 ≥ 𝑖 denote the sequence (𝑣 𝑗,...,𝑣 𝑖+1). [𝑖] is equivalent to
0:𝑖 = (0,...,𝑖−1).Forshorthand,wealsolet𝑣× 𝑗:𝑖 denotetheproduct𝑣 𝑗 ×···×𝑣 𝑖+1.3
Dimensions. Todistinguishfrommatricesandtensors,weoftenusecapitallettersintypewriterfonts(e.g. D,N,T) to
denote dimensions and tensor shapes. Instead of the traditional notation 𝑀 ∈ R𝑇×𝑇 we frequently use 𝑀 ∈ R(T,T) to
reflecttensorshapesincode.
TensorContractions. Wewillheavilyrelyontensorcontractionoreinsumnotationbothforclarityandasacentral
toolinstatingandprovingourresults. Weassumethereadertobefamiliarwiththisnotation,whichiscommonlyused
2Inthiswork,thishappensonlywiththe𝐴parameterofSSMs.
3Insomecontexts,itisalwaysclearthatthenotation𝑎𝑖:𝑗 or𝐴𝑖:𝑗 means𝑎 𝑖× :𝑗,andthesuperscriptisomitted.
6in modern tensor libraries such as numpy. For example, we can use contract(MN,NK→MK) to denote the matrix-matrix
multiplicationoperator,andinournotationcontract(MN,NK→MK)(𝑋,𝑌)(whichisequivalentto𝑋 ·𝑌)canbetranslated
tocodeasnumpy.einsum(′mn,nk→mk′,X,Y).
AlargeglossaryofnotationisincludedinAppendixA.
3 State Space Models are Structured Matrices
Thissectionexploresdifferentperspectivesofthestatespacemodelasasequencetransformation,andoutlinesproperties
and algorithms of such maps. The main results of this section are about the equivalence between state space models
and a family of structured matrices called semiseparable matrices, which imply new efficiency results (Theorems 3.5
and3.7).
3.1 TheMatrixTransformationFormofStateSpaceModels
RecallthatourdefinitionofanSSMisdefinedasaparameterizedmapdefinedthrough(2). Ourtheoreticalframework
startsbysimplywritingthistransformationasamatrixmultiplicationmappingthevectors𝑥 ∈RT ↦→𝑦 ∈RT.
Bydefinition,ℎ
0
=𝐵 0𝑥 0.Byinduction,
ℎ
𝑡
=𝐴
𝑡
...𝐴 1𝐵 0𝑥 0+𝐴
𝑡
...𝐴 2𝐵 1𝑥 1+···+𝐴 𝑡𝐴 𝑡−1𝐵 𝑡−2𝑥 𝑡−2+𝐴 𝑡𝐵 𝑡−1𝑥 𝑡−1+𝐵 𝑡𝑥
𝑡
𝑡
∑︁
= 𝐴 𝑡× :𝑠𝐵 𝑠𝑥 𝑠.
𝑠=0
Multiplyingby𝐶 𝑡 toproduce𝑦 𝑡 andvectorizingtheequationover𝑡 ∈ [T],wederivethematrixtransformationformof
SSMs.
𝑡
∑︁
𝑦 𝑡 = 𝐶 𝑡⊤𝐴 𝑡× :𝑠𝐵 𝑠𝑥 𝑠
𝑠=0 (3)
𝑦 =SSM(𝐴,𝐵,𝐶)(𝑥) =𝑀𝑥
𝑀
𝑗𝑖
(cid:66)𝐶⊤
𝑗
𝐴 𝑗···𝐴 𝑖+1𝐵
𝑖
3.2 SemiseparableMatrices
𝑀 in equation (3) is a particular representation of a class of matrices known as semiseparable matrices. Semiseparable
matricesareafundamentalmatrixstructure.Wefirstdefinethesematricesandtheirproperties.
Definition3.1. A(lowertriangular)matrix𝑀isN-semiseparableifeverysubmatrixcontainedinthelowertriangularportion
(i.e.onorbelowthediagonal)hasrankatmostN.WecallNtheorderor rankofthesemiseparablematrix.
Definition 3.1, and other forms of related “separable” structure (e.g. quasiseparable matrices and other definitions of
semiseparablematrices)aresometimescalledstructuredrankmatrices(orrank-structuredmatrices)becausetheyare
characterized by rank conditions on their submatrices. Semiseparable matrices have many structured representations
includingthehierarchicalsemiseparable(HSS),sequentialsemiseparable(SSS),andBruhatforms(PernetandStorjohann
2018).WewillprimarilyusetheSSSform.
3.2.1 TheSequentiallySemiseparable(SSS)Representation
Definition3.2. Alowertriangularmatrix𝑀 ∈ R(T,T) hasaN-sequentiallysemiseparable(SSS)representationifitcan
bewrittenintheform
𝑀
𝑗𝑖
=𝐶⊤
𝑗
𝐴 𝑗···𝐴 𝑖+1𝐵
𝑖
(4)
forvectors𝐵 0,...,𝐵 T−1,𝐶 0,...,𝐶
T−1
∈RN andmatrices𝐴 0,...,𝐴
T−1
∈R(N,N).
WedefinetheoperatorSSSsothat𝑀 =SSS(𝐴 0:T,𝐵 0:T,𝐶 0:T).
7AfundamentalresultofsemiseparablematricesisthattheyareexactlyequivalenttomatriceswithSSSrepresentations.
Onedirectioncanbededucedwithasimpleconstructiveproof.
Lemma3.3. AnN-SSSmatrix𝑀 withrepresentation(4)isN-semiseparable.
Proof. Consideranyoff-diagonalblock𝑀 𝑗:𝑗′,𝑖′:𝑖 where 𝑗′ > 𝑗 ≥𝑖 >𝑖′.Thishasanexplicitrank-Nfactorizationas
𝐶⊤𝐴× 𝐵 ... 𝐶⊤𝐴× 𝐵 𝐶⊤𝐴×
 𝑗 𝑗:𝑖′ 𝑖′ 𝑗 𝑗:𝑖−1 𝑖−1   𝑗 𝑗:𝑗 
   . . . . . .    =    . . .   𝐴× 𝑗:𝑖−1 (cid:2)𝐴 𝑖× −1:𝑖′𝐵 𝑖′ ··· 𝐴 𝑖× −1:𝑖−1𝐵 𝑖−1(cid:3). (5)
𝐶⊤ 𝐴× 𝐵 ... 𝐶⊤ 𝐴× 𝐵  𝐶⊤ 𝐴× 


𝑗′−1 𝑗′−1:𝑖′ 𝑖′ 𝑗′−1 𝑗′−1:𝑖−1 𝑖−1



𝑗′−1 𝑗′−1:𝑗

□
Equation (5) will be used extensively in deriving our fast algorithms for sequence models. The other direction is well-
establishedintheliteratureonsemiseparablematrices.
Proposition3.4. EveryN-semiseparablematrixhasaN-SSSrepresentation.
Furthermore,notethatalthoughDefinition3.2involves𝑂(N2T)parametersfortherepresentation(inparticulartostorethe
𝐴matrices),itcanactuallybecompresseddownto𝑂(NT) parameters,whichisasymptoticallytight(Pernet,Signargout,
and Villard 2023). Therefore in the rest of this paper we will conflate the structured matrix class (Definition 3.1) and a
particularrepresentationofit(Definition3.2);wewillalwaysusethisrepresentationinsteadofothercandidates.Inturn
wewilluseN-SStorefertoanN-semiseparablematrixinSSSform.
Semiseparablematricesareafundamentalmatrixstructureandhavemanyimportantproperties.Theyaredeeplyrelated
torecurrencesatlarge,andcanbedefinedbymultiplecharacterizations(e.g.Definitions3.1and3.2)whichrevealdifferent
connectionsandefficientalgorithmsforthem.WementionsomeoftheirotherpropertiesinAppendixC.1.
Remark 2. The notion of semiseparability is very broad and many similar but subtlely different definitions appear in the
literature;ourdefinitionsmaydifferslightlyfromotherconventions.First,becauseweareprimarilyconcernedwithcausalor
autoregressivesettingsinthispaper,wehaverestrictedthedefinitionofsemiseparabilitytothetriangularcase;Definition3.1
moreformallymightbecalled(N,0)-semiseparabilitybysomeauthors.Someauthorsmayalsoinsteadrefertoitasaformof
quasiseparability(EidelmanandGohberg1999;Pernet2016).SeeVandebriletal.(2005)forabriefsurvey.
3.2.2 1-SemiseparableMatrices:theScalarSSMRecurrence
Wewillsingleoutthespecialcaseof1-SSmatrices. Notethatinthiscase,the𝐶 and𝐵 arescalars,andcanbefactored
𝑗 𝑖
outoftheSSSrepresentation(4)(wealsouselower-casetoemphasizethattheparametersarescalarsinthiscase)
SSS(𝑎,𝑏,𝑐) =diag(𝑐)·𝑀 ·diag(𝑏) where 𝑀
𝑗𝑖
=𝑎× 𝑗:𝑖.
Since diagonal matrices are easy to handle (e.g. multiplication by a diagonal matrix is the same as elementwise scalar
multiplication),wecanignoretheseterms.Thusourbasicrepresentationofa1-SSmatrixis𝑀
𝑗𝑖
=𝑎
𝑗:𝑖
or
1
 
 𝑎 1 
 1 
 𝑎 𝑎 𝑎 1 
𝑀 =1SS(𝑎 0:𝑇) (cid:66) 
 
2
. . .
1
. .
.2
... ...


. (6)
 
𝑎 ...𝑎 𝑎 ...𝑎 ... 𝑎 1
 𝑇−1 1 𝑇−1 2 𝑇−1 
 
The importance of 1-SS matrices lies in their equivalence to the minimal form of a scalar recurrence – the case of a
degenerateSSMwithstatedimensionN=1andno (𝐵,𝐶) projections. Notethatmultiplication𝑦 =𝑀𝑥 canbecomputed
bytherecurrence
𝑦
𝑡
=𝑎 𝑡:0𝑥 0+···+𝑎 𝑡:𝑡𝑥
𝑡
=𝑎
𝑡
(𝑎 𝑡−1:0𝑥 0+···+𝑎 𝑡−1:𝑡−1𝑥 𝑡−1)+𝑎 𝑡:𝑡𝑥
𝑡
(7)
=𝑎 𝑡𝑦 𝑡−1+𝑥 𝑡.
8Outputs 𝑌
Sequence
Transformation
Matrix 𝑀
Sequencedim. T
Head dim.
P State Space Models are Semiseparable Matrix Transformations
Inputs 𝑋
Figure2: (StateSpaceModelsareSemiseparableMatrices.) Assequencetransformations,statespacemodelscanbe
representedasamatrixtransformation𝑀 ∈R(T,T) actingonthesequencedimensionT,sharingthesamematrixforeach
channel in a head (Left). This matrix is a semiseparable matrix (Right), which is a rank-structured matrix where every
submatrixcontainedon-and-belowthediagonal(Blue)hasrankatmostN,equaltotheSSM’sstatedimension.
Wethusalsorefertomatrixmultiplicationby1-SSmatricesasthescalarSSMrecurrenceorthecumprodsum(cumu-
lativeproductsum; ageneralizationofcumulativeproductandcumulativesum)operator. Asthefundamentalformof
recurrence,multiplicationby1-SSmatricesisimportantasabuildingblockforourmainalgorithms.
We emphasize that one of the central themes of this paper is that many algorithms on sequence models can be reduced
to structured matrix multiplication algorithms. 1-SS matrices exemplify this connection: there are many fast algorithms
forcomputingtheprimitivescalarrecurrenceorcumprodsumoperator,andallofthemturnouttobeequivalenttodif-
ferentstructuredfactorizationof1-SSmatrices. WededicateAppendixBtothesealgorithmsfor1-SSmatrixmultiplica-
tion.
3.3 StateSpaceModelsareSemiseparableMatrices
RecallthatourdefinitionofanSSMisdefinedasaparameterizedmapdefinedthroughDefinition2.1. Theconnection
between SSMs and semiseparable matrices follows from simply writing this transformation as a matrix multiplication
mappingthevectors𝑥 ↦→𝑦 ∈RT.
Equation(3)directlyestablishesthelinkbetweenstatespacemodelsandthesequentiallysemiseparablerepresentation,
whichinturnareequivalenttosemiseparablematricesingeneral(Lemma3.3andProposition3.4).
Theorem3.5. Thestatespacemodeltransformation𝑦 =SSM(𝐴,𝐵,𝐶)(𝑥)withstatesizeNisidenticaltomatrixmultiplica-
tionbyanN-SSmatrixinsequentiallysemiseparablerepresentation𝑦 =SSS(𝐴,𝐵,𝐶)·𝑥.
InotherwordsthesequencetransformationoperatorSSM(Definition2.2)coincideswiththematrixconstructionoper-
atorSSS(Definition3.2),andweusetheminterchangeably(orsometimesSSasshorthand). Furthermore—byatwistof
fate—structuredstatespacemodelsandsequentiallysemiseparablematriceshavethesameacronyms,underscoringtheir
equivalence!ConvenientlywecanuseanyoftheseacronymsSSM(statespacemodelorsemiseparablematrix),SSS(struc-
turedstatespaceorsequentiallysemiseparable),orSS(statespaceorsemiseparable)interchangeablytounambiguously
refertoeitherconcept. However,wewillgenerallyusetheconventionthatSSMreferstostatespacemodel,SSrefersto
semiseparable,andSSSreferstosequentiallysemiseparable.
Figure2illustratesthesequencetransformationperspectiveofstatespacemodelsassemiseparablematrices.
9
noitacilpitlum
xirtaM3.4 ComputingStateSpaceModelsthroughStructuredMatrixAlgorithms
The reason Theorem 3.5 is important is that it will allow us to reduce the problem of efficient computation of SSMs (and
other sequence models) into efficient algorithms for structured matrix multiplication. We briefly provide an overview and
deferourmainnewalgorithmtoSection6,aftershowingtheequivalenceofSSMstoothersequencemodelsinSections4
and5.
Aspreviouslydefined,semiseparablematrices(i.e.rank-structuredmatrices)areaclassicaltypeofstructuredmatrix:
(i) TheyhavecompressedrepresentationssuchastheSSSformwhichhasonly𝑂(T)insteadof𝑂(T2)parameters.
(ii) Theyhavefastalgorithmsoperatingdirectlyonthecompressedrepresentation.
Furthermore,theparameterizationandmatrixmultiplicationcostcanbetightinthesemiseparableorder.
Proposition3.6(Pernet,Signargout,andVillard(2023)). AnN-SSmatrixofsizeTcanberepresentedin𝑂(NT)parameters
andhasmatrix-vectormultiplicationintimeandspace𝑂(NT).
Forexample,1-SSmatricesillustratetheessenceofthisconnection. Thematrix𝑀 = 1SS(𝑎) isdefinedbyexactlyT−1
parameters𝑎
0:T−1
=𝑎 1,...,𝑎 T−1,andcanbecomputedin𝑂(T)timebyfollowingthescalarrecurrence(7).
3.4.1 TheLinear(Recurrent)Mode
Proposition 3.6 can be easily seen in the case of diagonal structured SSMs (S4D (Gu, Gupta, et al. 2022)), simply by
leveragingthestatespacemodelformulation(2)andunrollingtherecurrence.Weprovidetheformaltensor-contraction
algorithmin(8),wherethedimensionSisequaltoT4.
𝑍 =contract(SP,SN→SPN)(𝑋,𝐵) (S,P,N) (8a)
𝐻 =contract(TSN,SPN→TPN)(𝐿,𝑍) (T,P,N) (8b)
𝑌 =contract(TN,TPN→TP)(𝐶,𝐻) (T,P) (8c)
Here,𝐿 ∈R(T,T) isdefinedas1SS(𝐴),orinotherwords𝐿 0:T,0:T =1SS(𝐴 0:T)for𝑖 ∈ [N].Thisalgorithminvolvesthreesteps
correspondingto(2):
(i) expandingtheinput𝑋 bytheinputmatrix𝐵(8a),
(ii) unrollingindependentscalarSSMrecurrences(8b),and
(iii) contractingthehiddenstate𝐻 bytheoutputmatrix𝐶 (8c).
NotethatwehaveusedtheequivalencebetweenscalarSSMsand1-SSmatricesinstep(8b).
Remark 3. We note that (8) is a special case of the Mamba (S6) model. however, a naive implementation is slow because
oftheexpandedtensors𝑍 and𝐻 ofsize (T,P,N);GuandDao(2023)introducedahardware-awareimplementationtoavoid
materializingthesetensors.
Surprisingly, Theorem 3.5 and Proposition 3.6 immediately imply that all SSMs have the same asymptotic efficiency as
algorithm(8).
Theorem3.7. Anystatespacemodel(Definition2.2)ofstatesizeNonsequencelengthTcanbecomputedintime𝑂(TN)(not
accountingforpotentialpreprocessing).
WenotethatthisresultisnewtothestructuredSSMliterature. Inparticular,givendenseunstructured𝐴 matrices,the
𝑡
total representation alone seems to be of size𝑂(TN2). Thus Theorem 3.7 states the non-trivial result that with a pre-
processingstep,evenanunstructuredSSMcanbecomputedoptimallyefficiently,withupperboundmatchingthelower
bound𝑂(TN)givenbythesizeof𝐵and𝐶.
Remark 4. Theorem 3.7 is perhaps not too surprising in light of the fact that almost all dense matrices over
R(N,N)
are
diagonalizableoverC,leadingtotheresultthat almostalldenserealSSMsareequivalenttoadiagonalcomplexSSM.This
factunderliesthereasonwhydiagonalSSMsarethemostpopularformofstructuredSSM(Gu,Gupta,etal.2022;Gupta,Gu,
4Adifferentsymbolisrequiredforthecontractionnotation.
10andBerant2022;J.T.Smith,Warrington,andLinderman2023). However,Theorem3.7impliesthemuchstrongerresultfor
allrealSSMs(notjustthediagonalizableones),aswellasdenseSSMsoverotherfields(includingCitself).
In practice, efficiently computable SSMs still require additional structure on𝐴, particularly to avoid the expensive pre-
processingstep(whichbothhasorderNextraFLOPsandinvolveshardware-inefficientoperationssuchassingularvalue
decompositions). ThesestructuresarethefocusofpastworkonstructuredSSMs(e.g.S4(D)andMamba)aswellasour
newalgorithms. Inparticular,whenslightlystrongerstructureisimposedon𝐴,wewilldesignveryhardware-efficient
algorithmsthroughblockdecompositionsoftheSSMmatrix𝑀 =SSS(𝐴,𝐵,𝐶)inSection6.
3.4.2 TheQuadratic(Naive)Mode
WenotethatthereisanotherwaytocomputeanSSMexposedbyournewmatrixpointofview.Anaivecomputationof
thematrixSSMrepresentation(3)involvessimplymaterializingthesequencetransformationmatrix𝑀 = SSS(𝐴,𝐵,𝐶).
Thisisa(T,T)matrix,andthereforethisnaivealgorithmwillscalequadraticallyinsequencelength.However,whenthe
sequencelengthTisshort, thiscanactuallybemoreefficientthanthelinearalgorithmduetoconstantfactorsandthe
hardware-friendlinessofthecomputationpattern(e.g. leveragingmatrix-matrixmultiplications). Infact,foraparticular
caseofstructuredSSMs,thislooksverysimilartoaquadraticattentioncomputation(Section5).
3.4.3 Summary
Many sequence models are explicitly motivated or defined as matrix sequence transformations – most notably Trans-
formers,wherethematrixmixeristheattentionmatrix. Ontheotherhand,RNNsandSSMshavenotpreviouslybeen
describedinthisway. Byprovidinganexplicitmatrixtransformationformofstatespacemodels,werevealnewwaysof
understandingandusingthem. Fromacomputationalperspective,anymethodofcomputingtheforwardpassofastate
space model can be viewed as a matrix multiplication algorithm on semiseparable matrices. The semiseparable matrix
perspective provides one lens into state space duality (SSD), where the dual modes respectively refer to a linear-time
semiseparablematrixmultiplicationalgorithmandquadratic-timenaivematrixmultiplication.
Moreover,leveragingtherichstructureofsemiseparablematricescanleadtoevenbetteralgorithmsandmoreinsights(e.g.
Section6andAppendixB).InAppendixC.1,wedescribesomeadditionalpropertiesofsemiseparablematrices.
4 Structured Masked Attention: Generalizing Linear Attention
with Structured Matrices
Inthissectionwerevisitthelinearattentionframeworkfromfirstprinciples.Themainresultsinthissectionareasimple
tensor-contraction-basedproofoflinearattention(Proposition4.1),andourgeneralizedabstractionofstructuredmasked
attentioninDefinition4.2. Wenotethatthissectionderivesthemaindualityresultsfromadifferentdirectionthanstate
spacemodelsandcanbereadcompletelyindependentlyofSection3.
• Section4.1setsupourframeworkforvariantsofattention,withaparticularfocusonkernelattentionandmasked
kernelattention.
• Section 4.2 provides our first main attention result, a simple proof of linear attention through the lens of tensor
contractions.
• Section4.3definesstructuredmaskedattention,ourgeneralizationofpriorattentionvariantsthroughstructured
matrices.
114.1 TheAttentionFramework
4.1.1 Attention
Thebasicformof(single-head)attentionisamaponthreesequencesofvectors(𝑄,𝐾,𝑉) ↦→𝑌.
𝑄 =input (T,N)
𝐾 =input (S,N)
𝑉 =input (S,P)
(9)
𝐺 =𝑄𝐾⊤ (T,S)
𝑀 = 𝑓(𝐺) (T,S)
𝑌 =𝐺𝑉 (T,P)
Weuse“shapeannotations”toindicatethedimensionsoftensors,e.g.𝑄 ∈ R(T,N).Inthisgeneralform,SandTrepresent
sourceandtarget sequencelengths,Nrepresentsthefeaturedimension,andPrepresentstheheaddimension.
Themostcommonvariantof softmaxattentionusesasoftmaxactivation 𝑓 = softmaxtonormalizetherowsofthe𝐺
matrix.
4.1.2 Self-Attention
Ourtreatmentismotivatedbythemostimportantcaseofself-attention,where
(i) thesourceandtargetsequencesarethesame(i.e.S=T),
(ii) usuallythefeatureandheaddimensionsarethesame(i.e.N=P),
(iii) and𝑄,𝐾,𝑉 aregeneratedbylinearprojectionsonthesameinputvector(𝑄 =𝑊
𝑄
·𝑋,𝐾 =𝑊
𝐾
·𝑋,𝑉 =𝑊
𝑉
·𝑋).
However,ourpresentationabstractsawaythesechoicesandbeginsfromthe𝑄,𝐾,𝑉 matrices.
Remark 5. Our focus is on the self-attention case with equal head and feature dimensions (i.e. S = T and N = P), which
should be used as the running example. We define the general formulation of attention not only so that our framework
captures variants such as cross-attention, but also because separating the notation for dimensions (e.g. S and T) makes the
contractionnotationproofsofourmainresultsinthissectionmoreclear.
Remark6. Whileattentionisusuallyframedasanoperationonthesethreeinputs𝑄,𝐾,𝑉 whichareviewedsymmetrically,
theinputandoutputdimensionsin(9)indicateotherwise. Inparticular,thefeaturedimensionNisnotpresentintheoutput;
therefore in the case when S = T (e.g. self-attention), we view𝑉 as the main input, so that (9) defines a proper sequence
transformation𝑉 ↦→𝑌 (Definition2.1).
4.1.3 KernelAttention
ThestepwherethesoftmaxfunctionisappliedtotheGrammatrix𝐺 canbedecomposedintotwoparts:
1. Exponentiatingthe𝐺 matrix.
2. Normalizingthe𝐺 matrixontheSaxis.
Wecanignorethenormalizationtermfornow,asitamountstosimplypassingin𝑉 = 1anddividing(werevisitthisin
Section7.3).Theexponentiationtermcanbeviewedasakerneltransformation:thereisan(infinite-dimensional)feature
map𝜑 such that exp(𝑄𝐾⊤) = 𝜑(𝑄)𝜑(𝐾)⊤. By abstracting away the feature map into the definition of𝑄 and 𝐾 itself
(i.e.define𝑄,𝐾 asthepost-transformedversions),wecanignorethesoftmaxtransformation,andassumethat𝑄,𝐾 are
arbitrarilygeneratedbykernelfeaturemapsandpotentiallyN≠P.
Manyinstantiationsofkernelattentionhavebeenproposed,including:
• TheoriginalLinearAttention(Katharopoulosetal.2020)definesthekernelfeaturemapasanarbitrarypointwise
activationfunction,suchas𝑥 ↦→1+elu(𝑥).
• Random Feature Attention (RFA) (H. Peng et al. 2021) chooses the kernel feature map to approximate softmax
attention(i.e. theexpfeaturemap)usingtherandomFourierfeatureapproximationofGaussiankernels(Rahimi
12andRecht2007).Thisinvolvesrandomprojections(i.e.multiplying𝑄and𝐾byarandomprojection𝑊 andapplying
theactivation𝑥 ↦→ (cos(𝑥),sin(𝑥)).
• Performer(Choromanskietal.2021)proposesthefastattentionviapositiveorthogonalrandomfeatures(FAVOR+).
Thepositiverandomfeatures(PRF)partchoosesthekernelfeaturemaptobearandomprojectionfollowedbythe
featuremap𝑥 ↦→2−1/2(exp(𝑥),exp(−𝑥)). Thischoiceismotivatedsothatthekernelelementsarepositive-valued
andprovablyapproximatesthesoftmaxattention.[Italsoproposeschoosingtherandomprojectionsinorthogonal
directions,whichwedonotconsider.]
• cosFormer (Qin, Weixuan Sun, et al. 2022) augment RFA with a cosine reweighting mechanism that incorpo-
rates positional information to emphasize locality. This effectively passes 𝑄 𝑡,𝐾 𝑡 through the feature map 𝑥 ↦→
(𝑥cos(𝜋𝑡/2𝑇),sin(𝜋𝑡/2𝑇)).
• Linear Randomized Attention (Zheng, C. Wang, and Kong 2022) generalize RFA from the perspective of impor-
tance sampling, and generalize it to provide better estimates of the full softmax kernel (rather than just the exp-
transformednumerator).
OtherrelatedattentionvariantsincludeLinformer(SinongWangetal.2020)andNyströformer(Xiongetal.2021),which
bothuselow-rankapproximationsoftheattentionmatrix𝑀(andarethuscompatiblewithequation(9)),throughrandom
projections(Johnson-Lindenstrauss)andkernelapproximation(theNyströmmethod)respectively.
4.1.4 Masked(Kernel)Attention
Let𝐿 beamaskofshape (T,S). Mostcommonly,intheautoregressiveself-attentioncasewhenS = T,𝐿 maybealower-
triangularmatrixof1’srepresentingacausalmask.Besidesenforcingcausality,manyothertypesofmaskscanbeapplied
–inparticularvarioussparsitypatternssuchasbanded,dilated,orblockdiagonal–whicharemotivatedbyreducingthe
complexityofdenseattention.
Maskedattentionisusuallywritteninmatrixnotationas
𝑦 = (𝐿◦(𝑄𝐾⊤))·𝑉. (10)
Moreprecisely,withshapeannotationsandbreakingthisdownintotheprecisesequenceofcomputations:
𝐺 =𝑄𝐾⊤ (T,S)
𝑀 =𝐺 ◦𝐿 (T,S) (11)
𝑌 =𝑀𝑉 (T,P)
Ourimprovedderivationofattentionvariantsinthissectionstartsbynoticingthatthisformulacanbewrittenasasingle
contraction:
𝑌 =contract(TN,SN,SP,TS→TP)(𝑄,𝐾,𝑉,𝐿) (12)
andthealgorithmin(11)canbereframedascomputing(12)byaparticularorderingofpairwisecontractions
𝐺 =contract(TN,SN→TS)(𝑄,𝐾) (T,S) (13a)
𝑀 =contract(TS,TS→TS)(𝐺,𝐿) (T,S) (13b)
𝑌 =contract(TS,SP→TP)(𝑀,𝑉) (T,P) (13c)
4.2 LinearAttention
Linearattention, andmanyothervariantsofefficientattention, isoftenmotivatedbychangingtheorderofmatrixas-
sociativity in the core attention computation (𝑄𝐾⊤)𝑉 = 𝑄(𝐾⊤𝑉). However when the mask is added, the derivation is
somewhatlessstraightforward(forexample,theoriginalpaper(Katharopoulosetal.2020)andvariants(Y.Sunetal.2023)
statetheformulawithoutproof).
Roughly, thelinearattentionmethodclaimsthatthefollowingformulaisequivalentto(10), whichmustbeverifiedby
expandingthesumandtrackingindicescarefully.
𝑌 =𝑄 ·cumsum(𝐾⊤𝑉) (14)
13Proposition4.1((Katharopoulosetal.2020)). Autoregressivekernelattention,i.e.maskedkernelattentionwiththecausal
mask,canbecomputedin𝑂(𝑇)timebyarecurrencetakingconstanttimeperstep.
4.2.1 ATensorContractionProofofLinearAttention
Wepresentasimpleandrigorousderivationoflinearattentionthatwillalsoimmediatelyrevealhowtogeneralizeit.The
mainideaistoperformthecontraction(12)inanalternateorder.Weavoidambiguousmatrixnotationandworkdirectly
withcontractionnotation:
𝑍 =contract(SP,SN→SPN)(𝑉,𝐾) (S,P,N) (15a)
𝐻 =contract(TS,SPN→TPN)(𝐿,𝑍) (T,P,N) (15b)
𝑌 =contract(TN,TPN→TP)(𝑄,𝐻) (T,P) (15c)
Intuitively,weinterpretthiscontractionorderasfollows.
The first step (15a) performs an “expansion” into more features, by a factor of the feature dimension N. The third step
(15c)contractstheexpandedfeaturedimensionaway. If𝐾 isviewedastheinput(Remark6),then𝑉 and𝑄 performthe
expansionandcontraction,respectively.
Thesecondstepisthemostcritical,andexplainsthelinear partoflinearattention. Firstnoticethat(15b)isjustadirect
matrixmultiplicationby𝐿 (sincethe (P,N) axescanbeflattened). Alsonotethatthisistheonlytermthatinvolvesboth
T and S axes, hence should have Ω(TS) complexity (i.e. quadratic in sequence length). However, when the mask 𝐿 is
thestandardcausalattentionmask(lowertriangular1’s),matrix-vectormultiplicationby𝐿isidenticaltoafeature-wise
cumulativesum
1
 
𝑦 =  . . . ...  𝑥 ⇐⇒ 𝑦 0 =𝑥 0 .

1 ...
1

𝑦
𝑡
=𝑦 𝑡−1+𝑥
𝑡
 
 
4.3 StructuredMaskedAttention
Withthe tensorcontraction perspectiveof maskedattention (15), we canimmediately seethat thecrux ofthe original
linearattentionisthefactthatmatrix-vectormultiplicationbythecausalmaskisequivalenttothecumulativesumopera-
tor.
However,weobservethatthereisnoreasontheattentionmaskhastobeall1’s.Allthatisnecessaryforlinearattention
tobefastisfor𝐿tobeastructuredmatrix,whichbydefinitionarethosethathavefastmatrixmultiplication(Section2.3).
In particular, we can use any mask matrix 𝐿 that has sub-quadratic (ideally linear) matrix-vector multiplication, which
wouldhavethesamecomplexityasstandardlinearattentionbyspeedingupthebottleneckequation(15b).
Definition 4.2. Structured masked attention (SMA) (or structured attention for short) is defined as a function on
queries/keys/values𝑄,𝐾,𝑉 as well as any structured matrix 𝐿 (i.e. has sub-quadratic matrix multiplication), through the
4-waytensorcontraction
𝑌 =contract(TN,SN,SP,TS→TP)(𝑄,𝐾,𝑉,𝐿).
The SMA quadratic mode algorithm is the sequence of pairwise contractions defined by (13), which corresponds to the
standard(masked)attentioncomputation.
The SMA linear mode algorithm is the sequence of pairwise contractions defined by (15), where step (15b) is optimized
throughthesubquadraticstructuredmatrixmultiplication.
We can instantiate structured masked attention to any given class of matrix structure. Some examples include (Fig-
ure3):
• Linearattentionusesacausalmask.
• RetNet(Y.Sunetal.2023)usesadecaymask𝐿 𝑖𝑗 =𝛾𝑖−𝑗 ·I[𝑗 ≥𝑖] forsomedecayfactor𝛾 ∈ [0,1].
14Sequence Transformation
Structured Mask $ Matrix %
Keys
"
Causal Mask Linear Attention
Decay Mask Retentive Network
SSD
Queries =
1-semiseparable 1-SS Structured Attention
!
Toeplitz Toeplitz Structured Attention
Discrete Fourier
Fourier Structured Attention
Transform
Figure3: (StructuredMaskedAttention.) SMAconstructsamaskedattentionmatrix𝑀 =𝑄𝐾⊤◦𝐿foranystructured
matrix𝐿,whichdefinesamatrixsequencetransformation𝑌 =𝑀𝑉. AllinstancesofSMAhaveadualsubquadraticform
inducedbyadifferentcontractionordering,combinedwiththeefficientstructuredmatrixmultiplicationby𝐿. Previous
examplesincludeLinearAttention(Katharopoulosetal.2020)andRetNet(Y.Sunetal.2023).BeyondSSD(1-semiseparable
SMA),thefocusofthispaper,manyotherpotentialinstantiationsofstructuredattentionarepossible.
• ThedecaymaskcouldbegeneralizedtoaToeplitzmatrix𝐿
𝑖𝑗
=𝛼
𝑖−𝑗
forsomelearnable(orinput-dependent)setof
parameters𝛼 ∈RT.Thiscanbeinterpretedasaformofrelativepositionalencoding,reminiscentofothermethods
suchasAliBi(Press,N.Smith,andLewis2022)butmultiplicativeinsteadofadditive.
• AnothervariantcoulduseaFouriermatrix𝐿
𝑖𝑗
=𝜔𝑖𝑗/Ttoencodepositionalstructureadifferentway.
InSection5,weconsidersemiseparableSMA,whichdefinesourmainSSDmodel.
4.3.1 Summary:TheDualFormsofMaskedAttention
Standard(maskedkernel)attentionis oftenconflatedbetweenafunctionandanalgorithm. Separating thisdistinction
presentsaclearwaytounderstanddifferentvariantsofattention.
• Weviewmaskedattentionasaparticularfunction(12).
• Thestandardquadraticattentioncomputation(13)canbeviewedasanalgorithmtocomputethefunction.
• Linearattention(15)isanalternatealgorithmtocomputethesamefunction.
Moreover,inthiscase
• Themaskedattentionfunctionissimplyaparticularcontractiononfourterms.
• Thequadraticandlinearattentionalgorithmsaresimplytwodifferentorderstoperformthecontractions.
Itisknownthatcontractionorderingscanmakelargedifferencesincomputationcomplexity,leadingtothequadraticvs.
linearsplit. Justasstatespacemodelsareatransformationthatcanbecomputedinmultipleways,withdualquadratic
vs.linearforms(Section3.4),linearattentionhasasimilardualitythatresultsfromtwocontractionorders. Infact,these
turnouttobedifferentperspectivesonthesameunderlyingduality,whichwemakeexplicitinSection5.
155 State Space Duality
In Sections 3 and 4, we defined structured state space models and structured attention, discussed their properties, and
showedthattheybothhaveaquadraticalgorithmandalinearalgorithm.Thissectionconnectsthemtogether.Ourmain
result is showing that a particular case of structured state space models coincides with a particular case of structured
attention,andthatthelinear-timeSSMalgorithmandquadratic-timekernelattentionalgorithmaredualformsofeach
other.
• Section5.1specializesstatespacemodelstoscalarstructure,wherethenaivequadraticcomputationcanbeseenas
aninstanceofkernelattention.
• Section 5.2 specializes structured masked attention to semiseparable SMA, which characterizes masked attention
withefficientautoregression.
• Section 5.3 summarizes the connection between structured masked attention and structured state space models,
termedstructuredstatespaceduality.
5.1 Scalar-IdentityStructuredStateSpaceModels
InSection3weshowedthatstatespacemodelsareequivalenttosemiseparablematrixtransformations,resultinginboth
alinearrecurrentformandquadraticnaiveform.
RecallthatSSMsaredefinedby𝑦 =SSM(𝐴,𝐵,𝐶)(𝑥),andthematrixformofSSMsusestheSSS(sequentiallysemisepara-
ble)representation𝑀 =SSS(𝐴,𝐵,𝐶)where𝑀
𝑗𝑖
=𝐶⊤
𝑗
𝐴 𝑗:𝑖𝐵
𝑖
(equation(3)).
Nowletusconsiderthecasewhere𝐴 issimplyascalar;inotherwords,aninstantiationofastructuredSSMwherethe
𝑗
𝐴matricesareextremelystructured:𝐴=𝑎𝐼 forscalar𝑎andidentitymatrix𝐼.Thenwecanrearrange
𝑀
𝑗𝑖
=𝐴
𝑗:𝑖
·(𝐶⊤
𝑗
𝐵 𝑖).
Andthiscanbevectorizedinto
𝐿 (cid:66) 1SS(𝑎)
𝑀 =𝐿◦(𝐶𝐵⊤)
where𝐵,𝐶 ∈R(T,N).
Usingthisformulation,thefulloutput𝑌 =𝑀𝑋 iscomputedpreciselyas
𝐺 =contract(TN,SN→TS)(𝐶,𝐵) (T,S)
𝑀 =contract(TS,TS→TS)(𝐺,𝐿) (T,S) (16)
𝑌 =contract(TS,SP→TP)(𝑀,𝑋) (T,P)
whereS=T.Butthisisexactlythesameasoriginaldefinitionofmaskedkernelattentiondefinition(13)!
Therefore, as alluded to in Section 3.4, naively computing the scalar structured SSM—by materializing the semiseparable
matrix 𝑀 and performing quadratic matrix-vector multiplication—is exactly the same as quadratic masked kernel atten-
tion.
5.2 1-SemiseparableStructuredMaskedAttention
Structuredmaskedattentionallowsfortheuseofanystructuredmask𝐿.When𝐿isthecausalmask,itisstandardlinear
attention.Notethatthecausalmaskis𝐿 =SS(1 𝑇),i.e.the1-SSmaskisgeneratedby𝑎
𝑡
=1indefinition(6).Thismotivates
generalizing𝐿 totheclassof1-semiseparablemasks,or1-semiseparablestructuredmaskedattention(1-SSSMA),
wherethecumsuminlinearattention’srecurrenceisreplacedbyamoregeneralrecurrence–thescalarSSMscan, i.e.
1-semiseparablematrixmultiplication(Section3.2.2).
Finally, the most important reason we consider 1-semiseparable SMA is because the linear form for computing it is a
special case of diagonal state space model. The linear form of SMA is algorithm (15), where the bottleneck step (15b)
16canbeviewedasmatrixmultiplicationbythe1-SSmask. InSection3,wealsowroteoutthecomputationforadiagonal
SSM(8),wherethebottleneckstep(8b)isascalarSSMrecurrencewhichisequivalentto1-SSmultiplication. Theonly
differenceisthat(8b)hasanextraNdimensionin𝐿,becausethematrix𝐴isadiagonalmatrixofsizeN.ThisNdimension
woulddisappearifalldiagonalentriesof𝐴arethesame,whichresultsinCorollary5.1.
Corollary5.1. 1-SSSMA(maskedattentionwith1-semiseparablestructuredmatrices𝐿)(15)isaspecialcaseofadiagonal
SSM (8)wherethediagonalmatrixisascalarmultipleoftheidentity.
WhileCorollary5.1saysthat1-SSSMAhasanefficientrecurrentform,wecanalsoshowaconverseresultthatcharac-
terizeswhichinstancesofSMAhasefficientautoregression.
Theorem 5.2. For any instantiation of structured masked attention (Definition 4.2) that is an autoregressive process with
boundedorder,thestructuredmask𝐿mustbeasemiseparablematrix.
Inotherwords,efficientautoregressiveattentionisgeneralsemiseparableSMA.Theorem5.2isprovedinAppendixC.2.
Remark7. While1-semiseparableSMAisaspecialcaseofastatespacemodel,generalsemiseparableSMAisstrictlymore
expressivethan1-SSSMA,andcannotbedescribedbyastandardSSM.However,thesemiseparablemultiplicationby𝐿 and
thelinearformofSMA(equation(15a))eachinvolveanexpansionandcontractionstep,andcanbeabsorbedintoasimilar
instanceof1-SSSMAwithasingle(larger)expansion.
Insummary,1-semiseparablestructuredattentionisthemostimportantcaseofSMA,becauseitis:
• anaturalgeneralizationoflinearattentionwithaninput-dependentrecurrence.
• thesimplestcaseofgeneralsemiseparableattention,whichisequivalenttoefficientautoregressiveattention.
• aspecialcaseofadiagonalstatespacemodel.
5.3 StructuredState-SpaceDuality(SSD)
Tosummarizeourresults:
• Structuredstate-spacemodels(Section3)areamodelusuallydefinedthroughalinear-timerecurrence. However,
byexpandingthematrixformulationcharacterizingitslinearsequence-to-sequencetransformation,onecanderive
aquadraticform.
• Attentionvariants(Section4)areamodeldefinedthroughquadratic-timepairwiseinteractions.However,byview-
ingitasafour-waytensorcontractionandreducinginadifferentorder,onecanderivealinearform.
• A natural special case of each one – more precisely, state space models with scalar-identity structure on the 𝐴
matrices,andstructuredmaskedattentionwith1-semiseparablestructureonits𝐿 mask–aredualsofeachother
withtheexactsamelinearandquadraticforms.
Figure4summarizesthedualitybetweenthesetworepresentations.
Anextendedrelatedworkanddiscussion(Section10)describestherelationshipbetweenSSDandgeneralSSMs/attention
inmoredetail.
6 A Hardware-Efficient Algorithm for SSD Models
ThebenefitsofdevelopingthetheoreticalSSDframeworkbetweenSSMs,attention,andstructuredmatricesliesinusing
theconnectionstoimprovethemodelsandalgorithms. Inthissection,weshowhowvariousalgorithmsforcomputing
SSDmodelsefficientlycanbederivedfromvariousalgorithmsforcomputingstructuredmatrixmultiplication.
OurmaincomputationalresultisanalgorithmforcomputingSSDmodelsthatcombinesboththelinear(recurrent)mode
andquadratic(attention)mode.ThisalgorithmisascomputationefficientasSSMs(linearscalinginsequencelength)and
ashardware-friendlyasattention(primarilyusesmatrixmultiplications).
Theorem6.1. ConsideranSSDmodelwithstateexpansionfactorNandheaddimensionP = N. Thereexistsanalgorithm
forcomputingthemodelonanyinput𝑋 ∈R(T,P) whichonlyrequires𝑂(TN2)trainingFLOPs,𝑂(TN)inferenceFLOPs,𝑂(N2)
inferencememory,andwhoseworkisdominatedbymatrixmultiplications.
17StructuredStateSpaceModel StructuredMaskedAttention Structured State Space Model (SSM) Structured
State Space
𝐶 (contractionmatrix) 𝑄 (queries) S4 Diagonal State Space Model Duality (SSD)
𝐵 (expansionmatrix) 𝐾 (keys)
𝑋 (inputsequence) 𝑉 (values) DSS Scalar-Identity SSM
𝐴 𝑗:𝑖 (statematrix) 𝐿 𝑗𝑖 (mask) S4D RetNet GateLoop
N (stateexpansiondim.) N (kernelfeaturedim.) S5 TransNormer
𝐻 (hiddenstates(8b)) S6 Linear Attention
SMAlineardual(15) 1-Semiseparable SMA
=𝐿·𝑋𝐵 (linearmode)
Efficient Autoregressive Attention
𝐺 (Grammatrix(13a)) Semiseparable SMA
SSMquadraticdual(16)
=𝑄·𝐾⊤ (quadraticmode)
Structured Masked Attention (SMA)
Figure 4: (Structured State Space Duality.) State space duality describes the close relationship between state space
modelsandmaskedattention.(Left)GeneralSSMsandSMAbothpossesslinearandquadraticforms,withdirectanalogs
innotation.(Right)SSMsandSMAintersectatalargeclassofstatespacedualmodels(SSD)whichcapturemanysequence
modelsasspecialcases.
Note that all of these bounds are tight, because a state space model with state expansion N operating on a head size of
NhastotalstatesizeN2 (yieldingthelowerboundsfortrainingandinferenceFLOPsof𝑂(TN2) and𝑂(N2) respectively).
Furthermoretheinput𝑋 itselfhasTNelements,yieldingthememorylowerbound.
ThemainideabehindTheorem6.1isonceagainviewingtheproblemofcomputingastatespacemodelasasemiseparable
matrixmultiplication,butleveragingitsstructureinanewway.Insteadofcomputingthewholematrixineitherrecurrent
orattentionmode,weperformablockdecompositionofthematrix. Thediagonalblockscanbecomputedusingthedual
attentionmode,whichcanbeefficientlydonewithmatrixmultiplications,whiletheoff-diagonalblockscanbefactored
bytherank-structureofsemiseparablematricesandreducedtoasmallerrecurrence.WehighlightthatListing1provides
a self-contained implementation of the SSD algorithm. Compared to the general selective SSM of Gu and Dao (2023),
thisimplementationismuchsimpler,andrelativelyefficienteveninnativePyTorchwithoutrequiringspeciallow-level
kernels.
Tobegin,wepartitionthematrix𝑀 intoa T × T gridofsubmatricesofsizeQ×Q,forsomeblocksizeQ. Notethatthe
Q Q
off-diagonalblocksarelow-rankbythedefiningpropertyofsemiseparablematrices(Definition3.1).5
𝑀(0,0)
 
 𝑀(1,0) 𝑀(1,1) 
 
(BlockDecomposition) 𝑀 =   . . . . . . ...  
 
 𝑀(T/Q−1,0) 𝑀(T/Q−1,1) ... 𝑀(T/Q−1,T/Q−1) 
 
(DiagonalBlock) 𝑀(𝑗,𝑗) =SSM(𝐴 𝑗Q:(𝑗+1)Q,𝐵 𝑗Q:(𝑗+1)Q,𝐶 𝑗Q:(𝑗+1)Q)
𝐶⊤𝐴 𝐵⊤𝐴 ⊤
 𝑗Q 𝑗Q:𝑗Q−1   𝑖Q (𝑖+1)Q−1:𝑖Q 
 .   . 
(Low-RankBlock) 𝑀(𝑗,𝑖) =   . .  𝐴 𝑗Q−1:(𝑖+1)Q−1   . .  
𝐶⊤ 𝐴  𝐵⊤ 𝐴 

 (𝑗+1)Q−1
(𝑗+1)Q−1:𝑗Q−1


 (𝑖+1)Q−1
(𝑖+1)Q−1:(𝑖+1)Q−1

Thisiseasiestillustratedthroughanexample,e.g.forT = 9anddecomposingintochunksoflengthQ = 3. Theshaded
5Notethattheblockdecompositionisvalidevenwithpartitionsofvaryingsize,e.g.ifQ∤T,butweassumeevendivisibilityforsimplicity.
18cellsarelow-rankfactorizationsoftheoff-diagonalblocksofthesemiseparablematrix.
𝐶⊤𝐴 𝐵
 0 0:0 0 
 
𝐶⊤𝐴 𝐵 𝐶⊤𝐴 𝐵 
 1 1:0 0 1 1:1 1 
𝐶⊤𝐴 𝐵 𝐶⊤𝐴 𝐵 𝐶⊤𝐴 𝐵 
 2 2:0 0 2 2:1 1 2 2:2 2 
 
𝐶⊤𝐴 𝐵 𝐶⊤𝐴 𝐵 𝐶⊤𝐴 𝐵 𝐶⊤𝐴 𝐵 
 3 3:0 0 3 3:1 1 3 3:2 2 3 3:3 3 
𝑀 =  𝐶 4⊤𝐴 4:0𝐵 0 𝐶 4⊤𝐴 4:1𝐵 1 𝐶 4⊤𝐴 4:2𝐵 2 𝐶 4⊤𝐴 4:3𝐵 3 𝐶 4⊤𝐴 4:4𝐵 4  
 
 𝐶 5⊤𝐴 5:0𝐵 0 𝐶 5⊤𝐴 5:1𝐵 1 𝐶 5⊤𝐴 5:2𝐵 2 𝐶 5⊤𝐴 5:3𝐵 3 𝐶 5⊤𝐴 5:4𝐵 4 𝐶 5⊤𝐴 5:5𝐵 5 

𝐶⊤𝐴 𝐵 𝐶⊤𝐴 𝐵 𝐶⊤𝐴 𝐵 𝐶⊤𝐴 𝐵 𝐶⊤𝐴 𝐵 𝐶⊤𝐴 𝐵 𝐶⊤𝐴 𝐵 
 6 6:0 0 6 6:1 1 6 6:2 2 6 6:3 3 6 6:4 4 6 6:5 5 6 6:6 6 
 
 𝐶 7⊤𝐴 7:0𝐵 0 𝐶 7⊤𝐴 7:1𝐵 1 𝐶 7⊤𝐴 7:2𝐵 2 𝐶 7⊤𝐴 7:3𝐵 3 𝐶 7⊤𝐴 7:4𝐵 4 𝐶 7⊤𝐴 7:5𝐵 5 𝐶 7⊤𝐴 7:6𝐵 6 𝐶 7⊤𝐴 7:7𝐵 7 

𝐶⊤𝐴 𝐵 𝐶⊤𝐴 𝐵 𝐶⊤𝐴 𝐵 𝐶⊤𝐴 𝐵 𝐶⊤𝐴 𝐵 𝐶⊤𝐴 𝐵 𝐶⊤𝐴 𝐵 𝐶⊤𝐴 𝐵 𝐶⊤𝐴 𝐵 
 8 8:0 0 8 8:1 1 8 8:2 2 8 8:3 3 8 8:4 4 8 8:5 5 8 8:6 6 8 8:7 7 8 8:8 8
 
𝐶⊤𝐴 𝐵
 0 0:0 0 
 
𝐶⊤𝐴 𝐵 𝐶⊤𝐴 𝐵 
 1 1:0 0 1 1:1 1 
𝐶⊤𝐴 𝐵 𝐶⊤𝐴 𝐵 𝐶⊤𝐴 𝐵 
 2 2:0 0 2 2:1 1 2 2:2 2 
 
  𝐶 3⊤𝐴 3:2 𝐵 0⊤𝐴 2:0⊤ 𝐶 3⊤𝐴 3:3𝐵 3  
=    𝐶 4⊤𝐴 4:2 𝐴 2:2  𝐵 1⊤𝐴 2:1  𝐶 4⊤𝐴 4:3𝐵 3 𝐶 4⊤𝐴 4:4𝐵 4  
     


 𝐶 5⊤𝐴 5:2

 𝐵 2⊤𝐴 2:2

𝐶 5⊤𝐴 5:3𝐵 3 𝐶 5⊤𝐴 5:4𝐵 4 𝐶 5⊤𝐴 5:5𝐵 5 

        𝐶𝐶 76 ⊤⊤ 𝐴𝐴 76 :: 55   𝐴 5:2    𝐵 𝐵0 1⊤ ⊤𝐴 𝐴2 2: :0 1   ⊤    𝐶 𝐶6 7⊤ ⊤𝐴 𝐴6 7: :5 5   𝐴 5:5    𝐵 𝐵3 4⊤ ⊤𝐴 𝐴5 5: :3 4   ⊤ 𝐶 𝐶6 7⊤ ⊤𝐴 𝐴6 7: :6 6𝐵 𝐵6 6 𝐶 7⊤𝐴 7:7𝐵 7     
   𝐶 8⊤𝐴 8:5   𝐵 2⊤𝐴 2:2   𝐶 8⊤𝐴 8:5   𝐵 5⊤𝐴 5:5  𝐶 8⊤𝐴 8:6𝐵 6 𝐶 8⊤𝐴 8:7𝐵 7 𝐶 8⊤𝐴 8:8𝐵 8 
 
From here we can reduce the problem into these two parts. These can also be interpreted as dividing the output of a
“chunk”𝑦 intotwocomponents:theeffectofinputswithinthechunk𝑥 ,andtheeffectofinputsbeforethe
𝑗Q:(𝑗+1)Q 𝑗Q:(𝑗+1)Q
chunk𝑥 .
0:𝑗Q
6.1 DiagonalBlocks
Thediagonalblocksareeasytohandle, becausetheyaresimplyself-similarproblemsofasmallersize. The 𝑗-thblock
represents computing the answer SSM(𝐴 𝑅,𝐵 𝑅,𝐶 𝑅)(𝑥 𝑅) for the range 𝑅 = 𝑗Q : (𝑗 +1)Q = (𝑗Q,𝑗Q+1,...,𝑗Q+Q−1).
The key is that this block can be computed using any desired method. In particular, for small chunk lengths Q, this
problemiscomputedmoreefficientlyusingthedualquadraticSMAform. Additionally,thechunkscanbecomputedin
parallel.
Thesesubproblemscanbeinterpretedas: whatistheoutputperchunksupposingthattheinitialstate(tothechunk)is0.
Inotherwordsforchunk 𝑗,thiscomputesthecorrectoutputstakingintoaccountonlythechunkinputs𝑥 .
𝑗Q:(𝑗+1)Q
6.2 Low-RankBlocks
The low-rank factorizations consist of 3 terms, and there are correspondingly three pieces of the computation. In this
factorization,wewillusetheterminology
𝐵⊤𝐴 ⊤
 0 2:0
• Thetermslike 𝐵⊤𝐴  arecalledtherightfactorsor𝐵-block-factors.
 1 2:1
 
 𝐵 2⊤𝐴 2:2

• Thetermslike𝐴 arecalledthecenterfactorsor𝐴-block-factors.
5:2
𝐶⊤𝐴
 6 6:5
• Thetermslike 𝐶⊤𝐴  arecalledtheleftfactorsor𝐶-block-factors.
 7 7:5
 
 𝐶 8⊤𝐴 8:5

19Semiseparable Matrix 𝑀
Block Decomposition
DiagonalBlock: Input → Output
Low-Rank Block: Input → State
Outputs 𝑌 Low-Rank Block: State → State
Low-Rank Block: State → Output
States 𝐻
Inputs 𝑋
Figure 5: (SSD Algorithm.) By using the matrix transformation viewpoint of state space models to write them as
semiseparablematrices(Section3),wedevelopamorehardware-efficientcomputationoftheSSDmodelthroughablock-
decomposition matrix multiplication algorithm. The matrix multiplication also has an interpretation as a state space
model,whereblocksrepresentchunkingtheinputandoutputsequence. Diagonalblocksrepresentintra-chunkcompu-
tationsandtheoff-diagonalblocksrepresentinter-chunkcomputations,factoredthroughtheSSM’shiddenstate.
RightFactors. Thisstepcomputesthemultiplicationbytheright𝐵-block-factorsofthelow-rankfactorization. Note
that for each chunk, this is a (N,Q) by (Q,P) matrix multiplication, where N is the state dimension and 𝑃 is the head
dimension. Theresultisa (N,P) tensorforeachchunk,whichhasthesamedimensionalityastheexpandedhiddenstate
ℎ.
This can be interpreted as: what is the final state per chunk supposing that the initial state (to the chunk) is 0. In other
wordsthiscomputesℎ 𝑗Q+Q−1assumingthat𝑥
0:𝑗Q
=0.
CenterFactors. Thisstepcomputestheeffectofthecenter𝐴-block-factorstermsinthelow-rankfactorization.Inthe
previousstep,thefinalstatesperchunkhavetotalshape(T/Q,N,P).Thisisnowmultipliedbya1-SSmatrixgeneratedby
𝐴× ,𝐴× ,...,𝐴× .
2Q−1:Q−1 3Q−1:2Q−1 T−1:T−Q−1
This step can be computed by any algorithm for computing 1-SS multiplication (also known as the scalar SSM scan or
cumprodsumoperator).
Thiscanbeinterpretedas:whatistheactualfinalstateperchunktakingintoaccountallpreviousinputs;inotherwords,
thiscomputesthetruehiddenstateℎ takingintoaccountallof𝑥 .
𝑗Q 0:(𝑗+1)Q
LeftFactors. Thisstepcomputesthemultiplicationbytheleft𝐶-block-factorsofthelow-rankfactorization. Foreach
chunk,thiscanberepresentedbyamatrixmultiplicationcontract(QN,NP→QP).
Thiscanbeinterpretedas: whatistheoutputperchunktakingintoaccountthecorrectinitialstateℎ 𝑗Q−1,andsupposing
theinputs𝑥 𝑗Q:(𝑗+1)Qare0.Inotherwordsforchunk 𝑗,thiscomputesthecorrectoutputstakingintoaccountonlytheprior
inputs𝑥 .
0:𝑗Q
6.3 ComputationalCost
We define the notation BMM(B,M,N,K) to define a batched matrix multiplication contract(MK,KN→MN) with batch di-
mensionB.Fromthisnotationwecaninferthreeaspectsoftheefficiency:
• Computationcost:totalof𝑂(BMNK)FLOPs.
• Memorycost: totalof𝑂(B(MK+KN+MN))space.
20Listing1FullPyTorchexampleofthestatespacedual(SSD)model.
def segsum(x):
"""Naive segment sum calculation. exp(segsum(A)) produces a 1-SS matrix,
which is equivalent to a scalar SSM."""
T = x.size(-1)
x_cumsum = torch.cumsum(x, dim=-1)
x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]
mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)
x_segsum = x_segsum.masked_fill(~mask, -torch.inf)
return x_segsum
def ssd(X, A, B, C, block_len=64, initial_states=None):
"""
Arguments:
X: (batch, length, n_heads, d_head)
A: (batch, length, n_heads)
B: (batch, length, n_heads, d_state)
C: (batch, length, n_heads, d_state)
Return:
Y: (batch, length, n_heads, d_head)
"""
assert X.dtype == A.dtype == B.dtype == C.dtype
assert X.shape[1] % block_len == 0
# Rearrange into blocks/chunks
X, A, B, C = [rearrange(x, "b (c l) ... -> b c l ...", l=block_len) for x in (X, A, B, C)]
A = rearrange(A, "b c l h -> b h c l")
A_cumsum = torch.cumsum(A, dim=-1)
# 1. Compute the output for each intra-chunk (diagonal blocks)
L = torch.exp(segsum(A))
Y_diag = torch.einsum("bclhn,bcshn,bhcls,bcshp->bclhp", C, B, L, X)
# 2. Compute the state for each intra-chunk
# (right term of low-rank factorization of off-diagonal blocks; B terms)
decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))
states = torch.einsum("bclhn,bhcl,bclhp->bchpn", B, decay_states, X)
# 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries
# (middle term of factorization of off-diag blocks; A terms)
if initial_states is None:
initial_states = torch.zeros_like(states[:, :1])
states = torch.cat([initial_states, states], dim=1)
decay_chunk = torch.exp(segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0))))
new_states = torch.einsum("bhzc,bchpn->bzhpn", decay_chunk, states)
states, final_state = new_states[:, :-1], new_states[:, -1]
# 4. Compute state -> output conversion per chunk
# (left term of low-rank factorization of off-diagonal blocks; C terms)
state_decay_out = torch.exp(A_cumsum)
Y_off = torch.einsum('bclhn,bchpn,bhcl->bclhp', C, states, state_decay_out)
# Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks)
Y = rearrange(Y_diag+Y_off, "b c l h p -> b (c l) h p")
return Y, final_state
• Parallelization: largerM,N,Ktermscanleveragespecializedmatrixmultiplicationunitsonmodernaccelerators.
CenterBlocks. ThecostofthequadraticSMAcomputationconsistsofthreesteps(equation(16)):
• Computingthekernelmatrix𝐶⊤𝐵,whichhascostBMM(T/Q,Q,Q,N).
• Multiplyingbythemaskmatrix,whichisanelementwiseoperationontensorsofshape(T/Q,Q,Q).
• Multiplyingbythe𝑋 values,whichhascostBMM(T/Q,Q,P,N)
21Low-RankBlocks:RightFactors. ThisstepisasinglematrixmultiplicationwithcostBMM(T/Q,N,P,Q).
Low-Rank Blocks: Center Factors. This step is a scalar SSM scan (or 1-SS multiplication) of length T/Q on (N,P)
independentchannels.TheworkofthisscanisTNP/Q,whichisnegligiblecomparedtotheotherfactors.
NotethatbecauseoftheblockingwhichreducesthelengthofthesequencefromTtoT/Q,thisscanhasQtimessmallercost
thanapureSSMscan(e.g.theselectivescanofMamba).Thusweobservethatonmostproblemlengths,otheralgorithms
(AppendixB)maybemoreefficientormucheasiertoimplementwithoutasignificantslowdown. Forexample,anaive
implementationofthisvia1-SSmatrixmultiplicationhascostBMM(1,T/Q,NP,T/Q),whichismucheasiertoimplement
andcanbemoreefficientthananaiverecurrence/scanimplementation.
Low-RankBlocks:LeftFactors. ThisstepisasinglematrixmultiplicationwithcostBMM(T/Q,Q,P,N).
TotalCost. IfwesetN=P=Q(inotherwordsthestatedimension,headdimension,andchunklengthareequal),then
allBMMtermsabovebecomeBMM(T/N,N,N,N).Thecomputationalchacteristicsofthisare:
• TotalFLOPcountof𝑂(TN2).
• Totalmemoryof𝑂(TN).
• Theworkconsistsprimarilyofmatrixmultiplicationsonmatricesofshape(N,N).
Notice that the memory consumption is tight; the inputs and outputs 𝑥,𝑦 have shape (T,P) = (T,N). Meanwhile the
flop count reflects an extra factor of N, which is cost incurred by the autoregressive state size and is common to all
models.
Asidefromthematmuls,thereisascalarSSMscanonNP=N2featuresandsequencelengthT/Q.Thishascost𝑂(T/QN2)
FLOPsand𝑂(log(T/Q))depth.Althoughitdoesnotusematrixmultiplications,itisstillparallelizableandthetotalwork
doneisnegligiblecomparedtotheothersteps;thishasanegligiblecostinourGPUimplementation.
ComparisontoPureSSMandAttentionModels. Quadraticattentionisalsoveryhardwareefficientbyonlylever-
agingmatrixmultiplications,buthasT2𝑁 totalFLOPs. Itsslowercomputationspeedatbothtrainingandinferencecan
directlybeseenasaconsequenceofhavingalargerstatesize–standardattentionhasastatesizescalingwithsequence
lengthTbecauseitcachesitshistoryanddoesnotcompressitsstate.
LinearSSMshaveTNP = TN2 totalFLOPs, whichisthesameasSSD.However, anaiveimplementationrequiresastate
expansion (15a) that materializes extra memory, and a scalar operation (15b) that does not leverage matrix multiplica-
tions.
Attention SSM SSD
Statesize T N N
TrainingFLOPs T2N TN2 TN2
InferenceFLOPs TN N2 N2
(Naive)memory T2 TN2 TN
Matrixmultiplication ✓ ✓
Wenotethatmanyothermatrixdecompositionsarepossible(forexample,seeAppendixBforacompendiumofalgorithms
for1-SSmultiplicationthroughdifferentstructuredmatrixdecompositions)whichmayleadtomorealgorithmsforSSDs
thatcouldbebetterforotherspecializedsettings. Evenmorebroadly, wenotethatsemiseparablematriceshavearich
literature and many more representations besides the SSS form that we use (Definition 3.2), and even more efficient
algorithmsmaybepossible.
7 The Mamba-2 Architecture
ByconnectingSSMsandattention,theSSDframeworkallowsustodevelopasharedvocabularyandlibraryoftechniques
for both. In this section we discuss some examples of understanding and modifying SSD layers using ideas originally
22X N
Y
X
SSM
Y
A X BC SSM Linear projection
A X BC
Sequence transformation
! ! ! !
Nonlinearity (activation,
Conv Conv normalization, multiplication)
Sequential Mamba Block Parallel Mamba Block
Figure 6: (Mamba-2 Architecture.) The Mamba-2 block simplifies the Mamba block by removing sequential linear
projections;theSSMparameters𝐴,𝐵,𝐶areproducedatthebeginningoftheblockinsteadofasafunctionoftheSSMinput
𝑋. An additional normalization layer is added as in NormFormer (Shleifer, Weston, and Ott 2021), improving stability.
The𝐵and𝐶 projectionsonlyhaveasingleheadsharedacrossthe𝑋 heads,analogoustomulti-valueattention(MVA).
developed for Transformers. We discuss several design choices, resulting in the Mamba-2 architecture. These axes of
variationareablatedinSection9.4.
7.1 BlockDesign
Wefirstdiscussmodificationstotheneuralnetworkblockthatareindependentoftheinnersequencemixinglayer(i.e.
outsidethecoreSSDlayer).
ParallelParameterProjections. Mamba-1wasmotivatedbyanSSM-centricpointofviewwheretheselectiveSSM
layerisviewedasamapfrom𝑋 ↦→𝑌.TheSSMparameters𝐴,𝐵,𝐶 areviewedassubsidiaryandarefunctionsoftheSSM
input𝑋.Thusthelinearprojectionsdefining(𝐴,𝐵,𝐶)occuraftertheinitiallinearprojectiontocreate𝑋.
InMamba-2,theSSDlayerisviewedasamapfrom𝐴,𝑋,𝐵,𝐶 ↦→𝑌.Itthereforemakessensetoproduce𝐴,𝑋,𝐵,𝐶inparallel
withasingleprojectionatthebeginningoftheblock.Notetheanalogytostandardattentionarchitectures,where𝑋,𝐵,𝐶
correspondtothe𝑄,𝐾,𝑉 projectionsthatarecreatedinparallel.
Notethatadoptingparallelprojectionsforthe𝐴,𝐵,𝐶,𝑋 inputstotheSSMslightlyreducesparametersandmoreimpor-
tantlyismoreamenabletotensorparallelismforlargermodels,byusingstandardMegatronshardingpatterns(Shoeybi
etal.2019)).
ExtraNormalization. Inpreliminaryexperiments,wefoundthatinstabilitieswerepronetoarisinginlargermodels.
Wewereabletoalleviatethisbyaddinganextranormalizationlayer(e.g.LayerNorm,GroupNorm,orRMSNorm)tothe
blockrightbeforethefinaloutputprojection. ThisusageofanormalizationismostdirectlyrelatedtotheNormFormer
architecture (Shleifer, Weston, and Ott 2021), which also added normalization layers at the end of the MLP and MHA
blocks.
WealsonotethatthischangeissimilartootherrecentmodelsrelatedtoMamba-2thatwerederivedfromalinearattention
viewpoint. Theoriginallinearattentionformulationnormalizesbyadenominatortermthatemulatesthenormalization
ofthesoftmaxfunctioninstandardattention.TransNormerLLM(Qin,DongLi,etal.2023)andRetNet(Y.Sunetal.2023)
findthatthisnormalizationisunstableandaddanextraLayerNormorGroupNormafterthelinearattentionlayer. Our
extranormalizationlayerdiffersslightlyfromthese,occuringafterthemultiplicativegatebranchinsteadofbefore.
237.2 MultiheadPatternsforSequenceTransformations
RecallthatSSMsaredefinedasasequencetransformation(Definition2.1)where:
• 𝐴,𝐵,𝐶 parametershaveastatedimensionN.
• TheydefineasequencetransformationRT →RT,whichforexamplecanberepresentedasamatrix𝑀 ∈R(T,T).
• Thistransformationoperatesoveraninputsequence𝑋 ∈R(T,P),independentlyoverthePaxis.
Onecanviewthisasdefiningonehead ofthesequencetransformation.
Definition7.1(Multiheadpatterns). AmultiheadsequencetransformationconsistsofHindependentheads,foratotalmodel
dimensionofD=d_model.Theparametersmaybetiedacrossheads,leadingtoaheadpattern.
ThestatesizeNandheaddimensionPareanalogoustothe𝑄𝐾headdimensionand𝑉 headdimensionofattention,respec-
tively. JustasinmodernTransformerarchitectures(Chowdheryetal.2023;Touvron,Lavril,etal.2023),inMamba-2we
generallychoosethesetobeconstantsaround64or128;whenthemodeldimensionDincreases,weincreasethenumberof
headswhilekeepingtheheaddimensionsNandPfixed.Inordertodescribehowtodothis,wecantransferandgeneralize
ideasfrommultiheadattentiontodefinesimilarpatternsforSSMs,oranygeneralsequencetransformation.
Multi-headSSM Multi-contractSSM Multi-expandSSM Multi-inputSSM
(Multi-headAttn.) (Multi-queryAttn.) (Multi-keyAttn.) (Multi-valueAttn.)
𝑋 (T,H,P) 𝑋 (T,1,P) 𝑋 (T,1,P) 𝑋 (T,H,P)
(17) (18) (19) (20)
𝐴 (T,H) 𝐴 (T,H) 𝐴 (T,H) 𝐴 (T,H)
𝐵 (T,H,N) 𝐵 (T,1,N) 𝐵 (T,H,N) 𝐵 (T,1,N)
𝐶 (T,H,N) 𝐶 (T,H,N) 𝐶 (T,1,N) 𝐶 (T,1,N)
Multihead SSM (MHS) / Multihead Attention (MHA) Pattern. The classic MHA pattern assumes that the head
dimensionPdividesthemodeldimensionD.ThenumberofheadsisdefinedasH=D/P.Then,Hcopiesofthecoresequence
transformationare createdbycreating H independentcopies ofeachparameter. Notethat whiletheMHApattern was
firstdescribedfortheattentionsequencetransformation,itcanbeappliedtoanythingcompatiblewithDefinition2.1.For
example,amulti-headSSDlayerwouldacceptinputswithshapesaccordingtoequation(17)wheretheSSDalgorithmis
broadcastedovertheH=n_headsdimension.
Multi-contractSSM(MCS)/Multi-queryAttention(MQA)Pattern. Multi-queryattention(Shazeer2019)isaclever
optimizationforattentionthatcandramaticallyimprovethespeedofautoregressiveinference,whichreliesoncaching
the𝐾 and𝑉 tensors.Thistechniquesimplyavoidsgiving𝐾 and𝑉 theextraheaddimension,orinotherwordsbroadcasts
asingleheadof(𝐾,𝑉)acrossalltheheadsof𝑄.
Usingthestatespaceduality,wecandefineanequivalentSSMversionofMQAasequation(18).Here,𝑋 and𝐵(theSSM
analogsofattention’s𝑉 and𝐾)aresharedacrosstheHheads.Wealsocallthisthemulti-contractSSM(MCS)headpattern,
becausethe𝐶 parameterwhichcontrolstheSSMstatecontractionhasindependentcopiesperhead.
Wecansimilarlydefineamulti-keyattention(MKA)ormulti-expandSSM(MES)headpattern,where𝐵 (whichcontrols
theSSMexpansion)isindependentperheadwhile𝐶 and𝑋 aresharedacrossheads.
Multi-inputSSM(MIS)/Multi-valueAttention(MVA)Pattern. WhileMQAmakessenseforattentionbecauseof
its KV cache, it is not the natural choice for SSMs. In Mamba, instead,𝑋 is viewed as the main input to the SSM, and
therefore𝐵and𝐶areparametersthataresharedacrosstheinputchannels.Wedefineanewmulti-valueattention(MVA)
of multi-input SSM (MIS) pattern in equation (20), which can again be applied to any sequence transformation such as
SSD.
Armedwiththisvocabulary,wecancharacterizetheoriginalMambaarchitecturemoreprecisely.
Proposition7.2. TheselectiveSSM(S6)layeroftheMambaarchitecture(GuandDao2023)canbeviewedashaving
24• Headdimension𝑃 =1:everychannelhasindependentSSMdynamics𝐴.
• Multi-inputSSM(MIS)or multi-valueattention(MVA)headstructure:the𝐵,𝐶 matrices(correspondingto𝐾,𝑄 inthe
attentionduality)aresharedacrossallchannelsoftheinput𝑋 (correspondingto𝑉 inattention).
WecanalsoablatetheseheadpatternvariantswhenappliedtoSSD(Section9.4.3).Interestingly,despitebeingcontrolled
inparametercountsandtotalstatedimension,thereisanoticeabledifferenceindownstreamperformance.Weempirically
findthattheMVApatternasoriginallyusedinMambaperformsbest.
Grouped Head Patterns. The ideas of multi-query attention can be extended to grouped-query attention (Ainslie et
al. 2023): instead of 1 K and V head, one can create G independent K and V heads, where 1 < G and G divides H. This
is motivated both by bridging the performance difference between multi-query and multi-head attention, and enabling
moreefficienttensorparallelismbysettingGtobeamultipleofthenumberofshards(Section8).
Similarly, the multi-input SSM head pattern used in Mamba-2 can be easily extended to grouped-input SSM (GIS),
or synonymously grouped-value attention (GVA). The generalization is straightforward and we omit the details for
simplicity.
7.3 OtherSSDExtensionsfromLinearAttention
We describe here an example of architectural modifications to SSD motivated by linear attention. We ablate these in
Section 9.4.3 as a form of negative result, finding that they do not significantly improve performance enough to adopt
themasdefaultsettings. Nonetheless,theseillustratehowthevastliteratureonattentioncanbeincorporatedtodefine
variantsofSSD.WetreatthechoiceofkernelfeaturemapasahyperparameterintheMamba-2architecture,andexpect
othersimplemodificationsinspiredbyattentiontobepossibleaswell.
KernelAttentionApproximationstoSoftmaxAttention. Manyvariantsoflinearattentionorkernelattentionare
motivatedbyviewingtheattentionscoressoftmax(𝑄𝐾⊤)ascomposedof
1. An exponential kernel𝑍 = exp(𝑄𝐾⊤), which can be approximated by𝑍 = 𝜓(𝑄)𝜓(𝐾)⊤ for some kernel feature
map.
2. Normalizingthekernelsothatrowssumto1via𝑀 =𝐺/𝐺11⊤,wherethedivisionhappenselementwiseand1is
theall1’svector.
ExponentialKernelFeatureMaps. InMamba-2,weincorporateaflexiblekernelfeaturemap,andapplyittothe𝐵
and𝐶 branches(correspondingtothe𝐾 and𝑉 branchesinattention). Thefeaturemapcanalsobeoptionallyappliedto
the𝑋 (𝑉)branch,forsimplicityandsymmetry. ThisisrepresentedinFigure6byanarbitrarynonlinearity. Bydefault,
wesimplychoose𝜓 tobeanelementwiseSwish/SiLUfunction(HendrycksandGimpel2016;Ramachandran,Zoph,and
Le 2017). We explore other options in the ablations in Section 9.4.3, including feature maps used by Linear Attention,
Performer,RandomFeatureAttention,andcosFormer(Section4.1.3).
IncorporatingaNormalization(Denominator)Term. Tofindthedenominatorterm, wesimplyhavetocompute
𝑀1.Butrecallthatthefinaloutputofthemodelisjust𝑌 =𝑀𝑋 (equation(16)).Sothenormalizationtermscanbefound
simplybyaugmenting𝑋 withanextracolumn1,resultinginatensorofshape(T,P+1).
Notethatinthiscase,thekernelfeaturemap𝜓 mustbepositivesothatthesumispositive.
8 Systems Optimization for SSMs
We describe several systems optimizations for SSMs, in particular the Mamba-2 architecture, for large-scale efficient
trainingandinference. Inparticular,wefocusontensorparallelandsequenceparallelforlarge-scaletraining,asawell
variable-lengthsequencesforefficientfinetuningandinference.
258.1 TensorParallel
Tensorparallelism(TP)(Shoeybietal.2019)isamodelparallelismtechniquethatsplitseachlayer(e.g.,attention,MLP)
to run on multiple accelerators such as GPUs. This technique is widely used to train most large models (Brown et al.
2020; Chowdhery et al. 2023; Touvron, Lavril, et al. 2023; Touvron, L. Martin, et al. 2023) on GPU clusters where each
node typically has 4-8 GPUs with fast networking such as NVLink. TP was originally developed for the Transformer
architecture,anditisnotstraight-forwardtoadaptitotherarchitecture.WefirstshowthechallengeofusingTPwiththe
Mambaarchitecture,andtheshowhowtheMamba-2architectureisdesignedtomakeTPefficient.
Recall the Mamba architecture, with a single input 𝑢 ∈ R𝐿×𝑑 (no batching for simplicity), input projection matrices
𝑊(𝑥),𝑊(𝑧) ∈R𝑑×𝑒𝑑 where𝑒 istheexpansionfactor(typically2),andoutputprojectionmatrix𝑊(𝑜) ∈R𝑒𝑑×𝑑:
𝑥 =𝑢𝑊(𝑥)⊤ ∈R𝐿×𝑒𝑑
𝑧 =𝑢𝑊(𝑧)⊤ ∈R𝐿×𝑒𝑑
𝑥
𝑐
=conv1d(𝑥) ∈R𝐿×𝑒𝑑 (depthwise,independentalong𝑑)
Δ,𝐵,𝐶 =low-rankprojection(𝑥 𝑐)
𝑦 =𝑆𝑆𝑀 𝐴,𝐵,𝐶,Δ(𝑥 𝑐) ∈R𝐿×𝑒𝑑 (independentalong𝑑)
𝑦
𝑔
=𝑦·𝜙(𝑧) (gating,e.g.,with𝜙 beingSiLU)
out=𝑦 𝑔𝑊(𝑜)⊤ ∈R𝐿×𝑑.
WithTP,supposethatwewanttosplitthecomputationalong2GPUs. Itiseasytosplittheinputprojectionmatrices
𝑊(𝑥) and𝑊(𝑧) intotwopartitionseachofsize𝑑 × 𝑒 2𝑑. TheneachGPUwouldholdhalfof𝑥 𝑐 ofsize𝐿× 𝑒 2𝑑. However,
weseethatsinceΔ,𝐵,𝐶 arefunctionsare𝑥 ,sowewouldneedanextraall-reducebetweentheGPUstogetthewhole
𝑐
of 𝑥 before computing Δ,𝐵,𝐶. After that the two GPUs can compute the SSM in parallel since they are independent
𝑐
along𝑑. Attheend,wecansplittheoutputprojectionmatrices𝑊(𝑜) intotwopartitionseachofsize 𝑒𝑑 ×𝑑,anddoan
2
all-reduceattheend.ComparedtoTransformers,wewouldincurtwoall-reducesinsteadofone,doublingthetimespent
incommunication.Forlarge-scaleTransformerstraining,communicationmightalreadytakeasignificantfractionoftime
(e.g.10-20%),anddoublingcommunicationwouldmakeMambanotasefficientforlarge-scaletraining.
With Mamba-2, our goal is to have only one all-reduce per block, similar to attention or MLP blocks in Transformers.
Asaresult,wehavetheprojectiontogetΔ,𝐵,𝐶 directlyfrom𝑢 insteadoffrom𝑥 ,allowingustosplittheseprojection
𝑐
matrices. This implies that we have different sets of Δ,𝐵,𝐶 on different GPUs, which is equivalent to having several
“groups”of Δ,𝐵,𝐶 onalarger“logicalGPU”.Moreover, weuseGroupNormwithineachblock, withnumberofgroups
divisiblebytheTPdegree,sothattheGPUsinaTPgroupdonothaveacommunicatewithintheblock:
𝑥 =𝑢𝑊(𝑥)⊤ ∈R𝐿×𝑒𝑑
𝑧 =𝑢𝑊(𝑧)⊤ ∈R𝐿×𝑒𝑑
Δ,𝐵,𝐶 =projection(𝑢) (oneormoregroupsofΔ,𝐵,𝐶 perGPU)
𝑥
𝑐
=conv1d(𝑥) ∈R𝐿×𝑒𝑑 (depthwise,independentalong𝑑)
𝑦 =𝑆𝑆𝑀 𝐴,𝐵,𝐶,Δ(𝑥 𝑐) ∈R𝐿×𝑒𝑑 (independentalong𝑑)
𝑦
𝑔
=𝑦·𝜙(𝑧) (gating,e.g.,with𝜙 beingSiLU)
𝑦 𝑛 =groupnorm(𝑦 𝑔) (numberofgroupsdivisiblebydegreeoftensorparallel)
out=𝑦 𝑔𝑊(𝑜)⊤ ∈R𝐿×𝑑.
Weseethatweonlyneedtosplittheinputprojectionmatrices,andtheoutputprojectionmatrices,andonlyneedtodo
all-reduce at the end of the block. This is similar to the design of TP for attention and MLP layers. In particular, if we
haveTPdegree2,wewouldsplit𝑊(𝑥) = [𝑊(𝑥),𝑊(𝑥)] with𝑊(𝑥) ∈ R𝑑×𝑒𝑑/2,𝑊(𝑧) = [𝑊(𝑧),𝑊(𝑧)] with𝑊(𝑧) ∈ R𝑑×𝑒𝑑/2,
1 2 𝑖 1 2 𝑖
26All-reduce
&!(') &&(')
Outputs "
GN GN
Y Y States #
A XBC A XBC
Inputs !
GPU1 GPU2 GPU3
&!(#) &!(%) &&(#) &&(%)
Layer Input
Figure7: (ParallelismwiththeMamba-2Block.) (Left: TensorParallelism)Wesplittheinputprojectionmatrices
𝑊(𝑥),𝑊(𝑧) andtheoutputprojectionmatrix𝑊(𝑜). EachSSMhead (𝐴,𝐵,𝐶,𝑋) ↦→ 𝑌 livesonasingledevice. Choosing
GroupNormforthefinalnormalizationlayeravoidsextracommunication.Weneedoneall-reduceperlayer,justlikethe
MLP or attention blocks in a Transformer. (Right: Sequence/Context Parallelism) Analogous to the SSD algorithm,
withmultipledevices,wecansplitalongthesequencedimension. Eachdevicecomputesthestateofitssequence,then
passthatstatetothenextGPU.
(cid:34) 𝑊(𝑜)(cid:35)
and𝑊(𝑜) = 1 with𝑊(𝑜) ∈R𝑒𝑑/2×𝑑.For𝑖 =1,2,theTPMamba-2layercanbewrittenas:
𝑊(𝑜) 𝑖
2
𝑥(𝑖) =𝑢𝑊(𝑥)⊤ ∈R𝐿×𝑒𝑑/2
𝑖
𝑧(𝑖) =𝑢𝑊(𝑧)⊤ ∈R𝐿×𝑒𝑑/2
𝑖
Δ(𝑖),𝐵(𝑖),𝐶(𝑖) =projection(𝑢) (oneormoregroupsofΔ,𝐵,𝐶 perGPU)
𝑥 𝑐(𝑖) =conv1d(𝑥(𝑖)) ∈R𝐿×𝑒𝑑/2
𝑦(𝑖) =𝑆𝑆𝑀 𝐴,𝐵,𝐶,Δ(𝑥 𝑐(𝑖)) ∈R𝐿×𝑒𝑑/2
𝑦 𝑔(𝑖) =𝑦(𝑖) ·𝜙(𝑧(𝑖))
𝑦 𝑛(𝑖) =groupnorm(𝑦 𝑔(𝑖)) (numberofgroupsdivisiblebydegreeoftensorparallel)
out(𝑖) =𝑦 𝑔(𝑖)𝑊 𝑖(𝑜)⊤ ∈R𝐿×𝑑/2
out=∑︁ out(𝑖). (summingoutputsfromallGPUswithanall-reduce)
𝑖
WeillustratetensorparallelwithMamba-2inFigure7(Left).
8.2 SequenceParallelism
For very long sequences, we might need to split the input and activation to different GPUs along the sequence length
dimension.Therearetwomaintechniques:
1. Sequenceparallelism(SP)fortheresidualandnormalizationoperations:firstproposedbyKorthikantietal.(2023),
this technique decomposes the all-reduce in TP as reduce-scatter and all-gather. Noticing that the residual and
normalizationoperationsarerepeatedonthesameinputforallGPUsinthesameTPgroup,SPsplitstheactivations
alongthesequencelengthdimensionbyperforming:reduce-scatter,residualandnormalization,thenall-gather.
SincetheMamba-2architectureusesthesameresidualandnormalizationstructure,SPapplieswithoutmodification.
2. Sequenceparallelismforthetoken-mixingoperations(attentionorSSM),alsoknownas“contextparallelism”(CP).
Severaltechniqueshavebeendevelopedforattentionlayer(e.g.,Ringattention(Liu,Yan,etal.2024;Liu,Zaharia,
27Sequence Length: 256 Sequence Length: 512 Sequence Length: 1024
1.00
0.75 Attention
Based
0.50 Mamba (N=16)
Mamba-2 (N=16)
0.25 Mamba-2 (N=64)
Mamba-2 (N=256)
0.00
32 64 128 256 32 64 128 256 32 64 128 256
Model dimension Model dimension Model dimension
Figure 8: (Multi-Query Associative Recall (MQAR)). Associative recall tasks are challenging for SSMs, which must
memorizeallrelevantinformationintotheirrecurrentstate.TheSSDlayercombinedwithimprovedarchitectureallows
formuchlargerstatesizesinMamba-2,whichperformssignificantlybetterthanMamba-1andevenvanillaattention.
andAbbeel2023)),withsophisticatedload-balancingtechnique(Brandonetal.2023).Thedifficultywithsequence
parallelisminattentionisthatwecansplitqueriesandkeysintoblock,buteachqueryblockneedstointeractwith
keyblocks,leadingtocommunicationbandwidthquadraticinthenumberofworkers.
With SSMs, we can split the sequence in a simple manner: each worker takes an initial state, compute the SSM
withrespecttotheirinputs,returnthefinalstate,andpassthatfinalstatetothenextworker.Thecommunication
bandwidthislinearinthenumberofworkers. Thisdecompositionisexactlythesameastheblock-decomposition
intheSSDalgorithm(Figure5)tosplitintoblocks/chunks.WeillustratethiscontextparallelisminFigure7(Right).
8.3 VariableLength
Whilepretrainingoftenusesthesamesequencelengthsforthebatch, duringfinetuningorinference, themodelmight
need to process different input sequences of different lengths. One naive way to handle this case is to right-pad all
sequencesinthebatchtothemaximumlength, butthiscanbeinefficientifsequencesarewildlydifferentlengths. For
transformers,sophisticatedtechniqueshavebeendeveloptoavoidpaddinganddoload-balancingbetweenGPUs(Zeng
et al. 2022; Y. Zhai et al. 2023), or packing multiple sequences in the same batch and adjust the attention mask (Ding
et al. 2024; Pouransari et al. 2024). With SSMs and Mamba in particular, we can handle variable sequence lengths by
simplytreatingthewholebatchasonelongsequence,andavoidpassingthestatesbetweenindividualsequences.Thisis
equivalenttosimplysetting𝐴
𝑡
=0fortokens𝑡 attheendofonesequencetopreventitfrompassinginformationtothe
token𝑡 +1,whichbelongstoadifferentsequence.
9 Empirical Validation
WeempiricallyevaluateMamba-2onsyntheticrecalltasksthathavebeenchallengingforrecurrentmodels(Section9.1),
andstandardlanguagemodelingpre-traininganddownstreamevaluations(Section9.2). WevalidatethatourSSDalgo-
rithmismuchmoreefficientthanMamba-1(Section9.3)andcomparabletooptimizedattentionformoderatesequence
lengths. Finally,weablatevariousdesignchoicesintheMamba-2architecture(Section9.4).
9.1 Synthetics: AssociativeRecall
Syntheticassociativerecalltaskshavebeenpopularfortestingtheabilityoflanguagemodelstolookupinformationin
theircontext.Broadly,theyinvolvefeedingautoregressivemodelspairsofkey-valueassociations,andthenpromptingthe
modeltoproducethecorrectcompletionuponbeingshownapreviously-seenkey.Themulti-queryassociativerecall
(MQAR)taskisaparticularformulationofthistaskthatrequiresthemodeltomemorizemultipleassociations(Arora,
Eyuboglu, Timalsina, et al. 2024). The original Mamba paper reported results on related synthetic tasks, in particular
SelectiveCopying(GuandDao2023)andInductionHeads(Olssonetal.2022), whichcanbeseenaseasierassociative
recalltasks.TheMQARtaskisalsocloselyrelatedto“phonebooklook-up”taskswhichhasbeenshowntobechallenging
forrecurrentmodelssuchasSSMs,duetotheirfinitestatecapacity(Deetal.2024;Jelassietal.2024).
28
ycaruccA 7 G E P M R K  0 E [ W  S R  8 L I  4 M P I   7 I U Y I R G I  0 I R K X L      
 8 V E R W J S V Q I V  
 1 E Q F E
     1 E Q F E  
     
     
     
     
         
 * 0 3 4 W   P S K  W G E P I 
Figure9:(ScalingLaws.)Modelsofsize≈125𝑀 to≈1.3𝐵parameters,trainedonthePile.Mamba-2matchesorexceeds
theperformanceofMambaaswellasastrong“Transformer++”recipe.ComparedtoourTransformerbaseline,Mamba-2
isParetodominantonperformance(perplexity),theoreticalFLOPs,andactualwall-clocktime.
Table1:(Zero-shotEvaluations.)Bestresultsforeachsizeinbold,secondbestunlined.WecompareagainstopensourceLMswith
varioustokenizers,trainedforupto300Btokens.Pilereferstothevalidationsplit,comparingonlyagainstmodelstrainedonthesame
datasetandtokenizer(GPT-NeoX-20B).Foreachmodelsize,Mamba-2outperformsMamba,andgenerallymatchesPythiaattwicethe
modelsize.FullresultsinTable10.
Model Token. Pile LAMBADA LAMBADA HellaSwag PIQA Arc-E Arc-C WinoGrande OpenbookQA Average
ppl↓ ppl↓ acc↑ acc↑ acc↑ acc↑ acc↑ acc↑ acc↑ acc↑
Pythia-1B NeoX 7.82 7.92 56.1 47.2 70.7 57.0 27.1 53.5 31.4 49.0
Mamba-790M NeoX 7.33 6.02 62.7 55.1 72.1 61.2 29.5 56.1 34.2 53.0
Mamba-2-780M NeoX 7.26 5.86 61.7 54.9 72.0 61.0 28.5 60.2 36.2 53.5
HybridH3-1.3B GPT2 — 11.25 49.6 52.6 71.3 59.2 28.1 56.9 34.4 50.3
Pythia-1.4B NeoX 7.51 6.08 61.7 52.1 71.0 60.5 28.5 57.2 30.8 51.7
RWKV4-1.5B NeoX 7.70 7.04 56.4 52.5 72.4 60.5 29.4 54.6 34.0 51.4
Mamba-1.4B NeoX 6.80 5.04 65.0 59.1 74.2 65.5 32.8 61.5 36.4 56.4
Mamba-2-1.3B NeoX 6.66 5.02 65.7 59.9 73.2 64.3 33.3 60.9 37.8 56.4
HybridH3-2.7B GPT2 — 7.92 55.7 59.7 73.3 65.6 32.3 61.4 33.6 54.5
Pythia-2.8B NeoX 6.73 5.04 64.7 59.3 74.0 64.1 32.9 59.7 35.2 55.7
RWKV4-3B NeoX 7.00 5.24 63.9 59.6 73.7 67.8 33.1 59.6 37.0 56.4
Mamba-2.8B NeoX 6.22 4.23 69.2 66.1 75.2 69.7 36.3 63.5 39.6 59.9
Mamba-2-2.7B NeoX 6.09 4.10 69.7 66.6 76.4 69.6 36.4 64.0 38.8 60.2
SSD, Scan, Convolution vs Attention time (A100 80GB PCIe) SSD vs Scan time (A100 80GB PCIe)
FlashAttention-2 FlashAttention-2
1000 Convolution 7 Scan (Mamba)
Scan (PyTorch), state dim 64 SSD (ours)
Scan (Mamba), state dim 64 6
100 SSD (ours), state dim 64 5
10 4
3
1
2
0.1 1
0
512 1k 2k 4k 8k 16k 32k 64k 128k 256k 512k 4816 32 64 128 256
Sequence length State dim
Figure10:(EfficiencyBenchmarks.) (Left)OurSSDis2−8×fasterthanaMambafusedscanforlargestateexpansion
(𝑁 =64)andfasterthanFlashAttention-2forsequencelength2kandabove.(Right)Sequencelength4K:Increasingstate
expansionslowsdowntheMambaoptimizedscanimplementationlinearly.SSDcanhandlemuchlargerstateexpansion
factorswithoutmuchslowdown.
29
)sm(
emiT
  I P E G W  K S P   ] X M \ I P T V I 4
)sm(
emiTWecompareonachallengingversionoftheMQARsetupfrom(Arora,Eyuboglu,Zhang,etal.2024),usingahardertask,
longersequences,andsmallermodels. Ourbaselinesincludestandardmulti-headsoftmaxattentionaswellastheBased
architecturewhichcombinesconvolutions,localattention,andalinearattentionvariant.
ResultsareshowninFigure8. WhileMamba-1strugglesonthistask,Mamba-2performswellacrossallsettings. Sur-
prisingly,itissignificantlybetterthanMamba-1evenwhenthestatesizesarecontrolled(N=16).(Wearenotsurewhich
aspectofthearchitectureisthepredominantfactor,whichremainsaquestiontoexploreinfuturework.) Additionally,
this task validates the importance of state size: increasing from N = 16 to N = 64 and N = 256 consistently improves
performanceonMQAR,asthelargerstateallowsmoreinformation(key-valuepairs)tobememorized.
9.2 LanguageModeling
FollowingstandardprotocolsinLLMs,wetrainandevaluatetheMamba-2architectureonstandardautoregressivelan-
guagemodelingagainstotherarchitectures.Wecomparebothpretrainingmetrics(perplexity)andzero-shotevaluations.
Themodelsizes(depthandwidth)followGPT3specifications,from125mto2.7B.WeusethePiledataset(L.Gao,Bider-
man,etal.2020),andfollowthetrainingrecipedescribedinBrownetal.(2020). Thisfollowsthesamesetupasreported
inMamba(GuandDao2023);trainingdetailsareinAppendixD.
9.2.1 ScalingLaws
Forbaselines,wecompareagainstbothMambaanditsTransformer++recipe(GuandDao2023),whichisbasedonthe
PaLMandLLaMaarchitectures(e.g.rotaryembedding,SwiGLUMLP,RMSNorminsteadofLayerNorm,nolinearbias,and
higherlearningrates). AsMambahasalreadydemonstratedthatitoutperformsthestandardTransformerarchitecture
(GPT3architecture)aswellasrecentsubquadraticarchitectures(H3(Dao,D.Y.Fu,etal.2023),Hyena(Polietal.2023),
RWKV-4(B.Peng,Alcaide,etal.2023),RetNet(Y.Sunetal.2023)),weomitthoseintheplotforclarity(seeGuandDao
(2023)forcomparisons).
Figure9showsscalinglawsunderthestandardChinchilla(Hoffmannetal.2022)protocol, onmodelsfrom≈ 125𝑀 to
≈1.3𝐵parameters.
9.2.2 DownstreamEvaluations
Table 1 shows the performance of Mamba-2 on a range of popular downstream zero-shot evaluation tasks, compared
tothemostwell-knownopensourcemodelsatthesesizes, mostimportantlyPythia(Bidermanetal.2023)whichwere
trainedwiththesametokenizer,dataset,andtraininglength(300Btokens)asourmodels.
9.2.3 HybridModels:CombiningSSDLayerwithMLPandAttention
Recentandconcurrentwork(Dao,D.Y.Fu,etal.2023;Deetal.2024;Gloriosoetal.2024;Lieberetal.2024)suggeststhata
hybridarchitecturewithbothSSMlayersandattentionlayerscouldimprovethemodelqualityoverthatofaTransformer,
orapureSSM(e.g.,Mamba)model,especiallyforin-contextlearning.WeexplorethedifferentwaysthatSSDlayerscan
becombinedwithattentionandMLPtounderstandthebenefitsofeach. Empiricallywefindthathavingaround10%of
thetotalnumberoflayersbeingattentionperformsbest. CombiningSSDlayers, attentionlayers, andMLPalsoworks
betterthaneitherpureTransformer++orMamba-2.
SSD and Attention We find that SSD and attention layers are complementary: by themselves (e.g. in the Mamba-2
architecture vs. Transformer++) their performance (measured by perplexity) is nearly the same, but a mixture of SSD
and attention layers outperforms the pure Mamba-2 or Transformer++ architecture. We show some results (Table 2)
forthe350Mmodel(48layers)trainedto7BtokensonthePilewiththeGPT-2tokenizer(samenumberofparameters,
same hyperparameters, same training and validation set). Adding in just a few attention layers already yields notable
improvementandstrikesthebestbalancebetweenqualityandefficiency. WehypothesizethattheSSMlayersfunction
well as a general sequence-to-sequence mapping, and attention layers act as a retrieval mechanism to quickly refer to
previoustokensinthesequenceinsteadofforcingthemodeltocompressallthecontexttoitsmemory(SSMstates).
30Table2:(CombiningSSDandAttentionBlocks.)Perplexityofa350Mmodelwith48layers,withdifferentnumberof
attentionlayers.Havingarounda10%ratioofattentionlayersperformsbest.
Num.AttnBlocks 0(Mamba-2) 1 2 3 4 5 6 7 9 11 15 24 Transformer++
Perplexity↓ 8.60 8.38 8.32 8.29 8.29 8.28 8.26 8.27 8.28 8.30 8.34 8.50 8.68
HybridModelswithSSD,MLP,andAttention WecomparedifferentwaysthatSSDcanbecombinedwiththe(gated)
MLPandattentionlayers,andevaluateatthe2.7Bscale(64layers),trainedto300BtokensonthePile(samenumberof
parameters,samehyperparameters,sametrainingandvalidationset,samedataorder):
1. Transformer++:32attentionlayersand32gatedMLP,interleaving.
2. Mamba-2:64SSDlayers.
3. Mamba-2-MLP:32SSDand32gatedMLPlayers,interleaving.
4. Mamba-2-Attention:58SSDlayersand6attentionlayers(atindices9,18,27,36,45,56)6.
5. Mamba-2-MLP-Attention:28SSDlayersand4attentionlayers,interleavingwith32gatedMLPlayers.
We report the validation perplexity on the Pile, as well as zero-shot evaluation, in Table 3. In general, the quality of
Transformer++andMamba-2modelsarearoundthesame.Weseethataddingjust6attentionlayersnoticeablyimproves
overthepureMamba-2model(andoverTransformer++).AddingMLPlayersreducesmodelquality,butcan(i)speedup
training and inference due to the simplicity and hardware-efficiency of the MLP layer (ii) be easier to up-cycle to MoE
modelsbyreplacingMLPlayerswithmixture-of-experts.
Table3: (Zero-shotEvaluations.) Bestresultsforeachsizeinbold. WecomparedifferentwaysSSD,MLP,andattentionlayerscan
becombined,evaluatedat2.7Bscaletrainedto300BtokensonthePile.
Model Token. Pile LAMBADA LAMBADA HellaSwag PIQA Arc-E Arc-C WinoGrande OpenbookQA Average
ppl↓ ppl↓ acc↑ acc↑ acc↑ acc↑ acc↑ acc↑ acc↑ acc↑
Transformer++ NeoX 6.13 3.99 70.3 66.4 75.2 67.7 37.8 63.9 40.4 60.2
Mamba-2 NeoX 6.09 4.10 69.7 66.6 76.4 69.6 36.4 64.0 38.8 60.2
Mamba-2-MLP NeoX 6.13 4.18 69.3 65.0 76.4 68.1 37.0 63.1 38.2 59.6
Mamba-2-Attention NeoX 5.95 3.85 71.1 67.8 75.8 69.9 37.8 65.3 39.0 61.0
Mamba-2-MLP-Attention NeoX 6.00 3.95 70.0 66.6 75.4 70.6 38.6 64.6 39.2 60.7
9.3 SpeedBenchmarks
WebenchmarkthespeedoftheSSDalgorithmagainstMamba’sscanimplementationandFlashAttention-2(Figure10).
SSD,thankstoitsreformulationtousematrixmultiplicationasasubroutine,canexploitspecializedmatrixmultiplication
(matmul)unitsonGPUs,alsoknownastensorcores. Asaresult,itis2-8×fasterthanMamba’sfusedassociativescan,
whichdoesnotleveragematmulunits. Duetoitslinearscalinginsequencelength,SSDisfasterthanFlashAttention-2
startingatsequencelength2𝐾.
However,wenotethattheMamba-2modelasawholemightnotbeasefficienttotrainasTransformeratshortsequence
length(e.g.at2𝐾),sinceaTransformerwith𝐿layerswouldhave 𝐿 MLPlayersand 𝐿 attentionlayers,whileaMamba-2
2 2
modelwouldhave𝐿SSDlayersforthesamenumberofparameters.GenerallytheMLPlayersareveryhardwareefficient
sincetheyconsistofsimplematrixmultiplicationandpointwiselinearity.AsshowninSection9.2.3,onecanalsocombine
𝐿 SSDlayersand 𝐿 MLPlayerstospeeduptrainingatshortsequencelength.
2 2
6Insmall-scaleexperiments,wefindthataslongastheattentionlayersarespacedout,notattheverybeginningorattheveryend,themodel
qualitydoesnotdependverymuchontheexactlocationoftheattentionlayers.
31Table 4: (Ablations: Mamba-2 block.) We ablate the major differences between the Mamba-2 and Mamba-1 neural
networkblocks(Figure6,Section7.1). Notethatthesecomponentsareindependentoftheinnersequencemixinglayer;
intheseablations,weuseSSDfortheinnerSSMlayer(differingfromtheS6layerofMamba-1).
Block 𝐴𝐵𝐶𝑋 Projections ExtraNormalization Parameters Perplexity
Mamba-1 Sequential ✗ 129.3M 11.76
Sequential ✓ 129.3M 11.54
Parallel ✗ 126.5M 11.66
Mamba-2 Parallel ✓ 126.5M 11.49
9.4 ArchitectureAblations
9.4.1 BlockDesign
Section7.1introducestheMamba-2block,whichhassmallmodificationstotheMamba-1blockwhicharepartlymotivated
bytheconnectiontoattentionandalsotoimprovethescalabilityofMamba-2.Table4ablatesthesearchitecturechanges
totheblock,whichoccuroutsideofthecoreSSMlayer.
Theablationsvalidatethatparallelprojectionstocreate (𝐴,𝐵,𝐶,𝑋) savesparametersandperformsslightlybetterthan
Mamba’s sequential projections. More importantly, this modification is amenable to tensor parallelism at larger model
sizes(Section8). Additionally,theextranormalizationlayeralsoslightlyimprovesperformance. Moreimportantly,pre-
liminaryexperimentsatlargerscalesobservedthatitalsohelpswithtrainingstability.
9.4.2 HeadStructure
Section 7.2 describes how the dimensions of the 𝐵,𝐶,𝑋 projections can be viewed as a hyperparameter analogous to
notions of multi-head attention and multi-query attention. We also showed how the original Mamba architecture is
analogous to multi-value attention (Proposition 7.2), which was a choice that naturally developed from the state-space
modelpointofviewandwasnotpreviouslyablated.
Table 5 ablates choices of the multi-head structure for the Mamba-2 architecture. Strikingly, we find a large difference
between multi-value and multi-query or multi-key head patterns, despite seeming very similar. Note that this is not
explainedbythetotalstatesize,whichisthesameforallofthem(equaltoHPNortheproductofthenumberofheads,
headdimension,andstatedimension).
Wealsocomparetomulti-headpatternswherethenumberof𝐶,𝐵,𝑋 (analogousto𝑄,𝐾,𝑉)headsisequal. Wecompare
against the standard multi-head pattern, as well as one with aggressive sharing where they all have only 1 head. Note
thatinthelattercase,themodelstillhasHdifferentsequencemixers𝑀,becauseeachheadstillhasadifferent𝐴. When
parameter matched, these multi-head patterns perform similarly to each other, in between the MVA and MQA/MKA
patterns.
9.4.3 AttentionKernelApproximations
Section 7.3 noted how SSD can be combined with ideas from the linear attention literature, such as various forms of
kernel approximations. We ablate several variants of these suggested by previous works in Table 6. These include the
cosFormer(Qin,WeixuanSun,etal.2022),RandomFeatureAttentionH.Pengetal.2021,andPositiveRandomFeatures
(Performer)(Choromanskietal.2021).
Wealsoablateaddinganormalizationterm,akintothedenominatorofthesoftmaxfunctioninstandardattention. We
foundthatthisintroducedinstabilitiestomostvariants,butslightlyimprovedperformancefortheReLUactivationfunc-
tion𝜓.
Table7alsotestsmorerecentproposalstoimprovelinearattentionthatinvolveexpandingthefeaturedimension(Based(Arora,
Eyuboglu, Zhang, etal.2024)andReBased(Aksenovetal.2024)). Theselinearattentionextensionsaimtoappropriate
theexpkernelwithaquadraticapproximation.ReBasedalsoproposestoreplacetheQKactivationfunctionwithalayer
normalization; from an SSM-centric view we apply a normalization on top of (𝐵,𝐶) before applying the SSM function.
32Table5: (Ablations: Multi-headstructure.) Allmodelshavestateexpansionfactor𝑁 = 64andheadsize𝑃 = 64and
aretrainedtoChinchillascalinglawtokencounts. Thenumberof𝐴headsisalwaysequaltothetotalheadsH,i.e. each
headhasaseparateinput-dependent𝐴decayfactor.(Top)125Mmodels,2.5Btokens(Bottom)360Mmodels,7Btokens
SSMHeadPattern Attn.Analog 𝐴heads 𝐵heads 𝐶heads 𝑋 heads Layers Params Ppl.
Multi-input(MIS) Multi-value(MVA) 24 1 1 24 24 126.5M 11.66
Multi-contract(MCS) Multi-query(MQA) 24 1 24 1 24 126.5M 12.62
Multi-expand(MES) Multi-key(MKA) 24 24 1 1 24 126.5M 12.59
Multi-head(MHS) Multi-head(MHA) 24 24 24 24 15 127.6M 12.06
Multi-state(MSS) - 24 1 1 1 36 129.6M 12.00
Multi-input(MIS) Multi-value(MVA) 32 1 1 32 48 361.8M 8.73
Multi-contract(MCS) Multi-query(MQA) 32 1 32 1 48 361.8M 9.33
Multi-expand(MES) Multi-key(MKA) 32 32 1 1 48 361.8M 9.36
Multi-head(MHS) Multi-head(MHA) 32 1 1 1 70 361.3M 9.01
Multi-state(MSS) - 32 32 32 32 29 357.3M 9.04
Table6:(Ablations:Kernelapproximations.)Wetestvar- Table 7: (Ablations: Kernel approximations.) We
iousproposalsforthekernelactivationfunction𝜓,including testthe(Re)Basedmethodsforlinearattentionapprox-
linearattentionvariantsaimingtoapproximatetheexpker- imations,whichinvolveexpandedfeaturemaps. (Top)
nelfromstandardsoftmaxattention. 130Mmodels.(Top)380Mmodelswith𝑁 =256.
Kernelactivation𝜑 Perplexity Kernelactivation𝜑 Perplexity
none 11.58 Swish 11.67
Swish 11.66 Swish+Taylor(Based) 12.19
Exp 11.62 LayerNorm 11.50
ReLU 11.73 LayerNorm+Square(ReBased) 11.84
ReLU+normalization 11.64
Swish 8.58
cosFormer 11.97 Swish+Taylor(Based) 8.71
RandomFeatureAttention 11.57 LayerNorm 8.61
PositiveRandomFeatures(Performer) 12.21 LayerNorm+Square(ReBased) 8.63
Wenotethatthistechniquehasbeenindependentlyproposedasthe“QK-Norm”forsoftmaxattention(Team2024)and
an“internalnormalization”forMamba(Lieberetal.2024).
Overall,Table6andTable7foundthatthekernelapproximationmethodswetrieddidnotseemtoimproveoversimple
pointwisenon-linearactivationfunctionsfor𝜓. ThusourdefaultsettingsforMamba-2used𝜓(𝑥) = Swish(𝑥) tofollow
Mamba-1, but we suggest that removing this activation entirely may be a simpler choice that we did not extensively
test.
WeemphasizehoweverthatSSDandvanillalinearattentiondifferintheinclusionofthe1-semiseparablemask𝐿,while
thevariouslinearattentionmethodsintheliteraturewerederivedtoapproximatesoftmaxattentionwithoutthisterm;
thus,ournegativeresultsmaybenotunexpected.
10 Related Work and Discussion
ThestatespacedualityframeworkbridgesconnectionsbetweenSSMs,structuredmatrices,andattention. Wediscussin
moredepththerelationsbetweenSSDandtheseconceptsmorebroadly.Usingideasfromeachoftheviewpoints,wealso
suggestsomedirectionsthattheSSDframeworkcanbeextendedinfuturework.
10.1 StateSpaceModels
Structuredstatespacemodelscanbecharacterizedalongtheaxes
33(i) whetheritistime-invariantortime-varying.
(ii) thedimensionalityofthesystem.
(iii) thestructureontherecurrenttransitions𝐴.
SSDcanbedescribedasaselectiveSSMwithSISOdimensionsandscalar-identitystructure.
TimeVariance(Selectivity). TheoriginalstructuredSSMs(S4)werelineartime-invariant(LTI)systems(Gu2023;Gu,
Goel,andRé2022)motivatedbycontinuous-timeonlinememorization(Gu,Dao,etal.2020;Gu,Johnson,Goel,etal.2021;
Gu,Johnson,Timalsina,etal.2023).ManyvariantsofstructuredSSMshavebeenproposed(Dao,D.Y.Fu,etal.2023;Gu,
Gupta,etal.2022; Gupta,Gu,andBerant2022; Maetal.2023; J.T.Smith,Warrington,andLinderman2023),including
severalthatdroptherecurrenceandfocusontheconvolutionalrepresentationofLTISSMs(D.Y.Fuetal.2023;Y.Lietal.
2023;Polietal.2023;Qin,Han,WeixuanSun,B.He,etal.2023).
SSDisatime-varyingstructuredSSM,alsoknownasaselectiveSSMintroducedinMamba(GuandDao2023).Selective
SSMs are closely related to gating mechanisms of RNNs, including classical RNNs such as the LSTM (Hochreiter and
Schmidhuber1997)andGRU(J.Chungetal.2014)aswellasmoremodernvariantssuchastheQRNN(Bradburyetal.
2016), SRU (Lei 2021; Lei et al. 2017), RWKV (B. Peng, Alcaide, et al. 2023), HGRN (Qin, Yang, and Zhong 2023), and
Griffin(Botevetal.2024;Deetal.2024).TheseRNNsdifferintheirparameterizationsinvariousways,mostimportantly
inthelackofastateexpansion.
DimensionalityandStateExpansion. AnimportantcharacteristicofSSD,sharedbypreviousSSMsinitslineage(S4,
H3,Mamba),isthatitisasingle-inputsingle-output(SISO)systemwhereinputchannelsareprocessedindependently.
ThisleadstoamuchlargereffectivestatesizeofNDwhereNistheSSMstatesize(alsocalledstateexpansionfactor)and
D is the standard model dimension. Traditional RNNs either have N = 1 or are multi-input multi-output (MIMO) with
dense𝐵,𝐶 matrices,eitherofwhichleadstoasmallerstate. WhileMIMOSSMshavebeenshowntoworkwellinsome
domains (Lu et al. 2023; Orvieto et al. 2023; J. T. Smith, Warrington, and Linderman 2023), Mamba showed that state
expansioniscrucialforinformation-densedomainssuchaslanguage.OneofthemainadvantagesofSSDisallowingfor
evenlargerstateexpansionfactorswithoutslowingdownthemodel. Manysubsequentworkshavesinceadoptedstate
expansion(Section10.4).
Structure. ComparedtopreviousstructuredSSMs,themainrestrictionofSSDisontheexpressivityofthestatetran-
sitions𝐴 .WenotethatmoregeneralSSMs,suchasthecaseofdiagonal𝐴 ,havethesametheoreticalefficiencyasSSD,
𝑡 𝑡
butarelesshardware-friendly.Thisisbecausethedualquadraticformlosesitsattention-likeinterpretationandbecomes
moredifficulttocompute. ThuscomparedtoMamba,SSDdiffersonlyinaslightlymorerestrictiveformofdiagonal𝐴 ,
𝑡
andtradesoffthisexpressivityforimprovedhardwareefficiency(andeaseofimplementation).
WehypothesizethatitmaybepossibletorefineourstructuredmatrixalgorithmstoimprovetothegeneraldiagonalSSM
caseaswell.
10.2 StructuredMatrices
Thefirstviewpointofthestatespacedualityadoptstheviewpointofthesemodelsasmatrixsequencetransformations
or“matrixmixers”:sequencetransformations(Definition2.1)thatcanberepresentedasmatrixmultiplication(byaT×T
matrix)alongthesequencedimensionT.
Severalsuchmatrixmixershavebeenproposedbefore,wheretheprimaryaxisofvariationistherepresentationofthe
matrix. These include MLP-Mixer (Tolstikhin et al. 2021) (unstructured matrix), FNet (Lee-Thorp et al. 2021) (Fourier
Transformmatrix),M2(Dao,B.Chen,etal.2022;Dao,Gu,etal.2019;Dao,Sohoni,etal.2020;D.Fuetal.2024)(butter-
fly/monarchmatrix),Toeplitzmatrices(Polietal.2023;Qin,Han,WeixuanSun,B.He,etal.2023),andevenmoreexotic
structures(DeSaetal.2018;Thomasetal.2018).
Animportantcharacterizationisthatefficient(sub-quadratic)matrixsequencetransformationsareexactlythosewhich
have structured matrix mixers. A core result of the SSD framework is viewing SSMs as matrix mixers with a particular
structure–semiseparablematrices(Section3). Thelinearvs. quadraticdualitythentakestheformofstructuredmatrix
multiplicationvs.naivematrixmultiplication.
34ThestructurematrixrepresentationledtoourefficientSSDalgorithmthroughblockdecompositionsofparticularsemisep-
arablematrices(Section6). Wenotethatsemiseparablematricesarewell-studiedinthescientificcomputingliterature,
andincorporatingthoseideasmaybeapromisingavenueformoreimprovementstostatespacemodels.Wealsosuggest
thatfocusingonthematrixmixerviewpointcanleadtomorefruitfuldirectionsforsequencemodels,suchasdesigning
principlednon-causalvariantsofMamba,orfindingwaystocharacterizeandbridgethegapbetweensoftmaxattention
andsub-quadraticmodelsthroughanalyzingtheirmatrixtransformationstructure.
10.3 (Linear)Attention
Comparedtostandard(causal)attention,SSDhasonlytwomaindifferences.
First,SSDdoesnotusethesoftmaxactivationofstandardattention(Bahdanau,Cho,andBengio2015;Vaswanietal.2017),
whichiswhatgivesattentionitsquadraticcomplexity.Whenthesoftmaxisdropped,thesequencecanbecomputedwith
linearscalingthroughthelinearattentionframework(Katharopoulosetal.2020).
Second,SSDmultipliesthelogitsmatrixbyaninput-dependent1-semiseparablemask. Thusthismaskcanbeviewedas
replacingthesoftmaxinstandardattention.
Thissemiseparablemaskcanalsobeviewedasprovidingpositionalinformation.Theelements𝑎 actas“gates”intheRNN
𝑡
sense,ora“selection”mechanism(seediscussioninMambapaper),andtheircumulativeproducts𝑎 controlhowmuch
𝑗:𝑖
interactionisallowedbetweenpositions𝑖and 𝑗.Positionalembeddings(e.g.sinusoidal(Vaswanietal.2017),AliBi(Press,
N.Smith,andLewis2022),andRoPE(Suetal.2021))areanimportantcomponentofTransformersthatareoftenviewed
asheuristics,andthe1-SSmaskofSSDcanbeseenasamoreprincipledformofrelativepositionalembeddings.Wenote
thatthisviewwasalsopositedconcurrentlybyGateLoop(Katsch2023).
The second viewpoint of state space duality is a special case of our more general structured masked attention (SMA)
framework,wherethedualityisrevealedasdifferentcontractionorderingsonasimple4-waytensorcontraction. SMA
isastronggeneralizationoflinearattentionthatismuchmoregeneralthanSSDaswell;otherformsofstructuredmasks
mayleadtomorevariantsofefficientattentionwithdifferentpropertiesthanSSD.
Besideleadingtonewmodels, theseconnectionstoattentioncanleadtootherdirectionsforunderstandingSSMs. For
example,wearecuriouswhetherthephenomenonofattentionsinks(Darcetetal.2024;Xiaoetal.2024)existforMamba
models, and more broadly whether interpretability techniques can be transferred to SSMs (Ali, Zimerman, and Wolf
2024).
Finally, many other variants of linear attention have been proposed (Arora, Eyuboglu, Timalsina, et al. 2024; Arora,
Eyuboglu,Zhang,etal.2024;Choromanskietal.2021;H.Pengetal.2021;Qin,Han,WeixuanSun,DongxuLi,etal.2022;
Qin, WeixuanSun, etal.2022; Schlag, Irie, andSchmidhuber2021; Zhangetal.2024; Zheng, C.Wang, andKong2022)
(see Section 4.1.3 for descriptions of several of these), and we expect that many techniques can be transferred to SSMs
(e.g.Section7.3).
WeemphasizethatSSDdoesnotgeneralizestandardsoftmaxattention,oranyothertransformationontheattention
kernel matrix that does not have a finite feature map𝜓. Compared to general attention, SSD’s advantage is having a
controllablestateexpansionfactorNthatcompressesthehistory, comparedtoquadraticattention’scacheoftheentire
historyscalingwithsequencelengthT≫N.Concurrentworkhasstartingstudyingthetradeoffsoftheserepresentations,
forexampleoncopyingandin-contextlearningtasks(Akyüreketal.2024;Grazzietal.2024;Jelassietal.2024;Parketal.
2024). We note that Mamba-2 significantly improves on Mamba on some of these capabilities (e.g. as demonstrated by
MQARresultsinSection9.1),butmoreremainstobeunderstood.
10.4 RelatedModels
Wefinallyhighlightagrowingbodyofrecentandconcurrentworkthathavedevelopedsequencemodelsverysimilarto
MambaandMamba-2.
• RetNet (Y. Sun et al. 2023) and TransNormerLLM (Qin, Dong Li, et al. 2023) generalize Linear Attention using decay
terms instead of a cumulative sum, and propose dual parallel/recurrent algorithms as well as a hybrid “chunkwise”
mode. These algorithms can be seen as an instantiation of SSD where𝐴 is time-invariant (constant for all𝑡); in the
𝑡
SMAinterpretation,themaskmatrix𝐿wouldbeadecaymatrix𝐿
𝑖,𝑗
=𝛾𝑖−𝑗. Thesemodelsalsodifferarchitecturallyin
35variousways.Forexample,sincetheywerederivedfromanattention-centricperspectivetheypreservethemulti-head
attention(MHA)pattern;sinceMamba-2wasderivedfromanSSM-centricpatternitpreservesthemulti-valueattention
(MVA)ormulti-expandSSM(MES)pattern,whichweshowtobebetter(Section9.4).
• GateLoop(Katsch2023)concurrentlyproposedusinginput-dependentdecayfactors𝐴 ,anddevelopedthesamedual
𝑡
quadraticformasinSSDwhichtheycalla“surrogateattention”form.
• GatedLinearAttention(GLA)(Yangetal.2024)proposedavariantoflinearattentionwithdata-dependentgates,along
withefficientalgorithmstocomputeachunkwisemodeandhardware-awareimplementations.
• HGRN(Qin,Yang,andZhong2023)introducedanRNNwithinput-dependentgates,whichwasimprovedtoincorporate
stateexpansioninHGRN2(Qin,Yang,WeixuanSun,etal.2024).
• Griffin (De et al. 2024) and RecurrentGemma (Botev et al. 2024) showed that an RNN with input-dependent gating,
combined with local attention, can be very competitive with strong modern Transformers. Jamba also showed that
combiningMambawithafewlayersofattentionperformsverywellonlanguagemodeling(Lieberetal.2024).
• xLSTM(Becketal.2024)improvesthexLSTMbyadoptingtheideaofstateexpansionandothergating,normalization,
andstabilizationtechniques.
• RWKV(-4)(B.Peng,Alcaide,etal.2023)isanRNNbasedonadifferentlinearattentionapproximation(theattention-free
Transformer(S.Zhaietal.2021)). IthasrecentlybeenimprovedtotheRWKV-5/6(EagleandFinch)architectures(B.
Peng,Goldstein,etal.2024)byadoptingtheideasofselectivityandstateexpansion.
11 Conclusion
Weproposedatheoreticalframeworkbasedonwell-studiedclassesofstructuredmatricesthatbridgestheconceptualgap
betweenSSMsandattentionvariants.ThisframeworkyieldsinsightsonhowrecentSSMs(e.g.Mamba)performaswellas
Transformersonlanguagemodeling.Moreover,ourtheoreticaltoolsprovidenewideastoimproveSSMs(andpotentially
Transformers)byconnectingthe algorithmicandsystemsadvancesonbothsides. As ademonstration, theframework
guidesourdesignofanewarchitecture(Mamba-2)attheintersectionofSSMsandstructuredattention.
Acknowledgments
WethankAngelaWuforthesuggestiononhowtoefficientlycomputethegradientofΔinanumericallystablemanner.
WethankSukjunHwangandAakashLahotiforassistancewiththeMQARexperiments.
References
[1] JoshuaAinslie,JamesLee-Thorp,MichieldeJong,YuryZemlyanskiy,FedericoLebrón,andSumitSanghai.“GQA:
TrainingGeneralizedMulti-QueryTransformerModelsfromMulti-HeadCheckpoints”.In:arXivpreprintarXiv:2305.13245
(2023).
[2] YaroslavAksenov,NikitaBalagansky,SofiaMariaLoCiceroVaina,BorisShaposhnikov,AlexeyGorbatovski,and
DaniilGavrilov.“LinearTransformerswithLearnableKernelFunctionsareBetterIn-ContextModels”.In:arXiv
preprintarXiv:2402.10644(2024).
[3] Ekin Akyürek, Bailin Wang, Yoon Kim, and Jacob Andreas. “In-Context Language Learning: Architectures and
Algorithms”.In:TheInternationalConferenceonMachineLearning(ICML).2024.
[4] Ameen Ali, Itamar Zimerman, and Lior Wolf. The Hidden Attention of Mamba Models. 2024. arXiv: 2403.01590
[cs.LG].
[5] SimranArora,SabriEyuboglu,AmanTimalsina,IsysJohnson,MichaelPoli,JamesZou,AtriRudra,andChristo-
pherRé.“Zoology:MeasuringandImprovingRecallinEfficientLanguageModels”.In:TheInternationalConference
onLearningRepresentations(ICLR).2024.
[6] Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri
Rudra,andChristopherRé.“SimpleLinearAttentionLanguageModelsBalancetheRecall-ThroughputTradeoff”.
In:TheInternationalConferenceonMachineLearning(ICML).2024.
[7] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. “Neural Machine Translation by Jointly Learning to
AlignandTranslate”.In:TheInternationalConferenceonLearningRepresentations(ICLR).2015.
36[8] GeorgeABaker,GeorgeABakerJr,PeterGraves-Morris,andSusanSBaker.PadeApproximants:Encyclopediaof
MathematicsandIt’sApplications,Vol.59GeorgeA.Baker,Jr.,PeterGraves-Morris.Vol.59.CambridgeUniversity
Press,1996.
[9] MaximilianBeck,KorbinianPöppel,MarkusSpanring,AndreasAuer,OleksandraPrudnikova,MichaelKopp,Gün-
ter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. “xLSTM: Extended Long Short-Term Memory”. In:
arXivpreprintarXiv:2405.04517 (2024).
[10] StellaBiderman,HaileySchoelkopf,QuentinGregoryAnthony,HerbieBradley,KyleO’Brien,EricHallahan,Mo-
hammadAflahKhan,ShivanshuPurohit,USVSNSaiPrashanth,EdwardRaff,etal.“Pythia:ASuiteforAnalyzing
LargeLanguageModelsacrossTrainingandScaling”.In:TheInternationalConferenceonMachineLearning(ICML).
PMLR.2023,pp.2397–2430.
[11] YonatanBisk,RowanZellers,JianfengGao,YejinChoi,etal.“PIQA:ReasoningaboutPhysicalCommonsensein
NaturalLanguage”.In:ProceedingsoftheAAAIconferenceonArtificialIntelligence.Vol.34.2020.
[12] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor
Leahy,KyleMcDonell,JasonPhang,etal.“Gpt-NeoX-20B:AnOpen-sourceAutoregressiveLanguageModel”.In:
arXivpreprintarXiv:2204.06745(2022).
[13] GuyEBlelloch.“PrefixSumsandTheirApplications”.In:(1990).
[14] AleksandarBotev,SohamDe,SamuelLSmith,AnushanFernando,George-CristianMuraru,RubaHaroun,Leonard
Berrada,RazvanPascanu,PierGiuseppeSessa,RobertDadashi,etal.“RecurrentGemma:MovingPastTransform-
ersforEfficientOpenLanguageModels”.In:arXivpreprintarXiv:2404.07839(2024).
[15] GeorgeEPBox,GwilymMJenkins,GregoryCReinsel,andGretaMLjung.TimeSeriesAnalysis:Forecastingand
Control.JohnWiley&Sons,2015.
[16] James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. “Quasi-recurrent Neural Networks”. In:
arXivpreprintarXiv:1611.01576(2016).
[17] WilliamBrandon,AniruddhaNrusimha,KevinQian,ZacharyAnkner,TianJin,ZhiyeSong,andJonathanRagan-
Kelley.“Stripedattention:Fasterringattentionforcausaltransformers”.In:arXivpreprintarXiv:2311.09431(2023).
[18] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakan-
tan,PranavShyam,GirishSastry,AmandaAskell,etal.“LanguageModelsareFew-shotLearners”.In:Advances
inNeuralInformationProcessingSystems(NeurIPS)33(2020),pp.1877–1901.
[19] KrzysztofChoromanski,ValeriiLikhosherstov,DavidDohan,XingyouSong,AndreeaGane,TamasSarlos,Peter
Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. “Rethinking Attention with Performers”. In: The
InternationalConferenceonLearningRepresentations(ICLR).2021.
[20] AakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,AdamRoberts,PaulBarham,
Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. “PaLM: Scaling Language Modeling with Path-
ways”.In:JournalofMachineLearningResearch24.240(2023),pp.1–113.url:http://jmlr.org/papers/v24/22-
1144.html.
[21] JunyoungChung,CaglarGulcehre,KyungHyunCho,andYoshuaBengio.“EmpiricalEvaluationofGatedRecur-
rentNeuralNetworksonSequenceModeling”.In:arXivpreprintarXiv:1412.3555(2014).
[22] PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,AshishSabharwal,CarissaSchoenick,andOyvindTafjord.
“ThinkyouhaveSolvedQuestionAnswering?TryARC,theAI2ReasoningChallenge”.In:arXivpreprintarXiv:1803.05457
(2018).
[23] TriDao.“FlashAttention-2:FasterAttentionwithBetterParallelismandWorkPartitioning”.In:TheInternational
ConferenceonLearningRepresentations(ICLR).2024.
[24] Tri Dao, Beidi Chen, Nimit S Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao,
AtriRudra,andChristopherRé.“Monarch:Expressivestructuredmatricesforefficientandaccuratetraining”.In:
InternationalConferenceonMachineLearning.PMLR.2022,pp.4690–4721.
[25] TriDao,DanielYFu,KhaledKSaab,ArminWThomas,AtriRudra,andChristopherRé.“HungryHungryHippos:
TowardsLanguageModelingwithStateSpaceModels”.In:TheInternationalConferenceonLearningRepresentations
(ICLR).2023.
[26] Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher Ré. “Learning Fast Algorithms for Linear
TransformsUsingButterflyFactorizations”.In:TheInternationalConferenceonMachineLearning(ICML).2019.
[27] TriDao,NimitSohoni,AlbertGu,MatthewEichhorn,AmitBlonder,MeganLeszczynski,AtriRudra,andChristo-
pherRé.“Kaleidoscope:AnEfficient,LearnableRepresentationforAllStructuredLinearMaps”.In:TheInterna-
tionalConferenceonLearningRepresentations(ICLR).2020.
37[28] TimothéeDarcet,MaximeOquab,JulienMairal,andPiotrBojanowski.“VisionTransformersNeedRegisters”.In:
TheInternationalConferenceonLearningRepresentations(ICLR).2024.
[29] Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba
Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. “Griffin: Mixing Gated Linear Recurrences
withLocalAttentionforEfficientLanguageModels”.In:arXivpreprintarXiv:2402.19427 (2024).
[30] ChristopherDeSa,AlbertGu,RohanPuttagunta,ChristopherRé,andAtriRudra.“ATwo-ProngedProgressin
StructuredDenseMatrixVectorMultiplication”.In:ProceedingsoftheTwenty-NinthAnnualACM-SIAMSymposium
onDiscreteAlgorithms.SIAM.2018,pp.1060–1079.
[31] HantianDing,ZijianWang,GiovanniPaolini,VarunKumar,AnoopDeoras,DanRoth,andStefanoSoatto.“Fewer
truncationsimprovelanguagemodeling”.In:arXivpreprintarXiv:2404.10830(2024).
[32] Yuli Eidelman and Israel Gohberg. “On a new class of structured matrices”. In: Integral Equations and Operator
Theory34.3(1999),pp.293–324.
[33] Dan Fu, Simran Arora, Jessica Grogan, Isys Johnson, Evan Sabri Eyuboglu, Armin Thomas, Benjamin Spector,
MichaelPoli,AtriRudra,andChristopherRé.“Monarchmixer:Asimplesub-quadraticgemm-basedarchitecture”.
In:AdvancesinNeuralInformationProcessingSystems36(2024).
[34] DanielYFu,ElliotLEpstein,EricNguyen,ArminWThomas,MichaelZhang,TriDao,AtriRudra,andChristopher
Ré. “Simple Hardware-efficient Long Convolutions for Sequence Modeling”. In: The International Conference on
MachineLearning(ICML)(2023).
[35] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He,
AnishThite,NoaNabeshima,ShawnPresser,andConnorLeahy.“ThePile:An800GBDatasetofDiverseTextfor
LanguageModeling”.In:arXivpreprintarXiv:2101.00027 (2020).
[36] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey
Hsu,KyleMcDonell,NiklasMuennighoff,JasonPhang,LariaReynolds,EricTang,AnishThite,BenWang,Kevin
Wang,andAndyZou.AFrameworkforFew-shotLanguageModelEvaluation.Versionv0.0.1.Sept.2021.doi:10.
5281/zenodo.5371628.url:https://doi.org/10.5281/zenodo.5371628.
[37] PaoloGlorioso,QuentinAnthony,YuryTokpanov,JamesWhittington,JonathanPilault,AdamIbrahim,andBeren
Millidge.“Zamba:ACompact7BSSMHybridModel”.In:arXivpreprintarXiv:2405.16712(2024).
[38] RiccardoGrazzi,JulienSiems,SimonSchrodi,ThomasBrox,andFrankHutter.“IsMambaCapableofIn-Context
Learning?”In:arXivpreprintarXiv:2402.03170(2024).
[39] AlbertGu.“ModelingSequenceswithStructuredStateSpaces”.PhDthesis.StanfordUniversity,2023.
[40] AlbertGuandTriDao.“Mamba:Linear-TimeSequenceModelingwithSelectiveStateSpaces”.In:arXivpreprint
arXiv:2312.00752(2023).
[41] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. “HIPPO: Recurrent Memory with Optimal
PolynomialProjections”.In:AdvancesinNeuralInformationProcessingSystems(NeurIPS).2020.
[42] AlbertGu,KaranGoel,andChristopherRé.“EfficientlyModelingLongSequenceswithStructuredStateSpaces”.
In:TheInternationalConferenceonLearningRepresentations(ICLR).2022.
[43] AlbertGu,AnkitGupta,KaranGoel,andChristopherRé.“OntheParameterizationandInitializationofDiagonal
StateSpaceModels”.In:AdvancesinNeuralInformationProcessingSystems(NeurIPS).2022.
[44] AlbertGu,IsysJohnson,KaranGoel,KhaledSaab,TriDao,AtriRudra,andChristopherRé.“CombiningRecurrent,
Convolutional,andContinuous-timeModelswiththeLinearStateSpaceLayer”.In:AdvancesinNeuralInformation
ProcessingSystems(NeurIPS).2021.
[45] AlbertGu,IsysJohnson,AmanTimalsina,AtriRudra,andChristopherRé.“HowtoTrainYourHIPPO:StateSpace
ModelswithGeneralizedBasisProjections”.In:TheInternationalConferenceonLearningRepresentations(ICLR).
2023.
[46] AnkitGupta,AlbertGu,andJonathanBerant.“DiagonalStateSpacesareasEffectiveasStructuredStateSpaces”.
In:AdvancesinNeuralInformationProcessingSystems35(2022),pp.22982–22994.
[47] Dan Hendrycks and Kevin Gimpel. “Gaussian Error Linear Units (GELUs)”. In: arXiv preprint arXiv:1606.08415
(2016).
[48] W Daniel Hillis and Guy L Steele Jr. “Data Parallel Algorithms”. In: Communications of the ACM 29.12 (1986),
pp.1170–1183.
[49] SeppHochreiterandJürgenSchmidhuber.“LongShort-TermMemory”.In:NeuralComputation9.8(1997),pp.1735–
1780.
[50] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego
de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. “An Empirical Analysis of Compute-
38Optimal Large Language Model Training”. In: Advances in Neural Information Processing Systems (NeurIPS) 35
(2022),pp.30016–30030.
[51] Samy Jelassi, David Brandfonbrener, Sham M Kakade, and Eran Malach. “Repeat After Me: Transformers Are
BetterThanStateSpaceModelsatCopying”.In:TheInternationalConferenceonMachineLearning(ICML).2024.
[52] AngelosKatharopoulos,ApoorvVyas,NikolaosPappas,andFrançoisFleuret.“TransformersareRNNs:FastAu-
toregressiveTransformerswithLinearAttention”.In:InternationalConferenceonMachineLearning.PMLR.2020,
pp.5156–5165.
[53] Tobias Katsch. “GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling”. In: arXiv preprint
arXiv:2311.01927 (2023).
[54] ShivaKaul.“LinearDynamicalSystemsasaCoreComputationalPrimitive”.In:AdvancesinNeuralInformation
ProcessingSystems33(2020),pp.16808–16820.
[55] VijayAnandKorthikanti,JaredCasper,SangkugLym,LawrenceMcAfee,MichaelAndersch,MohammadShoeybi,
andBryanCatanzaro.“Reducingactivationrecomputationinlargetransformermodels”.In:ProceedingsofMachine
LearningandSystems5(2023).
[56] JamesLee-Thorp,JoshuaAinslie,IlyaEckstein,andSantiagoOntanon.“Fnet:Mixingtokenswithfouriertrans-
forms”.In:arXivpreprintarXiv:2105.03824(2021).
[57] Tao Lei. “When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute”. In: Pro-
ceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing.2021,pp.7633–7648.
[58] TaoLei,YuZhang,SidaIWang,HuiDai,andYoavArtzi.“SimpleRecurrentUnitsforHighlyParallelizableRecur-
rence”.In:arXivpreprintarXiv:1709.02755(2017).
[59] YuhongLi,TianleCai,YiZhang,DemingChen,andDebadeeptaDey.“WhatMakesConvolutionalModelsGreat
onLongSequenceModeling?”In:TheInternationalConferenceonLearningRepresentations(ICLR).2023.
[60] OpherLieber,BarakLenz,HofitBata,GalCohen,JhonathanOsin,ItayDalmedigos,ErezSafahi,ShakedMeirom,
YonatanBelinkov,ShaiShalev-Shwartz,etal.“Jamba:AHybridTransformer-MambaLanguageModel”.In:arXiv
preprintarXiv:2403.19887 (2024).
[61] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. “World Model on Million-Length Video And Language
WithRingAttention”.In:arXivpreprintarXiv:2402.08268(2024).
[62] HaoLiu,MateiZaharia,andPieterAbbeel.“Ringattentionwithblockwisetransformersfornear-infinitecontext”.
In:arXivpreprintarXiv:2310.01889(2023).
[63] ChrisLu,YannickSchroecker,AlbertGu,EmilioParisotto,JakobFoerster,SatinderSingh,andFeryalBehbahani.
“StructuredStateSpaceModelsforIn-ContextReinforcementLearning”.In:AdvancesinNeuralInformationPro-
cessingSystems(NeurIPS).2023.
[64] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke
Zettlemoyer. “Mega: Moving Average Equipped Gated Attention”. In: The International Conference on Learning
Representations(ICLR).2023.
[65] EricMartinandChrisCundy.“ParallelizingLinearRecurrentNeuralNetsOverSequenceLength”.In:TheInter-
nationalConferenceonLearningRepresentations(ICLR).2018.
[66] TodorMihaylov,PeterClark,TusharKhot,andAshishSabharwal.“CanaSuitofArmorConductElectricity?A
NewDatasetforOpenBookQuestionAnswering”.In:arXivpreprintarXiv:1809.02789(2018).
[67] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann,
AmandaAskell,YuntaoBai,AnnaChen,TomConerly,DawnDrain,DeepGanguli,ZacHatfield-Dodds,Danny
Hernandez,ScottJohnston,AndyJones,JacksonKernion,LianeLovitt,KamalNdousse,DarioAmodei,TomBrown,
JackClark,JaredKaplan,SamMcCandlish,andChrisOlah.“In-contextLearningandInductionHeads”.In:Trans-
formerCircuitsThread(2022).https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.
[68] AntonioOrvieto,SamuelLSmith,AlbertGu,AnushanFernando,CaglarGulcehre,RazvanPascanu,andSoham
De.“ResurrectingRecurrentNeuralNetworksforLongSequences”.In:TheInternationalConferenceonMachine
Learning(ICML).2023.
[69] DenisPaperno,GermánKruszewski,AngelikiLazaridou,Ngoc-QuanPham,RaffaellaBernardi,SandroPezzelle,
Marco Baroni, Gemma Boleda, and Raquel Fernández. “The LAMBADA Dataset: Word Prediction Requiring a
BroadDiscourseContext”.In:Proceedingsofthe54thAnnualMeetingoftheAssociationforComputationalLinguis-
tics.2016,pp.1525–1534.
[70] Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, and
DimitrisPapailiopoulos.“CanMambaLearnHowtoLearn?AComparativeStudyonIn-ContextLearningTasks”.
In:TheInternationalConferenceonMachineLearning(ICML).2024.
39[71] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael
Chung, Matteo Grella, Kranthi Kiran GV, et al. “RWKV: Reinventing RNNs for the Transformer Era”. In: arXiv
preprintarXiv:2305.13048(2023).
[72] BoPeng,DanielGoldstein,QuentinAnthony,AlonAlbalak,EricAlcaide,StellaBiderman,EugeneCheah,Teddy
Ferdinan,HaowenHou,PrzemysławKazienko,etal.“EagleandFinch:RWKVwithmatrix-valuedstatesanddy-
namicrecurrence”.In:arXivpreprintarXiv:2404.05892(2024).
[73] HaoPeng,NikolaosPappas,DaniYogatama,RoySchwartz,NoahASmith,andLingpengKong.“RandomFeature
Attention”.In:TheInternationalConferenceonLearningRepresentations(ICLR).2021.
[74] ClémentPernet.“ComputingwithQuasiseparableMatrices”.In:ProceedingsoftheACMonInternationalSympo-
siumonSymbolicandAlgebraicComputation.2016,pp.389–396.
[75] ClémentPernet,HippolyteSignargout,andGillesVillard.“Exactcomputationswithquasiseparablematrices”.In:
arXivpreprintarXiv:2302.04515(2023).
[76] ClémentPernetandArneStorjohann.“Timeandspaceefficientgeneratorsforquasiseparablematrices”.In:Journal
ofSymbolicComputation85(2018),pp.224–246.
[77] MichaelPoli,StefanoMassaroli,EricNguyen,DanielYFu,TriDao,StephenBaccus,YoshuaBengio,StefanoErmon,
and Christopher Ré. “Hyena Hierarchy: Towards Larger Convolutional Language Models”. In: The International
ConferenceonMachineLearning(ICML).2023.
[78] HadiPouransari,Chun-LiangLi,Jen-HaoRickChang,PavanKumarAnasosaluVasu,CemKoc,VaishaalShankar,
andOncelTuzel.“DatasetDecomposition:FasterLLMTrainingwithVariableSequenceLengthCurriculum”.In:
arXivpreprintarXiv:2405.13226(2024).
[79] Ofir Press, Noah Smith, and Mike Lewis. “Train Short, Test Long: Attention with Linear Biases Enables Input
LengthExtrapolation”.In:InternationalConferenceonLearningRepresentations.2022.
[80] ZhenQin,XiaodongHan,WeixuanSun,BowenHe,DongLi,DongxuLi,YuchaoDai,LingpengKong,andYiran
Zhong.“ToeplitzNeuralNetworkforSequenceModeling”.In:TheInternationalConferenceonLearningRepresen-
tations(ICLR).2023.
[81] ZhenQin,XiaodongHan,WeixuanSun,DongxuLi,LingpengKong,NickBarnes,andYiranZhong.“Thedevilin
lineartransformer”.In:arXivpreprintarXiv:2210.10340(2022).
[82] ZhenQin,DongLi,WeigaoSun,WeixuanSun,XuyangShen,XiaodongHan,YunshenWei,BaohongLv,XiaoLuo,
YuQiao,etal.“TransNormerLLM:AFasterandBetterLargeLanguageModelwithImprovedTransNormer”.In:
arXivpreprintarXiv:2307.14995(2023).
[83] ZhenQin,WeixuanSun,HuiDeng,DongxuLi,YunshenWei,BaohongLv,JunjieYan,LingpengKong,andYiran
Zhong.“CosFormer:RethinkingSoftmaxinAttention”.In:TheInternationalConferenceonLearningRepresentations
(ICLR).2022.
[84] Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. “HGRN2: Gated
LinearRNNswithStateExpansion”.In:arXivpreprintarXiv:2404.07904(2024).
[85] ZhenQin,SonglinYang,andYiranZhong.“HierarchicallyGatedRecurrentNeuralNetworkforSequenceModel-
ing”.In:AdvancesinNeuralInformationProcessingSystems36(2023).
[86] AliRahimiandBenjaminRecht.“RandomFeaturesforLarge-ScaleKernelMachines”.In:AdvancesinNeuralIn-
formationProcessingSystems(NeurIPS)20(2007).
[87] PrajitRamachandran,BarretZoph,andQuocVLe.“Swish:ASelf-gatedActivationFunction”.In:arXivpreprint
arXiv:1710.059417.1(2017),p.5.
[88] KeisukeSakaguchi,RonanLeBras,ChandraBhagavatula,andYejinChoi.“Winogrande:AnAdversarialWinograd
SchemaChallengeatScale”.In:CommunicationsoftheACM 64.9(2021),pp.99–106.
[89] ImanolSchlag,KazukiIrie,andJürgenSchmidhuber.“LinearTransformersareSecretlyFastWeightProgrammers”.
In:TheInternationalConferenceonMachineLearning(ICML).PMLR.2021,pp.9355–9366.
[90] NoamShazeer.“FastTransformerDecoding:OneWrite-headisAllYouNeed”.In:arXivpreprintarXiv:1911.02150
(2019).
[91] SamShleifer,JasonWeston,andMyleOtt.“NormFormer:ImprovedTransformerPretrainingwithExtraNormal-
ization”.In:arXivpreprintarXiv:2110.09456(2021).
[92] MohammadShoeybi,MostofaPatwary,RaulPuri,PatrickLeGresley,JaredCasper,andBryanCatanzaro.“Megatron-
LM:TrainingMulti-BillionParameterLanguageModelsUsingModelParallelism”.In:arXivpreprintarXiv:1909.08053
(2019).
[93] JimmyTHSmith,AndrewWarrington,andScottWLinderman.“SimplifiedStateSpaceLayersforSequenceMod-
eling”.In:TheInternationalConferenceonLearningRepresentations(ICLR).2023.
40[94] JianlinSu,YuLu,ShengfengPan,AhmedMurtadha,BoWen,andYunfengLiu.“Roformer:EnhancedTransformer
withRotaryPositionEmbedding”.In:arXivpreprintarXiv:2104.09864(2021).
[95] YutaoSun,LiDong,ShaohanHuang,ShumingMa,YuqingXia,JilongXue,JianyongWang,andFuruWei.“Reten-
tivenetwork:Asuccessortotransformerforlargelanguagemodels”.In:arXivpreprintarXiv:2307.08621(2023).
[96] YiTay,MostafaDehghani,DaraBahri,andDonaldMetzler.“EfficientTransformers:ASurvey”.In:ACMComput-
ingSurveys55.6(2022),pp.1–28.
[97] ChameleonTeam.“Chameleon:Mixed-ModalEarly-FusionFoundationModels”.In:arXivpreprintarXiv:2405.09818
(2024).
[98] AnnaThomas,AlbertGu,TriDao,AtriRudra,andChristopherRé.“LearningCompressedTransformswithLow
DisplacementRank”.In:AdvancesinNeuralInformationProcessingSystems(NeurIPS).2018,pp.9052–9060.
[99] IlyaOTolstikhin,NeilHoulsby,AlexanderKolesnikov,LucasBeyer,XiaohuaZhai,ThomasUnterthiner,Jessica
Yung,AndreasSteiner,DanielKeysers,JakobUszkoreit,etal.“MLP-Mixer:AnAll-MLPArchitectureforVision”.
In:AdvancesinNeuralInformationProcessingSystems(NeurIPS)34(2021),pp.24261–24272.
[100] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,Baptiste
Rozière,NamanGoyal,EricHambro,FaisalAzhar,etal.“Llama:OpenandEfficientFoundationLanguageModels”.
In:arXivpreprintarXiv:2302.13971(2023).
[101] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi, YasmineBabaei,NikolayBashlykov,
SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal.“Llama2:Openfoundationandfine-tunedchatmodels”.
In:arXivpreprintarXiv:2307.09288(2023).
[102] RafVandebril,MVanBarel,GeneGolub,andNicolaMastronardi.“Abibliographyonsemiseparablematrices”.In:
Calcolo42(2005),pp.249–270.
[103] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN.Gomez,LukaszKaiser,and
IlliaPolosukhin.“AttentionIsAllYouNeed”.In:AdvancesinNeuralInformationProcessingSystems(NeurIPS).2017.
[104] Shida Wang and Beichen Xue. “State-space Models with Layer-wise Nonlinearity are Universal Approximators
withExponentialDecayingMemory”.In:arXivpreprintarXiv:2309.13414(2023).
[105] SinongWang,BelindaZLi,MadianKhabsa,HanFang,andHaoMa.“Linformer:Self-attentionwithLinearCom-
plexity”.In:arXivpreprintarXiv:2006.04768(2020).
[106] GuangxuanXiao,YuandongTian,BeidiChen,SongHan,andMikeLewis.“EfficientStreamingLanguageModels
withAttentionSinks”.In:TheInternationalConferenceonLearningRepresentations(ICLR).2024.
[107] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh.
“Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention”. In: Proceedings of the AAAI
ConferenceonArtificialIntelligence.Vol.35.2021.
[108] SonglinYang,BailinWang,YikangShen,RameswarPanda,andYoonKim.“GatedLinearAttentionTransformers
withHardware-EfficientTraining”.In:TheInternationalConferenceonMachineLearning(ICML).2024.
[109] RowanZellers,AriHoltzman,YonatanBisk,AliFarhadi,andYejinChoi.“HellaSwag:CanaMachineReallyFinish
YourSentence?”In:Proceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics.2019.
[110] JinleZeng,MinLi,ZhihuaWu,JiaqiLiu,YuangLiu,DianhaiYu,andYanjunMa.“Boostingdistributedtraining
performanceoftheunpaddedbertmodel”.In:arXivpreprintarXiv:2208.08124(2022).
[111] ShuangfeiZhai,WalterTalbott,NitishSrivastava,ChenHuang,HanlinGoh,RuixiangZhang,andJoshSusskind.
“AnAttentionFreeTransformer”.In:arXivpreprintarXiv:2105.14103(2021).
[112] YujiaZhai, ChengquanJiang, LeyuanWang, XiaoyingJia,Shang Zhang,Zizhong Chen,Xin Liu,andYibo Zhu.
“Bytetransformer:Ahigh-performancetransformerboostedforvariable-lengthinputs”.In:2023IEEEInternational
ParallelandDistributedProcessingSymposium(IPDPS).IEEE.2023,pp.344–355.
[113] MichaelZhang,KushBhatia,HermannKumbong,andChristopherRé.“TheHedgehog&thePorcupine:Expres-
siveLinearAttentionswithSoftmaxMimicry”.In:TheInternationalConferenceonLearningRepresentations(ICLR).
2024.
[114] LinZheng,ChongWang,andLingpengKong.“Linearcomplexityrandomizedself-attentionmechanism”.In:In-
ternationalConferenceonMachineLearning.PMLR.2022,pp.27011–27041.
41A Glossary
Table 8: Glossary of notation and terminology; mnemonics bolded. (Top) Frequently used tensor dimensions. (Bottom)
Matricesandtensorsusedinstatespacemodelsorstructuredmaskedattention.
Notation Description Definition
T Timeaxisortargetsequenceaxis Definition2.1
S Sourcesequenceaxis(inattention) Equation(9)
D Modeldimensionord_model Definition7.1
N State/featuredimensionord_state Equations(2)and(9)
P Headdimensionord_head Definition2.1
H Numberof headsorn_head Definition7.1
𝑀 Sequencetransformationmatrix Definition2.3
𝐴 DiscreteSSMrecurrent(state)matrix Equation(2)
𝐵 Statespacemodelinputprojection(expansion)matrix Equation(2)
𝐶 Statespacemodeloutputprojection(contraction)matrix Equation(2)
𝑋 Inputmatrix(shape(T,P)) Equations(2)and(9)
𝑌 Outputmatrix(shape(T,P)) Equations(2)and(9)
𝑄 Attentionquerymatrix Equation(9)
𝐾 Attentionkeymatrix Equation(9)
𝑉 Attentionvaluematrix Equation(9)
𝐺 AttentionGrammatrix 𝑄𝐾⊤(or𝐶𝐵⊤)
𝐿 (Structured)maskmatrix(lower-triangularinthecausalsetting) Definition4.2
B Efficient Algorithms for the Scalar SSM Scan (1-SS Multiplication)
InthissectionwefleshoutvariousalgorithmsforcomputingthescalarSSMscan,throughthelensofstructuredmatrix
decompositions.ThescalarSSMscanisdefinedascomputingtherecurrentpartofthediscreteSSM(7),inthecasewhen
𝑁 = 1(i.e.𝐴isascalar). ThisiscommonlyusedtocomputeSSMsrecurrently;inparticular,thecaseofstructuredSSMs
where𝐴isdiagonallystructuredreducesdowntothisoperation,suchasintheS5(J.T.Smith,Warrington,andLinderman
2023)andS6(GuandDao2023)models.
The goal of this section is to support a central theme of this paper that efficient algorithms for sequence models can be
viewedasstructuredmatrixmultiplicationalgorithms. Thevariousmatrixdecompositionideasweshowherearerelated
toideasusedtoderivefastSSMalgorithms(Section6),aswellasdirectlyusedasasubroutine.
B.1 ProblemDefinition
Let𝑎 : (D,)and𝑏 : (D,)besequencesofscalars.ThescalarSSMscanisdefinedas
ℎ
𝑡
=𝑎 𝑡ℎ 𝑡−1+𝑏 𝑡. (21)
Hereℎ −1canbeanarbitraryvaluerepresentingtheprevioushiddenstatetotheSSMrecurrence;unlessotherwisespeci-
fied,weassumeℎ
−1
=0.
Wealsocallequation(21)thecumprodsum(cumulativeproductsum). Notethatthecumprodsumreducestothecumprod
(cumulativeproduct)when𝑏 =0istheadditiveidentityanditreducestothecumsum(cumulativesum)when𝑎 =1isthe
multiplicativeidentity.
42Finally,notethatinvectorizedformwecanwrite
ℎ =𝑀𝑏
1
 
 𝑎 1 
 1 
 𝑎 𝑎 𝑎 1 
𝑀 =  2 1 2 
  . . . . . . ... ...  
 
𝑎 ...𝑎 𝑎 ...𝑎 ... 𝑎 1
 𝑇−1 1 𝑇−1 2 𝑇−1 
 
Inotherwords,thisissimplythematrix-vectorproductbya1-SSmatrix𝑀.
Thereforewehavethreewaysofviewingthisfundamentalprimitiveoperationthatareallequivalent:
• A(scalar)SSMscan.
• Acumprodsum.
• A1-SSmatrix-vectormultiplication.
B.2 ClassicalAlgorithms
WefirstdescribethetwoclassicalwaysofcomputingtheSSMscan(21),previouslyusedbypriorwork.
B.2.1 SequentialRecurrence
Therecurrentmodesimplycomputes(21)onetimestep𝑡 atatime. Fromtheperspectiveof1-SSmultiplication,thiswas
alsodescribedinSection3.4.1.
B.2.2 ParallelAssociativeScan
Second,animportantobservationisthatthisrecurrencecanbeturnedintoanassociativescan(E.MartinandCundy2018;
J.T.Smith, Warrington, andLinderman2023). Thisfactisnotcompletelyobvious. Forexample, S5definedthecorrect
associativescanoperatorandthenshowedassociativityoftheoperatorthroughrotecalculation.
Aslightlycleanerwaytoseethatthisiscomputablewithanassociativescanistoturnthemulti-termrecurrenceintoa
single-termrecurrenceonahiddenstateofsize2insteadof1:
ℎ
𝑡
=𝑎 𝑡ℎ 𝑡−1+𝑏
𝑡
(cid:20)ℎ (cid:21) (cid:20)𝑎 𝑏 (cid:21) (cid:20)ℎ (cid:21)
𝑡 = 𝑡 𝑡 𝑡−1 .
1 0 1 1
Then computing all theℎ
𝑡
is the same as taking the cumulative products of these 2×2 matrices. Since matrix multi-
plicationisassociative,thiscanbecomputedwithanassociativescan. Theassociativebinaryoperatorissimplymatrix
multiplicationontheseparticularmatrices:
(cid:20)𝑎
𝑡
𝑏 𝑡(cid:21) (cid:20)𝑎
𝑠
𝑏 𝑠(cid:21)
=
(cid:20)𝑎 𝑡𝑎
𝑠
𝑎 𝑡𝑏
𝑠
+𝑏 𝑡(cid:21)
.
0 1 0 1 0 1
EquatingthetoprowyieldsthesameassociativescanoperatorasdefinedbyS5:
(𝑎 𝑡,𝑏 𝑡) ⊗ (𝑎 𝑠,𝑏 𝑠) = (𝑎 𝑡𝑎 𝑠,𝑎 𝑡𝑏 𝑠 +𝑏 𝑡). (22)
Thereasonwhyassociativescansareimportantisthattheycanbeparallelizedusingadivide-and-conqueralgorithm(Blel-
loch1990).Weomitthedetailsofthisalgorithm,andinsteadshowthattheentireassociativeSSMscanalgorithmcanbe
derivedfromscratchthroughmatrixdecompositions(AppendixB.3.5).
43B.3 EfficientAlgorithmsviaStructuredMatrixDecompositions
WediscussseveralalgorithmsforcomputingtheSSMscan,allthroughthelensoffindingstructuredmatrixdecomposi-
tionsofthe1-SSmatrix𝑀.Thesealgorithmsorcomputationmodesinclude
• Adilated modewhereinformationispropagated1,2,4,8,... stepsatatime.
• Astate-passingmodewhereinformationispropagatedforwardinchunks.
• Afullyrecurrent modethatincrementsonestepatatime,whichisaspecialcaseofthestate-passingmode.
• Ablockdecompositionparallelmodewhere𝑀 isdividedintohierarchicalblocks.
• Ascanmodewhere𝑀 isdivideintoequalsizeblocksandreducedrecursively.
B.3.1 DilatedMode
This mode factors the 1-SS matrix in a particular way involving increasing “strides”. This is best illustrated through a
concreteexample:
𝑎0:0

 𝑎1:0 𝑎1:1 

 𝑎2:0 𝑎2:1 𝑎2:2 

𝑀= 𝑎3:0 𝑎3:1 𝑎3:2 𝑎3:3 

 𝑎4:0 𝑎4:1 𝑎4:2 𝑎4:3 𝑎4:4 

 𝑎5:0 𝑎5:1 𝑎5:2 𝑎5:3 𝑎5:4 𝑎5:5 

 𝑎6:0 𝑎6:1 𝑎6:2 𝑎6:3 𝑎6:4 𝑎6:5 𝑎6:6 

 𝑎7:0 𝑎7:1 𝑎7:2 𝑎7:3 𝑎7:4 𝑎7:5 𝑎7:6 𝑎7:7

𝑎0:0 𝑎0:0 𝑎0:0



𝑎1:1  

𝑎1:1   𝑎1:0 𝑎1:1 



𝑎2:2   𝑎2:0 𝑎2:2  

𝑎2:1 𝑎2:2 

=

𝑎3:3  

𝑎3:1 𝑎3:3  

𝑎3:2 𝑎3:3 

 𝑎4:0 𝑎4:4  

𝑎4:2 𝑎4:4  

𝑎4:3 𝑎4:4 



𝑎5:1 𝑎5:5  

𝑎5:3 𝑎5:5  

𝑎5:4 𝑎5:5 



𝑎6:2 𝑎6:6  

𝑎6:4 𝑎6:6  

𝑎6:5 𝑎6:6 



𝑎7:3 𝑎7:7 

𝑎7:5 𝑎7:7 

𝑎7:6 𝑎7:7

Notethatthiscloselyresemblesthecomputationofdilatedconvolutions.
Wealsonotethatthisfactorizationshowsthat1-SSmatricesareaspecialcaseofbutterflymatrices,anotherbroadand
fundamentaltypeofstructuredmatrix(Dao,Gu,etal.2019;Dao,Sohoni,etal.2020).
Remark8. Thisalgoritihmissometimesdescribedasa“work-inefficientbutmoreparallelizable”prefixsumalgorithm(Hillis
andSteeleJr1986),becausesituses𝑂(𝑇 log(𝑇))operationsbuthashalfthedepth/spanasthework-efficientassociativescan
algorithm.
B.3.2 State-Passing(Chunkwise)Mode
Thismodecanbeviewedasageneralizationofthestandardrecurrentmodewhereinsteadofpassingforwardtherecurrent
stateℎonestepatatime,wecomputetheansweronchunksofarbitrarylength𝑘 andpassthestatethroughthechunk.
Thiscanalsobederivedfromasimpleblockdecompositionofthe1-SSmatrix.
Remark 9. While we call this “state-passing” to refer to how states are passed from one local segment to another, this is
relatedtothe“chunkwise”algorithmsproposedbyrelatedmodels(Y.Sunetal.2023;Yangetal.2024).
Considercomputingℎ = 𝑀𝑏 in“chunks”: forsomeindex𝑘 ∈ [𝑇],wewanttocomputeℎ 0:𝑘 ortheoutputuptoindex𝑘,
andhaveawaytoreducetheproblemtoasmallerproblemonindices [𝑘 :𝑇].
44Wewrite𝑀 as
𝑎
 0:0 
 𝑎 𝑎 
 1:0 1:1 
  . . . ...  
 
𝑀 =  𝑎 𝑘−1:0 ... ... 𝑎 𝑘−1:𝑘−1  
 𝑎 ... ... 𝑎 𝑎 
 𝑘:0 𝑘:𝑘−1 𝑘:𝑘 
  . . . . . . . . . ...  
 
𝑎 ... ... 𝑎 𝑎 ... 𝑎 
 𝑇−1:0 𝑇−1:𝑘−1 𝑇−1:𝑘 𝑇−1:𝑇−1
 
Lettheupper-lefttrianglebe𝑀 ,lower-rightbe𝑀 (leftandrightsubproblems),andlower-leftbe𝑀 . Divideup𝑏 into
𝐿 𝑅 𝐶
𝑏
𝐿
=𝑏
0:𝑘
and𝑏
𝑅
=𝑏
𝑘:𝑇
inthesameway.Notethat
(cid:20) 𝑀 𝑏 (cid:21)
𝑀𝑏 = 𝐿 𝐿
𝑀 𝑅𝑏
𝑅
+𝑀 𝐶𝑏
𝐿
Also,𝑀 hastherank-1factorization(thisisessentiallythedefiningpropertyofsemiseparablematrices)
𝐶
𝑎
 𝑘:𝑘 
𝑀 𝐶 =    . . .   𝑎 𝑘 (cid:2)𝑎 𝑘−1:0 ··· 𝑎 𝑘−1:𝑘−1(cid:3)
𝑎 
 𝑇−1:𝑘
 
Thus
𝑎
 𝑘:𝑘 
 . 
𝑀 𝐶𝑏 𝐿 =   . .  𝑎 𝑘 ·(𝑀𝑏)𝑘−1.
𝑎 
 𝑇−1:𝑘
 
Here we think of (𝑀𝑏)𝑘−1 = ℎ 𝑘−1 as the “final state” of the left chunk, because the row vector in𝑀 𝐶’s factorization is
thesameasthefinalrowof𝑀 . Furthermore,notethatthecolumnvectorin𝑀 ’sfactorizationisthesameasthefinal
𝐿 𝐶
columnof𝑀 .7 Thus
𝑅
𝑎 𝑘ℎ 𝑘−1+𝑏
𝑘
 𝑏 
 𝑘+1 
𝑀 𝑅𝑏
𝑅
+𝑀 𝐶𝑏
𝐿
=𝑀
𝑅


.
. .


 
 𝑏 
 𝑇−1 
 
Finally, we use the observation that 𝑀 and 𝑀 are self-similar to the original matrix 𝑀; the answers for these two
𝐿 𝑅
smaller1-SSmatrixmultiplicationscanbeperformedarbitrarilyusinganyalgorithm.Intotal,thealgorithmproceedsas
follows:
1. Computethelefthalfoftheanswerℎ usinganydesiredmethod(i.e.anyofthemethodsfor1-SSmultiplication
0:𝑘
fromthissection).
2. Computethefinalstateℎ .
𝑘−1
3. Incrementthestatebyonesteptomodify𝑏 .
𝑘
4. Computetherighthalfoftheanswerℎ usinganydesiredmethod.
𝑘:𝑇
Inotherwords,wecomputetheleftsubproblemasablackbox,passitsfinalstateontotherightproblem,andcompute
therightsubproblemasablackbox.
Theutilityofthismethodcomesfrommorecomplicatedsettings,suchasinthegeneral𝑁-semiseparablecase,andwhen
theinput𝑏 hasanadditional“batch”dimension(orinotherwordsthisisamatrix-matrixinsteadofmatrix-vectormulti-
plication).Inthiscase,wecanuseanalternatealgorithmforthechunks(correspondingtoMMby𝑀 and𝑀 )thatdoes
𝐿 𝑅
notmaterializethefullhiddenstatesℎ. Instead,weskipthehiddenstatesanddirectlycomputethefinalstateℎ inan
𝑘−1
alternateway,then“pass”thestatetothenextchunk.
7BoththesefactscanbeseenfromtheWoodburyinverse...
45Complexity. Thismethodcanbeverywork-efficientbecausesteps2-3takesonlyconstanttime. Thereforeassuming
thetwosubproblems(steps1and4)arelineartime,thewholemethodtakeslineartime.
Thedownsideisthatthisisalsosequential.
B.3.3 FullyRecurrentMode
Notethatthefullyrecurrentmode,wheretherecurrenceisevolvedonestepatatime(21),issimplyaninstantiationof
thestate-passingmodewithchunksize𝑘 =1.
B.3.4 (Parallel)BlockDecompositionMode
Thisusesthesamematrixdecompositionasthestate-passingmode,butcomputessubproblemsinadifferentorderthat
tradesoffcomputationforparallelization.
Asusual,wewrite𝑀 as
1 1 −1
   


𝑎
1
1 

 −𝑎
1
1 

𝑀 =


𝑎 2𝑎
1
𝑎
2
1 
 =


0 −𝑎
2
1 

  . . . . . . ... ...     . . . . . . ... ...  
   
 𝑎 𝑇−1...𝑎
1
𝑎 𝑇−1...𝑎
2
... 𝑎
𝑇−1
1



0 0 ... −𝑎
𝑇−1
1

   
Thekeyobservationisagainthatthebottom-leftquadrantof𝑀 isrank-1.Asidefrominspection,anotherwaytoseethis
isbyusingtheRHS,observingthatthebottom-leftquadrantofitisatrivialrank-1matrix(itisall0exceptthetop-right
corneris−𝑎 𝑇/2),andusingtheWoodburyinversionformulatoseethatthebottom-leftcorneroftheLHSmustalsobe
rank1.Thisalsoprovidesawaytodeducetherank-1factorization,whichcanbeverifiedthroughinspection:
 (𝑎 𝑇/2...𝑎 1) ... 𝑎 𝑇/2 
𝑀 lower-left-quadrant =    . . . ... . . .   
 (𝑎 𝑇−1...𝑎 𝑇/2𝑎 𝑇/2−1...𝑎 1) ... (𝑎 𝑇−1...𝑎 𝑇/2) 
 
𝑎
 𝑇/2 
=    . . .    (cid:2) (𝑎 𝑇/2−1...𝑎 1) ... 𝑎 𝑇/2−1 1(cid:3).
𝑎 ...𝑎 
 𝑇−1 𝑇/2
 
A second observation is that this matrix is self-similar: any principle submatrix has the same form. In particular, the
top-leftandbottom-rightquadrantsareboth1-SSmatrices.
Thisprovidesaneasywaytoperformthematrixmultiplicationby𝑀:recurseonthetwohalves(i.e.top-leftandbottom-
right) in parallel, and then account for the bottom-left submatrix. This “combination” step in the divide-and-conquer
algorithmiseasysincethesubmatrixisrank1.Thisleadstoaparallelalgorithm.
Complexity. Likethestate-passingalgorithm,thismethodusesthesameblockdecompositionsoftherank-structured
semiseparablematrices. Thedifferenceisthatwerecurseonbothsubproblemsinparallel,whilethestate-passingalgo-
rithmhandlestheleftandthenrightsubproblems.Thislowersthedepth/spanofthealgorithmfromlineartolog(𝑇).The
tradeoffisthatthecombinationstep(accountingfortherank-1bottom-leftsubmatrix)requireslinearinsteadofconstant
work,sothetotalworkis𝑂(𝑇 log(𝑇))insteadoflinear.
Note also that in the recursion, we can stop at any time and compute the subproblems in any other way. This is a
main idea behind the SSD algorithm (Section 6), where we switch to the dual quadratic attention formulation on small
subproblems.
B.3.5 AssociativeScanMode
Thestatepassing(chunkwise)algorithmhaslinearwork,butalsoinvolvessequentialoperations.
46Theblockmatrixreductionanddilatedmodesareparallelizable: theyhavelog(𝑇) depth/span. However, theydoextra
work(𝑂(𝑇 log(𝑇)).
As noted in Appendix B.2.2, there is an algorithm that achieves both𝑂(log𝑇) depth and𝑂(𝑇) work by leveraging the
associativescan(alsocalledprefixscan)algorithm(Bakeretal.1996). ThisalgorithmismosteasilyseenfromtheSSM
scanorcumprodsumview,andeventhenisnotobvious: itrequiresseparatelyderivinganassociativeoperator(22),and
thenleveragingtheparallel/associative/prefixscanalgorithmasablackbox(Blelloch1990).
Here we show that it is actually possible to derive this parallel scan from leveraging a different matrix decomposi-
tion:
𝑎
 0:0 
𝑎 𝑎 
 1:0 1:1 
𝑎 𝑎 𝑎 
 2:0 2:1 2:2 
𝑎 𝑎 𝑎 𝑎 
𝑀 =  𝑎3:0 𝑎3:1 𝑎3:2 𝑎3:3 𝑎  
 4:0 4:1 4:2 4:3 4:4 
𝑎 𝑎 𝑎 𝑎 𝑎 𝑎 
 5:0 5:1 5:2 5:3 5:4 5:5 
𝑎 𝑎 𝑎 𝑎 𝑎 𝑎 𝑎 
 6:0 6:1 6:2 6:3 6:4 6:5 6:6 
𝑎 𝑎 𝑎 𝑎 𝑎 𝑎 𝑎 𝑎 
 7:0 7:1 7:2 7:3 7:4 7:5 7:6 7:7
 
 𝑎 0:0 
 
 
 𝑎 1:0 𝑎 1:1 
 
 
 (cid:20)𝑎 (cid:21) (cid:20)𝑎 (cid:21)⊤ 𝑎 2:2 
 2:2 𝑎 1:0 
  𝑎 3:2 2:1 𝑎 1:1 𝑎 3:2 𝑎 3:3  
 
=  
 (cid:20)𝑎 (cid:21) (cid:20)𝑎 (cid:21)⊤ (cid:20)𝑎 (cid:21) (cid:20)𝑎 (cid:21)⊤ 𝑎 4:4 
 4:4 𝑎 1:0 4:4 𝑎 3:2 
  𝑎 5:4 4:1 𝑎 1:1 𝑎 5:4 4:3 𝑎 3:3 𝑎 5:4 𝑎 5:5  
 
 
 (cid:20)𝑎 (cid:21) (cid:20)𝑎 (cid:21)⊤ (cid:20)𝑎 (cid:21) (cid:20)𝑎 (cid:21)⊤ (cid:20)𝑎 (cid:21) (cid:20)𝑎 (cid:21)⊤ 𝑎 6:6 
 

𝑎6 7: :6
6
𝑎 6:1 𝑎1 1: :0
1
𝑎6 7: :6
6
𝑎 6:3 𝑎3 3: :2
3
𝑎6 7: :6
6
𝑎 6:1 𝑎5 5: :4
5 𝑎 7:6 𝑎 7:7
 

 
 
Nowweproceedinthreestages.
Stage 1. First we compute the answers for each of the diagonal blocks in the multiplication 𝑀𝑏. This produces two
numbers,butthefirstelementisunchanged.Forexample,thesecondblockisgoingtocompute𝑏 2and𝑎 3𝑏 2+𝑏
3
Stage2. Nowconsidereachofthe2×2blocksfactoredasarank-1matrixinthestrictlylowertriangularpartofthe
matrix. Note that each of the right side row vectors is the same as the bottom row vector in the diagonal block in its
column:inparticularthe [𝑎 1:0𝑎 1:1], [𝑎 3:2𝑎 3:3],and [𝑎 5:4𝑎 5:5] rows.
ThereforewealreadyhavetheanswerstothesefromStage1,whichisthesecondelementofall𝑇/2subproblemsinStage
1. Ifwecallthisarrayofelements𝑏′ (ofhalfthesizeof𝑏),thenweneedtomultiply𝑏′ bythe1-SSmatrixgeneratedby
𝑎 ,𝑎 ,𝑎 ,𝑎 .
3:−1 3:1 5:3 7:5
Stage3. Finally,eachoftheanswerstoStage2canbebroadcastintotwofinalanswersbymultiplyingbytheleft-side
columnvectors:inparticularthe [𝑎 2:2𝑎 3:2]⊤, [𝑎 4:4𝑎 5:4]⊤,and [𝑎 6:6𝑎 7:6]⊤vectors.
Note that this can be slightly modified with some off-by-one shifting of the indices. An equivalent way to view this
algorithmisasthethree-stepmatrixfactorization
47𝑎0:0

 𝑎1:0 𝑎1:1 

 𝑎2:0 𝑎2:1 𝑎2:2 

𝑀= 𝑎3:0 𝑎3:1 𝑎3:2 𝑎3:3 

 𝑎4:0 𝑎4:1 𝑎4:2 𝑎4:3 𝑎4:4 

 𝑎5:0 𝑎5:1 𝑎5:2 𝑎5:3 𝑎5:4 𝑎5:5 

 𝑎6:0 𝑎6:1 𝑎6:2 𝑎6:3 𝑎6:4 𝑎6:5 𝑎6:6 

 𝑎7:0 𝑎7:1 𝑎7:2 𝑎7:3 𝑎7:4 𝑎7:5 𝑎7:6 𝑎7:7

𝑎0:0 𝑎0:0 𝑎0:0



𝑎1:1  

𝑎1:1   𝑎1:0 𝑎1:1 



𝑎2:1 𝑎2:2  

𝑎2:2  

𝑎2:2 

=

𝑎3:3  

𝑎3:1 𝑎3:3  

𝑎3:2 𝑎3:3 



𝑎4:3 𝑎4:4  

𝑎4:4  

𝑎4:4 



𝑎5:5  

𝑎5:1 𝑎5:3 𝑎5:5  

𝑎5:4 𝑎5:5 



𝑎6:5 𝑎6:6  

𝑎6:6  

𝑎6:6 



𝑎7:7 

𝑎7:1 𝑎7:3 𝑎7:5 𝑎7:7 

𝑎7:6 𝑎7:7

NotethatStage1andStage3require𝑂(𝑇)work,whileStage2reducestoaself-similarproblemofhalfthesize.Itiseasy
tocheckthatthisrequires𝑂(𝑇)totalworkand𝑂(log𝑇)depth/span.
Remark10. Infact,itispossibletoseethatthecomputationgraphofthisalgorithmisidenticaltothatoftheassociative
scanalgorithmdescribedinAppendixB.2.2. Thekeytakeawayisthatinsteadofthestepsof(1)recognizingthat𝑀 definesa
recurrence(2)observingthattherecurrencecanbedefinedwithanassociativebinaryoperator;thereisacompletelydifferent
perspectiveofsimplyfindingastructuredmatrixdecompositionalgorithmfor𝑀.
C Theory Details
C.1 Extras: ClosurePropertiesofSSMs
Wepresentheresomeadditionalpropertiesofsemiseparablematricestoillustratetheirflexibilityandutility.Thissection
isnotnecessarytounderstandourcoreresults.
PropositionC.1(SemiseparableClosureProperties). Semiseparablematricesareclosedunderseveralprimitiveoperations.
• Addition:Thesumofan𝑁-SSand𝑃-SSmatrixisatmost(𝑁 +𝑃)-SS.
• Multiplication:Theproductofan𝑁-SSand𝑃-SSmatrixis(𝑁 +𝑃)-SS.
• Inverse:Theinverseofan𝑁-SSmatrixisatmost(𝑁 +1)-SS.
Theadditionandmultiplicationpropertiesareeasilyseen. Theinversepropertyhasmanyproofs;oneapproachfollows
immediately from the Woodbury inversion identity, which has also featured prominently in the structured SSM litera-
ture(Gu,Goel,andRé2022).
Inturn,theseimplyclosurepropertiesofstatespacemodels.
Forexample,theadditionpropertysaysthatsummingtwoparallelSSMmodelsisstillanSSM.Themultiplicationproperty
saysthatsequentiallycomposingorchainingtwoSSMscanstillbeviewedasanSSM,whosetotalstatesizeisadditive–a
somewhatnontrivialfact.
Finally, the inverse property can let us relate SSMs to other types of models. For example, one can notice that banded
matrices are semiseparable, so their inverses are semiseparable. (In fact, the semiseparable family of structure is often
motivated by taking inverses of banded matrices (Vandebril et al. 2005)). Moreover, the fast recurrence properties of
semiseparablematricescanbeviewedasaconsequenceoftheirinversebeingbanded.
Remark11. Thefactthat1-SSmatricesaresimplerecurrences(7)areequivalenttothefactthattheinverseofa1-SSmatrix
48isa2-bandedmatrix:
1 1 −1
   


𝑎
1
1 

 −𝑎
1
1 

𝑀 =


𝑎 2𝑎
1
𝑎
2
1 
 =


0 −𝑎
2
1 

  . . . . . . ... ...     . . . . . . ... ...  
   
 𝑎 𝑇−1...𝑎
1
𝑎 𝑇−1...𝑎
2
... 𝑎
𝑇−1
1



0 0 ... −𝑎
𝑇−1
1

   
Thus𝑦 =𝑀𝑥 ↔𝑀−1𝑦 =𝑥,or
1
 
 −𝑎
1
1 



0 −𝑎
2
1 
𝑦 =𝑥.
  . . . . . . ... ...  
 


0 0 ... −𝑎
𝑇−1
1

 
Orelementwise,
𝑦
𝑡
−𝑎 𝑡𝑦
𝑡−1
=𝑥
𝑡
𝑦
𝑡
=𝑎 𝑡𝑦 𝑡−1+𝑥 𝑡.
Conversely,wealsousetheseclosureresultstoprovethatautoregressivestructuredattention(undercertainassumptions)
mustbeSSMs,allowingustoshowthatmoregeneralfamiliesofefficientsequencemodelsincludingattentionvariants
canbereducedtostatespacemodels(AppendixC.2).
C.2 AutoregressiveMaskedAttentionisSemiseparable-StructuredAttention
WeproveTheorem5.2fromSection5.2.InSection4.3wedefinedstructuredattentionasabroadgeneralizationofmasked
attention,wherethepropertyofefficiency(i.e.alinear-timeformforthekernelattention)isabstractedintotheefficiency
ofstructuredmatrixmultiplication.However,beyondcomputationalefficiency,standardlinearattention(Katharopoulos
et al. 2020) also has two important properties. First, it is causal, which is required for settings such as autoregressive
modeling. Moreover,ithasefficientautoregressivegeneration. Inotherwords,thecostofanautoregressivestep–i.e.the
incrementalcostofcomputingtheoutput𝑦 uponseeing𝑥 ,giventhat𝑥 hasalreadybeenseenandpreprocessed–
𝑇 𝑇 0:𝑇
requiresonlyconstanttime.
HerewecharacterizewhichinstancesofSMAhaveefficientautoregression.
IntheframeworkofSMA,causalityisequivalenttotheconstraintthatthemask𝐿isalower-triangular matrix.
Characterizingthespaceof𝐿matricesthathaveefficientautoregressionismoredifficult.Wewilluseanarrowtechnical
definition of autoregressive processes, in the spirit of classical definitions from the time series literature (e.g. ARIMA
processes(Boxetal.2015)).
Definition C.2. We define an autoregressive transformation𝑥 ∈ R𝑇 ↦→ 𝑦 ∈ R𝑇 of order𝑘 as one where each output𝑦 𝑡
dependsonlyonthecurrentinputandlast𝑘 outputs:
𝑦
𝑡
=𝜇 𝑡𝑥
𝑡
+ℓ 𝑡1𝑦 𝑡−1+···+ℓ 𝑡𝑘𝑦 𝑡−𝑘. (23)
Notethatthe casewhere𝐿 isthecumsum matrixisaspecial casewith𝑘 = 1 andthus𝑦 𝑡 = 𝑥 𝑡 +𝑦 𝑡1. With thisdefini-
tion, characterizingthespaceofefficientautoregressivelineartransformsfollowsfromthepropertiesofsemiseparable
matrices.TheoremC.3formalizesandprovesTheorem5.2.
TheoremC.3. Let𝐿 ∈R𝑇×𝑇 beanefficientautoregressivetransformationoforder𝑘.Then𝐿isastatespacemodeloforder
𝑘+1.
Proof. Let(𝑥,𝑦)beinputandoutputsequences,sothat𝑦 =𝐿𝑥.Rearrangingthedefinition(23),
𝑦
𝑡
−ℓ 𝑡1𝑦 𝑡−1−···−ℓ 𝑡𝑘𝑦
𝑡−𝑘
=𝜇 𝑡𝑥 𝑡.
49Vectorizingover𝑡,thiscanbeexpressedasamatrixtransformation
1 𝑦 𝜇 𝑥
   0   0   0 
 −ℓ
𝑡1
1 



𝑦
1




𝜇
1




𝑥
1


  . . . ... ...     . . .     ...     . . .  
  −ℓ
𝑡𝑘
... −ℓ
𝑡1
1  

 

𝑦
𝑘
 

=  

𝜇
𝑘
 

 

𝑥
𝑘
  .
  . . . ... . . . ... ...     . . .     ...     . . .  
       


0 ... −ℓ
𝑇−1,𝑘
... −ℓ
𝑇−1,1
1

 𝑦 𝑇−1



𝜇 𝑇−1

 𝑥 𝑇−1

       
The𝜇 diagonalmatrixcanbemovedtotheleftandfoldedintothematrixofℓ coefficients,whichremainsa𝑘 +1-band
lower-triangularmatrix.Butwealsohave𝐿−1𝑦 =𝑥,so𝐿istheinverseofthismatrix.
Next,notethat𝑘+1-bandmatricesare𝑘+1-semiseparablebytherankcharacterizationofsemiseparability(Definition3.1).
ByPropositionC.1,theinverse𝐿isthereforeatmost𝑘+2-semiseparable.Aslightlystrongerboundof𝑘+1canbeobtained
becauseoftheadditionalstructureofbandedmatrices.Finally,thecharacterizationof𝐿asanorder-𝑘+1statespacemodel
followsfromTheorem3.5. □
Inotherwords,efficientautoregressiveattentionissemiseparableSMA.
D Experimental Details
D.1 MQARDetails
WeuseaharderversionofthetaskintroducedinBased(Arora,Eyuboglu,Zhang,etal.2024)wheretokensthatarenot
query/key/values are replaced with random tokens. We also use more key-value pairs, longer sequences, and smaller
modelsizesthantheusualvariantofMQARusedbypriorwork,allofwhichmakethetaskharder.
Foreachsequencelength𝑇 ∈ {256,512,1024},weuse𝑇/4key-valuepairs.Thetotalvocabsizeis8192.
Weuseaformofcurriculumtrainingwheretrainingcyclesthroughdatasetsusing(𝑇/32,𝑇/16,𝑇/8,𝑇/4)key-valuepairs,
whereeachdatasethas218 ≈250000examples,foratotalof8epochsthrougheachdataset(totalof228 ≈270𝑀examples).
Thetotalbatchsizeis218 ≈0.25𝑀 tokens(e.g.for𝑇 =1024,thebatchsizeis256).
Allmethodsuse2layerswithdefaultsettings;theattentionbaselineadditionallyreceivespositionalembeddings.Foreach
method,wesweepovermodeldimensionsD = {32,64,128,256}andlearningrates{10−3.5,10−2,10−2.5}. Weusealinear
decayschedulethatdropsoneveryepoch(e.g. thelastepochwouldhavealearningrate1/8ofthemaximum/starting
learningrate).
D.2 ScalingLawDetails
AllmodelsweretrainedonthePile.Forthescalinglawexperiments,weusetheGPT2tokenizer.
Model Sizes. Table 9 specifies the model sizes we use for scaling laws following GPT3 (Brown et al. 2020), First, we
changedthebatchsizeofthe1.3Bmodelfrom1Mtokensto0.5Mtokensforuniformity.Second,wechangedthenumber
of training steps and total tokens to roughly match Chinchilla scaling laws (Hoffmann et al. 2022), which specify that
trainingtokensshouldincreaseproportionallytomodelsize.
TrainingRecipes. AllmodelsusedtheAdamWoptimizerwith
• gradientclipvalue1.0
• weightdecay0.1
• nodropout
• linearlearningratewarmupwithcosinedecay
50Table9: (ScalingLawModelSizes.) Ourmodelsizesandhyperparametersforscalingexperiments. (Modeldimension
andnumberofheadsappliesonlytoTransformermodels.)
Params n_layers d_model n_heads/d_head Trainingsteps LearningRate BatchSize Tokens
125M 12 768 12/64 4800 6e-4 0.5Mtokens 2.5B
350M 24 1024 16/64 13500 3e-4 0.5Mtokens 7B
760M 24 1536 16/96 29000 2.5e-4 0.5Mtokens 15B
1.3B 24 2048 32/64 50000 2e-4 0.5Mtokens 26B
Bydefault,thepeaklearningrateistheGPT3specification.
ComparedtoGPT3recipe,weusean“improvedrecipe”,inspiredbychangesadoptedbypopularlargelanguagemodels
suchasPaLM(Chowdheryetal.2023)andLLaMa(Touvron,Lavril,etal.2023).Theseinclude:
• linearlearningratewarmupwithcosinedecayto1𝑒−5,withapeakvalueof5×theGPT3value
• nolinearbiasterms
• RMSNorminsteadofLayerNorm
• AdamWhyperparameter𝛽 = (.9,.95)(theGPT3value)insteadofthePyTorchdefaultof𝛽 = (.9,.999)
D.3 DownstreamEvaluationDetails
To evaluate downstream performance of fully trained, we train Mamba-2 on 300B tokens on the Pile, using the GPT-
NeoX(Blacketal.2022)tokenizer.
Weusethesamehyperparametersasthescalingexperiments,exceptwithbatchsize1Mforthe1.3Band2.7Bmodel.For
the2.7Bmodel,wealsofollowGPT3specification(32layers,dimension2560).
Forallmodels,weuse5xthelearningrateofthecorrespondingGPT3model.
For downstreamevaluation, we use theLM evaluation harnessfrom EleutherAI (L. Gao, Tow, et al. 2021), on the same
tasksasMamba(GuandDao2023)withoneadditionalone:
• LAMBADA(Papernoetal.2016)
• HellaSwag(Zellersetal.2019)
• PIQA(Bisketal.2020)
• ARC-challenge(P.Clarketal.2018)
• ARC-easy:aneasysubsetofARC-challenge
• WinoGrande(Sakaguchietal.2021)
• OpenBookQA(Mihaylovetal.2018)
D.4 AblationDetails
(Re)BasedDetails. OurablationsinSection9.4.3consideredtheBased(Arora,Eyuboglu,Zhang,etal.2024)andRe-
Based(Aksenovetal.2024)models.
BasedapproximatestheexpkernelwithaquadraticTaylorexpansionexp(𝑥) ≈1+𝑥+𝑥2/2,whichcanbeaccomplished
bythefeaturemap
√
𝜓 Taylor(𝑥) =concatenate(1,𝑥,1/ 2𝑥 ⊗𝑥).
ReBasedproposestousethesimplerfeaturemap𝜓 Quadratic(𝑥) =𝑥⊗𝑥 correspondingtothekerneltransformation𝑥2,but
alsoappliesalayernormalizationbeforehand.Weviewthelayernormalizationasanalternativenon-linearactivationto
ourdefaultSwishactivation,andablatecombinationsofthese.
51Table10:(Zero-shotEvaluations.)Bestresultsforeachsizeinbold,secondbestunlined.WecompareagainstopensourceLMswith
varioustokenizers,trainedforupto300Btokens.Pilereferstothevalidationsplit,comparingonlyagainstmodelstrainedonthesame
datasetandtokenizer(GPT-NeoX-20B).Foreachmodelsize,Mamba-2outperformsMamba,andgenerallymatchesPythiaattwicethe
modelsize.
Model Token. Pile LAMBADA LAMBADA HellaSwag PIQA Arc-E Arc-C WinoGrande OpenbookQA Average
ppl↓ ppl↓ acc↑ acc↑ acc↑ acc↑ acc↑ acc↑ acc↑ acc↑
HybridH3-130M GPT2 — 89.48 25.8 31.7 64.2 44.4 24.2 50.6 27.0 38.2
Pythia-160M NeoX 29.64 38.10 33.0 30.2 61.4 43.2 24.1 51.9 29.2 39.0
Mamba-130M NeoX 10.56 16.07 44.3 35.2 64.5 48.0 24.2 51.9 28.8 42.4
Mamba-2-130M NeoX 10.48 16.86 43.9 35.3 64.9 47.4 24.2 52.1 30.6 42.6
HybridH3-360M GPT2 — 12.58 48.0 41.5 68.1 51.4 24.7 54.1 31.6 45.6
Pythia-410M NeoX 9.95 10.84 51.4 40.6 66.9 52.1 24.6 53.8 30.0 45.6
Mamba-370M NeoX 8.28 8.14 55.6 46.5 69.5 55.1 28.0 55.3 30.8 48.7
Mamba-2-370M NeoX 8.21 8.02 55.8 46.9 70.5 54.9 26.9 55.7 32.4 49.0
Pythia-1B NeoX 7.82 7.92 56.1 47.2 70.7 57.0 27.1 53.5 31.4 49.0
Mamba-790M NeoX 7.33 6.02 62.7 55.1 72.1 61.2 29.5 56.1 34.2 53.0
Mamba-2-780M NeoX 7.26 5.86 61.7 54.9 72.0 61.0 28.5 60.2 36.2 53.5
GPT-Neo1.3B GPT2 — 7.50 57.2 48.9 71.1 56.2 25.9 54.9 33.6 49.7
HybridH3-1.3B GPT2 — 11.25 49.6 52.6 71.3 59.2 28.1 56.9 34.4 50.3
OPT-1.3B OPT — 6.64 58.0 53.7 72.4 56.7 29.6 59.5 33.2 51.9
Pythia-1.4B NeoX 7.51 6.08 61.7 52.1 71.0 60.5 28.5 57.2 30.8 51.7
RWKV4-1.5B NeoX 7.70 7.04 56.4 52.5 72.4 60.5 29.4 54.6 34.0 51.4
Mamba-1.4B NeoX 6.80 5.04 65.0 59.1 74.2 65.5 32.8 61.5 36.4 56.4
Mamba-2-1.3B NeoX 6.66 5.02 65.7 59.9 73.2 64.3 33.3 60.9 37.8 56.4
GPT-Neo2.7B GPT2 — 5.63 62.2 55.8 72.1 61.1 30.2 57.6 33.2 53.2
HybridH3-2.7B GPT2 — 7.92 55.7 59.7 73.3 65.6 32.3 61.4 33.6 54.5
OPT-2.7B OPT — 5.12 63.6 60.6 74.8 60.8 31.3 61.0 35.2 55.3
Pythia-2.8B NeoX 6.73 5.04 64.7 59.3 74.0 64.1 32.9 59.7 35.2 55.7
RWKV4-3B NeoX 7.00 5.24 63.9 59.6 73.7 67.8 33.1 59.6 37.0 56.4
Mamba-2.8B NeoX 6.22 4.23 69.2 66.1 75.2 69.7 36.3 63.5 39.6 59.9
Mamba-2-2.7B NeoX 6.09 4.10 69.7 66.6 76.4 69.6 36.4 64.0 38.8 60.2
GPT-J-6B GPT2 – 4.10 68.3 66.3 75.4 67.0 36.6 64.1 38.2 59.4
OPT-6.7B OPT – 4.25 67.7 67.2 76.3 65.6 34.9 65.5 37.4 59.2
Pythia-6.9B NeoX 6.51 4.45 67.1 64.0 75.2 67.3 35.5 61.3 38.0 58.3
RWKV4-7.4B NeoX 6.31 4.38 67.2 65.5 76.1 67.8 37.5 61.0 40.2 59.3
Notethatbecausetheseexpandthefeaturedimension,wemustprojecttosmaller𝐵,𝐶 dimensions;inTable7,usestate
size𝑁 =64for130Mmodelsand𝑁 =256for380Mmodels.Forthe(Re)Basedmethods,weprojectto8and16dimensions
respectivelybeforeapplyingtheirfeaturemaps;thisresultsinatotalstatesizeof82 =64forReBasedand1+8+82 =73
forBasedinthe130Mmodelcase.Becausethe𝐵and𝐶 projectionsaresmaller,thesemethodsusefewerparameters,and
weadjustthelayercountappropriately.
52