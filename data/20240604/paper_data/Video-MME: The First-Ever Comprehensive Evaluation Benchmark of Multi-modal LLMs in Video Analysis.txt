Video-MME: The First-Ever Comprehensive
Evaluation Benchmark of Multi-modal
LLMs in Video Analysis
ChaoyouFu♠,YuhanDai1,YongdongLuo2,LeiLi3,ShuhuaiRen4
RenruiZhang5,ZihanWang6,ChenyuZhou2,YunhangShen,MengdanZhang
PeixianChen,YanweiLi5,ShaohuiLin6,SiruiZhao1,KeLi
TongXu1XiawuZheng2,EnhongChen1,RongrongJi2,XingSun†
1USTC,2XMU,3HKU,4PKU,5CUHK,6ECNU
♠ProjectLeader †CorrespondingAuthor
Abstract
Inthequestforartificialgeneralintelligence,Multi-modalLargeLanguageMod-
els(MLLMs)haveemergedasafocalpointinrecentadvancements. However,
the predominant focus remains on developing their capabilities in static image
understanding. ThepotentialofMLLMsinprocessingsequentialvisualdatais
stillinsufficientlyexplored, highlightingtheabsenceofacomprehensive, high-
qualityassessmentoftheirperformance. Inthispaper,weintroduceVideo-MME,
the first-ever full-spectrum, Multi-Modal Evaluation benchmark of MLLMs in
Videoanalysis. Ourworkdistinguishesfromexistingbenchmarksthroughfour
keyfeatures: 1)Diversityinvideotypes,spanning6primaryvisualdomainswith
30subfieldstoensurebroadscenariogeneralizability;2)Durationintemporal
dimension,encompassingbothshort-,medium-,andlong-termvideos,ranging
from11secondsto1hour, forrobustcontextualdynamics; 3)Breadthindata
modalities,integratingmulti-modalinputsbesidesvideoframes,includingsubtitles
andaudios,tounveiltheall-roundcapabilitiesofMLLMs;4)Qualityinannota-
tions,utilizingrigorousmanuallabelingbyexpertannotatorstofacilitateprecise
andreliablemodelassessment. 900videoswithatotalof256hoursaremanually
selectedandannotatedbyrepeatedlyviewingallthevideocontent,resultingin
2,700question-answerpairs. WithVideo-MME,weextensivelyevaluatevarious
state-of-the-artMLLMs,includingGPT-4seriesandGemini1.5Pro,aswellas
open-sourceimagemodelslikeInternVL-Chat-V1.5andvideomodelslikeLLaVA-
NeXT-Video. OurexperimentsrevealthatGemini1.5Proisthebest-performing
commercialmodel,significantlyoutperformingtheopen-sourcemodelswithan
average accuracy of 75.7%, compared to 52.5% for LLaVA-NeXT-Video. The
resultsalsodemonstratethatVideo-MMEisauniversalbenchmark,whichapplies
tobothimageandvideoMLLMs. Furtheranalysisindicatesthatsubtitleandaudio
informationcouldsignificantlyenhancevideounderstanding. Besides,adeclinein
MLLMperformanceisobservedasvideodurationincreasesforallmodels. Our
datasetalongwiththesefindingsunderscorestheneedforfurtherimprovementsin
handlinglongersequencesandmulti-modaldata,sheddinglightonfutureMLLM
development. Projectpage: https://video-mme.github.io.
Email:{bradyfu24,winfred.sun}@gmail.com
4202
yaM
13
]VC.sc[
1v57012.5042:viXraVideo-MME
On whatdatedidtheindividualinthevideoleaveaplacethatSimonthoughtwasveryimportanttohim?
A.May31,2022. B.June9,2021. C.May9,2021. D.June31,2021.
The date of Day 1is May 31, 2021. Simon is the camera man. Yosemite National Park did mean Depart Yosemite on Day 10.
[in Frames] [in Frames] a lot more to Simon. [in Subs/Audio] [in Frames]
01:10 02:22 youtF uu .bll e V /Vid Fe no t oL Bin Rk G: F1A 04:12 27:52 31:16
Video-MME
How did the man wearing a bandage and holding an envelop, who appeared in the latter part of this video, sustain his injury?
A. One of his hands was hit by a firework while he was setting it off.
B. His arms got injured while he was attempting to put out the fire at a burning house.
C. His hands were injured from falling down to the ground while he was chasing Wayne’s motorcycle.
D. One of his arms was dragged down by a dog lured with food by Wayne, while he was insulting Wayne's father.
Dragged down by a dog. The man wearing a bandage Chasing Wayne’s motorcycle. A burning house. Hit by a firework.
[Option D] and holding an envelope. [Option C] [Option B] [Option A]
03:35 Full Video Link: youtu.be/p84O3JAp_IM 27:30 27:58 28:10 30:35
Figure 1: Examples of Video-MME. The ground-truth answer is highlighted in green. In Video-
MME,alldata,includingquestion-answeringannotations,videos,subtitles,andaudios,aremanually
collectedandcurated,ensuringdiversityandquality.
1 Introduction
Therapiddevelopmentofmulti-modallargelanguagemodels(MLLMs)inrecentyears[60,45,15,47,
3,66]hashighlightedtheirimpressiveperceptionandcognitivecapabilitiesacrossvariousmultimodal
benchmarks[13,61,39,67].TheseadvancementsdemonstratethegreatpotentialofMLLMstoserve
asafoundationthatcandigestthemulti-modalrealworld[31]andpavethewaytowardartificial
generalintelligence. However,currentMLLMsandtheirevaluationprimarilyfocusonstaticvisual
dataunderstanding,whichfailstocapturethedynamicnatureoftherealworldinvolvingcomplex
interactionsbetweenobjectsovertime. Toapproximatereal-worldscenariosmoreaccurately,itis
crucialtoexploreandassessthecapabilitiesofMLLMsonsequentialvisualdata,suchasvideos.
Manyearlyefforts[65,50,24,28]havebeenmadetoinspirethevideounderstandingpotentialsof
MLLMswithpromisingresults. However,existingvideo-basedbenchmarks[27,24,42,36]arestill
limitedtothoroughlyrevealtheirperformance,suchasalackofdiversityinvideotypes,insufficient
coverageoftemporaldynamics,andthenarrowfocusonasinglemodality. Theseinevitablyhinder
theall-aroundevaluationofMLLMs.
To this end, we introduce Video-MME, the first-ever comprehensive Multi-Modal Evaluation
benchmark crafted for MLLMs in Video analysis. As exemplified in Figure 1, we meticulously
curate a dataset of 900 videos across various scenarios, and annotate a set of 2,700 high-quality
multiple-choicequestions(3pervideo)tofosterarobustevaluation. AspresentedinFigure2,for
generalizability,ourdatasetwidelyspans6visualdomains,includingKnowledge,Film&Television,
Sports Competition, Artistic Performance, Life Record, and Multilingual, with 30 fine-grained
categories,e.g.,astronomy,technology,documentary,newsreport,esports,magicshow,andfashion.
Importantly,thevideosvarysignificantlyinlength,rangingfrom11secondsto1hour,specifically
evaluatingtheadaptabilityofMLLMsacrossvaryingtemporalcontexts. Furthermore,Video-MME
enrichestheassessmentbyincorporatingtheassociatedsubtitlesandaudiotracks,therebyenhancing
theanalysisofmulti-modalinputsforvideounderstanding.
2Figure2: (Left)Videocategories. Ourbenchmarkcovers6keydomainsand30sub-classvideotypes.
(Right)Videodurationlengthandquestiontypedistributions. Video-MMEhasafullspectrumof
videolengthandcoversdifferentcoreabilitiesofMLLMs.
UsingVideo-MME,webenchmarkvariousstate-of-the-artMLLMs,includingGPT-4V[45],GPT-
4o[46],andGemini1.5Pro[51],alongsideopen-sourceimagemodelslikeInternVL-Chat-V1.5[9]
andvideomodelslikeLLaVA-NeXT-Video[68]. OurexperimentsinTable4indicatethatGemini
1.5Proisthehighest-performingcommercialmodel,achievinganaverageaccuracyof75.7%. In
comparison,open-sourceMLLMsexhibitsubstantialgapscomparedtocommercialmodels. For
instance,theleadingopen-sourcemodel,LLaVA-NeXT-Video,attainsanoverallaccuracyof52.5%.
Thesefindingssuggestthereisconsiderableroomforimprovementintheopen-sourcecommunity.
Ourbenchmarkisalsoavailabletoadvancedimage-basedmodelsbyextendingtheirinputtomulti-
frame images, e.g., Qwen-VL-Max [5] and InternVL-Chat-V1.5 [9]. The accuracies of both the
modelsreach51%,whichisclosetothatofthevideospecificmodelLLaVA-NeXT-Video,indicating
thatimageunderstandingisthebasisofvideounderstanding,andthewideapplicabilityofVideo-
MMEinthefiledofMLLMs. FurtherobservationsinTable5indicatethatintegratingsubtitlesand
audiossignificantlyenhancesvideocomprehensioncapabilities,e.g.,boostingGemini1.5Proby
5.9%and4.7%respectively,withthegainsbeingmorepronouncedforlongervideos. Afine-grained
analysisoftasktypesrevealsthatsubtitlesandaudiosareparticularlybeneficialforvideosrequiring
substantialdomainknowledge.WealsonoteageneraldeclineinMLLMperformancewithincreasing
videolength. Thistrendsuggeststhatlimitationsinprocessinglongervideosequencescouldbea
criticalbottleneckintheperformanceofMLLMs.
Finally,wediscusspromisingavenuesforimprovingthecapabilitiesofMLLMsinprocessingvideo
content. Potentialdirectionsincludearchitecturaldevelopmentforbetterhandlinglongcontextinputs
andconstructingtrainingdatafocusedoncomplextemporalreasoningscenarios. Weexpectthat
ourbenchmarking,evaluationfindings,detailedanalysis,andoutlinedinsightswillinspirefuture
progresstowardmorecapableandrobustMLLMs.
2 Video-MME
2.1 DatasetConstruction
ThedatasetconstructionprocessofVideo-MMEconsistsofthreesteps: videocollection,question-
answeringannotation,andqualityreview. Thedetailsareasfollows.
VideoCollection. Foracomprehensivecoverageofdifferentvideotypes,wefirstcreateadomain
hierarchyforcollectingrawvideosfromYouTube. Wedefine6keydomains: Knowledge, Film
&Television,SportsCompetition,LifeRecord,andMultilingual,basedonpopulartendencieson
3YouTube. Eachdomainisfurtherdividedintodetailedtags,suchasfootballandbasketballforsports
competition, resulting in a total of 30 fine-grained video classes. The full domain-tag hierarchy
anditsdistributioncanbefoundintheleftpartofFigure2. Foreachclass,wecollectvideoswith
varyingdurationlengths,includingshort(lessthan2minutes),medium(4-15minutes),andlong
videos(30-60minutes). Besides,wealsoobtaincorrespondingmeta-informationsuchassubtitles
(ifprovided)andaudiosforfurtherinvestigation. Ourfinaldatasetconsistsof900videosspanning
variousdomainswithrelativelybalanceddurationlengths,asdepictedintherightpartofFigure2.
Question-AnswerAnnotation. Aftergatheringtherawvideodata,weannotateitwithhigh-quality
question-answer(QA)pairstoevaluatetheproficiencyofMLLMsininterpretingvideocontent. We
employamultiple-choiceQAformattofacilitateastraightforwardandflexibleassessment. Almost
allauthorsofthisstudy,proficientinEnglishwithextensiveresearchexperienceinvision-language
learning,performtheannotations. Specifically,theyarefirstaskedtowatchthewholecontentofthe
video,andthentodevelop3correspondingquestions,eachwith4potentialoptionsbyrepeatedly
watchingthevideo,contributingto2,700QApairsintotal. Asshowninthebottomrightcornerof
Figure2,thereareatotalof12tasktypesinthequestions,includingbothperception,reasoning,and
informationsynopsis. Particularly,eachQApairisrequiredtobeassociatedwiththevideocontent,
avoidingMLLMsbeingabletoanswerwithoutlookingatthevideo.
QualityReview. Toguaranteethequalityofourdataset,weconductarigorousmanualreview
process. First,adifferentannotatorisassignedtoexamineeachQApairtoensurethat(i)language
expressionsarecorrectandunambiguous;(ii)thequestionisanswerable,andthecandidateoptions
andprovidedgoldenoptionarereasonable. Furthermore,toensurethatthequestionsarechallenging
enoughandrequirevideocontentasanecessarycondition[67],weprovidethetext-onlyquestionsto
Gemini1.5ProandfilteroutQApairsthatcanbeansweredsolelybasedonthetextualquestions.
Forexample,thequestion“Whatisthebiggestachievementof10ofArgentinain2022?” thatcan
be directly inferred to the World Cup winner will be filtered out during this process. Questions
thatdonotmeetthiscriterionarereturnedtotheoriginalannotatorsforrevision. Bystatistics,the
accuracy of Gemini 1.5 Pro in the question-only setting is less than 15%. Through our rigorous
datasetconstructionprocess,westrivetodeliverahigh-quality,diverse,andwell-balanceddataset
thatwillbeinstrumentalforresearchersinthefieldofmulti-modalunderstanding.
2.2 DatasetStatistics
Here,wepresentthedetailedstatisticsofourdatasettoprovideamorecomprehensiveunderstanding,
includingthemetainformation,QApairs,certificatelengths,qualitativeanalysis,andcomparisonto
previousworks.
Video&MetaInformation. Ourdatasetcomprisesatotalof900videos,713subtitles,and869
audiofiles. Mostvideosareaccompaniedbybothsubtitlesandaudios,providingvaluableresources
for investigating the impact of external information on video understanding performance. The
upperrightpartofFigure2illustratesthedurationdistributionofthecollectedvideos. Specifically,
withintheshortvideocategory,longervideosoccupyalargerportion. Formedium-lengthvideos,
the duration is more uniformly distributed. In the category of long videos, there is a long-tailed
distributionwherelongervideoshavefewersamples. ThebottomrightpartofFigure2showsthe
distributionoftasktypes. Shortervideospredominantlyinvolveperception-relatedtaskssuchas
actionandobjectrecognition. Incontrast,longervideosmainlyfeaturetasksrelatedtotemporal
reasoning. Overall,thisanalysishighlightsthatourdatasetcoversawiderangeofvideodurations
andvarioustasktypes,enablingacomprehensiveevaluationoftemporalunderstanding.
QAPair. Wedemonstratethelanguagediversityofthequestionsandanswersinourdataset.Table2
liststheaveragewordcountofthetextualfieldsinourdataset.Thewordcountsforquestions,options,
andanswersdisplaynotableconsistencyacrossdifferentvideolengths,suggestingauniformstyleof
QApairsinourdataset. Ontheotherhand,thewordcountforsubtitlesincreasessignificantlywith
thelengthofthevideos,e.g.,shortvideoshaveanaveragewordcountof200.9,whilethelongvideo
subsethavethecountupto6.5K.Thistrendindicatesthatlongervideoscontainmoreinformation,
asevidencedbytheincreasedvolumeofsubtitles. Theanalysisrevealsthatourquestionsarediverse,
andtheanswersarewell-balanced. Inaddition,thedistributionofthefouransweroptions(A/B/C/D)
followsanear-uniformdistribution(25.2%/27.2%/25.2%/22.3%),ensuringanunbiasedevaluation.
4Table1: AnalysisofCertificateLengthinseconds. Avg. V.L.: average
videolength,Med.C.L.:mediancertificatelength,Avg.C.L.:average
certificatelength.
Video Avg.V.L. Med.C.L. Avg.C.L.
EgoSchema[42] 180 ∼100 -
Short 82.5 26.0 28.8
Medium 582.9 167.3 169.7
Long 2385.5 890.7 967.7
Table2: AveragewordcountofdifferenttextualfieldsinVideo-MME.
Dataset Question Options Answer Subtitles
Short 11.5 17.1 4.0 200.9
Medium 11.5 17.1 4.0 1450.7
Long 14.4 31.1 7.60 6515.3
All 12.7 22.9 5.5 3086.5
Certificate Length Analysis. Inspired by EgoSchema [42], we adopt the certificate length to
analyzethetemporaldifficultyoftheQApairs. ThecertificateofagivenvideoQApairisdefined
astheminimumsetofsub-clipsofthevideothatarebothnecessaryandsufficienttoconvincea
humanverifierthatthemarkedannotationiscorrect. Thecertificatelengthiscalculatedasthesumof
thetemporallengthsofthesub-clipsidentified. Werandomlysample5videosfromeachclassand
calculatethecertificatelengthdistributionwithextraannotators. AsshowninTable1,ourdataset
yieldsamediancertificatelengthof26.0s,167.3s,and890.7sforshort,mediumandlongvideos,
respectively. ComparedwiththecertificatelengthofEgoSchema,ourmediumandlongvideosubset
requiresmuchlongervideocontentdigestiontoanswerthequestion. Tothebestofourknowledge,
thisanalysismakesourVideo-MMEthemostchallengingVideoQAdatasettodate.
QualitativeAnalysis. Buildingonourpreviousanalysis,wehaveestablishedthatourproposed
benchmark,Video-MME,isbothdiverseandchallenging,makingitanexemplarytestbedforMLLMs.
Figure1showcasesspecificcasesfromourVideo-MMEdatasettoillustratethis.
Inthefirstexample,themodelmustintegrateinformationfromvarioussources: visualdatafrom
videoframes(e.g.,“Day1isMay31,2021”)andauditory/subtitlecontent(e.g.,referringto“Yosemite
NationalPark”).Moreover,themodelisrequiredtoperformsimplearithmeticoperationstodetermine
theexactdeparturedate. Thismulti-modalandmulti-stepreasoninghighlightsthecomplexityand
high quality of our dataset. The second example involves a question placed towards the end of
avideo,withtheprovidedansweroptionsdispersedacrossdifferentsegmentsofthevideo. This
necessitates a comprehensive understanding of the entire content, which can be as long as 30
minutes. ThesehighlightedcasesunderscorethatourVideo-MMEdatasetismeticulouslydesigned
toposesignificantchallenges,therebyeffectivelyevaluatingthecompositionalvideounderstanding
capabilitiesofMLLMs.
Comparison with Previous Benchmarks. We compare the key difference of our dataset with
previousbenchmarksinTable3. Thefirstblockliststraditionalvideobenchmarks,whichtypically
focusonspecificdomainssuchasTVvideosorlackaclearhierarchy,makingthemlesssuitable
forcomprehensivelydiagnosingthelimitationsofMLLMs. Inthemiddleblock,althoughseveral
benchmarkslikeTempCompass[36]andMVBench[24]exploremulti-levelevaluationandsource
videosfrom opendomains, they stillonly cover videoswithshorter durations. Forexample, the
longest dataset, EgoSchema [42], includes videos up to 180 seconds, leaving the understanding
of longer videos unaddressed. Our Video-MME is the first manually annotated benchmark that
encompassesopen-domainvideoswithdurationsrangingfrom11secondsto1hour. Itevaluates
differentlevelsofvideounderstandingabilityandissupplementedwithmetainformationsuchas
subtitlesandaudios. ThiscomprehensiveapproachuniquelypositionsVideo-MMEtoadvancethe
evaluationanddevelopmentofMLLMs.
5Table3:Thecomparisonofvariousbenchmarksencompassesseveralkeyaspects:thetotalnumberof
videos(#Videos),thenumberofclips(#Clips),theaveragedurationofthevideos(Len.),thenumber
ofQApairs(#QAPairs),themethodofannotation(Anno.,M/Ameansthemanually/automatic
manner),theaveragenumberofQApairtokens(QATokens),theaveragenumberofsubtitletokens
(Sub. Tokens),whetherthevideoscovermultipledurationlevels(Multi-level),whetherthevideos
are sourced from a broad range of open domains (Open-domain), and whether provide subtitle
togetherwithaudioinformation(Sub.&Aud.). Video-MME-S/M/Ldenotestheshort/medium/long
part. Itisimportanttonotethatifadatasetincludesmultipletaskformats,ourcomparisonfocuses
solelyonthemultiple-choicesegment.
Benchmarks #Videos #Clips Len.(s) #QAPairs Anno. QATokens Sub.Tokens Multi-level Open-domain Sub.&Aud.
MSRVTT-QA 2,990 2,990 15.2 72,821 A 8.4 ✗ ✗ ✓ ✗
MSVD-QA 504 504 9.8 13,157 A 7.6 ✗ ✗ ✓ ✗
TGIF-QA[18] 9,575 9,575 3.0 8,506 A&M 20.5 ✗ ✗ ✓ ✗
ActivityNet-QA[62] 800 800 111.4 8,000 M 10.2 ✗ ✗ ✗ ✗
TVQA[21] 2,179 15,253 11.2 15,253 M 27.8 159.8 ✗ ✗ ✗
How2QA 1,166 2,852 15.3 2,852 M 16.9 31.1 ✗ ✓ ✗
STAR 914 7,098 11.9 7,098 A 19.5 ✗ ✗ ✓ ✗
NExT-QA[57] 1,000 1,000 39.5 8,564 A 25.3 ✗ ✗ ✓ ✗
MVBench[24] 3,641 3,641 16.0 4,000 A 27.3 ✗ ✗ ✓ ✗
Video-Bench[44] 5,917 5,917 56.0 17,036 A&M 21.3 ✗ ✗ ✓ ✗
EgoSchema[42] 5,063 5,063 180.0 5,063 A&M 126.8 ✗ ✗ ✗ ✗
AutoEval-Video[8] 327 327 14.6 327 M 11.9 ✗ ✗ ✓ ✗
TempCompass[36] 410 500 11.4 7,540 A&M 49.2 ✗ ✗ ✓ ✗
Video-MME-S 300 300 80.8 900 28.6 200.9
Video-MME-M 300 300 520.2 900 M 32.8 1450.7 ✓ ✓ ✓
Video-MME-L 300 300 2471.0 900 45.4 6515.3
Video-MME 900 900 1024.0 2,700 35.6 3066.5
3 Experiments
In this section, we evaluate a wide range of MLLMs on our Video-MME benchmark. We first
introducetheevaluationsettings,andthenpresentthequantitativeresultsforbothopen-sourceand
closed-sourcemodels. Finally,wepresentcasestudiestoprovideanintuitiveunderstanding,and
investigatetheeffectofthemodalityinformationanddurationlength.
3.1 Settings
Weconducttheevaluationon3commercialmodels,i.e.,GPT-4V,GPT-4o1,andGemini1.5Pro,
and5representativeopen-sourcevideoMLLMs,i.e.,Video-LLaVA,VideoChat2,ST-LLM,Chat-
UniVi-V1.5,andLLaVA-NeXT-Video. Inaddition,wealsoincludeadvancedimageMLLMs,i.e.,
Qwen-VL-Chat/MaxandInternVL-Chat-V1.5,whichusuallyhavetheabilitytogeneralizetomulti-
imagescenarios. Wefollowtheirofficialconfigurationsandtrytousemoreframes2forevaluation. A
specialcaseisGemini1.5Pro,becauseitsupportsextremelylongmultimodalcontexts,sowetake
framespersecondforbothshortandmediumvideos. Forlongvideos,wecaptureaframeeverytwo
secondstoensurethestabilityoftheAPItesting. Withrespecttothesettingofaddingsubtitles,all
modelsexceptGemini1.5Prousethesubtitlescorrespondingtothesampledvideoframes. Gemini
1.5 Pro uses all subtitles due to the full video frame sampling. In addition, only Gemini 1.5 Pro
supportstheinputofaudiosbynow,whoseresultsarelistedinTable5.
The evaluation adopts the format of “whole video frames + whole subtitles/audios (optional) +
questionwithprompt”. Wetrytousethemodel’sdefaultpromptformultiple-choicequestions,butif
notweuseacommonpromptas:
Thisvideo’ssubtitlesarelistedbelow: [Subtitles]Selectthebestanswertothe
followingmultiple-choicequestionbasedonthevideo. Respondwithonlytheletter
(A,B,C,orD)ofthecorrectoption. [Question]Thebestansweris:
1SincethevideointerfaceofGPT-4ohasnotbeenreleasedyet,wesample10framesandevaluatethemodel
usingmultipleimagesasinput.
2Thenumbersofthesampledframesare10forGPT-4V/o,8forVideo-LLaVA,16forVideoChat2,64
forST-LLM,32forChat-UniVi-V1.5, 32forLLaVA-NeXT-Video, 4forQwen-VL-Chat/Max, and10for
InternVL-Chat-V1.5.
6Table4: PerformanceofMLLMsonVideo-MMEwithshort,medium,andlongdurations,underthe
settingsof“withoutsubtitles”and“withsubtitles”.
LLM Short(%) Medium(%) Long(%) Overall(%)
Models
Params
w/osubs w/subs w/osubs w/subs w/osubs w/subs w/osubs w/subs
Open&Closed-sourceImageMLLMs
Qwen-VL-Chat[5] 7B 46.4 47.1 38.1 39.8 38.0 38.3 40.9 41.7
Qwen-VL-Max[5] - 56.5 58.3 49.9 49.8 49.0 46.9 51.8 51.7
InternVL-Chat-V1.5[9] 20B 61.2 62.4 47.3 50.0 46.0 47.0 51.5 53.2
Open-sourceVideoMLLMs
Video-LLaVA[28] 7B 45.9 47.1 38.1 40.2 37.3 39.6 40.4 42.3
VideoChat2[24] 7B 38.2 41.6 33.2 34.3 29.7 31.9 33.7 35.9
ST-LLM[33] 7B 47.0 49.9 36.9 42.2 31.8 37.3 38.6 43.2
Chat-UniVi-V1.5[19] 7B 46.3 51.4 40.3 45.2 36.9 42.3 41.2 46.3
LLaVA-NeXT-Video[68] 34B 63.1 66.4 51.1 53.2 44.6 48.7 52.5 56.0
Closed-sourceMLLMs
GPT-4V[45] - 71.4 74.5 56.5 59.3 54.2 57.2 60.7 63.7
GPT-4o[46] - 77.1 77.5 62.1 63.0 59.2 56.7 66.2 65.8
Gemini1.5Pro[51] - 82.3 84.7 75.3 82.6 67.5 76.3 75.7 81.6
Table5: PerformanceofGemini1.5ProonsixmajorcategoriesofVideo-MME.Theinputmodality
includesframesonly,frameswithsubtitles,andframeswithaudios.
Category
Subset Modality
Sports Artistic Life
Knowledge Film&Television Multilingual Overall
Competition Performance Record
Frames 79.8 82.5 78.7 88.6 86.7 70.0 82.3
Short +Subs 85.6(+5.8) 87.5(+5.0) 77.3(-1.4) 87.6(-1.0) 85.7(-1.0) 86.7(+16.7) 84.7(+2.4)
+Audios 84.8(+5.0) 85.0(+2.5) 78.7(+0.0) 86.7(-1.9) 87.1(+0.4) 83.3(+13.3) 84.5(+2.2)
Frames 70.0 82.8 73.9 84.3 71.1 95.8 75.3
Medium +Subs 84.3(+14.3) 89.9(+7.1) 78.3(+4.4) 84.3(-) 77.5(+6.4) 91.7(+4.1) 82.6(+7.3)
+Audios 80.5(+10.5) 87.5(+4.7) 73.9(-) 82.4(-1.9) 78.5(+7.4) 100.0(+4.2) 80.6(+5.3)
Frames 74.6 70.1 58.1 60.0 66.7 66.7 67.5
Long +Subs 82.1(+7.5) 73.6(+2.5) 72.6(+14.5) 70.0(+10.0) 75.6(+8.9) 75.0(+8.3) 76.3(+8.8)
+Audios 81.9(+7.3) 75.6(+5.5) 71.9(+13.8) 66.7(+6.7) 67.2(+0.5) 83.3(+16.6) 74.7(+7.2)
Frames 75.0 79.1 71.1 78.5 76.5 76.9 75.7
Overall +Subs 84.0(+9.0) 84.3(+5.2) 76.3(+5.2) 81.1(+2.6) 80.4(+3.9) 84.6(+7.7) 81.6(+5.9)
+Audios 82.5(+7.5) 83.2(+4.1) 75.1(+4.0) 79.1(+0.6) 79.4(+2.9) 88.5(+11.6) 80.4(+4.7)
Consideringthetestsampleinourbenchmarkisamulti-choicequestionwith4options,wetake
accuracyastheevaluationmetric,therandomguessofwhichis25%. Theaccuracyiscalculatedby
matchingtheoutputofthemodelwiththerealone,withoutintroducinganythirdpartymodelsuch
asChatGPT.Theexampleofaccuracycalculationcanbefoundonourprojectpage.
3.2 QuantitativeResults
PerformanceofCommercialModels. Asoneofthepioneeringcommerciallargemodelsinte-
gratedwithvideocomprehensioncapabilities,Gemini1.5Prohasachievedthebestperformance
amongitspeersonVideo-MME.AsdepictedinFigure3,withvideoframesasinputalone,Gemini
1.5Proattainsanaccuracyof75.7%,surpassingGPT-4VandGPT-4oby15%and9.5%,respectively.
Table5showsthefine-grainedperformanceofGemini1.5Pro.Amongthesixmajorvideocategories,
Gemini 1.5 Pro performs the best in Film & Television, achieving an accuracy of 79.1%, while
performingthelowestintheSportsCompetitioncategory, withanaccuracyof71.1%. Asvideo
durationincreases,Gemini1.5Pro’sperformancedeclines(e.g.,−14.8%fromshorttolongvideos),
highlightingthemodel’sweaknessincapturinglong-rangetemporalrelationships. Nevertheless,
Gemini1.5Pro’sperformanceonlongvideosstillsurpassesallopen-sourcemodelsonshortvideos,
demonstratingitssuperiorcapabilities. Inadditiontovisualframeinput,Gemini1.5Pro’ssupportfor
additionalmodalities,includingsubtitlesandaudios,providesopportunitiesforfurtherperformance
improvement. Forexample,Table5displaysthatusingaudioscanincreaseaccuracyby7.2%for
longvideos,theimprovementinthemultilingualcategoryevenreaches16.6%. Wecanalsoseethat
theeffectofsubtitlesandaudiosisdifferentinthesesixcategories. Thesemotivatefutureresearchto
developversatilemodelsthatcansupportawiderrangeofmodalityinputs.
7PerformanceofOpen-sourcedModels. AsshowninTable4,amongthe7Bmodels,Chat-UniVi-
V1.5 achieves the best performance with 41.2%. LLaVA-NeXT-Video with 34B LLM achieves
an accuracy of 52.5%, demonstrating its stronger capabilities, especially in the tasks of attribute
perception,spatialreasoning,andinformationsynopsis,asexhibitedinFigure3. Nevertheless,there
still remains a significant gap between LLaVA-NeXT-Video and Gemini Pro 1.5, particularly in
counting problems, action recognition, and temporal perception, indicating substantial room for
improvement.Itisobservedthataddingsubtitlescanalsohelptheopen-sourcedmodels.Forexample,
theaccuracyofLLaVA-NeXT-Videoimprovesfrom52.5%to56.0%. Itisregrettablethatnoneof
theopen-sourcedmodelssupportaudioinput.
ApartfromvideoMLLMs, wealsoevaluatetheperformanceofimageMLLMsonVideo-MME.
Table4revealsthatimage-basedQwen-VL-MaxandInterVL-Chat-V1.5attaincomparableperfor-
mancetoLLaVA-NeXT-Video,demonstratingtheirsuperiorgeneralizationcapacityonsequential
data, and the universality of Video-MME in both image and video MLLMs. Meanwhile, it also
indicatesthatimageunderstandingisthefoundationofvideounderstanding.
Weconductqualitativeevaluation(usingframes
andsubtitles)onthetwocasesinFigure1. As
analyzed in Section 2.2, these two cases com-
prehensivelyexaminethemodel’scapabilities
in OCR, attribute perception, object recogni-
tion,andlong-rangetemporalreasoning,mak-
ing them highly challenging. For the date-
relatedquestioninCase1,Video-LLaVAiden-
tifies the date (May 31st) from the frame at
01:10 and subtitles, but fails to perform rea-
soningbasedoncontextandincorrectlydeter-
mines the year of the event, leading to the er-
roneous selection of option A. The remaining
open-sourcedmodelsmiscalculatethedate10
days after May 31st during the reasoning pro-
cess,resultingintheincorrectchoiceofoption
C.Fortheevent-relatedquestioninCase2,
Video-LLaVA,VideoChat2,andST-LLMincor-
rectly associate the target person with nearby
events,resultingintheselectionofincorrectop-
Figure3: Performanceon12typesoftasks.
tionsAorC.Incontrast,LLaVA-NeXT-Video
andGemini1.5Prosuccessfullyidentifiesthe
eventsexperiencedbythetargetpersonthroughoutthevideoanddemonstrateslong-distancetemporal
modelingcapabilities. Theycorrectlylinkthetargetperson’sinjuryat03:35withhisreappearanceat
27:30,identifyingthetruecauseoftheinjury(optionD).Insummary,thequestionsinourbenchmark
posesignificantchallengestothemodels,whichmotivatesMLLMstoadvanceboththeirperception
andreasoningcapabilities.
3.3 Analysis
Weconductfurtheranalysistoexplorethefactorsinfluencingthevideounderstandingperformance,
e.g.,additionalinformationandvideoduration.
Couldadditionalmodalitiesbenefittheperformance? Mostofevaluationsonlytakevideoframes
as input, requiring models to answer questions solely based on visual contexts. However, many
videosinherentlycontainextrainformationfromothermodalities,suchassubtitlesandaudios. To
understandtheirimpact,wevarythecombinationsofinputmodalitiesintheevaluation,andreport
resultsinTable5andFigure4. Wecandrawthefollowingobservations. (1)Introducingsubtitles
andaudioscanimprovetheresults. Forexample,inthemultilingualtaskshowninTable5,with
theadditionofaudios,Gemini1.5Proachieves+16.6%accuracyonlongvideoscomparedtothe
frame-onlysetting. Thisindicatesthatsubtitlesandaudiosprovidesomenecessaryinformationto
answerthequestions. (2)Subtitlesandaudiosprovidegreaterassistanceinunderstandinglong
videoscomparedtoshortvideos. Forexample, inTable5, comparedtoonlyusingframes, the
additionofsubtitlesimprovesthemodel’sperformanceby2.4%onshortvideosandby8.8%onlong
8Frames Frames + Subs Frames + Audio
Humanity & History Animation Movie & TV Show Documentary Basketball
100 100 100 100 100
80 80 80 80 80
60 60 60 60 60
40 40 40 40 40
20 20 20 20 20
S M L S M L S M L S M L S M L
Acrobatics Fashion Daily Life Exercise Multilingual
100 100 100 100 100
80 80 80 80 80
60 60 60 60 60
40 40 40 40 40
20 20 20 20 20
S M L S M L S M L S M L S M L
Figure4: Theimpactofincorporatingadditionalmodalityinputs,i.e.,subtitlesandaudios,onthe
performanceofGemini 1.5Proacrossdifferenttypes ofvideos. Weonlyshow theresultsof10
classeshere,andthoseofthewhole30classescanbefoundonourprojectpage.
videos. Thisisbecausetestsamplesoflongvideosincludemorechallengingreasoningquestions,
whichrequiresthemodeltoutilizesubtitlesandaudioinformationforaccurateresponses. (3)For
MLLMs,usingsubtitlesismoreeffectivethanaudio. Subtitlesareusuallytranscriptionsofaudio,
primarilycapturingspeechcontent,whileaudiosencompassmoreambientsounds. Ascanbeseen
fromTable5,whetheritisshort,medium,orlongvideos,subtitlesbringhigherimprovementthan
audios. TherearesomeexceptionsinTable5andFigure4, suchasMultilingual, whichmaybe
duetothequalityofthesubtitlesthemselves. Inaddition,theaudioalsoincludessomecontentthat
subtitlescannotexpress,suchassingingandintonation.
HowMLLMsarerobusttovariedvideoduration? InTable4, werespectivelycomparethe
performanceofdifferentmodelsonshort,medium,andlongvideos. Asvideodurationincreases,
bothopen-sourcedandcommercialmodelsexhibitasignificantdeclineinperformance. Forexample,
the accuracy of LLaVA-NeXT-Video drops by 12% and 18.5% from short to medium and long
videos,respectively,whileGemini1.5Pro’saccuracydecreasesby7%and14.8%. Therearethree
mainreasonsfortheperformancedecline. (1)Increasedproportionofdifficulttasks. Asshown
in the bottom right corner of Figure 2, test samples for long videos contain a higher proportion
of reasoning questions. These questions are more challenging than perception and recognition
tasks,thusposingagreaterchallengetothemodel’scapabilities. (2)Increasedsparsityinframe
sampling,leadingtoareductionineffectiveinputinformation. Ideally,forvideosofvarying
lengths,modelsshouldsamplevideoframesatafixedfpstoensureconsistentinformationdensity
in frame sequences [53, 52]. However, many open-sourced models [23, 28] fix the number of
inputframes,e.g.,8frames,resultinginexcessivelysparseinformationdensityasthevideolength
increases. This sparsity prevents the model from retaining all useful visual semantics, hindering
accuratepredictions. Introducingadditionalmodalities,e.g.,subtitles,caneffectivelysupplementthe
missinginformation[7]. (3)Increaseddifficultyinlongcontextunderstanding. AlthoughGemini
1.5Procorrespondinglyincreasesthenumberofsampledframesinthelongvideo,thereisstilla
significantperformancedegradation. Understandingthelongcontextofeithersingle-modality(LLM)
ormulti-modality(MLLM)isalwaysagreatchallenge.
4 Discussions
OurevaluationusingVideo-MMEhasrevealedseveralcriticalinsightsintothecurrentMLLMsand
highlightedareasforfutureimprovement. Here,wefurtherdiscusspotentialfuturedirections.
ImprovingLongContextModelingCapabilitiesofMLLMs. Oneofthesignificantchallenges
identified in our evaluation is the decline in performance as video duration increases. For open-
9sourcemodels,therestrictedinputframescanbecomeaninformationbottleneckforunderstanding
thefullcontentoflongvideos. Innovativeapproachestocontextextension,botharchitecturaland
infrastructural,areessential.Forinstance,exploringtechniqueslikeringattention[32],asinvestigated
by large world models [31], and training-free context extension methods could be beneficial [2].
Additionally,developingarchitecturessuchasatemporalQ-Formertoadaptivelyidentifykeyframes
inthevideoorcompressvideotokenstoreducecomputationaloverheadbasedonthequestionsposed
isalsoworthexploring[53,11]. Inessence,improvinglongcontextmodelingabilityiscrucialfor
thenextgenerationofMLLMstounderstandlongsequentialworlddynamicseffectively.
BuildingDatasetswithComplexTemporalUnderstanding. Ourevaluationalsohighlightsthe
demand for temporal reasoning oriented instruction-tuning datasets, considering that traditional
videodatasetswithshortvideoinputs,suchasMSRVTT-QAandActivityNet-QA.Althoughthere
have been efforts to construct high-quality datasets involving complex temporal reasoning over
long videos [42, 23], the availability of such datasets is still insufficient compared to the text
only[37,43]andimagedatasets[25,59]. Thelong-tailednatureofthisdatamakesitchallengingto
acquire. Effortstowardsbetterannotationparadigmssuchashuman-in-the-loopframeworks[27]
andautomaticdatasynthesizingexplorationsarecrucial[34]. Developingsuchdatasetscanbetter
leverageadvancedarchitecturalinnovationstoprovideMLLMswithsufficienttrainingsupervision
forarobustunderstandingofthetemporaldimensionofvideos.
5 RelatedWork
AdvancementsinMLLMs. RecentadvancementsinMLLMshaveseennotableprogress[60,14].
MLLMstypicallycomprisethreecoremodules: (i)avisionencoderforvisualfeatureextraction,(ii)
amodalityalignmentmoduletointegratevisualfeaturesintotheembeddingspaceofthelanguage
model,and(iii)anLLMbackbonefordecodingmulti-modalcontext. CLIP[49]andSigLIP[64]
arewidely-usedforimageencoding,whileLLaMA[55]andVicuna[10]serveaspopularchoices
forLLMs. Thealignmentmodulevariesfromsimplelinearprojections[30,69]tomorecomplex
architecturessuchasQ-Former[22,11],andgatedcross-attentionlayerssubstantiatedbyFlamingo
andIDEFICS[1,4]. Additionally,Fuyu-8B[6]introducesanovelframeworkmappingrawimage
pixels directly to the LLM embedding space. Regarding MLLMs for processing videos [23, 65,
40,54,29,41,19,20,58],thekeydifferenceliesinhowtheyencodethevideointovisiontokens
compatiblewiththeLLMs. RepresentativeworklikeVideo-LLaMA[65]firstusesaViT[12]with
animageQ-FormertoencodeindividualframesandthenemploysavideoQ-Formerfortemporal
modeling. VideoChat2[24]utilizesavideotransformertoencodevideofeaturesandsubsequently
implementsaQ-Former[22]tocompressvideotokens. ToempowervideoMLLMswithtemporal
localizationcapability[17,48,56],TimeChat[53]constructstime-sensitiveinstructiontuningdatasets
and encodes timestamp knowledge into visual tokens. VTimeLLM [16] proposes a LLaVA-like
three-stagetrainingmethod. However,thepotentialofMLLMsinprocessingsequentialvisualdatais
stillunder-explored. Therefore,weintroduceVideo-MMEforfull-spectrum,multi-modalevaluation
ofMLLMsinvideoanalysis.
MLLMBenchmarks. Alongsideadvancementsinarchitecture,significanteffortshavebeenmade
to improve benchmarking for MLLMs, guiding the development of the next generation of these
models. Previous studies have integrated various aspects of evaluation, such as perception and
cognitivecapabilities,tocreatecomprehensivebenchmarksforassessingimageMLLMs[13,61,
35]. As image MLLMs have demonstrated exceptional performance in general perception tasks,
benchmarksregardingscientificunderstanding[26],multi-modalmathematicalreasoning[38,67],
andmulti-disciplinary[63]capabilitieshavedrawnincreasingattention. ForvideoMLLMs,similar
effortshavebeenmadetoincorporateexistingbenchmarksforevaluatingvideounderstanding[24,44].
Giventhetemporalnatureofvideomodalities,specificbenchmarkshavebeendevelopedtoaddress
temporalunderstanding,highlightingthelimitationsofcurrentvideoMLLMsincomprehending
videocontent[27,36].
Inthiswork,weintroduceanewhigh-qualityvideounderstandingbenchmark,Video-MME.Com-
paredtopreviousbenchmarks,Video-MMEincludesadiversesetofvideosofvaryingdurations,
supplementedwithexternalmodalitiessuchasaudiosandsubtitles. Additionally,itfeatureshuman-
annotatedmulti-levelQApairs,providingacomprehensiveassessmentframeworkforMLLMs. Our
10resultsindicatethatopen-sourceMLLMsstillhavealargegapwithclosedmodels. Ouranalysisand
discussionfurthershedlightsonthefuturedevelopmentofMLLMs.
6 Conclusion
Inthispaper, wehaveintroducedVideo-MME,thefirstcomprehensivemulti-modalbenchmark
designed to evaluate MLLMs for video tasks. Our benchmark incorporates a diverse range of
video types, varying temporal durations, and multiple data modalities, all annotated with high-
quality, expert-labeledQApairs. Ourextensiveevaluationofstate-of-the-artMLLMs, including
commercialandopen-sourcemodels,highlightssignificantperformancedifferences. Commercial
models,particularlyGemini1.5Pro,demonstratesuperiorperformancecomparedtoopen-source
variants. Theintegrationofsubtitlesandaudiotrackssignificantlyenhancesvideounderstanding,
especiallyforlongervideos. However,weobserveageneraldeclineinperformanceasvideoduration
increases. Thesefindingsunderscoretheneedforfurtheradvancementsinhandlinglongermulti-
modaldata. WehopeVideo-MMEwillinspirefutureresearchanddevelopmentinimprovingthe
capabilitiesofMLLMs.
Acknowledgments
WeappreciatetheeffortsmadebyYuBai, FangyuanLiu, YigengJiang, andZezhongWuinthe
constructionofthebenchmark.
References
[1] J.-B.Alayrac,J.Donahue,P.Luc,A.Miech,I.Barr,Y.Hasson,K.Lenc,A.Mensch,K.Millican,
M.Reynolds,R.Ring,E.Rutherford,S.Cabi,T.Han,Z.Gong,S.Samangooei,M.Monteiro,
J.Menick,S.Borgeaud,A.Brock,A.Nematzadeh,S.Sharifzadeh,M.Binkowski,R.Barreira,
O.Vinyals,A.Zisserman,andK.Simonyan. Flamingo: avisuallanguagemodelforfew-shot
learning. ArXivpreprint,2022.
[2] C.An,F.Huang,J.Zhang,S.Gong,X.Qiu,C.Zhou,andL.Kong. Training-freelong-context
scalingoflargelanguagemodels. ArXivpreprint,2024.
[3] Anthropic. Theclaude3modelfamily: Opus,sonnet,haiku. 2024.
[4] A.Awadalla,I.Gao,J.Gardner,J.Hessel,Y.Hanafy,W.Zhu,K.Marathe,Y.Bitton,S.Gadre,
S. Sagawa, J. Jitsev, S. Kornblith, P. W. Koh, G. Ilharco, M. Wortsman, and L. Schmidt.
Openflamingo: Anopen-sourceframeworkfortraininglargeautoregressivevision-language
models. ArXivpreprint,2023.
[5] J.Bai,S.Bai,S.Yang,S.Wang,S.Tan,P.Wang,J.Lin,C.Zhou,andJ.Zhou. Qwen-vl: A
frontierlargevision-languagemodelwithversatileabilities. ArXivpreprint,2023.
[6] R.Bavishi,E.Elsen,C.Hawthorne,M.Nye,A.Odena,A.Somani,andS.Tas¸ırlar. Introducing
ourmultimodalmodels,2023.
[7] S.Chen,L.Li,S.Ren,R.Gao,Y.Liu,X.Bi,X.Sun,andL.Hou. Towardsmultimodalvideo
paragraphcaptioningmodelsrobusttomissingmodality. ArXivpreprint,2024.
[8] X. Chen, Y. Lin, Y. Zhang, and W. Huang. Autoeval-video: An automatic benchmark for
assessinglargevisionlanguagemodelsinopen-endedvideoquestionanswering. ArXivpreprint,
2023.
[9] Z.Chen,W.Wang,H.Tian,S.Ye,Z.Gao,E.Cui,W.Tong,K.Hu,J.Luo,Z.Ma,etal. How
fararewetogpt-4v? closingthegaptocommercialmultimodalmodelswithopen-sourcesuites.
ArXivpreprint,2024.
[10] W.-L.Chiang,Z.Li,Z.Lin,Y.Sheng,Z.Wu,H.Zhang,L.Zheng,S.Zhuang,Y.Zhuang,J.E.
Gonzalez,I.Stoica,andE.P.Xing. Vicuna: Anopen-sourcechatbotimpressinggpt-4with
90%*chatgptquality,2023.
11[11] W.Dai,J.Li,D.Li,A.M.H.Tiong,J.Zhao,W.Wang,B.Li,P.Fung,andS.Hoi. Instructblip:
Towardsgeneral-purposevision-languagemodelswithinstructiontuning. ArXivpreprint,2023.
[12] A.Dosovitskiy,L.Beyer,A.Kolesnikov,D.Weissenborn,X.Zhai,T.Unterthiner,M.Dehghani,
M.Minderer,G.Heigold,S.Gelly,J.Uszkoreit,andN.Houlsby. Animageisworth16x16
words: Transformersforimagerecognitionatscale. InICLR,2021.
[13] C.Fu,P.Chen,Y.Shen,Y.Qin,M.Zhang,X.Lin,Z.Qiu,W.Lin,J.Yang,X.Zheng,etal.
Mme: Acomprehensiveevaluationbenchmarkformultimodallargelanguagemodels. ArXiv
preprint,2023.
[14] C.Fu,R.Zhang,H.Lin,Z.Wang,T.Gao,Y.Luo,Y.Huang,Z.Zhang,L.Qiu,G.Ye,etal. A
challengertogpt-4v? earlyexplorationsofgeminiinvisualexpertise. ArXivpreprint,2023.
[15] GeminiTeam. Gemini: afamilyofhighlycapablemultimodalmodels. ArXivpreprint,2023.
[16] B.Huang,X.Wang,H.Chen,Z.Song,andW.Zhu. Vtimellm: Empowerllmtograspvideo
moments. ArXivpreprint,2023.
[17] D.-A. Huang, S. Liao, S. Radhakrishnan, H. Yin, P. Molchanov, Z. Yu, and J. Kautz. Lita:
Languageinstructedtemporal-localizationassistant. ArXivpreprint,2024.
[18] Y.Jang,Y.Song,Y.Yu,Y.Kim,andG.Kim. TGIF-QA:towardspatio-temporalreasoningin
visualquestionanswering. InCVPR,2017.
[19] P.Jin,R.Takanobu,C.Zhang,X.Cao,andL.Yuan. Chat-univi: Unifiedvisualrepresentation
empowerslargelanguagemodelswithimageandvideounderstanding. ArXivpreprint,2023.
[20] R.Jung,H.Go,J.Yi,J.Jang,D.Kim,J.Suh,A.S.Lee,C.Han,J.Lee,J.Kim,J.-Y.Kim,
J. Kim, K. Park, L. Lee, M. Ha, M. Seo, A. Jo, E. Park, H. Kianinejad, S. Kim, T. Moon,
W.Jeong,A.Popescu,E.Kim,E.Yoon,G.Heo,H.Choi,J.Kang,K.Han,N.Seo,S.Nguyen,
R.Won,Y.E.Park,A.Giuliani,D.Chung,H.Yoon,J.Le,J.Ahn,J.Lee,M.Saini,M.Sanders,
S.Lee,S.Kim,andT.Couture. Pegasus-v1technicalreport. ArXivpreprint,2024.
[21] J. Lei, L. Yu, M. Bansal, and T. Berg. TVQA: Localized, compositional video question
answering. InEMNLP,2018.
[22] J.Li,D.Li,S.Savarese,andS.C.H.Hoi. Blip-2: Bootstrappinglanguage-imagepre-training
withfrozenimageencodersandlargelanguagemodels. InICML,2023.
[23] K.Li,Y.He,Y.Wang,Y.Li,W.Wang,P.Luo,Y.Wang,L.Wang,andY.Qiao. Videochat:
Chat-centricvideounderstanding. ArXivpreprint,2023.
[24] K.Li,Y.Wang,Y.He,Y.Li,Y.Wang,Y.Liu,Z.Wang,J.Xu,G.Chen,P.Luo,etal. Mvbench:
Acomprehensivemulti-modalvideounderstandingbenchmark. ArXivpreprint,2023.
[25] L.Li,Y.Yin,S.Li,L.Chen,P.Wang,S.Ren,M.Li,Y.Yang,J.Xu,X.Sun,L.Kong,and
Q.Liu. M3IT:Alarge-scaledatasettowardsmulti-modalmultilingualinstructiontuning. ArXiv
preprint,2023.
[26] L.Li,Y.Wang,R.Xu,P.Wang,X.Feng,L.Kong,andQ.Liu. Multimodalarxiv: Adatasetfor
improvingscientificcomprehensionoflargevision-languagemodels. ArXivpreprint,2024.
[27] S.Li,L.Li,S.Ren,Y.Liu,Y.Liu,R.Gao,X.Sun,andL.Hou. Vitatecs: Adiagnosticdataset
fortemporalconceptunderstandingofvideo-languagemodels. ArXivpreprint,2023.
[28] B. Lin, B. Zhu, Y. Ye, M. Ning, P. Jin, and L. Yuan. Video-llava: Learning united visual
representationbyalignmentbeforeprojection. ArXivpreprint,2023.
[29] H.Liu,Q.Fan,T.Liu,L.Yang,Y.Tao,H.Huang,R.He,andH.Yang. Video-teller: Enhancing
cross-modalgenerationwithfusionanddecoupling. ArXivpreprint,2023.
[30] H.Liu,C.Li,Q.Wu,andY.J.Lee. Visualinstructiontuning. ArXivpreprint,2023.
12[31] H.Liu,W.Yan,M.Zaharia,andP.Abbeel. Worldmodelonmillion-lengthvideoandlanguage
withringattention. ArXivpreprint,2024.
[32] H.Liu,M.Zaharia,andP.Abbeel. Ringattentionwithblockwisetransformersfornear-infinite
context. InICLR,2024.
[33] R.Liu,C.Li,H.Tang,Y.Ge,Y.Shan,andG.Li. St-llm: Largelanguagemodelsareeffective
temporallearners. ArXivpreprint,2024.
[34] R.Liu,J.Wei,F.Liu,C.Si,Y.Zhang,J.Rao,S.Zheng,D.Peng,D.Yang,D.Zhou,etal. Best
practicesandlessonslearnedonsyntheticdataforlanguagemodels. ArXivpreprint,2024.
[35] Y.Liu,H.Duan,Y.Zhang,B.Li,S.Zhang,W.Zhao,Y.Yuan,J.Wang,C.He,Z.Liu,etal.
Mmbench: Isyourmulti-modalmodelanall-aroundplayer? ArXivpreprint,2023.
[36] Y.Liu,S.Li,Y.Liu,Y.Wang,S.Ren,L.Li,S.Chen,X.Sun,andL.Hou. Tempcompass: Do
videollmsreallyunderstandvideos? ArXivpreprint,2024.
[37] S.Longpre,L.Hou,T.Vu,A.Webson,H.W.Chung,Y.Tay,D.Zhou,Q.V.Le,B.Zoph,J.Wei,
etal. Theflancollection: Designingdataandmethodsforeffectiveinstructiontuning. ArXiv
preprint,2023.
[38] P.Lu,H.Bansal,T.Xia,J.Liu,C.Li,H.Hajishirzi,H.Cheng,K.-W.Chang,M.Galley,and
J.Gao. Mathvista: Evaluatingmathreasoninginvisualcontextswithgpt-4v,bard,andother
largemultimodalmodels. ArXivpreprint,2023.
[39] P.Lu,H.Bansal,T.Xia,J.Liu,C.yueLi,H.Hajishirzi,H.Cheng,K.-W.Chang,M.Galley,
andJ.Gao. Mathvista: Evaluatingmathreasoninginvisualcontextswithgpt-4v,bard,and
otherlargemultimodalmodels. ArXivpreprint,2023.
[40] R.Luo,Z.Zhao,M.Yang,J.Dong,M.-H.Qiu,P.Lu,T.Wang,andZ.Wei. Valley: Video
assistantwithlargelanguagemodelenhancedability. ArXivpreprint,2023.
[41] M. Maaz, H. Rasheed, S. Khan, and F. S. Khan. Video-chatgpt: Towards detailed video
understandingvialargevisionandlanguagemodels. ArXivpreprint,2023.
[42] K.Mangalam,R.Akshulakov,andJ.Malik. Egoschema: Adiagnosticbenchmarkforvery
long-formvideolanguageunderstanding. InNeurIPS,2024.
[43] S. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi. Cross-task generalization via natural
languagecrowdsourcinginstructions. InACL,2022.
[44] M. Ning, B. Zhu, Y. Xie, B. Lin, J. Cui, L. Yuan, D. Chen, and L. Yuan. Video-bench: A
comprehensivebenchmarkandtoolkitforevaluatingvideo-basedlargelanguagemodels. ArXiv
preprint,2023.
[45] OpenAI. GPT-4V(ision)systemcard,2023.
[46] OpenAI. GPT-4osystemcard,2024.
[47] A.Ormazabal,C.Zheng,C.d.M.d’Autume,D.Yogatama,D.Fu,D.Ong,E.Chen,E.Lam-
precht,H.Pham,I.Ong,etal. Rekacore,flash,andedge: Aseriesofpowerfulmultimodal
languagemodels. ArXivpreprint,2024.
[48] L.Qian,J.Li,Y.Wu,Y.Ye,H.Fei,T.-S.Chua,Y.Zhuang,andS.Tang. Momentor: Advancing
videolargelanguagemodelwithfine-grainedtemporalreasoning. ArXivpreprint,2024.
[49] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,
P.Mishkin,J.Clark,G.Krueger,andI.Sutskever. Learningtransferablevisualmodelsfrom
naturallanguagesupervision. InICML,2021.
[50] M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac, R. Soricut,
A.Lazaridou,O.Firat,J.Schrittwieser,etal. Gemini1.5: Unlockingmultimodalunderstanding
acrossmillionsoftokensofcontext. ArXivpreprint,2024.
13[51] M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. P. Lillicrap, J.-B. Alayrac, R. Soricut,
A.Lazaridou,O.Firat,J.Schrittwieser,I.Antonoglou,R.Anil,S.Borgeaud,A.M.Dai,K.Mil-
lican,E.Dyer,M.Glaese,T.Sottiaux,B.Lee,F.Viola,M.Reynolds,Y.Xu,J.Molloy,J.Chen,
M.Isard,P.Barham,T.Hennigan,R.McIlroy,M.Johnson,J.Schalkwyk,E.Collins,E.Ruther-
ford,E.Moreira,K.W.Ayoub,M.Goel,C.Meyer,G.Thornton,Z.Yang,H.Michalewski,
Z.Abbas,ande.NathanSchucher. Gemini1.5: Unlockingmultimodalunderstandingacross
millionsoftokensofcontext. ArXivpreprint,2024.
[52] S.Ren,S.Chen,S.Li,X.Sun,andL.Hou. TESTA:Temporal-spatialtokenaggregationfor
long-formvideo-languageunderstanding. InFindingsofEMNLP,2023.
[53] S. Ren, L. Yao, S. Li, X. Sun, and L. Hou. Timechat: A time-sensitive multimodal large
languagemodelforlongvideounderstanding. ArXivpreprint,2023.
[54] E.Song,W.Chai,G.Wang,Y.Zhang,H.Zhou,F.Wu,X.Guo,T.Ye,Y.Lu,J.-N.Hwang,and
G.Wang. Moviechat: Fromdensetokentosparsememoryforlongvideounderstanding. ArXiv
preprint,2023.
[55] H.Touvron,T.Lavril,G.Izacard,X.Martinet,M.-A.Lachaux,T.Lacroix,B.Rozière,N.Goyal,
E.Hambro, F.Azhar, etal. Llama: Openandefficientfoundationlanguagemodels. ArXiv
preprint,2023.
[56] Y.Wang,X.Meng,J.Liang,Y.Wang,Q.Liu,andD.Zhao. Hawkeye: Trainingvideo-textllms
forgroundingtextinvideos. ArXivpreprint,2024.
[57] J. Xiao, X. Shang, A. Yao, and T. Chua. Next-qa: Next phase of question-answering to
explainingtemporalactions. InCVPR,2021.
[58] L.Xu,Y.Zhao,D.Zhou,Z.Lin,S.K.Ng,andJ.Feng. Pllava: Parameter-freellavaextension
fromimagestovideosforvideodensecaptioning. ArXivpreprint,2024.
[59] Z.Xu,T.Ashby,C.Feng,R.Shao,Y.Shen,D.Jin,Q.Wang,andL.Huang. Vision-flan:scaling
visualinstructiontuning. ArXivpreprint,2023.
[60] S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, and E. Chen. A survey on multimodal large
languagemodels. ArXivpreprint,2023.
[61] W.Yu,Z.Yang,L.Li,J.Wang,K.Lin,Z.Liu,X.Wang,andL.Wang. Mm-vet: Evaluating
largemultimodalmodelsforintegratedcapabilities. ArXivpreprint,2023.
[62] Z. Yu, D. Xu, J. Yu, T. Yu, Z. Zhao, Y. Zhuang, and D. Tao. Activitynet-qa: A dataset for
understandingcomplexwebvideosviaquestionanswering. InAAAI,2019.
[63] X.Yue,Y.Ni,K.Zhang,T.Zheng,R.Liu,G.Zhang,S.Stevens,D.Jiang,W.Ren,Y.Sun,
C.Wei,B.Yu,R.Yuan,R.Sun,M.Yin,B.Zheng,Z.Yang,Y.Liu,W.Huang,H.Sun,Y.Su,
andW.Chen. Mmmu: Amassivemulti-disciplinemultimodalunderstandingandreasoning
benchmarkforexpertagi. ArXivpreprint,2023.
[64] X.Zhai,B.Mustafa,A.Kolesnikov,andL.Beyer. Sigmoidlossforlanguageimagepre-training.
InICCV,2023.
[65] H.Zhang,X.Li,andL.Bing. Video-llama: Aninstruction-tunedaudio-visuallanguagemodel
forvideounderstanding. ArXivpreprint,2023.
[66] R.Zhang,J.Han,A.Zhou,X.Hu,S.Yan,P.Lu,H.Li,P.Gao,andY.Qiao. Llama-adapter:
Efficientfine-tuningoflanguagemodelswithzero-initattention. InICLR,2023.
[67] R.Zhang,D.Jiang,Y.Zhang,H.Lin,Z.Guo,P.Qiu,A.Zhou,P.Lu,K.-W.Chang,P.Gao,
etal. Mathverse: Doesyourmulti-modalllmtrulyseethediagramsinvisualmathproblems?
ArXivpreprint,2024.
[68] Y.Zhang,B.Li,h.Liu,Y.j.Lee,L.Gui,D.Fu,J.Feng,Z.Liu,andC.Li. Llava-next: Astrong
zero-shotvideounderstandingmodel,2024.
[69] D.Zhu,J.Chen,X.Shen,X.Li,andM.Elhoseiny. Minigpt-4: Enhancingvision-language
understandingwithadvancedlargelanguagemodels. ArXivpreprint,2023.
14