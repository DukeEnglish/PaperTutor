Fusion-PSRO: Nash Policy Fusion for
Policy Space Response Oracles
JiesongLian1∗,YucongHuang2∗,MingzhiWang3,ChengdongMa3,
YixueHao1,YingWen4,YaodongYang3†
1HuazhongUniversityofScience&Technology,Wuhan,China
2SchoolofSoftwareandMicroelectronics,PekingUniversity,Beijing,China
3InstituteforArtificialIntelligence,PekingUniversity,Beijing,China
4ShanghaiJiaoTongUniversity,Shanghai,China
Abstract
A popular approach for solving zero-sum games is to maintain populations of
policiestoapproximatetheNashEquilibrium(NE).Previousstudieshaveshown
thatPolicySpaceResponseOracles(PSRO)algorithmisaneffectivemulti-agent
reinforcementlearningframeworkforsolvingsuchgames. However,repeatedly
trainingnewpoliciesfromscratchtoapproximateBestResponse(BR)toopponents’
mixedpoliciesateachiterationisbothinefficientandcostly. WhilesomePSRO
variantsinitializeanewpolicybyinheritingfrompastBRpolicies,thisapproach
limitstheexplorationofnewpolicies,especiallyagainstchallengingopponents.
To address this issue, we propose Fusion-PSRO, which employs policy fusion
to initialize policies for better approximation to BR. By selecting high-quality
base policies from meta-NE, policy fusion fuses the base policies into a new
policythroughmodelaveraging. Thisapproachallowstheinitializedpoliciesto
incorporatemultipleexpertpolicies,makingiteasiertohandledifficultopponents
comparedtoinheritingfrompastBRpoliciesorinitializingfromscratch.Moreover,
ourmethodonlymodifiesthepolicyinitializationphase,allowingitsapplicationto
nearlyallPSROvariantswithoutadditionaltrainingoverhead. Ourexperiments
onnon-transitivematrixgames,LeducPoker,andthemorecomplexLiarsDice
demonstrate that Fusion-PSRO enhances the performance of nearly all PSRO
variants,achievinglowerexploitability.
1 Introduction
Zero-sumgames,suchasStarCraft[33,24]andDOTA2[37],involvestrongnon-transitivity,pre-
sentinguniquechallengesingametheoryandartificialintelligence[7]. Solvingthesenon-transitive
gamesofteninvolvestrainingandexpandingasetofpoliciestoapproximateaNashEquilibrium
(NE)capableofcontendingwithanyopponent. ApproacheslikeFictitiousPlay(FP)[3]andDouble
Oracle(DO)[18]canconvergetoanNEbylearningasetofpolicies,evenincyclicgames(e.g.,
rock-paper-scissors). Forlarger-scaleproblemswithcomplexgamelandscapes,thePolicySpace
ResponseOracles(PSRO)providesaneffectiveopen-endedlearningparadigm[12]. PSROiteratively
approximatestheNashEquilibriumbylearningtheBRtotheopponent’smixedpoliciesateach
iteration.
Previous research on PSRO has primarily concentrated on three key areas: enhancing diversity,
improvingtrainingefficiency, andrefiningsolutionconcepts. First, methodsaimedatenhancing
diversity,suchasDiversePSRO[2],BD&RD-PSRO[20],UDM-PSRO[15],andPSD-PSRO[36],
∗Equalcontribution.
†Correspondenceto:YaodongYang<yaodong.yang@pku.edu.cn>.
Preprint.Underreview.
4202
nuJ
3
]TG.sc[
2v72012.5042:viXra1. Compute 2. Solve 3. Initialize
Player 1's Player k's Player K's
policy set S t1 policy set S tk policy set S tK Profile distribution π
S1
...
Sk
...
SK
M
t t t
... ...
Meta-solver M Inherit Initialization
Game simulations
Profile distribution 
M
Profile distribution π
4. Expand
Player 1's new Player k's new Player K's new
policy set S t1 1 policy set S tk 1 policy set S tK 1 Policy Initialization Nash fusion F
S1
...
Sk
...
SK
t1 t1 t1
Oracle O Fresh Start
Figure1: OverviewoftheFusion-PSROFramework. Fusion-PSROextendsPSRObyaddingan
initializationstep. (1)Compute: Simulatinggameinteractionstocompletethemissingentriesin
thepayofftensorM;(2)Solve: Calculatingthemeta-NEσusingthemeta-solverM,basedonthe
updatedpayofftensor;(3)Initialize: GeneratinganinitialpolicythroughpolicyfusionF basedon
meta-NEσforeachplayer;(4)Expand: Traininganewpolicybasedontheinitializedpolicyusing
theoracleO,whichisthenaddedtoeachplayer’spolicyspace.
focusonpromotingamorediversesetofpoliciestocoverawiderPolicyHull,therebyincreasing
robustnessandgeneralizationagainstvariedopponents.Forimprovingtrainingefficiency,approaches
likePipeline-PSRO[17]parallelizethetrainingofmultipleBestResponse(BR)policiestoaccelerate
convergenceandreducecomputationaltime. Finally,refiningsolutionconceptsinvolvesmethods
suchasα-Rank[22],whichproposealternativeevaluationmetricsandoptimizationtechniquesto
betterapproximatetheNashEquilibrium(NE).Despitetheirinnovations,theseapproachesshare
acommonfeature: theyalmosteitherinitializeanewBRpolicyfromscratchorselectahistorical
policytoinheritforinitialization,whichcanbeinefficientorlimitingonpolicyexploration.
RepeatedlytrainingnewBRpoliciesfromscratchisinefficientandcostly[38,17]. Eventhough
inheritingpastBRpoliciescansavesometrainingtime, thisapproachoftenrestrictsexploration
during the training stages because of the behavioral preferences or distributions of the policies,
requiring abundant iterations to adjust to the original policy distribution. These original policy
distributionsaretypicallywell-trainedagainsthistoricalopponentpopulationsbutmaynoteffectively
handlenewopponentpolicies,especiallyagainstadversariesunseenbefore. Thismisgeneralization,
wherepoliciesinheritedfrompastBRfailtoeffectivelyhandlethelatestopponents’policies,might
hinderbetterBRapproximation. Thisproblemismorefrequentinlarge-scalegameswherepolicies
continuouslyevolve. SuchmisgeneralizationlimittheeffectivenessofPSROinadaptingtonew
opponents,highlightingtheneedforpoliciesthatcanbetterhandleunexpectedchallenges[7].
Ourfirstideaisbasedontheobservationthathistoricalpoliciesmaypossessuniqueinsightsfor
handlingpreviouslyunseenopponents. Incyclicgames,itisnotimpossibletotrainaapproximate
BRtospecificopponents. Thechallengeliesinexploringallbestresponsesandcombiningthem
intobalancedstrategiestoapproachaNashEquilibriummore. Manystudieshavedemonstratedthat
policyensemblesinreinforcementlearningcanenhancegeneralizationcapabilities[23,35,29,28],
suggestingthepotentialeffectivenessofintegratinghistoricalBRpoliciesinPSRO.However,simply
2integrating historical policies in PSRO does not guarantee the inclusion of all BRs for various
opponents. Therefore, we propose merging historical policies into a single policy to achieve an
ensemble-like effect, which can be fine-tuned to handle new opponents. This process of fusing
historicalpoliciesalignswiththeinitializationoftheBRpolicyinPSRO,formingthesecondkey
ideaofourapproach.
Inthispaper,weproposetheFusion-PSROframework,whichaimstoenhancethetraditionalPSRO
paradigmbyfocusingmoreonthefusionofhistoricalpolicestoachievebetterBRapproximation.
Basedonthis,weintroducetheNashPolicyFusionmethod. Thismethodselectshigh-qualitybase
policies according to a meta Nash Equilibrium (meta-NE) and then merges these policies into a
single policy using Nash-weighted averaging. When the policy weights are similar, this process
providesafirst-orderapproximationtoapolicyensemble[19][25]. Basedonthis,wetheoretically
analyze the improvement of Nash-weighted average initialization in terms of utility and policy
exploration. Finally,ourexperimentsvalidatetheeffectivenessofFusion-PSRO,demonstratinglower
exploitabilityandsignificantlybetterperformanceindiversity-enhancedmethods.
2 RelatedWork
EnsembleReinforcementLearning. Policyensemblesinreinforcementlearning(RL)leveragemul-
tiplepoliciestoenhanceperformance,stability,andgeneralizationacrossRLtasks. Thesemethods
addressoverfitting,exploration-exploitationdilemmas,andsampleinefficiencybyintegratingdiverse
policies. NotableexamplesincludeBootstrappedDQN,whichimprovesexplorationthroughmultiple
Q-networks[23],andEnsembleDDPG,enhancingrobustnessindeterministicpolicygradients[35].
Advancessuchasdeterminantalpointprocessesandstochasticensemblevalueexpansionaimto
maximizepolicydiversityandoptimizeexploration[29,28,4]. ApplicationslikeMulti-DQNfor
stockmarketforecastingdemonstratetheutilityofpolicyensembles[5].
ModelFusion. Applyingensemblesdirectlytoaddressevolvingopponentpoliciescouldnotably
increasecomputationaldemands[30]. However,modelfusionresearch,althoughnotdirectlytargeted
atpolicyensemblereinforcementlearning,offersvaluableinsightsthatcouldinformtheseapplica-
tions. Modelfusion,particularlythroughweightaveraging,servesasafirst-orderapproximation
ofmodelensembleswithoutincurringadditionalcomputationalcosts,therebyprovidingapractical
integrationpath[19]. Techniquessuchas“modeconnectivity”,“alignment”,and“weightaveraging”
can optimize parameter spaces, facilitating robust policy integration [13][34]. Approaches like
“modelmerge”,whichcombinesvariousmodelsintoasinglemodel,andmethodssuchasFisher
merging,maybehelpfulformodelgeneralization[16]. TheRewardedSoups(RS),whichblends
weightsfine-tunedondiverserewards,introducesaninnovativetechniqueforadaptingpoliciesto
various opponents [25]. By incorporating these methods into PSRO paradigm, using population
parameteraveraging(PAPA)andstochasticweightaveraging(SWA),policiesmaybediversifiedand
periodicallyenhanced,thusreducingcomputationaloverheadandsupportingiterativeimprovement
[11,21]. Thesemodelfusionapproachesholdthepotentialtosignificantlyenhancepolicies,offering
moreefficientandrobustsolutionsforcomplexgames.
DiverseMethodsinPSRO.PSROhasembracedvariousmethodstoenhancepolicyspacediversity,
similartotechniquesusedinpolicyensembleswithinreinforcementlearning,whichaimtomaximize
policydiversityandoptimizeexplorationefficiency[29,28,4]. DiversePSROpromotespolicydiver-
sitywithinthepopulationtobetterexplorethepolicyspace[2]. BD&RD-PSROcombinesbehavioral
and response diversity, aiming to reduce the exploitability of the meta-policy by ensuring that a
diversesetofpoliciesismaintainedwithinthepopulation[20]. However,thesemethodsprimarily
focusonenlargingthegamescape,whichmightnotdirectlytranslatetobetterapproximationsofa
NE.PSD-PSROfurtherextendsthisapproachbyintroducinganewdiversitymetricthatfocuseson
enlargingthePolicyHull(PH),whichmoreeffectivelyreducespopulationexploitabilityandleads
toacloserapproximationofanNE[36]. Theseeffortsunderlinethepromiseofintegratingdiverse
policyensembleswithinPSRO,enhancingadaptabilityandreducingpredictabilityinresponses.
3 NotationsandPreliminary
Extensive-FormGame. Weanalyzeextensive-formgameswithperfectrecall[9]. Inthesegames,
playersprogressthroughasequenceofactions,eachassociatedwithaworldstatew ∈ W. Inan
3N-playergame,thejointactionspaceisA = A ×···×A . Forplayeri ∈ N = {1,...,N},
1 N
thesetoflegalactionsatworldstatewisA (w)⊆A ,andajointactionisa=(a ,...,a )∈A.
i i 1 N
Afterplayerschooseajointaction,thetransitionfunctionT(w,a)∈∆W determinestheprobability
distributionofthenextworldstatew′. Upontransitioningfromwtow′viajointactiona,playeri
observeso = O (w,a,w′)andreceivesarewardR (w). Thegameendsafterafinitenumberof
i i i
actionswhenaterminalworldstateisreached.
MetricsinExtensive-FormGame. Ahistory,denotedh=(w0,a0,w1,a1,...,wt),isasequence
ofactionsandworldstatesstartingfromtheinitialstatew0. Aninformationsetforplayeri,denoted
s ,isasequenceofthatplayer’sobservationsandactionsuptothatpoint:s (h)=(a0,o1,a1,...,ot).
i i i i i i
Aplayer’sstrategyπ mapsfromaninformationsettoaprobabilitydistributionoveractions. A
i
strategyprofileπisatuple(π ,...,π ). Strategiesofallplayersexceptiaredenotedπ . When
1 N −i
a strategy π is learned through RL, it is referred to as a policy. The expected value (EV) vπ(h)
i i
for player i is the expected sum of future rewards in history h when all players follow strategy
profile π. The EV for an information set s is vπ(s ), and for the entire game, it is v (π). In a
i i i i
two-player zero-sum game, v (π)+v (π) = 0 for all strategy profiles π. A Nash equilibrium
1 2
(NE) is a strategy profile where no player can achieve a higher EV by deviating: π⋆ is a NE if
v (π⋆) = max v (π ,π⋆ ) for each player i. The exploitability e(π) of a strategy profile π is
e(i π) = (cid:80) π mi axi i v (− πi ′,π ). A BR strategy BR (π ) for player i maximizes exploitation
ofπ :
BRi∈ (N
π )
=π i′ ari gmi ax−i
v (π ,π ).
Anϵ-BRi stra− ti
egyBRϵ(π )forplayeriisatmostϵ
wors− ei thanti he− Bi
R:v (BRϵ(π
πi ),πi i )≥− vi
(BR (π ),π )−ϵ.
Ai nϵ− -Ni
ashequilibrium(ϵ-NE)is
i i −i −i i i −i −i
astrategyprofileπwhereπ isanϵ-BRtoπ foreachplayeri.
i −i
Normal-Form Game and Restricted Game. A normal-form game is a single-step extensive-
form game. An extensive-form game induces a normal-form game where the legal actions for
playeriareitsdeterministicstrategiesX A (s ). Thesedeterministicstrategiesarecalledpure
si∈Ii i i
strategies. Amixedstrategyisadistributionoveraplayer’spurestrategies. Arestrictedgame,as
employedinPSRO,referstoavariantofanormal-formgamewhereplayers’strategiesarelimitedto
specificpopulationsΠt. Ineachiteration,thegame’sNE(σ)isdeterminedbasedontheserestricted
i
populations. Subsequently,eachplayer’spopulationexpandsastheycomputeandincorporatethe
bestresponsetothisequilibrium.
4 Fusion-PSRO
Inthissection,weproposeFusion-PSRO,anenhancedframeworkofPSRO,designedforpolicy
initialization. Additionally,weintroduceasimpleandeffectivemethodcalledNashPolicyFusion.
Thismethodleveragesmeta-NEtoselecthigh-qualitypolicies,whicharethenfusedthroughweighted
averagingtoinitializetheBRpolicy. Finally,weprovideguidelinesforintegratingFusion-PSRO
withotherPSROvariants.
4.1 APolicyFusionFrameworkforPSRO
Inadditiontoeffortsinmaximizingpolicydiversityandexplorationefficiencythroughensemble
reinforcement learning [29, 28, 4], integrating diverse multi-policy models into a single policy
viadistillationhasalsobeenshowntoeffectivelyutilizehistoricalglobalinformation[6]. These
approachesarecollectivelyreferredtoaspolicyfusioninourwork. Policyfusionaimstoenhance
robustnessandadaptabilitybyintegratingdifferentiatedexistingpolicies.Inspiredbythis,wepropose
apolicyfusionframeworkforenhancingPSRO,namedFusion-PSRO,toleveragepastpoliciesmore
effectivelywhentrainingnewBRpolicies.
AsillustratedinFigure1,ourapproachenhancesPSRObyintroducingapolicyfusioncomponent
duringtheinitializationofnewpolicies. GiventhatnotallpoliciesgeneratedbyPSROarenear-
optimal, the meta-Nash Equilibrium (meta-NE) helps us select high-quality policies. Typically,
policyinitializationinPSROinvolveseitherinheritinghistoricalpoliciesorstartingfromscratch
[12],bothmethodsprovinginefficientandcostly. Whileemployingpolicydistillationorensemble
learningforpolicyfusiondoesenhancepolicygeneralizationandmitigatesBRmismatch,italso
leadstohighertrainingcosts. Incontrast,weightedaveragingservesasacost-effectivealternative.
Bydirectlyaveragingtheweightsofbasepolicies,itprovidesapracticalfusionmethod,makingit
thepreferredapproachinFusion-PSRO.Infact,whentheweightsofmultiplemodelsaresimilar,
weightedaveragingmayserveasthefirst-orderapproximationofpoliciesensemble[19].
4Theprocessoffusion-basedinitializationinPSROcanbesummarizedintotwomainactions:selection
andmergence. Theselectionphaseinvolveschoosingbasepoliciesfrompopulationthatarenotonly
strongbutalsodiverse,ensuringarepresentativesetofpolicies. Themergencephasethenfusesthese
selectedpoliciesinawaythatoptimizescomputationalresources,potentiallyevenreducingcosts.
Finally,theresultingfusedpolicyisthenintegratedintothetrainingoracleofPSROorotherPSRO
variants,allowingthisframeworktobeappliedacrossmultiplePSROadaptationsforenhancingtheir
abilitytoutilizeknowledgefromexistingpoliciesandimproveoverallperformance. Toillustrate,
usingweightedaveragingasanexample,thegenericexpressioncanbedefinedasEq.(1): Inour
framework,π representsthepolicyobtainedafterfusion,whereπ∗(i)denotesthei-thbasepolicy.
fusion
Theparametersofthesepoliciesaredenotedasθ forthefusedpolicyandθ foreachbase
πfusion π∗(i)
policy. Thefusionmethod,symbolizedbyF(·),employsweightedaveraging(WA)asindicatedby
thefollowingexpression:
k
(cid:88)
θ =F(θ ,θ ,...,θ )= λ θ , (1)
πfusion π∗(1) π∗(2) π∗(k) π∗(i) π∗(i)
i=1
whereλ
representsthefusionweightsassignedtoeachbasemodel,ensuringthat(cid:80)k
λ =
π∗(i) i=1 π∗(i)
1. Wedefinethepolicyfusionobjectivetoensurethattheutilityofthefusedpolicyπ ,when
fusion
contendingagainstaspecificopponentπ ,isatleastashighasthatobtainedbyanyhistoricalpolicy.
−i
Thisiscrucialtoguaranteethattheperformancedoesnotdegradepost-fusion. Theutilityfunction
U(π ,π ;θ)measuresthiseffectiveness,whereθrepresentsthenetworkparametersobtained
fusion −i
fromthebasemodelsthroughthefusionoperationF. Formally,theobjectivecanbeexpressedas:
θ =argmaxU(π ,π ;θ),
πfusion
θ
fusion −i
(2)
subjecttoU(π ,π ;θ )≥U(πj,π ;θj)for∀πj ∈Π ,
fusion −i πfusion i −i i i i
whereΠ isthepolicysetofplayeri,πj representsthej-thhistoricalpolicyofplayeriandθj areits
i i i
parameters. Thisensuresthatthefusedpolicymaximizesitsutilitywhilemaintainingperformance
thatisatleastequivalenttoanypolicyinthepolicyset,asdescribedinEq.(2).
4.2 NashPolicyFusion
InFusion-PSRO,ametricisrequiredtorepresenttheimportanceofpolicieswithinthepopulation,
which aids in selecting powerful base policies and determining their fusion weights. We choose
meta-NashEquilibrium(meta-NE)asthismetric,whichcalculatedfromthepayoffsgeneratedby
the current meta-game, indicates the relative importance of each policy under Nash equilibrium
withinthecurrentpopulation,offeringsignificantrepresentationalvalue. Furthermore,sinceitis
pre-generated,itdoesnotincuradditionalresourcecosts. Weprioritizefusingstrongerstrategies,
specifically those with higher probabilities in the Nash Equilibrium. These policies are defined
asΠ∗ = argtop (Π ,P(σ )), representingtheselectionofthetopk policiesfromtheplayeri’s
i k i i
policysetthroughthehighestprobabilitieswithmeta-NEσ (excludingopponentpoliciesσ−i). This
i
selectioncriterionensureswefocusonthemosteffectivepolicies,ashigherprobabilityindicates
greaterimportance.
Instead of traditional ensemble learning and distillation, which are resource-intensive and costly
tomaintain,weemployweightedaveraging(WA)forfusingbasepolicies. WAdirectlyaverages
themodelparameters,makingitasimpleandcomputationallyefficientapproach. Thismethodcan
yield solutions closer to the optimal policy in the policy space with low loss function values, as
demonstratedbytechniqueslikeSWAinRL[21]. ForthefusionweightsinWAforFusion-PSRO,
weusethesoftmaxfunctiontoensuretheweightssumtooneandtopromotediversitybynarrowing
thegapbetweentheweightsofbasepoliciesthroughexponentialoperations. Therefore,thefusion
weightsλ Π∗ canbeconstructedasshowninEq.(3),enablingscalabletrade-offsbetweenhigh-quality
i
policies.
λ Π∗ =softmax(top-k(P(σ i))), (3)
i
whereinotesthei-thplayer,Π∗representsthecurrentbasepoliciessubset[π∗(1),π∗(2),...,π∗(k)]
i i i i
ofplayeri. CorrespondingprocessisshowninAlgorithm1. Itisimportanttoclarifythatkcannot
belessthan2,aspolicyfusionrequiresatleasttwobasepolicies. Ifk = 1,itisaspecialcaseof
fusioninitializationknownasinheritinitialization. Basepoliciesarechosenexclusivelyfromthe
sameplayer’spopulationi. AndΠ∗representsthehigh-qualitypolicieswiththehighestprobabilities
i
inthecurrentmeta-NE(σt)tobemerged.
i
5Algorithm1InitializationviaNashPolicyFusion(WA)
1: Input: population Πt, meta-NE σt for player i, new policy πt+1 for player i, top-k ratio α,
i i i
currentiterationt,fusionstartconditionc.
2: ift≥cthen
3: Calculatenumberofpolicy-fusedk: k =max(2,α|Πt|)
i
4: Selecthigh-qualitypolicysetΠ∗fromΠt: Π∗ =argtop (Πt,P(σt))
i i i k i i
5: Calculatefusionproportionλ Π∗ i: λ Π∗
i
=softmax(top-k(P(σ it)))
6: Initializeπ it+1withθ π it+1 =λ Π∗ i ×θ Π∗ i
7: else
8: Initializeπt+1 ∼σt
i i
9: endif
10: Output: initializedπt+1forplayeri
i
4.3 NashPolicyFusionforPSROs
Followingthefusion-basedinitialization,
Algorithm2Fusion-PSRO
we initially adopt the standard PSRO
trainingprocessesandobjectivefunctions. 1: Input: initialpolicysetsforallplayersΠ
Integrating the aforementioned fusion- 2: ComputeutilitiesUΠforeachjointπ ∈Π
basedapproach,weintroduceNashPol- 3: Initializemeta-NEσ i =UNIFORM(Π i)
icy Fusion for PSRO, detailed in Algo- 4: fore∈{1,2,...}do
rithm 2. It should be emphasized that 5: forplayeri∈{1,2,...,n}do
whent<2,theprerequisitesforFusion 6: Initializeπt+1viaAlgorithm1//BetterIni-
i
are not satisfied (requiring at least two tialization
basepolicies),andthusthedefaultinitial- 7: formanyepisodesdo
ization method of PSRO is applied. If 8: Sampleπ −i ∼σ −t i
theobjectivefunctionutilizesregulariza- 9: Trainoracleπ it+1overρ∼(π it+1,π −i)
tionformsfromotherPSROvariants,or 10: endfor
ifthetrainingprocessemploysdifferent 11: Πt+1 =Πt∪{πt+1}
i i i
PSROvariants,theframeworkisreferred 12: endfor
to as Fusion-PSROs. We have applied 13: ComputemissingentriesinMt+1
thisfusionenhancementtonearlyallvari- 14: Computeameta-NEσfromMt+1
ants,withexperimentalresultspresented 15: endfor
in Section 5. To illustrate the plug-and- 16: Output: currentmeta-NEforeachplayer
playattributeofourwork,weintegrated
fusion process into PSD-PSRO as an example, whose algorithm is provided in Appendix A. In
addition,giventhatFusionistreatedasaninitializationmethodinthisstudy,wehaveconducted
corresponding ablation experiments to compare Fusion with other initialization approaches (in-
cludingnormalinitialization[26],inheritedinitialization[12],orthogonalinitialization[27],Kaiming
initialization[10])whenusedintoPSROanditsvariants,whicharedocumentedinAppendixB.
4.4 HowNashPolicyFusionWorks
TounderstandthemechanicsofNashPolicyFusion,weproposeAssumption1. Let{πj}N beaset
i j=1
oftrainedpolicymodelswithcorrespondingnetworkparameters{θ }N forplayeri. Thegoalis
πj j=1
i
tofusethesemodelsintoasinglepolicymodelπ withparametersθ . Eachpolicymodelπj
fusion πfusion i
hasbeentrainedinthesameenvironmentbutagainstdifferentopponents.
Assumption1. Theweightedaverageatthepolicynetworkparameterlevelprovidesafirst-order
approximationtotheensembleatthepolicyoutputlevelwhenthepolicynetworkparametersare
similar. Usingthefirst-orderTaylorexpansion,wehave:
N
π ≈π
+(cid:88)∂π
ensemble(πj −π ), (4)
fusion ensemble ∂πj i ensemble
j=1 i
whereπ =(cid:80)N w πj,andtheweightsw satisfy(cid:80)N w =1.
ensemble j=1 j i j j=1 j
6Proposition1. BasedonAssumption1,whentheweightsequalNashprobabilities,theNashfusion
policy is an approximate Nash policy in the restricted game. If we define the expected utility as
U(π ,π ),then
i −i
U(π ,π )≳U(πj,π ) ∀πj ∈Π , (5)
fusion −i i −i i i
whereπj representsthej-thhistoricalpolicyofplayeriandΠ isthepolicysetofplayeriinthe
i i
currentrestrictedgame.
Proof: SeeAppendixC.1. Proposition1demonstratesthatNash-weightedaverageenablesscalable
trade-offsbetweennear-optimalhistoricalpoliciesfromtheperspectiveoftheglobalutilityfunction.
Proposition2. IfAssumption1holds,Nash-weightedaverageinitializationissuperiortoinherit
initializationandscratchinitialization. Fortheirexpectedutilities,wehave
U(π ,π ;θ )≳U(π ,π ;θ )≫U(π ,π ;θ ), (6)
fusion −i πfusion inherit −i πinherit scratch −i πscratch
where(π ,θ ),(π ,θ )and(π ,θ )arethepoliciesandparametersinitialized
fusion πfusion inherit πinherit scratch πscratch
bythefusion,inherit,andscratchmethods,respectively.
Proof: See Appendix C.2. Proposition 2 indicates that Nash-weighted average initialization is
betterthaninheritandscratchinitialization,providingabaselineguaranteeforthefusionobjective
constraintsinEq.(2).
Additionally, we find that Nash-weighted initialization retains exploration terms by combining
multiplepolicies,eachcontributingitsownexploratorystrategy,asshowninProposition3. This
methodissuperiortotheinheritinitialization,asitpreventsoverfittingtocurrentopponents’policies
andenhancesoverallexplorationcapability.
Proposition3. TheNash-weightedinitializationcombinestheexplorationcapabilitiesofmultiple
policies,witheachpolicycontributingtotheoverallexploration:
N N
U(π ,π ;θ )≈(cid:88) w U(π ,π ;θ )+ 1(cid:88) w (θ −θ )TH (θ −θ ).
fusion −i πfusion j fusion −i π ij 2 j πfusion π ij j πfusion π ij (7)
j=1 j=1
(cid:124) (cid:123)(cid:122) (cid:125)
Explorationterm
whereθ representstheparametersofthej-thpolicyinplayeri’spolicyset.
πj
i
Proof: SeeAppendixC.2.
5 Experiments
We aim to validate the effectiveness of our Fusion-PSRO framework in generating better BR ap-
proximations,therebyinducingalessexploitablepopulation. WeincorporatenotonlyPSRO[12]
butalsomultiplestate-of-the-artPSROvariantsintoourframework,includingPipeline-PSRO[17],
PSRO [1],BD&RD-PSRO[14],Diverse-PSRO[20],andPSD-PSRO[36],collectivelyreferredto
rN
asFusion-PSROs,forcomparativeanalysisagainsttheiroriginalversions. Thebenchmarksinclude
single-stategames(non-transitivemixturegame)andcomplexextensivegames(LeducPokerand
LiarsDice). Inallcases,Fusion-PSROsdemonstratethecapabilitytoachievelowerexploitability
asshowninFigure2andFigure3,whichinasensesupportsProposition1ofapproximateNash
policies. Furthermore,inthenon-transitivemixturegame,trajectoriesgeneratedbyFusion-PSROs
aredenseraroundtheGaussiancenterpoint,asshowninFigure2b,indicatingmorecomprehensive
exploration,substantiatingProposition3. Moreover,theablationstudiesondifferentinitializations
inLiarsDice,asdepictedinFigure4(detailedinAppendixB),revealthatnewpoliciesgenerated
byFusion-PSROsattainhigherrewards,suggestingforbetterBRapproximation,therebyvalidating
Proposition2. BenchmarkandimplementationdetailsaredocumentedinAppendixD.
Non-TransitiveMixtureGameischaracterizedbysevenGaussianhumpsthatareevenlyspacedon
atwo-dimensionalplane. Eachpolicyinthegamecorrespondstoapointonthisplane,analogous
totheweights(theprobabilityofthatpointwithineachGaussiandistribution)thatplayersallocate
tothehumps. Toachievethebestpolicy,playersshouldpositionthemselvesnearthecentralarea
oftheGaussianhumpsandinvestigateallthedistributionsthoroughly. InFigure2a,wedepictthe
explorationtrajectoriesduringtrainingforboththeoriginalversions(PSROs)ofPSROanditsvarious
7PSRO P-PSRO PSRO-rN DPP-PSRO BD&RD-PSRO PSD-PSRO
Exp: 7.77 ± 2.43 Exp: 3.63 ± 0.9 Exp: 18.01 ± 3.65 Exp: 2.08 ± 0.28 Exp: 5.29 ± 2.56 Exp: 2.64 ± 0.83
(a)PSROs
PSRO P-PSRO PSRO-rN DPP-PSRO BD&RD-PSRO PSD-PSRO
Exp: 6.97 ± 3.17 Exp: 1.29 ± 0.68 Exp: 5.19 ± 1.35 Exp: 1.7 ± 0.67 Exp: 3.65 ± 1.17 Exp: 1.78 ± 0.98
(b)Fusion-PSROs
Figure2: TrainingexplorationtrajectoriesonNon-TransitiveMixtureGame. Thefinalexploitability
×100(Exp)foreachmethodisindicatedatthebottom.
variants,alongsidetheircounterpartsinFusion-PSROframework(Fusion-PSROs)showninFigure
2b. WhilethetrajectoriesofPSROsgraduallyexpandoutwardfromthecenter,thosegeneratedby
Fusion-PSROsaredenseraroundtheGaussiancenter,indicatingthepotentialforachievingbetterBR
policy. Uponanalyzingthisprocess,weobservethatpolicyfusionalterstheinitializationpositions,
resultinginstartingpointsoftrajectoriesclosertotheGaussiancenter. Astrainingproceeds,the
trajectoriessystematicallytraversealmosteveryGaussiancenter,clarifyingthereasonsbehindthe
betterBRapproximation. Moreover,Fusion-PSROsexhibitlowerexploitability(EXP),suggesting
that policy fusion of historical policies enhances the robustness of the population. Contrary to
conventionalapproaches,wherePSROandPSRO aretypicallylimitedbythenumberofthreads
rN
of only 1—compared to 4 in other PSRO variants—thus severely constraining their exploration
capabilities,weoptedforadifferentway. Tomitigatethisconstraint,wetripledthetrainingepisodes
forthesetwoalgorithmscomparedtoothers,aimingtobalancetheexplorationlimitations.
Leduc Poker Liars Dice
PSRO PSRO
1.4 PSD-PSRO 0.4 PSD-PSRO
Fusion-PSRO(Ours) Fusion-PSRO(Ours)
1.2 Fusion-PSD-PSRO(Ours) 0.3 Fusion-PSD-PSRO(Ours)
1.0
0.2
0.8
0.1
0.6
0.4 0.0
0 25 50 75 100 125 150 0 20 40 60 80 100
Iterations Iterations
(a)LeducPoker (b)LiarsDice
Figure3: Exploitabilityofthemeta-NEonLeducpokerandLiarsDice. Thenumbersofsamplesfor
trainingeachBRintwogamesaresetto2e4,2e5respectively.
LeducPokerisasimplifiedvariantofpoker[31],withadeckconsistingoftwosuits,eachcontaining
three cards. Each player antes one chip, and a single private card is dealt to each player. Since
Diverse-PSROcannotscaletotheRLsettingandthecodeforBD&RD-PSROincomplexgamesis
unavailable,wecomparePSROandPSD-PSROwiththeirpolicyfusioncounterparts,Fusion-PSRO
andFusion-PSD-PSRO,toillustratetheimpactofdiversityonFusion-PSROframework(similarlyin
LiarsDice). AsshowninFigure3a,bothFusion-PSROandFusion-PSD-PSROaremoreeffectiveat
reducingexploitabilitythantheircorrespondingoriginalversion,whichinasensesupportsProposition
1. Additionally,diversity-enhancedFusion-PSD-PSROoutperformsFusion-PSRO,highlightingthe
benefitsofdiversityinpolicyfusion,substantiatingProposition3.
Liars Dice is a bluffing game where each player begins with a set of concealed dice and makes
progressivelyhigherbidsregardingthecountofaspecificdiefaceacrossallplayers[8]. Thegame
alternatesbetweenmakingbidsandchallengingtheveracityofthepreviousbid,leadingtotheloss
ofdice(ordefeatinthegame)forincorrectchallengesorbids. AsshowninFigure3b,similarto
Leduc Poker, by merging historical policies, both Fusion-PSRO and Fusion-PSD-PSRO achieve
8
ytililibatiolpxE ytililibatiolpxELiars Dice
Normal Initialization Orthogonal Initialization Kaiming Initialization Inherited Initialization
0.0 0.0 0.0 0.050
0.025
0.2 0.2 0.2 0.000
0.4 F Nu os ri mon a- lB -BR R (Ours) 0.4 F Ou rs thio on g- oB nR a ( l-O Bu Rrs) 0.4 F Ku as imio in n- gB -R B R(Ours) 0.025 F Inu hs eio rn it- eB dR -B ( ROurs)
0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0
Episodes(2e5) ×105 Episodes(2e5) ×105 Episodes(2e5) ×105 Episodes(2e5) ×105
Figure4: AverageReward foreachiteration(71-80)duringthetrainingofeachapproximateBR
withinPSD-PSRO.
lowerexploitability[32]thantheircorrespondingoriginalversions,meanwhileFusion-PSD-PSRO
outperformsFusion-PSRO.Additionally,sincepolicyfusioninFusion-PSROcanbeconsidereda
methodofpolicyinitialization,weconductedablationexperimentstocompareotherfourdifferent
initializationmethodsincludingnormalinitialization(randomvaluesfromanormaldistribution)
[26], orthogonalinitialization(orthogonalweightmatrices)[27], Kaiminginitialization(weights
scaledbythesquarerootofthenumberofinputunits)[10]andinheritedinitialization(inheriting
weights from previously trained models or policies, which is the default initialization method in
OpenSpiellibrary’sPSRO)[12]. Intheexperiment,weinitializedtheinitialpopulationsusingthese
fourmethodsrespectivelyandcontinuedusingthemtogenerateapproximateBRs(correspondingto
Normal-BR,Orthogonal-BR,Kaiming-BRandInherited-BRrespectively)duringnewpolicytraining
withinPSROandPSD-PSRO.Simultaneously,wereplacedthemwithNashPolicyFusiontogenerate
thesecondBR(Fusion-BR)forcomparison. Asanexample,Figure4showstheaveragerewardfor
eachiteration(71-80)duringthetrainingofeachapproximateBRwithinPSD-PSRO(detailedin
AppendixB).Atthestartoftrainingeachpolicy,Fusion-BR,bymerginghistoricalpolicies,and
Inherited-BR,byinheritinghistoricalpolicies,bothachievehigherinitialrewards,whiletheother
threeBRsneedtobetrainedfromscratchwithlowerrewards. Furthermore,Fusion-BRconverges
toahigheraveragerewardinnearlyhalftheepisodescomparedtootherBRs,potentiallyreducing
thecomputationalresourcesrequiredforFusion-PSROs. Intermsoffinalconvergedaveragereward,
Fusion-BRoutperformsallotherBRs,therebyvalidatingProposition2. Theexperimentalresults
indicatethatFusion-PSROscangeneratebetterapproximateBRpoliciesbyusingNashPolicyFusion
toinitializethepolicyclosertotheBR,enablingfasterandmoreaccurateBRapproximationduring
trainingandpotentiallyreducingthecomputationalcostofPSROanditsvariants.
6 ConclusionsandLimitations
Inthispaper,weintroducedFusion-PSROframework,aninnovativeapproachforinitializingpolicies
withinthePSROparadigmusingpolicyfusiontechniques. Ourmethodleveragesoptimizedbase
policies,integratingthemviaweightaveragingtoenhancetheapproximationofBRpolicieswithout
additionaltrainingoverhead. Extensiveexperimentsinvariousgameenvironments,includingnon-
transitivematrixgames,LeducPokerandLiarsDice,demonstratethatFusion-PSROsignificantly
improvestheperformanceofPSROanditsvariants,achievinglowerexploitabilityandhigherreturns,
especiallyasthenumberofpoliciesincreases.
Despitethesepromisingresults,ourapproachhassomelimitations. WhileFusion-PSROshowed
significantimprovementsintestedenvironments,itsscalabilitytolargerandmorecomplexgames
requiresfurtherinvestigation. Additionally,theperformanceofFusion-PSROissensitivetohyperpa-
rameterchoices,suchasthenumberofbasepoliciestofuse,necessitatingextensivehyperparameter
tuningforoptimalperformance. Furthermore,ourcurrentmethodreliesonweightaveraging,which
requiresthepolicyweightstobeveryclosetoeachother. Thereareothermodelfusionmethodsthat
couldbeexploredandvalidated.
Future research could address these limitations by optimizing the selection and fusion process,
applyingFusion-PSROtomorecomplexandlarger-scalegames,andintegratingadvancedpolicy
fusiontechniquestoenhanceperformanceandscalability. Additionally,exploringthetheoretical
foundationsofpolicyfusioninreinforcementlearningcouldprovidedeeperinsightsintoitsbenefits
andpotentialapplications.
9
draweR
egarevAReferences
[1] DavidBalduzzi,MartaGarnelo,YoramBachrach,WojciechCzarnecki,JulienPerolat,Max
Jaderberg,andThoreGraepel. Open-endedlearninginsymmetriczero-sumgames. InInterna-
tionalConferenceonMachineLearning,pages434–443.PMLR,2019.
[2] DavidBalduzzi,SébastienRacanière,JamesMartens,JakobFoerster,KarlTuyls,andThore
Graepel. Diversepopulation-basedreinforcementlearning. arXivpreprintarXiv:1901.08106,
2019.
[3] GeorgeWBrown. Iterativesolutionofgamesbyfictitiousplay. Act.Anal.ProdAllocation,
13(1):374,1951.
[4] JacobBuckman,DanijarHafner,GeorgeTucker,EugeneBrevdo,andHonglakLee. Sample-
efficientreinforcementlearningwithstochasticensemblevalueexpansion. Advancesinneural
informationprocessingsystems,31,2018.
[5] SalvatoreCarta,AndreiaFerreira,AlessandroSimonettaPodda,DiegoReforgiatoRecupero,
and Andrea Sanna. Multi-dqn: An ensemble of deep q-learning agents for stock market
forecasting. Expertsystemswithapplications,164:113820,2021.
[6] Yiqun Chen, Hangyu Mao, Tianle Zhang, Shiguang Wu, Bin Zhang, Jianye Hao, Dong Li,
BinWang, andHongxingChang. Ptde: Personalizedtrainingwithdistillatedexecutionfor
multi-agentreinforcementlearning. arXivpreprintarXiv:2210.08872,2022.
[7] WojciechM.Czarnecki,GauthierGidel,BrendanD.Tracey,KarlTuyls,ShayeganOmidshafiei,
David Balduzzi, and Max Jaderberg. Real world games look like spinning tops. ArXiv,
abs/2004.09468,2020.
[8] ChristopherPFergusonandThomasSFerguson. ModelsfortheGameofLiar’sDice. Springer,
1991.
[9] EricAHansen,DanielSBernstein,andShlomoZilberstein.Dynamicprogrammingforpartially
observablestochasticgames. ConferenceonArtificialIntelligence(AAAI),2004.
[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers:
Surpassinghuman-levelperformanceonimagenetclassification. InProceedingsoftheIEEE
internationalconferenceoncomputervision,pages1026–1034,2015.
[11] AlexiaJolicoeur-Martineau,EmyGervais,KilianFatras,YanZhang,andSimonLacoste-Julien.
Populationparameteraveraging(papa). arXivpreprintarXiv:2304.03094,2023.
[12] MarcLanctot,ViniciusZambaldi,AudrunasGruslys,AngelikiLazaridou,KarlTuyls,Julien
Pérolat,DavidSilver,andThoreGraepel. Aunifiedgame-theoreticapproachtomultiagent
reinforcementlearning. InAdvancesinNeuralInformationProcessingSystems,LongBeach,
CA,USA,2017.
[13] WeishiLi,YongPeng,MiaoZhang,LiangDing,HanHu,andLiShen. Deepmodelfusion: A
survey. ArXivpreprintArXiv:2309.15698,2023.
[14] XiangyuLiu,HangtianJia,YingWen,YaodongYang,YujingHu,YingfengChen,Changjie
Fan,andZhipengHu. Unifyingbehavioralandresponsediversityforopen-endedlearningin
zero-sumgames. InAdvancesinNeuralInformationProcessingSystems,ShanghaiJiaoTong
University;NeteaseFuxiAILab;UniversityCollegeLondon,2021.NeurIPS.
[15] ZongkaiLiu,ChaoYu,YaodongYang,ZifanWu,YuanLi,etal. Aunifieddiversitymeasure
formultiagentreinforcementlearning. InAdvancesinNeuralInformationProcessingSystems,
2022.
[16] Michael S Matena and Colin A Raffel. Merging models with fisher-weighted averaging.
AdvancesinNeuralInformationProcessingSystems,35:17703–17716,2022.
[17] StephenMcAleer,JohnLanier,RoyFox,andPierreBaldi. Pipelinepsro: Ascalableapproach
forfindingapproximatenashequilibriainlargegames. InAdvancesinNeuralInformation
ProcessingSystems,Vancouver,Canada,2020.
[18] HBrendanMcMahan,GeoffreyJGordon,andAvrimBlum. Planninginthepresenceofcost
functionscontrolledbyanadversary. InProceedingsofthe20thInternationalConferenceon
MachineLearning(ICML-03),pages536–543,2003.
10[19] RémiMunos,MichalValko,DanieleCalandriello,MohammadGheshlaghiAzar,MarkRowland,
ZhaohanDanielGuo,YunhaoTang,MatthieuGeist,ThomasMesnard,AndreaMichi,etal.
Nashlearningfromhumanfeedback. arXivpreprintarXiv:2312.00886,2023.
[20] NicolasPerezNieves,YaodongYang,OliverSlumbers,DavidHenryMguni,YingWen,and
JunWang. Modellingbehaviouraldiversityforlearninginopen-endedgames. InAdvancesin
NeuralInformationProcessingSystems,2021.
[21] Evgenii Nikishin, Pavel Izmailov, Ben Athiwaratkun, Dmitrii Podoprikhin, Timur Garipov,
PavelShvechikov,DmitryVetrov,andAndrewGordonWilson. Improvingstabilityindeep
reinforcementlearningwithweightaveraging. InUncertaintyinartificialintelligenceworkshop
onuncertaintyinDeeplearning,2018.
[22] ShayeganOmidshafiei,ChristosPapadimitriou,GeorgiosPiliouras,KarlTuyls,MarkRowland,
Jean-BaptisteLespiau,WojciechMCzarnecki,MarcLanctot,JulienPerolat,andRemiMunos.
α-rank: Multi-agentevaluationbyevolution. ScientificReports,9(1):1–29,2019.
[23] IanOsband,CharlesBlundell,AlexanderPritzel,andBenjaminVanRoy. Deepexplorationvia
bootstrappeddqn. Advancesinneuralinformationprocessingsystems,29,2016.
[24] PengPeng,YingWen,YaodongYang,QuanYuan,ZhenkunTang,HaitaoLong,andJunWang.
Multiagentbidirectionally-coordinatednets:Emergenceofhuman-levelcoordinationinlearning
toplaystarcraftcombatgames. arXivpreprintarXiv:1703.10069,2017.
[25] AlexandreRame,GuillaumeCouairon,CorentinDancette,Jean-BaptisteGaya,MustafaShukor,
Laure Soulier, and Matthieu Cord. Rewarded soups: towards pareto-optimal alignment by
interpolatingweightsfine-tunedondiverserewards.AdvancesinNeuralInformationProcessing
Systems,36,2024.
[26] DavidERumelhart,GeoffreyEHinton,andRonaldJWilliams. Learningrepresentationsby
back-propagatingerrors. nature,323(6088):533–536,1986.
[27] AndrewMSaxe,JamesLMcClelland,andSuryaGanguli. Exactsolutionstothenonlinear
dynamicsoflearningindeeplinearneuralnetworks. arXivpreprintarXiv:1312.6120,2013.
[28] GauravSharma,AmitSingh,andSurbhiJain. Deepevap: Deepreinforcementlearningbased
ensemble approach for estimating reference evapotranspiration. Applied Soft Computing,
125:109113,2022.
[29] HassamSheikh,KizzaFrisbee,andMarianoPhielipp. Dns: Determinantalpointprocessbased
neuralnetworksamplerforensemblereinforcementlearning. InInternationalConferenceon
MachineLearning,pages19731–19746.PMLR,2022.
[30] YanjieSong,PonnuthuraiNagaratnamSuganthan,WitoldPedrycz,JunweiOu,YongmingHe,
Yingwu Chen, and Yutong Wu. Ensemble reinforcement learning: A survey. Applied Soft
Computing,page110975,2023.
[31] FinneganSouthey,MichaelPBowling,BryceLarson,CarmeloPiccione,NeilBurch,Darse
Billings, and Chris Rayner. Bayes’ bluff: Opponent modelling in poker. arXiv preprint
arXiv:1207.1411,2012.
[32] FinbarrTimbers,NolanBard,EdwardLockhart,MarcLanctot,MartinSchmid,NeilBurch,
Julian Schrittwieser, Thomas Hubert, and Michael Bowling. Approximate exploitability:
Learningabestresponseinlargegames. arXivpreprintarXiv:2004.09677,2020.
[33] OriolVinyals,IgorBabuschkin,WojciechMCzarnecki,MichaëlMathieu,AndrewDudzik,Jun-
youngChung,DavidHChoi,RichardPowell,TimoEwalds,PetkoGeorgiev,etal. Grandmaster
levelinstarcraftiiusingmulti-agentreinforcementlearning. Nature,575(7782):350–354,2019.
[34] MitchellWortsman,GabrielIlharco,SamirYaGadre,RebeccaRoelofs,RaphaelGontijo-Lopes,
AriSMorcos,HongseokNamkoong,AliFarhadi,YairCarmon,SimonKornblith,etal. Model
soups: averagingweightsofmultiplefine-tunedmodelsimprovesaccuracywithoutincreasing
inferencetime. InInternationalconferenceonmachinelearning,pages23965–23998.PMLR,
2022.
[35] JuntaWuandHuiyunLi. Deepensemblereinforcementlearningwithmultipledeepdeterminis-
ticpolicygradientalgorithm. MathematicalProblemsinEngineering,2020:1–12,2020.
[36] JianYao,WeimingLiu,HaoboFu,YaodongYang,StephenMcAleer,QiangFu,andWeiYang.
Policyspacediversityfornon-transitivegames. InAdvancesinNeuralInformationProcessing
Systems,2023.
11[37] DehengYe, GuibinChen, WenZhang, ShengChen, BoYuan, BoLiu, JiaChen, ZhaoLiu,
FuhaoQiu,HongshengYu,etal. Towardsplayingfullmobagameswithdeepreinforcement
learning. In 34th Conference on Neural Information Processing Systems (NeurIPS 2020),
Vancouver,Canada,2020.
[38] MingZhou,JingxiaoChen,YingWen,WeinanZhang,YaodongYang,YongYu,andJunWang.
Efficientpolicyspaceresponseoracles. arXivpreprintarXiv:2202.00633,2022.
12A AlgorithmforFusion-PSD-PSRO
Algorithm3Fusion-PSD-PSRO
1: Input: initialpolicysetsforallplayersΠ
2: ComputeutilitiesUΠforeachjointπ ∈Π
3: Initializemeta-NEσ i =UNIFORM(Π i)
4: fore∈{1,2,...}do
5: forplayeri∈{1,2,...,n}do
6: Initializeπt+1viaAlgorithm1andsampleJ policies{πj}J fromPolicyHullΠt
i i j=1 i
7: formanyepisodesdo
8: Sampleπ −i ∼σ −t iandcollectthetrajectoryτ byplayingπ iagainstπ −i
9: Discounttheterminalrewardu(π i,π −i)toeachstateastheextrinsicrewardr 1
10: DiscountRkl(τ)toeachstateastheintrinsicrewardr 2
11: Store(s,a,s′,r)tothebuffer,wheres′isthenextstateandr =r 1+r 2
12: Estimate the gradient with the samples in the buffer and train oracle πt+1 over
i
ρ∼(πt+1,π )
i −i
13: endfor
14: Πt+1 =Πt∪{πt+1}
i i i
15: endfor
16: ComputemissingentriesinMt+1
17: Computeameta-NEσfromMt+1
18: endfor
19: Output: currentmeta-NEforeachplayer.
B AverageRewardduringTrainingonLiarsDice
Inablationexperiments,weinitializedtheinitialpopulationsusingthesefourinitializationmethods
respectivelyandcontinuedusingthemtogenerateapproximateBRs(correspondingtoNormal-BR,
Orthogonal-BR, Kaiming-BR and Inherited-BR respectively) during new policy training within
PSROandPSD-PSRO.Simultaneously,wereplacedtheseinitializationmethodswiththeNashPolicy
FusionmethodtogeneratethesecondBR(Fusion-BR)forcomparison. Figure5showstheaverage
reward foreachiteration(61-100)duringthetrainingofeachapproximateBRwithinPSROand
PSD-PSRO.
Fusion-BR in PSRO (Ours) Normal-BR in PSRO
Iterations: 61-70 Iterations: 71-80 Iterations: 81-90 Iterations:91-100
0.0 0.0 0.0
0.0
0.1 0.1
0.2 0.2 0.2 0.2
0.3 0.3
0.4
0.4 0.4 0.4
0.5 0.5
0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0
Episodes(2e5) ×105 Episodes(2e5) ×105 Episodes(2e5) ×105 Episodes(2e5) ×105
Fusion-BR in PSD-PSRO (Ours) Normal-BR in PSD-PSRO
Iterations: 61-70 Iterations: 71-80 Iterations: 81-90 Iterations:91-100
0.0 0.0 0.0 0.0
0.1
0.2 0.2 0.2 0.2
0.3
0.4 0.4 0.4
0.4
0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0
Episodes(2e5) ×105 Episodes(2e5) ×105 Episodes(2e5) ×105 Episodes(2e5) ×105
13
draweR
egarevA
draweR
egarevAFusion-BR in PSRO (Ours) Orthogonal-BR in PSRO
Iterations: 61-70 Iterations: 71-80 Iterations: 81-90 Iterations:91-100
0.0 0.0 0.0 0.0
0.1 0.1 0.1
0.2 0.2 0.2 0.2
0.3 0.3 0.3
0.4 0.4 0.4 0.4
0.5
0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0
Episodes(2e5) ×105 Episodes(2e5) ×105 Episodes(2e5) ×105 Episodes(2e5) ×105
Fusion-BR in PSD-PSRO (Ours) Orthogonal-BR in PSD-PSRO
Iterations: 61-70 Iterations: 71-80 Iterations: 81-90 Iterations:91-100
0.0 0.0 0.0
0.0
0.1
0.1
0.2 0.2 0.2 0.2
0.3
0.3 0.4 0.4
0.4
0.4
0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0
Episodes(2e5) ×105 Episodes(2e5) ×105 Episodes(2e5) ×105 Episodes(2e5) ×105
Fusion-BR in PSRO (Ours) Kaiming-BR in PSRO
Iterations: 61-70 Iterations: 71-80 Iterations: 81-90 Iterations:91-100
0.0 0.0 0.0 0.0
0.1 0.1 0.1
0.2
0.2 0.2 0.2
0.3 0.3
0.3 0.4
0.4 0.4
0.4
0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0
Episodes(2e5) ×105 Episodes(2e5) ×105 Episodes(2e5) ×105 Episodes(2e5) ×105
Fusion-BR in PSD-PSRO (Ours) Kaiming-BR in PSD-PSRO
Iterations: 61-70 Iterations: 71-80 Iterations: 81-90 Iterations:91-100
0.0 0.0 0.0 0.0
0.1 0.1 0.1
0.2 0.2 0.2 0.2
0.3
0.3 0.3
0.4 0.4
0.4 0.4
0.5
0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0
Episodes(2e5) ×105 Episodes(2e5) ×105 Episodes(2e5) ×105 Episodes(2e5) ×105
Fusion-BR in PSRO (Ours) Inherited-BR in PSRO
Iterations: 61-70 Iterations: 71-80 Iterations: 81-90 Iterations:91-100
0.04 0.04
0.01 0.01
0.02 0.02
0.00 0.00
0.00 0.00 0.01 0.01
0.02 0.02 0.02 0.02
0.03 0.03
0.04 0.04
0.04
0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0
Episodes(2e5) ×105 Episodes(2e5) ×105 Episodes(2e5) ×105 Episodes(2e5) ×105
Fusion-BR in PSD-PSRO (Ours) Inherited-BR in PSD-PSRO
Iterations: 61-70 Iterations: 71-80 Iterations: 81-90 Iterations:91-100
0.06
0.06
0.04 0.04 0.02 0.02
0.02 0.02
0.00 0.00 0.00 0.00
0.02 0.02 0.02 0.02
0.04 0.04
0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0
Episodes(2e5) ×105 Episodes(2e5) ×105 Episodes(2e5) ×105 Episodes(2e5) ×105
Figure5: AverageRewardforeachiteration(61-100)duringthetrainingofeachapproximateBR
withinPSROandPSD-PSRO.
14
draweR
egarevA
draweR
egarevA
draweR
egarevA
draweR
egarevA
draweR
egarevA
draweR
egarevAC TheoreticalAnalysis
Consider a set of trained policy models {πj}N for player i, each with corresponding network
i j=1
parameters{θ }N . Thesemodelshavebeentrainedinthesameenvironmentbutagainstdifferent
πj j=1
i
opponents. Theobjectiveistofusethesemodelsintoasinglepolicymodel,π ,withparameters
fusion
θ .
πfusion
Assumption1. Theweightedaverageatthepolicynetworkparameterlevelprovidesafirst-order
approximationtotheensembleatthepolicyoutputlevelwhenthepolicynetworkparametersare
similar. Usingthefirst-orderTaylorexpansion,wehave:
N
π ≈π
+(cid:88)∂π
ensemble(πj −π ), (8)
fusion ensemble ∂πj i ensemble
j=1 i
whereπ =(cid:80)N w πj,andtheweightsw satisfy(cid:80)N w =1.
ensemble j=1 j i j j=1 j
Lemma1. IfAssumption1holds,thefusedpolicyandtheensemblepolicyatthepolicyoutputlevel
aresimilar. Thatis,
N
(cid:88)
π ≈ w πj =π . (9)
fusion j i ensemble
j=1
Proof. Inmostcases, ∂πensemble isaconstantbecauseπ isalinearcombinationofπj. Therefore,
∂πj ensemble i
i
thistermsimplifiesto:
N N (cid:32) N (cid:33)
(cid:88) (cid:88) (cid:88)
π ≈ w πj + w πj − w πk . (10)
fusion j i j i k i
j=1 j=1 k=1
Since
N (cid:32) N (cid:33) N N N
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
w πj − w πk = w πj − w w πk,
j i k i j i j k i
j=1 k=1 j=1 j=1 k=1
andbecause(cid:80)N
w =1,itfollowsthat:
k=1 k
N N N
(cid:88) (cid:88) (cid:88)
w w πk = w πk,
j k i k i
j=1 k=1 k=1
thus,
N (cid:32) N (cid:33) N N
(cid:88) (cid:88) (cid:88) (cid:88)
w πj − w πk = w πj − w πk =0.
j i k i j i k i
j=1 k=1 j=1 k=1
Aboveall,wehave
N
(cid:88)
π ≈ w πj. (11)
fusion j i
j=1
Lemma2. Theexpectedutilityoftheensemblepolicycanbeexpressedastheweightedsumofthe
expectedutilitiesoftheindividualpolicies:
N
(cid:88)
U(π ,π )= w U(πj,π ). (12)
ensemble −i j i −i
j=1
Proof. Foramixedstrategyπ =(cid:80)N w πj againstanopponent’sstrategyπ ,theexpected
ensemble j=1 j i −i
utilityis:
U(π ,π )=E [u(a,b)]. (13)
ensemble −i πensemble,π−i
15whereu(a,b)istheutilityfunctiondependingontheactionatakenbyπ ,andtheactionb
ensemble
takenbyπ .
−i
Becausethemixedstrategyπ canbeexpressedasalinearcombinationofpurestrategies:
ensemble
N
(cid:88)
π (a)= w πj(a),
ensemble j i
j=1
thus,theexpectedutilitycanbewrittenas:
(cid:88)(cid:88)
U(π ,π )= π (a)π (b)u(a,b)
ensemble −i ensemble −i
a b
 
N
(cid:88)(cid:88) (cid:88)
=  w jπ ij(a)π −i(b)u(a,b).
a b j=1
Theexpectedutilityofeachpurestrategyπj againstπ is:
i −i
(cid:88)(cid:88)
U(πj,π )= πj(a)π (b)u(a,b).
i −i i −i
a b
Substitutingback,weget:
N N
(cid:88) (cid:88)(cid:88) (cid:88)
U(π ,π )= w πj(a)π (b)u(a,b)= w U(πj,π ). (14)
ensemble −i j i −i j i −i
j=1 a b j=1
C.1 ProofofNash-weightedAverageInitialization
Proposition1. BasedonAssumption1,whentheweightsequalNashprobabilities,theNashfusion
policy is an approximate Nash policy in the restricted game. If we define the expected utility as
U(π ,π ),then
i −i
U(π ,π )≳U(πj,π ) ∀πj ∈Π , (15)
fusion −i i −i i i
whereπj representsthej-thhistoricalpolicyofplayeriandΠ isthepolicysetofplayeriinthe
i i
currentrestrictedgame.
Proof. Based on Lemma 1 and 2, we have that combined utility function for Nash-weighted
initializationis:
 
N N
(cid:88) (cid:88)
U(π fusion,π −i)≈U w ijπ ij,π −i= w jU(π ij,π −i). (16)
j=1 j=1
InaNashequilibrium,theprobabilities{p }N associatedwitheachpolicyaresuchthat:(cid:80)N p =
i i=1 i=1 i
1andnoplayercanunilaterallyimprovetheirutilitybychangingtheirstrategy. Wesetw = p ,
j i
then:
N
(cid:88)
w U(πj,π )≥U(πj,π )for∀πj ∈Π . (17)
j i −i i −i i i
j=1
Aboveall,weget
U(π ,π )≳U(πj,π )for∀πj ∈Π . (18)
fusion −i i −i i i
It means that Nash-weighted average enable scalable trade-offs between near-optimal historical
policies,describeasEq.18.
16C.2 SuperiorityofNash-weightedInitialization
Proposition2. IfAssumption1holds,Nash-weightedaverageinitializationissuperiortoinherit
initializationandscratchinitialization. Fortheirexpectedutilities,wehave
U(π ,π ;θ )≳U(π ,π ;θ )≫U(π ,π ;θ ), (19)
fusion −i πfusion inherit −i πinherit scratch −i πscratch
where(π ,θ ),(π ,θ )and(π ,θ )arethepoliciesandparametersinitialized
fusion πfusion inherit πinherit scratch πscratch
bythefusion,inherit,andscratchmethods,respectively.
Proof. Forinheritinitialization,theutilityfunctionisgivenby:
U(π ,π ;θ )=U(π ,π ;θ ), (20)
inherit −i πinherit inherit −i π ij
forsomejin{1,2,...,N},πj representsthej-thhistoricalpolicyofplayeri.
i
ForNash-weightedaverageinitialization,theutilityfunctionis:
N
(cid:88)
U(π ,π ;θ )=U(π ,π ; w θ ). (21)
fusion −i πfusion fusion −i j π ij
j=1
Forscratchinitialization,theutilityfunctioncanbedefiniedas:
U(π ,π ;θ ), (22)
scratch −i πscratch
whereθ canbeinitializedusingvariousinitializationapproaches,includingnormalinitial-
πscratch
ization[26],inherited[12],orthogonalinitialization[27],andKaiminginitialization[10],among
others.
-Comparison: BasedonLemma1and 2,thefirsttermintheNash-weightedutilityfunctionisa
convexcombinationofnear-optimalutilities,whichisatleastasgoodasanysinglenear-optimal
utility:
N
(cid:88)
w U(π ,π ;θ )≥U(π ,π ;θ ), (23)
i i −i πi inherit −i πj
i
i=1
therefore:
U(π ,π ;θ )≳U(π ,π ;θ ). (24)
fusion −i πfusion inherit −i πinherit
Anditissignificantlybetterthanascratchinitialization:
N
(cid:88)
w U(π ,π ;θ )≫U(π ,π ;θ ), (25)
i fusion −i πi scratch −i πscratch
i=1
therefore:
U(π ,π ;θ )≫U(π ,π ;θ ). (26)
fusion −i πfusion scratch −i πscratch
Aboveall,wehave:
U(π ,π ;θ )≳U(π ,π ;θ )≫U(π ,π ;θ ). (27)
fusion −i πfusion inherit −i πinherit scratch −i πscratch
Proposition3. TheNash-weightedinitializationcombinestheexplorationcapabilitiesofmultiple
policies,witheachpolicycontributingtotheoverallexploration:
N
(cid:88)
U(π ,π ;θ )≈ w U(π ,π ;θ )
fusion −i πfusion j fusion −i πj
i
j=1
1(cid:88)N (28)
+ w (θ −θ )TH (θ −θ ).
2 j πfusion π ij j πfusion π ij
j=1
(cid:124) (cid:123)(cid:122) (cid:125)
Explorationterm
whereθ representstheparametersofthej-thpolicyinplayeri’spolicyset.
πj
i
17Proof. GiventheexpectedutilityasU(π ,π ;θ ),usingtheTaylorexpansionandsummingover
i −i πi
allpoliciesofplayeri,weget:
N
(cid:88)
U(π ,π ;θ )≈ w U(π ,π ;θ )
fusion −i πfusion j fusion −i π ij
j=1
N
(cid:88)
+ w ∇U(π ,π ;θ )T(θ −θ ) (29)
j fusion −i π ij πfusion π ij
j=1
N
1(cid:88)
+ w (θ −θ )TH (θ −θ ),
2 j πfusion π ij j πfusion π ij
j=1
whereH istheHessianmatrixofU(π ,π ;θ)atθ .
j fusion −i πj
i
Tosimplifythesecondterm,weusethedefinitionofθ :
πfusion
N
(cid:88)
θ −θ = w θ −θ . (30)
πfusion πi k π ik πi
k=1
Thus,
N N (cid:32) N (cid:33)
(cid:88) (cid:88) (cid:88)
w ∇U(π ,π ;θ )T(θ −θ )= w ∇U(π ,π ;θ )T w θ −θ .
j fusion −i π ij πfusion π ij j fusion −i π ij k π ik π ij
j=1 j=1 k=1
(31)
Since(cid:80)N
w =1,wesimplifythistermfurther:
k=1 j
N (cid:32) N N (cid:33)
(cid:88) (cid:88) (cid:88)
w ∇U(π ,π ;θ )T w θ − w θ . (32)
j fusion −i πj k πk k πj
i i i
j=1 k=1 k=1
Consideringθ asfixedparametersforeachpolicymodel,werewritethesecondtermas:
πj
i
 
N N
(cid:88) (cid:88)
w j∇U(π fusion,π −i;θ πj)T  w k(θ πk −θ πj). (33)
i i i
j=1 k̸=j
Giventhenear-optimalityandsimilarityofweights,thistermbecomesrelativelysmall,leavingus
withtheprimarytermandthehigher-ordercorrection:
N
(cid:88)
U(π ,π ;θ )≈ w U(π ,π ;θ )
fusion −i πfusion j fusion −i π ij
j=1
1(cid:88)N (34)
+ w (θ −θ )TH (θ −θ ).
2 j πfusion π ij j πfusion π ij
j=1
(cid:124) (cid:123)(cid:122) (cid:125)
Explorationterm
D BenchmarkandImplementationDetails
D.1 Non-TransitiveMixtureGame
ThisgameischaracterizedbysevenGaussianhumpsthatareevenlyspacedonatwo-dimensional
plane. Eachpolicyinthegamecorrespondstoapointonthisplane,analogoustotheweights(the
probabilityofthatpointwithineachGaussiandistribution)thatplayersallocatetothehumps. The
18payoffcontainingbothnon-transitiveandtransitivecomponentsisπTSπ + 1(cid:80)7 (πk−πk ),
i −i 2 k=1 i −i
where
 0 1 1 1 −1 −1 −1
−1 0 1 1 1 −1 −1
 
−1 −1 0 1 1 1 −1
 
S =−1 −1 −1 0 1 1 1 .
 
 1 −1 −1 −1 0 1 1 
 1 1 −1 −1 −1 0 1 
1 1 1 −1 −1 −1 0
Contrarytoconventionalapproaches,wherePSROandPSRO aretypicallylimitedbythenumber
rN
of threads of only 1—compared to 4 in other PSRO variants—thus severely constraining their
exploration capabilities, we opted for a different way. To mitigate this constraint, we tripled the
trainingepisodesforthesetwoalgorithmscomparedtoothers,aimingtobalancetheexploration
limitations.
D.2 LeducPoker
SinceDiverse-PSROcannotscaletotheRLsettingandthecodeforBD&RD-PSROincomplex
games is unavailable, we compare PSRO and PSD-PSRO with their policy fusion counterparts,
Fusion-PSROandFusion-PSD-PSRO.WeimplementthePSROparadigmwithNashsolver,using
DQNastheoracleagent. Hyper-parametersareshowninTable1.
Table1: HyperparametersforLeducPoker.
Hyperparameters Value
Oracle
Oracleagent DQN
Replaybuffersize 104
Mini-batchsize 512
Optimizer Adam
Learningrate 5×10−3
Discountfactor(γ) 1
Epsilon-greedyExploration(ϵ) 0.05
Targetnetworkupdatefrequency 5
Policynetwork MLP(state_dim-256-256-256-action_dim)
ActivationfunctioninMLP ReLu
PSRO
EpisodesforeachBRtraining 2×104
meta-policysolver Nash
PSD-PSRO
EpisodesforeachBRtraining 2×104
meta-policysolver Nash
diversityweight(λ) 0.1
Fusion-PSROFramework
Fusionstartcondition(c) 2
Top-kselectionfactor(α) 0.1
Maximumnumberofbasepolicies(k=|Π∗|=α|Π |) 4
i i
Minimumnumberofbasepolicies(k=|Π∗|=α|Π |) 2
i i
D.3 LiarsDice
TheLiarsDicegameinvolvestwoplayers,eachequippedwithasingledie. Incorrectchallengesor
bidsresultinimmediatelossofthegame,astheyleadtothelossofdice. WeimplementthePSRO
paradigmwithNashsolver,usingRainbow-DQNastheoracleagent. Hyper-parametersareshownin
Table2.
19Table2: HyperparametersforLiarsDice.
Hyperparameters Value
Oracle
Oracleagent Rainbow-DQN
Replaybuffersize 105
Mini-batchsize 512
Optimizer Adam
Learningrate 5×10−4
Learningratedecay lineardecay
Discountfactor(γ) 0.99
Epsilon-greedyExploration(ϵ) 0.05
Targetnetworkupdatefrequency 5
Networksoftupdateratio 0.005
PrioritizedExperienceReplayparameter 0.6
Importantsamplingparameter 0.4
Gradientclip 10
Policynetwork MLP(state_dim-256-256-128-action_dim)
ActivationfunctioninMLP ReLu
PSRO
EpisodesforeachBRtraining 2×105
meta-policysolver Nash
PSD-PSRO
EpisodesforeachBRtraining 2×105
meta-policysolver Nash
diversityweight(λ) 0.1
Fusion-PSROFramework
Fusionstartcondition(c) 2
Top-kselectionfactor(α) 0.1
Maximumnumberofbasepolicies(M =|Π∗|=α|Π |) 4
i i
Minimumnumberofbasepolicies(M =|Π∗|=α|Π |) 2
i i
20