Mixed Diffusion for 3D Indoor Scene Synthesis
SiyiHu1 DiegoMartinArroyo2 StephanieDebats2 FabianManhardt2
LucaCarlone1 FedericoTombari2,3
1MassachusettsInstituteofTechnology 2Google,Inc 3TechnischeUniversitätMünchen
{siyi, lcarlone}@mit.edu
{martinarroyo, sdebats, fabianmanhardt, tombari}@google.com
Abstract
Realisticconditional3Dscenesynthesissignificantlyenhancesandacceleratesthe
creationofvirtualenvironments,whichcanalsoprovideextensivetrainingdatafor
computervisionandroboticsresearchamongotherapplications. Diffusionmodels
haveshowngreatperformanceinrelatedapplications,e.g.,makingprecisearrange-
ments of unordered sets. However, these models have not been fully explored
infloor-conditionedscenesynthesisproblems. WepresentMiDiffusion,anovel
mixeddiscrete-continuousdiffusionmodelarchitecture, designedtosynthesize
plausible 3D indoor scenes from given room types, floor plans, and potentially
pre-existingobjects. Werepresentascenelayoutbya2Dfloorplanandasetof
objects,eachdefinedbyitscategory,location,size,andorientation. Ourapproach
uniquely implements structured corruption across the mixed discrete semantic
andcontinuousgeometricdomains,resultinginabetterconditionedproblemfor
thereversedenoisingstep. Weevaluateourapproachonthe3D-FRONTdataset.
OurexperimentalresultsdemonstratethatMiDiffusionsubstantiallyoutperforms
state-of-the-artautoregressiveanddiffusionmodelsinfloor-conditioned3Dscene
synthesis. In addition, our models can handle partial object constraints via a
corruption-and-maskingstrategywithouttaskspecifictraining. WeshowMiDiffu-
sionmaintainsclearadvantagesoverexistingapproachesinscenecompletionand
furniturearrangementexperiments.
1 Introduction
Generating diverse and realistic 3D indoor scenes has attracted extensive research in computer
visionandgraphics. Inrecentyears,thisproblemhasbeenlargelymotivatedbyinteriordesignsof
apartmentsandhouses,aswellasautomatic3Dvirtualenvironmentcreationforvideogamesand
data-driven3Dlearningtasks[25,50,8,51].
Wefocusonobject-based3Dindoorscenesynthesisinwhichthegoalistopredictasetofobjects
characterizedbysemanticlabelsandgeometricarrangements. Resultsusingstate-of-the-artautore-
gressivemodelsshowthat,whileitisrelativelyeasytopredictreasonableobjectlabelsgivenaroom
type,themainbottleneckisgeneratingaccurategeometricarrangementsofobjects,i.e.,predicting
realisticpositionsandorientations. Autoregressivemodelsmightfailbecausetheycannotreverta
badpredictioninanearlierpreviousstep. Therefore,“de-noising”approachesthatiterativelyrefine
predictedresultsarebettersuitedforthistypeofproblems[46].
Inthiswork,weproposetoadoptdiffusionmodels[35]forthisproblem,astheiterativedenoising
schemeiswellsuitedforimprovingobjectgeometryin3Dscenes. DenoisingDiffusionProbabilistic
Models (DDPM) [15] proposed Gaussian corruption and denoising of continuous domain data,
resulting in efficient training of diffusion models for image synthesis tasks. Discrete Denoising
Preprint.Underreview.
4202
yaM
13
]VC.sc[
1v66012.5042:viXraDiffusionProbabilisticModels(D3PM)[2]extendedthepioneeringworkofHoogeboometal.[16]to
structuredcategoricalcorruptionprocessesfordataindiscretestatespace.Unlikecontinuousdiffusion
models,discretediffusioninvolvesdatacorruptionandrecoveryusingtransitionprobabilitiesbetween
discretestates. Sinceobjectattributesarenaturallydefinedonbothdiscreteandcontinuousdomains,
weproposetocombinetheseapproachestoformamixeddiscrete-continuousdiffusionmodelfor3D
scenesynthesisproblems.
Thereareafewrecentworksadoptingdiffusionmodelsoriterativedenoisingstrategiesfor3Dscene
synthesisproblems[17,13,46,41]. TheclosestworktooursisDiffuScene[41],whichgenerates
3DindoorsceneswithoutfloorplanconstraintsusingDDPMbyfirstconvertingsemanticlabelsto
one-hotencodingvectorsinthecontinuousdomainandthenusingargmaxtoretrievelabelpredictions.
We instead focus on floor-conditioned problems, as these are more practical setups in automatic
designofrealorvirtualworlds. Wepresentthreetechnicalcontributionsinthispaper:
• WeproposeMiDifussion,anovelmixeddiscrete-continuousdiffusionmodelcombining
DDPMandD3PMtoiterativelydenoiseobjects’semanticandgeometricattributeswhich
arenaturallydefinedindiscreteandcontinuousdomains,respectively.
• Wedesignatimevarianttransformer-baseddenoisingnetworkwithfloorplanfeaturesas
conditionvectorsforourmixeddiffusionapproach.
• Forapplicationswithpartialobjectconstraints,suchasscenecompletiongivenexisting
objects,weproposeacorruption-and-maskingstrategytohandleknownobjectconditions
withouttheneedtore-trainthemodels.
We compare MiDiffusion against state-of-the-art autoregressive and diffusion models in floor-
conditioned3Dscenesynthesisproblems.Experimentalevaluationusingthecommon3D-FRONT[9]
datasetdemonstratesthatourapproachgeneratesmorerealisticscenelayouts,outperformingexisting
methods. Weprovideablationstudiessupportingourdesignofthemixeddiffusionmodelandthe
denoisingnetwork. Withoutre-training,wealsoshowexperimentalevaluationsonscenecompletion
andfurniturearrangement,confirmingthatourapproachmaintainssuperiorperformanceonlarger
scenes. Codeanddataareavailableathttps://github.com/MIT-SPARK/MiDiffusion.
2 RelatedWorks
3DSceneSynthesis. 3Dscenesynthesisistypicallyachievedbygeneratingasetofobjectlayouts
fromscratch. Classicalapproachesinthecomputergraphicscommunitytypicallyemployprocedural
modelingtoapplyasetoffunctionsthatcaptureobjectrelationshipsinindoor[50,25,49,32]and
outdoor[40,25,49,29,18,5]settings. Ahandfulofmethodsarebasedongraphrepresentationsof
thescene. Meta-Sim[18]andMeta-Sim2[5]learntomodifyattributesofscenegraphsobtainedfrom
probabilisticscenegrammarstomatchsynthesizedandtargetimages.SG-VAE[30]learnsagrammar-
basedauto-encodertocapturerelationshipsanddecodethelatentspacetoaparsetree. Mostrecently,
it is common to learn inter-object relationships implicitly without specifying hand-crafted rules.
Existingmodelsincludefeed-forwardnetworks[53],VAEs[30,47],GANs[48],andautoregressive
models[32,33,43,23,53,54,27,44,28]. Thestate-of-the-artautoregressivemodels[44,28]are
builtwithatransformer[42]backbone,capableofwellmodellingobjectinteractions.
DiffusionModels. Diffusionprobabilisticmodels[38,15,2,26,36]aregenerativemodelsdefined
bytwoMarkovprocesses. Theforwardprocessslowlyinjectsrandomnoisetothedata,whereas
thereversedenoisingprocessrecoversthedata. Conceptually,thisapproachhasconnectionswith
denoisingscorematchingmethods[37,39]. Followingthepioneeringworkby[35],diffusionmodels
firstgainedpopularityin2Dimagesynthesis[19,36],outperformingexistingworks[20,6]. Starting
fromDenoisingDiffusionProbabilisticModels(DDPM),diffusionmodelstypicallyworkwithlatent
variablesinthecontinuousdomain. However,itismorenaturaltorepresentdiscretevariables,such
as text, in discrete state space. Discrete diffusion models were first applied to text generation in
argmax flow [16], and then Diffusion Probabilistic Models (D3PMs) [2] and VQ-Diffusion [11]
showedstrongresultsoncharacterlevelimagegenerationandtext-to-imagesynthesis.
DiffusionModelsforLayoutSynthesis. Veryrecently,diffusionmodelsbecamepopularforlay-
out synthesis, including document layout generation [17, 13], 3D scene synthesis [41], furniture
re-arrangement [46, 41], graph-conditioned 3D layout generation [52, 24], and text conditioned
scene synthesis [7]. LayoutDM [17] applies diffusion models in discrete state space. After dis-
cretizingpositionandsizetoafixednumber ofbins, theyusethediscretecorruptionprocessby
VQ-Diffusion[11]totrainthereversetransformernetworkforpredictingcategory,position,and
2sizefordocumentlayoutgeneration. Mostrelatedtoours,DiffuScene[41]predicts3Dscenelayout
using DDPM after converting semantic labels to one-hot encodings to allow joint diffusion with
geometricattributesinthecontinuousdomain. Incontrast,ourdiffusionmodelworksdirectlyonthe
mixeddiscrete-continuousdomains. Moreover,wedesignanovelcorruption-and-maskingstrategy
forreversediffusionunderpartialobjectconstraints,withouttheneedtore-trainanymodels.
3 Preliminary: DiffusionModels
Diffusionmodels[35]typicallyconsistoftwoMarkovprocesses: aforwardcorruptionprocessanda
reversedenoisingprocess. Theforwardprocessq(x |x
)=(cid:81)T
q(x |x )typicallycorrupts
1:T 0 t=1 t t−1
the data x ∼ q(x ) into a sequence of latent variables x by iteratively injecting controlled
0 0 1:T
noise. Thereverseprocessp (x )=p(x
)(cid:81)T
p (x |x )progressivelydenoisesthelatent
θ 0:T T t=1 θ t−1 t
variablesviaalearneddenoisingnetworkθ. Thestandardapproachtotrainthedenoisingnetworkis
tominimizethevariationalboundonthenegativelog-likelihood:
(cid:20) (cid:21)
p (x )
E [−logp (x )]≤E −log θ 0:T =:L (1)
q(x0) θ 0 q(x0:T) q(x |x ) vb
1:T 0
whichcanbere-arrangedto
T
(cid:88)
L =E [D (q(x |x )||p(x ))+ E [D (q(x |x ,x )||p (x |x ))]
vb q(x0) KL T 0 T q(xt|x0) KL t−1 t 0 θ t−1 t
(cid:124) (cid:123)(cid:122) (cid:125) t=2(cid:124) (cid:123)(cid:122) (cid:125)
LT Lt−1 (2)
+E [−logp (x |x )]].
q(x1|x0) θ 0 1
(cid:124) (cid:123)(cid:122) (cid:125)
L0
Iftheforwardprocessinjectsknownnoise,theapproximateposteriorqhasnolearnableparameters
andhenceL canbeignored. Then,efficientlearningofnetworkθrequires: (1)efficientsampling
T
ofx fromq(x |x );and(2)tractablecomputationofq(x |x ,x ).
t t 0 t−1 t 0
Diffusion models also apply to conditional synthesis problems, such as text conditioned image
generation[15,2]. Givenaconditionalinputy,wecanwritethereverseprocessasp (x |y)=
θ 0:T
p(x
)(cid:81)T
p (x |x ,y)andsubstitutep (x |x )byp (x |x ,y)inEq.(2). Wedropyin
T t=1 θ t−1 t θ t−1 t θ t−1 t
theremainderofthissectionforsimplicity.
ContinuousStateSpace. DenoisingDiffusionProbabilisticModel(DDPM)[15]injectsGaussian
noisetoacontinuousstatevariablex withfixedvariancescheduleβ ,...,β ∈(0,1):
0 1 T
(cid:112)
q(x |x ):=N(x ; 1−β x ,β I). (3)
t t−1 t t t−1 t
Therefore,thereisaclosedformexpressionforsamplingx givenx atanytimet:
t 0
√
q(x |x )=N(x ; α¯ x ,(1−α¯ )I), (4)
t 0 t t 0 t
whereα := 1−β andα¯ :=
(cid:81)t
α . Theforwardprocessposteriorsarealsotractablewhen
t t t s=1 s
conditionedonx :
0
q(x |x ,x )=N(x ;µ˜ (x ,x ),β˜I), (5)
t−1 t 0 t−1 t t 0 t
√ √
where µ˜ (x ,x ) := α¯t−1βtx + αt(1−α¯t−1)x and β˜ := 1−α¯t−1β . To best approximate
t t 0 1−α¯t 0 1−α¯t t t 1−α¯t t
Eq.(5),thereversedistributionsareGaussianaswell:
p (x |x )=N(x ;µ (x ,t),Σ (x ,t)). (6)
θ t−1 t t−1 θ t θ t
Inpractice,thecovarianceΣ (x ,t)=σ2I canbelearned,orfixedtoσ2 =β˜ orσ2 =β .
θ t t t
Discrete State Space. For discrete data, such as semantic labels, it is more natural to define
corruptioninthediscretedomain. WedenoteascalardiscretevariablewithK categoriesbyz ∈ZK
andusev(z )∈{0,1}K torepresentitsone-hotencoding. Thevariationalboundovercontinuous
t
vector x in Eq. (2) also holds for z. For multi-dimensional z, training loss is summed over all
elements. ThentheforwardprocessattimetisdefinedbyatransitionprobabilitymatrixQ ,with
t
[Q ] =q(z =m|z =n). Thecategoricaldistributionoverz givenz is
t mn t t−1 t t−1
q(z |z ):=Cat(z ;p=Q v(z ))=v(z )TQ v(z ). (7)
t t−1 t t t−1 t t t−1
3WiththeMarkovproperty,wecanderive
q(z |z )=v(z )TQ¯ v(z ) (8)
t 0 t t 0
(cid:0) v(z )TQ v(z )(cid:1)(cid:0) v(z )TQ¯ v(z )(cid:1)
t t t−1 t−1 t−1 0
q(z |z ,z )= (9)
t−1 t 0 v(z )TQ¯ v(z )
t t 0
where Q¯ = Q Q ···Q . The denoising network θ is trained to compute the categorical
t t t−1 1
distributionsp (z |z ). Weimplementthemask-and-replacestrategybyVQ-Diffusion[11],which
θ t−1 t
improvesupontheproposedchoicesofQ in[2]byintroducinganadditionalspecialtoken[MASK].
t
4 3DSceneSynthesisviaMixedDiffusion
WepresentMiDiffusion,amixeddiscrete-continuousdiffusionmodelfor3Dindoorscenesynthesis.
WeassumeeachsceneS isinaworldframewiththeoriginatthecenter. Itconsistsofafloorplan
andatmostN objects. Wedenotethefloorplanbyaconditionalvectory. Eachobjectinthescene
ischaracterizedbyitssemanticlabelz ∈{1,2,...,C},centroidpositiont∈R3,boundingboxsize
s ∈ R3,androtationanglearoundtheverticalaxisϕ ∈ SO(2). InspiredbyLEGO-Net[46],we
representϕas[cos(γ),sin(γ)]TtomaintaincontinuityonSO(2)[55]. Weconcatenateallgeometric
attributestox=(t,s,ϕ). Then,wecanrepresentan3DindoorsceneasS =({(zi,xi)} ,y).
1≤i≤N
Wedesignatethelastsemanticlabelz =C toan“empty”labeltoworkwithscenescontainingless
thanN objectsanddefinegeometricattributestoallzerosforan“empty”object.
The problem of synthesising 3D scenes can be viewed as learning a network θ that maximizes
logp (zi,xi|y)overallobjectsandacrossallscenes,i.e.,thesamevariationalboundinEq.(2)but
θ 0 0
overtwodomains. Wedropthesuperscriptiforsimplicity.
4.1 DiffusioninMixedDiscrete-ContinuousDomains
Althoughobjectsemanticandgeometricattributesareinseparatedomains,wedefineacorruption
processinMiDiffusionthatindependentlyinjectsdomain-specificnoisestoz andx asinD3PM
t t
andDDPM,respectively:
q(z ,x |z ,x ):=q˜(z |z )·qˆ(x |x ). (10)
t t t−1 t−1 t t−1 t t−1
Thismeanswecansamplez ,x independentlyintraining,andfactortheposteriordistributionas
t t
q(z ,x |z ,x ,z ,x )=q˜(z |z ,z )·qˆ(x |x ,x ). (11)
t−1 t−1 t t 0 0 t−1 t 0 t−1 t 0
Inthebackwarddiffusionprocess,wecandesignanetworkθthatcomputesprobabilitydistributions
overlatentvariablesbydomainstobeconsistentwiththeforwardprocess:
p (z ,x |z ,x ,y):=p˜ (z |z ,x ,y)·pˆ (x |x ,z ,y). (12)
θ t−1 t−1 t t θ t−1 t t θ t−1 t t
Note that the general loss function to train diffusion models in Eq. (2) consists of two types:
{L } , the KL-divergence terms, and L , a negative log probability. The factorization
t−1 2≤t≤T 0
inEq.(10)to(12)allowsustore-arrangethesetermstoapairofdomainspecificlosses:
Lmixed =E [D (q(z ,x |z ,x ,z ,x )||p (z ,x |z ,x ,y))]
t−1 q(zt,xt|z0,x0) KL t−1 t−1 t t 0 0 θ t−1 t−1 t t
=E [D (q˜(z |z ,z )||p˜ (z |z ,x ,y))]
q˜(zt|z0) KL t−1 t 0 θ t−1 t t
(cid:124) (cid:123)(cid:122) (cid:125)
LD3PM (13)
t−1
+E [D (qˆ(x |x ,x )||pˆ (x |x ,z ,y))],
qˆ(xt|x0) KL t−1 t 0 θ t−1 t t
(cid:124) (cid:123)(cid:122) (cid:125)
LDDPM
t−1
Lmixed =E [−logp (z ,x |z ,x ,y)]
0 q(z1,x1|z0,x0) θ 0 0 1 1
=E q˜(z1|z0)[−logp˜ θ(z 0|z 1,x 1,y)]+E qˆ(x1|x0)[−logpˆ θ(x 0|x 1,z 1,y)]. (14)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
LD3PM LDDPM
0 0
Since (x ,y) is the joint condition term in the reverse step p to predict z , the first term in
t θ t−1
Eq.(13)matchesexactlyL inD3PM.Similarly,thesecondtermmatchesL inDDPM.The
t−1 t−1
4factorizationinEq.(14)issimilartothatinEq.(13). Weprovidemoredetailsaboutthefactorization
stepinEq.(13)inAppendixB.
CombiningEq.(13),(14),wegettheexactsumofvariationalboundsinD3PMandDDPM.Intuitively,
summing over discrete and continuous losses is similar to summing losses over each coordinate
givenmulti-dimensionaldatainvanillaDDPMorD3PM.WeuseasimplifiedversionofLDDPM
vb
afterre-parameterizingtheforwardprocessassuggestedinDPPMtoimprovetrainingstabilityand
efficiency. Weaddanauxiliaryloss,LD3PM,asproposedinD3PMtoencouragegoodpredictions
aux
forz ateachtimestep. Weincludere-parameterizationstepsassuggestedbyDDPMandD3PMin
0
AppendixA.ThecombinedlossforMiDiffusionis:
Lmixed =LDDPM +LD3PM +λLD3PM. (15)
vb vb aux
4.2 DenoisingNetwork
S ate trm iba un tetic eS me bm ea dn dt inic g MLP
Geometric
attributes MLP MLP
Transformer decoder
t
time
Floor plan C
input AdaLN M Au tl tt ei- nH te ioa nd Ad0aLN M Au tl tt ei- nH te ioa nd foF re we ad rd output
Position
embedding
condition
Figure1: Denoisingnetwork. Thetime-variantdecoderbackbonetakesobjectfeaturesasinput,
conditionedbythefloorplanfeatureafterconcatenationwithpositionalembeddingsofobjectindices.
Weutilizethetime-varianttransformerdecoderblockfromVQ-Diffusion[11].
Weuseatransformer-baseddenoisingnetworkasshowninFig.1,whichtakesinlatentvariables
(z ,x )atsteptandpredictsthecategoricaldistributionp (z |z ,x ,y),togetherwiththeGaussian
t t θ t−1 t t
meanofp (x |x ,z ,y)assumingafixedcovariance. Attesttime,wecansimplysamplefrom
θ t−1 t t
thesedistributionstoretrieve(z ,x ).
t−1 t−1
Featureencoder. WeencodetheobjectfeaturesbypassinggeometricattributesthroughanMLPand
combinethemwithtrainablesemanticembeddings. Foreachscene,wesample256pointsfroma
floorplanimagealongtheboundaryandcomputeoutwardnormalvectors. WeusethePointNet[31]
floorplanfeatureextractorfromLEGO-Net[46]asitismorelightweightandbettercapturesfloor
boundary than image-based feature extractors. The floor plan feature is then concatenated with
thelearnedpositionembedding,whichiscommonlyusedintransformerarchitectures,toformthe
conditioningvector.
Transformerdecoder. Thebackboneofourdenoisingnetworkisatime-varianttransformerdecoder
adaptedfromVQ-Diffusion[11].Weshowinexperimentallythatthemulti-headattentionmechanism
inthetransformerdecoderbettercapturesboundaryconstraintsthantheconditional1-DU-Net[34]
architectureproposedinDiffuScene[41]. WeshowourdenoisingnetworkinFig.1. Withineach
transformerblock,thetimestepisinjectedthroughtheAdaptiveLayerNormalization(AdaLN)[3]
operator. Theconditioninginputsaresentthroughamulti-headcross-attentionlayer.
Featureextractor.WefeedtheoutputofthetransformerdecodertotwoMLPstogeneratecategorical
distributionforthesemanticlabelsandGaussianmeansforthegeometricfeatures.
4.3 PartialConditiononObjects
3Dscenesynthesisissometimesconstrainedbyknownobjectinformation,suchasscenecompletion
givenexistingobjects. DiffuScene[41]considerstheseasadditionalconditioninputstothedenoising
network,whichrequiresatleastatask-specificfeatureencoderfortheconditioninputsandhence
re-training of the entire model. We instead propose to address this flexibility issue through a
corruption-and-maskingstrategywhichdoesnotrequireanyadditionaltraining. Sincethecoreidea
ofdiffusionmodelsistorecoverrealdatabylearningtheinverseofaknowncorruptionprocess,we
canforcetheconstrainedsectionsofthelatentvariablestofollowexactlytheinverseofthecontrolled
5
teNtnioP
kcolb
remrofsnarT
kcolb
remrofsnarTcorruptionstepsinthedenoisingprocess. Forexample,forscenecompletion: givenM objects,we
firstcomputeasequenceoflatentvariables{(zi,xi)} throughthecorruptionprocess. This
t t 1≤i≤M
allowsustomaskoutthefirstM objectsusingpre-computedlatentvariablesattimetinthereverse
process. Theselatentvariableseventuallyconvergetotheknowninput{(zi,xi)} without
0 0 1≤i≤M
additionaltraining. Thisstrategycangeneralizetootherapplications,suchasfurniturearrangement
andlabelconditionedscenesynthesis,bytargetingdifferentsectionsoflatentobjectvariables.
5 ExperimentalEvaluation
Inthissection,wepresentevaluationresultscomparingMiDiffusionagainstexistingmethodsand
provideablationstudies. Wealsoshowthat,withoutre-training,ourmodelscanbeappliedtoother
taskssuchasscenecompletionandfurniturearrangement. Additionalresultsandimplementation
detailscanbefoundintheappendix.
Dataset. We use the common benchmarking dataset, 3D-FRONT [9], for evaluation. This is a
syntheticdatasetof18,797roomsfurnishedby7,302texturedobjectsfrom3D-FUTURE[10]. We
evaluateonthreeroomtypes: 4041bedrooms,900diningrooms,and813livingrooms. Weusethe
samedataprocessingandtraining/testingdatasplitas[28]. Weuserotationaugmentationsin90◦
incrementsduringtrainingasin [41]tobeconsistentwiththebaselines.
Baselines. Wecompareourproposedapproachesagainsttwostate-of-the-art3Dscenesynthesis
methods:(1)ATISS[28],anautoregressivemodelwithatransformerencoderbackbonethatgenerates
sequentialpredictionsofobjects;(2)DiffuScene[41],adiffusionmodelwithaconditional1DU-
Net[34]denoisingbackbonebasedonDDPM.Wetrainbaselinemodelswiththeiropen-sourcecode
anddefaulthyperparameters. WecompareMiDiffusionwithDiffuSceneinitsoriginalapplication,
i.e.,unconditionedscenesynthesis,inAppendixE.2.
Implementation. WetrainallourmodelswiththeAdamoptimizer[21]atl = 2e−4 witha0.5
r
decayrateevery10kfor50kepochsonthebedroomdataset,andevery15kepochsfor10kepochs
onthediningroomandlivingroomdatasets. Wesetλ=0.05fortheauxiliarylosstermLD3PM.
aux
MoredetailsaboutimplementationandnetworkparametersareincludedinAppendixC.
EvaluationMetrics. Werandomlyselect1000floorplansfromthetestsetasinputandevaluate
modelsusingmetricsfoundinpriorworks[44,28,41]: KLdivergenceofobjectlabeldistributions
(KL×0.01),Fréchetinceptiondistance(FID),Kernelinceptiondistance(KID×0.001),andclassifi-
cationaccuracy(CA%)usingtheimplementationbyATISS.WhiletheKLdivergenceevaluatesthe
predictedobjectlabeldistributions,theothermetricsareusedtocomparethelayoutimageswiththe
ground-truthtestdata. Werendereachscenetoa256x256top-downorthographicprojectionimage
usingCADmodelsfrom3D-FUTURE.InlinewithDiffuScene,wecoloreachobjectaccordingtoto
itssemanticclass. Werenderbedroomsto6.1msquares,andthelivinganddiningroomsto12.2m
squares. AppendixDincludesexamplelayoutimages. WereportCA%over10runswithrandom
samplingofpredictedlayoutsfortrainingandtesting. Notethatrealisticlayoutsshouldhavealow
FID/KID,alowKLandaCA%thatiscloseto50%. Wealsocomputeadditionalmetrics,averaged
overallscenestofurtherevaluateobjectplacement,includingthenumberofgeneratedobjects(Obj),
percentageofout-of-boundaryobjects(OOB%),andboundingboxIntersectionoverUnion(IoU%).
Wedilatetheroomboundaryby0.1mforOOB%computationtoaccountforslightunderestimation
offloorboundaryintheraw3D-FRONTdata.
5.1 FloorPlanConditioned3DSceneSynthesis
WefirstcompareMiDiffusionagainstbaselinesinfloor-conditioned3Dscenesynthesisexperiments
withoutexistingobjects. Fig.2includequalitativeresultsrenderedinPyVista. Theobjectmeshis
retrievedfromthe3D-FUTURE[10]datasetbyfindingtheclosestmatchinsizeunderthepredicted
semanticcategory. WereplaceoriginaltexturesintheCADmodelsbylabelspecificcolorforclarity.
Weincludeexamplesoforthographicprojectionimages,whichweuseforquantitativeevaluations,
in the appendix. In general, our approach is able to generate realistic object arrangement whilst
respectingtheboundaryconstraints.ATISSgeneratesslightlylesspreciseobjectplacementcompared
tothetwodiffusionbasedapproaches,e.g.,thetable-chairsfurnituresetinthelastexampleofFig.2
isnotexactlysymmetric. BothDiffuSceneandATISSshowhighertendencytoplaceobjectsoutside
thefloorboundarythanours. ThisproblemismoreprominentforDiffuScene,mostlikelyduetothe
conditionalU-Netarchitectureintheirdenoisingnetwork,whichdoesnotlearnboundaryconstraints
equallywellastransformerarchitectures.
6ATISS
[28]
DiffuScene
[41]
MiDiffusion
(Ours)
Figure2: Floor-conditionedscenesynthesis. Themeshesareretrievedfromthe3D-FUTURE[10]
datasetbysizematchingwithinpredictedsemanticcategory. MiDiffusiongeneratesrealisticarrange-
mentswhilerespectingboundaryconstraints.
WeshowquantitativeresultsinTables1and2. Overall,ourapproachachievessubstantiallybetter
performanceingeneratingrealisticlayouts,especiallyinthediningroomandlivingroomdatasets.
TherearelargemarginsbetweenourmodelsandthebaselinemodelsforFID,KID×0.001andCA
%, comparingimagesofsynthesizedlayouts. Asforobjectlabeldistributions, thereseemstobe
nolargedifferencesbetweentheindividualmethodswithrespecttoKL×0.01. Infact,allmethods
haverathersmallKLvaluesandthedifferencecouldbeduetorandomsampling. InTable2,there
isatrade-offbetweenkeepingobjectsinsidetheboundary(lowOOB%)andmaintainingminimal
overlap(lowIoU%). AlthoughDiffuSceneachievesthelowestIoU%,theirmodelstendtoplace
moreobjectsoutsidetheboundarythantheothers,whichexplainstheirsub-optimalperformancein
Table1. Overall,ourapproachisstillthebestintermsofobjectplacementthatarecompatiblewith
otherpredictedobjectsandthefloorboundaryconstraints.
Bedroom Diningroom Livingroom
Method
FID KIDx0.001 CA% KLx0.01 FID KIDx0.001 CA% KLx0.01 FID KIDx0.001 CA% KLx0.01
ATISS 65.76 1.21 54.44±3.69 0.94 41.06 10.54 64.06±5.63 1.89 38.03 9.20 63.98±4.60 1.55
DiffuScene 66.45 1.05 58.70±4.93 3.41 45.88 9.30 65.34±6.55 1.55 47.91 11.75 73.96±4.93 1.95
MiDiffusion(Ours) 63.71 0.24 53.65±2.46 1.05 30.63 2.74 52.10±1.72 1.90 28.99 1.60 54.69±1.54 1.54
Table1: Evaluationresultsforfloor-conditioned3Dscenesynthesis.
Bedroom Diningroom Livingroom
Method
Obj OOB% IoU% Obj OOB% IoU% Obj OOB% IoU%
ATISS 5.52 16.26 1.00 11.43 17.43 1.86 13.00 17.39 1.49
DiffuScene 5.05 8.06 0.41 10.90 28.16 0.56 11.79 33.75 0.43
MiDiffusion(Ours) 5.22 3.91 0.61 10.92 5.77 0.91 12.28 8.71 0.69
Groundtruth 5.22 3.37 0.24 11.11 0.73 0.48 11.67 1.55 0.27
Table2: Geometricevaluationsforfloor-conditioned3Dscenesynthesis.
Diversity.Westudythediversityofthepredictedlayoutsbymeasuringtheaveragestandarddeviation
(std)ofobjectcentroidpositionsandboundingboxsizes.Sinceverticalpositionsarehighlycorrelated
withsemanticlabels,whicharealreadyevaluatedthroughKL-divergence,thestdoverpositionsis
averagedoverthetwoplanaraxes. TheresultsareinTable3,wherethe“IB”suffixmeansresultsare
computedover“in-boundary”objectsonlytoremovebadobjectplacement. However,thesenumbers
arenotdirectlycomparableduetolargedifferencesinOOB%acrossapproaches. Nevertheless,our
approachgeneratesthemostdiverseobjectplacementsthatarewithinthefloorplanboundary(i.e.,
Position-IB).Weareslightlylessdiverseinsizes(i.e.,Size-IB)thanDiffuScene,butourapproach
approachshowstheleastdropinsizevarietyafterremovingout-of-boundaryobjects.
Ablationstudy. InTable4,westudytheeffectofourmixed-diffusionformulationandPointNet
floorplanfeatureextractor.WeprovideresultsofmodifiedversionsofMiDiffusion: (1)withDDPM
7Bedroom Diningroom Livingroom
Method
Position Position-IB Size Size-IB Position Position-IB Size Size-IB Position Position-IB Size Size-IB
ATISS 1.074 1.035 0.709 0.663 1.586 1.504 0.434 0.385 1.730 1.663 0.442 0.409
DiffuScene 1.073 1.059 0.718 0.691 1.596 1.427 0.454 0.423 1.752 1.575 0.482 0.439
MiDiffusion(Ours) 1.073 1.067 0.698 0.680 1.568 1.558 0.414 0.399 1.670 1.667 0.441 0.427
Table3: Averagestdofpredictedobjectpositionsandsizesforfloor-conditioned3Dscenesynthesis.
andResNet-18[12]asfloorplanfeatureextractorasinATISSandDiffuScene;(2)withDDPMand
PointNet. ThelastrowshowsourmethodMiDiffusion(i.e.,usingourmixeddiffusionformulation
togetherwithPointNet). Thefirstvariant,DDPM+ResNetissimilartoDiffuScene,exceptweuse
atransformerbaseddenoisingnetworkratherthanitsU-Netcounterpart. Notethattherespective
resultsalreadyexceedDiffuSceneinTable1,suggestingthatthetransformerarchitectureisbetter
atlearningboundaryconstraints. However,thereisalargedeteriorationinKLdivergence. Next,
whilereplacingtheResNetfloorfeatureextractorbyamorelightweightPointNetextractorbrings
asignificantperformanceboostinthediningandlivingroomdatasets,whichcontainlesstraining
scenesbutareoflargerroomsizes,theperformancedecreasesonthebedroomdataset. Itislikely
thatPointNetworksbetterundercomplexandlimitedtrainingdata,butsuffersfromsamplingnoise
duringconversionofthefloorplanimagetopointsandnormalsintheoppositesituation. Overall,we
achievethebestresultswithourcompleteformulationinwhichthemixeddiffusionapproachavoids
liftingthediscreteinputstoacontinuousspaceasrequiredbytheDDPMformulation. Weinclude
additionalgeometricevaluationresultsinAppendixE.1
Bedroom Diningroom Livingroom
Method
FID KIDx0.001 CA% KLx0.01 FID KIDx0.001 CA% KLx0.01 FID KIDx0.001 CA% KLx0.01
DDPM+ResNet 63.73 0.35 54.86±4.22 4.72 31.35 3.77 53.58±4.06 5.88 30.44 3.05 63.48±6.21 6.06
DDPM+PointNet 65.25 1.25 57.00±4.67 2.14 31.43 2.85 52.71±4.55 2.72 28.63 1.68 55.27±5.44 1.93
Mixed+PointNet 63.71 0.24 53.65±2.46 1.05 30.63 2.74 52.10±1.72 1.90 28.99 1.60 54.69±1.54 1.54
Table4: Ablationstudyforfloor-conditioned3Dscenesynthesis.
5.2 FloorPlanConditioned3DSceneSynthesiswithObjectConstraints
RecallfromSec.4.3thatwecanre-usethesameMiDiffusionmodelsforproblemsinvolvingpartial
object constraints through a corruption-and-masking strategy. This approach forces the relevant
objectattributestofollowthereverseofapre-computedcorruptionprocessinthedenoisingstep,
withouttheneedoftask-specificretraining.
5.2.1 Scenecompletion
Figure 4: Living room completion given sofa
Figure3: Bedroomcompletiongivenabed.
andcornertables.
Wefirstcompareourapproachwithbaselinesforscenecompletion(i.e.,scenesynthesisconditioned
onexistingobjects).WetraintheDiffuScene-SCvariant,whichincludesanadditionalmoduletolearn
existingobjectfeaturesasaconditionalinput,usingtheirsuggestedhyper-parametersandproviding
3existingobjectsduringtraining. Thenumberofexistingobjectscanbearbitraryattesttime. Weuse
thesamemodelsforATISSandMiDiffusion. Weusetheproposedcorruption-and-maskingstrategy
overMiDiffusionmodelstrainedinSection5.1. Fig.3showsasimpleexampleofcompletinga
bedroomdesignthatoriginallyonlyconsistsofasinglebed. Fig.4showsanothermorecomplex
exampleforcompletionofalivingspace. NotethatMiDiffusionisabletogenerateanaturaland
symmetricsetoffurniturelayout.
WeperformquantitativeevaluationsinTable5. ThisisaneasierproblemforATISSandDiffuScene
thantheprevioussetup. Inparticular,ATISSisanautoregressivemodelandthereforeexistingobjects
allowthemodelstoskipthefirstfewiterationsreducingthechanceformistakes.DiffuScene-SCis,on
theotherhand,speciallydesignedandtrainedforthistask. BasedresultsinTable1,theseapproaches
indeedbenefitfromusingexistingobjectsasanchors. Withoutanyretraining, ourapproachstill
8outperformstheseapproaches,whengivenonly1existingobject. Whengiven3objects,ourmodels
are still better for the dining room and living room datasets and, however, are outperformed by
DiffuScene-SConthebedroomdataset. Notethatthebedroomdatahasonlyanaverageofonly5.22
objects. Therefore,speciallytrainednetworks,suchasDiffuScene-SC,mightbebettersuitedgiven
theveryhighpercentageofpre-existingobjects.
Bedroom Diningroom Livingroom
Method
FID KIDx0.001 KLx0.01 OOB% FID KIDx0.001 KLx0.01 OOB% FID KIDx0.001 KLx0.01 OOB%
ATISS 60.59 0.19 1.04 13.52 36.63 7.77 1.79 15.30 34.34 7.15 1.46 16.19
DiffuScene-SC 83.90 19.17 18.34 3.11 34.12 4.82 1.59 12.13 36.32 6.64 1.30 21.74
MiDiffusion(ours) 60.08 -0.03 0.91 3.32 28.56 1.88 1.58 5.29 28.21 1.78 1.47 8.04
ATISS 51.83 -0.57 0.66 8.89 33.53 5.39 1.60 12.26 31.78 5.39 1.43 13.03
DiffuScene-SC 42.77 -1.08 0.80 5.13 30.58 3.67 0.84 10.04 32.07 4.58 0.57 16.13
MiDiffusion(ours) 52.19 -0.32 1.00 4.05 27.60 1.85 1.72 3.97 27.46 1.91 0.95 7.22
Table5: Evaluationresultsforscenecompletion.
5.2.2 FurnitureArrangement
Figure6: Differentlivingroomfurnitureplace-
Figure5: Differenttable-chairarrangements.
mentdirections.
MiDiffusioncanalsobeusedforfurniturearrangement,whichreferstoascenesynthesisproblem
conditionedonobjectlabelsandboundingboxsizeswithoutaninitialconfiguration. Weuseour
proposed corruption-and-masking strategy over these attributes to test our models in this setting,
andcomparetheresultsagainstDiffuScene’sfurniturearrangementvariant(DiffuScene-FA).ATISS
modelsarenotwellsuitedforthistask,sincereplacingthepredictedobjectlabelbyconstraintsbreaks
thestoppingconditionintheirautoregressivepipeline. Fig.5showsasimpleexampleofadining
roomwithtwodifferentchairarrangements,asgeneratedbyMiDiffusion. Fig.6insteadillustrates
twooppositeplacementsofthelivingroomsectiongiventhesameinputs. Weshowquantitative
resultsinTable6. Withoutre-training,ourmodelsoutperformDiffuScene-FAinallmetrics.
Bedroom Diningroom Livingroom
Method
FID KIDx0.001 OBB% IoU% FID KIDx0.001 OBB% IoU% FID KIDx0.001 OBB% IoU%
DiffuScene-FA 61.03 0.14 8.03 0.75 33.90 4.14 12.30 1.16 34.62 6.04 17.84 1.31
MiDiffusion(ours) 59.08 -0.24 7.34 0.55 31.72 3.91 6.35 0.96 29.79 3.13 8.33 0.84
Table6: Evaluationresultsforfurniturearrangementexperiment.
5.2.3 AdditionalApplications
Ourmodelscaneasilyextendtootherapplicationswiththerightchoiceofmasking. Forexample,
theycanbeappliedtoobjecttypeconditionedscenesynthesisgivenalistoflabels. Wecaneven
combinepartialconstraintsonobjectattributes(e.g.,label)withexistingobjectsforconstrainedscene
completionproblems. WeshowqualitativeexamplesinAppendixF.1.
6 Limitations
Thoughourmethodshowscompetitiveperformanceindifferentlayoutgenerationtasks,ithasseveral
limitations: thecurrentdefinitionofobjectsasacollectionofboundingboxfeaturesandlabelsisnot
themostpreciserepresentationfora3Dsetting,andgainingagoodunderstandingof3Dfeatures
canpotentiallyimproveuponthisaspect. Inaddition,MiDiffusionstillrequiresamodelretrieving
strategytocomposethefinal3Dsceneconfiguration. Inthefuturewewanttoinvestigatetheuseofa
strong3Dshapepriors(e.g. intheformofapointcloudormeshencoder)andintegratingmeshand
texturesynthesiscapabilities,similartoSceneTex[4],toenhancerealism. Finally,wealsoplanto
investigatehowtoremovetheneedforseparatemodelsperroom.
9
jbo1
jbo37 Conclusion
InthisworkweintroducedMiDiffusion,anovelmixeddiffusionmodelcombiningDDPMandD3PM
forobjectbased3Dindoorscenesynthesis. Thisformulationcircumventstheneedofadoptingvector
representationofsemanticlabelsinthecontinuousdomainasdoneinpriorworks.Wedesigneda
denoisingarchitecturethathasatime-varianttransformerdecoderbackboneandoutputspredictions
separatelyfordiscretesemanticattributesandcontinuousgeometricattributes. Weproposedaunique
corruption-and-maskingstrategytoconductscenecompletionandfurniturearrangementwithout
the need to re-train our models. We provided extensive experimental evaluation using common
benchmarksagainststate-of-the-artautoregressiveanddiffusionmodels. Theresultsshowaclear
advantage of our approach over the state of the art with or without partial constraints. We also
presentedablationstudiessupportingourdesignofthemixeddiffusionframeworkaswellasthe
denoisingarchitecture.
References
[1] KatharopoulosAngelosandDespoinaPaschalidou. simple-3dviz. https://simple-3dviz.
com,2020.
[2] Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg.
Structured Denoising Diffusion Models in Discrete State-Spaces. In Advances in Neural
InformationProcessingSystems,November2021.
[3] JimmyLeiBa,JamieRyanKiros,andGeoffreyE.Hinton. LayerNormalization,July2016.
[4] Dave Zhenyu Chen, Haoxuan Li, Hsin-Ying Lee, Sergey Tulyakov, and Matthias Nießner.
Scenetex: High-qualitytexturesynthesisforindoorscenesviadiffusionpriors,2023.
[5] JeevanDevaranjan,AmlanKar,andSanjaFidler. Meta-Sim2: UnsupervisedLearningofScene
StructureforSyntheticDataGeneration. InAndreaVedaldi,HorstBischof,ThomasBrox,and
Jan-MichaelFrahm,editors,ComputerVision–ECCV2020,volume12362,pages715–733.
SpringerInternationalPublishing,2020.
[6] PatrickEsser,RobinRombach,andBjornOmmer. TamingTransformersforHigh-Resolution
ImageSynthesis. In2021IEEE/CVFConferenceonComputerVisionandPatternRecognition
(CVPR),pages12868–12878,Nashville,TN,USA,June2021.IEEE.
[7] WeixiFeng,WanrongZhu,Tsu-JuiFu,VarunJampani,ArjunReddyAkula,XuehaiHe,S.Basu,
XinEricWang, andWilliamYangWang. LayoutGPT:CompositionalVisualPlanningand
GenerationwithLargeLanguageModels. InConferenceonNeuralInformationProcessing
Systems,November2023.
[8] MatthewFisher,ManolisSavva,andPatHanrahan. Characterizingstructuralrelationshipsin
scenesusinggraphkernels. InACMSIGGRAPH2011papers,SIGGRAPH’11,pages1–12.
AssociationforComputingMachinery,July2011.
[9] HuanFu,BowenCai,LinGao,Ling-XiaoZhang,JiamingWang,CaoLi,QixunZeng,Chengyue
Sun,RongfeiJia,BinqiangZhao,etal.3d-front:3dfurnishedroomswithlayoutsandsemantics.
InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages10933–
10942,2021.
[10] HuanFu,RongfeiJia,LinGao,MingmingGong,BinqiangZhao,SteveMaybank,andDacheng
Tao. 3d-future: 3d furniture shape with texture. International Journal of Computer Vision,
pages1–25,2021.
[11] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan,
andBainingGuo. VectorQuantizedDiffusionModelforText-to-ImageSynthesis. In2022
IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),pages10686–
10696.IEEE,2022.
[12] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimage
recognition. In 2016 IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR),pages770–778,2016.
[13] LiuHe,YijuanLu,JohnCorring,DineiFlorencio,andChaZhang. Diffusion-basedDocument
LayoutGeneration,2023.
[14] DanHendrycksandKevinGimpel. GaussianErrorLinearUnits(GELUs),June2016.
10[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. In
AdvancesinNeuralInformationProcessingSystems, volume33, pages6840–6851.Curran
Associates,Inc.,2020.
[16] EmielHoogeboom,DidrikNielsen,PriyankJaini,PatrickForré,andMaxWelling. Argmax
FlowsandMultinomialDiffusion: LearningCategoricalDistributions. InAdvancesinNeural
InformationProcessingSystems,November2021.
[17] NaotoInoue,KotaroKikuchi,EdgarSimo-Serra,MayuOtani,andKotaYamaguchi.LayoutDM:
DiscreteDiffusionModelforControllableLayoutGeneration. In2023IEEE/CVFConference
onComputerVisionandPatternRecognition(CVPR),pages10167–10176.IEEE,June2023.
[18] AmlanKar,AayushPrakash,Ming-YuLiu,EricCameracci,JustinYuan,MattRusiniak,David
Acuna,AntonioTorralba,andSanjaFidler. Meta-Sim: LearningtoGenerateSyntheticDatasets.
In2019IEEE/CVFInternationalConferenceonComputerVision(ICCV),pages4550–4559,
Seoul,Korea(South),October2019.IEEE.
[19] TeroKarras, TimoAila, MiikaAittala, andSamuliLaine. ElucidatingtheDesignSpaceof
Diffusion-BasedGenerativeModels. In36thConferenceonNeuralInformationProcessing
Systems,2022.
[20] TeroKarras, MiikaAittala, JanneHellsten, SamuliLaine, JaakkoLehtinen, andTimoAila.
TrainingGenerativeAdversarialNetworkswithLimitedData. InAdvancesinNeuralInforma-
tionProcessingSystems,volume33,pages12104–12114.CurranAssociates,Inc.,2020.
[21] DiederikP.KingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InYoshua
BengioandYannLeCun,editors,3rdInternationalConferenceonLearningRepresentations,
ICLR2015,SanDiego,CA,USA,May7-9,2015,ConferenceTrackProceedings,2015.
[22] AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton. ImageNetclassificationwithdeep
convolutionalneuralnetworks. InConferenceonNeuralInformationProcessingSystems,pages
1097–1105,2012.
[23] Manyi Li, Akshay Gadi Patil, Kai Xu, Siddhartha Chaudhuri, Owais Khan, Ariel Shamir,
ChangheTu,BaoquanChen,DanielCohen-Or,andHaoZhang.GRAINS:GenerativeRecursive
AutoencodersforINdoorScenes. ACMTransactionsonGraphics,38:12:1–12:16,February
2019.
[24] ChenguoLinandYadongMu. InstructScene: Instruction-Driven3DIndoorSceneSynthesis
withSemanticGraphPrior. InInternationalConferenceonLearningRepresentations(ICLR),
February2024.
[25] PaulMerrell,EricSchkufza,ZeyangLi,ManeeshAgrawala,andVladlenKoltun. Interactive
furniturelayoutusinginteriordesignguidelines.InACMSIGGRAPH2011papers,SIGGRAPH
’11,pages1–10.AssociationforComputingMachinery,July2011.
[26] AlexanderQuinnNicholandPrafullaDhariwal. ImprovedDenoisingDiffusionProbabilistic
Models. In38thInternationalConferenceonMachineLearning,pages8162–8171.PMLR,
July2021.
[27] WamiqPara,PaulGuerrero,TomKelly,LeonidasGuibas,andPeterWonka. GenerativeLayout
ModelingusingConstraintGraphs. In2021IEEE/CVFInternationalConferenceonComputer
Vision(ICCV),pages6670–6680.IEEE,October2021.
[28] DespoinaPaschalidou,AmlanKar,MariaShugrina,KarstenKreis,AndreasGeiger,andSanja
Fidler. ATISS:AutoregressiveTransformersforIndoorSceneSynthesis. InAdvancesinNeural
Information Processing Systems, volume 34, pages 12013–12026. Curran Associates, Inc.,
2021.
[29] AayushPrakash,ShaadBoochoon,MarkBrophy,DavidAcuna,EricCameracci,GavrielState,
OmerShapira,andStanBirchfield. StructuredDomainRandomization: BridgingtheReality
Gap by Context-Aware Synthetic Data. In 2019 International Conference on Robotics and
Automation(ICRA),pages7249–7255,Montreal,QC,Canada,May2019.IEEEPress.
[30] Pulak Purkait, Christopher Zach, and Ian Reid. SG-VAE: Scene Grammar Variational Au-
toencodertoGenerateNewIndoorScenes. InECCV,pages155–171.SpringerInternational
Publishing,2020.
[31] CharlesRQi,HaoSu,KaichunMo,andLeonidasJGuibas. Pointnet: Deeplearningonpoint
setsfor3Dclassificationandsegmentation. In2017IEEE/CVFConferenceonComputerVision
andPatternRecognition(CVPR),pages652–660,2017.
11[32] SiyuanQi,YixinZhu,SiyuanHuang,ChenfanfuJiang,andSong-ChunZhu. Human-centricIn-
doorSceneSynthesisUsingStochasticGrammar. In2018IEEE/CVFConferenceonComputer
VisionandPatternRecognition(CVPR).arXiv,August2018.
[33] DanielRitchie,KaiWang,andYu-AnLin. FastandFlexibleIndoorSceneSynthesisviaDeep
ConvolutionalGenerativeModels. In2019IEEE/CVFConferenceonComputerVisionand
PatternRecognition(CVPR),pages6175–6183.IEEE,June2019.
[34] OlafRonneberger, PhilippFischer, andThomasBrox. U-Net: ConvolutionalNetworksfor
BiomedicalImageSegmentation. InMedicalImageComputingandComputer-AssistedIn-
tervention – MICCAI 2015, Lecture Notes in Computer Science, pages 234–241. Springer
InternationalPublishing,2015.
[35] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep Un-
supervised Learning using Nonequilibrium Thermodynamics. In Proceedings of the 32nd
InternationalConferenceonMachineLearning,pages2256–2265.PMLR,June2015.
[36] JiamingSong,ChenlinMeng,andStefanoErmon. DenoisingDiffusionImplicitModels. In
InternationalConferenceonLearningRepresentations,October2020.
[37] Yang Song and Stefano Ermon. Generative Modeling by Estimating Gradients of the Data
Distribution. In Advances in Neural Information Processing Systems, volume 32. Curran
Associates,Inc.,2019.
[38] YangSongandStefanoErmon. ImprovedTechniquesforTrainingScore-BasedGenerative
Models. InAdvancesinNeuralInformationProcessingSystems, volume33, pages12438–
12448.CurranAssociates,Inc.,2020.
[39] YangSong,JaschaSohl-Dickstein,DiederikP.Kingma,AbhishekKumar,StefanoErmon,and
BenPoole. Score-BasedGenerativeModelingthroughStochasticDifferentialEquations. In
InternationalConferenceonLearningRepresentations,2021.
[40] Jerry O. Talton, Yu Lou, Steve Lesser, Jared Duke, Radomír Meˇch, and Vladlen Koltun.
Metropolisproceduralmodeling. ACMTransactionsonGraphics, 30(2):11:1–11:14, April
2011.
[41] Jiapeng Tang, Yinyu Nie, Lev Markhasin, Angela Dai, Justus Thies, and Matthias Nießner.
DiffuScene: SceneGraphDenoisingDiffusionProbabilisticModelforGenerativeIndoorScene
Synthesis.In2024IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),
2024.
[42] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In Advances in Neural
InformationProcessingSystems,volume30.CurranAssociates,Inc.,2017.
[43] KaiWang,Yu-AnLin,BenWeissmann,ManolisSavva,AngelX.Chang,andDanielRitchie.
PlanIT:planningandinstantiatingindoorsceneswithrelationgraphandspatialpriornetworks.
ACMTransactionsonGraphics,38(4):1–15,August2019.
[44] Xinpeng Wang, Chandan Yeshwanth, and Matthias Niesner. SceneFormer: Indoor Scene
GenerationwithTransformers. In2021InternationalConferenceon3DVision(3DV),pages
106–115,London,UnitedKingdom,December2021.IEEE.
[45] MichaelL.Waskom. seaborn: statisticaldatavisualization. JournalofOpenSourceSoftware,
6(60):3021,2021.
[46] QiuhongAnnaWei,SijieDing,JeongJoonPark,RahulSajnani,AdrienPoulenard,Srinath
Sridhar,andLeonidasGuibas. LEGO-Net: LearningRegularRearrangementsofObjectsin
Rooms. In2023IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),
pages19037–19047,Vancouver,BC,Canada,June2023.IEEE.
[47] HaitaoYang,ZaiweiZhang,SimingYan,HaibinHuang,ChongyangMa,YiZheng,Chandrajit
Bajaj,andQixingHuang. SceneSynthesisviaUncertainty-DrivenAttributeSynchronization.
In2021IEEE/CVFInternationalConferenceonComputerVision(ICCV),pages5610–5620.
IEEE,2021.
[48] Ming-Jia Yang, Yu-Xiao Guo, Bin Zhou, and Xin Tong. Indoor Scene Generation from a
CollectionofSemantic-SegmentedDepthImages. In2021IEEE/CVFInternationalConference
onComputerVision(ICCV),pages15183–15192,Montreal,QC,Canada,October2021.IEEE.
[49] Yi-TingYeh,LingfengYang,MatthewWatson,NoahD.Goodman,andPatHanrahan. Syn-
thesizingopenworldswithconstraintsusinglocallyannealedreversiblejumpMCMC. ACM
TransactionsonGraphics,31(4):56:1–56:11,July2012.
12[50] Lap-FaiYu,Sai-KitYeung,Chi-KeungTang,DemetriTerzopoulos,TonyF.Chan,andStanleyJ.
Osher. Makeithome: automaticoptimizationoffurniturearrangement. ACMTransactionson
Graphics,30(4):86:1–86:12,July2011.
[51] Lap-FaiYu,Sai-KitYeung,andDemetriTerzopoulos. TheClutterpalette: AnInteractiveTool
for Detailing Indoor Scenes. IEEE Transactions on Visualization and Computer Graphics,
22(2):1138–1148,February2016.
[52] GuangyaoZhai,EvinPınarÖrnek,Shun-ChengWu,YanDi,FedericoTombari,NassirNavab,
andBenjaminBusam. CommonScenes: GeneratingCommonsense3DIndoorSceneswith
SceneGraphs,October2023.
[53] ZaiweiZhang,ZhenpeiYang,ChongyangMa,LinjieLuo,AlexanderHuth,EtienneVouga,and
QixingHuang. DeepGenerativeModelingforSceneSynthesisviaHybridRepresentations.
ACMTransactionsonGraphics,39(2):1–21,April2020.
[54] YangZhou, ZacharyWhile, andEvangelosKalogerakis. SceneGraphNet: NeuralMessage
Passingfor3DIndoorSceneAugmentation. In2019IEEE/CVFInternationalConferenceon
ComputerVision(ICCV),pages7383–7391.IEEE,October2019.
[55] YiZhou,ConnellyBarnes,JingwanLu,JimeiYang,andHaoLi. OntheContinuityofRotation
RepresentationsinNeuralNetworks. In2019IEEE/CVFConferenceonComputerVisionand
PatternRecognition(CVPR),pages5738–5746.IEEE,June2019.
13A DiffusionModels
A.1 DDPMforwardprocessre-parameterization,simplifiedloss
DDPM[15]suggestsre-parameterizingEq.(4)as
√ √
x (x ,ϵ)= α¯ x + 1−α¯ ϵ (16)
t 0 t 0 t
withϵ ∼ N(0,I). Then,letϵ beafunctionapproximatortopredictϵfromx suchthatforthe
θ t
(cid:16) (cid:17)
reverseprocessµ θ(x t,t)= √1
αt
x t− √ 1β −t α¯tϵ θ(x t,t) andL t−1for2≤t≤T in (2)becomes
L
t−1
=E x0,ϵ(cid:20) 2σ2αβ (1t2
−α¯
)(cid:13) (cid:13)ϵ−ϵ θ(√ α¯ tx 0+√ 1−α¯ tϵ,t)(cid:13) (cid:13)2(cid:21) +const (17)
t t t
wheretheconstanttermcanbedroppedintraining.
Onimagedata,[15]founditbeneficialtosamplequalitytotrainonthissimplifiedversionofthe
variationalbound(L ):
0:T−1
L simple(θ):=E t,x0,ϵ(cid:104)(cid:13) (cid:13)ϵ−ϵ θ(√ α¯ tx 0+√ 1−α¯ tϵ,t)(cid:13) (cid:13)2(cid:105) (18)
withuniformtbetween1andT,droppingthescalingfactorsofthesquarednorms. WeuseEq.(18)
tocomputeLDDPM inMiDiffusion.
vb
A.2 D3PMreverseparameterization,auxiliaryloss
SimilarDDPM,[2,16]suggestedthatapproximatingsomesurrogatevariablesgivesbetterquality.
Specifically,theytrainedaneuralnetworkp˜ (z˜ |z ),multiplieditwiththeposteriorq(z |z ,z ),
θ 0 t t−1 t 0
andmarginalizedz˜ toobtainp (z |z ):
0 θ t−1 t
(cid:88)
p (z |z )∝ q(z |z ,z )p˜ (z˜ |z ). (19)
θ t−1 t t−1 t 0 θ 0 t
z˜0
D3PM[2]introducedanauxiliarydenoisingobjectivetoencouragegoodpredictionsofthedataz
0
ateachtimestep. Theircompletetraininglossis:
L (θ)=L (θ)+λE [−logp˜ (z |z )] (20)
λ vb q(z0)q(zt|z0) θ 0 t
whichcorrespondtoLD3PM andλLD3PM inourMiDiffusionalgorithm. Thiscanbeimplemented
vb aux
efficientlyintraining,sincetheparameterizationofthebackwardstepdirectlycomputesp˜ (z |z ).
θ 0 t
B MiDiffusionLossFactorization
We provide more details for the factorization step in Eq. (13). We first consider a general case
comparingKL-divergencebetweenq˜(z)qˆ(x)andp˜(z)pˆ(x)fordiscretezandcontinuousx:
(cid:90) (cid:88) q˜(z)qˆ(x)
D (q˜(z)qˆ(x)||p˜(z)pˆ(x))= q˜(z)qˆ(x)log
KL p˜(z)pˆ(x)
x z
(cid:90) (cid:88)(cid:20) q˜(z) qˆ(x)(cid:21)
= q˜(z)qˆ(x)log +q˜(z)qˆ(x)log
p˜(z) pˆ(x)
x z (21)
(cid:88) q˜(z)(cid:90) (cid:90) qˆ(x)(cid:88)
= q˜(z)log qˆ(x)+ qˆ(x)log q˜(z)
p˜(z) pˆ(x)
z x x z
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)
1
1
=D (q˜(z)||p˜(z))+D (qˆ(x)||p˜(x)).
KL KL
ThismeanswecandecoupleKL-divergencecomputationbetweenmixed-domainprobabilitydistri-
butionstoasumofdomain-specificKL-divergencecomputation. Notethatthisholdsforanychoices
14ofq˜,qˆ,p˜,pˆ. Forourspecificcase:
Lmixed
t−1
=E [D (q(z ,x |z ,x ,z ,x )||p (z ,x |z ,x ,y))]
q(zt,xt|z0,x0) KL t−1 t−1 t t 0 0 θ t−1 t−1 t t
=E [D (q˜(z |z ,z )qˆ(x |x ,x )||p˜ (z |z ,x ,y))pˆ (x |x ,z ,y))]
q˜(zt|z0)qˆ(xt|x0) KL t−1 t 0 t−1 t 0 θ t−1 t t θ t−1 t t
=E [D (q˜(z |z ,z )||p˜ (z |z ,x ,y)))]
q˜(zt|z0) KL t−1 t 0 θ t−1 t t
+E [D (qˆ(x |x ,x )||pˆ (x |x ,z ,y))],
qˆ(xt|x0) KL t−1 t 0 θ t−1 t t
(22)
wherethelastequalityholdsbecauseofEq.(21).
C NetworkandImplementationDetails
Weincludedetailednetworkhyper-parametersforourdenoisingnetwork,andimplementationfor
training. Weplantoreleaseourcodeandtrainingconfigurationsuponacceptanceofthispaper.
C.1 NetworkHyper-parameters
ThearchitectureofMiDiffusionhasatitscoreaseriesof8transformerblocksusingahiddendimen-
sionof512with8headsandfeed-forwardlayerswithaninternaldimensionof2048. Following[11],
weuseGELU[14]asnonlinearity. Thesemanticattributesareembeddedinalearnedvectoroflength
512,andthegeometricattributesaremappedbya3-layerMLP,withinternaldimensionsof[512,
1024],toa512-dimensionalspacebeforebeingfedtothetransformerdecoderblocks. Weextract
floorplanfeaturesofdimension64usinga4-layerPointNet[31]withinternaldimensions[64,64,
512]asinLEGO-Net[46]. Theconditionalinputisconcatenatedwiththelearned64-dimensional
index embedding . For baselines and ablation studies, we use the default ResNet-18 [12] image
featureextractorproposedbyATISS[28]andalsoimplementedbyDiffuScene[41]tocomputethe
64-dimensionalfloorplanfeaturesfrombinaryfloorplanmasks. Theoutputsofthetransformer
decoderarefedtotwoseparateMLPstodecodethesemanticandgeometricpredictions.Thesemantic
featuredecoderisa1-layerMLPthatproducesacategoricaldistributionoverz,andthegeometric
featuredecoderisa3-layerMLPwithhiddendimensions[512,1024],producingthe8-dimensional
Gaussianmeanforthegeometricattributesx.
C.2 Implementation
Wesetafixedlearningrateofl =2e−4 usingtheAdamoptimizer,andadropoutratioof0.1for
r
multi-headattentionandfeed-forwardlayersinthetransformerblocks. Wetrain50kepochsonthe
bedroomdatasetwith0.5learningratedecayevery10kepochs,and100kepochsonthelivingand
diningroomdatasetswith0.5decayevery15kepochs. Weusealinearscheduleover1000diffusion
stepsforallnoiseparametersintheforwardprocess. Inthediscretedomainα andγ rangefrom
t t
1−1e−5to0.99999andfrom9e−6to0.99999respectively. Inthecontinuousdomain,β startsfrom
t
1e−4 atreaches0.02. WetrainallourmodelsonasingleNVIDIAV100GPUwithunder8GBof
GPURAMusage. Thetrainingtimerangefromaround20hoursonlivinganddiningroomdatasets
toabout36hoursonthebedroomdatasetforabatchsizeof128scenes.
D ExampleSynthesizedLayoutImages
Weusetexture-lessobjectrenderingonwhitefloorplanforquantitativeevaluations,sincefloorand
object textures affect the clarity of the results. The object labels are sorted in alphabetical order
foreachroomtype,andthentheassociatedcolorsareevenlysampledalongacircularpathusing
Seaborn’s [45] “hls” color palette. We render top-down layout images using simple-3dviz [1] in
accordancewithpriorworks[28,41]. Weincludeexamplesofrenderedimagesforthreecomparing
approachesinFig.7,8,9fromthefloor-conditionedscenesynthesisexperimentinSec2. Some
ground-truthlayoutsincludeobjectsslightlyoutofboundary.Thisisaproblemintheraw3D-FRONT
dataset. Therefore,weinflatetheroomboundaryby0.1mwhencountingnumberofoutofboundary
objects. Thebedroomdatasetsistheeasiest(lessnumberofobjects, smallerinroomsize, more
trainingdata)sothatallcomparingapproachescangenerategoodpredictions. Ontheharderliving
roomanddiningroomdatasets,MiDiffusionclearlyoutperformsthebaselinesbygeneratingrealistic
geometricarrangementswithdesiredsymmetryandalignmentbetweenobjects,whilerespectingthe
15ATISS
[28]
DiffuScene
[41]
MiDiffusion
(Ours)
Ground-truth
Figure7: Examplebedroomtop-downorthographicprojectionimagesforquantitativeevaluations.
ATISS
[28]
DiffuScene
[41]
MiDiffusion
(Ours)
Ground-truth
Figure8: Examplediningroomtop-downorthographicprojectionimagesforquantitativeevaluations.
16ATISS
[28]
DiffuScene
[41]
MiDiffusion
(Ours)
Ground-truth
Figure9: Examplelivingroomtop-downorthographicprojectionimagesforquantitativeevaluations.
floorboundaryconstraints. DiffuSceneisalsocapableofgeneratinggoodgeometricarrangementbut
theirarchitectureislessoptimalforlearningfloorboundaryconstraints.
E AdditionalExperimentalEvaluations
E.1 AblationStudy
Weprovideadditional geometric evaluation results forour ablation study in Sec.5.1. Across all
threedataset,thereisaconsistentimprovementoverOOB%aswemodifythefloorplanfeature
extractoranddiffusionframeworktowardsourfinaldesignofMiDiffusion. Thisisconsistentwith
theevaluationmetricsreportedinSec.5.1.
Bedroom Diningroom Livingroom
Method
Obj OOB% IoU% Obj OOB% IoU% Obj OOB% IoU%
DDPM+ResNet 5.07 6.63 0.50 10.87 7.08 0.70 11.90 10.93 0.70
DDPM+PointNet 4.92 5.29 0.70 10.78 6.75 0.78 11.83 10.37 0.64
Mixed+PointNet 5.22 3.91 0.61 10.92 5.77 0.91 12.28 8.71 0.69
Groundtruth 5.22 3.37 0.24 11.11 0.73 0.48 11.67 1.55 0.27
Table7: Geometricevaluationsonablationstudyforfloor-conditioned3Dscenesynthesis.
E.2 Unconditional3DSceneSynthesis
DiffuScenewasoriginallyproposedfor3Dscenesynthesiswithoutfloorconditioning. Although
thisisnotthemainfocusofthispaper,weprovideexperimentalresultscomparingMiDiffusionand
DiffuSceneforcompletenessusingthereleasedmodelweights. DiffuSceneusedsquaresofthesame
sizetorenderthelayouts,whichcutsoffsomeobjectsinthediningandlivingrooms. Wetherefore
re-computeDiffuSceneresultsusingtheirreleasedmodelsandrenderlayoutstodatasetspecificsizes.
17TheresultsaresummarizedinTable8. OurapproachworksbetterthanDiffuSceneonthedining
roomdataset,comparablyonthelivingroomdataset,andslightlyworseonthebedroomdataset.
Table9showsresultsusingDiffuScene’sdefaultrenderingandevaluationapproachoverthesameset
ofresultsasTable8.Specifically,werenderallground-truthandsynthesizedlayoutstoa3.1msquare.
Wecomparethesyntheticresultswiththeground-truthtrainingsetinaccordancewithDiffuScene’s
evaluation script. We include the original results in [41] in the first row as a reference. Using
pre-trained models and open-sourced evaluation scripts, we are able generate DiffuScene results
closetopublicationvalues. TheonlyexceptionisCA%,forwhichDiffuScenedoesnotreleasetheir
classificationnetwork. Therefore,weusetheAlexNet[22]basedclassifierbyATISS[28]. These
resultsareconsistentwithTable8thatMiDiffusionbehavessub-optimallyonthebedroomdataset.
OurmodelsarequitesimilarinperformancecomparedtoDiffuSceneondiningroomandliving
roomdatasets,eventhoughtheyarenotspecificallydesignedforthistask.
Bedroom Diningroom Livingroom
Method
FID KIDx0.001 CA% KLx0.01 FID KIDx0.001 CA% KLx0.01 FID KIDx0.001 CA% KLx0.01
DiffuScene 61.12 0.46 53.70±2.94 1.24 45.04 0.70 51.53±1.05 1.10 43.30 0.21 53.50±2.84 0.67
MiDiffusion(ours) 65.72 2.83 54.93±3.85 1.12 43.61 0.42 51.21±1.33 0.71 43.66 0.84 53.16±2.32 0.61
Table8: Evaluationresultsforunconditional3Dscenesynthesis.
Bedroom Diningroom Livingroom
Method
FID KIDx0.001 CA% KLx0.01 FID KIDx0.001 CA% KLx0.01 FID KIDx0.001 CA% KLx0.01
DiffuScene-[41] 17.21 0.70 52.15 0.35 32.60 0.72 55.50 0.22 36.18 0.88 57.81 0.21
DiffuScene 17.43 0.82 51.09±0.47 0.66 33.07 0.93 53.82±4.01 0.34 35.27 0.58 54.24±3.65 0.36
MiDiffusion(Ours) 21.32 2.62 54.93±3.85 0.32 33.05 0.87 52.32±2.34 0.16 36.21 1.43 54.86±2.59 0.16
Table9: Evaluationresultsforunconditional3Dscenesynthesisagainsttrainingscenesandusing
layoutsrenderedon3.1msquares.
F AdditionalLayoutExamples
F.1 QualitativeExamplesforAdditionalApplications
WeshowqualitativeexamplesofadditionalapplicationsofourmodelsasexplainedinSec.5.2.3.
Fig. 10 and 11 are two examples of label constrained scene synthesis. In particular, Fig. 11 is
conditionedonfivediningchairs,whichoccurlessoftenthenevennumberofchairs. Ourmodelis
stillabletogeneratereasonablelayout.
WeshowlabelconstrainedscenecompletionexamplesinFig.12andFig.13. Withourcorruption-
and-maskingstrategy,ourmodelsareabletocompletescenelayoutunderobjectcategoryconstraints.
Figure10: Labelconstrainedbedroom Figure 11: Label constrained dining
scenesynthesis. roomscenesynthesis.
Figure 12: Label constrained bedroom scene Figure13: Labelconstrainedlivingroomscene
completion. completion.
18F.2 AdditionalLayoutsforFloor-ConditionedSceneSynthesis
WeprovideadditionallayoutimagegeneratedbyMiDiffusioninourfloor-conditionedscenesynthesis
experiment. Theselayoutsaregeneratedinasequenceofrandomsamplingoftestfloorplansafter
removingfloorplanduplicates.
19