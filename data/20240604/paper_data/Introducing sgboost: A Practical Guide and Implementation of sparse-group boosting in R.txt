APPLICATION NOTE
Introducing sgboost: A Practical Guide and Implementation of
sparse-group boosting in R
Fabian Obster a b and Christian Heumannb
aDepartment of Business Administration, University of the Bundeswehr Munich,
Werner-Heisenberg-Weg 39, Bavaria, Germany bDepartment of Statistics, LMU Munich,
Ludwigstr. 33, Bavaria, Germany
ARTICLE HISTORY
Compiled June 3, 2024
ABSTRACT
This paper introduces the sgboost package in R, which implements sparse-group
boosting for modeling high-dimensional data with natural groupings in covariates.
Sparse-groupboostingoffersaflexibleapproachforbothgroupandindividualvari-
ableselection,reducingoverfittingandenhancingmodelinterpretability.Thepack-
ageusesregularizationtechniquesbasedonthedegreesoffreedomofindividualand
groupbase-learners,andisdesignedtobeusedinconjunctionwiththemboostpack-
age. Through comparisons with existing methods and demonstration of its unique
functionalities,thispaperprovidesapracticalguideonutilizingsparse-groupboost-
ing in R, accompanied by code examples to facilitate its application in various re-
searchdomains.Overall,thispaperservesasavaluableresourceforresearchersand
practitioners seeking to use sparse-group boosting for efficient and interpretable
high-dimensional data analysis.
KEYWORDS
group selection; variable selection; R package; Ridge regression; within-group
sparsity
1. Introduction
Regularized regression is used to model high-dimensional data to reduce the risk of
overfitting and to perform variable selection. In many cases, covariates have natural
groupings, such as with gene data or categorical data often found in survey data. In
such cases, one may want to select whole groups of variables or just individual parts
(univariate regression model). ‘sgboost‘ implements the sparse-group boosting in R and
other useful functions for sparse group model interpretation unique to boosting and
visualization for group and individual variable selection. The package is available on
CRAN [8]. A formula object defining group base-learners and individual base-learners
is utilized for model fitting. Regularization is based on the degrees of freedom (df(·)) of
individualandgroupbase-learnersasdefinedin[10]. Foranindividualbase-learner,the
Regularization parameter λ, and for a group base-learners the regularization parameter
λ(g) is defined, such that df(λ) = α and df(λ(g)) = 1 − α. The package should be
used with the ’mboost’ package [4]. Sparse-group boosting is an alternative method to
CONTACTFabianObsterEmail:fabian.obster@unibw.de
4202
yaM
13
]PA.tats[
1v73012.5042:viXrasparse-group lasso, employing boosted Ridge regression [14]. Although there are many
methodsofvariableselection,mostfocusongroupselectiononly,e.g.[7],[5]and[16],or
individual variable selection e.g. [1], [15] and [2]. However, it should be noted, that with
group variable selection with overlapping groups, one could also end up with sparse-
group variable selection. There are not many R packages implementing sparse-group
variable selection. There is ’SGL’ [13] implementing [14] and ’sparsegl’ [6] with a faster
implementation of the sparse-group lasso.
The goal of this paper is to provide a practical guide including the code on how to use
sparse-group boosting in R and get the most out of the method. The code is presented
withinthismanuscriptandcanbefoundalsoonGitHubtogetherwiththeuseddataset
(https://github.com/FabianObster/sgboost-introduction).
2. Sparse-group boosting workflow with sample data
We first simulate the sample data and corresponding group structure with 40 equal-
sized groups. Based on a linear regression model we simulate the response variable y as
part of the data.frame with n = 100 observations and p = 200 predictor variables (each
group is formed by 5 predictors).
beta <− c(
rep(5, 5), c(5, −5, 2, 0, 0), rep(−5, 5),
c(2, −3, 8, 0, 0), rep(0, (200 − 20))
)
X <− matrix(data = rnorm(20000, mean = 0, sd = 1), 100, 200)
df <− data.frame(X) %>%
mutate(y = X %∗% beta+rnorm(100, mean = 0, sd = 1)) %>%
mutate_all(function(x){as.numeric(scale(x))})
group_df <− data.frame(
group_name = rep(1:40 , each = 5),
variable_name = colnames(df )[1:200]
)
2.1. Defining the model
Now we use the group structure to describe the sparse group boosting formula with
the function create_formula(). We only need the data.frame() describing the group
structure. It should contain two variables one indicating the name of the variable in the
modeling data (var_name), and one indicating the group it belongs to (group_name).
Additionally,weneedtopassthemixingparameteralphaandthenameoftheoutcome
variable.
sgb_formula <− create_formula(
alpha = 0.4 , group_df = group_df , outcome_name = "y",
group_name = "group_name", var_name = "variable_name")
This function returns an R-formula consisting of p model terms defining the individual
base-learners and G group base-learners.
labels(terms(sgb_formula ))[[1]]
## bols(X1, df = 0.4 , intercept = FALSE)
2labels(terms(sgb_formula ))[[201]]
## bols(X1, X2, X3, X4, X5, df = 0.6 , intercept = FALSE)
2.2. Fitting and tuning the model
sgboost is to be used in conjunction with the mboost package which provides many
useful functions and methods which can also be used for sparse-group boosting models.
Now we pass the formula to mboost() and use the arguments as seems appropriate.
The main hyperparameters are nu and mstop. For model tuning, the function cvrisk
can be used and plotted. Running the cross-validation/bootstrap in parallel can speed
up the process.
sgb_model <− mboost(
formula = sgb_formula , data = df ,
control = boost_control(nu = 1, mstop = 600)
)
cv_sgb_model <− cvrisk(sgb_model)
mstop(cv_sgb_model)
## 204
plot(cv_sgb_model
Figure 1. Outofsampleerrordependingontheboostingiteration
In this example, the lowest out-of-sample risk is obtained at 204 boosting iterations,
so we only use the first 204 updates for the final model.
2.3. Interpreting and plotting a sparse-group boosting model
sgboost has useful functions to understand sparse-group boosting models and reflects
thatfinalmodelestimatesofaspecificvariableinthedatasetcanbeattributedtogroup
base-learners as well as individual base-learners depending on the boosting iteration.
32.3.1. Variable importance
A good starting point for understanding a sparse-group boosting model is the vari-
able importance. In the context of boosting, the variable importance can be defined as
the relative contribution of each predictor to the overall reduction of the loss function
(negative log-likelihood). get_varimp() returns the variable importance of each base-
learner/predictor selected throughout the boosting process. In the case of the selection
of an individual variable - call it x - as well as the group it belongs to -x ,x ,...x -,
1 1 2 p
both base-learners (predictors) will have an associated variable importance as defined
before. This allows us to differentiate between the individual contribution of x as its
1
own variable and the contribution of the group x belongs to. It is impossible to com-
1
pute the aggregated variable importance of x as it is unclear how much x contributes
1 1
to the group. However, the aggregated coefficients can be computed using get_coef(),
whichalsoreturnstheaggregatedimportanceofallgroupsvs.allindividualvariablesin
a separate data.frame. With plot_varimp() one can visualize the variable importance
as a barplot. Since group sizes can be large, the function allows to cut of the name of
a predictor after max_char_length characters. One can indicate the maximum number
of predictors to be printed through n_predictors or through the minimal variable im-
portance a predictor has to have through prop. Through both parameters, the number
of printed entries can be reduced. Note, that in this case, the relative importance of
groups in the legend is based only on the plotted variables and not the ones removed.
Adding information about the direction of effect sizes, one could add arrows behind the
bars [9]. For groups, one can use the aggregated coefficients from get_coef().
slice (get_varimp(sgb_model =sgb_model_linear)$varimp ,1:5)
# A tibble: 5 6
reduction blearner predictor selfreq type relative_
importance
<dbl> <chr> <chr> <dbl> <chr> <dbl>
1 0.297 bols(X1, X2,... X1, X2, ... 0.206 group 0.301
2 0.288 bols(X18, in... X18 0.0196 indi... 0.292
3 0.230 bols(X11, X1... X11, X12... 0.25 group 0.233
4 0.0414 bols(X7, int... X7 0.0784 indi... 0.0419
5 0.0392 bols(X6, int... X6 0.0833 indi... 0.0397
get_varimp(sgb_model = sgb_model_linear)$group_importance
# A tibble: 2 × 2
type importance
<chr> <dbl>
1 group 0.534
2 individual 0.466
plot_varimp(sgb_model = sgb_model_linear , n_predictors = 15)
4Figure 2. Variableimportanceofthesparse-groupboostingmodelforsimulateddata
In this example, we see that both individual variables and groups were selected and
contributed to the reduction of the loss function. The most important predictor is the
first group, followed by variable 18, and then followed by group three. This is in line
with what was simulated, as variable 18 has the biggest beta value, and groups one
and three are full groups, meaning all variables within the groups have a non-zero beta
coefficient.Groupstwoandfourhavewithin-groupsparsity,thereforetheywereselected
as individual variables rather than groups.
2.3.2. Model coefficients
Theresultingcoefficientscanberetrievedthroughget_coef()Insparse-groupboosting
models, a variable in a dataset can be selected as an individual variable or through a
group. Therefore, there can be two associated effect sizes for the same variable. This
function aggregates both and returns them in a data.frame sorted by the effect size
’effect’.
slice (get_coef(sgb_model = sgb_model)$raw,1:5)
# A tibble: 5 × 5
variable effect blearner predictor type
<chr> <dbl> <chr> <chr> <chr>
1 X18 0.364 bols(X18, int... X18 individual
2 X5 0.250 bols(X1, X2, ... X1, X2, X3, X4, X5 group
3 X15 -0.249 bols(X11, X12... X11, X12, X13, X14, X15 group
4 X4 0.234 bols(X1, X2, ... X1, X2, X3, X4, X5 group
5 X11 -0.228 bols(X11, X12... X11, X12, X13, X14, X15 group
slice (get_coef(sgb_model = sgb_model)$aggregate ,1:5)
# A tibble: 5 × 4
variable effect learner predictor
<chr> <dbl> <chr> <chr>
1 X18 0.364 bols(X18, inte... X18
52 X15 -0.272 bols(X11, X12,...; X11, X12, X13, X14, X15; X15
bols(X15, inte...
3 X5 0.250 bols(X1, X2,... X1, X2, X3, X4, X5
4 X4 0.234 bols(X1, X2,... X1, X2, X3, X4, X5
5 X13 -0.230 bols(X11, X12,...; X11, X12, X13, X14, X15; X13
bols(X13, inte...
We see that the effect sizes differ between the two perspectives. The variable X15 for
examplehasamoreextrememodelcoefficientof-0.272intheaggregatedcasecompared
to the coefficient of -0.249 derived only from the group base-learner. Consequently, also
the ordering differs. X11 has a greater absolute model coefficient from the group than
X13, but in the aggregated version the absolute model coefficient of X13 exceeds the
one of X11.
2.3.3. Plotting model coefficients and importance
With plot_effects() we can plot the effect sizes of the sparse-group boosting model
in relation to the relative importance to get an overall picture of the model. Through
the parameter ’plot_type’ one can choose the type of visualization. ’radar’ refers to
aradarplotusingpolarcoordinates.Heretheangleisrelativetothecumulativerelative
importance of predictors and the radius is proportional to the effect size. ’clock’ does
thesameas’radar’butusesclockcoordinatesinsteadofpolarcoordinates.’scatter’
uses the effect size as the y-coordinate and the cumulative relative importance as the
x-axis in a classical Scatter plot.
plot_effects(sgb_model = sgb_model, n_predictors = 5,
base_size = 10)
plot_effects(sgb_model = sgb_model, n_predictors = 5,
plot_type = "clock", base_size = 10)
plot_effects(sgb_model = sgb_model, n_predictors = 5,
plot_type = "scatter ", base_size = 10)
6Figure 3. Effectsizeand
2.3.4. Coefficient path
plot_pathcallsget_coef_path()toretrievetheaggregatedcoefficientsfromamboost
object for each boosting iteration and plots it, indicating if a coefficient was updated
by an individual variable or group.
plot_path(sgb_model = sgb_model)
7Figure 4. Coefficientpathofasparse-groupboostingmodelwithsimulateddata
In the coefficient path shown in Figure 4, we see the change in model coefficients. Since
thepathshowstheaggregatedmodelcoefficients,thepathofonevariableinthedataset
mayhavebothcolors.ThisisthecasewithvariableX1whichwasfirstupdatedthrough
the group and then also as an individual variable or with variable X15 in reverse order.
3. Real data
Inthissection,wewillfitasparse-groupboostingmodelwithsgboosttoarealdataset.
We will use behavioral ecological data and an associated group structure [11] to explain
whether farmers in Chile and Tunisia are planning to take adaptive measures against
climate change in the following years. We will use a logistic regression model for this
binary decision. The data consists of 14 groups and 84 variables for the 801 farmers.
Groups include vulnerability to climate change [12], social, biophysical, and economic
assets,aswellasperceptionsofthefarmers.Afterloadingthedataandgroupstructure,
we create the formula with mixing parameter α = 0.3. Then, we pass the formula to
mboost() with 1000 boosting iterations and a learning rate of 0.3.
model_df <− readRDS(’model_df.RDS’) %>%
mutate_at(index_df$col_names , factor)
index_df <− readRDS(’index_df.RDS’)
sgb_formula <− create_formula(
group_df = index_df , var_name = ’col_names ’ ,
group_name = ’index ’ , outcome_name = ’S5.4 ’
)
model <− mboost(
sgb_formula , data = model_df,
family = Binomial(link = ’logit ’) ,
8control = boost_control(mstop = 1000, nu = 0.3)
)
cv_model <− cvrisk(model)
model <− model[mstop(cv_model)]
The model is stopped early after 466 Boosting iterations. We examine the coefficient
path and see that in the early stage, individual base-learners were dominantly selected
like the variable ’S1.8b or ’S8.11 river’ which indicates whether river irrigation is used.
Many of the variables were first included as individual variables and later also through
group base-learners like ’S8.1b’ or ’S2.5c proximity’ (Proximity to extreme weather
events), which we also saw in the simulated data.
plot_path(model)
Figure 5. Coefficientpathusingtheecologicaldataset
In figure 6, we look at the variable importance with the default values, plotting all 27
selected predictors of which 8 are groups, the latter having a relative variable impor-
tance of 22 percent. The most important base-learner is the individual variable ’S1.8b’
indicating whether farming journals are being used and the most important group is
the social asset group followed by the group consisting of the four considered regions.
plot_varimp(model)
9Figure 6. Variableimportanceusingtheecologicaldataset
Plotting the effect sizes of all predictors having a relative importance of greater than
1.5 percent shows the tendency for more important variables to have greater absolute
effect sizes. For readability, we set the number of printed characters per variable to 6
and use the ’scatter’ version of the plot.
plot_effects(
model, plot_type = ’scatter ’ ,
prop = 0.015, max_char_length = 6
)
10Figure 7. Coefficientplotusingtheecologicaldataset
4. The sparse-group boosting algorithm
General functional gradient descent Algorithm ([3])
(1) Define base-learners of the structure h : Rn×p → R
(2) Initialize m = 0 and fb[0] ≡ 0 or fb[0] ≡ y
(3) Set m = m+1 and compute the negative gradient ∂ l(y,f) and evaluate it at
∂f
fb[m−1]. Doing this yields the pseudo-residuals u 1,...,u
n
with
∂
u[m] = l(y ,f)| ,
i ∂f i f=fb[m−1]
for all i = 1,...,n
(4) Fit the base-learner h with the response (u[m] ,...,u[m]) to the data. This yields
1 n
hb[m], which is an approximation of the negative gradient
(5) Update
fb[m] = fb[m−1]+η·hb[m]
here η can be seen as learning rate with η ∈]0,1[
(6) Repeat steps 3,4 and 5 until m = M
‘sgboost‘ uses the following adjustment to the general functional gradient descent al-
gorithm. The sparse-group-boosting optimization problem is formulated as the sum
of mean squared error (linear regression) or negative log-likelihood (generalized linear
models e.g. logistic regression) within each boosting iteration in steps 3 and 4. The
11base-learner with the smallest evaluated loss function will be selected.
Linear regression:
• Group base-learner:
(cid:18) 1 (cid:19)
argmin min ∥u[m]−X(g)β(g)∥2+λ(g)∥β(g)∥2 .
g≤G β(g)∈Rp
2n 2 2
• Individual base-learner:
(cid:18) 1 (cid:19)
argmin min ∥u[m]−X(g) β(g) ∥2+λ(g) ∥β(g) ∥2 .
g≤G,j≤pgβ j(g)∈R
2n j j 2 j j 2
If the group base-learner with the smallest loss as defined above has a smaller loss than
the individual base-learner with the smallest loss then the group base-learner will be
selected in the update and vice versa.
Generalized linear models:
• Group base-learners:
(cid:16) (cid:17)
argmin min −loglik(u[m],X(g),β(g))+λ(g)∥β(g)∥2 .
2
g∈G β(g)∈R
• Individual base-learners:
(cid:16) (cid:17)
argmin min −loglik(u[m],X(g) ,β(g))+λ(g) ∥β(g) ∥2 .
j j j j 2
g∈G,j∈pgβ j(g)∈R
where
• df(λ(g)) = α, df(λ(g)) = 1−α
j
• df(λ) = tr(2H −HTH ) and H is the Ridge Hat matrix of a base-learner
λ λ λ λ
• X(g) is the submatrix of X with columns corresponding to group g and X(g) the
j
j-th column of group g.
• β(g) and β(g) are the corresponding parameter vectors of the features in group g.
j
• α adjusts the weight between group and individual base-learners.
• λ(g),λ(g) Ridge penalty parameters.
j
• u[m] is the current negative gradient vector from the previous boosting iteration.
Funding
This research is funded by dtec.bw – Digitalization and Technology Research Center
of the Bundeswehr. dtec.bw is funded by the European Union – NextGenerationEU.
All statements expressed in this article are the authors’ and do not reflect the official
opinionsorpoliciesoftheauthors’hostaffiliationsoranyofthesupportinginstitutions.
References
[1] K.N. Berk, Forward and backward stepping in variable selection, Journal of
Statistical Computation and Simulation 10 (1980), pp. 177–185. Available at
12https://doi.org/10.1080/00949658008810367, Publisher: Taylor & Francis _eprint:
https://doi.org/10.1080/00949658008810367.
[2] P. Bühlmann and T. Hothorn, Boosting Algorithms: Regularization, Predic-
tion and Model Fitting, Statistical Science 22 (2007), pp. 477–505. Available at
https://projecteuclid.org/journals/statistical-science/volume-22/issue-4/
Boosting-Algorithms-Regularization-Prediction-and-Model-Fitting/10.1214/
07-STS242.full, Publisher: Institute of Mathematical Statistics.
[3] J.H. Friedman, Greedy function approximation: A gradient boosting machine.,
The Annals of Statistics 29 (2001), pp. 1189–1232. Available at https:
//projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/
Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/
1013203451.full.
[4] T. Hothorn, P. Buehlmann, T. Kneib, M. Schmid, B. Hofner, F. Otto-Sobotka, F.
Scheipl,andA.Mayr,mboost: Model-Based Boosting (2023).Availableathttps://CRAN.
Rproject.org/package=mboost.
[5] J. Huang, S. Ma, H. Xie, and C.H. Zhang, A group bridge approach for variable selec-
tion, Biometrika 96 (2009), pp. 339–355. Available at https://www.jstor.org/stable/
27798828, Publisher: [Oxford University Press, Biometrika Trust].
[6] X.Liang,A.Cohen,A.S.Heinsfeld,F.Pestilli,andD.J.McDonald,sparsegl: An R Pack-
age for Estimating Sparse Group Lasso (2023). Available at http://arxiv.org/abs/
2208.02942, arXiv:2208.02942 [stat].
[7] L. Meier, S. Van De Geer, and P. Bühlmann, The Group Lasso for Logistic Regression,
Journal of the Royal Statistical Society Series B: Statistical Methodology 70 (2008), pp.
53–71. Available at https://doi.org/10.1111/j.1467-9868.2007.00627.x.
[8] F. Obster, sgboost: Sparse-Group Boosting (2024). Available at https://CRAN.
R-project.org/package=sgboost.
[9] F. Obster, H. Bohle, and P.M. Pechan, The financial well-being of fruit farmers in Chile
and Tunisia depends more on social and geographical factors than on climate change,
Communications Earth & Environment 5 (2024), pp. 1–12. Available at https://www.
nature.com/articles/s43247-023-01128-2, Number: 1 Publisher: Nature Publishing
Group.
[10] F.ObsterandC.Heumann,Sparse-groupboosting–Unbiasedgroupandvariableselection,
Tech. Rep. arXiv:2206.06344, arXiv, 2022. Available at http://arxiv.org/abs/2206.
06344, arXiv:2206.06344 [stat] type: article.
[11] F. Obster, C. Heumann, H. Bohle, and P. Pechan, Using interpretable boosting algo-
rithms for modeling environmental and agricultural data, Scientific Reports 13 (2023), p.
12767. Available at https://www.nature.com/articles/s41598-023-39918-5, Num-
ber: 1 Publisher: Nature Publishing Group.
[12] P.M. Pechan, H. Bohle, and F. Obster, Reducing vulnerability of fruit orchards to cli-
mate change, Agricultural Systems 210 (2023), p. 103713. Available at https://www.
sciencedirect.com/science/article/pii/S0308521X2300118X.
[13] N. Simon, J. Friedman, T. Hastie, and a.R. Tibshirani, SGL: Fit a GLM (or Cox Model)
with a Combination of Lasso and Group Lasso Regularization (2019).Availableathttps:
//CRAN.R-project.org/package=SGL.
[14] N. Simon, J. Friedman, T. Hastie, and R. Tibshirani, A Sparse-Group Lasso, Journal
of Computational and Graphical Statistics 22 (2013), pp. 231–245. Available at http:
//www.tandfonline.com/doi/abs/10.1080/10618600.2012.681250.
[15] Z. Zhang, Variable selection with stepwise and best subset approaches, Annals of Trans-
lational Medicine 4 (2016), p. 136. Available at https://www.ncbi.nlm.nih.gov/pmc/
articles/PMC4842399/.
[16] N. Zhou and J. Zhu, Group Variable Selection via a Hierarchical Lasso and Its Oracle
Property (2010).Availableathttp://arxiv.org/abs/1006.2871,arXiv:1006.2871[stat].
13