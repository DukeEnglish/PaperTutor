No-Regret Learning for Fair Multi-Agent
Social Welfare Optimization
MengxiaoZhang RamiroDeo-CampoVuong
UniversityofSouthernCalifornia UniversityofSouthernCalifornia
mengxiao.zhang@usc.edu rdeocamp@usc.edu
HaipengLuo
UniversityofSouthernCalifornia
haipengl@usc.edu
Abstract
Weconsidertheproblemofonlinemulti-agentNashsocialwelfare(NSW)maxi-
mization.WhilepreviousworksofHossainetal.[2021],Jonesetal.[2023]study
similarproblemsinstochasticmulti-agentmulti-armedbanditsandshowthat√T-
regretispossibleafterT rounds,theirfairnessmeasureistheproductofallagents’
rewards, instead of their NSW (that is, their geometricmean). Given the funda-
mentalroleofNSWinthefairnessliterature,itismorethannaturaltoaskwhether
no-regretfairlearningwithNSWastheobjectiveispossible.Inthiswork,wepro-
videacompleteanswertothisquestioninvarioussettings.Specifically,instochas-
ticN-agentK-armedbandits,wedevelopanalgorithmwith (KN2 TN N−1 )regret
O
andprovethatthedependenceonT istight,makingitasharpcontrasttothe√T-
regret bounds of Hossain et al. [2021], Jones et al. [2023].eWe then consider a
more challenging version of the problem with adversarial rewards. Somewhat
surprisingly,despiteNSWbeingaconcavefunction,weprovethatnoalgorithm
canachievesublinearregret. Tocircumventsuchnegativeresults,wefurthercon-
siderasettingwithfull-informationfeedbackanddesigntwoalgorithmswith√T-
regret: the first one has no dependenceon N at all and is applicable to not just
NSWbuta broadclassofwelfarefunctions,whilethe secondonehasbetterde-
pendence on K and is preferable when N is small. Finally, we also show that
logarithmicregretispossible wheneverthereexists oneagentwho is indifferent
aboutdifferentarms.
1 Introduction
In this paper, we study online multi-agent Nash social welfare (NSW) maximization, which is a
generalizationoftheclassicmulti-armedbandit(MAB)problem[Thompson,1933,LaiandRobbins,
1985]. Different from MAB, in which the learner makes her decisions sequentially in order to
maximizeherownreward,inonlinemulti-agentNSWmaximization,thelearner’sdecisionaffects
multiple agents and the goal is to maximize the NSW over all the agents. Specifically, NSW is
definedasthegeometricmeanoftheexpectedutilitiesoverallagents[Moulin,2004],whichcanbe
viewedasameasureoffairnessamongtheagents. Thisproblemincludesmanyimportantreal-life
applicationssuch as resourceallocation [Joneset al., 2023], where the learner needsto guarantee
fairallocationsamongmultipleagents.
RecentworkbyHossainetal.[2021],Jonesetal.[2023]studiesasimilarproblembutwithNSW
prod
astheobjective,avariantofNSWthatisdefinedastheproductoftheutilitiesoveragentsinsteadof
Preprint.Underreview.
4202
yaM
13
]GL.sc[
1v87602.5042:viXratheirgeometricmean.Whiletheoptimalstrategyisthesameiftheutilityforeachagentisstationary,
this is not the case with a non-stationary environment. Moreover, NSW is homogeneous of
prod
degree N instead of degree 1, where N is the number of agents, meaning that NSW is more
prod
sensitive to the scale of the utility. Specifically, if the utilities of each agentare scaled by 2, then
NSWisscaledby2aswell,butNSW isscaledby2N. Therefore,itisarguablymorereasonable
prod
toconsiderregretwithrespecttoNSW,whichhasnotbeenstudiedbefore(toourknowledge)and
isthemainobjectiveofourwork.
From a technical perspective, however, due to the lack of Lipschitzness, NSW poses much more
challengesinregretminimizationcomparedtoNSW .Forexample,onecannotdirectlyapplythe
prod
algorithmforLipschitzbandits[Kleinbergetal.,2019]toourproblem,whileitisdirectlyapplicable
toNSW asmentionedin[Hossainetal.,2021,Jonesetal.,2023]. Despitesuchchallenges,we
prod
managetoprovidecompleteanswerstothisproblemin varioussetting. Specifically,ourcontribu-
tionsare listedbelow(whereT,N, andK denotethe numberofrounds,agents, andarms/actions
respectively):
• (Section3)Wefirststudythestochasticbanditsetting,wheretheutilitymatrixateachroundisi.i.d.
drawnfromanunknowndistribution,andthelearnercanonlyobservetheutilities(fordifferent
agents)oftheactionshepicked.Inthiscase,wedevelopanalgorithmwith (KN2 TN N−1 )regret.1
O
WhileouralgorithmisalsonaturallybasedontheUpperConfidenceBound(UCB)algorithmas
in Hossain et al. [2021],Jones et al. [2023], we show that a novelanalysies with Bernstein-type
confidenceintervalsis importantforhandlingthe lack ofLipschitznessofNSW. Moreover,we
prove a lower bound of order Ω( N1
3 ·
KN1 TN N−1 ), showing that the dependence on T is tight.
Thisisinsharpcontrasttothe√T-regretboundofHossainetal.[2021],Jonesetal.[2023]and
demonstratesthedifficultyofleearningwithNSWcomparedtoNSW .
prod
• (Section4.1)Wethenconsideramorechallengingsettingwheretheutilitymatrixateachround
can be adversariallychosen by the environment. Somewhatsurprisingly, we show that no algo-
rithmcanachievesublinearregretinthiscase,despiteNSWbeingconcaveandthevastliterature
onbanditonlinemaximizationwithconcaveutilityfunctions(thesubtletyliesintheslightlydif-
ferentfeedbackmodel).Infact,thesameimpossibilityresultalsoholdsforNSW asweshow.
prod
• (Section4.2)Tobypasssuchimpossibility,wefurtherconsiderthisadversarialsettingunderricher
feedback,wherethe learner observesthe fullutility matrixafter herdecision (theso-called full-
informationfeedback). Contrarytothebanditfeedbacksetting,learningisnotonlypossiblenow
butcan also be much faster despite having adversarialutilities. Specifically, we design two dif-
ferentalgorithmswith√T-regret.ThefirstalgorithmisbasedonFollow-the-Regularized-Leader
(FTRL) with the log-barrier regularizer, which achieves (√KT logT) regret (Section 4.2.1).
O
Notably, this algorithmdoesnothave anydependenceon the numberof agentsN and can also
be generalizedto a broaderclass ofsocial welfare functions. The secondalgorithmis based on
FTRLwithaTsallisentropyregularizer,whichachieves (K1 2− N1√NT)regretandisthusmore
O
favorablewhenK ismuchlargerthanN (Section4.2.2).Finally,wealsoshowthatimprovedlog-
arithmicregretispossibleaslongasateachroundthereexeistsatleastoneagentwhoisindifferent
aboutthelearner’schoiceofarm(Section4.2.3).
1.1 RelatedWork
Hossain et al. [2021], Jones et al. [2023] are most related to our work. Hossain et al. [2021] is
the first to consider designing no-regret algorithms under NSW for the stochastic multi-agent
prod
multi-armedbanditproblem. Specifically, theyproposetwo algorithms. The first one is based on
ε-greedyandachieves (T32 )regretefficiently,andthesecondoneisbasedonUCBandachieves
O
(√T) regretinefficiently. Jones etal. [2023]improvesthese results by providinga betterUCB-
O
basedalgorithmthatisefficientandachievesthesame (√T)regret.Tothebestofourknowledge,
O
teherearenopreviousresultsforregretminimizationoverNSWunderthisparticularsetup.
However, several other models of fairness have beeneintroduced in (single-agent or multi-agent)
multi-armedbandits,someusingNSWaswell. Thesemodelsdifferinwhethertheyaimtobefair
amongdifferentobjectives,differentarms,differentagents,differentrounds,orothers.Mostrelated
1Thenotation e ()andΩe ()hidelogarithmicdependenceonK,N,andT.
O · ·
2tothispaperismulti-objectivebandits,inwhichthelearnertriestoincreasedifferentandpossibly
competingobjectivesinafairmanner.Forexample,DruganandNowe[2013]introducesthemulti-
objectivestochasticbanditproblemandoffersaregretmeasuretoexploreParetoOptimalsolutions,
andBusa-Feketeetal.[2017]investigatesthesamesettingusingtheGeneralizedGiniIndexintheir
regretmeasuretopromotefairnessoverobjectives. Theirregretmeasurecloselyresemblestheone
weuse,excepttheyapplysomesocialwelfarefunction(SWF)tothecumulativeexpectedutilityof
agentsoverallroundsasopposedto theexpectedutilityofagentseachround. Onthe otherhand,
someotherworksstudyfairnessamongdifferentroundswhichincentivizesthelearnertoperform
wellconsistentlyoverallrounds[Barmanetal.,2023,Sawarnietal.,2024]. Yetanothermodelwe
areawareofmeasuresfairnesswithrespecttohowofteneacharmispulled[Josephetal.,2016,Liu
etal.,2017,Gillenetal.,2018,Chenetal.,2020].
Kaneko and Nakamura [1979] axiomatically derives the NSW function. It is a fundamental and
widely-adopted fairness measure and is especially popular for the task of fairly allocating goods.
Caragiannisetal.[2019]justifiesthefairnessofNSWbyshowingthatitsmaximumsolutionensures
somedesirableenvy-freeproperty.Thisresultpromptedthedesignofapproximationalgorithmsfor
the problem of allocating indivisible goods by maximizing NSW, which is known to be NP-hard
even for simple valuation functions [Barman et al., 2018, Cole and Gkatzelis, 2015, Garg et al.,
2023,LiandVondrák,2021].
There is a vast literature on the multi-armed bandit problem; see the book by Lattimore and
Szepesvári [2020] for extensive discussions. The standard algorithm for the stochastic setting is
UCB[LaiandRobbins,1985,Aueretal.,2002a],whilethestandardalgorithmfortheadversarial
settingisFTRLorthecloselyrelatedOnlineMirrorDescent(OMD)algorithm[Aueretal.,2002b,
Audibert and Bubeck, 2010, Abernethy et al., 2015]. For FTRL/OMD, the log-barrier or Tsallis
entropy regularizershave been extensivelystudied in recent years due to some of their surprising
properties(e.g.,[Fosteretal.,2016,WeiandLuo,2018,ZimmertandSeldin,2019,Leeetal.,2020]).
Theyarerarelyusedinthefull-informationsettingasfarasweknow,butouranalysisrevealsthat
theyareusefuleveninsuchsettings,especiallyfordealingwiththelackofLipschitznessofNSW.
2 Preliminaries
GeneralNotation. Throughoutthispaper,wedenotetheset 1,2,...,n by[n]foranypositive
integern. Fora matrixM Rm×n, we denotethe i-throw v{ ectorof M} byM Rn, the j-th
i,:
columnvectorofM byM ∈ Rm,andthe(i,j)-thentryofM byM . WesayM∈ 0ifM isa
:,j i,j
∈ (cid:23)
positivesemi-definitematrix. The(K 1)-dimensionalsimplexisdenotedas∆ ,anditsclipped
K
versionwithaparameterδ >0isdeno− tedas∆ = p ∆ p δ, i [K] . Weuse0and
K,δ K i
1 todenotetheall-zeroandall-onevectorinanappro{ pria∈ tedim| ensi≥ on. F∀ or∈ twora} ndomvariables
d
X andY,weuseX =Y tosayX isequivalenttoY indistribution.
Foratwicedifferentiablefunctionf,weuse f()and 2f()todenoteitsgradientandHessian.
∇ · ∇ ·
For concavefunctionsthatare notdifferentiable, f() denotesa super-gradient. Throughoutthe
∇ ·
paper,westudyfunctionsoftheformf(u⊤p)foru [0,1]m×n andp ∆ . Insuchcases, the
m
∈ ∈
gradient,super-gradient,orhessianareallwithrespecttopunlessdenotedotherwise(forexample,
wewrite f(u⊤p),withanexplicitsubscriptu,todenotethegradientwithrespecttou).
u
∇
SocialWelfareFunctions Asocialwelfarefunction(SWF)f :[0,1]N [0,1]measuresthede-
→
sirabilityoftheagents’expectedutilities. Specifically,fortwodifferentvectorsofexpectedutilities
µ,µ′ [0,1]N,f(µ)>f(µ′)meansthatµisafaireralternativethanµ′. Ineachsettingweexplore,
∈
eachactionby thelearneryieldssome expectedutilityforeachof theN agents, and thelearner’s
goalismaximizesomeSWFappliedtotheseN expectedutilities.
NashSocialWelfare(NSW) Forthemajorityofthispaper,wefocusonaspecifictypeofSWF,
namelytheNashSocialWelfare(NSW)function[Nashetal.,1950,KanekoandNakamura,1979].
Specifically,forµ [0,1]N,NSWisdefinedasthegeometricmeanoftheN coordinates:
∈
NSW(µ)= µ1/N. (1)
n
nY∈[N]
Asmentioned,Hossainetal.[2021],Jonesetal.[2023]consideredacloselyrelatedvariantthatis
simplytheproductofthecoordinates:NSW (µ)= µ . ItisclearthatNSWhasabetter
prod n∈[N] n
Q
3scalingpropertysince itishomogeneous: scalingeachµ bya constantcalso scalesNSW(µ)by
n
c, but it scales NSW (µ) by cN. This makes NSW an unnatural learning objective, which
prod prod
motivatesus to use NSW as our choice of SWF. Learningwith NSW, however,bringsextra chal-
lengessinceitisnotLipschitzinthesmall-utilityregime(whileNSW isLipschitzovertheentire
prod
[0,1]N). Weshallseeinsubsequentsectionshowweaddresssuchchallenges.
WeremarkthatwhileourmainfocusisregretminimizationwithrespecttoNSW,someofourresults
alsoapplytoNSW ormoregeneralclassesofSWFs(aswillbecomeclearlater).
prod
Problem Setup. The N-agentK-armed social welfare optimization problem we consider is de-
fined as follows. Ahead of time, with the knowledge of the learner’s algorithm, the environment
decidesT utilitymatricesu ,...,u [0,1]K×N, whereu istheutilityofagentnifarm/ac-
1 T t,i,n
∈
tioniisselectedatroundt. Then,thelearnerinteractswiththeenvironmentforT rounds: ateach
round t, the learner decides a distribution p ∆ and then samples an action i p . In the
t K t t
∈ ∼
full-informationfeedbacksetting, the learner observesthe full utility matrix u after her decision,
t
andinthebanditfeedbacksetting,thelearneronlyobservesu foreachagentn [N],thatis,
t,it,n
∈
theutilitiesoftheselectedaction.
Weconsidertwodifferenttypesofenvironments,thestochasticoneandtheadversarialone,witha
slightdifferenceintheirregretdefinition. Inthestochasticenvironment,thereexistsameanutility
matrixu [0,1]K×N such thatat eachroundt, u is an i.i.d. randomvariablewith meanu. Fix
t
∈
anSWFf. Thesocialwelfareofastrategyp ∆ isdefinedasf(u⊤p),whichiswithrespectto
K
∈
theagents’expectedutilitiesovertherandomnessofboththelearner’sandtheenvironment’s. The
regretisthendefinedasfollows:
T
Reg =T max f(u⊤p) E f(u⊤p ) , (2)
sto ·p∈∆K − "
t=1
t #
X
whichisthedifferencebetweenthetotalsocialwelfareoftheoptimalstrategyandthatofthelearner.
Whenf is chosentobeNSW , Eq. (2)reducestotheregretnotionconsideredinHossainetal.
prod
[2021],Jonesetal.[2023].
On the other hand, in the adversarialenvironment,we do not make any distributionalassumption
on the utility matrices and allow them to be selected arbitrarily. The social welfare of a strategy
p ∆ for time t is defined as f(u⊤p), and the overallregret of the learner is correspondingly
∈ K t
definedas:
T T
Reg = max f(u⊤p) E f(u⊤p ) . (3)
adv p∈∆K
t=1
t − "
t=1
t t #
X X
InbothEq. (2)andEq.(3),theexpectationistakenwithrespecttotherandomnessofthealgorithm.
ConnectiontoBanditConvexoptimization. Whentakingf = NSW(ourmainfocus)andcon-
sidering the bandit feedback setting, our problem is seemingly an instance of the heavily-studied
Bandit Convex optimization (BCO) problem, since NSW is concave. However, there is a slight
butcriticaldifferenceinthe feedbackmodel: a BCO algorithmwouldrequireobservingf(u⊤p ),
t t
or equivalently u⊤p , at the end of each round t, while in our problem the learner only observes
t t
u , a much more realistic scenario. Even though they have the same expectation, due to the
t,it,:
non-linearityofNSW,thisslightdifferenceinthefeedbackturnsouttocauseahugedifferencein
terms of learning — the minimax regret for BCO is known to be Θ(√T), while in our problem
(withbanditfeedback),aswewillsoonshow,theregretiseitherΘ(TN N−1 )inthestochasticsetting
or even Ω(T)in the adversarialsetting. Therefore, in a sense our problemis muchmore difficult
thanBCO.FormoredetailsonBCO,wereferthereadertoarecentsurveybyLattimore[2024].
3 Stochastic Environments with BanditFeedback
Inthissection,weconsiderregretminimizationoverf =NSWwithbanditfeedbackinthestochas-
tic setting, where the utility matrix u at each round t [T] is i.i.d. drawn from a distribution
t
∈
withmeanu. Again,thisisthesamesetupasHossainetal.[2021],Jonesetal.[2023]exceptthat
NSW isreplacedwithNSW.
prod
4Algorithm1UCBforN-agentK-armedNSWmaximization
Input:warm-upphaselengthN >0.
0
Initialization:u =1foralln [N],i [K]. N =0foralli [K].
1,i,n 1,i
∈ ∈ ∈
fort=1,2,...,T do
ift KN then selecti = t ;
≤ b0 t ⌈N0⌉
elsecalculatep =argmax NSW(p,u )andselecti p ;
t p∈∆K t t ∼ t
Observeu foralln [N].
t,it,n
∈
UpdatecountersN =N +1andN =N fori=i .
t+1,it t,it bt+1,i t,i
6
t
Updateupperconfidenceutilitymatrix:
u¯ log(NKT2) 8log(NKT2)
u =u¯ +4 t,i,n + , (4)
t+1,i,n t,i,n
s N t+1,i N t+1,i
foralln [N]anbdi [K]whereu¯ = 1 u i =i .
∈ ∈ t,i,n Nt+1,i τ≤t τ,i,n { τ }
end
P
3.1 UpperBound: aRefinedAnalysisofUCBwithaBernstein-TypeConfidenceSet
We start by describingouralgorithmandits regretguarantee,followedby discussion onwhat the
key ideas are and how the algorithm/analysis is different from previous work. Specifically, our
algorithm,showninAlgorithm1,isbasedontheclassicUCBalgorithm. Itstartsbypickingeach
action for N = (1) rounds. After this warm-upphase, at each time t, the algorithm picks the
0
O
optimalstrategyp thatmaximizestheNSWwithrespecttosomeupperconfidenceutilitymatrixu .
t t
Aftersamplinganeactioni
t
p t,thealgorithmobservestheutilityofeachagentforactioni
t
and
∼
thenupdatestheupperconfidenceutilitymatrixu astheempiricalaverageutilityplusacertain
t+1 b
Bernstein-typeconfidencewidth(Eq.(4)).
ThefollowingtheoremshowsthatAlgorithm1gbuarantees (KN2 TK K−1 )expectedregret(withf,
O
inthedefinitionofReg ,settoNSW;thesamebelowunlessstatedotherwise).Thefullproofcan
sto
befoundinAppendixA. e
Theorem3.1. WithN
0
=1+18logKT,Algorithm1guaranteesE[Reg sto]= O(KN2TN N−1 +K).
OtherthanreplacingNSW withNSW, ouralgorithmdiffersfromthatofJonesetal.[2023]in
prod e
theformoftheconfidencewidth,andtheanalysissketchbelowexplainswhyweneedthischange.
Specifically,foreitherf =NSW orf =NSW,standardanalysisofUCBstatesthattheregretis
prod
boundedby T f(u⊤p ) f(u⊤p ) . Whenf isNSW , aLipschitzfunction,Hossainetal.
t=1 t t − t prod
[2021,Lemma3]shows
P (cid:12) (cid:12)
(cid:12) b (cid:12) N K
NSW (u⊤p ) NSW (u⊤p ) p u u , (5)
prod t t − prod t ≤ t,i | t,i,n − i,n |
n=1i=1
(cid:12) (cid:12) XX
andtherestofth(cid:12)eanalysisf bollowsbydirectcalculati(cid:12)ons.However,wh benf isNSW,anon-Lipschitz
function,wecannotexpectsomethingsimilartoEq. (5)anymore. Indeed,directcalculationshows
thattheLipschitzconstantofNSW(u⊤p)withrespecttou
:,n
equalstoΘ N n=1hp,u
:,n
i−N N−1 ,
whichcanbearbitrarilylargewhen hp,u :,n iiscloseto0forsomen ∈[N](cid:16)a PndN ≥2. (cid:17)
Tohandlethisissue,werequireamorecarefulanalysis. Specifically,usingFreedman’sinequality,
weknowthatwithahighprobability,
u log(NKT2)
u u ,u +8 i,n + (1/N ) u ,2u + (1/N ) . (6)
t,i,n i,n i,n t,i i,n i,n t,i
∈" s N t,i O #⊆ O
h i
Wit bhthehelpofEq. (6),weconsidertwodifferentecasesateachroundt. Thefirsetcaseisthatthere
existscertainn [N]suchthat p ,u σ forsomeσ > 0tobechosenlater. Inthiscase,we
t :,n
∈ h i ≤
useEq. (6)toshow
K 1
p N
NSW(u⊤p ) NSW(u⊤p ) NSW(u⊤p ) + t,i
t t − t ≤O t O (cid:18)i=1 N t,i(cid:19) !
(cid:12) (cid:12) (cid:0) (cid:1) X
(cid:12) b (cid:12) e
5
1K 1
σN1 + p t,i N , (7)
≤ O (cid:18)i=1 N t,i(cid:19) !
X
where the first inequality uses Eq. (6) and the see cond inequality is because NSW(u⊤p )
t
1 ≤
p t,u n N for any n [N]. For the second term in Eq. (7), a standard analysis shows that it is
h upperbi oundedby ∈ KN1 TN N−1 .
O
Nowweconsiderthe(cid:0)casewhere(cid:1) p t,u
:,n
σ foralln [N]. Inthiscase, viaadecomposition
e h i ≥ ∈
lemma(LemmaC.1),weshowthat
N N
(cid:12)
(cid:12)NSW(u b⊤
t
p t) −NSW(u⊤p t)
(cid:12)
(cid:12)≤
n
X=1hhp t,u
bt,:,n
iN1 −hp t,u
:,n
iN1 i=
O
n
X=1h Np t, hu
p
btt ,,: u,n :,−
n
iu
N
N: −,n 1i
(!
8).
ToboundEq. (8),weuseEq. (6)again:
K K
hp t p, tu ,t u,:, :n ,n−
N
Nu −: 1,n i
≤O
p t,u
:,n1
N N−1−1
2
i=1rNp t t, ,i
i!≤O
σ1 2−N N−1 i=1rNp t t, ,i i!, (9)
h b i h i X X
wherethelastinequalityisduetothecondition p ,u σ foralln [N]. Finally,combining
t :,n
h i ≥ ∈
Eq. (7),Eq. (8),Eq. (9),followedbydirectcalculations,weshowthat
T
E[Reg sto]
≤
NSW(u⊤
t
p t) −NSW(u⊤p t)
≤O
TσN1 +KN1 TN N−1 +σ1 2−N N−1 K√T .
Xt=1 (cid:12) (cid:12) (cid:16) (cid:17)
Pickingtheoptimal(cid:12)choicebofσfinishestheproof(cid:12). e
We now highlight the importance of using a Bernstein-type confidence width in Eq. (4): if the
standardHoeffding-typeconfidencewidthisusedinstead,thenonecanonlyobtainu u
t,i,n i,n
− ≤
( 1 ), and consequently, Eq. (8) can only be bounded by σ−N N−1√KT after taking
O Nt,i O
sumqmationovert [T]. Thiseventuallyleadstoaworseregretbound(cid:16)of (K21 NTb2(cid:17)N 2N−1 ).
∈ O
3.2 LowerBound e
Next,weproveanΩ(TN N−1 )lowerboundforthissetting. Thisnotonlyshowsthattheregretbound
weachieveviaAlgorithm1istightinT,butalsohighlightsthedifferenceanddifficultyoflearning
withNSW compareedtolearningwithNSW prod, sincein thelattercase, Θ(√T)regretisminimax
optimal[Hossainetal.,2021,Jonesetal.,2023].
Theorem3.2. Inthebanditfeedbacksetting,foranyalgorithm,thereexistsastochasticenvironment
inwhichtheexpectedregret(withrespecttoNSW)ofthisalgorithmisΩ (lo NgK 3)3 ·KN1TN N−1 for
N logK andsufficientlylargeT. (cid:16) (cid:17)
≥
WedeferthefullprooftoAppendixA.2anddiscussthehardinstanceusedintheproofbelow.First,
the mean utility vectoru foreach agentn 2 is a constantvector1. Thismakesthe problem
:,n
≥
equivalent to a one-agent problem, but with p,u 1/N as the reward, instead of p,u as in
:,1 :,1
h i h i
standardstochasticK-armedbandits.
Then,forthefirstagent,differentfromthestandardK-armedbandits,wherethehardestinstanceis
tohideonearmwithaslightlybetterexpectedrewardof 1+ K/T amongotherK 1armswith
2 −
expectedrewardofexactly 1,2wehideonearmwithexpectedrewardK/T amongotherK 1arms
2 p −
withexactly0reward(sooveralltherewardsareshiftedtowards0butwithasmallergapbetween
thebestarmandtheothers).Bystandardinformationtheoryarguments,withinT roundsthelearner
cannotdistinguishthebestarmfromtheothers.Therefore,thebeststrategyshecanapplyistopick
auniformdistributionoveractions,sufferingΩ((1 K− N1 ) (K/T)N1 )=Ω(KN1 T− N1 )regretper
roundandleadingtoΩ(KN1 TN N−1 )regretoverall.− ·
e
2OnecanshowthatΘ(√T)regretispossibleinthisenvironment,thusnotsuitableforourpurpose.
e
64 Adversarial Environments
Nowthatwehavea completeanswerforthestochasticsetting, wemoveontoconsidertheadver-
sarial case where each u is chosen arbitrarily, a multi-agentgeneralizationof the expertproblem
t
(full-information feedback) [Freund and Schapire, 1997] and the adversarial multi-armed bandit
problem (banditfeedback) [Auer et al., 2002b]. There are no prior studies on this problem, be it
withf =NSWorf =NSW ,asfarasweknow.
prod
4.1 ImpossibilityResultswithBanditFeedback
Westartbyconsideringthebanditfeedbacksetting. AsmentionedinSection2,eventhoughNSW
isaconcavefunction,ourproblemisnotaninstanceofBanditConvexOptimization,sincewecan
onlyobserveu insteadofNSW(u⊤p )attheendofroundt. Somewhatsurprisingly,thisslight
t,it,: t t
differenceinthefeedbackinfactmakesasharpseparationinlearnability—while (√T)regretis
O
achievableinBCO,weprovethato(T)regretisimpossibleinourproblem.
Before showing the theorem and its proof, we first give high level ideas on the construction of
the hard environments. Specifically, we consider the environment with 2 agents, 2 arms, and bi-
nary utility matrix u 0,1 2×2. Similar to the hard instance in the stochastic environment,
t
we set u = 1, redu∈ cin{ g th} e problem to a single-agent one. For the first agent, we let u
t,:,2 t,:,1
at each round t be i.i.d. drawn from a stationary distribution over the 4 binary utility vectors
(0,0),(0,1),(1,0),(1,1) . Then, we construct two different distributions, and ′, over these
{ } E E
4binaryutilityvectorssatisfyingthat:1)thedistributionofthelearner’sobservationisidenticalfor
and ′;2)theoptimalstrategyfor and ′ aresignificantlydifferent. Thefirstpropertyguaran-
E E E E
tees that no algorithm can distinguish these two environments, while the second propertyensures
thatthereisnoonesinglestrategythatcanperformwellinbothenvironments. Formally,weprove
thefollowingtheorem.
Theorem4.1. Inthebanditfeedbacksetting,foranyalgorithm,thereexistsanadversarialenviron-
mentsuchthatE[Reg ]=Ω(T)forf =NSW.
adv
Proof. As sketched earlier, we consider two different environments with 2 agents, 2 arms, and
binary utility matrices u 0,1 2×2, t [T]. In both environments, we have u = 1.
t t,:,2
∈ { } ∈
Next, we construct two different distributions from which u is potentially drawn from, and
t,:,1
′, over (0,0),(0,1),(1,0),(1,1) . Specifically, is characterized by (q ,q ,q ,qE ) =
00 01 10 11
(E4 , 2 , 1{ , 3 ), where q is the p} robability of theE vector (x,y) in ; ′ is characterized by
10 10 10 10 xy E E
(q′ ,q′ ,q′ ,q′ ) = ( 3 , 3 , 2 , 2 ), where q′ is the probability of vector (x,y) in ′. With a
00 01 10 11 10 10 10 10 xy E
slight abuse of notation, we write u for a matrix u 0,1 2×2 if u is drawnfrom and
:,1
u =1;thesamefor ′. ∼ E ∈ { } E
:,2
E
Wearguethatthelearner’sobservationsareequivalentindistributionin and ′,sincethemarginal
E E
distributionsoftheutilityofeachactionarethesame. Specifically,
• When action 1 is chosen, the distributions of the learner’s observation in both and ′ are a
Bernoullirandomvariablewithmeanq +q =q′ +q′ = 4 ; E E
10 11 10 11 10
• When action 2 is chosen, the distributions of the learner’s observation in both and ′ are a
Bernoullirandomvariablewithmeanq +q =q′ +q′ = 5 . E E
01 11 01 11 10
Direct calculation shows p = argmax E NSW(u⊤p) = q 12 0 , q 02 1 =
⋆ p∈∆K u∼E q 02 1+q 12
0
q 02 1+q 12
0
(0.2,0.8)andp′ ⋆ = argmax p∈∆KE u∼E′ NSW(u⊤p) (cid:2) = q′2q +1′2 0 q′2,(cid:3) q′2q +0′2 1 q′(cid:16) 2 = ( 14 3, 19 3), w(cid:17) hich
(cid:18) 10 01 10 01(cid:19)
areconstantapartfromeachother. Picka(cid:2)thresholdva(cid:3)lueθ = 33 (0.2, 4 ). Directcalculation
130 ∈ 13
showsthat for a strategyp with p θ, we have E [NSW(u⊤p ) NSW(u⊤p)] ∆ where
1 u∼E ⋆
∆= 51 00;similarly,forastrategypw≥ ithp
1
<θ,wehaveE u∼E′[NSW(− u⊤p ⋆) −NSW(≥ u⊤p)] ≥∆
aswell. Now, givenanalgorithm,letα be theprobabilitythatthe numberof roundsp θ is
E t,1
≥
largerthan T
2
underenvironment E,andα¯ E′ betheprobabilityofthecomplementofthiseventunder
7Algorithm2FTRLforN-agentK-armedSWFmaximizationwithfull-infofeedback
Inputs:aSWFf,alearningrateη >0,andastronglyconvexregularizerψ :∆ R.
K
→
fort=1,2,...,T do
Playp =argmin p, t−1 f(u⊤p ) + 1ψ(p).
t p∈∆Kh − s=1∇ s s i η
Observeu .
t
end P
environment ′. Wehave,
E
T T
α T∆
E [Reg ] E NSW(u⊤p ) NSW(u⊤p ) E ,
E adv ≥ E " t ⋆ − t t #≥ 2
t=1 t=1
X X
T T
E E′[Reg adv] ≥E E′ " NSW(u⊤ t p′ ⋆) − NSW(u⊤ t p t) #≥ α¯ E′ 2T∆ .
t=1 t=1
X X
Finally,sincethefeedbackforthealgorithmisthesameindistributioninthesetwoenvironments,
weknowα E +α¯ E′ =1,andthus
max {E E[Reg adv],E E′[Reg adv]
}≥
E E[Reg adv]+ 2E E′[Reg adv]
≥
(α
E
+α 4¯ E′)T∆
=Ω(T),
whichfinishestheproof.
In fact, by a similar but more involved construction (that actually requires using two agents in a
non-trivialway),thesameimpossibilityresultalsoholdsforf = NSW ;seeAppendixB.1. We
prod
remarkthatnon-linearityoff intheseresultsplaysanimportantroleinthehardinstanceconstruc-
tion,sinceotherwise,theoptimalstrategyfor and ′willbethesameastheybothinducethesame
E E
marginaldistributions.
4.2 Full-InformationFeedback
To sidestep the impossibility result due to the bandit feedback, we shift our focus to the full-
informationfeedbackmodel,wherethe learnerobservesthe entiretyoftheutility matrixu atthe
t
endofroundt. Asmentioned,thiscorrespondstoamulti-agentgeneralizationofthewell-knownex-
pertproblem[FreundandSchapire,1997]. Weproposeseveralalgorithmsforthissetting,showing
thatthericherfeedbacknotonlymakeslearningpossiblebutalsoleadstomuchlowerregret.
4.2.1 FTRLwithLog-BarrierRegularizer
When f is concave, ourproblemis in factalso an instance of the well-knownOnline ConvexOp-
timization(OCO)[Zinkevich,2003]. However,standardOCOalgorithmssuchasOnlineGradient
Descent,aninstanceofthemoregeneralFollow-the-Regularized-Leaderalgorithmwithaℓ regu-
2
larizer,requiretheutilityfunctiontoalsobeLipschitzandthuscannotbedirectlyappliedtolearning
NSW. Nevertheless,wewillshowthatusingadifferentregularizerthatinducesmorestabilitythan
theℓ regularizercanresolvethisissue.
2
More specifically, the FTRL algorithm is shown in Algorithm 2, which predicts at time t the dis-
tribution p = argmin p, t−1 f(u⊤p ) + 1ψ(p) for some learning rate η and some
t p∈∆Kh − s=1∇ s s i η
strongly convex regularizer ψ. Standard analysis shows that the regret of FTRL contains two
P
terms: the regularization penalty term that is of order 1/η and the stability term that is of order
η f(u⊤p ) 2 where we use the notation a = √a⊤Ma. To deal with the lack
tk∇ t t k∇−2ψ(pt) k kM
the Lipschitzness, thatis, the potentially large f(u⊤p ), we need to find a regularizerψ so that
P ∇ t t
theinducedlocalnorm k∇f(u⊤ t p t) k∇−2ψ(pt) isalwaysreasonablysmalldespite ∇f(u⊤ t p t)being
large(inℓ normforexample).
2
Itturnsoutthatthelog-barrierregularizer,ψ(p)= K logp ,ensuressuchaproperty.Infact,it
− i=1 i
inducesasmalllocalnormnotjustforNSW,butalsoforabroadfamilyofSWFsaslongastheyare
concaveandParetooptimal—anSWFf :[0,1]N P[0,1]isParetooptimaliffortwoutilityvectors
→
8xandysuchthatx y foralli [N],wehavef(x) f(y). NSWisclearlyinthisfamily,and
n n
≥ ∈ ≥
there are many other standard fairness measuresthat fall into this class; see AppendixB.2.1. For
anySWFinthisfamily,weprovethefollowingregretbound,whichremarkablyhasnodependence
onthenumberofagentsN atall.
Theorem 4.2. For any f : [0,1]N [0,1] that is concave and Pareto optimal, Algorithm 2
→
with the log-barrierregularizer ψ(p) = K logp and η = KlogT guaranteesReg =
− i=1 i T adv
(√KTlogT). q
O P
ProofSketch. Usingtheconcreteformofψ,itisclearthatthelocalnorm f(u⊤p ) 2 sim-
k∇ t t k∇−2ψ(pt)
plifiesto K p2 [ f(u⊤p )]2 p , f(u⊤p ) 2 ,wheretheinequalityisdueto[ f(u⊤p )]
i=1 t,i ∇ t t i ≤ t ∇ t t ∇ t t i ≥
0 implied by Pareto optimality. Furthermore, by concavity, we have p , f(u⊤p ) f(p )
P (cid:10) (cid:11) t ∇ t t ≤ t −
f(0) 1,andthusthelocalnormatmost1. Therestoftheproofisbydirectcalculation.
≤ (cid:10) (cid:11)
4.2.2 FTRLwithTsallisEntropyRegularizer
In fact, when f = NSW, using the special structure of the welfare function, we find yet another
regularizerthatensuresasmall (N)localnorm,withthebenefitofhavingsmallerdependenceon
O
K forthepenaltyterm.
Theorem4.3. Forf = NSW,Algorithm2withtheTsallisentropyregularizerψ(p) =
1−PK i=1pβ
i ,
1−β
β = N2,andtheoptimalchoiceofηguaranteesReg
adv
= O(K21− N1√NT).
The proof is more involved and is deferred to Appendix B.2.3. While the regret in Theorem 4.3
e
sufferspolynomialdependenceonN,ithasbetterdependenceonK comparedtoTheorem4.2,and
isthusmorepreferablewhenK ismuchlargerthanN.
4.2.3 LogarithmicRegretforaSpecialCase
Finally,wediscussaspecialcasewithf =NSWwherelogarithmicregretispossible.Thisisbased
on a simple observationthatwhen there is oneagentwho is indifferentaboutthe learner’schoice
(thatis,theagent’sutilityisthesameforallarmsforthisround),then NSWisnotonlyconvex,but
−
alsoexp-concave,astrongercurvatureproperty. Therefore,byapplyingknownresults,specifically
theEWOOalgorithm[Hazanetal.,2007],weachievethefollowingresult.
Theorem4.4. Fixf = NSW. Supposethatforeachtimet,thereisasetofagentsA [N]such
t
that A M andu =c 1withc 0foreachagentn A . ThentheEWOO⊆ algorithm
t t,:,n t,n t,n t
| |≥ ≥ ∈
guaranteesReg = N−M KlogT .
adv O M ·
The proof, which verifie(cid:0)s the exp-concav(cid:1)ity of NSW in this special case, can be found in Ap-
−
pendixB.2.4.WenotethatthereasonthatweapplyEWOOinsteadofOnlineNewtonStep,another
algorithm discussed in [Hazan et al., 2007] for exp-concavelosses, is that the latter requires Lip-
schizness(which,again,isnotsatisfiedbyNSW).
5 Conclusion
Inthiswork,motivatedbyrecentresearchonsocialwelfaremaximizationfortheproblemofmulti-
agent multi-armed bandits, we consider a variant with the arguablymore natural version of Nash
social welfare as the objective function, and develop multiple algorithms and regret upper/lower
boundsin differentsettings (stochastic versus adversarialand full-informationversus banditfeed-
back). Our results show a sharp separation between our problemand previoussettings, including
theheavilystudiedBanditConvexOptimizationproblem.
There are many interesting future directions. First, in the stochastic bandit setting, we have only
shownthetightdependenceonT,sowhataboutK andN? Second,isthereamoregeneralstrate-
gy/analysisthatworksfordifferentsocialwelfarefunctions(similartoourresultinTheorem4.2)?
Takingonestepfurther,similartotherecentresearchon“omniprediction”[Gopalanetal.,2022],is
thereonesinglealgorithmthatworksforaclassofsocialwelfarefunctionssimultaneously?
9References
JacobDAbernethy,ChansooLee,andAmbujTewari. Fightingbanditswithanewkindofsmooth-
ness. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in
NeuralInformationProcessingSystems,volume28.CurranAssociates,Inc.,2015.
Alekh Agarwal, Haipeng Luo, Behnam Neyshabur, and Robert E Schapire. Corralling a band of
banditalgorithms. InConferenceonLearningTheory,pages12–38.PMLR,2017.
Jean-YvesAudibertandSébastienBubeck. Regretboundsandminimaxpoliciesunderpartialmon-
itoring. TheJournalofMachineLearningResearch,11:2785–2836,2010.
Peter Auer, Nicolo Cesa-Bianchi, and PaulFischer. Finite-time analysisofthe multiarmedbandit
problem. Machinelearning,47:235–256,2002a.
Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-
armedbanditproblem. SIAMjournaloncomputing,32(1):48–77,2002b.
Siddharth Barman, Sanath Kumar Krishnamurthy, and Rohit Vaish. Finding fair and efficient al-
locations. InProceedingsofthe2018ACMConferenceonEconomicsandComputation,pages
557–574,2018.
SiddharthBarman,ArindamKhan,ArnabMaiti,andAyushSawarni. Fairnessandwelfarequantifi-
cationforregretinmulti-armedbandits.AAAI’23.AAAIPress,2023.ISBN978-1-57735-880-0.
doi:10.1609/aaai.v37i6.25829.
AlinaBeygelzimer,JohnLangford,LihongLi,LevReyzin,andRobertSchapire. Contextualbandit
algorithmswithsupervisedlearningguarantees. InProceedingsofthe FourteenthInternational
Conference on Artificial Intelligence and Statistics, pages19–26.JMLR Workshop and Confer-
enceProceedings,2011.
RóbertBusa-Fekete, BalázsSzörényi,PaulWeng, andShie Mannor. Multi-objectivebandits: Op-
timizing the generalized gini index. In International Conference on Machine Learning, pages
625–634.PMLR,2017.
IoannisCaragiannis,DavidKurokawa,HervéMoulin,ArielD.Procaccia,NisargShah,andJunxing
Wang. Theunreasonablefairnessofmaximumnashwelfare. ACMTrans.Econ.Comput.,7(3),
sep2019. ISSN2167-8375. doi:10.1145/3355902.
Violet Xinying Chen and J.N. Hooker. A guide to formulatingfairness in an optimizationmodel.
AnnalsofOperationResearch,326:581–619,2023.
YifangChen,AlexCuellar,HaipengLuo,JigneshModi,HerambNemlekar,andStefanosNikolaidis.
Faircontextualmulti-armedbandits: Theoryandexperiments. InConferenceonUncertaintyin
ArtificialIntelligence,pages181–190.PMLR,2020.
RichardCole andVasilisGkatzelis. Approximatingthenash socialwelfare withindivisibleitems.
InProceedingsoftheforty-seventhannualACMsymposiumonTheoryofcomputing,pages371–
380,2015.
MadalinaMDruganandAnnNowe. Designingmulti-objectivemulti-armedbanditsalgorithms:A
study. InThe2013internationaljointconferenceonneuralnetworks(IJCNN),pages1–8.IEEE,
2013.
Dylan J Foster, Zhiyuan Li, Thodoris Lykouris, Karthik Sridharan, and Eva Tardos. Learning in
games:Robustnessoffastconvergence.AdvancesinNeuralInformationProcessingSystems,29,
2016.
YoavFreundandRobertESchapire. Adecision-theoreticgeneralizationofon-linelearningandan
applicationtoboosting. Journalofcomputerandsystemsciences,55(1):119–139,1997.
JugalGarg,PoojaKulkarni,andRuchaKulkarni.Approximatingnashsocialwelfareundersubmod-
ularvaluationsthrough(un)matchings. ACMTransactionsonAlgorithms,19(4):1–25,2023.
10Stephen Gillen, Christopher Jung, Michael Kearns, and Aaron Roth. Online learning with an un-
knownfairnessmetric. Advancesinneuralinformationprocessingsystems,31,2018.
Parikshit Gopalan, Adam Tauman Kalai, Omer Reingold, Vatsal Sharan, and Udi Wieder. Om-
nipredictors. In13thInnovationsinTheoreticalComputerScienceConference(ITCS2022),vol-
ume215,pages79:1–79:21,2022.
Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex
optimization. MachineLearning,69(2):169–192,2007.
Elad Hazan et al. Introductionto online convexoptimization. Foundationsand Trends® in Opti-
mization,2(3-4):157–325,2016.
SafwanHossain,EviMicha,andNisargShah. Fairalgorithmsformulti-agentmulti-armedbandits.
AdvancesinNeuralInformationProcessingSystems,34:24005–24017,2021.
Matthew Jones, HuyNguyen, and ThyNguyen. An efficientalgorithmfor fair multi-agentmulti-
armedbanditwithlowregret. InProceedingsoftheAAAIConferenceonArtificialIntelligence,
volume37,pages8159–8167,2023.
Matthew Joseph, Michael Kearns, Jamie H Morgenstern, and Aaron Roth. Fairness in learning:
Classic andcontextualbandits. InD.Lee,M.Sugiyama,U.Luxburg,I.Guyon,andR.Garnett,
editors,AdvancesinNeuralInformationProcessingSystems,volume29.CurranAssociates,Inc.,
2016.
Mamoru Kaneko and Kenjiro Nakamura. The nash social welfare function. Econometrica, 47:
423–435,1979.
RobertKleinberg,AleksandrsSlivkins,andEliUpfal.Banditsandexpertsinmetricspaces.Journal
oftheACM(JACM),66(4):1–77,2019.
TzeLeungLaiandHerbertRobbins. Asymptoticallyefficientadaptiveallocationrules. Advances
inappliedmathematics,6(1):4–22,1985.
TorLattimore. Banditconvexoptimisation. arXivpreprintarXiv:2402.06535,2024.
TorLattimoreandCsabaSzepesvári. Banditalgorithms. CambridgeUniversityPress,2020.
Chung-WeiLee,HaipengLuo,Chen-YuWei,andMengxiaoZhang.Biasnomore:high-probability
data-dependentregretboundsforadversarialbanditsandmdps. Advancesinneuralinformation
processingsystems,33:15522–15533,2020.
Wenzheng Li and Jan Vondrák. Estimating the nash social welfare for coverage and other sub-
modularvaluations. InProceedingsofthe2021ACM-SIAMSymposiumonDiscreteAlgorithms
(SODA),pages1119–1130.SIAM,2021.
YangLiu,GoranRadanovic,ChristosDimitrakakis,DebmalyaMandal,andDavidCParkes. Cali-
bratedfairnessinbandits. arXivpreprintarXiv:1707.01875,2017.
Haipeng Luo. Lecture note 13, Introduction to Online Learning. 2017. URL
https://haipeng-luo.net/courses/CSCI699/lecture13.pdf.
HervéMoulin. Fairdivisionandcollectivewelfare. MITpress,2004.
JohnFNashetal. Thebargainingproblem. Econometrica,18(2):155–162,1950.
AyushSawarni,SoumyabrataPal,andSiddharthBarman. Nashregretguaranteesforlinearbandits.
AdvancesinNeuralInformationProcessingSystems,36,2024.
WilliamRThompson. Onthelikelihoodthatoneunknownprobabilityexceedsanotherinviewof
theevidenceoftwosamples. Biometrika,25(3-4):285–294,1933.
Chen-YuWei andHaipengLuo. Moreadaptivealgorithmsforadversarialbandits. InConference
OnLearningTheory,pages1263–1291.PMLR,2018.
11Julian Zimmertand YevgenySeldin. An optimalalgorithmfor stochastic and adversarialbandits.
In The 22nd International Conference on Artificial Intelligence and Statistics, pages 467–475.
PMLR,2019.
Martin Zinkevich. Online convexprogrammingand generalized infinitesimal gradient ascent. In
Proceedingsofthe20thinternationalconferenceonmachinelearning(icml-03),pages928–936,
2003.
12A OmittedDetailsinSection 3
In this section, we provide the omitted proofs for the results in Section 3. In Appendix A.1, we
providetheproofforTheorem3.1andinAppendixA.2,weprovidetheproofforTheorem3.2.
A.1 ProofofTheorem3.1
ToproveTheorem3.1,wefirstconsiderthefollowingtwoevents.
Event1. Forallt KN +1,...,T andi [K],
0
∈{ } ∈
t
1
N N + p 18log(KT),
t,i 0 τ,i
≥ 2 −
τ= XKN0
whereN ’sandp ’saredefinedinAlgorithm1.
t,i t,i
Event2. Forallt KN +1,...,T ,i [K],andn [N],
0
∈{ } ∈ ∈
u log(NKT2) 15log(NKT2)
i,n
u u u +8 + ,
i,n t,i,n i,n
≤ ≤ s N t,i N t,i
whereN ’saredefinedbinAlgorithm1.
t,i
As we prove in Lemma A.1 and Lemma A.2, Event 1 and Event 2 hold with probability at least
1 1. NowweproveTheorem3.1.Forconvenience,werestatethetheoremasfollows.
− T
Theorem3.1. WithN
0
=1+18logKT,Algorithm1guaranteesE[Reg sto]= O(KN2 TN N−1 +K).
Proof. Letp⋆ =argmax NSW(u⊤p).AccordingtoastandardregretdecomepositionforUCB-
p∈∆K
typealgorithms,weknowthatReg canbeupperboundedasfollows:
sto
E[Reg ]
sto
T
=E NSW(u⊤p⋆) NSW(u⊤p )
t
" − #
t=1
X(cid:0) (cid:1)
T
=E NSW(u⊤p⋆) NSW(u⊤p ) Event1andEvent2hold +2
t
" − (cid:12) #
Xt=1
(cid:0) (cid:1)
(cid:12)
(cid:12) (accordingtoLemmaA.1andLemmaA.2)
(cid:12)
(cid:12)
T
E NSW(u⊤p⋆) NSW(u⊤p⋆) Event1andEvent2hold +KN +2
≤ " − t (cid:12) # 0
t=K XN0+1
(cid:0) (cid:1)
(cid:12)
(cid:12)
T b (cid:12)
+E NSW(u⊤p⋆) NSW(u⊤p(cid:12)) Event1andEvent2hold
" t − t t (cid:12) #
t=K XN0+1
(cid:0) (cid:1)
(cid:12)
(cid:12)
T b b (cid:12)
+E NSW(u⊤p ) NSW(u⊤p ) (cid:12)Event1andEvent2hold
" t t − t (cid:12) #
t=K XN0+1
(cid:0) (cid:1)
(cid:12)
(cid:12)
T b T(cid:12)
E NSW(u⊤p⋆) NSW(u⊤p ) +E (cid:12) NSW(u⊤p ) NSW(u⊤p ) +KN +2
≤ " t − t t # " t t − t # 0
t=1 t=1
X(cid:0) (cid:1) X(cid:0) (ba(cid:1)sedonEvent2)
b b b
T
E NSW(u⊤p ) NSW(u⊤p ) Event1andEvent2hold +KN +2.
≤ " t t − t (cid:12) # 0
t=K XN0+1
(cid:0) (cid:1)
(cid:12)
(cid:12) (basedonthedefinitionofp )
b (cid:12) t
(cid:12)
13Inthefollowing,weboundthefirstterm
T
E NSW(u⊤p ) NSW(u⊤p ) Event1andEvent2hold .
" t t − t #
t=K XN0+1
(cid:0) (cid:1)
(cid:12)
(cid:12)
AsdiscussedinSection3.1,wecobnsidertwocases. First,(cid:12)considerthesetofrounds suchthatfor
σ
T
allt thereexistsatleastonen [N]suchthat p ,u σ forsomeσthatwewillspecify
σ t :,n
∈ T ∈ h i ≤
later. Denotesuchntoben (iftherearemultiplesuchn’s,wepickanarbitraryone).Accordingto
t
Event2,weknowthatforalli [K],
∈
u log(NKT2) 15log(NKT2) log(NKT2)
u u u +8
i,nt
+ 2u + ,
i,nt
≤
t,i,nt
≤
i,nt
s N t,i N t,i ≤
i,nt
O (cid:18) N t,i (cid:19)
wherethbelastinequalityisbecauseofAM-GMinequality.Therefore,weknowthat
K K
p p
t,j t,j
p ,u 2 p ,u + 2σ+ .
h
t t,:,nti≤
h
t :,nti
O N ≤ O N 
t,j t,j
j=1 j=1
X X
b e  e 
Nowconsider NSW(u⊤p ) NSW(u⊤p ) . Directcalculationshowsthat
t∈Tσ t t − t
P NS(cid:0) W(u⊤p ) NSW(u⊤p ) (cid:1)
t bt − t
t X∈Tσ(cid:0)
(cid:1)
NSWb(u⊤p ) (sinceNSW(u⊤p ) 0)
≤ t t t ≥
t X∈Tσ
≤
hp t,u t,b :,ntiN1
t X∈Tσ
b
T K
N1
p
≤2
|Tσ
|·σN1
+
O
Nt,j


(since(a+b)N1 ≤aN1 +bN1)
t,j
t=1 j=1
X X
e
 

 1
T K N
≤2 |T
σ
|·σN1 +TN N−1
O
Np t,j


(Hölder’sinequality)
t,j
t=1j=1
XX
1 1
e
N −1 


2T σN + KN T N . (usingLemmaA.3)
≤ · O ·
(cid:16) (cid:17)
e
Nowconsidertheregretwithint NK +1,...,T ,inwhichwehave p ,u σforall
0 σ t :,n
∈{ }\T h i≥
n [N]. Inthiscase,webound NSW(u⊤p ) NSW(u⊤p ) asfollows:
∈ t∈/Tσ t t − t
P (cid:0) (cid:1)
b
NSW(u⊤p ) NSW(u⊤p )
t t − t
tX∈/Tσ(cid:0)
(cid:1)
b p t,u
t,:,n
N1 p t,u
:,n
N1 (usingLemmaC.1andEvent2)
≤ h i −h i
tX∈/TσnX∈[N]h i
b p ,u u
t t,:,n :,n
= h − i
tX∈/TσnX∈[N]
N k=− 01 hp t,u
t b,:,n
iNk hp t,u
:,n
iN− N1−k
p ,u u
P h t t,:,n − :,n i (sinceu u foralli,t,nbasedonEvent2)
≤ bN−1 t,i,n ≥ i,n
tX∈/TσnX∈[N] N hp bt,u :,n
i
N
K p 8 un,jlog(NKT2) +b8log(NKT2)
j=1 t,j Nt,j Nt,j
(usingEvent2)
≤ tX∈/TσnX∈[N]P (cid:16) q N hp t,u
:,n
iN N−1 (cid:17)
14K 8 pt,jlog(NKT2) K 8pt,jlog(NKT2)
j=1 Nt,j
+
j=1 Nt,j
≤ tX∈/TσnX∈[N]PN
hp
tq
,u
:,n
iN N−1− 21
PN hp t,u
:,n
iN N−1 
 (since p t,u
:,n
√p t,iu i,nforalli [K])
h i≥ ∈
1 K p 1 p T K p
t,j t,j
+
≤ONσN N−1−1
2
Xt∈TnX∈[N]Xj=1rN
t,j
NσN N−1
Xt=1nX∈[N]Xj=1
N t,j
e σ1 2−N N−1 K√T +K σ−N N−1 , 
≤O ·
(cid:16) (cid:17)
wherethelastinequalityisbecauseLemmaA.3.Combiningtheregretforbothparts,weknowthat
e
E[Reg ] KN1 TN N−1 +T σN1 +σ1 2−N N−1 K√T +Kσ−N N−1 +K .
sto ≤O ·
(cid:16) (cid:17)
Pickingtheoptimalσ leaedstothe expectedregretboundedbyE[Reg sto]
≤ O
KN2 TN N−1 +K .
(cid:16) (cid:17)
e
LemmaA.1. Event1happenswithprobabilityatleast1 1.
− T
Proof. According to Algorithm 1, we know that N = N for each i [K]. Consider
(KN0+1),i 0
∈
the case when t KN +1. According to Freedman’s inequality (Lemma C.3), we have with
0
≥
probabilityatleast1 δ,forafixedt KN +1,
0
− ≥
t t t
i =i p 2 p log(1/δ) log(1/δ)
{ τ }≥ τ,i − v τ,i −
τ=K XN0+1 τ=K XN0+1 u uτ= XKN0
t t
1
p 9log(1/δ).
τ,i
≥ 2 −
τ=K XN0+1
Therefore,weknowthatwithprobabilityatleast1 δ,forafixedt KN +1,
0
− ≥
t t
1
N =N + i =i N + p 9log(1/δ).
t,i 0 τ 0 τ,i
{ }≥ 2 −
τ=K XN0+1 τ=K XN0+1
Picking δ = 1 and taking a union bound over all i [K] and KN +1 t T reach the
KT2 ∈ 0 ≤ ≤
result.
LemmaA.2. Event2happenswithprobabilityatleast1 1.
− T
Proof. According to Freedman’s inequality Lemma C.3, applying a union bound over t [T],
∈
i [K], andn [N], we knowthatwithprobabilityatleast1 δ, forallt [T], i [K], and
∈ ∈ − ∈ ∈
n [N],
∈
u log(NKT/δ) log(NKT/δ)
u¯ u 2 i,n + . (10)
t,i,n i,n
| − |≤ s N t,i N t,i
Solvingtheinequalitywithrespecttou ,weknowthat
i,n
log(NKT/δ) 2log(NKT/δ)
√u
i,n
+ u¯ t,i,n+
≤s N t,i s N t,i
6log(NKT/δ)
2u¯ + , (usingAM-GMinequality)
t,i,n
≤s N
t,i
2
log(NKT/δ)
u¯
t,i,n
√u i,n+
≤ s N t,i !
15
1
12log(NKT/δ)
2u + . (usingAM-GMinequality)
i,n
≤ N
t,i
Usingtheaboveinequalityandpickingδ = 1,wecanlowerboundu asfollows:
T t,i,n
u¯ log(NKT2) 8log(NKT2)
t,i,n
u =u¯ +4 + b
t,i,n t,i,n
s N t,i N t,i
b log(NKT2) u 3log(NKT2) 8log(NKT2)
i,n
u¯ +4 +
t,i,n
≥ s N t,i (cid:18) 2 − N t,i (cid:19) N t,i
u log(NKT2) (8 4√3)log(NKT2)
i,n
u¯ +4 + −
t,i,n
≥ s 2N t,i N t,i
(using√a b √a √bfora,b 0)
− ≥ − ≥
u log(NKT2) log(NKT2)
i,n
u¯ +2 +
t,i,n
≥ s N t,i N t,i
u ,
i,n
≥
wherethelastinequalityusesEq. (10).Toupperboundu ,wehave
t,i,n
u¯ log(NKT2) 8log(NKT2)
t,i,n
u t,i,n =u¯ t,i,n+4 + b
s N t,i N t,i
b u log(NKT2) log(NKT2)
u +2 i,n + (usingEq. (10))
i,n
≤ s N t,i N t,i
log(NKT2) 2log(NKT2) 8log(NKT2)
+4 2u + +
i,n
s N t,i (cid:18) N t,i (cid:19) N t,i
u log(NKT2) 15log(NKT2)
u +8 i,n + . (usingAM-GMinequality)
i,n
≤ s N t,i N t,i
Combiningthelowerandtheupperboundfinishestheproof.
LemmaA.3. UnderEvent1,Algorithm1guaranteesthat
t
p
τ,i
(logT),
N ≤O
τ,i
τ=K XN0+1
t
p
τ,i
T logT ,
N ≤O
τ,i
τ=K XN0+1r (cid:16)p (cid:17)
foralli [K]andKN +1 t T.
0
∈ ≤ ≤
Proof. SinceEvent1holds,weknowthatN 1 t p +1holdsforalli [K]and
t,i ≥ 2 τ=KN0+1 τ,i ∈
τ KN +1 based on the choice of N = 18logKT +1. Therefore, we know that for each
0 0
t ≥ KN +1, P
0
≥
t t
p 2p
τ,i τ,i
τ=K
XN0+1N
τ,i
≤
τ=K XN0+1
τ τ′=KN0+1p τ′,i+2
Pt τ=KNP 0+1pτ,i 1
2 dx 2log(T +2).
≤ x+2 ≤
Z0
Asfortheterm t pτ,i,wehave
τ=KN0+1 Nτ,i
P q
t t
p p
τ,i (t KN ) τ,i (Cauchy-Schwarzinequality)
N ≤v − 0 N
τ=K XN0+1r τ,i u
u
τ=K XN0+1 τ,i
t
16( T logT).
≤O
p
A.2 OmittedDetailsinSection3.2
Theorem3.2. Inthebanditfeedbacksetting,foranyalgorithm,thereexistsastochasticenvironment
inwhichtheexpectedregret(withrespecttoNSW)ofthisalgorithmisΩ (lo NgK 3)3 ·KN1 TN N−1 for
N logK andsufficientlylargeT. (cid:16) (cid:17)
≥
Proof. Consider the environment that picks u uniformly from u(1),...,u(K) , where u(i) =
E { } :,1
ε e RK andu(i) =1forallj 2,3,...,N .Here,ε (0,1]issomeconstanttobespecified
· i ∈ :,j ∈{ } ∈ 9
later. Denoteu(0) tobetheenvironmentwhereu = 0foralln [N]. Ateachroundt,u is
:,n t,i,n
ani.i.d. Bernoullirandomvariablewithmeanu . Fornotational∈ convenience,weuseE []when
i,n i
·
wetakeexpectationovertheenvironmentu(i) fori 0 [K]. Letn bethenumberofrounds
i
∈ { }∪
thatactioniisselectedoverthetotalhorizonT foralli [K]. Therefore,theexpectedregretwith
∈
respecttoenvironment (auniformdistributionoveru(i),i [K])islowerboundedasfollows:
E ∈
K T
E E[Reg]= K1 E
i
"εN1 (1 −p tN1 ,i)
#
i=1 t=1
X X
1
TεN1
εN1TN N−1 K
E
i
T
p
t,i
N
(Hölder’sinequality)
≥ − K  ! 
i=1 t=1
X X
 1
TεN1
εN1TN N−1 K
E
i
T
p
t,i
N
(Jensen’sinequality)
≥ − K " #!
i=1 t=1
X X
1
K T N
TεN1 K− N1 εN1 TN N−1 E
i
p
t,i
, (11)
≥ − " #!
i=1 t=1
X X
wherethelastinequalityisagainduetoHölder’sinequality.LetBer(α)betheBernoullidistribution
with meanα. CombiningExercise14.4of[Lattimoreand Szepesvári,2020],Pinsker’sinequality,
andLemma15.1of[LattimoreandSzepesvári,2020],wehave
T
1
E p =E [n ] E [n ]+T E [n ]KL(Ber(0)Ber(ε))
i t,i i i 0 i 0 i
"
t=1
# ≤ r2 |
X
1 ε
E [n ]+T E [n ]log 1+
0 i 0 i
≤ s2 1 ε
(cid:18) − (cid:19)
ε
E [n ]+T E [n ] (usinglog(1+x) x)
0 i 0 i
≤ 2(1 ε) ≤
r −
3T
E [n ]+ E [n ]ε
0 i 0 i
≤ 4
p
T T
3T
=E p + E p ε. (12)
0 t,i 4 v 0 t,i
" t=1 # u " t=1 #
X u X
t
wherethelastinequalityisbecauseε 1.
≤ 9
Takingsummationoveralli [K],weobtainthat
∈
T
E p
i t,i
" #
iX∈[K] Xt=1
17T K T
3
E p + T E p ε
≤ iX∈[K] 0 " Xt=1 t,i # 4 Xi=1v u
u
0 " Xt=1 t,i #
t
K T
3T
T + KεE p
≤ 4 v u 0 " i=1 t=1 t,i #
u XX
3Tt
=T + √KTε (13)
4
wherethesecondinequalityisduetoCauchy-Schwarzinequality.
ApplyingEq. (13)toEq.(11),weobtainthat
E [Reg]
E
1
TεN1 K− N1 εN1 TN N−1 T + 3T √KTε N
≥ − 4
(cid:18) (cid:19)
1 K− N1 TεN1 K− 21 Nε23 NT2N 2N+1 (using(a+b)N1 aN1 +bN1)
≥ − − ≤
(cid:16)logK
TεN1
(cid:17)
K− 21 Nε23 NT2N 2N+1 ,
≥ 2N −
where the third inequality is according to Lemma C.2 with x = 1 and α = 1, meaning that
N K
1
N 1 1 N logK.
− K ≥ 2
(cid:16) (cid:17)
Pickingε=
(logK)2N·K,weknowthat
(4N)2NT
K− 21 Nε23 NT2N 2N+1 = εN1T logK =Ω (logK)3 KN1 TN N−1 ,
4N N3 ·
(cid:18) (cid:19)
Combiningtheabovealltogether,we knowthatE E[Reg]
≥
Ω (lo NgK 3)3 ·KN1TN N−1 . Therefore,
there exists one environmentamong u(i),i
∈
[K] such that E i(cid:16) [Reg]
≥
Ω (lo NgK 3)3 ·(cid:17) KN1TN N−1 ,
whichfinishestheproof. (cid:16) (cid:17)
B OmittedDetailsinSection 4
B.1 OmittedDetailsinSection4.1
In this section, we prove that that in the adversarial environment, it is also impossible to achieve
sublinear regretwhen f = NSW . The hardinstance constructionshares a similar spirit to the
prod
oneforf =NSWshowninTheorem4.1.
TheoremB.1. Inthebanditfeedbacksetting,foranyalgorithm,thereexistsanadversarialenviron-
mentsuchthatE[Reg ]=Ω(T)forf =NSW .
adv prod
Proof. Weconsiderthelearningenvironmentwithtwoagentsandtwoarms.Theagentsutilitiesare
binary,meaningthatu 0,1 2×2. Weconstructtwodistributions and ′withsupport 0,1 2×2.
∈{ } E E { }
To define environment , we use q for any w,x,y,z 0,1 to denote the probability that
wxyz
E ∈ { }
u = (w,x)andu = (y,z)whenu . Forsimplicityofnotation,thebinarynumberwxyz
1,: 2,:
∼ E
willbewrittenindecimalform(i.e. q = Pr [u = (1,0),u = (0,0)]). Forenvironment ,
8 u∼E 1,: 2,:
E
weassigntheprobabilities
1 1 1
q = fori 0,...,15 0,2,4,6 (q ,q ,q ,q )= ,0,0,
i 0 2 4 6
16 ∈{ }\{ } 8 8
(cid:18) (cid:19)
Similarly,forenvironment ′,weuseq′ foranyw,x,z,y 0,1 todenotetheprobabilitythat
E wxyz ∈{ }
u′ = (w,x) andu′ = (y,z) when u′ ′. Again, we will write the binarynumberwxyz in
1,: 2,: ∼ E
decimalformforeaseofnotation.Toenvironment ′weassignprobabilities
E
1 1 1
q′ = fori 0,...,15 1,3,5,7 (q′,q′,q′,q′)= 0, , ,0
i 16 ∈{ }\{ } 1 3 5 7 8 8
(cid:18) (cid:19)
18Next, we arguethatthe learner’sobservationsare equivalentin distributionin and ′, since the
E E
marginaldistributionofeverypossibleobservationofeachactionisthesame. Specifically,
• Whenaction1isplayed,thelearner’sobservationu inboth and ′aregivenbythefollowing
1,:
E E
marginaldistribution.
– Theprobabilityofobservation(0,0)isq +q +q +q =q′ +q′ +q′ +q′ = 1
0 1 2 3 0 1 2 3 4
– Theprobabilityofobservation(0,1)isq +q +q +q =q′ +q′ +q′ +q′ = 1
4 5 6 7 4 5 6 7 4
– Theprobabilityofobservation(1,0)isq +q +q +q =q′ +q′ +q′ +q′ = 1
8 9 10 11 8 9 10 11 4
– Theprobabilityofobservation(1,1)isq +q +q +q =q′ +q′ +q′ +q′ = 1
12 13 14 15 12 13 14 15 4
• Whenaction2isplayed,thelearner’sobservationu inboth and ′aregivenbythefollowing
2,:
E E
marginaldistribution.
– Theprobabilityofobservation(0,0)isq +q +q +q =q′ +q′ +q′ +q′ = 1
0 4 8 12 0 4 8 12 4
– Theprobabilityofobservation(0,1)isq +q +q +q =q′ +q′ +q′ +q′ = 1
1 5 9 13 1 5 9 13 4
– Theprobabilityofobservation(1,0)isq +q +q +q =q′ +q′ +q′ +q′ = 1
2 6 10 14 2 6 10 14 4
– Theprobabilityofobservation(1,1)isq +q +q +q =q′ +q′ +q′ +q′ = 1
3 7 11 15 3 7 11 15 4
Directcalculationshowsthat
E NSW (u⊤p)
u∼E prod
=(q 5(cid:2) −q 6 −q 9+q 10(cid:3))p2 1+(q 6 −q 7 −2q 5+q 9+q 11 −q 13+q 14)p 1+(q 5+q 7+q 13+q 15)
1 1 1
= p2+ p + ,
−16 1 16 1 4
E u∼E′ NSW prod(u⊤p)
=(q 5′ (cid:2) −q 6′ −q 9′ +q 1′ 0(cid:3))p2 1+(q 6′ −q 7′ −2q 5′ +q 9′ +q 1′ 1−q 1′ 3+q 1′ 4)p 1+(q 5′ +q 7′ +q 1′ 3+q 1′ 5)
1 1 1
= p2 p + .
16 1− 16 1 4
Therefore,wecomputethelearner’sbeststrategyforenvironments and ′bydirectcalculation:
E E
1 1 1 1 1
p =argmaxE NSW (u⊤p) =argmax + p p2 = ,
⋆ u∼E prod 4 16 1 − 16 1 2 2
p∈∆2 p∈∆2 (cid:20) (cid:21) (cid:18) (cid:19)
(cid:2) (cid:3)
1 1 1
p′ ⋆ =argmaxE u∼E′ NSW prod(u⊤p) =argmax 4 − 16p 1+ 16p2 1 = {(0,1),(1,0) }
p∈∆2 p∈∆2 (cid:20) (cid:21)
(cid:2) (cid:3)
Next,consideradistributionp ∆ suchthatp 0,1 3,1 . Forsuchp,directcalculation
∈ 2 1 ∈ 4 ∪ 4
showsthatE NSW (u⊤p ) NSW (u⊤p) ∆,where∆= 1 . Ontheotherhand,for
u∼E prod ⋆ − prod (cid:2)≥ (cid:3) (cid:2) (cid:3) 256
astrategyp ∈ ∆ (cid:2)2 withp 1 ∈ 41,3 4 , wehaveE u∼E′ (cid:3) NSW prod(u⊤p′ ⋆) −NSW prod(u⊤p) > ∆as
well. Givenanyalgorithm,letα betheprobabilitythatthenumberofroundsp 0,1 3,1
(cid:0)E (cid:1) (cid:2) t,1 ∈ 4(cid:3)∪ 4
is largerthan T
2
underenvironment E. Letα¯ E′ bethe probabilityof thecomplemen (cid:2)toft (cid:3)his (cid:2)even (cid:3)t
underenvironment ′. Bydefinition
E
T T
α T∆
E [Reg ] E NSW (u⊤p ) NSW (u⊤p ) E
E adv ≥ E " prod t ⋆ − prod t t #≥ 2
t=1 t=1
X X
T T
E E′[Reg adv] ≥E E′ " NSW prod(u⊤ t p′ ⋆) − NSW prod(u⊤ t p t) #≥ α¯ E′ 2T∆
t=1 t=1
X X
Sincethefeedbackforthealgorithmisthesameindistribution,wehaveα E +α¯ E′ = 1. Thus,we
have
max {E E[Reg adv],E E′[Reg adv]
}≥
E E[Reg adv]+ 2E E′[Reg adv]
≥
(α
E
+α 4¯ E′)T∆
=Ω(T).
19B.2 OmittedDetailsinSection4.2
B.2.1 ConcaveandParetoOptimalSWFs
Formally,forafunctionf :[0,1]N [0,1],concavityandParetoOptimalityaredefinedas:
7→
• Concavity:f(αx+(1 α)y) αf(x)+(1 α)f(y)foranyα [0,1]andx,y [0,1]N.
− ≤ − ∈ ∈
• Paretooptimality:foranyx,y [0,1]N,x y foralln [N]impliesf(x) f(y).
n n
∈ ≥ ∈ ≥
Paretooptimalityisa“fundamentalproperty”insocialchoicetheorybecauseitensuresthataSWF
prefers alternatives that strictly more efficient: everyone is no worse off [Kaneko and Nakamura,
1979]. Concavityappearslessinsocialchoiceliterature. However,itpromotesequitybymodeling
diminishinglevelsofdesirabilitywiththeincreaseofasingleagent’sutility.
Inthefollowing,weprovideexamplesofSWFsthatsatisfyconcavityandParetooptimality. Each
ofthefollowingSWFsareparameterizedbyfixedweightsw ∆ .
K
∈
• UtilitarianSWF:f(u)= w,u ;
h i
• G ove en re Nral ii tz ee md sG ai nn di I wnde ix sw(G eiG gI h) t: sf w(u p) er= mum tei dn aπ c∈ cS oN rdh iw ngπ, tu oi π, wh Sere ;S N is the set of permutations
π N
∈
• WeightedNSW:f(u)= uwn.
n∈N n
ThelastnotablefactaboutQtheclassofconcaveandParetoOptimalSWFsisthatitisclosedunder
convexcombinations. Specifically,fortwoconcaveandParetoOptimalfunctionsf,g : [0,1]N
→
[0,1], the function h() = λf()+(1 λ)g() for any λ [0,1] is concaveand Pareto Optimal.
· · − · ∈
[ChenandHooker,2023]discusseshowsuchconvexcombinationscanbeusedtocombineaSWF
prioritizing efficiency and another prioritizing equity to derive a different SWF that prioritizes a
balancebetweenefficiencyandequity.
B.2.2 OmittedDetailsinSection4.2.1
Inthissection,weproveTheorem4.2,whichshowsthat (√KTlogT)regretisachievableforall
O
concaveandParetooptimalSWFs.
Theorem 4.2. For any f : [0,1]N [0,1] that is concave and Pareto optimal, Algorithm 2
→
with the log-barrierregularizer ψ(p) = K logp and η = KlogT guaranteesReg =
− i=1 i T adv
(√KTlogT). q
O P
Proof. Usingtheconcavityoff,wecanupperboundReg asfollows:
adv
T T
Reg = max f(u⊤p) f(u⊤p )
adv p∈∆K t − t t
t=1 t=1
X X
T T T
max f(u⊤p ),p p + max f(u⊤p) max f(u⊤p),
≤p∈∆ K,K1
T
Xt=1h−∇ t t t − i p∈∆K
Xt=1
t −p∈∆ K,K1
T Xt=1
t
Term(1) Term(2)
where∆ = p ∆ p 1 , i [K] .
K, K1 T| { ∈ K | i{z ≥ KT ∀ ∈ } } | {z }
ToboundTerm(1),accordingtoastandardanalysisofFTRL/OMDwithlog-barrierregularizer(e.g.
Lemma12of[Agarwaletal.,2017]),weknowthat:
T K
Term(1) max D ψ(p,p 1) +η p2 f(u⊤p ) 2 , (14)
≤p∈∆
K,K1
T
η
Xt=1 Xi=1
t,i· (cid:2)∇ t t (cid:3)i
whereD (p,q),ψ(p) ψ(q) ψ(q),p q istheBregmandivergencebetweenpandqwith
ψ
respecttoψ. Tofurther− boundth− eh r∇ ight-hand− sidei ,notethatp = 1 1. Directcalculationshows
1 K ·
thatforanyp ∆ ,
∈ K, K1 T
K
p p
i i
D (p,p )= 1 log
ψ 1
p − − p
i=1(cid:18) 1,i (cid:18) 1,i(cid:19)(cid:19)
X
20K
1
= log
Kp
i=1 (cid:18) i(cid:19)
X
1
Klog (sincep 1 foralli [K])
≤ K 1 i ≥ KT ∈
(cid:18) · KT (cid:19)
KlogT. (15)
≤
Usingthe Paretooptimalitypropertyoff andthepositivityofthe utilitymatrixu , we knowthat
t
[ f(u⊤p)] 0,meaningthat K p2 f(u⊤p ) 2 p , f(u⊤p ) 2 .
∇ t i ≥ i=1 t,i ∇ t t i ≤ t ∇ t t
Moreover,usingtheconcavitypPropertyof(cid:2)f,weknow(cid:3)that (cid:10)p , f(u⊤p ) (cid:11) f(u⊤p ) f(u⊤0)=
t ∇ t t ≤ t t − t
f(u⊤p ) 1. Combiningtheabovetwoinequalitiesmeansthat
t t ≤ (cid:10) (cid:11)
T K
p2 f(u⊤p ) 2 T. (16)
t,i ∇ t t i ≤
t=1i=1
XX (cid:2) (cid:3)
CombiningEq. (15)andEq. (16),wecanupperboundTerm(1)asfollows:
KlogT
Term(1) +ηT. (17)
≤ η
Denotetheoptimaldistributionp⋆ =argmax T f(u⊤p). Recallthatp = 1 1. Nowwe
upperboundTerm(2)asfollows: p∈∆K t=1 t 1 K ·
P
T T
Term(2)= f(u⊤p⋆) max f(u⊤p)
Xt=1
t −p∈∆
K,K1
T Xt=1
t
T T
1 1
f(u⊤p⋆) f u⊤ 1 p⋆+ p
≤ t − t − T T · 1
t=1 t=1 (cid:18) (cid:18)(cid:18) (cid:19) (cid:19)(cid:19)
X X ((1 1)p⋆+ 1p ∆ )
− T T 1 ∈ K, K1 T
T T
1 1
f(u⊤p⋆) 1 f(u⊤p⋆)+ f(u⊤p ) (Concavity)
≤ t − − T · t T · t 1
t=1 t=1(cid:20)(cid:18) (cid:19) (cid:21)
X X
T
1
f(u⊤p⋆) (sincef(u⊤p ) 0)
≤ T · t t 1 ≥
t=1
X
1. (18)
≤
CombiningEq. (17)andEq. (18),andchoosingη = KlogT finishestheproof.
T
q
B.2.3 OmittedDetailsinSection4.2.2
Inthissection,wepresenttheomittedproofforTheorem4.3,whichshowsabetterdependencyon
K comparedwithTheorem4.2.
Theorem4.3. Forf = NSW,Algorithm2withtheTsallisentropyregularizerψ(p) =
1−PK i=1pβ
i ,
1−β
β = N2,andtheoptimalchoiceofηguaranteesReg
adv
= O(K21− N1√NT).
Proof. ConsiderthecasewhenN 3. Directcalculationsehowsthat
≥
N
1 u
f(u⊤p) = i,n . (19)
∇ i N
n=1
p,u
:,n
1− N1
(cid:2) (cid:3) X h i
Using the concavity of f and a standard analysis of FTRL with Tsallis entropy (e.g., [Luo, 2017,
Theorem1]),weknowthat
T
Reg = f(u⊤p⋆) f(u⊤p )
adv t − t t
t=1
X(cid:0) (cid:1)
21f(u⊤p ),p⋆ p (concavityoff)
≤h∇ t t − t i
K1−β −1 + η T K p2−β f(u⊤p ) 2
≤ η(1 β) β t,i ∇ t t i
− t=1 i=1
XX (cid:2) (cid:3)
= K1−β −1 + η T K N p t1 ,− i β 2u t,i,n 2 (usingEq. (19))
η(1 −β) N2β
Xt=1
Xi=1
n
X=1( K j=1u
t,j,n
·p t,j)1− N1 
K1−β 1 η T K N Pp2−βu2 
− + t,i t,i,n (Cauchy-Schwarzinequality)
≤ η(1 −β) Nβ
Xt=1 Xi=1n
X=1( K j=1u
t,j,n
·p t,j)2− N2
K1−β 1 η T N K Pp2−βu2
− + i=1 t,i t,i,n (since( x )α xα forα 1)
≤ η(1 −β) Nβ Xt=1n X=1 PK j=1p t2 ,− jN2 u t2 ,− j,N n2 Pi i ≥ Pi i ≥
K1−β η T N PK p2−βu2− N2
+ i=1 t,i t,i,n . (sinceu [0,1])
≤ η(1 −β) Nβ Xt=1n X=1 PK j=1p t2 ,− jN2 u2 t,− j,N n2 t,i,n ∈
Pickingβ = 2,thefirsttermcanbeuPpperboundedby
3K1− N2
andthesecondtermcanbeupper
N η
boundedby ηT = ηNT. Furtherpickingtheoptimalchoiceofηfinishestheproof.
β 2
WhenN = 2andβ = 2 = 1,theregularizerψ(p) =
1−PK i=1pβ
i becomesthenegativeShannon
N 1−β
entropyψ(p)= K p logp .Usingtheconcavityoff andfollowingastandardanalysisofFTRL
i=1 i i
withShannonentropyregularizer(e.g.,[Hazanetal.,2016,Theorem5.2]),weobtainthat
P
T
Reg = f(u⊤p⋆) f (u⊤p )
adv t − t t t
t=1
X(cid:0) (cid:1)
T
f(u⊤p ),p⋆ p (usingtheconcavityoff)
≤ h∇ t t − t i
t=1
X
ψ(p⋆) −ψ(p 1) +2η T K p f(u⊤p ) 2 (by[Hazanetal.,2016,Theorem5.2])
≤ η t,i ∇ t t i
t=1 i=1
XX (cid:2) (cid:3)
T K 2
logK u u
t,i,1 t,i,2
= +2η p +
t,i
η Xt=1 Xi=1 2 hp t,u t,:,1 i 2 hp t,u t,:,2 i!
logK
+η
T K i=1p t,iup2
t,i,1 +
K i=1p t,ipu2
t,i,2 (AM-GMinequality)
≤ η t=1 PK i=1p t,iu t,i,1 PK i=1p t,iu t,i,2!
X
logK
+2ηT. P P (sinceu [0,1]forallt,i,n)
t,i,n
≤ η ∈
Pickingη = logK showsthatReg = √T logK = (√T)forN =2.
T adv O O
q
(cid:0) (cid:1)
B.2.4 OmittedDetailsinSection4.2.3 e
Inthissection,weshowtheproofforTheorem4.4,whichshowsthatlogarithmicregretisachievable
whenthereisatleastoneagentwhoisindifferentaboutthelearner’schoice.
Theorem4.4. Fixf = NSW. Supposethatforeachtimet,thereisasetofagentsA [N]such
t
that A M andu =c 1withc 0foreachagentn A . ThentheEWOO⊆ algorithm
t t,:,n t,n t,n t
| |≥ ≥ ∈
guaranteesReg = N−M KlogT .
adv O M ·
(cid:0) (cid:1)
Proof. To showthatEWOO algorithmachieveslogarithmicregret, we needto show thatf (p) ,
t
NSW(u⊤p)isα-exp-concaveforsomeα>0forallt [T],meaningthat
− t ∈
2f (p) α f (p) f (p)⊤ 0.
t t t
∇ − ∇ ∇ (cid:23)
22Let A [N] be the set of agents with u = c 1 for all n A on round t [T]. It is
t t,:,n t,n
⊆ · ∈ ∈
guaranteedthat A M forallt [T].DenoteB =[N] A . Directcalculationshowsthat
t t t
| |≥ ∈ \
1
f (p)=
Π m∈Atc tN
,m
f t(p)
u ,
t t,:,n
∇ N p,u
t,:,n
n X∈Bt h i
1
Π cN u f (p)⊤ p,u f (p)u u⊤
2f (p)= m∈At t,m t,:,n ∇ t h t,:,n i− t t,:,n t,:,n
∇ t N p,u 2
n X∈Bt h t,:,n i
2 ⊤
=
f t(p)Π m∈Atc tN
,m
u
t,:,n
u
t,:,n
N2 p,u p,u
t,:,n ! t,:,n !
n X∈Bt h i n X∈Bt h i
1
f (p)Π cN u u⊤
t m∈At t,m t,:,n t,:,n.
− N p,u 2
n X∈Bt h t,:,n i
1
Fornotationalconvenience,letλ =Π cN 1. Pickingα= M ,weknowthat
t m∈At t,m ≤ N−M
2f (p) α f (p) f (p)⊤
t t t
∇ − ∇ ∇
⊤
=
λ2 tf t(p) u
t,:,n
u
t,:,n
λ tf t(p) u t,:,nu⊤
t,:,n
N2 n X∈Bt hp,u t,:,n i! n X∈Bt hp,u t,:,n i! − N n X∈Bt hp,u t,:,n i2
⊤
αλ2f (p)2 u u
t t t,:,n t,:,n
− N2 p,u t,:,n ! p,u t,:,n !
n X∈Bt h i n X∈Bt h i
⊤
=
−λ tf t(p) u t,:,nu⊤
t,:,n
λ t(1 −αf t(p)) u
t,:,n
u
t,:,n
N  n X∈Bt hp,u t,:,n i2 − N n X∈Bt hp,u t,:,n i! n X∈Bt hp,u t,:,n i! 
 ⊤ 
λ tf t(p) u t,:,nu⊤
t,:,n
1 αf t(p) u
t,:,n
u
t,:,n
− −
(cid:23) N  n X∈Bt hp,u t,:,n i2 − N n X∈Bt hp,u t,:,n i! n X∈Bt hp,u t,:,n i! 
 ⊤ 
λ tf t(p) u t,:,nu⊤
t,:,n
1+α u
t,:,n
u
t,:,n
−
(cid:23) N  n X∈Bt hp,u t,:,n i2 − N n X∈Bt hp,u t,:,n i! n X∈Bt hp,u t,:,n i! 
 ⊤
λ tf t(p) u t,:,nu⊤
t,:,n
1 u
t,:,n
u
t,:,n
= −
N  n X∈Bt hp,u t,:,n i2 − N −M n X∈Bt hp,u t,:,n i! n X∈Bt hp,u t,:,n i! 
 ⊤ 
λ tf t(p) u t,:,nu⊤
t,:,n
1 u
t,:,n
u
t,:,n
−
(cid:23) N  n X∈Bt hp,u t,:,n i2 − |B t | n X∈Bt hp,u t,:,n i! n X∈Bt hp,u t,:,n i! 
0,  (Cauchy-Schwarzinequality)
(cid:23)
wherethefirstinequalityisbecauseλ 1andf (p) 0;thesecondinequalityisbecausef (p)
t t t
≤ ≤ ≥
1;thethirdinequalityisbecause B 1 . Thisshowsthatthef (p)is M -exp-concave.
− | t |≤ N−M t (N−M)
Therefore, accordingto Theorem4.4 of [Hazan et al., 2016], we know that the EWOO algorithm
guaranteesthat
T
N M 2(N M)
(f (p ) f (p⋆)) − KlogT + − .
t t t
− ≤ M · M
t=1 (cid:18) (cid:19)
X
23C AuxiliaryLemmas
Inthissection,weincludeseveralauxiliarylemmasthatareusefulintheanalysis.
Lemma C.1. Let a ,...,a ,b ,...b [0,1], where a b for all i [n]. Then, n a
n b n (a1 b ). n 1 n ∈ i ≥ i ∈ i=1 i −
i=1 i ≤ i=1 i − i Q
Q P
Proof. Directcalculationshowsthat
n n n j n j−1 n
a b = a b a b
i i i i i i
−  − 
i=1 i=1 j=1 i=1 i=j+1 i=1 i=j
Y Y X Y Y Y Y
 
n j−1 n
= (a b ) a b
j j i i
 − 
j=1 i=1 i=j+1
X Y Y
n  
(a b ).
j j
≤ −
j=1
X
LemmaC.2. Forallx (0,1)andα (0,1)satisfying1+xlogα 0,wehave 1−αx logα.
∈ ∈ ≥ x ≥− 2
Proof. Lety =logα<0. Weknowthat
1 αx logα
−
x ≥− 2
xy
1 exy
⇐⇒ − ≥− 2
xy
exy 1+ ,
⇐⇒ ≤ 2
whichistruesinceeu 1+ u forallu [ 1,0]andxy =xlogα 1.
≤ 2 ∈ − ≥−
LemmaC.3(Theorem1in[Beygelzimeretal.,2011]). LetX ,...,X [ B,B]forsomeB >0
1 T
∈ −
be a martingale differencesequence and with T E [X2] V for some fixed quantity V > 0.
t=1 t t ≤
Wehaveforallδ (0,1),withprobabilityatleast1 δ,
∈ P −
T
log(1/δ)
X min λV + 2 V log(1/δ)+Blog(1/δ).
t
≤λ∈[0,1/B] λ ≤
t=1 (cid:18) (cid:19)
X p
24