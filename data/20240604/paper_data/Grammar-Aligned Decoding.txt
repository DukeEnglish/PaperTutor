Grammar-Aligned Decoding
KangheePark1∗ JiayuWang1∗ TaylorBerg-Kirkpatrick2 NadiaPolikarpova2 LorisD’Antoni1
1UniversityofWisconsin-Madison 2UniversityofCaliforniaSanDiego
{kpark247, jwang2782, ldantoni}@wisc.edu, {tberg, npolikarpova}@ucsd.edu
Abstract
LargeLanguageModels(LLMs)strugglewithreliablygeneratinghighlystructured
outputs,suchasprogramcode,mathematicalformulas,orwell-formedmarkup.
Constrained decoding approaches mitigate this problem by greedily restricting
whattokensanLLMcanoutputateachsteptoguaranteethattheoutputmatches
a given constraint. Specifically, in grammar-constrained decoding (GCD), the
LLM’soutputmustfollowagivengrammar. Inthispaperwedemonstratethat
GCDtechniques(andingeneralconstraineddecodingtechniques)candistortthe
LLM’sdistribution,leadingtooutputsthataregrammaticalbutappearwithlikeli-
hoodsthatarenotproportionaltotheonesgivenbytheLLM,andsoultimatelyare
low-quality. Wecalltheproblemofaligningsamplingwithagrammarconstraint,
grammar-aligneddecoding(GAD),andproposeadaptivesamplingwithapprox-
imateexpectedfutures(ASAp),adecodingalgorithmthatguaranteestheoutput
tobegrammaticalwhileprovablyproducingoutputsthatmatchtheconditional
probabilityoftheLLM’sdistributionconditionedonthegivengrammarconstraint.
Ouralgorithmusespriorsampleoutputstosoundlyoverapproximatethefuture
grammaticality of different output prefixes. Our evaluation on code generation
andstructuredNLPtasksshowshowASApoftenproducesoutputswithhigher
likelihood(accordingtotheLLM’sdistribution)thanexistingGCDtechniques,
whilestillenforcingthedesiredgrammaticalconstraints.
1 Introduction
Despitetheirremarkablesuccess,pre-trainedLargeLanguageModels(LLMs)oftenstrugglewith
generating highly structured outputs, such as program code, configuration files, or mathematical
formulas. Anaïveapproachtoenforcingstructureisrejectionsampling,whichrepeatedlysamples
stringsfromtheLLMandchecksthemagainstavalidityoracle,typicallyintheformofacontext-
freegrammar(CFG).Rejectionsamplingishighlyinefficientorsimplyintractableforrestrictive
grammarsandlongoutputsequences—i.e.,mostgeneratedstringswillnotbeinthetargetgrammar.
Constrained decoding addresses the inefficiency of rejection sampling by greedily “forcing” the
LLMoutputtosatisfythegivenconstraint. Specifically,whentheconstraintisgivenasagrammar,
grammar-constrained decoding (GCD) [8, 27, 28], can build automata that allow for on-the-fly
maskingoftokensthatwillprovablyleadtooutputsoutsideofthegrammarduringdecoding.
WhileGCDdoesnotincurtheoverheadofrejectionsampling—i.e.,thegeneratedoutputisalwaysin
thelanguageofthegrammar—weshowthatGCDandingeneralallformsofstructureddecoding
introduceanewproblem: structureddecodingdistortstheLLM’slearnedlanguagedistribution,
effectivelyhinderingtheLLM’scapabilities.
Thispaperintroducesandformalizesgrammar-aligneddecoding(GAD),theproblemofsampling
from an LLM so that the outputs (1) are guaranteed to adhere to a given grammar, and (2) are
∗Equalcontribution
Preprint.Underreview.
4202
yaM
13
]IA.sc[
1v74012.5042:viXraunbiased wrt. the LLM’s distribution. Although exact GAD is intractable in general (similar to
rejectionsampling),weproposeanewadaptivedecodingalgorithmforapproximateGAD,which
startsoffasGCDandgraduallyconvergestotheLLM’sdistribution,andthusallowstradingoff
betweenefficiencyandaccuracy. Thealgorithm,whichwedubAdaptiveSamplingwithApproximate
ExpectedFutures(ASAp),isbuilt“ontop”ofexistingconstraineddecodingalgorithms. Whereas
GCDapproachessimplymaskouttokensthatleadtonon-grammaticalsequencesforagivenprefix,
ASApremembersforallsampledprefixestheprobabilityassociatedwithmasked-outtokensanduses
ittoupperboundtheprobabilityofgrammaticality. Byupdatingthisboundwhenmoresamplesare
observed,thedecodingalgorithmconvergestothedesiredprobabilitydistribution—i.e.,itsamples
outputsfromtheLLM-inducedprobabilityconditionedontheoutputsbeingacceptedbythegrammar.
TheideaworksforanystructureddecodingapproachandnotjustforGCD,butinthispaperwefocus
ourevaluationonconstraintsexpressedviagrammars.
WeevaluateASApontwostructuredpredictiontasks: formalprogramsynthesisandconstituency
parsing. OurexperimentsonprogramsynthesisandNLPtasksshowthatGCDtechniquesgenerate
outputsthataregrammaticalbutunlikelyaccordingtotheLLM,whilewithASAp,thelikelihood
ofthegeneratedoutputsimprovesovertime,convergingtothetargetconstrainedLLM—i.e.,GAD
betterrespectstheLLMwhilestillenforcingtheconstraints.
2 Grammar-AlignedDecoding
Inthissection,weformalizetheproblemofgrammar-aligneddecoding(GAD)asdecodingfrom
anautoregressivelanguagemodelwhileenforcingtheoutputsequencetobeacceptedbyagiven
context-freegrammar. Wealsodemonstratethelimitationsofexistingapproachestothisproblem.
LanguageModels An(autoregressive)languagemodeldefinesaprobabilitydistributionP on
thesetofallstringsw ∈Σ∗overavocabularyoftokensΣviaaproductofleft-to-rightnext-token
conditionaldistributionsP(w ...w )=Πn P(w |w ).
1 n i=1 i 1:i−1
Context-Free Grammars A context-free grammar
(CFG) is a quadruple G = (Σ,N,S,R), where Σ is
a vocabulary of tokens (also called terminal symbols), S ::= 00000|1A
2
N is a finite set of non-terminal symbols, S ∈ N is A
i
::= 0A
i+1
|1A i+1,fori=2,3,4
thestartingnon-terminal,andRisthesetofproduction A
5
::= 0|1
rules. An example CFG is shown in Fig. 1. A gram-
mar G defines a single-step derivation relation on se- Figure1: CFGG overtokensΣ = {0,1},
sk
quencesofsymbolsα,β,γ ∈ (N ∪Σ)∗: αAγ ⇒ αβγ writteninBackus-Naurform(BNF)notation.
ifA → β ∈ R. Thereflexivetransitiveclosureofthis Thisgrammaracceptsthestring00000andall
relationiscalledderivationandwritten⇒∗. Asequence length-5stringsthatstartwitha1.
oftokenswisasentenceifitisderivablefromS;theset
of all sentences is called the language of the grammar G, that is, L(G) = {w ∈ Σ∗ | S ⇒∗ w}.
In addition, we define the prefix language of G as the set of all prefixes of sentences in L(G):
L (G)={w ∈Σ∗ |wv ∈L(G)}.
prefix
Grammar-Aligned Decoding Given a model distribution P and a CFG G, grammar-aligned
decoding(GAD)isthetaskofsamplingfromthedistributionQP,G thatisproportionaltoP but
restrictedtosentencesinG:
1[w ∈L(G)]·P(w)
QP,G(w)=
(cid:80) 1[w′ ∈L(G)]·P(w′)
w′
WhenP andG areclearfromcontext,wewillwriteQ(w)insteadofQP,G(w).
Example1. ConsiderthedistributionP thatarisesfrompromptinganLLMto“generateabinary
stringthatendswitha1”. WeexpectP toassignhighprobabilitytostringsoftheform(0|1)∗ 1—i.e.
thosethatsatisfytheprompt(Mixtral-8x7B-Instruct-v0.1(temperature=1)generatesbinarystrings
thatendwitha1around90%ofthetime.) AsnippetofapossibledistributionP isdepictedinFig.2.
Supposeweconstrainthemodel’soutputtothelanguageofthegrammarG inFig.1,whichonly
sk
acceptsstringsoflength5. Moreover,G
sk
onlyacceptsonestringthatstartswith0,i.e.,00000,which
does not end with 1. In Fig. 2, the grayed out parts of the trie are tokens that lead to sequences
2outsideofthegrammarG . AccordingtothedefinitionofGAD,thetargetsamplingdistribution
sk
QP,Gsk shouldassign: (i)highprobabilitytoalleightstringsoftheform1w 2w 3w 41—whichconform
bothtothegrammarandtheprompt;(ii)lowprobabilitytothestring00000—whichconformstothe
grammarbutnottheprompt;and(iii)zeroprobabilitytoallotherstrings.
Exact GAD Can one exactly sample from
QP,G? Rejection sampling, which repeatedly
0 (0.45) 0 (0.45) 0 (0.45)
draws from P until a sample lands in L(G), …
… … … … $ (10-8)
provably yields exact samples according to
QP,G,butifP assignsmostofitsmassoutside …
ofL(G),itisintractablyslow,especiallyifthe … (0.1)
promptisnotincludinginformationaboutthe …
grammar (see [27]). For Ex. 1, rejection sam- … … … …
pling would be highly inefficient because the 1 (0.3) 1 (0.3) 1 (0.3) 1 (0.3) …
modelwouldgeneratemanystringsthatarenot $ (0.3)
oflengthfive. … (0.1)
… (0.1)
Incontrast,exactsamplingfromP isefficient
becauseitsjointdistributionisrepresentedby Figure2:Fragmentoftheconditionalmodeldistribution
aproductofeasilycomputedleft-to-rightcon- P forEx.1depictedasatrie.Eachnodecorrespondsto
ditionals,enablingancestralsampling(i.e.,gen- aprefixw 1:i−1,andeachedgeisannotatedwiththenext
tokenw anditsconditionalprobabilityP(w |w ).
erating tokens left to right, conditioned on al- i i 1:i−1
Fillednodesarecompletestrings. Grayedoutpartsof
readygeneratedtokens). Canwesimilarlyfac-
thetrieareoutsideofthegrammarG .
torQintoaproductofleft-to-rightconditionals sk
QP,G(w |w ),toenableancestralsampling?
i 1:i−1
Forsimplicity,letusassumethatP isadistributionoversequencesofexactlylengthn(although,in
practice,languagemodelscanproduce‘stop’tokenswhichallowforavaliddistributiononsequences
ofalllengths). TheexactconditionalsofQP,G aregivenby:
QP,G(w |w ) ∝ (cid:80) (cid:2)1[w ∈L(G)]·Πn P(w |w )(cid:3)
i 1:i−1 ∝ P(w wi+1 |:n w )·E j=i [1[j w ∈1 L:j (− G1 )]] (1)
i 1:i−1 P(wi+1:n|w1:i)
Thus,exactleft-to-rightsamplingfromQP,G consistsofsamplingfrommodelconditionalsP(w |
i
w ),withanadditionalweightingtermc(w )=E [1[w ∈L(G)]]thatconsiders
1:i−1 1:i P(wi+1:n|w1:i)
thegrammar.
Werefertoc(w )asexpectedfuturegrammaticality(EFG),i.e.theprobabilitythatacontinuationof
1:i
w sampledfromP landsinL(G). Usingthisnotation,wecanwritetheexactleft-to-rightsampling
1:i
conditionalexplicitlyas:
P(w |w )·c(w )
QP,G(w |w )= i 1:i−1 1:i (2)
i 1:i−1 (cid:80) P(w′ |w )·c(w ,w′)
w′ i 1:i−1 1:i−1 i
i
To see why computing this conditional is intractable, consider using dynamic programming to
computec(w )bymarginalizingoveraproductofpotentialfunctions: thesetofmodelconditionals
1:i
andanindicatorpotentialforthegrammar.Whiletheindicatorpotentialcanbefactorizedacrossrules
inthegrammar,themodel’scontributiongenerallydoesnotfactorize:inpractice,thefinalconditional
probabilityP(w | w )isaglobalpotentialfunction,definedbyanon-linearneuralnetwork
n 1:n−1
touchingeveryvariable. Thus,themaingoalofthispaperistodevelopeffectiveapproximationsto
theEFGc(w ),whichwouldenableustocomputetheleft-to-rightconditionalsofQ.
1:i
LimitationsofGrammar-ConstrainedDecoding Existingwork[27,28]hasproposedgrammar-
constraineddecoding(GCD)asawaytoefficientlysamplefromanautoregressivelanguagemodel
subject to grammar constraints. Although the exact details of these techniques vary depending
onclassofgrammarstheysupport,thecommonthreadisthattheyrelyonanincrementalparser,
whichcanefficientlycheckwhetheragivenstringwisaprefixofasentenceinthegrammar,i.e.,
w ∈ L (G). Whengivenasentencew ,GCDtechniquesusethisparserduringdecoding
prefix 1:i−1
tomaskoutanynexttokenw thatresultsinaprefixw forwhichnocompletionwillproducea
i 1:i
sequenceinthegrammar. UsingthetrieinFig.2asanexample,onecanthinkofGCDassamplinga
paththroughthetriebyselectingonlyamongtheblackoutgoingedgesfromeverynode,proportional
totheirconditionalprobabilitiesinthediagram(e.g.thefirsttokenis0or1withequalprobability).
3IntermsoftheGADproblem,wecanviewGCDasapproximatingtheexactleft-to-rightconditionals
QP,G(w |w )bytheconditionaldistributionQ˜ (w |w ),definedasfollows:
i 1:i−1 GCD i 1:i−1
P(w |w )·1[w ∈L (G)]
Q˜ (w |w )= i 1:i−1 1:i prefix
GCD i 1:i−1 (cid:80) P(w′ |w )·1[w ,w′ ∈L (G)]
w′ i 1:i−1 1:i−1 i prefix
i
Thoughnotoriginallyformulatedinthisway,wecanviewrecentworkonGCD[27,28]asforminga
binaryapproximation1[w ∈L (G)]totheEFGc(w ). Inotherwords,whileGCDconsiders
1:i prefix 1:i
the possibility of future grammaticality, it makes no attempt to integrate the model’s likelihood
to estimate expected future grammaticality, which can lead to substantial bias in the sampling
distribution—i.e.,everyEFGsuchthatc(w )>0willsimplybeapproximatedviathevalue1.
1:i
Example 2 (GCD). Consider again the GAD problem from Ex. 1, where our target sampling
distributionQP,Gsk assignshighprobabilitytostringsthatbothstartandendwitha1andalow
probabilitytothestring00000. However,weobservethatGCD[7]generatesstringsendingwitha
1only30%ofthetime—i.e.,GCDhaseffectivelyruinedtheLLM’sabilitytofollowthepromptby
biasingsamplingtowards00000,anincorrectoutput.
Whengeneratingthefirsttoken(0or1),theGCDalgorithmdoesnotknowhowmanygrammatical
stringscanstartwitheachcharacterand,moreimportantly,howlikelythesestringsareunderP.
Sincebothtokens0and1havethepossibilityofleadingtoagrammaticalstring,GCDwillestimate
theirexpectedfuturegrammaticalityas1,andchooseeachofthemroughlyhalfofthetime(since
P(0) ≈ P(1)). OnceGCDhaschosen0,however,itbecomes“trapped”inthepartofthesearch
spacewheretheonlygrammaticalstringisthelow-probabilitysequence00000.
Ex.2illustrateshowexistingGCDapproachescanhinderthelanguagemodel’sabilitiestoexplore
thespaceofpossibleoutputsaccordingtothelearneddistribution,thushighlightingtheimportance
ofdesigningabetterapproximationtotheEFGc(w );thisisaddressedinthenextsection.
1:i
3 AdaptiveSamplingwithApproximateExpectedFutures(ASAp)
Inthissection,weproposeanadaptivesamplingalgorithmthatiterativelybuildsbetterapproximations
ofthefuturegrammaticalityofasequence. Ourprocedureoperatesbysamplingrepeatedly,each
timeboundinglostprobabilitymasstoprovablyungrammaticalareasofthesearchspaceinorderto
betterguidethenextsamplingiteration. Asaresult,ouralgorithmconvergesovermanyiterations
toexactsamplesfromtheconstrainedLLMdistribution,allowingforaflexibletrade-offbetween
efficiencyandaccuracy.
OverviewoftheAlgorithm GCDapproachespoorlyapproximatethedesireddistributionbecause
theygreedilysampleprefixeswithoutworryingabouttheEFG.Whensamplingthefirsttokenin
Ex.2,GCDsimplyusesthelikelihoodfortokens0and1assignedbytheLLMwithoutconsidering
the probability that these next tokens would result in grammatical completions if sampling were
unconstrained—i.e. withoutincorporatingthecriticalEFGre-weightingtermsthatarenecessary
forunbiasedsamplingfromtheconstrainedLLMdistribution. However,ifGCDendsupsampling
0asthefirsttokenforEx.2, itwillnecessarilysamplethestring00000sincenoothersequences
startingwith0areallowedbythegrammar. Wecan“learn”fromthisresult: thetrueprobability
massassignedtoallgrammaticalsequencesstartingwitha0isnot0.45astheLLM’snexttoken
probabilitywouldhaveusbelieve;instead,thetotalgrammaticalmassinthissectionofthesearch
spaceisthejointprobabilityofthesinglestring00000,whichisthemuchlowervalueof0.455∗10−8
asdepictedinFig.3. Inotherwords,simplybysampling00000,wecanbetterapproximate(inthis
case,exactly)theEFGoftokensalongthispath.
Thekeyinsightbehindouralgorithm,whichwecallASAp,isthatwecaniteratethisprocessof
discoveringlostgrammaticalprobabilitymassbyrepeatedlysamplingandrevisingtransitionweights
aftereachsampleisproduced. Moreformally,wecanthinkofthisprocedureasstartingwithGCD’s
over-approximationtoeachEFGc(w )term,andthen,throughrepeatedsamplinganddiscoveryof
1:i
massassignedtonon-grammaticalcompletions,reducingeachoverapproximationtomakeitmore
accurate. Inthelimit,theapproximationsconvergetoexactEFGestimatesandunbiasedsampling.
TwopossiblefirstiterationsoftheASApalgorithmaredepictedinFig.3. Inthefirstiteration(leftof
Fig.3),aftersamplingthesequence00000,thealgorithmdirectlyaddressestheissuethatarosein
4*0.453*10-8 *0.452*10-8 *0.45*10-8 *10-8 0 (0.453*10-8) 0 (0.452*10-8) 0 (0.45*10-8) $ (10-8)
*0.455*10-8 0 (0.45) 0 (0.45) 0 (0.45) $ (10-8)
0 (0.45)
1 (0.45) 1 (0.3) 1 (0.3) 1 (0.3) 1 (0.3) $ (0.3)
*0.13 + 0.3 *0.12 + 0.3 *0.09 + 0.3 *0.3
Figure3:IllustrationofthetriebuiltbyASApaftersampling00000asthefirststring(left)andaftersampling
11111asthesecondstring(right).EFGupdatesaftereachiterationareshowninred.
Ex.2byattemptingtobetterapproximatetheprobabilitymassofpotentialgrammaticalcompletions
ofeachprefixof00000(redquantities). Forexample,theexpectedfuturegrammaticalityoftheprefix
0000itisnow0.45∗10−8—i.e.,thealgorithmeffectively“looksahead”todeterminethatonlyone
valid(butlowprobability)string0$thatcanfollow0000. TheideasdevelopedinGCDallowusto
efficientlycompute,foragivenstring,thelikelihoodofthenexttokensthatwillimmediatelyresult
innon-grammaticality.
If we only sample one string from the LLM, we cannot hope to do better than GCD in terms of
samplingfaithfullyinagrammar-alignedway. However,ifweweretonowsampleoncemore,we
couldnowbetterdirectoursamplingstrategy. Intheseconditeration(rightofFig.3),thestring11111
issampledandtheexpectedfuturegrammaticalityisupdated(redquantities). Notethatatthispoint
theprobabilitesassignedtothestring00000fromtheearlieriterationhavealreadybeenupdated.
By repeating the above approach multiple times (i.e., by producing more samples), the ASAp
algorithmproducespreciseapproximationsoftheexpectedfuturegrammaticalitiesandthusbetter
samplesfromtheconstrainedLLM.
AlgorithmFormalization Thekeyquantitythatthealgorithmapproximatesbasedonpastsamples
istheexpectedfuturegrammaticality(EFG)c(w ) = E [1[w ∈ L(G)]]. Atiteration
1:i p(wi+1:n|w1:i)
m+1, our algorithm uses the set of samples S = {s ,...,s } observed so far to compute an
1 m
overapproximationc˜ (w )ofc (w )foreverypossiblestringw . Theoverapproximationis
S 1:i S 1:i 1:i
inductivelydefined:
c˜ (w )=1[w ∈L (G)] nostringinS startswithw
S 1:i (cid:80) 1:i prefix 1:i (3)
c˜ (w )= P(w |w )·c˜ (w ) otherwise
S 1:i wi+1 i+1 1:i S 1:i+1
Intuitively,ifnosamplesinSstartwiththeprefixw ,thenc˜ (w ),theoverapproximationofEFG
1:i S 1:i
issimplywhetherthestringisorisnotavalidprefixinthegrammar—i.e.thesameoverapproximation
usedbyGCD.If,ontheotherhand,wehaveencounteredtheprefixw beforeinprevioussamples
1:i
inS,theoverapproximationusesthenexttokenlikelihoodsthatwerecomputedduringtheprevious
samplingrunsofthealgorithmtocomputeabetterestimateofEFG.
Forexample,inFig.3,oncewehavesampledthesequences00000and11111,wehavethatc˜ S(0000)=
0.45∗10−8andc˜ S(110)=1(i.e.,wehavenotseenasamplewiththeprefix110yet).
Theorem1. ∀w ∈L (G),c˜ (w )≥c(w ).
1:i prefix S 1:i 1:i
Proof. Toseethatc˜ (w )isindeedanupperboundonc(w ),considertwocases: First,suppose
S 1:i 1:i
w is not a prefix of any string in S. In this case, c˜ (w ) = 1[w ∈ L (G)] and, like
1:i S 1:i 1:i prefix
GCD, provides a trivial upper bound. When 1[w ∈ L (G)] = 0, there is no possibility of
1:i prefix
grammaticalityalongthispathandtheEFGisthereforealsozero. When1[w ∈L (G)]=1
1:i prefix
it trivially bounds EFG, which is a probability. Second, suppose w is a prefix in S. Then, by
1:i
induction:
(cid:88) (cid:88)
c˜ (w )= P(w |w )·c˜ (w )≥ P(w |w )·c(w )=c(w ) (4)
S 1:i i+1 1:i S 1:i+1 i+1 1:i 1:i+1 1:i
wi+1 wi+1
ThesamplingprocedureitselfproceedsautoregressivelylikeGCD,butusingtheiterativelyupdated
EFGestimateswehavejustdefined,c˜ . Specifically,theleft-to-rightsamplingconditionalforour
S
5procedure,Q˜ (w |w ),afterhavingpreviouslysampledthestringsinS,isdefinedasfollows:
S i 1:i−1
P(w |w )·c˜ (w )
Q˜ (w |w )= i 1:i−1 S 1:i (5)
S i 1:i−1 (cid:80) P(w′ |w )·c˜ (w ,w′)
w′ i 1:i−1 S 1:i−1 i
i
Our overall algorithm, which is presented in Algo-
rithm1,thenproceedsiteratively,usingpastsamples
to improve subsequent samples. In the listing, we Algorithm1ASApalgorithm
assumethatweareonlyinterestedinthefinalsample, InitializeS ←{}
but in our evaluation we will analyze whether the form≤M do
algorithminducesthedesireddistribution. Nextwe Draws∼Q˜ viaancestralsampling
S
provideashortproofthatthisalgorithmconverges S :=S∪s
toexactestimatesofEFGinthelimitofinfiniteit- returnFinalsamples
erations, and therefore to exact samples from the
constrainedLLMdistribution.
Theorem2. LetS ={s ,...,s }bethesetofrecordedsamplesuptothemthiterationofASAp.
m 1 m
∀w ∈L (G) and ∀ϵ,δ >0, ∃M s.t. form>M P(c˜ (w )−c(w )<ϵ)≥1−δ.
1:i prefix Sm 1:i 1:i
Proof. Letw beanarbitrarysequenceinL (G). Theapproximationgapaftermiterations
1:i prefix
of sampling with ASAp, c˜ (w )−c(w ), is equal to the marginal probability under P (the
Sm 1:i 1:i
unconstrainedLLMdistribution)ofallungrammaticalcontinuationsofw thathavenotyetbeen
1:i
encounteredinthefirstmsamples,S . ASApsamplesaccordingtoP,butre-weightedbyanupper
m
bound on the true EFG (Theorem 1). Thus, the probability of encountering a previously unseen
ungrammaticalcontinuationofw onanygiveniterationisatleastashighastheprobabilityof
1:i
encountering the same continuation when sampling directly from P. Thus, for any ϵ > 0, there
must be a number of sames M such that we have encounter all but ϵ of the probability mass on
ungrammaticalcontinuationsofw underP witharbitrarilyhighprobability.
1:i
4 Experiments
We implemented the ASAp algorithm as an extension of the Transformers-CFG implementation
of GCD [7]. When the LLM generates a sequence w , the ASAp algorithm keeps track of the
1:n
originalLLM’sprobabilityP(w |w )for1≤i≤nandthesetofallowednexttokens{w |
i 1:i−1 i
w ,w′ ∈L(G)}determinedbytheincrementalparserintheTransformers-CFGlibrary. Afterthe
1:i−1 i
LLMfinishesgeneratingasequence,ourimplementationofASApupdatestheoverapproximationc˜
S
fromtheendofsequencebyback-propagatingthequantity1minusprobabilityofthetokensthatwill
forsureleadtonon-grammaticalsequences. TheimplementationofASApupdatesc˜ (w ,w′)
S 1:n−1 n
forallpossibletokensw′,andthenmovesontoupdatec˜ (w ,w′ )...,c˜ (w ,w′),c˜ (w′)
n S 1:n−2 n−1 S 1 2 S 1
usingEquation(3).
DatasetsandModels. WeconsiderthebenchmarkfromExample2andthreestructured-decoding
tasks. TwoofourtasksinvolvesolvingSyntax-GuidedSynthesisProblems(SyGuS)[2]. SyGuS
isastandardizedformatwhereoneprovidesalogicalspecificationandacontextfreegrammarof
first-ordertermsandthegoalistosynthesizeaterminthegrammarthatsatisfiesthespecification.
SyGuSisanaturalfitforGADandweconsidertwotasksfromthestandardSyGuSbenchmarks
wheregrammarsvaryfrombenchmarktobenchmark: stringswithlinearintegerarithmetic(SLIA)
andloopinvariantgenerationwithbit-vectorarithmetic(INV-BV).Intheformer,thegrammarisused
torestrictwhatconstantstringsonecanusewhenbuildingstring-manipulatingprogramsandinthe
latterthegrammarisusedtorestrictconstantbit-vectorsandoperationsusedtobuildinvariants. For
bothfamiliesofbenchmarks,ourpromptsconsistof3in-contextexamplesoftheform(specification,
solution)andthegrammaristhenprovidedasaconstraintforGAD.Ourthirdtaskistheconstituency
parsing(CP)taskalreadyusedinpriorGCDwork[8]wherethegrammarisusedtohelpthemodel
producewell-parenthesizedparsetreesforEnglishsentences.
Duetoconstrainedresourcesandneedingtoruninferencemultipletimestomeasurewhetherthe
distributionQ˜ isfaithfultoQ,werandomlyselect15SLIAproblems,15INV-BVproblems,and6
CPproblems. Weselecttheopen-sourceMistral-7B[12]forevaluationduetoitssuperiorreasoning
andcodegenerationcapabilities.
6(a)IllustrativeEx.2 (b)SLIA/name-combine-4-long (c)SLIA/initials-small
Figure4:KL(Q ∥P)andKL(Q ∥P)
ASAp GCD
(a)IllustrativeEx.2 (b)SLIA/name-combine-4-long (c)SLIA/initials-small
Figure5:ExpectationsofQ˜ ,Q˜ ,andP
ASAp GCD
Measures. Werunbothalgorithmsfor2,000iterations/sampleoneachbenchmark.
To assess converge to the target distribution, we measure the Kullback–Leibler (KL) divergence
betweenthedistributionsofGCDandASApfromthetargetdistributionQforagivennumberof
samples. BecausetheidealGADdistributionQ isproportionaltotheoriginalLLM’sdistribution
P,G
P for sequences allowed by a grammar G, we can use the LLM’s distribution P on all observed
samples as an estimate Q . The quantity KL(Q∥P) only differs by a constant from the KL
P,G
divergencebetweenempiricaldistributionsandtheidealGADdistribution:
(cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35)
Q˜ Q˜ Q˜
KL(Q˜∥P)=E log =E log =E log −logC=KL(Q˜∥Q )−logC
Q˜ P Q˜ C·Q Q˜ Q P,G
P,G P,G
whereC =(cid:80) 1[w ∈L(G)]P(w).Thus,KL(Q˜∥P)canbeusedtoquantifythealignmentbetween
w
theempiricaldistributionsofGCDandASApwiththeidealGADdistribution.
Forexample,Fig.4ashowsconvergenceresultsforthefirst75iterationsontheillustrativeEx.2—i.e.,
theKLdivergenceforQ˜ quicklyconvergesto0whereastheoneforQ˜ doesn’t.
ASAp GCD
WealsocomparetheempiricalexpectationsofthevariablesQ˜ ,Q˜ ,andP. Forexample,
GCD ASAp
Fig.5ashowsconvergenceresultsforthefirst75iterationsontheillustrativeEx.2—i.e.,Q˜
ASAp
convergestotherightexpectation.
Results. Fig.4bandFig.5billustrateabenchmarkinwhichourASApalgorithmquicklyconverges
tothetargetdistribution. Fig.4depictstheKLdivergenceofaslidingwindowofsize500(e.g.,the
pointsatx=800denotetheKLdivergenceofthesamples800-1300). Fig.5depictsallthesamples
fromtheexperiment,aswellashowtheexpectationsconverges(apointatx=idenotestheempirical
expectationonthefirstisamples. ForthiscasetheexpecationforGCDstaysverycloseto0.
Similarly,Fig.4candFig.5cillustrateabenchmarkinwhichourASApalgorithmconvergesslowly.
In this case, bot ASAp and GCD are far from the target expectation (Fig. 5c), but because GCD
happenstobebiasedtowardsthemostlikelyoutcome,itexhibitsbetterKLdivergence. Thecomplete
setofplotsisshowninSec.A.5.1.
To better understand how the algorithms respectively converge, Fig. 6 plot for each benchmark
category the expectations for each benchmark computed by GCD and ASAp against the target
7(a)SLIA (b)INV-BV4 (c)CP
Figure6:ScatterplotsofQ˜ (•)andQ˜ (×)vs.expectationsofP after2,000samples.Proximitytothe
ASAp GCD
diagonalindicatesproximitytotheactualexpectation—e.g.,a•at(0.45,0.4)indicatesabenchmarkwherethe
empiricalexpectationofP was0.45andQ˜ hadconvergedtoanexpectationof0.4after2,000iterations.
ASAp
expectationofP after2,000iterations. Thesumofleastsquaredifferencebetweenexpectations
computedbyGCDandtheexpectationsofP are2.259(SLIA),1.852(INV-BV4),and0.109(CP).
ThesumofleastsquaredifferencebetweenexpectationscomputedbyASApandtheexpectationand
thoseofP are1.242(SLIA),0.802(INV-BV4),and0.159(CP).Whilewehavetoofewpointsfor
CPtodrawconclusions,theexpectationscomputedbyASAparemuchclosertotheonescomputed
byGCDacrossourexperiments.
WhileourworkisinterestedinthetheoreticalconvergenceoftheASApalgorithm,wealsoreport
howtheGCDandASApdifferforsolvingtheSLIAandINV-BV4tasks—i.e.,howmanyofthe
sampledprogramsarecorrectsolutionstothegivenproblem. GCDandASApsolveapproximately
thesamesetofproblems(thereisjustoneSLIAbenchmarkforwhichASApreturnsavalidsolution
ononesampleandGCDneverdoesso). ASApproducescorrectsamples38%moreoftenthanGCD
(geomean),whereasforSLIAbenchmarksthatbothtoolscansolve,ASApproducescorrectsamples
73%lessoftenthanGCD(geomean). DetailedresultscanbefoundinSec.A.5.2. Theseresultsare
inlinewiththefactASApshowsfasterconvergenceonINV-BV4benchmarks. Forexample,forthe
benchmarkillustratedinFig.4b,ASApreturnsthecorrectsolutionfor1588samples,whereasGCD
onlyreturnsthecorrectsolution12times,whereasforthebenchmarkinFig.4c,ASApreturnsthe
correctsolution69timesandGCD363times.
Discussion and Limitations. As predicted by our theorems, on most benchmarks the ASAp
algorithmconvergestothedesireddistributionP whereasGCDdoesnotimproveovertime(i.e.,it
exhibitsthebiasdescribedinthispaper).
WhileASAphasnostrongeffectonsolvingdownstreamtasks,weobservethatoninstanceswhere
theconvergenceisprominent,ASApendsupsamplingcorrectsolutionsmoreoftenthanGCD,which
iswhatweexpectwhentheLLMhas“learned”howtosolvethegiventask.
ThekeylimitationofourworkisthecurrentslowconvergenceoftheASApalgorithm. Insome
benchmarks, even after 2,000 iterations the KL divergence barely improves and even though the
expectationofQ˜ isimproving,itconvergesveryslowly.
ASAp
Wehighlightthatthecontributionsofthispaperarediscoveringandformalizingthebiasofexisting
constraineddecodingapproachesandproposingthefirstconvergingalgorithmtoaddressthisproblem.
Now that we have identified the problem, there are many “low-hanging fruits” to improve our
samplingstrategy,whicharegreattargetsforfuturework—e.g.,usingformsoftargetedbeamsearch
tobootstrapoursamplesettobetterexploregrammarpathsandavoidsamplingsimilarstrings.
5 RelatedWork
Constrained Decoding Past work has extensively explored constrained decoding algorithms,
whichmodifytheoriginaldecodingprocessofLLMstoensuretheoutputadherestoauser-specified
regular[18,28]orcontext-freelanguage[5,6,8,19,23,24,25,26]inadiscretespace. Otherworks
enforcehardoutputconstraintsusingdynamicmonitoringandverificationmethods[1,15,27]orby
modifyingbeamsearchtechniquestoimposelexicalconstraints,whichrequirespecifickeywordsto
8appearinthegeneratedtext[4,9,10,16,17,20]. Atahighlevel,thesemethodsinvolverunningthe
LLMdecodeinparallelwithamonitoringscheme(e.g.,parsingalgorithmsforCFGs)toidentify
which next tokens or beams can produce valid output sequences that meet the constraints. The
decoderthenmasksoutanytokensthatwouldleadtoinvalidsequences,samplingonlyfromthe
permissibleones.
Toincorporatesequence-levelsoftsemanticorcontextualconstraints,Aminietal.[3],Kumaretal.
[13],Lietal.[14],Qinetal.[21]haveappliedgradient-basedsamplingtechniquesthatrelaxthose
constraintstodifferentiableones,usedthemasclassifierstofurtherguidethedecodingprocess.While
theseworksguaranteethatthedecodedoutputmeetsthespecifiedconstraints(whetherintheformof
grammar,monitoringschemes,ordifferentiablefunctions),theyoftenoperategreedilyandintroduce
biasintotheoutputdistributioninthewaythathasbeendiscussedinthispaper. Dependingonthe
applicationoneconsiders,thisproblemmayormaynotaffectdownstreamtasks,butaswehave
arguedinthispaper,thebiascanbequiteprominentandsometimesaffectdownstreamperformance.
Ouradaptivedecodingalgorithmimprovesdecodingovertimebyanalyzinghowprevioussamples
ledtonongrammaticaility.
Constraint-AlignedDecoding Toourknowledge,ourpaperisthefirsttodefinetheproblemof
aligningtheoutputdistributionofanLLMinthepresenceofaconstraint. Wefocusourattention
onconstraintsexpressedasgrammars,butourdefinitionsandalgorithmapplytoanyconstraintfor
whichpossiblesatisfaction(inourcasegrammaticality)canbeevaluatedinaleft-to-rightmanner.
Insomesettings,oneisinterestedingeneratingmultipleoutcomeswithanLLMtoapproximatea
distributionofinterest[11,22]—e.g.,togeneratearandomnumberorasetofgoodtestcasesfor
aprogram. Aswehaveshown,constraineddecodingcanheavilyskewtheLLMsdistributionand
resultinbiasingthemodeltowardscertainconstraint-matchingsequences. Whileourworkisatthis
pointtheoretical,nowthattheproblemofaligninganLLM’sdistributionwithconstraintshasbeen
defined,weexpectadvancesinhowsamplingisperformedtoquicklyconvergetobetterdistributions
faster(e.g.,usingbeamsearchtoquicklyexplorepossiblepathsinsteadofjustsampling).
6 Conclusion
WehaveintroducedanewanalysisoftheidealtargetforconstrainedsamplingfromanLLMusinga
grammar,whichwecallgrammar-aligneddecoding(GAD).WeproposedanewalgorithmforGAD
whichwecallASApthatiterativelybuildsbetterapproximationstothecriticalre-weightingterm
requiredforGAD:theexpectedfuturegrammaticality. Weanalyzedtheconvergenceofourproposed
algorithmanddemonstrateitseffectivenessinrelationtoexistinggrammar-constraineddecoding
techniquesonasetofbenchmarkcodegenerationtasks. Weanalyzedandevaluatedourapproach
usingconstraintsenforcedbyacontext-freegrammar;however,extensionsofourapproachmightbe
appliedtomoregeneralclassesofconstraintsforLLMdecoding.
References
[1] LakshyaAAgrawal,AdityaKanade,NavinGoyal,ShuvenduK.Lahiri,andSriramK.Raja-
mani.2023. Monitor-guideddecodingofcodelmswithstaticanalysisofrepositorycontext.
In Advances in Neural Information Processing Systems 36: Annual Conference on Neural
InformationProcessingSystems2023,NeurIPS2023,NewOrleans,LA,USA,December10-
16,2023.
[2] RajeevAlur,DanaFisman,SaswatPadhi,RishabhSingh,andAbhishekUdupa.2019. Sygus-
comp2018: Resultsandanalysis.
[3] AfraAmini,LiDu,andRyanCotterell.2024. Structuredvoronoisampling. AdvancesinNeural
InformationProcessingSystems,36.
[4] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. 2016. Guided open
vocabularyimagecaptioningwithconstrainedbeamsearch. arXivpreprintarXiv:1612.00576.
[5] LucaBeurer-Kellner,MarcFischer,andMartinVechev.2023. Promptingisprogramming: A
querylanguageforlargelanguagemodels. Proc.ACMProgram.Lang.,7(PLDI).
9[6] YihongDong,XueJiang,YuchenLiu,GeLi,andZhiJin.2022. Codepad: Sequence-based
codegenerationwithpushdownautomaton. arXivpreprintarXiv:2211.00818.
[7] SaiboGeng,MartinJosifoski,MaximePeyrard,andRobertWest.2023. Transformers-CFG.
https://github.com/epfl-dlab/transformers-CFG.
[8] SaiboGeng,MartinJosifoski,MaximePeyrard,andRobertWest.2024. Grammar-constrained
decodingforstructurednlptaskswithoutfinetuning.
[9] ChrisHokampandQunLiu.2017. Lexicallyconstraineddecodingforsequencegeneration
usinggridbeamsearch. arXivpreprintarXiv:1704.07138.
[10] J Edward Hu, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt Post, and
Benjamin Van Durme. 2019. Improved lexically constrained decoding for translation and
monolingualrewriting. InProceedingsofthe2019ConferenceoftheNorthAmericanChapter
oftheAssociationforComputationalLinguistics: HumanLanguageTechnologies,Volume1
(LongandShortPapers),pages839–850.
[11] LinghanHuang,PeizhouZhao,HuamingChen,andLeiMa.2024. Largelanguagemodels
basedfuzzingtechniques: Asurvey.
[12] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
Chaplot,DiegodelasCasas,FlorianBressand,GiannaLengyel,GuillaumeLample,Lucile
Saulnier,etal.2023. Mistral7b. arXivpreprintarXiv:2310.06825.
[13] SachinKumar,BiswajitParia,andYuliaTsvetkov.2022. Gradient-basedconstrainedsampling
fromlanguagemodels.InProceedingsofthe2022ConferenceonEmpiricalMethodsinNatural
LanguageProcessing,pages2251–2277.
[14] XiangLi,JohnThickstun,IshaanGulrajani,PercySLiang,andTatsunoriBHashimoto.2022.
Diffusion-lmimprovescontrollabletextgeneration. AdvancesinNeuralInformationProcessing
Systems,35:4328–4343.
[15] YixuanLi,JulianParsert,andElizabethPolgreen.2024.Guidingenumerativeprogramsynthesis
withlargelanguagemodels. arXivpreprintarXiv:2403.03997.
[16] Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Ronan
Le Bras, Lianhui Qin, Youngjae Yu, Rowan Zellers, Noah A. Smith, and Yejin Choi. 2022.
NeuroLogic a*esque decoding: Constrained text generation with lookahead heuristics. In
Proceedings of the 2022 Conference of the North American Chapter of the Association for
ComputationalLinguistics: HumanLanguageTechnologies,pages780–799,Seattle,United
States.AssociationforComputationalLinguistics.
[17] Ximing Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra Bhagavatula, and Yejin
Choi.2021. NeuroLogicdecoding: (un)supervisedneuraltextgenerationwithpredicatelogic
constraints. In Proceedings of the 2021 Conference of the North American Chapter of the
AssociationforComputationalLinguistics: HumanLanguageTechnologies,pages4288–4299,
Online.AssociationforComputationalLinguistics.
[18] DanielMelcer,NathanFulton,SanjayKrishnaGouda,andHaifengQian.2024. Constrained
decodingforcodelanguagemodelsviaefficientleftandrightquotientingofcontext-sensitive
grammars. arXivpreprintarXiv:2402.17988.
[19] GabrielPoesia,OleksandrPolozov,VuLe,AshishTiwari,GustavoSoares,ChristopherMeek,
andSumitGulwani.2022. Synchromesh: Reliablecodegenerationfrompre-trainedlanguage
models. arXivpreprintarXiv:2201.11227.
[20] Matt Post and David Vilar. 2018. Fast lexically constrained decoding with dynamic beam
allocationforneuralmachinetranslation. arXivpreprintarXiv:1804.06609.
[21] LianhuiQin,SeanWelleck,DanielKhashabi,andYejinChoi.2022. Colddecoding: Energy-
basedconstrainedtextgenerationwithlangevindynamics. InAdvancesinNeuralInformation
ProcessingSystems.
10[22] Alex Renda, Aspen K. Hopkins, and Michael Carbin. 2023. Can LLMs generate random
numbers? evaluatingLLMsamplingincontrolleddomains. InICML2023Workshop:Sampling
andOptimizationinDiscreteSpace.
[23] TorstenScholak,NathanSchucher,andDzmitryBahdanau.2021. PICARD:Parsingincremen-
tallyforconstrainedauto-regressivedecodingfromlanguagemodels. InProceedingsofthe
2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages9895–9901,
OnlineandPuntaCana,DominicanRepublic.AssociationforComputationalLinguistics.
[24] RichardShin,ChristopherHLin,SamThomson,CharlesChen,SubhroRoy,EmmanouilAnto-
niosPlatanios,AdamPauls,DanKlein,JasonEisner,andBenjaminVanDurme.2021. Con-
strainedlanguagemodelsyieldfew-shotsemanticparsers. arXivpreprintarXiv:2104.08768.
[25] EliasStengel-Eskin,KyleRawlins,andBenjaminVanDurme.2023.Zeroandfew-shotsemantic
parsingwithambiguousinputs. arXivpreprintarXiv:2306.00824.
[26] ShubhamUgare,TarunSuresh,HangooKang,SasaMisailovic,andGagandeepSingh.2024.
Improvingllmcodegenerationwithgrammaraugmentation. arXivpreprintarXiv:2403.01632.
[27] BailinWang,ZiWang,XuezhiWang,YuanCao,RifA.Saurous,andYoonKim.2023.Grammar
promptingfordomain-specificlanguagegenerationwithlargelanguagemodels.
[28] BrandonTWillardandRémiLouf.2023. Efficientguidedgenerationforlargelanguagemodels.
arXive-prints,pagesarXiv–2307.
11A SupplementaryMaterial
A.1 HardwareandSoftware
Ourexperimentsareconductedon4NVIDIARTXA6000GPUsand4NVIDIAA100GPUs. Our
implementationisbasedonPython3.10andPyTorch2.1.2.
A.2 Hyperparameters
Thehyperparametersdiscussedinthispaperpertaintothedecodingstrategyoflanguagemodels. As
weaimtoinvestigatetheLM’soriginaldistribution,wesetTop-Pat1.0,Temperatureat1.0,and
Top-Kat0toconsiderthecompletetokenvocabulary.
A.3 ModelCheckpoint
We use the Mistral-7B model checkpoint provided by Hugging Face: https://huggingface.co/
mistralai/Mistral-7B-Instruct-v0.2.
A.4 ExperimentalDetails
A.4.1 SLIAandINV-BV
PromptConstruction Forbothfamiliesofbenchmarks,ourpromptsadoptstandardin-context
learningformatwhichconsistof3in-contextexamplesoftheform(specification,solution)andask
themodeltoprovidethesolutionforthelastexample. Aconcreteexamplewouldbe
You are an expert in program synthesis.
You are tasked with solving a Syntax-Guided Synthesis (SyGuS) problem.
Your goal is to output a function that should produce outputs that satisfy
a series of constraints when given specific inputs.
Question:
(set-logic BV)
(synth-fun inv ((s (BitVec 4)) (t (BitVec 4))) (BitVec 4))
(declare-var s (BitVec 4))
(declare-var t (BitVec 4))
(define-fun udivtotal ((a (BitVec 4)) (b (BitVec 4))) (BitVec 4)
(ite (= b #x0) #xF (bvudiv a b)))
(define-fun uremtotal ((a (BitVec 4)) (b (BitVec 4))) (BitVec 4)
(ite (= b #x0) a (bvurem a b)))
(define-fun min () (BitVec 4)
(bvnot (bvlshr (bvnot #x0) #x1)))
(define-fun max () (BitVec 4)
(bvnot min))
(define-fun l ((s (BitVec 4)) (t (BitVec 4))) Bool
(bvsle (bvlshr s (inv s t)) t))
(define-fun SC ((s (BitVec 4)) (t (BitVec 4))) Bool
(or (bvult t min) (bvsge t s)))
(constraint (=> (SC s t) (l s t)))
(check-synth)
Solution:
(define-fun inv ((s (BitVec 4)) (t (BitVec 4))) (BitVec 4) (bvnot (bvor s #b0111)))
... (2 more examples)
Question:
(set-logic BV)
12(synth-fun inv ((s (BitVec 4)) (t (BitVec 4))) (BitVec 4))
(declare-var s (BitVec 4))
(declare-var t (BitVec 4))
(define-fun udivtotal ((a (BitVec 4)) (b (BitVec 4))) (BitVec 4)
(ite (= b #x0) #xF (bvudiv a b)))
(define-fun uremtotal ((a (BitVec 4)) (b (BitVec 4))) (BitVec 4)
(ite (= b #x0) a (bvurem a b)))
(define-fun min () (BitVec 4)
(bvnot (bvlshr (bvnot #x0) #x1)))
(define-fun max () (BitVec 4)
(bvnot min))
(define-fun l ((s (BitVec 4)) (t (BitVec 4))) Bool
(bvsgt (bvnot (inv s t)) t))
(define-fun SC ((s (BitVec 4)) (t (BitVec 4))) Bool
(distinct t max))
(constraint (=> (SC s t) (l s t)))
(check-synth)
Solution:
GrammarConstraint WhilemostSYGUSproblemscontaingrammarconstraints,someproblems
havegrammarsimplicitlydefinedbythetheory. Weexplicitlyconvertedthegrammarconstraintof
theproblemintoEBNFformatforconstrained-decoding. Theexampleforthelastexamplewouldbe
root ::= "(define-fun inv ((s (BitVec 4)) (t (BitVec 4))) (BitVec 4) " Start ")"
Start ::= "s" | "t" | "#x0" | "#x8" | "#x7"
| "(" "bvneg" " " Start ")" | "(" "bvnot" " " Start ")"
| "(" "bvadd" " " Start " " Start ")" | "(" "bvsub" " " Start " " Start ")"
| "(" "bvand" " " Start " " Start ")" | "(" "bvlshr" " " Start " " Start ")"
| "(" "bvor" " " Start " " Start ")" | "(" "bvshl" " " Start " " Start ")"
A.4.2 ConstituencyParsing
ForConstituencyparsingtask,ourpromptsconsistof8in-contextexamplesoftheform. Aconcrete
examplewouldbe
Perform constituency parsing on the provided sentences in accordance with the Penn TreeBank
annotation guidelines. Fill in the last mapping.
Ad Notes
->
[ ( NP-HLN ( NN Ad ) ( NNS Notes ) ) ]
The market crumbled
->
[ ( S ( NP-SBJ ( DT The ) ( NN market ) ) ( VP ( VBD crumbled ) ) ) ]
I felt betrayed he later said
->
[ ( S ( S-TPC-1 ( NP-SBJ ( PRP I ) ) ( VP ( VBD felt ) ( ADJP-PRD ( VBN betrayed ) ) ) )
( NP-SBJ ( PRP he ) ) ( ADVP-TMP ( RB later ) ) ( VP ( VBD said ) ) ) ]
Friday October 13 1989
->
[ ( NP ( NNP Friday ) ( NNP October ) ( CD 13 ) ( CD 1989 ) ) ]
The Arabs had merely oil
->
13[ ( S ( NP-SBJ ( DT The ) ( NNPS Arabs ) ) ( VP ( VBD had )
( NP ( RB merely ) ( NN oil ) ) ) ) ]
Energy
->
[ ( NP-HLN ( NN Energy ) ) ]
Some U.S. entrepreneurs operate on a smaller scale
->
[ ( S ( NP-SBJ ( DT Some ) ( NNP U.S. ) ( NNS entrepreneurs ) ) ( VP ( VBP operate )
( PP-MNR ( IN on ) ( NP ( DT a ) ( JJR smaller ) ( NN scale ) ) ) ) ) ]
Knowledgeware Inc.
->
[ ( NP-HLN ( NNP Knowledgeware ) ( NNP Inc. ) ) ]
They are more sophisticated this time
->
GrammarConstraint Forconstituencyparsing(CP)taskweusedthegrammarprovidedinprior
GCDwork[8]. Thegrammaristoolargetoattach,butitisusedtohelpthemodelproducewell-
parenthesizedparsetreesandensurethatallwordsinagivenEnglishsentenceappearinleft-to-right
order.
14(a)KLdivergences (b)Expectations (a)KLdivergences (b)Expectations
Figure7:SLIA/dr-name Figure8:SLIA/firstname_small
(a)KLdivergences (b)Expectations (a)KLdivergences (b)Expectations
Figure9:SLIA/firstname Figure10:SLIA/initials_small
(a)KLdivergences (b)Expectations (a)KLdivergences (b)Expectations
Figure11:SLIA/initials-long-repeat Figure12:SLIA/lastname
(a)KLdivergences (b)Expectations (a)KLdivergences (b)Expectations
Figure13:SLIA/name-combine-2_short Figure14:SLIA/name-combine-2-long-repeat
(a)KLdivergences (b)Expectations (a)KLdivergences (b)Expectations
Figure15:SLIA/name-combine-4_short Figure16:SLIA/name-combine-4-long
A.5 DetailedExperimentalResults
Weprovideadditionalplotsandexperimentaldata.
A.5.1 Plots
Figures7–21providetheKLdivergenceandexpectationresultsfortheSLIAbenchmarks.Figures22–
36providetheKLdivergenceandexpectationresultsfortheINV-BVbenchmarks. Figures37–42
providetheKLdivergenceandexpectationresultsfortheINV-BVbenchmarks.
15(a)KLdivergences (b)Expectations (a)KLdivergences (b)Expectations
Figure17:SLIA/phone-3-long Figure18:SLIA/reverse-name-long
(a)KLdivergences (b)Expectations (a)KLdivergences (b)Expectations
Figure19:SLIA/univ_1_short Figure20:SLIA/univ_1
(a)KLdivergences (b)Expectations (a)KLdivergences (b)Expectations
Figure21:SLIA/univ_2_short Figure22:INV-BV/find_inv_bvsge_bvlshr1_4bit
(a)KLdivergences (b)Expectations (a)KLdivergences (b)Expectations
Figure23:INV-BV/find_inv_bvsge_bvneg_4bit Figure24:INV-BV/find_inv_bvsge_bvnot_4bit
(a)KLdivergences (b)Expectations (a)KLdivergences (b)Expectations
Figure25:INV-BV/find_inv_bvsgt_bvor_4bit Figure26:INV-BV/find_inv_bvugt_bvashr0_4bit
A.5.2 CorrectnessResultsforSYGUSTasks
Table1showshowmanysamples(outof2000)yieldedcorrectsolutionsforeachbenchmark(boldis
better). Thetaskinitials_long-repeatwasonlysolvedusingASAp.
16(a)KLdivergences (b)Expectations (a)KLdivergences (b)Expectations
Figure27:INV-BV/find_inv_bvugt_bvneg_4bit Figure28:INV-BV/find_inv_bvule_bvurem0_4bit
(a)KLdivergences (b)Expectations (a)KLdivergences (b)Expectations
Figure29:INV-BV/find_inv_bvule_bvurem1_4bit Figure30:INV-BV/find_inv_eq_bvand_4bit
(a)KLdivergences (b)Expectations (a)KLdivergences (b)Expectations
Figure31:INV-BV/find_inv_eq_bvlshr0_4bit Figure32:INV-BV/find_inv_ne_bvneg_4bit
(a)KLdivergences (b)Expectations (a)KLdivergences (b)Expectations
Figure33:INV-BV/find_inv_ne_bvudiv0_4bit Figure34:INV-BV/find_inv_ne_bvudiv1_4bit
(a)KLdivergences (b)Expectations (a)KLdivergences (b)Expectations
Figure35:INV-BV/find_inv_ne_bvurem0_4bit Figure36:INV-BV/find_inv_ne_bvurem1_4bit
17(a)KLdivergences (b)Expectations (a)KLdivergences (b)Expectations
Figure37:CP/CP_re_ptb_460 Figure38:CP/CP_re_ptb_482
(a)KLdivergences (b)Expectations (a)KLdivergences (b)Expectations
Figure39:CP/CP_re_ptb_486 Figure40:CP/CP_re_ptb_605
(a)KLdivergences (b)Expectations (a)KLdivergences (b)Expectations
Figure41:CP/CP_re_ptb_1434 Figure42:CP/CP_re_ptb_1643
NeurIPSPaperChecklist
1. Claims
Question: Dothemainclaimsmadeintheabstractandintroductionaccuratelyreflectthe
paper’scontributionsandscope?
Answer: [Yes]
Justification: The main claims are well-supported by the algorithm formalization and
theoreticalresultsinSection3,andcomprehensiveexperimentsinSection4,andfurther
resultsintheAppendix.
2. Limitations
Question: Doesthepaperdiscussthelimitationsoftheworkperformedbytheauthors?
Answer: [Yes]
Justification: WeprovidedetailedlimitationsinSection4.
3. TheoryAssumptionsandProofs
Question: Foreachtheoreticalresult,doesthepaperprovidethefullsetofassumptionsand
acomplete(andcorrect)proof?
Answer: [Yes]
Justification: Alltheoremsareaccompaniedbyformalproofs.
4. ExperimentalResultReproducibility
Question: Doesthepaperfullydisclosealltheinformationneededtoreproducethemainex-
perimentalresultsofthepapertotheextentthatitaffectsthemainclaimsand/orconclusions
ofthepaper(regardlessofwhetherthecodeanddataareprovidedornot)?
Answer: [Yes]
18Table1:Correctnessofsolutionsfordifferentalgorithms.
Benchmark CorrectASAP CorrectGCD
phone-3-long 0 0
name-combine-2_short 171 319
name-combine-2-long-repeat 0 0
name-combine-4-short 20 11
name-combine-4-long 1588 12
lastname 285 1526
firstname 1960 1997
firstname_small 1754 1997
SLIA
reverse-name-long 1981 1859
univ_1 67 40
univ_1_short 605 1859
univ_2_short 0 0
dr-name 357 1654
initials_small 69 363
initials_long 540 1584
initials_long-repeat 3 0
find_inv_ne_bvudiv1_4bit 0 0
find_inv_bvugt_bvashr0_4bit 83 49
find_inv_eq_bvlshr0_4bit 635 228
find_inv_eq_bvand_4bit 1599 1305
INV-BV find_inv_bvule_bvurem0_4bit 1813 1710
find_inv_bvsgt_bvor_4bit 11 10
find_inv_bvugt_bvneg_4bit 84 36
find_inv_bvule_bvurem1_4bit 143 227
Justification: WeprovideallthehyperparametersweuseinAppendixA.2,modelcheck-
points in Appendix A.3 and experimental details in Appendix A.4 to support the repro-
ducibilityofourexperiments. Thecodebaseanddatasetswithdetailedinstructionswillalso
bereleased.
5. Openaccesstodataandcode
Question: Doesthepaperprovideopenaccesstothedataandcode,withsufficientinstruc-
tionstofaithfullyreproducethemainexperimentalresults,asdescribedinsupplemental
material?
Answer: [Yes]
Justification: Wewillattachdatasetswecoveredinthispaperduringsubmissionandthe
codewillbereleasedwithdetailedinstructions.
6. ExperimentalSetting/Details
Question: Doesthepaperspecifyallthetrainingandtestdetails(e.g.,datasplits,hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: HyperparametersareincludedinAppendixA.2. Allexperimentaldetailsare
includedinAppendixA.4tofacilitateabetterunderstandingofoursetting.
7. ExperimentStatisticalSignificance
Question:Doesthepaperreporterrorbarssuitablyandcorrectlydefinedorotherappropriate
informationaboutthestatisticalsignificanceoftheexperiments?
Answer: [No]
Justification: Ourexperimentsarerunon2,000samplingiterationsforwhichweshowplots
forconvergence. Giventhecostoftheexperimentswecannotrunonmultipleseeds,buta
highnumberofiterationsmitigatestheproblem. Therefore,errorbarsarenotapplicable.
198. ExperimentsComputeResources
Question: Foreachexperiment,doesthepaperprovidesufficientinformationonthecom-
puterresources(typeofcomputeworkers,memory,timeofexecution)neededtoreproduce
theexperiments?
Answer: [Yes]
Justification: WeprovidehardwareandsoftwaredetailsinAppendixA.1.
9. CodeOfEthics
Question: Doestheresearchconductedinthepaperconform, ineveryrespect, withthe
NeurIPSCodeofEthicshttps://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: TheauthorshavereadtheNeurIPSCodeofEthicsandmadesurethepaper
followstheNeurIPSCodeofEthicsineveryaspect.
10. BroaderImpacts
Question: Does the paper discuss both potential positive societal impacts and negative
societalimpactsoftheworkperformed?
Answer: [NA]
Justification: Thispapersimplyimprovessampling. ExistingLLMapproacheswillbenefit
fromit,buttheworkwillnotdirectlyleadtospecificbroaderimpacts.
11. Safeguards
Question: Doesthepaperdescribesafeguardsthathavebeenputinplaceforresponsible
releaseofdataormodelsthathaveahighriskformisuse(e.g.,pretrainedlanguagemodels,
imagegenerators,orscrapeddatasets)?
Answer: [NA]
Justification: Thepaperposesnosuchrisks.
12. Licensesforexistingassets
Question: Arethecreatorsororiginalownersofassets(e.g.,code,data,models),usedin
thepaper,properlycreditedandarethelicenseandtermsofuseexplicitlymentionedand
properlyrespected?
Answer: [Yes]
Justification: Thepaperproperlycitestheoriginalpaperorsourceswheneveranassetis
used. URLofthemodelcheckpointisincludedinAppendixA.3.
13. NewAssets
Question:Arenewassetsintroducedinthepaperwelldocumentedandisthedocumentation
providedalongsidetheassets?
Answer: [Yes]
Justification: We will attach the datasets as part of our submission and the code will be
releasedwithwell-documentedinstructions.
14. CrowdsourcingandResearchwithHumanSubjects
Question: Forcrowdsourcingexperimentsandresearchwithhumansubjects,doesthepaper
includethefulltextofinstructionsgiventoparticipantsandscreenshots,ifapplicable,as
wellasdetailsaboutcompensation(ifany)?
Answer: [NA]
Justification: Thispaperdoesnotinvolvecrowdsourcingnorresearchwithhumansubjects.
15. InstitutionalReviewBoard(IRB)ApprovalsorEquivalentforResearchwithHuman
Subjects
Question: Doesthepaperdescribepotentialrisksincurredbystudyparticipants,whether
suchrisksweredisclosedtothesubjects,andwhetherInstitutionalReviewBoard(IRB)
approvals(oranequivalentapproval/reviewbasedontherequirementsofyourcountryor
institution)wereobtained?
20Answer: [NA]
Justification: Thispaperdoesnotinvolvecrowdsourcingnorresearchwithhumansubjects.
21