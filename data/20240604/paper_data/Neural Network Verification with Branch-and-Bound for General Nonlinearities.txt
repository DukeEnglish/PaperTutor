Neural Network Verification with Branch-and-Bound
for General Nonlinearities
ZhouxingShi*1, QiruiJin*2, ZicoKolter3, SumanJana4, Cho-JuiHsieh1, HuanZhang5
1UniversityofCalifornia,LosAngeles 2UniversityofMichigan
3CarnegieMellonUniversity 4ColumbiaUniversity 5UniversityofIllinoisUrbana-Champaign
z.shi@ucla.edu, qiruijin@umich.edu
zkolter@cs.cmu.edu, suman@cs.columbia.edu, chohsieh@cs.ucla.edu, huan@huan-zhang.com
*Equalcontribution
Abstract
Branch-and-bound(BaB)isamongthemosteffectivemethodsforneuralnetwork(NN)verification.
However, existing works on BaB have mostly focused on NNs with piecewise linear activations,
especiallyReLUnetworks.Inthispaper,wedevelopageneralframework,namedGenBaB,toconduct
BaBforgeneralnonlinearitiesingeneralcomputationalgraphsbasedonlinearboundpropagation. To
decidewhichneurontobranch,wedesignanewbranchingheuristicwhichleverageslinearbounds
asshortcutstoefficientlyestimatethepotentialimprovementafterbranching. Todecidenontrivial
branchingpointsforgeneralnonlinearfunctions,weproposetooptimizebranchingpointsoffline,
which can be efficiently leveraged during verification with a lookup table. We demonstrate the
effectivenessofourGenBaBonverifyingawiderangeofNNs,includingnetworkswithactivation
functionssuchasSigmoid,Tanh,SineandGeLU,aswellasnetworksinvolvingmulti-dimensional
nonlinearoperationssuchasmultiplicationsinLSTMsandVisionTransformers. Ourframeworkalso
allowstheverificationofgeneralnonlinearcomputationgraphsandenablesverificationapplications
beyond simple neural networks, particularly for AC Optimal Power Flow (ACOPF). GenBaB is
partofthelatestα,β-CROWN,thewinnerofthe4thInternationalVerificationofNeuralNetworks
Competition(VNN-COMP2023).
1 Introduction
Neuralnetwork(NN)verificationaimstoformallyverifywhetheraneuralnetworksatisfiesspecificproperties,suchas
safetyorrobustness,priortoitsdeploymentinsafety-criticalapplications. Verifierstypicallycomputecertifiedbounds
ontheoutputneuronswithinapre-definedinputregion. AscomputingexactboundsisNP-complete(Katzetal.,2017)
evenforsimpleReLUnetworks,itbecomescrucialtorelaxtheboundcomputationprocesstoimprovetheefficiency.
Boundpropagationmethods(Wangetal.,2018b;Wong&Kolter,2018;Zhangetal.,2018;Dvijothametal.,2018;
Henriksen&Lomuscio,2020;Singhetal.,2019b)arecommonlyused,whichrelaxnonlinearitiesinneuralnetworks
intolinearlowerandupperboundswhichcanbeefficientlypropagated.
Toobtaintighterverifiedbounds,Branch-and-Bound(BaB)hasbeenwidelyutilized(Buneletal.,2018,2020;Xuetal.,
2021;Lu&Mudigonda,2020;DePalmaetal.,2021;Wangetal.,2021;Ferrarietal.,2021). However,previousworks
mostlyfocusedonReLUnetworksduetothesimplicityofReLUfromitspiecewiselinearnature. BranchingaReLU
neurononlyrequiresbranchingat0,anditimmediatelybecomeslinearineitherbrancharound0. Conversely,handling
neuralnetworkswithnonlinearitiesbeyondReLUintroducesadditionalcomplexityastheconvenienceofpiecewise
linearitydiminishes. Itisimportantforverifyingmanymodelswithnon-ReLUnonlinearities,includingNNswith
non-ReLUactivationfunctions,NNssuchasLSTMs(Hochreiter&Schmidhuber,1997)andTransformers(Vaswani
etal.,2017)whichhavenonlinearitiesbeyondactivationfunctionsincludingmultiplicationanddivision,verification
applicationssuchasACOptimalPowerFlow(ACOPF)(Guhaetal.,2019)wheretheverificationproblemisdefined
onacomputationalgraphconsistingofaneuralnetworkandalsovariousnonlinearoperatorsencodingthenonlinear
constraintstobeverified. AlthoughsomepreviousworkshaveconsideredBaBforNNsbeyondReLUnetworks,e.g.,
Henriksen&Lomuscio(2020);Wuetal.(2022)consideredBaBonnetworkswithS-shapedactivationssuchasSigmoid,
theseworksstilloftenspecializeinspecificandrelativelysimpletypesofnonlinearities. Amoreprincipledframework
forhandlinggeneralnonlinearitiesislacking,leavingampleroomforfurtheradvancementsinverifyingnon-ReLU
networks.
Preprint.
4202
yaM
13
]GL.sc[
1v36012.5042:viXraInthispaper,weproposeGenBaB,aprincipledneuralnetworkverificationframeworkwithBaBforgeneralnonlin-
earities. ToenableBaBforgeneralnonlinearitiesbeyondReLU,wefirstformulateageneralBaBframework, and
weintroducegeneralbranchingpoints,wherewemaybranchatpointsotherthan0fornonlinearfunctions,hichis
neededwhenthenonlinearityisnotsimplypiecewiselineararound0. Wethenproposeanewbranchingheuristic
named“BoundPropagationwithShortcuts(BBPS)”forbranchinggeneralnonlinearities,whichcarefullyleveragesthe
linearboundsfromboundpropagationasshortcutstoefficientlyandeffectivelyestimatetheboundimprovementfrom
branchinganeuron. Moreover,weproposetodecidenontrivialbranchingpointsbypre-optimizingbranchingpoints
offline,accordingtothetightnessoftheresultedlinearrelaxationforeachpossibilityofpre-activationbounds,andwe
savetheoptimizedbranchingpointsintoalookuptabletobeefficientlyusedwhenverifyinganentireNN.
WedemonstratetheeffectivenessofourGenBaBonavarietyofnetworks,includingfeedforwardnetworkswithSigmoid,
Tanh,Sine,orGeLUactivations,aswellasLSTMsandVisionTransformers(ViTs). Thesemodelsinvolvevarious
nonlinearitiesincludingS-shapedactivations,periodictrigonometricfunctions,andalsomultiplicationanddivision
whicharemulti-dimensionalnonlinearoperationsbeyondactivationfunctions. Wealsoenableverificationonmodelsfor
theACOptimalPowerFlow(ACOPF)application(Guhaetal.,2019). GenBaBisgenerallyeffectiveandoutperforms
existing baselines. The improvement from GenBaB is particularly significant for models involving functions with
strongernonlinearity. Forexample,ona4×100networkwiththeSineactivation,GenBaBimprovestheverification
from4%to60%instancesverified(NNswiththeSineactivationhavebeenproposedforneuralrepresentationsand
neuralrenderinginSitzmannetal.(2020)).
GenBaB has been integrated to the latest version of α,β-CROWN1 which is the winner of the 4th International
VerificationofNeuralNetworksCompetition(VNN-COMP2023). Codeforreproducingtheresultsisavailableat
https://github.com/shizhouxing/GenBaB.
2 Background
TheNNverificationproblem. Letf :Rd (cid:55)→RK beaneuralnetworktakinginputx∈Rdandoutputtingf(x)∈RK.
Suppose C is the input region to be verified, and s : RK (cid:55)→ R is an output specification function, h : Rd (cid:55)→ R is
thefunctionthatcombinestheNNandtheoutputspecificationash(x)=s(f(x)). NNverificationcantypicallybe
formulatedasverifyingifh(x)>0,∀x∈Cprovablyholds. Acommonlyadoptedspecialcaseisrobustnessverification
givenasmallinputregion,wheref(x)isaK-wayclassifierandh(x):=min {f (x)−f (x)}checkstheworst-case
i̸=c c i
marginbetweentheground-truthclasscandanyotherclassi. Theinputregionisoftentakenasasmallℓ -ballwith
∞
radiusϵaroundadatapointx ,i.e.,C := {x | ∥x−x ∥ ≤ϵ}. Thisisasuccinctandusefulproblemforprovably
0 0 ∞
verifyingtherobustnesspropertiesofamodelandalsoforbenchmarkingNNverifiers,althoughthereareotherNN
verificationproblemsbeyondrobustness(Brixetal.,2023). Wemainlyfocusonthissettingforitssimplicityfollowing
priorworks.
Linearboundpropagation. WedevelopourGenBaBbasedonlinearboundpropagation(Zhangetal.,2018;Xuetal.,
2020)forcomputingtheverifiedboundsofeachdomainduringtheBaB.Linearboundpropagationcanlowerbound
(cid:80)
h(x)bypropagatinglinearboundsw.r.t. theoutputofoneormoreintermediatelayersash(x)≥ A xˆ +c,where
i i i
xˆ (i≤n)istheoutputofintermediatelayeriinthenetworkwithnlayers,A arethecoefficientsw.r.t. layeri,andc
i i
isabiasterm. Inthebeginning,thelinearboundissimplyh(x)≥I·h(x)+0whichisactuallyanequality. Inthe
boundpropagation,A xˆ isrecursivelysubstitutedbythelinearboundofxˆ w.r.titsinput. Forsimplicity,supposelayer
i i i
i−1istheinputtolayeriandxˆ =h (xˆ ),whereh (·)isthecomputationforlayeri. Andsupposewehavethe
i i i−1 i
linearboundsofxˆ w.r.titsinputxˆ as:
i i−1
a xˆ +b ≤xˆ =h (xˆ )≤a xˆ +b , (1)
i i−1 i i i i−1 i i−1 i
with parameters a ,b ,a ,b for the linear bounds, and “≤” holds elementwise. Then A xˆ can be substituted
i i i i i i
(cid:0) (cid:1)
and lower bounded by A xˆ ≥ A xˆ + A b +A b , where A = A a +A a , (“+” and “-
i i i−1 i−1 i,+ i i,− i i−1 i,+ i i,− i
” in the subscripts denote taking positive and negative elements, respectively). In this way the linear bounds are
propagatedfromlayeritolayeri−1. Ultimately,thelinearboundscanbepropagatedtotheinputofthenetworkxas
h(x)≥A x+c(A ∈R1×d),wheretheinputcanbeviewedasthe0-thlayer. DependingonC,thislinearboundcan
0 0
beconcretizedintoalowerboundwithoutx. IfC isanℓ ball,wehave
∞
∀∥x−x ∥ ≤ϵ, A x+c≥A x −ϵ∥A ∥ +c. (2)
0 ∞ 0 0 0 0 1
ToobtainEq. (1),ifh isalinearoperator,wesimplyhavea xˆ +b =a xˆ +b =h (xˆ )whichish itself.
i i i−1 i i i−1 i i i−1 i
Otherwise,linearrelaxationisused,whichrelaxesanonlinearityandboundthenonlinearitybylinearfunctions. An
1https://github.com/Verified-Intelligence/alpha-beta-CROWN
2intermediateboundonxˆ asl ≤xˆ ≤u isusuallyrequiredfortherelaxation,whichcanbeobtainedby
i−1 i−1 i−1 i−1
runningadditionalboundpropagationandtreatingtheintermediatelayersastheoutputofanetwork.
3 Method
3.1 OverallFramework
Notations. AlthoughinSection2,weconsideredafeedforwardNNforsimplicity,linearboundpropagationhasbeen
generalizedtogeneralcomputationalgraphs(Xuetal.,2020). Inourmethod,wealsoconsiderageneralcomputational
graph h(x) for input region x ∈ C. Instead of a feedforward network with n layers in Section 2, we consider a
computationalgraphwithnnodes,whereeachnodeicomputessomefunctionh (·)whichmayeithercorrespondtoa
i
linearlayerintheNNoranonlinearity. Weusexˆ todenotetheoutputofnodei,whichmaycontainmanyneurons,and
i
weusexˆ todenotetheoutputofthej-thneuroninnodei. Intermediateboundsofnodeimaybeneededtorelaxand
i,j
boundh (·),andweusel ,u todenotetheintermediatelowerandupperboundrespectively. Weuselanduto
i i,j i,j
denotealltheintermediatelowerboundsandupperbounds,respectively,fortheentirecomputationalgraph.
OverviewofGenBaB. InourGenBaB,webranchtheintermediateboundsofneuronsconnectedtogeneralnonlin-
earities. Wemaintainadynamicpoolofintermediatebounddomains,D = {(l(i),u(i))}m ,wherem = |D|isthe
i=1
numberofcurrentdomains,andinitiallywehaveD ={(l,u)}withtheintermediateboundsfromtheinitialverification
before BaB. Then in each iteration of our BaB, we pop a domain from D, and we select a neuron to branch and a
branchingpointonitsintermediatebounds. Tosupportgeneralnonlinearities,weformulateanewandgeneralbranching
frameworkinSection3.2,whereweintroducegeneralbranchingpoints,incontrasttobranchingReLUactivationsat0
only,andwealsosupportmorecomplicatednetworksarchitectureswhereanonlinearitycaninvolvemultipleinput
nodesoroutputnodes. Inordertodecidewhichneuronwechoosetobranch, weproposeabranchingheuristicin
Section3.3toestimatethepotentialimprovementforeachchoiceofabranchedneuron,wherewecarefullyleverage
linearboundsasanefficientshortcutforamorepreciseestimation. Finally,todecidenontrivialbranchingpoints,in
Section3.4,weproposetopre-optimizethebranchingpointsoffline,whichaimstoproducethetightestlinearrelaxation
aftertakingtheoptimizedbranchingpoint.
Eachbranchingstepgeneratesnewsubdomains. Forthenewsubdomains,weupdatel,uforthebranchedneurons
accordingtothebranchingpoints,andthebranchingdecisionisalsoencodedintotheboundpropagationasadditional
constraintsbyLagrangemultipliersfollowingWangetal.(2021). Foreachnewsubdomain,givenupdatedl,u,we
use V(h,C,l,u) to denote a new verified bound computed with new intermediate bounds l,u. Subdomains with
V(h,C,l,u)>0areverifiedanddiscarded,otherwisetheyareaddedtoDforfurtherbranching. Werepeattheprocess
untilnodomainisleftinDandtheverificationsucceeds,orwhenthetimeoutisreachedandtheverificationfails.
3.2 BranchingforGeneralNonlinearities
Figure1:Illustrationofthemorecomplicatednatureofbranchingforgeneralnonlinearities(branchingaReLUactivation
v.s. branchingfornonlinearitiesinanLSTM).NotationsforpartofanLSTMfollowsPyTorch’sdocumentation(https:
//pytorch.org/docs/stable/generated/torch.nn.LSTM.html). Nodes in orange are being branched. For
generalnonlinearities,branchingpointscanbenon-zero(0.86intheLSTMexamplehere),anonlinearitycantake
multipleinputnodes(f ⊙c here),andanodecanalsobefollowedbymultiplenonlinearities(c isfollowedbya
t+1 t t
multiplicationandalsoTanh,andbranchingc affectsbothtwononlinearities).
t
AsillustratedinFigure1,branchingforgeneralnonlinearitiesongeneralcomputationalgraphsismorecomplicated,
incontrasttoBaBforReLUnetworkswhereitalwaysbranchesat0andReLUislinearonbothsides. Forgeneral
nonlinearities,weneedtoconsiderbranchingatpointsotherthan0. Inaddition,unliketypicalactivationfunctions,
somenonlinearitiesmaytakemorethanoneinputsandtherebyhavemultipleinputnodesthatcanbebranched,suchas
3multiplicationinLSTM(“f ⊙c ”inFigure1)orTransformers(Hochreiter&Schmidhuber,1997;Vaswanietal.,
t+1 t
2017). Ongeneralcomputationalgraphs,anodecanalsobefollowedbymultiplenonlinearities,asappearedinLSTMs
(suchas“c ”inFigure1),andthenbranchingtheintermediateboundsofthisnodecanaffectmultiplenonlinearities.
t
Toresolvethesechallenges,weproposeamoregeneralformulationforbranchingongeneralcomputationalgraphs
withgeneralnonlinearities. Eachtimeweconsiderbranchingtheintermediateboundsofaneuronj inanodei,namely
[l ,u ],ifnodeiistheinputofsomenonlinearity. Weconsiderbranchingtheconcernedneuroninto2branches
i,j i,j
withanontrivialbranchingpointp ,as[l ,u ]→[l ,p ], [p ,u ]. Hereweconsidereachnodeiwhichis
i,j i,j i,j i,j i,j i,j i,j
theinputtoatleastonenonlinearityanddecideifwebranchtheintermediatebounds[l ,u ]ofthisnode,insteadof
i i
consideringeachnonlinearity. Thisformulationallowsustosupportnonlinearitieswithmultipleinputnodesaswellas
multiplenonlinearitiessharingtheinputnode. Inthenexttwosubsections,wewillfurtherdiscusssupportinggeneral
computationalgraphsinourbranchingheuristicandbranchingpoints.
3.3 WhichNeurontoBranch? ANewBranchingHeuristic
Weproposeanewbranchingheuristicwhichisascoringfunctionforefficientlyestimatingthenewverifiedboundafter
branchingataneuronwithabranchingpoint,sothatwecanchooseaneurontobranchwhichpotentiallyleadtoagood
improvementafterthebranching. Specifically,wedesignafunctionV˜(l,u,i,j,k,p )whichestimatesthenewbound
i,j
ofthek-th(1≤k≤2)branch,afterbranchingneuronjinnodeiusingbranchingpointsp . WeuseB(l,u,i,j,k,p )
i,j i,j
todenotetheupdatedintermediateboundsafterthisbranching,andessentiallyweaimtouseV˜(l,u,i,j,k,p )to
i,j
efficiently estimate V(h,C,B(l,u,i,j,k,p )) which is the actual verified bound after the branching, but it is too
i,j
costlytodirectlycomputeanactualverifiedboundforeachbranchingoption.
Supposeweconsiderbranchinganeuronj innodeiandweaimtoestimateV(·)foreachbranchk. Inthelinearbound
propagation,whentheboundsarepropagatedtonodei,wehave:
h(x)≥A(k)xˆ +c(k) ≥V(h,C,B(l,i,j,k,p )), (3)
i,j i,j i,j
whereweuseA(k)andc(k)todenotetheparametersinthelinearboundsforthek-thbranch.Sincewedonotupdatethe
i,j
intermediateboundsexceptforthebranchedneuronsduringBaBforefficiencyfollowingWangetal.(2021),branching
aneuroninnodeionlyaffectsthelinearrelaxationofnonlinearnodesimmediatelyafternodei(i.e.,outputnodesof
i). Therefore,A(k)andc(k)canbecomputedbyonlypropagatingthelinearboundsfromtheoutputnodesofi,using
i,j
previouslystoredlinearbounds,ratherthanfromthefinaloutputofh(x).
Foramoreefficientestimation,insteadofpropagatingthelinearboundstowardstheinputofthenetworkstepbystep,
we propose a new branching heuristic named Bound Propagation with Shortcuts (BBPS), where we use a shortcut
todirectlypropagatetheboundstotheinput. Specifically,wesavethelinearboundsofallthepotentiallybranched
intermediatelayersduringtheinitialverificationbeforeBaB.Foreveryneuronj inintermediatenodei,werecord:
∀x∈C, Aˆ x+cˆ ≤xˆ ≤Aˆ x+cˆ , (4)
ij ij ij ij ij
whereAˆ ,cˆ ,Aˆ ,cˆ areparametersforthelinearbounds. Theseareobtainedwhenlinearboundpropagationisused
ij ij ij ij
forcomputingtheintermediatebounds[l ,u ]andthelinearboundsarepropagatedtotheinputx. WethenuseEq.
i,j i,j
(4)tocomputealowerboundforA(k)xˆ +c(k):
i,j i,j
A(k)xˆ +c(k) ≥(A(k) Aˆ +A(k) Aˆ )x+A(k) cˆ +A(k) cˆ +c(k). (5)
i,j i,j i,j,+ ij i,j,− ij i,j,+ ij i,j,− ij
Theright-hand-sidecanbeconcretizedbyEq. (2)toserveasanapproximationforV(·)afterthebranching. Inthisway,
thelinearboundsaredirectlypropagatedfromnodeitoinputxandconcretizedusingashortcut. Wetherebytakethe
concretizedboundasV˜(l,u,i,j,k,p )forourBBPSheuristicscore.
i,j
Ourbranchingheuristicisgenerallyformulated. Weleverageupdatesonthelinearrelaxationofanynonlinearity,and
generalbranchingpointsandgeneralnumberofinputsnodesaresupportedwhenweupdatethelinearrelaxation. Node
icanalsohavemultipleoutputnodeswhicharenonlinearities,whereweaccumulatethelinearboundspropagatedfrom
alltheoutputnodesinEq. (3).
Comparisontopreviousworks. Existingbranchingheuristicsfrompreviousworks(Buneletal.,2018,2020;Lu
&Mudigonda,2020;DePalmaetal.,2021)aremorerestrictive,astheymostlyfocusedonbranchingReLUneurons
withafixedbranchingpoint(0forReLU)andtheirheuristicisspecificallyformulatedforReLU,unlikeourgeneral
formulationabove. Wealsoempiricallyfindtheyareoftennotpreciseenoughongeneralnonlinearitiesduetotheir
4approximations. IntheexistingBaBSRheuristicoriginallyforReLUnetworks(Buneletal.,2020),theyessentially
propagatetheboundsonlytothenodebeforethebranchedonewithanearlystop,andtheythenignorethecoefficients
(A(k) forafeedforwardNN)withoutpropagatingfurther. Incontrast,inourBBPSheuristic,wecarefullyutilizea
i−1,j
shortcuttopropagatetheboundstotheinputasEq. (5)ratherthandiscardlineartermsearly. Therefore,weexpectour
BBPSheuristictobemorepreciseandeffective,whileitdoesnotincreasethecomplexitywiththeefficientshortcut.
3.4 WheretoBranch? NewConsiderationsforGeneralNonlinearFunctions
1.5 1.5
Sin(x) Sin(x) Optimal branching point
1.0 Branching at 0.50 1.0 Branching at 1.52 5.0 Tightness loss
Linear relaxation Linear relaxation 4.5
x [--2.0, 3.0] x [--2.0, 3.0]
0.5 0.5
4.0
0.0 0.0 3.5
0.5 0.5 3.0
2.5
1.0 1.0
2.0
1.5 1.5
4 3 2 1 0 1 2 3 4 4 3 2 1 0 1 2 3 4 2 1 0 1 2 3
x x Branching point
(a)BranchingaSineactivationinthemid- (b)BranchingaSineatourpre-optimized (c)ThetightnesslossdefinedinEq.(6)for
dle. branchingpoint. differentbranchingpoints.
Figure2: IllustrationofbranchingtheintermediateboundsofaneuronconnectedtotheSineactivation(Sitzmannetal.,
2020).
Themorecomplexnatureofgeneralnonlinearfunctionsalsobringsflexibilityonchoosingbranchingpoints,compared
to the ReLU activation where only branching at 0 is reasonable. A straightforward way is to branch in the middle
betweentheintermediatelowerandupperbounds,asshowninFigure2a. However,thiscanbesuboptimalformany
nonlinear functions. In contrast, we perform an optimization offline for each case of nonlinearities before running
ourBaB.WealsoconsidercaseswheremultiplenonlinearitiesareconnectedtosamenodeasshowninFigure1. We
enumerateallpairsofpossibleintermediateboundswithinacertainrangewithastepsize,wherewesetasmallstep
sizewhichdefinesthegapbetweentheadjacentenumeratedintermediatebounds. Wesavetheoptimizedbranching
pointsintoalookuptable. Duringverification,foreachpairofintermediateboundsweactuallyencounter,weefficiently
querythelookuptableandtakethebranchingpointfortheclosestintermediateboundpairinthelookuptable(ifno
validbranchingpointisobtainedfromthelookuptable,wetrybranchinginthemiddleinstead,asabackup). Wecallit
“pre-optimizedbranchingpoints”,asanexampleshowninFigure2b.
Forsimplicityhere,wemainlyassumethatwehaveaunarynonlinearfunctionq(x),althoughourmethodsupports
functionswithanynumberofinputsinpractice. Supposetheinputintermediateboundsforq(x)isl≤x≤u,weaim
tofindabranchingpointp = P(l,u)suchthattheoveralltightnessofthelinearrelaxationforinputrange[l,p]and
[p,u],respectively,isthebest. Supposethelinearrelaxationforinputrange[l,p]isa x+b ≤q(x)≤a x+b ,and
1 1 1 1
similarlya x+b ≤q(x)≤a x+b forinputrange[p,u]. FollowingpreviousworkssuchasShietal.(2020),we
2 2 2 2
usetheintegralofthegapbetweenthelowerlinearrelaxationandtheupperlinearrelaxationtomeasurethetightness
(thelinearrelaxationisconsideredastighterwhenthegapissmaller). WedefineitasatightnesslossP(q(x),l,u,p)
fornonlinearityq(x)withinputrange[l,u]andbranchingpointp:
(cid:90) p(cid:18) (cid:19) (cid:90) u(cid:18) (cid:19)
(cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1)
P(q(x),l,u,p)= a x+b − a x+b dx+ a x+b − a x+b dx, (6)
1 1 1 1 2 2 2 2
l p
wheretheparametersforthelinearrelaxation(a ,a ,b ,b ,a ,a ,b ,b )alldependonp. Wetakethebestbranching
1 1 1 1 2 2 2 2
pointp(l <p<u)whichminimizesP(q(x),l,u,p). Figure2cplotsthetightnesslossfortheSineactivation. This
problem can be solved by gradient descent, or an enumeration over a number of potential branching points if the
nonlinearfunctiononlyhasoneortwoinputs.
Moreover, we also support a generalized version of Eq. (6) for nonlinear functions with multiple inputs (such as
multiplicationinvolvingtwoinputs),whereweuseamultipleintegraltomeasurethetightnessformulti-dimensional
nonlinearities. Andwhenabranchednodehasmultiplenonlinearoutputnodes,wetakethesumformultiplenonlineari-
(cid:80)
tiesas P(q(x),l,u,p),whereQisthesetofoutputnonlinearities. Assuch,ourpre-optimizedbranchingpoints
q∈Q
supportgeneralcomputationalgraphs.
54 Experiments
Table1: Listofmodelswithvariousnonlinearitiesinourexperiments.
Model Nonlinearitiesinthemodel
Feedforward sigmoid,tanh,sin,GeLU
LSTM sigmoid,tanh,xy
√
ViTwithReLU ReLU,xy,x/y,x2, x,exp(x)
ML4ACOPF ReLU,sigmoid,sin,xy,x2
Models and Data. We focus on verifying NNs with nonlinearities beyond ReLU, and we experiment on models
withvariousnonlinearitiesasshowninTable1. Wemainlyconsiderthecommonlyusedℓ robustnessverification
∞
specificationonimageclassification. WeadoptsomeMNIST(LeCunetal.,2010)modelswithSigmoidandTanh
activationfunctionsfrompreviousworks(Singhetal.,2019a,b;Mülleretal.,2022),alongwiththeirdatainstances.
Besides, we train many new models with various nonlinearities on CIFAR-10 (Krizhevsky et al., 2009) by PGD
adversarialtraining(Madryetal.,2018),usinganℓ perturbationwithϵ = 1/255inbothtrainingandverification.
∞
FortheseCIFAR-10models,wefirstrunvanillaCROWN(Zhangetal.,2020;Xuetal.,2020),i.e.,CROWNwithout
optimizedlinearrelaxation(Xuetal.,2021;Lyuetal.,2020)orBaB(Xuetal.,2021;Wangetal.,2021),toremove
instanceswhicharetooeasywherevanillaCROWNalreadysucceeds. WealsoremoveinstanceswherePGDattack
succeeds,assuchinstancesareimpossibletoverify. Weonlyretainthefirst100instancesiftherearemoreinstances
left. Wesetatimeoutof300secondsforourBaBinalltheseexperiments. Inaddition,weadoptanNNverification
benchmarkforverifyingpropertiesintheMachineLearningforACOptimalPowerFlow(ML4ACOPF)problem(Guha
etal.,2019)2whichisbeyondrobustnessverification. Forcompleteness,wealsoshowresultsonaReLUnetworkin
AppendixB.2. AdditionaldetailsareinAppendixC.
Baselines. WecompareourGenBaBwiththepreviousα,β-CROWNwhichdoesnotsupportBaBonnon-ReLU
nonlinearities. Wealsocomparewithseveralotherbaselines,includingDeepPoly(Singhetal.,2019b),PRIMA(Müller
etal.,2022),VeriNet(Henriksen&Lomuscio,2020),PROVER(Ryouetal.,2021),DeepT(Bonaertetal.,2021),Wu
etal.(2022),Weietal.(2023),onthemodelstheysupport,respectively. Amongthesebaselines,onlyVeriNetandWu
etal.(2022)supportBaBonSigmoidorTanhmodels,andnoneofthebaselinesupportsBaBongeneralnonlinearities.
While the original BaBSR heuristic in Bunel et al. (2020) only supported ReLU networks, we also implemented a
generalizedversionofBaBSRfornonlinearitiesbeyondReLUforanempiricalcomparisoninTable3,basedonthe
differenceintreatingthelineartermdiscussedinSection3.3.
Experiments on Sigmoid and Tanh networks for MNIST. We first experiment on Sigmoid networks and Tanh
networksforMNISTandshowtheresultsinTable2. On6outofthe8models,ourGenBaBisabletoverifymore
instancesoverα,β-CROWNwithoutBaBandalsooutperformsallthenon-CROWNbaselines. Wefindthatimproving
onSigmoid9×100andTanh6×100networksbyBaBisharder,astheinitialboundsaretypicallytoolooseonthe
unverifiableinstancesbeforeBaB,possiblyduetothesemodelsbeingtrainedwithoutrobustnessconsideration.
ExperimentsonfeedforwardnetworkswithvariousactivationfunctionsonCIFAR-10. InTable3andFigure3,
weshowresultsformodelsonCIFAR-10. Onallthemodels,GenBaBverifiesmuchmoreinstancescomparedtoα,β-
CROWNwithoutBaB.WealsoconductablationstudiestoinvestigatetheeffectofourBBPSheuristicandbranching
points,withresultsshowninthelastthreerowsofTable3. Comparing“BaseBaB”and“+BBPS”,onmostofthe
models,wefindthatourBBPSheuristicsignificantlyimprovesoverdirectlygeneralizingtheBaBSRheuristic(Bunel
etal.,2020)usedin“BaseBaB”.Comparing“+BBPS”and“+BBPS,+pre-optimized”,wefindthatourpre-optimized
branchingpointsachieveanoticeableimprovementonmanymodelsoveralwaysbranchinginthemiddleusedin“+
BBPS”.TheresultsdemonstratetheeffectivenessofourGenBaBwithourBBPSheuristicandpre-optimizedbranching
points. For PRIMA and vanilla CROWN, as we only use relatively hard instances for verification here, these two
methodsareunabletoverifyanyinstanceinthisexperiment. ForVeriNet,allthemodelsherearetoolargewithouta
licensefortheFICOXpresssolver(weareunabletoobtainanacademiclicenseasmentionedinTable2);wehavenot
obtainedthecodetorunWuetal.(2022)onthesemodelseither. Thus,wedonotincludetheresultsforVeriNetorWu
etal.(2022).
ExperimentsonLSTMs. Next,weexperimentonLSTMscontainingmorecomplexnonlinearities,includingboth
SigmoidandTanhactivations,aswellasmultiplicationassigmoid(x)tanh(y)andsigmoid(x)y. Wecomparewith
PROVER(Ryouetal.,2021)whichisaspecializedforRNNsanditoutperformsearlierworks(Koetal.,2019). While
2RepositoryfortheML4ACOPFbenchmark:https://github.com/AI4OPT/ml4acopf_benchmark.
6Table2: Numberofverifiedinstancesoutofthefirst100testexamplesonMNISTforseveralSigmoidnetworksand
Tanhnetworksalongwiththeirϵ. ThesettingsarethesameasthoseinPRIMA(Mülleretal.,2022). “L×W”inthe
networknamesdenoteafully-connectedNNwithLlayersandW hiddenneuronsineachlayer. Theupperboundsin
thelastrowarecomputedbyPGDattack(Madryetal.,2018),asasoundverificationshouldnotverifyinstanceswhere
PGDcansuccessfullyfindcounterexamples.
SigmoidNetworks TanhNetworks
Method 6×100 6×200 9×100 ConvSmall 6×100 6×200 9×100 ConvSmall
ϵ=0.015 ϵ=0.012 ϵ=0.015 ϵ=0.014 ϵ=0.006 ϵ=0.002 ϵ=0.006 ϵ=0.005
DeepPolyab 30 43 38 30 38 39 18 16
PRIMAa 53 73 56 51 61 68 52 30
VeriNetc 65 81 56 - 31 30 16 -
Wuetal.(2022)? 65 75 96? 63 - - - -
VanillaCROWNb 53 63 49 65 18 24 44 55
α,β-CROWN(w/oBaB) 62 81 62 84 65 72 58 69
GenBaB(ours) 71 83 62 92 65 78 59 75
Upperbound 93 99 92 97 94 97 96 98
aResultsforDeepPolyandPRIMAaredirectlyfromMülleretal.(2022).
bWhileDeepPolyandCROWNarethoughttobeequivalentonReLUnetworks(Mülleretal.,2022),thesetwoworksadoptdifferent
relaxationforSigmoidandTanh,whichresultsindifferentresultshere.
cResultsforVeriNetareobtainedbyrunningthetool(https://github.com/vas-group-imperial/VeriNet)byourselves.
VeriNetdependsontheFICOXpresscommercialsolverwhichrequiresalicenseformodelsthatarerelativelylarge.FICOXpress
declinedtherequestwesubmittedfortheacademiclicense,directingustoobtainitviaa(course)tutor,whichisnotapplicabletoour
research.ThusresultsonConvSmallmodelsarenotavailable.
?WefoundthattheresultWuetal.(2022)reportedontheSigmoid9×100modelexceedstheupperboundbyPGDattack(96>92),
andthustheresulttendstobenotfullyvalid.ResultsonTanhnetworksareunavailable.
Table 3: Number of verified instances out of 100 filtered instances on CIFAR-10 with ϵ = 1/255 for feedforward
networkswithvariousactivationfunctions. Thelastthreerowscontainresultsfortheablationstudy,where“BaseBaB”
doesnotuseourBBPSheuristicorpre-optimizedbranchingpoints,butitusesageneralizedBaBSRheuristic(Bunel
etal.,2020)andalwaysbranchesintermediatebounds[l,u]inthemiddleinto[l,l+u]and[l+u,u].
2 2
SigmoidNetworks TanhNetworks SineNetworks GeLUNetworks
Method
4×100 4×500 6×100 6×200 4×100 6×100 4×100 4×200 4×500 4×100 4×200 4×500
PRIMAa 0 0 0 0 0 0 - - - - - -
VanillaCROWNb 0 0 0 0 0 0 0 0 0 0 0 0
α,β-CROWN(w/oBaB)c 28 16 43 39 25 6 4 2 4 44 33 27
GenBaB(ours) 58 24 64 50 49 10 60 35 22 82 65 39
AblationStudies
BaseBaB 34 19 44 41 34 8 9 8 7 64 54 39
+BBPS 57 24 63 49 48 10 56 34 21 74 59 36
+BBPS,+pre-optimized 58 24 64 50 49 10 60 35 22 82 65 39
aResultsforPRIMAareobtainedbyrunningERAN(https://github.com/eth-sri/eran)whichcontainsPRIMA.PRIMA
doesnotsupportSineorGeLUactivations.
bWehaveextendedthesupportofvanillaCROWNtoGeLU,asdiscussedinAppendixA.3.
cWehaveextendedoptimizablelinearrelaxationinα,β-CROWNtoSineandGeLU,asdiscussedinAppendixA.
thereareotherworksonverifyingRNNandLSTM,suchasDuetal.(2021);Mohammadinejadetal.(2021);Paulsen
&Wang(2022),wehavenotobtainedtheircode,andwealsomakeorthogonalcontributionscomparedtothemon
improvingtherelaxationforRNNverificationwhichcanalsobecombinedwithourBaB.Wetakethehardestmodel,
anLSTMforMNIST,fromthemainexperimentsofPROVER(othermodelscanbeverifiedbyPROVERonmore
than90%instancesandarethusomitted),whereeach28×28imageisslicedinto7framesforLSTM.Wealsohave
twoLSTMstrainedbyourselvesonCIFAR-10,wherewelinearlymapeach32×32imageinto4patchesastheinput
tokens,similartoViTswithpatches(Dosovitskiyetal.,2021). Table4showstheresults. α,β-CROWNwithoutBaB
canalreadyoutperformPROVERwithspecializedrelaxationforRNNandLSTM.OurGenBaBoutperformsboth
PROVERandα,β-CROWNwithoutBaB.
ExperimentsonViTs. WealsoexperimentonViTswhichcontainmoreothernonlinearities,asshowninTable1.
ForViTs,wecomparewithDeepT(Bonaertetal.,2021)whichisspecializedforverifyingTransformerswithoutBaB.
WeshowtheresultsinTable4,whereourmethodsoutperformDeepT,andBaBeffectivelyimprovestheverification.
Usingourpre-optimizedbranchingpointsalsobringsanoticeableimprovementcomparedtobranchinginthemiddle.
7Sigmoid networks Tanh networks Sine networks GeLU networks
200 60
, -CROWN w/o BaB
150 100 150 B Ba as se e B Ba aB B + BBPS
40 75 GenBaB (BBPS, pre-optimized)
100 100
50
20
50 25 50
0 0 0 0
0 100 200 300 0 100 200 300 0 100 200 300 0 100 200 300
Time (seconds) Time (seconds) Time (seconds) Time (seconds)
Figure3: TotalnumberofverifiedinstancesagainstrunningtimethresholdonfeedforwardnetworksforCIFAR-10with
variousactivationfunctions. “BaseBaB”meansthatinthemostbasicBaBsetting,whereweuseageneralizedBaBSR
heuristicandalwaysbranchinthemiddlepointofintermediatebounds. “Base+BBPS”usesourBBPSheuristic. Our
fullGenBaBusesbothBBPSandpre-optimizedbranchingpoints.
Table 4: Number of verified instances out of 100 instances on LSTMs and ViTs. The MNIST model is from
PROVER (Ryou et al., 2021) with ϵ = 0.01, and the CIFAR-10 models are trained by ourselves with ϵ = 1/255.
“LSTM-7-32” indicates an LSTM with 7 input frames and 32 hidden neurons, similar for the other two mod-
els. “ViT-L-H” stands for L layers and H heads. Some models have fewer than 100 instances, after filter-
ing out easy or impossible instances, as shown in “upper bounds”. Results for PROVER are obtained by run-
ning the tool (https://github.com/eth-sri/prover). Results for DeepT are obtained by running the tool
(https://github.com/eth-sri/DeepT). PROVER and DeepT specialize in RNNs and ViTs, respectively. We
also show results when we branch in the middle without using our pre-optimized branching points (“GenBaB w/o
pre-optimized”).
MNISTModel CIFAR-10Models
Method
LSTM-7-32 LSTM-4-32 LSTM-4-64 ViT-1-3 ViT-1-6 ViT-2-3 ViT-2-6
PROVER 63 8 3 - - - -
DeepT - - - 0 1 0 1
α,β-CROWNw/oBaB 82 16 9 1 3 11 7
GenBaBw/opre-optimized 84 19 13 29 33 36 25
GenBaB 84 20 14 39 44 37 27
Upperbound 98 100 100 67 92 72 69
Moreover,inAppendixB.1,wecomparewithWeietal.(2023)whichsupportsverifyingattentionnetworksbutnotthe
entireViT,andweexperimentonmodelsfromWeietal.(2023),whereourmethodsalsooutperformWeietal.(2023).
ExperimentsonML4ACOPF. Finally,weexperimentonmodelsfortheMachineLearningforACOptimalPower
Flow(ML4ACOPF)problem(Guhaetal.,2019),andweadopttheML4ACOPFneuralnetworkverificationbenchmark,
astandardizedbenchmarkinthe2023InternationalVerificationofNeuralNetworksCompetition(VNN-COMP’23).
ThebenchmarkconsistsofaNNwithpowerdemandsasinputs,andtheoutputoftheNNgivesanoperationplanof
electricpowerplants. Then,thebenchmarkaimstocheckforafewnonlinearconstraintviolationsofthisplan,suchas
powergenerationandbalanceconstraints. Theseconstraints,aspartofthecomputationalgraphtoverify,involvemany
nonlinearitiesincludingSine,Sigmoid,multiplication,andsquarefunction. Ourframeworkisthefirsttosupportthis
verificationproblem. Amongthe23benchmarkinstances,PGDattackonlysucceedsononeinstance,andourBaB
verifiesalltheremaining22instances;withoutBaB,optimizingαonlycanverifyonly16instancesinthisbenchmark.
Thisshowsamorepracticalapplicationofourworkandfurtherdemonstratestheeffectivenessofourframework.
5 RelatedWork
Branch-and-bound(BaB)hasbeenshowntobeaneffectivetechniqueforNNverification(Buneletal.,2018;Lu&
Mudigonda,2020;Wangetal.,2018a;Xuetal.,2021;DePalmaetal.,2021;Kouvaros&Lomuscio,2021;Wangetal.,
2021;Henriksen&Lomuscio,2021;Shietal.,2022),butmostoftheexistingworksfocusonReLUnetworksandarenot
directlyapplicabletonetworkswithnonlinearitiesbeyondReLU.OnBaBforNNswithothernonlinearities,Henriksen
&Lomuscio(2020)conductedBaBonSigmoidandTanhnetworks,buttheirframeworkdependsonacommercialLP
solverwhichhasbeenarguedaslesseffectivethanrecentNNverificationmethodsusinglinearboundpropagation
withbranchingconstraints(Wangetal.,2021). Besides,Wuetal.(2022)studiedverifyingSigmoidnetworkswith
counter-example-guidedabstractionrefinement. TheseworkshaveonlyconsideredS-shapedactivations,andtherelacks
8
secnatsni
deifirev
fo
rebmuNageneralframeworksupportinggeneralnonlinearitiesbeyondaparticulartypeofactivationfunctions,whichweaddress
inthispaper. WithoutusingBaB,therearemanyworksstudyingtheverificationforNNswithvariousnonlinearities,by
improvingtherelaxationorextendingthesupportofverificationtovariousarchitectures: Sigmoidnetworks(Choietal.,
2023),RNNsandLSTMs(Koetal.,2019;Duetal.,2021;Ryouetal.,2021;Mohammadinejadetal.,2021;Zhang
etal.,2023;Tranetal.,2023),andTransformers(Shietal.,2019;Bonaertetal.,2021;Weietal.,2023;Zhangetal.,
2024). TheseworkshaveorthogonalcontributionscomparedtooursusingBaBforfurtherimprovementaboveabase
verifier. Inaddition,thereareworksstudyingthebranchingheuristicinverifyingReLUnetworks,suchasfiltering
initialcandidateswithamoreaccuratecomputation(DePalmaetal.,2021), usingGraphNeuralNetworksforthe
heuristic(Lu&Mudigonda,2020),orusingaheuristicguidedwithtightermultiple-neuronrelaxation(Ferrarietal.,
2021),whichmayinspirefutureimprovementontheBaBforgeneralnonlinearities.
6 Conclusions
To conclude, we propose a general BaB framework for NN verification involving general nonlinearities in general
computational graphs. We also propose a new branching heuristic for deciding branched neurons and an offline
optimization procedure for deciding branching points. Experiments on verifying NNs with various nonlinearities
demonstratetheeffectivenessofourmethod. WebelieveourworkcanenablemoreapplicationsofNNverification
involvingvariousnonlinearities.
Limitationsandfuturework. AlthoughwehavedemonstratedapracticalapplicationonML4ACOPF,mostofour
experimentshavefocusedonthetypicalrobustnessverificationforimageclassifiers. Webelieveourworkhasthe
potentialforbroaderapplicationsrequiringformallyverifyingneuralnetworksinvariousotherdomains,whichwillbe
interestingforfutureworks. Inaddition,manymodelswehavedemonstratedarestillmuchsmallerthanthosedeployed
inlarge-scalemachinelearningapplications,whichisalsoacommonlimitationofcurrentNNverificationmethods.
FurtherscalingupNNverificationtolargermodelswillbeimportantforfutureworks.
Acknowledgments
ThisprojectissupportedinpartbyNSF2048280,2331966,2331967andONRN00014-23-1-2300:P00001. Huan
ZhangissupportedinpartbytheAI2050programatSchmidtSciences(Grant#G-23-65921).
References
Bonaert,G.,Dimitrov,D.I.,Baader,M.,andVechev,M. Fastandprecisecertificationoftransformers. InProceedings
ofthe42ndACMSIGPLANInternationalConferenceonProgrammingLanguageDesignandImplementation,pp.
466–481,2021.
Brix, C., Bak, S., Liu, C., and Johnson, T. T. The fourth international verification of neural networks competition
(vnn-comp2023): Summaryandresults. arXivpreprintarXiv:2312.16760,2023.
Bunel,R.,Turkaslan,I.,Torr,P.H.S.,Kohli,P.,andMudigonda,P.K. Aunifiedviewofpiecewiselinearneuralnetwork
verification. InAdvancesinNeuralInformationProcessingSystems,pp.4795–4804,2018.
Bunel,R.,Mudigonda,P.,Turkaslan,I.,Torr,P.,Lu,J.,andKohli,P. Branchandboundforpiecewiselinearneural
networkverification. JournalofMachineLearningResearch,21(2020),2020.
Choi,S.W.,Ivashchenko,M.,Nguyen,L.V.,andTran,H.-D. Reachabilityanalysisofsigmoidalneuralnetworks. ACM
TransactionsonEmbeddedComputingSystems,2023.
DePalma,A.,Bunel,R.,Desmaison,A.,Dvijotham,K.,Kohli,P.,Torr,P.H.,andKumar,M.P. Improvedbranchand
boundforneuralnetworkverificationvialagrangiandecomposition. arXivpreprintarXiv:2104.06718,2021.
Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,T.,Dehghani,M.,Minderer,M.,
Heigold, G., Gelly, S., Uszkoreit, J., andHoulsby, N. Animageisworth16x16words: Transformersforimage
recognitionatscale. InInternationalConferenceonLearningRepresentations,2021.
Du,T.,Ji,S.,Shen,L.,Zhang,Y.,Li,J.,Shi,J.,Fang,C.,Yin,J.,Beyah,R.,andWang,T. Cert-rnn: Towardscertifying
therobustnessofrecurrentneuralnetworks. InProceedingsofthe2021ACMSIGSACConferenceonComputerand
CommunicationsSecurity,CCS’21,pp.516–534,2021. ISBN9781450384544. doi: 10.1145/3460120.3484538.
9Dvijotham,K.,Stanforth,R.,Gowal,S.,Mann,T.A.,andKohli,P. Adualapproachtoscalableverificationofdeep
networks. In Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI 2018,
Monterey,California,USA,August6-10,2018,pp.550–559,2018.
Ferrari,C.,Mueller,M.N.,Jovanovic´,N.,andVechev,M. Completeverificationviamulti-neuronrelaxationguided
branch-and-bound. InInternationalConferenceonLearningRepresentations,2021.
Guha, N., Wang, Z., Wytock, M., and Majumdar, A. Machine learning for ac optimal power flow. arXiv preprint
arXiv:1910.08842,2019.
Henriksen,P.andLomuscio,A. Efficientneuralnetworkverificationviaadaptiverefinementandadversarialsearch. In
ECAI2020,pp.2513–2520.IOSPress,2020.
Henriksen,P.andLomuscio,A. Deepsplit: Anefficientsplittingmethodforneuralnetworkverificationviaindirect
effectanalysis. InIJCAI,pp.2549–2555,2021.
Hochreiter,S.andSchmidhuber,J. Longshort-termmemory. Neuralcomputation,9(8):1735–1780,1997.
Katz,G.,Barrett,C.,Dill,D.L.,Julian,K.,andKochenderfer,M.J. Reluplex: Anefficientsmtsolverforverifying
deepneuralnetworks. InInternationalConferenceonComputerAidedVerification,pp.97–117,2017.
Ko,C.,Lyu,Z.,Weng,L.,Daniel,L.,Wong,N.,andLin,D. POPQORN:quantifyingrobustnessofrecurrentneural
networks. In International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning
Research,pp.3468–3477,2019.
Kouvaros,P.andLomuscio,A. Towardsscalablecompleteverificationofreluneuralnetworksviadependency-based
branching. InIJCAI,pp.2643–2650,2021.
Krizhevsky,A.,Hinton,G.,etal. Learningmultiplelayersoffeaturesfromtinyimages. TechnicalReportTR-2009,
2009.
LeCun, Y., Cortes, C., and Burges, C. Mnist handwritten digit database. ATT Labs [Online]. Available:
http://yann.lecun.com/exdb/mnist,2,2010.
Lu,J.andMudigonda,P. Neuralnetworkbranchingforneuralnetworkverification. InProceedingsoftheInternational
ConferenceonLearningRepresentations(ICLR2020).OpenReview,2020.
Lyu,Z.,Ko,C.,Kong,Z.,Wong,N.,Lin,D.,andDaniel,L. FastenedCROWN:tightenedneuralnetworkrobustness
certificates. InTheThirty-FourthAAAIConferenceonArtificialIntelligence,pp.5037–5044,2020.
Madry,A.,Makelov,A.,Schmidt,L.,Tsipras,D.,andVladu,A. Towardsdeeplearningmodelsresistanttoadversarial
attacks. InInternationalConferenceonLearningRepresentations,2018.
Mohammadinejad,S.,Paulsen,B.,Deshmukh,J.V.,andWang,C. Diffrnn: Differentialverificationofrecurrentneural
networks. InFormalModelingandAnalysisofTimedSystems: 19thInternationalConference,FORMATS2021,
Paris,France,August24–26,2021,Proceedings19,pp.117–134.Springer,2021.
Müller,M.N.,Makarchuk,G.,Singh,G.,Püschel,M.,andVechev,M. Prima: generalandpreciseneuralnetwork
certificationviascalableconvexhullapproximations. ProceedingsoftheACMonProgrammingLanguages,6(POPL):
1–33,2022.
Paulsen,B.andWang,C. Linsyn: Synthesizingtightlinearboundsforarbitraryneuralnetworkactivationfunctions. In
ToolsandAlgorithmsfortheConstructionandAnalysisofSystems: 28thInternationalConference,TACAS2022,
HeldasPartoftheEuropeanJointConferencesonTheoryandPracticeofSoftware,ETAPS2022,Munich,Germany,
April2–7,2022,Proceedings,PartI,pp.357–376.Springer,2022.
Ryou,W.,Chen,J.,Balunovic,M.,Singh,G.,Dan,A.,andVechev,M. Scalablepolyhedralverificationofrecurrent
neuralnetworks. InInternationalConferenceonComputerAidedVerification,pp.225–248,2021.
Shi,Z.,Zhang,H.,Chang,K.-W.,Huang,M.,andHsieh,C.-J. Robustnessverificationfortransformers. InInternational
ConferenceonLearningRepresentations,2019.
Shi,Z.,Zhang,H.,Chang,K.,Huang,M.,andHsieh,C. Robustnessverificationfortransformers. InInternational
ConferenceonLearningRepresentations,2020.
10Shi,Z.,Wang,Y.,Zhang,H.,Kolter,J.Z.,andHsieh,C.-J. Efficientlycomputinglocallipschitzconstantsofneural
networksviaboundpropagation. AdvancesinNeuralInformationProcessingSystems,35:2350–2364,2022.
Singh,G.,Ganvir,R.,Püschel,M.,andVechev,M.T. Beyondthesingleneuronconvexbarrierforneuralnetwork
certification. InAdvancesinNeuralInformationProcessingSystems,pp.15072–15083,2019a.
Singh,G.,Gehr,T.,Püschel,M.,andVechev,M. Anabstractdomainforcertifyingneuralnetworks. Proceedingsofthe
ACMonProgrammingLanguages,3(POPL):41,2019b.
Sitzmann,V.,Martel,J.,Bergman,A.,Lindell,D.,andWetzstein,G. Implicitneuralrepresentationswithperiodic
activationfunctions. AdvancesinNeuralInformationProcessingSystems,33:7462–7473,2020.
Tran, H. D., Choi, S. W., Yang, X., Yamaguchi, T., Hoxha, B., and Prokhorov, D. Verification of recurrent neural
networks with star reachability. In Proceedings of the 26th ACM International Conference on Hybrid Systems:
ComputationandControl,pp.1–13,2023.
Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,L.,andPolosukhin,I. Attentionis
allyouneed. InAdvancesinNeuralInformationProcessingSystems30: AnnualConferenceonNeuralInformation
ProcessingSystems2017,December4-9,2017,LongBeach,CA,USA,pp.5998–6008,2017.
Wang,S.,Pei,K.,Whitehouse,J.,Yang,J.,andJana,S. Efficientformalsafetyanalysisofneuralnetworks. InAdvances
inNeuralInformationProcessingSystems,pp.6369–6379,2018a.
Wang,S.,Pei,K.,Whitehouse,J.,Yang,J.,andJana,S. Formalsecurityanalysisofneuralnetworksusingsymbolic
intervals. In27th{USENIX}SecuritySymposium({USENIX}Security18),pp.1599–1614,2018b.
Wang,S.,Zhang,H.,Xu,K.,Lin,X.,Jana,S.,Hsieh,C.-J.,andKolter,J.Z. Beta-crown: Efficientboundpropagation
with per-neuron split constraints for neural network robustness verification. Advances in Neural Information
ProcessingSystems,34:29909–29921,2021.
Wei, D., Wu, H., Wu, M., Chen, P.-Y., Barrett, C., and Farchi, E. Convex bounds on the softmax function with
applications to robustness verification. In International Conference on Artificial Intelligence and Statistics, pp.
6853–6878.PMLR,2023.
Wong,E.andKolter,J.Z. Provabledefensesagainstadversarialexamplesviatheconvexouteradversarialpolytope.
InInternationalConferenceonMachineLearning,volume80ofProceedingsofMachineLearningResearch,pp.
5283–5292,2018.
Wu,H.,Tagomori,T.,Robey,A.,Yang,F.,Matni,N.,Pappas,G.,Hassani,H.,Pasareanu,C.,andBarrett,C. Toward
certifiedrobustnessagainstreal-worlddistributionshifts. arXivpreprintarXiv:2206.03669,2022.
Xu, K., Shi, Z., Zhang, H., Wang, Y., Chang, K., Huang, M., Kailkhura, B., Lin, X., and Hsieh, C. Automatic
perturbationanalysisforscalablecertifiedrobustnessandbeyond. InAdvancesinNeuralInformationProcessing
Systems,2020.
Xu,K.,Zhang,H.,Wang,S.,Wang,Y.,Jana,S.,Lin,X.,andHsieh,C. Fastandcomplete: Enablingcompleteneural
networkverificationwithrapidandmassivelyparallelincompleteverifiers. InInternationalConferenceonLearning
Representations,2021.
Zhang,H.,Weng,T.,Chen,P.,Hsieh,C.,andDaniel,L. Efficientneuralnetworkrobustnesscertificationwithgeneral
activationfunctions. InAdvancesinNeuralInformationProcessingSystems,pp.4944–4953,2018.
Zhang,H.,Chen,H.,Xiao,C.,Gowal,S.,Stanforth,R.,Li,B.,Boning,D.S.,andHsieh,C. Towardsstableandefficient
trainingofverifiablyrobustneuralnetworks. InInternationalConferenceonLearningRepresentations,2020.
Zhang,Y.,Du,T.,Ji,S.,Tang,P.,andGuo,S. Rnn-guard: Certifiedrobustnessagainstmulti-frameattacksforrecurrent
neuralnetworks. arXivpreprintarXiv:2304.07980,2023.
Zhang, Y., Shen, L., Guo, S., and Ji, S. Galileo: General linear relaxation framework for tightening robustness
certification of transformers. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp.
21797–21805,2024.
11A AdditionalOptimizableLinearRelaxation
Inthissection,wederivenewoptimizablelinearrelaxationfornonlinearitiesincludingmultiplication,sine,andGeLU,
whicharenotoriginallysupportedinα,β-CROWNforoptimizablelinearrelaxation.
A.1 OptimizableLinearRelaxationforMultiplication
Foreachelementarymultiplicationxywherex∈[x,x], y ∈[y,y]aretheintermediateboundsforxandy,weaimto
relaxandboundxyas:
∀x∈[x,x],y ∈[y,y], ax+by+c≤xy ≤ax+by+c, (7)
wherea,b,c,a,b,careparametersinthelinearbounds. Shietal.(2019)derivedoptimalparametersthatminimizethe
gapbetweentherelaxedupperboundandtherelaxedlowerbound:
(cid:90) (cid:90)
argmin (ax+by+c)−(ax+by+c)
a,b,c,a,b,c x∈[x,x] y∈[y,y]
s.t. Eq. (7)holds. (8)
However,theoptimalparameterstheyfoundonlyguaranteethatthelinearrelaxationisoptimalforthisnode,butnot
thefinalboundsafterconductingaboundpropagationontheentireNN.Therefore,weaimtomaketheseparameters
optimizabletotightenthefinalboundsaspreviousworksdidforReLUnetworksorS-shapedactivations(Xuetal.,
2021;Lyuetal.,2020).
WenoticethatShietal.(2019)mentionedthattherearetwosolutionsfora,b,canda,b,crespectivelythatsolvesEq.
(8):
 
a =y a =y
 1  1
b =x , b =x , (9)
1 1
c
=−xy
c
=−xy
1 1
 a 2 =y  a 2 =y
b =x , b =x . (10)
2 2

c =−xy
c
=−xy
2 2
Therefore,tomaketheparametersoptimizable,weintroduceparametersαandα,andweinterpolatebetweenEq. (9)
andEq. (10)as:

a=αy+(1−α)y

b=αx+(1−α)x s.t. 0≤α≤1, (11)
c=−αxy−(1−α)xy

a=αy+(1−α)y

b=αx+(1−α)x s.t. 0≤α≤1. (12)
c=−αxy−(1−α)xy
ItiseasytoverifythatinterpolatingbetweentwosoundlinearrelaxationssatisfyingEq. (7)stillyieldsasoundlinear
relaxation. AndαandαarepartofalltheoptimizablelinearrelaxationparametersαmentionedinSection2.
A.2 OptimizableLinearRelaxationforSine
Wealsoderivenewoptimizedlinearrelaxationforperiodicfunctions,inparticularsin(x). Forsin(x)wherex∈[x,x],
weaimtorelaxandboundsin(x)as:
∀x∈[x,x], ax+b≤sin(x)≤ax+b, (13)
where a,b,a,b are parameters in the linear bounds. A non-optimizable linear relaxation for sin already exists in
α,β-CROWNandweadoptitasaninitializationandfocusonmakingitoptimizable. Atinitialization,wefirstcheck
thelineconnecting(x,sin(x))and(x,sin(x)),andthislineisadoptedasthelowerboundortheupperboundwithout
furtheroptimization,ifitisasoundboundingline.
Otherwise,atangentlineisusedastheboundinglinewiththetangentpointbeingoptimized. Within[x,x],ifsin(x)
happenstobemonotonicwithatmostonlyoneinflectionpoint,thetangentpointcanbeoptimizedinawaysimilarto
boundinganS-shapedactivation(Lyuetal.,2020),asillustratedinFigure4.
121.5
Sin(x)
lower bound
1.0
upper bound
x [-1.5, 1.5]
0.5
0.0
0.5
1.0
1.5
3 2 1 0 1 2 3
x
Figure4: LinearrelaxationforaSinactivationinaninputrange[−1.5,1.5]wherethefunctionisS-shaped.
Otherwise,therearemultipleextremepointswithintheinputrange. Initially,weaimtofindatangentlinethatpasses
(x,sin(x))astheboundingline. Sincexmaybeatdifferentcyclesofthesinfunction,weprojectintothecyclewith
range[−0.5π,1.5π],bytakingx˜ =x−2k π,wherek =⌊x+0.5π⌋. Withabinarysearch,wefindatangentpointα
l l l 2π l
ontheprojectedcyclethatsatisfies
sin′(α )(α −x˜ )+sin(x˜ )=sin(α ), (14)
l l l l l
whichcorrespondstoatangentpointt = α +2k π attheoriginalcycleofx,andforanytangentpointwithinthe
l l l
rangeof[α +2k π,1.5π+2k π],thetangentlineisavalidlowerbound. Similarly,wealsoconsiderthetangentline
l l l
passing(x,sin(x)),andwetakex˜ =x−2k π,wherek =⌊x−1.5π⌋,sothatx˜ iswithinrange[1.5π,3.5π]. Wealso
l l l 2π l
conductabinarysearchtofindthetangentpointα ,whichcorrespondstotoα +2k πintheoriginalcycleofx,and
l l l
foranytangentpointwithintherange[1.5π+2k π,α +2k π],thetangentlineisalsoavalidlowerbound. Wemake
l l l
thetangentpointoptimizablewithaparameterα (α ≤α ≤α ),whichcorrespondstoatangentlineattangentpoint
l l l l
t asthelowerboundinEq. (13)andFigure5:
l
(cid:26)a=sin′(t )
l
, (15)
b=sin(t )−at
l l
(cid:40)
t =α +2k π if α ≤α ≤1.5π
l l l l l
where . (16)
t =α +2k π if 1.5π <α ≤α
l l l l l
Inparticular,whenα =1.5π,bothα +2k πandα +2k πaretangentpointsforthesametangentline.
l l l l l
2.5 2.5 2.5
Sin(x) Sin(x) Sin(x)
2.0 lower bound 2.0 lower bound 2.0 lower bound
1.5 x [-3.3, 6.5] 1.5 x [-3.3, 6.5] 1.5 x [-3.3, 6.5]
Optimization range Optimization range Optimization range
1.0 1.0 1.0
0.5 0.5 0.5
0.0 0.0 0.0
0.5 0.5 0.5
1.0 1.0 1.0
1.5 1.5 1.5
2.0 2.0 2.0
4 2 0 2 4 6 8 4 2 0 2 4 6 8 4 2 0 2 4 6 8
x x x
(a)ThelowerboundofSinactivationwhen (b)ThelowerboundofSinactivationwhen (c)ThelowerboundofSinactivationwhen
α =α. α =1.5π. α =α.
l l l l l
Figure5: OptimizingthelowerboundofaSinactivation,where“Optimizationrange”showsallthevalidtangentpoints
forthelowerboundduringtheoptimization.
Thederivationfortheupperboundissimilar. Wetakex˜ =x−2k π,wherek =⌊x−0.5π⌋,sothatx˜ isinrange
u u u 2π u
[0.5π,2.5π]. Andwetakex˜ = x−2k π,wherek = ⌊x−2.5π⌋,sothatx˜ isinrange[2.5π,4.5π]. Letα bethe
u u u 2π u u
tangentpointwherethetangentlinecrossesx˜ ,andα bethetangentpointwherethetangentlinecrossesx˜ ,asfound
u u u
byabinarysearch. Wedefineanoptimizableparameterα (α ≤α ≤α )whichcorrespondstoatangentlineasthe
u u u u
13upperbound:
(cid:40)
a=sin′(t )
u
, (17)
b=sin(t )−at
u u
(cid:40)
t =α +2k π if α ≤α ≤2.5π
u u u u u
where . (18)
t =α +2k π if 2.5π <α ≤α
u u u u u
A.3 OptimizableLinearRelaxationforGeLU
ForGeLUfunctionwherex∈[x,x]aretheintermediateboundsforx,weaimtorelaxandboundGeLU(x)as:
∀x∈[x,x], ax+b≤GeLU(x)≤ax+b, (19)
wherea,b,a,bareparametersinthelinearbounds.
Given input range [x,x], if x ≤ 0 or x ≥ 0, the range contains only one inflection point, the tangent point can be
optimizedinawaysimilartoboundinganS-shapedactivation(Lyuetal.,2020). Inothercases,x<0andx>0holds.
Fortheupperbound,weusethelinepassing(x,GeLU(x))and(x,GeLU(x)). Forthelowerbound,wederivetwo
setsoftangentlinesthatcrosses(x,GeLU(x))and(x,GeLU(x))withtangentpointsdenotedasαandαrespectively.
Wedetermineα,αusingabinarysearchthatsolves:
(cid:40)
GeLU′(α)(α−x)+GeLU(x)=GeLU(α)
. (20)
GeLU′(α)(α−x)+GeLU(x)=GeLU(α)
Anytangentlinewithatangentpointα(α≤α≤α)isavalidlowerbound,whichcorrespondstothelowerboundin
Eq. (19)with:
(cid:40)
a=GeLU′(α)
s.t. α≤α≤α. (21)
b=GeLU(α)−αGeLU′(α)
B AdditionalResults
B.1 ExperimentsonSelf-AttentionNetworksfromWeietal.(2023)
TocomparewithWeietal.(2023)thatonlysupportsverifyingsingle-layerself-attentionnetworksbutnottheentire
ViT,weadoptpre-trainedmodelsfromWeietal.(2023)andrunourverificationmethodsundertheirsettings,with500
testimagesinMNISTusingϵ=0.02. WeshowtheresultsinTable5,whereourmethodsalsooutperformWeietal.
(2023)onallthemodels.
Table5: Numberofverifiedinstancesoutof500instancesinMNISTwithϵ=0.02. A-small,A-mediumandA-bigare
threeself-attentionnetworkswithdifferentparametersizesfromWeietal.(2023).
Method A-small A-medium A-big
Weietal.(2023) 406 358 206
α,β-CROWNw/oBaB 444 388 176
GenBaB(ours) 450 455 232
Upperbound 463 479 482
B.2 ExperimentsonaReLUNetwork
Inthissection,westudytheeffectofourBBPSheuristiconReLUmodels. WeadoptsettingsinSinghetal.(2019a,b);
Mülleretal.(2022)andexperimentona“ConvSmall”modelwithReLUactivation. Theverificationisevaluatedon
1000instancesonCIFAR-10,followingpriorworks. WeshowtheresultsinTable6,WefindthatonthisReLUnetwork,
ourBBPSalsoworksbetterthantheBaBSRheuristic,whenthereisnobackupscore(46.0%verifiedbyBBPSv.s.
45.6%verifiedbyBaBSR).However, wefindthatrecentworkstypicallyaddabackupscoreforBaBSR,whichis
anotherheuristicscorethatservesasabackupforneuronswithextremelysmallBaBSRscores. Thebackupscoredid
notexistintheoriginalBaBSRheuristic(Buneletal.,2020)butitappearedinDePalmaetal.(2021)andhasalsobeen
14Table6: Resultsona“ConvSmall”modelwithReLUactivation(Singhetal.,2019a,b;Mülleretal.,2022)on1000
instancesfromCIFAR-10. Percentageofinstancesverifiedbyvariousmethodsarereported. Formethodsotherthan
PRIMA,weuseα,β-CROWNastheunderlyingverifierbutvarythebranchingheuristic. Seeexplanationaboutthe
backupscoreinAppendixB.2.
Method Verified
PRIMA 44.6%
BaBSRw/obackupscore 45.6%
BaBSRw/backupscore 46.2%
Backupscoreonly 45.0%
BBPSw/obackupscore 46.0%
BBPSw/backupscore 46.2%
adoptedbyworkssuchasWangetal.(2021)whenusingBaBSRforReLUnetworks. Thisbackupscorebasicallyuses
theinterceptofthelinearrelaxationfortheupperboundofaReLUneuronthatneedsbranching. UnlikeBaBSRor
BBPS,thebackupscoredoesnotaimtodirectlyestimatethechangeontheboundscomputedbyboundpropagation,but
aimstousetheintercepttoreflectthereductionofthelinearrelaxationafterthebranching. Whenthebackupscoreis
combinedwithBaBSRorBBPSforReLUnetworks,thebackupscoreseemstodominatetheperformance,whereboth
BaBSRandBBPShavesimilarperformancewiththebackupscoreadded(46.2%verified),whichhidestheunderlying
improvementofBBPSoverBaBSRbyprovidingamorepreciseestimation. However,thebackupscoreisspecifically
forReLU,assumingthattheinterceptofthelinearrelaxationcanreflectthereductionofthelinearrelaxation,whichis
notthecaseforgeneralnonlinearities. Weleaveitforfutureworktostudythepossibilityofdesigningabackupscore
forgeneralnonlinearities.
C ExperimentalDetails
Verification. Weimplementourverificationalgorithmbasedonauto_LiRPA3andα,β-CROWN4.OurBaBisbatched
wheremultipledomainsarebranchedinparallel,andthebatchsizeisdynamictunedbasedonthemodelsizetofitthe
GPUmemory. Topre-optimizethebranchingpoints,weenumeratethebranchingpoints(pinEq. (6))withastepsize
insteadofperforminggradientdescent,consideringthatweonlyhaveuptotwoparametersforthebranchingpointsin
ourexperiments. Fornonlinearitieswithasingleinput,weuseastepsizeof0.01,andfornonlinearitieswithtwoinputs,
weuseastepsizeof0.1. Wepre-optimizethebranchingpointsforintermediateboundswithintherangeof[−5,5]. For
alltheexperiments,eachexperimentisconductedusingasingleNVIDIAGTX1080TiGPU.
Trainingthemodels. TotrainourmodelsonCIFAR-10,weusePGDadversarialtraining(Madryetal.,2018). We
use7PGDstepsduringthetrainingandthestepsizeissettoϵ/4. FortrainingtheSigmoidnetworksinTable3,weuse
theSGDoptimizerwithalearningrateof5×10−2 for100epochs;andfortrainingtheTanhnetworks,weusethe
SGDoptimizerwithalearningrateof1×10−2for100epochs. FortrainingtheLSTMsinTable4,weusetheAdam
optimizerwithalearningof10−3for30epochs. AndfortrainingtheViTs,weusetheAdamoptimizerwithalearning
of5×10−3 for100epochs. ForSinenetworks,weusetheSGDoptimizerwithalearningrateof1×10−3 for100
epochs
3https://github.com/Verified-Intelligence/auto_LiRPA
4https://github.com/Verified-Intelligence/alpha-beta-CROWN
15