LayerMerge: Neural Network Depth Compression
through Layer Pruning and Merging
JinukKim12 MarwaElHalabi3 MingiJi4 HyunOhSong12
Abstract X(0) θ1 X(1) θ2 X(2)
id σ2
Recentworksshowthatreducingthenumberof
layersinaconvolutionalneuralnetworkcanen- ReLU
hance efficiency while maintaining the perfor- X(0) θmerged=θ2∗θ1 X(2)
manceofthenetwork. Existingdepthcompres- σ2
Ker(θ1)=Ker(θ2)=3
sionmethodsremoveredundantnon-linearactiva- Ker(θmerged)=5
ReLU Latencyspeed-up:1.45×
tionfunctionsandmergetheconsecutiveconvo-
lutionlayersintoasinglelayer. However,these
X(0) θ1 X(1) θ2 X(2) θ3 X(3)
methodssufferfromacriticaldrawback;theker-
id id σ3
nel size of the merged layers becomes larger,
significantlyunderminingthelatencyreduction
ReLU
gained from reducing the depth of the network. X(0) θmerged=θ3∗θ2∗θ1 X(2)
Weshowthatthisproblemcanbeaddressedby σ3
Ker(θ1)=Ker(θ2)=Ker(θ3)=3
jointlypruningconvolutionlayersandactivation Ker(θmerged)=7
functions. Tothisend,weproposeLayerMerge, ReLU Latencyspeed-up:1.02×
a novel depth compression method that selects
Figure1: Anillustrationoftheincreaseinkernelsizesig-
which activation layers and convolution layers
nificantlyunderminingthelatencyreductioninthedepth
toremove,toachieveadesiredinferencespeed-
compressionframework. Here, θ denotesthel-thconvo-
up while minimizing performance loss. Since l
lution parameter, X(l) denotes the l-th feature map, and
thecorrespondingselectionprobleminvolvesan
Ker(·)denotesthekernelsizeoftheparameter. Asthelay-
exponentialsearchspace, weformulateanovel
ersaremerged,thekernelsizeofthemergedlayercontinues
surrogate optimization problem and efficiently
togrow,impedingthelatencyreduction. Thelatencyismea-
solve it via dynamic programming. Empirical
suredforthedepictedmodel,onRTX2080Ti,withchannel
resultsdemonstratethatourmethodconsistently
size256,inputresolution56×56,andbatchsize128.
outperformsexistingdepthcompressionandlayer
pruning methods on various network architec-
tures, both on image classification and genera-
tion tasks. We release the code at https:// et al., 2012; Chen et al., 2018a; Girshick, 2015). More
github.com/snu-mllab/LayerMerge. recently,diffusionprobabilisticmodelsemployingU-Net
basedarchitecturearedemonstratinggreatperformancein
varioushigh-qualityimagegenerationtasks(Hoetal.,2020;
Ronnebergeretal.,2015). However,theimpressivecapa-
1.Introduction
bilities of these models on complex vision tasks come at
Convolutionalneuralnetworks(CNNs)haveshownremark- thecostofincreasinglyhighercomputationalresourcesand
ableperformanceinvariousvision-basedtaskssuchasclas- inferencelatencyastheyarescaledup(Nichol&Dhariwal,
sification,segmentation,andobjectdetection(Krizhevsky 2021;Liuetal.,2022).
1DepartmentofComputerScienceandEngineering,SeoulNa- Aneffectiveapproachtoaddressthisisstructuredpruning,
tionalUniversity2NeuralProcessingResearchCenter3Samsung which consists of removing redundant regular regions of
-SAITAILab,Montreal4Google.Correspondenceto:HyunOh weights,suchaschannels,filters,andentirelayers,tomake
Song<hyunoh@snu.ac.kr>.
themodelmoreefficientwithoutrequiringspecializedhard-
warewhilepreservingitsperformance. Inparticular,chan-
Proceedings of the 41st International Conference on Machine
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by nelpruningmethodsremoveredundantchannelsinCNNs
theauthor(s). (Molchanovetal.,2016;Lietal.,2017;Shenetal.,2022).
1
4202
nuJ
81
]GL.sc[
1v73821.6042:viXraLayerMerge:NeuralNetworkDepthCompressionthroughLayerPruningandMerging
Existing depth compression work
Fine-tuned … … …
network
!"# 1x1 !"$ 3x3 !"% 1x1 !"& !'( 1x1 id 3x3 id 1x1 id 1x1 id 3x3 id 1x1 id 1x1 id 3x3 id 1x1 !)*
Merged … … …
network !"# 1x1 !"$ 3x3 !"% 1x1 !"& !'( 7x7 !)*
Accuracy: 73.46% / Latency speed-up: 2.27×
LayerMerge (Ours)
Fine-tuned … … …
network !"# 1x1 id id id 1x1 !"& !'( 1x1 !)+ 3x3 !)" 1x1 !)' 1x1 id id id 1x1 !)$ 1x1 id id id 1x1 !)*
Merged … … …
network !"# 1x1 !"& !'( 1x1 !)+ 3x3 !)" 1x1 !)' 1x1 !)$ 1x1 !)*
Accuracy: 73.48% / Latency speed-up: 2.61×
Figure2: Aqualitativeexamplecomparingourmethodtothedepthcompressionbaseline(Kimetal.,2023),appliedto
MobileNetV2-1.4modelonImageNetdataset. Existingdepthcompressionmethodshavelimitationsinreducinglatencydue
totheinevitableincreaseinthekernelsizeofthemergedlayer. Ourmethodeffectivelybypassesthischallengebyjointly
optimizingtheselectionoftheconvolutionlayersandthenon-linearactivationlayers.
Whilethesemethodshaveshownsignificantimprovements Inparticular,wemakethefollowingcontributions:
inacceleratingmodelsandreducingtheircomplexity,they
arelesseffectiveonarchitectureswithalownumberofchan- • Weproposeanoveldepthcompressionapproachthat
nelscomparedtomethodsthatreducethenumberoflayers introducesanewpruningmodality;removingthecon-
inthenetwork(Elkerdawyetal.,2020;Fuetal.,2022;Kim volution layers in addition to activation layers. This
etal.,2023). Oneapproachtoreducethenumberoflayers formulationencompassesthesearchspaceofexisting
islayerpruning,whichremovesentirelayersinthenetwork depthcompressionmethodsandenablesustobypass
(Chen & Zhao, 2018; Elkerdawy et al., 2020). Although theincreaseinthekernelsizeofthemergedconvolu-
suchmethodsachievealargerspeed-upfactor,theytendto tionlayers(Figure2).
sufferfromseveredegradationinperformanceduetotheir • Weproposeasurrogateoptimizationproblemthatcan
aggressivenatureinremovingparameters(Fuetal.,2022). besolvedexactlyviaaDPalgorithm. Wefurtherde-
velopanefficientmethodtoconstructDPlookuptables,
Tothisend,alineofresearchcalleddepthcompressionor
leveragingtheinherentstructureoftheproblem.
depthshrinkingproposestoremoveredundantnon-linear
activationfunctionsandtomergetheresultingconsecutive • We conduct extensive experiments and demonstrate
convolutionlayersintoasinglelayertoachievehigherin- theeffectivenessofourmethodonimageclassification
ference speed-up (Dror et al., 2022; Fu et al., 2022; Kim andgenerationtaskswithdifferentnetworks,including
etal.,2023). However,thesemethodshaveafundamental ResNet, MobileNetV2, andDDPM (Heet al., 2016;
limitation;mergingconsecutiveconvolutionlayersleadsto Sandleretal.,2018;Hoetal.,2020).
anincreaseinthekernelsizeofthemergedlayers,signifi-
cantlyhinderingthelatencyreductiongainedfromreducing 2.Preliminaries
thedepthofthenetwork(Figure1).
Letf andσ denotethel-thconvolutionlayerandthel-th
Inthiswork,wearguethatthiscriticaldrawbackofexisting θl l
activationlayer,respectively. Here,θ denotestheparame-
depthcompressionmethodscanbeaddressedbyextending l
tersoftheconvolutionlayerf . AnL-layerCNNcanbe
thesearchspaceandjointlyoptimizingtheselectionofboth ìì θl ìì
representedas L (σ ◦f ),where denotesaniterated
non-linearactivationlayersandtheconvolutionlayersto l=1 l θl
functioncomposition,andthelastactivationfunctionσ is
remove(Figure2). Sincethecorrespondingoptimization L
settoidentity.
probleminvolvesanexponentialnumberofpotentialsolu-
tions,weproposeanovelsurrogateoptimizationproblem Depthcompressionmethodseliminatelessimportantnon-
thatcanbesolvedexactlyviaanefficientdynamicprogram- linearactivationlayers,thenmergeconsecutiveconvolution
ming(DP)algorithm. layersintoasinglelayerbyapplyingaconvolutionopera-
2LayerMerge:NeuralNetworkDepthCompressionthroughLayerPruningandMerging
tiontotheirparameters(Droretal.,2022;Fuetal.,2022; larly,ifalayerlisnotinC,wesubstituteitsconvolution
Kimetal.,2023). Thisapproachleveragesthefactthatthe layerf withtheidentitylayerf .
θl θid
successiveconvolutionlayersf ◦f ◦···◦f canbe
θj θj−1 θi Itisworthnotingthatitisnon-trivialtoremoveaconvolu-
representedassingleequivalentconvolutionlayerf ,
θj∗···∗θi tionlayerwhentheshapesoftheinputandoutputfeature
where∗denotestheconvolutionoperation.
mapsaredifferent. Toaddressthis,wedefineasetofirre-
However, this method has a fundamental limitation; the ducibleconvolutionlayerindicesR,wheretheinputshape
kernelsizeofthemergedlayerincreasesasmorelayersare and the output shapes differ. Concretely, we define R as
merged. Thissignificantlyunderminesthelatencyspeed- R := {l ∈ [L] : Shape(X(l−1)) ̸= Shape(X(l))}, where
up achieved by reducing the numberof layers(Figure 1). Shape(·)denotestheshapeofatensorandX(l)denotesthe
To illustrate, let us denote the convolution parameter of l-thlayerfeaturemap. WethenrestrictthechoiceofC to
themergedlayerasθˆ:= θ ∗···∗θ andassumethatall supersetsofR,i.e.,R⊆C.
j i
convolutionlayershaveastrideof1. Then,thekernelsize
GivenadesiredlatencytargetT >0,ourgoalistoselect
ofthismergedlayerisgivenby 0
A and C that maximize the performance of the resulting
j modelafterfine-tuning,whilesatisfyingthelatencytarget
Ker(θˆ)=1+(cid:88)
(Ker(θ l)−1), (1) aftermerging. Weformulatethisproblemasfollows:
l=i
wheretheKer(·)denotesthekernelsizeofthegivencon-
volutionparameter. Toaddressthis,weproposetojointly
removeunimportantconvolutionlayersandnon-linearacti-
(cid:18)ììL
(cid:0)
(cid:1)(cid:19)
max max Perf σ ◦f (2)
vationlayers(Figure2). A⊆[L−1],C⊆[L] θ l=1
A,l C,θl,l
subjectto
3.LayerMerge R⊆C, (irreducibleconv)
σ =(1 (l)σ +(1−1 (l))id), (replacedact)
A,l A l A
Inthissection,wepresentourproposeddepthcompression f =(1 (l)f +(1−1 (l))f ), (replacedconv)
C,θl,l C θl C θid
method LayerMerge, designed to address the increase in
⊛ai
kernelsizeresultingfromdepthcompressionandfindmore ∀i∈[|A|+1]:θˆ = (1 (l)θ +(1−1 (l))θ ),
i C l C id
efficientnetworks. WefirstformulatetheNP-hardsubset l=ai−1+1
selection problem we aim to solve. Then, we present a (mergedparameters)
surrogateoptimizationproblemthatcanbeexactlysolved (cid:32) (cid:33)
ì|ìA|(cid:0) (cid:1)
withanefficientdynamicprogrammingalgorithm. After- T σ ai ◦f θˆ
i
<T 0, (latencyconstraint)
wards,weexaminethetheoreticaloptimality,complexity, i=1
andpracticalcostofourapproach.
⊛
where denotesaniteratedconvolutionoperation,a =0,
0
3.1.Selectionproblem a = L, and (a )|A| denotes the elements of the set
|A|+1 i i=1
Weobservethatifweselectivelyreplacecertainconvolu- A in ascending order. Here, Perf(·) and T(·) denote the
tionlayerswithidentityfunctions(id),wecaneffectively performanceandlatencyofthenetwork,respectively. The
alleviate the problem of increasing kernel sizes resulting performanceofthenetworkisdefinedasatask-dependent
frommerginglayers. Indeed,anidentitylayercanberep- metric: accuracyforclassificationtasksandnegativediffu-
resentedwitha1×1depthwiseconvolutionlayer,where sionlossforgenerationtasks(Hoetal.,2020).Theindicator
theparametervaluesaresetto1. Wedenotethecorrespond- function1 X(x)isequalto1ifx∈X,and0otherwise. We
ing convolution parameters as θ id ∈ R1×Cin×1×1, where denotebyσ A,l thel-thactivationlayerreplacedaccording
C in denotesthenumberofinputchannels. Then,itisevi- tosetA,andbyf C,θl,l thel-thconvolutionlayerreplaced
dentfromEquation(1)thatθ doesnotcontributetothe accordingtosetC. Theparameterθˆ isthei-thconvolution
id i
expansionofthekernelsize. layerinthemergednetwork.
Tothisend,weproposetooptimizetwosetsoflayerindices: Note that we used the pruned network before merging in
A ⊆ [L−1] := {1,...,L−1}foractivationlayersand theobjective,whilethelatencyconstraintisappliedtothe
C ⊆ [L] = {1,...,L} for convolution layers, where L mergednetwork.Bothnetworksrepresentthesamefunction
representsthedepthoftheoriginalnetwork. Theindices andyieldthesameoutput,andthushavethesameperfor-
inAdenotewherewekeeptheoriginalactivationlayers, manceobjective. However,inpractice,weobservethatit
whiletheindicesinC correspondtowheretheoriginalcon- is better to merge consecutive convolution layers only at
volutionlayersaremaintained. ForanylayerlnotinA,we inferencetimeafterfine-tuningisfinished. Wechosetouse
replaceitsactivationfunctionσ withtheidfunction. Simi- thenetworkbeforemergingintheobjectivetostressthis.
l
3LayerMerge:NeuralNetworkDepthCompressionthroughLayerPruningandMerging
3.2.Surrogateoptimizationproblem themergedlayer,butwillvaryinperformance.
Solving Problem (2) in general is NP-hard. We propose Weproposetokeeptheconvolutionlayerswiththelargest
to assign an importance value for each merged layer and parameters ℓ 1-norm among those resulting in the same
optimizethesumoftheimportancevaluesasasurrogate mergedkernelsizek. Thissimpleyeteffectivecriterionis
objective. This is a common approximation used in the oftenusedforchannelandlayerpruning(Lietal.,2017;Elk-
literature(Shenetal.,2022;Frantar&Alistarh,2022;Kim erdawyetal.,2020). Concretely,foranyi,j ∈{0,...,L},
etal.,2023). Wealsoapproximatetheoveralllatencyofthe i<j,andk ∈K ij,welet
merged network with the sum of the layer-wise latencies
(Cai et al., 2019; Shen et al., 2022). The main challenge
(cid:88)
thatwefacethenistheexponentiallylargenumberofpoten- C(cid:98)ijk :=argmax ∥θ l∥
1
(3)
tialcombinationsofthemergedlayersthatarisefromthe Cij⊆(i,j] l∈Cij
jointoptimizationoverC. Tothisend,wedevelopaneffi- (cid:88)
subjectto 1+ (Ker(θ )−1)=k,
cientmethodformeasuringlatencyandimportancevalues, l
leveragingtheinherentcombinatorialstructureoftheprob-
l∈Cij
lem. Subsequently, we compute the optimal solutions of R∩(i,j]⊆C ij.
thesurrogateprobleminpolynomial-timeusingadynamic C(cid:101)ijk :={1,...,i}∪C(cid:98)ijk∪{j+1,...,L}.
programmingalgorithm.
A(cid:101)ij :={1,...,i}∪{j,...,L−1}.
Latency cost We construct a latency lookup table for
all possible merged layers. A straightforward approach
is to construct a table with entries T[i,j,C] for all i,j ∈ ComputingC(cid:101)ijk hasanegligiblecost. Wecannowdefine
{0,...,L},i<j,whereeachentrydenotesthelatencyof theimportanceI[i,j,k]asfollows:
thelayerobtainedbymergingfromthe(i+1)-thlayerto
thej-thlayerafterreplacingtheconvolutionlayersaccord-
ingtoC. However,thisapproachisnotfeasiblebecauseit
requiresmeasuringthelatencyfortheexponentialnumber 
ofpossiblesetsC∩(i,j]. I[i,j,k]:=expmaxPerf(cid:18) ììL (cid:0)
σ ◦ f
(cid:1)(cid:19)
Toaddressthis,wenotethatthechoiceofConlyaffectsthe  θ l=1 (cid:124)A (cid:123)(cid:101)i (cid:122)j, (cid:125)l (cid:124)C(cid:101)i (cid:123)jk (cid:122),θl, (cid:125)l
latencyofamergedlayerviathesizeofitskernel,sincethe Replacedact Replacedconv

numberofinputandoutputchannelsisfixed.Tothisend,we
proposetoconstructthelatencytablewithentriesT[i,j,k], − maxPerf(cid:18) ììL (cid:0) σ ◦f (cid:1)(cid:19) . (4)
wherethelastindexkde (cid:80)notesthekernelsizeofthemerged
θ l=1
l θl 

layer,givenbyk =1+ l∈C∩(i,j](Ker(θ l)−1). (cid:124) (cid:123)(cid:122) (cid:125)
Originalnetwork
LetK bethesetofpossiblemergedkernelsizesthatcan
ij
appearaftermergingfromthe(i+1)-thlayertothej-th
layer. Notethat|K
|≤1+(cid:80)j
(Ker(θ )−1). There-
ij l=i+1 l We use exp(·) to normalize the importance value. This
fore, constructing the latency table with entries T[i,j,k]
choiceisbasedonourempiricalobservationthatusingpos-
requires O(L2K ) latency measurements, where K :=
0 0 itivevaluesforimportanceleadstobetterperformanceby
(cid:80)
Ker(θ )denotesthesumofthekernelsizesintheorigi-
l l favoringsolutionswithmoreactivationlayers. Inpractice,
nalnetwork. ThisissignificantlylowerthantheO(L22L)
toestimatethefirstterm,wemeasuretheperformanceof
measurements needed to construct the latency table with
the network after fine-tuning it for a few steps. For the
entriesT[i,j,C].
secondterm,weusetheperformanceofthepre-trainedorig-
inal network. Constructing the importance table requires
Importancevalue Similarlytothelatencycostcase,we O(L2K )importancevalueevaluations,whichisidentical
0
constructtheimportancelookuptablewithentriesI[i,j,k]. tothenumberoflatencymeasurementsneeded.
Eachentrydenotestheimportanceofthelayerobtainedby
mergingfromthe(i+1)-thlayertothej-thlayer,withthe
mergedlayerhavingakernelsizek.
Optimizationproblem Afterwepre-computethelatency
Wedefinetheimportancevalueofeachmergedlayerasthe andimportancelookuptables,T andI,wemaximizethe
changeintheperformanceafterreplacingthecorresponding sum of the importance values of merged layer under the
partoftheoriginalnetworkwiththemergedlayer.However, constraint on the sum of the latency costs. This can be
multiplechoicesofC canyieldthesamekernelsizek in formulatedasfollows:
4LayerMerge:NeuralNetworkDepthCompressionthroughLayerPruningandMerging
Algorithm1DPalgorithmforProblem(5) Algorithm2LayerMerge
input Importance I, latency T, latency budget T , dis- input Inputnetworkf,latencybudgetT ,descretization
0 0
cretizationlevelP levelP
Initialize M[0,t] ← 0 for t ≥ 0, M[l,t] ← −∞ for fori=0toL−1do
t<0,A[0,t]←∅,C[0,t]←∅ forj =i+1toLdo
DiscretizelatencyvaluesinT fork ∈K do
ij
forl=1toLdo A(cid:101)ij,C(cid:101)ijk ←computeviaEquation(3)
fort∈{T P0,2 PT0 ...,T 0}do θˆ =⊛ j (cid:16) 1 (l)θ +(cid:16) 1−1 (l)(cid:17) θ (cid:17)
l∗,k∗ ← argmax (M[l′,t−T[l′,l,k]]+I[l′,l,k]) ij l=i+1 C(cid:101)ijk l C(cid:101)ijk id
0≤l′<l,k∈K l′l I[i,j,k]←computeviaEquation(4)
M[l,t]←M[l∗,t−T[l∗,l,k∗]]+I[l∗,l,k∗] T[i,j,k]←T(f )
A[l,t]←A[l∗,t−T[l∗,l,k∗]]∪{l∗ :l∗ >0} endfor
θˆ
ij
C(cid:98)l∗lk∗ ←computeviaEquation(3) endfor
C[l,t]←C[l∗,t−T[l∗,l,k∗]]∪C(cid:98)l∗lk∗ endfor
endfor A∗,C∗,k∗ ←Algorithm1(I,T,T ,P)
0
endfor ReplaceactivationfunctionsoutsideA∗ byidandconvo-
A∗ ←A[L,T ],C∗ ←C[L,T ] lutionlayersoutsideC∗byf
k∗ ←1+(cid:80) 0 (Ke0 r(θ )−1),∀i∈[|A∗|+1] Fine-tuneandmergethenetwθ oid rk
i l∈C∗∩(a∗ ,a∗] l
output
A∗,C∗,and(ki ∗− )1 |Ai ∗|+1. output Mergednetwork
i i=1
|A|+1
(cid:88)
max I[a ,a ,k ] (6)
i−1 i i
A⊆[l−1],ki
i=1
|A|+1
(cid:88)
|A (cid:88)|+1 subjectto T[a i−1,a i,k i]<t, (latencyconstraint)
max I[a ,a ,k ] (5)
A⊆[L−1],ki
i=1
i−1 i i i=1
k ∈K , (mergedkernelsize)
|A|+1 i ai−1ai
(cid:88)
subjectto T[a ,a ,k ]<T , (latencyconstraint)
i−1 i i 0
wherea =0anda =l. WedefineM[l,t]asthecor-
i=1 0 |A|+1
k ∈K , (mergedkernelsize) respondingmaximumobjectivevalueachievableinthesub-
i ai−1ai
problem(6). ThenM[L,T ]givesthemaximumobjective
0
valueachievableinProblem(5).WeinitializeM[0,t] = 0
fort≥0,andM[l,t]=−∞fort<0.Then,forl>0,the
recurrenceoftheDPalgorithmcanbewrittenasfollows:
where a = 0, a = L as before. Given a solution
0 |A|+1
A∗,(k∗)|A∗|+1ofProblem(5),thecorrespondingsetofcon-
volutioi ni l= ay1 erswekeepisgivenbyC∗ =(cid:83) iC(cid:98)a∗ i−1,a∗ i,k i∗. 
M[l,t]= max  M[l′,t−T[l′,l,k]]

3.3.Dynamicprogrammingalgorithm 0≤l′<l,k∈K l′l (cid:124) (cid:123)(cid:122) (cid:125)
Optimalimportancesumuntill′-layer

OnceweconstructthelookuptablesT andI,wecanobtain
an exact solution of Problem (5) for discretized latency + I[l′,l,k] . (7)

values,usingdynamicprogramming(DP).Inparticular,we (cid:124) (cid:123)(cid:122) (cid:125)
discretizelatencyvaluesinthelookuptableT byrounding Importancevalueofthelastcompressedlayer
themdowntotheclosestvaluesin{T P0,2 PT0 ...,T 0},where
WepresenttheDPalgorithmforProblem(5)inAlgorithm1.
P isalargenaturalnumberthatrepresentsthediscretization
OncewecomputetheoptimalsetsA∗andC∗,wefine-tune
level.
the network after replacing the layers accordingly. Then,
Then,weconsiderasub-problemofProblem(5)wherewe wemergeeveryconvolutionlayerbetweena∗ anda∗for
i−1 i
maximizeoverthefirstl∈{0,...,L}layerswithlatency alli ∈ [|A|+1]atinferencetime. Weoutlinetheoverall
budgett∈{T P0,2 PT0 ...,T 0},asfollows: procedureofLayerMergeinAlgorithm2.
5LayerMerge:NeuralNetworkDepthCompressionthroughLayerPruningandMerging
3.4.Theoreticalanalysis spectively. Wesolvethefollowingsurrogateproblem:
(cid:88)
WeshowthattheproposedDPalgorithmoptimallysolves max I[l] (8)
thesurrogateoptimizationProblem(5). Theproofisgiven R⊆C⊆[L]
l∈C
inAppendixB. (cid:88)
subjectto T[l]<T . (latencyconstraint)
0
l∈C
Theorem 3.1. The solution A∗ and (k∗)|A∗|+1 given by
i i=1
Algorithm1isanoptimalsolutionofProblem(5). Problem(8)isa0-1knapsackproblemthatcanbesolved
exactlyfordiscretizedlatencyvaluesviaaDPalgorithmin
O(LP)time,whereP denotesthediscretizationlevel. We
ThetimecomplexityoftheDPalgorithmisO(cid:0) L2PK (cid:1) .
0 denotethismethodasLayerOnly.
Inpractice,theDPalgorithmishighlyefficient,typically
completingwithinafewsecondsonCPU.Furthermore,our Weadditionallycomparewithachannelpruningbaselinefor
methodefficientlycomputestheDPlookuptables,exploit- eachnetwork. Notethatchannelpruningisanorthogonal
ing the structure of the problem. It is worth noting that approachtodepthcompression. Nonetheless,weinclude
thiscanbedoneinanembarrassinglyparallelfashion. We channelpruningresultstostudytheeffectivenessofreduc-
reportthewall-clocktimeforconstructingtheDPlookup ingthenumberoflayerscomparedtoreducingthewidthin
tablesinAppendixC. differenttypesofnetworks. WecomparewithHALP(Shen
et al., 2022) on ResNet-34, with AMC and MetaPruning
(Heetal.,2018;Liuetal.,2019)onMobileNetV2,andwith
4.Experiments
Diff-Pruning(Fangetal.,2023)onDDPM.
Inthissection,weprovideexperimentalresultsdemonstrat-
For ResNet-34, we apply the depth compression and the
ingtheeffectivenessofourmethodacrossdifferentnetwork
channel pruning baselines using their publicly available
architecturesandtasks. WeapplyourmethodonResNet-34
code (Shen et al., 2022; Kim et al., 2023). We also use
andMobileNetV2models(Heetal.,2016;Sandleretal.,
thepre-trainedweightsoftheoriginalnetworkfromWight-
2018)fortheimageclassificationtask,andontheDDPM
man et al. (2021) for all compression methods. For Mo-
model(Hoetal.,2020)fortheimagegenerationtask. We
bileNetV2,wereporttheresultsforthedepthcompression
presentadditionaldetailsonhowwehandlenormalization
baselinefromtheoriginalpaper(Kimetal.,2023). Forthe
layers,stridedconvolutions,padding,skipconnections,and
channelpruningbaselines,weprunechannelsofeachlayer
othernetwork-specificimplementationdetailsofourmethod
usingthesamechannelratiooftheiroptimizedmodelfrom
inAppendixA.
theiropen-sourcedcode(Heetal.,2018;Liuetal.,2019).
Weusethepre-trainedweightsoftheoriginalnetworkfrom
thepubliccodeofKimetal.(2023)toensureafaircompar-
ison. ForDDPM,weapplythedepthcompressionandthe
Baselines Wecompareourmethodtoadepthcompression
channel pruning baselines using their open-sourced code
methodandalayerpruningmethodsincebothapproaches
(Kimetal.,2023;Fangetal.,2023). Weusethepre-trained
rely,likeourmethod,onreducingthenumberoflayersof
weights of the original network from Song et al. (2021)
thenetworktoaccelerateit. Wealsoincludeacomparison
for all compression methods in DDPM. In all networks,
withaknowledgedistillationmethodinAppendixE.For
wecomparethecompressionresultsineachtableusingan
thedepthcompressionbaseline,weusethestate-of-the-art
identicalfine-tuningscheduleforfaircomparison.
workofKimetal.(2023),whichwedenoteasDepth.
Finally,weprovideablationstudiesontheimportanceof
Existinglayerpruningmethodsdonotdirectlytakelatency
jointoptimizationonactivationlayersandconvolutionlay-
intoconsiderationduringpruning(Jordaoetal.,2020;Chen
ersinourmethod,wherewecompareourmethodtosequen-
&Zhao,2018;Elkerdawyetal.,2020). Toaddressthisgap,
tiallyapplyingDepththenLayerOnly. Throughoutthissec-
weproposeavariantofourmethodtailoredspecificallyfor
tion,werefertoeachcompressedmodelobtainedbyDepth,
layerpruning,whichweuseasourlayerpruningbaseline.
LayerOnly,andLayerMergeasDepth-p%,LayerOnly-p%,
Specifically, we assign an importance value I[l] and a la- andLayerMerge-p%,respectively.Here,p%iscalculatedas
tency cost T[l] for each convolution layer l ∈ [L]. The T /T ,whereT isthechosenlatencybudgetandT
0 orig 0 orig
importancevalueoftheconvolutionlayerisdefinedasthe isthelatencyoftheoriginalmodel.
changeinperformanceresultingfromreplacingthelayer
withtheidentitylayerintheoriginalnetworkandthenfine- Experimentalsetup Weconstructthelatencylookupta-
tuningitforafewsteps.AsinLayerMerge,weapproximate ble of each method on RTX2080 Ti GPU and report the
the overall latency and importance of the network by the wall-clock latency speed-up of the compressed networks
sumofthelayer-wiselatenciesandimportancevalues,re- measuredonthesamedevice. Weprovidethedetailsonthe
6LayerMerge:NeuralNetworkDepthCompressionthroughLayerPruningandMerging
Table1: Accuracyandlatencyspeed-upofapplyingcom- Table2: Accuracyandlatencyspeed-upofapplyingcom-
pressionmethodstoResNet-34onImageNetdataset. The pressionmethodstoMobileNetV2-1.0onImageNetdataset.
latencyspeed-upismeasuredonRTX2080TiGPUatbatch Thelatencyspeed-upismeasuredonRTX2080TiGPUat
size128. batchsize128.
PyTorch TensorRT PyTorch TensorRT
Network Acc(%)↑ Speed-up↑ Speed-up↑ Network Acc(%)↑ Speed-up↑ Speed-up↑
ResNet-34 74.42 1.00× 1.00× MobileNetV2-1.0 72.89 1.00× 1.00×
HALP-80%(Shenetal.,2022) 73.98 1.23× 1.25× AMC-70%(Heetal.,2018) 72.01 1.32× 1.34×
Depth-78%(Kimetal.,2023) 73.49 1.24× 1.14× Depth-74%(Kimetal.,2023) 72.83 1.62× 1.42×
LayerOnly-73%(Ours) 74.06 1.33× 1.24× LayerOnly-73%(Ours) 69.66 1.30× 1.35×
LayerMerge-71%(Ours) 74.26 1.36× 1.25× LayerMerge-55%(Ours) 72.99 1.63× 1.42×
HALP-65%(Shenetal.,2022) 73.36 1.48× 1.45× Depth-66%(Kimetal.,2023) 72.13 1.88× 1.57×
Depth-68%(Kimetal.,2023) 73.35 1.40× 1.26× LayerMerge-46%(Ours) 72.46 1.90× 1.65×
LayerOnly-64%(Ours) 73.31 1.65× 1.48×
Depth-59%(Kimetal.,2023) 71.44 2.07× 1.79×
LayerMerge-68%(Ours) 73.46 1.56× 1.50×
LayerMerge-38%(Ours) 71.74 2.18× 1.84×
HALP-55%(Shenetal.,2022) 72.69 1.69× 1.69×
Depth-53%(Kimetal.,2023) 70.65 2.47× 1.97×
Depth-63%(Kimetal.,2023) 72.33 1.43× 1.24×
LayerMerge-33%(Ours) 70.99 2.49× 2.05×
LayerOnly-49%(Ours) 72.58 1.82× 1.64×
LayerMerge-50%(Ours) 72.84 1.79× 1.72×
0.77%pointhigheraccuracycomparedtothedepthcom-
measurementprocessinAppendixC.Notably,wemeasure
pressionbaseline(comparingLayerMerge-71%toDepth-
thelatencyofthenetworkintwodifferentformats,PyTorch
78%). Itisworthnotingthatthelayerpruningvariantof
formatandTensorRTcompiledformat(Paszkeetal.,2017;
ourmethodperformsonparwithLayerMergeonResNet-
Vanholder, 2016). When measuring latency speedup, we
34. ThisismainlyduetoResNet-34beingmoresuitable
use a batch size of 128 for the ImageNet dataset and 64
forlayerpruningthandepthcompression. Indeed,Layer-
fortheCIFAR10dataset,followingthesamemeasurement
Mergefrequentlyoptsforpruningconvolutionlayersover
protocolfromKimetal.(2023);Shenetal.(2022);Fang
activationfunctionswhenappliedtoResNet-34.
etal.(2023). ForResNet-34,wefine-tuneeachprunednet-
workfor90epochsfollowingthesamefine-tuningrecipeas
HALP(Shenetal.,2022). ForMobileNetV2,wefine-tune
MobileNetV2 Table2presentsthevariouscompression
for180epochs,usingthesamefine-tuningrecipeasKim
resultsonMobileNetV2-1.0. AMC-p%denotesthepruned
et al. (2023). For DDPM, we follow the fine-tuning and
modelobtainedbyAMCbysettingtheFLOPsbudgettobe
samplingrecipeofDiff-Pruning(Fangetal.,2023),except
p%oftheoriginalmodelFLOPs. LayerMergesurpassesex-
forthelearningratewhichwesetto4×10−4sinceitleads
istingchannelpruninganddepthcompressionbaselines,as
tobetterperformance. Wepresentarepresentativesubset
wellasourlayerpruningvariant. Inparticular,weachieve
of the results in Tables 1 to 6. For additional results, see
1.63×speed-upinPyTorchwithoutlosingaccuracyfrom
AppendixE.
the original network (LayerMerge-55%). Table 3 further
showsthecompressionresultsonMobileNetV2-1.4. Layer-
4.1.Classificationtaskresults Mergeoutperformsexistingmethods,offering0.23%point
higher accuracy with a larger speed-up compared to the
Inthissection,weevaluatetheperformanceofthediffer-
depthcompressionbaseline(comparingLayerMerge-43%
entpruningmethodsonResNet-34,MobileNetV2-1.0,and
toDepth-62%). Thegaininefficiencystemsfromitsunique
MobileNetV2-1.4 models, on the ImageNet dataset (He
capabilitytojointlyprunelayersandmergethem,asdemon-
etal.,2016;Sandleretal.,2018;Russakovskyetal.,2015).
stratedinFigure2.
Wereportthelasttop-1accuracyofthecompressedmodel
after fine-tuning, evaluated on the validation set, and its
4.2.Generationtaskresults
correspondinglatencyspeedup.
Inthissection,weevaluatetheperformanceofthediffer-
ResNet-34 Table1summarizesthedifferentcompression ent pruning methods on DDPM model (Ho et al., 2020)
resultsonResNet-34. HALP-p%referstotheprunedmodel ontheCIFAR10dataset(Krizhevsky,2009). Wemeasure
obtainedbyHALPbysettingthelatencybudgettobep% performanceusingthestandardFrechetInceptionDistance
of the original model latency. LayerMerge outperforms (FID)metric(Heuseletal.,2017). WereportthelastFID
existingchannelpruninganddepthcompressionbaselines. ofthecompressedmodelafterfine-tuning,evaluatedonthe
Specifically,weachieve1.10×speed-upinPyTorchwith validationset,anditscorrespondinglatencyspeedup.
7LayerMerge:NeuralNetworkDepthCompressionthroughLayerPruningandMerging
Table3: Accuracyandlatencyspeed-upofapplyingcom- Table5: FIDmetricandPyTorchlatencyspeed-upofcom-
pressionmethodstoMobileNetV2-1.4onImageNetdataset. pression methods applied to channel pruned DDPM on
Thelatencyspeed-upismeasuredonRTX2080TiGPUat CIFAR10 dataset. The latency speed-up is measured on
batchsize128. RTX2080TiGPUatbatchsize64. Diff-p%denotesapply-
ingDiff-Pruningwithp%compressionratio.
PyTorch TensorRT
Network Acc(%)↑ Speed-up↑ Speed-up↑
PyTorch Fine-tune
MobileNetV2-1.4 76.28 1.00× 1.00× Network FID↓ Speed-up↑ Steps↓
MetaPruning-1.0×(Liuetal.,2019) 73.69 1.59× 1.38× DDPM 4.18 1.00× -
Depth-62%(Kimetal.,2023) 74.68 1.93× 1.61× Diff-30%(Fangetal.,2023) 4.85 1.40× 100K
LayerOnly-75%(Ours) 73.94 1.27× 1.28× Diff-60%(Fangetal.,2023) 7.90 2.33× 100K
LayerMerge-43%(Ours) 74.91 1.99× 1.61×
Diff-70%(Fangetal.,2023) 9.89 2.57× 200K
Depth-60%(Kimetal.,2023) 74.19 1.99× 1.67× Diff-60%→Depth-86%(Kimetal.,2023) 9.09 2.42× 200K
LayerMerge-42%(Ours) 74.48 2.07× 1.73× Diff-60%→LayerOnly-63%(Ours) 10.15 2.59× 200K
Depth-53%(Kimetal.,2023) 73.46 2.27× 1.85× Diff-60%→LayerMerge-70%(Ours) 8.92 2.59× 200K
LayerMerge-35%(Ours) 73.99 2.39× 1.93×
Depth-46%(Kimetal.,2023) 72.57 2.41× 2.01× Table6:Accuracyandcorrespondinglatencyspeed-upcom-
LayerMerge-30%(Ours) 73.29 2.72× 2.12×
paredtosequentialoptimizationandourmethodevaluated
withMobileNetV2-1.0onImageNetdataset.
Table4: FIDmetricandPyTorchlatencyspeed-upofcom-
pressionmethodsappliedtoDDPMonCIFAR10dataset.
Thelatencyspeed-upismeasuredonRTX2080TiGPUat PyTorch TensorRT
Network Acc(%)↑ Speed-up↑ Speed-up↑
batchsize64.
MobileNetV2-1.0 72.89 1.00× 1.00×
Depth-74%→LayerOnly-72% 71.72 2.09× 1.79×
PyTorch Fine-tune LayerMerge-39%(Ours) 71.89 2.15× 1.80×
Network FID↓ Speed-up↑ Steps↓ Depth-74%→LayerOnly-64% 70.14 2.33× 2.09×
DDPM 4.18 1.00× - LayerMerge-33%(Ours) 70.81 2.47× 2.15×
Depth-89%(Kimetal.,2023) 4.21 1.04× 100K
LayerOnly-77%(Ours) 4.64 1.09× 100K
with Diff-Pruning achieves a larger speed-up than solely
LayerMerge-73%(Ours) 4.16 1.13× 100K
relyingonDiff-Pruning,andourmethodconsistentlyout-
Depth-85%(Kimetal.,2023) 4.78 1.08× 100K
performsthedepthcompressionandlayerpruningbaselines
LayerOnly-69%(Ours) 5.52 1.14× 100K
LayerMerge-70%(Ours) 4.55 1.16× 100K inthissettingaswell.
LayerOnly-54%(Ours) 6.23 1.26× 100K
LayerMerge-58%(Ours) 5.61 1.27× 100K 4.3.Ablationstudies
Ourmethodjointlyoptimizestheselectionoftheactivation
layersandconvolutionlayerstoprune. Analternativeway
DDPM Table4reportsthedifferentcompressionresults
todothisistosequentiallyoptimizetheselectionofeach
onDDPM.LayerMergeshowssuperiorperformancecom-
typeoflayerindependently. Wecompareourmethodtose-
pared to the existing depth compression baseline and the
quentiallyapplyingDepththenLayerOnlyonMobileNetV2
layer pruning variant of our method. Specifically, we
on the ImageNet dataset in Table 6. Our method outper-
achieve1.08×speed-upwithalowerFIDmetriccompared
formsthesequentialoptimizationbaseline,underliningthe
tothedepthcompressionbaseline(comparingLayerMerge-
importanceofourjointoptimizationapproach. Weprovide
73% to Depth-89%). The channel pruning baseline Diff-
thedetailsinAppendixD.
PruningshowssuperiorperformancethanLayerMergehere
(Table5). ThisislikelyduetoDDPMhavingmorechannel
5.RelatedWork
redundancythanothermodels.
Unstructuredpruning Unstructuredpruningmethodsre-
ChannelprunedDDPM Wenotethatchannelpruning moveindividualneuronsinthenetworktoachievenetwork
anddepthcompressionmethodscanbejointlyapplied. We sparsity (Han et al., 2015; Hubara et al., 2021; Benbaki
thusincluderesultswhereweapplyourmethodtothechan- etal.,2023;Frantar&Alistarh,2023). Theclosestmethod
nelprunedDDPMmodelobtainedbyDiff-PruninginTa- to ours in this line of work is Frantar & Alistarh (2022),
ble5(Fangetal.,2023). Diff-p%denotestheprunedmodel whichproposesadynamicprogrammingalgorithmthatde-
obtainedwithDiff-Pruningbyremovingp%ofthechannels termineslayer-wisesparsityunderagivenlatencyconstraint.
ineachlayer. Theresultsshowthatcombiningourmethod However,unstructuredpruningmethodsrequirespecialized
8LayerMerge:NeuralNetworkDepthCompressionthroughLayerPruningandMerging
hardwaretoachievecomputationalsavings. ImpactStatement
ThisworkcontributestotheareaofNNcompression,inpar-
Channelpruning Incontrast, structuredpruningmeth- ticulartocompressingpre-trainedCNNsanddiffusionmod-
ods,whichconsistofremovingredundantregularregions els. Assuch,ithelpsreducetheenergyconsumptionand
ofweights,canyieldcomputationalsavingsonoff-the-shelf computationalresourcesofsuchmodelsatinference.Hence,
hardware. Amongsuchmethodsarechannelpruningmeth- thisworkcontributestoreducingtheenvironmentalimpact
ods,whichremoveredundantchannelsinaconvolutional ofNNsandenablingtheiruseonresource-constrainedde-
neural network (Molchanov et al., 2016; 2019; Li et al., vices like mobile phones and for latency-critical applica-
2017). Aflalo et al. (2020) formulate this as a knapsack tionssuchasself-drivingcars. Ontheotherhand,pruning
problem that maximizes the sum of channel importance hasbeenshowntohaveadisparateimpactonperformance
valuesunderagivenFLOPsbudget. Similarly,Shenetal. betweendifferentsub-groupsofdata,whichamplifiesexist-
(2022)formulatesanotherknapsackproblem,whichmaxi- ingalgorithmicbias(Hookeretal.,2020;Paganini,2020).
mizesthesumofchannelimportancevaluesunderalatency Thereisanongoingefforttomitigatethisnegativeimpactof
constraintonatargetdevice. pruningeitherusingfairness-awarepruning(Linetal.,2022)
or by modifying the objective during fine-tuning of the
prunedmodel(Tranetal.,2022).Thelatterapproachcanbe
Layer pruning Layer pruning methods aim to make a appliedtoourpruningmethod(Hashemizadehetal.,2024).
shallowernetworkbyentirelyremovingcertainconvolution
layers(Jordaoetal.,2020;Chen&Zhao,2018;Elkerdawy
Acknowledgements
etal.,2020). However,theiraggressivenatureinremoving
parametersresultsinalargeperformancedegradationunder ThisworkwassupportedbySamsungAdvancedInstituteof
highcompressionratios(Fuetal.,2022). Technology, Samsung Electronics Co., Ltd. (IO220810-
01900-01), Institute of Information & Communications
TechnologyPlanning&Evaluation(IITP)grantfundedby
Depthcompression Instead,depthcompressionmethods
the Korea government (MSIT) [No. RS-2020-II200882,
focusoneliminatingunimportantnon-linearactivationlay-
(SW STAR LAB) Development of deployable learning
ers,thenmergingconsecutiveconvolutionlayertoreduce
intelligence via self-sustainable and trustworthy machine
thenetwork’sdepth(Droretal.,2022;Fuetal.,2022;Kim
learningandNo. RS-2021-II211343,ArtificialIntelligence
etal.,2023). Inparticular,Droretal.(2022)andFuetal.
GraduateSchoolProgram(SeoulNationalUniversity)],and
(2022)proposetotrainasoftparameterthatcontrolsthein-
BasicScienceResearchProgramthroughtheNationalRe-
tensityofnon-linearityofeachlayerwithanadditionalloss
searchFoundationofKorea(NRF)fundedbytheMinistry
thatpenalizestheabsolutevalueofthesoftparameter.More
ofEducation(RS-2023-00274280). HyunOhSongisthe
recently, Kim et al. (2023) propose to maximize the sum
correspondingauthor.
oftheimportancevaluesofmergedlayersunderalatency
constraint,viaadynamicprogrammingalgorithm.However,
References
thislineofworksuffersfromafundamentaldrawbackas
merging layers leads to an increase in the kernel size of Aflalo, Y., Noy, A., Lin, M., Friedman, I., and Zelnik, L.
themergedlayers. Fuetal.(2022)sidestepthisissue, as Knapsackpruningwithinnerdistillation. arXivpreprint
theyonlyconsidermergingwithininvertedresidualblocks. arXiv:2002.08258,2020.
However,thisrestrictionnotonlylimitstheapplicabilityof
theirmethodtomobile-optimizedCNNs,butalsolimitsits Benbaki, R., Chen, W., Meng, X., Hazimeh, H., Pono-
performance. Indeed, Kim et al. (2023) have shown that mareva,N.,Zhao,Z.,andMazumder,R. Fastaschita:
theirmethodoutperformsthatof(Fuetal.,2022). Neuralnetworkpruningwithcombinatorialoptimization.
InICML,2023.
6.Conclusion
Cai,H.,Zhu,L.,andHan,S. Proxylessnas: Directneural
WeproposeLayerMerge,anovelefficientdepthcompres- architecturesearchontargettaskandhardware,2019.
sionmethod,whichjointlyprunesconvolutionlayersand
activationfunctionstoachieveadesiredtargetlatencywhile Chen,L.-C.,Zhu,Y.,Papandreou,G.,Schroff,F.,andAdam,
minimizingtheperformanceloss. Ourmethodavoidsthe H. Encoder-decoderwithatrousseparableconvolution
problemofincreasingkernelsizeinmergedlayers,which forsemanticimagesegmentation. InECCV,2018a.
existing depth compression methods suffer from. It con-
sistentlyoutperformsexistingdepthcompressionandlayer Chen,S.andZhao,Q. Shallowingdeepnetworks: Layer-
pruningbaselinesinvarioussettings. wise pruning based on feature representations. IEEE
9LayerMerge:NeuralNetworkDepthCompressionthroughLayerPruningandMerging
transactionsonpatternanalysisandmachineintelligence, Ho,J.,Jain,A.,andAbbeel,P. Denoisingdiffusionproba-
2018. bilisticmodels. InNeurIPS,2020.
Chen, X., Mishra, N., Rohaninejad, M., and Abbeel, P. Hooker,S.,Moorosi,N.,Clark,G.,Bengio,S.,andDenton,
Pixelsnail: Animprovedautoregressivegenerativemodel. E. Characterising bias in compressed models. arXiv
InICML,2018b. preprintarXiv: 2010.03058,2020.
Ding, X., Zhang, X., Ma, N., Han, J., Ding, G., andSun, Hubara, I., Chmiel, B., Island, M., Banner, R., Naor, J.,
J. Repvgg: Makingvgg-styleconvnetsgreatagain. In and Soudry, D. Accelerated sparse neural training: A
CVPR,2021. provableandefficientmethodtofindn:mtransposable
masks. InNeurIPS,2021.
Dror,A.B.,Zehngut,N.,Raviv,A.,Artyomov,E.,Vitek,
R.,andJevnisek,R. Layerfolding:Neuralnetworkdepth Ioffe,S.andSzegedy,C. Batchnormalization:Accelerating
reductionusingactivationlinearization. InBMVC,2022. deepnetworktrainingbyreducinginternalcovariateshift.
InICML,2015.
Elkerdawy,S.,Elhoushi,M.,Singh,A.,Zhang,H.,andRay,
Jordao, A., Lie, M., andSchwartz, W.R. Discriminative
N. Tofilterprune,ortolayerprune,thatisthequestion.
InACCV,2020. layerpruningforconvolutionalneuralnetworks. IEEE
JournalofSelectedTopicsinSignalProcessing,2020.
Fang, G., Ma, X., and Wang, X. Structural pruning for
Kim, J., Jeong, Y., Lee, D., and Song, H. O. Efficient
diffusionmodels. InNeurIPS,2023.
latency-awarecnndepthcompressionviatwo-stagedy-
Frantar,E.andAlistarh,D. Spdy: Accuratepruningwith namicprogramming. InICML,2023.
speedupguarantees. InICML,2022.
Krizhevsky,A. Learningmultiplelayersoffeaturesfrom
Frantar,E.andAlistarh,D. SparseGPT:Massivelanguage tinyimages. Master’sthesis,DepartmentofComputer
modelscanbeaccuratelyprunedinone-shot. InICML, Science,UniversityofToronto,2009.
2023.
Krizhevsky,A.,Sutskever,I.,andHinton,G.E. Imagenet
Fu,Y.,Yang,H.,Yuan,J.,Li,M.,Wan,C.,Krishnamoorthi, classification with deep convolutional neural networks.
R., Chandra, V., and Lin, Y. Depthshrinker: A new InNeurIPS,2012.
compressionparadigmtowardsboostingreal-hardware
Li,H.,Kadav,A.,Durdanovic,I.,Samet,H.,andGraf,H.P.
efficiencyofcompactneuralnetworks. InICML,2022.
Pruningfiltersforefficientconvnets. InICLR,2017.
Girshick,R. Fastr-cnn. InICCV,2015.
Lin,X.-Z.,Kim,S.,andJoo,J. Fairgrape: Fairness-aware
gradientpruningmethodforfaceattributeclassification.
Han, S., Pool, J., Tran, J., and Dally, W. Learning both
InECCV,2022.
weightsandconnectionsforefficientneuralnetwork. In
NeurIPS,2015.
Liu,Z.,Mu,H.,Zhang,X.,Guo,Z.,Yang,X.,Cheng,K.,
andSun,J. Metapruning: Metalearningforautomatic
Hashemizadeh, M., Ramirez, J., Sukumaran, R., Farnadi,
neuralnetworkchannelpruning. InICCV,2019.
G.,Lacoste-Julien,S.,andGallego-Posada,J. Balancing
act: Constrainingdisparateimpactinsparsemodels. In
Liu,Z.,Mao,H.,Wu,C.-Y.,Feichtenhofer,C.,Darrell,T.,
ICLR,2024.
andXie,S. Aconvnetforthe2020s. InCVPR,2022.
He, K., Zhang, X., Ren, S., and Sun, J. Deep residual Molchanov, P., Tyree, S., Karras, T., Aila, T., and Kautz,
learningforimagerecognition. InCVPR,2016. J. Pruning convolutional neural networks for resource
efficient inference. arXiv preprint arXiv:1611.06440,
He, Y., Lin, J., Liu, Z., Wang, H., Li, L.-J., and Han, S.
2016.
Amc: Automlformodelcompressionandacceleration
onmobiledevices. InECCV,2018. Molchanov,P.,Mallya,A.,Tyree,S.,Frosio,I.,andKautz,
J. Importanceestimationforneuralnetworkpruning. In
Heusel,M.,Ramsauer,H.,Unterthiner,T.,Nessler,B.,and
CVPR,June2019.
Hochreiter,S. Ganstrainedbyatwotime-scaleupdate
ruleconverge toalocalnashequilibrium. In NeurIPS, Nichol,A.Q.andDhariwal,P.Improveddenoisingdiffusion
2017. probabilisticmodels. InICML,2021.
Hinton,G.,Vinyals,O.,andDean,J. Distillingtheknowl- Paganini, M. Prune responsibly. arXiv preprint arXiv:
edgeinaneuralnetwork. InNeurIPS-W,2014. 2009.09936,2020.
10LayerMerge:NeuralNetworkDepthCompressionthroughLayerPruningandMerging
Paszke,A.,Gross,S.,Chintala,S.,Chanan,G.,Yang,E.,
DeVito,Z.,Lin,Z.,Desmaison,A.,Antiga,L.,andLerer,
A. Automaticdifferentiationinpytorch. InNeurIPS-W,
2017.
Ronneberger,O.,Fischer,P.,andBrox,T. U-net: Convolu-
tionalnetworksforbiomedicalimagesegmentation. In
MICCAI,2015.
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh,
S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bern-
stein,M.,etal. Imagenetlargescalevisualrecognition
challenge. InIJCV,2015.
Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and
Chen,L.-C. Mobilenetv2: Invertedresidualsandlinear
bottlenecks. InCVPR,2018.
Shen,M.,Yin,H.,Molchanov,P.,Mao,L.,Liu,J.,andAl-
varez,J.Structuralpruningvialatency-saliencyknapsack.
InNeurIPS,2022.
Song, J., Meng, C., and Ermon, S. Denoising diffusion
implicitmodels. InICLR,2021.
Tran,C.,Fioretto,F.,Kim,J.-E.,andNaidu,R. Pruninghas
adisparateimpactonmodelaccuracy. InNeurIPS,2022.
Vanholder, H. Efficient inference with tensorrt. In GTC,
2016.
Wightman,R.,Touvron,H.,andJe´gou,H. Resnetstrikes
back: An improved training procedure in timm. In
NeurIPS-W,2021.
Wu,Y.andHe,K. Groupnormalization. InECCV,2018.
11LayerMerge:NeuralNetworkDepthCompressionthroughLayerPruningandMerging
A.ImplementationDetails
Inthissection,weprovidetheimplementationdetailsregardingthedifferenttypesoflayersandnetworks.
Normalizationlayers BothResNet-34andMobileNetV2utilizeabatchnormalizationlayerfornormalization(Ioffe&
Szegedy,2015). Atinferencetime,wefusethebatchnormalizationlayerwiththeconvolutionlayer,beforemergingthe
network. Ontheotherhand,DDPMemploysagroupnormalizationlayerfornormalization(Wu&He,2018). Unlikebatch
normalization,thislayercannotbefusedwiththeconvolutionlayeratinferencetimebecauseitusestest-timefeaturemap
statistics. Toaddressthis,beforefine-tuning,wemoveanygroupnormalizationlayerbetweensuccessiveconvolutionlayers
thatwillbemerged,toaftertheseconvolutionlayers. Iftherearemultiplesuchnormalizationlayers,weonlykeepthelast
one. Thisadjustmentallowsustomergetheconsecutiveconvolutionlayersatinferencetime.
Stridedconvolutionsandpadding Forconvolutionlayersthathaveastridelargerthan1,werestrictmergingthemwhen
thekernelsizeofthefollowingconvolutionlayerislargerthan1,asdoneinKimetal.(2023). Wethusrestrictthechoiceof
Atoincludetheactivationlayersbetweensuchlayers. Thisisbecausemergingtheconvolutionlayerwithastridelarger
than1significantlyincreasesthemergedkernelsize. Concretely,mergingtheconsecutiveconvolutionlayersf ◦f
θ2 θ1
resultsinthemergedkernelsizeof
Ker(θ )=(Ker(θ )−1)×Str(f )+Ker(θ ),
merged 2 θ1 1
whereStr(·)denotesthestrideoftheconvolutionlayer(Fuetal.,2022).
AsnotedinKimetal.(2023),consecutiveconvolutionlayersthatwillbemergedshouldnothaveanypaddinginbetweento
avoiddiscrepanciesattheboundaryoftheoutputbeforeandaftermerging. Toaddressthis,beforefine-tuning,wereorder
thepaddingofthenetworktobeappliedpriortoeachsuchblockofconsecutiveconvolutionlayers. asdoneinKimetal.
(2023).
Skip-connections There are two different types of skip-connection in a CNN: Skip-addition and skip-concatenation.
Skip-addition,employedbyResNetandMobileNetV2,addstheoutputofanearlierlayerdirectlytotheoutputofalater
layer. Wecanfuseskip-additionintotheconvolutionlayerifeveryintermediateconvolutionlayerismergedintoasingle
layer(Dingetal.,2021). Wethuscanmergeacrossaskip-additiononlyifeveryconvolutionlayerinbetweenismergedinto
asinglelayer. Conversely,skip-concatenation,usedinDDPM,concatenatestheoutputofanearlierlayerdirectlytothe
outputofalaterlayer. Wedonotmergelayersacrossaskip-concatenation.
MobileNetV2 InMobileNetV2,itisnotablethatthereisnonon-linearactivationlayerfollowingeachInvertedResidual
Block(Sandleretal.,2018). Priorworksondepthcompressionsuggestthattheperformanceofacompressednetworkcan
beimprovedbyaddinganon-linearactivationlayerafterthemergedlayers(Fuetal.,2022;Kimetal.,2023). Wealso
adaptthistrickinourimplementationonMobileNetV2architecture.
DDPM TheDDPMarchitectureusesanupsamplingmoduletoincreasethespatialdimensionofthefeaturemap,andit
furtheremploysaself-attentionlayeratthe16×16resolutionbetweentheresidualblocks(Chenetal.,2018b;Hoetal.,
2020). Wedonotmergeconvolutionlayersacrosstheself-attentionlayerortheupsamplinglayerintheDDPMarchitecture.
ItisalsoworthmentioningthattheDDPMarchitecturehasa3×3convolutionlayerwithstride1betweentheupsampling
layerandtheskipconcatenation. Weincludetheseconvolutionlayersaspotentialpruningcandidatesinouralgorithm,as
thisleadstoimprovedperformanceofthecompressednetwork.
B.Proof
Inthissection,weproveTheorem3.1,restatedhereforconvenience.
Theorem3.1.
ThesolutionA∗and(k∗)|A∗|+1givenbyAlgorithm1isanoptimalsolutionofProblem(5).
i i=1
Proof. Weprovethisbyinduction. Supposethatforl < l andt < t ,A[l,t]andk(lt) := C[l,t]∩(a(lt),a(lt)]arethe
0 0 i i−1 i
optimalsolutionofProblem(6),where(a(lt))|A[l,t]|denotestheelementofA[l,t]inascendingorder.
i i=1
12LayerMerge:NeuralNetworkDepthCompressionthroughLayerPruningandMerging
AssumethatA[l ,t ]andk(l0t0) fromEquation(7)isnottheoptimalsolutionofProblem(6)whenl = l ,t = t . We
0 0 i 0 0
denotetheoptimalsolutionofProblem(6)forl=l 0,t=t 0asA(cid:98)andkˆ i.
|A (cid:88)(cid:98)|+1
I(cid:2) aˆ ,aˆ ,kˆ (cid:3)
>|A[l0(cid:88),t0]|+1
I(cid:2) a(l0t0),a(l0t0),k(l0t0)(cid:3) , (9)
i−1 i i i−1 i i
i=1 i=1
|A(cid:98)|+1
(cid:88) T(cid:2) aˆ ,aˆ ,kˆ (cid:3) <t . (10)
i−1 i i 0
i=1
WefirstshowthatA(cid:98)isnotemptybecause
|A(cid:98)|+1
(cid:88) I(cid:2) aˆ ,aˆ ,kˆ (cid:3)
i−1 i i
i=1
>|A[l0(cid:88),t0]|+1
I(cid:2) a(l0t0),a(l0t0),k(l0t0)(cid:3)
=M[l ,t ] (fromtheassumption&definitionofA[l ,t ])
i−1 i i 0 0 0 0
i=1
= max (M[l′,t−T[l′,l ,k]]+I[l′,l ,k]) (fromtherecurrencerelationEquation(7))
0 0
0≤l′<l0,k∈K l′l0
≥ max I[0,l ,k].
0
k∈K0l0
Now,letˆl:=aˆ bethemaximumvalueofA(cid:98). Then,
|A(cid:98)|
|A(cid:98)|+1
(cid:88) I(cid:2) aˆ ,aˆ ,kˆ (cid:3)
i−1 i i
i=1
|A(cid:98)|
=(cid:88) I(cid:2) aˆ ,aˆ ,kˆ (cid:3) +I[ˆl,l ,kˆ ]
i−1 i i 0 |A(cid:98)|+1
i=1
≤M[ˆl,t −T[ˆl,l ,kˆ ]]+I[ˆl,l ,kˆ ] (fromtheoptimalityassumption)
0 0 |A(cid:98)|+1 0 |A(cid:98)|+1
≤M[l ,t
]=|A[l0(cid:88),t0]|+1
I(cid:2) a(l0t0),a(l0t0),k(l0t0)(cid:3)
, (fromtherecurrencerelationEquation(7))
0 0 i−1 i i
i=1
whichcontradictstoEquation(9). Notethattheinequalitywithfromtheoptimalityassumptionholdsbecauseweassumed
theoptimalityforl=ˆl<l andt=t −T[ˆl,l ,kˆ ]<t ,andfromEquation(10),wehave
0 0 0 |A(cid:98)|+1 0
|A(cid:98)|
(cid:88) T(cid:2) aˆ ,aˆ ,kˆ (cid:3) <t −T[ˆl,l ,kˆ ],
i−1 i i 0 0 |A(cid:98)|+1
i=1
whichsatisfiestheconstraintinProblem(6). Therefore,fromthecontradictionwehavethatA[l ,t ]andk(l0t0)areindeed
0 0 i
optimalsolutionsofProblem(6)(l=l andt=t ).
0 0
Forthebasecasel=0andt= T0,A[l,t]=∅isindeedthesolutionofProblem(6).
P
Now,plugginginl=Landt=T provesthetheorem.
0
C.DetailsonConstructingImportanceandLatencyTables
Inthissection,weprovidedetailsonhowwemeasuretheimportanceandlatencyvaluesforthelookuptablesI andT,
alongwiththeircorrespondingpracticalcomputationcostfordifferenttypesofnetworks.
13LayerMerge:NeuralNetworkDepthCompressionthroughLayerPruningandMerging
Table 7: Wall-clock time for constructing the importance and latency look-up tables. GPU hours for constructing the
importancetableismeasuredinRTX3090andthelatencytableismeasuredinRTX2080Ti.
Importancetable Latencytable #oftable
Network Dataset (GPUHours) (GPUHours) entries
ResNet-34 ImageNet 4.4hours 25.9minutes 150
MobileNetV2-1.0 ImageNet 13.2hours 6.2minutes 391
MobileNetV2-1.4 ImageNet 15.0hours 10.6minutes 391
DDPM CIFAR10 2.5hours 1.3minutes 98
Table8: Wall-clocktimeforconstructingtheimportancelook-uptableforResNet-34andMobileNetV2-1.0indifferent
methods. GPUhoursforconstructingtheimportancetableismeasuredinRTX3090.
(a)ResNet-34onImageNetdataset. (b)MobileNetV2-1.0onImageNetdataset.
#oftable #oftable
Method GPUHours entries Method GPUHours entries
Depth(Kimetal.,2023) 25.8hours 62 Depth(Kimetal.,2023) 126.0hours 315
LayerOnly(Ours) 0.8hours 29 LayerOnly(Ours) 0.4hours 13
LayerMerge(Ours) 4.4hours 150 LayerMerge(Ours) 13.2hours 391
Importancemeasurements WedefinedimportancevaluesinEquation(4). Recallthatthefirsttermisestimatedwiththe
performanceofthenetworkafterfine-tuningforafewsteps. Forthat,weselectarandomsubsetofthetrainingdatasetfor
fine-tuning,thenweevaluateperformanceonanotherseparatesubset,alsodrawnfromthetrainingdataset. Forthesecond
term,weevaluatetheperformanceofthepre-trainednetworkontheseparatesubset. Inparticular,weuseafine-tuning
subsetofsize4%ofthetotaltrainingdatasetsizeforImageNet,and1%forCIFAR10. Theseparatesubsetisalsothesame
sizeasthefine-tuningsubset. Wefine-tunethenetworkfor1epochforImageNet,and50epochsforCIFAR10withthe
fine-tuningsubsetwhenwemeasuretheimportance.
Wereportthewall-clocktimeforconstructingtheimportancelook-uptableinTable7. Itisworthmentioningthatthis
computationcanbedoneinanembarrassinglyparallelfashionwithoutanycommunicationbetweenGPUs,whichallows
forsignificantspeedupifmultipleGPUsareavailable. Forinstance,theimportancetableforMobileNetV2-1.0onlytook33
minuteswith24GPUs.
InTable8,wefurthercomparethewall-clocktimerequiredtoconstructtheimportancelook-uptablesusedinLayerMerge
andLayerOnlytotheoneusedinDepth(Kimetal.,2023). ItisworthnotingthattheDepthbaselinefine-tunesforone
epochoverthefulltrainingdatasettoevaluateeachtableentry. However,weobservethatfine-tuningforoneepochusing
onlyasmallrandomsubsetofthetrainingdatasetissufficientforestimatingtheimportancevalues. Thissignificantly
reducesthewall-clocktimerequiredtoconstructthelook-uptableinourmethodcomparedtotheDepthbaseline.
Latencymeasurements TomeasureeachlatencyvalueinthelookuptableT,wemeasuretheinferencetimeonPyTorch
byfirstwarminguptheGPUfor300forwardpasses,thenaveragingtheinferencetimeoverthesubsequent200forward
passes,withabatchsizeof128. Latencyvaluesaremeasuredinmilliseconds. Wereportthewall-clocktimeforconstructing
thelatencylook-uptableinTable7.
Recallthattosolvethesurrogateproblem,wefirstneedtodiscretizethelatencyvalues. Wedothatbymultiplyingthe
realvaluedlatenciesinthelookuptableandT by10,thenroundingthemdowntothenearestinteger. Notethatthisis
0
equivalenttochoosingthediscretizationlevelasP =10T .
0
D.DetailsonAblationStudies
Inthissection,weoutlinethedetailsofthesequentialoptimizationbaseline(Depth→LayerOnly)presentedinTable6.
RecallthatforMobileNetV2,wefine-tuneeverycompressednetworkfor180epochs,usingthesamefine-tuningrecipe
14LayerMerge:NeuralNetworkDepthCompressionthroughLayerPruningandMerging
Table9: Accuracyandlatencyspeed-upofapplyingcompressionmethodstoMobileNetV2-1.0onImageNetdatasetwith
90,30,and20fine-tuningepochs. Thelatencyspeed-upismeasuredonRTX2080TiGPUatbatchsize128.
(a)Resultsoffine-tuningfor90epochs. (b)Resultsoffine-tuningfor30epochs. (c)Resultsoffine-tuningfor20epochs.
PyTorch TensorRT PyTorch TensorRT PyTorch TensorRT
Network Acc(%)↑ Speed-up↑ Speed-up↑ Network Acc(%)↑ Speed-up↑ Speed-up↑ Network Acc(%)↑ Speed-up↑ Speed-up↑
MobileNetV2-1.0 72.89 1.00× 1.00× MobileNetV2-1.0 72.89 1.00× 1.00× MobileNetV2-1.0 72.89 1.00× 1.00×
AMC-70%(Heetal.,2018) 71.66 1.32× 1.34× AMC-70%(Heetal.,2018) 71.05 1.32× 1.34× AMC-70%(Heetal.,2018) 70.64 1.32× 1.34×
Depth-74%(Kimetal.,2023) 72.49 1.62× 1.42× Depth-74%(Kimetal.,2023) 71.59 1.62× 1.42× Depth-74%(Kimetal.,2023) 71.09 1.62× 1.42×
LayerOnly-73%(Ours) 69.29 1.30× 1.35× LayerOnly-73%(Ours) 67.60 1.30× 1.35× LayerOnly-73%(Ours) 66.96 1.30× 1.35×
LayerMerge-55%(Ours) 72.73 1.63× 1.42× LayerMerge-55%(Ours) 72.06 1.63× 1.42× LayerMerge-55%(Ours) 71.59 1.63× 1.42×
70.0
65.0
60.0
55.0
AMC(1.32×,72.01%)
50.0 Depth(1.62×,72.83%)
LayerOnly(1.30×,69.66%)
2e-1
1e-1 LayerMerge(1.63×,72.99%)
0
1 45 90 135 180
Epochs
Figure 3: Test accuracy recovery curve of different compression methods across fine-tuning epochs. We indicate the
associatedspeed-upandaccuracyafterfine-tuningintheparentheses. TheinferencetimeismeasuredonRTX2080TiGPU
atbatchsize128inPyTorchformat.
asKimetal.(2023). Forthesequentialoptimizationbaseline,wedividethefine-tuningepochsequallybetweenthetwo
pruningmethods,i.e.,wefine-tunefor90epochsaftereachpruningmethod,againusingthesamefine-tuningrecipe.
Wefixthelatencybudgetratiop%forDepthto74%,whichyieldsaspeed-upof1.63×. Thenweusetwodifferentvalues
ofpforLayerOnly: 72%and64%. NotethatpherecorrespondstoT /T ,whereT isthechosenlatencybudget
0 depth-pruned 0
ofthefinalprunedmodelandT isthelatencyofthemodelprunedusingDepth-74%. Itisworthnotingthatthe
depth-pruned
allocationsoffine-tuningepochsandcompressionratiosbetweenthetwopruningmethodsarehyperparametersthatneedto
betuned. However,ourmethodisfreefromthesehyperparametersduetothejointoptimizationonthetwotypesofpruning
modalities.
E.AdditionalExperiments
Resultswithsmallerfine-tuningepochs Inthissection,westudytheeffectoffine-tuningforashortertime. Inparticular,
wepresentinTable9compressionresultsonMobileNetV2-1.0wherewefine-tuneallmethodsfor90,30,and20epochs,
usingcosinelearningratedecay. WefurtherplotinFigure3therecoverycurveoftestaccuracyacrossfine-tuningsteps.
Ourmethodconsistentlyoutperformsbaselinesunderthesesmallerfine-tuningbudgetsaswell.
Comparisonwithknowledgedistillation Inthissection,wecompareourmethodtotheknowledgedistillationmethodof
Hintonetal.(2014). Forthat,weuseasmallerversionofMobileNetV2(Sandleretal.,2018)thantheoneusedforthe
pre-trainednetworkasthestudentnetworkandtrainitforthesamenumberofepochsweuseforfine-tuninginourmethod
(180epochs). WepresenttheresultsinTable10andplottherecoverycurveoftestaccuracyacrossfine-tuningstepsin
Figure4. Thekeybenefitofpruningmethodslikeoursoverknowledgedistillationisthattheyonlyrequirefine-tuning
themodel,whileknowledgedistillationrequirestrainingthesmallmodelfromscratch. Thisprovidesanadvantagewhen
bothmethodsarecomparedunderanidenticaltrainingbudget. Indeedourresultsshowthatourmethodoutperformsthe
knowledgedistillationmethodinthissetting.
Applyingknowledgedistillationduringfine-tuning Itisworthnotingthatknowledgedistillationmethodscanbejointly
appliedwithpruningmethods,byconsideringthepre-trainednetworkastheteachernetworkandtheprunednetworkasthe
studentnetworktotrain. WepresentinTable11theresultsofapplyingtheknowledgedistillationmethodofHintonetal.
(2014)todifferentpruningmethods. Weobservethatapplyingknowledgedistillationfurtherimprovestheaccuracyofthe
15
↑)%(ycaruccALayerMerge:NeuralNetworkDepthCompressionthroughLayerPruningandMerging
Table10: Accuracyandlatencyspeed-upcomparisonbetweenknowledgedistillation(Hintonetal.,2014)andourmethod
withMobileNetV2-1.0andMobileNetV2-1.4onImageNetdataset. Thelatencyspeed-upismeasuredonRTX2080TiGPU
atbatchsize128.
(a)MobileNetV2-1.0onImageNetdataset. (b)MobileNetV2-1.4onImageNetdataset.
PyTorch TensorRT PyTorch TensorRT
Network Acc(%)↑ Speed-up↑ Speed-up↑ Network Acc(%)↑ Speed-up↑ Speed-up↑
MobileNetV2-1.0 72.89 1.00× 1.00× MobileNetV2-1.4 76.28 1.00× 1.00×
Knowledgedistillation(MobileNetV2-0.75) 69.69 1.17× 1.20× Knowledgedistillation(MobileNetV2-1.0) 72.30 1.51× 1.54×
LayerMerge-55%(Ours) 72.99 1.63× 1.42× LayerMerge-43%(Ours) 74.91 1.99× 1.61×
70
60
Knowledgedistillation(1.17×,69.69%)
LayerMerge(1.63×,72.99%)
50
1 45 90 135 180
Epochs
Figure4: Testaccuracyrecoverycurveofourmethodcomparedtoknowledgedistillationacrossfine-tuningepochsfor
MobileNetV2-1.0. Weindicatetheassociatedspeed-upandtheaccuracyafterfine-tuningintheparentheses. Theinference
timeonRTX2080TiGPUatbatchsize128inPyTorchformat.
prunednetwork(comparedtoTable2),andourmethodconsistentlyoutperformsthebaselinesinthissettingaswell.
Additionalcompressionresults Inthissection,wepresentadditionalresultscomparingvariouscompressionmethods
acrossdifferentnetworkarchitectures(ResNet-34,MobileNetV2-1.0,MobileNetV2-1.4,andDDPM)andcompression
ratios. We display the Pareto curves for each method on the different architectures in Figure 5. We report the latency
speed-upmeasuredinPyTorchformat. FortheResNetandMobileNetV2architectures,weplotaccuracyagainstspeed-up,
andfortheDDPMarchitecture,weplottheFIDmetricagainstthelatencyspeed-up.
16
↑)%(ycaruccALayerMerge:NeuralNetworkDepthCompressionthroughLayerPruningandMerging
Table11: Accuracyandlatencyspeed-upofapplyingknowledgedistillation(Hintonetal.,2014)withcompressionmethods
toMobileNetV2-1.0onImageNetdataset. Thelatencyspeed-upismeasuredonRTX2080TiGPUatbatchsize128.
PyTorch TensorRT
Network Acc(%)↑ Speed-up↑ Speed-up↑
MobileNetV2-1.0 72.89 1.00× 1.00×
AMC-70%(Heetal.,2018) 72.04 1.32× 1.34×
Depth-74%(Kimetal.,2023) 72.99 1.62× 1.42×
LayerOnly-73%(Ours) 69.70 1.30× 1.35×
LayerMerge-55%(Ours) 73.14 1.63× 1.42×
Depth-66%(Kimetal.,2023) 72.31 1.88× 1.57×
LayerMerge-46%(Ours) 72.56 1.90× 1.65×
Depth-59%(Kimetal.,2023) 71.76 2.07× 1.79×
LayerMerge-38%(Ours) 72.06 2.18× 1.84×
Depth-53%(Kimetal.,2023) 70.81 2.47× 1.97×
LayerMerge-33%(Ours) 71.32 2.49× 2.05×
17LayerMerge:NeuralNetworkDepthCompressionthroughLayerPruningandMerging
(a)ResNet-34onImageNetdataset. (b)MobileNetV2-1.0onImageNetdataset.
Original HALP Depth Original AMC Depth
LayerOnly(Ours) LayerMerge(Ours) LayerOnly(Ours) LayerMerge(Ours)
73
74.0
72
73.0
71
72.0
70
71.0
69
1.0 1.2 1.4 1.6 1.8 2.0 1.0 1.4 1.8 2.2 2.6
LatencySpeed-up(×)↑ LatencySpeed-up(×)↑
(c)MobileNetV2-1.4onImageNetdataset. (d)DDPMonCIFAR10dataset.
Original MetaPruning Depth Original Depth
LayerOnly(Ours) LayerMerge(Ours) LayerOnly(Ours) LayerMerge(Ours)
10
76
9
74
8
72 7
6
70
5
68
4
1.0 1.4 1.8 2.2 2.6 1.0 1.05 1.1 1.15 1.2 1.25 1.3 1.35
LatencySpeed-up(×)↑ LatencySpeed-up(×)↑
(e)ChannelprunedDDPMonCIFAR10dataset.
Diff-Pruning Diff-Pruning-60%→Depth
Diff-Pruning-60%→LayerOnly(Ours) Diff-Pruning-60%→LayerMerge(Ours)
11
10
9
8
7
2.3 2.35 2.4 2.45 2.5 2.55 2.6 2.65
LatencySpeed-up(×)↑
Figure5:Paretocurveofeachcompressionmethodappliedtoeachnetwork. Thelatencyspeed-upismeasuredonRTX2080
TiGPUinPyTorchformat,withbatchsizeof128forImageNetdatasetandbatchsizeof64forCIFAR10dataset.
18
↑)%(ycaruccA
↑)%(ycaruccA
↓cirtemDIF
↑)%(ycaruccA
↓cirtemDIF