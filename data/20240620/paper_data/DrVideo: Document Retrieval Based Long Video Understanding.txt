DrVideo: Document Retrieval Based Long Video Understanding
ZiyuMa*1,ChenhuiGou*2,HengcanShi1,2,BinSun†1,ShutaoLi1,
HamidRezatofighi2,JianfeiCai2
1 CollegeofElectricalandInformationEngineering,HunanUniversity,
2 DataScience&AIDepartment,FacultyofIT,MonashUniversity
{maziyu, sunbin611, shutao_li}@hnu.edu.cn
{chenhui.gou, hengcan.shi, hamid.rezatofighi, jianfei.cai}@monash.edu
*Equalcontribution,†Correspondingauthor
Abstract
Existing methods for long video understand-
ingprimarilyfocusonvideosonlylastingtens
of seconds, with limited exploration of tech-
niques for handling longer videos. The in-
creased number of frames in longer videos
presentstwomainchallenges: difficultyinlo-
cating key information and performing long-
rangereasoning. Thus,weproposeDrVideo,a
document-retrieval-basedsystemdesignedfor
longvideounderstanding. Ourkeyideaisto
convertthelong-videounderstandingproblem
into a long-document understanding task so
as to effectively leverage the power of large
languagemodels. Specifically,DrVideotrans-
formsalongvideointoatext-basedlongdoc-
umenttoinitiallyretrievekeyframesandaug-
Figure1: AccuracyonEgoSchema(Mangalametal.,
ment the information of these frames, which
2023),MovieChat-1K(Songetal.,2023)globalmode,
is used this as the system’s starting point. It
andLLama-Vid-QA(Lietal.,2023c). Ourmethod,
thenemploysanagent-basediterativeloopto
DrVideo,achievesthebestresultsonthesethreebench-
continuously search for missing information,
marksandhassignificantadvantagesonlongervideos.
augmentrelevantdata,andprovidefinalpredic-
tionsinachain-of-thoughtmanneroncesuffi-
cientquestion-relatedinformationisgathered.
Extensive experiments on long video bench- (Ouyangetal.,2022;OpenAI,2022;Brownetal.,
marksconfirmtheeffectivenessofourmethod. 2020; Scao et al., 2022; Touvron et al., 2023;
DrVideooutperformsexistingstate-of-the-art Chowdhery et al., 2022; Ma et al., 2024) demon-
methods with +3.8 accuracy on EgoSchema
strate strong abilities in language understanding
benchmark(3minutes),+17.9inMovieChat-
and reasoning across long text sequences. These
1Kbreakmode,+38.0inMovieChat-1Kglobal
advancements have inspired the development of
mode(10minutes),and+30.2ontheLLama-
video-language models (Video-LMs) to address
VidQAdataset(over60minutes).
longvideounderstandingissues.
1 Introduction
Video-LMs typically encode a video as a se-
Videounderstandingisachallengingtaskincom- quence of visual tokens, which are then concate-
puter vision, requiring the processing of spatio- nated with language tokens into a long sequence
temporalinformationandadvancedreasoningabil- andthenuseLLMsasdecoderstounderstandthis
ities. Previousworkshavesuccessfullyprocessed combined sequence. While Video-LMs improve
shortvideoslastingaroundtensofseconds(Yang videounderstanding,theirperformanceisstilllim-
etal.,2021;Chengetal.,2023;Leietal.,2021;Yu ited. Tohandlelongervideos,thesemethodstypi-
et al., 2023; Yang et al., 2022; Surís et al., 2023; callyperformuniformorrandomframesamplingat
Linetal.,2023b). However,howtoextendthese largestrideswithoutconsideringcontent(Yuetal.,
methods to long videos remains unclear. Recent 2024), leading to potential key information loss.
advancements in large language models (LLMs) Additionally,thesimpleconcatenationofvisualto-
1
4202
nuJ
81
]VC.sc[
1v64821.6042:viXrakensincreasesthedifficultyfortheLLMtolocate than 1 hour, which are shown in Fig. 1. Finally,
question-related (key) visual information within wesummarizethecontributionsofDrVideointhe
thelongvisionsequence,complicatinglong-range followingthreeaspects:
reasoningacrossthevisiontokensequences.
Asdiscussedabove,thesetwokeyissuesinlong • Asystemisproposedtoadaptlong-videoun-
videounderstandingremainchallengingforcurrent derstandingtolong-documentunderstanding,
Video-LMs: accurately locating question-related which can better take advantage of the long-
videoclipsorimagesandreasoningoverlongse- rangereasoningabilityofLLMs.
quences. While these problems are difficult for
• A novel multi-stage interactive loop is pro-
Video-LMstosolve,LLMshavedemonstratedim-
posedtocontinuouslysearchformissingin-
pressive capabilities in key information location
formationanddynamicallyaugmenttheinfor-
andlong-formreasoningonvariouslanguage-only
mation.
tasks,suchasdocumentretrievalandlong-context
understanding. Consideringthatsomelongvideos,
• DrVideo achieves state-of-the-art results on
likemovies,areinitiallywrittenasscriptsorbooks,
three long video benchmarks and exceeds
humanscananswerquestionsbyeitherwatching
existing state-of-the-art methods with +3.8
the videos or reading the books. Inspired by this
accuracy on the EgoSchema benchmark (3
observation,convertinglong-videounderstanding
minutes),+17.9inMovieChat-1Kbreakpoint
intoalong-documentunderstandingtaskmayeffec-
mode, +38.0inMovieChat-1Kglobalmode
tivelyleveragethepoweroflargelanguagemodels.
(10 minutes), and +30.2 on the LLama-Vid-
Recentworks(Zhangetal.,2023a)haveattempted
QAdataset(over60minutes).
todevelopstrategiestoconvertvideosintotextcap-
tionsanduseLLMsforreasoningbasedonthese
2 RelatedWork
captions. However,thechallengeofconvertinga
videointoalongdocumentwithoutlosingkeyin- Long Video Understanding. Modeling long
formation relevant to solving the question is not videos,whichareseveralminutesormoreinlength,
fullyexplored. Additionally,locatingthekeyinfor- generally necessitates advanced temporal model-
mation(clipsorframes)hasnotbeenthoroughly ing capabilities, resulting in complex model de-
investigated. signs. LF-VILA (Sun et al., 2022) introduces a
Therefore,inthispaper,weproposeDrVideo,a TemporalWindowAttention(HTWA)mechanism
documentretrieval-basedsystemforlongvideoun- tocapturelong-rangedependenciesinlongvideos.
derstandingthatprogressivelyfindskeyframesand MeMViT(Wuetal.,2022)andMovieChat(Song
generatesquestion-relatedinformationforLLMs et al., 2023) employs a memory-based design
tomakeconfidentpredictions. DrVideocomprises to save question-related information from previ-
fivecomponents: (i)avideo-documentconversion ouslyvideosegments. Somepublishedapproaches
modulethattransformsrawlongvideosintolong employ space-time graphs (Hussein et al., 2019;
documents, (ii) a retrieval module that identifies Wang et al., 2021) or relational space-time mod-
frames related to specific video questions, (iii) a ules(Yangetal.,2023)tocapturespatio-temporal
documentaugmentation(DA)modulethatenriches dependenciesfromtherawlongvideos. Recently,
the information of question-related frames (key S4ND(Nguyenetal.,2022),ViS4mer(Islamand
frames), (iv) a multi-stage agent interaction loop Bertasius,2022),andS5(Wangetal.,2023a)have
thatdynamicallyfindsthepotentialmissinginfor- used Structured State-Space Sequence (S4) (Gu
mationthroughsequentialstepsofreasoningand et al., 2021) layers to capture long-range depen-
theninteractswiththeDAmoduletorequestaug- dencies in videos. Different from these methods,
mentation of the information, (v) an answering ourDrVideodoesnotdesignacomplexmoduleto
modulethatprovidesanswersandthelogicalpro- performlongvideounderstandingcapacity. Onthe
cessofobtainingthoseanswersbasedonsufficient contrary, we develop a document-retrieval-based
information. Wevalidatetheeffectofeachcompo- systemforzero-shotlongvideounderstanding.
nent in Section 4.4. Extensive experiments show LLMsforVideoUnderstanding. Therecentrise
our method, DrVideo, exceeds existing state-of- of LLMs (Zeng et al., 2022) and VideoChat (Li
the-artmethodsacrossdifferentlongvideobench- etal.,2023b)alignthevisualfeaturesextractedby
marks, from 3 minutes to 10 minutes and longer pretrainedvisualmodelstoLLMsandapplythem
2Question
Taking into account all the
actionsperformedbyc,what Updated Video Document
can you deduce about the
Frame 1 p wr ii tm hia nry theo vb ij de ec otiv ce ona ten nd t?focus F…ram…e4
Question
Final Video Document
Fra …me 2 Document Frame24
TopKframes Document
(Frames)
Augmentation
Video- Retrieval
Ques-
Document Module tion
Initial Video Document
Frame 89
Conversion
Module Potential keyframes
UpdatedVideo Document
Frame 90 Answer
RawVideo Multi-Stage Agent Interaction Loop
Figure 2: Overview of our DrVideo. It comprises five components: a video-document conversion module, a
retrievalmodule,adocumentaugmentationmodule,amulti-stageagentinteractionloopandanansweringmodule.
into video understanding filed. Video ChatCap- todistinguishitfromexistingmethods.
tioner (Chen et al., 2023) and ChatVideo (Wang
et al., 2023b) utilize LLMs to represent videos 3 Method
andengageusersthroughdialogues,respectively.
VidIL(Wangetal.,2022b)appliestheimage-level Inthissection,wedetailourproposedDocument
modelsintovideounderstandingtaskviafew-shot RetrievalBasedsystemforlongvideounderstand-
learning. In addition to short-term video under- ing. AsillustratedinFigure2,givenalongvideo
standing,recentstudies(Linetal.,2023a;Chung andaquestionaboutthevideo,DrVideofirsttrans-
andYu,2023;Bhattacharyaetal.,2023)haveex- latesthelongvideointoalongdocument,referred
ploredLLMsforlong-rangevideomodeling. For toastheinitialvideodocument. Then,theretrieval
instance, GPT-4 is applied in various long-range moduleidentifiesthetopK keyframesbycalcu-
videomodelingtasksin(Linetal.,2023a),though latingthesimilaritybetweenthequestionandthe
quantitativeevaluationislimited. Meanwhile,re- initial video document. The document augmen-
searchin(ChungandYu,2023)focusesonmovie tation module enriches the information of these
datasetswithminimalvisualanalysis(Mangalam key frames and adds it to the initial video doc-
etal.,2023),relyinglargelyonspeechandsubtitles. ument, creating a new video document, referred
Unlike this, DrVideo focuses on vision modality to as the updated video document, which is used
andneverusesthepriorlanguageinformation. asthemulti-stageagentinteractionloop’sstarting
point.
LLM Agents. In parallel, the computer vision Theloopcontainstwodistinctagents: theplan-
communityhasstartedexploringtheuseofLLMs ningagentandtheinteractionagent. Theplanning
asagentsinvariousvisualdomainslikeGUIunder- agentjudgeswhethertheupdatedvideodocument
standing and robot navigation (Surís et al., 2023; issufficientforansweringthequestion. Ifnot,the
Hongetal.,2023;Driessetal.,2023;Brohanetal., updated video document is fed into the interac-
2023). In the realm of long-form video compre- tionagenttodynamicallyfindmissingkeyframes.
hension,initialeffortshaveemployedanagent-like Theinteractionagentinteractswiththedocument
approach,whereLLMsinteractwithexternaltools augmentation module to request the required in-
orintegrateadditionalfunctionalities(Surísetal., formationofthesekeyframes. Theoutputofthe
2023; Gao et al., 2023; Yang et al., 2024; Wang documentaugmentationmoduleisaddedtothecur-
etal.,2024;Fanetal.,2024). Incontrasttothese rent document to obtain the latest updated video
approaches, our method, DrVideo, reconceptual- document. Thenewvideodocumentisthensentto
izesvideounderstandingasaprocessaugmented theplanningagentagaintofurtherjudgewhether
by document retrieval, inspired by how humans thecurrentinformationissufficient. Thissearching
watchvideosorreadbooks. Thisviewleadstothe and interaction loop continues until the planning
developmentofDrVideo,whichfeaturesdocument agentconsidersthecurrentinformationsufficient
retrievalaugmentationandmulti-stageinteractivity foransweringthequestionorthemaximumitera-
3① Video-Document ConversionModule
… Image Captioning
Model
Frame 1 Frame 2 Frame 3 Frame 4 Frame 5 Frame 87 Frame 88 Frame 89 Frame 90
RawVideo InitialVideo Document
② Document (Frames) Retrieval Module ③ Document Augmentation Module
Frame4:Theimageshowsapersonwashing
aT ca
i
cnk oti uon
n
g
t Embedding sime ilm arb ite yd bd ei tn wg een Frame4
d ini ts ohe ts hein sina kk ai ntc dh te hn es pi rn ek s. e… nc… eofplacingadish
all the actions questionandthe Frame24:Theimageshowsapersonwashing
performe wholedocument dishesinakitchensink.……of
d by c, Frame24
what can
you Embedding Frame59:Theimageshowsapersonwashing
deduce dishesinakitchensink.……ofplacinga
about the
op br ji em ca tir vy e Frame59
and focus Frame79:Theimageshowsapersonwashing
within the dishesinakitchensink.……ofplacingadish
video intothesinkandthepresence
content? Frame79
Question Initial Video Documents TopKkeyframes DocumentAugmentation
Document (Frames) Retrieval Module Document Augmentation Module Updated Video Document
Figure 3: Illustration of video-document conversion module, document (frames) retrieval module and
documentaugmentationmodule.
tionisreached. Aftertheloopends,thefinalvideo Specifically, we use openAI embedding model
document(sufficientinformation)andtherelated (OpenAI, 2024) to get the vector representation
questionaregiventotheansweringmoduletoget of this document. E = ϕ (Doc ) , while
doc emb init
thefinalprediction. Theentiresystemissumma- ϕ is the embedding model. Given a specific
emb
rizedasAlgorithm1. retrieval text RT, the retrieval module first com-
putes the embedding E = ϕ (RT). RT is
RT emb
3.1 Video-DocumentConversionModule
setasthequestionQaboutthevideoinourmethod.
To represent the long video as the initial video Then,thetopK framesareretrievedbasedonthe
documentDoc init ,weconsidersinglevideoframe cosinesimilaritybetweenE RT andE doc:
orvideoclipV t asashortdescriptionS Vt,which topk_doc=argtopKcos(E RT,E doct) (2)
canbeexpressedaseq.(1), t
whereE istheembeddingofthet-thshortde-
doct
Doc
init
={{1,S V1},{2,S V2},...,{T,S VT}} (1) scriptionS Vt.
whereT denotestheframeid,S isgeneratedbya 3.3 DocumentAugmentationModule
Vt
LargeVision-LanguageModelϕ vlm (e.g.,LLaVA- Foreacht′ ∈ topk_doc,weusetheLLaVA-NeXT
NeXT (Liu et al., 2024)) , S Vt = ϕ vlm(P,V t), P model(Liu et al., 2024) with different prompts
is the prompt for requesting a short description AP (augment prompt) to generate a detailed de-
about the image, i.e., describe the picture no scription L . This is formulated as L =
more 15 words. The captioning model used in ϕ (V ,AV Pt′ ). The initial AP is a gV et n′ eral
vlm t′ t′ t′
DrVideo is replaceable and other captioning prompt: If there are factual errors in the ques-
models(e.g.,LaViLa(Zhaoetal.,2023),BLIP-2 tions, provide a precise description of the image;
(Lietal.,2023a))canalsobeusedinourDrVideo. ifnot,proceedansweringthequestion: {Q}. The
Experiments with different caption models are updatedvideodocumentAD isdefinedas:
showninTable6.
(cid:40)(cid:40) (cid:41)
AD=
{t,S Vt,L Vt′} ift=t′
|t=1,...,T (3)
{t,S Vt} otherwise
3.2 Document(Frames)RetrievalModule
GiventhatK issignificantlysmallerthanthetotal
AfterobtainingDoc ,weintroduceadocument number of frames (K = 5 compared to T = 90),
init
retrievalmoduletoidentifythetopquestion-related the additional descriptions do not substantially
frames via calculate the similarity between the increasetheoveralllengthofthedocument.
question about video and the whole document.
4
LLaVA-NeXT④ Multi-Stage Agent Interaction Loop
[Question & Prompt]
Is
Yes
suffice-
ent?
No
UpdatedVideo Document In1-stturn: Add Reason:Theimageshowsa Final Video Document
Key frame content: The image shows a personwashing dishes ina
personwashingdishesinakitchensink.……of kitchensink.……ofplacinga
Update Add p Rl ea ac sin og n:a Td his eh ii mnt ao gt ehe shs oin wk sa and peth rse op nr wes ae sn hc ine
g
d pi rs eh senin ct eo the sink and the [UpdatedVideo Document]
dishesinakitchensink.……ofplacingadish [Question & Prompt]
intothesinkandthepresence
In2-rdturn:
Keyframecontent:Theimageshowsa
Memory
Frame10:Theimageshowsapersonwashing
dishesinakitchensink.…
……
Frame21:Theimageshowsapersonwashing
dishesinakitchen…sin…k.……of Frame10 Frame21 Frame43
Frame43:Theimageshowsapersonwashing LLaVA-NeXT
dishesinakitchensink.……ofplacinga
DocumentAugmentationModule
⑤ Answering Module
Taking into "optionA": "C is cook.",
a ac cc tio ou nn st pea rl fl ormt eh de " lao up nt dio rn yB ."" ,: "C is doing Prediction:optionD
b dy edc u, cw eha at boca un ty to hu e " t "ho oep p t tki io oitn nchC De" ": n : " . ""C C, i is s c cl le ea an ni in ng g
primary objective dishes.",
and focus within "optionE": "C is cleaning
thevideocontent?" the bathroom." Confidence:3
Final Video Document Question Options
Figure4: Illustrationofthemulti-stageagentinteractionloopandansweringmodule. Therearetwoagentsin
themulti-stageagentinteractionloop: aplanningagenttoplanthenextstepandaninteractionagenttodynamically
findmissinginformationandinteractwiththedocumentaugmentationmodule.
3.4 Multi-StageAgentInteractionLoop videodocumentAD isinadequate. Theagentthen
i
updatesH withitsanalysisandpassestheupdated
i
Apartfromthesekeyframesretrievedbythedoc- H totheinteractionagent.
i
umentaugmentationmodule,someadditionalkey InteractionAgent. Giventhecurrentupdated
framesmayneedtobeidentifiedthroughcontex- video document AD and the updated H , the in-
i i
tual inference. Therefore, we propose a multi- teractionagentfindsN frameswithmissingdetails
stageagentinteractionlooptodynamicallyfindthe crucialforansweringQ:
potential missing key frames and augment these
frameswithdifferenttypesinformationforfinalan- ∀n∈N,n∈T,n∈/ topk_doc,andN <K (4)
swer. Oursystememploystwodistinctagents,i.e.,
planningagentandinteractionagent,eachtailored
Expectfromthekeyframenumber,theinterac-
for specific tasks. Both agents utilize the same
tionagentalsoidentifytheaugmentedinformation
LLM,i.e.,GPT-3.5,butaregivendifferentprompts
type for each n ∈ N. To achieve it, we define a
andreasonvaryingtypesofinformation.
task-specificprompt,i.e.,Yourtaskistodetermine
PlanningAgent. GiventhequestionQandthe whichframeneedswhichtypeofinformationand
updatedvideodocumentAD (whereidenotesthe can answer this question accurately, reasonably,
i
iteration step), along with the analysis history H andwithoutcontradiction. Thetwotypesofinfor-
from all previous steps (initialized as empty {}), mation are as follows: A: Given an image, get a
the planning agent first determines whether the detailed description of the image (image caption,
updatedvideodocumentissufficienttogeneratea justlikewhatisshowninthisimage?) B:Givenan
confident answer. If the information is sufficient, image,getaresponsetotheabovequestion(visual
theprocessmovestotheansweringmoduletoget question answering) to the interaction agent. Af-
thefinalanswer. Iftheinformationisinsufficient, ter obtaining N frames along with their required
theagentprovidesananalysisofwhytheupdated informationtype,theagentinteractswiththeDA
5Algorithm1DrVideo 4.1 DatasetsandMetrics
Require: V,Q
In our experiments, we evaluate our DrVideo us-
1:
▶Video-DocumentconversionModule
ing two well-established datasets and a self-built
2: Doc init ← getDoc(V,ϕ vlm,P) dataset, emphasizing its zero-shot long video un-
3: E doc ← getEmbedding(Doc init,ϕ emb) derstandingcapabilities.
4:
▶DocumentRetrievalModule
EgoSchema. EgoSchema dataset(Mangalam
5: init RT asQ
etal.,2023)consistsof5000multiple-choiceques-
6: E RT ← getEmbedding(RT,ϕ emb) tions sourced from 5000 three-minute egocentric
7: topk_doc ← Retrieval(E RT,E doc,K)
videos. Thisdatasetonlyhasasubsetof500ques-
8:
▶DocumentAugmentationModule
tions with publicly accessible labels. We need to
9: init AP
uploadthepredictionfiletotheleaderboardtover-
10: AD 0 ← Augment(topk_doc,Doc init,AP) ifytheperformanceofourmethod.
11: H ← addToMemory(topk_doc)
MovieChat-1K. MovieChat-1K (Song et al.,
12:
▶Multi-StageAgentInteraction
2023)isalongervideounderstandingbenchmark,
13: init i
which contains 1000 videos from movie and TV
14: fori ≤ I do
shows. These videos typically range between
15: PlanningAgent:
10000 to 12000 frames, with each video lasting
16: S,R = checkSufficient(AD i,H i,Q)
approximately 10 minutes. This dataset has two
17: ifS == 1then
mode: global mode and breakpoint mode. Sim-
18: break
ilar to the EgoSchema dataset, the test set of
19: else
MovieChat-1Kisnotopen. Weevaluatetheperfor-
20: H ← addToMemory(H,R)
manceofourDrVideototheleaderboard.
21: InteractionAgent:
LLama-Vid-QA.Wecreateahour-levelvideo
22: M ← FindMissInfo(AD i,H,Q)
benchmark from MovieNet dataset(Huang et al.,
23: N,AP ← M
2020) and LLama-Vid dataset(Li et al., 2023c),
24: H ← addToMemory(H,N)
namelyLLama-Vid-QA.Therawvideosarefrom
25: i ← i+1
MovieNet dataset, which includes 1100 videos,
26: AD i ← Augment(N,AD i,AP)
eachlastingbetween1-2hours. Theannotationre-
27: endif
sultsarefromLLama-Viddataset(Lietal.,2023c),
28: endfor
whichemploysGPT-4togeneratequestionandan-
29:
▶AnsweringModule
sweraccordingtothescriptandsubtitleofmovie.
30: P ← GetAnswer(AD i)
TheLLama-Vid-QAbenchmarkincludes400ques-
31: return P
tions and 23 videos. Note that our evaluation fo-
moduletoenrichtheinformationoftheseframes cusesonvisionmodality.
toupdatecurrentvideodocumentfornextstep. The accuracy is our evaluation metric in both
datasets. SinceEgoSchemaandMovieChat-1Kare
3.5 AnsweringModule
evaluatedthroughtheleaderboard,wewillnotdis-
GiventhefinalvideodocumentAD i,iisthefinal cussitindetailhere. TheLLama-Vid-QAdataset
stepid,weemployanotheragenttoprovideapre- focusesonopen-endedquestions,wefollowVideo-
dictionusingachain-of-thought(CoT)(Weietal., ChatGPT(MuhammadMaazandKhan,2023)to
2022)approach. Theansweringmoduleoutputsthe comparewiththegeneratedresultsandtheground
corresponding answer, the confidence score, and truth along with a confidence score. We select
thereasoningbehindtheanswer. Besidesimprov- GPT-3.5 as the evaluation assistant and use the
ingpredictionaccuracy,thisCoTapproachallows sameprompt(MuhammadMaazandKhan,2023)
ustotraceinferencesteps,ensuringtransparency toconductafaircomparison.
andexplainabilityinthedecision-makingprocess.
4.2 ImplementationDetails
4 Experiments
ForEgoSchema(Mangalametal.,2023)dataset,we
Inthissection,wefirstintroducethedatasetsand chooseLaViLa(Zhaoetal.,2023)asthecaption-
implementation details, and then we present the ingmodeltoconvertvideosintodocuments. Note
resultsandablationsofDrVideo. thatthetrainingdateofourLaViLamodeldoesnot
6Method LLM SubsetFullset Global Breakpoint
Method
Acc.ScoreAcc. Score
Basedonopen-sourceLLMs
SeViLA(Yuetal.,2023) FlanT5-3B 25.7 22.7 UsingGPT-3.5forLLM-assistedevaluation
MovieChat(Songetal.,2023) LLama 53.5 - VideoChat(Lietal.,2023b) 60.2 3.08 46.3 2.32
MovieChat+(Songetal.,2023) LLama 56.4 -
VideoLLaMA(Zhangetal.,2023b)56.3 2.72 45.8 2.11
End-to-EndModels Video-ChatGPT(Maazetal.,2023) 58.7 2.89 47.8 2.43
InternVideo(Wangetal.,2022a) - 32.1 - MovieChat(Songetal.,2023) 63.7 3.15 48.1 2.46
LongViViT(Papalampidietal.,2023) - 56.8 33.3
UsingGemini-ProforLLM-assistedevaluation
MC-ViT-L(Balaževic´etal.,2024) - 62.6 44.4
MovieChat(Songetal.,2023) 55.1 2.78 38.5 1.87
BasedonproprietaryLLMs
DrVideo(ours) 93.1 4.41 56.4 2.75
MoReVQA(Minetal.,2024) PaLM-2 - 51.7
LLoVi(Zhangetal.,2023a) GPT-3.5 57.6 50.3 Table2: QuantitativeevaluationonMovieChat-1K
IG-VLM(Kimetal.,2024) GPT-4V 59.8 - testsetwithGemini-Pro(Teametal.,2023)andGPT-
VideoAgent(Wangetal.,2024) GPT-4 60.2 54.1 3.5
VideoAgent(Fanetal.,2024) GPT-4 62.8 -
Method Acc. Score
DrVideo(ours) GPT-4 66.4 61.0
LLaMA-VID(Lietal.,2023b) 20.42 2.57
Table1: ResultsonEgoSchemacomparedtoexisting MovieChat(Songetal.,2023) 14.58 1.67
sate-of-the-artmethods.
DrVideo(ours) 50.58 3.09
includetheseEgoSchemavideos,whichissameas
LLoVi(Zhangetal.,2023a). ForMovieChat-1K Table3: QuantitativeevaluationonLLama-Vid-QA
withGPT-3.5.
andLLama-Vid-QAdatasets,LLaVA-NeXT(Liu
etal.,2024)asourcaptioningmodelisusedtogen- without losing key information relevant to solv-
eratebriefdescriptionsforeachframe. Wesample ing the question is not fully explored and our .
all videos at 2 Fps and also select LLaVA-NeXT Besides, compared with the end-to-end methods
(Liu et al., 2024) to enhance key frames in both (Wangetal.,2022a;Papalampidietal.,2023;Bal-
datasetsviadifferentprompts. Inthecontrastex- aževic´ et al., 2024), our DrVideo gains at least
periment,GPT-4,i.e.,gpt-4-1106-previewisused 16.6%improvementonEgoSchemafullset. Thisin-
as the agent to evaluate the performance of our dicatesthatthevideo-to-documentmethodmayhas
DrVideo. In the ablation study, we use GPT-3.5, more potential than the methods that are directly
i.e., gpt-3.5-turbo-1106 to demonstrate the effec- pre-trainedonalarge-scalemultimodalcorpusat
tivenessofourDrVideoduetotheAPIcost. present.
4.3 MainResults Comparative results on MovieChat-1K. In Ta-
ble 2, we show the comparative results on the
Comparative results on EgoSchema. Table 1
MoiveChat-1KdatasetofourDrVideoandthecur-
containsthecomparativeofourDrVideoandexist-
rent state-of-the-art methods (Song et al., 2023;
ingstate-of-the-artmethodsonEgoSchemadataset
Li et al., 2023b; Zhang et al., 2023c; Muham-
(Mangalam et al., 2023). From the table we can
madMaazandKhan,2023). Fromthistable, we
see that our DrVideo outperforms all the com-
canfindthattheMovieChatgains8.6%and9.6%
paredmethodsbyamargin. DrVideois3.6%ac-
declineonbothglobalmodeandbreakpointmode
curacy high than VideoAgent (Fan et al., 2024)
whenusingGemini-ProastheLLM-assistedeval-
onEgoSchemasubsetwhenbothmethodsusethe
uation. Despitethis,ourDrVideooutperformsall
same proprietary LLMs, i.e., GPT-4 to find miss-
the compared methods at least 29.4% accuracy
inginformationandpredictthefinalanswer. This
onglobalmodeand8.3%accuracyonbreakpoint
indicatesthatourdocument(frames)retrievalmod-
modeunderunfaircomparison. Thisshowstheour
ule and multi-stage interactive loop can find the
DrVideo can handle longer video effectively and
question-relatedkeyframesmoreaccurately. Com-
reflectsthegeneralizationofourDrVideo.
paredtoLLoVi(Zhangetal.,2023a),ourDrVideo
is5%accuracyhighonEgoSchemasubsetwhen ComparativeresultsonLLama-Vid-QA. Table
bothmethodsusethesameproprietaryLLMs,i.e., 3 presents the results of our DrVideo and other
GPT-3.5. Itconfirmsourhypothesisthatthechal- state-of-the-art methods (Li et al., 2023c; Song
lengeofconvertingavideointoalongdocument etal.,2023)onLLama-Vid-QAbenchmark. From
7RM MSAIL CoT Acc.(%) VQA Caption Acc.(%) Top-K Acc.(%)
✓ ✓ ✓ 62.6 ✓ ✓ 62.6 5 62.6
✓ ✓ ✗ 62.2 × ✓ 60.4 10 61.4
✓ × 61.8 20 60.6
✓ ✗ ✓ 60.6
✗ ✓ ✓ 59.4 (a)Performancewithdifferent (b)Performancewith
✗ ✗ ✓ 57.4 typeinformation differentTop-K
Table5: Resultsondifferentsettingsandcomparisons.
Table 4: Results on different combinations of re-
trievalmodule(RM),multi-stageagentinteractionloop
CaptioningModel Type Acc.(%)
(MSAIL),andCoT.
LaViLa(Zhaoetal.,2023) Clip-based 62.6
thistable,wecanseethatourDrVideoachievesthe
LLaVA-NeXT(Liuetal.,2024) Frame-based 61.2
bestperformanceandoutperformsothermethods
BLIP-2(Lietal.,2023a) Frame-based 59.6
byaabsolutemargin. Thisisastrongevidencethat
ourDrVideocanhandlehour-levelvideos. Table6: Performanceofdifferentcaptioningmodels.
interactionloop. Anintuitiveobservationisthaten-
4.4 AblationStudies
hancingmoreframesleadstobettermodelperfor-
EffectofdifferentcomponentsofDrVideo. We mance. Toverifythis,weconductexperimentswith
introducearetrievalmoduleandamulti-stageagent differentnumberinitialTopK keyframes. There-
interactionlooptofindpotentialmissinginforma- sultsareshowninTable5bandwecanfindthatthe
tion. Afterobtainingtheseaugmentedinformation, performanceofourDrVideobecomeslowerasthe
we employ another agent to predict answer via a improvementoftheK frames. Thereasonmaybe
CoT approach. To explore the effectiveness of it, thatenhancingmoreinitialTopK keyframeswill
we conduct experiments with different combina- introducealotofuselessandharmfulinformation.
tionsofretrievalmodule,multi-stageagentinterac-
tionloop,andCoT.TheresultsarepresentedinTa- Effectofdifferentcaptioningmodels. Toshow
ble4andwecanfindthat: (i)Withoutretrievemod- theeffectivenessofdifferentcaptioningmodels,we
uleandagentloop,theperformanceofourDrVideo conductexperimentswiththreemodels(i.e.,LaV-
issimilartoLLoVi(Zhangetal.,2023a),thisshows iLa(Zhao et al., 2023), LLaVA-NeXT(Liu et al.,
theimprovementofourmethodmainlycomesfrom 2024), and BLIP-2(Li et al., 2023a)). In Table 6,
the two modules (i.e., retrieve module and agent LaViLa get the best performance, while BLIP-2
loop) we presented. (ii) As retrieve module and yieldstheworstperformance. Thisshowsthatthe
agentloopareadded,theperformanceofDrVideo captioningmodelinfluencethefinalperformance
improvedprogressively(57.4%→60.6%→62.6%). ofourDrVideoandtheLaViLaismoresuitedfor
It means that our DrVideo can find key frames theEgoSchemadataset.
throughcontextualinference,notjustbycalculat-
ingsemanticsimilarity.
5 Conclusion
Effect of different types of augment informa-
tion. To better enhance the required information
foreachpotentialkeyframe,wedefinetwotypes WehaveproposedDrVideo,adocumentretrieval-
of augment information, namely VQA (question- basedsystemforlongvideounderstanding. Differ-
relatedinformation)andcaption(generaldetailed entfrompreviousmethods, DrVideoproposesto
information). Table 5a presents the performance adaptlong-videounderstandingtolong-document
ofDrVideowithdifferenttypeinformation. When understanding and searches for missing informa-
onlyonetypeofinformation(VQAorcaption)is tionviadocumentretrievalandmulti-stageagent
used for enhancement, the model’s performance loop. Extensivecomparativeexperimentsandabla-
decline. Thisindicatesthatthenecessityofadap- tionexperimentsonthethreechallengingdatasets,
tivelyenhancingdifferenttypesofinformationfor i.e., EgoSchema (3 mins), MovieChat-1K (10
eachpotentialkeyframe. mins), and LLama-Vid-QA (> 60 mins) demon-
EffectofdifferentinitialtopK keyframes. In stratethattheeffectivenessofDrVideo. Wehope
thispaper,wesetK = 5astheinitialkeyframesto our work as a strong baseline that promoted the
findadditionalkeyframesviaamulti-stageagent developmentofthelongvideounderstandingfield.
86 Limitation language modeling with pathways. arXiv preprint
arXiv:2204.02311.
Althoughourmethodachievesimpressiveresults
JiwanChungandYoungjaeYu.2023. Longstoryshort:
on different long video benchmarks, we identify
asummarize-then-searchmethodforlongvideoques-
somelimitationsintheDrVideoSystem:
tionanswering. InBMVC.
(1) DrVideo heavily relies on the performance
DannyDriess,FeiXia,MehdiSMSajjadi,CoreyLynch,
ofeachcomponent,e.g.,thecaptioningmodeland
AakankshaChowdhery,BrianIchter,AyzaanWahid,
LLMagents.
Jonathan Tompson, Quan Vuong, Tianhe Yu, et al.
(2) The longest video DrVideo can handle de- 2023. Palm-e: Anembodiedmultimodallanguage
pendsonthemaximumtokenlengthoftheLLM, model. arXivpreprintarXiv:2303.03378.
which may be a potential bottleneck for much
Yue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi
longer videos than existing benchmarks, e.g., 10
Li, Zhi Gao, and Qing Li. 2024. Videoagent: A
hours. memory-augmentedmultimodalagentforvideoun-
(3) Due to the use of proprietary models, derstanding. arXivpreprintarXiv:2403.11481.
DrVideo is not fully transparent and cannot be
DifeiGao,LeiJi,LuoweiZhou,KevinQinghongLin,
trained end-to-end. We hope to explore these as- JoyaChen,ZihanFan,andMikeZhengShou.2023.
pectsinthefuture. Assistgpt: Ageneralmulti-modalassistantthatcan
plan, execute, inspect, and learn. arXiv preprint
arXiv:2306.08640.
References
AlbertGu,KaranGoel,andChristopherRé.2021. Effi-
cientlymodelinglongsequenceswithstructuredstate
Ivana Balaževic´, Yuge Shi, Pinelopi Papalampidi,
spaces. arXivpreprintarXiv:2111.00396.
RahmaChaabouni, SkandaKoppula, andOlivierJ
Hénaff. 2024. Memory consolidation enables
Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng
long-context video understanding. arXiv preprint
Xu,WenmengYu,JunhuiJi,YanWang,ZihanWang,
arXiv:2402.05861.
YuxiaoDong,MingDing,etal.2023. Cogagent: A
visuallanguagemodelforguiagents. arXivpreprint
AanishaBhattacharya,YamanKSingla,BalajiKrish-
arXiv:2312.08914.
namurthy, Rajiv Ratn Shah, and Changyou Chen.
2023. A video is worth 4096 tokens: Verbalize
QingqiuHuang,YuXiong,AnyiRao,JiazeWang,and
storyvideostounderstandtheminzeroshot. arXiv
DahuaLin.2020. Movienet: Aholisticdatasetfor
preprintarXiv:2305.09758.
movie understanding. In Computer Vision–ECCV
2020: 16thEuropeanConference,Glasgow,UK,Au-
AnthonyBrohan,NoahBrown,JusticeCarbajal,Yevgen
gust 23–28, 2020, Proceedings, Part IV 16, pages
Chebotar,XiChen,KrzysztofChoromanski,Tianli
709–727.Springer.
Ding,DannyDriess,AvinavaDubey,ChelseaFinn,
et al. 2023. Rt-2: Vision-language-action models NoureldienHussein,EfstratiosGavves,andArnoldWM
transfer web knowledge to robotic control. arXiv Smeulders.2019. Videograph: Recognizingminutes-
preprintarXiv:2307.15818. long human activities in videos. arXiv preprint
arXiv:1905.05143.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind Md Mohaiminul Islam and Gedas Bertasius. 2022.
Neelakantan,PranavShyam,GirishSastry,Amanda Longmovieclipclassificationwithstate-spacevideo
Askell,etal.2020. Languagemodelsarefew-shot models. InEuropeanConferenceonComputerVi-
learners. Advancesinneuralinformationprocessing sion,pages87–104.Springer.
systems,33:1877–1901.
WonkyunKim,ChanginChoi,WonseokLee,andWon-
JunChen,DeyaoZhu,KilichbekHaydarov,XiangLi, jong Rhee. 2024. An image grid can be worth a
andMohamedElhoseiny.2023. Videochatcaptioner: video: Zero-shotvideoquestionansweringusinga
Towards the enriched spatiotemporal descriptions. vlm. arXivpreprintarXiv:2403.18406.
arXivpreprintarXiv:2304.04227.
Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L
FengCheng,XiziWang,JieLei,DavidCrandall,Mohit Berg,MohitBansal,andJingjingLiu.2021. Lessis
Bansal,andGedasBertasius.2023. Vindlu: Arecipe more: Clipbertforvideo-and-languagelearningvia
foreffectivevideo-and-languagepretraining. InThe sparsesampling. InProceedingsoftheIEEE/CVF
IEEE Conference on Computer Vision and Pattern conferenceoncomputervisionandpatternrecogni-
Recognition(CVPR). tion,pages7331–7341.
AakankshaChowdhery,SharanNarang,JacobDevlin, JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi.
Maarten Bosma, Gaurav Mishra, Adam Roberts, 2023a. BLIP-2: bootstrappinglanguage-imagepre-
Paul Barham, Hyung Won Chung, Charles Sutton, training with frozen image encoders and large lan-
Sebastian Gehrmann, et al. 2022. Palm: Scaling guagemodels. InICML.
9KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wen- LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,
haiWang,PingLuo,YaliWang,LiminWang,and CarrollWainwright,PamelaMishkin,ChongZhang,
YuQiao.2023b. Videochat: Chat-centricvideoun- SandhiniAgarwal,KatarinaSlama,AlexRay,etal.
derstanding. Preprint,arXiv:2305.06355. 2022. Training languagemodelsto followinstruc-
tions with human feedback. Advances in Neural
Yanwei Li, Chengyao Wang, and Jiaya Jia. 2023c. InformationProcessingSystems,35:27730–27744.
Llama-vid: Animageisworth2tokensinlargelan-
guagemodels. arXivpreprintarXiv:2311.17043. PinelopiPapalampidi,SkandaKoppula,ShreyaPathak,
JustinChiu,JoeHeyward,VioricaPatraucean,Jiajun
KevinLin,FaisalAhmed,LinjieLi,Chung-ChingLin, Shen,AntoineMiech,AndrewZisserman,andAida
EhsanAzarnasab,ZhengyuanYang,JianfengWang, Nematzdeh.2023. Asimplerecipeforcontrastively
Lin Liang, Zicheng Liu, Yumao Lu, Ce Liu, and pre-trainingvideo-firstencodersbeyond16frames.
Lijuan Wang. 2023a. Mm-vid: Advancing video arXivpreprintarXiv:2312.07395.
understanding with gpt-4v(ision). arXiv preprint
arXiv:2310.19773. Teven Le Scao, Angela Fan, Christopher Akiki, El-
lie Pavlick, Suzana Ilic´, Daniel Hesslow, Roman
KevinLin,FaisalAhmed,LinjieLi,Chung-ChingLin,
Castagné,AlexandraSashaLuccioni,FrançoisYvon,
EhsanAzarnasab,ZhengyuanYang,JianfengWang,
Matthias Gallé, et al. 2022. Bloom: A 176b-
Lin Liang, Zicheng Liu, Yumao Lu, et al. 2023b.
parameteropen-accessmultilinguallanguagemodel.
Mm-vid: Advancingvideounderstandingwithgpt- arXivpreprintarXiv:2211.05100.
4v(ision). arXivpreprintarXiv:2310.19773.
EnxinSong,WenhaoChai,GuanhongWang,Yucheng
HaotianLiu,ChunyuanLi,YuhengLi,BoLi,Yuanhan
Zhang, Haoyang Zhou, Feiyang Wu, Xun Guo,
Zhang,ShengShen,andYongJaeLee.2024. Llava-
Tian Ye, Yan Lu, Jenq-Neng Hwang, et al. 2023.
next: Improvedreasoning,ocr,andworldknowledge.
Moviechat: From dense token to sparse mem-
ory for long video understanding. arXiv preprint
Ziyu Ma, Shutao Li, Bin Sun, Jianfei Cai, Zuxiang
arXiv:2307.16449.
Long,andFuyanMa.2024. Gerea: Question-aware
promptcaptionsforknowledge-basedvisualquestion
Yuchong Sun, Hongwei Xue, Ruihua Song, Bei Liu,
answering. arXivpreprintarXiv:2402.02503.
HuanYang,andJianlongFu.2022. Long-formvideo-
languagepre-trainingwithmultimodaltemporalcon-
Muhammad Maaz, Hanoona Rasheed, Salman Khan,
trastive learning. Advances in neural information
and Fahad Shahbaz Khan. 2023. Video-chatgpt:
processingsystems,35:38032–38045.
Towards detailed video understanding via large
vision and language models. arXiv preprint
DídacSurís, SachitMenon, andCarlVondrick.2023.
arXiv:2306.05424.
Vipergpt: Visualinferenceviapythonexecutionfor
reasoning. ProceedingsofIEEEInternationalCon-
KarttikeyaMangalam,RaiymbekAkshulakov,andJi-
ferenceonComputerVision(ICCV).
tendraMalik.2023. Egoschema:Adiagnosticbench-
markforverylong-formvideolanguageunderstand-
Gemini Team, Rohan Anil, Sebastian Borgeaud,
ing. arXivpreprintarXiv:2308.09126.
Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,
Radu Soricut, Johan Schalkwyk, Andrew M Dai,
JuhongMin,ShyamalBuch,ArshaNagrani,MinsuCho,
Anja Hauth, et al. 2023. Gemini: a family of
and Cordelia Schmid. 2024. Morevqa: Exploring
highlycapablemultimodalmodels. arXivpreprint
modularreasoningmodelsforvideoquestionanswer-
arXiv:2312.11805.
ing. In Proceedings of the IEEE/CVF Conference
onComputerVisionandPatternRecognition,pages
HugoTouvron,ThibautLavril,GautierIzacard,Xavier
13235–13245.
Martinet,Marie-AnneLachaux,TimothéeLacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro,
SalmanKhanMuhammadMaaz,HanoonaRasheedand
Faisal Azhar, et al. 2023. Llama: Open and effi-
FahadKhan.2023. Video-chatgpt: Towardsdetailed
cient foundation language models. arXiv preprint
video understanding via large vision and language
models. ArXiv2306.05424.
arXiv:2302.13971.
EricNguyen,KaranGoel,AlbertGu,GordonDowns, JueWang,WentaoZhu,PichaoWang,XiangYu,Linda
PreeyShah,TriDao,StephenBaccus,andChristo- Liu,MohamedOmar,andRaffayHamid.2023a. Se-
pherRé.2022. S4nd:Modelingimagesandvideosas lective structured state-spaces for long-form video
multidimensionalsignalswithstatespaces. Advances understanding. InProceedingsoftheIEEE/CVFCon-
inneuralinformationprocessingsystems,35:2846– ferenceonComputerVisionandPatternRecognition,
2861. pages6387–6397.
OpenAI. 2022. Introducing chatgpt. JunkeWang,DongdongChen,ChongLuo,XiyangDai,
https://openai.com/blog/chatgpt. Lu Yuan, Zuxuan Wu, and Yu-Gang Jiang. 2023b.
Chatvideo: A tracklet-centric multimodal and ver-
OpenAI.2024. Newembeddingmodelsandapiupdates. satile video understanding system. arXiv preprint
Accessed: 2024-06-08. arXiv:2304.14407.
10Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit
Yeung-Levy. 2024. Videoagent: Long-form video Bansal.2024. Self-chainedimage-languagemodel
understanding with large language model as agent. forvideolocalizationandquestionanswering. Ad-
arXivpreprintarXiv:2403.10517. vances in Neural Information Processing Systems,
36.
YangWang,GedasBertasius,Tae-HyunOh,Abhinav
Gupta,MinhHoai,andLorenzoTorresani.2021. Su- Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof
pervoxelattentiongraphsforlong-rangevideomod- Choromanski,AdrianWong,StefanWelker,Federico
eling. InProceedingsoftheIEEE/CVFWinterCon- Tombari,AveekPurohit,MichaelRyoo,VikasSind-
ferenceonApplicationsofComputerVision,pages hwani, Johnny Lee, Vincent Vanhoucke, and Pete
155–166. Florence.2022. Socraticmodels: Composingzero-
shotmultimodalreasoningwithlanguage. arXiv.
YiWang,KunchangLi,YizhuoLi,YinanHe,Bingkun
Huang,ZhiyuZhao,HongjieZhang,JilanXu,YiLiu, Ce Zhang, Taixi Lu, Md Mohaiminul Islam, Ziyang
ZunWang,etal.2022a. Internvideo: Generalvideo Wang,ShoubinYu,MohitBansal,andGedasBerta-
foundationmodelsviagenerativeanddiscriminative sius. 2023a. A simple llm framework for long-
learning. arXivpreprintarXiv:2212.03191. range video question-answering. arXiv preprint
arXiv:2312.17235.
ZhenhailongWang,ManlingLi,RuochenXu,Luowei
Zhou, Jie Lei, Xudong Lin, Shuohang Wang, Ziyi HangZhang,XinLi,andLidongBing.2023b. Video-
Yang, ChenguangZhu, DerekHoiem, etal.2022b. llama: An instruction-tuned audio-visual language
Languagemodelswithimagedescriptorsarestrong model for video understanding. arXiv preprint
few-shotvideo-languagelearners. AdvancesinNeu- arXiv:2306.02858.
ralInformationProcessingSystems,35:8483–8497.
HangZhang,XinLi,andLidongBing.2023c. Video-
JasonWei,XuezhiWang,DaleSchuurmans,Maarten llama: An instruction-tuned audio-visual language
Bosma,FeiXia,EdChi,QuocVLe,DennyZhou, model for video understanding. arXiv preprint
etal.2022. Chain-of-thoughtpromptingelicitsrea- arXiv:2306.02858.
soninginlargelanguagemodels. AdvancesinNeural
InformationProcessingSystems,35:24824–24837. Yue Zhao, Ishan Misra, Philipp Krähenbühl, and Ro-
hit Girdhar. 2023. Learning video representations
Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, fromlargelanguagemodels. InProceedingsofthe
HaoqiFan,BoXiong,JitendraMalik,andChristoph IEEE/CVFConferenceonComputerVisionandPat-
Feichtenhofer.2022. Memvit: Memory-augmented ternRecognition,pages6586–6597.
multiscalevisiontransformerforefficientlong-term
videorecognition. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecog-
nition,pages13587–13597.
AntoineYang,AntoineMiech,JosefSivic,IvanLaptev,
and Cordelia Schmid. 2021. Just ask: Learning to
answer questionsfrom millions ofnarrated videos.
InProceedingsoftheIEEE/CVFinternationalcon-
ferenceoncomputervision,pages1686–1697.
AntoineYang,AntoineMiech,JosefSivic,IvanLaptev,
andCordeliaSchmid.2022. Zero-shotvideoques-
tionansweringviafrozenbidirectionallanguagemod-
els. AdvancesinNeuralInformationProcessingSys-
tems,35:124–141.
XitongYang,Fu-JenChu,MattFeiszli,RaghavGoyal,
LorenzoTorresani, andDuTran.2023. Relational
space-timequeryinlong-formvideos. InProceed-
ingsoftheIEEE/CVFConferenceonComputerVi-
sionandPatternRecognition,pages6398–6408.
Zongxin Yang, Guikun Chen, Xiaodi Li, Wenguan
Wang, and Yi Yang. 2024. Doraemongpt: Toward
understandingdynamicsceneswithlargelanguage
models. arXivpreprintarXiv:2401.08392.
Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mo-
hit Bansal. 2023. Self-chained image-language
model for video localization and question answer-
ing. NeurIPS.
11