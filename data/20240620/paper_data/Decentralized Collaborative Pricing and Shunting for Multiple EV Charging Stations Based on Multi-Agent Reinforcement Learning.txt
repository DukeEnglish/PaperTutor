Decentralized Collaborative Pricing and Shunting
for Multiple EV Charging Stations Based on
Multi-Agent Reinforcement Learning
1st Tianhao Bu 2nd Hang Li⋆ 3rd Guojie Li
Glory Engineering & Tech Co., LTD School of Electronic Information School of Electronic Information
Shanghai, China and Electrical Engineering and Electrical Engineering
tianhao.bu19@alumni.imperial.ac.uk Shanghai Jiao Tong University Shanghai Jiao Tong University
Shanghai,China Shanghai,China
lihang9596@sjtu.edu.cn liguojie@sjtu.edu.cn
⋆Corresponding author
Abstract—The extraordinary electric vehicle (EV) populariza- optimizechargingcosts [4],[5];In [5],RLalgorithmisused
tion in the recent years has facilitated research studies in alle- tolearndevicebasedMDPmodels,withtheimplementationof
viating EV energy charging demand.Previous studies primarily
a Q-table to estimate the Q function. At the other hand, [4]
focused on the optimizations over charging stations’ (CS) profit
uses recurrent deep deterministic policy gradient (RDDPG) and EV users’ cost savings through charge/discharge scheduling
events. In this work, the random behaviours of EVs are consid- algorithm, which proven to have great scalability in solving
ered, with EV users’ preferences over multi-CS characteristics large-scaled MDP problems.
modelled to imitate the potential CS selection disequilibrium. While the above methods focus on single agent learning
A price scheduling strategy under decentralized collaborative
problems, the real-world environment involves multi-EVs
framework is proposed to achieve EV shunting in a multi-
charging under single or multi-CSs. The scenarios where
CS environment, while minimizing the charging cost through
multi-agent reinforcement learning. The proposed problem is multi-CS are participated in, the inherent competitive and
formulated as a Markov Decision Process (MDP) with uncertain cooperative natures among CSs should also be considered.
transition probability. To adapt with the corresponding increased complexity, the
IndexTerms—electricvehiclepricescheduling,electricvehicle emergence of multi-agent reinforcement learning (MARL)
shunting,multi-agentreinforcementlearning,centralisedtraining
based approaches opens up a new realm worth investigation
decentralised execution, markov decision process
[6], [7], [8], [9]. MARL based methods can be generalized
into two categories 1) centralized execution methods [8], [9],
I. INTRODUCTION
2) decentralized execution methods [6], [7]. For centralized
AccordingtothereportfromtheHouseofCommonLibrary training and decentralized execution (CTDE), [6] proposes
[1], UK has proposed and adopted two sets of energy con- anactor-attention-criticapproachtoboostrewardoptimization
sumption strategies to reach the net zero policy by 2050. The performance through an attention critic allowing dynamic
vehiclecarbonemissionshaveoccupiedthelargestproportion agents selections at each training time point; and in [7], a
of UK emission of 23‰ in 2022. Within the same year, decentralized collaborative framework is proposed to account
UK achieved the second largest electric vehicle (EV) sales situations where CS has various types of charging piles, i.e.
in Europe [2]. The convincing results further propelled the normal charging (NC) and fast charging (FC).
policies over petrol vehicle prohibition and EV mandate, in Althoughtheaforementionedmethodsaresufficientinprovid-
turn facilitated the EV popularization thus the ever-growing ing charging cost optimization improvements, there weren’t
charging demand. Correspondingly, developing solutions to many works on modelling and, or guiding EV users’ char-
alleviate the demand crisis and reduce charging peak load has acteristics. One of the branches conducts research within
gained noticeable attentions in the recent years. the direction of EV charging navigation [10], [11], where
Thenatureoftheaforementionedproblemsrequiressequential the primary objective is to suggest users with the shortest
decision-making approaches,thus implying theminto Markov routes between EVs and CSs, while achieving simultaneous
Decision Process (MDP) [3] is favoured. Reinforcement minimization over EV users’ traveling times and charging
learning (RL) as one of the main stream machine learning costs. In [10], a deterministic shortest charging route model
paradigms has been a hot topic within this domain, due (DSCRM)isutilizedtoextractstatefeaturesfromthestochas-
to its capability of learning optimal policies in complicated tic traffic, and RL is applied to obtain an optimal strategy for
environmentsinamodel-freemanner.RLisfrequentlyapplied the navigation problem with a practical case study in Xi’an’s
in managing the EV charging, discharging schedulings to real-sized zone. Whereas in [11], the features are extracted
4202
nuJ
71
]AM.sc[
1v69411.6042:viXraFig. 1. Decentralized collaborative framework for a single CS.
throughgraphconvolutionaltransformationandlearntthrough posed, by utilizing the shared information between the
graph reinforcement learning (GRL), the optimization goal stations, individual CS is able to influence EV users’
is achieved with the aid of a modified rainbow algorithm. selections through live price competition; the reduced
Other directions consider modelling the EV users’ prefer- pricefortheinfluencedEVisreclaimedintheremaining
ences [12], [13]. [12] implements a user preference and price timestepsbeforethedeparturetoensureCSprofit,while
basedchargingalgorithm,experimentedintheUCLAparking achieving EV shunting.
lots; and [13] proposed a modified probability model. 4) Through VDN and Q-mix [15] performance evalua-
In this work, the decentralized collaborative framework is tions,theresultsshowthatourpriceschedulingstrategy
extended to include multi-CSs, the CS characteristics are is sufficient in producing substantial average reward
representedin anasymmetricmanner,i.e. diversifiedcharging improvements and faster episode convergence over the
prices at different time periods, varied charging pile sizes. baseline.
To study the EV users’ behaviours, a probabilistic EV user The rest of the paper is structured as follows. section II
preferencemodelisbuiltaccountingrealtimechargingprices, presents the concept of single CS based decentralized collab-
EV to CS distances and CS sizes (pile numbers) . Hence, a orative framework. section III proposes the multi-CS based
normalizedlineardistancemodelisalsodevelopedalongwith collaborative framework, followed with the modified proba-
therandomarrivalsoftheEVs.Finally,sincetheprobabilistic bilistic EV user preference model definition, and the MDP
EV user preferences can potentially result in an asymmetric problem formulation. section IV explains the proposed price
selection over a certain CS, a price scheduling strategy is scheduling strategy procedure and the training phase. In
proposed to allow CSs to compete over prices and attract section V, rigorous evaluations were carried out to validate
EV users in real time through shared CS information, and the effectiveness of the proposed method, together with the
eventually achieve EV shunting, relieving the stress of the corresponding optimisation performances. Finally, section VI
potential space congestion and charging demands over certain concludes our work.
individual CS.
The main contributions of this paper are summarized as II. BACKGROUND
follows: A. Decentralized collaborative framework (single CS)
1) Amulti-CSbaseddecentralizedcollaborativeframework Under a single CS based decentralized collaborative frame-
is proposed, where each pile with an EV attached is work, the CS contains multiple charging piles, including
treated as an agent, which is able to execute local and both NC and FC, as shown in Figure 1. Once EV arrives
independent charging, discharging actions; the charging and attaches itself to an allocated pile, its information, i.e.
problemisformulatedasMDP.AValue-Decomposition current state of charge (SOC), time of arrival t , estimate
a
Network (VDN) [14] MARL algorithm is utilized to time of departure t and the charging demand e will be
d d
perform charging cost optimization. transmittedviathepileandCStothedataoperator;presented
2) A probabilistic EV user preference model is designed, as data signals.Since each pile has the ability to make local
which considers real time prices, EV-CS distances and and independent charging, discharging decisions (as forms
CS sizes to predict EV users selections over various of energy flow exchanges), data signals also embed these
CSs;eachtermisassignedwithatunedweight,together decisions together with the CS real time charging prices. In
withalineardistancemodeldevelopedtonormalizeand the single CS scenario, the data operator’s task is to gather
quantify the distance parameter. these data and send control signals to ensure the accumulated
3) A real time dynamic price scheduling strategy is pro- independent charging decisions will not cause CS capacityFig. 2. Multi(dual)-CS based decentralized collaborative framework
overload , this is implemented through live monitoring of the whereU istheEVuserutilityfunction,inturnisdirectly
i,ti,z
a
totalCSpowerdemandateach timestep,inthesamemanner proportional to the attractiveness Att of CS z on the ith
i,ti,z
a
as the procedure proposed in [4]. In our work, the single EV user . The user utility function can be expressed as
CS based framework is extended to a dual-CS decentralized
collaborative framework, as the result, the data operator also Att i,ti,z
U = a (3)
actsasamediumbetweentheCSstoexchangeinformationof i,ti a,z T
i,z
thechargingprices,realtimechargedEVnumberofeachCS,
allowing the proceeding of the proposed price ”competition” where T defines the closeness between EV i and CS z,
i,z
algorithm;thedetaileddescriptionsarepresentedinsectionIV. represented as
III. ENVIRONMENTFORMULATION T i,z =t c+t i,z (4)
A. Multi(dual)-CS based collaborative framework
with t beingtheaverage EVchargingtimeand t beingthe
c i,z
From the single-CS based framework mentioned in sec-
travelling time from EV i to CS z.
tion II, we propose a multi-CS based collaborative framework
In a practical scenario, we consider four major factors that
by extending the single station into dual stations, as depicted
havesignificantinfluencesonEVusers’decisions:1)Charging
in Figure 2. Under the new framework, CS 0 and CS 1 are
pile number N . By considering EV users’ risk stop loss
z
differentiated by their 1) Charging pile number N , with
z characteristics, the arrived users’ selections would lean to-
z ∈ [0..1]. 2) Real time charging price p . The data op-
t,z wards the CS with higher pile number, due to the likelihood
erator now carries additional responsibility of integrating and
of having more available spaces, and the smaller chances of
exchangingthepilenumberandpriceinformationbetweenthe
discovering fully occupied station at arrival, which results in
CSs.Theproposedframeworkinheritsthesameconceptsasits
longer distance travelled, towards inferior charging locations.
baseline, in which each arrived EV, equivalent as a pile under
2)Arrivaltimechargingpricep .EVusersgenerallywould
ti,z
operation, is treated as an agent, moreover, the optimization a
prefer cheaper prices, hence, the CS that offers cheaper prices
objective is to minimize the accumulated charging cost C ,
acc will gain more attractions from the users. 3) EV i and CS z
through each individual pile’s charging decisions, expressed
travel distance d . Longer travel distance will make CS less
i,z
as
(cid:88)(cid:88) attractivetotheEVusers.4)EVarrivalstateofchargeSOC a.
C = p (1)
acc t,z High SOC will increase CS z’s attractiveness. Furthermore,
a
z t we define a SOC threshold SOC , when SOC is less than
th a
B. Probabilistic EV user preference model SOC , the EV is urgent to get charged, leading to less
th
attraction under large d , and vice versa. This term will be
Inspired by the works from [16] and [13], we propose i,z
discussed in more details in the upcoming session.
a modified probabilistic model to imitate EV users’ selection
Based on the aforementioned factors, the attractiveness
preferencesovertheCSs.Thefollowingparametersbeingused
Att can be formulated as a linear weighing function:
are adopted from [16]. i,ti,z
a
The selection probability Pr of the ith arriving EV
i,z,ti
selecting CS z, at its arrival timea ti a can be formally defined Att i,ti a,z =w 0·N z−w 1·p z,ti a−w 2·d i,z+w 3·(SOC a−SOC th)
as follows: (5)
Pr = U i,ti a,z (2) w j with j ∈[0..3] are the weighing coefficients, the negative
i,z,ti a (cid:80)1 U sign indicates degrading effect on the user attraction.
z=0 i,ti a,z4) Reward function. The optimization goal when the action
causes current state transition to the future ’next’ state, and
it’s directly related to the accumulated charging cost.
In this section, we use MDP to model our decision problem,
Fig. 3. Linear EV-CS distance model with detailed definitions presented in the following subsec-
tions.
1) State: The state at time step t can be defined as s =
i,t,z
C. Distance normalization model (spile,sstation), where spile is the state of pile i in CS z at
i,t,z t,z i,t,z
Toillustratetheconceptofutilizingdistancenormalization, time t, which can be formally defined as
we first propose a linear EV-CS distance model, as shown in spile =(SOC ,Pmax,Pmin,tstay) (9)
i,t,z i,t,z i,t,z i,t,z i,t,z
Figure3.d andd arethecorrespondingdistancesbetween
i,0 i,1
EV i and CS 0, 1, and they are assumed to be the shortest where ts i,t ta ,zy is the remaining staying time of the ith EV in CS
routes by default. Recalling from equation (5), high SOC z at time step t, defined as
a
would result in higher station attraction to EV users, and the tstay =td −tcur (10)
i,t,z i,t,z
SOC is related to the EV-CS distances. But due to the unit
with td being the time of departure, and tcurbeing the
differences (SOC has values ranging from 0 to 1 while the i,t,z
current time step.
actualdistanced issignificantlylargerincomparison),they
i,z
Furthermore, sstation is the state of CS z at time t, and can
areseparatedandassignedwithdifferentweighingcoefficients t,z
be expressed as
w andw ,inturnincreasestheattractionfunctioncomplexity
2 3
and obscures the relationship between them. sstation =(tcur,p ,emetotal) (11)
t,z t,z t,z
Hence, we divide the original d by the total distance from
i,z where emetotal is CS z’s total emergency at time t, can also
EVitobothstations,andobtainthenormalizeddistanced(cid:103)i,z,
in turn be
rt e,z
presented as
expressed as
Ni,t,z
d(cid:103)i,z =
d
d +i,z
d
,z ∈[0..1] (6) emet to ,ztal = (cid:88) eme i,t,z (12)
i,0 i,1 i
Asaresult,theoriginalattractivenessAtt i,ti,z canbemodified with N i,t,z being the number of EVs attached to CS z at time
a t, and eme being the charge emergence of the individual
into A(cid:103)tt i,ti,z, as shown the following formula: i,t,z
a EV i at CS z and time step t, as shown in (13).
A(cid:103)tt i,ti a,z =w 0·N z−w 1·p ti a,z−w 2·(d(cid:103)i,z+SOC th−SOC (a 7)
) eme
= (SOC in ,e t,e zd− tsS taO yCi,t,z)·Ci,z if SOC
i,t,z
< SOC in ,te ,e zd
i,t,z i,t,z
From equation (26), w
3
is eliminated as d(cid:103)i,z, SOC
a
and 0 otherwise
SOC now share a highly overlapped value range. When (13)
th
the arrival SOC exceeds the threshold, the observation indi- SOCneed is the SOC needed for EV i at time t charging at
i,t,z
cates the EV having sufficient energy to travel relative long station z before its departure.
distances, in turn alleviates the attraction degradation caused 2) Action: In this section we define the ith pile’s action in
from d(cid:103)i,z, and vice versa. CS z at time step t as a i,t,z, within a range of [−1,1]. a i,t,z
Additionally, the traveling time t can be calculated using represents the corresponding charging,discharging decision
i,z
d(cid:103)i,z: made by the pile. Thus, the real time charging power P ir ,te ,a zl is
expressed as
t =
d(cid:103)i,z
(8)
i,z v (cid:101)i Preal = (a i,t,z+1) ∗(Pmax−Pmin)+Pmin (14)
i,t,z 2 i,t,z i,t,z i,t,z
where v is the normalized vehicle velocity of EV i.
(cid:101)i where Pmax and Pmin are the corresponding maximum and
i,t,z i,t,z
D. Markov decision process minimum pile charging,discharging powers at CS z at time
t.As a result, we defined the total charging power Ptotal in
As a discrete-time stochastic control process, MDP is suffi- t,z
CS z, time step t as
cientinmodelingourcharging,dischargingdecisionproblems,
(cid:88)
due to the sequential feature of the decision making process. Ptotal = Preal (15)
t,z i,t,z
MDP consists of four components: 1) State. It describes
i
the agent’s (arrived EV, or the pile under operation) current 3) State transition: To model the probability of state tran-
situation, and the future ’next’ state only depends on the sition from s to s under the influence of a , we formulate
t t+1 t
present state ,owing to the MDP’s definition. 2) Action. The the state transition as
charging, discharging decisions made by the NC, FC piles,
s =f(s ,a ,ω ) (16)
under the current state time step. 3) State transition function. t+1 t t t
Thefunctionthatevaluatestheprobabilityinwhichtheaction f(·) is the state transition function, and the term ω is defined
t
under current state causing the future ’next’ state transition. to indicate the uncertainty of the state transition.4) Reward function: The station reward rstation for CS z receives the pile occupation information N and N
t,z tcur,0 tcur,1
at time step t can be formally defined as from each CS, if the pile occupation is imbalanced,N ̸=
tcur,0
rstation =Ptotal·p (17) N tcur,1; the data operator will command the CS with lower
t,z t,z t,z attached EV to reduce the charging price by a sufficient
To ensure the total charging power P tt ,o ztal does not exceed amount to attract EV at step tcur +1, namely p
i,tcur+1,z
,
the maximum power limit P zmax for each CS z, we define a as shown in equation 20.
punishment term rpunish as
t,z 
(cid:40)
Pmax−Ptotal if Pmax < Ptotal
p tcur,0−α tcur if N tcur,0 < N tcur,1
rp t,u znish = 0z t,z othez
rwise
t,z (18) p i,tcur+1,z = pp tcur,1−β tcur oif thN ert wcu ir s, e0 > N tcur,1 (20)
tcur,z
Therefore, the total reward at each time step t is expressed as
The terms α and β are the corresponding price reduc-
1 tcur tcur
r =(cid:88) rc +A·rpunish (19) tions at each CS and the current time tcur.
t t,z t,z Moreover, p is a subset of CS charging price p , it is
z=0 i,t,z t,z
only implied to those attracted EVs at time step t, and will be
Where A is a scaling factor.
charged back at the next step t+1, in turn can be expressed
as
Algorithm 1: EV price scheduling training procedure
(cid:40)
input : state s i,t,z =(sp i,i tl ,e z,ss t,t zation) p = p t−1,0+α t−1 if z =0, p i−N ta −tt 1,0,t−1,0 ̸=0
output: VDN network parameters i,t+1,z p +β if z =1,p ̸=0
t−1,1 t−1 i−Natt ,t−1,1
1 Randomly initialize VDN network parameter θ. t−1,1 (21)
2 Copy VDN network parameter to target network where Natt and Natt are the numbers of price attracted
t−1,0 t−1,1
θ′ =θ. EVs to CS 0 and 1 at time step t−1.
3 for episode m=1:M do
4 for t=t start :t end do B. Training phase
5 for CS z = CS 0 : CS 1 do In our work, we use VDN [14] MARL algorithm,a Q-
6 for pile i = pile 0 : pile n do learning based approach to resolve the proposed multi-agent
7 obtain EV i’s state reinforcement learning problem. The primary task of the
s t,z =(sp i,i tl ,e z,ss t,t zation) network is to learn the total action value function Q tot(τ,a);
8 if p i−1,tcur−1,0 ̸=0, p i−1,tcur−1,1 ̸=0 where τ = (τ 1,0,...,τ n,1) is observation history, with τ i,z
then being the joint action observation history of agent i from
9 p i−1,t−1,0+α, p i−1,t−1,1+β. CS z and a = (a 1,0,...,a n,1) is the action set. Q tot(τ,a)
10 if N tcur,0 < N tcur,1 then can be represented as a summation of each value function
11 p i,tcur+1,z =p tcur,0−α Q i,z(τ i,z,a i,t,z):
12 if N tcur,0 > N tcur,1 then Qtot(τ,a)=(cid:88) Q (τ ,a ;θ ) (22)
i,z i,z i,z i
13 p i,tcur+1,z =p tcur,0−β i
14 select action a i,t,z based on ϵ−greedy where θ is the network parameter.
i
policy
The loss function of the VDN network is defined as
15 calculate reward rs t,t zation =P tt ,o ztal·p t,z
L
=(cid:88)(cid:88)
(ytot−Q (τ,a,s ;θ))2 (23)
16 sample transition θ i,z tot i,z
<s ,a ,r ,s > in B i z
i,t,z i,t,z t,z i,t+1,z 1
17 calculate total reward ytot is the total target action value, which is calculated as
r =(cid:80)1 rc +A·rpunish shown in equation (24).
t z=0 t,z t,z
18 sample mini batch <s i,z,a z,r z,s i+1,z > from B 1 ytot =r z+γ·max a′Q tot(τ′,a′,s′ i,z;θ′) (24)
19 set ytot =r i,z+γ·max a′Q tot(τ′,a′,s′ i,z;θ′) The term γ is the discount factor, τ′, a′ and s′ are the
20 update VDN network by minimizing loss corresponding observation history, action and statei, az fter state
L =(cid:80) (cid:80) (ytot−Q (τ,a,s ;θ))2
θ i z i,z tot i,z s takesactionaandreceiverewardr ;θ′ istheparameter
i,z i,z
of the target network.
IV. PROPOSEDPRICESCHEDULINGSTRATEGY Furthermore, we apply the ϵ−greedy action selection to bal-
ance the exploration-exploitation trade off, the optimal action
A. Price scheduling
that maximizes the action value function is set with a proba-
Inthissection,weproposethenovelpriceschedulingstrat-
bility of 1-ϵ, as shown in equation (25).
egy to achieve EV shunting through multi-CS price ”competi-
tion”ateachsimultaneoustimestep.Attcur,thedataoperator Pr(a =aopt )=1−ϵ,0<ϵ<1 (25)
i,t,z i,t,zFig. 4. EV distributions over price scheduling and baseline methods, under each scenario
Where aopt is the optimal action, and a random ac- B. EV shunting evaluation
i,t,z
tion is chosen with a probability of ϵ, the transition <
Firstly,weevaluatetheeffectivenessofthepricescheduling
s ,a ,r ,s > at time step t is stored in a replay
i,t,z i,t,z t,z i,t+1,z strategy, i.e. it achieves sufficient EV shunting under each
buffer B . With the above information, the following price
1 case study. In this sub section, three different scenarios are
scheduling training procedure is summarised in algorithm 1.
generatedfortheshuntingevaluation,whereeachscenariohas
V. SIMULATIONS varied numbers of EVs requiring charging and charging pile
A. simulation Setup sizes at each station: 1) 6 arriving EVs, CS 0 with 3 FC piles
and 2 NC piles, CS 1 with 4 FC piles and 3 NC piles; 2) 12
In our simulation set up, we adopt the Chinese time-of-use
arrivingEVs,CS0with6FCpilesand4NCpiles,CS1with
tariff for our CS 0 from [7], [4]. Furthermore, we developed
8 FC piles and 6 NC piles; 3) 25 arriving EVs, CS 0 with 14
the CS 1 tariff based on the original version, with higher
FC piles and 6 NC piles, CS 1 with 20 FC piles and 10 NC
prices at each corresponding time period; the tariffs for both
piles. The weight coefficients for the attraction model under
stationsarepresentedinTableI.Thearrivaltimet ,departure
a
each scenario are configured as
time t , arrival state of charge SOC and the normalized
d a
EV-CS 0 distance d(cid:103)i,0 (and d(cid:103)i,1 = 1 - d(cid:103)i,0 ) all follow a
w ,w ,w =(0.25,3.8,0.7) (26)
0 1 2
normal distribution , with a corresponding boundary limit,
as shown in Table II. Moreover, the needed state of charge Eachscenarioissimulatedfor1000times,and16samplesare
SOCneed is set to 0.8, minimum state of charge SOCmin intercepted for representation, as shown in Figure 4. Under
and maximum state of charge SOCmax are set to 0.2, 0.9 scenario1,ourpriceschedulingmethodresultsintwomajority
respectively. The maximum charging power for NC pile is 6 arrived EV size pairs of (CS0,CS1) = (3,3),(4,2), whilst
kWh,with a maximum battery capacity of CNC = 24 kWh, withoutpriceschedulingthecommonsizepairis(5,1).Under
and for FC pile the maximum charging power is 30 kWh, scenario 2, the price scheduling results in common size pairs
with a battery capacity CFC = 180 kWh; the random seed of of(6,6),(7,5)incomparisonwiththebaselineof(10,2);and
50 is selected. finallyforscenario3,theresultshowsadifferenceof(12,13)
For our VDN network model, the layers in the network have vs (1,24). The above results are sufficient in proving our
64 neurons, and the learning rate of the network is set to be priceschedulingstrategyinachievingreasonableEVshunting.
0.001. The batch size is 32, and the buffer size is 2000.Fig. 5. VDN accumulation reward distribution
C. VDN performance evaluation is a recurrent neural network (RNN) that consists 3 layors:
To evaluate the performance of our price scheduling strat- multilayer perceptron (MLP), gated recurrent unit (GRU)
egy, we pick the VDN model performance with the identical and MLP. The agent network outputs each value function
EV user preference model (without price scheduling) as our Q i,z(τ i,z,a i,t,z) into the mixing network, where the mixing
baseline,andcomparethecorrespondingaccumulatedrewards network consists of hypernetworks that take in the state s t,z,
under the aforementioned scenarios, depicted in Figure 5. In all the generated Q i,z(τ i,z,a i,t,z) together with the weights
TableIII,theaveragerewardatconvergenceandtheepisodeof and biases from the hypernetworks are used to produce the
convergence are presented under each scenario, and results of total action value function Q tot(τ,a).
the price scheduling method, baseline are compared. Scenario Additionally,the local argmax applied on each individual
1: Our price scheduling method converges 68 episode faster action value function should obtain the same monotonicity
than the baseline, while the average reward at convergence effect as the global argmax applied on the total action value
of our method is 157.537 higher than the baseline, and the function, in which can be expressed as
accumulated rewards appear to be more compact. Scenario 2:  argmax Q (τ ,a )
u1,0 1,0 1,0 1,0
Thepriceschedulingmethodconverges82episodesfasterthan .
argmax Q (τ,a)= . 
the baseline, and the corresponding average reward is higher u tot  . 
argmax Q (τ ,a )
by 137.77; again, the accumulated rewards are virtually more un,1 n,1 n,1 n,1
(27)
compact in comparison with the baseline results. Scenario 3:
Our method converges 16 episodes faster, while achieving a Moreover, this expression can be further generalized to a
significant higher average award of -442.093, with a better constraint, as shown in equation (28).
overall reward compactness over the baseline.
∂Q
tot ≥0,∀i∈[1..n],∀z ∈[0..1] (28)
D. Q-mix performance evaluation ∂Q
i,z
To further investigate the effectiveness of the proposed TheQ-mixmodeladaptsthesameparametersettingsasVDN.
method,wereplacetheVDNmodelwithQ-mixnetwork.The The same three scenarios are implied, with the corresponding
CS z passes the agent observation o and previous action accumulated reward performances shown in Figure 6. From
i,t,z
step a to the agent network, where the agent network Table IV, under scenario 1 the price scheduling strategy
i,t−1,zFig. 6. Q-mix accumulation reward distribution
TABLE I
converges1epochfasteroverthebaseline,however,thecorre-
CHINESETIME-OF-TARIFF
sponding average reward at convergence is 182.975 less than
thebaselineandtheaccumulatedrewardsareshowntobeless
Time CS 0 CS 1
compact.Movingontoscenario2,thepriceschedulingmethod
Price(CNY/kWh)
converges95episodesslowerthanthebaseline,buttheaverage
rewardis200.864higherandthedatascatteringappearstobe 07:00 - 10:00 1.0044 1.2044
11:00 - 13:00 0.6950 0.7950
more compact. For scenario 3, the price scheduling method
14:00 - 17:00 1.0044 1.2044
achieves a noticeably higher average award of -696.103 over
18:00 - 19:00 0.6950 0.7950
the baseline, similar to the simulation results for the VDN 20:00 - 06:00 0.3946 0.4946
network, while achieving it with faster episode convergence
and a comparable overall reward compactness between the TABLE II
two methods. The above results indicate that, under scenarios COMMUTINGBEHAVIOURDISTRIBUTIONS
where charging pile sizes at each station and the arrived EV
sizes are sufficiently large, the price scheduling strategy is Parameters Distribution Boundary
capabletoproducingnoticeablerewardoptimizationimprove- t N(9,12) 7≤t ≤11
a a
ment over the baseline. Together with the VDN performance t N(19,12) 17≤t ≤21
d d
results, it is sufficient to claim that under reasonably large SOC a N(0.4,0.12) 0.2≤SOC a ≤0.6
dataset, the proposed price scheduling strategy is capable of d(cid:103)i,0 N(0.5,0.32) 0<d(cid:103)i,0 <1
providingnoticeablerewardoptimizationimprovements,while
achieving it with faster episode convergence.
With numerous simulations over two separate CSs and under
VI. CONCLUSION
three different scenarios , the proposed method is proven
Toconclude,weintroducedamulti-CSdecentralizedcollab- to achieve sufficient reward optimisation and convergence
orative framework, while employing a modified EV user pref- improvement results over the baseline. For potential future
erence model. Furthermore, we proposed a price scheduling research, we aim to introduce multi-neural network interface,
method that achieves comparable EV shunting performance. with the current price scheduling strategy reinforced withTABLE III
[9] T. Sousa, H. Morais, Z. Vale, and R. Castro, “A multi-objective opti-
VDNPERFORMANCERESULTS
mizationoftheactiveandreactiveresourceschedulingatadistribution
levelinasmartgridcontext,”Energy,vol.85,pp.236–250,2015.
Method Average reward Episode of convergence [10] T.Qian,C.Shao,X.Wang,andM.Shahidehpour,“Deepreinforcement
learning for ev charging navigation by coordinating smart grid and
SCENARIO1
intelligent transportation system,” IEEE transactions on smart grid,
Price Schedule -141.383 1170 vol.11,no.2,pp.1714–1723,2019.
[11] Q. Xing, Y. Xu, Z. Chen, Z. Zhang, and Z. Shi, “A graph reinforce-
Baseline -298.92 1238
ment learning-based decision-making platform for real-time charging
SCENARIO2 navigationofurbanelectricvehicles,”IEEETransactionsonIndustrial
Informatics,vol.19,no.3,pp.3284–3295,2022.
Price Schedule -63.288 1063 [12] B.Wang,B.Hu,C.Qiu,P.Chu,andR.Gadh,“Evchargingalgorithm
Baseline -201.058 1145 implementation with user price preference,” in 2015 IEEE Power &
EnergySocietyInnovativeSmartGridTechnologiesConference(ISGT),
SCENARIO3
pp.1–5,IEEE,2015.
Price Schedule -442.093 1176 [13] J. Liu, S. Wang, and X. Tang, “Pricing and charging scheduling for
cooperative electric vehicle charging stations via deep reinforcement
Baseline -547.313 1192
learning,”in2022IEEEInternationalConferenceonCommunications,
Control, and Computing Technologies for Smart Grids (SmartGrid-
TABLE IV Comm),pp.212–217,IEEE,2022.
Q-MIXPERFORMANCERESULTS [14] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi,
M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls, et al.,
“Value-decomposition networks for cooperative multi-agent learning,”
Method Average reward Episode of convergence
arXivpreprintarXiv:1706.05296,2017.
SCENARIO1 [15] T. Rashid, M. Samvelyan, C. S. De Witt, G. Farquhar, J. Foerster,
and S. Whiteson, “Monotonic value function factorisation for deep
Price Schedule -401.474 1214 multi-agentreinforcementlearning,”TheJournalofMachineLearning
Baseline -218.499 1215 Research,vol.21,no.1,pp.7234–7284,2020.
[16] Y.Zhao,Y.Guo,Q.Guo,H.Zhang,andH.Sun,“Deploymentofthe
SCENARIO2 electricvehiclechargingstationconsideringexistingcompetitors,”IEEE
Transactionsonsmartgrid,vol.11,no.5,pp.4236–4248,2020.
Price Schedule -112.736 1128
Baseline -313.6 1223
SCENARIO3
Price Schedule -696.103 1096
Baseline -1732.4 1142
additionalnetworkstoimprovethecapabilityofhandlinglarge
scaled EV transportation models. Moreover, the optimization
goalcanbeextended,suchasmergingCSrouterecommenda-
tion goals with price reduction, adapting further practicalities
in a real world situation.
REFERENCES
[1] “Theuk’splansandprogresstoreachnetzeroby2050,”pp.1–31,The
HouseofCommonsLibrary,2023.
[2] “Poweringupbritain:Netzerogrowthplan,”pp.1–126,Departmentfor
EnergySecurityandNetzero,2023.
[3] H. Song, C.-C. Liu, J. Lawarre´e, and R. W. Dahlgren, “Optimal elec-
tricity supply bidding by markov decision process,” IEEE transactions
onpowersystems,vol.15,no.2,pp.618–624,2000.
[4] H. Li, G. Li, T. T. Lie, X. Li, K. Wang, B. Han, and J. Xu, “Con-
strained large-scale real-time ev scheduling based on recurrent deep
reinforcement learning,” International Journal of Electrical Power &
EnergySystems,vol.144,p.108603,2023.
[5] Z. Wen, D. O’Neill, and H. Maei, “Optimal demand response using
device-basedreinforcementlearning,”IEEETransactionsonSmartGrid,
vol.6,no.5,pp.2312–2324,2015.
[6] S.IqbalandF.Sha,“Actor-attention-criticformulti-agentreinforcement
learning,” in International conference on machine learning, pp. 2961–
2970,PMLR,2019.
[7] H.Li,B.Han,G.Li,K.Wang,J.Xu,andM.W.Khan,“Decentralized
collaborative optimal scheduling for ev charging stations based on
multi-agent reinforcement learning,” IET Generation, Transmission &
Distribution,2023.
[8] L.Zou,M.S.Munir,Y.K.Tun,S.Kang,andC.S.Hong,“Intelligentev
chargingforurbanprosumercommunities:Anauctionandmulti-agent
deepreinforcementlearningapproach,”IEEETransactionsonNetwork
andServiceManagement,vol.19,no.4,pp.4384–4407,2022.