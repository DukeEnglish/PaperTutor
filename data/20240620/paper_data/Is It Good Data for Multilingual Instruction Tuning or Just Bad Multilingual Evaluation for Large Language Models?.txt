Is It Good Data for Multilingual Instruction Tuning or
Just Bad Multilingual Evaluation for Large Language Models?
PinzhenChen1 SimonYu1 ZhichengGuo2 BarryHaddow1
1UniversityofEdinburgh 2TsinghuaUniversity
{pinzhen.chen,s1967531,bhaddow}@ed.ac.uk,guo-zc21@mails.tsinghua.edu.cn
Abstract anddevelopedusingtranslationaspartofthepro-
cess(Lietal.,2023a;Chenetal.,2023b;Laietal.,
Largelanguagemodels,particularlymultilin-
2023).
gualones,aredesigned,claimed,andexpected
tocatertonativespeakersofvariedlanguages. Notably,ayear-longprojectinvitedvolunteers
We hypothesise that the current practices of around the globe to write prompt-completion re-
fine-tuning and evaluating these models may sourcesintheirnativelanguage(Singhetal.,2024,
mismatch this intention owing to a heavy re- “AyaDataset”,nottobeconfusedwith“AyaCollec-
lianceontranslation,whichcanintroducetrans-
tion”composedofmanytranslatedcomponents),
lation artefacts and defects. It remains un-
makingitalanguage-nativedataset. Althoughthe
known whether the nature of the instruction
Ayadatasetwascontributedbyvolunteers,itbears
data has an impact on the model output; on
theotherhand,itremainsquestionablewhether anexpansivesocialutilitycostconsideringtheper-
translated test sets can capture such nuances. sonnelhoursdevoted. Incomparisonwithbuilding
Due to the often coupled practices of using language-nativedatasetsfromscratch,translating
translateddatainbothstages, suchimperfec- existing(usuallyEnglish)resourcesintomorelan-
tionscouldhavebeenoverlooked. Thiswork
guagesisaconvenientoption. Nonetheless,trans-
investigatestheseissuesbyusingcontrolledna-
lateddatacarryapparentlimitations(Clarketal.,
tiveortranslateddataduringinstructiontuning
2020; Artetxe et al., 2020a): 1) it represents the
andevaluationstagesandobservingmodelre-
cultureandknowledgespecifictotheoriginallan-
sults. Experimentsoneightbasemodelsand
eightdifferentbenchmarksrevealthatnativeor guage; and 2) the translation process introduces
generation benchmarks display a notable dif- translationese,anunnaturallanguagestyle(Geller-
ferencebetweennativeandtranslatedinstruc- stam,1986;Baker,1996),aswellasdefectssince
tiondataespeciallywhenmodelperformance certaintaskscouldbecorrupted,e.g. grammatical
ishigh,whereasothertypesoftestsetscannot.
errorcorrection. Ontheotherhand,recentresearch
Finally,wedemonstratethatregularizationis
discovered that instruction tuning is “superficial”
beneficialtobridgingthisgaponstructuredbut
notgenerativetasks.1 whereanLLMmainlylearnstheresponseformat
(Zhouetal.,2023),anditcannotenhanceknowl-
1 Introduction
edgeatthecurrentscale(Ghoshetal.,2024). These
Instructiontuning, orsupervisedfine-tuning, can insightsimplythattheshortcomingsoftranslated
prepare a large language model (LLM) for bet- datamightnotpropagateintoaninstruction-tuned
ter task generalization and natural interactions in model. Hence,whenwideninginstructionlanguage
downstreamapplications(Mishraetal.,2022;Wei support,aquestionarises: Wouldtranslateddata
besufficientforinstructiontuning?
etal.,2022;Sanhetal.,2022;Ouyangetal.,2022).
Majoreffortsofbuildinginstructiondatasetscen- Weregard“sufficiency”asthattranslatedinstruc-
treonEnglish(Weietal.,2022;Taorietal.,2023; tion data should lead to at least the same output
Conoveretal.,2023;Ivisonetal.,2023),whereas quality as native data when used to fine-tune an
themultilingualcounterpartsremainmodestinsize, LLMforaspecificlanguage. Yet,comparingnative
variety,andcoverage. Manymultilingualinstruc- andtranslateddatacannotbeseparatedfromcare-
tiondatasetshavebeenseededfromEnglishdata fullyconsideringtheevaluationprotocol,because
manyexistingmultilingualbenchmarkshavebeen
1To facilitate reproduction, our code and data will
createdviatranslation. Thistranslationbias,when
be released at https://github.com/PinzhenChen/
good-data-or-bad-eval. presentinbothtrainingandtestsets,couldhinder
1
4202
nuJ
81
]LC.sc[
1v22821.6042:viXraameaningfulconclusion. Wethusputforwardour Languages Westudymodelperformanceinthree
secondresearchquestion: Iftranslatedandnative languages—Spanish (es), Russian (ru), and Chi-
instructiondatamakeadifference,wouldatrans- nese (zh)—with the following considerations: 1)
lated benchmark capture it? Subsequently, since these languages cover a combination of different
nativedataisexpensivetoobtain,whenwehaveto languagefamiliesandwritingscripts;2)theseare
usetranslateddata,whattechniquescanweadopt medium-to-highresourced,wherethequalityofthe
tobridgetheperformancegap? training data, native or translated instructions, is
Thisworksystematicallyinvestigatesnativeand satisfactory;3)theirpresenceinLLMpre-training
translateddataduringinstructiontuningandeval- data is significant, so we can expect reasonable
uation. Our experiments are carried out on eight outputquality.
models with varying sizes and pre-training data
Nativedata WeusethetrainingsplitintheAya
distributions. The models are evaluated on eight
dataset(Singhetal.,2024),whichwaswrittenfrom
benchmarksofdifferentdatanatures. Empiricalre-
scratch and then edited by human annotators in
sultssuggestthataprudentchoiceinevaluationop-
theirnativelanguage. Spanish,Russian,andChi-
tionswhenevaluatingdatafactorsiscrucial. Fore-
nesedatahavesizesof3854,423,and3944each.
shadowing our empirical findings answering the
researchquestionsraisedearlier: Translated data We generate translated data
equivalentinvolumetotheoriginaldatasets. This
1) Nativeandtranslateddatacansometimeslead
isdonebysamplingAya’sEnglishsplittomatch
to a performance gap especially when the
thesizeofnativedataineachlanguageandtrans-
modelperformanceisstrong.
latingittothatlanguage. Wealwaystranslatethe
instructionsandresponsesseparately. Twodistinct
2) Such a difference is more pronounced on
versionsoftranslateddataareobtainedviaGoogle
benchmarks that are natively created (TyDi
TranslateandCohereCommand-R2.GoogleTrans-
QA,CMMLU,C-Eval)orgenerativeinnature
lateisawell-knowncommercialtranslationengine,
(XQuAD,open-endedgeneration)backedby
whereasCommandR,asalargelanguagemodel,
acorrelationanalysis.
iscapableofadheringtospecificguidelines. Tech-
3) Training regularization like a lower learn- nically, we prompt Command R to maintain the
ing rate or multilingual instruction tuning originaldataformattingwhiletranslating,asillus-
canbebeneficialwhenusingtranslateddata. tratedAppendixA.1.
We demonstrate that it can close the native-
2.2 Close-endedevaluation
translatedperformancegaponstructuredtasks
butnotgenerativetasks. Weperformautomaticevaluationsonclose-ended
tasks,whereamodelneedstogenerateanoption
Finally, these insights recommend multilingual fromapre-definedsetofresponsesgivenaquestion.
(non-English)LLMevaluationbecarriedoutona Theevaluationcoversmultilingualunderstanding
rangeofbenchmarksincludinglanguage-nativeor andreasoningtaskscommonlyusedtobenchmark
generativetasks. LLMs. Thesetestsetscomefromvarioussources
suchasnativeannotation,humantranslation,and
2 ExperimentDesign
machine translation. All close-ended evaluations
areconductedusingLMEvalHarness(Gaoetal.,
2.1 Instructiondata
2023)withdefaultsettingsunlessstatedbelow.
Thefocusofourstudyistoanswerthethreeques-
tionsrevolvingaroundthenatureofinstructiondata Native benchmarks We first evaluate our
andevaluationdataandtheirimpactonatrustwor- instruction-tunedmodelsontestsetsthathavebeen
thy evaluation. We consider experimenting with constructed from scratch by native speakers, on
non-Englishtrainingandtestdatacreatedthrough whichwehypothesizeaperformancegapbetween
distinct processes: created natively and trans- nativeandtranslatedinstructionfine-tuning.
lated. Weexperimentwithmonolingualinstruction
• TyDiQAiscreatedbyinvitingnativespeak-
tuning: anLLMisfine-tunedinasinglelanguage
erstowritedownquestionsrelatedtoarticles
everytimetopreventpotentialcross-lingualinflu-
ences. 2https://docs.cohere.com/docs/command-r
2showntothem(Clarketal.,2020). Weusethe • Native: 50questionsinSpanish,Russian,and
Russiansplitandmeasuremodels’F1scores. Chinese,directlysampledfromOASST1. We
Wefeedaone-shotexampletofamiliarisethe only use the first-round queries in the multi-
modelwiththeanswerformat. turnconversations.
• CMMLU (Li et al., 2023b) and C-Eval Given the open-ended generation nature, there
(Huang et al., 2024). Both are multi- isnogoldresponsetocompareamodelgeneration
disciplinary test sets including questions on against. To avoid expensive human evaluation at
the Chinese culture and domain, made from scale,weuseLLM-as-a-judge,whichhasshowna
resourcesinChinese. Weevaluatewithfive- strongcorrelationwithhumanjudgement(Zheng
shotexamplesanduseaccuracyasthemetric. etal.,2024). Sinceoneofthetranslatedversions
hasbeenobtainedfromCommandR,weusetwo
Unfortunately,wecouldnotidentifyanativebench-
otherjudgestoavoidLLMpreferencebias,GPT-
markthatassessesgeneralknowledgeinSpanish.
4-TurboandCommandR+,whicharealsoconsid-
Translatedbenchmarks Weusethreetranslated eredtobethecurrentmostpowerfulLLMs.3 The
testsetswheretwoarehuman-translatedandone judgesdirectlyscoreeachinstruction-responsepair
isChatGPT-translated. Allthreetestsetscomprise accordingtoa5-pointLikertscale(1to5),which
asubsetforeachofthethreelanguagesweintend can avoid position bias in response comparisons.
toevaluate. The total score for a model therefore ranges be-
tween50to250. Theexactwordingofthejudging
• XQuAD is a question answering task re-
promptisthesameforbothLLMsandisattached
quiring text extraction from a given con-
inAppendixA.2Figure6.
text (Artetxe et al., 2020b). It was human-
translated from the English SQuAD (Ra- 3 ExperimentsandAnalysis
jpurkar et al., 2016). Evaluation is done in
3.1 Technicalsetup
a zero-shot setting. We adopt two metrics:
a strict exact string match and a lenient “in- Base models We fine-tune base models of dif-
clude”tocheckwhetherthegoldreferenceis ferent sizes from three sources: 1) Llama 2 at
asubstringofthemodelgeneration. 7B, trained on 2T tokens with a 32K vocabulary
(Touvron et al., 2023) and released in Jul 2023;
• MGSM (Shi et al., 2023) has been human-
2) Gemma at 2B and 7B (circa 8.54B parame-
translatedfromtheEnglishGSM8K(Cobbe
ters), trained on 3T and 6T tokens respectively
et al., 2021) containing grade school mathe-
witha256KvocabularyandreleasedinFeb2024
maticsquestions. Weprovide5-shotexamples
(GemmaTeametal.,2024);3)Qwen1.5at0.5B,
with chain-of-thought and measure exact to-
1.8B,4B,7B,and14BreleasedinFeb2024(Qwen
kenmatch.
Team,2024).
• M-MMLU refers to Lai et al. (2023)’s
Instruction tuning Let I represent an instruc-
machine-translated multilingual MMLU
tion,X anoptionalinput,andY = y ,y ,...,y
1 2 |Y|
(Hendrycks et al., 2021), designated as
asequenceofoutputtokens. Theinstructionandin-
M-MMLU in this work. We follow the
putsequencesarefirsttemplatedintoapre-defined
defaultsettingofproviding5-shotexamples
format, denoted as T(I,X). We fine-tune an
andusingaccuracyasthemetric.
LLM parameterized by θ by optimizing the log-
2.3 Open-endedgeneration likelihoodontheoutputtokensonly:
Wethenevaluatemodelresponsestoopen-ended
questionsundercomparabletranslatedandnative L(Y,T(I,X);θ)=−logP(Y|T(I,X);θ)
settings: |Y|
(cid:88)
=− logP(y |y ,T(I,X);θ)
t <t
• Translated: 50EnglishquestionsfromOpe- t=1
nAssistant(OASST1;Köpfetal.,2023)and
We apply low-rank adaptation for LLM fine-
thenhuman-translatedbyChenetal.(2024).
tuningwherethebasemodelisloadedas8-bitand
We use the translated questions in Spanish,
Russian,andChinese. 3BothaccessedviaAPIinApr2024.
3TyDiQA(ru;F1%) XQuAD(averageofes,ru,zh;exactmatch%)
40 40 30
30
20 20
10 10
Llama2-7 GB emma-2 GB emma Q-7 wB en1.5- Q0 w.5 eB n1.5-1 Q.8 wB en1.5- Q4 wB en1.5 Q-7 wB en1.5-14B Llama2-7 GB emma-2 GB emma Q-7 wB en1.5- Q0 w.5 eB n1.5-1 Q.8 wB en1.5- Q4 wB en1.5 Q-7 wB en1.5-14B
C-Eval(zh;accuracy%) XQuAD(averageofes,ru,zh;include%) 80 60
60 45 40 30
20 15
Llama2-7 GB emma-2 GB emma Q-7 wB en1.5- Q0 w.5 eB n1.5-1 Q.8 wB en1.5- Q4 wB en1.5 Q-7 wB en1.5-14B Llama2-7 GB emma-2 GB emma Q-7 wB en1.5- Q0 w.5 eB n1.5-1 Q.8 wB en1.5- Q4 wB en1.5 Q-7 wB en1.5-14B
CMMLU(zh;accuracy%) MGSM(averageofes,ru,zh;accuracy%) 60 80
45 60
40 30
20 15
Llama2-7 GB emma-2 GB emma Q-7 wB en1.5- Q0 w.5 eB n1.5-1 Q.8 wB en1.5- Q4 wB en1.5 Q-7 wB en1.5-14B Llama2-7 GB emma-2 GB emma Q-7 wB en1.5- Q0 w.5 eB n1.5-1 Q.8 wB en1.5- Q4 wB en1.5 Q-7 wB en1.5-14B
M-MMLU(averageofes,ru,zh;accuracy%)
Native Cohere-translated Google-translated 60
45
Figure1: Resultsonnativeclose-endedtestsets: native
30
instruction-tunedmodelshaveanedge.
15
frozen during training (Hu et al., 2022; Dettmers Llama2-7 GB emma-2 GB emma Q-7 wB en1.5- Q0 w.5 eB n1.5-1 Q.8 wB en1.5- Q4 wB en1.5 Q-7 wB en1.5-14B
etal.,2023). Weattachtoallkey,query,andvalue
Native Cohere-translated Google-translated
matricesalow-rankadapterwitharankof8,anal-
phaof16,andadropoutof0.05. Thelearningrate
issetto1e−4 andtheeffectivebatchsizeto64. All
Figure2: Resultsontranslatedclose-endedtestsets: na-
models are given a training budget of 10 epochs
tiveinstruction-tunedmodelsaresuperioronXQuAD,
and we validate perplexity after every epoch to
but all data conditions have comparable results on
keepthebestcheckpoint. Weusedacombination MGSMandMMMLU.
ofNVIDIA3090,A100-40,andA100-80GPUs;
convergencetookbetween1hour(Russian)and7
hours(Spanish,Chinese). tentlyandsignificantlyoutperformstranslateddata
underbothmetrics,however,itlosestheadvantage
3.2 Isthereagap,andonwhat?
onMGSMandM-MMLU.
We display results for the native tests, TyDi QA, For open-ended question answering, we show
C-Eval, and CMMLU, in Figure 1. It shows that differentcombinationsofthedata(nativeorhuman-
modelsfine-tunedwithnativeinstructionssurpass translatedquestions)andjudges(GPT-4-Turboor
thosefine-tunedwithtranslateddatainmostcases Command R+) in Figure 3. It can be found that
withconsistentpatternsacrossthetwolanguages. the largest discrepancy between native and trans-
Intermsofbenchmarkscreatedviatranslation, lated data occurs when the models are tested on
asshowninFigure2,weobservedivergingtrends. translatedquestionsandjudgedbyGPT-4-Turbo.
On the XQuAD benchmark, native data consis- Whentestingontranslatedquestionsandjudgedby
4
3.82
8.03
7.23
8.82
8.03
2.03
5.52
4.13
2.13
5.82
0.03
8.13
2.82
1.82
4.92
3.82
5.92
7.03
6.33
1.24
7.84
4.43
9.44
3.84
7.03
0.54
4.64
5.22
5.74
1.44
8.91
2.34
6.14
5.71
1.94
8.24
1.02
9.55
9.55
2.02
1.75
8.35
1.81
4.55
9.25
9.12
2.66
3.56
0.02
1.06
3.95
1.02
7.95
8.95
0.73
5.17
3.27
2.43
5.86
4.86
2.72
5.86
6.76
4.43
4.77
2.87
0.33
8.37
2.67
1.23
6.27
8.57
3.03
8.94
3.9
6.53
8.02
1.13
1.9
4.33
1.12
2.83
1.8
7.33
7.01
8.82
1.11
5.13
5.9
7.72
2.31
2.03
9.9
0.23
9.01
3.13
4.71
7.33
5.23
5.25
8.41
9.32
6.33
4.45
5.41
3.13
8.63
1.35
2.81
6.63
2.3
9.03
8.01
6.52
0.4
7.03
5.11
8.92
4.4
8.13
0.12
0.54
3.9
1.63
8.01
5.82
6.9
2.63
2.9
9.33
9.8
2.63
9.13
1.45
5.52
8.24
8.42
6.14
2.91
3.14
8.02
6.34
2.52
9.04
9.43
0.75
9.04
2.05
9.42
4.24
1.83
6.94
0.22
4.54
8.04
8.94
5.63
1.85
8.25
3.85
5.82
4.24
1.94
3.65
4.62
1.64
1.25
1.65Translatedquestions(averageofes,ru,zh;CmdR+) 1
250 0.8 0.81
0.70 0.67 200 0.6
0.44
150 0.4 0.34
0.25
100 0.2
50
Llama2-7 GB emma-2 GB emma Q-7 wB en1.5- Q0 w.5 eB n1.5-1 Q.8 wB en1.5- Q4 wB en1.5 Q-7 wB en1.5-14B TyDi QA C-MMLU C-Eval XQuAD QA-GPT4 QA-CmdR+
Translatedquestions(averageofes,ru,zh;GPT4)
Figure4: Pearson’scorrelationbetweennativedataper-
250 formanceandnative-translatedperformancedifference
200 forvariousbenchmarks: weakercorrelationforstruc- turedtasksandstrongercorrelationforgenerativetasks.
150
100
50 translatedinstructiondataisnotalwayssufficient.
Llama2-7 GB emma-2 GB emma Q-7 wB en1.5- Q0 w.5 eB n1.5-1 Q.8 wB en1.5- Q4 wB en1.5 Q-7 wB en1.5-14B Whiletheseobservationshavebeenmadefromthe
aspectofdata/modelperformance,theycannotbe
Nativequestions(averageofes,ru,zh;CmdR+)
decoupledfromthepotentialtestsetimperfections.
250
Assumingnativedatashouldleadtobettermetric 200 numbers,ithasbeenrevealedthattwotypesofeval-
150 uationbenchmarksaremoreeffectiveinreflecting
100 this: 1)thosethatoriginateinthetestlanguageit-
50 self(TyDiQA,C-Eval,andCMMLU)and2)those
Llama2-7 GB emma-2 GB emma Q-7 wB en1.5- Q0 w.5 eB n1.5-1 Q.8 wB en1.5- Q4 wB en1.5 Q-7 wB en1.5-14B that are generative in nature (XQuAD and open-
endedquestions)eventhoughtheycouldhavebeen
Nativequestions(averageofes,ru,zh;GPT4)
translatedfromEnglish.
250 200 3.3 Whenisthegapobvious?
150 Considering the absolute scores on each test set
100 in Figure 1, we hypothesise that the output qual-
50 itydifferencebetweenusingnativeandtranslated
Llama2-7 GB emma-2 GB emma Q-7 wB en1.5- Q0 w.5 eB n1.5-1 Q.8 wB en1.5- Q4 wB en1.5 Q-7 wB en1.5-14B data would be more noticeable when a model’s
overallperformanceisbetter—namely,thesubtle
Native Cohere-translated Google-translated translation bias might not be pivotal if a model’s
capabilityissoundesirablethatmanyinstancesare
predictedincorrectlyinthefirstplace. Hence,for
Figure3: Restulsonnativeandtranslatedopen-ended each previous benchmark where native data out-
question answering: native instruction-tuned models
performs translated data, we conduct a post hoc
are superior for translated questions when judged by
analysisonthecorrelationbetweenthenativedata
GPT-4-Turbo, butalldataconditionsresultinsimilar
performanceandthenative-translateddataperfor-
numbersinothercases.
mancegap.
We average the Cohere-translated and Google-
translated scores to represent the final score for
CommandR+,nativedataisslightlyaheadwhen
translateddata. Thescoredifferencebetweenmod-
the model size is big. In other cases, native data
elsinstruction-tunedonnativedataandtranslated
is not better than translated data. These results
datacanthenbedefinedas
also suggest that the LLM-as-a-judge metric af-
fectsempiricalresults,however,wedonotmakea 1
∆S = S − (S +S )
conclusionsincewecannothavetransparentaccess native 2 cohere google
toGPTorCommandmodels.
where S , S , and S stand for model
native cohere google
Overall, we see that in terms of model perfor- scores on native, Cohere-translated, and Google-
mance,nativedatacansurpasstranslateddatawhen translateddatarespectively. Then,wecomputethe
evaluated in various ways, which suggests that Pearsoncorrelationcoefficientr between
∆S,S
native
5
0.061
5.171
6.361
2.961
2.161
3.621
3.951
7.461
0.461
7.521
0.661
5.361
7.941
8.541
0.941
5.831
3.751
7.611
0.161
7.951
0.551
7.511
0.751
2.441
7.871
5.612
8.871
3.602
0.171
0.051
3.271
5.402
0.671
3.751
3.181
2.891
3.911
0.201
7.921
1.111
3.911
3.67
3.621
3.901
3.321
7.18
0.421
3.001
0.831
5.621
0.151
0.431
3.241
7.001
3.741
7.131
3.441
7.19
0.051
0.121
7.461
5.571
3.461
0.851
0.951
3.711
0.961
3.551
0.851
0.611
7.261
0.851
0.161
5.781
3.761
0.281
7.361
2.731
0.661
7.191
7.261
7.231
3.761
0.571
3.071
2.402
7.761
3.281
3.161
7.251
0.871
5.502
0.261
0.041
0.571
8.881∆S andS
native
foreachtestset. Itisworthnoting 1e−6← 1e−4 1e−4
BaseModel Data
that we consider all individual languages’ scores Mono Mono →Multi
insteadoftheaveragednumberacrosslanguages native 33.4 28.3 25.1
whereapplicable. Llama2-7B cohere 33.3 28.8 23.4
google 33.3 25.5 22.9
We cover all benchmarks where a clear native-
native 37.7 33.6 31.5
translated difference has been observed earlier.
Gemma-7B cohere 38.1 34.4 31.4
Open-endedquestionansweringisabbreviatedas google 37.9 30.7 30.9
QA-GPT4andQA-CmdR+dependingontheLLM native 18.1 21.9 28.3
judge used. The outcome is shown in Figure 4. Qwen1.5-4B cohere 17.9 20.0 22.5
google 17.9 20.1 22.5
As can be seen, the correlation between ∆S and
S is weak for structured tasks like TyDi QA, native 22.4 37.0 37.0
native
Qwen1.5-7B cohere 22.9 34.2 33.0
C-MMLU, and C-Eval, but very strong for tasks
google 22.7 27.2 27.1
involvinggenerationlikeXQuADandopen-ended
native 24.8 34.4 32.8
questionanswering. Thispatternindicatesthat1) Qwen1.5-14B cohere 24.6 33.0 29.3
concerningtheinstructiondata,thenatureofbeing google 24.9 32.1 35.2
nativeortranslatedshinesthroughasthemodelper-
Table1: SometimesthegapcanbeclosedonTyDiQA.
formance gets stronger; 2) on the evaluation end,
suchdatadifferenceleavesamorepronouncedgap
ongenerativebenchmarks. Multilingualism Another exploration is multi-
Onarelatednote, inKewetal.(2023)’sinves- lingual instruction tuning, which could prevent a
tigation on multilingual instruction tuning, it is model from overfitting to a single language. In
witnessedthatcross-lingualtransferismorepromi- additiontoSpanish, Russian, andChinesewhich
nent in generative tasks but less in classification weevaluate,wealsoaddanotherfivelanguages—
tasks. Althoughouranalysesareoncompletelydis- Arabic (ar), German (de), Finnish (fi), Irish (ga),
tinct data characteristics, it might be conjectured andHindi(hi)—intothemultilingualpot. Forthe
that the quality of the instruction data could be nativemultilingualdata,wesimplydown-sample
moredistinguishedwhenthemodelisevaluatedby all languages in the Aya dataset to a size of 241
generationtasksasopposedtostructuredtasks. (thesizeoftheGermansplitinAya,whichisthe
smallestamongtheeightlanguages),leadingtoa
3.4 Canwebridgethegap?
totalsizeof1928. Forthetranslateddataineach
We have found empirical evidence that native in- language,werandomlyselect241instancesfrom
structiondatacanleadtobetterbenchmarkresults Englishandtranslatethem(differentdatapointsfor
thantranslateddata. We,therefore,investigatetwo differentlanguages). Thissimulatesamultilingual
techniques which can apply better regularization instructionsetderivedfromtranslatingEnglishre-
duringinstructiontuninginordertoreducetheneg- sources.
ativeimpactoftranslateddata. Thisalsorepresents
anefforttopursueamoregeneralizablefinding. Setup Foreachofourpreviousdata-modelcom-
binations, we now have two variants. Due to
A lower learning rate Our first inspiration is thespaceconstraint,wedisplayonlyresultsfrom
drawnfromChirkovaandNikoulina(2024),whose largerbasemodelsinthemaincontentforthefol-
experiments show that English instruction-tuned lowingbenchmarks: TyDiQA,CMMLU,XQuAD,
modelsdisplayremarkablydifferentlevelsofcross- MSGM,M-MMLU,andopen-endedquestionan-
lingualtransferwhenthemerechangingfactoris swering. Weboldthebestnativeresultsandunder-
the learning rate—a smaller one leads to better linetranslatedresultsiftheyareclosetonative—
instructionfollowinginzero-shotlanguages. This meaningthatthegapcanbeclosed. Moreover,ex-
means that it is possible to teach a base model haustiveresultsforallmodelsandalllanguageson
a desired instruction-response style without even variousbenchmarksareenclosedinTables7to20
touchingonthecontentorlanguage. Inthiscase, inAppendixB.
theundesirablepropertiesintranslateddatacould
be mitigated. Following this, we run another set Native,structuredbenchmarks Wemakebold
ofexperimentswiththelearningratereducedfrom thosescoresthatarehigherthantherestforeach
1e−4 to1e−6. modelunderallhyperparametersettings. Wefind
61e−6← 1e−4 1e−4 1e−6← 1e−4 1e−4
BaseModel Data BaseModel Data
Mono Mono →Multi Mono Mono →Multi
native 31.8 32.7 32.6 native 9.5 9.3 10.8
Llama2-7B cohere 32.0 30.2 32.7 Llama2-7B cohere 9.8 9.1 10.8
google 32.0 31.2 32.1 google 9.7 8.1 12.0
native 49.9 48.7 50.1 native 38.9 32.5 37.1
Gemma-7B cohere 49.7 48.3 50.4 Gemma-7B cohere 38.8 33.6 37.2
google 49.8 46.4 50.7 google 39.5 36.8 36.4
native 66.0 65.3 66.4 native 42.1 40.9 41.2
Qwen1.5-4B cohere 65.7 59.3 65.2 Qwen1.5-7B cohere 41.5 38.1 40.8
google 66.0 59.8 65.2 google 43.3 40.8 44.5
native 72.0 72.3 72.6 native 55.9 52.8 55.2
Qwen1.5-7B cohere 71.9 68.4 71.4 Qwen1.5-14B cohere 55.7 49.1 53.5
google 71.9 67.6 71.4 google 55.7 52.1 56.4
native 77.7 78.2 78.2
Qwen1.5-14B cohere 77.8 76.2 77.6 Table4: ThereisalwaysnogaponMGSM
google 77.8 75.8 77.2
Table2: SometimesthegapcanbeclosedonCMMLU. 1e−6← 1e−4 1e−4
BaseModel Data
Mono Mono →Multi
1e−6← 1e−4 1e−4
native 35.8 35.6 36.3
BaseModel Data
Mono Mono →Multi Llama2-7B cohere 35.8 33.4 34.1
google 35.8 33.7 34.3
native 18.5 30.3 31.0
Llama2-7B cohere 18.0 20.8 21.6 native 53.7 52.5 53.7
google 17.8 21.1 24.1 Gemma-7B cohere 53.8 54.4 55.6
google 54.0 53.1 55.6
native 17.8 17.4 16.8
Gemma-7B cohere 17.3 14.8 16.3 native 50.3 50.2 50.6
google 17.2 14.5 15.3 Qwen1.5-7B cohere 50.2 49.6 51.2
google 50.1 49.8 50.9
native 27.8 31.9 40.2
Qwen1.5-4B cohere 26.9 24.8 29.9 native 58.2 58.3 58.3
google 26.8 20.8 31.8 Qwen1.5-14B cohere 58.3 56.3 58.5
google 58.3 56.1 58.2
native 30.7 34.9 42.6
Qwen1.5-7B cohere 30.2 24.9 31.5
google 29.9 22.0 27.7 Table5: ThereisalwaysnogaponM-MMLU.
native 33.4 36.5 45.6
Qwen1.5-14B cohere 33.6 28.5 30.8
google 33.5 26.4 32.2
that for the generative XQuAD, most of the time
Table3: ThereisalwaysalargegaponXQuAD(exact nativeinstructiondatamaintainsahugeadvantage
match). overtheothertwotranslatedversions. Nonetheless,
forMGSMandM-MMLU,thedifferencebetween
usingtranslatedandnativedataisnotclearunder
that the pattern seems to be affected by the base
mostconditions. Thesealsoindicatethatthestabil-
modelandthetask. ItcanbeseenthatLlama2-7B
ity of our results on translated structured tasks is
andGemma-7Benjoyaperformanceleapintwo
notaffectedbythetwohyperparameters.
scenarios: 1) on TyDi QA with a lower learning
rate;and2)onCMMLUwithmultilingualinstruc-
tion tuning. In both cases, the performance gap
Open-endedquestionansweringwithtranslated
between native and translated data can be closed.
questions Finally, we conduct a smaller scale
However,forQwen1.5,whiletheresultschangeas
test on open-ended generation in Table 6 where
thetrainingconfigurationchanges,nativedatastill
monolingual and multilingual training are com-
isthebestdataconditiontogowith.
pared. Despite some fluctuations in results, the
Translated,structuredbenchmarks Movingon native-translated difference cannot be mitigated
to the translated test set results listed in Tables 3 when evaluated on open-ended generation with
to5,wefindthatourpreviousfindingsstillapply translated questions. This is consistent with pat-
evenwhenthelearningrateisloweredormultilin- ternsonXQuADthatgenerativebenchmarkscan
gual instruction tuning is applied. It can be seen moreeffectivelydifferentiatethedatasource.
7BaseModel Data Mono Multi 4.2 (Multilingual)LLMevaluation
native 171.5 121.7 Machinetranslationhasbeenusedtoextendmulti-
Llama2-7B cohere 126.3 126.3
plebenchmarkstomorelanguages(Conneauetal.,
google 125.7 131.0
2018;Artetxeetal.,2020b;Dumitrescuetal.,2021;
native 216.5 164.7
Gemma-7B cohere 150.0 146.3 Bandarkaretal.,2023),andmanypapersexploring
google 157.3 147.3 multilingualisminlargelanguagemodelsyielded
native 175.5 178.8 findingsbasedontranslatedinstructiondataand/or
Qwen1.5-4B cohere 117.3 129.3 translatedevaluationsets(Xueetal.,2020;Cañete
google 116.0 124.7
et al., 2023; Ahuja et al., 2023; Cui et al., 2023;
native 187.5 189.3
Puduppullyetal.,2023;Yangetal.,2023;Laietal.,
Qwen1.5-7B cohere 137.2 138.0
google 132.7 133.7 2023; Kew et al., 2023; Chen et al., 2024; Singh
native 204.2 210.2 etal.,2024;Liuetal.,2024;Shahametal.,2024,
Qwen1.5-14B cohere 152.7 145.7 inter alia). While these works have significantly
google 140.0 140.7
pushedtheboundaryofmultilingualisminLLMs,
weattempttorevisitthecurrentpractices.
Table 6: There is always a large gap on open-ended
Clarketal.(2020)discussedthedisadvantages
questionanswering(translated,GPT-4-Turbojudged).
ofusingtranslatedtestsbecausetheyincorporate
translationeseandrepresentthesourcelanguage’s
4 RelatedWork
knowledge; Artetxe et al. (2020a) revealed how
minortranslationartefactscansignificantlyimpact
4.1 Instructiontuningdataandpractices
evaluationoutcomes. Ithasbeenshownandargued
Instruction data can be created by writing ques-
that,albeitintuitively,translatedtrainingdataim-
tions and responses from scratch (Conover et al.,
proves scores on test data created via translation
2023; Singh et al., 2024), collecting user-system
(Singhetal.,2019;Artetxeetal.,2020a). Thema-
interactions(Köpfetal.,2023),ortemplatingstruc-
chinetranslationcommunityfoundthattranslated
tureddatainstancesintonaturaltexts(Mishraetal.,
testinput“canhaveadetrimentaleffectontheac-
2022;Sanhetal.,2022). Itisalsofeasibletodistil
curacyofevaluation”(Läublietal.,2020;Graham
large language models by feeding existing exam-
etal.,2020;Farhadetal.,2021). Thispaperdemon-
ples(Taorietal.,2023;Weietal.,2023). Stemming
stratesthatbyalteringthenatureoftheinstruction
fromEnglishdata,manymultilingualinstruction
orevaluationdata,evaluationcanleadtodifferent
datasets,especiallyopen-endedquestion-response
conclusionsforLLMinstructiontuning.
pairs,havebeencreatedbyincorporatingmachine
Our comparative analysis of native and trans-
translation(Muennighoffetal.,2023;Chenetal.,
lated data also relates to understanding the in-
2023a,b,2024;Chaietal.,2024;LaiandNissim,
tegrity of LLM evaluation and the representa-
2024). Slightlydifferently,Lietal.(2023a)trans-
tionoflanguage-specificknowledgefromameta-
lated the English Alpaca questions into multiple
evaluationperspective. Webelievethistobeacru-
languagesbutusedGPTtogenerateresponses,ar-
cialandtimelytopicinthecurrentLLMlandscape.
guingthatitcouldavoidtranslationese. Theseop-
Earlier,Lyuetal.(2024)examinedvariousmecha-
tionsaremoreaffordablethancreatinglanguage-
nismsofobtainingLLMresponses. Concurrently,
nativedatadirectly,buttheyarenotflawlesssince
Gema et al. (2024) found correctness issues in a
they can introduce model-generated content and
specificbenchmark;Etxanizetal.(2024)showed
knowledgemismatch.
thatmodelscanhavedistinctbehavioursonlocal
InrecentLLMinstructiontuningresearch, the
andglobalknowledge;Guetal.(2024)calledfor
“superficial alignment hypothesis” (Zhou et al.,
transparentevaluationinchoosingformattingand
2023) might alleviate such concerns, as it claims
configurations. Incomparison,ourworklooksat
thatastrongfoundationmodelmostlylearnsthere-
multilingualevaluationfromthedimensionofdata
sponsetemplatefrominstructiontuning—therefore
characteristics.
thetranslationartefactsorlanguage-specificknowl-
edgewouldnotbeoverlyconsumed(Ghoshetal.,
5 ConclusionandFutureWork
2024). To our knowledge, there is no prior work
thatsystematicallycomparednativeandtranslated This work systematically analysed the effects of
instructiondata. nativeandtranslateddataonboththeLLMinstruc-
8tiontuningandevaluationends. Thedifferencein (EP/W032244/1)andisoperatedbyAdvancedRe-
dataleadstoresultgapsonnativetestsetsandgen- searchComputingattheUniversityofBirmingham.
erative benchmarks. With careful regularization, It was also supported by the Edinburgh Interna-
translatedinstructiondatacancatchupwithnative tional Data Facility (EIDF) and the Data-Driven
dataonstructuredbenchmarksbutnotgenerative Innovation Programme at the University of Edin-
tasks. Givenourfindings,wewouldliketocallfor burgh.
prudent choices in multilingual LLM evaluation
benchmarks. While this work provides compre-
References
hensiveempiricalresultsandextrinsicevaluation,
futureworkcanconsiderinspectinginstructionand
KabirAhuja,RishavHada,MillicentOchieng,Prachi
evaluationdataintrinsically. Jain, Harshita Diddee, Krithika Ramesh, Samuel
Maina, Tanuja Ganu, Sameer Segal, Maxamed
Limitations Axmed, Kalika Bali, and Sunayana Sitaram. 2023.
Mega: Multilingual evaluation of generative AI.
As pointed out in our future work, this paper fo- arXivpreprint.
cusedonprovidingempiricalresultsasanextrinsic
MikelArtetxe,GorkaLabaka,andEnekoAgirre.2020a.
evaluationofdatacharacteristics. Itlacksanintrin-
Translationartifactsincross-lingualtransferlearning.
sicunderstandingofthedistinctionbetweennative
InProceedingsofthe2020ConferenceonEmpirical
andtranslateddata,e.g. theknowledgeorlanguage MethodsinNaturalLanguageProcessing.
features missing in the translated data and what
errorsthiscanleadto. Mikel Artetxe, Sebastian Ruder, and Dani Yogatama.
2020b. Onthecross-lingualtransferabilityofmono-
Also, our work centred around instruction tun-
lingualrepresentations. InProceedingsofthe58th
ing, but we have very limited knowledge of the AnnualMeetingoftheAssociationforComputational
pre-training data for the LLMs we study. This Linguistics.
workassumesthatthebasemodelsaredescribed
MonaBaker.1996. Corpus-basedTranslationStudies:
accuratelyandgenuinelybyrespectivemakersand
TheChallengesthatLieAhead. BenjaminsTransla-
the LLM pre-training data would not prevent us
tionLibrary.JohnBenjaminsPublishingCompany.
frommakingmeaningfulconclusions.
LucasBandarkar,DavisLiang,BenjaminMuller,Mikel
EthicalConsiderations Artetxe,SatyaNarayanShukla,DonaldHusa,Naman
Goyal,AbhinandanKrishnan,LukeZettlemoyer,and
We consider our work to have almost no ethical MadianKhabsa.2023. TheBelebelebenchmark: a
risks. Like most papers on LLMs, it is difficult parallel reading comprehension dataset in 122 lan-
guagevariants. arXivpreprint.
to make sure that the fine-tuned model is safe in
all cases, but our models are not intended for the
JoséLuisGonzálezCañete,GabrielChaperon,Rodrigo
public. In terms of LLM evaluation, we believe
Fuentes,Jou-HuiHo,HojinKang,andJorgeP’erez.
this paper makes a positive contribution towards 2023. Spanishpre-trainedBERTmodelandevalua-
trustworthyevaluation. tiondata. arXivpreprint.
Acknowledgments LinzhengChai,JianYang,TaoSun,HongchengGuo,
JiahengLiu,BingWang,XiannianLiang,JiaqiBai,
The work has received funding from the Euro- TongliangLi,QiyaoPeng,etal.2024. xCoT:Cross-
lingualinstructiontuningforcross-lingualchain-of-
pean Union’s Horizon Europe research and in-
thoughtreasoning. arXivpreprint.
novation programme under grant agreement No
101070350andfromUKResearchandInnovation NuoChen,ZinanZheng,NingWu,LinjunShou,Ming
(UKRI) under the UK government’s Horizon Eu- Gong, Yangqiu Song, Dongmei Zhang, and Jia Li.
2023a. Breaking language barriers in multilingual
ropefundingguarantee[grantnumbers10052546
mathematicalreasoning: Insightsandobservations.
and10039436].
arXivpreprint.
Computations described in this research were
supported by the Baskerville Tier 2 HPC service PinzhenChen,ShaoxiongJi,NikolayBogoychev,An-
(https://www.baskerville.ac.uk/). Baskervillewas dreyKutuzov,BarryHaddow,andKennethHeafield.
2024. Monolingualormultilingualinstructiontun-
funded by the EPSRC and UKRI through the
ing:WhichmakesabetterAlpaca. InFindingsofthe
World Class Labs scheme (EP/T022221/1) and Association for Computational Linguistics: EACL
the Digital Research Infrastructure programme 2024.
9ZhihongChen,ShuoYan,JuhaoLiang,FengJiang,Xi- Leo Gao, Jonathan Tow, Stella Biderman, Sid Black,
angbo Wu, FeiYu, GuimingHardy Chen, Junying AnthonyDiPofi,CharlesFoster,LaurenceGolding,
Chen,HongboZhang,LiJianquan,WanXiang,and Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
BenyouWang.2023b. MultilingualSIFT:Multilin- et al. 2023. A framework for few-shot language
gualSupervisedInstructionFine-tuning. GitHub. modelevaluation. Zenodo.
Nadezhda Chirkova and Vassilina Nikoulina. 2024. Martin Gellerstam. 1986. Translationese in Swedish
Zero-shotcross-lingualtransferininstructiontuning novelstranslatedfromEnglish. InTranslationstud-
oflargelanguagemodels. arXivpreprint. ies in Scandinavia: Proceedings from the Scandi-
navianSymposiumonTranslationTheoryII.CWK
JonathanH.Clark,EunsolChoi,MichaelCollins,Dan Gleerup.
Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and
Jennimaria Palomaki. 2020. TyDi QA: A bench- Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon
markforinformation-seekingquestionansweringin Hong,AlessioDevoto,AlbertoCarloMariaMancino,
typologicallydiverselanguages. Transactionsofthe Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du,
AssociationforComputationalLinguistics. MohammadRezaGhasemiMadani,etal.2024. Are
wedonewithMMLU? arXivpreprint.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
MarkChen,HeewooJun,LukaszKaiser,Matthias Gemma Team, Thomas Mesnard, Cassidy Hardin,
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro RobertDadashi,SuryaBhupatiraju,ShreyaPathak,
Nakano,etal.2021. Trainingverifierstosolvemath LaurentSifre,MorganeRivière,MihirSanjayKale,
wordproblems. arXivpreprint. Juliette Love, et al. 2024. Gemma: Open models
based on Gemini research and technology. arXiv
AlexisConneau,RutyRinott,GuillaumeLample,Adina preprint.
Williams, Samuel Bowman, Holger Schwenk, and
Veselin Stoyanov. 2018. XNLI: Evaluating cross- SreyanGhosh,ChandraKiranReddyEvuru,SonalKu-
lingualsentencerepresentations. InProceedingsof mar,DeepaliAneja,ZeyuJin,RamaniDuraiswami,
the2018ConferenceonEmpiricalMethodsinNatu- DineshManocha, etal.2024. Acloserlookatthe
ralLanguageProcessing. limitationsofinstructiontuning. arXivpreprint.
MikeConover,MattHayes,AnkitMathur,JianweiXie, Yvette Graham, Barry Haddow, and Philipp Koehn.
Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, 2020. Statistical power and translationese in ma-
MateiZaharia,andReynoldXin.2023. FreeDolly: chinetranslationevaluation. InProceedingsofthe
Introducingtheworld’sfirsttrulyopeninstruction- 2020ConferenceonEmpiricalMethodsinNatural
tunedLLM. OnlineBlog. LanguageProcessing.
YimingCui,ZiqingYang,andXinYao.2023. Efficient YulingGu,OyvindTafjord,BaileyKuehl,DanyHad-
andeffectivetextencodingforChineseLLaMAand dad, Jesse Dodge, and Hannaneh Hajishirzi. 2024.
Alpaca. arXivpreprint. Olmes: Astandardforlanguagemodelevaluations.
arXivpreprint.
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and
LukeZettlemoyer.2023. QLoRA:Efficientfinetun- Dan Hendrycks, Collin Burns, Steven Basart, Andy
ing of quantized LLMs. In Thirty-seventh Confer- Zou,MantasMazeika,DawnSong,andJacobStein-
enceonNeuralInformationProcessingSystems. hardt.2021. Measuringmassivemultitasklanguage
understanding. ProceedingsoftheInternationalCon-
StefanDanielDumitrescu,PetruRebeja,BeataLorincz, ferenceonLearningRepresentations.
MihaelaGaman,AndreiAvram,MihaiIlie,Andrei
Pruteanu, Adriana Stan, Lorena Rosia, Cristina Ia- Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
cobescu, etal.2021. Liro: Benchmarkandleader- Allen-Zhu,YuanzhiLi,SheanWang,LuWang,and
board for romanian language tasks. In Thirty-fifth WeizhuChen.2022. LoRA:Low-rankadaptationof
ConferenceonNeuralInformationProcessingSys- largelanguagemodels. InInternationalConference
temsDatasetsandBenchmarksTrack(Round1). onLearningRepresentations.
JulenEtxaniz,GorkaAzkune,AitorSoroa,OierLopez Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei
deLacalle,andMikelArtetxe.2024. BertaQA:How Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu,
muchdolanguagemodelsknowaboutlocalculture? Chuancheng Lv, Yikai Zhang, Yao Fu, et al. 2024.
arXivpreprint. C-Eval: Amulti-levelmulti-disciplineChineseeval-
uation suite for foundation models. Advances in
AkhbardehFarhad,ArkhangorodskyArkady,Biesialska NeuralInformationProcessingSystems.
Magdalena,BojarOndˇrej,ChatterjeeRajen,Chaud-
hary Vishrav, Marta R Costa-jussa, España-Bonet Hamish Ivison, Yizhong Wang, Valentina Pyatkin,
Cristina, Fan Angela, Federmann Christian, et al. Nathan Lambert, Matthew Peters, Pradeep Dasigi,
2021. Findingsofthe2021conferenceonmachine JoelJang,DavidWadden,NoahASmith,IzBeltagy,
translation (WMT21). In Proceedings of the Sixth etal.2023. Camelsinachangingclimate:Enhancing
ConferenceonMachineTranslation. LMadaptationwithTulu2. arXivpreprint.
10TannonKew,FlorianSchottmann,andRicoSennrich. LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,
2023. Turningenglish-centricLLMsintopolyglots: CarrollWainwright,PamelaMishkin,ChongZhang,
Howmuchmultilingualityisneeded? arXivpreprint. SandhiniAgarwal,KatarinaSlama,AlexRay,etal.
2022. Training languagemodelsto followinstruc-
Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, tionswithhumanfeedback. InAdvancesinNeural
Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, InformationProcessingSystems.
AbdullahBarhoum,NguyenMinhDuc,OliverStan-
ley, Richárd Nagyfi, et al. 2023. OpenAssistant Ratish Puduppully, Anoop Kunchukuttan, Raj Dabre,
conversations-democratizinglargelanguagemodel AiTiAw,andNancyChen.2023. DecoMT:Decom-
alignment. InThirty-seventhConferenceonNeural posed prompting for machine translation between
InformationProcessingSystemsDatasetsandBench- relatedlanguagesusinglargelanguagemodels. In
marksTrack. Proceedings of the 2023 Conference on Empirical
MethodsinNaturalLanguageProcessing.
HuiyuanLaiandMalvinaNissim.2024. mCoT:Multi-
lingualinstructiontuningforreasoningconsistency QwenTeam.2024. IntroducingQwen1.5. OnlineBlog.
inlanguagemodels. arXivpreprint.
PranavRajpurkar,JianZhang,KonstantinLopyrev,and
Viet Lai, Chien Nguyen, Nghia Ngo, Thuat Nguyen,
Percy Liang. 2016. SQuAD: 100,000+ questions
FranckDernoncourt,RyanRossi,andThienNguyen. formachinecomprehensionoftext. InProceedings
2023. Okapi: Instruction-tunedlargelanguagemod- of the 2016 Conference on Empirical Methods in
elsinmultiplelanguageswithreinforcementlearning NaturalLanguageProcessing.
fromhumanfeedback. InProceedingsofthe2023
Conference on Empirical Methods in Natural Lan-
VictorSanh,AlbertWebson,ColinRaffel,StephenH
guageProcessing: SystemDemonstrations.
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chaffin,ArnaudStiegler,TevenLeScao,ArunRaja,
SamuelLäubli,SheilaCastilho,GrahamNeubig,Rico
etal.2022. Multitaskpromptedtrainingenableszero-
Sennrich, Qinlan Shen, and Antonio Toral. 2020.
shottaskgeneralization. InInternationalConference
A set of recommendations for assessing human-
onLearningRepresentations.
machineparityinlanguagetranslation. Journalof
ArtificialIntelligenceResearch.
Uri Shaham, Jonathan Herzig, Roee Aharoni, Idan
Szpektor,ReutTsarfaty,andMatanEyal.2024. Mul-
HaonanLi,FajriKoto,MinghaoWu,AlhamFikriAji,
tilingualinstructiontuningwithjustapinchofmulti-
andTimothyBaldwin.2023a. Bactrian-X:Amulti-
linguality. arXivpreprint.
lingualreplicableinstruction-followingmodelwith
low-rankadaptation. arXivpreprint.
FredaShi,MiracSuzgun,MarkusFreitag,XuezhiWang,
SurajSrivats,SoroushVosoughi,HyungWonChung,
HaonanLi,YixuanZhang,FajriKoto,YifeiYang,Hai
Yi Tay, Sebastian Ruder, Denny Zhou, et al. 2023.
Zhao, Yeyun Gong, Nan Duan, and Tim Baldwin.
Languagemodelsaremultilingualchain-of-thought
2023b. CMMLU:Measuringmassivemultitasklan-
reasoners. InTheEleventhInternationalConference
guageunderstandinginChinese. arXivpreprint.
onLearningRepresentations.
ChaoqunLiu,WenxuanZhang,YiranZhao,AnhTuan
JasdeepSingh,BryanMcCann,NitishShirishKeskar,
Luu,andLidongBing.2024. Istranslationallyou
CaimingXiong,andRichardSocher.2019. XLDA:
need? a study on solving multilingual tasks with
largelanguagemodels. arXivpreprint. Cross-lingualdataaugmentationfornaturallanguage
inferenceandquestionanswering. arXivpreprint.
Chenyang Lyu, Minghao Wu, and Alham Fikri Aji.
2024. Beyondprobabilities: Unveilingthemisalign- Shivalika Singh, Freddie Vargus, Daniel Dsouza,
ment in evaluating large language models. arXiv Börje F Karlsson, Abinaya Mahendiran, Wei-Yin
preprint. Ko,HerumbShandilya,JayPatel,DeividasMataci-
unas,LauraOMahony,etal.2024. Ayadataset: An
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and open-access collection for multilingual instruction
Hannaneh Hajishirzi. 2022. Cross-task generaliza- tuning. arXivpreprint.
tionvianaturallanguagecrowdsourcinginstructions.
In Proceedings of the 60th Annual Meeting of the Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
AssociationforComputationalLinguistics. Dubois,XuechenLi,CarlosGuestrin,PercyLiang,
andTatsunoriB.Hashimoto.2023. StanfordAlpaca:
NiklasMuennighoff,ThomasWang,LintangSutawika, Aninstruction-followingLLaMAmodel. GitHub.
Adam Roberts, Stella Biderman, Teven Le Scao,
MSaifulBari,ShengShen,Zheng-XinYong,Hailey Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
Schoelkopf,etal.2023. Crosslingualgeneralization bert, Amjad Almahairi, Yasmine Babaei, Nikolay
throughmultitaskfinetuning. InProceedingsofthe Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
61stAnnualMeetingoftheAssociationforComputa- Bhosale,etal.2023. Llama2: Openfoundationand
tionalLinguistics. fine-tunedchatmodels. arXivpreprint.
11JasonWei,MaartenBosma,VincentZhao,KelvinGuu, scoring. Theinstructionandresponsevariablesare
Adams Wei Yu, Brian Lester, Nan Du, Andrew M. replacedbytheirstringvaluesduringprompting.
Dai,andQuocVLe.2022. Finetunedlanguagemod-
elsarezero-shotlearners. InInternationalConfer-
Please act as an impartial judge and evaluate
enceonLearningRepresentations.
the quality of a response to a user instruction
displayed below. Your evaluation should
XiangpengWei,HaoranWei,HuanLin,TianhaoLi,Pei
consider factors such as helpfulness, relevance,
Zhang,XingzhangRen,MeiLi,YuWan,ZhiweiCao, accuracy, depth, creativity, and level of detail.
Binbin Xie, et al. 2023. PolyLM: An open source Begin your evaluation with a brief explanation.
polyglotlargelanguagemodel. arXivpreprint. After that, please rate the response on a scale
of 1 to 5 by strictly following this format:
LintingXue,NoahConstant,AdamRoberts,MihirKale, “[[rating]]”. The rating must be enclosed by two
RamiAl-Rfou,AdityaSiddhant,AdityaBarua,and square brackets, for example: “Rating: [[2]]”.
ColinRaffel.2020. mt5: Amassivelymultilingual
pre-trainedtext-to-texttransformer. InNorthAmer- [User Instruction]
icanChapteroftheAssociationforComputational ${instruction}
Linguistics.
[Response]
${response}
Wen Yang, Chong Li, Jiajun Zhang, and Chengqing
Zong. 2023. BigTranslate: Augmenting large lan-
guagemodelswithmultilingualtranslationcapability Figure 6: Prompt template for requesting a model re-
over100languages. arXivpreprint. sponseevaluationfromGPT-4-TurboorCommand-R+.
LianminZheng,Wei-LinChiang,YingSheng,Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024.
JudgingLLM-as-a-judgewithMT-benchandChat-
botArena. AdvancesinNeuralInformationProcess-
ingSystems.
ChuntingZhou,PengfeiLiu,PuxinXu,SriniIyer,Jiao
Sun,YuningMao,XuezheMa,AviaEfrat,PingYu,
LILIYU,SusanZhang,GargiGhosh,MikeLewis,
Luke Zettlemoyer, and Omer Levy. 2023. LIMA:
Lessismoreforalignment. InThirty-seventhCon-
ferenceonNeuralInformationProcessingSystems.
A Prompts
A.1 CommandRtranslationprompt
WelistthetranslationpromptweusetoqueryCom-
mandRinFigure5,whichaskstheLLMtotrans-
late a given text while preserving the formatting.
Thesourcelanguage,targetlanguage,andtextvari-
ables are replaced by their string values during
prompting.
Please translate from ${source_lang} to
${target_lang}. Do your best to preserve the
formatting. The following content should and
should only be translated.
${text}
Figure5: Prompttemplateforrequestingatranslation
fromCommandR.
A.2 LLM-as-a-judgeprompt
WelisttheLLM-as-a-judgepromptweusetoquery
GPT-4-TurboandCommand-R+inFigure6,which
requiresthejudgetogiveabriefexplanationbefore
12B ComprehensiveResults
Due to the space constraint, we can only list the
resultbreakdownforallmodelsandalllanguages
onvariousbenchmarksinthisappendix. Theseare:
• Table7: TyDiQARussian,F1score
• Table8: CMMLU,accuracy
• Table9: XQuAD,1e−4,exactmatch
• Table10: XQuAD,1e−6,exactmatch
• Table11: XQuAD,1e−4,“include”
• Table12: XQuAD,1e−6,“include”
• Table13: MGSM,1e−4,exacttokenmatch
• Table14: MGSM,1e−6,exacttokenmatch
• Table15: M-MMLU,1e−4,accuracy
• Table16: M-MMLU,1e−6,accuracy
• Table17: translatedquestions,GPT-4judge
• Table18: translatedquestions,CmdR+judge
• Table19: nativequestions,GPT-4judge
• Table20: nativequestions,CmdR+judge
131e−4 1e−6
BaseModel Data
Mono Multi Mono Multi
native 28.3 25.1 33.4 33.4
Llama2-7B cohere 28.8 23.4 33.3 33.4
google 25.5 22.9 33.3 33.4
native 28.5 24.9 27.7 27.6
Gemma-2B cohere 28.2 28.8 27.7 27.8
google 28.3 28.8 28.0 27.5
native 33.6 31.5 37.7 38.1
Gemma-7B cohere 34.4 31.4 38.1 39.0
google 30.7 30.9 37.9 38.7
native 22.5 23.9 16.6 17.1
Qwen1.5-0.5B cohere 19.8 20.0 16.9 17.7
google 17.5 19.8 16.8 17.1
native 20.1 27.6 19.5 19.7
Qwen1.5-1.8B cohere 20.2 18.1 19.7 19.6
google 18.1 19.3 19.8 19.8
native 21.9 28.3 18.1 18.2
Qwen1.5-4B cohere 20.0 22.5 17.9 18.2
google 20.1 22.5 17.9 18.2
native 37.0 37.0 22.4 23.8
Qwen1.5-7B cohere 34.2 33.0 22.9 23.9
google 27.2 27.1 22.7 23.9
native 34.4 32.8 24.8 25.1
Qwen1.5-14B cohere 33.0 29.3 24.6 25.1
google 32.1 35.2 24.9 25.9
Table7: AllmodelresultsonTyDiQARussian(F1,%).
1e−4 1e−6
BaseModel Data
Mono Multi Mono Multi
native 32.7 32.6 31.8 31.9
Llama2-7B cohere 30.2 32.7 32.0 31.6
google 31.2 32.1 32.0 31.8
native 31.8 31.5 31.2 31.0
Gemma-2B cohere 29.4 31.2 31.2 31.0
google 30.7 31.8 31.3 31.2
native 48.7 50.1 49.9 49.7
Gemma-7B cohere 48.3 50.4 49.7 49.9
google 46.4 50.7 49.8 50.1
native 44.1 43.9 42.2 42.1
Qwen1.5-0.5B cohere 41.6 42.3 42.1 42.2
google 42.8 42.7 42.0 42.2
native 55.9 56.6 56.7 56.7
Qwen1.5-1.8B cohere 53.8 56.6 56.6 56.4
google 52.9 57.0 56.9 56.6
native 65.3 66.4 66.0 65.8
Qwen1.5-4B cohere 59.3 65.2 65.7 65.7
google 59.8 65.2 66.0 65.8
native 72.3 72.6 72.0 71.9
Qwen1.5-7B cohere 68.4 71.4 71.9 71.8
google 67.6 71.4 71.9 72.0
native 78.2 78.2 77.7 77.6
Qwen1.5-14B cohere 76.2 77.6 77.8 77.7
google 75.8 77.2 77.8 77.7
Table8: AllmodelresultsonCMMLU(accuracy,%).
14Monolingual Multilingual
Basemodel Data
es ru zh average es ru zh average
native 30.7 18.7 41.3 30.3 28.0 21.8 43.0 31.0
Llama2-7B cohere 17.2 15.3 29.8 20.8 18.4 7.6 38.8 21.6
google 20.8 14.3 28.2 21.1 22.9 10.3 39.2 24.1
native 11.3 5.9 15.0 10.7 11.1 6.0 14.1 10.4
Gemma-2B cohere 10.8 6.0 11.6 9.5 11.2 5.0 9.2 8.5
google 10.4 5.8 13.4 9.9 9.9 4.8 5.4 6.7
native 12.4 10.3 29.4 17.4 13.0 10.8 26.7 16.8
Gemma-7B cohere 12.7 10.1 21.5 14.8 12.9 9.0 27.1 16.3
google 12.4 8.8 22.3 14.5 13.2 8.5 24.1 15.3
native 26.5 7.6 20.6 18.2 18.1 10.2 21.7 16.6
Qwen1.5-0.5B cohere 9.8 6.6 16.1 10.8 11.3 7.4 15.5 11.4
google 10.8 6.5 17.2 11.5 12.7 7.7 16.1 12.2
native 28.7 10.9 23.3 21.0 28.7 20.7 31.9 27.1
Qwen1.5-1.8B cohere 11.7 5.9 14.7 10.8 14.3 5.5 21.9 13.9
google 10.3 5.8 11.6 9.2 15.9 6.3 25.5 15.9
native 33.1 24.9 37.7 31.9 36.5 31.4 52.6 40.2
Qwen1.5-4B cohere 29.6 19.3 25.4 24.8 31.9 18.4 39.4 29.9
google 19.7 18.7 23.9 20.8 35.1 20.3 40.2 31.8
native 43.9 30.4 30.3 34.9 39.9 35.4 52.5 42.6
Qwen1.5-7B cohere 27.6 25.5 21.6 24.9 36.3 22.9 35.4 31.5
google 23.6 21.0 21.5 22.0 32.1 19.9 31.0 27.7
native 44.7 34.2 30.7 36.5 49.7 38.7 48.3 45.6
Qwen1.5-14B cohere 35.3 28.4 21.9 28.5 39.9 24.0 28.6 30.8
google 28.3 26.8 24.2 26.4 42.1 26.6 27.7 32.2
Table9: AllmodelandalllanguageresultsonXQuAD(1e−4,exactmatch,%).
Monolingual Multilingual
Basemodel Data
es ru zh average es ru zh average
native 12.9 7.3 35.3 18.5 12.4 7.1 31.9 17.1
Llama2-7B cohere 12.9 7.1 33.9 18.0 12.4 7.3 32.3 17.3
google 12.9 7.1 33.3 17.8 12.4 7.2 32.2 17.3
native 10.0 6.1 4.8 7.0 10.0 6.2 4.9 7.0
Gemma-2B cohere 10.0 5.9 4.6 6.8 9.9 6.1 4.8 6.9
google 10.0 6.1 4.7 6.9 10.0 6.0 4.9 6.9
native 12.5 9.1 31.7 17.8 12.5 9.1 31.2 17.6
Gemma-7B cohere 12.4 9.1 30.6 17.3 12.4 9.3 30.3 17.3
google 12.3 9.1 30.3 17.2 12.9 9.1 30.3 17.4
native 12.1 6.2 12.8 10.4 10.5 6.5 11.2 9.4
Qwen1.5-0.5B cohere 10.8 6.2 11.8 9.6 10.2 6.5 10.6 9.1
google 11.1 6.2 12.0 9.8 9.9 6.2 11.0 9.0
native 15.6 5.1 20.4 13.7 14.1 5.1 16.0 11.7
Qwen1.5-1.8B cohere 15.0 5.2 18.7 13.0 14.1 5.0 16.3 11.8
google 14.9 5.0 18.2 12.7 14.0 5.0 16.5 11.8
native 24.4 14.3 44.9 27.8 21.0 14.9 37.8 24.6
Qwen1.5-4B cohere 23.3 14.2 43.4 26.9 20.9 14.7 37.6 24.4
google 22.7 14.3 43.5 26.8 20.8 15.0 37.6 24.5
native 31.3 19.0 41.8 30.7 23.4 19.9 33.2 25.5
Qwen1.5-7B cohere 32.8 18.8 39.0 30.2 23.2 19.4 33.3 25.3
google 31.9 19.1 38.7 29.9 22.8 19.6 32.9 25.1
native 37.7 26.9 35.7 33.4 37.2 27.6 28.1 31.0
Qwen1.5-14B cohere 38.3 27.1 35.3 33.6 36.7 27.5 27.3 30.5
google 37.6 27.0 35.9 33.5 36.8 27.3 27.4 30.5
Table10: AllmodelandalllanguageresultsonXQuAD(1e−6,exactmatch,%).
15Monolingual Multilingual
Basemodel Data
es ru zh average es ru zh average
native 51.7 31.8 66.0 49.8 55.6 30.8 61.9 49.4
Llama2-7B cohere 23.5 26.3 43.4 31.1 30.3 16.9 55.3 34.2
google 43.7 27.1 43.8 38.2 46.1 28.4 58.7 44.4
native 17.2 18.8 50.3 28.8 19.6 13.9 44.7 26.1
Gemma-2B cohere 26.3 20.3 36.6 27.7 36.1 17.8 47.1 33.7
google 33.7 22.6 39.7 32.0 39.2 22.8 52.8 38.3
native 14.7 24.7 61.8 33.7 15.7 17.5 67.2 33.5
Gemma-7B cohere 13.9 20.3 37.5 23.9 13.4 12.3 44.6 23.4
google 16.5 29.0 48.5 31.3 23.7 19.3 48.2 30.4
native 35.7 19.7 54.5 36.6 29.7 20.4 46.1 32.1
Qwen1.5-0.5B cohere 19.7 23.8 33.3 25.6 25.5 16.7 45.2 29.1
google 26.9 24.5 37.9 29.8 28.8 16.9 46.2 30.6
native 44.9 25.8 64.4 45.0 45.0 30.6 56.4 44.0
Qwen1.5-1.8B cohere 24.0 20.8 40.6 28.5 34.2 20.0 56.2 36.8
google 35.5 19.5 46.8 33.9 35.4 23.2 59.1 39.2
native 57.8 35.5 69.0 54.1 57.2 41.8 72.3 57.1
Qwen1.5-4B cohere 40.7 34.4 49.7 41.6 51.6 28.1 63.9 47.8
google 38.9 33.4 58.5 43.6 51.9 31.5 67.5 50.3
native 58.6 44.0 68.4 57.0 65.0 45.5 72.6 61.0
Qwen1.5-7B cohere 48.9 33.5 44.6 42.4 43.5 31.7 61.6 45.6
google 44.2 37.4 54.7 45.4 44.9 33.9 62.5 47.1
native 61.1 43.5 69.6 58.1 65.0 51.0 69.7 61.9
Qwen1.5-14B cohere 48.3 35.6 43.3 42.4 43.1 30.1 56.7 43.3
google 48.9 34.0 55.2 46.1 48.5 30.8 57.8 45.7
Table11: AllmodelandalllanguageresultsonXQuAD(1e−4,“include”,%).
Monolingual Multilingual
Basemodel Data
es ru zh average es ru zh average
native 15.3 11.7 53.0 26.7 14.2 11.8 52.6 26.2
Llama2-7B cohere 15.0 11.6 53.4 26.7 14.0 11.8 53.2 26.4
google 15.0 11.6 53.1 26.6 14.3 11.8 52.7 26.2
native 37.6 19.2 45.5 34.1 36.5 19.3 45.2 33.7
Gemma-2B cohere 37.8 19.7 45.4 34.3 36.8 19.2 45.0 33.7
google 38.4 19.6 45.0 34.3 36.5 19.2 45.5 33.7
native 14.2 11.8 50.7 25.6 15.9 11.6 52.4 26.6
Gemma-7B cohere 13.5 11.0 45.4 23.3 16.0 10.3 51.3 25.9
google 12.4 11.3 45.7 23.1 16.6 10.4 50.9 26.0
native 41.1 31.1 53.9 42.0 43.9 30.8 55.4 43.4
Qwen1.5-0.5B cohere 43.2 32.2 54.9 43.4 44.5 31.9 55.7 44.1
google 42.8 31.8 55.5 43.3 44.1 31.6 55.7 43.8
native 39.4 24.4 59.5 41.1 42.4 24.4 60.4 42.4
Qwen1.5-1.8B cohere 41.1 24.8 59.8 41.9 42.6 24.3 61.0 42.6
google 40.6 24.4 59.7 41.6 42.2 24.5 60.8 42.5
native 59.4 37.7 70.5 55.9 62.1 37.3 71.0 56.8
Qwen1.5-4B cohere 59.7 37.2 70.7 55.9 62.4 37.4 71.3 57.0
google 60.3 36.6 70.6 55.8 62.4 37.8 70.8 57.0
native 63.4 44.9 70.6 59.6 64.9 43.8 72.9 60.5
Qwen1.5-7B cohere 62.4 45.2 70.3 59.3 65.8 44.0 73.6 61.1
google 63.4 45.1 70.3 59.6 65.2 44.5 73.0 60.9
native 57.1 43.4 70.8 57.1 59.7 43.3 72.6 58.5
Qwen1.5-14B cohere 56.1 43.8 70.3 56.7 59.8 43.4 72.9 58.7
google 56.6 43.4 70.2 56.8 59.0 43.1 72.4 58.2
Table12: AllmodelandalllanguageresultsonXQuAD(1e−6,“include”,%).
16Monolingual Multilingual
Basemodel Data
es ru zh average es ru zh average
native 8.4 10.0 9.6 9.3 12.0 10.4 10.0 10.8
Llama2-7B cohere 5.2 11.2 10.8 9.1 10.0 12.4 10.0 10.8
google 8.4 9.2 6.8 8.1 12.0 10.8 13.2 12.0
native 11.6 12.8 8.8 11.1 13.2 10.8 9.2 11.1
Gemma-2B cohere 11.6 12.4 15.6 13.2 11.6 10.8 10.8 11.1
google 12.8 11.2 8.8 10.9 10.0 9.2 11.2 10.1
native 30.0 48.8 18.8 32.5 36.4 47.2 27.6 37.1
Gemma-7B cohere 34.4 46.8 19.6 33.6 36.4 44.0 31.2 37.2
google 30.0 48.8 31.6 36.8 37.6 42.8 28.8 36.4
native 2.8 2.0 4.8 3.2 2.0 1.6 10.4 4.7
Qwen1.5-0.5B cohere 1.6 2.4 8.0 4.0 3.6 2.4 8.8 4.9
google 2.0 2.0 9.2 4.4 2.4 3.6 7.6 4.5
native 6.0 6.4 15.6 9.3 9.6 6.0 19.6 11.7
Qwen1.5-1.8B cohere 8.4 5.6 14.8 9.6 6.8 7.6 15.6 10.0
google 6.4 5.6 14.8 8.9 6.0 5.2 21.2 10.8
native 18.0 24.0 34.4 25.5 22.0 26.0 40.4 29.5
Qwen1.5-4B cohere 20.0 21.6 16.0 19.2 21.6 24.8 42.0 29.5
google 17.6 22.4 35.6 25.2 21.2 21.6 38.0 26.9
native 40.4 40.8 41.6 40.9 34.8 40.0 48.8 41.2
Qwen1.5-7B cohere 37.2 40.4 36.8 38.1 37.2 39.6 45.6 40.8
google 42.4 40.8 39.2 40.8 42.8 42.4 48.4 44.5
native 44.8 60.0 53.6 52.8 49.2 59.6 56.8 55.2
Qwen1.5-14B cohere 32.8 63.6 50.8 49.1 49.2 59.6 51.6 53.5
google 42.4 60.0 54.0 52.1 50.4 63.2 55.6 56.4
Table13: AllmodelandalllanguageresultsonMGSM(1e−4,exacttokenmatch,%).
Monolingual Multilingual
Basemodel Data
es ru zh average es ru zh average
native 9.6 10.0 8.8 9.5 10.0 9.6 8.8 9.5
Llama2-7B cohere 10.4 11.2 8.0 9.9 10.0 10.4 9.2 9.9
google 9.6 9.6 10.0 9.7 10.4 9.6 9.2 9.7
native 13.2 10.8 10.4 11.5 12.4 11.2 12.4 12.0
Gemma-2B cohere 12.4 10.4 12.0 11.6 12.4 11.6 12.8 12.3
google 13.6 11.6 13.2 12.8 14.4 11.6 11.6 12.5
native 34.8 44.8 37.2 38.9 36.8 46.4 36.4 39.9
Gemma-7B cohere 35.6 44.4 36.4 38.8 34.8 45.2 38.0 39.3
google 35.2 46.8 36.4 39.5 37.2 44.0 36.8 39.3
native 2.4 2.8 8.8 4.7 2.4 2.4 8.4 4.4
Qwen1.5-0.5B cohere 2.8 2.0 10.8 5.2 3.2 2.4 8.8 4.8
google 2.8 1.6 6.8 3.7 2.4 2.4 8.8 4.5
native 7.2 6.0 20.8 11.3 7.2 6.8 24.4 12.8
Qwen1.5-1.8B cohere 8.0 6.8 20.4 11.7 5.6 6.0 23.2 11.6
google 6.0 5.6 20.8 10.8 6.8 6.0 24.4 12.4
native 21.6 28.0 40.0 29.9 20.8 28.0 40.0 29.6
Qwen1.5-4B cohere 21.2 27.6 40.0 29.6 22.0 28.8 38.8 29.9
google 22.8 28.8 41.6 31.1 21.2 28.0 38.0 29.1
native 38.0 41.6 46.8 42.1 35.2 42.4 53.2 43.6
Qwen1.5-7B cohere 36.0 41.2 47.2 41.5 34.8 40.0 48.8 41.2
google 38.4 43.2 48.4 43.3 34.0 43.6 50.8 42.8
native 46.0 63.6 58.0 55.9 47.2 62.8 58.0 56.0
Qwen1.5-14B cohere 47.6 61.6 58.0 55.7 46.8 63.6 57.6 56.0
google 46.8 62.4 58.0 55.7 48.0 61.6 57.6 55.7
Table14: AllmodelandalllanguageresultsonMGSM(1e−6,exacttokenmatch,%).
17Monolingual Multilingual
Basemodel Data
es ru zh average es ru zh average
native 38.4 33.9 34.4 35.6 39.6 35.3 33.9 36.3
Llama2-7B cohere 38.0 34.6 27.6 33.4 37.8 31.6 32.9 34.1
google 36.4 34.4 30.4 33.7 37.6 31.9 33.5 34.3
native 33.3 30.8 30.5 31.5 32.4 30.4 30.7 31.2
Gemma-2B cohere 30.8 30.3 29.6 30.2 33.7 31.9 32.8 32.8
google 31.8 30.0 32.0 31.3 34.0 31.0 32.8 32.6
native 55.9 53.0 48.7 52.5 57.3 53.0 50.7 53.7
Gemma-7B cohere 58.4 53.8 50.8 54.4 58.7 55.3 52.9 55.6
google 56.1 53.6 49.7 53.1 58.8 55.5 52.5 55.6
native 30.2 27.1 35.3 30.9 29.4 26.5 35.2 30.4
Qwen1.5-0.5B cohere 30.1 26.5 35.5 30.7 28.7 26.8 34.8 30.1
google 32.4 26.2 36.9 31.8 29.5 26.6 34.6 30.2
native 34.0 33.5 40.8 36.1 36.0 32.9 42.1 37.0
Qwen1.5-1.8B cohere 35.4 32.2 40.9 36.2 36.3 32.2 42.0 36.8
google 36.7 31.9 39.9 36.2 36.8 32.8 42.0 37.2
native 40.9 38.2 49.3 42.8 45.5 38.7 49.6 44.6
Qwen1.5-4B cohere 39.9 39.4 44.5 41.3 43.9 37.8 49.4 43.7
google 39.6 38.9 44.2 40.9 43.3 36.2 49.0 42.8
native 50.3 47.2 52.9 50.2 51.0 46.7 54.0 50.6
Qwen1.5-7B cohere 49.6 46.7 52.6 49.6 52.0 47.3 54.3 51.2
google 50.4 47.2 51.8 49.8 51.9 46.7 54.0 50.9
native 58.1 55.4 61.3 58.3 58.6 54.9 61.5 58.3
Qwen1.5-14B cohere 55.8 55.2 57.9 56.3 59.3 55.3 61.1 58.5
google 54.9 55.5 58.0 56.1 59.1 55.2 60.4 58.2
Table15: AllmodelandalllanguageresultsonM-MMLU(1e−4,accuracy,%).
Monolingual Multilingual
Basemodel Data
es ru zh average es ru zh average
native 39.1 34.5 33.9 35.8 38.9 34.7 33.7 35.8
Llama2-7B cohere 39.1 34.5 33.7 35.8 39.0 34.5 33.5 35.7
google 39.1 34.5 33.9 35.8 38.9 34.3 33.6 35.6
native 31.8 30.8 31.7 31.4 31.7 30.6 31.6 31.3
Gemma-2B cohere 31.8 30.8 31.7 31.4 32.0 30.6 31.6 31.4
google 32.0 30.8 31.7 31.5 31.8 30.7 31.7 31.4
native 56.5 53.6 51.0 53.7 56.7 53.7 51.5 54.0
Gemma-7B cohere 56.5 53.6 51.2 53.8 56.8 54.1 51.3 54.1
google 56.2 53.9 51.7 54.0 57.3 54.0 51.2 54.1
native 28.0 25.7 34.8 29.5 27.9 25.8 34.6 29.4
Qwen1.5-0.5B cohere 27.9 25.9 34.5 29.5 28.2 25.9 34.5 29.5
google 28.0 25.9 34.5 29.5 28.1 25.8 34.6 29.5
native 36.1 31.8 41.4 36.4 35.9 31.8 41.6 36.5
Qwen1.5-1.8B cohere 36.0 31.7 41.5 36.4 36.0 31.7 41.5 36.4
google 36.2 31.8 41.3 36.4 36.1 31.7 41.3 36.4
native 45.1 39.0 49.3 44.5 44.9 39.0 49.4 44.4
Qwen1.5-4B cohere 45.0 38.9 49.3 44.4 44.7 39.1 49.3 44.4
google 45.1 38.9 49.4 44.5 44.8 38.8 49.4 44.3
native 51.3 46.4 53.3 50.3 51.0 46.3 53.1 50.1
Qwen1.5-7B cohere 51.2 46.4 53.1 50.2 51.1 46.4 53.2 50.2
google 51.2 46.3 52.9 50.1 51.0 46.2 53.2 50.2
native 58.7 55.1 60.8 58.2 58.6 55.2 61.0 58.3
Qwen1.5-14B cohere 58.7 55.1 61.1 58.3 58.6 55.1 61.0 58.2
google 58.7 55.1 61.0 58.3 58.7 55.1 61.1 58.3
Table16: AllmodelandalllanguageresultsonM-MMLU(1e−6,accuracy,%).
18Monolingual Multilingual
Basemodel Data
es ru zh average es ru zh average
native 203.0 180.0 131.5 171.5 134.0 129.0 102.0 121.7
Llama2-7B cohere 144.0 131.0 104.0 126.3 134.0 130.0 115.0 126.3
google 140.0 122.0 115.0 125.7 148.0 125.0 120.0 131.0
native 161.5 151.0 125.0 145.8 122.0 112.0 109.0 114.3
Gemma-2B cohere 125.0 110.0 115.0 116.7 155.5 138.0 115.0 136.2
google 110.0 118.0 119.0 115.7 126.0 116.0 130.0 124.0
native 224.5 230.0 195.0 216.5 172.0 161.0 161.0 164.7
Gemma-7B cohere 146.0 156.0 148.0 150.0 146.0 144.0 149.0 146.3
google 168.0 157.0 147.0 157.3 151.0 144.0 147.0 147.3
native 89.0 76.0 141.0 102.0 93.5 77.0 133.0 101.2
Qwen1.5-0.5B cohere 70.0 60.0 99.0 76.3 78.0 61.0 107.0 82.0
google 75.0 61.0 109.0 81.7 72.0 58.0 88.0 72.7
native 119.5 112.0 148.0 126.5 105.0 104.0 162.0 123.7
Qwen1.5-1.8B cohere 88.0 88.0 126.0 100.7 86.0 87.0 123.0 98.7
google 87.0 80.0 108.0 91.7 101.5 82.0 115.0 99.5
native 187.0 149.5 190.0 175.5 186.5 151.0 199.0 178.8
Qwen1.5-4B cohere 107.0 108.0 137.0 117.3 123.0 109.0 156.0 129.3
google 113.0 116.0 119.0 116.0 121.0 108.0 145.0 124.7
native 196.0 180.5 186.0 187.5 188.0 178.0 202.0 189.3
Qwen1.5-7B cohere 150.5 118.0 143.0 137.2 139.0 116.0 159.0 138.0
google 134.0 121.0 143.0 132.7 132.0 111.0 158.0 133.7
native 204.0 205.5 203.0 204.2 205.0 209.5 216.0 210.2
Qwen1.5-14B cohere 142.0 151.0 165.0 152.7 150.0 129.0 158.0 145.7
google 137.0 132.0 151.0 140.0 141.0 134.0 147.0 140.7
Table17: Allmodelandalllanguageresultsonopen-endedtranslatedquestions(GPT-4-Turbojudged).
Monolingual Multilingual
Basemodel Data
es ru zh average es ru zh average
native 184.0 167.0 129.0 160.0 180.0 168.0 138.0 162.0
Llama2-7B cohere 174.5 162.0 147.0 161.2 183.0 166.0 150.0 166.3
google 179.0 165.0 148.0 164.0 178.0 162.0 137.0 159.0
native 168.0 155.0 126.0 149.7 170.0 150.0 149.0 156.3
Gemma-2B cohere 166.0 154.0 152.0 157.3 166.0 160.0 151.0 159.0
google 162.0 159.0 144.0 155.0 168.0 153.0 157.0 159.3
native 194.0 190.0 152.0 178.7 190.0 181.0 166.0 179.0
Gemma-7B cohere 171.0 174.0 168.0 171.0 178.0 161.0 161.0 166.7
google 186.0 172.0 170.0 176.0 181.0 172.0 164.0 172.3
native 131.0 96.0 131.0 119.3 121.0 99.0 138.0 119.3
Qwen1.5-0.5B cohere 117.0 98.0 143.0 119.3 126.0 98.0 145.0 123.0
google 133.0 104.0 133.0 123.3 127.0 102.0 129.0 119.3
native 143.0 131.0 140.0 138.0 133.0 115.0 135.0 127.7
Qwen1.5-1.8B cohere 147.0 128.0 152.0 142.3 145.0 121.0 159.0 141.7
google 163.0 125.0 145.0 144.3 148.0 124.0 152.0 141.3
native 179.0 160.0 155.0 164.7 169.0 149.0 170.0 162.7
Qwen1.5-4B cohere 156.0 156.0 165.0 159.0 171.0 153.0 167.0 163.7
google 168.0 155.0 151.0 158.0 157.0 147.0 158.0 154.0
native 181.0 158.0 144.0 161.0 177.0 156.0 166.0 166.3
Qwen1.5-7B cohere 178.0 159.0 154.0 163.7 183.0 148.0 160.0 163.7
google 174.0 153.0 161.0 162.7 172.0 141.0 168.0 160.3
native 183.0 172.0 156.0 170.3 182.0 173.0 169.0 174.7
Qwen1.5-14B cohere 172.0 163.0 149.0 161.3 179.0 158.0 155.0 164.0
google 172.0 159.0 155.0 162.0 172.0 155.0 160.0 162.3
Table18: Allmodelandalllanguageresultsonopen-endedtranslatedquestions(CommandR+judged).
19Monolingual Multilingual
Basemodel Data
es ru zh average es ru zh average
native 195.0 172.5 140.0 169.2 175.0 181.0 140.0 165.3
Llama2-7B cohere 173.0 172.0 149.0 164.7 158.0 152.0 135.0 148.3
google 176.5 163.5 150.5 163.5 173.0 164.0 161.0 166.0
native 156.0 129.5 130.0 138.5 174.0 149.0 136.5 153.2
Gemma-2B cohere 175.0 144.0 160.0 159.7 160.0 152.5 155.0 155.8
google 157.0 129.5 140.0 142.2 166.0 148.5 141.0 151.8
native 220.0 222.0 177.0 206.3 208.0 199.0 180.0 195.7
Gemma-7B cohere 197.0 209.5 207.0 204.5 206.0 175.5 194.0 191.8
google 204.0 190.5 200.0 198.2 187.5 198.0 211.0 198.8
native 88.0 88.0 157.0 111.0 91.0 76.0 162.0 109.7
Qwen1.5-0.5B cohere 91.5 91.0 145.5 109.3 78.0 70.0 156.0 101.3
google 93.0 66.0 142.0 100.3 91.0 72.0 157.0 106.7
native 121.0 107.0 174.0 134.0 118.0 88.0 156.0 120.7
Qwen1.5-1.8B cohere 116.0 102.0 177.0 131.7 121.0 108.0 193.5 140.8
google 114.0 96.0 153.0 121.0 136.5 101.0 196.5 144.7
native 162.0 126.0 186.0 158.0 163.0 129.0 208.0 166.7
Qwen1.5-4B cohere 152.0 141.0 173.0 155.3 168.5 140.5 208.0 172.3
google 146.0 137.0 191.0 158.0 161.0 133.0 213.0 169.0
native 191.0 179.0 176.0 182.0 201.0 139.0 186.0 175.3
Qwen1.5-7B cohere 194.0 175.5 205.5 191.7 185.0 165.0 213.0 187.7
google 178.0 164.0 183.0 175.0 188.0 153.0 184.0 175.0
native 206.0 144.0 197.0 182.3 204.5 184.0 219.0 202.5
Qwen1.5-14B cohere 194.0 206.5 216.0 205.5 211.0 187.5 236.0 211.5
google 177.0 167.0 222.5 188.8 197.0 182.0 212.0 197.0
Table19: Allmodelandalllanguageresultsonopen-endednativequestions(GPT-4-Turbojudged).
Monolingual Multilingual
Basemodel Data
es ru zh average es ru zh average
native 194.0 165.0 131.0 163.3 184.0 172.0 144.0 166.7
Llama2-7B cohere 173.0 160.0 145.0 159.3 184.0 163.0 149.0 165.3
google 184.0 161.0 153.0 166.0 179.0 158.0 142.0 159.7
native 168.0 151.0 128.0 149.0 179.0 152.0 155.0 162.0
Gemma-2B cohere 176.0 152.0 155.0 161.0 178.0 155.0 151.0 161.3
google 178.0 153.0 140.0 157.0 179.0 162.0 151.0 164.0
native 201.5 187.0 148.0 178.8 202.0 181.0 169.0 184.0
Gemma-7B cohere 183.0 172.0 162.0 172.3 180.0 162.0 168.0 170.0
google 190.0 180.0 174.0 181.3 182.0 175.0 170.0 175.7
native 133.0 103.0 153.0 129.7 136.0 96.0 150.0 127.3
Qwen1.5-0.5B cohere 136.0 98.0 145.0 126.3 129.0 99.0 151.0 126.3
google 130.0 97.0 145.0 124.0 125.0 102.0 157.0 128.0
native 160.0 138.0 155.0 151.0 152.0 112.0 145.0 136.3
Qwen1.5-1.8B cohere 151.0 121.0 170.0 147.3 149.0 118.0 167.0 144.7
google 156.0 128.0 166.0 150.0 161.0 125.0 163.0 149.7
native 180.0 152.0 161.0 164.3 181.0 149.0 171.0 167.0
Qwen1.5-4B cohere 173.0 154.0 180.0 169.0 173.0 154.0 176.0 167.7
google 168.0 149.0 171.0 162.7 182.0 147.0 177.0 168.7
native 184.0 164.0 154.0 167.3 193.0 153.0 159.0 168.3
Qwen1.5-7B cohere 184.0 154.0 160.0 166.0 184.0 146.0 173.0 167.7
google 181.0 155.0 166.0 167.3 182.0 149.0 164.0 165.0
native 191.0 160.0 152.0 167.7 192.0 167.0 169.0 176.0
Qwen1.5-14B cohere 189.0 173.0 172.0 178.0 190.0 164.0 173.0 175.7
google 184.0 163.0 178.0 175.0 188.0 160.0 165.0 171.0
Table20: Allmodelandalllanguageresultsonopen-endednativequestions(CommandR+judged).
20