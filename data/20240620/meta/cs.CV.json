[
    {
        "title": "Depth Anywhere: Enhancing 360 Monocular Depth Estimation via Perspective Distillation and Unlabeled Data Augmentation",
        "authors": "Ning-Hsu WangYu-Lun Liu",
        "links": "http://arxiv.org/abs/2406.12849v1",
        "entry_id": "http://arxiv.org/abs/2406.12849v1",
        "pdf_url": "http://arxiv.org/pdf/2406.12849v1",
        "summary": "Accurately estimating depth in 360-degree imagery is crucial for virtual\nreality, autonomous navigation, and immersive media applications. Existing\ndepth estimation methods designed for perspective-view imagery fail when\napplied to 360-degree images due to different camera projections and\ndistortions, whereas 360-degree methods perform inferior due to the lack of\nlabeled data pairs. We propose a new depth estimation framework that utilizes\nunlabeled 360-degree data effectively. Our approach uses state-of-the-art\nperspective depth estimation models as teacher models to generate pseudo labels\nthrough a six-face cube projection technique, enabling efficient labeling of\ndepth in 360-degree images. This method leverages the increasing availability\nof large datasets. Our approach includes two main stages: offline mask\ngeneration for invalid regions and an online semi-supervised joint training\nregime. We tested our approach on benchmark datasets such as Matterport3D and\nStanford2D3D, showing significant improvements in depth estimation accuracy,\nparticularly in zero-shot scenarios. Our proposed training pipeline can enhance\nany 360 monocular depth estimator and demonstrates effective knowledge transfer\nacross different camera projections and data types. See our project page for\nresults: https://albert100121.github.io/Depth-Anywhere/",
        "updated": "2024-06-18 17:59:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.12849v1"
    },
    {
        "title": "ChangeViT: Unleashing Plain Vision Transformers for Change Detection",
        "authors": "Duowang ZhuXiaohu HuangHaiyan HuangZhenfeng ShaoQimin Cheng",
        "links": "http://arxiv.org/abs/2406.12847v1",
        "entry_id": "http://arxiv.org/abs/2406.12847v1",
        "pdf_url": "http://arxiv.org/pdf/2406.12847v1",
        "summary": "Change detection in remote sensing images is essential for tracking\nenvironmental changes on the Earth's surface. Despite the success of vision\ntransformers (ViTs) as backbones in numerous computer vision applications, they\nremain underutilized in change detection, where convolutional neural networks\n(CNNs) continue to dominate due to their powerful feature extraction\ncapabilities. In this paper, our study uncovers ViTs' unique advantage in\ndiscerning large-scale changes, a capability where CNNs fall short.\nCapitalizing on this insight, we introduce ChangeViT, a framework that adopts a\nplain ViT backbone to enhance the performance of large-scale changes. This\nframework is supplemented by a detail-capture module that generates detailed\nspatial features and a feature injector that efficiently integrates\nfine-grained spatial information into high-level semantic learning. The feature\nintegration ensures that ChangeViT excels in both detecting large-scale changes\nand capturing fine-grained details, providing comprehensive change detection\nacross diverse scales. Without bells and whistles, ChangeViT achieves\nstate-of-the-art performance on three popular high-resolution datasets (i.e.,\nLEVIR-CD, WHU-CD, and CLCD) and one low-resolution dataset (i.e., OSCD), which\nunderscores the unleashed potential of plain ViTs for change detection.\nFurthermore, thorough quantitative and qualitative analyses validate the\nefficacy of the introduced modules, solidifying the effectiveness of our\napproach. The source code is available at\nhttps://github.com/zhuduowang/ChangeViT.",
        "updated": "2024-06-18 17:59:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.12847v1"
    },
    {
        "title": "DrVideo: Document Retrieval Based Long Video Understanding",
        "authors": "Ziyu MaChenhui GouHengcan ShiBin SunShutao LiHamid RezatofighiJianfei Cai",
        "links": "http://arxiv.org/abs/2406.12846v1",
        "entry_id": "http://arxiv.org/abs/2406.12846v1",
        "pdf_url": "http://arxiv.org/pdf/2406.12846v1",
        "summary": "Existing methods for long video understanding primarily focus on videos only\nlasting tens of seconds, with limited exploration of techniques for handling\nlonger videos. The increased number of frames in longer videos presents two\nmain challenges: difficulty in locating key information and performing\nlong-range reasoning. Thus, we propose DrVideo, a document-retrieval-based\nsystem designed for long video understanding. Our key idea is to convert the\nlong-video understanding problem into a long-document understanding task so as\nto effectively leverage the power of large language models. Specifically,\nDrVideo transforms a long video into a text-based long document to initially\nretrieve key frames and augment the information of these frames, which is used\nthis as the system's starting point. It then employs an agent-based iterative\nloop to continuously search for missing information, augment relevant data, and\nprovide final predictions in a chain-of-thought manner once sufficient\nquestion-related information is gathered. Extensive experiments on long video\nbenchmarks confirm the effectiveness of our method. DrVideo outperforms\nexisting state-of-the-art methods with +3.8 accuracy on EgoSchema benchmark (3\nminutes), +17.9 in MovieChat-1K break mode, +38.0 in MovieChat-1K global mode\n(10 minutes), and +30.2 on the LLama-Vid QA dataset (over 60 minutes).",
        "updated": "2024-06-18 17:59:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.12846v1"
    },
    {
        "title": "LayerMerge: Neural Network Depth Compression through Layer Pruning and Merging",
        "authors": "Jinuk KimMarwa El HalabiMingi JiHyun Oh Song",
        "links": "http://arxiv.org/abs/2406.12837v1",
        "entry_id": "http://arxiv.org/abs/2406.12837v1",
        "pdf_url": "http://arxiv.org/pdf/2406.12837v1",
        "summary": "Recent works show that reducing the number of layers in a convolutional\nneural network can enhance efficiency while maintaining the performance of the\nnetwork. Existing depth compression methods remove redundant non-linear\nactivation functions and merge the consecutive convolution layers into a single\nlayer. However, these methods suffer from a critical drawback; the kernel size\nof the merged layers becomes larger, significantly undermining the latency\nreduction gained from reducing the depth of the network. We show that this\nproblem can be addressed by jointly pruning convolution layers and activation\nfunctions. To this end, we propose LayerMerge, a novel depth compression method\nthat selects which activation layers and convolution layers to remove, to\nachieve a desired inference speed-up while minimizing performance loss. Since\nthe corresponding selection problem involves an exponential search space, we\nformulate a novel surrogate optimization problem and efficiently solve it via\ndynamic programming. Empirical results demonstrate that our method consistently\noutperforms existing depth compression and layer pruning methods on various\nnetwork architectures, both on image classification and generation tasks. We\nrelease the code at https://github.com/snu-mllab/LayerMerge.",
        "updated": "2024-06-18 17:55:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.12837v1"
    },
    {
        "title": "GroPrompt: Efficient Grounded Prompting and Adaptation for Referring Video Object Segmentation",
        "authors": "Ci-Siang LinI-Jieh LiuMin-Hung ChenChien-Yi WangSifei LiuYu-Chiang Frank Wang",
        "links": "http://arxiv.org/abs/2406.12834v1",
        "entry_id": "http://arxiv.org/abs/2406.12834v1",
        "pdf_url": "http://arxiv.org/pdf/2406.12834v1",
        "summary": "Referring Video Object Segmentation (RVOS) aims to segment the object\nreferred to by the query sentence throughout the entire video. Most existing\nmethods require end-to-end training with dense mask annotations, which could be\ncomputation-consuming and less scalable. In this work, we aim to efficiently\nadapt foundation segmentation models for addressing RVOS from weak supervision\nwith the proposed Grounded Prompting (GroPrompt) framework. More specifically,\nwe propose Text-Aware Prompt Contrastive Learning (TAP-CL) to enhance the\nassociation between the position prompts and the referring sentences with only\nbox supervisions, including Text-Contrastive Prompt Learning (TextCon) and\nModality-Contrastive Prompt Learning (ModalCon) at frame level and video level,\nrespectively. With the proposed TAP-CL, our GroPrompt framework can generate\ntemporal-consistent yet text-aware position prompts describing locations and\nmovements for the referred object from the video. The experimental results in\nthe standard RVOS benchmarks (Ref-YouTube-VOS, Ref-DAVIS17, A2D-Sentences, and\nJHMDB-Sentences) demonstrate the competitive performance of our proposed\nGroPrompt framework given only bounding box weak supervisions.",
        "updated": "2024-06-18 17:54:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.12834v1"
    }
]