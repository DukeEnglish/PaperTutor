[
    {
        "title": "\"A Lot of Moving Parts\": A Case Study of Open-Source Hardware Design Collaboration in the Thingiverse Community",
        "authors": "Kathy ChengShurui ZhouAlison Olechowski",
        "links": "http://arxiv.org/abs/2406.12801v1",
        "entry_id": "http://arxiv.org/abs/2406.12801v1",
        "pdf_url": "http://arxiv.org/pdf/2406.12801v1",
        "summary": "Open-source is a decentralized and collaborative method of development that\nencourages open contribution from an extensive and undefined network of\nindividuals. Although commonly associated with software development (OSS), the\nopen-source model extends to hardware development, forming the basis of\nopen-source hardware development (OSH). Compared to OSS, OSH is relatively\nnascent, lacking adequate tooling support from existing platforms and best\npractices for efficient collaboration. Taking a necessary step towards\nimproving OSH collaboration, we conduct a detailed case study of DrawBot, a\nsuccessful OSH project that remarkably fostered a long-term collaboration on\nThingiverse - a platform not explicitly intended for complex collaborative\ndesign. Through analyzing comment threads and design changes over the course of\nthe project, we found how collaboration occurred, the challenges faced, and how\nthe DrawBot community managed to overcome these obstacles. Beyond offering a\ndetailed account of collaboration practices and challenges, our work\ncontributes best practices, design implications, and practical implications for\nOSH project maintainers, platform builders, and researchers, respectively. With\nthese insights and our publicly available dataset, we aim to foster more\neffective and efficient collaborative design in OSH projects.",
        "updated": "2024-06-18 17:13:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.12801v1"
    },
    {
        "title": "Generating Educational Materials with Different Levels of Readability using LLMs",
        "authors": "Chieh-Yang HuangJing WeiTing-Hao 'Kenneth' Huang",
        "links": "http://arxiv.org/abs/2406.12787v1",
        "entry_id": "http://arxiv.org/abs/2406.12787v1",
        "pdf_url": "http://arxiv.org/pdf/2406.12787v1",
        "summary": "This study introduces the leveled-text generation task, aiming to rewrite\neducational materials to specific readability levels while preserving meaning.\nWe assess the capability of GPT-3.5, LLaMA-2 70B, and Mixtral 8x7B, to generate\ncontent at various readability levels through zero-shot and few-shot prompting.\nEvaluating 100 processed educational materials reveals that few-shot prompting\nsignificantly improves performance in readability manipulation and information\npreservation. LLaMA-2 70B performs better in achieving the desired difficulty\nrange, while GPT-3.5 maintains original meaning. However, manual inspection\nhighlights concerns such as misinformation introduction and inconsistent edit\ndistribution. These findings emphasize the need for further research to ensure\nthe quality of generated educational content.",
        "updated": "2024-06-18 16:55:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.12787v1"
    },
    {
        "title": "Unsupervised explainable activity prediction in competitive Nordic Walking from experimental data",
        "authors": "Silvia García-MéndezFrancisco de Arriba-PérezFrancisco J. González-CastañoJavier Vales-Alonso",
        "links": "http://dx.doi.org/10.1109/MCE.2024.3387019",
        "entry_id": "http://arxiv.org/abs/2406.12762v1",
        "pdf_url": "http://arxiv.org/pdf/2406.12762v1",
        "summary": "Artificial Intelligence (AI) has found application in Human Activity\nRecognition (HAR) in competitive sports. To date, most Machine Learning (ML)\napproaches for HAR have relied on offline (batch) training, imposing higher\ncomputational and tagging burdens compared to online processing unsupervised\napproaches. Additionally, the decisions behind traditional ML predictors are\nopaque and require human interpretation. In this work, we apply an online\nprocessing unsupervised clustering approach based on low-cost wearable Inertial\nMeasurement Units (IMUs). The outcomes generated by the system allow for the\nautomatic expansion of limited tagging available (e.g., by referees) within\nthose clusters, producing pertinent information for the explainable\nclassification stage. Specifically, our work focuses on achieving automatic\nexplainability for predictions related to athletes' activities, distinguishing\nbetween correct, incorrect, and cheating practices in Nordic Walking. The\nproposed solution achieved performance metrics of close to 100 % on average.",
        "updated": "2024-06-18 16:29:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.12762v1"
    },
    {
        "title": "MAGIC: Generating Self-Correction Guideline for In-Context Text-to-SQL",
        "authors": "Arian AskariChristian PoelitzXinye Tang",
        "links": "http://arxiv.org/abs/2406.12692v1",
        "entry_id": "http://arxiv.org/abs/2406.12692v1",
        "pdf_url": "http://arxiv.org/pdf/2406.12692v1",
        "summary": "Self-correction in text-to-SQL is the process of prompting large language\nmodel (LLM) to revise its previously incorrectly generated SQL, and commonly\nrelies on manually crafted self-correction guidelines by human experts that are\nnot only labor-intensive to produce but also limited by the human ability in\nidentifying all potential error patterns in LLM responses. We introduce MAGIC,\na novel multi-agent method that automates the creation of the self-correction\nguideline. MAGIC uses three specialized agents: a manager, a correction, and a\nfeedback agent. These agents collaborate on the failures of an LLM-based method\non the training set to iteratively generate and refine a self-correction\nguideline tailored to LLM mistakes, mirroring human processes but without human\ninvolvement. Our extensive experiments show that MAGIC's guideline outperforms\nexpert human's created ones. We empirically find out that the guideline\nproduced by MAGIC enhance the interpretability of the corrections made,\nproviding insights in analyzing the reason behind the failures and successes of\nLLMs in self-correction. We make all agent interactions publicly available to\nthe research community, to foster further research in this area, offering a\nsynthetic dataset for future explorations into automatic self-correction\nguideline generation.",
        "updated": "2024-06-18 15:06:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.12692v1"
    },
    {
        "title": "Transforming Surgical Interventions with Embodied Intelligence for Ultrasound Robotics",
        "authors": "Huan XuJinlin WuGuanglin CaoZhen ChenZhen LeiHongbin Liu",
        "links": "http://arxiv.org/abs/2406.12651v1",
        "entry_id": "http://arxiv.org/abs/2406.12651v1",
        "pdf_url": "http://arxiv.org/pdf/2406.12651v1",
        "summary": "Ultrasonography has revolutionized non-invasive diagnostic methodologies,\nsignificantly enhancing patient outcomes across various medical domains.\nDespite its advancements, integrating ultrasound technology with robotic\nsystems for automated scans presents challenges, including limited command\nunderstanding and dynamic execution capabilities. To address these challenges,\nthis paper introduces a novel Ultrasound Embodied Intelligence system that\nsynergistically combines ultrasound robots with large language models (LLMs)\nand domain-specific knowledge augmentation, enhancing ultrasound robots'\nintelligence and operational efficiency. Our approach employs a dual strategy:\nfirstly, integrating LLMs with ultrasound robots to interpret doctors' verbal\ninstructions into precise motion planning through a comprehensive understanding\nof ultrasound domain knowledge, including APIs and operational manuals;\nsecondly, incorporating a dynamic execution mechanism, allowing for real-time\nadjustments to scanning plans based on patient movements or procedural errors.\nWe demonstrate the effectiveness of our system through extensive experiments,\nincluding ablation studies and comparisons across various models, showcasing\nsignificant improvements in executing medical procedures from verbal commands.\nOur findings suggest that the proposed system improves the efficiency and\nquality of ultrasound scans and paves the way for further advancements in\nautonomous medical scanning technologies, with the potential to transform\nnon-invasive diagnostics and streamline medical workflows.",
        "updated": "2024-06-18 14:22:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.12651v1"
    }
]