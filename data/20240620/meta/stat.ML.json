[
    {
        "title": "Can Go AIs be adversarially robust?",
        "authors": "Tom TsengEuan McLeanKellin PelrineTony T. WangAdam Gleave",
        "links": "http://arxiv.org/abs/2406.12843v1",
        "entry_id": "http://arxiv.org/abs/2406.12843v1",
        "pdf_url": "http://arxiv.org/pdf/2406.12843v1",
        "summary": "Prior work found that superhuman Go AIs like KataGo can be defeated by simple\nadversarial strategies. In this paper, we study if simple defenses can improve\nKataGo's worst-case performance. We test three natural defenses: adversarial\ntraining on hand-constructed positions, iterated adversarial training, and\nchanging the network architecture. We find that some of these defenses are able\nto protect against previously discovered attacks. Unfortunately, we also find\nthat none of these defenses are able to withstand adaptive attacks. In\nparticular, we are able to train new adversaries that reliably defeat our\ndefended agents by causing them to blunder in ways humans would not. Our\nresults suggest that building robust AI systems is challenging even in narrow\ndomains such as Go. For interactive examples of attacks and a link to our\ncodebase, see https://goattack.far.ai.",
        "updated": "2024-06-18 17:57:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.12843v1"
    },
    {
        "title": "Evaluating the design space of diffusion-based generative models",
        "authors": "Yuqing WangYe HeMolei Tao",
        "links": "http://arxiv.org/abs/2406.12839v1",
        "entry_id": "http://arxiv.org/abs/2406.12839v1",
        "pdf_url": "http://arxiv.org/pdf/2406.12839v1",
        "summary": "Most existing theoretical investigations of the accuracy of diffusion models,\nalbeit significant, assume the score function has been approximated to a\ncertain accuracy, and then use this a priori bound to control the error of\ngeneration. This article instead provides a first quantitative understanding of\nthe whole generation process, i.e., both training and sampling. More precisely,\nit conducts a non-asymptotic convergence analysis of denoising score matching\nunder gradient descent. In addition, a refined sampling error analysis for\nvariance exploding models is also provided. The combination of these two\nresults yields a full error analysis, which elucidates (again, but this time\ntheoretically) how to design the training and sampling processes for effective\ngeneration. For instance, our theory implies a preference toward noise\ndistribution and loss weighting that qualitatively agree with the ones used in\n[Karras et al. 2022]. It also provides some perspectives on why the time and\nvariance schedule used in [Karras et al. 2022] could be better tuned than the\npioneering version in [Song et al. 2020].",
        "updated": "2024-06-18 17:56:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.12839v1"
    },
    {
        "title": "Privacy Preserving Federated Learning in Medical Imaging with Uncertainty Estimation",
        "authors": "Nikolas KoutsoubisYasin YilmazRavi P. RamachandranMatthew SchabathGhulam Rasool",
        "links": "http://arxiv.org/abs/2406.12815v1",
        "entry_id": "http://arxiv.org/abs/2406.12815v1",
        "pdf_url": "http://arxiv.org/pdf/2406.12815v1",
        "summary": "Machine learning (ML) and Artificial Intelligence (AI) have fueled remarkable\nadvancements, particularly in healthcare. Within medical imaging, ML models\nhold the promise of improving disease diagnoses, treatment planning, and\npost-treatment monitoring. Various computer vision tasks like image\nclassification, object detection, and image segmentation are poised to become\nroutine in clinical analysis. However, privacy concerns surrounding patient\ndata hinder the assembly of large training datasets needed for developing and\ntraining accurate, robust, and generalizable models. Federated Learning (FL)\nemerges as a compelling solution, enabling organizations to collaborate on ML\nmodel training by sharing model training information (gradients) rather than\ndata (e.g., medical images). FL's distributed learning framework facilitates\ninter-institutional collaboration while preserving patient privacy. However,\nFL, while robust in privacy preservation, faces several challenges. Sensitive\ninformation can still be gleaned from shared gradients that are passed on\nbetween organizations during model training. Additionally, in medical imaging,\nquantifying model confidence\\uncertainty accurately is crucial due to the noise\nand artifacts present in the data. Uncertainty estimation in FL encounters\nunique hurdles due to data heterogeneity across organizations. This paper\noffers a comprehensive review of FL, privacy preservation, and uncertainty\nestimation, with a focus on medical imaging. Alongside a survey of current\nresearch, we identify gaps in the field and suggest future directions for FL\nresearch to enhance privacy and address noisy medical imaging data challenges.",
        "updated": "2024-06-18 17:35:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.12815v1"
    },
    {
        "title": "Quasi-Bayes meets Vines",
        "authors": "David HukYuanhe ZhangMark SteelRitabrata Dutta",
        "links": "http://arxiv.org/abs/2406.12764v1",
        "entry_id": "http://arxiv.org/abs/2406.12764v1",
        "pdf_url": "http://arxiv.org/pdf/2406.12764v1",
        "summary": "Recently proposed quasi-Bayesian (QB) methods initiated a new era in Bayesian\ncomputation by directly constructing the Bayesian predictive distribution\nthrough recursion, removing the need for expensive computations involved in\nsampling the Bayesian posterior distribution. This has proved to be\ndata-efficient for univariate predictions, but extensions to multiple\ndimensions rely on a conditional decomposition resulting from predefined\nassumptions on the kernel of the Dirichlet Process Mixture Model, which is the\nimplicit nonparametric model used. Here, we propose a different way to extend\nQuasi-Bayesian prediction to high dimensions through the use of Sklar's theorem\nby decomposing the predictive distribution into one-dimensional predictive\nmarginals and a high-dimensional copula. Thus, we use the efficient recursive\nQB construction for the one-dimensional marginals and model the dependence\nusing highly expressive vine copulas. Further, we tune hyperparameters using\nrobust divergences (eg. energy score) and show that our proposed Quasi-Bayesian\nVine (QB-Vine) is a fully non-parametric density estimator with \\emph{an\nanalytical form} and convergence rate independent of the dimension of data in\nsome situations. Our experiments illustrate that the QB-Vine is appropriate for\nhigh dimensional distributions ($\\sim$64), needs very few samples to train\n($\\sim$200) and outperforms state-of-the-art methods with analytical forms for\ndensity estimation and supervised tasks by a considerable margin.",
        "updated": "2024-06-18 16:31:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.12764v1"
    },
    {
        "title": "Implicit Bias of Mirror Flow on Separable Data",
        "authors": "Scott PesmeRadu-Alexandru DragomirNicolas Flammarion",
        "links": "http://arxiv.org/abs/2406.12763v1",
        "entry_id": "http://arxiv.org/abs/2406.12763v1",
        "pdf_url": "http://arxiv.org/pdf/2406.12763v1",
        "summary": "We examine the continuous-time counterpart of mirror descent, namely mirror\nflow, on classification problems which are linearly separable. Such problems\nare minimised `at infinity' and have many possible solutions; we study which\nsolution is preferred by the algorithm depending on the mirror potential. For\nexponential tailed losses and under mild assumptions on the potential, we show\nthat the iterates converge in direction towards a $\\phi_\\infty$-maximum margin\nclassifier. The function $\\phi_\\infty$ is the $\\textit{horizon function}$ of\nthe mirror potential and characterises its shape `at infinity'. When the\npotential is separable, a simple formula allows to compute this function. We\nanalyse several examples of potentials and provide numerical experiments\nhighlighting our results.",
        "updated": "2024-06-18 16:30:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.12763v1"
    }
]