[
    {
        "title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts",
        "authors": "Haoxiang WangWei XiongTengyang XieHan ZhaoTong Zhang",
        "links": "http://arxiv.org/abs/2406.12845v1",
        "entry_id": "http://arxiv.org/abs/2406.12845v1",
        "pdf_url": "http://arxiv.org/pdf/2406.12845v1",
        "summary": "Reinforcement learning from human feedback (RLHF) has emerged as the primary\nmethod for aligning large language models (LLMs) with human preferences. The\nRLHF process typically starts by training a reward model (RM) using human\npreference data. Conventional RMs are trained on pairwise responses to the same\nuser request, with relative ratings indicating which response humans prefer.\nThe trained RM serves as a proxy for human preferences. However, due to the\nblack-box nature of RMs, their outputs lack interpretability, as humans cannot\nintuitively understand why an RM thinks a response is good or not. As RMs act\nas human preference proxies, we believe they should be human-interpretable to\nensure that their internal decision processes are consistent with human\npreferences and to prevent reward hacking in LLM alignment. To build RMs with\ninterpretable preferences, we propose a two-stage approach: i) train an\nAbsolute-Rating Multi-Objective Reward Model (ArmoRM) with multi-dimensional\nabsolute-rating data, each dimension corresponding to a human-interpretable\nobjective (e.g., honesty, verbosity, safety); ii) employ a Mixture-of-Experts\n(MoE) strategy with a gating network that automatically selects the most\nsuitable reward objectives based on the context. We efficiently trained an\nArmoRM with Llama-3 8B and a gating network consisting of a shallow MLP on top\nof the ArmoRM. Our trained model, ArmoRM-Llama3-8B, obtains state-of-the-art\nperformance on RewardBench, a benchmark evaluating RMs for language modeling.\nNotably, the performance of our model surpasses the LLM-as-a-judge method with\nGPT-4 judges by a margin, and approaches the performance of the much larger\nNemotron-4 340B reward model.",
        "updated": "2024-06-18 17:58:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.12845v1"
    },
    {
        "title": "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation",
        "authors": "Seyedarmin AziziSouvik KunduMassoud Pedram",
        "links": "http://arxiv.org/abs/2406.12832v1",
        "entry_id": "http://arxiv.org/abs/2406.12832v1",
        "pdf_url": "http://arxiv.org/pdf/2406.12832v1",
        "summary": "Low-rank adaptation (LoRA) has become the default approach to fine-tune large\nlanguage models (LLMs) due to its significant reduction in trainable\nparameters. However, trainable parameter demand for LoRA increases with\nincreasing model embedding dimensions, leading to high compute costs.\nAdditionally, its backward updates require storing high-dimensional\nintermediate activations and optimizer states, demanding high peak GPU memory.\nIn this paper, we introduce large model fine-tuning via spectrally decomposed\nlow-dimensional adaptation (LaMDA), a novel approach to fine-tuning large\nlanguage models, which leverages low-dimensional adaptation to achieve\nsignificant reductions in trainable parameters and peak GPU memory footprint.\nLaMDA freezes a first projection matrix (PMA) in the adaptation path while\nintroducing a low-dimensional trainable square matrix, resulting in substantial\nreductions in trainable parameters and peak GPU memory usage. LaMDA gradually\nfreezes a second projection matrix (PMB) during the early fine-tuning stages,\nreducing the compute cost associated with weight updates to enhance parameter\nefficiency further. We also present an enhancement, LaMDA++, incorporating a\n``lite-weight\" adaptive rank allocation for the LoRA path via normalized\nspectrum analysis of pre-trained model weights. We evaluate LaMDA/LaMDA++\nacross various tasks, including natural language understanding with the GLUE\nbenchmark, text summarization, natural language generation, and complex\nreasoning on different LLMs. Results show that LaMDA matches or surpasses the\nperformance of existing alternatives while requiring up to 17.7x fewer\nparameter updates and up to 1.32x lower peak GPU memory usage during\nfine-tuning. Code will be publicly available.",
        "updated": "2024-06-18 17:52:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.12832v1"
    },
    {
        "title": "What Are the Odds? Language Models Are Capable of Probabilistic Reasoning",
        "authors": "Akshay ParuchuriJake GarrisonShun LiaoJohn HernandezJacob SunshineTim AlthoffXin LiuDaniel McDuff",
        "links": "http://arxiv.org/abs/2406.12830v1",
        "entry_id": "http://arxiv.org/abs/2406.12830v1",
        "pdf_url": "http://arxiv.org/pdf/2406.12830v1",
        "summary": "Language models (LM) are capable of remarkably complex linguistic tasks;\nhowever, numerical reasoning is an area in which they frequently struggle. An\nimportant but rarely evaluated form of reasoning is understanding probability\ndistributions. In this paper, we focus on evaluating the probabilistic\nreasoning capabilities of LMs using idealized and real-world statistical\ndistributions. We perform a systematic evaluation of state-of-the-art LMs on\nthree tasks: estimating percentiles, drawing samples, and calculating\nprobabilities. We evaluate three ways to provide context to LMs 1) anchoring\nexamples from within a distribution or family of distributions, 2) real-world\ncontext, 3) summary statistics on which to base a Normal approximation. Models\ncan make inferences about distributions, and can be further aided by the\nincorporation of real-world context, example shots and simplified assumptions,\neven if these assumptions are incorrect or misspecified. To conduct this work,\nwe developed a comprehensive benchmark distribution dataset with associated\nquestion-answer pairs that we will release publicly.",
        "updated": "2024-06-18 17:51:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.12830v1"
    },
    {
        "title": "From RAGs to rich parameters: Probing how language models utilize external knowledge over parametric information for factual queries",
        "authors": "Hitesh WadhwaRahul SeetharamanSomyaa AggarwalReshmi GhoshSamyadeep BasuSoundararajan SrinivasanWenlong ZhaoShreyas ChaudhariEhsan Aghazadeh",
        "links": "http://arxiv.org/abs/2406.12824v1",
        "entry_id": "http://arxiv.org/abs/2406.12824v1",
        "pdf_url": "http://arxiv.org/pdf/2406.12824v1",
        "summary": "Retrieval Augmented Generation (RAG) enriches the ability of language models\nto reason using external context to augment responses for a given user prompt.\nThis approach has risen in popularity due to practical applications in various\napplications of language models in search, question/answering, and chat-bots.\nHowever, the exact nature of how this approach works isn't clearly understood.\nIn this paper, we mechanistically examine the RAG pipeline to highlight that\nlanguage models take shortcut and have a strong bias towards utilizing only the\ncontext information to answer the question, while relying minimally on their\nparametric memory. We probe this mechanistic behavior in language models with:\n(i) Causal Mediation Analysis to show that the parametric memory is minimally\nutilized when answering a question and (ii) Attention Contributions and\nKnockouts to show that the last token residual stream do not get enriched from\nthe subject token in the question, but gets enriched from other informative\ntokens in the context. We find this pronounced shortcut behaviour true across\nboth LLaMa and Phi family of models.",
        "updated": "2024-06-18 17:46:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.12824v1"
    },
    {
        "title": "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?",
        "authors": "Pinzhen ChenSimon YuZhicheng GuoBarry Haddow",
        "links": "http://arxiv.org/abs/2406.12822v1",
        "entry_id": "http://arxiv.org/abs/2406.12822v1",
        "pdf_url": "http://arxiv.org/pdf/2406.12822v1",
        "summary": "Large language models, particularly multilingual ones, are designed, claimed,\nand expected to cater to native speakers of varied languages. We hypothesise\nthat the current practices of fine-tuning and evaluating these models may\nmismatch this intention owing to a heavy reliance on translation, which can\nintroduce translation artefacts and defects. It remains unknown whether the\nnature of the instruction data has an impact on the model output; on the other\nhand, it remains questionable whether translated test sets can capture such\nnuances. Due to the often coupled practices of using translated data in both\nstages, such imperfections could have been overlooked. This work investigates\nthese issues by using controlled native or translated data during instruction\ntuning and evaluation stages and observing model results. Experiments on eight\nbase models and eight different benchmarks reveal that native or generation\nbenchmarks display a notable difference between native and translated\ninstruction data especially when model performance is high, whereas other types\nof test sets cannot. Finally, we demonstrate that regularization is beneficial\nto bridging this gap on structured but not generative tasks.",
        "updated": "2024-06-18 17:43:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.12822v1"
    }
]