[
    {
        "title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts",
        "authors": "Haoxiang WangWei XiongTengyang XieHan ZhaoTong Zhang",
        "links": "http://arxiv.org/abs/2406.12845v1",
        "entry_id": "http://arxiv.org/abs/2406.12845v1",
        "pdf_url": "http://arxiv.org/pdf/2406.12845v1",
        "summary": "Reinforcement learning from human feedback (RLHF) has emerged as the primary\nmethod for aligning large language models (LLMs) with human preferences. The\nRLHF process typically starts by training a reward model (RM) using human\npreference data. Conventional RMs are trained on pairwise responses to the same\nuser request, with relative ratings indicating which response humans prefer.\nThe trained RM serves as a proxy for human preferences. However, due to the\nblack-box nature of RMs, their outputs lack interpretability, as humans cannot\nintuitively understand why an RM thinks a response is good or not. As RMs act\nas human preference proxies, we believe they should be human-interpretable to\nensure that their internal decision processes are consistent with human\npreferences and to prevent reward hacking in LLM alignment. To build RMs with\ninterpretable preferences, we propose a two-stage approach: i) train an\nAbsolute-Rating Multi-Objective Reward Model (ArmoRM) with multi-dimensional\nabsolute-rating data, each dimension corresponding to a human-interpretable\nobjective (e.g., honesty, verbosity, safety); ii) employ a Mixture-of-Experts\n(MoE) strategy with a gating network that automatically selects the most\nsuitable reward objectives based on the context. We efficiently trained an\nArmoRM with Llama-3 8B and a gating network consisting of a shallow MLP on top\nof the ArmoRM. Our trained model, ArmoRM-Llama3-8B, obtains state-of-the-art\nperformance on RewardBench, a benchmark evaluating RMs for language modeling.\nNotably, the performance of our model surpasses the LLM-as-a-judge method with\nGPT-4 judges by a margin, and approaches the performance of the much larger\nNemotron-4 340B reward model.",
        "updated": "2024-06-18 17:58:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.12845v1"
    },
    {
        "title": "Synergizing Foundation Models and Federated Learning: A Survey",
        "authors": "Shenghui LiFanghua YeMeng FangJiaxu ZhaoYun-Hin ChanEdith C. -H. NgaiThiemo Voigt",
        "links": "http://arxiv.org/abs/2406.12844v1",
        "entry_id": "http://arxiv.org/abs/2406.12844v1",
        "pdf_url": "http://arxiv.org/pdf/2406.12844v1",
        "summary": "The recent development of Foundation Models (FMs), represented by large\nlanguage models, vision transformers, and multimodal models, has been making a\nsignificant impact on both academia and industry. Compared with small-scale\nmodels, FMs have a much stronger demand for high-volume data during the\npre-training phase. Although general FMs can be pre-trained on data collected\nfrom open sources such as the Internet, domain-specific FMs need proprietary\ndata, posing a practical challenge regarding the amount of data available due\nto privacy concerns. Federated Learning (FL) is a collaborative learning\nparadigm that breaks the barrier of data availability from different\nparticipants. Therefore, it provides a promising solution to customize and\nadapt FMs to a wide range of domain-specific tasks using distributed datasets\nwhilst preserving privacy. This survey paper discusses the potentials and\nchallenges of synergizing FL and FMs and summarizes core techniques, future\ndirections, and applications. A periodically updated paper collection on FM-FL\nis available at https://github.com/lishenghui/awesome-fm-fl.",
        "updated": "2024-06-18 17:58:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.12844v1"
    },
    {
        "title": "Can Go AIs be adversarially robust?",
        "authors": "Tom TsengEuan McLeanKellin PelrineTony T. WangAdam Gleave",
        "links": "http://arxiv.org/abs/2406.12843v1",
        "entry_id": "http://arxiv.org/abs/2406.12843v1",
        "pdf_url": "http://arxiv.org/pdf/2406.12843v1",
        "summary": "Prior work found that superhuman Go AIs like KataGo can be defeated by simple\nadversarial strategies. In this paper, we study if simple defenses can improve\nKataGo's worst-case performance. We test three natural defenses: adversarial\ntraining on hand-constructed positions, iterated adversarial training, and\nchanging the network architecture. We find that some of these defenses are able\nto protect against previously discovered attacks. Unfortunately, we also find\nthat none of these defenses are able to withstand adaptive attacks. In\nparticular, we are able to train new adversaries that reliably defeat our\ndefended agents by causing them to blunder in ways humans would not. Our\nresults suggest that building robust AI systems is challenging even in narrow\ndomains such as Go. For interactive examples of attacks and a link to our\ncodebase, see https://goattack.far.ai.",
        "updated": "2024-06-18 17:57:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.12843v1"
    },
    {
        "title": "Demystifying Higher-Order Graph Neural Networks",
        "authors": "Maciej BestaFlorian ScheidlLukas GianinazziShachar KlaimanJürgen MüllerTorsten Hoefler",
        "links": "http://arxiv.org/abs/2406.12841v1",
        "entry_id": "http://arxiv.org/abs/2406.12841v1",
        "pdf_url": "http://arxiv.org/pdf/2406.12841v1",
        "summary": "Higher-order graph neural networks (HOGNNs) are an important class of GNN\nmodels that harness polyadic relations between vertices beyond plain edges.\nThey have been used to eliminate issues such as over-smoothing or\nover-squashing, to significantly enhance the accuracy of GNN predictions, to\nimprove the expressiveness of GNN architectures, and for numerous other goals.\nA plethora of HOGNN models have been introduced, and they come with diverse\nneural architectures, and even with different notions of what the\n\"higher-order\" means. This richness makes it very challenging to appropriately\nanalyze and compare HOGNN models, and to decide in what scenario to use\nspecific ones. To alleviate this, we first design an in-depth taxonomy and a\nblueprint for HOGNNs. This facilitates designing models that maximize\nperformance. Then, we use our taxonomy to analyze and compare the available\nHOGNN models. The outcomes of our analysis are synthesized in a set of insights\nthat help to select the most beneficial GNN model in a given scenario, and a\ncomprehensive list of challenges and opportunities for further research into\nmore powerful HOGNNs.",
        "updated": "2024-06-18 17:57:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.12841v1"
    },
    {
        "title": "Evaluating the design space of diffusion-based generative models",
        "authors": "Yuqing WangYe HeMolei Tao",
        "links": "http://arxiv.org/abs/2406.12839v1",
        "entry_id": "http://arxiv.org/abs/2406.12839v1",
        "pdf_url": "http://arxiv.org/pdf/2406.12839v1",
        "summary": "Most existing theoretical investigations of the accuracy of diffusion models,\nalbeit significant, assume the score function has been approximated to a\ncertain accuracy, and then use this a priori bound to control the error of\ngeneration. This article instead provides a first quantitative understanding of\nthe whole generation process, i.e., both training and sampling. More precisely,\nit conducts a non-asymptotic convergence analysis of denoising score matching\nunder gradient descent. In addition, a refined sampling error analysis for\nvariance exploding models is also provided. The combination of these two\nresults yields a full error analysis, which elucidates (again, but this time\ntheoretically) how to design the training and sampling processes for effective\ngeneration. For instance, our theory implies a preference toward noise\ndistribution and loss weighting that qualitatively agree with the ones used in\n[Karras et al. 2022]. It also provides some perspectives on why the time and\nvariance schedule used in [Karras et al. 2022] could be better tuned than the\npioneering version in [Song et al. 2020].",
        "updated": "2024-06-18 17:56:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.12839v1"
    }
]