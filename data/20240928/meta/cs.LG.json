[
    {
        "title": "Multi-View and Multi-Scale Alignment for Contrastive Language-Image Pre-training in Mammography",
        "authors": "Yuexi DuJohn OnofreyNicha C. Dvornek",
        "links": "http://arxiv.org/abs/2409.18119v1",
        "entry_id": "http://arxiv.org/abs/2409.18119v1",
        "pdf_url": "http://arxiv.org/pdf/2409.18119v1",
        "summary": "Contrastive Language-Image Pre-training (CLIP) shows promise in medical image\nanalysis but requires substantial data and computational resources. Due to\nthese restrictions, existing CLIP applications in medical imaging focus mainly\non modalities like chest X-rays that have abundant image-report data available,\nleaving many other important modalities under-explored. Here, we propose the\nfirst adaptation of the full CLIP model to mammography, which presents\nsignificant challenges due to labeled data scarcity, high-resolution images\nwith small regions of interest, and data imbalance. We first develop a\nspecialized supervision framework for mammography that leverages its multi-view\nnature. Furthermore, we design a symmetric local alignment module to better\nfocus on detailed features in high-resolution images. Lastly, we incorporate a\nparameter-efficient fine-tuning approach for large language models pre-trained\nwith medical knowledge to address data limitations. Our multi-view and\nmulti-scale alignment (MaMA) method outperforms state-of-the-art baselines for\nthree different tasks on two large real-world mammography datasets, EMBED and\nRSNA-Mammo, with only 52% model size compared with the largest baseline.",
        "updated": "2024-09-26 17:56:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.18119v1"
    },
    {
        "title": "Find Rhinos without Finding Rhinos: Active Learning with Multimodal Imagery of South African Rhino Habitats",
        "authors": "Lucia GordonNikhil BehariSamuel CollierElizabeth Bondi-KellyJackson A. KillianCatherine RessijacPeter BoucherAndrew DaviesMilind Tambe",
        "links": "http://dx.doi.org/10.24963/ijcai.2023/663",
        "entry_id": "http://arxiv.org/abs/2409.18104v1",
        "pdf_url": "http://arxiv.org/pdf/2409.18104v1",
        "summary": "Much of Earth's charismatic megafauna is endangered by human activities,\nparticularly the rhino, which is at risk of extinction due to the poaching\ncrisis in Africa. Monitoring rhinos' movement is crucial to their protection\nbut has unfortunately proven difficult because rhinos are elusive. Therefore,\ninstead of tracking rhinos, we propose the novel approach of mapping communal\ndefecation sites, called middens, which give information about rhinos' spatial\nbehavior valuable to anti-poaching, management, and reintroduction efforts.\nThis paper provides the first-ever mapping of rhino midden locations by\nbuilding classifiers to detect them using remotely sensed thermal, RGB, and\nLiDAR imagery in passive and active learning settings. As existing active\nlearning methods perform poorly due to the extreme class imbalance in our\ndataset, we design MultimodAL, an active learning system employing a ranking\ntechnique and multimodality to achieve competitive performance with passive\nlearning models with 94% fewer labels. Our methods could therefore save over 76\nhours in labeling time when used on a similarly-sized dataset. Unexpectedly,\nour midden map reveals that rhino middens are not randomly distributed\nthroughout the landscape; rather, they are clustered. Consequently, rangers\nshould be targeted at areas with high midden densities to strengthen\nanti-poaching efforts, in line with UN Target 15.7.",
        "updated": "2024-09-26 17:49:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.18104v1"
    },
    {
        "title": "MALPOLON: A Framework for Deep Species Distribution Modeling",
        "authors": "Theo LarcherLukas PicekBenjamin DeneuTitouan LorieulMaximilien ServajeanAlexis Joly",
        "links": "http://arxiv.org/abs/2409.18102v1",
        "entry_id": "http://arxiv.org/abs/2409.18102v1",
        "pdf_url": "http://arxiv.org/pdf/2409.18102v1",
        "summary": "This paper describes a deep-SDM framework, MALPOLON. Written in Python and\nbuilt upon the PyTorch library, this framework aims to facilitate training and\ninferences of deep species distribution models (deep-SDM) and sharing for users\nwith only general Python language skills (e.g., modeling ecologists) who are\ninterested in testing deep learning approaches to build new SDMs. More advanced\nusers can also benefit from the framework's modularity to run more specific\nexperiments by overriding existing classes while taking advantage of\npress-button examples to train neural networks on multiple classification tasks\nusing custom or provided raw and pre-processed datasets. The framework is\nopen-sourced on GitHub and PyPi along with extensive documentation and examples\nof use in various scenarios. MALPOLON offers straightforward installation,\nYAML-based configuration, parallel computing, multi-GPU utilization, baseline\nand foundational models for benchmarking, and extensive\ntutorials/documentation, aiming to enhance accessibility and performance\nscalability for ecologists and researchers.",
        "updated": "2024-09-26 17:45:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.18102v1"
    },
    {
        "title": "Self-supervised Pretraining for Cardiovascular Magnetic Resonance Cine Segmentation",
        "authors": "Rob A. J. de MooijJosien P. W. PluimCian M. Scannell",
        "links": "http://arxiv.org/abs/2409.18100v1",
        "entry_id": "http://arxiv.org/abs/2409.18100v1",
        "pdf_url": "http://arxiv.org/pdf/2409.18100v1",
        "summary": "Self-supervised pretraining (SSP) has shown promising results in learning\nfrom large unlabeled datasets and, thus, could be useful for automated\ncardiovascular magnetic resonance (CMR) short-axis cine segmentation. However,\ninconsistent reports of the benefits of SSP for segmentation have made it\ndifficult to apply SSP to CMR. Therefore, this study aimed to evaluate SSP\nmethods for CMR cine segmentation.\n  To this end, short-axis cine stacks of 296 subjects (90618 2D slices) were\nused for unlabeled pretraining with four SSP methods; SimCLR, positional\ncontrastive learning, DINO, and masked image modeling (MIM). Subsets of varying\nnumbers of subjects were used for supervised fine-tuning of 2D models for each\nSSP method, as well as to train a 2D baseline model from scratch. The\nfine-tuned models were compared to the baseline using the 3D Dice similarity\ncoefficient (DSC) in a test dataset of 140 subjects.\n  The SSP methods showed no performance gains with the largest supervised\nfine-tuning subset compared to the baseline (DSC = 0.89). When only 10 subjects\n(231 2D slices) are available for supervised training, SSP using MIM (DSC =\n0.86) improves over training from scratch (DSC = 0.82).\n  This study found that SSP is valuable for CMR cine segmentation when labeled\ntraining data is scarce, but does not aid state-of-the-art deep learning\nmethods when ample labeled data is available. Moreover, the choice of SSP\nmethod is important. The code is publicly available at:\nhttps://github.com/q-cardIA/ssp-cmr-cine-segmentation",
        "updated": "2024-09-26 17:44:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.18100v1"
    },
    {
        "title": "Infer Human's Intentions Before Following Natural Language Instructions",
        "authors": "Yanming WanYue WuYiping WangJiayuan MaoNatasha Jaques",
        "links": "http://arxiv.org/abs/2409.18073v1",
        "entry_id": "http://arxiv.org/abs/2409.18073v1",
        "pdf_url": "http://arxiv.org/pdf/2409.18073v1",
        "summary": "For AI agents to be helpful to humans, they should be able to follow natural\nlanguage instructions to complete everyday cooperative tasks in human\nenvironments. However, real human instructions inherently possess ambiguity,\nbecause the human speakers assume sufficient prior knowledge about their hidden\ngoals and intentions. Standard language grounding and planning methods fail to\naddress such ambiguities because they do not model human internal goals as\nadditional partially observable factors in the environment. We propose a new\nframework, Follow Instructions with Social and Embodied Reasoning (FISER),\naiming for better natural language instruction following in collaborative\nembodied tasks. Our framework makes explicit inferences about human goals and\nintentions as intermediate reasoning steps. We implement a set of\nTransformer-based models and evaluate them over a challenging benchmark,\nHandMeThat. We empirically demonstrate that using social reasoning to\nexplicitly infer human intentions before making action plans surpasses purely\nend-to-end approaches. We also compare our implementation with strong\nbaselines, including Chain of Thought prompting on the largest available\npre-trained language models, and find that FISER provides better performance on\nthe embodied social reasoning tasks under investigation, reaching the\nstate-of-the-art on HandMeThat.",
        "updated": "2024-09-26 17:19:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.18073v1"
    }
]