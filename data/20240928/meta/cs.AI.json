[
    {
        "title": "Multi-View and Multi-Scale Alignment for Contrastive Language-Image Pre-training in Mammography",
        "authors": "Yuexi DuJohn OnofreyNicha C. Dvornek",
        "links": "http://arxiv.org/abs/2409.18119v1",
        "entry_id": "http://arxiv.org/abs/2409.18119v1",
        "pdf_url": "http://arxiv.org/pdf/2409.18119v1",
        "summary": "Contrastive Language-Image Pre-training (CLIP) shows promise in medical image\nanalysis but requires substantial data and computational resources. Due to\nthese restrictions, existing CLIP applications in medical imaging focus mainly\non modalities like chest X-rays that have abundant image-report data available,\nleaving many other important modalities under-explored. Here, we propose the\nfirst adaptation of the full CLIP model to mammography, which presents\nsignificant challenges due to labeled data scarcity, high-resolution images\nwith small regions of interest, and data imbalance. We first develop a\nspecialized supervision framework for mammography that leverages its multi-view\nnature. Furthermore, we design a symmetric local alignment module to better\nfocus on detailed features in high-resolution images. Lastly, we incorporate a\nparameter-efficient fine-tuning approach for large language models pre-trained\nwith medical knowledge to address data limitations. Our multi-view and\nmulti-scale alignment (MaMA) method outperforms state-of-the-art baselines for\nthree different tasks on two large real-world mammography datasets, EMBED and\nRSNA-Mammo, with only 52% model size compared with the largest baseline.",
        "updated": "2024-09-26 17:56:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.18119v1"
    },
    {
        "title": "Find Rhinos without Finding Rhinos: Active Learning with Multimodal Imagery of South African Rhino Habitats",
        "authors": "Lucia GordonNikhil BehariSamuel CollierElizabeth Bondi-KellyJackson A. KillianCatherine RessijacPeter BoucherAndrew DaviesMilind Tambe",
        "links": "http://dx.doi.org/10.24963/ijcai.2023/663",
        "entry_id": "http://arxiv.org/abs/2409.18104v1",
        "pdf_url": "http://arxiv.org/pdf/2409.18104v1",
        "summary": "Much of Earth's charismatic megafauna is endangered by human activities,\nparticularly the rhino, which is at risk of extinction due to the poaching\ncrisis in Africa. Monitoring rhinos' movement is crucial to their protection\nbut has unfortunately proven difficult because rhinos are elusive. Therefore,\ninstead of tracking rhinos, we propose the novel approach of mapping communal\ndefecation sites, called middens, which give information about rhinos' spatial\nbehavior valuable to anti-poaching, management, and reintroduction efforts.\nThis paper provides the first-ever mapping of rhino midden locations by\nbuilding classifiers to detect them using remotely sensed thermal, RGB, and\nLiDAR imagery in passive and active learning settings. As existing active\nlearning methods perform poorly due to the extreme class imbalance in our\ndataset, we design MultimodAL, an active learning system employing a ranking\ntechnique and multimodality to achieve competitive performance with passive\nlearning models with 94% fewer labels. Our methods could therefore save over 76\nhours in labeling time when used on a similarly-sized dataset. Unexpectedly,\nour midden map reveals that rhino middens are not randomly distributed\nthroughout the landscape; rather, they are clustered. Consequently, rangers\nshould be targeted at areas with high midden densities to strengthen\nanti-poaching efforts, in line with UN Target 15.7.",
        "updated": "2024-09-26 17:49:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.18104v1"
    },
    {
        "title": "AI-Powered Augmented Reality for Satellite Assembly, Integration and Test",
        "authors": "Alvaro PatricioJoao ValenteAtabak DehbanInes CadilhaDaniel ReisRodrigo Ventura",
        "links": "http://arxiv.org/abs/2409.18101v1",
        "entry_id": "http://arxiv.org/abs/2409.18101v1",
        "pdf_url": "http://arxiv.org/pdf/2409.18101v1",
        "summary": "The integration of Artificial Intelligence (AI) and Augmented Reality (AR) is\nset to transform satellite Assembly, Integration, and Testing (AIT) processes\nby enhancing precision, minimizing human error, and improving operational\nefficiency in cleanroom environments. This paper presents a technical\ndescription of the European Space Agency's (ESA) project \"AI for AR in\nSatellite AIT,\" which combines real-time computer vision and AR systems to\nassist technicians during satellite assembly. Leveraging Microsoft HoloLens 2\nas the AR interface, the system delivers context-aware instructions and\nreal-time feedback, tackling the complexities of object recognition and 6D pose\nestimation in AIT workflows. All AI models demonstrated over 70% accuracy, with\nthe detection model exceeding 95% accuracy, indicating a high level of\nperformance and reliability. A key contribution of this work lies in the\neffective use of synthetic data for training AI models in AR applications,\naddressing the significant challenges of obtaining real-world datasets in\nhighly dynamic satellite environments, as well as the creation of the Segmented\nAnything Model for Automatic Labelling (SAMAL), which facilitates the automatic\nannotation of real data, achieving speeds up to 20 times faster than manual\nhuman annotation. The findings demonstrate the efficacy of AI-driven AR systems\nin automating critical satellite assembly tasks, setting a foundation for\nfuture innovations in the space industry.",
        "updated": "2024-09-26 17:44:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.18101v1"
    },
    {
        "title": "EfficientCrackNet: A Lightweight Model for Crack Segmentation",
        "authors": "Abid Hasan ZimAquib IqbalZaid Al-HudaAsad MalikMinoru Kuribayash",
        "links": "http://arxiv.org/abs/2409.18099v1",
        "entry_id": "http://arxiv.org/abs/2409.18099v1",
        "pdf_url": "http://arxiv.org/pdf/2409.18099v1",
        "summary": "Crack detection, particularly from pavement images, presents a formidable\nchallenge in the domain of computer vision due to several inherent complexities\nsuch as intensity inhomogeneity, intricate topologies, low contrast, and noisy\nbackgrounds. Automated crack detection is crucial for maintaining the\nstructural integrity of essential infrastructures, including buildings,\npavements, and bridges. Existing lightweight methods often face challenges\nincluding computational inefficiency, complex crack patterns, and difficult\nbackgrounds, leading to inaccurate detection and impracticality for real-world\napplications. To address these limitations, we propose EfficientCrackNet, a\nlightweight hybrid model combining Convolutional Neural Networks (CNNs) and\ntransformers for precise crack segmentation. EfficientCrackNet integrates\ndepthwise separable convolutions (DSC) layers and MobileViT block to capture\nboth global and local features. The model employs an Edge Extraction Method\n(EEM) and for efficient crack edge detection without pretraining, and\nUltra-Lightweight Subspace Attention Module (ULSAM) to enhance feature\nextraction. Extensive experiments on three benchmark datasets Crack500,\nDeepCrack, and GAPs384 demonstrate that EfficientCrackNet achieves superior\nperformance compared to existing lightweight models, while requiring only 0.26M\nparameters, and 0.483 FLOPs (G). The proposed model offers an optimal balance\nbetween accuracy and computational efficiency, outperforming state-of-the-art\nlightweight models, and providing a robust and adaptable solution for\nreal-world crack segmentation.",
        "updated": "2024-09-26 17:44:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.18099v1"
    },
    {
        "title": "DiffSSC: Semantic LiDAR Scan Completion using Denoising Diffusion Probabilistic Models",
        "authors": "Helin CaoSven Behnke",
        "links": "http://arxiv.org/abs/2409.18092v1",
        "entry_id": "http://arxiv.org/abs/2409.18092v1",
        "pdf_url": "http://arxiv.org/pdf/2409.18092v1",
        "summary": "Perception systems play a crucial role in autonomous driving, incorporating\nmultiple sensors and corresponding computer vision algorithms. 3D LiDAR sensors\nare widely used to capture sparse point clouds of the vehicle's surroundings.\nHowever, such systems struggle to perceive occluded areas and gaps in the scene\ndue to the sparsity of these point clouds and their lack of semantics. To\naddress these challenges, Semantic Scene Completion (SSC) jointly predicts\nunobserved geometry and semantics in the scene given raw LiDAR measurements,\naiming for a more complete scene representation. Building on promising results\nof diffusion models in image generation and super-resolution tasks, we propose\ntheir extension to SSC by implementing the noising and denoising diffusion\nprocesses in the point and semantic spaces individually. To control the\ngeneration, we employ semantic LiDAR point clouds as conditional input and\ndesign local and global regularization losses to stabilize the denoising\nprocess. We evaluate our approach on autonomous driving datasets and our\napproach outperforms the state-of-the-art for SSC.",
        "updated": "2024-09-26 17:39:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.18092v1"
    }
]