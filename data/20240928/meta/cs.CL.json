[
    {
        "title": "Open-World Evaluation for Retrieving Diverse Perspectives",
        "authors": "Hung-Ting ChenEunsol Choi",
        "links": "http://arxiv.org/abs/2409.18110v1",
        "entry_id": "http://arxiv.org/abs/2409.18110v1",
        "pdf_url": "http://arxiv.org/pdf/2409.18110v1",
        "summary": "We study retrieving a set of documents that covers various perspectives on a\ncomplex and contentious question (e.g., will ChatGPT do more harm than good?).\nWe curate a Benchmark for Retrieval Diversity for Subjective questions (BERDS),\nwhere each example consists of a question and diverse perspectives associated\nwith the question, sourced from survey questions and debate websites. On this\ndata, retrievers paired with a corpus are evaluated to surface a document set\nthat contains diverse perspectives. Our framing diverges from most retrieval\ntasks in that document relevancy cannot be decided by simple string matches to\nreferences. Instead, we build a language model based automatic evaluator that\ndecides whether each retrieved document contains a perspective. This allows us\nto evaluate the performance of three different types of corpus (Wikipedia, web\nsnapshot, and corpus constructed on the fly with retrieved pages from the\nsearch engine) paired with retrievers. Retrieving diverse documents remains\nchallenging, with the outputs from existing retrievers covering all\nperspectives on only 33.74% of the examples. We further study the impact of\nquery expansion and diversity-focused reranking approaches and analyze\nretriever sycophancy. Together, we lay the foundation for future studies in\nretrieval diversity handling complex queries.",
        "updated": "2024-09-26 17:52:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.18110v1"
    },
    {
        "title": "Infer Human's Intentions Before Following Natural Language Instructions",
        "authors": "Yanming WanYue WuYiping WangJiayuan MaoNatasha Jaques",
        "links": "http://arxiv.org/abs/2409.18073v1",
        "entry_id": "http://arxiv.org/abs/2409.18073v1",
        "pdf_url": "http://arxiv.org/pdf/2409.18073v1",
        "summary": "For AI agents to be helpful to humans, they should be able to follow natural\nlanguage instructions to complete everyday cooperative tasks in human\nenvironments. However, real human instructions inherently possess ambiguity,\nbecause the human speakers assume sufficient prior knowledge about their hidden\ngoals and intentions. Standard language grounding and planning methods fail to\naddress such ambiguities because they do not model human internal goals as\nadditional partially observable factors in the environment. We propose a new\nframework, Follow Instructions with Social and Embodied Reasoning (FISER),\naiming for better natural language instruction following in collaborative\nembodied tasks. Our framework makes explicit inferences about human goals and\nintentions as intermediate reasoning steps. We implement a set of\nTransformer-based models and evaluate them over a challenging benchmark,\nHandMeThat. We empirically demonstrate that using social reasoning to\nexplicitly infer human intentions before making action plans surpasses purely\nend-to-end approaches. We also compare our implementation with strong\nbaselines, including Chain of Thought prompting on the largest available\npre-trained language models, and find that FISER provides better performance on\nthe embodied social reasoning tasks under investigation, reaching the\nstate-of-the-art on HandMeThat.",
        "updated": "2024-09-26 17:19:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.18073v1"
    },
    {
        "title": "IFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning",
        "authors": "Soeun LeeSi-Woo KimTaewhan KimDong-Jin Kim",
        "links": "http://arxiv.org/abs/2409.18046v1",
        "entry_id": "http://arxiv.org/abs/2409.18046v1",
        "pdf_url": "http://arxiv.org/pdf/2409.18046v1",
        "summary": "Recent advancements in image captioning have explored text-only training\nmethods to overcome the limitations of paired image-text data. However,\nexisting text-only training methods often overlook the modality gap between\nusing text data during training and employing images during inference. To\naddress this issue, we propose a novel approach called Image-like Retrieval,\nwhich aligns text features with visually relevant features to mitigate the\nmodality gap. Our method further enhances the accuracy of generated captions by\ndesigning a Fusion Module that integrates retrieved captions with input\nfeatures. Additionally, we introduce a Frequency-based Entity Filtering\ntechnique that significantly improves caption quality. We integrate these\nmethods into a unified framework, which we refer to as IFCap\n($\\textbf{I}$mage-like Retrieval and $\\textbf{F}$requency-based Entity\nFiltering for Zero-shot $\\textbf{Cap}$tioning). Through extensive\nexperimentation, our straightforward yet powerful approach has demonstrated its\nefficacy, outperforming the state-of-the-art methods by a significant margin in\nboth image captioning and video captioning compared to zero-shot captioning\nbased on text-only training.",
        "updated": "2024-09-26 16:47:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.18046v1"
    },
    {
        "title": "Unveiling the Role of Pretraining in Direct Speech Translation",
        "authors": "Belen AlastrueyGerard I. GállegoMarta R. Costa-jussà",
        "links": "http://arxiv.org/abs/2409.18044v1",
        "entry_id": "http://arxiv.org/abs/2409.18044v1",
        "pdf_url": "http://arxiv.org/pdf/2409.18044v1",
        "summary": "Direct speech-to-text translation systems encounter an important drawback in\ndata scarcity. A common solution consists on pretraining the encoder on\nautomatic speech recognition, hence losing efficiency in the training process.\nIn this study, we compare the training dynamics of a system using a pretrained\nencoder, the conventional approach, and one trained from scratch. We observe\nthat, throughout the training, the randomly initialized model struggles to\nincorporate information from the speech inputs for its predictions. Hence, we\nhypothesize that this issue stems from the difficulty of effectively training\nan encoder for direct speech translation. While a model trained from scratch\nneeds to learn acoustic and semantic modeling simultaneously, a pretrained one\ncan just focus on the latter. Based on these findings, we propose a subtle\nchange in the decoder cross-attention to integrate source information from\nearlier steps in training. We show that with this change, the model trained\nfrom scratch can achieve comparable performance to the pretrained one, while\nreducing the training time.",
        "updated": "2024-09-26 16:46:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.18044v1"
    },
    {
        "title": "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions",
        "authors": "Kai ChenYunhao GouRunhui HuangZhili LiuDaxin TanJing XuChunwei WangYi ZhuYihan ZengKuo YangDingdong WangKun XiangHaoyuan LiHaoli BaiJianhua HanXiaohui LiWeike JinNian XieYu ZhangJames T. KwokHengshuang ZhaoXiaodan LiangDit-Yan YeungXiao ChenZhenguo LiWei ZhangQun LiuLanqing HongLu HouHang Xu",
        "links": "http://arxiv.org/abs/2409.18042v1",
        "entry_id": "http://arxiv.org/abs/2409.18042v1",
        "pdf_url": "http://arxiv.org/pdf/2409.18042v1",
        "summary": "GPT-4o, an omni-modal model that enables vocal conversations with diverse\nemotions and tones, marks a milestone for omni-modal foundation models.\nHowever, empowering Large Language Models to perceive and generate images,\ntexts, and speeches end-to-end with publicly available data remains challenging\nin the open-source community. Existing vision-language models rely on external\ntools for the speech processing, while speech-language models still suffer from\nlimited or even without vision-understanding abilities. To address this gap, we\npropose EMOVA (EMotionally Omni-present Voice Assistant), to enable Large\nLanguage Models with end-to-end speech capabilities while maintaining the\nleading vision-language performance. With a semantic-acoustic disentangled\nspeech tokenizer, we notice surprisingly that omni-modal alignment can further\nenhance vision-language and speech abilities compared with the corresponding\nbi-modal aligned counterparts. Moreover, a lightweight style module is proposed\nfor flexible speech style controls (e.g., emotions and pitches). For the first\ntime, EMOVA achieves state-of-the-art performance on both the vision-language\nand speech benchmarks, and meanwhile, supporting omni-modal spoken dialogue\nwith vivid emotions.",
        "updated": "2024-09-26 16:44:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.18042v1"
    }
]