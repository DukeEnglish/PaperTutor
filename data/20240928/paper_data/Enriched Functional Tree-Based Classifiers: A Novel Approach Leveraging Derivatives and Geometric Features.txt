Enriched Functional Tree-Based Classifiers: A Novel
Approach Leveraging Derivatives and Geometric Features
FabrizioMATURO*
DepartmentofEconomics,StatisticsandBusiness,
FacultyofTechnologicalandInnovationSciences,
UniversitasMercatorum,Rome,Italy
AnnamariaPORRECA†
DepartmentofEconomics,StatisticsandBusiness,
FacultyofEconomicsandLaw,
UniversitasMercatorum,Rome,Italy
Abstract
The positioning of this research falls within the scalar-on-function classification literature, a field
of significant interest across various domains, particularly in statistics, mathematics, and com-
puter science. This study introduces an advanced methodology for supervised classification by
integrating Functional Data Analysis (FDA) with tree-based ensemble techniques for classifying
high-dimensional time series. The proposed framework, Enriched Functional Tree-Based Classi-
fiers (EFTCs), leverages derivative and geometric features, benefiting from the diversity inherent
in ensemble methods to further enhance predictive performance and reduce variance. While our
approach has been tested on the enrichment of Functional Classification Trees (FCTs), Functional
K-NN (FKNN), Functional Random Forest (FRF), Functional XGBoost (FXGB), and Functional
LightGBM(FLGBM),itcouldbeextendedtoothertree-basedandnon-tree-basedclassifiers,with
appropriateconsiderationsemergingfromthisinvestigation. Throughextensiveexperimentalevalu-
ationsonsevenreal-worlddatasetsandsixsimulatedscenarios,thisproposaldemonstratesfascinat-
ingimprovementsovertraditionalapproaches, providingnewinsightsintotheapplicationofFDA
incomplex,high-dimensionallearningproblems.
Keywords: Functionaldataanalysis,derivatives,geometricfeatures,enrichedfunctionaltree-based
classifiers,supervisedclassification,enrichedfunctionalrandomforest.
1 Introduction
In today’s world, data is collected from diverse sources such as biomedical devices, smartphones, and
environmentalsensors,andusedacrossapplicationsinhealthcare,environmentalmonitoring,andmore.
Technological advancements have improved our capacity to store and process this data, but managing
high-dimensional datasets remains a challenge. Dimensionality reduction and classification techniques
are essential for effectively handling such data in fields like medicine, environmental monitoring, se-
curity, and robotics. Key issues include irregularly spaced time points, computational complexity, the
bias-variance trade-off, and the need for interpretable models with strong performance metrics such as
accuracy,precision,andrecall.
In the supervised classification literature, one of the most well-known challenges is the curse of
dimensionality, which arises whenever dealing with a large number of variables or, in the context of
time series, when there are many time points. This issue impacts numerous statistical aspects, such as
distancemeasures,theidentificationofcausalrelationships,orfindingthebest-performingmodelwhen
*fabrizio.maturo@unimercatorum.it
†annamaria.porreca@unimercatorum.it
1
4202
peS
62
]LM.tats[
1v40871.9042:viXramanymodelswithsimilarperformanceexistbutrelyondifferentvariables. Italsointroducesproblems
likedatasparsityandmulticollinearity. Forthesereasons,thechallengeofbothsupervisedandunsuper-
visedclassificationinhigh-dimensionaldata, whetheritinvolvesnumerousdifferentvariablesormany
timepointsforthesamevariable,remainsacomplexandrelevantareaofresearchinmathematics,statis-
tics, and computer science. Functional data analysis (FDA) is a research area that has actively tackled
many of these challenges over the past decades. In FDA, dimensionality reduction is inherent, as it is
achievable simply through the representation of the data itself. More generally, FDA represents a sta-
tisticaldomainfocusedonthetheoryandapplicationofstatisticalmethodsinscenarioswheredatacan
beexpressedasfunctions,contrastingwiththetraditionalrepresentationusingrealnumbersorvectors.
FDAintroducesaparadigmshiftinstatisticalconcepts,representations,modeling,andpredictivetech-
niques by treating functions as single entities. The benefits of employing FDA have been extensively
discussed in contemporary literature, including the utilization of derivatives when they provide more
insight than the original functions due to the nature of the phenomena, the adoption of non-parametric
strategieswithoutrestrictiveassumptions,datadimensionalityreduction,andtheexploitationofcritical
sources of pattern and variation [Ramsay and Silverman, 2002, Ferraty and Vieu, 2003, Ramsay and
Silverman,2005,Ferraty,2011].
The literature on FDA is currently dynamic and highly relevant, especially in regression, ANOVA,
unsupervised classification, supervised classification, and outlier detection. Within this broad frame-
work, we focus on supervised classification with functional predictors and a scalar response variable.
Recent research has explored the development of classification methods that combine the strengths of
FDA and tree-based techniques. [Yu and Lambert, 1999] advocated using spline trees for functional
data,applyingthemtoanalyzetime-of-daypatternsforinternationalcallcustomers. Assessingvariable
importanceinthefusionofFDAandtree-basedmethodswasthefocusoftheworkby[Gregoruttietal.,
2015]. [Möller et al., 2016] proposed a classification approach based on random forests for functional
covariates. Investigating the construction of a classifier for dose-response predictions involving curve
outcomes was the aim of [Rahman et al., 2019]. [Maturo and Verde, 2023] proposed using functional
principalcomponentstotrainaclassificationtree. [MaturoandVerde,2022a]suggestedcombiningclus-
tering and supervised tree-based classification to enhance prediction model accuracy. Finally, [Maturo
andVerde,2022b]proposedaninnovativeevaluationofleaves’qualityforfunctionalclassificationtrees
appliedtobiomedicalsignalswithbinaryoutcomes. Moindjiéetal.[2024]exploremethodsforclassi-
fying multivariate functional data adapting and extending PLS techniques to handle the complexity of
functional data across varying domains. Brault et al. [2024] propose a mixture-based segmentation ap-
proach for heterogeneous functional data, aimed at identifying hidden structures and subgroups within
complexfunctionaldatasetsbycombiningmultiplesegmentations. Recently,Riccioetal.[2024a]pro-
posed to exploit functional representation to increase diversity in ensemble methods and improve the
accuracyofclassifiers. Finally, Riccioetal.[2024b]suggestedanewalgorithmtoexploittheprevious
idea but further improving the accuracy and variance of estimates. Building on the established foun-
dation of combining FDA and statistical learning techniques, significant exploration is still needed to
handlelargedatasetsandinterpretresultsfrombothstatisticalandcausalperspectives. Researchinthis
area is rapidly evolving and holds great potential. We expect a growing focus on improving functional
classifiers’precision,interpretability,andexplainabilityinthecomingyears.
Leveragingthislandscapeanditsvastresearchopportunities,thispaperintroducesanovelfunctional
supervised classification framework, namely the Enriched Functional Tree-Based Classifiers (EFTCs).
Toaddressthechallengeoflearningfromhigh-dimensionaldataandenhancingfunctionalclassification
performancebyleveragingadditionalcharacteristicsoftheoriginaldata,derivatives,curvature,radiusof
curvature,andelasticityareusedtoenrichtheinformationprovidedtofunctionalclassificationtreeen-
sembles. Inotherwords,werefertoEFTCstodenotethejointutilisationofsequentialtransformations
forextractingunexploredfeaturesfromtheoriginalsignals. Inessence,thisapproachinvolvesviewing
functionsfromdiverseperspectivestocaptureadditionalaspectsthatcancontributetoenhancingclassi-
ficationperformance. Practically,itislikeusingamagnifyingglasstorevealattributesthattheoriginal
functions may miss. Moreover, the motivation behind this proposal is also driven by the well-known
2fact that ensemble methods, such as tree-based classifiers, benefit significantly from introducing diver-
sity,asittendstoimprovegeneralisationandperformance. Byenrichingthefeaturespacewithdiverse
functional characteristics, ETBCs can leverage this diversity to further enhance classification accuracy,
exploitingthestrengthsofeachtransformationtocapturecomplementaryinformationfromthedata.
The paper conducts extensive experimental evaluations on seven real-world datasets and six sim-
ulated signals to measure the proposed methodology’s efficacy. Comparative analyses with existing
methodsrevealpromisingresultsintermsofclassificationperformance. Thestudyyieldspromisingre-
sults,indicatingthattheenrichmentapproachsignificantlyimprovesperformancewithcertainmethods.
While our approach has been tested on functional classification trees, KNN, random forest, XGBoost,
and LightGBM. However, it can be extended to other tree-based or non-tree-based classifiers, with ap-
propriateadjustmentsbasedonourfindings. Thisframeworkdemonstratesnotableimprovementsover
traditionalmethods,offeringvaluableinsightsintoapplyingFDAincomplex,high-dimensionallearning
problems.
Thepaper’sstructureisasfollows: Section2introducesthecoreconceptsofFDA,EnrichedFunc-
tional Data, and the Enriched Functional Classification frameworks, including trees, random forests,
XGBoost, andLightGBM.Section3coversapplyingtheproposedmethodstorealandsimulateddata.
Section4discusseskeyissuesrelatedtomodelexplainability. Finally,Section5concludesthepaperby
discussingthemainfindingsandhighlightingdirectionsforfutureresearch.
2 Material and methods
2.1 DataRepresentationintheFunctionalDataAnalysis(FDA)framework
In FDA, the fundamental concept revolves around treating data functions as distinct entities. However,
in practical scenarios, functional data is frequently encountered as discrete data points. This means
thattheoriginalfunction,denotedbyz= f(x),istransformedintoacollectionofdiscreteobservations
represented by T pairs (x ,z ), where x denotes the points at which the function is assessed, and z
j j j j
represents the corresponding function values at those points. We define a functional variable X as a
random variable with values in a functional space Ξ. Accordingly, a functional data set is a sample
x ,x ,...,x drawn from the functional variable X [Ramsay and Silverman, 2005, 2002, Ramsay et al.,
1 2 N
2009,FerratyandVieu,2003]. FocusingspecificallyonthecaseofaHilbertspacewithametricd(·,·)
associatedwithanorm, suchthatd(x (t),x (t))=|x (t)−x (t)|, andwherethenorm|·|isassociated
1 2 1 2
withaninnerproduct⟨·,·⟩,suchthat|x(t)|=⟨x(t),x(t)⟩1/2,wecanderivethespaceL2 ofrealsquare-
(cid:82)
integrable functions defined on τ by ⟨x (t),x (t)⟩= x (t)x (t)dt, where τ is a Lebesgue measurable
1 2 τ 1 2
set on T. Therefore, considering the specific case of L , a basis function system comprises a set of
2
linearlyindependentfunctionsφ (t)thatspanthespaceL [RamsayandSilverman,2005].
j 2
The initial step in FDA involves transforming the observed values z ,z ,...,z for each unit i=
i1 i2 iT
1,2,...,N into a functional form. The prevalent method for estimating functional data is basis approx-
imation. Various basis systems can be employed depending on the characteristics of the curves. A
common approach is to represent functions using a finite set of basis functions in a fixed basis system.
Thiscanbemathematicallyexpressedas:
S
x(t)≈ ∑c φ (t), (1)
i is s
s=1
where,c =(c ,...,c )T representsthevectorofcoefficientsdefiningthelinearcombination,φ (t)isthe
i i1 iS s
s-th basis function, and S is a finite subset of functions used to approximate the complete basis expan-
sion. Another trending methodology involves leveraging a data-driven basis, with Functional Principal
Components(FPCs)decomposition. Thisapproacheffectivelyreducesdimensionalitywhilepreserving
essentialinformationfromtheoriginaldataset[RamsayandSilverman,2005,Febrero-Bandeanddela
Fuente,2012]. Inthiscontext,theapproximationoffunctionaldatacanbeexpressedasfollows:
3K
x(t)≈ ∑ν ξ (t), (2)
i ik k
k=1
whereK isthenumberofFPCs, ν representsthescoreofthegenericFPCξ forthegenericfunction
ik k
x (i=1,2,...,N). By reducing this representation to the initial p FPCs, we obtain an estimate of the
i
p
samplecurves,andtheexplainedvarianceisgivenby∑ k=1λ k,whereλ
k
denotesthevarianceassociated
with the k-th functional principal component. The construction of the FPCs approximation is designed
suchthatthevarianceexplainedbythek-thFPCdecreaseswithincreasingvaluesofk.
Various domains, such as time, space, or other parameters, can represent the variable T. The re-
sponsecanbecategoricalornumerical,leadingtoclassificationorregressionchallenges. However,this
study is specifically concerned with a particular scenario: a scalar-on-function classification problem.
In functional classification, the objective is to forecast the class or labelY for an observation X within
a separable metric space (Ξ,d). Consequently, our methodology is tailored for functional data repre-
sented as y,x(t), where x(t) is a predictor curve defined fort ∈T, and y denotes the scalar response
i i i i
observedatsamplei=1,...,N. TheclassificationofanovelobservationxfromX involvesthecreation
of a mapping f :Ξ−→{0,1,...,C}, referred to as a “classifier”, which assigns x to its predicted label.
TheerrorprobabilityisquantifiedbyP{f(X)̸=Y}.
2.2 EnrichedFunctionalFeatures
2.2.1 FunctionalDerivatives
Let the functional derivative of order r for the i-th curve be represented by a fixed basis system (e.g.
b-splines)as:
S
x(r)
(t)=
∑c(r) φ(r)
(t) j=1,...,S (3)
i ij j
j=1
(r) (r)
where c is the coefficient of the i-th curve, j-th b-spline, and r-th derivative order; φ (t) is the r-th
ij j
derivativeofthe j-thbasisfunction.
Ramsay and Silverman [2005] stressed that the selection of the basis system plays a crucial role
in estimating derivatives. It is essential to ensure that the chosen basis for representing the object can
accommodatetheorderofthederivativetobecalculated. Inthecaseofb-splinebases,thisimpliesthat
thespline’sordermustbeatleastonehigherthantheorderofthederivativeunderconsideration. Inthis
research,weconcentrateonab-splinebasisoffourthorder.
Inthefollowingsections,wewilllimitourattentiontothefirsttwoderivatives. Thefirstderivative
ofthefunctionx(t)intheB-splinerepresentationisgivenby:
i
S
x(1)
(t)=
∑c(1) φ(1)
(t) (4)
i ij j
j=1
(1) (1)
wherec arethecoefficientscorrespondingtothefirstderivativeofthefunction,andφ (t)isthefirst
ij j
derivativeofthe j-thB-splinebasisfunction.
Similarly,thesecondderivativeofthefunctionx(t)canbeexpressedas:
i
S
x(2)
(t)=
∑c(2) φ(2)
(t) (5)
i ij j
j=1
(2) (2)
wherec arethecoefficientsforthesecondderivativeofthefunction,andφ (t)isthesecondderiva-
ij j
tiveofthe j-thB-splinebasisfunction.
In the context of functional supervised classification, B-spline versions of derivatives enhance the
representationoffunctionalfeaturesinthedatabyprovidingadditionalinformationonlocalvariations
of the curves, such as local speed and acceleration, which can be crucial for distinguishing between
4different classes. In supervised classification, the speed at which a functional signal changes over time
can be a key factor for class separation. For example, knowing how the heart rate varies over time in
a medical dataset becomes more illuminating when considering the speed of these changes at different
time intervals. On the other hand, acceleration can indicate specific events or sharp changes that help
differentiateoneclassfromanother,thusfurtherimprovingtheaccuracyofthemodel. Additionally,B-
splinederivativesallowforsmoothedandstablederivativecalculationsthatarelessnoise-sensitivethan
directly computed derivatives. This enriches the feature set used by classification models, improving
predictiveperformanceandclassrecognition.
2.2.2 FunctionalCurvatureandRadiusofCurvature
The curvature κ(t) of the function x(t) is a measure of how rapidly the function changes direction at
i
eachpointt. Thecurvatureisdefinedas:
(2)
|x (t)|
κ(t)= i (6)
(cid:18)
(cid:16)
(cid:17)2(cid:19)3/2
(1)
1+ x (t)
i
(1) (2)
wherex (t)isthefirstderivativeandx (t)isthesecondderivativeofthefunctionx(t).
i i i
(2)
Thenumerator|x (t)|representsthemagnitudeoftheacceleration,whilethedenominatoradjusts
i
fortheinfluenceoftheslopetoensurethatthecurvatureisindependentofthescaleoft. Thisexpression
providesacomprehensivemeasureofthefunction’stendencytobendateachpointt,capturingboththe
speedofchangeandtherateatwhichthisspeeditselfchanges.
Thecurvatureκ(t)ofthefunctionx(t)canalsobedefinedintermsofB-splinebasisasfollows:
i
(cid:12) (cid:12)
(cid:12)∑S c(2) φ(2) (t)(cid:12)
(cid:12) j=1 ij j (cid:12)
κ(t)= (7)
(cid:18)
(cid:16)
(cid:17)2(cid:19)3/2
1+ ∑S c(1) φ(1) (t)
j=1 ij j
(1) (2)
where c and c are the coefficients corresponding to the first and second derivatives of the function
ij ij
(1) (2)
x(t), and φ (t) and φ (t) are the first and second derivatives of the j-th B-spline basis function,
i j j
respectively.
To use the curvature κ(t) in a classifier, we need to extract the curvature coefficients associated
with the B-spline basis functions. However, directly extracting coefficients from the above nonlinear
expressionforcurvatureischallengingbecauseitinvolvesanonlinearcombinationoftheB-splinebasis
functions. To overcome this, for practical use in classification, we can use the following steps. First,
we compute the curvature κ(t) at a set of sampled points t ,t ,...,t over the domain τ. This results
1 2 M
in a vector of curvature values κ(t ),κ(t ),...,κ(t ). Next, we fit these discrete curvature values to a
1 2 M
B-splinebasis:
S
κ(t)≈ ∑d φ (t) (8)
ik j
k=1
where φ (t) are the B-spline basis functions, and d are the coefficients representing the curvature in
j ik
the B-spline basis. The coefficients d extracted from the B-spline fit are then used as features in the
ik
classifier.
TheradiusofcurvatureR(t)isdefinedasthereciprocalofthecurvatureκ(t):
1 1
R(t)= ≈ (9)
κ(t) ∑S k=1d ikφ j(t)
In this form, the radius of curvature is represented as the reciprocal of the B-spline expansion of
curvature. However,thisformisnotlinear,whichcomplicatesdirectcoefficientextraction. Tofacilitate
5coefficient extraction, we can compute the radius of curvature at sampled points and then refit these
valuesusingaB-splinebasis:
S
R(t)≈ ∑e φ (t) (10)
ik j
k=1
where φ (t) are the B-spline basis functions, and e are the coefficients representing the radius of cur-
j ik
vatureintheB-splinebasis.
Figure1illustratestwoexamplesofthegeometricinterpretationofcurvatureandradiusofcurvature
for a smooth curve. The blue curve represents the original function, while the purple circle is the
osculating circle at a point of interest. The red dot marks the point on the curve where the curvature
is being evaluated. The closeness of the osculating circle to the curve at this location visualizes the
curvature.
Curvature measures how sharply the curve bends at a given point, while the radius of curvature,
being the reciprocal of the curvature, provides insight into the tightness or gentleness of this bend. A
high curvature results in a small radius of curvature, indicating a sharp turn, whereas a low curvature
corresponds to a larger radius, reflecting a gentler bend. The radius of curvature is depicted as the dis-
tancefromthereddottothecenteroftheosculatingcircle. Curvatureandradiusofcurvaturedescribe
the local geometric properties of a curve and are valuable features for supervised classification. They
providecomplementaryinsights: curvaturehighlightssharplocalchangesindirection,makingitcrucial
for detecting sudden transitions in the signal, while the radius of curvature offers additional context on
how the curve behaves over a wider range. These features are particularly useful when distinguishing
between different classes in time series or functional data. By incorporating both curvature and radius
ofcurvature,classificationmodelsgainaricherunderstandingofthesignal’slocalandglobalgeometry.
Thesefeaturescapturelocalshapedetails,suchasturningpointsorabruptchanges,whichmayindicate
aspecificclass. Forinstance,inamedicaldataset,significantvariationsincurvaturecouldsignalpatho-
logical conditions. Moreover, curvature and radius of curvature are robust to translations and scaling,
which enhances their ability to generalize across different datasets. This robustness makes them valu-
able forimproving classification models, asthey helppreserve importantgeometric patternsregardless
ofhowthedataispresented.
Figure1: Curvatureandradiusofcurvatureandtheirgeometricalinterpretation.
2.2.3 FunctionalElasticity
The elasticity E(t) of a function x(t) is a measure of how responsive the function is to changes in its
i
input,oftenexpressedastheproductofthefirstderivativeofthefunctionandtheratiooftheinputt to
thefunctionvaluex(t):
i
t
(1)
E(t)=x (t)· (11)
i x(t)
i
6(1)
Given that the first derivative x (t) and the function x(t) can be expressed using B-spline basis
i i
functions,theelasticitycanberepresentedas:
(cid:32) (cid:33)
S t
E(t)=
∑c(1)
φ (t) · (12)
j=1
ij j ∑S j=1c ijφ j(t)
(1)
Here, c are the coefficients for the first derivative, and c are the coefficients for the original
ij ij
function, both represented using the same B-spline basis φ (t). This expression, however, is not linear
j
duetotheratio t ,makingdirectcoefficientextractioncomplex.
xi(t)
To facilitate the extraction of coefficients for elasticity, we can compute the elasticity at sampled
pointsandthenrefitthesevaluesusingaB-splinebasis:
S
E(t)≈ ∑ f φ (t) (13)
ik k
k=1
whereφ (t)aretheB-splinebasisfunctions,and f arethecoefficientsrepresentingtheelasticityinthe
k ik
B-splinebasis.
Elasticity offers a different perspective from other geometric features, such as curvature or radius
of curvature, focusing specifically on the function’s rate of change relative to the input itself. While
curvature deals with how sharply a function bends, elasticity quantifies the proportional change of the
functioninresponsetochangesintheindependentvariable. Thisadditionalinformationcanbecrucial
incaseswherethemagnitudeoftheinputplaysaroleininterpretingthedynamicsofthesignal.
Onekeyaspectofelasticityisitsabilitytocapturescale-invariantpropertiesofthefunction. Unlike
curvature, which focuses on the geometry of the curve, elasticity reflects how the function reacts to
the growth or decline of its input, making it highly relevant in scenarios where relative change matters
more than absolute values. This is especially useful in fields like economics, where the proportional
responsiveness of variables is more significant than their absolute variations, or in biological systems
whereresponsetostimulimayscalewithintensity.
In supervised classification, elasticity highlights the signal’s sensitivity to changes in the indepen-
dent variable over time. For instance, in time series classification, elasticity could identify periods of
rapid growth or decay that differentiate one class from another, such as distinguishing between stable
versus volatile behaviour periods in financial data. Another vital aspect is elasticity’s ability to reveal
non-linearrelationshipsbetweentheinputandoutput. Unlikederivative-basedmeasuresthatarelinear,
elasticity incorporates both the function’s value and its rate of change, capturing a richer, non-linear
interaction. Finally, elasticitycomplementsfeatureslikecurvaturebyfocusingoninput-outputrespon-
siveness, making it useful for applications requiring a holistic understanding of local behaviours and
global trends. When used together in classification tasks, these features provide a more nuanced un-
derstanding of the signal’s behaviour, enriching the feature space and improving the model’s ability to
capturediversepatterns.
2.2.4 TheEnrichedFunctionalFeaturesMatrix
Usingfixedsystems,thematrixoffeaturesfortheoriginalfunctionalrepresentationisdeterminedby:
 
c ... c
11 1S
C=  . . . ... . . .  , (14)
c ... c
N1 NS
where the generic element c is the coefficient of the i-th curve (i=1,...,N) relative to the s-th (s=
is
1,...,S) basis function φ (t). As a natural consequence, c is the vector containing the i-th statistical
s i
unit’scharacteristics.
Incorporatingcoefficientsderivedfromderivatives,curvature,radiusofcurvature,andelasticity,we
have:
71. FirstDerivativeCoefficients:
 (1) (1)
c ... c
11 1S
C(1)=  . . . ... . . .  , (15)
(1) (1)
c ... c
N1 NS
(1)
wherec arethecoefficientsassociatedwiththefirstderivativeofthefunction.
is
2. SecondDerivativeCoefficients:
 (2) (2)
c ... c
11 1S
C(2)=  . . . ... . . .  , (16)
(2) (2)
c ... c
N1 NS
(2)
wherec arethecoefficientsassociatedwiththesecondderivativeofthefunction.
is
3. CurvatureCoefficients:
 
d ... d
11 1S
D=  . . . ... . . .  , (17)
d ... d
N1 NS
whered arethecoefficientsrepresentingthecurvatureintheB-splinebasis.
is
4. RadiusofCurvatureCoefficients:
 
e ... e
11 1S
E=  . . . ... . . .  , (18)
e ... e
N1 NS
wheree arethecoefficientsrepresentingtheradiusofcurvatureintheB-splinebasis.
is
5. ElasticityCoefficients:
 
f ... f
11 1S
F=  . . . ... . . .  , (19)
f ... f
N1 NS
where f arethecoefficientsrepresentingtheelasticityintheB-splinebasis.
is
By incorporating these additional coefficients into the feature matrix, we can significantly enhance
the power of the functional classifiers. The complete feature matrix, combining all these elements, can
bewrittenas:
 
(1) (1) (2) (2)
c ··· c c ··· c c ··· c d
11 1S 11 1S 11 1S 11
d e ··· e f ··· f ··· ··· 
 1S 11 1S 11 1S 
S=  . . . . . . ... . . . . . . ... . . . . . . ... . . .   (20)
 
 (1) (1) (2) (2) 
c ··· c c ··· c c ··· c d
 N1 NS N1 NS N1 NS N1
d e ··· e f ··· f ··· ···
NS N1 NS N1 NS
With the theoretical framework presented above, in the subsequent application and simulations,
we focus on the case presented in Equation 20 to demonstrate the potential of the proposed functional
informationenhancementviab-splinedecomposition. Equation20representstheEnrichedfeaturesused
totrainthedifferentfunctionalclassifiers. Itisimportanttoemphasizethatthecurvesinthetestsetare
alsorepresentedusingthesamefixedB-splinebasissystemasthetrainingset. Sincethebasisfunctions
are fixed and predefined, the representation of test set curves is consistent with that of the training set.
8Thisapproachensuresthatthecoefficientsderivedfromthetestcurvesaredirectlycomparabletothose
obtainedfromthetrainingcurves.
Incontrast,withadata-drivenbasissystem,wherethebasisfunctionsarederivedfromthedataitself
(e.g., using functionalprincipal component analysis), we wouldface the challenge ofhaving to project
thetest setcurves ontoa potentiallydifferent setofbasis functionsthan thoseused forthe trainingset.
This could lead to inconsistencies and complications, as the basis functions might differ depending on
the specific characteristics of the training data. By using a fixed basis system, we avoid these issues,
allowingforamorestraightforwardandrobustmodelapplicationtonew,unseendata.
2.3 EnrichedFunctionalClassificationTrees
In the context of functional data analysis (FDA), classifying functional observations into distinct cat-
egories based on their intrinsic properties is a central problem. Suppose we have a set of functional
observations{x(t)}N ,whereeachx(t)isafunctiondefinedoveradomaint∈τ,andy ∈{1,2,...,C}
i i=1 i i
represents the categorical outcome associated with each function. The task of functional classification
theninvolvesfindingamapping f :RS→{1,2,...,C}suchthat:
yˆ = f(S) (21)
i i
where yˆ is the predicted class label for the i-th observation and S they are the scores that concern the
i
curvei.
By using the entire set of B-spline coefficients S as input features, we ensure that the classifier
can comprehensively represent the functional data. This allows the classifier to effectively distinguish
between different classes based on the shape and other characteristics of the functions. Furthermore,
becausetheB-splinebasisisfixedacrossthetrainingandtestsets,therepresentationremainsconsistent,
avoidingthecomplexitiesarisingfromdata-drivenbasissystems,wherethebasisfunctionscoulddiffer
betweendatasets.
The core idea behind Enriched Functional Classification Trees (EFCTs) is to extend the classical
decisiontreemethodologybyincorporatingfeaturesderivedfromfunctionaldatarepresentations. This
isdoneexplicitlyusingB-splinecoefficientsoftheoriginalfunctionanditsvariousfunctionaltransfor-
mations,includingthefirstandsecondderivatives,curvature,radiusofcurvature,andelasticity.
LetS denotethevectorofB-splinecoefficientsassociatedwiththei-thfunctionalobservation. This
i
vectoriscomposedofcoefficientsderivedfromtheoriginalfunctionx(t),aswellasitsfirstandsecond
i
derivatives,curvature,radiusofcurvature,andelasticity:
(cid:16) (cid:17)⊤
(1) (1) (2) (2)
S = c ,...,c ,c ,...,c ,c ,...,c ,d ,...,d ,e ,...,e ,f ,...,f (22)
i i1 iS i1 iS i1 iS i1 iS i1 iS i1 iS
where:
• c aretheB-splinecoefficientsfortheoriginalfunctionx(t),
ij i
(1) (2)
• c andc arethecoefficientsforthefirstandsecondderivatives,respectively,
ij ij
• d , e , and f are the coefficients corresponding to the curvature, radius of curvature, and elas-
ij ij ij
ticity,respectively.
ThefeaturevectorS providesacomprehensiverepresentationofthei-thfunctionaldata,capturing
i
its various transformations and ensuring that the functional nature of the data is effectively utilized
within the decision tree framework. In EFCTs, each split in the tree is based on one or more of these
coefficients, allowing the tree to make decisions that are sensitive to specific parts of the functional
domainτ anddifferentordersofderivatives.
Thetaskofclassificationcanthenbeexpressedasfindingamapping f :RpS→{1,2,...,C},wherep
isthetotalnumberoffunctionaltransformationsconsidered(includingtheoriginalfunction,derivatives,
curvature,etc.):
9yˆ = f(S) (23)
i i
whereyˆ isthepredictedclasslabelforthei-thobservation.
i
ThefeaturematrixusedtotrainthefunctionalclassifiersisconstructedfromtheB-splinecoefficients
of all functional transformations considered. This matrix, as illustrated in Equation 20, serves as the
inputtotheEFCTmodel.
 
S
1
S 2
X= .  (24)
 . 
 . 
S
N
Obtained as a natural extension of the functional classification tree proposed by Maturo and Verde
[2023], that however focuses on the functional principal components scores of the original functions
only,theEFCTalgorithmisillustratedinAlgorithm1.
Algorithm1EnrichedFunctionalClassificationTree(EFCT)Algorithm
1: Input: Training data {(S i,y i)}N i=1, where S i is the feature vector of B-spline coefficients and y i is
thecategoricaloutcome.
2: Output: Aclassificationtreeforpredictingclasslabels.
3: procedureBUILDTREE({(S i,y i)}N i=1)
4: ifStoppingcriteriaaremetthen
5: Returnaterminalnodewithclasslabelassignedbasedonthemajorityclassinthenode.
6: else
7: Select the best coefficient S ik and threshold θ based on the splitting criterion (e.g., Gini
impurity,informationgain).
8: Partitionthedataintotwosubsets:
9: Leftsubset: {(S i,y i)|S ik ≤θ}
10: Rightsubset: {(S i,y i)|S ik >θ}
11: Recursivelyapply BUILDTREEtotheleftandrightsubsets.
12: Returnthecurrentnodewiththesplittingruleandchildnodes.
13: endif
14: endprocedure
15: procedurePREDICT(S new)
16: Startattherootnodeofthetree.
17: whilecurrentnodeisnotaterminalnodedo
18: ifS k(S new)≤θ then
19: Movetotheleftchildnode.
20: else
21: Movetotherightchildnode.
22: endif
23: endwhile
24: Returntheclasslabeloftheterminalnode.
25: endprocedure
26: Trainthetree: BUILDTREE({(S i,y i)}N i=1)
27: Makepredictions: PREDICT(S new)
When reasoning with a single tree, we can think of pruning it using classical methods like cost-
complexity pruning to avoid overfitting. The increase in available features caused by enrichment with
functional transformations makes pruning necessary to prevent poor generalization ability. As in the
classic case, a single EFCT, although quite accurate and easy to interpret, suffers from high variance,
10as will also be illustrated in the application section. For this reason, referring to ensemble methods is
increasinglyadvantageous,atleastintermsofestimationperformance.
2.4 EnrichedEnsemblesMethodsforFunctionalonScalarClassificationProblems
While ECTs provide a robust framework for functional data classification using B-spline coefficients,
they can be further enriched through ensemble methods. Ensemble methods, such as Random Forests,
XGBoost,andLightGBM,leveragethestrengthsofmultiplemodelstoimprovepredictiveperformance,
reducevariance,andincreaserobustness.
2.4.1 EnrichedFunctionalRandomForests(EFRF)
The Random Forest algorithm is a natural extension of decision trees, where multiple trees are con-
structed, and their predictions aggregated to produce a final classification. In the context of functional
data,theEnrichedFunctionalRandomForest(EFRF)algorithmoperatesbyconstructingacollectionof
EFCTs, each trained on a bootstrap sample of the functional data represented by B-spline coefficients.
EachEFCTwithintheEFRFisconstructedbyrecursivelysplittingthefeaturespace,wherethefeatures
aretheB-splinecoefficients. Ateachnodeinthetree,asplitismadebasedononeofthesecoefficients,
whichcorrespondstoaspecificaspectofthefunctionaldata,suchasthevalueoftheoriginalfunction,
itsfirstderivative,secondderivative,curvature,radiusofcurvature,orelasticity. Thethresholdusedfor
the split at each node represents a critical value of a particular B-spline coefficient that best separates
the data into distinct categories. For example, a split might be based on a coefficient associated with
the first derivative, indicating that the decision is influenced by the rate of change in the function at a
specific point in time. In other words, each EFCT in the EFRF is built using the same process as the
EFCT but with the added randomness of selecting a subset of the B-spline coefficients at each split.
ThisprocessensuresthateachEFCTintheforestisslightlydifferent,reducingthecorrelationbetween
EFCTs and thereby improving the overall predictive accuracy of the ensemble. The final prediction is
madebyaggregatingthepredictionsofalltreesintheforest,typicallythroughmajorityvoting.
ThestructureofeachEFCTcanbeviewedasahierarchicalsequenceofdecisions,startingfromthe
root node representing the entire functional dataset and progressing down to the leaf nodes where final
classdecisionsaremade. Eachpathfromtheroottoaleafnodereflectsaseriesofrulesthatsuccessively
refinetheclassificationbasedondifferentfeaturesofthefunctionaldata. Forinstance,apathmightstart
with a split on a B-spline coefficient related to the original function x(t) and then proceed with a split
i
(1)
onacoefficientassociatedwiththefirstderivativex (t),suggestingthatboththefunction’svalueand
i
itsrateofchangearecrucialforclassifyingthedata.
AnessentialaspectofinterpretingEFRFmodelsisevaluatingvariableimportance,whichmeasures
how often each B-spline coefficient is used in the splits and how much those splits contribute to the
model’saccuracy. Thisanalysishelpsidentifywhichfunctionalfeaturesaremostcriticalindistinguish-
ingbetweenclasses. Forexample,ifcoefficientsrelatedtothesecondderivativearefrequentlyselected
for splits, it indicates that the acceleration of the function plays a significant role in the classification
process. Overall, while the individual trees in the forest may be complex, the EFRF model provides
a coherent framework for classification by leveraging the rich information in the functional data. The
consistency of using the same number of B-spline coefficients across all trees enhances the model’s
explainability,allowingforameaningfulunderstandingofhowthefunctionaldata’svarioustransforma-
tionscontributetotheclassificationdecisions.
2.4.2 EnrichedFunctionalXGBoost(EFXGB)
XGBoost (Extreme Gradient Boosting) is an advanced and scalable implementation of gradient boost-
ing algorithms that excels in both predictive accuracy and computational efficiency. In the context of
functional data analysis, we extend XGBoost by using B-spline coefficients as features derived from
11functionaldata. Thisapproachallowsthemodeltocaptureandutilisetheintricatestructureinherentin
functionaldata. ThemodelweproposeistermedEnrichedFunctionalXGBoost(EFXGB).
Let S represent the vector of B-spline coefficients for the i-th functional observation, which in-
i
cludes coefficients from the original function x(t), its first and second derivatives, curvature, radius of
i
curvature,andelasticity. Thepredictedclasslabelforthei-thobservationisgivenbyyˆ = f(S).
i i
In EFXGB, the goal is to minimise a loss function L(S,y) over the predictions yˆ, where y =
(y ,...,y )⊤ are the true labels. The model builds an ensemble of EFCTs sequentially, where each
1 N
EFCT f (S) is trained to correct the errors made by the previous trees. The prediction for the i-th
m
observationaftermEFCTsis:
m
yˆ(m) = ∑α f (S) (25)
i k k i
k=1
where α are weights assigned to each tree, typically learned during training. The model iteratively
k
updatestheseEFCTsbyminimisingthefollowingobjectivefunction:
N
L(m)= ∑l(y,yˆ(m−1) +α f (S))+Ω(f ) (26)
i i m m i m
i=1
where l(·) is a differentiable convex loss function, such as logistic loss or squared error, and Ω(f ) is
m
a regularization term that penalizes the complexity of the EFCT f (S). The regularization term Ω(f )
m m
typicallyincludesthenumberofleavesT intheEFCTandtheL -normoftheleafweights:
2
1 T
Ω(f )=γT+ λ ∑w2 (27)
m 2 j
j=1
wherew representsthe weightassignedto leaf j, γ controlsthe complexityof the model, andλ isthe
j
regularizationparameter. Duringeachiteration,themodelcalculatesthefirstandsecond-ordergradients
(m) (m)
ofthelossfunctionconcerningthepredictions,knownasthegradientg andHessianh :
i i
(m−1)
∂l(y,yˆ )
g(m) = i i (28)
i (m−1)
∂yˆ
i
∂2l(y,yˆ(m−1)
)
h(m) = i i (29)
i (m−1)2
∂yˆ
i
These gradients and Hessians are used to fit the new EFCT f (S) by minimising a second-order
m
approximation of the loss function. The decision rules within each EFCT are based on the B-spline
coefficients, allowing the model to leverage the functional characteristics of the data throughout the
boostingprocess.
EFXGB’s flexibility in handling various loss functions and incorporating regularisation makes it
particularly powerful for complex functional classification tasks. Using B-spline coefficients ensures
thatthefunctionalnatureofthedataispreservedandleveragedateachstep,resultinginamodelthatis
bothaccurateandwithlowvariance. EachEFCTaddsinformationaboutthefunctionaldata,gradually
refiningthemodel’spredictionsbyfocusingoncorrectingtheerrorsfrompreviousiterations.
2.4.3 EnrichedFunctionalLightGBM(EFLGBM)
In this section, we extend the Light Gradient Boosting Machine (LightGBM) to functional data clas-
sification tasks by incorporating previously enriched features, including B-spline coefficients derived
fromtheoriginalfunction, itsderivatives, curvature, radiusofcurvature, andelasticity. Thisextension,
termedEnrichedFunctionalLightGBM(EFLGBM),efficientlycapturesthestructureoffunctionaldata
whileleveragingLightGBM’scomputationaladvantages.
12SimilartoEFXGB,EFLGBMconstructsanensembleofEFCTs,tominimisethesamelossfunction
L(S,y)definedinEquation26. Thepredictionforthei-thobservationaftermEFCTsfollowsthesame
formulation:
m
yˆ(m) = ∑α f (S) (30)
i k k i
k=1
whereα aretheweightsassociatedwitheachEFCT,asdescribedintheEFXGBsection.
k
A critical difference between EFLGBM and EFXGB lies in the EFCT-growing strategy. While
EFXGBgrowsEFCTviaalevel-wiseapproach,EFLGBMemploysaleaf-wisegrowthstrategy,where
ateachiteration,themodelsplitstheleafleadingtothemostconsiderablereductioninthelossfunction.
ThisapproachallowsEFLGBMtoexploremorecomplexEFCT,potentiallycapturingsubtlepatternsin
the functional data. The objective function for EFLGBM is identical to the one used for EFXGB, with
the regularisation term Ω(f ) controlling the model’s complexity through the number of leaves T and
m
theleafweightsw :
j
1 T
Ω(f )=γT+ λ ∑w2 (31)
m 2 j
j=1
(m) (m)
AsinEFXGB,themodelreliesonfirstandsecond-ordergradientsg andHessiansh tofiteach
i i
newEFCT,usingg(m)
=
∂l(yi,yˆ( im−1)) andh(m)
=
∂2l(yi,yˆ i(m−1))
.
i ∂yˆ( im−1) i
∂yˆ
i(m−1)2
The leaf-wise growth of EFCTs in EFLGBM, combined with the enriched functional features, en-
ables the model to efficiently capture complex interactions in the functional data, leading to highly
accurate and low-variance classification models. By focusing on leaves with the greatest potential to
reduce loss, EFLGBM balances computational efficiency with model complexity, making it a robust
choice for functional data classification. EFLGBM, like EFXGB, preserves the data’s functional char-
acteristics by using B-spline coefficients as input features, ensuring that the underlying structure of the
functionaldataisleveragedthroughouttheboostingprocess. However,itsleaf-wisestrategyallowsitto
scalemoreefficiently,especiallyinlargedatasetswheresubtlefunctionalpatternsmustbedetected.
3 Applications
Insubsections3.1and3.2,weuseseven-timeseriesdatasetsfromtheTimeSeriesClassificationRepos-
itory [Bagnall et al., 2018], covering various application domains such as electrocardiogram (ECG)
signals, image analysis, and energy demand. Table 1 summarises the main characteristics of these
datasets, including the number of training and test samples, the length of the time series, and the num-
berofclasses. Supervisedclassificationoffunctionaldataisperformedusingthemethodsdescribedin
Section 2. While we evaluate the performance of the proposed methods across all datasets, we focus
particularly on the Car dataset to illustrate the methodology in detail. This includes the steps of data
preparation,applyingfunctionalclassificationmodels,andinterpretingtheresults. Wewillonlypresent
thefinalresultsfortheotherdatasets,comparingourapproachwithexistingmethodsintheliterature.
Insubsection3.3,wetestthemethodonsixadditionalsimulateddatasets,eachwithadifferentnum-
ber of classes. A detailed description of the simulation scenarios can be found in the same subsection.
Table 2 summarises key details of the simulated datasets, including the number of classes, time points,
andthesizeofthetrainingandtestdatasets.
3.1 DetailedMethodologyDescriptionusingtheCarDataset
The Car dataset contains outlines of four different types of cars, extracted from traffic videos using
motioninformation. Theseimagesweremappedontoa1-Dtimeseries,andthevehicleswereclassified
into one of the four categories: sedan, pickup, minivan, or SUV. Further details about the dataset are
13Dataset TrainSize TestSize Length No. ofClasses Type
Car 60 60 577 4 Sensor
ECG200 100 100 96 2 ECG
ECGFiveDays 23 861 136 2 ECG
ItalyPowerDemand 67 1029 24 2 Sensor
Plane 105 105 144 7 Sensor
Trace 100 100 275 4 Sensor
TwoLeadECG 23 1139 82 2 ECG
Table1: SelectedTimeSeriesClassificationDatasets[Bagnalletal.,2018].
Scenario TrainSize TestSize Length No. ofClasses Type
Scenario1 100 100 50 2 SIMULATED
Scenario2 100 100 50 2 SIMULATED
Scenario3 100 100 50 2 SIMULATED
Scenario4 200 200 50 4 SIMULATED
Scenario5 150 150 50 3 SIMULATED
Scenario6 150 150 50 3 SIMULATED
Table2: Detailsofthesimulateddatasetsusedforclassificationexperiments.
available in the work by Thakoor and Gao [2005]. We utilize B-spline basis functions to approximate
theoriginalcurvesandextractenrichedfeatures,suchasderivatives,curvature,radiusofcurvature,and
elasticity,asdescribedinSection2.
Figures 2 and 3 present the functional data for the training and test sets. The original curves and
the first and second derivatives, curvature, radius of curvature, and elasticity, are shown. The first plot
displays the original curves, highlighting the characteristic shapes of the four vehicle types. The other
plots focus on enriched functional features, such as the rate of change captured by the first derivatives,
accelerationanddecelerationseeninthesecondderivatives,thedegreeofbendinginthecurvatureand
radiusofcurvature,andtheresponsivenessmeasuredbyelasticity.
Concerning the functional representation through the b-splines fixed basis system, we stress that
despite the fact we could select the number of bases through cross-validation on each dimension con-
sidered,inthiscontext,weprefertousetheclassicrule,i.e. thenumberofb-splinesequalsthenumber
of time instants plus the order of b-splines minus two [Ramsay and Silverman, 2005]. This guarantees
we have an identical number of bases for each dimension. Since the coefficients of the linear combi-
nationsarethefunctionalclassifiers’features,havingadifferentnumberofcoefficientstorepresentthe
dimensionsofeachstatisticalunitcouldleadtobiaswithintheclassifiers. Inotherwords, weavoidan
imbalancebetweentheweightofthevariousdimensionsandgivemoreimportancetosomedimensions
rather than others (we aim to prevent, for example, that cross-validation recommends using hundreds
of b-splines for the second derivative and few b-splines for the first derivative or other dimensions).
Instead, using a consistent number of b-spline basis functions across all curve dimensions ensures that
the coefficient matrices used as features have the same dimensionality. Once we have chosen the same
number of bases for all, only based on the number of time instants and the order of the b-splines (here
we always use cubic splines, therefore of order equal to 4), we proceed to extract the scores and use
themasfeaturestotraindifferentfunctionalclassifiers.
Although, from a conceptual point of view, we expect that the proposed enrichment may perform
14Figure2: FunctionalDatafortheTrainingSet(CarDataset).
15Figure3: FunctionalDatafortheTestSet(CarDataset).
16poorlywhenextendingtheenrichedfeaturestothecontextoffunctionalK-NN,wewanttotestitsper-
formance and compare it with a traditional functional K-NN without enriched features. This choice
is driven both by a desire for experimentation to understand whether performance deteriorates and to
provideacomparisonbetweentheenrichedtree-basedclassifiersandafunctionalclassifierthat,inpre-
viousstudies, hasoftenshowncompetitiveperformancecomparedtomoreadvancedmethods[Maturo
and Verde, 2023, 2022a]. The expectation that the proposed enrichment may perform poorly in func-
tional K-NN is based on how K-NN operates as a distance-based classifier. Functional K-NN relies on
calculating distances between entire functions to classify new observations based on their proximity to
existinglabelleddata. Theintroductionofenrichedfeatures,suchasderivatives,curvature,andelastic-
ity, could alter the functional data’s underlying geometry, leading to distorted distance metrics. More-
over, K-NN’s sensitivity to local noise and outliers could further exacerbate this issue when working
withenrichedfunctionaldata,astheadditionalfeaturesmightamplifyminorvariationsinthefunctional
curvesthatareirrelevantforclassification.
Therefore,inthissection,weadopttheEnrichedFunctionalK-NN(EFKNN),EFCTs,EFRF,EFXGB,
andEFLGBM.Themaingoalistocomparetheaccuracyofeachofthesemethods,usingonlythecoef-
ficientsofthesplinesoftheoriginalfunctions(non-enrichedfunctionalclassifiers)andthenconsidering
theenrichedversion,thatis,ourproposalusingallthecoefficientsofeachdimension(derivatives,cur-
vature,radius,andelasticity). Subsequently,tocomparewithotherclassicalfunctionalmethods,which
do not necessarily use splines, we refer to some known functional classifiers in the R package fda.usc
[Febrero-BandeanddelaFuente,2012].
Although parameter optimisation could improve individual model performance, we intentionally
avoidin-depthoptimisationforeachclassifier. Thisdecisionisbasedontworeasons. First,wecompare
15 different models, each requiring separate optimisations, leading us to various configurations, even
betweenthepairsofmethodswewanttodirectlycompare(forexample,FRFwithandwithoutenrich-
ment). Hence, we aim to use the same configuration for each couple of classifiers and understand if,
under the same conditions, without optimising either one or the other, the enrichment produces effects
in terms of performance. The second reason is that to produce more robust results and not limit our-
selvestothetrivialcomparisonbetweensingleaccuracyvaluesofeachmodel,weintroducevariability
inthebasicconfigurationsofthehyperparameterstohaveamorerobustcomparisonbetweenaccuracy
distributions for each functional classifier. In practice, this randomisation we produce to compare the
resultsturnsouttobeasortofrandomsearchtuningbecausewecangetadeeperunderstandingofthe
classifier’spotentialbyexaminingtheupperrangeofitsaccuracydistribution. Itrevealshoweachmodel
canpushitsperformancewithoutextensiveparametertuning. Additionally,thegoalisnottoachievethe
best possible accuracy but to evaluate whether enriching the features improves classification and, if so,
with which models it works best. Most importantly, this approach ensures that improvements or drops
in performance are due to the enriched feature representations and not a result of optimised hyperpa-
rameters for any specific method. This controlled approach helps eliminate the potential confounding
effectofhyperparametertuningandcreatesacontrolledenvironmenttoobservehowincludingenriched
functional features impacts classification accuracy directly. Therefore, while in-depth optimisation for
eachclassifierispossible,weprioritisecomparabilityandrobustnessacrossallmethods.
Estimates’variabilityisintroducedinseveralwaysacrossdifferentclassifierstoensurerobustresults
thatarenotduetorandomchanceoroverfitting. Foreachclassificationmethod,randomnessisinjected
primarilythroughrandomlyalteringspecifichyperparametersduringeachiterationandtheinherentran-
domness in the algorithms themselves. For the EFCTs and FCTs, the maximum depth and minimum
numberofsamplesrequiredtosplitnodesarerandomlyselected,ensuringthatdifferentmodelsaregen-
eratedforeachrun. EFRFandFRFutilisetheirnaturalvariabilityinbootstrappingandfeatureselection.
ForEFXGB,FXGB,EFLGBM,andFLGBM,parametersliketreedepth, learningrate, andsamplera-
tios are randomly adjusted in each iteration. For EFKNN and FKNN, the number of neighbours varies
to observe the impact on classification accuracy. This approach ensures that the models are evaluated
underawiderangeofconditions,allowingamorerobustperformancecomparisonacrossclassifiers.
In addition, several classical methods from the fda.usc package are used, including recursive
17partitioning(rpart),neuralnetworks(nnet),SupportVectorMachines(svm),RandomForest,andcross-
validated elastic-net regression (cv.glmnet). In these last methods, we use the default starting param-
eters, introduce variability as previously to ensure robustness, and do not work on b-splines or even
enrichedfeatures. Thiscomprehensivecomparisonofmethodsallowsforasystematicevaluationofthe
performanceoftraditionalandmodernmachinelearningtechniquesappliedtorawfunctionaldataand
feature-enrichedrepresentationsacrossallsimulatedscenarios.
The accuracy results for different classifiers applied to the Car dataset are summarised in Figure
4. EFRF, EFXGB, EFLGBM, and EFCTs show improvements with enriched features. Classical FDA
methods implemented using the fda.usc package also yields competitive results but is inferior to
EFRF and EFXGB. As we expected, EFCTs have greater variability than ensemble methods. As ex-
pected,EFKNNispoorlysuitedforhandlingenrichedfeatures,asitsperformancesignificantlydeclines
whenincorporated.
Figure 4: Comparison of Classifier Performance on the Car Dataset. The accuracy is compared across
originalcurves,enrichedfeatures,andclassicalFDAmethods.
The enriched features significantly enhance classification accuracy, particularly in ensemble meth-
ods. EFRF and EFXGB show notable improvements, while classical methods, particularly SVM and
RandomForestwithoutenrichmentandb-splines’scores,alsoachievecompetitiveresults.
3.2 ApplicationtootherRealData
Thissectionpresentsthemainresultsofcomparisonsacrosssixadditionaldatasets,eachwithadifferent
number of classes. As shown in Table 1, the outcomes classes range between 2 and 7, the time series
lengths range from 24 to 275 instants of time, and the training and test sets have different sizes from a
18minimum of 23 to a maximum of 1029 instances. In all cases, the classes are well-balanced, ensuring
faircomparisons.
Figure 5 illustrates the original curve representations of the six datasets, showing the time series
for each class. The visualisations highlight the varying characteristics of the datasets, from the simple
patterns to the more intricate structures in datasets like Plane and Trace. Figure 6 shows the boxplots
for accuracy across six datasets. The methods used are the same as those proposed for illustrating the
Cardataset.
Figure 5 highlights a sharp distinction between the enriched and non-enriched versions of several
classifiersfortheECG200dataset. EFRFdemonstratesthebestoverallperformance,surpassingallother
methods in accuracy. In comparison, the non-enriched FRF, FXGB, and FLGBM show consistently
lower performance, reinforcing the value of enriched features. However, FKNN performs poorly with
enriched features, showing a significant drop in accuracy compared to using original curves. Classical
methodssuchasrandomForestandcv.glmnetfromtheFDA.uscpackageperformcompetitively.
IntheresultsoftheECGFiveDaysdataset,thereisaclearadvantagefortheenrichedversionofthe
FCT,surpassingitsnon-enrichedcounterpart. Ontheotherhand,FKNNandFLGBMperformextremely
poorly with both original and enriched features, while FXGB shows an improvement in enriched and
original features. The FRF demonstrates improved performance with enriched features, positioning
itself as one of the top-performing models, together with nnet. This suggests that the enriched features
generallycontributepositivelytotheperformance,especiallyforEFRFandEFXGB.
In the Italy Power Demand dataset, the enriched versions of FRF, FXGB, and FLGBM perform
slightly worse than their original curve counterparts, making this an exception to the usual trend. FRF
still delivers high accuracy and close to the best results, but the enriched features don’t provide a clear
advantagethistime. Ontheotherhand,theSVMfromfda.uscshowsawidedistributionofaccuracy,
indicatingmorevariabilityinperformancecomparedtoothermethods,particularlyinthisdataset.
In the Plane dataset, the distinction between the enriched and original curve methods is minimal
and slightly in favour of the enriched features for FRF and the two functional boosting methods. As
usual,FKNNfailstoperformwhenenriched. FCTshowssignificantvariabilityandevenexperiencesa
slightdropinperformancewhenenriched. Despitethesesmallshifts,EFRFremainsthetop-performing
classifier,withconsistentaccuracyacrosstheboard,demonstratingitsrobustnesseveninthisdataset.
In the results of the Trace dataset, we observe a similar trend to the previous datasets. While both
EFRF and EFXGB show strong performance, the best-performing model, in this case, slightly favours
nnet. TheenrichedfeaturesboosttheperformanceofFRFcomparedtotheiroriginalcurvecounterparts,
but EFKNN continues to underperform. As before, FCT shows considerable variability, though still
yieldingrelativelyhighaccuracywithenrichedandoriginalfeatures.
Finally,fortheTwoLeadECGdataset,FRFandFXGBshowsignificantimprovementwhenenriched.
EFXGB achieves the highest overall accuracy in this dataset, surpassing other classifiers. On the other
hand,FKNNandFLGBMexhibitpoorperformance,especiallyintheenrichedversions. Thishighlights
thelimitationsofFKNNandFLGBMinhandlingenrichedfeaturescomparedtomethodslikeFRFand
FXGB,whichgreatlybenefitfromtheenrichedrepresentation.
3.3 ApplicationtoSimulatedDatasets
Togatherstrongerevidenceoftheeffectivenessofourmethods,whichalreadyemergefromtheanalysis
of the seven datasets presented, we conduct a simulation study. This approach allows us to evaluate
performanceacrossabroaderrangeofscenarios,enhancingthereliabilityofourconclusions. Through
controlledsimulations, wecanfurtherassesshowtheenrichedfeaturesinteractwithvariousclassifiers
and verify whether the observed improvements are consistent and significant across different synthetic
datasettings.
Toevaluatetheclassifier’sperformance,wemodifyandadaptseveralmodelspreviouslyconsidered
by Cuevas et al. [2007], Preda et al. [2007], Taiwo Ojo et al. [2021] to generate functional data with
distinctshapes. Figure7presentssixsimulatedscenarios,eachinvolvingadifferentnumberofclasses.
19Figure5: Originalcurverepresentationsofthesixdatasets.
20Figure6: Comparisonofaccuracyacrossmethodsonsixdatasets.
21Thefirstthreepanels(Simulations1,2,and3)representbinaryclassificationproblems,wherethefunc-
tional data are divided into two groups. Simulation 4 introduces a scenario with four distinct classes,
whileSimulations5and6involvethreeclasseseach. Inbinaryclassificationscenarios, 100curvesper
class are generated, and the functional data are plotted over 50 time observations. The only difference
formulti-classclassificationsisthatinthesecaseswegenerate50curvesperclass,insteadof100.
Scenario1: Tosimulatethefirstscenario,wegeneratedtwogroupsoffunctionaldatausingamodel
basedonaGaussianprocess. ThebasemodelisdefinedasX(t)=µt+e(t),wheret ∈[0,1]ande(t)
i i i
is a Gaussian process with zero mean and covariance function γ(s,t) = αexp(−β|t−s|ν). The two
groupsarecreatedbyadjustingsomeparametersinawaythatintroducesmoderatedifferencesbetween
them, making the classification task non-trivial but not too simple. For the first group, we set µ =8
and generated 100 curves over 50 time points. For the second group, we slightly modified the base
model by introducing a shift in the function, defined as X(t)= µt+qkI +e(t), where q=3 and
i i Ti≤t i
k ∈{−1,1} with equal probability, and T is drawn from a uniform distribution over [a,b]=[0.5,0.9].
i i
This shift creates functional curves for the second group that differ moderately from those in the first
group, ensuring the classification task is not overly simple. The covariance structure for both groups
is controlled by the parameters α = 1, β = 1, and ν = 1, ensuring consistent variability across the
curves. Theprobabilityoftheshiftbeingpositiveornegativeissetto0.5toavoidoverlydistinctgroup
separation.
Scenario 2: For the second scenario, we generated two groups of periodic functional data using
a model based on sinusoidal components with Gaussian noise. The base model for the first group is
defined as X(t) = a sin(πt)+a cos(πt)+e(t), where e(t) is a Gaussian process with zero mean
i 1i 2i i i
and covariance function γ(s,t) = αexp(−β|t−s|ν). The parameters a and a were set to a = 1
1i 2i 1i
and a = 8, respectively, to generate periodic curves for the first group. For the second group, we
2i
applied a slight variation to the model by adding a shift, resulting in the modified model X(t) =
i
(b sin(πt)+b cos(πt))(1−u)+(c sin(πt)+c cos(πt))u +e(t),whereu followsaBernoullidis-
1i 2i i 1i 2i i i i
tributionwithP(u =1)=0.60,andb ∈[1.5,2.5],c ∈[5,10.5],creatingfunctionalcurvesthathave
i 1i 1i
subtledifferencesfromthoseinthefirstgroup,whilestillretainingtheperiodicnatureofthedata. The
covariance structure remains the same for both groups, with α =1, β =1, and ν =1. The variations
introducedbytheparametersb andc , combinedwiththeprobabilisticshiftgovernedbyu, createa
1i 1i i
moderatedifferencebetweenthetwogroups,ensuringthattheclassificationproblemisnon-trivial.
Scenario 3: For the third scenario, we generated two groups of functional data using a model that
introduces differences in the shape of the curves over a specific portion of the domain. The base
model for the first group is defined as X(t)= µt+e(t), where e(t) is a Gaussian process with zero
i i i
mean and covariance function γ(s,t) = αexp(−β|t−s|ν). For this group, we set µ = 8. The sec-
ond group is generated by applying a shift and a change in the shape of the function, modeled as
(cid:16) (cid:17)
X(t) = µt+(−1)uq+(−1)1−u √1 exp(−z(t−v)w)+e(t), where u follows a Bernoulli distribu-
i πr i
tionwithprobabilityP(u=1)=0.1. Inthisscenario,wesetq=1.8,r=0.02,z=90,andw=2. The
parameter v is drawn from a uniform distribution over the interval [0.45,0.55], introducing a localized
changeintheshapeofthecurveforthesecondgroup. Thecovariancestructureforbothgroupsiscon-
trolledbytheparametersα =1,β =1,andν =1,ensuringconsistentvariabilityacrossthecurves. The
slightshiftandshapechangesinthesecondgroupmaketheclassificationtaskmorechallenging,asthe
groupsarenottriviallydistinguishable.
Scenario4: Forthefourthscenario,weusedthesamemodeldescribedinthethirdsimulation. The
model introducesdifferences in theshape of the curvesfor a portionof the domain. The basemodel is
given by X(t)=µt+e(t), where e(t) is a Gaussian process with zero mean and covariance function
i i i
γ(s,t)=αexp(−β|t−s|ν). For the first two groups (Group 1 and Group 2), we set µ =0, q=1, and
controlled the timing of the shift and shape change using a uniform distribution for v, drawn from the
interval[0.45,0.45]. Theotherparametersgoverningtheshapechangewerer=0.02,w=2,andz=90.
Thecovarianceparametersweresettoα =1.3,β =1.2,andν =1. Forthesecondsetofgroups(Group
3 and Group 4), we introduced further variations. Here, we used µ =−2, q=1.8, and controlled the
shift with v drawn from the interval [0.15,0.15]. The shape-related parameters were set to r =0.01,
22w=5,andz=90. Thecovarianceparameterswereadjustedtoα =0.8,β =0.8,whilekeepingν =1.
Scenario5: Forthefifthscenario,weusedthesamemodeldescribedintheprevioussimulations,but
generatedthreedistinctgroups. ForGroups1and2,theparametersweresetasfollows: µ =0,q=1.8,
and the timing of the shape and shift was controlled by drawing v from the interval [0.45,0.45]. The
shape-related parameters were configured as r =0.02, w=2, and z=90. The covariance parameters
were set to α =1, β =1, and ν =1. For Group 3, we introduced different parameter values to create
a distinct third group. Specifically, we set µ = 1, q = 0.8, and drew v from the interval [0.65,0.65]
to control the shift. The other parameters remained the same: r =0.02, w=2, and z=90, while the
covarianceparameterswerekeptatα =1,β =1,andν =1.
Scenario 6: For the sixth scenario, we adapted the model used in the first simulation to generate
three distinct classes by adjusting the parameters for each class. For the first two groups (Group 1 and
Group 2), the model was configured with µ =2, q=3, and a uniform distribution for T drawn from
i
theinterval[0.6,0.75]. Thecovarianceparametersweresettoα =2,β =1,andν =0.5. Forthethird
group, we applied further parameter variations to create a distinct class. Here, µ =2 and q=3 were
kept the same, but the uniform distribution for T was drawn from a narrower interval [0.8,0.9]. The
i
covarianceparametersremainedunchanged,withα =2,β =1,andν =0.5. Thisgroupwasgenerated
with50curves,introducingmorepronounceddifferencescomparedtothefirsttwo,addingcomplexity
totheclassificationtask.
Figure 8 presents the accuracy results for each of the six simulated scenarios, comparing the per-
formance of various classification methods applied to both the original curves and enriched feature
representationsalongsideclassicalfunctionaldataanalysismethods.
ThebinaryclassificationtaskinScenarion.1showsthattheenrichedfeaturesgenerallyoutperform
the original curve methods. The enriched version for FCT provides better accuracy than the original
curves. Similarly,forFKNNandFLGBM,theenrichedfeatureversionsyieldhigheraccuracythantheir
counterparts. FRF-enrichedfeaturesoutperformallothermethods, achievingthebestoverallaccuracy.
FXGBslightlyimproveswithenrichedfeatures,thoughitsperformanceremainsbehindFRF.Classical
methods,suchasnnet,performwellbutareoutperformedbyFRFenriched,makingnnetthesecond-
bestapproach.
In Scenario 2, the enriched features yield better performance for FCT, showing clear improvement
compared to the original curves. FRF, on the other hand, performs similarly for both original and
enriched features, with no difference in accuracy. However, other methods such as FKNN, FLGBM,
and FXGB show a slight decrease in performance when enriched features are used. Notably, FKNN
with enriched features performs significantly worse than the original curve version, highlighting that
thisapproachdoesnotworkwellwithFKNN.
The binary classification problem in Scenario n.3 highlights a more robust performance from the
enriched feature methods. FCT, FKNN, and FLGBM all perform better with enriched features than
their original curve counterparts. FRF and FXGB, while already performing well with original curves,
show slight improvements when enriched features are used. The only method that has performance
comparabletoenrichedFRFisnnet.
Scenario4dealswithafour-classproblem. Theenrichedfeaturesimproveperformanceforseveral
methods. FRFwithenrichedfeaturesemergesasthebestperformer,achievingthehighestoverallaccu-
racy. FLGBM and FXGB also benefit from enriched features, showing clear improvements compared
totheiroriginalcurvecounterparts. FCTandFKNN,ontheotherhand,donotexhibitsubstantialgains
fromtheenrichedfeatures. Amongtheclassicalmethods,nnnetisthebest.
In Scenario n. 5’s three-class classification problem, FRF, FCT, and FXGB show improved per-
formance when using enriched features. FRF remains highly competitive, but FXGB, with enriched
features, emerges as the best performer. FCT also benefits from enriched features, achieving better ac-
curacy than its original curve counterpart. However, FKNN continues to perform poorly with enriched
features,reinforcingtheexpectationthatthismethodisnotwell-suitedforfeatureenrichment.
Inthefinalthree-classscenarion.6,enrichedfeaturemethodsagaintendtooutperformtheiroriginal
curvecounterparts. FCTandFLGBMshowbetterresultswhenusingenrichedfeatures,whileFRFand
23Figure7: SimulatedScenarioswithdifferentNumberofClasses.
24FXGBremaincompetitivewithminimaldifferencesbetweenthetwoapproaches.
4 On Enriched Functional Tree-Based Classifiers Interpretability and
Explainability
There is no doubt that enriching functional features provides fascinating benefits in terms of accuracy.
However, within the proposed framework, we must pay particular attention to two distinct aspects: in-
terpretabilityandexplainability. Althoughtheprimaryfocusofthispaperisnotonexplainableartificial
intelligence (XAI), but rather on introducing methodologies to enhance performance, some considera-
tions regarding interpretability and explainability within the proposed framework can help deepen the
understandingofthemodelandsuggestpotentialavenuesforfutureresearch.
4.1 EnrichedFunctionalClassificationTrees’Interpretability
Fromtheperspectiveofinterpretability,itisevidentthat,aswithallensemblemethods,welosetheabil-
ity to interpret the classification rules easily. Simpler models, such as regression or classification trees,
allowforastraightforwardinterpretationofhowpredictorsaffecttheoutcome,buttheylosecredibility
when assumptions are violated in the former and due to high variability in the latter. Consequently, a
modelperfectforinterpretability,accuracy,andlowvariancedoesnotexist.
Focusing on the EFCT model, however, interpretability is still possible. The EFCT is an extension
toenrichedfunctionaldataoftheun-enrichedFCT(FunctionalClassificationTree)proposedbyMaturo
andVerde[2023]. Therefore,withappropriateconsiderationsforderivatives,curvature,radiusofcurva-
ture,andelasticity,itisalwayspossibletointerpretthesplittingrulesinEFCT.Similarly,followingthe
approach of Maturo and Verde [2023], it is also feasible to construct both theoretical separation curves
(basedontheformulathatreconstructstheseparationcurveasalinearcombinationofbasisfunctions)
and empirical separation curves (based on the actual curve that is closest to the theoretical separation
curve). The main difference here is that the reconstruction is based on splines rather than functional
principalcomponents,andtheinterpretationmustbemadebasedonthespecificfunctionaltransforma-
tioninvolvedineachnode’ssplittingrule. Thus,somesplittingrulesbetweengroupsofcurvesthatend
in a left or right node must be interpreted based on the original curves, speed, acceleration, curvature
andradiusofcurvature,elasticity,andtheirintrinsicmeanings.
LetS =(s ,s ,...,s )representtheB-splinecoefficientsofthei-thfunctionalobservationinthe
i i1 i2 iD
D-dimensional enriched feature space. Each dimension d corresponds to different aspects of the func-
tional data, such as the original function, derivatives, curvature, radius of curvature, or elasticity. At
eachnodezintheclassificationtree(EFCT),adecisionismadetosplitthedatabasedonaspecificfea-
tureorcombinationoffeatures, whichcanincludescoresfromanydimension(e.g., originalfunctions,
derivatives, or geometric features like curvature and radius of curvature). The theoretical separation
curve at node z, denoted by f (t), is defined as a linear combination of the B-spline basis functions
sep,z
andthecorrespondinggeneralisedcoefficientsselecteduptothatspecificnode. Weintroduceγ asthe
zs
generalised coefficients representing any feature in the enriched feature matrix, whether corresponding
totheoriginalfunction,derivatives,oranygeometricfeaturessuchascurvatureorelasticity.
Forthez-thnode,thetheoreticalseparationcurveisexpressedas:
f (t)= ∑ γ φ (t) (32)
sep,z zs s
s∈Ωz
whereΩ ={k ,...,k }isthesetofB-splinebasisfunctionsandcorrespondinggeneralisedcoefficients
z 1 z
selecteduptothesplitatnodez,γ arethegeneralisedcoefficientsassociatedwiththes-thbasisfunc-
zs
tion φ (t), where γ generalises the coefficients for any functional transformation (original function,
s zs
first/secondderivatives,curvature,etc.),φ (t)istheB-splinebasisfunctioncorrespondingtothefeature
s
involvedinthesplit.
25Figure8: SimulatedScenariosResults.
26In this notation, γ is the generic element of the enriched feature matrix, which we previously
zs
defined in Equation 20. γ refers to any coefficient from this expanded feature matrix, covering all
zs
possible dimensions (original functions, derivatives, and geometric features like curvature, elasticity,
etc.). Aseachsplitoccurs,thespecificcoefficientsinvolvedaredeterminedbythesplittingrule,which
canspandifferentdimensionsofthefeaturematrix.
Theinterpretationofthetheoreticalseparationcurve f (t)dependsonthetypeofB-splinebasis
sep,z
functions involved in the split, which can vary across different dimensions. If the basis functions cor-
respondtotheoriginalfunction, theseparationcurvereflectstheoverallshapeofthefunctionalsignal.
If the basis functions represent first or second derivatives, the separation curve highlights the signal’s
local speed or acceleration. If the split involves curvature or radius of curvature, the separation curve
focuses on how sharply the signal bends at different points. If the split is based on elasticity, the sepa-
rationcurvecapturesthefunction’sresponsivenesstochangesinitsinput. Thus, thetypeoffunctional
feature that drives the split dictates the specific interpretation of the theoretical separation curve. The
generalisedcoefficientγ ensuresthatthecombinationofdifferentfunctionaltransformationsisflexible
zs
andaccuratelyrepresentsthedecisionboundaryateachnodeoftheclassificationtree.
4.2 EnrichedFunctionalTree-BasedClassifiersExplainability
Focusingonensemblemodels,itisnaturalthatinterpretabilityisdiminished, andwemustinsteadrely
on explainability as a tool to understand what is happening within the black-box model. The unique
aspect of the proposed framework is that introducing these enriched features can result in correlations
between variables. While it is widely accepted that multicollinearity does not pose significant issues
intree-basedmethods,unlikeinmultipleregressionwhereitcanartificiallyinflateR-squaredvalues,it
is essential to acknowledge that the presence of multicollinearity can distort explainability measures in
black-box models. In other words, while from a performance standpoint, the introduction of enriched
featureshasapurelypositiveeffectbyimprovingaccuracythroughtheobservationofvariousfunctional
characteristics and increasing the diversity of the ensemble, which further boosts accuracy and signifi-
cantly reduces variance, there is a trade-off when it comes to explainability measures. This results in a
potentialbias,asthecomplexityaddedbytheenrichedfeaturescanmakeitmorechallengingtounder-
stand the model’s internal decision-making processes fully. In other words, their importance measures
maybecomeskewedwhenintroducingcorrelatedscores.
Inreality, whileitistruethatenrichmentexacerbatesthisbiasinexplainabilitymeasures, thesolu-
tionisrelativelystraightforward. Thekeyisthatwhenperformingenrichment,asproposedinthisstudy,
itisessentialtorelyonvariableimportancemeasuresthataccountforcorrelationsbetweenfeatures. De-
spitethefixedbasissystemproducinguncorrelatedfunctions,thesubstantialincreaseinpredictorswhen
enrichment introduces numerous new features across many dimensions, including curvature, radius of
curvature, and elasticity, can lead to correlations between coefficients across different splines and di-
mensions. To solve this issue, we can use two possible approaches. The first option is to condition on
the scores of the same B-spline of different dimensions when calculating the importance of a feature.
Forexample,whenassessingtheimportanceoftheB-splinescoresfortheradiusofcurvature,wecould
conditionontheB-splinescoresforthederivatives,curvature,elasticity,andoriginalfunctions,asthere
islikelyasignificantassociationbetweenthesecoefficients. Thesecondalternativeisnottoassumeany
correlationbetweenthescoresofthesameb-splines’differentdimensionsandconditioningonallcorre-
latedvariablesbeyondacertainthresholdwhenassessingfeatureimportanceviaclassicalmethods. The
latterapproachissimilartothoseusedinbioinformatics,wherethenumberofindependentvariablesof-
tenfarexceedsthenumberofobservations,resultinginhighdimensionality[seee.g.Connetal.,2022].
Byaccountingforallpotentialcorrelations,thismethodprovidesamorerobustandreliablemeasureof
modelexplainability. Extendingthetwoapproachestothecontextdescribedisquiteimmediate.
Let S = (s ,s ,...,s ) represent the B-spline coefficients of the i-th functional observation in
i i1 i2 iD
the D-dimensional enriched feature space. Each dimension corresponds to a different aspect of the
functional data, such as original function coefficients, first derivatives, second derivatives, curvature,
27radius of curvature, and elasticity. Let I(f,S ) represent the importance of a feature S (e.g., the B-
j j
splinescoresfortheradiusofcurvature)inpredictingtheoutcomey.
The first approach can be summarized as follows. LetC represent the set of coefficients for these
j
associated dimensions (i.e. associated in the sense that we deal with the scores of the same b-spline
function used to reconstruct different transformations of the functional data). The conditional feature
importanceofS (e.g.,radiusofcurvature)isgivenby:
j
I(f,S |C )=E [L(f(S ,C ),y)]−E [L(f(C ),y)] (33)
j j Sj|Cj j j Sj|Cj j
where f(S ,C ) represents the model including both the feature S and the conditioning setC , f(C )
j j j j j
represents the model excluding S but including C , L(f,y) is the loss function used to evaluate the
j j
model (e.g. cross-entropy), E denotes the expectation conditioned on C . Equation 33 quantifies
Sj|Cj j
thedifference inperformancebetween thefullmodel (withS andC )and thereduced model(without
j j
S , but conditioned onC ). This approach provides the conditional importance of S by controlling for
j j j
correlationswiththeassociateddimensions.
Alternatively, the second approach does not assume any direct correlation between the scores of
different dimensions. Instead, it conditions on all variables correlated beyond a certain threshold. Let
ρ denote the correlation between two feature dimensions S and S . We define the set of conditioning
ij i j
variablesC basedonathresholdτ as:
j
C ={S :|ρ |>τ} (34)
j i ij
The importance of S is then calculated by conditioning on the setC as in Equation 33 but, in this
j j
case, the conditioning setC includes all variables that exceed the correlation threshold τ, providing a
j
moreflexiblemethodtoaccountforthecorrelationswithintheenrichedfeaturespace. Thisapproachis
particularlyusefulwhenthecorrelationsarenotrestrictedtocertaindimensionsbutareinsteadscattered
acrossthefeaturespace.
5 Discussion and Conclusions
Theevolvingfieldofsupervisedcurveclassificationhasmadesignificantadvancesinrecentdecades,yet
integrating Functional Data Analysis (FDA) with tree-based classifiers remains an area ripe for further
development. Whilesomepreviousstudieshaveexaminedthiscombinationfromvariousperspectives,
criticalareasstillrequiredeeperexploration. Keyareasforenhancementincludeimprovingtheaccuracy
of functional classifiers, developing advanced graphical tools for interpreting classification rules, con-
ductingcomprehensivesimulationstudies, anddesigningeffectivestrategiesforoptimisingparameters
inthesupervisedclassificationoffunctionaldata.
ThispaperintroducesanovelsupervisedclassificationstrategythatsynergisesFDAwithtree-based
ensemblestoextractricherinsightsfromcurveanalysis. TheproposedEnrichedFunctionalTree-Based
Classifiers(EFTCs)addressthechallengesassociatedwithhigh-dimensionaldata,focusingonimprov-
ing classification performance. By incorporating additional features derived from various functional
transformations,suchassequentialderivatives,curvature,theradiusofcurvature,andelasticity,theen-
richedfunctionaldatastrategycapturesdetailedinformationaboutthefunctionaldata’sglobalandlocal
behaviour.
Extensive experimental evaluations on real-world and simulated datasets underscore the effective-
nessoftheproposedapproach. Theresultsdemonstratesubstantialimprovementsinclassificationper-
formance over existing methods, confirming the value of the enriched functional features in managing
high-dimensional data. The proposed classifiers effectively capture local characteristics often over-
looked by traditional methods, highlighting the importance of these additional features in achieving
accurateclassification,eveninscenariosinvolvingmultipleclassesandcomplexcurveshapes. Further-
more, theenhancedperformanceobservedintheEFTCscanalsobeattributedtointroducingdiversity,
a crucial factor in ensemble methods. By incorporating multiple perspectives of the functional data,
28suchasderivatives,curvature,andelasticity,intothemodel,weeffectivelyincreasethevarietyofdeci-
sionpatternsavailabletotheensemble,allowingittocapitaliseonthecomplementarystrengthsofeach
feature and ultimately boost classification accuracy. This significant result, achieved through a truly
original approach to introducing diversity in the ensemble by leveraging the available functional tools,
alignswithinsightsfromthebroadermachinelearningliteratureinnon-functionalcontexts. Thisinno-
vativeintegrationstrengthensthemodel’sperformanceandopensnewpathwaysforexploringensemble
methodsinfunctionaldataanalysis.
WhilethisstudyusesB-splinesforfeatureextraction,theunderlyingmethodologycanbeextended
to other functional transformations, functional classifiers, and basis functions. The fixed-basis system
here offers a consistent framework for training and testing, avoiding the complications associated with
data-drivenbasissystems, wherethebasisfunctionsmayvarybetweendatasets. Futureresearchcould
delveintoenhancingtheinterpretabilityandexplainabilityofthesemodels,orexploretheintegrationof
a weighted selection of the number of splines, potentially guided by cross-validation criteria, a choice
deliberately avoided in this context, as explained in Section 2. At the same time, once it has been
demonstratedthattheenrichmentperformswellintermsofaccuracy,futurestudiescanfurtherexplore
parameteroptimisationbyfocusingonspecificfunctionalclassifiers. AsthoroughlyexplainedinSection
2,weavoideddeepoptimisationtomaintainanexperimentalsetupconducivetocomparison,whichwas
theprimaryobjectiveofthestudy.
Declarations
Theauthorsdeclarethattheyreceivednofundingforthisstudyandhavenoaffiliationsorinvolvement
withanyorganizationthathasafinancialornon-financialinterestinthesubjectmatterofthismanuscript.
Fundingand/orConflictsofInterest/CompetingInterests
Theauthorsconfirmthattheyreceivednosupportfromanyorganizationforthesubmittedwork. They
also declare no affiliations or involvement with any organization or entity that has a financial or non-
financialinterestinthesubjectmatterofthismanuscript.
UseofGenerativeAIinScientificWriting
AI-assistedtechnologieswereusedonlyinthewritingprocesstoimprovethereadabilityandlanguage
ofthemanuscript. Theauthorsreviewedandeditedthecontentasnecessaryandtookfullresponsibility
forthepublication’scontent.
Dataavailabilitystatement
The authors used publicly available data for real-world applications. Simulation data can be provided
uponrequest.
References
J.O. Ramsay and B.W. Silverman. Applied Functional Data Analysis: Methods and Case Studies.
Springer,NewYork,2002. doi: 10.1007/b98886.
F. Ferraty and P. Vieu. Curves discrimination: a nonparametric functional approach. Computational
Statistics&DataAnalysis,44(1-2):161–173,2003. doi: 10.1016/s0167-9473(03)00032-x.
J.O.RamsayandB.W.Silverman. FunctionalDataAnalysis,2ndedn. Springer,NewYork,2005. doi:
10.1007/b98888.
29FrédéricFerraty. RecentAdvancesinFunctionalDataAnalysisandRelatedTopics. Physica-VerlagHD,
Berlin,2011. doi: 10.1007/978-3-7908-2736-1.
YanYuandDianeLambert. Fittingtreestofunctionaldata,withanapplicationtotime-of-daypatterns.
JournalofComputationalandGraphicalStatistics,8(4):749–762,dec1999. doi: 10.1080/10618600.
1999.10474847.
Baptiste Gregorutti, Bertrand Michel, and Philippe Saint-Pierre. Grouped variable importance with
randomforestsandapplicationtomultiplefunctionaldataanalysis. ComputationalStatistics&Data
Analysis,90:15–35,2015. doi: 10.1016/j.csda.2015.04.002.
AnnetteMöller,GerhardTutz,andJanGertheiss. Randomforestsforfunctionalcovariates. Journalof
Chemometrics,2016. ISSN1099128X. doi: 10.1002/cem.2849.
RaziurRahman,SaugatoRahmanDhruba,SouparnoGhosh,andRanadipPal. Functionalrandomforest
with applications in dose-response predictions. Scientific Reports, 9(1), feb 2019. doi: 10.1038/
s41598-018-38231-w.
FabrizioMaturoandRosannaVerde.Supervisedclassificationofcurvesviaacombineduseoffunctional
data analysis and tree-based methods. Computational Statistics, 38(1):419–459, 2023. doi: https:
//doi.org/10.1007/s00180-022-01236-1.
Fabrizio Maturo and Rosanna Verde. Combining unsupervised and supervised learning techniques for
enhancingtheperformanceoffunctionaldataclassifiers.ComputationalStatistics,pages1–32,2022a.
doi: https://doi.org/doi.org/10.1007/b98886.
Fabrizio Maturo and Rosanna Verde. Pooling random forest and functional data analysis for biomedi-
cal signals supervised classification: Theory and application to electrocardiogram data. Statistics in
Medicine,41(12):2247–2275,2022b. doi: https://doi.org/10.1002/sim.9353.
Issam-Ali Moindjié, Sophie Dabo-Niang, and Cristian Preda. Classification of multivariate functional
data on different domains with partial least squares approaches. Statistics and Computing, 34(1):5,
2024.
Vincent Brault, Emilie Devijver, and Charlotte Laclau. Mixture of segmentation for heterogeneous
functionaldata. ElectronicJournalofStatistics,18(2):3729–3773,2024.
Donato Riccio, Fabrizio Maturo, and Elvira Romano. Supervised learning via ensembles of diverse
functionalrepresentations: thefunctionalvotingclassifier. arXivpreprintarXiv:2403.15778,2024a.
Donato Riccio, Fabrizio Maturo, and Elvira Romano. Randomized spline trees for functional data
classification: Theoryandapplicationtoenvironmentaltimeseries. arXivpreprintarXiv:2409.07879,
2024b.
J.O. Ramsay, Giles Hooker, and Spencer Graves. Introduction to functional data analysis. In
Functional Data Analysis with R and MATLAB, pages 1–19. Springer New York, 2009. doi:
10.1007/978-0-387-98185-7\_1.
Manuel Febrero-Bande and Manuel Oviedo de la Fuente. Statistical computing in functional data
analysis: The R package fda.usc. Journal of Statistical Software, 2012. ISSN 15487660. doi:
10.18637/jss.v051.i04.
AnthonyBagnall,JasonLines,AaronBostrom,JamesLarge,andEamonnKeogh. Theuea&ucrtime
series classification repository, 2018. URL http://www.timeseriesclassification.
com. Accessed: 2024-09-08.
30Ninad Thakoor and Jean Gao. Shape classifier based on generalized probabilistic descent method with
hidden markov descriptor. In Tenth IEEE International Conference on Computer Vision (ICCV’05)
Volume1,volume1,pages708–713,2005. doi: 10.1109/ICCV.2005.52.
AntonioCuevas, ManuelFebrero, andRicardoFraiman. Robustestimationandclassificationforfunc-
tional data via projection-based depth notions. Computational Statistics, 22(3):481–496, mar 2007.
doi: 10.1007/s00180-007-0053-0.
CristianPreda,GilbertSaporta,andCarolineLévéder. PLSclassificationoffunctionaldata. Computa-
tionalStatistics,22(2):223–235,feb2007. doi: 10.1007/s00180-007-0041-4.
OluwasegunTaiwoOjo,RosaElviraLillo,andAntonioFernandezAnta. fdaoutlier: OutlierDetection
ToolsforFunctionalDataAnalysis,2021. URLhttps://CRAN.R-project.org/package=
fdaoutlier. Rpackageversion0.2.0.
Daniel Conn, Tuck Ngun, and Christina M. Ramirez. fuzzyforest: Fuzzy Forests for Feature Selection
inthePresenceofCorrelatedCovariates,2022. URLhttps://cran.r-project.org/web/
packages/fuzzyforest/fuzzyforest.pdf. Rpackageversion1.0.8.
31