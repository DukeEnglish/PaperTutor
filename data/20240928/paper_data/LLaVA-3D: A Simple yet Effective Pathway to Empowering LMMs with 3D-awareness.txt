LLaVA-3D: A Simple yet Effective Pathway to Empowering
LMMs with 3D-awareness
ChenmingZhu1,2 TaiWang2,† WenweiZhang2 JiangmiaoPang2 XihuiLiu1,†
1TheUniversityofHongKong 2ShanghaiAILaboratory
https://zcmax.github.io/projects/LLaVA-3D
†correspondingauthor
Figure1.OverviewofLLaVA-3D.Block(a)showsthatLLaVA-3Dcouldperformboth2Dand3Dvision-languagetasks.Theleftblock(b)
showsthatcomparedwithprevious3DLMMs,ourLLaVA-3Dachievesstate-of-the-artperformanceacrossawiderangeof3Dbenchmarks
whilemaintainingacomparableperformanceonvarious2DbenchmarkscomparedwithLLaVA-1.5.Themiddleblock(c)demonstratesthat
LLaVA-3Disbuiltonthe2DLMM:LLaVA,andleverages3Dpatchestoendowitwith3Dspatialawareness,enablingittoperformvarious
3Dvision-and-languagetasksinthephysicalworld.Therightblocks(d)and(e)highlightsthesignificantlyfasterconvergenceandinference
speedsofLLaVA-3Dcomparedtoexisting3DLMMs.
Abstract troduceasimpleyeteffectiveframeworkcalledLLaVA-3D.
Leveragingthestrong2DunderstandingpriorsfromLLaVA,
ourLLaVA-3DefficientlyadaptsLLaVAfor3Dsceneunder-
Recent advancements in Large Multimodal Models
standingwithoutcompromising2Dunderstandingcapabili-
(LMMs)havegreatlyenhancedtheirproficiencyin2Dvisual
ties. Toachievethis,weemployasimpleyeteffectiverepre-
understandingtasks,enablingthemtoeffectivelyprocessand
sentation,3DPatch,whichconnects2DCLIPpatchfeatures
understandimagesandvideos. However,thedevelopmentof
withtheircorrespondingpositionsin3Dspace. Byintegrat-
LMMswith3D-awarenessfor3Dsceneunderstandinghas
ingthe3DPatchesinto2DLMMsandemployingjoint2D
beenhinderedbythelackoflarge-scale3Dvision-language
and3Dvision-languageinstructiontuning,weestablisha
datasets and powerful 3D encoders. In this paper, we in-
4202
peS
62
]VC.sc[
1v52181.9042:viXraunifiedarchitectureforboth2Dimageunderstandingand and3Dencoders?
3D scene understanding. Experimental results show that In light of recent progress in 2D LMMs, we propose
LLaVA-3Dconverges3.5×fasterthanexisting3DLMMs a simple yet effective framework, LLaVA-3D, which ex-
when trained on 3D vision-language datasets. Moreover, tendsthewell-established2DLLaVAmodeltoefficiently
LLaVA-3Dnotonlyachievesstate-of-the-artperformance comprehend the 3D world while preserving its robust 2D
acrossvarious3Dtasksbutalsomaintainscomparable2D multimodalperceptionandreasoningcapabilities. Thecore
imageunderstandingandvision-languageconversationca- innovationinourapproachistheintroductionof3DPatch,
pabilitieswithLLaVA. anew3Drepresentationthatbridges2Dfeatureswithina
3D spatial context. This representation is derived by sim-
plyaugmenting2Dpatch-wisefeatureswith3Dpositional
1.Introduction embeddings, eliminating the need for additional complex
processing. After enhancing the 2D visual tokens to 3D
Recent advancements in Large Multimodal Models
patchfeatures,weexplorevariouspoolingstrategiestocom-
(LMMs) [2, 8, 30, 43] have significantly enhanced their
press the 3D patch features across extensive input frames
abilitytounderstandandreasonovervisualandlanguage
beforebeingfedintoLLM.Fine-tunedontheexisting3D
inputs,leadingtoremarkableperformancein2Dvisualtasks,
vision-languagedatasets,ourmodelconvergesrapidlyand
suchas2Dperception,understanding,andreasoning. De-
acquires 3D spatial understanding and grounding capabil-
spitetheiradvancedperceptualandreasoningcapabilities,
ities. Furthermore, the unified model architecture allows
LMMsareprimarilyconfinedtovirtualinteractionsthrough
LLaVA-3Dtoretainthestrong2Dunderstandingandrea-
imagesorvideo,lackingthecriticalabilitytointeractwith
soningabilitiesofLLaVAthroughjointinstruction-tuning
thephysicalworld. Toenabletheirdeploymentinreal-world
on2Dvision-languagedatasets.
applicationsandtofacilitatetheemergenceofnewcapabili-
Our experimental results demonstrate that LLaVA-3D
tiesthroughphysicalinteractions,itisimperativetoequip
achieves state-of-the-art performance on a wide range of
LMMswith3Dspatialintelligence.
3Dtasksandbenchmarks[3,6,13,39–41,51], including
Akeyaspectof3Dspatialintelligenceistheabilityto
3Dcaptioning,3Dquestionanswering,and3Dgrounding
perceiveandunderstandthe3Dworld. Similartohow2D
while requiring significantly less training time and fewer
LMMsalign2Dvisualfeatureswithlanguagemodelsus-
epochsthanexisting3DLMMs. Additionally,LLaVA-3D
inglarge-scale2Dvision-languagedatasets,acommonap-
achievescomparablecapabilitiesin2Dimageunderstanding,
proachtodeveloping3DLMMs[12,19,20]involvesinte-
reasoning,andconversationtoLLaVAthroughjointtuning
grating3DfeaturesencodedfrompointcloudsintoLarge
on2Dand3Dvision-languageinstructions.
LanguageModels(LLMs)andtrainingthemon3Dpoint
cloud-languagedatasets. However,incontrasttotheabun-
2.RelatedWork
danceoflarge-scale2Ddatasets, 3Ddatasetsremainrela-
tivelyscarce. Meanwhile,therearenopowerfulpre-trained 2DLMMs. BuildingonthesuccessofrecentLLMs,nu-
3Dpointcloudencoders,akintoCLIPViT[42]in2D,to merousstudies[2,8,30,35,37,38]exploredLMMsthat
serve as a foundational model that can provide strong 3D can jointly process visual and linguistic information. For
featurestoLLMs. example,LLaVA[37,38]aligned2Dimageswithlanguage
Sincereal-worldembodiedagentstypicallyrelyonego- models through an image encoder and a projection layer,
centric,multi-viewimagesastheirrawobservations,weaim whileBLIP2[30]employedamoresophisticatedQ-Former
tobuilda3Dfoundationmodelbasedonsuchinputsrather architecturetoguidethecompressionofvisualfeaturesusing
than 3D point clouds. There have been attempts [17, 18] textualcues. However,mostearly2DLMMsweretrained
toleveragethe2Dfoundationmodels,likeCLIP,alongside onsingle-imagedatasets,limitingtheirabilitytotacklethe
LLMs to advance this goal. These methods resort to 2D complexitiesofmulti-imageunderstanding. Recently,there
object segmentation results [26] to extract and aggregate hasbeenincreasinginterestinexpandingLMMstohandle
CLIPfeaturesfromobject-centricimagepatches,construct- multi-imageinputs,addressingthedemandsofreal-world
ing pixel-aligned 3D scene features [23]. However, this scenarios. ForvideoLMMs[28,31,34,47],multi-image
multi-view image object segmentation and object-centric inputformsthebasisforcapturingtemporaloraction-related
featureextractionpipelineisinherentlycomplexandcompu- dynamicsacrosssequencesofvideoframes. Ontheother
tationallyintensive. Incontrast,2DLMMs[2,8,30,37,38] hand,multi-viewimagesofthe3Dscenecanimplicitlyre-
directlyleverageCLIP’simagepatchfeatureswithricher, veal3Dspatialrelationshipsandotherabstractrelationsin
fine-grainedinformationforeffectiveimageunderstanding theenvironment. Recentworks [36,41]exploredwhether
andvisualreasoning. Thisnaturallyleadstothequestion: 2DLMMs[1,44]canleveragemulti-viewimagestoperform
Canwedirectlybuilda3DLMMuponthestrong2Dpriors spatial understanding. However, these methods primarily
from2DLMMs,bypassingtheobstaclesin3Ddatascale relied on implicit learning from the data, without directlymodelingthe3Dworld. Incontrast,ourLLaVA-3Dexplic- LLaVAwith3Dawareness,weintroduce3DPatch,anovel
itlymodelsthe3Dworldfrommulti-viewimages,enabling 3Drepresentationthatintegrates3Dspatialinformationinto
advanced3Dspatialunderstandingandgroundingcapabili- 2Dpatchfeatures.
ties.
3.2.3DPatch
Our3DPatchrepresentationsarebuiltuponthe2Dpatch
Injecting3DintoLLMs. As2DLMMsachievedsubstan-
features X′ extracted frommulti-viewimages with CLIP
tialprogressinvisualperception,similareffortshavebeen v
visualencodertoleveragethestrongvisual-semanticalign-
madeinthe3Ddomain. For3Dscene-levelunderstanding,
ment. To construct the 3D Patches, we inject the 3D po-
recentworksexploredwaystointegrate3Dinputssuchas
sition information into the aforementioned 2d patches so
pointclouds[12,19,20]ormulti-viewimages[17,18]into
that the 3D Patches can explicitly model 3D spatial infor-
LLMstoenableadvanced3Dsceneunderstandingandrea-
mationwhilepreservingthesemanticinformationfrom2D
soning. Animportantdistinctionamongthesemethodsis
patches. As illustrated in left block of Fig. 2, given the
howtheyconstructthe3Dscenerepresentation.LL3DA[12]
multi-view2DpatchfeaturesX′ ∈RV×c×w×h,weobtain
directlyusedascene-level3Dpointcloudencodertoextract v
their3DpositionsP ∈RV×3×w×h inthe3Dworld,using
the3Dscenerepresentation. LEO[20]andChat3Dv2[19]
nearestneighbordepthandknowncameraintrinsicandex-
firstsegmented3Dobjectsfromthescenepointcloudus-
trinsic parameters. The 3D positions P are then encoded
ingtheoff-the-shelf3Dinstancesegmentationmodel,and
into 3D position embeddings P′ ∈ RV×w×h×d through
thenindependentlyextracted3Dobjectfeatureswithobject-
alearnabletwo-layerMLP,whicharesubsequentlyadded
level3Dencoderstorepresentthe3Dscene. Ontheother
tothe2Dpatchvisualtokens, resultinginthe3Dpatches
hand,startingfrommulti-viewimages,3D-LLM[18]and
X′ ∈RV×w×h×d:
Scene-LLM [17] resorted to manually crafted 2D object 3D
segmentationtoextractandaggregateCLIPfeaturesfrom
X′ =X′ +MLP(P′) (1)
object-centricimagepatches,constructingpixel-aligned3D 3D v
pointrepresentation. Unliketheseapproaches,ourLLaVA-
3.3.3DPatchPooling
3Ddirectlybuildsonthewell-trained2DLMMwithmulti-
viewimagesasinput. Utilizingthe3Dpositionembeddings, While the 3D Patches equip 2D patches with 3D spatial
itbringsthethe2Dpatcheswithina3Dspatialcontextto awareness, thenumberof3Dpatchesscalesdirectlywith
construct3DPatches. This3Drepresentationenablesquick the number of input images. Capturing a full 3D scene
adaptionofLLaVAfor3Dsceneunderstandingwhilepre- oftennecessitatesalargesetofimages,whichsignificantly
servingitsstrong2Dimageunderstandingability. increases the computational overhead for large language
models(LLMs). Toaddressthis,weintroducea3D-aware
3.Method poolingmechanismtoreducethenumberof3Dpatches,as
illustratedinthemiddleblockofFig.2.
Previous 2D LMMs typically consist of a visual encoder Inthe2Dimageorvideodomain,poolingiscommonly
toextract2Dimagefeatures,whicharethenalignedwith appliedalongthe2Dspatialortemporaldimensionstocom-
the LLM via the projection layer for joint visual and lan- pressthenumberoftokensandextractessentialsemantic
guagereasoningtasks. Inthissection,weintroducehowto information. However,for3Dsceneunderstanding,wemust
bridgethe2Dimagefeatureswithin3Dspatialcontextto poolthe3Dpatchesbasedontheir3Dlocationstoensure
construct3Dpatches(Sec.3.1,3.2),andthendemonstrate thesefeaturescancoverandpreservetheentirescene’sstruc-
the3D-awarepoolingstrategiestocompressthe3Dpatches tureascompletelyaspossible. Weexploretwoparameter-
(Sec.3.2)andfinallypresentthe3D-awarepositionencoding freepoolingstrategiestoachievethis:
anddecodingprocess(Sec.3.4),asillustratedinFig.2.
Voxelization Pooling. Voxelization discretizes the 3D
3.1.Preliminary
spaceintoavolumetricgrid. Withineachoccupiedvoxel,
WechooseLLaVA[38],a2DLMM,toexplorebuildinga the3Dpatchesundergoaveragepooling,resultinginupdated
3D LMM based on it. LLaVA uses the pre-trained CLIP voxelvisualtokens. Onlythevisualtokensfromoccupied
encoder to extract the 2D patch features X ∈ Rc×w×h voxels are passedto the LLM, with the number of tokens
v
fromtheinputimageX ∈R3×W×H,andthenalignthe2D varyingacrossdifferent3Dscenes. Whilethenumberof3D
patchfeaturesX intowithLLMspacewiththeprojection patches scales with the number of images, the number of
v
layer.Asimplemulti-viewimageadaptation[29]forLLaVA voxelpatchesisonlydeterminedbythepartitionofvoxel
couldbeextractingmulti-view2DPatchfeaturesfrommulti- grids.Wecaneasilybalancethenumberofvisualtokensand
view images X′ ∈ RV×c×w×h and sequentially feeding thepreservationoffine-grainedscenefeaturesbycontrolling
v
themintoLLMformulti-imageunderstanding. Toempower thevoxelsize.Figure2.LLaVA-3DArchitecture.BasedonLLaVA,wedirectlyaddthecorresponding3Dpositionembeddingsto2Dpatchvisualtokens
ofmulti-viewimagestoconstructthe3DPatches,thenthe3DPatcheswillundergo3Dpoolingandbesentintotheprojectionlayerof
LLaVAtomapintotheLLMspaceandalignwiththeLLMusing3D-visual-languagedata.
FPSPooling. FarthestPointSampling(FPS)isawidely throughatwo-layerMLP.These3Dcoordinatetokensare
usedsamplingstrategytoselectarepresentativesubsetof fedintoLLMtogetherwith3DPatchtokensandtexttokens,
points from a larger set of points cloud. We apply FPS enabling3D-awareperceptionandreasoning.
to sample 3D patches from multi-view images to a fixed
numberoftokens,ensuringthatthesampledtokensrepresent
Decodingof3DBoundingBoxOutput. Theintegration
theentirescenestructure. Whilefixingthenumberoftokens
ofthe3Dcoordinatetokenempowersthemodeltoprocess
helpstheLLMefficientlyprocessvisualinformation,itmay
3D coordinate information from input instructions effec-
alsoresultinlossofsceneinformation.
tively. However,experimentsshowthatdirectlyoutputting
ComparedtoFPSPooling,VoxelizationPoolingoffersequiv-
3DboundingboxesisquitechallengingforLLM,andem-
alentefficiencyinvisualtokencompressionwhilepreserving
pirically, the performance is poor. To overcome this, we
moredetailedsceneinformation. Furthermore,theexplicit
leverageanapproachsimilartopreviousmethod [51],intro-
constructionofvoxelgridscanbetterhandledynamic3D
ducingaspecializedlocationtokenthatguidesthegrounding
sceneupdates,whereasFPSPoolingexcelsatpreservingthe
module to generate accurate 3D bounding boxes. Specifi-
overall3Dscenestructure. Weconductquantitativeexperi-
cally,theLLMpredictsaspeciallocationtokentorepresent
mentsinSec.5.7tothoroughlyassesstheeffectivenessof
a3Dboxpredictionwhenthetasknecessitates3Dbounding
thesepoolingstrategies.
boxoutputs. Wethenderivethelastlayerembeddingofthis
3.4.3D-awarePositionEncoding&Decoding locationtokenandsenditintothegroundingmodule. The
groundingmoduleutilizesthelocationtokenembeddingand
Intheprevioussections,wedetailedtheconstructionofthe the3Dscenefeaturetopredictthe3Dboxcoordinatesofthe
3Dscenerepresentationfrommulti-viewimages,establish- targetobject. Thisprocessfacilitatestheprecisegeneration
ingthefoundationforfurtherinteractionwiththe3Dscene. of3Dboundingboxoutputs. Moredetailscanbefoundin
Buildingonthis,theLLMcouldprocessmulti-modalinputs theappendix.
suchasthe3Dscene,languageinstructions,and3Dcoordi-
natecuestogenerateoutputssuchaslanguageresponsesand 4.Training
3Dboundingboxes,asillustratedintherightblockofFig.2.
Inthissection,weintroducehowthemodelisequippedto Toleveragethe2Dpriorsfromestablished2DLMMs,we
interpret 3D coordinate information from inputs and sub- trainourLLaVA-3Dmodelbasedonthepre-trainedLLaVA-
sequentlyoutputprecise3Dboundingboxeswhenspecific 1.5. Consideringthescarcityof3Dscene-languagedata,our
location-relatedtaskrequirementsareneeded. trainingprocedurecomprisestwostages,eachfocusingon
differenttrainingtargetsofthemodel.
Encoding of 3D Coordinate Input. In scenarios such
as 3D dense object captioning or object-centric question Stage 1: 3D Patch Language Alignment. During the
answering,thelanguageinstructioncontains3Dcoordinates. firsttrainingstage,weusetheregion-levelandscene-level
Tohandlesuchtasks,weintroducethe3DCoordinateToken captiondatathatdescribespatialrelationshipsamong3Dob-
to allow the model to integrate the provided coordinates jectstoalignthe3DpatcheswiththeLLMforenhanced3D
as context into its reasoning processes. Specifically, we spatialcomprehension. Atthisstage,theinputmulti-view
obtainthe3Dcoordinatetokenbyfeedingthe3Dcoordinates images used are selected from sequences that correspondTable1.QuantitativecomparisonwithSOTAmodelsonvarious
3DQAtasks.“C”standsfor“CIDEr”,“B-4”for“BLEU-4”,“M”
for“METEOR”,“R”for“ROUGE”,“Sim”forsentencesimilarity,
and“EM@1”fortop-1exactmatch. Grayindicatesevaluation
resultswithrefinedexact-matchprotocol.
ScanQA(val) SQA3D(test)
C B-4 M R EM@1 EM@1
Task-specificmodels
Scan2Cap[13] - - - - - 41.0†
ScanRefer+MCAN[49] 55.4 7.9 11.5 30.0 18.6 -
ClipBERT[27] - - - - - 43.3
ScanQA[3] 64.9 10.1 13.1 33.3 21.1 47.2
Figure3.LLaVA-3D-Instruct-1M.Thehybrid2Dand3DDataset 3D-VisTA[52] 69.6 10.4 13.9 35.7 22.4 48.5
Collection. Left: Distributionofdataacrosscategories,withthe Task-specificfine-tuned3DLMMs
3D-LLM(FlanT5)[18] 69.4 12.0 14.5 35.7 20.5
outercirclerepresentingallcategoriesandtheinnercircleillustrat- LL3DA[37] 76.8 13.5 15.9 37.3 -
ingdatasubsetdistribution.Right:Detaileddatasetquantities. Chat-3Dv2[19] 87.6 14.0 - - - 54.7
LEO[20] 101.4 13.2 20.0 49.2 24.5(47.6) 50.0(52.4)
Scene-LLM [17] 80 12.0 16.6 40.0 27.2 54.2
Zero-shot2DLMMs
VideoChat2[32] 49.2 9.6 9.5 28.2 19.2 37.3
to specific regions or entire scenes. We freeze the vision
LLaVA-NeXT-Video[28] 46.2 9.8 9.1 27.8 18.7 34.2
encoderandLLMparameters,andonlytraintheprojection GPT-4V 59.6 - 13.5 33.4 - -
Gemini 68.3 - 11.3 35.4 - -
layer and 3D position embedding layer, encouraging effi- Claude 57.7 - 10.0 29.3 - -
cientalignmentbetween3Dpatchfeaturesandtextspace. LLaVA-3D 91.7 14.5 20.7 50.1 27.0(45.0) 55.6(57.6)
Since3DpatchesarederivedfromCLIPfeaturesaugmented
with3Dpositionalinformation,thealignmentbetween3D
PatchandLLMconvergesrapidly. requirements.
Stage2:TaskInstructionTuning. Duringtheinstruction-
5.Experiments
tuningstage,LLaVA-3Disoptimizedtorespondtocomplex
3DV&Ltasksandmaintainitsinherent2Dimagereason-
Inthissection,weconductextensiveevaluationstoexamine
ingandinstruction-followingcapabilities. Tofacilitatethis
thecapabilitiesofLLaVA-3D.Tobeginwith,weintroduce
capability,wecollecttheLLaVA-3D-Instruct-1Mdataset,
the implementation details (Sec. 5.1). Then, we compare
ahybridcollectionof2Dand3Ddataspecificallytailored
ourmodel’s3Dsceneunderstanding(Sec.5.2,5.3,5.4)and
forinstructiontuning. Theoveralldistributionofthedataset
2Dimageunderstanding(Sec.5.5)capabilitywithprevious
collectionisshowninFig3,moredetailsabouttheinstruc-
methods. Finally, we conduct a thorough analysis to vali-
tionaltuningdatasetsarelistedintheappendix.The2DData
datetheeffectivenessofthecomponentsanddesignsofour
ofLLaVA-3D-Instruct-1MisderivedfromexistingLLaVA-
LLaVA-3D(Sec.5.6,5.7).
1.5instructiontuningdata,ensuringthepreservationof2D
imagecomprehensionandvision-languageconversationabil-
ities. Whentuningwith2Ddata,wekeepthevisionencoder 5.1.ImplementationDetails
frozenandjointlytraintheprojectionlayerandLLM.The
3DDataofLLaVA-3D-Instruct-1M,ontheotherhand,com- LLaVA-3DisbuiltupontheLLaVA-1.5-7B,utilizingtheir
prisesdatafromdiverse3DQA,3Ddensecaptioning,and pre-trainedweightsfromtheHuggingFacelibrary. For3D
3Dgroundingtasks. Duringthe3Ddatainstructiontuning, tasks,weaddthe3Dpositionembeddingstothe2Dpatch
the3Dpositionembeddinglayerwillbeaddedtojointlytrain visualtokens,andutilizethevoxelizationpoolingstrategy
withtheothermodules. Additionally, fortaskswherethe toreduce3Dpatchnumberbeforepassingtheinputvisual
instructioncontains3Dcoordinateinformationorrequires tokens to the projection layer and LLM. The number of
3Dboundingboxoutputs,thecorrespondingencodingand viewsV issettobe20andvoxelsizeissetto0.2m. Due
decodingmoduleswillbetrainedtogether. Duringinstruc- totheLLMcontextlengthlimitation,themaximumnumber
tiontuning,the3Ddatapathwayincludesthe3Dposition of 3D patch tokens after 3D-aware pooling is set to 3096.
embeddingsand3Dpatches,whilethe2Ddatapathwayis For2Dtasks,LLaVA-3DfunctionsthesameasLLaVA.All
theoriginalLLaVA.Allmodulesexceptforthe3Dposition experiments are conducted on 8 × 80G A100 GPUs. We
embeddingstoconstruct3Dpatchesaresharedacross2D trainourmodelfor1epochwithalearningrateof1e-3and
and 3D data. This training setup ensures that LLaVA-3D abatchsizeof32instage1,andfine-tuneonthecollected
iscapableofprocessingboth2Dand3Dvisualtokensef- LLaVA-3D-Instruct-1Mdataset,withalearningrateof2e-5
fectively,andisadaptivetovarioustaskformulationsand andabatchsizeof16instage2.Table2.QuantitativecomparisononMMScanQAbenchmark.“S.-BERT",“B-1”,“B-4”,“R.-L.”,“MET.”represents“Sentence-BERT",
“BLEU-1”,“BLEU-4”,“ROUGE-L”,“METEOR”,respectively.Here,wereportthetop-1exactmatchwith(therefinedexact-matchprotocol
results)for“EM@1”.
Single-target Inter-target Data-drivenMetrics TraditionalMetrics
Methods Setting Overall Advanced
ST-attr ST-space OO-attr OO-space OR SimCSE S.-BERT B-1. B-4. R.-L MET. EM@1
3D-LLM[18] 28.6 37.8 18.8 13.7 26.3 15.4 20.8 40.4 40.3 13.4 1.5 17.3 6.0 6.2(19.6)
Chat3D-v2[19] 27.9 38.1 18.3 9.3 22.4 13.5 25.4 45.4 46.3 18.0 3.0 22.9 7.5 10.2(19.6)
Zero-Shot
LL3DA[12] 15.8 15.5 14.7 14.2 25.2 4.3 6.4 40.7 43.6 5.4 2.1 16.4 4.4 8.3(19.4)
LEO[20] 22.2 28.9 17.6 18.1 20.4 15.0 16.3 40.4 41.0 11.0 0.7 17.1 4.9 9.6(18.7)
LL3DA[12] 38.5 40.4 46.2 14.7 47.1 26.4 7.1 65.3 67.0 26.4 8.5 44.3 14.7 30.2(37.6)
Fine-tuning
LEO[20] 47.8 55.5 49.5 36.1 45.6 32.1 38.4 71.2 72.2 32.0 12.5 52.1 17.7 36.6(44.5)
LLaVA-3D Generalist 52.3 61.2 54.4 28.7 61.2 44.5 43.6 74.6 76.3 38.7 13.1 55.5 19.5 45.2(51.4)
Table 3. Quantitative comparison with SOTA models on LLaVA-3D could perform as a generalist and achieve the
OpenEQAbenchmark SOTAperformanceonthesebenchmarks.
Models Frame Accuracy
Coordinate Spatial Understanding with MMScan QA.
LLaMA2[45] 0 28.3
MMScanQAincludes5.2kscansfromScanNet, 3RScan,
GPT-4[1] 0 33.5
Claude3 20 36.3 andMatterport3D,alongwith116ktrainingquestionsand
Gemini-Pro[44] 15 44.9 29kvalidationquestions.Thesequestionsspanexistentialin-
GPT-4V[1] 15 54.6 quiries,attributeunderstanding,andmoreadvancedqueries.
GPT-4V[1] 50 55.3 UnlikeScanQAandSQA3D,someMMScanQAquestions
Human Full 86.8 require3Dreasoningbasedonobjectcoordinates,ratherthan
LLaVA-3D 20 51.2 relyingsolelyontextdescriptions,demandingthemodelca-
pable of understanding 3D coordinates information. All
data samples are classified into one of the following sub-
5.2.Evaluationon3DQuestionAnswering categories:ST-attr,ST-space,OO-attr,OO-space,OR,where
STstandsforSingle-target,attrforattribute,OOforObject-
3D Question Answering requires a model to generate re- Object,andORforObjectRegion. Besides,thereisaminor
sponses to the natural language queries questioning to- partofQAsamplesforadvancedunderstandingandreason-
wards a 3D scene. In this section, we validate LLaVA- ing,suchassituatedQArelatedtoeverydaylife. Wepresent
3Dperformanceonvarious3Dquestionansweringbench- theresultsunderGPT-4evaluation,data-drivenmetrics,and
marks: ScanQA[3],SQA3D[40],MMScanQA[39],and traditionalmetricsrespectivelyinTab.2. Inthisbenchmark,
OpenEQA[41]. thewell-trainedLL3DAandLEOarefurtherfine-tunedon
thefull1.2MMMScanQAtrainingdataset. OurLLaVA-3D,
trained on LLaVA-3D-Instruct-1M (which includes 440K
Spatial Understanding with ScanQA and SQA3D.
MMScanQAtrainingsamples),achievessignificantlybetter
ScanQA and SQA3D are both built on ScanNet dataset.
performanceontheMMScanQAbenchmarkcomparedto
TheScanQAdatasetconsistsof41363questionsabout800
thespeciallyfine-tunedLL3DAandLEO.Theresultshigh-
scenes,including32337uniquequestions. Itsvalidationset
lightthetrainingefficiencyofLLaVA-3Danditsstrong3D
contains4675questionsabout71scenes. SQA3Dcomprises
understandingabilitytoserveasthegeneralistmodel.
20.4kdescriptionsof6.8kuniquesituationscollectedfrom
650ScanNetscenesand33.4kquestionsaboutthesesitua-
tions. QuestionsinScanQArequirebasicrecognitionand Embodied Question Answering with OpenEQA.
3Dreasoningcapabilities,andSQA3Dfurtherincorporates OpenEQAisthefirstopen-vocabularybenchmarkdesigned
thesituationunderstandingandsituatedreasoningintoem- for spatial understanding and embodied reasoning in
bodied3Dsceneunderstanding. Followingpriorworks,we embodied question answering, specifically in the era of
adoptBLEUscores,METEOR,ROUHE-L,CIDErandEM foundation models. It features an automated evaluation
(“exactmatch”)asourevaluationmetricsforScanQAand protocolpoweredbyLLMs,whichshowsstrongalignment
EMforSQA3Drespectively. AsshowninTab.1,current with human judgment. Our evaluations are conducted
2DLMMsfailtoachievecompetitiveperformancewiththe usingtheEM-EQAdatasplitofOpenEQA,whichincludes
latest3DLMMstrainedwith3Dawareness,whichmightbe over 1,600 high-quality, human-generated questions from
attributedtothelackofexplicit3Drepresentation. Besides, diversereal-worldenvironments. InEM-EQA,themodel
comparedwithtask-specificfine-tuned3DLMMsthatneed is required to generate a textual answer to a question
tobefurtherfine-tunedonthecorrespondingdatasets,our based on an episode history, which typically consistsTable4.QuantitativeComparisonswithSOTAmodelsfor3D Table6.QuantitativecomparisonwithSOTAmodelsonvarious
DenseCaptioningonScan2Cap.“C”standsfor“CIDEr”,“B-4” 3DVGtasks.
for“BLEU4”, “M”for“METEOR”, “R”for“ROUGE”, “Sim”
forsentencesimilarity,and“EM@1”fortop-1exactmatch.The ScanRefer
n-grammetricsforScan2CaparegovernedbyIoU@0.5.
Acc@0.25 Acc@0.5
Task-specificmodels
Scan2Cap(Val)
C@0.5↑ B-4@0.5↑ M@0.5↑ R@0.5↑ ScanRefer[6] 37.3 24.3
Scan2Cap[13] 39.08 23.32 21.97 44.78 MVT[21] 40.8 33.3
MORE[24] 40.94 22.93 21.66 44.42 3DVG-Trans[50] 45.9 34.5
SpaCap3D[46] 44.02 25.26 22.33 45.36 ViL3DRel[10] 47.9 37.7
D3Net[7] 46.07 30.29 24.35 51.67 BUTD-DETR[22] 52.2 39.8
UniT3D[14] 46.69 27.22 21.91 45.98 ReGround3D[51] 53.1 41.1
3DJCG[5] 49.48 31.03 24.22 50.80
3D-VLP[25] 54.94 32.31 24.83 51.51 Task-specificfine-tuned3DLLMs
3D-VisTA[52] 61.60 34.10 26.80 55.00 LLM-Grounder[48] 17.1 5.3
Vote2Cap-DETR[11] 61.81 34.46 26.22 54.40 3D-LLM[18] 30.3 -
LL3DA[12] 65.19 36.79 25.97 55.06 Chat3D-v2[19] 35.9 30.4
LEO[20] 72.4 38.2 27.9 58.1
LLaVA-3D 79.21 41.12 30.21 63.41 LLaVA-3D 54.1 42.2
Table5. QuantitativecomparisonwithSOTAmodelsonthe
MMScanCaptioningbenchmark. benchmark utilizes GPT4 to assess whether these aspects
are correct in the object description. We benchmark var-
model Evaluator Type Color Shape Position Function Design Overall ious methods on the MMScan Captioning benchmark in
LL3DA[12] GPT 10.0 26.3 40.6 38.9 67.5 21.7 33.6
LEO[20] GPT 34.9 29.7 63.0 63.7 75.0 42.7 51.3 Tab.5. Theresultsshowthatourmethodsurpassesexisting
LLaVA-3D GPT 39.9 79.2 89.1 82.2 94.1 88.0 78.8
approachesacrossallmetricsbyasubstantialmargin,espe-
ciallyachievinga49.5%improvementintheColorscoreand
of RGB images, depth information, camera poses, and a43.3%improvementintheDesignscore. Thissignificant
intrinsic camera data. The results in Tab. 3 demonstrate performanceboostcanbeattributedtoour2DLMM-based
that LLaVA-3D surpasses Claude3 and Gemini-Pro, and architecture.
achievescomparableperformancewithpowerfulGPT-4Von Uniquely,LLaVA-3Dtakesmulti-viewimagesasinputs,
thisbenchmarkwithsignificantlyfewermodelparameters. enablingauser-friendlyfeaturewhereuserscansimplyclick
onselectedimagestogenerateboth3Dobjectcaptionsand
5.3.Evaluationon3DDenseCaptioning
3Dboundingboxes. Werefertothiscapabilityas2DClick-
based3DDenseCaptioning. (asillustratedinFig.4).
3Ddensecaptioningrequiresthemodeltolocalizeallthe
objectsina3Dsceneandthengeneratedescriptivesentences
5.4.Evaluationon3DVisualGrounding
foreachobject. Toevaluateourmodelonthe3Ddensecap-
tioningtasks,weutilizetheoff-the-shelfsegmentationmodel
3Dvisualgroundingaimstolocalizethetargetobjectinthe
Mask3Dtogenerateobjectproposals. Thenwefurthercon-
3Dsceneusingnaturallanguagedescriptions.Inthissection,
structthe3Dcoordinatetokensbasedonthe3Dobjectcenter
we initially report the performance on the ScanRefer [6]
coordinatestoguidethemodeltoperformthetask.Addition-
benchmark. Forquantitativecomparisons,weincludeboth
ally,weutilizetwotypesoftextualinstructionsthatprompt
task-specific approaches and generalist models: the state-
themodeltoeitherdescribetheobjectordescribeandoutput
of-the-artspecialistsin3DVGandgeneralistmodelslike
theboundingboxoftheobject. Wereporttheperformance
LLM-Grounder [48], 3D-LLM [18], and Chat3D-v2 [19].
ofvariousmethodsontwo3Ddensecaptioningbenchmarks:
3D-LLM[18]]usesthelocationtokenstodiscretizethe3D
Scan2CapandMMScanCaptioning.
sceneandpredictstheboundingboxasthelocationtokens
that were added to the vocabulary. Chat-3D v2 [19] first
Scan2Cap. Scan2Caprequiresthemodeltodescribethe detects all the objects and then identifies the object that
object’s appearance and the spatial relations with nearby bestmatchestheprovideddescription. TheresultsinTab.6
objectsandoutputthecorresponding3Dboundingbox. As demonstratethatbycombiningwiththegroundingmodule,
illustratedinTab.4, ourmethodconsistentlyoutperforms ourLLaVA-3Dcouldachieveevenbetterperformancecom-
theexistingmethodontheScan2Capbenchmark. paredwiththetask-specificspecialistsmodelasageneralist.
SuchaparadigmallowsLLaVA-3Dtointegratewithvarious
MMScanCaptioning. MMScanCaptioningfocuseson powerful 3D grounding modules and inject the 3D under-
identifyingcommonaspectsof3DobjectssuchasObject standingcapabilityandworldknowledgeintothegrounding
Type, Color, Shape, Position, Function, and Design. The process,asillustratedinFig.7.Figure4.LLaVA-3Denablestheuser-friendlyinteractionwiththe3Dsceneacrossvarious3Dunderstandingandreasoningtasks.Itallows
theuserstojustclickonthe2Dimagesorthevideoframetosimplyconducttheinteractive3Dquestionansweringand3Ddensecaptioning.
Table7.QuantitativeComparisonswithSOTAmodelsonzero- Table9. Effectiveness of3DPatch Representation. Training
shot2Dbenchmarks. usingtheinstructiontuningdatasets,theonlydifferencebetween
multi-imageLLaVAandLLaVA-3Disthepatchtype.
Method LLM Res. VQAT MMB MME MM-Vet
MobileVLM[15] MLLaMA2.7B 336 47.5 59.6 1289 - Method PatchType ScanQA SQA3D MMScanQA Scan2Cap
InstructBLIP[16] Vicuna-7B 224 50.1 36.0 – 26.2 multi-imageLLaVA 2D 24.1 52.3 32.7 29.1
InstructBLIP[16] Vicuna-13B 224 50.7 – 1213 25.6 LLaVA-3D 3D 27.0(+2.9) 55.6(+3.3) 41.5(+8.8) 79.2(+50.1)
Qwen-VL[4] Qwen-7B 448 63.8∗ 38.2 – –
Qwen-VL-Chat[4] Qwen-7B 448 61.5∗ 60.6 1488 –
Shikra[9] Vicuna-13B 224 – 58.8 – –
LLaMA-VID[33] Vicuna-7B 336 – 65.1 1521 – (LMM)toa3DLMM,asopposedtodevelopinga3DLMM
LLaMA-VID[33] Vicuna-13B 336 – 66.6 1542 –
LLaVA-1.5[38] Vicuna-7B 336 58.2 65.2 1511 31.1 solelyfromLLMs. Anotableapproachinthelattercategory
LLaVA-1.5[38] Vicuna-13B 336 61.3 69.2 1531/295 36.1 is3D-LLM,whichleveragesfoundational2Dvisualmodels
LLaVA-3D Vicuna-7B 336 57.8 65.0 1502 30.9
suchasSAMandCLIPforfeatureextraction. Thismethod
then employs 3D multi-view fusion to generate 3D point
Table8. ArchitectureComparisononvarious3DV&LBench-
features, followed by the use of a Q-Former to condense
mark.
thesepointfeaturesintoafixednumberoftokens.
3DFeature Connector LLM/LMM ScanQA SQA3D Inferencetime
(a) (SAM+CLIP)w/PE Q-Former Vicuna-7B 21.9 49.3 900s
(b) (SAM+CLIP)w/PE Pooling+MLP Vicuna-7B 22.1 49.2 900s ArchitectureComparison. Toensurethefairnessofthe
(c) CLIPw/PE Pooling+MLP Vicuna-7B 23.4 51.2 0.2s experiment as much as possible, starting from LLM, we
(d) CLIPw/PE Pooling+MLP LLaVA-1.5 27.0 55.6 0.2s
(e) CLIPw/PE Pooling+MLP PLLaVA 27.9 56.2 0.2s firstablatedifferent3Dfeatureencoders,and3D-language
connectorsusingthesameinstructiontuningdatasets. As
shown in Tab. 8, the Q-Former (a) and Pooling + MLP
5.5.Evaluationon2Dbenchmarks
(b) share a similar performance on 3D V&L benchmarks.
Sinceourmodelistrainedonamixof2Dand3Ddatasets, Notably,usingCLIP(c)aloneinsteadofSAM+CLIP(b)
we evaluate it across various 2D benchmarks to ensure it achieves better performance and significantly reduces 3D
retainsthe2Dimageunderstandingcapabilitiesoftheorigi- featureextractioncomputationtimefrom900sto0.2s.
nalLLaVA.AsdemonstratedinTab.7,LLaVA-3Dachieves
performancecomparabletothatofLLaVA-1.5acrosssev-
Effectivenessof3DPatch. Oncewedon’tdecoratethe2D
eral2Dimageunderstandingbenchmarks.Thisperformance
patcheswithcorresponding3Dpositionembedding,LLaVA-
underscoresthearchitecturalsuperiorityofourmodelcom-
3Dwilldegenerateinto2Dmulti-imageLMMs. Tofurther
paredtoother3DLMMs.
ascertaintheefficacyofourproposed3Drepresentation: 3D
Patch,weconductadditionalexperimentsacrossavariety
5.6.ArchitectureAnalysis
of3Dquestionansweringand3Ddensecaptioningbench-
Inthissection,wedelvedeeperintothearchitecturalbene- marks. AsshowninTab.9,integrating2Dpatcheswithin
fitsandefficacyofadaptinga2Dlargemultimodalmodel a3Dcontextenhancesthemodel’s3Dspatialunderstand-Table10.Comparsionondifferentpoolingstrategies. Table11.Comparisononperformanceon3DQAtasksunder
differentnumberofmulti-viewimages.
PoolingStratgey VoxelSize TokenNumber ScanQA SQA3D
Voxelization 0.4 Dynamic 24.1 53.2 NumberofViews NumberofTokens ScanQA SQA3D
Voxelization 0.3 Dynamic 25.9 54.8 16 9216 26.2 55.1
Voxelization 0.2 Dynamic 27.0 55.6
20 11520 27.0 55.6
FPS - 576 25.7 54.9
24 13824 27.0 55.4
FPS - 1024 26.3 55.2
40 23040 26.7 55.2
ingandreasoningcapabilities,resultingin2.9%and3.3% egocentricimagesofthe3Dsceneinourexperiment. Inthis
improvementsontheScanQAandSQA3Dbenchmarks,re- section,weexploretheeffectofdifferentimagesampling
spectively. Additionally,3Dpatchesarecrucialfortasksthat strategiesduringtheinferencestage: 1)UniformSampling:
requireexplicit3Dworldmodeling,leadingtosignificant Toachievecomprehensivecoverageoftheentire3Dscene,
improvementsof8.8%ontheMMScanQAbenchmarkand astraightforwardapproachisuniformsampling,whichsam-
animpressive50.1%ontheScan2Capbenchmark. plesimagesevenly.2)Text-GuidedSampling:suchsampling
strategy uses CLIP to select the frames related to the text
Benefits from pre-trained 2D LMM. Leveraging our instructionduringinferencebasedontheimage-textCLIP
foundation in 2D LMMs, our framework benefits signifi- similarityscore.Ourexperimentsdemonstratethatthesetwo
cantlyfromtherobustpre-trainingonextensiveimage/video- samplingstrategiessharesimilarperformanceonScanQA
text datasets. In Tab. 8, we explore the advantages of ini- andSQA3D,sowechooseuniformsamplingforsimplicity.
tializing from a pre-trained 2D LMM compared to start-
ing directly from an LLM. The experimental results con- Number of Views. An intuitive assumption is that sam-
sistently demonstrate that starting with a well-tuned 2D pling more views from the 3D scene will preserve more
LMMsubstantiallycouldenhanceperformancein3Dspa- informationaboutthe3Dscene. Weconductacomparative
tialunderstandingtasks. Besides,weobservethatstronger experimentvaryingthenumberofviewssampledfrom3D
base2DLMMcouldleadtobetter3Dspatialunderstand- scenes. Tab.11presentstheExactMatch(EM)scoreson
ing performance. Besides, we find that initializing from ScanQAandSQA3Dacrossdifferentsettings,revealingthat
a pre-trained 2D LMM could achieve 3.5× faster training theincreaseinEMscoreismarginalasthenumberofviews
convergencespeedofLLaVA-3Dcomparedwithexisting increases. Additionally, the experimental results indicate
3D LMMs [12, 20]. More details could be found in our thatexceedingacertainnumberofviewscandegradethe
appendix. model’sperformance.
5.7.MoreAnalysis
6.Conclusion
Tobetterunderstandtheimpactofdifferentcomponentsof
WeproposeLLaVA-3D,asimpleyeteffectiveframework
ourLLaVA-3D,weconductathoroughablationstudyon
builtuponthewell-established2DLLaVAmodel. LLaVA-
theScanQAandSQA3Dbenchmarks.
3DextendstheLLaVA’scapabilitiestounderstandthe3D
worldbyusing3Dpatchestobridge2Dfeatureswithina
ImpactofPoolingStrategy. Giventhemaximumtoken 3Dspace,enablingspatialunderstandingwhileefficiently
lengthlimitationofLLMs,weapplypoolingtothe3Dpatch preservingtheoriginal2Dimageunderstandingandreason-
tokensextractedfrommulti-viewimagestoreducethenum- ingcapability. Experimentalresultsshowthatourmethod
beroftokens.However,thispoolingprocessinevitablyleads setsnewstate-of-the-artperformanceonawiderangeof3D
tosomeinformationloss. Tounderstanditsimpactonper- tasksandbenchmarks. Wehopethatourmodelwillinspire
formance,weconductedexperimentstoevaluatetheeffects newideasforbuilding3DLMMs,andinthefuture,weplan
ofpooling. AsshowninTab.10,thevoxelizationpooling toexploretheapplicationofLLaVA-3Dinmoredownstream
strategy outperforms the FPS pooling method on 3D QA scenarios,suchasrobotmanipulationandnavigation.
benchmarks. Reducing the voxel size in the voxelization
pooling or increasing the number of 3D patch tokens ob- References
tainedthroughFPSpoolingcanbothenhancethemodel’s
[1] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,
performancetosomeextent.
IlgeAkkaya,FlorenciaLeoniAleman,DiogoAlmeida,Janko
Altenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4
Multi-View Images Sampling Strategy. Due to the re- technicalreport.arXivpreprintarXiv:2303.08774,2023.2,6
dundancyofinformationamongmulti-viewimagesandcon- [2] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,Antoine
sideringcomputationalcosts,wesampleV viewsfromthe Miech,IainBarr,YanaHasson,KarelLenc,ArthurMensch,KatherineMillican,MalcolmReynolds,etal. Flamingo: a [14] ZhenyuChen,RonghangHu,XinleiChen,MatthiasNießner,
visuallanguagemodelforfew-shotlearning. Advancesin andAngelXChang. Unit3d: Aunifiedtransformerfor3d
Neural Information Processing Systems, 35:23716–23736, densecaptioningandvisualgrounding. InProceedingsof
2022. 2 theIEEE/CVFinternationalconferenceoncomputervision,
[3] DaichiAzuma,TaikiMiyanishi,ShuheiKurita,andMotoaki pages18109–18119,2023. 7
Kawanabe. Scanqa:3dquestionansweringforspatialscene [15] XiangxiangChu, LimengQiao, XinyangLin, ShuangXu,
understanding. InproceedingsoftheIEEE/CVFconference YangYang,YimingHu,FeiWei,XinyuZhang,BoZhang,
oncomputervisionandpatternrecognition,pages19129– XiaolinWei,etal.Mobilevlm:Afast,reproducibleandstrong
19139,2022. 2,5,6 visionlanguageassistantformobiledevices. arXivpreprint
[4] JinzeBai, ShuaiBai, ShushengYang, ShijieWang, Sinan arXiv:2312.16886,2023. 8
Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren [16] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuat
Zhou. Qwen-vl:Afrontierlargevision-languagemodelwith Tiong,JunqiZhao,WeishengWang,BoyangLi,PascaleFung,
versatileabilities. arXivpreprintarXiv:2308.12966,2023. 8 andStevenHoi.Instructblip:Towardsgeneral-purposevision-
[5] DaigangCai,LichenZhao,JingZhang,LuSheng,andDong languagemodelswithinstructiontuning,2023. 8
Xu. 3djcg: Aunifiedframeworkforjointdensecaptioning [17] Rao Fu, Jingyu Liu, Xilun Chen, Yixin Nie, and Wen-
andvisualgroundingon3dpointclouds. InProceedingsof han Xiong. Scene-llm: Extending language model for
theIEEE/CVFConferenceonComputerVisionandPattern 3d visual understanding and reasoning. arXiv preprint
Recognition,pages16464–16473,2022. 7 arXiv:2403.11401,2024. 2,3,5
[6] DaveZhenyuChen,AngelXChang,andMatthiasNießner. [18] YiningHong,HaoyuZhen,PeihaoChen,ShuhongZheng,
Scanrefer:3dobjectlocalizationinrgb-dscansusingnatural YilunDu,ZhenfangChen,andChuangGan.3d-llm:Injecting
language. InEuropeanconferenceoncomputervision,pages the 3d world into large language models. arXiv preprint
202–221.Springer,2020. 2,7 arXiv:2307.12981,2023. 2,3,5,6,7
[7] DaveZhenyuChen, QiruiWu, MatthiasNießner, andAn- [19] HaifengHuang,ZehanWang,RongjieHuang,LupingLiu,
gelXChang. D3net:Aunifiedspeaker-listenerarchitecture XizeCheng,YangZhao,TaoJin,andZhouZhao. Chat-3d
for3ddensecaptioningandvisualgrounding. InEuropean v2:Bridging3dsceneandlargelanguagemodelswithobject
ConferenceonComputerVision,pages487–505.Springer, identifiers. arXivpreprintarXiv:2312.08168,2023. 2,3,5,6,
2022. 7 7
[8] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun [20] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun
Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Linghu,PuhaoLi,YanWang,QingLi,Song-ChunZhu,Baox-
VikasChandra, YunyangXiong, andMohamedElhoseiny. iongJia,andSiyuanHuang. Anembodiedgeneralistagentin
Minigpt-v2: large language model as a unified interface 3dworld. arXivpreprintarXiv:2311.12871,2023. 2,3,5,6,
for vision-language multi-task learning. arXiv preprint 7,9
arXiv:2310.09478,2023. 2 [21] ShijiaHuang,YilunChen,JiayaJia,andLiweiWang. Multi-
[9] KeqinChen,ZhaoZhang,WeiliZeng,RichongZhang,Feng viewtransformerfor3dvisualgrounding. InProceedingsof
Zhu,andRuiZhao. Shikra: Unleashingmultimodalllm’s theIEEE/CVFConferenceonComputerVisionandPattern
referentialdialoguemagic. arXivpreprintarXiv:2306.15195, Recognition,pages15524–15533,2022. 7
2023. 8 [22] AyushJain,NikolaosGkanatsios,IshitaMediratta,andKate-
[10] Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, rinaFragkiadaki. Bottomuptopdowndetectiontransformers
CordeliaSchmid,andIvanLaptev. Languageconditioned forlanguagegroundinginimagesandpointclouds. InCom-
spatialrelationreasoningfor3dobjectgrounding. Advances puterVision–ECCV2022: 17thEuropeanConference,Tel
inNeuralInformationProcessingSystems,35:20522–20535, Aviv,Israel,October23–27,2022,Proceedings,PartXXXVI,
2022. 7 pages417–433.Springer,2022. 7
[11] SijinChen,HongyuanZhu,XinChen,YinjieLei,GangYu, [23] KrishnaMurthyJatavallabhula,AlihuseinKuwajerwala,Qiao
andTaoChen.End-to-end3ddensecaptioningwithvote2cap- Gu, Mohd Omama, Tao Chen, Shuang Li, Ganesh Iyer,
detr. InProceedingsoftheIEEE/CVFConferenceonCom- SoroushSaryazdi,NikhilKeetha,AyushTewari,etal. Con-
puterVisionandPatternRecognition,pages11124–11133, ceptfusion:Open-setmultimodal3dmapping. arXivpreprint
2023. 7 arXiv:2302.07241,2023. 2
[12] SijinChen,XinChen,ChiZhang,MingshengLi,GangYu, [24] YangJiao,ShaoxiangChen,ZequnJie,JingjingChen,Lin
HaoFei,HongyuanZhu,JiayuanFan,andTaoChen. Ll3da: Ma,andYu-GangJiang. More:Multi-orderrelationmining
Visualinteractiveinstructiontuningforomni-3dunderstand- fordensecaptioningin3dscenes. InEuropeanConference
ingreasoningandplanning. InProceedingsoftheIEEE/CVF onComputerVision,pages528–545.Springer,2022. 7
Conference on Computer Vision and Pattern Recognition, [25] Zhao Jin, Munawar Hayat, Yuwei Yang, Yulan Guo, and
pages26428–26438,2024. 2,3,6,7,9 YinjieLei. Context-awarealignmentandmutualmaskingfor
[13] ZhenyuChen,AliGholami,MatthiasNießner,andAngelX 3d-languagepre-training. InProceedingsoftheIEEE/CVF
Chang. Scan2cap: Context-awaredensecaptioninginrgb- Conference on Computer Vision and Pattern Recognition,
d scans. In Proceedings of the IEEE/CVF conference on pages10984–10994,2023. 7
computervisionandpatternrecognition,pages3193–3203, [26] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,
2021. 2,5,7 ChloeRolland,LauraGustafson,TeteXiao,SpencerWhite-head,AlexanderCBerg,Wan-YenLo,etal. Segmentany- [40] Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao
thing. InProceedingsoftheIEEE/CVFInternationalConfer- Liang, Song-Chun Zhu, and Siyuan Huang. Sqa3d: Sit-
enceonComputerVision,pages4015–4026,2023. 2 uated question answering in 3d scenes. arXiv preprint
[27] JieLei,LinjieLi,LuoweiZhou,ZheGan,TamaraLBerg, arXiv:2210.07474,2022. 6
MohitBansal,andJingjingLiu. Lessismore: Clipbertfor [41] ArjunMajumdar,AnuragAjay,XiaohanZhang,PranavPutta,
video-and-languagelearningviasparsesampling.InProceed- SriramYenamandra,MikaelHenaff,SnehaSilwal,PaulMc-
ings of the IEEE/CVF conference on computer vision and vay,OleksandrMaksymets,SergioArnaud,etal. Openeqa:
patternrecognition,pages7331–7341,2021. 5 Embodiedquestionansweringintheeraoffoundationmodels.
[28] BoLi,YuanhanZhang,DongGuo,RenruiZhang,FengLi, InProceedingsoftheIEEE/CVFConferenceonComputer
HaoZhang,KaichenZhang,YanweiLi,ZiweiLiu,andChun- VisionandPatternRecognition,pages16488–16498,2024.
yuanLi. Llava-onevision: Easyvisualtasktransfer. arXiv 2,6
preprintarXiv:2408.03326,2024. 2,5 [42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
[29] FengLi,RenruiZhang,HaoZhang,YuanhanZhang,BoLi, Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
WeiLi,ZejunMa,andChunyuanLi. Llava-next-interleave: AmandaAskell,PamelaMishkin,JackClark,etal. Learning
Tackling multi-image, video, and 3d in large multimodal transferable visual models from natural language supervi-
models. arXivpreprintarXiv:2407.07895,2024. 3 sion. InInternationalconferenceonmachinelearning,pages
[30] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. Blip- 8748–8763.PMLR,2021. 2
2: Bootstrappinglanguage-imagepre-trainingwithfrozen [43] MohitShridhar,LucasManuelli,andDieterFox. Perceiver-
imageencodersandlargelanguagemodels. arXivpreprint actor:Amulti-tasktransformerforroboticmanipulation. In
arXiv:2301.12597,2023. 2 ConferenceonRobotLearning,pages785–799.PMLR,2023.
[31] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai 2
Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. [44] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui
Videochat:Chat-centricvideounderstanding. arXivpreprint Wu,Jean-BaptisteAlayrac,JiahuiYu,RaduSoricut,Johan
arXiv:2305.06355,2023. 2 Schalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: a
[32] KunchangLi, YaliWang, YinanHe, YizhuoLi, YiWang, familyofhighlycapablemultimodalmodels. arXivpreprint
Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. arXiv:2312.11805,2023. 2,6
Mvbench:Acomprehensivemulti-modalvideounderstand- [45] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,Am-
ingbenchmark. InProceedingsoftheIEEE/CVFConference jadAlmahairi,YasmineBabaei,NikolayBashlykov,Soumya
onComputerVisionandPatternRecognition,pages22195– Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:
22206,2024. 5 Openfoundationandfine-tunedchatmodels. arXivpreprint
[33] YanweiLi,ChengyaoWang,andJiayaJia. Llama-vid: An arXiv:2307.09288,2023. 6
image is worth 2 tokens in large language models. arXiv [46] HengWang,ChaoyiZhang,JianhuiYu,andWeidongCai.
preprintarXiv:2311.17043,2023. 8 Spatiality-guided transformer for 3d dense captioning on
[34] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and pointclouds. arXivpreprintarXiv:2204.10688,2022. 7
Li Yuan. Video-llava: Learning united visual represen- [47] LinXu,YilinZhao,DaquanZhou,ZhijieLin,SeeKiongNg,
tation by alignment before projection. arXiv preprint andJiashiFeng. Pllava:Parameter-freellavaextensionfrom
arXiv:2311.10122,2023. 2 imagestovideosforvideodensecaptioning. arXivpreprint
[35] JiLin,HongxuYin,WeiPing,PavloMolchanov,Mohammad arXiv:2404.16994,2024. 2
Shoeybi, and Song Han. Vila: On pre-training for visual [48] JianingYang,XuweiyiChen,ShengyiQian,NikhilMadaan,
languagemodels. InProceedingsoftheIEEE/CVFConfer- MadhavanIyengar,DavidFFouhey,andJoyceChai. Llm-
ence on Computer Vision and Pattern Recognition, pages grounder:Open-vocabulary3dvisualgroundingwithlarge
26689–26699,2024. 2 languagemodelasanagent.arXivpreprintarXiv:2309.12311,
[36] BenlinLiu,YuhaoDong,YiqinWang,YongmingRao,Yan- 2023. 7
songTang,Wei-ChiuMa,andRanjayKrishna. Coarsecorre- [49] ZhouYu,JunYu,YuhaoCui,DachengTao,andQiTian.Deep
spondenceelicit3dspacetimeunderstandinginmultimodal modularco-attentionnetworksforvisualquestionanswering.
languagemodel. arXivpreprintarXiv:2408.00754,2024. 2 In Proceedings of the IEEE/CVF conference on computer
[37] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. visionandpatternrecognition,pages6281–6290,2019. 5
Visualinstructiontuning. arXivpreprintarXiv:2304.08485, [50] LichenZhao,DaigangCai,LuSheng,andDongXu. 3dvg-
2023. 2,5 transformer:Relationmodelingforvisualgroundingonpoint
[38] HaotianLiu, ChunyuanLi, YuhengLi, andYongJaeLee. clouds. InProceedingsoftheIEEE/CVFInternationalCon-
Improvedbaselineswithvisualinstructiontuning. InPro- ferenceonComputerVision,pages2928–2937,2021. 7
ceedingsoftheIEEE/CVFConferenceonComputerVision [51] ChenmingZhu,TaiWang,WenweiZhang,KaiChen,and
andPatternRecognition,pages26296–26306,2024. 2,3,8 XihuiLiu. Empowering3dvisualgroundingwithreasoning
[39] RuiyuanLyu,TaiWang,JingliLin,ShuaiYang,XiaohanMao, capabilities. arXivpreprintarXiv:2407.01525,2024. 2,4,7
Yilun Chen, Runsen Xu, Haifeng Huang, Chenming Zhu, [52] ZiyuZhu,XiaojianMa,YixinChen,ZhidongDeng,Siyuan
DahuaLin,etal. Mmscan:Amulti-modal3dscenedataset Huang,andQingLi. 3d-vista:Pre-trainedtransformerfor3d
with hierarchical grounded language annotations. arXiv visionandtextalignment. arXivpreprintarXiv:2308.04352,
preprintarXiv:2406.09401,2024. 2,6 2023. 5,7Figure5.LLaVA-3Dcouldperform2DClick-based3Ddensecaptioning,generatingthecorrespondingobjectcaptionand3Dboundingbox.Figure6.LLaVA-3Dcouldperform2DClick-based3Dquestionanswering,nowuserscouldclickonthe2Dimagesandaskthequestion.Figure7.LLaVA-3Dexhibitspowerful3Dvisualgroundingcapability,enablingaccurate3Dboundingboxesoutput.