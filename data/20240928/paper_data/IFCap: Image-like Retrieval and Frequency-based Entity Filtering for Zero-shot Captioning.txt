IFCap: Image-like Retrieval and Frequency-based Entity Filtering
for Zero-shot Captioning
SoeunLee∗ Si-WooKim∗ TaewhanKim Dong-JinKim†
HanyangUniversity,SouthKorea.
{soeun, boreng0817, taewhan, djdkim}@hanyang.ac.kr
Abstract etal.,2019b,2024). Toovercomethis,recentstud-
ies have explored text-only training methods for
Recentadvancementsinimagecaptioninghave
imagecaptioning,aimingtosolvetheproblemus-
explored text-only training methods to over-
ingonlytextualdata(Nukraietal.,2022;Lietal.,
comethelimitationsofpairedimage-textdata.
2023; Fei et al., 2023; Zeng et al., 2024; Wang
However,existingtext-onlytrainingmethods
often overlook the modality gap between us- etal.,2023;Liuetal.,2023;Maetal.,2023).
ing text data during training and employing Text-onlytrainingintroducesanewdirectionin
imagesduringinference. Toaddressthisissue, which models are trained solely using text data.
weproposeanovelapproachcalledImage-like Recent existing works have studied what to use
Retrieval,whichalignstextfeatureswithvisu-
as extra cues, such as extracted nouns (Fei et al.,
allyrelevantfeaturestomitigatethemodality
2023),generatedsyntheticimages(Liuetal.,2023;
gap. Our method further enhances the accu-
Ma et al., 2023) for training, and extracted tags
racyofgeneratedcaptionsbydesigningaFu-
sionModulethatintegratesretrievedcaptions fromobjectdetectors(Liuetal.,2023). However,
withinputfeatures. Additionally,weintroduce existing methods that rely on object information
aFrequency-basedEntityFilteringtechnique are sensitive to incorrect data, and utilizing large
thatsignificantlyimprovescaptionquality. We external models (e.g., stable diffusion Rombach
integrate these methods into a unified frame-
etal.,2022orobjectdetectorsCarionetal.,2020)
work,whichwerefertoasIFCap(Image-like
incurs additional costs. Thus, we aim to address
RetrievalandFrequency-basedEntityFiltering
theproblembyacquiringdiverseinformationcost-
forZero-shotCaptioning). Throughextensive
effectivelywithoutadditionalmodels.
experimentation,ourstraightforwardyetpow-
erful approach has demonstrated its efficacy, The retrieval task involves finding relevant in-
outperformingthestate-of-the-artmethodsby formationinadatabaseforagivenquery. Initially
asignificantmargininbothimagecaptioning rooted in NLP (Lewis et al., 2020), the field has
and video captioning compared to zero-shot expandedintoCVandintomulti-modalretrieval.
captioningbasedontext-onlytraining.1
Dependingontheinputdataanddatabase,various
retrieval methods are possible, such as image-to-
1 Introduction
text(Ramosetal.,2023)andtext-to-textretrieval
Thetaskofimagecaptioninggeneratesappropriate (Wangetal.,2023). Intheexistingtext-onlytrain-
textualdescriptionsforimagesbycombiningcom- ingstudy,therehavebeenattemptstousethetext-
putervision(CV)andnaturallanguageprocessing to-textretrievalmethod. However,existingworks
(NLP). With the emergence of Large Language can’taddressthemodalitygapinherentintext-only
Models (LLMs) and Vision and Language Mod- trainingsettings,wheretrainingisperformedwith
els (VLMs), various works have studied efficient textandinferencewithimages. Inaddition,such
training methods for image captioning (Mokady worksrelytoomuchonretrievedcaptionswithout
etal.,2021;Luoetal.,2023;Ramosetal.,2023). consideringvisualinformation. Thismodalitygap
Theseapproachesdevelopeffectivecaptioningby
andtheuseofanarrowscopeofinformationmay
using pre-trained models with few parameters or leadtoperformancedegradation.
lightweightnetworks. However,theseworksrely Toverifythis,wevisualizetheanalysisresultof
on paired image-text data, which is costly (Kim theCLIPembeddingfeatureofretrievedcaptions
thatthemodelusesintrainingviat-SNEinFig.2.
∗Equalcontribution.†Correspondingauthor.
1Code:https://github.com/boreng0817/IFCap The analysis is done on the COCO (Chen et al.,
4202
peS
62
]VC.sc[
1v64081.9042:viXraTrainingtimeretrieval
Text-to-text Retrieval Inferencetimeretrieval Image-like Retrieval (Ours)
Retrieved sentences in training
Embedding Space Retrieved sentences in inference Embedding Space
Modality gap
Text Image Text Image
CLIP Classifier Entity Retrieval Frequency-based Entity Filtering (Ours)
(Softmax) Vocabulary "A ma1nis taking a ride on his motor1cyclenear the country side."
"A ma2nsitting on a motor2cyclenear the edge of a moun1tain."
man "A person riding a motor3cycleon a narrow road."
backpack ⋮
X
motorcycle (Frequency) : 2 : 3 : 1
Threshold ≥0.3, (wrong) Threshold ≥2,
→Hard Prompt:“There are man, backpackin image.” →Hard Prompt: “There are motorcycle, manin image.”
Figure 1: (Top) The previous text-to-text retrieval approach overlooks the modality gap, leading to different
informationusebetweentrainingandinference. Ourapproachaddressesthisbyaligningtextfeatureswiththe
imageembeddingspaceduringretrieval. (Bottom)ThetraditionalCLIPclassifier-basedentityretrievalmethod
struggles with entity detection as vocabulary size grows. Our approach detects frequently occurring words in
retrievedcaptions,extractingentitiesmoreaccuratelywithoutrelyingonalimitedvocabulary.
2015) validation split, and the CLIP similarity- the original input and additional representations.
basedKNNalgorithmisusedforretrieval. Inthe Additionally, as shown by numerous studies (Fei
figure,thereisalargedifferencebetweenthedistri- etal.,2023;Ramosetal.,2023),promptscanclar-
butionoffeaturesusedafterimage-to-textretrieval ifytheinformationprovidedtothelanguagemodel.
andtext-to-textretrieval,whichshowsthatamodal- Weextractkeywordsfromtheinputcaptiontocon-
itygapexistsbetweenimageandtext. struct a hard prompt, which is fed to the LLM,
offering explicit guidance. This approach maxi-
Totacklethisissue,weproposeanovelapproach
mizestheutilityoftextdata,guidingthemodelto
called “Image-like Retrieval,” that addresses the
generateaccurateandrelevantcaptions.
modality gap between image and text data. We
inject a noise into the CLIP text feature to act as
Guidingcaptiondecoderwithextractedentities
a query in image feature distribution. Visualiza-
fromanimagehelpsthemodelgenerateanaccu-
tion results for this approach are shown in Fig. 2
rate description of the image. However, we find
right,demonstratingthatourmethodexhibitsadis-
thatthepreviousworks (Feietal.,2023;Liuetal.,
tributionhighlysimilartothatofimage-to-retrieval
2023) show low entity detection precision, espe-
resultsandgroundtruthcaptions,unliketraditional
cially when the vocabulary is large as shown in
text-to-text retrieval methods. Indeed, when our
Fig.3. Therefore,weproposeaFrequency-based
methodisappliedtotheexistingresearch(Wang
Entity Filtering technique precisely utilizing en-
et al., 2023), performance improvements are ob-
tityinformationwithoutrelyingonthevocabulary.
served,asshowninTable 12.
During inference, we utilize retrieved sentences
Priorresearch(Wangetal.,2023)reliessolely from images, parsing them into nouns and calcu-
onretrievedcaptions,whichmayincludewrongin- latingtheirfrequency. Then,wefilternounswith
formationintheinputcaption,potentiallyleading pre-definedthresholdsandcuratehardpromptsfor
to inaccurate outputs. To address this, we design the text decoder. This simple method yields re-
a Fusion Module that effectively integrates both markableperformanceimprovements.Entity Filtering (Ours)
ViECap
(39.5%) DETR
231 / 585
(85.1%)
1,374 / 1,614
(15.5%)
354 / 2,281
(22.1%)
227 / 1,029
(78.4%)
1,277 / 1,628
(40.0%)
5,464 / 13,659
(68.8%)
3,052 / 4,438
(86.1%)
Figure2: ThedistributionofCLIPembeddingfeatures 8,678 / 10,083
correspondingtoimages■,pairedcaptions(cid:8),retrieved
0 20% 40% 60% 80%
captions(cid:8)foraspecificimage,andtheresultoftext-to-
Figure3: PrecisionofextractedentitiesintheCOCO
textretrieval(cid:8)andourImage-likeRetrieval(cid:8).
test set, total 5,000 images. If an extracted entity ex-
ists in the ground-truth caption, it counts as correct
Insummary,ourcontributionsareasfollows: or else wrong. Three methods (Ours, ViECap2023,
• We propose a novel approach, Image-like Re- DETR2020)arecomparedwiththreedifferentsettings.
trieval,whichachieveseffectssimilartoimage- Ourmethodisillustratedin3.3,andViECapusesCLIP
based classifier with the source domain’s vocabulary
to-textretrievalintext-onlytraining. Then,we
list. WefollowthewaySynTIC(Liuetal.,2023)uses
introduce a Fusion Module for interaction be-
DETRandemploytheCOCOvocabularylist. Dueto
tweenexistingandadditionalrepresentations.
the inaccessible vocabulary list of Flickr30k, DETR
• Weproposeanentityfilteringtechniqueininfer-
can’tbecompared,andViECapusestheVGOI(Zhang
ence,Frequency-basedEntityFiltering,enhanc- etal.,2021)vocabularylistinFlickr30k. Ourmethod
ing the language model by filtering frequently dominatestheprecisionscoreandquantityofentitiesin
appearingentitiesinretrievedcaptions. everysetting.
• Extensive evaluations show IFCap achieves
state-of-the-art performance in various bench- ever,ithasbeenshownthattheseembeddingsare
marks,includingvideocaptioning. locatedintwoseparateregions,withasignificant
gap between the modalities (Liang et al., 2022).
2 Relatedwork
Thismodalitygaphinderstheinteractionbetween
2.1 Text-onlyCaptioning visionandtextmodalitiesandlimitsthequalityof
generatedcaptions. Amongthenotableapproaches
TheadvantageofCLIP(Radfordetal.,2021)has
addressingthisissue,CapDec(Nukraietal.,2022)
been utilized in a variety of tasks, such as image
assumes that the image embeddings paired with
captioning,imagegeneration,andobjectdetection.
textembeddingsarelocatedwithinasmallradius
Intherealmofimagecaptioning,text-onlytraining
aroundthetextembeddingsandmitigatesthegap
research is emerging that uses only text data for
withnoiseinjection. CLOSE(Guetal.,2022)high-
learningwithoutimagedata,takingadvantageof
lightsthelowcosinesimilaritybetweenimagesand
theCLIPcharacteristicthatimageembeddingsand
theirpairedtextsandusesahyper-parameter-scaled
textembeddingsarelearnedtobeclose. DeCap(Li
noiseinjectiontechniquetobridgethegap.
etal.,2023)trainsatextdecoderusingonlytextual
Wefocusonthemodalitygapforretrievalfrom
dataandintroducesasupportmemorymechanism
a new perspective. Our goal is to perform text
to project input images into the text embedding
retrievalsimilartoimage-to-textretrieval,consider-
spaceduringinference,facilitatingthegeneration
ingthemodalitygap. Thedistinctionfromexisting
ofcaptions. ViECap(Feietal.,2023)recognizes
methodscanbeobservedinFig.2left.
themainentityoftextdatathatcomesasinputand
configuresitasaprompt,allowingLLMtoperform
2.3 RetrievalAugmentedGeneration
object-agnosticlearningbasedonopenvocabulary
retrievalusingCLIP. RetrievalhasbeenusedindiversewaysinNLP.Im-
agecaptioningalsobenefitsfromretrievalmodules
2.2 ModalityGap
by incorporating novel objects and new informa-
VisionlanguagemodelssuchasCLIPaimtoembed tionintocaptions,allowingaccesstonewdomains
images and text closely in a shared space. How- without additional training. Retrieval is applied
k03rkcilF
k03rkcilF
>-
OCOC
OCOCImage-like Top-𝒌 Text Fusion Module
e
Retrieval sentences Encoder
Fusion
𝑻𝒆 𝑹𝒆
m 𝒇 𝒇
iT Module 𝒍𝟏 𝒍𝟐
g “A man is in a kitchen Text 𝑸 𝑲,𝑽
n in making pizzas.” Encoder 𝒇
ia Object parsing 𝝐~𝑵 𝟎,𝝈𝟐 𝑨𝒕𝒕
r
T
man There are man,
𝜽𝒒
kitchen kitchen, pizza
pizza in the image.
Mapping Network
GPT-2
Image-to-text Top-𝒌 Text
Retrieval sentences Encoder
e m Fusion
iT Module
e c Image Frozen parameter
n Encoder
e r Learnable parameter
e
fn
I Entity There are dog, Concatenation
sandin the
Filtering image. Noise injection
Figure 4: Theoverview of IFCap. During training, we extractnouns from theinput text and retrieve k similar
sentencesusingourImage-likeRetrievalmethod. Extractednounsareincorporatedintoaprompttemplatetoform
ahardprompt. Boththeinputtextandretrievedsentencesareencodedusingthetextencoder. Theseembeddings
interactandcombinethroughourFusionModulebeforebeingfedintotheLLMforsentencegeneration. During
inference,weretrievelsentencessimilartotheinputimageandconstructahardpromptbyextractingentitiesvia
Frequency-basedEntityFilteringfromtheretrievedsentences. Thesentencesareencodedusingatextencoder,and
theinputimageisencodedusinganimageencoder,followedbyinputintotheFusionModule. Thesubsequent
processfollowsaproceduresimilartothetrainingphase.
invariouswaysinimagecaptioningmodels. For Our IFCap utilizes a simple yet powerful re-
instance,Smallcap(Ramosetal.,2023)retrieves trievalmechanismandaddressesthemodalitygap
captionsrelevanttotheinputimageandusesthem betweenimageandtextwithImage-likeRetrieval
asinstructionsforthetextdecoder. Intext-onlyim- (Section 3.1). After performing Image-like Re-
agecaptioning,ViECap(Feietal.,2023)retrieves trieval,weemployaFusionModule(Section3.2)
novelobjectsfromtheinputimageandusesthem tomergeinputembeddingswiththeretrievedfea-
asprompts,whileKnight(Wangetal.,2023)uses tures. Duringinference,weusetheretrievedcap-
retrievedcaptionsastextfeatures. tionsfromtheimagetofindaccurateanddetailed
Most retrieval methods are based on image-to- entitieswithFrequency-basedEntityFiltering(Sec-
text retrieval, but text-only captioning performs tion3.3).
text-to-text retrieval. However, during inference,
themodalitygapcausedbytheinputimageleadsto 3.1 Image-likeRetrieval(ILR)
poorperformance. Ourmethodcarefullyaddresses
Whiletext-to-textretrievalcanbeeffectivelyper-
thisissuetoimproveperformancebyconsidering
formed during training, it is likely to suffer from
thegapbetweenimageandtext.
performance degradation during inference when
animageisprovidedasinputduetothemodality
3 Methods
gap. Therefore, Image-like Retrieval (ILR) aims
We propose a new text-only image captioning to perform text-to-text retrieval in a manner that
model,IFCap,whichisillustratedin Fig.4. Dur- resemblesimage-to-textretrievaloutcomes,given
ing training, the model only utilizes text data, as text input. For this, we propose an approach that
isstandardfortext-onlytrainingmodels. First,we insertsnoiseintothefeaturespaceoftheinputtext,
embedtheinputtextusingatextencoder. Thetext bringingitclosertotheimagefeaturespace. The
embeddingsarethenfedintoamappingnetworkto augmentationprocessisasfollows:
closethegapbetweendifferentmodalities. Finally, First,weutilizetheCLIPtoembedtheinputtext
the processed embeddings go through a caption t andthetextcorpusT = {t}Nc withatextencoder
i i i=1
decodertogeneratetheoutputcaption. E . Then, we introduce noise ϵ ∼ N(0,σ2) into
T r rtheembeddingofinputtextT ,aimingtoadjustthe The noun implies intuitive and explicit informa-
i
textfeaturestoalignmorecloselywiththeimage tionaboutobjectsintheimage. Foremployingthe
featurespace: propertyofnouns,weextractentitiesineachtrain-
ingtextcorpusandinputimages. Webuildahard
T = E (t), Tϵ = T +ϵ . (1) prompthwithextractedentitiesE = {e ,e ,...,e }
i T i i i r 1 2 n
tomakethemodelawareofexistingentitiesinthe
Next,theretrievalstepisperformedusingthenoise- image. Withretrievedcaptionsandhardprompts
injectedinputtextTϵ. Toidentifythedescriptions with entities, the model can learn the ability to
i
mostrelevanttoTϵ, thetop-k descriptionsarere- generatepropercaptionswithoutimages. Weuse
i
trievedbycalculatingthecosinesimilaritybetween auto-regressivelosstooptimizeourprojectorand
Tϵ andallsentenceembeddingsinthetextcorpus. captiondecoder. (DetailsabouttheFusionModule
i
Thisprocesscloselyfollowspreviousmethodsin arein Sec.4.1).
image-to-textretrieval(Ramosetal.,2023),with
thedistinctionthatweperformretrievalbasedon 1
(cid:88)N
L = − log(y|F;h;y ;θ). (5)
Tϵ insteadofimages. θ N i <i
i i=1
By utilizing this approach during training, we
3.3 Frequency-basedEntityFiltering(EF)
canenhancetheabilityofamodeltoprovideimage-
likeinformationeveninatext-onlytrainingsetting, After retrieving l captions from an image, we
therebynarrowingthemodalitygapandimproving use grammar parser tools (e.g., NLTK Bird and
performance. Loper, 2004) to extract nouns from the retrieved
sentencesandcalculatethefrequencyoftheseex-
3.2 FusionModule(FM) tractednounsas F = [f , f ,..., f ]. Wethenselect
1 2 n
Intext-onlyimagecaptioning,choosingwhichad- nouns that have a frequency larger than a prede-
ditional information to inject into the model and finedthresholdandplacethemintoahardprompt.
dealingwithnewrepresentationswithgivendata Heuristicthreshold: Since frequency is dis-
appropriatelyareimportantissues. Tohandlethis crete,wecanmanuallyfindthebestthresholdby
problem,weusetheattentionmechanism(Vaswani conductingexperimentswitheverypossiblethresh-
etal.,2017)tofuseinputtextfeaturesandretrieved old. Thisallowsustodeterminetheglobaloptimal
captionsfeaturestoextracttheirmeaningfulinter- threshold.
action. Theattentionmechanismemphasizescer- Adaptivethreshold: We can use a heuristic
tainimportantfeatures,andduetoitseffectiveness, threshold,butthesethresholdsareoftenunsuitable
ithasbeenwidelyutilizedinthefieldofcaptioning
fordifferentenvironments,andperformingexten-
(Xuetal.,2015). siveexperimentsincursunnecessarycosts. Instead,
wecanestimatethecommondistributionofnoun
Wefirstencodeinputtextandretrievedcaptions
frequenciesascertainprobabilitydistributions. We
using CLIP (Radford et al., 2021) text encoder,
theninjectaGaussiannoiseϵ ∼ N(0,σ2)toinput canassumefrequenciesfollowN(µ F,σ2 F).
textfeatureforrelievingthemodalitygapbetween
τ = µ +σ . (6)
adap F F
imageandtext. Thenweadjustthedimensionof
the input text feature and retrieved captions fea- Any nouns with a frequency larger than τ ,
adap
ture to the embedding space of caption decoder whichplacesthemintheupper15%, canbecon-
withlinearlayers f l1 and f l2 respectively,andapply sideredoutliers. Usingthisadaptivethreshold,we
cross-attention f Att withT e asqueryandR e askey, canimplementaflexiblethresholdthatfitsvarious
thencreatefusionrepresentationF e containingin- settings. However, it does not guarantee global
put text and retrieved captions. Finally, F
e
is fed optima,leadingtoatrade-offrelationshipbetween
intoatrainableMappingNetwork,whichencodes heuristicthresholdsandadaptivethresholds.
the overall contents of the given input. We can
summarizethisprocesswithequations. 4 Experiments
4.1 ImplementationDetails
T = T +ϵ, R = E (ILR(T )), (2)
e i e T i
While verifying the state-of-the-art performance
F = f (f (T ), f (R )), (3)
e Att l1 e l2 e ofourmodel,weuseCLIP(ViT-B/32)astheim-
F = Map(F ;θ ). (4)
e q age encoder and GPT2 (Radford et al., 2019)
baseImage Text COCO Flickr30k
Method
Encoder Decoder B@4 M C S B@4 M C S
CapDec(2022) RN50x4 GPT-2 26.4 25.1 91.8 11.9 17.7 20.0 39.1 9.9
Large
DeCap(2023) ViT-B/32 Transformer 24.7 25.0 91.2 18.7 21.2 21.8 56.7 15.2
Base
CLOSE(2022) ViT-L/14 T5 - - 95.3 - - - - -
base
ViECap(2023) ViT-B/32 GPT-2 27.2 24.8 92.9 18.2 21.4 20.1 47.9 13.6
Base
MeaCap (2024) ViT-B/32 GPT-2 27.2 25.3 95.4 19.0 22.3 22.3 59.4 15.6
InvLM Base
Knight(2023) RN50x64 GPT-2 27.8 26.4 98.9 19.6 22.6 24.0 56.3 16.3
Large
ICSD♠ (2023) ViT-B/32 BERT 29.9 25.4 96.6 - 25.2 20.6 54.3 -
Base
SynTIC♠† (2023) ViT-B/32 TransformerL=4 29.9 25.8 101.1 19.3 22.3 22.4 56.6 16.6
H=4
IFCap ViT-B/32 GPT-2 30.8 26.7 108.0 20.3 23.5 23.0 64.4 17.0
Base
Table1: ResultontheIn-domaincaptioningincludingCOCOtestsplitandFlickr30ktestsplit. Everyresultis
copiedfromtheoriginalpapers. ♠: Utilizestext-to-imagegenerationmodelinthetrainingtime,†: Utilizesobject
detectorduringthetrainingandinferencetime. IFCapachievesstate-of-the-artinmostmetrics. Thebestnumber
overallisinboldandthesecondbestinunderline.
COCO=⇒Flickr Flickr=⇒COCO COCO=⇒NoCapsVal
Method
B@4 M C S B@4 M C S Method In Near Out Entire
DeCap(2023) 16.3 17.9 35.7 11.1 12.1 18.0 44.4 10.9 C S C S C S C S
ViECap(2023) 17.4 18.0 38.4 11.2 12.6 19.3 54.2 12.5
DeCap(2023) 65.2 - 47.8 - 25.8 - 45.9 -
Knight(2023) 21.1 22.0 48.9 14.2 19.0 22.8 64.4 15.1
CapDec(2022) 60.1 10.2 50.2 9.3 28.7 6.0 45.9 8.3
SynTIC(2023) 17.9 18.6 38.4 11.9 14.6 19.4 47.0 11.9
ViECap(2023) 61.1 10.4 64.3 9.9 65.0 8.6 66.2 9.5
SynTIC-TT 19.4 20.2 43.2 13.9 20.6 21.3 64.4 14.3
IFCap⋆ 17.8 19.4 47.5 12.7 14.7 20.4 60.7 13.6 IFCap⋆ 70.1 11.2 72.5 10.9 72.1 9.6 74.0 10.5
IFCap-TT 21.2 21.8 59.2 15.6 19.0 23.0 76.3 17.3
Table 3: Results on the NoCaps validation split. ⋆:
Table2: ResultsontheCross-domaincaptioning. −TT: withoutEntityFilteringmoduleintheinferencetime.
models can access to target domain’s corpus during IFCapachievesstateoftheartineverymetric.
inferencetime. ⋆: withoutEntityFilteringmodulein
the inferencetime. IFCap achieves state-of-the-artin
mostmetrics.
theMappingNetwork,whichconsistsof8layered
transformers(Vaswanietal.,2017).
as the text decoder. Parameters in the image en- Frequency-basedEntityFiltering: Fromthein-
coder are frozen during training, and the text de- put image, we retrieve l sentences and extracted
coder and Fusion Module are trained. We train a nounstoobtainfrequencyF. Withthepredefined
total of 5 epochs, learning rate of 2 × 10−5, use threshold,wefilterentitiesandbuildhardprompt
schedulerforlearningratescheduler,AdamWopti- h,providingmoreaccurateanddiverseentitiesto
mizer(KingmaandBa,2014), andsetbatchsize thecaptiondecoder.
80. WeuseasingleNVIDIARTX4090with24GB Datasets, metrics We evaluate our model in
VRAM;ittakesaboutanhouranduses12GBof human-annotateddatasets. Forin-domaingeneral-
VRAMduringtraining. ization, we test our model on MS-COCO (Chen
Image-like Retrieval: We first discover ade- et al., 2015), Flickr30k (Young et al., 2014),
quate σ r for Image-like Retrieval. Based on our and utilize Karpathy split (Karpathy and Fei-
experiment(Fig.5),wechooseσ r as0.04inmost Fei, 2015). Also, to check the model’s perfor-
cases. Weretrieveksentenceswithnoise-injected mance in the unseen scenarios, we use the No-
inputtextfeatureT e. Caps (Agrawal et al., 2019) validation set. For
Fusion Module: We project T ∈ Rd and metrics,weusecommonimagecaptioningmetrics
e
R
e
∈ Rd×k with f l1, f
l2
into Rdgpt, Rdgpt×k respec- CIDEr(Vedantametal.,2015),SPICE(Anderson
tively where d is the CLIP dimension and d is etal.,2016),BLEU@n(Papinenietal.,2002),and
gpt
thedimensionofGPT-2embeddingspace. Weuse METEOR (Banerjee and Lavie, 2005). More de-
projected T as query and R as key in f layer. tailsaboutdatasetsandmetricsareincludedinthe
e e Att
Finally, F and θ are concatenated and fed into appendix(Sec.A).
e qMSR-VTT MSVD DesignChoice COCO
Method Pre-ϵ Post-ϵ Retrieval
B@4 M C S B@4 M C S Reference B@4 M C S
ZeroCap(2022b) 2.3 12.9 5.8 - 2.9 16.3 9.6 - ViECap 27.2 24.8 92.9 18.2
MAGIC(2022) 5.5 13.3 7.4 4.2 6.6 16.1 14.0 2.9 Smallcap ✓ 23.5 24.2 88.5 18.2
Knight ✓ ✓ 26.0 24.6 92.9 18.3
CLMs(2022) 6.2 17.8 10.1 6.5 7.0 16.4 20.0 3.1
Knight+ILR ✓ ✓ ✓ 27.2 25.0 93.9 18.3
CapDec(2022) 8.9 23.7 11.5 5.9 7.9 23.3 34.5 3.2
IFCap ✓ ✓ 28.5 26.0 102.0 20.0
EPT(2022a) 3.0 14.6 11.3 - 3.0 17.8 17.4 -
Knight(2023) 25.4 28.0 31.9 8.5 37.7 36.1 63.8 5.0
Table6: ImportanceofnoiseinjectiontimingofImage-
IFCap 27.1 25.9 38.9 6.7 40.6 34.2 83.9 6.3
like Retrieval. Pre-ϵ refers to noise injection before
retrieval,andPost-ϵreferstonoiseinjectiontoretrieved
Table 4: Results on the Video captioning including
features.
MSR-VTTandMSVD.IFCapachievesstate-of-the-art
inmostmetrics.
kretrieved COCO
sentences B@4 M C S
Image-like Fusion Entity COCO
Retrieval Module Filtering B@4 M C S 3 28.1 25.7 100.0 19.5
✓ ✓ ✓ 30.8 26.7 108.0 20.3 5 28.5 26.0 102.0 20.0
✓ ✓ 29.2 26.0 104.0 19.9 7 28.2 26.0 101.7 19.8
✓ ✓ 28.5 26.0 102.0 20.0
✓ 27.2 24.7 97.3 18.5 Table 7: Ablation studies of the number of retrieved
✓ 27.7 25.6 99.0 19.4 captionskforFusionModule.
27.2 24.8 92.9 18.2
ingtime(Maetal.,2023;Liuetal.,2023). Also,
Table 5: Ablation studies of the key components of
IFCap. inFlickr30k,IFCapshowsdecentperformancein
BLEU@4 and METEOR and achieves the best
scoresinCIDErandSPICE.
4.2 Text-onlyCaptioning
4.4 Cross-domainCaptioning
We compare our model with other state-
of-the-art text-only image captioning models. WevalidateIFCap’stransferabilitythroughdiverse
CapDec(Nukraietal.,2022)andViECap(Feietal., domains,includingtheNoCapsvalidationsetand
2023)arebasedonClipcap(Mokadyetal.,2021). cross-domainfromCOCO→Flickr30kandvice
They use predefined Gaussian noise for aligning versa. InNoCaps,weusethesamemodeltrained
text and image features. Similarly, CLOSE (Gu intheCOCOdomaintotesthowthemodelrecog-
et al., 2022) uses various noise settings, and De- nizesunseenobjectsduringtraining. IntheNoCaps
Cap (Li et al., 2023) uses a memory bank. And validationsplit,ourIFCapperformsthebestinev-
a recent approach to text-only image captioning, erymetricandeverydomaincomparedtoprevious
Knight (Wang et al., 2023) only utilizes text state-of-the-arttext-onlyimagecaptioningmodels
features with a retrieval mechanism, also Mea- (Lietal.,2023;Nukraietal.,2022;Feietal.,2023).
Cap (Zeng et al., 2024) processes retrieved sen- Also,incross-domainsettingsbetweenCOCOand
tences into Subject-Predicate-Object triplets and Flickr30k,IFCapwinsstate-of-the-artinmostmet-
employsthemasadditionalinformation. ICSD(Ma ricsandthesecondbestinsomemetrics.
etal.,2023)andSynTIC(Liuetal.,2023)utilize
text-to-imagegenerationmodelslikeStableDiffu- 4.5 VideoCaptioning
sion(Rombachetal.,2022)toclosethegap.
Invideocaptioning,wetrainourmodelinthesame
manneraspreviousexperiments. First,weperform
4.3 In-domainCaptioning
Image-likeRetrievalonthecorpusfromeachvideo
We benchmark our IFCap on in-domain settings captioning dataset MSVD (Wu et al., 2017) and
in Table 1 including COCO and Flickr30k. We MSR-VTT (Xu et al., 2016). For inference time,
compareourmethodswithpreviousstate-of-the-art we sample 5 images from input video and calcu-
in text-only image captioning. Our IFCap domi- latetheaverageoftheirCLIPimagefeatures. We
nateseverymetricintheCOCOdatasetcompared also retrieve 5 sentences from each sampled im-
to models that utilize larger models (Gu et al., age, 25intotal, andalsocalculatetheaverageof
2022;Wangetal.,2023)andhavecomplextrain- CLIP text features per image. Most of the met-Transformer Cross-Attention COCO COCO Flickr30k
τ
#Layers #Layers B@4 M C S B@4 M C S B@4 M C S
1 23.9 24.6 86.9 17.8 1 6.5 18.7 6.4 17.0 6.8 18.9 3.9 15.4
1
4 26.2 24.4 92.8 18.0 2 21.4 26.5 80.3 21.0 18.9 23.4 52.2 17.9
1 27.4 24.9 95.0 18.5 3 28.1 26.8 103.6 21.1 23.5 23.0 64.4 17.0
2
4 26.4 24.9 95.5 18.4 4 30.2 26.7 107.7 20.7 23.8 22.3 61.1 15.9
5 30.8 26.7 108.0 20.3 23.8 21.9 59.1 15.3
1 27.4 25.5 99.7 19.1
4 6 30.4 26.4 106.2 19.9 23.6 21.7 57.3 15.0
4 27.9 25.8 99.1 19.4
7 30.0 26.1 104.6 19.6 23.6 21.6 56.5 14.8
1 28.3 26.0 102.0 20.0
8 8 29.8 26.0 103.4 19.4 23.7 21.6 55.9 14.7
4 28.4 25.7 100.6 19.5
Table10: Ablationstudiesofheuristicthresholdτof
Table8: Ablationstudiesofthenumberoftransformer
EntityFiltering.
layersandcross-attentionlayersoftheFusionModule.
COCO Flickr30k
lretrieved COCO Flickr τ adap B@4 M C S B@4 M C S
sentences B@4 M C S B@4 M C S
Lognormal(µ,σ2)
5 29.9 26.4 106.1 20.2 23.5 22.2 61.9 16.0
µ 22.0 26.6 83.8 21.1 19.0 23.4 52.7 17.9
7 30.3 26.5 107.2 20.3 23.5 23.0 64.4 17.0 µ+σ 29.1 26.7 106.6 20.7 22.0 22.9 63.0 17.2
9 30.8 26.7 108.0 20.3 23.4 22.6 62.9 16.6
µ+2σ 29.6 26.1 103.5 19.6 23.3 21.8 58.1 15.3
Table 9: Ablation studies of the number of retrieved
N(µ,σ2)
µ 24.9 26.7 95.9 21.1 19.2 23.2 55.6 17.7
sentenceslforEntityFiltering.
µ+σ 30.1 26.6 107.5 20.4 22.3 22.5 62.3 16.4
µ+2σ 29.8 26.2 104.7 19.7 23.4 21.9 58.5 15.5
ricsinbothdatasets,IFCap,fulfillsstate-of-the-art Best(H) 30.8 26.7 108.0 20.3 23.5 23.0 64.4 17.0
performance,exceptMETEOR.
Table11: Ablationstudiesofadaptivethresholdτ of
adap
4.6 AblationStudy EntityFiltering.
We conduct extensive experiments to identify
the impact of each key component in IFCap, videocaptioning,refertoTable14.
Image-likeRetrieval(ILR),FusionModule(FM), Image-like Retrieval: It is crucial to identify
and Frequency-based Entity Filtering(EF). Also, adequate timing for injecting noise into text fea-
for each component, we search the best hyper- tures for successful text-to-text retrieval that imi-
parameterintheCOCOtestsplitwithanin-domain tatesimage-to-textretrieval. Wecansplitinjecting
setting. timingintoPre-ϵ andPost-ϵ. Wefindthatourset-
Key Components: We check the strength of tingwhichinjectsnoisebeforeperformingretrieval
eachcomponentbydetachingfromourbestmodel, is the best among all possible combinations. We
whichconsistsofall3componentsTable5. First, canverifythisinTable6. Thefirstcolumnofthe
removingFM,wesimplyconcatenatetheinputtext table indicates how the model performs retrieval,
featureandretrievedfeaturesafterapplyingdimen- just for easy understanding of noise injection in
sionmappinglayers f and f andpassingittothe retrieval.
l1 l2
captiondecoder. RemovingEFissimplyapplying Fusion Module: We utilize a cross-attention
entityextractionviaCLIPclassifierlike(Feietal., layer and transformer layer for mapping the net-
2023) does. Demounting ILR makes inaccessi- work. In Table 8, we try multiple combinations
bletoretrievalfeaturessolelyusinginputfeatures; ofthenumberofeachlayer. Themorelayerswe
henceFMcan’texistwithoutILR.Addingmore use, the more performance gain we can get until
components into the baseline, we can explicitly the number of transformer layers is 4. The per-
notice performance improvement. So, using all formance gain is also observed when we use 8
threekeycomponentsconstitutesastate-of-the-art transformer layers but it is so slight. Increasing
model,whichisIFCap. NotethatIFCaphas2vari- the number of cross-attention layers is effective
ants,IFCapandIFCap⋆,withEFandwithoutEF when the transformer layer is small, but the ten-
respectively. To see a full comparison of various dency does not last while the transformer layer
datasets,includingin-domain,cross-domain,and grows. Weconcludeusing8transformerlayersand5 Conclusion
B@4
1.05
M
1.00 C In this paper, we propose a zero-shot caption-
S ing method, IFCap, through text-only training.
0.95
IFCap performs Image-like Retrieval to address
0.90
the gap between image-to-text retrieval and text-
0.85
to-textretrieval,FusionModuleforinteractionbe-
tweenexistingandadditionalrepresentations,and
0.30
Frequency-basedEntityFilteringduringinference
0.25 timetoextractfrequentlyoccurringentitiesfrom
0.20 the retrieved sentences. Our method can be eas-
0.15 ilyappliedtovarioustasksandprovidesvaluable
0.10 guidanceforretrieval-basedmethodsinatext-only
0 0.00010.001 0.005 0.01 0.016 0.02 0.03 0.04 0.05 0.1 0.5 1
setting. It offers clear and precise information
Figure5: Hyper-parametersearchforfindingbestσ
r
usedinImage-likeRetrieval. Allexperimentsarecon- toLLMswithoutrelyingonalimitedvocabulary.
ducted with the COCO test set. The X-axis denotes ThesimplicityandrobustnessofIFCaparedemon-
σ2, and the Y-axis denotes scores of commonly used stratedthroughstate-of-the-artperformanceacross
r
captioningmetricsBLEU@4(B@4), METEOR(M), various datasets in image and video captioning.
CIDEr(C),andSPICE(S).
The future direction of our method includes the
extensionofourmethodonmorecomplexdatasets,
suchasregion-basedcaptioning(Kimetal.,2019a,
asinglecross-attentionlayershowsthebestperfor-
2021) or visual question answering (Cho et al.,
mance. For a fair comparison, we detach the EF
2023a,b),whichsufferfromdataissues.
module. Also,thenumberofretrievedcaptionsis
crucial. Weconductablationstudiestofindoptimal
6 Limitations
k,whichcanbefoundinTable7.
Frequency-basedEntityFiltering: Weneedto WedemonstratethatIFCapexhibitssuperiorperfor-
choose1)howmanyretrievedsentencesl,touse manceacrossvariousimagecaptioningandvideo
and 2) the threshold τ, for filtering nouns for EF captioning datasets compared to other zero-shot
toextractaccurateanddiverseentities. Theformer image captioning models with text-only training.
canbefoundinTable9, notethatindifferentdo- However, the optimal value of ϵ for Image-like
r
mains,optimallmayvary. FortheCOCOdomain, Retrieval currently requires a heuristic approach
usinglas9showsthebestperformance,while7is todetermine. Weleavethetaskoffindingamore
thebestinFlickr30k. convenientmethodfordeterminingtheoptimalϵ
r
Wefindthebestthresholdsettinginaheuristic asfutureworktofurtherimproveimagecaptioning
andadaptiveway. IntheformercaseTable10,we modelswithtext-onlytraining.
set τ ranging from 1 to 8, which is the minimum
and maximum value of the given setting. Above Acknowledgements
8, performance freeze due to none of the entities
ThiswaspartlysupportedbytheInstituteofInfor-
being retrieved. In the COCO test, we use l = 9
mation&CommunicationsTechnologyPlanning&
andl = 7intheFlickr30ktestsplit. Wenoticethat
Evaluation(IITP)grantfundedbytheKoreangov-
each domain has different optimal τ, COCO at 5
ernment(MSIT)(No.RS-2020-II201373,Artificial
andFlickr30kat3fortheCIDErscore. Incontrast
Intelligence Graduate School Program(Hanyang
totheheuristicway,wecanassumesuchdistribu-
University))andtheNationalResearchFoundation
tion exists from frequencies F. We try Gaussian
ofKorea(NRF)grantfundedbytheKoreagovern-
distribution and Log-normal distribution with µ,
ment(MSIT)(No. RS-2023-00245661).
µ+σ, and µ+2σ, capturing upper 50%, 15.8%,
and2.2%basedonthefrequencyofentity. InTa-
ble11,weobserveτ adap = µ+σalmostreproduces References
theperformanceofglobaloptimalintheheuristic
HarshAgrawal,KaranDesai,YufeiWang,XinleiChen,
threshold. If ground truth does not exist or com-
Rishabh Jain, Mark Johnson, Dhruv Batra, Devi
puting resource is limited, the adaptive threshold
Parikh,StefanLee,andPeterAnderson.2019. No-
becomesattractive. caps: Novelobjectcaptioningatscale. InProceed-ings of the IEEE/CVF international conference on Dong-JinKim,JinsooChoi,Tae-HyunOh,andInSo
computervision,pages8948–8957. Kweon.2019a. Denserelationalcaptioning: Triple-
stream networks for relationship-based captioning.
PeterAnderson,BasuraFernando,MarkJohnson,and InProceedingsoftheIEEEConferenceonComputer
Stephen Gould. 2016. Spice: Semantic proposi- VisionandPatternRecognition,pages6271–6280.
tionalimagecaptionevaluation. InComputerVision–
ECCV2016: 14thEuropeanConference,Amsterdam, Dong-JinKim,JinsooChoi,Tae-HyunOh,andInSo
TheNetherlands,October11-14,2016,Proceedings, Kweon.2019b. Imagecaptioningwithveryscarce
PartV14,pages382–398.Springer. superviseddata: Adversarialsemi-supervisedlearn-
ingapproach. InProceedingsofthe2019Conference
SatanjeevBanerjeeandAlonLavie.2005. Meteor: An onEmpiricalMethodsinNaturalLanguageProcess-
automaticmetricformtevaluationwithimprovedcor- ing and the 9th International Joint Conference on
relationwithhumanjudgments. InProceedingsof NaturalLanguageProcessing(EMNLP-IJCNLP).
theaclworkshoponintrinsicandextrinsicevaluation
measuresformachinetranslationand/orsummariza- Dong-JinKim,Tae-HyunOh,JinsooChoi,andInSo
tion,pages65–72. Kweon. 2021. Dense relational image captioning
viamulti-tasktriple-streamnetworks. IEEETransac-
StevenBirdandEdwardLoper.2004. NLTK:Thenatu- tionsonPatternAnalysisandMachineIntelligence,
rallanguagetoolkit. InProceedingsoftheACLIn- 44(11):7348–7362.
teractivePosterandDemonstrationSessions,pages
Dong-JinKim,Tae-HyunOh,JinsooChoi,andInSo
214–217,Barcelona,Spain.AssociationforCompu-
Kweon.2024. Semi-supervisedimagecaptioningby
tationalLinguistics.
adversariallypropagatinglabeleddata. IEEEAccess.
Nicolas Carion, Francisco Massa, Gabriel Synnaeve,
Diederik P Kingma and Jimmy Ba. 2014. Adam: A
Nicolas Usunier, Alexander Kirillov, and Sergey
methodforstochasticoptimization. arXivpreprint
Zagoruyko.2020. End-to-endobjectdetectionwith
arXiv:1412.6980.
transformers. InEuropeanconferenceoncomputer
vision,pages213–229.Springer.
PatrickLewis,EthanPerez,AleksandraPiktus,Fabio
Petroni,VladimirKarpukhin,NamanGoyal,Hein-
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-
richKüttler, MikeLewis, Wen-tauYih, TimRock-
ishna Vedantam, Saurabh Gupta, Piotr Dollár, and
täschel,etal.2020. Retrieval-augmentedgeneration
C.LawrenceZitnick.2015. MicrosoftCOCOcap-
forknowledge-intensivenlptasks. AdvancesinNeu-
tions: Datacollectionandevaluationserver. arXiv
ralInformationProcessingSystems,33:9459–9474.
preprintarXiv:1504.00325.
Wei Li, Linchao Zhu, Longyin Wen, and Yi Yang.
Jae Won Cho, Dawit Mureja Argaw, Youngtaek Oh,
2023. Decap: Decoding clip latents for zero-shot
Dong-JinKim,andInSoKweon.2023a. Empirical
captioning via text-only training. arXiv preprint
studyonusingadaptersfordebiasedvisualquestion
arXiv:2303.03032.
answering. ComputerVisionandImageUnderstand-
ing,237:103842.
VictorWeixinLiang, YuhuiZhang, YongchanKwon,
SerenaYeung,andJamesYZou.2022. Mindthegap:
Jae Won Cho, Dong-Jin Kim, Hyeonggon Ryu, and
Understandingthemodalitygapinmulti-modalcon-
In So Kweon. 2023b. Generative bias for robust
trastiverepresentationlearning. AdvancesinNeural
visual question answering. In Proceedings of the
InformationProcessingSystems,35:17612–17625.
IEEE/CVFConferenceonComputerVisionandPat-
ternRecognition,pages11681–11690.
Zhiyue Liu, Jinyuan Liu, and Fanrong Ma. 2023.
Improving cross-modal alignment with synthetic
Junjie Fei, Teng Wang, Jinrui Zhang, Zhenyu He, pairs for text-only image captioning. Preprint,
Chengjie Wang, and Feng Zheng. 2023. Transfer- arXiv:2312.08865.
abledecodingwithvisualentitiesforzero-shotim-
age captioning. In Proceedings of the IEEE/CVF Ziyang Luo, Zhipeng Hu, Yadong Xi, Rongsheng
InternationalConferenceonComputerVision,pages Zhang,andJingMa.2023. I-tuning: Tuningfrozen
3136–3146. languagemodelswithimageforlightweightimage
captioning. In ICASSP 2023-2023 IEEE Interna-
SophiaGu,ChristopherClark,andAniruddhaKemb- tionalConferenceonAcoustics,SpeechandSignal
havi.2022. Ican’tbelievethere’snoimages! learn- Processing(ICASSP),pages1–5.IEEE.
ing visual tasks using only language supervision.
arXivpreprintarXiv:2211.09778. FeipengMa,YizhouZhou,FengyunRao,YueyiZhang,
andXiaoyanSun.2023. Imagecaptioningwithmulti-
Andrej Karpathy and Li Fei-Fei. 2015. Deep visual- contextsyntheticdata. Preprint,arXiv:2305.18072.
semantic alignments for generating image descrip-
tions. In Proceedings of the IEEE conference on RonMokady,AmirHertz,andAmitHBermano.2021.
computervisionandpatternrecognition,pages3128– Clipcap: Clip prefix for image captioning. arXiv
3137. preprintarXiv:2111.09734.DavidNukrai,RonMokady,andAmirGloberson.2022. Junyang Wang, Ming Yan, Yi Zhang, and Jitao Sang.
Text-onlytrainingforimagecaptioningusingnoise- 2023. From association to generation: Text-only
injectedclip. arXivpreprintarXiv:2211.00575. captioning by unsupervised cross-modal mapping.
arXivpreprintarXiv:2304.13273.
KishorePapineni,SalimRoukos,ToddWard,andWei-
JingZhu.2002. Bleu: amethodforautomaticevalu- JunyangWang,YiZhang,MingYan,JiZhang,andJitao
ationofmachinetranslation. InProceedingsofthe Sang.2022. Zero-shotimagecaptioningbyanchor-
40thannualmeetingoftheAssociationforComputa- augmentedvision-languagespacealignment. arXiv
tionalLinguistics,pages311–318. preprintarXiv:2211.07275.
ZuxuanWu,TingYao,YanweiFu,andYu-GangJiang.
AlecRadford,JongWookKim,ChrisHallacy,Aditya
2017. Deep learning for video classification and
Ramesh,GabrielGoh,SandhiniAgarwal,GirishSas-
captioning,page3–29. AssociationforComputing
try, Amanda Askell, Pamela Mishkin, Jack Clark,
MachineryandMorgan&Claypool.
etal.2021. Learningtransferablevisualmodelsfrom
naturallanguagesupervision. InInternationalconfer-
JunXu,TaoMei,TingYao,andYongRui.2016. Msr-
enceonmachinelearning,pages8748–8763.PMLR.
vtt: A large video description dataset for bridging
videoandlanguage. InProceedingsoftheIEEEcon-
AlecRadford,JeffreyWu,RewonChild,DavidLuan,
ferenceoncomputervisionandpatternrecognition,
DarioAmodei,IlyaSutskever,etal.2019. Language
pages5288–5296.
modelsareunsupervisedmultitasklearners. OpenAI
blog,1(8):9. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
AaronCourville,RuslanSalakhudinov,RichZemel,
RitaRamos,BrunoMartins,DesmondElliott,andYova and Yoshua Bengio. 2015. Show, attend and tell:
Kementchedjhieva.2023. Smallcap: lightweightim- Neural image caption generation with visual atten-
age captioning prompted with retrieval augmenta- tion. InInternationalconferenceonmachinelearn-
tion. In Proceedings of the IEEE/CVF Conference ing,pages2048–2057.PMLR.
onComputerVisionandPatternRecognition,pages
2840–2849. PeterYoung,AliceLai,MicahHodosh,andJuliaHock-
enmaier. 2014. From image descriptions to visual
RobinRombach,AndreasBlattmann,DominikLorenz, denotations: Newsimilaritymetricsforsemanticin-
Patrick Esser, and Björn Ommer. 2022. High- ferenceovereventdescriptions. Transactionsofthe
resolutionimagesynthesiswithlatentdiffusionmod- AssociationforComputationalLinguistics,2:67–78.
els. In Proceedings of the IEEE/CVF conference
Zequn Zeng, Yan Xie, Hao Zhang, Chiyu Chen,
oncomputervisionandpatternrecognition, pages
Zhengjue Wang, and Bo Chen. 2024. Meacap:
10684–10695.
Memory-augmented zero-shot image captioning.
arXivpreprintarXiv:2403.03715.
Yixuan Su, Tian Lan, Yahui Liu, Fangyu Liu, Dani
Yogatama, Yan Wang, Lingpeng Kong, and Nigel
Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei
Collier.2022. Languagemodelscansee: Plugging
Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and
visual controls in text generation. arXiv preprint
Jianfeng Gao. 2021. Vinvl: Revisiting visual rep-
arXiv:2205.02655.
resentations in vision-language models. Preprint,
arXiv:2101.00529.
YoadTewel,YoavShalev,RoyNadler,IdanSchwartz,
and Lior Wolf. 2022a. Zero-shot video caption-
ing with evolving pseudo-tokens. arXiv preprint A Image-likeRetrieval
arXiv:2207.11100.
YoadTewel,YoavShalev,IdanSchwartz,andLiorWolf.
COCO
2022b. Zerocap: Zero-shotimage-to-textgeneration Method
for visual-semantic arithmetic. In Proceedings of B@4 M C S
theIEEE/CVFConferenceonComputerVisionand
Knight 27.8 26.4 98.9 19.6
PatternRecognition,pages17918–17928.
Knight+ILR 29.8 25.6 102.7 19.7
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Table12: EffectofImage-likeRetrievalonKnight.
Kaiser,andIlliaPolosukhin.2017. Attentionisall
youneed. Advancesinneuralinformationprocessing
systems,30.
HyperParameters COCO Flickr30k NoCaps MSVD MSR-VTT
RamakrishnaVedantam,CLawrenceZitnick,andDevi Epochs 5 30 - 10 10
Parikh. 2015. Cider: Consensus-based image de- l 9 7 7 7 7
τ 5 3 3 5 6
scription evaluation. In Proceedings of the IEEE
conferenceoncomputervisionandpatternrecogni-
Table13: Hyperparametertable.
tion,pages4566–4575.In−domain Cross−domain VideoCaptioning
COCO Flickr
COCO=⇒NoCapsVal
COCO=⇒Flickr Flickr=⇒COCO MSR-VTT MSVD
Method In Near Out Entire
C S C S C S C S C S C S C S C S C S C S
ViECap 92.9 18.2 47.9 13.6 61.1 10.4 64.3 9.9 65.0 8.6 66.2 9.5 38.4 11.2 54.2 12.5 - - - -
Knight 98.9 19.6 56.3 16.3 - - - - - - - - 48.9 14.2 64.4 15.1 31.9 8.5 63.8 5.0
IFCap⋆ 102.0 20.0 59.8 15.8 70.1 11.2 72.5 10.9 72.1 9.6 74.0 10.5 47.5 12.7 60.7 13.6 20.8 4.1 40.2 3.4
IFCap 108.0 20.3 64.4 17.0 75.8 12.4 72.3 11.6 60.2 8.9 70.5 10.8 59.2 15.6 76.3 17.3 38.9 6.7 83.9 6.3
Table14: OverallcomparisonamongbaselinesandIFCap. ⋆: withoutEntityFilteringmoduleintheinferencetime.
WeobservethatImage-likeRetrievalisalsoap-
plicable to other models that employ text-to-text
retrieval(Wangetal.,2023). BasedonFig.5,we
perform ILR with ϵ = 0.04 in the training time
r
ofKnight. IntheCOCOtestset,everymetricex-
cept METEOR is improved compared to vanilla
Knight(Wangetal.,2023),verifyingtheeffective-
nessofourILR.
B Hyperparameter
We include the details about our experiments in
eachdatasetinTable13.
C ComparisonwithBaselines
Wecomparebaselines(Feietal.,2023;Wangetal.,
2023) with IFCap and IFCap⋆ in every domain,
includingin-domaincaptioning,cross-domaincap-
tioning,andvideocaptioning. Resultscanbefound
inTable14.
D QualitativeResults
WeshowadditionalqualitativeresultsinFig.6.Knight: Asilverpassengertraintraveling Knight: Amanwithabeardand adogona Knight: Aviewof amountainrangewith
downatracknexttoanelevatedwalkway. couch. anairplaneinthebackground.
ViECap: Acarisshowninfrontof alarge ViECap: Amanstandingnexttoabrown ViECap: Alargeairplaneflyingthrougha
billboard. and whitedog. bluesky.
ViECap entity: [] ViECap entity: [dog] ViECap entity: [airplane]
IFCap: Amonorailtraintravelingdown IFCap: Amanand adogaresmilinginfront IFCap: The wingof anairplanewith
tracksnexttoabuilding. of aChristmastree. mountainsinthebackground.
IFCap entity: [monorail, train] IFCap entity: [man, dog] IFCap entity: [mountain, wing, airplane]
GT:Amonorailmakingit'swaydownthe GT:Amaninfrontof aChristmastreewith GT: The viewoutof anairplanewithpartof
trackaboveabunchof cars. hisdog. thewing.
Knight: Agiraffestandingnexttoalarge Knight: Agroupof menracingeachother Knight: Amotorcycleisparkedontheside
tree. onacourse. of theroadnexttoatree.
ViECap: Twogiraffesstandingnexttoeach ViECap: Askierinaredjacketisskiing ViECap: Abasketfullof bananashanging
otherinagrassyarea. downahill. fromatree.
ViECap entity: [giraffe] ViECap entity: [skis] ViECap entity: []
IFCap: Agiraffestandingnexttoatreein IFCap: Twocrosscountryskiersracing IFCap: Amotorcyclethatissittingontopof
thewater. downahill. afence.
IFCap entity: [giraffe, tree, water] IFCap entity: [country] IFCap entity: [motorcycle]
GT: Agiraffeinafieldnexttotreeand body GT: Twocrosscountryskiersheadingonto GT: Amotorcyclesittingontopof afence
of water. thetrail. asdécor.
Knight: Agroupof trafficlightssittingon Knight: Abowlof fruitsittingontopof a Knight: Ablackvasewithawhiteflowerin
topof aroad. counter. it.
ViECap: Astreetfilledwithtrafficlights ViECap: Acloseupof fruitsand vegetables ViECap: Ablackand silverspoonwitha
nexttoatallbuilding. onatable. toothbrushinit.
ViECap entity: [trafficlight] ViECap entity: [] ViECap entity: [spoon]
IFCap: Abunchof trafficlightsatan IFCap: Acloseupof abowlof orangesand IFCap: Ablackand whitevasewitha
intersection. applesonacounter. flowerinit.
IFCap entity: [light, intersection] IFCap entity: [bowl, apple, orange, counter] IFCap entity: [vase, flower]
GT: A photo taken from one vehicle of GT: abowlof applesand abowlof oranges. GT: Thin black and white vase with black
another at an intersection. flowers.
Figure6: QualitativeresultontheCOCOtestset. Wehighlighttheretrievedentitiesandtheirappearanceinthe
generatedcaptionswith IFCap, ViECap and Intersection.