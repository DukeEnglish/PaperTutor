Adapting Large Language Model for
Cross-Subject Semantic Decoding from
Video-Stimulated fMRI
Ruizhe Zheng1 and Lichao Sun2
1 Fudan University, 220 Handan Road, Shanghai 200433 China
2 Lehigh University, 27 Memorial Drive West Bethlehem, PA 18015 USA
rzzheng23@m.fudan.edu.cn, james.lichao.sun@gmail.com
Abstract. Decodingvisual-semanticinformationfrombrainsignals,such
asfunctionalMRI(fMRI),acrossdifferentsubjectsposessignificantchal-
lenges, including low signal-to-noise ratio, limited data availability, and
cross-subject variability. Recent advancements in large language mod-
els (LLMs) show remarkable effectiveness in processing multimodal in-
formation. In this study, we introduce an LLM-based approach for re-
constructing visual-semantic information from fMRI signals elicited by
videostimuli.Specifically,weemployfine-tuningtechniquesonanfMRI
encoder equipped with adaptors to transform brain responses into la-
tent representations aligned with the video stimuli. Subsequently, these
representations are mapped to textual modality by LLM. In particular,
we integrate self-supervised domain adaptation methods to enhance the
alignmentbetweenvisual-semanticinformationandbrainresponses.Our
proposedmethodachievesgoodresultsusingvariousquantitativeseman-
tic metrics, while yielding similarity with ground-truth information.
Keywords: BrainDecoding·LargeLanguageModel·SemanticRecon-
struction · Unsupervised domain adaptation
1 Introduction
Advancements in semantic brain decoding, which aims at reconstructing the se-
manticinformationimplicitlycontainedinvariousexternalstimulisuchasimage
and video signals from brain activity patterns, have showcased the remarkable
potential of decoding information that offers a pathway to mind-reading tech-
nologies that can have important clinical and scientific applications [3,9]. How-
ever, such endeavors carry many challenges. Noninvasive imaging techniques
such as functional MRI (fMRI) have lower temporal or spatial resolution and
varies a lot across different individuals due to unique anatomical and functional
attributes. The rarity of data is also an important concern as the brain de-
coders are often insufficiently trained and may have problem generalizing on
newly encountered subjects with different condition and content of stimuli. De-
spitebreakthroughsinartificialintelligenceforbraindecoding,theselimitations
4202
peS
62
]VC.sc[
1v78971.9042:viXra2 R. Zheng, L. Sun
raises questions regarding generalizability and effectiveness of cross-subject re-
construction of visual-semantic information.
In recent years, large language models (LLMs) have continuously pushed
the upper limit of natural language understanding with ever increasing param-
eter sizes and pre-training data scales [8] [6]. In particular, LLMs have demon-
strated remarkable multimodal information processing and have achieved great
success in generating visual-semantic contents. In terms of visual understand-
ing, by conditioning the model with one or more modalities or instructions,
LLMs can achieve strong few-shot or zero-shot performance on vision-language
tasks. In a lot of scenarios, the LLM-generated texts have high quality and fi-
delity and cannot easily be distinguished from genuine human texts. Adapting
LLMsforvisualunderstandingiscomputationallyintensive,resultinginconsid-
erablememoryconsumption.Therefore,manyresearchershaveattempttoapply
frozen pre-trained language decoders and vision encoders in accommodating vi-
sual input. Image- or video-text cross-modal learning has achieved remarkable
performance in many downstream tasks by using various computationally effi-
cient strategies that optimizes a small proportion of parameters or additionally
equipped adaptor modules. For instance, BLIP-2 [5] uses a small Transformer-
basedadaptorduringvision-languagealignmenttrainingandinstructiontuning.
Video-LLaMA[7]appliessimilarapproachinvideo-languagerepresentation.The
advantage of this solution is that it takes advantage of existing models and re-
quires only parameter-efficient tuning rather than full finetuning of LLMs and
visual encoders.
Inspired by these observations, we propose a novel multimodal finetuning
framework that fully leverages frozen brain and visual encoders, coupled with
an instruction-tuned video-language foundation model, to decode linguistic rep-
resentations from brain signals recorded in subjects who receive dynamic visual
stimuli. Given the large size of raw fMRI data and the intrinsic spatio-temporal
dynamics, we design a three-dimensional Convolutional Neural Network (CNN)
tokenizer to transform raw fMRI data into tokens which will be further encoded
by a brain encoder pretrained on Human Connectome Project (HCP) datasets.
In the first stage, it jointly optimizes low-rank adaptors, which are attached to
fMRI encoder and video Q-former, along with projection adaptors that connect
the intermediate embedding of fMRI with LLM, and the spatio-temporal tok-
enizer, to learn visual-linguistic patterns from raw neural data by contrastive
learning.Inthesecondstage,aswelackgroundtruthlinguisticrepresentationof
video contents, we sample texts from Video-LLaMA, one of the state-of-the-art
multimodal LLMs for video understanding. Then, video query tokens concate-
natedwithpertinentvideo-relatedquestionsareprocessedbyLLaMA.Thegen-
erated answers can be used to construct paired fMRI-text data for supervised
instruction finetuning. During inference, LLaMA will receive only text prompts
andfMRItokensforcomprehensionofthevisual-semanticbrainactivities.More-
over,weemployaself-superviseddomainadaptationapproachtolearnresilient,
discriminative feature embeddings across individuals while preventing the inad-
vertentleakageofvisualinformation.Importantly,theentiretrainingprocedureAbbreviated paper title 3
remains agnostic to both the stimuli and their corresponding labels within the
validation set while reducing the subject-wise domain discrepancies.
We summarize the contributions of the work as follows. First, an end-to-
endLLM-centricpipelineisestablishedtoreplacetraditionalmultimodalneural
networks. Despite no groundtruth semantic information is available, we manage
to use LLM for automatic annotation to create aligned fMRI-video-text triads.
Second, we investigate on video rather than image modality, which further in-
creases the difficulty because both spatial and temporal information is required
for holistic visual understanding. Third, our method demonstrates good gener-
alizability on distinct individuals and stimuli, which is of pivotal importance in
neuroscientific research and applications.
2 Related Work
Many previous work focus on direct reconstruction of stimulating signals by
mapping latent representation of fMRI and visual signals from one to the other.
For instance, [3] uses linear regression regularized by sparsity constraints on
preprocessed fMRI data to predict features extracted from low-level neural rep-
resentation by pretrained CNN for images, which does not involve more subtle
alignment of distinct modalities. [12] uses fMRI data and stimulus images to
create an end-to-end reconstruction model involing training a generative adver-
sarial network (GAN). [9] uses conditioned GAN to reconstruct images that are
consistent with groundtruth in terms of semantic meanings. [13] apply diffusion
model guided by semantic information of image content to reconstruct image
from fMRI. [1] manages to reconstruct video from fMRI by using multimodal
alignment to extract semantically rich representations as guidance for diffusion-
based decoding. High-level or semantic information reconstruction, which is the
maintaskinvestigatedinthepaper,involvesmorecomplicatedtechniqueswhich
requires more discriminative representation of stimulating signals. [2] have in-
vestigated captioning of image data by utilizing a combination of a pre-trained
visual encoder and language decoder for semantic reconstruction. [14] use simi-
larapproachtoreconstructintelligiblewordsequencesthatrecoverthesemantic
representation of speech and video stimuli in human brains.
3 Methods
3.1 Architecture
FoundationModels. WefollowtheVideo-LLaMAarchitecture[18].AQuery-
Former is an encoder-only transformer with 32 learned query tokens as input: it
contextualizes the query tokens – via the cross-attention mechanism – with the
representations of the image patches encoded by a large (frozen) Vision Trans-
former (ViT) The visual tokens that are the output of the Q-Former are then
projected into the LLM embedding space with a single linear projection. For
fMRIdata,weuseSparse-CodedMaskedBrainModeling(SC-MBM)model[1],4 R. Zheng, L. Sun
a fMRI encoder developed on large amount of data downloaded from Human
Connectome Projects (HCP), which contains high-quality fMRI recorded under
resting and task-evoked paradigm. The encoder is pretrained through a masked
autoencoder strategy, such that the model will acquire strong contextual ab-
straction of the temporal and spatial associations by enforcing it to restore the
masked voxels.
Spatio-Temporal Convolutional Tokenizer. To make fully usage of the
fMRIdata,wedecidetonotrestrictouranalysistothevisualcortex,sinceother
cortical regions involving in visual and semantic information flow may also be
beneficial for decoding. However, each block of fMRI data contains more than
1,000,00 voxels in total in our research. This renders the tackling of spatio-
temporal sequence by voxel-based SC-MBM very challenging because the self-
attention mechanism demands quadratic memory consumption. Therefore, we
are motivated to design a three-dimensional convolutional tokenizer that will
transform the huge amount of voxels into super-voxel sequence representation
suitable for further processing by SC-MBM.
Design of Adaptor. Finetuning large pretrained models for downstream
adaptationshasbecomeastandardtechnicalpipeline.Recentresearch[4,16,17]
have highlighted the feasibility of tuning models by freezing the pre-trained
parametersandintroducingusuallystructuredsmallamountofnewparameters
to the original architecture. As shown in Figure 2, we design a nonlinear low-
rank adaptor of parameter-efficient repurposing of the multimodal combination
of models for our goal. By inserting these adaptors into the query projection
layers in self-attention and the multilayer perceptron modules of the ViT-based
fMRI encoder and the BERT Transformer-based Q-Former.
3.2 Training Procedure
Ourapproachadoptsatwo-stagetrainingparadigmfortrainingthemodel:cross-
modalalignment(StageI)andsupervisedinstructionfine-tuning(StageII).Let
X be the video embedding of L tokens calculated by Q-Former, Z be the fMRI
embedding calculated by fMRI encoder equipped with adaptors, we formulate
training objectives as follows.
Stage I. In this stage, we aim at learng a cross-modal alignment between the
embeddingspacesoffMRIandvideofromcorporaoftheirrespectivemodalities.
We aim to distinguish the right fMRI patterns out of a batch of data, each
containsdifferentneuralvisualandsemanticrepresentations.Todoso,weadopt
theCLIPlossproposedin[10].Specifically,weconductcross-modalalignmentby
drawingthepairedvideoandfMRIembeddingsextractedbySC-MBMtogether
while pushing unpaired away in the latent space.
To do so, we train f using the CLIP loss on batches of size B with exactly
θ
(cid:0) (cid:1)
one positive example: 1
B (cid:32) (cid:33)
L
(θ)=−1 (cid:88)
log
exp(s(zˆ i,z i)/τ)
+log
exp(s(zˆ i,z i)/τ)
CLIP B (cid:80)B exp(s(zˆ,z )/τ) (cid:80)B exp(s(zˆ ,z )/τ)
i=1 j=1 i j k=1 k iAbbreviated paper title 5
Fig.1: The overall framework of our approach for brain visual-semantic reconstruc-
tion.ThefMRIisencodedbya3DCNNtokenizerandSC-MBM.Thevideoisencoded
byViT.TheparametersofSC-MBM,ViTandQ-Formerareallfrozen,butSC-MBM
andQ-Formerisinsertedwiththenonlinearadaptormodule.Duringtraining,itlearns
cross-subject semantically informed fMRI latent representation by cross-modal align-
ment and domain adaptation, and the quality of decoding is improved by minimizing
thedifferencebetweenvideo-andfMRI-basedvideounderstandingbytheinstruction-
tuned LLM.
where s is the cosine similarity, z and zˆ = f (X ) are the latent video repre-
i i θ i
sentation and the corresponding fMRI-based prediction, respectively, and τ is a
learned temperature parameter, which is set as 0.05 during training.
Next, as we also want the LLM to process fMRI tokens so as to extract
immanent visual-language information, we also train adaptors to directly map
thefMRItovisual-languageembeddingssuchthattheycanbeunderstoodbythe
frozenLLMtoreconstructindividualvisual-languagecognitionofvideostimuli.
This is achieved using weighted L and L reconstruction losses:
2 1
N L
1 (cid:88)(cid:88)
L (θ)= (1−α)∥z(l)−x(l)∥2+α∥z(l)−x(l)∥ (1)
L2−L1 NL n n 2 n n 1
n=1l=1
Finally,wecombinetheCLIPandreconstructionlossesusingaconvexcombina-
tionwithtunedweighttotrainmodelsthatbenefitfrombothtrainingobjectives:
L =βL +(1−β)L (2)
Total CLIP L2−L1
Stage II. As there is no groundtruth for video-language understanding in our
experimentalsetting,weadoptabootstrappingapproachfortrainingthemodel
to reconstruct semantic information from video stimuli-induced fMRI activities.
ForagivenbatchofN fMRIrecords,weassigneachofthemarandomlyselected
instructionfromacandidateinstructionlist,andgeneratesurrogategroundtruth6 R. Zheng, L. Sun
data from ViT-embedded video tokens and the instructions. These surrogate
texts are used for supervised instruction tuning that will allow the model to
learn more intricate semantic information. Then, we freeze all parameters of
LLM,QformerandfMRIencoderexceptforanadaptorthatbridgestheencoders
and the LLM. The adaptor is trained with cross-entropy loss
B T
1 (cid:88)(cid:88)
L =− logp(y |y ,θ), (3)
CE B j,t j<t
j=1t=1
wherey denotesthetruetokenatpositiontinthej-thsequenceinthebatch,
j,t
y represents the tokens preceding y .
j<t j,t
We introduce an additional classifier head to the original framework to con-
ductdomainadaptation.Specifically,aneighborhoodclustering-basedapproach
[11] applied in order to learn better fMRI representation from a proportion of
target domain data. Let g be a trainable linear projection layer. Its weight
[w ,w ,...,w ] is conceived as C video classes contained in the training data.
1 2 C
y is the output of projection after SoftMax activation, or the predicted categor-
ical distribution. According to the approach proposed in [11], a memory pool
that stores N target domain feature vectors is to be trained and concatenated
with the weight vectors of g. Then the similarity of i-th (i ̸= j) target do-
main feature f to the memory features F is formulated as p =
exp(Fj⊤fi/τ),,
i i i,j Zi
whereZ = (cid:80)N+C exp(F ⊤f /τ). The scale parameter τ is set as 0.5 in our
i j=1,j̸=i j i
experiments. Thus, the proportion of fMRI data from validation subjects are
usedtocalculateadomainalignmentloss,whichiscalledneighborhoodcluster-
ing (NC) loss in [11], so as to minimize the discrepancies inevitably encountered
in the proprotion of fMRI data that will be used for test:
B N+K
1 (cid:88) (cid:88)
L =− p log(p ), (4)
NC B i,j i,j
i=1j=1,j̸=i
where B is batch size.
Because we assume that we have no prior knowledge of the categorical in-
formation of test data, we also apply the entropy separation (ES) loss proposed
in [11] that will be optimized to make the entropy of target sample semantic
classeslargerandthesourcesamplessmaller.Letp bethepredictedclassprob-
i
ability vector, m and rho are hyperparameters, the loss is formulated as
B (cid:40)
L =
1 (cid:88)
L (p ), L (p )=
−|H(p i)−ρ| (|H(p i)−ρ|>m),
(5)
ES B ES i es i 0 otherwise.
i=1
To be clear, no target domain videos or annotation labels are used during train-
ing. The final evaluation is conducted on the rest proprotion of test subjects.
Therefore, we boost better fMRI learning by utilizing the strong generation ca-
pabilities of LLM as well as domain adapation. The training objective is set as
L =λL +(1−λ)L (6)
StageII CrossEntropy DomainAdaptationAbbreviated paper title 7
4 Experiments
4.1 Datasets
Throughout this work, we use two openly available fMRI-video datasets. They
were collected and published with all participants read and signed an informed
consent form approved by the respective ethics committee. No identifiable sub-
ject information is contained in the data.
Large-Scale fMRI Human
ActionRecognitionDataset.
Thisdatasetisdescribedin[19].
It is a large-scale fMRI dataset
for human action recognition
consisting of fMRI responses to
21,600 video clips from 30 par-
ticipants. The video clips en-
compass 180 human action cat-
egories and offer a comprehen-
sive coverage of complex activi-
ties in daily life. 26 subjects are
assigned as training data and 4
subjectsareusedforvalidation.
Urgen Natural Human Ac- Fig.2:Thenonlinearadaptorusedforfinetuning.
tion Dataset. This dataset is
described in [15]. It is a fMRI
dataset recorded on 4 subjects under visual stimuli randomly sampled from a
largevideosetconsistingof100differentnaturalactions.2subjectsareassigned
as training data and 2 subjects are used for validation.
4.2 Results
As shown in Table 1 and 2, our approach achieves effective results among differ-
ent individuals, resulting in the average BERTScore and SacredBLEU-1 across
all validation individuals of 53.27% and 33.91% on Large-Scale fMRI Human
Action Recognition Dataset. Further validation of our method on the Urgen
Natural Human Action Dataset achieve results of 66.10% and 53.59%, respec-
tively. The outcomes suggest the importance of using strong LLM as semantic
decoder and the effectiveness of both appropriate multimodal alignment train-
ing and domain adaption in finetuning the fMRI-encoder so that it can actually
retrieve consistent video-evoked brain responses.
We showcase some reconstructed semantic information from fMRI with the
groundtruth video understanding prompted by several questions. The current
approach demonstrates strong temporal understanding ability, which is crucial
for accurate visual-semantic decoding as the actions and scenes in the video
continually changes by time order.
– Question: What is the main thing happening in the video?8 R. Zheng, L. Sun
BERTScore (%) SacredBLEU (%) Rouge-L (%)
Subject
SacredBLEU-1 SacredBLEU-2 F P R
Subject-1 54.48 32.92 22.35 33.97 35.44 40.02
Subject-2 51.77 34.87 23.27 34.22 37.11 35.82
Subject-3 52.01 33.50 22.71 34.35 36.61 38.36
Subject-4 54.81 34.34 23.44 34.30 33.86 41.48
Total 66.10 53.59 43.98 53.22 54.47 54.55
Table1:ResultsofBERTScore,SacredBLEU,andRouge-LmetricsforfMRIsemantic
reconstruction on Large-Scale fMRI Human Action Recognition Dataset.
BERTScore (%) SacredBLEU (%) Rouge-L (%)
Subject
SacredBLEU-1 SacredBLEU-2 F P R
Subject-1 67.14 54.19 45.00 54.42 56.49 54.99
Subject-2 65.06 52.98 42.95 52.02 55.96 54.11
Total 66.10 53.59 43.98 53.22 54.47 54.55
Table2:ResultsofBERTScore,SacredBLEU,andRouge-LmetricsforfMRIsemantic
reconstruction on Urgen Natural Human Action Dataset.
– Video2Text:In the video, a young man is playing squash in a court. He is
wearing a white shirt and is holding a tennis racket. He is hitting the ball
with the racket and running around the court.
fMRI2Text:Based on the visual content, the main thing happening in the
video is a young man playing squash in a gym. He is wearing a black shirt
and is playing with a racket. He is hitting the ball against the wall of the
gym, and the ball is bouncing back and forth between him and the wall.
– Video2Text: In the video, we see a young woman getting her hair cut by a
hairdresser in a salon. The hairdresser is cutting her hair with scissors, and
the woman is sitting in a chair.
fMRI2Text: In main thing happening in the video is a young woman get-
ting her hair cut by a hairstylist in a salon.
5 Conclusion
Inthisstudy,wehighlightthestrongcapacityofLargeLanguageModels(LLMs)
toreconstructvisual-semanticinformationfromfMRIbrainresponses.Ourfind-
ings demonstrate that the proposed methodology can produce summaries of
videocontentinamannerthatisindependentofboththesubjectandthestim-
uli. This research presents a novel approach to semantic decoding, which holds
promiseforapplicationsinbrain-machineinterfacesandtheinvestigationofhu-
manbrainresponsecharacteristicstoalterationsinexternalstimuli.Drawingon
these foundations, we could further our comprehension of how the human brain
processes visual and linguistic information, ultimately enhancing generative AI
and its associated applications.Abbreviated paper title 9
References
1. Chen, Z., Qing, J., Zhou, J.H.: Cinematic mindscapes: High-quality video recon-
struction from brain activity (2023)
2. Ferrante,M.,Ozcelik,F.,Boccato,T.,VanRullen,R.,Toschi,N.:Braincaptioning:
Decoding human brain activity into images and text (2023)
3. Horikawa, T., Kamitani, Y.: Generic decoding of seen and imagined objects us-
ing hierarchical visual features. Nature Communications 8(1), 15037 (Aug 2017).
https://doi.org/10.1038/ncomms15037, http://www.nature.com/articles/
ncomms15037
4. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L.,
Chen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685 (2021)
5. Li, J., Li, D., Xiong, C., Hoi, S.: Blip: Bootstrapping language-image pre-training
for unified vision-language understanding and generation. In: ICML. pp. 12888–
12900. PMLR (2022)
6. Li, K., He, Y., Wang, Y., Li, Y., Wang, W., Luo, P., Wang, Y., Wang, L., Qiao,
Y.:Videochat:Chat-centricvideounderstanding.arXivpreprintarXiv:2305.06355
(2023)
7. Lin,B.,Ye,Y.,Zhu,B.,Cui,J.,Ning,M.,Jin,P.,Yuan,L.:Video-llava:Learning
united visual representation by alignment before projection (2023)
8. Maaz, M., Rasheed, H., Khan, S., Khan, F.S.: Video-chatgpt: Towards de-
tailed video understanding via large vision and language models. arXiv preprint
arXiv:2306.05424 (2023)
9. Ozcelik, F., Choksi, B., Mozafari, M., Reddy, L., VanRullen, R.: Reconstruction
of Perceived Images from fMRI Patterns and Semantic Brain Exploration us-
ing Instance-Conditioned GANs (Feb 2022), http://arxiv.org/abs/2202.12692,
arXiv:2202.12692 [cs, eess, q-bio]
10. Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agarwal,S.,Sastry,G.,
Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from
naturallanguagesupervision.In:Internationalconferenceonmachinelearning.pp.
8748–8763. PMLR (2021)
11. Saito, K., Kim, D., Sclaroff, S., Saenko, K.: Universal domain adaptation through
self supervision. Advances in neural information processing systems 33, 16282–
16292 (2020)
12. Shen, G., Dwivedi, K., Majima, K., Horikawa, T., Kamitani, Y.: End-to-end deep
imagereconstructionfromhumanbrainactivity.Front.Comput.Neurosci.13, 21
(Apr 2019)
13. Takagi, Y., Nishimoto, S.: High-resolution image reconstruction with latent dif-
fusion models from human brain activity. bioRxiv (2023). https://doi.org/10.
1101/2022.11.18.517004, https://www.biorxiv.org/content/early/2023/03/
11/2022.11.18.517004
14. Tang, J., LeBel, A., Jain, S., Huth, A.G.: Semantic reconstruction of continuous
language from non-invasive brain recordings. Nature Neuroscience pp. 1–9 (2023)
15. Urgen,B.A.,Nizamoğlu,H.,Eroğlu,A.,Orban,G.A.:Alargevideosetofnatural
humanactionsforvisualandcognitiveneurosciencestudiesanditsvalidationwith
fmri. Brain Sciences 13(1), 61 (2022)
16. Yin, D., Yang, Y., Wang, Z., Yu, H., Wei, K., Sun, X.: 1% vs 100%: Parameter-
efficientlowrankadapterfordensepredictions.In:ProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition.pp.20116–20126(2023)10 R. Zheng, L. Sun
17. Yuan, Z., Zhang, J., Shan, S.: Fulllora-at: Efficiently boosting the robustness of
pretrained vision transformers (2024)
18. Zhang, H., Li, X., Bing, L.: Video-llama: An instruction-tuned audio-visual lan-
guage model for video understanding. arXiv preprint arXiv:2306.02858 (2023)
19. Zhou,M.,Gong,Z.,Dai,Y.,Wen,Y.,Liu,Y.,Zhen,Z.:Alarge-scalefmridataset
for human action recognition. Scientific Data 10(1), 415 (2023)