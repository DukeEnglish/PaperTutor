Unveiling the Role of Pretraining in Direct Speech Translation
BelenAlastruey1,2,GerardI.Gállego2,MartaR.Costa-jussà1
FAIR,Meta1
TALPResearchCenter,UniversitatPolitècnicadeCatalunya,Barcelona2
alastruey@meta.com,gerard.i.gallego@upc.edu,costajussa@meta.com
Abstract a method to enhance ST performance for low-
resource source languages by utilizing ASR pre-
Direct speech-to-text translation systems en-
counteranimportantdrawbackindatascarcity. trainingfromahigh-resourcelanguage. Alinejad
A common solution consists on pretraining andSarkar(2020)enhancedtheperformanceofan
theencoderonautomaticspeechrecognition, STsystembypretrainingboththeencoderandthe
hencelosingefficiencyinthetrainingprocess. decoderonASRandMTrespectively. Wangetal.
Inthisstudy,wecomparethetrainingdynam-
(2020b)andLeetal.(2023)proposedvariationsof
icsofasystemusingapretrainedencoder,the
ASRpretrainingthatyieldedsuperiorresults.
conventional approach, and one trained from
However, pretraining has some drawbacks too.
scratch. Weobservethat,throughoutthetrain-
ing, therandomlyinitializedmodelstruggles Ithasadditionaldatarequirements,whichcanbea
toincorporateinformationfromthespeechin- problemparticularlyinlanguagesthatdon’thavea
putsforitspredictions. Hence,wehypothesize writtenformandhencenoASRdata. Furthermore,
thatthisissuestemsfromthedifficultyofef- itcomplicatesthetrainingpipelineandworsensthe
fectivelytraininganencoderfordirectspeech
efficiencyoftheoveralltrainingprocess.
translation. Whileamodeltrainedfromscratch
Recentstudieshavealreadyquestionedthepre-
needstolearnacousticandsemanticmodeling
trainingapproach,Zhangetal.(2022),demonstrat-
simultaneously, a pretrained one can just fo-
ingthatsimilarresultscanbeachievedundercer-
cusonthelatter. Basedonthesefindings,we
proposeasubtlechangeinthedecodercross- tain conditions without the need for pretraining.
attentiontointegratesourceinformationfrom However, the authors show that many strategies
earlier steps in training. We show that with needtobesimultaneouslyusedtoachievethis,such
thischange,themodeltrainedfromscratchcan
as an exhaustive hyperparameter tunning, CTC-
achieve comparable performance to the pre-
basedregularizationandtheirproposedparameter-
trainedone,whilereducingthetrainingtime.
izeddistancepenalty.
1 Introduction
Complementingpreviousinterpretabilityworks
in ST (Xu et al., 2021; Alastruey et al., 2022), in
Inrecentyears,extensiveresearchhasbeendonein
thisstudy,weconductthefirst-everanalysisofthe
thefieldofspeech-to-texttranslation(ST).These
training dynamics of a ST system, and based on
models have transitioned from cascaded systems
itsresults,weproposeasubtlemodificationinthe
to direct ones (Salesky et al., 2023). While this
Transformer(Vaswanietal.,2017)architectureto
shifthelpsmitigateerrorpropagation,itintroduces
bypassthepretrainingstage.
other challenges such as scarcity of training data
First, we compare the training dynamics of a
and the need for the model to tackle translation
conventionalsystemthatusesapretrainedencoder
andspeechrecognitionsimultaneously. Tobypass
withonetrainedfromscratch1. Throughthisanal-
these issues, a common approach to train direct
ysis,weobservesignificantdisparitiesintheirbe-
STsystemsinvolvespretrainingtheencoderonthe
haviors. Particularly, we note that when making
AutomaticSpeechRecognition(ASR)task(Bérard
predictions,themodeltrainedfromscratchdelays
et al., 2018). This enables the encoder to learn
theutilizationofinformationextractedbytheen-
acousticmodelinginthesourcelanguagebylever-
coderuntilalaterstageoftraining.
aging ASR data, and the model can focus on se-
Wehypothesizethatthisdelayoccursduetothe
manticmodelingduringtheSTtraining.
Various studies have been conducted on pre-
1Thepretrainingisdoneonthesameamountoftraining
training for ST. Bansal et al. (2019), introduced datathantheSTtraining.
4202
peS
62
]LC.sc[
1v44081.9042:viXracomplexityoftheacousticmodelingtask, thatin 2.2 TrainingDynamicsonMachine
this setting needs to be learned together with the Translation
semantic modeling. Hence, it takes a significant
Previous work has been done to understand how
amountofupdatestosufficientlytraintheencoder
Transformers learn in the task of Machine Trans-
sothatitcanextractmeaningfulinformation. Con-
lationontext. Voitaetal.(2021)analysehowthe
sequently, themodelignorestheencoderoutputs
sourcecontributionvariesalongthetrainingusing
and focuses on training the decoder for language
LayerwiseRelevancePropagationmethodtotrack
modeling. Oncetheencodercanextractvaluable
sourceandtargetcontribution,anddescribethree
representations,themodelhasalreadyconverged
differenttrainingphases.
towards language modeling and struggles to rely
ontheinformationobtainedbytheencoder. Target-sidelanguagemodeling: Thebeginning
Secondly,webelievethatbyforcingthemodel oftrainingisdevotedtotarget-sidelanguagemod-
toutilizeencoderoutputsearlierinthetrainingpro- eling. Thetotalcontributionofthesourcesubstan-
cess, the model would not converge towards lan- tially decreases. This means that in the trade-off
guagemodelingandtheencoderwouldbetrained betweeninformationcomingfromthesourceand
more rapidly, leading to higher-quality represen- the target prefix, the model gives more and more
tations in its outputs. Through a modification prioritytotheprefix.
in the residual connection in the decoder cross-
Learninghowtousesource: Inthesecondstage,
attentionmechanism, weforcethemodeltrained
thesourceinfluenceincreasesquickly. Thismeans
fromscratchtointegratesourceinformationfrom
that,oppositetothefirststage,inthetrade-offbe-
earliertrainingsteps,andweobserveacomparable
tweeninformationcomingfromthesourceandthe
performancethaninthepretrainedone.
targetprefix,themodelprogressivelygivesmore
Overall,themaincontributionsofourworkare:
andmoreimportancetosourceinformation.
(1)thefirststudyoftrainingdynamicsinST,that
unveilstheroleofthepretraining,and(2)amodi- Refining translations: In the last stage, the
ficationintheTransformerarchitecturetobypass sourcecontributionremainsconstant. Byanalysing
thepretrainingstage. othermetricstheauthorsseethatthemodelislearn-
ingtorefinesometranslations. Themodellearns
toalignbetter,andisabletogeneratemorenatural
2 RelatedWork
translationsinsteadofword-to-wordones.
2.1 InterpretabilityofTransformerModels 3 TrainingDynamicsinSpeech
Translation
Our research aims to quantify the source of in-
formation used for making predictions in these Inthissection,weanalysehowmuchtwotraining
models,specificallywhetheritoriginatesfromthe strategiesonaTransformer-basedsystem2 relyon
source (encoder input) or the target prefix (previ- thespeechsourcewhenmakingpredictionsalong
ouslypredictedwordsservingasdecoderinputs). a ST training. In particular, we study: (1) a stan-
Toachievethis,weemploytheALTI+interpretabil- dardSTsystem,consistingonanASRpretraining,
itymethod(Ferrandoetal.,2022a). followedbyare-initializationofthedecoderand
ALTI+ employs a strategy to rewrite attention a training on ST data, and (2) a system that only
blocks, as introduced by Kobayashi et al. (2020), performs a ST training on a randomly initialized
alongwiththecontributiondefinitionprovidedby model.
Ferrando et al. (2022b) and a variation of rollout Tomeasuretheamountofinputinformationused
inspired by Abnar and Zuidema (2020). By uti- by the model to generate a prediction we use the
lizing ALTI+, we determine the extent to which sourcecontributiondefinedbyALTI+,coveredin
eachinputtokeninthesourceandtargetprefixcon- Section2.1.
tributetothepredictionofatoken. Furthermore,by Togeneralizethisscoretoasentence-wisescore,
summingtheindividualcontributionofeachtoken we average the source contribution used for the
intheencodersource,theauthorsobtainaunique
2We train the small S2T-Transformer architecture
scorereferredtoassourcecontribution,thatweuse
from Fairseq (https://github.com/facebookresearch/
tostudytrainingdynamicsonST. fairseq).Figure1: Sourcecontribution(±std)andBLEUalongthefulltraining(right)andalongthefirst10kupdates(left).
predictionofeachtokeninasentence. Finally,to Instead,theSTmodeltrainedfromscratchunder-
obtainascoreoveratestset,weaverageagainthe goesthesamethree-stageprocessthantexttrans-
scoreobtainedbyeverysentenceintheset. lation. However, each stage appears to require
Given that we use a different source contribu- significantly more time compared to text transla-
tionmeasurethanVoitaetal.(2021)intheirwork tion. Specifically, the model does not achieve a
describedinSection2.2,wedecidetotrainanad- stable level of source contribution until after ap-
ditionalMTmodel,toconfirmthatthethreestages proximately 30k updates, whereas the other two
describedintheirworkstillhappenonoursetting. modelsachievethisstabilityafteronly6kupdates.
Forallouranalysis,westoreacheckpointevery
We hypothesize this happens due to the diffi-
1kupdatesduringthefulltraining,andevery100
culty of training the encoder for the task of ST
updatesduringthefirst10k3. Foreachofthecheck-
from scratch. Unlike an encoder in text transla-
points, we evaluate the model computing BLEU
tion, which solely requires semantic modeling, a
andsourcecontributionscoresonEnglish-German
STencoderlearnsbothacousticandsemanticmod-
MuST-Cdataset(Cattonietal.,2021)4.
eling. This dual requirement makes the training
process for an ST encoder more time-consuming
3.1 ResultsAnalysis
thanthatofatexttranslationmodel. Consequently,
InFigure1,weseetheobtainedresults. Whenfo- the model tends to overlook the encoder during
cusingonthefirst10kupdateswefirstobservethat the early stages of training, focusing instead on
thethreestagesdescribedinSection2.2stillhap- languagemodeling.
peninthetexttranslationmodel. However,when
Overall,webelievethattheinitialstageoutlined
analysing both ST variants, we observe different
in Section 2.2 is not a result of the need to learn
behaviours.
language modeling. Rather, it’s a strategy to by-
Inthestandardsettingwithapretrainedencoder, pass the encoder information until the encoder is
weobserveatwo-stageprocess. Thismodelskips adequately trained. This process is quick in text
thefirststagedescribedinSection2.2,andrapidly translation,non-existentwhenusingapre-trained
integratessourcedatafromthebeginningoftrain- encoder in ST, and lengthy when training an ST
ing. Thisphenomenoniscoherent,astheencoder systemfromscratch.
hasbeenpretrained,resultinginhigh-qualityrepre-
Moreover,wethinkthatbythetimetheSTen-
sentations that areimmediately beneficial for the
codertrainedfromscratchbecomescapableofex-
decoderduringpredictiontasks. Asinthecaseof
tractingrelevantinformation,themodelhasalready
text translation, the last stage starts after the first
convergedtowardsrelyingonlanguagemodeling.
around6kupdates.
As a result, it never reaches the level of contribu-
3SetupdetailsoftheexperimentsareinAppendixA. tionachievedbythepretrainedmodel(asshownin
4WeusetranscriptsandtexttranslationsfortheMTmodel. Figure1),leadingtoinferiorperformance.Model Pretrained En-De En-Es En-Fr
Baseline Yes 22.4 27.3 32.3
LN
Baseline No 20.4 26.3 30.9
LN WeRC No 22.2 27.2 32.4
MHA
MHA WeRCw/onorm No 21.8 - -
WeRCw/oweights No 21.6 - -
LN LN
Table1: BLEU(↑)onMuST-Ctestsetaveragingthe
Transformer WeRC best10checkpoints.
Figure 2: Standard S2T-Transformer cross-attention
layer(left)andproposedWeRC(right).
x = λ·x +(1−λ)·x (1)
out attn res
However, a potential issue of this approach is
4 TrainingSTfromScratch
that the cross-attention block could converge to-
wardsproducingsmall-normvectors,sothatthey
Buildingonourpreviousanalysis,wehypothesize
wouldstillhaveasmallcontributionregardlessof
that forcing a Speech Translation model trained
the weighting. To solve this potential issue, we
fromscratchtoutilizesourceinformationfromthe
normalizeeachtermofthesummation(Eq. 2),by
start could enhance the training process. If the
addinglayernormalizationlayers(Baetal.,2016).
model is required to use the encoder’s represen-
Thisensuresbothtensorshavethesamenormbe-
tations, regardless of their quality, poor represen-
foretheweighting. Thereforetheywillcontribute
tationswillnegativelyimpactthemodel’soverall
tothesumwiththetargetproportion.5
performance. This,inturn,willcauseafastertrain-
ingoftheencodertoextractbetterrepresentations.
x = λ·LN(x )+(1−λ)·LN(x ) (2)
Inparticular,consideringtheresultsinFigure1, out attn res
weobservethatboththetexttranslationmodeland
ResultsinTable1showthatourmodel,which
thepretrainedspeechtranslationmodelachievea
incorporatesWeRCandistrainedfromscratch,out-
stablesourcecontributionofapproximately65%.
performsthebaselineby+1.3BLEUpoints. Addi-
Hence,weconsiderthisproportiontobeoptimal
tionally,itnearlyachievesthesameperformance
and aim to enforce it in the speech translation
asthemodelwithpretraining,whilereducingthe
modeltrainedfromscratch.
trainingtimebyskippingthepretrainingstage. Ad-
Totestourhypothesisweproposeasubtlearchi-
ditionally,weextendthisexperimenttoEn-Esand
tecturemodification,thatforcestheTransformerto
En-FrMuST-Csetsandobtainanalogousresults.
usesourceinformationalongthefulltraining. Our
modification focuses on the cross-attention layer 4.2 AblationStudy
ofthedecoder,whichisthestepwheresourceand
We perform an ablation study on the usefulness
target information is aggregated, with source in-
of the weighted sum and the layer normalization
formation coming from the attention block and
individually. InTable1weobservethatbothstrate-
target-prefixdetailscomingfromtheresidualflow.
giesachieveabetterperformancethanthebaseline
trainedfromscratch,buttheyarestillconsiderably
4.1 WeRC:WeightedResidualConnection
behindWeRCandthepretrainedbaseline. Inthe
Wemodifytheresidualconnectionaftereachcross- case of the variant without normalization, we be-
attentionblockinthedecoder. Thissumaggregates lievethisisasaresultofthetrainableparameters
theoutputofthecross-attentionblock(x )and intheattentionblock(asdescribedearlier). Inthe
attn
theresidualstream(x ). Ourgoalistoincrease caseofthemodelwithoutweights,webelievethis
res
the information flow coming from the source, so happensbecausethemodelisforcedtousea50%
we scale these two components giving a higher of source contribution, which is below the opti-
weighttotheoutputofthecross-attention(Eq. 1). mal(asobservedinFigure1). Additionalablation
Specifically, we aim to approximately match the
5Notethatweremovethelearnableparametersfromlayer
proportionofsourcecontributionfoundinSection
normalizationtoavoidanyscalingthatcouldaffecttheprede-
3.1,hencewesetλ = 0.65. finedweights.studiesregardingtheuseofWeRConaMTanda dynamics analysis, and hence we decided to use
pretrainedSTmodelscanbefoundinAppendixB. the same learning rate. Furthermore, this should
nothaveanimpactintheconclusionsofthepaper,
5 Conclusions
given that our proposed modification (WeRC) is
alsotrainedfromscratchandusesthesamelearn-
Inthiswork,wepresentthefirststudyonthetrain-
ingrate.
ing dynamics of direct ST systems, comparing a
standard ST model with a pretrained encoder to
onetrainedfromscratch. Theanalysisshowsthat, References
withoutpretraining,themodelstrugglestoincorpo-
SamiraAbnarandWillemZuidema.2020. Quantify-
rateinformationfromtheencoder’soutputswhen
ing attention flow in transformers. In Proceedings
making predictions. As an explanation, we sug-
of the 58th Annual Meeting of the Association for
gesttheencoderneedsmoreupdatesthaninatext Computational Linguistics, pages 4190–4197, On-
taskuntilitcanextractvaluablerepresentationsof line.AssociationforComputationalLinguistics.
theinputtokens. Oncethisisachieved,themodel
Belen Alastruey, Javier Ferrando, Gerard I. Gállego,
hasalreadyconvergedtowardslanguagemodeling, andMartaR.Costa-jussà.2022. Onthelocalityof
hencefailingtoutilizetheinformationextractedby attentionindirectspeechtranslation. InProceedings
of the 60th Annual Meeting of the Association for
theencodereffectivelyeveninlatersteps. Toad-
ComputationalLinguistics: StudentResearchWork-
dressthisissue,weproposeasubtlemodificationto
shop,pages402–412,Dublin,Ireland.Association
thetransformerarchitecturethatforcesthemodel forComputationalLinguistics.
toincorporatesourceinformationthroughoutthe
AshkanAlinejadandAnoopSarkar.2020. Effectively
wholetraining. Bydoingso,weachievecompara-
pretraining a speech translation decoder with ma-
bleperformanceinamodeltrainedfromscratchto
chine translation data. In Proceedings of the 2020
onewithpretraining,whilereducingtrainingtime Conference on Empirical Methods in Natural Lan-
anddatarequirements. guageProcessing(EMNLP),pages8014–8020,On-
line.AssociationforComputationalLinguistics.
Limitations
JimmyLeiBa,JamieRyanKiros,andGeoffreyEHin-
ton. 2016. Layer normalization. In Advances in
While our study provides valuable insights into
NeuralInformationProcessingSystems.
the training dynamics of direct ST systems and
proposesanovelapproachtoimprovetheefficiency SameerBansal,HermanKamper,KarenLivescu,Adam
Lopez, and Sharon Goldwater. 2019. Pre-training
ofthetrainingprocess,ourfindingsarebasedona
onhigh-resourcespeechrecognitionimproveslow-
specificmodel,datasetandlanguages. Webelieve
resourcespeech-to-texttranslation. InProceedings
differentresultscouldbeobtainedinothersettings, ofthe2019ConferenceoftheNorthAmericanChap-
suchaslowresourcespeechtranslation. teroftheAssociationforComputationalLinguistics:
HumanLanguageTechnologies,Volume1(Longand
Furthermore,ourpaperfocusesonaclassicand
ShortPapers),pages58–68,Minneapolis,Minnesota.
widely extended pretraining strategy. ASR and
AssociationforComputationalLinguistics.
STtrainingsetscorrespondtothesamedatasetand
havethesamesize,differingonlyinthelanguageof Alexandre Bérard, Laurent Besacier, Ali Can Ko-
cabiyikoglu,andOlivierPietquin.2018. End-to-end
thetargets. Wealsodon’tuseadditionaltechniques
automaticspeechtranslationofaudiobooks. In2018
suchasCTCauxiliaryloss. However,ourgoalin IEEEInternationalConferenceonAcoustics,Speech
thisworkisnotobtaininganewstate-of-the-artST andSignalProcessing(ICASSP),pages6224–6228.
trainingstrategybutanalysingandunderstanding
RoldanoCattoni,MattiaAntoninoDiGangi,LuisaBen-
acommontrainingstrategyusinginterpretability
tivogli,MatteoNegri,andMarcoTurchi.2021. Must-
tools, and performing additional experiments to c: Amultilingualcorpusforend-to-endspeechtrans-
validatethehypothesisextractedfromtheanalysis. lation. ComputerSpeech&Language,66:101155.
Finally, in our work we use the learning rate
Javier Ferrando, Gerard I. Gállego, Belen Alastruey,
defined by Wang et al. (2020a) for ST finetuning CarlosEscolano, andMartaR.Costa-jussà.2022a.
alsoontheexperimentstrainedfromscratch. We Towards opening the black box of neural machine
acknowledgethattheperformanceofexperiments translation: Sourceandtargetinterpretationsofthe
transformer. InProceedingsofthe2022Conference
trained from scratch could be pushed further by
onEmpiricalMethodsinNaturalLanguageProcess-
tuningthishyperparameter. However,wewanted
ing,pages8756–8769,AbuDhabi,UnitedArabEmi-
to keep experiments comparable for the training rates.AssociationforComputationalLinguistics.JavierFerrando,GerardI.Gállego,andMartaR.Costa- pages2619–2630,Online.AssociationforComputa-
jussà. 2022b. Measuring the mixing of contextual tionalLinguistics.
information in the transformer. In Proceedings of
the2022ConferenceonEmpiricalMethodsinNat-
uralLanguageProcessing,pages8698–8714,Abu BiaoZhang,BarryHaddow,andRicoSennrich.2022.
Dhabi,UnitedArabEmirates.AssociationforCom- Revisitingend-to-endspeech-to-texttranslationfrom
putationalLinguistics. scratch. In International Conference on Machine
Learning.
Diederik P. Kingma and Jimmy Ba. 2017. Adam: A
methodforstochasticoptimization.
A ExperimentalSetup
GoroKobayashi,TatsukiKuribayashi,ShoYokoi,and
KentaroInui.2020. Attentionisnotonlyaweight:
Analyzing transformers with vector norms. In STModelsDetails: Inourexperimentsweuse
Proceedings of the 2020 Conference on Empirical commonly used Fairseq6 Transformer and S2T-
MethodsinNaturalLanguageProcessing(EMNLP),
Transformerarchitectures. Inthecaseofspeech,it
pages7057–7075,Online.AssociationforComputa-
tionalLinguistics. consistsof12encoderlayersand6decoderlayers.
Boththeencoderandthedecoderuse4attention
Phuong-HangLe,HongyuGong,ChanghanWang,Juan
heads,theembeddingdimensionis256,andinthe
Pino,BenjaminLecouteux,andDidierSchwab.2023.
MLPblocksitis2048. Thedecoderoutputdimen-
Pre-trainingforspeechtranslation:Ctcmeetsoptimal
transport. sion is 256, the same as the decoder embedding
dimension. Themodelhaslayernormalizationbe-
Elizabeth Salesky, Marcello Federico, and Marine
foreitsmainblocksinsteadofafter,andadropout
Carpuat,editors.2023. Proceedingsofthe20thIn-
ternationalConferenceonSpokenLanguageTrans- of0.1isusedinboththeattentionweightsandin
lation(IWSLT2023).AssociationforComputational theMLPactivations. ReLUisusedastheactivation
Linguistics,Toronto,Canada(in-personandonline).
function for the MLP. Regarding text models we
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob have6encoderand6decoderlayers,nodropout,an
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz embeddingdimensionof512and8attentionheads
Kaiser,andIlliaPolosukhin.2017. Attentionisall
(othersettingsremainthesamethanforspeech).
youneed. InProceedingsofthe31stConferenceon
NeuralInformationProcessingSystems(NeurIPS),
pages5998–6008. Training Setup: In the case of speech transla-
tion and speech recognition trainings, we follow
ElenaVoita,RicoSennrich,andIvanTitov.2021. Lan-
guagemodeling,lexicaltranslation,reordering: The the setup defined by (Wang et al., 2020a). We
trainingprocessofNMTthroughthelensofclassi- fix a maximum of 20000 tokens per batch. We
cal SMT. In Proceedings of the 2021 Conference useAdamoptimizer(KingmaandBa,2017)anda
onEmpiricalMethodsinNaturalLanguageProcess-
learningrateof1·10−3withaninversesquareroot
ing,pages8478–8491,OnlineandPuntaCana,Do-
scheduler. Weapplyawarm-upforthefirst10000
minican Republic. Association for Computational
Linguistics. updates and we clip the gradient to 10 to avoid
explodinggradients. WeuselabelsmoothedCross-
Changhan Wang, Yun Tang, Xutai Ma, Anne Wu,
entropyloss,withasmoothingfactorof0.1. The
Dmytro Okhonko, and Juan Pino. 2020a. fairseq
s2t: Fastspeech-to-textmodelingwithfairseq. arXiv updatefrequencyissetto16,simulatingtheuseof
preprintarXiv:2010.05171. 16GPUs. Wetraineachmodelforamaximumof
100000updates. InSTtrainingsweusealearning
Chengyi Wang, Yu Wu, Shujie Liu, Ming Zhou, and
ZhengluYang.2020b. Curriculumpre-trainingfor rate of 2·10−3 while on speech recognition it is
end-to-endspeechtranslation. InProceedingsofthe 1·10−3,asdoneby(Wangetal.,2020a).
58thAnnualMeetingoftheAssociationforCompu-
In the text translation system, we again follow
tationalLinguistics,pages3728–3738,Online.Asso-
ciationforComputationalLinguistics. thesetupdefinedby(Wangetal.,2020a)forMa-
chine Translation. It is similar than the speech
ChenXu,BojieHu,YanyangLi,YuhaoZhang,Shen
translation one but the maximum number of to-
Huang, Qi Ju, Tong Xiao, and Jingbo Zhu. 2021.
kensperbatchislimitedto4096andthenumber
Stackedacoustic-and-textualencoding: Integrating
the pre-trained models into speech translation en- ofwarmupdatesis4000. Gradientclippingisre-
coders. In Proceedings of the 59th Annual Meet- movedandlearningrateissetto5·10−4.
ingoftheAssociationforComputationalLinguistics
andthe11thInternationalJointConferenceonNatu-
ralLanguageProcessing(Volume1: LongPapers), 6https://github.com/facebookresearch/fairseqModel BLEU
BaselineMT 31.6
WeRC 31.6
BaselinePretrainedST 21.8
WeRC 21.8
Table2: WeRCperformanceonMTandpretrainedST.
B ResultsonMachineTranslationand
SpeechTranslationwithPretraining
In this section, we aim to study the impact of us-
ingWeRContheanalysedMTsystemandonthe
ST training with a pretrained encoder. These set-
tingsachieveanoptimallevelofsourcecontribu-
tion from the first updates of the training, so we
hypothesize that WeRC might have a less notice-
able impact than in the main study of this paper
(STfromscratch). InTable27 weseetheobtained
results. Weobservethatbothsettingsmaintainthe
same performance, which is consistent with our
hypothesis.
7Notethatthisresultsareobtainedevaluatingonthebest
checkpointwithoutcheckpointaveraging.