Lotus: Diffusion-basedVisualFoundationModelforHigh-qualityDensePrediction
LOTUS: DIFFUSION-BASED VISUAL FOUNDATION MODEL
FOR HIGH-QUALITY DENSE PREDICTION
JingHe1‚ú±HaodongLi1‚ú±WeiYin2YixunLiang1LehengLi1KaiqiangZhou3
HongboZhang3BingbingLiu3YingcongChen1,4(cid:0)
1HKUST(GZ)2UniversityofAdelaide3HuaweiNoah‚ÄôsArkLab4HKUST
{jhe812, hli736}@connect.hkust-gz.edu.cn; yingcongchen@ust.hk
DepthAnything V2 Lotus (Ours)
DepthAnything V2 Lotus (Ours)
DSINE Lotus (Ours)
DSINE Lotus (Ours)
Avg. Rank Avg. Rank
Omnidata OASIS
DPT Omnidata
HDN EESNU
GenPercept GenPercept
DepthAnything V2 Omnidata V2
Lotus-D DSINE
DepthAnything Lotus-D
Avg. Rank Avg. Rank
GeoWizard Marigold
Marigold(LCM) GeoWizard
Marigold StableNormal
Lotus-G Lotus-G
Training Data Training Data
Figure1: WepresentLotus,adiffusion-basedvisualfoundationmodelfordensegeometrypredic-
tion.Withminimaltrainingdata,LotusachievesSoTAperformanceintwokeygeometryperception
tasks,i.e.,zero-shotdepthandnormalestimation. ‚ÄúAvg. Rank‚Äùindicatestheaveragerankingacross
allmetrics,wherelowervaluesarebetter. Barlengthrepresentstheamountoftrainingdataused.
‚ú±Bothauthorscontributedequally(orderrandomized).(cid:0)Correspondingauthor.
1
4202
peS
62
]VC.sc[
1v42181.9042:viXraLotus: Diffusion-basedVisualFoundationModelforHigh-qualityDensePrediction
ABSTRACT
Leveragingthevisualpriorsofpre-trainedtext-to-imagediffusionmodelsoffersa
promisingsolutiontoenhancezero-shotgeneralizationindensepredictiontasks.
However, existing methods often uncritically use the original diffusion formula-
tion,whichmaynotbeoptimalduetothefundamentaldifferencesbetweendense
predictionandimagegeneration. Inthispaper,weprovideasystemicanalysisof
the diffusion formulation for the dense prediction, focusing on both quality and
efficiency. And we find that the original parameterization type for image gener-
ation, which learns to predict noise, is harmful for dense prediction; the multi-
step noising/denoising diffusion process is also unnecessary and challenging to
optimize. Based on these insights, we introduce Lotus, a diffusion-based visual
foundationmodelwithasimpleyeteffectiveadaptationprotocolfordensepredic-
tion. Specifically,Lotusistrainedtodirectlypredictannotationsinsteadofnoise,
therebyavoidingharmfulvariance. Wealsoreformulatethediffusionprocessinto
asingle-stepprocedure,simplifyingoptimizationandsignificantlyboostinginfer-
encespeed. Additionally, weintroduceanoveltuningstrategycalleddetailpre-
server,whichachievesmoreaccurateandfine-grainedpredictions. Withoutscal-
ingupthetrainingdataormodelcapacity, LotusachievesSoTAperformancein
zero-shotdepthandnormalestimationacrossvariousdatasets.Italsosignificantly
enhancesefficiency, beinghundredsoftimesfasterthanmostexistingdiffusion-
basedmethods. Lotus‚Äôsuperiorqualityandefficiencyalsoenableawiderangeof
practicalapplications, suchasjointestimation, single/multi-view3Dreconstruc-
tion,etc. Projectpage: lotus3d.github.io.
1 INTRODUCTION
Densepredictionisafundamentaltaskincomputervision,benefitingawiderangeofapplications,
such as 3D/4D reconstruction [Huang et al. (2024); Long et al. (2024); Wang et al. (2024); Lei
etal.(2024)],tracking[Xiaoetal.(2024);Songetal.(2024)],andautonomousdriving[Yurtsever
etal.(2020);Huetal.(2023)]. Estimatingpixel-levelgeometricattributesfromasingleimagere-
quirescomprehensivesceneunderstanding. Althoughdeeplearninghasadvanceddenseprediction,
progressislimitedbythequality,diversity,andscaleoftrainingdata,leadingtopoorzero-shotgen-
eralization. Insteadofmerelyscalingdataandmodelsize,recentworks[Leeetal.(2024);Keetal.
(2024);Fuetal.(2024);Xuetal.(2024)]leveragediffusionpriorsforzero-shotdenseprediction.
Thesestudiesdemonstratethattext-to-imagediffusionmodelslikeStableDiffusion[Rombachetal.
(2022)],pretrainedonbillionsofimages,possesspowerfulandcomprehensivevisualpriorstoele-
vatedensepredictionperformance. However,mostofthesemethodsdirectlyinheritthepre-trained
diffusionmodelsfordensepredictiontasks,withoutexploringmoresuitablediffusionformulations.
Thisoversightoftenleadstochallengingissues. Forexample,Marigold[Keetal.(2024)]directly
fine-tunesStableDiffusionforimage-conditioneddepthgeneration. Whileitsignificantlyimproves
depth estimation, its performance is still constrained by overlooking the fundamental differences
betweendensepredictionandimagegeneration. Especially,itsefficiencyisalsoseverelylimitedby
standarditerativedenoisingprocessesandensembleinferences.
Motivatedbytheseconcerns, wesystematicallyanalyzethediffusion formulation, tryingto finda
better formulation to fit the pre-trained diffusion model into dense prediction. Our analysis yields
severalimportantfindings:‚ë†Thewidelyusedparameterization,i.e.,noiseprediction,fordiffusion-
based image generation is ill-suited for dense prediction. It results in large prediction errors due
to harmful prediction variance at initial denoising steps, which are subsequently propagated and
magnifiedthroughouttheentiredenoisingprocess(Sec.4.1). ‚ë°Multi-stepdiffusionformulationis
computation-intensive and is prone to sub-optimal with limited data and resources. These factors
significantlyhindertheadaptationofdiffusionpriorstodensepredictiontasks,leadingtodecreased
accuracyandefficiency(Sec.4.2). ‚ë¢Thoughremarkableperformanceachieved,weobservedthat
the model usually outputs vague predictions in highly-detailed areas (Fig. 8). This vagueness is
attributedtocatastrophicforgetting: thepre-traineddiffusionmodelsgraduallylosetheirabilityto
generatedetailedregionsduringfine-tuning(Sec.4.3).
2Lotus: Diffusion-basedVisualFoundationModelforHigh-qualityDensePrediction
single-step
ùë• -prediction
Image x ùê≥ùê± ùíõ$ùê± (
ùë°=T
Training Objective:
‚ùÑ
concat. ùê≥ùê±‚àíùëì ùê≥ùê≤,ùê≥ùê±,ùë°,ùë† &
üî• ! ùíï ‚Äô
‚Ñ∞ denoiser (image reconstruction)
( add ( U-Net ùëì ! ùê≥ùê≤‚àíùëì ùê≥+ ùê≤,ùê≥ùê±,ùë°,ùë† &
noise ! ùíï %
(predict annotation)
Annotation y ùê≥ùê≤ ùê≥ ùíïùê≤ switcher ùë† ùíõ$ùê≤
ùë°=T
detail preserver
single-step
Figure3: AdaptationprotocolofLotus. Afterthepre-trainedVAEencoderE encodestheimage
x and annotation y to the latent space: ‚ë† the denoiser U-Net model f is fine-tuned using x -
Œ∏ 0
prediction;‚ë°weemploysingle-stepdiffusionformulationattime-stept=T forbettercoverage;‚ë¢
weproposeanoveldetailpreserver,toswitchthemodeleithertoreconstructtheimageorgenerate
the dense prediction via a switcher s, ensuring a more fine-grained prediction. The noise zy in
T
bracketisusedforourgenerativeLotus-GandisomittedforthediscriminativeLotus-D.
Following our analysis, we propose Lo-
tus, a diffusion-based visual foundation
model for dense prediction, featuring a
simple yet effective fine-tuning protocol
(see Fig. 3). First, Lotus is trained to di-
rectly predict annotations, thereby avoid-
ing the harmful variance associated with
standardnoiseprediction. Next,weintro-
duceaone-stepformulation,i.e.,onestep
between pure noise and clean output, to
facilitate model convergence and achieve
betteroptimizationperformancewithlim-
ited high-quality data. It also consider-
ablyboostsbothtrainingandinferenceef-
ficiency. Moreover,weimplementanovel Figure 2: Inference time comparison in depth esti-
detail preserver through a task switcher, mation between Lotus and SoTA methods. Lotus
allowing the model either to generate an- is hundreds of times faster than Marigold and slightly
notations or reconstruct the input images. fasterthanDepthAnythingV2athighresolutions(Our
It preserves fine-grained details in the in- performance at high resolutions is also promising, as
put image during dense annotation gener- evidenced on the ETH3D dataset presented in Tab. 1
ation,achievinghigherperformancewith- and Fig. 11). DepthAnything V2‚Äôs inference time at
outcompromisingefficiency,requiringad-
2048√ó2048isnotplottedbecauseitrequires>18.0OGn SBingle A800
2. We keep the original shape of input
ditional parameters, or being affected by graphicmemory.
image during inference.
surfacetextures. 3. DAÂú®2048‰∏ãÁàÜÊòæÂ≠òÂï¶ÔºÅ
To validate Lotus, we conduct extensive experiments on two primary geometric dense predic-
tion tasks: zero-shot monocular depth and normal estimation. The results demonstrate that Lotus
achievesSoTAperformanceonthesetasksacrossawiderangeofevaluationdatasets. Comparedto
traditional discriminative methods, Lotus delivers remarkable results with only 59K training sam-
ples by effectively leveraging the powerful diffusion priors. Among generative approaches, Lotus
also outperforms previous methods in both accuracy and efficiency, being significantly faster than
methods like Marigold [Ke et al. (2024)] (Fig. 2). Beyond these improvements, Lotus seamlessly
supportsvariousapplications,suchasjointestimation,single/multi-view3Dreconstruction,etc.
Inconclusion,ourkeycontributionsareasfollows:
‚Ä¢ Wesystematicallyanalyzethediffusionformulationandfindtheirparameterizationtype,
designed for image generation, is unsuitable for dense prediction and the computation-
intensivemulti-stepdiffusionprocessisalsounnecessaryandchallengingtooptimize.
3Lotus: Diffusion-basedVisualFoundationModelforHigh-qualityDensePrediction
‚Ä¢ Weproposeanoveldetailpreserverthatensuresmoreaccuratedensepredictionsespecially
in detail-rich areas, without compromising efficiency, introducing additional network pa-
rameters,orbeingaffectedbysurfacetextures.
‚Ä¢ Basedonourinsights,weintroduceLotus,adiffusion-basedvisualfoundationmodelfor
densepredictionwithsimpleyeteffectivefine-tuningprotocol. LotusachievesSoTAper-
formanceonbothzero-shotmonoculardepthandsurfacenormalestimation.Italsoenables
awiderangeofapplications.
2 RELATED WORKS
2.1 TEXT-TO-IMAGEGENERATIVEMODELS
Inthefieldoftext-to-imagegeneration,theevolutionofmethodologieshastransitionedfromgen-
erative adversarial networks (GANs) [Goodfellow et al. (2014); Zhang et al. (2017; 2018; 2021);
He et al. (2022); Karras et al. (2019; 2020; 2021); Zhang et al. (2017; 2018); Xu et al. (2018);
Zhangetal.(2021)]toadvanceddiffusionmodels[Hoetal.(2020);Rameshetal.(2022);Saharia
etal.(2022);Rameshetal.(2021);Nicholetal.(2021);Chenetal.(2023);Rombachetal.(2022);
Rameshetal.(2021)]. Aseriesofdiffusion-basedmethodssuchasGLIDE[Nicholetal.(2021)],
DALL¬∑E2[Rameshetal.(2022)],andImagen[Sahariaetal.(2022)]havebeenintroduced,offering
enhancedimagequalityandtextualcoherence. TheStableDiffusion(SD)[Rombachetal.(2022)],
trainedonlarge-scaleLAION-5Bdataset[Schuhmannetal.(2022)],furtherenhancesthegenerative
quality,becomingthecommunitystandard. Inourpaper,weaimtoleveragethecomprehensiveand
encyclopedicvisualpriorsofSDtofacilitatezero-shotgeneralizationfordensepredictiontasks.
2.2 GENERATIVEMODELSFORDENSEPERCEPTION
Currently, a notable trend involves adopting pre-trained generative models, particularly diffusion
models,intodensepredictiontasks. Marigold[Keetal.(2024)]andGeoWizard[Fuetal.(2024)]
directly apply the standard diffusion formulation and the pre-trained parameters, without address-
ingtheinherentdifferencesbetweenimagegenerationanddenseprediction,leadingtoconstrained
performance. Theirefficiencyisalsoseverelylimitedbystandarditerativedenoisingprocessesand
ensembleinferences. Inthispaper,weproposeanoveldiffusionformulationtailoredtothecharac-
teristicsofdenseprediction. Aimingtofullyleveragingthepre-traineddiffusion‚Äôspowerfulvisual
priors,Lotusenablesmoreaccurateandefficientpredictions,finallyachievingSoTAperformance.
Morerecentworks,GenPercept[Xuetal.(2024)]andStableNormal[Yeetal.(2024)],alsoadopted
single-step diffusion. However, GenPercept [Xu et al. (2024)] first removes noise input for de-
terministic characteristic based on DMP [Lee et al. (2024)], and then adopts one-step strategy to
avoid surface texture interference. It lacks systematic analysis of the diffusion formulation, only
treatstheU-Netasadeterministicbackboneandstillfallsshortinperformance. Incontrast,Lotus
systematicallyanalyzesthestandardstochasticdiffusionformulationfordensepredictionandpro-
posesinnovationssuchasthedetailpreservertoimproveaccuracyespeciallyindetailedarea,finally
deliveringmuchbetterresults(Tab.1andFig.11).Additionally,Lotusisastochasticmodel,incon-
trasttoGenPercept‚Äôsdeterministicnature,enablinguncertaintypredictions. StableNormal[Yeetal.
(2024)] predicts normal maps through a two-stage process. While the first stage produces coarse
normal maps with single-step diffusion, the second stage performs refinement still with iterative
diffusionwhichiscomputation-intensive. Incomparison,Lotusnotonlyachievesfine-grainedpre-
dictions via the novel detail preserver without extra stages or parameters, but also delivers much
superior results (Tab. 2 and Fig. 12) thanks to our designed diffusion formulation that better fits
pre-traineddiffusionfordenseprediction.
2.3 MONOCULARDEPTHANDNORMALPREDICTION
Monoculardepthandnormalpredictionaretwocrucialdensepredictiontasks. Solvingthemtyp-
ically demands comprehensive scene understanding capability. Starting from Eigen et al. (2014),
early CNN-based methods for depth prediction, such as Fu et al. (2018), Lee et al. (2019), Yuan
et al. (2022), focus only on specific domains. Subsequently, in pursuit of a generic depth estima-
tor, many methods expand model capacity and train on larger and more diverse datasets, such as
4Lotus: Diffusion-basedVisualFoundationModelforHigh-qualityDensePrediction
Image x ùê≥ùê± multi-step
‚ùÑ ùë°‚àà[1,T]
concat ùúñ-prediction
.
‚Ñ∞ üî• denoiser Training Objective:
na od id se U-Net ùëì ! ùúñ ùúñ‚àíùëì ! ùê≥ ùíïùê≤,ùê≥ùê±,ùë° %
Annotation y
ùê≥ùê≤ ùê≥ ùíïùê≤ (predict noise)
ùë°‚àà[1,T]
multi-step
Figure4: AdaptationprotocolofDirectAdaptation. Startingwithapre-trainedStableDiffusion
model,imagexandannotationyareencodedusingthepre-trainedVAE.Thenoisyannotationzyis
t
obtainedbyaddingnoiseatlevelt ‚àà [1,T]. TheU-Netinputlayeriscoupledtoaccommodatethe
concatenatedinputsandthenfine-tunedusingthestandarddiffusionobjective, œµ-prediction, under
theoriginalmulti-stepformulation.
DiverseDepth[Yinetal.(2021a)]andMiDaS[Ranftletal.(2020)]. DPT[Ranftletal.(2021)]and
Omnidata [Eftekhar et al. (2021)] are further proposed based on vision transformer [Ranftl et al.
(2021)], significantly enhancing performance. LeRes [Yin et al. (2021b)] and HDN [Zhang et al.
(2022)] further introduce novel training strategies and multi-scale depth normalization to improve
predictionsindetailedareas. Morerecently, theDepthAnythingseries[Yangetal.(2024a;b)]and
Metric3D series [Yin et al. (2023); Hu et al. (2024)] collect and leverage millions of labeled data
todevelopmorepowerfulestimators. Normalpredictionfollowsthesametrend. Startingwiththe
early CNN-based methods like OASIS [Chen et al. (2020)], EESNU [Bae & Davison (2021)] and
Omnidataseries[Eftekharetal.(2021);Karetal.(2022)]expandthemodelcapacityandscaleup
thetrainingdata.Recently,DSINE[Bae&Davison(2024)]achievesSoTAperformancebyrethink-
inginductivebiasesforsurfacenormalestimation. Inourpaper,wefocusonleveragingpre-trained
diffusion priors to enhance zero-shot dense predictions, rather than expanding model capacity or
relyingonlargetrainingdata,whichavoidstheneedforintensiveresourcesandcomputation.
3 PRELIMINARIES
DiffusionFormulationforDensePrediction. FollowingKeetal.(2024)andFuetal.(2024),we
alsoformulatedensepredictionasanimage-conditionedannotationgenerationtaskbasedonStable
Diffusion[Rombachetal.(2022)],whichperformsthediffusionprocessinlow-dimensionallatent
space for computational efficiency. First, there is a pair of auto-encoders {E(¬∑),D(¬∑)} trained to
mapbetweenRGBspaceandlatentspace,i.e.,E(x)=zx,D(zx)‚âàx. Theauto-encoderalsomaps
betweendenseannotationsandlatentspaceeffectively,i.e.,E(y)=zy,D(zy)‚âày[Keetal.(2024);
Fu et al. (2024); Xu et al. (2024); Ye et al. (2024)]. Following Ho et al. (2020), Stable Diffusion
establishes a pair of forward nosing and reversal denoising processes in latent space. In forward
process, Gaussian noise is gradually added at levels t ‚àà [1,T] into sample zy to obtain the noisy
samplezy:
t ‚àö ‚àö
zy = Œ± zy+ 1‚àíŒ± œµ, (1)
t t t
whereœµ‚àºN(0,I),Œ±
:=(cid:81)t
(1‚àíŒ≤ ),and{Œ≤ ,Œ≤ ,...,Œ≤ }isthenoiseschedulewithT steps.
t s=1 s 1 2 T
Attime-stepT,thesamplezy isdegradedtopureGaussiannoise. Inthereversalprocess,aneural
networkf ,usuallyaU-Netmodel[Ronnebergeretal.(2015)),istrainedtoiterativelyremovenoise
Œ∏
fromzytopredictthecleansamplezy. Thenetworkistrainedbysamplingarandomt‚àà[1,T]and
t
minimizingthelossfunctionL .
t
ParameterizationTypes. Toenablegradientcomputationfornetworktraining,therearetwobasic
parameterizationsofthelossfunctionL . ‚ë†œµ-prediction[Hoetal.(2020)]: themodelf learnsto
t Œ∏
predicttheaddednoiseœµ;‚ë°x -prediction[Hoetal.(2020)]: themodelf learnstodirectlypredict
0 Œ∏
thecleansamplezy. Thelossfunctionsfortheseparameterizationsareformulatedas:
Lœµ =||œµ‚àífœµ(zy,zx,t)||2, (2)
t Œ∏ t
Lz =||zy‚àífz(zy,zx,t)||2, (3)
t Œ∏ t
5Lotus: Diffusion-basedVisualFoundationModelforHigh-qualityDensePrediction
ùúÄ-prediction, seed 1 ùúÄ-prediction, seed 2
ùë•!-prediction, seed 1 ùë•!-prediction, seed 2
Input Image
ùúè =1000 ùúè =600 ùúè =200 ùúè=1 ùúè =1000 ùúè =600 ùúè =200 ùúè=1
Figure5: Comparisonsamongdifferentparameterizationsusingvariousseeds. Allmodelsare
trainedonHypersim[Robertsetal.(2021)]andtestedontheinputimagefordepthestimation. The
standardDDIMsamplerisusedwith50denoisingsteps.Fourstepsareselectedforclearillustration.
Fromleft(largerœÑ)toright(smallerœÑ)istheiterativedenoisingprocess.
where f‚àó is the denoiser model to be learnt, ‚àó ‚àà {œµ,z}. œµ-prediction is commonly chosen as the
Œ∏
standardforparameterizingthedenoisingmodel,asitempiricallyachieveshigh-qualityimagegen-
erationwithfinedetailsandrealism.
DenoisingProcess. DDIM[Songetal.(2020)]isakeytechniqueformulti-stepdiffusionmodels
to achieve fast sampling, which implements an implicit probabilistic model that can significantly
reduce the number of denoising steps while maintaining output quality. Formally, the denoising
processfromzy tozy is:
œÑ œÑ‚àí1
zy =(cid:112) Œ± ÀÜzy+direction(zy)+œÉ œµ , (4)
œÑ‚àí1 œÑ‚àí1 œÑ œÑ œÑ œÑ
whereÀÜzyisthepredictedcleansampleatthedenoisingstepœÑ,direction(zy)representsthedirection
œÑ œÑ
pointingtozyandœÉ canbesetto0ifdeterministicinferenceisneeded. AndœÑ ‚àà{œÑ ,œÑ ,...,œÑ },
œÑ œÑ 1 2 S
anincreasingsub-sequenceofthetime-stepset[1,T],isusedforfastsampling. Duringinference,
DDIMiterativelydenoisesthesamplefromœÑ toœÑ toobtainthecleanone.
S 1
4 METHODOLOGY
Westartouranalysisbydirectlyadaptingtheoriginaldiffusionformulationwithminimalmodifica-
tionsasillustratedinFig.41. Wecallthisstartingpointas‚ÄúDirectAdaptation‚Äù. DirectAdaptation
isoptimizedusingthestandarddiffusionobjectiveasformulatedinEq.2andinferredbystandard
multi-stepDDIMsampler. AsshowninTab.3,DirectAdaptationfailstoachievesatisfactoryper-
formance.Infollowingsections,wewillsystematicallyanalyzethekeyfactorsthataffectadaptation
performance step by step: parameterization types (Sec. 4.1); number of time-steps (Sec. 4.2); and
thenoveldetailpreserver(Sec.4.3).
4.1 PARAMETERIZATIONTYPES
The type of parameterization is a vital configuration, it not only determines the loss function dis-
cussedinSec.3,butalsoinfluencestheinferenceprocess(Eq.4). Duringinference,thepredicted
cleansampleÀÜzy,akeycomponentinEq.4,iscalculatedaccordingtodifferentparameterizations2
œÑ
1 ‚àö
œµ-prediction:ÀÜzy = ‚àö (zy‚àí 1‚àíŒ± fœµ(zy,zx,œÑ))
œÑ Œ± œÑ œÑ œÑ Œ∏ œÑ (5)
x -prediction:ÀÜzy =fz(zy,zx,œÑ)
0 œÑ Œ∏ œÑ
In the community, œµ-prediction is chosen as the standard for image generation. However, it is not
effectivefordensepredictiontask. Inthefollowing,wewilldiscusstheimpactofdifferentparame-
terizationtypesindenoisinginferenceprocessfordensepredictiontask.
1Detailsof‚ÄúDirectAdaptation‚Äùwillbeprovidedinthesupplementarymaterials.
2Thelatestparameterization,v-prediction,combinesœµ-predictionandx -prediction,producingresultsthat
0
areintermediatebetweenthetwo.Pleaseseethesupplementarymaterialsformoredetails.
6Lotus: Diffusion-basedVisualFoundationModelforHigh-qualityDensePrediction
Insights from the literature [Benny & Wolf
NYUv2 AbsRel vs. Denoising Step
(2022); Salimans & Ho (2022)] reveal that
œµ-predictionintroduceslargerpixelvariance
12
P Pr re ed d.
.
T Ty yp pe e= =x0
comparedtox -prediction,especiallyatthe
0
initial denoising steps (large œÑ). This vari- 11
ance mainly originates from the noise in-
put. Specifically, for œµ-prediction in Eq. 5, 10
at initial denoising step, œÑ ‚Üí T, the value
9
‚àö1 ‚Üí +‚àû. Even small prediction vari-
Œ±œÑ
ance from fœµ(zy,zx,œÑ) will be amplified 8
Œ∏ œÑ
significantly, resulting in large variance of
1000 800 600 400 200 0
predictedÀÜzy. Incontrast,thereisnocoeffi- Denoising Step ( )
œÑ
cientforx -predictiontore-scalethemodel
0 Figure6: Quantitativeevaluationofthepredicted
output,achievingmorestablepredictionsof depth maps ÀÜzy along the denoising process. The
ÀÜzy at initial denoising steps. Subsequently, œÑ
œÑ experimental settings are same as Fig. 5. Six steps
the predicted ÀÜzy is used in Eq. 4. This it-
œÑ are selected for illustration. The banded regions
erative denoising process will preserve and
around each line indicate the variance, wider areas
amplify the influence of large variance. In
‚àö representinglargervariance.
Eq. 4, the coefficients Œ± of predicted
œÑ‚àí1
ÀÜzy are same across two parameterizations,
œÑ
and other terms are of the same order of magnitude. Therefore, theÀÜzy predicted by œµ-prediction,
œÑ
whichhaslargervariance,exertsamoresignificantinfluenceonthedenoisingprocess.
Wetakethedepthestimationasanexample.
During the inference process, we compute
NYUv2 AbsRel vs. Training Data
the predicted depth mapÀÜzy at each denois-
œÑ 11
ingstepœÑ. AsillustratedinFig.5,thedepth T'=1
T'=2
mapspredictedbyœµ-predictionsignificantly 10 T'=5
vary under different seeds while those pre- T'=10
dictedbyx -predictionaremoreconsistent. 9 T'=100 0
T'=1000
Althoughthelargevarianceenhancesdiver-
sity for image generation, it lead to unsta- 8
blepredictionsindensepredictiontasks,po-
7
tentially resulting in significant errors. For
example in Fig. 5, the ‚Äúdark gray cabinet‚Äù
6
(highlighted in red circles) maybe wrongly
5K 10K 19K 39K
considered as an ‚Äúopened door‚Äù with sig- Training Data
nificantly larger depth. While the predicted
Figure 7: Comparisons among various training
depth map looks more and more ‚Äúplausi-
time-steps and data scales evaluated on NYUv2
ble‚Äù,theerrorgraduallypropagatestothefi-
in depth estimation. All models are fine-tuned on
nal prediction (œÑ = 1) along the denoising
Hypersim using x -prediction. During inference, if
process, indicating the persistent influence 0
T‚Ä≤ > 50,theDDIMsamplerisusedwith50denois-
of the large variance. We further quantita-
ing steps; otherwise, the number of denoising steps
tively measure the predicted depth maps by
is equal to T‚Ä≤. The results demonstrate improved
theabsolutemeanrelativeerror(AbsRel)on
performancewithdecreasedtrainingtime-steps. The
NYUv2dataset[Silbermanetal.(2012)].As
single-step diffusion formulation (T‚Ä≤ = 1) exhibits
showninFig.6,œµ-predictionexhibitshigher
bestperformanceacrossdifferentdatavolumes.
errorwithmuchlargervariancecomparedto
x -prediction at the initial denoising steps
0
(œÑ ‚Üí T), and the prediction error propagates along the denoising process with higher slope. In
contrast, x -prediction, directly predicting ÀÜzy without any coefficients to amplify the prediction
0 œÑ
variance,yieldsmorestableandcorrectdensepredictionsthanœµ-prediction.
In conclusion, to mitigate the errors from large variance that adversely affect the performance of
denseprediction,wereplacethestandardœµ-predictionwiththemoretailoredx -prediction.
0
7
)%(
leRsbA
2vUYN
)%(
leRsbA
2vUYNLotus: Diffusion-basedVisualFoundationModelforHigh-qualityDensePrediction
Input Image Reconstruction Input Image Reconstruction
w/o Preserver w/ Preserver w/o Preserver w/ Preserver
Figure8:Depthmapsw/andw/othedetailpreserverandreconstructionoutputs. Fine-tuning
thediffusionmodelfordensepredictiontaskscanpotentiallydegradeitsabilitytogeneratehighly
detailedimages,resultinginblurredpredictionsinregionswithrichdetail. Topreservethesefine-
graineddetails, weintroduceadetailpreserverthatincorporatesanadditionalreconstructiontask,
enhancingthemodel‚Äôscapacitytoproducemoreaccuratedenseannotations.
4.2 NUMBEROFTIME-STEPS
Althoughx -predictioncanimprovethepredictionquality,themulti-stepdiffusionformulationstill
0
leadstothepropagationofpredictederrorsduringthedenoisingprocess(Fig.5,6). Furthermore,
utilizingmultipletime-stepsenhancesthemodel‚Äôscapacity,typicallyrequiringlarge-scaletraining
datatooptimizeandisbeneficial‚Äîorevennecessary‚Äîforcomplextaskssuchasimagegeneration.
However,forsimplertaskslikedenseprediction,wherelarge-scale,high-qualitytrainingdataisalso
scarce,employingmultipletime-stepscanmakethemodeldifficulttooptimize. Additionally,train-
ing/inferringamulti-stepdiffusionmodelisslowandcomputation-intensive,hinderingitspractical
application.
Therefore,toaddressthesechallenges,weproposefine-tuningthepre-traineddiffusionmodelwith
fewertrainingtimesteps. Specifically,theoriginalsetoftrainingtime-stepsisdefinedas[1,T] =
{1,2,3,...,T},whereT denotesthetotalnumberoforiginaltrainingtime-steps. Wefine-tunethe
pre-traineddiffusionmodelusingasub-sequencederivedfromthisset. Wedefinethelengthofthis
sub-sequence as T‚Ä≤, where T‚Ä≤ ‚â™ T and T is divisible by T‚Ä≤. This sub-sequence is obtained by
evenlysamplingtheoriginalsetatintervals,definedas:
{t =i¬∑k |i=1,2,...,T‚Ä≤}, (6)
i
wherek = T/T‚Ä≤ isthesamplinginterval. Duringinference, theDDIMdenoisesthesamplefrom
noisetoannotationusingthesamesub-sequence.
As illustrated in Fig. 7, we conduct experiments by varying the number of time-steps T‚Ä≤ under
x -prediction. Theresultsclearlyshowthattheperformancegraduallyimprovesasthenumberof
0
time-steps is reduced, no matter the training data scales, culminating in the best result when re-
duced to only a single step. We further consider more strict scenarios with more limited training
data to assess its impact on model optimization. As depicted in Fig. 7, these experiments reveal
thatthemulti-stepformulationismoresensitivetoincreasesintrainingdatascalescomparedwith
single-step. Notably, the single-step formulation consistently yields lower prediction errors and
demonstrates greater stability. Although it is conceivable that multi-step and single-step formula-
tions might achieve comparable performance with unlimited high-quality data, it‚Äôs expensive and
sometimesimpracticalindenseprediction.
Decreasingthenumberofdenoisingstepscanreducetheoptimizationspaceofthediffusionmodel,
leading to more effective and efficient adaption, as suggested by the above phenomenon. There-
fore, for better adaptation performance under limited resource, we reduce the number of training
time-steps of diffusion formulation to only one, and fixing the only time-step t to T. Addition-
ally, thesingle-stepformulationismuchmorecomputationallyefficient. Italsonaturallyprevents
theharmfulerrorpropagationasdiscussedinSec.4.1,furtherenhancingthediffusion‚Äôsadaptation
performanceindenseprediction.
8InputImage Seed 0 Seed 1 Seed 2 Seed 3 Uncertainty Map
Lotus: Diffusion-basedVisualFoundationModelforHigh-qualityDensePrediction
InputImage Seed 0 Seed 1 Seed 2 Seed 3 Uncertainty Map
Figure 9: Depth maps of multiple inferences and uncertainty maps. Areas like the sky, object
edges,andintricatedetails(e.g.,catwhiskers)typicallyexhibithighuncertainty.
4.3 DETAILPRESERVER
Despite the effectiveness of the above designs, the model still struggles with processing detailed
areas (Fig. 8, w/o Preserver). The original diffusion model excels at generating detailed images.
However,whenadaptedtopredictdenseannotations,itcanlosesuchdetailedgenerationability,due
tounexpectedcatastrophicforgetting[Zhaietal.(2023);Duetal.(2024)]. Thisleadstochallenges
inpredictingdenseannotationsinintricateregions.
Topreservetherichdetailsoftheinputimages,weintroduceanovelregularizationstrategycalled
DetailPreserver. Inspiredbypreviousworks[Longetal.(2024);Fuetal.(2024)],weutilizeatask
switchers ‚àà {s ,s },enablingthedenoisermodelf toeithergenerateannotationorreconstruct
x y Œ∏
the input image. When activated by s , the model focuses on predicting annotation. Conversely,
y
when s is selected, it reconstructs the input image. The switcher s is a one-dimensional vector
x
encoded by the positional encoder and appended with the time embeddings of diffusion model,
ensuringseamlessdomainswitchingwithoutmutualinterference. Thisdualcapabilityenablesthe
diffusion model to make detailed predictions and thus leading to better performance. Overall, the
lossfunctionL is:
t
L =||zx‚àíf (zy,zx,t,s )||2+||zy‚àíf (zy,zx,t,s )||2, (7)
t Œ∏ t x Œ∏ t y
wheret=T andthuszy isapureGaussiannoise.
t
4.4 STOCHASTICNATUREOFDIFFUSIONMODEL
One major characteristic of generative models is
their stochastic nature, which, in image generation, ùê≥ùê≤
enables the production of diverse outputs. In per- ùëª
( (
ceptiontaskslikedenseprediction,thisstochasticity Imagex
denoiser
hasthepotentialtoallowthemodelgeneratingpre- U-Net ùëì!
dictionswithuncertaintymaps. Specifically,forany ‚Ñ∞
inputimage,wecanconductmultipleinferencesus-
ùê≥ùê±
ingdifferentinitializationnoisesandaggregatethese ùë°=T switcher ùë†"
predictions to create its uncertainty map. Thanks ùíü
to our systematic analysis and tailored fine-tuning
ùíõ#ùê≤
protocol, our method effectively reduces excessive Prediction
flickering (large variance), only allowing for more
Figure10:InferencePipelineofLotus.The
accurateuncertaintycalculationsinnaturallyuncer- noisezy inbracketisusedforLotus-Gand
tain areas, such as the sky, object edges, and fine T
omittedforLotus-D.
details(e.g. catwhiskers),asshowninFig. 9.
Most existing perception models are deterministic.
Toalignwiththese,wecanremovethenoiseinputzyandonlyinputtheencodedimagefeatureszx
t
totheU-Netdenoiser. Themodelstillperformswell. Inthispaper,wefinallypresenttwoversions
9
.tacnocLotus: Diffusion-basedVisualFoundationModelforHigh-qualityDensePrediction
ofLotus: Lotus-G(generative)withnoiseinputandLotus-D(discriminative)withoutnoiseinput,
cateringtodifferentneeds.
4.5 INFERENCE
The inference pipeline is illustrated in Fig. 10. We initialize the annotation map with standard
Gaussiannoisezy,andencodetheinputimageintoitslatentcodezx. Thenoisezy andtheimage
T T
zx are concatenated and fed into the denoiser U-Net model. In our single-step formulation, we
set t = T and the switcher to s . The denoiser U-Net model then predicts the latent code of the
y
annotation map. The final annotation map is decoded from the predicted latent code via the VAE
decoder. Fordeterministicprediction,weeliminatetheGaussiannoisezy andonlyfeedthelatent
T
codeoftheinputimageintothedenoiser.
5 EXPERIMENTS
5.1 EXPERIMENTALSETTINGS
Implementation details. We implement Lotus based on Stable Diffusion V2 [Rombach et al.
(2022)],withtextconditioningdisabled.Duringtraining,wefixthetime-stept=1000.Tooptimize
themodel,weutilizethestandardAdamoptimizerwiththelearningrate3√ó10‚àí5. Allexperiments
areconductedon8NVIDIAA800GPUsandthetotalbatchsizeis128. Forourdiscriminativevari-
ant, we train for 4,000 steps, which takes ‚àº8.1 hours, while for the generative variant, we extend
trainingto10,000steps,requiring‚àº20.3hours.
TrainingDatasets. Bothdepthandnormalestimationaretrainedontwosyntheticdatasetcovering
indoorandoutdoorscenes.
‚ë†Hypersim[Robertsetal.(2021)]isaphotorealisticsyntheticdatasetfeaturing461indoorscenes.
We use the official training split, which contains approximately 54K samples. After filtering out
incompletesamples,around39Ksamplesremain,allresizedto576√ó768fortraining.
‚ë°VirtualKITTI[Cabonetal.(2020)]isasyntheticstreet-scenedatasetwithfiveurbanscenesunder
various imaging and weather conditions. We utilize four of these scenes for training, comprising
about20Ksamples. Allsamplesarecroppedto352√ó1216,withthefarplanesetat80meters.
Following Marigold [Ke et al. (2024)], we employ a mixed dataset strategy for training. For each
batch,weprobabilisticallychooseoneofthetwodatasetsandthendrawsamplesfromit(Hypersim
90%andVirtualKITTI 10%). Thisapproachyieldsbetterperformanceonbothindoorandoutdoor
realdatasetscomparedtotrainingonasinglesyntheticdataset.
EvaluationDatasets. ‚ë†Foraffine-invariantdepthestimation,weevaluateon4real-worlddatasets
thatarenotseenduringtraining: NYUv2[Silbermanetal.(2012))andScanNet[Daietal.(2017))
allcontainimagesofindoorscenes; KITTI[Geigeretal.(2013))containsvariousoutdoorscenes;
ETH3D[Schopsetal.(2017)),ahigh-resolutiondataset,containingbothindoorandoutdoorscenes.
‚ë° For surface normal prediction, we employ 4 datasets for evaluation: NYUv2 [Silberman et al.
(2012)), ScanNet [Dai et al. (2017)), and iBims-1 [Koch et al. (2018)) contain real indoor scenes;
Sintel[Butleretal.(2012))containshighlydynamicoutdoorscenes.
Metrics. ‚ë†Foraffine-invariantdepth,wefollowtheevaluationprotocolfrom[Ranftletal.(2020);
Ke et al. (2024); Yang et al. (2024a;b)], aligning the estimated depth predictions with available
groundtruthsusingleast-squaresfitting. Theaccuracyofthealignedpredictionsisassessedusing
theabsolutemeanrelativeerror(AbsRel),i.e., 1 (cid:80)M |a ‚àíd |/d ,whereM isthetotalnumber
M i=1 i i i
ofpixels, a isthepredicted depthmapand d representstheground truth. We alsoreportŒ¥1 and
i i
Œ¥2,theproportionofpixelssatisfyingMax(a /d ,d /a )<1.25and<1.252respectively.
i i i i
‚ë°Forsurfacenormal,following[Bae&Davison(2024);Yeetal.(2024)],weevaluatethepredic-
tionsofLotusbymeasuringthemeanangularerrorforpixelswithavailablegroundtruth. Addition-
ally,wereportthepercentageofpixelswithanangularerrorbelow11.25‚ó¶and30‚ó¶.
‚ë¢Foralltasks,wereporttheAvg. Rank,whichindicatestheaveragerankingofeachmethodacross
variousdatasetsandevaluationmetrics. Alowervaluesignifiesbetteroverallperformance.
10Lotus: Diffusion-basedVisualFoundationModelforHigh-qualityDensePrediction
Table1:Quantitativecomparisononzero-shotaffine-invariantdepthestimationbetweenLotus
andSoTAmethods. Theuppersectionlistsdiscriminativemethods,thelowerlistsgenerativeones.
The best and secondbest performancesarehighlighted. Lotus-Goutperformsallothersmethods
whileLotus-DisslightlyinferiortoDepthAnything. PleasenotethatDepthAnythingistrainedon
62.6MimageswhileLotusisonlytrainedon0.059Mimages.¬ßindicatesresultsrevisedbyourselves.
‚ãÜdenotesthemethodreliesonpre-trainedStableDiffusion.
Training NYUv2(Indoor) KITTI(Outdoor) ETH3D(Various) ScanNet(Indoor) Avg.
Method
Data AbsRel‚Üì Œ¥1‚Üë Œ¥2‚Üë AbsRel‚Üì Œ¥1‚Üë Œ¥2‚Üë AbsRel‚Üì Œ¥1‚Üë Œ¥2‚Üë AbsRel‚Üì Œ¥1‚Üë Œ¥2‚Üë Rank
DiverseDepth 320K 11.7 87.5 - 19.0 70.4 - 22.8 69.4 - 10.9 88.2 - 9.5
MiDaS 2M 11.1 88.5 - 23.6 63.0 - 18.4 75.2 - 12.1 84.6 - 9.5
LeRes 354K 9.0 91.6 - 14.9 78.4 - 17.1 77.7 - 9.1 91.7 - 7.6
Omnidata 12.2M 7.4 94.5 - 14.9 83.5 - 16.6 77.8 - 7.5 93.6 - 6.4
DPT 1.4M 9.8 90.3 - 10.0 90.1 - 7.8 94.6 - 8.2 93.4 - 5.5
HDN 300K 6.9 94.8 - 11.5 86.7 - 12.1 83.3 - 8.0 93.9 - 5.0
GenPercept‚ãÜ¬ß
74K 5.6 96.0 99.2 13.0 84.2 - 7.0 95.6 98.8 6.2 96.1 99.1 3.8
DepthAnythingV2 62.6M 4.5 97.9 99.3 7.4 94.6 98.6 13.1 86.5 - 4.2 97.8 99.3 2.9
Lotus-D(Ours)‚ãÜ 59K 5.3 96.7 99.2 9.3 92.8 98.8 6.8 95.3 98.9 6.0 96.3 99.1 2.5
DepthAnything 62.6M 4.3 98.1 99.6 7.6 94.7 99.2 12.7 88.2 - 4.3 98.1 99.6 2.0
GeoWizard‚ãÜ¬ß
280K 5.6 96.3 99.1 14.4 82.0 96.6 6.6 95.8 98.4 6.4 95.0 98.4 3.3
Marigold(LCM)¬ß‚ãÜ 74K 6.1 95.8 99.0 9.8 91.8 98.7 6.8 95.6 99.0 6.9 94.6 98.6 2.9
Marigold‚ãÜ 74K 5.5 96.4 99.1 9.9 91.6 98.7 6.5 95.9 99.0 6.4 95.2 98.8 1.8
Lotus-G(Ours)‚ãÜ 59K 5.4 96.6 99.2 11.3 87.7 97.8 6.2 96.1 99.0 6.0 96.0 99.0 1.5
Table 2: Quantitative comparison on zero-shot surface normal estimation between Lotus and
SoTAmethods. Discriminativemethodsareshownintheuppersection,generativemethodsinthe
lower. Both Lotus-D and Lotus-G outperform all other methods. ‚Ä°refers the Marigold normal
modelasdetailedinthislink. ‚ãÜdenotesthemethodreliesonpre-trainedStableDiffusion.
Training NYUv2(Indoor) ScanNet(Indoor) iBims-1(Indoor) Sintel(Outdoor) Avg.
Method Data m.‚Üì 11.25‚ó¶‚Üë 30‚ó¶‚Üë m.‚Üì 11.25‚ó¶‚Üë 30‚ó¶‚Üë m.‚Üì 11.25‚ó¶‚Üë 30‚ó¶‚Üë m.‚Üì 11.25‚ó¶‚Üë 30‚ó¶‚Üë Rank
OASIS 110K 29.2 23.8 60.7 32.8 15.4 52.6 32.6 23.5 57.4 43.1 7.0 35.7 7.0
Omnidata 12.2M 23.1 45.8 73.6 22.9 47.4 73.2 19.0 62.1 80.1 41.5 11.4 42.0 5.3
EESNU 2.5M 16.2 58.6 83.5 - - - 20.0 58.5 78.2 42.1 11.5 41.2 4.4
GenPercept‚ãÜ 74K 18.2 56.3 81.4 17.7 58.3 82.7 18.2 64.0 82.0 37.6 16.2 51.0 3.7
OmnidataV2 12.2M 17.2 55.5 83.0 16.2 60.2 84.7 18.2 63.9 81.1 40.5 14.7 43.5 3.6
DSINE 160K 16.4 59.6 83.5 16.2 61.0 84.4 17.1 67.4 82.3 34.9 21.5 52.7 1.8
Lotus-D(Ours)‚ãÜ 59K 16.8 58.2 83.6 15.3 62.9 85.7 17.7 64.9 82.5 34.6 20.5 55.8 1.6
Marigold‚Ä°‚ãÜ 74K 20.9 50.5 - 21.3 45.6 - 18.5 64.7 - - - - 4.2
GeoWizard‚ãÜ 280K 18.9 50.7 81.5 17.4 53.8 83.5 19.3 63.0 80.3 40.3 12.3 43.5 3.2
StableNormal‚ãÜ 250K 18.6 53.5 81.7 17.1 57.4 84.1 18.2 65.0 82.4 36.7 14.1 50.7 2.0
Lotus-G(Ours)‚àó 59K 16.9 59.1 83.2 15.3 64.0 85.2 17.5 66.1 82.7 35.2 19.9 54.8 1.0
5.2 QUALITATIVEANDQUANTITATIVECOMPARISONS
Depth Estimation. As shown in Tab. 1, Lotus-G achieves the best comprehensive performance
comparedtoallgenerativebaselinesonzero-shotaffine-invariantdepthestimation. Noticethatwe
only require single step denoising process, significantly boosting the inference speed as shown in
Table3:Ablationstudiesonthestep-by-stepdesignofouradaptationprotocolforfittingpre-trained
diffusionmodelsintodenseprediction. Hereweshowtheresultsinmonoculardepthestimation.
Training NYUv2(Indoor) KITTI(Outdoor) ETH3D(Various) ScanNet(Indoor)
Method
Data AbsRel‚Üì Œ¥1‚Üë Œ¥2‚Üë AbsRel‚Üì Œ¥1‚Üë Œ¥2‚Üë AbsRel‚Üì Œ¥1‚Üë Œ¥2‚Üë AbsRel‚Üì Œ¥1‚Üë Œ¥2‚Üë
DirectAdaptation 39K 11.551 87.69296.122 20.164 70.40390.996 19.894 76.46487.960 15.726 78.88593.651
+x0-prediction 39K 8.332 92.76997.941 17.008 74.96993.611 11.075 87.95294.978 10.212 89.13097.181
+SingleTime-step 39K 5.587 96.27299.113 13.262 83.21097.237 7.586 94.14397.678 6.262 95.39498.791
+DetailPreserver 39K 5.555 96.30399.118 13.170 83.65797.454 7.147 95.00098.058 6.201 95.47098.814
+MixtureDataset(Lotus-G) 59K 5.425 96.59799.156 11.324 87.69297.780 6.172 96.07798.980 6.024 96.02699.730
‚àíNoiseInput(Lotus-D) 59K 5.311 96.73399.186 9.662 91.63798.643 6.757 95.38298.992 5.786 96.33999.136
11Lotus: Diffusion-basedVisualFoundationModelforHigh-qualityDensePrediction
Input Image DepthAnythingV2 Marigold Lotus-D Lotus-G Ground Truth
(LCM)
Figure 11: Qualitative comparison on zero-shot affine-invariant depth estimation. Lotus
demonstrateshigheraccuracyespeciallyindetailedareas.
Input Image DSINE StableNormal Lotus-D Lotus-G Ground Truth
Figure 12: Qualitative comparison on zero-shot surface normal estimation. Lotus offers im-
provedaccuracyparticularlyincomplexregions.
Fig. 2. Lotus-D also performs well, though it is slightly inferior to DepthAnything. However,
it is worthy to notice that Lotus is trained on only 0.059M images compared to DepthAnything‚Äôs
62.6Mimages. InFig. 11, wefurthercomparetheperformanceofourLotuswithothermethods
indetailedareas. Thequantitativeresultsobviouslydemonstratethatourmethodcanproducemuch
finerandmoreaccuratedepthpredictions,particularlyincomplexregionswithintricatestructures,
whichsometimescannotbereflectedbythemetrics.
NormalEstimation. InTab.2,bothLotus-GandLotus-Doutperformallothermethodsonzero-
shot surface normal estimation. Also, as illustrated in Fig. 12, Lotus consistently provides accu-
ratesurfacenormalpredictions,effectivelyhandlingcomplexgeometriesanddiverseenvironments,
highlightingitsrobustnessonfine-grainedprediction.
12
2vUYN
ITTIK
teNnacS
D3HTE
2vUYN
teNnacS
1-smiBi
letniSLotus: Diffusion-basedVisualFoundationModelforHigh-qualityDensePrediction
5.3 ABLATIONSTUDY
As shown in Tab. 3, we conduct ablation studies to validate our designs. Starting with ‚ÄúDirect
Adaptation‚Äù, we incrementally test the effects of different components, such as parameterization
types,thesingle-stepdiffusionprocess,andthedetailpreserver. Initially,wetrainthemodelusing
only the Hypersim dataset to establish a baseline. We then expand the training dataset using a
mixturedatasetstrategybyincludingVirtualKITTI,aimingtoenhancethemodel‚Äôsgeneralization
abilityacrossdifferentdomains. Thefindingsfromtheseablationsvalidatetheeffectivenessofour
proposed adaptation protocol, demonstrating that each design plays a vital role in optimizing the
diffusionmodelsfordensepredictiontasks.
6 CONCLUSION AND FUTURE WORKS
In this paper, we introduce Lotus, a diffusion-based visual foundation model for dense prediction.
Through systematic analysis and specifically tailored diffusion formulation, Lotus finds a way to
better fit the rich visual prior from pre-trained diffusion models into dense prediction. Extensive
experiments demonstrate that Lotus achieves SoTA performance on zero-shot depth and normal
estimationwithminimaltrainingdata,pavingthewayofvariouspracticalapplications.
Future Work. While we have applied Lotus to two geometric dense prediction tasks, it can be
seamlesslyadaptedtootherdensepredictiontasksrequiringper-pixelalignmentwithgreatpoten-
tial,suchaspanoramicsegmentationandimagematting. Additionally,ourperformanceisslightly
behindDepthAnything[Yangetal.(2024a)]whichutilizeslarge-scaletrainingdata. Inthefuture,
scalingupthetrainingdata,asrevealinFig.7andTab.3(‚ÄúMixtureDataset‚Äù),hasgreatpotentialto
furtherenhanceLotus‚Äôsperformance.
REFERENCES
GilwonBaeandAndrewJDavison. Aleatoricuncertaintyinmonocularsurfacenormalestimation.
IEEETransactionsonPatternAnalysisandMachineIntelligence(TPAMI),pp.1472‚Äì1485,2021.
Gilwon Bae and Andrew J Davison. Rethinking inductive biases for surface normal estimation.
IEEEConferenceonComputerVisionandPatternRecognition(CVPR),2024.
Yaniv Benny and Lior Wolf. Dynamic dual-output diffusion models. In Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.11482‚Äì11491,2022.
Daniel J Butler, Jonas Wulff, Garrett B Stanley, and Michael J Black. A naturalistic open source
movieforopticalflowevaluation. InComputerVision‚ÄìECCV2012: 12thEuropeanConference
onComputerVision,Florence,Italy,October7-13,2012,Proceedings,PartVI12,pp.611‚Äì625.
Springer,2012.
Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2. arXiv preprint
arXiv:2001.10773,2020.
JunsongChen,JinchengYu,ChongjianGe,LeweiYao,EnzeXie,YueWu,ZhongdaoWang,James
Kwok,PingLuo,HuchuanLu,etal. Pixart-Œ±: Fasttrainingofdiffusiontransformerforphotore-
alistictext-to-imagesynthesis. arXivpreprintarXiv:2310.00426,2023.
WeifengChen,ShengyiQian,DavidFan,NoriyukiKojima,MaxHamilton,andJiaDeng. Oasis: A
large-scaledatasetforsingleimage3dinthewild. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pp.679‚Äì688,2020.
Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias
Nie√üner. Scannet: Richly-annotated3dreconstructionsofindoorscenes. InProceedingsofthe
IEEEconferenceoncomputervisionandpatternrecognition,pp.5828‚Äì5839,2017.
Wenyu Du, Shuang Cheng, Tongxu Luo, Zihan Qiu, Zeyu Huang, Ka Chun Cheung, Reynold
Cheng, and Jie Fu. Unlocking continual learning abilities in language models. arXiv preprint
arXiv:2406.17245,2024.
13Lotus: Diffusion-basedVisualFoundationModelforHigh-qualityDensePrediction
Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. Omnidata: A scalable pipeline
formakingmulti-taskmid-levelvisiondatasetsfrom3dscans. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision,pp.10786‚Äì10796,2021.
DavidEigen,ChristianPuhrsch,andRobFergus. Depthmappredictionfromasingleimageusing
amulti-scaledeepnetwork. Advancesinneuralinformationprocessingsystems,27,2014.
HuanFu,MingmingGong,ChaohuiWang,KayhanBatmanghelich,andDachengTao.Deepordinal
regression network for monocular depth estimation. In Proceedings of the IEEE conference on
computervisionandpatternrecognition,pp.2002‚Äì2011,2018.
Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and
XiaoxiaoLong. Geowizard: Unleashingthediffusionpriorsfor3dgeometryestimationfroma
singleimage. arXivpreprintarXiv:2403.12013,2024.
Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The
kittidataset. TheInternationalJournalofRoboticsResearch,32(11):1231‚Äì1237,2013.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
AaronCourville,andYoshuaBengio.Generativeadversarialnets.Advancesinneuralinformation
processingsystems,27,2014.
JingHe,YiyiZhou,QiZhang,JunPeng,YunhangShen,XiaoshuaiSun,ChaoChen,andRongrong
Ji. Pixelfolder: An efficient progressive pixel synthesis network for image generation. arXiv
preprintarXiv:2204.00833,2022.
JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advancesin
neuralinformationprocessingsystems,33:6840‚Äì6851,2020.
Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang
Yu, Chunhua Shen, and Shaojie Shen. Metric3d v2: A versatile monocular geometric
foundation model for zero-shot metric depth and surface normal estimation. arXiv preprint
arXiv:2404.15506,2024.
Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du,
TianweiLin,WenhaiWang,etal. Planning-orientedautonomousdriving. InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.17853‚Äì17862,2023.
BinbinHuang,ZehaoYu,AnpeiChen,AndreasGeiger,andShenghuaGao. 2dgaussiansplatting
forgeometricallyaccurateradiancefields. InSIGGRAPH2024ConferencePapers.Association
forComputingMachinery,2024. doi: 10.1145/3641519.3657428.
OgÀòuzhanFatihKar,TeresaYeo,AndreiAtanov,andAmirZamir. 3dcommoncorruptionsanddata
augmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pp.18963‚Äì18974,2022.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarialnetworks.InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
recognition,pp.4401‚Äì4410,2019.
TeroKarras,SamuliLaine,MiikaAittala,JanneHellsten,JaakkoLehtinen,andTimoAila. Analyz-
ingandimprovingtheimagequalityofstylegan. InProceedingsoftheIEEE/CVFconferenceon
computervisionandpatternrecognition,pp.8110‚Äì8119,2020.
Tero Karras, Miika Aittala, Samuli Laine, Erik Ha¬®rko¬®nen, Janne Hellsten, Jaakko Lehtinen, and
TimoAila. Alias-freegenerativeadversarialnetworks. AdvancesinNeuralInformationProcess-
ingSystems,34:852‚Äì863,2021.
Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Kon-
rad Schindler. Repurposing diffusion-based image generators for monocular depth estimation.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
9492‚Äì9502,2024.
14Lotus: Diffusion-basedVisualFoundationModelforHigh-qualityDensePrediction
Tobias Koch, Lukas Liebel, Friedrich Fraundorfer, and Marco Korner. Evaluation of cnn-based
single-imagedepthestimationmethods.InProceedingsoftheEuropeanConferenceonComputer
Vision(ECCV)Workshops,pp.0‚Äì0,2018.
Hsin-YingLee,Hung-YuTseng,andMing-HsuanYang.Exploitingdiffusionpriorforgeneralizable
denseprediction. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pp.7861‚Äì7871,2024.
JinHanLee, Myung-KyuHan, DongWookKo, andIlHongSuh. Frombigtosmall: Multi-scale
localplanarguidanceformonoculardepthestimation. arXivpreprintarXiv:1907.10326,2019.
Jiahui Lei, Yijia Weng, Adam Harley, Leonidas Guibas, and Kostas Daniilidis. Mosca: Dynamic
gaussian fusion from casual videos via 4d motion scaffolds. arXiv preprint arXiv:2405.17421,
2024.
Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma,
Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d
usingcross-domaindiffusion. InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,pp.9970‚Äì9980,2024.
Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,
IlyaSutskever,andMarkChen. Glide: Towardsphotorealisticimagegenerationandeditingwith
text-guideddiffusionmodels. arXivpreprintarXiv:2112.10741,2021.
AdityaRamesh,MikhailPavlov,GabrielGoh,ScottGray,ChelseaVoss,AlecRadford,MarkChen,
andIlyaSutskever. Zero-shottext-to-imagegeneration. InInternationalConferenceonMachine
Learning,pp.8821‚Äì8831.PMLR,2021.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-
conditionalimagegenerationwithcliplatents. arXivpreprintarXiv:2204.06125,1(2):3,2022.
Rene¬¥Ranftl,KatrinLasinger,DavidHafner,KonradSchindler,andVladlenKoltun. Towardsrobust
monoculardepthestimation: Mixingdatasetsforzero-shotcross-datasettransfer. IEEEtransac-
tionsonpatternanalysisandmachineintelligence,44(3):1623‚Äì1637,2020.
Rene¬¥ Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction.
InProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,pp.12179‚Äì12188,
2021.
Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan
Paczan, Russ Webb, and Joshua M Susskind. Hypersim: A photorealistic synthetic dataset for
holistic indoor scene understanding. In Proceedings of the IEEE/CVF international conference
oncomputervision,pp.10912‚Äì10922,2021.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo¬®rn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconfer-
enceoncomputervisionandpatternrecognition,pp.10684‚Äì10695,2022.
OlafRonneberger,PhilippFischer,andThomasBrox. U-net: Convolutionalnetworksforbiomed-
ical image segmentation. In Medical image computing and computer-assisted intervention‚Äì
MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceed-
ings,partIII18,pp.234‚Äì241.Springer,2015.
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar
Ghasemipour,RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,etal. Photorealistic
text-to-imagediffusionmodelswithdeeplanguageunderstanding. AdvancesinNeuralInforma-
tionProcessingSystems,35:36479‚Äì36494,2022.
TimSalimansandJonathanHo.Progressivedistillationforfastsamplingofdiffusionmodels.arXiv
preprintarXiv:2202.00512,2022.
15Lotus: Diffusion-basedVisualFoundationModelforHigh-qualityDensePrediction
ThomasSchops,JohannesLSchonberger,SilvanoGalliani,TorstenSattler,KonradSchindler,Marc
Pollefeys,andAndreasGeiger. Amulti-viewstereobenchmarkwithhigh-resolutionimagesand
multi-camera videos. In Proceedings of the IEEE conference on computer vision and pattern
recognition,pp.3260‚Äì3269,2017.
ChristophSchuhmann, RomainBeaumont, RichardVencu, CadeGordon, RossWightman, Mehdi
Cherti, TheoCoombes, AarushKatta, ClaytonMullis, MitchellWortsman, etal. Laion-5b: An
open large-scale dataset for training next generation image-text models. Advances in Neural
InformationProcessingSystems,35:25278‚Äì25294,2022.
NathanSilberman,DerekHoiem,PushmeetKohli,andRobFergus. Indoorsegmentationandsup-
portinferencefromrgbdimages. InComputerVision‚ÄìECCV2012: 12thEuropeanConference
onComputerVision,Florence,Italy,October7-13,2012,Proceedings,PartV12,pp.746‚Äì760.
Springer,2012.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv
preprintarXiv:2010.02502,2020.
YunzhouSong,JiahuiLei,ZiyunWang,LingjieLiu,andKostasDaniilidis. Trackeverythingevery-
wherefastandrobustly,2024.
QianqianWang,VickieYe,HangGao,JakeAustin,ZhengqiLi,andAngjooKanazawa. Shapeof
motion: 4dreconstructionfromasinglevideo. arXivpreprintarXiv:2407.13764,2024.
YuxiXiao,QianqianWang,ShangzhanZhang,NanXue,SidaPeng,YujunShen,andXiaoweiZhou.
Spatialtracker: Trackingany2dpixelsin3dspace. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition(CVPR),2024.
GuangkaiXu,YongtaoGe,MingyuLiu,ChengxiangFan,KangyangXie,ZhiyueZhao,HaoChen,
andChunhuaShen.Diffusionmodelstrainedwithlargedataaretransferablevisualmodels.arXiv
preprintarXiv:2403.06090,2024.
Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong
He. Attngan: Fine-grained text to image generation with attentional generative adversarial net-
works. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition, pp.
1316‚Äì1324,2018.
LiheYang,BingyiKang,ZilongHuang,XiaogangXu,JiashiFeng,andHengshuangZhao. Depth
anything: Unleashingthepoweroflarge-scaleunlabeleddata. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pp.10371‚Äì10381,2024a.
Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang
Zhao. Depthanythingv2. arXivpreprintarXiv:2406.09414,2024b.
ChongjieYe,LingtengQiu,XiaodongGu,QiZuo,YushuangWu,ZilongDong,LiefengBo,Yuliang
Xiu,andXiaoguangHan.Stablenormal:Reducingdiffusionvarianceforstableandsharpnormal.
arXivpreprintarXiv:2406.16864,2024.
Wei Yin, Yifan Liu, and Chunhua Shen. Virtual normal: Enforcing geometric constraints for ac-
curateandrobustdepthprediction. IEEETransactionsonPatternAnalysisandMachineIntelli-
gence,44(10):7282‚Äì7295,2021a.
Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Long Mai, Simon Chen, and Chunhua
Shen. Learningtorecover3dsceneshapefromasingleimage. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pp.204‚Äì213,2021b.
WeiYin,ChiZhang,HaoChen,ZhipengCai,GangYu,KaixuanWang,XiaozhiChen,andChunhua
Shen. Metric3d: Towardszero-shotmetric3dpredictionfromasingleimage. InProceedingsof
theIEEE/CVFInternationalConferenceonComputerVision,pp.9043‚Äì9053,2023.
WeihaoYuan,XiaodongGu,ZuozhuoDai,SiyuZhu,andPingTan.Neuralwindowfully-connected
crfs for monocular depth estimation. In Proceedings of the IEEE/CVF conference on computer
visionandpatternrecognition,pp.3916‚Äì3925,2022.
16Lotus: Diffusion-basedVisualFoundationModelforHigh-qualityDensePrediction
EkimYurtsever,JacobLambert,AlexanderCarballo,andKazuyaTakeda. Asurveyofautonomous
driving: Commonpracticesandemergingtechnologies. IEEEaccess,8:58443‚Äì58469,2020.
Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, and Yi Ma. In-
vestigating the catastrophic forgetting in multimodal large language models. arXiv preprint
arXiv:2309.10313,2023.
ChiZhang,WeiYin,BillzbWang,GangYu,BinFu,andChunhuaShen.Hierarchicalnormalization
forrobustmonoculardepthestimation. AdvancesinNeuralInformationProcessingSystems,35:
14128‚Äì14139,2022.
Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dim-
itris N Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative ad-
versarialnetworks. InProceedingsoftheIEEEinternationalconferenceoncomputervision,pp.
5907‚Äì5915,2017.
Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dim-
itrisNMetaxas. Stackgan++: Realisticimagesynthesiswithstackedgenerativeadversarialnet-
works. IEEEtransactionsonpatternanalysisandmachineintelligence,41(8):1947‚Äì1962,2018.
HanZhang,JingYuKoh,JasonBaldridge,HonglakLee,andYinfeiYang. Cross-modalcontrastive
learningfortext-to-imagegeneration. InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,pp.833‚Äì842,2021.
17