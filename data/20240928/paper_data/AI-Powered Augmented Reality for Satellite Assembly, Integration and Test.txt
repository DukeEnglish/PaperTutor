AI-Powered Augmented Reality for Satellite
Assembly, Integration and Test
1st A´lvaro Patr´ıcio 2nd Joa˜o Valente 3rd Atabak Dehban
Institute For Systems and Robotics Institute For Systems and Robotics Institute For Systems and Robotics
University of Lisbon University of Lisbon University of Lisbon
Lisbon, Portugal Lisbon, Portugal Lisbon, Portugal
alvaro.felipe@tecnico.ulisboa.pt joao.f.valente@tecnico.ulisboa.pt adehban@isr.tecnico.ulisboa.pt
5th Ineˆs Cadilha 5th Daniel Reis 6th Rodrigo Ventura
Lusospace Lusospace Institute For Systems and Robotics
Lisbon, Portugal Lisbon, Portugal University of Lisbon
icadilha@lusospace.com dreis@lusospace.com Lisbon, Portugal
rodrigo.ventura@isr.tecnico.ulisboa.pt
Abstract—The integration of Artificial Intelligence (AI) and
Augmented Reality (AR) is set to transform satellite Assembly,
Integration,andTesting(AIT)processesbyenhancingprecision,
minimizing human error, and improving operational efficiency
in cleanroom environments. This paper presents a technical
description of the European Space Agency’s (ESA) project ”AI
for AR in Satellite AIT,” which combines real-time computer
vision and AR systems to assist technicians during satellite
assembly.LeveragingMicrosoftHoloLens2astheARinterface,
the system delivers context-aware instructions and real-time
feedback, tackling the complexities of object recognition and 6D
pose estimation in AIT workflows. All AI models demonstrated
over 70% accuracy, with the detection model exceeding 95%
accuracy,indicatingahighlevelofperformanceandreliability.A
keycontributionofthisworkliesintheeffectiveuseofsynthetic
data for training AI models in AR applications, addressing the
significant challenges of obtaining real-world datasets in the
highly dynamic satellite environments, as well as the creation
of the Segmented Anything Model for Automatic Labelling Fig.1. DemonstrativeoutputsoftheAI-drivenaugmentedrealitysystemfor
(SAMAL), which facilitates the automatic annotation of real satellite assembly procedures include color-coded bounding boxes: yellow,
data,achievingspeedsupto20timesfasterthanmanualhuman green, and blue indicate object detection, the red 3D bounding box denotes
annotation. The findings demonstrate the efficacy of AI-driven 6D pose identification, and the pink bounding box represents OCR for
measurementinstruments.
ARsystemsinautomatingcriticalsatelliteassemblytasks,setting
a foundation for future innovations in the space industry.
Index Terms—Artificial Intelligence (AI), Augmented Reality
(AR), Space Industry, Computer Vision, Synthetic Data, Mi- This project explores the potential of AI-driven real-time
crosoft Hololens 2. object detection systems integrated with AR to assist tech-
nicians during satellite assembly. The system is designed to
I. INTRODUCTION ensure proper component identification and the elimination
In recent years, the convergence of Artificial Intelligence of unwanted objects within the workspace. Additionally, the
and Augmented Reality has created new opportunities for project addresses the challenge of generating training data for
innovation across multiple industries, with the space sector AImodelsduetothefrequentchangesinSatellitecomponents
being a prime candidate for significant advancements [1]. The by employing synthetic data. Collaborating organizations, in-
European Space Agency (ESA) has launched a project titled cluding Luxspace [3], OHB [4], Lusospace [5], and Instituto
”AI for AR in Satellite Assembly, Integration, and Testing Superior Te´cnico (IST), contribute their expertise to develop
(AIT),” which aims to enhance the accuracy, efficiency, and an AI and AR system built around the Microsoft Hololens
automation of satellite production processes. Given the high- 2 [6], which provides real-time, context-aware instructions to
stakes nature of satellite manufacturing, minimizing human operatorswhilerecordingrelevantdataduringAITprocedures.
error during AIT processes within controlled environments, The design of the AI system encompasses the entire data
such as clean rooms, is critical to avoiding costly failures that processing pipeline, from the capture of RGB, depth, and
could lead to multi-million-euro losses [2]. inertial sensor data through the Hololens 2, to the deployment
4202
peS
62
]VC.sc[
1v10181.9042:viXraFig. 2. The system architecture consists of green modules for processing on the computer, red blocks for communication, and a pink block in the headset
foracquiringinputdata.Grayarrowsrepresentdatastreams,whileblackarrowsindicateinternaldataflows.
of advanced computer vision algorithms for object detec- II. COMPONENTSOVERVIEW
tion, 6D pose, and alphanumeric optical character recognition
The AI4AR system consists of two key components, as
(OCR). A notable technical challenge is the system’s reliance
illustratedinFigure2:theregularcomputerandtheARhead-
on synthetic data for training 6D pose models—an approach
set. Together, these components work in harmony to enable
necessaryduetotheconstantevolutionofsatellitecomponents
text and object detection, recognition, real-time feedback, and
andtheimpracticalityofmanuallylabelingreal-worlddatasets.
seamlessinteractionwithinaworkstationunitenvironmentde-
Severalprojectsacrossvariousdomainsillustratethepoten- signedforcomplexassemblytasks,suchassatelliteassembly.
tialofAIandARintegration.IncollaborationwithMicrosoft,
NASAdevelopedaprojectcalledSidekickthatusesMicrosoft A. Computer
HoloLens to provide AR-based guidance to astronauts aboard
Theregularcomputerhandlesthebulkofthecomputational
the International Space Station, assisting them in complex
tasks involved in detecting, tracking, and interpreting objects
procedures by overlaying virtual objects and real-time in-
within the environment. Key modules within this component
structions [7]. In medicine, recent advancements in brain
include:
surgery techniques have leveraged AI and AR to create 3D
1) Detection Module: utilizes state-of-the-art object detec-
holographic models of patient brains, enabling surgeons to
tion algorithms to identify and locate objects or parts within
visualize pathology with unprecedented clarity and precision
theassemblyenvironment.Thesedetectionsareusedtoinform
during operations, leading to improved outcomes in treating
subsequent actions and instructions provided to the user via
complex neurological conditions [8]. These examples demon-
the AR headset.
strate the versatility of AI-AR systems across industries, from
2) 6DPoseEstimationModule: Thismoduleisresponsible
space and medicine to education and cultural heritage.
for determining the 6D pose (position and orientation) of
The key contributions of this paper are threefold. First, objects and the AR headset in real-time, requiring both rapid
we present a novel systems engineering approach specifically andhighlyprecisemeasurements,oftentothemillimeterlevel.
developed for the integration of AI and AR in satellite AIT, ThisprecisionandspeedarecrucialforeffectiveARheadsets,
marking a pioneering effort in the field. This includes the cre- where even minor inaccuracies or delays can significantly
ation of a new, end-to-end system designed to operate offline, impacttheuserexperiencebyaffectingthealignmentofvirtual
addressing the unique technical and operational challenges of elements with the real world.
this context. Second, we study the effective use of synthetic 3) OCR Module: this component extracts measurements
data for training AI models within AR environments, offering andtextfromlabels,instruments,orothertext-basedmaterials
a practical solution to the data scarcity issues faced in highly present in the assembly scene. It processes text and number
controlled satellite environments. Finally, the development data to assist in validation, error detection, or for providing
of the Segmented Anything Model for Automatic Labeling relevant assembly instructions.
(SAMAL) improves the system’s efficacy by enabling the 4) Communication Module: ensures a smooth data flow
automatic annotation of real data, making the process up to between the regular computer and the AR headset. This
20 times faster. These contributions lay the groundwork for is accomplished through communication protocols such as
futureadvancementsinARapplicationsandsatelliteassembly ZeroMQ, enabling fast, real-time data transmission necessary
processes, with broader implications for the space industry. for guiding users and processing feedback [9].robustness and scalability of object detection solutions across
various applications.
YOLOv7 was chosen for this project due to its exceptional
balance of speed and accuracy in object detection, making
it ideal for real-time applications such as augmented reality.
Its advanced architecture, which builds on the strengths of
previous YOLO versions, includes improvements like ”Bag
of Freebies” for data augmentation and multi-head detection,
which enhance precision without sacrificing efficiency. This
makes YOLOv7 highly effective for detecting and localizing
objects in complex environments [15].
Additionally, YOLOv7’s support for transfer learning and
Fig. 3. Illustrative image of a technician utilizing the headset during compatibility with large datasets like COCO allows for faster
operationalprocedures. trainingandbettergeneralizationacrossdiversescenarios[16].
The model’s ability to leverage pre-trained weights and adapt
tospecifictasksensuresrobustperformance,evenwithsmaller
B. AR Headset
datasets. Its proven success in industry and research, coupled
The AR headset serves as the user interface, projecting vir-
with its flexibility, made YOLOv7 the optimal choice for
tualinstructionsandaugmentationsontothephysicalassembly
achieving high-performance object detection in this project.
environment, Figure 3 illustrates how a technician views the
information on the device. The key components of the AR
B. Accelerating Data Annotation with SAM2
headset include:
1) Video Camera and Depth Camera: these cameras cap- In the context of object recognition, we employed a novel
ture real-time visual and depth data from the assembly envi- technique to annotate real-world data with significantly in-
ronment, providing essential information for object detection, creased efficiency. This approach utilizes SAM2 (Segment
6D pose estimation, and scene interpretation. Anything Model 2), an advanced segmentation model specif-
2) Head and Eye Pose Tracking: Tracks the position and ically designed to automatically identify and segment objects
orientation of the user’s head, allowing the virtual overlays across a diverse array of images and videos [17].
to move naturally with the user’s perspective, ensuring proper SAM2 operates on the principle of utilizing large-scale
alignment and consistency as the user navigates the assembly training data and sophisticated machine learning algorithms
environment. to achieve state-of-the-art performance in object segmentation
3) Holographic Processing Unit: Renders 3D holographic tasks. It leverages a combination of deep learning techniques
overlaysintotheuser’sfieldofview.Thisunitprovidesvisual and a transformer-based architecture to facilitate the seg-
instructions, such as highlighting assembly steps or showing mentation of objects in varying contexts, lighting conditions,
the placement of parts, directly within the user’s augmented and backgrounds. This versatility allows SAM2 to generalize
reality display. effectively across different domains, making it particularly
TheCommunicationModuleenablesefficientreal-timedata valuable in applications requiring rapid and accurate image
exchange between the computer and AR headset, ensuring annotation.
synchronizedobjectdetection,poseestimation,andvirtualin- SAM2automatesthelabelingprocess,significantlydecreas-
struction projection. Its low-latency communication is critical ing the time and effort needed for manual annotation. This
for the system to respond instantly to user movements during automation enhances the quality and consistency of the anno-
assembly. tations, which is vital for training effective object detection
models and improving performance in real-world recognition
III. LITERATUREREVIEW tasks.
A. Object Detection
C. 6D pose
Over the past decade, object detection has evolved from
traditional techniques reliant on handcrafted features, such as Overthepastdecade,6Dposeestimationhasadvancedsig-
Haar cascades and Histograms of Oriented Gradients (HOG) nificantly,transitioningfromtraditionalgeometry-basedmeth-
[10],toadvanceddeeplearningmodels[11].Thebreakthrough ods, such as template matching and point-pair features (PPF),
came with Convolutional Neural Networks (CNNs), high- whichreliedheavilyonmanualfeatureengineering,tosophis-
lightedbyAlexNet’ssuccessin2012[12],whichdemonstrated ticated deep learning models [18]. Early techniques struggled
the effectiveness of deep learning in image classification. to handle complex real-world scenarios with occlusions and
Subsequentmodels,suchasRegion-basedCNN(R-CNN)[13] lighting variations. The advent of deep learning, particularly
and YOLO (You Only Look Once) [14], revolutionized object withConvolutionalNeuralNetworks(CNNs),transformedthe
detectionbyenablingautomaticfeaturelearningandreal-time, fieldbyenablingmodelslikePoseCNN[19]andDenseFusion
accurate detection. This transition has greatly enhanced the [20] to learn robust, data-driven representations that couldaccurately predict 6D object poses under challenging condi- A. Unity
tions, offering greater flexibility and precision across diverse
Unity serves as the core 3D engine in the AI4AR system,
environments.
chosenforitspowerfulandaccessibleplatformfordeveloping
We selected GDRNPP (Geometrically Disentangled Repre-
augmented reality interfaces on Microsoft HoloLens 2. The
sentation Network with Probabilistic Pose) for our 6D pose
system relies heavily on Unity’s capabilities to create high-
estimation task due to its state-of-the-art architecture, which
fidelity 3D visualizations and overlay real-time information,
optimizes both precision and computational efficiency. More-
assistingoperatorsinsatelliteAssembly,Integration,andTest-
over, GDRNPP supports transfer learning, enabling the model
ing. [25].
tobenefitfrompre-trainednetworksandadaptquicklytonew
The project utilizes the Mixed Reality Toolkit (MRTK) 3,
datasets with minimal retraining. This efficiency in leveraging
an open-source toolkit provided by Microsoft, to streamline
both geometric information and deep-learned features ensures
thedevelopmentofmixedrealityapplications.MRTKenables
robustperformanceevenwithsmallerormorediversedatasets
efficienthandlingofuserinteractions,handtracking,andgaze
[21] [22].
input, allowing for intuitive and responsive user experiences
OurchoiceofGDRNPPisparticularlysuitedforaugmented
on the HoloLens 2 [26]. In conjunction with Unity, MRTK
reality (AR) applications, where both speed and precision are
provides pre-built components and controls optimized for
crucial.InAR,real-timeprocessingisessentialforensuringa
mixed reality, simplifying the implementation of interactive
seamless user experience, as any delay or inaccuracy in pose
and immersive interfaces.
estimation can result in poor alignment of virtual objects with
Input data from the HoloLens is processed using OpenXR,
the real world, breaking immersion. Additionally, GDRNPP
which integrates seamlessly with MRTK to manage device
incorporates geometric constraints, which contribute to more
input such as hand gestures and spatial tracking [27]. These
accurate pose predictions in scenarios involving partial visi-
inputs enable precise interaction with the virtual elements
bility or overlapping objects, making it a suitable choice for
overlaid in the operator’s field of view.
complex, real-world applications.
To handle data communication between Unity and the
regular computer, a custom C# DLL was developed. This
D. OCR DLL enables asynchronous communication, using ZeroMQ
to efficiently send requests and receive data such as object
For the task of Optical OCR, two open-source solutions
detection, OCR, 6D pose estimation, and other AI outputs.
wereconsidered:EasyOCR[23]andMMOCR[24].Tesseract,
The data is then processed by Unity to dynamically update
a widely known OCR engine, was excluded from the bench-
the holographic overlays and provide real-time feedback to
markduetoitsrelianceonpre-processed,clean,text-onlyim-
the operator, as shown in Figure 2.
ages and its limitation to horizontal rectangular boxes, which
renders it unsuitable for OCR in unconstrained environments
as encountered in this study. B. Yolov7
The benchmarking of text detection models revealed that
YOLOv7 (You Only Look Once, version 7) is a cutting-
EasyOCRdemonstratesthehighestinferencespeed(6.47FPS)
edge real-time object detection model, recognized for its high
andthebestperformanceatthe0.5F1-scorethreshold,making
performance across a range of object detection benchmarks
it the most suitable choice for real-time applications despite
[15]. To optimize performance, the project experimented with
loweraccuracyatmorestringentthresholdscomparedtoother
differentpre-trainedmodelsinYOLOv7.Whilesomeofthese
models. For text recognition, the speed of inference was
modelsachievedsuperiorprecisionandrecall,theyoftencame
significantly higher (130.61 FPS), which is crucial given the
at the cost of reduced detection speed. Given the importance
real-time constraints. This trade-off in accuracy is acceptable
of real-time detection for effective AR implementation, this
considering the need for rapid processing.
trade-offbetweenspeedandaccuracywascarefullyconsidered
Considering all reasons stated above, EasyOCR was iden-
[1]. To test your object detection and ensure the reliability of
tified as the most appropriate model for both text detection
themodelinpracticalapplications,weestablishedadedicated
and recognition tasks, offering a favorable balance between
test set captured using the actual AR equipment employed
speed and baseline performance, thus serving as an optimal
in the assembly process. The test images were manually
candidate for further enhancements.
annotated using Roboflow’s labeling tools [28], providing
high-quality ground truth labels for performance evaluation.
IV. IMPLEMENTATIONPIPELINECOMPONENTS Metrics such as accuracy, recall, mean Average Precision
(mAP) (specifically mAP@0.5-0.95, and mAP@0.95) were
This section provides a detailed overview of the individual employed to assess and compare the performance of different
components used in the project ”AI for AR in satellite AIT.” YOLOv7 models. This rigorous evaluation process ensures
Each component contributes to the system’s capability of thatthemodelselectedfordeploymentoffersthebestpossible
integrating real-time object detection, computer vision, and balanceofspeed,accuracy,androbustnessinareal-worldAR-
AR to assist technicians in satellite assembly tasks. enhanced Satellite assembly scenario.D. GDRNPP
GDRNPP (Generalized Differentiable Rendering Network
for Pose Prediction) is a state-of-the-art 6D pose estimation
method deployed in this project to accurately localize and
provide 6D position of Satellite components during the as-
sembly process [21]. GDRNPP was chosen for its ability to
provide highly precise pose estimations, which are essential
in augmented reality (AR) environments where even a slight
deviation can lead to critical errors. For instance, in our
specific case, if the model’s estimated pose is off by just a
few millimeters from the real component, a hole intended for
a screw could be misaligned, causing issues during assembly.
The model’s ability to operate exclusively with RGB data
optimizes the utilization of the HoloLens camera, bypassing
the limitations associated with the lower resolution and frame
rate of depth sensors while ensuring smooth real-time perfor-
mance. The performance of GDRNPP in the BOP Challenge
2022furthervalidateditsreliability,establishingitasthemost
appropriate open-source solution for this project [29].
To evaluate the system’s performance, we use the most
widely accepted metrics in 6D pose estimation and tracking:
ADD and ADD(-S) for symmetric objects. These metrics rely
Fig. 4. SAMAL is an advanced annotation tool that accurately generates
on the average distance between corresponding points in the
bounding boxes around objects. It effectively handles occlusions, such as
user hands, and adapts to changes in object positions, making it especially 3D model, comparing the estimated and ground-truth poses.
valuable for augmented reality applications where hands frequently interact An estimated pose is considered correct when the average
withobjects.
distance is below a specific threshold, typically set at 10%
of the object’s diameter. Accuracy is then calculated as the
ratio of correct estimations to the total number of images in
C. SAM2 thedataset,ensuringthatGDRNPPdeliversboththeprecision
and reliability required for AR-enhanced Satellite assembly.
To evaluate the GDRNPP framework, we created a custom
TheintegrationofSAM2forautomaticannotationenableda
test dataset with 188 images (640 × 360 resolution) captured
more efficient data preparation pipeline, allowing for the gen-
usingtheHoloLens.Inoneofthescenes,73framesweretaken
eration of large volumes of annotated images within a matter
while the PDT was rotated in the user’s hands, simulating
ofminutes.Theseannotateddatasetswillsubsequentlybeused
real-world manipulation and introducing challenges like hand
to train object detection models, enhancing their performance
occlusions. The 6D poses were annotated using 6D-PAT [30]
in real-world satellite assembly applications. According to
by manually aligning the 3D model to the object in the
experimental results, the use of SAM2 for automating the
image and establishing at least six 2D-3D correspondences,
annotation process yielded highly positive outcomes, as you
which were then used to compute the pose with a PnP solver.
can see in some of the examples in Figure 4, reducing
This provided accurate ground truth data for performance
annotation time by several orders of magnitude compared to
evaluation.
traditional manual methods, thereby streamlining the dataset
preparation phase for model training. E. EasyOCR
To facilitate automatic labeling, an adaptation of the origi- EasyOCR is a Python module for extracting text from
nal SAM2 code was developed to create SAMAL. Initially, images, developed by Jaided AI in 2020. It is a general OCR
a series of recordings were captured from various angles, packagethatcanreadbothnaturalscenetextanddensetextin
simulating the diverse perspectives a worker might have of a document, with support for multiple languages. It leverages
the object in question. This process included variations in theCRAFT[31]algorithmfortextdetectionandaCRNN[32]
distance, lighting conditions and others. Subsequently, the for text recognition.
object of interest is selected in the first frame of the video In this project, we recognize the critical importance of ac-
for tracking throughout the entire sequence of frames. SAM2 curate readings from the display of devices, which influenced
then generates a mask that delineates the location of the ourdecisiontotrainOCRmodels.TwoOCRmodelsarebeing
targeted object in all frames. Using these masks, we identify developed: one dedicated to recognizing numbers, which has
the extreme points in both the X and Y coordinates, from been completed and is currently utilized to detect multimeter
which we derive the bounding box coordinates in the format and power supply measurements, as shown in Figure 1, and
required to train YOLOv7. another designed for recognizing both letters and numbers,specifically for processing tags. This decision of training two less favorable. This is because it was possible to generate
different OCR models was made due to the importance of complete training datasets for various objects in just a few
readingsfromthedisplayofthedevices.It’salsoimportantto minutes. However, due to the model’s low recall (high rate
note that both the text detection and text recognition tasks of false negatives), we developed SAMAL as an alternative,
for the alphanumeric OCR model are actively undergoing efficient method for collecting training data. This allowed us
modifications. toconductacomprehensivecomparisonbetweenSAMALand
1) Numerical OCR Model: With regard to the numeri- the previously generated synthetic data.
cal ocr model, the training datasets for text detection and To address these challenges and enable more robust model
recognition tasks were synthetically generated to enhance the comparisons, we utilized two datasets: one with real data and
performance of the EasyOCR models. For the text detection another with synthetic data. The results of these experiments
model focused on numbers, 100,000 synthetic samples were will be presented, comparing two distinct data generation
created using SynthText [33]. This dataset generation process strategies for training object detection models:
involved several key steps to create realistic synthetic images • Real Data Only: This experiment utilized 4,443 images
for training the OCR system. First, video footage from the for training and 1,111 images for validation.
test dataset was captured to provide realistic backgrounds • Synthetic Data Only: For this setup, 8,210 images
that closely mirrored real-world environments. Text was then were used for training, and 1,760 images were used for
sampled from a dictionary of four-digit positive and negative validation.
numbers and placed on suitable regions of the video frames For consistency and comparability, all models were trained
byanalyzingpredicteddepthandsegmentationmaps,ensuring for 10 epochs using an NVIDIA A100 GPU with 80GB of
the text fit semantically with the scene. The text was rendered memory.Additionally,allmodelsweretrainedusingthesame
in the seven-segment DSEG font to replicate the appearance pre-trained weights, specifically yolov7.pt. The results for
of LCD displays and blended with the background images, all models were reported using a confidence threshold of 0.5.
maintaining natural illumination gradients for enhanced real- For the generation of synthetic data, various approaches
ism. were employed, including object size variation, changes in
For text recognition, a synthetic dataset of 100,000 sam- lighting conditions, the inclusion of distractors, and the ap-
ples was generated, covering 12 character classes (“ .- plication of motion blur to simulate rapid camera movements.
1234567890”).TextcropswerecreatedusingtheTextRecog- The model trained exclusively on synthetic data demonstrated
nition Data Generator (TRDG [34]), introducing variations good overall precision of 0.99, but with a recall of 0.60 and
in size, color, noise, and other factors to improve robustness a mAP@0.5:0.95 score of 0.467, it fell short compared to
in diverse real-world conditions. Techniques inspired by the the real data model, as illustrated in Table I. Conversely, the
MJSynth dataset generation process [35], such as random model trained on real data achieved near-perfect precision
perspective transformations, simulated real-world variations of 0.991 and a recall of 0.978, with a significantly higher
in text orientation. Finally, synthetic images were further mAP@0.5:0.95 score of 0.753, as demonstrated in Table
enhancedbyblendingthemwithpatchesfromtheIC15Scene I. This highlights the superior performance of real data in
Text Detection dataset, adding diverse textures and lighting capturing the variability and complexity of real-world object
conditions to better simulate natural environments. detection scenarios.
2) AlphanumericOCRModel: Boththetextdetectionand Historically, the manual annotation of real data has been
recognition tasks for this model follow a similar approach as a major bottleneck, often consuming hours of labor. For in-
thenumericalOCR,withthemaindifferencebeingtheclasses stance,labelingasetof459imagesrequiredahumanoperator
that represent the characters identified in the test set, which 72 minutes to complete, and even then, the resulting dataset
include a broader range of alphanumeric characters beyond was not perfectly annotated, with many bounding boxes of
numbers. differentsizesthanthosedesired.Inourexperiments,SAMAL
annotated459imagesinjust3minutesand20seconds—more
V. EXPERIMENTSANDRESULTS
than 20 times faster than manual annotation, as illustrated in
All benchmarks were conducted on a standard desktop Figure5.Thisremarkablespeedimprovementisaccompanied
equipped with an AMD Ryzen 5 2600 @ 3.4GHz CPU and by increased accuracy and consistency, as it minimizes the
anNVIDIAGeForceGTX1070GPU.Theevaluationmetrics potential for human error.
included average Frames Per Second (FPS) for inference SAMAL’s effectiveness in handling occlusions, such as
speed, alongside accuracy measures specific to each task. those caused by a user’s hand in AR environments. Since
users’ hands frequently interact with objects of interest,
A. Object Detection Task:
SAMAL’s ability to accurately generate bounding boxes
Although synthetic data played a crucial role in addressing around objects, even in complex scenarios with occlusions
otherAItasks,itbecameevidentthatrealdataproducedbetter and positional changes, is essential. While synthetic data
results when compared to synthetic data in object detection. provides a useful alternative for some object detection tasks,
With the development of a tool for automating the annotation the superior performance of real data remains evident. The
ofthesedatasetsfortraining,theuseofsyntheticdatabecame primary limitation of real data, which is its time-consumingTABLEI
PERFORMANCEMETRICSCOMPARISONACROSSDIFFERENTCLASSESFORTHEDETECTIONMODULE
Dataset Class Precision Recall mAP@0.5 mAP@0.5:0.95
Synthetic Data All 0.99 0.60 0.604 0.467
All 0.991 0.978 0.976 0.753
Powersupply 1.0 0.960 0.965 0.828
Real Data
Multimeter 1.0 0.992 0.998 0.826
Tag 0.980 0.975 0.966 0.604
• The 3D model of PDT was randomly placed alongside
distractor objects with varied materials to create more
complex and realistic interactions;
• Physics simulations were also carried out to ensured
plausible object poses and consistency with reality;
• Multiple cameras were used to render RGB images,
object masks, and visibility masks with annotated 6D
poses.
Furthermore, because the materials used as backgrounds of
the scenes rooms were very different from the background
present in the test set, rendered PDTs were blended into im-
agesofthetestenvironmentusingvisibilitymasks,preserving
occlusioneffectsandthusminimizingthedomaingapbetween
synthetic and real test images.
The GDRNPP requires an initial bounding box around the
Fig.5. Thechartcomparesthetimerequiredformanualannotationof459
object, which was derived from object masks rendered in
imagesversusSAMAL.Whilemanualannotationtook72minutes,SAMAL
completed the same task in just 3 minutes and 20 seconds, demonstrating a BlenderProc for training, and from an object detector during
substantialincreaseinefficiency. test-time. Considering two scenes: scene A (figure 6), where
thePDTisbeingrotatedinthehandsoftheuser,thusinducing
strong occlusions with the hands, and scene B (figure 7), in
annotation process, has been largely addressed by the devel-
whichthePDTisstaticonthetableandthecameraismoving
opment of SAMAL. This tool transforms the labeling process
around while looking at it, one can visualize the results after
intoanefficient,automatedworkflow,enablingfasterandmore
training in Table II.
accuratedatasetpreparation.Theinnovationnotonlyenhances
thescalabilityofrealdatainobjectdetectionbutalsoimproves
the accuracy and consistency of models, particularly in real-
world and AR-enhanced applications.
B. 6d Pose Task
Fig.6. 6DPoseTestSceneA Fig.7. 6DPoseTestSceneB
The 6D Pose Tracking model was trained using a modified
version of the GDRNPP training script, originally designed
for public BOP Challenge datasets [36], to accommodate
TABLEII
custom data. Key parameter adjustments included setting the
6DPOSETRACKINGRESULTSONTESTDATASET.
”DZI PAD SCALE” to 1 and ”COLOR AUG PROB” to 0,
preventing background replacement in training images. The BlenderProc +
Training Dataset BlenderProc
Room backgrounds
training dataset, comprising 50,000 samples, was generated
Scene A (%) 66.00 73.97
usingBlenderProc[37],adheringtothesyntheticdatacreation
Scene B (%) 76.19 88.70
steps used in the BOP Challenge 2022 [36].
Full test dataset (%) 69.33 82.98
With regards to the data generation process, several steps Speed (FPS) 12.92 12.92
were taken:
• Synthetic scenes were created by arranging planes as One can see that training with blended images showed sig-
walls with realistic PBR textures; nificant improvements particularly in scene B, which presents
• Lighting conditions were simulated by randomizing the fewer occlusions. The model operated at 13 FPS, suitable for
number of lights in the scene,their respective strength, the intended application. Accuracy, though lower than OCR
color and position; models, established a satisfactory baseline for system testing.To evaluate the impact of using less reliable bounding The trained model achieved an accuracy of 98.36%, far
boxes, the performance of the model was tested using altered surpassing the performance of the pre-trained model. It also
bounding boxes to simulate the output of an object detector. exhibited high inference speeds (188.82 FPS), meeting real-
For each test sample, the ground-truth bounding box was time requirements. This increase in speed is partly due to
randomlyshiftedupto25%ofitswidthandheightandscaled the reduced number of classes (digits, decimal, and negative
by a factor between 0.75 and 1.25 of its original size. The symbols), which simplified the character classification task.
averageaccuracyoverfivetestsonthefulldatasetispresented Thespeedgainswerefurtherenhancedbysettingabatchsize
in Table III. greater than one during inference, which was done because
multiple bounding boxes are detected per frame and then sent
TABLEIII in order to be recognized.
EFFECTOFALTERINGTHEINITIALBOUNDINGBOXESIN6DPOSE
The combined OCR pipeline, integrating the text detection
TRACKINGRESULTS.
and recognition models, was evaluated to assess its overall
Bounding box Full test dataset (%) performance in reading numbers. The text detection model
Ground-truth 82.98
identifies bounding boxes in input images, which are then
Altered 78.51
cropped and processed by the recognition model to extract
the detected numbers.
Despitesignificantdistortionstotheboundingboxes,model
The text recognition test dataset was created using ground-
performance decreased by only about 5%, which suggests the
truthboundingboxestocompareperformanceagainstdetected
model’s robustness to bounding box inaccuracies.
bounding boxes, highlighting the impact of using real versus
C. OCR Task ideal conditions. The quantitative results are shown in Table
V.
1) Numerical OCR: The text detection model was trained
using EasyOCR’s [23] synthetic data training script, which
TABLEV
requirescharacter-levelannotations.Keyconfigurationparam-
TEXTRECOGNITIONRESULTSCOMPARISON
eters were adjusted to optimize performance, including a text
threshold of 0.5, a low text value of 0.3, a link threshold of Model Accuracy (%) Speed (FPS)
EasyOCR (pre-trained) 2.35 130.61
0.1, a canvas size of 2240, a magnification ratio of 1.5, no
Oracle OCR 98.36 188.82
additional margin, and a slope threshold of 0.
OCR Pipeline 96.03 -
Asmentionedinaprevioussection,thetextdetectionmodel
was trained using the CRAFT [31] algorithm on a synthetic
The OCR pipeline achieved an overall accuracy of 96.03%,
dataset. The comparison of the trained model’s performance
reflectingaslightdecreaseof2.3%comparedtousingground-
with the pre-trained EasyOCR model can be viewed in Table
truth bounding boxes. Despite this, the combined pipeline
IV.
remains highly effective, correctly reading numbers in 96%
of cases. These results confirm the robustness of the devel-
TABLEIV
oped OCR module, demonstrating its potential for accurate
TEXTDETECTIONRESULTSAFTERTRAINING.
numerical reading in practical scenarios.
Model F1 @ 0.5 F1 @ 0.7 Speed (FPS) 2) Alphanumeric OCR: As mentioned above, the text de-
EasyOCR (pretrained) 0.8858 0.4890 6.47
tectionandrecognitiontasksforthisOCRmodelarecurrently
Trained model 0.9933 0.9797 2.91
undergoingmodifications,howeveracomparisonbetweenthis
model and its corresponding Oracle version (obtained by
The trained model achieved near-perfect F1 scores at both
performingonlythetextrecognitiontaskontopoftheground
IoU thresholds (0.5 and 0.7), demonstrating substantial im-
truth bounding boxes) was made and can be seen in Table VI.
provement over the pre-trained EasyOCR model. This perfor-
mance was attained by setting the magnification ratio to 1.5,
TABLEVI
whichincreasesinputimagesize,therebyenhancingdetection
ALPHANUMERICANDORACLEOCRMODELSCOMPARISON
accuracybutreducinginferencespeedto2.91FPS.Thisspeed,
while not real-time, is adequate for applications involving Model Version Accuracy (%)
Alphanumeric OCR 65
measuring instruments with static or slow-changing displays.
Oracle OCR 71.25
For applications requiring higher frame rates, adjustments
to the magnification ratio can be made, providing a balance
The Oracle OCR model achieved a higher accuracy of
betweenaccuracyandspeed.Loweringtheratioreducesimage
71.25%, a modest improvement over the 60% accuracy ob-
resolution, impacting detection performance and allowing for
tained with the alphanumeric OCR, which utilizes both the
flexible optimization based on specific use-case requirements.
text detection and recognition trained models.
With regards to the text recognition model, it was trained
using the CRNN algorithm on the generated synthetic dataset VI. CONCLUSIONANDFUTUREWORK
anditdemonstratedsignificantimprovementscomparedtothe This paper presented the ”AI for AR in Satellite AIT”
EasyOCR pre-trained model, as one can see in Table V. project, demonstrating the integration of AI and AR forenhanced satellite assembly. The system, utilizing Microsoft [13] RossGirshick,JeffDonahue,TrevorDarrell,andJitendraMalik. Rich
HoloLens 2, employs computer vision for object detection, featurehierarchiesforaccurateobjectdetectionandsemanticsegmenta-
tion.arXivpreprintarXiv:1311.2524,2014.ExtendedversionofCVPR
6D pose estimation, and OCR, offering real-time operator
2014 paper; latest update (v5) includes results using deeper networks
guidance. (seeAppendixG.Changelog).
A key achievement was the successful use of synthetic [14] Joseph Redmon, Santosh Divvala, Ross B. Girshick, and Ali Farhadi.
Youonlylookonce:Unified,real-timeobjectdetection. arXivpreprint
data for AI model training, addressing the scarcity of real-
arXiv:1506.02640,2015. arXiv:1506.02640[cs.CV].
world datasets in dynamic satellite manufacturing. Experi- [15] Alexey Bochkovskiy Chien-Yao Wang and Hong-Yuan Mark Liao.
mental results showed high precision in object detection and Yolov7:Trainablebag-of-freebiessetsnewstate-of-the-artforreal-time
objectdetectors. arXivpreprintarXiv:2207.02696,2022.
satisfactory accuracy in 6D pose estimation, despite real-
[16] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPer-
time constraints. The OCR module, particularly for numerical ona,DevaRamanan,PiotrDolla´r,andC.LawrenceZitnick. Microsoft
recognition, demonstrated promising performance. coco:Commonobjectsincontext. InDavidFleet,TomasPajdla,Bernt
Schiele,andTinneTuytelaars,editors,ComputerVision–ECCV2014,
Moreover, the creation of the SAMAL significantly accel-
pages740–755,Cham,2014.SpringerInternationalPublishing.
erates the annotation process, enhancing the overall efficacy [17] NikhilaRavi,ValentinGabeur,Yuan-TingHu,RonghangHu,Chaitanya
of the system by up to 20 times. Ryali,TengyuMa,HaithamKhedr,RomanRa¨dle,ChloeRolland,Laura
Gustafson,etal. Sam2:Segmentanythinginimagesandvideos,2024.
Future work includes enhancing synthetic data genera-
[18] Markus Ziegler, Martin Rudorfer, Xaver Kroischke, and Sebastian
tion, improving 6D pose estimation robustness, developing Krone.Pointpairfeaturematching:Evaluatingmethodstodetectsimple
a complete alphanumeric OCR model, seamless AI module shapes. InComputerVisionSystems,pages445–456.Springer,2019.
[19] Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox.
integration, and user evaluation for refined usability.
Posecnn:Aconvolutionalneuralnetworkfor6dobjectposeestimation
inclutteredscenes. Robotics:ScienceandSystemsXIV,2018.
ACKNOWLEDGMENT [20] Chen Wang, Danfei Xu, Yuke Zhu, Roberto Mart´ın-Mart´ın, Cewu Lu,
LiFei-Fei,andSilvioSavarese. Densefusion:6dobjectposeestimation
by iterative dense fusion. 2019 IEEE/CVF Conference on Computer
Thanks to Gonc¸alo Silva for his contributions in the earlier
VisionandPatternRecognition(CVPR),pages3338–3347,2019.
stages of this research, and to James Pandey and LusoSpace [21] G.Wang,F.Manhardt,F.Tombari,andX.Ji.Gdr-net:Geometry-guided
for their support throughout the project. direct regression network for monocular 6d object pose estimation. In
Proceedings of the IEEE/CVF Conference on Computer Vision and
PatternRecognition,pages16611–16621,2021.
REFERENCES [22] X.Liu,R.Zhang,C.Zhang,B.Fu,J.Tang,X.Liang,J.Tang,X.Cheng,
Y. Zhang, G. Wang, and X. Ji. Gdrnpp, 2022. [Online]. Available:
[1] Jeevan S. Devagiri, Sidike Paheding, Quamar Niyaz, Xiaoli Yang, https://github.com/shanice-l/gdrnpp bop2022.
and Samantha Smith. Augmented reality and artificial intelligence in [23] JaidedAI. EasyOCR library, 2020. [Online]. Available:
industry: Trends, tools, and future challenges. Expert Systems with https://github.com/JaidedAI/EasyOCR.
Applications,207:118002,2022. [24] Z. Kuang, H. Sun, Z. Li, X. Yue, T.H. Lin, J. Chen, H. Wei, Y. Zhu,
[2] Douglas Stewart and Richard Chase. The impact of human error on T.Gao,W.Zhang,K.Chen,W.Zhang,andD.Lin. MMOCR:ACom-
delivering service quality. Production and Operations Management, prehensiveToolboxforTextDetection,RecognitionandUnderstanding.
8:240–263,012009. InProceedingsoftheIEEEConferenceonComputerVisionandPattern
[3] Luxspace. Luxspace. https://luxspace.lu/,2024. Recognition,2021.
[4] OHB. Ohbspacesystems. https://www.ohb.de/en/space-systems,2024. [25] JohnKHaas. Ahistoryoftheunitygameengine,2014.
Accessed:2024-08-15. [26] Microsoft. Mixedrealitytoolkit(mrtk)3. https://github.com/microsoft/
[5] Lusospace. Lusospace. https://lusospace.com/,2024. MixedRealityToolkit-Unity,2024.
[6] Dorin Ungureanu, Federica Bogo, Silvano Galliani, Pooja Sama, Xin [27] KhronosGroup. Openxr. https://www.khronos.org/openxr,2024.
Duan,CaseyMeekhof,JanStu¨hmer,ThomasJCashman,BugraTekin, [28] RoboflowTeam. Roboflow:Imageannotationanddatasetmanagement.
JohannesLScho¨nberger,etal. Hololens2researchmodeasatoolfor https://roboflow.com,2024. Accessed:2024-09-03.
computervisionresearch. arXivpreprintarXiv:2008.11239,2020. [29] MartinSundermeyer,Toma´sˇHodanˇ,YannLabbe´,GuWang,EricBrach-
[7] Benjamin Nuernberger, Robert Tapella, Samuel-Hunter Berndt, mann, Bertram Drost, Carsten Rother, and Jiˇr´ı Matas. Bop challenge
So Young Kim, and Sasha Samochina. Under water to outer space: 2022 on detection, segmentation and pose estimation of specific rigid
Augmentedrealityforastronautsandbeyond.IEEEComputerGraphics objects.In2023IEEE/CVFConferenceonComputerVisionandPattern
andApplications,40(1):82–89,2020. RecognitionWorkshops(CVPRW),pages2785–2794,2023.
[8] Grace Hey, Michael Guyot, Ashley Carter, and Brandon Lucke-Wold. [30] FlorianBlume. 6d-pat:6dposeandaffordancetransfer. https://github.
Augmented reality in neurosurgery: A new paradigm for training. com/florianblume/6d-pat,2024. Accessed:2024-09-03.
Medicina(Kaunas),59(10):1721,2023. [31] Y.Baek,B.Lee,D.Han,S.Yun,andH.Lee.Characterregionawareness
[9] ZeroMQ. Zeromq. https://github.com/zeromq/zeromq3-x, 2024. Ac- for text detection. In Proceedings of the IEEE/CVF Conference on
cessed:2024-08-15. ComputerVisionandPatternRecognition,pages9365–9374,2019.
[10] Muhammad Zacky Asy’ari, Sebastian Filbert, and Zener Lie Sukra. [32] B. Shi, X. Bai, and C. Yao. An end-to-end trainable neural network
Histogram of oriented gradients (hog) and haar cascade with convolu- for image-based sequence recognition and its application to scene
tionalneuralnetwork(cnn)performancecomparisonintheapplication text recognition. IEEE transactions on pattern analysis and machine
ofedgehomesecuritysystem.InSubhasChandraMukhopadhyay,S.M. intelligence,39(11):2298–2304,2016.
NamalAroshaSenanayake,andP.W.ChandanaWithana,editors,Inno- [33] A. Gupta, A. Vedaldi, and A. Zisserman. Synthetic data for text
vative Technologies in Intelligent Systems and Industrial Applications, localisationinnaturalimages. InProceedingsoftheIEEEConference
pages13–22,Cham,2023.SpringerNatureSwitzerland. onComputerVisionandPatternRecognition,pages2315–2324,2016.
[11] YiboSun,ZheSun,andWeitongChen.Theevolutionofobjectdetection [34] Belval. TextRecognitionDataGenerator, 2019. [Online]. Available:
methods. EngineeringApplicationsofArtificialIntelligence,2024. https://github.com/Belval/TextRecognitionDataGenerator.
[12] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet [35] MJaderberg,KSimonyan,AVedaldi,andAZisserman. Syntheticdata
classificationwithdeepconvolutionalneuralnetworks.InF.Pereira,C.J. andartificialneuralnetworksfornaturalscenetextrecognition.InNIPS
Burges, L. Bottou, and K.Q. Weinberger, editors, Advances in Neural DeepLearningWorkshop,2014.
Information Processing Systems, volume 25. Curran Associates, Inc., [36] M.Sundermeyer,T.Hodan,Y.Labbe,G.Wang,E.Brachmann,B.Drost,
2012. C.Rother,andJ.Matas.Bopchallenge2022ondetection,segmentationand pose estimation of specific rigid objects. arXiv preprint, 2023.
arXiv:2302.13075.
[37] Maximilian Denninger, Dominik Winkelbauer, Martin Sundermeyer,
Wout Boerdijk, Markus Knauer, Klaus H. Strobl, Matthias Humt, and
RudolphTriebel.Blenderproc2:Aproceduralpipelineforphotorealistic
rendering. JournalofOpenSourceSoftware,8(82):4901,2023.