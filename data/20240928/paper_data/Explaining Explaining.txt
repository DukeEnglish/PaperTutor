Explaining Explaining
Sergei Nirenburg1, Marjorie McShane1, Kenneth W. Goodman2, and Sanjay Oruganti1
1 Department of Cognitive Science, Rensselaer Polytechnic Institute, 110 8th St. Troy, NY 12180-3590, USA
2 Institute for Bioethics and Health Policy, Miller School of Medicine, University of Miami, P.O. Box 016960 (M-825)
Miami, FL 33101, USA
{nirens, mcsham2, orugas2}@rpi.edu, KGoodman@med.miami.edu
Keywords: Explanation, Explainable AI, Cognitive Modeling
Abstract: Explanation is key to people having confidence in high-stakes AI systems. However, machine-learning-based
systems – which account for almost all current AI – can’t explain because they are usually black boxes. The
explainable AI (XAI) movement hedges this problem by redefining “explanation”. The human-centered
explainable AI (HCXAI) movement identifies the explanation-oriented needs of users but can’t fulfill them
because of its commitment to machine learning. In order to achieve the kinds of explanations needed by real
people operating in critical domains, we must rethink how to approach AI. We describe a hybrid approach to
developing cognitive agents that uses a knowledge-based infrastructure supplemented by data obtained
through machine learning when applicable. These agents will serve as assistants to humans who will bear
ultimate responsibility for the decisions and actions of the human-robot team. We illustrate the explanatory
potential of such agents using the under-the-hood panels of a demonstration system in which a team of
simulated robots collaborates on search task assigned by a human.
1 INTRODUCTION book was titled “The Nature of Explanation”
discusses a variety of the senses of this suitcase word
and, among other things, stresses the distinction
Explanation is clearly one of Marvin Minsky’s
between causal explanation in terms of a formal world
“suitcase” words “that we use to conceal the
model (what would be later termed ontology) and
complexity of very large ranges of different things
statistical explanation seeking to explain by pointing
whose relationships we don’t yet comprehend
out uninterpreted relations among observable entities.
(Minsky 2006, p. 17). The Stanford Encyclopedia of
The above distinction still remains in the
Philosophy includes detailed entries for mathematical,
spotlight today. Most current generative AI systems
metaphysical and scientific explanation and a
are black boxes whose functioning cannot be
separate one on causal approaches to the latter.
explained in normal human terms. For certain
Specialist philosophical literature is devoted to
applications, this is not a problem:
discussions of Carl Hempel’s (1965) deductive-
nomological model of explanation and the rival
1. Non-critical AI capabilities – such as GPS
inductive-statistical approaches. Explanation is also
systems, machine translation systems, and
discussed in other disciplines, e.g., psychology (e.g.,
search engines – are widely and happily
Lombrozo 2010). Special attention is also paid to the
differences between explainability and employed by end users who don’t require
interpretability, transparency, explicitness and explanations.
faithfulness. (e.g., Rosenfeld and Richardson 2019). 2. AI capabilities that emulate physical rather
Recent years have also seen a pronounced interest in than cognitive capabilities – such as robotic
developing novel explanation theories (Yang et al. movement and speech recognition – are
2022, Rizzo et al. 2023). incompatible with the kinds of user-
Explanation in AI has a long history as well. elucidating explanations we address here.
Arguably the first AI-related contribution was Craik That is, we all understand that it would be
(1943). Kenneth Craik was a psychologist and an pointless to ask a robot exactly how it
early cyberneticist whose work influenced AI and extends its arm or keeps its balance when
cognitive science (Boden 2006, pp. 210-218). His walking on uneven surfaces.3. AI systems that emulate cognitive discussion, see McShane, Nirenburg, and English,
capabilities (e.g., X-ray analysis systems) 2024 Section 2.7.1.)
can be useful, despite their lack of The unexplainability problem has been addressed in
explainability, as long as they are leveraged earnest by the Explainable-AI (XAI) movement
as orthotic assistants rather than autonomous (Finzel et al. 2023, Bodria et al. 2021, Cambria et al.
systems. This requires that domain experts 2022, Nagahisarchoghaei et al. 2023, Schwalbe and
Finzel 2023), but the results are, in general, less than
understand the reliability and deficits of the
satisfying (Barredo et al. 2020). XAI investigators
technology well enough to judiciously
hedge the explainability problem by redefining
incorporate it into their workflow (Chan &
“explanation” (Gunning, 2017; Mueller et al., 2019).
Siegel, 2019; Nirenburg, 2017).
XAI research does not seek to explain how systems
arrived at their output. Instead, it concentrates on
Fig. 1. The demonstration view of the robotic search-and-retrieve system at the end of the system run. The simulation
window is to the left, the dialog window is in the middle, and four of many available under-the-hood panels are to the right.
This figure is intended only to show the look-and-feel of the system; subsequent figures of individual components will be
legible.
By contrast, lack of explainability is a problem for “post hoc algorithmically generated rationales of
ML-based AI systems that are designed to operate black-box predictions, which are not necessarily the
autonomously in critical domains. For example, as of actual reasons behind those predictions or related
June 2021, the FDA cleared 343 AI/ML-based causally to them... [and which] are unlikely to
medical devices, with over 80% of clearances contribute to our understanding of [a system’s] inner
occurring after 2018 (Matzkin, 2021). This supply of workings” (Babic et al., 2021).
new AI systems has continued unabated even though The related human-centered explainable AI
their adoption has been less than enthusiastic. Fully (HCXAI) movement, for its part, identifies the
70% of these devices offer radiological diagnostics explanation-oriented needs of users but is hampered
and typically claim to exceed the precision and in fulfilling them because of its commitment to
efficiency of humans. But, according to Gary Marcus machine learning (Babic et al., 2021; Ehsan et al.,
(2022), as of March 2022, “not a single radiologist 2022; Liao & Varshney, 2022).
has been replaced.” So, regulators keep approving
The solution to the problem of unexplainable ML-
systems whose operation cannot be explained, and
based AI is not to keep trying to square that circle:
developers keep hoping that their systems, though
generative AI techniques are, and will remain,
unexplainable, will be adopted. (For further
unexplainable. The solution is to step back and
reconsider how to develop AI systems so that they areexplainable to the degree, and in the ways, that are LEIAs are implemented in a dual-control
necessary for different types of applications. cognitive-robotic architecture that integrates
Intelligent behavior by humans and AI agents strategic, cognitive-level decision-making with
involves a variety of different capabilities of tactical, skill-level robot control (Oruganti et al.,
perception, reasoning, decision-making and action. 2024). The strategic (cognitive) layer relies primarily
Some of them are arguably better fit to be on knowledge-based computational cognitive
modeling for interpreting perceptive inputs,
implemented using generative AI approaches, some
reasoning, decision-making, learning, etc. The
others, by symbolic AI approaches. Therefore, hybrid
tactical (robotic, skill-level) module relies on data-
AI systems are better suited for comprehensive (non-
driven tools for recognizing perceptive inputs and
silo) applications than either of the above approaches
rendering actions.
alone. This observation was first made over 60 years
LEIAs can explain their operation because they
ago (Minsky 1961) and has finally received due
are configured using human-inspired computational
attention in the field. Indeed, hybrid “neurosymbolic”
cognitive modeling. Their explanations make clear
AI architectures are at present one of the foci of work
the relative contributions of symbolic and data-driven
in AI (Hitzler et al. 2023).
methods, which is similar to a human doctor
Our team is developing a family of hybrid explaining a recommended procedure using both
cognitive systems that we call LEIAs: Language- causal chains, such as how the procedure works, and
Endowed Intelligent Agents. Our work is a part of the population-level statistics, such as the percentage of
movement toward integrating empirical (deep patients for whom it is curative.
learning-based) and deductive/rational (knowledge-
based) approaches to building intelligent agent
systems. Explanation is an important component of
3 EXPLANATION VIA UNDER-
such systems (de Graaf et al. 2021).
THE-HOOD PANELS
We believe that explanations are needed only for
actions, plans, attitudes and expectations that are of
interest to users and collaborators of AI agents, not As detailed in McShane, Nirenburg, and English
their developers. It follows that generative AI (2024, chapter 8, “Explaining”), there are many
methods are most useful for implementing system things that a LEIA can explain (what it knows, how it
modules for whose behavior explanations are not interpreted an input, why it made a given decision,
typically required, such as motor control and etc.) and there are many ways to present explanations
uninterpreted perception. For any process that to people. Although the most obvious way is through
requires explainable reasoning (perception language, other expressive means can be even more
interpretation, decision-making, action specification, useful in some contexts. One such way is by
etc.) and any application where confidence in the dynamically showing traces of system operation
system output is important, the black-box LLMs are using what we call under-the-hood panels.
not a good fit. Accordingly, LEIAs use LLMs We first introduced under-the-hood panels in the
whenever this simplifies or speeds up work on tasks Maryland Virtual Patient (MVP) proof-of-concept
that do not require explanation and where clinician training application (McShane et al., 2008;
hallucinations are either benign or expected to be McShane & Nirenburg, 2021). There, the under-the-
detected by human or system inspectors. hood panels showed traces of the physiological
In what follows we briefly illustrate some of the simulation of the virtual patient, the patient’s
explanation capabilities of LEIAs. interoception, its thoughts, the knowledge it learned,
and how it interpreted text inputs from the user, who
was playing the role of attending physician. These
insights into system functioning were geared at
2 LEIAs
earning the trust of medical educators, who would
ultimately need to choose to incorporate such a
The LEIA program of R&D is a theoretically system into their pedagogical toolbox.
grounded, long-term, effort that has two main
emphases: developing cognitive robotic systems 3.1 A search-and-retrieve request
whose capabilities extend beyond what machine
learning alone can offer, and earning people’s trust in
We will illustrate the explanatory power of under-the-
those systems through explainability (McShane,
hood panels using a new system in which two
Nirenburg, & English, 2024).
simulated robots, a drone and a ground vehicle, workas a team to fulfill a search-and-retrieve request by a Fig. 3 shows the UGV’s TMR of Danny’s request
person (Fig. 1). A human named Danny, who is and Fig. 4 shows its subsequent thoughts, which are
located remotely, asks the team of robots – a drone natural language traces of its reasoning for the benefit
and ground vehicle (UGV) – to find keys that he lost of humans. (It reasons in the ontological
in his apartment. Danny communicates with the UGV metalanguage.)
since it is serving as the robotic team leader, with the
drone as its subordinate. The full dialog, which we’ll
walk through, is shown in Fig. 2.
Fig. 3. The UGV's interpretation of Danny's request.
Fig. 4. The UGV's thoughts in response to Danny's
request.
Because the UGV has received a request for action,
and because it knows that has a helper (the drone), it
places a COLLABORATIVE-ACTIVITY on its Agenda
(Fig. 5). Before it launches the plan for SEARCH-FOR-
LOST-OBJECT, it has to check if its preconditions are
met (Fig. 5). The first precondition, knowing the
object type, is already met (i.e., keys), but the second
and third are not: knowing the keys’ features and
knowing where they were last seen; so the UGV asks
about these things in turn. The reasoning associated
Fig. 2. The full dialog in the demo. The prompt for typing
with this sequence of actions is shown in Fig. 6.
is at the bottom.
When Danny issues his request, both robots
semantically interpret the input, resulting in text-
meaning representations (TMRs) that are written in
the ontologically-grounded metalanguage used for all
agent knowledge, memory, and reasoning. This
metalanguage uses unambiguous ontological
concepts (not words of English) and their instances,
described by ontologically-grounded properties and
values. Ontological concepts are written in small caps
to distinguish them from words of English, and their
instances are indicated by numerical suffixes. The Fig. 5. The UGV's agenda while it is fulfilling
process of language understanding is complicated, as preconditions for searching for the keys.
detailed in McShane and Nirenburg (2021).Fig. 6. The UGV’s thoughts as it fulfilling preconditions
for searching for the keys.
The UGV then proposes the plan of searching the
apartment to the drone, the drone agrees, and it
Figure 8. The room layout just before the robots being
launches a plan to do that. Its thoughts – including
searching, with the robots and keys shown in green boxes.
those running up to this move – are shown in Fig. 7.
The robots are equipped with sensors to detect,
identify, and tag objects, and to combine this data to
localize objects and themselves. Interpreted traces of
what they are seeing are shown in visual meaning
representations (VMRs) in the associated under-the-
hood panel. VMRs are similar in form and content to
TMRs since, no matter what an agent perceives or
which mode of perception it uses (hearing, vision,
etc.), it has to interpret the stimuli into ontologically-
grounded knowledge that feeds its reasoning. Fig. 9
shows a moment when the UGV is looking at a
particular spot on the blue-green striped carpet.
Figure 7. The drone's thoughts leading up to and including
its starting to search the apartment.
Since the robots in this simulation are decentralized,
each having its own cognitive layer, the drone
independently carries out much of the same reasoning
as the UGV. (Note that our architecture also permits
centralized robots that share a cognitive layer.)
Having agreed upon a plan, the UGV and the
drone leave their charging stations, highlighted in
green in Fig. 8, and begin searching the apartment.
Fig. 9. A visual meaning representation (VMR).
The robots engage in a search strategy involving
waypoints, zones and sub-zones that are pre-
designated for the apartment environment (Oruganti
et al., 2024). The search action is triggered through
action commands from the strategic layer but thesearch itself is controlled by the tactical (robotic) interpretation of language inputs and visual stimuli,
layer. The cognitive (strategic) module knows which their reasoning, and their agenda. Much more could
zones exist but does not guide how the robots be shown if target users would find that helpful: the
maneuver through those zones. The simulation agents’ ontologies, episodic memories, lexicons,
system is equipped with timing strategies and decision-making about language generation, and so
modules to ensure process and data synchronization on. The under-the-hood panels do not attempt to
capture unexplainables that are implemented using
between the tactical and strategic layers.
machine learning, such as what drives robotic
Searching each zone is a subtask of the plan FIND-
movement or the robots’ approach to searching a
LOST-OBJECT. After completing each subtask – i.e.,
space.
searching each zone – each robot reports to the other
In the current benchmark-driven climate, under-
one about whether it was successful, which is driven
the-hood panels offer an alternative as a standard of
by the COLLABORATIVE-ACTIVITY plan.
evaluation.
When the team leader finds the keys, it ceases
Under-the-hood panels are just one mode of
searching and first reports this to its subordinate and
evaluation for LEIAs. The other primary one is
then to Danny. The trace of this reasoning is shown in
language. The many things that a LEIA can explain
Fig. 9. It uses different formulations for each of them using language are detailed in Chapter 8 of McShane,
because its language generation system (whose traces Nirenburg, and English.
are not shown in this demo system) is designed to Although the theoretical, methodological, and
mindread its interlocutors and present information in knowledge prerequisites for explanation by LEIAs
the most useful way for them. Whereas these robots are quite mature, this doesn’t mean that all problems
operate in terms of cardinal directions, making north associated with explanation are solved.
of the couch a good descriptor, most humans prefer Consider the example of physicians explaining
relative spatial terms like behind the couch. relevant aspects of clinical medicine to patients, a
capability that was relevant for the MVP clinician-
training system mentioned above. The task has two
parts: deciding what to say and how to say it. Both of
these depend not only medical and clinical
knowledge, but also on the salient features of patients,
such as their health literacy (as hypothesized by the
physician), their interest in medical details, their
ability to process information based on their physical,
mental, and emotional states, and so on. Identifying
these salient features involves mindreading
(Spaulding, 2020) – also known as mental model
ascription. For example, an explanation may be
presented in many different ways:
• as a causal chain: “You feel tired because of an
iron deficiency.”
• as a counterfactual argument: “If you hadn’t
stopped taking your iron supplement you
Figure 10. The UGV's thoughts when it finds the keys and
wouldn’t be feeling so tired.”
decides to report that.
• as an analogy: “Most people find it easier to
remember to take their medicine first thing in
the morning; you should try that.”
4 CONCLUSIONS • using a future-oriented mode of explanation:
“If you take your iron supplement regularly,
Explainability is essential to critical applications, you should feel much more energetic.”
and in order for systems to be truly explanatory, they
must first of all understand what they are doing. This Moreover, explanations are not limited to speech –
requires that they be grounded in high-quality they can include images, videos, body language, live
knowledge bases that optimally integrate causal and demonstration, and more. Overall, generating
correlational reasoning. explanations tailored to particular humans is a
This paper focused on explanation via traces of difficult task. However, as with all other aspects of
system operation using under-the-hood panels. The cognitive modeling, simplified solutions hold
panels selected for this demo displayed the agents’ promise to be useful, particularly given the well-established fact that adding more content to an concepts. Data Mining and Knowledge Discovery.
explanation does not necessarily make it better https://doi.org/10.1007/s10618-022-00867-8
(Kahneman, 2011). Gunning, D. (2017). Explainable artificial intelligence
(XAI). DARPA/I20. Program Update November
2017.
ACKNOWLEDGEMENTS Hempel, Carl G. 1965. Aspects of Scientific Explanation.
In Hempel, Carl G. 1965. Aspects of Scientific
Explanation and Other Essays in the Philosophy of
This research was supported in part by Grant
Science Free Press, pp. 331– 396.
#N00014-23-1-2060 from the U.S. Office of Naval
Research. Any opinions or findings expressed in this Hitzler, P., M.K. Sarker and A. Eberhart (eds). 2023.
material are those of the authors and do not Compendium of Neurosymbolic Artificial
necessarily reflect the views of the Office of Naval Intelligence. Frontiers in Artificial Intelligence and
Research. Applications, vol. 369. IOS Press.
Kahneman, D. (2011). Thinking: Fast and slow. Farrar,
Strauss and Giroux.
REFERENCES Liao, V. Q., Varshney, K. R. (2022). Human-centered
explainable AI (XAI): From algorithms to user
experiences. arXiv:2110.10790.
Babic, B., Gerke, S., Evgeniou, T., Cohen, I. G. (2021).
Beware explanations from AI in health care. Science, Lombrozo, Tania, 2010, “Causal–Explanatory Pluralism:
373(6552), 284–286. How Intentions, Functions, and Mechanisms Influence
Causal Ascriptions”, Cognitive Psychology, 61(4):
Barredo Arrieta, A., N. Díaz-Rodríguez, J. Del Ser, A.
303–332. doi:10.1016/j.cogpsych.2010.05.002
Bennetot, S. Tabik, A. Barbado, S. Garcia, S Gil-
Lopez, D. Molina, R. Benjamins, R. Chatila, F. Marcus, G. (2022, March 10). Deep learning is hitting a
Herrera. 2020. Explainable Artificial Intelligence wall. Nautilus. https://nautil .us/deep-learning-is-
(XAI): concepts, taxonomies, opportunities and hitting-a-wall-14467
challenges toward responsible AI. Information Fusion
Matzkin, A. (2021, September 29). AI in Healthcare:
548: 82-115. Insights from two decades of FDA approvals. Health
Boden, M. 2006. Minds as Machines. Oxford University Advances blog.
Press. McShane, M., Jarrell, B., Fantry, G., Nirenburg, S., Beale,
Bodria, F., F. Giannotti, R. Guidotti, F. Naretto, D. S., Johnson, B. (2008). Revealing the conceptual
Pedreschi, and S. Rinzivillo. 2021. Benchmarking and substrate of biomedical cognitive models to the wider
Survey of Explanation Methods for Black Box community. In J. D. Westwood, R. S. Haluck, H. M.
Models. Arxiv:2102.13076v1. Hoffman, G. T. Mogel, R. Phillips, R. A. Robb, & K.
G. Vosburgh (Eds.), Medicine meets virtual reality 16:
Chan, S., Siegel, E. L. (2019). Will machine learning end
Parallel, combinatorial, convergent: NextMed by
the viability of radiology as a thriving medical
design (pp. 281–286). IOS Press.
specialty? British Journal of Radiology, 92(1094).
https://doi.org /10.1259/bjr.20180416 McShane, M., Nirenburg. S. (2021). Linguistics for the age
of AI. MIT Press. Available, open access, at
Craik, K. J. W. (1943), The Nature of Explanation.
https://direct.mit.edu/books/book/5042/Linguistics-
Cambridge University Press.
for-the-Age-of-AI.
De Graaf, M. M. A., A. Dragan, B. F. Malle, and T. Ziemke.
McShane, M., Nirenburg, S., English, J. (2024). Agents in
2021. Introduction to the Special Issue on Explainable
the Long Game of AI: Computational cognitive
Robotic Systems. J. Hum.-Robot Interact. 10, 3,
modeling for trustworthy, hybrid AI. MIT Press.
Article 22. https://doi.org/10.1145/3461597
Minsky, M. 1961. Steps Toward Artificial Intelligence.
Ehsan, U., Wintersberger, P., Liao, Q. V., Watkins, E. A.,
Proceedings of the Institute of Radio Engineers, 49: 8–
Manger, C., Daumé, Hal, III, Riener, A., Riedl, M. O.
30; reprinted in Feigenbaum and Feldman, (eds.)
(2022). Human-centered explainable AI (HCXAI):
Computers and Thought. McGraw-Hill. 1963.
Beyond opening the black-box of AI. CHI EA ’22:
Extended abstracts of the 2022 CHI Conference on Minsky. M. 2006. The Emotion Machine. Simon and
Human Factors in Computing Systems, pp. 1–7. Schuster.
Association for Computing Machinery.
Mueller, S. T., Hoffman, R. R., Clancey, W., Emrey, A., &
https://doi.org/10.1145/3491101.3503727 Klein, G. (2019). Explanation in human-AI systems: A
Finzel, B. and G. Schwalbe. 2023. A comprehensive literature meta-review. Synopsis of key ideas and pub-
taxonomy for explainable artificial intelligence: a lications, and bibliography for explainable AI.
systematic survey of surveys on methods andTechnical Report, DARPA Explainable AI Program.
arXiv:1902.01876.
Nagahisarchoghaei, M., N. Nur, L. Cummins, M.M.
Karimi, S. Nandanwar, S. Bhattacharyya, and S.
Rahimi. 2023. An Empirical Survey on Explainable AI
Technologies: Recent Trends, Use-Cases, and
Categories from Technical and Application
Perspectives. Electronics 2023, 12 1092.
https://doi.org/10.3390/ electronics12051092
Nirenburg, S. (2017). Cognitive systems: Toward human-
level functionality. Special Issue on Cognitive
Systems. Artificial Intelligence Magazine, 38(4), 5–
12. https://doi .org/10.1609/aimag.v38i4.2760
Oruganti, S.S.V., S. Nirenburg, M. McShane, J. English, M.
Roberts & C. Arndt. (Submitted). HARMONIC:
Cognitive and Control Collaboration in Human-
Robotic Teams. Submitted to ICRA-2025.
Rizzo, M, A. Albarelli, C. Lucchese and C. Conati. 2023. A
Theoretical Framework for AI Models Explainability.
Proceedings of IJCAI-23.
Rosenfeld, A. and A. Richardson. 2019. Explainability in
human–agent systems. Autonomous Agents and Multi-
Agent Systems 33 (2019): 673 - 705.
Schwalbe, G. and B. Finzel. 2023. A comprehensive
taxonomy for explainable artificial intelligence: a
systematic survey of surveys on methods and
concepts. Data Mining and Knowledge Discovery.
https://doi.org/10.1007/s10618-022-00867-8
Spaulding, S. (2020). What is mindreading?
Interdisciplinary Review of Cognitive Science.
https://wires.onlinelibrary.wiley.com/doi/10.1002/wc
s.1523
Yang, S. C-H., T. Folke and P. Shafto. 2022. A
Psychological Theory of Explainability.
arXiv:2205.08452.