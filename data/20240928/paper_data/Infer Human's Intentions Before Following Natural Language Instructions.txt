Infer Human’s Intentions Before Following Natural Language Instructions
YanmingWan1,YueWu1,YipingWang1,JiayuanMao2*,NatashaJaques1*
1UniversityofWashington,Seattle,WA98195
2MITCSAIL,Cambridge,MA02139
{ymwan,nj}@cs.washington.edu,jiayuanm@mit.edu
Abstract etal.1995).Resolvingthisambiguitydependsonleveraging
othersourcesofinformation(e.g.,humaninternalgoalsand
ForAIagentstobehelpfultohumans,theyshouldbeable
historicalactions)thatarepartiallyobservabletotherobot.
tofollownaturallanguageinstructionstocompleteeveryday
cooperativetasksinhumanenvironments.However,realhu- ConsidertheexampleshowninFig.1,whereahumanis
maninstructionsinherentlypossessambiguity,becausethe tidyinguparoom.Inthemiddleofheractions,sheasksa
humanspeakersassumesufficientpriorknowledgeabouttheir robotforhelp,saying“Couldyoupassthatfromthesofa?”
hiddengoalsandintentions.Standardlanguagegroundingand Thisinstructiondoesnotappeartobesolvablewithoutfurther
planningmethodsfailtoaddresssuchambiguitiesbecause informationabouttheperson’sunderlyingintention.While
they do not model human internal goals as additional par-
suchinternalmentalstatesarenotdirectlyobserved,agents
tially observable factors in the environment. We propose a
can infer them from human’s past actions. Specifically, if
newframework,FollowInstructionswithSocialandEmbod-
therobotcanobservethatinprevioussteps,thehumanput
iedReasoning(FISER)*,aimingforbetternaturallanguage
several books into a box one by one, it can infer that she
instruction following in collaborative embodied tasks. Our
frameworkmakesexplicitinferencesabouthumangoalsand intendstousethatboxtostoreallthebooks.Basedonthis
intentionsasintermediatereasoningsteps.Weimplementa guess,therobotcancheckifthereareanyremainingbooks
setofTransformer-basedmodelsandevaluatethemovera onthesofaandthenhandthemtotheperson.
challengingbenchmark,HandMeThat.Weempiricallydemon- Generallyspeaking,theambiguityintheinstructionmainly
strate that using social reasoning to explicitly infer human arisesfromtwoaspects.First,thehumanassumessufficient
intentionsbeforemakingactionplanssurpassespurelyend-to-
priorknowledgeaboutherhiddenintentions(Dennett1987;
endapproaches.Wealsocompareourimplementationwith
Gergelyetal.1995),whichisbasedonthecommonsense
strongbaselines,includingChainofThoughtpromptingon
knowledgethatpeopletendtogroupsimilaritemstogether
thelargestavailablepre-trainedlanguagemodels,andfindthat
FISERprovidesbetterperformanceontheembodiedsocial whentidyingup,andtheobservationthatthehumanisgath-
reasoningtasksunderinvestigation,reachingthestate-of-the- eringbooks.Second,peoplemaketrade-offsbetweenaccu-
artonHandMeThat. racyandefficiencyofcommunication(Grice1975;Sperber
andWilson1986;Clark1996).Thisleadstothechallenge
ofbuildingAIagentsthatcanfollowefficient,ambiguous
1 Introduction speechthatpeoplenaturallyadoptwhengivingdirections.
Weconsiderthecasewherethere’sahumanandarobot
Building AI assistants that can interact with people in a
collaboratinginasharedenvironment.Thehumanisworking
sharedenvironmentandfollowtheirinstructionswouldun-
onsometasks,andspecifiesasub-taskfortherobottohelp
lockassistiveroboticsandfreeupdomesticlabor.Toward
withbygivinganaturallanguageinstruction.Pastmethods
this broad goal, we need to address the problem of “trans-
(e.g.,languagegrounding)attempttodirectlycompletethe
lating” realistic natural language instructions into actions
specified command from the given instructions, since the
executablebyrobots.Theconventionalwaythatpeoplefor-
humanonlyactsasadisembodiedissuerofinstructionsand
mulatethisproblemisgroundedlanguagelearning,which
isnotanotheractiveagentintheirenvironments.Thehuman,
aims at mapping abstract natural language phrases to con-
asanotherpartiallyobservablefactorintheenvironment,has
cretelyexecutableactions.However,theseapproachesmiss
beenoverlooked.Inthispaper,wepresentanewframework,
animportantcomponentofmanyhuman-robotcollaborative
Follow Instructions with Social and Embodied Reasoning
tasks,whichisthatthelanguagehumanstendtouseinev-
(FISER), which suggests that we should introduce the hu-
erydayscenariosisinherentlyambiguous.Humanspeakers
man’sintentionasexplicitvariablesforthemodeltodraw
assumethatlistenerspossesspriorknowledge,leadingthem
inferencesabout.Byleveragingthisstructure,ourframework
toomitcertaininformationforefficiency(Grice1975;Sper-
optstodecomposetheproblemintotwoparts–socialreason-
ber and Wilson 1986; Clark 1996; Dennett 1987; Gergely
ingandembodiedreasoning.Specifically,socialreasoning
*Theseauthorscontributedequally. isaimedatpredictingthesub-taskforwhichthehumanis
*Projectwebsite:https://sites.google.com/view/fiser-hmt/ askingforassistance,whichcanbeinferredfromthecontext
4202
peS
62
]IA.sc[
1v37081.9042:viXraStandard Language Grounding and Planning
Objects on the sofa:
Could you pass that from the sofa?
Move to the sofa
Pick up THAT?
Follow Instructions with Social and Embodied Reasoning (FISER)
Historical Actions of
Human is…
Give
storing books Move to the sofa
a book on the sofa
into a box
Pick up
table shelf
Human’s Plan Recognition (optional) Robot’s Task Recognition Grounded Planning
Phase 1. Social Reasoning Phase 2. Embodied Reasoning
Figure 1: An example scenario where the human’s natural language instruction (“Could you pass that from the sofa?”) is
inherentlyambiguous.Standardlanguagegroundingandplanningmethodsfailtoresolveambiguity.WeproposeFISER,which
explicitlyreasonsabouthuman’sinternalintentionsasintermediatesteps.Therobotdisamiguatestheinstructionintoaconcrete
robot-understandabletaskinthesocialreasoningphase(Phase1),andthenaccomplishesthegroundedplanningintheembodied
reasoningphase(Phase2).WefurtherproposeanoptionalenhancementtoPhase1byexplicitlyrecognizethehuman’soverall
planfirst,andtheninferwhatthehumanwantstherobottodo.
of both the instruction and the observed historical actions soningabouthumanintentionsisbeneficial.Second,training
of the person in the shared environment. After grounding small-scale models from scratch on this task outperforms
the instructions into robot-understandable tasks, the robot our most sophisticated CoT prompting methods for large
canthendoplanningandinteractwiththeenvironment,ina pre-trained LLMs, indicating that pretraining and domain-
separateembodiedreasoningphase.Tofurtherenhancethe specificpromptsareinsufficientforLLMstoperformwellon
model’sabilitytofollowambiguousinstructions,wepropose thechallengingsocialandembodiedreasoningtasksunder
to explicitly add an extra plan recognition stage, where a investigation.Weconductathoroughanalysisoverthefailure
set of logical predicates is used to help with inferring the modesofGPT-4Turboandgaininsightintowhystate-of-the-
human’s overall plan. Weimplement a Transformer-based artLLMsarenotcapableofthistypeofsocialandembodied
model trained in a supervised learning manner to predict reasoningskillsneededtosolvethetasks.
specified sub-tasks (and the human’s underlying plan) at Tosummarize,thecontributionsofthispaperaretopro-
intermediatelayers.Thisstep-by-stepapproachdistinctlydif- posetheFISERframework,whichperformsinstructionfol-
fersfromthemorecommonlyemployedend-to-endmethods lowing by first using social reasoning and additional con-
inpreviousworks. text to disambiguate what the human is asking, before us-
Overall,thekeyinsightofourmethodisthatseparating ing embodied reasoning to decide what actions to take
social and embodied reasoning by explicitly modeling the to complete the task. We further introduce a human plan
human’sintentionscansignificantlyimproveperformance recognitionstagetoenhancesocialreasoningabilitieswhen
whenfollowingambiguousnaturallanguageinstructions.To tasksareparticularlycomplexorambiguous.Weempirically
testthishypothesis,weevaluateourmodelsonachallenging demonstrate that our FISER models show 64.5% success
benchmark, HandMeThat (HMT) (Wan, Mao, and Tenen- rate on the test set on average, achieving the state-of-the-
baum2022),whichinvolvesambiguousinstructionfollowing art on HMT benchmark. Open-source code is available at:
tasksinatext-basedhouseholdenvironment.HMTcontains https://github.com/Simon-Wan/FISER.
alargenumberofphysicalobjectsandvalidactionsineach
episode,aswellasanenormoushumangoalspace.Wefind
2 RelatedWork
that these properties make HMT challenging even for the
largest state-of-the-art large language models (LLMs). As
2.1 Groundedlanguagelearning
acompetitivebaseline,wealsodesignaChain-of-Thought
(CoT)approachtopromptGPT-4basedonourframework. In order for AI to be useful to people in our homes and
The experimental results reveal two important findings. naturalenvironments,non-expertsneedtobeabletocommu-
First,modelswhichseparatesocialandembodiedreasoning nicatewithAIagentsusingnaturallanguage.Thisissuehas
usingtheFISERframeworkoutperformend-to-endreason- longcapturedtheattentionofresearchers(Winograd1972;
ing,inbothTransformer-basedmodelsandCoT-prompted Siskind1994),andtheprimarychallengeinvolvesmapping
LLMs,whichindicatesthatexplicitlydoingintermediaterea- naturallanguagetoconcretemeaningswithinthephysicalen-
2vironment.Severalstudiesexplorelanguage-conditionedtask 2024; Zhang et al. 2024) leverage LLMs to conduct goal
completioninspecificenvironments(Shridharetal.2020; inferencebasedontheobservedhumanactionsormessages.
Sugliaetal.2021;Kojima,Suhr,andArtzi2021).Withthe Inthiswork,weemployasmalllanguagemodeltrainedfrom
emergenceofLLMs,manyworksdiscussedgroundinglan- scratchtoundertakethispartofthereasoning,sincetheaim
guagebyleveragingpre-trainedLLMs (Blukisetal.2021; isnottoachieveprecisegoalrecognitionbuttoassistwith
Nairetal.2022;Zellersetal.2021).Aprominentexample thestep-by-stepsocialreasoningprocess.
isSayCan(Ahnetal.2022),whichproposedextractingthe
knowledgeinLLMsbyusingthemtoscorethelikelihood 2.4 Reasoningwithintermediatesteps
thatasubtaskavailabletotherobotwillhelpcompleteahigh- Thisworkisalsoinspiredbytheresearchthatusesintermedi-
levelinstruction.Althoughtheabovestudiesmayincorporate atestepstosolvecomplexreasoningproblems,includingfor-
common sense reasoning about language as well as infor- malandmathematicalreasoningandprogramsynthesis(Roy,
mation within the physical environment, their instructions Vieira,andRoth2015;Aminietal.2019;ChiangandChen
explicitlyexpresshumanintentions.Forexample,themost 2019;Chenetal.2020;Nyeetal.2021).Specifically,Nye
ambiguous instruction solved by SayCan is, “I spilled my etal.showsthatstep-wisepredictionmethodperformsbetter
coke,canyoubringmesomethingtocleanitup?”wherethe thandirectlypredictingthefinaloutputsinprogramsynthesis
ambiguitycanstillbeeasilyresolvedgiventhatthesponge whenpromptingLLMs.Chain-of-Thought(CoT)(Weietal.
istheonlycleaningtoolintheenvironment.Incontrast,we 2022)thoroughlyexploreshowgeneratingintermediaterea-
addresstheproblemthatrealistichumaninstructionsomit soningstepsimprovestheperformancesofpromptingLLMs
certaininformationforefficiency,makingthemmuchmore todealwithcomplexreasoningtasks.Inthispaper,weshow
ambiguous,andnecessitatinginferringhumanintentionsto thatsocialreasoningtasksbenefitfromthesameapproaches,
fillinthegaps. anddemonstratethatinferringhumanintentionsisacritical
componentofsuccessfulhuman-robotcollaboration.
2.2 Collaborativecommunication
We consider the case where the human and the robot are
3 FISER:FollowInstructionswithSocialand
workinginasharedenvironment,whichiscloselyrelatedto
EmbodiedReasoning
theliteratureoncollaborativecommunication(e.g.TwoBody
Problem(Jainetal.2019)).CerealBar(Suhretal.2019),Di-
3.1 ProblemFormulation
alFRED(Gaoetal.2022)andTEACh(Padmakumaretal.
2022)introducecollaborativetaskswherethehumanworks Ahuman-robotMarkovDecisionProcessisdescribedasa
asadisembodiedissuerofinstructions,possiblyresponding tuple ⟨S,Ah,r,T,U,Rr,γ,T⟩. s ∈ S are object-oriented
torobot’squestionsviaexplicitmessages.Incontrast,wecon- statesincludingthelocations,statusandtypeofeachobject
sidertheprobleminwhichtheAIassistantneedstoconsider andagent.Ah,r isthejointactionspacewithAh,Ar being
bothexplicitmessagesinnaturallanguageandtheimplicit thesetsofactionsavailabletothehumanandtherobot,re-
informationinobservedhumanactions.Further,weassume spectively. T : S ×Ah,r ×S → {0,1} is the transition
thatinstructionsarenotexhaustivelydescribingtherequired function where T(s,ah,r,s′) = 1 if and only if taking ac-
information,butaregeneratedbasedonatrade-offbetween tions ah,r at state s gives s′ as the next state. U is a set of
informativeness and communication cost. To this end, we instructionsthatthehumancangivetotherobot.Rr :S →R
focusontheHandMeThat(Wan,Mao,andTenenbaum2022) is a reward function for the robot, γ is the discount factor,
(HMT)benchmark,thatcallsfortheabilitytoconsiderboth andT isthehorizon.Throughoutthepaper,weconsidera
explicitandimplicitmessageswhenfollowingambiguous scenariowithonlyasingleroundofinstructionfollowingfor
instructions. The previous state-of-the-art work (Cao et al. therobot.Ineachepisode,startingfromaninitialstates 0,
2024) on HMT performs iterated goal inference over the thehumanbeginsworkingintheenvironment,andtherobot
goalspaceinsymbolicrepresentation.However,itrequires iswaiting.Humanstopsatatimestept′ ≤ T,leadingtoa
hand-crafted, pre-defined structures and extensive domain trajectoryτ t′ =(s 0,ah 0,s 1,ah 1,...,s t′−1,ah t′−1)andafinal
knowledge,whichisnotapplicableinreal-worldscenarios. states t′.Thenthehumanproducesanaturallanguageinstruc-
tionu∈U thataskstherobotforhelp.Givenτ ,s andu,
t′ t′
2.3 Goalrecognition therobotneedstointeractwiththeenvironmentbytaking
asequenceofactions{ar} tomaximizeitsdiscounted
In our method, we hope to infer the human’s intentions t t≥t′
based on the observed historical actions, which is related rewards(cid:80)T t=t′[γt−t′Rr(s t+1)].
togoalrecognitionproblem(LeshandEtzioni1995;Baker,
3.2 ModelingtheHuman’sIntentions
Tenenbaum,andSaxe2007;Levesque2011;Meneguzziand
Pereira2021).Mostoftheworksarebasedontheassumption A straightforward solution to the human-robot MDP may
ofrationalethatanagentshouldmake(approximately)op- treatτ anduasadditionalstateinformation.However,in
t′
timaldecisionstowardsthegoalseverystep(Dennett1987; reality,τ ,u,andRr haveimportantcorrelations:whenthe
t′
Gergelyetal.1995).Understandinghumanintentionsinem- human is taking actions and producing instructions, their
bodiedenvironmentshasalsobeenstudiedinmanyworks; behavior can be modeled as optimizing for an internal re-
for example in Watch-and-Help (Puig et al. 2021) the AI ward function Rh : S → R, which is not revealed to the
must infer the human’s goal from demonstrations, but no robot.Ourinsightintothisbroadproblemclassistolever-
naturallanguageisinvolved.Somerecentworks(Yingetal. age the causal relation between the human’s behavior and
3Gh
∈
𝒢h
Human’s p p …… p …… p
Overall Plan 1 2 k l
Unfinished Subgoals
a
0
a
1
…… a
t 1−1
a
t 1
a
t 1+1
…… …… a
t′− 1
p* Gr
Selected Subgoal
Robot’s Task
s s …… s s s …… …… s s u
0 1 t 1−1 t 1 t 1+1 t′− 1 t′
Human Trajectory τ Natural Language Utterance
t′
Figure2:Graphforourproblemformulationandproposedmethod.Whitenodesareobservablevariables,whilegreynodesare
unobservable.Therobotisgiventhetrajectoryτ ,afinalstates ,andanutteranceu.Weproposetoexplicitlymodelhuman’s
t′ t′
intentionsbymodelingthehuman’soverallplanGh ∈ Gh asasetofpredicatesp .Wefurtherassumethathumanselectsa
k
subgoalp∗thatneedshelp,andthenspecifiesarobot’staskGr,whichistheunderlyingintentionofhumanwhensayingu.
theirinstructionbyexplicitlymodelinghidden,unobserved be an utterance for Gr = ⟨human-holding(book#0)⟩ and
variablesrepresentingthehuman’sgoalsandintentions.This p∗ =⟨∃y,box(y),∀x,book(x)⇒in(x,y)⟩.
enablesmakingbetteruseofthehuman’strajectorytodisam-
biguatetheinstruction,byrecognizingthetaskthehumanis 3.3 Step-wiseReasoningoverHumanIntentions
assigningtotherobot. Ourmodel,FISER,buildsontopofthefactorizedhuman-
We start by assuming the reward functions Rh can be robot MDP formulation above. We formulate the problem
parameterized by a set of possible goals Gh. The human’s intothesocialandembodiedreasoningphases.
goal Gh ∈ Gh is sampled from an underlying distribution
SocialReasoning:Robot’sTaskRecognition
overGhatthebeginningofanepisode,andisfixedacrossthe
Therobotneedstodisambiguatethenaturallanguagein-
horizon.However,itisnotrevealedtotherobotdirectly.The
structionuintoanunderstandableandexecutabletaskwithin
goalinGhisusuallyglobalandcomplex,suchas“organize
itsowngoalspacebasedontheobservationofcurrentstate
thebedroom.”Weassumethehumantrajectoryτ isrational,
t s and the historical trajectory τ . Therefore, we hope to
in the sense that it was produced to maximize the reward t′ t′
estimateafunctionTR,suchthatTR(s ,τ ,u)→Gr.
Rh(· | Gh).BasedonGh andthecurrentprogressτ ,the t′ t′
t′
humanthenselectsasubgoalp∗ ∈Gh(apartofthehuman’s SocialReasoning:Human’sPlanRecognition.
overallplan)forwhichthehumanwouldliketherobot’shelp, Wefurtherproposeavariantthatexplicitlyestimatesthe
suchas“havingallbooksputinthebox,”andthenspecifies human’sunderlyingoverallplanGhbasedonτ andreplaces
t′
ataskGr fortherobot(e.g.,askingtherobottohandovera thattrajectorybythepredictedgoalwhendoinginstruction
specificbook).TheinstructionuisgeneratedbasedonGr. disambiguation. However, since recognizing the full plan
TherelationsbetweenthesevariablesareillustratedinFig.2. isusuallyintractable,weopttoalsotakeinuands ,and
t′
Although we do not put specific assumptions over the directly predict the predicate p∗ ∈ Gh (subgoal) that the
structure of goals in Gh and how τ is generated, we il- humanwantstherobottohelpwith.Therefore,welearntwo
t
lustrate them with a simplified example in Fig. 2. We de- functions PR and TR, such that PR(s t′,τ t′,u) → p∗ and
fine P as a set of predicates where each p ∈ P is a TR(s t′,p∗,u)→Gr.
classifier over states (to say whether the predicate is sat-
EmbodiedReasoning:GroundedPlanning.
isfied or not). For example, one predicate can be written Once the robot goal Gr is obtained, the problem is re-
as ⟨∃y,box(y),∀x,book(x) ⇒ in(x,y)⟩, which describes
ducedtoapuregroundingandplanningtask.Wecanreplace
puttingallbooksinabox.Nowweassumethatthehuman
the ambiguous natural language instruction u by the accu-
goal Gh = {p 1,p 2,··· ,p l} is a set of predicates. The hu- ratelyexpressedrobotgoalGr.Thefinalgroundedplanning
manchoosestoworkonpredicatesonebyoneandhasbeen functionGPshouldsatisfythatGP(s ,τ ,Gr)→{ar} ,
workingonallp ’s(1 ≤ k ≤ l)beforestoppingattimet′, t′ t′ t t≥t′
k whichisbasicallylearningatypicalgoal-conditionedrobot
and then the subgoal p∗ is chosen from the set of remain- policyπ(a|s ,τ ,Gr)†.
ingpredicates:p∗ ∈{p ,...,p }.Next,thehumanspecifies y′ t′
k l SincethefunctionsTRandPRinvolvenaturallanguage
a robot’s task Gr such that the robot actions will result in
inputs,languagemodelsarerequiredforthesetwomodules.
a state s′ where Rh(s′ | {p∗}) > 0 (i.e., Gr is a useful
ForGP,wecaneitherimplementplanningalgorithmsoruse
steptowardsp∗,aremainingsubgoaltoaccomplish).Note
neuralnetworks,sinceallinputscanbesymbolic.
that neither p∗ nor Gr is accessible to the robot, since the
robotcanonlygetaccesstothenaturallanguageinstruction
u.Forexample,“couldyoupassthatfromthesofa”could †τ
t′
shouldbep∗insteadifthePRstageisincluded.
44 Transformer-basedModelImplementation overtheirintermediatepredictionsofrobot’stask(andhu-
man’splan).TheMSmodels,however,disentanglethesocial
We implement a Transformer-based model following our reasoning(functionsTRandPR)fromembodiedreasoning
framework,illustratedinFig.3.Weassumeallinputsareren- (functionGP),andtrainthemastwoseparatemodules.The
deredintexts,andthemodelneedstopredictactionstrings. lattermoduleistrainedwiththeground-truthp∗orGr,but
isevaluatedusingthepredictionsfromtheformermodule.
4.1 Inputs
Since small-scale language models cannot process exces- 4.3 EncoderLayers
sivelylonginputs,wedividetheinformationintofourparts,
EachencoderlayeriscomposedoffourTransformerlayers
including world state description (s ), human’s trajectory
t′ andaModalityInteractionmodule.
(τ ),languageinstruction(u),andthemodel’spastoutputs.
t′
Worldstatedescription. TransformerLayers.
Weconsideranobject-centricrepresentationfortheworld WeusefourseparateTransformerencoder-onlylayersto
state.Specifically,theworldstateisdescribedasasequence processtheinputs.Weremovethepositionalencodingforthe
ofobjecttokens.Foreachobjectintheworld,wefusethe worldstatedescription,becausewedonotexpectthemodel
informationofitscategory(objecttypeandgenre),attributes tolearnanorderingofobjects.
(e.g.,size,color,is-open),andspatialrelation(insideoron
ModalityInteraction.
topof anotherobject)intoonesingleembedding,whichwe
In order to fuse the information from the four parts of
calledasanobjecttoken.Thehumanandtherobotarealso
inputs,wedesignamodalityinteractionmodule(anMLP)
treatedastwospecial“objects”.
within each layer, following the architecture proposed by
Human’strajectory. GreaseLM (Zhang et al. 2021). We reserve a special “in-
Astraightforwardwaytorepresentthehuman’strajectory teraction” token at the front of each part of inputs. These
istodirectlyuseaparagraphoftexttodescribetheaction tokensareexpectedtogatherrespectiveinformationinthe
sequence,e.g.,“thehumanpicksupbook#1fromthetable”. Transformerlayersandtheninteractwitheachotherthrough
Tobetteralignitwiththeworldstate,wereplacetheembed- thisMLP.Theupdatedspecialtokenswillthenreplacethe
dingsforobjectnames(“book#1”)bytheobjecttokensthat originalfirsttokenineachpartofinputs.
weobtainedintheworldstatedescription.
4.4 PredictionLayers
Languageinstruction.
Thenaturallanguageinstructionsentenceistokenizedand Nowweintroduceourpredictorsforintermediatereasoning
thendirectlyturnedintoembeddings. stepsandthefinalactions.Allthepredictionsaretrainedwith
crossentropylossovercorrespondingsupervisions.
Model’spastoutputs.
Everytimetheagenttakesastep,theenvironmentreturns SocialReasoning:Human’sPlanRecognition.
asentencedescribingtheeffectofitsaction,i.e.,theupdate
We assume that there is a vocabulary of concepts that
inobservations.Ineachepisode,suchsentencesforpaststeps
allow us to represent human goals as first order logic
areconcatenatedandservedasanextrainput,e.g.,“...[SEP]
predicates (e.g., ⟨∃y,box(y), ∀x,book(x) ⇒ in(x,y)⟩).
slice apple 0 [SEP] The apple (apple 0) is sliced into two
While such logical predicates could be more complex,
pieces...”Theconcatenatedresultisalsotokenizedandthen
here we assume human plans follow a simpler form: a
turned into embeddings. This information is necessary be-
Q(uantifier), a S(ubjective), a V(erb), and an O(bjective)
causethemodelneedstoknowwhatithasdoneandwhether
(e.g., ⟨for-all,book,inside,box⟩). Therefore, the human’s
theworldstateischanged.
plan recognition module of our model needs to predict a
tuple of four tokens. The prediction is conditioned on the
4.2 ModelArchitectureOverview
embeddingsofthefourinputs.Specifically,wecalculatethe
Theproposedmodelconsistsof3N (N =3)encoderlayers log-likelihoodoveralltuplesasfollows,wherethepredic-
thatareusedtoupdatetherepresentationoverallfourparts tionsoftheSubjectiveandObjectiveareconditionedonthe
oftheinputs,inordertopredictthehuman’sintentionsorthe predictedvaluesofQuantifierandVerb:
robot’sactions.Specifically,weusetheembeddingsatLayer
2N topredicttherobot’staskGr (socialreasoningphase). logPr[Q,S,V,O]≈logPr[Q]+logPr[V]
Then, we replace the instruction input for Layer 2N + 1 +logPr[S|Q,V]+logPr[O|Q,V],
bythepredictedGr andthenusethelastlayerembeddings
topredictrobot’sactions.IftheHuman’sPlanRecognition SocialReasoning:Robot’sTaskRecognition.
stageisfurtherincluded,weusetheembeddingsatLayerN The main target of the entire social reasoning phase is
topredicttheselectedhumansubgoalp∗ andthenreplace topredictthetaskassignedtotherobot,suchasaspecific
thetrajectoryinputforLayerN +1bythepredictedp∗. objecttomanipulate.Notethatinthisstepthemodelneeds
Themodelistrainedineitheramulti-staged(MS)oran tospecifyaconcreteobjectintheworld,whiletheSubjective
end-to-end (E2E) manner. The E2E models are trained to andObjectivepredictionsinpreviousplanrecognitionstep
directlyoutputrobot’sactions,butanauxiliarylossisapplied areobjecttypes.
5World Transformer Transformer Action Args
Description Layer Layer
Predictor
TH rau jm eca tn o’ rs y Tra Lns af yo er rmer InM teo rd aa cl ti it oy n Tra Lns af yo er rmer InM teo rd aa cl ti it oy n Book#1 (Obj#30)
(MLP) (MLP)
Action Type
Language Transformer Only the first tokens Robot’s Transformer Only the first tokens Predictor
Instruction Layer are updated. Task Layer are updated.
Pick-up
Model’s Transformer Transformer
Past Outputs Layer Layer
Next Action
Social Reasoning ×2N layers Embodied Reasoning ×N layers
Figure3:TheTransformer-basedmodelhasfourpartsofinputs,whicharepassedseparatelyintodifferentTransformerEncoder
Layers,andinteractwitheachotherthroughaModalityInteractionmoduleaftereachTransformerlayer.Thefirst2N layers
formthesocialreasoningphaseandthelastN layersformtheembodiedreasoningphase.TheembeddingsatLayer2N are
usedforrecognizingrobot’stask,andthelastlayerembeddingsareusedforpredictingactions.
EmbodiedReasoning:ActionPrediction. household ambiguous instruction following task rendered
The model needs to output the next action in each step, in text. HandMeThat instructions are split into four diffi-
whichweassumetobeatripleconsistingofactiontypeand cultylevels,andthegapsbetweenlevelscorrespondtodif-
oneortwoarguments,(e.g.,move-to(sofa),put-into(book#0, ferentchallenges.TheinstructioninaLevel1taskhasno
box#1)). The action type prediction is conditioned on the ambiguity—itisapureplanningtask.ALevel2taskrequires
embeddingsofthefourinputs,whiletheargumentsarefur- socialreasoningwherearobotcansuccessfullyaccomplish
therconditionedonthepredictedactiontype.Specifically, thetaskifitcanalsoinferthegoalfromthehumantrajectory.
we calculate the log-likelihood over all actions (no matter On Level 3, the robot needs to further consider pragmatic
applicableornot). reasoninginlanguageuse.Forexample,iftherearemultiple
bookseverywhereintheroomandonlyonecoat,whichison
logPr[Action,Arg1,Arg2]≈logPr[Action]
thesofa,andboththebookandthecoatarehelpfultothehu-
+logPr[Arg1|Action]+logPr[Arg2|Action].
man’sgoal,thehumanmightsay“Couldyoupassthatfrom
Weassumethemodelhasaccesstoallapplicableactionsat thesofa?”.Usingpragmatics,wecanunderstandthehuman
eachstep,sowetakethemaximumoverallapplicabletriples isaskingforthebook,since“fromthesofa”isrequiredto
togetthefinalprediction. disambiguatewhichbookthehumanisreferringto,butifthe
humanwantedthecoattheycouldsimplyaskforit.Thefinal
Level4containstaskswithinherentambiguitiesthatcannot
5 Experiments
beresolvedwiththeexistinginformation,butcanpotentially
be resolved with a strong prior over what human is likely
WeevaluateourframeworkbytrainingaTransformer-based
todo.Forexample,takingonemorefruitwillcompletethe
modelfromscratchforthechallengingHandMeThatbench-
goalofpackingpicnics,buttherearemanykindsoffruits
mark(Wan,Mao,andTenenbaum2022),andthencompare
intherefrigeratortochoosefrom.Fromtheperspectiveof
themwithmultiplecompetitivebaselines,includingthestate-
completingthegoal,anyfruitwilldo,buthumanpreferences
of-the-artpriorworkonHMT,andtheCoTpromptingonthe
maymakeadifference–thehumanmaywantapplesinstead
largestavailablepre-trainedlanguagemodels.Wewillinves-
ofbananasatthistime.
tigatethefollowinghypothesesthroughempiricalanalysis.
Weevaluateallthemodelsontheirsuccessratesinachiev-
H1: Explicitlymodelingunderlyinghumanintentionsworks ingtherobot’sgoal.Notethattheoriginalevaluationmetric
betterthandirectlypredictingactions. inHandMeThatadditionallyconsidersthenumberofrobot’s
steps. An agent can trivially improve success rate with in-
(a) Separatingthesocialandembodiedreasoningstepsby
creasedsteps,bysimplysearchingallobjectsinabruteforce,
explicitlyrecognizingtherobot’staskisbeneficial.
trial-and-error fashion. We believe that enumeration over
(b) Explicitlyrecognizingthehuman’splanfurtherhelpswith
objects is not realistic in the real world, so we restrict our
thesocialreasoningstage.
experimentstoonetrial(tobecompletedwithin4or5steps).
H2: Pre-trained LLMs, despite having access to common-
5.2 ModelDetails
sense knowledge, do not adequately perform the complex
socialandembodiedreasoninginthistask.Incorporationof Baselinemodels.
domain-specificknowledgethroughCoTcanhelp. We compare our results to human performance on the
task(Human),ahand-codedbaseline(Heuristic)whichhas
5.1 HandMeThatEnvironment
access to ground-truth symbolic state information, and a
We evaluate our models over the HandMeThat (version 2) neuralnetworkbaseline(Seq2Seq(Sutskever,Vinyals,and
dataset(Wan,Mao,andTenenbaum2022).Itintroducesa Le2014))introducedintheHandMeThatpaper.Theexisting
6SOTA work (Cao et al. 2024) was implemented based on H1:Explicitlymodelinghumanintentionsworksbetter
theoriginalHandMeThat(version1)dataset.Therefore,to thandirectlypredictingactions.
faciliatecomparisonwiththisworkweadditionallyreportthe
(a)Separatingthesocialandembodiedreasoningsteps
resultsofourFISERmodelsoverversion1datapointsthat
byexplicitlyrecognizingtherobot’staskisbeneficial.
lieintheversion2domain.Furtherdetailsofthiscomparison
areprovidedinAppendixA.1. ForbothpromptedGPT-4TurboandTransformer-based
models,explicitlypredictingtherobot’stasksignificantlyim-
Transformer-basedmodel. provesthesuccessratesacrossalldifficultylevels,whichsup-
Followingtheproposedmodelarchitecture,weimplement portsourhypothesisthatseparatingthesocialandembodied
asetofTransformer-basedmodels.Wecomparetheimple- reasoningstepsisbeneficialinthesecomplexreasoningtasks.
mentationswithnointermediatesupervision(Transformer), Thecomparisonbetweentwodifferenttrainingschemesof
withRobot’sTaskRecognitiononly(Transformer+FISER), ourTransformer-basedmodelsarepresentedinTable2.Re-
and with Human’s Plan Recognition in addition (Trans- sultsshowthattraininginamulti-stagedmannerworksbetter
former+FISER+PR).Ourmodelsaretrainedfromscratchbe- thanend-to-endinourtasks.Itmayimplythatthelow-level
causeexistingsmall-scalepre-trainedmodelscannothandle groundedplanning(embodiedreasoning)isrequiringasuffi-
theexcessivetokenlengthsofHandMeThatdatainputs.We cientlydifferenttaskrepresentationfrominferringhuman’s
comparetwowaysoftrainingtheTransformer-basedmodel internalgoals(socialreasoning),thatallowinggradientsfrom
usingFISERframework,end-to-end(E2E)ormulti-staged theembodiedreasoningmoduletoflowintothesocialreason-
(MS),asexplainedinSection4.2.Thiscomparisonaimsto ingmoduleactuallyhurtsperformance.Itisafurthersupport
provideinsightsforwhethertoblockthegradientflowfrom on empirical side that we should make explicit inferences
embodied reasoning back to the social reasoning module. abouthumanintentionsasintermediatereasoningsteps.
Wefurtherreporttheaccuracyoftheintermediateprediction
(b)Explicitlyrecognizingthehuman’splanfurtherhelps
stepsfortheseTransformer-basedmodels,includingQSVO
withthesocialreasoningstage.
(simplifiedhumansubgoal),andObj(concreteobjecttobe
WhenwefurtherincludetheHuman’sPlanRecognition
manipulatedinexpertdemonstration,i.e.,therobot’stask).
(PR)stage,wefindthatitonlyhelpsforthemostambiguous
The number of parameters in an E2E model is 5.1M; the
cases(likeinLevel4).ForGPT-4Turbo,addingPRisshow-
number of parameters in an MS model is 4.7M (3.0M for
ingapproximatelythesameperformancesasnormalFISER
socialreasoningand1.7Mforembodiedreasoning).
method.Weattributethistothefactthatpre-trainedLLMs
PromptedGPT-4. arenotgoodatleveraginghierarchicalpredictionstoimprove
WedesignpromptingmethodsforGPT-4TurbooverHMT onthistask.ForTransformer-basedmodels,introducingPR
tasks.Thevanillaimplementation(GPT-4)simplyprovides gives better performance on Level 4, but is harmful to the
allinputstothemodelandrequestsittooutputactions.We simplestLevel1.WeattributethepoorperformanceinLevel
first conduct prompt engineering (GPT-4+PE) to incorpo- 1tothefactthatsuchsimpletasksdonotrequireknowing
ratesomedomain-specificknowledgeandhelptoparsethe thehumans’high-levelgoal.Therefore,forcingthemodelto
complexinputs.ThenweimplementFISERframeworkby predictthisinformationreducesthemodel’scapacitytofocus
applyingCoTpromptingtoexplicitlypredictthesameinter- onplanningforlow-levelactions.Ontheotherhand,theim-
mediatedata(human’splansandspecifiedrobot’stasks)that provedperformanceonLevel4showsthatexplicithuman’s
we use for Transformer-based models step-by-step, which planrecognitionhelpstobetterlearnpriorsoverhumaninten-
similarly gives two models (i.e., GPT-4+FISER and GPT- tions.Evenontheseintrinsicallyambiguoustasks,themodel
4+FISER+PR).Tobemorespecific,inbothsocialreasoning canleveragethestrongpriortotakehelpfulactions.
steps,wepromptGPT-4todostep-by-stepreasoningexcept
H2:Pre-trainedLLMs,despitehavingaccesstocommon-
thatweaskdifferentquestions.Human’sPlanRecognition
senseknowledge,donotadequatelyperformthecomplex
asksaboutthehuman’shigher-levelgoal,whileRobot’sTask
socialandembodiedreasoninginthistask.Incorporation
Recognitionasksabouttheintendedmeaningofanambigu-
ofdomain-specificknowledgethroughCoTcanhelp.
ous instruction. We further consider providing additional
Results show that training much smaller, more efficient
assistancebyfilteringoutaproportionofirrelevantobjects
Transformer-basedmodelsfromscratchisexhibitingabout
fromtheenvironment,toassesstheimpactofexcessiveitem
70%increasedperformancethanpromptingstate-of-the-art
quantityonGPT-4’sembodiedreasoning.Thepromptsfor
pre-trainedLLMs.GPT-4Turbo’sresultsonLevel1showit
GPT-4TurboareprovidedinAppendixB.
hasthecapabilitytodosomelevelofembodiedreasoning
whengivenexplicittasks.However,theperformancedrop
6 Results onsubsequentlevelsindicatesthattherequiredknowledge
tosolveHandMeThattasksisnotfullycoveredbycommon-
WeevaluateallthemodelsovertheHandMeThat(version senseknowledgeinpre-trainedLLMs.Withwell-designed
2)datasetinthefullyobservablesetting.Overall,ourbest- promptengineering(PE)thatcontainssomedomainknowl-
performing Transformer+FISER model achieves a 64.5% edge(e.g.,goalspaceandfew-shotexamples),GPT-4Turbo
successrateonaverage,achievingthestate-of-the-artonthe improvessignificantlyacrossalldifficultylevels.However,
HandMeThatbenchmark.Themainresultsarepresentedin evenwithcarefulCoTpromptsandfew-shotexamples,itis
Table1.Nowwediscussthepreviouslystatedhypotheses. farfromsmall-scaleTransformermodelsacrossalllevels.
7BaselineModels GPT-4Turbo Transformer-basedModels OnHMTVersion1
Model
Human Heur. Seq2Seq Vanilla +PE +FISER +PR Vanilla +FISER +PR Caoetal. FISER
Level1 100.0 100.0 30.4 72.0 82.0 80.0 77.0 77.7±1.6 89.0±1.5 72.0±1.5 27.7±0.3 89.7±0.4
Level2 80.0 64.0 28.8 16.0 25.0 36.0 34.0 55.3±0.4 74.0±0.3 74.0±0.9 24.8±0.4 63.0±0.3
Level3 40.0 39.0 12.8 5.0 13.0 18.0 17.0 36.3±1.0 52.3±2.3 52.3±1.0 21.0±0.1 28.3±2.5
Level4 30.0 29.0 14.8 9.0 9.0 17.0 20.0 38.3±1.4 42.7±0.2 51.0±1.2 21.7±0.2 40.7±1.9
Table1:Successrate(%)ofmodelsoverHMTinthefullyobservablesetting.TheresultsforTransformer-basedmodelsarethe
meanandstandarderrorvaluesoverthreeruns.*Caoetal.(2024)isevaluatingversion1ofHandMeThatdataset,andthuswe
providetheresultsofTransformer+FISERmodeloverasubsetofversion1forafaircomparison.Overall,FISERimproves
theperformanceacrossalllevelscomparedtothevanillaTransformer.WhileapplyingFISERandPRtoGPT-4improvesits
performance,overallGPT-4cannotperformwellonthesetasksevenwithverycarefulprompting,achievinglessthanhalfthe
successrateofourmodelforambiguousinstructionsinlevels2-4.
Model Level1 Level2 Level3 Level4
End-to-End 77.7±1.6(N/A,N/A) 55.3±0.4(N/A,N/A) 36.3±1.0(N/A,N/A) 38.3±1.4(N/A,N/A)
End-to-End+FISER 74.3±0.2(N/A,87.8) 73.7±0.5(N/A,80.9) 47.3±2.1(N/A,73.1) 41.3±1.9(N/A,64.7)
End-to-End+FISER+PR 61.0±1.5(73.2,81.3) 66.7±1.1(64.9,81.0) 47.0±1.2(59.0,73.4) 42.0±0.3(55.0,66.5)
Multi-Staged+FISER 89.0±1.5(N/A,93.4) 74.0±0.3(N/A,82.3) 52.3±2.3(N/A,76.5) 42.7±0.2(N/A,68.1)
Multi-Staged+FISER+PR 72.0±1.5(74.4,82.4) 74.0±0.9(71.9,82.3) 52.3±1.0(67.1,75.6) 51.0±1.2(65.3,71.2)
Table 2: Comparison between End-to-End and Multi-staged training of Transformer-based models over HMT in the fully
observablesetting.Allmodelsareevaluatedbythesuccessrate(%).Thepredictionaccuracy(%)ofQSVOandObj(intermediate
outputs)arepresentedinparentheses.Theresultsarethemeanvaluesoverthreeruns,andthestandarderrorvaluesforsuccess
ratesareprovided.Overall,traininginamulti-stagedmannerworksbetterthanend-to-endinourtasks,implyingthatfully
separatingsocialfromembodiedreasoningprovidesthebestperformance.
• Redundant Behavior: The model gives the human an
object that is already at its target location or even one
whichwasjustmanipulatedbythehuman.
• IncorrectIntention:ForGPT-4,common-sensereason-
ing is performed but not aligned with the ground-truth
humanintention.(Asanexample,itwillpickupradish
giventhatthepersonwaspreparingsoup,because”radish
isacomponentofsomesoups.”Butthisrelationshipdoes
notexistinthehuman’sgoalspace.)ForTransformer,the
modelcanreachtheobjectitwants,butthatobjectisnot
whatthehumanwants.
Here,PlanningFailurecorrespondstothedifficultyinem-
bodiedreasoning,whileIncorrectIntentionandRedundant
Behaviorcorrespondstothechallengesinsocialreasoning.‡
ResultsshowthataTransformer-basedmodelcaneffectively
avoidbothPlanningFailureandRedundantBehavior,while
Figure 4: Failure case analysis for state-of-the-art LLMs
recognizingthetargetobjectcorrectlyisstillachallenging
with CoT prompts following FISER framework versus
issue.NotethatthePlanningFailurerateofGPT-4Turbois
Transformer-based models trained from scratch with the
around20%,whichalignswithitsperformanceonLevel1
FISERframeworkover100datapointsonLevel2.
(80%successrate)inTable1,sinceLevel1onlyincorporates
challengesinembodiedreasoning.Webelievetheunsatisfac-
Asaqualitativeanalysis,weevaluatethefailurecasesfor toryperformanceofGPT-4Turboisbecausetheprompting
FISERframeworkonpromptingGPT-4Turboandtraining methods alone cannot provide the model with the type of
Transformer-basedmodels.WeprovidetheresultsonLevel socialandembodiedreasoningneededtosolvethistask.The
2over100datapointsinFig.4.Wedefinethefailuremodes informationlearnedfromlanguagedatasetscollectedonline
asfollows: may also be significantly different from that required for
• PlanningFailure:Hallucination(gotoaplacewherethe ‡Asinglefailurecasecouldresultfrommultipleerrors,which
targetobjectisnotlocatedat,i.e.,failtolocatetheobject), meansitispossiblethatafailurecasethatcountsasPlanningFailure
missingsteps,orinvalidactions. couldalsoapplytoIncorrectIntention.
8thishouseholdassistancetask.Trainingasmall-scalemodel, References
however,cansolvetheproblemmoreefficientlyandreliably.
Ahn,M.;Brohan,A.;Brown,N.;Chebotar,Y.;Cortes,O.;
Weconductedanadditionalexperimenttoseeiftheper- David,B.;Finn,C.;Fu,C.;Gopalakrishnan,K.;Hausman,
formanceofthepre-trainedLLMscouldbeimproved.Here K.;Herzog,A.;Ho,D.;Hsu,J.;Ibarz,J.;Ichter,B.;Irpan,A.;
weprovideadditionalassistancebyfilteringoutaproportion Jang,E.;Ruano,R.J.;Jeffrey,K.;Jesmonth,S.;Joshi,N.J.;
ofirrelevantobjectsfromtheenvironment(whichassumes Julian,R.;Kalashnikov,D.;Kuang,Y.;Lee,K.-H.;Levine,
accesstotheground-truthhumangoals),toassesstheimpact S.; Lu, Y.; Luu, L.; Parada, C.; Pastor, P.; Quiambao, J.;
ofexcessiveitemquantityonGPT-4’sembodiedreasoning. Rao,K.;Rettinghouse,J.;Reyes,D.;Sermanet,P.;Sievers,
TheresultsarepresentedinFig.5.Eveninthecasethatonly N.; Tan, C.; Toshev, A.; Vanhoucke, V.; Xia, F.; Xiao, T.;
Xu,P.;Xu,S.;Yan,M.;andZeng,A.2022. DoAsICan,
NotAsISay:GroundingLanguageinRoboticAffordances.
arXiv:2204.01691.
Amini,A.;Gabriel,S.;Lin,S.;Koncel-Kedziorski,R.;Choi,
Y.;andHajishirzi,H.2019. MathQA:TowardsInterpretable
Math Word Problem Solving with Operation-Based For-
malisms. InNAACL-HLT,2357–2367.
Baker, C. L.; Tenenbaum, J. B.; and Saxe, R. 2007. Goal
InferenceasInversePlanning. CogSci.
Blukis,V.;Paxton,C.;Fox,D.;Garg,A.;andArtzi,Y.2021.
APersistentSpatialSemanticRepresentationforHigh-level
NaturalLanguageInstructionExecution. InCoRL,volume
164,706–717.
Cao,C.;Fu,Y.;Xu,S.;Zhang,R.;andLi,S.2024.Enhancing
Human-AICollaborationThroughLogic-GuidedReasoning.
Figure5:SuccessratesofGPT-4Turbounderdifferentset-
InICLR.
tingsgiventhatdifferentpercentagesofirrelevantobjectsare
Chen,X.;Liang,C.;Yu,A.W.;Zhou,D.;Song,D.;andLe,
filteredoutfromtheenvironment.
Q.V.2020. NeuralSymbolicReader:ScalableIntegration
of Distributed and Symbolic Representations for Reading
relevantobjectsareremained,GPT-4Turbocanonlyachieve
Comprehension. InICLR.
lessthan85%successrate,whichalignswiththeplanning
Chiang,T.-R.;andChen,Y.-N.2019. Semantically-Aligned
failureratesinthefailurecaseanalysisabove,indicatingthat
EquationGenerationforSolvingandReasoningMathWord
the LLM cannot easily resolve the difficulty in embodied
Problems. InBurstein,J.;Doran,C.;andSolorio,T.,eds.,
reasoning.Astheirrelevantobjectsincrease,thesuccessrate
NAACL-HLT,2656–2668.
ofGPT-4Turbodropsdramatically,showingthechallenges
ofsocialreasoninginHandMeThattasks.Tosummarize,we Clark,H.H.1996. Usinglanguage. Cambridgeuniversity
observethatLLM’sperformancereliesonaverylargepro- press.
portionofobjectsbeingfilteredout,whichprovidesfurther Dennett,D.C.1987. Theintentionalstance. MITpress.
insightthatLLMscannoteffectivelyselectrelevantenviron-
Gao, X.; Gao, Q.; Gong, R.; Lin, K.; Thattai, G.; and
ment information and focus on relevant objects, which is
Sukhatme,G.S.2022. DialFRED:Dialogue-EnabledAgents
requiredinembodiedreasoning.
forEmbodiedInstructionFollowing. RA-L,7:10049–10056.
Gergely,G.;Na´dasdy,Z.;Csibra,G.;andB´ıro´,S.1995. Tak-
7 Conclusion ing the intentional stance at 12 months of age. Cognition,
56(2):165–193.
WestudythechallengingHandMeThatbenchmark,compris- Grice, H. P. 1975. Logic and Conversation. Syntax and
ingambiguousinstructionfollowingtasksrequiringsophisti- Semantics,3:41–58.
catedembodiedandsocialreasoning.Wefindthatexisting
Jain,U.;Weihs,L.;Kolve,E.;Rastegari,M.;Lazebnik,S.;
approachesfortrainingmodelsend-to-end,orforprompting
Farhadi,A.;Schwing,A.G.;andKembhavi,A.2019. Two
powerful pre-trained LLMs, are both insufficient to solve
Body Problem: Collaborative Visual Task Completion. In
thesetasks.Wehypothesizedthatperformancecouldbeim-
CVPR.
provedbybuildingamodelthatexplicitlyperformssocial
Kojima,N.;Suhr,A.;andArtzi,Y.2021. ContinualLearning
reasoningtoinferthehuman’sintentionsfromtheirpriorac-
forGroundedInstructionGenerationbyObservingHuman
tionsintheenvironment.Ourresultsprovideevidenceforthis
FollowingBehavior. TACL,9.
hypothesis,andshowthatourapproach,FollowInstructions
withSocialandEmbodiedReasoning(FISER),enhancesper- Lesh, N.; and Etzioni, O. 1995. A Sound and Fast Goal
formanceoverthemostcompetitivepromptingbaselinesby Recognizer. InIJCAI.
70%,settingthenewstate-of-the-artforHandMeThat. Levesque, R. J. R. 2011. Social Reasoning, 2808–2808.
SpringerNewYork. ISBN978-1-4419-1695-2.
9Meneguzzi, F.; and Pereira, R. 2021. A Survey on Goal Zhang, H.; Wang, Z.; Lyu, Q.; Zhang, Z.; Chen, S.; Shu,
RecognitionasPlanning. InIJCAI. T.; Du, Y.; and Gan, C. 2024. COMBO: Compositional
World Models for Embodied Multi-Agent Cooperation.
Nair,S.;Rajeswaran,A.;Kumar,V.;Finn,C.;andGupta,A.
arXiv:2404.10775.
2022. R3M:AUniversalVisualRepresentationforRobot
Manipulation. InCoRL,volume205,892–909. Zhang,X.;Bosselut,A.;Yasunaga,M.;Ren,H.;Liang,P.;
Manning,C.D.;andLeskovec,J.2021. GreaseLM:Graph
Nye,M.;Andreassen,A.J.;Gur-Ari,G.;Michalewski,H.;
REASoningEnhancedLanguageModels. InInternational
Austin,J.;Bieber,D.;Dohan,D.;Lewkowycz,A.;Bosma,
ConferenceonLearningRepresentations.
M.;Luan,D.;Sutton,C.;andOdena,A.2021. ShowYour
Work:ScratchpadsforIntermediateComputationwithLan-
guageModels. arXiv:2112.00114.
Padmakumar,A.;Thomason,J.;Shrivastava,A.;Lange,P.;
Narayan-Chen, A.; Gella, S.; Piramithu, R.; Tur, G.; and
Hakkani-Tu¨r,D.Z.2022. TEACh:Task-drivenEmbodied
AgentsthatChat. InAAAI.
Puig,X.;Shu,T.;Li,S.;Wang,Z.;Liao,Y.-H.;Tenenbaum,
J.B.;Fidler,S.;andTorralba,A.2021. Watch-And-Help:A
ChallengeforSocialPerceptionandHuman-{AI}Collabora-
tion. InICLR.
Roy, S.; Vieira, T.; and Roth, D. 2015. Reasoning about
QuantitiesinNaturalLanguage. InTACL,1–13.
Shridhar,M.;Thomason,J.;Gordon,D.;Bisk,Y.;Han,W.;
Mottaghi,R.;Zettlemoyer,L.;andFox,D.2020. ALFRED:
ABenchmarkforInterpretingGroundedInstructionsforEv-
erydayTasks. InCVPR.
Siskind,J.M.1994.Groundinglanguageinperception.JAIR,
8:371–391.
Sperber,D.;andWilson,D.1986. Relevance:Communica-
tionandcognition. Citeseer.
Suglia, A.; Gao, Q.; Thomason, J.; Thattai, G.; and
Sukhatme,G.2021. EmbodiedBERT:ATransformerModel
for Embodied, Language-guided Visual Task Completion.
arXiv:2108.04927.
Suhr,A.;Yan,C.;Schluger,J.;Yu,S.;Khader,H.;Mouallem,
M.;Zhang,I.;andArtzi,Y.2019. ExecutingInstructionsin
SituatedCollaborativeInteractions. InEMNLP-IJCNLP.
Sutskever,I.;Vinyals,O.;andLe,Q.V.2014. Sequenceto
SequenceLearningwithNeuralNetworks. InNeurIPS.
Wan,Y.;Mao,J.;andTenenbaum,J.2022. HandMeThat:
Human-RobotCommunicationinPhysicalandSocialEnvi-
ronments. InNeurIPSDatasetsandBenchmarksTrack.
Wei,J.;Wang,X.;Schuurmans,D.;Bosma,M.;Ichter,B.;
Xia, F.; Chi, E. H.; Le, Q. V.; and Zhou, D. 2022. Chain-
of-ThoughtPromptingElicitsReasoninginLargeLanguage
Models. InNeurIPS.
Winograd,T.1972. Understandingnaturallanguage. Cogni-
tivePsychology,3(1):1–191.
Ying,L.;Jha,K.;Aarya,S.;Tenenbaum,J.B.;Torralba,A.;
and Shu, T. 2024. GOMA: Proactive Embodied Coopera-
tiveCommunicationviaGoal-OrientedMentalAlignment.
arXiv:2403.11075.
Zellers,R.;Holtzman,A.;Peters,M.;Mottaghi,R.;Kemb-
havi,A.;Farhadi,A.;andChoi,Y.2021. PIGLeT:Language
Grounding Through Neuro-Symbolic Interaction in a 3D
World. InACL,2040–2050.
10A ModelImplementationDetails basedontheaverageperformanceonvalidatesets.Wechoose
32 as batch size by sweeping over 32,64,128. We choose
A.1 BaselineModels
Adamwithlearningrate1e−4,betas(0.9,0.98),andepsilon
BaselinesinHandMeThat
1e−9.Wechoose0,1,2astherandomseedsandtrainall
HandMeThatpaperprovidesaseriesofsimplebaselines
themodelsforthreeruns.
to compare with, including Human, Random, Heuristic,
Seq2Seq,andDRRN.WeusetheirreportedresultsonHand- ComputeResources
MeThatVersion2dataset. WeuseNVIDIARTXA6000totrainourmodels.Each
Asintroducedinthebenchmarkpaper,theHeuristicmodel modelistrainedonasingleGPU.Theend-to-endmodeland
isanagentthatheuristicallyrepeatstheprevioushumanac- embodied reasoning module take around 8 hours for each
tions.Tobemorespecific,thismodelhasaccesstoallob- trainingepoch,whilethesocialreasoningmoduletakesless
jectstatesandtheunderlyinglogicformulaoftheutterance. thanourhoureachepoch.Giventhattheembodiedreasoning
Therefore,itisonlyapplicableinthefully-observablesetting. module is trained for 20 epochs, it takes approximately a
Uponreceivingtheinstruction,theagentgeneratesallpossi- weektotrain.
blegroundingsofinstructionandthencomparesthemtothe
A.3 PromptedGPT-4
observedhumantrajectory.Thekeyheuristicofthismodel
isthat:humantendstoquestforobjectsthatareinthesame WeusedGPT-4TurbofromOpenAIAPI.GPT-4isalarge
categoriesasthepreviouslymanipulatedones.TheHeuristic multimodal model that can solve difficult problems with
modelguessesthegroundingoftheobjectsintheutterance greater accuracy than OpenAI’s previous models. It has
basedonthisheuristic. broader general knowledge and advanced reasoning capa-
bilities,whichmakesGPT-4Turbosuitableforourcomplex
Caoetal.(2024)
taskthatinvolvessocialreasoningandworldassumptions.
ThepreviousSOTAworkistrainedandevaluatedonVer-
WepaidtogettextgenerationfromtheAPI.
sion1dataset.Theircodeforimplementingthemethodon
HandMeThatwasnotreleaseduntilthispaperissubmitted.
We provide the results of our FISER model over a subset
of version 1. Here the subset is selected in a way that the
instructiontypeshouldbe“bringme”,andthegoalshouldbe
includedinVersion2dataset.Weargueitisafaircomparison
forthefollowingreasons:
1. They build in the goal space to their model with hand-
craftedways,whileweareusingasimplifiedversionof
thegoalassupervisiontotrainthemodels.
2. They use rule-based information extractor to deal with
longandredundanttextinputs,whileweareusingsym-
bolicrepresentationfortheworldstatedescription.
3. Theyclaimthattheyaredoingpartiallyobservableset-
tings, but they are actually having full access to all the
objects in the environment at least during the training
phase,anddidn’tlearntonavigateintheenvironment.
4. Theyonlyfilteredasmallsetofgoalsamongall69goal
templatesofVersion1dataset.We,however,aretraining
on25differentpossiblegoaltemplates,andevaluateour
modelsonVersion1datawherethegoalisseenbutnot
thedata.
A.2 Transformer-basedModels
Themodelarchitectureandimplementationdetailsaredis-
cussedinthemaintext.Wewillreleasethecodebaseforour
modelsinthefuture.
Hyperparameters
The number of trainable parameters for an end-to-end
modelis5,057,945;thenumbersoftrainableparametersfor
socialreasoningandembodiedreasoningmoduleswithina
multi-stagedmodelare3,020,483and1,701,750respectively.
Wetraintheend-to-endmodelandtheembodiedreasoning
modulefor20epochs,andtrainthesocialreasoningmodule
for 40 epochs. The number of training epochs are chosen
11B GPT-4Prompts
There are four rungs of system prompts for GPT-4, each of them building on the previous rung: GPT-4, GPT-4+PE, GPT-
4+PE+FISER,GPT-4+PE+FISER+PR.Welistthepromptsasfollows.Eachpromptissplitintomultipleparts,andeachpartis
putinonebox.Weomittheboxesthathaveshownupinpreviousprompts.Thedatapointisplacedattheword“Data”.
B.1 GPT-4
Part1:Taskinformation.
Part2:Currentdatapoint.
Youarearobot.Everytimeyouwillreceiveadescriptionoftheworldandaninstructiongivenbythehumaninthefollowing
templates:
[WorldDescription]:
[HumanTrajectory]:
[HumanInstruction]:
Youareexpectedtogenerateasequenceofactionsthatareconsistentwiththehuman’sinstructionandtheworlddescription.
PleaserememberthateachobjecthasanumberID.
!!!!!THELASTPARTOFTHERESPONSEshouldbeasequenceofactionsstartingwiththestring”Actions:”(!!Onlyactions!
Nonumberingforactions,nodescription,noextrawords)!!!!!
Theavailableactiontemplatescontain:
movetoXXX
pickupXXX
pickupXXXfromXXX
putXXXintoXXX
putXXXontoXXX
takeXXXfromXXX
giveXXXtohuman
openXXX
Atypicalanswerformatisasfollows,wherethe”open”stepisoptional.
Actions:
movetoXXX
openXXX
pickupXXX
movetohuman
giveXXXtohuman
========================Nowpleasebegintogeneratetheoutputforgivenscenarios========================
[WorldDescription]:Data
[HumanTrajectory]:Data
[HumanInstruction]:Data
Youareexpectedtogenerateasequenceofactionsthatareconsistentwiththehuman’sinstructionandtheworlddescription.
PleaserememberthateachobjecthasanumberID.
!!!!!THELASTPARTOFTHERESPONSEshouldbeasequenceofactionsstartingwiththestring”Actions:”(!!Onlyactions!
Nonumberingforactions,nodescription,noextrawords)!!!!!
========================Let’sstart!========================
B.2 GPT-4+PE
Part1:Taskinformation.SameasGPT-4,omitted.
Part2:Domainspecificknowledge.
Part3:Datapointexample.
Part4:Answerexample.
Part5:Currentdatapoint.SameasGPT-4,omitted.
12NOTE!!: Remember to open something like refrigerator if it is closed before pick up something. There are some openable
objects:vessel,bag,box,package,cabinet,microwave,oven,dishwasher,refrigerator
Takeanactionlikeopenrefrigerator,opencabinet,openbox,openbag,openvessel,openpackage,openmicrowave,openoven,
opendishwasher
========================================================================================
Toletyouanswerbetter,wegiveyouthedescriptionofthegoalspacefortheworldasfollows.Thehumanhasoneofthe
followinggoals:
Goal0:boxingbooksupforstorage
Goaldescription:Putallthepaperproductsofacertainkindintoabox.
Goal1:bringinwood
Goaldescription:Putallthebuildingmaterialsofacertainkindontopofthefloor.
Goal2:clearingthetableafterdinner
Goaldescription:Putallofonecertainkindofcutleryinsidesomebucket,putallofanotherkindofcutleryintoanotherbucket,
andputalltheflavorerofacertainkindintoabucket.
Goal3:collectmisplaceditems
Goaldescription:Putallofacertainfootwear,acertaindecoration,andacertainpaperproductontopofthetable.
Goal4:collectaluminumcans
Goaldescription:Putalldrinksintotheashcan.
Goal5:installingalarms
Goaldescription:Putanelectricaldeviceonthetable,onthecountertop,andonthesofa.
Goal6:layingtilefloors
Goaldescription:Putallofacertainbuildingmaterialontopofthefloor.
Goal7:loadingthedishwasher
Goaldescription:Putallthetablewaresoftwocertainkindsandallofacertainvesselintothesink.
Goal8:movingboxestostorage
Goaldescription:Putalltheboxesontopofthefloor.
Goal9:oraganizingboxesingarage
Goaldescription:Putallofacertainplaythingintoabox,putallofacertaincutleryintoanotherbox,putallofacertaincleasing
thingintoathirdbox,andputalltheboxesontopofthefloor.
Goal10:organizefilecabinet
Goaldescription:Putallthewritingtoolsofacertainkindontopofthetable,andputallthepaperproductsofacertainkind
intothecabinet.
Goal11:pickuptrash
Goaldescription:Putallofacertainpaperproductandallofacertaindrinkintotheashcan.
Goal12:putawayChristmasdecorations
Goaldescription:Putallthedecorationsofthreecertainkindsintothecabinet.
Goal13:putawayHalloweendecorations
Goaldescription:Putallthevegetableofacertainkindandalltheilluminationtoolsofacertainkindintothecabinet,andputall
ofacertainvesselsontopofthetable.
Goal14:putawaytoys
Goaldescription:Putalltheplaythingsofacertainkindintoaclosedbox.
Goal15:putdishesawayaftercleaning
Goaldescription:Putallthetablewaresofacertainkindintothecabinet.
Goal16:putleftoversaway
Goaldescription:Putallofacertainpreparedfoodandallofacertainflavorerintotherefrigerator.
Goal17:putupChristmasdecorationsinside
Goaldescription:Putallofacertainilluminationtoolandallthedecorationsoftwocertainkindsontopofthetable,andputall
thedecorationsofanothercertainkindontopofthesofa.
Goal18:re-shelvelibrarybooks
Goaldescription:Putallofacertainpaperproductontopoftheshelf.
Goal19:servehorsd’oeuvres
Goaldescription:Putallofacertainbakedfood,acertainvegetable,acertainpreparedfood,andthetraysontopofthetable.
Goal20:sortbooks
Goaldescription:Putallthepaperproductsoftwocertainkindsontopoftheshelf.
Goal21:storefood
Goaldescription:Putallofacertainpreparedfood,acertainsnacks,andtwocertainkindsofflavorersintothecabinet.
Goal22:storethegroceries
Goaldescription:Putallofacertainfruit,acertainprotein,andtwocertainkindsofvegetablesintotherefrigerator.
Goal23:thawfrozenfood
Goaldescription:Putallofacertainvegetable,acertainfruit,andacertainproteinintothesink.
Goal24:throwawayleftovers
Goaldescription:Putallthesnacksofacertainkindintothea1s3hcan.
=========================================================================Thefollowingisanexampleofpromptinputsandgroundtruthoutputs.Youshouldgenerateoutputinasimilarway:
[WorldDescription]:
Welcometotheworld!
Intheroomthereisthehumanacountertopasofaabedastoveatableashelfatoiletacabinetabathtubamicrowaveanovena
dishwasherarefrigeratorasinkapool.
Nowyouarestandingonthefloor.Youareatthefloor.
Youseefloor.Onflooryoucanseegreenhighchair0reddustyseat0bluedustyseat1bluechair0reddustychair1greendusty
chair2dustybucket0bucket1dustybucket2greenlargeclosedpackage2smalldustyashcan0smalldustyashcan1large
ashcan2xmas stocking0dustyxmas tree0xmas tree1xmas tree2shoe0dustysock0sandal2.Inbucket1youcansee
dishtowel1detergent0.
Youseecountertop.Oncountertopyoucanseegreenclosedbottle0redclosedkettle1redcloseddustykettle2smalldusty
plate0redcloseddustybriefcase1largedustytray2xmas stocking2strawberry1strawberry2carrot1beer1fish0fish1
parsley1bread2cookie0saw0saw1hammer0carving knife0toggled-offfacsimile0calculator1mouse0earphone0pencil
1highlighter1fork1spoon0.
Youseedustystainedsofa.Onsofayoucanseegreenclosedduffel bag0greenbasket0greenbasket1redlargeopendustybox
0redlargecloseddustybox1bluesmallcloseddustypackage0ribbon1dustybow1document1dustydocument2hardback1
hat0cube0.Inbox0youcanseedocument0.Inpackage0youcanseedustyribbon0bow0bracelet2hat2.Youseedusty
bed.Onbedyoucanseebluebasket2bracelet0dustyjewelery0hardback0dustyapparel0apparel1ball0.
Youseetoggled-offstove.Onstoveyoucanseenothing.
Youseedustytable.Ontableyoucanseegreenclosedbottle1greenclosedsack0blueclosedbriefcase0bluecloseddusty
briefcase2smalltray1bluelargecloseddustybox2xmas stocking1strawberry0banana2pop0vegetable oil0vegetable oil
1sugar1cracker0cracker1bread0bread1cookie2salad0soup0pasta2carving knife1scraper0scraper1screwdriver
1toggled-offprinter0toggled-offprinter1calculator0mouse2pencil2highlighter0lamp0lamp1.Ontray1youcansee
sandwich1soup2.
Youseestainedshelf.Onshelfyoucanseebluesmallcloseddustypackage1saw2hammer1screwdriver0toggled-offscanner
0mouse1earphone1lamp2.
Youseedustystainedtoilet.Ontoiletyoucanseetoothpaste1toothpaste2.
Youseeopendustycabinet.Incabinetyoucanseebluecloseddustykettle0smalldustybowl0largebowl1pan2pop1beer2
juice1parsley0vegetable oil2hammer2scraper2toggled-offmodem0toggled-offmodem1pencil0dishtowel0rag1soap1
toothpaste0detergent2bracelet1painting0dustypainting1painting2book0book1notebook0dustysandal0sandal1dusty
hat1cube1.
Youseestainedbathtub.Inbathtubyoucanseemakeup1rag0vacuum0broom0broom1shampoo2.
Youseeclosedtoggled-offstainedmicrowave.Inmicrowaveyoucanseecookedsalad1cookedsoup1.
Youseeclosedtoggled-offdustyoven.Inovenyoucanseecookedcracker2cookedcake0.
Youseeclosedtoggled-offdishwasher.Indishwasheryoucanseedustypan0knife0spoon1.
Youseeclosedtoggled-ondustyrefrigerator.Inrefrigeratoryoucanseelargedustymug0smallbowl2largetray0frozen
banana0frozenbanana1frozencarrot0frozencarrot2frozenbeer0frozenjuice0frozenfish2frozensugar0frozentea bag
0frozentea bag1frozencookie1frozencake2frozensandwich0frozensandwich2frozensalad2frozenpasta0fork0knife
1.Ontray0youcanseecake1pasta1.
Youseetoggled-offstainedsink.Insinkyoucanseedustypan1makeup0makeup2rag2hand towel0vacuum1scrubbrush0.
Youseestainedpool.Inpoolyoucanseesoap0detergent1shampoo0shampoo1dustytile0tile1.
Humaniscurrentlyholdingnothing.Nowitisyourturntohelphumantoachievethegoal!
[HumanTrajectory]:
Thehumanagenthastakenalistofactionstowardsagoal,whichincludes:
Humanmovestothecabinet.
Humanopensthecabinet.
Humanpicksupthedocument0atthecabinet.
Humanmovestothesofa.
Humanopensthebox0atthesofa.
Humanputsthedocument0intothebox0.
Humanstopsandsays,
[HumanInstruction]:
’Pleasegivemethedustyone.’
——————————–
Actions:
movetosofa
pickupdocument2
movetohuman
givedustydocument2tohuman
14B.3 GPT-4+PE+FISER
Part1:Taskinformation.SlightlydifferentfromGPT-4+PE,markedinblue.
Part2:Domainspecificknowledge.SameasGPT-4+PE,omitted.
Part3:Datapointexample.SameasGPT-4+PE,omitted.
Part4:Answerexample.
Part5:Currentdatapoint.SlightlydifferentfromGPT-4+PE,markedinblue.
Youarearobot.Everytimeyouwillreceiveadescriptionoftheworldandaninstructiongivenbythehumaninthefollowing
templates:
[WorldDescription]:
[HumanTrajectory]:
[HumanInstruction]:
Youareexpectedtogenerateasequenceofactionsthatareconsistentwiththehuman’sinstructionandtheworlddescription.
PleaserememberthateachobjecthasanumberID.
Rememberinthefinaloutput,thetypeofobjectshouldbespecific!!youCANNOTsaysomethinglikee.g.,note!!!!!
Pleasefirstthinkstep-by-step!
1.Whichobjectdoyouthinkthehumanisaskingfor?
2.Whatareyouractions?
AnswertheabovequestionsonebyoneNOMATTERYOUKNOWTHEGOALSPACEORNOT!
!!!!!THELASTPARTOFTHERESPONSEshouldbeasequenceofactionsstartingwiththestring”Actions:”(!!Onlyactions!
Nonumberingforactions,nodescription,noextrawords)!!!!!
Theavailableactiontemplatescontain:
movetoXXX
pickupXXX
pickupXXXfromXXX
putXXXintoXXX
putXXXontoXXX
takeXXXfromXXX
giveXXXtohuman
openXXX
Atypicalanswerformatisasfollows,wherethe”open”stepisoptional.
Actions:
movetoXXX
openXXX
pickupXXX
movetohuman
giveXXXtohuman
[Questions]:
1.Whichobjectdoyouthinkthehumanisaskingfor?
Thehumanhastakensomeactionsrelatedtothedocument,andisprobablyaskingforanotherdocumenttoputintothebox.The
humanalsorequiresthattheobjectshouldbedusty.Notethatthereisadustydocument2onthesofa,sothehumanisprobably
askingfordocument2.
2.Whatareyouractions?
Ishouldnavigatetothesofaandpickupthedustydocument2.ThenIshouldgiveittothehuman.
Actions:
movetosofa
pickupdocument2
movetohuman
givedocument2tohuman
15========================Nowpleasebegintogeneratetheoutputforgivenscenarios========================
[WorldDescription]:Data
[HumanTrajectory]:Data
[HumanInstruction]:Data
Youareexpectedtogenerateasequenceofactionsthatareconsistentwiththehuman’sinstructionandtheworlddescription.
PleaserememberthateachobjecthasanumberID.
Rememberinthefinaloutput,thetypeofobjectshouldbespecific!!youCANNOTsaysomethinglikee.g.,note!!!!!
Pleasefirstthinkstep-by-step!
1.Whichobjectdoyouthinkthehumanisaskingfor?
2.Whatareyouractions?
AnswertheabovequestionsonebyoneNOMATTERYOUKNOWTHEGOALSPACEORNOT!
!!!!!THELASTPARTOFTHERESPONSEshouldbeasequenceofactionsstartingwiththestring”Actions:”(!!Onlyactions!
Nonumberingforactions,nodescription,noextrawords)!!!!!
========================Let’sstart!========================
B.4 GPT-4+PE+FISER+PR
Part1:Taskinformation.SlightlydifferentfromGPT-4+PE+FISER,markedinblue.
Part2:Domainspecificknowledge.SameasGPT-4+PE+FISER,omitted.
Part3:Datapointexample.SameasGPT-4+PE+FISER,omitted.
Part4:Answerexample.
Part5:Currentdatapoint.SlightlydifferentfromGPT-4+PE+FISER,markedinblue.
Youarearobot.Everytimeyouwillreceiveadescriptionoftheworldandaninstructiongivenbythehumaninthefollowing
templates:
[WorldDescription]:
[HumanTrajectory]:
[HumanInstruction]:
Youareexpectedtogenerateasequenceofactionsthatareconsistentwiththehuman’sinstructionandtheworlddescription.
PleaserememberthateachobjecthasanumberID.
Rememberinthefinaloutput,thetypeofobjectshouldbespecific!!youCANNOTsaysomethinglikee.g.,note!!!!!
Pleasefirstthinkstep-by-step!
1.Whatishumandoing?(Pleasefindthemostpossiblegoalnameandgoalidifyouaregiventhegoalspace,otherwiseyou
shouldinferbyyourself,butyouSTILLneedoutput!)
2.Whichobjectdoyouthinkthehumanisaskingfor?
3.Whatareyouractions?
AnswertheabovequestionsonebyoneNOMATTERYOUKNOWTHEGOALSPACEORNOT!
!!!!!THELASTPARTOFTHERESPONSEshouldbeasequenceofactionsstartingwiththestring”Actions:”(!!Onlyactions!
Nonumberingforactions,nodescription,noextrawords)!!!!!
Theavailableactiontemplatescontain:
movetoXXX
pickupXXX
pickupXXXfromXXX
putXXXintoXXX
putXXXontoXXX
takeXXXfromXXX
giveXXXtohuman
openXXX
Atypicalanswerformatisasfollows,wherethe”open”stepisoptional.
Actions:
movetoXXX
openXXX
pickupXXX
movetohuman
giveXXXtohuman
16[Questions]:
1.Whatishumandoing?
Thehumanhastakensomeactionsrelatedtothedocument.Byputtingdocument0intobox0onthesofa,thehumanisprobably
organizingthedocuments.ItalignswithGoal0:boxingbooksupforstorage.
2.Whichobjectdoyouthinkthehumanisaskingfor?
Giventhattheoverallgoalcanbeorganizingthedocuments,thehumanisprobablyaskingforanotherdocumenttoputintothe
box.Thehumanalsorequiresthattheobjectshouldbedusty.Notethatthereisadustydocument2onthesofa,sothehumanis
probablyaskingfordocument2.
3.Whatareyouractions?
Ishouldnavigatetothesofaandpickupthedustydocument2.ThenIshouldgiveittothehuman.
Actions:
movetosofa
pickupdocument2
movetohuman
givedocument2tohuman
========================Nowpleasebegintogeneratetheoutputforgivenscenarios========================
[WorldDescription]:Data
[HumanTrajectory]:Data
[HumanInstruction]:Data
Youareexpectedtogenerateasequenceofactionsthatareconsistentwiththehuman’sinstructionandtheworlddescription.
PleaserememberthateachobjecthasanumberID.
Rememberinthefinaloutput,thetypeofobjectshouldbespecific!!youCANNOTsaysomethinglikee.g.,note!!!!!
Pleasefirstthinkstep-by-step!
1.Whatishumandoing?(Pleasefindthemostpossiblegoalnameandgoalidifyouaregiventhegoalspace,otherwiseyou
shouldinferbyyourself,butyouSTILLneedoutput!)
2.Whichobjectdoyouthinkthehumanisaskingfor?
3.Whatareyouractions?
AnswertheabovequestionsonebyoneNOMATTERYOUKNOWTHEGOALSPACEORNOT!
!!!!!THELASTPARTOFTHERESPONSEshouldbeasequenceofactionsstartingwiththestring”Actions:”(!!Onlyactions!
Nonumberingforactions,nodescription,noextrawords)!!!!!
========================Let’sstart!========================
17