Dimension-independent learning rates for high-dimensional
classification problems
Andres Felipe Lerma-Pineda2, Philipp Petersen2, Simon Frieder1,2, and Thomas
Lukasiewicz1,3
1Department of Computer Science, University of Oxford, UK
2Department of Mathematics, University of Vienna, Austria
3Institute of Logic and Computation, Vienna University of Technology, Vienna,
Austria
September 27, 2024
Abstract
We study the problem of approximating and estimating classification functions that have
their decision boundary in the RBV2 space. Functions of RBV2 type arise naturally as so-
lutions of regularized neural network learning problems and neural networks can approximate
these functions without the curse of dimensionality. We modify existing results to show that
everyRBV2 functioncanbeapproximatedbyaneuralnetworkwithboundedweights. There-
after, we prove the existence of a neural network with bounded weights approximating a clas-
sification function. And we leverage these bounds to quantify the estimation rates. Finally,
we present a numerical study that analyzes the effect of different regularity conditions on the
decision boundaries.
Keywords: Minimax bounds, noiseless training data, deep neural networks, classification
Mathematical Subject Classification: 68T05, 62C20, 41A25, 41A46
1 Introduction
Neural networks (NNs) have shown exceptional performance for highly demanding tasks that take
long periods of time and huge effort for humans [21, 11]. One famous application area of neural
networksisimageclassification,wheretheinputdimensionsoftheseNNscorrespondtothenumber
of pixels in the image, which is typically a large number. Therefore, one intriguing question is
whether or not these NNs are subject to the curse of dimensionality. An approximation method
is said to be subject to the curse of dimensionality if the performance of the method deteriorates
exponentially when the dimension grows [5, 26].
In[3],forexample,itwasprovedthatforacertainclassoffunctionsonEuclideanspaces—called
the Barron class—the number of neurons required for a neural network to approximate an element
of the Barron class does not grow exponentially with the dimension of the underlying space. More
concretely, it is possible to approximate a given Barron function by a shallow NN (i.e., two-layer)
1
4202
peS
62
]GL.sc[
1v19971.9042:viXrawith an arbitrary number of neurons N and approximation error in the L2 or L∞ norm of the
order N−1/2 [2, 3]. Such results have been generalized and applied to the study of discontinuous
functions as models for binary classification functions [8, 29]. In this work, we continue with the
studyoflearningdiscontinuousfunctionsbutnownotassociatedwiththeclassofBarronfunctions.
Instead, we consider the space of RBV2 functions [27], which is another functional class that has
been shown to be approximable without a curse of dimensionality.
The problem of learning discontinuous functions appears in many applications, such as, for
example, classification of images. A classification problem can be modeled via a function defined
on an Euclidean space, which is usually called the classifier. Here, we assume that a classifier
is a function of the form (cid:80)K c 1 where Ω ⊂ Rd, d ∈ N are disjoint sets and c is called
k=1 k Ωk k k
the label of Ω for k = 1,...,K. Examples of such labels are natural numbers 1,2,...,K. This
k
paper is concerned with the problem of approximating and estimating binary classifiers by NNs,
i.e., K = 2. Previous works on this topic can be found in [8, 15, 16, 30]. In all of these articles,
different assumptions on the boundaries ∂Ω are imposed, and results on the approximability and
k
estimability of the classifiers are discussed. Essentially, a more complex condition on the boundary
induces a harder learning problem. Our work complements these results by studying a further
assumption on the boundaries.
Wemodeltheregularityofthedecisionboundariesbyrequiringthemtobethegraphofaregular
function. This can be formalized through the concept of a horizon function. Horizon functions are
binary functions defined on a compact subset of Rd, d∈N taking values in {0,1}. In simple terms,
for a function f : Rd−1 → R, an associated horizon function h is given by h (x) = 1 ,
f f f(x[i])≤xi
where x = (x ,x ,...,x ...,x ) ∈ Rd and x[i] = (x ,...,x ,x ,...,x ) for some fixed i. For
1 2 i d 1 i−1 i+1 d
a more formal definition, we refer to Definition 4.1.
Asmentionedbefore,weconsiderthecasewheref belongstoRBV2. InSection3,weintroduce
several notions that lead us to the concept of RBV2 functions. For this space, we define the RTV2
seminorm. Intuitively, the RTV2 seminorm is a measure of the sparsity of the second derivatives
of a function f in the Radon domain. Under certain assumptions, shallow neural networks are
solutions for the problem of minimizing a functional of squared data-fitting errors plus the RTV2
seminorm [27, Theorem 1]. More importantly, the problem of training a NN that minimizes this
functional is equivalent to the problem of training a NN that minimizes squared data-fitting errors
with weight decay, i.e., a regularization term proportional to the squared Euclidean norm of the
NN’s weights. The RBV2 norm results from adding terms relating to the slope and value at 0 of
the underlying function.
We summarize our findings for the framework outlined above in the following subsection.
1.1 Our contribution
Here, we present our main results on the approximation and estimation of horizon functions with
the graph of an RBV2 function f as the decision boundary. A crucial role will be played by the
RBV2 norm (see Section 3). In particular, the magnitude of the weights of the approximating NN
of f depends on the RBV2 norm of f denoted as ∥f∥ . For the purpose of this study, we only
RBV2
consider the domain to be the closed unit ball with center at the origin. This ball is denoted as
Bd. When we refer to the RBV2 space, the domain is always Bd unless stated otherwise. Now, we
1 1
present the main results of this paper.
2Approximation of RBV2 functions: For every function f ∈ RBV2, it was proven in [27,
Theorem 8], that there is a shallow NN with K ∈ N neurons in the hidden layer that uniformly
approximates f with accuracy of the order of K−(d+3)/(2d). For our results in Section 4 to hold,
we need a slightly stronger result. For Theorem 4.3 and 4.5 to hold, we need a neural network
approximating f which has its weights bounded with a bound depending linearly on the RBV2
norm∥f∥ off. Thus,wehavemodifiedthemainstatementof[27,Theorem8]andcompleted
RBV2
the relevant missing steps of its proof to obtain Proposition 3.9, which is summarized below.
Proposition 1.1. Let d ∈ N, d ≥ 3 and let f ∈ RBV2(Bd). Then, for every N ∈ N, there is a
1
shallow NN f with N neurons in the hidden layer and with weights bounded by a constant C >0
N
that depends linearly on ∥f∥ such that
RBV2
∥f −f N∥
L∞(Bd)
≲
d
N−d 2+ d3 . (1.1)
1
Equation 1.1 tells us that the rate does not depend exponentially on the dimension d as the
exponent is −1/2−3/(2d). However, the exponent grows slowly when d→∞, which worsens the
approximation rate. Notice that as d → ∞, the approximation error behaves like N−1/2, which is
theratefortheBarronclassprovedin[3]. Thisapproximationratedoesnotexplodewhendgrows.
Thus, NNs overcome the curse of dimensionality for the class of RBV2 functions.
WeproveProposition1.1inSection3,butwenowprovideageneraloverviewofitsproof. Weusean
integral representation of every RBV2 function. Indeed, every f ∈RBV2(Bd) can be represented
1
as an integral term over the domain Sd−1×[−1,1] plus an affine linear function. The integral term
is given by
(cid:90)
ϱ(w⊤z−b)dµ(w,b), for all z ∈Bd (1.2)
1
Sd−1×[−1,1]
where µ is a measure on Sd×[−1,1] and ϱ is the ReLU activation function. We first consider the
case where z ∈ Sd and assume that µ is a probability measure, prove some intermediate results,
and then extend the argument to z ∈ Bd. We can see that (1.2) can be expressed as the sum of
1
three integral terms. In two out of three terms, the integral
(cid:90)
|w⊤z−b|dµ(w,b), for all z ∈Bd (1.3)
1
Sd−1×[−1,1]
appearsasafactorbutwithdifferentprobabilitymeasuresthatwillbedenotedasµ ,µ . Weprove
1 2
that this integral can be approximated by a NN thanks to [23, Theorem 1]. However, that result
requires the domain of integration to be a d-dimensional sphere. To solve this problem, we make a
change of variables to the Equation 1.3, which is presented in Lemma 3.10. Then, in Proposition
3.12, we finally show that we can approximate the integral (1.3) by a NN for all z ∈tSd, t>0. In
Lemma 3.14, we prove that the result holds even when the integral representation (1.3) is defined
for all z ∈ Bd. Finally, by showing that the factor (1.3) can be approximated by shallow NNs, we
1
prove in Lemma 3.14 that (1.2) can be approximated by a NN, which in turn leads to Theorem
3.15. In Theorem 3.15, we state that every RBV2 function where the integral representation (3.1)
has µ as a probability measure can be approximated by a shallow NN. Thereafter, the general case
where µ is an arbitrary measure is proved in Proposition 3.9.
3Approximation of horizon functions associated to a RBV2 functions: Based on Propo-
sition 3.9, we derive the following theorem on the existence of a NN that approximates a hori-
zon function associated with an RBV2 function. To state the result, we need the concept of a
tube-compatible measure introduced in [8, Section 6]. These are measures such that in every ϵ-
neighborhoodofeverycurve, themassoftheneighborhoodscaleslikeCϵα forconstantsC >0and
α>0.
Theorem 1.2. Let d∈N , N ∈N, q,C >0, and α∈(0,1]. Further let h be a horizon function
≥2 f
associated to f ∈{g ∈RBV2(Bd−1): ∥g∥ ≤q}. Then, there exists a NN I with two hidden
1 RBV2 N
layers such that for each tube compatible measure µ with parameters α,C, we have
µ({x∈B 1d :h f(x)̸=I N(x)})≲
d
N−αd 2+ d3 . (1.4)
Moreover, I has at most d+N +5 neurons and at most (d+3)N +2d+11 non-zero weights.
N
The weights (and biases) of I
N
are bounded in magnitude by O(Nd 2+ d3) for N →∞.
The proof of this theorem is organized as follows:
1. We note that h (x)=H(x −f(x[i])), where H is the Heaviside function.
f i
2. We use Proposition 3.9 to uniformly approximate f ∈ RBV2 by a NN f up to an error of
N
N−(d+3)/(2d).
3. We prove that the Heaviside function H(x) = 1 can be approximated by a NN H for
(0,∞) δ
all δ > 0 such that the function H and the Heaviside function H differ only on the interval
δ
(−δ,δ).
4. Choosing δ = N−(d+3)/(2d), we observe that h (x) ̸= H (x −f (x[i])) only for x outside of
f δ i N
a 2δ strip around the decision boundary f(x[i])=xi.
5. Finally,weprove(1.4)(cf. thecorrespondingEquation4.2fromSection4)usingtheproperties
of a tube-compatible measure.
Upper bounds on learning: Finally, our approximation results lead us to the problem of esti-
mating a horizon function associated with a function f when a training set S =(x ,y )m , m∈N
i i i=1
is given. In Section 4, we analyze the performance of the standard empirical risk minimization
procedure, where the loss function is the Hinge loss and the hypothesis set is a certain class of
ReLU NNs. Our result on learning is Theorem 4.5: If we consider as hypothesis set the set of NNs
with two layers and at most N = m2d/(3d+3) neurons, we prove that for all κ > 0, any minimizer
ϕ
m,S
for a training set S has a risk of at most O(m− 3d d+ +3 3+κ). The proof of Theorem 1.2 (cf. the
correspondingTheorem4.3fromSection4)issimilartothatof[29, Theorem5.7]. Thisfinalresult
is studied numerically in Section 5. As there are similar results when one changes the smoothness
conditions on f, we could ask ourselves how the smoothness of f affects the learnability of the
function in practice. To answer that question, we compare the test error after training NNs for
different conditions on the function f, which amounts to assuming f is an element of a ball with
respect to various norms or seminorms. We verify numerically that functions in a Barron-norm or
RBV2 norm ball can be learned better by NNs than functions in L∞ or L1 balls.
41.2 Related work
Ourresultsconcerntwocentralaspectsoflearningproblems. First,theapproximationandlearning
without the curse of dimensionality, and second, the approximation and learning of discontinuous
classifier functions. We will review the work related to these two themes below.
Approximation and estimation of functions by shallow neural networks: It has been
shown that shallow NNs can break the curse of dimensionality. This was widely discussed in [3].
In fact, the approximation problem for functions with one finite Fourier moment—called Barron
functions—isnotaffectedbytheunderlyingdimension. In[8,22,34,37],differentextensionsofthe
notion of a Barron function are discussed. The space of RBV2 functions is closely related to the
problem of function approximated by shallow NNs as well (see [3]). This space is associated with
the problem of estimating a function from a set of samples when a regularization term is added
to the loss function. Shallow NNs also break the curse of dimensionality in the approximation
problem for the space of RBV2 functions (see [27]). Several other function classes for which the
curse of dimensionality can be overcome by deep NNs instead of shallow NNs have been proposed,
such as the class of composition of low dimensional functions [32, 33, 25, 10, 31], bandlimited
functions [24] and solutions of some high-dimensional PDEs [13, 14, 17]. Strictly speaking, there
is a dependence in the approximation rate on the ambient dimension for these classes of functions,
but such dependency is usually polynomial.
Approximation of discontinuous functions by neural networks: There are different ap-
proaches to the problem of classification. In [36], the authors study the problem of classification
associated with two different sets C and C . It is assumed that the distance between these two
+ −
setsispositive. Undersuchconditions,someapproximationandestimationboundsbyshallowNNs
for the classification problem are presented. One may consider the classifier to be a function of the
form (cid:80)K f 1 , where each Ω ⊂ Rd has a piecewise smooth boundary ∂Ω and f : Rd → R
k=1 k Ωk k k
are smooth functions. The analysis for the problems of analyzing and estimating such classifiers
is discussed in [15, 16, 30]. Therein, it is proved that the approximation and estimation rates are
strongly determined by the regularity of the boundary ∂Ω when the functions f are sufficiently
k k
smooth. In [8], the same approach is followed, but a different condition on ∂Ω is imposed. The
k
boundaries are assumed to be locally parametrized by Barron functions. Indeed, this idea has
inspired our own work, and some of the ideas of [8] are similar to the ones we discussed here.
Approximation of discontinuous functions by other approaches: Toconcludethissubsec-
tion, we mention some non-deep-learning-based techniques that have been applied to the approxi-
mation of piecewise smooth functions. Although indicator functions belong to the set of piecewise
constant functions, it turns out that the set of piecewise constant functions and the set of piece-
wise smooth functions admit the same approximation rates [30]. Various representation methods
have been applied to approximate piecewise smooth functions. Shearlets and curvelet systems
achieve (almost) optimal N-term approximation rates for C2(R2) functions with C2 jump curves
(see [6, 7, 12, 19, 35]). For the study of approximation of two-dimensional functions with jump
curves smoother than C2, the bandelet system is proposed in [20]. This bandelet system is made
up of a set of properly smoothly-transformed boundary-adapted wavelets optimally adapted to the
smooth jump curves. Finally, a different representation system called surflets (see [9]) yields opti-
mal approximation of piecewise smooth functions. This system is constructed by using a partition
5of unity as well as local approximation of horizon functions.
1.3 Structure of the paper
In Section 2 we introduce several definitions related to NNs. The space of functions implemented
by ReLU NN with L layers, at most N neurons and W non-zero weights bounded by B, which is
Definition 2.2 is particularly important in Section 4. In Section 3, we provide a formal definition
of the space of RBV2 functions. We first define this space on an Euclidean space, and we then
present the notion of the RBV2 space on a bounded subset. We prove that each RBV2 function
can be efficiently uniformly approximated by a shallow NN in Proposition 3.9. In Section 4, we
studythelearningproblemofestimatingahorizonfunctionassociatedwithaRBV2 functionfrom
a sample. We present upper bounds for the risk of the minimizer empirical risk when we train our
NN with the Hinge loss in Theorem 4.5. Finally, in Section 5, we study numerically our results on
learning and compare the practical learning rates for various regularity conditions on the decision
boundary.
2 Neural Networks
Although there are different types of NNs, for this study, we restrict ourselves to the well-known
typeoffeed-forwardNNs. Wecollectseveralusefuldefinitionsthatpavethewayforourtheoretical
results, which are provided in the definition below. The NN formalism underlying this definition
was introduced in [30, Definition 2.1].
Definition 2.1. Let d,L∈N. A neural network (NN) Φ with input dimension d and L layers is a
sequence of matrix-vector tuples
(cid:0) (cid:1)
Φ= (A ,b ),(A ,b ),...,(A ,b ) ,
1 1 2 2 L L
where, for N
0
=d and N 1,...,N
L
∈N, each A
ℓ
is an N ℓ×N
ℓ−1
matrix, and b
ℓ
∈RNℓ.
For a NN Φ and an activation function σ :R→R, we define the associated realization of the NN
Φ as
R Φ: Rd →RNL, x(cid:55)→x =R Φ(x),
σ L σ
where the output x
L
∈RNL results from the scheme
x :=x∈Rd,
0
x
ℓ
:=σ(A ℓx ℓ−1+b ℓ)∈RNℓ for ℓ=1,...,L−1,
x :=A x +b ∈RNL.
L L L−1 L
Here σ is understood to act coordinate-wise. We call
N(Φ):=d+(cid:80)L
N the number of neurons
j=1 j
of Φ, L(Φ) := L the number of layers, and W(Φ) :=
(cid:80)L
(∥A ∥ +∥b ∥ ) is called the number
j=1 j 0 j 0
of weights of Φ. Here, ∥A∥ and ∥b∥ denote the number of non-zero entries of the matrix A and
0 0
the vector b, respectively. Moreover, we refer to N as the output dimension of Φ. The activation
L
function ϱ : R → R,x (cid:55)→ max{0,x} is called the ReLU. We call R Φ a ReLU neural network.
ϱ
Finally, the vector (d,N ,N ,...,N )∈NL+1 is called the architecture of Φ.
1 2 L
Asanindependentdefinition, weintroducethesetofNNswithfixedL, W, anddandanupper
bound B >0 on the modulus of the weights.
6Definition 2.2. Let d∈N , N,W,L∈N, and B >0. We denote by NN(d,L,N,W,B) the set
≥2
of ReLU NNs where the underlying NNs have L layers, at most N neurons per layer, and at most
W non-zero weights. We also assume that the weights of the NNs are bounded in absolute value by
B. Moreover, we set
NN
(d,L,N,W,B):=(cid:8)
f ∈NN(d,L,N,W,B) : 0≤f(x)≤1 for all
x∈[0,1]d(cid:9)
.
∗
3 Approximation of RBV 2 functions
In [28], the authors demonstrate that shallow NNs are an optimal ansatz system for solving the
estimation problem of a function f ∈ RBV2 from a sample S. In this section, we give some
insight into this RBV2 space and present some of its properties. Our main result, Proposition 3.9
demonstrates that every function f in RBV2 with f(0) = 0 can be approximated by a NN with
weights bounded depending on a norm of the function only. The bounds on the weights are crucial
to show learning bounds in Section 4. We point out that the existence of an approximating NN
was already shown in [28, Theorem 8]. That result, however, does not include any control of the
weights.
3.1 The set of RBV2 functions
We start this subsection with some useful definitions related to the notion of the RBV2 space.
For an extensive survey on the class of Radon-regular functions, we refer to [27], where all the
definitions and results of this subsection have been taken from. We first define the RBV2 space
whenthedomainistheEuclideanspaceRd andcontinuetorestrictitsdomaintoaboundedsubset
Ω. At this stage, it is necessary to introduce the notion of Radon transform.
Definition 3.1. Let d ∈ N. For a function f : Rd → R, we define its Radon transform as the
function Rf :Sd−1×R→R,
(cid:90)
Rf(w,s):= f(x)dS(x),
{x:w⊤x=s}
ifsuchanintegralexists, where(w,s)∈Sd−1×RanddS denotesthesurfaceintegralonthedomain
{x:w⊤x=s}.
We next introduce the concept of the ramp filter, which appears in the inversion formula of the
Radon transform.
Definition 3.2. Let d∈N. The ramp filter Λd−1 is defined as
Λd−1 :=(−∂2)(d−1)/2,
t
where ∂2 is the partial derivative with respect to t.
t
Therampfilterhelpsustodefinethesecond-ordertotalvariationofagivenfunctionf. Herein,
M(X) denotes the space of signed Borel measures defined on a set X.
7Definition 3.3. Let d∈N. We defined the second-order total variation of a function f :Rd →R
as
1
RTV2 (f):= ∥∂2Λd−1R(f)∥ ,
Rd 2(2π)d−1 t M(Sd−1×R)
where ∥·∥ denotes the total variation norm.
M
The second-order total variation of a function is not a norm but a seminorm. It can be made
into a norm for a Banach space when other terms are added. We first define the associated space,
RBV2, and then the norm.
Definition 3.4. Let d∈N. The RBV2 space is defined as the set
RBV2(Rd):={f ∈L∞,1: RTV2 (f)<∞},
Rd
where the space L∞,1 contains all functions f such that sup |f(x)|(1+∥x∥ )−1 <∞.
x∈Rd 2
We now introduce a norm on RBV2(Rd) that turns it into a Banach space.
Definition 3.5. Let d∈N. For every function f ∈RBV2(Rd) we define its RBV2 norm as
d
(cid:88)
∥f∥ :=RTV2 (f)+|f(0)|+ |f(e )−f(0)|,
RBV2(Rd) Rd k
k=1
where {e }d denotes the canonical basis of Rd.
k k=1
Note that the point evaluations in Definition 3.5 are well defined since f is guaranteed to be
Lipschitz continuous by [28, Lemma 2.11.].
Afterthiscrucialdefinition,ournextstepistodefinethespaceofRBV2 functionsonabounded
domain Ω. As we see below, the space RBV2(Ω) can be defined in terms of the elements of
RBV2(Rd).
Definition 3.6. Let d∈N and let Ω⊂Rd. We define the RBV2(Ω) space as
RBV2(Ω):={f ∈D′(Ω): ∃g ∈RBV2(Rd) s.t g| =f},
Ω
where D′(Ω) denotes the space of distributions on Ω. Moreover, we define
RTV2(f):= inf RTV2 (g)
Ω Rd
g∈RBV2(Rd):f=g|Ω
and
∥f∥ := inf ∥g∥ .
RBV2(Ω) RBV2(Ω)
g∈RBV2(Rd):f=g|Ω
Onecanprovethatforagivenf ∈RBV2(Ω), thereisafunctionf ∈RBV2(Rd)thatadmits
ext
an integral representation and has the property that f | = f. This is shown in the following
ext Ω
lemma.
8Lemma3.7([27,Lemma2]). Letd∈N, Ω⊂Rd beaboundedset, andletϱbetheReLUactivation
function. For each f ∈RBV2(Ω), there is a function f ∈RBV2(Rd) such that
ext
(cid:90)
f (x)= ϱ(w⊤x−b)dµ(w,b)+c⊤x+c ,
ext 0
Sd−1×R
for all x∈Rd, where µ∈M(Sd−1×R) and supp(µ)⊂Z , where the set Z is the closure of
Ω Ω
{z =(w,b)∈Sd−1×R: {x:w⊤x=b}∩Ω̸=∅},
and f | =f. Moreover, RTV2(f)=RTV2 (f )=∥µ∥ .
ext Ω Ω Rd ext Sd−1×R
Inpractice, wemayencounterfunctionsdefinedonarbitrarysubsetsΩ⊂Rd. Forsimplicity, all
theresultsofthispaperassumethatthedomainofthefunctionf istheunitballwithcenteratthe
origin. This unit ball is denoted by Ω = Bd(0) = Bd. In addition, we denote RTV2(f) = RTV2
1 1 Bd
1
and ∥f∥ =∥f∥ .
RBV2 RBV2(Bd)
1
Remark 3.8. In the setting of Lemma 3.7 with Ω = Bd, it is shown in [27, Lemma 2 and Remark
1
4] that
Z =Sd−1×[−1,1],
Ω
and therefore, every function f ∈RBV2(Bd) admits a representation
1
(cid:90)
f(x)= ϱ(w⊤x−b)dµ(w,b)+c⊤x+c , (3.1)
0
Sd−1×[−1,1]
for all x ∈ Bd. Notice that, since 0,e ,...,e ∈ Bd for every extension f of f, it holds that
1 1 d 1 ext
∥f ∥ =∥f∥ . We use this last equality to derive bounds for ∥c∥ and |c | in terms
ext RBV2(Rd) RBV2 ∞ 0
of ∥f∥ . Let us assume that ∥f∥ =C for C >0 and f(0)=0. Therefore,
RBV2 RBV2
(cid:90)
ϱ(−b)dµ(w,b)+c =0.
0
Sd−1×[−1,1]
(cid:82)
Clearly, 0 ≤ ϱ(−b) ≤ 1. Thus, −C ≤ ϱ(−b)dµ(w,b) ≤ C. Hence, |c | ≤ C. Now, as
Sd−1×[−1,1] 0
the canonical basis {e }d ⊂Bd, we can use (3.1) to compute f(e ) for all k =1,2,...,d. Notice
k k=1 1 k
that |f(e )|≤∥f∥ ≤C which leads us to
k RBV2
(cid:90)
−C ≤ ϱ(w −b)dµ(w,b)+c +c ≤C
k k 0
Sd−1×[−1,1]
becausewTe −b=w −bwherew isthek-thcoordinateofthevectorw. Clearly,−2≤w −b≤2
k k k k
and since by Lemma 3.7 ∥µ∥ =RTV2(f)≤∥f∥ , we conclude
Sd−1×R RBV2
(cid:90)
−2C ≤ ϱ(w −b)dµ(w,b)≤2C.
k
Sd−1×[−1,1]
(cid:82)
Then, −3C ≤ ϱ(w −b)dµ(w,b)+c ≤ 3C holds and in turn −4C ≤ c ≤ 4C. Thus,
Sd−1×[−1,1] k 0 k
we obtain that ∥c∥ ≤4∥f∥ and |c |≤∥f∥ .
∞ RBV2 0 RBV2
93.2 Approximation of RBV2 functions by neural networks
In the remainder of this section, we prove that f ∈RBV2(Bd) can be approximated by a NN with
1
bounded weights. This is an adapted version of [27, Theorem 8].
Proposition 3.9. Let d ∈ N, d ≥ 3 and let f ∈ RBV2(Bd). Then, for every N ∈ N, there are
1
w ∈Rd, b ,v ∈R c∈Rd and c ∈R with
k k k 0
|v |,|b |,|c |,∥c∥ ,∥w ∥ ≤5∥f∥ ,
k k 0 2 k 2 RBV2
where k =1,2,...,N, such that for
N
(cid:88)
f (x):= v ϱ(w⊤x−b )+c⊤x+c , for x∈Bd,
N k k k 0 1
k=1
it holds that
∥f −f N∥
L∞(Bd)
≲
d
∥f∥ RBV2N−d 2+ d3 .
1
In particular, we have that f ∈NN(d,2,N +2,(2+d)(N +2)+1,5∥f∥ ).
N RBV2
Before we prove Proposition 3.9, we spend some time stating and proving several other state-
ments that pave the way for its proof. Our starting point for the proof of Proposition 3.9 is the
integralrepresentationofg asinEquation3.1. Duetothepropertiesoftheabsolutevaluefunction,
wecandecomposethisintegralasthesumofotherintegrals. Thiswillbecomeevidentintheproof
of Theorem 3.15. At this stage, we focus our attention on the integral term
(cid:90)
|w⊤z−b|dµ(w,b), for z ∈Bd. (3.2)
1
Sd−1×[−1,1]
We begin implementing a change of variable for the integral (3.2), which in turn allows us to prove
theexistenceofaNN.WecaneasilyseethattheregionofintegrationSd×[−1,1]isacylinderwith
axis parallel to the axis z of Rd+1, radius r =1 and lower and upper boundaries at z =−1
d+1 d+1
and z =1, respectively. Let us first assume that z ∈Sd−1. Notice that the integral term
d+1
(cid:90)
|w⊤z−b|dµ(w,b), for z ∈Sd, (3.3)
Sd−1×[−1,1]
can be transformed into an equal integral by multiplying and dividing the integrand function by
√
(cid:112)
the term ∥w∥2+b2 = 1+b2 as follows
(cid:90)
(cid:112)
(cid:18) |w⊤z−b|(cid:19)
1+b2 √ dµ(w,b). (3.4)
Sd−1×[−1,1] 1+b2
Theprocesstotransformsuchanintegralintoanotherintegral,thedomainofwhichiscontained
in the sphere Sd ⊂Rd+1 is shown in Lemma 3.10.
Lemma 3.10. Let d ∈ R, z ∈ Sd, w = (w ,w ,...,w )⊤ ∈ Sd−1, and b ∈ [−1,1]. Further, let
1 2 d
v =(v ,v ,...,v )⊤ be defined as follows:
1 2 d+1
w
v := √ i ,
i
1+b2
10for i=1,2,...,d and
b
v := √ .
d+1
1+b2
Moreover, let ϕ:Sd−1×[−1,1]→Sd be the function defined as ϕ(w,b)=(v ,v ,...,v )⊤. Then,
1 2 d+1
for all z ∈Sd we have
(cid:90) |w⊤z−b|dµ(w,b)=(cid:90)
(cid:113)
1 (cid:12) (cid:12)v⊤z (cid:101)(cid:12) (cid:12)χ B(v)dϕ∗µ(v),
Sd−1×[−1,1] Sd 1−v2
d+1
where z =(z,−1)⊤ ∈Rd+1, B =ϕ(Sd−1×[−1,1])⊂Sd ⊂Rd+1, χ is the characteristic function
(cid:101) B
of B, and ϕ∗µ denotes the push-forward of the measure µ.
Proof. The integral term in (3.3) can be reorganized as Equation 3.4 which leads to
(cid:90)
(cid:112)
(cid:18) |w⊤z−b|(cid:19) (cid:90)
(cid:112)
(cid:18) |⟨(w,b),(z,−1)⟩|(cid:19)
1+b2 √ dµ(w,b)= 1+b2 √ dµ(w,b).
Sd−1×[−1,1] 1+b2 Sd−1×[−1,1] 1+b2
Besides, it is clear that
v2
b2 = d+1 ,
1−v2
d+1
and
v2 1
1+b2 =1+ d+1 = .
1−v2 1−v2
d+1 d+1
After applying change of variables (see for example [4, Section 19, Corollary 1]), we obtain
(cid:90) (cid:112) 1+b2(cid:18) |w √⊤z−b|(cid:19) dµ(w,b)=(cid:90)
(cid:113)
1 (cid:12) (cid:12)v⊤z (cid:101)(cid:12) (cid:12)χ B(v)dϕ∗µ(v),
Sd−1×[−1,1] 1+b2 Sd 1−v2
d+1
where z and v are defined as stated in the assumptions.
(cid:101) i
Remark 3.11. Under the assumptions of Lemma 3.10, we make the following observations.
1. Since b∈[−1,1], we have that 2b2 ≤b2+1 and hence b2/(b2+1)≤1/2. Taking the square
root yields that
1
|v |≤ √ .
d+1
2
Hence, B is a subset of the set of elements in the sphere for which the last coordinate lies
√ √
between −1/ 2 and 1/ 2.
2. We know that the function h:B ⊂Sd →R given by h(v)=(1−v2 )−1/2 is continuous and
d+1
thus integrable. This holds because the σ-algebra on Sd is that of the Borelian sets, which is
√
the restriction of the Borel σ-algebra of Rd+1. In addition, h is a function bounded by 2.
1
Indeed, from the previous step, we have v2 ≤ , and hence 1−v2 ≥1/2 and
d+1 2 d+1
1 √
≤ 2.
(cid:113)
1−v2
d+1
113. If we denote by C the following quantity
(cid:90) 1
C := χ (v)dϕ∗µ(v),
(cid:113) B
Sd 1−v2
d+1
and by ν the following measure
1
dν = χ (v)dϕ∗µ,
(cid:113) B
C 1−v2
d+1
we have that ν is a probability measure on the Borelian set of Sd. Then, we can apply [23,
Theorem 1] for the integral
(cid:90)
(cid:12) (cid:12)v⊤z (cid:101)(cid:12) (cid:12)dν(v),
Sd
if z ∈ Sd. That is, for ϵ > 0, there is a set Q = {v(1),v(2),...,v(r)} ⊂ Sd with r ∈ N and
(cid:101)
r ≤C(d)ϵ−2+6/(d+3) for C =C(d)>0 a constant and d≥3, such that
(cid:12) (cid:12)
(cid:12)(cid:90) 1(cid:88)r (cid:12)
(cid:12) |v⊤z|dν(v)− |((v(i))⊤z)|(cid:12)<ϵ.
(cid:12) r (cid:12)
(cid:12) Sd (cid:12)
i=1
We cannot apply Remark 3.11 directly, because, even though v =(v ,...,v )⊤ ∈Sd ⊂Rd+1,
1 d+1√
it holds that z ∈/ Sd. Indeed, for z ∈ Sd, the vector z˜= (z,−1) is an element of 2Sd. However,
(cid:101)
the next result shows that the conclusions of Remark 3.11 can be generalized for z in a sphere of
arbitrary radius.
Proposition 3.12. Lett>0, d∈N, andd≥3. Then, forϵ>0, thereis{v(1),v(2),...,v(r)}⊂Sd
wherer ∈N,andr ≤C(t,d)ϵ−2+6/(d+3) forC =C(t,d)>0aconstantdependingonthedimension,
such that for all z ∈tSd ⊂Rd+1
(cid:12) (cid:12)
(cid:12)(cid:90) 1(cid:88)r (cid:12)
(cid:12) |v⊤z|dν(v)− |((v(i))⊤z)|(cid:12)<ϵ.
(cid:12) r (cid:12)
(cid:12) Sd (cid:12)
i=1
Proof. As z ∈ tSd, there is a z ∈ Sd such that z = tz. We now consider two cases. First, let us
(cid:101) (cid:101)
assume that t > 1. By Remark 3.11, for ϵ > 0 there is a set {v(1),v(2),...,v(r)} ⊂ Sd with r ∈ N,
r ≤C(d)ϵ−2+6/(d+3)t2−6/(d+3) such that
(cid:12) (cid:12)
(cid:12)(cid:90) 1(cid:88)r (cid:12)
(cid:12) |v⊤z|dν(v)− |(v(i))⊤z|(cid:12)<ϵ/t.
(cid:12) (cid:101) r (cid:101)(cid:12)
(cid:12) Sd (cid:12)
i=1
Thus,
(cid:12) (cid:12)
(cid:12)(cid:90) 1(cid:88)r (cid:12)
(cid:12) |v⊤tz|dµ(v)− |(v(i))⊤tz|(cid:12)<ϵ.
(cid:12) (cid:101) r (cid:101)(cid:12)
(cid:12) Sd (cid:12)
i=1
Now, if t<1
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)(cid:90) 1(cid:88)r (cid:12) (cid:12)(cid:90) 1(cid:88)r (cid:12)
(cid:12) |v⊤tz|dµ(v)− |(v(i))⊤tz|(cid:12)=t(cid:12) |v⊤z|dµ(v)− |(v(i))⊤z|(cid:12)<tϵ<ϵ.
(cid:12) (cid:101) r (cid:101)(cid:12) (cid:12) r (cid:12)
(cid:12) Sd (cid:12) (cid:12) Sd (cid:12)
i=1 i=1
Therefore, in any case r ≤C(d)ϵ−2+6/(d+3)max{1,t}2−6/(d+3) =C(d,t)ϵ−2+6/(d+3).
12√
Remark 3.13. We would like to highlight that the previous result holds in particular for t = 2.
√
As (z,−1) ∈ 2Sd when z ∈ Sd, this case is especially relevant. That being said, we proceed to
approximate the term (3.2). For a given element v(i) ∈ B ⊂ Sd as in Proposition 3.12, we define
w(i) = (v(i),v(i),...,v(i)) and b = v(i) where v(i) denotes the k−th component of v(i) for all
1 2 d i d+1 k
i=1,2,...r. It is clear that sup |w(i)|≤1 and |b |≤1. Therefore, for z ∈Sd
k=1,...,d+1 k i
(cid:12) (cid:12)
(cid:12)(cid:90) 1(cid:88)r (cid:12)
(cid:12) |w⊤z−b|dµ(v)− |(w(i))⊤z−b |(cid:12)<ϵ.
(cid:12) r i (cid:12)
(cid:12) Sd (cid:12)
i=1
Now, we are set to prove that there is a NN that approximates the integral term of Equation
3.1. We first assume that µ of (3.1) is a probability measure.
Lemma 3.14. Let d ∈ N, d ≥ 3 and let µ be a probability measure over Sd−1×[−1,1]. Then, let
f :B1 →R be given by
d (cid:90)
f(z)= ϱ(w⊤z−b)dµ(w,b),
Sd−1×[−1,1]
where z ∈Sd−1. Then, there exist vectors w(1),w(2),...w(r) ∈Rd and real numbers b ,b ,...b ∈R
1 2 r
with r ∈N, such that for all z ∈Sd−1
(cid:12) (cid:12)
(cid:12)(cid:90) 1(cid:88)r (cid:12)
(cid:12) |w⊤z−b|dµ(w,b)− ϱ((w(i))⊤z−b )−ϱ(−(w(i))⊤z+b )(cid:12)<ϵ,
(cid:12) r i i (cid:12)
(cid:12) Sd−1×[−1,1]
i=1
(cid:12)
where sup |w(i)|≤1 and |b |≤1, i=1,2,...r, where r ≤C(d)ϵ−2+6/(d+3).
k=1,...,d+1 k i
Proof. By Remark 3.13 there are w(i) ∈[−1,1]d and b ∈[−1,1] such that
i
(cid:12) (cid:12)
(cid:12)(cid:90) 1(cid:88)r (cid:12)
(cid:12) |w⊤z−b|dµ(v)− |(w(i))⊤z−b |(cid:12)<ϵ.
(cid:12) r i (cid:12)
(cid:12) Sd−1×[−1,1]
i=1
(cid:12)
As |x|=ϱ(x)−ϱ(−x), we have
r r
1(cid:88) 1(cid:88)
|(w(i))⊤z−b |= ϱ((w(i))⊤z−b )−ϱ(−(w(i))⊤z+b ), (3.5)
r i r i i
i=1 i=1
and hence
(cid:12) (cid:12)
(cid:12)(cid:90) 1(cid:88)r (cid:12)
(cid:12) |w⊤z−b|dµ(w,b)− ϱ((w(i))⊤z−b )−ϱ(−(w(i))⊤z+b )(cid:12)<ϵ.
(cid:12) r i i (cid:12)
(cid:12) Sd−1×[−1,1]
i=1
(cid:12)
We now have all the ingredients to prove Theorem 3.15. In the proof of the result, we use
Lemma3.7twicewhen(3.1)isexpressedastheadditionofthreeterms. Weconsiderthecasewhen
µ is a probability measure. The case of general measures will be shown in a corollary thereafter.
13Theorem 3.15. Let d≥3 and µ∈M(Sd−1×[−1,1]) be a probability measure. Let g :Bd →R be
1
the function defined as
(cid:90)
g(z)= ϱ(w⊤z−b)dµ(w,b), for z ∈Bd.
1
Sd−1×[−1,1]
Then, for all ϵ>0, g can be uniformly approximated with approximation error ϵ by a shallow NN
K
(cid:88)
f(cid:101)(z)= v iϱ((w(i))⊤z−b i)+(w(0))⊤z+b 0,
i=1
where w(0),w(i) ∈Rd, b ,b ∈R, v ∈R and
0 i i
∥w(0)∥ ,∥w(i)∥ ,|v |,|b |,|b |≤1,
∞ ∞ i i 0
for all i=1,2,...,K where K ≤C(d)ϵ−2+6/(d+3).
Proof. BecausetheReLUactivationfunctioncanbeexpressedasϱ(x)=(x+|x|)/2andduetothe
following argument in [1, Proposition 1], we have that
1(cid:90) 1(cid:90)
g(z)= (w⊤z−b)dµ(w,b)+ |w⊤z−b|dµ(w,b)
2 2
Sd−1×[−1,1] Sd−1×[−1,1]
1(cid:90)
= (w⊤z−b)dµ(w,b)
2
Sd−1×[−1,1]
µ (Sd−1×[−1,1])(cid:90) dµ (w,b)
+ + |w⊤z−b| +
2 µ (Sd−1×[−1,1])
Sd−1×[−1,1] +
µ (Sd−1×[−1,1])(cid:90) dµ (w,b)
− − |w⊤z−b| − , (3.6)
2 µ (Sd−1×[−1,1])
Sd−1×[−1,1] −
for z ∈ Sd and µ = µ +µ is a Jordan decomposition. Furthermore, it was proved in Lemma
+ −
3.14 that for ϵ > 0 and probability measure µ ∈ M(Sd−1 × [−1,1]), there is an r ∈ N and
w(1),w(2),...,w(r) ∈Rd, b ,b ,...,b ∈[−1,1] such that
1 2 r
(cid:12) (cid:12)
(cid:12)(cid:90) 1(cid:88)r (cid:12)
(cid:12) |w⊤z−b|dµ− ϱ((w(i))⊤z−b )−ϱ(−(w(i))⊤z+c )(cid:12)<ϵ,
(cid:12) r i i (cid:12)
(cid:12) Sd−1×[−1,1]
i=1
(cid:12)
for all z ∈ Bd and r ≤ C(d)ϵ−2+6/(d+3) where C = C(d) > 0 is a constant depending on the
1
dimension. Due to the fact that
dµ dµ
+ and −
µ (Sd−1×[−1,1]) µ (Sd−1×[−1,1])
+ −
are probability measures, we can approximate each of the last two terms of Equation 3.6 according
to Lemma 3.14 as
(cid:12) (cid:12)
(cid:12) (cid:12)(cid:90)
|w⊤z−b|dµ −
µ
s,±
(cid:88)r
ϱ((w(i))⊤z−b )−ϱ(−(w(i))⊤z+b
)(cid:12)
(cid:12)<ϵµ ≤ϵ,
(cid:12) ± r ± ±,i ± ±,i (cid:12) s,±
(cid:12) Sd−1×[−1,1]
i=1
(cid:12)
14respectively,whereµ =µ (Sd−1×[−1,1]))≤1andw(i) andb denotestheweightsandbiases
s,± ± + +,i
when the measure involved is µ and w(i) and b are defined likewise for µ . Moreover, the first
+ − −,i −
term of (3.6) can be expressed as
1(cid:90)
(w⊤z−b)dµ(w,b)=(w(0))⊤z−b ,
2 0
Sd−1×[−1,1]
where ∥w(0)∥ ≤1 and |b |≤1. Setting
∞ 0
r
f(cid:101)(z)=(w(0))⊤z−b 0+ µ 2s r,+ (cid:88) ϱ((w +(i))⊤z−b +,i)−ϱ(−(w +(i))⊤z+b +,i)
i=1
r
+ µ s,− (cid:88) ϱ((w(i))⊤z−b )−ϱ(−(w(i))⊤z+b ),
2r − −,i − −,i
i=1
and K =4r, we can rearrange f(cid:101)in such a way that
K
(cid:88)
f(cid:101)(z)= v iϱ((w(i))⊤z−b i)+(w(0))⊤z+b 0,
i=1
wherev iseitherµ /(2r)orµ /(2r)dependingonwhetherw(i) wasdeterminedbyµ orµ .
i s,+ s,− s,+ s,−
Notice that, by construction K ≤ C(d)ϵ−2+6/(d+3) for C = C(d) > 0 a constant. Additionally, all
weights and biases are bounded in norm by 1,
|v |,|b |,|b |,∥w(i)∥ ,∥w(0)∥ ≤1
i i 0 ∞ ∞
for i=1,2,...,K. To conclude, notice that by construction ∥g−f(cid:101)∥
∞
<ϵ.
We now proceed by generalizing Theorem 3.15 to arbitrary measures.
Corollary 3.16. Let d ∈ N, d ≥ 3 and let f ∈ RBV2(Bd) with f(0) = 0. Then, for all ϵ > 0,
1
there exist w(i) ∈Rd, b ,v ∈R, c∈Rd and c ∈R with
i i 0
|v |,|b |,|c |,∥c∥ ,∥w(i)∥ ≤5∥f∥ ,
i i 0 ∞ ∞ RBV2
where i=1,2,...,K and K ≤C(d)∥f∥2−6/(d+3)ϵ−2+6/(d+3), such that f can be uniformly approx-
RBV2
imated with approximation error ϵ by a shallow NN
K
(cid:88)
f (x)= v ϱ((w(i))⊤x−b )+c⊤x+c , for x∈Rd.
K i i 0
i=1
Proof. We assume first that ∥f∥ = 1. By Remark 3.8, we know that f has an integral
RBV2
representation of the form
(cid:90)
f(x)= ϱ(w⊤x−b)dµ(w,b)+c⊤x+c , for all x∈B1,
(cid:101) (cid:101)0 d
Sd−1×[−1,1]
where ∥c∥ ≤4 and |c |≤1 and µ∈M(Sd−1×[−1,1]) such that ∥µ∥ =RTV2(f).
(cid:101) ∞ (cid:101)0 Sd−1×[−1,1]
15If RTV2(f) = 0 the result holds, since f(x) = cTx in this case. If RTV2(f) ̸= 0, we define
(cid:101)
fˆ:=f/RTV2(f) and choose ϵ>0.
According to Theorem 3.15, the function
1 (cid:90)
x(cid:55)→ ϱ(w⊤x−b)dµ(w,b)
RTV2(f)
Sd−1×[−1,1]
can be approximated by the realization of a NN
K
(cid:88)
f(cid:101)K(x)= v (cid:101)iϱ((w (cid:101)(i))⊤x−(cid:101)b i)+(w (cid:101)(0))⊤x+(cid:101)b 0, for x∈Rd
i=1
with approximation error ϵ/RTV2(f) and where K ≤ C(d)RTV2(f)2−6/(d+3)ϵ−2+6/(d+3) for a
constant C = C(d) > 0 depending on the dimension d. This holds because the measure of the
integrand ν =µ/RTV2(f) fulfills that ∥ν∥ =1. Moreover, for all i=1,2,...,K,
Sd−1×[−1,1]
|v (cid:101)i|,|(cid:101)b i|,|(cid:101)b 0|,∥w (cid:101)(i)∥ ∞,∥w (cid:101)(0)∥
∞
≤1.
Let us define
f(cid:98)K :=f(cid:101)K +a⊤x+a 0.
where a=c/RTV2(f), a =c /RTV2(f). This leads us to the fact that
(cid:101) 0 (cid:101)0
(cid:12) (cid:12)
(cid:12)(cid:90) (cid:12)
|f(cid:98)(x)−f(cid:98)K(x)|=(cid:12)
(cid:12)
ϱ(w⊤x−b)dν(w,b)−f(cid:101)K(x)(cid:12) (cid:12)<ϵ/RTV2(f)
(cid:12) Sd−1×[−1,1] (cid:12)
for all x∈Bd. Finally, if we set
1
K
(cid:88)
f K(x)=RTV2(f)f(cid:98)(x)= v iϱ((w(i))⊤x−b i)+(w(0)+ (cid:101)c)⊤x+(b 0+ (cid:101)c 0),
i=1
where w(0) = RTV2(f)w (cid:101)(0), b
0
= RTV2(f)(cid:101)b
0
and for all i = 1,2,...,K, w(i) = RTV2(f)w (cid:101)(i),
v
i
=RTV2(f)v
(cid:101)i
and b
i
=RTV2(f)(cid:101)b i, it holds that
|f(x)−f K(x)|=|RTV2(f)f(cid:98)(x)−RTV2(f)f(cid:98)K(x)|=RTV2(f)|f(cid:98)(x)−f(cid:98)K(x)|<ϵ.
We denote c=w(0)+c and c =b +c and observe that |c |≤|b |+|c |≤1+∥f∥ ≤2. As
(cid:101) 0 0 (cid:101)0 0 0 (cid:101)0 RBV2
∥c∥ ≤4,weconcludethat∥c∥ ≤5. IncombinationwiththeboundsfortheweightsinTheorem
(cid:101) ∞ ∞
3.15, we obtain
|v |,|b |,|c |,∥c∥ ,∥w(i)∥≤5.
i i (cid:101)0 (cid:101) 2
For the general case where ∥f∥ ̸=1, we define
RBV2
f
fˆ:= ,
∥f∥
RBV2
if ∥f∥ ̸= 0 and apply the previous argument to conclude that for every ϵ > 0 there is a NN
RBV2
suchthat∥f(cid:98)−f(cid:98)K∥<ϵ/∥f∥
RBV2
whereK ≤C(d)∥f∥ R2− B6 V/( 2d+3)ϵ−2+6/(d+3)withallweightsbounded
by 5. If we denote f
K
= ∥f∥ RBV2f(cid:98)K, we observe that ∥f −f K∥
∞
< ϵ. The boundedness of the
weights follows immediately. If ∥f∥ =0, f =0 and the result holds trivially.
RBV2
16We now have all the results to prove Proposition 3.9. Essentially, we only need to express the
approximation accuracy ϵ in terms of the number of neurons of a neural network.
Proof of Proposition 3.9. For N ∈N, we set
(cid:16) (cid:17)(d+3)/(2d)
ϵ= C(d)∥f∥2−6/(d+3)/N .
RBV2
Then, according to Corollary 3.16, there is {w(1),...,w(r)} ⊂ [−1,1]d, {v ,v ,...,v } ⊂ [−1,1]
1 2 r
and {b ,b ,...,b }⊂[−1,1] such that if we set
1 2 r
r
(cid:88)
f (x)= v ϱ((w(i))⊤z−b )+cTx+c ,
r i i 0
i=1
it holds that
(cid:12) (cid:12)
(cid:12)(cid:90) (cid:88)r (cid:12)
|f(x)−f (x)|=(cid:12) ϱ(w⊤x−b)dµ(w,b)− v ϱ((w(i))⊤x−b )(cid:12)<ϵ,
K (cid:12) i i (cid:12)
(cid:12) Sd (cid:12)
i=1
where r ≤C(d)∥f∥2−6/(d+3)ϵ−2+6/(d+3) =N for all x∈Bd. We set w(r+1) =···=w(N) =0∈Rd,
RBV2 1
v =···=v =0∈R and b =···=b =0∈R and obtain
r+1 N r+1 N
(cid:12) (cid:12)
(cid:12)(cid:90) (cid:88)N (cid:12)
(cid:12) ϱ(w⊤x−b)dµ(w,b)− v ϱ((w(i))⊤x−b )(cid:12)<ϵ
(cid:12) i i (cid:12)
(cid:12) Sd (cid:12)
i=1
for all x∈Bd. Then, if
1
r
(cid:88)
f (x)= v ϱ((w(i))⊤x−b )+cTx+c ,
K i i 0
i=1
the result follows. Finally, notice that c⊤x=ϱ(cTx)−ϱ(−cTx). Hence, f can be expressed as
N
N+2
(cid:88)
f (x)= v ϱ((w(i))⊤x−b )+c ,
N i i 0
i=1
and after counting the number of weights we conclude that
f ∈NN(d,2,N +2,(2+d)(N +2)+1,5∥f∥ ).
N RBV2
4 Upper bounds on learning horizon functions
In this section, we achieve two main goals: we show that there is a NN that approximates horizon
functionsassociatedtoRBV2functionsandwepresentupperboundsforthecorrespondinglearning
problem. Letusformalizethenotionofhorizonfunctionsassociatedtoanarbitrarysetoffunctions
B. Thesearesetsofbinaryfunctionssuchthatthediscontinuityortheboundarybetweentheclasses
can be parameterized as a regular function in all but one coordinate.
17Definition 4.1. Let d∈N, d≥2 and assume that B ⊂C(Bd−1,R). We define the set of horizon
1
functions associated to B by
H :={fi =1 : h∈B,i∈[d]},
B h xi≤h(x[i])
wherex[i] :=(x ,...,x ,x ,...,x )fori∈[d]. Afunctionfi ∈H iscalled ahorizonfunction
1 i−1 i+1 d h B
with decision boundary described by h∈B.
In the sequel, we will analyze horizon functions with RBV2 functions as the decision boundary,
i.e., we choose B =RBV2(Bd−1). Based on Proposition 3.9, we now produce an approximation of
1
horizonfunctionswithRBV2 decisionboundarybyNNs. Wewillcomputetheerrorofapproximat-
ingHorizonfunctionsviathe0-1loss. Tothisend, weneedtospecifyanunderlyingmeasure. This
wouldtypicallybeauniform/Lebesguemeasure,butattheveryleast,itneedstobetube-compatible
(see [8, Section 6]).
Definition 4.2. Let µ be a finite Borel measure on Rd. We say that µ is tube compatible with
parameters α ∈ (0,1] and C > 0 if for each measurable function f: Rd−1 → R, each i ∈ [d] and
each ϵ∈(0,1], we have
µ(Ti )≤Cϵα where Ti :={x∈Rd: |x −f(x[i])|≤ϵ}, (4.1)
f,ϵ f,ϵ i
The sets Ti are called tubes of width ϵ (associated to f).
f,ϵ
We now construct a NN that approximates horizon functions associated to RBV2 functions.
The proof is based on the first two steps of the proof of [8, Theorem 3.7], but replacing Barron-
by RBV2 functions. [8, Theorem 3.7] obtains an approximation rate of O(N−α/2) for N →∞, for
Horizon functions associated with Barron functions, which is slightly slower than that for horizon
functions with RBV2 decision boundaries.
Theorem 4.3. Let d ∈ N , N ∈ N, q,C > 0, and α ∈ (0,1]. Further let, h ∈ H , where
≥2 B
B :={f ∈RBV2(Bd−1): f(0)=0, ∥f∥ ≤q}.
1 RBV2(Bd−1)
1
There exists a NN I with two hidden layers such that for each tube compatible measure µ with
N
parameters α,C, we have
µ({x∈B 1d :h(x)̸=I N(x)})≲
d
CqαN−αd 2+ d3 . (4.2)
Moreover, 0≤I (x)≤1 for all x∈Rd and the architecture of I is given by
N N
(cid:0) (cid:1)
A= d, N +2, 2,1 .
Thus, I has at most d+N +5 neurons and at most (d+3)N +2d+11 non-zero weights. The
N √
weights (and biases) of I
N
are bounded in magnitude by max{1,⌈ 2qNd 2+ d3⌉}.
Proof. Since h∈H , there exists f ∈B such that h(x)=1 . We have by Proposition 3.9,
B xi≤f(x[i])
that for all N ∈N, there exist w(k) ∈Rd, b ,v ∈R for k =1,2,...,N, c∈Rd and c ∈R with
k k 0
|v |,|b |,|c |,∥c∥,∥w(k)∥≤5∥f∥ ,
k k 0 RBV2
such that for
N
(cid:88)
f (x)= v ϱ((w(k))⊤x−b )+c⊤x+c ,
N k k 0
k=1
18it holds that
∥f −f N∥
L∞(Bd−1)
≲
d
∥f∥ RBV2N−d 2+ d3 . (4.3)
1
We define for 1≥δ >0 the one layer NN
1
H (z):= ·(ϱ(z)−ϱ(z−δ)) for z ∈R
δ δ
and for x∈Rd
(cid:16) (cid:17)
hδ (x):=H (f (x[i])−x )=H f (x[i])−ϱ(x )+ϱ(−x ) ,
N δ N i δ N i i
h N(x):=1 R+(f N(x[i])−x i).
Note that hδ is a NN with two hidden layers, architecture
N
(cid:0) (cid:1)
A= d, N +2, 2,1 ,
and all weights bounded by max{1/δ,5∥f∥ RBV2}. We choose δ := N−d 2+ d3 and proceed to prove
(4.2) for I =fδ. Towards this estimate, we define
N N
S :={x∈Bd: h(x)=1} and T :={x∈Bd: h =1}.
1 1 N
We have that
(cid:8) x∈Bd :h(x)̸=hδ (x)(cid:9) ⊂(cid:8) x∈Bd :0<hδ (x)<1(cid:9) ∪(cid:8) x∈Bd :hδ (x)=1,h(x)=0(cid:9)
1 N 1 N 1 N
∪(cid:8) x∈Bd :hδ (x)=0,h(x)=1(cid:9)
1 N
=(cid:8) x∈Bd :0<hδ (x)<1(cid:9) ∪(cid:8) x∈Bd :h (x)=1,h(x)=0(cid:9)
1 N 1 N
∪(cid:8) x∈Bd :h (x)=0,h(x)=1(cid:9)
1 N
=:(cid:8) x∈Bd :0<hδ (x)<1(cid:9) ∪S∆T.
1 N
We observe that by (4.3), for γ ∼ RTV2(f)
d
S∆T
(cid:110) (cid:111) (cid:110) (cid:111)
= x∈Bd: f(x[i])<x ≤f (x[i]) ∪ x∈Bd: f (x[i])<x ≤f(x[i])
1 i N 1 N i
(cid:110) (cid:111) (cid:110) (cid:111)
⊂ x∈B 1d: 0≤f N(x[i])−x
i
<γN−d 2+ d3 ∪ x∈B 1d: −γN−d 2+ d3 ≤f N(x[i])−x
i
<0
(cid:110) (cid:111)
⊂ x∈B 1d: |f N(x[i])−x i|≤γN−d 2+ d3 .
In addition,
(cid:8) x∈Bd :0<hδ (x)<1(cid:9) ⊂{x∈Bd :|f (x[i])−x |≤δ}.
1 N 1 N i
We conclude by the α,C tube-compatibility of µ that
µ((cid:8) x∈Bd :h(x)̸=hδ (x)(cid:9) )
1 N
≤µ(cid:0)(cid:8) x∈Bd :0<hδ (x)<1(cid:9)(cid:1) +µ(S∆T)
1 N
(cid:16) (cid:17) (cid:16) (cid:17)
≤µ {x∈B 1d :|f N(x[i])−x i|≤δ} +µ {x∈B 1d: |f N(x[i])−x i|≤γN−d 2+ d3 }
(cid:16) (cid:17)
≤C·
δα+γαN−αd 2+ d3
.
19Due to the choice of δ =N−d 2+ d3, this completes the proof.
4.1 Estimation bounds
Now, we state our result regarding upper bounds for the estimation problem. We begin by intro-
ducing two concepts: the Hinge loss for a target concept and the empirical risk minimizer.
Although Theorem 4.3, showed a bound for the 0-1 loss, this is not a useful error measure in
practice,duetoitslackofcontinuity. Instead,weformulateourlearningboundswithrespecttothe
Hinge loss, which is significantly more practical in applications. As we consider horizon functions
h:[0,1]d →{0,1} as our target classifiers and the range is not {−1,1} as would be typical for the
Hinge loss, we present a slightly different definition of Hinge function.
Definition 4.4. Define ϕ: R → R, ϕ(x) := max{0,1 − x}. Let d ∈ N and let µ be a Borel
measure on Bd. For a (measurable) target concept h∗: Bd → {0,1}, we define the Hinge risk of a
1 1
(measurable) function h: Bd →[0,1] as
1
E (f):=E (cid:104) ϕ(cid:16)(cid:0) 2h∗(X)−1(cid:1) ·(cid:0) 2h(X)−1(cid:1)(cid:17)(cid:105) .
ϕ,µ,h∗ X∼µ
Let Λ = Bd×[0,1]. For a sample S = (x ,y )m ∈ Λm, m ∈ N, we define the empirical ϕ-risk of
1 i i i=1
h: Bd →[0,1] as
1
m
1 (cid:88) (cid:0)(cid:0) (cid:1) (cid:0) (cid:1)(cid:1)
E(cid:98)ϕ,S(h):=
m
ϕ 2y i−1 · 2h(x i)−1 .
i=1
Finally, for a sample S = (x ,y )m ∈ Λm and a set H ⊂ {h: Bd → [0,1]}, we call h ∈ H an
i i i=1 1 S
empirical ϕ-risk minimizer, if
E(cid:98)ϕ,S(h S)=minE(cid:98)ϕ,S(h). (4.4)
h∈H
Finally,wepresentupperboundsforestimatinghorizonfunctionsassociatedtoRBV2functions.
The proof of this theorem is similar to that of [8, Theorem 5.7].
Theorem 4.5. Let κ > 0, d ∈ N . There is a constant τ ≥ 1 depending on d only such that the
≥4
following holds: Let h ∈ H , where B := {f ∈ RBV2(Bd): f(0) = 0 ∥f∥ ≤ q} and let λ be
B 1 RBV2
a probability measure on Bd. For each m∈N, let
1
N(cid:101)(m):=(cid:6) τm2d/(3d+3)(cid:7)
,
(cid:6) (cid:7)
N(m):= N(cid:101)(m)+d+3 ,
(cid:6) (cid:7)
W(m):= (d+4)N(cid:101)(m)+3 ,
(cid:108) √ (cid:109)
B(m):= max{1, 2qN(cid:101)(m)d 2+ d3 } .
For each m∈N, let S be a training sample of size m; that is, S =(cid:0) X ,h(X )(cid:1)m with X i ∼id λ for
i i i=1 i
(cid:0) (cid:1)
i ∈ {1,...,m}. Furthermore, let Φ ∈ NN d,2,N(m),W(m),B(m) be an empirical Hinge-
m,S ∗
loss minimizer; that is,
(cid:0) (cid:1)
E(cid:98)ϕ,S Φ
m,S
= min E(cid:98)ϕ,S(f).
f∈NN∗(d,N(m),W(m),B(m))
20Then, with H :=1 , we have
[1/2,∞)
E S(cid:104) E X∼λ(cid:0)(cid:12) (cid:12)H(Φ m,S(X))−h(X)(cid:12) (cid:12)2(cid:1)(cid:105) =E S(cid:2)P X∼λ(cid:0) H(Φ m,S(X))̸=h(X)(cid:1)(cid:3)≲m− 3d d+ +3 3+κ, (4.5)
where the implied constant only depends on d,κ,τ.
Proof. The proof is analogous to [29, Theorem 5.7] with two differences, first, we choose the values
2d
a
m
=m− 3d d+ +3 3, δ m∗ =m− 3d d+ +3 3+κ, and ι=
3d+3
+κ
differentfromthosein[29,Theorem5.7]. Second,toconstructappropriateNNapproximations,we
use Theorem 4.3 instead of [29, Theorem 5.3]. Note that, [29, Theorem 5.7] is stated for functions
defined on [0,1]d. However, this is only due to the fact that it is based on [18, Theorem 5] which,
whilestatedforfunctionson[0,1]d holdsforeverycompactsubsetofRd asstatedatthebeginning
ofSection2of[18]. Therefore,[29,Theorem5.7]holdsonBd equippedwithaprobabilitymeasure,
1
as well.
5 Numerical Experiments
Inthissection,weillustratehowdifferentnormalizationsoftheboundaryfunctionaffectthelearning
problem of binary classification. We assume that our target classifier outputs a binary label for
eachpointx∈Rd withd=2,3and4. Here,thedecisionboundaryisasufficientlysmoothfunction
f :B (0)→R.
d−1
5.1 Experiment set-up
Our objective is to compare the performance of empirical risk minimization over NNs for classifi-
cation problems with decision boundaries normalized with respect to various norms. To this end,
we invoke the following set-up:
• We first describe a base set of functions that will later be normalized to form the boundary
functions to be learned. For the case d=2, we consider the set of functions
B :={B (0)∋x(cid:55)→sin(2πkx):k =1+s/24, s=0,1,...,24, x∈[−1,1]},
1
where for each f ∈B we define |f|:=|k|. Now, for d=3, the functions to be learned belong
to the set
B :={B (0)∋(x,y)(cid:55)→sin(2πkx)sin(2πly):k,l=1+s/5,s=0,1,...,5 x∈[−1,1]},
2
and for f ∈B we denote |f|=|k|+|l|. Finally, for d=4, we learn the functions
B :={B (0)∋(x,y,z)(cid:55)→sin(2πkx)sin(2πly)sin(2πjz):k,l,j =1,4/3,7/3,2 x∈[−1,1]}.
3
and again, for f ∈B we employ the notation |f|=|k|+|l|+|j|.
21• We consider the following norms: first, the uniform norm
∥f∥ = sup |f(x)|,
∞
x∈Bd−1
1
second, the L1-norm
(cid:90)
∥f∥ = |f(x)|dx,
L1
Bd−1
1
third, the C1-norm
∥f∥ = sup ∥fk∥ ,
C1 ∞
|k|≤1
where k ∈Nd, fourth, the Barron norm,
0
(cid:90) ∞
∥f∥
Barron
= |f(cid:98)(w)||w|dw
−∞
as defined in [3, Equation 3], where f(cid:98)is the Fourier transform of the function f, and the
Barron norm. As the function f is defined on a bounded domain, its Fourier transform is not
well defined. To solve this issue and leverage the properties of the functions in the sets B to
be learned, we replace the Fourier transform with the appropriate term
∥f∥ ≈2π|f|.
Barron
Finally,weusetheRBV2 norm. ForthecomputationoftheRBV2 norm,weusedifferentap-
proaches. Forthecased=2,weuse[27,Remark4]. ItisstatedtherethattheRBV2([−1,1])
space is equivalent to the set of functions of second-order bounded variation
BV2([−1,1])={f :[−1,1]→R:TV2(f)<∞},
where
(cid:90) 1
TV2(f)= |D2f(x)|dx
−1
is the second-order total variation of f :[−1,1]→R.
In [27], it is proved that RTV2(f)=TV2(f) and the second total-variation seminorm of f is
well-defined for all smooth functions f. Hence, for each f with f(0)=0, we can compute its
RBV2 norm by
∥f∥ =TV2(f)+|f(1)|.
RBV2
For the case d = 3 and d = 4, we use a different approach to compute the integral term in
Definition 3.5. To this end, we generate a set of 20 vectors w ∈ Sd−1 and 20 equidistant
i
parameters t ∈ [−1,1]. Next, we compute the Radon transform at these points by using
i
randomly generated points drawn according to the uniform distribution in the hyperplane
{z : wTz = t}. Then, by using a method of finite differences, we compute the corresponding
derivatives, and we continue to compute the total variation of the function. To compute the
norm, we have to evaluate the function f at the origin and at the elements of the canonical
basis as in Definition 3.5.
22• To produce the functions to be learned, we normalize the elements of the corresponding base
setB withrespecttotheuniform,L1,C1,Barron,andRBV2 norms,whichyieldsfivesetsof
functions for each dimension d. Next, we compare perform empirical risk minimization over
appropriatesetsofNNstolearntheclassifiersassociatedtothesenormalizedfunctionclasses.
• The sample set consists of m points (x ,y )m . The points x are randomly generated points
i i i=1 i
drawn with respect to uniform distribution on the set B (0)×[−2,2]. To determine the
d−1
value of each y , we evaluate the function y =1 at each x . We split this sample set
i xd≤f(x[d]) i
into two subsets: the training and the test sets. The first one is randomly chosen with 80%
of all samples and is used to find a NN that minimizes the empirical error.
• We use a three-layer NN with ReLU activation function, and the number of neurons is deter-
mined according to Theorem 4.5 with τ =1. We use the Hinge function as our loss function
and the Adam optimizer.
• We compare the performance of the selected NN on the test sets. This error is measured in
the mean squared sense.
Werecordthegeneralizationerrorforeachfunctionf ∈Bandnumberofsamplesmandpresent
the results in Figure 5.1. In the x-axis, we register the number of samples, and in the y-axis, the
average test error. In each figure, we plot the error for the five different norms introduced above.
Sup
101 L1
101 101 B Ra Br Vr 2on
C1
102 102 102
Sup Sup
L1 L1
103 B Ra Br Vr 2on 103 B Ra Br Vr 2on 103
C1 C1
102 103 104 105 103 104 105 103 104 105
Figure 1: Plot of the mean relative test error for the sets of case d=2 (left), d=3 (middle) and
d = 4 (right). The number of samples varies along the horizontal axis and the mean relative test
error is shown on the vertical axis.
5.2 Evaluation and interpretations of the results
There are a few points we would like to highlight that can be observed from the figures.
1. In Figure 5.1, we can observe that when the classification function boundary is normalized
with respect to L1 or L∞ normalized, then the test error is usually the highest. Interestingly,
in the case of d = 2, the test error behaves more favorably compared to higher dimensions.
For dimensions larger than two, increasing the number of samples results in almost no im-
provement in classification performance. This finding is consistent with the lower bounds
on learnability discussed by [29], which show that classification functions with boundaries in
normed spaces characterized by large packing numbers are difficult to learn effectively.
2. Forthecased=2andd=3in5.1,weobservethatlearningclassificationfunctionsnormalized
usingeithertheC1 normortheBarronnormexhibitsimilarperformance. However,whenthe
23dimension increases to d=4, the Barron norm yields better test error results, which suggests
that its advantages become more evident in higher dimensions.
3. In all cases, the classification functions with RBV2 normalized boundary shows the best
performance among all the tested norms. Moreover, the test errors show essentially the same
decay for all dimensions tested. These examples support the idea that functions with RBV2
boundary are easier to learn than functions with lower regularity and are in line with the
theoretical results of Theorem 4.5.
4. We can observe that the performance of RBV2 boundaries surpasses that of Barron spaces.
This is consistent with our theoretical results from Theorem 4.5 and aligns with the findings
in [29], further supporting the fact that higher regularity in function boundaries leads to a
better learning outcome.
Acknowledgement
P. P. was supported by the Austrian Science Fund (FWF) [P37010]. T. L. was supported by the
AXA Research Fund.
References
[1] F. Bach. Breaking the curse of dimensionality with convex neural networks. The Journal of Machine
Learning Research, 18(1):629–681, 2017.
[2] A.R.Barron. Neuralnetapproximation. InProc.7thYaleworkshoponadaptiveandlearningsystems,
volume 1, pages 69–72, 1992.
[3] A. R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE
Transactions on Information theory, 39(3):930–945, 1993.
[4] H. Bauer. Measure and integration theory, volume 26. Walter de Gruyter, 2001.
[5] R.Bellman. Onthetheoryofdynamicprogramming. ProceedingsofthenationalAcademyofSciences,
38(8):716–719, 1952.
[6] E.J.Cand`esandD.L.Donoho. Newtightframesofcurveletsandoptimalrepresentationsofobjects
withpiecewiseC2 singularities. CommunicationsonPureandAppliedMathematics: AJournalIssued
by the Courant Institute of Mathematical Sciences, 57(2):219–266, 2004.
[7] E. J. Cand`es, D. L. Donoho, et al. Curvelets: A surprisingly effective nonadaptive representation for
objects with edges. Department of Statistics, Stanford University USA, 1999.
[8] A. Caragea, P. Petersen, and F. Voigtlaender. Neural network approximation and estimation of clas-
sifiers with classification boundary in a Barron class. arXiv preprint arXiv:2011.09363, 2020.
[9] V. Chandrasekaran, M. Wakin, D. Baron, and R. G. Baraniuk. Compressing piecewise smooth multi-
dimensional functions using surflets: Rate-distortion analysis. Rice University ECE Technical Report,
2004.
[10] A.CloningerandT.Klock.Adeepnetworkconstructionthatadaptstointrinsicdimensionalitybeyond
the domain. Neural Networks, 141:404–419, 2021.
[11] I. Goodfellow, Y. Bengio, and A. Courville. Deep learning. MIT press, 2016.
[12] K. Guo and D. Labate. Optimally sparse multidimensional representation using shearlets. SIAM
journal on mathematical analysis, 39(1):298–318, 2007.
24[13] J. Han, A. Jentzen, and W. Ee. Solving high-dimensional partial differential equations using deep
learning. Proceedings of the National Academy of Sciences, 115(34):8505–8510, 2018.
[14] M. Hutzenthaler, A. Jentzen, T. Kruse, T. Anh Nguyen, and P. von Wurstemberger. Overcoming
the curse of dimensionality in the numerical approximation of semilinear parabolic partial differential
equations. Proceedings of the Royal Society A, 476(2244):20190630, 2020.
[15] M. Imaizumi and K. Fukumizu. Deep neural networks learn non-smooth functions effectively. In The
22nd international conference on artificial intelligence and statistics, pages 869–878. PMLR, 2019.
[16] M. Imaizumi and K. Fukumizu. Advantage of deep neural networks for estimating functions with
singularity on hypersurfaces. The Journal of Machine Learning Research, 23(1):4772–4825, 2022.
[17] A.Jentzen,D.Salimova,andT.Welti. Aproofthatdeepartificialneuralnetworksovercomethecurse
of dimensionality in the numerical approximation of Kolmogorov partial differential equations with
constant diffusion and nonlinear drift coefficients. arXiv preprint arXiv:1809.07321, 2018.
[18] Y. Kim, I. Ohn, and D. Kim. Fast convergence rates of deep neural networks for classification. arXiv
preprint arXiv:1812.03599, 2018.
[19] G. Kutyniok and W. Lim. Compactly supported shearlets are optimally sparse. Journal of Approxi-
mation Theory, 163(11):1564–1589, 2011.
[20] E.LePennecandS.Mallat.Sparsegeometricimagerepresentationswithbandelets.IEEEtransactions
on image processing, 14(4):423–438, 2005.
[21] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436–444, 2015.
[22] C.Ma,S.Wojtowytsch,L.Wu,etal. Towardsamathematicalunderstandingofneuralnetwork-based
machine learning: what we know and what we don’t. arXiv preprint arXiv:2009.10713, 2020.
[23] J.Matouˇsek. Improvedupperboundsforapproximationbyzonotopes. Acta Mathematica,177(1):55–
73, 1996.
[24] H. Montanelli, H. Yang, and Q. Du. Deep ReLU networks overcome the curse of dimensionality for
bandlimited functions. arXiv preprint arXiv:1903.00735, 2019.
[25] R.NakadaandM.Imaizumi. Adaptiveapproximationandgeneralizationofdeepneuralnetworkwith
intrinsic dimensionality. The Journal of Machine Learning Research, 21(1):7018–7055, 2020.
[26] R. Novak and H. Wo´zniakowski. Approximation of infinitely differentiable multivariate functions is
intractable. Journal of Complexity, 25(4):398–404, 2009.
[27] R. Parhi and R. D. Nowak. Near-minimax optimal estimation with shallow ReLU neural networks.
IEEE Transactions on Information Theory, 2022.
[28] R. Parhi and R. D. Nowak. What kinds of functions do deep neural networks learn? insights from
variational spline theory. SIAM Journal on Mathematics of Data Science, 4(2):464–489, 2022.
[29] P. Petersen and F. Voigtlaender. Optimal learning of high-dimensional classification problems using
deep neural networks. arXiv preprint arXiv:2112.12555, 2021.
[30] R. Petersen and F. Voigtlaender. Optimal approximation of piecewise smooth functions using deep
ReLU neural networks. Neural Networks, 108:296–330, 2018.
[31] A. F. L. Pineda and P. C. Petersen. Deep neural networks can stably solve high-dimensional, noisy,
non-linear inverse problems. Analysis and Applications, 21(01):49–91, 2023.
[32] T. Poggio, H. Mhaskar, L. Rosasco, B. Miranda, and Q. Liao. Why and when can deep–but not
shallow–networksavoidthecurseofdimensionality. CenterforBrains, MindsandMachines(CBMM)
Memo No. 58, arXiv preprint arXiv, 1611, 2016.
[33] U. Shaham, A. Cloninger, and R. R. Coifman. Provable approximation properties for deep neural
networks. Applied and Computational Harmonic Analysis, 44(3):537–557, 2018.
25[34] J. W. Siegel and J. Xu. Characterization of the variation spaces corresponding to shallow neural
networks. arXiv preprint arXiv:2106.15002, 2021.
[35] F.VoigtlaenderandA.Pein. Analysissparsityversussynthesissparsityforα-shearlets. arXivpreprint
arXiv:1702.03559, 2017.
[36] S.WojtowytschandW.E. Aprioriestimatesforclassificationproblemsusingneuralnetworks. arXiv
preprint arXiv:2009.13500, 2020.
[37] S. Wojtowytsch et al. Representation formulas and pointwise properties for Barron functions. arXiv
preprint arXiv:2006.05982, 2020.
26