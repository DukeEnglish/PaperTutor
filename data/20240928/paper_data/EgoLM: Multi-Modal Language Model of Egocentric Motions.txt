TechnicalReport
EGOLM: MULTI-MODAL LANGUAGE MODEL OF EGO-
CENTRIC MOTIONS
FangzhouHong1,2, VladimirGuzov1,3, HyoJinKim1, YutingYe1
RichardNewcombe1, ZiweiLiu2(cid:0), LingniMa1(cid:0)
1MetaRealityLabsResearch, 2S-Lab,NanyangTechnologicalUniversity
3UniversityofTuebingen
a b Input#1:Sparse Motion Sensors Task#1:Motion Tracking
EgoLM Input#2:Egocentric Videos
Task#2:Motion Understanding
“The person b
“The person is standing a turns around
straight as she puts the piece then walks out of
of clothing on the hanger.” the bedroom.”
Figure1: WeproposeEgoLM,amulti-modallanguagemodelthatunifiesegocentricmotiontrack-
ingandunderstandingfromwearablesensordata,e.g.,sparsemotionsensorsandegocentricvideos.
ABSTRACT
Astheprevalenceofwearabledevices, learningegocentricmotionsbecomeses-
sential to develop contextual AI. In this work, we present EgoLM, a versatile
framework that tracks and understands egocentric motions from multi-modal
inputs, e.g., egocentric videos and motion sensors. EgoLM exploits rich con-
texts for the disambiguation of egomotion tracking and understanding, which
are ill-posed under single modality conditions. To facilitate the versatile and
multi-modal framework, our key insight is to model the joint distribution of
egocentric motions and natural languages using large language models (LLM).
Multi-modal sensor inputs are encoded and projected to the joint latent space
of language models, and used to prompt motion generation or text generation
for egomotion tracking or understanding, respectively. Extensive experiments
on large-scale multi-modal human motion dataset validate the effectiveness of
EgoLM as a generalist model for universal egocentric learning. Project Page:
https://hongfz16.github.io/projects/EgoLM.
1 INTRODUCTION
Withtherecentexplosiveadvancementoflargelanguagemodels,theirvaluesasintelligentagents
havebeenthoroughlystudied(Radfordetal.,2019;Brownetal.,2020;Achiametal.,2023;Tou-
vron et al., 2023a;b). To better play the role of everyday smart assistant, the contextualization of
AI is proposed and studied (Vercauteren et al., 2019; Deepika et al., 2020). Agents are expected
to interact with users in a context-aware style, through multi-modal sensors on wearable devices,
e.g.,smartwatches,smartglasses(Somasundarametal.,2023). Humanmotionsplayanimportant
role in the user-agent interaction (Plizzari et al., 2023), which requires egocentric human motion
learning(Lietal.,2015).
In this work, we propose a versatile framework EgoLM that approaches human motions learning
fromegocentricperspective.Specifically,EgoLMunifiestwoaspectsofegocentricmotionlearning,
i.e., tracking andunderstanding. a) Egocentric motion trackingaims to recover full-body motions
1
4202
peS
62
]VC.sc[
1v72181.9042:viXraTechnicalReport
fromsparsemotionsensors,e.g.,three-points(headandbothwrists)6-DoFposes(Jiangetal.,2022;
Castillo et al., 2023; Du et al., 2023; Jiang et al., 2023) or one-point (only head) 6-DoF poses (Li
et al., 2023). b) Egocentric motion understanding aims to recognize or describe human motions
from wearable sensors, e.g., egocentric videos (Damen et al., 2021; 2022; 2018; Nagarajan et al.,
2024; Xue et al., 2023; Escobar et al., 2022; Grauman et al., 2022; Rodin et al., 2021; Yonetani
etal.,2016;DelMolinoetal.,2016;Chenetal.,2023). Bothtasksarehighlychallengingduetothe
incomplete observation from egocentric perspectives. To this end, we propose to approach the
challengesinanunifiedwaybyincorporatingmultiplemodalitiesandmulti-tasktraining,which
areelaboratedbelow.
Egocentricmotiontrackingfromsparsesensorsisanill-posedproblem. Three-points(Jiangetal.,
2022) and one-point inputs (Li et al., 2023) miss information of the lower body parts and even
hand positions, making it a one-to-many mapping problem. In order to disambiguate the tracking
ofunobservedbodyparts,weexploretheenvironmentcontextsbyegocentricvideoscapturedfrom
head-mounted cameras. Although other body parts are not always visible from egocentric videos,
thesemanticsoftheenvironmentprovidesvaluablecluestodisambiguatefullbodymotions.
For egocentric motion understanding, the common input setting is the egocentric video (Jia et al.,
2022). However, egocentric videos lacks the accurate information of full-body motion, for their
restrictedviewingangles. Forbetterunderstandingofhumanmotion,sparsemotionsensordatais
valuableintermsofprovidingaccuratebodypartpositions(Tanetal.,2024). Therefore,weunify
the input conditions as egocentric videos and sparse sensors for both tracking and understanding.
Further unifying the training of both tasks can also be beneficial, especially for the understanding
part. Thesupervisionsignalsoffull-bodymotionsfrommotiontrackingtrainingcancontributeto
motionunderstanding.
In summary, we aim at a multi-modal multi-tasking generative framework. As shown in Fig. 1,
EgoLMtakessparsemotionsensordata(three-pointsorone-point)andegocentricvideosasinputs.
Then motion and natural languages are generated for motion tracking and understanding, respec-
tively. Tofacilitatethisversatileframework,therearetwomainchallengesintheframeworkdesign,
which are large modality gaps and large task gaps. To that end, our key insight is to use a lan-
guagemodeltohandlethemulti-modalinputsandmulti-tasktraining.
UnlikerecentVLMs(Liuetal.,2023b;a),oursettingismorecomplexandchallenging,wherefour
modalities are involved, including sparse motion sensor data, egocentric videos, motion represen-
tations, and texts. These modalities provide different granularity of information. Human motions
andsparsemotionsensordataarelow-levelandcontiguousrepresentationswithphysicalmeanings.
Natural languages, on the other hand, are unstructured and discrete representations. To bridge the
gap, we adopt three strategies: a) Treat motions as languages. A motion VQ-VAE is trained to
tokenize motions, which can be generated autoregressively by a language model. b) Unify differ-
entinputstothelanguagemodelspace. Sparsesensordataandegocentricvideosareencodedand
projectedbylight-weighttemporalencoders. c)Useinstructiontuningformulti-taskjointtraining.
To validate the proposed framework, we perform extensive experiments on a large-scale motion
dataset, Nymeria (Ma et al., 2024). Compared with previous motion tracking and understanding
methods,ournewlyproposedmulti-modalsetupshowsitsadvantages. Ourcontributionsaresum-
marizedbelow.
1)Weproposeaversatilemulti-modalgenerativeframework,EgoLM,thatunifiesegocentricmotion
trackingandunderstandingtaskswithalanguagemodel.
2) A new egocentric motion tracking setup is proposed. We combine sparse sensor inputs with
egocentricvideostoprovidemorecontextsthatdisambiguatethisill-posedproblems.
3)Weproposeapracticalparadigmofmotionunderstandingbycombiningsparsesensordataand
egocentricvideos,whichprovidesmoreaccuratefull-bodymotionnarration.
4)Extensiveexperimentsandstudiesareperformedtoshowtheeffectivenessoftheproposedframe-
work. Oursetupachievesthebestperformancecomparedwiththestate-of-the-artmethods.
2TechnicalReport
2 RELATED WORK
MotionRegression. Largeamountsofeffortsaredevotedtodetectandtrack2Dor3Dkeypoints
fromhumanimagesandvideos(Toshev&Szegedy,2014;Martinezetal.,2017;Pavlloetal.,2019).
To incorporate more human structure prior, parametric human models, e.g., SMPL (Loper et al.,
2023), are used as the regression target (Bogo et al., 2016; Kanazawa et al., 2018). Other than
thecameras,wearablemotionsensorsarestraight-forwardintermsofmotioncapture(Pontonetal.,
2023;Mollynetal.,2023;Milefetal.,2023;Yietal.,2023;Jiangetal.,2023).Recentadvancements
inVR/ARdevicesandapplicationshavedevelopedanewsetupformotiontracking,i.e.,three-points
body tracking (Du et al., 2023; Jiang et al., 2022; Castillo et al., 2023). EgoEgo (Li et al., 2023)
proposestotrackmotionsfromonlyheadposes. Inthiswork,wealsotargetmotiontrackingfrom
sparsesensors. Thedifferenceisthatweproposetouseegocentricvideostodisambiguateill-posed
scenariosinthissetup.
MotionGeneration. Therehavebeenmanyeffortsingeneratingmotionsfromvariousconditions,
i.e., action labels (Petrovich et al., 2021; Guo et al., 2020), natural languages (Zhang et al., 2024;
Tevetetal.,2022;Punnakkaletal.,2021;Guoetal.,2022a;Zhangetal.,2023b;Guoetal.,2022b).
Recently, researchers take advantage of powerful LLMs to model the joint motion-language dis-
tribution for text-to-motion generation (Jiang et al., 2024; Zhang et al., 2023c; Zhou et al., 2023).
In this work, we also adopt the similar idea of modeling motion together with language models.
Asaby-product,wecanalsoperformtext-to-motiongeneration. Butourmainfocusisonmotion
trackingandunderstandingfrommulti-modalinputs.
Motion Understanding. There have been many different setups in motion understanding. From
the input side, human videos, either from third-person view (Soomro et al., 2012; Kuehne et al.,
2011; Tran et al., 2015; Wang et al., 2016; Yan et al., 2018) or first-person view (Damen et al.,
2021; 2022; 2018), are used for this task. From the output side, action recognition/classification
has been a classic task definition (Soomro et al., 2012; Damen et al., 2018). More recently, with
the development of language models, some researches also propose to use natural languages as
output(Jiaetal.,2022;Xuetal.,2024;Graumanetal.,2022;Xueetal.,2023;Chenetal.,2023).
In our work, from the input side, we propose to combine egocentric videos, which provide high-
level semantic information, with motion sensor inputs, which carries low-level motion clues, for
more holistic motion understanding. For the output, we use natural language responses for more
versatilityanddiversity.
LanguageModels. Languagemodelshavebeenahugesuccessinrecentyearswiththelarge-scale
pre-training (Radford et al., 2019; Brown et al., 2020) and alignment (cha, 2022; Achiam et al.,
2023). To take advantage of the powerful text generation ability, image (Liu et al., 2023b;a) or
video understanding (Zhang et al., 2023a) are defined as conditional text generation. LLaVA (Liu
etal.,2023b)proposestoencodeimageswithpowerfulpre-trainedvisionencoders(Radfordetal.,
2021)andinjectthefeaturestolanguagemodels. BytuningfromapowerfulLLM(Touvronetal.,
2023a), LLaVA achieves wonderful abilities of vision question answering. In this work, we also
adoptthesimilarideaofencodingandinjectingfeaturesofothermodalitiesinthelanguagemodel
andunifyingdifferenttaskswithinstructiontuning.
3 METHOD
TheoverviewofEgoLMisdemonstratedinFig.2. TherearethreestepsinEgoLMtraining. Inthe
firststep,wetrainamotionVQ-VAEasthemotiontokenizer(Sec.3.2). Thesecondstepismotion
pre-training for motion distribution learning (Sec. 3.3). The last step is multi-modal instruction
tuningtoguidethemodeltoperformmotiontrackingandunderstanding(Sec.3.4).
3.1 PRELIMINARIES
Language Model. Language models model the distribution of natural languages. Recent
breakthroughs in language models suggest the effectiveness of the transformer-based architec-
ture(Vaswanietal.,2017). Thelanguagemodelconsistsofthreeparts. Thefirstisalook-uptable
(LMembedding)thatstorestheembeddingsforeachtexttoken. Thesecondpartisthetransformer
3TechnicalReport
Instructions
<BOS>
Vision
Encoder
Encoder
Motion VQ-VAE Language Model EgoLM
<EOS> Motion Narrations
1) Motions Tokenization 2) Motion Pre-Training 3) Multi-Modal Instruction Tuning
Figure 2: Overview of EgoLM. Three steps are designed for the training of EgoLM, i.e., motion
tokenizertraining,motionpre-trainingandmulti-modalinstructiontuning.
backbonethattakestextembeddingsasinputs. Theoutputfeaturesaremappedtoprobabilitiesof
thenexttokensbythethirdpartofLMhead.
MotionRepresentation. Humanmotionsarerepresentedassequencesofposes,globaltranslations
and rotations defined on the root joint. Each frame of pose is represented by joint angles, defined
onakinematictree. Forbetterlearningofmotiondynamics,wealsoincludejointanglevelocityin
therepresentation. Toavoidthenormalizationofglobaltranslation,weusethetranslationvelocity
Vr ∈R3foreachframe,whichcanbeintegratedbacktoglobaltranslations. Toeasetheregression
t
difficulty of rotation angles, we use 6D rotation representations (Hempel et al., 2022) for the root
rotation Rr ∈ R6, root rotation velocity Rrv ∈ R6, joint angles Rj ∈ R22×6, and joint angle
t t t
velocity Rjv ∈ R22×6. Formally, we represent human motions with T frames as M = {P }T ,
t t t=1
whereP = [Vr;Rr;Rrv;Rj;Rjv] ∈ R279. Forwardkinematics(FK)togetherwithintegrationof
t t t t t t
rootvelocitycanbeusedtorecoverthejointpositionsJ =FK(M)∈R23×3.
3.2 MOTIONTOKENIZER
To treat the motion as a foreign language and train with language models, we first need a motion
tokenizer,whichcanberealizedbyVQ-VAE(Oordetal.,2017). ThemotionVQ-VAEconsistsof
afullyconvolutionalencoderE anddecoderD. Thefullyconvolutionaldesignenablesprocessing
motions with arbitrary lengths. The encoder embeds raw motion representation to latent features
fm =E(M),wherefm ∈RT/r×c,M ∈RT×279. risthedown-samplerate.
Then,codebooksarelearnedtoquantizethemotionlatentfeatures. Weusethreetechniquesinthe
quantizationprocess,whichare1)exponentialmovingaverage(EMA),2)codebookreset(Dhariwal
etal.,2020),3)productquantization(Jegouetal.,2010;Lucasetal.,2022).Thefirsttwotechniques
increasetheusagerateofcodebooks. Productquantizationincreasesthecodebookexpressiveness
by decomposing the latent space into a Cartesian product of sub-spaces with lower dimensions.
Specifically, the latent feature fm is split equally into N trucks {fm}N , which are quantized
n n=1
separatelybyN codebooks{Z }N . EachcodebookwithK entriesisdefinedasZ = {z }K ,
n n=1 n i i=1
wherez ∈Rc/N. Thequantizationprocessforfeaturefmatframetandtrunknisformulatedas
i tn
i =Q(fm)=arg min ∥fm−z ∥ . (1)
tn tn tn i 2
zi∈Zn
Theresultingindicesi areflattenedandusedasmotiontokensequencesW = {[(i )N ] }T/r,
tn n n=1 t t=1
whichhasthelengthofL = N ×(T/r). Afterquantization,weobtainthecorrespondingcode-
W
bookentryforthemotionlatentfeaturefˆm ={fˆm}T/r ={z }T/r. ItisinputintothedecoderD
t t=1 it t=1
todecoderawmotionrepresentationMˆ =D(fˆm).
ForthetrainingofVQ-VAE,twotypesoftraininglossesareused. Thefirstisthecommitmentloss
L =∥fm−fˆm∥ forthecodebooklearning. ThesecondismotionreconstructionlossL ,which
c 2 r
consistsofrawrepresentationlossL , jointpositionlossL , rotationvelocitylossL , whichare
m j v
definedas
L =λ L +λ L +λ L =λ ∥M −Mˆ∥ +λ ∥FK(M)−FK(Mˆ)∥ (2)
r m m j j v v m 1 j 1
+λ ∥Rrv −(Rr )−1Rr ∥ +λ ∥Rjv −(Rj )−1Rj ∥ . (3)
v 1:T−1 1:T−1 2:T 1 v 1:T−1 1:T−1 2:T 1
4TechnicalReport
“<s>Perform ... based on the given ... Input CLIP embeddings: <CLIP_Placeholder>. Input three-points: <TP_Placeholder>”
Text Tokenizer Text Tokenizer
... ...
1 27313 2729 373 278 10567 24492 29925 8297 10567 2211 29899
CLIP Encoder
TP Encoder
LM Embedding Linear Layer LM Embedding
Concatenate
Figure3:DetailsofMulti-ModalInstructionTuning.Differentmodalitiesareencodedseparately.
Theirfeaturesareconcatenatedintheorderoftheinstructiontemplateandinputintothetransformer
layersofthelanguagemodel.
WedefinethesmoothedL1lossas∥·∥ . Insummary,thetraininglossofthemotionVQ-VAEis
1
L =λ L +λ L ,whereλ aremanuallyadjustedweights.
vq c c r r ∗
3.3 MOTIONPRE-TRAINING
As discussed before, we build the motion learning framework on a pre-trained language model.
However,thepre-trainedlanguagemodelsonlymodelthedistributionofnaturallanguages. There-
fore,toempowerthemtogeneratemotions,weperformmotionpre-trainingtolearnmotiondistri-
butions. Themotionpre-trainingisconductedsimilarlytolanguagemodelpre-training.
Beforewecanstarttrainingthelanguagemodel,twomodificationstothemodelareneeded. Firstly,
sincethepre-trainedlanguagemodelonlycontainsembeddingsfortexttokens,weexpandtheem-
beddingsinaccordancewiththemotioncodebooksize. Secondly,theoutputshapeofthelanguage
model head is also expanded for the same reason. The language model is ready for motion pre-
trainingaftertheabovepreparations. Usingthemotiontokenizerdescribedabove,motionrepresen-
tationsM canbeencodedintoasequenceofmotiontokensW = {w }LW. Theyarefedintothe
i i=1
languagemodeltolearnthemotiontokendistributionbyconductingtheclassicnext-tokenpredic-
tion(Radfordetal.,2019). ThelossfunctionofthisstageL isformulatedas
pre
(cid:88)LW
L =− P(w |w ...w ;Θ), (4)
pre i 1 i−1
i=2
wherewemaximizethelog-likelihoodofthenext-tokenprobabilitygiventheprevioustokeninputs
andnetworkparameterΘ.
Afterthetrainingofthisstage,asaby-product,weobtainanunconditionalmotiongenerator. Given
aleadingmotionsequenceastheprompt,itcanautoregressivelysampleanarbitrarylengthofrea-
sonable human motion that continues the given motion. More importantly, the language model
learnsthedistributionofhumanmotionsandhastheabilityofsamplingplausiblehumanmotions,
whichlaysasolidfoundationforthenextstage.
3.4 MULTI-MODALINSTRUCTIONTUNING
Inspired by recent advancements in LLMs (Achiam et al., 2023; cha, 2022; Zheng et al., 2023),
to squeeze the power out of generative pre-training models, instruction tuning is adopted to guide
models with instructions to perform specific tasks. The instruction template usually consists of 1)
instructionsthatspecifywhichtaskstoperform;2)inputsofthetask;3)outputs. Wealsoenvision
our model accepting multi-modal sensor data as inputs. However, even with motion pre-training,
the model only accepts text or motion tokens as inputs. It is not practical or necessary to design
tokenizersandperformpre-trainingforalltheinvolvedmodalities. Therefore,wedrawinspiration
from vision language models (Liu et al., 2023b;a), where they directly map vision data to LLM
featurespacetoenablevisualquestionanswering.
Specifically, weconsidertwoinputmodalitiesotherthanmotionandnaturallanguages, whichare
egocentric videos and motion sensor inputs. Motion sensor inputs can be three-points (head and
wrists)6-DoFposesorone-point(onlyhead)6-DoFposes. Bothareencodedwithpositions,veloc-
ity, rotation and angular velocity. Below, we use three-points as examples. We unify both motion
trackingandmotionunderstandingusingthefollowingtemplates.
5TechnicalReport
Task:MotionTracking Task:MotionUnderstanding
Instruction:Performmotiontrackingbasedon Instruction:Describethehumanmotionbasedon
thegiventhree-pointsandCLIPembeddings. thegiventhree-pointsandCLIPembeddings.
Input: Input CLIP embeddings: Input: Input CLIP embeddings:
<CLIP Placeholder>. Input three- <CLIP Placeholder>. Input three-points
pointsfeature:<TP Placeholder> feature:<TP Placeholder>
Output:<Motion Placeholder> Output:<Narration Placeholder>
The encoded three-points 6-DoF poses would replace <TP Placeholder>.
<CLIP Placeholder> is the placeholder for egocentric video features. Motion tokens
arefilledin<Motion Placeholder>. <Narration Placeholder>istheplaceholderfor
corresponding motion narration. A detailed illustration of how we organize different modalities
of data is shown in Fig. 3. Texts are tokenized and translated to feature vectors through LM
embedding. EgocentricvideosarefirstencodedbyCLIPimageencoder(Radfordetal.,2021)per
frame,whicharefurtherprojectedbylinearlayerstothelanguagemodelfeaturespace. Similarly,
motionsensordata,e.g.,sequencesofthree-points6-DoFposes,isencodedbyafullyconvolutional
encoder. Lastly, alltheencodedfeaturesareconcatenatedandinputintothetransformerlayersof
thelanguagemodel.
Forthetrainingofmotionunderstanding,tobetterlearnthejointdistributionofmotionandnatural
languages, we also include two auxiliary tasks in the joint instruction training, which are motion-
to-textandtext-to-motiongeneration. Theyarealsodefinedwiththetemplatessimilartotheabove
ones. In summary, we train the four tasks jointly as the last step. The loss function is the same
next-tokenpredictionloss,asdefinedinEq.4.
4 EXPERIMENTS
4.1 EXPERIMENTALSETUP
Dataset. WeusetheNymeriadatasetMaetal.(2024)totrainandvalidateourmethod. Thedataset
provides a) full body motions, captured by the Xsens Mocap system (Roetenberg et al., 2009),
b) egocentric videos, captured by Aria glasses (Somasundaram et al., 2023), and c) narrations of
motionswrittenbyhumanannotators. Three-points6-DoFposesaretakenfromgroundtruthjoints.
Formotiontracking,thetrainingsetconsistsof147.89hofdataandthetestsethas41.93hofdata.
Formotionunderstanding,thetrainingsethas16673segments,eachlastingfor3-5seconds,adding
upto15.77h. Thetestsetconsistsof7468segments,6.76hofdata.
TrainingDetails. MotionVQ-VAEhastwocodebooks,eachhaving8192entriesandcodedimen-
sionof64. Thedown-samplerateisr =4. Formotiontracking,allexperimentsareconductedwith
windowsizeof60frames,whichis1second. Randomrotationaugmentationisappliedonmotions.
WechoosetouseGPT2-Medium(Radfordetal.,2019)asthelanguagebackbone.
Evaluation Protocols. For motion tracking, we calculate joint position errors (for full, upper and
lowerbody),jointangleerrors(forfullbodyandrootjoint). Formotionunderstanding,theoutputs
are natural languages. Therefore, we adopt NLP metrics, including BERT (Zhang et al., 2019),
BLEU(Papinenietal.,2002),andROUGE(Lin,2004)scores.
4.2 MOTIONTRACKING
Quantitative Results. We report quantitative results of motion tracking in Tab. 1. All methods
are evaluated with batch inference, meaning that every 60 frames are inferenced independently.
Weevaluateseveraldifferentinputcombinationsofthreemodalities,whicharethree-points6-DoF
poses(“3pts”),one-point6-DoFposes(“1pt”)andegocentricvideos(“Vid”). Forthe3pts-onlyand
1pt-onlysettings,EgoLMachievescomparableperformancewithbaselinemethods. Thisshowthe
effectiveness of using language models to perform precise motion tracking tasks. Moreover, we
also use egocentric videos to provide environment contexts for motion tracking. For three-points
tracking,theadditionalmodalitybrings10mmimprovementinfullbodyjointserror. Fortheone-
pointtracking,addingegocentricvideosimprovesjointserrorby20mm. Itshowstheeffectiveness
ofusingegocentricvideosascontextinformationfordisambiguationoftheill-posedproblem.
6TechnicalReport
Table 1: Quantitative Results of Motion Tracking. “Full”, “Upper”, “Lower” are joint position
errors in mm. “J.A.”, “Root” are joint angle errors for full body and root joint in degree. †We
directlyreplacethree-pointswithone-pointtotrainAvatarPoser.
InputModality
Method Full Upper Lower J.A. Root
3pts 1pt Vid.
AvatarPoser(Jiangetal.,2022) ✓ 85.89 52.78 165.18 12.41 14.78
Bodiffusion(Castilloetal.,2023) ✓ 79.80 52.79 152.68 12.74 13.09
Ours ✓ 83.88 54.06 148.37 13.31 14.13
Ours ✓ ✓ 73.38 49.67 124.58 12.48 13.23
AvatarPoser†(Jiangetal.,2022) ✓ 129.23 94.19 192.34 16.55 21.60
EgoEgo(Lietal.,2023) ✓ 132.16 100.02 190.32 18.90 21.80
Ours ✓ 127.45 97.87 174.92 16.97 20.57
Ours ✓ ✓ 106.95 83.73 141.26 14.67 19.04
0mm 200mm
Figure4:QualitativeResultsofThree-PointsMotionTracking. Skeletonsarecolor-codedbythe
jointpositionerrors. Baselinemethodsonlyusethree-pointsasinputs. Oursusesthree-pointsand
egocentricvideosasinputs.
Qualitative Results. Three-points motion tracking results and comparisons are shown in Fig. 4.
Duetotheambiguityofthree-points,AvatarPosermistakenlygeneratesstandingposesforsquatting
sequences (right example). BoDiffusion, for its generative nature, can sample correct results in
some cases, e.g., the squatting example. But it also suffers from the ambiguity issue, as shown in
the bending down sequence (left example). They show the importance of considering contexts in
the motion tracking task for the purpose of disambiguation. Our full model can reliably perform
three-pointsbodytrackingfortheshownchallengingcases.
One-point motion tracking results are shown in Fig. 5. It is a more challenging task especially
for upper body. As shown in the left example, the upper body motions generated by EgoEgo are
completelydifferentfromthegroundtruth. Intherightexample,EgoEgowronglygeneratessitting
posesforstandingframesandstandingposesforsittingframes, whichiscausedbytheambiguity
problem. Egocentric videos in this task not only help to eliminate the ambiguity, but also provide
somecluesaboutthehandposition. Intheleftexample,whenhandsarevisibleintheframes,our
modelcapturesthisinformationthroughCLIPembeddingsandgeneratescorrectarmmovements.
7
cirtnecogE
resoPratavA
noisuffiDoB
sruO
TG
oediVTechnicalReport
0mm 200mm
Figure5: QualitativeResultsofOne-PointMotionTracking. Skeletonsarecolor-codedbyjoint
positionerrors. EgoEgoonlyusesone-pointasinputs. Oursincludesegocentricvideosasinputs.
4.3 MOTIONUNDERSTANDING
Quantitative Results. We report quantitative results of motion understanding in Tab. 2. For
this task, we tested three input modalities, i.e., three-points (“3pts”), motions, and egocentric
videos (“Vid”). Different combinations of these modalities are evaluated. We first test and com-
pare with two motion understanding methods that only take motion as inputs, TM2T (Guo et al.,
2022b)andMotionGPT(Jiangetal.,2024). TM2Ttrainslanguagegenerationfromscratch,which
explains its poor performance. MotionGPT uses a pre-trained T5 model (Raffel et al., 2020).
EgoLM(M2T&T2M) achieves the best performance for the scalability advantage brought by the
decoder-onlyarchitecture.
Using motion as inputs requires precise motion tracking, which is not always available. So, we
exploredusingsensorinputsinstead. Wetestedtwovariants: three-points-only(TP2T)andegocen-
tricvideosonly(V2T).TheTP2Tvariantshowedanoticeabledropinperformancecomparedtothe
motion-onlyversion,asthree-pointsprovidelimitedinformationaboutbodymotion.Incontrast,the
V2Tvariantoutperformedthemotion-onlyversionbecauseegocentricvideoscaptureenvironmen-
talcontextrelevanttoourmotionnarrations. Thishighlightstheimportanceofegocentricvideosin
understandingmotion.
We then test our proposed setup of combing three-points and egocentric videos for motion under-
standing. There are three ways of achieving this setup. The first one is to combine two existing
setups: 1) three-points motion tracking and 2) motion-to-text generation (TPV2M + MV2T). The
performanceofthisvariantslightlydropscomparedwithMV2Tvariant,duetoerroraccumulation.
The second way is directly training three-points plus egocentric video to text generation (TPV2T)
withtheproposedmulti-modalinstructiontuning. Itisbetterthanonlyusingegocentricvideosor
motions. However, it still falls behind MV2T variant for the missing lower body information. To
solvethat,weproposetoalsoincludethree-pointsmotiontrackingintrainingtoactivelyestablish
theconnectionbetweenthree-pointsandmotionnarrations. Jointtrainingimprovesmotionunder-
standingfromthree-pointsplusegocentricvideo,whichprovestheeffectivenessofusingmotionas
abridgebetweendifferentmodalities.
Qualitative Results. We show four examples of motion understanding in Fig. 6. TM2T and Mo-
tionGPTusefullbodymotionsasinputs. Oursisthefullversionwiththree-pointsandegocentric
videosasinputs.TM2T’slanguagegenerationpartistrainedfromscratch.Therefore,itoftenmakes
mistakesaboutmotionsandevengeneratestextsthatdoesnotmakesense.MotionGPTcangenerate
reasonabledescriptionsforthemotions.Inthelowerleftexample,justfromthemotions,“removing
apieceofclothingfromthehanger”isareasonableanswer. However,ourtargetmotionnarration
8
ogEogE
sruO
TG
cirtnecogE
oediVTechnicalReport
Table2: QuantitativeResultsofMotionUnderstanding. Differentinputmodalitycombinations
aretested. Allmetricsarehigherthebetter.
InputModality
Method Bert↑ Bleu@1↑ Bleu@4↑ RougeL↑
3pts Motion Vid.
TM2T(Guoetal.,2022b) ✓ 11.08 40.11 8.99 30.70
MotionGPT(Jiangetal.,2024) ✓ 14.09 42.22 10.31 32.33
Ours(M2T&T2M) ✓ 15.90 42.68 11.06 33.71
Ours(TP2T) ✓ 11.94 41.70 9.85 31.47
Ours(V2T) ✓ 16.62 43.03 11.34 33.13
Ours(TPV2M+MV2T) ✓ ✓ 19.97 45.41 12.81 35.04
Ours(TPV2T) ✓ ✓ 18.38 44.55 12.12 33.80
Ours(JointTraining) ✓ ✓ 19.40 45.45 12.72 34.82
TM2T:The person is sitting at the table as he lays her body on the sofa then leans TM2T:The person is standing still in front of the cabinet while making a hanger. The
backwards while talking and looking at her colleague. The person is resting both of her person bends and raises her left hand then lays the hanger on her side of her chest then
arms on her lap, lifts and bends both of her arms as she sits down on the sofa. The spreads both arms on her side below her chest. The person stands with both legs
person is sitting on the sofa with both legs bent and slightly spread apart. stretched upright and both feet fixed on the floor.
MotionGPT: The person is standing still in front of the sofawhile holding a piece of MotionGPT: The person is standing straight at the living room … The human has both
clothing. The human's left arm is bent and raised upward with his left hand holding a arms naturally hanging at her sides then she bends, extends and raises her right arm
piece of clothing. The human is standing with both legs apart and both feet resting on and throws the object on the living room with her right hand. … The human has both
the floor. feet fixed on the floor with both legs stretched upright then she slightly bends and
spreads both of her legs widely apart.
Ours: The human is standing in the bedroomto fold the piece of clothing. The human is
folding the piece of clothing with his left and right hand. The person is resting his left Ours: The person is standing still in the living room while talking to her peer. The
and right foot on the floor. human lifts both of his arms and then moves both hands in circular motion as she
gesticulates. The human rests both of his feet on the ground.
TM2T: the person stands up straight as she holds the pillow and place them on the TM2T: The person lowers her thigh as she lays down on the floor while kneeling on the
table. the person then arrange the pillow in the middle of the room with her right hand floor. The person extends both her arms as she moves her right elbow on the floor to
and places it on the table, while her left arm is slightly bent in front as she holds and support her body.
arrange the pillow in the direction of the table.
MotionGPT:The person stands in front of the cabinet to remove the clothes from the MotionGPT: The human bends down while kneeling in the living area. The person
hanger. the human raises both of his arms to remove a piece of clothing from the extends both of her arms on the floor to support her body. The human extends both of
hanger. the human stands with both feet fixed on the floor. her legs on the ground.
Ours: The person is standing by the refrigerator while putting the pack of food inside
the freezer. The human puts the pack of food inside the freezer with her right hand as Ours:The person bends down as she plankson the floor. The human extends both of her
her left hand holds the refrigerator door. The human is standing with both feed fixed on arms on the floor to support her body. The person extends both of her legs while
the floor. tiptoeing both of her feet.
Figure6: QualitativeResultsofMotionUnderstanding. Weusegreentohighlightcorrectparts
andredformistakes.
ishighlyrelatedtoenvironments. TM2TandMotionGPTfailtogeneratecorrectnarrationsforthe
lackofvisionsignals. Forourmodel,eventhoughwedonotdirectlyusemotionsasinputs,EgoLM
jointlymodelthedistributionsofdifferentmodalitiesandcangeneratecorrectnarrationsaccording
todifferentscenarios.
4.4 ABLATIONSTUDY
Window Size of Motion Tracking. As shown in Tab. 3, we increase the window size for three-
points motion tracking from 60 to 120 frames, which brings an improvement of 4.2mm in joint
position errors. This is reasonable since increasing the window size brings more contexts, which
helps the disambiguation. If we further include egocentric videos in the inputs, the improvement
ofincreasingwindowsizeisnotaslarge. Moreover, using60framesplusegocentricvideoshows
betterperformancethanonlyusing120frames. Thisindicatesthatthecontextofegocentricvideo
mightbemoreeffectivethanincreasingwindowsize.
9TechnicalReport
Table 3: Ablation Study Table4:AblationStudyonRecon- Table 5: Ablation on
on Window Size for Motion struction Results of Motion VQ- the LM size. Medium:
Tracking. VAE.[mm] 345M;Large: 1.5B
Win Vid Full Upper Lower J.A. PQ CB Dim MPJPE PA-MPJPE ACCEL GPT-2Size Medium Large
60 83.88 54.06 148.37 13.31 1 2048 512 51.60 37.55 1.09 Bert↑ 18.38 19.56
120 79.61 52.66 138.87 13.01 2 2048 512 39.63 29.77 0.71 Bleu@1↑ 44.55 44.48
60 ✓ 73.38 49.67 124.58 12.48 2 16384 256 39.13 29.78 1.08 Bleu@4↑ 12.12 12.49
120 ✓ 72.76 49.20 123.09 12.52 2 16384 64 34.49 26.83 0.67 RougeL↑ 33.80 35.21
Input Prompt: Input Prompt:
The human leans The person walks
forward and then toward the kitchen
turns right while gas range and then
walking towards the grabs the fork while
kitchen sink. The her left arm rest
person holds and beside her. The
close the kitchen person is walking
drawer with her left forward to kitchen
hand while the right gas range with her
arm rest beside her. both feet and then
The person bends steps sideward with
her both legs and her right and left
then steps backward. footrespectively.
a) Text-to-Motion Generation Results b) Motion Prediction Results
Figure7: MoreAnalysisonEgoLM.a)Qualitativeresultsoftext-to-motiongeneration. b)Quali-
tativeresultsofmotionprediction.
MotionVQ-VAE.AblationstudiesonmotionVQ-VAEarereportedinTab.4. “PQ”isthenumber
of codebooks. “CB” is the total number of entries in codebooks. The first two lines shows that
large improvements can be achieved by simply using product quantization. Moreover, increasing
thenumberofcodesanddecreasingcodedimensionsbringfurtherimprovement.
Larger Language Model. We use GPT-2 Medium (345M) to conduct most of our experiments
forefficiency. ToexaminethepotentialofusinglargerLM,wetrainwithGPT-2Large(1.5B)and
report performance on TPV2T in Tab. 5. The improved scores suggest EgoLM’s scalability as a
versatileframework.
4.5 MOREAPPLICATIONS
Text-to-MotionGeneration. Aspartofourjointtraining,EgoLMiscapableofgeneratingmotions
from texts, as shown in Fig. 7 a). Even with long prompts separately describing upper body and
lowerbody,ourmodelisabletogeneratemotionsthatmatchtheinputs.
MotionPrediction. Asaby-productofthemotionpre-training,EgoLMcanfunctionasamotion
predictor. As shown in Fig. 7 b), given motion prompts (the red skeleton in the left), subsequent
motionscanberandomlysampled. Weshowthreedifferentsamplesindifferentcolors.
5 DISCUSSION
WeproposeEgoLM,amulti-modallanguagemodelforegocentricmotiontrackingandunderstand-
ing. A three-steps paradigm, including motion tokenization, motion pre-training and multi-modal
instructiontuning,isproposedtofacilitatethetraining. Incontrasttopreviousworks,theproposed
frameworkunifiestheegocentricmotiontaskswithalanguagemodel,andincorporatesmulti-modal
sensordataascontextinformation,whichisproveneffectiveforbothtasks.
Limitations. Firstly, our motion tokenizer is a VQ-VAE, which carries reconstruction errors. It
sets an upper bound for motion tracking. Moreover, for the motion tracking training, the loss is
calculatedondiscretemotiontokens,insteadofrawmotionrepresentations,whichmightalsoharm
the performance of motion tracking. Secondly, for motion understanding, since each egocentric
videoframeiscompressedbytheCLIPencodertoaone-dimensionalvector,itishardformodels
topreciselynametheobjectthatthepersonisinteractingwith. Moreover,asiscommonlyobserved
inlanguagemodels(Jietal.,2023),EgoLMalsosuffersfromthehallucinationproblem.
Potential Societal Impact. While contextual AI offers opportunities for efficiency improvement
andsocietaladvancement,thecollectionandanalysisofhumandatacouldleadtoprivacyissuesfor
bothusersandpeoplearound.
10TechnicalReport
REFERENCES
Nov2022. URLhttps://openai.com/blog/chatgpt.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man,DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technical
report. arXivpreprintarXiv:2303.08774,2023.
FedericaBogo,AngjooKanazawa,ChristophLassner,PeterGehler,JavierRomero,andMichaelJ.
Black. KeepitSMPL:Automaticestimationof3Dhumanposeandshapefromasingleimage.
In Computer Vision – ECCV 2016, Lecture Notes in Computer Science. Springer International
Publishing,October2016.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare
few-shotlearners. Advancesinneuralinformationprocessingsystems,33:1877–1901,2020.
AngelaCastillo,MariaEscobar,GuillaumeJeanneret,AlbertPumarola,PabloArbela´ez,AliThabet,
andArtsiomSanakoyeu. Bodiffusion: Diffusingsparseobservationsforfull-bodyhumanmotion
synthesis. arXivpreprintarXiv:2304.11118,2023.
YiChen,YuyingGe,YixiaoGe,MingyuDing,BohaoLi,RuiWang,RuifengXu,YingShan,and
XihuiLiu. Egoplan-bench: Benchmarkingegocentricembodiedplanningwithmultimodallarge
languagemodels. arXivpreprintarXiv:2312.06722,2023.
DimaDamen,HazelDoughty,GiovanniMariaFarinella,SanjaFidler,AntoninoFurnari,Evangelos
Kazakos,DavideMoltisanti,JonathanMunro,TobyPerrett,WillPrice,andMichaelWray. Scal-
ing egocentric vision: The epic-kitchens dataset. In European Conference on Computer Vision
(ECCV),2018.
DimaDamen,HazelDoughty,GiovanniMariaFarinella,SanjaFidler,AntoninoFurnari,Evangelos
Kazakos,DavideMoltisanti,JonathanMunro,TobyPerrett,WillPrice,andMichaelWray. The
epic-kitchensdataset: Collection,challengesandbaselines. IEEETransactionsonPatternAnal-
ysis and Machine Intelligence (TPAMI), 43(11):4125–4141, 2021. doi: 10.1109/TPAMI.2020.
2991965.
Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Jian Ma, Evange-
los Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray.
Rescalingegocentricvision: Collection,pipelineandchallengesforepic-kitchens-100. Interna-
tionalJournalofComputerVision(IJCV),130:33–55, 2022. URLhttps://doi.org/10.
1007/s11263-021-01531-2.
KanakamedalaDeepika,VeerankiTilekya,JatrothMamatha,andTSubetha. Jollitychatbot-acon-
textual ai assistant. In 2020 Third International Conference on Smart Systems and Inventive
Technology(ICSSIT),pp.1196–1200.IEEE,2020.
AnaGarciaDelMolino, ChestonTan, Joo-HweeLim, andAh-HweeTan. Summarizationofego-
centricvideos: Acomprehensivesurvey. IEEETransactionsonHuman-MachineSystems,47(1):
65–76,2016.
PrafullaDhariwal,HeewooJun,ChristinePayne,JongWookKim,AlecRadford,andIlyaSutskever.
Jukebox: Agenerativemodelformusic. arXivpreprintarXiv:2005.00341,2020.
YumingDu, RobinKips, AlbertPumarola, SebastianStarke, AliThabet, andArtsiomSanakoyeu.
Avatarsgrowlegs: Generatingsmoothhumanmotionfromsparsetrackinginputswithdiffusion
model.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pp.481–490,2023.
Maria Escobar, Laura Daza, Cristina Gonza´lez, Jordi Pont-Tuset, and Pablo Arbela´ez. Video
swin transformers for egocentric video understanding@ ego4d challenges 2022. arXiv preprint
arXiv:2207.11329,2022.
11TechnicalReport
KristenGrauman,AndrewWestbury,EugeneByrne,ZacharyChavis,AntoninoFurnari,RohitGird-
har, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in
3,000hoursofegocentricvideo.InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,pp.18995–19012,2022.
ChuanGuo,XinxinZuo,SenWang,ShihaoZou,QingyaoSun,AnnanDeng,MinglunGong,and
LiCheng. Action2motion: Conditionedgenerationof3dhumanmotions. InProceedingsofthe
28thACMInternationalConferenceonMultimedia,pp.2021–2029,2020.
Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating
diverseandnatural3dhumanmotionsfromtext. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition(CVPR),pp.5152–5161,June2022a.
ChuanGuo, XinxinZuo, SenWang, andLiCheng. Tm2t: Stochasticandtokenizedmodelingfor
thereciprocalgenerationof3dhumanmotionsandtexts. InEuropeanConferenceonComputer
Vision,pp.580–597.Springer,2022b.
Thorsten Hempel, Ahmed A Abdelrahman, and Ayoub Al-Hamadi. 6d rotation representation for
unconstrainedheadposeestimation.In2022IEEEInternationalConferenceonImageProcessing
(ICIP),pp.2496–2500.IEEE,2022.
CatalinIonescu,DragosPapava,VladOlaru,andCristianSminchisescu. Human3.6m: Largescale
datasetsandpredictivemethodsfor3dhumansensinginnaturalenvironments.IEEEtransactions
onpatternanalysisandmachineintelligence,36(7):1325–1339,2013.
Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor
search. IEEEtransactionsonpatternanalysisandmachineintelligence,33(1):117–128,2010.
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,
AndreaMadotto,andPascaleFung. Surveyofhallucinationinnaturallanguagegeneration. ACM
ComputingSurveys,55(12):1–38,2023.
Baoxiong Jia, Ting Lei, Song-Chun Zhu, and Siyuan Huang. Egotaskqa: Understanding human
tasksinegocentricvideos. AdvancesinNeuralInformationProcessingSystems,35:3343–3360,
2022.
BiaoJiang,XinChen,WenLiu,JingyiYu,GangYu,andTaoChen. Motiongpt: Humanmotionas
aforeignlanguage. AdvancesinNeuralInformationProcessingSystems,36,2024.
Jiaxi Jiang, Paul Streli, Huajian Qiu, Andreas Fender, Larissa Laich, Patrick Snape, and Christian
Holz. Avatarposer: Articulatedfull-bodyposetrackingfromsparsemotionsensing. InEuropean
ConferenceonComputerVision,pp.443–460.Springer,2022.
Jiaxi Jiang, Paul Streli, Manuel Meier, Andreas Fender, and Christian Holz. Egoposer: Robust
real-timeego-bodyposeestimationinlargescenes. arXivpreprintarXiv:2308.06493,2023.
AngjooKanazawa,MichaelJ.Black,DavidW.Jacobs,andJitendraMalik. End-to-endrecoveryof
humanshapeandpose. InComputerVisionandPatternRecognition(CVPR),2018.
HildegardKuehne,HueihanJhuang,Est´ıbalizGarrote,TomasoPoggio,andThomasSerre.Hmdb:a
largevideodatabaseforhumanmotionrecognition.In2011Internationalconferenceoncomputer
vision,pp.2556–2563.IEEE,2011.
Jiaman Li, Karen Liu, and Jiajun Wu. Ego-body pose estimation via ego-head pose estimation.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
17142–17151,2023.
YinLi,ZhefanYe,andJamesMRehg. Delvingintoegocentricactions. InProceedingsoftheIEEE
conferenceoncomputervisionandpatternrecognition,pp.287–295,2015.
Chin-YewLin. Rouge: Apackageforautomaticevaluationofsummaries. InTextsummarization
branchesout,pp.74–81,2004.
12TechnicalReport
HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee.Improvedbaselineswithvisualinstruction
tuning,2023a.
HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. InNeurIPS,
2023b.
MatthewLoper,NaureenMahmood,JavierRomero,GerardPons-Moll,andMichaelJBlack.Smpl:
A skinned multi-person linear model. In Seminal Graphics Papers: Pushing the Boundaries,
Volume2,pp.851–866.2023.
ThomasLucas,FabienBaradel,PhilippeWeinzaepfel,andGre´goryRogez. Posegpt: Quantization-
based3dhumanmotiongenerationandforecasting.InEuropeanConferenceonComputerVision,
pp.417–435.Springer,2022.
Lingni Ma, Yuting Ye, Fangzhou Hong, Vladimir Guzov, Yifeng Jiang, Rowan Postyeni, Luis
Pesqueira, Alexander Gamino, Vijay Baiyya, Hyo Jin Kim, et al. Nymeria: A massive collec-
tionofmultimodalegocentricdailymotioninthewild. arXivpreprintarXiv:2406.09905,2024.
JulietaMartinez,RayatHossain,JavierRomero,andJamesJ.Little. Asimpleyeteffectivebaseline
for3dhumanposeestimation. InICCV,2017.
NicholasMilef,ShinjiroSueda,andNKhademiKalantari.Variationalposepredictionwithdynamic
sampleselectionfromsparsetrackingsignals.InComputerGraphicsForum,volume42,pp.359–
369.WileyOnlineLibrary,2023.
Vimal Mollyn, Riku Arakawa, Mayank Goel, Chris Harrison, and Karan Ahuja. Imuposer: Full-
body pose estimation using imus in phones, watches, and earbuds. In Proceedings of the 2023
CHIConferenceonHumanFactorsinComputingSystems,pp.1–12,2023.
TusharNagarajan,SanthoshKumarRamakrishnan,RutaDesai,JamesHillis,andKristenGrauman.
Egoenv: Human-centricenvironmentrepresentationsfromegocentricvideo. AdvancesinNeural
InformationProcessingSystems,36,2024.
AaronvandenOord,OriolVinyals,andKorayKavukcuoglu. Neuraldiscreterepresentationlearn-
ing. arXivpreprintarXiv:1711.00937,2017.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluationofmachinetranslation. InProceedingsofthe40thannualmeetingoftheAssociation
forComputationalLinguistics,pp.311–318,2002.
Dario Pavllo, Christoph Feichtenhofer, David Grangier, and Michael Auli. 3d human pose es-
timation in video with temporal convolutions and semi-supervised training. In Conference on
ComputerVisionandPatternRecognition(CVPR),2019.
MathisPetrovich,MichaelJBlack,andGu¨lVarol. Action-conditioned3dhumanmotionsynthesis
with transformer vae. In Proceedings of the IEEE/CVF International Conference on Computer
Vision,pp.10985–10995,2021.
Chiara Plizzari, Gabriele Goletto, Antonino Furnari, Siddhant Bansal, Francesco Ragusa, Gio-
vanniMariaFarinella,DimaDamen,andTatianaTommasi. Anoutlookintothefutureofegocen-
tricvision. arXivpreprintarXiv:2308.07123,2023.
JoseLuisPonton, HaoranYun, AndreasAristidou, CarlosAndujar, andNuriaPelechano. Sparse-
poser:Real-timefull-bodymotionreconstructionfromsparsedata.ACMTransactionsonGraph-
ics,43(1):1–14,2023.
Abhinanda R. Punnakkal, Arjun Chandrasekaran, Nikos Athanasiou, Alejandra Quiros-Ramirez,
andMichaelJ.Black. BABEL:Bodies,actionandbehaviorwithenglishlabels. InProceedings
IEEE/CVFConf.onComputerVisionandPatternRecognition(CVPR),pp.722–731,June2021.
AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal.Language
modelsareunsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
13TechnicalReport
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
modelsfromnaturallanguagesupervision. InInternationalconferenceonmachinelearning,pp.
8748–8763.PMLR,2021.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,Yanqi
Zhou,WeiLi,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-text
transformer. TheJournalofMachineLearningResearch,21(1):5485–5551,2020.
IvanRodin,AntoninoFurnari,DimitriosMavroeidis,andGiovanniMariaFarinella. Predictingthe
futurefromfirstperson(egocentric)vision:Asurvey.ComputerVisionandImageUnderstanding,
211:103252,2021.
Daniel Roetenberg, Henk Luinge, and Per Slycke. Xsens mvn: Full 6dof human motion tracking
usingminiatureinertialsensors. XsensMotionTechnol.BVTech.Rep.,3,012009.
Kiran Somasundaram, Jing Dong, Huixuan Tang, Julian Straub, Mingfei Yan, Michael Goesele,
Jakob Julian Engel, Renzo De Nardi, and Richard Newcombe. Project aria: A new tool for
egocentricmulti-modalairesearch. arXivpreprintarXiv:2308.13561,2023.
KhurramSoomro,AmirRoshanZamir,andMubarakShah.Ucf101:Adatasetof101humanactions
classesfromvideosinthewild. arXivpreprintarXiv:1212.0402,2012.
ShuhanTan,TusharNagarajan,andKristenGrauman. Egodistill: Egocentricheadmotiondistilla-
tionforefficientvideounderstanding. AdvancesinNeuralInformationProcessingSystems, 36,
2024.
Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit H Bermano.
Humanmotiondiffusionmodel. arXivpreprintarXiv:2209.14916,2022.
AlexanderToshevandChristianSzegedy. Deeppose: Humanposeestimationviadeepneuralnet-
works. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition, pp.
1653–1660,2014.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe´e
Lacroix, Baptiste Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023a.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Niko-
layBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfounda-
tionandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023b.
Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spa-
tiotemporal features with 3d convolutional networks. In Proceedings of the IEEE International
ConferenceonComputerVision(ICCV),December2015.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-
tionprocessingsystems,30,2017.
TomVercauteren,MathiasUnberath,NicolasPadoy,andNassirNavab. Cai4cai: theriseofcontex-
tual artificial intelligence in computer-assisted interventions. Proceedings of the IEEE, 108(1):
198–214,2019.
Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool.
Temporal segment networks: Towards good practices for deep action recognition. In European
conferenceoncomputervision,pp.20–36.Springer,2016.
JilanXu,YifeiHuang,JunlinHou,GuoChen,YuejieZhang,RuiFeng,andWeidiXie. Retrieval-
augmentedegocentricvideocaptioning. arXivpreprintarXiv:2401.00789,2024.
ZihuiXue,YaleSong,KristenGrauman,andLorenzoTorresani. Egocentricvideotasktranslation.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
2310–2320,2023.
14TechnicalReport
Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial temporal graph convolutional networks for
skeleton-based action recognition. In Proceedings of the AAAI conference on artificial intelli-
gence,volume32,2018.
Xinyu Yi, Yuxiao Zhou, Marc Habermann, Vladislav Golyanik, Shaohua Pan, Christian Theobalt,
andFengXu. Egolocate: Real-timemotioncapture,localization,andmappingwithsparsebody-
mountedsensors. arXivpreprintarXiv:2305.01599,2023.
Ryo Yonetani, Kris M Kitani, and Yoichi Sato. Recognizing micro-actions and reactions from
pairedegocentricvideos.InProceedingsoftheIEEEConferenceonComputerVisionandPattern
Recognition,pp.2629–2638,2016.
Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language
modelforvideounderstanding. arXivpreprintarXiv:2306.02858,2023a.
Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli Huang, Yong Zhang, Hongwei Zhao,
Hongtao Lu, and Xi Shen. T2m-gpt: Generating human motion from textual descriptions with
discreterepresentations. arXivpreprintarXiv:2301.06052,2023b.
Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei
Liu. Motiondiffuse: Text-drivenhumanmotiongenerationwithdiffusionmodel. IEEETransac-
tionsonPatternAnalysisandMachineIntelligence,2024.
TianyiZhang,VarshaKishore,FelixWu,KilianQWeinberger,andYoavArtzi. Bertscore: Evaluat-
ingtextgenerationwithbert. arXivpreprintarXiv:1904.09675,2019.
Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei Bai, Qi Chu, Nenghai Yu,
and Wanli Ouyang. Motiongpt: Finetuned llms are general-purpose motion generators. arXiv
preprintarXiv:2306.10900,2023c.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
ZiLin,ZhuohanLi,DachengLi,Eric.PXing,HaoZhang,JosephE.Gonzalez,andIonStoica.
Judgingllm-as-a-judgewithmt-benchandchatbotarena,2023.
Zixiang Zhou, Yu Wan, and Baoyuan Wang. Avatargpt: All-in-one framework for motion under-
standing,planning,generationandbeyond. arXivpreprintarXiv:2311.16468,2023.
15TechnicalReport
SUPPLEMENTARY
We provide more implementation details (Sec. A) and qualitative results (Sec. B) in this supple-
mentary material. To better showcase our results, We also provide videos in our project page
https://hongfz16.github.io/projects/EgoLM.
1 𝑡−1 𝑡 𝑡−2 𝑡−1 𝑡 𝑡−1 𝑡 𝑡+1 𝑡−1 𝑡 𝑡+1
… … … … …
𝑡−1 𝑡
EgoLM EgoLM
… a) Inference b) Auto-regressive motion
1 𝑡−1 𝑡 Initialization Inference for next step of 𝑡+1 𝑡+1
Figure8:OnlineMotionTrackingInference. Forthenewtimestepoft+1withnewdatacoming
in,lastmotiontokensarecombinedwiththenewinputtokenstodecodethenextmotiontokent+1.
A IMPLEMENTATION DETAILS
A.1 AUTO-REGRESSIVEINFERENCEFORMOTIONTRACKING
Atinferencetime,motionunderstandingisthesameasthelanguagemodelinference. Formotion
tracking,itusuallyrequiresonlineinferenceoveralongperiod. Withalanguagemodel,whichisan
auto-regressivemodel,itisstraight-forwardtoperformonlinemotiontracking. AsshowninFig.8,
firstly, an initialization over the first t frames of data is required. When the new data frame t+1
comesin, theinputconditionsareupdatedaccordingly. Then, itisnotnecessarytopredictallthe
motiontokensfromframe2toframet+1. Wetakethepreviouslygeneratedmotiontokensfrom
frame2toframetasinputsandpromptthenetworktogenerateonemoretokenforframet+1.
A.2 EVALUATIONMETRICS
Formotiontracking,weusejointpositionerrorsandjointangleerrorstoevaluatetheperformance.
Specifically,forthejointpositionerrors,wefirstaligngroundtruthskeletonsandgeneratedskele-
tonsbytheheadpositionsonlybytranslation. Thenfullbody,upperbodyandlowerbodyjointpo-
sitionerrorsarecalculatedseparately. Jointangleerrorsarecalculatedonfullbodyandrootjoints.
FortheevaluationofmotionVQ-VAEinmainpaperTab. 4, weapplywidelyadoptedmetricsfor
motion regression, i.e., Mean Per-Joint Position Error (MPJPE) (Ionescu et al., 2013), Procrustes-
Aligned(PA-)MPJPE(Kanazawaetal.,2018),andjointpositionacceleration(ACCL)error.Forthe
motion understanding, we use standard NLP metrics, please kindly refer to corresponding papers
formoredetails.
B MORE QUALITATIVE RESULTS
B.1 THREE-POINTSMOTIONTRACKING
Weshowfourmorevisualexamplesofthree-pointsmotiontrackinginFig.9,Fig.10andFig.11.
AvatarPoser(Jiangetal.,2022)andBoDiffusion(Castilloetal.,2023)aresolidbaselinesthatper-
form well on easy walking cases, e.g., upper example in Fig. 10. For the workout sequence, i.e.,
lowerexampleinFig.11,evenonlygiventhreepointsofupperbody,thedistributionoflowerbody
motioncanbecollapsedandgeneratereasonablemotionsthatmatchesthegroundtruth. InFig.11,
wedemonstratetheeffectivenessofincludingegocentricvideosasinputs.Withoutanyenvironment
context,AvatarPoserandBoDiffusionoftenfailtodistinguishstandingandsittingdown. Wedonot
assumetheknowledgeoftheheadheightoverthefloor,meaningthatthethree-pointspositionsare
normalizedtothelocalcoordinatesofthefirstframe. Therefore,itishardforbaselinemethodsto
disambiguate certain scenarios. We propose to introduce contexts using egocentric videos, which
contains rich information about the environment and how the person is interacting with it. There-
16
snekoT
noitoMTechnicalReport
0mm 200mm
Figure 9: Qualitative Results of Three-Points Motion Tracking. Skeletons are color-coded by
jointpositionerrors.
fore, our model can generate the most accurate motions by utilizing these information. For more
visualizationofthree-pointsmotiontracking,pleasekindlyrefertooursupplementaryvideos.
17
cirtnecogE
resoPratavA
noisuffiDoB
sruO
TG
oediVTechnicalReport
0mm 200mm
Figure10: QualitativeResultsofThree-PointsMotionTracking. Skeletonsarecolor-codedby
jointpositionerrors.
18
cirtnecogE
cirtnecogE
resoPratavA
noisuffiDoB
sruO
TG
resoPratavA
noisuffiDoB
sruO
TG
oediV
oediVTechnicalReport
0mm 200mm
Figure11: QualitativeResultsofThree-PointsMotionTracking. Skeletonsarecolor-codedby
jointpositionerrors.
19
cirtnecogE
cirtnecogE
resoPratavA
noisuffiDoB
sruO
TG
resoPratavA
noisuffiDoB
sruO
TG
oediV
oediVTechnicalReport
0mm 200mm
Figure12:QualitativeResultsofOne-PointMotionTracking. Skeletonsarecolor-codedbyjoint
positionerrors.
B.2 ONE-POINTMOTIONTRACKING
Weshowfourmoreexamplesofone-pointmotiontrackinginFig.12andFig.13. Theintroduction
ofegocentricvideoshastwoadvantages. Firstly, similartothecaseinthree-pointsbodytracking,
theenvironmentcontextsinegocentricvideoscandisambiguatecaseslikestandingandsitting.Sec-
ondly,specificallyforone-pointmotiontracking,egocentricvideosprovidecluesofhandpositions.
Asshowninallfourexamples,whenthepersonraisesthearmsinfrontofthebody,handswouldbe
visibleintheegocentricvideos,whichhelpsthehandpositiontracking. Admittedly,high-levelse-
manticinformationprovidedbyCLIP(Radfordetal.,2021)encoderscannotaccuratelytrackhand
positions.Therefore,asshowninthelowerexampleinFig.12,ourmethodcorrectlygeneratesarms
movingintheair,butlacksaccuracy.Formorevisualexamplesofone-pointmotiontracking,please
kindlyrefertooursupplementaryvideo.
20
cirtnecogE
cirtnecogE
ogEogE
sruO
TG
ogEogE
sruO
TG
oediV
oediVTechnicalReport
0mm 200mm
Figure13:QualitativeResultsofOne-PointMotionTracking. Skeletonsarecolor-codedbyjoint
positionerrors.
21
cirtnecogE
cirtnecogE
ogEogE
sruO
TG
ogEogE
sruO
TG
oediV
oediVTechnicalReport
Figure14: ThreeRandomSamplesofOne-PointMotionTrackingwithEgocentricVideosas
Inputs. Sinceweuselanguagemodelsasourbackbone,EgoLMhastheabilitytorandomlysample
outputsgiventhesameinputs. Egocentricvideosprovidestrongcluesforhandpositions,leadingto
lessdiversityasshowninthehighlightedareas.
B.2.1 MULTIPLESAMPLES.
NotethatEgoLMisessentiallyagenerativemodel. Therefore,ourmodeliscapableofgenerating
different samples with the same inputs. In Fig. 14, we show three random samplings on the same
input one-point and egocentric video. When hands are not visible in the frame, i.e., the left high-
lightedframe,handpositionsarenotconstrained,andthereforeshowshighdiversityacrossdifferent
samples. Fortheotherhighlightedframes,handsarevisibleintheegocentricvideos,whichhelpsto
collapsethedistributionofpossiblepositionsofhands.Butasdiscussedabove,ourwayofencoding
egocentricvideoscannotaccuratelytrackthehandpositions.Therefore,ourmodelalsoshowssome
diversityofhandpositionsinthesecases.
To further demonstrate the diversity of our model, we also show three random samples from our
one-pointmotiontrackingmodelthatdoesnottakeegocentricvideosasinputsinFig.15. Lackof
anyindicationofthehandpositions,theupperbodygenerationisevenlessconstrainedthanthatof
thelowerbodyandshowshighdiversityacrossthreesamples.
22
cirtnecogE
1
elpmaS
2
elpmaS
3
elpmaS
TG
oediVTechnicalReport
Figure15: ThreeRandomSamplesofOne-PointMotionTrackingwithoutEgocentricVideos
as Inputs. With only head poses as inputs, the generation of full body motion, especially upper
bodymotions,islessconstrained.
23
1
elpmaS
2
elpmaS
3
elpmaS
TGTechnicalReport
TM2T: The person still marches in place while facing his peers. The person still swings TM2T: The person is sitting on a chairand leaning backward on the table while talking
both of his hands up and down. The person still marches in place with his left foot and to her peers. The person is resting both of her arms on the table, lifts and bends her left
right foot alternately. The person still repeatedly bends both of his legs alternately. The arm as she touches the table with her left hand. The person is sitting with both legs bent
person still marches in place with his left foot and right foot alternately. and with both feet flat on the floor widely apart.
MotionGPT: The human swings his body to the right and swings back to the left while MotionGPT: The person is still sitting on the chair with a hunched backwhile playing
standing, hunching his back and doing some exercise in the living area with his arcade and eating some chips. The person's both arms are bent forward while holding
colleagues. The human slightly swings both of his arms back and forth on his side. The and sliding the joystick with his left hand to the left then his right hand is on top of the
human raises his right leg to his waist level then stretches and lowers it while his left buttons and clicks them with his right fingers. The person's both legs are still bent while
foot is fixed on the floor. sitting on the chair with both feet flat on the floor and slightly apart.
V2T: The human is standing in the living room while watching the television. The V2T: The human is sittingon the sofa and leaning forward while arranging the chess
person is resting both arms on his sides. The human has both feet fixed on the floor. pieces on the chessboard. The person has both of her arms extended forward while
Ours: The person is swaying her body side to side while exercising in the living area. picking up the chess pieces with her left hand and puts down the chess piece with her
The person repeatedly swings and bends both of her arms in front of her then lowers it right hand on the chess board. The human is sitting with both feet fixed on the floor and
down on her side. The person repeatedly raises both of her feet in front of her then shoulder-width apart.
lowers them down on the floor alternately. Ours: The person is sitting in front of the checkerboard. The person is extending his right
GT: The person is walking in place in front of the laptop. The human repeatedly bends arm toward the checkerboard while keeping his left arm on top of his leg. The human is
both of her arm in front of her them lowers them down on her side. The human bending both of his knees while keeping both of his feet flat on the floor.
repeatedly steps both of her feet alternately. GT:Thehumanis sitting in front of the table as he plays chess. The person is moving the
knight with his right hand while his left hand remains resting on his leg. The human is
bending both of his knees while keeping both of his feet flat on the floor.
TM2T: The person walks towards the cabinet then bends forward to pick up and reach TM2T: The person is still standing straightin front of the table while playing the board
for the clothes. The person extends his right arm to pick up the clothes from the cabinet game with his peer. The person's both arms are still bentforward while both hands are
then bends his left arm to hold the clothes. still holding the edge of the knife.
MotionGPT: The person bends forwardwhile standing in the living room. The person MotionGPT: The human still standsnear the closet. the human still holds the hanger
extends her right arm to open the cabinet and extends her left arm to grab the keys on the with his left hand and his right hand holding the hanger. The person still stands with his
right. The person slightly bends both of her legs then steps her right foot forward while feet slightly apart.
her left foot is fixed on the floor. V2T: The person is standing straight in the living areawith his colleagues while doing
V2T: The human walks towards the couch and bends down while putting down the piece some exercise. The person raises both of his arms straight above his head from the back
of clothing. The person extends both of her arms to pick up and put downthe piece of then lowers them in front and rests them on his side. The person is standing with both feet
clothing with her right hand while holding the clothes with her left hand. The human apart and fixed on the floor.
steps both of her feet forward alternately. Ours: The person is standing in the living area. The human repeatedly swings both of his
Ours:Thehumanwalkstowardsthesofathenslightlyleans forward to put down the arms in front of himand in front of his stomach. The person is standing with both feet
folded piece of clothing. The person extends her right armto put down the folded piece of fixed on the floor.
clothing on the sofa, then extends her left arm to pick upanother piece of clothing on the GT:Thehumanisstandingstillinfrontofhiscolleagues in the living room while
sofa. The human is stepping both of her feet forward alternately then bends both of her playing charades. The person is slightly raising and lowering both of his arms to
legs to support her body. gesticulate. The human is resting both of his legs fixedtothefloor.
GT:Thepersonbendshisbodytogetanotherclothesonthesofa.Thepersonextendshis
rightarmtogettheclotheswithhis right hand then raises his left arm to hold the clothes
with his left hand. The person steps both feet forward towards the sofa.
Figure16: QualitativeResultsofMotionUnderstanding. Weusegreentohighlightcorrectparts
intheanswerswhileredforwrongones.
B.3 MOTIONUNDERSTANDING
We show eight more examples of motion understanding in Fig. 16 and Fig. 17. Similar to the
main paper, we use green to highlight correct parts in the answers and red for wrong answers.
Similar to the observation made in the main paper, even though TM2T (Guo et al., 2022b) and
MotionGPT (Jiang et al., 2024) have access to the full body motion, the generated narrations are
reasonable but completely wrong if consider the environment context. For example, in the upper
right example in Fig. 17, given the simple walking sequence, both TM2T and MotionGPT can
correctly understanding that the person is walking forward. But they all give the wrong answers
abouttheplacesthepersoniswalkingin. Thankstotheegocentricvideos,ourmodelsuccessfully
producesthecorrectdescriptionas“walkingtowardsthebeds”.
24TechnicalReport
TM2T: The person walksout of the bedroom then turns to the leftto enter another TM2T: The person is walkingforward in the pathwaythen she slightly leans forward as
bedroom. The person rests both her arms on her sides. the person is stepping forward she sitson the pathway. The person alternately swings both hands on her sides while
with her right and left legs alternately. The person is stepping forward with her right and both arms hang naturally at her sides.
left legs alternately. The person then steps forward with her right feet. MotionGPT: The human is walkingforward while looking at the office surrounding. The
MotionGPT: The person walksin the garage. the person sways his hands on the sides. human has her both arms swaying them back and forth. The human extends both legs
The human extends both legs forward alternately. forward alternately.
V2T: The human walks towards the bedroom. The human slightly sways her hands on V2T: The person is walking forward towards the bed. the person rests both arms on her
her sides. The human takes four steps towards the bedroom. sides. The person is extending both her legs forward alternately.
Ours: The human walks towards the door. The human puts down her right arm and sways Ours:The human is walking towards the bed. The person is resting both of her arms
both hands on the side. The person extends both legs forward alternately. beside her. The person is extending both of her legs forward alternately.
GT:Thepersonwalkstowardsthedoor.The person walks towards the door. The person GT:Thepersonwalkstowardsthebed.Thepersonslightlyswingsbothofherarmsback
rests his left arm on the sideand he raises his right arm while holding the hanger with his andforth.Thepersonstepsbothofher legs forward alternately.
right hand. The human extends both legs forward alternately.
TM2T: The person walkstowards the doorthen leans forward as he tucks in the chairand TM2T: The person is standingin front of the door. the person is raising his left arm and
stands in front of the door to open it. The person's right arm is swinging back and forth on is resting his right arm on his side. The person bends both of his legs while resting on the
his side while his left arm is bent and his left hand holding the top railings then pushes the floor.
door open with his left hand. MotionGPT: The person standsin the bedroomwhile talking to her colleague. The
MotionGPT: The person is walkingforward towards the shower room, pauses on the human is resting and bending her left arm in front while she lowers down her right hand
shower room and then leans forward to put down the towelon the shower curtain holder. before touching the wall with her right hand. The person stands with both feet fixed on
The person is bending both of his arms and then extends his left arm forward to put down the floor.
the towelon the shower curtain holder. The person is alternately stepping both of his feet V2T: The human is standingstraight while picking a condiment jar in the hanging
forward. cabinet. The human grabs a condiment jar with her right hand and flips up the other
V2T: The person straightens up as she slightly turns to the left while walking towards the condiment jar in front of her with right hand and then she bends and slightly lowers
closet. The person keeps holding the clothes with her bent left arm as she lowers down and down her right arm. The person is standing with both feet fixed on the ground.
slightly raises her right arm and then she bends it back. The person steps both of her feet Ours: The person is standing in front of the hanging cabinet and slightly leaning forward
forward alternately. while picking up a condiment jar. The person is extending her right hand forward, picks
Ours: The human turns clockwise as she walks towards the closet to put the clothes on the up the condiment jar cover then puts it down again on the top of the hanging cabinet
top shelf in the bedroom. The human is holding the clothes hanger with both of her bent while resting her left arm on her side. The human is standing with both of her legs
arms in front of her then she extends her left arm froward and grabs the clothes hangerwith parallel to each other and both of her feet spread slightly apart.
her left hand. The human turns her right foot to the right, steps her left foot forward then GT:Thepersonisstanding on tiptoes while checking inside the cupboard. The human
slightly moves her right foot forward. grabs and places the bottle down on the countertop with her right hand while her left
GT:Thehumanwalkstowards the closet. The human raises his left arm to grab the clothes hand is resting on the countertop. The human is standing on tiptoes with both feet as she
while he holds the hanger with his right hand. The person extends both legs forward reaches inside the cupboard.
alternately.
Figure17: QualitativeResultsofMotionUnderstanding. Weusegreentohighlightcorrectparts
intheanswerswhileredforwrongones.
25TechnicalReport
Motion Prompt #1 Temperature 0.6
Motion Prompt #1 Temperature 1.0
Motion Prompt #1 Temperature 1.4
Motion Prompt #2 Temperature 1.0
Motion Prompt #3 Temperature 1.0
Motion Prompt #4 Temperature 1.0
Figure18: QualitativeResultsofMotionPrediction. Thefirstskeletonsinredareinputmotion
prompts. The following motions are randomly sampled auto-regressively from our motion pre-
trainingnetwork.
B.4 MOTIONPREDICTION
Asaby-productofthesecondstageofourtrainingpipeline,motionpre-training,webuildamotion
predictionnetwork.Givenleadingmotionsastheprompts,ourmodeliscapableofauto-regressively
sample motions that complete the motion prompts. As shown in Fig. 18, the first three samples
show three different samples given the same motion prompt. We can increase the intensity of the
generatedmotionsbyincreasingthetemperature.Thelastthreesamplesshowthreerandomsamples
givenvariousmotionprompts,e.g.,bendingforward,sittingdownandstanding.
26