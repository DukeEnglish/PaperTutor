Valid Conformal Prediction for Dynamic GNNs
EdDavis1 IanGallagher1 DanielJohnLawson1 PatrickRubin-Delanchy2
1UniversityofBristol,U.K. 2TheUniversityofEdinburgh,U.K.
{edward.davis, ian.gallagher, dan.lawson}@bristol.ac.uk,
prubind@ed.ac.uk
Abstract
Graphneuralnetworks(GNNs)arepowerfulblack-boxmodelswhichhaveshown
impressiveempiricalperformance. However,withoutanyformofuncertaintyquan-
tification,itcanbedifficulttotrustsuchmodelsinhigh-riskscenarios. Conformal
predictionaimstoaddressthisproblem,however,anassumptionofexchangeability
isrequiredforitsvaliditywhichhaslimiteditsapplicabilitytostaticgraphsand
transductiveregimes.
Weproposetouseunfolding,whichallowsanyexistingstaticGNNtooutputa
dynamicgraphembeddingwithexchangeabilityproperties. Usingthis,weextend
thevalidityofconformalpredictiontodynamicGNNsinbothtransductiveand
semi-inductiveregimes. Weprovideatheoreticalguaranteeofvalidconformal
prediction in these cases and demonstrate the empirical validity, as well as the
performancegains,ofunfoldedGNNsagainststandardGNNarchitecturesonboth
simulatedandrealdatasets.
1 Introduction
Graphneuralnetworks(GNNs)haveseenformidablesuccessinawidevarietyofapplicationdomains
including prediction of protein structure [1] and molecular properties [2, 3], drug discovery [4],
computervision[5],naturallanguageprocessing[6,7],recommendationsystems[8–10],estimating
timeofarrival(ETA)inserviceslikeGoogleMaps[11],andadvancingmathematics[12]. Theyoften
topleaderboardsinbenchmarksrelatingtomachine-learningongraphs[13],forarangeoftaskssuch
asnode,edge,orgraphpropertyprediction. UsefulresourcesforgettingstartedwithGNNsinclude
theintroductions[14,15]andthePytorchGeometriclibrary[16].
Conformal prediction, advanced by Vovk and co-authors [17], and later expanded on by various
researchersintheStatisticscommunity[18–26],isanincreasinglypopularparadigmforconstructing
provablyvalidpredictionsetsfroma‘black-box’algorithmwithminimalassumptionsandatlow
cost. SeveralrecentpapershavebroughtthisideatoquantifyinguncertaintyinGNNs[27–29].
Therearemanyapplicationsofsignificantsocietalimportance,e.g. cyber-security[30–32],human-
traffickingprevention[33,34],socialmedia&misinformation[35],contact-tracing[36],patientcare
[37,38]wherewewouldliketobeabletouseGNNs,withuncertaintyquantification,ondynamic
graphs.
TheaddedtimedimensionindynamicgraphscausesseriousissueswithexistingGNN+conformal
methodology,broadlyrelatingtode-alignmentbetweenembeddingsacrosstimepoints(seeFigure1),
resultinginlargeorinvalidpredictionsetsinevensimpleandstabledynamicregimes.
Inspiredbyaconcurrentlineofstatisticalresearchonspectralembeddingandtensordecomposition
[39–45],weproposetouseoneoftheunfoldings[46]ofthetensorrepresentationofthedynamic
graphasinputtoastandardGNN,anddemonstratevalidityinbothtransductiveandsemi-inductive
regimes.
Preprint.Underreview.
4202
yaM
92
]LM.tats[
1v03291.5042:viXraRelated work. The only paper we found studying conformal inference on dynamic graphs is
[29],butthepaperconsidersastrictlygrowinggraph,whichisnotwhatwemeanbya‘dynamic
graph’,whereedgesappearanddisappearincomplexandrandomways,asinallofourexamples
andapplicationsalludedtoabove. Ourworknaturallybuildsonearliercontributionscombining
GNNs and conformal inference [27–29] but our theory is distinct from those contributions, with
argumentscloserto[24],andtheuseofunfoldingsforGNNsisnew. Inthedynamicgraphliterature,
unfoldingshaveexclusivelybeenproposedforunsupervisedsettings[43–45,47,48],particularly
spectralembedding,andhaveneverbeenappliedtoconformalinference.
2 Theory&Methods
Problemsetup. Inthispaper,adynamicgraphGisasequenceofT graphsoveragloballydefined
nodeset[n]={1,...,n},representedbyadjacencymatricesA(1),...,A(T). Itmayhelptoassume
theA(t) aren×nbinarysymmetricmatricescorrespondingtoundirectedgraphs,butinfactall
werequireisthattheA(t) havethesamenumberofrows,n ,allowingfordirected,bipartiteand
r
weightedgraphs.
Along with the dynamic graph G, we are given labels for certain nodes at certain points in time,
whichcoulddescribeastateorbehaviour. Ourtaskistopredictthelabelforadifferentnode/time
pair, whoselabelishiddenormissing. Ourpredictionmusttaketheformofasetwhichwecan
guaranteecontainsthetruelabelwithsomepre-specifiedprobability1−α(e.g. 95%).
Wetrackthelabelledandunlabelledpairsinthisproblemusingafixedsequenceν whereforeach
(i,t)∈ν weassumeexistenceof
1. aclasslabelY(t) ∈Y,whichmayormaynotbeobserved,whereY issomediscretesetof
i
cardinalityd;
2. asetofattributes,whichisobserved,andrepresentedbyafeaturevectorX(t) ∈Rp;
i
3. acorrespondingcolumninA(t),denotedA(t).
i
Forapairw =(i,t)∈ν,wewillusethenotationX :=X(t),Y :=Y(t),A :=A(t).
w i w i w i
Transductiveregime. Forsomem<|ν|,themissinglabelismissingcompletelyatrandomamong
thefirstm+1node/timepairsofν.
Semi-inductiveregime. Forsomem<|ν|,themissinglabelcorrespondstoafixedpairamongthe
firstm+1node/timepairsofν.
Letν denotethemissingnode/timepair,ν(training) :={ν ,ℓ>m+1},andν(calibration) :={ν ,ℓ≤
test ℓ ℓ
m+1}\{ν }.
test
Inthetransductiveregime,avalidconfidencesetcanbeconstructedwithnofurtherassumptions
(Algorithm1).Thismayseemsurprisingandcouldbeveryusefulinviewofthefollowingimplication:
thepairsν(training) cancorrespondtohistoricaldatapoints, attimest < T, andthepairs{ν }∪
test
ν(calibration) can correspond to datapoints in the present, T. We only need the missing label to be
uniformlychosenamongthelabelsattimeT. Thesemi-inductiveregimeismorechallenging,and
couldreflectascenariowherewehaveonlyhistoricallabels. Here,wewillneedtoinvokefurther
assumptionssuchassymmetryandexchangeability,familiarintheconformalpredictionliterature.
Proposedapproach. WewillrepurposeastandardGNN,designedforstaticgraphs,intoproducing
dynamicgraphembeddingswithdesirableexchangeabilityproperties. LetG denoteaGNNtaking
asinputanadjacencymatrixAcorrespondingtoanundirectedgraphonN nodes,alongwithnode
attributesandlabels(somemissing),andreturninganembeddingYˆ ∈RN×d. Typically,theithrow
ofYˆ predictsthelabelofnodeithrough
(cid:104) (cid:105)
Pˆ(nodeihaslabelk)= softmax(Yˆ ,...,Yˆ ) .
i1 id
k
InplaceofthisgenericinputA,wewillenterthedilatedunfolding[47]correspondingtothedynamic
graphG,
(cid:20) (cid:21)
0 A
AUNF := ,
A⊤ 0
2where A =
(cid:0) A(1),...,A(T)(cid:1)
(column-concatenation), along with the attributes X ,w ∈ ν and
w
traininglabelsY ,w ∈ν(training).
w
TheresultingoutputfromtheGNNsplitsintoembeddings
(cid:20) XˆUNF(cid:21)
:=G(cid:0) AUNF(cid:1)
,
YˆUNF
whereXˆUNF ∈Rnr×dcontainsglobalrepresentations.TheembeddingYˆUNFcontainsrepresentations
ofnode/timepairs,ofprincipalinteresthere,andweletYˆUNFdenotetherowrepresentingnode/pair
w
w ∈ν.
Finally,weshallrequiresomenon-conformityscorefunctionr :Rd×Y →R,withtheconvention
that r(yˆ,y) is large when yˆis a poor prediction of y. In practice, we make use of the adaptive
non-conformitymeasuresdetailedby[26].
With these ingredients in place, we are in a position to obtain a prediction set Cˆ for ν , the
test
computationofwhichisdetailedinAlgorithm1. Thenotationleavesimplicitthedependenceofthe
setCˆ ontheobserveddata;explicitly,thesetCˆ isadeterministicfunctionofthedynamicgraphG,
theattributes(X ,w ∈ν),andthelabels(Y ,w ∈ν\{ν }).
w w test
Algorithm1Splitconformalinference
Input: DynamicgraphG,node/timepairsν =ν(training)∪ν(calibration)∪{ν },attributes
test
(X ,w ∈ν),labels(Y ,w ∈ν\{ν }),confidencelevelα,non-conformityfunctionr
w w test
1:
LearnYˆUNFbasedonν(training)
2: LetCˆ =Y
3: LetR w =r(Yˆ wUNF,Y w)forw ∈ν(calibration)
4: Let
qˆ=⌊α(m+1)⌋largestvalueinR ,w ∈ν(calibration)
w
5: fory ∈Y do
6: LetR =r(YˆUNF,y)
test νtest
7: RemoveyfromCˆ ifR ≥qˆ
test
8: endfor
Output: PredictionsetCˆ
Theory. Asmentionedearlier,thetransductiveregimerequiresnofurtherassumptions.
Lemma1. Inthetransductiveregime,thepredictionsetoutputbyAlgorithm1isvalid,thatis,
P(Y ∈Cˆ)≥1−α.
νtest
Letν+ =(ν ,ℓ∈[m+1]),comprisingν andν . Forthesemi-inductiveregime,we
calibration ℓ calibration test
makethefollowingassumptions:
A1. The columns A , attributes X , and labels Y corresponding to w ∈ ν+ are jointly
w w w calibration
exchangeable.
Wemakethedefinitionofexchangeabilitypreciseinthesupplementarymaterial. Informally,wecan
swapcolumnsofA=(cid:0) A(1),...,A(T)(cid:1)
correspondingtotestorcalibrationpairs,alongwiththe
attributesandlabels,withoutchangingthelikelihoodofthedata.
A2. TheGNNG islabelequivariant.
Thisdefinitionisagainmadepreciseinthesupplementarymaterial. Informally,ifwere-orderthe
nodes,attributesandlabels,were-ordertheresultingembedding.
Theorem1. Underassumptions1and2,thepredictionsetoutputbyAlgorithm1isvalidinthe
semi-inductiveregime,thatis,
P(Y ∈Cˆ)≥1−α.
νtest
3Discussion. Assumption2isrelativelymildandroughlysatisfiedbystandardGNNarchitectures.
Assumption1ismorechallenging,andcanguideustowardscuratingacalibrationset. Forexample,
ifwehypothesisethatthe(A(t),t∈[T])arei.i.d. stationarystochasticprocesses(overtime),then
i
wecansatisfyAssumption1byensuringν+ containsonlydistinctnodes. Theindependence
calibration
hypothesis will typically break in undirected graphs because of the necessary symmetry of each
A(t),andinthiscasewemayalsowishtoensurethatν+ containsonlydistincttimepoints.
calibration
Ingeneral,forpredictingthelabelofnodesbasedonhistoricalinformation,Assumption1should
makeuswaryofglobaldrift,apoordistributionaloverlapbetweenthetestandcalibrationnodes,and
autocorrelation. Thesemi-inductiveregimeiswhereourtheoryactivelyrequiresadilatedunfolding,
andwherealternativeapproachescanfailcompletely;however,eveninthetransductiveregimewhere
ourtheorycoversalternativeapproaches,thepredictionsetsfromunfoldedGNNs(UGNNs)areoften
smaller.
2.1 AVisualMotivation: i.i.d. DrawsofaStochasticBlockModel
Considerasimpletwo-communitydynamicstochasticblockmodel(DSBM)[49,50]forundirected
graphs. Let
(cid:20) (cid:21)
0.5 0.5
B(1) =B(2) = , (1)
0.5 0.9
bematricesofinter-communityedgeprobabilities,andletτ ∈{1,2}n beacommunityallocation
vector. We then draw each symmetric adjacency matrix point as A(1) i ∼nd Bernoulli(B(1) ) and
ij τi,τj
A(2) i ∼nd Bernoulli(B(2) ),fori≤j.
ij τi,τj
AtypicalwayofapplyingaGNNtoaseriesofgraphsistofirststacktheadjacencymatricesas
blocksonthediagonal[51],
 A(1) 0 
ABD =

...  ,
0 A(T)
fromwhichadynamicgraphembeddingisobtainedthrough
Yˆ =G(cid:0) ABD(cid:1) .
BD
Figure1plotstherepresentations,Yˆ andYˆ ofA(1),A(2)usingaGCN[52]asG. Itisclear
BD UNF
that,despitethefactthatA(1) andA(2) comefromthesamedistribution,theoutputoftheblock
diagonalGCNappearstoencodeachangethatisnotpresent. Werefertothisissueastemporalshift.
Incontrast,theoutputofUGCNisexchangeableovertimeandsofeaturesnotemporalshift.
ThisexamplehighlightstwoadvantagesofunfoldedGNNoverthestandardGNNcase. First,asthere
isnotemporalshift,wewouldexpectittoshowimprovedpredictivepower. Second,exchangeability
overtimeallowsfortheapplicationofconformalinferenceacrosstime.
3 Experiments
WeevaluatetheperformanceofUGNNsusingfourexamples,comprisingsimulatedandrealdata,
summarisedinTable1. Wewillthendelvedeeperintoaparticulardatasettoshowhowvariationin
predictionsetscantellussomethingabouttheunderlyingnetworkdynamics. Figure2displaysthe
numberofedgesineachoftheconsidereddatasetsovertime. TheSBM,schoolandflightdataeach
featureabruptchangesinstructure,whilethetradedataisrelativelysmooth.
SBM.Athree-communityDSBMwithinter-communityedgeprobabilitymatrix
(cid:34) s 0.02 0.02(cid:35)
1
B(t) = 0.02 s 0.02 ,
2
0.02 0.02 s
3
wheres ,s ,ands representwithin-communityconnectionstates. Eachscanbeoneoftwovalues:
1 2 3
0.08or0.16. WesimulateadynamicnetworkoverT =8timepoints,correspondingtothe8=23
4Figure1: UGCNandblockdiagonalGCNrepresentationsofani.i.d. stochasticblockmodelafter
applying PCA. The models were trained with transductive masks. Block diagonal GCN appears
toencodeasignificantchangeovertimedespitetherebeingnone. TheembeddingfromUGCNis
exchangeableovertime,aswouldbeexpected.
SBM School
6000 4000
5500 3500
5000 3000
4500 2500
2000
4000
1500
0 1 2 3 4 5 6 7 10am12pm 2pm 4pm 10am12pm 2pm 4pm
Time Day 1 Day 2
Time
Flight Trade
70000
18000 60000
16000
50000
40000 14000
30000 12000
20000 10000
Jan M 2a 0y 19Sep Jan M 2a 0y 20Sep Jan M 2a 0y 21Sep 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 2006 2008 2010 2012 2014
Time Time
Figure2: Numbersofedgesovertime. TheSchoolandFlightdatashowroughperiodic/seasonal
structure,whichtheTradedatafeaturesdriftwiththenumberofedgesgrowingsmoothlywithtime.
possiblecombinations,drawingeachA(t)fromeachuniqueB(t)asdetailedinSection2.1. Thetask
istopredictthecommunitylabelofeachnode.
School. AdynamicsocialnetworkbetweenpupilsataprimaryschoolinLyon,France[53]. Each
ofthe232pupilsworearadioidentificationdevicesuchthateachinteraction,withitstimestamp,
couldberecorded,formingadynamicnetwork. Aninteractionwasdefinedbycloseproximityfor20
seconds. Thetaskhereistopredicttheclassroomallocationofeachpupil. Thisdatasethastemporal
structureparticularlydistinguishingclasstime,wherepupilsclustertogetherbasedontheirclass
(easier),andlunchtime,wheretheclusterstructurebreaksdown(harder). Thedatacoverstwofull
schooldays,makingitroughlyrepeating.
Flight. TheOpenSkydatasettracksthenumberofflights(edges)betweenairports(nodes)overeach
monthfromthestartof2019totheendof2021[54]. Thetaskistopredictthecountryofagiven
5
segdE
fo
rebmuN
segdE
fo
rebmuN(Europeanonly)airport. Thenetworkexhibitsseasonalandperiodicpatterns,andfeaturesachange
instructurewhentheCOVID-19pandemichitEuropearoundMarch2020.
Trade. AnagriculturaltradenetworkbetweenmembersoftheUnitedNationstrackedyearlybetween
1986and2016[55],whichfeaturesintheTemporalGraphBenchmark[27]. Thenetworkisdirected
and edges are weighted by total trade value. Unlike the other examples, this network exhibits
importantdriftovertime. ThisbehaviourisvisualisedinFigure2. Weconsiderthegoalofpredicting
thetoptradepartnerforeachnationforthenextyear,aslightdeviationfromthebenchmarkwhere
performanceisquantifiedbytheNormalizedDiscountedCumulativeGainofthetop10rankedtrade
partners(lessnaturallyconvertedintoaconformalpredictionproblem).
Dataset Nodes Edges Timepoints Classes Weighted Directed Drift
SBM 300 38,350 8 3 No No No
School 232 53,172 18 10 No No No
Flight 2,646 1,635,070 36 46 Yes No No
Trade 255 468,245 32 163 Yes Yes Yes
Table1: Asummaryofconsidereddatasets.
3.1 ExperimentalSetup
On each dataset, we apply GCN [52] and GAT [56] to the block diagonal and unfolded matrix
structures (referred to as UGCN and UGAT respectively) in both the transductive and the semi-
inductivesettings. Thedatasetsconsidereddonothaveattributes. Inthetransductiveregime,we
randomlyassignnodestotrain,validation,calibrationandtestsetswithratios20/10/35/35,regardless
oftheirtimepointlabel. DuetothehighcomputationalcostoffittingmultipleGNNstoquantify
randomisationerror,wefollow[57]byfittingtheGNN10times,thenconstructing100different
permutationsofcalibrationandtestsetstoallow1,000conformalinferenceinstancespermodeland
dataset,andapplyAlgorithm2.
Forthesemi-inductiveregime,weapplyasimilarapproach,exceptthatwereservethelast35%of
theobservationperiodasthetestset,roundedtouseanintegernumberoftimepointsfortesting
toensureeverynode/timepairinthetestsetisunlabelled. Thetraining,validationandcalibration
setsarethenpickedatrandom,regardlessoftimepointlabel,fromtheremainingdatawithratios
20/10/35. Asthetestsetisfixed,efficiencysavingisnotpossibleandweinsteadrunAlgorithm1
(splitconformal)on50randomdatasplits.
PredictionsetsarecomputedusingtheAdaptivePredictionSets(APS)algorithm[26]inbothregimes.
For each experiment, we return the mean accuracy, coverage and prediction set size across all
conformalrunstoevaluatethepredictivepowerofeachGNN,aswellasitsconformalperformance.
To quantify error, we quote the standard deviation of each metric. We additionally consider an
empirical evaluation of conditional coverage, which is desirable but known to be theoretically
impossible to guarantee [58]. We empirically estimate this property by returning the worst-case
coverageacrosstimeforeachofourexamples[59].
CodetoreproduceexperimentscanbefoundinAppendixA.
3.2 Results
UGNNhashigheraccuracyineverysemi-inductiveexampleandmosttransductiveexamples.
Inthetransductiveregimesuchadvantagesareoftenminor,howeverinthesemi-inductiveregime
UGNNdisplaysamoresignificantadvantage(Table2). BlockGNNisclosetorandomguessingin
thesemi-inductiveSBMandSchoolexamples(having3and10classesrespectively). Incontrast,
UGNNmaintainsastrongaccuracyinbothregimes.
ConformalpredictiononUGNNandblockGNNisempiricallyvalidforeverytransductive
example.Thisconfirms(Lemma1)thatexchangeability/labelequivariancearenotnecessaryforvalid
conformalpredictioninthetransductiveregime,asevenblockGNN(whichhasnoexchangeability
properties)isvalidhere(Table3). However,UGNNachievesvalidcoveragewithsmallerprediction
setsinmostcases,makingitthepreferablemethod(Table4).
6Methods SBM School
Trans. Semi-ind. Trans. Semi-ind.
BlockGCN 0.964±0.011 0.334±0.024 0.856±0.011 0.116±0.011
UGCN 0.980±0.004 0.985±0.003 0.924±0.009 0.915±0.013
BlockGAT 0.916±0.028 0.346±0.024 0.807±0.016 0.107±0.024
UGAT 0.947±0.032 0.969±0.017 0.896±0.016 0.868±0.017
Methods Flight Trade
Trans. Semi-ind. Trans. Semi-ind.
BlockGCN 0.405±0.075 0.121±0.059 0.095±0.017 0.049±0.018
UGCN 0.441±0.069 0.477±0.061 0.082±0.031 0.050±0.017
BlockGAT 0.417±0.031 0.114±0.061 0.125±0.017 0.046±0.015
UGAT 0.408±0.060 0.427±0.061 0.111±0.010 0.048±0.015
Table 2: Accuracy (higher is better) for 2 GNNs (GCN or GAT) under 2 representations (block
diagonaladjacencyorunfolding)for4datasets. Boldvaluesindicatethehighestaccuracyforagiven
GNN/representationpair.
Methods SBM School
Trans. Semi-ind. Trans. Semi-ind.
BlockGCN 0.901±0.014 0.659±0.045 0.901±0.012 0.812±0.033
UGCN 0.901±0.015 0.918±0.025 0.901±0.012 0.924±0.013
BlockGAT 0.901±0.015 0.450±0.154 0.901±0.012 0.662±0.084
UGAT 0.901±0.014 0.914±0.022 0.901±0.012 0.909±0.021
Methods Flight Trade
Trans. Semi-ind. Trans. Semi-ind.
BlockGCN 0.900±0.002 0.853±0.013 0.900±0.009 0.842±0.015
UGCN 0.900±0.002 0.910±0.003 0.900±0.009 0.847±0.017
BlockGAT 0.900±0.002 0.862±0.013 0.900±0.009 0.840±0.021
UGAT 0.900±0.002 0.906±0.002 0.901±0.009 0.854±0.023
Table3: Coverage(targettedto≥0.9)for2GNNs(GCNorGAT)under2representations(block
diagonaladjacencyorunfolding)for4datasets. Boldvaluesindicatevalidcoveragewithtarget≥0.9.
ConformalpredictiononUGNNisempiricallyvalidforeverysemi-inductiveexamplewithout
drift, whileblockGNNisvalidfornone. Inallexamples, exceptforthetradedataset, UGNN
producesvalidconformal(Table3)withsimilarpredictionsetsizestothetransductivecase(Table4).
We include the trade data as an example of where UGNN fails to achieve valid coverage in the
semi-inductiveregime. Thisfailureisduetothedriftpresentinthetradenetwork(Figure2),which
growsovertime(approximatelydoublinginedgesoverthewholeperiod). Therefore,thenetworkat
thestartofthisseriesisnotapproximatelyexchangeablewiththenetworkattheendoftheseries.
Neither method achieves conditional coverage. In Appendix C we show that (as anticipated),
theworst-caseconditionalcoverageacrossalltimepointsforbothmethodsislessthanthetarget
coverage,bothforblockandunfoldedmatricesandbothforGCNandGATs. Inmostsemi-inductive
regimes,UGNNisclosertothistarget,buttheyperformsimilarlyinthetransductiveregimehere.
TheproblemsofdriftandconditionalcoveragearedifficulttohandleinsideanunfoldedGNN,but
wehypothesisethatdownstreammethodscouldbeappliedtoimprovecoverageinthesecases,for
example[24]and[28]. Weleavethisinvestigationforfuturework.
Duetotheunfoldedmatrixhavingdoubletheentriesoftheblockdiagonalmatrix,thecomputation
timesforUGNNwereroughlydoublethatofblockGNN.Themaximumtimetotrainanindividual
modelwasaroundaminuteonanAMDRyzen53600CPUprocessor. Afullrunofexperimentson
thelargestdatasettookaround4.5hours.
3.3 TemporalAnalysis
Thegoalofconformalpredictionistoprovideanotionofuncertaintytoanotherwiseblack-box
model. Asaproblembecomesmoredifficult,thisshouldbereflectedinalargerpredictionsetsizeto
maintaintargetcoverage. Weanalysethisbehaviourbyfocusingontheschooldataexample.
7Methods SBM School
Trans. Semi-ind. Trans. Semi-ind.
BlockGCN 1.258±0.053 1.977±0.138 4.542±0.167 8.079±0.188
UGCN 1.263±0.206 1.097±0.171 2.763±0.311 3.037±0.251
BlockGAT 1.063±0.180 1.320±0.466 3.863±0.813 6.540±0.830
UGAT 1.053±0.249 1.042±0.201 3.552±0.756 4.185±1.228
Methods Flight Trade
Trans. Semi-ind. Trans. Semi-ind.
BlockGCN 24.116±2.711 23.283±2.285 82.556±4.966 80.652±6.709
UGCN 22.369±1.953 23.030±2.115 86.319±7.520 85.006±9.516
BlockGAT 25.173±1.631 24.956±1.977 87.603±6.945 84.708±9.690
UGAT 24.453±2.368 24.451±2.307 92.200±7.364 90.021±11.592
Table 4: Set sizes (lower is better) for 2 GNNs (GCN or GAT) under 2 representations (block
adjacencyorUnfolding)for4datasets. ValuesinboldindicateasmallersetsizeforagivenGNN.
Valuesinitalicsindicateinvalidsets.
Figure 3 compares accuracy of both UGAT and block GAT for each time window of the school
dataset. Duringlunchtime, pupilsarenolongerclusteredintheirclassrooms, makingprediction
of their class more difficult. Figure 3 confirms a significant drop in accuracy for both models at
lunchtimeinthetransductivecase. UGATalsodisplaysthisbehaviourinthesemi-inductivecase,
whileblockGAT’sperformanceisnobetterthanrandomselection.
At these more difficult lunchtime windows, the prediction set sizes increase for both methods in
thetransductivecaseasshowninFigure4. UGATalsodisplaysthisincreaseinthesemi-inductive
regime,whileblockGATdoesnotadapt. Thisdemonstratesthatconformalpredictioncanquantify
classificationdifficultyovertimewhenusingaUGNN.Notethatwhilecomputingaccuracyrequires
theknowledgeoftestlabelstoquantifydifficulty,conformalpredictionofsetsizedoesnot. Inthis
particularexample,bothmethodsmaintaincoverageinthetransductivecase(increasinguncertainty
atlunchtimes),andUGATmaintainscoverageinthesemi-inductivecase. Thestoryissimilarfor
UGCN(AppendixD).
1.2 1.2
Lunchtime Lunchtime Block Diagonal
Unfolded
1.0 1.0
100% Accuracy UGAT has much
higher accuracy
0.8 0.8
0.6 0.6 Lunchtime
Accuracy drops Accuracy drops
at lunchtime at lunchtime
0.4 0.4
0.2 0.2 Block diagonal GAT
Block Diagonal Random choice selects randomly
Unfolded
0.0 0.0
Time Window Time Window
(a)Transductive. (b)Semi-inductive.
Figure3: PredictionaccuracyforeachtimewindowoftheschooldatasetforunfoldedGATand
blockdiagonalGAT.Thepredictiontaskgetsmoredifficultatlunchtime,asshownbythedropin
accuracyofbothmethodsinthetransductivecase. UGAThasmarginallybetterperformanceinthe
transductivecaseandsignificantlybetterperformanceinthesemi-inductivecase.
4 Discussion
ThispaperproposesunfoldedGNNs,whichexhibitexchangeabilitypropertiesallowingconformal
inference in both transductive and semi-inductive regimes. We demonstrate improved predictive
performance,coverage,andpredictionsetsizeacrossarangeofsimulatedandrealdataexamples.
8
ycaruccA
ma01 mp21 1
yaD
mp2 mp4 ma01 mp21 2
yaD
mp2 mp4
ycaruccA
ma01 mp21 1
yaD
mp2 mp4 ma01 mp21 2
yaD
mp2 mp48
Block Diagonal Block Diagonal
8
Unfolded Unfolded
7
Set size increases 7 Block diagonal GAT
6 at lunchtime does not react to
lunchtime
6
5
5
4
4 UGAT set sizes
increase at
3 lunchtime
3
2 Lunchtime
Lunchtime Lunchtime 2
Time Window Time Window
(a)Transductive. (b)Semi-inductive.
Figure4: PredictionsetsizesforeachtimewindowoftheschooldatasetforunfoldedGATandblock
diagonalGAT.Asthepredictiontaskgetsmoredifficultatlunchtimethepredictionsetsizesincrease.
OnlytheUGATsetsizesreacttolunchtimeinthesemi-inductivecase.
1.2 1.2
Block Diagonal UGAT uncertainty
Unfolded UGAT increases
1.0 1.0 maintains
coverage
0.8 0.8
Block diagonal
0.6 0.6 GAT under covers
Uncertainty Uncertainty
increases increases
0.4 0.4 Block diagonal GAT
Lunchtime Lunchtime uncert ia ni cn rt ey a d seoes not
0.2 0.2
Block Diagonal
Lunchtime
Unfolded
0.0 0.0
Time Window Time Window
(a)Transductive. (b)Semi-inductive.
Figure5: CoverageateachtimewindowoftheschooldatasetforunfoldedGATandblockdiagonal
GAT.Bothmethodsmaintaintargetcoverageinthetransductivecase,withuncertaintyincreasingat
themoredifficultlunchtimewindow. UGATalsomaintainstargetcoverageinthesemi-inductive
case,whileblockGATunder-covers.
Asthesuppliedcodewilldemonstrate,wehaveintentionallyavoidedanytypeoffine-tuning,because
thegoalhereislesspredictionaccuracythanuncertaintyquantification. Therearesignificantoppor-
tunitiestoimprovethepredictionperformanceofunfoldedGNNs,suchasemployingarchitectures
bettersuitedtobipartitegraphs.
Therearealsoopportunitiestoimprovethedownstreamtreatmentofembeddings,suchasdeploying
theCF-GNNalgorithm[57]toreducepredictionsetsizes,orweightingcalibrationpointsaccording
totheirexchangeabilitywiththetestpoint[24,28].
Ouruseofunfoldingoriginatesfromabodyofstatisticalliteratureonspectralembeddingandtensor
decomposition,whereitmaybeusefultoobservethatinductiveembeddingisstraightforward: the
singularvectorsprovideaprojectionoperatorwhichcanbeappliedtonewnodes. Beingableto
somehowemulatethisoperationinunfoldedGNNscouldprovideapathtowardsfullyinductive
inference.
References
[1] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ron-
neberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al.
9
eziS
teS
noitciderP
egarevoC
ma01
ma01
mp21
mp21
1
yaD
1
yaD
mp2
mp2
mp4
mp4
ma01
ma01
mp21
mp21
2
yaD
2
yaD
mp2
mp2
mp4
mp4
eziS
teS
noitciderP
egarevoC
ma01
ma01
mp21
mp21
1
yaD
1
yaD
mp2
mp2
mp4
mp4
ma01
ma01
mp21
mp21
2
yaD
2
yaD
mp2
mp2
mp4
mp4Highlyaccurateproteinstructurepredictionwithalphafold. Nature,596(7873):583–589,2021.
[2] DavidKDuvenaud,DougalMaclaurin,JorgeIparraguirre,RafaelBombarell,TimothyHirzel,
Alán Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning
molecularfingerprints. Advancesinneuralinformationprocessingsystems,28,2015.
[3] JustinGilmer,SamuelSSchoenholz,PatrickFRiley,OriolVinyals,andGeorgeEDahl. Neural
message passing for quantum chemistry. In International conference on machine learning,
pages1263–1272.PMLR,2017.
[4] JonathanMStokes,KevinYang,KyleSwanson,WengongJin,AndresCubillos-Ruiz,NinaM
Donghia,CraigRMacNair,ShawnFrench,LindseyACarfrae,ZoharBloom-Ackermann,etal.
Adeeplearningapproachtoantibioticdiscovery. Cell,180(4):688–702,2020.
[5] Paul-EdouardSarlin,DanielDeTone,TomaszMalisiewicz,andAndrewRabinovich. Superglue:
Learning feature matching with graph neural networks. In Proceedings of the IEEE/CVF
conferenceoncomputervisionandpatternrecognition,pages4938–4947,2020.
[6] Hao Peng, Jianxin Li, Yu He, Yaopeng Liu, Mengjiao Bao, Lihong Wang, Yangqiu Song,
andQiangYang. Large-scalehierarchicaltextclassificationwithrecursivelyregularizeddeep
graph-cnn. InProceedingsofthe2018worldwidewebconference,pages1063–1072,2018.
[7] LingfeiWu,YuChen,KaiShen,XiaojieGuo,HanningGao,ShuchengLi,JianPei,BoLong,
et al. Graph neural networks for natural language processing: A survey. Foundations and
Trends®inMachineLearning,16(2):119–328,2023.
[8] RiannevandenBerg,ThomasNKipf,andMaxWelling.Graphconvolutionalmatrixcompletion.
arXivpreprintarXiv:1706.02263,2017.
[9] ShiwenWu,FeiSun,WentaoZhang,XuXie,andBinCui. Graphneuralnetworksinrecom-
mendersystems: asurvey. ACMComputingSurveys,55(5):1–37,2022.
[10] Fedor Borisyuk, Shihai He, Yunbo Ouyang, Morteza Ramezani, Peng Du, Xiaochen Hou,
ChengmingJiang,NitinPasumarthy,PriyaBannur,BirjodhTiwana,etal. Lignn: Graphneural
networksatlinkedin. arXivpreprintarXiv:2402.11139,2024.
[11] Austin Derrow-Pinion, Jennifer She, David Wong, Oliver Lange, Todd Hester, Luis Perez,
MarcNunkesser,SeongjaeLee,XueyingGuo,BrettWiltshire,etal. Etapredictionwithgraph
neuralnetworksingooglemaps. InProceedingsofthe30thACMInternationalConferenceon
Information&KnowledgeManagement,pages3767–3776,2021.
[12] AlexDavies,PetarVelicˇkovic´,LarsBuesing,SamBlackwell,DanielZheng,NenadTomašev,
RichardTanburn,PeterBattaglia,CharlesBlundell,AndrásJuhász,etal. Advancingmathemat-
icsbyguidinghumanintuitionwithai. Nature,600(7887):70–74,2021.
[13] Opengraphbenchmark. https://ogb.stanford.edu/. Accessed: 2024-05-22.
[14] WilliamLHamilton. Graphrepresentationlearning. Morgan&ClaypoolPublishers,2020.
[15] BenjaminSanchez-Lengeling,EmilyReif,AdamPearce,andAlexanderB.Wiltschko. Agentle
introductiontographneuralnetworks. Distill,2021. https://distill.pub/2021/gnn-intro.
[16] Pygdocumentation. https://pytorch-geometric.readthedocs.io/en/latest/. Accessed: 2024-05-22.
[17] VladimirVovk,AlexanderGammerman,andGlennShafer. Algorithmiclearninginarandom
world,volume29. Springer,2005.
[18] Glenn Shafer and Vladimir Vovk. A tutorial on conformal prediction. Journal of Machine
LearningResearch,9(3),2008.
[19] JingLei,JamesRobins,andLarryWasserman. Distribution-freepredictionsets. Journalofthe
AmericanStatisticalAssociation,108(501):278–287,2013.
[20] JingLeiandLarryWasserman.Distribution-freepredictionbandsfornon-parametricregression.
JournaloftheRoyalStatisticalSocietySeriesB:StatisticalMethodology,76(1):71–96,2014.
10[21] Jing Lei, Max G’Sell, Alessandro Rinaldo, Ryan J. Tibshirani, and Larry Wasserman.
Distribution-freepredictiveinferenceforregression. JournaloftheAmericanStatisticalAssoci-
ation,113(523):1094–1111,2018.
[22] RinaFoygelBarber,EmmanuelJCandes,AadityaRamdas,andRyanJTibshirani. Thelimits
ofdistribution-freeconditionalpredictiveinference. InformationandInference: AJournalof
theIMA,10(2):455–482,2021.
[23] RyanJTibshirani,RinaFoygelBarber,EmmanuelCandes,andAadityaRamdas. Conformal
predictionundercovariateshift. Advancesinneuralinformationprocessingsystems,32,2019.
[24] RinaFoygelBarber,EmmanuelJCandes,AadityaRamdas,andRyanJTibshirani. Conformal
predictionbeyondexchangeability. TheAnnalsofStatistics,51(2):816–845,2023.
[25] IsaacGibbs,JohnJCherian,andEmmanuelJCandès. Conformalpredictionwithconditional
guarantees. arXivpreprintarXiv:2305.12616,2023.
[26] YanivRomano,MatteoSesia,andEmmanuelCandes. Classificationwithvalidandadaptive
coverage. AdvancesinNeuralInformationProcessingSystems,33:3581–3591,2020.
[27] ShenyangHuang,FarimahPoursafaei,JacobDanovitch,MatthiasFey,WeihuaHu,Emanuele
Rossi,JureLeskovec,MichaelBronstein,GuillaumeRabusseau,andReihanehRabbany.Tempo-
ralgraphbenchmarkformachinelearningontemporalgraphs. AdvancesinNeuralInformation
ProcessingSystems,36,2024.
[28] Jase Clarkson. Distribution free prediction sets for node classification. In International
ConferenceonMachineLearning,pages6268–6278.PMLR,2023.
[29] SoroushHZargarbashiandAleksandarBojchevski. Conformalinductivegraphneuralnetworks.
InTheTwelfthInternationalConferenceonLearningRepresentations,2023.
[30] TristanBilot,NourElMadhoun,KhaldounAlAgha,andAnisZouaoui. Graphneuralnetworks
forintrusiondetection: Asurvey. IEEEAccess,2023.
[31] HaoyuHe,YuedeJi,andHHowieHuang.Illuminati:Towardsexplaininggraphneuralnetworks
forcybersecurityanalysis. In2022IEEE7thEuropeanSymposiumonSecurityandPrivacy
(EuroS&P),pages74–89.IEEE,2022.
[32] BenjaminBowmanandHHowieHuang. Towardsnext-generationcybersecuritywithgraphai.
ACMSIGOPSOperatingSystemsReview,55(1):61–67,2021.
[33] PedroSzekely,CraigAKnoblock,JasonSlepicka,AndrewPhilpot,AmandeepSingh,Chengye
Yin,DipsyKapoor,PremNatarajan,DanielMarcu,KevinKnight,etal. Buildingandusinga
knowledgegraphtocombathumantrafficking. InTheSemanticWeb-ISWC2015: 14thInterna-
tionalSemanticWebConference,Bethlehem,PA,USA,October11-15,2015,Proceedings,Part
II14,pages205–221.Springer,2015.
[34] PratheekshaNair,JavinLiu,CatalinaVajiac,AndreasOlligschlaeger,DuenHorngChau,Mirela
Cazzolato,CaraJones,ChristosFaloutsos,andReihanehRabbany. T-net: Weaklysupervised
graphlearningforcombattinghumantrafficking. InProceedingsoftheAAAIConferenceon
ArtificialIntelligence,volume38(20),pages22276–22284,2024.
[35] HuyenTrangPhan,NgocThanhNguyen,andDosamHwang. Fakenewsdetection: Asurvey
ofgraphneuralnetworkmethods. AppliedSoftComputing,page110235,2023.
[36] Chee Wei Tan, Pei-Duo Yu, Siya Chen, and H Vincent Poor. Deeptrace: Learning to op-
timize contact tracing in epidemic networks with graph neural networks. arXiv preprint
arXiv:2211.00880,2022.
[37] ZhengLiu,XiaohanLi,HaoPeng,LifangHe,andSYuPhilip. Heterogeneoussimilaritygraph
neuralnetworkonelectronichealthrecords. In2020IEEEinternationalconferenceonbigdata
(bigdata),pages1196–1205.IEEE,2020.
11[38] HeloísaOssBoll,AliAmirahmadi,MirfaridMusavianGhazani,WagnerOuriquedeMorais,
Edison Pignaton de Freitas, Amira Soliman, Kobra Etminani, Stefan Byttner, and Mariana
Recamonde-Mendoza. Graphneuralnetworksforclinicalriskpredictionbasedonelectronic
healthrecords: Asurvey. JournalofBiomedicalInformatics,page104616,2024.
[39] AnruZhangandDongXia. Tensorsvd:Statisticalandcomputationallimits. IEEETransactions
onInformationTheory,64(11):7311–7338,2018.
[40] JoshuaCape,MinhTang,andCareyE.Priebe. Thetwo-to-infinitynormandsingularsubspace
geometrywithapplicationstohigh-dimensionalstatistics. TheAnnalsofStatistics,47(5):2405–
2439,2019.
[41] EmmanuelAbbe,JianqingFan,KaizhengWang,andYiqiaoZhong. Entrywiseeigenvector
analysisofrandommatriceswithlowexpectedrank. Annalsofstatistics,48(3):1452,2020.
[42] PatrickRubin-Delanchy,JoshuaCape,MinhTang,andCareyEPriebe. Astatisticalinterpreta-
tionofspectralembedding: thegeneralisedrandomdotproductgraph. JournaloftheRoyal
StatisticalSocietySeriesB:StatisticalMethodology,84(4):1446–1473,2022.
[43] AndrewJonesandPatrickRubin-Delanchy. Themultilayerrandomdotproductgraph. arXiv
preprintarXiv:2007.10455,2020.
[44] IanGallagher,AndrewJones,andPatrickRubin-Delanchy. Spectralembeddingfordynamic
networkswithstabilityguarantees. InM.Ranzato,A.Beygelzimer,Y.Dauphin,P.S.Liang,and
J.WortmanVaughan,editors,AdvancesinNeuralInformationProcessingSystems,volume34,
pages10158–10170.CurranAssociates,Inc.,2021.
[45] JoshuaAgterbergandAnruZhang. Estimatinghigher-ordermixedmembershipsviatheℓ
2,∞
tensorperturbationbound. arXivpreprintarXiv:2212.08642,2022.
[46] Lieven De Lathauwer, Bart De Moor, and Joos Vandewalle. A multilinear singular value
decomposition. SIAMjournalonMatrixAnalysisandApplications,21(4):1253–1278,2000.
[47] Ed Davis, Ian Gallagher, Daniel John Lawson, and Patrick Rubin-Delanchy. A simple and
powerfulframeworkforstabledynamicnetworkembedding. arXivpreprintarXiv:2311.09251,
2023.
[48] FanWang,WanshanLi,OscarHernanMadridPadilla,YiYu,andAlessandroRinaldo. Multi-
layerrandomdotproductgraphs: Estimationandonlinechangepointdetection. arXivpreprint
arXiv:2306.15286,2023.
[49] TianbaoYang,YunChi,ShenghuoZhu,YihongGong,andRongJin. Detectingcommunities
and their evolutions in dynamic social networks—a bayesian approach. Machine learning,
82:157–189,2011.
[50] Kevin S Xu and Alfred O Hero. Dynamic stochastic blockmodels for time-evolving social
networks. IEEEJournalofSelectedTopicsinSignalProcessing,8(4):552–562,2014.
[51] MatthiasFeyandJanE.Lenssen. FastgraphrepresentationlearningwithPyTorchGeometric.
InICLRWorkshoponRepresentationLearningonGraphsandManifolds,2019.
[52] ThomasNKipfandMaxWelling. Semi-supervisedclassificationwithgraphconvolutional
networks. arXivpreprintarXiv:1609.02907,2016.
[53] JulietteStehlé,NicolasVoirin,AlainBarrat,CiroCattuto,LorenzoIsella,Jean-FrançoisPinton,
MarcoQuaggiotto,WouterVandenBroeck,CorinneRégis,BrunoLina,etal. High-resolution
measurements of face-to-face contact patterns in a primary school. PloS one, 6(8):e23176,
2011.
[54] XavierOlive,MartinStrohmeier,andJannisLübbe. CrowdsourcedairtrafficdatafromThe
OpenSkyNetwork2020[CC-BY],January2022.
[55] GrahamKMacDonald,KateABrauman,ShipengSun,KimberlyMCarlson,EmilySCassidy,
James S Gerber, and Paul C West. Rethinking agricultural trade relationships in an era of
globalization. BioScience,65(3):275–289,2015.
12[56] PetarVelicˇkovic´,GuillemCucurull,ArantxaCasanova,AdrianaRomero,PietroLio,andYoshua
Bengio. Graphattentionnetworks. arXivpreprintarXiv:1710.10903,2017.
[57] KexinHuang,YingJin,EmmanuelCandes,andJureLeskovec. Uncertaintyquantificationover
graphwithconformalizedgraphneuralnetworks. AdvancesinNeuralInformationProcessing
Systems,36,2024.
[58] VladimirVovk. Conditionalvalidityofinductiveconformalpredictors. InAsianconferenceon
machinelearning,pages475–490.PMLR,2012.
[59] AnastasiosNAngelopoulosandStephenBates. Agentleintroductiontoconformalprediction
anddistribution-freeuncertaintyquantification. arXivpreprintarXiv:2107.07511,2021.
A Codeavailability
Pythoncodeforreproducingexperimentsisavailableathttps://github.com/edwarddavis1/
valid_conformal_for_dynamic_gnn.
B SupportingtheoryforSection2
B.1 Transductivetheory
Ourargumentsaresimilarinstyleto[24],showingvalidityoffullconformalinference,andthen
splitconformalasaspecialcaseoffullconformal.
WeobserveY forallw ∈ν apartfromY ,wherethequerypointK ischosenindependentlyand
w νK
uniformlyatrandomamong{1,...,m+1}.
B.1.1 Fullconformalprediction
SupposewehaveaccesstoanalgorithmA(g,x,y)whichtakesasinput:
1. g,adynamicgraphonnnodesandT timepoints;
2. x,a|ν|-longsequenceoffeaturevectors;
3. y,a|ν|-longsequenceoflabels;
andreturnsasequencer =(r ,...,r )ofrealvalues,whichwillactasnon-conformityscores.
1 m+1
Algorithm2Fullconformalinference
Input: DynamicgraphG,indexsetν,attributes(X ,w ∈ν),labels(Y ,w ∈ν\ν ),confidence
w w K
levelα,algorithmA,querypointK
1: LetCˆ =Y
2: fory ∈Y do
3: Let
X =(X ,...,X ); Y+ =(Y ,...,Y ,y,Y ,...,Y ).
ν1 ν|ν| ν1 νK−1 νK+1 ν|ν|
4: Compute
(R ,...,R )=A(G,X,Y+).
1 m+1
5: RemoveyfromCˆ if
R among⌊α(m+1)⌋largestofR ,...,R .
K 1 m+1
6: endfor
Output: PredictionsetCˆ
Lemma2. ThepredictionsetoutputbyAlgorithm2isvalid.
13ind
Proof. BecauseK ∼ uniform([m+1]),theevent
E =“R among⌊α(m+1)⌋largestofR ,...,R ”,
K 1 m+1
occurswithprobability
P(E)≤α.
ButE occursifandonlyifY ̸∈Cˆ. Therefore,
νK
P(Y ∈Cˆ)=1−P(Y ̸∈Cˆ)=1−P(E)≥1−α.
νK νK
Whatthisformalismmakesclearisthatthevalidityoffullconformalinferenceinatransductive
settinghasnothingtodowithexchangeabilityoranyformofsymmetryinA(neitherofwhichwere
assumed).
B.1.2 Splitconformalprediction
Algorithm3Splitconformalinference(abstractversion)
Input: DynamicgraphG,indexsetν,attributes(X ,w ∈ν),labels(Y ,w ∈ν\ν ),confidence
w w K
levelα,predictionfunctionfˆ,non-conformityfunctionr,querypointK
1: LetCˆ =Y
2: LetR i =r(fˆ(ν i),Y νi)fori∈[m+1]\K
3: Let
qˆ=⌊α(m+1)⌋largestvalueinR ,i∈[m+1]\K
i
4: fory ∈Y do
5: LetR K =r(fˆ(ν K),y)
6: RemoveyfromCˆ ifR K ≥qˆ
7: endfor
Output: PredictionsetCˆ
SupposeweconstructAaccordingtoasplittraining/calibrationroutine, asfollows. First, using
g,ν,thefeaturevectors(X ,w ∈ ν),andthelabelsY ,ℓ > m+1,learnapredictionfunction
w νℓ
fˆ:ν →Rd.
Thevectorfˆ(ν )might,forexample,relatetopredictedclassprobabilitiesvia
ℓ
(cid:104) (cid:105)
Pˆ(Y =k)= softmax(fˆ(ν ) ,...,fˆ(ν ) ) ,
νℓ ℓ 1 ℓ d
k
inwhichcasefˆ(ν )isoftenknownasanembedding.
ℓ
Second,foranon-conformityfunctionr :Rd×Y →R,adynamicgraphg,andx,yoflength|ν|,
define
[A(g,x,y)] =r(fˆ(ν ),y ), ℓ∈[m+1].
ℓ ℓ ℓ
ThenAlgorithm2canpotentiallybemademuchmoreefficient,asshowninAlgorithm1. Thisis
nothingbuttheusualsplitconformalinferencealgorithm,wherethelabelsY ,ℓ>m+1arebeing
νℓ
usedfortraining,andY ,ℓ∈[m+1]\K forcalibration.
νℓ
Thussplitconformalinferenceisaspecialcaseoffullconformalinference,inwhichspecialstructure
isimposedonA,andmustthereforeinherititsgeneralvalidity(Lemma1).
B.2 Semi-inductivetheory
Otherwisekeepingthesamesetup,supposeK isnotuniformlychosenbutisinsteadfixed,to(say)
K =1.
Givenapermutationπ :[m+1]→[m+1],wewillwriteπGtodenoteadynamicgraphinwhich
the columns of G corresponding to ν ,...,ν are permuted according to π. More precisely,
1 m+1
14πG is the dynamic graph corresponding to the sequence of adjacency matrices A(1)′,...,A(T)′
obtainedasfollows. First,copyA(1),...,A(T) toA(1)′,...,A(T)′. Next,overwriteanycolumn
ν ∈ ν ,...,ν to A′ = A . If x is a sequence of length at least m + 1, we write
ℓ 1 m+1 νℓ ν π−1(ℓ)
πx to denote the same sequence with its first m+1 elements permuted according to π, that is,
(πx) =x ifℓ≤m+1and(πx) =x otherwise.
ℓ π−1(ℓ) ℓ ℓ
AmorefullystatedversionofAssumption1is
A3. ThecolumnsA ,w ∈ν,attributesX ,w ∈ν,andlabelsY ,w ∈ν arejointlyexchangeable,
w w w
thatis,foranypermutationπ :[m+1]→[m+1],
{πG,π(X ,w ∈ν),π(Y ,w ∈ν)}≜{G,(X ,w ∈ν),(Y ,w ∈ν)}.
w w w w
Thesymbol≜means“equalindistribution”.
A4. ThealgorithmA(g,x,y)issymmetricinitsinput. Foranypermutationπ :ν →ν,
A(πg,πx,πy)=πA(g,x,y)
Theorem2. Underassumptions3and4,thepredictionsetoutputbyAlgorithm2(andsoAlgorithm3)
isvalidevenifK =1deterministically.
Proof. Thecombinationofassumptions3and4ensuresthatR ,...,R areexchangeable,and
1 m+1
thereforetheevent
E =“R among⌊α(m+1)⌋largestofR ,...,R ”,
1 1 m+1
occurswithprobability
P(E)≤α.
ButE occursifandonlyifY ̸∈Cˆ. Therefore,
ν1
P(Y ∈Cˆ)=1−P(Y ̸∈Cˆ)=1−P(E)≥1−α.
ν1 νK
Forapermutationπ : [N] → [N]withassociatedpermutationmatrixΠ,wesaythattheGNNis
labelequivariantif
G(ΠAΠ⊤)=ΠG(A).
TomapTheorem2toTheorem1,weobservethatAssumptions1and3arethesame(thelattera
detailedversionoftheformer);andthatinputtingAUNFtoaGNNsatisfyingAssumption2,along
with the attributes X ,w ∈ ν and training labels Y ,w ∈ ν(training), results in an A satisfying
w w
Assumption4.
C ConditionalCoverage
Table5displaystheworst-casecoverageacrossalltimepointsforbothmethods,whichasexpected
isnotalwaysconditionallyvalid. Theproblemhereissimilartotheproblemmentionedwithdrift.
As no two time points are exactly exchangeable, no two embedded time points will be exactly
exchangeable. Thisthentranslatestovariationincoveragebetweentimepoints. Inthemarginal
coveragecase,variationisfineaslongastheaveragecoverageisontarget. However,underthis
metric,anyvariationcausesadecrease.
15Methods SBM School
Trans. Semi-ind. Trans. Semi-ind.
BlockGCN 0.760±0.067 0.619±0.063 0.746±0.045 0.767±0.040
UGCN 0.790±0.035 0.857±0.026 0.693±0.045 0.710±0.041
BlockGAT 0.774±0.054 0.450±0.154 0.702±0.061 0.616±0.085
UGAT 0.836±0.030 0.879±0.031 0.763±0.048 0.810±0.085
Methods Flight Trade
Trans. Semi-ind. Trans. Semi-ind.
BlockGCN 0.872±0.008 0.849±0.014 0.745±0.043 0.749±0.029
UGCN 0.871±0.007 0.893±0.004 0.738±0.061 0.755±0.037
BlockGAT 0.874±0.006 0.858±0.014 0.763±0.037 0.755±0.032
UGAT 0.878±0.006 0.895±0.004 0.729±0.049 0.765±0.041
Table5: Worst-casecoverageovertimewithtarget≥ 0.90. Valuesinboldindicateclosesttothe
targetforagivenGNN.
D TemporalAnalysisusingGCN
Inthissection,wepresentatemporalanalysisoftheschooldataexampleusingGCNinsteadofGAT
astheGNNmodel. WeseesimilarconclusionstothosestatedinSection3. TheaccuracyofUGCN
isslightlyhigherateverytimepointinthetransductivecaseandmuchhigherinthesemi-inductive
case. BlockGCNisessentiallyguessingrandomlyinthesemi-inductivecase. Thepredictionsets
forUGCNarealwayssmallerthanthoseofblockGCN,withUGCN’ssetsizesalsoreactingtothe
difficultyprobleminthesemi-inductiveregime. However,neithermethodachievesvalidcoverageof
everytimepointinthetransductiveregime,incontrasttoGAT.UGCNalsounder-coversthefirsttest
pointinthesemi-inductivecase(unlikeUGAT),whileblockGCNcontinuallyunder-covershere.
GCN Accuracy GCN Accuracy
1.0 Block Diagonal
1.0
Unfolded
0.8
0.8
0.6
0.6
0.4
0.4
0.2
Block Diagonal
0.2
Unfolded
10am12pm 2pm 4pm 10am12pm 2pm 4pm 10am 12pm 2pm 4pm 10am 12pm 2pm 4pm
Day 1 Day 2 Day 1 Day 2
Time Window Time Window
(a)Transductive. (b)Semi-inductive.
Figure6: PredictionaccuracyforeachtimewindowoftheschooldatasetforunfoldedGCNand
blockdiagonalGCN.Thepredictiontaskgetsmoredifficultatlunchtime,asshownbythedropin
accuracyofbothmethodsinthetransductivecase. UGCNhasmarginallybetterperformanceinthe
transductivecaseandsignificantlybetterperformanceinthesemi-inductivecase.
16
ycaruccA ycaruccAGCN Coverage GCN Coverage
1.00 1.00 Block Diagonal
Unfolded
0.95 0.95
0.90 0.90
0.85 0.85
0.80 0.80
0.75
0.75
0.70
Block Diagonal 0.70
0.65 Unfolded
10am12pm 2pm 4pm 10am12pm 2pm 4pm 10am 12pm 2pm 4pm 10am 12pm 2pm 4pm
Day 1 Day 2 Day 1 Day 2
Time Window Time Window
(a)Transductive. (b)Semi-inductive.
Figure7: CoverageforeachtimewindowoftheschooldatasetforunfoldedGCNandblockdiagonal
GCN.Bothmethodsmaintaintargetcoverageonaverageinthetransductivecase,butnotatevery
pointintime. Inthesemi-inductivecase,blockGCNunder-coverscontinuouslyandUGCNunder-
coversforthefirsttestpoint.
GCN Prediction Set Size GCN Prediction Set Size
Block Diagonal Block Diagonal
8
Unfolded Unfolded
6
7
5
6
4 5
4
3
3
2
2
10am12pm 2pm 4pm 10am12pm 2pm 4pm 10am 12pm 2pm 4pm 10am 12pm 2pm 4pm
Day 1 Day 2 Day 1 Day 2
Time Window Time Window
(a)Transductive. (b)Semi-inductive.
Figure8: PredictionsetsizesforeachtimewindowoftheschooldatasetforunfoldedGCNand
blockdiagonalGCN.Asthepredictiontaskgetsmoredifficultatlunchtimethepredictionsetsizes
increase. OnlytheUGCNsetsizesreacttolunchtimeinthesemi-inductivecase.
17
egarevoC
eziS
teS
noitciderP
egarevoC
eziS
teS
noitciderP