MAP-Neo: Highly Capable and Transparent
Bilingual Large Language Model Series
M-A-P,UniversityofWaterloo,WuhanAIResearch,01.AI
https://map-neo.github.io/
Abstract
LargeLanguageModels(LLMs)havemadegreatstridesinrecentyearstoachieve
unprecedentedperformanceacrossdifferenttasks. However,duetocommercial
interest,themostcompetitivemodelslikeGPT,Gemini,andClaudehavebeen
gated behind proprietary interfaces without disclosing the training details. Re-
cently,manyinstitutionshaveopen-sourcedseveralstrongLLMslikeLLaMA-3,
comparabletoexistingclosed-sourceLLMs. However,onlythemodel’sweights
areprovidedwithmostdetails(e.g.,intermediatecheckpoints,pre-trainingcorpus,
andtrainingcode,etc.) beingundisclosed. ToimprovethetransparencyofLLMs,
theresearchcommunityhasformedtoopen-sourcetrulyopenLLMs(e.g.,Pythia,
Amber,OLMo),wheremoredetails(e.g.,pre-trainingcorpusandtrainingcode)are
beingprovided. Thesemodelshavegreatlyadvancedthescientificstudyofthese
largemodelsincludingtheirstrengths,weaknesses,biasesandrisks. However,we
observethattheexistingtrulyopenLLMsonreasoning,knowledge,andcoding
tasksarestillinferiortoexistingstate-of-the-artLLMswithsimilarmodelsizes.
Tothisend,weopen-sourceMAP-Neo,ahighlycapableandtransparentbilingual
language model with 7B parameters trained from scratch on 4.5T high-quality
tokens. OurMAP-Neoisthefirstfullyopen-sourcedbilingualLLMwithcom-
parableperformancecomparedtoexistingstate-of-the-artLLMs. Moreover,we
open-sourcealldetailstoreproduceourMAP-Neo,wherethecleanedpre-training
corpus, data cleaning pipeline, checkpoints, and well-optimized training/evalu-
ation framework1 are provided. Finally, we hope our MAP-Neo will enhance
andstrengthen theopen researchcommunity andinspire moreinnovationsand
creativitiestofacilitatethefurtherimprovementsofLLMs.
Figure1:MAP-Neoshowsimpressiveperformanceonbase(Left)andchat(Right)modelscompared
tobothpopularopen-weightandrecenttransparentlargelanguagemodelswithsimilarsizes.
1https://github.com/multimodal-art-projection/MAP-NEO
4202
yaM
92
]LC.sc[
1v72391.5042:viXraContents
1 Introduction 4
2 RelatedWorks 5
3 Tokenizer 5
4 MatrixDataPile 6
4.1 Re-processingPipelineforOpenDatasets . . . . . . . . . . . . . . . . . . . . . . 7
4.1.1 Filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4.1.2 Deduplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4.2 CorporaCrawlfromScratchPipeline. . . . . . . . . . . . . . . . . . . . . . . . . 9
4.2.1 Filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
4.2.2 Deduplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4.2.3 SimilarLineDeduplication . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4.3 DocumentConversionPipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4.4 High-QualitySupplementDataCollection . . . . . . . . . . . . . . . . . . . . . . 12
5 Model 13
5.1 ModelArchitecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
5.2 ModelScaleHyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
6 Pre-training 13
6.1 FundamentalPhase: GeneralAbilityAcquisition . . . . . . . . . . . . . . . . . . 14
6.2 DecayPhase: ImprovementandRectification . . . . . . . . . . . . . . . . . . . . 15
7 Alignment 15
7.1 SupervisedFine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
7.1.1 Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
7.1.2 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
7.2 IterativeDPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
8 ScalingLawofMAP-Neo 16
8.1 ProblemDefinition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
8.2 NEOScalingLaw . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
8.3 GeneralizationofNEOScalingLaw . . . . . . . . . . . . . . . . . . . . . . . . . 18
9 Infrastructure 19
10 Evaluations 19
10.1 BaseModelPerformance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
10.1.1 MainResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
210.1.2 Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
10.2 AlignedModelPerformance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
10.2.1 MainResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
10.2.2 Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
11 SocietalImpact 23
12 Conclusion 24
13 ContributionsandAcknowledgments 24
A Appendix 36
A.1 DetailsofHeuristicRulesforEnglishTexts . . . . . . . . . . . . . . . . . . . . . 36
A.2 DetailsofHeuristicRulesforChineseTexts . . . . . . . . . . . . . . . . . . . . . 37
A.3 TrainingFrameworkOverflowDetails . . . . . . . . . . . . . . . . . . . . . . . . 38
A.4 DetailedPromptsinIntermediateCheckpointsEvaluation . . . . . . . . . . . . . . 38
A.5 DetailedResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
A.6 DetailsofOpenSourceDatasetsUsedinPre-training . . . . . . . . . . . . . . . . 46
A.7 DetailedCompressionRate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
A.8 AdditionalExperimentalResultsinScalingLaw . . . . . . . . . . . . . . . . . . . 46
A.9 CompressionRate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
A.10 OCRPostProcessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
31 Introduction
Theadventofgeneralistlargelanguagemodels(LLMs)suchasGPT-4[1],Claude[4],andGem-
ini[80]hassignificantlyexpandedtheboundariesofNaturalLanguageProcessing(NLP)andis
pavingthewaytowardsArtificialGeneralIntelligence(AGI).Thesemodelsexhibituniversalcapabili-
ties,includingcomplexreasoning[116,89],role-playing[107],creativewriting[105],psychological
assessment[112],scientificeducation[18],andmusicgeneration[115,75,29],amongothers. How-
ever,themostadvancedonesremainclosed-sourceduetocommercialinterests[1,4,80]. Inthis
paper,wearguethatopen-sourceandtransparentLLMsareessentialforboththedemocratizationof
LLMsandfurtheracademicresearch,especiallyconsideringthesubstantialresourcesthesemodels
consume.
Previousworkshavereleasednumerousopen-sourceoreventransparentLLMs. Forexample,the
LLaMAseries[101,102,3]releasedtheweights,therebysignificantlyboostingthedevelopmentof
theopen-sourceLLMcommunity. However,theyarenottransparentbecausetheydonotdisclose
the details of their training data. BLOOM [86] trained a multilingual language model with 176
billionparametersandopen-sourceditsmodelweights,intermediatecheckpoints,andtrainingcorpus.
Models like LLM360 [66] and Pythia [9] further provided their training codes, optimizer state
checkpoints,analysiscodes,anddatapipelines.
Thesemodelsmakesignificantcontributionstobuildingtransparentecosystems,yetgenerallylag
behindindustry-levelLLMssuchasLLaMA[3],Mistral[48]andYi[113],etc.OLMo[36]hasmade
agreatstrideinnarrowingthisgapbyimprovingpre-trainingdataanddataprocessingpipelines,and
introducingmoreopen-sourcecomponents,includingtraininglogsandablations. Nonetheless,it
remainslessproficient,especiallyinareaslikecoding(HumanEval[15]),reasoning(MATH[41],
GSM8K[23]),knowledge(MMLU[40]),andmultilingualism(CMMLU[60]).
Toremedytheseissues,weintroduceMAP-Neo,afullyopen-sourceandtransparentbilingualLLM
suitethatachievessuperiorperformancetoclosethegapwithclosed-sourcemodels. Specifically,the
entireworkflowofbuildinganLLMincludes:
1. DataCurationPipeline: Weprovidethecodeforthecurationandcleaningoftrainingdata
(bothEnglishandChinese),includingastableOCRsystem,thedatarecallingmechanism
inDeepSeek-Math[89],theintegrationofpreviousopen-sourcedataprocessingpipelines,
andsupportfordistributeddataprocessingbasedonSpark2,amongothers.
2. Data: Wereleaseourpre-trainingcorpus,namelyMatrixDataPile,alongwiththetraining
dataforsupervisedfine-tuningandalignmenttraining.
3. ModelArchitecture: Weprovidethecodesanddetailsofourmodelingarchitecture.
4. ModelTraining: Weofferthetrainingcodesforourtokenizer,basemodels,instruction-
tunedmodels,andalignedmodels. Additionally,weaddresssomeissuesoftheMegatron-
LMframework3,enhancingitssupportformorerobustandefficientdistributedtraining.
Moreover,weintroducetheNEOScalingLawdesignedtooptimizescalingupLLMsusing
apre-trainingdatasetsourcedfromdiversecorpora.
5. ModelCheckpoints: WenotonlyreleasethefinalmodelsonHuggingFacebutalsomake
theintermediatecheckpointsavailableforreproducibility.
6. Infrastructure: Thisreportdetailstheinfrastructureforstabletraining.
7. Evaluation: Wealsoprovidedetailedevaluationcodesandthoroughevaluationsettingsfor
benchmarkingtheperformanceofLLMs.
8. AnalysisandLessons: Thisreportelaboratesonnumeroustechniquesandrecipes,such
asoptimizationtricksatdifferentphasesofpre-training,andoffersinsightsintobuilding
LLMsthroughrigorousanalysisandablations.
OurworkisamilestonetowardsfullytransparentLLMswithadvancedabilities,evencompetitive
withthetopclosed-sourceLLMs. Notably,ourcontributionisnotjustanovelfoundationalmodelbut
alsoacomprehensivehandbookforbuildingLLMsfromscratch,coveringtheentireworkflow. We
2https://spark.apache.org/
3https://github.com/NVIDIA/Megatron-LM
4believethatourmodelprovidesacriticalreferenceforthecommunity,particularlyfornon-English
regionsoftheworldengagedinLLMresearch.
2 RelatedWorks
Table1: Comparewithotheropen-sourcelargelanguagemodels(LLMs). Allmetricsareobtained
usingthesameevaluationmanner,andthedetailsareshowninTable9. Non-transparentmodelsare
listedabovethedashedline,whilethetransparentLLMsareshownbelow.
Intermediate Pre-training Reproduction DataCleaning
Model C-EVAL MMLU GSM8K HumanEval
Checkpoints Corpus Code Process
Mistral-7B[48] ✗ ✗ ✗ ✗ 47.54 64.04 47.46 28.0
LLaMA2-7B[102] ✗ ✗ ✗ ✗ 32.37 46.80 16.22 13.4
LLaMA3-8B[3] ✗ ✗ ✗ ✗ 49.83 66.52 54.74 33.5
Pythia-6.9B[9] ✓ ✓ ✗ ✗ 24.64 26.39 3.41 9.1
Amber-7B[66] ✓ ✓ ✓ ✗ 23.82 28.07 3.64 13.4
OLMo-7B[36] ✓ ✓ ✓ ✗ 35.21 53.52 28.43 11.6
MAP-Neo-7B ✓ ✓ ✓ ✓ 57.68 58.14 53.68 23.8
Thedevelopmentofopen-sourcelargelanguagemodels(LLMs)ispivotalforadvancingartificial
intelligenceresearchandapplications. Recenteffortsinthisdomainhavebeenfocusedonnotonly
enhancingmodelperformance[48,3]butalsoensuringtransparencyandreproducibility[9,66,36,
128]. Ourmodel,MAP-Neo-7B,emergesasthenewleadinthisevolvinglandscape,asshownin
Table1,whichbalancesperformanceandtransparency.
TheMAP-Neomodelseriesrepresentsastepforwardinemphasizingfulltransparency,aligningit
alongsideothercontemporarymodelssuchasMistral[48],LLaMA3[3],Pythia[9],Amber[66],and
OLMo[36]. Unlikethesemodels,whichoftenlackeitherintermediatecheckpoints,comprehensive
datacleaningprocesses,oraccessiblepre-trainingcorpusandreproductioncode,MAP-Neoexcels
byintegratingalltheseelements. ThiscommitmenttotheopennessofMAP-Neofacilitatesin-depth
analysisandindependentvalidationbytheresearchcommunity.
Performance-wise,MAP-Neo-7Bdemonstratessuperiorcapabilitiesacrossabroadscopeofbench-
marksincludingChineseandEnglishunderstandingonC-EVAL[46]andMMLU[20],mathematical
abilityonGSM8K[23]andMATH[41],andcodeabilityonHumanEval[15].Notably,MAP-Neo-7B
istheonlymodelinourcomparativeanalysistoachieveallchecksintransparency,aswellasthe
highestscoresacrossalltestscomparedwithothertransparentLLMs,underscoringtheeffectiveness
ofthetrainingandthequalityofthedata.
ThemostsimilarworktoMAP-NeoisOLMo[36],whichisthepioneeringworktofullyopen-source
LLMs. However,theirperformanceiscompromisedinseveralaspectslikeknowledge,coding,and
mathematicalreasoning. Moreover,OLMocannothandlelanguagesbeyondEnglish. MAP-Neosets
anewstandardfortransparencyandperformanceinthefieldofopen-sourceLLMs. Byfosteringa
fullytransparentdevelopmentprocess,MAP-Neonotonlyenhancesitsutilityandtrustworthiness
butalsoprovidesavaluableframeworkforfutureresearch,promotingfurtheradvancementsand
collaborativeeffortsinthecommunity.
3 Tokenizer
Wetrainourtokenizerusingthebyte-pairencoding(BPE)algorithm[88]viatheimplementationof
SentencePiece[56]. Thetrainingdataconsistsof50Bsamplesfromthepre-trainingcorpus,andthe
maximumlengthiscutto64K.Weassignhighersamplingweightstocode,math,andhigh-quality
academic data. To balance the computational efficiency and model performance, we propose to
setthevocabularysizeto64000andconstrainthemaxsentence-piecelengthto16toimprovethe
Chineseperformance.
Notably,wesliceallnumbersintoindividualdigitsandfallbackunknownUTF-8characterstobyte
granularity. Wedonotuseanynormalizationstrategyonthetrainingsamplesanddonotadddummy
prefixes. Thecharactercoveragerateissetto0.9999. Particularly,theremoveextrawhitespaces
parameterissettoFalse,whichisturnedonbydefaultintheSentencePieceTrainer. Thissetting
canseverelyimpactcodeperformanceduringpre-training,asnormalcodeindentationistreatedasa
5singlespace. Weencounteredaspecificissueduringtheinitialphaseofourmodel’spre-training.
Initially,wedidnotdisablethe‘removeextrawhitespaces’parameter,whichisenabledbydefault
intheSentencePieceTrainer. Inthetrainingprocess,weobservesteadyimprovementsintheQA
reasoningandmathematicsbenchmarks,butthecodemetricsexhibitfluctuationsanddonotshow
expectedimprovements. Toaddressthisissue,wefixedthisbuginthesecondphaseofourtraining
(§6.2),whichstabilizesandsignificantlyimprovesthecodemetrics. Furthermore,weobservethat
thisissueiswelladdressedinthedecayphasetrainingstagesunderthenewtokenizersettings,where
rapidimprovementsareachieved.
Moreover,wealsoinvestigatethecompressionratesacrossvariouscategoriesofdata,categorizedby
bothlanguage(ChineseandEnglish)anddatasourcequality(high-qualityandweb-sourced)asshown
inTable2.Specifically,first,weobservethatthehigh-qualitydata(HQ)includingcomplexreasoning,
mathematical,andgeneralknowledgetexts,showingdifferentcompressionratesbetweenChinese
(HQ cn) and English (HQ en). The HQ cn category has a compression rate of 1.577, while the
HQ encategoryexhibitedahigherrateof3.311characterspertoken. Second,datasourcedfromthe
web(Web)alsocomprisemorecharactersthanChineseones. Thissuggestsasignificantvariationin
tokenizationefficiencyorcharacterusagebetweenlanguages,possiblyduetothelinguisticstructure
andthetokenizationmethods. Third,itshouldbementionedthatevenwithsimilarcompressionrates,
thesettingsofthetokenizercancausesignificantfluctuationsinthepre-trainingprocess. Therefore,
itremainsnecessarytofurtherinvestigatetokenizationstrategiesforsubsequentusagescenarios.
Table2: AverageCompressionRatesbyCategory. Thesesubsetsarenotuniformlyproportionedin
thetrainingset. AdetaileddistributionisshowninAppendix18.
Code HQ cn HQ en Web cn Web en Others
2.951 1.577 3.311 1.418 3.699 2.558
4 MatrixDataPile
Figure2: StatisticsoftheMatrixPileDataDistribution: Theinnerpiechartrepresentsthelanguage
distribution,whiletheouterloopindicatestheproportionofmeta-categoriesinthecorpus.
Itiswidelyrecognizedthatawell-constructedtrainingcorpusisessentialfortrainingLLMs. The
trainingcorpusservesasthefueldrivingadvancementsinlanguagemodeling,asdemonstratedby
theemergentcapabilitiesofmodelslikeChatGPT,Claude,Gemini,andLlama. However,dueto
intellectual property restrictions, the pre-training data and processing toolkits of these (partially)
proprietaryLLMsarenotdiscloseduponrelease. Althoughtheopen-sourceresearchcommunityhas
madesubstantialeffortstoincreasetransparencyinthecollectionandprocessingpipelineoflanguage
modelpre-trainingdata[9,86,95],thedevelopmentoffullyopen-sourcedLLMsstilllagsbehind
proprietaryLLMstosomeextent,primarilyduetogapsinthequantityandqualityofthedatasets.
Toaddressthepressingneedformorediverseandtransparentdatasetsinlanguagemodeling,we
introduceMatrix,abilingualpre-trainingcorpusof4.5Ttokens. Uponitsrelease,Matrixcouldbethe
largesttransparentLLMpre-trainingcorpustoourbestknowledge. Specifically,Matrixprovidesthe
6detailsofthedatacollectionandprocessingalongwithahigh-performancetoolkit. Additionally,we
designMatrixbasedontheideaofretrieving,filtering,andcleaninghigh-qualitydataundervarious
practicalcircumstances,whicharediscussedasfollows:
• Givenasetofexisting(English)pre-trainingdatasets,howdowere-processandimprove
thequality? §4.1
• Howdoweconstructalarge-scale,topic-comprehensivecorpusfromscratch,ontheless
exploredChinesecontent?§4.2
• Ifwehaveenormousprinteddocuments,howdowebuildanefficientandeffectivesystem
toextractviabletextualcontents? §4.3
• Whenspecifyingadomainofinterest,howdowefindrelevanthigh-qualitydatafromthe
wildofwebcontent? §4.4
The final composition of the corpus is as follows: 52.55% from Common Crawl, 22.29% from
programming code, and the rest from academic papers, books, and other printed materials, as
illustratedinFigure2. Thedetailedmethodologiesforprocessingthesesourcesaredescribedinthe
subsequentsections,andacomprehensiveillustrationofthesourcesisprovidedinTable16.
Table 3: The composition sources of re-processed English web subset. The proportion denotes
dividingthesizeofthecurrentdatasetbythetotalsizeofthewholedataset.
Dataset Parts UTF-8bytes(TB) Availability Proportion(%)
RedPajama-Data-V2[25] HeadandMiddle 200 Public 92.38
Dolma[95] CC 6.4 Public 2.96
Cultrax[72] EN 1.2 Public 0.55
Amber[66] Refined-Web 4.23 Public 1.95
SlimPajama[94] Whole 2.43 Public 1.12
Falcon[74] Whole 1.01 Public 0.47
CultraY[100] EN 1.24 Public 0.57
4.1 Re-processingPipelineforOpenDatasets
Althoughseveralprocessedpre-trainigcorpus(mostlyinEnglish)havebeenreleasedbyprevious
works[95,74],wearguethatthereisstillroomforamoremeticulouslydesignedpipelinetoimprove
theexistingdata. Besides,itshouldbementionedthatexistingLLMscanbeeasilyimprovedby
continuouspre-trainingwithhigh-qualitydata. Therefore,wefurtherre-processtheselectedweb
content-basedcorporatoproducetheEnglishsubsetofMatrixdatamixture. Thesourcecomesfrom
the Head and Middle parts of RedPajama-Data-V2 [25], CC part of Dolma [95], the EN part of
Cultrax[72],theRefined-WebpartofAmber[66],SlimPajama[94]andfalcon[74]. Theprecise
distributionofourEnglishdatasetislistedinTable3. Theprocedureinvolvesfilteringandmulti-step
deduplication. ThediagraminFigure3ashowstheprocessingordersandtheretentionrates.
4.1.1 Filtering
Tofurtherfilterouttherelativelylow-qualitycorpusfromopen-sourcedatasets,weproposetouse
heuristicrulesfortextfiltering. Theserulesaredesignedtoidentifyandremovepoor-qualitydata,
therebypreventingpotentialmodelperformancedegradationcausedbyaflawedpre-trainingcorpus.
Sinceourcompositedatasetismadeupofcorporafrommultiplesources,weadaptwell-designed
cleaningmethods[74,14,76,78]andtailorourrulesforeachonetoensurequalityconsistency.
For the RedPajama-Data-v2 dataset [25], which provides quality annotations for each text, we
integrate our heuristic rules with these annotations to refine data quality evaluation and further
performrandomsamplingonthedatasettoconfirmthethresholdsforeveryrule. Fordatasetslacking
qualityannotations,weapplytheestablishedrulesandthresholdsderivedfromRedPajama-V2,while
customizingthemtoalignwiththeuniquecharacteristicsofeachdataset. Forexample,theDolma
dataset[95]comprisessixsubsets,namelyWikipedia,PeS2o,StackCode,Gutenberg,C4,andCC,
eachwithdifferentdatacharacteristics. Giventheuniquecharacteristicsofeachsubset,weconduct
individualsamplingandevaluationtoensurethatthemodificationsinrulesandthresholdsarealigned
withourfilteringrequirements. Specifically,fortheCCsubset,weadjusttheuniquewordandtext
7lengththresholds. FortheGutenbergsubset,whichpredominantlycontainsbooktexts,weapplyonly
afewrulestoavoidthetime-consumingprocessofexecutingextensiveheuristicchecksonlongtexts.
Thefilteringprocessinvolves: 1)Document-levelandsentence-levelfilteringtoensuretextlength
adequacy,charactermeaningfulness,andconsistency;2)Duplicatetextremoval,includingn-grams
andsentences;3)Sensitivewordchecktoeliminatetextscontaininganytermsfromablacklist.
4.1.2 Deduplication
It has been reported that repetitive text can lead to a decline in model performance [58, 51, 42],
whichmakesdeduplicationacrucialstepincorpusprocessing. Byeliminatingduplicates,wecan
significantlyreducetherateofemittedmemorizationandmakemodeltrainingmoreefficient[58].
Repetitionscanbecategorizedintoexactduplicatesandnearduplicates. Forexactduplicates,we
employ exact document deduplication to remove them. For near duplicates, we utilize Minhash
LSHdeduplicationtoremovethemasmuchaspossible. Inaddition,thereareinstanceswhereparts
ofthetextarecompletelyduplicated,andinthesecases,theMinhashmethodstrugglestoremove
them. Toaddressthis,wehaveadoptedtwomethodsforpartiallyremovingsuchcontent: paragraph
deduplicationandexactsubstringdeduplication.
ExactDocumentDeduplication Exactdocumentdeduplicationisamethodusedtoevaluatean
entiretexttodetermineifitisidenticaltoanother. Ifitisfoundtobeexactlythesame,theduplicate
willberemoved. ForprocessingdatainEnglish,Sparkisemployedtohandlethedataset. Dueto
thevastvolumeofdata,theremaybeissueswithinsufficientmemory. Thesolutiontothisproblem
involvesbatchingthetextdataintoseparatebucketsforstorage. Eachbucket’sdataisthenprocessed
inturntoremoveduplicates.
MinhashLSHDeduplication Minhash[13]isanexcellentmethodforremovingnearduplicates,
especiallyforwebpagedata,andiswidelyusedforsimilaritysearchandduplicatedetectioninlarge
datasets[104,33,37]. Itcanhandleverycommonscenarioswherethetextcontentisessentiallythe
same,butthescatteredtemplateblocksofthewebpagesaredifferent. TheprincipleofMinHashisto
representasetwithsmallerhashvalues,whichcanthenbeusedtoestimatetheJaccardsimilarity[47]
betweentwosets: Jaccard(A,B)=(A∩B)/(A∪B).
MinHashinvolvesusingmultipledistincthashfunctionsthatmapeachelementofasettoalarger
numericaldomain. Foreachset, thesemultiplehashfunctionsareappliedtoallelementswithin
theset,andthesmallesthashvalueproducedbyeachhashfunctionischosenasitsminimumhash
value. Thus,eachsetcanberepresentedbyavectoroftheseminimumhashvalues,formingtheset’s
MinHashsignature. Fortextdata,ann-gramapproachcanbeusedtoconstructaset.
Afterobtainingthesignatureofthetext,Locality-SensitiveHashing(LSH) [35]isemployedtorapidly
identifycandidatesetpairsthatexceedacertainthresholdinJaccardsimilarity. Thisacceleratesthe
searchprocessforsimilaritems. Thespecificapproachdividesthesignatureintoseveralbands,each
containingseveralhashvalues. Anotherhashfunctionisthenusedtomapeachbandtoahashbucket.
Allsetswiththesamebandhasharemappedtothesamehashbucket. Allsetpairsinthesamehash
bucketareconsideredcandidatesimilarpairswithoutfurtherspecificityregardingtheirsimilarity.
Here,weutilize128uniquehashfunctionstoformsignatures,dividedinto9bands,witheachband
containing13hashvalues. Consequently,theJaccardthresholdissetat0.8.
Uponidentifyingsimilarpairs,connectedcomponentsareconstructed. Withineachcomponentof
theconnectedcomponents,onetextisretainedwhiletheothersareeliminated. Forprocessingvast
amountsofdataefficiently,adistributedimplementation[53]basedonmap-reduceisadopted.
ParagraphDeduplication Paragraphdeduplicationinvolvesremovingallduplicateparagraphs
withinatext. AparagraphisdefinedasasectionoftextseparatedbythenewlineUTF-8character
”\n”. Paragraph deduplication is an effective method for removing website navigation headers,
advertisements,andsimilarelements. Sinceparagraphdeduplicationinvolvesdeletingpartsofthe
text,itmaycausesomeinterferencewithcontentanalysis.
Itsconcreteimplementationfirstinvolvessplittingthetextintomultipleparagraphsusingnewline
utf-8character”\n”,witheachparagraphbeingtaggedwithitscorrespondingdocumentidandoffset
inthetext. Then,eachparagraphishashedusingSHA256. Next,thehashvaluesarededuplicated.
Afterdeduplication,thededuplicatedtextisrestoredaccordingtothedocumentIDandoffset.
8ExactSubstringDeduplication Thismethodfollows[58]. Giventhediversityoflanguages,when
thelengthofrepeatedtextissufficientlylong,itishighlylikelythattheyareeitherderivedfromone
anotherorsourcedfromthesamereference. Therefore,whentwotexts,t andt sharesufficiently
i j
alongsubstring,thatista..a+k = tb..b+k,oneofthemisremoved. Fortheselectionofthelength
i j
threshold, we adhere to the setting in [58], choosing k=50. Due to our distributed environment,
the memory of a single node is insufficient to hold all the data. Therefore, we did not adopt the
implementationin[58]. Inourwork,wesegmenteachtextintoslidingwindowsof50characters
with a step size of 1. We then compute the SHA256 hash value for each window along with its
correspondingdocumentIDandoffset. Subsequently,forwindowswithidenticalhashvalues,we
markthemasduplicatesexceptthefirstone. Finally,usingthetextIDandoffset,werestorethe
originalstringsanddecidewhethertodeleteasegmentbasedontheduplicatemarker.
(a)Re-processingretentionratesforthecor- (b)Processingretentionratesforthecorpora
porain§4.1. crawledfromscratchin§4.2.
Figure3: FunnelDiagramforthetwomaindatapipelines. Thedarkerpartofeachrowrepresentsthe
retentionproportionforeachprocessingstepandthelighteroneforthefilteredcorpora.
4.2 CorporaCrawlfromScratchPipeline
Wefurtherprovideapipelinetocrawlandprocessthewebcontentfromscratchandshowcaseit
withtheChineselanguagedata,whichcouldbeastep-by-stepguideforfollow-upresearchtobuild
anewup-to-datecorpus. WetakethecorpusproducedinsuchapipelineastheChinesesubsetof
Matrix,where80.6%isderivedfromtheChinesewebpageswecrawledandothersfromseveral
opendatasets,aslistedinTable4. ThepipelineoverviewanddetailsareillustratedinFigure3b.
Table4: ThecompositionsourcesoftheChinesewebsubset.
Dataset Parts UTF-8bytes(TB) Availability Proportion(%)
CrawledWebData Whole 14.3 Self-constructed 80.60
CCI Whole 0.10 Public 0.59
Chinesewebtext[14] Whole 1.40 Public 7.89
Wanjuan[38] Text 0.57 Public 3.19
Yayi2[69] Whole 0.49 Public 2.76
Cultrax[72] ZH 0.28 Public 1.56
Skypile[109] Whole 0.60 Public 3.41
4.2.1 Filtering
ThefilteringrulesforChinesedatasetsarespecificallytailoredtoaddresstheiruniquechallenges,
differingfromthoseappliedtorelativelywell-processedEnglishdatasetsin§4.1. Consideringthe
largeproportionofHTML-converteddatainChinesedatasets,wefocusintensivelyoneliminating
HTML-relatedartifactsandrectifyingtextualinconsistencies. Furthermore,giventhesignificant
linguisticdifferencesbetweenChineseandEnglish, weconducttargetedsamplingofdocuments
withinChinesedatasets,whichaimstoreassessandadjustthethresholdsanddetailsofourfiltering
9rules,ensuringtheirsuitabilityfortheuniquelanguagecharacteristicsofChinesetext. Forexample,
werefinetherulestodistinguishbetween‘characters’and‘words’inChinesetexts,adaptingthe
tokenizationmethodaccordingly.
OurChinesefilteringstepsaresimilartotherulesadaptedtofilterMassiveAppropriatePre-train
ChineseCorpus(MAP-CC)[30]: 1)Dataformatunificationtoboostprocessingefficiency. 2)URL
removal. Thisstepisconductedintwostages: first,removingtextswithURLslistedinBlacklistT1;
followedbyacomprehensivesweeptoeliminateresidualURLs. 3)Sentence-levelanddocument
filteringtodiscardtextthatisexcessivelybrief,substandard,orlogicallyincoherent. 4). Duplicates
removal,includingn-gramsandsentences.
4.2.2 Deduplication
ThededuplicationofChinesedataincludesExactDocumentDeduplication,MinHashDeduplication,
andSimilarLineDeduplication. DuetodifficultiesindeployingSparkintheenvironmentforpro-
cessingChinese,wehavere-implementedthefirsttwomethods. ForExactDocumentDeduplication,
thereareslightdifferencesfromtheimplementationforEnglish,mainlytosavememory,wherewe
haveadoptedaBloomFilterapproachandsetthefalsepositiverateoftheBloomFilterto0.001.
DiscussionsonExactDocumentandMinHashLSHDeduplicationcanbefoundin§4.1.2.
WedidnotuseExactsubstringdeduplicationbecausewhencrawlingwebpages,itiscommonto
repeatedlycrawlthesamecontentmultipletimesinasignaldocument. Additionally,whenextracting
themaintextfromHTML,thereisoftenalossofoneortwowords. Thecombinationofthesetwo
situationsviolatestheassumptionin[58]that“itisrareforthesameideatobeexpressedidentically
inmultipledocumentsunlessoneexpressionisderivedfromtheother, orbotharequotingfrom
a shared source.” Therefore, after Exact substring deduplication, there will be cases where extra
wordsareretained,greatlyreducingthereadabilityofthetext. Hence,weproposeaSimilarLine
deduplicationmethodtoaddressthisissue.
4.2.3 SimilarLineDeduplication
Toaddressthescenariowhereidenticalcontentappearsmultipletimeswithinatext,adirectmethod
istodividethetextintolinesusingspecificdelimitersandthencomparethesimilaritybetweeneach
line. Iftheyaresimilar,thesubsequentlineisremoved. Thedivisionoflinesincludestheuseofthe
followingdelimiters: “[”,“.”,“!”,“?”,“\”,“......”,“]”. Weuseeditdistancetojudgewhethertwo
linesL andL aresimilarasfollows:
1 2

True min(|L |,|L |)≥15∧editDist(L ,L )<0.1×min(|L |,|L |)
 1 2 1 2 1 2
isSimilar(L ,L )= True min(|L |,|L |)<15∧L =L
1 2 1 2 1 2
False otherwise,
where|L|isthelengthoflineLand“editDist”isshortforeditdistance.
DuetothecomputationalcomplexityofcalculatingeditdistancebeingO(len(L )×len(L )),to
1 2
acceleratethisprocess,weadditionallyproposetwomethodstojudgedissimilarity:
1. Isthelengthdifferencebetweenthetwolinesgreaterthanone-tenthofthelengthofthe
shorterline?
2. Istheratiooftheintersectionofthesetsofcharactersandtheunionofthesetsofcharacters
inL andL lessthanone-third?
1 2
NotethatthefirstmethodhasacomputationalcomplexityofO(1),andthesecondmethodhasa
complexityofO(len(L )+len(L )). Thus,thesemethodscansignificantlyimprovethespeedof
1 2
calculation. Clearly,ifeitheroftheabovetwoquestionsispositive,thelinescannotbeconsidered
similar. Otherwise,wecalculateisSimilar(L ,L )toobtainthesimilaritybetweenL andL .
1 2 1 2
4.3 DocumentConversionPipeline
The documents are usually better formatted, in concentrated topics, and with more consistent
expressionscomparedtonoisywebcontent. However,itseemstobeagoldmineofhigh-quality
corpus except that the golds lie deeply under the digital dirt. Such digital documents are mostly
10Ordering
Content
Blocks
Text/Title...
Image
Blocks
MarkDown
Figure
Post
Table Process
Blocks
Table
Document Layout Detection
Latex
Blocks
Equation
Others Drop
Figure4:Thedocumentconversionframeworkiscomposedofvarioussub-modelsfordifferentparts.
storedasstandardPDFswithdiverselayoutsorscannedimageswithinconsistentquality,makingit
challengingtobuilddatasetsupon. Weobservetwocoreissuesindesigninganeffectiveconversion
pipelinetoextractplaintextfromdocuments:i)analyzinglayoutinformationandidentifyingdifferent
layoutelementsincludingtext,titles,captions,images,tables,andformulas,andii)recognizingthe
relationshipsamongtheselayoutcomponents.
Wesurveytheexistingopen-sourcesolutionsfordocumentconversionandfindsomedistinguished
projects with good performances: PP-StructureV2 [59], Marker4, Vary [108], and Nougat [11].
However,alongwiththeirmerits,eachofthemexhibitslimitationsthatcouldbeaddressedtofurther
enhanceperformance: PP-StructureV2cannotrecognizeLaTeXformatcontentandnecessarypost-
processingstages;MarkerandTexify5supportfewlanguagesanddonotprocessfigureseffectively;
Nougat has limited support for multi-column data and recognized languages; Vary and Vary-toy
requireconsiderable computationalresources. Therefore, weproposea framework consistingof
disentangledprocessingcomponents,allowingustoleveragethestrengthsofthesemodelstogether.
For example, we utilize Marker for enhanced language support and PP-StructureV2 for efficient
layoutparsing. AsillustratedinFig.4,ourdocumentconversionframeworkiscomprisedoffour
parts: LayoutDetection,ElementRecognition,Ordering,andPostProcess. Thedecouplingbetween
each module enhances interpretability and simplifies the upgrade, addition, and replacement of
variouscomponents.
LayoutDetection segmentsthedocumentintomultiplepartssuchasformulas,text,headers,and
footers. ThePipelineemploysalightweighttargetdetectionmodelprovidedbyPP-StructureV2,
whichiscomputationallyefficientandperformsexceptionallywell. Thismodel’sperformanceis
furtherenhancedbyemployingtheFGD(FeatureGradientDescent)algorithm,whichoptimizes
featureextractionformoreaccuratelayoutdetection.
ElementRecognition incorporatesvariousmodelstoidentifydifferentelements. Forformula
recognition,theTrOCRmodeltrainedthroughPix2Textoutperformsotherformularecognitionmod-
elssuchasLatex-OCRandTaxify,supportingrecognitionofformulasembeddedwithinparagraphs
andnon-conventionalformulas,thuseffectivelyaddressingmostformularecognitionscenarios. Text
recognitionemploysPP-OCRv4,TextrecognitionemploysPP-OCRv4,notableforitscompatibility
with multiple computing devices and boasts strong recognition capabilities; approximately one
hundredlanguagerecognitionmodelshavebeenpubliclyreleased,applicabletoabroaderrangeof
documentrecognitiontasks. Figuresaresavedasimagesandinsertedinthesubsequentmerging
phase. TablereconstructionisachievedusingSLANet,whichrepresentstablesinHTMLformat.
Otherregions,suchasheaders,footers,andpagenumbers,arediscardedanddonotproceedtothe
post-processingandreconstructionstages.
4https://github.com/VikParuchuri/marker
5https://github.com/VikParuchuri/texify
11Ordering Indocumentconversiontasks,correctlyhandlingtherelationshipsbetweenblocksisof
paramountimportance. Toacquirehigh-qualityconversiondata,weneedtoproperlyhandlecomplex
layout scenarios such as multi-column and cross-page conditions. In the ordering stage, we use
LayoutLMv3[45]forcolumndetectionandsortingdifferentareasaccordingtospecificrules. This
strategynotonlyenhancestheaccuracyofthetaskbutalsosignificantlyoptimizesthereadability.
Post-processing. The texts extracted by OCR usually could not be directly used and require
additionalprocessingasfollows:
1. Broken-upsentences: Intextextractedfromimages,sentencesmaybefragmentedacross
different lines or pages, resulting in a single sentence being divided into multiple seg-
ments. EffectiveOCRtextextractionnecessitatestheidentificationandrejoiningofthese
fragmentedsentencestoreconstructcoherent,completesentences.
2. Hyphenated words: Certain words may be split into two parts within the text due to
formatting constraints, connected by hyphens (e.g., network-ing). Text extraction must
recognizethesehyphenatedwordsandmergethembackintoasingle,completeword(e.g.,
networking).
3. Broken math formulas: OCRed mathematical formulas in Markdown may experience
issuessuchasmissingelements,incorrectsymbols,orfragmentedexpressions. Toaddress
thisissue,wefine-tunea7-billionparameteropen-sourcepre-trainedlanguagemodelon
supervisedlearningdatapairs(x ,y ). Here,x representstheinstructionfordetectingand
i i i
correctingerrorsinthegiventexts,andy representsthecorrectedoutputtexts. Weadopt
i
vLLMtoenablefasterinferencethroughquantizationandefficientmemorymanagementof
attentionkeysandvaluesusingPagedAttention,amongotheroptimizations. Theprompt
templatesusedforprocessingbothbothlanguagesareprovidedinAppendixA.10.
Byincorporatingthesestrategies,wecansignificantlyimprovethequalityandcoherenceofOCR-ed
texts, mitigating common errors and enhancing the overall readability and usability of extracted
content. WeuseFastDeploy6,ahighlyefficientAIinferencedeploymenttool,asthecodebaseofour
implementation,whichcanfullyexploittheadvantagesofmultithreadingtooptimizeinferencespeed
andcomputationaloverhead. Overall,whilemaintainingperformanceanddeploymentefficiency,
weprovideaframeworkfordocumentconversionthatcoverscomprehensivescenarios,including
recognizinglayoutinformation,supportingtablereconstruction,andformularecognition.
4.4 High-QualitySupplementDataCollection
Inthissection,wepresentourmethodforHigh-QualitySupplementDataCollection,whichapplies
toadiverserangeoftopicsandenhancestherobustnessofdatasets. Inspiredby[89],whichadopts
aniterativepipelinetofacilitatetheacquisitionoflarge-scale,high-qualitydatafromCommonCrawl,
weproposetoselecthigh-qualitydataformathematics,scientificexamsyntheticdata,andwiki-based
contentinourMatrix.
Theproceduralphasesoftheiterativepipelineareenumeratedasfollows:
• SeedDatasetCollection: Collectahigh-qualityseeddatasetforthefieldofinterest,like
mathematics,code,orwiki-basedcontent.
• DomainDefinitionandSampling: Defineadomainasdataentrieswithintheseeddataset
sharingthesamebaseURLandextractsamplesfromeachdomainintheseeddatasetas
positivesamplestoenhanceformatdiversity. Correspondingly,acquireanequalamountof
datafromCommonCrawlasnegativesamples.
• ModelTraining: EmployaFastTextmodel[50]forbinaryclassificationtodiscerndata
relevance to the specified field. Training parameters are set as follows: three epochs, a
learningrateof0.1,anembeddingdimensionof256,andann-gramof3. Themodelis
quantizedtoaugmentoperationalefficiencywithinconstrainedmemorycapacities,reducing
itssizetoapproximately10%ofitsoriginalfootprint.
• DataConfidenceAssessment:UtilizethetrainedFastTextmodeltoestimatetheconfidence
ofCommonCrawldataqualifyingaspositive. Retaindatasequencedfromhighesttolowest
6https://github.com/PaddlePaddle/FastDeploy
12confidence. Tostreamlinetheconfidencesortingprocess,initiallysampleasubsetofdatato
establishaviablethresholdthatbalancesdataexclusionwithretentionneeds.
• DataEvaluation: AssesstheretaineddataviaChatGPT3.5[1],employingtheURLto
determinefieldspecificity. Thisstageaimstomitigatetheincidenceoffalsepositiveswhile
maintainingarequisiterecallrate.
• DataRecallandAnnotation: Revisitdomainswhereover10%ofthedatawasrecognized
asfield-specific. AnnotatethisdatasubsetusingChatGPT3.5[1]viaURL.
• ModelRefinementandIteration: Integrateunconfirmedpositivedatafromprioriterations
into the positive samples to diversify the FastText model’s training base. Subsequently,
initiateanewiterationcyclebeginningfromthetrainingstage.
ThedataselectionforCommonCrawlfocusedontheEnglishcontentoftheRedPajamaV2dataset
[25]. The seed dataset for the mathematics segment is sourced from OpenWebMath [6], while
thesciencesyntheticdatasetisfromspecificdomainssuchasChemrxiv,biorxiv,andproprietary
crawledexercisedatafromopen-sourcedatasets,e.g. wanjuan-exam[38],WebInstruct[117],Web
OfScience[55]. Wikidataisprocureddirectlyfromwikiwebsites.
5 Model
5.1 ModelArchitecture
TheMAP-NeomodelarchitectureisgroundedonthetransformerdecoderasoutlinedbyVaswani
etal.[103]. TheessentialparametersdefiningthisarchitecturearedetailedinTable5. Themodels
aretrainedwithacontextlengthof8192tokens,incorporatingseveralenhancementsproposedafter
theoriginaltransformerconcept. Theseenhancementsarelistedbelow:
Multi-QueryAttention[92]. The7Bmodelvariantemploysmulti-headattention,whereasthe2B
modelcheckpointsimplementmulti-queryattention,usingasinglekey-valueheadconfiguration
(num kv heads = 1). This modification is based on ablation studies indicating that multi-query
attentionisparticularlyeffectiveatmoreminorscales[92].
RoPEEmbeddings[97]. Insteadoftraditionalabsolutepositionalembeddings,weutilizerotary
positionalembeddingsateachlayerandsharetheseembeddingsbetweentheinputsandoutputs,
minimizingtheoverallmodelsize.
RMSNorm. Toensurestabletraining,eachtransformersub-layer—includingboththeattentionand
feedforwardlayers—isnormalizedusingRMSNorm[120].
ActivationFunctionWeuseSwiGLU[93]asouractivationfunction.
5.2 ModelScaleHyperparameters
Inthiswork,wecomparetwodifferentmodelscales: 2Band7Bparameters. Sincethesemodelsare
standarddenseTransformers. ThesemodelsareconstructedusingthehyperparametersinTable5.
Thetwomodelsaretrainedidentically(exceptfortrainingdata)usingthesamevocabularyandbatch
size. Trainingdetailsareshownin§3and§5.1.
Table5: Modelarchitecturedetails. Welistthenumberoflayers,d ,thenumberofattention
model
heads,andattentionheadsize. Thefeed-forwardsized isalways8×d .
ff model
Model #Layers #Heads d #Feedforwarddims #KVheads
model
MAP-Neo2B 18 8 2048 16384 1
MAP-Neo7B 28 16 3072 24576 16
6 Pre-training
Inthepre-trainingprocess,weemployatwo-stagepre-trainingstrategytotraintheMAP-Neomodel.
Thefirststagetermedthefundamentalphase,involvestrainingthemodelonavastcorpusofgeneric
13Table6: Modeltrainingdetails.
Phases LearningRate WeightDecay WarmupRatio Batchsize
Pre-training(FundamentalPhase) 2e-4 0.1 0.0055 1024
Pre-training(DecayPhase) 2e-4 0.1 0 1024
SFT(FundamentalPhase) 2e-5 0 0.05 512
SFT(ChatPhase) 2e-5 0 0.05 512
IterativeDPO 5e-6 0 0.1 256
textstodevelopitsgeneraltextgenerationcapability. Subsequently,duringthedecayphase,wefocus
onenhancingthereliabilityofthemodel’sgeneratedcontentbyincorporatinghigh-qualitydataand
modecodedata.ThedistributionofdatausedacrossdifferentphasesisdepictedinFigure5.Notethat
weincreasethevolumeofcodedatainthedecayphase. Specifically,duringthefundamentalphase,
sinceStackV2[68]wasnotyetavailable,weutilizedStackV1[54]andrepeatedthedatasettwiceto
achieveabalanceddataratio. Inthedecayphase,withthereleaseofStackV2[68],weincorporated
itasthecodecomponentfortraining.Moreover,weperformfurtherdatadistributiontuningincluding
duplicatedhigh-qualitydatasources,suchasbooks,judicialdecisions,andgovernmentreportsfor
training,toimprovethemodel’sperformance. Theopen-sourcedatausedforpre-trainingisshown
inTable16,thedatarepetitiondetailsareshowninTable17andthetraininghyperparametersare
showninTable6.
Figure5: ThedatamixtureratiosinMAP-Neopre-trainingstage. Theleftisthefundamentalphase
andtherightshowsthedecayphase.
6.1 FundamentalPhase: GeneralAbilityAcquisition
Duringthefundamentalphase,weemployatwo-stagelearningratescheduler(LRS)toequipthe
model with a robust capability for general text generation. The LRS is modeled as a piecewise
function,consistingofaninitialwarmupphasewherethelearningratelinearlyascendsfromabase
rate of η = 2×10−5 to peak learning rate η = 2×10−4 over t = 2k steps. This is
a max warmup
followedbyacosinedecayphase,duringwhichtherategraduallydiminishesbacktoη =2×10−5
b
overabout365ksteps. Thelearningratef(t)asafunctionoftimetcanbedelineatedasfollows:
(cid:40) η +(η −η ) t ift≤t
f(t)=
a max a (cid:104)twarm(cid:16)up
(cid:16) (cid:17)(cid:17)(cid:105)
warmup
, (1)
η +(η −η ) 1 1+cos π t−twarmup ift <t≤t
b max b 2 ttotal−twarmup warmup total
wheretisthecurrenttimestep,t denotesthedurationofthewarmupphase,andt represents
warmup total
thetotalnumberoftrainingtimesteps. Thislearningphaseprocessesabout3,726billiontokens,
ensuring the model’s robust training on diverse textual data. This meticulous configuration of
learningratesandextensiveprocessingoptimizetrainingdynamicsandefficiency,fosteringasteady
maturationofthemodel’scapabilities.
146.2 DecayPhase: ImprovementandRectification
Owing to the issue in training tokenizer as claimed in §3, the model encounters test failures in
codegenerationtasks, despiteitsstronglanguageunderstandingcapabilitiesacquiredduringthe
fundamentalphase. Toaddressthisissue,wehaveintroducedanadditionaldecayphasespecifically
designedtoutilizeatokenizerofthefixedversion. Thelearningrateinthisdecayphaseinitiates
at η = 2×10−4 and undergoes exponential decay over t = 148k steps, with a half-life T
c decay
correspondingtohalfthet steps,similartothedecayphaseemployedbyMiniCPM[44],which
decay
canbeformulatedasfollows:
f(t)=η c×0.5Tt ift≤t delay, (2)
where t is the current timestep of the decay phase. This strategic adjustment not only rectifies
theinitialtokenizationflawsbutalsoenhancesthemodel’sperformanceoncodegenerationtasks.
Duringthisphase,themodelprocessesatotalofabout778billiontokens,whichprimarilyconsist
of high-quality instruction data. We also simultaneously increased the proportion of code in the
datafrom14.77%to17.04%. Thisadjustmentsignificantlyenhancestheoverallperformanceofthe
model.Thedeliberateenrichmentofthedatasetwithahigherratioofcode,coupledwithinstructional
inputs,ensuresamorerobustandversatilemodel,adeptattacklingcomplexcodingtasksaswellas
understandingandgeneratingprofessionalresponsesindifferentfields.
7 Alignment
7.1 SupervisedFine-tuning
ToalignwiththehumanbehaviorofLLMs,theinitialstepistoperformSupervisedFine-Tuning
(SFT).OurSFTalsoconsistsoftwophases.Inthefirstphase,wecollectalargeamountofinstruction
datatoenhancethefoundationalabilitiesofLLMs.Inthesecondphase,webuilduponthecapabilities
establishedinthefirstphaseandproposetoimprovethechatabilitiesofMAP-Neo. Thisprocess
finetunesapre-trainedLLMonchat-styledata,includingbothqueriesandresponses. Weillustrate
thedetailsofdataconstructionandtrainingstrategies.
7.1.1 Data
FoundationalPhase: EnhancingInstructionFollowingAbilities Inthefirstphase,ourfocusis
tosignificantlyboostthemodel’sfoundationalabilities(e.g.,codeandmathskills),whereweutilize
over2millioninstructionaldatapointsduringthisphase. Specifically,thefirstphaseincludesthe
entireOpenHermes2.5[99],whereweexcludesegmentsrelatedtotheTheoremQAbenchmark[16]
topreventbenchmarkdataleakage. Additionally,weincorporatethecompleteCode-Feedback [125]
datasetandasubsetofWebInstructSub[117]data.
ChatPhase: EnhancingChatAbilities Inthesecondphase,wefocusonimprovingthemodel’s
chatabilitieswhilemaintainingthefoundationalskillsacquiredinthefirstphase.Forthispurpose,we
collectover100kmulti-turndialoguedatasourcedfromrealuserconversations. Toensurethemodel
retainsitsfoundationalcapabilities,weinclude5kmathandcode-relateddatapointsextractedfrom
thefirstphase. OurexperimentshavedemonstratedthatthisadditionalphaseofSFTsignificantly
booststhemodel’sperformanceonchatbenchmarks,suchasMT-Bench[124]andAlpacaEval[62],
withoutcompromisingitsfoundationalabilities.
By following this two-phase approach, we ensure that our model can not only maintain a strong
foundationinessentialskillsbutalsogeneratenatural,helpful,andcontextuallyaccurateresponses.
7.1.2 Training
Consistentwithpre-training,wealsoapplythenext-tokenpredictionobjectiveasthetrainingtaskfor
SFT.Notethatweapplythelossmasksforthesystemanduserinputs. Themodel’strainingprocess
utilizestheAdamWoptimizerwiththehyperparametersinTable6.
Thesequencelengthislimitedto8192,andthebatchsizeis512. Thetrainingprocessconsistsoftwo
phasesusingthesamehyperparameters. Inthefirstphase,themodelistrainedfor3epochsusing
over2millioninstructionaldatapoints,focusingonenhancingfoundationalabilities. Inthesecond
15phase,themodelistrainedfor1epochusingover100kmulti-turndialoguedatatoenhanceitschat
abilitieswhilemaintainingthefoundationalskillsacquiredinthefirstphase.
7.2 IterativeDPO
DPO DirectPreferenceOptimization(DPO)[77]isastraightforwardandeffectivemethodfor
aligning language models with human feedback. It converts the preference loss [12] into a loss
function over the language model, thereby bypassing the need for explicit reward modeling [12]
andreinforcementlearning[19,87]. Startingwithasupervisedfine-tunedlanguagemodel,denoted
asπ ,DPOcollectsadatasetD = {(x,y ,y )i},whichconsistsofhumanpreferencesbetween
sft w l
tworesponsesgeneratedbyπ : y (preferred)andy (dispreferred)tothesamepromptx. Using
sft w l
thisdataset,DPOparameterizesalanguagemodelπ anddirectlyestimatesitsparametersthrough
θ
maximumlikelihoodestimationonthehumanpreferencedatasetDasfollows:
(cid:20) (cid:18) (cid:19)(cid:21)
π (y |x) π (y |x)
L (π ;π ,D)=−E logσ βlog θ w −βlog θ l . (3)
DPO θ sft (x,yw,yl)∼D π (y |x) π (y |x)
sft w sft l
IterativeDPO. WefollowStorm-7B[64]tousetheIterativeDPO[111]pipelinetodevelopour
chatmodel. Specifically,weemploythreeiterations,witheachiterationconsistingofthreestages: 1)
generatingpairedresponses,2)labelingresponsesusingrewardmodels,and3)trainingtheLLMwith
DPOlossasdescribedinEq.3.WeutilizeNectar7asourpromptdatasetandStarling-RM-34B8[126]
asourrewardmodel. ThismodelisfinetunedfromYi-34B-Chat[113]andgeneratesascalaroutput
foranygivenpromptandresponse. Topreservethemultilingualcapabilitiesofourmodel,wealso
adoptapreferencedataset9inChineseinthe3-rditeration.
We report the length-controlled win rate of AlpacaEval2.0 [32] to demonstrate the performance
progressofourmodelinTable7. Theresultsshowthatperformanceimproveswitheachiteration,
indicatingthatourmodelbecomesincreasinglyalignedwithhumanvalues.
Table7: Thelength-controlledwinrateofMAP-NeoatdifferentiterationsontheAlpacaEval2.0
leaderboard. For“SFT”,wereporttheperformanceofourmodelusingtwo-phaseSFT.
Model SFT Iteration1 Iteration2 Iteration3
LCWinRate(%) 9.77 10.02 15.59 16.65
8 ScalingLawofMAP-Neo
8.1 ProblemDefinition
The scaling laws are capable of predicting training configuration for the training of LLMs. This
principleemphasizestheimportanceoftheratiobetweentheamountoftrainingdataD(measured
in tokens) and the size of the model N (in terms of parameters). In this section, we applied the
ChinchillaLawinEq.4[43],OpenAILawinEq.5[52],aderivationofSymbolicMusicScalinglaw
inEq.6[75]andourproposedmethodonourdatasettofitourmodels,whereA,B,E,α,β,α ,D ,
c c
α ,N anddarehyperparameterstobeoptimized.
N c
A B
L(N,D)= + +E (4)
Nα Dβ
(cid:32)(cid:18) (cid:19)αN (cid:33)αD
L(N,D)= N c αD + D c (5)
N D
d A B
L(N,D)= + + +E. (6)
Nα·Dβ Nα Dβ
7https://huggingface.co/datasets/berkeley-nest/Nectar
8https://huggingface.co/Nexusflow/Starling-RM-34B
9https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data-zh
16250M params MAP-Neo model 460M params MAP-Neo model 980M params MAP-Neo model 7790M params MAP-Neo model
1.5
Loss Value Loss Value Loss Value Loss Value
1.4 Chinchilla Law Chinchilla Law Chinchilla Law Chinchilla Law
NEO Scaling Law NEO Scaling Law NEO Scaling Law NEO Scaling Law
1.3
1.2
1.1
1.0
0.9
0.8
0.7
0.6
0 200 400 600 800 1000 0 200 400 600 800 1000 0 200 400 600 800 1000 0 500 1000 1500 2000 2500 3000
# Tokens (B) # Tokens (B) # Tokens (B) # Tokens (B)
Figure6: Thetraininglossvalueisrepresentedbytheblueline. TheChinchillalawpredictionis
showninyellow,andtheNEOscalinglawpredictionisdepictedingreen. WefittheChinchillalaw
andNEOlawon250M,460M,and980Mandpredictthemodelbehavioronbothtrainingsamples
andsamplesfromthe7Bmodel.
TheoriginalSMSscalinglawintroducestwomodificationstotheChinchillalaw. Thefirstmodi-
ficationaddressestherepetitionoftrainingdata,whichisnotconsideredinourstudy. Thesecond
modificationconcernstheinteractionbetweenthenumberofmodelparameters,N,andthedataset
size,D. Specifically,itpositsthatthelosscurveasafunctionofD,representedas B ,isinfluenced
Dβ
byN. Thisinteractionbetweenthenumberofmodelparametersanddatasetsizeisalsoreflectedin
theOpenAIscalinglaw. However,ourversionofSMSlaw,asdetailedinEq. 6,issimplerandyields
superiorresultscomparedtothecorrespondingmodelintheOpenAIframework.
Themotivationforfittingscalinglawsistooptimizethelossundertheboundsofcomputational
resources. Thisprocessisformalizedasminimizingthevalidationcross-entropylossL,subjectto
constraintsimposedbyavailablecomputationalresources(C),specificallyfloating-pointoperations
persecond(FLOPs),asdenotedbelow:
argminL(N,D) s.t. FLOPs(N,D)=C (7)
N,D
Giventhatourmodelistrainedonalmostnon-repetitiveandhigh-qualitydata,weutilizethetraining
lossinsteadofthevalidationlossforthescalinglawapplication.
8.2 NEOScalingLaw
Wetrainmodelswithsizesof250M,460M,and980Mparametersusing1000Btokensoftraining
data. Thesemodelsarethenusedtopredictthescalinglaw,whichguidesthetrainingofamodelwith
7.8Bparameterson3.07T(3065B)tokensduringphase1. Toevaluatethefitofthescalinglaw,we
employtheHuberloss(δ =1e−3)betweentheactualloglossandthepredictedlogloss,alongwith
theR2valuebetweenthetruelossandpredictedloss. Optimizationofthescalinglawisperformed
usingtheLBFGSalgorithm. ThisapproachisappliedconsistentlyacrosstheChinchillalawand
thesymbolicmusicscalinglaw. Byleveragingthesemethods,weaimtoensuretheaccuracyand
reliabilityofourscalinglawpredictions,enablingefficienttrainingoflarge-scalelanguagemodels.
Figure6illustratesthetraininglossvaluesalongsidetheChinchillalawpredictions. Althoughthe
Chinchillalawfitswell,withthepredictedlosscurvefallingwithinthefluctuationsoftheactual
loss curve, its trend appears flatter compared to the actual loss curve. The actual loss decreases
morerapidlythanpredictedbytheChinchillaformula(i.e. B ),suggestingourdatasetwithdiverse
Dβ
high-qualitycorporacanfurtherdecreasethelossvaluewhenDislarge. Toaddressthisdiscrepancy
betweenChinchillapredictionandobservation,weintroducethefollowingequation,denotedasNEO
scalinglaw,whichincludesoneadditionalregularizationtermlog(D)fordatasetscontainingseveral
trilliontokensacrossvariouscorpora:
A B
L(N,D)= + +E−d·log(D) (8)
Nα Dβ
17
seulaV
ssolNotethatalthoughtheregularizationterm−d·log(D)theoreticallyresultsinnolowerboundon
lossasDapproachesnegativeinfinitysuggestingpotentialimperfectionoftheformula,thevalueof
dtypicallyrangesinourexperimentsbetween1e-2and3e-2. Therefore,foradatasetsizelessthan
hundredsoftrilliontokens,thelossremainswithinareasonablerange.
FromthefollowingTable8,weobservethattheNEOscalinglawequationyieldssignificantlybetter
resultsonthetrainingsetandtestingset.
Table8: ComparisonofparametricfittingonR2andHuberLossofdifferentscalinglaws.
Paramaticfit R2Value(train)↑ HuberLoss(train)↓ R2Value(test)↑ HuberLoss(test)↓
ChinchillaLaw 0.2483 0.1665 0.4308 0.3372
OpenAILaw 0.2268 1.0424 -0.2916 0.6023
SMSLaw 0.2484 0.1665 0.4306 0.3375
NEOScalingLaw 0.7361 0.2961 0.6720 0.2081
UnderthepredictionoftheNEOscalinglawandthecomputationalresourceconstraintof1.5×1023
FLOPs,theoptimalconfigurationistotraina10Bparametermodelwith2.5Ttokens,providinga
predictedlossvalueof0.6597. Toensurecomparabilitywithbaselinemodels,wechoosetokeep
ourmodelsizeat7.8Bparameters,similartotheLlama-basemodel. Thisconfigurationwitha7.8B
parametermodelwith3.07Ttokensrequiresslightlyfewercomputationalresourcesandresultsina
similarpredictionlossvalue(0.6618). Meanwhile,aftertraining,Weobservethattherealtraining
lossinthisconfigurationis0.6591,whichisclosetothepredictionlossvalueanddemonstratesthe
effectivenessoftheNEOscalinglaw.
8.3 GeneralizationofNEOScalingLaw
TheNEOscalinglawcanbeapplicabletoabroaderrangeofmodelsbeyondMAP-Neo. Specifically,
inFigure7,weillustratethefitresultsoftheChinchillascalinglaw(yellowdashedline)andthe
NEOscalinglaw(redsolidline)totheDeepSeekLLM[28]withthe7Band67Bparameters,which
alsopre-trainedonadatasetwithmultiplecorpuraincludingChinese,Englishandcodes.
Weobservethatforthelargestmodelsizes(i.e. MAP-Neo-7BandDeepSeek-67B),thepredictions
of Chinchilla Law tend to underestimate the actual loss when the dataset size (D) is small and
overestimatetheactuallossasmodelparametersandtrainingdatascaleup.Incontrast,ourpredictions
ofourNEOScalingLawproducebetterfittingresultswhencomparedwiththeresultsofChinchilla
LawforMAP-Neo-7BandDeepSeek-67B.
7000M params DeepSeek model 67000M params DeepSeek model
2.5
Loss Value Loss Value
2.4 Chinchilla Law Chinchilla Law
NEO Scaling Law NEO Scaling Law
2.3
2.2
2.1
2.0
1.9
1.8
0 200 400 600 800 1000 1200 1400 1600 0 200 400 600 800 1000 1200 1400 1600
# Tokens (B) # Tokens (B)
Figure7: ThelosscurveofChinchillaLawpredictionandtheNEOScalinglawpredictionforthe
DeepSeekLLM.Weuselossvaluesfromboth7Band67Bforfittingandprediction.
WefurthersuggestNEOScalinglawmightbemoresuitableforthesituationwithalargediverse
pre-trainingdatasetwithmultiplehigh-qualitydatasetsources. FormorediscussiononNEOscaling
lawonothermodels,pleaserefertoAppendixA.8.
18
seulaV
ssol9 Infrastructure
Our advanced infrastructure supports a comprehensive two-part data processing system for pre-
trainingandfine-tuning.
ForEnglishdataprocessing,Spark[118]isusedfordistributedcomputing,andobjectstorageisused
tosavethedata. Eachmachineisconfiguredwitha64-coreCPU,256GBofmemory,and1TBof
localdisk. Thereareatotalof94machines. FortheChinesedataprocessing,thereareatotalof14
machines. Amongthem,6machineshavea96-coreCPUand180GBofmemory,whiletheother8
machineshavea48-coreCPUand190GBofmemory. NetworkFileSystem(NFS)[84]isusedasthe
distributedfilestoragesystem.
Inthepre-trainingstage,theMegatron-Coretoolkitisutilizedforitscapacitytotrainlarge-scale
languagemodels,featuringuptohundredsofbillionsofparameters. Comparedtothetokensper
second(TPS)metric,theusageofMegatron-coreachievesarateof7200TPSwhentraininga7B
model, which surpasses the performance of 6400 TPS observed under the same settings without
employingMegatron-core. Thisisaccomplishedusingbothmodelanddataparallelismtechniques.
Weimplementseveralstrategiestomanageourlargedatasetsandmodelcomplexitieseffectively.
Firstly,weintroduceprogramstoidentifyandtemporarilyremovetaintedcomputingnodesfromthe
resourcepoolduetosoftwareorhardwareerrorsbyautomaticinspection,prediction,andlabeling.
Secondly,wemakemodificationstoMegatron-LMtospecificallypreventoverflowissuesdetailedin
A.3whenprocessinglargedatacorpora. Lastly,weimplementtaskrecoverymechanismsthatutilize
strategically selected checkpoint iterations to safeguard against potential failures during training.
Theseenhancementsensureoptimalperformanceandreliabilityinourlarge-scaletrainingoperations.
Ourinfrastructureutilizesdistributedcomputingtechniquestooptimizethetrainingofourmodels.
Specifically,our7BmodelistrainedusinganH800configurationwith512GPUsacross64nodesand
employsNCCLforbackenddistributionwithibpasthenetworkinterfaceandmlx5ofInfiniBand
hardwaretoenhanceinter-GPUcommunication. Tensormodelparallelismisconfiguredtoutilize
2 GPUs, distributing the execution of a single transformer module across these units to enhance
efficiency.Forour2Bmodels,weutilizeall256GPUswithtensormodelparallelismsetto1toensure
effectivedatareplication. Wefurtheramplifyscalabilityandefficiencybyemployingtechniques
similartoZeRO-1forshardingtheoptimizerstate. Thisapproachenablesthemanagementofmore
extensivedatasetsandmorecomplexmodeltrainingwithsignificantlyreducedmemoryoverhead.
OurclusterconsistsofmachineswithdualIntelXeonCPUsandeightNVIDIAH800GPUs. The
architecturefacilitateshigh-speeddatatransfer,witheachCPUsocketinterfacingwithtwoPCIe
Gen4x16lanesconnectedtodedicatedPCIeswitches. Theseswitchesmanagetheconnectionsto
alocalNVMeSSD,anRDMA-capableNetworkInterfaceCard(NIC),andtwoGPUs. Inter-CPU
communication is facilitated by Intel’s Ultra Path Interconnect (UPI), with both CPUs linked to
adual-portTCPNICsupporting100Gbps. Eachmachine’snetworkconfigurationincludesfour
RDMANICs,eachoffering200GbpsoffullduplexbandwidthandintegratedGPUDirectRDMA
capabilities. Notably,theGPUarrayisinterconnectedthroughfourNVIDIANVSwitches,enabling
robust intra-GPU communication with a bandwidth of 400 Gbps. This advanced configuration
underscoresthecluster’scapabilitytohandlelarge-scalemodeltrainingwithexceptionalefficiency
andspeed.
Regardingtheinter-machineconnectionsofourdatacenter,weimplementadual-layerClosnetwork
architecturewhereineachminipodaccommodatesatleast512A100serversinterconnectedviaa
high-speedRDMAnetwork. Withinthisarchitecture,eachS0switchisequippedwith64ports,each
supportingabandwidthof400Gbps. Thisarrangementensuresanetworkconvergenceratioof1:1,a
criticalfactorinmaintainingoptimaldataflowandreducingbottlenecks. Connectivitywithinthis
structureismeticulouslyorganizedsuchthateverytwoS0switchesserve32servers,withatotalof
32S0switchesnetworkingwithineachminipod. Thissetupexemplifiesanadvancedimplementation
designedtomaximizethroughputandminimizelatencyindatacenterenvironments.
10 Evaluations
ThethoroughevaluationdemonstratesthattheMAP-Neomodelfamilyachievesinspiringperfor-
mancebothonautomaticbenchmarksofbasemodelsandchatmodels. Comparedtotheprevious
transparentLLMseries,weunderlineMAP-Neo’sdistinctiveperformanceoncode,math,andin-
19Table9: Performancecomparisonofvariousbasemodelsondifferentbenchmarks. Thebestresults
arein blue,thesecond-bestresultsareunderline,andthethird-bestresultsarein fbox .
Dataset LLama3-8B Mistral-7B LLama2-7B Amber-7B OLMo-7B OLMo-1.7-7B Pythia-6.9B Pythia-12B MAP-Neo-7B
StandardBenchmarks
BoolQ 66.82 64.1 70.67 63.52 68.41 70.49 62.45 61.07 81.07
PIQA 81.12 81.18 78.18 76.82 79 80.25 75.52 76.17 76.55
SIQA 47.34 47.13 45.50 42.89 44.11 54.71 42.32 44.32 68.22
HellaSwag 74.52 76.49 71.27 66.76 70.32 72.37 59.6 63.04 70.74
WinoGrande 72.38 75.3 69.53 64.64 66.54 69.22 60.85 63.69 59.83
ARC-c 79.66 71.53 35.93 24.41 24.07 49.83 22.71 25.08 68.14
OpenBookQA-Fact 69.0 81.0 42.60 26.6 24.6 64.4 25 28.6 82.0
CommonsensQA 69.7 67.57 66.50 57 60.44 69.04 55.45 54.79 69.94
MMLU-AVG 66.52 64.04 46.80 28.07 28.51 53.52 26.39 27.06 58.14
*-humanities 70.41 68.04 51.47 30.17 25.52 55.03 26.87 27.39 60.7
*-stem 56.22 53.21 38.02 27.66 28.68 44.17 26.77 28.13 49.84
*-social-science 76.0 73.65 52.20 27.18 30.05 62.19 24.32 26.26 66.78
*-other 68.94 67.0 49.99 27.37 29.86 57.67 27.25 25.91 59.73
CodeGeneration
Humaneval 33.5 28.0 13.4 13.4 11.6 17.1 9.1 8.5 23.8
Humaneval-Plus 29.3 23.2 11.6 12.2 9.8 15.2 8.5 7.3 20.1
MBPP 61.4 46.8 29.1 22.8 27 32.3 16.1 15.6 34.9
MBPP-Plus 51.6 38.9 22.8 18.5 21.2 25.7 13.2 11.1 29.9
WorldKnowledge
NQ 10.14 9.31 5.07 3.1 0.66 1.02 0.86 1.83 9.97
TriviaQA 51.94 56.47 52.44 26.65 31.97 45.16 16.97 24.31 42.36
ReadingComprehension
SQuAD2.0 40.88 12.53 41.32 31.15 27.05 30.43 22.54 23.11 30.98
Exams
MATH 20.76 15.74 6.14 3.88 1.6 4.86 3.82 4.54 20.7
GSM8K 54.74 47.46 16.22 3.64 5.84 28.43 3.41 3.94 53.68
Chinese
C-EVAL-AVG 49.83 47.54 32.37 23.82 27.39 35.21 24.64 24.82 57.68
*-stem 45.26 44.74 28.28 22.36 25.75 32.36 23.94 27.27 50.35
*-social-science 58.09 54.8 39.22 25.95 31.87 40.43 26.34 23.78 70.23
*-humanities 50.6 51.52 37.11 21.19 26.29 35.5 21.7 20.05 63.49
*-other 49.84 42.06 28.84 27.16 27.4 35.36 27.28 26.08 53.78
*-hard 32.41 33.97 25.21 19.63 27.12 29.16 22.99 27.05 41.07
CMMLU-AVG 50.72 44.63 31.85 25.77 25.53 36.74 25.34 24.88 55.1
*-humanities 53.1 44.59 32.50 24.86 26.65 37.04 25.81 25.41 62.24
*-stem 43.59 37.82 29.05 25.61 25.24 31.94 24.29 23.7 45.62
*-social-science 52.59 46.37 32.60 25.83 25.17 38.14 25.78 25.17 59.39
*-other 53.98 49.83 33.35 26.65 25.43 39.88 25.47 25.33 53.39
*-china-specific 44.81 40.84 29.27 24.96 24.97 34.91 26.5 25.34 55.84
struction following abilities, which not only endows the MAP-Neo with academic and practical
value.
10.1 BaseModelPerformance
10.1.1 MainResults
Wepresenttheresultsofourbasemodelscomparedtoseveralwell-knownLLMs,e.g. LLama3-8B
andMistral-7B,acrossstandardacademicbenchmarks. Allourevaluationmetricsarederivedfrom
20Table10: Performancecomparisonofvariousalignedmodelsondifferentbenchmarks. Thebest
resultsarein blue,thesecond-bestresultsareunderline,andthethird-bestresultsarein fbox .
LLama-3-8B Mistral-7B LLama-2-7B Amber-7B OLMo-7B MAP-Neo-7B MAP-Neo-7B
Dataset
(Instruct) (Instruct-v0.2) (Chat) (Chat) (Instruct) (SFT) (Instruct)
ChatBenchmarks
AlignBench 6.17 5.27 4.33 2.85 3.2 4.63 5.04
AlpacaEval 22.9 17.1 5.4 1.21 3.64 9.77 16.65
Arena-Hard 20.6 12.6 4.6 1.2 1.7 10 11.5
CHC-Bench 5.53 6.86 4.7 3.13 3.91 6.14 7.42
MT-Bench 8.1 7.5 6.6 5.2 5.3 7.1 7.1
StandardBenchmarks
BoolQ 75.05 82.87 74.77 66.51 72.2 84.59 81.28
PIQA 80.09 82.43 76.01 77.48 75.3 76.06 75.24
SIQA 51.23 50.41 48.72 44.88 48.41 51.69 52.25
HellaSwag 71.39 80.11 71.32 67.84 75.18 68.5 68.7
WinoGrande 71.9 73.4 68.35 64.96 66.69 65.19 66.06
ARC-c 81.36 73.56 55.59 37.29 57.63 80 80
OpenBookQA-Fact 87 85.4 74.4 36.6 74 85.4 85.4
CommonsenseQA 73.55 75.84 70.11 60.28 63.47 68.39 70.35
MMLU-Pro 38.12 30.86 21.61 14.65 16.27 28.08 28.74
MMLU 67.1 60.81 48.22 38.8 47.47 58.28 58.28
*-humanities 70.67 66.58 52.71 39.19 48.33 60.4 60.85
*-stem 56.97 50.01 37.98 33.78 38 51.86 52.29
*-social-science 76.9 69.75 55.81 42.85 56.57 66.19 65.6
*-other 69.3 62.55 51.69 42.03 52.06 58.26 57.68
CodeGeneration
HumanEval 48.8 42.1 14 17.7 14.63 34.1 45.1
HumanEval-Plus 44.5 36.0 12.2 14 12.8 31.7 37.8
MBPP 70.1 39.7 29.1 28.0 20.1 44.4 44.4
MBPP-Plus 59.3 33.3 22.8 23.5 16.7 38.1 36
WorldKnowledge
NQ 8.25 1.14 1.5 3.02 0.53 3.8 2.41
Triviaqa 56.32 45.06 46.79 30.95 27.91 38.77 27.09
ReadingComprehension
SQuAD2.0 66.99 15.01 19.61 13.12 42.13 44.58 25.2
Exams
MATH 29.28 13.14 6.9 4.2 1.8 35.36 35.58
GSM8K 79.23 49.2 26 7.59 13.5 72.02 73.16
Chinese
C-Eval 50.76 43.72 35.67 26.29 35.18 55.42 56.97
*-stem 47.47 41.35 32.59 23.99 31.43 47.37 49.08
*-social-science 57.05 47.75 40.04 26.77 42.13 69.21 70.75
*-humanities 48.32 47.33 36.96 28.26 34.03 63.17 63.14
*-other 53.48 40.74 36.01 28.06 36.81 49.78 52.63
*-hard 31.04 27.32 28.45 22.77 26.33 38.41 39.55
CMMLU 51.68 42.67 33.9 30.09 35.55 55.27 55.01
*-humanities 52.55 42.01 35.45 30.48 34.78 63.4 62.99
*-stem 44.09 36.82 29.33 26.76 30.36 47.29 46.69
*-social-science 53.02 44.41 34.55 30.97 38.04 57.55 57.79
*-other 57.58 47.3 36.77 32.25 38.45 53.93 53.44
*-china-specific 45.86 39.22 32.64 28.38 33.97 55.69 55.9
21ourassessments,ensuringconsistencyandtransparency. Wedonotperformanypost-processingon
theevaluationcontent,maintainingtheintegrityoftherawoutputs.
OurevaluationspansacomprehensivesuiteofpublicbenchmarksinbothEnglishandChinese,lever-
aginganinternalevaluationframeworkdesignedforrigorousassessment. Thesebenchmarksinclude
adiverserangeofdatasetscateringtomultipledisciplinesandaspectsoflanguageunderstanding
andreasoning. Ourevaluationstrategyencompassesvariousmetrics,includinglanguagemodeling,
specializedknowledge,andcodegeneration. Fordatasetsrequiringmultiple-choiceselection,we
employaperplexity-basedevaluation. Forgeneration-baseddatasets,wegeneratefreetextandparse
theresultsaccordingly. Thedetailedresultsofourcomparisonwithotherbasemodelsareshownin
Table9.
Standard Benchmarks We include Boolean Questions(BoolQ) [21], Physical Interaction
QA(PIQA) [10], Social Interaction QA(SIQA) [85], HellaSwag [119], WinoGrande [83], ARC-
Challenge(ARC-c)[22],OpenBookQA-Fact[70],CommonsenseQA[98],andMMLU[40]toassess
generalreasoningcapabilities. Allthesebenchmarksaretestedwitha0-shotconfiguration,except
forMMLU,whichisevaluatedwitha5-shotsetup.
Code Generation We report the pass@1 scores of the evaluated models on HumanEval [15],
HumanEval-Plus,MBPP[5],andMBPP-Plus,allwitha0-shotconfiguration,followingtheEvalPlus
framework[63].
World Knowledge We include NaturalQuestions(NQ) [57] and TriviaQA [49] to assess world
knowledge. Bothbenchmarksaretestedwitha0-shotconfiguration.
ReadingComprehensionWereportthe0-shotaverageonSQuAD2.0[79].
ExamsWereporttheaveragescoresforMATH[41]andGSM8K[23],bothwitha4-shotconfigura-
tion. ForGSM8K,weemployasimpleChain-of-Thoughtpromptingstrategy: ”Let’sthinkstepby
step.”Forbothdatasets,weusetheMAmmoTHevaluationframework[116].
ChineseWeuseCMMLU[60]andCEval[46]toassessperformanceonChineselanguagetasks.
Bothbenchmarksaretestedwitha5-shotconfiguration.
10.1.2 Discussions
DataQuality MAP-Neodemonstratessignificantlybetterperformanceonmath,code,andcom-
plex reasoning by incorporating high-quality data, compared to previous transparent LLMs, e.g.
Amber[66]andPythia[9],adopting(presumably)lowerqualitydata.
GapbetweenourMAP-NeoandothertransparentLLMs InTable9,wenotethattransparent
LLMsstillsignificantlylagbehindtheperformanceoffrontierindustrialOpen-weightLLMswith
similarsizes(e.g. LLama3-8B,Mistral-7B).Incontrast,ourMAP-Neocanmatchorevensurpass
them on part of the automatic benchmarks about math, code, and Chinese knowledge. We call
for increased participation in the development of transparent LLMs to further advance the LLM
democratization.
10.2 AlignedModelPerformance
10.2.1 MainResults
Toaccuratelyevaluatetherealisticconversationalperformanceofouralignedmodels,weselected
severalbenchmarksthatmeasurevariousaspectsofmodelcapabilities. Thesebenchmarkswere
chosen for their ability to comprehensively assess key abilities such as alignment, instruction-
following,real-worldperformance,andalignmentwithhumanpreferences. Belowarethespecific
benchmarksweusedandtheuniquecapabilitiestheyevaluate:
AlignBench[65]AlignBenchevaluatesthealignmentcapabilitiesofChineseLLMs,ensuringhigh
reliabilityandinterpretabilitythroughacomprehensive,multi-dimensionalbenchmarkandhuman-in-
the-loopdatacuration.
AlpacaEval[62,32,31]AlpacaEvalmeasuresinstruction-followingmodels’performanceefficiently
andreliablythroughanLLM-basedautomaticevaluation,validatedagainstextensivehumanannota-
tions.
22Arena-Hard[61]Arena-HardevaluatesLLMs’real-worldperformanceandabilitytoreflecthu-
manpreferencesbyconstructingbenchmarksfromlivedataandensuringrobustmodelcapability
separation.
CHC-Bench[30]CHC-BenchevaluatesLLMsontheirproficiencyinChineseculture,history,and
language,withtaskslikecomposingpoetry,understandingancientChinese,andexplainingChinese
terms,emphasizingthechallengesformodelstrainedmainlyonEnglishdatasets.
MT-Bench[124]MT-BenchassessesLLM-basedchatassistants’alignmentwithhumanpreferences
usingstrongLLMsasjudges,achievinghighagreementwithhumanevaluations.
MMLU-Pro [106] For the aligned models, we further evaluate MMLU-Pro [106] with a 5-shot
configurationtoreflectthemodel’scapabilitiesmorecomprehensively.
10.2.2 Discussions
TheeffectivenessofIterativeDPO InTable 10,whencomparedtoNeo-7B-SFT,Neo-7B-Instruct
showssignificantimprovementonthechat-relatedbenchmarkdatasets(e.g.,AlignBench,AlpacaEval,
Arena-Hard,andCHC-Bench),whichfurtherdemonstratestheeffectivenessofourIterativeDPO.
Theperformanceofthechatmodel Table 10showsthatAmber-7B-ChatandOLMo-7B-Instruct
performpoorlyonChatBenchmarks. Weassumethatthelimitedcapabilitiesofthebasemodelmay
severelylimittheperformanceofcorrespondinginstruction-tunedmodelsonchatbenchmarks.
11 SocietalImpact
DataColonialismisadeepconcernwhenfirmsdecidetoexploitanalgorithmproduct. [27]con-
ceptualizethedatacolonialismframeworkandarguethatBigTechGiants,particularlyintheU.S.,
use their massive data power to manipulate human behaviors and judgments and track people’s
traces continuously, forming a new social order. This suggests that controlling and owning data
benefitsfirms’marketstatusandgenerateslargereturns. So,makingLLMsasfirms’proprietary
modelsisacommonpracticeintheindustry. [2]discussthebarrierstoAIdemocratization,suchas
theconcentrationofAIcapabilitiesinlargetechfirmsandeliteuniversities. Theyunderscorethe
importanceofdemocratizingaccesstoAIresourcestomitigatetherisksofdatacolonialismand
promoteequitableaccesstoAItechnologiesacrossallinstitutions. [91]discussthedominanceof
proprietaryLLMsandtheneedforhigh-performingopen-sourcealternatives. Theyproposemethods
toenhanceopen-sourcemodelstocompetewithproprietarymodelswhileaddressingprivacyand
resource-constrainedconcerns. Theyalsopointouthowimportanttheopen-sourcemodelisinthe
LLMscommunityandacknowledgethatfirmswithfewerresourcesandsensitiveinformationare
hesitanttotrusttheproprietarymodels. However,mostLLMsaretheproductofamassiveEnglish
corpus and are trained from English scratch [122]. How the open-source model can benefit the
non-Englishlanguagecommunityanditsdatademocratizationremainsunclear.
Additionally,mostopen-sourcemodelsarenotthoroughlytransparent. Open-sourcelargelanguage
models (LLMs) often claim to be transparent and accessible, but many critical aspects of their
development,suchasdatacleaningprocessesandpre-trainingcode,remainundisclosed. Thislack
oftransparencyhampersreproducibilityandtheabilitytofullyunderstandandtrustthesemodels
[110]. Forfirmswithfinancialconstraintsandprivacyconcerns,itisnoteconomicaltotraintheir
LLMs. Eventhoughmostopen-sourcemodelsgiveopenaccesstothefinalandsomeintermediate
checkpoints,theykeepdatasources,datapre-trainingcode,anddataprocessingmethodsopaque,
thoseofwhicharethemostcostlypartsofsettingupanLLM.Thatisthekeyissuewewanttotackle
andthenhopetopromotefulltransparencyinourcommunity.
In our report, the MAP-Neo model might complement the current scarcity of Chinese corpus in
LLMs. Importantly,ourbi-linguallanguagemodelisa”thorough”open-sourcemodel–disclosing
all key processes from sources of searching original data, and data cleaning to pre-training code
base. ThosedisclosuressignificantlyreducethecostofdeployingandcustomizingaLLM,especially
for a Chinese LLM. It might have potential societal impacts. Firms with the need for a Chinese
versionofLLMbutfaceconstraintscanbemoreabletoleveragebenefitsfromLLMsbyusingor
referencingourMAP-NeoModel. Itmightimprovesocialwelfareintotalandmakeamorevivid
and diversified Chinese LLMs community [24]. Our advocates for thorough open-source action
23mayattractmoreChineseLLMresearchersorrelevantfirmstofullydisclosetheirmodelsbecause
thoroughtransparentopen-sourcemodelscanbringthemsizablebenefitsfrommoreconstructive
feedbackandcriticism. Thosemightmaketheirmodelsbetterandeventuallyacceleratetheiterations
ofChineseLLMsandempowerthelocalcommunity[81]. Overall,openinnovationpracticeslike
disclosingtheMAP-NeomodelmightalleviatethedominanceofEnglishLLMsandimprovethe
inclusivityoftheinternationalLLMscommunity.
ThoseopeninnovationpracticesmayalsobenefitSmallandMediumenterprises(SME)tointroduce
newproductseffectively[96]andefficientlywitheasierimplementationoftheirowncustomized
LLMs,whichmaypartiallymitigatethethreatsofdatacolonialismfromBigTechGiants. OurMap-
Neomodel’sopenandeconomicalattributesgiveanoptimisticoutlookforresearchersinacademia.
Those attributes suggest that it is not hard and costly to set up the university’s own AI without
dependingon specificBigTech Giants’help. If universities have independentanddecentralized
controlovertheirdataandAIprocesses,itwillpreventlargecompaniesfromAImonopolizationand
promotedataandAIdemocratization.
12 Conclusion
Inthispaper,weintroduceMAP-Neo,whichmakesstridestowardenhancingthetransparencyand
accessibilityoflargelanguagemodels(LLMs)byofferingafullyopen-sourcebilingualLLMsuite.
Bysharingthoroughlydetailedprocesses,fromdatacuration,pre-trainingcorpus(i.e.,MatrixData
Pile),andmodeltrainingtoevaluation,weaimtosupporttheacademicandopen-sourcecommunities
inadvancingtransparentNLPresearch. Moreover,MAP-Neonarrowsthegapwithindustry-level
models(typicallyclosed-source)withenhancedreasoning,instruction-following,andcodingabilities.
Wehopethatourworkprovidesavaluableresourceforresearchersanddevelopers,contributingtoa
broaderefforttodemocratizeaccesstoadvancedLLMtechnologies.
13 ContributionsandAcknowledgments
TeamLeaders:
• GeZhang,M-A-P,UniversityofWaterloo,01.AI,Data&Pretrain&Evaluation&Model
Architecture&Codebase&Alignment
• ScottQu,M-A-P,UniversityofManchester,01.AI,Codebase&ModelArchitecture&Infra
&Pretrain
• JiahengLiu,M-A-P,ScalingLaw&Alignment
CoreContributors: (AlphabetOrder)
• ChenchenZhang,IndependentResearcher,Pretrain
• ChenghuaLin. M-A-P,UniversityofManchester,Data
• ChouLeuangYu,CUHK-Shenzhen,Alignment&Data
• DannyPan,PekingUniversity,Data&Codebase
• EstherCheng,PekingUniversity,Data
• JieLiu,TheChineseUniversityofHongKong,Alignment
• QunshuLin,2077AI,Data
• RavenYuan,M-A-P,Pretrain&Infra
• TuneyZheng,M-A-P,01.AI,UniversityofWaterloo,Pretrain&Evaluation&Alignment
• WeiPang,UniversityofWaterloo,Data
• XinrunDu,M-A-P,01.AI,Codebase&Pretrain&Alignment&Evaluation
• YimingLiang,InstituteofAutomation,ChineseAcademyofSciences,Alignment&Evalua-
tion
• YinghaoMa,M-A-P,QueenMaryUniversityofLondon,ScalingLaw
• YizhiLi,M-A-P,UniversityofManchester,Data
• ZiyangMa,M-A-P,ShanghaiJiaoTongUniversity,Alignment
24Contributors: (AlphabetOrder)
• BillLin,UniversityofSouthernCalifornia,Alignment
• EmmanouilBenetos,QueenMaryUniversityofLondon,ScalingLaw
• HuanYang,UniversityofWarwick,Ethics&SocietalImpact
• JuntingZhou,PekingUniversity,Data&ScalingLaw
• KaijingMa,TongjiUniversity,Data
• MinghaoLiu,2077AI,Data
• MorryNiu,01.AI,Codebase
• NoahWang,01.AI,Alignment
• QuehryQue,IndependentResearcher,Data
• RuiboLiu,DartmouthUniversity,Pretrain&ModelArchitecture
• SineLiu,IndependentResearcher,Infra
• ShawnGuo,01.AI,Data
• SorenGao,FudanUniversity,Tokenization
• WangchunshuZhou,M-A-P&AIWavesInc.,Data
• XinyueZhang,Unity,Ethics&Data
• YizhiZhou,NanjingUniversity,Data
• YuboWang,UniversityofWaterloo,Pretrain
• YuelinBai,M-A-P,ShenzhenInstituteofAdvancedTechnology,CAS,Data
• YuhanZhang,M-A-P,Data
• YuxiangZhang,M-A-P,WasedaUniversity,Codebase&Data
• ZenithWang,IndependentResearcher,Data
• ZhenzhuYang,ChinaUniversityofGeosciencesBeijing,Ethics&Data
• ZijianZhao,2077AI,Data
Advisors:
• JiajunZhang,WuhanAIResearch,InstituteofAutomation,ChineseAcademyofSciences
• WanliOuyang,TheChineseUniversityofHongKong,ShanghaiAILab
• WenhaoHuang,01.AI
• WenhuChen,UniversityofWaterloo
25References
[1] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoni
Aleman,DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4
technicalreport. arXivpreprintarXiv:2303.08774,2023.
[2] Nur Ahmed and Muntasir Wahed. The de-democratization of ai: Deep learning and the
computedivideinartificialintelligenceresearch. arXivpreprintarXiv:2010.15581,2020.
[3] AI@Meta. Llama3modelcard. 2024. URLhttps://github.com/meta-llama/llama3/
blob/main/MODEL_CARD.md.
[4] AIAnthropic. Theclaude3modelfamily: Opus,sonnet,haiku. Claude-3ModelCard,2024.
[5] JacobAustin,AugustusOdena,MaxwellNye,MaartenBosma,HenrykMichalewski,David
Dohan,EllenJiang,CarrieCai,MichaelTerry,QuocLe,etal. Programsynthesiswithlarge
languagemodels. arXivpreprintarXiv:2108.07732,2021.
[6] ZhangirAzerbayev,HaileySchoelkopf,KeiranPaster,MarcoDosSantos,StephenMcaleer,
AlbertQJiang,JiaDeng,StellaBiderman,andSeanWelleck. Llemma: Anopenlanguage
modelformathematics.
[7] JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,XiaodongDeng,YangFan,Wenbin
Ge,YuHan,FeiHuang,etal. Qwentechnicalreport. arXivpreprintarXiv:2309.16609,2023.
[8] Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von
Werra. Cosmopedia,2024. URLhttps://huggingface.co/datasets/HuggingFaceTB/
cosmopedia.
[9] StellaBiderman,HaileySchoelkopf,QuentinGregoryAnthony,HerbieBradley,KyleO’Brien,
EricHallahan,MohammadAflahKhan,ShivanshuPurohit,USVSNSaiPrashanth,Edward
Raff,etal. Pythia: Asuiteforanalyzinglargelanguagemodelsacrosstrainingandscaling. In
InternationalConferenceonMachineLearning,pp.2397–2430.PMLR,2023.
[10] YonatanBisk,RowanZellers,JianfengGao,YejinChoi,etal. Piqa: Reasoningaboutphysical
commonsense in natural language. In Proceedings of the AAAI conference on artificial
intelligence,volume34,pp.7432–7439,2020.
[11] Lukas Blecher, Guillem Cucurull, Thomas Scialom, and Robert Stojnic. Nougat: Neural
opticalunderstandingforacademicdocuments,2023.
[12] RalphAllanBradleyandMiltonE.Terry. Rankanalysisofincompleteblockdesigns: I.the
methodofpairedcomparisons. Biometrika,39(3/4):324–345,1952. ISSN00063444. URL
http://www.jstor.org/stable/2334029.
[13] Andrei Z Broder. On the resemblance and containment of documents. In Proceedings.
CompressionandComplexityofSEQUENCES1997(Cat.No.97TB100171),pp.21–29.IEEE,
1997.
[14] JianghaoChen,PuJian,TengxiaoXi,DongyiYi,QianlongDu,ChenglinDing,GuiboZhu,
ChengqingZong,JinqiaoWang,andJiajunZhang. Chinesewebtext: Large-scalehigh-quality
chinesewebtextextractedwitheffectiveevaluationmodel,2023.
[15] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,
JaredKaplan,HarriEdwards,YuriBurda,NicholasJoseph,GregBrockman,etal. Evaluating
largelanguagemodelstrainedoncode. arXivpreprintarXiv:2107.03374,2021.
[16] Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi
Wang,andTonyXia. Theoremqa: Atheorem-drivenquestionansweringdataset. InThe2023
ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,2023.
[17] ZehuiChen,KuikunLiu,QiuchenWang,JiangningLiu,WenweiZhang,DahuaLin,KaiChen,
andFengZhao. Agent-flan: Designingdataandmethodsofeffectiveagenttuningforlarge
languagemodels. arXivpreprintarXiv:2403.12881,2024.
26[18] AlexisChevalier,JiayiGeng,AlexanderWettig,HowardChen,SebastianMizera,ToniAnnala,
MaxJamesonAragon,ArturoRodr´ıguezFanlo,SimonFrieder,SimonMachado,Akshara
Prabhakar, EllieThieu, JiachenT.Wang, ZiruiWang, XindiWu, MengzhouXia, Wenhan
Jia,JiatongYu,Jun-JieZhu,ZhiyongJasonRen,SanjeevArora,andDanqiChen. Language
modelsassciencetutors. arXivpreprintarXiv: 2402.11111,2024.
[19] PaulFChristiano,JanLeike,TomBrown,MiljanMartic,ShaneLegg,andDarioAmodei.Deep
reinforcementlearningfromhumanpreferences. Advancesinneuralinformationprocessing
systems,30,2017.
[20] HyungWonChung,LeHou,ShayneLongpre,BarretZoph,YiTay,WilliamFedus,Yunxuan
Li,XuezhiWang,MostafaDehghani,SiddharthaBrahma,etal. Scalinginstruction-finetuned
languagemodels. JournalofMachineLearningResearch,25(70):1–53,2024.
[21] ChristopherClark,KentonLee,Ming-WeiChang,TomKwiatkowski,MichaelCollins,and
KristinaToutanova. Boolq: Exploringthesurprisingdifficultyofnaturalyes/noquestions.
arXivpreprintarXiv:1905.10044,2019.
[22] PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,AshishSabharwal,CarissaSchoenick,
andOyvindTafjord. Thinkyouhavesolvedquestionanswering? tryarc,theai2reasoning
challenge. arXivpreprintarXiv:1803.05457,2018.
[23] KarlCobbe,VineetKosaraju,MohammadBavarian,MarkChen,HeewooJun,LukaszKaiser,
MatthiasPlappert,JerryTworek,JacobHilton,ReiichiroNakano,etal. Trainingverifiersto
solvemathwordproblems. arXivpreprintarXiv:2110.14168,2021.
[24] MassimoGColombo,EvilaPiva,andCristinaRossi-Lamastra. Openinnovationandwithin-
industrydiversificationinsmallandmediumenterprises: Thecaseofopensourcesoftware
firms. Researchpolicy,43(5):891–902,2014.
[25] TogetherComputer. Redpajama: anopendatasetfortraininglargelanguagemodels,2023.
URLhttps://github.com/togethercomputer/RedPajama-Data.
[26] OpenCompassContributors. Opencompass: Auniversalevaluationplatformforfoundation
models. https://github.com/open-compass/opencompass,2023.
[27] NickCouldryandUlisesAMejias. Datacolonialism: Rethinkingbigdata’srelationtothe
contemporarysubject. Television&NewMedia,20(4):336–349,2019.
[28] DeepSeek-AI. Deepseek llm: Scaling open-source language models with longtermism.
arXiv preprint arXiv:2401.02954, 2024. URL https://github.com/deepseek-ai/
DeepSeek-LLM.
[29] QixinDeng,QikaiYang,RuibinYuan,YipengHuang,YiWang,XuboLiu,ZeyueTian,Jiahao
Pan,GeZhang,HanfengLin,etal. Composerx: Multi-agentsymbolicmusiccomposition
withllms. arXivpreprintarXiv:2404.18081,2024.
[30] XinrunDu,ZhouliangYu,SongyangGao,DingPan,YuyangCheng,ZiyangMa,RuibinYuan,
XingweiQu,JiahengLiu,TianyuZheng,XinchenLuo,GuoruiZhou,BinhangYuan,Wenhu
Chen,JieFu,andGeZhang. Chinesetinyllm: Pretrainingachinese-centriclargelanguage
model,2024.
[31] YannDubois,XuechenLi,RohanTaori,TianyiZhang,IshaanGulrajani,JimmyBa,Carlos
Guestrin,PercyLiang,andTatsunoriB.Hashimoto. Alpacafarm: Asimulationframeworkfor
methodsthatlearnfromhumanfeedback,2023.
[32] YannDubois,Bala´zsGalambosi,PercyLiang,andTatsunoriBHashimoto. Length-controlled
alpacaeval: Asimplewaytodebiasautomaticevaluators. arXivpreprintarXiv:2404.04475,
2024.
[33] Rodney A Gabriel, Tsung-Ting Kuo, Julian McAuley, and Chun-Nan Hsu. Identifying
andcharacterizinghighlysimilarnotesinbigclinicalnotedatasets. Journalofbiomedical
informatics,82:63–69,2018.
27[34] XinyangGengandHaoLiu. Openllama: Anopenreproductionofllama,May2023. URL
https://github.com/openlm-research/open_llama.
[35] AristidesGionis,PiotrIndyk,RajeevMotwani,etal. Similaritysearchinhighdimensionsvia
hashing. InVldb,volume99,pp.518–529,1999.
[36] DirkGroeneveld,IzBeltagy,PeteWalsh,AkshitaBhagia,RodneyKinney,OyvindTafjord,
AnanyaHarshJha,HamishIvison,IanMagnusson,YizhongWang,etal. Olmo: Accelerating
thescienceoflanguagemodels. arXivpreprintarXiv:2402.00838,2024.
[37] BikashGyawali,LucasAnastasiou,andPetrKnoth. Deduplicationofscholarlydocuments
usinglocalitysensitivehashingandwordembeddings. 2020.
[38] ConghuiHe,ZhenjiangJin,ChaoXu,JiantaoQiu,BinWang,WeiLi,HangYan,JiaqiWang,
andDahuaLin. Wanjuan: Acomprehensivemultimodaldatasetforadvancingenglishand
chineselargemodels,2023.
[39] PeterHenderson*,MarkS.Krass*,LuciaZheng,NeelGuha,ChristopherD.Manning,Dan
Jurafsky,andDanielE.Ho. Pileoflaw: Learningresponsibledatafilteringfromthelawanda
256gbopen-sourcelegaldataset,2022. URLhttps://arxiv.org/abs/2207.00220.
[40] DanHendrycks,CollinBurns,StevenBasart,AndyZou,MantasMazeika,DawnSong,and
Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint
arXiv:2009.03300,2020.
[41] DanHendrycks,CollinBurns,SauravKadavath,AkulArora,StevenBasart,EricTang,Dawn
Song,andJacobSteinhardt. Measuringmathematicalproblemsolvingwiththemathdataset.
arXivpreprintarXiv:2103.03874,2021.
[42] DannyHernandez,TomBrown,TomConerly,NovaDasSarma,DawnDrain,SheerEl-Showk,
NelsonElhage,ZacHatfield-Dodds,TomHenighan,TristanHume,etal. Scalinglawsand
interpretabilityoflearningfromrepeateddata. arXivpreprintarXiv:2205.10487,2022.
[43] JordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,Eliza
Rutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.
Trainingcompute-optimallargelanguagemodels. arXivpreprintarXiv:2203.15556,2022.
[44] ShengdingHu,YugeTu,XuHan,ChaoqunHe,GanquCui,XiangLong,ZhiZheng,Yewei
Fang,YuxiangHuang,WeilinZhao,etal. Minicpm: Unveilingthepotentialofsmalllanguage
modelswithscalabletrainingstrategies. arXivpreprintarXiv:2404.06395,2024.
[45] YupanHuang,TengchaoLv,LeiCui,YutongLu,andFuruWei. Layoutlmv3: Pre-training
for document ai with unified text and image masking. In Proceedings of the 30th ACM
InternationalConferenceonMultimedia,pp.4083–4091,2022.
[46] YuzhenHuang,YuzhuoBai,ZhihaoZhu,JunleiZhang,JinghanZhang,TangjunSu,Junteng
Liu,ChuanchengLv,YikaiZhang,YaoFu,etal.C-eval:Amulti-levelmulti-disciplinechinese
evaluationsuiteforfoundationmodels. AdvancesinNeuralInformationProcessingSystems,
36,2024.
[47] PaulJaccard. Thedistributionoftheflorainthealpinezone.1. Newphytologist,11(2):37–50,
1912.
[48] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
Chaplot,DiegodelasCasas,FlorianBressand,GiannaLengyel,GuillaumeLample,Lucile
Saulnier,etal. Mistral7b. arXivpreprintarXiv:2310.06825,2023.
[49] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large
scale distantly supervised challenge dataset for reading comprehension. arXiv preprint
arXiv:1705.03551,2017.
[50] ArmandJoulin,EdouardGrave,PiotrBojanowski,MatthijsDouze,Herve´ Je´gou,andTomas
Mikolov. Fasttext.zip: Compressing text classification models. arXiv: Computation and
Language,arXiv: ComputationandLanguage,Nov2016.
28[51] Jean Kaddour. The minipile challenge for data-efficient language models. arXiv preprint
arXiv:2304.08442,2023.
[52] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon
Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural
languagemodels. arXivpreprintarXiv:2001.08361,2020.
[53] RaimondasKiveris,SilvioLattanzi,VahabMirrokni,VibhorRastogi,andSergeiVassilvitskii.
Connectedcomponentsinmapreduceandbeyond. InProceedingsoftheACMSymposiumon
CloudComputing,pp.1–13,2014.
[54] DenisKocetkov,RaymondLi,LoubnaBenAllal,JiaLi,ChenghaoMou,CarlosMun˜ozFer-
randis,YacineJernite,MargaretMitchell,SeanHughes,ThomasWolf,DzmitryBahdanau,
LeandrovonWerra,andHarmdeVries. Thestack: 3tbofpermissivelylicensedsourcecode.
Preprint,2022.
[55] KamranKowsari,DonaldEBrown,MojtabaHeidarysafa,KianaJafariMeimandi,,MatthewS
Gerber,andLauraEBarnes. Hdltex: Hierarchicaldeeplearningfortextclassification. In
MachineLearningandApplications(ICMLA),201716thIEEEInternationalConferenceon.
IEEE,2017.
[56] TakuKudoandJohnRichardson. Sentencepiece:Asimpleandlanguageindependentsubword
tokenizeranddetokenizerforneuraltextprocessing. arXivpreprintarXiv:1808.06226,2018.
[57] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh,
ChrisAlberti,DanielleEpstein,IlliaPolosukhin,JacobDevlin,KentonLee,etal. Natural
questions: abenchmarkforquestionansweringresearch. TransactionsoftheAssociationfor
ComputationalLinguistics,7:453–466,2019.
[58] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris
Callison-Burch,andNicholasCarlini. Deduplicatingtrainingdatamakeslanguagemodels
better. arXivpreprintarXiv:2107.06499,2021.
[59] ChenxiaLi,RuoyuGuo,JunZhou,MengtaoAn,YuningDu,LingfengZhu,YiLiu,Xiaoguang
Hu,andDianhaiYu. Pp-structurev2: Astrongerdocumentanalysissystem. arXivpreprint
arXiv:2210.05391,2022.
[60] HaonanLi,YixuanZhang,FajriKoto,YifeiYang,HaiZhao,YeyunGong,NanDuan,and
TimothyBaldwin. Cmmlu: Measuringmassivemultitasklanguageunderstandinginchinese.
arXivpreprintarXiv:2306.09212,2023.
[61] TianleLi*,Wei-LinChiang,EvanFrick,LisaDunlap,BanghuaZhu,JosephE.Gonzalez,and
IonStoica. Fromlivedatatohigh-qualitybenchmarks: Thearena-hardpipeline,April2024.
URLhttps://lmsys.org/blog/2024-04-19-arena-hard/.
[62] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin,
PercyLiang,andTatsunoriB.Hashimoto. Alpacaeval: Anautomaticevaluatorofinstruction-
followingmodels. https://github.com/tatsu-lab/alpaca_eval,2023.
[63] JiaweiLiu,ChunqiuStevenXia,YuyaoWang,andLingmingZhang. Isyourcodegenerated
bychatGPTreallycorrect? rigorousevaluationoflargelanguagemodelsforcodegeneration.
InThirty-seventhConferenceonNeuralInformationProcessingSystems,2023. URLhttps:
//openreview.net/forum?id=1qvx610Cu7.
[64] JieLiu,ZhanhuiZhou,ChaoYang,Han-SenZhong,andWanliOuyang. Storm-7b,April2024.
URLhttps://huggingface.co/jieliu/Storm-7B.
[65] XiaoLiu,XuanyuLei,ShengyuanWang,YueHuang,ZhuoerFeng,BosiWen,JialeCheng,
PeiKe,YifanXu,WengLamTam,XiaohanZhang,LichaoSun,HongningWang,JingZhang,
MinlieHuang,YuxiaoDong,andJieTang. Alignbench: Benchmarkingchinesealignmentof
largelanguagemodels,2023.
29[66] ZhengzhongLiu,AurickQiao,WillieNeiswanger,HongyiWang,BowenTan,TianhuaTao,
JunboLi,YuqiWang,SuqiSun,OmkarPangarkar,RichardFan,YiGu,VictorMiller,Yonghao
Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen,
Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Tim
Baldwin,andEricP.Xing. Llm360: Towardsfullytransparentopen-sourcellms,2023.
[67] ShayneLongpre,LeHou,TuVu,AlbertWebson,HyungWonChung,YiTay,DennyZhou,
QuocVLe,BarretZoph,JasonWei,etal. Theflancollection: Designingdataandmethods
foreffectiveinstructiontuning. arXivpreprintarXiv:2301.13688,2023.
[68] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier,
NouamaneTazi,AoTang,DmytroPykhtar,JiaweiLiu,YuxiangWei,TianyangLiu,MaxTian,
DenisKocetkov,ArthurZucker,YounesBelkada,ZijianWang,QianLiu,DmitryAbulkhanov,
IndraneilPaul,ZhuangLi,Wen-DingLi,MeganRisdal,JiaLi,JianZhu,TerryYueZhuo,
EvgeniiZheltonozhskii,NiiOsaeOsaeDade,WenhaoYu,LucasKrauß,NamanJain,Yixuan
Su,XuanliHe,MananDey,EdoardoAbati,YekunChai,NiklasMuennighoff,XiangruTang,
MuhtashamOblokulov,ChristopherAkiki,MarcMarone,ChenghaoMou,MayankMishra,
AlexGu,BinyuanHui,TriDao,ArmelZebaze,OlivierDehaene,NicolasPatry,CanwenXu,
JulianMcAuley,HanHu,TorstenScholak,SebastienPaquet,JenniferRobinson,CarolynJane
Anderson,NicolasChapados,MostofaPatwary,NimaTajbakhsh,YacineJernite,CarlosMun˜oz
Ferrandis,LingmingZhang,SeanHughes,ThomasWolf,ArjunGuha,LeandrovonWerra,
andHarmdeVries. Starcoder2andthestackv2: Thenextgeneration,2024.
[69] Yin Luo, Qingchao Kong, Nan Xu, Jia Cao, Bao Hao, Baoyu Qu, Bo Chen, Chao Zhu,
Chenyang Zhao, Donglei Zhang, et al. Yayi 2: Multilingual open-source large language
models. arXivpreprintarXiv:2312.14862,2023.
[70] TodorMihaylov,PeterClark,TusharKhot,andAshishSabharwal. Canasuitofarmorconduct
electricity? anewdatasetforopenbookquestionanswering. arXivpreprintarXiv:1809.02789,
2018.
[71] NamPham. tiny-strange-textbooks(revision6f304f1),2024. URLhttps://huggingface.
co/datasets/nampdn-ai/tiny-strange-textbooks.
[72] ThuatNguyen,ChienVanNguyen,VietDacLai,HieuMan,NghiaTrungNgo,FranckDernon-
court,RyanARossi,andThienHuuNguyen.Culturax:Acleaned,enormous,andmultilingual
datasetforlargelanguagemodelsin167languages. arXivpreprintarXiv:2309.09400,2023.
[73] KeiranPaster,MarcoDosSantos,ZhangirAzerbayev,andJimmyBa. Openwebmath: An
opendatasetofhigh-qualitymathematicalwebtext,2023.
[74] GuilhermePenedo,QuentinMalartic,DanielHesslow,RuxandraCojocaru,AlessandroCap-
pelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The
RefinedWebdatasetforFalconLLM:outperformingcuratedcorporawithwebdata,andweb
dataonly. arXivpreprintarXiv:2306.01116,2023. URLhttps://arxiv.org/abs/2306.
01116.
[75] XingweiQu,YuelinBai, YinghaoMa, ZiyaZhou,KaManLo, JiahengLiu,RuibinYuan,
LejunMin,XuelingLiu,TianyuZhang,etal. Mupt: Agenerativesymbolicmusicpretrained
transformer. arXivpreprintarXiv:2404.06393,2024.
[76] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis
Song,JohnAslanides,SarahHenderson,RomanRing,SusannahYoung,ElizaRutherford,
Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driess-
che,LisaAnneHendricks,MaribethRauh,Po-SenHuang,AmeliaGlaese,JohannesWelbl,
SumanthDathathri, SaffronHuang, JonathanUesato, JohnMellor, IrinaHiggins, Antonia
Creswell, NatMcAleese, AmyWu, ErichElsen,SiddhantJayakumar, ElenaBuchatskaya,
DavidBudden,EsmeSutherland,KarenSimonyan,MichelaPaganini,LaurentSifre,Lena
Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya,
DomenicDonato,AngelikiLazaridou,ArthurMensch,Jean-BaptisteLespiau,MariaTsim-
poukelli,NikolaiGrigorev,DougFritz,ThibaultSottiaux,MantasPajarskas,TobyPohlen,
ZhitaoGong,DanielToyama,CypriendeMassond’Autume,YujiaLi,TayfunTerzi,Vladimir
30Mikulik,IgorBabuschkin,AidanClark,DiegodeLasCasas,AureliaGuy,ChrisJones,James
Bradbury,MatthewJohnson,BlakeHechtman,LauraWeidinger,IasonGabriel,WilliamIsaac,
Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub,
JeffStanway,LorrayneBennett,DemisHassabis,KorayKavukcuoglu,andGeoffreyIrving.
Scalinglanguagemodels: Methods,analysis&insightsfromtraininggopher,2022.
[77] RafaelRafailov,ArchitSharma,EricMitchell,StefanoErmon,ChristopherD.Manning,and
ChelseaFinn. Directpreferenceoptimization:Yourlanguagemodelissecretlyarewardmodel,
2023.
[78] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the Limits of Transfer Learning with a
UnifiedText-to-TextTransformer. arXive-prints,art.arXiv:1910.10683,October2019. doi:
10.48550/arXiv.1910.10683.
[79] PranavRajpurkar,RobinJia,andPercyLiang. Knowwhatyoudon’tknow: Unanswerable
questionsforsquad. arXivpreprintarXiv:1806.03822,2018.
[80] MachelReid,NikolaySavinov,DenisTeplyashin,DmitryLepikhin,TimothyLillicrap,Jean-
baptisteAlayrac,RaduSoricut,AngelikiLazaridou,OrhanFirat,JulianSchrittwieser,etal.
Gemini1.5: Unlockingmultimodalunderstandingacrossmillionsoftokensofcontext. arXiv
preprintarXiv:2403.05530,2024.
[81] PaolaRicaurte. Dataepistemologies,thecolonialityofpower,andresistance. Television&
NewMedia,20(4):350–365,2019.
[82] Ronsor. Bigknow2022: Bringing language models up to speed. https://github.com/
RyokoAI/BigKnow2022,2023.
[83] KeisukeSakaguchi,RonanLeBras,ChandraBhagavatula,andYejinChoi. Winogrande: An
adversarialwinogradschemachallengeatscale. CommunicationsoftheACM,64(9):99–106,
2021.
[84] RusselSandberg,DavidGoldberg,SteveKleiman,DanWalsh,andBobLyon. Designand
implementationofthesunnetworkfilesystem. InProceedingsofthesummer1985USENIX
conference,pp.119–130,1985.
[85] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa:
Commonsensereasoningaboutsocialinteractions. arXivpreprintarXiv:1904.09728,2019.
[86] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic´, Daniel Hess-
low,RomanCastagne´,AlexandraSashaLuccioni,Franc¸oisYvon,MatthiasGalle´,Jonathan
Tow,AlexanderM.Rush,StellaBiderman,AlbertWebson,PawanSasankaAmmanamanchi,
Thomas Wang, Benoˆıt Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji
Ruwase,RachelBawden,StasBekman,AngelinaMcmillan-Major,IzBeltagy,HuuNguyen,
LucileSaulnier,SamsonTan,PedroOrtizSuarez,VictorSanh,HugoLaurenc¸on,YacineJer-
nite,JulienLaunay,MargaretMitchell,ColinRaffel,AaronGokaslan,AdiSimhi,AitorSoroa,
AlhamFikriAji,AmitAlfassy,AnnaRogers,ArielKreisbergNitzav,CanwenXu,Chenghao
Mou,ChrisEmezue,ChristopherKlamm,ColinLeong,DanielvanStrien,DavidIfeoluwaAde-
lani,DragomirRadev,EduardoGonza´lezPonferrada,EfratLevkovizh,EthanKim,EyalBar
Natan,FrancescodeToni,Ge´rardDupont,Germa´nKruszewski,GiadaPistilli,HadyElsahar,
HamzaBenyamina,HieuTran,IanYu,IdrisAbdulmumin,IsaacJohnson,ItziarGonzalez-Dios,
JavierdelaRosa,JennyChim,JesseDodge,JianZhu,JonathanChang,Jo¨rgFrohberg,Joseph
Tobing,JoydeepBhattacharjee,KhalidAlmubarak,KimboChen,KyleLo,LeandrovonWerra,
LeonWeber,LongPhan,LoubnaBenAllal,LudovicTanguy,MananDey,ManuelRomero
Mun˜oz, Maraim Masoud, Mar´ıa Grandury, Mario Sˇasˇko, Max Huang, Maximin Coavoux,
MayankSingh,MikeTian-JianJiang,MinhChienVu,MohammadA.Jauhar,MustafaGhaleb,
NishantSubramani,NoraKassner,NurulaqillaKhamis,OlivierNguyen,OmarEspejel,Ona
deGibert,PauloVillegas,PeterHenderson,PierreColombo,PriscillaAmuok,QuentinLhoest,
RhezaHarliman,RishiBommasani,RobertoLuisLo´pez,RuiRibeiro,SalomeyOsei,Sampo
Pyysalo,SebastianNagel,ShamikBose,ShamsuddeenHassanMuhammad,ShanyaSharma,
31ShayneLongpre,SomaiehNikpoor,StanislavSilberberg,SuhasPai,SydneyZink,TiagoTim-
poniTorrent,TimoSchick,TristanThrush,ValentinDanchev,VassilinaNikoulina,Veronika
Laippala,VioletteLepercq,VrindaPrabhu,ZaidAlyafeai,ZeerakTalat,ArunRaja,Benjamin
Heinzerling, Chenglei Si, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht
Sharma,AndreaSantilli,AntoineChaffin,ArnaudStiegler,DebajyotiDatta,ElizaSzczechla,
GunjanChhablani,HanWang,HarshitPandey,HendrikStrobelt,JasonAlanFries,JosRozen,
LeoGao, LintangSutawika, MSaifulBari, MagedS.Al-Shaibani, MatteoManica, Nihal
Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach,
TaewoonKim,TaliBers,ThibaultFevry,TrishalaNeeraj,UrmishThakker,VikasRaunak,Xi-
angruTang,Zheng-XinYong,ZhiqingSun,ShakedBrody,YallowUri,HadarTojarieh,Adam
Roberts, HyungWonChung, JaesungTae, JasonPhang, OfirPress, ConglongLi, Deepak
Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra,
Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi,
OmarSanseviero,PatrickvonPlaten,PierreCornette,PierreFranc¸oisLavalle´e,Re´miLacroix,
SamyamRajbhandari,SanchitGandhi,ShadenSmith,Ste´phaneRequena,SurajPatil,Tim
Dettmers,AhmedBaruwa,AmanpreetSingh,AnastasiaCheveleva,Anne-LaureLigozat,Ar-
junSubramonian,Aure´lieNe´ve´ol,CharlesLovering,DanGarrette,DeepakTunuguntla,Ehud
Reiter,EkaterinaTaktasheva,EkaterinaVoloshina,EliBogdanov,GentaIndraWinata,Hailey
Schoelkopf,Jan-ChristophKalo,JekaterinaNovikova,JessicaZosaForde,JordanClive,Jungo
Kasai,KenKawamura,LiamHazan,MarineCarpuat,MirunaClinciu,NajoungKim,Newton
Cheng,OlegSerikov,OmerAntverg,OskarvanderWal,RuiZhang,RuochenZhang,Sebas-
tianGehrmann,ShaniPais,TatianaShavrina,ThomasScialom,TianYun,TomaszLimisiewicz,
VerenaRieser,VitalyProtasov,VladislavMikhailov,YadaPruksachatkun,YonatanBelinkov,
ZacharyBamberger,ZdeneˇkKasner,AliceRueda,AmandaPestana,AmirFeizpour,Ammar
Khan,AmyFaranak,AnaSantos,AnthonyHevia,AntigonaUnldreaj,ArashAghagol,Arezoo
Abdollahi, Aycha Tammour, Azadeh Hajihosseini, Bahareh Behroozi, Benjamin Ajibade,
Bharat Saxena, Carlos Mun˜oz Ferrandis, Danish Contractor, David Lansky, Davis David,
DouweKiela,DuongA.Nguyen,EdwardTan,EmiBaylor,EzinwanneOzoani,FatimaMirza,
FranklineOnoniwu,HabibRezanejad,HessieJones,IndraniBhattacharya,IreneSolaiman,
IrinaSedenko,IsarNejadgholi,JessePassmore,JoshSeltzer,JulioBonisSanz,LiviaDutra,
Mairon Samagaio, MaraimElbadri, MargotMieskes, MarissaGerchick, MarthaAkinlolu,
MichaelMckenna,MikeQiu,MuhammedGhauri,MykolaBurynok,NafisAbrar,Nazneen
Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan
Hao,SamiraAlizadeh,SarmadShubber,SilasWang,SouravRoy,SylvainViguier,ThanhLe,
TobiOyebade,TrieuLe,YoyoYang,ZachNguyen,AbhinavRameshKashyap,AlfredoPalas-
ciano,AlisonCallahan,AnimaShukla,AntonioMiranda-Escalada,AyushSingh,Benjamin
Beilharz,BoWang,CaioBrito,ChenxiZhou,ChiragJain,ChuxinXu,Cle´mentineFourrier,
DanielLeo´nPerin˜a´n, DanielMolano, DianYu, EnriqueManjavacas, FabioBarth, Florian
Fuhrimann,GabrielAltay,GiyaseddinBayrak,GullyBurns,HelenaU.Vrabec,ImaneBello,
IshaniDash,JihyunKang,JohnGiorgi,JonasGolde,JoseDavidPosada,KarthikRangasai
Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz,
MaikoTakeuchi,MarcPa`mies,MariaACastillo,MariannaNezhurina,MarioSa¨nger,Matthias
Samwald,MichaelCullan,MichaelWeinberg,MichieldeWolf,MinaMihaljcic,MinnaLiu,
MoritzFreidank,MyungsunKang,NatashaSeelam,NathanDahlberg,NicholasMichioBroad,
NikolausMuellner,PascaleFung,PatrickHaller,RamyaChandrasekhar,RenataEisenberg,
RobertMartin,RodrigoCanalli,RosalineSu,RuisiSu,SamuelCahyawijaya,SamueleGarda,
Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-Aroonsiri,
SrishtiKumar,StefanSchweter,SushilBharati,TanmayLaud,The´oGigant,TomoyaKainuma,
WojciechKusa,YanisLabrak,YashShaileshBajaj,YashVenkatraman,YifanXu,YingxinXu,
YuXu,ZheTan,ZhongliXie,ZifanYe,MathildeBras,YounesBelkada,andThomasWolf.
BLOOM:A176B-ParameterOpen-AccessMultilingualLanguageModel. workingpaperor
preprint,November2023. URLhttps://inria.hal.science/hal-03850124.
[87] JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. Proximal
policyoptimizationalgorithms,2017.
[88] RicoSennrich,BarryHaddow,andAlexandraBirch. Neuralmachinetranslationofrarewords
withsubwordunits. arXivpreprintarXiv:1508.07909,2015.
32[89] ZhihongShao,PeiyiWang,QihaoZhu,RunxinXu,JunxiaoSong,MingchuanZhang,YKLi,
YWu,andDayaGuo. Deepseekmath: Pushingthelimitsofmathematicalreasoninginopen
languagemodels. arXivpreprintarXiv:2402.03300,2024.
[90] EvaSharma,ChenLi,andLuWang. BIGPATENT:Alarge-scaledatasetforabstractiveand
coherent summarization. CoRR, abs/1906.03741, 2019. URL http://arxiv.org/abs/
1906.03741.
[91] SumukShashidhar,AbhinavChinta,VaibhavSahai,ZhenhailongWang,andHengJi. De-
mocratizingllms: Anexplorationofcost-performancetrade-offsinself-refinedopen-source
models. In The 2023 Conference on Empirical Methods in Natural Language Processing,
2023.
[92] NoamShazeer. Fasttransformerdecoding: Onewrite-headisallyouneed,2019.
[93] NoamShazeer. Gluvariantsimprovetransformer,2020.
[94] Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel
Hestness, and Nolan Dey. SlimPajama: A 627B token cleaned and
deduplicated version of RedPajama. https://www.cerebras.net/blog/
slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama,
2023. URLhttps://huggingface.co/datasets/cerebras/SlimPajama-627B.
[95] LucaSoldaini,RodneyKinney,AkshitaBhagia,DustinSchwenk,DavidAtkinson,Russell
Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann,
AnanyaHarshJha,SachinKumar,LiLucy,XinxiLyu,NathanLambert,IanMagnusson,Jacob
Morrison,NiklasMuennighoff,AakankshaNaik,CrystalNam,MatthewE.Peters,Abhilasha
Ravichander,KyleRichardson,ZejiangShen,EmmaStrubell,NishantSubramani,Oyvind
Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy,
Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: An Open Corpus of Three Trillion
Tokens for Language Model Pretraining Research. arXiv preprint, 2024. URL https:
//arxiv.org/abs/2402.00159.
[96] Andre´ Spithoven,WimVanhaverbeke,andNadineRoijakkers. Openinnovationpracticesin
smesandlargeenterprises. Smallbusinesseconomics,41:537–562,2013.
[97] JianlinSu,YuLu,ShengfengPan,AhmedMurtadha,BoWen,andYunfengLiu. Roformer:
Enhancedtransformerwithrotarypositionembedding,2023.
[98] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa:
A question answering challenge targeting commonsense knowledge. arXiv preprint
arXiv:1811.00937,2018.
[99] Teknium. Openhermes2.5: Anopendatasetofsyntheticdataforgeneralistllmassistants,
2023. URLhttps://huggingface.co/datasets/teknium/OpenHermes-2.5.
[100] HuuNguyenThuatNguyenandThienNguyen. Culturay: Alargecleanedmultilingualdataset
of75languages,2024.
[101] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Tim-
othe´e Lacroix, Baptiste Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Ro-
driguez,ArmandJoulin,EdouardGrave,andGuillaumeLample. Llama: Openandefficient
foundationlanguagemodels. ARXIV,2023.
[102] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas
Blecher,CristianCantonFerrer,MoyaChen,GuillemCucurull,DavidEsiobu,JudeFernandes,
JeremyFu,WenyinFu,BrianFuller,CynthiaGao,VedanujGoswami,NamanGoyal,Anthony
Hartshorn,SagharHosseini,RuiHou,HakanInan,MarcinKardas,ViktorKerkez,Madian
Khabsa,IsabelKloumann,ArtemKorenev,PunitSinghKoura,Marie-AnneLachaux,Thibaut
Lavril,JenyaLee,DianaLiskovich,YinghaiLu,YuningMao,XavierMartinet,TodorMi-
haylov,PushkarMishra,IgorMolybog,YixinNie,AndrewPoulton,JeremyReizenstein,Rashi
Rungta,KalyanSaladi,AlanSchelten,RuanSilva,EricMichaelSmith,RanjanSubramanian,
33XiaoqingEllenTan,BinhTang,RossTaylor,AdinaWilliams,JianXiangKuan,PuxinXu,
ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,MelanieKambadur,SharanNarang,
AurelienRodriguez,RobertStojnic,SergeyEdunov,andThomasScialom. Llama2: Open
foundationandfine-tunedchatmodels. arXivpreprintarXiv: 2307.09288,2023.
[103] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN.Gomez,
LukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed,2023.
[104] YannickVersleyandYanaPanchenko. Notjustbigger: Towardsbetter-qualitywebcorpora.
InProceedingsoftheseventhWebasCorpusWorkshop(WAC7),pp.44–52,2012.
[105] TiannanWang,JiaminChen,QingruiJia,ShuaiWang,RuoyuFang,HuilinWang,Zhaowei
Gao,ChunzhaoXie,ChuouXu,JihongDai,YibinLiu,JialongWu,ShengweiDing,Long
Li,ZhiweiHuang,XinleDeng,TengYu,GanganMa,HanXiao,ZixinChen,DanjunXiang,
YunxiaWang,YuanyuanZhu,YiXiao,JingWang,YiruWang,SiranDing,JiayangHuang,
Jiayi Xu, Yilihamu Tayier, Zhenyu Hu, Yuan Gao, Chengfeng Zheng, Yueshu Ye, Yihang
Li,LeiWan,XinyueJiang,YujieWang,SiyuCheng,ZhuleSong,XiangruTang,Xiaohua
Xu,NingyuZhang,HuajunChen,YuchenEleanorJiang,andWangchunshuZhou. Weaver:
Foundationmodelsforcreativewriting. arXivpreprintarXiv: 2401.17268,2024.
[106] YuboWang,XueguangMa,GeZhang,YuanshengNi,AbhranilChandra,LiTianle,Shiguang
Guo,AaranArulraj,XuanHe,WeimingRen,ZiyanJiang,AlexZhuang,KaiWang,Richard
Fan,MaxKu,XiangYue,andWenhuChen. Mmlu-pro: Towardsmorerobustandchallenging
multi-tasklanguageunderstandingevaluation. Manuscriptinpreparation,2024.
[107] ZekunMooreWang,ZhongyuanPeng,HaoranQue,JiahengLiu,WangchunshuZhou,Yuhan
Wu,HongchengGuo,RuitongGan,ZehaoNi,ManZhang,ZhaoxiangZhang,WanliOuyang,
Ke Xu, Wenhu Chen, Jie Fu, and Junran Peng. Rolellm: Benchmarking, eliciting, and
enhancingrole-playingabilitiesoflargelanguagemodels. arXivpreprintarXiv: 2310.00746,
2023.
[108] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, Jinrong Yang, Jianjian
Sun, Chunrui Han, and Xiangyu Zhang. Vary: Scaling up the vision vocabulary for large
vision-languagemodels. arXivpreprintarXiv:2312.06109,2023.
[109] TianwenWei,LiangZhao,LichangZhang,BoZhu,LijieWang,HaihuaYang,BiyeLi,Cheng
Cheng,WeiweiLu¨,RuiHu,ChenxiaLi,LiuYang,XilinLuo,XuejieWu,LunanLiu,Wenjun
Cheng, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Lei Lin, Xiaokun Wang, Yutuan Ma,
ChuanhaiDong,YanqiSun,YifuChen,YongyiPeng,XiaojuanLiang,ShuichengYan,Han
Fang,andYahuiZhou. Skywork: Amoreopenbilingualfoundationmodel,2023.
[110] Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. A systematic
evaluation of large language models of code. In Proceedings of the 6th ACM SIGPLAN
InternationalSymposiumonMachineProgramming,pp.1–10,2022.
[111] Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston. Some things are more
cringe than others: Preference optimization with the pairwise cringe loss. arXiv preprint
arXiv:2312.16682,2023.
[112] QisenYang,ZekunWang,HonghuiChen,ShenzhiWang,YifanPu,XinGao,WenhaoHuang,
ShijiSong,andGaoHuang. Llmagentsforpsychology: Astudyongamifiedassessments.
arXivpreprintarXiv: 2402.12326,2024.
[113] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li,
JiangchengZhu,JianqunChen,JingChang,etal. Yi:Openfoundationmodelsby01.ai. arXiv
preprintarXiv:2403.04652,2024.
[114] BotaoYu,FrazierN.Baker,ZiqiChen,XiaNing,andHuanSun. Llasmol: Advancinglarge
languagemodelsforchemistrywithalarge-scale,comprehensive,high-qualityinstruction
tuningdataset. arXivpreprintarXiv:2402.09391,2024.
[115] RuibinYuan, HanfengLin, YiWang, ZeyueTian, ShangdaWu, TianhaoShen,GeZhang,
YuhangWu,CongLiu,ZiyaZhou,etal. Chatmusician: Understandingandgeneratingmusic
intrinsicallywithllm. arXivpreprintarXiv:2402.16153,2024.
34[116] XiangYue,XingweiQu,GeZhang,YaoFu,WenhaoHuang,HuanSun,YuSu,andWenhu
Chen. Mammoth: Buildingmathgeneralistmodelsthroughhybridinstructiontuning. arXiv
preprintarXiv:2309.05653,2023.
[117] XiangYue,TuneyZheng,GeZhang,andWenhuChen. Mammoth2: Scalinginstructionsfrom
theweb. arXivpreprintarXiv:2405.03548,2024.
[118] MateiZaharia,MosharafChowdhury,TathagataDas,AnkurDave,JustinMa,MurphyMc-
Cauly,MichaelJFranklin,ScottShenker,andIonStoica. Resilientdistributeddatasets: A
{Fault-Tolerant}abstractionfor{In-Memory}clustercomputing. In9thUSENIXsymposium
onnetworkedsystemsdesignandimplementation(NSDI12),pp.15–28,2012.
[119] RowanZellers,AriHoltzman,YonatanBisk,AliFarhadi,andYejinChoi. Hellaswag: Cana
machinereallyfinishyoursentence? arXivpreprintarXiv:1905.07830,2019.
[120] BiaoZhangandRicoSennrich. Rootmeansquarelayernormalization,2019.
[121] GeZhang,YeminShi,RuiboLiu,RuibinYuan,YizhiLi,SiweiDong,YuShu,ZhaoqunLi,
ZekunWang,ChenghuaLin,etal. Chineseopeninstructiongeneralist: Apreliminaryrelease.
arXivpreprintarXiv:2304.07987,2023.
[122] XiangZhang,SenyuLi,BradleyHauer,NingShi,andGrzegorzKondrak. Don’ttrustchatgpt
whenyourquestionisnotinenglish: Astudyofmultilingualabilitiesandtypesofllms. In
Proceedingsofthe2023ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,
pp.7915–7927,2023.
[123] YifanZhang,YifanLuo,YangYuan,andAndrewChi-ChihYao. Automathtext: Autonomous
dataselectionwithlanguagemodelsformathematicaltexts. arXivpreprintarXiv:2402.07625,
2024.
[124] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with
mt-benchandchatbotarena. AdvancesinNeuralInformationProcessingSystems,36,2024.
[125] Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu
Chen,andXiangYue. Opencodeinterpreter: Integratingcodegenerationwithexecutionand
refinement. arXivpreprintarXiv:2402.14658,2024.
[126] BanghuaZhu,EvanFrick,TianhaoWu,HanlinZhu,andJiantaoJiao. Starling-7b: Improving
llmhelpfulness&harmlessnesswithrlaif,November2023.
[127] AlexZhuang,GeZhang,TianyuZheng,XinrunDu,JunjieWang,WeimingRen,StephenW.
Huang,JieFu,XiangYue,andWenhuChen. Structlm: Towardsbuildinggeneralistmodels
forstructuredknowledgegrounding,2024.
[128] XiaominZhuang,YufanJiang,QiaozhiHe,andZhihuaWu. Chuxin: 1.6btechnicalreport.
arXivpreprintarXiv:2405.04828,2024.
35A Appendix
A.1 DetailsofHeuristicRulesforEnglishTexts
Table11: DetailsofHeuristicRulesforEnglishTexts
Rule Note
Document-levelFiltering
Meanwordlength[3,10] -
Linesthatendwithanellipsis≤0.2 Definedasellipsis:’...’,’...’,’......’
Linesstartingwithbulletpoint≤0.9 Bulletpoints: ”•”,”•”,”⃝”,”□”,”※”,”·” etc.
Wordsthatcontainnoalphabeticalcharacter≤0.4 -
FractionofStopwordsinthedocumentmustbe≥0.06 -
Numberofstopwordsinthedocumentmustbe≥2 -
Symbolstowordsinthecontentmustbe<0.5 -
Numberofwordsinthecontentafternormalization[50,10000] -
Scoreofthelanguageidentificationmodelmustbe>0.8 Evaluatedbyfasttext
Numberofcharactersmustbe≥200 -
Numberoflines>1 -
Numberofsentences>1 -
Ratioof’’or’’andwordsinbetweenmustbe<0.025 -
’loremipsum’countmustbe<3e−08 -
Numberofsentencesmustbe<7500 -
Wordsonlyconsistofuppercaseletters≤0.1 -
FractionofUniquewords[0.1,+inf) -
Entropyoftheunigramdistributionofthecontentwithin[3,6] -
Fractionoflinesendwith’readmore’≤0.1 -
Fractionofnonconsecutivehashtagsinwords≤0.1 -
Fractionofnonconsecutiveellipsisinwords≤0.1 -
punctuationsinwords>0 -
Non-alphawordsovernon-punctuationwords≤0.2 -
Digitalwordsovernon-punctuationwords≤0.3 -
DuplicatesFiltering
Fractionofcharactersinduplicateword10-grams≤0.10 -
Fractionofcharactersinduplicateword9-grams≤0.11 -
Fractionofcharactersinduplicateword8-grams≤0.12 -
Fractionofcharactersinduplicateword7-grams≤0.13 -
Fractionofcharactersinduplicateword6-grams≤0.14 -
Fractionofcharactersinduplicateword5-grams≤0.15 -
Fractionofcharactersintopword4-grams≤0.16 -
Fractionofcharactersintopword3-grams≤0.18 -
Fractionofcharactersintopword2-grams≤0.20 -
Fractionofduplicatesentences≤0.30 -
Fractionofcharactersinduplicatesentences≤0.20 -
ProhibitedWordsFiltering
Textshouldnotcontainwordsinthebadwordslist Wordsrelatedtopornography,politics,violence,etc.
36A.2 DetailsofHeuristicRulesforChineseTexts
Table12: DetailsofHeuristicRulesforChineseTexts.
Rule Note
DataFormatUnification
Convertfull-anglesymbolstohalf-angle -
URLFiltering
ThetextshouldnotcontainblacklistedURLs BlacklistsobtainedfromBlacklistsUT1.
Removelinksviaregularexpression -
Sentence-levelFiltering
Onlyretainsentenceswithterminalpunctuation Terminalpunctuation:[’.’,’!’,’?’,’......’,’...’].
Excludesentencescontaining”javascript” -
Containatleast3words Wordtokenizationbyjieba.
Excludesentenceswith”loremipsum” -
Excludesentenceswithbadwords Wordsrelatedtopornography,politics,violence,etc.
Document-levelFiltering
Numberofsentences>1 -
Charactersafternormalization[50,10000] -
Meanwordlength[1.3,10] -
Fractionofnonconsecutivehashtags≤0.1 -
Fractionofnonconsecutiveellipsis≤0.1 Definedasellipsis:’...’,’...’,’......’.
Fractionoffullbrackets 【】≤0.1 -
Fractionofdigitalwordsovernon-punctuationwords≤0.3 -
Linesendingwith”readmore”etc.≤0.3 Endingsinclude:”readmore”, ”展开”,”更多”,”。。。”
Linesstartingwithbulletpoint≤0.9 Bulletpoints: ”•”,”•”,”⃝”,”□”,”※”,”·” etc.
Fractionofpunctuationinwords>0 -
Fractionofuniquewords>0.1 -
Entropyofunigramdistribution≥3 -
Textqualityscore>0.4 Evaluatedbyfasttext
DuplicatesFiltering
Fractionofcharactersinduplicateword10-grams≤0.60 -
Fractionofcharactersinduplicateword9-grams≤0.60 -
Fractionofcharactersinduplicateword8-grams≤0.60 -
Fractionofcharactersinduplicateword7-grams≤0.60 -
Fractionofcharactersinduplicateword6-grams≤0.60 -
Fractionofcharactersinduplicateword5-grams≤0.60 -
Fractionofcharactersintopword4-grams≤0.16 -
Fractionofcharactersintopword3-grams≤0.18 -
Fractionofcharactersintopword2-grams≤0.20 -
Fractionofduplicatesentences≤0.30 -
Fractionofcharactersinduplicatesentences≤0.20 -
37A.3 TrainingFrameworkOverflowDetails
Inthissection,weaddressanoverflowproblemwithintheMegatron-core. Theissueariseswhen
the variable num samples, defined as int64 t, exceeds the capacity of int32 t associated with the
sample idx. Thisdiscrepancycanleadtomemoryleaksandundefinedbehavior,asillustratedinFig.
8.
Figure8: Codemodificationforourtrainingframeworkmodification.
A.4 DetailedPromptsinIntermediateCheckpointsEvaluation
Herewepresentthepromptsusedforeachdatasetintheintermediatecheckpointsevaluation. Since
thepromptsfordatasets(BoolQ,PIQA,SIQA,HellaSwag,WinoGrande,ARCeasyandchallenge,
OpenBookQA,CommonsenseQA,MMLU,CEval,CMMLU)areconcatenatedwiththequestions
andtheanswers(options),wehavenotlistedthemseparately. Below,weprovidethepromptsused
forotherdatasets.
BoolQ
HUMAN: Passage: {passage}\nQuestion: {question}?
BOT: Answer: No/Yes
HumanEval
HUMAN: Complete the following python code:\n{prompt}
MBPP
HUMAN: You are an expert Python programmer, and here is your
(cid:44)→ task: Write a function to find the similar elements from
(cid:44)→ the given two tuple lists. Your code should pass these
(cid:44)→ tests:\n\n assert similar_elements((3, 4, 5, 6),(5, 7, 4,
(cid:44)→ 10)) == (4, 5)\n assert similar_elements((1, 2, 3, 4),(5,
(cid:44)→ 4, 3, 7)) == (3, 4) \n assert similar_elements((11, 12,
(cid:44)→ 14, 13),(17, 15, 14, 13)) == (13, 14) \n
38BOT: [BEGIN]\n ’def similar_elements(test_tup1, test_tup2):\r\n
(cid:44)→ res = tuple(set(test_tup1) & set(test_tup2))\r\n return (
(cid:44)→ res)’ \n[DONE] \n\n
HUMAN: You are an expert Python programmer, and here is your
(cid:44)→ task: Write a python function to identify non-prime
(cid:44)→ numbers. Your code should pass these tests:\n\n assert
(cid:44)→ is_not_prime(2) == False \n assert is_not_prime(10) ==
(cid:44)→ True \n assert is_not_prime(35) == True \n
BOT: [BEGIN]\n ’import math\r\ndef is_not_prime(n):\r\n result
(cid:44)→ = False\r\n for i in range(2,int(math.sqrt(n)) + 1):\r\n
(cid:44)→ if n % i == 0:\r\n result = True\r\n
(cid:44)→ return result’ \n[DONE] \n\n
HUMAN: You are an expert Python programmer, and here is your
(cid:44)→ task: Write a function to find the largest integers from
(cid:44)→ a given list of numbers using heap queue algorithm. Your
(cid:44)→ code should pass these tests:\n\n assert heap_queue_largest
(cid:44)→ ( [25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65] \n
(cid:44)→ assert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75,
(cid:44)→ 22, 58],2)==[85, 75] \n assert heap_queue_largest( [25,
(cid:44)→ 35, 22, 85, 14, 65, 75, 22, 58],5)==[85, 75, 65, 58, 35]
(cid:44)→ \n
BOT: [BEGIN]\n ’import heapq as hq\r\ndef heap_queue_largest(nums
(cid:44)→ ,n):\r\n largest_nums = hq.nlargest(n, nums)\r\n return
(cid:44)→ largest_nums’ \n[DONE] \n\n
HUMAN: You are an expert Python programmer, and here is your
(cid:44)→ task: {text} Your code should pass these tests:\n\n {
(cid:44)→ test_list} \n
BOT: [BEGIN]\n
NaturalQuestions
HUMAN: Answer these questions, your answer should be as simple
(cid:44)→ as possible, start your answer with the prompt \’The
(cid:44)→ answer is \’.\nQ: {question}?
BOT: A:
Triviaqa
HUMAN: Answer these questions, your answer should be as simple
(cid:44)→ as possible, start your answer with the prompt \’The
(cid:44)→ answer is \’.\nQ: {question}?
BOT: A:
SQuAD2.0
HUMAN: {context}\nAccording to the above passage, answer the
(cid:44)→ following question. If it is impossible to answer
(cid:44)→ according to the passage, answer ‘impossible to answer‘:\n
(cid:44)→ Question: {question}
BOT: Answer:
MATH
HUMAN: Problem:\nFind the domain of the expression $\\frac{{\
(cid:44)→ sqrt{{x-2}}}}{{\sqrt{{5-x}}}}$.}}\nSolution:"
BOT: The expressions inside each square root must be non-
(cid:44)→ negative. Therefore, $x-2 \ge 0$, so $x\ge2$, and $5 - x
(cid:44)→ \ge 0$, so $x \le 5$. Also, the denominator cannot be
(cid:44)→ equal to zero, so $5-x>0$, which gives $x<5$. Therefore,
(cid:44)→ the domain of the expression is $\\boxed{{[2,5)}}$.\nFinal
(cid:44)→ Answer: The final answer is $[2,5)$. I hope it is
(cid:44)→ correct.\n
39HUMAN: Problem:\nIf $\det \mathbf{{A}} = 2$ and $\det \mathbf{{B
(cid:44)→ }} = 12,$ then find $\det (\mathbf{{A}} \mathbf{{B}}).$\n
(cid:44)→ Solution:
BOT: We have that $\det (\mathbf{{A}} \mathbf{{B}}) = (\det \
(cid:44)→ mathbf{{A}})(\det \mathbf{{B}}) = (2)(12) = \\boxed
(cid:44)→ {{24}}.$\nFinal Answer: The final answer is $24$. I hope
(cid:44)→ it is correct.\n
HUMAN: Problem:\nTerrell usually lifts two 20-pound weights 12
(cid:44)→ times. If he uses two 15-pound weights instead, how many
(cid:44)→ times must Terrell lift them in order to lift the same
(cid:44)→ total weight?\nSolution:
BOT: If Terrell lifts two 20-pound weights 12 times, he lifts a
(cid:44)→ total of $2\cdot 12\cdot20=480$ pounds of weight. If he
(cid:44)→ lifts two 15-pound weights instead for $n$ times, he will
(cid:44)→ lift a total of $2\cdot15\cdot n=30n$ pounds of weight.
(cid:44)→ Equating this to 480 pounds, we can solve for $n$: \\
(cid:44)→ begin{{align*}} 30n&=480\\\\ \Rightarrow\qquad n
(cid:44)→ &=480/30=\\boxed{{16}} \end{{align*}}\nFinal Answer: The
(cid:44)→ final answer is $16$. I hope it is correct.\n
HUMAN: Problem:\nIf the system of equations: \\begin{{align*}} 6
(cid:44)→ x-4y&=a,\\\\ 6y-9x &=b. \end{{align*}}has a solution $(x,
(cid:44)→ y)$ where $x$ and $y$ are both nonzero, find $\\frac{{a
(cid:44)→ }}{{b}},$ assuming $b$ is nonzero.\nSolution:
BOT: If we multiply the first equation by $-\\frac{{3}}{{2}}$,
(cid:44)→ we obtain $$6y-9x=-\\frac{{3}}{{2}}a.$$Since we also know
(cid:44)→ that $6y-9x=b$, we have $$-\\frac{{3}}{{2}}a=b\
(cid:44)→ Rightarrow\\frac{{a}}{{b}}=\\boxed{{-\\frac{{2}}{{3}}}}.
(cid:44)→ $$\nFinal Answer: The final answer is $-\\frac{{2}}{{3}}$.
(cid:44)→ I hope it is correct.\n
HUMAN: Problem:\n{problem}\nSolution:\n
GSM8K
HUMAN: Question: Angelo and Melanie want to plan how many hours
(cid:44)→ over the next week they should study together for their
(cid:44)→ test next week. They have 2 chapters of their textbook to
(cid:44)→ study and 4 worksheets to memorize. They figure out that
(cid:44)→ they should dedicate 3 hours to each chapter of their
(cid:44)→ textbook and 1.5 hours for each worksheet. If they plan
(cid:44)→ to study no more than 4 hours each day, how many days
(cid:44)→ should they plan to study total over the next week if
(cid:44)→ they take a 10-minute break every hour, include 3 10-
(cid:44)→ minute snack breaks each day, and 30 minutes for lunch
(cid:44)→ each day?\nLet’s think step by step\nAnswer: ,
BOT: Angelo and Melanie think they should dedicate 3 hours to
(cid:44)→ each of the 2 chapters, 3 hours x 2 chapters = 6 hours
(cid:44)→ total.\nFor the worksheets they plan to dedicate 1.5 hours
(cid:44)→ for each worksheet, 1.5 hours x 4 worksheets = 6 hours
(cid:44)→ total.\nAngelo and Melanie need to start with planning 12
(cid:44)→ hours to study, at 4 hours a day, 12 / 4 = 3 days.\n
(cid:44)→ However, they need to include time for breaks and lunch.
(cid:44)→ Every hour they want to include a 10-minute break, so 12
(cid:44)→ total hours x 10 minutes = 120 extra minutes for breaks.\n
(cid:44)→ They also want to include 3 10-minute snack breaks, 3 x
(cid:44)→ 10 minutes = 30 minutes.\nAnd they want to include 30
(cid:44)→ minutes for lunch each day, so 120 minutes for breaks +
(cid:44)→ 30 minutes for snack breaks + 30 minutes for lunch = 180
(cid:44)→ minutes, or 180 / 60 minutes per hour = 3 extra hours.\nSo
(cid:44)→ Angelo and Melanie want to plan 12 hours to study + 3
40(cid:44)→ hours of breaks = 15 hours total.\nThey want to study no
(cid:44)→ more than 4 hours each day, 15 hours / 4 hours each day =
(cid:44)→ 3.75\nThey will need to plan to study 4 days to allow for
(cid:44)→ all the time they need.\nThe answer is 4,
HUMAN :Question: Mark’s basketball team scores 25 2 pointers, 8
(cid:44)→ 3 pointers and 10 free throws. Their opponents score
(cid:44)→ double the 2 pointers but half the 3 pointers and free
(cid:44)→ throws. What’s the total number of points scored by both
(cid:44)→ teams added together?\nLet’s think step by step\nAnswer:,
BOT: Mark’s team scores 25 2 pointers, meaning they scored
(cid:44)→ 25*2= 50 points in 2 pointers.\nHis team also scores 6 3
(cid:44)→ pointers, meaning they scored 8*3= 24 points in 3
(cid:44)→ pointers\nThey scored 10 free throws, and free throws
(cid:44)→ count as one point so they scored 10*1=10 points in free
(cid:44)→ throws.\nAll together his team scored 50+24+10= 84 points\n
(cid:44)→ Mark’s opponents scored double his team’s number of 2
(cid:44)→ pointers, meaning they scored 50*2=100 points in 2
(cid:44)→ pointers.\nHis opponents scored half his team’s number of
(cid:44)→ 3 pointers, meaning they scored 24/2= 12 points in 3
(cid:44)→ pointers.\nThey also scored half Mark’s team’s points in
(cid:44)→ free throws, meaning they scored 10/2=5 points in free
(cid:44)→ throws.\nAll together Mark’s opponents scored 100+12+5=117
(cid:44)→ points\nThe total score for the game is both team’s
(cid:44)→ scores added together, so it is 84+117=201 points\nThe
(cid:44)→ answer is 201\n,
HUMAN: Question: Bella has two times as many marbles as
(cid:44)→ frisbees. She also has 20 more frisbees than deck cards.
(cid:44)→ If she buys 2/5 times more of each item, what would be
(cid:44)→ the total number of the items she will have if she
(cid:44)→ currently has 60 marbles?\nLet’s think step by step\nAnswer
(cid:44)→ :,
BOT: When Bella buys 2/5 times more marbles, she’ll have
(cid:44)→ increased the number of marbles by 2/5*60 = 24\nThe total
(cid:44)→ number of marbles she’ll have is 60+24 = 84\nIf Bella
(cid:44)→ currently has 60 marbles, and she has two times as many
(cid:44)→ marbles as frisbees, she has 60/2 = 30 frisbees.\nIf Bella
(cid:44)→ buys 2/5 times more frisbees, she’ll have 2/5*30 = 12
(cid:44)→ more frisbees.\nThe total number of frisbees she’ll have
(cid:44)→ will increase to 30+12 = 42\nBella also has 20 more
(cid:44)→ frisbees than deck cards, meaning she has 30-20 = 10 deck
(cid:44)→ cards\nIf she buys 2/5 times more deck cards, she’ll have
(cid:44)→ 2/5*10 = 4 more deck cards.\nThe total number of deck
(cid:44)→ cards she’ll have is 10+4 = 14\nTogether, Bella will have
(cid:44)→ a total of 14+42+84 = 140 items\nThe answer is 140\n,
HUMAN: Question: A group of 4 fruit baskets contains 9 apples,
(cid:44)→ 15 oranges, and 14 bananas in the first three baskets and
(cid:44)→ 2 less of each fruit in the fourth basket. How many
(cid:44)→ fruits are there?\nLet’s think step by step\nAnswer:,
BOT: For the first three baskets, the number of apples and
(cid:44)→ oranges in one basket is 9+15=24\nIn total, together with
(cid:44)→ bananas, the number of fruits in one basket is 24+14=38
(cid:44)→ for the first three baskets.\nSince there are three
(cid:44)→ baskets each having 38 fruits, there are 3*38=114 fruits
41(cid:44)→ in the first three baskets.\nThe number of apples in the
(cid:44)→ fourth basket is 9-2=7\nThere are also 15-2=13 oranges in
(cid:44)→ the fourth basket\nThe combined number of oranges and
(cid:44)→ apples in the fourth basket is 13+7=20\nThe fourth basket
(cid:44)→ also contains 14-2=12 bananas.\nIn total, the fourth
(cid:44)→ basket has 20+12=32 fruits.\nThe four baskets together
(cid:44)→ have 32+114=146 fruits.\nThe answer is 146\n,
HUMAN: Question: {question}\nLet’s think step by step\nAnswer:
TheoremQA
HUAMN: You are a mathematician, you are supposed to answer the
(cid:44)→ given question. You need to output the answer in your
(cid:44)→ final sentence like "Therefore, the answer is ...". The
(cid:44)→ answer can only be one of the following forms:\n1. a
(cid:44)→ numerical value like 0.1, no symbol and no unit at all.\n
(cid:44)→ 2. a list of number like [2, 3, 4].\n3. True/False.\n4. an
(cid:44)→ option like (a), (b), (c), (d)\nQuestion: {Question}\nLet\’
(cid:44)→ s think step by step.
42A.5 DetailedResults
TheevaluationresultsofallintermediatecheckpointsareobtainedusingtheOpenCompassframe-
work[26].
Table13: Thistableshowcasesevaluationresultsacrossavarietyofdatasetsformodelstrainedwith
differenttokenamounts,rangingfrom20Bto1859.86B.Additionally,resultsformodelstrained
with2099.84Bto3726.33BtokenscanbefoundinTable14. “Avg”representstheaverageoverthe
benchmark. The“*”symbolreferstosubsetswithintheMMLU,CMMLU,andC-Eval.
Dataset 20.00B 60.00B 99.99B 359.97B 599.95B 859.93B 1099.91B 1299.90B 1599.88B 1859.86B
StandardBenchmarks
BoolQ 58.81 60.28 58.96 61.9 61.62 62.29 62.35 63.67 59.02 61.35
PIQA 67.25 70.35 73.56 76.06 76.12 77.64 77.75 77.58 77.58 77.91
SIQA 38.33 41.04 40.43 41.71 41.4 42.48 42.99 42.99 42.43 44.06
HellaSwag 32.53 47.07 52.03 61.32 63.61 64.83 65.75 66.11 67.35 67.69
WinoGrande 52.09 53.12 53.2 55.25 55.41 57.38 57.93 58.09 58.09 59.67
ARC-e 35.27 43.39 51.15 57.5 57.32 57.5 58.02 58.91 62.08 60.14
ARC-c 23.39 20 23.73 29.49 28.14 31.86 32.2 32.2 32.54 33.22
OpenBookQA-Fact 26.2 23.8 23.8 28.8 43.6 48.6 51.8 59.6 62 70
CommonsenseQA 34.32 48.32 51.43 59.54 61.43 63.72 66.09 64.95 65.19 65.44
MMLU-AVG 24.8 24.38 26.72 36.06 43.92 47.32 47.96 50.65 51.18 51.95
*-humanities 24.5 25.25 26.71 37.5 44.58 49.26 50.58 52.92 53.62 54.29
*-stem 24.4 23.26 26.76 30.83 36.82 39.98 40.89 42.7 42.72 44.24
*-social-science 22.8 23.58 26.9 39.7 49.07 53.77 53.71 57.75 58.93 58.73
*-other 27.52 25.87 26.52 38.89 48.9 50.15 50.37 53.42 53.94 54.62
CodeGeneration
HumanEval 0.61 2.44 4.27 6.1 7.32 7.93 7.32 7.32 9.15 6.1
MBPP 0 0.4 0 3.4 6.6 6.4 9.2 9.4 8.8 6.6
WorldKnowledge
NQ 0.08 1.55 2.8 5.1 5.79 7.51 7.84 9.34 9.03 8.01
TriviaQA 1.2 6.9 9.54 19.64 25.97 22.24 28.6 28.22 34.19 31.31
ReadingComprehension
SQuAD2.0 4.54 15.94 24.2 27.06 31.05 31.48 30.68 12.56 31.35 25.76
Exams
MATH 0.6 1.22 1.16 2.62 2.8 3.18 3.6 3.82 3.44 4.24
GSM8k 1.59 0.76 0.99 4.09 7.66 9.78 12.05 11.52 15.24 14.48
TheoremQA 0 0 0.5 0.75 1.38 1.5 1.38 0.75 0.62 0.38
Chinese
C-EVAL-AVG 24.87 24.66 25.48 36.55 44.3 46.9 50.01 52.1 52.4 52.95
*-stem 26.8 24.04 24.47 31.43 35.45 38.5 39.86 42.67 45.14 45.49
*-social-science 26.99 29.16 27.19 47.15 56.78 61.6 66.94 68.71 68.29 67.01
*-humanities 24.5 22.64 25.42 41.47 49.66 49 53.04 58.19 56.2 58.41
*-other 19.82 23.72 25.82 31.29 43.67 46.71 50.04 48.06 47.33 48.28
*-hard 30.97 23.78 21.87 25.69 28.04 31.1 36.06 37.5 33.66 38.08
CMMLU-AVG 25.11 25.18 25.96 35.48 42.93 47.54 48.85 50.14 50.94 52.18
*-humanities 25.54 25.62 25.79 38.44 47.19 50.58 51.76 54.35 54.22 55.55
*-stem 24.96 24.26 25.15 28.82 34.34 38.7 39.26 39.23 40.92 42.79
*-social-science 25.05 25.91 26.78 38.72 46.14 51.96 53.18 54.53 55.26 56.85
*-other 24.99 24.76 25.83 35.69 44.29 48.44 50.83 52.41 53.11 53.05
*-china-specific 24.4 25.62 25.14 38.02 43.86 48.96 50.14 52.46 53.15 54.03
43Table 14: This table showcases evaluation results across a variety of datasets for models trained
withdifferenttokenamounts,rangingfrom2099.84Bto3726.33B.Additionally,resultsformodels
trainedwith20Bto1859.86BtokenscanbefoundinTable13. “Avg”representstheaverageoverthe
benchmark. The“*”symbolreferstosubsetswithintheMMLU,CMMLU,andC-Eval.
Dataset 2099.84B 2359.82B 2599.80B 2859.78B 3099.76B 3299.74B 3599.72B 3726.33B
StandardBenchmarks
BoolQ 60.89 63.12 59.36 64.56 63.67 63.18 65.35 66.09
PIQA 77.91 78.02 78.35 78.56 78.94 78.67 78.13 78.29
SIQA 44.06 44.06 43.3 43.71 44.01 43.45 44.63 43.19
HellaSwag 68.6 68.52 69.04 69.78 70.06 70.02 70.46 70.17
WinoGrande 58.96 59.83 59.91 60.06 61.25 59.75 59.67 60.46
ARC-e 62.43 62.61 63.32 61.73 61.73 62.26 62.43 64.02
ARC-c 23.39 20 37.29 35.93 35.59 36.95 34.58 34.58
OpenBookQA-Fact 63.8 61.6 60 66 59.6 59.4 69 62.2
CommonsenseQA 66.42 65.77 67.32 67.98 67.57 67.57 67.81 67.73
MMLU-AVG 52.72 53.25 53.93 54.71 55.34 55.8 55.42 55.91
*-humanities 54.18 56.75 57.2 57.25 58.34 58.19 58.71 59.22
*-stem 44.48 44.08 44.19 45.56 45.92 46.53 46.42 46.37
*-social-science 60.72 61.02 61.87 63.45 63.87 64.86 62.63 63.72
*-other 55.9 55.96 57.58 57.49 58.23 58.61 58.61 59.33
CodeGeneration
HumanEval 8.54 3.66 6.71 6.71 7.32 3.66 9.76 9.15
MBPP 8.4 9.4 8.8 8.6 8.8 8.8 9.2 9
WorldKnowledge
NQ 10.97 10.19 10.03 11.77 10.66 12.63 11.44 11.27
TriviaQA 36.53 31.06 37.9 39.29 40.81 41.27 41.08 39.54
ReadingComprehension
SQuAD2.0 25.29 26.98 11.35 5.13 6.18 16.68 15.55 8.72
Exams
MATH 4.84 4.34 4.94 5.36 5.6 5.72 5.9 5.76
GSM8k 14.94 17.36 17.29 18.95 19.18 19.79 19.11 21.3
TheoremQA 1.38 0.5 1 3 2.38 2 1.5 2.5
Chinese
C-EVAL-AVG 52.52 55.62 57.4 57.03 56.02 57.57 58.1 57.13
*-stem 44.77 49.52 51.84 49.08 46.52 50.26 50.26 49.47
*-social-science 66.71 70.3 71.62 71.43 70.74 73.33 73.05 72.61
*-humanities 58.08 58.6 61.96 62.09 61.14 60.96 61.46 62.06
*-other 48.18 50.39 50.03 53.35 54.79 53.12 55.41 52.03
*-hard 34.8 39.89 44.08 39.87 36.42 41.26 38.64 40.47
CMMLU-AVG 52.45 54.79 56.15 56.63 57.33 58.11 57.7 58.32
*-humanities 56.42 60.23 60.97 61.09 63 63.69 63.73 65.04
*-stem 42.17 44.38 45.95 46.16 46.54 47.82 46.63 47.91
*-social-science 57.34 59.26 60.42 61.79 62.27 62.93 62.83 63.25
*-other 53.51 55.34 57.27 57.09 57.4 57.84 57.5 57.05
*-china-specific 54.82 56.86 58.15 58.07 59.51 60.47 60.74 60.29
44Table15: Thistableshowstheevaluationresultsacrossavarietyofdatasetsformodelsofdifferent
train tokens in the decay phase, from 62.91B to 723.52B. “Avg” represents the average over the
benchmark. The“*”symbolreferstosubsetswithintheMMLU,CMMLU,andC-Eval.
Dataset 62.91B 104.86B 199.23B 293.60B 419.43B 524.29B 639.63B 723.52B
StandardBenchmarks
BoolQ 52.51 50.73 47 63.7 65.38 78.32 70.34 81.07
PIQA 75.41 75.41 75.73 76.71 77.04 75.9 75.3 76.55
SIQA 48.36 49.13 50.31 50.31 51.28 69.45 68.73 68.22
HellaSwag 62.79 63.98 65.19 66.17 66.43 69.57 70 70.74
WinoGrande 62.04 63.69 64.01 65.75 66.06 59.43 59.59 59.83
ARC-c 58.64 60.34 63.73 61.36 68.47 45.42 63.39 68.14
OpenBookQA-Fact 75.6 76.2 77.2 74.2 79 79.6 73.4 82
CommonsenseQA 60.36 63.06 63.72 64.54 63.14 68.96 69.7 69.94
MMLU-AVG 52.53 53.31 54.83 55.51 56.11 57.17 57.36 58.14
*-humanities 54.59 57.44 57.8 58.12 59.5 60.76 59.77 60.7
*-stem 45.68 45.58 48.37 47.29 48.48 49.82 49.31 49.84
*-social-science 59.6 60.69 61.19 63.95 64.45 64.78 65.27 66.78
*-other 53.94 53.65 55.43 57.14 56.17 57.31 59.42 59.73
CodeGeneration
HumanEval 6.1 7.32 3.05 11.59 0.61 21.95 20.12 24.39
MBPP 20.8 25 24.8 28.2 28.4 27 27.8 27
WorldKnowledge
NQ 4.04 6.4 5.43 5.04 3.21 9.94 8.23 9.97
TriviaQA 15.27 27.31 24.76 34.32 44.03 37.5 32.66 42.36
ReadingComprehension
SQuAD2.0 33.72 13.57 27.1 30.89 16.8 29.56 19.37 30.98
Exams
MATH 6.62 8.62 10.08 12.88 12.24 14.06 14.12 14.66
GSM8K 18.35 37.83 41.85 45.03 49.43 50.64 53.37 52.01
Chinese
C-EVAL-AVG 48.96 51.29 53.66 54.96 55.71 57.58 54.25 57.68
*-stem 43.61 45.69 49.82 47.14 49.71 52.12 45.77 50.35
*-social-science 60.77 66.43 66.94 69 67.7 70.33 71.08 70.23
*-humanities 50.55 51.2 53.26 60.74 59.11 63.29 59.05 63.49
*-other 46.37 47.77 48.95 50.63 52.3 50.22 49.55 53.78
*-hard 35.01 38.33 42.48 39.61 41.27 46.02 37.02 41.07
CMMLU-AVG 48.03 49.1 51.37 53.26 53.32 54.48 54.59 55.1
*-humanities 52.86 53.77 56.23 58.01 59.12 60.08 61.75 62.24
*-stem 39.16 40.38 43.52 43.64 44.01 45.89 45.23 45.62
*-social-science 52.01 52.89 55.19 57.57 57.53 58.36 58.31 59.39
*-other 48.04 49.37 50.45 53.72 52.65 53.66 53.55 53.39
*-china-specific 47.63 48.99 51.51 52.74 53.57 54.78 54.87 55.84
45(a)MMLU (b)CEval (c)CMMLU
(d)Hellaswag (e)GSM8K (f)ARC-c
Figure9: PerformanceoffundamentalphaseintermediatecheckpointsonMMLU,CEvel,CMMLU,
Hellaswag,GSM8K,andARC-c.
A.6 DetailsofOpenSourceDatasetsUsedinPre-training
A.7 DetailedCompressionRate
A.8 AdditionalExperimentalResultsinScalingLaw
1000M params OLMo model 7000M params OLMo model
3.4 Loss Value Loss Value
Chinchilla Law Chinchilla Law
3.2 NEO Scaling Law NEO Scaling Law
3.0
2.8
2.6
2.4
2.2
2.0
1.8
0 500 1000 1500 2000 2500 3000 0 500 1000 1500 2000 2500
# Tokens (B) # Tokens (B)
Figure10: Thelosscurve,ChinchillaLawpredictionandtheNEOScalinglawpredictionofthe
OLMoLLM.Weuselossvaluesfromboth1Band7Bforfittingandprediction.
Fromfigure10,WecanobservethattheChinchillalawalreadyprovidesagoodfitforOLMoand
doesnotunderestimatethelosswhenthemodelparametersarelargebutthedatavolumeissmall
likeMAP-Neo7BandDeepSeek67B.Suchaphenomenonmightbeduetothedistributionofthe
pre-trainingdataset. Deepseek’spre-trainingdatadistributioncloselyresemblesthatofNEO,with
ahigherproportionofChinesedata,codedata,andhigh-qualityfiltereddatacomparedtoOLMo
whosepre-trainingdatainEnglish
TheChinchillascalinglawisoriginallyformulatedforscenarioswherethetrainingdataisrelatively
homogeneousandprimarilyEnglish-centric.Ittendstoperformwellundertheseconditions.However,
whenthetrainingdatasetsizeissmaller(e.g.,significantlylessthan500billiontokens)andthemodel
parametercountishigh(e.g.,7billionormore),thediversityofthedataleadstoaslowerreduction
inlossthanpredictedbyChinchilla. Conversely,withlargerdatasets(e.g.,greaterthan1.5trillion
tokens),thediversitycontributestoacontinueddecreaseinloss,divergingfromtheflatteningand
lower-boundedtrajectorysuggestedbythe B termintheChinchillalaw. Currentevidenceislimited
Dβ
asfewmodelsarepre-trainedacrossmultiplelargehigh-qualitycorpora. YiandQwen[7]undergo
46
seulaV
ssolTable16: Listofopen-sourcedatasetsusedduringpretraining.
Dataset URL
Agent-FLAN[17] https://huggingface.co/datasets/internlm/Agent-FLAN
ChatDoctor-HealthCareMagic-100k https://huggingface.co/datasets/lavita/ChatDoctor-HealthCareMagic-100k
Fandom23K[82] https://huggingface.co/datasets/RyokoAI/Fandom23K
LoC-PD-Books https://huggingface.co/datasets/storytracer/LoC-PD-Books
MNBVC https://huggingface.co/datasets/liwu/MNBVC
Refined-Anime-Text https://huggingface.co/datasets/CausalLM/Refined-Anime-Text
SKGInstruct-skg-only[127] https://huggingface.co/datasets/TIGER-Lab/SKGInstruct-skg-only
US-PD-Books https://huggingface.co/datasets/storytracer/US-PD-Books
UltraTextbooks https://huggingface.co/datasets/Locutusque/UltraTextbooks
bigpatent[90] https://huggingface.co/datasets/big_patent
cleannotebooksfiltered https://huggingface.co/datasets/vikp/clean_notebooks_filtered
librechemtextbooks https://huggingface.co/datasets/Hack90/libre_chem_textbooks
mentalhealthchatbotdataset https://huggingface.co/datasets/heliosbrahma/mental_health_chatbot_dataset
mini-peS2o https://huggingface.co/datasets/nampdn-ai/mini-peS2o
textbooks https://huggingface.co/datasets/open-phi/textbooks
pile-of-law[39] https://huggingface.co/datasets/pile-of-law/pile-of-law
prepared-automathtext https://huggingface.co/datasets/Locutusque/prepared-automathtext
scimag https://scimag.github.io/sciMAG2015/
textbookqualityprogramming https://huggingface.co/datasets/vikp/textbook_quality_programming
textbooks https://huggingface.co/datasets/open-phi/textbooks
tiny-strange-textbooks[71] https://huggingface.co/datasets/nampdn-ai/tiny-strange-textbooks
COIG-PC[121] https://huggingface.co/datasets/BAAI/COIG-PC
FinCorpus https://huggingface.co/datasets/Duxiaoman-DI/FinCorpus
archive https://huggingface.co/datasets/linux-cn/archive
medical https://huggingface.co/datasets/shibing624/medical
AutoMathText[123] https://huggingface.co/datasets/math-ai/AutoMathText
BioInstructQA https://huggingface.co/datasets/BioMistral/BioInstructQA
SMolInstruct[114] https://huggingface.co/datasets/osunlp/SMolInstruct
cosmopedia[8] https://huggingface.co/datasets/HuggingFaceTB/cosmopedia
starcoder[54] https://huggingface.co/datasets/bigcode/starcoderdata
the-stack-v2-train-full-ids[68] https://huggingface.co/datasets/bigcode/the-stack-v2-train-full-ids
flanv2[67] https://github.com/google-research/FLAN/tree/main/flan/v2
open-web-math[73] https://huggingface.co/datasets/open-web-math/open-web-math
Table17: Theterm“nROUND”indicatesthenumberofroundsforeachdataset.
Dataset Language UsedduringtheFundamentalPhase UsedduringtheDecayPhase
MNBVC(govreport) Chinese 2ROUND -
US-PD-Books English 1ROUND 1ROUND
MNBVC(lawjudgement) Chinese 2ROUND -
cosmopedia English - 2ROUND
AutoMathText English 1ROUND 2ROUND
BioInstructQA English 1ROUND 2ROUND
SMolInstruct English 1ROUND 2ROUND
Agent-FLAN English - 2ROUND
MNBVC(govxuexiqiangguo) Chinese 2ROUND -
open-web-math English 1ROUND 2ROUND
TheStack Code 2ROUND -
multi-stagepre-trainingwithasignificantlysmallerinitialtrainingcorpuscomparedtoMAP-Neo
andDeepSeek,whileOpenLLaMA[34]lackssmaller-scaledatatovalidatetheseobservations.
47A.9 CompressionRate
Table18: DetailedCompressionRatesbyCategoryandDataset.
Category Dataset CompressionRate
SampledCode(cpp) 2.988
SampledCode(Java) 3.301
SampledCode(All) 3.355
SampledGithub 2.988
Code
SampledCode(Other) 3.426
CodeGPT-CN 2.458
SampledLeetCode 2.050
TheStackV1 3.041
COIG-PC 1.835
SampledNovel 1.284
HQ cn SampledReferenceBook 1.240
ExamsHighQuality 2.290
ZhihuHighQuality 1.377
ZhihuInstruction 1.434
ArxivHighQuality 2.976
SampledNewsPaper 3.613
SampledEnglishBooks 2.079
HQ en
flan v2 3.645
HuggingfaceWiki 3.520
UltraTextbooks 4.030
AutoMathText 2.756
BioInstructQA 3.284
Others Syntheticscienceexaminstruction 1.508
open-web-math 3.263
SMolInstruct 1.978
Web cn CommonCrawl 1.418
Web en CommonCrawl 3.699
A.10 OCRPostProcessing
48Table19: TheOCRprompttemplateswithDemonstrationsinChineseandEnglish
PromptTemplateforOCRPost-processing
➤PromptTemplates
PromptforEnglishContents PromptforChineseContents
From an original document using OCR technol- 请扮演一个AI校对员，我需要你的专业技能
ogy,theremaybeerrorsincharacterrecognition, 来帮助我校对一段文本。这段文本是我通
potentiallyincludingspellingmistakes,grammat- 过OCR技术从一份原始文档中提取出来的，
ical errors, incorrect punctuation, or formatting 我怀疑在字符识别的过程中可能发生了一些
issues. Payspecialattentiontomisplacedspaces 错误。具体来说，可能存在拼写错误、语法
andlinebreaksthatoftenoccurinOCR-generated 错误、标点用错或者格式排列问题。请特别
content. I need you to reorganize the paragraph 注意生成的内容中有很多识别错误的空格与
intoaproperlyformattedandsemanticallycoher- 换行符。请将段落整理成正确的语义通顺的
ent form. Here’s the text I’ve provided. Kindly 格式。输出格式应为组织完善的Markdown内
check and correct it meticulously. Please output 容。不能改变语言，即不能将中文内容改为
onlytherevisedtextwithoutincludinganyaddi- 英文。一些内容是混合语言的，即中文主
tionalcontenti.e. anycommentsfromyou. The 要内容夹杂英文符号，请按照原段落位置的
output format should be a well-organized mark- 语言输出。下面是我提供的文本内容，请你
down content. Do not change the language, i.e. 帮我仔细检查并校对，请直接输出修订后文
DonotchangeChinesecontenttoEnglish. Some 本，并不要包含其他内容。{内容} 你应该输
contents are mixed language i.e. Chinese main 出：
contentwithEnglishsymbols. Also,donotchange
theoriginallanguage. Pleasedonotgenerateany
unrelatedadditionalcomments! Here’soneofthe
texts that needs to be processed: {content} You
shouldoutput:
➤Demonstrations
EnglishContentBeforePost-processing EnglishContentAfterPost-processing
TheDevelopingPathofCivilServant TheDevelopingPathoftheCivilServantSystem
SysteminChina：BasedonComprehen inChina: BasedonComprehensiveInterpretation
siveInterpretationofCivilServantLa oftheCivilServantLawAbstract: TheCivilSer-
wAbstract：CivilServantLaWiSthef vantLawisthefirstcomprehensivelawonhuman
h-stcomprehensivelawofhumanmanag resourcemanagementforcivilservantsinourcoun-
ementofcivilservantinourcountry．The try. Thecivilservantsystemhasmadeagreatleap
civilservantsystemhasundergoneagreatleap forwardfromtheTemporaryRegulationofCivil
forwardfromTemporaryRegulationofCivilS ServantstotheCivilServantLaw. Comparedto
ervantstoC}vilServantLaw．Compar the Temporary Regulation of Civil Servants, the
ingtotheTemporaryRegulationC}vilS Civil Servant Law contains many new contents,
ervants，CilServantWhasmanynew includingnewconnotationsofconceptsandsome
contentsincludingnewconnotationoftheco newrulesthatarewrittenintothelawforthefirst
nceptsandsomenewrulesthatarewritteninto time. Therearealsosomeadjustmentstothefor-
thelawforthefirsttime．TherearealSOsome merarticles. Keywords: Civilservant;Developing
adiustmentstotheformerarticles．KeyWords path;CivilServantLaw112
：Civilservant；DevelopingPath；Civi
lServantLaw112
ChineseContentBeforePost-processing ChineseContentAfterPost-processing
路上 只我 一个人， 背着手踱着。 路上只我一个人，背着手踱着。这一片天地
这一片天地好像 是 我的 ；我 也像 超 好像是我的；我也像超出了平常的自己，到
出了平常的自己，到 了 另一世界里。 了另一世界里。我爱热闹，也爱冷静；爱群
我爱热闹， 也爱 冷静；爱群居， 也爱 居，也爱独处。像今晚上，一个人在这苍茫
独处。像今晚上， 一个人 在这苍茫的月 的月下，什么都可以想，什么都可以不想，
下，什么都可以 想，什么都可以不 想 便觉是个自由的人。白天里一定要做的事，
，便觉是个自由的人。白天里一定要 做 一定要说的话，现在都可不理。这是独处的
的 事，一定要说的话，现在 都 可不 妙处，我且受用这无边的荷香月色好了。
理。这是 独处的妙处，我且 受用这无
边的荷香月色 好了。
49