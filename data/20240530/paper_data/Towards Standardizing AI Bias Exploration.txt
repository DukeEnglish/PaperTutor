Towards Standardizing AI Bias Exploration
EmmanouilKrasanakis*, SymeonPapadopoulos
CentreforResearch&TechnologyHellas,6thkmCharilaou-Thermi,Thessaloniki,Greece,57001
Abstract
CreatingfairAIsystemsisacomplexproblemthatinvolvestheassessmentofcontext-dependentbias
concerns.Existingresearchandprogramminglibrariesexpressspecificconcernsasmeasuresofbias
thattheyaimtoconstrainormitigate. Inpractice,oneshouldexploreawidevarietyof(sometimes
incompatible)measuresbeforedecidingwhichoneswarrantcorrectiveaction,buttheirnarrowscope
meansthatmostnewsituationscanonlybeexaminedafterdevisingnewmeasures.Inthiswork,we
presentamathematicalframeworkthatdistilsliteraturemeasuresofbiasintobuildingblocks,hereby
facilitating new combinations to cover a wide range of fairness concerns, such as classification or
recommendationdifferencesacrossmultiplemulti-valuesensitiveattributes(e.g.,manygendersand
races,andtheirintersections).Weshowhowthisframeworkgeneralizesexistingconceptsandpresent
frequentlyusedblocks.Weprovideanopen-sourceimplementationofourframeworkasaPythonlibrary,
calledFairBench,thatfacilitatessystematicandextensibleexplorationofpotentialbiasconcerns.
Keywords
Measuringbias,Auditingtools,Algorithmicframeworks,Multidimensionalbias
1. Introduction
ArtificialIntelligence(AI)systemsseewidespreadadoptionacrossmanyapplicationsthataffect
people’slives. Sincetheytendtopickupandexacerbatereal-worldbiasesordiscrimination,
aswellasspuriouscorrelationsbetweenpredictedvaluesandsensitiveattributes(e.g.,gender,
race,age,financialstatus),makingthemfairisasubjectofintensiveresearchandregulatory
efforts. These include quantification of bias concerns through appropriate measures so that
unfairbehaviorcanbedetectedandcorrected. Tothisend,severalmeasuresofbiashavebeen
proposedinresearchpapersandimplementedasreusablecomponentswithinprogramming
librariesortoolkits(Section2). Recognizingthatbiasand,moregenerally,fairnessdependson
thesocialcontextandtheparticularsettingsinwhichAIsystemsaredeployed,eachcreated
measureislimitedtoassessingadifferenttypeofconcern. Intheend,researchanddevelopment
focusesonmitigatingorconstrainingmeasureswhenthoserevealfairnessissues,butnoton
howasystematicexplorationofmanymeasurescouldbecarriedouttoperformfairnessaudits.
Therefore, and despite the obvious value of presenting reusable algorithmic solutions to
specificproblems,thereisaneedformethodsthatcriticallyexaminereal-worldsystemsacross
awiderangeofconcernsandnotjustafewofthem. Themainbarrierinpursuingsuchmethods
is that measures of bias are designed in a monolithic manner and rarely consider how they
AIMMES2024WorkshoponAIbias:Measurements,Mitigation,ExplanationStrategies|co-locatedwithEUFairness
ClusterConference2024,Amsterdam,Netherlands
*Correspondingauthor.
$maniospas@iti.gr(E.Krasanakis);papadop@iti.gr(S.Papadopoulos)
(cid:26)0000-0002-3947-222X(E.Krasanakis);0000-0002-5441-7341(S.Papadopoulos)
©2024Copyrightforthispaperbyitsauthors.UsepermittedunderCreativeCommonsLicenseAttribution4.0International(CCBY4.0).
4202
yaM
92
]GL.sc[
1v22091.5042:viXracouldbegeneralizedorportedtodifferentsettings. Forexample,differentialfairness[1]was
onlyrecentlyproposedasameansofgeneralizingdisparateimpactassessmenttointersectional
fairness,whichrecognizesthecumulativeeffectofsensitiveattributes(e.g.,multiplegenders
andraces),despitedisparateimpactmeasuresbeingaroundfordecades[2].
Inthiswork,weassistsystematicexplorationoffairnessconcernsbydecomposingmeasures
ofbiasintosimplebuildingblocks. Thesecanberecombinedtocreatenewmeasurescovering
awiderangeofcontextsandconcerns. Forexample,ablockthataggregatesclassificationbias
inmultidimensionalsettings(e.g.,withmultipleracesandgenders)canbecombinedwitha
blockthatanothermeasureusestoassessrecommendationbiasbetweenonlytwogroupsof
people(e.g.,onlywhitesvsnon-whites)tocreateanewmeasurethatassessesmultidimensional
recommendation bias. We implement the proposed framework in a Python library, called
FairBench, that standardizes how measures of bias are defined by combining interoperable
blocksofeachkind. Thelibrary’sfunctionalinterfacesetsupafixedrepresentationofexisting
andnewmeasuresofbias,andcancreatebiasreportswhiletracingprospectivefairnessissues
totherawquantitiescomputedoverpredictiveoutcomes. Ourcontributionisthreefold:
a) Wepresentamathematicalframeworkthatsystematicallycombinesbuildingblocksto
constructmanyexistingandnewmeasuresofbias.
b) Weexpressseveralbuildingblocksofliteraturemeasuresofbiaswithinthisframework.
c) WeintroducetheFairBenchPythonlibrarythatimplementstheaboveblocksandcombina-
tionmechanismstocomputemeasuresofbiasinawiderangeofcomputingenvironments.
This paper is structured as follows. After this section’s introduction, Section 2 presents
theoretical background and related work. Section 3 describes our proposed mathematical
framework. Section 4 extracts several bias building blocks from the AI fairness literature.
Section5introducestheprogramminginterfaceprovidedbyourFairBenchPythonlibraryto
explorebias. Finally,Section6summarizesourworkandpointstofuturedirections.
2. Background and Related Work
2.1. FairAI
The problem of creating fair AI systems is the subject has attracted attention as part of the
broaderthemeofTrustworthyAIinworldwideregulationandethicalguidelines,liketheEU’s
AssessmentListforTrustworthyAI[3],andtheNIST’sAIRiskManagementFramework[4].
Evaluatingsystemfairnessisacomplexproblemthatdependsonthecontextbeingexamined
andthesystemsbeingcreated. Apartofansweringthisproblemconsistsofmathematicalor
algorithmicexplorationthatenablesautomatedsystemassessmentandoversightwithpractices
likeTrustAIOps[5],whichmonitortheevolutionofsystembiasinthedeploymentcontext.
Mathematicaldefinitionsoffairnessareoftencategorizedintothefollowingtypes[6,7,8,9]:
i) Groupfairnessfocusesonequaltreatmentbetweenpopulationgroupsorsubgroups. This
isthesubjectofmostresearchandcoveredextensivelyinthenextsubsection.
ii) Individual fairness [10] focuses on the fair treatment of individuals, for example by
obtaining similar predictions for those with similar features. This can be modelled as
groupfairnesstoo,byconsideringeverypersontobelongtotheirowngroup.iii) Counterfactualfairness[11,12]learnscausalmodelsofpredictivemechanismsthataccount
for discrimination and then makes predictions in a would-be fair reality. Although
originally coined as a variation of individual fairness, recent understanding [13] also
suggeststhatcounterfactualfairnessisakindofgroupfairness.
Making(e.g.,training)AItobefairtypicallyinvolvesmeasuresofbiasthatquantifynumerical
deviation from exact definitions of fairness. Measures are either minimized or subjected to
constraints[14],butidentifyingwhichareimportantisnotonlyamatterofapplyingscientific
principles. Inparticular,itismathematicallyimpossibletosimultaneouslysatisfyallconceivable
definitions of fairness [15, 16], which means that systems should only address the fairness
concernsthatmattertohumans(e.g.,stakeholdersorpolicymakers)intheparticularsituation.
Therearealsotrade-offsbetweensatisfyingfairnessandmaintainingpredictiveperformance.
Inthisworkweproposethatmanytypesofbiasshouldbemonitoredsimultaneouslytoreveal
concerningtrendsthatrequirefurthercontext-dependenthumanevaluation.
Inpractice,AIsystemsemployfairnesslibrariesortoolkitstocomputepopularmeasuresof
biasandeitherconstrainpredictivetaskstoachievetheacceptedmeasurevaluesorcarryout
trade-offsofthelatterwithpredictiveperformance. Somepopularsoftwareprojectsforfair
AIareAIF360[17],FairLearn[18],Whatif[19],andAequitas[20]. Eachofthesefocuseson
supportingdifferentcomputationalbackendsanddatastructures. AIF360andFairLearnare
programminglibrariesandprovidebiasmitigationalgorithms. WhatIfandAequitasfocuson
assessmentoffairness,andspecializeinassessingcounterfactualandgroupfairnessrespectively.
All libraries and toolkits implement ad-hoc measures of bias that capture fairness concerns
well-studied in the literature, where the latter typically account for very restricted types of
evaluationthatdonotmodelcomplexconcerns.
2.2. Measuresofbias
We now present common measures of bias, starting from ones that quantify group fairness
concerns for classifiers [21]. We organize people with the same sensitive attribute values
intopopulationgroups. Allpresentedmeasuresassumevaluesintherange[0,1]and,when
necessary,weshowthecomplementsofmeasuresoffairnesswithrespectto1toletthemassess
biasinstead,i.e.,avalueof0indicatesperfectfairness. Forexample,belowweuse1−pruleas
ameasureofbias,insteadofprule,whichisameasureoffairness.
Measuresofclassificationbias. Themeasuresof1−prule[2]andCalders-Verwerdisparity
cv[22]quantifydisparateimpactconcerns,i.e.,predictionrateinequalitybetweentwogroupsof
samples𝒮and𝒮′ = 𝒮 ∖𝒮,wherethelattercomplementstheformerwithinthepopulation𝒮 .
𝑎𝑙𝑙 𝑎𝑙𝑙
Thismodelsonebinarysensitiveattributeindicatingmembershiptothegroup. Mathematically:
{︁ }︁
prule = min P(𝑐(𝑥)=1|𝑥∈𝒮) , P(𝑐(𝑥)=1|𝑥∈𝒮′) , cv = |P(𝑐(𝑥) = 1|𝑥 ∈ 𝒮)−P(𝑐(𝑥) = 1|𝑥 ∈ 𝒮′)|
P(𝑐(𝑥)=1|𝑥∈𝒮′) P(𝑐(𝑥)=1|𝑥∈𝒮)
whereP(·|·)denotesconditionalprobabilityand𝑐(𝑥) ∈ {0,1}isthebinaryoutcomeofclassi-
fyingdatasample𝑥. Disparateimpactiseliminatedwhen1−prule = cv = 0. Anotherconcern
isdisparatemistreatment[23],whichcapturesmisclassificationdifferencesbetweengroupsper:
|Δ𝑚| = |𝑚(𝒮)−𝑚(𝒮′)|where 𝑚(·) denotes a misclassification measure over groups of samples, such as their false
positive rate (fpr) or their false negative rate (fnr). The same formula can express cv or ac-
commodate error measures for other predictive tasks, and therefore constitutes a building
blockofgeneralizedfairnessevaluationframeworks[24](alsoseeSubsection4.3). Anexam-
pleofdifference-basedbiasforprobabilisticmeasuresofpredictiveperformanceisequalized
opportunitydifference[25,26]:
|Δ𝑒𝑜| = |P(𝑐(𝑥) = 1|𝑥 ∈ 𝒮,𝑌(𝑥) = 𝑦)−P(𝑐(𝑥) = 1|𝑥 ∈ 𝒮′,𝑌(𝑥) = 𝑦)|
where𝑦 ∈ {0,1}and𝑌(𝑥)isthetruetest/validationdatalabelcorrespondingtosample𝑥.
Measuresofscoringandrecommendationbias. Thesameunderlyingprinciplescanbe
portedtootherpredictivetasks[5,14]suchasrecommendationandscoring.Inrecommendation,
thebasemeasure𝑚(·)in|Δ𝑚|maybereplacedbysomeformofexposureofitemstogroup
membersorthefractionoftop-𝑘 recommendationsthataremembersofthegroup. Inscoring
tasks, 𝑚(·) may represent the fraction of score mass concentrated on groups, which can be
comparedtoadesiredfractionorbetweengroupstosatisfyconcernssimilartodisparateimpact
[27]. Aggregatestatistics,liketheareaundertheroc(auc)orscoremeans,mayalsobecompared
betweengroups[28]. Adefinitionthatweuselaterforreceiveroperatingcharacteristic(roc)
curvesofgroups𝒮 accountsforallpairsoffalsepositiverate(fpr)andtruepositiverate(tpr)
𝑖
valuesobtainedforthegroupsoverdifferentthresholds𝜃 ofwhetherscoresareinterpretedas
positivepredictionsornot(weusethemapletarrowtoexpressrocsasmapsfromfprtotpr):
roc(𝒮 ) = {fpr(𝒮 ,𝜃) ↦→ tpr(𝒮 ,𝜃) : 𝜃 ∈ (−∞,∞)} (1)
𝑖 𝑖 𝑖
Morefine-grainedapproachessummarizethedifferencesbetweencurvesordistributions(a
mathematicalexpressionforthisappearsinSubsection4.3).Forexample,viablemeasuresofbias
aretheabsolutebetweennessareabetweenroccurves(abroca)[29]andtheKullback-Leibler
divergencebetweendistributionsofsystemoutcomesforeachgroup[30].
2.3. Frameworkstoassessmultidimensionalbias
Fairness concerns may span several sensitive groups and their intersections [31]. This set-
ting is known as multidimensional fairness. For example, there may be multiple protected
demographics(e.g.,genders,races,andtheirintersections). Twoframeworksforexpressing
several measures of bias in the multidimensional setting are a) what we later call groups vs
all comparison[24]andb)worst-casebias[30]. Inthiswork,wegeneralizetheseframeworks
underamoreexpressiveone. Ouranalysisalsoaccountsforc)individualfairnessbytreatingit
underamultidimensionalframeworkwhereeachindividualisaseparategroupofoneelement.
Groupsvsall. Thissetsupthefollowinggenericframeworkformeasuresofbias:
𝐹 𝑏𝑖𝑎𝑠(S) = ⊙ 𝒮𝑖∈S𝐹 𝑏𝑎𝑠𝑒(𝒮 𝑖) (2)
whereSareallgroups,𝐹 (𝒮 )isameasureofbiasforonegroup𝒮 ∈ S(thiscorresponds
𝑏𝑎𝑠𝑒 𝑖 𝑖
to treating that group as having only one binary sensitive attribute), and ⊙ is a reductionmechanism,suchastheminimumormaximum. Groupsmaybeoverlappingwhenexamining
each sensitive attribute value independently without considering their intersections [32, 33,
34,35]. Ithasbeenproposedthatgroupintersectionsmayalsoreplacethegroupswithinthis
analysis,thereforecreatingsubgroupsintheirplace[1,31,30].
In Equation 2, the base measure of bias compares one group against the total population.
Examplemeasuresthatemploythisschemearestatisticalparitysubgroupfairness(spsf)and
falsepositivesubgroupfairness(fpsf)[31]. Forthese,eachsubgroupiscomparedtothetotal
populationtocreatemultidimensionalvariationsofΔ𝑒𝑜andΔfprrespectively. Bothemploya
reductionmechanismthatweighscomparisonsbygroupsize.
Worst-casebias. Thisframeworkstartsfrompairwisegrouporsubgroupcomparisonsand
keepstheworstcase. Wecomputeworst-casebias(wcb)foranyprobabilisticmeasure𝐹(𝒮 ),
𝑖
suchasofaccurateorerroneouspredictionsovergroupsorsubgroups𝒮 ∈ S,per:
𝑖
wcb(S) = 1− min 𝐹(𝒮 ⟩) (3)
(𝒮𝑖,𝒮𝑗)∈S2 𝐹(𝒮 |)
Forexample,differentialfairness[1]statesthatpruleshouldresideintherange[𝑒−𝜖,𝑒𝜖]for
some𝜖 ≥ 0whenitcomparesallsubgroupspairwise.Aviablemeasureofbiasforthisdefinition,
whichwecalldifferentialbias(db),isgivenforsubgroupsSwhen𝐹(𝒮 ) = P(𝑐(𝑥) = 1|𝑥 ∈ 𝒮 ),
𝑖 𝑖
forwhichdifferentialfairnessisequivalenttosatisfyingwcb(S) ≤ 1−𝑒−𝜖.
Individual fairness Multidimensional discrimination can also model individual fairness
[10,36]bysettingeachindividualasaseparategroup. Inthiscase,andgiventhatindividual
fairnessisformallydefinedtosatisfy𝑑 (𝑐(𝑥 ),𝑐(𝑥 )) ≤ 𝑑 (𝑥 ,𝑥 )acrossallpairsofindividuals
𝑦 𝑖 𝑗 𝑥 𝑖 𝑗
(𝑥 ,𝑥 ),where𝑐isapredictivemechanismand𝑑 ,𝑑 aresomedistancemetrics,aviablemeasure
𝑖 𝑗 𝑦 𝑥
ofindividualbias(ib)thatsatisfiesindividualfairnesswhenib ≤ 1is:
ib = max 𝑑𝑦(𝑐(𝑥𝑖),𝑐(𝑥𝑗)) (4)
𝑥𝑖,𝑥𝑗 𝑑𝑥(𝑥𝑖,𝑥𝑗)
3. A General Bias Measurement Framework
Inthissectionwepresentaframeworkfordefiningmeasuresofbiasfromfundamentalbuilding
blocks. Thisletsusdecomposeexistingmeasuresintoblocks,introducevariations,andcreate
novelcombinations. Werecognizefourtypesofblocks,whichcorrespondtosuccessivecompu-
tationalsteps: a)selectingwhichpairsofpopulation(sub)groupstocompare,b)basemeasures
thatassesssomesystempropertyoneachgroup,c)comparisonsbetweengroupassessments,
andd)reductionsthatconvertmultiplepairwisecomparisonstoonevalue.
Mathematically,weannotatebasemeasuresas𝐹(𝒮 )andcomputethemovergroupsofdata
𝑖
samples 𝒮 ⊆ 𝒮 of some population 𝒮 . We consider any information necessary for this
𝑖 𝑎𝑙𝑙 𝑎𝑙𝑙
computation(e.g.,predictedandgroundtruthlabels)directlyretrievablefromtherespective
samples. We also annotate the set of all these groups as S = {𝒮 : 𝑖 = 0,1,...}. The base
𝑖
measurescouldbeerrorratesofpredictionsorunsupervisedstatistics,likepositiverates. Then,
weconsiderasetofsubgrouppairstocompare𝐶(S,𝒮 𝑎𝑙𝑙) ∈ (︀ 2𝒮 𝑎𝑙𝑙)︀2,where2𝒮 𝑎𝑙𝑙 denotesthepowerset. Thecreatedsetofsubgrouppairscomprisesanynecessarydetectedcomparisons
betweengroupsorsubgroupswithin𝒮 ,eventhosethatdonotdirectlyresideinSbutmay
𝑎𝑙𝑙
be derived from the latter, such as subgroups. We will make pairwise comparisons 𝑓 =
𝑖𝑗
𝐹(𝒮 )⊘𝐹(𝒮 )between(sub)groups(𝒮 ,𝒮 ) ∈ 𝐶(S,𝒮 ). Finally,thereductionmechanism
𝑖 𝑗 𝑖 𝑗 𝑎𝑙𝑙
willbeexpressedas⊙ 𝑓 acrossallpairs(𝑆 ,𝑆 )andoutcomesofcomparingthem𝑓 .
(𝑆𝑖,𝑆𝑗) 𝑖𝑗 𝑖 𝑗 𝑖𝑗
Puttingtheaboveinoneformulayieldsourframeworkforexpressingmeasuresofbias:
𝐹 = ⊙ (︀ 𝐹(𝒮 )⊘𝐹(𝒮 ))︀ (5)
𝑏𝑖𝑎𝑠 (𝒮𝑖,𝒮𝑗)∈𝐶(S,𝒮 𝑎𝑙𝑙) 𝑖 𝑗
This is a direct generalization of Equation 2 in that it supports more varied strategies for
expressinggroupcomparisons. Atthesametime,itsystematizesthecomparisonmechanisms
betweengroupsofpeoplethroughtheoperation⊘. Ourframeworkisalsoageneralizationof
Equation3inthat,inadditiontothebasemeasure(whichweallowtoalsobenon-probabilistic,
ifsodesired),itprovidesflexibilityinthereductionandcomparisonmechanismsbeyondthe
choicesofminimumandfractionalcomparisonthatcharacterizetheworstcase.
Eachtupleofchoices(𝐹,𝐶,⊘,⊙)createsadifferentmeasureofbias. Inthenextsection,we
delveintohoweachbuildingblocktypeamongthemembersofthistuplecouldvarybetween
contexts that comprise different fairness concerns and predictive tasks. For every building
block,asystematicexplorationwouldconsiderallpossiblecombinationswithothersofdifferent
kindssothateventuallywenotonlyreconstructtheoriginalmeasuresofbias,butalsocreate
variationsthataddressdifferentsettings. Table1exemplifieshowthemeasuresdiscussedin
Subsection2.2arisefromspecificchoices. Creatingafulltaxonomyofexistingmeasuresunder
ourframeworkisnotthiswork’sobjective(weonlyaimtodemonstrateitswideexpressive
breadth)andislefttofuturework.
Measure 𝐹(𝒮 ) 𝐶(S,𝒮 ) 𝑓 ⊘𝑓 ⊙
⟩ 𝑎𝑙𝑙 𝑖 𝑗
Examplemeasures
|Δfpr| [23] P(𝑐(𝑥)=1|𝑥∈𝒮 ,𝑌(𝑥)=0) {𝒮,𝒮 ∖𝒮}2forS={𝒮} 𝑓 −𝑓 max
𝑖 𝑎𝑙𝑙 𝑖 𝑗
|Δfnr| [23] P(𝑐(𝑥)=0|𝑥∈𝒮 ,𝑌(𝑥)=1) {𝒮,𝒮 ∖𝒮}2forS={𝒮} 𝑓 −𝑓 max
𝑖 𝑎𝑙𝑙 𝑖 𝑗
|Δ𝑒𝑜| [25] P(𝑐(𝑥)=1|𝑥∈𝒮 ) {𝒮,𝒮 ∖𝒮}2forS={𝒮} 𝑓 −𝑓 max
𝑖 𝑎𝑙𝑙 𝑖 𝑗
spsf [31] P(𝑐(𝑥)=1|𝑥∈𝒮 ) S×{𝒮 }∪{𝒮 }×S |𝑓 −𝑓 | wmean
𝑖 𝑎𝑙𝑙 𝑎𝑙𝑙 𝑖 𝑗
fpsf [31] P(𝑐(𝑥)=1|𝑥∈𝒮 ,𝑌(𝑥)=0) S×{𝒮 }∪{𝒮 }×S |𝑓 −𝑓 | wmean
𝑖 𝑎𝑙𝑙 𝑎𝑙𝑙 𝑖 𝑗
cv [22] P(𝑐(𝑥)=1|𝑥∈𝒮 ) {𝒮,𝒮 ∖𝒮}2 𝑓 −𝑓 max
𝑖 𝑎𝑙𝑙 𝑖 𝑗
1−prule [2] P(𝑐(𝑥)=1|𝑥∈𝒮 ) {𝒮,𝒮 ∖𝒮}2 1−𝑓 /𝑓 max
𝑖 𝑎𝑙𝑙 𝑖 𝑗
db [1] P(𝑐(𝑥)=1|𝑥∈𝒮 ) S2 1−𝑓 /𝑓 max
𝑖 𝑖 𝑗
Multidimentionalbiasframeworks
𝐹 [24] any* S×{𝒮 }∪{𝒮 }×S any any
𝑏𝑖𝑎𝑠 𝑎𝑙𝑙 𝑎𝑙𝑙
wcb [30] any S2 1−𝑓 /𝑓 max
𝑖 𝑗
ib [10] 𝑐(𝑥) {{𝑥}:𝑥∈𝒮 } 𝑑 (𝑓 ,𝑓 )/𝑑 (𝒮 ,𝒮 ) max
𝑎𝑙𝑙 𝑦 𝑖 𝑗 𝑥 𝑖 𝑗
Table1
ExpressingthemeasuresofclassificationbiasofSubsection2.2andmultidimensionalbiasframeworks
ofSubsection2.3underourbiasmeasuredefinitionframework. Interpretationofreductionscanbe
foundinSubsection4.4.Frameworksallowforanyblocksofcertainkinds.*𝐹 doesnotexplicitly
𝑏𝑖𝑎𝑠
acknowledgebasemeasurebuildingblocks.4. Bias building blocks
Here we present building blocks that occur from decomposing popular measures of bias of
Sections2.2and2.3. Combinationsunderourframeworkcancreatenewmeasures.
4.1. Selectinggroupsorsubgroupstocompare
Thegroupselectionmechanismswestudyincludea)baseselection,b)accountingforindividual
fairness[10],andc)accountingforintersectionality. Moreblockscanbecreatedinthefuture.
Baseselection. Severalfairnessmeasuresaredefinedbycomparingeachpopulationgroup
orsubgroupwiththetotalpopulation𝒮 . Giventhatthepairwisecomparisonmechanism
𝑎𝑙𝑙
isnotsymmetric, i.e., itmayholdthat𝑓 ⊘𝑓 ̸= 𝑓 ⊘𝑓 , weaccountforboth(𝒮 ,𝒮 )and
𝑖 𝑗 𝑗 𝑖 𝑖 𝑎𝑙𝑙
(𝒮 ,𝒮 )forsubgroups𝒮 ∈ S. Mathematically,wedefineagroupvsanysampleselectionper:
𝑎𝑙𝑙 𝑖 𝑖
vsany(S,𝒮 ) = S×{𝒮 }∪{𝒮 }×S (6)
𝑎𝑙𝑙 𝑎𝑙𝑙 𝑎𝑙𝑙
Alternatively,onemayconsiderallgrouppairs(weletreductionremoveanyself-comparisons):
pairs(S,𝒮 ) = S2 (7)
𝑎𝑙𝑙
Forone-dimensionalfairness,i.e.,onlyonesensitiveattributewithonlytwopotentialvalues,
such as indicating which samples belong to a protected group of people and which do not,
eachgroup𝒮 istypicallycomparedtoitscomplement withinthewholepopulation𝒮 ∖𝒮 .
𝑎𝑙𝑙 𝑖
Inthiscase,S = {𝒮}andwewritethepairwiseselectionbetweenitanditscomplementper
compl(S,𝒮 ) = {𝒮,𝒮 ∖𝒮}2. Onegeneralizationtomultidimensionalsettingswouldbeto
𝑎𝑙𝑙 𝑎𝑙𝑙
consider𝒮 ∖𝒮 asagroupandrecreateEquation7. However,anequallyvalidgeneralization
𝑎𝑙𝑙
ittocomparegroupstotheircomplementsonly:
⋃︁
compl(S,𝒮 ) = {𝒮 ,𝒮 ∖𝒮 }2 (8)
𝑎𝑙𝑙 𝑖 𝑎𝑙𝑙 𝑖
𝒮𝑖∈S
Accountingforindividualfairness. Individualfairnessdisregardsthenotionofpopulation
groupsorsubgroupsandinsteadcomparesallindividualspairwise. Thisisaninstanceofthe
pairsmechanism,whereeachindividualisassignedtotheirowngroup. Mathematically,this
wouldoccurifwespecifiedsubgroupsS = {{𝑥} : 𝑥 ∈ 𝒮 }. Weherebyextendallcomparison
𝑎𝑙𝑙
mechanisms𝐶 tofollowthisschemawhenanemptysetofgroups∅isprovided1 per:
𝐶(∅,𝒮 ) = 𝐶({{𝑥} : 𝑥 ∈ 𝒮 },𝒮 ) (9)
𝑎𝑙𝑙 𝑎𝑙𝑙 𝑎𝑙𝑙
Accounting for intersectionality. Measures of bias like db, spsf, and spsf account for
subgroupsthatformintersectionsofgroupstoo. Tomodelthisscenario,wedefineintersectional
comparisonsbetweengroupswhileignoringemptyintersections(thisconvenientlyignores
intersectionsbetweenmutuallyexclusivesensitiveattributevalues)per:
𝐶 (S,𝒮 ) = 𝐶({𝒮 = 𝒮 ∩𝒮 ∩··· : 𝒮 ∈ S,𝒮 ≠ ∅},𝒮 ) (10)
𝑖𝑛𝑡𝑒𝑟𝑠𝑒𝑐𝑡 𝑎𝑙𝑙 ⟩∞ ⟩∈ ⟩
‖
𝑎𝑙𝑙
1Theemptysetindicatesnoknowledgeofagroup,inwhichcaseourfallbackbecomesindividualfairness.4.2. Basemeasures
Ourframeworkacceptsanymeasureofpredictiveperformance. Inthissubsection,wedemon-
stratetwoconcepts: a)computingaggregateassessmentstofeedas-areincomparisonmecha-
nisms,andb)trackingunderlyingmetadatathatcouldbeusedbyotherbuildingblockswithout
altering Equation 5. Base measures can also be ported from fairness definitions that do not
specifymeasuresofbias,suchasequalgroupbenefit[37]. Ourframeworkcansupplytherestof
buildingblockstotryallavailableoptionsatdefiningbias,suchasallcomparisonmechanisms.
Aggregateassessment. Here,welistbasemeasuresofaggregatesystemassessmentoften
usedtodefinebias. Probabilisticdefinitionsforthemcanbefoundindomainliterature. When
working with classifiers, popular base measures are positive rates (pr), fpr, and fnr. Other
misclassification measures are also used in expressions of disparate mistreatment [23]. In
multiclass settings, each class can be treated as a different group of data samples with its
own target labels. Bias has been assessed for the top-𝑘 individual/item recommendations
bycomparingthebasemeasuresofhitrateandskew[38,39]acrossgroupswhenchecking
for proportional representation. Recommendation correctness measures may be similarly
accommodated,suchastheclick-throughrate[39]. Recommendationisassessedacrossseveral
valuesof𝑘,whichleadstomeasuresthatkeeptrackofmetadata(seebelow).
Keeping track of metadata. An emerging concern is that measures of bias affirm the
presence of certain biases but not their absence. To verify the latter, one needs to look at
nuances of underlying distributions [30]. For example, it has been argued that statistical
testsshouldreplacethepruletoshownon-discriminationincertainlegalsettings[40]. Thus,
our framework retains building block metadata to be used in subsequent computations. To
understandthis,thinkoftheabrocameasure[29]thatcomparestheareabetweentheroccurves
ofEquation1insteadofcomparingaucsthroughdifferencesorfractions.
We model curve comparisons by tracking computational metadata and retrieving them
throughanappropriatepredicatecurve(·)definedalongsidebasemeasures. Forexample,auc’s
definitionasabuildingblockshouldincludethestatement:
curve(auc(𝒮 )) = roc(𝒮 )
𝑖 𝑖
We do not directly return the metadata (e.g., the roc curves) in order to compare aggregate
assessmentswithasmanymechanismsaspossible. Forinstance,alsocomputing|auc(𝒮 )−
𝑖
auc(𝒮 )|andcheckingdifferenceswithabrocamaycreatehigh-levelinsights(e.g.,ifthesetwo
𝑗
quantitiesareequalbetweentwogroups,thecorrespondingroccurvesdonotintersect).
We now present a second base measure used in recommendation systems to assess top
predictionsandcontainsacurve,namelyaveragerepresentation(ar)withinthetoppredictions:
𝐾
∑︁
ar = 1 P(𝑥 ∈ 𝒮 |𝑥 ∈ top )
𝐾 𝐾 𝑖 𝑘
𝑘=1
Tokeepworkingwithintegratablecurveswithcontinuoushorizontalaxes,weuseDirac’sdeltafunction𝛿(𝑘) = 0for𝑘 ̸= 0and∫︀∞ 𝛿(𝑘)d𝑘 = 1todefinetheinjection:
−∞
curve(ar ) = {︀ 𝑘 ↦→ P(𝑥 ∈ 𝒮 |𝑥 ∈ top )𝛿(0)if𝑘 ∈ {1,2,...,𝐾},0otherwise : 𝑘 ∈ (−∞,∞)}︀
𝐾 𝑖 𝑘
4.3. Comparisonmechanisms
We examine three types of comparison to serve as building blocks in our framework: a) no
comparison,b)numericerrors(differencesandfractions),andc)curvecomparisons. Thelast
typeencompassesmechanismsthatcomputeweighedcurvedifferences. Othermechanisms
canbereadilycreated,andwedemonstratethresholdedvariations.
Nocomparison. Thisisavalidoptionwhenaimingtoremoveconfoundingbias,i.e.,bias
thatdirectlyleadstoerroneoussystempredictions. Forexample,theworstmeasureassessment
acrossgroupsorsubgroupsismaximizedifallgroupsexhibithighpredictiveperformance[41].
Inthiscase,ourframeworkjustassessesAIperformanceforallgroups. Mathematically:
𝑓 ⊘ 𝑓 = 𝑓
𝑖 𝑛𝑜𝑛𝑒 𝑗 𝑖
Numericerrors. Thesecomputesomedeviationbetween𝑓 and𝑓 ,suchasabsoluteerror
𝑖 𝑗
(abs),relativeerror(rel),andtheirsignedvalues(sabsandsrelrespectively):
𝑓 ⊘ 𝑓 = |𝑓 −𝑓 |, 𝑓 ⊘ 𝑓 = |1−𝑓 /𝑓 |, 𝑓 ⊘ 𝑓 = 𝑓 −𝑓 , 𝑓 ⊘ 𝑓 = 1−𝑓 /𝑓
𝑖 𝑎𝑏𝑠 𝑗 𝑖 𝑗 𝑖 𝑟𝑒𝑙 𝑗 𝑖 𝑗 𝑖 𝑠𝑎𝑏𝑠 𝑗 𝑖 𝑗 𝑖 𝑠𝑟𝑒𝑙 𝑗 𝑖 𝑗
Curvecomparisons. Letusconsiderthatthecurvepredicatecanextractfrombasemeasure
assessmentcurves𝑐𝑟𝑣 = curve(𝑓 ),𝑐𝑟𝑣 = curve(𝑓 )withaknowncommondomain𝒟. We
𝑖 𝑖 𝑗 𝑗
cancomparethesegivenaweightingmechanism𝐼(𝑘)forthedomainandcomparison⊘ as:
𝑐𝑟𝑣
∫︁
𝑓 ⊘𝑓 = 1 𝐼(𝑘)·(︀ 𝑐𝑟𝑣 (𝑘)⊘ 𝑐𝑟𝑣 (𝑘))︀d𝑘
𝑖 𝑗 ∫︀ 𝐼(𝑘)d𝜃 𝑖 𝑐𝑟𝑣 𝑗
𝒟 𝒟
Essentially, we are building curve comparisons from the simpler components (𝐼,⊘ ). As
𝑐𝑟𝑣
an example, we reconstruct the abroca pairwise group comparison [29] based on the auc
basemeasureofSubsection4.2giventhecurvecomparisondefinedbythepairofoperations
⊘ = (𝐼 (𝑘) = 1,⊘ ). Measuresthatretaincurvesoftop-𝑘 recommendationsmay
𝑎𝑏𝑟𝑜𝑐𝑎 𝑐𝑜𝑛𝑠𝑡 𝑎𝑏𝑠
alsoconsiderNDCG-likeweighting𝐼 (𝑘) = (︀ 1 if𝑘 > 0,0otherwise)︀[38,39].
𝐷𝐶𝐺 log(1+𝑘)
Thresholdedvariations. Oftentimes,smalldeviationsfromperfectfairnessareaccepted.
Settlingforthresholdsunderwhichmeasuresofbiasoutput0cantakeplaceeitherattheendof
theassessment,orduringintermediatesteps. Ifthelastoptionischosen,comparisons𝑓 ⊘𝑓
𝑖 𝑗
needtobereplacedwiththefollowingvariationsofmaximumacceptablethreshold𝜖 ≥ 0:
𝑓 ⊘ 𝑓 = max{0,𝑓 ⊘𝑓 −𝜖}
𝑖 𝜖 𝑗 𝑖 𝑗4.4. Reductionmechanisms
Thefinalstepofourframeworkconsistsofreducingtoonequantityallpairwisecomparisons
𝑓 betweengroupsorsubgroups(𝒮 ,𝒮 ) ∈ 𝐶(S,𝒮 ). Commonreductionsarethemaximum,
𝑖𝑗 𝑖 𝑗 𝑎𝑙𝑙
minimum,orarithmeticmeanofallvalues. Especiallythemaximumisacornerstoneofthe
worst-casebiasframework,althoughitdoesnotdifferentiatebetweensystemsthathavethe
sameworstcase. Alternatively,reductioncouldadoptsomeformulathatbecomestheweighted
meaninthegroupvsallcase[31]. Here,wedemonstrateonefeasibleoption,whichobtains
weights1−|P(𝑥 ∈ 𝑆 )−P(𝑥 ∈ 𝑆 ) = 1−|P(𝑥 ∈ 𝑆 )−1| = P(𝑥 ∈ 𝑆 )undervsanybut
𝑖 𝑎𝑙𝑙 𝑖 𝑖
alsoignoresself-comparisonsamonggroupsunderpairs(·,·)orcompl(·,·):
∑︁
wmean 𝑓 = (1−|P(𝑥 ∈ 𝑆 )−P(𝑥 ∈ 𝑆 )|)𝑓
𝑖,𝑗 𝑖𝑗 𝑖 𝑗 𝑖𝑗
𝑓𝑖𝑗:𝒮𝑖̸=𝒮𝑗
5. The FairBench Library
Weimplementourbiasmeasuredefinitionframeworkwithinanopen-sourcePythonlibrary
called FairBench.2 This focuses on ease-of-use, comprehensibility, and compatibility with
popularworkingenvironments. Toachievethese,weadoptedaforward-orientedparadigm[42]
todesigntheprogramminginterfacesofcodeblocksascallablemethodsthatcanbecombined
inreportingmechanisms. Blockparametersaretransferredthroughkeywordarguments.
5.1. Forksofsensitiveattributevalues
Ourdesigniscenteredarounduniformlyrepresentingmanygroupsofdatasamples. Previous
fairnesslibrariestendtoparselistsofsensitiveattributenamesandmatchthesewithcolumnsof
clearlyunderstoodprogrammingdatatypes,suchasPandasdataframes[43]. However,coupling
dataloadingandfairnessassessmentcreatesinflexibleusagepatternsandsourcecodethatis
hardertoextend. Forexample,itneedsnewmethodsandclassestosupportgraphorimageAI.
Tobypassthisissue,FairBenchparsesvectorsofpredictions(e.g.,scores),groundtruth(e.g.,
classificationlabels),andbinarymembershiptoprotectedgroups. Vectorscancomefromany
computationalbackendwithaduck-typeextensionofPython’s interface;theycould
Iterable
beNumPyarrays[44],PyTorchtensors[45],TensorFlowtensors[46],JAXtensors[47],Pandas
columns[43],orPythonlists. Wesupportend-to-endintegrationwithautomaticdifferentiation
frameworksbyextendingtheircommonfunctionalinterfaceprovidedbytheEagerPylibrary
[48]andsettingtheinternalcomputationalbackendper .
fairbench.backnend(name)
Wesimplifyhandlingofmultipleprotectedgroupsbyorganizingthemintoonedatastructure,
which we call a of the sensitive attribute. This is a programming equivalent of S. All
Fork
interfaces accept a sensitive attribute fork, and base measures run for every group. Thus,
everygroupbeinganalysedbecomesacomputationalbranchofthefork. Weprovidedynamic
constructorpatternstoeasilydeclareforksincommonsetups. Onepatternisdemonstrated
below,whereafork letsuscomputetheaccuracyfork thatholdsassessmentoutcomesfor
s acc
bothwhitesandblacks:
2ThedocumentationofFairBenchisavailableat:https://fairbench.readthedocs.ioimport fairbench as fb
race, predictions, labels = ... # arrays, tensors, etc
s = fb.Fork(white=..., black=...) # branches hold binary iterables
acc = fb.accuracy(predictions=..., labels=..., sensitive=s)
fb.visualize(acc) # visualize accuracy, which is also a fork
Forksmayalsobeconstructedfromcategoricaliterables(e.g.,Pandasdataframecolumns
ornativelistslike )withtheconstructorpattern
cats=[’man’, ’woman’, ’man’, ’nonbinary’]
. Forksofmultidimensionalsensitiveattributescanbecreatedby
fb.Fork(fb.categories@cats)
addingallcategoricalparsingtotheconstructoraspositionalarguments. Intersectionsbetween
forkbranchesperEquation10canbeachievedbycallingacorrespondingmethod:
races, genders = ... # categorical iterables
s = fb.Fork(fb.categories@races, fb.categories@genders)
s = s.intersectional()
print(s.sum()) # visualization is not very instructive for many branches
5.2. Fairnessreports
Multidimensional fairness reports for several default popular measures, comparisons, and
reductionscanbecomputedanddisplayedwiththefollowingsnippet:
vsany = fb.unireport(predictions=..., labels=..., sensitive=s)
pairs = fb.multireport(predictions=..., labels=..., sensitive=s)
report = fb.combine(pairs, vsany)
fb.describe(report) # or print or fb.visualize
ThiscombinesthecomparisonsofEquation6(unireport)andEquation7(multireport). Formore
reporttypesorhowtodeclareonespecificmeasureofbias,refertothelibrary’sdocumentation.
Thebasemeasurestoanalysearedeterminedfromkeywordarguments. Forexample,theabove
codesnippetcomputesclassificationbasemeasures,butifweaddeda arguments
scores=...
(in addition to or instead of ) recommendation measures would be obtained too.
predictions
Reportscanalsoparseacustomselectionofbasemeasures,includingexternallydefinedones.
TheoutcomeofrunningtheabovecodeispresentedinFigure1.Columnnamescorrespondto
thecombinationofreductionandcomparisonmechanisms,whereasrowstothebasemeasures.
Familiarizationwithboththelibraryandourmathematicalframeworkisrequiredtounderstand
FairBenchreports,butthesepresent-toourknowledge-thefirstsystematicwayoflookingat
manybiasassessmentsatonce,regardlessoftheexactsetting. Forexample,inthereportbelow
itiseasytodetectthesmallminimumratio(minratio)ofpr,whichcorrespondstoprule,even
iftherearegenerallybalancedtprandtnr(equivalenttobalancedfnrandfpr).
Figure1:ExampleofacombinedFairBenchreport.
Finally,reportscanbeinteractivelyexploredthrough . Thisusesthe
fb.interactive(report)
Bokehlibrary[49]toruninnotebookoutputsornewbrowsertabsandcreatevisualizationsimilartoFigure2. Throughthevisualinterface,userscannotonlyseereportvaluesbutalso
focus on columns or rows, and delve into explanations of which raw values contributed to
computations,asindicatedviathe partoftheexplorationpathontop. Explanations
.explain
correspond to tracking metadata values through predicates per Subsection 4.2. The same
explorationcanbeperformedpurelyprogrammaticallytoo.
Figure2:ExamplestepsduringtheinteractiveexplorationofaFairBenchreport.
6. Conclusions
Inthisworkweprovidedamathematicalframeworkthatstandardizesthedefinitionofmany
existingandnewmeasuresofbiasbysplittingtheminbasebuildingblocksandcompilingall
possiblecombinations. ThisframeworksystematicallyexploresAIfairnessthroughmeasures
that account for a wide range of settings and bias concerns, beyond the confines of ad-hoc
exploration. ImplementationofpopularbuildingblocksareprovidedintheFairBenchlibrary
viainteroperablePythoninterfaces. Thelibraryprovidesadditionalfeatures,likevisualization
andcomputationbacktracking,thatenablein-depthexplorationofawiderangeofbiasconcerns.
Itcanalsobeusedalongsidepopularcomputationalbackends.
Futureresearchanddevelopmentcouldworktowardsidentifyingmorebiasbuildingblocks
by decomposing more measures from the literature, as well as additional block types that
mayariseinthefuture. FairBenchisopensourceandwealsoencourageimplementationsof
suchanalysisfromthecommunity. Wefurtherplantoextendcurrentlyexperimentalfeatures,
likecreatingfairnessmodelcardsthatincludecaveatsandrecommendationsobtainedfrom
interdisciplinarycollaborationswithsocialscientists,andcreatinghigher-levelassessments.
Acknowledgement
ThisresearchworkwasfundedbytheEuropeanUnionundertheHorizonEuropeMAMMOth
project,GrantAgreementID:101070285. UKparticipantinHorizonEuropeProjectMAMMOth
issupportedbyUKRIgrantnumber10041914(TrilateralResearchLTD).References
[1] J. R. Foulds, R. Islam, K. N. Keya, S. Pan, An intersectional definition of fairness, in:
2020 IEEE 36th International Conference on Data Engineering (ICDE), IEEE, 2020, pp.
1918–1921.
[2] D.Biddle,Adverseimpactandtestvalidation:Apractitioner’sguidetovalidanddefensible
employmenttesting,Routledge,2017.
[3] P.Ala-Pietilä,Y.Bonnet,U.Bergmann,M.Bielikova,C.Bonefeld-Dahl,W.Bauer,L.Bouarfa,
R.Chatila,M.Coeckelbergh,V.Dignum,etal.,Theassessmentlistfortrustworthyartificial
intelligence(ALTAI),EuropeanCommission,2020.
[4] N.AI, Artificialintelligenceriskmanagementframework(airmf1.0)(2023).
[5] B.Li,P.Qi,B.Liu,S.Di,J.Liu,J.Pei,J.Yi,B.Zhou, Trustworthyai: Fromprinciplesto
practices, ACMComputingSurveys55(2023)1–46.
[6] E.Ntoutsi,P.Fafalios,U.Gadiraju,V.Iosifidis,W.Nejdl,M.-E.Vidal,S.Ruggieri,F.Turini,
S.Papadopoulos,E.Krasanakis,etal., Biasindata-drivenartificialintelligencesystems—an
introductory survey, Wiley Interdisciplinary Reviews: Data Mining and Knowledge
Discovery10(2020)e1356.
[7] S. Barocas, M. Hardt, A. Narayanan, Fairness and machine learning: Limitations and
opportunities,MITPress,2023.
[8] S. Mitchell, E. Potash, S. Barocas, A. D’Amour, K. Lum, Algorithmic fairness: Choices,
assumptions, and definitions, Annual Review of Statistics and Its Application 8 (2021)
141–163.
[9] N.Mehrabi,F.Morstatter,N.Saxena,K.Lerman,A.Galstyan, Asurveyonbiasandfairness
inmachinelearning, ACMcomputingsurveys(CSUR)54(2021)1–35.
[10] C.Dwork, M.Hardt, T.Pitassi, O.Reingold, R.Zemel, Fairnessthroughawareness, in:
Proceedingsofthe3rdinnovationsintheoreticalcomputerscienceconference,2012,pp.
214–226.
[11] M.J.Kusner,J.Loftus,C.Russell,R.Silva, Counterfactualfairness, Advancesinneural
informationprocessingsystems30(2017).
[12] A.N.Carey,X.Wu, Thecausalfairnessfieldguide: Perspectivesfromsocialandformal
sciences, FrontiersinBigData5(2022)892837.
[13] L.Rosenblatt, R.T.Witter, Counterfactualfairnessisbasicallydemographicparity, in:
ProceedingsoftheAAAIConferenceonArtificialIntelligence,volume37,2023,pp.14461–
14469.
[14] V.XinyingChen, J.Hooker, Aguidetoformulatingfairnessinanoptimizationmodel,
AnnalsofOperationsResearch(2023)1–39.
[15] J.Kleinberg,S.Mullainathan,M.Raghavan, Inherenttrade-offsinthefairdetermination
ofriskscores, arXivpreprintarXiv:1609.05807(2016).
[16] T.Miconi, Theimpossibilityof"fairness": ageneralizedimpossibilityresultfordecisions,
arXivpreprintarXiv:1707.01195(2017).
[17] R.K.Bellamy,K.Dey,M.Hind,S.C.Hoffman,S.Houde,K.Kannan,P.Lohia,J.Martino,
S. Mehta, A. Mojsilović, et al., Ai fairness 360: An extensible toolkit for detecting and
mitigatingalgorithmicbias, IBMJournalofResearchandDevelopment63(2019)4–1.
[18] S.Bird,M.Dudík,R.Edgar,B.Horn,R.Lutz,V.Milan,M.Sameki,H.Wallach,K.Walker,Fairlearn: A toolkit for assessing and improving fairness in ai, Microsoft, Tech. Rep.
MSR-TR-2020-32(2020).
[19] J.Wexler,M.Pushkarna,T.Bolukbasi,M.Wattenberg,F.Viégas,J.Wilson, Thewhat-if
tool: Interactiveprobingofmachinelearningmodels, IEEEtransactionsonvisualization
andcomputergraphics26(2019)56–65.
[20] P.Saleiro,B.Kuester,L.Hinkson,J.London,A.Stevens,A.Anisfeld,K.T.Rodolfa,R.Ghani,
Aequitas: Abiasandfairnessaudittoolkit, arXivpreprintarXiv:1811.05577(2018).
[21] A. Castelnovo, R. Crupi, G. Greco, D. Regoli, I. G. Penco, A. C. Cosentini, The zoo of
fairnessmetricsinmachinelearning(2021).
[22] T.Calders,S.Verwer, Threenaivebayesapproachesfordiscrimination-freeclassification,
Dataminingandknowledgediscovery21(2010)277–292.
[23] M.B.Zafar,I.Valera,M.GomezRodriguez,K.P.Gummadi, Fairnessbeyonddisparate
treatment&disparateimpact: Learningclassificationwithoutdisparatemistreatment, in:
Proceedingsofthe26thinternationalconferenceonworldwideweb,2017,pp.1171–1180.
[24] A.Roy,J.Horstmann,E.Ntoutsi, Multi-dimensionaldiscriminationinlawandmachine
learning-acomparativeoverview,in:Proceedingsofthe2023ACMConferenceonFairness,
Accountability,andTransparency,2023,pp.89–100.
[25] M.Hardt,E.Price,N.Srebro, Equalityofopportunityinsupervisedlearning, Advancesin
neuralinformationprocessingsystems29(2016).
[26] M.Donini,L.Oneto,S.Ben-David,J.S.Shawe-Taylor,M.Pontil, Empiricalriskminimiza-
tionunderfairnessconstraints, Advancesinneuralinformationprocessingsystems31
(2018).
[27] S. Tsioutsiouliklis, E. Pitoura, P. Tsaparas, I. Kleftakis, N. Mamoulis, Fairness-aware
pagerank, in: ProceedingsoftheWebConference2021,2021,pp.3815–3826.
[28] T.Calders,A.Karim,F.Kamiran,W.Ali,X.Zhang, Controllingattributeeffectinlinear
regression, in: 2013IEEE13thinternationalconferenceondatamining,IEEE,2013,pp.
71–80.
[29] J. Gardner, C. Brooks, R. Baker, Evaluating the fairness of predictive student models
throughslicinganalysis, in: Proceedingsofthe9thinternationalconferenceonlearning
analytics&knowledge,2019,pp.225–234.
[30] A.Ghosh,L.Genuit,M.Reagan, Characterizingintersectionalgroupfairnesswithworst-
casecomparisons, in: ArtificialIntelligenceDiversity,Belonging,Equity,andInclusion,
PMLR,2021,pp.22–34.
[31] M.Kearns,S.Neel,A.Roth,Z.S.Wu, Preventingfairnessgerrymandering: Auditingand
learningforsubgroupfairness, in: Internationalconferenceonmachinelearning,PMLR,
2018,pp.2564–2572.
[32] M.Kearns,S.Neel,A.Roth,Z.S.Wu, Anempiricalstudyofrichsubgroupfairnessfor
machine learning, in: Proceedings of the conference on fairness, accountability, and
transparency,2019,pp.100–109.
[33] J.Ma,J.Deng,Q.Mei, Subgroupgeneralizationandfairnessofgraphneuralnetworks,
AdvancesinNeuralInformationProcessingSystems34(2021)1048–1061.
[34] N.L.Martinez,M.A.Bertran,A.Papadaki,M.Rodrigues,G.Sapiro, Blindparetofairness
andsubgrouprobustness, in: InternationalConferenceonMachineLearning,PMLR,2021,
pp.7492–7501.[35] C.Shui,G.Xu,Q.Chen,J.Li,C.X.Ling,T.Arbel,B.Wang,C.Gagné, Onlearningfairness
andaccuracyonmultiplesubgroups, AdvancesinNeuralInformationProcessingSystems
35(2022)34121–34135.
[36] M.P.Kim, A.Korolova, G.N.Rothblum, G.Yona, Preference-informedfairness, arXiv
preprintarXiv:1904.01793(2019).
[37] J. Gartner, A new metric for quantifying machine learn-
ing fairness in healthcare, https://www.closedloop.ai/blog/
a-new-metric-for-quantifying-machine-learning-fairness-in-healthcare/, 2020. Ac-
cessed: 17-2-2024.
[38] M. Zehlike, K. Yang, J. Stoyanovich, Fairness in ranking: A survey, arXiv preprint
arXiv:2103.14000(2021).
[39] E. Pitoura, K. Stefanidis, G. Koutrika, Fairness in rankings and recommendations: an
overview, TheVLDBJournal(2022)1–28.
[40] E.A.Watkins,M.McKenna,J.Chen, Thefour-fifthsruleisnotdisparateimpact: awoeful
tale of epistemic trespassing in algorithmic fairness, arXiv preprint arXiv:2202.09519
(2022).
[41] I. Sarridis, C. Koutlis, S. Papadopoulos, C. Diou, Flac: Fairness-aware representation
learning by suppressing attribute-class associations, arXiv preprint arXiv:2304.14252
(2023).
[42] E. Krasanakis, A. L. Symeonidis, Forward oriented programming: A meta-dsl for fast
developmentofcomponentlibraries, AvailableatSSRN4180025(????).
[43] W.McKinney,etal., pandas: afoundationalpythonlibraryfordataanalysisandstatistics,
Pythonforhighperformanceandscientificcomputing14(2011)1–9.
[44] C.R.Harris,K.J.Millman,S.J.VanDerWalt,R.Gommers,P.Virtanen,D.Cournapeau,
E.Wieser,J.Taylor,S.Berg,N.J.Smith,etal., Arrayprogrammingwithnumpy, Nature
585(2020)357–362.
[45] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,
N. Gimelshein, L. Antiga, et al., Pytorch: An imperative style, high-performance deep
learninglibrary, Advancesinneuralinformationprocessingsystems32(2019).
[46] M.Abadi,A.Agarwal,P.Barham,E.Brevdo,Z.Chen,C.Citro,G.S.Corrado,A.Davis,
J. Dean, M. Devin, et al., Tensorflow: Large-scale machine learning on heterogeneous
distributedsystems, arXivpreprintarXiv:1603.04467(2016).
[47] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula,
A.Paszke,J.VanderPlas,S.Wanderman-Milne,Q.Zhang,JAX:composabletransformations
ofPython+NumPyprograms,2018.URL:http://github.com/google/jax.
[48] J.Rauber,M.Bethge,W.Brendel, Eagerpy: Writingcodethatworksnativelywithpytorch,
tensorflow,jax,andnumpy, arXivpreprintarXiv:2008.04175(2020).
[49] C.BokehDevelopmentTeam,Bokeh: Pythonlibraryforinteractivevisualization,2014.