NPGA: Neural Parametric Gaussian Avatars
SIMONGIEBENHAIN,TechnicalUniversityofMunich,Germany
TOBIASKIRSCHSTEIN,TechnicalUniversityofMunich,Germany
MARTINRÜNZ,Synthesia,Germany
LOURDESAGAPITO,UniversityCollegeLondon,UnitedKingdom
MATTHIASNIESSNER,TechnicalUniversityofMunich,Germany
Fig.1. NPGA:WeutilizetherichexpressionspaceofNeuralParametricHeadModelstocreatehigh-fidelityavatarswithfine-grainedexpressioncontrol.Our
avatarsconsistofadynamicsmoduleandacanonicalGaussianpointcloud,whichisaugmentedwithper-primitivefeaturesthatencodevaluablesemantic
information,asindicatedontheleft.Ontheright,wedemonstrateahighlydetailedcross-reenactmentusingtheinsetimageasadrivingexpression.
Thecreationofhigh-fidelity,digitalversionsofhumanheadsisanimportant In particular, there is a strong motivation to reconstruct digital
steppingstoneintheprocessoffurtherintegratingvirtualcomponents avatarsfromreal-worldcaptures,suchasmulti-viewrecordings,
intooureverydaylives.Constructingsuchavatarsisachallengingresearch to obtain a digital copy of a specific real person. The resulting
problem,duetoahighdemandforphoto-realismandreal-timerendering digitalavatarscanthenbeanimatedandrenderedfromarbitrary
performance.Inthiswork,weproposeNeuralParametricGaussianAvatars viewpointswhileexpectinghighvisualfidelity;e.g.,withrespectto
(NPGA),adata-drivenapproachtocreatehigh-fidelity,controllableavatars
photo-realisticcolorsanddetails,preservationofidentity,andthe
frommulti-viewvideorecordings.Webuildourmethodaround3DGaussian
adoptionofperson-specificmannerisms.Atthesametime,many
splattingforitshighlyefficientrenderingandtoinheritthetopologicalflexi-
avatarapplicationsdemandreal-timerenderingcapabilitieswithout
bilityofpointclouds.Incontrasttopreviouswork,weconditionouravatars’
dynamicsontherichexpressionspaceofneuralparametricheadmodels compromisingvisualquality.
(NPHM),insteadofmesh-based3DMMs.Tothisend,wedistillthebackward Recentadvancesattheintersectionofcomputergraphicsand
deformationfieldofourunderlyingNPHMintoforwarddeformationswhich vision research have steadily improved methods to digitally re-
arecompatiblewithrasterization-basedrendering.Allremainingfine-scale, construct3Dobjectswithphoto-realisticrenderingquality[Kerbl
expression-dependentdetailsarelearnedfromthemulti-viewvideos.To etal.2023a;Mildenhalletal.2021;Mülleretal.2022].Inpartic-
increasetherepresentationalcapacityofouravatars,weaugmentthecanon- ular,3DGaussianSplatting(3DGS)[Kerbletal.2023a]hasbeen
icalGaussianpointcloudusingper-primitivelatentfeatureswhichgovern quicklyadoptedinrecentworkondigitalhumans,e.g.[Lietal.2024;
itsdynamicbehavior.Toregularizethisincreaseddynamicexpressivity,we
Zielonkaetal.2023]andvirtualheadavatars,e.g.,[Qianetal.2023;
proposeLaplaciantermsonthelatentfeaturesandpredicteddynamics.We
Xuetal.2024],duetoitsefficientrenderingandphoto-realisticre-
evaluateourmethodonthepublicNeRSembledataset,demonstratingthat
constructions.Atthesametimerecentpubliclyavailablemulti-view
NPGAsignificantlyoutperformsthepreviousstate-of-the-artavatarsonthe
self-reenactmenttaskby≈2.6PSNR.Furthermore,wedemonstrateaccurate datasets[Işıketal.2023;Kirschsteinetal.2023b;Panetal.2024;Wuu
animationcapabilitiesfromreal-worldmonocularvideos. etal.2022],offeranevermoreexcitingbasisforavatarresearch.A
centralquestionishowcontrollabilitycanbeachieved.Themost
1 INTRODUCTION prominentapproachforheadsistoutilizea3Dmorphablemodel
(3DMM),whichofferscompactdescriptionsoffacesusingdisentan-
Creatingphoto-realistic3Davatarsisoneofthecorechallenges
gledparametricspacesforidentityandfacialexpressions.When
incomputergraphicsandincludesawiderangeofapplications
utilizingtheexpressionsofanunderlying3DMM,e.g.,[Gafnietal.
suchasmovies,games,AR/VRteleconferencing,andthemetaverse.
2021;Grassaletal.2022;Qianetal.2023;Xuetal.2024;Zielonka
ProjectWebsite:https://simongiebenhain.github.io/NPGA/
4202
yaM
92
]VC.sc[
1v13391.5042:viXra2 • SimonGiebenhain,TobiasKirschstein,MartinRünz,LourdesAgapito,andMatthiasNießner
etal.2022],theavatarisoptimizedtofollowageneralizedexpres- NeuralRadianceFields[Mildenhalletal.2021]providesuchflexi-
sionspacewhichenablesexpressiontransferoranimationthrough bilityandcanbeextendedtodeformablescenes.Thisextensionis
trackinginmonocularvideos[Thiesetal.2016].While3DMMsoffer eitherachievedbymodelingtemporalchangesviaadeformation
acompactparameterization,theirlinearnatureinherentlylimitsthe fieldthataccompaniesacanonicalframe[Parketal.2021a,b],by
fidelityofrepresentedexpressions.Atthesametime,wearguethat addingtimeasaconditioningvariable[Lietal.2022],ordirectlyde-
theunderlyingexpressionspaceplaysacrucialroleindetermining composingthe4Dscenevolumeintoacomputationallymanageable
thequalityofthecreatedavatars.Itnotonlyinfluencesthecon- representation[Attaletal.2023;Songetal.2023].Theextensive
trollabilityoftheresultingavatarsbutitalsolimitsthesharpness useofMLPsinneuralradiancefieldsentailsahighcomputational
ofdetails.Ifthestreamofinputexpressioncodesisinsufficiently burden,however,andstillprovescostlyafterimprovementssuchas
correlatedwiththeobservedimages,theoptimizationproblemcan hashencodings[Mülleretal.2022],triplanes[Chanetal.2022]or
becomefundamentallyill-posedandleadtooverfitting. otherlow-rankapproximations[Shaoetal.2023].MVP[Lombardi
Tothisend,weproposeNPGA,anewavatarrepresentationthat etal.2021]usesaCNNforamortizeddecodingofsmallprimitives
leveragesalearneddeformationrepresentationwhileensuringthat thatcanbecomparedtotheGaussiansin3DGS[Kerbletal.2023a],
thepredictedfacialdynamicsstayclosetothepriorofanunderly- but typically at lower sharpness.One approach to extend 3DGS
ingneuralparametricheadmodel(NPHM)[Giebenhainetal.2023, todynamicscenesistooptimizeparameterslikepositionsover
2024].NPHMprovidesouravatarswithmorefine-grainedexpres- time[Luitenetal.2024],resultinginarepresentationthatishardto
sioncontrolcomparedtoclassical,public3DMMs[Lietal.2017; control.Thislimitationcanbeovercomebyanimatingacanonical
Paysanetal.2009],whichwerepreviouslyusedforavatarcreation. 3DGSrepresentationwithadeformationfield[Yangetal.2023],
NPGAconsistsofacanonicalGaussianpointcloud,thatcanbe similartotheNerfies[Parketal.2021a]approach.NPGAadopts
forward-deformedusinganexpressioncodeandrenderedusing thisparadigmtoo.
3DGS,similartopreviouswork[Qianetal.2023;Xuetal.2024;
Zielonkaetal.2023].Asarasterization-basedapproach,3DGScan- 2.2 3DMorphableModels
notbeefficientlycombinedwiththebackward deformationfield
Traditionalmorphablemodels[BlanzandVetter1999;Paysanetal.
ofMonoNPHM.Therefore,weproposeadistillationstrategyto
2009]learnarepresentationofbodygeometryviaPCA.Theyare
invertthedeformationdirectionofMonoNPHM’sexpressionprior
oneoftheprimarytoolstodrivehumananimationandareheavily
usingacycleconsistencyloss.Theresultingforwarddeformation
usedinindustryapplications,makingthemacorebuildingblockfor
fieldbecomescompatiblewiththerasterization-basedrenderingof
workonvirtualavatars.Whilesomemodelsarededicatedtospecific
3DGS.Toincreasetheoveralldynamicexpressivityofouravatars,
regionsliketheface[Paysanetal.2009]andhead[Lietal.2017],
wefurtherproposetoaugmentourcanonicalGaussianwithper-
somevariantsincludetheneck[Zhangetal.2023]oreventheentire
Gaussianlatentfeatures,whichenableourdeformationmodule
body[Pavlakosetal.2019].Morerecently,neuralequivalentsof
tooperateinahigherdimensionalspace,thatcandescribefacial
3DMMs,suchasi3DMM[Yenamandraetal.2021],ImFace[Zheng
dynamicsmoreeffectively.Weshowthatthisaddedexpressivity
et al. 2022a] or NPHM [Giebenhain et al. 2023], have improved
resultsinhigher-fidelityimagesynthesis,butisrequiredtobeappro-
upontheexpressionfidelitycomparedtoclassicalPCA-basedmod-
priatelyregularizedtoachieveartifact-freerenderings.Tothisend,
els.Theyencodeacanonicalrepresentationofthegeometryvia
weformulateLaplaciansmoothnesstermsonthelatentfeatures
asigneddistancefunctions(SDF),whichcanbemappedtoarbi-
andpredicteddynamics,basedonthek-nearestneighborgraph
traryexpressionsviaadeformationfield.NPGAutilizesNPHMas
incanonicalspace.Furthermore,wemodifytheadaptivedensity
itoffersseveralbeneficialcharacteristics:Itmodelsthefacedensely
controlstrategyof3DGSformoredetailedavatarreconstructions.
includingeyes,hair,andteeth,itcaptureslocaldetailswellandit
Tosummarize,ourcontributionsarethefollowing:
disentanglesshapefromexpressions.
• WeproposeadistillationstrategytoutilizeNPHM’srichex-
pressionpriorformoreexpressiveandcontrollableavatars.
2.3 HumanHeadReconstructionandAnimation
• Weintroduceper-Gaussianlatentfeaturesthatlifttheinput
domainofourdeformationmoduletoahigher-dimensional Existingapproachesforanimatingavatarsmainlydifferintwofun-
space,yieldingincreaseddynamiccapacity. damentalaspects:first,theutilized3Drepresentationincombina-
• Ouravatarsoutperformthepreviousstate-of-the-artby2.6 tionwithitsrenderingmechanism,andsecond,howtheexpression
PSNRand0.021SSIMontheself-reenactmenttask.Further- codesaretransferredintoscenedynamics.Existingworkhasex-
more,wedemonstrateaccurateavatarreconstructionfrom plored,amongothers,meshesincombinationwithdeferredneural
monocularRGBsequences. rendering [Grassal et al. 2022; Kim et al. 2018], neural radiance
fields [Athar et al. 2022; Gafni et al. 2021; Zielonka et al. 2022],
and3DCNNsincombinationwithvolumerendering[Caoetal.
2 RELATEDWORK
2022;Lombardietal.2019,2021].Recently,therehasbeenalotof
2.1 DynamicSceneRepresentations
interestinpoint-basedrepresentationsandrendering[Qianetal.
Sincehumansareinherentlydynamic,andsubjecttotopological 2023;Xuetal.2024;Zhengetal.2023],especiallysinceKerbletal.
variationwhenforexampleopeningandclosingthemouth,general [2023a]proposed3DGaussianSplatting,whichwealsoadoptinour
dynamicscenerepresentationsaimtosolveasetofsimilarproblems work,duetoitsefficientrenderingandtopologicalflexibility.While
withtheexemptionofcontrollability.Implicitapproachesbasedon someapproacheschoosetoexplaintheface’smovementexplicitlyNPGA:NeuralParametricGaussianAvatars • 3
b) Cycle-Consistency Distillation c) NeuralParametricGaussianAvatars
Time CanonicalGaussians DeformedGaussians Rendering
3DGS
CanonicalSpace
a) NPHM Tracking
: per Gaussianfeatures
d) Dynamics : Gaussiancenter
Fig.2. MethodOverview:Thebasisofouravataroptimizationaremulti-viewvideorecordingsalongsideaMonoNPHMtrackingthereof,see(a).Next,we
extractaforward-deformationpriorFfromMonoNPHM’sbackwarddeformationfieldBusingacycle-consistencyloss,see(b).Ouravatarsconsistofa
canonicalGaussianpointcloud(c),whichiswarpedintoposedspaceusingourdynamicsmoduleD,consistingofthecoarsepre-trainedcomponentFanda
detailnetworkG.WeconditionbothnetworksonperGaussianfeatures,whichdictateeachprimitive’sbehavior.Afterrenderingtheavatarwith3DGS,we
employascreen-spaceCNNtosuppresssmall-scaleartifacts.
throughtheunderlyingmeshofa3DMM,e.g.,[Atharetal.2022; deformationfield
Qianetal.2023;Zielonkaetal.2022],otherschoosetheopposing 𝑥
𝑐
=B(𝑥 𝑝; zid,zexp) (2)
extremeofamoredata-drivenapproachbyfreelylearningtheface
thatwarpspoints𝑥 inposedspaceintocanonicalspace𝑥 .
𝑝 𝑐
movementusinganeuralcomponent,whichisdirectlyconditioned
ontheexpressioncodes[Gafnietal.2021;Lombardietal.2021;Xu 4 METHOD
etal.2024].NPGAadoptsthelatterideabutreplacesthe3DMM
Fig.2showsanoverviewofourproposedrepresentationandmethod-
withaneuralparametricmodel.
ologytobuildourNeuralParametricGaussianAvatars(NPGA).As
describedinSection4.1,ouravatarsarecomposedoftwokeycompo-
3 PRELIMINARIES
nents:acanonicalGaussianpointcloudA𝑐 andadynamicsmodule
3.1 3DGaussianSplatting(3DGS)
DwhichdeformstheGaussianswhenprovidedwithanexpression
3DGSusesapoint-basedscenerepresentation,whereeachpoint code,similartorecentwork[Qianetal.2023;Xuetal.2024].In
representsaGaussianprimitivethatisdescribedbyaposition𝜇, Section4.2wedescribeourdistillationstrategythatallowsNPGAto
rotationq,scaleS,opacity𝛼 andsphericalharmonicscoefficients leveragetherichlatentexpressionspaceanddetailedmotionprior
SH.Inthefollowing,weletthefollowingnotation ofMonoNPHM[Giebenhainetal.2024].Givenmulti-viewvideo
recordings alongside tracked MonoNPHM expression codes, we
A={𝜇,q,S,𝛼,SH}, 𝐼 =3dGS(cid:0)A,𝜋 𝐾,𝐸(cid:1) (1)
jointlyoptimizeforourcanonicalGaussiansanddynamicsmodule,
denotethesetofattributesAcomposingtheGaussianpointcloud, asdescribedinSection4.3.
anditstile-baseddifferentiablerasterizationintoanimage𝐼 under
thecameraprojectiondescribedbyintrinsicandextrinsicparame- 4.1 NeuralParametricGaussianAvatars
ters𝐾 and𝐸respectively. 4.1.1 CanonicalRepresentation. Comparedtothedefaultscenerep-
resentationof3DGS,outlinedinEq.(1),weaugmentourcanonical
3.2 NeuralParametricHeadModels
Gaussianpointcloud
3DMMsdescribethegeometry(andappearance)offaces(orheads)
usingdisentangledparametricspacesforidentityandexpression
A𝑐 ={𝜇,q,S,𝛼,SH,}∪{f} (3)
variations.NPHMisaspecialcaseofa3DMM,whichrepresentsa withper-Gaussianfeaturesf ∈R𝑁×8.Whilethesefeaturesarestatic
person’sheadgeometryusinganeuralSDFanddeformationfield themselves,theyprovidecrucialsemanticinformationtodescribe
conditionedonidentitylatentcodeszidandexpressioncodeszexp, thedynamicbehavioroftherespectiveprimitives.Insomesense,
respectively.Inparticular,ourworkbuildsontopofMonoNPHM our per-Gaussian features serve a similar purpose as positional
formulation,whichdescribesexpressionsusinganeuralbackward encodings[Mildenhalletal.2021;Mülleretal.2022],whichare
CCNNNN4 • SimonGiebenhain,TobiasKirschstein,MartinRünz,LourdesAgapito,andMatthiasNießner
uncorrelatedwithspatialcoordinates,haveinfinitespatialresolution expressioncodeszexp.Note,thatinEq.(8)weomitthedependence
anddonotrequireadditionaldatastructures. onexpressioncodeszexpforclarity.Duringthisstage,thecanonical
spaceisnotyetdiscretizedintoasetofGaussianprimitives.Hence,
4.1.2 DynamicsModule. Wemodelfacialexpressionsusingady-
weutilizeafeaturefield
namicsmodelDwhichisdecomposedintotwoMulti-LayerPer-
ceptrons(MLPs),acoarseprior-basednetworkF andanetwork f(𝑥 𝑐)=TriPlane(𝑥 𝑐) (9)
Gresponsibleformodelingallremainingdetails.Ourprior-guided
represented as low-resolution 64x64 triplanes [Chan et al. 2022;
forwarddeformationfield
Pengetal.2020],whichcanbeevaluatedatarbitrarypoints𝑥 in
𝑐
𝛿 𝜇F =F(𝜇,f; zexp) ∈R3 (4) canonicalspace.
isoptimizedtoactastheinverseofMonoNPHM’sbackwarddefor- WetrainF once,usingthetrackedsequencesof20peoplein
mationsB,aswedescribelaterinSection4.2.F isacoordinate- theNeRSembledataset[Kirschsteinetal.2023b],andusethesame
basednetworkwhichpredictsoffsets𝛿F totheGaussiancenters𝜇 F forallouravatars.Duringtraining,eachpersonhastheirown
𝜇
andisconditionedonspatialcoordinates𝜇,featuresf andexpres- TriPlane,whichweregularizeusingatotalvariationloss.Further-
sioncodezexp.Note,thatF actsindependentlyoneachprimitive, more,weregularizethenormofpredictedoffsets∥F(𝑥 𝑐,f(𝑥 𝑐)∥2
whichweomitinEq.(4)andbelowforclarity. tobesmall.
TorepresentdynamicsbeyondNPHM’sprior,suchasfine-scaled
4.3 AvatarOptimizationStrategy
expression-dependentwrinkles,andappearancechanges,e.g.due
toambientocclusionsandchangesinbloodflowconcentration,we After obtaining a forward deformation field F using our cycle-
relyonasecondMLP consistencydistillationstrategy,weaimtojointlyoptimizeforthe
𝛿 𝑎G =G𝑎(𝜇,f; zexp) (∀𝑎∈A𝑐), (5) c ea nn eo rgn yic ta el rp ma .ra Tm oe it ne ir ts iaA liz𝑐 ea tn hd eM caL nP onG ict ao lm Gain ui sm sii az ne sa cp eh no teto rsm 𝜇et wri ec
whichpredictsoffsetsforallcanonicalGaussianattributes𝑎.Weuse sample30.000pointsuniformlyontheiso-surfaceofthetracked
thesamearchitectureforGasforF,besideshavingmoreoutput MonoNPHMmodel.TheperGaussianfeaturesareinitializedby
channelsduetotheincreasednumberofattributeoffsets.Intotal, queryingTriPlane(𝜇)atthesampledGaussiancenters.Allremain-
givenanexpressioncodezexp ∈R100weobtaintheGaussianpoint ingattributesareinitializedusingthedefault3DGSprocedure.In
cloudinposedspaceA𝑝 byadding𝛿F and𝛿G totheirrespective practice,weobservedthatkeepingF frozenresultsinsub-optimal
canonicalattributes,whichwedenoteas performance,whichislikelycausedthroughtopologicalissuesdur-
ingdistillationinthemouthregion.Hence,wedecidetofurther
A𝑝 =D(A𝑐; zexp). (6)
optimize F alongside G and A𝑐, however, using a significantly
4.1.3 Screen-SpaceRefinement. Finally,afterrenderingtheposed smallerlearningrateandawarm-upschedulethatencouragesthe
GaussiansA𝑝 usingthedifferentiablerasterizerfromKerbletal. preservationofthedistilledprior.
[2023b],weapplyascreen-spaceCNNnetwork[Xuetal.2023]: Ouroptimizationstrivestominimizethephotometricdataterm
[𝐼ˆ rgb,𝐼 h] =3dGS(A𝑝; 𝜋 𝐾,𝐸), 𝐼ˆ cnn=CNN([𝐼ˆ rgb,𝐼 h]), (7) L=∥𝐼−𝐼ˆ rgb∥1+𝜆(cid:16) 1−SSIM(𝐼,𝐼ˆ rgb)(cid:17) +𝜆(cid:16) 1−SSIM(𝐼,𝐼ˆ cnn)(cid:17) , (10)
wherethe𝐼ˆ denotesrenderedRGBcolors.𝐼 isalatentimage
rgb ℎ
usedfortheCNNrefinementmodule,whichisobtainedbyrendering where𝐼 denotesarandomlysamplegroundtruthimagefromthe
h+𝛿G,whereharelatentCNNfeatureswhichweadditionally NeRSembledatasetwithcorrespondingexpressioncodeszexp.
h
includeinA𝑐.Note,thatcomparedto[Xuetal.2024]wedonot
4.3.1 Regularization. Wefindthatregularizingbothourcanonical
performsuper-resolution,butmotivatetheuseoftheCNNthrough
representationA𝑐,aswellasourdynamicsmoduleDiscrucialto
increasedperformanceinourablationstudy.
avoidoverfittingtothetrainingexpressions.ToregularizeNPGA
weutilizeaLaplaciansmoothnesstermbasedonthe𝑘NNgraphof
4.2 Cycle-ConsistencyDistillation
canonicalGaussiancenters.Tothisendlet
OneofourcoreideasistoleverageMonoNPHM’smotionpriorand
(cid:13) (cid:13)2
e fix ep ldre Bss ,i won his cp hac we a. rH po sw poev ine tr s,s inin toce cait nu ot nil ii cz ae ls sa pab ca ec ,k ww ear cd and ne ofo tr dm ira et ci to ln y Rlap(𝑥)=(cid:13) (cid:13) (cid:13) (cid:13)|N1
𝑖|
(cid:169) (cid:173)∑︁ 𝑥 𝑗(cid:170) (cid:174)−𝑥 𝑖(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:169) (cid:173)𝑥 ∈{f}∪ (cid:216) 𝛿 𝑎G(cid:170) (cid:174), (11)
incorporatethisdeformationpriorinourpipeline.Instead,weneed (cid:13) (cid:171)𝑗∈N𝑗
(cid:172)
(cid:13)2
(cid:171)
𝑎∈A𝑐
(cid:172)
forwarddeformations,whichwarppointsintoposedspace,suchthat denoteaLaplaciansmoothnessterm,whichweusetoregularize
theycanbedirectlyrasterized.Whileitispossibletonumerically theperGaussianfeaturesf,aswellas,theoffsetpredictions𝛿 𝑎G for
approximatetheinverseofBusingiterativeroot-finding[Chenetal.
allattributes𝑎∈A𝑐.Note,thatwheneverthenumberofcanonical
2023,2021],wesearchforamorecomputationallyefficientmethod.
Gaussianschangesduetodensificationorpruning,werecompute
Instead,weproposetodistillaforwarddeformationnetworkF as
the𝑘NNgraph.
theinverseofBusingacycleconsistencyloss
Inadditiontothesesmoothnessterms,weencourageF andG
Lcyc(𝑥 𝑐)=∥B(F(𝑥 𝑐,f(𝑥 𝑐)))−𝑥 𝑐∥2 2. (8) topredictsmalloffsets
U Bsi fn og raE rq b. it( r8 a) rw ilyec sa an md pi lr ee dct pl oy insu tspe 𝑥r 𝑐vi ∈se RF 3 iw ni cth ant oh ne ik can lo sw pl ae cd ege ano df R𝛿 = 𝜆 𝜇F∥𝛿 𝜇F∥2 2 + 𝑎∑︁ ∈A𝜆 𝑎G∥𝛿 𝑎G−e𝑎∥2 2, (12)NPGA:NeuralParametricGaussianAvatars • 5
wheree𝑎denotestheneutralelementforthegroupoperationact- totheFLAME3DMMmodel[Lietal.2017].Therefore,GaussianA-
ingonattribute𝑎.Similarly,weimposeregularizationontheper vatarscanbeextremelyefficientlyanimated,sincethereisnoneed
GaussianattributesR
f
=∥f∥2 2toremainsmall,andutilizethescale toevaluateacostlyneuralcomponent.Onthedownside,GaussianA-
regularizationlossof[Saitoetal.2024],whichpunishesscales𝑆 vatarsarelimitedtothefacialmovementslyinginsidetheFLAME
lyingoutsideofawell-behavedrange. expressionspace.
4.3.2 AdaptiveDensityControl(ADC). Acentralingredienttothe GaussianHeadAvatar (GHA) [Xu et al. 2024]: GHA is another
successof3DGSisitsstrategytoadaptivelyaddandpruneGaussians recent3DGS-basedavatarmethod,whichalsolearnsdeformation
inareaswheretheyareneededorredundant,basedonasetofsimple fieldsfrommulti-viewvideoandiscontrolledthroughtheircustom
yeteffectiveheuristicsthatareperiodicallyinvoked.Therulesof multi-viewBFM[Paysanetal.2009]tracking.WhileGHAalsouses
ADChavebeendesignedwithstaticscenesinmindandwefind perGaussianfeatures,weallowthesefeaturestoinfluencethepre-
thedefaultsettingstobesuboptimalforouravatarcreation.Inthe dictedmovementandallotherattributes,whileGHArestrictsthem
dynamicscenario,therecanbeareasthatremainhiddenforlarge toinfluenceonlydynamicappearancechanges.Anotherdifference
partsofthetrainingsequence,suchasthemouthinterior.Therefore, toourworkisthatweuseADC,whileGHAassumesafixedsetof
weadjusttheADCbyemployingageneralizedmean Gaussians,andGHAperformssuper-resolution.Forcomputational
(cid:32) (cid:33)1/𝑒 reasonsweremoveoneupsamplinglayer,resultinginatraining
𝑀 𝑖𝑒 = 𝑁1 ∑︁ 𝜏 t𝑒 (13) r Ge Hso Alu wtio hn ico hf i1 s0 c2 o4 nx d1 i0 ti2 o4 nf eo dr oG nH oA u. rF tu rr at ch ke er dm Mor oe n, ow Ne Pa Hdd Ma ev xe pr rs ei so sn ioo nf
𝑡∈𝑇
toaggregatetheview-spacegradients𝜏
𝑡
ofthe𝑖-thprimitiveof codes,indicatedasGHA NPHM.
allframes𝑇 betweeninvocationsoftheADCmechanism.Note,
MixtureofVolumetricPrimitives(MVP)[Lombardietal.2021]:
that𝑒 = 1resultsinthedefault3DGSsettings.Byincreasingthe MVPutilizesacombinationofvolumerenderingandahead-geometry
exponent𝑒theaggregation𝑀𝑒 becomesclosertoamaximumfunc-
𝑖 awareCNNthatcreatesavolumetricpayloadinanamortizedfash-
tion.Therefore,afewvisibleframescanbesufficientfortheADC
ion.Incontrasttoourmethodandtheotherbaselines,MVPutilizes
totriggerdensification,whichisespeciallyimportantforregions
aVariationalAuto-Encoder(VAE)[KingmaandWelling2014]to
liketheteethandmouthinterior.Wefindthat𝑒 = 2alreadyre- learnalatentexpressionencodingbasedontheir3DMMtracking.
sultsinanincreasednumberofGaussians,leadingtomoredetailed
Note, however, that we do not provide MVP with view-average
reconstruction,especiallyinthemouthinterior.
texturestoobtainamorecomparableevaluationsetting.
Furthermore,wereplacethehardopacityresetmechanismof
3DGS, which we find to be harmful to our optimization, with a 5.1.2 Metrics. Toevaluatetheself-reenactmenttaskweusethe
softervariantproposedin[Bulòetal.2024].Insteadofinfrequently PeakSignal-to-NoiseRatio(PSNR),structuralsimilarityindexmea-
setting the 𝛼 values to be almost transparent, the opacities get sure(SSIM),andperceptualLPIPS[Zhangetal.2018]metrics.For
reducedfrequentlyforasmallamountonly,i.e.inourexperiments thesakeofcompleteness,wealsoreportnumbersforadynamic
by0.01. novelviewsynthesis(NVS)scenario,wherewecompareallmeth-
ods on the held-out camera view of the training sequences. We
5 RESULTS focusourevaluationonthefacialregion,sinceneckandtorsoare
Forourexperiments,weusethestate-of-the-art,publicmulti-view notaccuratelyexplainedbyNPHMandtheunderlying3DMMsof
videoNeRSembledataset[Kirschsteinetal.2023b],fromwhichwe ourbaselines.Tothisend,weleveragesegmentationmasksfrom
chooseadiversesetofsixsubjectsperformingchallengingfacial Facer[Zhengetal.2022b]tomaskouttheneckandtorsobefore
expressions.Afterprovidingadditionaldetailsontheperformed computingthemetrics.Furthermore,wecomputemetricsataresolu-
experimentsandbaselinemethodsinSections5.1and5.2,wepresent tionof550x802.SincewetrainGHAon1024x1024wedownsample
our main results on the tasks of self- and cross-reenactment in andcropthegeneratedimagesaccordingly.
Sections5.3and5.4,respectively.Finally,inSection5.5weablatea
5.2 ImplementationDetails
seriesofexperimentsvalidatingourproposedmodelcomponents.
Wehighlyencouragethereadertoconsultoursupplementalvideo Hyper-Parameters. ForbothourdeformationnetworksF andG
forcompleteresultsincludingtemporalinformation. weuse6-layerMLPswithahiddendimensionalityof256.Inorder
topreservethepriorthatF obtainedinourdistillationprocedure,
5.1 ExperimentalSetup
wesetitslearningrateto4𝑒−5,whileGisequippedwithamuch
TheNeRSembledatasetprovides16synchronizedandcalibrated higherlearningrateof2𝑒−3.Additionally,wefreezethenetwork
videos, from which we choose 15 cameras for training and the parametersofF forthefirst5.000optimizationsteps.Wedecayboth
frontalcameraforevaluation.Furthermore,wetrainouravatarson learningratestwicebyafactorof2duringthecourseof800.000
allsequences,exceptforthe"FREE"-sequencewhichwekeepasa optimizationsteps.Furthermore,weemployweightdecayonF
held-outevaluationsequencefortheself-reenactmenttask. andG,usingaweightof0.1asanadditionalregularizationmeasure.
WeperformanADCstepevery5.000iterations,andmultiplythe
5.1.1 Baselines.
gradientthresholdforthedensificationbyafactorof2,toaccom-
GaussianAvatars(GA)[Qianetal.2023]: GaussianAvatarsisa modateforthefactthatourlosscombinesthelossesoftheRGB
recentmethodthatcreates3DGS-basedavatars,bybindingthem renderingandCNN-refinedpredictions.6 • SimonGiebenhain,TobiasKirschstein,MartinRünz,LourdesAgapito,andMatthiasNießner
MVP GaussianAvatars GHA Ours GroundTruth
Fig.3. Self-Reenactment:Qualitativecomparisonofdifferentmethodsontheheld-outsequence.
Runtime. Whilewedonotfocusonefficienttraining,animation, resolutionforanother5hoursoftrainingtime.Furthermore,we
orrenderingofavatars,weacknowledgetheimportanceoffast maskoutthetorso,sinceitisneithercontainedinNPHM’sexpres-
animationandrendering.Inourunoptimizedimplementation,we sionspacenorthefocusofourwork.
canrenderimagesat31framespersecond(FPS)for550x802and
18FPSat1100x1604onanNVIDIARTX3080graphicscard,which
includesdeformation,rendering,andCNN.WhenomittingtheCNN Table1. QuantitativeComparison:Wecompareagainstourbaselines
thespeedincreasesto43and38FPS,respectively.Asacomparison, onself-reenactmentusingaheld-outsequence.Forcompletenesswealso
GHArunsat22FPSat1024x1024onthesamemachine.Wetrainall reportmetricsontheheld-outcameraofthetrainingsequences,denoted
ouravatars,andbaselines,untilconvergence,whichroughlytakes asnovel-viewsynthesis(NVS).
7hoursforGA(onanRTX2080),30hours(onanRTX3090)forGHA
andourmethod,and60hours(onanRTX2080)forMVP. NVS Self-Reenactment
Method
PSNR↑ SSIM↑ LPIPS↓ PSNR↑ SSIM↑ LPIPS↓
DataPreparation. WeobtainMonoNPHMtrackingsontheNeRSem- MVP 33,42 0,957 0,083 27.19 0.919 0.114
bledatasetusingapurelygeometricconstraintbetweentheMonoN-
GA 32,95 0,956 0,080 27.77 0.926 0.104
PHM’s predicted surface and a point cloud reconstructed using
GHA 33,92 0,953 0,045 26.81 0.914 0.077
COLMAP[SchönbergerandFrahm2016].Weutilizethesametrack-
GHA 33,09 0,952 0,049 26.60 0.911 0.078
NPHM
ingalgorithmthathasbeenpreviouslyusedin[Anejaetal.2024;
Ours 37,68 0,973 0,032 30.42 0.935 0.057
Kirschsteinetal.2023a].Fortrainingandquantitativeevaluation,
weusearesolutionof550x802,thesameasweuseforMVPandGA.
Forourqualitativeresults,wefine-tuneouravatarson1100x1604NPGA:NeuralParametricGaussianAvatars • 7
DrivingExpression MVP GaussianAvatars GHA Ours
Fig.4. Cross-Reenactment:Qualitativecomparisonoftransferringadrivingexpressionfromadifferentidentity(left)toanavatar.
5.3 Self-Reenactment 5.4 Cross-Reenactment
Ourmainevaluationisconcernedwiththeself-reenactmenttask. Anothercrucialtaskiscross-reenactment,wheredrivingexpres-
Forthispurpose,allavatarsaretrainedonasetof21trainingse- sionsfromanotherpersonaretransferredtotheavatar.Sincea
quencesalongsidetheirrespectivetrackingresults.Toevaluatethe ground truth for cross-reenactment does not exist, we only re-
avatars,theyareanimatedusingthetrackedexpressionsfroma port a qualitative comparison, which is presented in Fig. 4. We
held-outtestsequence.Wepresentqualitativeandquantitativere- observethatallmethodssuccessfullydisentangleidentityandex-
sultsinFig.3andTable1,respectively,andrecommendthereaderto pressioninformation,allowingforeffectivecross-reenactment.Our
considerthesupplementalvideofortemporalresults.Ourpredicted avatars,however,preservethemostdetailsfromthedrivingex-
self-reenactmentsportraytheunseenexpressionmoreaccurately pressions.Todemonstratereal-worldapplicability,Fig.6depicts
andcontainsharperdetailsinrelativelystaticareaslikethehair cross-reenactmentanimationsofouravatarsusingmonocularRGB
region.Interestingly,GHA performsslightlyworsethanGHA, videosfromacommoditycameraunderreal-worldcircumstances.
NPHM
indicatingthatMonoNPHMexpressioncodesalonedonotimme- Tothisend,weutilizethemonocularMonoNPHMtrackerproposed
diatelyboostperformance.Instead,wehypothesizethatwithout byGiebenhainetal.[2024].
NPHM’smotionpriorasinitialization,NPHM’slatentexpression
distributionmightprovideamorecomplicatedtrainingsignalcom-
5.5 AblationsStudy
paredtothelinearblendshapesofBFM.
InordertoverifyseveralimportantcomponentsofNPGA,weper-
formablationexperimentsusingthreesubjects.Quantitativeand
qualitativeresultsofourablationscanbefoundinTable2andFig.5,
respectively.First,weruna"vanilla"versionofNPGAthatserves8 • SimonGiebenhain,TobiasKirschstein,MartinRünz,LourdesAgapito,andMatthiasNießner
Vanilla +p.G.F +Lap.smoothness Ours Ours-ADC GroundTruth
Fig.5. AblationStudy:WithoututilizingperGaussiansfeatures("Vanilla"),theavatarsfailtorepresentfineexpressiondetailsandcomplicatedregions
liketheeyesandbottomteeth.AddingperGaussianfeatures(p.G.F.)resultsinsignificantlysharperreconstructionsbutispronetoartifactsunderextreme
expressions.AddingourLaplacianregularization("+Lap.smoothness")andascreen-spaceCNN("Ours")finallyresolvesallartifacts.Furthermore,"Ours-ADC"
demonstratesthatthedefaultdensificationstrategyinhibitsdetailedreconstructions.
Table2. Ablations:Weperformourablationexperimentsonasubsetof
threesubjects.WereportNovel-ViewSynthesis(NVS)forcompleteness.
NVS Self-Reenactment
Method
PSNR↑ SSIM↑ LPIPS↓ PSNR↑ SSIM↑ LPIPS↓
Vanilla 35.80 0.965 0.048 30.16 0.927 0.067
+PGF 37.04 0.970 0.037 30.54 0.929 0.059
+Lap.smooth 36.85 0.969 0.038 30.56 0.928 0.059
Ours 37.23 0.972 0.033 30.65 0.933 0.053
Ours-ADC 36.12 0.967 0.045 30,49 0,933 0.070
removedusingourproposedLaplaciansmoothnessterms.Table2in-
dicatesthatusingthisregularizationsignificantlyshrinksthegener-
alizationgapbetweentraining(NVS)andtesting(self-reenactment)
Driving
Expression Cross-Reenactments sequences.Comparedtothismodel,"Ours"alsoincludesaCNN,
whichfurtherboostsmetricsandvisualquality.
Fig.6. Real-WorldApplication:WeutilizethemonocularRGBtracking
fromMonoNPHMtoanimateourhigh-fidelityavatars,demonstratingthe AdaptiveDensityControl. Finally,weshowtheimportanceof
applicabilityofouravatarsoutsideofmulti-viewcapturestudios. using an adopted ADC strategy. Surprisingly, using the default
ADC settings with a densification interval of 5000 steps and an
opacityresetintervalof50.000stepsalmostcompletelydiminishes
theimprovementsofourothercontributions.Whilewedonotclaim
asabaseline.ThisversiondoesnotutilizeperGaussianfeatures, thatusing𝑒 =2inEq.(13)isnecessaryforgreatperformance,we
Laplacian smoothness terms, and uses𝑒 = 1 in Eq. (13) for the simplynotethatfindingasettingthatletsenoughGaussiansappear
ADC.Thismodelfailstoproducesharprenderingsforfine-scale isimportant,especiallyforfine-scaledwrinklesandteeth.
details,andareasthatarecomplicatedduetofrequentocclusions
6 LIMITATIONSANDFUTUREWORK
andreflections,likethebottomteethandeyes.
Inourexperiments,weshowthatNPGAcancreatecontrollableand
Per-GaussianFeatures. WhenaddingperGaussianfeaturestothe high-fidelityvirtualheadavatarsfrommulti-viewvideodata.How-
vanillamodel,denotedas"+p.G.F.",theincreasedrepresentational ever,bothcontrollabilityandreconstructionqualityofouravatars
capacityresultsinsharperreconstructions.Atthesametime,we arefundamentallyrestrictedtowhattheunderlying3DMMcan
occasionallyobserveartifactsof"free-floating"primitives,ashigh- explain.Therefore,regionsliketheneck,torso,tongue,andeyeball
lightedinthesecondcolumnofFig.5.Theseartifactscanbelargely rotation, which are not explained by NPHM’s expression codes,NPGA:NeuralParametricGaussianAvatars • 9
cannotbeanimatedasreliablyormightevenleadtoartifactsdue REFERENCES
tooverfitting.Possiblesolutionsareextensionsoftheunderlying Shivangi Aneja, Justus Thies, Angela Dai, and Matthias Nießner. 2024. Fac-
3DMMtoprovideamorecompletedescriptionofaperson’sstate, eTalk: Audio-Driven Motion Diffusion for Neural Parametric Head Models.
arXiv:2312.08459[cs.CV]
e.g.theinclusionoftheneck[Zhangetal.2023]oreventorsoand
ShahRukhAthar,ZexiangXu,KalyanSunkavalli,EliShechtman,andZhixinShu.2022.
completebodies[Pavlakosetal.2019]. RigNeRF:FullyControllableNeural3DPortraits.InProceedingsoftheIEEE/CVF
Furthermore,asadata-drivenapproachtoavatarcreation,our ConferenceonComputerVisionandPatternRecognition(CVPR).20364–20373.
BenjaminAttal,Jia-BinHuang,ChristianRichardt,MichaelZollhoefer,JohannesKopf,
methodislimited,tosomedegree,totheavailabletrainingdataper MatthewO’Toole,andChangilKim.2023.HyperReel:High-Fidelity6-DoFVideo
person.Webelievethatrecentlarge-scalemulti-viewvideodataset withRay-ConditionedSampling.InConferenceonComputerVisionandPattern
ofhumanheads[Kirschsteinetal.2023b;Panetal.2024]openup
Recognition(CVPR).
VolkerBlanzandThomasVetter.1999.Amorphablemodelforthesynthesisof3Dfaces.
opportunitiestolearnageneralizedheadmodel,suchas[Caoetal. InProceedingsofthe26thannualconferenceonComputergraphicsandinteractive
2022],withmuchhigherfidelitythanNPHMandotheravailable techniques.187–194.
SamuelRotaBulò,LorenzoPorzi,andPeterKontschieder.2024.RevisingDensification
3DMMs,throughtheuseofphotometricoptimizationandefficient
inGaussianSplatting. arXiv:2404.06109[cs.CV]
rendering,like3DGS. ChenCao,TomasSimon,JinKyuKim,GabeSchwartz,MichaelZollhoefer,Shun-Suke
Saito,StephenLombardi,Shih-EnWei,DanielleBelko,Shoou-IYu,YaserSheikh,and
7 CONCLUSION JasonSaragih.2022.AuthenticVolumetricAvatarsfromaPhoneScan.ACMTrans.
Graph.41,4,Article163(jul2022),19pages. https://doi.org/10.1145/3528223.3530143
Inthiswork,wehaveproposedNeuralParametricGaussianAvatars EricR.Chan,ConnorZ.Lin,MatthewA.Chan,KokiNagano,BoxiaoPan,ShaliniDe
Mello,OrazioGallo,LeonidasGuibas,JonathanTremblay,SamehKhamis,Tero
(NPGA),amethodforcreatingaccuratelycontrollableandhigh-
Karras,andGordonWetzstein.2022. EfficientGeometry-aware3DGenerative
fidelityvirtualheadavatars.Akeycomponentofourworkisthe AdversarialNetworks.InCVPR.
usageofMonoNPHM’srichexpressionspaceandmotionprior.To XuChen,TianjianJiang,JieSong,MaxRietmann,AndreasGeiger,MichaelJ.Black,
andOtmarHilliges.2023. Fast-SNARF:AFastDeformerforArticulatedNeural
thisend,weproposeacycle-consistencystrategytodistillafor- Fields.PatternAnalysisandMachineIntelligence(PAMI)(2023).
ward deformation field from MonoNPHM, such that it becomes XuChen,YufengZheng,MichaelJBlack,OtmarHilliges,andAndreasGeiger.2021.
SNARF:DifferentiableForwardSkinningforAnimatingNon-RigidNeuralImplicit
compatiblewith3DGaussianSplatting.Furthermore,wehavein-
Shapes.InInternationalConferenceonComputerVision(ICCV).
troduced per Gaussian features, a simple technique, to help our GuyGafni,JustusThies,MichaelZollhöfer,andMatthiasNießner.2021.DynamicNeural
deformationmodulesexplainthedynamicbehaviorofouravatar’s RadianceFieldsforMonocular4DFacialAvatarReconstruction.InProceedingsofthe
canonicalGaussianprimitives.Weproposedaneffectiveregular-
IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR).8649–8658.
SimonGiebenhain,TobiasKirschstein,MarkosGeorgopoulos,MartinRünz,Lourdes
izationstrategyandadjustedtheadaptivedensitycontrolstrategy, Agapito,andMatthiasNießner.2023.LearningNeuralParametricHeadModels.In
bothofwhichareimportantforidealavatarquality.Inourexper- Proc.IEEEConf.onComputerVisionandPatternRecognition(CVPR).
SimonGiebenhain,TobiasKirschstein,MarkosGeorgopoulos,MartinRünz,Lourdes
iments,wesignificantlyoutperformthepreviousstate-of-the-art Agapito,andMatthiasNießner.2024. MonoNPHM:DynamicHeadReconstruc-
avatarsonself-reenactment.Finally,weshowedtheapplicability tionfromMonocularVideos.InProc.IEEEConf.onComputerVisionandPattern
ofouravatarsbeyondacontrolledmulti-viewset-upbyanimating
Recognition(CVPR).
Philip-WilliamGrassal,MaltePrinzler,TitusLeistner,CarstenRother,MatthiasNießner,
themfrommonocularRGBvideotrackings. andJustusThies.2022.NeuralheadavatarsfrommonocularRGBvideos.InPro-
ceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
Acknowledgements. ThisworkwasfundedbySynthesiaandsup- 18653–18664.
portedbytheERCStartingGrantScan2CAD(804724),theGerman MustafaIşık,MartinRünz,MarkosGeorgopoulos,TarasKhakhulin,JonathanStarck,
LourdesAgapito,andMatthiasNießner.2023.Humanrf:High-fidelityneuralradi-
ResearchFoundation(DFG)ResearchUnit“LearningandSimulation
ancefieldsforhumansinmotion.arXivpreprintarXiv:2305.06356(2023).
inVisualComputing”.Wewouldliketothankourresearchassistant BernhardKerbl,GeorgiosKopanas,ThomasLeimkühler,andGeorgeDrettakis.2023a.
MohakMansharamani,andAngelaDaiforthevideovoice-over. 3DGaussianSplattingforReal-TimeRadianceFieldRendering.ACMTransactionson
Graphics42,4(July2023).https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/
BernhardKerbl,GeorgiosKopanas,ThomasLeimkühler,andGeorgeDrettakis.2023b.
3DGaussianSplattingforReal-TimeRadianceFieldRendering.ACMTransactionson
Graphics42,4(July2023).https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/
HyeongwooKim,PabloGarrido,AyushTewari,WeipengXu,JustusThies,Matthias
Nießner,PatrickPérez,ChristianRichardt,MichaelZollöfer,andChristianTheobalt.
2018.DeepVideoPortraits.ACMTransactionsonGraphics(TOG)37,4(2018),163.
Diederik P. Kingma and Max Welling. 2014. Auto-Encoding Variational
Bayes. In 2nd International Conference on Learning Representations, ICLR
2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings.
arXiv:http://arxiv.org/abs/1312.6114v10[stat.ML]
TobiasKirschstein,SimonGiebenhain,andMatthiasNießner.2023a. DiffusionA-
vatars:DeferredDiffusionforHigh-fidelity3DHeadAvatars. arXivpreprint
arXiv:2311.18635(2023).
TobiasKirschstein,ShenhanQian,SimonGiebenhain,TimWalter,andMatthiasNießner.
2023b. NeRSemble:Multi-ViewRadianceFieldReconstructionofHumanHeads.
ACMTrans.Graph.42,4,Article161(jul2023),14pages. https://doi.org/10.1145/
3592455
TianyeLi,TimoBolkart,Michael.J.Black,HaoLi,andJavierRomero.2017.Learninga
modeloffacialshapeandexpressionfrom4Dscans.ACMTransactionsonGraphics,
(Proc.SIGGRAPHAsia)36,6(2017),194:1–194:17. https://doi.org/10.1145/3130800.
3130813
TianyeLi,MiraSlavcheva,MichaelZollhöfer,SimonGreen,ChristophLassner,Changil
Kim,TannerSchmidt,StevenLovegrove,MichaelGoesele,RichardNewcombe,
andZhaoyangLv.2022. Neural3DVideoSynthesisFromMulti-ViewVideo.In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition
(CVPR).5521–5531.10 • SimonGiebenhain,TobiasKirschstein,MartinRünz,LourdesAgapito,andMatthiasNießner
ZheLi,ZerongZheng,LizhenWang,andYebinLiu.2024. AnimatableGaussians: YuelangXu,HongwenZhang,LizhenWang,XiaochenZhao,HuangHan,QiGuojun,
LearningPose-dependentGaussianMapsforHigh-fidelityHumanAvatarModeling. andYebinLiu.2023.LatentAvatar:LearningLatentExpressionCodeforExpressive
InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition NeuralHeadAvatar.InACMSIGGRAPH2023ConferenceProceedings.
(CVPR). ZiyiYang,XinyuGao,WenZhou,ShaohuiJiao,YuqingZhang,andXiaogangJin.2023.
StephenLombardi,TomasSimon,JasonSaragih,GabrielSchwartz,AndreasLehrmann, Deformable3DGaussiansforHigh-FidelityMonocularDynamicSceneReconstruc-
andYaserSheikh.2019.NeuralVolumes:LearningDynamicRenderableVolumes tion.arXivpreprintarXiv:2309.13101(2023).
fromImages.ACMTrans.Graph.38,4,Article65(July2019),14pages. TarunYenamandra,AyushTewari,FlorianBernard,Hans-PeterSeidel,MohamedEl-
StephenLombardi,TomasSimon,GabrielSchwartz,MichaelZollhoefer,YaserSheikh, gharib,DanielCremers,andChristianTheobalt.2021.i3DMM:DeepImplicit3D
andJasonSaragih.2021. MixtureofVolumetricPrimitivesforEfficientNeural MorphableModelofHumanHeads.InProceedingsoftheIEEE/CVFConferenceon
Rendering.ACMTrans.Graph.40,4,Article59(jul2021),13pages. https://doi.org/ ComputerVisionandPatternRecognition.12803–12813.
10.1145/3450626.3459863 LongwenZhang,ZijunZhao,XinzhouCong,QixuanZhang,ShuqiGu,YuchongGao,
JonathonLuiten,GeorgiosKopanas,BastianLeibe,andDevaRamanan.2024.Dynamic RuiZheng,WeiYang,LanXu,andJingyiYu.2023.HACK:LearningaParametric
3DGaussians:TrackingbyPersistentDynamicViewSynthesis.In3DV. HeadandNeckModelforHigh-FidelityAnimation.ACMTrans.Graph.42,4,Article
BenMildenhall,PratulPSrinivasan,MatthewTancik,JonathanTBarron,RaviRa- 41(jul2023),20pages. https://doi.org/10.1145/3592093
mamoorthi,andRenNg.2021.Nerf:Representingscenesasneuralradiancefields RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang.2018.
forviewsynthesis.Commun.ACM65,1(2021),99–106. TheUnreasonableEffectivenessofDeepFeaturesasaPerceptualMetric.InCVPR.
ThomasMüller,AlexEvans,ChristophSchied,andAlexanderKeller.2022. Instant MingwuZheng,HongyuYang,DiHuang,andLimingChen.2022a.ImFace:ANonlinear
NeuralGraphicsPrimitiveswithaMultiresolutionHashEncoding. ACMTrans. 3DMorphableFaceModelwithImplicitNeuralRepresentations.InProceedingsof
Graph.41,4,Article102(July2022),15pages. https://doi.org/10.1145/3528223. theIEEE/CVFConferenceonComputerVisionandPatternRecognition.
3530127 YinglinZheng,HaoYang,TingZhang,JianminBao,DongdongChen,YangyuHuang,Lu
DongweiPan,LongZhuo,JingtanPiao,HuiwenLuo,WeiCheng,YuxinWang,Siming Yuan,DongChen,MingZeng,andFangWen.2022b.Generalfacialrepresentation
Fan,ShengqiLiu,LeiYang,BoDai,ZiweiLiu,ChenChangeLoy,ChenQian,Wayne learninginavisual-linguisticmanner.InProceedingsoftheIEEE/CVFConferenceon
Wu,DahuaLin,andKwan-YeeLin.2024. RenderMe-360:ALargeDigitalAsset ComputerVisionandPatternRecognition.18697–18709.
LibraryandBenchmarksTowardsHigh-fidelityHeadAvatars.AdvancesinNeural YufengZheng,WangYifan,GordonWetzstein,MichaelJ.Black,andOtmarHilliges.
InformationProcessingSystems36(2024). 2023.PointAvatar:DeformablePoint-basedHeadAvatarsfromVideos.InProceedings
KeunhongPark,UtkarshSinha,JonathanTBarron,SofienBouaziz,DanBGoldman, oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR).
StevenMSeitz,andRicardoMartin-Brualla.2021a. Nerfies:Deformableneural Wojciech Zielonka, Timur Bagautdinov, Shunsuke Saito, Michael Zollhöfer, Jus-
radiancefields.InProceedingsoftheIEEE/CVFInternationalConferenceonComputer tus Thies, and Javier Romero. 2023. Drivable 3D Gaussian Avatars. (2023).
Vision.5865–5874. arXiv:2311.08581[cs.CV]
KeunhongPark,UtkarshSinha,PeterHedman,JonathanT.Barron,SofienBouaziz, WojciechZielonka,TimoBolkart,andJustusThies.2022. InstantVolumetricHead
DanBGoldman,RicardoMartin-Brualla,andStevenM.Seitz.2021b.HyperNeRF: Avatars. arXiv:2211.12499[cs.CV]
AHigher-DimensionalRepresentationforTopologicallyVaryingNeuralRadiance
Fields.ACMTrans.Graph.40,6,Article238(dec2021).
GeorgiosPavlakos,VasileiosChoutas,NimaGhorbani,TimoBolkart,AhmedA.A.
Osman,DimitriosTzionas,andMichaelJ.Black.2019.ExpressiveBodyCapture:3D
Hands,Face,andBodyfromaSingleImage.InProceedingsIEEEConf.onComputer
VisionandPatternRecognition(CVPR).10975–10985.
PascalPaysan,ReinhardKnothe,BrianAmberg,SamiRomdhani,andThomasVetter.
2009.A3Dfacemodelforposeandilluminationinvariantfacerecognition.In2009
sixthIEEEinternationalconferenceonadvancedvideoandsignalbasedsurveillance.
Ieee,296–301.
SongyouPeng,MichaelNiemeyer,LarsMescheder,MarcPollefeys,andAndreasGeiger.
2020. ConvolutionalOccupancyNetworks.InEuropeanConferenceonComputer
Vision(ECCV).
ShenhanQian,TobiasKirschstein,LiamSchoneveld,DavideDavoli,SimonGiebenhain,
andMatthiasNießner.2023. GaussianAvatars:PhotorealisticHeadAvatarswith
Rigged3DGaussians.arXivpreprintarXiv:2312.02069(2023).
ShunsukeSaito,GabrielSchwartz,TomasSimon,JunxuanLi,andGiljooNam.2024.
RelightableGaussianCodecAvatars.InCVPR.
JohannesLutzSchönbergerandJan-MichaelFrahm.2016. Structure-from-Motion
Revisited.InConferenceonComputerVisionandPatternRecognition(CVPR).
RuizhiShao,ZerongZheng,HanzhangTu,BoningLiu,HongwenZhang,andYebin
Liu.2023.Tensor4D:EfficientNeural4DDecompositionforHigh-fidelityDynamic
ReconstructionandRendering.InProceedingsoftheIEEEConferenceonComputer
VisionandPatternRecognition.
LiangchenSong,AnpeiChen,ZhongLi,ZhangChen,LeleChen,JunsongYuan,YiXu,
andAndreasGeiger.2023.NeRFPlayer:AStreamableDynamicSceneRepresentation
withDecomposedNeuralRadianceFields.IEEETransactionsonVisualizationand
ComputerGraphics29,5(2023),2732–2742. https://doi.org/10.1109/TVCG.2023.
3247082
J.Thies,M.Zollhöfer,M.Stamminger,C.Theobalt,andM.Nießner.2016.Face2Face:
Real-timeFaceCaptureandReenactmentofRGBVideos.InProc.ComputerVision
andPatternRecognition(CVPR),IEEE.
Cheng-hsinWuu,NingyuanZheng,ScottArdisson,RohanBali,DanielleBelko,Eric
Brockmeyer,LucasEvans,TimothyGodisart,HyowonHa,XuhuaHuang,Alexan-
derHypes,TaylorKoska,StevenKrenn,StephenLombardi,XiaominLuo,Kevyn
McPhail,LauraMillerschoen,MichalPerdoch,MarkPitts,AlexanderRichard,Ja-
sonSaragih,JunkoSaragih,TakaakiShiratori,TomasSimon,MattStewart,Au-
tumnTrimble,XinshuoWeng,DavidWhitewolf,ChengleiWu,Shoou-IYu,and
YaserSheikh.2022. Multiface:ADatasetforNeuralFaceRendering.InarXiv.
https://doi.org/10.48550/ARXIV.2207.11243
YuelangXu,BenwangChen,ZheLi,HongwenZhang,LizhenWang,ZerongZheng,
andYebinLiu.2024. GaussianHeadAvatar:UltraHigh-fidelityHeadAvatarvia
DynamicGaussians.InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition(CVPR).