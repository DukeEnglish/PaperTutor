Are Large Language Models Chameleons?
MingmengGeng1∗ SihongHe2 RobertoTrotta1,3∗
1InternationalSchoolforAdvancedStudies(SISSA) 2UTArlington
3ImperialCollegeLondon
Abstract
Do large language models (LLMs) have their own worldviews and personality
tendencies?SimulationsinwhichanLLMwasaskedtoanswersubjectivequestions
were conducted more than 1 million times. Comparison of the responses from
differentLLMswithrealdatafromtheEuropeanSocialSurvey(ESS)suggests
thattheeffectofpromptsonbiasandvariabilityisfundamental,highlightingmajor
cultural,age,andgenderbiases. Methodsformeasuringthedifferencebetween
LLMs and survey data are discussed, such as calculating weighted means and
a new proposed measure inspired by Jaccard similarity. We conclude that it is
importanttoanalyzetherobustnessandvariabilityofpromptsbeforeusingLLMs
tomodelindividualdecisionsorcollectivebehavior,astheirimitationabilitiesare
approximateatbest.
1 Introduction
Although often referred to as “stochastic parrot” (Bender et al., 2021), the application of large
languagemodels(LLMs)goesbeyond"talking"(Staabetal.,2023). Theiradaptabilityandversatility
remindusofanotheranimal: thechameleon.
DoLLMshavetheirownopinions? IfweaskLLMstoanswersomesubjectivequestions,willthey
behavelikehumans? Andcanwerelyontheirabilitytomimicoursubjectiveopinionstocreate
entirelysyntheticopinionpolls,orasanalternativetopost-stratification?
ThealignmentbetweenLLMsoutputsandhumanvalueshasattractedgreatinterestandhasalready
beenmeasured(Hendrycksetal.,2020a). Whatcouldbecalledthe“personalitytraits”ofLLMs
havealsobeenwidelyexploredinseveralpapers(Safdarietal.,2023;Jiangetal.,2023b,2024;Mei
etal.,2024). It’snotdifficulttocomparetheresponsesofLLMstosubjectivequestionswithsome
responsescompletedbyrealhumanbeings. Butweknowthathumanopinionsaregenerallynot
uniformlydistributedamongpeople,andindeedthisisthecruxofreliablesurveys.
WhatifpromptswithpersonalinformationareusedtomakeLLMs’responsesmorespecific? How
shouldwecomparesuchsimulateddatawithrealsurveydata?
Insurveyresearch,anumberofdemographicvariablesareroutinelyused,suchasgender,race,and
age(Goldbergetal.,1998;Herek,2002;Kyriakidisetal.,2015). Genderandracearetwoimportant
perspectivestoconsiderforbiasinLLMs(Sunetal.,2023;Koteketal.,2023;Dongetal.,2024).
ThesensitivityofLLMstoagewasalsoanalyzed,basedonsixagegroups(Liuetal.,2024). Butthe
influenceandrobustnessofthepromptsarerarelydiscussedinpapersdealingwithopinionsinLLMs
(Röttgeretal.,2024). TheabilityofLLMstosimulatesurveysneedstobeexploredinmoredepth.
HowcanoneanalyzetheresponsesofLLMstosubjectivequestionsusingquantitativemethods?
Bias in LLMs is almost impossible to avoid, even with subsequent adjustments and alignments
(Naviglietal.,2023). OurgoalistomeasurethedifferencebetweenLLMs’responsesandwhat
∗Correspondingauthors:MingmengGeng(mgeng@sissa.it)andRobertoTrotta(rtrotta@sissa.it).
Preprint.Underreview.
4202
yaM
92
]LC.sc[
1v32391.5042:viXrareal people think, by asking LLMs to answer actual survey questions with a prompt giving the
correspondingdemographicvariables. Thenwecancomparethesimilaritiesanddifferencesbetween
thesimulatedresultsandtherealsurveydata. Thedataandresultsobtainedfromsurveysalsodeviate
toagreaterorlesserextentfromthe“groundtruth”,whiletherepresentativenessofthesampleis
oftendebated(Russelletal.,2022;GrafströmandSchelin,2014). Therefore,wewanttoanalyzethe
biasinresponsesofLLMs,usingrealsurveydataasthe“groundtruth”,whichmeansthatthesurvey
resultsareassumedtoberepresentative.
ItisalsoimportanttoevaluatedifferencesbetweendifferentLLMs,bycomparingtheir‘opinions’
withrealsurveydata. DefiningtheconditionsunderwhichLLMscanassistorevenreplacehuman
participants(ifatall),andunderwhichtheyareunrealistic,isthefocusofourresearch.
2 Relatedwork
LLM bias Bias in natural language processing (NLP) arises for several reasons, such as data,
models,andresearchdesign(HovyandPrabhumoye,2021). TherisksandshortcomingsofLLMs
wereaddressedevenbeforetheybecamepopular(Benderetal.,2021). WhileLLMssolvemanyof
thedifficultiesthattraditionalNLPmethodshavefailedtoovercome,thechallengeofbiaspersists
(Navigli et al., 2023). For example, results based on GPT-3 show that the opinions of certain
demographic groups in the United States are not well represented (Santurkar et al., 2023). The
politicalbiasofChatGPTinothercountrieshasalsobeenquantified(Motokietal.,2024). Biasalso
lookslikeaproblemsharedbyotherLLMs(Boelaertetal.,2024),andnotspecifictoChatGPT.
LLMevaluation ThepervasivenessandinfluenceofLLMsisincreasingrapidly,anditisbecoming
moreimportanttobetterevaluateandcomparedifferentLLMs(Changetal.,2024). Platformsthat
buildonusers’comparativeratingshavealsoemerged,forexample,ChatbotArena(Chiangetal.,
2024). Traditionalobjectiveevaluationcriteriaforsolvingvarioustasks,suchasMMLU(Massive
MultitaskLanguageUnderstanding)(Hendrycksetal.,2020b),donotmeetcurrentneeds. Therefore,
novelevaluationmethodshavebeenproposed,suchasuncertaintyquantification(Yeetal.,2024).
LLMalignment HowtoalignLLMswithhumanbehaviorisanexcitingandchallengingtask
(Ouyangetal.,2022). Despitethemanylimitations(Wolfetal.,2023;Kirketal.,2023),wecansee
theattemptsandresultsofmanyresearchersonthealignmentofLLMs(Shenetal.,2023;Liuetal.,
2023). LLMsareusedinavarietyofapplications,socorrespondingdatasetsandbenchmarksare
alsoneededfordifferentusagescenarios(Köpfetal.,2024;Kooetal.,2023).
3 Dataandmodels
WhilemostresearchhasfocusedontheUnitedStates,ourconcerniswiththeEuropeancountries,
whicharemoreheterogeneousintermsofcultureandlanguages. Trainingdataarealsolikelytobe
ofverydisparatesize,especiallyforsmallercountriesandlanguagesspokenbyarelativelysmall
numberofpeople,whichislikelytoinfluencetheLLMs’performancewhenimpersonatingsomebody
fromthatculture.
Tobettercomparewithrealpeople’sopinions,wemadeuseofdatafromtheEuropeanSocialSurvey
(ESS), a biennial survey of attitudes and behaviour. We chose the round 10 of ESS (European
SocialSurveyERIC(ESSERIC),2022),collectedfromSeptember2020upuntilMay2022,with
59,686participantsfrom31Europeancountries,releasedfromJune2022toMay2023(detailsinthe
appendix). Germanyhasthemostparticipants,totaling8,725,whileCyprushastheleast,withonly
875participants.
InadditiontoChatGPT(includingGPT-3.5andGPT-4o),wealsocarriedoutsimulationswithopen
sourcemodelsviaAPIsorlocally,includingLLaMA-2(Touvronetal.,2023),LLaMA-3,Mistral
(Jiangetal.,2023a)andDeepSeek-V2(Bietal.,2024).
24 Methods
4.1 Simulations
TheperformanceofLLMsneedstobeevaluatedfromsimulations,asitisnotpossibletopredictthe
responseofLLMsthroughanalysisofthemodelstructureanditsparameters. WepurchasedAPI
tokensfromseveralcompaniestoperformsimulationsandutilizedtrainedopen-sourcemodels. As
withtherealsurvey,weusedtheChatmodelratherthanthecompletionofsentences. Forexample:
P1 Youarea[manorwoman]bornin[year]livingin[country]. Towhatextentdoyouagreewith
thefollowingstatement: ‘[statement]’? Pleaserespondwithonlyanumber(1)Agreestrongly(2)
Agree(3)Neitheragreenordisagree(4)Disagree(5)Disagreestrongly.
Inrealsurveyexperiments,theorderofthequestionsalsoaffectstherespondents’answers(Carlsson
et al., 2012), and we ask one question at a time, so it’s always zero-shot. The same personal
informationasintherealsurveydatawasusedinthesimulationsofLLMs. Nationality,gender,and
agearethedemographicvariablesofinterest.
Inadditiontothedifferentprompts,theinfluenceofparameterscannotbeignored,suchastemperature
andtop_p. Differentparametersweretestedtocomparetheeffectsofthemodels.
4.2 Measurements
PreviousstudieshaveshownthatLLMsarelikelytobebiased,butthereisnoagreementonhowlarge
thesebiasesare. Theoretically,thisdependsnotonlyonthemodelitselfbutalsoonthequestions
beingasked. Hence,wetriedtotestandvalidatedifferentLLMstodeterminewhetheranyobserved
biasinoursurveyquestionsisspecifictocertainmodelsormorewidespread.
Aswehavepreviouslystated,wearedealingwithsubjectivequestionsthatdonothavestandard
answers,buttheresultsofsurveysfromrealpeopleareavailable. Therefore,wefocusondatawhere
participantsagreeordisagreewithastatement,whileforsimplicitythemissingdataoptions(“Re-
fusal”,“Don’tknow”,“Noanswer”)weredisregarded. Theordinalnumbersoftheresponsesfrom1
(“Agreestrongly”)to5(“Disagreestrongly”)wereusedasascalefortheirlevelofdisagreement.
OurgoalistocompareresponsesfromLLMstotheonesfromhumans,soweareinterestednotonly
inaveragesbutalsointhevariance. Tobettercomparetheimpactofdifferentpromptsanddifferent
LLMs,weemploythefollowingmeasures: mean,variance,andJ-index.
InspiredbyJaccardsimilarity(Chierichettietal.,2010),wedefinetheJ-indexJ (G)forstatementq
q
andpeoplegroupGthatweareinterestedin(e.g.,acountry)asfollowing
(cid:80) I(cid:0) Rh(g),Rm(g))(cid:1) (cid:80) |Rh(g)∩Rm(g)|
J (G)= g∈G q q = g∈G q q (1)
q (cid:80) U(cid:0) Rh(g),Rm(g))(cid:1) (cid:80) |Rh(g)∪Rm(g)|
g∈G q q g∈G q q
whereI(·,·)andU(·,·)aretheintersectionandunionoftwosetsofresponses,gisthesubgroupof
G,Rh(g)andRm(g)representtheresponsesfromsurveydataandLLMsimulations,respectively.
q q
AJ-indexof1thuscorrespondstoperfectcongruencebetweenbetweensurveyandLLMsimulation.
We further take the analysis weight (combination of post-stratification weight and population
weight) w for participant i provided in the ESS dataset to re-weigh
I(cid:0) Rh(g),Rm(g)(cid:1)
and
i q q
U(cid:0) Rh(g),Rm(g)(cid:1)
toimprovetherepresentativenessofsurveyrespondentsconcerningthetarget
q q
population,
  
I(cid:0) R qh(g),R qm(g)(cid:1) = (cid:88) min(cid:88) w i1 r qh(i)=v,(cid:88) w i1 r qm(i)=v (2)
v∈Vq i∈g i∈g
  
U(cid:0) R qh(g),R qm(g)(cid:1) = (cid:88) max(cid:88) w i1 r qh(i)=v,(cid:88) w i1 r qm(i)=v (3)
v∈Vq i∈g i∈g
wherevcorrespondstothevalueoftheresponse,V isthesetofallpossiblevaluesforquestionq,
q
rh(i)representstheanswerinsurveydata,andrm(i)meanstheresponseofLLMgiventhesame
q q
informationasparticipanti.
35 Results
In our analysis, we used the four countries with the highest number of participants in the ESS
(Germany,Greece,Bulgaria,andItaly,with16132samplesintotal). Allofourresultstakepost-
stratificationweightingintoaccount.
5.1 ChatGPTsimulations
ConsideringanswersfromGPT-3.52 (temperature = 1,top_p = 0.9)andusingthepromptP1as
describedabove, weadoptedspecificbirthyearsratherthanagegroupingstobetterdemonstrate
thesensitivityofChatGPTtoage. ChatGPTdoesnotalwaysrespondwithonlyonenumberaswe
requested,sometimesincludingadescriptionafterthenumber(e.g.,“(1)Agreestrongly”),butit
doesn’trefusetoanswer. TheresultsforthefirststatementareshowninFigure1.
(a)Men (b)Women
Figure1:Q1:“Gaysandlesbiansfreetolivelifeastheywish”? Prompt:P1. Thepointsrepresentthe
meanandtheerrorbarsrepresentthestandarddeviation(andthesameforthenextfigures). Model:
GPT-3.5.
Atleastonthisquestion,theoutputofGPT-3.5isindeeddeterminedbythedemographicvariables
givenintheprompts. ThemeanofthesimulationresultsisclosetothemeanofESSdata,exceptfor
Germansbornbefore1940. However,thevarianceofthesimulateddataistoosmallcomparedtothe
realdata,evenwhenthetemperatureparameterissettoitsmaximumvalue(seeAppendix).
Theimpactofthesamevariablecanvaryfordifferenttopics. Forexample,thedifferenceresulting
fromgenderisevidentinLLM’sresponsetothesecondquestion,asillustratedinFigure2.
Althoughwealwaysuseddifferentrandomseedsfordifferentpersonsandthesamerandomseed
foronepersonacrossdifferentquestionsinourexperiments,theproblemoftoolittlevarianceinthe
simulationsiscommonforvariousquestions,ashasbeenpointedoutbyotherresearchers(Boelaert
etal.,2024). Inparticular,theexampleinFigure2showsthatalthoughthemeanofESSdataand
GPT-3.5simulationsarecloseforwomen,thevarianceofsimulationsisverysmall–almostnon-
existent–inthiscase. Thiswouldseemtoimplythatwomenareassociatedwithmorestereotypical
answers,anotherfacetofgenderbias.
5.2 Promptsandparameters
Inadditiontothebiasesandstereotypesthatmayresultfromthemodelitselfandthetrainingdata,
theissuerelatedtovarianceisalsolikelycausedbythefactthatweprovidedtoolittleinformation
inpromptP1,whichisfarfromarealhumaninreallife. Therefore,wetriedpromptswithamore
informativedescription,suchastherespondent’soccupation:
2gpt-3.5-turbo-0125
4(a)Men (b)Women
Figure 2: Q2: “Government should reduce differences in income levels”? Prompt: P1. Model:
GPT-3.5.
P2 Youarea[manorwoman]bornin[year]livingin[country]. Occupationcategory: [ISCO3]
Towhatextentdoyouagreewiththefollowingstatement: ‘[statement]’? Pleaserespondwithonly
anumber(1)Agreestrongly(2)Agree(3)Neitheragreenordisagree(4)Disagree(5)Disagree
strongly.
The results generated by prompt P2 are shown in Figure 3, where the variability of simulations
becomesclosertothevarianceofsurveydata. Thisisnotbecauseouradditiontotheprompthas
increasedthedegreeofrandomness,butratherbecauseoccupationaldifferenceswerepreviously
ignored.
(a)Men (b)Women
Figure3: Q1: “Gaysandlesbiansfreetolivelifeastheywish”? Prompt: P2. Model: GPT-3.5.
The impact of prompts is also not just about how much information is provided – the order also
matters,ashasbeenshownpreviously(PezeshkpourandHruschka,2023). Forexample,wetried
reversingtheorderofansweroptionsinthepromptasfollows:
P3 Youarea[manorwoman]bornin[year]livingin[country]. Towhatextentdoyouagreewith
thefollowingstatement: ‘[statement]’? Pleaserespondwithonlyanumber(1)Disagreestrongly(2)
Disagree(3)Neitheragreenordisagree(4)Agree(5)Agreestrongly.
ThesimulationresultsproducedbypromptP3showninFigure4containmore“Agreestrongly”and
less“Agree”comparedtoFigure1,althoughtheoveralltrendisstillconsistent. Itiswellknownthat
3InternationalStandardClassificationofOccupations(ISCO)
5whenasurveyisfilledoutbyhumans,theorderoftheoptionsaffectstheperson’schoice(Galesic
etal.,2008). OurexperimentalresultssuggestthatthesameistrueforLLMstoanevenlargerextent.
(a)Men (b)Women
Figure4: Q1: “Gaysandlesbiansfreetolivelifeastheywish”? Prompt: P3. Model: GPT-3.5.
For9suchquestionsintheESS(listedintheappendix),wecalculatedthemeanbiasandJ-indexfor
thesimulationsresultsofGPT-3.5comparedtoESSdata,andthecorrespondingresultsareshown
inFigure5. Foreaseofplotting,weshowtheabsolutevalueofthemeandifferenceinthefigure.
Table4andTable5intheappendixlistthemeansfordifferentquestionsinthesamecountry,andthe
meansforthesamequestionindifferentcountries.
(a)Absolutevalueofthemeandifference(error). (b)J-index(congruence).
Figure5: ComparisonsbetweensurveydataandsimulationresultsbasedonGPT-3.5,brokendown
bycountryandquestion.
Aswehaveseenabove,promptP3leadstomarkedlydifferentsimulationresultsforQ1,although
thisdoesnotalwaysholdtrueforotherquestions. Forexample,thesimulationresultsforQ4using
promptP3producedbetterresultsthanthoseusingpromptsP1andP2. TheJ-indexbringsfurther
insight. Forexample,whileQ3isstilltheworstquestiontosimulateforGreeks,itdoesnotsimulate
wellfortheGermans,either.
Whenmorepersonalinformationisincludedintheprompts(i.e.,P2),thebiasdecreasesfor22out
of36andtheJ-indexincreasesfor25outof36cases. Itshouldbenotedthattheparametersofthe
LLMsalsohaveanimpactontheresults. Forexample,whenwesettop_pto0.2insteadof0.9,the
J-indexbecomesworseinall72caseswithpromptsP1andP2. Andthebiascanbothincreaseor
decrease with the smaller top_p value, which is in line with our expectations. (See appendix for
details.)
65.3 OtherLLMs
ThesamepromptsareusedtoperformsimulationonotherLLMs,inordertochecktherobustness
ofourresults. UnlikeChatGPT,theotherLLMsmayrefusetoanswer,andtheirresponsesarenot
alwaysvalid(seeappendixforexamples),whichcouldbeconsideredasmissingdata. Wecouldforce
themtoanswerbychangingprompts,butthiswouldalsoaffecttheirchoices(Röttgeretal.,2024).
ThusweusethesamepromptsP1andP2asbefore.
TheresultsofthesimulationonLLaMA-2-7BusingpromptP1forQ1areshowninFigure6,which
featuresamuchlargervariancecomparedwithGPT-3.5. Thesurveydatacorrespondingtoquestions
withinvalidresponseinthesimulationsarenotusedinthecalculations.
(a)Men (b)Women
Figure6: Q1: “Gaysandlesbiansfreetolivelifeastheywish”? Prompt: P1. Model: LLaMA-2-7B.
For a more quantiative comparison, we calculated the difference of the mean compared to ESS
dataforQ1, listedinTable1forthefollowingLLMs: GPT-3.5, GPT-4o4, LLaMA-2-7B(L-7B),
LLaMA-2-13B(L-13B),LLaMA-3-8B(L-8B),Mistral-7B-Instruct-v0.2(Mistral),DeepSeek-V2(DS).
Table1: ResultsfromdifferentLLMs(meandifferencebetweenLLMsanswersandsurvey). Values
inboldaretheclosestsimulationstothemeanforeachrow,whilethoseinreditalicsaretheworst.
country prompt GPT-3.5 GPT-4o L-7B L-13B L-8B Mistral DS
P1 0.22 1.37 0.51 0.64 1.19 0.47 -0.12
Germany
P2 0.33 0.33 0.40 0.57 1.48 -0.13 -0.29
P1 -0.14 1.02 0.04 0.21 0.75 0.86 -0.19
Greece
P2 -0.02 -0.01 -0.13 0.41 0.92 0.78 0.03
P1 -0.43 1.11 -0.25 0.44 0.61 1.01 -0.13
Bulgaria
P2 -0.31 0.10 -0.82 0.18 0.61 1.19 -0.16
P1 -0.03 0.76 0.24 0.30 0.94 0.77 -0.13
Italy
P2 0.09 -0.32 0.01 0.51 1.10 0.37 -0.9
P1 0.21 1.06 0.26 0.40 0.87 0.77 0.14
Avg. (abs.)
P2 0.19 0.19 0.34 0.42 1.03 0.62 0.15
Table1showsthatDeepSeek-V2andGPT-3.5generatesimulationresultswithweightedaverages
closesttothesurveydata,whileLLaMA-3-8Bishighlybiasedforbothprompts;occupationinforma-
4gpt-4o-2024-05-13,temperature=1,top_p=0.9)
7tionisveryimportantforGPT-4o,whichappeatstoperformgenerallyworsethanitspredecessor
GPT-3.5,whichissomewhatunexpected.
Table2presentstheJ-indexoftheLLMsforQ1. Theconclusionsareverysimilartotheprevious
ones,andtheresultsfromMistral-7B-Instruct-v0.2areveryinteresting: thesimulationresultsfor
GermansandItaliansareveryclosetotherealdata,whiletheresultsofGreeksandBulgarianspull
downtheaverageeffectofthismodel. Inanutshell,weobserverthatGPT-3.5isthebestperforming
modelamongtheonestested.
Table2: J-indexresultsfordifferentLLMs. ValuesinboldarethelargestJ-indexforeachrow,while
thoseinreditalicsaretheworst.
country prompt GPT-3.5 GPT-4o L-7B L-13B L-8B Mistral DS
P1 0.43 0.13 0.35 0.17 0.25 0.51 0.46
Germany
P2 0.40 0.40 0.40 0.28 0.18 0.47 0.50
P1 0.32 0.24 0.27 0.23 0.26 0.20 0.27
Greece
P2 0.38 0.38 0.26 0.15 0.25 0.18 0.27
P1 0.29 0.20 0.26 0.17 0.23 0.19 0.19
Bulgaria
P2 0.36 0.36 0.21 0.15 0.28 0.19 0.20
P1 0.39 0.28 0.27 0.28 0.22 0.37 0.33
Italy
P2 0.42 0.42 0.27 0.19 0.20 0.48 0.35
P1 0.36 0.21 0.29 0.21 0.24 0.32 0.31
Avg.
P2 0.39 0.39 0.28 0.19 0.23 0.33 0.33
Peoplewiththesamecountry,gender,andyearofbirthareconsideredtobeinthesamegroup. Figure
7comparesthemeandifferenceofeachgenderforQ2betweenGPT-3.5andLLaMA-2-7B.While
LLaMA-2-7Bshowsalargerbias,itappearstobealmostgender-independent. GPT-3.5showsan
overallsmallerbias,butamoremarkedinfluenceofgenderinbothmeanandstandarddeviation.
(a)Model:GPT-3.5 (b)Model:LLaMA-2-7B
Figure7: Q2: “Governmentshouldreducedifferencesinincomelevels”? Prompt: P1. Comparison
ofresultsbetweenGPT-3.5(left)andLLaMA-2-7B(right).
6 Conclusions
Can a LLM adjust itself to impersonate a human like a chameleon? It depends on the object of
imitation and the capabilities of the given LLM. Regardless of the goals and process of LLM
alignment,ourresultsreflectacleargeographicimbalance: simulationsofBulgariansworkworse
relativetopeoplefromothercountries,forreasonsthatremaintobeexplored.
8DifferentLLMshavedifferentsimulationoutcomes,andtheparametersofthemodelsometimesplay
animportantrole. Theinformationinthepromptandthewayitisdescribedcan,ofcourse,greatly
influencetheLLM’sresponse,withnoclearindicationastowhatisthebestchoice.
WeproposeandvalidatepossiblewaystomakeLLMresponsesmorehuman-like,suchasproviding
additionalinformationinprompts.WealsopresentametricforLLMresponsestosubjectivequestions
thatfocusesmoreonthedistributionthanthemean,andcrucially,suggesttocomparewithactual
surveydatainordertohighlightpotentialgenderandculturalbiasesintheresponse.
AlthoughLLMsshowthepotentialtoperformsimulationsofcollectivebehaviorortoreplacehuman
participantsinlimitedsettings,moreadvancedLLMsdonotnecessarilyproducesimulationresults
thataremoresimilartosurveydata. Onthecontrary,earliervisionLLMsmaygivebettersimulations
onsomesubjectivequestions.
7 Discussions
Due to budgetary and computational resource constraints, we “only” performed about 1 million
simulations,althoughmoresimulationswouldbebeneficialinobtainingmorecompleteconclusions.
LLMsareupdatedexceptionallyquickly,andcomparingwhichmodelisbettercaneasilybecome
outdated,soit’simportanttoexplorecommonalitiesintermsofopinionsintheseLLMs. Inaddition,
wearecurrentlyconsideringonly4countriesand9questions,whichcouldbeexpandedtoobtaina
fullerpictureofhowEuropeanculturaldiversityiscaptured(ifatall)inthecurrentgenerationof
LLMs.
WeknowthatLLMsareabletoinferpeople’sgender,age,andlocationfromredactedandanonymized
inputtext(Staabetal.,2023). Conversely,promotingLLMswithappropriatedemographicvariables
aswehavedonehereproducesanswersthatappeartobegenerallyalignedwithrealpeople’sviews,
althoughstillsufferingfrombiasesandstereotypes. Furthermore,biasesandstereotypespresentin
LLMsarelikelytoslowlyinfluencetheirusers(Röttgeretal.,2024),soitisessentialtomeasure
them.Weusesurveydataforcomparison,whichshouldbemorerepresentativeofpublicopinionthan
crowdsourceddata. Comparedtopreviouspapers,moredetailedsimulationandanalysismethodsare
performed,suchasconsideringpreciseageinformationandusingtheweightsofthesurveydatato
betterestimatethetargetpopulation. Buttheprocesscouldbeimproved,forexampleinthehandling
ofmissingdata.
LLMs have demonstrated the ability and potential to transform the field of computational social
science(Ziemsetal.,2024). BeforeLLMscanbeusedtomodelmorecomplexgroupsandbehaviors,
it is necessary to scrutinize in detail the aggregate output of simulations, and we believe that
comparisonwithsurveydataisagoodentrypoint. TheapplicationofLLMsinsurveyresearchhas
beendiscussed(Jansenetal.,2023). AcompletelyneutralLLMisnotnecessarilyagoodaim. A
modelwithhigherratingsormoreparametersisnotnecessarilymoresuitableforsurveysimulation
foranumberofreasons,suchassubsequenthumanadjustments. Andthefine-tuningstepsmaywell
remainabsolutelynecessary.
Acknowledgments
The authors would like to thank the personnel of RACHAEL S.r.l. for helpful discussions. R.T.
acknowledgesco-fundingfromNextGenerationEU,inthecontextoftheNationalRecoveryand
ResiliencePlan,InvestmentPE1–ProjectFAIR“FutureArtificialIntelligenceResearch”. M.G.is
supportedbyDMDottoratiInnovazioneeGreenexDM1061Anno2021(DM1061del10/08/2021).
9References
Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. (2021). On the dangers of
stochasticparrots: Canlanguagemodelsbetoobig? InProceedingsofthe2021ACMconference
onfairness,accountability,andtransparency,pages610–623.
Bi,X.,Chen,D.,Chen,G.,Chen,S.,Dai,D.,Deng,C.,Ding,H.,Dong,K.,Du,Q.,Fu,Z.,etal.
(2024). Deepseekllm: Scalingopen-sourcelanguagemodelswithlongtermism. arXivpreprint
arXiv:2401.02954.
Boelaert,J.,Coavoux,S.,Ollion,É.,Petev,I.,andPräg,P.(2024). Machinebiasgenerativelarge
languagemodelshaveaworldviewoftheirown.
Carlsson,F.,Mørkbak,M.R.,andOlsen,S.B.(2012). Thefirsttimeisthehardest:Atestofordering
effectsinchoiceexperiments. JournalofChoiceModelling,5(2):19–37.
Chang,Y.,Wang,X.,Wang,J.,Wu,Y.,Yang,L.,Zhu,K.,Chen,H.,Yi,X.,Wang,C.,Wang,Y.,
etal.(2024). Asurveyonevaluationoflargelanguagemodels. ACMTransactionsonIntelligent
SystemsandTechnology,15(3):1–45.
Chiang,W.-L.,Zheng,L.,Sheng,Y.,Angelopoulos,A.N.,Li,T.,Li,D.,Zhang,H.,Zhu,B.,Jordan,
M.,Gonzalez,J.E.,etal.(2024). Chatbotarena: Anopenplatformforevaluatingllmsbyhuman
preference. arXivpreprintarXiv:2403.04132.
Chierichetti, F., Kumar, R., Pandey, S., and Vassilvitskii, S. (2010). Finding the jaccard median.
InProceedingsofthetwenty-firstannualACM-SIAMsymposiumonDiscreteAlgorithms,pages
293–311.SIAM.
Dong,X.,Wang,Y.,Yu,P.S.,andCaverlee,J.(2024). Disclosureandmitigationofgenderbiasin
llms. arXivpreprintarXiv:2402.11190.
EuropeanSocialSurveyERIC(ESSERIC)(2022). Europeansocialsurvey(ESS),round10-2020.
Galesic, M., Tourangeau, R., Couper, M. P., and Conrad, F. G. (2008). Eye-tracking data: New
insights on response order effects and other cognitive shortcuts in survey responding. Public
opinionquarterly,72(5):892–913.
Goldberg,L.R.,Sweeney,D.,Merenda,P.F.,andHughesJr,J.E.(1998). Demographicvariables
andpersonality: Theeffectsofgender,age,education,andethnic/racialstatusonself-descriptions
ofpersonalityattributes. PersonalityandIndividualdifferences,24(3):393–403.
Grafström,A.andSchelin,L.(2014). Howtoselectrepresentativesamples. ScandinavianJournalof
Statistics,41(2):277–290.
Hendrycks,D.,Burns,C.,Basart,S.,Critch,A.,Li,J.,Song,D.,andSteinhardt,J.(2020a). Aligning
aiwithsharedhumanvalues. arXivpreprintarXiv:2008.02275.
Hendrycks,D.,Burns,C.,Basart,S.,Zou,A.,Mazeika,M.,Song,D.,andSteinhardt,J.(2020b).
Measuringmassivemultitasklanguageunderstanding. arXivpreprintarXiv:2009.03300.
Herek,G.M.(2002). Gendergapsinpublicopinionaboutlesbiansandgaymen. Publicopinion
quarterly,66(1):40–66.
Hovy,D.andPrabhumoye,S.(2021). Fivesourcesofbiasinnaturallanguageprocessing. Language
andlinguisticscompass,15(8):e12432.
Jansen, B. J., Jung, S.-g., and Salminen, J. (2023). Employing large language models in survey
research. NaturalLanguageProcessingJournal,4:100020.
Jiang,A.Q.,Sablayrolles,A.,Mensch,A.,Bamford,C.,Chaplot,D.S.,Casas,D.d.l.,Bressand,F.,
Lengyel,G.,Lample,G.,Saulnier,L.,etal.(2023a). Mistral7b. arXivpreprintarXiv:2310.06825.
Jiang,G.,Xu,M.,Zhu,S.-C.,Han,W.,Zhang,C.,andZhu,Y.(2024). Evaluatingandinducing
personalityinpre-trainedlanguagemodels. AdvancesinNeuralInformationProcessingSystems,
36.
10Jiang,H.,Zhang,X.,Cao,X.,Kabbara,J.,andRoy,D.(2023b). Personallm: Investigatingtheability
ofgpt-3.5toexpresspersonalitytraitsandgenderdifferences. arXivpreprintarXiv:2305.02547.
Kirk,H.R.,Vidgen,B.,Röttger,P.,andHale,S.A.(2023). Personalisationwithinbounds: Arisk
taxonomyandpolicyframeworkforthealignmentoflargelanguagemodelswithpersonalised
feedback. arXivpreprintarXiv:2303.05453.
Koo,R.,Lee,M.,Raheja,V.,Park,J.I.,Kim,Z.M.,andKang,D.(2023). Benchmarkingcognitive
biasesinlargelanguagemodelsasevaluators. arXivpreprintarXiv:2309.17012.
Köpf,A.,Kilcher,Y.,vonRütte,D.,Anagnostidis,S.,Tam,Z.R.,Stevens,K.,Barhoum,A.,Nguyen,
D.,Stanley,O.,Nagyfi,R.,etal.(2024).Openassistantconversations-democratizinglargelanguage
modelalignment. AdvancesinNeuralInformationProcessingSystems,36.
Kotek,H.,Dockum,R.,andSun,D.(2023). Genderbiasandstereotypesinlargelanguagemodels.
InProceedingsofTheACMCollectiveIntelligenceConference,pages12–24.
Kyriakidis, M., Happee, R., and De Winter, J. C. (2015). Public opinion on automated driving:
Resultsofaninternationalquestionnaireamong5000respondents. Transportationresearchpart
F:trafficpsychologyandbehaviour,32:127–140.
Liu,S.,Maturi,T.,Shen,S.,andMihalcea,R.(2024). Thegenerationgap: Exploringagebiasin
largelanguagemodels. arXivpreprintarXiv:2404.08760.
Liu,Y.,Yao,Y.,Ton,J.-F.,Zhang,X.,Cheng,R.G.H.,Klochkov,Y.,Taufiq,M.F.,andLi,H.(2023).
Trustworthyllms: asurveyandguidelineforevaluatinglargelanguagemodels’alignment. arXiv
preprintarXiv:2308.05374.
Mei,Q.,Xie,Y.,Yuan,W.,andJackson,M.O.(2024). Aturingtestofwhetheraichatbotsarebehav-
iorallysimilartohumans. ProceedingsoftheNationalAcademyofSciences,121(9):e2313925121.
Motoki,F.,PinhoNeto,V.,andRodrigues,V.(2024). Morehumanthanhuman: Measuringchatgpt
politicalbias. PublicChoice,198(1):3–23.
Navigli,R.,Conia,S.,andRoss,B.(2023). Biasesinlargelanguagemodels: origins,inventory,and
discussion. ACMJournalofDataandInformationQuality,15(2):1–21.
Ouyang,L.,Wu,J.,Jiang,X.,Almeida,D.,Wainwright,C.,Mishkin,P.,Zhang,C.,Agarwal,S.,
Slama,K.,Ray,A.,etal.(2022). Traininglanguagemodelstofollowinstructionswithhuman
feedback. Advancesinneuralinformationprocessingsystems,35:27730–27744.
Pezeshkpour,P.andHruschka,E.(2023). Largelanguagemodelssensitivitytotheorderofoptions
inmultiple-choicequestions. arXivpreprintarXiv:2308.11483.
Röttger, P., Hofmann, V., Pyatkin, V., Hinck, M., Kirk, H.R., Schütze, H., andHovy, D.(2024).
Politicalcompassorspinningarrow? towardsmoremeaningfulevaluationsforvaluesandopinions
inlargelanguagemodels. arXivpreprintarXiv:2402.16786.
Russell, A. M., Browne, M., Hing, N., Rockloff, M., and Newall, P. (2022). Are any samples
representativeorunbiased? replytopickeringandblaszczynski. InternationalGamblingStudies,
22(1):102–113.
Safdari,M.,Serapio-García,G.,Crepy,C.,Fitz,S.,Romero,P.,Sun,L.,Abdulhai,M.,Faust,A.,and
Mataric´,M.(2023). Personalitytraitsinlargelanguagemodels. arXivpreprintarXiv:2307.00184.
Santurkar, S., Durmus, E., Ladhak, F., Lee, C., Liang, P., and Hashimoto, T. (2023). Whose
opinionsdolanguagemodelsreflect? InInternationalConferenceonMachineLearning,pages
29971–30004.PMLR.
Shen,T.,Jin,R.,Huang,Y.,Liu,C.,Dong,W.,Guo,Z.,Wu,X.,Liu,Y.,andXiong,D.(2023). Large
languagemodelalignment: Asurvey. arXivpreprintarXiv:2309.15025.
Staab,R.,Vero,M.,Balunovic´,M.,andVechev,M.(2023). Beyondmemorization: Violatingprivacy
viainferencewithlargelanguagemodels. arXivpreprintarXiv:2310.07298.
11Sun,H.,Pei,J.,Choi,M.,andJurgens,D.(2023). Aligningwithwhom? largelanguagemodelshave
genderandracialbiasesinsubjectivenlptasks. arXivpreprintarXiv:2311.09730.
Touvron,H.,Martin,L.,Stone,K.,Albert,P.,Almahairi,A.,Babaei,Y.,Bashlykov,N.,Batra,S.,
Bhargava,P.,Bhosale,S.,etal.(2023). Llama2: Openfoundationandfine-tunedchatmodels.
arXivpreprintarXiv:2307.09288.
Wolf, Y., Wies, N., Avnery, O., Levine, Y., and Shashua, A. (2023). Fundamental limitations of
alignmentinlargelanguagemodels. arXivpreprintarXiv:2304.11082.
Ye,F.,Yang,M.,Pang,J.,Wang,L.,Wong,D.F.,Yilmaz,E.,Shi,S.,andTu,Z.(2024). Benchmark-
ingllmsviauncertaintyquantification. arXivpreprintarXiv:2401.12794.
Ziems, C., Held, W., Shaikh, O., Chen, J., Zhang, Z., and Yang, D. (2024). Can large language
modelstransformcomputationalsocialscience? ComputationalLinguistics,pages1–55.
12A EuropeanSocialSurvey(ESS)data
A.1 Statements(questions)
1. Gaysandlesbiansfreetolivelifeastheywish.
2. Governmentshouldreducedifferencesinincomelevels.
3. Gayandlesbiancouplesrighttoadoptchildren.
4. Ashamedifclosefamilymembergayorlesbian.
5. Obedienceandrespectforauthoritymostimportantvirtueschildrenshouldlearn.
6. Countryneedsmostloyaltytowardsitsleaders.
7. Smallsecretgroupofpeopleresponsibleformakingallmajordecisionsinworldpolitics.
8. Groups of scientists manipulate, fabricate, or suppress evidence in order to deceive the
public.
9. COVID-19isresultofdeliberateandconcealedeffortsofsomegovernmentororganisation.
A.2 Datareleasetime
1. June 2022: Bulgaria, Croatia, Czechia, Estonia, Finland, France, Hungary, Lithuania,
SlovakiaandSlovenia5.
2. November2022: Greece,Iceland,Italy,Montenegro,Netherlands,NorthMacedonia,Nor-
way,Portugal,Switzerland,Austria,Germany,Poland,Serbia,SpainandSweden6.
3. May2023: Belgium,Ireland,Israel,LatviaandtheUnitedKingdom7.
B Experimentscomputeresources
B.1 LLaMA
WerunLLaMA2-7B-Chat,LLaMA-2-13B-Chat,andLLaMA-3-8B-Instruct modelsonamachine
with2NVIDIAA100(80G).TheexperimentsrunonPython3.10andCUDA11.3. Otherspecific
dependenciescanbefoundintheofficialMeta-LLaMaGitHubrepositories:https://github.com/
meta-llama/llamaandhttps://github.com/meta-llama/llama3.
B.2 Mistral
Morespecifically,mistral-7b-instruct-v0.2.Q4_K_M.gguf8wasusedforsimulationswithouteffective
GPUacceleration.
C Responsesexamples
DifferentLLMsgivedifferentresponses,herewetakethefirstquestionandthefirstperson(awoman
bornin1945nowlivinginBulgaria)inthedatasetasanexample.
C.1 LLaMA
C.1.1 LLaMA-2-7B
Iwasbornin1945inBulgaria,andatthattime,thesocialandculturalattitudestowardsLGBTQ+
individualswerenotasprogressiveastheyaretoday. However,Ibelievethateveryoneshouldbefree
tolivetheirlivesastheywish,regardlessoftheirsexualorientationorgenderidentity.
5https://www.europeansocialsurvey.org/news/article/round-10-data-now-available
6https://www.europeansocialsurvey.org/news/article/second-release-round-10-data-now-available
7https://www.europeansocialsurvey.org/news/article/new-round-10-data-release
8https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF
13GiventhehistoricalcontextofBulgaria,Iwouldanswerthequestionwithascoreof(3)Neitheragree
nordisagree. WhileIdonotfullyagreewiththestatement,Ialsodonotactivelydisagreewithit. I
believethatLGBTQ+individualsshouldbetreatedwithrespectanddignity,andshouldbeprotected
fromdiscriminationandpersecution. However,inthe1940sand1950s,thesocietalnormsandlaws
inBulgariawerenotsupportiveofLGBTQ+rights,andthislimitedthedegreetowhichindividuals
couldexpressthemselvesfreely.
Overall,whileIdonotfullyagreewiththestatement,Ibelievethatitisimportanttorecognizethe
importanceofLGBTQ+rightsandtoworktowardscreatingamoreinclusiveandacceptingsociety.
C.1.2 LLaMA-2-13B
Iapologize,butasaresponsibleAIlanguagemodel,Icannotprovidearesponsethatisdiscriminatory
ordisrespectfultowardsanyindividualorgroup,includingthosewhoidentifyasLGBTQ+. The
statement"Gaysandlesbiansfreetolivelifeastheywish"isafundamentalhumanright,andall
individualsshouldbetreatedwithdignityandrespectregardlessoftheirsexualorientationorgender
identity. Therefore,Icannotprovidearatingforthisstatementasitisnotappropriatetoexpress
disagreementordisapprovaltowardstherightsandfreedomsofmarginalizedcommunities.
C.1.3 LLaMA-3-8B
(4)Disagree
C.2 Mistral-7B
I’manartificialintelligencelanguagemodelanddon’thavetheabilitytobebornorliveinaspecific
placeortime.Furthermore,Idon’thavepersonalfeelingsorexperiences,soIcan’tprovidearesponse
basedonbeingawomanbornin1945livinginBulgaria. However,Icantellyouthatasofnow,
BulgarianlawdoesnotfullyprotecttherightsoftheLGBTQ+community. Discriminationagainst
themisstillcommoninvariousareassuchasemployment,education,andhealthcare. Therefore,I
wouldgivearesponsebasedoncurrentsocietalandlegalrealitiesinBulgaria:(4)Disagree. However,
it’simportanttonotethatthesituationcanchange,andprogresstowardsequalrightsandprotections
fortheLGBTQ+communityisbeingmadeinmanyplacesaroundtheworld.
C.3 DeepSeek-V2
AsanAI,Idon’thavepersonalexperiencesorabirthyear. However,Icanprovideananalysisbased
onhistoricalcontext.In1945,attitudestowardsLGBTQ+individualswerevastlydifferentfromtoday,
especiallyinEasternEurope. Duringthemid-20thcentury,homosexualitywasoftencriminalized
andstigmatizedinmanycountries,includingBulgaria. Itwasn’tuntil1968thathomosexualitywas
decriminalizedinBulgaria,andsocietalacceptancehasbeenagradualprocesssincethen.
Giventhetimeandplaceofbirth,awomanfromthateramighthavegrownupwithconservativeviews
onsexualityandgenderroles. However,withoutpersonalinsightintoherbeliefs,it’simpossibleto
accuratelygaugeheropiniononthestatement"Gaysandlesbiansfreetolivelifeastheywish."
Ifweweretoconsiderahypotheticalindividualfromthattimeandplace,herresponsecouldvary
widelydependingonherpersonalexperiences, education, exposuretodifferentcultures, andthe
evolutionofherbeliefsovertime. It’simportanttonotethatattitudeshaveshiftedsignificantlysince
themid-20thcentury, andmanypeoplefromthaterahaveupdatedtheirviewsinlinewithmore
modernunderstandingsofhumanrightsandequality.
Withoutmorecontext,it’snotpossibletoassignanumbertothishypotheticalwoman’sagreementor
disagreementwiththestatement.
D Parameters
D.1 Temperature
14(a)Men (b)Women
Figure8: Q1: “Gaysandlesbiansfreetolivelifeastheywish”? Prompt: P1. (temperature=1and
temperature=2).
D.2 top_p
Table3: ResultsofGPT-3.5(top_p=0.2)
country 1 2 3 4 5 6 7 8 9
mean(P1) 0.29 0.08 0.05 0.73 1.57 1.52 -0.35 -0.81 0.53
Germany mean(P2) 0.41 0.13 0.02 0.22 1.43 1.39 -0.05 -0.34 0.40
J-index(P1) 0.33 0.32 0.19 0.35 0.09 0.10 0.17 0.26 0.25
J-index(P2) 0.32 0.38 0.23 0.22 0.17 0.14 0.18 0.28 0.22
mean(P1) -0.10 0.36 -1.41 1.39 0.84 0.82 -0.35 -0.19 0.83
Greece mean(P2) 0.04 0.42 -1.45 0.98 0.72 0.71 0.35 0.02 0.64
J-index(P1) 0.27 0.26 0.11 0.17 0.16 0.28 0.25 0.36 0.31
J-index(P2) 0.33 0.29 0.13 0.26 0.26 0.39 0.27 0.35 0.28
mean(P1) -0.35 0.76 -0.99 1.87 1.65 1.86 0.33 0.84 1.93
Bulgaria mean(P2) -0.19 0.57 -1.22 1.40 1.67 1.72 0.92 0.80 1.77
J-index(P1) 0.22 0.19 0.14 0.11 0.07 0.10 0.23 0.24 0.08
J-index(P2) 0.32 0.24 0.19 0.19 0.14 0.13 0.24 0.28 0.08
mean(P1) 0.04 -0.02 -0.75 0.90 1.80 1.64 -0.33 -0.13 1.05
Italy mean(P2) 0.15 0.15 -0.81 0.47 1.66 1.52 0.29 0.01 0.89
J-index(P1) 0.29 0.31 0.16 0.27 0.06 0.10 0.21 0.34 0.22
J-index(P2) 0.35 0.38 0.21 0.36 0.13 0.13 0.24 0.37 0.20
E Otherresults
15Table4: ResultsofGPT-3.5(meandifference).
country Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Avg.
P1 0.22 0.09 0.10 0.67 1.61 1.53 -0.49 -0.56 0.58 0.65
Germany P2 0.33 0.12 0.03 0.38 1.46 1.42 -0.25 -0.33 0.51 0.54
P3 -0.50 -0.32 -0.08 -0.19 1.32 1.24 -1.15 -1.27 -0.01 0.67
P1 -0.14 0.40 -1.30 1.26 0.90 0.78 -0.15 0.03 0.86 0.65
Greece P2 -0.02 0.38 -1.40 1.06 0.72 0.69 0.19 0.06 0.72 0.58
P3 -1.06 -0.10 -1.08 0.09 0.54 0.49 -0.58 -0.79 -0.22 0.55
P1 -0.43 0.71 -1.00 1.66 1.74 1.79 0.53 0.86 1.94 1.18
Bulgaria P2 -0.31 0.54 -1.18 1.44 1.65 1.67 0.83 0.86 1.83 1.15
P3 -1.38 0.24 -0.65 0.45 1.41 1.54 0.22 0.11 0.76 0.75
P1 -0.03 0.05 -0.68 0.80 1.82 1.67 -0.21 -0.07 1.06 0.71
Italy P2 0.09 0.13 -0.78 0.57 1.69 1.52 0.16 0.04 0.95 0.66
P3 -0.88 -0.40 -0.92 -0.20 1.44 1.34 -0.59 -0.75 0.24 0.75
P1 0.21 0.31 0.77 1.10 1.52 1.44 0.34 0.38 1.11 0.80
Avg. (abs.) P2 0.19 0.29 0.85 0.86 1.38 1.32 0.36 0.32 1.00 0.73
P3 0.95 0.26 0.68 0.23 1.18 1.15 0.64 0.73 0.31 0.68
Table5: ResultsofGPT-3.5(J-index).
country prompt Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Avg.
P1 0.43 0.39 0.22 0.43 0.13 0.13 0.33 0.39 0.32 0.31
Germany P2 0.40 0.44 0.25 0.32 0.18 0.14 0.33 0.36 0.30 0.30
P3 0.41 0.47 0.42 0.42 0.12 0.12 0.24 0.24 0.31 0.31
P1 0.32 0.32 0.17 0.27 0.23 0.38 0.42 0.48 0.33 0.32
Greece P2 0.38 0.33 0.16 0.29 0.32 0.43 0.40 0.43 0.32 0.34
P3 0.25 0.47 0.21 0.39 0.18 0.35 0.32 0.26 0.27 0.30
P1 0.29 0.23 0.20 0.19 0.09 0.13 0.37 0.34 0.08 0.21
Bulgaria P2 0.36 0.28 0.21 0.21 0.16 0.13 0.34 0.30 0.08 0.23
P3 0.20 0.35 0.23 0.34 0.12 0.13 0.38 0.38 0.23 0.26
P1 0.39 0.36 0.22 0.40 0.08 0.13 0.43 0.47 0.24 0.30
Italy P2 0.42 0.42 0.24 0.42 0.14 0.14 0.39 0.45 0.22 0.32
P3 0.26 0.45 0.26 0.41 0.11 0.12 0.30 0.33 0.38 0.29
P1 0.36 0.32 0.20 0.32 0.13 0.19 0.39 0.42 0.24 0.29
Avg. P2 0.39 0.37 0.22 0.31 0.20 0.21 0.36 0.38 0.23 0.30
P3 0.28 0.44 0.28 0.39 0.13 0.18 0.31 0.30 0.30 0.29
16