Multi-Modal Generative Embedding Model
FeipengMa1∗,HongweiXue1,3†,GuangtingWang2,YizhouZhou2‡,FengyunRao2
ShilinYan4,YueyiZhang1,SiyingWu5,MikeZhengShou3,XiaoyanSun1,5‡
1UniversityofScienceandTechnologyofChina 2WeChat,TencentInc.
3ShowLab,NationalUniversityofSingapore 4FudanUniversity
5InstituteofArtificialIntelligence,HefeiComprehensiveNationalScienceCenter
{mafp,xuehongwei}@mail.ustc.edu.cn
harryizzhou@tencent.com,sunxiaoyan@ustc.edu.cn
Abstract
Most multi-modal tasks can be formulated into problems of either generation
or embedding. Existing models usually tackle these two types of problems by
decouplinglanguagemodulesintoatextdecoderforgeneration,andatextencoder
forembedding. Toexploretheminimalismofmulti-modalparadigms,weattempt
toachieveonlyonemodelpermodalityinthiswork. WeproposeaMulti-Modal
GenerativeEmbeddingModel(MM-GEM),wherebythegenerativeandembedding
objectives are encapsulated in one Large Language Model. We also propose a
PoolAggregatortoboostefficiencyandenabletheabilityoffine-grainedembedding
andgeneration. Asurprisingfindingisthatthesetwoobjectivesdonotsignificantly
conflict with each other. For example, MM-GEM instantiated from ViT-Large
andTinyLlamashowscompetitiveperformanceonbenchmarksformultimodal
embeddingmodelssuchascross-modalretrievalandzero-shotclassification,while
has good ability of image captioning. Additionally, MM-GEM can seamlessly
execute region-level image caption generation and retrieval tasks. Besides, the
advancedtextmodelinMM-GEMbringsover5%improvementinRecall@1for
longtextandimageretrieval.
1 Introduction
In recent years, the multi-modal learning field has witnessed a unifying trend [3, 17, 30, 33, 54].
Thistrendisdrivenbytheadvancedunderstandingabilityandmoreefficientcomputationbrought
bysharedrepresentations. Moreover,thesimplificationofthemodelstructuremakesitmuchmore
directtoperformvariousdownstreamtasks[30,40].
Mostcutting-edgemulti-modalmodelscanbecategorizedintotwoparadigms: embeddingmodels
andgenerativemodels. Embeddingmodels[20,40,50]typicallyutilizeadualencoderstructure.
Thisframeworkprojectsdistinctmodalitiesintoaunifiedlatentspace,therebyfacilitatingefficient
cross-modal retrieval and classification tasks. Generative models [2, 32, 55] forge a connection
betweenvisualrepresentationsandLargeLanguageModels(LLMs). Thisintegrationenablesthe
modelstoharnesscapabilitiessuchasinstructionfollowing[32,55]orin-contextlearning[2]. These
twoparadigmsintersectinthevisualmodality, i.e., thevisionmoduleofthegenerativemodelis
usuallyderivedfromapowerfulembeddingmodel[40]. However,thetextualmodalityrevealsa
divergenceinapproach. Whilegenerativemodelscommonlyemployanauto-regressivetextdecoder,
embeddingmodelsfavoratextencodertoextractaglobalrepresentationofthetext.
∗ThisworkwasperformedwhileFeipengMaandHongweiXuewereinternsatWeChat,TencentInc.
†ProjectLeader.
‡Correspondingauthors.
Preprint.Underreview.
4202
yaM
92
]VC.sc[
1v33391.5042:viXraThedivergenceintextualmodalityremainsasapivotalobstacletoachievingthegoalofunification,
namely,usingonlyonemodelpermodality. severalworksstepindifferentdirectionstowardsthis
goal. BLIP[30]sharesallparametersinthetextencoderanddecoderexceptfortheself-attention
layers. CoCa[52]splitsthetextdecoderintounimodalandmultimodalcomponents,byremovingthe
cross-attentionmoduleintheunimodaldecoderlayers. Thesemethodsdifferentiatetheforwardpath
ofunimodalandmultimodal,introducingahindrancetothedirectuseofpre-trainedtextmodels.
FROMAGe[24]trulyachievesunificationbygroundingtheimagefeaturetotheinputsandoutputs
ofafrozenlargelanguagemodel. However,thelackofjointtrainingwiththevisualmodalityresults
inaperformancedeficiency.
Toexploretheminimalismofmulti-modalparadigms,weproposeMulti-ModalGenerativeEmbed-
dingModel(MM-GEM)inthispaper. MM-GEMisanend-to-endoptimizedmodelthatcombines
twoparadigmsbyencapsulatingthegenerativeandembeddingobjectivesinthesamelanguagemodel.
Specifically,forembedding,wealigntheimagefeatureswiththesentenceembeddingsderivedfrom
the last token. Concurrently, for the generative task, we concatenate the image features with the
wordembeddingsofthelanguagemodeltoexecutethecaptioningprocess. Notably,bothobjectives
leverageasharedforwardpathwithinthelanguagemodel. Toboostefficiencyandenabletheability
offine-grainedembeddingandgeneration,weproposeaPoolAggregatortorepresentanimageby
thefeaturemap,insteadofaglobalfeatureon[CLS]token[12,40].
ExperimentalresultsdemonstratethesuperiorityofMM-GEM.MM-GEMinstantiatedfromViT-
LargeandTinyLlama[53]achievescomparableresultswithOpenCLIP[19]onimage-textretrieval
benchmarkssuchasCOCO[31]andFlickr30K[39],andzero-shotimageclassificationbenchmark
ICinW[29].Meanwhile,MM-GEMshowscompetitiveperformanceonimagecaptioningbenchmarks
such as COCO [31] and NoCaps [1]. Additionally, qualitative results show that MM-GEM can
generateregion-levelimagecaptionsandfine-grainedtext-to-imageretrievalwithoutfurthertraining
or modification. Besides, the advanced text module in MM-GEM brings better ability of text
understanding. MM-GEM achieves over 5% higher Recall@1 for long text and image retrieval,
comparedtoCLIP.
Ourcontributionsaresummarizedasfollows:
1. WeproposeaMulti-ModalGenerativeEmbeddingModel(MM-GEM),wherebythegenera-
tiveandembeddingobjectivesareencapsulatedtoachieveunification.
2. APoolAggregatorandMulti-StageTrainingstrategyareproposedtorepresentanimage
fromthefeaturemap,whichefficientlyenablesthefine-grainedability.
3. ExperimentalresultdemonstratesthatMM-GEMshowscompetitiveperformanceonbench-
marksforembeddingmodels,whilestillkeepsthegoodabilityofgeneration.
2 RelatedWorks
Vision-LanguagePre-trainedModelshaveundergoneasignificantevolutionintheirapplicationto
downstreamtasks. Earlierworks[7,18,22,28,48,49]learncross-modalrepresentationsthrough
proxytaskssuchasmaskedmodelingandimage-textmatching,butrequirefurtherfine-tuningfor
specificdownstreamtasks. Modernworksbuildpre-trainedmodelsinamannerthatmirrorstheir
ultimateusetoachieveseamlessintegrationwithdownstreamtasks. Theseworkscanbecategorized
into two paradigms: embedding models and generative models. Embedding models [20, 40, 50]
independentlyextractandmapfeaturesfromeachmodalityintoasharedspace,facilitatingefficient
cross-modal retrieval and open-set classification. Generative models [2, 32, 47, 55] reformulate
downstreamtaskslikeVisualQuestionAnswering(VQA)asauto-regressivegenerationtasks. In-
heritingcapabilitiessuchasinstructionfollowing[32,55]orin-contextlearning[2]fromLLMs,
generativemodelsareoftenutilizedtotacklecomplexunderstandingtasksthatcannotbewellsolved
byembeddingmodels.
ModalityUnificationhasbeenalongstandinggoaltopursuebetterunderstandingabilityandhigher
computationefficiencybroughtbysharedrepresentations. However, thedistinctrequirementsof
embeddingandgenerativemodelsoftenleadtothepreferenceforseparatetextencodersanddecoders.
BLIP[30]andInternVL[8]approachthisbysharingmostparametersacrossthetextencoderand
decoder,withtheexceptionoftheself-attentionorcross-attentionlayers. CoCa[52]splitsthetext
decoderintounimodalandmultimodalcomponents,thenremovethecross-attentionmoduleinthe
2Generative
a cat lying asleep on a chair Loss
E oP
nV o
redoc lausi
rotagerggAl
jorP
.
jorP
.
IMG
L [Ca Ar Pg
]
e aLan cag tua lyig nge
a
sM leeo pd oe nl
a
Share
weights
P
gatS
IMG [CAP] a cat …
IMG Info-NCE Loss jor
.
EMB
e
1 Large Language Model
S
egat
IMG1 …IMGM [CAP1]… [CAPN] a cat …
2 a cat lying asleep on a chair [EMB]
Figure1: OverviewofMM-GEM,inwhichalargelanguagemodelactsasbothtextencoderfor
embeddingandtextdecoderforgeneration. ThevisualfeatureisalignedwiththeLLMbyseveral
projectionlayersandaPoolAggregator.
unimodaldecoderlayers. FROMAGe[24]groundstheimagefeaturetotheinputsandoutputsofa
frozenlargelanguagemodelbyseveralprojectionlayers. However,thelackofjointtrainingresults
inaperformancedeficiency. OneveryrecentworkGRIT[35]successfullyunifiesembeddingand
generativeNaturalLanguageProcessing(NLP)tasks. Itisworthnotingthatunimodalmodelsimply
anaturalcorrespondenceinembedding. Forexample,generative-onlymodelsshowacertainlevelof
performanceonembeddingNLPtasks[35]. However,whetherthereisasignificantconflictbetween
themulti-modalembeddingandgenerativeobjectivesremainsunderexplored.
3 Approach
TheprimarygoalofMulti-ModalGenerativeEmbeddingModel(MM-GEM)isunifyingtextencoder
anddecoder,whilesupportingbothembeddingandgenerativeparadigms. However,therearethree
uncertaintiesorchallengestoachievethisunification: 1)Whetherthesuperpositionoftwolearning
targetsinthesamelanguagemodelwillsignificantlyconflictwitheachother. 2)Embeddingmodels
likeCLIPextractoneglobalvisualfeatureforeachimage,whichisinsufficientforgenerativetasks.
3)Thedatasuitablefortrainingembeddingmodelsisoftennoisy,whichresultinginthesub-optimal
abilityofgeneration. Inthissection,wewillintroduceourapproachoftacklingthesethreeproblems.
3.1 EncapsulatedGenerativeandEmbeddingObjectives
A generative language model consists of a word embedding layer, a stacked transformer, and a
predictionhead. Thewordembeddinglayercanberegardedasalineartransformationtoproject
theone-hotprobabilitydistributionp oftheinputsequencetothelatentspaceW . The
i=1,2,..,L in
predictionheadactsasalineartransformationtoprojectthelatentspaceW totheunnormalized
out
probabilitydistributionpˆ . Asthepˆ isawellestimationofp ,thetransformationbetween
i=1,2,..,L t t+1
latentspaceW andW isapproximatelylinear. Therefore,weleveragelightprojectionlayersh ,
in out 1
h andh totransformlatentspacebetweenvisualspaceV,W andW :
2 3 in out
V =h (V), W =h (W ), V =h (V ), (1)
Emb 1 Emb 2 out in 3 Emb
whereV andW isthespaceofimageandtextembeddings,respectively. V isthespaceof
Emb Emb in
visualfeaturesV tobeconcatenatedwithwordembeddingsfromspaceW .
in in
Forembedding,wefollow[40]andadopttheinfo-NCElosstolearningthecross-modalalignment:
1 (cid:88)B ev i⊤ti/τ 1 (cid:88)B et⊤ i vi/τ
L =− log , L =− log , (2)
v2t B
i=1
(cid:80)B j=1ev i⊤tj/τ t2v B
i=1
(cid:80)B j=1et⊤
i
vj/τ
andL =L +L ,wherev andt arethenormalizedembeddingsofi-thvisualfeatureand
Emb v2t t2v i j
j-thtextfeatureinabatchofsizeB.τ isalearnabletemperature.Thevisualfeaturewillbediscussed
3inSection3.2andthetextfeatureisthelasthiddenstatesontopofa[EMB]tokenappendedtothe
textsequence.
Forgeneration,weadopttheimagecaptioninglosstopredictthenexttokenx(i)basedonthevisual
inputV ,aspecial[CAP]token,andprevioustokensx(<i):
in
B
1 (cid:88)
L =− logP(f (x(i))|f ([V ,[CAP],x(<i)])), (3)
Gen B θ θ in
i=1
wheref isthelanguagemodelparameterizedbyθand[*]istheconcatenateoperation.
θ
During training, both learning objectives are applied on all data samples. The language model
forwardstwicetogettwolossesandthefinallossfunctionisthedirectsummationwithoutcareful
adjustmentofweightingfactors:
L =L +L . (4)
MM-GEM Emb Gen
3.2 VisionPoolAggregatorandMulti-StageTraining
Simultaneouslytraininggenerativeandembeddingobjectivesencounterstwomainchallenges: 1)
embeddingmodelsusuallytaketheglobalfeatureasvisualrepresentation,whileaglobalfeatureis
insufficientforimagecaptiongeneration. 2)thedatasuitablefortrainingCLIPislarge-scalealt-texts.
Sometimesaalt-textisnoisy,lackingoflinguisticcoherence,andtherelevancetovisualinformation
isweak. Toalleviatetheseissues,weproposeamulti-stagetrainingstrategyequippedwithaVision
PoolAggregator.
Insteadofextractingaglobalvisualfeatureontopofa[CLS]token,MM-GEMrepresentsanimage
asaspatialfeaturemapV ∈ RC×H×W, whereC, H andW indicatethedimension, heightand
widthofthefeaturemap. AVisionPoolAggregatorflexiblyaggregatesthevisualinformationfromV.
Toboostthetrainingefficiency,instage-onepre-training,weapplybothembeddingandgenerative
lossonthemean-pooledvisualfeature:
V1 =h (h (MeanPool(V))), V =h (MeanPool(V)). (5)
in 3 1 Emb 1
Thissignificantlyimprovesthetrainingefficiencyofstageone,andenableslargerbatchsizepreferred
by contrastive learning. To further improve MM-GEM’s ability of generation and fine-grained
understanding,wesetastage-twopre-trainingprocedureonimage-captionanddensecaptiondata.
Inthisstage,visualfeatureV isaggregatedtoafix-sizedfeaturemapshapeofH ×W accordingto
theregionRofthedescription(wholeimageforregularimage-captiondata):
V2 =h (h (RoIAlign(V,R))). (6)
in 3 1
In stage two, the [CAP] token is replaced with a series of soft prompts [CAP ],i ∈ [1,N], only
i
h layer and soft prompts are updated in this stage. Therefore, the embedding ability learned in
3
stageoneiscompletelymaintained. Inparallelwithdensecaptioning,wealsoboostMM-GEM’s
fine-grainedretrievalabilitybytrainingaheadonregionalimage-textpairs. Toeliminateinterference
withexistingabilities,weaddalinearheadh ontheoutputofh :
4 1
V2 =h (h (RoIAlign(V,R))), (7)
Emb 4 1
whereRoIAlignaggregatesvisualfeatureV toembeddingaccordingtotheregionR. Wefindthat
originalCLIPorCaptioningmodelfailsinfocusingregionalvisualinformation,whichisdramatically
improvedbytheproposedstagetwo. MoredetailswillbediscussedinSection4.3.
4 Experiments
4.1 ImplementationDetails
Pre-training Stage One. We pre-train MM-GEM using LAION-2B [41] and COYO-700M [5]
datasets,containingatotalof2.3billionimage-textpairs. Thelanguagemodelisinitializedfrom
TinyLlama[53]. Forthevisualencoder,westudythreevariantsofViTs: ViT-B/16,ViT-L/14,and
ViT-L/14-336,andthesevisualencodersareinitializedfromOpenCLIP[19]. WeuseLAMB[51]
optimizerwithaweightdecayof0.05. Thelearningratefortheprojectionlayersissetto5e-4,while
4Table1: PerformancecomparisononCOCO[31]andFlickr30K[39]image-to-text(I2T)andtext-to-
image(T2I)retrieval. R@1,R@5,andR@10indicaterecallratioattop1,5,and10. Allmodelsin
thistableuseViT-Baseasvisionencoder.
COCOI2T COCOT2I Flickr30KI2T Flickr30KT2I
Model
R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10
CLIP-Only 57.8 80.3 87.3 41.7 67.2 76.7 86.2 97.7 99.2 70.3 91.4 95.3
MM-GEM 57.0 79.7 87.2 41.4 66.6 76.3 84.6 97.5 99.3 70.1 90.7 95.0
Table2: PerformancecomparisononImageClassificationintheWild(ICinW)[29]. Themetricof
eachdatasetfollows[29]. AllmodelsinthistableuseViT-Baseasvisionencoder.
Model
CLIP-Only 94.8 75.5 59.2 54.7 38.6 20.2 27.0 70.3 50.1 79.9 90.3 18.6 82.3 42.4 57.7 58.1 84.0 59.9 64.6 81.8 60.5
MM-GEM 94.6 75.8 60.5 54.8 43.7 19.9 12.1 73.8 50.4 80.2 90.2 18.2 82.1 48.1 58.4 57.6 81.0 63.8 66.6 80.5 60.6
forthevisualencoderandthelargelanguagemodel,itissetto5e-5. Inputimagesarerandomly
croppedtoaresolutionof224×224duringpre-trainingexceptforViT-L/14-336. Weadoptalinear
warm-upthencosinedecaylearningrateschedule. Thetrainingprocedureisperformedonatotal
batchsizeof81,920for80,000iterations. ThetextprocessingfollowsTinyLlamaexceptthatthe
maximum length of the text is truncated to 50. Following CLIP [40], the learnable temperature
parameterτ isinitializedto0.07andclippedat0.01. Weuse64×H800GPUstotrainthemodelin
thisstage.
Pre-trainingStageTwo. Instagetwo,Forfine-grainedcaptioning,MM-GEMisfurthertrainedon
CC3M[43],CC12M[6],SBU[37]andLAION[42]filteredbyBLIP[30]andVisualGenome’s
densecaptiondata[26],containing38millionimage-captionand1.8millionregion-descriptionpairs
intotal. Inthisstage,onlytheh layerandsoftpromptsareupdated. Thenumberofsoftpromptsis
3
setas64. Thistrainingprocedureutilizesatotalbatchsizeof2048for60,000iterations,andthe
learningrateisconsistentwiththatofstageone.
For fine-grained retrieval, the training data is in line with fine-grained captioning. The training
procedureisperformedonatotalbatchsizeof49,152for15,000iterations. Toavoiddisrupting
the alignment learned in Stage one, h is initialized as an Identity Mapping Matrix. Only h is
4 4
updatedduringthisstage. ThusallresultsofMM-GEMstageoneoncross-modalretrievalandimage
classificationwillnotbealtered.
Evaluation. To thoroughly evaluate MM-GEM, we include various downstream tasks in the ex-
perimentsection. Unlessotherwiseindicated,allresultsarereportedunderthezero-shotprotocol
withoutfurtherfine-tuning. WeevaluateMM-GEMon: 1)Image-TextRetrieval. Forthistask,we
utilizetwoprominentbenchmarkdatasets: COCO[31]andFlickr30K[39],whichfeatureadiverse
collectionofimageswithcomplexscenes. Weevaluateonthestandard1KtestsetforFlickr30K
and 5K test set for COCO. The evaluation metrics are Recall@K, where K = 1,5,10, on both
text-to-imageandimage-to-textretrieval. 2)ImageClassification. Weevaluatethecapabilityof
zero-shotimageclassificationonthetrack“ImageClassificationintheWild”oftheELEVATER
benchmark[29]. ELEVATERisdesignedtochallengemodelswiththetaskofcategorizingimages
thatarecapturedinreal-world,unconstrainedenvironments. Thepredefinedcategoriesofeachsubset
couldrangefromcommonobjectstospecificscenes. WefollowallmetricsofELEVATERforall
subsets. 3)ImageCaptioning. Inthistask,weassessthemodel’sabilitytogeneratedescriptiveand
coherentcaptionsforimages. Weemploytwowell-establisheddatasets: COCO[31]andNoCaps[1].
COCOcaptionsincludeawidevarietyofobjects,scenes,andactivities,whileNoCapsencompass
novelvisualobjects. Besidesthesebenchmarks,wealsodemonstrateMM-GEM’sspecialabilityby
moreevaluationmanners. Thedetailswillbeintroducedinthespecificsections.
5
]72[01-RAFIC ]72[001-RAFIC
]01[DTD
]61[TASoruE ]04[3102-REF ]43[tfarcriA-CVGF ]51[.tsiD-ITTIK ]11[TSINM ]64[noylemaChctaP ]31[7002-COV ]41[101-hcetlaC ]04[112-yrtnuoC ]4[101-dooF ]44[BRSTG ]12[semeMlufetaH ]63[srewolF.xO ]83[stePTIII.xO ]04[2TSS-deredneR ]9[54-CSISER ]52[sraC-drofnatS
egarevATable3: Performancecomparisononzero-shotimagecaptioningtaskonCOCO[31]andNoCaps
[1]. AllmodelsinthistableuseViT-Baseasvisionencoder.
COCOCaption NoCapsCaption
Model
BLEU@4 Meteor Rouge CIDEr BLEU@4 Meteor Rouge CIDEr
Cap-Onlystageone 13.4 15.8 36.2 49.8 16.6 15.9 37.3 46.3
MM-GEMstageone 12.9 15.8 37.0 48.8 17.3 16.2 38.6 47.0
Cap-Onlystagetwo 31.2 25.4 53.2 103.9 38.8 26.6 57.0 96.5
MM-GEMstagetwo 28.7 24.5 52.0 96.3 36.2 25.7 55.6 91.0
Table4: PerformancecomparisononCOCO[31]andFlickr30K[39]image-to-text(I2T)andtext-to-
image(T2I)retrieval. R@1,R@5,andR@10indicaterecallratioattop1,5,and10. Allmodelsin
thistableuseViT-Largeasvisionencoder.
COCOI2T COCOT2I Flickr30KI2T Flickr30KT2I
Model
R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10
CLIP[40] 57.4 80.0 87.1 34.3 58.6 69.5 87.0 97.5 99.1 63.5 86.4 91.8
OpenCLIP[19] 61.3 83.4 89.7 45.8 70.2 79.1 89.8 98.7 99.5 74.9 92.5 95.8
MM-GEM 61.0 82.6 89.2 45.6 70.5 79.3 89.0 99.0 99.5 75.4 92.6 96.0
4.2 EncapsulatedGenerativeandEmbeddingObjectives
4.2.1 ComparisonofTrainingObjectives
ThemainriskofMM-GEMistheembeddingandgenerativeobjectivesmayconflictwitheachother
inthesametextmodel. Themoststraightforwardwaytoverifythisistocomparetheperformance
ofamodeltrainedusingthetwoobjectivesaloneandtheobjectivestogether. Therefore,wetrain
threemodelsunderthesameexperimentalsettingexceptfortrainingobjectives: 1)CLIP-Onlywhich
usesembeddingobjectivesL alone;2)Cap-OnlywhichusesgenerativeobjectivesL alone;
Emb Gen
3) MM-GEM which uses two objectives simultaneously. These three models adopt ViT-Base as
vision encoder and only trained with 1.5 billion seen samples for computational savings. From
theresultlistedinTable1and2,MM-GEMachievesverysimilarperformancetoCLIP-Onlyon
bothcross-modalretrievalandzero-shotimageclassificationtasks. Table3showsthattheimage
captioningperformancegapbetweenMM-GEMandCap-Onlyisnegligible,andthisconclusion
holdsforbothstageoneandtwo. Forstageone,althoughMM-GEMlagsalittlebitbehindCap-Only
on COCO in some metrics such as BLEU@4, Rouge and CIDEr, it will be slightly better than
Cap-OnlyonNoCaps,thereforetheoverallcaptioningperformanceisclose. Afterstage-twotuning,
theperformancegapbetweenCap-OnlyandMM-GEMincreases. Inthemeantimewenotethat
MM-GEMstillachievesrelativelygoodvisualdescriptiongenerationcapabilities. Sincenegligible
performancegapsinthefirststagecansupportourconclusions,weleavethecontinuedexploration
ofthispartforfuturework. Theseresultssolidlysupportourconclusion: encapsulatingembedding
andgenerativeobjectivesinthesametextmodelwillnotleadtosignificantconflict.
4.2.2 ComparisonwithState-of-the-arts
ToverifyMM-GEM’sscalabilityandmakecomparisonwithstate-of-the-artworks,wetrainMM-
GEMwithViT-Largeasvisionencoder. Forembeddingmodelopponents,wemainlycomparewith
OpenAICLIP[40]andOpenCLIP[19]. Table4showsthatMM-GEMachievessimilarcross-modal
performancetoOpenCLIP,whileoutperformingCLIPbyalargemargin. Thismarginmaycome
fromthepre-trainingdata: MM-GEM’sdataisclosertoOpenCLIP,whileOpenAICLIPusesprivate
data. InTable5,theresultsaresimilartothecross-modalretrievalbenchmark. MM-GEMachieves
66.3% in average, slightly better than OpenCLIP and 4.5% ahead of OpenAI CLIP. For image
captioningmodels,wechooseFlamingo[2]andClipCapforcomparison. Amongthesemodels,only
ClipCap’strainingdataincludeCOCO.Flamingobridgescontrastivepretrainedvision-onlymodels
andlanguage-onlymodelsbyonlytrainingaPerceiverResamplerandgatedcrossattentionlayers.
ClipCapuseCLIP encoding asaprefixto the caption, by employingasimplemapping network.
FromtheresultsinTable6,MM-GEMsignificantlyoutperformsFlamingoeventhoughthelatter
adoptsamuchlargerlanguagedecoder. AsClipCapistrainedonCOCO,MM-GEMperformsworse
thanClipCaponCOCObutissubstantiallyaheadonNoCaps.
6Table5: PerformancecomparisononImageClassificationintheWild(ICinW)[29]. Themetricof
eachdatasetfollows[29]. AllmodelsinthistableuseViT-Largeasvisionencoder.
Model
CLIP[40] 94.0 67.4 52.6 49.5 45.5 25.7 20.5 64.4 58.4 79.5 93.0 28.1 90.2 52.9 60.2 71.4 92.2 59.9 62.3 67.4 61.8
OpenCLIP[19] 96.0 82.5 61.5 65.1 47.7 32.4 22.5 65.2 57.2 80.7 94.1 25.4 89.9 56.5 54.5 74.2 92.9 60.6 72.1 91.4 66.1
MM-GEM 97.0 82.8 67.2 69.5 47.4 31.9 26.2 69.5 50.5 80.3 92.7 26.0 89.8 54.3 61.5 69.8 90.6 61.5 68.9 89.3 66.3
Table6:PerformancecomparisononimagecaptioningtaskonCOCO[31]andNoCaps[1].*denotes
modelfinetunedonCOCOtrainingsplit.
COCOCaption NoCapsCaption
Model
BLEU@4 Meteor Rouge CIDEr BLEU@4 Meteor Rouge CIDEr
Flamingo-9B[2] - - - 79.4 - - - -
ClipCap* 33.5 27.5 - 113.1 - - - 65.8
MM-GEM 32.8 26.5 54.8 110.9 39.8 27.3 57.8 100.7
4.3 Fine-grainedUnderstandingAbility
Insteadofaligningvisionandlanguagemodalityfromtheglobalperspective,MM-GEMisdesigned
toalignonthevisualfeaturemap. Therefore,MM-GEMisequippedwithfine-grainedunderstanding
ability. ThegoalisthatPoolAggregatorcanbedirectlyutilizedonthefeaturemaptogettheregion
featurerequiredbyembeddingordescription. However,weobservethatregulartrainingofCLIPor
imagecaptioningmodelachievesresultsthatfallfarshortofthisgoal. Thewholespatialfeaturemap
isdominatedbytheglobalvisualinformation,makingitdifficulttodistinguishbylocation. Inthis
paper,wetacklethisproblembytuningoraddinglightprojectionlayersonregion-leveldata,leading
todramaticimprovementonfine-grainedabilty. Inthissection,wewillpresentthedetailsintermsof
bothembeddinganddescriptiongeneration.
4.3.1 Fine-grainedDescriptionGeneration
Astraightforwardwaytolearnimagecaptioningistoflattenavisualfeaturemapanduseitasinput
toalanguagedecoder. Theidealvisualfeaturemaphasthepropertythatfeaturescroppedfroma
regioncangenerateadescriptionofthatregion. Wefindthattrainingonregularimage-captiondata
aloneisnotsufficienttoachievethisproperty. Therefore,weusethestrategypresentedintheSection
3.2totrainMM-GEMonmixeddataofimage-captionandregion-descriptiondata. WeadoptBLIP’s
filtereddataasimage-captiondataandVisualGenome’sdensecaptionasregion-descriptiondata. We
comparetheresultsundertwodifferentsettingsanddemonstrateinFigure2. Thegeneratedregion
descriptionongraybackgroundshowsthatthecaptioningmodeltrainedwithoutregion-description
datafailsindistinguishingvisualinformationbylocation. Forexample,ittendstogenerateglobal
image captions or descriptions of non-corresponding locations, like the red bounding box of the
buildinggenerates“Aboattravellingdownariverinfrontofalargebuilding”. Theexampleinthe
secondrowofFigure2demonstratesthatMM-GEMwithregiondescriptiondatacanaccurately
distinguish objects, and still retains the necessary contextual information, like “A car parked on
the side of the street”. These results validate the effectiveness of our stage-two training strategy,
avoidingMM-GEMfrombeinglimitedtoaregularimagecaptioningmodel,andenablingtheability
ofgeneratingfine-graineddescriptionsbasedonregions.
4.3.2 Fine-grainedImage-TextRetrieval
Traditionalcross-modalembeddingmodelslikeCLIPfocusonglobalalignment,thuscanonlyactas
aninstance-levelretriever. Therearetwoissueswiththesemodels: a. Forthetext-to-imageretrieval
task,thequerydescriptionmayonlycorrespondtopartoftheimage. Itisworthexploringhowwell
themodelcanlocalizethecorrespondingvisualinformationbasedonthetextquery. b. Thetraining
approachthatfocusesonlyonglobalalignmentmaycausethemodeltofocusonlyonsalientobjects
intheimage,whichinturnweakenstheunderstandingoffine-grainedinformation.
7
]72[01-RAFIC ]72[001-RAFIC
]01[DTD
]61[TASoruE ]04[3102-REF ]43[tfarcriA-CVGF ]51[.tsiD-ITTIK ]11[TSINM ]64[noylemaChctaP ]31[7002-COV ]41[101-hcetlaC ]04[112-yrtnuoC ]4[101-dooF ]44[BRSTG ]12[semeMlufetaH ]63[srewolF.xO ]83[stePTIII.xO ]04[2TSS-deredneR ]9[54-CSISER ]52[sraC-drofnatS
egarevAA white and red tram. A tall building in the distance. A car parked on the side of the street.
The ground is concrete. A boat in the water. The dog has a leash.
A train with people on it in front of a building. A boat traveling down a river in front of a large A dog is walking down the sidewalk with a
A man is walking down the street with a bag building. bicycle in front of him.
on his shoulder. A boat on the water in front of a city skyline. A dog is sitting on a bench in front of a car.
A sign on the side of a building.
A green street sign on the side of the street.
Green roof on building.
Green leaves on the tree.
A car parked on the street.
White lines on the road.
A white car on the road.
A tower on a building.
A car parked on the side of the street.
A black car parked on the street.
An open umbrella.
Blue sign on the pole.
Figure 2: Visualization of fine-grained description generation. This figure shows the captioning
resultsofusingregionfeaturesfromthevisualfeaturemapasinput. Thetextinthesamecolorasthe
boundingboxinthefigureisthedescriptionofthecorrespondingarea. Textonagraybackground
indicatesresultswithoutregiondescriptiondata.
Car People Bike
Car People Dog
Figure3: Visualizationoffine-grainedimage-textretrieval. Thisfigureshowsthesimilaritybetween
thevisualfeaturemapandthetextfeatureattwostages. Thebluebordersandundertonesrepresent
theresultfromthepre-trainingstageone,andyellowbordersandundertonesillustratetheresultsof
stagetwo. Thetextsuperimposedontheimagecorrespondstotheinputtext.
Tostudyissuea.,wecompareMM-GEMtrainedattwostagesbyshowingthesimilaritiesbetween
visualfeaturemapsandseveralgiventextqueries. Forthestage-onemodel,wecalculatethecosine
similaritiesbetweennormalizedvisualfeaturesoutputbyh andtextfeaturesoutputbyh . Forthe
1 2
stage-twomodel,wealtervisualfeaturestotheoutputofh . Thevisualizedresultsaredemonstrated
4
inFigure3. Thetwocolumnsontheleftclearlyshowthatstage-twomodelwelllocalizesthetext
querytothecorrespondingregion,whilestage-onemodeltotallyfails. Therightmostcolumnshows
thestage-twomodelwellrespondstodifferenttextqueriesforthesameimage. Theseresultsshow
thatstage-twotrainingessentiallyenablestheabilityoffine-grainedretrieval.
Tostudyissueb.,weaddanewquantitativebenchmarkL-DCIbasedonDenselyCaptionedImages
(DCI)[45]. DCIconsistsof8012imagesfromSA-1B[23],eachimagecorrespondstoacomplete
descriptionaimingtocapturethefullvisualdetailsintheimage. Wedirectlyevaluatecross-modal
retrievalonallimagesandoveralldescriptionsinDCI.Welistthetext-to-imageretrievalresultsin
8Table7: PerformancecomparisononCOCO[31],Flickr30K[39]andDCI[45]text-to-image(T2I)
retrieval. R@1,R@5,andR@10indicaterecallratioattop1,5,and10. Allmodelsinthistableuse
ViT-Largeasvisionencoder.
COCOT2I FlickrT2I L-DCIT2I
Model
R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10
CLIP-Only-Base 41.7 67.2 76.7 70.3 91.4 95.3 47.3 66.3 73.0
MM-GEM-Base 41.4 66.6 76.3 70.1 90.7 95.0 46.3 66.0 72.6
MM-GEM-Basestagetwo 42.5 68.9 78.5 72.3 91.6 95.7 47.9 68.4 74.9
CLIP[40] 34.3 58.6 69.5 63.5 86.4 91.8 30.4 49.4 57.3
OpenCLIP[19] 45.8 70.2 79.1 74.9 92.5 95.8 43.0 62.3 69.2
MM-GEM 45.6 70.5 79.3 75.4 92.6 96.0 49.4 68.1 74.5
MM-GEMstagetwo 47.2 72.3 81.1 76.4 94.1 96.8 54.1 72.5 78.6
Table8: Long-formtextimageretrievalperformancecomparisononDCI[45]. R@1,R@5, and
R@10indicaterecallratioattop1,5,and10.
L-DCII2T L-DCIT2I
Model
R@1 R@5 R@10 R@1 R@5 R@10
CLIP-Only-Base 46.7 67.4 73.9 47.3 66.3 73.0
MM-GEM-Base 46.6 67.4 74.4 46.3 66.0 72.6
CLIP-Large[40] 33.7 52.8 60.0 30.4 49.4 57.3
OpenCLIP-Large[19] 44.4 64.0 70.5 43.0 62.3 69.2
MM-GEM-Large 49.4 69.4 75.4 49.4 68.1 74.5
MM-GEM-Large-336 51.0 70.2 76.5 51.8 70.2 76.0
Table7. Theresultsinthelasttworowsshowthatthestage-twotrainedMM-GEMimproveson
allretrievalmetrics,especiallyonL-DCIwithnearly5%atRecall@1. Thisresultshowsthatthe
proposedstagetwocanimprovethemodel’sabilitytounderstandfine-grainedinformation.
4.4 AdvancedTextModelinMM-GEM
AsMM-GEMappliesalargelanguagemodel(LLM)astextmodule,it’scriticaltofigureoutother
benefitsbesidesintroducinggenerativecapabilities. LLMstypicallyhavegoodabilityoflanguage
processing due to the large amount of language data. We therefore hypothesize that MM-GEM
performsbetterthanregularCLIPondatacontainingmorecomplextext. Toverifythis,wemainly
focusonlong-formtextimagecaptiondatainthiswork. WeevaluateMM-GEMandregularCLIPon
L-DCIcross-modalretrievalbenchmarkdescribedinSection4.3.2. ResultsshowninTable8indicate
thatanadvancedtextmodulewillsignificantlyimprovestheperformanceonthebenchmarkwith
long-formtext,byover5%marginonRecall@1. Andtheperformancefurtherincreaseswhilethe
imagesizeis336. ItisworthnotingthatMM-GEMwastrainedwithamaximumtextlengthof50,
whichisshorterthanCLIP.Weadjusteditto200whentestingonDCIonly. Thecomparisonbetween
CLIP-Only-BaseandMM-GEM-Basedemonstratethattheimprovementcomesfromthetextmodule
insteadoftrainingobjectives. EventhoughMM-GEMdoesnotshowadvantageontypicalcross-
modalretrievalbenchmarkslikeCOCOandFlickr30K,accordingtoTable4,theimprovementona
morecomplexbenchmarkissignificant. Theresultsinthissectioninspirefutureworksonexploring
benefitsofanadvancedtextencoderinCLIP.
5 Conclusion
TheMulti-ModalGenerativeEmbeddingModel(MM-GEM)presentsaunifiedapproachtomulti-
modallearningbyintegratinggenerativeandembeddingobjectiveswithinasingleLargeLanguage
Model(LLM).Ourexperimentsdemonstratethatthesetwoobjectivesdonotsignificantlyconflict
witheachother. MM-GEMachievescompetitiveperformanceacrossarangeoftasks, including
cross-modal retrieval, zero-shot classification, and image captioning. A key contribution is the
9PoolAggregator,enhancingthemodel’sabilitytohandlefine-grainedtasks.Additionally,MM-GEM’s
advancedtextmodulesignificantlyimprovesperformanceonlong-formtextretrieval,showcasingthe
benefitsofleveragingarobustLLMfortextprocessing.
MM-GEMrepresentsasignificantsteptowardsunifiedmulti-modalmodels,yettherearestillmany
subsequentpotentialdirections: 1)Wemainlyfocusonimagecaptioningforgenerativetasksinthis
work, the performance impact of adding plain language data needs to be further investigated. 2)
MM-GEMenablesLLMsgeneratediscriminativeoutputsbesideslanguagetokens,thismaybenefit
multi-modallargelanguagemodelbyretrievingorgroundingvisualinformationefficiently. Further
investigationintotheseaspectswillbeexploredinfuturework.
6 Limitations
AlthoughMM-GEMintegratesgenerativeandembeddingobjectiveswithinasingleLLM,thereare
stillsomelimitations. Captionlossallowsthemodeltofocusmoreonthedetailedinformationinthe
text,buttheexistingdatasetlimitsitsability. Thetextweusedinpre-trainingstageoneisnoisy,of
lowquality,andhaslessdetailedinformation,whichlimitstheabilityofourmodel. Thedesignof
thePoolAggregatorallowsourmodeltohandlefine-grainedtasks. However,inpre-trainingstage
one,themodeldoesnotdirectlyexhibitfine-grainedcapabilitiesduetothelackofregion-leveldata.
Still,theimpliedfine-grainedcapabilitiesrequireonlyaverysmallamountofregion-leveldatatobe
bootstrapped. Wecantrytointroduceaverysmallamountofregion-leveldatainpre-trainingstage
oneaswell,sothatthemodelcanhaveastrongerfine-grainedcapability.
References
[1] HarshAgrawal,KaranDesai,YufeiWang,XinleiChen,RishabhJain,MarkJohnson,DhruvBatra,Devi
Parikh,StefanLee,andPeterAnderson. Nocaps:Novelobjectcaptioningatscale. InProceedingsofthe
IEEE/CVFinternationalconferenceoncomputervision,pages8948–8957,2019.
[2] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,KarelLenc,
ArthurMensch,KatherineMillican,MalcolmReynolds,etal. Flamingo: avisuallanguagemodelfor
few-shotlearning. Advancesinneuralinformationprocessingsystems,35:23716–23736,2022.
[3] HangboBao,WenhuiWang,LiDong,QiangLiu,OwaisKhanMohammed,KritiAggarwal,SubhojitSom,
SonghaoPiao,andFuruWei.Vlmo:Unifiedvision-languagepre-trainingwithmixture-of-modality-experts.
AdvancesinNeuralInformationProcessingSystems,35:32897–32912,2022.
[4] LukasBossard,MatthieuGuillaumin,andLucVanGool. Food-101–miningdiscriminativecomponents
withrandomforests. InEuropeanconferenceoncomputervision,pages446–461.Springer,2014.
[5] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim.
Coyo-700m:Image-textpairdataset. https://github.com/kakaobrain/coyo-dataset,2022.
[6] SoravitChangpinyo,PiyushSharma,NanDing,andRaduSoricut. Conceptual12m:Pushingweb-scale
image-textpre-trainingtorecognizelong-tailvisualconcepts.InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages3558–3568,2021.
[7] Yen-ChunChen,LinjieLi,LichengYu,AhmedElKholy,FaisalAhmed,ZheGan,YuCheng,andJingjing
Liu. Uniter:Universalimage-textrepresentationlearning. InEuropeanconferenceoncomputervision,
pages104–120.Springer,2020.
[8] ZheChen,JiannanWu,WenhaiWang,WeijieSu,GuoChen,SenXing,ZhongMuyan,QinglongZhang,
XizhouZhu,LeweiLu,etal. Internvl: Scalingupvisionfoundationmodelsandaligningforgeneric
visual-linguistictasks. arXivpreprintarXiv:2312.14238,2023.
[9] GongCheng,JunweiHan,andXiaoqiangLu. Remotesensingimagesceneclassification:Benchmarkand
stateoftheart. ProceedingsoftheIEEE,105(10):1865–1883,2017.
[10] MirceaCimpoi,SubhransuMaji,IasonasKokkinos,SammyMohamed,andAndreaVedaldi. Describing
texturesinthewild. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,
pages3606–3613,2014.
[11] LiDeng. Themnistdatabaseofhandwrittendigitimagesformachinelearningresearch[bestoftheweb].
IEEEsignalprocessingmagazine,29(6):141–142,2012.
10[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,etal. Animageis
worth16x16words:Transformersforimagerecognitionatscale. InInternationalConferenceonLearning
Representations,2020.
[13] MarkEveringham,LucVanGool,ChristopherK.I.Williams,JohnM.Winn,andAndrewZisserman. The
pascalvisualobjectclasses(VOC)challenge. Int.J.Comput.Vis.,88(2):303–338,2010.
[14] LiFei-Fei,RobFergus,andPietroPerona. Learninggenerativevisualmodelsfromfewtrainingexamples:
Anincrementalbayesianapproachtestedon101objectcategories. In2004conferenceoncomputervision
andpatternrecognitionworkshop,pages178–178.IEEE,2004.
[15] JannikFritsch,TobiasKuehnl,andAndreasGeiger.Anewperformancemeasureandevaluationbenchmark
forroaddetectionalgorithms.In16thInternationalIEEEConferenceonIntelligentTransportationSystems
(ITSC2013),pages1693–1700.IEEE,2013.
[16] PatrickHelber,BenjaminBischke,AndreasDengel,andDamianBorth. Eurosat:Anoveldatasetanddeep
learningbenchmarkforlanduseandlandcoverclassification. IEEEJournalofSelectedTopicsinApplied
EarthObservationsandRemoteSensing,12(7):2217–2226,2019.
[17] YupanHuang,HongweiXue,BeiLiu,andYutongLu. Unifyingmultimodaltransformerforbi-directional
imageandtextgeneration. InProceedingsofthe29thACMInternationalConferenceonMultimedia,
pages1138–1147,2021.
[18] ZhichengHuang,ZhaoyangZeng,BeiLiu,DongmeiFu,andJianlongFu. Pixel-bert: Aligningimage
pixelswithtextbydeepmulti-modaltransformers. arXivpreprintarXiv:2004.00849,2020.
[19] GabrielIlharco,MitchellWortsman,RossWightman,CadeGordon,NicholasCarlini,RohanTaori,Achal
Dave,VaishaalShankar,HongseokNamkoong,JohnMiller,HannanehHajishirzi,AliFarhadi,andLudwig
Schmidt. Openclip. https://github.com/mlfoundations/open_clip,2021.
[20] ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh,HieuPham,QuocLe,Yun-HsuanSung,
ZhenLi,andTomDuerig. Scalingupvisualandvision-languagerepresentationlearningwithnoisytext
supervision. InInternationalconferenceonmachinelearning,pages4904–4916.PMLR,2021.
[21] DouweKiela,HamedFirooz,AravindMohan,VedanujGoswami,AmanpreetSingh,PratikRingshia,and
DavideTestuggine. Thehatefulmemeschallenge:Detectinghatespeechinmultimodalmemes. Advances
inNeuralInformationProcessingSystems,33:2611–2624,2020.
[22] WonjaeKim,BokyungSon,andIldooKim. Vilt:Vision-and-languagetransformerwithoutconvolutionor
regionsupervision. InInternationalconferenceonmachinelearning,pages5583–5594.PMLR,2021.
[23] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,TeteXiao,
SpencerWhitehead,AlexanderCBerg,Wan-YenLo,etal. Segmentanything. InProceedingsofthe
IEEE/CVFInternationalConferenceonComputerVision,pages4015–4026,2023.
[24] JingYuKoh,RuslanSalakhutdinov,andDanielFried. Groundinglanguagemodelstoimagesformulti-
modalinputsandoutputs. InInternationalConferenceonMachineLearning,pages17283–17300.PMLR,
2023.
[25] JonathanKrause,MichaelStark,JiaDeng,andLiFei-Fei. 3dobjectrepresentationsforfine-grained
categorization. InProceedingsoftheIEEEinternationalconferenceoncomputervisionworkshops,pages
554–561,2013.
[26] RanjayKrishna,YukeZhu,OliverGroth,JustinJohnson,KenjiHata,JoshuaKravitz,StephanieChen,
YannisKalantidis,Li-JiaLi,DavidAShamma,etal. Visualgenome: Connectinglanguageandvision
usingcrowdsourceddenseimageannotations. Internationaljournalofcomputervision,123:32–73,2017.
[27] AlexKrizhevsky,GeoffreyHinton,etal. Learningmultiplelayersoffeaturesfromtinyimages. 2009.
[28] JieLei,LinjieLi,LuoweiZhou,ZheGan,TamaraLBerg,MohitBansal,andJingjingLiu. Lessismore:
Clipbertforvideo-and-languagelearningviasparsesampling. InProceedingsoftheIEEE/CVFconference
oncomputervisionandpatternrecognition,pages7331–7341,2021.
[29] ChunyuanLi,HaotianLiu,LiunianLi,PengchuanZhang,JyotiAneja,JianweiYang,PingJin,Houdong
Hu,ZichengLiu,YongJaeLee,etal.Elevater:Abenchmarkandtoolkitforevaluatinglanguage-augmented
visualmodels. AdvancesinNeuralInformationProcessingSystems,35:9287–9301,2022.
11[30] JunnanLi,DongxuLi,CaimingXiong,andStevenHoi. Blip:Bootstrappinglanguage-imagepre-training
forunifiedvision-languageunderstandingandgeneration.InInternationalconferenceonmachinelearning,
pages12888–12900.PMLR,2022.
[31] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,PiotrDollár,
and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on
ComputerVision,pages740–755.Springer,2014.
[32] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.Visualinstructiontuning.Advancesinneural
informationprocessingsystems,36,2024.
[33] JiasenLu,ChristopherClark,RowanZellers,RoozbehMottaghi,andAniruddhaKembhavi. Unified-io:A
unifiedmodelforvision,language,andmulti-modaltasks. InTheEleventhInternationalConferenceon
LearningRepresentations,2022.
[34] SubhransuMaji,EsaRahtu,JuhoKannala,MatthewBlaschko,andAndreaVedaldi. Fine-grainedvisual
classificationofaircraft. arXivpreprintarXiv:1306.5151,2013.
[35] NiklasMuennighoff, HongjinSu, LiangWang, NanYang, FuruWei, TaoYu, AmanpreetSingh, and
DouweKiela. Generativerepresentationalinstructiontuning. arXivpreprintarXiv:2402.09906,2024.
[36] Maria-ElenaNilsbackandAndrewZisserman. Automatedflowerclassificationoveralargenumberof
classes. In2008SixthIndianConferenceonComputerVision, Graphics&ImageProcessing, pages
722–729.IEEE,2008.
[37] VicenteOrdonez,GirishKulkarni,andTamaraBerg.Im2text:Describingimagesusing1millioncaptioned
photographs. Advancesinneuralinformationprocessingsystems,24,2011.
[38] OmkarM.Parkhi,AndreaVedaldi,AndrewZisserman,andC.V.Jawahar. Catsanddogs. In2012IEEE
ConferenceonComputerVisionandPatternRecognition,pages3498–3505.IEEEComputerSociety,
2012.
[39] BryanAPlummer,LiweiWang,ChrisMCervantes,JuanCCaicedo,JuliaHockenmaier,andSvetlana
Lazebnik. Flickr30kentities:Collectingregion-to-phrasecorrespondencesforricherimage-to-sentence
models. InProceedingsoftheIEEEinternationalconferenceoncomputervision,pages2641–2649,2015.
[40] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisualmodelsfrom
naturallanguagesupervision. InInternationalconferenceonmachinelearning,pages8748–8763.PMLR,
2021.
[41] ChristophSchuhmann,RomainBeaumont,RichardVencu,CadeGordon,RossWightman,MehdiCherti,
TheoCoombes,AarushKatta,ClaytonMullis,MitchellWortsman,etal. Laion-5b:Anopenlarge-scale
dataset for training next generation image-text models. Advances in Neural Information Processing
Systems,35:25278–25294,2022.
[42] ChristophSchuhmann,RichardVencu,RomainBeaumont,RobertKaczmarczyk,ClaytonMullis,Aarush
Katta,TheoCoombes,JeniaJitsev,andAranKomatsuzaki. Laion-400m:Opendatasetofclip-filtered400
millionimage-textpairs. arXivpreprintarXiv:2111.02114,2021.
[43] PiyushSharma, NanDing, SebastianGoodman, andRaduSoricut. Conceptualcaptions: Acleaned,
hypernymed,imagealt-textdatasetforautomaticimagecaptioning. InProceedingsofthe56thAnnual
MeetingoftheAssociationforComputationalLinguistics,pages2556–2565,2018.
[44] JohannesStallkamp,MarcSchlipsing,JanSalmen,andChristianIgel. Thegermantrafficsignrecognition
benchmark:Amulti-classclassificationcompetition. InIJCNN2011,pages1453–1460.IEEE,2011.
[45] JackUrbanek, FlorianBordes, PietroAstolfi, MaryWilliamson, VasuSharma, andAdrianaRomero-
Soriano. Apictureisworthmorethan77texttokens: Evaluatingclip-stylemodelsondensecaptions,
2023.
[46] BastiaanSVeeling,JasperLinmans,JimWinkens,TacoCohen,andMaxWelling. Rotationequivariant
cnnsfordigitalpathology.InInternationalConferenceonMedicalimagecomputingandcomputer-assisted
intervention,pages210–218.Springer,2018.
[47] JianfengWang, ZhengyuanYang, XiaoweiHu, LinjieLi, KevinLin, ZheGan, ZichengLiu, CeLiu,
andLijuanWang. Git: Agenerativeimage-to-texttransformerforvisionandlanguage. arXivpreprint
arXiv:2205.14100,2022.
12[48] HongweiXue,TiankaiHang,YanhongZeng,YuchongSun,BeiLiu,HuanYang,JianlongFu,andBaining
Guo. Advancinghigh-resolutionvideo-languagerepresentationwithlarge-scalevideotranscriptions. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages5036–5045,
2022.
[49] HongweiXue,YupanHuang,BeiLiu,HouwenPeng,JianlongFu,HouqiangLi,andJieboLuo. Probing
inter-modality:Visualparsingwithself-attentionforvision-and-languagepre-training. AdvancesinNeural
InformationProcessingSystems,34:4514–4528,2021.
[50] HongweiXue,YuchongSun,BeiLiu,JianlongFu,RuihuaSong,HouqiangLi,andJieboLuo. Clip-
vip:Adaptingpre-trainedimage-textmodeltovideo-languagerepresentationalignment. arXivpreprint
arXiv:2209.06430,2022.
[51] YangYou,JingLi,SashankReddi,JonathanHseu,SanjivKumar,SrinadhBhojanapalli,XiaodanSong,
JamesDemmel,KurtKeutzer,andCho-JuiHsieh. Largebatchoptimizationfordeeplearning:Training
bertin76minutes. InInternationalConferenceonLearningRepresentations,2019.
[52] JiahuiYu,ZiruiWang,VijayVasudevan,LeggYeung,MojtabaSeyedhosseini,andYonghuiWu. Coca:
Contrastivecaptionersareimage-textfoundationmodels. arXivpreprintarXiv:2205.01917,2022.
[53] PeiyuanZhang,GuangtaoZeng,TianduoWang,andWeiLu. Tinyllama:Anopen-sourcesmalllanguage
model,2024.
[54] LuoweiZhou,HamidPalangi,LeiZhang,HoudongHu,JasonCorso,andJianfengGao. Unifiedvision-
languagepre-trainingforimagecaptioningandvqa. InProceedingsoftheAAAIconferenceonartificial
intelligence,volume34,pages13041–13049,2020.
[55] DeyaoZhu,JunChen,XiaoqianShen,XiangLi,andMohamedElhoseiny. Minigpt-4:Enhancingvision-
languageunderstandingwithadvancedlargelanguagemodels. InTheTwelfthInternationalConferenceon
LearningRepresentations,2023.
13