X-VILA: Cross-Modality Alignment for
Large Language Model
HanrongYe1,2∗,De-AnHuang1,YaoLu1,ZhidingYu1,WeiPing1,AndrewTao1,
JanKautz1,SongHan1,3,DanXu2,PavloMolchanov1,HongxuYin1
NVIDIA1 HKUST2 MIT3
Image-Language Language-Video Video-Audio Video-Image
Prompt: What is in the image? Prompt: Can you make a Prompt: Can you make some Prompt: Can you make an
similar video with people in it? sound for the video? image based on the video?
X-VILA: A video with people X-VILA: This is the requested X-VILA: Here is the image.
enjoying the beach.
audio.
X-VILA: The image shows a (Sound of waves)
beach with a sandy shore.
Video-Language Video-Video Image+Audio-Video Audio-Video
Prompt: What is in the video? Prompt: What do you think Prompt: Can you make a video Prompt: Generate a video
is going to happen? from the image and audio? following the audio.
X-VILA: The man starts to ski. (Girls laughing)
X-VILA:
(Sound of sea)
X-VILA:
X-VILA: A man is seen speaking
to the camera while holding a ski.
Image-Language Video-Language Image+Video-Video Video-Audio
Prompt: What is in the image? Prompt: What about this Prompt: Can you make a Prompt: Can you make
video? What is shown in it? video combining them? some music for the video?
X-VILA: Here is the video. X-VILA: Here is the audio.
(Girl speaking)
X-VILA: A large field of X-VILA: The video shows a
colorful flowers in a park. young girl standing in a field.
Figure1: WeintroduceX-VILA,afoundationmodelforcross-modalityunderstanding,reasoning,
andgenerationinthedomainsofvideo,image,language,andaudio. X-VILAdemonstratesthe
abilitytoperceive(see,hear,andread)multi-modalityinputs,andgenerate(draw,speak,andwrite)
multi-modalityresponses. Conversationsarecontinuouswithineachgreenbox. Bestviewedincolor.
Abstract
WeintroduceX-VILA,anomni-modalitymodeldesignedtoextendthecapabili-
tiesoflargelanguagemodels(LLMs)byincorporatingimage,video,andaudio
modalities. Byaligningmodality-specificencoderswithLLMinputsanddiffusion
decoderswithLLMoutputs,X-VILAachievescross-modalityunderstanding,rea-
soning,andgeneration. Tofacilitatethiscross-modalityalignment,wecuratean
effectiveinterleavedany-to-anymodalityinstruction-followingdataset. Further-
more,weidentifyasignificantproblemwiththecurrentcross-modalityalignment
method,whichresultsinvisualinformationloss. Toaddresstheissue,wepropose
avisualalignmentmechanismwithavisualembeddinghighwaymodule. Wethen
∗WorkdoneduringaninternshipatNVIDIA.
Preprint.
4202
yaM
92
]VC.sc[
1v53391.5042:viXraintroducearesource-efficientrecipefortrainingX-VILA,thatexhibitsproficiency
in any-to-any modality conversation, surpassing previous approaches by large
margins. X-VILAalsoshowcasesemergentpropertiesacrossmodalitiesevenin
theabsenceofsimilartrainingdata. Theprojectwillbemadeopen-source.
1 Introduction
Largelanguagemodels(LLMs)provideanemergingfoundationforenhancingvariousdeeplearning
tasksbeyondtherealmofnaturallanguageprocessing. Asanexample, researchcommunityhas
beenquicklyextendingthefastprogressofLLMs[1,2,3,4,5,6,7,8,9,10,11,12,13]towardsthe
computervision(CV)domain[14,15,16,17,18,19,20,21,22]. TheintroductionofLLMsinCV
tasksenablesvisionmodelstoperformmanyzero/few-shotandin-contextlearningtasksthatare
“promptable”throughuserquestions,potentiallyempoweringreasoningcapabilitiesforthefirsttime.
Despiteremarkableprogress,cross-modalityalignmentisstillachallengingtaskasthejointtraining
stageforcross-modalitylearningrequirescarefullydesignedfeedbacksignal[23,24]toguidethe
connectedfoundationmodels[15,14,18],backedbycross-modalitydatasetsatscale[25,26,27].
Hence, themajorityofexistingstudiesrevolvearoundasolitaryinputmodalitylinkedtoLLMs,
withtheoutputbeingsolelytext. Forexample,Flamingo[15],LLaVA[14],andVILA[28]delve
intoimageinput,whileVideo-LLaMA[29]andLITA[30]specificallyconcentratesonvideoinput.
Exploringtheintegrationofvariousmodalitiesintoacohesiveframeworkisacrucialyetrelatively
unexploredresearchchallenge[31,32,33]inthedomainofmulti-modalityLLMs, yetobserved
practicalinproprietaryGPT-4o[21].
Thisstudyfocusesonthedevelopmentofasystematicapproachtointegratemultiplemodalities,
suchasvideo,image,andaudio,intoanLLMatboththeinputandoutputstages. Ourobjectiveis
tofacilitatecross-modalconversationsinanany-to-anymodality(or“X-to-X”)LLM,allowingfor
generationindifferentmodalities. Toaccomplishtheambitiousobjective,wepresentatwo-phase
alignmentmechanism: (i)Textualalignment. Wealigninputandoutputrepresentationsofdifferent
modalities to the textual embedding space of the LLM [32]. Specifically, in regard to the input
ofLLM,weuseaunifiedembeddingspacethatallowsforthesharingoffeaturesextractedfrom
encodersacrossdiversemodalities. AsfortheoutputofLLM,weemployfine-tunablemodality-
specificdiffusionmodelstoconvertthegeneratedoutputsoftheLLMintocontentthatalignswith
therespectivemodalities. (ii)Visualalignment. Weobservethattheprevioustextualalignment
alonefailstopreservevisualfeaturesadequatelyinvision-to-visiongenerationtasks,suchasimage-
to-videoandvideo-to-imagegeneration. Thislimitationcanbeattributedtothelossofinformation
duringtheprojectionprocessfromvisualencoderstotheLLM,aswellastheLLM’stendencyto
prioritizecommonconceptsoverspecificvisualdetails. Toaddressthisissue,weproposeanew
modulenamedVisualEmbeddingHighway(VEH).TheVEHmodulefacilitatesthedirectguidance
ofvisualdecodersbyenablingvisualfeaturestobypasstheLLM.ByincorporatingVEH,wehave
observedanotableenhancementinthecorrespondenceofvisualcontentbetweentheinputandoutput
stagesofourframework.
On the other hand, the scarcity of cross-modality instruction-following data poses a significant
challengeinthedevelopmentofany-to-anymodality(or“X-to-X”)LLMs. Thislimitationseverely
restrictstheprogressincreatingLLMsthatcanseamlesslyhandlemultiplemodalitiesinbothinput
andoutputends. Existingdatasetsprovidelimiteddata,mostlyintheformofX-to-textortext-to-X.
Therefore, we curate a new X-to-X dataset based on multi-modality data from WebVid [34] and
ActivityNetCaptions[35]tofacilitatecross-modalityinteractionsbetweentext,audio,image,and
video. Overall,wesynthesizemorethan1.5Mmulti-modalityconversations,witheachconversation
containingatleastonecross-modalityquestion-and-answerpair.
Toachievethecross-modalityinput-outputalignmentofLLMsinourX-to-XLLM,wedesignthree
majortrainingphases: (i)Adata-effectivealignmentphasethatinvolvesaligningthemulti-modality
encoders with the LLM inputs and the multi-modality decoders with the LLM outputs. (ii) An
interleaved multi-modality pre-training phase with interleaved instruction data across modalities
forenhancedin-contextlearningperformance. (iii)AnX-to-Xcross-modalityinstructiontuning
phasethatincludesatwo-stepalignmentprocess: textualalignmentandvisualalignmentmechanism.
Throughourinnovativeapproachtomulti-modalityalignment,webuildapowerfulX-to-Xmulti-
modalityLLMwiththeabilitytocomprehendandgeneratemulti-modalitycontent. Wetermour
newmodel“X-VILA”forcross-modalityunderstanding,reasoning,andgenerationinthedomains
2Multi-Modality Input Multi-Modality Output
Textual Embedding Space Textual Embedding Space
... Language Tokernizer ... LLM ... Language
Image Image
Visual Embedding Highway
...
Video Video
Audio Textual Alignment Visual Alignment Audio
Figure2: X-VILAschematicdiagram. X-VILAaugmentsapretrainedLLMtowardsnewmodalities
via(i)connectingpretrainedencoderstoLLMinputtextualembeddingspaceand(ii)connecting
pretraineddiffusiondecoderstotheLLMoutputtextualembeddingspace(Section2.1). Thesystem
isjointlytrainedviaanewcross-modalityalignmentprocedure(SectionA).
ofVideo,Image,Language,andAudio. Forinstance,asshowninFigure1andFigure8,X-VILA
demonstrates its capacity to recognize the subjects in the image, which results from our vision-
languagealignmenttraining. Then, itcanretrieveitsknowledgeandmakelogicaldeductionsto
answertheuser’squestionsaboutthecontentintheimage. Lastbutnotleast,itcangeneratealigned
multi-modalityoutputthatmatchesthegivencontext.
Insummary,thisworkmakescontributionsinthreeaspects:
• A new family of any-to-any modality chat LLM that is able to conduct multi-modality
conversationsbyunderstandingsignalsfromdifferentmodalitiesandgeneratingcontentin
variousformats,includingvideo,audio,image,andtext.
• Anovel2-stepalignmentmechanismthateffectivelyalignsbothsemanticandvisualdetails
betweentheinputandoutputspaces. Thismechanismensuresacomprehensiveandaccurate
correspondencebetweentheinputandoutputofourX-to-XLLM.
• The creation of a new X-to-X multi-modality instruction tuning dataset that is proven
effectiveforcross-modalityalignment. Thisdatasetservesasavaluableresourceforfuture
researchintherealmofmulti-modalityfoundationmodels.
2 Methodology
2.1 X-VILAArchitecture
We consider four common modalities in this work: text, image, video, and audio. The tenet of
X-VILAisanalignment-basedarchitecturetoaugmentanLLMwiththeabilityto“see/hear/read”
multi-modalityinputsand“draw/speak/write”multi-modalityoutputs,asshowninFigure2.
Modality-specificencoders. Weadoptmodality-specificencoderstohandleinputsfromdifferent
modalities. Thisstrategyharveststhepre-trainedunderstandingabilityofthedomainexpertencoders
and has been proven successful in many vision-language models [15, 18, 14]. To better align
embeddingsofdifferentmodalities,weuseImageBindencoders[36],whichunifyfeaturesfrom
differentmodalities,includingimage,video,andaudio,intoonefeaturespace. Technically,foreach
modalitym∈{‘text’,‘image’,‘video’,‘audio’},wenotatetheencodersasEnc . Fortextmodality,
m
theencoderisatexttokenizer[37],whileforothermodalitiestheyareImageBindtransformers[36].
Wethenusemodality-specifictrainablelinearlayers,notatedasPin,toprojectEnc outputinto
m m
embeddingsequencesSinthetextualembeddingspaceofthefollowingLLM.Wecanformulatethis
processas:
Sin ={Pin(Enc (X ))}, (1)
m m m
whereX isinputfromdifferentmodalitiesm∈{‘text’,‘image’,‘video’,‘audio’}.
m
3
Encoder
Encoder
Encoder
Proj
Proj
Proj
Proj
Proj
Proj
Decoder
Decoder
Decoder
Diffusion
Diffusion
DiffusionLargelanguagemodel(LLM).LLMservesasthe“brain”ofourframework.Itprocessesinformation
fromthetextualembeddingspaceandpredictslanguageoutputscorrespondingly. WeadoptVicuna-
7B-1.5[8,6], whichdemonstratesstate-of-the-artlanguageunderstandingandgenerationability.
Foreasierunderstanding,weslightlyabusetheannotationandwritetheautoregressiveprocessof
generatingoutputembeddingsequenceSoutbytheLLMas:
Sout =LLM(Sin). (2)
Modality-specificdecoders. Forgeneratingmulti-modalityoutputsotherthantext,weadoptthe
“modality-specific generation tokens” designed by [32]. Other than common text tokens, there
are three types of modality-specific generation tokens: image generation tokens {[IMG ], i ∈
i
[1,N ]},videogenerationtokens{[VID ],i∈[1,N ]},andaudiogenerationtokens{[AUD ],
img i vid i
i ∈ [1,N ]}. N , N , and N are the numbers of generation tokens for image, video,
aud img vid aud
andaudio,respectively. Thesemodality-specificgenerationtokensareaddedtothevocabularyof
LLM.TheLLMistrainedtopredictwhentogeneratethesemodality-specificgenerationtokens,and
thesegenerationtokenswillbetranslatedforthesynthesisofimage,video,oraudio,viaasetof
modality-specificdecoders(i.e.,generationmodels). Technically,weextractthesubsetofoutput
embeddingsequenceSout correspondingtotheaforementionedgenerationtokensofmodalitym. We
namethissubsetthegenerationembeddingsequenceSgen. Weusemodality-specifictransformer
m
layers,denotedasoutputprojectionlayersPout,toprojectSgen tothefeaturespaceoftheoriginal
m m
pre-trainedtextencoderofthemodality-specificdecoder. Astheresultingembeddingwillbeusedto
controlthemodality-specificdecoderviacross-attention,wenametheresultingembeddingvectoras
“textualcontrollerembedding”Etext. Thuswehave:
m
Etext =Pout(Sgen). (3)
m m m
[32]freezesthedecodermodelsandonlysupervisestheEtexttobesimilartotheoriginaltextencoders
m
ofthediffusionmodels. Thisbehaviorlargelylimitsthesynergybetweengenerationmodelsandthe
otherpartsofthemodel,asthelearningtargetisessentiallytomimicthepre-trainedtextencoderof
thediffusionmodels. Differently,weincludethemodality-specificdecodermodelsinfine-tuningto
betteralignthemwiththeLLMandotherpartsoftheunifiedgenerativeframework. Thetraining
detailswillbediscussedinalatersection. Specifically,toachieveabettermulti-modalitygeneration
ability,weemploystate-of-the-artgenerationmodelstrainedonlarge-scaledataasmodality-specific
decoders. WeadoptVideoCrafter2[38]forvideogeneration,StableDiffusion1.5[39]forimage
generation,andAudioLDM[40]foraudiogeneration.
Visual embedding highway. The weakness
of the previously introduced text-space-based
alignmentistheinadequatevisualfeaturesavail- Visual Highway RB RB RB RB RB Visual Controlelr
Embedding Module (VisCtrl)
ableattheoutputend,ascanbeseeninexam- ZC ZC
ZC
plesinFigure4. Intuitively,thisstemsfromthe ZC ZC
one-to-manycorrespondencebetweentextand U-Net of Visual
Decoder
visualsemanticspaces,e.glet@tokeneonedot,“a
dress” may relate to images varying in colors Latent
andstyles.
Textual Controller
Toaddressthisissue,weproposeavisualembed-
Embedding
dinghighwaythatbridgesthevisualencoders
Add Cross-Attention ZC Zero-Convolution RB Residual Block
anddecoders,builttoalleviatetheinformation
Figure3:Illustrationoftheproposedvisualembed-
loss when projecting high-dimensional visual
dinghighwayinX-VILA.Thevisualhighwayem-
contenttothetextualembeddingspace. Specif-
beddingEvis isobtainedfromthevisualencoder.
ically, we obtain the layer-wise feature maps
Thedesignincorporatesavisualcontrollermodule
fromtheImageBindvisualencoderandaddup
thesefeaturesasvisualhighwayembeddingEvis.
responsibleforprocessingEvisandgeneratingcon-
Evis hasshapeH ×W ×C,whereH andW trolsignals. Thesesignalsarethenincorporated
intovariouslayersoftheU-Netinvisualdecoders.
areheightandwidthofthefeaturemaps,C is
Etext is“textualcontrollerembedding”, whichis
theembeddingvector. Tocontrolthedecoder m
usingEvis,wedesignalight-weightvisualcon-
thesubsetofoutputembeddingsequenceSoutcor-
responding to the generation tokens of modality
troller (VisCtrl) module based on the philoso-
phyof[41,42]toprocessEvis. Thecontroller m. z(t) is the latent at reverse step t. ϵp is the
predictednoisebyU-Net.
module comprises 4 stages, where each stage
consists of two residual convolutional blocks.
4These blocks have cascading spatial dimensions that match the resolution settings in the U-Net
encoder [39] of image/video decoders. In each stage, there is an additional convolutional block
initializedwithzeroweights. Thisblockgeneratesoutputcontrolsignalsforthestage,whichare
initiallyzeroatthestartofthetraining. Thesecontrolsignalsareaddedtodifferentstagesofthe
U-Net,asshowninFigure3. Inspiredby[43],weemployaconditioningrateα∈[0,1]toregulate
theproportionofstepsconditionedbyvisualfeatures. Therefore,thenoisepredictionprocessineach
reversesteptinthevisualdecoderscanbewrittenas:
(cid:26) U-Net (z(t),VisCtrl (Evis),Etext) ift<T ×α
ϵp = m m m ,m∈{‘image’,‘video’}. (4)
U-Net (z(t),Null,Etext) ift≥T ×α
m m
whereϵpisthepredictednoisegiveninputlatentz(t),T isthenumberofdiffusionsteps,U-Net
m
is the U-Net of the diffusion decoder for modality m, and VisCtrl is the visual control module
m
formodalitym.“Null”meansnoVEHfeatureispassedtotheU-Netatthecorrespondingtimestep.
DuringinstructiontuningprocessonX-to-Xdatasets,boththeU-Netandthecontrollermodulesare
fine-tunedtogether. ThismannerensuresabettersynergybetweendecodersandtheLLM.
Theexperimentalresultsintroducedinthelatersectionsshowthattheproposedvisualembedding
highway can significantly increase the consistency between the generation results and the visual
contextofourmulti-modalityunifiedgenerationmodel.
2.2 X-VILATraining
The training process of X-VILA is divided into three phases, namely (i) encoder-LLM-Decoder
alignment training, (ii) interleaved data pre-training, and (iii) X-to-X cross-modality instruction
fine-tuning. WedescribethedetailsofX-VILAtraininginAppendixAduetospacelimit.
3 Experiments
3.1 DatasetsandEvaluation
Setup. Inthiswork,weutilizedifferentdatasetsfordifferenttrainingphases. Forthefirstencoder-
LLM-decoderalignmenttraining,theX-textpairsfromLLaVA-pretrain[14],cc3m[44],WebVid[34],
AudioCaps[45],andWavCaps[46]areutilized. Duringtheinterleaveddatapre-trainingphase,we
constructinterleavedmulti-modalitycorpusfromMMC4[25]andActivityNetCaptions[35].
In the final X-to-X cross-modality instruction
tuning,wecreateanewX-to-Xdatasetfrom
WebVid[34]andActivityNetCaptions. Wesyn-
thesize conversation samples in 6 types based
on the modalities in input and output ends:
video-to-image,video-to-video,image-to-video,
video-to-audio, audio-to-video, image&audio-
to-video. Statistically,forActivityNetCaptions,
wemake10009image-to-video, 10009video-
to-image, and 10009 video-to-video conversa-
tions. For WebVid, we randomly select 500k
training samples and build 499,915 image-to-
video,499,915video-to-image,499,915video-
to-video,32,874audio-to-video,32,874video- Visual Textual TA+VEH
Reference Alignment(TA) (X-VILA)
to-audio,and32874image+audio-to-videocon-
Figure 4: Effectiveness of the proposed visual
versations. Each conversation contains more
embedding highway network. Given the visual
than one pair of cross-modality Q&A pairs.
reference image/video, we prompt the model
SomeconversationexamplesareshowninFig-
with “Please generate an image similar
ure9. WeblendourX-to-XdatasetwithSFT
to the semantics in the input.” Com-
datasets from LLaVA [14], VideoChat [47],
pared to textual alignment only (TA), our visual
NextGPT-instructions[32],andAlpaca[7].
embeddinghighway(VEH)helpspreservevisual
Evaluation. For benchmarking the X-to-X detailsfromthevisualinputs.
alignment ability of different models, we ran-
domly sample a validation subset of WebVid
5Long-Context Cross-Modality Generation
Image-Language Audio-Language Image+Audio-Image Image-Video
Prompt: What is in the animal? Prompt: What is in the audio? Prompt: Can you generate Prompt: Can you make a
an image based on the video based on the image?
image and the audio?
X-VILA: Here is the video.
X-VILA: Here is the image.
(Sound of waves)
X-VILA: The audio is the
X-VILA: It is a dog. sound of waves crashing.
Unseen Cross-Modality Ability
Prompt: Can you show an X-VILA: Checkout this image. Prompt: Generate an audio X-VILA: Checkout this audio.
e image given the audio? ofor the image.
g
a m
id
u A
o id-I e g a-
u m
A (Baby laughing) I (Bird calling)
Figure 5: We observe emergent abilities of X-VILA without training on similar data: (i) Long-
contextcross-modalitygenerationability. Combinemultipleinputsfromdifferentmodalitiesand
generateconsistentcontent. (ii)Newtypesofcross-modalityability. Conductimage-to-audioand
audio-to-imagegenerationtasks. Conversationsarecontinuousleft-to-rightwithineachgreenbox.
Method VID2IMG VID2VID IMG2VID
Next-GPT[32] 27.85 10.47 13.08
X-VILAw/X2Xtext 36.09 46.18 45.93
X-VILAw/X2Xtext+VEH(img) 44.06 46.68 45.94
X-VILAw/X2Xtext+VEH(img+vid) 43.95 49.76 48.81
Table1: X2AscoresonActivityNetCaptions. “w/X2Xtext”denotesusingourX-to-Xdatasetfor
textualalignmentonly. “VEH(img)”denotesusingtheproposedvisualembeddinghighway(VEH)
forimagedecoder,while“VEH(img+vid)”denotesusingVEHforbothimageandvideodecoders.
WeobservethatimagegenerationtaskissignificantlyimprovedafterusingVEH(img),andthevideo
generationtasksareboostedafterusingVEHonvideodecoder.
andActivityNetCaptionstobuildthecross-modalityconversationsforevaluation. Specifically,for
ActivityNetCaptionswegenerate100video-to-image,100video-to-video,and100image-to-video
conversations. ForWebVid,wecurate100video-to-image,100image-to-video,100video-to-video,
62audio-to-video,62image+Audio-to-video,and62audio-to-videoconversationsforevaluation.
Inordertoevaluatethesimilaritybetweenground-truthannotationsandmodelpredictionsacross
differentmodalities,weintroduceametriccalledthe“X-to-XAlignmentScore(X2AScore)”. To
computethisscore,weemploytheImageBindtransformer[36]toextractembeddingvectorsfrom
theaudio,video,andimagepredictionsaswellasthecorrespondinggroundtruths. Wethencalculate
thecosinesimilarityscoresbetweenthesevectors. Theresultingscoresarepresentedaspercentages,
rangingfrom0to100. Finally,weaveragethescoresacrossallvalidationsamplestoobtaintheX2A
scoresforeachtypeofdata.
Baselinemethods. WeconductacomparisonbetweenourmodelandNext-GPT[32],arecently
introducedinstruction-followingLLMdesignedformulti-modalityunderstandingandgeneration.
Theirmethodisrestrictedtotextualalignmentexclusively.
3.2 QuantitativeAnalysisandAblationStudy
Effectiveness of Visual Embedding Highway. We compute the aforementioned X2A scores of
differentmodelsontheX-to-XalignmentbenchmarksofbothActivityNetCaptionsandWebVid
datasets,andpresenttheresultsinTable1andTable2. Specifically,westudytheX2AscoresofNext-
GPTanddifferentversionsofourX-VILAmodel.Weinvestigatetheperformanceofourmodelunder
differentscenarios: (i)utilizingonlytextualalignment,(ii)incorporatingvisualalignmentthrough
6Method VID2IMG IMGAUD2VID VID2AUD IMG2VID VID2VID AUD2VID
Next-GPT[32] 15.31 44.63 8.17 38.23 31.81 37.13
X-VILAw/X2Xtext 53.82 49.54 22.79 42.94 44.42 42.23
X-VILAw/X2Xtext+VEH(img) 67.40 48.64 23.53 42.66 43.04 42.04
X-VILAw/X2Xtext+VEH(img+vid) 67.94 59.71 23.87 57.01 57.39 49.44
Table 2: X2A scores on WebVid. “w/ X2X text” denotes using our X-to-X dataset for textual
alignmentonly. “VEH(img)”denotesusingtheproposedvisualembeddinghighway(VEH)for
imagedecoder,while“VEH(img+vid)”denotesusingVEHforbothimageandvideodecoders. The
effectivenessofvisualembeddinghighwayissolidforimageandvideogeneration.
Ground-Truth CoDi (Tang et al, 2023) Next-GPT (Wu et al.,2023) GPT-4o (2024) X-VILA (ours)
Figure 6: Visual comparison to the recent any-to-any modality LLMs including Next-GPT [32],
CoDi[31],andGPT-4o[48]onthecross-modalityalignmenttasktogenerateavideosimilartothe
inputimagecontext. X-VILAdemonstratesgoodgenerationqualityandbettervisualcross-modality
consistency. GPT-4oisonlyabletogenerateimagesbutnotvideos.
theproposedvisualembeddinghighway(VEH)ontheimagedecoder,and(iii)extendingVEHto
boththeimageandvideodecoders. Ourfindingsindicatethatevenbyutilizingtextualalignment
alonewithourcarefullycuratedX-to-Xdatasets,ourmodeldemonstratesasubstantialperformance
advantageoverNext-GPT.Moreover,asweprogressivelyintroducethevisualembeddinghighway
to the image and video decoders, we observe consistent and significant improvements in visual
understandingandgenerationtasks. Insummary,ourX-VILAdemonstratessignificantlystronger
cross-modalityunderstanding,reasoning,andgenerationabilityonalltypesofconversationdata.
TheseresultssuggesttheeffectivenessofourX-to-Xalignmentstrategyandtheproposedvisual
embedding highway design. Notably, both Next-GPT and X-VILA are based on the ImageBind
model,makingitfairtouseImageBindscoresforbothmodels.
Influence of conditioning rates. We present
theX2Ascoresplottedwithvaryingcondition- Method VQAv2 VisWiz MMMU-val
ing rates α (Equation 4) in VEH (image), as BLIP-213B[49] 65.0 19.6 -
depictedinFigure7. Ourobservationsindicate InstructBLIP13B[24] - 33.4 -
thatanincreaseinα,correspondingtomorere- Qwen-VL-Chat7B[20] 78.2 38.9 35.9
versestepsexposedtoVEHfeaturesduringim- LLaVA1.57B[50] 78.5 50.0 36.4
agesampling,leadstoimprovedmulti-modality X-VILA7B 72.9 50.9 33.9
alignment. Thisoutcomealignswithourintu-
itiveexpectations. Table3: X-VILAdemonstratescomparableperfor-
mancetodomainexpertswhenevaluatedonextra
Extramulti-modalitybenchmarks. Tofurther
multi-modalitybenchmarks.
evaluatethemulti-modalityunderstandingcapa-
bilitiesofX-VILA,weperformzero-shotexperimentsonseveralmulti-modalityVQAbenchmarks,
includingVQAv2[51],VisWiz[52],andMMMU-val[53]. TheresultsinTable3indicatethatX-
VILAiscompetitivewiththeleadingdomain-expertVLMs,whilepossessingtheX-to-Xcapability.
7Ground-Truth Te Dx et- cA ol dig in ne gd Text-E Dm eb coed d- inA gligned T De ext c- oE dm inb ge d w- iA thli g Vn Ee Hd
ActivityNet VID2IMG WebVid VID2IMG
68.0
44.0 66.0
64.0
42.0
62.0
40.0 60.0
Video-to-Image
58.0
38.0
56.0
36.0 54.0
52.0
0 0.25 0.5 0.75 1.0 0 0.25 0.5 0.75 1.0
Image-to-Video
Figure 7: (left, middle) Study of using different conditioning rates in VEH (image). Higher
conditioning rates brings generally better X-to-X alignment. (right) An in-depth comparison of
varyingdesignchoicesofX-VILAoncross-modalityalignmenttasks. WeobservethatbothText-
AlignedDecodingandText-Embed-AlignedDecodingfallshortineffectivelycapturingsemantic
details from visual inputs. However, with the incorporation of our Visual Embedding Highway
(VEH),wewitnessasubstantialimprovementinvisualconsistency.
3.3 QualitativeAnalysisandAblationStudy
QualitativeX-to-Xalignmentmeasurement. Weprovideaqualitativecomparisontothestate-
of-the-artany-to-anyLLMs,namelyNext-GPT[32],CoDi[31],andGPT-4o[48]onvisualcross-
modalityalignmenttasksinFigure6. Weassesstheirperformancebysupplyinganimagetothe
modelsandprompting“Pleasegenerateavideo(oranimageinthecaseofGPT-4owhichcannot
generatevideo)similartothesemanticsintheinput.”X-VILAdemonstratessignificantimprovements
invisualcorrespondenceoverpreviousmethods,thankstotheintegrationoftheVisualEmbedding
Highway(VEH)intooutputdiffusionmodels.
EmergentX-to-Xability. Duringourexperiments,weobservehighlypromisingemergentabilities
displayedbyX-VILAfollowingitstrainingonourX-to-Xdatasets. AsdepictedinFigure5,wehave
identifiedtwokeycapabilitiesthathavesurfaced:
(i)Long-contextcross-modalitygeneration. X-VILAexhibitsanimpressivecapacityforcompre-
hendingandcombiningdiverseconceptsfrommultipleiterationsofinput. Consequently,itproduces
naturalandcoherentoutput,assuggestedbytheusers.
(ii)Unseencross-modalityability. Remarkably,X-VILAshowcasestheabilitytoperformimage-
to-audio and audio-to-image tasks without any explicit training on similar data. This newfound
competenceemergesorganicallythroughthemodel’sexposuretoourcomprehensiveX-to-Xdataset.
These remarkable emergent abilities underscore the efficacy of our meticulously curated X-to-X
dataset. Not only does it enable the model to excel in the specified data types as suggested in
Section3.2,butitalsofacilitatesgeneralizationacrossawiderangeofmulti-modalityinteractions
betweenusersandthemodel.
Moreinsightsonvaryingdesignchoicesondecoderalignment. Wenextpresentourfindings
whenaligningLLMoutputendtothemodality-specificdecoders. Westudydifferentwaystobridge
LLMoutputandthediffusionmodels: (i)“Text-AlignedDecoding”: LLMgeneratestextdescription
fortheexpectedimage/video/audiopredictionsandthenfeedsthetextdescriptionintopre-trained
image/video/audiodecoders. (ii)“Text-Embed-AlignedDecoding”: LLMgeneratesmodality-specific
generationtokensandthenweusethecorrespondinghigh-dimensionaltextualembeddingstocontrol
themodality-specificdecoders(asdescribedinSection2.1). (iii)“Text-Embed-AlignedDecoding
withVEH”:Buildinguponmethod(ii),weintroducetheVisualEmbeddingHighway(VEH)toalign
thevisualfeaturebetweenencodersanddecoders. Weconductexperimentsonvideo-to-imageand
image-to-videocross-modalityalignmenttasks,andshowtheresultsontherightsideofFigure7.
The findings suggest that conveying specific details such as visual style, object appearance, and
precisehumanactionsfromtheinputtotheoutputischallengingforText-AlignedDecoding. This
difficultyarisesduetothelow-dimensionalnatureofpuretextdescriptions,whichlimitstheamountof
informationtheycancontain. Ontheotherhand,Text-Embed-AlignedDecodingoffersasignificantly
greater“bandwidth”inthetextualembeddingspacebetweentheLLMandmodality-specificdecoders.
Consequently,Text-Embed-AlignedDecodingiscapableofgeneratingmoreconsistentoutcomes.
8Prompt: Hey X-VILA, I am temporarily moving to Seattle for Prompt: I see. I am there for leisure. Could you please Prompt: I heard that Seattle is a dream city for hiking.
the summer, do you know any nice place in Seattle? display a captivating image featuring one renowned What is the most famous mountain in Seattle? Can you
X-VILA: Absolutely! Seattle is dish of Seattle? show a video of it?
a city full of rich history and X-VILA: Here's an image X-VILA: The most
cultural gems. One of the showcasing one of the famous mountain in
most famous places in most iconic dishes Seattle is
Seattle is the Space Needle. associated with Seattle: undoubtedly Mount
It's an iconic structure that salmon bakes. This Rainier. It's the
offers breathtaking views of delectable dish is made highest peak in the
the city and the surrounding with perfectly cooked Cascades and offers
area. The needle was built for salmon fillets that are breathtaking views
the 1962 World's Fair and has generously topped with a of the surrounding
become an beloved symbol savory brown sugar and landscape. Here's a
of Seattle. Another famous rustic spice blend. video showcasing
place to visit is the Pike Place Served with a side of wild the majestic beauty
Market, a bustling hub of rice and steamed of Mount Rainier,
activity with various shops, broccoli, it's a true one of the most
restaurants, and cafes. Enjoy representation of the popular peaks in the
your new home in Seattle! city's culinary heritage. United States.
Figure8: ExamplesofX-VILAperformingamulti-turnany-to-anymodalityconversation. Prompts
aregivenlefttorightinamulti-roundmanner. Bestviewedincolor.
Nevertheless, Text-Embed-Aligned Decoding alone is still not good enough for capturing visual
details, asasubstantialamountofvisualinformationislostduringtheprojectionfromencoders
totheLLM.ThisiswhereourVisualEmbeddingHighwaydemonstratesitsperformanceandaids
X-VILAinattainingnotablyenhancedvisualconsistency.
Conversationexamples.Tothoroughlyinvestigatetheperformanceofourany-to-anymodalityLLM,
weconductedextensivetestingonX-VILAexaminingmanyusecases. Wepresentconversation
examples of X-VILA across varying tasks in Figure 1 and Figure 8. It can be observed that X-
VILAprovidesuserswithacomprehensivesetofmulti-modalityresponsesleveragingtheencoders
for perception, LLM for understanding and reasoning, and decoders for multi-modality content
generation. AsshowninFigure14,X-VILAnotonlyexhibitsitsunderstandingofthevisualinput,
includingthesceneandobjects,butalsopredictstheactionsofthepersondepictedintheimage.
ThiscapabilityisaresultoftrainingonourextensiveX-to-Xdataset. Basedonthevisualinput,
itgeneratesoutputsvisuallyconsistentwiththeinput,e.g. thesnowmountainandredskisuitare
presentedinthegenerationoutputcorrectly.
4 RelatedWork
The era of Large Language Models (LLM) arguably started with the introduction of transform-
ers [54] and a series of works that scaled them. Particularly, OpenAI introduced the Generative
Pre-trained Transformer (GPT) models [55], [56], from GPT-2 (1.5B parameters) to GPT-4 [21]
(1.76T), and showed that parameter scaling, together with more high-quality data, can generate
coherentandcontextuallyrelevanttextacrossvariousdomains. BERT[1]introducedaparadigmof
bidirectionaltextprocessingenablingstrongercontextunderstandingandboostedquestionanswering.
T5[2]convertedlanguageproblemintoatext-to-textformatadvancingtranslationandsummarizing.
Transformer-XL[3]demonstratedthecapabilityofextendingthecontextwindowallowingfora
betterunderstandingoflongertext. TheapplicationeraofLLMwaskickstartedbyChatGPT[4]
whichshowcasedtheunprecedentedabilityofLLMchatbots.
CurrentVision-LanguageModels(VLM)benefitedfromthedevelopmentofViT[57]thatoffersauni-
fiedwayforvisionmodelstocommunicatewithothertransformersfromdifferentmodalities. Rapid
progresshasbeenshowninthreestreams[58]: (i)textuallypromptedmodelsthatacceptimageand
textasinput(CLIP[59],Frozen[60],BLIP[18],PaLI[17],LLaVa[14],VILA[28],miniGPT4[22]);
(ii)visuallypromptedmodels(CLIPSeg[61],SAM[62]);and(iii)multi-modalinput-outputmodels
(Painter[63],ImageBind[36],Palm-E[64],VideoChatGPT[65],RegionGPT[66],mPLUG-owl[67],
PandaGPT[68],CoDi[31],NextGPT[32],Unified-IO[33,69]).Amongthefirst,Frozen[60]demon-
stratedthatVLMcanbeconstructedbylinearprojectionofViTfeaturesintoLLMandonlytuning
ViT on image-text captioning data. They are the first that discover the few-shot capabilities of
VLMwithoutinstruction. Flamingo[15]usedcross-attentionforvisionlanguagebinding,andfora
firsttimedemonstratedsurpassingstate-of-the-artfinetunedmodelsformultipletasks. PALI[17]
createdauniversalmodelthatcandovisionandlanguagetasksseparately,theyscaledViTto4Band
demonstratedtheimportanceofaddinglanguage-onlydatatothepretrainingstage. Overall,VLM
followsthepipelineoftakingapretrainedLLM;addingapretrainedvisionencoder;learningfeature
alignmentatscaleviaaprojectororcross-attention;followedbyinstruct-tuning(InstructBLIP[24],
FLAN [23]). In close relation to our research, Next-GPT introduces an LLM that possesses the
9capabilitytocomprehendmulti-modalityinputsandgeneratecorrespondingmulti-modalityoutputs
throughtextualalignment,yetitcannoteffectivelyhandlevisualdetailspresentintheinput.
5 Conclusion
This paper presents X-VILA, an any-to-any modality LLM that is able to understand, infer, and
generatemulti-modalitycontents. Thisabilityisachievedthroughany-to-anymodalityalignment,
for which we curate a dataset for any-to-any modality instruction tuning. We further identify a
significantdrawbackintheprevioustextualalignmentmethodthatleadstothelossofcrucialvisual
details. Accordingly, we propose an innovative visual alignment mechanism that incorporates a
visualfeaturehighwaymodule. Thissolutionhelpspreserveessentialvisualdetailsfromtheinput.
Theexperimentalresults,bothquantitativeandqualitative,indicatetheeffectivenessofourdataand
methodology. X-VILA’sperformancecanbefurtherenhancedacrossvariousVLMbenchmarks.
References
[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training
ofdeepbidirectionaltransformersforlanguageunderstanding. InNAACL-HLT2019,pages
4171–4186.AssociationforComputationalLinguistics,2019.
[2] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,
YanqiZhou,WeiLi,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunified
text-to-texttransformer. TheJournalofMachineLearningResearch,21(1):5485–5551,2020.
[3] ZihangDai,ZhilinYang,YimingYang,JaimeCarbonell,QuocVLe,andRuslanSalakhutdinov.
Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint
arXiv:1901.02860,2019.
[4] OpenAI. ChatGPT:Optimizinglanguagemodelsfordialogue. https://openai.com/blog/
chatgpt,2023. Accessed: 2023.
[5] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timo-
théeLacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Open
andefficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.
[6] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Open
foundationandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
[7] RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,XuechenLi,CarlosGuestrin,Percy
Liang,andTatsunoriB.Hashimoto. Stanfordalpaca: Aninstruction-followingllamamodel.
https://github.com/tatsu-lab/stanford_alpaca,2023.
[8] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,
SiyuanZhuang,YonghaoZhuang,JosephE.Gonzalez,IonStoica,andEricP.Xing. Vicuna:
Anopen-sourcechatbotimpressinggpt-4with90%*chatgptquality,March2023.
[9] SiddharthKaramcheti,LaurelOrr,JasonBolton,TianyiZhang,KaranGoel,AvanikaNarayan,
RishiBommasani,DeepakNarayanan,TatsunoriHashimoto,DanJurafsky,etal. Mistral–a
journeytowardsreproduciblelanguagemodeltraining,2021.
[10] GuilhermePenedo,QuentinMalartic,DanielHesslow,RuxandraCojocaru,AlessandroCappelli,
HamzaAlobeidli,BaptistePannier,EbtesamAlmazrouei,andJulienLaunay. Therefinedweb
datasetforfalconllm: outperformingcuratedcorporawithwebdata,andwebdataonly. arXiv
preprintarXiv:2306.01116,2023.
[11] AakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,Adam
Roberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal. Palm:
Scalinglanguagemodelingwithpathways. arXivpreprintarXiv:2204.02311,2022.
[12] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li,
JiangchengZhu,JianqunChen,JingChang,etal. Yi: Openfoundationmodelsby01.ai. arXiv
preprintarXiv:2403.04652,2024.
[13] JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,XiaodongDeng,YangFan,Wenbin
Ge,YuHan,FeiHuang,etal. Qwentechnicalreport. Technicalreport,AlibabaGroup,2023.
https://arxiv.org/abs/2303.08774.
10[14] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.Visualinstructiontuning.Advances
inneuralinformationprocessingsystems,36,2024.
[15] Jean-BaptisteAlayrac, JeffDonahue, PaulineLuc, AntoineMiech, IainBarr, YanaHasson,
KarelLenc,ArthurMensch,KatherineMillican,MalcolmReynolds,etal. Flamingo: avisual
languagemodelforfew-shotlearning. AdvancesinNeuralInformationProcessingSystems,
35:23716–23736,2022.
[16] DannyDriess,FeiXia,MehdiSMSajjadi,CoreyLynch,AakankshaChowdhery,BrianIchter,
Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied
multimodallanguagemodel. arXivpreprintarXiv:2303.03378,2023.
[17] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu,
CarlosRiquelmeRuiz,SebastianGoodman,XiaoWang,YiTay,etal. Pali-x: Onscalingupa
multilingualvisionandlanguagemodel. arXivpreprintarXiv:2305.18565,2023.
[18] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-
image pre-training with frozen image encoders and large language models. arXiv preprint
arXiv:2301.12597,2023.
[19] RohanBavishi,ErichElsen,CurtisHawthorne,MaxwellNye,AugustusOdena,ArushiSomani,
andSag˘nakTas¸ırlar. Fuyu-8B:AmultimodalarchitectureforAIagents. https://www.adept.
ai/blog/fuyu-8b,2023.
[20] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,Chang
Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile
abilities. arXivpreprintarXiv:2308.12966,2023.
[21] OpenAI. GPT-4 technical report. Technical report, OpenAI, 2023. https://arxiv.org/
abs/2303.08774.
[22] DeyaoZhu, JunChen, XiaoqianShen, XiangLi, andMohamedElhoseiny. Minigpt-4: En-
hancingvision-languageunderstandingwithadvancedlargelanguagemodels. arXivpreprint
arXiv:2304.10592,2023.
[23] JasonWei,MaartenBosma,VincentYZhao,KelvinGuu,AdamsWeiYu,BrianLester,Nan
Du,AndrewMDai,andQuocVLe. Finetunedlanguagemodelsarezero-shotlearners. arXiv
preprintarXiv:2109.01652,2021.
[24] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuatTiong,JunqiZhao,WeishengWang,
BoyangAlbertLi,PascaleFung,andStevenC.H.Hoi. Instructblip: Towardsgeneral-purpose
vision-languagemodelswithinstructiontuning. ArXiv,abs/2305.06500,2023.
[25] WanrongZhu,JackHessel,AnasAwadalla,SamirYitzhakGadre,JesseDodge,AlexFang,
YoungjaeYu,LudwigSchmidt,WilliamYangWang,andYejinChoi. Multimodalc4: Anopen,
billion-scalecorpusofimagesinterleavedwithtext. arXivpreprintarXiv:2304.06939,2023.
[26] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Sae-
hoon Kim. Coyo-700m: Image-text pair dataset. https://github.com/kakaobrain/
coyo-dataset,2022.
[27] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,
MehdiCherti,TheoCoombes,AarushKatta,ClaytonMullis,MitchellWortsman,etal. Laion-
5b: Anopenlarge-scaledatasetfortrainingnextgenerationimage-textmodels. Advancesin
NeuralInformationProcessingSystems,35:25278–25294,2022.
[28] JiLin,HongxuYin,WeiPing,YaoLu,PavloMolchanov,AndrewTao,HuiziMao,JanKautz,
MohammadShoeybi,andSongHan. Vila: Onpre-trainingforvisuallanguagemodels. CVPR,
2024.
[29] HangZhang,XinLi,andLidongBing.Video-llama:Aninstruction-tunedaudio-visuallanguage
modelforvideounderstanding. arXivpreprintarXiv:2306.02858,2023.
[30] De-AnHuang,ShijiaLiao,SubhashreeRadhakrishnan,HongxuYin,PavloMolchanov,Zhiding
Yu,andJanKautz. Lita: Languageinstructedtemporal-localizationassistant. arXivpreprint
arXiv:2403.19046,2024.
[31] Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. Any-to-any
generation via composable diffusion. In Thirty-seventh Conference on Neural Information
ProcessingSystems,2023.
11[32] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any
multimodalllm. arXivpreprintarXiv:2309.05519,2023.
[33] JiasenLu,ChristopherClark,RowanZellers,RoozbehMottaghi,andAniruddhaKembhavi.
Unified-io: Aunifiedmodelforvision,language,andmulti-modaltasks. InICLR,2022.
[34] MaxBain,ArshaNagrani,GülVarol,andAndrewZisserman. Frozenintime: Ajointvideo
andimageencoderforend-to-endretrieval. InIEEEInternationalConferenceonComputer
Vision,2021.
[35] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-
captioningeventsinvideos. InInternationalConferenceonComputerVision(ICCV),2017.
[36] RohitGirdhar, KalyanVasudevAlwala, ArmandJoulin, andIshanMisra. Imagebind: One
embeddingspacetobindthemall. arXiv: ComputerVisionandPatternRecognition,2023.
[37] TakuKudoandJohnRichardson. Sentencepiece: Asimpleandlanguageindependentsubword
tokenizeranddetokenizerforneuraltextprocessing. arXivpreprintarXiv:1808.06226,2018.
[38] HaoxinChen,YongZhang,XiaodongCun,MenghanXia,XintaoWang,ChaoWeng,andYing
Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models.
arXivpreprintarXiv:2401.09047,2024.
[39] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF
conferenceoncomputervisionandpatternrecognition,pages10684–10695,2022.
[40] HaoheLiu,ZehuaChen,YiYuan,XinhaoMei,XuboLiu,DaniloMandic,WenwuWang,and
MarkDPlumbley. Audioldm: Text-to-audiogenerationwithlatentdiffusionmodels. arXiv
preprintarXiv:2301.12503,2023.
[41] ChongMou,XintaoWang,LiangbinXie,YanzeWu,JianZhang,ZhongangQi,YingShan,and
XiaohuQie.T2i-adapter:Learningadapterstodigoutmorecontrollableabilityfortext-to-image
diffusionmodels. arXivpreprintarXiv:2302.08453,2023.
[42] LvminZhang,AnyiRao,andManeeshAgrawala. Addingconditionalcontroltotext-to-image
diffusionmodels. InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
Vision,pages3836–3847,2023.
[43] GuangxuanXiao,TianweiYin,WilliamTFreeman,FrédoDurand,andSongHan. Fastcom-
poser: Tuning-free multi-subject image generation with localized attention. arXiv preprint
arXiv:2305.10431,2023.
[44] PiyushSharma,NanDing,SebastianGoodman,andRaduSoricut. Conceptualcaptions: A
cleaned,hypernymed,imagealt-textdatasetforautomaticimagecaptioning. InProceedingsof
the56thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: Long
Papers),pages2556–2565,2018.
[45] ChrisDongjooKim,ByeongchangKim,HyunminLee,andGunheeKim. Audiocaps: Generat-
ingcaptionsforaudiosinthewild. InNAACL-HLT,2019.
[46] XinhaoMei,ChutongMeng, HaoheLiu, QiuqiangKong, TomKo, ChengqiZhao, MarkD
Plumbley,YuexianZou,andWenwuWang. Wavcaps: Achatgpt-assistedweakly-labelledaudio
captioningdatasetforaudio-languagemultimodalresearch. arXivpreprintarXiv:2303.17395,
2023.
[47] KunChangLi,YinanHe,YiWang,YizhuoLi,WenhaiWang,PingLuo,YaliWang,LiminWang,
andYuQiao. Videochat: Chat-centricvideounderstanding. arXivpreprintarXiv:2305.06355,
2023.
[48] OpenAI. Chatgpt-4ohttps://www.openai.com/chatgpt,2024.
[49] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-
imagepre-trainingforunifiedvision-languageunderstandingandgeneration. InInternational
ConferenceonMachineLearning,pages12888–12900.PMLR,2022.
[50] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual
instructiontuning. arXivpreprintarXiv:2310.03744,2023.
[51] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making
thevinvqamatter: Elevatingtheroleofimageunderstandinginvisualquestionanswering.
12In Proceedings of the IEEE conference on computer vision and pattern recognition, pages
6904–6913,2017.
[52] DannaGurari,QingLi,AbigaleJStangl,AnhongGuo,ChiLin,KristenGrauman,JieboLuo,
andJeffreyPBigham. Vizwizgrandchallenge: Answeringvisualquestionsfromblindpeople.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages
3608–3617,2018.
[53] XiangYue,YuanshengNi,KaiZhang,TianyuZheng,RuoqiLiu,GeZhang,SamuelStevens,
DongfuJiang,WeimingRen,YuxuanSun,CongWei,BotaoYu,RuibinYuan,RenliangSun,
MingYin,BoyuanZheng,ZhenzhuYang,YiboLiu,WenhaoHuang,HuanSun,YuSu,and
WenhuChen. Mmmu: Amassivemulti-disciplinemultimodalunderstandingandreasoning
benchmarkforexpertagi. InCVPR,2024.
[54] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,
ŁukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. Advancesinneuralinformation
processingsystems,30,2017.
[55] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Languagemodelsareunsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
[56] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,Ariel
Herbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray,BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRadford,Ilya
Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle,
M.Ranzato, R.Hadsell, M.F.Balcan, andH.Lin, editors, AdvancesinNeuralInformation
ProcessingSystems,volume33,pages1877–1901.CurranAssociates,Inc.,2020.
[57] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
JakobUszkoreit,andNeilHoulsby. Animageisworth16x16words: Transformersforimage
recognitionatscale. arXiv: ComputerVisionandPatternRecognition,2021.
[58] Muhammad Awais, Muzammal Naseer, Salman Khan, Rao Muhammad Anwer, Hisham
Cholakkal,MubarakShah,Ming-HsuanYang,andFahadShahbazKhan. Foundationalmodels
defininganewerainvision: Asurveyandoutlook. arXivpreprintarXiv:2307.13721,2023.
[59] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgar-
wal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark,GretchenKrueger,andIlya
Sutskever. Learning transferable visual models from natural language supervision. arXiv:
ComputerVisionandPatternRecognition,2021.
[60] MariaTsimpoukelli,JacobMenick,SerkanCabi,S.M.AliEslami,OriolVinyals,andFelix
Hill. Multimodalfew-shotlearningwithfrozenlanguagemodels. arXiv: ComputerVisionand
PatternRecognition,2021.
[61] TimoLüddecke. Imagesegmentationusingtextandimageprompts. arXiv: ComputerVision
andPatternRecognition,2021.
[62] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,
TeteXiao,SpencerWhitehead,AlexanderCBerg,Wan-YenLo,etal. Segmentanything. arXiv
preprintarXiv:2304.02643,2023.
[63] XinlongWang,WenWang,andTiejunHuang. Imagesspeakinimages: Ageneralistpainterfor
in-contextvisuallearning. arXiv: ComputerVisionandPatternRecognition,2022.
[64] DannyDriess,FeiXia,MehdiS.M.Sajjadi,CoreyLynch,AakankshaChowdhery,BrianIchter,
AyzaanWahid,JonathanTompson,QuanVuong,TianheYu,WenlongHuang,YevgenChebotar,
PierreSermanet,DanielDuckworth,SergeyLevine,VincentVanhoucke,KarolHausman,Marc
Toussaint,KlausGreff,AndyZeng,IgorMordatch,andPeteFlorence. Palm-e: Anembodied
multimodallanguagemodel. arXiv: ComputerVisionandPatternRecognition,2023.
[65] MuhammadMaaz,HanoonaRasheed,SalmanKhan,andFahadShahbazKhan.Video-ChatGPT:
Towardsdetailedvideounderstandingvialargevisionandlanguagemodels. arXivpreprint
arXiv:2306.05424,2023.
13[66] QiushanGuo,ShaliniDeMello,HongxuYin,WonminByeon,KaChunCheung,YizhouYu,
PingLuo, andSifeiLiu. Regiongpt: Towardsregionunderstandingvisionlanguagemodel.
CVPR,2024.
[67] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang,
Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large
languagemodelswithmultimodality. arXivpreprintarXiv:2304.14178,2023.
[68] YixuanSu,TianLan,HuayangLi,JialuXu,YanWang,andDengCai. Pandagpt: Onemodelto
instruction-followthemall. arXivpreprintarXiv:2305.16355,2023.
[69] JiasenLu,ChristopherClark,SanghoLee,ZichenZhang,SavyaKhosla,RyanMarten,Derek
Hoiem,andAniruddhaKembhavi. Unified-io2: Scalingautoregressivemultimodalmodels
withvision,language,audio,andaction. arXivpreprintarXiv:2312.17172,2023.
[70] AnasAwadalla,IrenaGao,JoshuaGardner,JackHessel,YusufHanafy,WanrongZhu,Kalyani
Marathe,YonatanBitton,SamirGadre,JeniaJitsev,SimonKornblith,PangWeiKoh,Gabriel
Ilharco,MitchellWortsman,andLudwigSchmidt. Openflamingo,March2023.
[71] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv
preprintarXiv:2106.09685,2021.
14A X-VILATraining
The training process of X-VILA is divided into three phases, namely (i) encoder-LLM-Decoder
alignment training, (ii) interleaved data pre-training, and (iii) X-to-X cross-modality instruction
fine-tuning.
A.1 Encoder-LLM-decoderalignmenttrainingphase.
Asthefirststep,wealigntheoutputofmodality-specificencodersandtheinputofmodality-specific
decoderstothetextualembeddingspaceofLLM,asdetailedin[32]. Toachievethisgoal,weonly
traintheinputprojectionlayers,outputprojectionlayers,andthevocabularyembeddinglayerof
LLM, while keeping all other parameters frozen. We use corpus with “X”-text pairs to train the
model,where“X”isoneofthevideo,image,oraudiomodalities.
Forthisstage,wedesigntwoprimarytaskstotraintheprojectionlayers: X-to-textgenerationand
text-to-Xgeneration. (a)X-to-textgenerationincludesvideo,image,andaudiocaptioningtasks. The
modelissupervisedtogeneratetextbasedonthemulti-modalityinputs. Duringthisprocess,the
inputprojectionlayersaretrainedtoaligntheoutputembeddingofmodality-specificencodersand
thetextualembeddingspaceofpre-trainedLLM.(b)Text-to-Xgenerationaimsataligningtheoutput
textualembeddingspaceofLLMandtheinputendofmodality-specificdecoders. Weusevideo,
image,andaudiogenerationtaskstotrainthemodel,whereonlytheoutputprojectionlayersare
optimized.Aspreviouslymentioned,thetrainingobjectivehereispuretextualalignment:minimizing
thefeaturedistancebetweenthetextualcontrollerembeddingEtextgeneratedbytheoutputprojection
m
layersandtheembeddinggeneratedbytheoriginalpre-trainedtextencoderofdiffusionmodel. This
trainingstrategyensuresthatEtextsharesadistributionsimilartothatofthepre-trainedtextencoder
m
inthediffusionmodel. Aftertraining,Etextreplacesthediffusiontextencoderfeaturetocontrolthe
m
U-Netsofthemodality-specificdecodersviacross-attention.
A.2 Interleaveddatapre-trainingphase.
Interleaveddatatraininghasbeenproventobeaneffectivestrategyforvision-languagemodelsin
alleviatingthecatastrophicforgettingissueaftertrainingononlyvisual-textpairs,andobtaininglong-
contextunderstandingability[28,70]. Therefore,weintroduceadedicatedphaseforpre-training
X-VILAusingamulti-modalityinterleavedcorpus. Inadditiontointerleavedimage-textpairsasin
MMC4[25],wefurtherconstructanewdatasetfromActivityNetCaptions[35]. Themainideais
toexploitthenatureofvideothatcontainssequentialflowoftext(e.glet@tokeneonedot,captions),
audio,shortvideo,andimage. Thisenablesustoputtheimages/videosandtextsinaninterleaved
manner,andusethecorpustopre-trainX-VILA.
Specifically,weconstructinterleavedmulti-modalitydatasequencesfromeachtargetvideoclipas:
{<img. 1>, <aud. 1>, <vid. 1>, <txt 1>},...,{<img. n>, <aud. n>, <vid. n>, <txt n>},
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
sampledfromvideochunk1 sampledfromvideochunkn
wherethevideochunksaresampledfromanentirevideoclipthatoffersnaturalsourcesofinterleaved
cross-modalitydatastructure. Onceconstructed,themodalitiesaresampledduringtrainingtoalign
varyingtargetsforgradientcomputationandnetworkprojectoralignment. Inthiswork,weobserve
theevensamplingmethodandn=3aresufficientforthetask,namelyconstructingcross-modality
tasksforthebeginning,middlestage,andendingofvideoclips. Duringthisstage,wejointlytrain
theinputandoutputprojectionlayers,anduseLoRA[71]onLLMforfine-tuning.
A.3 X-to-Xcross-modalityinstructiontuningphase.
Aftertheprevioustwophases,wehavetextuallyaligneddifferentcomponentsofX-VILAinaunified
framework. However,themodelisstillnotreadyforunderstandingandgeneratingmulti-modality
contentinapropermanner. Toachievethisgoal,wecurateacomprehensive“X-to-Xdataset”for
cross-modalitygenerationinstructiontuning. Asvideocaptioningdatasetsareinherentlymulti-modal
andprovideabundantcorpusinvideo,audio,image,andtextforms,webuildourX-to-Xdatasetbased
ontwovideocaptioningdatasets: Webvid[34]andActivityNetCaptions[35]. OurX-to-Xdataset
15Prompt: What is shown in the image? Prompt: What is he doing in the video?
Target: A man was sitting inside a room. Target: He is holding a bowl of noodles and broth.
Prompt: Can you show me what will happen next in the Prompt: Can you predict what will happen next in the
scene using a video? video?
Target: The man is likely savoring the Target: Here is a video showing the
taste of the broth. man savor the broth.
Image to Text / Video Data Video to Text / Video Data
Prompt: Create a captivating video
Prompt: Please create theaudio
montage using elements from this
component for this video.
audio.
Target: Sure! Here is the requested
Target: Of course. This is the video.
audio.
Video to Audio Data Audio to Video Data
Prompt: Given this video, could you Prompt: Can you generate a video
generate a similar image? by animating this image and audio?
Target: Of course. This is the Target: Here's the generated video.
image. Enjoy!
Video to Image Data Image + Audio to Video Data
Figure9: ExamplesofdifferenttypesofconversationsinourX-to-Xdataset. Theycoversixtypesof
cross-modalityunderstandingandgenerationtasks.
features six different types of cross-modality generative conversations, namely video-to-image,
video-to-video,image-to-video,video-to-audio,audio-to-video,andimage+audio-to-video. We
showexamplesofdifferenttypesofconversationsinFigure9. Eachconversationcontainsoneor
moreroundsofcross-modalityconversation. MoredetailsabouttheX-to-Xdatasetaredescribedin
theexperimentsection.
WefurtherdividetheX-to-Xcross-modalityinstructiontuningphaseintotwodistinctsteps,each
basedondifferentalignmentmethods: textualalignmentandvisualalignment.
(a)Toachievetextualalignment,wefirstprojectthemulti-modalityinputsintothetextualembedding
spaceofLLM.Then,LLMgeneratestextualembeddingsthataresubsequentlyconvertedintothe
correspondingmodality’scontent. Wefollowa processsimilartophases(i)and(ii). Firstly, for
image,video,oraudiooutputs,wegenerateembeddingsusingthetextencodersofcorresponding
diffusionmodels. WethenoptimizethedistancebetweentheseembeddingsandtheEtextgenerated
m
byourmodel. Duringthisstep,wekeepallthedecoderweightsfrozenandtraintheinputprojection
layers,outputprojectionlayers,andvocabularyembeddinglayeraswellasLoRAparametersof
LLM.Fortrainingdata,weblendourX-to-XdatasetwithcommonSFTdatasetsusedbyotherVLM
models[14,32](moredetailsintheexperimentsection).
(b)Asmentionedearlier,relyingsolelyontextualalignmentisinherentlyinsufficienttoretainthe
visualdetailsoftheinputwhengeneratingvisualoutputs. Toaddresssuchanissue,wedesignanovel
visualalignmentmethod. Weproposeavisualembeddinghighway(VEH)moduleasintroducedin
Section2.1,whichisutilizedfortheimageandvideodecoderswhenthereisavisualmodalityin
theinput. Duringtraining,weupdatetheparametersofthevisualdecodersandthevisualcontroller
module. Meanwhile, wekeepallothernetworkparametersfixed, includingtheinputandoutput
projectionlayersandLLM.Inthisway,themodel’sabilitytoconducttasksinothermodalitiesisnot
influencedbythevisualalignmentprocess.
16B MoreQualitativeResults
B.1 ExamplesofourX-to-Xdataset.
ToprovideanintuitiveunderstandingofthesixtypesofconversationsinourcuratedX-to-Xdataset,
wevisualizetheconversationsamplesofthedatasetinFigure9. Thedesignofthedatasetfocuseson
buildingany-to-anymodalityconnectionthroughvariousconversationtemplates.
B.2 VisualcomparisonwithCoDioncross-modalityalignment.
TofurtherexaminethevisualalignmentadvantageofX-VILA,wecompareitwiththestate-of-the-art
any-to-anymodelCoDi[31]inFigure10. WeobservethatCoDifailstocapturetherealsemantics
anddetailsoftheinput. Notably,CoDiisunabletoperformX-to-Xchatting,unlikeX-VILA,which
isspecificallydesignedforomni-modalitychattingwhilebeingabletoproducesuperiorvisually
alignedgenerationresults.
B.3 Human-modelinteractiondemonstration.
Toconductacomprehensiveassessmentofourany-to-anymodalityLLM’sperformance,weunder-
takemoretestingonX-VILA,meticulouslyexaminingdifferentusecases. Wepresentacollection
ofhuman-modelconversationexamplesinFigure11,12,13and14,showcasingtheversatilityof
X-VILAacrossdiversetasks. TheseresultsdemonstratetheeffectivenessofX-VILAinaddressing
theneedsofusersbyofferingcomprehensiveandgenerativemulti-modalitycapabilities.
Input CoDi (Tang et al. 2023) X-VILA (ours)
Image to Video
Video to Video
Figure10: VisualcomparisontotherecentworkCoDi[31]oncross-modalityalignment. X-VILA
demonstrateslargelyimprovedgenerationqualityandcross-modalityconsistency.
C MoreImplementationDetails
AsintroducedinSectionA,X-VILAtrainingisseparatedintothreephases. (i)Intheinitialphase,
referredtoasencoder-LLM-decoderalignmenttraining,themodelundergoes20,000iterationsusing
anAdamoptimizer. Thebaselearningrateissetto4×10−4,andalearningratewarm-upstrategy
isemployed. Thebatchsizeforthisphaseissetto200. (ii)Duringthesecondphase, knownas
interleaveddatapre-training,abatchsizeof192isutilized. Thebaselearningrateissetto1×10−4,
and the training is conducted for 10,000 iterations. (iii) The final phase, called cross-modality
instructiontuning,involvesseparatetrainingfortextualandvisualalignment. Fortextualalignment,
abatchsizeof192ismaintained,andthemodelistrainedfor30,000iterationsusingabaselearning
rate of 1×10−4. Regarding visual alignment, both the Visual Embedding Highway (VEH) and
modality-specificdecodersaretrainedfor20,000iterations. Thebatchsizeforthisphaseissetto
64,andthelearningrateisadjustedto1×10−6. Fordataamount,Ourtrainingpipelineishighly
efficientcomparedtomanypreviousvision-languagemodels[15,20,17]. Weutilizeatotalof4
NVIDIAA10080GBservernodesinthetrainingprocess.
17Prompt: Who is the man on the left?
X-VILA: The man on the left is Albert Einstein, a renowned physicist.
Prompt: What is he famous for?
X-VILA: He is famous for his contributions to the field of modern physics, particularly his
theory of relativity.
Prompt: Wow, do you know any of his hobbies after work?
X-VILA: He was known for pursuing various interests like playing the violin.
Prompt: I see. Can you show me the sound of violin?
X-VILA: Here is an audio clip of a person playing a violin. Enjoy!
(Violin playing)
Figure 11: Example of X-VILA performing cross-modality chat. X-VILA shows strong multi-
understandingandgenerationability.
18Prompt: I am going to travel in Paris, how does it look like?
X-VILA: Here's a video showcasing the beauty of Paris, the city of romance. You'll get to see its
iconic landmarks, charming cafes, and the vibrant atmosphere that makes it so special. Enjoy
the virtual tour!
Prompt: Does Paris have any special music genre?
X-VILA: Absolutely! Paris is known for its diverse and vibrant music culture. Some of the
most popular styles include French pop, jazz, and classical music. You can also find a wide
range of modern and experimental musicians in the city's bustling music scenes.
Prompt: Wow, can you show me an audio of French Jazz?
X-VILA: Sure! Here's an audio clip that captures the lively and vibrant sounds of French Jazz.
Enjoy the rhythmic beats and harmonious melodies that make this genre so enjoyable.
Figure 12: Example of X-VILA performing cross-modality chat. X-VILA shows strong multi-
understandingandgenerationability.
19Prompt: The summer is coming, where do you think is a good place for summer vacation?
X-VILA: There are many great places to visit for a summer vacation. Some popular options
include beaches, national parks, and mountain resorts. Beaches offer a chance to relax and
soak up the sun, while national parks provide opportunities for hiking, camping, and exploring
nature. Mountain resorts offer a range of outdoor activities such as golfing, hiking, and
climbing, as well as a variety of cultural events and attractions. Ultimately, the best
destination for your summer vacation will depend on your personal interests and travel
preferences.
Prompt: Thanks. I would like to go to the beach, do you have any recommendations? Please
show me a video.
X-VILA: Certainly! There are many beautiful beaches around the world that would make for a
great summer vacation. Here is a video showcasing some of them. Enjoy!
Prompt: Cool. I am actually a businessman. What do you think sell well on the beach? Can
you show an image example?
X-VILA: As a businessman, you may be interested in selling products or services that cater
to the needs and desires of beachgoers during their summer vacation. Here is an image of a
delicious summer picnic on the beach, complete with a variety of fresh and delicious food
items like fruits, vegetables, sandwiches, and more.
Figure13: MoreexamplesofX-VILAperformingcross-modalitychatting.
20Prompt: Can you tell me what is shown in the image?
X-VILA: The man is standing on a snowboard.
Prompt: What is going to happen? Can you show a video of it?
X-VILA: The man is snowboarding down the hill.
Figure14: ConversationexamplesofX-VILAperformingcross-modalityconversation. X-VILA
demonstratesaremarkablecapabilitytocomprehendthevisualinputandperformreasoningbased
onit. Ourproposedvisualalignmenttechniqueplaysacrucialroleinestablishingvisualconsistency
betweentheinputandoutput.
21