Self-Exploring Language Models:
Active Preference Elicitation for Online Alignment
ShenaoZhang1 DonghanYu2 HiteshiSharma2
ZiyiYang2 ShuohangWang2 HanyHassan2 ZhaoranWang1
1NorthwesternUniversity 2Microsoft
Abstract
Preferenceoptimization,particularlythroughReinforcementLearningfromHuman
Feedback(RLHF),hasachievedsignificantsuccessinaligningLargeLanguage
Models (LLMs) to adhere to human intentions. Unlike offline alignment with
a fixeddataset,online feedbackcollection from humans orAI on modelgener-
ations typicallyleads to more capable rewardmodels andbetter-alignedLLMs
throughaniterativeprocess.However,achievingagloballyaccuraterewardmodel
requiressystematicexplorationtogeneratediverseresponsesthatspanthevast
spaceofnaturallanguage.Randomsamplingfromstandardreward-maximizing
LLMs alone is insufficient to fulfill this requirement. To address this issue,we
proposeabilevelobjectiveoptimisticallybiasedtowardspotentiallyhigh-reward
responsestoactivelyexploreout-of-distributionregions.Bysolvingtheinner-level
problemwiththereparameterizedrewardfunction,theresultingalgorithm,named
Self-ExploringLanguageModels(SELM),eliminatestheneedforaseparateRM
anditerativelyupdatestheLLMwithastraightforwardobjective. Comparedto
DirectPreferenceOptimization(DPO),theSELMobjectivereducesindiscriminate
favorofunseen extrapolations andenhances exploration efficiency. Ourexperi-
mentalresultsdemonstratethatwhenfinetunedonZephyr-7B-SFTandLlama-3-
8B-Instructmodels,SELM significantlyboosts theperformanceon instruction-
followingbenchmarkssuchasMT-BenchandAlpacaEval2.0,aswellasvarious
standard academic benchmarks in different settings. Our code and models are
availableathttps://github.com/shenao-zhang/SELM.
1 Introduction
LargeLanguageModels(LLMs)haverecentlyachievedsignificantsuccesslargelyduetotheirability
tofollowinstructionswithhumanintent.AsthedefactomethodforaligningLLMs,Reinforcement
LearningfromHumanFeedback(RLHF)worksbymaximizingtherewardfunction,eitheraseparate
model[44;5;18]orreparameterizedbytheLLMpolicy[49;48;4;70],whichislearnedfromthe
prompt-response preference data labeled by humans. The key to the success of alignment is the
responsediversitywithinthepreferencedata,whichpreventsrewardmodels(RMs)fromgetting
stuckinlocaloptima,therebyproducingmorecapablelanguagemodels.
Offlinealignmentmethods[49;54]attempttomanuallyconstructdiverseresponsesforfixedprompts
[11;24;72],which,unfortunately,strugglestospanthenearlyinfinitespaceofnaturallanguage.On
theotherhand,onlinealignmentfollowsaniterativeprocedure:samplingresponsesfromtheLLM
andreceivingfeedbacktoformnewpreferencedataforRMtraining[44;21].Theformerstephelps
exploreout-of-distribution(OOD)regionsthroughrandomnessinsampling.However,instandard
onlineRLHFframeworks,maximizingtheexpectedrewardlearnedfromthecollecteddataisthe
onlyobjectivefortheLLM,samplingfromwhichoftenleadstoresponsesclusteredaroundlocal
optima.Thispassiveexplorationmechanismcansufferfromoverfittingandprematureconvergence,
leavingthepotentiallyhigh-rewardregionsunexplored.
4202
yaM
92
]GL.sc[
1v23391.5042:viXraa Figure1:Intuitionofourmethod.Forafixedprompt
dat
optimisticRM x,arewardmodelr(x,y)triestofittheground-truth
d
ve rewardr‚àó(x,y).TheblueandgreenRMsareequally
er
obs
vanillaRM
goodwhenusingstandardreward-fittinglossL lr,since
the observed preference data (red stars) are fitted
equally well. However, the green RM has a larger
uncertainty ground-truth max r(x,y) and thus a lower optimistically biased
y
lossL ‚àíŒ±max r(x,y).Therefore,theresponsey
lr y u
ùë¶! atwhichtheuncertaintyishighcanbeelicitedandthen
responseùë¶ proceededforhumanfeedbacktoreduceuncertainty.
To address this issue,we propose an active exploration method for online alignment that elicits
novelfavorableresponses. Initssimplestform,anoptimismtermŒ±max r(x,y)isaddedtothe
y
reward-fittingobjective(e.g.,logisticregressionondatasetD),denotedas‚àíL ,resultinginabilevel
lr
optimizationobjectivefortherewardmodelr:
maxmaxŒ±r(x,y)‚àíL (r;D), (1.1)
lr
r y
whereŒ±isahyperparametercontrollingthedegreeofoptimism.TheintuitionisillustratedinFigure
1.Specifically,minimizingthevanillareward-fittinglossL islikelytogivealocallyaccurateRM
lr
thatoverfitstheobserveddataandgetsstuckinlocalminima.Randomsamplingfromthisvanilla
RMmaytakealongtimetoexploretheOODregionsthatcontainthebestresponse.Byincorporating
theoptimismterm,weobtainanRMthatbothfitsthedatawellandhasalargemax r(x,y).This
y
ensuresthatthegreedyresponsey fromitiseithergloballyoptimalwhenuncertaintyinhigh-reward
u
regionsiseliminated,orpotentiallygoodinunexploredareaswherer(x,y )canbearbitrarilyhuge
u
duetotherelaxedreward-fittingloss.Feedbackfromhumansontheseresponsesy canthenreduce
u
uncertaintyandtrainamoreaccurateRM.
Inthispaper,weformulatethisideawithinthecontextofonlinedirectalignment,wheretheLLMis
iterativelyupdatedwithoutaseparateRM.WefirstintroducetwomodificationstothebilevelRM
objectivein1.1,namelyaddingKLconstraintsandusingrelativemaximumreward.Thenwederive
asimpleLLMtrainingobjectivebyapplyingtheclosed-formsolutionoftheinner-levelproblem
andreparameterizingtherewardwiththeLLMpolicy. Theresultingiterativealgorithmiscalled
Self-Exploring Language Models (SELM). We show that the policy gradient of SELM is biased
towardsmorerewardingareas.Furthermore,byreducingthechanceofgeneratingresponsesthatare
assignedlowimplicitrewards,SELMmitigatestheindiscriminatefavoringofunseenextrapolations
foundinDPO[49;48]andenhancesexplorationefficiency.
In experiments,we implement SELM using Zephyr-7B-SFT [57] and Llama-3-8B-Instruct [38]
asbasemodels.ByfinetuningsolelyontheUltraFeedback[11]datasetandusingthesmall-sized
PairRM[25]foriterativeAIfeedback,SELMbooststheperformanceofZephyr-7B-SFTandLlama-
3-8B-Instruct by a large margin on AlpacaEval 2.0 [14] (+16.24% and +11.75% LC win rates)
andMT-Bench[71](+2.31and+0.32).SELMalsodemonstratesstrongperformanceonstandard
academicbenchmarksandachieveshigherpairwiseLCwinratesagainsttheiterativeDPObaseline.
Ourcodeandmodelsareavailableathttps://github.com/shenao-zhang/SELM.
2 Background
2.1 LargeLanguageModels
A language modelœÄ ‚àà ‚àÜX typically takes the promptx ‚àà X as inputandoutputs the response
Y
y ‚àà Y.Here,X andY arefinitespacesofpromptsandresponses,respectively.Giventheprompt
x‚ààX,adiscreteprobabilitydistributionœÄ(¬∑|x)‚àà‚àÜ isgenerated,where‚àÜ isthesetofdiscrete
Y Y
distributions overY. Modern recipes fortraining LLMs consistofpre-training andpost-training
procedures,whereduringpre-training,LLMslearntopredictthenextwordonahugeanddiverse
datasetoftextsequences in orderto understandthe underlying patterns andstructures ofnatural
languageinanunsupervisedmanner.Thepost-trainingprocedureaimstoalignbettertoendtasks
and human preferences with two phases happening in order: Supervised Fine-Tuning (SFT) and
humanpreferencealignment.Here,SFTfine-tunesthepre-trainedLLMwithsupervisedlearning
2
draweron high-qualitydata to followinstructions on downstream tasks andobtain a modelœÄSFT. In the
followingofthispaper,wefocusmainlyonpreferencealignment.
2.2 RewardModelingandPreferenceOptimization
ReinforcementLearningfromHumanFeedback(RLHF). StandardRLHFframeworksconsist
oflearningarewardmodelandthenoptimizingtheLLMpolicyusingthelearnedreward.
Specifically,apoint-wiserewardr(x,y):X √óY ‚ÜíRrepresentstheEloscore[16]oftheresponse
ygiventhepromptx.ThenthepreferencedistributioncanbeexpressedbytheBradley-Terrymodel
thatdistinguishesbetweenthepreferredresponsey andthedispreferredresponsey givenprompt
w l
x,denotedasy ‚âªy |x,usingthelogisticfunctionœÉ:
w l
p(y ‚âªy |x):=E (cid:2) 1(hprefersy overy givenx)(cid:3)
w l h w l
(cid:0) (cid:1)
exp r(x,y )
(cid:0) (cid:1) w
=œÉ r(x,y )‚àír(x,y ) = , (2.1)
w l (cid:0) (cid:1) (cid:0) (cid:1)
exp r(x,y ) +exp r(x,y )
w l
wherehdenotesthehumanraterandtheexpectationisoverhtoaccountfortherandomnessofthe
choicesofhumanratersweaskfortheirpreference.WhenprovidedastaticdatasetofN comparisons
D ={x ,y ,y }N ,theparameterizedrewardmodelcanbelearnedbyminimizingthefollowing
i w,i l,i i=1
logisticregressionloss:
L (r;D)=‚àíE (cid:2) logœÉ(cid:0) r(x,y )‚àír(x,y )(cid:1)(cid:3) . (2.2)
lr (x,yw,yl)‚àºD w l
Usingthelearnedreward,theLLMpolicyœÄ ‚àà‚àÜX isoptimizedwithreinforcementlearning(RL)to
Y
maximizetheexpectedrewardwhilemaintainingasmalldeviationfromsomebasereferencepolicy
œÄ ,i.e.,maximizingthefollowingobjective
ref
J(œÄ;D)=E (cid:2) r(x,y)(cid:3) ‚àíŒ≤D (œÄ||œÄ ), (2.3)
x‚àºD,y‚àºœÄ(¬∑|x) KL ref
whereŒ≤ isahyperparameterandD (œÄ||œÄ ):=E [KL(œÄ(¬∑|x)||œÄ (¬∑|x))]istheexpected
KL ref x‚àºD ref
Kullback-Leibler(KL)divergence.AnidealœÄ isthepolicythathelpsmitigatethedistributionshift
ref
issue[49;21]betweenthetruepreferencedistributionandthepolicyœÄ duringtheoff-policyRL
training.SinceweonlyhaveaccesstothedatasetDsampledfromtheunavailabletruepreference
distribution,œÄ canbeobtainedbyfine-tuningonthepreferredresponsesinD orsimplysetting
ref
œÄ =œÄSFTandperformingRLHFbasedontheSFTmodel.
ref
DirectAlignmentfromPreference. Withthemotivationtogetridofaseparaterewardmodel,
whichiscomputationallycostlytotrain,recentworks[49;4;70;57;17]derivedthepreferenceloss
asafunctionofthepolicybychangingofvariables.Amongthem,DPO[49]showsthatwhentheBT
modelin(2.1)canperfectlyfitthepreference,theglobaloptimizersoftheRLHFobjectivein(2.3)
andthefollowinglossareequivalent:
(cid:20) (cid:18) (cid:19)(cid:21)
œÄ(y |x) œÄ(y |x)
L (œÄ;D)=‚àíE logœÉ Œ≤log w ‚àíŒ≤log l .
DPO (x,yw,yl)‚àºD œÄ (y |x) œÄ (y |x)
ref w ref l
3 Self-ExploringLanguageModels
3.1 RM-FreeObjectiveforActiveExploration
Inthissection,wepresentseveralmodificationstotheoptimisticallybiasedobjective(1.1)motivated
intheintroduction.ThenwederiveanRM-freeobjectivefortheLLMpolicyandanalyzehowactive
explorationworksbyexaminingitsgradient.
First,we considerthe equivalence of(1.1): max ‚àíL (r;D)+Œ±max E [r(x,y)],where the
r lr œÄ y‚àºœÄ
innerœÄisdeterministicwhenoptimal.ToaccountforthechangeofœÄrelativetothereferencepolicy
œÄ ,weintroducetwomodifications:(1)replacingtheoptimisticbiastermmax E [r(x,y)]with
ref œÄ y‚àºœÄ
max E [r(x,y)‚àír(x,y‚Ä≤)],and(2)incorporatingaKL-divergencelosstermbetweenœÄ
œÄ y‚àºœÄ,y‚Ä≤‚àºœÄref
andœÄ .ThesechangesensurethattheresultingoptimisticRMelicitsresponseswithhighpotential
ref
unknowntothereferencepolicyœÄ whileminimizingthedeviationbetweenœÄandœÄ .
ref ref
3Formally,fortherewardfunctionr,thebileveloptimizationproblemwithoptimismisformulatedas:
(cid:32) (cid:33)
(cid:104) (cid:105)
max‚àíL (r;D )+Œ±max E r(x,y)‚àír(x,y‚Ä≤) ‚àíŒ≤D (œÄ||œÄ ) , (3.1)
r
lr t
œÄ
x‚àº yD ‚Ä≤‚àºt, œÄy re‚àº f(œÄ ¬∑|( x¬∑ )|x) KL ref
(cid:124) (cid:123)(cid:122) (cid:125)
F(œÄ;r)
whereD ={x ,yt ,yt }N istheassociateddatasetatiterationtandL isthelogisticregression
t i w,i l,i i=1 lr
loss defined in (2.2). The nested optimization in (3.1) can be handled by first solving the inner
optimizationF(œÄ;r)toobtainœÄ thatisoptimalunderr.Thesolutionisasfollowsandwedeferall
r
thederivationsinthissectiontoAppendixA.
1 (cid:0) (cid:1)
œÄ (y |x):=argmaxF(œÄ;r)= œÄ (y |x)exp r(x,y)/Œ≤ ,
r Z(x) ref
œÄ
(cid:80)
where the partition function Z(x) = œÄ (y|x)exp(r(x,y)/Œ≤). By substituting œÄ = œÄ into
y ref r
F(œÄ;r),wecanrewritethebilevelobjectivein(3.1)asasingle-levelone:
max‚àíL (r;D )+Œ±F(œÄ ;r).
lr t r
r
Following the implicit reward formulation in DPO,we reparameterize the reward function with
Œ∏ ‚àà Œòasr (x,y) = Œ≤(logœÄ (y | x)‚àílogœÄ (y | x)),whichistheoptimalsolutionof(2.3)and
(cid:98)Œ∏ Œ∏ ref
canexpressallrewardclassesconsistentwiththeBTmodelasprovedin[49].Withthischangeof
variable,weobtaintheRM-freeobjectivefordirectpreferencealignmentwithoptimism:
max‚àíL (œÄ ;D )‚àíŒ±Œ≤E (cid:2) logœÄ (y |x)(cid:3) . (3.2)
œÄŒ∏
DPO Œ∏ t x‚àºD,y‚àºœÄref(¬∑|x) Œ∏
Wenowanalyzehowthisnewobjectiveencouragesactiveexploration.Specifically,wederivethe
gradientof(3.2)withrespecttoŒ∏as
‚àíŒ≤E (cid:104) œÉ(cid:0) r (x,y )‚àír (x,y )(cid:1)(cid:0) ‚àá logœÄ (y |x)‚àí‚àá logœÄ (y |x)(cid:1)(cid:105)
(x,yw,yl)‚àºDt (cid:98)Œ∏ l (cid:98)Œ∏ w Œ∏ Œ∏ w Œ∏ Œ∏ l
(cid:124) (cid:123)(cid:122) (cid:125)
‚àáŒ∏LDPO(œÄŒ∏;Dt)
‚àíŒ±Œ≤E (cid:2) exp(cid:0) ‚àír (x,y)/Œ≤(cid:1) ‚àá logœÄ (y |x)(cid:3) . (3.3)
x‚àºD,y‚àºœÄŒ∏(¬∑|x) (cid:98)Œ∏ Œ∏ Œ∏
We note that the second line,corresponding to the gradient of the optimism term,decreases the
log-likelihoodofresponseygeneratedbyœÄ thathasalowvalueofexp(‚àír (x,y)/Œ≤).Therefore,
Œ∏ (cid:98)Œ∏
theaddedoptimismtermbiasesthegradienttowardparameterregionsthatcanelicitresponsesywith
highimplicitrewardr ,consistentwithourintuitionoutlinedinFigure1.
(cid:98)Œ∏
This also explains why E [logœÄ ] is minimized in our objective (3.2), which is equivalent to
œÄref Œ∏
maximizingtheKLdivergencebetweenœÄ andœÄ ,whilethereverseKLinthepolicyoptimization
ref Œ∏
objective(2.3)isminimized.FortheDPOgradient‚àá L (œÄ ;D ),thedegreeofdeviationofpolicy
Œ∏ DPO Œ∏ t
œÄ fromœÄ onlyaffectsthepreferenceestimatedwithr .Inotherwords,œÉ(r (x,y )‚àír (x,y ))
Œ∏ ref (cid:98)Œ∏ (cid:98)Œ∏ l (cid:98)Œ∏ w
isascalarvalueandthepolicydeviationonlydeterminesthestepsizeofthepolicygradient,instead
ofitsdirection.Ontheotherhand,ouraddedexplorationtermdirectlycontrolsthedirectionofthe
gradienttowardpotentiallymorerewardingareaswhilestillfittingthepreferencedatainD . As
t
morefeedbackdataiscollectediteratively,deviatingfromtheunbiasedlyfittedmodelincursahigher
DPOloss,whichultimatelydominatesourobjectiveatconvergence.Thismechanismensuresthat
theresultingLLMeffectivelybalancesbetweenexploringnovelresponsesandexploitingpreviously
observedones,leadingtoamoreaccurateandalignedmodel.
3.2 Algorithm
Withtheoptimisticallybiasedobjectivederivedabove,thelanguagemodelcanactivelygenerate
OODresponsesworthexploring.HumanorAIfeedbackfollowstoreducetheuncertaintyinthese
regions.Thesetwostepsareexecutediterativelytogetamoreandmorealignedmodel.
Inpractice,wesplittheofflinepreferencedatasetintothreeportionswithequalsizes,oneforeach
iteration. Besides,we use AI rankers,such as external RMs,to provide feedback on the model-
generatedresponseandtheoriginalchosen,rejectedresponses. Thecompletepseudocodeofour
algorithm,namedSelf-ExploringLanguageModels(SELM),isoutlinedinAlgorithm1.
4Algorithm1Self-ExploringLanguageModels(SELM)
Input: ReferencemodelœÄ ,preferencedatasetD,onlineiterationsT,optimismcoefficientŒ±.
ref
1: foriterationt=1,2,...,T do
2: SetD tasthet-thportionofDandgeneratey ‚àºœÄ ref(¬∑|x)foreachpromptxinD t.
3: Rank{y,y w,y l}andupdateD ttocontainthebest(chosen)andworst(rejected)responses.
4: TraintheLLMœÄ Œ∏t =argmax œÄŒ∏‚àíL DPO(œÄ Œ∏;D t)‚àíŒ±E x‚àºDt[logœÄ Œ∏(y |x)]andletœÄ ref =œÄ Œ∏t.
5: endfor
3.3 Self-ExplorationReducesIndiscriminateFavorofUnseenExtrapolations
Ithasbeenobservedrecently[48;46;64]thatDPOdecreasesthelikelihoodofresponsesgenerated
bythereferencepolicy.Itisbecauseforanypromptx,atconvergencewhenœÄ Ã∏=œÄ ,itholdsthat
Œ∏ ref
E (cid:2) r (x,y)/Œ≤(cid:3) =E (cid:2) logœÄ (y |x)‚àílogœÄ (y |x)(cid:3) =‚àíKL(cid:0) œÄ (¬∑|x)||œÄ (¬∑|x)(cid:1) <0,
y‚àºœÄref (cid:98)Œ∏ y‚àºœÄref Œ∏ ref ref Œ∏
while at the beginning of training when œÄ = œÄ ,the above terms are zero. Thus,the expected
Œ∏ ref
implicitrewardr aswellasthelikelihoodofœÄ willdecreaseonthereferencemodel‚Äôsresponses.
(cid:98)Œ∏ Œ∏
ThisindicatesthatDPOstimulatesabiaseddistributionfavoringunseenextrapolatedresponses.Inthe
onlineiterativesettingthatweconsider,theLLMpolicygeneratesresponsesandreceivespreference
feedbackalternately,wherebiasingtowardsOODregionsmaysometimeshelpdiscoveroutstanding
novelresponses.However,DPOindiscriminatelyfavorsunseenextrapolationsandpassivelyexplores
basedpurelyontherandomnessinherentinsamplingfromtheLLM.Asaconsequence,thevastspace
ofnaturallanguagemakesitalmostimpossibletoexhaustivelyexploreallthepossibleresponsesand
identifythosethatmosteffectivelybenefitalignment.
Next,wedemonstratethatSELMmitigatesthisissuebyperformingguidedexploration.Specifically,
considertheproposedself-explorationobjectivein(3.2),which,inadditiontothestandardDPOloss,
alsominimizesE [logœÄ (y |x)].Wenowinvestigatehowtheprobabilitydistributionchanges
x,y‚àºœÄref Œ∏
withthistermincorporated.
Theorem 3.1. For any œÅ ‚àà Œò in the policy parameter space, let r (x,y) = Œ≤(logœÄ (y|x) ‚àí
(cid:98)œÅ œÅ
logœÄ (y|x)) be the reparameterized implicit reward. Denote œÄmin as the policy that minimizes
ref œÅ
theexpectedimplicitrewardundertheKLconstraint,i.e.,
œÄmin(¬∑|x):=argminE (cid:2) r (x,y)(cid:3) +Œ≤D (œÄ||œÄ ). (3.4)
œÅ x,y‚àºœÄ(¬∑|x) (cid:98)œÅ KL œÅ
œÄ
ThenminimizingE [logœÄ (y|x)]decreasesthelikelihoodofresponsessampledfromœÄmin:
x,y‚àºœÄref Œ∏ œÅ
minE (cid:2) logœÄ (y |x)(cid:3) =minE (cid:2) logœÄ (y |x)(cid:3) .
œÄŒ∏
x,y‚àºœÄref(¬∑|x) Œ∏
œÄŒ∏
x,y‚àºœÄ œÅmin(¬∑|x) Œ∏
TheabovetheoremstatesthatmaximizingthedivergencebetweenœÄ andœÄ isessentiallyreducing
Œ∏ ref
the probability ofgenerating responses withlow implicitrewards reparameterizedby any policy
parameterœÅduringtraining.Inotherwords,thepolicynotonlyexploitstheexistingpreferencedata
butalsolearnstoavoidgeneratingthetextythatisassignedalowrewardvalue.Thisprocessoccurs
ineveryiterationwithupdatedreferencemodels.Consequently,responseswithhighpotentialrewards
areselectivelypreferredandmanycommonplaceresponsesreceiveasmallprobabilitymass,thus
mitigatingtheindiscriminatefavoringofunseenresponsesandimprovingexplorationefficiency.
4 RelatedWork
DataSynthesisforLLMs. Akeychallengeforfine-tuninglanguagemodelstoalignwithusers‚Äô
intentionsliesinthecollectionofdemonstrations,includingboththeSFTinstruction-followingexpert
dataandtheRLHFpreferencedata.Gatheringsuchdatafromhumanlabelersisexpensive,time-
consuming,andsometimessuffersfromvariantquality[44;29].Toaddressthisissue,syntheticdata
[34]hasbeenusedforaligningLLMs.Onelineofworkfocusesongeneratingplausibleinstruction
promptsforunlabeleddatabyregardingthetargetoutputasinstruction-followingresponses[31;
60; 27; 55]. Besides, high-quality data can also be distilled from strong models for fine-tuning
weakerones[20;1;32;12;47].ToconstructsyntheticdatasetsforofflineRLHF,apopularpipeline
[11;57;59;24;72]involvesselectingresponsessampledfromvariousLLMsonasetofpromptsin
5thehopetoincreasethediversityofthedatathatcanspanthewholelanguagespace.However,data
manuallycollectedinsuchapassivewaydoesnotconsiderwhatimprovesthemodelmostthrough
itstraining,leavingthepotentiallyhigh-rewardregionsunexplored.
IterativeOnlinePreferenceOptimization ComparedtoofflineRLHFalgorithms[49;70;4]that
collectpreferencedatasetsaheadoftraining,onlineRLHF[44;21],especiallytheiterative/batched
onlineRLHF[5;63;19;22;62;6;50]hasthepotentialtogatherbetterandbettersyntheticdataasthe
modelimproves.Asaspecialcase,self-alignmentlanguagemodelsaligntheirresponseswithdesired
behaviors,suchasmodel-generatedfeedback[66;67;53;58]. Unfortunately,theabovemethods
stillpassivelyexplorebyrelyingontherandomnessduringsamplingandeasilygetstuckatlocal
optimaandoverfittothecurrentdataduetothevastspaceofnaturallanguage.Anotableexception
is [15],which proposed to use ensembles of RMs to approximately measure the uncertainty for
posterior-samplingactiveexploration.Onthecontrary,ourmethodexploresbasedontheoptimistic
biasanddoesnotestimatetheuncertaintyexplicitly,bypassingtheneedtofitmultipleRMs.
ActiveExploration. Infact,activeexplorationhasbeenwidelystudiedbeyondLLMs.Similarto
[15],mostexistingsample-efficientRLalgorithmsfirstestimatetheuncertaintyoftheenvironment
usinghistoricaldataandthenplanwithoptimism[3;51;26],orselectingtheoptimalactionfroma
statisticallyplausiblysetofactionvaluessampledfromitsposteriordistribution[52;41;42;69].The
proposedself-explorationobjectivecanbecategorizedasanoptimism-basedexplorationmethod.
However,mostpreviousworksrequiretheestimationoftheupperconfidencebound,whichisoften
intractable.Ensemblemethods[43;8;37]canserveasapproximationstotheuncertaintyestimation
butarestillcomputationallyinefficient.MEX[35]proposedtocombineestimationandplanningina
singleobjectivesimilartooursandestablishedtheoreticalguaranteesundertraditionalRLsetups.
Concurrenttoourwork,RPO[36]alsoproposedtouseanadversariallychosenrewardmodelfor
policyoptimization,butfocusesonmitigatingoveroptimizationinofflineRLHFsettings.
5 Experiments
5.1 ExperimentSetup
WeselecttheUltraFeedback[11]datasetasourtrainingset,whichcontains61kpreferencepairsof
single-turnconversations.FortherankerprovidingAIfeedbackduringonlinealignment,wechoose
thesmall-sizedPairRM(0.4B)[25].Allexperimentsareconductedon8xA100GPUs.
Duetotheabsenceofperformantopen-sourceonlinedirectalignmentcodebasesatthetimeofthis
study,wefirstimplementaniterativeversionofDPOasthebaseline,adheringtothesamesteps
as Algorithm 1 but training the LLM with the standard DPO objective. Then we conduct a grid
searchoverhyperparameters,suchasthebatchsize,learningrate,anditerationnumber,toidentify
theoptimalsettingsfortheiterativeDPObaseline.Detailsonthehyperparametersandgridsearch
resultsareprovidedinAppendixC.WefollowthesebestsettingstotrainSELMforafaircomparison.
Inaddition,thetopchoiceforthebasemodelsofSELMareLLMsthatarefinetunedwithRLHF
afterSFT,sincetheyaretypicallymorecapablethanSFT-onlyandpertrainedmodels.Weconsider
twoseriesofLLMs:Zephyr[57]andLlama-3[38],todemonstratetherobustnessofSELM.Since
theofficialZephyr-7B-Œ≤ modelisfinetunedwithDPOonthesameUltraFeedbackdataset,toavoid
overoptimization,wechooseZephyr-7B-SFT1asthebasemodelandperform3iterationsofSELM
afterasingleiterationofstandardDPOtrainingonthefirstportionofthetrainingdata(wereferto
thismodelasZephyr-7B-DPO).ForLlama-3-8B-Instruct2thatisalreadyfinetunedwithRLHF,we
directlyapply3iterationsofSELMtraining.
5.2 ExperimentResults
WefirstreporttheperformanceofSELMandthebaselinesontheinstruction-followingchatbench-
marksAlpacaEval2.0[14]andMT-Bench[71]inTable1.WecanobservethatforAlpacaEval2.0,
SELMsignificantlyboostsZephyr-7B-SFTandLlama-3-8B-Instruct,achievinglength-controlled
(LC)winrateimprovementsof+16.24%and+11.75%,respectively.Thisenhancementresultsin
1https://huggingface.co/HuggingFaceH4/mistral-7b-sft-beta
2https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct
6AlpacaEval2.0 MT-Bench
Model LCWinRate WinRate Avg.len Avgerage 1stTurn 2ndTurn
Zephyr-7B-SFT 8.01 4.63 916 5.30 5.63 4.97
Zephyr-7B-DPO 15.41 14.44 1752 7.31 7.55 7.07
DPOIter1(Zephyr) 20.53 16.69 1598 7.53 7.81 7.25
DPOIter2(Zephyr) 22.12 19.82 1717 7.55 7.85 7.24
DPOIter3(Zephyr) 22.19(‚Üë14.18) 19.88 1717 7.46(‚Üë2.16) 7.85 7.06
SELMIter1(Zephyr) 20.52 17.23 1624 7.53 7.74 7.31
SELMIter2(Zephyr) 21.84 18.78 1665 7.61 7.85 7.38
SELMIter3(Zephyr) 24.25(‚Üë16.24) 21.05 1694 7.61(‚Üë2.31) 7.74 7.49
Llama-3-8B-Instruct 22.92 22.57 1899 7.93 8.47 7.38
DPOIter1(Llama3-It) 30.89 31.60 1979 8.07 8.44 7.70
DPOIter2(Llama3-It) 33.91 32.95 1939 7.99 8.39 7.60
DPOIter3(Llama3-It) 33.17(‚Üë10.25) 32.18 1930 8.18(‚Üë0.25) 8.60 7.77
SELMIter1(Llama3-It) 31.09 30.90 1956 8.09 8.57 7.61
SELMIter2(Llama3-It) 33.53 32.61 1919 8.18 8.69 7.66
SELMIter3(Llama3-It) 34.67(‚Üë11.75) 34.78 1948 8.25(‚Üë0.32) 8.53 7.98
SPIN 7.23 6.54 1426 6.54 6.94 6.14
Orca-2.5-SFT 10.76 6.99 1174 6.88 7.72 6.02
DNO(Orca-2.5-SFT) 22.59 24.97 2228 7.48 7.62 7.35
Mistral-7B-Instruct-v0.2 19.39 15.75 1565 7.51 7.78 7.25
SPPO(Mistral-it) 28.53 31.02 2163 7.59 7.84 7.34
Yi-34B-Chat 27.19 21.23 2123 7.90 - -
Llama-3-70B-Instruct 33.17 33.18 1919 9.01 9.21 8.80
GPT-4Turbo(04/09) 55.02 46.12 1802 9.19 9.38 9.00
Table1:ResultsonAlpacaEval2.0andMT-Bench.Namesinsidethebracketsarethebasemodels
thatarealignedbasedupon.Theredarrowsindicatetheincrementordecrementfromthebasemodel.
ComparedtoiterativeDPOandotheronlinealignmentbaselines,SELMgainsmoreimprovements
basedontheweakerZephyr-7B-SFTmodelandachievessuperiorperformancethatiscompetitive
withmuchlargerSOTAmodelswhenfinetunedfromLlama-3-8B-Instruct.
modelsthatarecompetitivewithorevensuperiortomuchlargerLLMs,suchasYi-34B-Chat[65]
andLlama-3-70B-Instruct.Forthemulti-turnMT-Bench,whichexhibitshighervariance,wereport
theaveragescoresofSELMandDPObaselinesacross3runs. WeobservethatSELMimproves
thescoresby+2.31and+0.32,respectively.Furthermore,theproposedmethodself-exploresand
enhances the model monotonically,with consistent performance improvements in each iteration.
Thisvalidatestherobustnessofouralgorithm.Comparedtootheriterativepost-trainingalgorithms,
suchasSPIN[7],DNO[50],andSPPO[61],SELMgainsmoreimprovementsonbothbenchmarks
whenusingtheweakerbasemodel(Zephyr-7B-SFT),andachievesthebestperformancewhenusing
Llama-3-8B-Instructasthebasemodel.
Figure2:Pairwiselength-controlledwinratescomparisonbetweenSELM,iterativeDPO,andbase
models on the AlpacaEval2.0 benchmark. Scores representthe LC win rates ofthe row models
againstthecolumnmodels.ModelspositionedinhigherrowshavehigherLCwinratesagainstthe
basemodelandthusbetterperformance.
7GSM8K HellaSwag ARC TruthfulQA EQ OBQA
Models Average
(8-sCoT) (10-s) (25-s) (0-s) (0-s) (10-s)
Zephyr-7B-SFT 43.8 82.2 57.4 43.6 39.1 35.4 50.3
Zephyr-7B-DPO 47.2 84.5 61.9 45.5 65.2 38.0 57.0
DPOIter1(Zephyr) 45.5 85.2 62.1 52.4 68.4 39.0 58.8
DPOIter2(Zephyr) 44.9 85.4 62.0 53.1 69.3 39.4 59.0
DPOIter3(Zephyr) 43.2 85.2 60.8 52.5 69.1 39.6 58.4
SELMIter1(Zephyr) 46.3 84.8 62.9 52.9 68.8 39.6 59.2
SELMIter2(Zephyr) 46.2 85.4 62.1 53.1 69.3 39.6 59.3
SELMIter3(Zephyr) 43.8 85.4 61.9 52.4 69.9 39.8 58.9
Llama-3-8B-Instruct 76.7 78.6 60.8 51.7 61.8 38.0 61.3
DPOIter1(Llama3-It) 78.5 81.7 63.9 55.5 64.1 42.6 64.4
DPOIter2(Llama3-It) 79.4 81.7 64.4 56.4 64.3 42.6 64.8
DPOIter3(Llama3-It) 80.1 81.7 64.1 56.5 64.1 42.6 64.8
SELMIter1(Llama3-It) 78.7 81.7 64.5 55.4 64.1 42.4 64.5
SELMIter2(Llama3-It) 79.3 81.8 64.7 56.5 64.2 42.6 64.9
SELMIter3(Llama3-It) 80.1 81.8 64.3 56.5 64.2 42.8 65.0
SPIN 44.7 85.9 65.9 55.6 54.4 39.6 57.7
Mistral-7B-Instruct-v0.2 43.4 85.3 63.4 67.5 65.9 41.2 61.1
SPPO(Mistral-it) 42.4 85.6 65.4 70.7 56.5 40.0 60.1
Table2:PerformancecomparisonbetweenSELMandthebaselinesonacademicmulti-choiceQA
benchmarksinstandardzero-shot,few-shot,andCoTsettings.Here,n-sreferston-shot.Theredand
bluetextsrepresentthebestandthesecond-bestresults.
WealsoconductpairwisecomparisonsbetweenSELM,iterativeDPO,andthebasemodelstovalidate
theeffectivenessofourmethod.TheresultsforAlpacaEval2.0areshowninFigure2.Weobserve
thatwiththesamenumberoftrainingiterationsanddata,SELMconsistentlyoutperformstheiterative
DPOcounterpart.Additionally,whenusingZephyr-7B-SFTasthebasemodel,SELMoutperforms
iterativeDPOevenwhenthelatteristrainedwithtwicethedata.
Beyond instruction-following benchmarks,we also evaluate SELM and the baselines on several
academicbenchmarks,includingGSM8K[10],HellaSwag[68],ARCchallenge[9],TruthfulQA[33],
EQ-Bench[45],andOpenBookQA(OBQA)[39].TobetterreflectthecapabilitiesofLLMs,weadopt
varioussettingsforthesebenchmarks,includingzero-shot,few-shot,andfew-shotChain-of-Thought
(CoT)settings.Theaccuracyresultsforthesemultiple-choiceQAbenchmarksareprovidedinTable
2.ItcanbeobservedthatbothourmethodandthebaselinescandegradeaftertheRLHFphaseon
somebenchmarks,whichisknownasthealignmenttax[2;40;30].Nevertheless,ourmethodisstill
abletoimprovethebasemodelsonmostofthebenchmarksandoffersthebestoverallperformance.
WenotethatSELMisoneoftheinstantiationsoftheproposedself-explorationobjectivein(1.1),with
reparameterizedrewardfunctionsandalgorithm-specificdesignsdescribedinSection3.2,suchasthe
datasetpartitionandupdaterule.However,thisobjectiveisnotrestrictedtothecurrentimplementation
andcanalsobedirectlyappliedtoanyotheronlinealignmentframework,withorwithoutaseparate
rewardmodel,regardlessofdifferencesinalgorithmdesigns.Thus,theproposedmethodisorthogonal
toandcanbeintegrateddirectlyintotherecentonlineRLHFworkflows[13;62;23]thatincorporate
additionaldelicatedesignswithcarefullycurateddatasets.
5.3 AblationStudy
Wefirstprovideablationstudiestobetterunderstandtheexplorativeoptimismterm.Webeginby
investigatingtheeffectoftheoptimismcoefficientŒ±.InFigure3(Left),weplottheLCwinratesof
SELMwhenusingZephyr-7B-SFTasthebasemodelfordifferentŒ±intheAlpacaEval2.0benchmark.
WefindthatsettingasmallŒ±,suchas0.0001,leadstoverysimilarbehaviorstotheiterativeDPO
(Œ± = 0) baseline,while SELM with a large Œ± may become overly optimistic and thus not very
effective.Theseresultsmeetourexpectations,suggestingthatpropervaluesofŒ±areessentialfor
achievingthebesttrade-offbetweenexplorationandexploitation.
Next,westudythedifferenceinrewarddistributionswithvaryingŒ±anditerations.Specifically,we
greedilysamplefromtheLLMusingpromptsfromtheholdouttestset(2kintotal)ofUltraFeedback
andgeneraterewardsfortheseresponseswithPairRM.Wethencalculatethefractionofdatathatlies
ineachpartitionofrewardvalues.TheresultsfordifferentŒ±valuesofSELMIter2(Zephyr)are
8Figure3:AblationontheoptimismcoefficientŒ±andthechangeoftherewarddistribution.Left:The
length-controlledwinratesofSELMwithdifferentŒ±onAlpacaEval2.0.Middle:Comparisonof
rewarddistributionsatiteration2withdifferentŒ±.Right:SELMinitiallyexploresandthenshiftsto
higher-rewardregionsasmoretrainingiterationsareperformed.
showninFigure3(Middle),whichindicatethatincreasingŒ±resultsindistributionsthataremore
concentratedinhigher-rewardregions.
Additionally,Figure3(Right)demonstratesthattherewarddis-
tributionshiftstotheright(higher)asmoretrainingiterations
areperformed.Thisshiftcorrespondstoaninitialexploration
phase,wheretheLLMgeneratesuncertainresponsesofvary-
ingquality,followedbyanexploitationphaseasfeedbackis
incorporatedandmoretrainingdataiscollected.
Wealsoconductablationstudiesontheimplicitrewardcaptured
by the SELM and DPO models. Recall that for both SELM
and DPO, the implicit reward takes the form of r (x,y) =
(cid:98)Œ∏
Œ≤(logœÄ (y | x)‚àílogœÄ (y | x)). We calculate the reward
Œ∏ ref
differencer (x,y)‚àír (x,y)foreachpromptxinthe
(cid:98)SELM (cid:98)DPO
UltraFeedback holdout test set. Here, we study the implicit
rewardofthegood(chosen)andbad(rejected)responses,so
y =y ory =y .Wethensorttherewarddifferenceandplot
w l Figure4:Differenceofimplicitre-
theresultsforZephyr-basedmodelsafteriteration1inFigure
wardbetweenSELMandDPOon
4. The plot clearly shows that for both chosen and rejected
thechosenandrejectedresponses.
responses,SELMproduceshigherimplicitrewardscompared
SELMassignsahigherimplicitre-
toDPO,aligningwiththeproposedoptimisticallybiasedself-
wardthanDPOforbothresponses.
explorationobjective.
6 Conclusion&FutureWork
Inthispaper,weintroducedanactivepreferenceelicitationmethodfortheonlinealignmentoflarge
languagemodels.Byincorporatinganoptimismtermintothereward-fittingobjective,theproposed
bilevelself-exploringobjectiveeffectivelybalancesbetweenexploitingobserveddataandexploring
potentiallyhigh-rewardregions.UnlikestandardonlineRLHFalgorithmsthatpassivelyexplorethe
responsespacebysamplingfromthetrainingLLM,whosesoleobjectiveismaximizingtheexpected
learnedreward,ourmethodactivelyseeksdiverseandhigh-qualityresponses.Thisself-exploration
mechanismhelpsmitigatetheriskofprematureconvergenceandoverfittingwhentherewardmodel
is onlylocallyaccurate. To optimize this bilevelobjective,we solve the inner-levelproblem and
reparameterizetherewardwiththeLLMpolicy,resultinginasimpleyetnoveliterativealignment
algorithmcalledSelf-ExploringLanguageModels(SELM).ComparedtoDPO,SELMimproves
theexplorationefficiencybyselectivelyfavoringresponseswithhighpotentialrewardsratherthan
indiscriminatelysamplingunseenresponses.
Ourexperiments,conductedwithZephyr-7B-SFTandLlama-3-8B-Instructmodels,demonstrated
the efficacy of SELM. Finetuning on the UltraFeedback dataset and leveraging PairRM for AI
feedback,SELMachievedsubstantialimprovementsinperformanceonAlpacaEval2.0,MT-Bench,
andacademicbenchmarks.TheseresultsunderscoretheabilityofSELMtoenhancethealignment
andcapabilitiesoflargelanguagemodelsbypromotingmorediverseandhigh-qualityresponses.
SincetheproposedtechniqueisorthogonaltotheadoptedonlineRLHFworkflow,itwillbeinteresting
9toapplyourmethodwithinmoresophisticatedalignmentframeworkswithadvanceddesigns,which
wewouldliketoleaveasfuturework.
References
[1] MarahAbdin,SamAdeJacobs,AmmarAhmadAwan,JyotiAneja,AhmedAwadallah,Hany
Awadalla,NguyenBach,AmitBahree,ArashBakhtiari,HarkiratBehl,etal. Phi-3technicalre-
port:Ahighlycapablelanguagemodellocallyonyourphone. arXivpreprintarXiv:2404.14219,
2024.
[2] AmandaAskell,YuntaoBai,AnnaChen,DawnDrain,DeepGanguli,TomHenighan,Andy
Jones,NicholasJoseph,BenMann,NovaDasSarma,etal. Agenerallanguageassistantasa
laboratoryforalignment. arXivpreprintarXiv:2112.00861,2021.
[3] PeterAuer.Usingconfidenceboundsforexploitation-explorationtrade-offs.JournalofMachine
LearningResearch,3(Nov):397‚Äì422,2002.
[4] MohammadGheshlaghiAzar,MarkRowland,BilalPiot,DanielGuo,DanieleCalandriello,
MichalValko,andRe¬¥miMunos. Ageneraltheoreticalparadigmtounderstandlearningfrom
humanpreferences. arXivpreprintarXiv:2310.12036,2023.
[5] YuntaoBai,AndyJones,KamalNdousse,AmandaAskell,AnnaChen,NovaDasSarma,Dawn
Drain,StanislavFort,DeepGanguli,TomHenighan,etal. Trainingahelpfulandharmless
assistantwithreinforcementlearningfromhumanfeedback. arXivpreprintarXiv:2204.05862,
2022.
[6] DanieleCalandriello,DanielGuo,RemiMunos,MarkRowland,YunhaoTang,BernardoAvila
Pires,PierreHarveyRichemond,CharlineLeLan,MichalValko,TianqiLiu,etal. Human
alignmentoflargelanguagemodelsthroughonlinepreferenceoptimisation. arXivpreprint
arXiv:2403.08635,2024.
[7] ZixiangChen,YiheDeng,HuizhuoYuan,KaixuanJi,andQuanquanGu. Self-playfine-tuning
convertsweaklanguagemodelstostronglanguagemodels. arXivpreprintarXiv:2401.01335,
2024.
[8] KurtlandChua,RobertoCalandra,RowanMcAllister,andSergeyLevine. Deepreinforcement
learning in a handful of trials using probabilistic dynamics models. Advances in neural
informationprocessingsystems,31,2018.
[9] PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,AshishSabharwal,CarissaSchoenick,
andOyvindTafjord. Thinkyou have solvedquestion answering? tryarc,the ai2 reasoning
challenge. arXivpreprintarXiv:1803.05457,2018.
[10] KarlCobbe,VineetKosaraju,MohammadBavarian,MarkChen,HeewooJun,LukaszKaiser,
MatthiasPlappert,JerryTworek,JacobHilton,ReiichiroNakano,etal. Trainingverifiersto
solvemathwordproblems. arXivpreprintarXiv:2110.14168,2021.
[11] GanquCui,LifanYuan,NingDing,GuanmingYao,WeiZhu,YuanNi,GuotongXie,Zhiyuan
Liu,andMaosongSun. Ultrafeedback:Boostinglanguagemodelswithhigh-qualityfeedback.
arXivpreprintarXiv:2310.01377,2023.
[12] NingDing,YulinChen,BokaiXu,YujiaQin,ZhiZheng,ShengdingHu,ZhiyuanLiu,Maosong
Sun,andBowenZhou. Enhancingchatlanguagemodelsbyscalinghigh-qualityinstructional
conversations. arXivpreprintarXiv:2305.14233,2023.
[13] HanzeDong,WeiXiong,BoPang,HaoxiangWang,HanZhao,YingboZhou,NanJiang,Doyen
Sahoo,CaimingXiong,andTongZhang. Rlhfworkflow:Fromrewardmodelingtoonlinerlhf.
arXive-prints,pagesarXiv‚Äì2405,2024.
[14] YannDubois,Bala¬¥zsGalambosi,PercyLiang,andTatsunoriBHashimoto. Length-controlled
alpacaeval:Asimplewaytodebiasautomaticevaluators. arXivpreprintarXiv:2404.04475,
2024.
10[15] VikranthDwaracherla,SeyedMohammadAsghari,BotaoHao,andBenjaminVanRoy.Efficient
explorationforllms. arXivpreprintarXiv:2402.00396,2024.
[16] ArpadEEloandSamSloan. Theratingofchessplayers:Pastandpresent. IshiPressInterna-
tional,1978.
[17] Kawin Ethayarajh,Winnie Xu,Niklas Muennighoff,Dan Jurafsky,andDouwe Kiela. Kto:
Modelalignmentasprospecttheoreticoptimization. arXivpreprintarXiv:2402.01306,2024.
[18] LeoGao,JohnSchulman,andJacobHilton. Scalinglawsforrewardmodeloveroptimization.
InInternationalConferenceonMachineLearning,pages10835‚Äì10866.PMLR,2023.
[19] Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts,
AbhishekSharma,AdityaSiddhant,AlexAhern,MiaosenWang,ChenjieGu,etal. Reinforced
self-training(rest)forlanguagemodeling. arXivpreprintarXiv:2308.08998,2023.
[20] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Ce¬¥sar Teodoro Mendes, Allie Del Giorno,
SivakanthGopi,Mojan Javaheripi,Piero Kauffmann,Gustavo de Rosa,Olli Saarikivi,et al.
Textbooksareallyouneed. arXivpreprintarXiv:2306.11644,2023.
[21] ShangminGuo,BiaoZhang,TianlinLiu,TianqiLiu,MishaKhalman,FelipeLlinares,Alexandre
Rame,ThomasMesnard,YaoZhao,BilalPiot,etal. Directlanguagemodelalignmentfrom
onlineaifeedback. arXivpreprintarXiv:2402.04792,2024.
[22] BradenHancockHoangTran,ChrisGlaze. Snorkel-mistral-pairrm-dpo. 2024.
[23] JianHu,XibinWu,WeixunWang,Xianyu,DehaoZhang,andYuCao.Openrlhf:Aneasy-to-use,
scalableandhigh-performancerlhfframework,2024.
[24] HamishIvison,YizhongWang,ValentinaPyatkin,NathanLambert,MatthewPeters,Pradeep
Dasigi,JoelJang,DavidWadden,NoahASmith,IzBeltagy,etal. Camelsinachangingclimate:
Enhancinglmadaptationwithtulu2. arXivpreprintarXiv:2311.10702,2023.
[25] Dongfu Jiang,Xiang Ren,and Bill Yuchen Lin. Llm-blender: Ensembling large language
modelswithpairwiserankingandgenerativefusion. arXivpreprintarXiv:2306.02561,2023.
[26] ChiJin,ZhuoranYang,ZhaoranWang,andMichaelIJordan. Provablyefficientreinforcement
learningwithlinearfunctionapproximation. InConferenceonlearningtheory,pages2137‚Äì
2143.PMLR,2020.
[27] MartinJosifoski,MarijaSakota,MaximePeyrard,andRobertWest. Exploitingasymmetry
forsynthetictrainingdatageneration:Synthieandthecaseofinformationextraction. arXiv
preprintarXiv:2303.04132,2023.
[28] Dahyun Kim,Yungi Kim,Wonho Song,Hyeonwoo Kim,Yunsu Kim,Sanghoon Kim,and
ChanjunPark. sdpo:Don‚Äôtuseyourdataallatonce. arXivpreprintarXiv:2403.19270,2024.
[29] AndreasKo¬®pf,YannicKilcher,DimitrivonRu¬®tte,SotirisAnagnostidis,ZhiRuiTam,Keith
Stevens,AbdullahBarhoum,DucNguyen,OliverStanley,Richa¬¥rdNagyfi,etal. Openassistant
conversations-democratizinglargelanguagemodelalignment. AdvancesinNeuralInformation
ProcessingSystems,36,2024.
[30] Shengzhi Li, Rongyu Lin, and Shichao Pei. Multi-modal preference alignment remedies
regressionofvisualinstructiontuningonlanguagemodel. arXivpreprintarXiv:2402.10884,
2024.
[31] Xian Li,Ping Yu,Chunting Zhou,Timo Schick,Luke Zettlemoyer,Omer Levy,Jason We-
ston, and Mike Lewis. Self-alignment with instruction backtranslation. arXiv preprint
arXiv:2308.06259,2023.
[32] YuanzhiLi,Se¬¥bastienBubeck,RonenEldan,AllieDelGiorno,SuriyaGunasekar,andYinTat
Lee. Textbooksareallyouneedii:phi-1.5technicalreport. arXivpreprintarXiv:2309.05463,
2023.
11[33] Stephanie Lin,JacobHilton,andOwain Evans. Truthfulqa: Measuring howmodels mimic
humanfalsehoods. arXivpreprintarXiv:2109.07958,2021.
[34] RuiboLiu,JerryWei,FangyuLiu,ChengleiSi,YanzheZhang,JinmengRao,StevenZheng,
DaiyiPeng,DiyiYang,DennyZhou,andAndrewM.Dai. Bestpracticesandlessonslearned
onsyntheticdataforlanguagemodels,2024.
[35] ZhihanLiu,MiaoLu,WeiXiong,HanZhong,HaoHu,ShenaoZhang,SiruiZheng,Zhuoran
Yang,and Zhaoran Wang. Maximize to explore: One objective function fusing estimation,
planning,andexploration. AdvancesinNeuralInformationProcessingSystems,36,2024.
[36] ZhihanLiu,MiaoLu,ShenaoZhang,BoyiLiu,HongyiGuo,YingxiangYang,JoseBlanchet,
andZhaoranWang. Provablymitigatingoveroptimizationinrlhf:Yoursftlossisimplicitlyan
adversarialregularizer,2024.
[37] Xiuyuan Lu andBenjamin Van Roy. Ensemble sampling. Advances in neuralinformation
processingsystems,30,2017.
[38] Meta. Introducingmetallama3:Themostcapableopenlyavailablellmtodate. 2024.
[39] TodorMihaylov,PeterClark,TusharKhot,andAshishSabharwal. Canasuitofarmorconduct
electricity?anewdatasetforopenbookquestionanswering. arXivpreprintarXiv:1809.02789,
2018.
[40] MichaelNoukhovitch,SamuelLavoie,FlorianStrub,andAaronCCourville. Languagemodel
alignmentwithelasticreset. AdvancesinNeuralInformationProcessingSystems,36,2024.
[41] IanOsband,DanielRusso,andBenjaminVanRoy. (more)efficientreinforcementlearningvia
posteriorsampling. AdvancesinNeuralInformationProcessingSystems,26,2013.
[42] IanOsband,ZhengWen,SeyedMohammadAsghari,VikranthDwaracherla,MortezaIbrahimi,
XiuyuanLu,andBenjaminVanRoy. Approximatethompsonsamplingviaepistemicneural
networks. InUncertaintyinArtificialIntelligence,pages1586‚Äì1595.PMLR,2023.
[43] IanOsband,ZhengWen,SeyedMohammadAsghari,VikranthDwaracherla,MortezaIbrahimi,
XiuyuanLu,andBenjaminVanRoy. Epistemicneuralnetworks. AdvancesinNeuralInforma-
tionProcessingSystems,36,2024.
[44] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,
ChongZhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Traininglanguagemodelsto
followinstructionswithhumanfeedback. Advancesinneuralinformationprocessingsystems,
35:27730‚Äì27744,2022.
[45] SamuelJPaech. Eq-bench:Anemotionalintelligencebenchmarkforlargelanguagemodels.
arXivpreprintarXiv:2312.06281,2023.
[46] ArkaPal,DeepKarkhanis,SamuelDooley,ManleyRoberts,SiddarthaNaidu,andColinWhite.
Smaug: Fixing failure modes of preference optimisation with dpo-positive. arXiv preprint
arXiv:2402.13228,2024.
[47] BaolinPeng,ChunyuanLi,PengchengHe,MichelGalley,andJianfengGao. Instructiontuning
withgpt-4. arXivpreprintarXiv:2304.03277,2023.
[48] RafaelRafailov,JoeyHejna,RyanPark,andChelseaFinn. Fromrtoq‚àó:Yourlanguagemodel
issecretlyaq-function. arXivpreprintarXiv:2404.12358,2024.
[49] RafaelRafailov,ArchitSharma,EricMitchell,ChristopherDManning,StefanoErmon,and
ChelseaFinn. Directpreferenceoptimization:Yourlanguagemodelissecretlyarewardmodel.
AdvancesinNeuralInformationProcessingSystems,36,2024.
[50] CorbyRosset,Ching-AnCheng,ArindamMitra,MichaelSantacroce,AhmedAwadallah,and
Tengyang Xie. Direct nash optimization: Teaching language models to self-improve with
generalpreferences. arXivpreprintarXiv:2404.03715,2024.
12[51] DanielRussoandBenjaminVanRoy.Eluderdimensionandthesamplecomplexityofoptimistic
exploration. AdvancesinNeuralInformationProcessingSystems,26,2013.
[52] MalcolmStrens. Abayesianframeworkforreinforcementlearning. InICML,volume2000,
pages943‚Äì950,2000.
[53] ZhiqingSun,YikangShen,QinhongZhou,HongxinZhang,ZhenfangChen,DavidCox,Yiming
Yang,andChuangGan. Principle-drivenself-alignmentoflanguagemodelsfromscratchwith
minimalhumansupervision. AdvancesinNeuralInformationProcessingSystems,36,2024.
[54] Yunhao Tang,Daniel Zhaohan Guo,Zeyu Zheng,Daniele Calandriello,Yuan Cao,Eugene
Tarassov,Re¬¥miMunos,BernardoA¬¥vilaPires,MichalValko,YongCheng,etal. Understand-
ing the performance gap between online and offline alignment algorithms. arXiv preprint
arXiv:2405.08448,2024.
[55] RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,XuechenLi,CarlosGuestrin,Percy
Liang,andTatsunoriB.Hashimoto. Stanfordalpaca:Aninstruction-followingllamamodel.
https://github.com/tatsu-lab/stanford_alpaca,2023.
[56] LewisTunstall,EdwardBeeching,NathanLambert,NazneenRajani,ShengyiHuang,Kashif
Rasul,AlexanderM.Rush,andThomasWolf. Thealignmenthandbook. https://github.
com/huggingface/alignment-handbook,2023.
[57] LewisTunstall,EdwardBeeching,NathanLambert,NazneenRajani,KashifRasul,Younes
Belkada,ShengyiHuang,LeandrovonWerra,Cle¬¥mentineFourrier,NathanHabib,etal. Zephyr:
Directdistillationoflmalignment. arXivpreprintarXiv:2310.16944,2023.
[58] XiyaoWang,JiuhaiChen,ZhaoyangWang,YuhangZhou,YiyangZhou,HuaxiuYao,Tianyi
Zhou,Tom Goldstein,Parminder Bhatia,Furong Huang,and Cao Xiao. Enhancing visual-
languagemodalityalignmentinlargevisionlanguagemodelsviaself-improvement,2024.
[59] YizhongWang,HamishIvison,PradeepDasigi,JackHessel,TusharKhot,KhyathiChandu,
DavidWadden,KelseyMacMillan,NoahASmith,IzBeltagy,etal. Howfarcancamelsgo?
exploringthestateofinstructiontuningonopenresources. AdvancesinNeuralInformation
ProcessingSystems,36,2024.
[60] ShengguangWu,KemingLu,BenfengXu,JunyangLin,QiSu,andChangZhou. Self-evolved
diversedatasamplingforefficientinstructiontuning. arXivpreprintarXiv:2311.08182,2023.
[61] YueWu,ZhiqingSun,HuizhuoYuan,KaixuanJi,YimingYang,andQuanquanGu. Self-play
preferenceoptimizationforlanguagemodelalignment. arXivpreprintarXiv:2405.00675,2024.
[62] Wei Xiong,Hanze Dong,Chenlu Ye,Han Zhong,Nan Jiang,andTong Zhang. Gibbs sam-
pling from human feedback: A provable kl-constrained framework forrlhf. arXiv preprint
arXiv:2312.11456,2023.
[63] Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston. Some things are more
cringe than others: Preference optimization with the pairwise cringe loss. arXiv preprint
arXiv:2312.16682,2023.
[64] ShushengXu,WeiFu,JiaxuanGao,WenjieYe,WeilinLiu,ZhiyuMei,GuangjuWang,Chao
Yu,andYiWu. Isdposuperiortoppoforllmalignment?acomprehensivestudy. arXivpreprint
arXiv:2404.10719,2024.
[65] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li,
Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01. ai.
arXivpreprintarXiv:2403.04652,2024.
[66] WeizheYuan,RichardYuanzhePang,KyunghyunCho,SainbayarSukhbaatar,JingXu,and
JasonWeston. Self-rewardinglanguagemodels. arXivpreprintarXiv:2401.10020,2024.
[67] RichardYuanzhePang,WeizheYuan,KyunghyunCho,HeHe,SainbayarSukhbaatar,andJason
Weston. Iterativereasoningpreferenceoptimization. arXive-prints,pagesarXiv‚Äì2404,2024.
13[68] RowanZellers,AriHoltzman,YonatanBisk,AliFarhadi,andYejinChoi. Hellaswag:Cana
machinereallyfinishyoursentence? arXivpreprintarXiv:1905.07830,2019.
[69] ShenaoZhang. Conservativedualpolicyoptimizationforefficientmodel-basedreinforcement
learning. Advancesinneuralinformationprocessingsystems,35:25450‚Äì25463,2022.
[70] YaoZhao,RishabhJoshi,TianqiLiu,MishaKhalman,MohammadSaleh,andPeterJLiu. Slic-
hf:Sequencelikelihoodcalibrationwithhumanfeedback. arXivpreprintarXiv:2305.10425,
2023.
[71] LianminZheng,Wei-LinChiang,YingSheng,SiyuanZhuang,ZhanghaoWu,YonghaoZhuang,
ZiLin,ZhuohanLi,DachengLi,EricXing,etal. Judgingllm-as-a-judgewithmt-benchand
chatbotarena. AdvancesinNeuralInformationProcessingSystems,36,2024.
[72] BanghuaZhu,EvanFrick,TianhaoWu,HanlinZhu,andJiantaoJiao. Starling-7b:Improving
llmhelpfulnessandharmlessnesswithrlaif,November2023.
14A DerivationsinSection3.1
We begin by deriving (3.2). The solution for the inner-level optimization problem of (3.1) is as
follows:
(cid:104) (cid:105)
maxF(œÄ;r)=maxE r(x,y)‚àír(x,y‚Ä≤) ‚àíŒ≤D (œÄ||œÄ )
œÄ œÄ
x‚àº yD ‚Ä≤‚àºt, œÄy re‚àº f(œÄ ¬∑|( x¬∑ )|x) KL ref
=E (cid:104) Œ≤logE (cid:2) exp(r(x,y)/Œ≤)(cid:3)(cid:105) ‚àíE (cid:2) r(x,y‚Ä≤)(cid:3) (A.1)
x‚àºDt y‚àºœÄref(¬∑|x) x‚àºDt,y‚Ä≤‚àºœÄref(¬∑|x)
Whentherewardrisreparameterizedbyr (x,y)=Œ≤(logœÄ (y |x)‚àílogœÄ (y |x)),wehavethat
(cid:98)Œ∏ Œ∏ ref
thefirsttermin(A.1)is0.Thebilevelobjective(3.1)thenbecomes
max‚àíL (r;D )‚àíŒ±E (cid:2) r(x,y‚Ä≤)(cid:3) .
r
lr t x‚àºD,y‚Ä≤‚àºœÄref(¬∑|x)
ByreparameterizingtherewardwiththeLLM,weobtainthedesiredresultsin(3.2).
Thenweprovidethederivationof(3.3).Weprimarilyconsiderthegradientofthenewlyincorporated
termE [logœÄ (y |x)].Specifically,wehave
x‚àºD,y‚àºœÄref(¬∑|x) Œ∏
‚àá E (cid:2) logœÄ (y |x)(cid:3) =E (cid:104)(cid:88) œÄ (y |x)‚àá logœÄ (y |x)(cid:105)
Œ∏ x‚àºD,y‚àºœÄref(¬∑|x) Œ∏ x‚àºD ref Œ∏ Œ∏
y
(cid:104)œÄ (y |x) (cid:105)
=E ref ‚àá logœÄ (y |x)
x‚àºD,y‚àºœÄŒ∏ œÄ (y |x) Œ∏ Œ∏
Œ∏
=E (cid:104) exp(cid:0) ‚àír (x.y)/Œ≤(cid:1) ‚àá logœÄ (y |x)(cid:105) .
x‚àºD,y‚àºœÄŒ∏ (cid:98)Œ∏ Œ∏ Œ∏
ForthederivationoftheDPOgradient‚àá L (œÄ ;D ),wereferthereadersto[49].
Œ∏ DPO Œ∏ t
B ProofofTheorem3.1
Proof. ThesolutiontotheKL-constrainedrewardminimizationobjective(3.4)is
œÄmin(y |x)=œÄ (y |x)exp(cid:0) ‚àír (x,y)/Œ≤(cid:1) /Z(x),
œÅ œÅ (cid:98)œÅ
whereZ(x)=(cid:80) œÄ (y |x)exp(‚àír (x,y)/Œ≤)=1.ThenwehaveœÄmin(y |x)=œÄ (y |x),i.e.,
y œÅ (cid:98)œÅ œÅ ref
thereferencepolicyœÄ achievesthelowestimplicitrewardreparameterizedbyanyœÅ.
ref
C ExperimentSetup
Inexperiments,weusetheAlignmentHandbook[56]frameworkasourcodebase.Wefindthebest
hyperparametersettingsbyconductingagridsearchovertheiterationnumber,batchsize,learning
rate,andlabelupdaterulefortheiterativeDPObaseline.TheresultsfortheZephyr-basedmodels
areshowninFigure5.Specifically,wefindthatusingthesameamountofdata,updatingthemodel
toomanyiterationscanleadtoinstability.Sowesettheiterationnumberto3forLlama3-It-based
andZephyr-basedmodels(excludingthefirstiterationofDPOtraining).Besides,weobservethat
choosingdifferentbatchsizeshasalargeeffectonthemodels‚Äôperformanceandtheoptimalbatchsize
heavilydependsonthemodelarchitecture.Inexperiments,wesetthebatchsizeto256and128for
theZephyr-basedandLlama3-It-basedmodels,respectively.Forthelearningrate,weconsiderthree
designchoices:cycliclearningratewithconstantcycleamplitude,linearlydecayedcycleamplitude,
anddecayedcycleamplitudeatthelastiteration.Wefindthatadecayingcycleamplitudeperforms
betterthanconstantamplitudesingeneral.Thus,forZephyr-basedmodels,wesetthelearningto
5e‚àí7forthefirstthreeiterationsand1e‚àí7forthelastiteration.Ineachiteration,thewarmupratio
is0.1.ForLlama3-It-basedmodels,weusealinearlydecayedlearningratefrom5e‚àí7to1e‚àí7
within3iterationswiththesamewarmupratio.Wealsotesttwoupdatewaysforthepreferencedata.
Oneistoranky ,y ,y andkeepthebestandworstresponsesintheupdateddataset,whichisthe
w l ref
settingthatisdescribedinthemainpaper.Theotheristocomparey andy andreplacethechosen
w ref
orrejectedresponsebyy basedonthecomparisonresult.Wefindthattheformerdesignperforms
ref
15Figure5:AblationoftheiterativeDPObaseline.Weconductagridsearchovertheiterationnumber,
batchsize,learningrate,anddesignsofthedatasetupdaterule.
betterthanthelatter.WealsocomparedwithstepwiseDPO[28],whichupdatesthereferencemodel
at each iteration but uses the original dataset instead of the updated one. This demonstrates that
exploringandcollectingnewdataisnecessary.
FortheproposedSELMmethod,wefollowtheabovehyperparametersettingsforafaircomparison.
TheoptimismcoefficientŒ±issearchedover0.005,0.001,0.0005,and0.0001andisselectedbased
on the average external reward on the holdout test set of UltraFeedback. We set Œ± = 0.001 for
Zephyr-basedSELMandŒ±=0.0001forLlama3-It-basedSELM.FortrainingSELMbasedonother
models,werecommendsettingŒ±=0.005or0.001asitshowsminimalsensitivitytovariations.
16