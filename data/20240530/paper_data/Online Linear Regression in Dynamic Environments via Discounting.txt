Online Linear Regression in Dynamic Environments via Discounting
AndrewJacobsen1 AshokCutkosky2
Abstract alwayspossibletomakepredictionsy f x foranyar-
t t t
bitrarytransformationf Rd →R,forinstancebysetting
We develop algorithms for online linear re- w f x x x 2. t ̂ = ( )
gression which achieve optimal static and dy- t t t t t
∶
namic regret guarantees even in the complete The=classical measureofthe learner’sperformancein this
( ) /∥ ∥
absence of prior knowledge. We present a settingisregret,thecumulativepredictionerrorrelativeto
novel analysis showing that a discounted vari- somefixedbenchmarkpointu Rd:
ant of the Vovk-Azoury-Warmuth forecaster
achieves dynamic regret of the form R u T ∈
T
O dlog T dPγ u T , where Pγ u is a R T u ℓ t w t ℓ t u .
T T t=1
measure of va√riability of the comparat(or)se≤- =∑
( ) ( )− ( )
qu(ence,a(nd)s∨howthat(the)di)scountfactor(ac)hiev- Noticethatthisperformancemeasurecanonlyproperlyre-
ingthisresultcanbelearnedon-the-fly.Weshow flect predictionaccuracywhen there existsa fixed u Rd
that thisresult is optimalby providinga match- whichpredictswellonaverage. Forexample,thismayoc-
∈
ing lower bound. We also extend our results to cur when when the x t,y t pairs are all generated i.i.d.
strongly-adaptiveguaranteeswhichholdoverev- from some well-behaved distribution. However, in many
erysub-interval a,b 1,T simultaneously.
truestreamingsetting(stheda)ta-generatingdistributionmay
changeovertime dueto changesin the environment. Dy-
[ ]⊆[ ] namicregretattemptstomodelsuchsettingsbycomparing
1. OnlineLinear Regression againstasequenceofcomparatorsu u ,...,u :
1 T
This paper presents new techniques and analyses for T = ( )
R u ℓ w ℓ u .
online linear regression, a variant of the classic least- T t t t t
t=1
squares regression problem tailored to streaming data =∑
( ) ( )− ( )
(Azoury&Warmuth, 2001; Vovk, 2001; Orabonaetal., Noticethatdynamicregretcapturestheusualnotionofre-
2015;Fosteretal.,2016). Formally,considerT roundsof gret (referred to as static regret) as a special case by set-
interactionbetweenalearnerandanenvironment,inwhich ting u 1 ... u T. Our goal in this work is to make fa-
learner’sobjectiveistoaccuratelypredictsomeobservable vorable dynamic regret guarantees even in the complete
= =
target signal y R before it’s revealed. On each round, absence of any prior knowledge of the underlying data-
t
a vectorof featuresx Rd is first revealed, representing generatingprocess. Naturally, because such an algorithm
t
thecontextofthe∈environmentatthestartoftheround,and leveragesnopriorknowledge,itnecessarilymustbeadap-
thelearnerpredictsy ∈x ,w bymeansofaweightvec- tive to all problem-dependentquantitieswithoutrequiring
t t t
torw Rd. Thesignaly Rd isthenobserved,andthe anyinstance-specifichyperparametertuning.
t = t
learner incurs a loŝs pro⟨portio⟩nal to the prediction error,
∈ ∈ Contributions. In this work we achieve the goal laid
ℓ w 1 y x ,w 2. Sincew isallowedtodepend
t t 2 t t t t outaboveanddevelopthefirstalgorithmsforonlinelinear
on x , this protocol is sometimes referred to as improper
t = regression that require no prior knowledge about the data
on(line)regre(ssio−n⟨,asthe⟩l)earnerisabletomakepredictions
stream, yet still make strong performance guarantees. In
outside of the class of linear models. Indeed, since x is
t particular,ourcontributionsareasfollows:
revealedbeforethelearnermustmaketheirprediction,itis
1Department of Computing Science, University of Alberta, • We show that even in the absence of any boundedness
Edmonton,Canada2DepartmentofElectricalandComputerEn- assumptions,adiscountedvariantoftheVAWforecaster
gineering, Boston University, Boston, Massachussetts. Corre- with a well-chosen discount factor achieves dynamic
spondenceto:AndrewJacobsen<ajjacobs@ualberta.ca>. regret R u O dlog T dPγ u T , where
T T
Proceedings of the 41st International Conference on Machine
P Tγ u is a me ≤asure of variabilit√y of the comparator
Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 sequence ((i.e). the m(agnit(ude) ∨of P Tγ u( i)s r)elated to
bytheauthor(s). how(d)rasticallythecomparatorchangesovertime). We
( )
1
4202
yaM
92
]GL.sc[
1v57191.5042:viXraOnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
also obtainsmall-loss guaranteesof the formR u form to be applied to the online linear regression prob-
T
O dlog T dP Tγ u T t=1ℓ
t
u
t
,sothatthe (al )go≤- l re itm hm.
s
fF oo rr gi en nst ea rn ac le o, nm lina eny rep gr ri eo sr siow nor sk es ttid ne gv sel to hp ata clg ao p-
-
rithm will auto√matically perform better on “easy” data
( ( )∨ ( )∑ ( )) ture linear regression as a special case (Orabonaetal.,
wherethecomparatorhaslowloss.
2015; Luoetal., 2016; Kotłowski, 2017; Kempkaetal.,
• We provide a matching lower bound of the form 2019; Mhammedi&Koolen, 2020). Even more gener-
R u Ω dlog T dTPγ u , demonstrating ally, one might hope to approach online linear regression
T T
via reduction to a more general online convex optimiza-
optimalityofthediscounte√dVAWforecaster.
≥
( ) ( ( )∨ ( )) tionsetting (Zhangetal., 2018;Yuan&Lamperski, 2019;
• Weshowthatthediscountfactorsrequiredtoobtainthe
Zhaoetal., 2020; Babyetal., 2021; Baby&Wang, 2021;
results in the first pointcan be learned on-the-fly,lead-
Luoetal.,2022;Jacobsen&Cutkosky,2022;Zhangetal.,
ing to algorithms that make guarantees matching our
2023;Zhaoetal.,2024). Unfortunately,alloftheseworks
lowerbound. Moreover,weshowhowtoextendourap-
require additionalboundednessassumptions on the losses
proach to achieve bounds of a similar form over every
such as Lipschitzness or exp-concavity,both of which re-
sub-interval a,b 1,T simultaneously. These are
quire a bounded domain in the context of losses ℓ w
t
t ih ne thfi ers at bs str eo nn cg [el oy f-a ]ad lla ⊆p bt o[iv ue ndg e]u da nra en sste ae ss sh ua mv pe tb ioe ne sn .achieved 21 y
t
x t,w 2. Yetassumingaboundeddomainamount =s
amounts to having strong prior knowledge that the(co)m-
pa(rato−r⟨seque⟩)nce u u ,...,u lies entirely within
1 T
1.1.RelatedWorks someboundedsubsetW Rd,whichmustbeknownand
=
accountedforapriorifo(rtheguaran)teestohold.
Despite being a well-studied problem setting, there are ⊂
no prior works which approach online linear regression One recent exception to the limitations mentioned above
with sufficientgeneralityto be consideredfree from prior is the work of Jacobsen&Cutkosky (2023). They de-
knowledge. The closest works to our own are Vovk velop an approach that can be applied to any loss func-
(2001);Azoury&Warmuth(2001);Orabonaetal.(2015); tions satisfying ℓ w G L w for some non-
t t t
Mayoetal. (2022), each of which consider the same im- negative constants G and L , and hence could be ap-
properonlinelearningsettingas thisworkandpresental- plied in our sett∥in∇g f(t or)G∥ ≤ t y+ x ∥ a∥nd L x 2.
t t t t t
gorithmsthat can be run in an unboundeddomain (hence Their algorithm achieves a dynamic regret guarantee on
= =
requiring no prior knowledge about the comparator) and the order of O M3/2 P T w∣he∣r∥e M∥ max u∥ a∥nd
T t t
withoutanypriorknowledgeofthedatastream. Yetthese P
T
T
t=2
u
t
u t−1√. However, their =approach fails to
worksprovideguaranteesthatonlyholdforstaticregret— achievelogarith(micregretag)ainsta fixedcompar∥ator∥,and
= ∑
thedynamicregretofthealgorithmsintheseworksmaybe their approa∥ch r−equire∥s prior knowledge of a G G
max t
arbitrarilybad.Inthissense,deployinganysuchalgorithm andL L forallt. Moreovertheirapproachrequires
max t ≥
implicitlyrequiresratherstrongpriorknowledge: thatthe O dTlog T per-roundcomputation,makingitinappro-
≥
data-generatingdistributionisnotchangingovertime. priate for many of the long-runningproblemswhere non-
sta(tionarity(n)a)turallyemergesduetosubtlechangesinthe
A closely related problem setting which does account
environmentovertime.
for potential non-stationarity is the classic filtering prob-
lem (Kalman, 1960; Simon, 2006; Kozdobaetal., 2019;
1.2.Notations
Hazan&Singh,2022). Thisproblemsettingassumesthat
the y t are generated from a dynamical system of a spe- Wedefineℓ 0 w λ 2 w 2 2,sothatupdatescanbewritten
cific form, and seeks to estimate the hidden state of the purelyin termsoflossesℓ . Givenapositivedefinitema-
= t
system. Thus, these works revolve around strong struc- trixM,thew(eig)htedn∥orm∥ w.r.tM is w w,Mw .
M
tural assumptions about the data-generating process from Foranysequencea 1,a 2,..., we denotea ma=x√max
t
a t.
the outset. Similarly, there is a large literature on adap- Positivethresholdingis denotedas ∥ ∥ max ⟨,0 . Th⟩e
tive filtering which seeks to solve the filtering problem Bregman divergence w.r.t. a differe⋅ n+ tiable fu= nc⋅ tion ψ∣ i∣s
=
without a priori knowledge of the system (Kivinenetal., D xy ψ x ψ y ψ y ,[x] y . We{deno}tea
ψ
2006; Hazanetal., 2017; 2018; Rashidinejadetal., 2020; b max a,b an− da b− m∇ in a,b ,− N 1,...,N∨ ,
Tsiamis&Pappas, 2022; Ghaietal., 2020), though these N ( ∣0),1= ,..(. )deno(t∧ es)th⟨e na(tur)al num⟩bers, and 1 is
= = = N
worksstill implicitly requirepriorknowledgethatthe un- the N-dim{ens}ionalvectorof one{s. W}e[use]the{short-han}d
=
derlyingdynamicalsystem isfromsomespecific class, as Clip { y }y a bandthecompressedsumnotations
a,b
anyperformanceguaranteesmayotherwisefailtohold. g j g and∨ g 2∧ b g 2. TheN-dimensional
i∶j [ ]t(=i)t= ( )a∶b t=a t
Alternatively, there are several related problem settings simplex is denoted ∆ . O hides constant factors and
that one might hope to leverage results from, but these O = h∑ idesconstan∥ta∥nN dl= og∑ lo⋅ gf∥act∥ors.
all inevitably require additional assumptions of some ⋅ ()
̂()
2OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
2. The Vovk-Azoury-WarmuthForecaster sense,stationary.Becauseofthis,itspredictionscannotbe
trusted in the absence of prior knowledge,but ratheronly
Inthecontextofstaticregret,itiswellknownthattheop-
whenthepractitionerknowstheyaredealingwithdatathat
timalstrategyinourimproperonlinelinearregressionset-
can be reasonably predicted using only a single fixed hy-
ting is the Vovk-Azoury-Warmuth(VAW) forecaster, dis- pothesisu Rd. In the nextsection, we will see thatthis
coveredindependentlyby Azoury&Warmuth (2001) and
issuecanbealleviatedbyincorporatingasuitablerecency
Vovk(2001). Oneachround,thestandardVAWforecaster ∈
biastothestatisticsoftheVAWforecaster.
sets
t −1 t−1 3. DynamicRegret viaDiscounting
w λI x x⊺ y x . (1)
t s s s s
+
s=1 s=1 Despite makingstrongstatic regretguarantees,we saw in
= ∑ ∑
( ) theprevioussectionthatthestandardVAWforecastermay
The VAW forecaster is well-known for the following re- fail to attain low regret when competing against a time-
gret guarantee (Azoury&Warmuth, 2001; Vovk, 2001; varyingcomparator. Looselyspeaking,theproblemisthat
Orabonaetal.,2015). theVAWforecastertreatsalltime-stepsasequallyimpor-
Theorem2.1. Foranyu Rd andanysequences y T tant. Indeed, it can be shownthat VAW forecastercan be
t t=1
inRand x T inRd,theVAWforecasterguarantees understoodasupdating
t t=1 ∈
( )
R T u( λ) 2 u 2 2+ dma 2x ty t2 log 1 + T t=1 λdx t 2 2 , w t ar wg ∈m Rdin 1 2 w 2 Λt + st− =1 1ℓ s w ,
≤ ⎛ ∑ ∥ ∥ ⎞ = ∥ ∥ ∑ ( )
( ) ∥ ∥ whereΛ λI x x⊺.1 Thelatterterm t−1ℓ w forces
⎝ ⎠ t + t t s=1 s
Letusbrieflypausetoappreciatesomeofthesubtletiesof theVAWforecastertochooseawwhichbalancesallofthe
= ∑
this result, as it represents a very high standard of excel- losses encountered so-far. Yet in dynamic sce(nar)ios, the
lence in online learning. First, note that the result holds losses that contain the most-relevant information for pre-
using no prior knowledge about the data — there are no dictingy aretypicallytheonesthathavebeenobservedthe
t
underlying assumptions about how the features x or the mostrecently.Inordertomorecloselytracktheserecently-
t
targetsy aredistributed,thealgorithmrequiresnospecific observed losses, we make two modifications to the VAW
t
statisticsorboundssuchas y Y or x X,andtheal- forecaster. First, we incorporate a forgetting or discount
t t
gorithmworksinanunboundeddomain—arelativerarity factorγintothealgorithm’sstatistics,placinglessempha-
≤ ≤
inadversarialsettings. Yet∣des∣pitethis∥inc∥redibledegreeof sisonlossesobservedfarinthepast. Second,weallowthe
generality,theVAWforecasterboastsastronglogarithmic updatetoadditionallymakeuseofasequenceof“predicted
regretguarantee, whichcan be shown to be optimalupto labels”or“hints”y thatareavailablebeforewecommitto
t
constant factors (See, e.g., Cesa-Bianchi&Lugosi (2006, y . Intuitively, we would like our algorithm to do better
t
Theorem11.9)). Thus,theVAWforecasterachievesahar- wheny y . Lat̃er,we willprovidesomeconcreteways
t t
mony between theory and practice which is quite rare in t̂osety thatyieldstrongregretbounds.
t =
onlinelearning,requiringnoproblem-specificinformation ̃
ThevariantoftheVAWforecasterdescribedaboveispro-
orassumptionswhilestillguaranteeingoptimalregret. ̃
vided concretely in Algorithm 1. Observe that by un-
However, a major caveat to the above discussion is that rolling the recursions for θ and Σ , the update w
t t t
these favorable propertieshold only within the contextof Σ−1 y x γθ canbewritteninclosed-formas
t t t + t =
staticregret.ThedynamicregretoftheVAWforecastercan
bearbitrarilybadingeneral.Toseewhy,letusconsiderthe [̃ t] −1 t−1
w γtλI γt−sx x⊺ y x γ γt−1−sy x .
simplecasewhered 1andx 1forallt. Inthiscase, t s s t t s s
t + +
t wh he iV chAW appfo rore xc ia mst ae tr espr = ae ndic et ms py it ric= ax ltw avt eragλ e+ ot f − th1 e tt s a− = r1 1 gy es ts, Bys= et( tingγ s ∑ 1=1 andy t 0,) the[ ũ pdatepres ∑ c= i1 selyreduces] to
= = ∑
observed up to round t. It̂is easy to s(ee th)at any such Equation(1), so the discountedVAWforecasteris a strict
= =
prediction strategy can fail when competing with a time- generalization of thẽstandard VAW forecaster. Likewise,
varying comparator. For instance, if the first T 2 targets the following theorem shows that Algorithm 1 obtains a
are 1butthesecondhalfare 1,theVAWforecasterwill regretguarantee which capturesTheorem 2.1 as a special
quic− klyconvergeto predicting+ 1 in the first T /2rounds, case. ProofcanbefoundinAppendixA.2.
−
butwillbeunabletoquicklyadaptafterthechangeinthe
1The equivalence to Equation (1) is readily checked via the
latter T 2 rounds, leading to linear regretovera/ll. In this
first-orderoptimalitycondition,thoughthisclaimcanalsobede-
sense,theVAWforecasteractuallyimplicitlyrequiresquite rived asa special case of a more general claimProposition A.1
strong p/rior knowledge about the data: that it is, in some providedintheappendix.
3OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
Algorithm1:DiscountedVAWForecaster canrevealamoreexplicittrade-off. Weprovideproofofa
slightlymoregeneralstatementofthefollowinglemmain
Inputλ 0,γ 0,1
AppendixA.3.
Initializew 0,Σ λI,θ 0
> 1 ∈ 0 1
fort 1 T do ( ] Lemma 3.2. (simplified) Let ℓ 0,ℓ 1,...,ℓ T be arbitrary
UR Se=e ptc dΣe ai∶ tv
t
ee wf xe= a txtu
⊺
t
Σre +−s 1γx=
Σ
yt
t∈−
x1R ,d
ch
γo=
θosey
t
R
no
t
sn =- 0n γeg t−a st ℓiv se wfu .n Fc oti ro an ls l, t,γ
defi ∈ne
(0,1 ), and F tγ (w
)
=
= t t t t + t ̃ ∈ ∑ d¯γ u( ,v) t γt−s ℓ u ℓ v
Predict x t,= w
t
an[̃dobservey]
t
t =s ∑=0 t s′=0γt−s′ s − s +
I Sn ec tu θrlo ⟨ssℓ yt xw ⟩t γθ1 2 y t − x t,w t 2 andP Tγ u( ) T t=− 11d¯γ t∑u t+1,u t .[ The( n) foran( yV)] T 0,
end
t+1
=
t (t +)= t
( ⟨ ⟩) ( γT)−=1∑ Fγ u ( Fγ ) u log 1 V ≥
t t+1 − t t + γ T
t=1
Theorem 3.1. Let λ 0 and γ 0,1 . Then for any ∑ [ ( 1γ) γP Tγ (u ) +]1 − γγ V( T )
s de yq nu ae mn ic ce ru egretu R1,.. u., >u bT ouni dn eR dd a, b∈A ol v(g eo br yit ]hm1guarantees ≤ −
( )
= T The lemma bounds the variability term
( ) γ T−1 Fγ u Fγ u from Theorem 3.1 in
γ 2λ u
1
2
2+
d 2m tax( y)
t
−y
t
2log 1
+
∑T t=1γ λT d−t x t 2 2 mter
∑
em at s= s u1 ro e[f oat f(n ve at w r+ i1 a)bo i− n le ityt P ,(Tγ fot u r)]e. achTo
t
u len tde ur ssta fin rsd
t
t dh eis finn eew
a
⎛ ∥ ∥ ⎞
∥T−1∥ ( ̃) d T γ-exponentially-decaying(dis)tributionovertime-stepss t
w+
hγ
eret ∑= F1
γ[F wtγ (u t+ γ1
t)
λ−F wtγ 2(u t
)]
t+
⎝
2
γl to −g sℓ(1
/
wγ
)
.t ∑=1
(y t −y ̃t⎠ )2 Pas Tγp uγ
t
(s
a )s=
∑t
s′γ =0t− γs t−s′. Then, given γ we can expre ≤ss
t 2 2+ s=1 s ( ) d¯γ t ut+1,ut
The regret(de)c= ompos∥itio∥nob∑tained in Th(eo)rem 3.1 is ap-
( )
T−1 t
pealingfortworeasons. First,itcapturesTheorem2.1asa Pγ u pγ s ℓ u ℓ u
specialcase: settingγ 1,y 0,andu ... u u, T ³¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹t¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹s¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹·t+¹1¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹ −¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹s¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹t¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹+¹µ
t 1 T t=1s=0
thelasttwotermsoftheboundevaluatetozero,sothere- ( )= T∑ −1∑ ( )[ ( ) ( )]
gretisboundedby λ
2
u= 2 2+̃ d 2m= ax ty t2log = 1
+
= T t=1 λdxt= 2 2 , t=1E s pγ
t
ℓ s u t+1 −ℓ s u t + ,
whichispreciselytheguaranteepromisedbyTh∑eor∥ em∥ 2.1. = ∑ ∼ [( ( ) ( )) ]
∥ ∥ ( ) so each term of Pγ u is a measure of how different the
Second, the decomposition displays a clean separation of T
predictionerrorsofu andu areonaverageacross“re-
concerns. The terms in the first line are the unavoidable (t) t+1
cent” losses. The quantity Pγ u can also be naively re-
penaltiesassociatedwithstaticregret,whichareofcourse T
lated to the more common measure of variability — the
also unavoidablehere in the more generaldynamic regret ( )
setting.Inthesecondline,anypenaltiesincurredasaresult path-lengthP T⋅ T t=− 11 u t −u t+1 —asfollows:
∥∥
of a changing comparator sequence are captured entirely T−1=∑ ∥ ∥
b thy et th ee rmva dr li oa gbili 1ty γterm Tγ 1T t y=1F tγ
y
u 2t+ r1
epr−
esF entγ tsu at s, taw bih lii tl ye P Tγ u t=1m sax ∇ℓ s u t+1 u t −u t+1
penaltyincurredduetot= d1 is∑2 cout n− ting(t . ) ( ) ( )≤ m∑ ax ℓ∥ u ( P ⋅)∥∥ O max ∥ u P ⋅ .
( / )∑ ( ̃) t,s ∇ s t T t t T
Intuitively, the terms in the second line represent a track- ∥∥ ∥∥
≤ ∥ ( )∥ ≤ ( ∥ ∥ )
ing/stability trade-off: against a volatile comparator se- Thus, Pγ u is proportional to the usual path-length.
T
quence, we would ideally like to set the discount factor Note that ( a ) multiplicative penalty of max t u t is
γ to be small to control the variability penalty, yet this the same worst-case penalty that appears in prior
∥ ∥
willcomeattheexpenseofincreasingthestabilitypenalty works, even in bounded domains (Zhangetal., 2018;
dlog 1 γ T 1 y y 2. Initscurrentform,however, Jacobsen&Cutkosky, 2022; Zhangetal., 2023;
thistrade-offt i= s1 st2 illat − bitt mysterious. Thevariabilityterm Zhaoetal.,2024).
γ afuT t n= c(− 1 t1 i/ oF ntγ) o∑ fu γt+1 no− r( iF stγ itu ñ et) ceis ssn ao rit ln ye pc oe ss is ta ivri el ,y mm ao kn io nt go in tic dia fs
-
Lettingη 1−γ γ,Lemma3.2tellsusthatthatlattertermsof
Theorem3.1areboundedby
fic∑ ulttome(aning)fullya(nal)yzeorunderstandhowitrelates =
to the stability penalty dlog 1 γ T y y 2. If we d T
insteadconsideramode2 stupperbount d=1 ont th− eset termswe ηP Tγ u + 2η y t −y t 2,
∑ t=1
( / ) ( ̃) ( ) ∑( ̃)
4OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
a trade-off which can be optimized by choosing η corresponding to small γ∗ can be regarded as the worst-
d
2
T
t=1
yt−yt 2
toget =
case measures of variability. Yet as γ∗ approaches zero,
√ ∑P Tγ (u ̃) P Tγ∗ u approaches t t− =11 ℓ t u t+1 −ℓ t u t +, whichcan
ηPγ u ( ) d T y y 2 2¿dPγ u T 1 y y 2. b Ine dn e(a et du ,)r ta hl il sy pr ee nla at le td yo i∑ sth se imrs i[lt aa rn (id nar sd p)im ritea tos (u thre es )t]eo mfv pa or ri aa lb vil ait ry i-.
T ( )+ 2η t ∑=1 ( t − ̃t ) = Á ÁÀ T ( )t ∑=12 ( t − ̃t ) ability T t=− 11 ℓ t+1 u t −ℓ t u t studied in works such as
Campolongo&Orabona(2021);Besbesetal. (2015), and
T p theh in sis a cl hi ts y oiv o ce efr oy th fip ηsro bfom acri m ksi in nisg to; u ηnas avw o 1ie −γd γaw b ai l nl el dis sne oe lg vei in n ne gS r fae olc . rti γo P ,n l wu3 g e. g2 fii, nn dga c va en xib tye or∑ e fla ℓt te .d I∣ nto tt hh( ie sp s) ea nth s- el ,e( Png Tγt) ∗h∣ ∑uT t=− c11
an
∥u bt
e−
thu ot+ u1
g
∥htvi oa fc ao sn a-
relaxationofthemorecommonmeasuresofvariability.
thattheidealchoiceofdiscountfactorwouldbeaγ 0,1
= ( )
satisfying
∈
d T y y 2 [ ] 3.1.Small-lossBoundsviaSelf-confidentPredictions
γ √2 t=1 t − t .
=
d
2
T
t=1
∑y
t
−y(t 2 +̃)P Tγ u I Vn AWthe p fore rev cio asu ts ersec ct aio nn, aw che ies va ew t rh ea gt retthe scd ais lic no gunte ad
s
√ √
Noticeinparticular∑ thatγ( appẽa)rsonboth(sid)esoftheex- O dPγ∗ u T y y 2 , where y R is an
pression, and solving for this γ explicitly is non-trivialin √ T t=1 t − t t
general. Nonetheless, the followingtheoremshows thata
arb (itrary“hi (nt” )a ∑vaila (blebe ̃fo )re )observing ̃thet∈ruey t. One
particularly interesting option is to use the learner’s own
discountfactorsatisfyingtheaboveexpressionalwaysex-
predictionasahint,y x ,w . Thereasoningisthatany
ists,andifitcouldsomehowbeprovidedtothediscounted t t t
learnerachievinglowdynamicregretmustbepredictingy
VAWforecasterwe would achievedynamicregretmatch- = t
reasonablywellonãverag⟨e,soth⟩eirownpredictionswould
ingthelowerboundinSection3.2. Proofcanbefoundin
naturally make for reasonable predicted labels y . Con-
AppendixA.5. t
cretely, observe that by choosing y x ,w we would
t t t
T y 1h ,e .o .r .e ,m y T in3. R3. anF do ar nya sn ey ques ne cq eue unces u 1y ,1 ., .. .. ,. u, Ty T ina Rn dd , h anav de henT t c= e1 fy ot r− sy ot m2 eγ T t= 01 ,1y t t− hex gt ũ, aw r=t an⟨t2 eein2 T⟩hT t= ẽ 1 orℓ et mw 3t .3,
thereisadiscountfactorγ∗ 0,1 satisfying would∑ scal(eas̃) =∑ ( ⟨ ⟩) = ∑ ( )
̃ ̃ = ( ) ∈ [ ]
∈
d T[ y ] y 2 T T
γ∗ √2 t=1 t − t (2) R T u ℓ t w t −ℓ t u t O ¿dP Tγ u ℓ t w t ,
= √d 2 T t=1 y∑ t −y(t 2 +̃ √)P Tγ∗ u ( )=t ∑=1 ( ) ( )≤ ̃⎛ ⎜Á ÁÀ ( )t ∑=1 ( )⎞ ⎟
withwhichtheregre∑tofA(lgorit̃hm) 1isbound(ed)aboveby where the O hides the logarith⎝mic factor. Now notic⎠e
that T ℓ w⋅ appears on both sides of this inequal-
ity. Sot l= v1 int ̃ g(f)ot r T ℓ w , one finds that this implies
R T u O dmax y t y t 2log T ∑ ( ) t=1 t t
( )≤ ( t dP( γ∗− ũ)
T
y( y)
2
t ph la ut gg√
in
∑gT t t= h1 isℓ t
b
(aw ct k∑ )in≤toO
th
(( e√red ) gP reTγ
t
(bu
o
)u+ nd√
w
∑eT t h= a1 vℓ et (u t )), so
+¿ T t − t
Á ÁÀ ( )t ∑=1 ( ̃) ) R u O Pγ u Pγ u T ℓ u .
Whilethisresultispromising,itisimportanttonotethatit
T
⎛
T +¿
Á
T
t=1
t t
⎞
stillfallsshortofourdesiredgoalofprior-knowledge-free ( )≤ ̃⎜ ( ) ÁÀ ( )∑ ( )⎟
learning. Indeed, it seems that we require exceptionally Bounds of this for⎝m, sometimes called small-los⎠s or L∗
strong prior knowledge to choose the prescribed discount bounds,arehighlydesirablebecausetheynaturallyadaptto
factor γ∗ satisfying Equation (2). We will return to this thetotallossofthecomparatorsequence,potentiallylead-
issue in Section 4 to showthatthisdiscountfactorcan be ing to lower regret than more naive hint choices such as
learnedon-the-fly,resultinginalgorithmsthataretrulyfree y t y t−1ory t 0.
ofpriorknowledge.
Un=fortunately,=the above argument does not quite go
̃ ̃
Interestingly, the discount factor γ∗ in Theorem 3.3 can through because the now the logarithmic penalty in
helptoshedsomelightonthevariabilitymeasurePγ∗ u . Theorem 3.3 scales as O dmax t y t y t 2log T
Observe from the relation in Equation (2) that
γ∗T
can be
O dmax tℓ
t
w
t
log T ,andthisma− x tℓ
t
w
t
couldb =e
near zero only when Pγ∗ u is very large relative to( th) e arbitrarily large. Fortunat(ely, it tu(rns õu)t that(th)is)is-
T sue( can be r(eme)died(by))a simple trust-regi(on)argument.
stabilitypenalty,andlikew (ise ),ifγ∗ isnear1thenP Tγ∗ u On each round, instead of directly using hints y
t
mustbeinconsequentiallysmall. Inthissense,thePγ∗ u x ,w , we can constrain these predictions to be close
T ( ) t t ̃ =
( ) 5 ⟨ ⟩OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
to some arbitrary reference point yRef. In particular, in Theorem 3.5. For any d,T 1 and P,Y 0
t such that dP 2TY2, there is a sequence of losses
Lemma D.1 we show by clipping the learner’s predic-
≥ >
t
a
g
ai
n
rio
v
gtn
e
e
us
e
s
mt
u
(eo
y
s
nta tth−s
e
tu
oy
̃i btt ea )asb
2
btl
-
oe
≤o
vfi e-n
O
bte
o
(sr
t
tmv
h
ila
-
la
wl xc
o
yte
r
(iln
ey
dt
lt
se d:−r se ayd
atR
sa
e
ift
m
)sy
2
miltR
∧
a
ae
r
lf
ℓ
l-stw
l(e
ow
le
sf-
stc
b
)a
o
)n
pu.
eng
nT
du aiha
ln
tir ygs- uℓ t
(T
t=w
− 1=1)m= a (xu
s21
1 (,
ℓy
.
s.t .≤−
u,u t⟨
+Tx
1
)t, −w
s ℓa ⟩
s)ti2
s ufy
ta in nd
g
+
a
m
Pc ao
x
sm
ut
cp
∣y
ha tr ∣ta ht ao
t
≤r se Yque an nc de
∑ ≤
[ ( ) ( )]
O dP Tγ u T t=1ℓ t u t , while thelogarithmicpenalty R T u Ω dY2log T dP ¿dP T y t y t−1 2 .
√ + + −
0Oca .(n
d
Tmb he
a ex
fb
t
o(o
y
lu
lt2
on
) l
wod∑ ge ind gTa ths
e(
oO
b ry e)
(m)d sem
ftt
oa
in
lx
lg ot
w(yy
stRt e t− f
hiy stRe af
y b)t
o2
−
vl 1o eg
o a(r
rT
gy u)tR m)ef en≤
t
T ofhe l( ok ss) e ey≥ so sb
u⎛
⎜ ⎝s ce hrv ta ht ai ton ( i Ts ) t ℓhat
uthereÁ
ÁÀ i 0s ca alw
nt ∑= a2
by( es ea nss ue rq eu d) en
u⎞
⎟ ⎠c se
-
= = t=1 t t
thr(ough,demonst(rat)in)gthatthediscountedVAWforecaster
ing only T d different comparators. Indeed, letting the
can achieve small-loss bounds when using a well-chosen features x cycle thr∑ ough t(he )sta= ndard basis vectors, for
t
discountfactor. any sub-inte/rval s,s d 1,T we can choose a sin-
gle u Rd such that+ x ,u y for each t in the inter-
Theorem 3.4. Let yRef R be an arbitrary reference t ⊆ t
t val. Then by sam[plingth]e y [rand]omlyfrom Yσ,Yσ
point and let t y tR ∈ef M t,y tRef M t for M t for som∈ e σ 0,1 , w⟨e can⟩t e= nsure variability− of at most
max y
yRBef. Supposet− hatweappl+
yAlgorithm1with
hintss< yt s − Cls ip = x[,w . Then for any]sequence o= f O TY2σ2 d ∈ P but regret of at least Ω{TY2σ2 }
t B t t [ ]
l Ro dss ,e ths ̃eℓ r∣ 1 e, = i. s. a., γℓ T ○∣t a (n 0⟨d ,1an sy a⟩ts i)e sq fyu ie nn gce u u 1,...,u T in Ω ( √dP / T t)=1≤y t −y t−1 2 ∨dP . ( ) ≥
= ( ) No(tethat[t∑ heco(nditiondP) 2TY])2 capturesanaturalre-
∈ [ ] striction of the problem setting, in that for larger P the
≤
γ○ √d T t=1ℓ t u t
. (3)
v sta rc uu co teu ds
.
l Io nw dee er db ,o inun thd eR boT unu darycaΩ seT wY he2 redca Pn b 2e Tc Yon 2-
,
= √d T t=1ℓ t∑u t +( √P) Tγ○ u RTheo urem 3 Ω.5 tel dls Pus that( Ωthe)r de P≥ is a(s Ωequ Te) Ync 2e .s =u Yc eh
t
t th ha ist
Moreover,runningAl∑ gorithm( 1w) ithdiscou( nt)
γ○ γ for
boT undis ≥achie√vedagV aT inst =anycompa =ratorsequencebythe
γ 2d ensuresregretboundedaboveby ∨
min algo(rit)hmtha(tnaivelyp)redicts(0on)every(round):R
T
u
min 2d+1 T ℓ 0 ℓ u T 1y2 1TY2. Hence,nolower
=
bot u= n1 dt cane− xcet edt 1TY2t ,= s1 o2 itit ssu2 fficienttoconsider(co)m=
-
∑ 2 ≤∑ ≤
parator(seq)uenc(esw)ithvariabilityboundedbyP 2TY2.
R u O dPγmin u dmax y yRef 2log T
T T + t t − t If we instead consider a more restricted problem≤ setting
≤
( ) ( ( ) ( ) ( ) by assuming a bounded domain, then the losses ℓ w
T t
dPγ○ u ℓ u , 1 y x ,w 2 can be considered to be exp-concave.
+¿
Á
T
t=1
t t I2
n
tht is− settt
ing, Baby&Wang (2021) have shown
a(low)e=
r
ÁÀ ∑
( ) ( )) bo(undo⟨f ⟩)
Notice that unlike the previoussection, there are two dif- R T u Ω Y4 3d1 3T1 3C T2 3 , (4)
ferent variability penalties, Pγ○ u and Pγmin u . The / / / /
first mirrors the measure encoT untered in thT e last section. where C T (T t=− ) 11≥ u t( −u t−1 1. A natur)al question is
The other, Pγmin u , is rather a(nn)oying; in hig(h)dimen- whethersim =ilar ∑resultsalsoholdintheunboundedsetting,
T andhowtheycompar∥etoourlo∥werboundinTheorem3.5.
sions γ 2d is generally quite large, so Pγmin u
min 2d+1 T Note that even in the exp-concave setting, the bound in
may evaluate loss(es)at irrelevant comparators that are far
= Equation(4)isnotnecessarilytight.Indeed,Baby&Wang
away in time. Nevertheless, notice that this term satis(fie)s
(2021)provideanalgorithmwhichguarantees
P wT hγm ici hn u we will T t s= h− 1 o1 wma isx s unℓ as vou idt+ a1 ble− iℓ ns gu et ner+ a, la inpe Tn ha el oty - R u O Y4 3d3.5T1 3C2 3 ,
≤ ∑ T T
rem3.(5.) [ ( ) ( )]
/ / /
whichdoesnotm (a )tc≤ht̃h (elowerboundw.r.t )thedimension
d. In contrast, our lower bound in Theorem 3.5 matches
3.2.Dimension-dependentLowerBound
ourupperboundsinallinvolvedquantities(seeSections3
In this section, we show that the regret penal- and 4). Regardless, we also demonstratein AppendixF.1
ties observed in the previous sections are un- that the same O Y4 3d3.5T1 3C2 3 upper bound can be
T
avoidable without further assumptions. The fol- attained, even in unb/ounded/doma/ins, using the strongly-
lowing lower bound is proven in Appendix C.1. adaptiveguarañ te(esdevelopedinSec)tion5.
6OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
4. Learning the OptimalDiscountFactor
Algorithm2:Range-clippedMeta-algorithm
Recallthatourgoalfromtheoutsethasbeentodesignal- InputOnlinelearningalgorithms 1,..., N,
A A
gorithms that achieve favourable dynamic regret guaran- expertsalgorithm Meta overthesimplex∆ N.
A
tees using no prior knowledge. To this end, we showed Initialize Meta, 1,..., N,andsetM 1 0
A A A
in Section 3 that the discounted VAW forecaster can fort 1 T do
achieve dynamic regret guarantees of the form R u
Recei∶vefeaturesx
t
=
O dPγ u T dlog T where Pγ u is aT certain C= hoosereferencepointy tRef
mea√sureT
of
varia∨
bilityof the
comparatoT rsequence,(and)i≤
n
Define
Bt
y tRef −M t,y tRef +M
t
Se(ction 3.(2 w)e showed(th)a)t these penal(tie)s are unavoid- fori 1,. =..,N do
Sendx [to ]
able in general. However,these resultsholdunderthe as- G= etpret dictA ioi ny i x ,w i from
sumptionthatthelearnerchoosesdiscountratessatisfying t t t Ai
Computey i C()lip y i()
specialconditions(Equations(2)and(3)),eitherofwhich t =B ⟨t t ⟩
would require exceptionally strong prior knowlege to en- end () = ()
Getp ∆ from ( )
sure. Indeed, the learner would need to know the future! t N Meta
Inordertoachieveourgoaloflearninginthecompleteab- Predict ∈y t N i=1pA tiy ti andobservey t
senceofpriorknowledge,weneedtoensurethatthelearner UpdateM t=+1
∑
M t ∨(y t) −y tRef
can adequately guess or learn these ideal discount factors
=
on-the-fly. Sendℓ t w 1 2 y t −∣x t,w 2∣to Ai ∀i
Sendℓ y 1 ,...,ℓ y N to
Acommonwaytoachieveruntimeparameter-tuningofthis t ( t)= ( t ⟨ t ⟩) AMeta
end ( ) ( )
sort would be to run many instances of the algorithm for
( ) ( )
differentchoicesofγ inparallel,andcombinethepredic-
tions using a suitable meta-algorithm. In particular, sup-
posewehaveacollectionofalgorithms ,..., andon algorithm(Cesa-Bianchietal.,2012)toget:
1 N
eachroundwecanqueryeach foraA predictioA ny i R.
M telo lsre uo sve hr o, wsu tp opo cs oe mw be inh ea tv he esa emA pe ri t ea d- ia cl tg ioo nri sth bm yA ouM tpet uat ( tw t) inh ∈ gich a R TMeta e j ≤O log α (TN +1T ) ≤O m ta ,ixℓ t y t (i ) log NT ,
( ) ( ) ( ( ) ( ))
p from the N-dimensional simplex ∆ . Then by pre- asshowninTheoremE.1.However,justlikeinSection3.1,
t N
dicting y N p y i , 2 for any benchmark sequence the term max ℓ y i is hard to quantify and could be
t i=1 ti t t,i t t
u u 1,... =,u
∑T
anda(ny) j N wehave be arbitrarily large in()general. Fortunately the very same
clippingtrickused(inSe)ction3.1alsoworkshere: instead
= R(
u
T ℓ)
y ℓ
u∈ [ ] ofhavingthemeta-algorithmcombinetherawpredictions
T t t − t t y i , we can simply clip the predictions to a trust-region
t=1 t
( )=∑ T
ℓ
t(
y
t)
j
ℓ(
t
u)
t
T
ℓ t y t ℓ t y tj
a shr(o o)u wnd thaa tg thiv ee cn lir pe pf ie nr gen sc tre atp eo gi ynt dey ttR ae if l. edI in nL Ae lm gom ria thD m.3 2w ine
-
− + −
t=1 ( ) t=1 ( ) cursonlyanadditionalconstantpenaltyintheregret.Then,
=∑ ∑
( ) ( ) ( ) ( )
=∶RA Tj u =∶RM Teta ej usingLemmaD.1,usingtheseclippedpredictionsleadsto
wherethelas´ t¹¹¹¹¹ l¹¹¹¹ i¹¹¹¹¹ n¹¹¹¹¹¹¹¹¹ e¹¹¹¹¹¹¹¹¹¹¹¹¹ o¹¹¹¹¹¹¹¹¹ b¹¹¹¹¹¹¹¹¹ s¹¸ er(¹ v¹¹¹¹¹¹¹¹¹ e¹¹¹¹ )¹¹¹¹ s¹¹¹¹¹¹¹¹¹¹¹ t¹¹¹¹¹ h¹¹¹¹¹¹¹¹¹¹ a¹¹¹¹¹¹¹¹ t¹¹¶
y
tj´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹ x¹¹¹¹¹¹¹¹¹¹ t¹¹¹¹¹¹¹ ,¹¹¹¹¹¹¹ w¸ t(¹ j¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹ )¹¹¹¹¹ .¹¹¹¹¹¹¹¹¹¹¹¹ H¹¹¹¹¹¹¹¹¹¹¹¹¹ e¹¹¹¹¹¹¹ n¶
ce,
R TMeta e
j
O m tax y
t
−y tRef 2log NT .
we may achieveour goalif we ca(n)ensure 1)(th)at there is ≤
= Notethata p(ena)ltyof(asim(ilarorder)isalre(ady)p)resentin
aj N suchthat usesanear-optim⟨aldiscou⟩ntfactor
Aj theregretoftheVAWforecaster(e.g.Theorem3.1)sothis
γ , and 2) we can provide a meta-algorithm which guar-
j ∈ resultwillbe sufficientforourpurposes. Overall, thefol-
antees[lo]wregretRMeta e . We firstinvestigatethelatter
T j lowingtheoremformalizesthe argumentdescribedabove.
point,andreturntotheformerinTheorems4.2and4.3.
Weprovideasimplifiedstatementhereforbrevity,butthe
( )
Theobviousapproachtoboundingthemeta-algorithm’sre- fullstatementanditsproofcanbefoundinAppendixD.4.
gretwouldbetoobservethatthelossesℓ y 1 y y 2
t t 2 t − t Theorem 4.1. (simplified) Let Meta be the instance of
a wr he icα ht- wex ilp l- ac lo lon wcav ue stf oor apα pt ly=an2m ina sx ti a1 ℓ nt cy et(i ( o) ft)( hL = ee fim x(m eda -sD h. a2 ) r) e, sfi ex qe ud e-s nh ca ere uchar ua 1c ,te ..ri .z ,e ud Tin iT nh ReA o are nm
d
aE n. y1.
j
The Nn f ,o Ar la gn oy
-
( ) rithm2guarantees
2Recallfromtheintroductionthatbecausethefeaturesx tare = ( ) ∈ [ ]
provided at the start of the round, we can work directly in the R u O RA j u max y yRef 2 log NT ,
output space R if we so choose by setting w t = y tx t/∥x t∥2 . T T + t t − t
tH ine gnc ℓe t, (yg )iv =en 1 2y (y∈ t−R yw )2e .allow a slight abuse of notation by let- where( O)≤ ĥ id( eslog( lo) gterms.( ) ( ))
⋅
7
̂()OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
A similar target-clipping strategy was recently used by (Cesa-Bianchi&Lugosi, 2006), so for simplicity we treat
Mayoetal. (2022) toprovea static regretresultforscale- T as part of the problem setting rather than a potentially
freeunconstrainedonlineregression.Theorem4.1general- unknownpropertyofthedata. Aninterestingdirectionfor
izestheirapproachbyclippingtoatrust-regionofanarbi- futuredevelopmentwouldbetoconstructthesetofexperts
trarycenteryRef R,andoffersasomewhatstreamlinedar- inamoreon-the-flyway,soastoavoidusingthedoubling
t
gumentwhichdoesnotappealtoprobabilisticnotionssuch tricktoadapttounknownT.
∈
asmixibility.
5. Strongly-Adaptive Guarantees
Finally, with Theorem 4.1 in hand, we can achieve our
desired result by running Algorithm 2 with the base al-
While our original goal was only to achieve dynamic re-
gorithms beinginstancesof thediscountedVAWfore-
Ai gretguaranteesintheabsenceofpriorknowledge,itturns
casterwithdifferentdiscountfactorsγ. Thefollowingthe-
out that we can actually achieve an even stronger re-
oremsshowthatfora well-chosensetof discountfactors,
sult: dynamic regret guarantees that hold over every sub-
wecanmakeguaranteesthatmatchtheboundsattainedun-
interal a,b 1,T simultaneously. To our knowledge,
deroracletuningofγ (Theorems3.3and3.4),yetrequire
strongly-adaptive guarantees of this sort have previously
no prior knowledge of any sort. Proofs can be found in ⊆
only be[en a]chie[ved ]under various boundedness assump-
AppendicesD.5andD.6respectively.
tions(Babyetal.,2021;Baby&Wang,2022b;a;Junetal.,
Theorem4.2. Letb 1,η min 2d,η max dT,andforall 2017;Cutkosky,2020;Danielyetal.,2015).
i N letη η bi η , andconstructthe set ofdis-
countfactoi rs min > γ∧ ma ηx i = i N = 0 .Foranyγ in Theresultscanbederivedusingtheresultsintheprevious
∈ = Sγ i 1+ηi ∶ ∪ section.AsshowninAppendixD.4,forany s,τ 1,T ,
,let denoteaninstanceofAlgorithm1withdiscount
S iγ n.γ 3 TL hee otA
rA
eγ
mMet 4a
.1b ,e aa= nn d{in sust pa= pn oc se eo wf eth se∈ eta ylg} Ro efri{th ym} fc oh ra ar la lc tt .e Tri hz ee nd u
gu
=arau nts e, e. s.. th, au tτ ,andγ ∈Sγ,Algorithm2m [ore ]g ⊆en [erall ]y
t t ( )
foranyu u ,...,u inRd,Algorithm2guarantees
=
1 T = ̃ R s,τ u O RA sγ ,τ u +m tax y t −y tRef 2log Nτ ,
R T (u O dm) ax y t y tRef 2log T [ ]( )≤ ̂( [ ]( ) ( ) ( ))
t − where R denotes the regret over sub-interval s,τ
s,τ
( )≤ ( ( ) ( ) 1,T . Theonlycaveatisthattheregretguaranteesofthe
b dPγ∗ u T y y 2 discounte[ dV] AW forecasteronly hold when the alg[orit]hm⊆
+ ¿ Á T t=1 t − t b[egin]s learning on round s.4 However, suppose that for
ÁÀ ∑
whereγ∗ 0,1 satisfiesEquati( on) (2).( ̃) ) each s 1,T and each γ γ we define an algorithm
S
whichuses discountγ butbeginslearningattime s.
Theorem∈4.3. Under the same conditions as Theo- A Thγ e,s nfo∈ ra[ny s],τ LemmaD∈ .4impliesthatthereisa
[ ] γ,s
Cre lm
ip
4.2, ,xsu ,wpp γose ,we ha ec rh
e
Aγ s ye Rts
ef
hi Mnts ,yy Rt
ef
Myγ t
and
suchthatRA sγ [,τ,s u
]
O dmax t y t −y tRef 2log τ −A s
+
M Rdt
,
=AB lt
m
g(
o⟨a rx its
ht
< mt ∣y
2t
s
⟩ g−)
uy asR raef
n
∣. teT
ehB set
n =fo
[rt any−
u
=t (ũt 1,.+= ..,ut
]T
)i=
n
pb N√revd
i
OP
ou
[γ
s
s
T∗
,τ
d
]
li
(
os[u
gpl )a
T∑y](
aτ
t= n)s
d
a(
n≤y
c
dt ho−
ano(y
̃s
ot
i )n
v2
g
e)
r.
aS
lP lγ(l ru eg gg rOi en tg
l
bot ogh u)i ns
T
dba oc f,(k win eto ha)th vee
≤
∣ ∣ ( ( ))
R u O dPγmin u dmax y yRef 2 log T ≤
T T + t t − t R( u( )) O dmax y y 2log T
( )≤ ( ( ) ( ) ( ) s,τ t t − t
T ⎛
b dPγ○ u ℓ u [ ]( )≤ ̂ ( ̃) ( )
+ ¿ Á T t=1 t t ⎝ b dPγ∗ u τ y y 2 .
whereγ 2d ÁÀ andγ○ ( 0) ,∑ 1 sa( tisfi) e) sEquation(3). + ¿ Á s,τ t=s t − t ⎞
min 2d+1 ÁÀ [ ]( )∑ ( ̃)
ThisistheessenceoftheFollowtheLeadingHistor⎠yalgo-
It is worth n=oting that The∈o[rems]4.2 and 4.3 use knowl-
rithmofHazan&Seshadhri(2007;2009).
edge of the horizon T to construct the set of ex-
perts. All of our results extend immediately to the un- While the above approach leads to a strongly-adaptive
known T setting as well via the standard doubling trick guarantee, it would be excessively expensive in general,
3For brevity, herewe refer toanalgorithm that directly pre- 4More generally, it can be seen from the analysis that if the
dicts ỹt on every round as being an instance of the discounted algorithmstartsattimet=1andwetrytoboundtheregretover
VAWforecasterwithγ =0. Thisterminologycanbejustifiedby [s,τ],thenaftertelescopingthedivergencetermswewillendup
RemarkA.2,butforourpurposeshereit’ssufficienttoconsiderit withanon-trivialtermD ψs(u s∣w s)whichishardtoquantifyin
convenientalias. generalfors>1withoutfurtherassumptions.
8OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
since we’d now have O Tlog T total experts to up- An important direction for future work is to reduce the
date on every round. We may instead lower this to computationalcomplexityofthealgorithms.Similartothe
O log2 T expertsusing(thege(om)e)triccoveringintervals traditional VAW forecaster, the approach developed here
ofDanielyetal.(2015);Venessetal.(2013).Theideaisas can be infeasible for very high-dimensional features, re-
fo(llows(: in)s)teadof initializing a new instance ofeach quiring roughly O d2log T computation every round.
γ
oneveryrounds T ,wewillconstructasetofintervA als Thed2 factorlikelycanbereducedbyextendingouranal-
S such thatany s,τ 1,T canbe coveredusingonly ysistousemodern(sketchin(g)te)chniques(Luoetal.,2016),
∈
asmallnumberofi[nte]rvalsfromS. Thenforeachγ andthelog T factorcanpossiblybereducedusingsimi-
andeachI
S,w[eca]n⊆ de[finea]ninstanceofthediscountS eγ
d lartechniquestotherecentworkofLu&Hazan(2022).
∈
VAWforecaster whichisrunonlyduringtheinterval ( )
∈ Aγ,I
I. Thegeometriccoveringintervalsareconstructedinsuch Acknowledgements
awaythat1 anyroundtcanfallintoatmostO log T
oftheintervals,and2 any s,τ 1,T canbecovered We thank David Janz for valuable feedback on the initial
using only O) log τ s disjoint intervals from( S.(Th))e draftofthiswork. AJwassupportedbyanNSERCCGS-
firstpropertyensures− t)hatthe[rea]tm⊆ o[stO]log2 T active Dscholarship. ACacknowledgessupportfromNSFgrant
experts on ea(ch ro(und, w))hile the second property implies no. 2211718.
thatthereisa disjointsetofintervalsI ,.(..,I( s)u)chthat
1 K
R s,τ u K i=1R Ii u ,soboundingeachoftheseusing ImpactStatement
asimilarargumenttotheabovefollowedbyanapplication
of[ Ca] u(ch)y-= Sc∑ hwarzin(eq)ualityyields This paper presents theoreticalwork that improvesonline
linearregression.Wedonotanticipateanysignificantneg-
ativesocietalconsequences.
R u O dmax y y 2log2 T
s,τ t t
t −
[ ]( )≤ ̂( ( ̃) τ ( ) References
b dPγ∗ u y y 2 ,
+ ¿ s,τ t − t Azoury,K.S.andWarmuth,M.K.Relativelossboundsfor
Á t=s
ÁÀ [ ]( )∑ ( ̃) ) on-linedensityestimationwiththeexponentialfamilyof
where P u is the total variability over the intervals distributions. Machinelearning,43:211–246,2001.
s,τ
andwe’veusedKlog T O log2 T . Hence, overall
[ ]( ) Baby,D.andWang,Y.-X. Optimaldynamicregretinexp-
thepenaltywe incurforusingthegeometriccoveringisa
≤ concaveonlinelearning.InProceedingsofThirtyFourth
modestincreasefroml(og)T to(Klo(g )T) O log2 T
ConferenceonLearningTheory.PMLR,2021.
intheleadingterm. Likewise,asimilarargumentholdsfor
≤
oursmall-lossbounds. We(pr)ovideafo(rm)alstat(emen(tan)d)
Baby, D. and Wang, Y.-X. Optimaldynamicregretin lqr
proofoftheseresultsinAppendixF.
control. InAdvancesin NeuralInformationProcessing
Systems,2022a.
6. Conclusion
Baby, D. and Wang, Y.-X. Optimal dynamic regret in
Inthispaper,we designedalgorithmsforonlinelinearre-
proper online learning with strongly convex losses and
gressionwhichachieveoptimaldynamicregretguarantees,
beyond. InProceedingsofThe25thInternationalCon-
evenintheabsenceofallpriorknowledge.Wedevelopeda
ference on Artificial Intelligence and Statistics. PMLR,
novelanalysisofadiscountedvariantoftheVovk-Azoury-
2022b.
Warmuthforecaster,showingthatitcanguaranteedynamic
regret of the form R T u O dlog T
∨
dP Tγ u T Baby, D., Hasson, H., and Wang, Y. Dynamic regret for
when equipped with an appropriate discoun√t factor (Sec- stronglyadaptivemethodsand optimality of online krr,
≤
tion3). Wealsoprovid(ed)amatc(hinglo(we)rbound,d(em)on)- 2021.
strating that these penalties are unavoidable in general
(Section3.2). Wethenshowedthattheidealdiscountfac- Besbes,O.,Gur,Y.,andZeevi,A. Non-stationarystochas-
tors can be learned on-the-fly,resulting in algorithmsthat ticoptimization. OperationsResearch,2015.
canbeappliedwithnopriorknowledgeyetstillmakeop-
timaldynamicregretguarantees(Section 4) andstrongly- Campolongo,N.andOrabona,F. Acloserlookattemporal
adaptive guarantees (Section 5). These are the first algo- variabilityindynamiconlinelearning,2021.
rithms for online linear regression that make meaningful
guaranteeswithoutmakingassumptionsofanykindonthe Cesa-Bianchi,N.andLugosi,G. Prediction,learning,and
underlyingdata. games. Cambridgeuniversitypress,2006.
9OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
Cesa-Bianchi, N., Gaillard, P., Lugosi, G., and Stoltz, G. Jacobsen,A.andCutkosky,A.Unconstrainedonlinelearn-
Mirror descent meets fixed share (and feels no regret). ingwithunboundedlosses. InInternationalConference
AdvancesinNeuralInformationProcessingSystems,25, onMachineLearning(ICML).PMLR,2023.
2012.
Jun, K.-S., Orabona, F., Wright, S., and Willett, R. Im-
Cutkosky, A. Parameter-free, dynamic, and strongly- proved Strongly Adaptive Online Learning using Coin
adaptiveonlinelearning. InProceedingsofthe37thIn- Betting. In Proceedingsofthe 20th InternationalCon-
ternationalConferenceonMachineLearning,2020. ference on Artificial Intelligence and Statistics. PMLR,
2017.
Daniely, A., Gonen, A., and Shalev-Shwartz, S. Strongly
Kalman,R.E. Anewapproachtolinearfilteringandpre-
adaptive online learning. In Proceedings of the 32nd
dictionproblems. TransactionsoftheASME–Journalof
InternationalConferenceonMachineLearning.PMLR,
BasicEngineering,1960.
2015.
Kempka, M., Kotlowski, W., and Warmuth, M. K. Adap-
Foster, D., Kale, S., and Karloff, H. Online sparse linear
tivescale-invariantonlinealgorithmsforlearninglinear
regression.In29thAnnualConferenceonLearningThe-
models. In Proceedingsof the 36th InternationalCon-
ory.PMLR,2016.
ferenceonMachineLearning.PMLR,2019.
Gaillard, P., Gerchinovitz, S., Huard, M., and Stoltz, G.
Kivinen, J., Warmuth, M., and Hassibi, B. The p-norm
Uniform regret bounds over Rd for the sequential lin-
generalizationofthelmsalgorithmforadaptivefiltering.
earregressionproblemwiththesquareloss. InProceed-
IEEETransactionsonSignalProcessing,2006.
ingsofthe30thInternationalConferenceonAlgorithmic
LearningTheory.PMLR,2019. Kotłowski,W. Scale-invariantunconstrainedonlinelearn-
ing.InProceedingsofthe28thInternationalConference
Ghai,U.,Lee,H.,Singh,K.,Zhang,C.,andZhang,Y. No- onAlgorithmicLearningTheory.PMLR,2017.
regret prediction in marginally stable systems. In Pro-
ceedings of Thirty Third Conference on Learning The- Kozdoba, M., Marecek, J., Tchrakian, T., and Mannor, S.
ory.PMLR,2020. On-linelearningoflineardynamicalsystems:Exponen-
tial forgetting in kalman filters. In Proceedings of the
Hazan, E. Introduction to online convex optimization. AAAIConferenceonArtificialIntelligence,2019.
CoRR,abs/1909.05207,2019.
Lu, Z. and Hazan, E. Efficient adaptive regret minimiza-
Hazan,E.andSeshadhri,C.Adaptivealgorithmsforonline tion,2022.
decisionproblems. InElectroniccolloquiumoncompu-
Luo, H., Agarwal, A., Cesa-Bianchi, N., and Langford,
tationalcomplexity(ECCC),number088,2007.
J. Efficient second order online learning by sketching.
InAdvancesinNeuralInformationProcessingSystems,
Hazan,E.andSeshadhri,C. Efficientlearningalgorithms
2016.
for changingenvironments. In Proceedingsof the 26th
AnnualInternationalConferenceonMachineLearning,
Luo,H.,Zhang,M.,Zhao,P.,andZhou,Z.-H. Corrallinga
2009.
largerbandofbandits: Acasestudyonswitchingregret
forlinearbandits.InProceedingsofThirtyFifthConfer-
Hazan,E.andSingh,K.Introductiontoonlinenonstochas-
enceonLearningTheory.PMLR,2022.
ticcontrol,2022.
Mayo, J. J., Hadiji, H., and van Erven, T. Scale-free un-
Hazan, E., Singh, K., and Zhang, C. Learning linear dy-
constrained online learning for curved losses. In Pro-
namicalsystemsviaspectralfiltering. AdvancesinNeu-
ceedingsofThirtyFifthConferenceonLearningTheory,
ralInformationProcessingSystems,2017.
2022.
Hazan, E., Lee, H., Singh, K., Zhang, C., and Zhang, Mhammedi, Z. and Koolen, W. M. Lipschitz and
Y. Spectral filtering for general linear dynamical sys- comparator-normadaptivityinonlinelearning. InAber-
tems. Advancesin Neural InformationProcessing Sys- nethy, J. and Agarwal, S. (eds.), Proceedings of Thirty
tems,2018. ThirdConferenceonLearningTheory.PMLR,2020.
Jacobsen, A. and Cutkosky, A. Parameter-freemirrorde- Orabona,F.,Crammer,K.,andCesa-Bianchi,N. Agener-
scent. In Proceedings of Thirty Fifth Conference on alizedonlinemirrordescentwithapplicationsto classi-
LearningTheory.PMLR,2022. ficationandregression. Mach.Learn.,2015.
10OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
Rashidinejad,P.,Jiao,J.,andRussell,S. Slip: Learningto
predict in unknown dynamical systems with long-term
memory. InAdvancesinNeuralInformationProcessing
Systems,2020.
Simon, D. Optimal state estimation: Kalman, H infinity,
andnonlinearapproaches. JohnWiley&Sons,2006.
Tsiamis,A.andPappas,G.J.Onlinelearningofthekalman
filterwithlogarithmicregret. IEEETransactionsonAu-
tomaticControl,2022.
Veness,J.,White,M.,Bowling,M.,andGyörgy,A. Parti-
tiontreeweighting. In2013DataCompressionConfer-
ence,2013.
Vovk,V. Competitiveon-linestatistics. InternationalSta-
tisticalReview,2001.
Yuan, J. and Lamperski, A. G. Trading-offstatic and dy-
namicregretinonlineleast-squaresandbeyond. CoRR,
abs/1909.03118,2019.
Zhang,L.,Lu,S.,andZhou,Z.-H. Adaptiveonlinelearn-
ing in dynamic environments. In Advances in Neural
InformationProcessingSystems,2018.
Zhang,Z.,Cutkosky,A.,andPaschalidis,Y.Unconstrained
dynamicregretviasparsecoding.InAdvancesinNeural
InformationProcessingSystems,2023.
Zhao, P., Zhang, Y.-J., Zhang, L., and Zhou, Z.-H. Dy-
namic regret of convex and smooth functions. In Ad-
vancesinNeuralInformationProcessingSystems,2020.
Zhao,P., Zhang,Y.-J., Zhang,L., andZhou,Z.-H. Adap-
tivityandnon-stationarity:Problem-dependentdynamic
regret for online convex optimization. Journal of Ma-
chineLearningResearch,2024.
11OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
A. Proofs forSection 3 (DynamicRegretvia Discounting)
A.1.EquivalencetoFTRLandMirrorDescent
WeaccomplishouranalysisofthediscountedVAWforecasterusingtheequivalenceinthefollowingproposition,proving
both optimistic FTRL andand optimistic mirrordescentinterpretationsof the discountedVAW forecaster. Equation(6)
isperhapsthemostnaturalinterpretationoftheupdate: itsaysthatthediscountedVAWforecasterchoosesthew which
minimizesthediscountedsumh t w γℓ t−1 w γ2ℓ t−2 w ...,thusplacinggreateremphasisonthemost-recentlosses
+ + +
andthehintfunctionh w . However,itisnotatallobvioushowtoanalyzethedynamicregretofthediscountedVAW
t
forecasterwheninterpretedinth(isF)TRL-lik(ef)orm. Rath(er,)thekeytoourresultsinthisworkistoinsteadapproachthe
analysisthroughthelens(of)themirrordescentupdate(Equation(7)). Interestingly,asimilarmirrordescentinterpretation
wasusedintheseminalworkofAzoury&Warmuth(2001),thoughtheydidnotaccountforanarbitraryy andtheydid
t
notrefertothealgorithmintermsofmirrordescent.
̃
Proposition A.1. (Discounted VAW Forecaster) Let γ 0,1 , λ 0, y 0, and y R for t 1. Define h w
1 t t
a21 ndy
t s−
etwx t,w arg2a mn id nℓ
0
w
ψ
λ
2
w
w 02 2. .R Te hc eu nr ts hiv ee fl oy lld oe wfi in n∈e gΣ
(at
ree]x
qt
ux iv⊺
t> a+
leγ nΣ t̃t−1=startingf ̃rom
∈
Σ
0
λI, >letψ
t
w 1
2
(w )2
Σ=t
1 w∈Rd =1 = = =
(̃ ⟨ ⟩) ( ) ∥ ∥ t−1 ( ) ∥ ∥
= ( )= Σ− t1 y tx t γ γt−1−sy sx s (5)
+
s=1
[̃ t−1∑ ]
argminh w γ
γt−1−sℓ
s (6)
t s
w∈Rd + s=0
( ) ∑ ( )
ar wg ∈m Rdin γℓ t−1 −γh t−1 +h t w +γD ψt−1 ww t−1 (7)
( )( ) ( ∣ )
RemarkA.2. Notethatwithγ 0,Equations(6)and(7)prescribechoosinganyw satisfying w ,x y . Thechoice
t t t t
⟨ ⟩ ̃
isnotunique,butneverthelessitwilloftenbeconvenienttorefertoanalgorithmwhichgreedilypredictsy oneachround
= =̃t
asaninstanceofAlgorithm1withγ 0.
=
Proof. The result follows by showing that Equations(6) and (7) are both equivalentto Equation(5). First consider the
former,Equation(6). Fromthefirst-orderoptimalityconditionwehave
t−1
0 h w γ γt−1−s ℓ w
t t s t
∇ ( )+ ∇ ( )
s=0
= ∑ t−1
y x ,w x γ γt−1−s y x ,w x γtλw ,
t t t t s s t s t
−(̃ −⟨ ⟩) − ( −⟨ ⟩) +
s=1
= ∑
wherethelastlinerecallsthatwedefinedℓ w λ w 2 . Hence,
0 ( ) 2 ∥ ∥2
=
t t
γtλI γt−sx x⊺ w y x γt−sy x
s s t t t s s
( + ) ̃ +
s=1 s=1
∑ = ∑ t −1 t−1
Ô⇒ w γtλI γt−sx x⊺ y x γ γt−1−sy x
t s s t t s s
( + ) [̃ + ]
s=1 s=1
= ∑ t−1 ∑
Σ−1
y x γ
γt−1−sy
x ,
t t t s s
[̃ + ]
s=1
= ∑
wherethelastlinecanbeseenbyunrollingtherecursionforΣ .
t
Likewise,considerEquation(7). Fromthefirst-orderoptimalityconditionw t argmin w∈Rd γℓ t−1 γh t−1 h t w
( − + )( )+
γD ww ,wehave
ψt−1( t =
∣ ) 0 γ ℓ t−1 w t h t−1 w t h t w t γ ψ t−1 w t ψ t−1 w t−1
∇ −∇ +∇ + ∇ −∇
= −(γy t−1x (t−1 +)γy t−1x t−(1 −)y )tx t +x (tx⊺ tw )t +γ [Σ t−1w (t −γ )Σ t−1w t−1
( )]
= γy t−1x t−1 γy t−1x t−1 y tx t Σ tw t γΣ t−1w t−1,
− + ̃ −̃ + −
=
̃ ̃
12OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
wherethelastlineobservesthatΣ
t
x tx⊺
t
γΣ t−1byconstruction.Hence,re-arrangingwehave
+
Σ tw t y tx t γy t−1=x t−1 γy t−1x t−1 γΣ t−1w t−1
+ − +
andunrollingtherecur=sĩon:
̃
y tx t γy t−1x t−1 γy t−1x t−1 γ y t−1x t−1 γy t−2x t−2 γy t−2x t γΣ t−2w t−2
+ − + + − +
=y ̃tx t +γy t−1x t−1 +γ2 ̃y t−2x t−2 −γ [2 ̃y t−2x t−2 +γ2Σ t−2w t−2 ̃ ]
...
=
̃ ̃
=
y x
γt−1y
x
γt−1
γt−1−sy
x
t t 1 1 s s
− +
s=1
= ̃ t−1̃ ∑
y x γ
γt−1−sy
x ,
t t s s
+
s=1
= ∑
̃
fory 0. Hence,applyingΣ−1tobothsideswehave
1 t
̃
=
w
Σ−1
y x
γt−1
γt−1−sy
x
t t t t s s
+
s=1
= ∑
[̃ ]
A.2.ProofofTheorem3.1
Theorem3.1. Letλ 0andγ 0,1 . Thenforanysequenceu u ,...,u inRd,Algorithm1guaranteesdynamic
1 T
regretR u boundedaboveby
T > ∈ =
( ] ( )
( ) γλ d T γT−t x 2
u 2 max y y 2log 1 t=1 t 2
2 1 2+ 2 t t − t + λd
∑
⎛ ∥ ∥ ⎞
∥T−1∥ ( ̃) d T
+γ F tγ u t+1 −F tγ u t
+
⎝ 2log 1 γ y t −y t⎠2
t=1 t=1
∑ ∑
[ ( ) ( )] ( / ) ( ̃)
whereFγ w γtλ w 2 t γt−sℓ w .
t 2 2+ s=1 s
= ∑
Proof. Be(gin)byappl∥yin∥gtheregrettemp(lat)eprovidedbyLemmaA.3:
T T 1 T
R T u t=1D ψt u tw t −D ψt+1 u tw t+1 + t=1h t+1 u t −h t u t + 2
t=1
y t −y t 2 x t 2 Σ− t1,
≤∑ ∑ ∑
( ) ( ∣ ) ( ∣ ) ( ) ( ) ( ̃) ∥ ∥
boundthefirsttwosummationsusingLemmaA.4:
γλ T−1 1 T
2 u 1 2 2+h T+1 u T +γ
t=1
F tγ u t+1 −F tγ u t + 2
t=1
y t −y t 2 x t 2 Σ− t1,
≤ ∑ ∑
∥ ∥ ( ) [ ( ) ( )] ( ̃) ∥ ∥
andapplyadiscountedvariantofthelog-determinantlemma(LemmaG.2)toboundthefinalsummation:
γλ d T γT−t x 2
2
u 1 2 2+h T+1 u T
+
2m tax y t −y t 2log 1
+
t=1
λd
t 2
∑
≤ ⎛ ∥ ∥ ⎞
∥ T∥−1 ( ) ( d ̃) T
+γ F tγ u t+1 −F tγ u t
+
2log 1 γ⎝ y t −y t 2 ⎠
t=1 t=1
∑ ∑
[ ( ) ( )] ( / ) ( ̃)
Finally,sincetheregretdoesnotdependonh T+1 wemayseth T+1 0intheanalysisandhideconstantstoarriveat
⋅ ⋅
thestatedbound.
≡
() ()
13OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
A.2.1.PROOF OF LEMMAA.3
Thefollowinglemmaprovidesthebaseregretdecompositionthatweuseasajumping-offpointtoproveTheorem3.1. The
resultfollowsusingmostlystandardmirrordescentanalysis,thoughwithabitofadditionalcaretohandleissuesrelated
tothediscountedregularizer.
LemmaA.3. Letγ 0,1 . Thenforanysequenceu u ,...,u inRd,Algorithm1guarantees
1 T
∈ =
( ] ( )
T
R T u D ψt u tw t −D ψt+1 u tw t+1
t=1
≤∑
( ) T ( ∣ ) ( ∣ )
h t+1 u t h t u t
+ −
t=1
∑
T 1 ( ) ( )
y y 2 x 2
+ t=12 t − t t Σ− t1
∑
( ̃) ∥ ∥
Proof. We will proceed following a mirror-descent-basedanalysis, and thus begin by exposing the terms γℓ γh
t t
− +
h t+1 w t+1 observedinthemirror-descentinterpretationoftheupdate(Equation(7)):
(
)( )
T
R u ℓ w ℓ u
T t t t t
−
t=1
=∑
( ) T ( ) ( ) T
γ ℓ w ℓ u 1 γ ℓ w ℓ u
t t t t t t t t
− + − −
t=1 t=1
=∑ ∑
T [ ( ) ( )] ( ) (T) ( )
γ ℓ h w ℓ h u γh w γh u
t t t t t t t t t t
− − − + −
t=1 t=1
=∑ ∑
[( T)( ) ( )( )] ( ) ( )
1 γ ℓ w ℓ u
t t t t
+ − −
t=1
∑
T ( ) ( ) ( ) T
γ ℓ t h t w t+1 ℓ t h t u t γh t w t γh t u t
− − − + −
t=1 t=1
=∑ ∑
[(T )( ) ( )( )] ( ) ( )
γ ℓ t h t w t ℓ t h t w t+1
+ − − −
t=1
∑
( T )( ) ( )( )
1 γ ℓ w ℓ u
t t t t
+ − −
t=1
∑
T ( ) ( ) ( )
γℓ t γh t h t+1 w t+1 γℓ t γh t h t+1 u t
− + − − +
t=1
=∑
( T )( ) ( T )( )
γh t w t h t+1 w t+1 h t+1 u t γh t u t
+ − + −
t=1 t=1
∑ ∑
T ( ) ( ) ( ) ( )
γ ℓ t h t w t ℓ t h t w t+1
+ − − −
t=1
∑
( T )( ) ( )( )
1 γ ℓ w ℓ u
t t t t
+ − −
t=1
∑
( ) ( ) ( )
14OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
Re-arrangingfactorsofγ fromthesecond-lineandobservingthat T t=1h t w t h t+1 w t+1 h 1 w 1 h T+1 w T+1 :
− −
∑ =
T ( ) ( ) ( ) ( )
γℓ t γh t h t+1 w t+1 γℓ t γh t h t+1 u t
− + − − +
t=1
=∑
( T )( ) (T )( ) T
h t w t h t+1 w t+1 1 γ h t w t 1 γ h t u t h t+1 u t h t u t
+ − + − − + − + −
t=1 t=1 t=1
∑ ∑ ∑
T ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( )
γ ℓ t h t w t ℓ t h t w t+1
+ − − −
t=1
∑
( T )( ) ( )( )
1 γ ℓ w ℓ u
t t t t
+ − −
t=1
∑
T ( ) ( ) ( )
γℓ t γh t h t+1 w t+1 γℓ t γh t h t+1 u t
− + − − +
t=1
=∑
( )( ) (T )( )
h 1 w 1 h T+1 w T+1 h t+1 u t h t u t
+ − + −
t=1
∑
T( ) ( ) ( ) ( )
γ ℓ t h t w t ℓ t h t w t+1
+ − − −
t=1
∑
( T )( ) ( )( )
1 γ ℓ h w ℓ h u (8)
t t t t t t
+ − − − −
t=1
∑
( ) ( )( ) ( )( )
Moreover,fromthefirst-orderoptimalityconditionw t+1 argmin w∈Rd γℓ t −γh t +h t+1 w +γD ψt ww t ,wehave
=
γℓ t γh t h t+1 w t+1 γ ψ t w t+1 γ(ψ t w t ,w t+1 u)( t )0 ( ∣ )
∇ − + + ∇ − ∇ −
≤
sore-arranging: ⟨ ( )( ) ( ) ( ) ⟩
γℓ t γh t h t+1 w t+1 ,w t+1 u t γ ψ t w t ψ t w t+1 ,w t+1 u t
∇ − + − ∇ −∇ −
⟨ ( )( )
⟩≤γD ⟨ψt u (tw )t −γD (ψt u t )w t+1 −γD ⟩ψt w t+1w t ,
=
where the last line uses the three-point relation for bregman( di∣ver)gences, ( f∣ w ) f w′(,w′ ∣u ) D uw
f
D uw′ D w′w . Thus, ∇ −∇ − −
f − f ⟨ ( ) ( ) ⟩ = ( ∣ )
( ∣ ) ( T∣ )
γℓ t γh t h t+1 w t+1 γℓ t γh t h t+1 u t
− + − − +
t=1
∑
( T )( ) ( )( )
a
∇
γℓ t −γh t +h t+1 w t+1 ,w t+1 −u t −D γℓt−γht+ht+1 u tw t+1
( )t=1
= ∑
T ⟨ ( )( ) ⟩ ( ∣ )
γD ψt u tw t −γD ψt u tw t+1 −γD ψt w t+1w t −D γℓt−γht+ht+1 u tw t+1
t=1
≤∑
T ( ∣ ) ( ∣ ) ( ∣ ) ( ∣ )
b
γD ψt u tw t −γD ψt u tw t+1 −D ht+1 u tw t+1 −γD ψt w t+1w t
( )t=1
= ∑
T ( ∣ ) ( ∣ ) ( ∣ ) ( ∣ )
c
γD ψt u tw t −D ψt+1 u tw t+1 −γD ψt w t+1w t
( )t=1
= ∑
T ( ∣ ) ( ∣ ) ( ∣ )
D ψt u tw t −D ψt+1 u tw t+1
−
1 −γ D ψt u tw t −γD ψt w t+1w t ,
t=1
=∑
( ∣ ) ( ∣ ) ( ) ( ∣ ) ( ∣ )
where a usesthedefinitionofBregmandivergencetore-writef w f u f w ,w u D uw , b observes
f
thatγ (ℓ t −)h t w γ 1 2y t2 − 21y t2 + y t −y t x t,w ,soD γℓt−γh (t+h )t+− 1 ⋅(⋅ )=D ⟨h∇ t+1 (⋅⋅)due− to ⟩th− einv (ari ∣an )ce (of )Bregman
= =
( )( ) ( ̃ (̃ )⟨ ⟩) 15 (∣) (∣)OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
divergencestolinearterms,and c recallsthatΣ t+1 x t+1x⊺ t+1+γΣ tsothatoverallwehave:
γD ψt u t( w t) +1 +D ht+1 u tw t= +1 γ 2 u t −w t+1 2 Σt+ 21 x t+1,u t −w t+1 2
= 1
( ∣ ) ( ∣ )
2
∥u
t
−w t+1∥2
Σt+1
⟨ ⟩
=D ψ∥ t+1 u tw t+∥1 .
=
PluggingthisbackintoEquation(8),wehave ( ∣ )
T
R T u D ψt u tw t −D ψt+1 u tw t+1
t=1
≤∑
( ) ( ∣ ) ( ∣ T)
h 1 w 1 h T+1 w T+1 h t+1 u t h t u t
+ − + −
t=1
∑
T( ) ( ) ( ) ( )
+γ ℓ t −h t w t
−
ℓ t −h t w t+1 −D ψt w t+1w t
t=1
∑
( T )( ) ( )( ) ( ∣ )
+
1 −γ ℓ t −h t w t
−
ℓ t −h t u t −D ψt u tw t+1 .
t=1
∑
( ) ( )( ) ( )( ) ( ∣ )
Finally,observethatforanyu,v Rd, ℓ h u ℓ h v y y x ,u v ,soanapplicationofFenchel-Young
t t t t t t t
− − − − −
inequalityyields
∈ =
( )( ) ( )( ) (̃ )⟨ ⟩
1
ℓ h u ℓ h v D vu y y x ,u v u v 2
t − t − t − t − ψt t − t t − − 2 − Σt
= 1
( )( ) ( )( ) ( ∣ ) (̃y y)⟨2 x 2 ⟩ . ∥ ∥
2 t − t t Σ− t1
≤
Applyingthisinthelasttwolinesofthepreviousdisplayyields ( ̃) ∥ ∥
T
R T u D ψt u tw t −D ψt+1 u tw t+1
t=1
≤∑
( ) ( ∣ ) ( ∣ T )
h 1 w 1 h T+1 w T+1 h t+1 u t h t u t
− + −
t=1
∑
( ) ≤0 ( ) ( ) ( )
´¹¹¹¹¹¹T¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹1¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶ T 1
γ y y 2 x 2 1 γ y y 2 x 2
t=12 t − t t Σ− t1 + − t=12 t − t t Σ− t1
∑ ∑
T ( ̃) ∥ ∥ ( ) ( ̃) ∥ ∥
D ψt u tw t −D ψt+1 u tw t+1
t=1
≤∑
T ( ∣ ) ( ∣ )
h t+1 u t h t u t
+ −
t=1
∑
T 1 ( ) ( )
y y 2 x 2
+ t=12 t − t t Σ− t1
∑
( ̃) ∥ ∥
A.2.2.PROOF OF LEMMAA.4
Thefollowinglemmaboundsthesumofdivergenceterms. Intuitively,thegoalhereistoremoveallinstancesofw from
t
theanalysis, since inanunboundeddomainanytermsdependingonw willbe hardtoquantifyandcouldbearbitrarily
t
largeingeneral. LemmaA.4showshowgetridofthew -dependenttermsleftintheboundfromLemmaA.3, suchthat
t
onlydependenciesonthecomparatorsu remain.
t
16OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
LemmaA.4. UnderthesameconditionsasLemmaA.3,
T T γλ T−1
D ψt u tw t −D ψt+1 u tw t+1
+
h t+1 u t −h t u t
2
u 1 2 2+h T+1 u T +γ F tγ u t+1 −F tγ u t .
t=1 t=1 t=1
∑ ∑ ≤ ∑
( ∣ ) ( ∣ ) ( ) ( ) ∥ ∥ ( ) ( ) ( )
whereFγ w t γt−sℓ w .
t s=0 s
=∑
Proof. Ob( ser) ve that by Lem( m) a G.1 we have D uv 1 x ,u v 2 D uv for any u,v W. Hence, let-
ℓt 2 t − ht
ting F tγ w t s=0γt−sℓ
s
w and F tγ w h
t (
w
∣
)+=γF t⟨γ
−1
w , a ⟩nd=recalli (ng ∣ψ
)t
w 1
2
w∈2
Σt
γ 2tλ w 2
2 +
1 t γt−s x ,w 2,wehaveD uv D uv foranyu,v Rd. Thus:
2 s=1 ( ) =s ∑ ( )ψt ̂ ( )F tγ= ( ) ( ) ( ) = ∥ ∥ = ∥ ∥
∑ = ̂ ∈∈
⟨ T ⟩ ( ∣ ) ( ∣ )
D ψt u tw t −D ψt+1 u tw t+1
t=1
∑
( ∣ ) ( ∣ ) T
D ψ1 u 1w 1 −D ψT+1 u T w T+1
+
D ψt u tw t −D ψt u t−1w t
t=2
= ∑
( ∣ ) ( ∣ ) T ( ∣ ) ( ∣ )
D ψ1 u 1w 1 −D ψT+1 u T w T+1 + t=2D F tγ u tw t −D F tγ u t−1w t
= ∑ ̂ ̂
( ∣ ) ( ∣ ) T ( ∣ ) ( ∣ )
D ψ1 u 1w 1 −D ψT+1 u T w T+1
+
F tγ u t −F tγ u t−1
−
∇F tγ w t ,u t −u t−1 .
t=2
= ( ∣ ) ( ∣ ) ∑ ̂ ( ) ̂ ( ) ⟨ ̂ ( ) ⟩
Moreover,byPropositionA.1wehave
t−1
w argmin h w γ γt−1−sℓ w argmin Fγ w ,
t t s t
w∈Rd + s=0 w∈Rd
= ( ) ∑ ( )= ̂ ( )
hencebyconvexityofFγ andthefirst-orderoptimalityconditionwehave Fγ w 0,sooverallwehave
t t t
∇
T ̂ T ̂ ( )=
D ψt u tw t −D ψt u tw t+1
+
h t+1 u t −h t u t
t=1 t=1
∑ ∑
( ∣ ) ( ∣ ) (T ) ( ) T
D ψ1 u 1w 1 −D ψT+1 u T w T+1
+
F tγ u t −F tγ u t−1
+
h t+1 u t −h t u t
t=2 t=1
= ( ∣ ) ( ∣ ) ∑ T ̂ ( ) ̂ ( ) ∑ ( ) ( ) T
D ψ1 u 1w 1 −D ψT+1 u T w T+1
+
h t u t −h t u t−1 +γF tγ −1 u t −γF tγ −1 u t−1
+
h t+1 u t −h t u t
t=2 t=1
= ( ∣ ) ( ∣ ) ∑ T[−1 ( ) ( ) T( ) ( )] ∑ ( ) ( )
D ψ1 u 1w 1 −D ψT+1 u T w T+1 +γ F tγ u t+1 −F tγ u t
+
h t+1 u t −h t u t−1 +h 2 u 1 −h 1 u 1
t=1 t=2
=
( ∣ ) ( ∣ )
∑
( ) (
T)−1∑
( ) ( ) ( ) ( )
D ψ1 u 1w 1 −D ψT+1 u T w T+1 +h T+1 u T −h 1 u 1 +γ F tγ u t+1 −F tγ u t .
t=1
= ∑
Finally,obser( vet∣ hat)
withw
(0an∣
dy
) 0weha(
ve
) ( ) ( ) ( )
1 1
= = γλ
D u w ψ u ̃ ψ 0 ψ 0 ,u h u γℓ u h u u 2
ψ1 1 1 1 1 − 1 − ∇ 1 1 1 1 + 0 1 1 1 + 2 1 2
= = =
sowecanexpressth(ebo∣un)dasth(ebo)undas( ) ⟨ ( ) ⟩ ( ) ( ) ( ) ∥ ∥
T T
D ψt u tw t −D ψt u tw t+1
+
h t+1 u t −h t u t
t=1 t=1
∑
(γλ∣ ) ( ∣ )
T∑−1
( ) ( )
2
u 1 2 2+h T+1 u T +γ F tγ u t+1 −F tγ u t .
t=1
≤ ∑
∥ ∥ ( ) ( ) ( )
17OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
A.3.ProofofLemma3.2
Thefollowinglemma boundsthevariabilityandstability termsfromTheorem3.1to exposea moreexplicittrade-offin
termsofthediscountfactorγ.
Lemma3.2. Letℓ ,ℓ ,...,ℓ bearbitrarynon-negativefunctions,0 γ β 1,andFγ w t γt−sℓ w . Forall
0 1 T t s=0 s
t,define
< ≤ < =∑
( ) ( )
1 t
d¯β u,v βt−s ℓ u ℓ v
t t βt−s s − s +
s=0 s=0
= ∑
( ) [ ( ) ( )]
andletP Tβ u T t=− 11d¯β
t
u t+1,u
t
. Thenforan∑yV
T
0,
( )=∑ ( T−1 ) ≥ 1 β 1 γ
γ F tγ u t+1 −F tγ u t +log
γ
V T
1
βP Tβ u
+
−
γ
V T
t=1
−
∑ ≤
[ ( ) ( )] ( ) ( )
Proof. Thefirstsummationcanbeboundedas
T−1 T−1 t
γ F tγ u t F tγ u t−1 γ γt−s ℓ s u t+1 ℓ s u t
− −
t=1 t=1s=0
∑
[ ( ) (
)]= T∑−1∑
t [ ( ) ( )]
γ
γt−s
ℓ s u t+1 ℓ s u t +
−
t=1s=0
≤ T∑−1∑ t t [ β(t−s′ ) ( )]
β ts′=0 βt−s′βt−s ℓ s u t+1 −ℓ s u t +
t=1 s=0∑s′=0
≤ β∑ ∑ T−1 ∑t βt−s [ ( ) ( )]
1 β t βt−s′ ℓ s u t+1 −ℓ s u t +
≤ −
β
t ∑=1 s ∑=0 s′=0
[ ( ) ( )]
Pβ u ,∑
1 β T
−
=
wherethelastinequalityuses t βt−s 1−βt+1 1 . Usin( gt) hisalongwiththeelementaryinequalitylog x x 1,
s=0 1−β 1−β
−
foranyV 0wehave
T ∑ = ≤ ≤
( )
≥ T−1 1 β 1
γ F tγ u t −F tγ u t−1 +log
γ
V T
1
βP Tβ u
+ γ
−1 V T
t=1
−
∑ [ ( ) ( )] ( ) ≤ β ( ) (1 γ )
1
βP Tβ u
+
−
γ
V T
−
=
( )
A.4.ExistenceofaGoodDiscountFactor
The followinglemma establishesthe existenceof a discountfactorthatwill lead to favorabletuningof the γ-dependent
termsinLemma3.2.
Lemma A.5. Let ℓ ,ℓ ,... be arbitrary non-negative functions, V 0, denote d¯γ u,v t s=0γt−s ℓs u −ℓs v + for
γ 0,1 ,andletP
Tγ0 u1
T t=− 11d¯γ t u t+1,u t . Thenthereisaγ∗
T
0,≥1 suchthat
t
( ) = ∑
∑t
s[=0
(γt )−s
( )]
∈ [ ] ( )=∑ ( ) γ∗ V T ∈ [ .]
V √ Pγ∗ u
= T + T
√
√
( )
Proof. First,noticethatthatanysuchγ withthestatedpropertymustbein 0,1 since
0 V T V T [ 1. ]
≤
V T +√ P Tγ u
≤
√V T
=
√ √ √
( )
18OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
Next,observethattheconditioncanbeequivalentlyexpressedasfollows:
V
γ T
V √ Pγ u
= T + T
⇐⇒ V T 1 −γ γ√ P Tγ √ u ( )
√ √
( )= T− (1 t) γt−s
γ ¿ t γt−s ℓ s u t+1 −ℓ s u t +
Át=1 s=0 s=0
= ÁÀ∑ ∑
[ ( ) ( )]
T−1 t ∑γt−s
γ ¿ 1 γt+1 1 −γ ℓ s u t+1 −ℓ s u t +
Át=1 s=0
−
= ÁÀ∑ ∑
( )[ ( ) ( )]
T−1 t γt−s
⇐⇒ V T 1 −γ γ ¿ 1 γt+1 ℓ s u t+1 −ℓ s u t +.
√ Át=1 s=0 −
= ÁÀ∑ ∑
( ) [ ( ) ( )]
ThequantityontheLHSbeginsat V (forγ 0)andthendecreasesto0asafunctionofγ. Likewise,theRHSbegins
T
at0(forγ 0)andincreasesasafu√nctionofγ,approaching asγ →1. Hence,theremustbesomeγ 0,1 atwhich
= ∞
thetwolinescross,andhenceaγ 0,1 whichsatisfiestheaboverelation,sothereisaγ 0,1 suchthat
= ∈
[ ]
∈ ∈
[ ] [ ]
V
γ T .
V √ Pγ u
= T + T
√ √
( )
A.5.ProofofTheorem3.3
Nowcombiningeverythingwe’veseenintheprevioussections,wecaneasilyprovethefollowingboundforthediscounted
VAWforecasterunderoracletuningofthediscountfactor.
Theorem 3.3. For any sequencesy ,...,y andy ,...,y in R and any sequenceu u ,...,u in Rd, there is a
1 T 1 T 1 T
discountfactorγ∗ 0,1 satisfying
=
̃ ̃ ( )
∈
[ ]
d T y y 2
γ∗ √2 t=1 t − t (2)
=
d
2
T
t=1
y∑
t
−y(t 2 +̃)P Tγ∗ u
√ √
∑
( ̃) ( )
withwhichtheregretofAlgorithm1isboundedaboveby
R u O dmax y y 2log T
T t t
t −
≤
( ) ( ( ̃) ( )
T
dPγ∗ u y y 2
+¿ T t − t
Á t=1
ÁÀ ∑
( ) ( ̃) )
Proof. LemmaA.5showsthatforanysequenceu u ,...,u ,thereisaγ∗ 0,1 suchthat
1 T
= ∈
( ) [ ]
d T 1 y y 2
γ∗ √ t=1 2 t − t ,
=
d T
t=1
1 2∑y
t
−y(t 2 +̃)P Tγ∗ u
√ √
∑
( ̃) ( )
19OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
sochoosingγ γ∗andapplyingTheorem3.1,wehave
= γ∗λ d T x 2
R u u 2 max y y 2log 1 t=1 t 2
T 2 1 2+ 2 t t − t + λd
∑
≤ ⎛ ∥ ∥ ⎞
( ) +γ∥ ∗T∥−1 F tγ∗ u t( +1 −̃ F t) γ∗ u t⎝
+
d 2log 1 γ∗⎠T y t −y t 2
t=1 t=1
∑ [ ( ) ( )] ( / )∑( ̃)
∗ λ d T x 2
u 2 max y y 2log 1 t=1 t 2
( ) 2 1 2+ 2 t t − t ⎛ + ∑ λ ∥d ∥ ⎞
≤ ∥ ∥ ( ̃)
+
1γ∗ γ∗P Tγ∗ u
+
1 − γ∗γ∗d
2
T⎝ y t −y t 2 ⎠
t=1
−
( ) ∑( ̃)
λ d T x 2 T
u 2 max y y 2log 1 t=1 t 2 ¿2dPγ∗ u y y 2
2 1 2+ 2 t t − t ⎛ + ∑ λ ∥d ∥ ⎞+Á ÁÀ T t=1 t − t
= ∥ ∥ ( ̃) ( )∑( ̃)
⎝ ⎠
where usesLemma3.2(withβ γ γ∗). Thestatedresultfollowsbyhidinglower-orderterms.
∗
( ) = =
B. Proofs forSection 3.1(Small-lossBounds viaSelf-confident Predictions)
B.1.ProofofTheorem3.4
WesplittheproofofTheorem3.4intotwoparts. Thefollowinglemma,proveninAppendixB.1.1,firstderivesaninitial
regrettemplatethatdoesmostoftheheavylifting. Wewilllaterre-usethistemplateintheproofofTheorem4.3toavoid
repeatingtheargument.Thehigh-levelintuitionisthatchoosinghintsy x ,w leadsto T y y 2 T ℓ w ,
t t t t=1 t t t=1 t t
whichleadstoaself-boundingargumentthatletsusreplace T y y 2⟨ with ⟩T ℓ u inth( ere− gr̃ et) bound.We( defe) r
t=1 ( t −̃̃t )≈ t=1 t ( t )∑ ≈∑
proofofthelemmatothenextsubsection,AppendixB.1.1.
∑ ∑
Lemma B.1. Let yRef R be an arbitrary reference point, available at the start of round t, and let
y R yRef M yt yRef M forM max y yRef. SupposethatweapplyAlgorithm1withhintsy B yt
{ ∶ t − t t∈ + t } t s<t ∣ s − s ∣ ̃t t ∶=
Clip x ,w . Thenforanysequenceu u ,...,u inRd andanyγ,β 0,1 suchthatβ γ γ 2d ,
∈B t(⟨ t t ⟩) ≤ ≤ = ( 1 T ) min 2d+=1 =
= ∈ T[γT]−t x 2 ≥ ≥ =
R u γλ u 2 4dmax y yRef 2log 1 t=1 t 2
T 1 2+ t t − t + λd
∑
≤ ⎛ ∥ ∥ ⎞
( ) ∥ ∥ β ( 1 γ ) T
+2
1
βP Tβ u
+
−
γ
2d ℓ t⎝u t ⎠
t=1
−
∑
( ) ( )
Nowusingthistemplate,Theorem3.4iseasilyprovenbyplugginginthestateddiscountfactorγ γ○ γ
min
∨
Theorem3.4. Lety tRef Rbeanarbitraryreferencepointandlet
t
y tRef M t,y tRef M
t
forM=t max
s<t
y
s
y sRef.
B − + −
SupposethatweapplyAlgorithm1withhintsy Clip x ,w . Thenforanysequenceoflossesℓ ,...,ℓ andany
∈ t B t t t = = 1 T
sequenceu u ,...,u inRd,thereisaγ○ 0,1 satisfying [ ] ∣ ∣
1 T =
̃ (⟨ ⟩)
= ∈
( ) [ ] d T ℓ u
○ t=1 t t
γ √ . (3)
= √d T t=1ℓ t∑u t +( √P) Tγ○ u
Moreover,runningAlgorithm1withdiscountγ○ γ∑ forγ( ) 2d en(su)resregretboundedaboveby
∨
min min 2d+1
=
R u O dPγmin u dmax y yRef 2log T
T T + t t − t
≤
( ) ( ( ) ( ) ( )
T
dPγ○ u ℓ u ,
+¿ T t t
Á t=1
ÁÀ ∑
( ) ( ))
20OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
Proof. ByLemmaB.1(withβ γ),foranyγ γ 2d ,wehave
min 2d+1
= ≥ =
T γT−t x 2
R u γλ u 2 4dmax y yRef 2log 1 t=1 t 2
T 1 2+ t t − t + λd
∑
≤ ⎛ ∥ ∥ ⎞
( ) ∥ ∥ γ ( 1 γ ) T
+2
1
γP Tγ u
+
−
γ
2d ℓ t⎝u t . ⎠
t=1
−
∑
( ) ( )
NowbyLemmaA.5,thereisaγ○ 0,1 satisfyingγ○ d T t=1ℓt ut . Ifγ○ γ ,thenforγ γ○ γ ,the
d T t√ =1ℓ∑t ut + ( P)Tγ○ u min ∨ min
termsinthesecondlinereduceto∈ = √ √ ≥ =
[ ] ∑ ( ) ( )
2 1γ○ γ○P Tγ○ u
+
1 − γ○γ○ 2d T ℓ t u t 4 ¿dP Tγ○ u T ℓ t u t ,
− t=1 Á t=1
∑ = ÁÀ ∑
( ) ( ) ( ) ( )
andotherwiseforγ○ γ wehave
min
≤
γ 1 γ T γ 1 γ○ T
2 1 m γi mn inP Tγmin u + − γ mim nin2d t=1ℓ t u t 2 1 m γi mn inP Tγmin u + − γ○ 2d t=1ℓ t u t
− −
∑ ≤ ∑
( ) ( ) ( ) ( )
T
4dPγmin u 2 dPγ○ u ℓ u ,
T + ¿ T t t
Á t=1
≤ ÁÀ ∑
( ) ( ) ( )
socombiningthesetwoboundsandpluggingbackintotheregretboundabove,wehave
T γT−t x 2
R u γλ u 2 4dmax y yRef 2log 1 t=1 t 2
T 1 2+ t t − t + λd
∑
≤ ⎛ ∥ ∥ ⎞
( ) ∥ ∥ ( )
T⎝ ⎠
4dPγmin u 4 dPγ○ u ℓ u
+ T + ¿ T t t
Á t=1
ÁÀ ∑
( ) ( ) ( )
T
O dPγmin u dmax y yRef 2log T dPγ○ u ℓ u .
T + t t − t +¿ T t t
⎛ Á t=1 ⎞
≤ ÁÀ ∑
⎜ ( ) ( ) ( ) ( ) ( )⎟
⎝ ⎠
B.1.1.PROOF OF LEMMA B.1
Lemma B.1. Let yRef R be an arbitrary reference point, available at the start of round t, and let
y R yRef M yt yRef M forM max y yRef. SupposethatweapplyAlgorithm1withhintsy B yt
t t t∈ t t s<t s s t t =
∶ − + − ∶
Clip x ,w . Thenforanysequenceu u ,...,u inRd andanyγ,β 0,1 suchthatβ γ γ 2d ,
{ ∈B t t t ≤ ≤ } = 1 ∣ T ∣ min ̃2d+=1 =
= ∈ ≥ ≥ =
(⟨ ⟩) ( ) [ ]
T γT−t x 2
R u γλ u 2 4dmax y yRef 2log 1 t=1 t 2
T 1 2+ t t − t + λd
∑
≤ ⎛ ∥ ∥ ⎞
( ) ∥ ∥ β ( 1 γ ) T
+2
1
βP Tβ u
+
−
γ
2d ℓ t⎝u t ⎠
t=1
−
∑
( ) ( )
21OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
Proof. ApplyingTheorem3.1followedbyLemma3.2,foranyγ 0,1 andβ γ wehave
RA γ u γλ u 2 d max y y 2∈ lo( g 1] T t=1≥ γT−t x t 2 2
T 2 1 2+ 2 t t − t + λd
∑
≤ ⎛ ∥ ∥ ⎞
( ) ∥ T∥−1 ( ) d T
+γ F tγ u t+1 −F tγ u t +⎝ 2log 1 γ y t −⎠y t 2
t=1 t=1
∑ ∑
γλ [ d ( ) ( )] T( /γT)−t (x 2 )
u 2 max y y 2log 1 t=1 t 2
2 1 2+ 2 t t − t + λd
∑
≤ ⎛ ∥ ∥ ⎞
∥ β∥ ( 1 γd) T
+ 1 βP Tβ u + − γ 2 y⎝ t −y t 2, ⎠
t=1
−
∑
( ) ( )
UsingLemmaD.1wehave
T T T
y y 2 M2 M2 2ℓ w M2 2 ℓ w ,
t t t+1 t t t T+1 t t
− − + +
t=1 t=1 t=1
∑ ≤∑ ≤ ∑
( ) [ ( )] ( )
soforanyγ 2d ,wehave
2d+1
≥ 1 γ d T 1 γ 1 T
− γ 2 y t −y t 2 − γ d 2M T2 +1 + ℓ t w t
t=1 t=1
∑ ≤ ∑
( ) 1 γ [1 T ( )] T
−
γ
d 2M T2 +1
+
ℓ t w t −ℓ t u t
+
ℓ t u t
t=1 t=1
= ∑ ∑
1 [ 1 T ( ) (1 )γ T ( )]
4M T2 +1
+ 2
ℓ t w t −ℓ t u t
+
−
γ
d ℓ t u t ,
t=1 t=1
≤ ∑ ∑
wherethefinalinequalityusesγ 2d Ô⇒ 1−γ 1 andbou( nds) 1−γd( T) ℓ w ℓ u( ) 1 T ℓ w ℓ u
2d+1 γ 2d γ t=1 t t − t t 2 t=1 t t − t t
(assuming T t=1ℓ t w t −ℓ t u t ≥ 0, which can be≤assumed withoutloss ∑of gene (rali )tysin (ce o )th≤erw ∑ise the (sta )ted bo (und
)
triviallyholds).Pluggingthisbackintotheregretboundandre-arrangingterms,wehave
∑ ≥
( ) ( )
γλ d T γT−t x 2
R u u 2 max y y 2log 1 t=1 t 2
T 2 1 2+ 2 t t − t + λd
∑
≤ ⎛ ∥ ∥ ⎞
( ) ∥ γ∥ ( 1 ) 1 γ T
+ 1
γP Tγ u
+
2R T u
+
⎝ −
γ
ℓ t u t ⎠
t=1
−
∑
( ) ( ) T( γ)T−t x 2
Ô⇒ R u γλ u 2 4dmax y yRef 2log 1 t=1 t 2
T 1 2+ t t − t + λd
∑
≤ ⎛ ∥ ∥ ⎞
( ) ∥ ∥ β ( 1 γ ) T
+2
1
βP Tβ u
+
−
γ
2d ℓ t⎝u t , ⎠
t=1
−
∑
( ) ( )
wherewe’veboundedmax t y t y t 2 4M T+1 4max t y t y tRef 2usingLemmaD.1.
− −
≤ =
( ) ( )
C. Proofs forSection 3.2(Dimension-dependent Lower Bound)
C.1.ProofofTheorem3.5
Theorem3.5. Foranyd,T 1andP,Y 0suchthatdP 2TY2,thereisasequenceoflossesℓ w 1 y x ,w 2
and a comparatorsequence ≥u u 1,... >,u
T
satisfying m ≤ax
t
y
t
Y and T t=− 11max
s
ℓ
s
u t+1t (−)ℓ
=s
u2
(t
t +−
⟨
Pt su ⟩c )h
that
= ≤ ∑ ≤
( ) ∣ ∣ [ ( ) ( )]
T
R T u Ω dY2log T dP ¿dP y t y t−1 2 .
+ + −
⎛ Á t=2 ⎞
≥ ÁÀ ∑
( ) ⎜ ( ) ( ) ⎟
⎝ 22 ⎠OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
Proof. First notice that the trivial comparator sequence with u ... u always satisfies
1 T
T t=2max s ℓ s u t+1 −ℓ s u t + 0 P, so we can always lower-bound the =dynamic re =gret using the well-known
lower bound for the static regret in this setting (see, e.g., Vovk (2001); Gaillardetal. (2019); Mayoetal. (2022)). In
∑ = ≤
particular,f[ora(nyu) W w(eh)]ave
∈ sup R u Ω dY2log T (9)
T
y1,...,yT
≥
( ) ( ( ))
Next, let σ 0,1 and let σ ,...,σ be a sequence of iid random variables drawn uniformly from σ,σ , and let
1 t
−
y Yσ . Choose feature vectors x which cycle through the standard basis vectors (e.g. define ι t t mod d 1
ant d let xt ∈ e[ )]. Now observe tt hat the comparator sequence can always exactly fit a sequence y{,...,y} by s+ et-
= t ι t 1= T
ting u
t
to s =atisfy
(
)x t,u
t
u
t,ι t
y t. In particular, by letting u
1
y 1,...,y
d
, u
2
y d+1,..(.,)y
2d
,(...,u T)
d
y ,...,y ,0,0,... wecansetu u toguarantee x ,u y onallrounds,whileonlychangingthecom-
parT atod r+1 T d timT e⟨satm⟩os= t. Fro( m) th= is,wt ehavt ed thefollowinginitit ãlbt= ou(ndt onthere)gr̃et:= ( ) ̃⌈ / ⌉ =
( ⌈ / ⌉ ) = ̃⌈/ ⌉ ⟨ ⟩=
⌈ / ⌉ T
sup R u E ℓ w ℓ u
T y t t t t
y1,...,yT [ t=1 ( )− ( )]
( )≥ ∑1 1
E y2 x ,w 2 y x ,w
y [2 t + 2⟨ t t ⟩ + t ⟨ t t ⟩]
≥ 1
σ2Y2T, (10)
2
where the last line uses y2 Y2σ2 and E y ≥ 0. Moreover, since the comparator changes only every d rounds, the
t t
variabilityisboundedas
= =
[ ]
T−1 T d −1
m sax ℓ s u t+1 −ℓ s u t + ⌈ / ⌉ m sax ℓ s u i+1 −ℓ s u i +.
t=1 i=1
∑ ≤ ∑
[ ( ) ( )] [ (̃ ) (̃)]
Observethatℓ s u i+1 ℓ s u i canonlybepositivewhen x s,u i y sand x s,u i+1 y s,hence
− −
T−1 T d −1= =
(̃ ) (̃) ⟨ ̃⟩ ⟨ ̃ ⟩
m sax ℓ s u t+1 −ℓ s u t + ⌈ / ⌉ m sax ℓ s u i+1 −ℓ s u i +
t=1 i=1
∑ ≤ ∑
[ ( ) ( )] T d −11 [ (̃ ) (̃)]
y y 2
s s
⌈ / ⌉ 2 − −
i=1
≤ 2T∑ Y2 ( ( ))
σ2.
d
≤
Hence,settingσ 2TdP Y2 1ensures T t=− 11max s ℓ s u t+1 −ℓ s u t + 2T dY2 σ2 P,andtheregretisboundedbelow
by √
= ≤ ∑ ≤ ≤
[ ( ) ( )]
1 1
sup R u σ2Y2T dP,
T
2 4
y1,...,yT
≥ =
whichwecanfurtherlowerbo(un)das:
1 1 T−1
4 dP ⋅dP 4¿dP ⋅d m sax ℓ s u t+1 −ℓ s u t +
√ Á t=1
= ≥ ÁÀ ∑
[ ( ) ( )]
1 T−1 T 1
4¿dP ℓ t u t+1 −ℓ t u t + Ω ¿dP 2 y t −y t−1 2 . (11)
Á t=1 ⎛Á t=2 ⎞
≥ ÁÀ ∑ = ÁÀ ∑
[ ( ) ( )] ⎜ ( ) ⎟
TakentogetherwithEquation(9),wehave
⎝ ⎠
R u Ω dY2log T dP
T T
∨ V
√
where VT dP
∨
T t=2 21 y t −y t−1 2. ( )≥ ( ( ) )
= ∑
( )
23OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
D. Proofs forSection 4 (Learning the OptimalDiscount Factor)
D.1.ProofofLemmaD.1
Thefollowinglemmashowsthatbyclippingourpredictionstosomecrude“trust-region”,thelossoftheclippedprediction
isatworstprortionaltothemaximaldeviationofthetruey fromthetrustregion.Intuitively,wecanthinkofyRefasbeing
t
somedata-dependentbutalready-observedquantity,suchasy t−1.
LemmaD.1. DefineM max y yRef, x R yRef M x yRef M ,andlety Clip x ,w for
t s<t s − s Bt ∶ t − t t + t t B t t t
somew Rd. Thenforanytwehave
t = = ∈ ≤ ≤ =
∣ ∣ { } (⟨ ⟩)
∈
y y 2 min 4M2 ,2ℓ w M2 M2
t t t+1 t t t+1 t
− + −
≤
( ) { ( ) }
Proof. First,observethatwealwayshave
y y 2 y yRef yRef y 2 2 y yRef 2 2 yRef y 2 2M2 2M2 4M2 .
t t t t t t t t t t t+1 t t+1
− − + − − + − +
= ≤ ≤ ≤
Next,observe(thatif )x ,w( y ,thenwetrivia)llyhav(e y y )2 y( x ,w ) 2 2ℓ w .Otherwise,when x ,w
t t t t t t t t t t t t
y , we have clippedy to be a distance of M away fromy−Ref and the− reare two cases to consider. If Sign y yRef
St ign y yRef ,then⟨ tt hecli⟩ p= pingoperationyt Clip ( x ,t w) = m( oves⟨ usclos⟩ e) rt= oy ,h( enc) e y y y t x⟨− ,wt .⟩ I≠ f
t − t t B t t t t t − t t −( t t ) ≠
Sign y t y tRef Sign y t y tRef ,thenwepre =ciselyhave y t y t M t+1 M twheny t tand y t y ≤t y t x t,w t
( − ) − (⟨ −⟩) − B ∣ −∣ ∣ ⟨− ⟩∣
wheny . Hence,combiningthesecaseswehave
(
t Bt )=
( ) ∣
∣= ∉
∣
∣≤
∣ ⟨ ⟩∣
∈
y t y t 2 y t w t,x t 2 M t+1 M t 2 2ℓ t w t M t2 +1 M t2,
− − + − + −
≤ ≤
wherewehaveused u (l 2 u2) l2(foru⟨ l 0⟩.)Hen(ce,combinin)gwitht(hefi)rstdisplaywehave
− −
≤ ≥ ≥
( ) y y 2 min 4M2 ,M2 M2 2ℓ w .
t t t+1 t+1 t t t
− − +
≤
( ) { ( )}
D.2.ProofofLemmaD.2
Thefollowinglemmashowsthefollowingimportantpropertyofthemeta-learner’slosses: theyareα -exp-concavewith
t
α 1 inthedomainY y N p y i N p 1 .
t 2maxiℓt y t(i) t i=1 i t ()∶ i=1 i
Lem= ma D.2.( Le)t y 1 ,...,y N ̂be= a{rbit= ra∑ ry real numb∑ ers and= let}Y
t
y N i=1p iy i p RN ≥0, N i=1p
i
1 . Then
∶
ℓ y 1 y y 2i(sα) -Exp-(Co)ncaveonY forα 1 . ()
t 2 t − t t t 2maxiℓt y(i) ̂ = { =∑ ∈ ∑ = }
( )= ( ) ̂ ≤ ( )
Proof. Lettingf y exp α ℓ y wehaveforanyy Y :
t t t t
−
(
)=
( ( )) α
∈′̂
α
f′ y exp t y y 2 exp t y y 2 α y y
t − 2 t − − 2 t − t t −
= α =
f′′(y) [exp ( t y( y 2) )]α2 y (y 2 (α ) ) ( )
t − 2 t − t t − − t
= α
( ) exp( t(y y)2)[2α(2ℓ y ) α ]
− 2 t − t t − t
=
( ( ) )[ ( ) ]
Henceforα 1 wehave
t 2maxiℓt y(i)
≤
( ) α
f′′ y exp t y y 2 α 2α ℓ y 1 0
t − 2 t − t t t −
≤ ≤
( ) ( ( ) ) [ ( ) ]
sof y exp α ℓ y isconcaveandℓ isα -Exp-ConcaveoverY forα 1 .
t − t t t t t 2maxiℓt y(i)
( )= ( ( )) 24 ̂ ≤ ( )OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
D.3.RegretoftheRange-ClippedMeta-Algorithm
Inthissectionweproveasimpleresultshowingthattherange-clippingreductiondescribedbyAlgorithm2incursonlyan
constantadditionalpenalty.Thislemmawillbeusedtodomostoftheheavy-liftinginprovingTheorem4.1,whichsimply
appliesthefollowinglemmaandthenchoosesaspecificmeta-algorithmforA .
Meta
LemmaD.3. Forany a,b 1,T ,sequenceu u ,...,u inR,andj N ,Algorithm2guarantees
a b
⊆ = ∈
[ ] [ R ] u 1 max( y yRef 2) RA j u R[ Me] ta e ,
a,b 2 t t − t + a,b + a,b j
[ ]( )≤ ( ) [ ]( ) [ ]( )
whereRA j u b ℓ w j ℓ u isthedynamicregretA andRMeta e b ℓ y ℓ y j .
a,b t=a t t − t t j a,b j t=a t t − t t
( ) ( )
[ ](
)=∑
( ) ( ) [ ](
)=∑
( ) ( )
Proof. Foreaseofnotationwelety i x ,w i ,wherew i istheoutputofalgorithmA ,andslightlyabusenotation
t t t t i
⟨ ⟩
bywritingℓ
t
(y
)
21 (y
t
−y )2fory (R) .=Hence,w( e) maywriteℓ( t) (w
t
)
ℓ
t
(y ti )interchangeably.Notethatthisequivalence
is valid in the improperonline learningsetting since the featuresare obse(rv)ed before the learner makes a prediction, as
= ∈ ≡
discussedintheintroduction.
Now,forforanyj N wehave
∈
[ ]
b
R u ℓ y ℓ u
a,b t t t t
−
t=a
[ ]( )=∑ b ( ) ( ) b
ℓ w j ℓ u ℓ y ℓ w j
t t t t t t t t
− + −
t=a ( ) t=a ( )
=∑ ∑
( ) b ( ) ( ) ( )
RA j u ℓ y ℓ y j ,
a,b + t t − t t
t=a ( )
= [ ]( ) ∑ ( ) ( )
wherewehaveobservedy j x ,w j . ObservethatbyLemmaD.1wehave
t t t
⟨ ⟩
( ) ( )
=
1 1 1
ℓ y j M2 M2 y y j 2
t ( t ) 2 t − 2 t+1 + 2( t − t )
( ) ( )
≥ 1 1
M2 M2 ℓ y j ,
2 t − 2 t+1 + t ( t )
( )
=
whereM max y yRef. Hence,
t s<t s s
∣ − ∣
=
b
R u RA j u ℓ y ℓ y j
a,b ( ) a,b ( )+ t ( t )− t ( t )
t=a ( )
[ ] ≤ [ ] ∑ b
RA j u ℓ y ℓ y j
a,b ( )+ t ( t )− t ( t )
t=a ( )
≤ [ ]b 1 ∑ 1
M2 M2
+ 2 t+1 − 2 t
t=a
∑
1 M2 RA j u b ℓ y ℓ y j
2 b+1 + a,b ( )+ t ( t )− t ( t )
t=a ( )
≤ [ ] ∑
´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶
=∶RM [ae ,ta
b]
ej
( )
25OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
D.4.ProofofTheorem4.1
T Thh ee nor fe om ra4 n. y1. seL qe ut eA ncM ee utabe (a un 1,in ..s .ta ,n uc Te )o if nA Rlg ao nr dith am ny3 jwith Nα ,t A=lg2 om ra itx ht, mi1 ℓt
2
(y g( t ui)
a
)r, aβ ntt e+ e1
s=
(e+t )log1 2 (e+t )+1 andp 1 =1 N /N.
= ∈
R u O RA j u m[ ax] y y 2log Nblog2 b ,
a,b a,b + t t − t
[ ]( )≤ ( [ ]( ) ( ̃) ( ( )))
whereR denotesregretoverthesub-interval a,b .
a,b
Proof. T[ hep] rooffollowsalmostimmediatelyusin[gthe]regretguaranteeoftherange-clippedmeta-algorithm(LemmaD.3),
fromwhichwehave
R u 1 max y y 2 RA j u RMeta e .
a,b 2 t t − t + a,b + a,b j
Nowapplyingtheguaranteeofanap[pro](pri)at≤ einstanc(eofth̃e)fixed-s[har]e(alg)orith[m(]T(heo)remE.1withα 1 ,
t 2maxt,iℓt y( ti)
β 1 ,andp 1 N),wehave =
t e+t log2 e+t +1 1 N ( )
= ( ) ( ) R= Meta / e 1 2log 1 1
a,b j α b+1 β b+1p 1j +
[ ]( )≤ maxℓ[ y i ( 2log )e b] log2 e b 1 N 1
t t
t,i + + + +
()
≤
O
ma(x
y
)[
y
2lo( g(( blog)2
b
N(
,
) ) ) ]
t t
t −
wherethelastlineappliesLemmaD.1≤and(hides(constãn)ts. All(togeth(er,)we)h)ave
R u O RA j u max y y 2log Nblog2 b .
a,b a,b + t t − t
[ ]( )≤ ( [ ]( ) ( ̃) ( ( )))
D.5.ProofofTheorem4.2
The proofof Theorem 4.2 follows by applyingTheorem 4.1, and then showing that there exists a A which attains the
γ
desiredbound. WefirstprovideproofofthelatterclaiminLemmaD.4forthesakeofmodularity. Inparticular,wewill
alsore-usethisresulttoarguestrongly-adaptiveguaranteesinSection5. ProofofTheorem4.2istheneasilyprovenatthe
endofthissection.
Lemma D.4. Let b 1, η 2d, η dT, and define S η η bi η i 0,1,... and S
min max η i min max γ
∧ ∶
γ
i
1+ηi
ηi
∶i 0,1,..>.
∪
0 . Fo=r any γ in S =γ, let A
γ
denote an inst=an {ce o=f Algorithm 1 with=discount }γ. Then fo=r
a{ny=u u ,.= ..,u in}Rd{,th}ereisaγ∗ 0,1 satisfyingγ∗ d T t=11 2 yt−yt 2 andaγ S suchthat
1 T d T t√ =11 2∑yt−yt( 2+̃)P Tγ∗ u γ
= ∈ = √ √ ∈
( ) [ ] ∑ ( ̃) T ( )
RA γ u O dmax y y 2log T b dPγ∗ u y y 2 .
T t t − t + ¿ T t − t
⎛ Á t=1 ⎞
≤ ÁÀ ∑
( ) ( ̃) ( ) ( ) ( ̃)
Proof. DenoteV d T y y 2.⎝ ByLemmaA.5,thereexistsaγ∗ 0,1 suchthat ⎠
T 2 t=1 t − t
= ∑ V ∈
( ̃) γ∗ T . [ ]
V √ Pγ∗ u
= T + T
√
Throughoutthe proof it will be convenientto work in√ terms of the(rel)ated quantity η∗ γ∗ VT . Let us first
1−γ∗ Pγ u
supposethat0 η∗ η . Inthiscase,wehave √ T
min = =
( )
≤ ≤ V 1 T 1
η∗ T η Ô⇒ y y 2 η Pγ u .
¿Pγ∗ u min ¿2 t − t min√d T
Á T Á t=1
=ÁÀ ≤ ÁÀ ∑
( ̃)
≤
( )
( )
26OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
ConsiderthealgorithmA withγ 0: inthiscasewe havew argmin h w , so x ,w y andtheregretis
0 t w∈Rd t t t t
trivially
= = =
( ) ⟨ ⟩ ̃
T T 1
ℓ wA0 ℓ u y y 2
t t − t t 2 t − t
t=1 t=1
∑ ≤∑
( ) ( ) ( ̃)
T 1 T 1
y y 2 y y 2
¿ 2 t − t 2 t − t
Át=1 t=1
=ÁÀ∑ ∑
( ̃) ( ̃)
η T 1
min Pγ∗ u y y 2
d ¿ T 2 t − t
Á t=1
≤ ÁÀ ∑
2√ V Pγ∗ u( ) ( ̃) (12)
T T
√
=
forη min 2d. ( )
Otherwis=e,forη∗ η ,usingTheorem3.1wehavethatforanyγ S ,
min γ
≥ RA γ u γλ u 2 d max y y 2lo∈ g 1 T t=1γT−t x t 2 2
T 2 1 2+ 2 t t − t + λd
∑
≤ ⎛ ∥ ∥ ⎞
( ) ∥ T∥−1 ( ̃)
γ F tγ u t+1 F tγ u t ⎝log 1 γ V T ⎠
+ − +
t=1
∑
∗ γλ [ d( ) ( )] ( /T)γT−t x 2
u 2 max y y 2log 1 t=1 t 2
( ) 2 1 2+ 2 t t − t + ∑ λd
≤ ⎛ ∥ ∥ ⎞
η∥ ∗ P∥ γ∗ u V T( ̃) ⎝ ⎠
+ T + η
( )
where observesthatη γmin η∗ γ∗ Ô⇒ γ γ∗andappliesLemma3.2(withβ γ∗)andsubstitutes
η γ .∗ Ifη∗ η thenm cin hoos1 i− nγ gmi ηn η 1− dγ T∗ yields min
1−γ max = ≤ ma=x ≤ =
( )
= ≥ = =
V d T 1
T y y 2 max y y 2,
t t t t
η 2dT − 2 t −
t=1
= ∑ ≤
( ̃) ( ̃)
andotherwise,thereisanη inS suchthatη η∗ bη ,sochoosingη η yields
k η k k k
V≤ ≤V =
T b T b Pγ∗ u V
η η∗ T T
k √
≤ =
( )
Hence,overallwehavethatthereisaγ S suchthat
γ
∈
R TA γ (u )≤
=
γ γ2 2λ λ∥ ∥u u1 1∥ ∥2 2
2
2+
+
2 21 1m mt ta ax x( (y yt t− −y ỹ ̃t t) )2 2⎡ ⎢ ⎢ ⎢ ⎢⎣
⎡
⎢
⎢
⎢⎣d dl lo og g⎛ ⎝
⎛
⎝1 1+
+
∑ ∑T t T t= =1 1γ γλ λT Td d− −t t∥ ∥x xt t∥ ∥2 2 2 2⎞ ⎠
⎞
⎠∨ ∨1 1⎤ ⎥ ⎥ ⎥
⎥ ⎦⎤
⎥
⎥
⎥+ +η (b∗ P +Tγ 1∗ )( √u V) T+ Pb TγV η ∗T ∗
(u
)
⎢ T ⎥
O dmax y y 2log T b dPγ∗ u y y 2 . ⎦
t t − t ∨ ¿ T t − t
⎛ Á t=1 ⎞
≤ ÁÀ ∑
⎜ ( ̃) ( ) ( ) ( ̃) ⎟
⎝ ⎠
Withthepreviouslemmainhand,theproofofTheorem4.2followseasily. Thetheoremisre-statedforconvenience.
27OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
Theorem4.2. Letb 1,η 2d,η dT,andforalli Nletη η bi η ,andconstructthesetofdiscount
min max i min max
∧
factorsS γ ηi i N 0 . Foranyγ inS , letA denoteaninstanceofAlgorithm1withdiscountγ.5 Let
γ i 1+>ηi
∶
=
∪
= γ ∈γ =
A
Meta
bean=in {sta=nceofthe∈alg }orit {hm }characterizedinTheorem4.1,andsupposewesety tRef y
t
forallt. Thenforany
u u ,...,u inRd,Algorithm2guarantees
1 T =
̃
=
( )
R u O dmax y yRef 2log T
T t t
t −
≤
( ) ( ( ) ( )
T
b dPγ∗ u y y 2
+ ¿ T t − t
Á t=1
ÁÀ ∑
( ) ( ̃) )
whereγ∗ 0,1 satisfiesEquation(2).
∈
[ ]
Proof. ApplyingTheorem4.1,foranysequenceu u ,...,u inRdandanyγ S wehave
1 T γ
= ∈
( )
R u O RA γ u max y yRef 2log NT
T T + t t − t
( )≤ Ô(RA γ(u) max(y yRef)2log(T ),) (13)
T + t t − t
≤ ̂( ( ) ( ) ( ))
wherethelastlineusesN S log η η O log T ,thenhideslog log factors. Finally,byLemmaD.4,
γ b max min b
thereisindeedaγ∗ 0,1=satisfy=ingγ∗ d ≤T t=11 2 yt−yt 2 andaγ S suchthat
∣ ∣ ( d/
T
t√ =1)
21
∑yt−y(
t(
2+̃(
)P
Tγ)∗)
u
γ( )
∈ = √ √ ∈
[ ] ∑ ( ̃) ( )
T
RA γ u O dmax y y 2log T b dPγ∗ u y y 2 .
T t t − t + ¿ T t − t
⎛ Á t=1 ⎞
≤ ÁÀ ∑
( ) ⎜ ( ̃) ( ) ( ) ( ̃) ⎟
⎝ ⎠
PluggingthisbackintoEquation(13)andchoosingyRef y provestheresult.
t t
=
D.6.ProofofTheorem4.3 ̃
As in Appendix D.5, the proof of Theorem 4.3 follows by applying Theorem 4.1 and then showing that there is a A
γ
attainingthedesiredregretbound. WefirstprovideproofofthelatterclaiminLemmaD.5forthesakeofmodularity,so
thatwecanuseitwhenarguingstrongly-adaptiveguaranteesinSection5. ProofofTheorem4.3isprovenattheendof
thissection.
LemmaD.5. UnderthesameconditionsasLemmaD.4,supposeeachA setshintsy yγ Clip x ,wγ ,where
γ t t B t t t
B yRef M ,yRef M and M max y yRef. Then for any u u ,...,u in W, there is a γ○ 0,1
t t t t t t s<t s s 1 =T =
− + − ̃ (⟨ ⟩)
sati=sfy[ingγ○ d T t√ =1d ℓt∑uT t= t1 ]ℓ +t (u dt P)Tγ○ u=andaγ∣ S γ such∣that = ( ) ∈ [ ]
= √ √ ∈
∑ ( ) ( )
RA γ u O dPγmin u dmax y yRef 2 log T
T T + t t − t
≤
( ) ( ( ) ( ) ( )
T
b dPγ○ u ℓ u ,
+ ¿ T t t
Á t=1
ÁÀ ∑
( ) ( ))
whereγmin min γ S 2d .
γ 2d+1
5For brevi=ty, here {we∈refe }r=to an algorithm that directly predicts ỹt on every round as being an instance of the discounted VAW
forecasterwithγ=0.ThisterminologycanbejustifiedbyRemarkA.2,butforourpurposeshereit’ssufficienttoconsideritconvenient
alias.
28OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
Proof. UsingLemmaB.1,foranyu u ,...,u ,γ 0,1 ,andβ γ γ 2d ,wehave
1 T min 2d+1
R u γ= λ( u 2 4d) max∈ y( y)Ref 2log≥ 1≥ T t=1=γT−t x t 2 2
T 1 2+ t t − t + λd
∑
≤ ⎛ ∥ ∥ ⎞
( ) ∥ ∥ β ( 1 γ ) T
+2
1
βP Tβ u
+
−
γ
2d ℓ t⎝u t , ⎠
t=1
−
∑
( ) ( )
Wewillproceedbyshowingthatthereisaβandγthatsuitablybalancesthesummationsinthelastline.Tothisend,recall
thatbyLemmaA.5,thereisaγ○satisfying
d T ℓ u
○ t=1 t t
γ √
= √d T t=1ℓ t∑u t +( √P) Tγ○ u
Denoteη 1−γ
γ
andη○ 1−γ γ○
○
√d ∑PT t T= γ1 ○ℓt
u
(ut ). Ifη○∑ η max( ) 1−γm γmax ax,the(n)wecantakeβ γ○andγ γ maxtoget
= = = ≥ = = =
β Pβ u γ( ) d T ℓ u η○ Pγ○ u d T t=1ℓ t u t
1 −β T + 1 −γ t ∑=1 t t
=
T + ∑η max
( )
( ) ( ) ( ) T d T ℓ u
dPγ○ u ℓ u t=1 t t
=¿ Á ÁÀ T t ∑=1 t t + ∑η max ( )
( ) ( )
T
dPγ○ u ℓ u maxℓ u ,
¿ T t t + t t t
Á t=1
≤ÁÀ ∑
wherethelastlinerecallsη dT.Otherwise,ifη○ η γm(in ) 2d,(then)takingβ( γ) γ yields
max min 1−γmin min
= d T ℓ u ≤ = = d T ℓ u = =
η Pγmin u t=1 t t η Pγmin u t=1 t t
min T + ∑η min min T + ∑ η○
( ) ≤ ( )
( ) ( ) T
2dPγmin u dPγ○ u ℓ u .
T +¿ T t t
Á t=1
= ÁÀ ∑
Lastly,ifη η○ η ,thereisaη γk S suchthatη (η○) bη ,soch(oo)singβ( γ)○ andγ γ yields
min max k 1−γk η k k k
≤ ≤ η○ Pγ○ u= d ∈T t=1ℓ t u t η○ Pγ≤ ○ u≤ bd T t=1ℓ t u t = =
T + ∑ η k T + ∑ η○
( ) ≤ ( )
( ) ( ) T
b 1 dPγ○ u ℓ u
+ ¿ T t t
Á t=1
= ÁÀ ∑
Combiningthethreecases,wehave ( ) ( ) ( )
β 1 γ T T
2
1
βP Tβ u
+
−
γ
2d ℓ t u t 4dP Tγmin u +2m taxℓ t u t +2 b +1 ¿dP Tγ○ u ℓ t u t
− t=1 Á t=1
∑ ≤ ÁÀ ∑
( ) ( ) ( ) ( ) ( ) ( ) ( )
Hence,overalltheregretcanbeboundas
RA γ u γλ u 2 dmax y yγ 2log 1 T t=1γT−t x t 2 2
T 1 2+ t t − t + λd
∑
≤ ⎛ ∥ ∥ ⎞
( ) ∥ ∥ ( )
⎝ ⎠ T
4dPγmin u 2maxℓ u 2 b 1 dPγ○ u ℓ u
+ T + t t t + + ¿ T t t
Á t=1
ÁÀ ∑
( ) ( ) ( ) ( ) ( )
T
O dPγmin u dmax y yRef 2log T b dPγ○ u ℓ u ,
T + t t − t + ¿ T t t
⎛ Á t=1 ⎞
≤ ÁÀ ∑
⎜ ( ) ( ) ( ) ( ) ( )⎟
⎝ ⎠
29OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
where we’ve applied Lemma D.1 to bound max y yγ 2 4M2 4max y yRef 2. Plugging this back into
t t
−
t T+1 t t
−
t
Equation(14)provesthestatedbound.
≤ =
( ) ( )
Now the proofof Theorem4.3followsby composingTheorem4.1and Lemma D.5. The theoremis restated below for
convenience.
Theorem4.3. UnderthesameconditionsasTheorem4.2,supposeeachA setshintsy yγ Clip ,x ,wγ ,where
γ t t B t t t
B yRef M ,yRef M andM max y yRef. Thenforanyu u ,...,u inRd,Algorithm2guarantees
t t − t t + t t s<t s − s 1 T ̃ = = (⟨ ⟩)
= = =
[ ] R u O∣ dPγmin∣u dmax y y(Ref 2 log T)
T T + t t − t
≤
( ) ( ( ) ( ) ( )
T
b dPγ○ u ℓ u
+ ¿ T t t
Á t=1
ÁÀ ∑
whereγ 2d andγ○ 0,1 satisfiesEquation(3). ( ) ( ))
min 2d+1
= ∈
Proof. AsintheproofofTh[ eorem] 4.2,weapplyTheorem4.1,fromwhichitfollowsthatforanyu u ,...,u inRd
1 T
andanyγ S ,thedynamicregretisboundedas
γ =
( )
∈ R u O RA γ u max y yRef 2log NT
T T + t t − t
( )≤ Ô(RA γ(u) max(y yRef)2log(T ),) (14)
T + t t − t
wherethelastlineusesN S log η≤ ̂(η ( )O log (T ,then)hides(log))log factors. AndusingLemmaD.5,
γ b max min b
foranyu u ,...,u th=ereisa=γ○ 0,1 satisfyin≤gγ○ d T t=1ℓt ut andaγ S suchthat
1 T ∣ ∣ ( / ) ( ( )√) ( ) γ
d T t=1∑ℓt ut + ( P)T○ u
= ∈ = √ √ ∈
( ) [ ] ∑ ( ) ( )
T
RA γ u O dPγmin u dmax y yRef 2log T b dPγ○ u ℓ u ,
T T + t t − t + ¿ T t t
⎛ Á t=1 ⎞
≤ ÁÀ ∑
( ) ⎜ ( ) ( ) ( ) ( ) ( )⎟
PluggingthisbackintoEquation(14)completestheproof.
⎝ ⎠
E. Adaptive Fixed-share
Algorithm3:AdaptiveFixed-Share
InputExpertsA ,...,A ,p ∆
1 N 1 N
fort 1 T do
∶ ∈
Gety i fromA foralli
= t i
Playy() N p y i
t i=1 ti t
Observelossℓ y ()1 y y 2andletℓ ℓ y i foralli
=∑ t 2 t − ti t t
L Ce ht oq ot s+ e1, βi
t=+1
∑aN j n=p (1 dti p se )tx ejp t=e (x p− p tα (+(−t 1ℓ αt ti ℓ )tj
1
)) −fo βr t+al 1li
q t+1
+=
β
t+( 1p( 1) )
end
=
( )
Inthissection,weprovideforcompletenessanalysisrelatedtothefixed-sharealgorithm(Cesa-Bianchietal.,2012)with
time-varyingmodulus.ThefollowingisamodestgeneralizationoftheanalysisofHazan(2019,Theorem10.3).Through-
outthissectionweassumethatthelossesℓ Y →Rareexp-concaveintheirdomain.
t
∶
Theorem E.1. For all t let ℓ t be an α t-Exp- ̂Concave function and assume that α t α t+1 for all t. For all t, set β t
1 . Thenforanyj N andany a,b 1,T ,Algorithm3guarantees
e+t log2 e+t +1 ≥ ≤
( ) ( ) ∈ [ b] [ ]⊆ [ 1] 1
ℓ y ℓ y j 2log 1
t=a t t − t t ( ) α b+1 β b+1p 1j +
∑ ≤
( ) ( ) [ ( ) ]
30OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
Proof. The heavy lifting is done mostly using Lemma E.2, after which the proof follows by choosing the sequence of
mixingparametersβ . ApplyingLemmaE.2andobservingthetelescopingsum,wehave
t
b b 1 1 1 1
ℓ y ℓ y j log log
t=a t t − t t ( ) t=aα t p tj − α t+1 p t+1,j
∑ ≤∑
( ) ( ) b 1( ) 1 ( )
log
+ t=aα t 1 β t+1
−
∑
b 1 ( 1 ) 1
log
+ t=a α t+1 − α t β t+1p 1j
∑
1 ∣ 1 1∣ ( 1 )
log log
α a p aj − α b+1 p b+1,j
=
b(1 ) 1 ( )
log
+ t=aα t 1 β t+1
−
∑
b 1 ( 1 ) 1
log .
+ t=a α t+1 − α t β t+1p 1j
∑
Nowobservethatwithβ t+1 e+t log1 2 e+t +1,usingtheeleme∣ntaryinequ∣ality(log 1 +y) ywehave
lo≤
g
( )1 ( )
log 1
β t+1 β t+1 ( 1 )≤
1 β t+1 + 1 β t+1 1 β t+1 e t log2 e t
− = − ≤ − = + +
sofornon-increasingα weha(ve ) ( )
t ( ) ( )
b 1 1 b 1 1
log
t=aα t 1 β t+1 t=aα t e t log2 e t
∑ − ≤∑ + +
( ) 1 b 1
( ) ( )
α b t=a e t log2 e t
≤
1
∑e+b+
1
+
α b ∫ e ( ylo) g2yd(y )
≤ e+b
1 1 1
−
andsimilarly,
=
α blog (y )RRRRRRRRRRRy=e≤ α b
b 1 1 1 1 b 1 1
log log
∑ ∑
t=a α t+1 − α t β t+1p 1j β b+1p 1j t=aα t+1 − α t
≤
∣ ∣ ( ) 1( )1
log ,
α b+1 β b+1p 1j
≤
sooverallwehave ( )
b ℓ y ℓ y j 1 log 1 1 log 1 log βb+1 1p1j +1
t∑ =a t t − t t ( ) α a p aj − α b+1 p b+1,j + ( α b+1 )
≤
( ) ( ) ( ) log pb+1(,j 1)
1 log 1 βb+1p1j +
α a p aj + ( α b+1 )
=
( ) log pb+1,j 1
1 log 1 βb+1p1j +
α b+1 1 −β a q aj +β ap 1j + ( α b+1 )
≤
1 ( 1 )
2log( ) 1
α b+1 β b+1p 1j +
≤ ≤
[ ( ) ]
31OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
E.1.ProofofLemmaE.2
Thefollowingprovidesaninitialone-stepboundtoworkfrom,whichweuseintheproofofTheoremE.1.
LemmaE.2. Foralltletℓ beanα -Exp-Concavefunction.Thenforanyj N ,Algorithm3guarantees
t t
∈
[ ]
1 1 1 1
ℓ y ℓ y j log log
t t − t t ( ) α t p tj − α t+1 p t+1,j
( ) ( )≤ 1( ) 1 ( )
log
+ α t 1 β t+1
−
1 ( 1 ) 1
log
+ α t+1 − α t β t+1p 1j
∣ ∣ ( )
Proof. Byα -Exp-Concavityofℓ ,wehavethaty↦exp α ℓ y isconcave.Hence,applyingJensen’sinequality:
t t t t
−
( ( ))
N N
exp α ℓ y p exp α ℓ y i p exp α ℓ
t t t ∑ ti t t t ∑ ti t ti
− − −
i=1 () i=1
≥ =
( ( )) ( ( )) ( )
andtakingthenaturallogarithmofbothsideswehave
N
α ℓ y log p exp α ℓ
t t t ∑ ti t ti
− −
i=1
≥
( ) 1( N ( ))
ℓ y log p exp α ℓ .
t t −α t i∑ =1 ti − t ti
≤
( ) ( ( ))
Hence,foranyj N wehave
∈
[ ]
1 N
ℓ y ℓ y j log p exp α ℓ ℓ
t t − t t ( ) −α t i∑ =1 ti − t ti − tj
≤
( ) ( ) 1 (N ( )) 1
log p exp α ℓ log exp α ℓ
∑ ti t ti t tj
−α t i=1 − + α t −
=
1 ( exp α(ℓ )) ( ( ))
t tj
log −
=
α t ∑N i=1p ti (exp −α )tℓ ti
1 ( p exp α ℓ )
tj t tj
log (− )
=
α t p tj∑N i=1p ti (exp −α )tℓ ti
1 ( q t+1,j )
log ( )
α p
t tj
=
1 [ ( 1 )] 1
log log .
α t p tj − q t+1,j
=
[ ( ) ( )]
32OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
Addingandsubtracting 1 log 1 ,
αt+1 pt+1,j
( ) 1 1 1 1
ℓ y ℓ y j log log
t t − t t ( ) α t p tj − α t+1 p t+1,j
≤
( ) ( ) 1( ) 1 (1 ) 1
log log
+ α t+1 p t+1,j − α t q t+1,j
1 1 ( 1 ) 1 ( )
log log
α t p tj − α t+1 p t+1,j
=
1( ) 1 1( )1
log log
+ α t p t+1,j − α t q t+1,j
( ) ( )
log qt+1,j pt+1,j αt
´¹¹¹¹¹¹¹¹¹¹¹¹¹1¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹1¹¹¹¹¹¹¹¹¹ (¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸
/
¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹1¹¹¹¹¹¹¹¹¹¹¹¹ )¹¹¹¹¹¹ /¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶
log
+ α t+1 − α t p t+1,j
[ ] ( )
recallingp t+1,j 1 β t+1 q t+1,j β t+1p 1j,
− +
=
( ) 1 1 1 1
log log
α t p tj − α t+1 p t+1,j
=
1( ) q t(+1,j )
log
+ α t 1 β t+1 q t+1,j β t+1p 1j
− +
1 ( 1 1 )
( log )
+ α t+1 − α t 1 β t+1 q t+1,j β t+1p 1j
− +
1 [ 1 ]1 ( 1 )
log log( )
α t p tj − α t+1 p t+1,j
≤ 1( ) 1 ( )
log
+ α t 1 β t+1
−
1 ( 1 ) 1
log
+ α t+1 − α t β t+1p 1j
∣ ∣ ( )
33OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
F. Strongly-Adaptive Guarantees
InthissectionweprovideaformalstatementoftheresultsketchedinSection5. Theresultfollowseasilyfromtheresults
inSection4,afterborrowingthegeometriccoveringintervalsfromDanielyetal.(2015).
Theorem F.1. Let S be the set of discount factors defined in Theorem 4.2, let S denote a set of geometric covering
γ
intervalsover 1,T ,andforeachγ S andI S,letA beaninstanceofAlgorithm1usingdiscountγ andapplied
γ γ,I
duringintervalI(andpredictsyReffort I).LetA beaninstanceofthemeta-algorithmcharacterizedinTheorem4.1.
t ∈ ∈ Meta
Thenforany s[,τ ] 1,T ,thereisasetofdisjointintervalsI ,...,I inS suchthat K I s,τ ,andmoreover,for
anyu u ,...,u
Algorithm2withy∉
Ref y guarantees
1 K ∪i=1 i
s τ⊆ t t =
[ ] [ ] [ ]
= =
( ) R u O dmax ỹ yRef 2log2 T b dPγ∗ u y y 2
s,τ ⎛ t t − t + √ s,τ t∈∑ s,τ t − t ⎞
[ ]( )≤ ̂ ( ) ( ) [ ]( ) ( ̃)
whereP [γ s∗
,τ
](u )=∑K i=1P Iγ ii∗ (u )and⎝ eachγ i∗
∈
[0,1 ]satisfiesγ i∗
=
√d
2
√
t∈Id 2 i∑ytt −∈I yi t(y 2t +−[ √y ̃t )P]2
Iγ ii∗ u
. ⎠
IfweinsteadsupposeeachA setshintsasinTheorem4.3,thenfora∑nyu( ũ),...,u (A)lgorithm2guarantees
γ,I s τ
=
R u O dPγmin u dmax y yRef 2log2 T b ( dPγ○ u) ℓ u
s,τ s,τ + t t − t + s,τ ∑ t t
⎛ √ t∈s,τ ⎞
[ ]( )≤ ̂ [ ]( ) ( ) ( ) [ ]( ) ( )
whereP [γ s○
,τ
](u )=∑K i=1P Iγ ii○ (u )⎝ andeachγ i○
∈
[0,1 ]satisfiesγ i○
=
√d t√
∈Id
iℓ∑
tt∈ uI tiℓ +t √(ut
P) Iγ ii○ u
. [ ] ⎠
∑ ( ) ( )
Proof. Forany s,τ 1,T ,Danielyetal.(2015,Lemma1.2)showsthatthereexistsadisjointsetofintervalsI ,...,I
1 K
inS suchthat K I s,τ andK O log τ s . Hence,wecandecompose K R u ,soapplyingTheorem4.1
toeachofthese∪[i s= u1 b-]i in⊆ te[rvals],foranyγ S we− have: ∑i=1 Ii
= ≤ γ
[ ] ( ( )) ( )
R
s,τ
u ∑K R Ii∈ u ∑K O R IA iγ,Ii u +m tax y
t
−y
t
2log N I
i
i=1 i=1
[ ]( )= ( )≤ ̂K( ( ) ( ̃) ( ∣ ∣))
O ∑R IA iγ,Ii u +Km tax y
t
−y
t
2log N τ −s
i=1
≤ ̂(K ( ) ( ̃) ( ( )))
O ∑R IA iγ,Ii u +m tax y
t
−y
t
2log2 T , (15)
i=1
≤ ̂( ( ) ( ̃) ( ))
whereO hideslog log factorsandthelastlineboundsK O log τ s O log T andN O Tlog T . The
boundon⋅ N canbeseenfromthefactthat S O log T ,andfrom− thefactthatS isconstructedasS log T S
w ath mer oe stŜ Ti( =)
2 {i
[ik n2 tei r,
v
(ak ls(
+
,h1
e
)) n2 ci
e−
s1
u
]m∶
mk i=ng0, th1 e, m. ∣..
a
}γ l, l∣f u≤r pom
y(
iew ldh si (ch S)i )t≤ is ∑e ia( =los 1gily T( see Sn it) h) at≤
O
∣S
T
∣( ≤.O( (T) )) byobser≤ ving( tha =te∪( ai ⌊c= h1 )) (S i)h⌋asi
⌊ ( )⌋
Now fora/ny intervalI , Lemma D.4showsthat thereis a∣ γ∣∗= 0,1 sa∣tisf∣y≤ ing(γ∗) d t∈Ii 21 yt−yt 2 anda
i i i √
γ S suchthat
∈
[ ]
=
√d t∈Ii∑ 21 yt−yt(2+ √̃)P Iγ ii∗ u
γ ∑ ( ̃) ( )
∈
R IA iγ,Ii u O ⎛dm tax y
t
−y
t
2log I
i
+b √dP Iγ ii∗ u
t∑
∈Ii
y
t
−y
t
2
⎞
≤
( ) ( ̃) (∣ ∣) ( ) ( ̃)
sosummingtheseupandapplyingCauchy-Schwarzinequlityleadsto
⎝ ⎠
K K
i∑
=1R IA iγ,Ii u O ⎛Kdm tax y
t
−y
t
2log I
i
+
i∑
=1b √dP Iγ ii∗ u
t∑
∈Ii
y
t
−y
t
2
⎞
≤
( ) ( ̃) (∣ ∣) ( ) ( ̃)
O⎝ dmax y y 2log2 τ s b dPγ∗ u y y ⎠ 2
⎛ t t − t − + √ s,τ t∈∑ s,τ t − t ⎞
≤
( ̃) ( ) [ ]( ) ( ̃)
⎝ [ ] ⎠
34OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
wherewe’vedefinedPγ s∗
,τ
u ∑K i=1P Iγ ii∗ u . PluggingthisbackintoEquation(15),overallwemaybound:
=
[ ]( ) ( )
R u O dmax y yRef 2log2 T b dPγ∗ u y y 2
s,τ ⎛ t t − t + √ s,τ t∈∑ s,τ t − t ⎞
[ ]( )≤ ̂ ( ) ( ) [ ]( ) ( ̃)
wherewe’vechoseny yRefforsim⎝plicity. [ ] ⎠
t t
An identical argumenth=olds for the second statement: for any interval I , Lemma D.5 shows that there is a γ○ 0,1
̃ i i
satisfyingγ○ d t∈Iiℓt ut andaγ S suchthat ∈
i
=
√d t√ ∈Iiℓ∑t ut + √( P) Iγ ii○ u
∈
γ [ ]
∑ ( ) ( )
R IA iγ,Ii u O ⎛dP Iγ imin u +dm tax y
t
−y tRef 2log I
i
+b √dP Iγ ii○ u
t∑
∈Iiℓ
t
u
t
⎞
≤
( ) ( ) ( ) (∣ ∣) ( ) ( )
sosummingtheseupandapplying⎝Cauchy-Schwarzinequalityagainleadsto ⎠
K K
i∑
=1R IA iγ,Ii u O ⎛dPγ sm ,τin u +Kdm tax y
t
−y
t
2log I
i
+
i∑
=1b √dP Iγ i○ u
t∑
∈Iiℓ
t
u
t
⎞
≤
( ) [ ]( ) ( ̃) (∣ ∣) ( ) ( )
O⎝ dPγmin u dmax y y 2log2 τ s b dPγ○ u ℓ u⎠
s,τ + t t − t − + s,τ ∑ t t
⎛ √ t∈s,τ ⎞
≤
[ ]( ) ( ̃) ( ) [ ]( ) ( )
wherewe’vedefinedPγ s∗
,τ
u ⎝ ∑K i=1P Iγ ii∗ u ,sopluggingthisbackintoEquation(15),overal[ lwe] mayb⎠ ound:
=
[ ]( ) ( )
R u O dPγmin u dmax y yRef 2log2 T b dPγ○ u ℓ u ,
s,τ s,τ + t t − t + s,τ ∑ t t
⎛ √ t∈s,τ ⎞
[ ]( )≤ ̂ [ ]( ) ( ) ( ) [ ]( ) ( )
wherewe’vedefinedPγ s○
,τ
∑⎝ K i=1P Iγ ii○ u . [ ] ⎠
=
[ ] ( )
F.1.MatchingtheExp-concaveGuaranteeinUnboundedDomains
RecallfromSection3.2thatintheExp-concavesetting,thealgorithmofBaby&Wang(2021)achievesadynamicregret
boundoftheformR T u O T1 3C T2 3 forC T ∑T t=− 11 u t −u t−1 1. Ourstrongly-adaptiveguaranteesinTheoremF.1
showthataboundofthisformcan/beac/hievedevenintheunboundeddomainsetting. Toseewhy,notethattheessential
intuition of Baby&W(an)g≤ (2̃ 0(21) is that)if we ha= ve acces∥s to a stro∥ngly-adaptive algorithm guaranteeing R u
a,b
O log b a static regreton all intervals a,b 1,T , then to attain the desired boundup to log termsit sufficesto
show tha− t there exists a set of intervals I ,...,I partitioning 1,T such that N T1 3C2 3 and that the[ dy]( nam) i≤ c
1 ⊆N T
( ( )) [ ] [ ]
regretisboundedbythestaticregretsoverthepartition,leadingtoregretmatchingO T1 3C/2 3 /uptologarithmicterms.
{ } [ ] ≤ T
/ /
Our strongly-adaptive guarantee in Theorem F.1 actually achieves a stronger guarantee than is necessary to invoke the
( )
aboveargument,byguaranteeingO log b a dPγ u b a dynamicregretoneveryinterval a,b ,andhence
− ∨ a,b −
as a special case we have O log b a static r√egret on each interval as well. A similar partitioning argument then
providesananalogousT1 3C2 3
bou(− nd,( eveni) nunboun[ ded]( do) m∣ ains.∣) Ifthisissurprising,notethatthee[ xp-c]
oncave(and
T
( ( ))
hence boundeddomain) r/estric/tion is only really used to provide an algorithm which achieves logarithmic static regret,
nottoconstructtheessentialpartition. Intheonlinelinearregressionsetting,wedonotneedexp-concavitytoguarantee
logarithmicstaticregret—theVAWforecastercanprovidethenecessaryguaranteeeveninanunboundeddomain.
35OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
G.Supporting Lemmas
ThefollowingprovidesausefulrelationbetweenthesquaredlossanditsBregmandivergence.
LemmaG.1. Letℓ w 1 y x ,w 2. Thenforanyu,w W,
t 2 t − t t
= ∈
( ) ( ⟨ ⟩) 1
2
D uw x ,u w
ℓt
2
t
−
=
( ∣ ) ⟨ ⟩
Proof. BydefinitionofBregmandivergence,wehave:
D uw ℓ u ℓ w ℓ w ,u w .
ℓt t
−
t
− ∇
t
−
=
Expandingthedefinitionofℓ ,wehave ( ∣ ) ( ) ( ) ⟨ ( ) ⟩
t
1 1
ℓ u ℓ w y x ,u 2 y x ,w 2
t t t t t t
− 2 − − 2 −
= 1 1 1 1
( ) ( ) (y2 ⟨ x ,u⟩)2 y( x ,u⟨ ⟩y)2 x ,w 2 y x ,w
2 t + 2 t − t t − 2 t − 2 t + t t
= 1 1
x ,u 2⟨ ⟩x ,w ⟨2 y ⟩x ,w u .⟨ ⟩ ⟨ ⟩
t t t t
2 − 2 + −
=
⟨ ⟩ ⟨ ⟩ ⟨ ⟩
Moreover,wehave
ℓ w ,u w y x ,w x ,u w
t t t t
− ∇ − − −
⟨ ( ) ⟩= ⟨−(y t x t⟨,w −u ⟩)+ x t,w ⟩2 − x t,w x t,u ,
=
socombiningwiththepreviousdisplaywehave ⟨ ⟩ ⟨ ⟩ ⟨ ⟩⟨ ⟩
1 1
ℓ u ℓ w ℓ w ,u w x ,u 2 x ,w 2 y x ,w u
t t t t t t t
− − ∇ − 2 − 2 + −
( ) ( ) ⟨ ( ) ⟩= ⟨ y t ⟩x t,w ⟨u ⟩x t,w ⟨2 x t,w⟩ x t,u
− − + −
1 1
2 2
x t,u⟨ x t⟩,w⟨ x⟩t,w⟨ x t,u⟩⟨ ⟩
2 + 2 −
= 1
⟨ x ,u⟩ x⟨,w 2⟩ ⟨ ⟩⟨ ⟩
t t
2 −
= 1
(x⟨ ,u ⟩w⟨2. ⟩)
t
2 −
=
⟨ ⟩
Thefollowingprovidesadiscountedversionofthelog-determinantlemma.
LemmaG.2. Letγ 0,1 ,λ 0,x t Rd, anddefineM 0 λI andM t x tx⊺ t γM t−1 foreacht 0. Thenforany
sequence∆ ,∆ ,...inR, +
1 2 ∈ > ∈ = = >
( ]
T T γT−t x 2
∆2 x 2 dlog 1 γ ∆2 max∆2dlog 1 ∑t=1 t 2
t∑
=1
t t M t−1 1∶T + t t + λd
≤ ⎛ ∥ ∥ ⎞
∥ ∥ ( / )
⎝ ⎠
Proof. BydefinitionwehaveM
t
x tx⊺
t
γM t−1,sore-arrangingandtakingthedeterminantofbothsideswehave
+
=
Det γM t−1 Det M t x
tx⊺
t Det M t Det I M
t− 21
x
tx⊺
tM
t− 21
− −
(
)=
Det(M t 1 −
x)t=2
M t−1( ) ( )
=
( )( ∥ ∥ )
36OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
where the last line uses the fact that Det I −yy⊺ 1
−
y 2 2. Re-arranging, using Det γM t−1 γdDet M t−1 , and
usingthefactthat1 x log x wehave
− − ( ) = ∥ ∥ ( ) = ( )
≤
T ∆( 2 ) x 2 T ∆2 1 γdDet M t−1
t∑ =1 t t M t−1 t∑ =1 t − Det M t
= ( )
∥ ∥ T [ Det M ]
∆2log ( t)
t∑ =1 t γdDet M t−1
≤ ( )
T ( T ) Det M
∆2dlog 1 γ ( ∆)2log t
t∑ =1 t + t∑ =1 t Det M t−1
= ( )
( / ) ( T Det M)
dlog 1 γ ∆2 max∆2log ( ) t
1∶T + t t ∏ t=1Det M t−1
≤ ( )
( / ) (Det M )
dlog 1 γ ∆2 max∆2log (T . )
1∶T + t t Det M 0
= ( )
( / ) ( )
ObservethatDet M Det λI λd,andusingAM-GMinequalitywehave ( )
0
( )= ( Det)=
M
Tr M
t
d Tr λγTI +∑T t=1γT−tx tx⊺
t
d
T
d d
≤ ( ) =⎛ ( )⎞
( ) ( dλγT +∑) T t=1⎝γT−t x
t
2
2
d
,
⎠
d
=⎛ ∥ ∥ ⎞
Hence Det MT dλγT+ T t=1γT−t xt 2 2 d ,s⎝ ooverallwehave ⎠
Det M0 dλ
( ) ∑ ∥ ∥
≤
( ) ( )
T dλγT T γT−t x 2 d
t∑
=1∆2
t
x
t
2
M t−1
≤dlog 1 γ ∆2
1∶T
+m tax∆2 tlog
⎛⎛
+∑t λ=1
d
∥
t ∥2
⎞ ⎞
∥ ∥ ( / ) ⎜ ⎟
dlog 1 γ ∆2
1∶T
+m tax∆2 tdlog⎝⎝dλγT +∑T t λ=1 dγT−t x t 2 2⎠ ⎠
= ⎛ ∥ ∥ ⎞
( / ) T γT−t x 2
dlog 1 γ ∆2 max∆2dlog⎝ 1 ∑t=1 t 2 ⎠
1∶T + t t + λd
≤ ⎛ ∥ ∥ ⎞
( / )
⎝ ⎠
NotethattheLemmaG.2alsoimmediatelygivesustheusuallogdeterminantlemmaasaspecialcasewhereγ 1:
LemmaG.3. Letλ 0,x t Rd,anddefineLetM 0 λI andM t x tx⊺ t M t−1 foreacht 0. Thenforany=sequence
∆ ,∆ ,...inR, +
1 2 > ∈ = = >
T T x 2
∆2 x 2 dmax∆2log 1 ∑t=1 t 2
t∑
=1
t t M t−1 t t + λd
≤ ⎛ ∥ ∥ ⎞
∥ ∥
⎝ ⎠
Thefollowinglemmaiscommoninadaptiveonlinelearningandprovidedforcompleteness.
LemmaG.4. Leta ,...,a bearbitrarynon-negativenumbersinR. Then
1 T
T T a T
a t 2 a
¿ Át∑ =1 t t∑ =1 ∑t s=1a s ¿ Át∑ =1 t
ÁÀ ≤ ≤ ÁÀ
√
37OnlineLinearRegressioninDynamicEnvironmentsviaDiscounting
Proof. Byconcavityofx↦ x,wehave
√ a
a 1∶t a 1∶t−1 t ,
− 2 a 1∶t
√ √ ≥
√
sosummingovertandobservingtheresultingtelescopingsumyields
T a T
∑ t 2 ∑ a 1∶t a 1∶t−1 2 a 1∶T.
t=1 a 1∶t t=1 −
≤ √ √ = √
√
Forthelowerbound,observethat
∑T a t ∑T a t a 1∶T a 1∶T
t=1 a 1∶t t=1 a 1∶T a 1∶T
≥ = =√
√ √ √
38