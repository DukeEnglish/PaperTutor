Alt4Blind: A User Interface to Simplify Charts
Alt-Text Creation
Omar Moured1,2⋆, Shahid Ali Farooqui3⋆, Karin Müller2, Sharifeh
Fadaeijouybari4, Thorsten Schwarz2, Mohammed Javed3, and Rainer
Stiefelhagen1,2
1 CV:HCI@KIT, Karlsruhe Institute of Technology, Germany
https://cvhci.anthropomatik.kit.edu/
2 ACCESS@KIT, Karlsruhe Institute of Technology, Germany
https://www.access.kit.edu/
3 Computer Vision & Biometrics Lab, Indian Institute of Information Technology
Allahabad, India
https://cvbl.iiita.ac.in/
4 University of Trier, Germany
Abstract. Alternative Texts (Alt-Text) for chart images are essential
for making graphics accessible to people with blindness and visual im-
pairments. Traditionally, Alt-Text is manually written by authors but
often encounters issues such as oversimplification or complication. Re-
cent trends have seen the use of AI for Alt-Text generation. However,
existing models are susceptible to producing inaccurate or misleading
information. We address this challenge by retrieving high-quality alt-
textsfromsimilarchartimages,servingasareferencefortheuserwhen
creating alt-texts. Our three contributions are as follows: (1) we intro-
duce a new benchmark comprising 5,000 real images with semantically
labeled high-quality Alt-Texts, collected from Human Computer Inter-
action venues. (2) We developed a deep learning-based model to rank
andretrievesimilarchartimagesthatsharethesamevisualandtextual
semantics.(3)Wedesignedauserinterface(UI)tofacilitatethealt-text
creationprocess.Ourpreliminaryinterviewsandinvestigationshighlight
the usability of our UI. For the dataset and further details, please refer
to our project page: https://moured.github.io/alt4blind/.
Keywords: Alt-Text · Image Retrieval · CLIP Model.
1 Introduction
Makingchartsaccessibletoabroaderaudienceisessentialtoensureequalaccess.
Part of this audience includes people with blindness and visual impairments
individuals who access data through different modalities: tactile and auditory.
Thechoicebetweenthesemodalitiesoftendependsontheavailabletechnologies
and the preferences of people with blindness and visual impairments. Although
⋆ Both authors contributed equally to this work.
4202
yaM
92
]VC.sc[
1v11191.5042:viXra2 O. Moured and S.A. Farooqi
technologieslikesonificationandtactilematerialsareavailable,Alt-Text remains
the primary method for BVI individuals to acquire information about images
throughscreenreaders[4].InaccordancebytheWCAG5,analttextisrequired
for a graphic to be accessible. These summaries are embedded as hidden tags
within documents or web pages. A study [2] revealed that over 80% of web
pages often neglect to include alt text. Furthermore, even when available, it
oftendoesn’tcomplywithrecommendedguidelines(e.g.WCAG)andstandards
(e.g. W3C).
Chart alt-text can originate from two primary sources: manually created
by humans or (semi-)automatically generated by AI systems. Human-generated
descriptions are often accurate but typically require expertise. Recent studies
havehighlightedasignificantAlt-textdeficiencyinpublications[1].Ontheother
hand, automatically generated captions are quicker to obtain, and require no
expertise but may suffer from information inaccuracy. Thus, crafting a high-
quality chart alt-text is not a trivial task [6].
Instead,thisworkinvestigatestheroleofusingAIinaidingbothexpertsand
non-expertuserstowritehigh-qualityalt-textbypresentinghigh-quality,similar
charts with alt-text as references for them. To achieve this, we built a compre-
hensive dataset composing 5,000 chart images sourced from venues providing
alt-text with their publications. We then trained an AI model to categorize im-
agesbasedontheirvisualandtextualsimilarities.Subsequently,themostsimilar
images are retrieved and presented to the user through our Alt4Blind interface
(Figure 1).
2 Related Work
In this section, we first examine existing datasets in the field of chart analysis,
followed by a discussion on the available models and tools for creating textual
descriptions of charts.
2.1 Datasets
Chart-2-Text [5] is a popular dataset in computer vision. It comprises a large
number of chart images of various types, with each image accompanied by sum-
mary and bounding box annotations. However, it is not tailored for people with
blindness and visual impairments, the summaries are not structured and do not
adhere to the accessibility guidelines. In contrast, Vistext [10] comprises 8822
synthesized images, each manually assigned alt-texts, representing a significant
effort. However, it features synthetic charts, restricted to line and bar charts,
and the alt-texts were not authored by the original image creators, potentially
omitting crucial context. Another dataset, HCI Alt-Text [1], collected chart fig-
uresfromHCIconferencesandaimstoanalysethequalityofprovidedalt-texts.
However,itconsistsof511chartimages,makingitlesssuitablefordeep-learning
models.
5 https://www.w3.org/WAI/alt/Alt4Blind 3
Fig.1. Alt4Blind UI: (1) Menu bar offering access to guidelines and a tutorial. (2)
Space for uploaded images featuring a function bar (zoom, move, fit). (3) Text field
for user input, accompanied by a button to update the retrieved image. (4) Retrieved
charts based on the uploaded image, can be further enhanced with text query.
2.2 Alt-Text Creation Models
Chart Analysis is an emerging field in computer vision, featuring recent works
such as ChartAssistant [8] and ChartLLama [3]. These models are trained for
various chart-related tasks, including description generation, chart2table, and
chart2code, among others. Despite being trained on a vast chart corpus, these
models fall short of addressing accessibility concerns.
2.3 Tools
K. Mack [7] analyzed various interface designs to facilitate the creation of high-
quality alt-texts. Their results concluded that interfaces assisting authors
in deciding what to include in the alt-text were well-received and
led to higher-quality descriptions. However, there are very few tools that
adopt this approach. WATAA [4] is one such tool, which presents users with
AI-generateddescriptionsasareference.Nevertheless,AIdescriptionsareprone
to hallucinations.
In this paper, we focus on addressing both limitations: (i) creating a database
with high-quality alt-texts, and (ii) developing an intelligent tool that aids people
with and without accessibility knowledge to author high-quality alt-text.4 O. Moured and S.A. Farooqi
3 Exploratory Interviews and Findings
WiththehelpoftheCenterforDigitalAccessibilityandAssistiveTechnologiesin
Karlsruhe Institute of Technologies 6, we recruited six participants with varying
levels of accessibility expertise, from none to advanced (working in the field
of accessibility). We conducted exploratory interviews with these participants
(experts (P1-P3), non-experts (P4-P6)) to identify necessary features for the
development of an intuitive user interface, the Alt4Blind app. Participants were
not compensated.
3.1 Procedure
The interviews took place in a one-hour face-to-face session following the steps:
1. Initially, we introduced a basic line chart featuring three distinct lines, see
the illustration in the upper left of Figure 1-2.
2. Participants were then instructed to write a description for a blind person.
3. Following this, we inquired about the challenges faced and their perspective
on essential features for a web tool designed for this task.
No compensation was offered for participation in this study. Furthermore,
the user study received approval from the KIT Internal Ethical Committee.
3.2 Qualitative Results
During the interview, we took notes that we analyzed after the sessions. The
analysisrevealedthefollowingobservations:P1-P3,maintainedacoherentread-
ing order, starting with visual semantics (e.g., chart type, title, and axis labels)
followed by contextual information (e.g., line trends, and comparisons). In re-
sponse to the follow-up questions, P1 noted that they usually find Multivariate
and Panel charts challenging. P4-P6 discussed how they were unfamiliar with
alt-textsasit’softenhiddenintags,andsuggestedtheneedforaninitialguide-
line before start writing. Based on these insights, we identified as a feature that
an initial suggestion for user-uploaded images is necessary. Thus, we chose to
(1) display a real initial suggestion, i.e. displaying real chart images with
human-authored high-quality alt-texts. (2) the images should be visually
or contextually relevant to the user chart.
6 www.access.kit.eduAlt4Blind 5
Fig.2.Ourretrievalsystemleveragesboththetextandimageencodermodulesofthe
fine-tuned CLIP model. This ensures similarity at both visual and contextual levels.
4 Alt4Blind
In this section, we describe (i) our dataset, (ii) the AI-based model for ranking
andretrievingsimilarchartswithalt-texts,and(iii)thecreationoftheAlt4Blind
User Interface.
4.1 Creation of our Dataset
Wefirstcrawled25,000imageswithalt-textfrompubliclyavailableHCI-related
conferencesspanningthelast10yearstoexpandontheHCIAlt-Textdataset[1].
In the next step, we filtered out images that are not charts, have a short alt-
text or lack semantics, so we end up with 5,000 chart images, marking a tenfold
increase from the previous dataset.
Our dataset contains a wide variety of chart types, from familiar forms like
Line,Bar,Area,Pie,andScatterchartstomoreuniquevisualizationsnotpresent
in previous works such as multi-variate and panel charts. We used the 4-level
semantic model by Lundgard et al. [6] to rate alt-text quality. Initial scoring
was conducted using ChatGPT-4 by assessing the number of semantics and
levels present in each alt-text. The richest samples were subsequently manually
reviewed.
4.2 Chart Image Retrieval
Image retrieval involves searching for images similar to a given query image or
text,withafocusonensuringthetopresultsincludesimilarchartsemantics.For
this purpose, we employed CLIP ViT-B/32 [9] as our baseline model. The CLIP
architecture encompasses both a text and an image encoder, each producing6 O. Moured and S.A. Farooqi
a 512-dimensional feature vector. The CLIP model uses the unsupervised con-
trastive pre-training approach to cluster the samples in the latent space. Hence,
we fine-tuned the model on our dataset, with a batch size of 16 for 10 epochs.
Next, during the inference phase, Figure 2, we input the image, extract the en-
coders’representationsandcompareittothesamplesfromthedatasets.Thetop
threecandidateswiththehighestcosinesimilarityscoresarethenretrieved.Our
model achieved 92% in P@3 and 85% in R@3, demonstrating high capabilities
in Precision (P@3) and Recall (R@3) for displaying similar chart images within
the top three results, respectively.
4.3 User Interface
For our prototype, we utilized React JS7, a JavaScript library ideal for building
userinterfaces.Itincludesalandingpageforuserstodrag-and-dropchartimages
and a tutorial for first-time use. Once an image is uploaded, the backend model
display the top three similar candidates on the main page (see Figure 1). Users
can enlarge to view the full alt-text. The "Get Similar" button allows users to
refresh the candidates when they type their summary in the text field.
5 Preliminary Evaluation
We conducted tests with the same participants, P1-P6, to assess our user in-
terface. Participants were presented with Panel and Multivariate charts, which
were unfamiliar to all. The samples are depicted in Figure 1, and participants
were instructed to:
1. Review the guidelines and participate in the tutorial chart session first.
2. Use Alt4Blind to upload the chart and create alternative text for the image
provided.
3. Describe their experience in detail.
During the session, we tracked mouse movements and recorded user interac-
tion behaviors using pen and paper. All participants produced detailed descrip-
tions. P4-P6 especially benefited from the feature that allows copying sentences
from various similar charts to craft their descriptions. Expert users appreciated
this feature and recommended adding chart captions to further complement the
alternativetext.P1suggestedincreasingaccesstomorethanthreesimilarcharts,
whereas P3 proposed a feature to replace irrelevant images among the selected
charts.
7 https://react.dev/Alt4Blind 7
6 Discussion and Limitations
In this section, we shortly discuss our results and the limitations of this work.
Intelligent Features: Our tool has demonstrated efficiency in enabling both
inexperienced and experienced users to author high-quality alt-texts. However,
it could further benefit from additional intelligent functionalities, such as LLM-
based feedback and descriptions, which are currently under development.
Captions: While the current implementation assists users in creating alt-text,
we believe that integrating captions into the tool is essential, as captions and
alt-texts often complement each other
Interactions: Current UI does not allow users to have control over the similar
chartsection.Futureiterationsshouldofferuserscontrol,allowingthemtoomit,
replace, or view more charts.
User Study: Our initial investigations, though encouraging, were conducted
with a limited number of users and lacked comprehensive control measures. Fu-
ture studies should engage a larger and more diverse group of participants and
should involve people with blindness and visual impairments.
7 Conclusion
We presented an online, open-source tool to enhance alt-text writing by using
AI image retrieval, making it more engaging. Our shared dataset invites fur-
ther development by the NLP and Computer Vision communities to advance
chart summarization tasks with a focus on people with blindness and visual im-
pairments. Currently, while our work primarily provides examples of alternative
texts, it lays the groundwork for future research incorporating Vision-Language
models to address accessibility concerns.
Acknowledgments. TheauthorswouldliketoacknowledgethehelpofP.Venkatesh
forhissupportindevelopingtheUI.WewouldalsoliketothanktheHoreKacomputing
cluster at KIT for the computing resources used to conduct this research.
Disclosure of Interests. This research was funded by the European Union’s Hori-
zon 2020 research and innovation program under the Marie Sklodowska-Curie grant
agreements no. 861166.8 O. Moured and S.A. Farooqi
References
1. Chintalapati, S.S., Bragg, J., Wang, L.L.: A dataset of alt texts from hci publi-
cations: Analyses and uses towards producing more descriptive alt texts of data
visualizations in scientific papers. In: Proceedings of the 24th International ACM
SIGACCESS Conference on Computers and Accessibility. pp. 1–12 (2022)
2. Commission, D.R.: The web: Access and inclusion for disabled people; a formal
investigation. The Stationery Office (2004)
3. Han, Y., Zhang, C., Chen, X., Yang, X., Wang, Z., Yu, G., Fu, B., Zhang, H.:
Chartllama: A multimodal llm for chart understanding and generation (2023)
4. Jeong, H., Chun, M., Lee, H., Oh, S.Y., Jung, H.: Wataa: Web alternative text
authoring assistant for improving web content accessibility. In: Companion Pro-
ceedings of the 28th International Conference on Intelligent User Interfaces. pp.
41–45 (2023)
5. Kantharaj, S., Leong, R.T.K., Lin, X., Masry, A., Thakkar, M., Hoque, E., Joty,
S.:Chart-to-text:Alarge-scalebenchmarkforchartsummarization.arXivpreprint
arXiv:2203.06486 (2022)
6. Lundgard,A.,Satyanarayan,A.:Accessiblevisualizationvianaturallanguagede-
scriptions:Afour-levelmodelofsemanticcontent.IEEEtransactionsonvisualiza-
tion and computer graphics 28(1), 1073–1083 (2021)
7. Mack, K., Cutrell, E., Lee, B., Morris, M.R.: Designing tools for high-quality alt
textauthoring.In:Proceedingsofthe23rdInternationalACMSIGACCESSCon-
ference on Computers and Accessibility. pp. 1–14 (2021)
8. Meng,F.,Shao,W.,Lu,Q.,Gao,P.,Zhang,K.,Qiao,Y.,Luo,P.:Chartassisstant:
A universal chart multimodal language model via chart-to-table pre-training and
multitask instruction tuning (2024)
9. Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agarwal,S.,Sastry,G.,
Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from
naturallanguagesupervision.In:Internationalconferenceonmachinelearning.pp.
8748–8763. PMLR (2021)
10. Tang,B.J.,Boggust,A.,Satyanarayan,A.:Vistext:Abenchmarkforsemantically
rich chart captioning. arXiv preprint arXiv:2307.05356 (2023)