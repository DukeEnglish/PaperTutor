Inverse Concave-Utility Reinforcement Learning
is Inverse Game Theory
MustafaMertÇelikok Jan-WillemvandeMeent
DelftUniversityofTechnology UniversityofAmsterdam
TheNetherlands TheNetherlands
m.m.celikok@tudelft.nl j.w.vandemeent@uva.nl
FransA.Oliehoek
DelftUniversityofTechnology
TheNetherlands
f.a.oliehoek@tudelft.nl
Abstract
Weconsiderinversereinforcementlearningproblemswithconcaveutilities. Con-
caveUtilityReinforcementLearning(CURL)isageneralisationofthestandard
RLobjective,whichemploysaconcavefunctionofthestateoccupancymeasure,
ratherthanalinearfunction. CURLhasgarneredrecentattentionforitsability
torepresentinstancesofmanyimportantapplicationsincludingthestandardRL
suchasimitationlearning,pureexploration,constrainedMDPs,offlineRL,human-
regularizedRL,andothers. Inversereinforcementlearningisapowerfulparadigm
thatfocusesonrecoveringanunknownrewardfunctionthatcanrationalizethe
observed behaviour of an agent. There has been recent theoretical advances in
inverseRLwheretheproblemisformulatedasidentifyingthesetoffeasiblere-
wardfunctions. However,inverseRLforCURLproblemshasnotbeenconsidered
previously. In this paper we show that most of the standard IRL results do not
applytoCURLingeneral,sinceCURLinvalidatestheclassicalBellmanequations.
ThiscallsforanewtheoreticalframeworkfortheinverseCURLproblem. Using
arecentequivalenceresultbetweenCURLandMean-fieldGames,weproposea
newdefinitionforthefeasiblerewardsforI-CURLbyprovingthatthisproblemis
equivalenttoaninversegametheoryprobleminasubclassofmean-fieldgames.
WepresentinitialqueryandsamplecomplexityresultsfortheI-CURLproblemun-
derassumptionssuchasLipschitz-continuity. Finally,weoutlinefuturedirections
andapplicationsinhuman–AIcollaborationenabledbyourresults.
1 Introduction
In this work, we present the first theoretical results and formalization of inverse concave-utility
reinforcementlearning(I-CURL),whichistheproblemofrationalisinganoptimalCURLpolicyby
inferringitsrewardfunction. Instandardreinforcementlearning,theobjectiveisonewayoranother
anexpectationofreturn. Itisanage-oldresultthatthisobjectivecanbeexpressedasaninner-product
between the reward function R and the state-action occupancy measure dπ induced by a policy.
However,inrecentyears,differentapplicationsandproblemsrelatedtoRLwithobjectivesthatbreak
thelinearityhaveemerged. Asaresultofthis,concaveutilityreinforcementlearning(CURL),also
called convex RL, has garnered significant recent interest due to its ability to encompass a large
varietyofproblemsrelatedtoreinforcementlearning(seetable1foranon-exhaustivelist)[1–7].
However,thequestionofinverseRLforCURLproblemshasnotbeenaddressedbefore.
Preprint.Underreview.
4202
yaM
92
]GL.sc[
1v42091.5042:viXraTable1: ProblemsrelatedtoRLandtheircorrespondingconcaveutilityfunctionsF.
Application TheObjectiveF
StandardRL ⟨dπ,R⟩
PureExploration[3] dπlogdπ
ConstrainedMDPs[8–11] ⟨dπ,R⟩ s.t. ⟨dπ,C⟩≤c
OfflineRL[2] ⟨dπ,R⟩−λKL(dπ,ddata)
ImitationLearning[12–14] ||dπ−dE||2, KL(dπ,dE)
2
Risk-sensitiveRL[15–17] CVaR (⟨dπ,R⟩), ⟨dπ,R⟩−Var(⟨dπ,R⟩)
α
Cautious/Human-regularisedRL[18–20] ⟨dπ,R⟩−λKL(dπ,dρ),dρisaprior.
Figure1: Illustrativeexampleofaninformation-limitedMDP.Acaptainmustnavigatetooneofthe
ports(a,b,c). Thesharksdenotedangerouswaterswithhighlynegativereward. Themostpreferred
portisaandtheleastpreferredisc. Theenvironmentisnotstochastic,butacognitivelyconstrained
captaincanmakemistakes. ThecaptainchoosestogotoportB,knowingtheycouldmakemistakes
whenfollowingthegoldenpathtoa. Thegreen-shadedareashowsthepathscaptainfollowtob.
Theymakeaspecialefforttoavoidc,butotherwisefollowanoisypath.
Inversereinforcementlearning(IRL,[21])methodsinfertherewardfunctionanagentisoptimising
for,undertheassumptionthattheagentisfollowingthestandardRLobjectiveofmaximisingthe
expectedreturn. Considerobservinganagent(AIorhuman)performingataskintheenvironment.
Wedonotknowwhatistheirgoal,howeverwewouldliketolearnhowtoperformthistaskourselves.
We can try to clone the agent’s behaviour directly, but a stronger objective is to rationalise the
behaviour of the agent by inferring their reward function. This is due to the fact that the reward
functionisamuchmoresuccinctandtransferabledefinitionofthetask.
Unfortunately,therearedifficultiesofapplyingIRLtoinferrewardfunctionsfromhumandemonstra-
tions,duetoboundedrationality[22]. Humanshavecognitiveconstraintsthatleadtodeviationsfrom
perfectlyrationalbehaviour. Thisisoftenmodelledunderacomputationalrationalityframework,
whereahuman’sbehaviourisoptimalwithrespecttotheircognitiveconstraintsatthetime[23,24].
AnestablishedresultinIRLpresentsano-free-lunchtheoremforIRLwithhumans,wheretryingto
inferafeasiblerewardfunctionwithouttakingintoaccountthehumans’boundedrationalityisin
generalnotpossible[25]. However,variousformsofboundedrationalbehaviourcanberepresented
assolutionstoCURLproblems,whichisanimportantmotivationforinverseCURL.
Imaginethatwearetryingtoinfertherewardfunctionofahumanwhohasinformation-processing
bounds,whichisaformofboundedrationalityduetocognitiveconstraints[26–29]. Thissettingis
oftenmodelledasinformation-limiteddecision-makingthatisequivalenttothecautious/human-
2regularisedRLformulationintable1[30–32]. Here,dρrepresentsaprior,lowmental-effortpolicy
forthehuman,andtheypayacostfordeviatingfromitproportionaltotheKLdivergence. Thefigure
1presentsanillustrativeexample,whereacaptainmustnavigatetoaport1.Thecaptain’struereward
functionissuchthateachtime-stephasasmallcost,crossingintodangerouswaters(representedas
sharks)haveahighcost,andtherewardsattheportsinduceapreferenceorderingsothattheport
Aispreferredthemost,andCtheleast. Theenvironmentisnotstochastic,thusactionsarealways
executedcorrectly. Here,theobviouslyoptimalpolicyforthecaptainistodeterministicallyfollow
thegoldenpath. However,beingunderaheavycognitiveburden,thecaptainknowstheyareproneto
makingerrors. Thusinstead,theyfollowamoreforgivingpathtoportB,whichleavesroomforerror.
Insimilarepisodes,theydomakemistakes,andweendupobservingadatasetofdemonstrations
withinthegreen-shadedregion.Thisisnotthesameashavingastochasticenvironment,sinceanother
captainmightsuccessfullyexecutethegoldenpathinthesameenvironment,andwealreadyknow
thedynamicsaredeterministic. Here,eveninthelimitofinfinitedemonstrations,wecannotinferthe
truerewardfunctionwithstandardIRL.Asweproveinlemma7,theproblemismoresevere. We
maybeunabletoinferanyfeasiblestate-actionrewardfunctionfromtheoptimalpolicyofaCURL
problembyapplyingthestandardIRLformulation.
Inthiswork,weaddressthequestionofhowtoformulatetheproblemofrationalisingthebehaviour
ofanagentwhosepolicyissolvingaCURLproblem. Insection2.1,westartbypresentingtheformal
definitionofCURLandanimportantresultfromliteraturethatshowshowitcannotbereducedtoa
standardRLproblemingeneral. Theninsection2.2,weexplainhoweveryCURLproblemisproven
tobeequivalenttoamean-fieldgame(MFG).Acorecontributionofourworkistakingadvantageof
thisresulttoprovethatIRLinCURLproblemsisinfactequivalenttoinversegametheory(IGT)in
mean-fieldgames. Inordertomakethisconnectionclear,wegiveabriefbackgroundofthestandard
IRLandinversegametheoryinsections2.3and2.4.
Insection3,weprovehowthefeasiblerewardsetdefinitionofstandardIRLfailsininverseCURL(I-
CURL).ThenusingtheequivalencetoMFGs,weproposeanewset,andprovethatitissufficientand
necessary,containingallrewardsthatrationalisetheoptimalCURLpolicy. Weshowanequivalent
characterizationofthissetwherecomputingafeasiblerewardfunctionissolvingaconstant-sum
gameitself. Therestofthissectionanalysestwodifferentcases: (1)GiventheexpertpolicyπE,
underwhatassumptionscanwecomputeafeasiblerewardfunctionefficiently,and(2)Givenan
empiricalestimateoftheexpertpolicyπˆE fromadataset,whatistherelationshipbetweenthesets
offeasiblerewardsforπˆE andπE. Finally,wederiveasamplecomplexityresultforthecase(2).
Thesection3.3examinessomeoftheassumptionswemadeforourtheoreticalresults,andshows
that in practice they hold for many CURL problems. Since the inverse CURL problem has not
beenaddressedbefore,ourworkfillsanimportanttheoreticalgap,demonstratingthelimitationsof
standardIRLmethodsforI-CURL.Ourformulationenablestheinferenceofrewardfunctionsfrom
boundedrationalbehavioursuchasinformation-boundednessandrisk-aversion.
2 Background
2.1 ConcaveUtilityReinforcementLearning
Definition1(MarkovDecisionProcess) AMarkovdecisionprocess(MDP)isdefinedbythetuple
M =(S,A,T,R,µ )wheretheSandAarefinitesetsofstatesandactions,theT :S×A→∆(S)
0
isthetransitionkernelthatmapsstate-actionpairstodistributionsoverstates,R:S×A→[0,1]is
therewardfunction,andµ ∈∆(S)istheinitialstatedistribution.
0
When paired with a memoryless policy π : S → ∆(A) that maps states to distributions over
actions, the MDP M induces a probability measure Pπ(τ) over infinite-length trajectories τ =
(S ,A ,....)wherePπ(S = s) = µ (s),Pπ(A = a|S ,A ,...,S ) = π(a|S ),andPπ(S =
0 0 0 0 t 0 0 t t t+1
s|S ,A )=T(s|S ,A ).AnMDPisusuallycoupledwithanoptimalitycriterionOPT,andunless
t t t t
stated otherwise, we assume the infinite-horizon expected sum of discounted rewards criterion,
OPT∞(π)≜E [(cid:80)∞ γtR(s ,a )].Essentially,anoptimalitycriterioninducesapartialordering
Pπ t=0 t t
amongstpoliciesandprovidesuswiththecriterionweusefordecidinghowgoodapolicyis. The
optimalpolicyisthendefinedasπ∗ ∈argmax OPT∞(π).
π
1Theshark,ship,andporticonscreatedbyGilberto,hartadidesign,andDarwinMulyafromtheNounProject.
3Definition2(DiscountedState-ActionOccupancyMeasure) WhenconnectedtoanMDPM,ev-
ery policy π induces a discounted state-action occupancy measure defined as dπ(s,a) ≜ (1 −
γ)(cid:80)∞ γtPr(S =s,A =a|S ∼µ ,π).
t=0 t t 0 0
Inessencedπ(s,a)tellsusthejointprobabilityofvisitingstatesandtakingactiona,underpolicy
π.Fromhereon,wewillletdπ denotethemeasureinducedbythepolicyπinanMDPthatwillbe
clearfromthecontext. Thesetofallpossiblediscountedstate-actionoccupancymeasuresformsa
polytopethatisaconvexandcompactset([33])anditisdefinedasfollows.
Definition3(SetofDiscountedState-ActionOccupancyMeasures) For a given MDP M and
discount factor γ, the set of all possible discounted state-action occupancy measures is defined
asK ={dπ|dπ ≥0,(cid:80) dπ(s,a)=(1−γ)µ (s)+γ(cid:80) T(s|s′,a′)dπ(s′,a′) ∀s∈S}.
γ a∈A 0 s′,a′
AstandardresultinRListhattheOPT∞(π)canequivalentlybeexpressedasalinearfunctionof
dπ,⟨dπ,R⟩. Inthatcase,thestandardreinforcementlearningobjectiveoffindingtheoptimalpolicy
canbecastasthefollowinglinearprogram
max ⟨dπ,R⟩. (StandardRLObjective) (1)
dπ∈Kγ
Concaveutilityreinforcementlearning(CURL)replacesthelinearobjectivefunctionofequation1
withaconcavefunctionF :∆(S×A)→(−∞,K]ofthestate-actionoccupancymeasures. Then
thenewoptimizationproblembecomesthefollowingconvexprogram
max F(dπ). (CURLObjective) (2)
dπ∈Kγ
SolvingCURLproblemsisnotasstraightforwardasapplyingstandardRLmethodsdirectly,demon-
stratedbythefollowinglemmafromZahavyetal.[6].
Lemma4 There exists an MDP M with concave utility F such that there can be no stationary
rewardR∈RS×Awithargmax ⟨dπ,R⟩=argmax F(dπ).
dπ∈Kγ dπ∈Kγ
The lemma above shows that in general, CURL problems cannot be reduced into standard RL
problemsbyre-definingrewards.Inotherwords,wecannotbesureofanequivalentstationaryreward
function that only depends on (s,a). In addition, unlike in standard RL, a deterministic optimal
policymaynotexistforaCURLproblem[2,6,5].
2.2 CURLasMean-fieldGames
In a bid to develop methods that can solve CURL problems, two concurrent formulations have
emerged:CURLasmean-fieldgames[2]andCURLaszero-sumgames[6]. Asweshowinappendix
A.1,thesetwoformulationsarestronglyrelated. Here,wewillfocusonthemean-fieldgames.
AnyCURLproblemcanbeformulatedasaspecialcaseofamean-fieldgame(thatbelongstoa
sub-classofMFGs)[2]. Inthissub-class,anMFGisatupleG=(S,A,T,R,γ,µ )wheretheonly
0
differencefromMDPsisintherewardfunction2R:S×A×∆(S×A)→R.Theinterpretation
isthat,inalargepopulationofagents,R(s,a,d)representstherewardanagentreceivesatstates
byplayingactiona,whilethepopulationpolicyinducesthedistributiond. Forafixedmean-field
distributiond, wecanwritetheobjectiveofanagentintheMFGas⟨dπ,R(·,·,d)⟩,whichisthe
expectedsumofrewardstheagentwouldreceivebyplayingpolicyπwhiletherestofthepopulation
continuesinducingd.ThecoreassumptioninMFGsisthefactthatthechangesinoneagent’spolicy
has no influence on the mean-field distribution. Theorem 1 of Geist et al. [2] proves that every
CURLproblemisinfactsuchanMFGwiththerewardR(·,·,dπ)=∇F(dπ)∈RS×A3,whereany
solutiontotheCURLproblemisamean-fieldNashequilibrium(definition5)andviceversa. From
nowon,wewillrefertoMFGsinducedbyaCURLproblemasCURL-MFGs.
Definition5(Mean-fieldNashEquilibrium(MFNE)andExploitability) Define the exploitabil-
ityofpolicyπinaCURL-MFGasϕ(π) = max ⟨d,R(·,·,dπ)⟩−⟨dπ,R(·,·,dπ)⟩. AnMFNE(or
d
equivalentlytheoptimalpolicyfortheCURLproblem)isatuple(π∗,dπ∗)suchthatϕ(π∗)=0.
2WeoverloadtherewardnotationR.Unlessstatedotherwise,RwillbeofformR(s,a,d).
3Thedπandthus∇F(dπ)canbeflattenedintoavectoroflength|S×A|whennecessary.
42.3 InverseReinforcementLearning
LetM¯ =(S,A,T,µ )denoteanMDPwithoutreward. AnIRLproblemisformalizedbythetuple
0
B=(M¯,πE)whereπE isanexpertpolicythatbehavesoptimallyinM¯ withregardstoanunknown
rewardfunction. ThestandardIRLproblemhasemergedasrationalisingπE byinferringthereward
functionitisoptimalfor.However,thisproblemisill-conditionedsinceingeneraltherewardfunction
isnotunique. RecentworksinIRLhavethenre-definedtheproblemasawell-posedone[34–36]:
inferringthesetofallrewardfunctionscompatiblewithπE,denotedbyRB.Asdefinedbelow,the
setconditionrequiresnon-positiveadvantageeverywhere,sinceapositiveadvantagevaluewould
meanthereisatleastonestatewheredeviatingfromπE improvesreturn.
Definition6(FeasibleRewardsSetforIRL[34–36]) ThesetoffeasiblerewardsfortheIRLprob-
lemBisdefinedasRB ={R∈RS×A|AM π¯ E∪R ≤0},whereAM π¯ E∪R(s,a)=QM π¯ E∪R(s,a)−V πM E¯∪R(s)
istheadvantagefunctionofπE computedaccordingtoR.
Unfortunately in practice, we do not have access to πE, but have a dataset of state-action pairs
generatedbyπE 4. WhentheexpertpolicyisestimatedfromadatasetasπˆE,itinducesanempirical
IRLproblemdenotedasBˆ = (M¯,πˆE)withitsfeasiblerewardsetR definedaccordingly. One
Bˆ
ofthemainquestionsstudiedbythislineofresearchishowcloseR istothetruesetoffeasible
Bˆ
rewards RB. Under certain assumptions, it is possible to derive sample complexity bounds with
respecttothisgapwithinaPACframework[34–36].
2.4 InverseGameTheory
Inversegametheory([37])essentiallydealswiththeproblemofinferringrewardfunctionsofagents
from observed equilibrium behaviour in games. In other words, IGT asks the question: "Which
rewardfunctionswouldrationalizetheobservedbehaviourinagame?"EventhoughtheIGTasaterm
isrelativelyrecent,ithasalonghistoryineconomicsandoptimizationwhereitiscalleddifferent
namessuchasmodelestimation,inverseoptimization,andcomputationalrationalization. IRLand
IGTsharecommonhistoryandmethodologysuchasthemaximumentropymethods[38]. Since
weareinterestedinformulatingI-CURLproblemswithinIGT,wewillfocusonIGTinmean-field
games.
IGTforMean-fieldGames. TherearetwomainapproachestoIGTforMFGs: thepopulation-
level and the individual-level. The former focuses on learning a reward function of the form
R(dπ,π) whereas the latter focuses on learning the individual agent’s reward R(s,a,d) from a
dataset consisting of (s ,a ) ∼ d∗ where (π∗,d∗) is the MFNE. Therefore in this work, we will
i i
usetheindividual-levelformulationpresentedinChenetal.[39]asastartingpointfortheproblem
formulationofI-CURL.WeletB=(G¯,πE,dE)denoteanindividual-levelIGTproblemwhereG¯ is
anMFGwithoutrewardand(πE,dE)isanMFNEofG¯ withrespecttoanunknownrewardfunction.
3 InverseGameTheoryforCURL
BothinverseRLandinversegametheorysharethesameobjective: tofindarewardfunctionthat
would rationalize the observed behaviour of agents. Since the lemma 4 states there cannot be a
stationaryrewardfunctionR(s,a)ingeneralforCURLproblems,thestandardformulationofIRL
doesnotapplyanymore. Inparticular,thedefinitionofthefeasiblerewardssetRBisproblematic
fortheCURLsetting. ItassumesthatπE isoptimisingforthestandardRLobjectivewithstationary
rewardfunctionsthatonlydependon(s,a)andthesetconditionreliesontheadvantagefunction.
However,asweknowfromthelemma4,thereareoptimalCURLpoliciesthatcannotbeinduced
bysuchrewardfunctionsviathelinearobjective. LetC = (M¯,πE)denoteanI-CURLproblem
whereπE istheoptimalpolicyforaconcaveutilityfunctionF. Thefollowinglemmastatesthe
impossibilityofapplyingstandardIRLtoCingeneral.
Lemma7 ThereexistsanI-CURLproblemC=(M¯,πE)suchthattheRC =∅,whereRC isthe
feasiblerewardsetforastandardIRLproblem,asdefinedindefinition6.
4WeassumeT isknown,butitalsocanbeestimatedfromdata
5Proof. Let(M,F)beaCURLproblemwithexpertpolicyπE ∈ argmax F(dπ).Thelemma4
π
statesthattheremaybenorewardfunctionR ∈ RS×A suchthatπE ∈ argmax ⟨dπ,R⟩.Thisis
π
equivalenttosayingtheremaybenoRsuchthatAM¯∪R(s,a)≤0everywhere.
πE
3.1 AGame-theoreticFeasibleRewardSetforI-CURL
Thelemma7motivatestheneedforanewdefinitionforsetsoffeasiblerewardsinI-CURLproblems.
LetB = (G¯,πE,dE)denoteanindividual-levelIGTproblemwhereG¯ isaCURL-MFGwithout
reward and (πE,dE) is an MFNE in G¯ for some reward function R. We propose the following
definitionasthefeasiblesetofrewardfunctionsandprovethatitisnecessaryandsufficient.
Proposition8(FeasibleRewardSetforI-CURL) The set of feasible reward functions for the I-
CURLproblemformulatedastheindividual-levelIGTinCURL-MFGs,B=(G¯,πE,dE),isdefined
asRB = {R ∈ RS×A×Kγ|ϕ(πE;R) = 0}wheretheparameterizedexploitabilityisϕ(πE;R) ≜
max ⟨d,R(·,·,dE)⟩−⟨dE,R(·,·,dE)⟩.
d
Proof. ThefeasiblesetconditionsimplymeansthattherewardfunctionRmustmake(πE,dE)an
MFNE.Thisisasufficientandnecessarycondition,sincethetheorem1ofGeistetal.[2]provesthat
anysolutiontotheCURLproblemisanMFNEinitsCURL-MFG,andvice-versa.
AnimportantpointtonoticeisthefactthateventhoughS andAarefinite,theCURL-MFGreward
functionsarenottabularsincetheK isingeneralinfinite. Thismeansinpracticewemustchoosea
γ
functionclasswhensolvingI-CURLproblems. Throughoutthispaper,wewillmakethefollowing
assumption.
Assumption9(Convexparameterization) TheCURL-MFGrewardfunctionsR(·,·,d;θ)arepa-
rameterizedbyθ ∈ΘwhereΘisanon-empty,compactandconvexset. Thenthefeasiblerewardset
isexpressedmoresuccinctlyasRB ={θ ∈Θ|ϕ(πE;θ)=0}.
ForagivenθandπE,computingϕ(πE;θ)involvestwosteps: solvingastandardRLproblemand
performingpolicyevaluation. Specifically,computingthetermmax ⟨d,R(·,·,dE;θ)⟩isequivalent
d
tosolvingtheMDPMπE =M¯ ∪RπE wheretheRπE(s,a)=R(s,a,dE;θ). Thenthesecondterm
isequivalenttoevaluatingπE inMπE.
Proposition10 The set RB is equivalent to the set of θ that solves the saddle-point problem
min {max ⟨d,R(·,·,dE;θ)⟩−⟨dE,R(·,·,dE;θ)⟩}.
θ∈Θ d∈Kγ
Theproposition8canbeseenastheexplicitcharacterisationofthefeasiblerewardset,whereasthe
saddle-pointformulationofproposition10presentsanalternativethatisusefulfortheoreticalanalysis.
Thismeanswecanformulatetheproblemoffindingafeasiblerewardfunctionasamin-maxgame
itself with a parameterized utility function U(θ,d;dE) = ⟨d,R(·,·,dE;θ)⟩−⟨dE,R(·,·,dE;θ)⟩
wheredE isaparameter. Thisisin-linewithrecentconcurrentworkbyGoktasetal.[40]. Unlike
their case, we do not explicitly need to assume the maximisation is concave. We know that the
setK isaconvexpolytope,andthereforeforafixedθ,theproblemisconcaveind,sinceU isa
γ
linearfunctionofd. Thereforeweonlyneedthefollowingassumptionstoextendtheirresultsto
CURL-MFGs.
Assumption11 (Convex exploitability) The parameterized exploitability ϕ(πE;θ) of the expert
policyisconvexinθand(Lipschitz-smoothness)therewardfunctionsareℓ-Lipschitzsmooth.
Thismakesthesaddle-pointformulationofproposition10aconvex-concavegamewhereℓ-Lipschitz
smoothnessallowsforefficientsolutionsusinggradientdescent-ascent. Thereforetheadversarial
inversemulti-agentplanning(AIMP)algorithmofGoktasetal.[40]canbedirectlyapplied. Then
thecomplexityresultofAIMPextendstoourproblemasfollows.
Theorem12 Letθ beanϵ-feasiblerewardfunctionsuchthatϕ(πE;θ )≤ϵ,orinotherwords,the
ϵ ϵ
πE isanϵ-MFNEfortherewardfunctionR(·,·,d;θ ). Undertheassumption11, theAIMPcan
ϵ
computeaθ inO(1)iterations.
ϵ ϵ2
TheseresultshighlightthedifficultyofsolvingI-CURLproblemsexactly,eveniftheπE isknown.
Thismakeintuitivesense,sincetheK isinfiniteregardlessofSandA,andthefeasiblesetcondition
γ
6isanequilibriumrequirement. However,ifthesetofrewardfunctionscanbeconstrainedtosome
well-behavingfunctionclasses,wecanhaveaconvex-concavegamethatcanbesolvedbygradient
descent-ascentmethodsasstatedbythetheoremabove. Unfortunately,inreal-worldapplications,we
donothaveaccesstoπE,butonlyadatasetofdemonstrationsfromit. Thisinducestheempirical
I-CURLproblem,definedandanalysedasfollows.
3.2 EmpiricalI-CURL
As in the IRL setting, we now define the empirical version of the individual-level IGT problem
of a CURL-MFG B = (G¯,πE,dE). Denoted by Bˆ, the empirical problem emerges when the
expert policy πE, and thus the MFNE (πE,dE) is not known. Here, the assumption is that we
haveadatasetD = {(s ,a ),...,(s ,a )}ofN state-actionpairssampledfromtheequilibrium
1 1 N N
(πE,dE)where(s ,a )∼dE(s,a).ThegoalistoinferarewardfunctionoftheformR(s,a,d)that
i i
rationalisestheobservedbehaviour. Therearetwochallengeshere: (1)Theempiricalestimation
of (πE,dE) from data and (2) The computation of R . The dE is estimated with the empirical
Bˆ
distribution dˆE(s,a) = (cid:80)N i=0I[(si,ai)=(s,a)] if (s,a) in the dataset, and 1 otherwise. Then the
N SA
πˆE(a|s)= dˆE(s,a) wheredˆE(s)=(cid:80) dˆE(s,a).ThentheempiricalfeasiblesetR isdefinedas
dˆE(s) a Bˆ
R ={θ ∈Θ|ϕ(πˆE;θ)=0}. (3)
Bˆ
AnimportantobjecttostudyishowRBrelatestoR
Bˆ
intermsofthequalityoftheestimationπˆE.It
isclearthatπˆE →πE asthedatasetgrowssothatN →∞,andalsoself-evidentthatifπˆE =πE
thenR
Bˆ
=RB.Indeedthetheorem1ofChenetal.[39]provesthatintheasymptoticcasewecan
recoveratleastsomeelementsinRB.However,thisdoesnotsayanythingaboutthefinitesample
setting:evenifπˆE isclosetotheπE,thisdoesnotgiveanyguaranteesonthesimilarityofthereward
sets. Theproposition10suggeststhatanysuchresultwouldbeboundingthedifferencebetweenthe
solutionsetsoftwodifferentconstant-sumgameswithutilitiesU(θ,d;dE)andU(θ,d;dˆE).Withno
furtherassumptions,provinganysuchresultisdifficult.
Assumption13 The CURL-MFG reward function R(·,·,d) is L-Lipschitz continuous so that
||R(·,·,d′)−R(·,·,d)|| ≤L||d′−d|| .
∞ ∞
SincethetrueCURL-MFGrewardfunctionisassumedtobeLipschitz-continuous, herewewill
restrictRB andR
Bˆ
toL-Lipschitzfunctions. AnimportantrelaxationoftheNashequilibriumis
theϵ-Nashequilibrium,definedasanypolicyπthatachievesareturnthatisϵclosetothereturnof
aNashpolicy,sothatitsexploitabilitysatisfiestheinequalityϕ(π;θ)≤ϵ.Thefollowingtheorem
establishesthesymmetricrelationshipbetweenRBandR
Bˆ
intermsofϵ-equilibria.
Theorem14 Under the assumption 13, let ||dE −dˆE||
∞
≤ ϵ. Then for any R ∈ RB, πˆE is a
2Lϵ-equilibrium. Similarly,foranyRˆ ∈R ,theπE isa2Lϵ-equilibrium.
Bˆ
Proof. Let R ∈ RB and thus ϕ(πE;R) = 0. Then |ϕ(πˆE;R) − ϕ(πE;R)| can be written as
|max ⟨d,R(·,·,dˆE)⟩−max ⟨d,R(·,·,dE)⟩−⟨dˆE,R(·,·,dˆE)⟩+⟨dE,R(·,·,dE)⟩|.Under
d∈Kγ d∈Kγ
the assumption 13, we have that |⟨dE,R(·,·,dE)⟩−⟨dˆE,R(·,·,dˆE)⟩| ≤ L||dˆE −dE|| . Same
∞
clearlyholdsforthedifferencebetweenmaxterms. If||dˆE−dE|| ≤ϵ,thenwehavetheinequality
∞
|ϕ(πˆE;R)−ϕ(πE;R)|≤2L||dˆE−dE|| ≤2Lϵ.Sinceϕ(πE;R)=0andϕ(π;R)≥0foranyπ
∞
bydefinition,wehaveϕ(πˆE;R)≤2Lϵ.ThereforeπˆE isa2Lϵ-equilibriumforrewardR.Similarly,
takeRˆ ∈ R suchthatϕ(πˆE;Rˆ) = 0.Wecandoasimilarderivationwithϕ(πE;Rˆ)−ϕ(πˆE;Rˆ)
Bˆ
andendupwithϕ(πE;Rˆ)≤2Lϵ.ThereforeπE isan2Lϵ-equilibriumforrewardRˆ.
WewillusethissymmetrytoshowhowthesimilaritybetweenRBandR
Bˆ
canbemeasured. Namely,
we propose that two CURL-MFG reward functions are similar if the equilibrium of one is an ϵ-
equilibrium of the other. Such reward functions are sometimes called ϵ-inverse equilibria [40].
ImportanttonotethattheinequalitiesaboveholdforanypairofrewardsR∈RB,Rˆ ∈R Bˆ.Thenwe
willsaythatthedissimilaritybetweenthetwosetsisD(RB,R Bˆ)=2Lϵ.Itisclearthatifthegap
||dE −dˆE|| getssmaller,thetwosetsbecomemoresimilarsinceϵgetssmaller. Thismeanswithin
∞
7theclassofL-Lipschitzrewardfunctions,wecancontroltheI-CURLerrorbytheestimationerror.
Thefollowinglemmapresentsasamplecomplexityboundtoachieve≤ϵerrorwithhighprobability.
Lemma15(SampleComplexityofEstimatingdE) Letnbethenumberofsamplesfromtheequi-
librium(πE,dE)suchthat(s ,a )∼dE(s,a)fori∈[n]. Thenforeveryϵ>0,theestimationerror
i i
canbeboundedasPr(||dE −dˆE|| >ϵ)≤2e−2nϵ2.
∞
Proof. Theproofisadirectapplicationofaso-called"folklorefact"aboutlearningdiscretedistribu-
tions(seeTheorem10of[41]),whichitselfisduetotheclassicalresultofDvoretzkyetal.[42]and
thefactthat||dE −dˆE|| ≤2d (dE,dˆE)whered istheKolmogorovdistance.
∞ k k
Thefollowingcorollaryisastructuralresultthatfollowsdirectlyfromdefinitions,andtellsushow
thedynamicsT andtheinitialstatedistributionµ affectthedifficultyoftheI-CURLproblem. For
0
example,ifthestartingstateisfixedinachain-likedynamicswithlowstochasticity,thediameterD
canbereallylow. Thenanypolicyπwillinduceverysimilaroccupancymeasures. Thismeanswe
mightwantamorefine-grainederrorcontrolwithanϵthatisrelativetoLDinthefuture.
Corollary16(TheEffectofK ’sDiameter) Undertheassumption13,letD =max ||d′−
γ d′,d∈Kγ
d|| denotethediameterofK .Thenϕ(π;R)≤2LDforanyπ.
∞ γ
Proof. Followsfromtheorem14byreplacingϵwithD.
3.3 DiscussionontheTheoreticalAssumptionsandPracticalApplications
The assumption 13 is in general a reasonable assumption for a reward function, since even for
continuousstate-actionspaces,boundedrewardsisacommonassumptioninRL.Thismeanswe
canalwayschooseL=R DtobethemaximumpossiblerewardtimesthediameteroftheK .
max γ
However,inthisparticularcase,theCURL-MFGrewardisdefinedas∇F,andthegradientofa
concavefunctionneednotbefiniteeverywhere. Below,wewillshowthatinpracticethisisnotan
issueformanyCURLproblems.
ConsiderthesettingwhereF(dπ) = (1−λ)⟨dπ,R⟩−λKL(dπ||dρ)withafixeddρ. Depending
on λ, this formulation can represent various CURL problems such as divergence minimization,
state-marginalmatching,offlineRL,human-regularizedRL,andcautiousRL(seetable1). Herethe
inducedCURL-MFG’srewardfunctionisR(·,·,dπ)=(1−λ)⟨dπ,R⟩−λ∇[KL(dπ||dρ)].Ignoring
thefirsttermwhichislinearandbounded,theassumption13holdsifandonlyif|∇KL(d′||dρ)−
∇KL(d||dρ)|≤L′||d−d′||forsomeL′aswell. Unfortunately,KLdivergencedoesnotsatisfythis
bound,sinceitisnotLipschitzsmooth. Therefore,itappearstobethatanyCURLproblemwitha
KL-divergencetermintheobjectiveF failstosatisfyourassumptions.
ThereasonwhyKL-divergenceisnotLipschitzsmoothisduetothefactthatitsgradientcanbe
infinite. Fortunately, in many practical CURL problems this is not the case, which means our
assumptionsarevalidforthem. ConsiderthecaseR(·,·,dπ)=⟨dπ,R⟩+∇[−1KL(dπ||dρ)]where
β
nowdρ anddπ arethestateoccupancymeasures. Aslongasdρ anddπ areabsolutelycontinuous
withrespecttoeachother,thegradientoftheKLdivergenceisguaranteedtobefinite. Onecorollary
thatfollowsfromthisisthefactthatergodicity(i.e. everystatehasanon-zerooccupancyforany
policy)guaranteesassumption13inthesetypeofproblems. InthecontextofofflineRL,thedρ
canbeastate-actionoccupancymeasureestimatedfromadataset. Acommonassumptioninoffline
RListheuniformlyboundedconcentrabilitycoefficient,whichmeans
dπ(s,a)
≤C forsomefinite
dρ(s,a)
C everywhere[43]. ThisassumptionalsoensuresLipschitz-smoothnessoftheKLterminfinite
state-action spaces. Similar arguments can be made for instance for constrained MDPs through
smoothbarrierfunctionssuchasZhangetal.[44].
4 RelatedWorks
Therecentworkoninversereinforcementlearninghasfocusedidentifyingthesetofallfeasible
reward functions instead of a single reward function [34–36]. This re-formulation resolves the
ill-posednessoftheIRLproblemandprovidesamorerobusttheoreticalapproachtoanalysingthe
complexityofIRL.InourworkwefollowthesameapproachforI-CURLproblems.
8InversedecisionmodellingwasproposedbyJarrettetal.[45]asanapproachtolearnmoreinter-
pretableandexpressiverepresentationsofboundedrationalbehaviour. I-CURLfollowsthesameline
ofreasoning,sinceCURLcanrepresentvariousbehavioursduetocognitiveconstraints.Animportant
lineofworkinrecentyearshasbeenconsideringdifferentformsofboundedrationalityininverseRL
[46–50]. Thisisduetoanimportantresultshowingefficientrewardinferenceisnotpossiblewithout
makingassumptionsabouttheagent’srationality[25]. InverseRLhasbeenacontenderforsolving
thevaluealignmentproblembyinferringtheconstraintsandgoalsofhumansthroughinferringthe
rewardfunction,eventuallyleadingtoAIagentsthatactaccordingtotheobjectivesofhumanusers
[51,52]. RecentworkhasappliedIRLtotheproblemofaligningAIagentswithhumannorms[53].
Thegoalofpopulation-levelapproachesistoinferthepopulation-levelrewardoftheformR(dπ,π)
fromadatasetconsistingofoccupancymeasureandpolicypairs(d ,π ),...,(d ,π )insteadof
0 0 T T
state-actiontuples[54]. Ourapproachisbasedonindividual-levelinversegametheoryformean-
fieldgames,whichhasbeenexploredempiricallyforgeneralmean-fieldgames[39,55]. Wefocus
particularly on leveraging the structure of MFGs induced by CURL problems, and derive finite-
sampleresults. Theproblemofrationalisingobservedmulti-agentbehaviourbyinferringrewardshas
beenreferredtoasinversemulti-agentRL[56],inversegametheory[37],andinverseequilibrium
problems[38]indifferentcontexts. RecentformulationbyGoktasetal.[40]representedtheIGT
problemitselfasazero-sumgamewhosesolutionsetisreferredtoastheinverseNashequilibria.
5 Discussion
AsdiscussedinSection1,theboundedrationalityofhumanbehaviourcanoftenbemodelledasa
CURLpolicy. Forinstance,thecautiousreinforcementlearningformulationbyZhangetal.[18]
representsanagentactingcautiouslybystayingclosetotheirpriorbehaviour. Similarly,information-
bounded decision-making models the cognitive cost of decision-making as a divergence from a
low-effortbaselinepolicy. Furtherhumancognitiveconstraintscanbeframedwithinaconstrained
MDP,presentingaCURLproblem. Inthiscontext,boundedrationality(alsoknownasresource-
rationality)impliesthatobservedhumanbehaviourreflectsanoptimalpolicyundertheseconstraints.
Consequently,ourinverseCURLapproachiscrucialforlearningrewardfunctionsandpreferences
fromhumanbehaviour. Thisargumentissupportedbyrecentworksadvocatingforcognitivemodels
of humans as inverse models in robotics and for making machine learning models more human-
centricbyincorporatingusermodelingpipelines[57,58]. Inmulti-agentsettings,wherehumans
cooperatewithandoverseeAIagents,additionalcognitivecostsandconstraintscansignificantly
impactoutcomes[59]. Furthermore,asnotedbyJarrettetal.[45],aninverseRLmethodologythat
accommodatesboundedrationalbehaviourmodelsleadstomoreinterpretablerewardfunctionsand
taskdescriptions. Thereforetheabilitytolearnrewardfunctionsandtaskdescriptionsfrombounded
rationalhumanbehaviouriscrucialforenhancedhuman–AIcollaborationandalignment.
Limitations and Future Work. In this work, we assumed known transition dynamics. Even
thoughacommonassumptioninstandardIRL,therearepracticalapplicationswherethismaynotbe
realistic. Thus,relaxingthisassumptionisanimportantfuturedirection. Inthiswork,wefocusedon
formulatingthenovelproblemofI-CURLandtheoreticallyanalysingitsfeasibility. Adirectionthis
opensupisimplementingtheindividual-levelIGTformean-fieldgamesandshowingitsempirical
performanceforI-CURLwithdifferentfunctionclasses.WehavefocusedontheMFGformulationof
CURL,andaninterestingdirectioniswhetherasimilarformulationcanbederivedforthezero-sum
formulationofZahavyetal.[6]. Finally,perhapsthemostinterestingdirectionistoapplyI-CURLto
learninginterpretablerewardfunctionsfromdifferentmodelsofboundedrationalhumanbehaviour
suchasinformation-boundednessorrisk-aversion.
6 Conclusion
Standardinversereinforcementlearningisnotdirectlyapplicabletoinferringrewardfunctionsof
agentsthathaveoptimizedtheirpoliciesforsolvingaconcaveutilityreinforcementlearningproblem.
EvenintheidealizedcaseofhavingaccesstotheexpertpolicyπE,wehaveshownthatthesetof
feasiblerewardfunctionsasdefinedbythestandardIRLliteraturecanbeempty. Wehaveresolved
this problem by proving a game-theoretic characterization of the feasible reward set for inverse
CURL,leveragingaresultthatshowseveryCURLproblemisequivalenttoamean-fieldgame. Our
9theorem12showsthattheexactcomputationunderknownπE canbetackledefficientlywiththe
AIMPmethodofconcurrentworkunderaconvexity-concavityassumption[40]. Intheorem14,we
haveproventhatfortheclassofL-Lipschitzrewardfunctions,abetterestimationofdE impliesa
feasiblesetmoresimilartoRBintermsoftheinducedequilibria. Thisallowsustocontroltheerror
ofI-CURLviatheestimationerror,providinguswiththesamplecomplexityresultinlemma15. To
ourknowledge,ourworkpresentsthefirstformulationandtheoreticalanalysisoftheinverseCURL
problem.
AcknowledgmentsandDisclosureofFunding
Thisresearchwas(partially)fundedbytheHybridIntelligenceCenter,a10-yearprogrammefunded
bytheDutchMinistryofEducation,CultureandSciencethroughtheNetherlandsOrganisationfor
ScientificResearch,https://hybrid-intelligence-centre.nl.
References
[1] JunyuZhang,AlecKoppel,AmritSinghBedi,CsabaSzepesvari,andMengdiWang.Variational
policygradientmethodforreinforcementlearningwithgeneralutilities. AdvancesinNeural
InformationProcessingSystems,33:4572–4583,2020.
[2] MatthieuGeist,JulienPérolat,MathieuLaurière,RomualdElie,SarahPerrin,OliverBachem,
RémiMunos,andOlivierPietquin. Concaveutilityreinforcementlearning: Themean-field
gameviewpoint.InProceedingsofthe21stInternationalConferenceonAutonomousAgentsand
MultiagentSystems,AAMAS’22,page489–497,Richland,SC,2022.InternationalFoundation
forAutonomousAgentsandMultiagentSystems. ISBN9781450392136.
[3] EladHazan,ShamKakade,KaranSingh,andAbbyVanSoest. Provablyefficientmaximum
entropy exploration. In International Conference on Machine Learning, pages 2681–2691.
PMLR,2019.
[4] MridulAgarwal,QinboBai,andVaneetAggarwal. Concaveutilityreinforcementlearningwith
zero-constraintviolations. TransactionsonMachineLearningResearch,2022.
[5] MircoMutti,RiccardoDeSanti,PiersilvioDeBartolomeis,andMarcelloRestelli. Convex
reinforcementlearninginfinitetrials. JournalofMachineLearningResearch,24(250):1–42,
2023.
[6] TomZahavy, BrendanO’Donoghue, GuillaumeDesjardins, andSatinderSingh. Rewardis
enoughforconvexmdps.AdvancesinNeuralInformationProcessingSystems,34:25746–25759,
2021.
[7] PedroP.Santos. Generalizingobjective-specificationinmarkovdecisionprocesses. InPro-
ceedingsofthe23rdInternationalConferenceonAutonomousAgentsandMultiagentSystems,
AAMAS’24,page2767–2769,Richland,SC,2024.InternationalFoundationforAutonomous
AgentsandMultiagentSystems. ISBN9798400704864.
[8] EitanAltman. ConstrainedMarkovdecisionprocesses. Routledge,2021.
[9] VivekSBorkar. Anactor-criticalgorithmforconstrainedmarkovdecisionprocesses. Systems
&controlletters,54(3):207–213,2005.
[10] ChenTessler,DanielJMankowitz,andShieMannor. Rewardconstrainedpolicyoptimization.
InInternationalConferenceonLearningRepresentations,2018.
[11] DanACalian,DanielJMankowitz,TomZahavy,ZhongwenXu,JunhyukOh,NirLevine,and
TimothyMann. Balancingconstraintsandrewardswithmeta-gradientd4pg. InInternational
ConferenceonLearningRepresentations,2020.
[12] JonathanHoandStefanoErmon. Generativeadversarialimitationlearning. Advancesinneural
informationprocessingsystems,29,2016.
10[13] Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric Xing, Sergey Levine, and Rus-
lan Salakhutdinov. Efficient exploration via state marginal matching. arXiv preprint
arXiv:1906.05274,2019.
[14] Seyed Kamyar Seyed Ghasemipour, Richard Zemel, and Shixiang Gu. A divergence mini-
mizationperspectiveonimitationlearningmethods. InConferenceonrobotlearning,pages
1259–1277.PMLR,2020.
[15] AvivTamar,YinlamChow,MohammadGhavamzadeh,andShieMannor. Policygradientfor
coherentriskmeasures. Advancesinneuralinformationprocessingsystems,28,2015.
[16] YinlamChow,AvivTamar,ShieMannor,andMarcoPavone. Risk-sensitiveandrobustdecision-
making: acvaroptimizationapproach. Advancesinneuralinformationprocessingsystems,28,
2015.
[17] YinlamChow,MohammadGhavamzadeh,LucasJanson,andMarcoPavone. Risk-constrained
reinforcementlearningwithpercentileriskcriteria. JournalofMachineLearningResearch,18
(167):1–51,2018.
[18] JunyuZhang,A.S.Bedi,MengdiWang,andAlecKoppel. Cautiousreinforcementlearningvia
distributionalriskinthedualdomain. IEEEJournalonSelectedAreasinInformationTheory,
2:611–626,2020. URLhttps://api.semanticscholar.org/CorpusID:211572706.
[19] DaphneCornelisseandEugeneVinitsky. Human-compatibledrivingpartnersthroughdata-
regularizedself-playreinforcementlearning. arXivpreprintarXiv:2403.19648,2024.
[20] AthulPaulJacob,DavidJWu,GabrieleFarina,AdamLerer,HengyuanHu,AntonBakhtin,Ja-
cobAndreas,andNoamBrown. Modelingstrongandhuman-likegameplaywithkl-regularized
search. InInternationalConferenceonMachineLearning,pages9695–9728.PMLR,2022.
[21] AndrewYNgandStuartJRussell. Algorithmsforinversereinforcementlearning. InPro-
ceedingsoftheSeventeenthInternationalConferenceonMachineLearning,pages663–670,
2000.
[22] HerbertASimon. Modelsofman: Socialandrational. Wiley,1957.
[23] Richard L Lewis, Andrew Howes, and Satinder Singh. Computational rationality: Linking
mechanismandbehaviorthroughboundedutilitymaximization. Topicsincognitivescience,6
(2):279–311,2014.
[24] Samuel J Gershman, Eric J Horvitz, and Joshua B Tenenbaum. Computational rationality:
Aconvergingparadigmforintelligenceinbrains,minds,andmachines. Science,349(6245):
273–278,2015.
[25] StuartArmstrongandSörenMindermann. Occam’srazorisinsufficienttoinferthepreferences
ofirrationalagents. Advancesinneuralinformationprocessingsystems,31,2018.
[26] PedroAOrtegaandDanielABraun. Informationprocessingandboundedrationality: Asurvey.
FrontiersinDecisionNeuroscience,7:1–22,2013.
[27] PedroAOrtegaandDanielABraun. Thermodynamicsasatheoryofdecision-makingwith
information-processingcosts. ProceedingsoftheRoyalSocietyA:Mathematical,Physicaland
EngineeringSciences,469(2153):20120683,2013.
[28] Daniel A. Braun and Pedro A. Ortega. Information-theoretic bounded rationality and ϵ-
optimality. Entropy, 16(8):4662–4676, 2014. ISSN 1099-4300. doi: 10.3390/e16084662.
URLhttps://www.mdpi.com/1099-4300/16/8/4662.
[29] MarkKHo,DavidAbel,JonathanDCohen,MichaelLLittman,andThomasLGriffiths. The
efficiencyofhumancognitionreflectsplannedinformationprocessing. InProceedingsofthe
34thaaaiconferenceonartificialintelligence,2020.
[30] JonathanRubin, OhadShamir, andNaftaliTishby. Tradingvalueandinformationinmdps.
Decisionmakingwithimperfectdecisionmakers,pages57–74,2012.
11[31] Tim Genewein, Felix Leibfried, Jordi Grau-Moya, and Daniel Alexander Braun. Bounded
rationality,abstraction,andhierarchicaldecision-making: Aninformation-theoreticoptimality
principle. FrontiersinRoboticsandAI,2:27,2015.
[32] GianLucaLancia,MattiaEluchans,MarcoD’Alessandro,HugoJSpiers,andGiovanniPezzulo.
Humansaccountforcognitivecostswhenfindingshortcuts: Aninformation-theoreticanalysis
ofnavigation. PLOSComputationalBiology,19(1):e1010829,2023.
[33] MartinLPuterman. Markovdecisionprocesses: discretestochasticdynamicprogramming.
JohnWiley&Sons,2014.
[34] AlbertoMariaMetelli,GiorgiaRamponi,AlessandroConcetti,andMarcelloRestelli. Provably
efficientlearningoftransferablerewards. InInternationalConferenceonMachineLearning,
pages7665–7676.PMLR,2021.
[35] AlbertoMariaMetelli,FilippoLazzati,andMarcelloRestelli.Towardstheoreticalunderstanding
ofinversereinforcementlearning. InInternationalConferenceonMachineLearning,pages
24555–24591.PMLR,2023.
[36] David Lindner, Andreas Krause, and Giorgia Ramponi. Active exploration for inverse re-
inforcement learning. Advances in Neural Information Processing Systems, 35:5843–5853,
2022.
[37] VolodymyrKuleshovandOkkeSchrijvers. Inversegametheory: Learningutilitiesinsuccinct
games.InWebandInternetEconomics:11thInternationalConference,WINE2015,Amsterdam,
TheNetherlands,December9-12,2015,Proceedings11,pages413–427.Springer,2015.
[38] KevinWaugh,BrianDZiebart,andJAndrewBagnell. Computationalrationalization: thein-
verseequilibriumproblem.InProceedingsofthe28thInternationalConferenceonInternational
ConferenceonMachineLearning,pages1169–1176,2011.
[39] Yang Chen, Libo Zhang, Jiamou Liu, and Shuyue Hu. Individual-level inverse reinforce-
mentlearningformeanfieldgames. InProceedingsofthe21stInternationalConferenceon
AutonomousAgentsandMultiagentSystems,pages253–262,2022.
[40] DenizalpGoktas,AmyGreenwald,SadieZhao,AlecKoppel,andSumitraGanesh. Generative
adversarialinversemultiagentlearning. InTheTwelfthInternationalConferenceonLearning
Representations,2024. URLhttps://openreview.net/forum?id=JzvIWvC9MG.
[41] ClémentL.Canonne. Ashortnoteonlearningdiscretedistributions. https://github.com/
ccanonne/probabilitydistributiontoolbox/blob/master/learning.pdf, 2023.
Accessed: 2024-05-19.
[42] AryehDvoretzky,JackKiefer,andJacobWolfowitz. Asymptoticminimaxcharacterofthesam-
pledistributionfunctionandoftheclassicalmultinomialestimator. TheAnnalsofMathematical
Statistics,pages642–669,1956.
[43] PariaRashidinejad,BanghuaZhu,CongMa,JiantaoJiao,andStuartRussell. Bridgingoffline
reinforcement learning and imitation learning: A tale of pessimism. Advances in Neural
InformationProcessingSystems,34:11702–11716,2021.
[44] BaoheZhang,YuanZhang,LilliFrison,ThomasBrox,andJoschkaBödecker. Constrained
reinforcementlearningwithsmoothedlogbarrierfunction. arXivpreprintarXiv:2403.14508,
2024.
[45] DanielJarrett,AlihanHüyük,andMihaelaVanDerSchaar.Inversedecisionmodeling:Learning
interpretablerepresentationsofbehavior. InInternationalConferenceonMachineLearning,
pages4755–4771.PMLR,2021.
[46] OwainEvans,AndreasStuhlmüller,andNoahGoodman. Learningthepreferencesofignorant,
inconsistentagents.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume30,
2016.
12[47] AnirudhaMajumdar,SumeetSingh,AjayMandlekar,andMarcoPavone. Risk-sensitiveinverse
reinforcementlearningviacoherentriskmodels. InRobotics: scienceandsystems,volume16,
page117,2017.
[48] LawrenceChan,AndrewCritch,andAncaDragan. Humanirrationality: bothbadandgoodfor
rewardinference. arXivpreprintarXiv:2111.06956,2021.
[49] Sumeet Singh, Jonathan Lacotte, Anirudha Majumdar, and Marco Pavone. Risk-sensitive
inversereinforcementlearningviasemi-andnon-parametricmethods. TheInternationalJournal
ofRoboticsResearch,37(13-14):1713–1740,2018.
[50] RohinShah,NoahGundotra,PieterAbbeel,andAncaDragan. Onthefeasibilityoflearning,
rather than assuming, human biases for reward inference. In International Conference on
MachineLearning,pages5670–5679.PMLR,2019.
[51] DylanHadfield-Menell,StuartJRussell,PieterAbbeel,andAncaDragan. Cooperativeinverse
reinforcementlearning. Advancesinneuralinformationprocessingsystems,29,2016.
[52] DhruvMalik,MalayandiPalaniappan,JaimeFisac,DylanHadfield-Menell,StuartRussell,and
AncaDragan. Anefficient,generalizedbellmanupdateforcooperativeinversereinforcement
learning. InInternationalConferenceonMachineLearning,pages3394–3402.PMLR,2018.
[53] M Peschl, A Zgonnikov, FA Oliehoek, and L Cavalcante Siebert. Moral: Aligning ai with
human norms through multi-objective reinforced active learning. In AAMAS 2022: 21st
International Conference on Autonomous Agents and Multiagent Systems (Virtual), pages
1038–1046.InternationalFoundationforAutonomousAgentsandMultiagentSystems,2022.
[54] JiachenYang,XiaojingYe,RakshitTrivedi,HuanXu,andHongyuanZha. Learningdeepmean
fieldgamesformodelinglargepopulationbehavior. InInternationalConferenceonLearning
Representations,2018.
[55] YangChen,LiboZhang,JiamouLiu,andMichaelWitbrock. Adversarialinversereinforcement
learning for mean field games. In Proceedings of the 2023 International Conference on
AutonomousAgentsandMultiagentSystems,pages1088–1096,2023.
[56] SriraamNatarajan,GautamKunapuli,KshitijJudah,PrasadTadepalli,KristianKersting,and
JudeShavlik.Multi-agentinversereinforcementlearning.In2010ninthinternationalconference
onmachinelearningandapplications,pages395–400.IEEE,2010.
[57] Mark K Ho and Thomas L Griffiths. Cognitive science as a source of forward and inverse
modelsofhumandecisionsforroboticsandcontrol. AnnualReviewofControl,Robotics,and
AutonomousSystems,5:33–53,2022.
[58] MustafaMertÇelikok, Pierre-AlexandreMurena, andSamuelKaski. Modelingneedsuser
modeling. FrontiersinArtificialIntelligence,6:1097891,2023.
[59] Mustafa Mert Çelikok, Frans A Oliehoek, and Samuel Kaski. Best-response bayesian rein-
forcement learning with bayes-adaptive pomdps for centaurs. In International Conference
onAutonomousAgentsandMultiagentSystems,pages235–243.InternationalFoundationfor
AutonomousAgentsandMultiagentSystems(IFAAMAS),2022.
A Appendix/supplementalmaterial
A.1 TheConnectionbetweenZero-sumandMean-fieldFormulationsofCURL
AnyCURLproblemcanbere-formulatedasazero-sumgamewithconvex-concavepayoffs[6]. To
explain,weneedthefollowingdefinition.
Definition17(FenchelConjugate) TheFenchelconjugateofafunctionf isdefinedasf∗(y) ≜
sup ⟨x,y⟩−f(x).
x
13Fenchelconjugateisoftencalledconvexconjugate. Eventhoughthereisanequivalentdefinitionof
concaveconjugatewithinf insteadofsup,togoalongwiththeestablishednotation,wewillusethe
convexequivalentoftheCURLproblem,min E(dπ)whereE =−F.LetE∗betheFenchel
dπ∈Kγ
conjugateofE.SinceE isconvex(andsemi-continuous),theconjugateofitsconjugateisequalto
itself,(E∗)∗ =E.Then,theCURLproblemcanbere-writtenastheconvex-concavezero-sumgame
min E(dπ)= min max⟨dπ,λ⟩−E∗(λ), (4)
dπ∈Kγ dπ∈Kγ λ∈Λ
whereΛ={∇E(dπ)|dπ ∈K }istheclosureofthe(sub-)gradientspace. Aninterestingpointto
γ
noticehereisthefactthatforfixedλ, theminimisationproblemcanbethoughtofasastandard
reinforcementlearningproblemwithrewardfunction−λ.Wecanconnectthiszero-sumformulation
tothemean-fieldoneasfollows. SinceE = −F andF isconcave,the∇E isthesameas−∇F.
ThenΛ = {−∇F(d)|d ∈ K } = {−R(·,·,d)|d ∈ K }whereR(·,·,d)istherewardfunctionof
γ γ
theCURL-MFG.Thenthezero-sumgamecanbeequivalentlyexpressedas
min max⟨dπ,−R(·,·,d)⟩−E∗(−R(·,·,d)). (5)
dπ∈Kγd∈Kγ
14