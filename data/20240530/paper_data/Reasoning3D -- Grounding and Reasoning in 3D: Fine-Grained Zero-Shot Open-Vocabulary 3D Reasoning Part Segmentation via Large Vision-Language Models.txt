Reasoning3D - Grounding and Reasoning in 3D:
Fine-Grained Zero-Shot Open-Vocabulary 3D
Reasoning Part Segmentation via Large
Vision-Language Models
Tianrun Chen1,2+ and Chunan Yu3+, Jing Li3, Jianqi Zhang3, Lanyun Zhu4,
Deyi Ji6, Yong Zhang3, Ying Zang3∗, Zejian Li5, Lingyun Sun2
1 KOKONI, Moxin (Huzhou) Tech. Co., LTD.
2 College of Computer Science and Technology, Zhejiang University.
3 School of Information Engineering, Huzhou University.
4 Information Systems Technology and Design Pillar, Singapore University of
Technology and Design.
5 School of Software Technology, Zhejiang University.
6 School of Information Science and Technology, University of Science and
Technology of China.
+ Equal Contribution * Corresponding Author
tianrun.chen@zju.edu.cn; 02750@zjhu.edu.cn
Project Page: http://tianrun-chen.github.io/Reason3D/
(a) Reasoning 3D Segmentation
If you are hungry, Can you segment the portions that taste If you want to move from one place to another, Can you
best in this image? Please output segmentation mask and segment which part you need to use in this image? Please
explain why. output segmentation mask and explain why.
In the picture, the part of the If you want to move from one
hamburger that is likely to taste the place to another, you can use
best is the beef patty. The patty is the the two legs of the chair as
meat component of the hamburger, wheels to roll it across the floor.
and its quality and flavor directly This would allow you to move
contribute to the overall taste of the the chair easily from one
sandwich. location to another without
having to lift it
"What can cut fish into small pieces? Please output Can you segment the part that is essential if you want to
segmentation mask and explain why." detonate it in this image? Please output segmentation mask
and explain why.
In the picture, there is a large fish on a cutting board, and a
knife is present nearby. The knife can be used to cut the fish In the picture, the essential part
into smaller pieces or portions, as needed. for detonating the bomb is the
red button located on the top of
the bomb. This button is
typically used to arm or trigger
the bomb, causing it to explode
when pressed.
Can you segment the abdomen and the foot and the butt
Can you segment the man's head in this image? and the area below the abdomen in this image?
<SEG> <SEG>
(b) Open-Vocabulary 3D Segmentation
Fig.1: In this work, we propose a new task: reasoning 3D segmentation. We also
proposeamethodthatcansegment3Dobjectpartswithexplanationsbasedonvarious
criteria such as reasoning, shape, location, function, and conceptual instructions.
4202
yaM
92
]VC.sc[
1v62391.5042:viXra2 T, Chen et al.
Abstract. Inthispaper,weintroduceanewtask:Zero-Shot3DReason-
ing Segmentation for parts searching and localization for objects, which
is a new paradigm to 3D segmentation that transcends limitations for
previous category-specific 3D semantic segmentation, 3D instance seg-
mentation, and open-vocabulary 3D segmentation. We design a simple
baseline method, Reasoning3D, with the capability to understand and
execute complex commands for (fine-grained) segmenting specific parts
for3Dmesheswithcontextualawarenessandreasonedanswersforinter-
activesegmentation.Specifically,Reasoning3Dleveragesanoff-the-shelf
pre-trained2Dsegmentationnetwork,poweredbyLargeLanguageMod-
els(LLMs),tointerpretuserinputqueriesinazero-shotmanner.Previ-
ous research have shown that extensive pre-training endows foundation
modelswithpriorworldknowledge,enablingthemtocomprehendcom-
plex commands, a capability we can harness to "segment anything" in
3D with limited 3D datasets (source efficient). Experimentation reveals
that our approach is generalizable and can effectively localize and high-
lightpartsof3Dobjects(in3Dmesh)basedonimplicittextualqueries,
includingthesearticulated3dobjectsandreal-worldscanneddata.Our
method can also generate natural language explanations corresponding
to these 3D models and the decomposition. Moreover, our training-free
approachallowsrapiddeploymentandservesasaviableuniversalbase-
line for future research of part-level 3d (semantic) object understanding
in various fields including robotics, object manipulation, part assem-
bly, autonomous driving applications, augment reality and virtual real-
ity (AR/VR), and medical applications. The code, the model weight,
the deployment guide, and the evaluation protocol are: http://tianrun-
chen.github.io/Reason3D/.
Keywords: Reasoning Segmentation · 3D Segmentation · 3D Model
Parsing·3DPartUnderstanding·LargeLanguageModel·LargeVision-
Language Model · Computer-Human Interaction
1 Introduction
The importance of 3D segmentation cannot be overstated - it is foundational in
fields like robotics, autonomous driving, and augmented reality [24,63,67]. Tra-
ditional approaches have often required extensive manual labeling or complex
rule-based algorithms that struggle to generalize to diverse real-world scenar-
ios [37,39,66,68]. The sheer complexity of 3D data, combined with the inherent
ambiguities and varying viewpoints, has posed significant challenges in develop-
ing robust and generalizable 3D segmentation techniques.
In this work, we introduce fine-grained Zero-Shot 3D Reasoning Seg-
mentation for parts in 3D objects, which aims to bring 3D segmentation to a
newlevel.Imagineinstructingasystemwithwordslike"segmentthepartofthe
chair where you would sit" or "highlight the nutritious parts of this vegetable"
and watching it magically understand and perform the task in the 3D world.
You can have natural conversations with the system and see it output the seg-
mentationmaskalongwithexplanations(SeeFig.1forexamples).ItisafutureReasoning3D - Grounding and Reasoning in 3D 3
where 3D systems can intuitively understand and respond to intricate queries –
The possibilities are endless.
However, achieving this vision is no small feat. Traditional 3D segmentation
approaches are typically confined to fixed object categories, severely limiting
theirflexibility.Recentendeavorsinopen-vocabularysegmentationcanhandlea
broaderrangeoflabelsbutarestilllimitedtodealingwithstraightforwardtasks
like labeling "the apple" and cannot handle complex, reasoning-based queries.
Askingasystemtoperformmorenuancedoperationslike"segmentedibleparts
ofafruit"requiresalevelofcontextualunderstandingandreasoningthatcurrent
methods do not possess.
Thanks to the recent advancements in Multi-modal Large Language Models
(LLMs) [33,38,51,56,74,78], we can now bring our aforementioned vision to
life. Recently, Large Vision-Language Models (LVLM) have shown remarkable
capabilitiesincomprehending2Dimages,excellingintasksthatrequirecomplex
reasoning,multi-turnconversations,andexplanatoryanswers[28,65,76].Weaim
toextendtheircapabilitiesintothe3Drealm,andwebelievethatthistransition
ispromisingwithmuchpracticalvalue–neverforgetthatweliveina3Dworld!
However, extending the success of reasoning segmentation from 2D to 3D
domains also presents substantial challenges. The scarcity of available 3D data
and ground-truth Question-and-answer pairs stopped us from performing large-
scaletraining.Theaddeddimensionalsoincreasesthecomputationaldemandsof
3Darchitecturalcomponents.Here,inspiredbyresearchthathastackledsimilar
challenges in 3D generation [16,17,52,54,60,72] – using network models in 2D
and then lifting some information to 3D, we introduce our approach to leverage
off-the-shelf 2D models to perform the task in a zero-manner. This approach,
which we named Reasoning3D, allows us to circumvent the limitations imposed
by the scarcity of extensive 3D datasets and the high computational costs with
its training-free property and 2D pre-training.
Specifically, our Reasoning3D approach involves rendering a 3D model from
multiple viewpoints and applying a pre-trained reasoning segmentation network
toeach2Dviewbasedonthegivenqueryinput.Bydoingso,wegeneratesegmen-
tation masks and accompanying text explanations for each perspective. These
individual masks and explanations are then fused to produce a comprehensive
3D segmentation mask (labels are assigned to the vertices of the 3D model).
We have evaluated our approach in various models in the wild, both with and
without textures. We have also tested our approach in existing open-vocabulary
segmentation benchmarks, which validates the effectiveness of our approach.
WhileReasoning3Disastraightforwardbaselinemethod,webelieveitserves
as a good starting point for researchers to explore and expand the future of 3D
part segmentation. We will release the implementation code and the benchmark
codepubliclytofacilitatefutureresearch,withthehopethatourinitialstepsets
the stage for further innovation and refinement, and eventually bring us closer
to a future where 3D computer vision systems are as versatile and perceptive
as human cognition, capable of revolutionizing a myriad of applications across
various fields.4 T, Chen et al.
2 Related Work
2.1 3D Semantic Segmentation.
Segmentation in 2D scenes has achieved significant progress in recent years
[11,58,70,77,79], yet understanding and reasoning in 3D environments is still a
crucial research area that needs more attention. In the domain of 3D semantic
segmentation, our objective is to predict the semantics of each point in a point
cloud. Notable advancements in this field encompass point-based approaches
[5,23], leveraging sophisticated point convolution techniques [55,61], and voxel-
based approaches [12,18]. Some techniques generate point-level segmentation
results using 3D sparse convolutions [11], while others utilize transformer-based
approaches [27]. Furthermore, multi-view semantic segmentation methods such
as DeepViewAgg [47], Diffuser [26,42], 3D-CG [20], and 3D-CLR [21] enhance
representation learning by creating 2D projections of 3D scenes from different
viewpoints. Studies have demonstrated that multi-view representations effec-
tively improve the performance and robustness of various 3D tasks. Neverthe-
less, these methods usually depend on predefined semantic label sets, whereas
our approach is tailored to address and interpret complex reasoning queries. We
believe that, following the trend that researchers trying to use various inputs to
improve the computer-human interaction in 3D models [7–10,69,71], this work
can provide another avenue for manipulating 3D contents.
2.2 Large Multimodal Models.
Extensive research on large language models (LLMs) has highlighted their rea-
soning capabilities, leading to efforts to expand these abilities into the visual
domain using large multimodal models (LMMs). LMMs are highly adaptable
andversatile,capableofperformingtasksthatrequirebothlanguageandvision
skills. Notable models like BLIP-2 [30], LLaVA [35], and MiniGPT-4 [75] typi-
cally utilize a two-phase training approach, which aligns visual representations
with LLMs’ linguistic embeddings through extensive image-text and video-text
datasets[2,4,34,43,48–50].Recently,thefocushasbeenonmergingmultimodal
LLMs with vision tasks. VisionLLM [57], for example, offers a versatile inter-
face for various vision-centric tasks via instruction tuning. Nevertheless, this
model does not fully leverage the sophisticated reasoning capabilities of LLMs.
Kosmos-2 [45] seeks to bolster the foundational abilities of LLMs by creating
large datasets of aligned image-text pairs. DetGPT [46] smoothly combines
a fixed multimodal LLM framework with an open-vocabulary detector to en-
ableinstruction-baseddetection.LISA,LISA++[28,65]generatessegmentation
masksusingembeddingsfromvision-languagemodels,andLLaFSgeneratesseg-
mentationmasksusingcoordinatesexportedfromLLM.GPT4RoI[73]presents
an innovative method by incorporating spatial boxes as inputs and training on
region-text pairs.
Unlikepreviousapproaches,ourapproachaimstointegratethevision-language
capabilitiesofLMMswiththereasoningstrengthsofLLMsinnew3Dperception
tasks, taking advantage of these developments in the LMM field.Reasoning3D - Grounding and Reasoning in 3D 5
2.3 Language Instructed 3D Tasks.
Thefusionofpointcloudswithnaturallanguageprocessinghasprofoundimpli-
cations, generating significant interest in the field of 3D scene comprehension.
This rapidly evolving domain holds promises for advancing human-robot inter-
action,metaversedevelopment,robotics,andembodiedintelligence.Twopivotal
abilities crucial to 3D environment dialogue systems include spatial perception
and logical reasoning. Recently, there has been a surge in tasks integrating 3D
scenes and languages, such as 3D captioning, question answering, situated Q
andA,embodieddialogue,planning,navigation,multi-turndialogueassistance,
object detection, and scene description. We categorize 3D perception task mod-
els into three groups (refer to Table 1, split by dashed lines). The first group
encompasses models handling tasks like 3D captioning, situated question an-
swering,andvisualgrounding[41,80].Thesemodelscangeneratesinglewordsor
phrasesastextualoutputs.Thesecondcategoryconsistsof3Dsemanticsegmen-
tation models producing 3D segmentation masks, such as 3DOVS [36], Open-
mask3D [53], OpenScene [44], and PLA [15], which perform open-vocabulary
semantic segmentation for 3D scenes. However, these methods do not offer con-
versational responses to user queries or provide reasoning for their tasks. The
third category comprises models employing LLMs to conduct visual perception
tasks like captioning, scene understanding, and visual grounding, offering con-
versational outputs [6,19,22,32,59,62,64]. Nonetheless, they lack fine-grained
semantic segmentation or reasoning-based 3D vision tasks.
3 Method
As illustrated in Fig. 2, Reasoning3D begins with a mesh input fed into the
renderer for viewpoint rendering, generating the face ID for each corresponding
viewpoint. Next, the rendered viewpoints and the user-input prompt are pro-
cessed by the pre-trained 2D reasoning segmentation network, which segments
theimagetoextractthedesiredpartsandoutputexplanations.Finally,usingthe
mapping relationship between each viewpoint and its corresponding mesh face
ID, the segmented parts are reconstructed back onto the mesh with a specially
designed multi-view fusion mechanism.
3.1 Multi-View Image Rendering and Face ID Generation.
Humaninteractionwiththe3Denvironmentofteninvolvesdynamicexploration,
amalgamatingviewpointsfromvariousanglestoconstructacohesive3Dcompre-
hension, rather than assimilating a 3D setting instantaneously. Our methodol-
ogy advocates for 3D reasoning cultivated from multi-perspective imagery. This
strategyalsoleveragestheextensive2Dpretrainingaccessibleinvision-language
models, akin to prior methodologies capitalizing on pre-trained vision-language
models for 3D visual tasks. The input for this process is mesh F = {f }N ,
n n=1
which is composed of N sets of faces f . During this rendering process, the 3D
n6 T, Chen et al.
Can you segment the part of a fish that helps it
maintain balance in the water in this image? Please
output the segmentation mask and explain why.
Multi-Modal
LLM LLooRRAA
Renderer ... Vision Decoder
3D Model Backbone
Muti-view Image
Segmentation Mask
N-Ring
Visibility Weights Per Face
Score
... ...
Aggregation The large goldfish is swimming
in the water with its dorsal fin visible,
Bounding Boxes Segmentation Gaussian which contributes to its ability to
maintain balance and navigate its
Geodesic Weights surroundings
Fig.2: The overview of Reasoning3D. First, a 3D model represented by 3D meshes
is fed into a renderer to obtain multi-view images. Then, each image goes through a
vision backbone and a multi-modal LLM along with user input queries. The decoder
decodes the final layer embedding which contains the extra token, thus producing
K segmentation masks. We also extract the bounding boxes in this stage. Finally, a
specially designed mask-to-3D segmentation algorithm elevates the projections back
into the 3D space.
model is converted into multiple 2D images X = {x }11 from various per-
img i i=1
spectives. In addition to generating these 2D images, the rendering process also
producescorrespondingfaceIDsforeachimage.ThesefaceIDsserveasacrucial
link between the 2D images and the original 3D mesh. Specifically, they form a
mapping matrix W that connects each pixel P = {p }Mm in the 2D images
pf i i=k
to a specific face f on the 3D mesh, ensuring ensures that the 2D and 3D data
n
remain accurately aligned. The process is formulated as follows:
N
(cid:88)
P =W f (1)
pf n
n=1
where P represents the pixels in the rendered images, and f represents the
n
facesofthe3Dmeshfromtheviewpoint.Denotedbyk,thefaceIDW connects
pf
the pixels P in the rendered image and the faces of the 3D mesh f visible in
n
the current view.
3.2 Reasoning and Segmenting with User Input Prompt
Unlikepreviousmethods(e.g.CLIPSeg[40],LSeg[29],andGLIP[31])whichcan
handleopen-vocabularyexplicitprompt,ourmethodaimstohandletheimplicit
promptsuchas"CanyousegmenttheappropriatepartsoftheimagecontainingReasoning3D - Grounding and Reasoning in 3D 7
a’cagedbird’?"Here,weleveragetherecentadvancesoflargefoundationmodels
to perform this multi-modal reasoning task.
FollowingLaietal.,[28],weextendtheoriginalLLMvocabularywithanew
token,<SEG>,whichdenotesarequestforsegmentationoutput.Giventheuser-
inputpromptquestionX andtheinputimageX ,theseareinputinto
question img
the multimodal large language model (LLM)F , which subsequently outputs
MM
the textual response Y . The process is formulated as:
answer
Y =F (X ,X ) (2)
answer MM img question
Next, the generation of segmentation masks corresponding to the input im-
ages involves a series of steps. Firstly, we extract the embedding Eˆ cor-
answer
responding to the <SEG> token from the output answer textual Y . This
answer
step enables us to capture information relevant to the segmentation task from
the language prompts. Subsequently, we process Eˆ through the MLP γ
answer
projection layer to obtain the feature vector E . Concurrently, utilizing a
answer
visual backbone network F , we extract visual embeddings E from the vi-
vb img
sualinputsX .Finally,wefeedboththefeaturevectorE andthevisual
img answer
embeddingsE intothedecoderF .ThedecoderF utilizesthesefeatures
img dec dec
to generate the final segmentation mask M and confidence scores S for each
M
mask. This yields segmentation results based on both language prompts and vi-
sualinformation,whereeachsegmentationmaskisaccompaniedbyitsrespective
confidencescoreandcorrespondinganswertextual.Thedetailedstructureofthe
decoder follows Segment Anything [25]. The process is formulated as follows:
E =γEˆ (3)
answer answer
E =F (X ) (4)
img vb img
M,S =F (E ,E ) (5)
M dec answer img
3.3 Mask Fusion and Refinement in 3D
The obtained 2D segmentation mask is eventually needed to be fused in 3D
space to obtain the desired 3D segmentation result. We find that the result
from directly merging the multi-view segmentation may not be coherent and
high-quality due to the accumulated error and lack of comprehensive multi-
view3Dinformation.Therefore,wedesignedamulti-stagefusionandrefinement
mechanism to fully exploit the semantic information and viewpoint information
to obtain better 3D segmentation results.
First,weusethetop-kmethodtofilterthegeneratedmaskstoreduceerrors
in 2D segmentation. Specifically, if the area difference between two masks is8 T, Chen et al.
greater than a certain threshold T, we select k=1, indicating that is the mask
(the most salient part) we want, we generate a bounding box that fits with the
mask;otherwise,weselectmultiplemasksandgeneratemultipleboundingboxes.
The filtered top-k masks S , the corresponding confidence scores S , and the
M M
face ID is then used as the input to the fusion algorithm. We use the mapping
relationship W that maps the 2D image mask regions onto the faces of the 3D
pf
mesh, resulting in an initial segmented mesh. Note that only the masks within
the generated bounding box are involved in the fusion process.
Following [1], we smooth and refine the segmentation boundaries, reducing
noise and errors with Gaussian Geodesic Reweighting. Subsequently, we apply
theVisibilitySmoothingtechniquetoeliminatediscontinuitiescausedbychanges
in viewpoints, ensuring that the segmented mesh appears natural and coherent
from all angles. Finally, we use a Global Filtering Strategy that filters out the
masked regions with low confidence scores.
Specifically, for each 2D mask M, we estimate its central face Gj, where i
i
denotes the view and j denotes the mask within the view. For the 3D mesh
under the current view, we retrieve the vertices of all faces corresponding to
the current mask and compute their area-weighted average. This average point
is then projected onto point F, and the face Fj containing this projection is
i
identified as the central face for the current view and mask. Subsequently, we
calculate the geodesic distance vector dj ∈ RN from the central face Gj to Fj
i i i
for all faces in f ∈ Fj. Here, N represents the number of faces in the mesh for
i
the current mask.
(cid:40)
gdist(Gj,f), if f ∈Fj
f(x)= i i (6)
0, otherwise
Where gdist(,) represents the geodesic length between two faces computed us-
ing a heat method [13] on mesh F. The geodesic distance between mesh faces
measures the path length along the surface from one face to another.
Next, we fit a Gaussian distribution on the distances and calculate the cor-
responding probability density values given the geodesic distances between each
face and the uppercase face.
rj =ξ[(d;µj,(σj)2),d∈dj] (7)
i i i i
Whereµj andσj representthemeanandstandarddeviationofthedistances
i i
to dj, respectively. Subsequently, we tally the number of times n each face in
i
the mesh is segmented in each view. Finally, we multiply the frequency of each
face by the corresponding probability density, and then by the corresponding
confidence score S , to obtain the final confidence for each mesh face.
M
However, using only the above method may result in insufficient weighting
around the central face Gj, especially in regions where the average distances
i
between faces are large. To address this issue, we use computes its local neigh-
borhood, where neighbors are determined by mesh connectivity: if two facesReasoning3D - Grounding and Reasoning in 3D 9
share at least one vertex, then face m is considered a neighbor of face n. To
achieve this, we construct a q-rank neighborhood N (n) (q = 5) as follows. For
q
facem∈F,ifthereexistsapathonthegraphconnectingmandnwithatmost
q otherverticesalongthepath,thenweincludefacen∈F intheneighborhood.
Finally, we adopt a global filtering using the calculated threshold. We fil-
ter out masked regions with low confidence scores. The threshold is the mean
confidence score calculated for every face.
4 Experiment
4.1 Experimental Setup
Dataset and Evaluation Metric: Since there are no existing zero-shot rea-
soning 3D segmentation benchmarks, we first evaluated the zero-shot open-
vocabulary segmentation performance on the FAUST [3] benchmark (an open-
vocabulary 3D segmentation benchmark) proposed in SATR [1]. We also vali-
dated the effectiveness of our method on reasoning 3D segmentation by our col-
lectedin-the-wilddatafromSketchFab.TheFAUSTdatasetconsistsofmanually
annotated registered meshes of human body scans, re-meshed independently for
eachscantocontainapproximately20Ktriangularfaces.Werandomlycollected
samples from the 3D modeling website SketchFab and asked human volunteers
to give implicit segmentation commands. For the evaluation metric, we employ
themeanIntersectionoverUnion(mIoU)forsemanticsegmentationasdescribed
in[67]forqualitativeevaluationforeachsemanticcategoryacrossalltestshapes
in open-vocabulary 3D segmentation. For the reasoning 3D segmentation, the
result is visualized and rated by the user.
Implementation Details: We utilized a single NVIDIA A100 GPU for each
setofexperiments.Duringtherenderingprocess,wecenteredtheinputmeshat
the origin and normalized it within a unit sphere. We evenly sample 8 images
horizontallyaroundall360degrees,maintainingconsistencyinviewpointsacross
allexperiments.Duringtherenderingprocess,weusedaresolutionof1024×1024
andsetauniformblackbackgroundcolor.Multiplereasons(orexplanations)will
begeneratedineachviewtogiveacomprehensiveunderstandingfortheobject,
and users can choose one as the desired answer.
Comparison Experiments for Open-Vocabulary Segmentation Since
there is no existing reasoning 3D segmentation approach that can be compared,
we first compared our method with existing open-vocabulary 3D segmentation
models such as SATR [1] and 3DHighlighter [14] following the protocol in [1]
but use the same rendering protocol in our method. As illustrated in Table. 1
and Table. 2 We show that though not designed for open-vocabulary segmenta-
tion tasks and without fine-tuning or specially designed structure, our method
achieves competitive performance in the open-vocabulary segmentation bench-
mark.10 T, Chen et al.
Table1:Performanceonthecoarse-grainedsemanticsegmentationonFAUSTdataset
Model Backbone Arm Head Leg Torso
3DHighlighter CLIP 28.60 14.20 14.90 8.20
SATR GLIP 61.54 76.8987.4152.32
Ours LISA 64.65 72.60 83.58 50.39
Table 2: Performance on the fine-grained semantic segmentation on FAUST dataset
Model Arm Belly button Chin Ear Elbow Eye Foot Forehead Hand
3DHighlighter 18.39 1.99 0.46 0.72 0.08 0 20.81 0.70 0.02
SATR 24.23 22.00 26.53 34.55 33.67 22.55 75.20 30.35 75.11
Ours 26.47 1.87 3.36 10.61 18.18 2.77 71.85 6.56 43.15
Head Knee Leg Mouth Neck Nose Shoulder Torso
3DHighlighter 3.49 6.17 3.91 0.05 1.94 0.07 0.04 7.28
SATR 40.31 46.96 56.5 20.46 22.01 37.41 24.41 50.52
Ours 39.81 13.95 62.23 4.12 11.88 5.5 9.6 48.78
4.2 Performance in Reasoning 3D Segmentation
A better property that our method has compared to existing open-vocabulary
segmentationisthatourmethodcanusenaturallanguageastheinputinforma-
tion. The LLM parses the natural language and gives the segmentation result
directly, which enables a more natural and convenient computer-human interac-
tionexperience.AnexampleisshowninFig.4.ThemodelsarefromtheFAUST
dataset.
In the open-vocabulary segmentation, only explicit segmentation command
isgiven,inwhichReasoning3D’spotentialhasnotbeenfullyexploited.Weran-
domly collect 3D models from the 3D modeling website SketchFab perform the
assessmentwiththesein-the-wild3Dmodelsandlethumanvolunteersgive“im-
plicit" segmentation commands. Figure. 6 and fig. 1 shows some examples. The
examplesshowthatReasoning3Dhasthecapabilitiestoofferin-depthreasoning,
3D understanding, part segmentation, and conversational abilities. The model
can output the segmentation masks and the explanation as we desire.
To better allow users to interact with our system, we also designed a User
Interface (UI) so that users can input arbitrary 3D models and their desired
prompttosegmentthedesiredregion.(Fig.6)ThisUIwillalsobeopen-sourced.
5 Discussion and Limitations
This research represents preliminary findings in the task of reasoning 3D seg-
mentation, and several areas require further exploration and validation. One
majoraspectistheneedforcomprehensivebenchmarkingtorigorouslyevaluate
our method’s performance. Additionally, conducting user studies will provide
valuable insights into the practical applicability and usability of our approach.Reasoning3D - Grounding and Reasoning in 3D 11
GT 3DHighlighter SATR Our GT 3DHighlighter SATR Our
Arm Torso
Head Leg
Fig.3: Qualitative results and comparison between our method and baseline method
in FAUST benchmark. The segmented regions are shown in red.
Can you segment the man's arms and hand in Can you segment the man's head in this Can you segment the abdomen and the foot Can you segment the man's torso area
this image? image? and the butt and the area below the and the abdomen area and not the neck
abdomen in this image? and not the arms in this image?
Fig.4:Anaturallanguagecommandcanmakethemodelsegmentthedesiredregions.
The segmented regions are shown in red.
Our findings indicate that view information plays a critical role in the per-
formance of 3D segmentation tasks. Optimizing view selection to align with the
pre-trained vision encoder could significantly enhance outcomes. This suggests
that a strategic approach to view selection is essential for leveraging the full
potential of the pre-trained models.
The flexibility of our method is noteworthy, as the LVLM can perform zero-
shot inference without the need for additional training. While fine-tuning with
datacouldpotentiallyimproveperformance,weobservedthatfine-tuningwitha
very small dataset might negatively impact the network’s generalization ability,
sometimes resulting in poorer performance compared to fine-tuning. It is also
worth noting that our multi-view 2D segmentation and 3D projection method
canbeappliedtoscenes,whichwillbebeneficialformorereal-worldapplications.
Tofosterfurtheradvancementsandcollaborativeinnovationin3Dreasoning
and segmentation, we are releasing our code. We encourage the community to
build upon our work and develop improved methods.12 T, Chen et al.
Fig.5: We offer a user-friendly interface designed for performance assessment, facili-
tatingtheeasyuploadof3Dmodelsandpromptsbyusers.Itenablesswiftacquisition
of 3D segmentation outcomes. This tailored software is available as open-source.
What part of a fish can move it forward quickly in this Can you segment the part that is essential if you want to hear
image? Please output segmentation mask and explain why. others speak in this image? Please output segmentation mask
and explain why.
In the picture, the part of the fish that allows it to move forward
quickly is its tail. The tail is an essential part of a fish's anatomy In the picture, the essential part for
that helps it swim efficiently through the water. By flexing its hearing others speaking is the
tail back and forth, the fish can generate force and propel itself phone handset, which is the grey
forward, enabling it to move quickly through the water. object shaped like a house that is
connected to the phone. The
handset is the part of the phone
that captures your voice and
allows you to hear the person on
the other end.
Can you segment the supporting part in this image? Please
Can you segment the supporting part and explain why?
output segmentation mask and explain why.
In the image, the table is a rectangular wooden dining table The segmentation result is <SEG>. In the image, the bed is a
with a flat top and four legs for support. The legs of the table metal bed frame with a mattress on it. The bed is placed on a
can be seen at the corners, providing stability and structure to hardwood floor
the table.
Fig.6: ThisfigureshowsReasoning3D’sabilitytosegment3Dobjectparts(inafine-
grainedmanner)fromin-the-wildsamples,includingreal-worldscanneddata(samples
arerandomlycollectedfromSketchFab).TheseexampleshighlightReasoning3D’sad-
vanced capabilities in in-depth reasoning, comprehensive 3D understanding, precise
partsegmentation,androbustconversationalabilities.Theoriginalmeshandtheseg-
mentation result are visualized, and the segmented region is highlighted in Red.Reasoning3D - Grounding and Reasoning in 3D 13
6 Conclusion
This paper introduces a new task: Zero-Shot 3D Reasoning Segmentation for
part searching and localization within objects. This new approach moves be-
yond the constraints of traditional category-specific 3D semantic segmentation,
3D instance segmentation, and open-vocabulary 3D segmentation. We have de-
velopedReasoning3D,asimpleyeteffectivebaselinemethodthatcanunderstand
and perform complex commands to segment specific parts of 3D meshes with
contextual understanding and reasoned outputs for interactive segmentation.
Reasoning3Dleveragespre-trained2Dsegmentationnetworksinconjunction
with Large Language Models (LLMs) to interpret user queries in a zero-shot
manner.Previousstudieshaveshownthatextensivepre-trainingequipsfounda-
tionalmodelswithabroadunderstandingoftheworld,enablingthemtoprocess
complex commands. Our method harnesses this capability, allowing for effective
3D segmentation with limited 3D datasets, making it a resource-efficient solu-
tion.
OurexperimentsdemonstratethatReasoning3Disgeneralizableandcapable
of accurately localizing and identifying parts of 3D objects based on implicit
textualqueries.Thisincludesbotharticulated3Dobjectsandreal-worldscanned
data. Additionally, our method can produce natural language explanations for
thesegmented3Dmodelsandtheircomponents.Thetraining-freenatureofour
approach facilitates rapid deployment and provides a robust baseline for future
research in part-level 3D object understanding. This has potential applications
across various domains, such as robotics, object manipulation, part assembly,
autonomousdriving,augmentedandvirtualreality(AR/VR),andmedicalfields.
We are releasing the code, model weights, deployment guide, and evaluation
protocol to encourage further innovation and collaboration. These resources are
available at: http://tianrun-chen.github.io/Reason3D/.
References
1. Abdelreheem, A., Skorokhodov, I., Ovsjanikov, M., Wonka, P.: Satr: Zero-shot
semantic segmentation of 3d shapes (2023)
2. Bain, M., Nagrani, A., Varol, G., Zisserman, A.: Frozen in time: A joint video
and image encoder for end-to-end retrieval. In: 2021 IEEE/CVF International
Conference on Computer Vision (ICCV) (Oct 2021). https://doi.org/10.1109/
iccv48922.2021.00175, http://dx.doi.org/10.1109/iccv48922.2021.00175
3. Bogo, F., Romero, J., Loper, M., Black, M.J.: Faust: Dataset and evaluation for
3d mesh registration. In: 2014 IEEE Conference on Computer Vision and Pattern
Recognition (Jun 2014). https://doi.org/10.1109/cvpr.2014.491, http://dx.
doi.org/10.1109/cvpr.2014.491
4. Changpinyo, S., Sharma, P., Ding, N., Soricut, R.: Conceptual 12m: Pushing
web-scale image-text pre-training to recognize long-tail visual concepts. In: 2021
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
(Jun 2021). https://doi.org/10.1109/cvpr46437.2021.00356, http://dx.doi.
org/10.1109/cvpr46437.2021.0035614 T, Chen et al.
5. Charles,R.Q.,Su,H.,Kaichun,M.,Guibas,L.J.:Pointnet:Deeplearningonpoint
setsfor3dclassificationandsegmentation.In:2017IEEEConferenceonComputer
Vision and Pattern Recognition (CVPR) (Jul 2017). https://doi.org/10.1109/
cvpr.2017.16, http://dx.doi.org/10.1109/cvpr.2017.16
6. Chen,S.,Chen,X.,Zhang,C.,Li,M.,Yu,G.,Fei,H.,Zhu,H.,Fan,J.,Chen,T.:
Ll3da:Visualinteractiveinstructiontuningforomni-3dunderstanding,reasoning,
and planning (2023)
7. Chen, T., Cao, R., Li, Z., Zang, Y., Sun, L.: Deep3dsketch-im: rapid high-fidelity
ai3dmodelgenerationbysinglefreehandsketches.FrontiersofInformationTech-
nology & Electronic Engineering 25(1), 149–159 (2024)
8. Chen, T., Ding, C., Zhu, L., Zang, Y., Liao, Y., Li, Z., Sun, L.: Reality3dsketch:
Rapid 3d modeling of objects from single freehand sketches. arXiv preprint
arXiv:2310.18148 (2023)
9. Chen, T., Fu, C., Zang, Y., Zhu, L., Zhang, J., Mao, P., Sun, L.: Deep3dsketch+:
Rapid3dmodelingfromsinglefree-handsketches.In:InternationalConferenceon
Multimedia Modeling. pp. 16–28. Springer (2023)
10. Chen, T., Fu, C., Zhu, L., Mao, P., Zhang, J., Zang, Y., Sun, L.: Deep3dsketch:
3d modeling from free-hand sketches with view-and structural-aware adversarial
training. In: ICASSP. pp. 1–5. IEEE (2023)
11. Chen, T., Zhu, L., Deng, C., Cao, R., Wang, Y., Zhang, S., Li, Z., Sun, L., Zang,
Y., Mao, P.: Sam-adapter: Adapting segment anything in underperformed scenes.
In: Proceedings of the IEEE/CVF International Conference on Computer Vision.
pp. 3367–3375 (2023)
12. Choy,C.,Gwak,J.,Savarese,S.:4dspatio-temporalconvnets:Minkowskiconvolu-
tional neural networks. In: 2019 IEEE/CVF Conference on Computer Vision and
PatternRecognition(CVPR)(Jun2019).https://doi.org/10.1109/cvpr.2019.
00319, http://dx.doi.org/10.1109/cvpr.2019.00319
13. Crane, K., Weischedel, C., Wardetzky, M.: The heat method for distance compu-
tation. Communications of the ACM p. 90âĂŞ99 (Oct 2017). https://doi.org/
10.1145/3131280, http://dx.doi.org/10.1145/3131280
14. Decatur,D.,Lang,I.,Hanocka,R.:3dhighlighter:Localizingregionson3dshapes
via text descriptions (2022)
15. Ding, R., Yang, J., Xue, C., Zhang, W., Bai, S., Qi, X.: Language-driven open-
vocabulary 3d scene understanding (Nov 2022)
16. Fu, X., Zhang, S., Chen, T., Lu, Y., Zhou, X., Geiger, A., Liao, Y.: Panopticnerf-
360: Panoramic 3d-to-2d label transfer in urban scenes (2023)
17. Fu,X.,Zhang,S.,Chen,T.,Lu,Y.,Zhu,L.,Zhou,X.,Geiger,A.,Liao,Y.:Panoptic
nerf: 3d-to-2d label transfer for panoptic urban scene segmentation (2022)
18. Graham,B.,Engelcke,M.,Maaten,L.v.d.:3dsemanticsegmentationwithsubman-
ifoldsparseconvolutionalnetworks.In:2018IEEE/CVFConferenceonComputer
Vision and Pattern Recognition (Jun 2018). https://doi.org/10.1109/cvpr.
2018.00961, http://dx.doi.org/10.1109/cvpr.2018.00961
19. Guo,Z.,Zhang,R.,Zhu,X.,Tang,Y.,Ma,X.,Han,J.,Chen,K.,Gao,P.,Li,X.,Li,
H., Heng, P.A.: Point-bind & point-llm: Aligning point cloud with multi-modality
for 3d understanding, generation, and instruction following
20. Hong,Y.,Du,Y.,Lin,C.,Tenenbaum,J.,Gan,C.:3dconceptgroundingonneural
fields (Jul 2022)
21. Hong,Y.,Lin,C.,Du,Y.,Chen,Z.,Tenenbaum,J.,Gan,C.,Ucla,U.:3dconcept
learning and reasoning from multi-view images
22. Hong,Y.,Zhen,H.,Chen,P.,Zheng,S.,Du,Y.,Chen,Z.,Gan,C.:3d-llm:Injecting
the 3d world into large language modelsReasoning3D - Grounding and Reasoning in 3D 15
23. Huang,Q.,Wang,W.,Neumann,U.:Recurrentslicenetworksfor3dsegmentation
ofpointclouds.In:2018IEEE/CVFConferenceonComputerVisionandPattern
Recognition (Jun 2018). https://doi.org/10.1109/cvpr.2018.00278, http://
dx.doi.org/10.1109/cvpr.2018.00278
24. Jones, R.K., Habib, A., Ritchie, D.: Shred: 3d shape region decomposition with
learned local operations (2022)
25. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T.,
Whitehead,S.,Berg,A.C.,Lo,W.Y.,DollÃąr,P.,Girshick,R.:Segmentanything
(2023)
26. Kundu,A.,Yin,X.,Fathi,A.,Ross,D.,Brewington,B.,Funkhouser,T.,Pantofaru,
C.: Virtual multi-view fusion for 3d semantic segmentation. Cornell University -
arXiv,Cornell University - arXiv (Jul 2020)
27. Lai, X., Liu, J., Jiang, L., Wang, L., Zhao, H., Liu, S., Qi, X., Jia, J.: Stratified
transformer for 3d point cloud segmentation
28. Lai, X., Tian, Z., Chen, Y., Li, Y., Yuan, Y., Liu, S., Jia, J., Kong, H., Research,
M.: Lisa: Reasoning segmentation via large language model
29. Li, B., Weinberger, K.Q., Belongie, S., Koltun, V., Ranftl, R.: Language-driven
semantic segmentation (2022)
30. Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large language models
31. Li, L.H., Zhang, P., Zhang, H., Yang, J., Li, C., Zhong, Y., Wang, L., Yuan, L.,
Zhang, L., Hwang, J.N., Chang, K.W., Gao, J.: Grounded language-image pre-
training (2022)
32. Li, M., Chen, X., Zhang, C., Chen, S., Zhu, H., Yin, F., Yu, G., Chen, T.:
M3dbench: Let’s instruct large models with multi-modal 3d prompts (2023)
33. Lin, J., Yang, A., Bai, J., Zhou, C., Jiang, L., Jia, X., Wang, A., Zhang, J., Li,
Y.,Lin,W.,Zhou,J.,Yang,H.:M6-10t:Asharing-delinkingparadigmforefficient
multi-trillion parameter pretraining (2021)
34. Lin,T.Y.,Maire,M.,Belongie,S.,Hays,J.,Perona,P.,Ramanan,D.,DollÃąr,P.,
Zitnick, C.L.: Microsoft COCO: Common Objects in Context, p. 740âĂŞ755 (Jan
2014). https://doi.org/10.1007/978-3-319-10602-1_48, http://dx.doi.org/
10.1007/978-3-319-10602-1_48
35. Liu, H., Li, C., Wu, Q., Lee, Y., Madison, M., Research, M.: Visual instruction
tuning
36. Liu, K., Zhan, F., Zhang, J., Xu, M., Yu, Y., Saddik, A., Theobalt, C., Xing, E.,
Lu, S.: Weakly supervised 3d open-vocabulary segmentation (Sep 2023)
37. Liu,M.,Zhu,Y.,Cai,H.,Han,S.,Ling,Z.,Porikli,F.,Su,H.:Partslip:Low-shot
partsegmentationfor3dpointcloudsviapretrainedimage-languagemodels(2023)
38. Liu, Z., He, Y., Wang, W., Wang, W., Wang, Y., Chen, S., Zhang, Q., Lai, Z.,
Yang, Y., Li, Q., Yu, J., Li, K., Chen, Z., Yang, X., Zhu, X., Wang, Y., Wang,
L.,Luo,P.,Dai,J.,Qiao,Y.:Interngpt:Solvingvision-centrictasksbyinteracting
with chatgpt beyond language (2023)
39. Lyu, Y., Huang, X., Zhang, Z.: Learning to segment 3d point clouds in 2d image
space (2020)
40. LÃĳddecke, T., Ecker, A.S.: Image segmentation using text and image prompts
(2022)
41. Ma,X.,Yong,S.,Zheng,Z.,Li,Q.,Liang,Y.,Zhu,S.C.,Huang,S.:Sqa3d:Situated
question answering in 3d scenes (Oct 2022)
42. Mascaro,R.,Teixeira,L.,Chli,M.:Diffuser:Multi-view2d-to-3dlabeldiffusionfor
semanticscenesegmentation.In:2021IEEEInternationalConferenceonRobotics16 T, Chen et al.
and Automation (ICRA) (May 2021). https://doi.org/10.1109/icra48506.
2021.9561801, http://dx.doi.org/10.1109/icra48506.2021.9561801
43. Miech, A., Zhukov, D., Alayrac, J.B., Tapaswi, M., Laptev, I., Sivic, J.:
Howto100m: Learning a text-video embedding by watching hundred million nar-
rated video clips. In: 2019 IEEE/CVF International Conference on Computer Vi-
sion (ICCV) (Oct 2019). https://doi.org/10.1109/iccv.2019.00272, http:
//dx.doi.org/10.1109/iccv.2019.00272
44. Peng, S., Genova, K., Jiang, C., Tagliasacchi, A., Pollefeys, M., Funkhouser, T.:
Openscene: 3d scene understanding with open vocabularies (Nov 2022)
45. Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S.: Kosmos-2: Grounding
multimodal large language models to the world
46. Pi, R., Gao, J., Diao, S., Pan, R., Dong, H., Zhang, J., Yao, L., Han, J., Xu, H.,
Kong, L., Zhang, T.: Detgpt: Detect what you need via reasoning
47. Robert, D., Vallet, B., Landrieu, L.: Learning multi-view aggregation in the wild
for large-scale 3d semantic segmentation (Apr 2022)
48. Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Aarush,
K.,Theo,C.,Jitsev,J.,Komatsuzaki,A.:Laion-400m:Opendatasetofclip-filtered
400millionimage-textpairs.CornellUniversity-arXiv,CornellUniversity-arXiv
(Nov 2021)
49. Schuhmann,C.,Beaumont,Â.,Vencu,V.,Gordon,A.,Wightman,W.,Cherti,M.,
Coombes,T.,Katta,A.,Mullis,C.,Schramowski,P.,Kundurthy,S.,Crowson,K.,
Schmidt, L., Kaczmarczyk, R., Jitsev, Â., Berkeley, U., Data, G.: Laion-5b: An
open large-scale dataset for training next generation image-text models
50. Sharma, P., Ding, N., Goodman, S., Soricut, R.: Conceptual captions: A cleaned,
hypernymed, image alt-text dataset for automatic image captioning. In: Proceed-
ingsofthe56thAnnualMeetingoftheAssociationforComputationalLinguistics
(Volume 1: Long Papers) (Jan 2018). https://doi.org/10.18653/v1/p18-1238,
http://dx.doi.org/10.18653/v1/p18-1238
51. Shen, Y., Song, K., Tan, X., Li, D., Lu, W., Zhuang, Y.: Hugginggpt: Solving ai
tasks with chatgpt and its friends in hugging face (2023)
52. Shi,Y.,Wang,P.,Ye,J.,Long,M.,Li,K.,Yang,X.:Mvdream:Multi-viewdiffusion
for 3d generation (2023)
53. Takmaz, A., Fedele, E., Sumner, R., Pollefeys, M., Tombari, F., Engelmann, F.:
Openmask3d: Open-vocabulary 3d instance segmentation (Jun 2023)
54. Tang, J., Wang, T., Zhang, B., Zhang, T., Yi, R., Ma, L., Chen, D.: Make-it-3d:
High-fidelity 3d creation from a single image with diffusion prior (2023)
55. Thomas, H., Qi, C.R., Deschaud, J.E., Marcotegui, B., Goulette, F., Guibas, L.:
Kpconv:Flexibleanddeformableconvolutionforpointclouds.In:2019IEEE/CVF
International Conference on Computer Vision (ICCV) (Oct 2019). https://doi.
org/10.1109/iccv.2019.00651, http://dx.doi.org/10.1109/iccv.2019.00651
56. Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao,
L., Song, X., Xu, J., Xu, B., Li, J., Dong, Y., Ding, M., Tang, J.: Cogvlm: Visual
expert for pretrained language models (2024)
57. Wang,W.,Chen,Z.,Chen,X.,Wu,J.,Zhu,X.,Zeng,G.,Luo,P.,Lu,T.,Zhou,J.,
Qiao, Y., Dai, J.: Visionllm: Large language model is also an open-ended decoder
for vision-centric tasks
58. Wang, Y., Cheng, J., Chen, Y., Shao, S., Zhu, L., Wu, Z., Liu, T., Zhu, H.: Fvp:
Fourier visual prompting for source-free unsupervised domain adaptation of med-
ical image segmentation. IEEE Transactions on Medical Imaging (2023)
59. Wang, Z., Huang, H., Zhao, Y., Zhang, Z., Zhao, Z.: Chat-3d: Data-efficiently
tuning large language model for universal dialogue of 3d scenes (2023)Reasoning3D - Grounding and Reasoning in 3D 17
60. Xiang, J., Yang, J., Huang, B., Tong, X.: 3d-aware image generation using 2d
diffusion models. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV). pp. 2383–2393 (October 2023)
61. Xu, M., Ding, R., Zhao, H., Qi, X.: Paconv: Position adaptive convolution with
dynamic kernel assembling on point clouds. In: 2021 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) (Jun 2021). https://doi.
org/10.1109/cvpr46437.2021.00319, http://dx.doi.org/10.1109/cvpr46437.
2021.00319
62. Xu, R., Wang, X., Wang, T., Chen, Y., Pang, J., Lin, D.: Pointllm: Empowering
large language models to understand point clouds (2023)
63. Xue, Y., Chen, N., Liu, J., Sun, W.: Zerops: High-quality cross-modal knowledge
transfer for zero-shot 3d part segmentation (2023)
64. Yang,J.,Chen,X.,Qian,S.,Madaan,N.,Iyengar,M.,Fouhey,D.,Chai,J.:Llm-
grounder: Open-vocabulary 3d visual grounding with large language model as an
agent (Sep 2023)
65. Yang,S.,Qu,T.,Lai,X.,Tian,Z.,Peng,B.,Liu,S.,Jia,J.:Lisa++:Animproved
baseline for reasoning segmentation with large language model (2024)
66. Ying, H., Yin, Y., Zhang, J., Wang, F., Yu, T., Huang, R., Fang, L.: Omniseg3d:
Omniversal 3d segmentation via hierarchical contrastive learning (2023)
67. Yu,F.,Liu,K.,Zhang,Y.,Zhu,C.,Xu,K.:Partnet:Arecursivepartdecomposition
network for fine-grained and hierarchical shape segmentation (2022)
68. Yu, F., Qian, Y., Gil-Ureta, F., Jackson, B., Bennett, E., Zhang, H.: Hal3d: Hier-
archical active learning for fine-grained 3d part labeling (2024)
69. Zang,Y.,Ding,C.,Chen,T.,Mao,P.,Hu,W.:Deep3dsketch+\+:High-fidelity3d
modeling from single free-hand sketches. arXiv preprint arXiv:2310.18178 (2023)
70. Zang, Y., Fu, C., Cao, R., Zhu, D., Zhang, M., Hu, W., Zhu, L., Chen, T.: Res-
match: Referring expression segmentation in a semi-supervised manner. arXiv
preprint arXiv:2402.05589 (2024)
71. Zang, Y., Fu, C., Chen, T., Hu, Y., Liu, Q., Hu, W.: Deep3dsketch+: Obtain-
ing customized 3d model by single free-hand sketch through deep learning. arXiv
preprint arXiv:2310.18609 (2023)
72. Zhang,S.,Peng,S.,Chen,T.,Mou,L.,Lin,H.,Yu,K.,Liao,Y.,Zhou,X.:Painting
3d nature in 2d: View synthesis of natural scenes from a single semantic mask
(2023)
73. Zhang, S., Sun, P., Chen, S., Xiao, M., Shao, W., Zhang, W., Chen, K., Luo, P.:
Gpt4roi: Instruction tuning large language model on region-of-interest
74. Zheng,K.,He,X.,Wang,X.E.:Minigpt-5:Interleavedvision-and-languagegener-
ation via generative vokens (2024)
75. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-
language understanding with advanced large language models
76. Zhu, L., Chen, T., Ji, D., Ye, J., Liu, J.: Llafs: When large language models meet
few-shot segmentation (2024)
77. Zhu, L., Chen, T., Yin, J., See, S., Liu, J.: Continual semantic segmentation with
automaticmemorysampleselection.In:ProceedingsoftheIEEE/CVFConference
on Computer Vision and Pattern Recognition. pp. 3082–3092 (2023)
78. Zhu, L., Ji, D., Chen, T., Xu, P., Ye, J., Liu, J.: Ibd: Alleviating hallucina-
tions in large vision-language models via image-biased decoding. arXiv preprint
arXiv:2402.18476 (2024)
79. Zhu, L., Ji, D., Zhu, S., Gan, W., Wu, W., Yan, J.: Learning statistical texture
forsemanticsegmentation.In:ProceedingsoftheIEEE/CVFConferenceonCom-
puter Vision and Pattern Recognition. pp. 12537–12546 (2021)18 T, Chen et al.
80. Zhu,Z.,Ma,X.,Chen,Y.,Deng,Z.,Huang,S.,Li,Q.:3d-vista:Pre-trainedtrans-
former for 3d vision and text alignment (Aug 2023)