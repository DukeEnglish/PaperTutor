PublishedasaconferencepaperatICLR2024
MATRIX MANIFOLD NEURAL NETWORKS++
XuanSonNguyen,ShuoYang,AymericHistace
ETIS,UMR8051,CYCergyParisUniversity,ENSEA,CNRS,France
{xuan-son.nguyen,shuo.yang,aymeric.histace}@ensea.fr
ABSTRACT
Deepneuralnetworks(DNNs)onRiemannianmanifoldshavegarneredincreasing
interestinvariousappliedareas. Forinstance,DNNsonsphericalandhyperbolic
manifoldshavebeendesignedtosolveawiderangeofcomputervisionandnature
languageprocessingtasks. Oneofthekeyfactorsthatcontributetothesuccessof
thesenetworksisthatsphericalandhyperbolicmanifoldshavetherichalgebraic
structures of gyrogroups and gyrovector spaces. This enables principled and ef-
fectivegeneralizationsofthemostsuccessfulDNNstothesemanifolds. Recently,
someworkshaveshownthatmanyconceptsinthetheoryofgyrogroupsandgy-
rovector spaces can also be generalized to matrix manifolds such as Symmetric
Positive Definite (SPD) and Grassmann manifolds. As a result, some building
blocksforSPDandGrassmannneuralnetworks,e.g.,isometricmodelsandmulti-
nomiallogisticregression(MLR)canbederivedinawaythatisfullyanalogous
to their spherical and hyperbolic counterparts. Building upon these works, we
design fully-connected (FC) and convolutional layers for SPD neural networks.
We also develop MLR on Symmetric Positive Semi-definite (SPSD) manifolds,
andproposeamethodforperformingbackpropagationwiththeGrassmannloga-
rithmicmapintheprojectorperspective. Wedemonstratetheeffectivenessofthe
proposedapproachinthehumanactionrecognitionandnodeclassificationtasks.
1 INTRODUCTION
In recent years, deep neural networks on Riemannian manifolds have achieved impressive perfor-
manceinmanyapplications(Ganeaetal.,2018;Skopeketal.,2020;Cruceruetal.,2021;Shimizu
etal.,2021). Themostpopularneuralnetworksinthisfamilyoperateonhyperbolicspaces. Such
spaces of constant sectional curvature, like spherical spaces, have the rich algebraic structure of
gyrovector spaces. The theory of gyrovector spaces (Ungar, 2002; 2005; 2014) offers an elegant
andpowerfulframeworkbasedonwhichnaturalgeneralizations(Ganeaetal.,2018;Shimizuetal.,
2021)ofessentialbuildingblocksinDNNsareconstructedforhyperbolicneuralnetworks(HNNs).
MatrixmanifoldssuchasSPDandGrassmannmanifoldsofferaconvenienttrade-offbetweenstruc-
turalrichnessandcomputationaltractability(Cruceruetal.,2021;Lo´pezetal.,2021). Therefore,in
manyapplications,neuralnetworksonmatrixmanifoldsareattractivealternativestotheirhyperbolic
counterparts. However, unlike the approaches in Ganea et al. (2018); Shimizu et al. (2021), most
existing approaches for building SPD and Grassmann neural networks (Dong et al., 2017; Huang
&Gool,2017;Huangetal.,2018;Nguyenetal.,2019;Brooksetal.,2019;Nguyen,2021;Wang
etal.,2021)donotprovidenecessarytechniquesandmathematicaltoolstogeneralizeabroadclass
ofDNNstotheconsideredmanifolds.
Recently, theauthorsofKim(2020);Nguyen(2022b)haveshownthatSPDandGrassmannman-
ifolds have the structure of gyrovector spaces or that of nonreductive gyrovector spaces (Nguyen,
2022b)thatshareremarkableanalogieswithgyrovectorspaces.TheworkinNguyen&Yang(2023)
takesonestepforwardinthatdirectionbygeneralizingseveralnotionsingyrovectorspaces,e.g.,the
innerproductandgyrodistance(Ungar,2014)toSPDandGrassmannmanifolds. Thisallowsoneto
characterizecertaingyroisometriesofthesemanifoldsandtoconstructMLRonSPDmanifolds.
AlthoughsomeusefulnotionsingyrovectorspaceshavebeengeneralizedtoSPDandGrassmann
manifolds (Nguyen, 2022a;b; Nguyen & Yang, 2023) that set the stage for an effective way of
buildingneuralnetworksonthesemanifolds,manyquestionsremainopen. Inthispaper,weaimat
1
4202
yaM
92
]LM.tats[
1v60291.5042:viXraPublishedasaconferencepaperatICLR2024
addressingsomelimitationsofexistingworksusingagyrovectorspaceapproach. Ourcontributions
canbesummarizedasfollows:
1. WegeneralizeFCandconvolutionallayerstotheSPDmanifoldsetting.
2. We propose a method for performing backpropagation with the Grassmann logarithmic
mapintheprojectorperspective(Bendokatetal.,2020)withoutresortingtoanyapproxi-
mationschemes. Wethenshowhowtoconstructgraphconvolutionalnetworks(GCNs)on
Grassmannmanifolds.
3. WedevelopMLRonSPSDmanifolds.
4. Weshowcaseourapproachinthehumanactionrecognitionandnodeclassificationtasks.
2 PRELIMINARIES
2.1 SPDMANIFOLDS
Thespaceofn×nSPDmatrices,whenprovidedwithsomegeometricstructureslikeaRiemannian
metric,formsSPDmanifoldSym+ (Arsignyetal.,2005). DatalyingonSPDmanifoldsarecom-
n
monlyencounteredinvariousdomains(Huang&Gool,2017;Brooksetal.,2019;Nguyen,2021;
Sukthankeretal.,2021;Nguyen,2022b;a;Nguyen&Yang,2023). Inmanyapplications,theuseof
EuclideancalculusonSPDmanifoldsoftenleadstounsatisfactoryresults(Arsignyetal.,2005). To
tacklethisissue,manyRiemannianstructuresforSPDmanifoldshavebeenintroduced.Inthiswork,
wefocus ontwo widelyused Riemannianmetrics, i.e., Affine-Invariant(AI)(Pennec etal., 2004)
andLog-Euclidean(LE)(Arsignyetal.,2005)metrics,andarecentlyintroducedRiemannianmet-
rics, i.e. Log-Cholesky (LC) metrics (Lin, 2019) that offer some advantages over Affine-Invariant
andLog-Euclideanmetrics.
2.2 GRASSMANNMANIFOLDS
GrassmannmanifoldsGr arethecollectionoflinearsubspacesoffixeddimensionpoftheEu-
n,p
clideanspaceRn(Edelmanetal.,1998).DatalyingonGrassmannmanifoldsarisenaturallyinmany
applications(Absiletal.,2007;Bendokatetal.,2020). PointsonGrassmannmanifoldscanberep-
resentedfromdifferentperspectives(Bendokatetal.,2020). Twotypicalapproachesuseprojection
matricesorthosewithorthonormalcolumns. Eachofthemcanbeeffectiveinsomeproblemsbut
mightbeinappropriateinsomeothercontexts(Nguyen,2022b). Althoughgeometricaldescriptions
of Grassmann manifolds have been given in numerous works (Edelman et al., 1998), some com-
putational issues remain to be addressed. For instance, the question of how to effectively perform
backpropagationwiththeGrassmannlogarithmicmapintheprojectorperspectiveremainsopen.
2.3 NEURALNETWORKSONSPDANDGRASSMANNMANIFOLDS
2.3.1 NEURALNETWORKSONSPDMANIFOLDS
TheworkinHuang&Gool(2017)introducesSPDNetwiththreenovellayers,i.e.,Bimap,LogEig,
and ReEig layers that has become one of the most successful architectures in the field. In Brooks
etal.(2019),theauthorsfurtherimproveSPDNetbydevelopingRiemannianversionsofbatchnor-
malizationlayers. Followingtheseworks,someworks(Nguyenetal.,2019;Nguyen,2021;Wang
etal.,2021;Kobleretal.,2022;Ju&Guan,2023)designvariantsofBimapandbatchnormalization
layersinSPDneuralnetworks. TheworkinChakrabortyetal.(2020)presentsadifferentapproach
basedonintrinsicoperationsonSPDmanifolds. Theirproposedlayershavenicetheoreticalprop-
erties. Acommonlimitationoftheaboveworksisthattheydonotprovidenecessarymathematical
toolsforconstructingmanyessentialbuildingblocksofDNNsonSPDmanifolds. Recently,some
works (Nguyen, 2022a;b; Nguyen & Yang, 2023) take a gyrovector space approach that enables
naturalgeneralizationsofsomebuildingblocksofDNNs,e.g.,MLRforSPDneuralnetworks.
2.3.2 NEURALNETWORKSONGRASSMANNMANIFOLDS
InHuangetal.(2018),theauthorsproposeGrNetthatexploresthesameruleofmatrixbackprop-
agation (Ionescu et al., 2015) as SPDNet. Some existing works (Wang & Wu, 2020; Souza et al.,
2PublishedasaconferencepaperatICLR2024
2020) are also inspired by GrNet. Like their SPD counterparts, most existing Grassmann neural
networksarenotbuiltuponamathematicalframeworkthatallowsonetogeneralizeabroadclass
ofDNNstoGrassmannmanifolds. Usingagyrovectorspaceapproach,Nguyen&Yang(2023)has
shownthatsomeconceptsinEuclideanspacescanbenaturallyextendedtoGrassmannmanifolds.
3 PROPOSED APPROACH
3.1 NOTATION
Let M be a homogeneous Riemannian manifold, T M be the tangent space of M at P ∈ M.
P
Denote by exp(P) and log(P) the usual matrix exponential and logarithm of P, Exp (W) the
P
exponentialmapatPthatassociatestoatangentvectorW ∈ T MapointofM, Log (Q)the
P P
logarithmic map of Q ∈ M at P, T (W) the parallel transport of W from P to Q along
P→Q
geodesicsconnectingPandQ. Forsimplicityofexposition,wewillconcentrateonrealmatrices.
DenotebyM thespaceofn×mmatrices,Sym+ thespaceofn×nSPDmatrices,Sym the
n,m n n
spaceofn×nsymmetricmatrices,S+ thespaceofn×nSPSDmatricesofrankp≤n,Gr the
n,p n,p
p-dimensional subspaces of Rn in the projector perspective. For clarity of presentation, let G(cid:102)r
n,p
bethep-dimensionalsubspacesofRnintheONB(orthonormalbasis)perspective(Bendokatetal.,
2020). For notations related to SPD manifolds, we use the letter g ∈ {ai,le,lc} as a subscript
(superscript)toindicatetheconsideredRiemannianmetric,unlessotherwisestated. Othernotations
willbeintroducedinappropriateparagraphs. OurnotationsaresummarizedinAppendixA.
3.2 NEURALNETWORKSONSPDMANIFOLDS
In Nguyen (2022a;b), the author has shown that SPD manifolds with Affine-Invariant, Log-
Euclidean, and Log-Cholesky metrics form gyrovector spaces referred to as AI, LE, and LC gy-
rovector spaces, respectively. We adopt the notations in these works and consider the case where
r = 1(seeNguyen(2022b),Definition3.1). Let⊕ ,⊕ ,and⊕ bethebinaryoperationsinAI,
ai le lc
LE,andLCgyrovectorspaces,respectively. Let⊖ ,⊖ ,and⊖ betheinverseoperationsinAI,
ai le lc
LE,andLCgyrovectorspaces,respectively. TheseoperationsaregiveninAppendixG.
3.2.1 FCLAYERSINSPDNEURALNETWORKS
OurmethodforgeneralizingFClayerstotheSPDmanifoldsettingreliesonareformulationofSPD
hypergyroplanes(Nguyen&Yang,2023). WefirstrecapthedefinitionofSPDhypergyroplanes.
Definition 3.1 (SPD Hypergyroplanes (Nguyen & Yang, 2023)). For P ∈ Sym+,g, W ∈
n
T Sym+,g,SPDhypergyroplanesaredefinedas
P n
Hspd,g ={Q∈Sym+,g :⟨Logg(Q),W⟩g =0},
W,P n P P
where⟨.,.⟩g denotestheinnerproductatPgivenbytheconsideredRiemannianmetric.
P
Proposition3.2givesanequivalentdefinitionforSPDhypergyroplanes.
Proposition 3.2. Let P ∈ Sym+,g, W ∈ T Sym+,g, and Hspd,g be the SPD hypergyroplanes
n P n W,P
definedinDefinition3.1. Then
Hspd,g ={Q∈Sym+,g :⟨⊖ P⊕ Q,Expg (Tg (W))⟩g =0},
W,P n g g In P→In
whereI denotesthen×nidentitymatrix,and⟨.,.⟩g istheSPDinnerproductinSym+,g (Nguyen
n n
&Yang,2023)(seeAppendixG.7forthedefinitionoftheSPDinnerproduct).
Proof SeeAppendixI.
In DNNs, an FC layer linearly transforms the input in such a way that the k-th dimension of the
outputcorrespondstothesigneddistancefromtheoutputtothehyperplanethatcontainstheorigin
and is orthonormal to the k-th axis of the output space. This interpretation has proven useful in
generalizingFClayerstothehyperbolicsetting(Shimizuetal.,2021).
3PublishedasaconferencepaperatICLR2024
Notice that the equation of SPD hypergyroplanes in Proposition 3.2 has the form ⟨⊖ P ⊕
g g
Q,W′⟩g = 0, where W′ ∈ Sym+,g. This equation can be seen as a generalization of the hy-
n
perplaneequation⟨w,x⟩+b = ⟨−p+x,w⟩ = 0, wherew,x,p ∈ Rn,b ∈ R, and⟨p,w⟩ = −b.
Therefore,Proposition3.2suggeststhatanylinearfunctionofanSPDmatrixX ∈ Sym+,g canbe
n
writtenas⟨⊖ P⊕ X,W⟩g,whereP,W ∈ Sym+,g. TheaboveinterpretationofFClayersnow
g g n
canbeappliedtoourcaseforconstructingFClayersinSPDneuralnetworks. Forconvenienceof
presentation, in Definition 3.3, we will index the dimensions (axes) of the output space using two
subscriptscorrespondingtotherowandcolumnindicesinamatrix.
Definition3.3. LetEg ,i ≤ j,i,j = 1,...,mbethe(i,j)-thaxisoftheoutputspace. AnSPD
(i,j)
hypergyroplanethatcontainstheoriginandisorthonormaltotheEg axiscanbedefinedas
(i,j)
Hspd,g ={Q∈Sym+,g :⟨Q,Eg ⟩g =0}.
Logg Im(E (g i,j)),Im m (i,j)
It remains to specify an orthonormal basis for each family of the considered Riemannian metrics
ofSPDmanifolds. Proposition3.4givessuchanorthonormalbasisforAIgyrovectorspacesalong
withtheexpressionfortheoutputofFClayerswithAffine-Invariantmetrics.
Proposition 3.4 (FC layers with Affine-Invariant Metrics). Let (e ,...,e ),∥e ∥ = 1,i =
1 m i
1...,m be an orthonormal basis of Rm. Let ⟨.,.⟩ai be the Affine-Invariant metric computed at
P
P∈Sym+,aias
m
⟨V,W⟩ai =Tr(VP−1WP−1)+βTr(VP−1)Tr(WP−1),
P
whereβ >−1. AnorthonormalbasisEai ,i≤j,i,j =1,...,mofSym+,aicanbegivenas
m (i,j) m
Eai
= exp(cid:16) e ieT
j
− m1(cid:0) 1− √ 1+1 mβ(cid:1) I m(cid:17) , ifi=j
(i,j) exp(cid:0)eieT j√+ejeT
i
(cid:1)
, ifi<j
2
Denote by v (X) = ⟨⊖ P ⊕ X,W ⟩ai,P ,W ∈ Sym+,ai,i ≤ j,i,j =
(i,j) √ ai (i,j) ai (i,j) (i,j) (i,j) n
1,...,m. Let α = 1( 1+mβ − 1). Then the output of an FC layer is computed as Y =
m
exp(cid:0) [y ]m (cid:1) , where[y ]m isthematrixhavingy astheelementatthei-throwand
(i,j) i,j=1 (i,j) i,j=1 (i,j)
j-thcolumn,andy isgivenby
(i,j)

v
(X)+α(cid:80)m
v (X), ifi=j
 (i,j) k=1 (k,k)
y
(i,j)
= √1 2v (i,j)(X), ifi<j
√1 v (X), ifi>j
2 (j,i)
Proof SeeAppendixJ.
As shown in Arsigny et al. (2005), a Log-Euclidean metric on Sym+,le can be obtained from any
n
inner product on Sym . In this work, we consider a metric that is invariant under all similarity
n
transformations,i.e.,themetric⟨W,V⟩le =Tr(WV). Wehavethefollowingresult.
In
Proposition 3.5 (FC layers with Log-Euclidean Metrics). An orthonormal basis Ele ,i ≤
(i,j)
j,i,j =1,...,mofSym+,lecanbegivenby
m
 (cid:16) (cid:17)
Ele
=exp e ieT
j
, ifi=j
(i,j) exp(cid:0)eieT j√+ejeT
i
(cid:1)
, ifi<j
2
Let v (X) = ⟨⊖ P ⊕ X,W ⟩le,P ,W ∈ Sym+,le,i ≤ j,i,j = 1,...,m.
(i,j) le (i,j) le (i,j) (i,j) (i,j) n
ThentheoutputofanFClayeriscomputedasY =exp(cid:0) [y ]m (cid:1) ,wherey isgivenby
(i,j) i,j=1 (i,j)

v (X), ifi=j
 (i,j)
y
(i,j)
= √1 2v (i,j)(X), ifi<j
√1 v (X), ifi>j
2 (j,i)
4PublishedasaconferencepaperatICLR2024
Proof SeeAppendixK.
Finally, we give the characterization of an orthonormal basis for LC gyrovector spaces and the
expressionfortheoutputofFClayerswithLog-Choleskymetrics.
Proposition 3.6 (FC layers with Log-Cholesky Metrics). An orthonormal basis Elc ,i ≤
(i,j)
j,i,j =1,...,mofSym+,lccanbegivenby
m
(cid:26) (e−1)e eT +I , ifi=j
Elc = i j m
(i,j) (e eT +I )(e eT +I ), ifi<j
j i m i j m
Let v (X) = ⟨⊖ P ⊕ X,W ⟩lc,P ,W ∈ Sym+,lc,i ≤ j,i,j = 1,...,m.
(i,j) lc (i,j) lc (i,j) (i,j) (i,j) n
ThentheoutputofanFClayeriscomputedasY = YYT , whereY = [y ]m , andy is
(i,j) i,j=1 (i,j)
givenby

exp(v (X)), ifi=j
 (i,j)
y = v (X), ifi<j
(j,i) (i,j)
0, ifi>j
Proof SeeAppendixL.
3.2.2 CONVOLUTIONALLAYERSINSPDNEURALNETWORKS
Consider applying a 2D convolutional layer to a multi-channel image. Let N and N be
in out
the numbers of input and output channels, respectively. Denote by yk ,i = 1,...,N ,j =
(i,j) row
1,...,N ,k =1,...,N thevalueofthek-thoutputchannelatpixel(i,j). Then
col out
(cid:88)Nin
yk = ⟨w(l,k),xl ⟩+bk, (1)
(i,j) (i,j)
l=1
where xl is a receptive field of the l-th input channel, w(l,k) is the filter associated with the
(i,j)
l-th input channel and the k-th output channel, and bk is the bias for the k-th output channel.
Let X
(i,j)
= concat(x1 (i,j),...,x (N i,i jn )), W
k
= concat(w(1,k),...,w(Nin,k)), where operation
concat(.)concatenatesallofitsarguments. ThenEq.(1)canberewritten(Shimizuetal.,2021)as
yk =⟨W ,X ⟩+bk. (2)
(i,j) k (i,j)
NotethatEq.(2)hastheform⟨w,x⟩+bandthusthecomputationsdiscussedinSection3.2.1can
be applied to implement convolutional layers in SPD neural networks. Specifically, given a set of
SPD matrices P ∈ Sym+,g,i = 1,...,N, operation concat (P ,...,P ) produces a block
i n spd 1 N
diagonalmatrixhavingP asdiagonalelements.
i
InChakrabortyetal.(2020), theauthorsdesignaconvolutionoperationforSPDneuralnetworks.
However,theirmethodisbasedontheconceptofweightedFre´chetMean,whileoursisbuiltupon
theconceptsofSPDhypergyroplaneandSPDpseudo-gyrodistancefromanSPDmatrixtoanSPD
hypergyroplane (Nguyen & Yang, 2023). Also, our convolution operation can be used for dimen-
sionalityreduction,whiletheirsalwaysproducesanoutputofthesamedimensionastheinputs.
3.3 MLRINSTRUCTURESPACES
MotivatedbytheworksinNguyen(2022a);Nguyen&Yang(2023),inthissection,weaimtobuild
MLR on SPSD manifolds. For any P ∈ S+ , we consider the decomposition P = U S UT,
n,p P P P
where U
P
∈ G(cid:102)r
n,p
and S
P
∈ Sym+ p. Each element of S+
n,p
can be seen as a flat p-dimensional
ellipsoid in Rn (Bonnabel et al., 2013). The flat ellipsoid belongs to a p-dimensional subspace
spannedbythecolumnsofU ,whilethep×pSPDmatrixS definestheshapeoftheellipsoid
P P
inSym+ p. AcanonicalrepresentationofPinstructurespaceG(cid:102)r n,p×Sym+
p
iscomputedbyiden-
tifyingacommonsubspaceandthenrotatingU tothissubspace. TheSPDmatrixS isrotated
P P
accordinglytoreflectthechangesofU . DetailsofthesecomputationsaregiveninAppendixH.
P
5PublishedasaconferencepaperatICLR2024
AssumingthatacanonicalrepresentationinstructurespaceG(cid:102)r n,p×Sym+
p
isobtainedforeachpoint
in S+ , we now discuss how to build MLR in this space. As one of the first steps for developing
n,p
networkbuildingblocksinagyrovectorspaceapproachistoconstructsomebasicoperationsinthe
consideredmanifold,wegivethedefinitionsofthebinaryandinverseoperationsinthefollowing.
Definition3.7(TheBinaryOperationinStructureSpaces). Let(U P,S P),(U Q,S Q)∈G(cid:102)r n,p×
Sym+ p,g. Thenthebinaryoperation⊕
psd,g
instructurespaceG(cid:102)r n,p×Sym+ p,g isdefinedas
(U P,S P)⊕ psd,g(U Q,S Q)=(U P⊕(cid:101)grU Q,S
P
⊕ gS Q),
where⊕(cid:101)gr isthebinaryoperationinG(cid:102)r n,p(seeAppendixG.6forthedefinitionof⊕(cid:101)gr).
Definition3.8(TheInverseOperationinStructureSpaces). Let(U P,S P) ∈ G(cid:102)r n,p×Sym+ p,g.
Thentheinverseoperation⊖
psd,g
instructurespaceG(cid:102)r n,p×Sym+ p,g isdefinedas
⊖ psd,g(U P,S P)=(⊖(cid:101)grU P,⊖ gS P),
where⊖(cid:101)gr istheinverseoperationinG(cid:102)r n,p(seeAppendixG.6forthedefinitionof⊖(cid:101)gr).
Our construction of the binary and inverse operations in G(cid:102)r
n,p
×Sym+ p,g is clearly advantageous
compared to the method in Nguyen (2022a) since this method does not preserve the information
aboutthesubspacesofthetermsinvolvedintheseoperations. Inadditiontothebinaryandinverse
operations,wealsoneedtodefinetheinnerproductinstructurespaces.
Definition 3.9 (The Inner Product in Structure Spaces). Let (U P,S P),(U Q,S Q) ∈ G(cid:102)r
n,p
×
Sym+ p,g. ThentheinnerproductinstructurespaceG(cid:102)r n,p×Sym+ p,g isdefinedas
⟨(U ,S ),(U ,S )⟩psd,g =λ⟨U UT,U UT⟩gr+⟨S ,S ⟩g,
P P Q Q P P Q Q P Q
whereλ>0,⟨.,.⟩gr istheGrassmanninnerproduct(Nguyen&Yang,2023)(seeAppendixG.7).
The key idea to generalize MLR to a Riemannian manifold is to change the margin to reflect the
geometry of the considered manifold (a formulation of MLR from the perspective of distances to
hyperplanes is given in Appendix C). This requires the notions of hyperplanes and margin in the
consideredmanifoldthatarereferredtoashypergyroplanesandpseudo-gyrodistances(Nguyen&
Yang, 2023), respectively. In our case, the definition of hypergyroplanes in structure spaces, sug-
gestedbyProposition3.2,canbegivenbelow.
Definition 3.10 (Hypergyroplanes in Structure Spaces). Let P,W ∈ G(cid:102)r
n,p
×Sym+ p,g. Then
hypergyroplanesinstructurespaceG(cid:102)r n,p×Sym+ p,g aredefinedas
H Wpsd ,P,g ={Q∈G(cid:102)r n,p×Sym+ p,g :⟨⊖ psd,gP⊕ psd,gQ,W⟩psd,g =0}.
Pseudo-gyrodistances in structure spaces can be defined in the same way as SPD pseudo-
gyrodistances. We refer the reader to Appendix G.8 for all related notions. Theorem 3.11 gives
anexpressionforthepseudo-gyrodistancefromapointtoahypergyroplaneinastructurespace.
Theorem 3.11 (Pseudo-gyrodistances in Structure Spaces). Let W = (U ,S ), P =
W W
(U P,S P), X = (U X,S X) ∈ G(cid:102)r
n,p
× Sym+ p,g, and H Wpsd ,P,g be a hypergyroplane in structure
spaceG(cid:102)r n,p×Sym+ p,g. Thenthepseudo-gyrodistancefromXtoH Wpsd ,P,g isgivenby
d¯(X,Hpsd,g)=
|λ⟨(⊖(cid:101)grU P⊕(cid:101)grU X)(⊖(cid:101)grU P⊕(cid:101)grU X)T,U WUT W⟩gr+⟨⊖ gS
P
⊕ gS X,S W⟩g|
,
W,P (cid:113)
λ(∥U UT ∥gr)2+(∥S ∥g)2
W W W
where∥.∥grand∥.∥garethenormsinducedbytheGrassmannandSPDinnerproducts,respectively.
Proof SeeAppendixM.
Thealgorithmforcomputingthepseudo-gyrodistancesisgiveninAppendixB.
6PublishedasaconferencepaperatICLR2024
3.4 NEURALNETWORKSONGRASSMANNMANIFOLDS
Inthissection,wepresentamethodforcomputingtheGrassmannlogarithmicmapintheprojector
perspective. WethenproposeGCNsonGrassmannmanifolds.
3.4.1 GRASSMANNLOGARITHMICMAPINTHEPROJECTORPERSPECTIVE
TheGrassmannlogarithmicmapisgiven(Batziesetal.,2015;Bendokatetal.,2020)by
Loggr(Q)=[Ω,P],
P
whereP,Q∈Gr ,andΩiscomputedas
n,p
1 (cid:0) (cid:1)
Ω= log (I −2Q)(I −2P) .
2 n n
Notice that the matrix (I −2Q)(I −2P) is generally not an SPD matrix. This raises an issue
n n
when one needs to implement an operation that requires the Grassmann logarithmic map in the
projector perspective using popular deep learning frameworks like PyTorch and Tensorflow, since
the matrix logarithm function is not differentiable in these frameworks. To deal with this issue,
we rely on the following result that allows us to compute the Grassmann logarithmic map in the
projectorperspectivefromtheGrassmannlogarithmicmapintheONBperspective.
Proposition3.12. Letτ bethemappingsuchthat
τ :G(cid:102)r
n,p
→Gr n,p, U(cid:55)→UUT.
gr
LetL(cid:103)og U(V),U,V∈G(cid:102)r n,pbethelogarithmicmapofVatUintheONBperspective. Then
Logg Pr(Q)=τ−1(P)(cid:0) L(cid:103)ogg τr −1(P)(τ−1(Q))(cid:1)T +L(cid:103)ogg τr −1(P)(τ−1(Q))τ−1(P)T.
Proof SeeAppendixN.
gr
Note that the Grassmann logarithmic map L(cid:103)og (V) can be computed via singular value decom-
U
position(SVD)thatisadifferentiableoperationinPyTorchandTensorflow(seeAppendixE.2.2).
Therefore,Proposition3.12providesaneffectiveimplementationoftheGrassmannlogarithmicmap
intheprojectorperspectiveforgradient-basedlearning.
3.4.2 GRAPHCONVOLUTIONALNETWORKSONGRASSMANNMANIFOLDS
We propose to extend GCNs to Grassmann geometry using an approach similar to Chami et al.
(2019);Zhaoetal.(2023). LetG =(V,E)beagraphwithvertexsetV andedgesetE,xl,i∈V be
i
theembeddingofnodeiatlayerl(l =0indicatesinputnodefeatures),N(i)={j :(i,j)∈E}be
thesetofneighborsofi∈V,Wlandblbetheweightandbiasforlayerl,andσ(.)beanon-linear
activationfunction. AbasicGCNmessage-passingupdate(Zhaoetal.,2023)canbeexpressedas
pl =Wlxl−1 (featuretransformation)
i i
(cid:88)
ql = w pl (aggregation)
i ij j
j∈N(i)
xl =σ(ql +bl) (biasandnonlinearity)
i i
For the aggregation operation, the weights w can be computed using different methods (Kipf &
ij
Welling,2017;Hamiltonetal.,2017). LetXl ∈Gr ,i∈V betheGrassmannembeddingofnode
i n,p
i at layer l. For feature transformation on Grassmann manifolds, we use isometry maps based on
leftGrassmanngyrotranslations(Nguyen&Yang,2023),i.e.,
ϕ (Xl)=exp([Loggr (M),I ])Xlexp(−[Loggr (M),I ]),
M i In,p n,p i In,p n,p
(cid:20) (cid:21)
I 0
where I = p ∈ M , and M ∈ Gr is a model parameter. Let Expgr(.) be the
n,p 0 0 n,n n,p
exponentialmapinGr . Thentheaggregationprocessisperformedas
n,p
(cid:16) (cid:88) (cid:17)
Ql =Expgr k Loggr (Pl) ,
i In,p i,j In,p j
j∈N(i)
7PublishedasaconferencepaperatICLR2024
Output class probabilities
Inputsequence
Initialization Feature transformation Aggregation
(a) (b)
Figure1: ThepipelinesofGyroSpd++(left)andGr-GCN++(right).
where Pl and Ql are the input and output node features of the aggregation operation, and k =
i i i,j
|N(i)|−1 2|N(j)|−1 2 represents the relative importance of node j to node i. For any X ∈ Gr n,p,
(cid:20) (cid:21)
I
let exp([Logg Inr ,p(X),I n,p])(cid:101)I
n,p
= VU be a QR decomposition, where (cid:101)I
n,p
= 0p ∈ M n,p,
V ∈ M is a matrix with orthonormal columns, and U ∈ M is an upper-triangular matrix.
n,p p,p
Thenthenon-linearactivationfunction(Nair&Hinton,2010;Huangetal.,2018)isgivenby
σ(X)=VVT.
Let Bl ∈ Gr be the bias for layer l. Then the message-passing update of our network can be
n,p
summarizedas
Pl =ϕ (Xl−1) (featuretransformation)
i Ml i
(cid:16) (cid:88) (cid:17)
Ql = Expgr k Loggr (Pl) (aggregation)
i In,p i,j In,p j
j∈N(i)
Xl =σ(Bl⊕ Ql) (biasandnonlinearity)
i gr i
TheGrassmannlogarithmicmapsintheaggregationoperationareobtainedusingProposition3.12.
AnotherapproachforembeddinggraphsonGrassmannmanifoldshasalsobeenproposedinZhou
et al. (2022). However, unlike our method, this method creates a Grassmann representation for a
graph via a SVD of the matrix formed from node embeddings previously learned by a Euclidean
neuralnetwork. Therefore,itisnotdesignedtolearnnodeembeddingsonGrassmannmanifolds.
4 EXPERIMENTS
4.1 HUMANACTIONRECOGNITION
Weusethreedatasets,i.e.,HDM05(Mu¨lleretal.,2007),FPHA(Garcia-Hernandoetal.,2018),and
NTURBG+D60(NTU60)(Shahroudyetal.,2016).Wecompareournetworksagainstthefollowing
state-of-the-artmodels:SPDNet(Huang&Gool,2017)1,SPDNetBN(Brooksetal.,2019)2,SPSD-
AI(Nguyen,2022a),GyroAI-HAUNet(Nguyen,2022b),andMLR-AI(Nguyen&Yang,2023).
4.1.1 ABLATIONSTUDY
Convolutional layers in SPD neural networks Our network GyroSpd++ has a MLR layer
stackedontopofaconvolutionallayer(seeFig.1). Themotivationforusingaconvolutionallayer
1https://github.com/zhiwu-huang/SPDNet.
2https://papers.nips.cc/paper/2019/hash/6e69ebbfad976d4637bb4b39de261bf7-Abstract.
html.
8
MLR
layer
Convolutional
layer
Node
j
Node
i
Node
k
Exp
map
Exp
map
Exp
map
Isometry
map
Isometry
map
Isometry
map
Bias
and
nonlinearityPublishedasaconferencepaperatICLR2024
Table 1: Results (mean accuracy ± standard deviation) and model sizes (MB) of various SPD
neuralnetworksonthethreedatasets(computedover5runs).
Method HDM05 #HDM05 FPHA #FPHA NTU60 #NTU60
SPDNet 71.36±1.49 6.58 88.79±0.36 0.99 76.14±1.43 1.80
SPDNetBN 75.05±1.38 6.68 91.02±0.25 1.03 78.35±1.34 2.06
GyroAI-HAUNet 77.05±1.35 0.31 95.65±0.23 0.11 93.27±1.29 0.02
SPSD-AI 79.64±1.54 0.31 95.72±0.44 0.11 93.92±1.55 0.03
MLR-AI 78.26±1.37 0.60 95.70±0.26 0.21 94.27±1.32 0.05
GyroSpd++(Ours) 79.78±1.42 0.76 96.84±0.27 0.27 95.28±1.37 0.07
GyroSpsd++(Ours) 78.52±1.34 0.75 97.90±0.24 0.27 96.64±1.35 0.07
Table2: Resultsandcomputationtimes(seconds)perepochofGr-GCN++anditsvariantbasedon
the ONB perspective. Node embeddings are learned on G(cid:102)r
14,7
and Gr
14,7
for Gr-GCN-ONB and
Gr-GCN++,respectively. Resultsarecomputedover5runs.
Method Gr-GCN-ONB Gr-GCN++
Accuracy±standarddeviation 81.9±1.2 82.8±0.7
Airport Training 0.49 0.97
Testing 0.40 0.69
Accuracy±standarddeviation 76.2±1.5 80.3±0.5
Pubmed Training 3.40 6.48
Testing 2.76 4.47
Accuracy±standarddeviation 68.1±1.0 81.6±0.4
Cora Training 0.57 0.77
Testing 0.46 0.52
isthatitcanextractglobalfeaturesfromlocalones(covariancematricescomputedfromjointcoor-
dinateswithinsub-sequencesofanactionsequence). WeuseAffine-Invariantmetricsfortheconvo-
lutionallayerandLog-EuclideanmetricsfortheMLRlayer.ResultsinTab.1showthatGyroSpd++
consistentlyoutperformstheSPDbaselinesintermsofmeanaccuracy. ResultsofGyroSpd++with
differentdesignsofRiemannianmetricsforitslayersaregiveninAppendixD.4.1.
MLRinstructurespaces WebuildGyroSpsd++byreplacingtheMLRlayerofGyroSpd++with
a MLR layer proposed in Section 3.3. Results of GyroSpsd++ are given in Tab. 1. Except SPSD-
AI, GyroSpsd++ outperforms the other baselines on HDM05 dataset in terms of mean accuracy.
Furthermore, GyroSpsd++ outperforms GyroSpd++ and all the baselines on FPHA and NTU60
datasetsintermsofmeanaccuracy. TheseresultsshowthatMLRiseffectivewhenbeingdesigned
instructurespacesfromagyrovectorspaceperspective.
4.2 NODECLASSIFICATION
We use three datasets, i.e., Airport (Zhang & Chen, 2018), Pubmed (Namata et al., 2012a), and
Cora(Senetal.,2008),eachofthemcontainsasinglegraphwiththousandsoflabelednodes. We
compareournetworkGr-GCN++(seeFig.1)againstitsvariantGr-GCN-ONB(seeAppendixE.2.4)
basedontheONBperspective.ResultsareshowninTab.2.Bothnetworksgivethebestperformance
forn = 14andp = 7. ItcanbeseenthatGr-GCN++outperformsGr-GCN-ONBinallcases. The
performancegapsaresignificantonPubmedandCoradatasets.
5 CONCLUSION
Inthispaper,wedevelopFCandconvolutionallayersforSPDneuralnetworks,andMLRonSPSD
manifolds. WeshowhowtoperformbackpropagationwiththeGrassmannlogarithmicmapinthe
projector perspective. Based on this method, we extend GCNs to Grassmann geometry. Finally,
wepresentourexperimentalresultsdemonstratingtheefficacyofourapproachinthehumanaction
recognitionandnodeclassificationtasks.
9PublishedasaconferencepaperatICLR2024
REFERENCES
Pierre-Antoine Absil, Robert E. Mahony, and Rodolphe Sepulchre. Optimization Algorithms on
MatrixManifolds. PrincetonUniversityPress,2007.
Vincent Arsigny, Pierre Fillard, Xavier Pennec, and Nicholas Ayache. Fast and Simple Computa-
tionsonTensorswithLog-EuclideanMetrics. TechnicalReportRR-5584,INRIA,2005.
E.Batzies,K.Hu¨per,L.Machado,andF.SilvaLeite. GeometricMeanandGeodesicRegressionon
Grassmannians. LinearAlgebraanditsApplications,466:83–101,2015.
ThomasBendokat, RalfZimmermann, andP.A.Absil. AGrassmannManifoldHandbook: Basic
GeometryandComputationalAspects. CoRR,abs/2011.13699,2020. URLhttps://arxiv.
org/abs/2011.13699.
Silve`re Bonnabel, Anne Collard, and Rodolphe Sepulchre. Rank-preserving Geometric Means of
PositiveSemi-definiteMatrices. LinearAlgebraanditsApplications,438:3202–3216,2013.
Daniel A. Brooks, Olivier Schwander, Fre´de´ric Barbaresco, Jean-Yves Schneider, and Matthieu
Cord. Riemannian Batch Normalization for SPD Neural Networks. In NeurIPS, pp. 15463–
15474,2019.
RudrasisChakraborty,JoseBouza,JonathanH.Manton,andBabaC.Vemuri.ManifoldNet:ADeep
NeuralNetworkforManifold-valuedDatawithApplications. TPAMI,44(2):799–810,2020.
InesChami,RexYing,ChristopherRe´,andJureLeskovec.HyperbolicGraphConvolutionalNeural
Networks. CoRR,abs/1910.12933,2019. URLhttps://arxiv.org/abs/1910.12933.
WeizeChen,XuHan,YankaiLin,HexuZhao,ZhiyuanLiu,PengLi,MaosongSun,andJieZhou.
FullyHyperbolicNeuralNetworks. InACL,pp.5672–5686,2022.
CalinCruceru,GaryBe´cigneul,andOctavian-EugenGanea.ComputationallyTractableRiemannian
ManifoldsforGraphEmbeddings. InAAAI,pp.7133–7141,2021.
JindouDai,YuweiWu,ZhiGao,andYundeJia. AHyperbolic-to-HyperbolicGraphConvolutional
Network. InCVPR,pp.154–163,2021.
ZhenDong,SuJia,ChiZhang,MingtaoPei,andYuweiWu.DeepManifoldLearningofSymmetric
PositiveDefiniteMatriceswithApplicationtoFaceRecognition. InAAAI,pp.4009–4015,2017.
AlanEdelman,Toma´sA.Arias,andStevenT.Smith. TheGeometryofAlgorithmswithOrthogo-
nalityConstraints. SIAMJournalonMatrixAnalysisandApplications,20(2):303–353,1998.
Octavian-Eugen Ganea, Gary Be´cigneul, and Thomas Hofmann. Hyperbolic neural networks. In
NeurIPS,pp.5350–5360,2018.
Guillermo Garcia-Hernando, Shanxin Yuan, Seungryul Baek, and Tae-Kyun Kim. First-Person
Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations. In CVPR, pp.
409–419,2018.
William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive Representation Learning on Large
Graphs. InNIPS,pp.1025–1035,2017.
Mehrtash Harandi, Mathieu Salzmann, and Richard Hartley. Dimensionality Reduction on SPD
Manifolds: TheEmergenceofGeometry-AwareMethods. TPAMI,40:48–62,2018.
ZhiwuHuangandLucVanGool. ARiemannianNetworkforSPDMatrixLearning. InAAAI,pp.
2036–2042,2017.
ZhiwuHuang,JiqingWu,andLucVanGool. BuildingDeepNetworksonGrassmannManifolds.
InAAAI,pp.3279–3286,2018.
Catalin Ionescu, Orestis Vantzos, and Cristian Sminchisescu. Matrix Backpropagation for Deep
NetworkswithStructuredLayers. InICCV,pp.2965–2973,2015.
10PublishedasaconferencepaperatICLR2024
CeJuandCuntaiGuan. GraphNeuralNetworksonSPDManifoldsforMotorImageryClassifica-
tion: APerspectiveFromtheTime-FrequencyAnalysis. IEEETransactionsonNeuralNetworks
andLearningSystems,pp.1–15,2023.
QiyuKang,KaiZhao,YangSong,SijieWang,andWeePengTay. NodeEmbeddingfromNeural
HamiltonianOrbitsinGraphNeuralNetworks. InICML,pp.15786–15808,2023.
SejongKim. OrderedGyrovectorSpaces. Symmetry,12(6),2020.
ThomasN.KipfandMaxWelling. Semi-SupervisedClassificationwithGraphConvolutionalNet-
works. CoRR,abs/1609.02907,2017. URLhttps://arxiv.org/abs/1609.02907.
Reinmar J. Kobler, Jun ichiro Hirayama, Qibin Zhao, and Motoaki Kawanabe. SPD Domain-
specific Batch Normalization to Crack Interpretable Unsupervised Domain Adaptation in EEG.
InNeurIPS,pp.6219–6235,2022.
GuyLebanonandJohnLafferty. HyperplaneMarginClassifiersontheMultinomialManifold. In
ICML,pp. 66,2004.
ZhenhuaLin.RiemannianGeometryofSymmetricPositiveDefiniteMatricesviaCholeskyDecom-
position. SIAMJournalonMatrixAnalysisandApplications,40(4):1353–1370,2019.
QiLiu,MaximilianNickel,andDouweKiela. HyperbolicGraphNeuralNetworks. InNeurIPS,pp.
8228–8239,2019.
ZiyuLiu,HongwenZhang,ZhenghaoChen,ZhiyongWang,andWanliOuyang. Disentanglingand
Unifying Graph Convolutions for Skeleton-Based Action Recognition. In CVPR, pp. 143–152,
2020.
Federico Lo´pez, Beatrice Pozzetti, Steve Trettel, Michael Strube, and Anna Wienhard. Vector-
valued Distance and Gyrocalculus on the Space of Symmetric Positive Definite Matrices. In
NeurIPS,pp.18350–18366,2021.
Meinard Mu¨ller, Tido Ro¨der, Michael Clausen, Bernhard Eberhardt, Bjo¨rn Kru¨ger, and Andreas
Weber. Documentation Mocap Database HDM05. Technical Report CG-2007-2, Universita¨t
Bonn,June2007.
Vinod Nair and Geoffrey E. Hinton. Rectified Linear Units Improve Restricted Boltzmann Ma-
chines. InICML,pp.807–814,2010.
Galileo Namata, Ben London, Lise Getoor, and Bert Huang. Query-driven Active Surveying for
CollectiveClassification. In10thInternationalWorkshoponMiningandLearningwithGraphs,
volume8,pp. 1,2012a.
Galileo Namata, Ben London, Lise Getoor, and Bert Huang. Query-driven Active Surveying for
CollectiveClassification. InWorkshoponMiningandLearningwithGraphs,2012b.
XuanSonNguyen. GeomNet: ANeuralNetworkBasedonRiemannianGeometriesofSPDMatrix
SpaceandCholeskySpacefor3DSkeleton-BasedInteractionRecognition. InICCV,pp.13379–
13389,2021.
Xuan Son Nguyen. A Gyrovector Space Approach for Symmetric Positive Semi-definite Matrix
Learning. InECCV,pp.52–68,2022a.
XuanSonNguyen. TheGyro-StructureofSomeMatrixManifolds. InNeurIPS,pp.26618–26630,
2022b.
XuanSonNguyenandShuoYang. BuildingNeuralNetworksonMatrixManifolds: AGyrovector
Space Approach. CoRR, abs/2305.04560, 2023. URL https://arxiv.org/abs/2305.
04560.
XuanSonNguyen,LucBrun,OlivierLe´zoray,andSe´bastienBougleux. ANeuralNetworkBased
onSPDManifoldLearningforSkeleton-basedHandGestureRecognition. InCVPR,pp.12036–
12045,2019.
11PublishedasaconferencepaperatICLR2024
XavierPennec,PierreFillard,andNicholasAyache. ARiemannianFrameworkforTensorComput-
ing. TechnicalReportRR-5255,INRIA,2004.
XavierPennec,StefanHorstSommer,andTomFletcher. RiemannianGeometricStatisticsinMedi-
calImageAnalysis. AcademicPress,2020.
Chiara Plizzari, Marco Cannici, and Matteo Matteucci. Skeleton-based Action Recognition via
SpatialandTemporalTransformerNetworks. ComputerVisionandImageUnderstanding, 208:
103219,2021.
Prithviraj Sen, Galileo Mark Namata, Mustafa Bilgic, Lise Getoor, Brian Gallagher, and Tina
Eliassi-Rad. CollectiveClassificationinNetworkData. AIMagazine,29(3):93–106,2008.
AmirShahroudy,JunLiu,Tian-TsongNg,andGangWang. NTURGB+D:ALargeScaleDataset
for3DHumanActivityAnalysis. InCVPR,pp.1010–1019,2016.
Ryohei Shimizu, Yusuke Mukuta, and Tatsuya Harada. Hyperbolic Neural Networks++. CoRR,
abs/2006.08210,2021. URLhttps://arxiv.org/abs/2006.08210.
OndrejSkopek,Octavian-EugenGanea,andGaryBe´cigneul. Mixed-curvatureVariationalAutoen-
coders. CoRR,abs/1911.08411,2020. URLhttps://arxiv.org/abs/1911.08411.
Lincon S. Souza, Naoya Sogi, Bernardo B. Gatto, Takumi Kobayashi, and Kazuhiro Fukui. An
InterfacebetweenGrassmannManifoldsandVectorSpaces. InCVPRW,pp.3695–3704,2020.
RheaSanjaySukthanker,ZhiwuHuang,SuryanshKumar,ErikGoronEndsjo,YanWu,andLucVan
Gool. NeuralArchitectureSearchofSPDManifoldNetworks. InIJCAI,pp.3002–3009,2021.
AbrahamAlbertUngar. BeyondtheEinsteinAdditionLawandItsGyroscopicThomasPrecession:
TheTheoryofGyrogroupsandGyrovectorSpaces. FundamentalTheoriesofPhysics, vol.117,
Springer,Netherlands,2002.
Abraham Albert Ungar. Analytic Hyperbolic Geometry: Mathematical Foundations and Applica-
tions. WorldScientificPublishingCo.Pte.Ltd.,Hackensack,NJ,2005.
Abraham Albert Ungar. Analytic Hyperbolic Geometry in N Dimensions: An Introduction. CRC
Press,2014.
Petar Velic˘kovic´, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio`, and Yoshua
Bengio. Graph Attention Networks. CoRR, abs/1710.10903, 2018. URL https://arxiv.
org/abs/1710.10903.
RuiWangandXiao-JunWu. GrasNet: ASimpleGrassmannianNetworkforImageSetClassifica-
tion. NeuralProcessingLetters,52(1):693–711,2020.
RuiWang,Xiao-JunWu,andJosefKittler. SymNet: ASimpleSymmetricPositiveDefiniteMani-
foldDeepLearningMethodforImageSetClassification. IEEETransactionsonNeuralNetworks
andLearningSystems,pp.1–15,2021.
Muhan Zhang and Yixin Chen. Link Prediction Based on Graph Neural Networks. CoRR,
abs/1802.09691,2018. URLhttps://arxiv.org/abs/1802.09691.
YidingZhang,XiaoWang,ChuanShi,NianLiu,andGuojieSong.LorentzianGraphConvolutional
Networks. InProceedingsoftheWebConference2021,pp.1249–1261,2021.
YidingZhang,XiaoWang,ChuanShi,XunqiangJiang,andYanfangYe. HyperbolicGraphAtten-
tionNetwork. IEEETransactionsonBigData,8(6):1690–1701,2022.
Wei Zhao, Federico Lopez, J. Maxwell Riestenberg, Michael Strube, Diaaeldin Taha, and Steve
Trettel. Modeling Graphs Beyond Hyperbolic: Graph Neural Networks in Symmetric Positive
DefiniteMatrices. CoRR,abs/2306.14064,2023. URLhttps://arxiv.org/abs/2306.
14064.
BingxinZhou,XuebinZheng,YuGuangWang,MingLi,andJunbinGao. EmbeddingGraphson
GrassmannManifold. NeuralNetworks,152:322–331,2022.
HuanyuZhou,QingjieLiu,andYunhongWang.LearningDiscriminativeRepresentationsforSkele-
tonBasedActionRecognition. InCVPR,pp.10608–10617,2023.
12PublishedasaconferencepaperatICLR2024
Symbol Name
M Spaceofn×mmatrices
n,m
Sym+ Spaceofn×nSPDmatrices
n
Sym+,ai Spaceofn×nSPDmatriceswithAIgeometry
n
Sym Spaceofn×nsymmetricmatrices
n
Gr Grassmannianintheprojectorperspective
n,p
G(cid:102)r
n,p
GrassmannianintheONBperspective
S+ Spaceofn×nSPSDmatricesofrankp≤n
n,p
M Matrixmanifold
T M TangentspaceofMatP
P
exp(P) MatrixexponentialofP
log(P) MatrixlogarithmofP
Expai(W) ExponentialmapofWatPinSym+,ai
P n
Logai(Q) LogarithmicmapofQatPinSym+,ai
P n
Tai (W) ParalleltransportofWfromPtoQinSym+,ai
P→Q n
Expgr(W) ExponentialmapofWatPinGr
P n,p
Loggr(Q) LogarithmicmapofQatPinGr
P n,p
gr
L(cid:103)og P(Q) LogarithmicmapofQatPinG(cid:102)r
n,p
⊕ ,⊖ BinaryandinverseoperationsinSym+,ai
ai ai n
⊕ ,⊖ BinaryandinverseoperationsinGr
gr gr n,p
⊕(cid:101)gr,⊖(cid:101)gr BinaryandinverseoperationsinG(cid:102)r
n,p
⊕
psd,ai
BinaryoperationinG(cid:102)r n,p×Sym+ p,ai
⊖
psd,ai
InverseoperationinG(cid:102)r n,p×Sym+ p,ai
Hspd,ai HypergyroplaneinSym+,ai
W,P n
H Wpsd ,P,ai HypergyroplaneinG(cid:102)r n,p×Sym+ p,ai
Eai OrthonormalbasisofSym+,ai
(i,j) m
⟨.,.⟩ai InnerproductinSym+,ai
n
⟨.,.⟩gr InnerproductinGr
n,p
⟨.,.⟩psd,ai InnerproductinG(cid:102)r n,p×Sym+ p,ai
⟨.,.⟩ai Affine-InvariantmetricatP
P
I n×nidentitymatrix
n
(cid:34) (cid:35)
I 0
I p ∈M
n,p n,n
0 0
(cid:34) (cid:35)
I
(cid:101)I
n,p
p ∈M
n,p
0
Table 3: The main notations used in the paper. For the notations related to SPD manifolds, only
thoseassociatedwithAffine-Invariantgeometryareshown.
A NOTATIONS
Tab.3presentsthemainnotationsusedinourpaper.
B MLR IN STRUCTURE SPACES
Algorithm1summarizesallstepsforthecomputationofpseudo-gyrodistancesinTheorem3.11.
Detailsofsomestepsaregivenbelow:
13PublishedasaconferencepaperatICLR2024
Algorithm1:ComputationofPseudo-gyrodistances
Input: AbatchofSPSDmatricesX ∈S+ ,i=1,...,N
i n,p
ThenumberofclassesC
TheparametersforeachclassUc P,Uc
W
∈G(cid:102)r n,p,Sc P,Sc
W
∈Sym+ p,c=1,...,C
Aconstantγ ∈[0,1]
Output: Anarrayd∈M ofpseudo-gyrodistances
N,C
1
Um ←(cid:101)I n,p;
2
(U i,ΣΣΣ i,V i)
i=1,...,N
←SVD((X i) i=1,...,N); /* X
i
=U iΣΣΣ iV iT */
(U ) ←(U [:,:p]) ;
3 i i=1,...,N i i=1,...,N
iftrainingthen
4
U←GrMean((U ) );
5 i i=1,...,N
Um ←GrGeodesic(Um,U,γ);
6
end
7
(Ui ,Si ) =GrCanonicalize((X ,U ) ,Um);
8 X X i=1,...,N i i i=1,...,N
forc←1toC do
9
10
d((X) i=1,...,N,c)=
|λ⟨(⊖(cid:101)grUc P⊕(cid:101)grUX)(√⊖(cid:101)grUc P⊕(cid:101)grUX)T,Uc W(Uc W)T⟩gr+⟨⊖gSc P⊕gSX,Sc W⟩g|
;
λ(∥Uc (Uc )T∥gr)2+(∥Sc ∥g)2
W W W
end
11
• SVD((X ) )performssingularvaluedecompositionsforabatchofmatrices.
i i=1,...,N
• GrMean((U ) )computestheFre´chetmeanofitsarguments.
i i=1,...,N
• GrGeodesic(Um,U,γ)computesapointonageodesicfromUmtoUatstepγ(γ =0.1
inourexperiments).
• GrCanonicalize((X ,U ) ,V)computesthecanonicalrepresentationsofX ,i =
i i i=1,...,N i
1,...,N usingVasacommonsubspace(seeAppendixH).
• Line 10: U = (Ui ) ,S = (Si ) , and the computation of pseudo-
X X i=1,...,N X X i=1,...,N
gyrodistancesisperformedinbatches.
C FORMULATION OF MLR FROM THE PERSPECTIVE OF DISTANCES TO
HYPERPLANES
GivenK classes,MLRcomputestheprobabilityofeachoftheoutputclassesas
exp(wTx+b )
p(y =k|x)=
(cid:80)K
expk (wTx+k
b )
∝exp(w kTx+b k), (3)
i=1 i i
wherexisaninputsample,b ∈R,x,w ∈Rn,i=1,...,K.
i i
AsshowninLebanon&Lafferty(2004),Eq.(3)canberewrittenas
p(y =k|x)∝exp(sign(wTx+b )∥w ∥d(x,H )),
k k k wk,bk
whered(x,H )isthedistancefrompointxtoahyperplaneH definedas
wk,bk wk,bk
H ={x∈Rn :⟨w,x⟩+b=0},
w,b
wherew ∈Rn\{0},andb∈R.
D HUMAN ACTION RECOGNITION
D.1 DATASETS
HDM05 (Mu¨ller et al., 2007) It has 2337 sequences of 3D skeleton data classified into 130
classes. Eachframecontainsthe3Dcoordinatesof31bodyjoints. Weusealltheactionclassesand
followtheexperimentalprotocolinHarandietal.(2018)inwhich2subjectsareusedfortraining
andtheremaining3subjectsareusedfortesting.
14PublishedasaconferencepaperatICLR2024
FPHA(Garcia-Hernandoetal.,2018) Ithas1175sequencesof3Dskeletondataclassifiedinto
45classes. Eachframecontainsthe3Dcoordinatesof21handjoints. Wefollowtheexperimental
protocol in Garcia-Hernando et al. (2018) in which 600 sequences are used for training and 575
sequencesareusedfortesting.
NTU60 (Shahroudy et al., 2016) It has 56880 sequences of 3D skeleton data classified into 60
classes. Eachframecontainsthe3Dcoordinatesof25or50bodyjoints. Weusethemutualactions
andfollowthecross-subjectexperimentalprotocolinShahroudyetal.(2016)inwhichdatafrom20
subjectsareusedfortraining,andthosefromtheother20subjectsareusedfortesting.
D.2 IMPLEMENTATIONDETAILS
D.2.1 SETUP
WeusethePyTorchframeworktoimplementournetworksandthosefrompreviousworks. These
networksaretrainedusingcross-entropylossandAdadeltaoptimizerfor2000epochs. Thelearning
rate is set to 10−3. The factors β (see Proposition 3.4) and λ (see Definition 3.9) are set to 0 and
1, respectively. For GyroSpd++, the sizes of output matrices of the convolutional layer are set
to 34×34, 21×21, and 11×11 for the experiments on HDM05, FPHA, and NTU60 datasets,
respectively. For GyroSpsd++, the sizes of SPD matrices in structure spaces are set to 20×20,
14×14,and8×8fortheexperimentsonHDM05,FPHA,andNTU60datasets,respectively. We
useabatchsizeof32forHDM05andFPHAdatasets,andabatchsizeof256forNTU60dataset.
D.2.2 INPUTDATA
We use a similar method as in Nguyen (2022b) to compute the input data for our network Gy-
roSpd++. We first identify a closest left (right) neighbor of every joint based on their distance to
thehip(wrist)joint,andthencombinethe3Dcoordinatesofeachjointandthoseofitsleft(right)
neighbor to create a feature vector for the joint. For a given frame t, a mean vectorµµµ and a co-
t
variancematrixΣΣΣ arecomputedfromthesetoffeaturevectorsoftheframeandthencombinedto
t
createanSPDmatrixas
(cid:20) (cid:21)
ΣΣΣ +µµµ (µµµ )T µµµ
Y = t t t t .
t (µµµ )T 1
t
The lower part of matrix log(Y ) is flattened to obtain a vector v˜ . All vectors v˜ within a time
t t t
window[t,t+c−1],wherecisdeterminedfromatemporalpyramidrepresentationofthesequence
(thenumberoftemporalpyramidsissetto2inourexperiments),areusedtocomputeacovariance
matrixas
t+c−1
1 (cid:88)
Z = (v˜ −v¯ )(v˜ −v¯ )T, (4)
t c i t i t
i=t
where v¯ =
1(cid:80)t+c−1v˜
. For GyroAI-HAUNet, SPSD-AI, MLR-AI, GyroSpd++, and Gy-
t c i=t i
roSpsd++,theinputdataarethesetofmatricesobtainedinEq.(4).
ForSPDNetandSPDNetBN,eachsequenceisrepresentedbyacovariancematrix(Huang&Gool,
2017;Brooksetal.,2019). Thesizesofthecovariancematricesare93×93,60×60,and150×150
for HDM05, FPHA, and NTU60 datasets, respectively. For SPDNet, the same architecture as the
oneinHuang&Gool(2017)isusedwiththreeBimaplayers.ForSPDNetBN,thesamearchitecture
astheoneinBrooksetal.(2019)isusedwiththreeBimaplayers. Thesizesofthetransformation
matricesfortheexperimentsonHDM05,FPHA,andNTU60datasetsaresetto93×93,60×60,
and150×150,respectively.
D.2.3 CONVOLUTIONALLAYERS
Inordertoreducethenumberofparametersandthecomputationalcostfortheconvolutionallayer
inGyroSpd++, weassumeadiagonalstructurefortheparameterP (seePropositions3.4,3.5,
(i,j)
and3.6),i.e.,
P =concat (P1 ,...,PL ),
(i,j) spd (i,j) (i,j)
whereListhenumberofinputSPDmatricesofoperationconcat (.).
spd
15PublishedasaconferencepaperatICLR2024
D.2.4 OPTIMIZATION
ForparametersthatareSPDmatrices,wemodelthemonthespaceofsymmetricmatrices,andthen
applytheexponentialmapattheidentity.
ForanyparameterP∈G(cid:102)r n,p,weparameterizeitbyamatrixB∈M p,n−psuchthat
(cid:20) (cid:21)
0 B
=[Loggr (PPT),I ].
−BT 0 In,p n,p
ThenparameterPcanbecomputedby
(cid:18)(cid:20) (cid:21)(cid:19)
0 B
P=exp([Logg Inr ,p(PPT),I n,p])(cid:101)I
n,p
=exp
−BT 0
(cid:101)I n,p.
Thus, we can optimize all parameters on Euclidean spaces without having to resort to techniques
developedonRiemannianmanifolds.
D.3 TIMECOMPLEXITYANALYSIS
Letn ×n bethesizeofinputSPDmatrices,n ×n bethesizeofoutputmatricesofthe
in in out out
convolutionallayerinGyroSpd++,n ×n bethesizeofSPDmatricesinstructurespaces,
rank rank
n bethenumberofactionclasses,n bethenumberofSPDmatricesencodingasequence.
c s
• GyroSpd++: TheconvolutionallayerhastimecomplexityO(n n2 n3 ). TheMLRlayer
s out in
hastimecomplexityO(n n3 ).
c out
• GyroSpsd++:TheconvolutionallayerhastimecomplexityO(n n2 n3 ).TheMLRlayer
s out in
hastimecomplexityO(n3 +n n3 ).
out c rank
D.4 MOREEXPERIMENTALRESULTS
D.4.1 ABLATIONSTUDY
ImpactofthefactorβinAffine-Invariantmetrics TostudytheimpactofthefactorβinAffine-
Invariant metrics on the performance of GyroSpd++, we follow the approach in Nguyen (2021).
Denoteby(µµµ,ΣΣΣ)theGaussiandistributionwhereµµµ ∈ Rn andΣΣΣ ∈ M areitsmeanandcovari-
n,n
ance. WecanidentifytheGaussiandistribution(µµµ,ΣΣΣ)withthefollowingmatrix:
(cid:20) (cid:21)
(detΣΣΣ)− n+1 k ΣΣΣ µµµ+ (kk )µµµ TµµµT µµµ I(k) ,
k
wherek ≥1,µµµ(k)isamatrixwithkidenticalcolumnvectorsµµµ.ThenaturalsymmetricRiemannian
metricresultingfromtheaboveembeddingisgiven(Nguyen,2021)by
1
⟨V,W⟩ =Tr(VP−1WP−1)− Tr(VP−1)Tr(WP−1),
P n+k
where P is an SPD matrix, V and W are two tangent vectors at point P of the manifold. This
RiemannianmetricbelongstothefamilyofAffine-Invariantmetricswhereβ =− 1 >−1.
n+k n
Forthisstudy,wereplacematrixZ inEq.(4)withthefollowingmatrix:
t
(cid:20) (cid:21)
Z(cid:101)t =(detZ t)− n11 +k Z t v¯+ (k kv¯ )Ttv¯ tT v¯ t I(k) ,
t k
wheren 1×n
1
isthesizeofmatrixZ t. TheinputdataforGyroSpd++arethencomputedfromZ(cid:101)t
asbefore.
Tab.4reportsthemeanaccuraciesandstandarddeviationsofGyroSpd++withrespecttodifferent
settingsofβonthethreedatasets.GyroSpd++withthesettingβ =0generallyworkswellonallthe
datasets. Settingk = 3improvestheaccuracyofGyroSpd++onNTU60dataset. Wealsoobserve
thatsettingktoahighvalue,e.g.,k =10lowerstheaccuraciesofGyroSpd++onthedatasets.
16PublishedasaconferencepaperatICLR2024
Table 4: Results (mean accuracy ± standard deviation) of GyroSpd++ with respect to different
settingsofβ onthethreedatasets(computedover5runs).
Dataset HDM05 FPHA NTU60
β=0 79.78±1.42 96.84±0.27 95.28±1.37
k=3 79.12±1.37 96.16±0.25 96.32±1.33
k=10 78.25±1.39 95.91±0.29 94.44±1.34
Table 5: Results and computation times (seconds) of GyroSpd++ with respect to different set-
tingsoftheoutputdimensionoftheconvolutionallayeronFPHAdataset(computedover5runs).
ExperimentsareconductedonamachinewithIntelCorei7-8565UCPU1.80GHz24GBRAM.
Computationtime/epoch
m Accuracy±standarddeviation
Training Testing
10 94.53±0.31 30.08 12.07
21 96.84±0.27 129.30 50.84
30 96.80±0.26 182.52 71.49
Outputdimensionofconvolutionallayers Tab.5presentsresultsandcomputationtimesofGy-
roSpd++ with respect to different settings of the output dimension of the convolutional layer on
FPHA dataset. Results show that the setting m = 21 clearly outperforms the setting m = 10 in
terms of mean accuracy and standard deviation. However, compared to the setting m = 21, the
settingm = 30onlyincreasesthetrainingandtestingtimeswithoutimprovingthemeanaccuracy
ofGyroSpd++.
DesignofRiemannianmetricsfornetworkblocks TheuseofdifferentRiemannianmetricsfor
the convolutional and MLR layers of GyroSpd++ results in different variants of the same archi-
tecture. Results of some of these variants on FPHA dataset are shown in Tab. 6. It is noted that
ourarchitecturegivesthebestperformanceintermsofmeanaccuracy,whilethearchitecturewith
Log-CholeskygeometryfortheMLRlayerperformstheworstintermsofmeanaccuracy.
D.4.2 COMPARISONOFGYROSPD++AGAINSTSTATE-OF-THE-ARTMETHODS
Here we present more comparisons of our networks against state-of-the-art networks. These net-
works belong to one of the following families of neural networks: (1) Hyperbolic neural net-
works: HypGRU(Ganeaetal.,2018)3;(2)Graphneuralnetworks: MS-G3D(Liuetal.,2020)4,
TGN (Zhou et al., 2023)5; (3) Transformers: ST-TR (Plizzari et al., 2021)6. Note that MS-G3D,
TGN, and ST-TR are specifically designed for skeleton-based action recognition. We use default
parameter settings for these networks. Results of our networks and their competitors on HDM05,
FPHA,andNTU60datasetsareshowninTabs.7,8,and9,respectively. OnHDM05dataset,Gy-
roSpd++ outperforms HypGRU, MS-G3D, ST-TR, and TGN by 25.6%, 10.8%, 11.9%, and 9.5%
points in terms of mean accuracy, respectively. On FPHA dataset, GyroSpd++ outperforms Hyp-
GRU, MS-G3D, ST-TR, and TGN by 38.6%, 8.5%, 10.9%, and 6.0% points in terms of mean
accuracy, respectively. On NTU60 dataset, GyroSpd++ outperforms HypGRU, MS-G3D, ST-TR,
andTGNby7.0%,3.1%,4.2%,and2.2%pointsintermsofmeanaccuracy,respectively. Overall,
ournetworksaresuperiortotheircompetitorsinallcases.
Finally,wepresentacomparisonofcomputationtimesofSPDneuralnetworksinTab.10.
17PublishedasaconferencepaperatICLR2024
Table 6: Results (mean accuracy ± standard deviation) of GyroSpd++ with different designs of
RiemannianmetricsforitslayersonFPHAdataset(computedover5runs).
AI-LE(GyroSpd++) LE-LE AI-AI LE-AI AI-LC
96.84±0.27 94.72±0.25 94.35±0.29 95.21±0.26 89.16±0.26
Table7: Resultsofournetworksandsomestate-of-the-artmethodsonHDM05dataset(computed
over5runs).
Method Accuracy±Standarddeviation
HypGRU(Ganeaetal.,2018) 54.18±1.51
MS-G3D(Liuetal.,2020) 68.92±1.72
ST-TR(Plizzarietal.,2021) 67.84±1.66
TGN(Zhouetal.,2023) 70.26±1.48
GyroSpd++(Ours) 79.78±1.42
GyroSpsd++(Ours) 78.52±1.34
E NODE CLASSIFICATION
E.1 DATASETS
Airport (Chami et al., 2019) It is a flight network dataset from OpenFlights.org where nodes
represent airports, edges represent the airline Routes, and node labels are the populations of the
countrywheretheairportbelongs.
Pubmed (Namata et al., 2012b) It is a standard benchmark describing citation networks where
nodesrepresentscientificpapersintheareaofmedicine,edgesarecitationsbetweenthem,andnode
labelsareacademic(sub)areas.
Cora(Senetal.,2008) Itisacitationnetworkwherenodesrepresentscientificpapersinthearea
ofmachinelearning,edgesarecitationsbetweenthem,andnodelabelsareacademic(sub)areas.
ThestatisticsofthethreedatasetsaresummarizedinTab.11.
E.2 IMPLEMENTATIONDETAILS
E.2.1 SETUP
OurnetworkisimplementedusingthePyTorchframework.WesethyperparametersasinZhaoetal.
(2023)thatarefoundviagridsearchforeachgrapharchitectureonthedevelopmentsetofagiven
dataset. Thebestsettingsofnandparefoundfrom(n,p)∈{(2k,k)},k =2,3,...,10. Thebatch
sizeissettothetotalnumberofgraphnodesinadataset(Chamietal.,2019;Zhaoetal.,2023). The
networksaretrainedusingcross-entropylossandAdamoptimizerforamaximumof500epochs.
The learning rate is set to 10−2. Early stopping is used when the loss on the development set has
notdecreasedfor200epochs. Eachnetworkhastwolayersthatperformmessagepassingtwiceat
oneiteration(Zhaoetal.,2023). Weusethe70/15/15percentsplits(Chamietal.,2019)forAirport
dataset,andstandardsplitsinGCNKipf&Welling(2017)forPubmedandCoradatasets.
3https://github.com/dalab/hyperbolic_nn.
4https://github.com/kenziyuliu/MS-G3D.
5https://github.com/zhysora/FR-Head.
6https://github.com/Chiaraplizz/ST-TR.
18PublishedasaconferencepaperatICLR2024
Table 8: Results of our networks and some state-of-the-art methods on FPHA dataset (computed
over5runs).
Method Accuracy±Standarddeviation
HypGRU(Ganeaetal.,2018) 58.24±0.29
MS-G3D(Liuetal.,2020) 88.26±0.67
ST-TR(Plizzarietal.,2021) 85.94±0.46
TGN(Zhouetal.,2023) 90.81±0.53
GyroSpd++(Ours) 96.84±0.27
GyroSpsd++(Ours) 97.90±0.24
Table9: Resultsofournetworksandsomestate-of-the-artmethodsonNTU60dataset(computed
over5runs).
Method Accuracy±Standarddeviation
HypGRU(Ganeaetal.,2018) 88.26±1.40
MS-G3D(Liuetal.,2020) 92.15±1.60
ST-TR(Plizzarietal.,2021) 91.04±1.52
TGN(Zhouetal.,2023) 93.02±1.56
GyroSpd++(Ours) 95.28±1.37
GyroSpsd++(Ours) 96.64±1.35
E.2.2 GRASSMANNLOGARITHMICMAPINTHEONBPERSPECTIVE
TheGrassmannlogarithmicmapintheONBperspectiveisgiven(Edelmanetal.,1998)by
gr
L(cid:103)og (Q)=Uarctan(ΣΣΣ)VT,
P
whereP,Q∈G(cid:102)r n,p,U,ΣΣΣ,andVareobtainedfromtheSVD(I n−PPT)Q(PTQ)−1 =UΣΣΣVT.
E.2.3 GR-GCN++
To create Grassmann embeddings as input node features, we first transform d-dimensional input
featuresintop(n−p)-dimensionalvectorsviaalinearmap. Wethenreshapeeachresultingvector
toamatrixB∈M . TheinputGrassmannembeddingX0,i∈V iscomputedas
p,n−p i
(cid:18)(cid:20) (cid:21)(cid:19) (cid:18) (cid:20) (cid:21)(cid:19)
0 B 0 B
X0 =exp I exp − .
i −BT 0 n,p −BT 0
E.2.4 GR-GCN-ONB
To create Grassmann embeddings as input node features, we first transform d-dimensional input
featuresintop(n−p)-dimensionalvectorsviaalinearmap. Wethenreshapeeachresultingvector
toamatrixB∈M . TheinputGrassmannembeddingX0,i∈V iscomputedas
p,n−p i
(cid:18)(cid:20) (cid:21)(cid:19)
0 B
X0
i
=exp
−BT 0
(cid:101)I n,p.
Feature transformation is performed by first mapping the input to a projection matrix (using the
mapping τ in Section 3.4.1), then applying an isometry map based on left Grassmann gyrotrans-
lations(Nguyen&Yang,2023), andfinallymappingtheresultbacktoamatrixwithorthonormal
columns. Thisisequivalenttoperformingthefollowingmapping:
ϕ M(Xl i)=M⊕(cid:101)grXl
i
=exp([Logg Inr ,p(MMT),I n,p])Xl i,
whereXl
i
∈G(cid:102)r n,pandM∈G(cid:102)r n,pisamodelparameter.
19PublishedasaconferencepaperatICLR2024
Table10: Computationtimes(seconds)perepochofournetworksandsomestate-of-the-artSPD
neural networks on FPHA dataset. Experiments are conducted on a machine with Intel Core i7-
8565UCPU1.80GHz24GBRAM.
Method SPDNet SPDNetBN GyroAI-HAUNet SPSD-AI MLR-AI GyroSpd++ GyroSpsd++
Training 17.52 40.08 62.21 73.73 102.58 129.30 126.08
Testing 3.48 6.22 30.83 35.54 46.28 50.84 48.06
Table11: Descriptionofthedatasetsfornodeclassification.
Dataset #Nodes #Edges #Classes #Features
Airport 3188 18631 4 4
Pubmed 19717 44338 3 500
Cora 2708 5429 7 1433
ForanyX ∈ G(cid:102)r n,p,letX = VUbeaQRdecompositionofX,whereV ∈ M
n,p
isamatrixwith
orthonormalcolumns,andU ∈ M isanupper-triangularmatrix. Thenthenon-linearactivation
p,p
functionisgivenby
σ(X)=V.
Biasadditionisperformedusingoperation⊕(cid:101)gr insteadofoperation⊕ gr. TheoutputofGr-GCN-
ONBismappedtoaprojectionmatrixfornodeclassification.
E.2.5 OPTIMIZATION
ForanyparameterP∈Gr ,weparameterizeitbyamatrixB∈M suchthat
n,p p,n−p
(cid:20) (cid:21)
0 B
=[Loggr (P),I ].
−BT 0 In,p n,p
ThenparameterPcanbecomputedby
(cid:18)(cid:20) (cid:21)(cid:19) (cid:18) (cid:20) (cid:21)(cid:19)
0 B 0 B
P=exp I exp − .
−BT 0 n,p −BT 0
E.3 MOREEXPERIMENTALRESULTS
E.3.1 ABLATIONSTUDY
Projector vs. ONB perspective More results of Gr-GCN++ and Gr-GCN-ONB are presented
in Tabs. 12 and 13. As can be observed, Gr-GCN++ outperforms Gr-GCN-ONB in all cases. In
particular,theformeroutperformsthelatterbylargemarginsonAirportandCoradatasets. Results
showthatwhileboththenetworkslearnnodeembeddingsonGrassmannmanifolds, thechoiceof
perspectiveforrepresentingtheseembeddingsandtheassociatedparameterscanhaveasignificant
impactonthenetworkperformance.
E.3.2 COMPARISONOFGR-GCN++AGAINSTSTATE-OF-THE-ARTMETHODS
Tab. 14 shows results of Gr-GCN++ and some state-of-the-art methods on the three datasets. The
hyperbolic networks outperform their SPD and Grassmann counterparts on Airport dataset with
high hyperbolicity (Chami et al., 2019). This agrees with previous works (Chami et al., 2019;
Zhangetal.,2022)thatreportgoodperformancesofhyperbolicembeddingsontree-likedatasets.
However,ournetworkanditsSPDcounterpartSPD-GCNoutperformtheircompetitorsonPubmed
andCoradatasetswithlowhyperbolicities. ComparedtoSPD-GCN,Gr-GCN++alwaysgivesmore
consistentresults.
20PublishedasaconferencepaperatICLR2024
Table 12: Results and computation times (seconds) per epoch of Gr-GCN++ and its variant Gr-
GCN-ONB based on the ONB perspective. Node embeddings are learned on G(cid:102)r
4,2
and Gr
4,2
for
Gr-GCN-ONB and Gr-GCN++, respectively. Results are computed over 5 runs. Experiments are
conductedonamachinewithIntelCorei7-9700CPU3.00GHz15GBRAM.
Method Gr-GCN-ONB Gr-GCN++
Accuracy±standarddeviation 53.2±1.9 60.1±1.3
Airport Training 0.07 0.21
Testing 0.05 0.12
Accuracy±standarddeviation 75.7±2.1 77.5±1.1
Pubmed Training 0.50 0.90
Testing 0.38 0.54
Accuracy±standarddeviation 33.9±2.3 64.4±1.4
Cora Training 0.10 0.12
Testing 0.07 0.08
Table 13: Results and computation times (seconds) per epoch of Gr-GCN++ and its variant Gr-
GCN-ONB based on the ONB perspective. Node embeddings are learned on G(cid:102)r
6,3
and Gr
6,3
for
Gr-GCN-ONB and Gr-GCN++, respectively. Results are computed over 5 runs. Experiments are
conductedonamachinewithIntelCorei7-9700CPU3.00GHz15GBRAM.
Method Gr-GCN-ONB Gr-GCN++
Accuracy±standarddeviation 65.8±1.5 74.1±0.9
Airport Training 0.19 0.34
Testing 0.14 0.21
Accuracy±standarddeviation 75.8±2.0 78.5±0.9
Pubmed Training 0.90 1.76
Testing 0.75 1.05
Accuracy±standarddeviation 41.4±2.2 70.5±1.1
Cora Training 0.16 0.22
Testing 0.12 0.16
F LIMITATIONS OF OUR WORK
Our SPD network GyroSpd++ relies on different Riemannian metrics across the layers, i.e., the
convolutional layer is based on Affine-Invariant metrics while the MLR layer is based on Log-
Euclidean metrics. Although we have provided the experimental results demonstrating that Gy-
roSpd++achievesgoodperformanceonallthedatasetscomparedtostate-of-the-artmethods, itis
notclearifourdesignisoptimalforthehumanactionrecognitiontask. Whenitcomestobuilding
adeepSPDarchitecture,itisusefultoprovideinsightsintoRiemannianmetricsoneshouldusefor
eachnetworkblockinordertoobtaingoodperformanceonatargettask.
In our Grassmann network Gr-GCN++, the feature transformation and bias and nonlinearity op-
erations are performed on Grassmann manifolds, while the aggregation operation is performed in
tangentspaces. Previousworks(Daietal.,2021;Chenetal.,2022)onHNNshaveshownthatthis
hybridmethodlimitsthemodelingabilityofnetworks. Therefore,itisdesirabletodevelopGCNs
wherealltheoperationsareformalizedonGrassmannmanifolds.
21PublishedasaconferencepaperatICLR2024
Table 14: Results (mean accuracy ± standard deviation) of Gr-GCN++ and some state-of-the-art
methods on the three datasets. The best and second best results in terms of mean accuracy are
highlightedinredandblue,respectively.
Method Airport Pubmed Cora
GCN(Kipf&Welling,2017) 82.2±0.6 77.8±0.8 80.2±2.3
GAT(Velic˘kovic´etal.,2018) 92.9±0.8 77.6±0.8 80.3±0.6
HGNN(Liuetal.,2019) 84.5±0.7 76.6±1.4 79.5±0.9
HGCN(Chamietal.,2019) 85.3±0.6 76.4±0.8 78.7±0.9
LGCN(Zhangetal.,2021) 88.2±0.2 77.3±1.4 80.6±0.9
HGAT(Zhangetal.,2022) 87.5±0.9 78.0±0.5 80.9±0.7
SPD-GCN(Zhaoetal.,2023) 82.6±1.5 78.7±0.5 82.3±0.5
HamGNN(Kangetal.,2023) 95.9±0.1 78.3±0.6 80.1±1.6
Gr-GCN++(Ours) 82.8±0.7 80.3±0.5 81.6±0.4
G SOME RELATED DEFINITIONS
G.1 GYROGROUPSANDGYROVECTORSPACES
Gyrovector spaces form the setting for hyperbolic geometry in the same way that vector spaces
form the setting for Euclidean geometry (Ungar, 2002; 2005; 2014). We recap the definitions of
gyrogroups and gyrocommutative gyrogroups proposed in Ungar (2002; 2005; 2014). For greater
mathematicaldetailandin-depthdiscussion,werefertheinterestedreadertothesepapers.
Definition G.1 (Gyrogroups (Ungar, 2014)). A pair (G,⊕) is a groupoid in the sense that it is
a nonempty set, G, with a binary operation, ⊕. A groupoid (G,⊕) is a gyrogroup if its binary
operationsatisfiesthefollowingaxiomsfora,b,c∈G:
(G1)Thereisatleastoneelemente∈Gcalledaleftidentitysuchthate⊕a=a.
(G2)Thereisanelement⊖a∈Gcalledaleftinverseofasuchthat⊖a⊕a=e.
(G3)Thereisanautomorphismgyr[a,b]:G→Gforeacha,b∈Gsuchthat
a⊕(b⊕c)=(a⊕b)⊕gyr[a,b]c (LeftGyroassociativeLaw).
Theautomorphismgyr[a,b]iscalledthegyroautomorphism,orthegyrationofGgeneratedbya,b.
(G4)gyr[a,b]=gyr[a⊕b,b](LeftReductionProperty).
DefinitionG.2(GyrocommutativeGyrogroups(Ungar,2014)). Agyrogroup(G,⊕)isgyrocom-
mutativeifitsatisfies
a⊕b=gyr[a,b](b⊕a) (GyrocommutativeLaw).
The following definition of gyrovector spaces is slightly different from Definition 3.2 in Ungar
(2014).
DefinitionG.3(GyrovectorSpaces). Agyrocommutativegyrogroup(G,⊕)equippedwithascalar
multiplication
(t,x)→t⊙x:R×G→G
iscalledagyrovectorspaceifitsatisfiesthefollowingaxiomsfors,t∈Randa,b,c∈G:
(V1)1⊙a=a,0⊙a=t⊙e=e,and(−1)⊙a=⊖a.
(V2)(s+t)⊙a=s⊙a⊕t⊙a.
(V3)(st)⊙a=s⊙(t⊙a).
(V4)gyr[a,b](t⊙c)=t⊙gyr[a,b]c.
(V5)gyr[s⊙a,t⊙a]=Id,whereIdistheidentitymap.
22PublishedasaconferencepaperatICLR2024
G.2 AIGYROVECTORSPACES
ForP,Q∈Sym+,thebinaryoperation(Nguyen,2022a)isgivenas
n
P⊕ aiQ=P21QP21.
Theinverseoperation(Nguyen,2022a)isgivenby
⊖ P=P−1.
ai
G.3 LEGYROVECTORSPACES
ForP,Q∈Sym+,thebinaryoperation(Nguyen,2022a)isgivenas
n
P⊕ Q=exp(log(P)+log(Q)).
le
Theinverseoperation(Nguyen,2022a)isgivenas
⊖ P=P−1.
le
G.4 LCGYROVECTORSPACES
ForP,Q∈Sym+,thebinaryoperation(Nguyen,2022a)isgivenas
n
P⊕
Q=(cid:0) ⌊L(P)⌋+⌊L(Q)⌋+D(L(P))D(L(Q))(cid:1) .(cid:0) ⌊L(P)⌋+⌊L(Q)⌋+D(L(P))D(L(Q))(cid:1)T
,
lc
where⌊Y⌋isamatrixofthesamesizeasmatrixY ∈M whose(i,j)elementisY ifi>j
n,n (i,j)
andiszerootherwise,D(Y)isadiagonalmatrixofthesamesizeasmatrixYwhose(i,i)element
isY ,andL(P)denotestheCholeskyfactorofP,i.e.,L(P)isalowertriangularmatrixwith
(i,i)
positivediagonalentriessuchthatP=L(P)L(P)T.
Theinverseoperation(Nguyen,2022a)isgivenby
⊖
P=(cid:0) −⌊L(P)⌋+D(L(P))−1(cid:1)(cid:0) −⌊L(P)⌋+D(L(P))−1(cid:1)T
.
lc
G.5 GRASSMANNMANIFOLDSINTHEPROJECTORPERSPECTIVE
ForP,Q∈Gr ,thebinaryoperation(Nguyen,2022b)isgivenas
n,p
P⊕ Q=exp([Loggr (P),I ])Qexp(−[Loggr (P),I ]),
gr In,p n,p In,p n,p
where[.,.]denotesthematrixcommutator.
Theinverseoperation(Nguyen,2022b)isdefinedas
⊖ P=Expgr (−Loggr (P)).
gr In,p In,p
G.6 GRASSMANNMANIFOLDSINTHEONBPERSPECTIVE
ForU,V∈G(cid:102)r n,p,thebinaryoperation(Nguyen&Yang,2023)isdefinedas
U⊕(cid:101)grV=exp([Logg Inr ,p(UUT),I n,p])V.
The inverse operation can be defined using the approach in Nguyen & Yang (2023) (see Section
2.3.1),i.e.,
⊖(cid:101)grU=τ−1(cid:0)
⊖
gr(UUT)(cid:1)
,
wherethemappingτ isdefinedinProposition3.12,i.e.,
τ :G(cid:102)r
n,p
→Gr n,p, U(cid:55)→UUT.
23PublishedasaconferencepaperatICLR2024
G.7 THESPDANDGRASSMANNINNERPRODUCTS
DefinitionG.4(TheSPDInnerProduct). LetP,Q∈Sym+,g. ThentheSPDinnerproductofP
n
andQisdefinedas
⟨P,Q⟩g =⟨Logg (P),Logg (Q)⟩g .
In In In
DefinitionG.5(TheGrassmannInnerProduct). LetP,Q ∈ Gr . ThentheGrassmanninner
n,p
productofPandQisdefinedas
⟨P,Q⟩gr =⟨Loggr (P),Loggr (Q)⟩ ,
In,p In,p In,p
where⟨.,.⟩ denotestheinnerproductatI givenbythecanonicalmetricofGr .
In,p n,p n,p
G.8 THEGYROCOSINEFUNCTIONANDGYROANGLESINSTRUCTURESPACES
Definition G.6 (The Gyrocosine Function and Gyroangles). Let P,Q, and R be three distinct
gyropoints in structure space G(cid:102)r
n,p
×Sym+ p. The gyrocosine of the measure of the gyroangle α,
0≤α≤π,between⊖ P⊕ Qand⊖ P⊕ Risgivenbytheequation
psd,g psd,g psd,g psd,g
⟨⊖ P⊕ Q,⊖ P⊕ R⟩psd,g
cosα= psd,g psd,g psd,g psd,g ,
∥⊖ P⊕ Q∥psd,g.∥⊖ P⊕ R∥psd,g
psd,g psd,g psd,g psd,g
where ∥.∥psd,g is the norm induced by the inner product in structure spaces. The gyroangle α is
denotedbyα=∠QPR.
G.9 THEGYRODISTANCEFUNCTIONINSTRUCTURESPACES
DefinitionG.7(TheGyrodistanceFunctioninStructureSpaces). LetP,Q ∈ G(cid:102)r
n,p
×Sym+ p.
ThenthegyrodistancefunctioninstructurespacesG(cid:102)r n,p×Sym+
p
isdefinedas
d(P,Q)=∥⊖ P⊕ Q∥psd,g.
psd,g psd,g
G.10 THEPSEUDO-GYRODISTANCEFUNCTIONINSTRUCTURESPACES
DefinitionG.8(ThePseudo-gyrodistanceFunctioninStructureSpaces). LetHpsd,gbeahyper-
W,P
gyroplaneinstructurespaceG(cid:102)r n,p×Sym+ p,andX∈G(cid:102)r n,p×Sym+ p.Thenthepseudo-gyrodistance
fromXtoHpsd,g isdefinedas
W,P
d¯(X,Hpsd,g)=sin(∠XPQ)d(X,P),
W,P
whereQisgivenby
Q= argmax cos(∠QPX).
Q∈Hpsd,g\{P}
W,P
Byconvention,sin(∠XPQ)=0foranyX,Q∈Hpsd,g.
W,P
H COMPUTATION OF CANONICAL REPRESENTATIONS
LetV bethespaceofn×pmatriceswithorthonormalcolumns. ForanyP ∈ S+ , letU ∈
n,p n,p P
G(cid:102)r n,p, S
P
∈ Sym+
p
such that P = U PS PUT P. Denote by W the common subspace used for
computingacanonicalrepresentationofP.Wefirstcomputetwobasesofspan(U )andspan(W),
P
denotedrespectivelybyUandW,suchthat
d (U,W)=d (span(U ),span(W)),
Vn,p G(cid:102)rn,p P
24PublishedasaconferencepaperatICLR2024
where d Vn,p(.,.) and d G(cid:102)rn,p(.,.) are the distances between two points in V
n,p
and G(cid:102)r n,p, respec-
tively. ThesetwobasescanbecomputedasU=U Y,W =WV,whereYandVareobtained
P
fromaSVDof(U )TW,i.e.,
P
(U )TW=Y(cosΣΣΣ)VT.
P
TheSPDmatrixS inthecanonicalrepresentationofPisthencomputedas
P
S =VUT PUVT.
P
I PROOF OF PROPOSITION 3.2
Proof. Wefirstrecallthedefinitionofthebinaryoperation⊕ inNguyen(2022b).
g
Definition I.1 (The Binary Operation (Nguyen, 2022b)). Let P,Q ∈ Sym+. Then the binary
n
operation⊕ isdefinedas
g
P⊕ Q=Expg(Tg (Logg (Q))).
g P In→P In
Wehave
⟨Logg(Q),W⟩g ( =1) ⟨Tg (cid:0) Logg(Q)(cid:1) ,Tg (W)⟩g
P P P→In P P→In In
(5)
( =2) ⟨Expg (cid:16) Tg (cid:0) Logg(Q)(cid:1)(cid:17) ,Expg (cid:16) Tg (W)(cid:17) ⟩g,
In P→In P In P→In
where(1)followsfromtheinvarianceoftheinnerproductunderparalleltransport,and(2)follows
fromDefinitionG.4.
LetR=Expg (cid:16) Tg (cid:0) Logg(Q)(cid:1)(cid:17) . Then
In P→In P
Logg (R)=Tg (cid:0) Logg(Q)(cid:1) ,
In P→In P
whichresultsin
Tg (cid:0) Logg (R)(cid:1) =Logg(Q).
In→P In P
Hence
Expg (cid:16) Tg (cid:0) Logg (R)(cid:1)(cid:17) =Q.
P In→P In
BytheLeftCancellationLaw,
Q=P⊕ (⊖ P⊕ Q).
g g g
Therefore
Expg (cid:16) Tg (cid:0) Logg (R)(cid:1)(cid:17) =P⊕ (⊖ P⊕ Q)
P In→P In g g g
=Expg (cid:16) Tg (cid:0) Logg (⊖ P⊕ Q)(cid:1)(cid:17) ,
P In→P In g g
wherethelastequalityfollowsfromDefinitionI.1.
Wethushave
⊖ P⊕ Q=R
g g
=Expg (cid:16) Tg (cid:0) Logg(Q)(cid:1)(cid:17) . (6)
In P→In P
CombiningEqs.(5)and(6),weget
⟨Logg(Q),W⟩g =⟨⊖ P⊕ Q,Expg (cid:0) Tg (W)(cid:1) ⟩g,
P P g g In P→In
whichconcludestheproofofProposition3.2.
25PublishedasaconferencepaperatICLR2024
J PROOF OF PROPOSITION 3.4
Proof. ThefirstpartofProposition3.4canbeeasilyverifiedusingthedefinitionoftheSPDinner
product(seeDefinitionG.4)andthatofAffine-Invariantmetrics(Pennecetal.,2020)(seeChapter
3).
To prove the second part of Proposition 3.4, we will use the notion of SPD pseudo-
gyrodistance (Nguyen & Yang, 2023) in our interpretation of FC layers on SPD manifolds, i.e.,
thesigneddistanceisreplacedwiththesignedSPDpseudo-gyrodistanceintheinterpretationgiven
inSection3.2.1. First,weneedthefollowingresultfromNguyen&Yang(2023).
TheoremJ.1(TheSPDPseudo-gyrodistancefromanSPDMatrixtoanSPDHypergyroplane
inanAIGyrovectorSpace(Nguyen&Yang,2023)). LetH beanSPDhypergyroplaneina
W,P
gyrovectorspace(Sym+,⊕ ,⊗ ),andX ∈ Sym+. ThentheSPDpseudo-gyrodistancefromX
n ai ai n
toH isgivenby
W,P
d¯(X,H )=
|⟨log(P− 21XP− 21),P−1 2WP− 21⟩ F|
.
W,P ∥P− 21WP−1 2∥
F
ByTheoremJ.1,thesignedSPDpseudo-gyrodistancefromYtoanSPDhypergyroplanethatcon-
tainstheoriginandisorthogonaltotheEai axisisgivenby
(i,j)
⟨log(Y),Logai (Eai )⟩
d¯(Y,H )= Im (i,j) F .
Loga Imi (E (a ii ,j)),Im ∥Loga Imi (E (a ii ,j))∥
F
AccordingtoourinterpretationofFClayers,
⟨log(Y),Logai (Eai )⟩
v (X)=
Im (i,j) F
.
(i,j) ∥Logai (Eai )∥
Im (i,j) F
Weconsidertwocases:
Case1: i<j.
v (X)=
⟨log(Y),√1 2(e ieT
j
+e jeT
i
)⟩
F
(i,j) ∥√1 2(e ieT
j
+e jeT
i
)∥
F
1
=⟨log(Y),√ (e eT +e eT)⟩
i j j i F
2
1 (cid:0) (cid:1)
= √ log(Y) +log(Y)
(i,j) (j,i)
2
√
= 2log(Y) .
(i,j)
Wethusdeducethat
1
log(Y) = √ v (X).
(i,j) (i,j)
2
Case2: i=j.
v (X)=
⟨log(Y),e ieT i − m1(cid:0) 1− √ 1+1 mβ(cid:1) I m⟩ F
(i,i) ∥e ieT i − m1(cid:0) 1− √ 1+1 mβ(cid:1) I m∥ F
=⟨log(Y),e eT − 1 (cid:0) 1− √ 1 (cid:1) I ⟩ .
i i m 1+mβ m F
26PublishedasaconferencepaperatICLR2024
Thisleadsto
m
1 (cid:0) 1 (cid:1)(cid:88)
v (X)=log(Y) − 1− √ log(Y) , (7)
(i,i) (i,i) m 1+mβ (j,j)
j=1
fori=1,...,m. Bysummingupv (X),i=1,...,m,weget
(i,i)
m m
(cid:88) 1 (cid:88)
v (X)= √ log(Y) ,
(i,i) (i,i)
1+mβ
i=1 i=1
orequivalently,
m m
(cid:88) (cid:112) (cid:0)(cid:88) (cid:1)
log(Y) = 1+mβ v (X) . (8)
(i,i) (i,i)
i=1 i=1
Replacingtheterm(cid:80)m
log(Y) inEq.(7)withtheexpressionontheright-handsideofEq.(8)
j=1 (j,j)
resultsin
m
1 (cid:112) (cid:88)
log(Y) =v (X)+ ( 1+mβ−1) v (X).
(i,i) (i,i) m (j,j)
j=1
NotethatY =exp([log(Y) ]m ). ThisconcludestheproofofProposition3.4.
(i,j) i,j=1
K PROOF OF PROPOSITION 3.5
Proof. ThispropositionisadirectconsequenceofProposition3.4forβ =0.
L PROOF OF PROPOSITION 3.6
Proof. ThefirstpartofProposition3.6canbeeasilyverifiedusingthedefinitionoftheSPDinner
product(seeDefinitionG.4)andthatofLog-Choleskymetrics(Lin,2019).
ToprovethesecondpartofProposition3.6,wefirstrecallthefollowingresultfromNguyen&Yang
(2023).
TheoremL.1(TheSPDGyrodistancefromanSPDMatrixtoanSPDHypergyroplaneinaLC
GyrovectorSpace(Nguyen&Yang,2023)). LetH beanSPDhypergyroplaneinagyrovector
W,P
space(Sym+,⊕ ,⊗ ),andX ∈ Sym+. ThentheSPDpseudo-gyrodistancefromXtoH is
n lc lc n W,P
equaltotheSPDgyrodistancefromXtoH andisgivenby
W,P
|⟨A,B⟩ |
d(X,H )= F ,
W,P ∥B∥
F
where
A=−⌊φ(P)⌋+⌊φ(X)⌋+log(D(φ(P))−1D(φ(X))),
B=⌊W(cid:102)⌋+D(φ(P))−1D(W(cid:102)),
(cid:16) (cid:17)
W(cid:102) =φ(P) φ(P)−1W(φ(P)−1)T ,
1
2
where⌊Y⌋andD(Y),Y ∈M aredefinedinSectionG.4,andφ(P)=L(P).
n,n
ByTheoremL.1,thesignedSPDpseudo-gyrodistancefromYtoanSPDhypergyroplanethatcon-
tainstheoriginandisorthogonaltotheElc axisisgivenby
(i,j)
⟨⌊φ(Y)⌋+log(D(φ(Y))),(cid:0) Loglc (Elc )(cid:1) ⟩
d(Y,H )=
Im (i,j) 21 F
.
Logl Ic m(E (l ic ,j)),Im ∥(cid:0) Logl Ic m(E (l ic ,j))(cid:1)
1
2∥
F
27PublishedasaconferencepaperatICLR2024
AccordingtoourinterpretationofFClayers,
⟨⌊φ(Y)⌋+log(D(φ(Y))),(cid:0) Loglc (Elc )(cid:1) ⟩
v (X)=
Im (i,j) 1
2
F
.
(i,j) ∥(cid:0) Loglc (Elc )(cid:1) ∥
Im (i,j) 1
2
F
Weconsidertwocases:
Case1: i<j.
⟨⌊φ(Y)⌋+log(D(φ(Y))),e eT⟩
v (X)= j i F
(i,j) ∥e eT∥
j i F
=⟨⌊φ(Y)⌋+log(D(φ(Y))),e eT⟩
j i F
=φ(Y) .
(j,i)
Wethushave
φ(Y) =v (X).
(j,i) (i,j)
Case2: i=j.
⟨⌊φ(Y)⌋+log(D(φ(Y))),e eT⟩
v (X)= i i F
(i,j) ∥e eT∥
i i F
=⟨⌊φ(Y)⌋+log(D(φ(Y))),e eT⟩
i i F
=log(φ(Y) ).
(i,i)
Hence
φ(Y) =exp(v (X)).
(i,i) (i,i)
Settingφ(Y)=[y ]m ,theny aregivenby
(i,j) i,j=1 (i,j)

exp(v (X)), ifi=j
 (i,j)
y = v (X), ifi<j
(j,i) (i,j)
0, ifi>j
Sinceφ(Y)istheCholeskyfactorofY,wehave
Y =φ(Y)φ(Y)T,
whichconcludestheproofofProposition3.6.
M PROOF OF THEOREM 3.11
Proof. LetH Wpsd ,P,g beahypergyroplaneinstructurespaceG(cid:102)r n,p×Sym+
p
andX∈G(cid:102)r n,p×Sym+ p.
Bythedefinitionofthepseudo-gyrodistancefunction,
d¯(X,Hpsd,g)=sin(∠XPQ)d(X,P),
W,P
whereQisgivenby
Q= argmax cos(∠QPX)
Q∈Hpsd,g\{P}
W,P
⟨⊖ P⊕ Q,⊖ P⊕ X⟩psd,g
= argmax psd,g psd,g psd,g psd,g .
∥⊖ P⊕ Q∥psd,g.∥⊖ P⊕ X∥psd,g
Q∈Hpsd,g\{P} psd,g psd,g psd,g psd,g
W,P
28PublishedasaconferencepaperatICLR2024
Bythedefinitionsofthebinaryandinverseoperationsinstructurespaces,
⊖ psd,gP⊕ psd,gX=(⊖(cid:101)grU P⊕(cid:101)grU X,⊖ gS
P
⊕ gS X),
⊖ psd,gP⊕ psd,gQ=(⊖(cid:101)grU P⊕(cid:101)grU Q,⊖ gS
P
⊕ gS Q).
Hence
⟨⊖ psd,gP⊕ psd,gX,⊖ psd,gP⊕ psd,gQ⟩psd,g =λ⟨(⊖(cid:101)grU P⊕(cid:101)grU X)(⊖(cid:101)grU P⊕(cid:101)grU X)T,
(⊖(cid:101)grU P⊕(cid:101)grU Q)(⊖(cid:101)grU P⊕(cid:101)grU Q)T⟩gr
+⟨⊖ S ⊕ S ,⊖ S ⊕ S ⟩g.
g P g X g P g Q
LLe ot
gg Inr
,p(cid:0)A
(⊖1
(cid:101)grU
P⊕(cid:101)=
grU
Q)(⊖(cid:101)gL ro Ug Pg Inr
⊕(cid:101),p
g(cid:0) r( U⊖(cid:101)
Qgr
)U
T(cid:1)P
,⊕(cid:101)g ArU 2X)(⊖(cid:101) =grU
P
L⊕(cid:101)
og gr
g
IU
n(X
⊖) gT S(cid:1) P,
⊕
gB
S1
X),
an=
d
B =Logg (⊖ S ⊕ S ). Thenwehave
2 In g P g Q
λ⟨A ,B ⟩ +⟨A ,B ⟩
Q= argmax 1 1 F 2 2 F
(cid:112) (cid:112)
Q∈Hpsd,g\{P} λ∥A 1∥2 F +∥A 2∥2 F. λ∥B 1∥2 F +∥B 2∥2 F
W,P √ √ (9)
⟨[ λA ∥A ],[ λB ∥B ]⟩
= argmax √ 1 2 √ 1 2 F ,
Q∈Hpsd,g\{P}∥[ λA 1∥A 2]∥ F.∥[ λB 1∥B 2]∥
F
W,P
where∥istheconcatenationoperationsimilartooperationconcat (.).
spd
FromtheequationofhypergyroplanesinstructurespaceG(cid:102)r n,p×Sym+ p,
⟨⊖ P⊕ Q,W⟩psd,g =0.
psd,g psd,g
LetW=(U ,S ). Thenwehave
W W
λ⟨(⊖(cid:101)grU P⊕(cid:101)grU Q)(⊖(cid:101)grU P⊕(cid:101)grU Q)T,U W(U W)T⟩gr+⟨⊖ gS
P
⊕ gS Q,S W⟩g =0. (10)
LetW =Loggr (cid:0) U (U )T(cid:1) ,W =Logg (S ). ThenEq.(10)canberewrittenas
1 In,p W W 2 In,p W
λ⟨B ,W ⟩ +⟨B ,W ⟩ =0,
1 1 F 2 2 F
whichisequivalentto
√ √
⟨[ λB ∥B ],[ λW ∥W ]⟩ =0. (11)
1 2 1 2 F
√
Now, the problem in (9) is to find the minimum angle between the vector [ λA ∥A ] and the
1 2
EuclideanhyperplanedescribedbyEq.(11). Thepseudo-gyrodistancefromXtoHpsd,g thuscan
W,P
beobtainedas
√ √
⟨[ λA ∥A ],[ λW ∥W ]⟩
d¯(X,Hpsd,g)= 1√ 2 1 2 F
W,P ∥[ λW ∥W ]∥
1 2 F
λ⟨A ,W ⟩ +⟨A ,W ⟩
= 1 1 F 2 2 F.
(cid:112)
λ∥W ∥2 +∥W ∥2
1 F 2 F
Somesimplemanipulationsleadto
d¯(X,Hpsd,g)=
|λ⟨(⊖(cid:101)grU P⊕(cid:101)grU X)(⊖(cid:101)grU P⊕(cid:101)grU X)T,U WUT W⟩gr+⟨⊖ gS
P
⊕ gS X,S W⟩g|
,
W,P (cid:113)
λ(∥U UT ∥gr)2+(∥S ∥g)2
W W W
whichconcludestheproofofTheorem3.11.
29PublishedasaconferencepaperatICLR2024
N PROOF OF PROPOSITION 3.12
Proof. WeneedthefollowingresultfromNguyen&Yang(2023).
Proposition N.1. Let M and N be two Riemannian manifolds. Let ϕ : M → N be an isometry.
Then
Log (Q)=(Dϕ−1 )(L(cid:103)og (ϕ(Q))),
P ϕ(P) ϕ(P)
where P,Q ∈ M, Dτ (W) denotes the directional derivative of a mapping τ at point R ∈ N
R
along direction W ∈ T RN, Log(.) and L(cid:103)og(.) are the logarithmic maps in manifolds M and N,
respectively.
We adopt the notations in Bendokat et al. (2020). The Riemannian metric gO(.,.) on O is the
Q n
standardinnerproductgiven(Edelmanetal.,1998;Bendokatetal.,2020)as
gO(Ω ,Ω )=Tr(ΩTΩ ),
Q 1 2 1 2
whereQ∈O ,Ω ,Ω ∈T O .
n 1 2 Q n
Let U ∈ G(cid:102)r n,p, D 1,D
2
∈ T UG(cid:102)r n,p. The canonical metric g UG(cid:102)r(D 1,D 2) on G(cid:102)r
n,p
is the restric-
tionoftheRiemannianmetricgO(.,.)tothehorizontalspaceofT O (multipliedby1/2)andis
Q Q n
given(Edelmanetal.,1998;Bendokatetal.,2020)by
gG(cid:102)r(D ,D )=Tr(cid:0) DT(I − 1 UUT)D (cid:1) . (12)
U 1 2 1 n 2 2
Let P ∈ Gr , ∆ ,∆ ∈ T Gr . The canonical metric gGr(∆ ,∆ ) on Gr is the restric-
n,p 1 2 P n,p P 1 2 n,p
tionoftheRiemannianmetricgO(.,.)tothehorizontalspaceofT O (multipliedby1/2)andis
Q Q n
given(Edelmanetal.,1998;Bendokatetal.,2020)by
gGr(∆ ,∆ )= 1 Tr(cid:0) (∆hor)T∆hor(cid:1) ,
P 1 2 2 1,Q 2,Q
where∆hor and∆hor arethehorizontalliftsof∆ and∆ toQ,respectively. Here,Qisrelated
1,Q 2,Q 1 2
toPbyQ=(U U ⊥)andP=UUT,whereU∈G(cid:102)r n,pandU ⊥istheorthogonalcompletionof
U.
DenotebyHor UG(cid:102)r n,pthehorizontalspaceofT UG(cid:102)r n,p. Thenthissubspaceischaracterizedby
Hor UG(cid:102)r
n,p
={U ⊥B|B∈M n−p,p}.
FromEq.(3.2)inBendokatetal.(2020),
gGr(∆ ,∆ )=Tr(cid:0) (∆hor)T∆hor(cid:1) , (13)
P 1 2 1,U 2,U
where∆hor and∆hor arethehorizontalliftsof∆ and∆ toU,respectively.
1,U 2,U 1 2
Therefore,byEq.(12),
gG(cid:102)rn,p(∆hor,∆hor)=Tr(cid:0) (∆hor)T(I − 1 UUT)∆hor(cid:1)
U 1,U 2,U 1,U n 2 2,U
=Tr(cid:0) (∆hor)T∆hor(cid:1)
−
1 Tr(cid:0) (∆hor)TUUT∆hor(cid:1)
1,U 2,U 2 1,U 2,U
=Tr(cid:0) (∆hor)T∆hor(cid:1) − 1 Tr(cid:0) (U B )TUUTU B (cid:1) (14)
1,U 2,U 2 ⊥ 1 ⊥ 2
=Tr(cid:0) (∆hor)T∆hor(cid:1) − 1 Tr(cid:0) BTUTUUTU B (cid:1)
1,U 2,U 2 1 ⊥ ⊥ 2
=Tr(cid:0) (∆hor)T∆hor(cid:1)
,
1,U 2,U
wherethelastequalityfollowsfromthefactthatUTU =0.
⊥
CombiningEqs.(13)and(14),weget
gGr(∆ ,∆ )=gG(cid:102)rn,p(∆hor,∆hor).
P 1 2 U 1,U 2,U
30PublishedasaconferencepaperatICLR2024
ByPropositionN.1,
Logg Pr(F)=(Dτ τ−1(P))(L(cid:103)ogg τr −1(P)(τ−1(F))),
whereP,F∈Gr .
n,p
FromEq.(3.15)inBendokatetal.(2020),
Dτ (W)=RWT +WRT.
R
Therefore
Logg Pr(F)=τ−1(P)(cid:0) L(cid:103)ogg τr −1(P)(τ−1(F))(cid:1)T +L(cid:103)ogg τr −1(P)(τ−1(F))τ−1(P)T,
whichconcludestheproofofProposition3.12.
31