ADAPTIVE GENERALIZED NEYMAN ALLOCATION: LOCAL
ASYMPTOTIC MINIMAX OPTIMAL BEST ARM IDENTIFICATION
APREPRINT
MasahiroKato
DataAnalyticsDepartment,Mizuho–DLFinancialTechnology,Co.,Ltd.
masahiro-kato@fintec.co.jp
May30,2024
ABSTRACT
This study investigates a local asymptotic minimax optimal strategy for fixed-budget best arm
identification(BAI).WeproposetheAdaptiveGeneralizedNeymanAllocation(AGNA)strategyand
showthatitsworst-caseupperboundoftheprobabilityofmisidentifyingthebestarmalignswiththe
worst-caselowerboundunderthesmall-gapregime,wherethegapbetweentheexpectedoutcomes
ofthebestandsuboptimalarmsissmall. OurstrategycorrespondstoageneralizationoftheNeyman
allocationfortwo-armedbandits(Neyman,1934;Kaufmannetal.,2016)andarefinementofexisting
strategiessuchastheonesproposedbyGlynn&Juneja(2004)andShinetal.(2018). Compared
toKomiyamaetal.(2022),whichproposesaminimaxrate-optimalstrategy,ourproposedstrategy
hasatighterupperboundthatexactlymatchesthelowerbound,includingtheconstantterms,by
restrictingtheclassofdistributionstotheclassofsmall-gapdistributions. Ourresultcontributesto
thelongstandingopenissueabouttheexistenceofasymptoticallyoptimalstrategiesinfixed-budget
BAI,bypresentingthelocalasymptoticminimaxoptimalstrategy.
1 Introduction
Thisstudyconsiderstheproblemoffixed-budgetbestarmidentification(BAI).Wepresentanasymptoticallyoptimal
strategyinthesensethattheupperboundalignswiththelowerboundundertheworst-casedistributionwhenthebudget
approachesinfinityandthegapbetweentheexpectedoutcomesofthebestandsuboptimalarmsapproacheszero.
Problemsetting. Inourproblem,weconsideradecision-makerwhoconductsanadaptiveexperimentwithafixed
budget (sample size) T and a fixed set of arms [K] := {1,2,...,K}. Each arm a ∈ [K] has a potential random
outcomeY ∈Y ⊂R,whereY isanoutcomespace(Neyman,1923;Rubin,1974). LetP beajointdistributionof
a
K-potentialoutcomes(Y ,Y ,...,Y ),andP bethemarginaldistributionofY underP foreacha∈[K]. LetP
1 2 K a a P
andE betheprobabilityandexpectationunderP,respectively. UnderP,leta∗(P):=argmax µ (P)bethe
P a∈[K] a
bestarm,whereµ (P):=E [Y ]istheexpectedvalueofY . LetP∗bethedistributionthatgeneratesdataduringan
a P a a
adaptiveexperiment,calledthetruedistribution.
Ineachroundt∈[T]:={1,2,...,T},
1. Potentialoutcomes(Y ,Y ,...,Y )aregeneratedfromP∗;
1,t 2,t K,t
2. Thedecision-makerdrawsarmA ∈[K]basedonpastobservations{(Y ,A )}t−1;
t s s s=1
3. Thedecision-makerobservesthecorrespondingoutcomeY linkedtotheallocatedarmA asY =(cid:80) 1[A =
t t t a∈[K] t
a]Y .
a,t
Attheendoftheexperiment,thedecision-makerestimatesa∗(P∗),denotedbya ∈[K]. Thedecision-maker’sgoal
(cid:98)T
istoidentifythearmwiththehighestexpectedoutcome,minimizingtheprobabilityofmisidentificationP (a ̸=
P∗ (cid:98)T
a∗(P∗))attheendoftheexperiment.
4202
yaM
92
]GL.sc[
1v71391.5042:viXraAdaptiveGeneralizedNeymanAllocation: LocalAsymptoticMinimaxOptimalBestArmIdentificAatiPoRnEPRINT
We define a strategy of a decision-maker as a pair of ((A ) ,a ), where (A ) is the allocation rule, and
t t∈[T] (cid:98)T t t∈[T]
a istherecommendationrule. Formally,withthesigma-algebrasF = σ(A ,Y ,...,A ,Y ),astrategyisapair
(cid:98)T t 1 1 t t
((A ) ,a ),where
t t∈[T] (cid:98)T
• (A ) is an allocation rule, which is F -measurable and allocates an arm A ∈ [K] in each round t using
t t∈[T] t−1 t
observationsuptoroundt−1.
• a isarecommendationrule,whichisanF -measurableestimatorofthebestarma∗usingobservationsuptoround
(cid:98)T T
T.
Wedenoteastrategybyπ. WealsodenoteA anda byAπ andaπ whenweemphasizethatA anda dependonπ.
t (cid:98)T t (cid:98)T t (cid:98)T
For this problem, we aim to derive a lower bound for the probability of misidentification P (a ̸= a∗(P∗)) and
P∗ (cid:98)T
developastrategywhoseprobabilityofmisidentificationalignswiththelowerbound.
Backgroundandrelatedwork. ToevaluatetheexponentialconvergenceofP (aπ ̸=a∗(P∗))underadesigned
P∗ (cid:98)T
strategyπ,weemploythefollowingmeasure,calledthecomplexity:
1
− logP (aπ ̸=a∗(P∗). (1)
T P∗ (cid:98)T
Thecomplexity−1 logP (aπ ̸=a∗(P∗))haswidelybeenemployedintheliteratureonlarge-deviationevaluation
T P∗ (cid:98)T
ofhypothesistesting(Bahadur,1960;vanderVaart,1998),ordinaloptimization(Glynn&Juneja,2004),andBAI
(Kaufmannetal.,2016).
Whenweknowthedistributionalinformationcompletely,wecancomputetheoptimalstrategiesbasedonthelarge-
deviationprinciple(Ga¨rtner,1977;Ellis,1984). WhenY followsaGaussiandistributionforalla∈[K],Chenetal.
a
(2000)developsanasymptoticallylarge-deviationoptimalstrategyforanyGaussiandistribution. Formoregeneral
distributions,Glynn&Juneja(2004)developsoptimalstrategies. Shinetal.(2018)refinestheirstrategies.
However,assumingthatthedistributionalinformationiscompletelyknownistoostrong. Thisassumptionimpliesthat
weknowvariousinformation,suchasthemean,variance,andbestarm,inadvanceofanexperiment. Ourinterest
ratherliesindesigningexperimentswithoutassumingsuchknowledge. InBAI,variousstrategieshavebeenproposed
forthattask(Bubecketal.,2009;Karninetal.,2013),butitisstillunderinvestigationwhatstrategyisoptimalinsome
sense(Ariuetal.,2021;Qin,2022;Komiyamaetal.,2022;Degenne,2023;Wangetal.,2024).
Todiscussoptimalexperiments,existingstudiesdiscusslowerboundsforthecomplexity(1)andthenexploreastrategy
whoseprobabilityofmisidentificationalignswiththelowerbound. Kaufmannetal.(2016)developsageneraltool
fordevelopinglowerboundsinBAIthatholdsbasedonthechange-of-measurearguments(Lai&Robbins,1985),
withoutproposingstrategieswhoseprobabilityofmisidentificationalignswiththelowerbound. Moreover,theyalsodo
notproposeaspecificlowerboundforfixed-budgetBAI.UsingtheargumentsinKaufmannetal.(2016),Garivier&
Kaufmann(2016)conjecturesalowerboundintheirconclusionsection,butKaufmann(2020)pointsoutthatthere
existsthereverseKLproblem,whichimpliesthattheconjecturedlowerboundinGarivier&Kaufmann(2016)maynot
work.
Theexistenceofoptimalstrategieshaslongbeenanopenissue(Kaufmann,2020),wherean“optimal”strategyimplies
thatitsupperboundalignswiththelowerbounddevelopedbyKaufmannetal.(2016). Thisproblemisaddressedby
Ariuetal.(2021),acommentpaperforKasy&Sautmann(2021). BasedontheargumentsaboutKasy&Sautmann
(2021), Ariu et al. (2021) shows that there exists a distribution under which we can construct a lower bound that
is larger than the lower bound in Kaufmann et al. (2016); that is, we cannot design a strategy whose probability
ofmisidentification alignswith thelowerbound inKaufmann etal. (2016)for any distribution ina givenclassof
distributions. ThisresultisfurtherrefinedandconfirmedbyDegenne(2023).
Motivatedbytheseimpossibilityresults,Komiyamaetal.(2022)andKomiyamaetal.(2023)developminimaxand
Bayesoptimalstrategies. However,theirstrategiesstillhavegapsbetweentheirlowerandupperbounds. Thereare
variousotherstudiesthatattempttoaddresstheopenissue(Barrieretal.,2023;Atsidakouetal.,2023;Nguyenetal.,
2024;Kato,2024b;Wangetal.,2024).
Tothebestofourknowledge,thereisnolowerandupperbound(strategy)thatstrictlymatchincludingtheconstant
terms.NotethatinBAIwithfixedconfidence,suchoptimalstrategieshavebeenproposed(Garivier&Kaufmann,2016).
InBayesianBAI,theasymptoticallyoptimalstrategieshavebeenproposedinthesenseoftheposteriorconvergence
rate,whichdoesnotimplytheasymptoticoptimalityinfixed-budgetBAI(Kasy&Sautmann,2021;Ariuetal.,2021).
Contribution. Weproposeanasymptoticallyoptimalstrategywhoseprobabilityofmisidentificationalignswith
a lower bound under the worst-case distribution and the small-gap regime, where the gaps between the expected
2AdaptiveGeneralizedNeymanAllocation: LocalAsymptoticMinimaxOptimalBestArmIdentificAatiPoRnEPRINT
outcomesofthebestandsuboptimalarmsaresmall. Werefertothisoptimalityasthelocal(asymptotic)minimax
optimality.
Thisstudyestablishestheworst-caselowerboundunderthesmall-gapregime,whichischaracterizedbythevariances
oftheoutcomeY . Then,motivatedbythelowerbound,wedeveloptheAdaptiveGeneralizedNeymanallocation
a
(AGNA)strategy. Weprovethatitsprobabilityofmisidentificationalignswithourdevelopedlowerboundintheworst
caseunderthesmall-gapregime.
WerefertoourproposedstrategyastheAGNAstrategysinceitisageneralizationoftheNeymanallocationstrategy
thatisproventobeoptimalintwo-armed(Gaussian)bandits(Neyman,1934;Glynn&Juneja,2004;Hahnetal.,2011;
Kaufmann et al., 2016; Kato et al., 2020). The extension of the Neyman allocation to multi-armed cases is also a
longstandingopenissue. Inthisstudy,weaddressthisissuebyshowinganaturalextensionoftheNeymanallocation.
OurresultdoesnotcontradictthoseinAriuetal.(2021)andDegenne(2023),whichfindthatthereexistsadistribution
underwhichwecannotdesignastrategywhoseprobabilityofmisidentificationalignswiththelowerboundinKaufmann
et al. (2016). For their claim, we point out that (i) given a restricted class of distributions, (ii) for the worst-case
distributionintheclass,(iii)wecandesignastrategywhoseupperboundalignswiththelowerbound. Therestricted
classischaracterizedbythesmall-gapregime.
To the best of our knowledge, asymptotically optimal strategies whose upper bound exactly aligns with the lower
boundhavenotbeenpreviouslyproposedinfixed-budgetBAI.Furthermore,ourargumentisnotrestrictedtoacertain
distributionandcanbewidelyusedbecauseweapproximatethelowerandupperboundsbyGaussianonesunderthe
small-gapregime.
Insummary,ourcontributionsareasfollows:
1. Atightworst-caselowerboundformulti-armedbandits(Theorem2.3).
2. TheAGNAstrategy(Algorithm1).
3. TheupperboundoftheAGNAstrategy,whichalignswiththelowerboundunderthesmall-gapregime(Theorem4.1).
4. Applicabilityforvariousdistributions.
Whenestimatingthemeanoutcome,weemploytheAdaptiveAugmentedInverseProbabilityWeighting(A2IPW)
estimatorfromKatoetal.(2020)andCooketal.(2023),whichhasbeenutilizedinexistingstudiesinvariousfields
(vanderLaan,2008). OurargumentaboutthelowerboundisinspiredbytheargumentsinKato(2024b),Komiyama
etal.(2022),andDegenne(2023). Theupperboundunderthesmall-gapregimeisderivedusingtheresultsinKato
(2024a).
Organization. InSection2,wedeveloptheworst-caselowerboundunderthesmall-gapregime. Then,wedevelop
theAGNAstrategyinSection3. InSection4,wederiveanupperboundoftheAGNAstrategyandshowthelocal
asymptoticminimaxoptimalitybyprovingthatthelowerandupperboundsmatchasthebudgetapproachesinfinityand
thegapapproacheszero.
2 Worst-caseLowerBound
Thissectionprovidesalowerboundfortheprobabilityofmisidentification. Wefocusontheworst-caselowerbound
underthesmall-gapregime. Lowerboundsalsoprovideintuitionaboutthesamplingruleaswellasthetheoretical
performancelimit.
First,wedefineaclassofdistributions,forwhichshowlowerandupperboundsfortheworst-casedistribution.
Definition2.1(Mean-parameterizeddistributionswithfinitevariances). LetΘ⊂Rbeacompactparameterspace,
andletY bethesupportofY foralla ∈ [K]. Letσ : Θ → (0,∞)beavariancefunctionthatiscontinuouswith
a a
respecttoθ ∈Θ. LetP beadistributionofY parameterizedbyµ ∈Θ. Wedefinebanditmodels(P ) as
a,µa a a a a∈[K]
(cid:110) (cid:111)
P :=P (σ (·),Θ,Y):= P : µ ∈Θ, (1), (2), and(3) , (2)
a a a a,µa a
where(1),(2),and(3)aredefinedasfollows:
(1) A distribution P has a probability mass function or probability density function, denoted by f (y | µ ).
µa,a a a
Additionally,f (y |µ )>0holdsforally ∈Y andµ ∈Θ.
a a a
(2) ThevarianceofY underP isσ2(µ ). Foreachµ ∈Θandeacha∈[K],theFisherinformationI (µ )>0
a a,µa a a a a a
ofP existsandisequaltotheinverseofthevariance1/σ2(µ ).
a,µa a a
3AdaptiveGeneralizedNeymanAllocation: LocalAsymptoticMinimaxOptimalBestArmIdentificAatiPoRnEPRINT
...
(3) Letℓ (µ )=ℓ (µ |y)=logf(y |µ )bethelikelihoodfunctionofP ,andℓ˙ ,ℓ¨ ,and ℓ bethefirst,second,
a a a a a
(cid:8) (cid:9)
µa,a a a a
andthirdderivativesofℓ . Thelikelihoodfunctions ℓ (µ ) arethreetimesdifferentiableandsatisfythe
a a a a∈[K]
following:
(cid:104) (cid:105)
(a) E ℓ˙ (µ ) =0;
Pµa,a a a
(cid:104) (cid:105)
(b) E ℓ¨ (µ ) =−I (µ )=1/σ2(µ );
Pµa,a a a a a a a
(c) Foreachµ ∈Θ,thereexistaneighborhoodU(θ)andafunctionu(y |µ )≥0,andthefollowingholds:
a a
(cid:12) (cid:12)
i. (cid:12)ℓ¨ (τ)(cid:12)≤u(y |θ) forU(µ );
(cid:12) a (cid:12) a
ii. E [u(Y |µ )]<∞.
Pµa,a a
LetP beajointdistributionof(Y ) ,underwhichthemarginaldistributionofY isgivenasP ∈P forall
µ a a∈[K] a a,µa a
a∈[K]. Wedefinethefollowingclassofdistributionsfor(Y ) :
a a∈[K]
(cid:0) (cid:1) (cid:110) (cid:111)
P b,θ,∆ := P : ∀a∈[K] P ∈P , ν =θ, ∀a∈[K]\{b} 0<θ−ν ≤∆ .
ν a,νa a b a
(cid:0) (cid:1) (cid:8) (cid:0) (cid:1) (cid:9)
WealsodefineP ∆ := P ∈∪ P b,θ,∆ : θ ∈Θ .
b∈[K]
Forexample,GaussiandistributionswithfixedvariancesandBernoullidistributionsbelongtothisclass. InGaussian
distributions,thevariancesareusuallydefinedtobeindependentofµ . InBernoullidistributions,thevariancesare
a
µ (1−µ ).
a a
Next,wedefineaclassofstrategiesandlatershowtheasymptoticoptimalityforastrategybelongingtothisclass. This
studyfocusesonconsistentstrategiesthatrecommendthebestarmwithprobabilityoneasT →∞(Kaufmannetal.,
2016).
Definition2.2(Consistentstrategy). WesaythatastrategyπisconsistentifP (aπ =a∗(P∗))→1asT →∞for
P∗ (cid:98)T
anytruedistributionP∗suchthata∗(P∗)isunique. WedenotetheclassofallpossibleconsistentstrategiesbyΠconst.
Then,wederivealowerboundthatanyconsistentstrategysatisfiesgiventhemean-parameterizeddistributions. To
deriveatightlowerbound,weemploythechange-of-measureargument(LeCam,1972,1986;Lehmann&Casella,
1998;vanderVaart,1991,1998;Lai&Robbins,1985). Usingthechange-of-measureargument,Kaufmannetal.(2016)
developedageneralframeworkfordiscussinglowerbounds. Intheirresults,lowerboundsarecharacterizedbythe
Kullback–Leibler(KL)divergenceKL(P,Q)betweentwodistributionsP andQ. WeareusuallyinterestedintheKL
divergencebetweenP∗andahypothesisQ. Thislowerboundalsoprovidesintuitionaboutthesamplingruleaswellas
thetheoreticalperformancelimit.
IfthedistributionP∗ofthedata-generatingprocessisknown,wecancomputetheKLdivergenceexactly,allowing
usto designanasymptoticallyoptimalstrategybasedon theKLdivergence, asshownbyGlynn &Juneja(2004).
However, P∗ isusuallyunknown(ifP∗ wereknown,wewouldknowwhicharmisthebestarminadvanceofan
experiment). InBAI,ourinterestliesindesigningbetterstrategieswithoutknowingP∗inadvance.
ToreflectthefactthatP∗isunknowninevaluation,weemploythestatisticaldecision-makingframeworkpioneeredby
Wald(1945). Amongseveralcriteria,suchasBayesianevaluation(Komiyamaetal.,2023),wefocusontheworst-case
orminimaxanalysis. Weevaluatetheworst-caseprobabilityofmisidentificationdefinedas
1
lim inf limsup− logP (aπ ̸=a∗(P)).
2 P (cid:98)T
∆→0P∈∪b∈[K]\{a∗(P∗)}P(b,θ∗,∆) T→∞ ∆ T
Thisworst-casecomplexityevaluatestheprobabilityofmisidentificationundertheworst-casescenariowherethebest
armisnota∗(P∗)andthegapsaresmall. Theinfimuminf representstheworstcaseconcerningthe
P∈∪b∈[K]\{a∗(P∗)}
bestarm. Specifically,thetruebestarmisa∗(P∗)underP∗,andweensurerobustnessagainstallcaseswherethebest
armisnota∗(P∗),thusguaranteeingrobustnessagainstthemisidentificationofa∗(P∗)duringanexperiment. The
small-gapregime∆→0alsoindicatestheworstcaseconcerningthegaps.
Fortheworst-casecomplexity,thefollowingtheoremprovidestheworst-caselowerbound. Theproofisshownin
AppendixA.
Theorem2.3(Worst-caselowerbound). ForeachP∗ ∈P(cid:0) ∆(cid:1) ,anyconsistentstrategyπ ∈Πconst (Definition2.2)
satisfies
1
lim inf limsup− logP (aπ ̸=a∗(P))≤V∗(P∗),
2 P (cid:98)T
∆→0P∈∪b∈[K]\{a∗(P∗)}P(b,θ∗,∆) T→∞ ∆ T
4AdaptiveGeneralizedNeymanAllocation: LocalAsymptoticMinimaxOptimalBestArmIdentificAatiPoRnEPRINT
whereθ∗ =µ (P∗),and
a∗(P∗)
1
V∗(P∗):= .
2(cid:16)
σ
(θ∗)+(cid:113)(cid:80) σ2(θ∗)(cid:17)2
a∗(P∗) a∈[K]\{a∗(P∗) a
Ourlowerbounddependsontheexpectedoutcomeofthebestarmθ∗ =max µ (P∗)andvariances(σ2(θ∗)) ,
b∈[K] b a a∈[K]
whiletakingtheworstcase.
Whileseveralexistingstudies,suchasCarpentier&Locatelli(2016),Komiyamaetal.(2022),Yang&Tan(2022),
andDegenne(2023),haveintroducedtheminimaxevaluationframework,thereisaconstantgapbetweenthelower
andupperbounds,andonlytheleadingfactorsinlowerandupperboundsmatch. Weconjecturethatthisisduetothe
estimationerrorofP∗affectingtheevaluation. Toaddressthisissue,weconsiderarestrictedclassofP,characterized
bythesmallgap;thatis,µ −µ issufficientlysmallforalla∈[K]. Underthisregime,wecanobtainmatching
a∗(P∗) a
lowerandupperbounds,asshowninSection4.2. Werefertothisaslocalminimaxoptimality.
3 TheAGNAStrategy
Basedonthelowerbounds,wedesigntheAGNAstrategy,whichconsistsofthesamplingruleusingthegeneralized
NeymanallocationandtherecommendationruleusingtheAdaptiveAugmentedInverseProbabilityWeighting(A2IPW)
estimator. OurstrategydependsonhyperparametersC andC ,whichareintroducedfortechnicalpurposestobound
µ σ2
theestimatorsandanylargepositivevaluecanbeused. Thepseudo-codeisshowninAlgorithm1.
3.1 SamplingRule
Let(cid:0) σ2(cid:1) bethevariancesofthetruedistributionP∗.
a a∈[K]
First,wedefineatargetallocationratio,whichisusedtodetermineourallocationrule,asfollows:
1
wAGNA :=argmax min ,
w∈W
a∈[K]\{a∗(P∗)}σ a2 ∗(P∗)/w a∗(P∗)+σ a2/w
a
where
σ
wAGNA = a∗(P∗) , (3)
a∗(P∗)
σ
+(cid:113)(cid:80)
σ2
a∗(P∗) c∈[K]\{a∗(P∗)} c
σ2/(cid:113)(cid:80)
σ2
(cid:16) 1−wAGNA(cid:17)
σ2
a c∈[K]\{a∗(P∗)} c a∗(P∗) a
wAGNA = = ∀a∈[K]\{a∗(P∗)}.
a σ a∗(P∗)+(cid:113)(cid:80) c∈[K]\{a∗(P∗)}σ c2 (cid:80) c∈[K]\{a∗(P∗)}σ c2
ThistargetallocationratioisgivenfromtheproofofthelowerboundinTheorem2.3. Weconjecturethatastrategy
usingtheratio(3)isoptimalanddesignastrategyusingit. Weconfirmthatsuchastrategyisactuallyoptimalby
checkingthematchingofthelowerandupperbounds.
Duringourexperiment,weestimatewAGNAusingobserveddata. SincewAGNAonlydependsonthevariances,we
canimplementsuchanalgorithmbysequentiallyestimatingthevariance.
Byusingthetargetallocationratio,wedefineoursamplingrule. Oursamplingruleischaracterizedbyasequence
{wAGNA}T ,wherewAGNA =(cid:0) wAGNA(cid:1) ∈∆K.ThefirstKroundsareinitializationphases.Ineachroundt=
(cid:98)t t=1 (cid:98)t (cid:98)a,t a∈[K]
1,...,K,wesetwAGNA =···=wAGNA =1/K andsampleA =t. Next,ineachroundt=K+1,K+2,...,T,
(cid:98)1,t (cid:98)K,t t
we estimate σ2 by using the past observations up to (t−1)-th round F . By using the estimate σ2 , we obtain
a t−1 (cid:98)a,t
wAGNAas
(cid:98)t
σ
wAGNA = (cid:98)(cid:98)at,t , (4)
(cid:98) (cid:98)at,t σ +(cid:113) (cid:80) σ2
(cid:98)(cid:98)at,t c∈[K]\{(cid:98)at}(cid:98)c,t
(cid:113)
wAGNA = σ (cid:98)a2 ,t/ (cid:80) c∈[K]\{(cid:98)at}σ (cid:98)c2 ,t = (cid:0) 1−w (cid:98) (cid:98)aA tGNA(cid:1) σ (cid:98)a2 ,t ∀a∈[K]\{a },
(cid:98)a,t σ (cid:98)(cid:98)at,t+(cid:113) (cid:80) c∈[K]\{(cid:98)at}σ (cid:98)c2
,t
(cid:80) c∈[K]\{(cid:98)at}σ (cid:98)c2 ,t (cid:98)t
5AdaptiveGeneralizedNeymanAllocation: LocalAsymptoticMinimaxOptimalBestArmIdentificAatiPoRnEPRINT
Algorithm1AGNAStrategy
Parameter: PositiveconstantsC andC .
µ σ2
Initialization:
Foreacht=1,2,...,K,drawA =t. Fora∈[K],setw =1/K.
t (cid:98)a,t
fort=3toT do
EstimatewAGNAfollowing(4).
DrawA =awithprobabilityw .
t (cid:98)a,t
ObserveY =(cid:80) 1[A =a]Y .
t a∈[K] t a,t
endfor
ConstructµA2IPW fora∈[K]. following(5).
(cid:98)a,T
Recommenda following(6).
(cid:98)T
wherea =argmax µ andµ = 1 (cid:80)t−11[A =a]Y . Then,wesamplearmawithprobabil-
(cid:98)t a∈[K](cid:101)a,t (cid:101)a,t (cid:80) st− =1 11[As=a] s=1 s a,s
itywAGNA.
(cid:98)a,t
Inthisstudy,wedefinetheestimatorσ2 asσ2 := thre(σ2 ,1/C ,C ),whereσ2 := v −µ2 ,andν =
(cid:98)a,t (cid:98)a,t (cid:101)a,t σ2 σ2 (cid:101)a,t (cid:101)a,t (cid:101)a,t (cid:101)a,t
1 (cid:80)t−11[A =a]Y2 .
(cid:80)t s− =1 11[As=a] s=1 s a,s
3.2 RecommendationRule
Attheendoftheexperiment,weestimatetheexpectedoutcomeµ foreacha ∈ [K]andrecommendarmwiththe
a
highestestimatedexpectedoutcome. WefollowtherecommendationrulethatemploysanA2IPWestimator,proposed
inKatoetal.(2020)andKato(2024a).
Withatruncatedversionoftheestimatedexpectedrewardµ :=thre(µ ,−C ,C ),wedefinetheA2IPWestimator
(cid:98)a,t (cid:101)a,t µ µ
ofµ foreacha∈[K]as
a
µA2IPW :=
1 (cid:88)T (cid:40) 1[A
t
=a](cid:0) Y a,t−µ (cid:98)a,t(cid:1)
+µ
(cid:41)
. (5)
(cid:98)a,T T wAGNA (cid:98)a,t
t=1 (cid:98)a,t
Attheendoftheexperiment(aftertheroundt=T),werecommenda as
(cid:98)T
aAGNA :=argmaxµA2IPW (6)
(cid:98)T (cid:98)a,T
a∈[K]
TheA2IPWestimatorconsistsofamartingaledifferencesequence(MDS),whichallowsustoapplyvarioustoolsfor
asymptoticanalysisandsimplifytheanalysis. UsingthepropertyofanMDS,fortheA2IPWestimator,Katoetal.
(2020)showsanasymptoticnormalitywiththecentrallimittheorem,andKato(2024a)developsalarge-deviation
bound. Whileweconjecturethatanaivesamplemeanalsohasthesameasymptotictheoreticalproperty,itisnoteasy
toshow,aswellasHiranoetal.(2003).
4 Worst-CaseUpperBoundandLocalAsymptoticMinimaxOptimality
ThissectionprovidesanupperboundoftheprobabilityofmisidentificationoftheAGNAstrategy.
4.1 Worst-CaseUpperBound
ForacompactparameterspaceΘ⊂R,letusdefine
(cid:0) (cid:1) (cid:110) (cid:111)
P b,θ,∆ := P : ∀a∈[K] P ∈P , ν =θ, ∀a∈[K]\{b} ∆≤θ−ν
ν a,νa a b a
(cid:0) (cid:1) (cid:8) (cid:0) (cid:1) (cid:9)
WealsodefineP ∆ := P ∈∪ P b,θ,∆ : θ ∈Θ .
b∈[K]
Then,weshowthefollowingupperbound. OurproofisbasedonKato(2024a). Forcompleteness,weshowtheproof
inAppendixB.
6AdaptiveGeneralizedNeymanAllocation: LocalAsymptoticMinimaxOptimalBestArmIdentificAatiPoRnEPRINT
Theorem4.1(UpperboundoftheAGNAstrategy). Supposethatµ ∈(−C ,C )andσ (µ )∈(1/C ,C )hold
a µ µ a a σ2 σ2
foralla∈[K]andallµ ∈Θ. Then,foreachP∗ ∈P(cid:0) ∆(cid:1) ,theAGNAstrategysatisfies
a
lim min limsup− 1 logP (cid:0) aAGNA ̸=a∗(P)(cid:1) ≥V∗(P∗).
∆→0 P∈∪b∈[K]\{a∗(P∗)}P(cid:0) b,θ∗,∆(cid:1) T→∞ ∆2T P (cid:98)T
Here,C andC areusedfortechnicalpurpose,andwecansetsufficientlylargevaluesforthem.
µ σ2
4.2 LocalMinimaxOptimality
ThissectionprovesthelocalminimaxoptimalityofourproposedAGNAstrategy. Letusdefine
(cid:0) (cid:1) (cid:110) (cid:111)
P b,θ,∆,∆ := P : ∀a∈[K] P ∈P , ν =θ, ∀a∈[K]\{b} ∆≤θ−ν ≤∆ .
ν a,νa a b a
(cid:0) (cid:1) (cid:8) (cid:0) (cid:1) (cid:9)
WealsodefineP ∆,∆ := P ∈∪ P b,θ,∆,∆ : θ ∈Θ .Then,thefollowinglemmaholdsfromTheorems2.3
b∈[K]
and4.1.
Lemma4.2(Lowerandupperbound). ForeachP∗ ∈P(cid:0) ∆,∆(cid:1) ,anyconsistentstrategyπ ∈Πconst(Definition2.2)
satisfies
1
lim inf limsup− logP (aπ ̸=a∗(P))≤V∗(P∗),
2 P (cid:98)T
∆,∆→0P∈∪b∈[K]\{a∗(P∗)}P(b,θ∗,∆,∆) T→∞ ∆ T
whereθ∗ := µ (P∗). Additionally,ifµ ∈ (−C ,C )andσ ∈ (1/C ,C )holdforalla ∈ [K],thenthe
a∗(P∗) a µ µ a σ2 σ2
AGNAstrategysatisfies
lim min limsup− 1 logP (cid:0) aAGNA ̸=a∗(P)(cid:1) ≥V∗(P∗).
∆,∆→0P∈∪b∈[K]\{a∗(P∗)}P(b,θ∗,∆,∆) T→∞
∆2T P (cid:98)T
Then,thefollowingresultshowsthattheprobabilityofmisidentificationofourproposedAGNAstrategyalignswithour
derivedlowerboundundertheworstcaseandthesmall-gapregime;thatis,ourproposedstrategyislocalasymptotic
minimaxoptimal. WeillustratetheconceptofthelocalasymptoticminimaxoptimalinFigure1.
Theorem4.3(Localasymptoticminimaxoptimality). ForeachP∗ ∈P(cid:0) ∆,∆(cid:1) ,itholdsthat
sup lim inf limsup− 1 logP (cid:0) aπ ̸=a∗(P)(cid:1)
2 P (cid:98)T
π∈Πconst∆,∆→0P∈∪b∈[K]\{a∗(P∗)}P(b,θ∗,∆,∆) T→∞ ∆ T
≤V∗(P∗)≤ lim inf limsup− 1 logP (cid:0) aAGNA ̸=a∗(P)(cid:1) .
∆,∆→0P∈∪b∈[K]\{a∗(P∗)}P(b,θ∗,∆,∆) T→∞
∆2T P (cid:98)T
Notethattheupperandlowerboundsareflippedduetothepropertyoflog(x).
ThistheoremisadirectconsequenceofLemma4.2.
In two-armed Bernoulli bandits, we draw each arm with the ratio wAGNA =
1
wAGNA = 1 (uniformallocation)becausethevariancesbecomethesameunder
2 2
thesmall-gapregime. ThisresultiscompatiblewiththoseinKaufmannetal.
(2016)andWangetal.(2024). Incontrast,inmulti-armedBernoullibanditswith
K ≥3,theratiobecomeswAGNA = 1√ andwAGNA = √ 1 √ forall
a∗(P∗) 1+ K a K(1+ K)
a∈[K]\{a∗(P∗)}. Tothebestofourknowledge,thisratiohasnotbeenproven
Figure 1: Illustration of the local
tobeoptimalintheliteratureofBAI.
asymptoticminimaxoptimality.
5 SimulationStudies
Inthissection,weinvestigatetheempiricalperformanceofourproposedAGNAstrategy. WecompareAGNAwiththe
Uniform-EBAstrategy(Uniform,Bubecketal.,2011),whichallocatesarmswiththeAGNAgivenknownvariances
(GNA);anequalallocationratio(1/K);thesuccessiverejectsstrategy(SR,Audibertetal.,2010);thelarge-deviation
optimal(GJ)strategyproposedbyGlynn&Juneja(2004);andthesuccessivehalving(Karninetal.,2013)usingthe
variances(SHVar,Lalithaetal.,2023).
7AdaptiveGeneralizedNeymanAllocation: LocalAsymptoticMinimaxOptimalBestArmIdentificAatiPoRnEPRINT
Figure 2: The results with σ = 5. The left and right columns are the results with the first and second cases,
respectively. We conduct 250 independent trials and report the empirical probability of misidentification at T ∈
{100,200,300,...,49900,50000}.
Figure 3: The results with σ = 10. The left and right columns are the results with the first and second cases,
respectively. We conduct 250 independent trials and report the empirical probability of misidentification at T ∈
{100,200,300,...,49900,50000}.
TheGJstrategyisanoraclestrategythatisproventobeasymptoticallyoptimalincaseswherewehavefullknowledge
ofthedistributions,includingmeanparametersandtheidentityofthebestarm. Notethatthisstrategyispractically
infeasible. TheGNAstrategydoesnotestimatevariancesbutdirectlyallocatesarmsaccordingtotheratiowAGNA
usingknownvariances.
Let K ∈ {3,5}. The best arm is arm 1 with µ = µ (P∗) = 1. We consider two cases. In the first case, we set
1 1
µ =µ (P∗)=0.95foralla∈[K]\{1}. Inthesecondcase,wesetµ =µ (P∗)=0.95andchooseµ =µ (P∗)
a a 2 2 a a
fromauniformdistributionwithsupport[0.90,0.95]foralla∈[K]\{1,2}.
Thevariancesaregivenasapermutationoftheset{σ,σ,σ ,...,σ },whereσischosenfrom{5,10},σ =0.1,
(3) (K)
andσ ischosenfromauniformdistributionwithsupport[σ,σ].
(3)
We continue the strategies until T = 50000. We conduct 100 independent trials for each setting. For each T ∈
{100,200,300,··· ,49900,50000},wecomputetheempiricalprobabilityofmisidentification. Weplottheempirical
probabilityofmisidentificationpinFigures3–2. Notethatthevarianceoftheempiricalprobabilityofmisidentification
(cid:98)
pisp(1−p). Thus,pprovidessufficientinformationaboutitsdistribution,sowedonotshowothergraphssuchasbox
(cid:98) (cid:98) (cid:98) (cid:98)
plotsandconfidenceintervals.
Accordingtoourresultsandexistingstudies,wetheoreticallyexpectthehighestperformancefromtheGJstrategy,
followedbytheGNAstrategyinlargesamples. OurAGNAstrategyfollowsthese,withtheotherstrategiestrailing.
Ourempiricalresultsalignwiththeoreticalexpectations. Interestingly,theAGNAstrategyoftenoutperformstheGJ
strategy. In finite samples, we observe cases where the SR and Uniform strategies outperform the GJ and AGNA
strategies. However,inlargesamples,theGJandAGNAstrategiestendtooutperformthem. Thisresultimpliesthat
ourproposedstrategiesareasymptoticallyoptimalbutcanbesuboptimalinfinitesamples.
8AdaptiveGeneralizedNeymanAllocation: LocalAsymptoticMinimaxOptimalBestArmIdentificAatiPoRnEPRINT
Infuturework,itwouldbeinterestingtoinvestigatethefinitesamplepropertiesoftheAGNAstrategyandfinitesample
optimalstrategies. Katoetal.(2020)explorethefinitesamplepropertiesoftheA2IPWstrategy,whichhasbeenrefined
byCooketal.(2023),usingtheconcentrationinequalitiesproposedbyBalsubramani&Ramdas(2016)andHoward
etal.(2021). Inthestudyoffinitesampleoptimalstrategies,thelowerboundbyCarpentier&Locatelli(2016)is
insightful,asithasalreadybeenusedinexistingstudies(Yang&Tan,2022;Ariuetal.,2021;Degenne,2023).
6 Conclusion
This study developed the AGNA strategy for the fixed-budget BAI problem. We show that its upper bound on
the probability of misidentification aligns with the worst-case lower bound. This result provides a solution to the
longstandingopenproblemregardingtheexistenceofoptimalstrategiesinfixed-budgetBAI.Ariuetal.(2021)and
Degenne(2023)identifieddistributionsunderwhichanotherlowerbound,largerthanthelowerboundofKaufmann
et al. (2016), can be constructed. In contrast, by restricting the class of distributions to the small-gap regime, we
eliminatedsuchinstancesanddevelopedalocalasymptoticminimaxoptimalstrategy.
References
Kaito Ariu, Masahiro Kato, Junpei Komiyama, Kenichiro McAlinn, and Chao Qin. Policy choice and best arm
identification: Asymptoticanalysisofexplorationsampling,2021. arXiv:2109.08229.
AlexiaAtsidakou,SumeetKatariya,SujaySanghavi,andBranislavKveton. Bayesianfixed-budgetbest-armidentifica-
tion,2023. arXiv:2211.08572.
Jean-YvesAudibert,Se´bastienBubeck,andRemiMunos.Bestarmidentificationinmulti-armedbandits.InConference
onLearningTheory,pp.41–53,2010.
R.R.Bahadur. StochasticComparisonofTests. TheAnnalsofMathematicalStatistics,31(2):276–295,1960.
AkshayBalsubramaniandAadityaRamdas. Sequentialnonparametrictestingwiththelawoftheiteratedlogarithm. In
AlexanderT.IhlerandDominikJanzing(eds.),ConferenceonUncertaintyinArtificialIntelligence,2016.
AntoineBarrier,Aure´lienGarivier,andGillesStoltz. Onbest-armidentificationwithafixedbudgetinnon-parametric
multi-armedbandits. InInternationalConferenceonAlgorithmicLearningTheory(AISTATS),2023.
Se´bastienBubeck,Re´miMunos,andGillesStoltz. Pureexplorationinmulti-armedbanditsproblems. InAlgorithmic
LearningTheory,pp.23–37.SpringerBerlinHeidelberg,2009.
Se´bastienBubeck,Re´miMunos,andGillesStoltz. Pureexplorationinfinitely-armedandcontinuous-armedbandits.
TheoreticalComputerScience,2011.
OvidiuCalinandConstantinUdris¸te. GeometricModelinginProbabilityandStatistics. MathematicsandStatistics.
SpringerInternationalPublishing,2014.
AlexandraCarpentierandAndreaLocatelli. Tight(lower)boundsforthefixedbudgetbestarmidentificationbandit
problem. InCOLT,2016.
Chun-Hung Chen, Jianwu Lin, Enver Yu¨cesan, and Stephen E. Chick. Simulation budget allocation for further
enhancingtheefficiencyofordinaloptimization. DiscreteEventDynamicSystems,10(3):251–270,2000.
Thomas Cook, Alan Mishler, and Aaditya Ramdas. Semiparametric efficient inference in adaptive experiments.
In NeurIPS 2023 Workshop on Adaptive Experimental Design and Active Learning in the Real World, 2023.
a]rXiv:2311.18274.
Re´myDegenne. Ontheexistenceofacomplexityinfixedbudgetbanditidentification. InConferenceonLearning
Theory,volume195,pp.1131–1154.PMLR,2023.
Hanna Do¨ring, Sabine Jansen, and Kristina Schubert. The method of cumulants for the normal approximation.
ProbabilitySurveys,19(none):185–270,2022.
JohnDuchi. Lecturenotesonstatisticsandinformationtheory,2023. URLhttps://web.stanford.edu/class/
stats311/lecture-notes.pdf.
RichardS.Ellis. LargeDeviationsforaGeneralClassofRandomVectors. TheAnnalsofProbability,12(1):1–12,
1984.
Aure´lienGarivierandEmilieKaufmann. Optimalbestarmidentificationwithfixedconfidence. InConferenceon
LearningTheory,2016.
Ju¨rgenGa¨rtner. Onlargedeviationsfromtheinvariantmeasure. TheoryofProbability&ItsApplications,22(1):24–39,
1977.
9AdaptiveGeneralizedNeymanAllocation: LocalAsymptoticMinimaxOptimalBestArmIdentificAatiPoRnEPRINT
PeterGlynnandSandeepJuneja. Alargedeviationsperspectiveonordinaloptimization. InProceedingsofthe2004
WinterSimulationConference,volume1.IEEE,2004.
JinyongHahn,KeisukeHirano,andDeanKarlan. Adaptiveexperimentaldesignusingthepropensityscore. Journalof
BusinessandEconomicStatistics,2011.
KeisukeHirano,GuidoImbens,andGeertRidder. Efficientestimationofaveragetreatmenteffectsusingtheestimated
propensityscore. Econometrica,2003.
Steven R. Howard, Aaditya Ramdas, Jon D. McAuliffe, and Jasjeet S. Sekhon. Time-uniform, nonparametric,
nonasymptoticconfidencesequences. AnnalsofStatistics,2021.
ZoharKarnin,TomerKoren,andOrenSomekh. Almostoptimalexplorationinmulti-armedbandits. InInternational
ConferenceonMachineLearning,2013.
MaximilianKasyandAnjaSautmann. Adaptivetreatmentassignmentinexperimentsforpolicychoice. Econometrica,
89(1):113–132,2021.
MasahiroKato. Locallyoptimalfixed-budgetbestarmidentificationintwo-armedgaussianbanditswithunknown
variances,2024a. arXiv:2312.12741.
Masahiro Kato. Worst-case optimal multi-armed gaussian best arm identification with a fixed budget, 2024b.
arXiv:2310.19788.
MasahiroKato,TakuyaIshihara,JunyaHonda,andYusukeNarita. Efficientadaptiveexperimentaldesignforaverage
treatmenteffectestimation,2020. arXiv:2002.05308.
EmilieKaufmann. ContributionstotheOptimalSolutionofSeveralBanditsProblems. Habilitationa´ Dirigerdes
Recherches, Universite´ de Lille, 2020. URL https://emiliekaufmann.github.io/HDR_EmilieKaufmann.
pdf.
EmilieKaufmann,OlivierCappe´,andAure´lienGarivier. Onthecomplexityofbest-armidentificationinmulti-armed
banditmodels. JournalofMachineLearningResearch,17(1):1–42,2016.
JunpeiKomiyama,TairaTsuchiya,andJunyaHonda. Minimaxoptimalalgorithmsforfixed-budgetbestarmidentifica-
tion. InAdvancesinNeuralInformationProcessingSystems,2022.
Junpei Komiyama, Kaito Ariu, Masahiro Kato, and Chao Qin. Rate-optimal bayesian simple regret in best arm
identification. MathematicsofOperationsResearch,2023.
T.LLaiandHerbertRobbins. Asymptoticallyefficientadaptiveallocationrules. AdvancesinAppliedMathematics,
1985.
AnushaLalitha,KoushaKalantari,YifeiMa,AnoopDeoras,andBranislavKveton.Fixed-budgetbest-armidentification
withheterogeneousrewardvariances. InConferenceonUncertaintyinArtificialIntelligence,2023.
LLeCam. Limitsofexperiments. InTheoryofStatistics,pp.245–282.UniversityofCaliforniaPress,1972.
LucienLeCam. AsymptoticMethodsinStatisticalDecisionTheory(SpringerSeriesinStatistics). Springer,1986.
ErichL.LehmannandGeorgeCasella. TheoryofPointEstimation. Springer-Verlag,1998.
Jerzy Neyman. Sur les applications de la theorie des probabilites aux experiences agricoles: Essai des principes.
StatisticalScience,5:463–472,1923.
JerzyNeyman. Onthetwodifferentaspectsoftherepresentativemethod: themethodofstratifiedsamplingandthe
methodofpurposiveselection. JournaloftheRoyalStatisticalSociety,97:123–150,1934.
NicolasNguyen,ImadAouali,Andra´sGyo¨rgy,andClaireVernade. Prior-dependentallocationsforbayesianfixed-
budgetbest-armidentificationinstructuredbandits,2024. arXiv:2402.05878.
ChaoQin. Openproblem: Optimalbestarmidentificationwithfixed-budget. InConferenceonLearningTheory,2022.
Donald B. Rubin. Estimating causal effects of treatments in randomized and nonrandomized studies. Journal of
EducationalPsychology,1974.
DongwookShin,MarkBroadie,andAssafZeevi. Tractablesamplingstrategiesforordinaloptimization. Operations
Research,66(6):1693–1712,2018.
Mark J. van der Laan. The construction and analysis of adaptive group sequential designs, 2008. URL https:
//biostats.bepress.com/ucbbiostat/paper232.
A.W.vanderVaart. Anasymptoticrepresentationtheorem. InternationalStatisticalReview/RevueInternationalede
Statistique,59(1):97–121,1991.
10AdaptiveGeneralizedNeymanAllocation: LocalAsymptoticMinimaxOptimalBestArmIdentificAatiPoRnEPRINT
A.W.vanderVaart. AsymptoticStatistics. CambridgeSeriesinStatisticalandProbabilisticMathematics.Cambridge
UniversityPress,1998.
A.Wald. Sequentialtestsofstatisticalhypotheses. TheAnnalsofMathematicalStatistics,16(2):117–186,1945.
Po-AnWang,KaitoAriu,andAlexandreProutiere. Onuniformlyoptimalalgorithmsforbestarmidentificationin
two-armedbanditswithfixedbudget. InInternationalConferenceonMachineLearning(ICML),2024.
JunwenYangandVincentTan. Minimaxoptimalfixed-budgetbestarmidentificationinlinearbandits. InAdvancesin
NeuralInformationProcessingSystems,2022.
11AdaptiveGeneralizedNeymanAllocation: LocalAsymptoticMinimaxOptimalBestArmIdentificAatiPoRnEPRINT
A ProofofTheorem2.3
ThissectionprovidestheproofforTheorem2.3. OurproofisinspiredbyKaufmannetal.(2016),Garivier&Kaufmann
(2016),andKato(2024b).
A.1 TransportationLemma
Letusdenotethenumberofdrawnarmsby
T
(cid:88)
N (T)= 1[A =a].
a t
t=1
First,weintroducethetransportationlemma,shownbyKaufmannetal.(2016).
PropositionA.1(Transportationlemma. FromLemma1inKaufmannetal.(2016)). LetP andQbetwobandit
modelswithK armssuchthatforalla,thedistributionsP andQ ofY aremutuallyabsolutelycontinuous. Then,
a a a
wehave
K
(cid:88)
E [N (T)]KL(P ,Q )≥ sup d(P (E),P (E)),
P a a a P Q
a=1
E∈FT
whered(x,y):=xlog(x/y)+(1−x)log((1−x)/(1−y))isthebinaryrelativeentropy,withtheconventionthat
d(0,0)=d(1,1)=0.
Here,Qcorrespondstoanalternativehypothesisthatisusedforderivinglowerboundsandnotanactualdistribution.
A.2 KLDivergenceandFisherInformation
Werecapthefollowingwell-knownrelationshipthatholdsbetweentheKLdivergenceandtheFisherinformation. This
resultisacansequenceoftheTaylorexpansion.
PropositionA.2(Proposition15.3.2. inDuchi(2023)andTheorem4.4.4inCalin&Udris¸te(2014)). ForP and
µa,a
Q ofP,Q∈P,wehave
νa,a
1 1
lim kl(µ ,ν )= I(µ ) (7)
νa→µa (µ a−ν a)2 a a 2 a
A.3 ProofofTheorem2.3
ByusingPropositionsA.1andA.2,weproveTheorem2.3below.
ProofofTheorem2.3. LetP =P∗;thatis,µ=(µ ) bethetruemeanoutcomesofY underP∗.
µ a a∈[K] a
FromPropositionA.11,anyconsistentstrategyπ ∈Πconstsatisfies
inf limsup−1 logP (cid:0) aπ ̸=a∗(P )(cid:1)
(cid:0) (cid:1) T Pν (cid:98)T ν
Pν∈∪b∈[K]\{a∗(P∗)}P b,θ∗,∆ T→∞
(cid:88)
≤ inf limsup κπ (P )KL(P ,Q ),
(cid:0) (cid:1) a,T µ a,µa a,νa
Pν∈∪b∈[K]\{a∗(P∗)}P b,θ∗,∆ T→∞ a∈[K]
whereκπ (P ):= 1E [N ]. Here,weusedtheproofofTheorem12inKaufmannetal.(2016).
a,T µ T Pµ a
FromPropositionA.2,foranyε > 0,thereexistsΞ (ε)suchthatforall−Ξ (ε) < ξ := −µ +ν < Ξ (ε),the
a a a a a a
followingholds:
kl(µ ,µ +ξ )≤ ξ a2 I(cid:0) µ (cid:1) +εξ2 = ξ a2 +εξ2, (8)
a a a 2 a a 2σ (µ ) a
a a
whereweusedI(cid:0) µ (cid:1) =σ2(µ ).
a a a
Then,wehave
1
inf limsup− logP (aπ ̸=a∗(P ))
(cid:0) (cid:1) T Pν (cid:98)T ν
Pν∈∪b∈[K]\{a∗(P∗)}P b,θ∗,∆ T→∞
12AdaptiveGeneralizedNeymanAllocation: LocalAsymptoticMinimaxOptimalBestArmIdentificAatiPoRnEPRINT
(cid:40) (cid:41)
≤ inf limsup (cid:88) κπ (P )(µ a−ν a)2 +ε(µ −ν )2
a,T µ 2σ2(µ ) a a
argmax
a( ∈νa [K)∈ ]R νK a̸=:
a∗(P∗)
T→∞
a∈[K]
a a
(cid:40) (cid:41)
≤ sup inf (cid:88) w (µ a−ν a)2 +ε(µ −ν )2 ,
a 2σ2(µ ) a a
w∈W
argmax
a( ∈νa [K)∈ ]R νK a̸=:
a∗(P∗)a∈[K]
a a
(cid:110) (cid:111)
whereW := w ∈[0,1]K: (cid:80) w =1 .
a∈[K] a
WecomputetheRHSasfollows:
(cid:0) (cid:1)2
(cid:88) µ a−ν a
inf w
a 2σ2(µ )
argmax
a( ∈νa [K)∈ ]R νK a̸=:
a∗(P∗)a∈[K]
a a
(cid:0) (cid:1)2
(cid:88) µ a−ν a
= min inf w
a∈[K]\{a∗ µ} νa( >νa ν) a∈ ∗R (K P:
∗)a∈[K]
a 2σ a2(µ a)
(cid:40) (cid:0)
ν −µ
(cid:1)2
(ν −µ
)2(cid:41)
= min inf w a∗(P∗) a∗(P∗) +w a a
a∈[K]\{a∗(P∗)}(νa∗(P∗),νa)∈R2: a∗(P∗) 2σ a2 ∗(P∗)(µ a∗(P∗)) a 2σ a2(µ a)
νa>νa∗(P∗)
(cid:40) (cid:0)
ν−µ
(cid:1)2
(ν−µ
)2(cid:41)
= min min w a∗(P∗) +w a .
a∈[K]\{a∗(P∗)}ν∈[µa,µa∗(P∗)] a∗(P∗)2σ a2 ∗(P∗)(µ a∗(P∗)) a 2σ a2(µ a)
Then,bysolvingtheoptimizationproblem,weobtain
(cid:40) (cid:0)
ν−µ
(cid:1)2
(ν−µ
)2(cid:41)
min min w a∗(P∗) +w a
a∈[K]\{a∗(P∗)}ν∈[µa,µa∗(P∗)] a∗(P∗) 2σ a2(µ a∗(P∗)) a 2σ a2(µ a)
(θ∗−µ )2
= min a .
(cid:18) (cid:19)
a∈[K]\{a∗(P∗)} 2 σ a2 ∗(P∗)(µa∗(P∗)) + σ a2(µa)
wa∗(P∗) wa
Therefore,wehave
1
lim inf limsup− logP (aπ ̸=a∗(P ))
(cid:0) (cid:1) 2 Pµ (cid:98)T µ
∆→0Pν∈∪b∈[K]\{a∗(P∗)}P b,θ∗,∆ T→∞ ∆ T
(θ∗−µ )2
≤ sup min a .
(cid:18) (cid:19)
w∈Wa∈[K]\{a∗(P∗)} 2 σ a2 ∗(P∗)(µa∗(P∗)) + σ a2(µa)
wa∗(P∗) wa
Lastly,wesolve
1
maxmin .
(cid:18) (cid:19)
w∈W a̸=b 2 σ a2 ∗(P∗)(µa∗(P∗)) + σ a2(µa)
wa∗(P∗) wa
Forsimplicity,wedenote(σ2(µ )) by(σ2) .
a a a∈[K] a a∈[K]
Wesolvetheoptimizationproblembysolvingthefollowingnon-linearprogramming:
max R
R>0,w={w1,w2...,wK}∈(0,1)K
(cid:32) σ2 σ2(cid:33)
s.t. R a∗(P∗) + a ζ−1≤0 ∀a∈[K]\{a∗(P∗)},
w w
a∗(P∗) a
(cid:88)
w −1=0,
a
a∈[K]
13AdaptiveGeneralizedNeymanAllocation: LocalAsymptoticMinimaxOptimalBestArmIdentificAatiPoRnEPRINT
w >0 ∀a∈[K].
a
Letλ = {λ } ∈ [−∞,0]K−1 andγ ≥ 0beLagrangianmultipliers. Then, wedefinethefollowing
a a∈[K]\{a∗(P∗)}
Lagrangianfunction:
L(λ,γ;R,w)=R+
(cid:88)
λ
a(cid:32) R(cid:32) wσ a2 ∗(P∗)
+
wσ a2(cid:33) −1(cid:33) −γ

(cid:88)
w
a−1
.
a∗(P∗) a
a∈[K]\{a∗(P∗)} a∈[K]
(cid:18) (cid:19)
Notethattheobjective(R)andconstraints(R wσ a2 a∗ ∗( (P P∗ ∗)
)
+ wσ a2
a
−1≤0and(cid:80) a∈[K]w a−1=0)aredifferentiable
convexfunctionsforRandw.
Here,theglobaloptimizerR†andw† ={w†}∈(0,1)K satisfiesthefollowingKKTconditions:
a
1+
(cid:88) λ†(cid:32) σ a2 ∗(P∗)
+
σ a2(cid:33)
=0 (9)
a w† w†
a∈[K]\{b} a∗(P∗) a
σ2
−2 (cid:88) λ†R† a∗(P∗) =γ† (10)
a (w† )2
a∈[K]\{a∗(P∗)} a∗(P∗)
σ2
−2λ†R† a =γ† ∀a∈[K]\{b} (11)
a (w†)2
a
(cid:32) (cid:32) σ2 σ2(cid:33) (cid:33)
λ† R† a∗(P∗) + a −1 =0 ∀a∈[K]\{a∗(P∗)} (12)
a w† w†
a∗(P∗) a
 
(cid:88)
γ†

w†(c)−1=0
c∈[K]
λ† ≤0 ∀a∈[K]\{a∗(P∗)}.
a
Here, (9) implies that there exists a ∈ [K]\{a∗(P∗)} such that λ† < 0 holds. This is because if λ† = 0 for all
a a
a∈[K]\{a∗(P∗)},1+0=1̸=0.
Withλ† <0,since−λ†R† σ a2 >0foralla∈[K],itfollowsthatγ† >0.Thisalsoimpliesthat(cid:80) wc†−1=0.
a a (wa†)2 c∈[K]
Then,(12)impliesthat
(cid:32) σ2 σ2(cid:33)
R† a∗(P∗) + a =1 ∀a∈[K]\{a∗(P∗)}.
w† w†
a∗(P∗) a
Therefore,wehave
σ2 σ2
a = c ∀a,c∈[K]\{a∗(P∗)}. (13)
w† w†
a c
Let σ a2 = σ c2 = 1 − σ c2 =U. From(13)and(9),
wa† wc† R† wc†
(cid:88) 1
λ† =− (14)
c σ c2 +U
c∈[K]\{a∗(P∗)} wc†
From(10)and(11),wehave
σ a2 ∗(P∗) (cid:88) λ† =λ† σ a2 ∀a∈[K]\{a∗(P∗)}. (15)
(w† )2 c a (w†)2
a∗(P∗) c∈[K]\{a∗(P∗)} a
From(14)and(15),wehave
σ2 σ2 (cid:32) σ2 (cid:33)
− a∗(P∗) =λ† a a∗(P∗) +U ∀a∈[K]\{a∗(P∗)}. (16)
(w† )2 a (w†)2 w†
a∗(P∗) a a∗(P∗)
14AdaptiveGeneralizedNeymanAllocation: LocalAsymptoticMinimaxOptimalBestArmIdentificAatiPoRnEPRINT
From(9)and(16),wehave
(cid:118)
w a†
∗(P∗)
=(cid:117) (cid:117) (cid:116)σ a2
∗(P∗)
(cid:88) (w σa† 2)2 .
a∈[K]\{a∗(P∗)} a
Insummary,theKKTconditionsaregivenasfollows:
(cid:118)
w a†
∗(P∗)
=(cid:117) (cid:117) (cid:116)σ a2
∗(P∗)
(cid:88) (w σa† 2)2
a∈[K]\{a∗(P∗)} a
σ2 σ2 (cid:32)(cid:32) σ2 σ2(cid:33)(cid:33)
a∗(P∗) =−λ† a a∗(P∗) + a ∀a∈[K]\{a∗(P∗)}
(w† )2 a (w†)2 w† w†
a∗(P∗) a a∗(P∗) a
σ2
−λ† a =γ† ∀a∈[K]\{a∗(P∗)}
a (w†)2 (cid:101)
a
σ2 1 σ2
a = − a∗(P∗) ∀a∈[K]\{a∗(P∗)}
w† R† w†
a a∗(P∗)
(cid:88)
w† =1
a
a∈[K]
λ† ≤0 ∀a∈[K]\{a∗(P∗)},
a
whereγ† =γ†/2R†.
(cid:101)
(cid:114)
Fromw† = σ2(cid:80) (wa†)2 and−λ† σ a2 =γ†,wehave
b b a∈[K]\{a∗(P∗)} σ a2 a(wa†)2 (cid:101)
(cid:115)
w† =σ (cid:88) −λ†/(cid:112) γ†
a∗(P∗) a∗(P∗) a (cid:101)
a∈[K]\{a∗(P∗)}
(cid:113)
w† = −λ†/γ†σ .
a a (cid:101) a
From(cid:80) w† =1,wehave
a∈[K] a
(cid:115) (cid:113)
σ (cid:88) −λ†/(cid:112) γ†+ (cid:88) −λ†/γ†σ =1.
b a (cid:101) a (cid:101) a
a∈[K]\{a∗(P∗)} a∈[K]\{a∗(P∗)}
Therefore,thefollowingholds:
(cid:115) (cid:113)
(cid:112) γ† =σ (cid:88) −λ† + (cid:88) −λ†σ .
(cid:101) a∗(P∗) a a a
a∈[K]\{a∗(P∗)} a∈[K]\{a∗(P∗)}
Hence,thetargetdrawingratioiscomputedas
(cid:113)
σ (cid:80) −λ†
w† = a∗(P∗) a∈[K]\{a∗(P∗)} a
a∗(P∗) σ (cid:113) (cid:80) −λ† +(cid:80) (cid:112) −λ†σ
a∗(P∗) a∈[K]\{a∗(P∗)} a a∈[K]\{a∗(P∗)} a a
(cid:112)
−λ†σ
w† = a a ,
a σ (cid:113) (cid:80) −λ† +(cid:80) (cid:112) −λ†σ
a∗(P∗) a∈[K]\{a∗(P∗)} a a∈[K]\{a∗(P∗)} a a
wherefrom (wσ a†a2 ∗∗ (( PP ∗∗ ))
)2
=−λ† a(wσ a†a2
)2
(cid:16) wσ b2
b†
+ wσ a2 a†(cid:17) ,(λ† a) a∈[K]\{a∗(P∗)}satisfies,
1
(cid:80) −λ†
a∈[K]\{b} a
15AdaptiveGeneralizedNeymanAllocation: LocalAsymptoticMinimaxOptimalBestArmIdentificAatiPoRnEPRINT
  
=(cid:113)
(cid:80)
a∈[Kσ ]b
\{b}−λ†
a
+ (cid:112)σ −a
λ†
aσ b(cid:115) c∈[(cid:88) K]\{b}−λc†+ c∈[(cid:88) K]\{b}(cid:112) −λc†σ 0c 
  √ 
=σ b+ (cid:112)σ −a
λ†
a(cid:115) c∈[(cid:88) K]\{b}−λc†σ b+ (cid:80) (cid:80)c∈[ cK ∈[] K\{ ]b \} {b}− −λ λc c† †σ 0c(cid:115) c∈[(cid:88) K]\{b}−λc†.
Then,thefollowingsolutionssatisfytheaboveKKTconditions:
 2
(cid:115)
(cid:88)
R† σ b+ σ a2 =1
a∈[K]\{b}
σ
(cid:113)(cid:80)
σ2
b a∈[K]\{b} a
w† =
b σ (cid:113)(cid:80) σ2+(cid:80) σ2
b a∈[K]\{b} a a∈[K]\{b} a
σ2
w† = a
a σ (cid:113)(cid:80) σ2+(cid:80) σ2
b a∈[K]\{b} a a∈[K]\{b} a
λ† =−σ2
a a
 2
(cid:115)
(cid:88) (cid:88)
γ† =σ b σ a2+ σ a2  .
a∈[K]\{b} a∈[K]\{b}
Thus,wecompletetheproof.
B ProofofTheorem4.1
ToshowTheorem4.1,wederiveanupperboundof
(cid:16) (cid:17)
P µA2IPW ≤µA2IPW
P∗ (cid:98)a⋆(P∗),T (cid:98)b,T
forb∈[K]\{a⋆(P∗)}.
LemmaB.1(FromTheorem4.1inKato(2024a)). UnderthesameconditionwithTheorem4.1,foreachP∗andeach
a∈[K],theAGNAstrategysatisfies
1 (cid:16) (cid:17) 1
lim liminf− logP µA2IPW ≤µA2IPW ≥ .
∆→0 T→∞ ∆2T P∗ (cid:98)a⋆(P),T (cid:98)a,T 2(cid:18) σ a2 ∗(P∗) + σ a2 (cid:19)
w aA ∗G (N PA ∗) w aAGNA
Forthecompleteness,weshowtheproofinAppendixC.
Then,weproveTheorem4.1inKato(2024a)asfollows:
Proof. Wehave
liminf− 1 logP (cid:0) aAGNA ̸=a∗(P∗)(cid:1)
T→∞
∆2T P∗ (cid:98)T
1 (cid:88) (cid:16) (cid:17)
≥liminf− log P µA2IPW ≤µA2IPW
T→∞
∆2T P∗ (cid:98)a⋆(P∗),T (cid:98)a,T
a̸=a∗(P∗)
1 (cid:26) (cid:16) (cid:17)(cid:27)
≥liminf− log (K−1) max P µA2IPW ≤µA2IPW
T→∞
∆2T
a̸=a∗(P∗)
P∗ (cid:98)a⋆(P∗),T (cid:98)a,T
1
≥ min .
(cid:18) (cid:19)
a̸=a∗(P∗) 2 σ a2 ∗(P∗) + σ a2
w aA ∗G (N PA ∗) w aAGNA
16AdaptiveGeneralizedNeymanAllocation: LocalAsymptoticMinimaxOptimalBestArmIdentificAatiPoRnEPRINT
C ProofofLemmaB.1
LetP =P∗;thatis,µbethetruemeanoutcomesofY underP∗.
µ a
Letusdefine
1 (cid:32) 1[A =a∗(P∗)](cid:0) Y −µ (cid:1) 1[A =a](cid:0) Y −µ (cid:1)
Ψ :=
t a∗(P∗),t (cid:98)a∗(P∗),t
−
t a,t (cid:98)a,t
a,t (cid:112) V a∗(P∗) w (cid:98)aA ∗G (PN ∗A
),t
w (cid:98)aA ,G tNA
(cid:33)
+µ −µ −∆ (P∗) ,
(cid:98)a∗(P∗),t (cid:98)a,t a
where∆ (P∗):=µ −µ ,and
a a∗(P∗) a
σ2 σ2
V∗(P∗):= a∗(P∗) + a .
a wAGNA wAGNA
a∗(P∗) a
Then,wehave
T
1 (cid:88)
µA2IPW −µA2IPW = Ψ .
(cid:98)a∗(P∗),T (cid:98)a,T T a,t
t=1
Byusingthisresult,weaimtoderivetheupperboundof
P
(cid:16)
µA2IPW
≤µA2IPW(cid:17)
=P
(cid:32) (cid:88)T
Ψ
≤−T∆
a(P∗)(cid:33)
.
P∗ (cid:98)a⋆(P∗),T (cid:98)a,T P∗ a,t (cid:112)
V∗(P∗)
t=1 a
Here,weshowthat{Ψ } isamartingaledifferencesequence(MDS).Foreacht∈[T],itholdsthat
a,t t∈[T]
(cid:112)
V∗(P∗)E [Ψ |F ]
a P∗ a,t t−1
(cid:34) 1[A =a∗(P∗)](cid:0) Y −µ (cid:1) (cid:35)
=E t a∗(P∗),t (cid:98)a∗(P∗),t +µ |F
P∗ w (cid:98)a∗(P∗),t t−1
(cid:98)a∗(P∗),t
(cid:34) 1[A =a](cid:0) Y −µ (cid:1) (cid:35)
−E t a,t (cid:98)a,t +µ |F −∆
P∗ w (cid:98)a,t t−1
(cid:98)a,t
(cid:0) (cid:1) (cid:0) (cid:1)
w µ −µ w µ −µ
(cid:98)a∗(P∗),t a∗(P∗) (cid:98)a∗(P∗),t (cid:98)2,t a (cid:98)a,t
= +µ − +µ −∆
w (cid:98)a∗(P∗),t w (cid:98)a,t
(cid:98)a∗(P∗),t (cid:98)a,t
(cid:0) (cid:1) (cid:0) (cid:1)
= µ −µ − µ −µ
a∗(P∗) a a∗(P∗) a
=0.
Thisresultimpliesthat{Ψ } isanMDS.
a,t t∈[T]
First,becausethereexistsaconstantC >0independentofT suchthatw >C byconstruction,thefollowinglemma
(cid:98)a,t
holds.
LemmaC.1. ForanyP∗ ∈P∗andalla∈[K],µ −a −→.s µ andσ2 −a −→.s σ2.
(cid:98)a,t a (cid:98)a a
Furthermore,fromσ2 −a −→.s σ2andcontinuousmappingtheorem,foralla∈[K],w −a −→.s wAGNAholds.
(cid:98)a a (cid:98)a,t a,t
LemmaC.2(FromLemma5.3inKato(2024a)). Foranyµ∈ΘK,underP∗ =P ,wehaveE (cid:2) Ψ2 |F (cid:3) −
µ P∗ a,t t−1
a.s
1−−→0ast→∞.
Thislemmaimmediatelyyieldsthefollowinglemma.
LemmaC.3(FromLemma5.4inKato(2024a)). Foranyµ∈ΘK,underP∗ =P ,itholdsthat
µ
(cid:32) T (cid:33)
P
P∗
Tl →im ∞T1 (cid:88)(cid:12) (cid:12)E P∗(cid:2) Ψ2
a,t
|F t−1(cid:3) −1(cid:12) (cid:12)=0 =1
t=1
17AdaptiveGeneralizedNeymanAllocation: LocalAsymptoticMinimaxOptimalBestArmIdentificAatiPoRnEPRINT
ThisresultisavariantoftheCesa`rolemmaforacasewithalmostsureconvergence.
FromDefinition2.1,thefollowinglemmaholds:
LemmaC.4. Foranyµ∈ΘK,underP∗ =P ,thefirstmomentofΨ iszero,andthesecondmomentofΨ exists.
µ a,t a,t
Byusingtheselemma,weproveLemmaB.1.
ProofofLemmaB.1. ByapplyingtheChernoffbound,foranyv <0andanyλ<0,itholdsthat
(cid:32) T (cid:33) (cid:34) (cid:32) T (cid:33)(cid:35)
1 (cid:88) (cid:88)
P Ψ ≤v ≤E exp λ Ψ exp(−Tλv).
P∗ T a,t P∗ a,t
t=1 t=1
FromtheChernoffboundandapropertyofanMDS,wehave
(cid:34) (cid:32) T (cid:33)(cid:35)
(cid:88)
E exp λ Ψ
P∗ a,t
t=1
(cid:34) T (cid:35)
(cid:89)
=E E [exp(λΨ )|F ]
P∗ P∗ a,t t−1
t=1
(cid:34) (cid:32) T (cid:33)(cid:35)
(cid:88)
=E exp logE [exp(λΨ )|F ] .
P∗ P∗ a,t t−1
t=1
Then,fromtheTaylorexpansionaroundλ=0,weobtainthefollowing(Section1.2,Do¨ringetal.,2022):
lim 1 logE [exp(λΨ )|F ]= 1 E (cid:2) Ψ2 |F (cid:3) . (17)
λ→0λ2 P∗ a,t t−1 2 P∗ a,t t−1
Here,weusedLemmaC.4,whichstatesthattheconditionalvarianceE (cid:2) Ψ2 |F (cid:3) ofΨ2 exists.
P∗ a,t t−1 a,t
Letv =λ=−√ ∆ . Then,fromtheresultsinSteps1and2,foranyϵ>0,thereexistst(ϵ)>0suchthatforall
V∗(P∗)
a
T >t(ϵ)andforanyϵ′ >0,thereexistsδ (ϵ′)>0suchthatforall0<∆<δ (ϵ′)andforallP∗ ∈P(∆),itholds
T T
that
(cid:32) T (cid:33)
(cid:88)
P Ψ ≤Tv
P∗ a,t
t=1
≤E
(cid:34) exp(cid:32)
−
Tλ2
+ϵTλ2−
∆2 (cid:32) (cid:88)T
E (cid:2) Ψ2 |F (cid:3)
−T(cid:33)(cid:33)(cid:35)
P∗ 2 2V∗(P∗) P∗ a,t t−1
a t=1
≤E
(cid:34) exp(cid:32) −Tλ2
+
λ2 (cid:32) (cid:88)T
E (cid:2) Ψ2 |F (cid:3)
−1(cid:33) +ϵTλ2(cid:33)(cid:35)
P∗ 2 2 P∗ a,t t−1
t=1
=exp(cid:18) −Tλ2 +ϵTλ2(cid:19)
E
(cid:34) exp(cid:32) λ2 (cid:32) (cid:88)T
E (cid:2) Ψ2 |F (cid:3)
−1(cid:33)(cid:33)(cid:35)
2 P∗ 2 P∗ a,t t−1
t=1
=exp(cid:18) −Tλ2 +ϵTλ2(cid:19)
exp(cid:0) ϵ′Tλ2(cid:1)
2
(cid:18) Tλ2 (cid:19)
=exp − +(ϵ+ϵ′)Tλ2 .
2
Therefore,bylettingλ=−√ ∆ ,wehave
V∗(P∗) (θ∗)
a a
1 (cid:16) (cid:17) 1
lim min liminf− logP µA2IPW ≤µA2IPW ≥ .
∆→0P∗∈P(∆) T→∞ ∆2T P∗ (cid:98)a∗(P∗),T (cid:98)a,T 2V a∗(P∗)
Thus,theproofiscomplete.
18