[
    {
        "title": "LLMs Meet Multimodal Generation and Editing: A Survey",
        "authors": "Yingqing HeZhaoyang LiuJingye ChenZeyue TianHongyu LiuXiaowei ChiRuntao LiuRuibin YuanYazhou XingWenhai WangJifeng DaiYong ZhangWei XueQifeng LiuYike GuoQifeng Chen",
        "links": "http://arxiv.org/abs/2405.19334v1",
        "entry_id": "http://arxiv.org/abs/2405.19334v1",
        "pdf_url": "http://arxiv.org/pdf/2405.19334v1",
        "summary": "With the recent advancement in large language models (LLMs), there is a\ngrowing interest in combining LLMs with multimodal learning. Previous surveys\nof multimodal large language models (MLLMs) mainly focus on understanding. This\nsurvey elaborates on multimodal generation across different domains, including\nimage, video, 3D, and audio, where we highlight the notable advancements with\nmilestone works in these fields. Specifically, we exhaustively investigate the\nkey technical components behind methods and multimodal datasets utilized in\nthese studies. Moreover, we dig into tool-augmented multimodal agents that can\nuse existing generative models for human-computer interaction. Lastly, we also\ncomprehensively discuss the advancement in AI safety and investigate emerging\napplications as well as future prospects. Our work provides a systematic and\ninsightful overview of multimodal generation, which is expected to advance the\ndevelopment of Artificial Intelligence for Generative Content (AIGC) and world\nmodels. A curated list of all related papers can be found at\nhttps://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation",
        "updated": "2024-05-29 17:59:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.19334v1"
    },
    {
        "title": "Self-Exploring Language Models: Active Preference Elicitation for Online Alignment",
        "authors": "Shenao ZhangDonghan YuHiteshi SharmaZiyi YangShuohang WangHany HassanZhaoran Wang",
        "links": "http://arxiv.org/abs/2405.19332v1",
        "entry_id": "http://arxiv.org/abs/2405.19332v1",
        "pdf_url": "http://arxiv.org/pdf/2405.19332v1",
        "summary": "Preference optimization, particularly through Reinforcement Learning from\nHuman Feedback (RLHF), has achieved significant success in aligning Large\nLanguage Models (LLMs) to adhere to human intentions. Unlike offline alignment\nwith a fixed dataset, online feedback collection from humans or AI on model\ngenerations typically leads to more capable reward models and better-aligned\nLLMs through an iterative process. However, achieving a globally accurate\nreward model requires systematic exploration to generate diverse responses that\nspan the vast space of natural language. Random sampling from standard\nreward-maximizing LLMs alone is insufficient to fulfill this requirement. To\naddress this issue, we propose a bilevel objective optimistically biased\ntowards potentially high-reward responses to actively explore\nout-of-distribution regions. By solving the inner-level problem with the\nreparameterized reward function, the resulting algorithm, named Self-Exploring\nLanguage Models (SELM), eliminates the need for a separate RM and iteratively\nupdates the LLM with a straightforward objective. Compared to Direct Preference\nOptimization (DPO), the SELM objective reduces indiscriminate favor of unseen\nextrapolations and enhances exploration efficiency. Our experimental results\ndemonstrate that when finetuned on Zephyr-7B-SFT and Llama-3-8B-Instruct\nmodels, SELM significantly boosts the performance on instruction-following\nbenchmarks such as MT-Bench and AlpacaEval 2.0, as well as various standard\nacademic benchmarks in different settings. Our code and models are available at\nhttps://github.com/shenao-zhang/SELM.",
        "updated": "2024-05-29 17:59:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.19332v1"
    },
    {
        "title": "NPGA: Neural Parametric Gaussian Avatars",
        "authors": "Simon GiebenhainTobias KirschsteinMartin RünzLourdes AgapitoMatthias Nießner",
        "links": "http://arxiv.org/abs/2405.19331v1",
        "entry_id": "http://arxiv.org/abs/2405.19331v1",
        "pdf_url": "http://arxiv.org/pdf/2405.19331v1",
        "summary": "The creation of high-fidelity, digital versions of human heads is an\nimportant stepping stone in the process of further integrating virtual\ncomponents into our everyday lives. Constructing such avatars is a challenging\nresearch problem, due to a high demand for photo-realism and real-time\nrendering performance. In this work, we propose Neural Parametric Gaussian\nAvatars (NPGA), a data-driven approach to create high-fidelity, controllable\navatars from multi-view video recordings. We build our method around 3D\nGaussian Splatting for its highly efficient rendering and to inherit the\ntopological flexibility of point clouds. In contrast to previous work, we\ncondition our avatars' dynamics on the rich expression space of neural\nparametric head models (NPHM), instead of mesh-based 3DMMs. To this end, we\ndistill the backward deformation field of our underlying NPHM into forward\ndeformations which are compatible with rasterization-based rendering. All\nremaining fine-scale, expression-dependent details are learned from the\nmulti-view videos. To increase the representational capacity of our avatars, we\naugment the canonical Gaussian point cloud using per-primitive latent features\nwhich govern its dynamic behavior. To regularize this increased dynamic\nexpressivity, we propose Laplacian terms on the latent features and predicted\ndynamics. We evaluate our method on the public NeRSemble dataset, demonstrating\nthat NPGA significantly outperforms the previous state-of-the-art avatars on\nthe self-reenactment task by 2.6 PSNR. Furthermore, we demonstrate accurate\nanimation capabilities from real-world monocular videos.",
        "updated": "2024-05-29 17:58:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.19331v1"
    },
    {
        "title": "MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series",
        "authors": "Ge ZhangScott QuJiaheng LiuChenchen ZhangChenghua LinChou Leuang YuDanny PanEsther ChengJie LiuQunshu LinRaven YuanTuney ZhengWei PangXinrun DuYiming LiangYinghao MaYizhi LiZiyang MaBill LinEmmanouil BenetosHuan YangJunting ZhouKaijing MaMinghao LiuMorry NiuNoah WangQuehry QueRuibo LiuSine LiuShawn GuoSoren GaoWangchunshu ZhouXinyue ZhangYizhi ZhouYubo WangYuelin BaiYuhan ZhangYuxiang ZhangZenith WangZhenzhu YangZijian ZhaoJiajun ZhangWanli OuyangWenhao HuangWenhu Chen",
        "links": "http://arxiv.org/abs/2405.19327v1",
        "entry_id": "http://arxiv.org/abs/2405.19327v1",
        "pdf_url": "http://arxiv.org/pdf/2405.19327v1",
        "summary": "Large Language Models (LLMs) have made great strides in recent years to\nachieve unprecedented performance across different tasks. However, due to\ncommercial interest, the most competitive models like GPT, Gemini, and Claude\nhave been gated behind proprietary interfaces without disclosing the training\ndetails. Recently, many institutions have open-sourced several strong LLMs like\nLLaMA-3, comparable to existing closed-source LLMs. However, only the model's\nweights are provided with most details (e.g., intermediate checkpoints,\npre-training corpus, and training code, etc.) being undisclosed. To improve the\ntransparency of LLMs, the research community has formed to open-source truly\nopen LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-training\ncorpus and training code) are being provided. These models have greatly\nadvanced the scientific study of these large models including their strengths,\nweaknesses, biases and risks. However, we observe that the existing truly open\nLLMs on reasoning, knowledge, and coding tasks are still inferior to existing\nstate-of-the-art LLMs with similar model sizes. To this end, we open-source\nMAP-Neo, a highly capable and transparent bilingual language model with 7B\nparameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is the\nfirst fully open-sourced bilingual LLM with comparable performance compared to\nexisting state-of-the-art LLMs. Moreover, we open-source all details to\nreproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning\npipeline, checkpoints, and well-optimized training/evaluation framework are\nprovided. Finally, we hope our MAP-Neo will enhance and strengthen the open\nresearch community and inspire more innovations and creativities to facilitate\nthe further improvements of LLMs.",
        "updated": "2024-05-29 17:57:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.19327v1"
    },
    {
        "title": "Are Large Language Models Chameleons?",
        "authors": "Mingmeng GengSihong HeRoberto Trotta",
        "links": "http://arxiv.org/abs/2405.19323v1",
        "entry_id": "http://arxiv.org/abs/2405.19323v1",
        "pdf_url": "http://arxiv.org/pdf/2405.19323v1",
        "summary": "Do large language models (LLMs) have their own worldviews and personality\ntendencies? Simulations in which an LLM was asked to answer subjective\nquestions were conducted more than 1 million times. Comparison of the responses\nfrom different LLMs with real data from the European Social Survey (ESS)\nsuggests that the effect of prompts on bias and variability is fundamental,\nhighlighting major cultural, age, and gender biases. Methods for measuring the\ndifference between LLMs and survey data are discussed, such as calculating\nweighted means and a new proposed measure inspired by Jaccard similarity. We\nconclude that it is important to analyze the robustness and variability of\nprompts before using LLMs to model individual decisions or collective behavior,\nas their imitation abilities are approximate at best.",
        "updated": "2024-05-29 17:54:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.19323v1"
    }
]