[
    {
        "title": "X-VILA: Cross-Modality Alignment for Large Language Model",
        "authors": "Hanrong YeDe-An HuangYao LuZhiding YuWei PingAndrew TaoJan KautzSong HanDan XuPavlo MolchanovHongxu Yin",
        "links": "http://arxiv.org/abs/2405.19335v1",
        "entry_id": "http://arxiv.org/abs/2405.19335v1",
        "pdf_url": "http://arxiv.org/pdf/2405.19335v1",
        "summary": "We introduce X-VILA, an omni-modality model designed to extend the\ncapabilities of large language models (LLMs) by incorporating image, video, and\naudio modalities. By aligning modality-specific encoders with LLM inputs and\ndiffusion decoders with LLM outputs, X-VILA achieves cross-modality\nunderstanding, reasoning, and generation. To facilitate this cross-modality\nalignment, we curate an effective interleaved any-to-any modality\ninstruction-following dataset. Furthermore, we identify a significant problem\nwith the current cross-modality alignment method, which results in visual\ninformation loss. To address the issue, we propose a visual alignment mechanism\nwith a visual embedding highway module. We then introduce a resource-efficient\nrecipe for training X-VILA, that exhibits proficiency in any-to-any modality\nconversation, surpassing previous approaches by large margins. X-VILA also\nshowcases emergent properties across modalities even in the absence of similar\ntraining data. The project will be made open-source.",
        "updated": "2024-05-29 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.19335v1"
    },
    {
        "title": "LLMs Meet Multimodal Generation and Editing: A Survey",
        "authors": "Yingqing HeZhaoyang LiuJingye ChenZeyue TianHongyu LiuXiaowei ChiRuntao LiuRuibin YuanYazhou XingWenhai WangJifeng DaiYong ZhangWei XueQifeng LiuYike GuoQifeng Chen",
        "links": "http://arxiv.org/abs/2405.19334v1",
        "entry_id": "http://arxiv.org/abs/2405.19334v1",
        "pdf_url": "http://arxiv.org/pdf/2405.19334v1",
        "summary": "With the recent advancement in large language models (LLMs), there is a\ngrowing interest in combining LLMs with multimodal learning. Previous surveys\nof multimodal large language models (MLLMs) mainly focus on understanding. This\nsurvey elaborates on multimodal generation across different domains, including\nimage, video, 3D, and audio, where we highlight the notable advancements with\nmilestone works in these fields. Specifically, we exhaustively investigate the\nkey technical components behind methods and multimodal datasets utilized in\nthese studies. Moreover, we dig into tool-augmented multimodal agents that can\nuse existing generative models for human-computer interaction. Lastly, we also\ncomprehensively discuss the advancement in AI safety and investigate emerging\napplications as well as future prospects. Our work provides a systematic and\ninsightful overview of multimodal generation, which is expected to advance the\ndevelopment of Artificial Intelligence for Generative Content (AIGC) and world\nmodels. A curated list of all related papers can be found at\nhttps://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation",
        "updated": "2024-05-29 17:59:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.19334v1"
    },
    {
        "title": "Multi-Modal Generative Embedding Model",
        "authors": "Feipeng MaHongwei XueGuangting WangYizhou ZhouFengyun RaoShilin YanYueyi ZhangSiying WuMike Zheng ShouXiaoyan Sun",
        "links": "http://arxiv.org/abs/2405.19333v1",
        "entry_id": "http://arxiv.org/abs/2405.19333v1",
        "pdf_url": "http://arxiv.org/pdf/2405.19333v1",
        "summary": "Most multi-modal tasks can be formulated into problems of either generation\nor embedding. Existing models usually tackle these two types of problems by\ndecoupling language modules into a text decoder for generation, and a text\nencoder for embedding. To explore the minimalism of multi-modal paradigms, we\nattempt to achieve only one model per modality in this work. We propose a\nMulti-Modal Generative Embedding Model (MM-GEM), whereby the generative and\nembedding objectives are encapsulated in one Large Language Model. We also\npropose a PoolAggregator to boost efficiency and enable the ability of\nfine-grained embedding and generation. A surprising finding is that these two\nobjectives do not significantly conflict with each other. For example, MM-GEM\ninstantiated from ViT-Large and TinyLlama shows competitive performance on\nbenchmarks for multimodal embedding models such as cross-modal retrieval and\nzero-shot classification, while has good ability of image captioning.\nAdditionally, MM-GEM can seamlessly execute region-level image caption\ngeneration and retrieval tasks. Besides, the advanced text model in MM-GEM\nbrings over 5% improvement in Recall@1 for long text and image retrieval.",
        "updated": "2024-05-29 17:59:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.19333v1"
    },
    {
        "title": "NPGA: Neural Parametric Gaussian Avatars",
        "authors": "Simon GiebenhainTobias KirschsteinMartin RünzLourdes AgapitoMatthias Nießner",
        "links": "http://arxiv.org/abs/2405.19331v1",
        "entry_id": "http://arxiv.org/abs/2405.19331v1",
        "pdf_url": "http://arxiv.org/pdf/2405.19331v1",
        "summary": "The creation of high-fidelity, digital versions of human heads is an\nimportant stepping stone in the process of further integrating virtual\ncomponents into our everyday lives. Constructing such avatars is a challenging\nresearch problem, due to a high demand for photo-realism and real-time\nrendering performance. In this work, we propose Neural Parametric Gaussian\nAvatars (NPGA), a data-driven approach to create high-fidelity, controllable\navatars from multi-view video recordings. We build our method around 3D\nGaussian Splatting for its highly efficient rendering and to inherit the\ntopological flexibility of point clouds. In contrast to previous work, we\ncondition our avatars' dynamics on the rich expression space of neural\nparametric head models (NPHM), instead of mesh-based 3DMMs. To this end, we\ndistill the backward deformation field of our underlying NPHM into forward\ndeformations which are compatible with rasterization-based rendering. All\nremaining fine-scale, expression-dependent details are learned from the\nmulti-view videos. To increase the representational capacity of our avatars, we\naugment the canonical Gaussian point cloud using per-primitive latent features\nwhich govern its dynamic behavior. To regularize this increased dynamic\nexpressivity, we propose Laplacian terms on the latent features and predicted\ndynamics. We evaluate our method on the public NeRSemble dataset, demonstrating\nthat NPGA significantly outperforms the previous state-of-the-art avatars on\nthe self-reenactment task by 2.6 PSNR. Furthermore, we demonstrate accurate\nanimation capabilities from real-world monocular videos.",
        "updated": "2024-05-29 17:58:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.19331v1"
    },
    {
        "title": "Reasoning3D -- Grounding and Reasoning in 3D: Fine-Grained Zero-Shot Open-Vocabulary 3D Reasoning Part Segmentation via Large Vision-Language Models",
        "authors": "Tianrun ChenChunan YuJing LiJianqi ZhangLanyun ZhuDeyi JiYong ZhangYing ZangZejian LiLingyun Sun",
        "links": "http://arxiv.org/abs/2405.19326v1",
        "entry_id": "http://arxiv.org/abs/2405.19326v1",
        "pdf_url": "http://arxiv.org/pdf/2405.19326v1",
        "summary": "In this paper, we introduce a new task: Zero-Shot 3D Reasoning Segmentation\nfor parts searching and localization for objects, which is a new paradigm to 3D\nsegmentation that transcends limitations for previous category-specific 3D\nsemantic segmentation, 3D instance segmentation, and open-vocabulary 3D\nsegmentation. We design a simple baseline method, Reasoning3D, with the\ncapability to understand and execute complex commands for (fine-grained)\nsegmenting specific parts for 3D meshes with contextual awareness and reasoned\nanswers for interactive segmentation. Specifically, Reasoning3D leverages an\noff-the-shelf pre-trained 2D segmentation network, powered by Large Language\nModels (LLMs), to interpret user input queries in a zero-shot manner. Previous\nresearch have shown that extensive pre-training endows foundation models with\nprior world knowledge, enabling them to comprehend complex commands, a\ncapability we can harness to \"segment anything\" in 3D with limited 3D datasets\n(source efficient). Experimentation reveals that our approach is generalizable\nand can effectively localize and highlight parts of 3D objects (in 3D mesh)\nbased on implicit textual queries, including these articulated 3d objects and\nreal-world scanned data. Our method can also generate natural language\nexplanations corresponding to these 3D models and the decomposition. Moreover,\nour training-free approach allows rapid deployment and serves as a viable\nuniversal baseline for future research of part-level 3d (semantic) object\nunderstanding in various fields including robotics, object manipulation, part\nassembly, autonomous driving applications, augment reality and virtual reality\n(AR/VR), and medical applications. The code, the model weight, the deployment\nguide, and the evaluation protocol are: http://tianrun-chen.github.io/Reason3D/",
        "updated": "2024-05-29 17:56:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.19326v1"
    }
]