[
    {
        "title": "Normative Modules: A Generative Agent Architecture for Learning Norms that Supports Multi-Agent Cooperation",
        "authors": "Atrisha SarkarAndrei Ioan MuresanuCarter BlairAaryam SharmaRakshit S TrivediGillian K Hadfield",
        "links": "http://arxiv.org/abs/2405.19328v1",
        "entry_id": "http://arxiv.org/abs/2405.19328v1",
        "pdf_url": "http://arxiv.org/pdf/2405.19328v1",
        "summary": "Generative agents, which implement behaviors using a large language model\n(LLM) to interpret and evaluate an environment, has demonstrated the capacity\nto solve complex tasks across many social and technological domains. However,\nwhen these agents interact with other agents and humans in presence of social\nstructures such as existing norms, fostering cooperation between them is a\nfundamental challenge. In this paper, we develop the framework of a 'Normative\nModule': an architecture designed to enhance cooperation by enabling agents to\nrecognize and adapt to the normative infrastructure of a given environment. We\nfocus on the equilibrium selection aspect of the cooperation problem and inform\nour agent design based on the existence of classification institutions that\nimplement correlated equilibrium to provide effective resolution of the\nequilibrium selection problem. Specifically, the normative module enables\nagents to learn through peer interactions which of multiple candidate\ninstitutions in the environment, does a group treat as authoritative. By\nenabling normative competence in this sense, agents gain ability to coordinate\ntheir sanctioning behaviour; coordinated sanctioning behaviour in turn shapes\nprimary behaviour within a social environment, leading to higher average\nwelfare. We design a new environment that supports institutions and evaluate\nthe proposed framework based on two key criteria derived from agent\ninteractions with peers and institutions: (i) the agent's ability to disregard\nnon-authoritative institutions and (ii) the agent's ability to identify\nauthoritative institutions among several options. We show that these\ncapabilities allow the agent to achieve more stable cooperative outcomes\ncompared to baseline agents without the normative module, paving the way for\nresearch in a new avenue of designing environments and agents that account for\nnormative infrastructure.",
        "updated": "2024-05-29 17:57:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.19328v1"
    },
    {
        "title": "Act Natural! Projecting Autonomous System Trajectories Into Naturalistic Behavior Sets",
        "authors": "Hamzah I. KhanAdam J. ThorpeDavid Fridovich-Keil",
        "links": "http://arxiv.org/abs/2405.19292v1",
        "entry_id": "http://arxiv.org/abs/2405.19292v1",
        "pdf_url": "http://arxiv.org/pdf/2405.19292v1",
        "summary": "Autonomous agents operating around human actors must consider how their\nbehaviors might affect those humans, even when not directly interacting with\nthem. To this end, it is often beneficial to be predictable and appear\nnaturalistic. Existing methods to address this problem use human actor intent\nmodeling or imitation learning techniques, but these approaches rarely capture\nall possible motivations for human behavior or require significant amounts of\ndata. In contrast, we propose a technique for modeling naturalistic behavior as\na set of convex hulls computed over a relatively small dataset of human\nbehavior. Given this set, we design an optimization-based filter which projects\narbitrary trajectories into it to make them more naturalistic for autonomous\nagents to execute while also satisfying dynamics constraints. We demonstrate\nour methods on real-world human driving data from the inD intersection dataset\n(Bock et al., 2020).",
        "updated": "2024-05-29 17:21:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.19292v1"
    },
    {
        "title": "Inverse Concave-Utility Reinforcement Learning is Inverse Game Theory",
        "authors": "Mustafa Mert ÇelikokFrans A. OliehoekJan-Willem van de Meent",
        "links": "http://arxiv.org/abs/2405.19024v1",
        "entry_id": "http://arxiv.org/abs/2405.19024v1",
        "pdf_url": "http://arxiv.org/pdf/2405.19024v1",
        "summary": "We consider inverse reinforcement learning problems with concave utilities.\nConcave Utility Reinforcement Learning (CURL) is a generalisation of the\nstandard RL objective, which employs a concave function of the state occupancy\nmeasure, rather than a linear function. CURL has garnered recent attention for\nits ability to represent instances of many important applications including the\nstandard RL such as imitation learning, pure exploration, constrained MDPs,\noffline RL, human-regularized RL, and others. Inverse reinforcement learning is\na powerful paradigm that focuses on recovering an unknown reward function that\ncan rationalize the observed behaviour of an agent. There has been recent\ntheoretical advances in inverse RL where the problem is formulated as\nidentifying the set of feasible reward functions. However, inverse RL for CURL\nproblems has not been considered previously. In this paper we show that most of\nthe standard IRL results do not apply to CURL in general, since CURL\ninvalidates the classical Bellman equations. This calls for a new theoretical\nframework for the inverse CURL problem. Using a recent equivalence result\nbetween CURL and Mean-field Games, we propose a new definition for the feasible\nrewards for I-CURL by proving that this problem is equivalent to an inverse\ngame theory problem in a subclass of mean-field games. We present initial query\nand sample complexity results for the I-CURL problem under assumptions such as\nLipschitz-continuity. Finally, we outline future directions and applications in\nhuman--AI collaboration enabled by our results.",
        "updated": "2024-05-29 12:07:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.19024v1"
    },
    {
        "title": "Distributed Management of Fluctuating Energy Resources in Dynamic Networked Systems",
        "authors": "Xiaotong ChengIoannis TsetisSetareh Maghsudi",
        "links": "http://arxiv.org/abs/2405.19015v1",
        "entry_id": "http://arxiv.org/abs/2405.19015v1",
        "pdf_url": "http://arxiv.org/pdf/2405.19015v1",
        "summary": "Modern power systems integrate renewable distributed energy resources (DERs)\nas an environment-friendly enhancement to meet the ever-increasing demands.\nHowever, the inherent unreliability of renewable energy renders developing DER\nmanagement algorithms imperative. We study the energy-sharing problem in a\nsystem consisting of several DERs. Each agent harvests and distributes\nrenewable energy in its neighborhood to optimize the network's performance\nwhile minimizing energy waste. We model this problem as a bandit convex\noptimization problem with constraints that correspond to each node's\nlimitations for energy production. We propose distributed decision-making\npolicies to solve the formulated problem, where we utilize the notion of\ndynamic regret as the performance metric. We also include an adjustment\nstrategy in our developed algorithm to reduce the constraint violations.\nBesides, we design a policy that deals with the non-stationary environment.\nTheoretical analysis shows the effectiveness of our proposed algorithm.\nNumerical experiments using a real-world dataset show superior performance of\nour proposal compared to state-of-the-art methods.",
        "updated": "2024-05-29 11:54:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.19015v1"
    },
    {
        "title": "Resilient Average Consensus with Adversaries via Distributed Detection and Recovery",
        "authors": "Liwei YuanHideaki Ishii",
        "links": "http://arxiv.org/abs/2405.18752v1",
        "entry_id": "http://arxiv.org/abs/2405.18752v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18752v1",
        "summary": "We study the problem of resilient average consensus in multi-agent systems\nwhere some of the agents are subject to failures or attacks. The objective of\nresilient average consensus is for non-faulty/normal agents to converge to the\naverage of their initial values despite the erroneous effects from malicious\nagents. To this end, we propose a successful distributed iterative resilient\naverage consensus algorithm for the multi-agent networks with general directed\ntopologies. The proposed algorithm has two parts at each iteration: detection\nand averaging. For the detection part, we propose two distributed algorithms\nand one of them can detect malicious agents with only the information from\ndirect in-neighbors. For the averaging part, we extend the applicability of an\nexisting averaging algorithm where normal agents can remove the effects from\nmalicious agents so far, after they are detected. Another important feature of\nour method is that it can handle the case where malicious agents are\nneighboring and collaborating with each other to mislead the normal ones from\naveraging. This case cannot be solved by existing detection approaches in\nrelated literature. Moreover, our algorithm is efficient in storage usage\nespecially for large-scale networks as each agent only requires the values of\nneighbors within two hops. Lastly, numerical examples are given to verify the\nefficacy of the proposed algorithms.",
        "updated": "2024-05-29 04:32:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18752v1"
    }
]