[
    {
        "title": "X-VILA: Cross-Modality Alignment for Large Language Model",
        "authors": "Hanrong YeDe-An HuangYao LuZhiding YuWei PingAndrew TaoJan KautzSong HanDan XuPavlo MolchanovHongxu Yin",
        "links": "http://arxiv.org/abs/2405.19335v1",
        "entry_id": "http://arxiv.org/abs/2405.19335v1",
        "pdf_url": "http://arxiv.org/pdf/2405.19335v1",
        "summary": "We introduce X-VILA, an omni-modality model designed to extend the\ncapabilities of large language models (LLMs) by incorporating image, video, and\naudio modalities. By aligning modality-specific encoders with LLM inputs and\ndiffusion decoders with LLM outputs, X-VILA achieves cross-modality\nunderstanding, reasoning, and generation. To facilitate this cross-modality\nalignment, we curate an effective interleaved any-to-any modality\ninstruction-following dataset. Furthermore, we identify a significant problem\nwith the current cross-modality alignment method, which results in visual\ninformation loss. To address the issue, we propose a visual alignment mechanism\nwith a visual embedding highway module. We then introduce a resource-efficient\nrecipe for training X-VILA, that exhibits proficiency in any-to-any modality\nconversation, surpassing previous approaches by large margins. X-VILA also\nshowcases emergent properties across modalities even in the absence of similar\ntraining data. The project will be made open-source.",
        "updated": "2024-05-29 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.19335v1"
    },
    {
        "title": "Self-Exploring Language Models: Active Preference Elicitation for Online Alignment",
        "authors": "Shenao ZhangDonghan YuHiteshi SharmaZiyi YangShuohang WangHany HassanZhaoran Wang",
        "links": "http://arxiv.org/abs/2405.19332v1",
        "entry_id": "http://arxiv.org/abs/2405.19332v1",
        "pdf_url": "http://arxiv.org/pdf/2405.19332v1",
        "summary": "Preference optimization, particularly through Reinforcement Learning from\nHuman Feedback (RLHF), has achieved significant success in aligning Large\nLanguage Models (LLMs) to adhere to human intentions. Unlike offline alignment\nwith a fixed dataset, online feedback collection from humans or AI on model\ngenerations typically leads to more capable reward models and better-aligned\nLLMs through an iterative process. However, achieving a globally accurate\nreward model requires systematic exploration to generate diverse responses that\nspan the vast space of natural language. Random sampling from standard\nreward-maximizing LLMs alone is insufficient to fulfill this requirement. To\naddress this issue, we propose a bilevel objective optimistically biased\ntowards potentially high-reward responses to actively explore\nout-of-distribution regions. By solving the inner-level problem with the\nreparameterized reward function, the resulting algorithm, named Self-Exploring\nLanguage Models (SELM), eliminates the need for a separate RM and iteratively\nupdates the LLM with a straightforward objective. Compared to Direct Preference\nOptimization (DPO), the SELM objective reduces indiscriminate favor of unseen\nextrapolations and enhances exploration efficiency. Our experimental results\ndemonstrate that when finetuned on Zephyr-7B-SFT and Llama-3-8B-Instruct\nmodels, SELM significantly boosts the performance on instruction-following\nbenchmarks such as MT-Bench and AlpacaEval 2.0, as well as various standard\nacademic benchmarks in different settings. Our code and models are available at\nhttps://github.com/shenao-zhang/SELM.",
        "updated": "2024-05-29 17:59:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.19332v1"
    },
    {
        "title": "MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series",
        "authors": "Ge ZhangScott QuJiaheng LiuChenchen ZhangChenghua LinChou Leuang YuDanny PanEsther ChengJie LiuQunshu LinRaven YuanTuney ZhengWei PangXinrun DuYiming LiangYinghao MaYizhi LiZiyang MaBill LinEmmanouil BenetosHuan YangJunting ZhouKaijing MaMinghao LiuMorry NiuNoah WangQuehry QueRuibo LiuSine LiuShawn GuoSoren GaoWangchunshu ZhouXinyue ZhangYizhi ZhouYubo WangYuelin BaiYuhan ZhangYuxiang ZhangZenith WangZhenzhu YangZijian ZhaoJiajun ZhangWanli OuyangWenhao HuangWenhu Chen",
        "links": "http://arxiv.org/abs/2405.19327v1",
        "entry_id": "http://arxiv.org/abs/2405.19327v1",
        "pdf_url": "http://arxiv.org/pdf/2405.19327v1",
        "summary": "Large Language Models (LLMs) have made great strides in recent years to\nachieve unprecedented performance across different tasks. However, due to\ncommercial interest, the most competitive models like GPT, Gemini, and Claude\nhave been gated behind proprietary interfaces without disclosing the training\ndetails. Recently, many institutions have open-sourced several strong LLMs like\nLLaMA-3, comparable to existing closed-source LLMs. However, only the model's\nweights are provided with most details (e.g., intermediate checkpoints,\npre-training corpus, and training code, etc.) being undisclosed. To improve the\ntransparency of LLMs, the research community has formed to open-source truly\nopen LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-training\ncorpus and training code) are being provided. These models have greatly\nadvanced the scientific study of these large models including their strengths,\nweaknesses, biases and risks. However, we observe that the existing truly open\nLLMs on reasoning, knowledge, and coding tasks are still inferior to existing\nstate-of-the-art LLMs with similar model sizes. To this end, we open-source\nMAP-Neo, a highly capable and transparent bilingual language model with 7B\nparameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is the\nfirst fully open-sourced bilingual LLM with comparable performance compared to\nexisting state-of-the-art LLMs. Moreover, we open-source all details to\nreproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning\npipeline, checkpoints, and well-optimized training/evaluation framework are\nprovided. Finally, we hope our MAP-Neo will enhance and strengthen the open\nresearch community and inspire more innovations and creativities to facilitate\nthe further improvements of LLMs.",
        "updated": "2024-05-29 17:57:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.19327v1"
    },
    {
        "title": "Are Large Language Models Chameleons?",
        "authors": "Mingmeng GengSihong HeRoberto Trotta",
        "links": "http://arxiv.org/abs/2405.19323v1",
        "entry_id": "http://arxiv.org/abs/2405.19323v1",
        "pdf_url": "http://arxiv.org/pdf/2405.19323v1",
        "summary": "Do large language models (LLMs) have their own worldviews and personality\ntendencies? Simulations in which an LLM was asked to answer subjective\nquestions were conducted more than 1 million times. Comparison of the responses\nfrom different LLMs with real data from the European Social Survey (ESS)\nsuggests that the effect of prompts on bias and variability is fundamental,\nhighlighting major cultural, age, and gender biases. Methods for measuring the\ndifference between LLMs and survey data are discussed, such as calculating\nweighted means and a new proposed measure inspired by Jaccard similarity. We\nconclude that it is important to analyze the robustness and variability of\nprompts before using LLMs to model individual decisions or collective behavior,\nas their imitation abilities are approximate at best.",
        "updated": "2024-05-29 17:54:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.19323v1"
    },
    {
        "title": "Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF",
        "authors": "Shicong CenJincheng MeiKatayoon GoshvadiHanjun DaiTong YangSherry YangDale SchuurmansYuejie ChiBo Dai",
        "links": "http://arxiv.org/abs/2405.19320v1",
        "entry_id": "http://arxiv.org/abs/2405.19320v1",
        "pdf_url": "http://arxiv.org/pdf/2405.19320v1",
        "summary": "Reinforcement learning from human feedback (RLHF) has demonstrated great\npromise in aligning large language models (LLMs) with human preference.\nDepending on the availability of preference data, both online and offline RLHF\nare active areas of investigation. A key bottleneck is understanding how to\nincorporate uncertainty estimation in the reward function learned from the\npreference data for RLHF, regardless of how the preference data is collected.\nWhile the principles of optimism or pessimism under uncertainty are\nwell-established in standard reinforcement learning (RL), a\npractically-implementable and theoretically-grounded form amenable to large\nlanguage models is not yet available, as standard techniques for constructing\nconfidence intervals become intractable under arbitrary policy\nparameterizations.\n  In this paper, we introduce a unified approach to online and offline RLHF --\nvalue-incentivized preference optimization (VPO) -- which regularizes the\nmaximum-likelihood estimate of the reward function with the corresponding value\nfunction, modulated by a $\\textit{sign}$ to indicate whether the optimism or\npessimism is chosen. VPO also directly optimizes the policy with implicit\nreward modeling, and therefore shares a simpler RLHF pipeline similar to direct\npreference optimization. Theoretical guarantees of VPO are provided for both\nonline and offline settings, matching the rates of their standard RL\ncounterparts. Moreover, experiments on text summarization and dialog verify the\npracticality and effectiveness of VPO.",
        "updated": "2024-05-29 17:51:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.19320v1"
    }
]