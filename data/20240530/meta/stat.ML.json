[
    {
        "title": "Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF",
        "authors": "Shicong CenJincheng MeiKatayoon GoshvadiHanjun DaiTong YangSherry YangDale SchuurmansYuejie ChiBo Dai",
        "links": "http://arxiv.org/abs/2405.19320v1",
        "entry_id": "http://arxiv.org/abs/2405.19320v1",
        "pdf_url": "http://arxiv.org/pdf/2405.19320v1",
        "summary": "Reinforcement learning from human feedback (RLHF) has demonstrated great\npromise in aligning large language models (LLMs) with human preference.\nDepending on the availability of preference data, both online and offline RLHF\nare active areas of investigation. A key bottleneck is understanding how to\nincorporate uncertainty estimation in the reward function learned from the\npreference data for RLHF, regardless of how the preference data is collected.\nWhile the principles of optimism or pessimism under uncertainty are\nwell-established in standard reinforcement learning (RL), a\npractically-implementable and theoretically-grounded form amenable to large\nlanguage models is not yet available, as standard techniques for constructing\nconfidence intervals become intractable under arbitrary policy\nparameterizations.\n  In this paper, we introduce a unified approach to online and offline RLHF --\nvalue-incentivized preference optimization (VPO) -- which regularizes the\nmaximum-likelihood estimate of the reward function with the corresponding value\nfunction, modulated by a $\\textit{sign}$ to indicate whether the optimism or\npessimism is chosen. VPO also directly optimizes the policy with implicit\nreward modeling, and therefore shares a simpler RLHF pipeline similar to direct\npreference optimization. Theoretical guarantees of VPO are provided for both\nonline and offline settings, matching the rates of their standard RL\ncounterparts. Moreover, experiments on text summarization and dialog verify the\npracticality and effectiveness of VPO.",
        "updated": "2024-05-29 17:51:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.19320v1"
    },
    {
        "title": "Adaptive Generalized Neyman Allocation: Local Asymptotic Minimax Optimal Best Arm Identification",
        "authors": "Masahiro Kato",
        "links": "http://arxiv.org/abs/2405.19317v1",
        "entry_id": "http://arxiv.org/abs/2405.19317v1",
        "pdf_url": "http://arxiv.org/pdf/2405.19317v1",
        "summary": "This study investigates a local asymptotic minimax optimal strategy for\nfixed-budget best arm identification (BAI). We propose the Adaptive Generalized\nNeyman Allocation (AGNA) strategy and show that its worst-case upper bound of\nthe probability of misidentifying the best arm aligns with the worst-case lower\nbound under the small-gap regime, where the gap between the expected outcomes\nof the best and suboptimal arms is small. Our strategy corresponds to a\ngeneralization of the Neyman allocation for two-armed bandits (Neyman, 1934;\nKaufmann et al., 2016) and a refinement of existing strategies such as the ones\nproposed by Glynn & Juneja (2004) and Shin et al. (2018). Compared to Komiyama\net al. (2022), which proposes a minimax rate-optimal strategy, our proposed\nstrategy has a tighter upper bound that exactly matches the lower bound,\nincluding the constant terms, by restricting the class of distributions to the\nclass of small-gap distributions. Our result contributes to the longstanding\nopen issue about the existence of asymptotically optimal strategies in\nfixed-budget BAI, by presenting the local asymptotic minimax optimal strategy.",
        "updated": "2024-05-29 17:43:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.19317v1"
    },
    {
        "title": "Valid Conformal Prediction for Dynamic GNNs",
        "authors": "Ed DavisIan GallagherDaniel John LawsonPatrick Rubin-Delanchy",
        "links": "http://arxiv.org/abs/2405.19230v1",
        "entry_id": "http://arxiv.org/abs/2405.19230v1",
        "pdf_url": "http://arxiv.org/pdf/2405.19230v1",
        "summary": "Graph neural networks (GNNs) are powerful black-box models which have shown\nimpressive empirical performance. However, without any form of uncertainty\nquantification, it can be difficult to trust such models in high-risk\nscenarios. Conformal prediction aims to address this problem, however, an\nassumption of exchangeability is required for its validity which has limited\nits applicability to static graphs and transductive regimes. We propose to use\nunfolding, which allows any existing static GNN to output a dynamic graph\nembedding with exchangeability properties. Using this, we extend the validity\nof conformal prediction to dynamic GNNs in both transductive and semi-inductive\nregimes. We provide a theoretical guarantee of valid conformal prediction in\nthese cases and demonstrate the empirical validity, as well as the performance\ngains, of unfolded GNNs against standard GNN architectures on both simulated\nand real datasets.",
        "updated": "2024-05-29 16:07:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.19230v1"
    },
    {
        "title": "Matrix Manifold Neural Networks++",
        "authors": "Xuan Son NguyenShuo YangAymeric Histace",
        "links": "http://arxiv.org/abs/2405.19206v1",
        "entry_id": "http://arxiv.org/abs/2405.19206v1",
        "pdf_url": "http://arxiv.org/pdf/2405.19206v1",
        "summary": "Deep neural networks (DNNs) on Riemannian manifolds have garnered increasing\ninterest in various applied areas. For instance, DNNs on spherical and\nhyperbolic manifolds have been designed to solve a wide range of computer\nvision and nature language processing tasks. One of the key factors that\ncontribute to the success of these networks is that spherical and hyperbolic\nmanifolds have the rich algebraic structures of gyrogroups and gyrovector\nspaces. This enables principled and effective generalizations of the most\nsuccessful DNNs to these manifolds. Recently, some works have shown that many\nconcepts in the theory of gyrogroups and gyrovector spaces can also be\ngeneralized to matrix manifolds such as Symmetric Positive Definite (SPD) and\nGrassmann manifolds. As a result, some building blocks for SPD and Grassmann\nneural networks, e.g., isometric models and multinomial logistic regression\n(MLR) can be derived in a way that is fully analogous to their spherical and\nhyperbolic counterparts. Building upon these works, we design fully-connected\n(FC) and convolutional layers for SPD neural networks. We also develop MLR on\nSymmetric Positive Semi-definite (SPSD) manifolds, and propose a method for\nperforming backpropagation with the Grassmann logarithmic map in the projector\nperspective. We demonstrate the effectiveness of the proposed approach in the\nhuman action recognition and node classification tasks.",
        "updated": "2024-05-29 15:47:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.19206v1"
    },
    {
        "title": "Online Linear Regression in Dynamic Environments via Discounting",
        "authors": "Andrew JacobsenAshok Cutkosky",
        "links": "http://arxiv.org/abs/2405.19175v1",
        "entry_id": "http://arxiv.org/abs/2405.19175v1",
        "pdf_url": "http://arxiv.org/pdf/2405.19175v1",
        "summary": "We develop algorithms for online linear regression which achieve optimal\nstatic and dynamic regret guarantees \\emph{even in the complete absence of\nprior knowledge}. We present a novel analysis showing that a discounted variant\nof the Vovk-Azoury-Warmuth forecaster achieves dynamic regret of the form\n$R_{T}(\\vec{u})\\le O\\left(d\\log(T)\\vee\n\\sqrt{dP_{T}^{\\gamma}(\\vec{u})T}\\right)$, where $P_{T}^{\\gamma}(\\vec{u})$ is a\nmeasure of variability of the comparator sequence, and show that the discount\nfactor achieving this result can be learned on-the-fly. We show that this\nresult is optimal by providing a matching lower bound. We also extend our\nresults to \\emph{strongly-adaptive} guarantees which hold over every\nsub-interval $[a,b]\\subseteq[1,T]$ simultaneously.",
        "updated": "2024-05-29 15:17:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.19175v1"
    }
]