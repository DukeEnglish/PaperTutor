Preprint
Advancing LLM Reasoning Generalists with Preference Trees
LifanYuan1,2,‚àóGanquCui1‚àó,‚Ä†HanbinWang3,4‚àó,NingDing1,‚Ä†XingyaoWang2,JiaDeng5,
BojiShan6,HuiminChen1,RuobingXie7,YankaiLin5,ZhenghaoLiu3,BowenZhou1,
HaoPeng2,ZhiyuanLiu1,‚Ä†MaosongSun1
1TsinghuaUniversity2UniversityofIllinoisUrbana-Champaign3NortheasternUniversity
4ModelBest.Inc 5RenminUniversityofChina 6BUPT 7Tencent
lifan4@illinois.edu cgq22@mails.tsinghua.edu.cn wanghanbinpanda@gmail.com
Abstract
WeintroduceEURUS,asuiteoflargelanguagemodels(LLMs)optimized
for reasoning. Finetuned from Mistral-7B and CodeLlama-70B, EURUS
models achieve state-of-the-art results among open-source models on a
diverse set of benchmarks covering mathematics, code generation, and
logicalreasoningproblems. Notably,EURUS-70BbeatsGPT-3.5Turboin
reasoningthroughacomprehensivebenchmarkingacross12testscovering
fivetasks,andachievesa33.3%pass@1accuracyonLeetCodeand32.6%
onTheoremQA,twochallengingbenchmarks,substantiallyoutperform-
ingexistingopen-sourcemodelsbymarginsmorethan13.3%. Thestrong
performanceof EURUS canbeprimarilyattributedto ULTRAINTERACT,
ournewly-curatedlarge-scale,high-qualityalignmentdatasetspecifically
designedforcomplexreasoningtasks. ULTRAINTERACT canbeusedin
bothsupervisedfine-tuningandpreferencelearning. Foreachinstruction,
itincludesapreferencetreeconsistingof(1)reasoningchainswithdiverse
planningstrategiesinaunifiedformat,(2)multi-turninteractiontrajectories
withtheenvironmentandthecritique,and(3)pairwisedatatofacilitate
preferencelearning. ULTRAINTERACT allowsustoconductanin-depth
explorationofpreferencelearningforreasoningtasks. Ourinvestigationre-
vealsthatsomewell-establishedpreferencelearningalgorithmsmaybeless
suitableforreasoningtaskscomparedtotheireffectivenessingeneralcon-
versations. Inspiredbythis,wederiveanovelrewardmodelingobjective
which,togetherwithULTRAINTERACT,leadstoastrongrewardmodel.1
45
~7B GPT-4
40 ~40B
~70B
GPT-Series
35 Ours
Eurus-70B-NCA
30 DeepSeek-Coder-33B-Instruct
ed25 MagiCoder-S-DS-6.7B GPT-3.5-Turbo
o C20 DeepSeek-LLM-67B-Chat OpenCI-CL-70B
teeL
15
CL-70B-InstructOpenCI-DS-6.7 QB
WenE 1u .r 5u -s 7- 27 BB -- CK hT aO
t
OpenMath-CL-70B
10 OpenMath-Mistral-7B
WizardMath-7B -v1.1
5 Mistral-7B-Instruct-v0.2 Mixtral-8x7B-Instruct
Zephyr-7B-ùõΩ
0
5 10 15 20 25 30 35 40 5455 50 55
TheoremQA
Figure1: EvaluationresultsonLeetCodeandTheoremQA,twochallengingOODcoding
andmathbenchmarkswithonlytestsets. OurEURUS-7Biscomparablewithbaselinesthat
are10xlargerandEURUS-70BistheonlyoneonparwithGPT-3.5Turbo.
‚àóEqualContribution.
‚Ä†CorrespondingAuthors.
1Modelsanddataareavailableat:https://github.com/OpenBMB/Eurus.
1
4202
rpA
2
]IA.sc[
1v87020.4042:viXraPreprint
1 Introduction
Currentalignmenttechniqueshavesignificantlyadvancedthedevelopmentofopen-source
largelanguagemodels(LLMs)thateffectivelymeetuserexpectationsandalignwithhuman
values(Touvronetal.,2023;Tunstalletal.,2023). Oncomplexreasoning,successhasbeen
achievedbyspecializingmodelsforspecificcapabilities,suchascoding(Weietal.,2023;
Guoetal.,2024a;Zhengetal.,2024)andsolvingmathproblems(Fuetal.,2023;Yueetal.,
2023;Luoetal.,2023a;Toshniwaletal.,2024). However,thesemodelsstillfallshort,by
large margins, of the most advanced proprietary models in their all-around capabilities
to tackle a diverse range of challenging problems. We conjecture that this performance
gap can be primarily attributed to (1) the lack of high-quality alignment data and (2)
the underexploration of preference learning techniques for improving models‚Äô complex
reasoningcapabilities.Inthispaper,wetakestridestowardsbridgingthisgapbyaddressing
bothfactorsanddevelopingEURUS.
EURUS consists of a suite of LLMs finetuned from Mistral-7B (Jiang et al., 2023a) and
CodeLLaMA-70B(Roziereetal.,2023). Acrossadiversesetofcomplexreasoningbench-
marksthataremostlyout-of-distribution(OOD),EURUSachievesstate-of-the-artoverall
performanceamongallopen-sourcemodels. Inparticular, EURUSexcelsinsolvingchal-
lengingproblemsthatoftenrequiresophisticatedplanning,reasoning,toolintegration,and
theabilitytointeractwithandlearnfromtheenvironmentandusers. AsshowninFigure1,
onuniversity-levelSTEMquestionsTheoremQA(Chenetal.,2023)andcompetition-level
codingproblemsLeetCodeContest(Guoetal.,2024a),EURUS-70Bsignificantlyoutperforms
allopen-sourcemodels,achievingcomparableperformancetoGPT-3.5Turbo.
EURUS models are trained on ULTRAINTERACT, our newly-curated, large-scale, and
high-qualityalignmentdataspecificallydesignedtoimproveLLMs‚Äôreasoningcapabilities.
ULTRAINTERACTconsistsofadiversesetofinstructionsspanningmath,coding,andlogical
reasoningproblemsfrom12establisheddatasets. Foreachinstruction,ULTRAINTERACT
collectsapreferencetreethatincludes: (1)Diverseplanningstrategiesinaunifiedpattern,
suchassequentialprocessing(Weietal.,2022)andtoolcreation(Qianetal.,2023),followed
by executing step-by-step actions formatted in either text or code, to provide divserse
reasoning trajectories. (2) Multi-turn interaction trajectories with the environment
and the critique, to improve models‚Äô capabilities to learn from feedback and correct
previous errors (Wang et al., 2023b). (3) Paired correct and incorrect actions organized
intreestructures,tofacilitatepreferencelearning. Intotal,ULTRAINTERACTcontains86K
instructions and 220K action pairs, where each pair consists of an instruction, a correct
response,andanincorrectone. Conceptually,ULTRAINTERACT‚Äôsdataresembleimbalanced
binarytreesasshowninFigure2.
ULTRAINTERACT can be used in both supervised fine-tuning and preference learning.
Our experiments show that, using ULTRAINTERACT along with established datasets in
instruction fine-tuning already achieves strong performance. ULTRAINTERACT further
facilitatespreferencelearningforreasoningtasks,improvingtheperformanceevenfurther
withKTO(Ethayarajhetal.,2024)andNCA(Chenetal.,2024a). Surprisingly,appliedtoan
instructionfinetunedEURUSmodel,DPO(Rafailovetal.,2023)hurtstheperformance.
Throughcarefulanalysis,weprovideevidencethattheperformanceinreasoningcorrelates
withthevalueofrewardsofchosendata‚Äîahigherfinalrewardoftenindicatesabetter
reasoning capability. Besides, our investigation suggests that DPO may be less suitable
forreasoningtasksthanKTOandNCA.Inspiredbythisfreshfinding,wedeviseanew
objectiveforrewardmodelingtoaugmenttheBradley-Terryobjective(Bradley&Terry,
1952),explicitlyencouragingtrainingtoincreasetheabsoluterewardsofchosensolution
anddecreasethoseofrejecteddata. Furthermore, ULTRAINTERACT leadstoourreward
model EURUS-RM-7B,whichachievesabettercorrelationwithhumanannotatorsthan
allexistingmodelsonAutoJ(Lietal.,2023a)andMT-Bench(Zhengetal.,2023),including
GPT-4(OpenAI,2023). EURUS-RM-7Bdemonstratesespeciallystrongpreferencemodeling
performanceonreasoningtasks.
Checkpoints of our EURUS models, accompanying ULTRAINTERACT alignment data to
reproducethisresearch,willbepubliclyavailable.
2Preprint
2 ULTRAINTERACT: Tree-structuredAlignmentDataforReasoning
Solving complex problems
often requires the model‚Äôs User
U U U U
Instruction
capability in planning and
reasoning, integrating with A A R C A Unpaired
Action
tools, and interacting with
Chosen
and learning from both the O U O&C C Action
environment and the users.
Rejected
A A R C R
ThisisreflectedinULTRAIN- Action
TERACT‚Äôs design choices: (1)
O U O&C O
Observation
Its instructions are diverse,
challenging, and of a large A R C R C O&C Observation
& Critique
scale (¬ß2.1); (2) It provides
multi-turn trajectories that
solve the input instruction Figure 2: Left: CodeActInstruct (Wang et al., 2024) and
through multiple turns of Code-Feedback(Zhengetal.,2024);Middle: HH-RLHF(Bai
interactionwithandlearning etal.,2022); Right: ULTRAINTERACT. Eachinstructionin
from the environment and ULTRAINTERACTisconstructedasapreferencetree.
critique. At each turn, it
breaksdowntheproblemintosmallerones(¬ß2.2). (3)ULTRAINTERACTincludespairwise
datatofacilitatepreferencelearning(¬ß2.3).
Conceptually, ULTRAINTERACT collects a preference tree for each instruction, with the
instructionbeingtherootandeachactionanode(Figure2). Atrajectoryisaroot-to-leaf
pathconsistingofasequenceofactions. Ineachpreferencetree,allnodesofcorrectactions
and all trajectories ending with correct actions can be used for SFT. Paired correct and
incorrectnodesortrajectoriescanbeusedforpreferencelearning.
2.1 InstructionSelectionEmphasizingComplexity,Quality,andDiversity
We target three representative reasoning tasks: math problem-solving, code generation,
and logical reasoning. The complexity, quality, and diversity of the alignment data are
crucial to the model‚Äôs performance (Liu et al., 2023). Following Wang et al. (2023b), we
select challenging problems that GPT-3.5-Turbo fails to solve. We intentionally restrict
theselectionofthedatasetstothosewithground-truthsolutions,aimingtoensurehigh-
qualityoversightsignalsratherthanrelyingonLLM-as-a-judgeannotation(Weyssowetal.,
2024). Besides,thegoldsolutionsalsoserveasreferencesforthecritiquemodeltogenerate
feedback. TopromoteULTRAINTERACT‚Äôsdiversity,wepickdatasetsofdifferentcategories.
Foreachdataset,weincludedistinctreasoningpatternsbasedonquestioncategoriesor
formulationsnecessarytosolvetheproblems. Table6summarizesthedatasetsselectedby
ULTRAINTERACT. ExceptforMATH,noneofthetrainingdatasetsisusedinourevaluation.
2.2 DecompositionandInteractionatEachTurn
Figure 3 provides an illustrative example. In what follows, we connect the actor model
withaPythoninterpreterasthe‚Äúenvironment‚Äù. Unlessotherwisespecified,weuseGPT-3.5
Turboastheactormodel.
FollowingWangetal.(2024),theactormodelfirstdecomposestheinputproblemintoseveral
sub-problemsandthensolveseachbygeneratingPythoncodepiecesasactionsandusing
theenvironmenttoexecutethem. Topromotesolutiondiversity,theactormodelrandomly
samplesonereasoningschemaintheformofeitherCoT(Weietal.,2022)ormodularization
programming(Qianetal.,2023;Yuanetal.,2023). Theactorthengeneratesactionsintext
orcodetosolveeachsub-problem,witheachstepbeingmarkedbyexplicitnotations.
Multi-turn interactions with the environment are often necessary to solve challenging
problems(Wangetal.,2023b). Toimprovesuchcapabilitiesofthemodels,ULTRAINTERACT
collectstrajectoriesinwhichtheactormodelinteractswiththeenvironmentandacritique
model(aproxyforuser)andrefinesitsactionbasedontheirfeedback.
3Preprint
In a classroom, x children have 7 books each. [‚Ä¶] what is the value of unknown variable x?
1 Generation 2 Observation 4 Generation 5 Observation
<thought> [‚Ä¶] <thought> [‚Ä¶]
Step 1: First, let's calculate the total [‚Ä¶]. NameError: name 'x' is Step 1: First, create an equation [‚Ä¶]. Out: 10.0
Step 2: Then, we need to add the [‚Ä¶]. not defined. Step 2: Each child has 7 books, so [‚Ä¶].
Step 3: The sum of the books should [‚Ä¶]. Step 3: The teacher brings 8 [‚Ä¶]. Your answer is
</thought> Your answer is wrong. Step 4: Now we can solve for x by [‚Ä¶]. correct.
<execute> </thought>
# Step 4: First, let's calculate [‚Ä¶]. 3 Critique <execute> 6 Critique
total_books_child = x * 7 # Step 5: Setting up equation:
# Step 5: Next, we need to add [‚Ä¶]. Expert feedback: equation = "7x + 8 = 78" Expert feedback:
total_books = total_books_child + 8 Your thought process is # Step 6: Solving for x: Good job! You have fixed
# x S = te (p 7 86 : - W 8e ) c /a n 7 now solve for [‚Ä¶]. accurate and your error lies <x / e= x e( c7 u8 t e- > 8) / 7 the error in last turn.
</execute> in your code. You need to <solution> Now your answer is
<solution> define `x` first [‚Ä¶] x correct. [‚Ä¶]
x </solution>
</solution>
Figure3: AnillustrativeexampleofanULTRAINTERACTtrajectoryovertwoturns. Ineach
turn,theactormodelgeneratesstep-by-stepreasoningchains,andtheenvironmentandthe
critiquemodelprovideobservationsandtextualcritiquerespectively.
Theenvironmentreceivesanactionfromtheactormodelalongwiththeinteractionhistory,
and then the code interpreter returns two kinds of ‚ÄúObservation‚Äù: (1) Python execution
results,eitherprogramoutputsorerrortracebackmessages;(2)binaryfeedback,indicating
whether the solution is correct or not. Then, the observations along with the history
willbepassedtoacritiquemodel,whichlocatestheerrorsandprovidessuggestionsfor
improvements. Toavoidpotentialbiasintroducedbyself-correction(Wangetal.,2023b;Xu
etal.,2024),weadoptastrongermodel,GPT-4,asthecritiqueandensurecritiquequality
byprovidingGPT-4withgroundtruthanswersasreferences.
ThisprocedureresemblesWangetal.(2024). However,weadoptmorediversereasoning
patternstoteachLLMstolearnrationalesratherthansimplymemorizinganswers(Mitra
etal.,2023),andlearntocreateandusetools(Qianetal.,2023;Yuanetal.,2023;Qinetal.,
2023). Besides,webelievethatitisimportantforLLMstolearnfromthefeedbackprovided
bythecritiqueratherthansolelyfromobservationsoftheenvironment.
2.3 PreferenceTreesFacilitatesPreferenceLearningAcrossMultipleTurns
Unlikeopen-endedconversations,wherehumanpreferenceisambiguousandchallenging
tospecify,manyreasoningtaskshaveclearandobjectivepreferencesforcorrectactions. The
preferenceannotationisthreforeanevaluationofthecorrectnessofthesolutionsconditioning
groundtruthones,whichcomewiththedatasetsinULTRAINTERACT. Thiseliminatesthe
needforhumanorLLM-basedpreferenceannotationandensureshighdataquality. To
facilitatepreferencelearning,ULTRAINTERACTpairscorrectandincorrectactions.
Sampling Paired Correct and Incorrect Actions at Each Turn. For each instruction in
ULTRAINTERACT,wesample,fromtheactormodel,apairofcorrectandincorrectactions
following¬ß2.2. WefollowCuietal.(2023)tosamplethepairfromdifferentactormodels
toensureresponsediversity. Topreventmodelsfromexploitingshortcutsbasedonsurface
features,weexcludeinstancesthatfailtopassthePythonsyntaxcheck.
Certain challenging problems in ULTRAINTERACT pose difficulties in obtaining correct
actions,evenusingstrongactorssuchasGPT-4,withnearlyzeropass@100accuracies. To
improvethepassratesoftheactormodelswhilekeepingtheexpenseundercontrol,we
sequentiallytakethefollowingsteps.(1)Directlysampling20actionsandrandomlykeeping
acorrectone, ifany. (2)Ifnocorrectactionisobtained, werepeattheaboveprocessup
tothreetimes,progressivelyswitchingfrommorecost-effectivemodelstothestrongyet
expensiveGPT-4Turbo. (3)Fortheremainingdifficultproblemswherenocorrectactionis
acquiredaftertheprevioustwosteps,weprovidetheactorwithground-truthrationalesand
answers,andthenapplyvarioustechniquestoelicitcorrectactions.Thespecificinformation
providedandthetechniquesappliedvarydependingonthetasks(AppendixA.2).
4Preprint
Tree-structuredActionPairsAcrossMultipleTurns. Aftereachturn,thecorrectaction
concludesitstrajectory. Weexpandtheincorrectactionintothenextturn,andhavetheactor
interactwiththeenvironmentandthecritiquetorefineitssolution(¬ß2.2). Wethenrepeat
the procedures introduced earlier in this section to collect an additional action pair. By
expandingtheincorrectaction,ULTRAINTERACTcanprovidedatatohelpmodelslearnfrom
feedback,andcollectmultipleactionpairsforpreferencelearningacrossmultipleturns.
Conceptually,foreveryinstruction,ULTRAINTERACTconstructsabinarypreferencetree
witheachactionbeinganode(Figure2). Wecapthetreeatamaximumoffiveturns.
AdditionalInstruction-actionPairsforChallengingProblems. Webelievethechallenging
instructionsthatmakeittostep(3)abovecanprovidevaluabletrainingsignals. Therefore,
for a subset of these problems with multiple ground truth solutions, we further sample
additional correct actions to cover all ground truths. Accordingly, we further sample
incorrectactionstopairwiththeseadditionalcorrectactions,sothattheycanbeusedin
bothsupervisedfine-tuningandpreferencelearning.
With the tree-structured data, ULTRAINTERACT enables comparisons at every turn, in
contrast to comparing only at the last turn (Bai et al., 2022), and thus can improve the
models‚Äô interaction ability. Closing this section, Table 1 summarizes some statistics of
ULTRAINTERACT,andmoredetailsareinAppendixA.4.
Table1: SomestatisticsofULTRAINTERACT.
Type #TurnsperTraj. #Tokens Avg.#Traj Total #Correct
Task #Instructions
w/Interaction? w/Tool? T1 T2 T3 T4 T5 perTraj. perIns. #Pairs Answers
(cid:33) (cid:33) 22,928 10,440 4,122 1,898 904 5,564 1,750.0 1.0 42,780 68,033
Math (cid:37) (cid:33) 2,757 16,154 - - - - 439.1 5.9 13,217 16,154
(cid:33) (cid:37) 22,639 10,708 3,521 1,459 723 6,228 1,521.9 1.0 44,750 62,182
(cid:37) (cid:37) 2,083 16,348 - - - - 538.1 7.8 12,624 16,348
(cid:33) - 20,463 13,265 2,584 987 379 3,248 1,728.5 1.0 18,106 22,215
Coding
(cid:37) - 8,495 92,618 - - - - 1,070.4 5.5 78,634 92,618
(cid:33) (cid:33) 2,086 1,685 298 72 8 23 1,299.8 1.0 1,750 2,198
Logic
(cid:33) (cid:37) 4,467 2,453 1,674 340 0 0 1,266.7 1.0 7,958 7,231
Total - - 85,918 163,671 12,199 4,756 2,014 15,063 1,201.8 2.3 219,819 286,979
3 EURUS: State-of-the-artOpenLLMsinReasoning
ULTRAINTERACThelpsusdevelopEURUS,asuiteofLLMsandarewardmodel(RM).
SupervisedFine-Tuning. EURUS-7B-SFTisfine-tunedfromMistral-7B(Jiangetal.,2023a)
andEURUS-70B-SFTfromCodeLLaMA-70B(Roziereetal.,2023). First,weperformSFT
usingallcorrectactions(287K)inULTRAINTERACT. Wefindityieldsbetterperformanceto
discardinteractionhistoryandtrainonlyoncorrectleafnodesineachtree. Toimprovegen-
eralinstruction-followingability,weincludeintoourSFTdatamixtureUltraChat(Dingetal.,
2023),ShareGPT2,andOpenOrca(Lianetal.,2023).PleasefindmixtureratiosinAppendixB.
PerferenceLearning. BasedonEURUS-SFTmodels,weexplorethreepreferencelearning
algorithms, DPO (Rafailov et al., 2023), KTO (Ethayarajh et al., 2024), and NCA (Chen
etal.,2024a). DifferentlyfromSFT,hereweincludeallmulti-turntrajectorypairsinour
ULTRAINTERACT(220K)andincludeallUltraFeedback(Cuietal.,2023)pairs(340K).
RewardModeling. Similarlytothepreferencelearning,weuseall220Kmulti-turntrajec-
torypairsfromULTRAINTERACT;itisfurtheraugmentedwiththe240Ksingle-turnaction
pairsfromULTRAINTERACT. MoredetailsareintheAppendixB.Weincludeall340Kpairs
fromUltraFeedbackandonepairforeachinstructionfromUltraSafety(Guoetal.,2024b),
totaling3K.EURUS-RM-7BisinitializedfromEURUS-7B-SFTwithanewlinearlayer.
Ourfindingsin¬ß6indicatethattheabsolutevaluesofrewardsmakeabigdifferenceinthe
models‚Äôreasoningperformance. WethereforeaugmenttheestablishedBradley-Terry(BT)
objective L withanadditionalterm L todirectlyincreasetherewardofthechosen
BT DR
actionsforinstancesfromULTRAINTERACT,anddecreasethoseoftherejectedones:
2https://huggingface.co/datasets/openchat/openchat sharegpt4 dataset
5Preprint
Table2: Open-sourceLLMbaselinesthatwecompareto.
Type Models
Mistral-7B-Instruct-v0.2(Jiangetal.,2023a),Zephyr-7B-Œ≤(Tunstalletal.,2023),OpenChat-3.5-1210(Wang
GeneralPurpose etal.,2023a),Starling-LM-7B-Œ±(Zhuetal.,2023),Mixtral-8x7B-Instruct(Jiangetal.,2023a),DeepSeek-
LLM-67B-Chat(DeepSeek-AI,2024),QWen1.5-72B-Chat(Baietal.,2023)
Magicoder-S-DS-6.7B(Weietal.,2023),OpenCodeInterpreter(OpenCIforshort,DS-6.7B/CL-70B)(Zheng
Coding
etal.,2024),DeepSeek-Coder-33B-Instruct(Guoetal.,2024a),andCodeLLaMA-70B-Instruct(Roziereetal.,
2023).
MAmmoTH-7B-Mistral(Yueetal.,2023),WizardMath-7B-v1.1(Luoetal.,2023a),OpenMath(Mistral-
Math
7B/CodeLLaMA-70B)(Toshniwaletal.,2024).
(cid:16) (cid:0) (cid:1)(cid:17) (cid:16) (cid:0) (cid:1)(cid:17) (cid:16) (cid:0) (cid:1)(cid:17)
L = ‚àílog œÉ r (x,y )‚àír (x,y ) ‚àílog œÉ r (x,y ) ‚àílog œÉ ‚àír (x,y )
ULTRAINTERACT Œ∏ c Œ∏ r Œ∏ c Œ∏ r
(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
L BT:optimizerelativerewards L DR:increaserŒ∏(x,yc)anddecreaserŒ∏(x,yr)
Forinstancesfromotherdatasets,wetrainwithL . Œ∏denotestherewardmodel‚Äôsparame-
BT
ters,r (¬∑)andr (x,y )therewardsonthechosenandrejectedactionsrespectively. Our
Œ∏ Œ∏ r
ablationstudydemonstratestheimportanceofbothL andL .
BT DR
4 Evaluationof EURUS-7B and EURUS-70B
EvaluationSetup. Weconsiderbothsingle-turnandmulti-turnreasoning. Forsingle-turn
evaluation, weconsiderHumanEval(Chenetal.,2021), MBPP(Austinetal.,2021), and
LeetCode(Guoetal.,2024a)forcoding, GSM-Plus(Lietal.,2024), MATH,TheoremQA
(Chenetal.,2023),SVAMP(Pateletal.,2021),andASDiv(Miaoetal.,2020)formath,and
BBH-Hard (Suzgun et al., 2022) for reasoning. We evaluate with pass@1 accuracy. We
alsouseIFEval(Zhouetal.,2023)toassesstheinstruction-followingabilityandreportthe
prompt-levelloosescore.Formulti-turnevaluation,weadoptMINT(Wangetal.,2023b)and
onlyconsiderthecodingandmathproblems. WereportthesuccessrateatTurn5. Please
findfurtherdetailsonevaluationsetupsandevaluationsbeyondreasoninginAppendixC.
As shown in Table 2, we compare our EURUS with general-purpose models, and those
specializedincodingandmathofvarioussizes. WealsosummarizetheresultsofGPT-3.5
TurboandGPT-4reportedinpreviousworks.
Table 3: Overall performance. All test sets except MATH are out-of-distribution to our
modelsandmostbaselines. MAmmoTH,OpenChat,andStarling-LMhavebeentrainedon
TheoremQAtestsets. Westrikethroughthecontaminatednumbers.
Coding Math Reasoning Ins-Following Multi-Turn
Model Avg.
HumanE. MBPP LeetC. GSM-Plus MATH Theo.QA SVAMP ASDiv BBH IFEval Code Math
‚àº7B
Mistral-7B-Instruct-v0.2 39.0 30.8 6.1 15.7 9.5 8.5 42.9 49.5 62.4 44.4 7.4 26.2 28.5
Zephyr-7B-Œ≤ 29.3 35.8 2.2 23.3 5.0 7.8 19.1 28.0 61.8 39.7 5.2 16.9 22.8
OpenChat-3.5-1210 64.0 61.7 11.7 46.7 28.1 19.1 75.4 77.0 67.0 50.3 21.3 32.4 46.2
Starling-LM-7B-Œ± 46.3 51.1 8.9 23.7 21.5 12.0 26.3 39.8 67.1 26.1 18.4 28.9 30.8
Magicoder-S-DS-6.7B 75.6 70.4 23.9 16.4 19.9 13.1 61.6 62.8 57.0 21.1 27.9 8.0 38.1
OpenCI-DS-6.7B 76.8 66.2 16.1 41.5 31.6 16.1 74.5 79.8 53.9 22.6 5.9 1.3 40.5
MAmmoTH-7B-Mistral 24.4 42.4 7.2 40.1 36.0 26.3 60.7 72.3 57.7 34.9 3.7 6.7 34.4
WizardMath-7B-v1.1 50.0 53.9 6.7 54.6 30.0 16.5 57.8 73.5 64.4 22.6 16.2 8.9 37.9
OpenMath-Mistral-7B 33.5 46.6 11.7 59.4 39.1 13.1 83.4 79.8 58.6 15.0 2.9 5.3 37.4
EURUS-7B-SFT 55.5 59.1 20.0 52.1 32.6 20.0 82.2 84.1 64.6 44.0 15.4 28.4 46.5
+DPO 50.6 52.1 8.3 51.0 28.3 20.9 78.7 83.8 65.0 42.5 20.6 32.4 44.5
+KTO 56.1 58.6 18.9 55.0 33.2 20.6 84.4 85.0 67.6 43.1 19.1 43.6 48.8
+NCA 55.5 60.2 14.4 54.9 34.2 20.9 84.6 85.4 64.3 42.7 21.3 38.7 48.1
‚àº40B
Mixtral-8x7B-Instruct 50.6 50.1 5.6 49.6 25.9 20.4 66.4 68.8 73.5 48.8 12.5 37.3 42.5
DeepSeek-Coder-33B-Ins 82.3 73.9 27.8 29.5 20.2 21.9 75.2 85.0 61.5 26.1 35.3 21.8 46.7
‚àº70B
CodeLLaMA-70B-Instruct 56.7 58.6 14.4 34.9 12.0 8.4 63.5 70.1 74.5 24.0 3.7 14.2 36.3
DeepSeek-LM-67B-Chat 70.7 65.7 20.0 65.0 41.0 17.9 74.0 84.0 78.9 52.7 30.9 41.8 53.5
QWen1.5-72B-Chat 71.3 56.9 15.6 65.4 43.4 18.5 79.5 79.1 78.0 53.4 27.2 38.2 52.2
OpenCI-CL-70B 77.4 71.7 20.0 46.1 29.2 18.8 76.1 79.4 66.7 26.8 30.9 12.0 46.3
OpenMath-CL-70B 39.0 52.6 15.0 62.2 45.9 15.9 86.6 82.8 59.9 15.7 14.0 0.4 40.8
EURUS-70B-SFT 75.6 74.2 33.3 58.1 40.6 28.0 86.3 88.5 79.9 49.2 31.6 40.4 57.1
+KTO 76.8 68.2 26.1 62.2 41.3 30.6 90.4 89.0 80.8 46.4 39.0 49.8 58.4
+NCA 79.3 71.9 33.3 62.8 41.7 32.6 89.5 90.3 80.0 49.2 38.2 39.6 59.0
ProprietaryModels
GPT-3.5Turbo 76.8 82.5 23.3 61.2 37.8 35.6 83.0 90.6 70.1 56.6 29.4 36.9 57.0
GPT-4 85.4 83.5 41.8 85.6 69.7 52.4 94.8 92.6 86.7 79.7 59.6 65.8 74.8
6Preprint
4.1 Results
ResultsareshowninTable3. Wesummarizethetakeawaysasfollows:
EURUS, both the 7B and 70B variants, achieve the best overall performance among
open-source models of similar sizes. EURUS even outperform specialized models in
correspondingdomainsinmanycases. Notably, EURUS-7B outperformsbaselinesthat
are5√ólargerandEURUS-70BachievesbetterperformancethanGPT-3.5Turbo. EURUS‚Äôs
instruction-followingperformanceisamongthebestgeneral-purposemodels,substantially
betterthanspecializedones.
PreferencelearningwithULTRAINTERACTcanfurtherimprovetheperformance,espe-
ciallyinmathandthemulti-turnability. KTOandNCAconsistentlyimprovethemodels‚Äô
performance in all five math benchmarks and mult-turn evaluations, while their effects
varyinothers. SinceSFTmodelsonlyusethesingle-turndatafromULTRAINTERACTwhile
preferencelearningusesthemulti-turnones,theimprovementsininteractionabilityshould
alsobeattributedtoULTRAINTERACTratherthanthealgorithmsalone. Surprisingly,we
observethatDPOhurtsmodelperformanceonmostbenchmarks. DPOtrainingofour
70Bmodelfailssincetherewardsgodownto‚àí‚àû. Weanalyzethisphenomenonin¬ß6.1.
5 Evaluationof EURUS-RM-7B
Evaluation Setup. We evaluate EURUS-RM-7B on three RM benchmarks, Reward-
Bench (Lambert et al., 2024), AutoJ (Li et al., 2023a), and MT-Bench (Zheng et al., 2023).
AimingforamorerealisticOODevalation,weexcludethe‚Äúpriorsets‚ÄùsplitfromReward-
Bench,sincemanybaselinestrainonthedatasetsthatthissplitcontains. Wecomparewith
PairRM(Jiangetal.,2023b),Starling-RM-7B/34B(Zhuetal.,2023),UltraRM-13B(Cuietal.,
2023),GPT-3.5Turbo,andGPT-4.TofurtherexploreEURUS-RM-7B‚Äôspotentialinimproving
models‚Äô performance through reranking, we use it to rerank Mistral-7B-Instruct-v0.2‚Äôs
responsesonHumanEval,MBPP,GSM8K,andMATH.Wereporttheresultsofrandom
sampling,self-consistency,andStarling-RM-34Basbaselines.
5.1 Results
Table 4 summarizes reward modeling performance, and Figure 4 plots some reranking
resultswithothersinAppendixD.1.
EURUS-RM-7B stands out as the best 7B RM overall, and achieves similar or better
performancethanmuchlargerbaselines. Particularly,itoutperformsGPT-4incertain
tasks. EURUS-RM-7Bachievesabettercorrelationwithhumanexpertsthanallexisting
modelsonAutoJandMT-Bench,anditachievescomparableperformancetothe5√ólarger
Starling-RM-34B on RewardBench. On RewardBench, EURUS-RM-7B outperforms all
baselinesonthe‚ÄúChat-Hard‚Äùsplitwhileachievingverycompetitiveperformanceonthe
‚ÄúReasoning‚Äùsplit. AcrosstheAutoJsplits,EURUS-RM-7Boutperformsnearlyallexisting
models,withtheonlyexceptionbeingGPT-4‚ÄôsresultsonCoding.
OurtrainingobjectiveisbeneficialinimprovingRMperformanceonhardproblemsand
reasoning. Table4showsthatoptimizing L improvesRM‚Äôsreasoningability, butBT
DR
modelingisstillbeneficialinequippingRMwithabilitiesingeneralchattingassuggested
inthe‚ÄúChat-Hard‚Äùcolumn,thoughitseffectonreasoningmayvary.
ULTRAINTERACTiscompatiblewithotherdatasetslikeUltraFeedbackandUltraSafety,
and mixing these datasets can balance different RM abilities. Improving RM‚Äôs capa-
bilitiesinreasoningwithULTRAINTERACTdoesnotsacrificeothers,whichindicatesthat
ULTRAINTERACTcanbeagreatingredientforthetrainingdatamixtureofrewardmodels.
EURUS-RM-7BimprovesLLMs‚Äôreasoningperformancebyalargemarginthroughrerank-
ing. EURUS-RM-7Bconsistentlyimprovespass@1accuracyacrossalltasksandperforms
betterthan5√ólargerbaselineStarling-RM-34B.Also,EURUS-RM-7B‚Äôsrerankingperfor-
mancescaleswellwith#responsesperinstruction,exceptaslightdecreaseinHumanEval
7Preprint
HumanEval MBPP GSM8K MATH
44 57.5
48 42 55.0 16
HumanEval MBPP GSM8K MATH 14
46 40 44 52.5 17.5 12
44 38 48 42 5550.0 15.0 10
46 40 12.5
42 47.5 8 36 44 38 50 10.0
6
2 4 6 8 10 12 14 16 2 424 6 8 10 3162 14 16 2 4 6 87.5 10 12 14 16 2 4 6 8 10 12 14 16
#Samples Per Instruction #Samples Per Instruction #Samples Per Instruction #Samples Per Instruction
1 2 4 8 16 1 2 4 8 16 1 2 4 8 16 1 2 4 8 16
#Responses Per Instruction #Responses Per Instruction #Responses Per Instruction #Responses Per Instruction
Self-Consistency Starling-RM-34B Eurus-RM-7B
Figure4: ResultsonrerankingMistral-7B-Instruct-v0.2‚Äôsresponses. FullresultsinTable9.
Table4: Resultsonrewardmodelingbenchmarks. UF:UltraFeedback;US:UltraSafety. The
bestperformanceineachbenchmarkisinboldandthesecondbestoneisunderlined. Most
baselineresultsarefromJiangetal.(2023b)andLambertetal.(2024).
RewardBench AutoJ
Model MT-Bench
Chat Chat-Hard Safety Reasoning Avg. Code Math Others Overall
PairRM 90.2 53.0 31.5 60.0 58.7 58.3 52.8 58.9 59.1 59.0
Starling-RM-7B 98.0 43.4 88.6 74.6 76.2 59.2 47.2 61.4 60.8 56.8
Starling-RM-34B 96.9 59.0 89.9 90.3 84.0 65.8 54.2 62.3 62.6 60.4
UltraRM-13B 96.1 55.3 45.8 82.0 69.8 55.0 43.1 59.6 59.9 56.0
GPT-3.5Turbo - - - - - 36.6 40.3 41.2 42.7 57.1
GPT-4 - - - - - 69.2 51.4 61.4 61.9 63.9
EURUS-RM-7B 96.5 65.3 80.7 87.0 82.4 67.5 62.5 63.6 64.5 72.9
w/oL DR 96.4 59.9 79.5 77.5 78.3 64.2 59.7 64.7 65.0 72.8
w/oL BT 96.8 58.5 83.8 84.2 80.8 67.5 66.7 64.8 65.6 72.6
w/oUS 96.5 66.2 67.7 81.7 73.3 66.7 61.1 65.0 65.7 72.6
w/oUF+US 95.1 61.1 63.7 73.4 78.0 55.8 58.3 59.0 58.7 67.2
whenincreasingresponsenumberform8to16. Incontrast,Starling-RM-34Bsuffersfrom
severeperformancedroponHumanEvalanditconsistentlyhurtsmodelaccuracyonMATH.
6 Analysis
DPO KTO NCA
3
4
2 2
2 1 0 0.4 0.16
-1.26 0 0
2 Margins 2 Margins 1 Margins
Rewards/Chosen Rewards/Chosen 2 Rewards/Chosen
4 Rewards/Rejected 4 Rewards/Rejected Rewards/Rejected
3
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Steps (%) Steps (%) Steps (%)
Figure5: Rewardpatternsof EURUS-7B preferencelearningwithDPO,KTO,andNCA.
Forallalgorithms,therewardsofrejecteddatakeepdecreasingandthemarginsbetween
chosenandrejecteddatakeepincreasing. However,therewardsofchosendatadecrease
belowzeroinDPOwhilekeepingincreasingandstayingpositiveinKTOandNCA.The
absolute values of the reward in the last step (in red) of the three algorithms positively
correlatewiththeirperformanceinTable3.
6.1 ExplicitRewardasAProxy? HypothesisforPreferenceLearninginReasoning
WeinvestigatethereasonwhyDPObehavesdifferentlythanKTOandNCA.Westartby
empiricallyinspectingtherewardsthroughoutthepreferencelearningprocess,asshown
inFigure5. RewardsforchosenrejecteddatabothkeepdecreasingthroughDPO,though
therewardsforchosendataisstillhigherhencethelossdecreases. InKTOandNCA,the
rewardsofchosendatakeepincreasingwiththoseofrejecteddatadecreasing.
Therefore,wehypothesizeitisthedistinctioninthetrendofrewardsthatleadstotheper-
formancegapbetweenDPOandtheothertwoalgorithms. Thisdistinctioncanbeattributed
tothatDPO,derivedfromtheBradley-Terrymodel,onlyoptimizestherelativedifferences
betweenchosenandrejecteddataoverlookingtheabsolutevaluesoftherewards. Thisisa
non-issueinalignmentwithgeneralhumanvalueswherepreferenceis‚Äúrelative‚Äùandthere
8
ycaruccA
1@ssaP
ycaruccA
1@ssaP
ycaruccA
1@ssaP
eulaV
ycaruccA
1@ssaP
eulaV
ycaruccA
1@ssaP
ycaruccA
1@ssaP
eulaV
ycaruccA
1@ssaP
ycaruccA
1@ssaPPreprint
canbemanyvalidanswerstothesameinput. However,inreasoningtasks,thespaceofcor-
rectanswersismuchsmallerthanthatofincorrectones. Further,wenoticethattherewards
ofchosendatainthelasttrainingstepfollowtherankingorderofKTO>NCA>DPO,
positivelycorrelatewiththeirperformancetrends. Therefore,webelievethatincreasingthe
rewardsofthechosendataisespeciallybeneficialinpreferencelearningforreasoningtasks.
6.2 AblationStudy
We study the impact of ULTRAINTERACT Table5: AblationstudyofSFTdata.
and other open-source alignment data Model Coding Math BBH IFEval Avg.
on EURUS-7B-SFT‚Äôs performance. We EURUS-7B-SFT 44.9 58.5 64.6 44.0 53.6
consider three settings: (1) With original Ground-truth 33.9 46.1 64.4 42.9 44.0
Open-sourceOnly 31.2 33.5 65.3 43.6 37.0
ground-truth answers, which replaces ULTRAINTERACTOnly 37.3 56.2 67.0 17.4 47.7
the generated actions with ground-truth
rationalesandanswersfromtheoriginaldatasets.Ifnorationalesareavailable,weusethose
fromULTRAINTERACT. (2)Open-sourcedataonly. (3)ULTRAINTERACTonly. Weevaluate
withthesamesettingas¬ß4andreporttheaveragedscores. SeefullresultsinAppendixE.
In Table 5, EURUS outperforms the ‚ÄúGrouth-truth‚Äù model on all tasks, confirming the
advantageof ULTRAINTERACT‚Äôsdesignsof divide-and-conquerandcode-as-actionpat-
terns,inlinewithconclusionsofconcurrentwork(Chenetal.,2024b;Wangetal.,2024).
Trainingonlyonopen-sourcedatawithoutULTRAINTERACTgreatlyhurtsthereasoning
performance,confirmingtheeffectivenessofULTRAINTERACT. Meanwhile,trainingonly
onULTRAINTERACTsuffersaperformancedropexceptforBBH,especiallyininstruction
following. Weattributetheperformancedroptoaworseinstruction-followingability. This
suggests the necessity of mixing ULTRAINTERACT with other alignment data for better
all-aroundsupervisedfine-tuning.
7 RelatedWork
OpenLLMsinReasoning.Open-sourceLLMshaveshownremarkableprogressinbuilding
specialiststhatexcelinmathematicsreasoning(Luoetal.,2023a;Yueetal.,2023;Toshniwal
etal.,2024)orcodingabilities(Roziereetal.,2023;Weietal.,2023;Guoetal.,2024a;Zheng
etal.,2024). Onthecontrary,masteringgeneralreasoningcapabilitiesstillchallengesopen
models,whilethemostadvancedones(DeepSeek-AI,2024;Baietal.,2023;Touvronetal.,
2023; Jiang et al., 2024) are well behind proprietary models. More, these cutting-edge
opengeneral-purposemodelsmaintaintheiralignmentrecipesconfidential,whichfurther
hindersthereplicationanddevelopmentofopen-sourcereasoningmodels.
PreferenceLearningforReasoning. AligninglanguagemodelsfromhumanorAIpref-
erences has emerged as a prevalent approach in the open-source community (Tunstall
etal.,2023;Baietal.,2023)withtheproposalofDPO(Rafailovetal.,2023)andhigh-quality
preferencedatasets(Cuietal.,2023;Zhuetal.,2023). Differentfromopen-domainchatbots,
preferencelearningislargelyunderexploredincomplexreasoning. Recentresearchshowed
performancedegradationwhenapplyingDPOonreasoningtasks, butsomenewlypro-
posedalgorithmsdemonstratedapositiveeffect(Ethayarajhetal.,2024;Chenetal.,2024a;
Mitraetal.,2024;Shaoetal.,2024). However,adeepunderstandingofpreferencelearning,
specificallyitsefficacyoncomplexreasoning,isnotyetestablished.
8 Conclusion
Westrivetonarrowthehugegapbetweenopen-sourcemodelsandproprietarymodelsfrom
theperspectiveofalignment. Ourworkpushestheboundariesofopen-sourcereasoning
generalistsby(1)releasingahigh-qualitymulti-turnreasoningdatasetULTRAINTERACT
with preference trees, (2) introducing EURUS-series LLMs which achieve new SOTA on
challengingreasoningbenchmarksand(3)providinginsightsonpreferencelearningfor
reasoningthroughanalysis,leadingtonewrewardmodelingobjectivesaswellasapowerful
rewardmodelforreasoning.
9Preprint
References
AidaAmini,SaadiaGabriel,ShanchuanLin,RikKoncel-Kedziorski,YejinChoi,andHan-
naneh Hajishirzi. MathQA: Towards interpretable math word problem solving with
operation-basedformalisms. InProc.ofNAACL-HLT,2019.
JacobAustin,AugustusOdena,MaxwellNye,MaartenBosma,HenrykMichalewski,David
Dohan,EllenJiang,CarrieCai,MichaelTerry,QuocLe,etal. Programsynthesiswith
largelanguagemodels. ArXivpreprint,abs/2108.07732,2021.
JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,XiaodongDeng,YangFan,WenbinGe,
YuHan,FeiHuang,BinyuanHui,LuoJi,MeiLi,JunyangLin,RunjiLin,DayihengLiu,
GaoLiu,ChengqiangLu,KemingLu,JianxinMa,RuiMen,XingzhangRen,Xuancheng
Ren,ChuanqiTan,SinanTan,JianhongTu,PengWang,ShijieWang,WeiWang,Sheng-
guangWu,BenfengXu,JinXu,AnYang,HaoYang,JianYang,ShushengYang,YangYao,
BowenYu,HongyiYuan,ZhengYuan,JianweiZhang,XingxuanZhang,YichangZhang,
ZhenruZhang,ChangZhou,JingrenZhou,XiaohuanZhou,andTianhangZhu. Qwen
technicalreport. ArXivpreprint,abs/2309.16609,2023.
YuntaoBai,AndyJones,KamalNdousse,AmandaAskell,AnnaChen,NovaDasSarma,
Dawn Drain, Stanislav Fort, Deep Ganguli, T. J. Henighan, Nicholas Joseph, Saurav
Kadavath, John Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-
Dodds,DannyHernandez,TristanHume,ScottJohnston,ShaunaKravec,LianeLovitt,
NeelNanda,CatherineOlsson,DarioAmodei,TomB.Brown,JackClark,SamMcCan-
dlish, Christopher Olah, Benjamin Mann, and Jared Kaplan. Training a helpful and
harmlessassistantwithreinforcementlearningfromhumanfeedback. ArXivpreprint,
abs/2204.05862,2022.
RalphAllanBradleyandMiltonE.Terry. Rankanalysisofincompleteblockdesigns: I.the
methodofpairedcomparisons. Biometrika,39,1952.
HuayuChen,GuandeHe,HangSu,andJunZhu. Noisecontrastivealignmentoflanguage
modelswithexplicitrewards. ArXivpreprint,abs/2402.05369,2024a.
MarkChen,JerryTworek,HeewooJun,QimingYuan,HenriquePondedeOliveiraPinto,
JaredKaplan,HarriEdwards,YuriBurda,NicholasJoseph,GregBrockman,AlexRay,
RaulPuri,GretchenKrueger,MichaelPetrov,HeidyKhlaaf,GirishSastry,PamelaMishkin,
BrookeChan,ScottGray,NickRyder,MikhailPavlov,AletheaPower,LukaszKaiser,Mo-
hammadBavarian,ClemensWinter,PhilippeTillet,FelipePetroskiSuch,DaveCummings,
MatthiasPlappert,FotiosChantzis,ElizabethBarnes,ArielHerbert-Voss,WilliamHebgen
Guss,AlexNichol,AlexPaino,NikolasTezak,JieTang,IgorBabuschkin,SuchirBalaji,
ShantanuJain,WilliamSaunders,ChristopherHesse,AndrewN.Carr,JanLeike,Josh
Achiam,VedantMisra,EvanMorikawa,AlecRadford,MatthewKnight,MilesBrundage,
MiraMurati,KatieMayer,PeterWelinder,BobMcGrew,DarioAmodei,SamMcCandlish,
IlyaSutskever,andWojciechZaremba. Evaluatinglargelanguagemodelstrainedoncode,
2021.
WenhuChen,MingYin,MaxW.F.Ku,YixinWan,XueguangMa,JianyuXu,TonyXia,Xinyi
Wang,andPanLu. Theoremqa: Atheorem-drivenquestionansweringdataset. ArXiv
preprint,abs/2305.12524,2023.
ZehuiChen,KuikunLiu,QiuchenWang,WenweiZhang,JiangningLiu,DahuaLin,Kai
Chen,andFengZhao. Agent-flan: Designingdataandmethodsofeffectiveagenttuning
forlargelanguagemodels. volumeabs/2403.12881,2024b.
KarlCobbe,VineetKosaraju,MohammadBavarian,MarkChen,HeewooJun,LukaszKaiser,
MatthiasPlappert,JerryTworek,JacobHilton,ReiichiroNakano,etal. Trainingverifiers
tosolvemathwordproblems. volumeabs/2110.14168,2021.
Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie,
ZhiyuanLiu,andMaosongSun. Ultrafeedback: Boostinglanguagemodelswithhigh-
qualityfeedback. ArXivpreprint,abs/2310.01377,2023.
10Preprint
DeepSeek-AI. Deepseek llm: Scaling open-source language models with longtermism.
ArXivpreprint,abs/2401.02954,2024.
Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu,
MaosongSun,andBowenZhou.Enhancingchatlanguagemodelsbyscalinghigh-quality
instructionalconversations. InConferenceonEmpiricalMethodsinNaturalLanguage
Processing,2023.
KawinEthayarajh,WinnieXu,NiklasMuennighoff,DanJurafsky,andDouweKiela. Kto:
Model alignment as prospect theoretic optimization. ArXiv preprint, abs/2402.01306,
2024.
Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. Specializing smaller
language models towards multi-step reasoning. In Proceedings of the International
ConferenceonMachineLearning,2023.
MorGeva,DanielKhashabi,EladSegal,TusharKhot,DanRoth,andJonathanBerant. Did
aristotleusealaptop? aquestionansweringbenchmarkwithimplicitreasoningstrategies.
TransactionsoftheAssociationforComputationalLinguistics,9,2021.
DayaGuo,QihaoZhu,DejianYang,ZhendaXie,KaiDong,WentaoZhang,GuantingChen,
XiaoBi,YuWu,Y.K.Li,FuliLuo,YingfeiXiong,andWenfengLiang. Deepseek-coder:
Whenthelargelanguagemodelmeetsprogramming-theriseofcodeintelligence. ArXiv
preprint,abs/2401.14196,2024a.
Yiju Guo, Ganqu Cui, Lifan Yuan, Ning Ding, Jiexin Wang, Huimin Chen, Bowen Sun,
RuobingXie,JieZhou,YankaiLin,ZhiyuanLiu,andMaosongSun. Controllablepref-
erence optimization: Toward controllable multi-objective alignment. ArXiv preprint,
abs/2402.19085,2024b.
DanHendrycks,StevenBasart,SauravKadavath,MantasMazeika,AkulArora,EthanGuo,
CollinBurns,SamirPuranik,HoraceHe,DawnSong,etal. Measuringcodingchallenge
competence with apps. In Thirty-fifth Conference on Neural Information Processing
SystemsDatasetsandBenchmarksTrack(Round2),2021a.
DanHendrycks,CollinBurns,SauravKadavath,AkulArora,StevenBasart,EricTang,Dawn
Song, andJacobSteinhardt. Measuringmathematicalproblemsolvingwiththemath
dataset. InThirty-fifthConferenceonNeuralInformationProcessingSystemsDatasets
andBenchmarksTrack(Round2),2021b.
AlbertQJiang,AlexandreSablayrolles,ArthurMensch,ChrisBamford,DevendraSingh
Chaplot,DiegodelasCasas,FlorianBressand,GiannaLengyel,GuillaumeLample,Lucile
Saulnier,etal. Mistral7b. ArXivpreprint,abs/2310.06825,2023a.
AlbertQ.Jiang, AlexandreSablayrolles, AntoineRoux, ArthurMensch, BlancheSavary,
ChrisBamford,DevendraSinghChaplot,DiegodeLasCasas,EmmaBouHanna,Florian
Bressand,GiannaLengyel,GuillaumeBour,GuillaumeLample,L‚ÄôelioRenardLavaud,
LucileSaulnier,Marie-AnneLachaux,PierreStock,SandeepSubramanian,SophiaYang,
Szymon Antoniak, Teven Le Scao, The¬¥ophile Gervet, Thibaut Lavril, Thomas Wang,
Timothe¬¥eLacroix,andWilliamElSayed. Mixtralofexperts. 2024.
Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large lan-
guagemodelswithpairwiserankingandgenerativefusion. InAnnualMeetingofthe
AssociationforComputationalLinguistics,2023b.
NathanLambert,ValentinaPyatkin,JacobDanielMorrison,LesterJamesValidadMiranda,
BillYuchenLin,KhyathiRaghaviChandu,NouhaDziri,SachinKumar,TomZick,Yejin
Choi,NoahA.Smith,andHannaHajishirzi. Rewardbench: Evaluatingrewardmodels
forlanguagemodeling. 2024.
JunlongLi,ShichaoSun,WeizheYuan,Run-ZeFan,HaiZhao,andPengfeiLiu. Generative
judgeforevaluatingalignment. ArXivpreprint,abs/2310.05470,2023a.
11Preprint
QintongLi,LeyangCui,XueliangZhao,LingpengKong,andWeiBi. Gsm-plus: Acom-
prehensivebenchmarkforevaluatingtherobustnessofllmsasmathematicalproblem
solvers. ArXivpreprint,abs/2402.19255,2024.
RongaoLi,JieFu,Bo-WenZhang,TaoHuang,ZhihongSun,ChenLyu,GuangLiu,ZhiJin,
andGeLi. Taco: Topicsinalgorithmiccodegenerationdataset. volumeabs/2312.14852,
2023b.
YujiaLi,DavidChoi,JunyoungChung,NateKushman,JulianSchrittwieser,Re¬¥miLeblond,
Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter
Choy, Cyprien de Masson d‚ÄôAutume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang,
JohannesWelbl,SvenGowal,AlexeyCherepanov,JamesMolloy,DanielMankowitz,Esme
SutherlandRobson,PushmeetKohli,NandodeFreitas,KorayKavukcuoglu,andOriol
Vinyals. Competition-levelcodegenerationwithalphacode. volumeabs/2203.07814,
2022.
Wing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and
‚ÄùTeknium‚Äù. Openorca: An open dataset of gpt augmented flan reasoning traces.
https://https://huggingface.co/Open-Orca/OpenOrca,2023.
WeiLiu,WeihaoZeng,KeqingHe,YongJiang,andJunxianHe. Whatmakesgooddata
foralignment? acomprehensivestudyofautomaticdataselectionininstructiontuning.
2023.
PanLu,LiangQiu,Kai-WeiChang,YingNianWu,Song-ChunZhu,TanmayRajpurohit,
Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for
semi-structuredmathematicalreasoning. InProceedingsofICLR,2023.
Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo
Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering
mathematicalreasoningforlargelanguagemodelsviareinforcedevol-instruct. ArXiv
preprint,abs/2308.09583,2023a.
ZiyangLuo,CanXu,PuZhao,QingfengSun,XiuboGeng,WenxiangHu,ChongyangTao,
JingMa,QingweiLin,andDaxinJiang. Wizardcoder: Empoweringcodelargelanguage
modelswithevol-instruct,2023b.
Shen-yunMiao,Chao-ChunLiang,andKeh-YihSu. Adiversecorpusforevaluatingand
developingEnglishmathwordproblemsolvers. InProc.ofACL,2020.
SwaroopMishra,ArindamMitra,NeerajVarshney,BhavdeepSachdeva,PeterClark,Chitta
Baral,andAshwinKalyan. NumGLUE:Asuiteoffundamentalyetchallengingmathe-
maticalreasoningtasks. InProc.ofACL,2022.
Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes,
SahajAgrawal,XuxiChen,AnastasiaRazdaibiedina,ErikJones,KritiAggarwal,Hamid
Palangi,GuoqingZheng,CorbyRosset,HamedKhanpour,andAhmedAwadallah. Orca
2: Teachingsmalllanguagemodelshowtoreason. ArXivpreprint,abs/2311.11045,2023.
Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-math:
Unlockingthepotentialofslmsingradeschoolmath. ArXivpreprint,abs/2402.14830,
2024.
OpenAI. Gpt-4technicalreport,2023.
PanupongPasupatandPercyLiang. Compositionalsemanticparsingonsemi-structured
tables. InProc.ofACL,2015.
ArkilPatel,SatwikBhattamishra,andNavinGoyal. AreNLPmodelsreallyabletosolve
simple math word problems? In Proceedings of the 2021 Conference of the North
AmericanChapteroftheAssociationforComputationalLinguistics: HumanLanguage
Technologies,2021.
12Preprint
ChengQian,ChiHan,YiRenFung,YujiaQin,ZhiyuanLiu,andHengJi. Creator: Tool
creationfordisentanglingabstractandconcretereasoningoflargelanguagemodels. In
ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,2023.
YujiaQin,ShiLiang,YiningYe,KunlunZhu,LanYan,Ya-TingLu,YankaiLin,XinCong,
Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Marc H.
Gerstein,DahaiLi,ZhiyuanLiu,andMaosongSun. Toolllm: Facilitatinglargelanguage
modelstomaster16000+real-worldapis. ArXivpreprint,abs/2307.16789,2023.
RafaelRafailov,ArchitSharma,EricMitchell,StefanoErmon,ChristopherD.Manning,and
ChelseaFinn. Directpreferenceoptimization: Yourlanguagemodelissecretlyareward
model. ArXivpreprint,abs/2305.18290,2023.
BaptisteRoziere,JonasGehring,FabianGloeckle,StenSootla,ItaiGat,XiaoqingEllenTan,
Yossi Adi, Jingyu Liu, Tal Remez, Je¬¥re¬¥my Rapin, et al. Code llama: Open foundation
modelsforcode. ArXivpreprint,abs/2308.12950,2023.
ZhihongShao,PeiyiWang,QihaoZhu,RunxinXu,JunxiaoSong,MingchuanZhang,YKLi,
YWu,andDayaGuo. Deepseekmath: Pushingthelimitsofmathematicalreasoningin
openlanguagemodels. ArXivpreprint,abs/2402.03300,2024.
MiracSuzgun,NathanScales,NathanaelScha¬®rli,SebastianGehrmann,YiTay,HyungWon
Chung,AakankshaChowdhery,QuocVLe,EdHChi,DennyZhou,,andJasonWei.Chal-
lengingbig-benchtasksandwhetherchain-of-thoughtcansolvethem. ArXivpreprint,
abs/2210.09261,2022.
Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and Igor
Gitman. Openmathinstruct-1: A 1.8 million math instruction tuning dataset. arXiv
preprintarXiv: Arxiv-2402.10176,2024.
Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine
Babaei,NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,DanielM.
Bikel,LukasBlecher,CristianCanto¬¥nFerrer,MoyaChen,GuillemCucurull,DavidEsiobu,
JudeFernandes,JeremyFu,WenyinFu,BrianFuller,CynthiaGao,VedanujGoswami,
NamanGoyal,AnthonyS.Hartshorn,SagharHosseini,RuiHou,HakanInan,Marcin
Kardas,ViktorKerkez,MadianKhabsa,IsabelM.Kloumann,A.V.Korenev,PunitSingh
Koura, Marie-AnneLachaux, ThibautLavril, JenyaLee, DianaLiskovich, YinghaiLu,
Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin
Nie,AndrewPoulton,JeremyReizenstein,RashiRungta,KalyanSaladi,AlanSchelten,
RuanSilva,EricMichaelSmith,R.Subramanian,XiaTan,BinhTang,RossTaylor,Adina
Williams,JianXiangKuan,PuxinXu,ZhengxuYan,IliyanZarov,YuchenZhang,Angela
Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey
Edunov,andThomasScialom. Llama2: Openfoundationandfine-tunedchatmodels.
ArXivpreprint,abs/2307.09288,2023.
LewisTunstall,EdwardBeeching,NathanLambert,NazneenRajani,KashifRasul,Younes
Belkada,ShengyiHuang,LeandrovonWerra,Cle¬¥mentineFourrier,NathanHabib,etal.
Zephyr: Directdistillationoflmalignment. ArXivpreprint,abs/2310.16944,2023.
Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. Open-
chat: Advancingopen-sourcelanguagemodelswithmixed-qualitydata. ArXivpreprint,
abs/2309.11235,2023a.
XingyaoWang,ZihanWang,JiatengLiu,YangyiChen,LifanYuan,HaoPeng,andHengJi.
Mint: Evaluatingllmsinmulti-turninteractionwithtoolsandlanguagefeedback. ArXiv
preprint,abs/2309.10691,2023b.
XingyaoWang,YangyiChen,LifanYuan,YizheZhang,YunzhuLi,HaoPeng,andHengJi.
Executablecodeactionselicitbetterllmagents. ArXivpreprint,abs/2402.01030,2024.
JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,EdHuaihsinChi,F.Xia,Quoc
Le,andDennyZhou. Chainofthoughtpromptingelicitsreasoninginlargelanguage
models. ArXivpreprint,abs/2201.11903,2022.
13Preprint
YuxiangWei,ZheWang,JiaweiLiu,YifengDing,andLingmingZhang. Magicoder: Source
codeisallyouneed,2023.
MartinWeyssow,AtonKamanda,andHouariSahraoui. Codeultrafeedback: Anllm-as-a-
judgedatasetforaligninglargelanguagemodelstocodingpreferences. ArXivpreprint,
abs/2403.09032,2024.
WendaXu,GuangleiZhu,XuandongZhao,LiangmingPan,LeiLi,andWilliamYangWang.
Perils of self-feedback: Self-bias amplifies in large language models. ArXiv preprint,
abs/2402.11436,2024.
ZhilinYang,PengQi,SaizhengZhang,YoshuaBengio,WilliamCohen,RuslanSalakhut-
dinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable
multi-hopquestionanswering. InProc.ofEMNLP,2018.
WeihaoYu,ZihangJiang,YanfeiDong,andJiashiFeng. Reclor: Areadingcomprehension
datasetrequiringlogicalreasoning. InProc.ofICLR,2020.
Lifan Yuan, Yangyi Chen, Xingyao Wang, Yi Ren Fung, Hao Peng, and Heng Ji. Craft:
Customizingllmsbycreatingandretrievingfromspecializedtoolsets. ArXivpreprint,
abs/2309.17428,2023.
XiangYue,XingweiQu,GeZhang,YaoFu,WenhaoHuang,HuanSun,YuSu,andWenhu
Chen. Mammoth: Buildingmathgeneralistmodelsthroughhybridinstructiontuning.
ArXivpreprint,abs/2309.05653,2023.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
Zhuang,ZiLin,ZhuohanLi,DachengLi,EricP.Xing,HaotongZhang,JosephGonzalez,
andIonStoica. Judgingllm-as-a-judgewithmt-benchandchatbotarena. ArXivpreprint,
abs/2306.05685,2023.
TianyuZheng,GeZhang,TianhaoShen,XuelingLiu,BillYuchenLin,JieFu,WenhuChen,
andXiangYue. Opencodeinterpreter: Integratingcodegenerationwithexecutionand
refinement. ArXivpreprint,abs/2402.14658,2024.
JeffreyZhou,TianjianLu,SwaroopMishra,SiddharthaBrahma,SujoyBasu,YiLuan,Denny
Zhou,andLeHou. Instruction-followingevaluationforlargelanguagemodels. ArXiv
preprint,abs/2311.07911,2023.
BanghuaZhu,EvanFrick,TianhaoWu,HanlinZhu,andJiantaoJiao.Starling-7b:Improving
llmhelpfulness&harmlessnesswithrlaif,2023.
14Preprint
Table6: ULTRAINTERACTcoversadiversesetofdatasetsspanningthreetasks.
Task Datasets
GSM8K(Cobbeetal.,2021),MATH(Hendrycksetal.,2021b),MathQA(Aminietal.,2019),
Math
NumGlue(Mishraetal.,2022),TabMWP(Luetal.,2023)
CodeContest(Lietal.,2022),TACO(Lietal.,2023b),WikiTableQuestions(Pasupat&Liang,
Coding
2015),Magicoder-Evol-Instruct(Luoetal.,2023b;Weietal.,2023)
Logic ReClor(Yuetal.,2020),HotpotQA(Yangetal.,2018),StrategyQA(Gevaetal.,2021)
A AdditionalDetailsin ULTRAINTERACT Construction
A.1 DatasetDetails
Math. WeadoptGSM8K(Cobbeetal.,2021),MATH(Hendrycksetal.,2021b),MathQA
(Amini et al., 2019), and NumGLUE (Mishra et al., 2022)for mathematic reasoning, and
include TabMWP (Lu et al., 2023) for tabular processing. We retain all the instructions
for all datasets except MathQA, NumGLUE, and TabMWP. MathQA divides problems
intodifferentcategoriesaccordingtothetopicsandannotatestheformulathatindicates
the pattern needed to solve each problem. We apply stratified sampling to sample at
mostfiveproblemsforeachpatternandprioritizetheproblemsthatcomefromthelong-
tail category. Numglue contains eight different reasoning tasks and we discard Task 5
(ReadingComprehension+ExplicitNumericalReasoning),Task6(ReadingComprehension
+ImplicitNumericalReasoning),andTask7(QuantitativeNLI)duetothesimplicityMishra
etal.(2022). ForTabMWP,weonlykeepthequestionswithdifficultylevels4and5since
therestaretooeasyforcurrentstate-of-the-artmodels.
Code. WefocusonprogrammingwithPythonforthesimplicityofintegrationoftheinter-
preter. WeuseCodeContest(Lietal.,2022)andTACO(Lietal.,2023b),twocompetition-
levelcodingdatasetscollectedfromvariousonlineplatforms. Wefilterouttheoverlapped
questions. NotethatpartofthequestionsinTACOonlycontainground-truthsolutions
anddonotcontaintestcasesforevaluation,henceweapplyGPT-4togenerate12testcase
inputs(4basicinputs,4edgecases,and4largenumbers)foreachquestionandthenexecute
theground-truthsolutionsnippetstoproduceoutputs. Giventhatthetwodatasetsmainly
focusoncompetitionproblemsthatmaydeviatefromreal-worlddailyuses,weexclusively
adoptMagicoder-Evol-Instruct(Luoetal.,2023b;Weietal.,2023),theonlydatasetinour
selectionthatdoesnotcontaintestcasesorground-truthsolutions. WeemployGPT-4Turbo
tojudgethecorrectnessofgeneratedcodeduringinteraction,andthereforewedonotuse
thisdatasetforpreferencelearningsincewecannotrigorouslyconstructpairsofcorrectand
incorrectactionslimitedbytheevaluationreliability. WealsoincludeWikiTableQuestions
(Pasupat&Liang,2015)fortableprocessingwithcode.
LogicalReasoning. weusethemulti-hopreasoningdatasetsHotpotQA(Yangetal.,2018)
and StrategyQA (Geva et al., 2021), and the logical reasoning dataset ReClor (Yu et al.,
2020). WefollowthesettingofWangetal.(2023b)andconvertHotpotQAtoageneration
task, removing the contexts and requiring LLMs to search relevant information using
WikipediaAPI.
A.2 DetailsonPreferenceTreeConstruction
Models Adopted for Incorrect Action Sampling. We randomly sample one model
from Mistral-7B-Instruct-v0.2, DeepSeek-Coder-33B-Instruct, Mixtral-8x7B-Instruct, and
DeepSeek-LLM-67B-Chattogenerateoneincorrectactiontopairwitheachcorrectone.
CorrectActionGenerationBasedonGroundTruthAnnotations.
WeadoptGPT-3.5Turboasthegeneratortogeneratecorrectactionsbasedongroundtruth
considering the instruction-following ability. We provide different access to the ground
truthinformationfordifferenttasks,specifically: (1)Forcoding,wheretestcasesareblack
boxestoreferencesolutions,weprovidefullaccesstothesolutioncodes. Theactormodel
willaddstepmarksandcorrespondingexplanationstotheground-truthcodetomakeit
15Preprint
Table7: Statsbreakdown
HumanAnnotation
Task Dataset w/Tool? #Prompts #Pairs #CorrectAnswers. Avg.Length
HasAnswer? HasRationale?
(cid:33) 4,522 10,277 17,392 1,746.7 (cid:33) (cid:33)
GSM8K
(cid:37) 7,257 10,879 15,752 823.3 (cid:33) (cid:33)
(cid:33) 7,474 22,905 34,667 1,189.0 (cid:33) (cid:33)
MATH
Math (cid:37) 7,471 25,765 36,005 1,735.0 (cid:33) (cid:33)
(cid:33) 7,552 15,079 20,328 2,338.5 (cid:33) (cid:33)
MathQA
(cid:37) 7,159 17,743 22,500 1,916.3 (cid:33) (cid:33)
(cid:33) 3,020 3,601 5,717 1,474.6 (cid:33) (cid:37)
NumGLUE
(cid:37) 2,835 2,987 4,273 1,056.1 (cid:33) (cid:37)
TabMWP (cid:33) 3,117 4,135 6,083 842.6 (cid:33) (cid:37)
CodeContest - 8,167 44,319 44,666 2,061.7 (cid:33) (cid:33)
Coding TACO - 9,016 50,877 58,191 2,143.5 (cid:33) (cid:33)
WikiTableQuestions - 1,401 1,544 1,738 1,794.8 (cid:33) (cid:37)
Magicoder-Evol-Instruct - 10,374 0 10,238 687.1 (cid:37) (cid:37)
Reclor (cid:37) 4,467 7,958 7,231 1,266.7 (cid:33) (cid:37)
Logic HotpotQA (cid:33) 1,182 1,009 1,230 1,333.2 (cid:33) (cid:37)
StrategyQA (cid:33) 904 741 968 1,256.2 (cid:33) (cid:37)
easier to understand, or further refine the code for optimization. (2) For tool-free math
problems,toavoidtheactormodeldirectlycopyingtheanswerstopassthecorrectness
checking,wemasktheanswernumbersintherationalebeforeprovidingittoLLMs. This
approachcanbetterensureresponsequalitysinceitencouragesLLMstogenerateresponses
withcompletereasoningchainswitheachstepclearlymarked. (3)Forprogram-enhanced
mathreasoning,wefirsttranslatethetextualrationaleintocode. Then,weeitherdirectly
provideittotheactormodeltogenerateplans,orasktheactormodeltoconvertthecode
intomodularizationprogrammingandthenmakeplanstocreatetoolstosolveproblems.
A.3 DataDecomtamination
Weconductcarefuldecontamination. Firstly,forLeetCode,weapplytheExactSubstring
MatchingAlgorithm3tocomparewitheachinstructionintheULTRAINTERACTandfind
nooverlaps. Forothers,weperform8-gramexactmatchingtocompareULTRAINTERACT
instructionswithtestsetsofthesametask. Weremovethoseinstructionsthatoverlap8
gramswithanytestsample.
A.4 DetailedStatistics
In total, ULTRAINTERACT has 86Kinstructions and220K actionpairs. The Total# Pairs
doesnotequalTotal#TurnsinULTRAINTERACT,sincewefailtogeneratesufficientcorrect
actionsforeveryincorrectactioninmulti-turntrajectoriesmainlyduetoalackofsufficient
groundtruthannotations. Thetotal#pairsmaynotequal#correctanswers,either,because
itisalsodifficultandunnecessarytosampleincorrectactionsforthecorrectonesforsome
simpleinstructions. Wepresentthespecificinformationforeachdataset. Inparticular,we
listinformationonhumanannotationineachdataset, whichplaysanimportantrolein
correctactiongeneration(¬ß2.3andAppendixA.2). Allthreestepsofcorrectactionsampling
methodsmentionedin¬ß2.3canbeappliedtodatasetsthathaverationales,whilefordatasets
onlycontaininganswers,onlythefirsttwostepsareapplicable. Wedonotapplyanyofthe
three-stepmethodstogeneratecorrectanswersforMagicoder,theonlydatasetwithoutany
humanannotation,toconstructpreferencepairs.
B AdditionalDetailsonTraining EURUS Models
SupervisedFine-Tuning. Wefinetunebasemodelsfor1epochwitha2e-5learningrate
and0.1warmupratiousingacosinescheduler. ForEURUS-7B,wemix32KUltraChat,30K
ShareGPT,and50KOpenOrca. ForForEURUS-70B,wemix63KUltraChat,30KShareGPT,
and70KOpenOrca.
3https://github.com/bigcode-project/bigcode-dataset/tree/main/decontamination
16Preprint
PreferenceLearning. Forhyperparameters,allŒ≤issetto0.1,andŒª+/Œª‚àí inKTOissetto
1.33asrecommended. Wefinetunemodelsfor1epochwitha5e-7learningrateand0.1
warmupratiousingacosinescheduler.
RewardModeling. WetrainRMfor1epochwithlr=1e-5learningrate. Wealsouseacosine
schedulerwithawarmupratioof0.1.
Regardingpairaugmentation,wescaleupthepairsbymatchingeverycorrectactionfor
eachinstructionwithoneincorrectactionofotherturns. ThisleadstoNxNpairsofsingle-
turnactionsforatrajectoryofdepthN.Weremovetheactionpairsconsistingofnodesat
thesameturn,astheyarealreadypartofthemulti-turntrajectorypairsweincluded. Next,
toavoidoverfittingonthetrainingset,weonlyselectinstructionswithNxN‚â§10,andfor
theseinstructions,werandomlysampleatmost9pairswitheachactionoccurringnomore
than3times. Thisleadstoanaugmentationof240ksingle-turnactionpairs.
C AdditionalEvaluationResultsof EURUS
DetailedSetupin¬ß4. Formath,wetestbothtextual Table8: MMLUandMT-Bench.
reasoningandprogram-enhancedsettingsandreport Model MMLU MT-Bench
thebestperformanceofthetwo. Allevaluationsare
‚àº7B
conductedin0-shotCoTwithtwoexceptions: BBH Mistral-7B-Instruct-v0.2 58.9 7.60
Zephyr-7B-Œ≤ 59.7 7.34
uses3shotsandIFEvaldoesnotuseCoT.ForMINT, OpenChat-3.5-1210 63.4 7.81
we select MATH, TheoremQA, and MMLU-math Starling-LM-7B-Œ± 64.0 8.09
Magicoder-S-DS-6.7B 37.1 -
from ‚Äúreasoning‚Äù as a new ‚Äúmath‚Äù split. We also OpenCI-DS-6.7B 37.2 -
evaluate5-shotMMLU(Hendrycksetal.,2021a)for MAmmoTH-7B-Mistral 56.2 -
WizardMath-7B-v1.1 60.3 -
STEMknowledgeandMT-Bench(Zhengetal.,2023) OpenMath-Mistral-7B 58.3 -
for conversation abilities to study whether EURUS EURUS-7B-SFT 61.8 7.15
+DPO 62.4 7.38
needstotradeoffothercapabilitiesforreasoning. +KTO 62.2 7.38
+NCA 62.2 7.38
Results. ResultsareshowninTable8.
‚àº40B
OnMMLU,EURUSoutperformsbaselinesdedicated Mixtral-8x7B-Instruct 70.3 8.30
DeepSeek-Coder-33B-Ins 40.2 -
tocodingandmath,andachieveshigherresultsthan
‚àº70B
Mistral-Instruct-v0.2andCodeLLaMA-70B-Instruct,
CodeLLaMA-70B-Instruct 55.1 -
theofficialalignedversionsofourbasemodelbuilt DeepSeek-LM-67B-Chat 72.3 -
bytheirauthors. Comparedtogeneral-purposebase- QWen1.5-72B-Chat 72.9 8.61
OpenCI-CL-70B 52.4 -
line models, EURUS-7B achieves comparable per- OpenMath-CL-70B 60.2 -
formancewiththetop-performanceOpenChatand EURUS-70B-SFT 59.1 7.69
+KTO 59.5 7.93
Starling-LM, though EURUS-70B does not achieve +NCA 59.4 7.54
the same level of performance as other general- ProprietaryModels
purposemodels,whichisexpectedduetothegapin GPT-3.5Turbo 70.0 7.94
thebasemodelssinceCodeLLaMA-70Bhasnotbeen GPT-4 86.4 8.96
intentionallyoptimizedforknowledge.
OnMT-Bench,wereportbaselinenumbersfromtheofficialleaderboard4. EURUSmatches
theperformanceofmainstreamopen-sourcegeneral-purposemodels,andEURUS-70B-KTO
furtherachievesthescoreofGPT-3.5Turbo.
D DetailedResultsonRewardModeling
D.1 AdditionalResultsonReranking
WepresentthefullresultsonrerankinginTable9, wheretheconclusionsareconsistent
withthosedrawnfrom¬ßD:(1)Ourrewardmodelsalwaysachievethehighestaccuracy
onalltestsetsacrossdifferentN,exceptwhenN=2onHumanEval. (2)BothL andL
BT DR
consistentlyhelpimprovererankingperformanceonthreetestsetsexceptforHumanEval,
where removing either of the objectives can prevent the accuracy from dropping when
increasingNfrom8to16. (3)Modelingsafetyhurtsrerankingperformanceinreasoning.
4https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard
17Preprint
Table9: DetailedresultsofrerankingMistral-Instruct-v0.2‚Äôsresponsesoncodingandmath.
Datasets HumanEval MBPP GSM8K MATH
N 2 4 8 16 2 4 8 16 2 4 8 16 2 4 8 16
Random 41.5 39.0 40.2 39.6 33.1 33.6 34.3 30.1 45.0 43.1 44.5 40.2 11.5 11.3 10.0 8.5
TopLogits 43.3 43.3 43.3 43.3 35.3 35.3 35.3 35.3 45.7 45.7 45.7 45.7 12.1 12.1 12.1 12.1
Self-Consistency 43.3 42.7 42.1 40.9 35.3 36.3 36.6 37.1 45.7 49.5 52.2 52.8 12.1 13.8 15.8 16.8
Starling-RM-34B 47.6 47.0 49.4 45.7 37.8 38.8 39.6 40.4 49.1 52.8 56.0 56.5 6.5 7.2 7.7 7.7
EURUS-RM-7B 44.5 45.7 47.6 47.0 39.3 42.6 43.4 43.9 49.8 53.7 56.3 57.3 14.3 16.2 17.1 17.3
w/oLDR 45.7 44.5 46.3 50.0 39.3 42.4 42.4 42.1 49.4 53.2 55.4 56.3 14.2 16.1 17.0 16.9
w/oLBT 45.1 44.5 47.0 48.2 38.6 40.6 39.6 40.1 49.1 52.5 55.2 57.8 14.3 16.3 17.2 17.1
w/oUS 45.7 47.0 49.4 50.6 39.3 41.1 41.4 42.9 49.4 53.8 57.4 58.7 14.5 16.6 17.2 17.5
w/oUF+US 43.9 43.3 47.0 46.3 36.3 38.1 36.6 35.3 49.4 52.3 54.6 57.2 14.3 16.5 17.4 17.4
Pass@N 62.8 73.8 88.4 92.7 42.4 48.1 52.6 58.6 54.9 64.1 73.2 80.4 16.9 22.7 28.9 35.5
Table10: AblationStudy.
Coding Math Reasoning Ins-Following
Model Avg.
HumanEval MBPP LeetCode GSM8K MATH TheoremQA SVAMP ASDiv BBH IFEval
EURUS-7B-SFT 55.5 59.1 20.0 73.7 32.6 20.0 82.2 84.1 64.6 44.0 53.6
Ground-Truth 46.3 46.4 8.9 62.2 15.0 9.6 75.1 68.8 64.4 42.9 44.0
Open-SourceOnly 38.4 44.1 11.1 45.3 10.8 9.3 52.7 49.4 65.3 43.6 37.0
ULTRAINTERACTOnly 46.3 50.1 15.6 67.6 30.9 20.1 80.4 82.0 67.0 17.4 47.7
WhenremovingUltraSafetyfromthetrainingdata,theRMachieveshigheraccuraciesthan
EURUS-RM-7BexceptonMBPP.
E DetailedAblationResults
Wepresentthefullresultsof¬ß5inTable10,withdetailedmetricsonallcodingandmath
datasets.
18