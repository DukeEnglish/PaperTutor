GINopic: Topic Modeling with Graph Isomorphism Network
SumanAdhya and DebarshiKumarSanyal
IndianAssociationfortheCultivationofScience,Jadavpur,Kolkata-700032,India
adhyasuman30@gmail.com, debarshi.sanyal@iacs.res.in
Abstract andedgesdepictrelationshipsbetweenwords,such
as syntax or semantic relations. For instance, in
Topicmodelingisawidelyusedapproachfor
the case of short texts, the Graph Biterm Topic
analyzing and exploring large document col-
Model (GraphBTM) (Zhu et al., 2018), an exten-
lections. Recentresearcheffortshaveincorpo-
sionoftheBitermTopicModel(BTM)(Yanetal.,
ratedpre-trainedcontextualizedlanguagemod-
els,suchasBERTembeddings,intotopicmod- 2013),representswordco-occurrenceasagraph,
eling. However,theyoftenneglecttheintrinsic withnodesrepresentingwordsandweightededges
informational value conveyed by mutual de- reflectingthecountsofcorrespondingbiterms. De-
pendenciesbetweenwords. Inthisstudy, we spite GraphBTM’s emphasis on capturing word
introduce GINopic, a topic modeling frame-
dependencies,ithasbeenreportedtoexhibitpoor
work based on graph isomorphism networks
performance(Shenetal.,2021). Additionally,its
tocapturethecorrelationbetweenwords. By
computationalcostescalateswithanexpandingvo-
conducting intrinsic (quantitative as well as
cabulary,asitconstructsasinglegraphusingthe
qualitative)andextrinsicevaluationsondiverse
benchmarkdatasets,wedemonstratetheeffec- entire vocabulary. In contrast, the Graph Neural
tivenessofGINopiccomparedtoexistingtopic TopicModel(GNTM)(Shenetal.,2021)employs
modelsandhighlightitspotentialforadvancing adirectedgraphwithworddependenciesasedges
topicmodeling. between word nodes to incorporate semantic in-
https://github.com/AdhyaSuman/ formation from words in documents. However,
GINopic GNTMconsidersworddependencysolelybylink-
ingwordswithinasmallslidingwindowforagiven
1 Introduction document. Thislimitationmakesitimpossibleto
accountforworddependenciesthatfalloutsideof
Theriseindigitaltextdatamakesorganizingthem
that specific window. Furthermore, the computa-
manually by theme increasingly difficult. Topic
tionalcomplexityofgeneratingdocumentgraphs
modeling plays a significant role here (Newman
increaseswiththelengthofthewindow.
etal.,2010;Boyd-Graberetal.,2017;Adhyaand
Sanyal, 2022), as it can uncover the underlying Approach: To model the mutual dependency
topicsindocumentsinanunsupervisedmanner. In between words while addressing the existing is-
topicmodeling,weassumethateachdocumentis suesofincorporationofdocumentgraphsintotopic
amixtureoftopicsandtheselatenttopicsarealso modeling,wedevelopedaneuraltopicmodelthat
definedasdistributionoverthewords. takesthewordsimilaritygraphsforeachdocument,
Motivation: Recentapproachestoneuraltopic wherethewordsimilaritygraphisconstructedus-
modeling (Bianchi et al., 2021a,b; Grootendorst, ingwordembeddingstocapturethecomplexcorre-
2022)focusontherepresentationofthedocument lationsbetweenthewords. Thesedocumentgraphs
asasequenceofwords,whichcapturesthecontex- along with their unordered frequency-based text
tual information. However, wordsin a document representationarethenusedasinput. Wehavealso
may be correlated to each other in a much more used the Graph Isomorphism Network (GIN) to
complexmanner. So,whynotexplicitlyconsider obtaintherepresentationforeachdocumentgraph.
theseworddependencypatternswhilelearningthe WehaveusedGINasitisprovablythemaximally
topics? Several studies in the field of topic mod- powerful GNN under the neighborhood aggrega-
eling delve into the representation of documents tionframework. ItisaspowerfulastheWeisfeiler-
usinggraphs. Inthiscontext,nodessignifywords, Lehmangraphisomorphismtest(Xuetal.,2019).
4202
rpA
2
]LC.sc[
1v51120.4042:viXraContributions: Insummary,ourworkpresents Numerous contemporary methodologies incor-
thefollowingkeycontributions: porate Graph Neural Networks (GNNs) for topic
modeling. In terms of the graph construction
• WeintroduceGINopic,aneuraltopicmodel
task,theGraphBitermTopicModel(GraphBTM)
thatleveragesagraphisomorphismnetwork
(Zhu et al., 2018) and the Graph Neural Topic
toenhancewordcorrelationsintopicmodel-
Model(GNTM)(Shenetal.,2021)employamov-
ing.
ingwindow-basedapproachwithaspecifiedwin-
• Weperformacomprehensiveanalysisthrough dowlengthtomodelwordco-occurrencerelation-
quantitative, qualitative, and task-specific ships, necessitating careful window length selec-
evaluations. Additionally, we visualize the tion. Thegraphtopicmodel(Zhouetal.,2020)con-
latentspacesgeneratedbyourmodeltoassess structsdocumentgraphsbasedonTF-IDFscores,
its capability to disentangle the latent repre- capturing relationships with graph convolutions.
sentationsofdocuments. Topicmodelingwithknowledgegraphembedding
(Li et al., 2019) incorporates external knowledge
• We also conducted a sensitivity analysis for graphs. Thegraphattentiontopicnetwork(Yang
the selection of GIN among the GNNs and
etal.,2020)addressesoverfittinginprobabilistic
thechoiceofourgraphconstructionmethod-
latentsemanticindexingwithamortizedinference
ology. andwordembeddings. Thegraphrelationaltopic
model(Xieetal.,2021)exploresdocumentrelation-
2 RelatedWork
shipsusinghigher-ordergraphattentionnetworks.
Topicmodelingprocessesextensivedocumentcol-
lectionsefficiently,preservingkeystatisticalrela- 3 ProposedMethodology
tionships for tasks like classification, novelty de-
Recognizingthechallengesintopicmodeling,we
tection,summarization,andsimilarityjudgments.
acknowledgethenecessityofcapturingsemantic
TraditionalmodelslikeLatentDirichletAllocation
similarityamongwordsinadocument. Addition-
(LDA)(Bleietal.,2003),ProbabilisticLatentSe-
ally, we note the importance of addressing the
manticIndex(pLSI)(Hofmann,2013),andCorre-
graphconstructionissueandobtaininguniquerep-
latedTopicModel(CTM)(LaffertyandBlei,2005)
resentationsfordissimilardocumentgraphs. Inre-
usesampling-basedalgorithmsorvariationalinfer-
sponsetothesechallenges,wehaveintroducedthe
ence, but their design is limited by the need for
Graph Isomorphism Network-based neural topic
careful selection, which limits the flexibility and
model,abbreviatedasGINopic. Thefollowingsub-
scalabilityofmodeldesign.
sectionsprovideadetailedexplanationofthegraph
Recentadvancementsinneuralvariationalinfer-
constructionmethodology,modelframework,and
ence,particularlyAutoEncodingVariationalBayes
objectivefunction.
(AEVB)(KingmaandWelling,2014),simplifypos-
teriorcomputation. NeuralVariationalDocument 3.1 GraphConstruction
Model(NVDM)(Miaoetal.,2016)isthepioneer
Let D be defined as the set of all documents, V
VAE-based topic model. However, following the
as the set of all words in the corpus such that
traditionaltopicmodelsofapplyingDirichlet-prior |V| = V, and E ∈ RV×τ as the word embed-
to the document-topic distribution becomes chal- dingsmatrixsuchthatitsi-throwE ∈ Rτ,corre-
i
lengingduetothelimitationsofthereparametriza-
spondstothewordw ∈ V. Nowforadocument
i
tiontrick. AutoencodingVariationalInferenceFor
d ∈ D, which contains a subset of words from
Topic Models (AVITM) (Srivastava and Sutton, V, specifically V′ words, we define its weighted
2017) resolves this by using Laplacian approxi-
undirected document graph G as the adjacency
d
mation of the Dirichlet parameter with Gaussian
matrixA = (a ) ,wheretheelementsa
ij 1≤i,j≤V′ ij
parameters. CombinedTM(Bianchietal.,2021a)
aredeterminedasfollows:
extendsAVITMbyincorporatingsentenceBERT
(cid:40)
embeddingsalongsideBag-of-Words(BoW)rep- 0 if Sim(E ,E ) < δ
i j
a = (1)
resentations. ZeroShotTM(Bianchietal.,2021b) ij Sim(E ,E ) otherwise
i j
further extends this approach, relying solely on
SBERTembeddings,ignoringwordco-occurrence Here,Sim(E ,E )representsthecosinesimilar-
i i
relationsininputdocuments. ity between the word embedding vectors E and
iruleforGINatlayerl+1isdefinedasfollows:
w2v
(cid:18)
h(l+1) = MLP(l+1) (1+ϵ)h(l) + (2)
i i
(cid:19)
(cid:16) (cid:17)
(cid:8) (l) (cid:9)
AGG ω h ,j ∈ N(i)
ji j
compute
cosine-similarity
(l)
Here, h represents the feature vector for the
i
i-th node at layer l, N(i) denotes the set of all
neighborsfornodei,ω signifiestheedgeweight
ji
Figure1: Graphconstructionmethodology. between the i-th and j-th nodes. The opera-
tor AGG(·) stands for aggregation, and ϵ is a
parameter that can be learned or a fixed scalar
E . InEq. (1),δ isathresholdthatindicatesifthe
j valueclosetozero. Furthermore,MLP(l+1) repre-
similarityscorebetweentwowordsislessthanδ
sentsthemulti-layerperceptronforthe(l+1)-th
then there should not be any edge between them.
layer. After applying the L number of GIN lay-
Thechoiceofthisthresholdiscrucial,asoptingfor
ers,theencodingofanodeessentiallycapturesits
alowervaluemakestheconnectionsinG denser,
d L-th order neighborhood’s information. The de-
consequentlyelevatingcomputationalcomplexity. (cid:2)
tailed transformations are: GINConv(τ,H) →
Conversely, opting for a higher threshold value
BN → ReLU → [GINConv(H,H) → BN →
leadstoasparsedocumentgraph,ascenarioalso ReLU]L−2 → GINConv(H,τ′) → BN(cid:3) , where
undesired. The optimal choice of δ depends on
GINConv(I,J)representsGINlayerwithaMLP
the type of corpus. To balance these factors, we
ofinputdimensionI andoutputdimensionJ,H
considerδ asahyperparametertobetuned.
isthenumberofhiddenunits,BNisthebatchnor-
malization, and ReLU is the activation function.
3.2 ModelArchitecture The final node embeddings of dimension τ′ are
thensummeduptoobtaintherepresentationofthe
The proposed model GINopic comprises a docu-
(cid:80) (L)
documentgraphasfollows: h = h .
ment graph representation learning network fol- G i i
lowed by an encoder which is followed by a de-
coder. Theoutputofthegraphrepresentationlearn- 3.2.2 VAEframework
ingnetworkisconcatenatedwiththeTF-IDFrep-
Encoder Network: The encoder network of
resentation of the input document before feeding
GINopic, takes the combination of graph repre-
intotheencoder. TheframeworkisshowninFig. sentation (h ∈ Rτ′ ) and TF-IDF representation
G
2 and a detailed description of these networks is (x ∈ RV) of the input document. For this
TFIDF
describedinthefollowing.
concatenation, h is first scaled to the dimen-
G
sion same as of x and then concatenated
TFIDF
3.2.1 GraphRepresentationLearning
with x . Therefore, the resultant representa-
TFIDF
(cid:0) (cid:1)
tionisx = CONCAT f (h ),x ,where
The Weisfeiler-Lehman (WL) test serves as a W G TFIDF
means to evaluate the isomorphism of two pro- W ∈ RV×τ′ isamatrix,representinglineartrans-
vided graphs. The graph representation learning formation f W : Rτ′ → RV whose weights are to
modulewithintheproposedmodelisdesignedto belearned.
processdocumentgraphsasitsinputandproduce Careful selection of the prior for our model-
auniquerepresentationforeachtopologicallydis- ing assumption is crucial. In topic modeling, the
tinct document graph, identified through the WL Dirichletdistributionhasbeendemonstrated(Wal-
test. Tomodelthisinjectivemappingwehaveused lach et al., 2009) as effective in assigning topic
theGraphIsomorphismNetwork(GIN),knownfor proportions for a given document. However, the
its equivalent expressive power to the WL graph reparametrizationtrickislimitedtoGaussiandis-
kernel(Shervashidzeetal.,2011). GINistheoreti- tributions. To integrate the Dirichlet assumption
callyprovenasthemostpowerfulGNN(Xuetal., intoaVAEfollowingthemethodproposedby(Sri-
2019). Mathematically,thelayer-wisepropagation vastavaandSutton,2017),weusedtheLaplacianTF-IDF
Graph Representation Learning
Document graph
Figure2: ProposedframeworkforGINopicmodel.
approximationtotheDir(α)distribution: Following (Srivastava and Sutton, 2017), we re-
laxed the simplex constraint on β, which is em-
1 (cid:88)
µ 1k = logα k − logα i pirically shown to produce better topic quality.
K
i The transformations of the decoder network are,
1 (cid:18) 2 (cid:19) 1 (cid:88)K 1 (cid:2) Linear(K,V) → BN → Softmax(cid:3) ,withσ em-
Σ = 1− +
1kk α K K2 α ployed in the output layer to generate the word
k i
i distribution.
where, α is the i-th component of the K-
i
3.3 TrainingObjective
dimensionalDirichlet’sparameter,µ isthek-th
1k
component of the vector µ ∈ RK and Σ is The objective function for GINopic is the same
1 1kk
thek-thcomponentofΣ ∈ RK×K,thediagonal as ELBO which needs to be maximized in order
1
covariancematrix. Givenapriordistributionand to maximize the log-likelihood of the input data
theresultantinputdocumentrepresentationvector distribution. Thelossfunctionweseektominimize
x, the encoder outputs the posterior distribution isdefinedas:
q (z|x) ≡ N (µ ,Σ ), where ϕ represents the
ϕ 0 0
L = L +L (3)
weights of the encoder. The transformations in RL KL
theencoderare: (cid:2) Linear(2V,H′) → Softplus → ≡ −E
z∼q
(z|x)[p β(x|z)]+D KL(q ϕ(z|x)∥p(z))
ϕ
[Linear(H′,H′) → Softplus]L′−1 →
Dropout(0.2)(cid:3)
. This is followed by the two
In the above expression, the first term (L RL)
represents the reconstruction loss, quantified by
separate and similar transformations as follows:
(cid:2) Linear(H′,K) → BN(cid:3) for µ and Σ respec- thecross-entropybetweenthepredictedoutputdis-
0 0
tribution xˆ and the input vector x . On the
tively. In these expressions, V represents the TFIDF
otherhand,thesecondterm(L )istheKullback-
vocabulary size, H′ and L′ represent the number KL
Leibler(KL)divergenceofthelearnedlatentspace
of hidden units and hidden layers respectively,
distribution q (z|x) from the prior p(z) of the la-
Softplusisanactivationfunction,andDropoutis ϕ
tentspace.
aregularizer.
SamplingProcedure: Alatentrepresentationz
4 ExperimentalSettings
isstochasticallysampledfromtheposteriordistri-
bution q (z|x) using the reparameterization trick
WehaveconductedtheexperimentsusingOCTIS1
ϕ
1/2 (Terragni et al., 2021a), a comprehensive frame-
(KingmaandWelling,2014)asz = µ +Σ ⊙ϵ.
0 0
workforcomparingandoptimizingtopicmodels,
Thesymbol⊙denotestheFrobeniusinnerproduct
availableundertheMITLicense.
and ϵ ∼ N(0,1). The obtained latent representa-
tion z is then used as logit to a softmax function
4.1 Datsets
σ(·)inordertogeneratethedocument-topicdistri-
Intheexperiments,weutilizedfivepubliclyavail-
butionθ suchthat,θ = σ(z).
able datasets. Among these, 20NewsGroups
Decoder Network: In the decoder, the topic-
(20NG)andBBCNews(BBC)(GreeneandCun-
word matrix β refers to the learnable weights of
ningham,2006)datasetswerealreadyincludedin
thedecodernetwork. Thismatrixisutilizedtore-
constructtheworddistributionxˆas: xˆ =
σ(cid:0) β⊤θ(cid:1)
1https://github.com/MIND-Lab/OCTIS
gniloop
edoN
detcurtsnoceR noitatneserper#Total #Tr #Ts/Va Avg.Doc. Hyperpramerts 20NG BBC SS Bio SO
Dataset Labels
Docs Docs Docs length Graphconstructionthreshold(δ): 0.4 0.3 0.2 0.05 0.1
Dim.ofinputnodefeature(τ): 2048 256 1024 1024 64
20NG 16309 11415 2447 48.020 20
#GINlayers(L): 2 3 2 2 2
BBC 2225 1557 334 120.116 5
#HiddenlayersinMLP: 1 1 1 1 1
SS 12270 8588 1841 13.104 8
Dim.ofHiddenlayersinMLP: 200 50 50 200 300
Bio 18686 13080 2803 7.022 20 Dim.ofoutputnodefeature(τ′): 768 512 256 256 512
SO 15696 10986 2355 5.106 20
Table2: ValueofthehyperparametersofGINopicfor
Table1: Statisticsoftheuseddatasets. eachdataset.
OCTISinpre-processedformats. Additionally,we parisonwehavealsotunedthehyperparametersfor
incorporatedtheSearchSnippets(SS),Biomedicine GNTM.However,duetocomputationallimitations,
(Bio), and StackOverflow (SO) datasets (Qiang weareunabletofine-tunethehyperparametersfor
et al., 2022) and pre-processed them. A detailed GraphBTM.
description of these datasets is mentioned in Ap-
pendixA.1andthepre-processingstepsaremen- 5 ResultsandDiscussions
tionedinAppendixA.2. Statisticaldescriptionsof
Wecategorizeourfindingsintothefollowingsec-
these datasets can be found in Table 1. Each of
tions: (1)quantitativeevaluation(Section5.1),(2)
thesecorporawasdividedintotraining,validation,
extrinsic evaluation (Section 5.2), (3) qualitative
andtestingsets,withadistributionratioof70%for
evaluation (Section 5.4), (4) latent space visual-
training,15%forvalidation,and15%fortesting,
ization (Section 5.3), and (5) sensitivity analysis
wherethetrainingpartisusedtotrainthemodel,
(Section5.5).
thevalidationpartisonlyusedfortheGNTMto
modify the learning rate accordingly and the test
5.1 QuantitativeEvaluation
partisusedtoconducttheextrinsicevaluationof
themodels. In the quantitative evaluation, we have evaluated
the topic models based on the generated topic
4.2 Baselines
qualitymeasuredbycoherenceanddiversitymet-
We conducted a comparative analysis of the pro- rics. To measure the topic coherence we have
posedmodelGINopicwiththegraph-basedtopic used Normalized Pointwise Mutual Informa-
models, namely GraphBTM (Zhu et al., 2018) tion (NPMI) (Lau et al., 2014) and Coherence
and GNTM (Shen et al., 2021). Unfortunately, Value (CV) (Röder et al., 2015). NPMI is com-
forothergraph-basedtopicmodels,wecouldnot monly utilized (Adhya et al., 2022) as a surro-
accesstheirsourcecode,makingitimpossibleto gate for human judgment of topic coherence, al-
includetheminourcomparison. Beyondthegraph- thoughsomeresearchersalsoemployCV,despite
basedmodels,ourevaluationextendedtovarious itsknownissues. Wemeasurethediversityoftop-
well-knownneuralandtraditionaltopicmodels,in- icsusingInvertedRank-BiasedOverlap(IRBO)
cludingECRTM(Wuetal.,2023),CombinedTM (Bianchietal.,2021a),WordEmbedding-based
(Bianchi et al., 2021a), ZeroShotTM (Bianchi Inverted Rank-Biased Overlap - Match (wI-
et al., 2021b), ProdLDA (Srivastava and Sutton, M),andWordEmbedding-basedInvertedRank-
2017),NeuralLDA(SrivastavaandSutton,2017), BiasedOverlap-Centroid(wI-C)(Terragnietal.,
ETM(Diengetal.,2020),LDA(Bleietal.,2003), 2021b). Higher values of NPMI, CV, IRBO, wI-
LSI (Dumais, 2004) and NMF (Zhao and Tan, C, and wI-M indicate better performance. These
2017). Adetaileddescriptionoftheconfigurations metricsareelaboratelydiscussedinAppendixC.
ofthesebaselinestogetherwiththeirimplementa- Experimental Setup: For a given dataset we
tiondetailscanbefoundinAppendixB. run all the models by varying the topic count in
{20,50,100}∪{k }wherek standsforthe
gold gold
4.3 HyperparameterTuning
goldentopiccountwhichisthenumberofground-
InGINopic,foragivendatasetthehyperparame- truthlabelsinthedataset(sincetheyareavailable
tersthataretunedarementionedinTable2. Here, forthedatasetsweused). Thevaluesofk for
gold
thehyperparametertuningwasconductedoneach 20NG, BBC, and M10 are 20, 5, and 8, respec-
dataset,maintainingatopiccountequaltothenum- tively. For the robustness of the results, we have
beroflabelsfor50epochs. Toensureafaircom- reported the mean value over 5 random runs forGINopic GNTM CombinedTM ZeroShotTM NMF
20NG BBC SS Bio SO
0.1
0.05 0.1
0.10 0.1 0.0
0.00
0.0
0.05 0.0 0.1
0.05
0.1 0.2
0.00
20 50 100 5 20 50 100 8 20 50 100 20 50 100 20 50 100
20NG BBC SS Bio SO
0.6
00 .. 46 00 .. 46 0.4 0.4 0.4
0.2 0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0 0.0
20 50 100 5 20 50 100 8 20 50 100 20 50 100 20 50 100
Figure3: Topiccoherence(NPMIandCV)scoresforeachtopiccountfortop-5topicmodelsonfivedatasets.
20NG BBC SS Bio SO
Model
NPMI CV NPMI CV NPMI CV NPMI CV NPMI CV
ECRTM -0.145 0.363 -0.041 0.625 -0.388 0.474 -0.435 0.529 -0.416 0.526
CombinedTM 0.086 0.617 0.042 0.637 0.040 0.510 0.123 0.587 0.065 0.491
ZeroShotTM 0.083 0.617 0.024 0.630 0.031 0.504 0.120 0.579 0.056 0.486
ProdLDA 0.071 0.593 0.035 0.628 -0.001 0.486 0.105 0.571 0.042 0.473
NeuralLDA 0.045 0.500 -0.065 0.472 -0.114 0.400 -0.061 0.435 -0.177 0.407
ETM 0.050 0.528 0.030 0.452 -0.099 0.309 -0.136 0.140 -0.332 0.441
LDA 0.069 0.562 0.049 0.518 -0.165 0.376 -0.118 0.392 -0.174 0.345
LSI -0.019 0.400 -0.042 0.406 -0.122 0.280 -0.118 0.392 -0.129 0.303
NMF 0.088 0.599 0.069 0.543 -0.035 0.412 0.019 0.446 -0.050 0.377
GraphBTM 0.017 0.605 -0.173 0.484 -0.322 0.444 -0.398 0.519 -0.451 0.558
GNTM 0.081 0.588 0.090 0.600 0.005 0.445 -0.039 0.400 -0.129 0.359
GINopic 0.102 0.647 0.130 0.701 0.048 0.517 0.123 0.589 0.059 0.493
Table3: Comparisonoftopicmodelsonfivedatasets. Foreachmetricandeachtopicmodel,wementionthemean
scoresovertopiccounts{20,50,100}∪{k }.
gold
a given model, a given dataset, and a given topic acrossalldatasetsduetoitsembeddingclustering
count. regularizationapproach,despiteitspoorcoherence
scores indicating ineffective topic representation
Findings: We present coherence scores for
learning. IRBO scores of GINopic are also com-
all models across datasets in Table 3. Notably,
petitive,beingclosetothehighestscoreacrossall
GINopic achieves the highest coherence scores
datasets.
(bothNPMIandCV)acrossmostdatasets,except
fortheSOdatasetwhereitrankssecondinNPMI
5.2 ExtrinsicEvaluation
score,followingCombinedTM.However,GINopic
stillleadsinCVscorefortheSOdataset. Topro- Wehavealsoincorporatedanextrinsictasktoas-
videacomprehensivecomparison,wefocusonthe sesstheperformanceofthetopicmodels,specifi-
top 5 models based on their NPMI scores across callybyevaluatingtheirpredictivecapabilitiesina
alldatasets. Figure3showsthemeanandstandard documentclassificationtask.
deviation of NPMI and CV scores for each topic Experimental Setup: Our datasets include
count. The results establish the consistent supe- category labels for each document. We trained
riorperformanceofGINopiccomparedtoexisting all models on the training subset of a particular
models. Intermsofdiversity,Table3displaysall datasettogeneratek topics. Theresultingk -
gold gold
threediversityscores. GINopicachievesthehigh- dimensionaldocument-topicvectorservesasarep-
est wI-M and wI-C diversity scores across most resentationofthedocument. Alinearsupportvec-
datasets, except for the 20NG dataset where its tor machine is then trained on these representa-
wI-M score is comparable to ECRTM’s highest tions,andmodelperformanceonthetestsubsetis
score. ECRTMexhibitsthehighestIRBOscores reported. Wecalculatetheaverageaccuracyover
IMPN
VC20NG BBC SS Bio SO
Model
IRBO wI-M wI-C IRBO wI-M wI-C IRBO wI-M wI-C IRBO wI-M wI-C IRBO wI-M wI-C
ECRTM 0.998 0.473 0.852 0.999 0.454 0.848 1.000 0.442 0.839 1.000 0.433 0.838 1.000 0.382 0.825
CombinedTM 0.988 0.468 0.895 0.978 0.442 0.888 0.993 0.45 0.888 0.983 0.443 0.887 0.985 0.392 0.878
ZeroShotTM 0.986 0.467 0.894 0.964 0.435 0.887 0.99 0.448 0.888 0.983 0.445 0.885 0.985 0.393 0.879
ProdLDA 0.990 0.469 0.895 0.975 0.44 0.888 0.994 0.45 0.888 0.987 0.446 0.888 0.977 0.394 0.878
NeuralLDA 0.989 0.466 0.892 0.984 0.444 0.887 0.997 0.453 0.887 0.996 0.452 0.888 0.979 0.390 0.875
ETM 0.802 0.37 0.87 0.802 0.354 0.874 0.647 0.294 0.867 0.344 0.138 0.843 0.490 0.187 0.842
LDA 0.981 0.462 0.893 0.947 0.424 0.885 0.988 0.447 0.886 0.991 0.446 0.886 0.913 0.390 0.875
LSI 0.925 0.429 0.887 0.869 0.385 0.879 0.845 0.382 0.881 0.991 0.399 0.881 0.927 0.337 0.868
NMF 0.975 0.458 0.892 0.966 0.432 0.886 0.978 0.443 0.887 0.988 0.443 0.887 0.984 0.388 0.876
GraphBTM 0.971 0.462 0.852 0.986 0.448 0.846 0.947 0.421 0.836 0.924 0.427 0.837 0.958 0.374 0.821
GNTM 0.984 0.461 0.852 0.983 0.444 0.845 0.995 0.454 0.846 0.999 0.455 0.845 0.949 0.406 0.831
GINopic 0.989 0.468 0.895 0.992 0.457 0.893 0.998 0.454 0.889 0.983 0.462 0.888 0.986 0.497 0.879
Table4: Comparisonoftopicmodelsonfivedatasets. Foreachmetricandeachtopicmodel,wementionthemean
scoresovertopiccounts{20,50,100}∪{k }.
gold
fiverunsforeachdatasetandpresentthescoresin 2018). UMAPtransformedthek -dimensional
gold
Table5. document-topicdistributionintoatwo-dimensional
Findings: Table 5 shows that GINopic attains representation, making it possible to visualize.
the highest accuracy across all datasets, except Each document was assigned to a cluster based
forthe20NGdatasetwhereitsecuresthesecond- on its topic distribution vector θ, where the clus-
highestaccuracy,withtheGNTMcloselyedging terwasdeterminedbyselectingthetopicwiththe
ahead. highestprobability. Figure4illustratestheclusters
obtainedforeachdataset.
Model 20NG BBC SS Bio SO
ECRTM 0.411 0.816 0.492 0.361 0.457 20NG BBC SS
CombinedTM 0.397 0.796 0.706 0.493 0.715 20 20
10
ZeroShotTM 0.385 0.817 0.698 0.501 0.687
10 10
ProdLDA 0.385 0.752 0.662 0.489 0.674 0
NeuralLDA 0.297 0.575 0.464 0.376 0.403 0 0
10
ETM 0.370 0.754 0.496 0.083 0.072 10
0 10 20 0 10 10 0 10
LDA 0.428 0.798 0.440 0.364 0.412 Bio SO
LSI 0.329 0.337 0.343 0.402 0.660 15
NMF 0.350 0.785 0.415 0.437 0.708 10 10
5
GraphBTM 0.052 0.231 0.224 0.060 0.050 0 0
GNTM 0.449 0.806 0.222 0.049 0.053 5
GINopic 0.441 0.888 0.713 0.566 0.785 10 10
0 10 0 10 20
Table5: Averageaccuracyscoresinthedocumentclas- Figure4: LatentspacevisualizationforGINopicmodel
sificationtaskforallthemodelstrainedwithtopiccount acrossallfivedatasets.
k forallfivedatasets.
gold
Findings: The disentanglement of clusters is
depictedinFigure4foreachdataset. Notably,the
5.3 LatentSpaceVisualization
clarityofdisentanglementismorepronouncedin
Wehavefurtherexaminedthelatentspacegener- the BBC and SS datasets compared to the other
ated by GINopic. In topic modeling, documents threedatasets20NG,Bio,andSO.Thisdifference
areprojectedintoalower-dimensionallatent(topic) can be attributed to the greater challenge of dis-
space. entangling 20 different labels in the 20NG, Bio,
Experimental Setup: To visualize the latent and SO datasets, as opposed to the BBC and SS
space,wehavetrainedGINopicforthetopiccount datasetswithfewerdistinctlabels.
of k associated with each of the five datasets.
gold
5.4 QualitativeEvaluation
Following the training phase, we captured the
document-topic distribution for each document. Sincetopicmodelsoperateasunsupervisedmeth-
WeappliedtheUniformManifoldApproximation ods,itisrecommendedtoassesstheirperformance
and Projection (UMAP) technique, a robust di- notsolelyrelyingonautomatedestimatesoftopic
mensionality reduction method (McInnes et al., coherence but also through manual evaluation ofModel Topics
turkish,soviet,bullet,minority,population,burn,jewish,cold,prepare,joke
ECRTM draft,baseball,game,shot,blue,luck,stupid,programming,basically,score
bike,car,controller,button,camera,strategy,win,black,atheism,attribute
german,publish,genocide,turkish,muslim,armenian,book,representative,european,century
CombinedTM team,hockey,season,game,draft,expansion,ticket,play,year,ice
car,engine,tire,bike,ride,brake,good,problem,buy,mile
greek,turkish,minority,genocide,state,muslim,soviet,armenian,israeli,struggle
ZeroShotTM ranger,hockey,playoff,team,game,devil,king,pen,wing,period
motorcycle,bike,clean,wave,ride,wheel,tip,mirror,remove,replace
arab,israeli,religious,people,religion,jewish,solution,territory,understanding,land
ProdLDA year,fund,money,spend,program,player,private,team,job,good
eat,food,car,problem,engine,brake,stone,weight,pain,day
army,muslim,genocide,international,turkish,village,armenian,population,organize,enter
NeuralLDA goal,win,score,play,wing,penalty,playoff,team,pass,game
front,clean,ride,foot,bike,bar,engine,pull,weight,remove
armenian,people,turkish,village,kill,genocide,woman,live,soldier,jewish
ETM good,year,win,game,back,play,make,post,line,goal
bike,engine,mission,orbit,temperature,car,earth,space,planet,solar
war,jewish,israeli,land,country,arab,peace,territory,force,attack
LDA double,trade,game,hockey,final,team,star,playoff,king,regular
bike,ride,hate,advice,bank,motorcycle,weight,good,instruction,surrender
turkish,drive,war,armenian,russian,government,secret,military,power,jewish
LSI year,car,scsi,love,bit,client,team,server,call,player
access,engine,power,kill,database,word,bus,attack,disk,card
kill,woman,time,soldier,start,child,back,leave,armenian,man
NMF power,play,government,constitution,team,control,level,individual,idea,zone
car,engine,price,buy,bike,mile,ride,make,driver,tire
armenian,afraid,neighbor,clock,soldier,turkish,floor,soviet,beat,arrive
GraphBTM game,score,car,engine,play,goal,season,playoff,shot,player
tire,bike,connector,ide,brake,scsi,cable,car,rear,engine
israeli,arab,jewish,policy,land,territory,area,peace,human,population
GNTM team,game,play,player,win,year,good,call,point,time
tire,oil,brake,bike,paint,weight,corner,air,lock,motorcycle
genocide,muslim,armenian,massacre,turkish,population,kill,government,troop,war
GINopic team,win,score,baseball,game,player,hockey,playoff,goal,play
car,bike,ride,brake,light,tire,engine,lock,side,mile
Table6: Somerepresentativetopicsextractedfromthe20NGdatasetwithatopiccountof100. Relevantterms
withineachtopicareemphasizedinbold.
the topics, as emphasized by (Hoyle et al., 2021; ics: “Armenian genocide", “Sports", and “Au-
AdhyaandSanyal,2023). tomobile" related. Across these distinct topics,
GINopic consistently generates more correlated
Experimental Setup: We conducted a quali-
wordscomparedtoothermodels. Thisobservation
tative analysis of the topics, utilizing the 20NG
issupportedbytheconsistentlyhighernumberof
dataset and training all models with the golden
boldwordsforeachtopicinGINopic,indicating
topiccounti.e. k = 20. Theresultsappearin
gold
strongerwordcorrelationsthantheothermodels.
Table 6. Note that the table exhibits aligned top-
ics,whereinthefirsttopiclistedforonemodelis
5.5 SensitivityAnalysis
similartothefirsttopicforeveryothermodel,and
thesamegoesfortherestofthetopics,following 5.5.1 ChoiceoftheGraphNeuralNetwork
the alignment method proposed by (Adhya et al.,
To empirically check the effectiveness of GIN
2023). Additionally,wordscloselyassociatedwith
overotherGNNsinourmodel,wesubstituteGIN
agiventopicarehighlightedinbold.
withGraphAttentionNetwork(GAT)(Velicˇkovic´
Findings: In Table 6, we showcase three top- etal.,2018),GraphSAmpleandaggreGatE(Graph-GIN GAT GraphSAGE GCN
0.8
0.1 0.7
0.0 0.6
0.1 0.5
0.4
0.2
0.3
20NG BBC SS Bio SO 20NG BBC SS Bio SO
Figure5: Boxplotoftopiccoherence(NPMIandCV)scoresincorporatingGIN,GAT,GraphSAGE,andGCNin
GINopiconfivedatasets.
and CV) for each dataset. This threshold signi-
20NG BBC SS Bio SO
0.15 fies that if the similarity between two nodes in a
0.70
document graph falls below it, no edge connects
0.65
0.10 0.60 thosenodes. Moreover,increasingtheδ valuere-
0.05 0.55 sults in a sparser document graph, leading to re-
0.50
duced training time. Table 7 provides details of
0.0 0.1 0.2 0.3 0.4 0.5 0.0 0.1 0.2 0.3 0.4 0.5
thedataset-wiseoptimalthreshold(δ)valuesand
Figure 6: Coherence (NPMI and CV) scores for the corresponding percentage reductions in train-
each dataset by varying the threshold (δ) value in ing time from that with the δ value of 0.0. Thus,
{0.0,0.05,0.1,0.2,0.3,0.4,0.5}. bytuningδ,weimprovethecoherencescoresand
simultaneouslyreducethetrainingtime.
SAGE)(Hamiltonetal.,2017)andGraphConvo-
Value 20NG BBC SS Bio SO
lutionalNetwork(GCN)(KipfandWelling,2017).
Optimalthreshold(δ) 0.4 0.3 0.2 0.05 0.1
ExperimentalSetup: Wetrainedourproposed Reduction(%) 154.27% 266.72% 16.71% 0.29% 1.06%
modelonthefivedatasets,adjustingthetopiccount
Table 7: Optimal threshold (δ) value along with the
withintheset{20,50,100}∪k . Toensureafair
gold percentageoftrainingtimereductionforallfivedatasets.
comparison, we maintained consistent parameter
valuesacrossallmodels,aligningthemwiththose
ofGINopic.
Findings: In Figure 5, a box plot is pre- 6 Conclusion
sented, illustrating the NPMI and CV scores de-
WehaveintroducedGINopic,aneuraltopicmodel
rivedfromfiverandomrunsforeachmodelacross
basedonagraphisomorphismnetwork,andevalu-
thefivedatasets. TheresultsindicatethattheGIN-
ateditsperformanceonfivewidelyuseddatasets
incorporatedmodelconsistentlyoutperformsother
for assessing topic models. Across the majority
GNN-basedmodelsacrossalldatasetsintermsof
ofourexperiments,GINopicconsistentlyexhibits
boththecoherencemeasures.
superior topic coherence and diversity compared
5.5.2 ChoiceoftheGraphConstruction toothercompetitivetopicmodelsfromthelitera-
Threshold(δ) ture. Manualevaluationofselectedtopicsfurther
We have examined how the graph construction confirms that GINopic generates more coherent
threshold δ, as specified in Eq. (1), influences topics than alternative models. In extrinsic eval-
modelperformanceandtrainingtime. uations, GINopic generally outperforms existing
ExperimentalSetup: Givenadataset,wehave modelsacrossallthedatasets,exceptforthe20NG
trained our model for the corresponding k dataset. We utilized visualizations of the latent
gold
number of topics by varying the value of δ over space generated by GINopic to assess its cluster-
{0.0,0.05,0.1,0.2,0.3,0.4,0.5}. Wereportedthe ingdisentanglementcapability. Sensitivityanaly-
mean and standard deviation of the coherence sisdemonstratestheimpactofgraphconstruction
scores(NPMIandCV)over5randomrunsinFig- thresholdvaluesontheperformanceandtraining
ure6. time of GINopic. Additionally, we highlight the
Findings: Figure6illustratestheoptimalthresh- effectivenessofGINoverothergraphneuralnet-
oldvaluesthatmaximizecoherencescores(NPMI worksinourtopicmodel.
IMPN
IMPN
VC
VCLimitations FedericoBianchi,SilviaTerragni,DirkHovy,Debora
Nozza,andElisabettaFersini.2021b. Cross-lingual
This paper focuses solely on utilizing word simi- contextualizedtopicmodelswithzero-shotlearning.
larityforconstructingdocumentgraphs. However, InProceedingsofthe16thConferenceoftheEuro-
peanChapteroftheAssociationforComputational
thereexistalternativemethodsforconstructingdoc-
Linguistics: MainVolume,pages1676–1683,Online.
ument graphs, such as incorporating dependency
AssociationforComputationalLinguistics.
parsegraphs. Futureextensionsofthisworkcould
explorecapturingdiverseworddependenciesand David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet allocation. Journal of Ma-
integratingthemtoconstructamultifaceteddocu-
chineLearningResearch,3(Jan):993–1022.
mentgraph.
Jordan L. Boyd-Graber, Yuening Hu, and David M.
EthicsStatement Mimno.2017. Applicationsoftopicmodels. Found.
TrendsInf.Retr.,11(2-3):143–296.
The topic words presented in Table 6 depict the
AdjiB.Dieng,FranciscoJ.R.Ruiz,andDavidM.Blei.
output of the topic models trained on the 20NG
2020. Topicmodelinginembeddingspaces. Trans-
dataset. The authors have no intention to cause
actionsoftheAssociationforComputationalLinguis-
harmoroffensetoanycommunity,religion,coun- tics,8:439–453.
try,orindividual.
SusanT.Dumais.2004. Latentsemanticanalysis. An-
nualReviewofInformationScienceandTechnology,
38(1):188–230.
References
DerekGreeneandPádraigCunningham.2006. Practi-
SumanAdhya,AvishekLahiri,DebarshiKumarSanyal,
calsolutionstotheproblemofdiagonaldominance
andParthaPratimDas.2022. Improvingcontextu-
inkerneldocumentclustering. InProceedingsofthe
alizedtopicmodelswithnegativesampling. InPro-
23rd International Conference on Machine Learn-
ceedingsofthe19thInternationalConferenceonNat-
ing,ICML’06,page377–384,NewYork,NY,USA.
uralLanguageProcessing(ICON),pages128–138,
AssociationforComputingMachinery.
New Delhi, India. Association for Computational
Linguistics.
MaartenGrootendorst.2022. BERTopic: Neuraltopic
modelingwithaclass-basedtf-idfprocedure. arXiv
Suman Adhya, Avishek Lahiri, and Debarshi Kumar
preprintarXiv:2203.05794.
Sanyal. 2023. Do neural topic models really need
dropout? analysis of the effect of dropout in topic
WillHamilton,ZhitaoYing,andJureLeskovec.2017.
modeling. InProceedingsofthe17thConferenceof
Inductiverepresentationlearningonlargegraphs. In
theEuropeanChapteroftheAssociationforCompu-
AdvancesinNeuralInformationProcessingSystems,
tationalLinguistics, pages2220–2229, Dubrovnik,
volume30.CurranAssociates,Inc.
Croatia.AssociationforComputationalLinguistics.
ThomasHofmann.2013. Probabilisticlatentsemantic
SumanAdhyaandDebarshiKumarSanyal.2022. What analysis. CoRR,abs/1301.6705.
doestheIndianParliamentdiscuss? anexploratory
analysisofthequestionhourintheLokSabha. In AlexanderHoyle,PranavGoel,AndrewHian-Cheong,
ProceedingsoftheLREC2022workshoponNatural Denis Peskov, Jordan Boyd-Graber, and Philip
LanguageProcessingforPoliticalSciences,pages72– Resnik.2021. Isautomatedtopicmodelevaluation
78,Marseille,France.EuropeanLanguageResources broken? the incoherence of coherence. Advances
Association. inNeuralInformationProcessingSystems,34:2018–
2033.
SumanAdhyaandDebarshiKumarSanyal.2023. Im-
provingneuraltopicmodelswithWassersteinknowl- Diederik P. Kingma and Max Welling. 2014. Auto-
edge distillation. In Advances in Information Re- encoding variational Bayes. In Proceedings of the
trieval, pages 321–330, Cham. Springer Nature 2ndInternationalConferenceonLearningRepresen-
Switzerland. tations,ICLR2014.
Federico Bianchi, Silvia Terragni, and Dirk Hovy. Thomas N. Kipf and Max Welling. 2017. Semi-
2021a. Pre-training is a hot topic: Contextualized supervised classification with graph convolutional
documentembeddingsimprovetopiccoherence. In networks. InInternationalConferenceonLearning
Proceedingsofthe59thAnnualMeetingoftheAsso- Representations.
ciationforComputationalLinguisticsandthe11th
InternationalJointConferenceonNaturalLanguage JohnLaffertyandDavidBlei.2005. Correlatedtopic
Processing(Volume2:ShortPapers),pages759–766, models. InAdvancesinNeuralInformationProcess-
Online.AssociationforComputationalLinguistics. ingSystems,volume18.MITPress.Jey Han Lau, David Newman, and Timothy Baldwin. Silvia Terragni, Elisabetta Fersini, Bruno Giovanni
2014. Machine reading tea leaves: Automatically Galuzzi, Pietro Tropeano, and Antonio Candelieri.
evaluatingtopiccoherenceandtopicmodelquality. 2021a. OCTIS: Comparing and optimizing topic
InProceedingsofthe14thConferenceoftheEuro- modelsissimple! InProceedingsofthe16thCon-
peanChapteroftheAssociationforComputational ferenceoftheEuropeanChapteroftheAssociation
Linguistics, pages 530–539, Gothenburg, Sweden. forComputationalLinguistics: SystemDemonstra-
AssociationforComputationalLinguistics. tions,pages263–270.AssociationforComputational
Linguistics.
DingchengLi, SiamakZamani, JingyuanZhang, and
PingLi.2019. Integrationofknowledgegraphem- SilviaTerragni,ElisabettaFersini,andEnzaMessina.
beddingintotopicmodelingwithhierarchicalDirich- 2021b. Wordembedding-basedtopicsimilaritymea-
letprocess. InProceedingsofthe2019Conference sures. InNaturalLanguageProcessingandInforma-
oftheNorthAmericanChapteroftheAssociationfor tionSystems,pages33–45,Cham.SpringerInterna-
ComputationalLinguistics: HumanLanguageTech- tionalPublishing.
nologies,Volume1(LongandShortPapers),pages
940–950,Minneapolis,Minnesota.Associationfor PetarVelicˇkovic´,GuillemCucurull,ArantxaCasanova,
ComputationalLinguistics. Adriana Romero, Pietro Liò, and Yoshua Bengio.
2018. Graph attention networks. In International
LelandMcInnes,JohnHealy,NathanielSaul,andLukas ConferenceonLearningRepresentations.
Grossberger. 2018. Umap: Uniform manifold ap-
HannaWallach,DavidMimno,andAndrewMcCallum.
proximation and projection. The Journal of Open
2009. RethinkingLDA:Whypriorsmatter. InAd-
SourceSoftware,3(29):861.
vances in Neural Information Processing Systems,
volume22.CurranAssociates,Inc.
Yishu Miao, Lei Yu, and Phil Blunsom. 2016. Neu-
ralvariationalinferencefortextprocessing. InPro-
WilliamWebber,AlistairMoffat,andJustinZobel.2010.
ceedings of The 33rd International Conference on
Asimilaritymeasureforindefiniterankings. ACM
MachineLearning,volume48ofProceedingsofMa-
TransactionsonInformationSystems(TOIS),28(4):1–
chine Learning Research, pages 1727–1736, New
38.
York,NewYork,USA.PMLR.
Xiaobao Wu, Xinshuai Dong, Thong Nguyen, and
David Newman, Youn Noh, Edmund Talley, Sarvnaz
Anh Tuan Luu. 2023. Effective neural topic mod-
Karimi, and Timothy Baldwin. 2010. Evaluating
elingwithembeddingclusteringregularization. In
topic models for digital libraries. In Proceedings
Proceedingsofthe40thInternationalConferenceon
ofthe10thAnnualJointConferenceonDigitalLi-
MachineLearning,ICML’23.
braries, JCDL ’10, page 215–224, New York, NY,
USA.AssociationforComputingMachinery.
Qianqian Xie, Jimin Huang, Pan Du, and Min Peng.
2021. Graph relational topic model with higher-
J. Qiang, Z. Qian, Y. Li, Y. Yuan, and X. Wu. 2022.
ordergraphattentionauto-encoders. InFindingsof
Shorttexttopic modelingtechniques, applications,
theAssociationforComputationalLinguistics: ACL-
andperformance: Asurvey. IEEETransactionson
IJCNLP2021,pages2604–2613,Online.Association
Knowledge&DataEngineering,34(03):1427–1445.
forComputationalLinguistics.
MichaelRöder,AndreasBoth,andAlexanderHinneb-
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie
urg.2015. Exploringthespaceoftopiccoherence
Jegelka.2019. Howpowerfularegraphneuralnet-
measures. InProceedingsoftheEighthACMInterna-
works? In International Conference on Learning
tionalConferenceonWebSearchandDataMining,
Representations.
pages399–408.
Xiaohui Yan, Jiafeng Guo, Yanyan Lan, and Xueqi
DazhongShen,ChuanQin,ChaoWang,ZhengDong, Cheng.2013. Abitermtopicmodelforshorttexts.
HengshuZhu,andHuiXiong.2021. Topicmodeling InProceedingsofthe22ndInternationalConference
revisited: Adocumentgraph-basedneuralnetwork on World Wide Web, WWW ’13, page 1445–1456,
perspective. InAdvancesinNeuralInformationPro- New York, NY, USA. Association for Computing
cessing Systems, volume 34, pages 14681–14693. Machinery.
CurranAssociates,Inc.
Liang Yang, Fan Wu, Junhua Gu, Chuan Wang, Xi-
Nino Shervashidze, Pascal Schweitzer, Erik Jan van aochunCao,DiJin,andYuanfangGuo.2020. Graph
Leeuwen,KurtMehlhorn,andKarstenM.Borgwardt. attention topic modeling network. In Proceedings
2011. Weisfeiler-Lehmangraphkernels. Journalof of The Web Conference 2020, WWW ’20, page
MachineLearningResearch,12(77):2539–2561. 144–154,NewYork,NY,USA.AssociationforCom-
putingMachinery.
Akash Srivastava and Charles Sutton. 2017. Autoen-
coding variational inference for topic models. In RenboZhaoandVincentY.F.Tan.2017. Onlinenon-
Proceedingsofthe5thInternationalConferenceon negative matrix factorization with outliers. IEEE
LearningRepresentations,ICLR2017. TransactionsonSignalProcessing,65(3):555–570.DeyuZhou,XuemengHu,andRuiWang.2020. Neural
topicmodelingbyincorporatingdocumentrelation-
#No. Label #Docs %Docs
shipgraph. InProceedingsofthe2020Conference
onEmpiricalMethodsinNaturalLanguageProcess- 1. misc.forsale 861 5.28
ing(EMNLP),pages3790–3796,Online.Association 2. comp.windows.x 883 5.41
forComputationalLinguistics. 3. soc.religion.christian 920 5.64
4. talk.religion.misc 521 3.19
Qile Zhu, Zheng Feng, and Xiaolin Li. 2018.
5. rec.autos 822 5.04
GraphBTM: Graph enhanced autoencoded varia-
6. sci.med 866 5.31
tionalinferenceforbitermtopicmodel. InProceed-
7. talk.politics.misc 689 4.22
ingsofthe2018ConferenceonEmpiricalMethods
8. talk.politics.mideast 828 5.08
inNaturalLanguageProcessing,pages4663–4672,
9. sci.electronics 867 5.32
Brussels, Belgium. Association for Computational
Linguistics. 10. rec.sport.hockey 843 5.17
11. rec.sport.baseball 787 4.83
12. talk.politics.guns 808 4.95
A DataOverview
13. sci.crypt 883 5.41
A.1 DatasetDescriptions 14. comp.sys.mac.hardware 838 5.14
15. comp.sys.ibm.pc.hardware 891 5.46
Datasetsusedinexperiments: 16. comp.graphics 836 5.13
17. comp.os.ms-windows.misc 828 5.08
1. 20NewsGroups(20NG)datasetcomprising 18. alt.atheism 689 4.22
16,309pre-processeddocumentsfrom20dif- 19. sci.space 856 5.25
ferent newsgroups posts. Each document is 20. rec.motorcycles 793 4.86
labeledwithitscorrespondingcategorytype.
Table 8: 20NG labels with corresponding document
countsandpercentageofdocuments.
2. BBCNews(BBC)(GreeneandCunningham,
2006)datasetconsistsof2,225newsarticles
from BBC. Documents are categorized into
5differentclasses: tech,business,entertain-
ment,sports,andpolitics. #No. Label #Docs %Docs
1. tech 401 18.02
3. SearchSnippets(SS)(Qiangetal.,2022)is
2. business 510 22.92
derivedfrompredefinedphrasesacross8do-
3. entertainment 386 17.35
mains, this dataset is constructed from web
4. sport 511 22.97
search transactions. The domains include
5. politics 417 18.74
business,computers,culture-arts,education-
science,engineering,health,politics-society,
Table 9: BBC labels with corresponding document
andsports.
countsandpercentageofdocuments.
4. Biomedicine (Bio) (Qiang et al., 2022)
makesuseofthechallengedatadeliveredon
BioASQ’sofficialwebsite.
#No. Label #Docs %Docs
5. StackOverflow(SO)(Qiangetal.,2022)The
1. business 2652 21.61
dataset is released on Kaggle.com. The raw
2. computers 2177 17.74
datasetcontains3,370,528samplesfromJuly
3. culture-arts 1499 12.22
31st, 2012 to August 14, 2012. Here, the
4. education-science 1498 3.01
datasetrandomlyselects20,000questiontitles
5. engineering 1491 12.15
from20differenttags. 6. health 1411 12.21
7. politics-society 1173 9.56
The initial two datasets, 20NG, and BBC, are
8. sports 369 11.5
available on OCTIS2. As for the remaining three
datasetsSS,Bio,andSO,wehavepre-processed Table 10: SS labels with corresponding document
themusingthemethoddetailedinSectionA.2. countsandpercentageofdocuments.
2https://github.com/MIND-Lab/OCTISA.2 Preprocessing windowintopick,andp(w )andp(w )represent
i j
theprobabilityoftheindividualwords’occurrence
UsingOCTIS,weconverteachdocumenttolower-
in topic k. ϵ is a small positive constant to pre-
case,removethepunctuations,lemmatizeit,filter
ventzerointhelog(·)function. NPMIrangesfrom
thevocabularywiththemostfrequent2000terms,
−1(wordsneverco-occur)to+1(theyalwaysco-
filter words with less than 3 characters, and filter
occur). CV is computed using an indirect cosine
documentswithlessthan3words.
measure along with NPMI scores over a boolean
B BaselineConfigurations slidingwindow. Inourexperiments,weconsider
the top 10 words for each topic (i.e., n = 10) to
We reproduced all baseline models by following
computeNPMIandCVscores.
theguidanceprovidedintheiroriginalpapersand
utilizing codes from either the original sources D DiversityMetrics
or from OCTIS. Specifically, for CombinedTM
Topicdiversityquantifiestheuniquenessofgener-
(Bianchi et al., 2021a), ZeroShotTM (Bianchi
atedtopics. Tomeasurethetopicdiversitywehave
et al., 2021b), ProdLDA (Srivastava and Sutton,
used three following metrics: (i) IRBO (Bianchi
2017),NeuralLDA(SrivastavaandSutton,2017),
et al., 2021b), (ii) wI-M (Terragni et al., 2021b),
ETM(Diengetal.,2020),LDA(Bleietal.,2003),
(iii)wI-C(Terragnietal.,2021b). TheIRBOgives
LSI(Dumais,2004),NMF(ZhaoandTan,2017),
0foridenticaltopicsand1forcompletelydissimi-
weemployedtheimplementationfromOCTISwith
defaultparametervalues. ForGraphBTM3 (Zhu lartopics. Supposewearegivenacollectionℵof
et al., 2018), GNTM4 (Shen et al., 2021), and T topics where each topic is a list of words such
ECRTM5 (Wu et al., 2023) we utilized the offi- that the words at the beginning of the list have a
higherprobabilityofoccurrence(i.e.,aremoreim-
cial source codes. Hyperparameter optimization
portantormorehighlyranked)inthetopic. Then,
wasperformedforGNTMoneachdataset,andthe
theIRBOscoreofthetopicsisdefinedas,
valuesaredetailedinTable11. However,hyperpa-
rameteroptimizationforGBTMiscomputationally
(cid:80)T (cid:80)i−1
RBO(l ,l )
intensive,likelyduetoitsexhaustiveconsideration i=2 j=1 i j
IRBO(ℵ) = 1−
ofallwordsinthevocabularywhenconstructing n
thegraph.
where n =
(cid:0)T(cid:1)
is the number of pairs of lists,
2
andRBO(l ,l )denotesthestandardRank-Biased
Hyperpramerts 20NG BBC SS i j
Overlap between two ranked lists l and l (Web-
TemperatureforSTGS: 0.6 0.6 0.7 i j
Windowsizeforgraphconstruction: 3 2 10 beretal.,2010). IRBOallowsthecomparisonof
lists that may not contain the same items, and in
Table11: HyperparametervaluesforGNTMoneach particular, maynotcoverallitemsinthedomain.
dataset. Twolists(topics)withoverlappingwordsreceive
asmallerIRBOscorewhentheoverlapoccursat
thehighestranksoftheliststhanwhenitoccursat
C CoherenceMetrics
lowerranks. IRBOisimplementedinOCTIS.
Coherencematricesareusedtocomputetherele-
E ComputingInfrastructure
vance of the top words within topics. The NPMI
topic coherence for a given topic β with n top
k Our experiments were run on a workstation with
wordsiscalculatedasfollows:
Intel® Xeon® W-1350 @ 3.30GHz, 6 Cores, 12
Threads, 16.0 GB RAM, NVIDIA RTX A4000
n n log p(wi,wj)+ϵ GPU,CUDAVersion: 12.2andUbuntu22.04op-
NPMI(β ) =
1 (cid:88) (cid:88) p(wi)p(wj)
eratingsystem.
k (cid:0)n(cid:1)
−log(p(w ,w )+ϵ)
2 i=1j=i+1 i j
Here, p(w ,w ) is the probability of co-
i j
occurrenceofwordsw andw inabooleansliding
i j
3https://github.com/valdersoul/GraphBTM
4https://github.com/SmilesDZgk/GNTM
5https://github.com/BobXWu/ECRTM