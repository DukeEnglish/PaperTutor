1
Adversarial Combinatorial Bandits with
Switching Costs
Yanyan Dong and Vincent Y. F. Tan, Senior Member, IEEE
Abstract—We study the problem of adversarial combinatorial an application of the switching cost in edge computing with
bandit with a switching cost λ for a switch of each selected artificial intelligence, where the edge server can only utilize
arm in each round, considering both the bandit feedback and
a small number of machine learning models in each round
semi-bandit feedback settings. In the oblivious adversarial case
to learn the best subset of models based on the feedback.
with K base arms and time horizon T, we derive lower bounds
for the minimax regret and design algorithms to approach Downloading a model that is not in the current edge server
them. To prove these lower bounds, we design stochastic loss from the cloud incurs a switching cost.
sequences for both feedback settings, building on an idea from TheproblemofMABwithswitchingcostshasbeenstudied
previousworkinDekeletal.(2014).Thelowerboundforbandit
forboththeobliviousadversarialcaseandthestochasticcase.
feedbackisΩ˜(cid:0) (λK)31(TI)2 3(cid:1)
whilethatforsemi-banditfeedback
Wewillfocusontheobliviousadversarialsetting,wherelosses
is Ω˜(cid:0) (λKI)1 3T32(cid:1) where I is the number of base arms in the
are generated by an arbitrary deterministic source before the
combinatorialarmplayedineachround.Toapproachtheselower
game. In the oblivious adversarial case with K arms and time
bounds,wedesignalgorithmsthatoperateinbatchesbydividing
the time horizon into batches to restrict the number of switches horizon T, Arora et al. [4] refined the Exp3 algorithm to
betweenactions.Forthebanditfeedbacksetting,whereonlythe achievearegretupperbound1 ofO˜(K31T2 3)whentheswitch-
totallossofthecombinatorialarmisobserved,weintroducethe ing cost λ = 1. Later, Dekel et al. [5] proved that the upper
BATCHED-EXP2 algorithmwhichachievesaregretupperbound
o fef edO˜ b(cid:0) a( cλ kK s) et1 3 tT in2 3 g,I w4 3(cid:1) hea rs
e
aT llt le on ssd es st fo orin thfi eni cty o. mI bn int ah te ors ie am
l
ai- rb man ad ri et b isou Ω˜n (d (λis Kti )g 1 3h Ttb 2 3y ).sh Row ouin yg erth ea ttt ah le . m [6i ]ni pm ra ox pore seg dret al now ae lgr ob ro itu hn md
observed, we propose the BATCHED-BROAD algorithm which which is a modification of the Tsallis-Switch algorithm to
achieves a regret upper bound of O˜(cid:0) (λK)1 3(TI)2 3(cid:1) . achieve an upper bound of O((λK)31T32). Without switching
costs, the minimax regret of the adversarial MAB problem is
Index Terms—multi-armed bandits, adversarial bandits, com- √
Θ( TK) [7], [8].
binatorial bandits, switching costs, online optimization
Combinatorial bandits form a general extension of the
standard framework, which is a linear bandit with the special
I. INTRODUCTION combinatorial action set A ⊆ {0,1}K. We generalize the
The classical multi-armed bandit (MAB) problem is a problem of MAB with switching costs by considering the
sequential decision making game between an agent and an combinatorial problem with I arms played in each round,
environment [1], where the agent plays the arms sequentially where the set of the played arms is called a combinatorial
tominimizethetotallossovertime.Aftereacharmisplayed, arm. There are many practical applications of combinatorial
the agent receives some feedback in the form of a loss (or problems with switching costs. For example, a hospital may
gain) associated with the chosen arm. In many applications plantoexperimentonadrugthatisknowntobeacombination
such as the financial trading or reconfiguration in industrial of I components of K (raw material) components. In our
environments, there is a cost λ > 0 for a switch of each parlance, there are K base arms and the combinatorial arm
selectedarmineachround,whichmustbeconsideredtoassess containsI outoftheK basearms.Thequalityofeachchosen
the overall performance of the algorithms designed for them. component depends on certain unknown complex effects of
For example, Guha and Munagala [2] constructed a sensor the environment and patients, and this may be modeled by an
network problem and refined probabilistic models of sensed adversarialsetting.Theoveralleffectofthedrugisthesumof
data at various nodes, which costs energy in transferring from thequalitiesofalltheindividualchosencomponents.Thereis,
the current node to a new node. Shi et al. [3] introduced however,anon-negligiblepurchasing(orswitching)costwhen
theagentdecidestoswaponecomponentforanotherfromone
ThisworkissupportedbyfundingfromaMinistryofEducationAcademic time step to the next. In [3], a special combinatorial problem
ResearchFund(AcRF)Tier2grantundergrantnumberA-8000423-00-00and
was considered when the costly full feedback was available
AcRFTier1grantsundergrantnumbersA-8000189-01-00andA-8000980-
00-00. andaswitchingcostwasaddedineachround.Oursettingdoes
YanyanDongiswiththeSchoolofScienceandEngineering,TheChinese not allow full feedback and only considers bandit feedback
UniversityofHongKong,Shenzhen,Shenzhen518172,China(email:yanyan-
and semi-bandit feedback. Under bandit feedback, the player
dong@link.cuhk.edu.cn).ThisworkwascarriedoutwhenYanyanDongwas
aResearchFellowattheDepartmentofElectricalandComputerEngineering, can only observe the total loss of the played combinatorial
NationalUniversityofSingapore. arm while under semi-bandit feedback, all the losses for the
Vincent Y. F. Tan is with the Department of Mathematics and the De-
partment of Electrical and Computer Engineering, National University of
Singapore(e-mail:vtan@nus.edu.sg),Singapore,119077. 1Inthispaper,weuseO˜andΩ˜todenotethebig-Oandbigomeganotations
TheworkwasacceptedinIEEETransactionsonInformationTheory. ignoringanylogarithmicfactorsinT.
4202
rpA
2
]LM.tats[
1v38810.4042:viXra2
combinatorial arm are observed. and l are respectively the i-th component of the vectors
t,i
In this paper, we will focus on analyzing the regret of A ∈ {0,1}K and l ∈ [0,1]K), where ⟨·,·⟩ denotes the
t t
adversarial combinatorial bandit when there are switching inner product operation. We consider both bandit and semi-
costs. We derive lower bounds for the minimax regret and bandit feedback [11]. Under bandit feedback and after the
propose the algorithms that approximately meet the lower combinatorial arm A is pulled at round t, the player can
t
bounds under both feedback scenarios. To prove the lower only observe the feedback X =⟨A ,l ⟩∈[0,I] while under
t t t
bounds, we design stochastic loss sequences for both bandit semi-bandit feedback, all the losses for the combinatorial arm
feedback and semi-bandit feedback, which generalize the idea X = A ◦ l ∈ [0,1]K are observed where ◦ stands for
t t t
in [5] for the combinatorial scenarios. Under different types the element-wise multiplication. From time t−1 to t, there
of feedback, the loss sequences designed are different. For is a switching cost of λ·d(A ,A ) for the player where
t t−1
a fixed time, the loss sequence under bandit feedback uses d(A ,A ) ≜ 1∥A ⊕ A ∥ , i.e., d(A ,A ) measures
t t−1 2 t t−1 1 t t−1
the same Gaussian noise for different base arms while the thenumberofarmsswitchedfromthecombinatorialarmA
t−1
loss sequence under semi-bandit feedback uses i.i.d. Gaussian pulledattimet−1toA pulledattimet.WesetA =0and
t 0
noises for different base arms. We show that the lower bound then the first action A will always incur a switching cost of
1
for bandit feedback is Ω˜(cid:0) (λK)1 3(TI)2 3(cid:1) while that for semi- λI. The cumulative loss over T rounds for the player equals
bandit feedback is Ω˜(cid:0) (λKI)1 3T2 3(cid:1) where I is the number of
T T
(cid:88) (cid:88)
base arms in the combinatorial arm played in each round. ⟨A ,l ⟩+λ d(A ,A ).
t t t t−1
Dekel et al. [4], Rouyer et al. [6] and Shi et al. [3] all
t=1 t=1
utilize the batch-based algorithms to restrict the number of
Giventhelosssequencel ∈[0,1]K×T andactionsequence
switches between actions by dividing the whole time horizon 1:T
A ∈AT, define the λ-switching regret as
into batches and forcing the algorithm to play the same 1:T
action for all the rounds within a batch. We also utilize this R (A ,l )
λ 0:T 1:T
technique in our algorithms under our combinatorial setting.
T T T
In the bandit feedback setting, we introduce the BATCHED- ≜(cid:88)
⟨A ,l
⟩+λ(cid:88)
d(A ,A
)−min(cid:88)
⟨A,l ⟩.
t t t t−1 t
EXP2 algorithm with John’s exploration, which is a batched A∈A
t=1 t=1 t=1
version of the Exp2 algorithm with John’s exploration [9] and
achieves a regret bound of O˜(cid:0) (λK)31T2 3I4 3(cid:1) when the time A policy π = π 1:T is composed of all the conditional
distributionsoveractionsateachtimetgivenpastactionsand
horizonT tendstoinfinity.Inthesemi-banditfeedbacksetting,
feedback, i.e., π (·|A ,X ,...,A ,X ). The set of all
we introduce the BATCHED-BROAD algorithm, which is a t 1 1 t−1 t−1
policies in AT is denoted as Π and the set of all deterministic
batched version of the Online Mirror Descent algorithm with
loss sequences in [0,1]K×T is denoted as L. We define the
log-barrier regularizer (BROAD) in [10] and achieves a regret
upper bound of O˜(cid:0) (λK)1 3(TI)2 3(cid:1) . expected λ-switching regret when the loss functions l 1:T ∈L
are specified and a policy π ∈Π is employed as follows,
In the remainder of this paper, we first formulate the
problem and introduce our main results in §II. Then §III R (π,l )=E(cid:2) R (A ,l )(cid:3) ,
λ 1:T λ 0:T 1:T
presents the main ideas for proving the lower bound for
where the expectation is taken over the player’s randomized
two different types of feedback. In §IV, we introduce two
choice of actions under the policy π. In this paper, we use
algorithms designed for the bandit feedback and semi-bandit
regretto representfor λ-switchingregret forsimplicity. When
feedback respectively. §V is dedicated to the compare our
λ = 0, i.e., the switching cost is not considered, we write
algorithms with some baselines by numerical experiments. In
R(π,l )=R (π,l )forsimplicityandwecallR(π,l )
Appendix, we provide complete proofs for the lower bounds. 1:T 0 1:T 1:T
the pseudo-regret. We use the minimax expected λ-switching
regret to measure the difficulty of the game as in [5], which
II. PROBLEMFORMULATIONANDMAINRESULTS
is defined as follows,
We use [n] to denote the set {1,...,n} for any positive in-
R∗ = inf sup R (π,l ).
tegernandx 1:r todenotethesequence{x i}r i=1.Weconsider λ π∈Πl1:T∈L λ 1:T
anadversarialcombinatorialbanditproblemwithK basearms
Under bandit feedback, the following theorem states that
[K]andaswitchingcostλforeachswitchedarmwithλ>0.
there exists a loss sequence l such that R (π,l ) is
Thelossvectorsl ∈[0,1]K fort≥1arearbitrarilygenerated 1:T λ 1:T
t (cid:16) 1 2(cid:17)
bytheadversaryanddonotdependontheactionstakenbythe Ω (λK)3(TI)3 for any player policy π ∈Π, which implies
log T
2
learner, where the i-th component of l is the loss incurred if (cid:16) 1 2(cid:17)
t that R∗ =Ω (λK)3(TI)3 .
arm i is pulled at time t. Let A={A∈{0,1}K :∥A∥ =I} λ log T
1 2
be the set of all combinatorial arms with I base arms where Theorem1. Considerthecombinatorialbanditwithswitching
the i-th components of A is one if arm i is pulled and costsunderthebanditfeedback.Foranyplayerpolicyπ ∈Π,
zero otherwise. For the sake of simplicity, we define the there exists a loss sequence l ∈L that incurs an expected
1:T
set of base arms within a combinatorial arm I ∈ A as λ-switching regret of
{i ∈ [K] : I = 1}, where I is the i-th component in I.
i i (cid:32) (cid:33)
In each time t ≥ 1, the player pulls a combinatorial arm
(λK)1 3(TI)32
R (π,l )=Ω ,
A ∈ A and incurs a loss of ⟨A ,l ⟩ = (cid:80)K A l (A λ 1:T log T
t t t i=1 t,i t,i t,i 23
provided that K ≥3I and T ≥max{λK,8}. TABLEI:ComparisonBetweentheLowerBoundsandUpper
I Bounds under Two Types of Feedback
Under semi-bandit feedback, the following theorem states
bandit feedback semi-bandit feedback
that there always exists a loss sequence l such that
R (π,l ) is
Ω((λKI)1 3T2
3) for any player
p1 o:T
licy π ∈ Π,
lower bound
Ω˜(cid:0) (λK)31 (TI)2 3(cid:1) Ω˜(cid:0) (λKI)1 3T2 3(cid:1)
λ 1:T log 2T upper bound O(cid:0) (λK)1 3T32 I4 3(cid:1) O˜(cid:0) (λK)1 3T2 3I2 3 +KI(cid:1)
1 2
which implies that R∗ =Ω((λKI)3T3).
λ log T
2
Theorem2. Considerthecombinatorialbanditwithswitching
For semi-bandit feedback, the BROAD algorithm [10]
costs under the semi-bandit feedback. For any player policy
achieves the minimax regret when switching cost is not
π ∈ Π, there exists a loss sequence l ∈ L that incurs an
1:T considered. We introduce a refined version of this algorithm
expected λ-switching regret of
called the BATCHED-BROAD algorithm when the switching
(cid:32) (λKI)1 3T32(cid:33) costsareinvolved,whichachievesaregretboundasshownin
R (π,l )=Ω , below.
λ 1:T log T
2
Theorem 4 (Informal). Consider the combinatorial ban-
provided that K ≥3I and T ≥max{λK,6}. dit problem under semi-bandit feedback. For any adversary
I2
We observe that the orders of the lower bound of regret
l
1:T
∈ L, the policy π of BATCHED-BROAD as detailed in
Algorithm 4 achieves a λ-switching regret of
in Theorems 1 and 2 differ only in the combinatorial size
I. Also, the lower bound in Theorem 1 is greater than that in R λ(π,l 1:T)=O˜(cid:0) (λK)1 3(TI)32 +KI(cid:1) .
Theorem2intermsoftheorderofI duetothefactthatsemi-
bandit feedback results in more observations for the player WecomparethelowerboundsandupperboundsinTableI,
compared with bandit feedback. whichshowsthattheregretgapbetweenBATCHED-EXP2with
To prove Theorems 1 and 2, we design two stochastic John’s exploration and the lower bound scales at most as I2 3
loss sequences for bandit feedback and semi-bandit feedback and that between BATCHED-BROAD and the lower bound
respectively, which are presented in Algorithms 1 and 2 of scalesatmostI1 3.Closingthesegapsappearstobechallenging
Sections III-A and III-B, respectively. The design of two loss and is left for future work.
sequences generalizes the idea in [5] for the combinatorial
scenarios. For both loss sequences, we apply Yao’s minimax
III. LOWERBOUNDANALYSIS
principle [12], which asserts that the regret of a randomized
playeragainsttheworst-caseadversaryisatleasttheminimax Following the method in [5], we apply Yao’s minimax
regret of the optimal deterministic player against a stochastic principle [12] to prove Theorems 1 and 2. The principle
loss sequence. By employing this principle, we are able to states that the regret of a randomized player against the
prove the two theorems using the constructed adversaries. worst-case loss sequence is at least the minimax regret of
Underdifferenttypesoffeedback,theadversariesdesignedare the optimal deterministic player against a stochastic loss
different.Forafixedtime,theadversaryunderbanditfeedback sequence. Thus Theorem 1 (resp. Theorem 2) holds if we
uses the same Gaussian noise for different base arms while can construct a stochastic sequence of loss vectors L 1:T (each
the adversary under semi-bandit feedback uses i.i.d. Gaussian L t =(cid:0) L t,1,...,L t,K(cid:1) ∈[0,1]K isarandomvector)suchthat
noises for different base arms.
R (π,L )
We also design two algorithms for two types of feedback λ 1:T
thatcanbeshowntobealmostoptimalwhencomparedtothe (cid:34) (cid:88)T (cid:88)T (cid:35) (cid:88)T
≜E ⟨A ,L ⟩+λ d(A ,A ) −min ⟨A,L ⟩
lower bounds. Similar to [3], [4], [6], we utilize the batched t t t t−1 t
A∈A
algorithm to limit the number of switches between actions by t=1 t=1 t=1
(1)
dividing the whole time horizon T into batches and forcing
(cid:32) (cid:33)
the algorithm to play the same combinatorial arm for all the (cid:16)(λK)1 3(TI)2 3(cid:17) (cid:16)(λKI)1 3T32(cid:17)
=Ω resp. Ω ,
rounds within a batch. For bandit feedback, the EXP2 with log 2T log 2T
John’s exploration algorithm [9] is the most efficient among
foranydeterministicplayerpolicyπ,wheretheexpectationis
theexistingalgorithmswhenswitchingcostisnotconsidered.
taken over the adversary’s randomized choice of loss vectors.
We introduce a refined version of this algorithm called the
Inthefollowingtwosubsections,wewilljudiciouslyconstruct
BATCHED-EXP2 algorithm with John’s exploration when the
specific loss sequences for the two feedback scenarios, which
switching cost is involved, which achieves a regret bound as
are key to proving Theorems 1 and 2.
shown in below.
Theorem 3 (Informal). Consider the combinatorial bandit
problem under bandit feedback. For any adversary l ∈ L, A. Proof of Theorem 1: Bandit Feedback
1:T
the policy π of BATCHED-EXP2 with John’s exploration In this section, we provide the proof of Theorem 1. First
distribution as detailed in Algorithm 3 achieves a λ-switching weobtainLemma5forthestochasticsequenceoflossvectors
regret of L in Algorithm 1 which is constructed by generalizing the
1:T
R λ(π,l 1:T)=O(cid:0) (λK)1 3T2 3I4 3(cid:1) . loss sequence in [5]. Let clip(α) := min{max{α,0},1} and4
Algorithm 1: The Combinatorial Identical-Noise due to the fact that the random variables in it are sums of
(CIN) loss sequence Gaussianrandomvariables.Thisallowsustoboundtheregret
Input : Time horizon T, switching cost λ, number of under L 1:T. The detailed proof for this guarantee is provided
base arms K and combinatorial arm size I. in Lemma 12 of Appendix A.
Step 1: Set The differences between the CIN loss sequence in Algo-
rithm 1 and the loss sequence in [5] include the presence of I
(λK)1 3(IT)−1
3 bestarmsineachroundinsteadofoneandthevariationsinthe
ϵ= , (2)
9log T values of the parameters ϵ and σ. As in [5], the definition of
2
1 W inducesthecommonuncertaintyofallbasearms.Ateach
σ = . t
6(cid:113) log T log 4T(λ+ϵ) time t, the losses of the base arms in χ are all the same and
2 2 ϵ greater than other base arms by a constant ϵ. As the player
Chooseχ∈Auniformlyatrandomandthengenerate can only observe the total loss of all the base arms in the
W , for t=[T] according to (7). chosen combinatorial arm, it is difficult to figure out whether
t
Step 2: For all t∈[T] and x∈[K], set x-th components of the loss observed is induced by the randomness of W or due
L˜ t =(cid:0) L˜ t,1,...,L˜ t,K(cid:1) and L t =(cid:0) L t,1,...,L t,K(cid:1) as tothechosenarmsbeingbetterthanotheronesifthealt gorithm
1 does not switch the chosen combinatorial arm for some time.
L˜ t,x =W t+ 2 −ϵχ x, L t,x =clip(L˜ t,x). (3) Therefore, by the constructed loss sequence L t, we can prove
Output: Loss sequence L . the lower bound in Theorem 1.
1:T
By Yao’s minimax principle, Theorem 1 can be proved if
the following lemma is verified.
Lemma 5. Consider the combinatorial bandit problem with
switching costs under the bandit feedback. Let L be the
1:T
0 1 2 3 4 5 6 t
stochastic sequence of loss vectors defined in Algorithm 1.
Fig. 1: An illustration of the definition of parent time ρ(t). When K ≥ 3I and T ≥ max{λK,8}, for any deterministic
I
There is an arrow from each time t-th parent time ρ(t) to t. player’s policy π, we have
For example, ρ(3)=2 and ρ(4)=0.
(λK)31(TI)32
R (π,L )≥ .
λ 1:T 260log T
2
let χ x denote the x-th coordinate of a vector χ ∈ A. Let the To prove Lemma 5, we need to analyze the expected regret
parent time of t be under an arbitrary deterministic policy π :RI×T →AT when
ρ(t)=t−2δ(t), δ(t)=max{i≥0:2i divides t}. (4) thelosssequenceisL 1:T.NotethatunderL 1:T,adeterministic
policy π yields an action sequence A ∈ AT so that A
1:T t
See Figure 1 for an illustration of ρ(t). We say a time slot t′ is a function of the player’s past observations X with
1:t−1
is an ancestor for t if t′ =ρ(c)(t) for some positive integer c X = ⟨A ,L ⟩. We first analyze the expected regret under
t t t
and ρ(c) stands for the c iterated composition of ρ. Given the the same deterministic policy π and the loss sequence L˜ ,
1:T
function ρ, recursively define as in [5] the set of all ancestors which is defined in (3) of Algorithm 1. Similar to L , the
1:T
of t as S(t) by S(0)=∅ and deterministic policy π yields an action sequence A˜ ∈ AT
1:T
sothatA˜ isafunctionofpastobservationsY underL˜ ,
S(t)=S(ρ(t))∪{ρ(t)}, t∈[T]. (5) t 0:t−1 1:T
where
The depth of ρ is then defined as d(ρ)=max |S(t)|. As Y =1/2, Y =⟨A˜ ,L˜ ⟩. (8)
t∈[T] 0 t t t
in [5], we define cut(t)={s∈[T]:ρ(s)<t≤s}, which is
Define the conditional probability measures
the set of time slots that are separated from their parent by t.
The width of ρ is defined as Q (·)=P(·|χ=I), I ∈A, (9)
I
w(ρ)=max|cut(t)|. (6) Q 0(·)=P(·|χ=∅), (10)
t∈[T]
where Q and Q are the probability distributions under the
Thedesignofρin(4)guaranteesthatthedepthd(ρ)andwidth I 0
adversarieswithχ=I andχ=∅,respectively.ThusQ (·)is
w(ρ) are both upper bounded by log T +1 [5]. Define W 0
2 t the probability distribution when all arms incur the same loss.
for t=1,...,T recursively by setting W 0 =0 and LetF˜ betheσ-algebrageneratedbytheplayer’sobservations
W t =W ρ(t)+ξ t, ∀t∈[T], (7) Y 1:T.ThetotalvariationdistancebetweenQ 0 andQ I overF˜
is defined as
where ξ , t = 1,...,T are independent zero-mean, variance
t
σ2 Gaussian veriables. The design of the parent time function dF T˜ V(Q 0,Q I)= sup |Q 0(A)−Q I(A)|. (11)
ρ(t) and W guarantees that L˜ lies in [0,1] with high A∈F˜
t t,x
probability, which allows us to control the difference between This distance captures the player’s ability to identify whether
the λ-switching regrets under L and L˜ when the same combinatorial arm I is better than or equivalent to the other
1:T 1:T
deterministicstrategyisapplied.Thenwecanfirstanalyzethe combinatorial arms based on the loss values he observes [5].
regret bound under the loss sequence L˜ , which is easier In the following, we first give a key lemma that relates the
1:T5
player’s ability to identify the best action to the number of and
switches he performs to or from base arms in I. 1
Y =⟨A˜ ,L˜ ⟩=(W + +ξ )I−ϵ·⟨A˜ ,I⟩.
Define Y S = {Y t} t∈S and let ∆(Y S|Y S′) be the KL t t t ρ(t) 2 t t
divergence between the distribution of Y conditioned on Y
S S′ Under Q , similarly we have
0
under Q and Q , i.e.,
0 I
(cid:16) 1(cid:17) (cid:16) 1 (cid:17)
(cid:34) (cid:35) Y = W + I, Y = W + +ξ I.
Q (Y |Y ) ρ(t) ρ(t) 2 t ρ(t) 2 t
∆(Y |Y )≜E ln 0 S S′ . (12)
S S′ Q0 Q (Y |Y ) Then the distribution of Y conditioned on Y is
I S S′ t S(t)
N(Y ,I2σ2)underQ andN(Y +N ϵ,I2σ2)underQ ,
ρ(t) 0 ρ(t) t I
Using the chain rule, where N =⟨A˜ ,I⟩−⟨A˜ ,I⟩ is the difference between the
t ρ(t) t
numbers of arms in I played at time ρ(t) and t, and ϵ is
T
(cid:88)
∆(Y )≜∆(Y |∅)= ∆(Y |Y ). defined in (2) of Algorithm 1. Therefore,
0:T 0:T t S(t)
t=1 (cid:88)I (cid:16) (cid:13) (cid:17)
The analysis in [5] focused on the evaluation of ∆(Y 0:T), ∆(Y t|Y ρ(t))= Q 0(N t =i)d KL N(0,I2σ2)(cid:13) (cid:13)N(iϵ,I2σ2)
i.e. the Kullback-Leibler (KL) divergence between Q (Y ) i=−I
0 0:T
and Q I(Y 0:T), d KL(Q 0(Y 0:T)∥Q I(Y 0:T)), which has a close (cid:88)I i2ϵ2
= Q (|N |=i) ,
connection to the number of switches of arms in their setting 0 t 2I2σ2
and therefore leads to the lower bound of the regret. As [5], i=1
we define where {|N t|=i} is the event that the player switched at least
i arms from or to base arms in I between rounds ρ(t) and
T T
M
I
≜2(cid:88) ⟨A t⊕A t−1,I⟩ and M˜
I
≜2(cid:88) ⟨A˜ t⊕A˜ t−1,I⟩, t. We observe that N t′ ≜ ⟨A˜ ρ(t)⊕A˜ t,I⟩ is the total number
of switches the player performs to or from base arms in I
t=1 t=1
(13) between rounds ρ(t) and t. Then |N t|≤N t′ and
asthetotalnumbersofswitchestheplayerperformstoorfrom ϵ2 (cid:88)T (cid:88)I
∆(Y )= Q (|N |=i)i2
basearms in I duringthe wholetime horizonunder L 1:T and 0:T 2I2σ2 0 t
L˜ ,respectively.Definethetotalnumbersofswitchesinthe t=1 i=1
1:T
whole time horizon under L and L˜ as ϵ2 (cid:88)T (cid:88)I
1:T 1:T ≤ Q (|N |=i)i
2Iσ2 0 t
T T t=1 i=1
M ≜(cid:88) d(A t,A t−1) and M˜ ≜(cid:88) d(A˜ t,A˜ t−1). (14) ϵ2 (cid:88)T
= E [|N |]
t=1 t=1 2Iσ2 Q0 t
(cid:12) (cid:12) t=1
o(cid:0)N K vIo e−−t ri 1c 1 a(cid:1)e l, lt wh Ia ht e ∈rf eo Ar I i∀ oi fi s Mt∈ he I[K i- (t i] h ., e.cw ,oe (cid:80)mh p Ia o ∈v n Ae en M(cid:12) t{ IoI f )∈ I is,A a (cid:0)n
K
Id −:
−
1tI 1h (cid:1)i en t= imth1 ee} ss(cid:12) u thm=
e
≤ 2Iϵ2
σ2
(cid:88) t=T 1E Q0[N t′]. (15)
total number of switches the player performs to or from base We have N′ = (cid:80) I ·1 , where Z = {A˜ ̸=
armsin[K].Sinceeachswitchiscountedtwiceinthe“to”and t i∈[K] i Zt,i t,i ρ(t),i
“from”directions,respectively,weconcludethat(cid:80) M = A˜ t,i} (A˜ x,i is used to denote the i-th component of A˜ x). Let
I∈A I
2(cid:0)K I−− 11(cid:1) M and (cid:80) I∈AM˜ I =2(cid:0)K I−− 11(cid:1) M˜. M i =(cid:12) (cid:12){t∈[T]:A˜ t−1,i ̸=A˜ t,i or A˜ t,i ̸=A˜ t+1,i}(cid:12) (cid:12),
The upper bound for ∆(Y ) in terms of M˜ and w(ρ)
0:T I denote the total number of switches the player performs to or
can be derived as follows, which directly leads to the upper
bound for dF T˜ V(Q 0,Q I) in terms of M˜
I
and w(ρ). Based on (cid:80)from act Iion ·Mi d .ur Lin eg
t
sthe who dl ee nt oi tm ee thh eor ti iz mo en. slW ote sh oa fv se wM i˜ tcI he=
s
Lemma 6, we can prove Lemma 5, and the whole proof is i∈[K] i i 1:Mi,i
from or to arm i, i.e. A˜ ̸=A˜ or A˜ ̸=A˜
provided in Appendix A. sj,i−1,i sj,i,i sj,i,i sj,i+1,i
for any j ∈ {1,...,M }. Since the event Z implies that
i t,i
Lemma 6. Under the loss sequence L˜ 1:T, which is defined there exists at least one time s of switch from or to action i,
in(3)ofAlgorithm1withχ=I ∈A,itholdsthat∆(Y 0:T)≤ such that t∈cut(s), we have
ϵ2 w(ρ)E [M˜ ], which implies that
2Iσ2 Q0 I T T
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
dF˜ (Q ,Q )≤ ϵ √ (cid:113) w(ρ)E [M˜ ]. N t′ = I i1 Zt,i ≤ I i 1 Zt,i
TV 0 I 2σ I Q0 I t=1 t=1i∈[K] i∈[K] t∈cut(sr,i)
≤ (cid:88) I
(cid:88)Mi
|cut(s )|≤ (cid:88) I M w(ρ)=M˜ w(ρ),
i r,i i i I
Proof: For any combinatorial arm A ∈ A, ⟨A,I⟩ is the i∈[K] r=1 i∈[K]
number of optimal arms in the combinatorial arm. Under Q , (16)
I
by (3) and (7), we have
where w(ρ) is the width of ρ defined in (6). Therefore,
1
Y =⟨A˜ ,L˜ ⟩=(W + )I−ϵ·⟨A˜ ,I⟩, ϵ2
ρ(t) ρ(t) ρ(t) ρ(t) 2 ρ(t) ∆(Y )≤ w(ρ)E [M˜ ].
0:T 2Iσ2 Q0 I6
Algorithm 2: The Combinatorial Diverse-Noise different and thus the KL divergence on the observed losses
(CDN) loss sequence under two adversaries is infinite. To overcome this issue, the
Input : Time horizon T, number of actions K and losssequenceinAlgorithm2isintroduced,whichinducesthe
combinatorial arm size I same support for the observed losses under both χ = ∅ and
Step 1: Set σ = 1 and ϵ= (λK)1 3I−32 T−1 3. Choose χ ∈ A. Further details will be provided in the proof of the
(9log 2T) 9log 2T following Lemma 7.
χ∈A uniformly at random and then generate Wi
t By Yao’s minimax principle, Theorem 2 can be proved if
for t∈[T] and i∈[K] according to (18).
the following lemma is verified.
Step 2: For ∀t∈[T] and ∀x∈[K], set x-th components of
L˜ t and L t as Lemma 7. Consider the combinatorial bandit problem with
L˜ =Wx+ 1 −ϵχ , L =clip(L˜ ). (17) switching costs under the semi-bandit feedback. Let L 1:T be
tx t 2 x tx tx the stochastic sequence of loss functions defined in Algo-
Output: Loss sequence L . rithm 2. When K ≥ 3I and T ≥ max{λK,6}, for any
1:T I2
deterministic player π, we have
By Pinsker’s inequality [13, Lemma 11.6.1], we have R (π,L )≥
(λKI)1 3T2
3 .
λ 1:T 60log T
2
dF˜
(Q ,Q )= sup |Q (A)−Q (A)|
TV 0 I 0 I To prove Lemma 7, we need to analyze the expected regret
A∈F˜
ϵ (cid:113) under an arbitrary deterministic policy π : [0,1]I×T → AT
≤ 2σ√
I
w(ρ)E Q0[M˜ I]. when the loss sequence is L 1:T. Note that under L 1:T, a
deterministic policy π yields an action sequence A ∈ AT
1:T
sothatA isafunctionoftheplayer’spastobservationsZ
t 1:t−1
It may be possible to improve Lemma 6 to obtain a tighter
withZ =A ◦L .Similarto§III-A,wefirstanalyzetheregret
t t t
lower bound. In the proof of Lemma 6, there is an inequality under the loss sequence L˜ defined in Algorithm 2 that the
|N |≤N′ usedin(15),whereN =⟨A˜ ,I⟩−⟨A˜ ,I⟩isthe 1:T
t t t ρ(t) t player would suffer on the deterministic policy π◦clip. The
difference between numbers of arms in I played at time ρ(t)
policy π◦clip yields the same action sequence A with that
andtandN t′ ≜⟨A˜ ρ(t)⊕A˜ t,I⟩isthetotalnumberofswitches of L
1:T
under L˜ 1:T. Thus we only need to analy1 z: eT the regret
theplayerperformstoorfrombasearmsinI betweenrounds under the loss sequence L˜ and the action sequence A .
ρ(t) and t. It is easy to observe that in many cases of A˜ 1:T 1:T
ρ(t) Let
and A˜ , this inequality is not tight and could even be quite 1
t Y = , Y =A ◦L˜ . (19)
loose. Therefore, the design of a loss sequence that tightens 0 2 t t t
this inequality is a good future research direction to possibly
and Y =L˜ A . Define Y ={Y } and let ∆(Y |Y )
obtain a tighter lower bound. t,j t,i t,i S t t∈S S S′
be the KL divergence between the distribution of Y condi-
S
tioned on Y under Q and Q as defined in (12), where Q
S′ 0 I 0
B. Proof of Theorem 2: Semi-bandit Feedback
and Q are definedas in (9)and (10)under the losssequence
I
Similar to the analysis of lower bound under bandit feed- L˜ in Algorithm 2.
1:T
back in §III-A, we will prove Theorem 2 by constructing a In the following lemma, we derive a relation between
stochastic loss sequence L in Algorithm 2. Let ρ(t) be ∆(Y ) and M , where M is the total number of switches
1:T 0:T I I
defined according to (4). For any i ∈ [K], define Wi for the player performs to or from base arms in I during the
t
t∈[T] and recursively by Wi =0 and wholetimehorizonanddefinedas(13)in§III-A.Basedonthe
0
inequality in the following Lemma 8, we can prove Lemma 7
Wi =Wi +ξi, ∀t∈[T], (18)
t ρ(t) t by the similar verification with [5] and the whole proof is
where ξi, t ∈ [T],i ∈ [K] are independent zero-mean, detailed in Appendix B.
t
variance σ2 Gaussian variables. The difference between the Lemma 8. Under the loss sequence L˜ , which is defined
1:T
loss sequences in Algorithm 2 and Algorithm 1 lies in the in (17) of Algorithm 2 with χ = I ∈ A, it holds that
independent and identically distributed (i.i.d.) nature of the ∆(Y )≤ ϵ2 w(ρ)E [M ].
added Gaussian noises ξi for each arm i ∈ [K]. This 0:T 2σ2 Q0 I
t
modification effectively tackles the challenge presented by Proof: Using the chain rule,
the semi-bandit feedback scenario, where all losses for each
T
base arm in the selected combinatorial arm are observed. ∆(Y )≜∆(Y |∅)=(cid:88) ∆(Y |Y ),
Underthe losssequence L˜ inAlgorithm 1andsemi-bandit 0:T 0:T t S(t)
1:T t=1
feedback, the observed losses for each base arm in the chosen
combinatorial arm at time t ∈ [T] when χ = ∅ are all whereS(t)isdefinedasin(5).SinceY t,j areindependentfor
the same while the losses for each base arm when χ ∈ A different j ∈[K] given Y S(t),
may not be the same and differ by ϵ (see definition of ϵ in
(cid:88)
∆(Y |Y )= ∆(Y |Y ).
Algorithm 1). Then the supports for observed losses under t S(t) t,i S(t)
the adversaries in Algorithm 1 when χ = ∅ and χ ∈ A are i:At,i=17
ξ 4i B 1 B 2 B 3 B 4
ξ 1i
ξ 2i ξ 3i
ξ
5iξ 6i ξ 7i t=0 t
W 0i W 1i W 2i W 3i W 4i W 5i W 6i W 7i t Switch Switch Switch Switch
Fig. 2: An illustration of the definition of Wi for i ∈ [K]. Fig. 3: An illustration of the batches in algorithm. The whole
t
The value of Wi is obtained by summing the i.i.d. Gaussian time horizon are divided into batches and during each batch,
t
variables ξi’s on the edges along the path from Wi, i.e. the player does not change the choice of the combinatorial
t′ 0
summing over all t′ ∈ S(t)∪{t}\{0}. For example, Wi = arm.
7
ξi +ξi +ξi.
4 6 7
atleastiarmstoactionsinI betweenroundsρ(t)andt.Then
Let I i denote the i-th component of I. In the following, we ϵ2 (cid:88)T 1 (cid:88)I
analyze the value of ∆(Y |Y ) for t ∈ [T] and i ∈ [K] ∆(Y )= Q (N∗ =i)i
t,i S(t) 0:T 2σ2 |S(t)| 0 t
such that A t,i =1. t=1 i=1
1) When I i = 0, the distributions of Y t,i conditioned on ≤
ϵ2 (cid:88)T
E [N∗]
Y are the same under both Q and Q . Specifically, 2σ2 Q0 t
S(t) 0 I t=1
when i is not the optimal arm, the probability distri-
b anu dtio Qns Io af reY t t,i he= saL˜ mt, ei c sio nn cd eiti to hn eed deo fin nitY ioS n(t s) ou fnd L˜e tr ,iQ in0 ≤ 2ϵ σ2
2
(cid:88) t=T 1E Q0[N t′] (20)
Algorithm2arethesameunderQ andQ wheni∈/ I. ϵ2
0 I ≤ w(ρ)E [M ],
Thus ∆(Y t,i|Y S(t))=0. 2σ2 Q0 I
2) When I =1 and A =0 for all t′ ∈S(t), the distri-
butions
oi
f Y
condit t′ i, oi
ned on Y are
N(cid:0)1,|S(t)|σ2(cid:1) where N t′ ≜ ⟨A˜ ρ(t)⊕A˜ t,I⟩ is the total number of switches
under Q
ant d,i N(cid:0)1 −ϵ,|S(t)|σS 2(cid:1)(t)
under
Q2
. Then
theplayerperformstoorfrombasearmsinI betweenrounds
0 2 I ρ(t) and t, and the last inequality holds due to (16).
It may also be possible to improve Lemma 8 to obtain a
∆(Y t,i|Y S(t))=d
KL(cid:0) N(0,|S(t)|σ2)(cid:13) (cid:13)N(ϵ,|S(t)|σ2)(cid:1)
.
tighter lower bound. In the proof of Lemma 8, there is an
inequality N∗ ≤N′ used in (20), where
t t
F tho er ne wxa em hp al ve e, s Wup ipo =se 1t += ξi7 +an ξid +A ξ4, ii u= ndeA r6, Qi = an0 d, N t∗ =(cid:12) (cid:12){j ∈[K]:A
t,j
=I
j
=1,A
t′,j
=0, ∀t′ ∈S(t)}(cid:12) (cid:12),
7 2 4 6 7 0
FW ig7i u= re21 2− ).ϵ T+ hξ u4i s+ Wξ 6i i+ ∼ξ 7i Nu (n 1de ,r 3σQ 2I )( us ne de et rhe Qillu as ntr dat Wion
i
i ∼n and N t′ ≜ ⟨A
ρ(t)
⊕ A t,I⟩. It is clear that for many cases
N(1 −ϵ,3σ2) und7 er Q . 2 0 7 of A ρ(t) and A t, this inequality is not tight and could even
2 I be quite loose. Therefore, the design of a loss sequence that
3) WhenA =1forsomet′ ∈S(t)andA =0forr ∈
t′,i r,i tightens this inequality constitutes a good future research
S(t)withr >t′,thedistributionsofY conditionedon
t,i direction to possibly obtain a tighter lower bound.
Y are both N(Y ,cσ2) under Q and Q , where
S(t) t′,i 0 I
(cid:12) (cid:12)
c = (cid:12)S(t)∪{t}\{0,...,t′}(cid:12). Then ∆(Y t,i|Y S(t)) = 0.
IV. ALGORITHMFORBANDITFEEDBACKAND
For example, suppose t = 7, A = 1 and A = 0,
then we have Wi = Y +ξi
+ξ4i
i (see the
illu6 si
tration
SEMI-BANDITFEEDBACK
7 4i 6 7
in Figure 2) and thus Wi ∼N(Y ,2σ2) when Y has In this section, we will introduce our algorithms for the
7 4i 4i
been observed. two types of feedback. We will use the batched algorithm to
restrict the number of switches between actions by dividing
Therefore, by setting
thewholetimehorizonintobatchesandforcingthealgorithm
to play the same action for all the rounds within a batch as
N t∗ =(cid:12) (cid:12){j ∈[K]:A t,j =I j =1,A t′,j =0, ∀t′ ∈S(t)}(cid:12) (cid:12), shown in Figure 3.
we have A. Algorithm for Bandit Feedback
The Exp2 with John’s exploration algorithm [9] is an
∆(Y t|Y S(t)) efficientalgorithmforthecombinatorialbanditproblemunder
I banditfeedback.InAlgorithm3,weintroducearefinementof
=(cid:88) Q 0(N t∗ =i)id KL(cid:0) N(0,|S(t)|σ2)(cid:13) (cid:13)N(ϵ,|S(t)|σ2)(cid:1) thisalgorithm,calledBATCHED-EXP2withJohn’sexploration
i=1 to take into account switching costs, where John’s exploration
(cid:88)I iϵ2 distribution can be obtained in [9, § 7.3.2]. In this section,
= Q (N∗ =i) .
0 t 2|S(t)|σ2 we prove the following theorem, which is a formal version of
i=1 Theorem 3, to obtain a bound for the regret of the proposed
algorithm.
Weobservethat{N∗ =i}istheeventthattheplayerswitched
t8
Algorithm3:BATCHED-EXP2withJohn’sexploration Algorithm 4: BATCHED-BROAD
Input : John’s exploration distribution µ over A; Define: F (a)= 1 (cid:80)K ln 1.
batch lengths B 1,...,B N s.t.
(cid:80)N
i=1B n =T; Input : η
1n
=η;
ηn i=1 ai
mixing coeff. γ∈(0,1) and learning rate η; batch lengths B ,...,B s.t. (cid:80)N B =T;
q 1 =( |A1 |,..., |A1 |)∈R|A|. n=1,N 0 =0.1 N i=1 n
for 1≤n≤N do for β =0,1,... do
(a) Let p n = (1−γ)q n +γµ, and select a combi- a′ n =argmin a∈Co(A)F 1(a).
natorial arm A(n) with respect to p n. Pull the while n≤N do
selected combinatorial arm (cid:10)for B n time (cid:11)s, which 1) a n = argmin a∈Co(A){D Fn(a,a′ n)}, where
then incurs a loss X(n) = A(n), l(n) , where D is defined as in (21).
l(n) = (cid:80) l(n,b) and l(n,b) is the loss 2) SaF mn ple A(n) such that E[A(n)] = a and
b∈[Bn] n
vector at the b-th time due to the pulling of A(n) then pull it for B times. Observe A(n) ◦
n
in this batch. l(n,b) for b∈[B ] and incur a loss
n
(b) Estimate the loss vector l(n) by ˜l(n) =
X(n)Σ+ A(n), with Σ = E [AAT] X(n)=⟨A(n), l(n)⟩,
n−1 n−1 A∼pn
(c) w Uh pe dr ae teΣ t+ n h− e1 ei xs pt oh ne enp ts ie au ld wo- ei in gv he tr ss .e To hf atΣ in s− ,1 f. or all w beh ie nr ge thl( en l) os= s ve(cid:80) ctb o∈ r[B an t] bl -( tn h, tb i) mewi oth f pl u(n lli, nb g)
A∈A, A(n) in this batch.
q
(A)exp(cid:0) −η⟨A,˜l(n)⟩(cid:1) 3) Compute the estimator ˆl(n) with
n
q (A)= .
n+1 (cid:80)
A′∈Aq
n(A′)exp(cid:0) −η⟨A′,˜l(n)⟩(cid:1)
(cid:0)ˆl(n)(cid:1) =
(cid:0) A(n)(cid:1) i(cid:0) l(n)(cid:1)
i,
end i (a )
n i
for i∈[K], where we use (v) to denote the
i
ith component in v.
Theorem 9. Let π be the policy of BATCHED-EXP2 with 4) Update
John’s exploration distribution. The time horizon T is di-
a′ =argmin⟨a,ˆl(n)⟩+D (a,a′ ).
(cid:108)vi λde
2
3d K−in
1
3to
T1
3N I−b
1
3a (cid:109)tc fh oe rs 1w ≤ith n≤len (cid:4)g BTth (cid:5)s as na dtis Bfy Nin =g TB (cid:114)−n (= N−B 1)B=
if (cid:80)n
s=Nβn ++ 11
∥A(a n∈ )c ◦o(A l()
n)∥2
2
≥ K 3l ηn 2T
F tn
hen
n
with N = (cid:4)T(cid:5) +1. Let γ = ηBIK and η = ln(K I) . η n+1 ←η n/2, N β+1 ←n, n←n+1;
B 3NK(BI)2 break;
Then for any adversary l 1:T ∈ L, the λ-switching regret end
satisfies η ←η . n←n+1.
n+1 n
R λ(π,l 1:T)=O(cid:0) (λK)1 3T2 3I4 3(cid:1) . end
end
Proof of Theorem 9: By definition, we have l(n) =
(cid:80)Bn
l ∈ [0,B]. Since A(n) ∈ A, the accumulated
b=1 (n−1)B+b
loss in each batch satisfies
B. Algorithm for Semi-Bandit Feedback
(cid:10) (cid:11)
X(n)= A(n),l(n) ≤BI.
We propose the BATCHED-BROAD algorithm as stated
(cid:113)
By[9,Theorem7.6],whenγ =ηBIK andη = ln|A| , in Algorithm 4, based on the BROAD algorithm in [10],
3NK(BI)2
the pseudo-regret satisfies which is an Online Mirror Descent algorithm with log-barrier
regularizer. For a regularizer F :RK →R, define
(cid:112)
R(π,l )≤2BI 3NKln|A|
1:T
(cid:114) eK D (p,q)≜F(p)−F(q)−⟨∇F(q), p−q⟩. (21)
≤4λ2 3K−1 3T1 3I−1 3I 6λ−2 3K1 3T32I1 3KIln F
I
(cid:114) eK In the following theorem, we first prove BATCHED-BROAD
=4 6ln λ1 3K1 3T2 3I4 3. can achieve a λ-switching regret as shown in Theorem 4.
I
Thus Theorem10. Letπ bethepolicyof BATCHED-BROAD.The
time horizon T is divided into N batches with lengths satis-
R λ(π,l 1:T)≤R (cid:114)T(π,l 1:T)+λIN fying B n =B =(cid:106) (TI)31λ2 3K−1 3 +1(cid:107) , for n=1,...,N−1
≤4 6lneK λ1 3K1 3T32I4 3 +2λ1 3K1 3T2 3I34 and B N = T − (cid:80) nN =− 11B n with N = (cid:4) BT(cid:5) + 1. Let
I η =min{ 1 , 1 }. Then for any adversary l ∈L, when
(cid:114) 18IB2 81 1:T
=(cid:16) 4 6lneK +2(cid:17) λ1 3K1 3T2 3I4 3. T ≥ IK λ2 the λ-switching regret satisfies
I
R λ(π,l 1:T)=O˜(cid:0) (λK)1 3(TI)32 +KI(cid:1) .9
In the following lemma, we first give the generalized anal- thesetwoalgorithmstheBATCHED-HYBRIDandBATCHED-
ysis of BROAD algorithm for losses in varying intervals over NEGENTROPY, respectively.
time. Based on the lemma, we can then prove Theorem 10. Sincetheoptimaladversarialadversaryisdifficulttodesign,
we use the CIN loss sequence and CDN loss sequence given
Lemma 11. Consider a general combinatorial multi-armed
by Algorithms 1 and 2 in §III for bandit and semi-bandit
bandit problem with semi-bandit feedback where l ∈[0,b ]K
t t feedback, respectively. Besides the lower-bound traces, we
and A = {A ∈ {0,1}K : ∥A∥ = I}. Let the agent’s policy
1 also design a stochastically constrained (SC) adversary which
π be obtained by Algorithm 4 with batch lengths B =1 for
n is very similar to that used in [15]. Specifically, the time
all n ≤ N. If for all t ≤ T, η ≤ min{ 1 , 1 }, then the
t 18Ib2 81 horizon of length T is split into phases:
pseudo-regret satisfies t
(cid:118) 1,2,...,t ,t +1,...,t ,...,t ,...,T,
(cid:32)(cid:117) (cid:117) (cid:88)T (cid:33) (cid:124) (cid:123)(cid:122) (cid:125)1 (cid:124)1 (cid:123)(cid:122) (cid:125)2 (cid:124)n−1 (cid:123)(cid:122) (cid:125)
R(π,l 1:T)=O (cid:116)(KIlnT) b2
t
+KIlnT T1 T2 Tn
t=1 where the length of phase i is T = ⌊1.6i⌋. The loss for
i
Proof: For the combinatorial arm A t pulled at time t and each arm i at time t is set to be an independent Bernoulli
the loss vector l t ∈[0,b t]K at time t, we have distribution with mean
(cid:88) (cid:40)
∥A ◦l ∥2 = l2 ≤Ib2. 1−αˇλ if i≤I
t t 2 t,i t µ =
ti
i:At,i=1 1 else
Then we have η ∥A ◦l ∥2 ≤ 1 and thus by [10, Theorem
t t t 2 18 if t belongs to an odd phase and
8], we have
(cid:40)
(cid:118) 0 if i≤I
(cid:32) (cid:34)(cid:117) T (cid:35)(cid:33) µ =
R(π,l )=O E (cid:117) (cid:116)(KlnT)(cid:88) ∥A ◦l ∥2+KIlnT , ti αˇλ else
1:T t t 2
t=1 otherwise. In the above setting, λ is the switching cost. We
where the expectation is taken over the randomized choice of denotetheSCadversarywithparameterαˇbySC(αˇ)adversary.
A t. Since ∥A t◦l t∥2
2
≤Ib2 t, the proof is completed. Notethatthemeanoftheoptimalarmoscillatesbetweenbeing
Proof of Theorem 10: In n-th batch, the accumulated close to 1 and close to 0 to create a challenging environment
loss vector l(n) ∈ [0,B n]K. By Lemma 11, when T ≥ IK
λ2
for our bandit algorithms.
the pseudo-regret satisfies
(cid:118)
(cid:32)(cid:117) N (cid:33) A. Bandit Feedback
(cid:117) (cid:88)
R(π,l )=O (cid:116)(KIlnT) B2 +KIlnT
1:T n Weusetwotypesofadversariestocomparetheperformance
n=1 of BATCHED-EXP2 with John’s exploration and BATCHED-
(cid:18)(cid:113) (cid:19)
≤O (2KIlnT)T(TI)1 3λ2 3K−1 3 +KIlnT EXP3 algor (cid:108)ithm. The batch (cid:109)length in the algorithms is fixed
to be B = 3λ32K− 31(TI)1 3 .
(cid:18)√ (cid:19)
=O 2lnT(λK)1 3(TI)2 3 +KIlnT . First, we use the lower-bound trace CIN adversary that
we designed in Algorithm 1 with σ = 10 and ϵ =
9log T
2
Since N ≤ T +1, the λ-switching regret satisfies 10(λK)1 3(IT)−1 3. From Figure 4a, we observe that the λ-
(TI)1 3λ32 K−1 3 switc9 hlo ing 2gT
regret of BATCHED-EXP2 with John’s exploration
R λ(π,l 1:T)≤R(π √,l 1:T)+λIN is much smaller than that of BATCHED-EXP3 when K =10,
=O( lnT(λK)31(TI)2 3 +KIlnT). I = 3, λ = 1. From Figure 4b, we observe that the λ-
switching regret of BATCHED-EXP2 with John’s exploration
is much smaller than that of BATCHED-EXP3 for a smaller
valueofλ=0.1,showingthatevenwhentheswitchingcostis
V. NUMERICALRESULTS
small, our algorithm outperforms the benchmark significantly.
In this section, we present the numerical results to compare In Figure 4e, we compare the λ-switching regret for different
our algorithms BATCHED-EXP2 with John’s exploration in values of I when K = 20, T = 10000 and λ = 1. It
Algorithm3andBATCHD-BROADinAlgorithm4withsome is observed that the regret grows as I0.304; the dependence
baselines from the literature after adding batches in which appears to be loose with respect to the upper bound of I4 3 in
the played combinatorial arm does not change within each Theorem 9 and suggests that the BATCHED-EXP2 algorithm
batch. For bandit feedback, our baseline algorithm is the workswellundertheCINlosssequence.Also,weobservethat
EXP3 algorithm [14] and we modified it to be a batched I0.304 isalsolooseintermsoflowerboundI2 3 inTheorem1,
algorithm called BATCHED-EXP3. For the semi-bandit feed- which can be explained by the following statement. Under
back,wechoosetheFollow-the-Regularized-Leaderalgorithm the CIN loss sequence L and the policy πExp2 of the
√ 1:T
with hybrid regularizer [15] F(a) =
(cid:80)K
i=1− a
i
+ γ(1 − BATCHED-EXP2 algorithm, we delineate two reasons that
a )log(1−a ) and the unnormalized negentropy potential [1] explain why the λ-switching regret R (πExp2,L ) is not
i i λ 1:T
F(a) = (cid:80)K i=1(cid:0) a ilna
i
− a i(cid:1) . We call the modification of lower bounded by Ω(I2 3).10
(a) K =10, I =3 and λ=1 under bandit (b)K =10,I =3andλ=0.1underbandit (c) K =10, I =3 and λ=1 under bandit
feedback using the CIN loss sequence. feedback using the CIN loss sequence. feedback using the SC(0.01) adversary.
(d)K =10,I =3andλ=0.1underbandit (e) K = 20, T = 10000 and λ = 1 under (f) K = 30, T = 10000 and I = 3 under
feedback using the SC(0.01) adversary. banditfeedbackusingtheCINlosssequence. banditfeedbackusingtheCINlosssequence.
Fig. 4: Comparison of the performance of different algorithms under Bandit Feedback
1) Let V be the set of all stochastic loss sequences V that the exponents of λ are not too far from each other is
1:T
(V ∈ [0,1]K is a random vector). Given the combina- reassuring.
t
torial arm size K, the switching cost λ and the time The second trace we used is the SC(0.01) adversary. From
horizon T, we have Figure 4c and Figure 4d, we observe that the λ-switching
regrets of BATCHED-EXP2 with John’s exploration are both
inf sup R (π,V )
π∈ΠV1:T∈V λ 1:T smaller than those of BATCHED-EXP3 when K = 10, I = 3
≥ inf sup R λ(π,l 1:T)≥Ω(I2 3), and λ = 1, and K = 10, I = 3 and λ = 0.1, respectively.
π∈Πl1:T∈L This again corroborates the efficacy of our proposed methods.
where the last inequality holds due to Theorem 1.
It is not comparable between R (πExp2,L ) and
λ 1:T
inf π∈Πsup V1:T∈VR λ(π,V 1:T). Thus R λ(πExp2,L 1:T) B. Semi-bandit Feedback
may not be Ω(I32).
We compare BATCHED-BROAD, BATCHED-HYBRID
2) Given the combinatorial arm size K, the switching
and BATCHED-NEGENTROPY algorithm under the lower-
cost λ and the time horizon T, by Lemma 5, for any
bound trace CDN adversary that we designed in Algorithm 2
deterministic player π, we have
with σ = 10 and ϵ=
10(λk)1 3I−2 3T−1
3 and the SC(0.005)
R λ(π,L 1:T)≥Ω(I32), adversary.9 Tlo hg e2T
batch length
of9 tlo hg e2T
algorithms is fixed to be
(cid:108) (cid:109)
under the CIN loss sequence L 1:T. Since the policy B = 3λ32K−1 3T31I2 3 .
πExp2 of the BATCHED-EXP2 algorithm is stochastic, Under the CDN adversary, we have the following results.
R λ(πExp2,L 1:T) may not be Ω(I32). From Figure 5a, we observe that the λ-switching regret of
In Figure 4f, we compare the λ-switching regret for different BATCHED-BROAD is much smaller than that of BATCHED-
values of λ when K = 30, T = 10000, and I = 3. It HYBRID and BATCHED-NEGENTROPY when K = 10,
is observed that the regret grows as λ0.379. Note that our I = 3, λ = 1. From Figure 5b, we observe that the λ-
theoretical results in Theorems 1 and 9 say that the expected switching regret of BATCHED-BROAD is much smaller than
λ-switching regret scales as Θ(λ1/3) when T, K and I are that of BATCHED-HYBRID and BATCHED-NEGENTROPY
fixed. Even though the empirical observation of the regret for a smaller value λ = 0.1, showing that even when
scalingasλ0.379cannotbedirectlycomparedtothetheoretical the switching cost is small, our algorithm outperforms the
resultofλ1/3 because,amongotherreasons,thelosssequence benchmark significantly. In Figure 5e, we compare the λ-
constructed here is, in fact, stochastically constrained, the fact switching regret for different values of I when K = 40,11
(a) K = 10, I = 3 and λ = 1 under (b) K = 10, I = 3 and λ = 0.1 under (c) K = 10, I = 3 and λ = 1 under
semi-bandit feedback using the CDN loss semi-bandit feedback using the CDN loss semi-bandit feedback using the SC(0.005)
sequence. sequence. adversary.
(d) K = 10, I = 3 and λ = 0.1 under (e) K = 40, T = 10000 and λ = 1 (f) K = 30, T = 10000 and I = 3 under
semi-bandit feedback using the SC(0.005) under semi-bandit feedback using the CDN semi-bandit feedback using the CDN loss
adversary. loss sequence. sequence.
Fig. 5: Comparison of the performances of different algorithms under Semi-bandit Feedback
T = 10000 and λ = 1. It is observed that the regret grows thoseinvolvedintheshortestpathproblem,rankingproblems,
as I0.163; the dependence appears to be loose with respect to andmultitaskproblemscanalsobeconsideredwhenswitching
the upper bound of I2 3 in Theorem 10 and suggests that the costs are involved.
BATCHED-BROADalgorithmworkswellundertheCDNloss
sequence.Also,weobservethatI0.163 isalsolooseintermsof
APPENDIX
lower bound I31 in Theorem 2, which is possible by a similar
A. Proof of Lemma 5
reasoning as that for bandit feedback in §V-A. In Figure 5f,
we compare the λ-switching regret for different values of λ GiventheconstructedstochasticlosssequenceL 1:T defined
when K =30, T =10000, and I =3. It is observed that the inAlgorithm1,wenowwanttoanalyzetheplayer’sexpected
regret grows as λ0.356, which is again close to λ1/3 as given regret under an arbitrary deterministic policy π which yields
by our theoretical result in Theorems 2 and 10. an action sequence A 1:T ∈AT so that A t is a function of the
Under the SC(0.005) adversary, from Figure 5c and Fig- player’s past observations X 1:t−1 with X t = ⟨A t,L t⟩. First
ure 5d, we observe that the λ-switching regrets of BATCHED- we define
BROAD are both smaller than that of BATCHED-HYBRID T T T
(cid:88) (cid:88) (cid:88)
and BATCHED-NEGENTROPY when K = 10, I = 3 and R≜ ⟨A ,L ⟩+λ d(A ,A )−min ⟨A,L ⟩.
t t t t−1 t
λ=1, and K =10, I =3 and λ=0.1, respectively. A∈A
t=1 t=1 t=1
(22)
VI. CONCLUSION
We also define the regret with respect to the unclipped
We derived lower bounds for the minimax regret for the stochasticlossfunctionsL˜ definedinAlgorithm1underthe
1:T
problem of adversarial combinatorial bandit with a switch-
same deterministic policy π which yields an action sequence
ing cost λ for each changed arm in each round. We also A˜ ∈ AT so that A˜ is a function of the player’s past
1:T t
designed algorithms that operate in batches to approach the observations Y with Y =⟨A˜ ,L˜ ⟩. Let
1:t−1 t t t
lower bounds. Our findings provide insights into the inherent
difficulty of the problem and suggest efficient approaches to
R˜
≜(cid:88)T
⟨A˜ ,L˜
⟩+λ(cid:88)T
d(A˜ ,A˜
)−min(cid:88)T
⟨A,L˜ ⟩.
minimize switching costs and optimize the overall perfor- t t t t−1 t
A∈A
mance in terms of regret. Further research involves deriving t=1 t=1 t=1
tighter bounds in both directions for both bandit and semi- Then E[R] = R (π,L ) where the expectation in E[R]
λ 1:T
banditfeedback.Also,othersetsofcombinatorialarmssuchas is taken over the adversary’s randomized choice of the loss12
sequences L , and R (π,L ) is defined in (1). The next Then using the concavity of the squared root function and by
1:T λ 1:T
lemmashowsthatinexpectation,theregretE[R]canbelower (cid:80) M˜ =2(cid:0)K−1(cid:1) M˜, it holds that
I∈A I I−1
bounded in terms of E[R˜] (the expectation in E[R˜] is taken
over the adversary’s randomized choice of the loss sequences
1 (cid:88) dF˜
(Q ,Q )
L˜ 1:T).
(cid:0)K I(cid:1)
I∈A
TV 0 I
Lemma 12. Assume that T ≥ max{λ IK,6}. Then E[R] ≥ ≤ 2σϵ √ I(cid:112) log 2T +1(cid:88) (cid:0)K1 (cid:1)(cid:113) E Q0[M˜ I]
E[R˜]− ϵTI. I∈A I
4 (cid:118)
(cid:117) (cid:34) (cid:35)
first
sP hr oo wof: thW ate Pco (n Bs )id ≥er 1th −e even ϵt B
.
F= or{∀ δt =: L t =
ϵ
L˜ ,t} b, ya [n 5d
,
≤ 2σϵ √ I(cid:112) log 2T +1(cid:117) (cid:116)E Q0 (cid:0)K1 (cid:1) (cid:88) M˜ I
4(λ+ϵ) 4(λ+ϵ) I I∈A
Lemma 1] we have that with probability at least 1−δ, (cid:115)
ϵ (log T +1)E [M˜]
= √ 2 Q0 .
(cid:114) (cid:114) 2σ K
T 4T(λ+ϵ)
|W |≤σ 2d(ρ)ln ≤2σ log T log ,
t δ 2 2 ϵ
Lemma 14. It holds that
for all t ∈ [T], where the last inequality holds due to E[R˜]≥ϵTI(cid:16) 1− I (cid:17) − ϵTI (cid:88) dF˜ (Q ,Q )+λE[M˜].
d(ρ) ≤ log T + 1 by [5, Lemma 2]. Thus, setting σ = K (cid:0)K(cid:1) TV 0 I
2 I I∈A
1 , we obtain that
(cid:113)
6 log 2Tlog 2 4T(λ ϵ+ϵ) roundP sro tho ef: plF ao yr era pny icki s∈ arm[K i] i, nl te ht eT ai ctd ioe nno st ee quth ene cn eu Am ˜ber
.
So of
(cid:16) (cid:17) 1:T
we can write R˜ = ϵ TI −(cid:80) χ T +λM˜, where we
(cid:16) 1 (cid:104)1 5(cid:105)(cid:17) i∈[K] i i
P ∀t∈[T], 2 +W t ∈ 6, 6 ≥1−δ. use χ i to denote the i-th component of χ. Also, we use I i to
denote the i-th component of I ∈ A in the following. Since
χ ∈ A is selected uniformly at random in Algorithm 1, we
For T ≥max(λ IK,6), we have ϵ< 1 6 and thus L˜ t(x)∈[0,1] have
forallx∈[K]whenever1/2+W ∈[1,5].Thisimpliesthat
P(B)≥1−δ. t 6 6 E[R˜]= (cid:0)K1
(cid:1)
(cid:88) E(cid:104) ϵ(cid:16) TI− (cid:88) I iT i(cid:17) +λM˜(cid:12) (cid:12) (cid:12)χ=I(cid:105)
If B occurs then R = R˜; otherwise, R˜ −R ≤ (λ+ϵ)TI I I∈A i∈[K]
since R,R˜ ∈[0,(λ+ϵ)TI]. Therefore, =ϵTI− ϵ (cid:88) (cid:88) I E (cid:2) T (cid:3) +λE[M˜].
(cid:0)K(cid:1) i QI i
I I∈Ai∈[K]
E[R˜]−E[R]=E[R˜−R|¬B]·P(¬B)≤ ϵTI . For all i ∈ [K] and t ∈ [T], the event {A˜ t,i = 1} belongs to
4 the σ-field F˜ by Remark 1 (A˜ is used to denote the i-th
x,i
component of A˜ ), so we have
x
Q (A˜ =1)−Q (A˜ =1)≤dF˜ (Q ,Q ).
Let Q and Q follow previous definition in (9) and (10), I t,i 0 t,i TV 0 I
0 I
F˜ be the σ-algebra generated by X defined in (8) and Summing over t∈[T] yields
1:T
dF˜ (Q ,Q ) follow the definition in (11). M˜ and M˜ are
TV 0 I I E (cid:2) T (cid:3) −E (cid:2) T (cid:3) ≤TdF˜ (Q ,Q ).
defined in (13) and (14), respectively. Then we have the QI i Q0 i TV 0 I
following lemma that bounds total variation from above.
Summing over i∈[K] such that I =1 yields
i
Remark 1. Note that A˜ t is a deterministic function of its past (cid:88) I (cid:16) E (cid:2) T (cid:3) −E (cid:2) T (cid:3)(cid:17) ≤TIdF˜ (Q ,Q ).
observations Y 1:t−1; thus the σ-algebra generated by A˜
1:T
is i QI i Q0 i TV 0 I
a subset of F˜. i∈[K]
Summing over I ∈A yields
Lemma 13. It holds that
(cid:88) (cid:88) I (cid:16) E (cid:2) T (cid:3) −E (cid:2) T (cid:3)(cid:17) ≤TI (cid:88) dF˜ (Q ,Q ).
i QI i Q0 i TV 0 I
1 (cid:88) dF˜ (Q ,Q )≤ √ϵ (cid:113) (log T +1)E [M˜]. I∈Ai∈[K] I∈A
(cid:0)K(cid:1) TV 0 I σ 2K 2 Q0 Thus
I I∈A
(cid:88) (cid:88) I E (cid:2) T (cid:3)
i QI i
I∈Ai∈[K]
Proof: By [5, Lemma 2], the width w(ρ) ≤ log T +1.
Then by Lemma 6, we have 2 ≤ (cid:88) (cid:88) I iE Q0(cid:2) T i(cid:3) +TI (cid:88) dF T˜ V(Q 0,Q I)
I∈Ai∈[K] I∈A
(cid:18) (cid:19)
dF T˜ V(Q 0,Q I)≤ 2σϵ √ I(cid:113) (log 2T +1)E Q0[M˜ I]. = K I−− 11 TI+TI I(cid:88) ∈AdF T˜ V(Q 0,Q I).13
Therefore, where the first inequality is due to K ≥3I. Combining λ≥ϵ
E[R˜]≥ϵTI− (cid:0)Kϵ (cid:1)(cid:16)(cid:18) K I−− 11(cid:19) TI+TI (cid:88) dF T˜ V(Q 0,Q I)(cid:17) a ϵn ≤d ( λ25 ≤), Twe ang det Tλ ≥≥ 8,T 2− t0 h2 ea rn igd htth he an ndϵ s≥ ide2T 3 o9− f2 (. 2T 4)hu iss lw owhe en r
1 2
I I∈A boundedby (λK)3(TI)3.Therefore,foranyK ≥3I andT ≥
+λE[M˜] 130log 2T
max{λK,8}, it holds that
=ϵTI(cid:16) 1− I (cid:17) − ϵTI (cid:88) dF˜ (Q ,Q )+λE[M˜]. I
K (cid:0)K I(cid:1)
I∈A
TV 0 I
E[R]≥
(λK)1 3(TI)32
(26)
130log T
2
Proof of Lemma 5: We first prove Lemma 5 for deter- For any general algorithm that has an arbitrary number of
ministic policies that make no more than S 0 = ϵT λI switches. switches, we can turn it to a new algorithm that makes at
For algorithms with this property, we have most S switches by halting the algorithm once it makes S
0 0
Q (M˜ >ϵTI)=Q (M˜ >ϵTI)=0. switchesandrepeatingthelastactionintheremainingrounds.
0 I The regret R∗ of the new algorithm (as defined in (22) under
As the event {M˜ ≥m} is in F˜ by Remark 1, then new algorithm) equals R when M ≤S and when M >S ,
0 0
E [M˜]−E [M˜]=
(cid:88)S0 (cid:16)
Q (M˜ ≥m)−Q (M˜
≥m)(cid:17) R∗ ≤R+ϵTI ≤2R,
Q0 QI 0 I
m=1 since R ≥ λS . Thus E[R∗] ≤ 2E[R]. Since E[R∗] is
0
ϵTI
≤ dF˜ (Q ,Q ). lower bounded by the right-hand side of (26), this implies
λ TV 0 I
the claimed lower bound on the expected regret of any
Then deterministic player.
E [M˜]−E[M˜]=
1 (cid:88)(cid:16)
E [M˜]−E
[M˜](cid:17)
Q0 (cid:0)K(cid:1) Q0 QI
I I∈A B. Proof of Lemma 7
≤ λϵ (cid:0)T KI (cid:1) (cid:88) dF T˜ V(Q 0,Q I). (23) GiventheconstructedstochasticlosssequenceL 1:T defined
I I∈A inAlgorithm2,wenowwanttoanalyzetheplayer’sexpected
Combining (23) with Lemma 12 and Lemma 14, we obtain regret under an arbitrary deterministic policy π which yields
an action sequence A ∈AT so that A is a function of the
(cid:16) I 1(cid:17) 2ϵTI (cid:88) 1:T t
E[R]≥ϵTI 1− K − 4 − (cid:0)K(cid:1) dF TV(Q 0,Q I) player’spastobservationsZ 1:t−1 withZ t =A t◦L t.Following
I I∈A the definition in (22) for R, we analyze the expected regret
+λE [M˜]. E[R] in the new semi-bandit feedback setting and CDN loss
Q0
sequence L given in Algorithm 2. Let Q and Q follow
1:T 0 I
By Lemma 13, and log 2T +1≤2log 2T, we have previous definition in (9) and (10). Let F be the σ-algebra
(cid:0)K1 (cid:1) (cid:88) dF T˜ V(Q 0,Q I)≤ σ√ϵ K(cid:113) (log 2T)E Q0[M˜]. g toe tn ae lr vat ae rd iatb iy onth de F To Vbs ie srv da et fiio nn es dZ a1 s:T in, w (1h 1e )re wZ itt h= reL spt e◦ cA t t to. T thh ee
I I⊂[K] σ-algebraF.ForY definedin(19)of§III-Bandtheaction
0:T
(cid:113) sequence A , we define
Using the notation m= E [M˜] and when K ≥3I, 1:T
Q0
T T T
E[R]≥
5
ϵTI−
2ϵ √2TI(cid:112)
log Tm+λm2,
R′ ≜(cid:88) ⟨A t,L˜ t⟩+λ(cid:88) d(A t,A˜ t−1)−min(cid:88) ⟨A,L˜ t⟩.
12 σ K 2 t=1 t=1 A∈A t=1
wher√e the right hand side is minimized when m = Remark 2. Note that A
t
is a deterministic function of its past
ϵ2TI √log 2T . Thus the right-hand side is lower bounded observations Z 1:t−1, thus the σ-algebra generated by A 1:T is
byλσ 5ϵTK I − ϵ4T2I2log 2T. Using our choice of σ = a subset of F.
12 λσ2K
1 and ϵ=
(λK)1 3(IT)−31
, we derive
Lemma 15. It holds that
6(cid:113) log 2Tlog 2 4T(λ ϵ+ϵ) 9log 2T 1 (cid:88) dF (Q ,Q )≤ √ϵ (cid:112) I(log T +1)E [M].
E[R]≥ 5(λK)1 3(TI)2 3 − 4(λK)1 3(TI)2 3 log 2(4T(λ ϵ+ϵ)) .
(cid:0)K I(cid:1)
I⊂[K]
TV 0 I
σ 2K
2 Q0
108log T 93(log T)2
2 2 Proof: By Lemma 8, Pinsker’s inequality [13, Lemma
(24)
11.6.1] and w(ρ)≤log T +1, we have
2
If λ < ϵ, the right hand side of (24) is lower bounded by
(λK 30) l1 3 og(T TI)2 3 under the assumption that T ≥ 8. Otherwise, if dF TV′ (Q 0,Q I)≤ 2ϵ σ(cid:112) (log 2T +1)E Q0[M I],
ϵ≤λ≤2 T, we have
where F′ is the σ-algebra generated by Y (defined in (19)
ϵ=
(λK)31(IT)−1
3 ≥
(3λ)1/3
≥
(λ)1/3
, (25) of §III-B). Since Z 1:T is a function of Y
0:T0: ,T
we have F ⊂F′
9log 2T 9(log 2T)T1/3 7T4/3 which implies dF TV(Q 0,Q I) ≤ 2ϵ σ(cid:112) (log 2T +1)E Q0[M I].14
Then using the concavity of the squared root function and by Summing over t∈[T] yields
(cid:80) I⊂[K]M I =2(cid:0)K I−− 11(cid:1) M, it holds that E (cid:2) T (cid:3) −E (cid:2) T (cid:3) ≤TdF (Q ,Q ).
QI i Q0 i TV 0 I
1 (cid:88)
dF (Q ,Q ) Summing over i∈[K] such that I =1 yields
(cid:0)K(cid:1) TV 0 I i
I I⊂[K] (cid:88) I (cid:16) E (cid:2) T (cid:3) −E (cid:2) T (cid:3)(cid:17) ≤TIdF (Q ,Q ).
≤ ϵ (cid:112) log T +1 (cid:88) 1 (cid:112) E [M ] i QI i Q0 i TV 0 I
2σ 2 (cid:0)K(cid:1) Q0 I i∈[K]
I⊂[K] I
Summing over I ∈A yields
(cid:118) (cid:117) (cid:34) (cid:35)
≤ 2ϵ σ(cid:112) log 2T +1(cid:117) (cid:116)E Q0 (cid:0)K1 (cid:1) (cid:88) M I (cid:88) (cid:88) I i(cid:16) E QI(cid:2) T i(cid:3) −E Q0(cid:2) T i(cid:3)(cid:17) ≤TI (cid:88) dF TV(Q 0,Q I).
I I⊂[K] I∈Ai∈[K] I∈A
ϵ (cid:112)
= √ I(log T +1)E [M]. Thus
σ 2K
2 Q0
(cid:88) (cid:88) I E (cid:2) T (cid:3)
i QI i
I∈Ai∈[K]
Lemma 16. Assume that T ≥ max{λ IK 2 ,I 6}. Then E[R] ≥ ≤ (cid:88) (cid:88) I E (cid:2) T (cid:3) +TI (cid:88) dF (Q ,Q )
E[R′]− ϵTI. i Q0 i TV 0 I
6 I∈Ai∈[K] I∈A
first sP hr oo wof: thW ate Pc (o Bns )id ≥er 5t /h 6e .e Fv oe rnt δB == TI{ ≤∀t 1 6: ,L byt = [5,L˜ Lt} e, ma mnd a =(cid:18) K I−− 11(cid:19) TI+TI (cid:88) dF TV(Q 0,Q I)
1] we have that with probability at least 5, for all t∈[T] and I∈A
6
i∈[K], Therefore,
(cid:114) (cid:32)(cid:18) (cid:19) (cid:33)
TI ϵ K−1 (cid:88)
|W ti|≤σ 2d(ρ)log 2 δ ≤3σlog 2T, E[R′]≥ϵTI− (cid:0)K(cid:1) I−1 TI+TI dF TV(Q 0,Q I)
I I∈A
where the last inequality is due to d(ρ) ≤ log T +1 by [5, +λE[M]
2
Lemma 2]. Thus, setting σ = 9lo1 gT we obtain that =ϵTI(cid:16) 1− I (cid:17) − ϵTI (cid:88) dF (Q ,Q )+λE[M]
K (cid:0)K(cid:1) TV 0 I
(cid:16) 1 1 5 (cid:17) 5
P ∀t∈[T],i∈[K], +Wi ∈[ , ] ≥ . I I∈A
2 t 6 6 6
For T ≥max(λK,6), we have ϵ< 1 and thus L˜ (x)∈[0,1] Proof of Lemma 7: We first prove the theorem for
I2 6 t
for all x ∈ [K] whenever 21 +W t ∈ [1 6,5 6]. This implies that deterministic players that make no more than S 0 = ϵTI/λ
P(B)≥ 5. switches. For algorithms with this property, we have
6
If B occurs, R = R′; otherwise, λM ≤ R ≤ R′ ≤ λM + (cid:16) ϵTI(cid:17) (cid:16) ϵTI(cid:17)
ϵTI, so that R′−R≤ϵTI. Therefore, Q 0 M > λ =Q I M > λ =0.
E[R′]−E[R]=E[R′−R|¬B]·P(¬B)≤ ϵTI . By Remark 2, the event {M ≥m}∈F which implies
6
ϵTI/λ
E [M]−E [M]= (cid:88) (cid:0) Q (M ≥m)−Q (M ≥m)(cid:1)
Q0 QI 0 I
Lemma 17. It holds that m=1
ϵTI
E[R′]≥ϵTI(cid:16)
1−
I (cid:17)
−
ϵTI (cid:88)
dF (Q ,Q )+λE[M]
≤
λ
dF TV(Q 0,Q I).
K (cid:0)K(cid:1) TV 0 I
I I∈A Then
Proof:Foranyi∈[K],letT denotethenumberoftimes 1 (cid:88)(cid:16) (cid:17)
i E [M]−E[M]= E [M]−E [M]
the player picks arm i when the time horizon is T. So we can Q0 (cid:0)K(cid:1) Q0 QI
write R′ = ϵ(cid:16) TI −(cid:80) i∈[K]χ iT i(cid:17) +λM, where χ
i
is the i-
≤
ϵI TII∈ (cid:88)A
dF (Q ,Q )
th component of χ. We also use I i is the i-th component of λ(cid:0)K(cid:1) TV 0 I
I ∈A. Thus I I∈A
1 (cid:88) (cid:104) (cid:16) (cid:88) (cid:17) (cid:12) (cid:105) Combining this with Lemma 16 and Lemma 17, we obtain
E[R′]= E ϵ TI− I T +λM(cid:12)χ=I
(cid:0)K(cid:1) i i (cid:12) (cid:16) I 1(cid:17) 2ϵTI (cid:88)
=ϵTI I−I∈A ϵ (cid:88) (cid:88)i I∈[K E] (cid:2) T (cid:3) +λE[M]. E[R]≥ϵ 1− K − 6 − (cid:0)K I(cid:1) I∈AdF TV(Q 0,Q I)
(cid:0)K(cid:1) i QI i +λE [M].
I I∈Ai∈[K] Q0
For all i ∈ [K] and t ∈ [T], the event {A
t,i
= 1} belongs to By Corollary 15, and log 2T +1≤2log 2T,
the σ-field F by Remark 2, so 1 (cid:88) ϵ (cid:112)
dF (Q ,Q )≤ √ I(log T)E [M].
Q I(A
t,i
=1)−Q 0(A
t,i
=1)≤dF TV(Q 0,Q I).
(cid:0)K I(cid:1)
I⊂[K]
TV 0 I
σ K
2 Q015
Using the notation
m=(cid:112)E
Q0[M] and when K ≥3I, [15] J.Zimmert,H.Luo,andC.-Y.Wei,“Beatingstochasticandadversarial
semi-banditsoptimallyandsimultaneously,”inInternationalConference
E[R]≥ ϵTI − 2ϵ2 √TI3/2 (cid:112) log Tm+λm2, onMachineLearning. PMLR,2019,pp.7683–7692.
2 σ K 2
where √the right hand side is minimized when m =
ϵ2TI3/2 log T
√ 2 . Thus the right hand side is lower bounded
λσ K
by ϵTI − ϵ4T2I3log 2T. Using our choice of σ = 1 and
2 λσ2K 9log T
2
ϵ=
(λK)1 3I−2 3T−31
, gives
9log T
2
E[R]≥
(λKI)1 3T2
3 . (27)
30log T
2
For any general algorithm that has an arbitrary number of
switches, we can turn it to a new algorithm that makes at
most S switches by halting the algorithm once it makes S
0 0
switchesandrepeatingthelastactionintheremainingrounds. Yanyan Dong received her B.S. degree from Jilin University in 2017, and
The regret R∗ of the new algorithm equals R when M ≤S Ph.D.degreefromTheChineseUniversityofHongKong,Shenzhenin2022.
0
She was a Research Fellow at the National University of Singapore from
and when M >S ,
0 Dec. 2022 to Dec. 2023. Her research interests include information theory,
R∗ ≤R+ϵTI ≤2R, codingtheory,networkcoding,andmachinelearning.
since R ≥ λS . Thus E[R∗] ≤ 2E[R]. Since E[R∗] is
0
lower bounded by the right-hand side of (27), this implies
the claimed lower bound on the expected regret of any
deterministic player.
REFERENCES
[1] T. Lattimore and C. Szepesva´ri, Bandit Algorithms. Cambridge
UniversityPress,2020.
[2] S.GuhaandK.Munagala,“Multi-armedbanditswithmetricswitching
costs,” in Automata, Languages and Programming: 36th International
Colloquium, ICALP 2009, Rhodes, Greece, July 5-12, 2009, Proceed-
ings,PartII36. SpringerBerlinHeidelberg,2009,pp.496–507.
[3] M. Shi, X. Lin, and L. Jiao, “Power-of-2-arms for bandit learning
withswitchingcosts,”inProceedingsoftheTwenty-ThirdInternational
Vincent Y. F. Tan (Senior Member, IEEE) was born in Singapore in 1981.
Symposium on Theory, Algorithmic Foundations, and Protocol Design
He received the B.A. and M.Eng. degrees in electrical and information
forMobileNetworksandMobileComputing,2022,pp.131–140.
sciencefromCambridgeUniversityin2005,andthePh.D.degreeinelectrical
[4] R.Arora,O.Dekel,andA.Tewari,“Onlinebanditlearningagainstan
engineeringandcomputerscience(EECS)fromtheMassachusettsInstituteof
adaptiveadversary:Fromregrettopolicyregret,”inProceedingsofthe
Technology(MIT)in2011.HeiscurrentlyaProfessorwiththeDepartment
29th International Coference on International Conference on Machine
ofMathematicsandtheDepartmentofElectricalandComputerEngineering
Learning. PMLR,2012,pp.1747–1754.
(ECE),NationalUniversityofSingapore(NUS).Hisresearchinterestsinclude
[5] O.Dekel,J.Ding,T.Koren,andY.Peres,“Banditswithswitchingcosts:
informationtheory,machinelearning,andstatisticalsignalprocessing.
T2/3 regret,” in Proceedings of the 46th Annual ACM Symposium on
Dr. Tan is an elected member of the IEEE Information Theory Society
TheoryofComputing,2014,pp.459–467.
Board of Governors. He was an IEEE Information Theory Society Distin-
[6] C.Rouyer,Y.Seldin,andN.Cesa-Bianchi,“Analgorithmforstochastic
guished Lecturer from 2018 to 2019. He received the MIT EECS Jin-Au
and adversarial bandits with switching costs,” in International Confer-
Kong Outstanding Doctoral Thesis Prize in 2011, the NUS Young Investi-
enceonMachineLearning. PMLR,2021,pp.9127–9135.
gator Award in 2014, the Singapore National Research Foundation (NRF)
[7] P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire, “The non-
Fellowship(Classof2018),andtheNUSYoungResearcherAwardin2019.
stochastic multiarmed bandit problem,” SIAM Journal on Computing,
He is currently serving as a Senior Area Editor for the IEEE Transactions
vol.32,no.1,pp.48–77,2002.
on Signal Processing and as an Associate Editor in Machine Learning and
[8] N. Cesa-Bianchi and G. Lugosi, Prediction, Learning, and Games.
StatisticsfortheIEEETransactionsonInformationTheory.Healsoregularly
Cambridgeuniversitypress,2006.
servesasanAreaChairofprominentmachinelearningconferencessuchas
[9] S.Bubeck,“Introductiontoonlineoptimization,”Lecturenotes,vol.2,
the International Conference on Learning Representations (ICLR) and the
pp.1–86,2011.
ConferenceonNeuralInformationProcessingSystems(NeurIPS).
[10] C.-Y. Wei and H. Luo, “More adaptive algorithms for adversarial
bandits,”inConferenceOnLearningTheory. PMLR,2018,pp.1263–
1291.
[11] R.Combes,M.SadeghTalebi,A.Proutiere,andM.Lelarge,“Combi-
natorial bandits revisited,” Advances in Neural Information Processing
Systems,vol.28,2015.
[12] A.C.-C.Yao,“Probabilisticcomputations:Towardaunifiedmeasureof
complexity,” in 18th Annual Symposium on Foundations of Computer
Science(sfcs1977). IEEEComputerSociety,1977,pp.222–227.
[13] T. M. Cover, Elements of Information Theory. John Wiley & Sons,
1999.
[14] J.-Y.Audibert,S.Bubeck,andG.Lugosi,“Regretinonlinecombinato-
rialoptimization,”MathematicsofOperationsResearch,vol.39,no.1,
pp.31–45,2014.