ROBUSTLY ESTIMATING HETEROGENEITY IN FACTORIAL DATA USING
RASHOMON PARTITIONS
APARAJITHAN VENKATESWARAN§, ANIRUDH SANKAR‡, ARUN G. CHANDRASEKHAR‡,⋆,
AND TYLER H. MCCORMICK§,¶
Abstract. Many statistical analyses, in both observational data and randomized control trials,
ask: howdoestheoutcomeofinterestvarywithcombinationsofobservablecovariates? Forexam-
ple, how do various drug combinations affect health outcomes, or how does technology adoption
dependonincentivesanddemographics? Ourgoalistopartitionthisfactorialspaceinto“pools” of
covariatecombinationswheretheoutcomediffersacrossthepools(butnotwithinapool). Existing
approaches for identifying such partitions either (i) search for a single “optimal” partition under
someassumptionsabouttheassociationbetweencovariatesor(ii)attempttosamplefromtheentire
setofpossiblepartitions. Boththeseapproachesignoretherealitythat,especiallywithcorrelation
structure in covariates, many ways to partition the covariate space may be indistinguishable from
astatisticalperspective,despiteverydifferentimplicationsforpolicyorscience. Wedevelopanal-
ternativeperspective,calledRashomonPartitionSets (RPSs). EachitemintheRPSpartitionsthe
factorial space of covariates using a tree-like geometry. RPSs incorporate all partitions that have
posterior values near that of the maximum a posteriori partition, even if they offer substantively
very different explanations, and do so using a prior that makes no assumptions about the associa-
tions between covariates. This prior is the ℓ prior, which we show is minimax optimal. Given the
0
RPSwecancalculatetheposteriorofany measurablefunctionofthevectoroffeaturecombination
effects on outcomes, conditional on being in the RPS. We also characterize approximation error
relativetotheentireposteriorandprovideboundsonthesizeoftheRPS.Simulationexperiments
demonstrate the usefulness of this framework and that it allows for robust conclusions relative to
conventional regularization techniques. We apply our method to three empirical settings: price
effects on charitable giving, heterogeneity in chromosomal structure (telomere length), and the in-
troduction of microfinance. We highlight robust conclusions, including affirmations and reversals
of extant literature’s findings, in each setting.
We gratefully acknowledge Abhijit Banerjee, Emily Breza, Kevin Chen, Paul Goldsmith-Pinkham, Rachel Heath,
Muriel Niederle, Cynthia Rudin, and Bo Zhang for their helpful discussions. We thank Garrett Allen and Jessica
Kunke for their feedback on earlier versions of our paper. We thank Brian Xu for exceptional research assistance.
§Department of Statistics, University of Washington.
‡Department of Economics, Stanford University.
⋆J-PAL, NBER.
¶Department of Sociology, University of Washington.
1
4202
rpA
2
]EM.tats[
1v14120.4042:viXraRASHOMON PARITITONS FOR HETEROGENEITY 2
1. Introduction
“You didn’t come here to make the choice, you’ve already made it. You’re here to
try to understand why you made it. I thought you’d have figured that out by now.”
— The Oracle, The Matrix Reloaded
Researchers and policymakers often study settings where an outcome of interest varies with
combinations of features or covariates (e.g., characteristics, treatment assignments) of a given unit.
Examples include (1) learning what combination of drugs, at what frequency and dosages, and for
what sub-groups in the population reduce a given illness (e.g., in the cases of HIV and non-small
cell lung cancer, Hammer et al. (1997); Cascorbi (2012); Nair et al. (2023)); (2) studying how an
individual’s wage is associated with combinations of age, education, parental wealth, race/ethnicity,
and gender (Mincer, 1958; Aakvik et al., 2010; Forster et al., 2021); (3) analyzing vaccination
campaigns that leverage incentives, reminders, network strategies across the wealth distribution
(Chernozhukov et al., 2018; Banerjee et al., 2021), and; (4) determining when and why microfinance
is more effective for certain classes of sub-populations and markets (Banerjee et al., 2019).
Fundamentally, the researcher wants to learn a response function that describes how the outcome
changes (or doesn’t) when moving between levels of a feature. If the outcome is very sensitive to
certain changes in feature combinations, the response function needs to be very local, meaning in
practice that the researcher needs to test between many similar feature combinations. In contrast, a
simplerresponsefunctionmeansthatseveralcombinationscaneffectivelybethoughtofasthesame,
which can also be useful for theorizing about mechanisms and policy. For example, will anyone who
has previous entrepreneurial experience and has had an active savings account put a micro-loan
to business use? Will the effectiveness of drug combinations follow the same pattern across the
patient weight distribution? Intuitively, if a patient is on a drug cocktail with drugs A and B and
the marginal effect of incrementally increasing the dosing of A from 1 to 2 is zero (holding fixed
the dosing of B and the patient’s weight), then one can think of combining, or pooling, treatments
with 1 and 2 units of drug A (holding fixed B and the patient’s weight). This teaches us the local
shape of the treatment effect function, which is interesting in its own right, but also increases the
effective amount of data to estimate this pooled average treatment effect.
Of course, the researcher doesn’t know the local complexity of the response function a priori and
needs to learn it while also estimating the effect of specific feature combinations. This problem has
a “factorial” structure, though: even with 3 values per feature and 3 features, there are 27 distinct
combinations. Thiscanputpressureontheeffectivenumberofobservationsperparameter,meaning
thattheanalysiscanbepotentiallyverynoisy. Additionally,duetointerdependence,decisionsaboutRASHOMON PARITITONS FOR HETEROGENEITY 3
pooling some feature combinations impact others. Finally, so many feature combinations could in
principle be pooled with each other that feasibility and multiple hypothesis concerns quickly arise;
the number of possible partitions is given by Bell’s number B = O((n/log(n+1))n), so in our
n
example B > 1020.
27
The enormous space of feature combinations and interdependence make the process of producing
a model for heterogeneity fragile. Different pooling models can both (a) explain the data nearly
equally well and yet (b) correspond to very different shapes of treatment effect functions. The
latter may also correspond to different interpretations as the differing models may treat different
combinations as poolable or not. As there will be distinct models of heterogeneity that nearly
equally well explain a given data set, it is entirely possible that the different pooling models can
lead to very distinct scientific interpretations, counterfactuals, and policy decisions.
Existing approaches to this problem either (i) search for a single “optimal” model under some
assumptions about the association between covariates (e.g., Lasso) or (ii) attempt to sample from
theentire setofpossiblemodels(e.g., BayesianModelAveraging, BMA).Ourapproachoffersanew
alternative using the idea of Rashomon Effect from Breiman (2001)’s “Two Cultures” paper (and
the highly related Occam’s Window approach from Madigan and Raftery (1994)’s seminal paper on
model uncertainty in graphical models using BMA) to enumerate and explore a small number of
high (posterior) probability models, which we call the Rashomon Partition Set (RPS) because each
item in the RPS partitions the factorial space of covariates using a tree-like geometry.
We show a number of results that give this seemingly simple idea profound implications. In a
Bayesian framework, we propose a set of novel priors over explanations for heterogeneity and then
show that the RPS is enumerable. We bound the difference between posterior quantities computed
using the entire posterior and using only the RPS. The result is that conclusions using RPS are
robust since it incorporates all partitions that are near the maximum a posteriori, even if they offer
substantively very different explanations. When constructing this posterior, we take the view that
marginal effects in a factorial setting are complex to uniformly model and use a prior that makes
no assumptions about the associations between covariates: the ℓ prior, which we show is minimax
0
optimal. Inthatsense,theRPSisrobustbecauseitusesarobustpriorand itenumerateseveryhigh
posteriormodel. Forexperiments,policymakerscanthenweighadditionalconsiderations(e.g.,cost,
equity, privacy) in choosing which policies from the RPS to implement. RPSs also yield insights to
generate new scientific theories. Looking across models in the RPS, one can build an archetype of
featurecombinationsthatappearconsistentlyandhaveconsistenteffectsontheoutcome,regardless
of the structure imposed on other covariates by other high posterior partitions.RASHOMON PARITITONS FOR HETEROGENEITY 4
In the remainder of this section, we first provide a detailed example to illustrate the environment
and fix ideas. Then, with this environment in mind, we preview the results and direction for the
remainder of the paper. To begin with the example, suppose that researchers are interested in the
health outcome of a possible drug combination on individuals. The two drugs in question are A
and B and each can take on three values: 0 (not taken), 1 (low dosage), and 2 (high dosage). Drug
effects can vary by weight – for the sake of discussion, say weights are low, medium, or high. Thus,
there are 27 distinct bundles of heterogeneity and it’s conceivable that for each different weight, a
different drug cocktail (or equivalence class thereof) is most effective.1 Let M be the number of
features (here 3) and R the number of values per feature (here 3). There are K := RM unique
feature combinations. Let w ∈ {low,medium,high} be weight, (a,b) ∈ {0,1,2}2 be the dosages of
A and B, and Y (a,b,w) be the health potential outcome for individual i, with E[Y (0,0,w)] = 0
i i
for all w as a normalization. The sample consists of n individuals, the n-vector of outcomes y, the
matrix of features X , the matrix D = D(X) with entries D = 1 if i has unique feature
1:n,1:M ik
combination k = (a ,b ,w ). The dataset is Z := (y,X). So, the researcher studies
i i i
(1) y = Dβ+ϵ,
where β = E[Y | D = 1] is the expected outcome in the population given the feature combination
k i ik
and ϵ is some idiosyncratic mean-zero residual.2
i
Inthisillustration, letusmakethreeassumptionsastohowdrugsmayoperateinthepopulation.
First,supposethatlowandmediumweightpeoplefacethesameeffectsacrossalldrugcombinations,
and only at the highest weight do the effects behave differently. This means that
(1) E[Y (a,b,low)] = E[Y (a,b,medium)] for every (a,b), and
i i
(2) E[Y (a,b,low)] ̸= E[Y (a,b,high)] for some (a,b).
i i
1We will use this example to build intuition, but the applications are well beyond health. For instance, we could
imagineanapplicationtothestudyofmicrofinance. Amicrocreditorganizationentersmarketsandrandomlyvaries
interestrateswithbaseline,lowdiscount,andhighdiscountlevels(KarlanandZinman,2010)tounderstanddemand.
Themarketsitentersmayhavezero,low,orhighamountsofpre-existingdebt. Credit-eligibleindividualsmayhave
no, low, or high levels of previous entrepreneurship. These potential future borrowers of the microcredit loans may
themselvesvaryintheireducationlevels. Learningwhetherpreviousentrepreneurialexperience,inthickversusthin
creditmarkets,foradverselyversusnotselectedagentsgivesinsightsintothefundamentaleconomicmechanics. One
of our three empirical examples in Section 7 concerns microfinance.
2This representation was developed in prior work in Banerjee et al. (2021) (Chandrasekhar and Sankar are co-
authors)inthecontextoffactorialrandomizedcontrolledtrials. Ourworkinthepresentpaper–asettingofgeneral
heterogeneity (not only cross-randomized trials), the study of robustness, development of the RPS, usage of robust
priors, and tree-based estimations circumventing correlation restrictions – is all entirely novel to the literature and
faces new technical challenges.RASHOMON PARITITONS FOR HETEROGENEITY 5
E[Y ] = 1
i
E[Y ] = −1
i
(2,2,w)
(2,2,w)
(1,2,w) (2,1,w)
(1,2,w) (2,1,w)
(1,1,w)
(1,1,w)
E[Y ] = 1
i
(a) w ∈{low,medium} (b) w ∈{high}
Figure 1. Hasse diagram representing the partition Π⋆ for the drug-weight inter-
action example.
Second, suppose that at the low and medium weight levels, the best outcome is delivered by
a cocktail of low doses of drug A, but overdosing is possible at these lesser weights: the maximal
dosingsof drug Aare too strongper pound andinteraction effects can bequite severe. For instance,
(1) E[Y (1,b,w)] = 1 for b ∈ {1,2}, w ∈ {low,medium}, and
i
(2) E[Y (2,b,w)] = −1 (i.e., worse than control) for b ∈ {1,2}, w ∈ {low,medium}, and
i
(3) E[Y (a,b,w)] = 0 for (a,b) ∈ {(0,1),(1,0),(0,2),(2,0)} and w ∈ {low,medium}, so single
i
drug treatments are ineffective.
Third, for the highest weight individuals, suppose that there is no overdosing in the dosage
levels in the support of the study (1 or 2 for each drug and combinations therein): beyond the
minimum therapeutic doses, all combination cocktails are equally effective. So E[Y (a,b,high)] = 1
i
for (a,b) ∈ {(1,1),(1,2),(2,1),(2,2)}. All single-drug therapies are ineffective. These assumptions
present a sketch of drug interaction patterns across weight classes. In practice, we would not know
these three rules a priori. Testing all possible factorial combinations could reveal these patterns
with enough data, but even in this small example, there are 27 scenarios so statistical power will
reduce quickly.
Instead,wewouldliketopooltogetherdatatoaggregateinformationandmoreefficientlyestimate
marginaleffects. Apartition Π,inthespaceofallpartitioningmodels,P,isamodelofheterogeneity
wherein every feature combination is assigned to a pool π ∈ Π, possibly a singleton, where β = β
k k′
if k,k′ ∈ π. These pools are referred to as leaves in the regression tree literature. In our example,
there is a single (maximal) pooling scheme, which we can call Π⋆. In Π⋆, all single-dose therapies
are pooled with control because they are not effective. Low and medium weight individuals are
fully pooled, but the drug combinations themselves are not, with {(1,1),(1,2)} being effective andRASHOMON PARITITONS FOR HETEROGENEITY 6
the others being detrimental. High weight individuals are split from the lower weight groups, and
all combination therapies are pooled. We visualize this in Figure 1. If we knew this, we could
double the data per cell in the low and medium weight samples and then quadruple the data in
the high weight sample between all cocktail treatments {(1,1),(1,2),(2,1),(2,2)}. Further, Π⋆ has
direct scientific value teaching us about the underlying scientific mechanisms. However, obviously,
we know none of this ex-ante.
In principle, any estimation strategy corresponds to some partition. A “saturated regression,”
treating all 27 cells as independent, corresponds to the most granular pooling (which is degenerate
as no cells are pooled), where there is no information across cells to be gleaned – Πˆ such that
every pool π ∈ Πˆ consists of a unique feature combination: π = {(a,b,w)}. Conducting a “short”
regression of outcomes on level of drug A and level of drug B in a linear regression can be thought
of as a pooling corresponding to additive separability with no interaction effects. Here Π˜ is such
that each π ∈ Π˜ consists of π = {(a,b,·)}.3 One could also use machine learning to regularize and
select the best pooling structure, leading to Π˘ (Banerjee et al., 2021). The important point is that
the specific approaches to handling data necessarily correspond to implicit partitions, e.g., Πˆ, Π˜,
and Π˘ here. The implicit partitions assumed can very much drive results and, therefore, policy,
counterfactuals, and interpretation of mechanisms and development of theory.
Returning to our Bayesian framework, we study the posterior given the data Z, P(Π | Z), and
thentheposteriordistributionoverβ thatfollows. Wewanttousearobustprior: onethatdoesnot
impose false independence or unwarranted assumptions on correlations on the relationship between
the β ’s. Given the complexity, it is unclear and perhaps unlikely that a medical researcher can
k
neatlydescribetheirprioronthejointdistributionofmarginaleffectsofthe27featurecombinations
in our drug cocktail and weight example. Thus, we use a prior that is robust to any correlational
structure. InTheorem2,weprovethatthiscorrespondstoaprioroverthenumberofuniquefeature
combinations that have distinct effects on the outcome, which is an ℓ penalty. Stated differently,
0
P(Π) ∝ P(H(Π))whereH countsthenumberofdistinctfeaturecombinationsafterallpoolingsi.e.,
the number of pools in the partition Π. This prior places no structure on the independence (e.g., ℓ )
1
or correlation structure (e.g., Gaussian processes with structured sparsity) within the partitioning,
and it is not obvious that doing so would be reasonable from a scientific perspective. For instance,
drugs may need to be combined in certain ratios to be optimally effective, or economic production
functions may exhibit strong complementarities in inputs. Our key motivation is those cases where
at the time of research the structure of these dependencies is not yet discovered, motivating the
3Infact,evenanadditivespecificationoflevelsofdrugsA,B,andweightfallsintothistrapinourgeneralizationin
Section 8. It pools cells implicitly using common slopes.RASHOMON PARITITONS FOR HETEROGENEITY 7
study in the first place.4 This also imposes no specific distributional structure on β, in contrast
with other methods.5
We next represent our problem using a specific type of tree known as a Hasse diagram (see
Figure 1). Several of the present paper’s authors first developed this representation in Banerjee
et al. (2021), which we extend considerably here in a different statistical context. This geometry
naturally handles partially ordered sets, unlike typical regression trees that impose a false hierarchy
between variables. We use these Hasse diagrams to construct partitions. After restricting ourselves
to a subset that meets our admissibility criteria, which we denote P⋆ ⊂ P, we enumerate all
partitions that have a posterior probability above some threshold θ of our choosing. To see the
value of this tree-based geometry, consider two feature combination variants, k = (a,b,w) and
k′ = (a+1,b,w) in our drug-by-weight example. We calculate the incremental change in our loss
function(orequivalently,theposterior)ifweweretoassumethetwofeatureshadthesameexpected
outcome. Thelowertheposteriorthatk andk′ belonginthesamepool,thelargertheloss. Further,
with the ℓ prior, the posterior for a tree of pooling decisions is lower when more splitting is done:
0
the only assumption is that there are not too many sources of meaningful heterogeneity, though one
is silent on their correlations.
With this statistical and geometric setup in place, we have the language to define the RPS.
Definition1(RashomonPartitionSet(RPS)). Forsomeposteriorprobabilitythresholdθ, wedefine
the Rashomon Partition Set P as,
θ
(2) P = {Π ∈ P⋆ : P(Π | Z) ≥ θ}.
θ
Further, given a posterior over the partition models – and in practice one that only specifies a
prior over how many effectively distinct feature combinations there are (or equivalently the number
ofpools)–wecaneasilydevelopaposteriorovertheeffectsofvariousfeaturecombinations,possibly
pooled, on the outcome of interest conditional on the partition models in this set. So
(cid:88)
(3) P(β | Z,P ) = P(β | Z,Π)P(Π | Z,P ),
θ θ
Π∈P
θ
4Of course, if the researcher had strong prior knowledge, one could freely incorporate this for a specific application.
Howeverthispriorknowledgewouldbetailoredfortheapplication,andthereisnotaone-size-fits-allparameterization
for all conceptual restrictions in all domains.
5E.g., ℓ implies independent Laplace random variables and Gaussian processes are intrinsically parametric.
1RASHOMON PARITITONS FOR HETEROGENEITY 8
and analogously for measurable functions of β. Using Hasse diagrams avoids imposing an artificial
hierarchy on the partially ordered set of covariates, which allows us to interpret the partitions in
the RPS.6
The posterior for β restricted to the RPS, Equation (3), is, of course not the same as the dis-
tribution over all possible partitions, P (β). Computing P (β) requires sampling from the
β|Z β|Z
distribution of admissible Hasse diagrams, which is computationally taxing and inefficient, since
evaluating the posterior at a partition with very low posterior probability requires just as much
effort as one with very high probability. Depending on the structure of the posterior and the θ of
theRPS,theposteriorwithintheRPSmaycomeclosetocapturingtheentiremassoftheposterior,
making the approximation using Equation (3) a reasonably proxy for using the full posterior.
This intuition brings us to the first of three main results in the remainder of the paper. First,
Theorem 1, characterizes the uniform approximation error of the posterior distribution of β, and
measurable functions of it, restricting to the RPS. Second, in Section 4, we show that the ℓ prior is
0
minimax optimal (Theorem 2). This prior is robust to any potential correlation structure between
covariates and assumes only sparsity in heterogeneity, without making statements about correlation
(or independence). This prior allows us to calculate bounds on the size of the RPS in Theorem
3. We characterize its size and its relationship to (a) the number of features, (b) the number of
values per feature, (c) the prior over H, the number of distinct pools in the partition, and (d) the
minimum posterior value θ. The main idea is that given θ, the prior dictates some threshold level
H(θ) with H(θ) = max H(Π). If there were more pools than this, then the prior would be
Π∈P
θ
so low that the partition would not make it into the RPS. So we can characterize the bound on
the size of P and, further, clearly study how this varies with the prior (and therefore the bound
θ
H(θ)). Third, in Section 5, we show that one can enumerate the entirety of P . In Algorithm 1, we
θ
develop a search process that enumerates the full RPS (Theorem 6). At any point in the search,
if one has a posterior about a potential partition that is sufficiently low such that no amount of
perfect matching down the road could make the posterior high enough to exceed θ, then one should
discard the entire collection of partitions that use this unlikely initial structure. This approach is
inspired by Xin et al. (2022).
ToconstructtheRPS,itisusefultostartwithareferencepartitionmodel. Thisissomereference
model that we can estimate by a machine learning algorithm that we deem sensible. This is useful
because θ of the RPS is the posterior of the reference model (which should already be high). The
RPSthenenumeratesallmodelswithposteriorsatleastashighasthisreference. Bydefinition, this
6WediscussthisdistinctioninmoredetailinSectionEinthecontextoffrequentisttree-basedmethodsthatusethe
same geometry.RASHOMON PARITITONS FOR HETEROGENEITY 9
willincludethemaximum a posteriori (MAP)partition. Giventhisset, onehasalistofmodelsthat
can be ordered by relative posterior value from the MAP downwards and one can take any decisions
querying this list. It is also helpful to note the computational and philosophical distinctions here.
Like in the Xin et al. (2022) approach, the reference model allows us to sidestep a computational
difficulty with the normalizing constant. But unlike Xin et al. (2022), in a Bayesian framework,
there is a natural interpretation and we know the MAP. Using the MAP as the reference, we can
interpret the size of the RPS as our tolerance for deviance from the MAP partition. The larger the
RPS, the more we’re willing to entertain high posterior probability models that are not the MAP.
Smaller RPSs hone in on explanations that are very similar, in terms of posterior probability, to
the MAP.
Alongwiththesetheoreticalresults,wealsoprovideseveralempiricalexamples. Section6reports
twosimulationstudies, thefirstacasewherethereisinterdependencebetweenfeaturecombinations
and the second illustrating the Rashomon Effect. We show results using three datasets in Section 7.
These examples highlight the interpretability of RPSs while also demonstrating how RPS generate
new scientific hypotheses. Looking for commonalities across partitions within the RPS provides
evidence for the robustness of conclusions across high posterior probability partitions. We also
demonstrate how RPSs can provide a pathway for robustly avoiding adverse events in experiments
by ensuring they are not present in the RPS.
We conclude the paper by discussing two generalizations to our work and then situating our
work with a discussion of related literature. Section 8 speaks to two possible extensions to our
work, one that allows for a broad class of heterogeneous effects functions (e.g. checking if there
is a linear increase in outcome with increasing a feature level) and the other that pools on the
space of covariances rather than on coefficients themselves. Given that this work is related to
several active areas of research, we have dedicated a section, Section 9, to contextualizing our
work. Section 10 concludes and offers areas for future work. All of our code is available at https:
//github.com/AparaV/rashomon-tva.
2. Environment
Suppose that there are n units (or individuals) and each has M features with the feature matrix
given by X. Every feature has R possible values, partially ordered. Let K be the set of all K = RM
unique feature combinations. Each feature combination k ∈ K can be represented in a dummy
matrix D with entries D = 1 if observation i has feature combination k. Depending on the
ik
context, we let k denote the vector of feature values or its index in K. Owing to the partial ordering
of the feature values, we can define a partial ordering on the feature combinations themselves. WeRASHOMON PARITITONS FOR HETEROGENEITY 10
say k ≥ k′ if and only if k ≥ k′ for all m = 1,...,M. We say k > k′ if k ≥ k′ but k ̸= k′, and
m m
say that k and k′ are incomparable if there are two features m and m such that k > k′ and
1 2 m1 m1
k < k′ . We denote incomparability by k ̸≶ k′. We denote the expected outcome of feature
m2 m2
combination k by β .
k
Wewillrestrictattentiontoasubsetofpartitionsthatisscientificallycoherent(admissible). Most
obviously, partitions should pool only those feature combinations with identical expected outcomes.
Recalling the Hasse diagrams representing the partial ordering of features, admissibility will restrict
the geometry of the partitions that appear on these diagrams. First, a pool of feature combinations
will considered inadmissible when these feature combinations are only in unrelated Hasse diagrams.
Second, a pool should have an interpretation as capturing (the lack of) marginal changes climbing
up or down the ordering. Finally, admissible partitions should be robust so that it doesn’t rely on
measure zero events on these climbs. We now explore these ideas in detail.
Definition 2 (Pool). Given M features taking on R partially ordered values each, a pool π is any
set of feature combinations having identical expected outcomes.
For a given pool π, two feature combinations k ,k ∈ π only if β = β . Note that the converse
1 2 k1 k2
is not true. That is, we could have k ∈ π and k ∈ π for π ̸= π even though β = β .
1 1 2 2 1 2 k1 k2
Definition 3 (Partition). Given M features taking on R partially ordered values each, a partition
Π is a partitioning of this feature space into pools.
Our goal is to learn partitions Π such that each element of a partition is a pool i.e., we want to
partition the feature space by heterogeneity in the expected outcomes.
Definition 4 (Variants). Two feature combinations k and k′ are variants if they have the same
value of features for all but one and they vary by exactly one intensity value (so they are orderable)
i.e., k = k′ for all except some feature m, and k′ takes a value that is the immediate next
−m −m m
incremental value after k .
m
At its core, heterogeneity boils down to whether variants have the same effect on the outcome of
interest, at least on a scale relevant to the researcher or policymaker.
Some feature combinations can be considered unrelated to one another. Let us take an ex-
ample, where researchers are exploring combinations of an antibiotic such as amoxicillin with an
anti-inflammatorydruglikeibuprofen. Itmakessensetoaskwhetheribuprofenhelpswhenitispre-
scribed alongside amoxicillin during a bacterial infection (e.g. whether it offers further temporary
relief from the pain) or, when both are administered simultaneously, what happens when dosagesRASHOMON PARITITONS FOR HETEROGENEITY 11
of both are increased (e.g., whether too strong a dose of both irritates the stomach). However, an
amoxicillin treatment on its own would not usually be compared in its efficacy to ibuprofen treat-
ment on its own – they are distinct drugs with distinct mechanisms. We will consider inadmissible
pools consisting solely of unrelated feature combinations (here single drug features). We mathe-
matically capture the (un)relatedness of feature combinations by first defining profiles in Definition
5.
Definition5(Profile). Aprofileρ(k)isabinaryvectorindicatingwhichoftheM featureshavenon-
zero values, with the understanding that in a factorial design experiment, this indicates assignment
to pure control and in a setting with heterogeneity we take (one of) the lowest value(s) as the base
of 0.
Returning to the example, if (a,b) captures dosages of amoxicillin and ibuprofen respectively,
then combinations such as k = (500 mg,100 mg) corresponds to the profile ρ(k) = (1,1), whereas
(500 mg,0) (amoxicillin on its own) corresponds to ρ(k) = (1,0) and (0,100 mg) (ibuprofen on its
own) corresponds to the profile ρ(k) = (0,1). An admissible pool π ∈ Π has to be contained within
a single profile or thread across other profiles through either the profile (0,0) or (1,1).
As part of admissibility, we want to interpret a pool π as the (lack of) marginal changes when
climbing up or down an ordering. Consider the profile where both amoxicillin and ibuprofen are
administered, and where amoxicillin is administered in dosages from the ordered set {250 mg,
500 mg} and ibuprofen is administered in dosages in the ordered set {200 mg, 400 mg}. This is
depicted on the Hasse in Figure 2. Beginning from the lowest dosages of both (250 mg, 200 mg),
we consider what happens when increasing the dosage of amoxicillin or ibuprofen. It could be that
increasing amoxicillin has no effect (e.g. because the bacterial infection was highly localized) but
increasing the dose of ibuprofen makes an appreciable difference (e.g., the patient feels much more
relief from the pain). This is captured in Figure 2a. Since we are asking about changes in marginal
values climbing up or down, admissible partitions pool contiguous features in the Hasse. In general,
admissible pools are “convex” shapes in the Hasse.
Beyondjustrespectingtheordering,wealsowantadmissiblepartitionstoavoidbrittlepoolsrely-
ing on measure zero events. Starting from the lowest dosage of an amoxicillin-ibuprofen treatment,
(250 mg, 200 mg), suppose that either increasing the dosage of just amoxicillin or the dosage of
just ibuprofen has an appreciable effect. This rules out Figure 2a. When amoxicillin and ibuprofen
are both raised to their largest dosages simultaneously, the effect would be a combination of each
drug’s individual dosage increase as well as an interaction effect between the two drugs. In a very
special case, this interaction effect can exactly offset the effect of one drug’s increase in dosage.RASHOMON PARITITONS FOR HETEROGENEITY 12
(500,400) (500,400) (500,400)
(250,400) (500,200) (250,400) (500,200) (250,400) (500,200)
(250,200) (250,200) (250,200)
(a) Admissible partition (b) Inadmissible partition (c) Admissible partition
Figure 2. Hasse diagrams for amoxicillin and ibuprofen example. To see why
Figure 2b fails Definition 6, consider π = {(250 mg,400 mg),(500 mg,400 mg)}
i
and π = {(500 mg,200 mg)} with incomparable minima (250 mg,400 mg) ̸≶
j
(500 mg,200 mg). This satisfies the antecedent but not the consequent of (3) (a).
For example, from a high 400 mg dose of ibuprofen, the benefits of a 250 mg dosage increase in
amoxicillin (from 250 mg) can be exactly offset by an equal amount of stomach irritation that it
causes so that 250 mg and 500 mg of amoxicillin have the exact same efficacy as 400 mg ibupro-
fen. Figure 2b captures this partition. However, this is a measure zero event. Almost surely, a
stomach irritation does not exactly wash out the effect of a 250 mg dosage increase in amoxicillin.
Any amount of estimation noise, which is inevitable, makes brittle pools like Figure 2b unreliable.
Since partitions are discrete objects, it makes sense to ignore any partition that may arise from such
measure zero events.7 In other words, an admissible partition needs to be robust to estimation noise
in the marginal increments. This would mean that the dosage combination (500 mg, 400 mg) has
to be in a distinct pool as in Figure 2c. Visually, this amounts to admissible partitions having to
be “parallel” in the Hasse. The top pool {(250 mg,400 mg),(500 mg,400 mg)} in Figure 2b is not
“parallel” to the other singleton pools below it. Figure 2a has two parallel pools, while Figure 2c
has 4 parallel (singleton) pools.
Motivated by traversing the partial ordering while discarding all measure zero partitions, we
present the formal definition of admissibility within a profile in Definition 6. We provide additional
technical details with examples in Appendix A.
Definition 6 (Admissible partition of a profile). A partition Π of a profile ρ is admissible if and
0 0
only if
(1) every π ∈ Π is a pool (cf. Definition 2),
0
7Other techniques such as Lasso or decision trees may falsely estimate a non-zero posterior probability for such
unrealistic partitions. We give examples of this in Appendix A.RASHOMON PARITITONS FOR HETEROGENEITY 13
(2) every π ∈ Π is strongly convex i.e., k,k′ ∈ π and k ≥ k′′ ≥ k′ implies k′′ ∈ π, and minπ
0
and maxπ both exist and are unique (and possibly equal to each other), and
(3) Π respects parallel splits, i.e., for every pair of distinct pools π ,π ∈ Π
0 i j 0
(a) if minπ ̸≶ minπ , then there exists a π′ ∈ Π such that minπ′ = p′, where for each
i j 0
feature m, p′ := max{p(i) ,p(j) } where p(i) = minπ and p(j) = minπ , and
m m m i j
(b) if maxπ ̸≶ maxπ , then there exists a π′′ ∈ Π such maxπ′′ = p′′, where for each
i j 0
feature m, p′′ := min{p˜(i) ,p˜(j) } where p˜(i) = maxπ and p˜(j) = maxπ .
m m m i j
Condition (1) is obvious. Condition (2) says pools must contain contiguous features. Condition
(3) is more technical but says that pools must be parallel on the Hasse.
Suppose m arms are active with R−1 non-control levels each. Then any admissible partition
for a profile can be represented by a binary matrix, Σ ∈ {0,1}m×(R−2). Each element of Σ tells us
whether a particular pair of adjacent levels in a feature is pooled. In particular, we define Σ = 1 if
ij
and only if feature combinations with level j is pooled with feature combinations with factor j +1
in feature i. We walk through some detailed examples of the setup in Appendix A.
Definition 6 captures admissibility within a single profile, but we also want to consider pooling
across profiles. For example, Definition 6 does not speak to the question of pooling decisions for
adding ibuprofen, as a temporary pain reliever, to a prescription of amoxicillin against a bacterial
infection. Does introducing ibuprofen make an appreciable difference (offering the patient relief
while waiting for the bacterial infection to work) or not (because the antibiotic itself offers pain
relief by attacking the root cause)? In order to reason about this, we consider partially ordering
of the profiles themselves using their binary representation in Definition 5. This also allows us to
embed the profiles in an M-d unit hypercube with profiles as the vertices. By the same intuition
behind convexity, we can pool two profiles if they are reachable on this hypercube. We formalize
this in Definition 7.
Definition 7 (Admissible partition). A partition Π of the entire feature space K is admissible if
and only if the following hold true:
(1) for every profile ρ , the partition induced by Π on ρ , Π = {π\{k | ρ(k) ̸= ρ } | π ∈ Π} is
0 0 0 0
admissible by Definition 6,
(2) every π ∈ Π is connected in feature levels across profiles i.e., if k ,k ∈ π such that ρ =
1 2 1
ρ(k ) and ρ = ρ(k ) are adjacent on the hypercube, then there are feature combinations
1 2 2
k′,k′ ∈ π such that ρ(k′) = ρ , ρ(k′) = ρ and ∥k′ −k′∥ = 1,8 and
1 2 1 1 2 2 1 2 1
8Along with (1), this means that we can reach k from k by traversing the Hasse for ρ to k′, then jumping to k′
2 1 1 1 2
along an edge on the M-d hypercube, and then moving from k′ to k′ while respecting the Hasse for ρ .
2 2RASHOMON PARITITONS FOR HETEROGENEITY 14
(3) every π ∈ Π is connected in profiles i.e., if π contains feature combinations from profiles
ρ and ρ where ρ < ρ , then π also contains features in profiles ρ ,...,ρ such that
0 k 0 k 1 k−1
∥ρ −ρ ∥ = 1 for i = 0,...,k−1.9
i i+1 0
Specifically,byallowingtopoolacrossdifferentprofiles,Definition7naturallyallowsustoexplore
heterogeneityintreatmenteffectswheretreatmentandcontrolaretwodistinctprofiles. Weillustrate
its usefulness in the empirical data analysis of microcredit access in Section 7.
Just like the Σ matrix within each profile, we can also construct the intersection matrix Σ∩ to
denote how features are pooled across two adjacent profiles. Consider partitions induced by Π on
two profiles ρ
1
and ρ 2. Let us call these Π 1,Π
2
respectively. Σ∩ = {0,1,∞}|Π1|×|Π2| where Σ∩
i,j
= 0
means that pools π ∈ Π and π ∈ Π are poolable according to (2) of Definition 7 but are not
i 1 j 2
pooled together in Π. Σ∩ = 1 means that these pools are poolable and are indeed pooled in Π.
i,j
Finally, Σ∩ = ∞ means that these pools are not poolable by Definition 7. Observe that if Σ∩ = 1,
i,j i,j
then Σ∩ = ∞ and Σ∩ = ∞ in order to respect (1) of Definition 7. This object will be useful in
i,−j −i,j
our enumeration step in Algorithm 1.
In the remainder of this paper, we will consider only admissible partitions and may refer to them
only as partitions (dropping the “admissible” quantifier) unless we need to distinguish them.
3. Rashomon Partitions
We elaborate on the statistical framework underlying the RPS. In Definition 1, we defined the
RPS as the set of partitions obtaining at least some posterior probability threshold θ. Now, we
obtain posteriors over the (functions of) effects of feature combinations. Given a unique partition
Π with some probability P(Π | Z), it may be useful to know the likely effects of using that specific
feature k ∈ π ∈ Π. This could be because, for instance, there may be scientific reasons to otherwise
prefer one versus the other, heterogeneity in costs, logistical considerations, etc. There is also a
statistical reason: the posterior may not be concentrated on just a few pools for some k but maybe
for others. Therefore, we may be interested in the posterior over the entire set of admissible pools
(cid:88)
P (β) = P (β | Π)·P(Π | Z),
β|Z β|Z
Π∈P⋆
where P is the distribution function of β | Z. Throughout our analysis, we will assume that
β|Z
P is a proper distribution i.e., it satisfies the Kolmogorov axioms. Our goal is to approximate
β|Z
9Along with (1) and (2), this means that we can reach ρ from ρ by traversing the M-d hypercube while staying
k 0
within π and respecting the Hasse at each vertex of the hypercube.RASHOMON PARITITONS FOR HETEROGENEITY 15
functions of P using only the RPS. That is,
β|Z
(cid:88) P(Π | Z)
P (β) = P (β | Π)·P(Π | Z,P ), P(Π | Z,P ) = ,
β|Z,P θ β|Z,P θ θ θ (cid:80) P(Π′ | Z)
Π∈P θ
Π′∈P
θ
meaning that the approximation only evaluates models in the RPS but is also normalized by the
RPS. The quality of this approximation, of course, depends on both the shape of the posterior (i.e.
how concentrated is the posterior around the highest probability models) and the structure of the
RPS. Our first goal, then, is to describe how well we can approximate P using the RPS. And
β|Z
then, wediscusshowtoconstructtheposterioroverpartitions, P(Π | Z), usinggeneralizedBayesian
inference. Technical details for results discussed here are deferred to Appendix B.
3.1. Posterior over effects. Consider the RPS, P . The Rashomon partitions allow for uniform
θ
approximation of the posterior over the effects vector β.
Theorem 1 (Rashomon approximation of posterior effects). Let f : RK → Rm be a measurable
function of the effects β, where K is the number of unique feature combinations and m ≥ 1. Then,
theposteriordistributionoff(β)overtheRashomonPartitionSetuniformlyapproximatestheentire
posterior of f(β) in the sense that
(cid:12) (cid:12) 1
sup(cid:12)F
β|Z,P
θ(t)−F β|Z(t)(cid:12) ≤
|P |θ
−|P θ|θ,
t θ
where F is the distribution function of the transformation f(β) | Z and F is the same but
β|Z β|Z,P
θ
conditioned on the RPS.
With small θ or large P , this tends to 0, meaning that the posterior approximation can be quite
θ
close to that calculated over the full support. Essentially this is saying that if we have enough
models of high enough posterior probability, then the error is low. This could arise as a result
of having a few very highly likely models or having many models that are only fairly likely. We
visualize the behavior of the 1/P θ term in our empirical data analyses in Section 7.
θ
Notice that setting f(β) = β recovers the posterior of β. f also covers other useful quantities
derived from the vector β. An obvious example is f(β) = max β since conditional on a given
k k
variant k being estimated as the one with the maximum effect. There is a winner’s curse since
the selection of the maximum is positively biased, so the bias needs to be corrected (the posterior
needs to be adjusted to have a lower mean) to undo this effect (Andrews et al., 2019). Other
examples include the variability over outcomes across the feature combinations ∥β−(cid:80) β /K∥2
K k 2
and quantiles of the expected outcome distribution.RASHOMON PARITITONS FOR HETEROGENEITY 16
(cid:80)
We now focus specifically on estimating the full posterior mean, E β = β P(Π | Z),
Π Π∈P⋆ Π
using the RPS. If we simply restricted ourselves to only models in the RPS, we would have
(cid:80)
E β = β P(Π | Z). As we discussed above, this quantity still depends on P(Π | Z),
Π,P θ Π∈P θ Π
which involves normalizing over all admissible models and, thus, remains computationally infeasi-
ble. For some priors on β, we could approximate P(Π | Z) but this requires specifying a prior on β
and a corresponding approximation with adequate accuracy (Appendix F.1 gives an example using
Gaussian priors and a Laplace approximation). More generally, the easiest quantity to compute is
the expectation by taking the mean of the effects weighted by the self-normalized posterior proba-
bilities as in Equation (4). Of course, if the RPS captures most of the posterior density, then this
method can validly approximate the posterior mean but that is not the goal of this estimator. This
estimator simply tells us what the effects are across the RPS.
(cid:88) P(Π | Z,P θ)
(4) E β = β .
Π|P θ Π(cid:80) P(Π′ | Z,P )
Π∈P θ
Π′∈P
θ
θ
This approach contrasts with Bayesian and frequentist methods based on resampling trees. When
resampling trees, under conditions where the mean is well-separated, we generally see the average
to have appealing asymptotic properties. In any finite sample, though, we also expect that there
will be several highly unappealing trees mixed in by chance. In our approach, in contrast, we look
explicitly for trees with the highest posterior probability, forgoing exploring the entire space to
instead focus on the partitions with the highest posterior probability. We can then characterize the
quality of this approximation for a given RPS construction.
Corollary 1. The mean conditional effect in Equation (4) approximates the posterior mean effect
restricted to the Rashomon set, E β, as
Π,P
θ
(cid:13) (cid:13)E Π|P θβ−E Π,P θβ(cid:13) (cid:13)
=
O(cid:18) 1 −1(cid:19)
.
∥E β∥ |P |θ
Π,P θ θ
If we further have that the effects are bounded like ∥β ∥ < ∞ for all Π ∈ P⋆, then mean conditional
Π
effect in Equation (4) approximates the posterior mean effect, E β, as
Π
(cid:18) (cid:19)
(cid:13) (cid:13) 1
(cid:13)E
Π|P
θβ−E Πβ(cid:13) = O
|P |θ
−|P θ|θ .
θ
Corollary 1 says that our approximation is highly dependant on both the Rashomon threshold
θ and the distribution of the models in the posterior space, which is indicated by |P |. Of course,
θ
this result naturally extends to functions of β as well. To better understand Corollary 1, first
assume that the models are uniformly distributed in the posterior probability space i.e., |P | ∝ 1
θRASHOMON PARITITONS FOR HETEROGENEITY 17
is independent of θ. As θ gets closer to 0, the RPS collects more admissible partitions. Therefore
|P |θ behaveslike1andwegetabetterapproximationusingEquation(4). Asθ getscloserto1,the
θ
RPSissparser. Therefore, |P |θ behaveslike0blowinguptheerrorinourapproximation. Now, we
θ
can consider a more complex model space where the models do not have a uniform posterior. Here,
if our models are clustered near the maximum a posteriori (MAP) model, then a large θ will in
fact give us a better approximation. Conversely, if models are clustered near a very small posterior
probability, then a large θ will blow up our approximation error
3.2. Posterioroverpartitions. Now,weturntotheproverbialelephantofconstructingatractable
posterior for Π, P(Π | Z) ∝ P(y | D,Π)·P(Π). We need to model the likelihood component and the
prior. Nested within P(y | D,Π) is a prior over β. In work on Bayesian tree models (e.g. Chipman
etal.(2010)),thetypicalstrategyistodefineaprioroverpartitionsandthendefineconjugatepriors
onβ andrelatedhyperparameterssothatitiseasytoevaluateeachdrawfromthedistributionover
trees. We take a different approach based on generalized Bayesian inference (Bissiri et al., 2016),
which requires specifying fewer distributions explicitly. However, in Appendix F.2, we also give an
example of a fully specified Bayesian model where maximizing the likelihood P(y | D,Π) is equiv-
alent to minimizing L(Π;Z), drawing a parallel with the previous work in the Bayesian literature.
Let exp{−L(Π;Z)} be the likelihood of Z where L(Π;Z) is the loss incurred by the partition Π.
Further, let exp{−λH(Π)} be the prior over P⋆. Then, we have
(5) P(Π | Z) ∝ exp{−L(Π;Z)}·exp{−λH(Π)} =: exp{−Q(Π)}.
Specifically, we use the mean-squared error for the loss function,
(cid:80)
(6) L(Π;Z) =
1 (cid:88) (cid:88)
(y −µ )2, µ =
k(i)∈πy i
.
i (cid:98)π (cid:98)π (cid:80)
n 1
π∈Πk(i)∈π k(i)∈π
For the prior, we take the view that, unless directed otherwise by the specifics of the science in the
context of the study, the researcher does not know the correlation structure between the various
possible pools. That is they do not have a strong view on whether β = β is correlated with
k k′
whetherβ = β . Therefore, wedefinetheprioroverthenumber of distinct pools i.e., H(Π) = |Π|,
k′′ k′
the size of the partition. The prior plays a regularizing role, putting more weight on less granular
aggregations. It corresponds to the ℓ penalty in a regression setting.
0
TheRPS,takentogetherwiththeℓ penalty,issimilarinspirittotheOccam’sWindowapproach
0
usedinthecontextofBayesianModelAveragingbyMadiganandRaftery(1994)andMadiganetal.
(1996). Thesepapersuseastochasticsearchoverthediscretespaceofmodelsthatultimatelyresults
in a set of high posterior models and discards more complicated models if simpler models are foundRASHOMON PARITITONS FOR HETEROGENEITY 18
to have higher posterior probability. Our approach, which does not do discrete model averaging,
formalizes this notion by including a prior with an ℓ penalty as part of the model, rather than
0
using it to guide the search. In Theorem 2 we show that this choice of prior is minimax optimal
and show how it lends to computational tractability in Section 4.
Now that we’ve developed language around describing the posterior probability of a partition
through a loss function, it is useful to characterize the Rashomon threshold in the loss space.
Specifically, we define θ := θ(q,ϵ) = q1+ϵcϵ where c := c(Z) is some scaling constant depending on
the observed data, q is the posterior probability of some good model, and ϵ is largest acceptable
deviation from ln(1/(qc)) i.e., the loss incurred by the good model. Then, the (q,ϵ)-Rashomon
Partition Set (RPS), P is defined as
q,ϵ
P = {Π ∈ P⋆ : P(Π | Z) ≥ q1+ϵ·cϵ}.
q,ϵ
This allows us to interpret Rashomon partitions with respect to a reference partition or pooling.
Without any context, it is difficult to choose a threshold θ. However, if we have some reference
model Π that we know is good, then it makes sense to pick a threshold that is not much worse than
0
the performance of Π . In particular, using a reference Π , we can define a measure of performance
0 0
of model Π with respect to Π as
0
logP(Π | Z)−logP(Π | Z) Q(Π)−Q(Π )
0 0
(7) ξ(Π,Π ) = = .
0
logP(Π | Z)+logP(y | X) Q(Π )
0 0
ξ is essentially the log-likelihood ratio of the two models weighted by the log-likelihood of the
reference model. For data that has a considerably higher likelihood (weighted by the likelihood of
the model), the measure goes to 1. So when Π is a better fit than the reference, ξ(Π,Π ) < 0.
0
Conversely, when Π is a poorer fit than the reference, ξ(Π,Π ) > 0. Note that if the two posteriors
0
are identical, then ξ(Π,Π ) = 0.
0
Suppose we know that Π is a good model such that P(Π | Z) = q. It makes sense to only look
0 0
at partitions Π such that ξ(Π,Π ) ≤ ϵ for some ϵ > 0. We show how to recover the RPS using (q,ϵ)
0
in Proposition 1.10
Proposition 1. Fix Π ∈ P⋆ and let q = P(Π | Z). Then P = {Π ∈ P⋆ : ξ(Π,Π ) ≤ ϵ}.
0 0 q,ϵ 0
By construction, this result is almost trivial. Proposition 1 says that selecting the Rashomon
threshold θ is equivalent to first selecting a reference model and choosing a tolerance ϵ relative to
10This is related to the candidate models for Bayesian model selection used by Madigan and Raftery (1994). Their
set of models is A={Π:P(Π |Z)/P(Π|Z)≤c˜} where Π is the maximum a posteriori estimate. In the language
0 0
of Rashomon sets, c˜=(P(Π |Z)P(Z))−ϵ.
0RASHOMON PARITITONS FOR HETEROGENEITY 19
Notation Definition
P Set of admissible partitions with h pools
|h
Q ∈ Q Prior over all β
Q ∈ Q Prior over β such that there is some partition Π ∈ P
|h β |h
Q ∈ Q Prior for partitions Π ∈ P
P|h |h
P The uniform prior over P (induced by ℓ over P⋆)
ℓ0 |h 0
P Posterior density (over partitions or β) with prior Q
Q,Z
δ(P,Q) Total variation distance between P and Q
Table 1. Notation used in Theorem 2.
it. Regarding the choice of the reference model, consider any good initial estimate of a model.
This could be for instance the MAP or a technique that converges to the MAP (e.g., regularization
through the Lasso). Then the RPS will trace out, given a tolerance epsilon, all the models that
have slightly lower or at least higher posterior values relative to the reference. This guarantees that
we can essentially be robust against the Rashomon Effect and gives guidance on the choice.
4. Size of the Rashomon set
Given that we would like to enumerate P it is useful to calculate bounds on both its size and
θ
also P⋆. Since any admissible partition requires each profile to respect Definition 6, it is sufficient
to consider each profile independently. To develop an upper bound, we will use m to denote the
number of features with non-zero values in the profile we are focusing on, so m ∈ {1,...,M}. All
technical details are deferred to Appendix C.
The first observation is that P⋆ is small relative to the total number of potential partitions.
Proposition 2. In each profile, the total number of:
(i) all possible partitions is
O(cid:0) 22(R−1)m(cid:1)
, and
(ii) admissible partitions is O(2m(R−2)).
In the following sections, we motivate the choice of the ℓ prior and discuss how this can be used
0
to control the size of the Rashomon set.
4.1. Choosing a robust prior. Next, we select a prior. RPS can be built using other priors, but
we advocate for using the robust one. We do not want to impose false independence or unwarranted
assumptions on correlations on the relationship between the βs and instead want to be robust in
an environment with possibly a complex and unknown correlational structure. We show that this
corresponds to the ℓ penalty: conditional on the number of pools in a partition, all admissible
0
partitions are equally likely. Let Q be a family of priors for some expected outcome β. For anyRASHOMON PARITITONS FOR HETEROGENEITY 20
prior Q ∈ Q, denote the posterior over β given some data Z as P , i.e.,
Q,Z
P(y | X,β)Q(β)
P (β) = P(β | Z,β ∼ Q) = .
Q,Z
P(y | X)
Let us fix the sparsity at h: there are h distinct pools in the partition. Define the restricted space
(cid:12) (cid:12)
of partitions as P
|h
= {Π ∈ P⋆ : H(Π) = h}. Let N(h) = (cid:12)P |h(cid:12). The ℓ
0
penalty imposes a sparsity
restriction on the number of pools. Therefore, at a fixed sparsity h, the ℓ penalty corresponds to
0
a uniform prior over P . Denote the ℓ prior as P . So for any Π ∈ P , P (Π) = 1/N(h).
|h 0 ℓ0 |h ℓ0
For any given β, there is a corresponding admissible partition Π ∈ P⋆. Then we can define Q
β |h
be the family of priors for the restricted space of β such that there is some Π ∈ P . Let Q
β |h P|h
denote the family of priors, derived from Q , over partitions in P . We can traverse from Q to
|h |h |h
Q by noticing that for a given β, there is a corresponding admissible partition Π ∈ P i,e, for
P|h β
any prior Q ∈ Q , we can define a prior over P ,
|h |h
(cid:90)
Q (Π) = I(Π = Π)Q(β)dβ, Π ∈ P .
P|h β |h
β
For reference, we define the supports for various priors in Table 1.
For two priors P,Q ∈ Q , define the total variation distance as
P|h
δ(P,Q) = sup |P (Π)−P (Π)|.
P,Z Q,Z
Π∈P
|h
Note that since P⋆ is finite, the supremum and maximum over P⋆ (and any of its subsets) are
identical.
Theorem 2. For a given sparsity h, the ℓ penalty is minimax optimal in the sense that
0
sup δ(P ,P ) = inf sup δ(P ,P ).
P ,Z Q,Z P,Z Q,Z
Q∈Q
P|h
ℓ0 P∈Q P|hQ∈Q
P|h
Inotherwords,ifoneisunwillingtocommittoanycorrelationstructureforthemodelcoefficients,
the ℓ penalty, which puts a prior on the number of selected features, is optimal for model selection.
0
4.2. Implications for Calculations. WewillshowthatthesizeoftheRashomonPartitionSetfor
each profile is actually polynomial in M and R. First, in Lemma 1, we make a crucial observation
that the number of pools in any RPS is bounded.
Lemma 1. For a given probability q, maximum distance ϵ, and regularization parameter λ, any
aggregation in the Rashomon set P can have at most H (λ) pools,
θ θ
(cid:22) (cid:23)
ln(cθ)
H (λ) = − ,
θ
λRASHOMON PARITITONS FOR HETEROGENEITY 21
where c := c(Z) is a normalization constant that depends only on Z and ⌊·⌋ is the floor function.
Essentially, the likelihood term, c(Z), is driving all of the penalty on the maximum number of
pools in a Rashomon partition. Since the likelihood of any given observation is small, it explodes
as we collect more observations. As we collect more data, we gain more information with which we
can distinguish finer partitions. Lemma 1 allows us to further reduce the size of the search space in
Proposition 2 by considering only partitions that meet this requirement.
We show that the size of the Rashomon partition set is bounded polynomially in Theorem 3.
Such a relationship between regularization and size of the model class was previously hypothesized
and shown for empirical data by Semenova et al. (2022).
Theorem 3. For a given Rashomon probability θ and regularization parameter λ, the size of the
Rashomon Partition Set is bounded by

(cid:88)H  O(cid:0) mRH−2(cid:1) , R > m1.41
|P | ≤ N(h) = ,
q,ϵ (cid:16) (cid:17)
h=1  O (mR)log 2H(log 2(mR))−1 , else
where H := H (λ) and N(h) is the number of possible splits that generate h pools.
θ
To illustrate the implications of Theorem 3, consider an example in which M = 3 and R = 5.
Also, suppose that we can have at most H = 8 pools in any Rashomon partition. Theorem 3 says
that the size of the RPS is |P | ≤ 27 using the first bound. The number of all admissible partitions
q,ϵ
is2M(R−2) = 512. Now,imagineasettingwhereweaddtwomorearmsandanotherdosageintensity
to get M = 5 and R = 6. While the size of the RPS is |P | ≤ 800, the total number of admissible
q,ϵ
partitions is 32,768. The RPS is less than 2% of the full search space. Let’s turn M = 10,R = 10.
Now, size of the RPS, 81,680, is only a fraction (≈ 7×10−20) of the full search space, ≈ 1024. As
we will see in our empirical examples, combined with Theorem 1, with just hundreds of models, we
can get very close to the full posterior.
5. Enumerating Rashomon Partitions
Since we do not pool across profiles, we can enumerate the Rashomon Partition for each profile
independently and then finally combine them in the end. We will first develop intuition to present
an algorithm to enumerate the RPS for a single profile. Then, we will discuss how to combine these
profile-specific RPS to get our RPS across all profiles. The intuition behind our enumeration is
that any split we make introduces a new set of pools. If for some reason this split is very bad,
then no matter what other split we make, we can never recover. Theorems 4 and 5 help us identifyRASHOMON PARITITONS FOR HETEROGENEITY 22
those poor splits. They rely on the fact that equivalent points having the exact same feature values
will always belong to the same pool. However, equivalent units may not have the same outcome.
Therefore,wewillalwaysincursomelossfromtheseequivalentunits(alsoseeAngelinoetal.(2017);
Xin et al. (2022)). We defer technical details of the results to Appendix D.
First, we develop some notation. Let K define the set of all K = RM possible feature combi-
nations. For some k ∈ K, let k denote the value that feature i takes for i = 1,...,M. Consider
i
some partition matrix Σ for profile ρ, where the partition is given by Π := Π(Σ). Given some data
Z = (X,y), we will use the mean squared error and the average outcome in pool π ∈ Π, µ , as
(cid:98)π
defined in Equation (6). However, the results generalize to any non-negative loss function as we will
see in Section 8 with weighted mean-squared error.
Consider the Σ matrix. Suppose we fix some indices M in Σ. Define a new matrix Σ ,
f

 Σ (i,j), (i,j) ∈ M
Σ = .
f,(i,j)
 0, else
In other words, Σ is a partition where all heterogeneity splits made by Σ corresponding to indices
f
in M are obeyed and we maximally split at all other places. Let Π := Π(Σ ) correspond to this
f f
maximal partition respecting Σ at indices M. Next, define
π = {k ∈ K | k ≤ j +1 ⇐⇒ (i,j) ∈ M}
f i
to be the set of all feature combinations covered by indices in M. And we define the complement
πc = K \ π . Finally, define H(Π,M) = (cid:80) I{π ∩ π ̸= ∅} to be the number of pools in Π
f f π∈Π f
consisting of feature combinations corresponding to indices M.
Now consider a procedure where we keep Σ constant at M and make further splits (not already
implied by M) at other indices only. Define child(Σ,M) to be the set of all such Σ′.
Using the intuition we built earlier, our search algorithm in Algorithm D.1 starts at some par-
tition and fixes some heterogeneity splits. Theorem 4 says that if the loss incurred by these fixed
heterogeneity splits is already too high, then our search is doomed to begin with and we should
abandon this partition.
Theorem 4. Let θ be the Rashomon threshold in the loss space i.e., Π ∈ P if and only if
ϵ q,ϵ
Q(Π) < θ . Given a partition Π := Π(Σ) for partition matrix Σ, a set of fixed indices M, and data
ϵ
Z consisting of n observations, define
1 (cid:88) (cid:88)
(8) b(Σ,M;Z) = I{k(i) ∈ π }(y −µ )2+λH(Π,M).
f i (cid:98)π
n
π∈Πfk(i)∈πRASHOMON PARITITONS FOR HETEROGENEITY 23
If b(Σ,M;Z) > θ , then Σ and all Σ′ ∈ child(Σ,M) are not in the Rashomon set P .
ϵ q,ϵ
Building on the same intuition, Theorem 5 “looks ahead” to see if this partition is of poor quality
– if the loss incurred by feature combinations yet to be split is too high, then we should abandon
this partition.
Theorem 5. Consider the same setting as Theorem 4. Define
(9) b (Σ,M;Z) = 1 (cid:88) (cid:88) I(cid:8) k(i) ∈ πc(cid:9) (y −µ )2,
eq n f i (cid:98)π
π∈Πfk(i)∈π
(10) B(Σ,M;Z) = b(Σ,M;Z)+b (Σ,M;Z).
eq
If B(Σ,M;Z) > θ , then Σ and all Σ′ ∈ child(Σ,M) are not in the Rashomon set P .
ϵ q,ϵ
Theorems 4 and 5 help aggressively cut down the search space by combining the lowest penalty
on the splits already made and the lowest mean-squared error on the splits yet to be made. If this
is already too high, then we abandon our search. We illustrate this in Algorithm D.1. Here, we
start with all feature combinations pooled together. We begin our search at the first feature trying
to split the two variants with the lowest dosages into separate pools. We keep a queue of possible
splits to consider. Whenever we remove a possible split from the queue, we check its viability using
Lemma 1, and Theorems 4 and 5. If this is a bad split, we go to the next split in the queue. And
if this is a good split (so far), we check if it already meets the Rashomon threshold and recursively
add other possible splits to this queue. We also maintain a cache of splits that have been added to
the queue at some point to avoid doubling back on old splits.
The choice of starting position in Algorithm D.1 is arbitrary. No matter with which feature we
start our search, Algorithm D.1 will eventually explore the feature space sufficiently to identify
partitions outside the Rashomon set, at which point we abandon that search. Theorems 4 and
5 guarantee this. Algorithm D.1 will correctly enumerate the RPS independently of our starting
search position.11 As noted before, we can solve each profile independently. In Algorithm 1, we
explicitly show how to do this. Note that in line 3, we once again leverage Theorem 5 by noting
that each profile will always incur some loss.
Once we solve each profile independently, we can start pooling across profiles as dictated by
Definition 7. The key insight here is the construction of the intersection matrix Σ∩ we discussed
earlier. Using this object, pooling across profiles can be achieved by a breadth-first search on the
11Obviously, some starting positions may be computationally favorable i.e., we do not need to search for too long
before we encounter low posterior partitions. We believe domain experts will have a better understanding of the
context and may be able to choose a starting location that can reduce computation costs.RASHOMON PARITITONS FOR HETEROGENEITY 24
Algorithm 1 EnumerateRPS(M,R,H,Z,θ )
ϵ
Input: M features, R factors per feature, max pools H, data Z, Rashomon threshold θ
ϵ
Output: Rashomon set P
q,ϵ
1: ρ = all 2M profiles
2: H′ = H −2M +1
3: E = [equivalent point bound of profile ρ i for ρ i ∈ ρ] ▷ b eq in Theorem 5 with the zero matrix
4: P = dict()
5: for ρ i ∈ ρ do
6: θ i = θ ϵ−sum(E)+E ρi
7: M i = active features in ρ i
8: R i = R[M i]
9: P[ρ i] = EnumerateRPS_profile(M i,R i,H′,Z,θ i) ▷ See Algorithm D.1
10: Sort partition matrices in P[ρ i] on loss
11: P′ =× ρi∈ρP[ρ i] ▷ Obtain candidate partitions with Cartesian product
12: P q,ϵ = PoolProfiles(P′,ρ 0,Z,θ ϵ) ▷ See Algorithm D.4
13: return P q,ϵ
M-d unit hypercube by starting at the profile with all features inactive. During this breadth-first
traversal, we recursively attempt to pool across neighbors in the hypercube using the intersection
matrix. We describe this in Algorithm D.4.
Theorem 6. Algorithm 1 correctly enumerates the Rashomon partition set.
Given our setup, it is easy to see how Algorithm 1 can be parallelized by delegating each call to
Algorithm D.1 to a separate thread. Since Algorithm D.4 is a breadth-first search, it can also be
parallelized. Its correctness is shown in Appendix D.
6. Simulations
We conduct two simulation experiments. The first is the drug treatment setting we have been
motivating throughout the paper. It demonstrates that in a very reasonable, real-world scenario –
minimumdosagestobeeffective,stronginteractioneffectswithexcessamounts–regularizationthat
relies on independence (Lasso) will fail to identify the best dosing policy. In contrast, the RPS will
both tend to include the best drug combination (in addition to others of course) but also converge
to the true best profile as the amount of data increases. The second illustrates the Rashomon effect.
In a given sample, it is possible through random chance that a sub-optimal drug profile happens to
be the best. The RPS contains the true best the vast majority of the time.
6.1. Simulation1: Anexampleinmedicine. Imagineasettingwhereapharmacistisinterested
in finding the best treatment as a combination of two drugs, A and B. In particular, we considerRASHOMON PARITITONS FOR HETEROGENEITY 25
π ,β = 4.5
6 6
(4,4)
π ,β = 6
5 5
(3,4) (4,3)
(2,4) (3,3) (4,2)
π ,β = 3 (1,4) (2,3) (3,2) (4,1) π ,β = 3
3 3 4 4
(1,3) (2,2) (3,1)
(1,2) (2,1)
π ,β = 1.5 π ,β = 0
2 2 1 1
(1,1)
Figure 3. Hasse diagram illustrating partition used in Simulation 1.
a scenario where each drug needs a minimum dosage to be effective. However, when the dosages
become too strong, the interaction between the drugs results in a reduced treatment effect.12
In this setup, suppose that each drug has 4 possible non-zero dosages d ∈ {1,2,3,4}. We will
denoteeachof42 = 16uniquedrugcocktailsbytheirdosage(d ,d ). Thetreatmentisnoteffective
A B
when d < 4 and d = 1. As we increase the dosages of the drugs, they become more effective.
A B
The treatment is most effective when drug A is maxed and drug B has a medium dosage i.e.,
d = 4, d = 2,3. When both drugs are maxed, a drug interaction produces a sub-optimal effect.
A B
Our parameters and partition are summarized in Figure 3. Observe that, by design, all non-zero
marginal increases in outcomes as we increase dosage levels are correlated. Therefore, we expect
Lasso to perform poorly as it penalizes selecting correlated features. However, the ℓ penalty does
0
not presume any such false independence assumptions.
In each dataset, we fixed the number of samples per feature combination to n and outcomes for
k
each feature were drawn from a N(β ,1) distribution. We varied n ∈ {10,100,1000,5000} and for
i k
each n simulated r = 100 datasets. For each dataset, we fit the Lasso model and found the RPS.
k
The Rashomon threshold was chosen to be 1.5×MSE where MSE is the MSE of the Lasso
Lasso Lasso
12Suchantagonisticinteractions whereonedrugimpedestheeffectofanotherdrugarenotuncommon(seeCascorbi,
2012; Triplitt, 2006; Wambaugh et al., 2020). For example, ACE inhibitors (Angiotensin-converting enzyme in-
hibitors),usedintreatinghighbloodpressure,andMetformin,usedintreatingdiabetes,areknowntohavenegative
interactions (Blackburn and Wilson, 2006); ACE inhibitors and NSAIDs (Non-steroidal anti-inflammatory drugs),
andAspirinandIbuprofenhavereducedeffectswhentakentogether(Cascorbi,2012),and;effectsofdruginteractions
in non-small cell lung cancer are highly context-specific (Nair et al., 2023).RASHOMON PARITITONS FOR HETEROGENEITY 26
Figure 4. Results for Simulation 1. The blue points correspond to models in the
Rashomon set and the red points correspond to Lasso estimates. From left to right:
mean squared error, best policy set coverage and best policy mean squared error.
model with n = 10. We use the same regularization parameter λ = 0.1 for the Rashomon and
k
Lasso models.
The results from the simulations are presented in Figure 4. The metrics reported here are de-
scribed in Appendix G. It is clear that the “average” model in the RPS not only performs much
better than Lasso in terms of overall MSE, but it is also able to recover the true best policy set. By
looking only for the optimal model, Lasso consistently misses out on coverage for the best policies
by incorrectly selecting features. However, when we look at the RPS of near-optimal models, we
can recover the full best policy coverage almost always. Our results also reveal that the poor per-
formance of Lasso is not a sample size issue. Lasso is simply not the right tool in situations with
correlated parameters. Appendix G also visualizes the the Rashomon set through a 2D heatmap.
6.2. Simulation 2: The Rashomon effect. We now illustrate the Rashomon effect. By using
just the optimal model, we miss out on insights into the true data-generating process offered by
near-optimal models.
Consider a setting with four features. Each feature takes on four ordered factors including the
control (which corresponds to zero, when the feature is inactive), {0,1,2,3}. There are sixteen
different feature profiles: 24 = 16 possible combinations of active and inactive features. The control
corresponds to the profile where all features are inactive.
Ourdata-generatingprocessgroupsallfeaturecombinationsinagivenprofileintothesamepool.
We will assume that the following profiles have a non-zero outcome:
β = 4.4, σ2 = 1, β = 4.3, σ2 = 1, β = 4.45, σ2 = 1,
(0,0,0,1) (0,0,0,1) (0,1,0,0) (0,1,0,0) (0,1,0,1) (0,1,0,1)
β = 4.5, σ2 = 1.5, β = 4.35, σ2 = 1.
(1,0,1,0) (1,0,1,0) (1,1,1,1) (1,1,1,1)RASHOMON PARITITONS FOR HETEROGENEITY 27
Figure 5. Results for Simulation 2. The plot shows often the true best profile
is discovered as we increase the Rashomon threshold in the blue curve. With just
ϵ ≈ 0.038, we recover the true best profile in the Rashomon set about 90% of the
time. The red dot corresponds to how often Lasso recovers the true best profile.
All other feature profiles have outcome β = 0 and variance σ2 = 1. Here, the feature profile
(1,0,1,0) is the best. However, the other four profiles listed above are very close.
Wegenerateddatawithn = 30datapointsperfeaturecombination. Eachprofilehasadifferent
k
number of feature combinations depending on how many features are active. Therefore, each profile
willhaveadifferentnumberofdatapoints. TheoutcomesweredrawnfromaN(β ,σ2)distribution.
i i
We averaged the results over r = 100 simulations.
Figure 5 tells us how often the true best feature profile is present in the Rashomon set as a
function of the threshold ϵ. If we were to focus only on the best-fit model, we lose out on models
that correctly recover the true best feature profile. However, by looking at the RPS, we can tease
apart the nuances in the data-generating process. And as we increase ϵ, we increase the chance that
we recover the true best profile.
7. Empirical data examples
In three distinct environments – impact of price match on donations, heterogeneity in chromoso-
malstructure,andtheimpactofmicrocreditonentrepreneurship–weemphasizerobustconclusions,
both affirming and challenging the established literature’s findings in each context through the use
of RPS. This underscores the importance of our approach in shedding light on various research
landscapes.
7.1. Does price matter in charitable giving? Our first example comes from the data of Karlan
and List (2007) and concerns charitable giving. This is important for both basic research and policy
considerations. Depending on the specific motivation for giving, whether or not there is a matchingRASHOMON PARITITONS FOR HETEROGENEITY 28
gift for one’s donation and how it scales with one’s own donation may impact one’s donation.
Further, these patterns may correlate with political perspectives, which is of particular importance
when the charity is a political NGO as is the case here.
Karlan and List (2007) conducted a field experiment to better understand the anatomy of char-
itable giving. They used mail solicitations to prior donors of a non-profit political organization to
studytheeffectofpriceoncharitabledonations. Thedatacontains50,083individualsintheUnited
States who had previously donated to the organization. All individuals received a letter soliciting
donations. Those in the treatment group (33,396 people) included an additional paragraph describ-
ing that their donation will be matched. The control group (16,687 subjects) did not receive this
paragraph. The letters were identical otherwise.
The treatment consists of three different features: (i) the maximum size of the matching gift
across all donations, (ii) the ratio of price match, and (iii) an example suggested donation amount.
The maximum size of the matching gift took on four different values – $25,000, $50,000, $100,000,
or unspecified. The price match ratio took on three different values — $1:$1, $2:$1, or $3:$1. Here
$a : $b means that the organization receives $a for every $b contribution. Finally, the suggested
donation amount was dependent on the highest previous contribution (HPC) from that individual
and took on three possible values – 1·HPC, 1.25·HPC or 1.5·HPC. This leads to a 4×3×3
factorial design, excluding control, with fully ordered factors.13 All feature combinations were fully
randomized.
Additionally,theyalsocollecteddemographiccensusdata(aggregatedatthezipcodelevel),state
andcountyreturnsfromthe2004presidentialelection, andfrequencyoftheorganization’sactivities
at the state level. They classify states as red or blue depending on whether they voted for George
Bush or John Kerry in the 2004 U.S. presidential election. They note that political affiliation is
the most robust indicator of the level of giving in the control group. Therefore, in our analysis, we
include this as a demographic covariate. There were 35 data points whose political affiliation was
missing and we removed these data points prior to our analysis. Thus, we have a 4×3×3×2
feature space. Our outcome is the amount, in hundreds of dollars, that were donated.
Karlan and List (2007) focus on two key findings. First, they find that (and puzzle over why)
matching grants work in red but not blue states. Second, though having a match at all (relative to
the control of none) has an effect, they argue that the matching ratio itself does not matter. That
is, having an overly steep ratio is essentially wasteful.
13We assume $25,000<$50,000<$100,000<unspecified.RASHOMON PARITITONS FOR HETEROGENEITY 29
Figure 6. ResultsfortheKarlanandList(2007)dataset. Thetoptwopanelsshow
the size of the RPS and error term in Theorem 1 as a function of ϵ. Our choice of
ϵ = 10−4 is highlighted by the black dashed line. The bottom panel shows the effect
of price match of $2:$1 and $3:$1 relative to $1:$1 are stratified by political leaning
and other treatments in the RPS.
We look at these two findings through the lens of the RPS. Figure 6 presents our choice of
ϵ = 10−4 as well as how the set size and error bound change with ϵ. Figure 6 also shows the effect
of price match of $2:$1 and $3:$1 relative to $1:$1 for all feature combinations.
Wefindthefollowing. First,thetreatmenteffectofmatchingisindeeddifferentiallydrivenbyred
statesalonefromarobustperspective. Inonlylessthan1%ofthepartitionsintheRPS,weseethat
the treatment effect of matching is pooled between red and blue indicating robust heterogeneity of
treatment effect across political affiliations. (This calculation is not directly presented in the figure,
thoughitcanbeglimpsedfromthefactthatthepatternsbetweenredandbluestateslookdifferent.)
Second, while having a match matters and is stratified across political affiliations, the matching
ratio itself matters. That is, there is a split in the treatment effect between $1:$1 and > $1:$1 in
100%ofthemodelsintheRPSforbluestatesand65%ofthemodelsintheredstates. Ineachmodel
intheRPS,wecomputedsign{y(2:1,x,z,p)−y(1:1,x,z,p)}andsign{y(3:1,x,z,p)−y(1:1,x,z,p)}
(cid:98) (cid:98) (cid:98) (cid:98)
for each maximum limit x, suggested donation z, and political affiliation p, where y(k) estimates
(cid:98)RASHOMON PARITITONS FOR HETEROGENEITY 30
E[Y (k)] with a sample mean.14 And we visualize the average of those quantities over the RPS in
i
the bottom panel of Figure 6. We clearly see robust patterns in the sign of the match ratio effect.
Specifically, for $3:$1 in blue states, we see a robust positive effect for a maximum limit of $25,000
and a robust negative effect for $100,000 or unspecified limits.15 In red states, we see a robust
positive effect for a suggested donation of 1 · HPC and a robust negative effect for 1.25 · HPC.
OurresultsdemonstratethatKarlanandList(2007)’sassertionofthered-bluedifferencerobustly
holds. Meanwhile, contrary to their assertion (which is an artifact of their particular regression), we
find that robustly match ratios matter and in particular this is the case with blue states across all
feature combinations though holds true for most of the red state configurations as well. The result
on match ratios mattering is of great policy relevance for the same reasons as argued in Karlan and
List (2007): if they did not matter, low ratio matches could be used to save money, but if they
robustly do matter, when they are positive high ratio matches ought to be leveraged and may have
excess returns and when they are negative the costs are even more damaging.
7.2. Heterogeneity in telomere length. Telomeres are regions of repeated nucleotide sequences
near the end of the chromosome that protect the chromosome from damage. Telomeres reduce in
length every time a cell divides eventually becoming so short that the cell can no longer divide.
Telomere length varies by cell type and more recent literature has begun to examine what features
are associated with (or possibly cause) changes in telomere length. For instance, the shortening of
telomeres has been linked to cellular senescence and aging and has been thought to hold biomarkers
as targets for genetic predispositions and anti-cancer therapies (Rossiello et al., 2022; Srinivas et al.,
2020). While it was long believed that longer telomeres are “better,” recent research suggests that
perhaps there is only a narrow range of telomere lengths that are healthy and anything that is
extreme is at increased risk of immune system problems or cancer (Alder et al., 2018; Protsenko
et al., 2020). Research has found heterogeneity by race and ethnicity and, further, features such as
stressandagehavebeenassociatedwithdifferencesintelomerelength(Chaeetal.,2014;Geronimus
et al., 2015; Hamad et al., 2016; Vyas et al., 2021). Nonetheless, these analyses are large-scale
associative statistical analyses rather than those derived from micro-experimental data.
Our RPS approach may be useful here as we can identify in data involving telomere length and
variousfeatures,whatrobust associationscanbemade. Thiscangiveamicro-orientedexperimental
researcher guidance on where to focus, rather than relying on a specific machine learning exercise.
14We define sign{x}=I{x>0}−I{x<0}∈{1,0,−1}.
15This robust discouragement effect is particularly interesting and may warrant further research.RASHOMON PARITITONS FOR HETEROGENEITY 31
Figure 7. The top two panels show what happens as we increase ϵ in the NHANES
dataset highlighting our choice of ϵ. In the bottom panel, we highlight heterogeneity
intelomerelengthacrossthefourfeatures(hoursworked,gender,age,andeducation)
relative to the lowest level of that feature, sorted into race.
We use the National Health and Nutrition Examination Survey (NHANES) collected in 1999
and 2002. The survey included blood draws and from the samples DNA analyses were performed
and telomere length was estimated. Specifically, the dataset reports the mean T/S ratio (telomere
length relative to a standard reference DNA).16 The dataset also contains socio-economic variables.
To speak to the emerging literature on telomere heterogeneity, we focus on hours worked (a proxy
for stress), age, gender, race, and education. Our goal is to study the RPS of this heterogeneity on
T/S.17
16Seehttps://wwwn.cdc.gov/nchs/nhanes/1999-2000/TELO_A.htmfordetails. Websitelastaccessedon2024-01-29.
17To operationalize, we removed all individuals who were missing data for our relevant covariates. We binned the
numberofhoursworkedintothreeordereddiscretefactors–≤20hours,21−40hours,and≥40hours. Genderwas
unordered, and either Male or Female. Age was categorized into five ordered discrete factors – ≤ 18 years, 19−30
years, 31−50 years, 51−70 years, and > 70 years. Education was categorized into 3 ordered discrete factors –
did not complete GED, finished GED but did not finish college, and received some college degree. Finally, race was
categorized into three unordered factors – Black, White, and Other.RASHOMON PARITITONS FOR HETEROGENEITY 32
We show our choice of ϵ for the Rashomon threshold in the top two panels of Figure 7. In the
RPS, we found robust heterogeneity in race – specifically, we found no partition that pools features
across different races. So the remainder of the analysis will stratify based on race.
WefoundrobustevidenceofheterogeneityingenderonlyinWhiteandBlackraces. Allpartitions
for these races split males and females into separate pools. However, this was absent in Other races
– only 23% of the partitions in the RPS split on gender.
Similar to the previous data exercise, for each race r, we find the length of telomeres stratified
by each feature m ∈ {Hours worked, Gender, Age, Education} relative to the lowest level of that
feature, E[Y (x ,x ,r)]−E[Y (1,x ,r)].18 We use the sample mean estimates y(x,r) and take
i m −m i −m (cid:98)
its sign, sign{y(x ,x ,r)−y(1,x ,r)} ∈ {1,0,−1}. We average the counts of each sign over all
(cid:98) m −m (cid:98) −m
partitions in the RPS and report them in the bottom panel of Figure 7. By visually inspecting this
plot, we find very few robust patterns. As discussed earlier, we find robust differences in telomere
lengthsacrossmalesandfemalesintheBlackpopulationandarobustnon-differenceinOtherraces.
Similarly, we find a robust non-difference in Black and Other races in telomere lengths for people
who work fewer than 40 hours.
Our findings reveal an absence of robust evidence supporting the patterns highlighted in existing
literature. Moreover, of the few robust patterns we do identify, several findings contradict prior
research. We find Black males have longer telomeres than females. Among White people, we find
older people have longer telomeres, which also contradicts existing research. This underscores the
necessity for further exploration in this field using comprehensive data and appropriate statistical
methods.
7.3. Heterogeneity in the impact of microcredit access. A large literature has looked at
the impact of microfinance on several outcomes, ranging from private consumption to business
outcomes to social outcomes (e.g., female empowerment). Mostly, the literature has found little
beyond basic consumption effects (Angelucci et al., 2015; Attanasio et al., 2015; Augsburg et al.,
2015; Banerjee et al., 2015; Crépon et al., 2015; Tarozzi et al., 2015; Meager, 2019), though there is
suggestive evidence of some potential heterogeneity. One specific heterogeneity of interest concerns
entrepreneurs: those with pre-existing businesses may be particularly benefited by the access to
microfinance loans (Banerjee et al., 2019). Another concerns family size (Baland et al., 2008): the
returns to credit access may vary by whether the household has more children.
Developing an RPS is useful both for (a) developing theory about “archetypes” of potential
borrowers: those who may respond heterogeneously due to credit access and (b) policymakers and
18For gender, we found telomere lengths of Male (x =2) relative to Female (x =1).
m mRASHOMON PARITITONS FOR HETEROGENEITY 33
microcredit institution strategies. On the latter, the RPS may give policymakers pause about
microcredit entry because they may worry about potential negative effects due to the complex
nature of a credit injection. Our analysis allows the policymaker to take in robust advice: when no
reasonable treatment of the data suggests negative effects, then policymakers can safely intervene.
An RPS analysis provides a blueprint for robust intervention for a policymaker.
We study the Rashomon Partition Set in the Banerjee et al. (2015) data. The data is generated
from a randomized controlled trial in which 102 neighborhoods in Hyderabad, India were randomly
assignedtotreatmentorcontrol, eachwithequalprobability, wheretreatmentmeantthatapartner
microfinance organization, Spandana, entered. At baseline a number of characteristics of sampled
individuals were collected, including the gender of the head of the household, the education status
of the head of the household, the number of businesses previously owned by the household, and the
numberofchildreninthehousehold. Additionally, attheneighborhoodlevel, informationaboutthe
share of households with debt, the share of households with businesses, total expenditure per capita
in the region, and average literacy rates in the region were also collected at baseline. Amongst these
regional characteristics, motivated by the literature we only look at the regional debt and business
variables.
We look at outcomes from the second (longer term) endline, focusing on four spheres: (i) loans,
(ii) household response (total expenditure, durables, temptation goods, labor supply), (iii) business
(revenue, size, assets, profits), (iv) female empowerment (female business participation, education
of daughters). We discretized the regional characteristics and the number of businesses previously
owned into four levels based using quartiles. And we set the first quartile as the “base control” i.e.,
we consider that characteristic to be active if it is one of the higher three levels.
To study the impact of access to microcredit, we allow features across treatment and control
profiles to be pooled together as per Definition 7. Then, we measure the heterogeneous impact as
the conditional average treatment effect, CATE(x) = E[Y (1,x)]−E[Y (0,x)]. We find the sample
i i
(cid:92)
mean estimate CATE(x) = y(1,x)−y(0,x) where y(1,x) is the estimated potential outcome for a
(cid:98) (cid:98) (cid:98)
household assigned to treatment with feature combination x, and y(0,z) is the estimated potential
(cid:98)
outcomeforahouseholdassignedtocontrolwithfeaturecombinationz. Iffeaturexispooledacross
(cid:92)
the treatment and control profiles, then CATE(x) = 0 indicating no treatment effect heterogeneity
(cid:92)
in feature x. Otherwise, CATE(x) ̸= 0 indicating treatment effect heterogeneity. Here we present
(cid:92)
sign{CATE(x)} ∈ {1,0,−1}, which captures robust (or non-robust) qualitative patterns. We sort
x into various profiles and count the number of features in each profile that have a positive, zero,
and negative effect, averaging the counts over all partitions in the RPS.RASHOMON PARITITONS FOR HETEROGENEITY 34
Figure 8. This plot visualizes the number of features with a positive, zero, or
negative effect, averaged across partitions in the RPS. Each column corresponds to
one of the five robust feature profiles described here where the label denotes which
features are active (i.e., do not take the lowest level). “None” means that all features
are taking these lowest values.
In Appendix H, Figure H.3 shows the full set of profiles and outcomes, demonstrating how non-
robust many of the conclusions about many of the archetypes would actually be. These results
indicate that there are no stable conclusions to be had for most profiles. Here, out of the 16 profiles,
we highlight the 5 most robust ones, though arguably only the first is particularly robust. In each
of these (and across all 16 profiles), profits and employees are robustly unaffected by microcredit.
We describe the patterns for the most robust profile.
(1) Most robust archetype: Large households, with no previous businesses, in a region with
low baseline debt and business presence:
(a) take more loans (including informal).
(b) consume more, particularly durables but not temptation goods, and supply less labor.
(c) increase revenue, but nothing robust to note about business asset accumulation.
(d) see no changes in female empowerment.
(2) Other archetypes exhibiting some robustness:
(a) Small households, with entrepreneurial experience, in a region with low baseline debt
but high business presence.
(b) Small households, with no entrepreneurial experience, in a region with low baseline
debt and business presence.
(c) Small households, with entrepreneurial experience, in a region with low baseline debt
and business presence.
(d) Large households, with no previous businesses, in a region with high baseline debt but
low business presence.RASHOMON PARITITONS FOR HETEROGENEITY 35
The RPS provides an avenue for the researcher to identify archetypes: profiles where the treatment
effects are robust across many outcomes of interest. This provides an avenue for theory-building.
It also clearly demonstrates when, for numerous profiles, there is little robustness to be said: the
data, without strong priors, cannot really speak to the impacts of microcredit in most cases.
In Appendix H, we also look at the treatment effect heterogeneity by gender, which has been a
point of interest in the literature. For the most part, we find no robust heterogeneity: the most
robust conclusion is that there is no gender heterogeneity (a robust zero) in profits, business assets,
and firm size. There are some minor robust differences in household expenditure response patterns,
but the overall message is of non-robustness.
Finally, it is worth reiterating that beyond its value in theory-building, the RPS gives policymak-
ers guidance on robust interventions. So for example, if the policymaker considered regions with
high baseline debt, since robustly there are no positive profits and half the RPS suggests negative
profits for entrepreneurs, they may not wish for the microcredit firm to enter in this market. But
in contrast, in other markets, e.g., low debt and business presence, for large non-entrepreneurial
families since there are robustly no effects on profits and robustly positive effects on consumption
and leisure, they can proceed with confidence.
8. Generalizations
Here, we consider two generalizations of the methods discussed so far. First, we consider a family
of heterogeneous effects functions beyond just heterogeneity splits. For example, there might be
some heterogeneity in slopes (and slopes of slopes, and so on). Second, we extend our method to
pool on the space of the covariance between coefficients, rather than on the coefficients themselves.
This means that coefficients no longer need to be exactly equal but, instead, related through a
sparse covariance structure. We defer all technical details to Appendix D.
8.1. Pooling higher order derivatives. We ask whether, given some feature combination k =
(k ,...,k ), the marginal effect of increasing, say, k has a linear effect. That is, we can just as
1 M 1
simply allow for outcomes as we increase the intensity up a given feature that is not just a step
function, butonethatchecksifthereisalinearrelationship.19 Inthiscase, thereisno“large” versus
“small” effect and no natural pool in the space of levels. However, there is a natural low dimensional
effect and even pools when considering the space of slopes. The result is a framework that captures
extensions of Bayesian treed models (e.g., Chipman et al. (2002)).
Before we proceed, we first generalize the notion of pools described in Definition 2.
19Extensionsofthiskindcanbemadetoaccommodatehigherorderderivativesandotherbasesaswell,e.g.,sinusoidal
effects.RASHOMON PARITITONS FOR HETEROGENEITY 36
Definition 8 (Generalization of pools). Given M features taking on R partially ordered values each
and some function g(k,β), a pool π is a set of feature combinations k whose outcomes are given by
g(k,β ) where β depends on π.
π π
ItiseasytoseethattheoriginalpooldefinedinDefinition2isrecoveredbysettingg(k,β ) = β .
π π
For instance, suppose we are interested in linear effects. Then the regression equation for pool π
M
(cid:88) ⊺
y = g(k,β ) = β + β k = β +β k
π π,0 π,m m π,0 π
m=1
where β is the linear slope within that pool. The estimated outcome for feature combination k ∈ π
π
is y (cid:98)= f(k,β(cid:98)π;Π), where β(cid:98)π is estimated within each pool using some procedure like least squares.
For some partition Π, define the block vector β = [β ,...,β ] where π ∈ Π. Then, the
π1 π
|Π|
i
general outcome function for any feature combination k can be written as
(cid:88)
y = g(k,β;Π) = I{k ∈ π}g(k,β ).
π
π∈Π
The practitioner is free to choose any domain-specific parametric function. For example, g could
be a higher-order Taylor series-like expansion. Or, g could even be sinusoidal because the practi-
tionerbelievestheoutcomesare(piece-wise)sinusoidal. Ofcourse, themorecomplextheestimation
procedure for β, the harder it is to enumerate the RPS.
Observe that the form of the posterior remains the same,
(cid:40) n (cid:41)
1 (cid:88)
P(Π | Z) ∝ exp{−L(Z)+λH(Π)} = exp − (y −y )2+λH(Π) .
i (cid:98)i
n
i=1
Therefore, the results in Section 3 still hold. We can freely choose any other non-negative loss
function, L(Z), and still use the same framework and algorithm to enumerate the RPS.
Further, the results in Section 5 are also valid when using an arbitrary parametric outcome
function as discussed here. We summarize this in Theorem 7.
Theorem 7. Suppose the outcome function is g(k,β;Π) for feature combination k, admissible
partition Π, and some unknown parameter β. Let us denote the estimated outcome for unit i with
feature combination k by y
(cid:98)i
= g(k,β(cid:98);Π) where β(cid:98) is estimated from the data. If we use y
(cid:98)i
instead of
µ in Equations 8 and 10, then
(cid:98)π
(i) Theorem 4 is still true,
(ii) Theorem 5 is still true, and
(iii) Algorithm 1 correctly enumerates the Rashomon partitions for outcome function f.RASHOMON PARITITONS FOR HETEROGENEITY 37
Figure 9. The black line corresponds to the true data-generating process and the
blue lines correspond to effects estimated in each model in the Rashomon set. We
estimate the outcome of each pool as a linear function of the features. The denser
the blue line, the more often it appears in the Rashomon set.
Toseetheusefulnessofthegeneralization, supposeweareinterestedinhowaperson’sageaffects
theirresponseto a treatmentconsisting ofa combination oftwo drugs, A and B.Supposethat there
are four possible dosages for drug A, {0,1,2,3}, six possible dosages for drug B, {0,1,2,3,4,5},
and people are classified as young aged or old aged where 0 indicates control. Suppose that the
treatment effects are piecewise linear (which generalizes the stepwise effects that we’ve assumed
in previous simulations). We illustrate the treatment effects for different combinations in black
dashed lines in Figure 9. By choosing a linear function as the outcome for each pool, we can find
the Rashomon set. In Figure 9, we show the estimated linear curves in 100 models present in the
Rashomon set (ϵ ≈ 5×10−4) in blue. The denser the blue line, the more often it appears in the
Rashomon set. We discuss the exact simulation setup in Appendix G.
8.2. Sparse correlation structure between coefficients. Next, we explore the space of poten-
tial (sparse) covariance matrices between the coefficients. We now apply the Hasse structure to the
elements of the variance-covariance matrix and pool on the space of covariances rather than theRASHOMON PARITITONS FOR HETEROGENEITY 38
coefficients themselves. This generalization requires an additional distributional assumption on the
coefficients. Specifically, assume that
β | µ,Λ ∼ N(µ,Λ)
where µ is some mean matrix and Λ is some covariance matrix. Then the posterior has the form
P(β,Λ,Π | Z) ∝ P(y | β,Λ,D,Π)·P(β,Λ,Π)
The likelihood component of the loss is
(cid:26) (cid:27)
1
⊺
P(y | β,Λ,D,Π) ∝ exp − (y−Dβ) Λ(y−Dβ) ,
N
where N is the number of observed data points.
We do not have additional information about the covariance structure (though this could of
course also be included in a prior) beyond the following three assumptions. First, we think that
Λ is dense i.e., Λ is sparse in the number of uncorrelated outcomes. Second, we neither know nor
want to know the correlation: it is an ℓ problem. Third, we assume independence across the mean
0
and correlation conditional on the covariance pooling. That is, Π is sufficient for the existence of
dependence. Then
P(β,Λ,Π) = P(β | Λ,Π)·P(Λ | Π)·P(Π)
Suppose that we have a partition Π = {π ,...,π } where H = |Π| and Π now is defined in
1 H
the space of covariance matrices, so pooling setting elements of the covariance matrix to be equal.
Then, consider the following procedure for drawing the covariance matrix, Λ ∈ RK×K. For each
pool π ∈ Π, draw Λ ∼ f independently where f is some prior (for example, inverse Wishart).
i i i i
Then, Λ = diag(Λ ,...,Λ ). The number of non-zero elements of Λ is given by ∥Λ∥ = (cid:80)H h2.
i H 0 i=1 i
Therefore, we penalize the number of zero elements, K2−(cid:80)H h2. Thus, the prior is
i=1 i
(cid:40) (cid:32) H (cid:33)(cid:41)
(cid:88)
P(Π) ∝ exp −λ K2− h2 .
i
i=1
So our penalized loss function is just weighted mean-squared error penalized differently,
(cid:32) H (cid:33)
(11) Q(Π;Z) = L(Π;Z)+λH(Π) = 1 (y−Dβ)⊺ Λ(y−Dβ)+λ K2−(cid:88) h2 .
n i
i=1
Theorem 8. Consider the same setup in Section 5 except the loss function is weighted mean squared
error penalized by the number of zeros in the covariance matrix as in Equation 11. Specifically,RASHOMON PARITITONS FOR HETEROGENEITY 39
Equations (8) and (9) are modified, respectively, as
1 (cid:88) (cid:88)
(12) b(Σ,M;Z) =
n
I{k(i) ∈ π f}Λ(cid:98)2 k(i),k(i)(y i−µ (cid:98)π)2+λH(Π,M),
π∈Πfk(i)∈π
(13) b eq(Σ,M;Z) = n1 (cid:88) (cid:88) I(cid:8) k(i) ∈ π fc(cid:9) Λ(cid:98)2 k(i),k(i)(y i−µ (cid:98)π)2.
π∈Πfk(i)∈π
where Λ(cid:98)2 is the estimated variance of feature combination k.
k,k
Then
(i) Theorem 4 is still true,
(ii) Theorem 5 is still true, and
(iii) Algorithm 1 correctly enumerates the Rashomon partitions.
9. Related work
Our work contributes based on ideas that are present in several vibrant literatures. In this
section, we contextualize our work in reference to three lines of existing work. We also provide a
more thorough discussion of four specific alternative approaches in Appendix E.
9.1. Related work on the Rashomon effect. Our work is, of course, related to literature prior
work grappling with the Rashomon effect (Chatfield, 1995; Breiman, 2001; McAllister, 2007; Tu-
labandhula and Rudin, 2014; D’Amour et al., 2022; Zhong et al., 2023). One line, reminiscent of
dealing with p-hacking, identifies sets of estimands that generate similar objective function val-
ues (Marx et al., 2020; Coker et al., 2021; Watson-Daniels et al., 2023) and has been explored in the
context of variable importance (Fisher et al., 2019; Dong and Rudin, 2020). Model multiplicity is
now being recognized as an important problem in fields such as fairness and causal inference (Black
et al., 2022; Pawelczyk et al., 2020; Kobylińska et al., 2023). The most related is Xin et al. (2022),
whoidentifyϵ-Rashomonsetsandadecisiontreealgorithmtoenumeratethesetofestimands(trees)
that have squared loss smaller than a threshold slightly higher than that of a reference model.
Our work considers the Rashomon effect when addressing pressing questions in statistical infer-
ence and decision-making. We develop a Bayesian framework and define the RPS as the set of
models with high posterior probability. This framework allows for unified inference across parti-
tions and for specific effects. We can also provide a bound on the error in estimating the posterior
using only the RPS. We show how to practically estimate the effects of different policies or feature
combinations using the RPS and show that the error vanishes quickly in our empirical simulations.
We show how to enumerate the entire RPS in the regression setting using scientifically sensible
restrictions to cut down our search space. Finally, we formalize the notion of simple models usingRASHOMON PARITITONS FOR HETEROGENEITY 40
the ℓ penalty as a sparsity constraint. We show in Theorem 2 that, in the absence of information
0
about the correlation structure of the parameters, the ℓ penalty is minimax optimal. Semenova
0
et al. (2022) hypothesized and showed using empirical simulations that regularization changes the
size of the RPS. We establish and prove this relationship for the ℓ penalty in Theorem 3.
0
9.2. RelatedworkonBayesianmodeluncertainty. Inourwork,weaddressuncertaintyacross
plausible models of heterogeneity. Our goal is to identify cases where distinct elements of the
factorial have (nearly) indistinguishable outcomes, which we accomplish by creating partitions of
the space of covariate interactions (though we generalize our approach to smoothing covariance
matrices in Section 8). In this sense, our setup is reminiscent of other work on Bayesian tree models
that leverage priors over partitions, or trees (e.g., Chipman et al. (1998), Denison et al. (1998),
Wu et al. (2007) or Bayesian Additive Regression Trees (BART) (Chipman et al., 2010)). Like this
work, we put priors over complexity in the space of trees. In contrast to many subsequent papers
in this line of work, our goal is not solely or even principally prediction, but the identification of
sets of combinations of characteristics that are heterogeneous with respect to the outcome. We use
Hasse diagrams that obey admissibility criteria and, critically, our computational approach does
not involve sampling from the posterior, but rather identifying partitions that make up the RPS.
Enumerating the RPS allows researchers to focus on a set of the highest posterior explanations for
heterogeneity while avoiding the computational issues associated with sampling the extremely large
space of trees. We also demonstrate in Section 8 how to extend our framework to functions across
pools (see, for example Chipman et al. (2002)). Our approach is also related to Bayesian Model
Averaging (BMA), where each element of the model space is inherently meaningful (Raftery et al.,
1997; Clyde, 2003). The notion of using a small set of simple models with high posterior probability
models arises in Madigan and Raftery (1994) in the context of BMA for graphical models and more
generallyinMadiganetal.(1996). UnlikeBMA,though, thedimensionofβ staysfixedthroughout,
though there are restrictions on β given a particular partition. This feature avoids the need for
the computational issues associated with searching the extremely large space of highly correlated
models of different dimensions (Raftery et al., 1997; Hans et al., 2007; Onorante and Raftery, 2016)
while preserving interpretability and a unified Bayesian inference framework. Analogously, Tian
and He (2009) and Chen and Tian (2014) use this for causal discovery by finding high posterior
equivalence classes of causal Bayesian networks.
9.3. Related work on learning treatment heterogeneity with machine learning tools. A
rapidly growing literature leverages ideas from machine learning to estimate treatment effect het-
erogeneity. Our approach is most closely related to Banerjee et al. (2021) (prior work in part byRASHOMON PARITITONS FOR HETEROGENEITY 41
Chandrasekhar and Sankar), which developed the Hasse representation for treatment variant aggre-
gation for a factorial randomized controlled trial. Their technique employs ℓ regularization (Lasso)
1
to pool treatment combinations. To grapple with the correlations in the design matrix from the fac-
torial data, they employ a Puffer transformation (Jia and Rohe, 2015) to satisfy irrepresentability.
They only prove that this transformation can be used for fully crossed RCTs since it is not obvious
that the conditions are satisfied for generically factorialized covariate data. Our approach differs
fundamentally in several ways, beyond taking a Bayesian rather than frequentist approach. First,
we focus on robustness and allow for uncertainty in the selected model through the RPS. There
is no such approach in their work. Second, regularizing using an ℓ penalty imposes independence
1
across adjacent models. This is exactly the opposite of what we would expect in practice (two
treatment conditions with the same drugs at slightly different levels should have related outcomes).
Third, their approach to robustness is to perturb the ℓ penalization parameter. But this traces out
1
a limited family of models for two reasons. To see this, notice that fundamentally the Rashomon
Effect is about multimodality: the ℓ approach privileges modes that are more consistent with in-
1
dependent priors irrespective of the penalization parameter. So this approach does not address the
Rashomon Effect. Further, the Lasso approach in some sense is limited by roughly being a greedy
algorithm: misleading local minima can severely affect which model is selected as optimal since it
cannot explore robust alternatives. Our robust prior approach immediately takes us to a decision
tree strategy that can explore beyond local minima. Fourth, their regression approach requires
recovering irrepresentability from a correlated design matrix, and the techniques are shown to work
only in the crossed RCT setting. Our approach with the decision tree strategy and ℓ prior applies
0
to arbitrary factorial data structures since it is agnostic to the correlational structure.
Our work is also related to existing tree-based methods (e.g. the seminal work of Wager and
Athey (2018) on causal forests in the context of treatment heterogeneity). Wager and Athey
(2018) construct regression trees (every tree corresponding to some partition Π in our language) to
describe heterogeneity in the space of covariates and then sample from the distribution over trees
to (honestly) estimate conditional average treatment effects. Honesty here refers to an iterative
sample splitting strategy to alleviate issues with estimating the tree and then doing inference using
the same data. Both their thought experiments and goals are distinct. Beyond being Bayesian
(which philosophically addresses “honesty”), our approach departs in several ways. First, in our
setting, data are partially ordered, a restriction that is not captured in the regression trees. Second,
we have a coherency requirement: without admissibility restrictions, unrestricted decision trees will
put considerable mass on and arrive at partitions that are not real-world meaningful and yet beRASHOMON PARITITONS FOR HETEROGENEITY 42
incorporated into their estimator. This is not desirable and we rule this out. Third, the sampling
over trees means that there is no guarantee that the selected trees (or Πs) will be high quality.
We provide guarantees, by definition of the RPS, on the quality of the selected partitions and we
enumerate all of them. Fourth, to understand whether two adjacent feature combinations should
be pooled, in their approach each of these queries must be tested individually which quickly runs
into multiple hypothesis issues when done in mass. But our approach natively returns a posterior
over all partitions and therefore jointly over all poolings. This delivers output amenable to theory-
building: “archetypes” of pools that robustly exhibit certain effects. Fifth, the manner in which
tree depth is controlled involves ensuring leafs (pools) have enough observations. This is sensible
from a certain perspective, but for our purposes corresponds to the statistician having a prior that
their data collection process stratifies observations against unknown true partition structure. This
is both not a reasonable prior on first principles and also one that changes as one samples more
data, since its shape is defined by the data collection itself. Taken together, these features make it
impossible to explore multiple potential models for heterogeneity, which is a fundamental goal of
our work.
Finally, we contrast our work with recent work in econometrics that uses machine learning “prox-
ies” tostudyheterogeneityintreatmenteffects. Thelogicisthatinsettingswithamoderate-to-high
dimensional covariate structure and little information about the relationship between covariates,
machine learning tools can effectively capture patterns of how covariates are associated with hetero-
geneityintreatmenteffectsratherthantheexactcovariate-basedeffects. Anovelmethoddeveloped
in Chernozhukov et al. (2018), for example, constructs a framework for inference using proxies con-
structed by an arbitrary machine learning model. After constructing proxies (predictions of the
outcome using features flexibly), Chernozhukov et al. (2018) cluster respondents into groups with
the highest and lowest treatment efficacy using treatment outcomes predicted based on the proxies.
These clusters, which are derived from amalgamating covariates through “black box” machine learn-
ing algorithms, can then be related back to observable covariates. We are interested in a different
set of goals: rather than finding what features are associated with heterogeneity (and more extreme
effects), we want to identify robust poolings.20 The outputs of the proxy techniques do not lend
themselves to addressing these questions nor do they readily provide any comment on robustness
for the same reasons as those faced by the causal forests techniques previously discussed. Our work
shows that admissibility restrictions and the geometry of the underlying problem provide enough
20Further, robustness is an exact finite sample n calculation, so in some sense, we are not worried about the high
dimensional case of the number of parameters exploding relative to the number of observations.RASHOMON PARITITONS FOR HETEROGENEITY 43
structure to make search in the space of covariates possible and interpretable, alleviating the need
to use black box algorithms to summarize relationships between covariates through proxies.
10. Discussion
In this paper, we present an approach for estimating heterogeneity in outcomes based on a set of
discrete covariates. We derive a fully Bayesian framework and an algorithm to identify all possible
poolingacrossfeaturecombinationswiththehighestposteriordensity: theRashomonPartitionSet.
We provide bounds on the portion of the posterior captured by models for heterogeneity in this set,
allowing researchers to compute posteriors for marginal effects and evaluate specific treatment com-
binations. Appealing to a Bayesian framework addresses the issue of multiple testing/selection that
leaves practitioners with the unappealing choice between invalid inference and procedures such as
datasplittingthathaveimplicationsforpower, whichareparticularlyproblematicinsettingswhere
we expect the cost of data collection to be high. Meanwhile, by identifying a set of high posterior
models rather than sampling from the entire posterior, we avoid the inefficiency and impracticality
of existing Bayesian approaches to model uncertainty. By only considering scientifically plausible
pools in a geometry that allows for partial ordering (Hasse diagrams), we substantially reduce the
number of possible explanations for heterogeneity without sacrificing flexibility. Additionally, and
critically, thesechoicesmeanthattheresultinghighposteriorpartitionsareinterpretable anduseful
for researchers and policymakers when designing future interventions or generalizations.
We now highlight two additional philosophical points about our approach. First, our approach
is fundamentally generative in the sense that it produces insights that are directly interpretable.
As we highlight in our empirical examples, we expect that Rashomon partitions themselves will
be of interest for researchers or policymakers. They allow for the identification of the most ro-
bust conclusions, settings where policymakers can intervene without worrying about likely negative
consequences, and defining “archetypes” for theory-building.
In this way, our work contributes to a growing literature in artificial intelligence and machine
learning that pushes back on the use of black box algorithms to make high-stakes decisions (see
e.g., Rudin (2019)). While machine learning models may be effective at estimating complex rela-
tionships between covariates, they also often do so in ways that obfuscate the influence of particular
features(orcombinationsoffeatures). Ourapproachpresentsanalternativestrategythatgenerates
insights about sources of treatment effect heterogeneity based on combinations of observed covari-
ates. Our work shows that admissibility restrictions and the geometry of the underlying problem
provide enough structure to make search in the space of covariates possible, alleviating the need to
use black box algorithms to summarize relationships between covariates through proxies.RASHOMON PARITITONS FOR HETEROGENEITY 44
Second, our work highlights the aperture that exists between statistical and practical decision-
making. We take as given that in many moderate to high dimensional settings there will be inter-
actions between features. With finite data, the result is a set of possible models for heterogeneity
whose statistical performance is indistinguishable. Said another way, our work posits that the quest
forthe“best” statisticalmodelisSisypheaninessentiallyanyscientificallyinterestingsetting. While
this may seem dire, it actually presents an opportunity to involve additional factors beyond model
performance that are often critical in practice for making decisions. Amongst models in the RPS,
a policymaker could choose based on, for example, implementation cost, equity considerations, or
preserving privacy without sacrificing statistical performance.
There are many promising areas for future work in extending the framework we present here.
First, we present results in terms of a posterior in a Bayesian framework. We could, however, also
construct a similar structure under a frequentist paradigm. In such a setup, we would need to
explore a re-splitting strategy (see Wager and Athey (2018), for example) to construct an “honest”
set of Hasse diagrams. Furthermore, we could use our approach to identify groups that are sys-
tematically underrepresented in randomized trials (see Parikh et al. (2024), for example) and, as
a further generalization, to compare results across trials (see for example Meager (2019)). Finally,
our computational approach could be more generally valuable in a wide range of settings. In the
context of model selection for graphical models, for example Madigan and Raftery (1994) suggest
a stochastic search strategy that leverages the structure of the graph. Unlike ours, however, their
approach averages over discrete models, leaving open the potential for an approach similar to ours
in the context of graphical models or discrete model averaging more generally.RASHOMON PARITITONS FOR HETEROGENEITY 45
References
Aakvik, A., Salvanes, K. G., and Vaage, K. (2010). Measuring heterogeneity in the returns to
education using an education reform. European Economic Review, 54(4):483–500.
Agrawal, D., Pote, Y., and Meel, K. S. (2021). Partition function estimation: A quantitative study.
arXiv preprint arXiv:2105.11132.
Alder, J. K., Hanumanthu, V. S., Strong, M. A., DeZern, A. E., Stanley, S. E., Takemoto, C. M.,
Danilova, L., Applegate, C. D., Bolton, S. G., Mohr, D. W., et al. (2018). Diagnostic utility
of telomere length testing in a hospital-based setting. Proceedings of the National Academy of
Sciences, 115(10):E2358–E2365.
Andrews, I., Kitagawa, T., and McCloskey, A. (2019). Inference on winners. Technical report,
National Bureau of Economic Research.
Angelino, E., Larus-Stone, N., Alabi, D., Seltzer, M., and Rudin, C. (2017). Learning certifiably op-
timalrulelists. InProceedings of the 23rd ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 35–44.
Angelucci,M.,Karlan,D.,andZinman,J.(2015).Microcreditimpacts: Evidencefromarandomized
microcreditprogramplacementexperimentbycompartamosbanco. American Economic Journal:
Applied Economics, 7(1):151–182.
Attanasio, O., Augsburg, B., De Haas, R., Fitzsimons, E., and Harmgart, H. (2015). The impacts
of microfinance: Evidence from joint-liability lending in mongolia. American Economic Journal:
Applied Economics, 7(1):90–122.
Augsburg, B., De Haas, R., Harmgart, H., and Meghir, C. (2015). The impacts of microcre-
dit: Evidence from bosnia and herzegovina. American Economic Journal: Applied Economics,
7(1):183–203.
Baland, J.-M., Somanathan, R., Vandewalle, L., et al. (2008). Microfinance lifespans: A study
of attrition and exclusion in self-help groups in india. In India policy forum, volume 4, pages
159–210. National Council of Applied Economic Research.
Banerjee, A., Breza, E., Duflo, E., and Kinnan, C. (2019). Can microfinance unlock a poverty trap
for some entrepreneurs? Technical report, National Bureau of Economic Research.
Banerjee, A., Chandrasekhar, A. G., Dalpath, S., Duflo, E., Floretta, J., Jackson, M. O., Kannan,
H., Loza, F. N., Sankar, A., Schrimpf, A., et al. (2021). Selecting the most effective nudge:
Evidence from a large-scale experiment on immunization. Technical report, National Bureau of
Economic Research.RASHOMON PARITITONS FOR HETEROGENEITY 46
Banerjee, A., Duflo, E., Glennerster, R., and Kinnan, C. (2015). The miracle of microfinance?
evidencefromarandomizedevaluation. American economic journal: Applied economics, 7(1):22–
53.
Bénard, C. and Josse, J. (2023). Variable importance for causal forests: breaking down the hetero-
geneity of treatment effects. arXiv preprint arXiv:2308.03369.
Bissiri, P. G., Holmes, C. C., andWalker, S. G. (2016). Ageneral framework for updating belief dis-
tributions. Journal of the Royal Statistical Society Series B: Statistical Methodology, 78(5):1103–
1130.
Black, E., Raghavan, M., and Barocas, S. (2022). Model multiplicity: Opportunities, concerns,
and solutions. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and
Transparency, pages 850–863.
Blackburn,D.F.andWilson,T.W.(2006). Antihypertensivemedicationsandbloodsugar: theories
and implications. Canadian Journal of Cardiology, 22(3):229–233.
Breiman, L. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the
author). Statistical Science, 16(3):199–231.
Cascorbi, I. (2012). Drug interactions – principles, examples and clinical consequences. Deutsches
Ärzteblatt International, 109(33-34):546.
Chae, D. H., Nuru-Jeter, A. M., Adler, N. E., Brody, G. H., Lin, J., Blackburn, E. H., and Epel,
E. S. (2014). Discrimination, racial bias, and telomere length in african-american men. American
journal of preventive medicine, 46(2):103–111.
Chatfield, C. (1995). Model uncertainty, data mining and statistical inference. Journal of the Royal
Statistical Society Series A: Statistics in Society, 158(3):419–444.
Chen, Y. and Tian, J. (2014). Finding the k-best equivalence classes of bayesian network structures
for model averaging. In Proceedings of the AAAI conference on artificial intelligence, volume 28.
Chernozhukov, V., Demirer, M., Duflo, E., and Fernandez-Val, I. (2018). Generic machine learning
inference on heterogeneous treatment effects in randomized experiments, with an application to
immunization in India. Technical report, National Bureau of Economic Research.
Chipman,H.A.,George,E.I.,andMcCulloch,R.E.(1998). BayesianCARTmodelsearch. Journal
of the American Statistical Association, 93(443):935–948.
Chipman, H. A., George, E. I., and McCulloch, R. E. (2002). Bayesian treed models. Machine
Learning, 48:299–320.
Chipman, H. A., George, E. I., and McCulloch, R. E. (2010). BART: Bayesian additive regression
trees. The Annals of Applied Statistics, 4(1).RASHOMON PARITITONS FOR HETEROGENEITY 47
Clyde, M. (2003). Model averaging. Subjective and objective Bayesian statistics, pages 636–642.
Coker, B., Rudin, C., and King, G. (2021). A theory of statistical inference for ensuring the
robustness of scientific results. Management Science, 67(10):6174–6197.
Crépon,B.,Devoto,F.,Duflo,E.,andParienté,W.(2015). Estimatingtheimpactofmicrocrediton
those who take it up: Evidence from a randomized experiment in morocco. American Economic
Journal: Applied Economics, 7(1):123–150.
D’Amour, A., Heller, K., Moldovan, D., Adlam, B., Alipanahi, B., Beutel, A., Chen, C., Deaton, J.,
Eisenstein, J., Hoffman, M.D., etal.(2022). Underspecificationpresentschallengesforcredibility
in modern machine learning. The Journal of Machine Learning Research, 23(1):10237–10297.
Denison, D. G., Mallick, B. K., and Smith, A. F. (1998). A bayesian CART algorithm. Biometrika,
85(2):363–377.
Dong, J. and Rudin, C. (2020). Exploring the cloud of variable importance for the set of all good
models. Nature Machine Intelligence, 2(12):810–824.
Fisher,A.,Rudin,C.,andDominici,F.(2019). Allmodelsarewrong,butmanyareuseful: Learning
a variable’s importance by studying an entire class of prediction models simultaneously. J. Mach.
Learn. Res., 20(177):1–81.
Flajolet, P. and Sedgewick, R. (2009). Analytic Combinatorics. Cambridge University Press.
Forster, A. G., van de Werfhorst, H. G., and Leopold, T. (2021). Who benefits most from college?
dimensions of selection and heterogeneous returns to higher education in the united states and
the netherlands. Research in Social Stratification and Mobility, 73:100607.
Gelman, A. (2006). Prior distributions for variance parameters in hierarchical models (comment on
article by browne and draper).
Geronimus, A. T., Pearson, J. A., Linnenbringer, E., Schulz, A. J., Reyes, A. G., Epel, E. S., Lin,
J., and Blackburn, E. H. (2015). Race-ethnicity, poverty, urban stressors, and telomere length in
a detroit community-based sample. Journal of health and social behavior, 56(2):199–224.
Hahn, P. R., Murray, J. S., and Carvalho, C. M. (2020). Bayesian regression tree models for causal
inference: Regularization, confounding, and heterogeneous effects (with discussion). Bayesian
Analysis, 15(3):965–1056.
Hamad, R., Tuljapurkar, S., and Rehkopf, D. H. (2016). Racial and socioeconomic variation in
genetic markers of telomere length: a cross-sectional study of us older adults. EBioMedicine,
11:296–301.
Hammer, S. M., Squires, K. E., Hughes, M. D., Grimes, J. M., Demeter, L. M., Currier, J. S.,
Eron Jr, J. J., Feinberg, J. E., Balfour Jr, H. H., Deyton, L. R., et al. (1997). A controlledRASHOMON PARITITONS FOR HETEROGENEITY 48
trial of two nucleoside analogues plus indinavir in persons with human immunodeficiency virus
infectionandcd4cellcountsof200percubicmillimeterorless. New England Journal of Medicine,
337(11):725–733.
Hans, C., Dobra, A., and West, M. (2007). Shotgun stochastic search for “large p” regression.
Journal of the American Statistical Association, 102(478):507–516.
Hu, X., Rudin, C., and Seltzer, M. (2019). Optimal sparse decision trees. Advances in Neural
Information Processing Systems, 32.
Jia, J. and Rohe, K. (2015). Preconditioning the lasso for sign consistency. Electronic Journal of
Statistics, 9:1150–1172.
Karlan,D.andList,J.A.(2007). Doespricematterincharitablegiving? evidencefromalarge-scale
natural field experiment. American Economic Review, 97(5):1774–1793.
Karlan, D. and Zinman, J. (2010). Expanding credit access: Using randomized supply decisions to
estimate the impacts. The Review of Financial Studies, 23(1):433–464.
Kobylińska, K., Krzyziński, M., Machowicz, R., Adamek, M., and Biecek, P. (2023). Exploration of
rashomon set assists explanations for medical data. arXiv preprint arXiv:2308.11446.
Madigan, D. and Raftery, A. E. (1994). Model selection and accounting for model uncertainty
in graphical models using occam’s window. Journal of the American Statistical Association,
89(428):1535–1546.
Madigan, D., Raftery, A. E., Volinsky, C. T., and Hoeting, J. A. (1996). Bayesian model averaging.
Integrating Multiple Learned Models (IMLM-96), (P. Chan, S. Stolfo, and D. Wolpert, eds).
Marx, C., Calmon, F., and Ustun, B. (2020). Predictive multiplicity in classification. In III, H. D.
and Singh, A., editors, Proceedings of the 37th International Conference on Machine Learning,
volume 119 of Proceedings of Machine Learning Research, pages 6765–6774. PMLR.
McAllister, J. W. (2007). Model selection and the multiplicity of patterns in empirical data. Phi-
losophy of Science, 74(5):884–894.
Meager, R. (2019). Understanding the average impact of microcredit expansions: A bayesian hier-
archical analysis of seven randomized experiments. American Economic Journal: Applied Eco-
nomics, 11(1):57–91.
Mincer, J. (1958). Investment in human capital and personal income distribution. Journal of
Political Economy, 66(4):281–302.
Nair, N. U., Greninger, P., Zhang, X., Friedman, A. A., Amzallag, A., Cortez, E., Sahu, A. D., Lee,
J. S., Dastur, A., Egan, R. K., et al. (2023). A landscape of response to drug combinations in
non-small cell lung cancer. Nature Communications, 14(1):3830.RASHOMON PARITITONS FOR HETEROGENEITY 49
Onorante, L. and Raftery, A. E. (2016). Dynamic model averaging in large model spaces using
dynamic occam’s window. European Economic Review, 81:2–14.
Parikh, H., Ross, R., Stuart, E., and Rudolph, K. (2024). Who are we missing? a principled
approach to characterizing the underrepresented population. arXiv preprint arXiv:2401.14512.
Pawelczyk, M., Broelemann, K., and Kasneci, G. (2020). On counterfactual explanations under
predictive multiplicity. In Peters, J. and Sontag, D., editors, Proceedings of the 36th Conference
on Uncertainty in Artificial Intelligence (UAI), volume 124 of Proceedings of Machine Learning
Research, pages 809–818. PMLR.
Protsenko, E., Rehkopf, D., Prather, A. A., Epel, E., and Lin, J. (2020). Are long telomeres better
than short? relative contributions of genetically predicted telomere length to neoplastic and
non-neoplastic disease risk and population health burden. PloS one, 15(10):e0240185.
Raftery, A. E., Madigan, D., and Hoeting, J. A. (1997). Bayesian model averaging for linear
regression models. Journal of the American Statistical Association, 92(437):179–191.
Rossiello, F., Jurk, D., Passos, J. F., and d’Adda di Fagagna, F. (2022). Telomere dysfunction in
ageing and age-related diseases. Nature cell biology, 24(2):135–147.
Rubin, D. B. (1981). Estimation in parallel randomized experiments. Journal of Educational
Statistics, 6(4):377–401.
Rudin, C. (2019). Stop explaining black box machine learning models for high stakes decisions and
use interpretable models instead. Nature machine intelligence, 1(5):206–215.
Semenova, L., Rudin, C., andParr, R.(2022). Ontheexistenceofsimplermachinelearningmodels.
InProceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pages
1827–1858.
Srinivas, N., Rachakonda, S., and Kumar, R. (2020). Telomeres and telomere length: a general
overview. Cancers, 12(3):558.
Tarozzi, A., Desai, J., and Johnson, K. (2015). The impacts of microcredit: Evidence from ethiopia.
American Economic Journal: Applied Economics, 7(1):54–89.
Tian, J. and He, R. (2009). Computing posterior probabilities of structural features in bayesian
networks. InProceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,
UAI ’09, pages 538–547, Arlington, Virginia, USA. AUAI Press.
Topley, K. (2016). Computationally efficient bounds for the sum of catalan numbers. arXiv preprint
arXiv:1601.04223.
Triplitt,C.(2006). Druginteractionsofmedicationscommonlyusedindiabetes. Diabetes Spectrum,
19(4):202.RASHOMON PARITITONS FOR HETEROGENEITY 50
Tulabandhula,T.andRudin,C.(2014). Robustoptimizationusingmachinelearningforuncertainty
sets. arXiv preprint arXiv:1407.1097.
Vyas, C. M., Ogata, S., Reynolds, C. F., Mischoulon, D., Chang, G., Cook, N. R., Manson, J. E.,
Crous-Bou, M., De Vivo, I., and Okereke, O. I. (2021). Telomere length and its relationships
with lifestyle and behavioural factors: variations by sex and race/ethnicity. Age and ageing,
50(3):838–846.
Wager, S. and Athey, S. (2018). Estimation and inference of heterogeneous treatment effects using
random forests. Journal of the American Statistical Association, 113(523):1228–1242.
Wambaugh, M. A., Denham, S. T., Ayala, M., Brammer, B., Stonhill, M. A., and Brown, J. C.
(2020). Synergistic and antagonistic drug interactions in the treatment of systemic fungal infec-
tions. Elife, 9:e54160.
Watson-Daniels, J., Parkes, D. C., and Ustun, B. (2023). Predictive multiplicity in probabilistic
classification. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages
10306–10314.
Wu, Y., Tjelmeland, H., and West, M. (2007). Bayesian CART: Prior specification and posterior
simulation. Journal of Computational and Graphical Statistics, 16(1):44–66.
Xin, R., Zhong, C., Chen, Z., Takagi, T., Seltzer, M., and Rudin, C. (2022). Exploring the
whole rashomon set of sparse decision trees. Advances in Neural Information Processing Sys-
tems, 35:14071–14084.
Zhao, P. and Yu, B. (2006). On model selection consistency of lasso. The Journal of Machine
Learning Research, 7:2541–2563.
Zhong, C., Chen, Z., Seltzer, M., and Rudin, C. (2023). Exploring and interacting with the set of
good sparse generalized additive models. arXiv e-prints, pages arXiv–2303.RASHOMON PARITITONS FOR HETEROGENEITY 51
Appendix A. Admissibility and Hasse diagrams
One way to understand admissibility is by arranging the data of feature combination assignments
into a feature variant aggregation design matrix, F ∈ {0,1}n,K. The entries of the matrix are as
follows. If k(i) is the feature combination that i is assigned to, we set
F := I{k(i) ≥ ℓ ∩ ρ(k(i)) = ρ(ℓ)}.
iℓ
So the variant design matrix switches on a dummy variable for all variants that are subordinate to
k(i). The utility is that it allows for us to understand the marginal value of climbing the ordering
up from k(i), as in the treatment variant aggregation (TVA) procedure of Banerjee et al. (2021).
To see this, it is useful to rewrite Equation (1) in its variant form,
(A.1) y = Fα+ϵ,
which is just a linear transformation of Equation (1), so that:
(cid:88)
(A.2) β = α
k k′
k′≤k;ρ(k)=ρ(k′)
This says that an expected outcome of a feature combination is the sum of expected marginal
values leading up to it.
It is useful to represent our framework in a Hasse diagram. The Hasse draws links between
features in the direction of the partial ordering We imagine that moving from one node to its
adjacent node in Hasse inherits a value that corresponds to the marginal change in the outcome
moving from an immediate subordinate variant to the present variant. In a setup with two features,
by Equation (A.2) the node (r,r′) has value α = (β −β )−(β −β ) wherever
r,r′ r,r′ r,r′−1 r−1,r′ r−1,r′−1
these indices on the right-hand side are not 0 (wherever the index is 0, we drop the corresponding
β term on the right-hand side – basically because the corresponding feature combination is in a
k
different profile). These will either capture a main effect of increasing a dosage (as on the sides
of the Hasse) or an interaction effect between multiple dosage increases (as in the interior of the
Hasse).
We discuss specific examples in Example A.1 and Example A.2 below.
ExampleA.1. ConsideranexamplewithM = 2features, eachwithR = 3discretevalues, {1,2,3}.
Then there are K = RM = 9 different feature combinations. The Hasse diagram is shown in Figure
A.1. So, we end up pooling (2,2) with (3,2) and (2,3) with (3,3). The corresponding Σ ∈ {0,1}2×2RASHOMON PARITITONS FOR HETEROGENEITY 52
[3,3] [3,3]
[2,3] [3,2] [2,3] [3,2]
[1,3] [2,2] [3,1] [1,3] [2,2] [3,1]
[1,2] [2,1] [1,2] [2,1]
[1,1] [1,1]
Figure A.1. Hasse diagram for Examples A.1 and A.2. The partition described in
Example A.1 is shown in blue ellipses on the left panel. The right panel describes a
different admissible partition in red ellipses seen in Example A.2
matrix for this profile is
 
0 1
Σ =  .
1 0
This indicates that we split variants with value 1 from value 2 in the first feature (by Σ = 0) and
11
pool variants of value 2 with value 3 in the first feature (by Σ = 1). Further, we pool variants with
12
value 1 and value 2 in the second feature (by Σ = 1) and split variants with value 2 from value 3
21
in the second features (by Σ = 0).
22
Example A.2. Consider the same setup in Example A.1 with M = 2 features, each with R = 3
discrete values, {1,2,3}. Another admissible partition can be defined by the matrix
 
1 0
Σ =  .
1 1
The pools are π = {(a,b) | a = {1,2},b = {1,2,3}} and π = {(a,b) | a = {3},b = {1,2,3}}. This
1 2
is illustrated in the right panel of Figure A.1.
Now,weturntoadmissibilityasdefinedinDefinition6. Case(1)simplycomesfromthedefinition
of a pool (cf. Definition 2). To understand the necessity of strong convexity in case (2) and the
parallel splitting criteria in case (3), we need to understand how the marginal increments α affect
k
the overall outcome β . Observe that α affect β for all feature combinations k′ ≥ k by Equation
k′ k k′
A.2. We can define this “sphere of influence” of k as A = {k′ ∈ K | ρ(k) = ρ(k′),k′ ≥ k}. Further,
k
when we are interested in the outcomes β , it is sufficient to consider only spheres A where α > 0
k′ k k
i.e., the “active spheres.” Therefore, the intersection of all active spheres (taken either directly orRASHOMON PARITITONS FOR HETEROGENEITY 53
through its complement) will give rise to a set of feature combinations with the same outcome i.e.,
a pool. It is easy to see that any such sphere is strongly convex in our sense. Therefore, any pool
will also be strongly convex.
The parallel splitting criteria “from above" in Case ((3)a) of Definition 6, also follows from these
spheres of influence interpretation of the active α . Specifically, tracking the active α ensures that
k k
if a segment through a Hasse is pooled, then any segment both parallel to it and below it must be
pooled as well. For the sake of contradiction, assume to the contrary that the top segment is pooled
while a parallel bottom segment is cleaved. There must be some node along the bottom segment
that was responsible for this cleaving through its marginal effect. However, the sphere of influence
of this marginal effect cuts through the top segment too, cleaving the top segment into distinct
pooled sets, a contradiction.
Of course, as mentioned in Section 2, it is possible to have exact marginal increments exactly
offset each other so that despite two feature combinations k ,k influenced by two different spheres
1 2
of action, β = β . However, we do not want to pool these features together because adding very
k1 k2
little noise to one of the corresponding active α will immediately render β ̸= β i.e., this is a
k′ k1 k2
measure zero event.
The above shows that admissibility (with case (3) limited to case ((3)a)) is necessary from just
using the spheres of influence of marginal effects, and nothing more. However, this version of
admissibility is also sufficient: any admissible partition Π (as per Definition 6) can be shown to
0
derive from a common set of nodes k ,...k so that each π ∈ Π = Aa1∩....∩Aan, where a ∈ {1,c},
1 n 0 k1 kn i
i.e. denoting either the sphere of influence or its complement. A quick proof sketch is as follows.
First, we define the spheres through Π . For each π ∈ Π , take k = minπ its unique minimum.
0 i 0 i i
These k will be the nodes that generate the spheres of influence. We will show that for any
i
p ,p ∈ π ∈ Π , p ∈ A ⇐⇒ p ∈ A . Observe that for any p ∈ π , then trivially p ∈ A . Now
1 2 i 0 1 kj 2 kj i ki
consider the case when p ∈ A for k ̸= k . Then, p > k . It is not possible that k ̸≶ k as this
kj j i j j i
would violate the parallel splits criteria (one can show there would be another π′ ∈ Π containing
0
p). It is also not possible for k > k as this would violate convexity (p > k > k would imply
j i j i
k ∈ π ). Thus, if p ∈ π and p ∈ A , then necessarily k ≤ k . Then, it follows that if p ,p ∈ π ,
j i i kj j i 1 2 i
then p ∈ A ⇐⇒ p ∈ A . Therefore, all members of each π ∈ Π are in the same unique
1 kj 2 kj 0
intersection of spheres of influence, so each pi ∈ Π is uniquely represented as π = Aa1 ∩....∩Aan.
0 k1 kn
Of course, in this particular parameterization of β, we chose to climb up the Hasse. We could
have alternatively chosen to climb down the Hasse as
(A.3) y = Gγ +ϵ,RASHOMON PARITITONS FOR HETEROGENEITY 54
(cid:88)
(A.4) β = γ ,
k k′
k′≥k;ρ(k)=ρ(k′)
where G := I{k(i) ≤ ℓ ∩ ρ(k(i)) = ρ(ℓ)}. The difference from Equation A.2 is in the indexing
iℓ
of the sum, k′ ≥ k. It is easy to see with an analogous sphere of influence argument with γ that
admissibility (With case (3) limited to ((3)b) this time) is necessary and sufficient characterization
of pools from active marginals in γ.
When the goal of the problem is to identify heterogeneity in β, there is no reason to prefer one
parameterization of climbing the Hasse over the other. Seeing that the parallel splitting criteria is
linked to robustly estimating the pools of heterogeneity, we want to obey both of them together at
the same time. This does run the risk of generating more granular partitions as a result of stronger
restrictions, but this is a small price to pay for robustly estimating heterogeneity when one wishes
to be agnostic about the system. Hence the full criterion for admissibility Definition 6, respecting
parallel splits from both above (case ((3)a)) and below (case ((3)b)).
One might imagine that asking for more restrictions can complicate the search process. However,
aby-productofbeingagnostictothedirectionofHassetraversalisthatthereisabijectivemapping
between the Σ partition matrices and admissible partitions. We show in Proposition 2 that this
significantly reduces the size of the model class, and later show in Theorem 3 that the size of the
RPS, which is our primary estimation goal, is only polynomial. This has very important practical
implications for computational feasibility.
One can quickly verify that Examples A.1 and A.2 satisfy the admissibility as defined in Defi-
nition 6 by visual inspection and identify the corresponding Σ matrices. In Example A.3 below,
we show an example of an inadmissible partition. Interestingly, there is a valid decision tree that
arrives at this partition.
Example A.3. Consider the same setup in Example A.1 with M = 2 features, each with R = 3
discrete values, {1,2,3}. In Figure A.2, we illustrate an inadmissible partition. This is inadmissible
because we have pools π = {(1,1),(1,2),(1,3)}, π = {(2,1),(2,2)}, π = {(3,1),(3,2)}, and
1 2 3
π = {(2,3),(3,3)}. Admissibility (see condition (3) of Definition 6) says that if π is in the
4 1
partition, thenfeaturecombinations(·,2)and(·,3)shouldalwaysbepooledtogether. Thiscontradicts
what we observe in π , π , and π . Similarly, if π is in the partition, admissibility would require
2 3 4 4
that feature combinations (2,·) and (3,·) need to be pooled together which contradicts π and π .
2 4
Since this partition is inadmissible, we cannot represent it using the Σ matrix.RASHOMON PARITITONS FOR HETEROGENEITY 55
X <2
1
[3,3]
π 1 X 2 >2
[2,3] [3,2]
[1,3] [2,2] [3,1] π 4 X 1 >2
[1,2] [2,1]
π π
3 2
[1,1]
Figure A.2. Hasse diagram with the inadmissible partition described in Exam-
ple A.3. The pools are π = {(1,1),(1,2),(1,3)}, π = {(2,1),(2,2)}, π =
1 2 3
{(3,1),(3,2)}, and π = {(2,3),(3,3)}. The decision tree illustrates how to gen-
4
erate this partition.
[5,3]
[4,3] [5,2]
[3,3] [4,2] [5,1]
[2,3] [3,2] [4,1]
[1,3] [2,2] [3,1]
[1,2] [2,1]
[1,1]
Figure A.3. Hasse diagram for Example A.4. The admissible partition is shown in
blue ellipses.
To see why this is inadmissible from the marginal perspective, let us look at π and π . From
3 4
these pools, it is evident that α ̸= 0, α ̸= 0, and α ̸= 0. And we know that,
2,3 3,1 3,3
β = α +α +α +α +C
3,3 3,3 3,2 3,1 2,3RASHOMON PARITITONS FOR HETEROGENEITY 56
β = α +C,
2,3 2,3
wherethetermC iscommontobothequations. Inthepoolingcurrently, itsohappensthatβ = β
3,3 2,3
– the terms α ,α ,α jointly make this true by α +α +α = 0. However, we know that
3,3 3,2 3,1 3,3 3,2 3,1
α ̸= 0. So if we add some noise ε > 0 to α to get α′ := α +ε. Then, β ̸= β anymore.
3,1 3,1 3,1 3,1 3,3 2,3
In other words, the pool π is not robust to noise in the non-zero marginals as any noise will almost
4
surely break π into {(2,3)} and {(3,3)} as separate pools. Hence, this partition is inadmissible.
4
Decision trees are not robust in this sense as they may generate inadmissible partitions. The right
panel of Figure A.2 illustrates a decision tree that generates the inadmissible partition discussed in
this example.
Example A.4. Consider a different setup with M = 2 features, The first feature takes on R = 5
1
discrete values {1,2,3,4,5} and the second feature takes on R = 3 discrete values, {1,2,3}. An
2
admissible partition can be defined by the matrix
 
1 0 1 0
Σ =  ,
1 0 − −
where we use “−” to denote that the second feature does not have dosages corresponding to those
entries in the Σ matrix. The pools are π = {(a,b) | a,b ≤ 2}, π = {(a,b) | a ≤ 2,b = 3},
1 2
π = {(a,b) | a = 3,4,b ≤ 2}, π = {(a,b) | a = 3,4,b = 3}, π = {(a,b) | a = 5,b ≤ 2}, and
3 4 5
π = {(5,3)}. This is illustrated in Figure A.3.
6
So far, we have been describing how to pool different feature combinations if they belong to the
same profile. Now, we turn our attention to pooling across profiles. The marginal representation in
Equation A.1 does not allow for features to be pooled together if they are in different profiles. We
now describe a similar representation that allows features to be pooled across profiles if and only if
they are variants,
(A.5) y = Fα+Aδ+ϵ,
A = I{k(i) > ℓ ∩ ρ(k(i)) > ρ(ℓ) = ρ},
i,(ℓ,ρ)
(cid:88) (cid:88) (cid:88)
(A.6) β = α + δ .
k k′ k′,ρ
k′≤k;ρ(k)=ρ(k′) k′<kρ;ρ(k′)<ρ(k)
By setting δ = 0, we can immediately see that Equation (A.5) is a generalization of Equation
(A.1). In fact, if depending on the context, we do not want to pool profile ρ with ρ , then this
1 2RASHOMON PARITITONS FOR HETEROGENEITY 57
corresponds to setting the appropriate entries in δ to 0. This is exactly what Banerjee et al. (2021)
do in their analysis of cross-randomized behavioral nudges for improving immunization.
This representation agrees with our admissibility in Definition 7. Case (1) follows from the fact
that this is a generalization of Equation A.1. Cases (2) and (3) follow from the definition of the A
matrix. For example, consider two features k ,k that belong to two different profiles. We can only
1 2
pool variants i.e., ||k −k || = 1. If they are variants, then the two profiles must be adjacent on
1 2 1
the M-d hypercube.
At this point, it is important to note that there are no restrictions such as the parallel splitting
criteria across different profiles. This is because the marginal δ only contributes to the outcome
k
acrossprofilesi.e.,fromtheperspectivewithinaprofile,thesphereofinfluenceofδ isindistinguish-
k
able from the sphere of influence of α where k′ is at the lower boundary of the Hasse adjacent to
k′
the Hasse of k. Since each pair of feature variants from different profiles have different across-profile
marginals δ , they are not coupled together like the α marginals are.
k
Appendix B. Approximating the posterior
Proof of Theorem 1. By the triangle inequality, we can write
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:88) P(Π | Z) (cid:88) (cid:12)
sup(cid:12)F
β|Z,P
θ(t)−F β|Z(t)(cid:12) = sup(cid:12)
(cid:12)
F β|Z(t | Π)
(cid:80) P(Π′ | Z)
− F β|Z(t | Π)P(Π | Z)(cid:12) (cid:12),
t t (cid:12)Π∈P θ Π′∈P θ Π∈P⋆ (cid:12)
≤ (I)+(II)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:88) P(Π | Z) (cid:88) (cid:12)
I = sup(cid:12)
(cid:12)
F β|Z(t | Π)
(cid:80) P(Π′ | Z)
− F β|Z(t | Π)P(Π | Z)(cid:12) (cid:12),
t (cid:12)Π∈P θ Π′∈P θ Π∈P θ (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:88) (cid:88) (cid:12)
II = sup(cid:12) F β|Z(t | Π)P(Π | Z)− F β|Z(t | Π)P(Π | Z)(cid:12)
(cid:12) (cid:12)
t
(cid:12)Π∈P Π∈P⋆ (cid:12)
θ
Let us denote K = (cid:80) P(Π′ | Z). Then the first term is,
Π′∈P
θ
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:88) P(Π | Z) (cid:88) (cid:12)
(I) = sup(cid:12)
(cid:12)
F β|Z(t | Π)
K
− F β|Z(t | Π)P(Π | Z)(cid:12)
(cid:12)
t
(cid:12)Π∈P Π∈P (cid:12)
θ θ
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) 1 (cid:12) (cid:12) (cid:88) (cid:12)
≤ (cid:12)
(cid:12)K
−1(cid:12) (cid:12)sup(cid:12)
(cid:12)
F β|Z(t | Π)P(Π | Z)(cid:12)
(cid:12)
t
(cid:12)Π∈P (cid:12)
θ
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) 1 (cid:12) (cid:12) (cid:88) (cid:12)
≤ (cid:12) −1(cid:12)sup(cid:12) P(Π | Z)(cid:12)
(cid:12)K (cid:12) (cid:12) (cid:12)
t
(cid:12)Π∈P (cid:12)
θRASHOMON PARITITONS FOR HETEROGENEITY 58
(cid:12) (cid:12)
(cid:12) 1 (cid:12) 1
≤ (cid:12) −1(cid:12) = −1,
(cid:12)K (cid:12) K
where in the third line, we trivially bound F (t | Π) ≤ 1 as it is a distribution function and in the
β|Z
(cid:80)
last time we bound P(Π | Z) ≤ 1 since it is a probability mass function. Note that we were
Π∈P
θ
able to remove the absolute values because K ≤ 1 giving us 1/K−1 > 0. Note that, by definition,
K ≥ |P |θ. Therefore,
θ
1
(I) ≤ −1.
|P |θ
θ
Moving on to the second term,
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:88) (cid:88) (cid:12)
(II) = sup(cid:12) F β|Z(t | Π)·P(Π | Z)− F β|Z(t | Π)·P(Π | Z)(cid:12)
(cid:12) (cid:12)
t
(cid:12)Π∈P Π∈P⋆ (cid:12)
θ
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:88) (cid:12)
= sup(cid:12) F β|Z(t | Π)·P(Π | Z)(cid:12)
(cid:12) (cid:12)
t
(cid:12)Π∈P⋆\P (cid:12)
θ
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:88) (cid:12)
≤ sup(cid:12) 1·P(Π | Z)(cid:12)
(cid:12) (cid:12)
t
(cid:12)Π∈P⋆\P (cid:12)
θ
(cid:88)
= P(Π | Z)
Π∈P⋆\P
θ
≤ 1−|P |θ,
θ
where in the third line, we again bound F (t | Π) ≤ 1, and in the final step, we use the definition
β|Z
of P .
θ
Therefore,
(cid:12) (cid:12) 1
sup(cid:12)F
β|Z,P
θ(t)−F β|Z(t)(cid:12) ≤ (I)+(II) ≤
|P |θ
−|P θ|θ.
t θ
□
Proof of Corollary 1. This argument is identical to Theorem 1 except for how we bound the
expectations. We have
(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:88) P(Π | Z) (cid:88) (cid:13)
(cid:13)E Π|P θβ−E Π,P θβ(cid:13) = (cid:13) (cid:13) β Π(cid:80) P(Π′ | Z) − β ΠP(Π | Z)(cid:13) (cid:13)
(cid:13)Π∈P θ Π′∈P θ Π∈P θ (cid:13)
(cid:13) (cid:13)
(cid:12) (cid:12)
(cid:13) (cid:13)
(cid:12) 1 (cid:12) (cid:13) (cid:88) (cid:13)
= (cid:12) (cid:12)(cid:80) P(Π′ | Z) −1(cid:12) (cid:12)(cid:13) (cid:13) β ΠP(Π | Z)(cid:13) (cid:13)
(cid:12) Π′∈P θ (cid:12) (cid:13)Π∈P θ (cid:13)RASHOMON PARITITONS FOR HETEROGENEITY 59
(cid:12) (cid:12)
(cid:12) 1 (cid:12)
= (cid:12) −1(cid:12)∥E Π,P β∥,
(cid:12)K (cid:12) θ
where K = (cid:80) P(Π′ | Z). Note that by definition, K ≥ |P |θ. Further, K ≤ 1 gives us
Π′∈P θ
θ
1/K −1 > 0. Therefore,
(cid:18) (cid:19)
(cid:13) (cid:13) 1
(cid:13)E
Π|P
θβ−E
Π,P
θβ(cid:13) ≤
|P |θ
−1 ∥E
Π,P
θβ∥
θ
=⇒
(cid:13) (cid:13)E Π|P θβ−E Π,P θβ(cid:13) (cid:13)
=
O(cid:18) 1 −1(cid:19)
.
∥E β∥ |P |θ
Π,P θ θ
If we assume that ∥β ∥ < ∞, then define C = max ∥β ∥ < ∞. Then, we have
Π Π∈P⋆ Π
(cid:18) (cid:19)
(cid:13) (cid:13) 1
(cid:13)E
Π|P
θβ−E
Π,P
θβ(cid:13) = O
|P |θ
−1 .
θ
Further,
(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13) (cid:88) (cid:88) (cid:13)
∥E Π,P β−E Πβ∥ = (cid:13) β ΠP(Π | Z)− β ΠP(Π | Z)(cid:13)
θ (cid:13) (cid:13)
(cid:13)Π∈P Π∈P⋆ (cid:13)
θ
(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13) (cid:88) (cid:13)
= (cid:13) β ΠP(Π | Z)(cid:13)
(cid:13) (cid:13)
(cid:13)Π∈P⋆\P (cid:13)
θ
(cid:88)
≤ C P(Π | Z)
Π∈P⋆\P
θ
= O(1−|P |θ),
θ
where in the last line we used the definition of Rashomon sets.. Therefore,
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)E
Π|P
θβ−E Πβ(cid:13) ≤ (cid:13)E
Π|P
θβ−E
Π,P
θβ(cid:13)+∥E
Π,P
θβ−E Πβ∥
(cid:18) (cid:19)
1
= O −|P |θ .
θ
|P |θ
θ
□
Proof of Proposition 1. We can easily see that
ξ(Π,Π ) ≤ ϵ
0
⇐⇒ exp(−Q(Π)) ≥ exp(−Q(Π )(1+ϵ))
0
⇐⇒ P(Π | Z) ≥ P(Π | Z)1+ϵcϵ,
0
where c := c(Z) is the normalization constant. □RASHOMON PARITITONS FOR HETEROGENEITY 60
Appendix C. Appendix to Size of the Rashomon Set
Proof of Proposition 2.
(i) To count the number of all possible partitions, we cast this as a decision tree problem.
There are (R−1)m possible treatment policies in the profile with all arms turned on. These
constitute possible nodes in a binary decision tree. The leaves in the decision tree are the
pools. The number of binary trees with n nodes is given by
(cid:18) (cid:19)
1 2n
C = ,
n
n+1 n
whereC isthe Catalan number(seeAn Invitation to Analytic Combinatorics fromFlajolet
n
and Sedgewick, 2009). Therefore, the number of trees we can construct (that may or may
not be admissible) is
(R−1)m
T =
(cid:88)
C =
O(cid:16) 22(R−1)m(cid:17)
,
n
n=1
where the big-O bound is given by Topley (2016).
(ii) To count the number of admissible aggregations, conceptualize the binary matrix, Σ ∈
{0,1}m×(R−2) again. Each element of Σ tells us whether a particular pair of adjacent levels
in a feature is pooled. In particular, we define Σ = 1 if and only if feature combinations
ij
with dosage j are pooled with feature combinations with factor j+1 in feature i. Therefore,
Σ enumerates all admissible partitions. This gives us the desired result.
□
Proof of Theorem 2. For any prior P ∈ Q , we have,
P|h
sup δ(P ,P ) = sup sup |P (Π)−P (Π)|
P,Z Q,Z P,Z Q,Z
Q∈Q Q∈Q Π∈P
P|h P|h |h
= sup sup |P (Π)−P (Π)|
P,Z Q,Z
Π∈P Q∈Q
|h P|h
1
= sup P(y | X,Π) sup |P(Π)−Q(Π)|.
P(y | X)
Π∈P Q∈Q
|h P|h
First, consider the ℓ prior.
0
(cid:12) (cid:12)
1 (cid:12) 1 (cid:12)
sup δ(P P ,Z,P Q,Z) = sup P(y | X,Π) sup (cid:12) −Q(Π)(cid:12).
Q∈Q
ℓ0 P(y | X)
Π∈P Q∈Q
(cid:12)N(h) (cid:12)
P|h |h P|hRASHOMON PARITITONS FOR HETEROGENEITY 61
Choose an adversarial prior Q⋆ such that Q⋆(Π⋆) = 1 for some arbitrary Π⋆ ∈ P . Then,
|h
(cid:12) (cid:12) (cid:12) (cid:12)
sup
(cid:12)
(cid:12)
1 −Q(Π)(cid:12)
(cid:12) =
(cid:12)
(cid:12)
1 −Q⋆(Π⋆)(cid:12)
(cid:12) = 1−
1
(cid:12)N(h) (cid:12) (cid:12)N(h) (cid:12) N(h)
Q∈Q
P|h
(cid:18) 1 (cid:19) sup Π∈P P(y | X,Π)
=⇒ sup δ(P ,P ) = 1− |h
P ,Z Q,Z
Q∈Q
ℓ0 N(h) P(y | X)
P|h
Next,consideranyotherpriorP ∈ Q ,P ̸= P . LetΠ = argmin P(Π). DenoteP(Π ) = p.
P|h ℓ0 m Π m
Observe that p < 1/N(h) because P ̸= P . Construct an adversarial prior Q⋆ such that Q⋆(Π ) =
ℓ0 m
1. Therefore,
sup |P(Π)−Q(Π)| = |P(Π )−Q⋆(Π )| = 1−p
m m
Q∈Q
P|h
1
=⇒ sup δ(P ,P ) = sup P(y | X,Π)(1−p)
P,Z Q,Z
P(y | X)
Q∈Q Π∈P
P|h |h
sup P(y | X,Π)
Π∈P
= (1−p) |h
P(y | X)
> sup δ(P ,P ).
P ,Z Q,Z
Q∈Q
ℓ0
P|h
Thus, the ℓ prior is minimax optimal,
0
sup δ(P ,P ) = inf sup δ(P ,P ).
P ,Z Q,Z P,Z Q,Z
Q∈Q
P|h
ℓ0 P∈Q P|hQ∈Q
P|h
□
Proof of Lemma 1. From the definition of the Rashomon set, if Π ∈ P , then
θ
P(Π | Z) ≥ θ
exp{−L(Π)−λH(Π)}
=⇒ ≥ θ
c
=⇒ exp{−λH(Π)} ≥ cθ
ln(cθ)
=⇒ H(Π) ≤ − ,
λ
which gives our desired result. □
Proof of Theorem 3. Suppose we have h pools in some partition. Let N(h) be the number of
possible splits that generate h pools. And define H := H (λ) for brevity. Then, the total number
θ
of Rashomon partitions is bounded by
H
(cid:88)
|P | ≤ N(h).
θ
h=1RASHOMON PARITITONS FOR HETEROGENEITY 62
Now, we use the asymptotic bound in Lemma C.3. When R > m1.41, we have the sum of a finite
geometric series,
(cid:88)H mRh−1 = m RH −1 = O(cid:0) mRH−2(cid:1) .
R(R−1)
h=1
For the other case, we bound it by the integral,
(cid:88)H (cid:90) H H(mR)log 2H −1
(mR)log 2h ≤ (mR)log 2hdh =
1+log (mR)
h=1 2
h=1
(cid:16) (cid:17)
= O (mR)log 2H(log (mR))−1 ,
2
where the integral is evaluated in Lemma C.4. This gives us the desired result. □
C.1. Helpful results. We state a useful result that helps us count the number of pools generated
by a partition matrix Σ.
Lemma C.1. Let Σ be the partition matrix for a profile with m active features. Suppose there are
z 1’s in the i-th row of Σ. Then the number of pools created by Σ is,
i
(cid:88) (cid:88)
H(Σ) = (R−1)m−(R−1)m−1 z +(R−1)m−2 z z
i i1 i2
i i1<i2
(cid:88)
−(R−1)m−3 z z z +···+(−1)mz ...z .
i1 i2 i3 1 m
i1<i2<i3
Proof of Lemma C.1. Observe that there are (R − 1)m feature combinations in total (R − 1
because we are assuming the R discrete values include the control). Suppose, we set Σ = 1, then
ij
we are pooling policies of type [r ,...,r ,j,r ,...,r ] with [r ,...,r ,j − 1,r ,...,r ],
1 i−1 i+1 m 1 i−1 i+1 m
where r can take on R−1 values. Therefore, (R−1)m−1 policies are pooled. So, if there are in
i′
nnz(Σ) =
(cid:80)
z , then
(R−1)m−1(cid:80)
z policies are pooled.
i i i i
However, if some of those 1’s are in a different treatment arm, then we end up double counting
those. Forexample,ifΣ = 1andΣ = 1,thenweremovepoliciesoftype[r ,...,j,...,j′,...,r ]
i1,j i2,j′ 1 m
twice wherej andj′ areatindicesi andi . So, weneedtoaddthemback. Similarly, theremaining
1 2
non-linear terms account for this “double counting.” □
Lemma C.1 tells us how to count the number of pools given a partition matrix. We now state
another result that bounds the sparsity of the partition matrix given some number of pools in
Lemma C.2.RASHOMON PARITITONS FOR HETEROGENEITY 63
Lemma C.2. Let Σ be the matrix defined in Proposition 2 for a profile with m active features.
Suppose there are H pools. Then,
(cid:88)
(2R−3)m+1−2H
z ≤
i 2(R−1)m−1
i
Proof of Lemma C.2. Rearranging and dropping negative terms from Lemma C.1,
(cid:88) (cid:88)
(R−1)m−1 z ≤ −H +(R−1)m+(R−1)m−2 z z
i i1 i2
i i1<i2
(cid:88)
+(R−1)m−4 z z z z +...
i1 i2 i3 i4
i1<···<i4
(cid:88)
≤ −H +(R−1)m+(R−1)m−2(R−2)2 1
i1<i2
(cid:88)
+(R−1)m−4(R−2)4 1+...
i1<···<i4
(cid:18) (cid:19)
(cid:88) m
= −H + (R−1)m−n(R−2)n
n
neven
(cid:88)
(2R−3)m+1−2H
=⇒ z ≤ ,
i 2(R−1)m−1
i
where the second inequality uses z ≤ R−2 and the last step uses the well-known identity
j
(cid:18) (cid:19)
(cid:88) n 1
an−kbk = ((a+b)n+(a−b)n).
k 2
k even
□
We state a stronger result in Lemma C.3 that tells us exactly how many partition matrices could
have generated a given number of pools. This result is crucial in bounding the size of the RPS in a
practically meaningful way as in Theorem 3.
Lemma C.3. Let Σ be the matrix defined in Proposition 2 for a profile with m active features.
Then the number of Σ matrices that generate h pools is given by
m (cid:18) (cid:19) k (cid:18) (cid:19)
(cid:88) m (cid:88) (cid:89) R−2
N(h) = ,
k z
i
k=0 (cid:81)k i=1(zi+1)=hi=1
where we define N(1) = 1 and
(cid:0)n(cid:1)
= 0 for k > n.
kRASHOMON PARITITONS FOR HETEROGENEITY 64
As m,R → ∞, we have

 O(mRh−1), R > m1.41
N(h) = .
 O((mR)log 2h), else
Proof of Lemma C.3. This is an exercise in counting. When we make z splits in one feature, we
generate z +1 pools. When we make z splits in feature i and z splits in feature j, we generate
i j
(z +1)(z +2) pools.
i j
When we want to generatehpools, we first choosethe features where we want the splits to occur.
This is what the first summation is doing. Suppose that we’ve chosen k features where we want
to perform splits. Next, we need to identify how many splits can be made in each feature. This
is what the inner summation is doing with the condition
(cid:81)k
(z +1) = h. Finally, we need to
i=1 i
identify where those splits are made, which is where the binomial coefficient comes in.
To get the asymptotic bound, we first consider the term where the exponent on R is the largest.
Thisiswhenwechooseallsplitsinthesamefeature. Next,weconsiderthetermwheretheexponent
on m is the largest. For this to happen, we need to choose as many arms as possible i.e., make the
smallestnumberofnon-zerosplitsineachfeature. Thiscorrespondstomaking1splitineachfeature
i.e., selecting log 2h feature. Hence, we get the asymptotic bound O(max{mRh−1,(mR)log 2h}).
Observe that
mRh−1 > (mR)log 2h ⇐⇒ R > mh−lo lg o2 g2h− h−1 1.
The exponent on m is a decreasing function in h. When h = 2, mRh−1 = (mR)log 2h. And when
h = 3, the exponent is approximately 1.41. Therefore mRh−1 > (mR)log 2h whenever R > m1.41,
which gives our desired result. □
Lemma C.3 has a nice implication. When h is a prime number, we expect N(h) to be small
because all of the splits need to be made in the same feature. On the other hand, when h = 2k is a
power-of-two, we expect N(h) to be very large since we can make splits in multiple features at the
same time.
Lemma C.4. For a > 1,
(cid:90) xalog 2x
alog 2xdx = +C.
1+log a
x 2
Proof of Lemma C.4. We use integration by parts to solve this,
(cid:90)
alog 2xdx = alog
2x(cid:90) dx−(cid:90)
x·
alog 2xlog 2a
dx
x
x x xRASHOMON PARITITONS FOR HETEROGENEITY 65
(cid:90)
= xalog 2x−log a alog 2xdx
2
x
(cid:90) xalog 2x
=⇒ alog 2xdx = +C.
1+log a
x 2
□
Appendix D. Appendix to Rashomon set enumeration and Generalizations
We organize this appendix into proofs for results in Section 5, additional algorithms used in
Section 5, and proofs for results in Section 8.
D.1. Proofs in Section 5.
Proof of Theorem 4. By definition,
1 (cid:88) (cid:88)
b(Σ,M;Z) ≤ (y −µ )2+λH(Π,M)
i (cid:98)π
n
π∈Πfk(i)∈π
Notice that |Π| ≥ H(Π,M). Further, by making more splits, we can only reduce the total mean-
squared error incurred. Therefore,
Q(Π;Z) = L(Π;Z)+λ|Π|
1 (cid:88) (cid:88)
= (y −µ )2+λ|Π|
i (cid:98)π
n
π∈Πk(i)∈π
1 (cid:88) (cid:88)
≥ I{k(i)∩π ̸= ∅}(y −µ )2+λ|Π|
f i (cid:98)π
n
π∈Πk(i)∈π
1 (cid:88) (cid:88)
≥ I{k(i)∩π ̸= ∅}(y −µ )2+λ|Π|
f i (cid:98)π
n
π∈Πfk(i)∈π
1 (cid:88) (cid:88)
≥ I{k(i)∩π ̸= ∅}(y −µ )2+λH(Π,M)
f i (cid:98)π
n
π∈Πfk(i)∈π
= b(Σ ;Z).
f
So if b(Σ,M;Z) > θ , then Σ is not in the Rashomon set. Now consider Σ′ ∈ child(Σ,M). Notice
ϵ
that the size of the fixed set of indices M′ in any child of Σ increases (because there are fewer
places to make further splits). With any further split we make in M, the number of pools increases.
Finally, the loss is non-negative. These together imply,
b(Σ′,M′;Z) ≥ b(Σ,M;Z)
=⇒ Q(Π(Σ′);Z) ≥ b(Σ′,M′;Z) ≥ b(Σ,M;Z).RASHOMON PARITITONS FOR HETEROGENEITY 66
Therefore, if b(Σ,M;Z) > θ , then Σ and all Σ′ ∈ child(Σ,M) are not in the Rashomon set. □
ϵ
Proof of Theorem 5. By definition of b ,
eq
b (Σ,M;Z) ≤ 1 (cid:88) (cid:88) I(cid:8) k(i)∩πc ̸= ∅(cid:9) (y −µ )2.
eq n f i (cid:98)π
π∈Πk(i)∈π
The idea in the inequality above is that any further split we make must obey the splits made at M.
Q(Π;Z) = L(Π;Z)+λ|Π|
1 (cid:88) (cid:88)
= I{k(i)∩π ̸= ∅}(y −µ )2+λ|Π|+
f i (cid:98)π
n
π∈Πk(i)∈π
1 (cid:88) (cid:88) I(cid:8) k(i)∩πc ̸= ∅(cid:9) (y −µ )2
n f i (cid:98)π
π∈Πk(i)∈π
≥ b(Σ,M;Z)+b (Σ,M;Z)
eq
= B(Σ,M;Z).
Therefore, if B(Σ,M;Z) > θ , then Q(Π;Z) > θ and Σ′ ∈ child(Σ,M). Let Π′ := Π(Σ′). Then,
ϵ ϵ
Q(Π′;Z) = L(Π′;Z)+λ(cid:12) (cid:12)Π′(cid:12) (cid:12)
= 1 (cid:88) (cid:88) I{k(i)∩π
f
̸= ∅}(y i−µ (cid:98)π)2+λ(cid:12) (cid:12)Π′(cid:12) (cid:12)+
n
π∈Π′k(i)∈π
1 (cid:88) (cid:88) I(cid:8) k(i)∩πc ̸= ∅(cid:9) (y −µ )2
n f i (cid:98)π
π∈Π′k(i)∈π
≥ b(Σ,M;Z)+ 1 (cid:88) (cid:88) I(cid:8) k(i)∩πc ̸= ∅(cid:9) (y −µ )2
n f i (cid:98)π
π∈Π′k(i)∈π
≥ b(Σ,M;Z)+b (Σ,M;Z)
eq
= B(Σ,M;Z).
In the steps above, we used the fact that making any split will increase the number of pools to
say that |Π′| ≥ |Π|. We also used the definition of b and the idea of a minimum loss incurred by
eq
equivalent units in the final step.
Therefore, if B(Σ,M;Z) > θ , then Q(Π′;Z) > θ for any Σ′ ∈ child(Σ,M). So Σ and all such
ϵ ϵ
Σ′ are not in the Rashomon set. □
Proof of Theorem 6. First note that Algorithm D.1 correctly enumerates the Rashomon set for
any given profile. This follows directly from Lemma 1, and Theorems 4 and 5.RASHOMON PARITITONS FOR HETEROGENEITY 67
Next, Algorithm D.4 performs a breadth-first search starting at the control profile. Since the
M-d hypercube has a unique source (the control profile) and sink (the profile with all features
active), the breadth-first search will terminate after a finite time and traverse every possible path
in the hypercube. When traversing an edge in the hypercube, Algorithm D.3 attempts to pool
adjacent profiles using the intersection matrix Σ∩ while obeying (1) and (2) of Definition 7. This
pooling attempt is done recursively guaranteeing that all admissible partitions are considered for
the Rashomon set.
The choice of Rashomon thresholds for each profile, described in Line 3, is justified by the usage
of Theorem 5.
Correctness of Algorithm 1 immediately follows. □
D.2. Additional algorithms. Algorithm 1 calls upon two important algorithms and uses a spe-
cific caching object that we describe here. First, Algorithm D.1 describes how to enumerate the
Rashomon partitions for a single profile. Second, Algorithm D.4 describes a breadth-first search
to enumerate partitions across different profiles by traversing the M-d hypercube. This algorithm
in turn relies on Algorithm D.2 to obtain the intersection matrix between partitions of adjacent
profiles and Algorithm D.3 to pool adjacent profiles recursively. Finally, Algorithm D.5 describes
the implementation of the caching object used in Algorithm 1.
D.3. Proofs in Section 8.
Proof of Theorem 7. The results follow directly from Theorems 4, 5, and 6. □
Proof of Theorem 8. The results follow directly from Theorems 4, 5, and 6. □
Appendix E. Further Details on Related Work
It is useful to contrast our method with several other (some recent) approaches to study hetero-
geneity. Specifically, we are interested in their application to settings with partial orderings (e.g.,
factorial structure and admissibility) which is easily interpretable.
Wewillfocusonfourmainrelatedapproaches: (1)canonicalBayesianHierarchicalModels(BHM)
(Rubin, 1981; Gelman, 2006; Meager, 2019); (2) ℓ regularization of marginal effects to identify
1
heterogeneity (Banerjee et al., 2021); (3) causal forests (Wager and Athey, 2018); and (4) machine
learnedproxies(Chernozhukovetal.,2018). Weintendthisdiscussiontobeaguideforpractitioners
considering implementing our proposed method or one of these state-of-the-art alternatives. We
discuss conceptually related work (e.g. Bayesian decision trees) in previous sections. an Let us
for the moment set aside the following immediate differences. Our focus on robustness, profiles,
and enumerating the entire Rashomon Partition are all novel. Instead, it is useful to identify theRASHOMON PARITITONS FOR HETEROGENEITY 68
Algorithm D.1 EnumerateRPS_profile(M,R,H,Z,θ )
ϵ
Input: M features, R factors per feature, max pools H, data Z, Rashomon threshold θ
ϵ
Output: Rashomon set P
q,ϵ
1: P q,ϵ = ∅
2: S = cache() ▷ See Algorithm D.5
3: Q = queue()
4: Σ = {1}M×(R−2)
5: Q.push(Σ,1,1) ▷ Can start at any arbitrary arm
6: while Q is not empty do
7: (Σ,i,j) = Q.dequeue()
8: if S.seen(Σ,i,j) then continue
9: S.insert((Σ,i,j))
10: if H(Σ) > H then continue
11: Σ 1 = Σ, Σ 1,i,j = 1
12: Σ 0 = Σ, Σ 0,i,j = 1
13: for m ≤ M do ▷ Branch and search
14: j 1 = min{j ≤ R−2 | not S.seen(Σ 1,m,j)}
15: if j 1 ̸= ∅ then Q.enqueue(Σ 1,m,j 1)
16: j 0 = min{j ≤ R−2 | not S.seen(Σ 0,m,j)}
17: if j 0 ̸= ∅ then Q.enqueue(Σ 0,m,j 1)
18: if B(Σ,i,j;Z) > θ ϵ then continue
19: if Q(Σ 1) ≤ θ ϵ then P q,ϵ.add(Σ 1)
20: if Q(Σ 0) ≤ θ ϵ and H(Σ 0) ≤ H then P q,ϵ.add(Σ 0)
21: if j < R−2 then ▷ Search deeper
22: if not S.seen(Σ 1,i,j +1) then Q.enqueue(Σ 1,i,j +1)
23: if not S.seen(Σ 0,i,j +1) then Q.enqueue(Σ 0,i,j +1)
24: return P q,ϵ
Algorithm D.2 IntersectionMatrix(Π,ρ ,ρ )
i j
Input: Partition Π, Adjacent profiles ρ ,ρ such that ρ < ρ
i j i j
Output: Intersection matrix Σ∩
1: m = ρ i∧ρ j ▷ Indices of features active in both profiles
2: m′ = ρ i⊕ρ j ▷ Index where ρ i,ρ j differ
3: Π ρi = {π\{k | ρ(k) ̸= ρ i} | π ∈ Π}
4: Π ρj = {π\{k | ρ(k) ̸= ρ j} | π ∈ Π}
5: Σ∩ = [∞]|Πρi|×|Πρj|
6: for π k ∈ Π ρi do
7: for π k′ ∈ (cid:80)Π ρj do
(cid:80)
▷ Features with lowest non-zero level in m′
98 :: iA
f
=
A ≠
a ∅1∈ tπ hk ena2∈π k′
I{∥x(a 1)−x(a 2)∥
1
= 1}
10: Σ∩ = 0
k,k′
11: return Σ∩
philosophical differences across the various approaches and how they relate to us. Every approach,
as we will note, effectively uses partitions Π at some point to determine which data to pool or not.
The specific techniques create distributions, possibly degenerate, over these partitions, and theseRASHOMON PARITITONS FOR HETEROGENEITY 69
Algorithm D.3 PoolAdjacentProfiles(P ,Π,z,Σ∩,Z,θ)
q,ϵ
Input: Rashomon set P , partition Π, list of pools that can be pooled across profiles z, data Z,
q,ϵ
Rashomon threshold θ, intersection matrices already seen S
Output: Rashomon set P
q,ϵ
1: while z ̸= ∅ do
2: (k,k′) = z.pop()
3: P q,ϵ = PoolAdjacentProfiles(P q,ϵ,Π,z,Σ∩,θ)
4: Σ∩,′ = Σ∩
5: Σ∩,′ = 1
k,k′
6: Σ k∩ ,, −′ k′ = ∞, Σ∩ −, k′ ,k′ = ∞ ▷ Cannot pool π k or π k′ with any other pool
7: Π′ = (Π\{π k,π k′})∪(π k ∪π k′) ▷ Update Π
8: if Q(Π′;Z) ≤ θ then
9: P q,ϵ = Π′∪PoolAdjacentProfiles(P q,ϵ,Π′,z,Σ∩,′,θ)
10: return P q,ϵ
Algorithm D.4 PoolProfiles(P,ρ ,Z,θ)
0
Input: Candidates for Rashomon set P, control profile ρ , data Z, Rashomon threshold θ
0
Output: Rashomon set P
q,ϵ
1: P q,ϵ = ∅
2: Q = queue()
3: while Q ≠ ∅ do
4: ρ i = Q.dequeue()
5: N(ρ i) = {ρ j | ∥ρ i−ρ j∥ 0 = 1,ρ j > ρ i} ▷ Neighbors of ρ i with additional active feature
6: for ρ j ∈ N(ρ i) do
7: Q.enqueue(ρ j)
8: for Π ∈ P′ do
9: Σ∩ = IntersectionMatrix(Π,ρ i,ρ j) ▷ See Algorithm D.2
10: z = {(k,k′) | Σ∩ = 0}
k,k′
11: P q,ϵ = PoolAdjacentProfiles(P q,ϵ,Π,z,Σ∩,Z,θ) ▷ See Algorithm D.3
12: return P q,ϵ
Algorithm D.5 Implementation of caching object used in Algorithm D.1
S = cache() ▷ Initialize caching object
C = {}
S.insert(Σ,i,j) ▷ Extract and insert Σ
f
Σ[i,j : (R−2)] = NA
C = C ∪{Σ}
S.seen(Σ,i,j) ▷ Extract and check presence of Σ
f
Σ[i,j : (R−2)] = NA
return Σ ∈ C
distributions are sampled from and marginalized to estimate treatment effects β . The interesting
k
thing therefore is in how one builds a distribution over Π.RASHOMON PARITITONS FOR HETEROGENEITY 70
Algorithm D.6 select_feasible_combinations(K,θ)
Input: K list of n sorted lists containing a numerical score, θ threshold
Output: F, list of lists of length n with indices of elements from each of K such that their sum is
i
less than θ
1: F = {}
2: n = len(K)
3: if n = 0 then return {}
4: K 1,feasibleindices = {i | K 1[i] ≤ θ}
5: if n = 1 then
6: F = {K 1,feasibleindices}
return F
7: x =
(cid:80)n
j=2K j[1]
8: for i ∈ K 1,feasibleindices do
9: θ i = θ−K 1[i]
10: if θ i < x then break
11: F i = select_feasible_combinations(K[2 :],θ i)
12: for f ∈ F i do
13: F.insert([i].append(f))
return F
E.1. Bayesian Hierarchical Models. We now discuss how our work relates to a canonical rep-
resentation of a Bayesian Hierarchical Model. As discussed previously, our work is more similar to
Bayesian Tree(d) models than to other methods for accounting for learning heterogeneity, such as
Bayesian Model Averaging. For context, however, we present how our approach compares to the
canonical Bayesian approach. The Bayesian perspective provides a compromise between complete
and partial pooling. Partial pooling occurs by encouraging similarity in the values for parame-
ters without requiring strict equality. Using the notation from our model, for example, we could
construct a model where
y ∼ N(Dβ,σ2)
y
and, for the sake of exposition, all β are draw independently from
β ∼ N(µ ,σ2).
β β
Requiring that all values of β come from the same distribution encourages sharing information
across potential feature combinations and encourages the effects on heterogeneity to be similar (but
not identical). Meager (2019) uses this approach when comparing treatment effects across multiple
domains. In that paper, the goal is not to pool across potentially similar treatment conditions but
instead to (partially) pool across geographic areas.RASHOMON PARITITONS FOR HETEROGENEITY 71
As one example of the classical model, Meager (2019) has outcomes for household i in study k
modeled as
y ∼ N(µ +τ T ,σ2 ) ∀ i,k
ik k k ik yk
     
µ µ σ2 σ
k µ µτ
  ∼ N  ,  ∀ i,k
τ τ σ σ2
k µτ τ
where τ , µ are the overall mean and treatment effect at area k, respectively. The vector T is
k k ik
the treatment indicator for household i in study k.
Onewaytomeasurethedegreeofpoolingisthe(partial)“poolingfactor” metricdefinedinGelman
(2006), ω(β) = σ2/σ2+σ2. The partial pooling metric quantifies how much the effect of treatment
y y β
combinations varies compared to the overall heterogeneity in the outcomes. The partial pooling
metric, the Meager (2019) context refers to the relative variation related to differences between
studies compared to sampling variability.
In contrast, we could think of our approach as using a prior on β conditional on the partitions
thatpotentiallyforcesomevaluesofβ ,β′ tobeequal. InAppendixF.2,weshowthattheobjective
k k
function we use in Equation 5 corresponds to a hierarchical model where we draw the β vector as
β | Π ∼ N(µ ,Λ),
Π
where µ is structured such that µ = µ for any k,k′ ∈ π ∈ Π. Then, given some feature
Π k k′
combinations D, we draw the outcomes as
y | D,β ∼ N(Dβ,Σ).
To understand the variation within the β vector, we need to average across potential partitions,
since some partitions will set β = β′ and others will not. This amounts to replacing the σ2 in the
k k β
pooling factor with the variance of the distribution of P(β|Z), which is defined in Equation 3.
We could also conceptualize the above derivation in terms of equality on β rather than the
means µ . If, for example, we replace Λ with Λ where Var(µ ,µ′) = 0 for any k,k′ ∈ π ∈ Π (or
Π Π k k
equivalently,whenµ = µ )thenweenforcethatβ = β′. Ofcourse,ifwegotheoppositedirection
k k′ k k
andletthediagonalofΛ beunconstrainedthenthereisessentiallynosharingofinformationacross
Π
feature combinations.RASHOMON PARITITONS FOR HETEROGENEITY 72
Finally, hierarchical models of this type are, of course, quite flexible and we could construct more
complex models that capture features of our pooling approach. Among those options would be
to use the Bayesian version of penalized regression, such as the Bayesian Lasso, which would be
philosophically related to the approach we describe in the next section.
E.2. Lasso regularization. This is the approach taken in prior work by several of the authors
of the present paper, in Banerjee et al. (2021). There the setting was one in which the researcher
faced a factorial experimental design: a crossed randomized controlled trial (RCT). The paper
developed the Hasse structure described above and an approach that required transforming D into
an equivalent form presented in Equation (A.1) in Appendix A. Here every parameter α represents
k
the marginal difference between β and β where ρ(k) = ρ(k′) (they are the same profile) and k
k k′
exactlydiffersfromk′ ononearmbyonedose. Theparametervectorαrecordsthemarginaleffects.
Notice the support of α therefore identifies Π (since non-zero entries determine splits).
The first difficulty in applying this to general settings of heterogeneity is that ℓ regularization
1
requires irrepresentability: that there is limited correlation between the regressors so that the sup-
port may be consistently recovered (Zhao and Yu, 2006). Unfortunately, the regression implied by
the Hasse does not satisfy this so some pre-processing is required. Banerjee et al. (2021) apply the
Puffer transformation of Jia and Rohe (2015) to retain irrepresentability and estimate the Lasso
model. However, this is not free: the approach requires conditions on the minimum singular value
of the design matrix. The authors leverage the structure of a crossed randomized controlled trial
(which places considerable restriction on the design matrix) to argue that indeed these conditions
are met. There is no guarantee and it is unlikely to be the case that these conditions are met for
general factorial data of arbitrary covariates. So, tackling the much more general structure required
moving away from regression (we use decision trees) and changing the regularization (we use ℓ ).
0
The second key observation is that the Bayesian lasso means that the ℓ penalty corresponds to
1
priors P(α) that are i.i.d. Laplace on every dimension k. That is
(cid:89) (cid:89) (cid:88)
−logP(α) = log P(α ) = log exp(−λ|α |) = λ |α |.
k k k
k k k
Note that this is true whether one uses regular lasso, Puffer transformed lasso, spike-and-slab lasso,
group lasso (up to the group level) and so on. No matter at whatever level the ℓ sum is being
1
taken, it corresponds to independence at that level in the prior.
In practice what this means is that given two partitions Π and Π′, which have the same number
of pools and which have the same loss value, if one is more consistent with independent values of
α than the other, it will receive a higher posterior. There are at least two problems.
kRASHOMON PARITITONS FOR HETEROGENEITY 73
The main philosophical problem is that there is no reason to place the meta-structure that the
marginal differences between adjacent variants should have an i.i.d. distribution. In fact, one might
think that the basic science or social science dictates exactly the opposite. Independence means
that a marginal increase in dosage of drug A, holding fixed B and C at some level, is thought to
be independent of increasing A holding fixed B and C at (potentially very similar) different levels.
Similarly, the marginal value of receiving a slightly larger loan given that the recipient has 10 years
of schooling and started 5 previous businesses is independent of receiving a slightly larger loan if the
recipient had 10 years of schooling and started 6 previous businesses. Independence is unreasonable
in both examples.
ThereisasecondissueinthatifanobjectofinterestisΠ, thisapproachprovidesnowayforward.
Regularization delivers posteriors over α: P(α | y,X). This implies a posterior over S . The map
α
from S to Π is deterministic, and is given by some ϕ(S ) = Π, which means that
α α
(cid:90)
P(Π | y,X) = 1{ϕ(S ) = Π}·P(α | y,X)
α
α
is the actual calculation of interest.
So the regularization approach requires the statistician to take all the marginal parameters to be
i.i.d., and given this, integrate over possible coefficient vectors that are consistent with this specific
aggregation. This makes calculating an RPS very difficult.
E.3. Causal Random Forests. WenowcompareourapproachtoCausalRandomForests(CRFs)
introducedby WagerandAthey(2018). CRFsconstructregressiontreesoverthespaceofpotential
combinations of covariates. Trees partition the space of covariates into “leaves.” Unlike our setting,
trees are heirarchical; the procedure to construct trees involves splitting the observed data in two
based on X being above or below a threshold. They then partition recursively, dividing each
i
subsequent group until the leaves contain very few observations. This approach can also be thought
of as finding nearest neighbors, where the number of neighbors is the number of observations in the
leaf and using distance on the tree as the closeness metric. CRFs construct a conditional average
treatment effect at a pre-determined point X = x, τ(x) = E[Y(1)−Y(0)|X = x] where Y(1) is
potential outcome for the treated and Y(0) is the potential outcome for the control.
Relating this back to our work, take T to be a tree and π ∈ Π(T) to be a leaf in the tree, which
corresponds to a pool in our language. Then, the estimated expected outcomes for each leaf is
1 (cid:88)
β(cid:98)π = Y i.
|{i : X ∈ π}|
i
{i:Xi∈π}RASHOMON PARITITONS FOR HETEROGENEITY 74
Further, taking τ to be the treatment effect of observations in pool (leaf) π and W as the
π i
treatment indicator, which we assume orthogonal to X and Y, the estimated treatment effect for π
is
1 (cid:88) 1 (cid:88)
τ = Y − Y .
(cid:98)π i i
|{i : W = 1,X ∈ π}| |{i : W = 0,X ∈ π}|
i i i i
{i:Wi=1,Xi∈π} {i:Wi=0,Xi∈π}
To summarize, the approach for forming trees splits the observed covariate space into partitions,
known as leaves. Each leaf consists of a mix of people in treatment and control groups and, in fact,
the specification of the tree depends on this balance across treatment and control groups since the
algorithm requires that splitting be done in a way that preserves a minimum number of treatment
and control in each leaf. To compute a treatment effect conditional on a particular value of X, look
at the difference in outcome between treated and control people in a given leaf. Outcomes are not
considered with constructing the tree (in contrast to our proposed approach) and treatment status
is not used to split explicitly but does influence the construction of the tree through the sample size
restriction.
Despite being similar in that we both use geometric objects that partition the space of covariates,
there are three fundamental differences between our approach and CRFs. The first difference is geo-
metric. CRFs use regression trees, whereas we use Hasse diagrams. Regression trees are appealing
in many settings because of their flexibility in representing complex, nonlinear, relationships be-
tween variables. Regression trees, however, require imposing a hierarchy between variables that is
not supported by the data. This hierarchy is “baked in” to the structure of the trees and is evident
from how we describe constructing trees in the previous paragraph. The data, however, are not
fully hierarchical and are instead partially ordered.
This mismatch creates an identification issue. Within education and within income, there are
clear orderings. There is, however, no heirarchy between education and income. One tree may,
therefore, split first based on income and then split on education conditional on income while
another tree does the opposite. In both cases, we can trace the trees to end up with the same
estimated treatment effects for any group of covariates (as shown in Wager and Athey (2018)).
The trees themselves, however, arise from this arbitrary ordering and are, thus, not interpretable.
Work such as Bénard and Josse (2023) describe measures of variable importance in CRFs, but the
problem of an arbitrarily imposed hierarchy is still present. Hasse diagrams, in contrast, are the
natural geometry for partially ordered sets, alleviating this issue and allowing the researcher to
interpret the pooling structure on the domain of the covariates directly.RASHOMON PARITITONS FOR HETEROGENEITY 75
The second difference is computational but has conceptual implications. In both our approach
and CRFs, we do not take the stucture of the partition as known. Both approaches must, therefore,
account for additional uncertainty in treatment effect estimates that arises from not knowing the
partition. In CRFs, bootstrap samples over the data propagate this uncertainty. CRFs then aggre-
gate over trees using Monte Carlo averaging over b = 1,..,B boostrap samples of the covariates and
outcomes, {Z ,...,Z },
1 n
B
1 (cid:88)
RF(π;Z ,...,Z ) ≈ T(π;ξ ;Z∗ ,...,Z∗ ),
1 n B b b1 bs
b=1
where π represents a pool or leaf specifying a combination of features and levels. The ξ term is
b
an additional stochastic component. The trees sampled as part of this process create a “forest” are,
bydefinition, randomdrawsgiventhedata. Thatis, givenadifferentsetofdata, thedistributionof
likely trees would change. They are also not guaranteed to be optimal or nearly optimal. If the goal
is to estimate average treatment effects, this approach represents a principled way to explore the
space of trees. If the goal, however, is to identify potential models of heterogeneity, then sampling
randomly is very unlikely to produce high quality trees. With Rashomon partitions, by definition,
we guarantee that all models in our set are of high posterior.
To this point, we have not discussed inference in CRFs. A key contribution of Wager and Athey
(2018) is forming so-called “honest” trees that account for issues that arise when using the same
data to learn trees and then to make inference conditional on the group of trees. In our work,
we use a Bayesian framework to address this issue, which also has the advantage of being able to
estimatefunctionsoftreatmenteffects(see1). Futurework, however, couldconsiderRashomonsets
for honest regression trees. This work would build upon our own work as well as Xin et al. (2022)
that introduces Rashomon sets for classification trees. The algorithm for inference would begin
with splitting as proposed by Wager and Athey (2018) to preserve honest inference, then construct
Rashomon sets using the algorithm from Xin et al. (2022). Since the space of trees is enormous,
finding the “best” tree is impossible, which creates issues for finding the Rashomon set since it is
used to define the reference partition. Fortunately, a recent paper by Hu et al. (2019) provides
an algorithm. While this approach would allow the CRF framework to find optimal trees, it does
not address the identifiability issue that arises when using trees for data that are only partially
ordered. Similar work was explored in Hahn et al. (2020), who estimate heterogeneous treatment
effects using a sum of Bayesian regression trees, which they refer to as the Bayesian causal forest.
They decompose the outcome into a mean outcome and a treatment effect. Since they are only
interested in the treatment effect, the mean outcome becomes a nuisance parameter. They imposeRASHOMON PARITITONS FOR HETEROGENEITY 76
avagueprioronthemeanandastrongprioronthetreatmenteffect. Otherwise, thetreeestimation
procedure is identical to Bayesian Additive Regression Trees (Chipman et al., 2010).
Third, both our approach and CRFs impose regularization but do so in philosophically very
differentways. Wetaketheperspectivethatwedonotknowandcannotfullyenumeratecorrelation
structureinahighdimensionalspace. Soweusetheℓ prior, whichweshowistheleastinformative
0
prior in Theorem 2. In other words, we regularize, and impose a prior, on the size of the partition.
In doing so, we are trading off information on full distribution to robustly identify partitions.
On the other hand, causal forests regularize on the number of observations in each leaf of the
tree. Specifically, they require at least k samples in each leaf. This choice is sensible because with
insufficientdatathereisnoinformation. Atthesametime, thisisoddastheregularizationdepends
directly on the data. Elaborating on this, we can write the posterior for some tree T given data
y,X as
(cid:26) (cid:27)
λ
P(T | y,X) ∝ P(y | T,X)P(T | X) where P(T | X) ∝ exp − ,
min n (X)
π∈Π(T) π
whereΠ(T)isthesetofpools(leaves)inT andn (X)isthenumberofobservationsinXthatbelong
π
to pool π. This prior down-weights and discards partitions where for some π the observations are
low. Inthatsense,theprioreffectivelyassumesthatinthebackgroundthereisakindofstratification
– that observations are sampled from some process such that all pools have enough observations,
thoughofcoursethe true partition is unknown. Thisfeelsawkwardasthereisarelationshipbetween
the data collection process and the actual true partitioning wherein the user of the causal forest is
assuming that they have effectively stratified data collection against the unknown partitions.
Together, these differences mean that the scope of our method is wider than CRFs. While both
methods can estimate heterogeneity in treatment effects and control for multiple testing, we also
produceinterpretableexplanationsofheterogeneity. Forthereasonsoutlinedabove,namelyidentifi-
cation and sampling, it is not possible to extract information on the relationship between covariates
from elements of the random forest. We can, of course, test for any hypothesis about potential
heterogneeity between arbitrary combinations of features, but CRFs require that we specify the
hypothesis a priori. In our setting, however, finding the set of high posterior probability partitions
gives a policymaker or researcher as set of potential models of heterogeneity and interaction be-
tween the covariates that can be used to design future policies or generate new research hypotheses.
On the other hand, our method assumes that the posterior has separated modes. If the posterior
distribution is very flat or has many (many) very similar modes, then the Rashomon set will be
very large and our benefits in terms of interpretability will diminish.RASHOMON PARITITONS FOR HETEROGENEITY 77
E.4. Treatment heterogeneity via Machine Learning Proxies. Chernozhukov et al. (2018)
propose a general framework for using machine learning proxies to explore treatment effect hetero-
geneity. They allow for estimation of multiple outcomes, including conditional average treatment
effectsandtreatmenteffectheterogeneitybetweenthemostandleastimpactedgroups. Ratherthan
search the space of covariates directly, Chernozhukov et al. (2018) uses a machine learning method
to create a “proxy” for the heterogeneous effects. This approach has the advantage that it can be
applied in high dimensional settings. A downside, however, is that the machine learning proxies are
often uninterpretable in terms of the original covariates, making it necessary to post-process the
treatment effect distributions to gain insights about particular covariates.
We now give a brief overview to unify notation but do not exhasutively cover all the estimators
presented in Chernozhukov et al. (2018). Say that s (Z) is the true conditional average treatment
0
effect, E[Y(1)|Z]−E[Y(0)|Z] Ascertaining the functional form of the relationship between the non-
intervention covariates X and the outcome Y, though, is complicated when X is high dimensional.
In response, Chernozhukov et al. (2018) use a machine learning method (e.g. neural networks,
random forests, etc) to construct a proxy for s (Z) using an auxillary dataset. In a heuristic sense,
0
this proxy serves the role of a partition π, in that it aggregates across covariates to separate the
data based on the treatment effect. This analogy is most direct when the machine learning model
is a decision tree (which it need not be) since in that case leaves of the tree would correspond to
partitions of the covariate space based on treatment effect. After computing the machine learning
proxy, Chernozhukov et al. (2018) then project it back to the space of the observed outcomes. It is
then also possible to construct clusterings based on the proxies and related those clusterings back
to the outcomes. Chernozhukov et al. (2018) differs from our approach in many of the same ways
as the comparison with Wager and Athey (2018), namely that we focus on identifying multiple
explanations for heterogeneity and that we utlize the Hasse diagram as a geometric representation
of partial ordering. We also find that this structure is sufficient to explore models for heterogeneity
on the space of the covariates without the need to use proxies.
Appendix F. Laplace approximation and generalized Bayesian inference
F.1. Laplace approximation. Here,webrieflyoutlinehowtoapproximatethefullposteriorusing
the Rashomon set and Laplace’s method. Our goal is to estimate
(cid:88)
p(β | Z) = p(β | Z,Π)P(Π | Z)
Π∈P
θRASHOMON PARITITONS FOR HETEROGENEITY 78
We will do this by constructing a specific data-generating process. Consider the following data-
generating process after fixing a partition Π. For each pool π ∈ Π, draw γ i. ∼i.d. N(µ ,τ2) i.e.,
j j 0
draw γ ∼ N(µ ,Λ ).21 Here, γ ∈ R|Π| and Λ = τ2I where and I is an identity matrix of size
0 0 0 |Π| m
m.
Then, we can define a transformation matrix P ∈ {0,1}K×|Π|, where K is the number of possible
feature combinations, that assigns each γ of each pool to the feature combinations in that pool,

 1, feature combination i ∈ π
j
P = .
ij
 0, else
Themeanvectorforthefeaturecombinationsisgivenbyβ = Pγ. Bypropertiesofthemultivariate
⊺
normal, we have β | Π ∼ N(µ ,Λ ), where µ = Pµ and Λ = PΛ P . Specifically, note that
Π Π Π 0 Π 0
the means of all feature combinations in a given pool don’t just share the same mean, but are
effectively equivalent to each other.
Then, given some feature combinations D, we draw the outcomes as
y | D,β,Π ∼ N(Dβ,Σ) =⇒ y | D,γ,Π ∼ N(DPγ,Σ).
Therefore, γ | Z,Π ∼ N
(cid:0)
µ
,Λ−1(cid:1)
where
n n
(cid:16) (cid:17)
µ = Λ (DP)⊤(DP)γ +Λ µ
n n (cid:98) 0 0
Λ−1 = (DP)⊤(DP)+Λ
n 0
γ = ((DP)⊤(DP))−1(DP)⊤y
(cid:98)
Next, P(Π | Z) = P(Π)(cid:82) p(Z | γ′,Π)p(γ′ | Π)dγ′. We know that Π(Π) = Cexp{−λ|Π|} where
γ′
C is the normalization constant (or the partition function). Therefore, we have
(cid:90)
P(Π | Z) = CA A e−λ|Π| exp(cid:8) −g(γ′)(cid:9) dγ′,
1,Π 2,Π
γ′
1 1
g(γ′) = (γ′−µ )⊤Λ (γ′−µ )+ (y−DPγ′)⊤Σ−1(y−DPγ′)
0 0 0
2 2
where A ,A are known constants from the normal distributions. It is easy to verify that
1,Π 2,Π
∇g(γ′) = Λ γ′+(DP)⊤Σ−1DPγ′−Λ µ −(DP)⊤Σ−1y
0 0 0
∇2g(γ′) = Λ +(DP)⊤Σ−1DP
0
21Inthiscase,thesedrawsneednotbeindependentofidentical. Thecomputationsjustbecomealittlemoretedious.RASHOMON PARITITONS FOR HETEROGENEITY 79
Since ∇2g(γ′) is positive semi-definite for all γ′, g is convex. Therefore, solving for ∇g(γ′) = 0
allows us to find the minimum, γ⋆ = (Λ +(DP)⊤Σ−1(DP))−1(Λ µ +(DP)⊤Σ−1y).
0 0 0
Using Laplace approximation, we get
P(Π | Z) ≈ CA A e−λ|Π|−g(γ⋆)(2π)|Π|/2det(Λ +(DP)⊤Σ−1(DP))1/2.
1,Π 2,Π 0
This allows us to approximate the original quantity of interest, p(β | Z) through a variable
transformation of γ where all the constants are known except for C. Exactly computing C is NP-
Hard and is reminiscent of partition functions used in graphical models (not to be confused with
the “partition” that we are using in this work). See Agrawal et al. (2021) for a survey of methods
used to estimate or approximate the constant C.
F.2. Generalized Bayesian inference. We have our mean squared error for a given partition Π,
1
⊺
L(Π;Z) = (y−y) (y−y),
(cid:98)
n
(cid:80) (cid:80)
I{k(i) ∈ π} I{k(j) ∈ π}y
π∈Π j j
(F.1) y =
(cid:98)i (cid:80) (cid:80)
I{k(i) ∈ π} I{k(j) ∈ π}
π∈Π j
where y is the mean outcome in the pool π ∈ Π containing the feature combination of unit i, k(i).
(cid:98)i
Our goal is to show that minimizing L(Π;Z) corresponds to maximizing the likelihood P(y |
D,Π). Consider the same data-generating process in Appendix F.1. Specifically, we require inde-
pendence over γ and we will assume that the prior over γ is diffuse i.e., τ2 ≫ 1. As before, given
i
some feature combinations D, we draw the outcomes as
y | D,β,Π ∼ N(Dβ,Σ) =⇒ y | D,γ,Π ∼ N(DPγ,Σ).
This allows us to find the likelihood,
(cid:90) (cid:90)
P(y | D,Π) = P(y | D,Π,β)P(β | Π)dβ = P(y | D,Π,γ)P(γ | Π)dγ
β γ
(cid:90)
= P(y | D,P,γ)P(γ | Π)dγ
γ
(cid:90) (cid:26) (cid:27) (cid:26) (cid:27)
1 1
∝ exp − (y−DPγ)⊺ Σ−1(y−DPγ) exp − (γ −µ )⊺ Λ−1(γ −µ ) dγ
2 2 0 0 0
γ
After re-arranging the terms in the exponent, we have
−1 (cid:0)(cid:0) γ −M−1u(cid:1)⊺ M(cid:0) γ −M−1u(cid:1) +y⊺ Σ−1y+µ⊺ Λ−1µ −u⊺ M−1u(cid:1) ,
2 0 0 0RASHOMON PARITITONS FOR HETEROGENEITY 80
where M = P⊺ D⊺ Σ−1PD+Λ−1 and u = P⊺ D⊺ Σ−1y +Λ−1µ . Notice that when integrating
0 0 0
with respect to γ, the first quadratic term becomes a constant in y. Therefore,
(cid:26) (cid:27)
P(y | D,Π) ∝ exp −1 (cid:0) y⊺ Σ−1y+µ⊺ Λ−1µ −u⊺ M−1u(cid:1) .
2 0 0 0
Now,astheprioroverγ becomesmorediffusei.e.,asτ2 → ∞,wehavethatΛ−1 → 0. Therefore,
0
µ⊺ Λ−1µ → 0, M → P⊺ D⊺ Σ−1PD, and u → P⊺ D⊺ Σ−1y. This allows us to simplify,
0 0 0
(cid:26) (cid:27)
P(y | D,Π) ∝ exp
−1 (cid:0) y⊺ Σ−1y−(P⊺ D⊺ Σ−1y)⊺ (P⊺ D⊺ Σ−1DP)−1(P⊺ D⊺ Σ−1y)(cid:1)
2
(cid:26) (cid:27)
∝ exp
−1 y⊺(cid:0) Σ−1−Σ−1DP(P⊺ D⊺ Σ−1DP)−1P⊺ D⊺ Σ−1(cid:1)
y
2
The likelihood is maximized when the log-likelihood is maximized,
logP(y | D,Π) =
−1 y⊺(cid:0) Σ−1−Σ−1DP(P⊺ D⊺ Σ−1DP)−1P⊺ D⊺ Σ−1(cid:1)
y+c
2
∂logP(y | D,Π)
set
= 0
∂y
=⇒ y = DP(P⊺ D⊺ Σ−1DP)−1P⊺ D⊺ Σ−1y
= DPγ
(cid:98)
= Dβ(cid:98),
where β(cid:98) = Pγ
(cid:98)
and γ
(cid:98)
= (P⊺ D⊺ Σ−1DP)−1P⊺ D⊺ Σ−1y. This is exactly the solution to is the
solution to the following ordinary least-squares problem,
min∥y−DPγ∥2.
2
γ
Next, we will show that this ordinary least squares problem is identical to L(Π;Z). In order to
makethisargumentcleaner,wewillassumethatΣ = σ2I . Now,observethestructureofD. Inany
n
⊺
rowi,D = 1ifobservationiisassignedtofeaturecombinationk,andD = 0otherwise. So,D D
ik ik
⊺
is a diagonal matrix of size K×K where (D D) is the number of observations assigned to feature
kk
⊺
combination k, n . And D y sums all outcomes y corresponding to each feature combination k.
k i
Similar to how D collects all observations into their respective feature combinations, P collects
all feature combinations into their respective pools. Therefore
(P⊺ D⊺ DP)−1P⊺ D⊺
y is the average
outcome in each pool. This is exactly our estimated y in Equation F.1. In other words, L(Π;Z) is
(cid:98)
exactly the minimized squared error (up to some scaling constant).
Therefore, maximizing the posterior P(y | D,Π)P(Π) corresponds to minimizing the mean-
squared error with the ℓ penalty. This has connections to loss-based generalized Bayesian inference
0
(Bissiri et al., 2016). Here, we have described one possible prior on β to recover the mean-squaredRASHOMON PARITITONS FOR HETEROGENEITY 81
error. However, other such priors exist that describe such analytic loss functions. We refer the
reader to Section 4 of Chipman et al. (2010) for examples of such priors used for BARTs.
Appendix G. Appendix to simulations
G.1. Performance metrics for Simulation 1. We used the following performance metrics to
evaluate Lasso and models in the RPS in Figure 4 in Section 6:
(1) Overall mean-squared error (MSE): Suppose y and y are the estimated and true outcomes
(cid:98)i i
for unit i, then the overall MSE is defined as
n
1 (cid:88)
MSE = (y −y )2.
(cid:98)i i
n
i=1
(2) Best policy MSE: Let y be the true best policy effect and y be the estimated best
max (cid:98)max
policy effect. Then the best policy MSE is
MSE = (y −y )2.
best (cid:98)max max
(3) Bestpolicycoverage: Letπ⋆ andπ⋆ bethetrueandestimatedsetofpolicieswiththehighest
(cid:98)
effect. Then, we define the best policy coverage as the intersection-over-union of these two
sets
|π⋆∩π⋆|
IOU = (cid:98) .
|π⋆∪π⋆|
(cid:98)
Thesemetricsareeasilyunderstoodforasinglemodel. FortheRPS,wereportedtheperformance
metric averaged across all partitions in the RPS.
We visualize the RPS through a heat map. An example heatmap with instructions on how to
read it is shown in Figure G.1. We also use these heatmaps in our empirical data examples in
Appendix H.
For Simulation 1, we visualize the RPS in a heatmap in Figure G.2. As the size of the dataset
increases, the Rashomon set becomes smaller as we become more confident in our estimates.
G.2. Simulation with linear outcomes. Here, we discuss the setup for the simulation described
in Section 8 and Figure 9. As described previously, there are three features, age, drug A, and drug
B. Age takes on two values, young and old, denoted by 1 and 2 respectively. Drug A takes on three
non-control dosages {1,2,3} and drug B takes on five non-control dosages {1,2,3,4,5}. We assume
that there is no treatment effect unless drug A and drug B are taken together. The partition matrixRASHOMON PARITITONS FOR HETEROGENEITY 82
Figure G.1. Visualizing the Rashomon set through a heat map. This heatmap
actually reflects a 2D histogram binned by the model size (number of pools in a
partition) and the relative posterior probability ratio i.e., (P(Π | Z) − maxP(Π |
Z))/maxP(Π | Z). The color of the bin reflects the number of times, averaged per
simulation, a model at that sparsity and probability (distinct models may be in the
same bin) appear in some Rashomon set. One might refine the set of partitions
further by the probability and the sparsity. For example, if we want models with
a relative probability of at least -0.25, then we look only at models that are above
the dashed black horizontal line. If we want models with fewer than 6 pools, then
we look only at models to the left of the dashed black vertical line. If we want both
criteria to be satisfied, we look at the top left box.
Figure G.2. Visualizing the Rashomon set in Simulation 1. Notice how as the
size of the data set grows, the Rashomon set concentrates around a few very good
models, one of which corresponds to the data generating process.RASHOMON PARITITONS FOR HETEROGENEITY 83
π π
11 12
[o,3,b] [o,3,b]
π π
9 10
[o,2,b] [y,3,b] [o,2,b] [y,3,b]
π π
7 8
π π
5 6
[o,1,b] [y,2,b] [o,1,b] [y,2,b]
π π
3 4
[y,1,b] [y,1,b]
π π
1 2
(a) Drug B dosages, b∈{1,2} (b) Drug B dosages, b∈{3,4,5}
Figure G.3. Hasse diagram for simulation with linear outcomes. y is young and o
is old.
is
 
0 − − −
 
Σ = 0 0 − −.
 
 
1 0 1 1
We visualize the twelve pools in Figure G.3 indicating heterogeneity in age and the dosages of drugs
A and B. And the linear coefficients for the outcomes in each pool are given by
β = [0,−1,0,1]
1
β = [1.5,−4,0,1.5]
2
β = [0,−1,−0,1]
3
β = [4.5,−4,0,0.5]
4
β = [4,−2,−1,1]
5
β = [1,1,1,−1]
6
β = [−3,2,−3,1]
7
β = [0,0,0,0]
8
β = [4,2,−3,−1]
9
β = [0,0,0,0]
10RASHOMON PARITITONS FOR HETEROGENEITY 84
β = [5,2,−3,0]
11
β = [5,−1,0,−1],
12
where the first coefficient is the intercept and the remaining elements are slopes on each feature.
For feature profiles with zero treatment effect, we set the effect to be 0, a constant. For the feature
profile where drugs A and B are administered together, a random error is drawn independently
and identically from N(0,1). We draw 10 measurements for each feature combination. We set
λ = 4×10−3.
Appendix H. Appendix to Empirical Data Examples
H.1. Charitable giving and telomere lengths. Figures H.1 and H.2 visualize the Rashomon
sets for the charitable giving datasets of Karlan and List (2007) and the NHANES telomere lengths
using the 2D histogram that is described in Figure G.1 in Appendix G.
H.2. Heterogeneity in the impact of microcredit access. For the microcredit data from
Banerjee et al. (2015), we present the results for all profiles in Figure H.3. This includes the
robust profiles we discussed in Figure 8 as well as the non-robust ones.
Additionally, we look at the treatment effect heterogeneity across genders,
HTE(x) = E[{Y (1,F,x)−Y (0,F,x)}−{Y (1,M,x)−Y (0,M,x)}],
i i i i
where Y (·,F,·) is interpreted as the potential outcome of household i were it headed by a woman,
i
and Y (·,M,·) is the potential outcome of household i were it headed by a man. As before, we
i
(cid:91) (cid:91)
use the sample means y(·) to find HTE(x) and sign{HTE(x)}. Again, we sort x into profiles and
(cid:98)
repeat the same counting and averaging exercise. We visualize the results as before in Figure H.4.
For most profiles, we see essentially no robust conclusions about gender heterogeneity in treatment
effects. We highlight a few robust items below.
We see an increase in loans procured by households headed by women with past business expe-
rience when compared to households headed by men. When these households are already in debt
with no previous experience, they tend to borrow less. We see no heterogeneity by gender in the
amount of informal loans procured.
Households headed by women tend to consistently spend more. However, they spend more
money on durable goods than households headed by men. We also see that, in the absence of past
experience, there is a decline in expenditure on tempting goods compared to households headed by
men. We also see a higher tendency for women to invest in business assets more than men.RASHOMON PARITITONS FOR HETEROGENEITY 85
We find that households headed by women with no past experience have a lower revenue than
men. Butthiseffectisreversedwhenthehouseholdsdohavepreviousbusinessexperience. However,
there is no heterogeneity by gender in the profit or the number of employees. We also find that
households headed by women tend to spend fewer hours working when they are in debt or when
there is regional competition. But this makes a negligible difference in the profits.
Wefindthatinhouseholdsheadedbywomen, thereislessparticipationinthebusinessbywomen
if the household is in debt and there is competition from neighbors. We also find that fewer girls
attend school in households headed by women with no previous experience than in households
headed by men.RASHOMON PARITITONS FOR HETEROGENEITY 86
Figure H.1. Visualizing the Rashomon set for Karlan and List (2007) charitable
donations dataset. The top two panels show the distribution of partition sizes and
a 2D histogram of how partition sizes and relative posterior probabilities vary. The
black dotted line in the 2D histogram shows our chosen Rashomon threshold. The
bottom two panels show the same after pruning low-posterior models.RASHOMON PARITITONS FOR HETEROGENEITY 87
Figure H.2. Visualizing the Rashomon set for NHANES telomeres dataset. The
top two panels show the distribution of size of models and their relative posterior
probabilityrelative. Theblackdashedverticalandhorizontallinesshowthesparsity
cutoffandRashomoncutoffrespectively. Thebottomtwopanelsshowthesameafter
pruning low-posterior models.RASHOMON PARITITONS FOR HETEROGENEITY 88
Figure H.3. Here, we visualize the average number of models in the Rashomon set
indicatingapositive, zero, ornegativeeffect. Eachcolumncorrespondstoadifferent
feature profile where the label denotes which features are active (i.e., do not take the
lowest level). “None” means that all features are taking these lowest values. We also
allow the gender of the household head and education status of the household head
to take on any value in all of the sixteen feature profiles.RASHOMON PARITITONS FOR HETEROGENEITY 89
Figure H.4. Here, we visualize the average number of models in the Rashomon set
indicating a positive, zero, or negative effect. The axis labels should be read as in
Figure H.3.