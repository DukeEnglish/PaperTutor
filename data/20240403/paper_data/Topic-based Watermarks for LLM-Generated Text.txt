Topic-basedWatermarksforLLM-GeneratedText
ALEXANDERNEMECEK,YUZHOUJIANG,andERMANAYDAY,CaseWesternReserveUniversity,USA
Recentadvancementsoflargelanguagemodels(LLMs)haveresultedinindistinguishabletextoutputscomparabletohuman-generated
text.WatermarkingalgorithmsarepotentialtoolsthatofferawaytodifferentiatebetweenLLM-andhuman-generatedtextby
embeddingdetectablesignatureswithinLLM-generatedoutput.However,currentwatermarkingschemeslackrobustnessagainst
knownattacksagainstwatermarkingalgorithms.Inaddition,theyareimpracticalconsideringanLLMgeneratestensofthousands
oftextoutputsperdayandthewatermarkingalgorithmneedstomemorizeeachoutputitgeneratesforthedetectiontowork.In
thiswork,focusingonthelimitationsofcurrentwatermarkingschemes,weproposetheconceptofa“topic-basedwatermarking
algorithm”forLLMs.TheproposedalgorithmdetermineshowtogeneratetokensforthewatermarkedLLMoutputbasedonextracted
topicsofaninputpromptortheoutputofanon-watermarkedLLM.Inspiredfrompreviouswork,weproposeusingapairoflists
(thataregeneratedbasedonthespecifiedextractedtopic(s))thatspecifycertaintokenstobeincludedorexcludedwhilegenerating
thewatermarkedoutputoftheLLM.Usingtheproposedwatermarkingalgorithm,weshowthepracticalityofawatermarkdetection
algorithm.Furthermore,wediscussawiderangeofattacksthatcanemergeagainstwatermarkingalgorithmsforLLMsandthebenefit
oftheproposedwatermarkingschemeforthefeasibilityofmodelingapotentialattackerconsideringitsbenefitvs.loss.
AdditionalKeyWordsandPhrases:LargeLanguageModels(LLMs),Watermarking,Fingerprinting
1 INTRODUCTION
Therapidexpansionofartificialintelligenceandnaturallanguageprocessing(NLP)inrecentyearshasallowedforthe
developmentoflargelanguagemodels(LLMs)tobecomeaprominentaspectinacademiaandindustry[6].Modelssuch
asOpenAI’sChatGPT[16],enablescapabilitiesforgeneratinghuman-liketextbyunderstandinguserinputandlearning
fromextensiveamountsofdata[18,26].Asmoreusersaredrawntothesesystems,theamountofLLM-generatedtext
overtheinternetincreases,causingpotentialmisinformation,copyright,andplagiarismissues[10,19].Theseconcerns
resultintheneedfortoolsthatcandifferentiateLLM-andhuman-generatedmaterial.
WatermarkingalgorithmshavepotentialtoprovidewaysinwhichonecanidentifyLLM-generatedtextbyembedding
anidentifiablepatterninthetextsequence[23].CurrentwatermarkingschemesforLLMsinvolveadjustingtheoutput
token(i.e.,unitsoftextusedinNLPanalysis,suchaswords,phrases,orcharactersaselementsforprocessinginNLP
tasks)distributionstrategicallybyseparatingtokensinto“green”𝐺 and“red”𝑅listsandenhancingthelikelihood
of𝐺 listedtokensover𝑅listedtokenstobeusedintheLLMoutput[11,27].ThedetectionofawatermarkedLLM-
orhuman-generatedtextsequenceisdeterminedbythenumberof𝑅listedtokensinthetextsequence.Ahuman-
generatedoutputisexpectedtocontainsignificantlymoretokensinthe𝑅listthaninthe𝐺 list,theoppositeistruefor
LLM-generatedoutputcontainingmore𝐺 listedtokenscomparedtothenumberof𝑅listedtokens.
Onemajorlimitationofcurrentwatermarkingalgorithmsisthelackofrobustnessagainstdifferentattackmodels
concerningtextinsertion,manipulation,substitution,anddeletion.Insuchattacks,thegoalofapotentialattackeris
totamperwiththeLLMoutputtoremoveordistortthewatermark,sothewatermarkdetectionmechanismcannot
identifyLLM-generatedoutputwithhighconfidence.Inaddition,efficiencyandpracticalitylimitationsarisewith
thegrowthofLLMsandtheirusage.Thecreationof𝐺 and𝑅listsforawatermarkbecomesimpracticalbecausethe
detectionmechanism’scomputationalloadrequiresittoexhaustivelyexamineallseedsusedtorandomlypartitionthe
vocabularyoftheLLMintoa𝐺 listthegeneratedoutputissampledfrom.Withcurrentmodelschemes,themodel
Authors’address:AlexanderNemecek,ajn98@case.edu;YuzhouJiang,yxj466@case.edu;ErmanAyday,exa208@case.edu,CaseWesternReserve
University,10900EuclidAve.,Cleveland,Ohio,USA,44106-1715.
1
4202
rpA
2
]RC.sc[
1v83120.4042:viXra2 Nemeceketal.
ownermemorizeseachqueryreceivedinordertodifferentiatebetweenanLLM-andhuman-generatedtextsequence
byusingthe𝐺 and𝑅pairoflists.
Toresolvetheaforementionedlimitationsofcurrentwatermarkingalgorithms,weproposetheconceptofanew
modeltoenhancetherobustnessandefficiencybyutilizing𝐺 and𝑅listsinatopic-basedmanner.Usinganinputtext
sequenceortheoutputfromanon-watermarkedLLMoutput,topics(e.g.,sports,health,politics,technology,etc.)are
extractedtogenerateapairoflistsforeachtopic.Thedesignatedlistsforeachtopicareemployedtodetectwhethera
targettextsequencewasgeneratedbyanLLMorbyahuman.ThepracticalityoftheproposedschemeallowstheLLM
ownertodisregardtheinfeasibletaskofiteratingoverallpossiblelistsorseedsforthelistpairsofanindividualtext
inputsequenceandtogenerateonlytopic-basedseedsforpairsofliststowatermarkthetextoutputsequence.On
theotherhand,potentialattacksagainstwatermarkingalgorithmsmainlyaimattemperingwiththewatermarkin
suchawaythatthedetectionalgorithmfailstodetectthewatermark.Usingatopic-basedwatermarkingalgorithmfor
LLMsalsomakesiteasiertomodelthebenefitvs.lossofsuchattacks.Forinstance,ifanattacker,aimingtodistort
thewatermark,modifiestheLLMoutputinsuchawaythatthetopicoftheoutputchanges,andhencethedetection
algorithmfailswithhighprobability,alsosignificantlyreducestheutilityoftheLLMoutput.Therefore,topic-based
watermarkingcanbeusedtoquantifythe(i)successoftheattackerincheatingthedetectionalgorithmand(ii)decrease
intheutilityoftheLLMoutputduetotheattack.
Thefocusofthisarticleistodiscusscurrentwatermarkingalgorithms,highlighttheirlimitations,andprovidethe
conceptofanewwatermarkingschemethatmitigatessuchlimitations.Specifically,weexaminethefunctionalityof
contemporarywatermarkingandextendablerobustnessschemes,includinganalgorithmicandlimitationstandpoint
foreachscheme.Wethenproposeanoverviewofourtopic-basedwatermarkingschemeforLLM-generatedtext,how
ourmodelcanaddresscurrentlimitations,andlimitationsoftheproposedmodelitself.Finally,wepresentfuture
researchandstudiestoempiricallyevaluatetheproposedtopic-basedscheme.
2 BACKGROUND
IntroducedbyVaswanietal.[25],LLMsleverageatransformarchitecturewhichutilizeattentionmechanisms,allowing
ittoweighrelevantfeatures(wordsortokens)inasentence,regardlessoftheirpositionaldistanceinrelationfrom
oneanother[4].TraininganLLMinvolvesfeedingitextensiveamountsoftextdataandadjustinginternalparameters
basedonpredictionerrorsthemodelmakes[21].TheobjectivefortrainingtheLLMisfocusedaroundnext-token
predictionswheretheaimistopredictthenexttokeninasequencegiventhetokenswhichprecedeit.
ThegenerationoftokenstooutputcoherenttextbyanLLMisasequentialprocess,whereeachnewtokenis
producedbasedonthecontextprovidedbypriortokens.Thisprocessreliesonaprobabilisticmodelwhereforany
givensequenceoftokens,theLLMcalculatestheprobabilitydistributionoveritsvocabularyforthenexttokeninthe
sequence.DifferentstrategiestheLLMemploystoselectthenexttokensuchas,selectingthetokenthathasthehighest
probability(greedyornaivemethods)[3].Theprobabilitiesassociatedwitheachnexttokenarederivedfromthe
LLM’slogits.Logitsaretheraw,unnormalizedoutputsoftheLLM’sfinallayer(directoutputsfromtheLLM’sneural
networkcomputationsbeforeanynormalizationhasbeenapplied)whichyieldstheprobabilitiesforeachtokeninthe
vocabulary.Theseprobabilitiesreflectthemodel’slearnedassociationsanditsconfidenceindifferentcontinuationsof
theoutputtextsequence.Topic-basedWatermarksforLLM-GeneratedText 3
3 RELATEDWORK
Inthissection,wediscuss,fingerprintingmethodologies(thatdonottargetLLMs),tothebestofourknowledge,the
firstwatermarkingschemeforLLMsandextensionsofrobustwatermarkingschemes.
3.1 Fingerprinting
Fingerprintingisatechniqueusedtoidentifycontentbycustomizing(fingerprinting)everyinstanceofadistributed
copyofthecontent.Bydoingthis,ifthecontentisdistributedwithoutauthorization,theoriginalsourceofthedataleak
canbeidentified[1].Themethodisanalogoustohowhumanfingerprintsareusedtoidentifyindividuals.Research
concerningtherobustnessoffingerprintingmethodologiesexistsinavarietyoffieldstoallowclaimtocopyrightand
toidentifysourcesofdatabreaches.PriorresearchbyJietal.[7]focusonrobustfingerprintingmethodsofgenomic
databases.Theyguaranteeliabilitywhensharinggenomicdata,specificallymitigatingcorrelationattacks(anattack
aimingtodetectanddistorttheaddedfingerprint)againstmaliciousdatabaserecipientsbychangingorfingerprinting
databaseentries.Multiplefingerprintingschemeshaveshownrobustnessagainstdifferentthreatmodels[7–9],however
anotablelimitationofmanyfingerprintapproachesistheirinapplicabilitywithinthecontentofanLLM.
3.2 WatermarkingAlgorithmsforLLMs[11]
Tothebestofourknowledge,thefirstwatermarkingschemeutilizesaprobabilisticpairof“red”𝑅(tokensthatcannot
beused)and“green”𝐺 (tokensthatcanbeused)listsforLLM-generatedtextoutput.TheworkbyKirchenbaueret
al.[11],offersthegenerationoftwotypesof𝑅lists,asoft𝑅list𝑅
𝑠
andahard𝑅list𝑅 ℎ,alongwiththeirrespective𝐺
lists.Bothtypesof𝑅and𝐺 listsareusedinthedetectionofhuman-versusLLM-generatedtextoutput,comparingthe
numberoftokensfromthe𝑅and𝐺 liststothetargettextsequence(e.g.,outputofanLLMorhumanindividual).
Thegenerationofthe𝑅 ℎ and𝑅 𝑠 listsutilizetheinputpromptandapriorsequenceofknowntokenscontaining
thefirst𝑡−1tokensalreadyproducedbytheLLMtoeithergenerateprobabilities(probabilityofeachwordoverthe
vocabularyoftheLLM)orlogits(discreteprobabilitiesoverthevocabularyoftheLLM)dependingonthetypeof𝑅list.
Byutilizingthelastknowntokenovertheinputsequence,ahashiscomputedtoseedarandomnumbergenerator
whichisthenusedtorandomlypartitionthevocabularytheinto𝐺 and𝑅lists.
Thedifferencebetweenthe𝑅 𝑠and𝑅 ℎlistsisduetothe𝑅 𝑠listutilizingtheadditionofaconstant“hardness”parameter
toeachofthe𝐺listlogits.Thehardnessparameterallowsthesamplingdistributionforthenexttokeninthegenerated
sequencetostronglyfavorthe𝐺 listforhighentropytokens.Highentropytokensareasequenceoftokensthatare
goodchoicesforthenexttokeninthegeneratedsequence,whilelowentropytokensconsistofoneorfewgoodchoices
oftokenstochoosefromasthenexttokeninthegeneratedoutput.Thedetectionofthewatermarkforboth𝑅listsare
similaranditisdonebycheckingeachtokeninatargettextsequenceandexamineforviolationsinthe𝑅list.
Therearelimitationsconcerningthe𝑅listgenerationofthewatermarkedLLM-generatedoutput.Tobeabletodetect
thewatermark,the𝑅listsneedtobecorrectlyreproducedspecifictothegeneratedoutputtext.Limitationsariseifthe
listscannotbeproperlygeneratedwhichmayleadtoincorrectconclusionsontheanalyzedtargettext(i.e.,whether
itwasLLM-orhuman-generated).Overheadonmaintainingthewatermarkingschemealsoaddscomplexitytothe
LLM.Itbecomesinfeasibletoexhaustivelyiterateoverallmemorizedlistsortheirspecifichashfunctionsandrandom
numbergeneratorsduetothenumberofusersproprietyLLMshaveexperiencedinrecentyears.Ourproposedmodel
ishypothesizedtodetectwatermarkedLLM-generatedtextsequencesefficiencyandfeasibly,reducingthecomplexity
ofthesizeofseedsforeachpairoflists,leadingtoaccurateclassificationsbetweenLLM-andhuman-generatedoutput.4 Nemeceketal.
3.3 RobustWatermarking
Limitations considering robustness of watermarking schemes has led to an influx of research for improving the
constraintsoftheseinitialalgorithms[2,12–14,22,24,27].Thefirstwatermarkingalgorithmextendsitsrobustness,
describingarobustprivatewatermarkingalgorithm[11].Therobustnessextensionofthewatermarkinvolvesthesame
generativeprocessofpreviouslymentionedwatermarkingschemeswiththeadditionofasinglesecretkeyormultiple
secretkeys.Thesecretkey(s)enhancesthesecurityandrobustnessonthewatermarkbygeneratinguniquetokensfor
the𝑅listutilizingapseudorandomfunction.Theadditionsofthekey(s)furtheraddressthelimitationofreproducibility
concernsforthe𝑅and𝐺 lists.
Continuationofworkdedicatedtothelackofattackrobustnessandsecurityrobustness,Liuetal.[14]examine
aproposedwatermarkingalgorithmtoenhanceattackrobustnessondifferentsemanticallyinvariantperturbations
(e.g.,textmodifications,textalterations),embeddingwatermarkingsbasedonsemanticsoftheentirepriortextinstead
ofafixednumberofpriortokens.Analysisoftheirproposedsemanticinvariantwatermarkingschememaintained
robustnessagainstattacksschemessuchastextparaphrasing(rewritingthewatermarkedtextwhilepreservingits
originalmeaning)andsynonymsubstitution(certainwatermarkedwordsarereplacedwiththeirrespectivesynonyms).
Christetal.[2]buildonsecretkeyutilizationproposinganundetectablewatermarkingschemeutilizingcryptographic
one-way functions making it computationally infeasible to distinguish a watermarked output compared to other
non-watermarkedtext.Thewatermarkingschemeembedsasecretintheoutputtexttowhichitdoesnotaffectthetext
qualitygeneratedbytheLLM.Thedetectionalgorithmoftheundetectablewatermarkingschemeidentifiestheoutput
ofthewatermarkedLLMwithoutanyinformationotherthantheoutputtextsequenceandthesecretkey.
WhileresearchinrobustnessofwatermarkingschemesforLLMsisquicklyadvancing,toourknowledge,thereis
littleworkconcerningactualcommercializedscenariosinwhichausablewatermarkingdetectionschemeisfeasible.
Lietal.[12],proposesawatermarkschemeusableinthecommercialsettings,introducingdifferentbackdoordata
paradigmswhichintegratethewatermarktotheLLMduringthetuningprocess.Themotivationoftheauthorsisto
guaranteethecopyrightofacustomizedLLMfromabusinessperspectivecomparabletopriorfingerprintingschemes
concerningthecopyrightofgenomicdatabases[7].Priorresearchhasalsoassumedblack-boxdetectionapproaches
wheretheinternalworkingsofthemodelareundisclosedtotheuser.Limitationsforblack-boxdetectionschemes
revolvearounddatabiaseswherethedatacollectionofLLMscanintroducebiasesimpactingperformanceandthe
abilitytogeneralizeacrossdifferentcontextofthedetectionmechanism,confidencecalibrationaffectingtheoverall
evaluationmetricsoftheLLM,indicatingthereliabilityandtrustworthinessoftheLLM,andthelackofadaptabilityto
advancethelearningoftheLLM[24].Ourproposedwatermarkingmethodologydifferentiatesfromexistingliterature
withanapproachtostorethe𝐺and𝑅liststailoredtoaspecifiedtopicofthetextinputsequencereducingcomputation
whilestillprovidingrobustnessfromanattackmodelperspective.
4 PROPOSEDMETHOD
Thissectionprovidesanoverviewoftheproposedwatermarkingframeworkandotheraspectsofthemodeltobe
considered.First,inSection4.1,wediscussdifferentrelevantattackmodels.InSection4.2,weintroducethehighlights
ofourproposedwatermarkingframeworkwhilecomparingittotheworkofKirchenbaueretal.[11].Then,inSection
4.3,wediscussourwatermarkdetectionschemetodifferentiatebetweenhuman-andLLM-generatedoutput.Finally,in
Section4.4,wetouchonthelimitationsofourproposedwatermarkingmethodology.Topic-basedWatermarksforLLM-GeneratedText 5
4.1 ThreatModel
EmbeddedwatermarksinanLLM’sgeneratedoutputaresubjecttovarioustypesofattackssummarizedinthefollowing.
Notethatallconsideredthreatmodelsarethosewherethemotivationofanattackeristodistortorremovethewatermark
fromthetargettextsequencetoavoiddetection.However,wehypothesizemanipulationtothetextoutputsequence
willdegradetheoverallqualityoftextandthetopicaccuracy.Toassesstheimpactofmanipulationsonthetextquality,
weutilizetheevaluationmetric,perplexityseeninpriorresearch[11]tomeasurehowwellaprobabilitymodelpredicts
asampledtoken.Theperplexitymetricisrelevantforquantifyingthedegradationintextqualityduetounexpected
tokensasaresultofeffortstoevadewatermarkdetection.
Whileweexaminearangeofreal-lifeapplicationsforwatermarking,wespecificallyhighlightthesubmissionof
courseworkwithinacademicenvironmentsasanexample.Thisscenarioservestoillustratethebroaderrelevanceand
necessityofwatermarkingtechniques,withoutconfiningthediscussionsolelytoacademia.Inthiscontext,thequality
ofmanipulatedtextisdeterminedbyassessingitscoherence,measuredbytheevaluationmetricofperplexity.Italso
examineswhetherthetextsequenceaccuratelyaddressesandanswersspecificquestionsrelatedtothecoursework.
Thisapproachhighlightstheimportanceofbothintegrityandaccuracyinanacademiccontext.Weassumethat,
inordertoremainundetectabletothedetectionmechanism,amalicioususerwillmanuallychangetokenswhile
preservingtextqualityinrelationtothespecifiedtopic(s).However,inthecaseofcourseworksubmissions,preserving
thequalityextendsbeyondtopicrelevancetoincludeprecisionandaccuracyoftheresponsesoftheposedquestionsin
thecourseworksubmission.Modelingofthisadditionalqualityisyettoberealized,butaddressestheimportanceof
generalandcontext-specificconcernsinwatermarkdetection.
Potentialattacksconcerntextinsertion,substitution,anddeletioninthegeneratedoutputtextsequenceatthe
character,word,sentence,andmulti-modellevel[5,11].Insertionistheadditionorinsertionoftokensintothetext
sequence.Withthewatermarkedtextencompassingsignificantlymore𝐺listedtokens,theadditionofnewtokensmay
violatethe𝑅list,increasingthecountof𝑅listedtokensrespectivetothe𝐺 listedtokens.Weassumetheattackerwill
needtoinsertasignificantamountof𝑅listedtokenstodecreasedetectionpowerofthewatermarktoclassifythetext
ashuman-generated.Inthecontextofcourseworksubmission,givena1-pageLLMoutputforanassignment,astudent
canaddanotheroneormoresubsequentpagesoftext(human-generated)todecreasedetectionpower.However,this
involvesatrade-offwhereifthestudentisabletowriteoneormorepagesworthofcontentmanuallytoremovethe
watermark,therewouldbenoneedtouseLLM-generatedtextatall.Thescenarioofinsertionattacksisrealisticbut
thescenarioinwhichanattackerwouldneedtomanuallygeneratethesameormorecontentthantheLLM-generated
outputisunrealistic,henceweexpectareasonablelevelofinsertiontokensduringourevaluation.Substitutionof
tokens,involveswappingtokensinthegeneratedoutputtext.Forexample,anattackermaysubstitutetokensinthe
textsequencewiththeirrespectivesynonym.Themotivationoftheattackerinaninstanceofsubstitutionistoremove
𝐺 listedtokensandadd𝑅listedtokenstodegradeorremovethewatermark.Deletioninvolvesdeletingtokensfrom
ageneratedoutputsequence.Ifasignificantamountof𝐺 listedtokensaredeleted,theamountof𝑅tokenswould
increasehowever,ifonlytokensaredeletedfromatextsequence,thereexistsapotentialtrade-offbetweenthetext
qualityanddetectionaccuracy.Wewouldalsoexpectareasonablelevelofdeletiontoexistintheevaluationphaseof
ourresearch.Tokeninsertion,manipulation,anddeletionareusedintandemtodistortorremovethewatermarkofan
LLM-generatedoutput.Intherestofthissection,wediscussconsideredthreatmodelsanattackercanperformagainst
theLLMinourevaluation.6 Nemeceketal.
4.1.1 BaselineAttack. Baselineattacksconsistoftheinsertion,substitution,anddeletionoftextforagivenoutput
sequence.Theattackerselectsasingleoracombinationoftechniques(insertion,substitution,anddeletion)withthe
objectivetodiminishdetectionaccuracy.Indoingso,thisapproachmayleadtoacompromiseinthetextqualityofthe
manipulatedsequence.
4.1.2 ParaphrasingAttack. Aparaphrasingattackisacategoryofabaselinesubstitutionattack.Aspreviouslystated,
executionofthisattackmaybemanualbyanindividualorbyrephrasingtheoutputviaanLLM.Theproposed
watermarkingschemeisrobustagainstvaryingscenariosofanattackerparaphrasingtheoutputfromthewatermarked
LLM.Weconsiderseparateparaphrasingattackscenarios.
• ParaphrasingwiththeOriginalLLM:TheattackergeneratesawatermarkedtextsequencebyfeedingtheLLM
aninputprompt.Weassumetheattackerdoesnotmanuallyalteranytokensintheoutput.Theattacker
subsequentlysuppliesthewatermarkedoutputintotheidenticalLLMwhichoriginallyproducedit,along
withaparaphrasecommand(forexample,“rephrasethistextprompt”).Theproposedwatermarkingschemeis
robustagainstparaphrasingthroughtheoriginalLLMduetothenatureoftopicsthatformthebasisofthe
watermarkingprocess.Forinstance,iftheattacker’sinitialpromptcontainedthegeneratedtopicof“sports”,
anyparaphrasedoutputwouldlikelymaintain“sports”asthetopic,ensuringthewatermarkfortheoriginal
andparaphrasedoutputsareconstant.
• ManualParaphrasing:TheattackergeneratesanoutputtextsequencebythewatermarkedLLMfromagiven
input prompt. The paraphrasing is conducted manually by the attacker without the aid of external tools.
Thismanualparaphrasingapproachisconsideredforshortersequencesoftext,aslongersequenceswould
necessitatemoreextensiveinterventionfromtheattacker.Thetopic-basedwatermarkingschemeproposedis
particularlyeffectiveagainstmanualparaphrasing,especiallyforlengthiertextsequences,whichwouldrequire
significantmodificationtomisleadthedetectionmechanismintoincorrectlyclassifyingtheLLM-generated
textforhuman-generatedtext.Therobustnessoftheproposedwatermarkingschemeisfurtherenhancedby
thepotentialdeteriorationintextqualityandthepossiblechangeintheoveralltopicsoftheparaphrasedtext
sequence.
4.1.3 TokenizationAttack. Atokenizationattackisclassifiedasaformofinsertion,whereatokenismodifiedinto
multiplesubsequenttokens.Forexample,giventhetoken“sports”withina𝐺 list.Anattackercanaddcharacters
suchas‘_’or‘*’,creatingadditionaltokenscategorizedas𝑅listedtokensbythedetectionscheme.Theoriginal𝐺
listedtoken,“sports”,ismanipulatedinto“s_p_o_r_t_s”,creatingmultiplenew𝑅listedtokensinthetextsequence.
Therobustnessoftheproposedtopic-basedwatermarkingschemeconcernsthedegradationintextqualityduetothe
intentionaladditionofnoise(unwantedcharacters)withintokensinthe𝐺 list.
4.1.4 DiscreteAlterationAttack. Anattacker’smotivationbehindthisattackistoinducealterationsintokenssub-
sequentlyintroducingmisspellingandgrammarerrors.Thesemanipulationscanbeexecutedthroughinsertionor
deletionofsingleormultiplecharacters,orevenentirestingsofcharacterswithinthespecifiedoutputsequence,
withtheaimofdiminishingtheeffectivenessofthewatermarkindetection.Weconsideranattackerwhichseeksto
underminetheintegrityofawatermarkedtextsequencewithoutcompletelyalteringthetext.Byinsertingordeleting
characterswithincertaintokens,theattackercanmodifythetexttointroduceerrors,whileminor,canimpactthe
detectabilityofthewatermark.Forexample,alteringtheword“watermarked”to“wat3rmark3d”throughcharacter
substitutionintroducingmisspellingbycomplicatingtheprocessofwatermarkrecognition.Theproposedtopic-basedTopic-basedWatermarksforLLM-GeneratedText 7
watermarkaddressescurrentwatermarkingalgorithmlimitationsthroughtheimplementationofthegeneralizedtopic
scheme.Eveninthepresenceofminorspellingorgrammaticalerrors,theoverarchingtopicoftheoutputremains
unaffected.Thisensuresthatthewatermark’sintegrityismaintained,showcasingtherobustnessoftheproposed
approachagainsttextualalterations.
4.1.5 CollusionAttack. AcollusionattackbeginswithanoriginaltextsequencethathasbeenprocessedbyanLLM,
whichincorporatesawatermarkingschemebasedontopicrelevance.WeassumethatallLLMshaveatopic-based
watermarkingschemeimplementedforoutputtextgeneration.Theattackerthenfeedsthiswatermarkedoutputintoa
differentLLM,aimingtoalterthecontentofthetextthroughinsertion,manipulation,ordeletion.Thisprocessmight
modifythetopicsextractedfromthetext,dependingontheextractionmethodsandthedefinedlistpairsofgeneral
andspecifictopics(𝐺and𝑅).ShouldthesecondLLMemployitsdetectionmechanismforclassificationpurposes,it
wouldlabeltheoutputas“LLM-generated”,effectivelycounteringcollusionattacksbyrecognizingthemanipulation.
HowevertheintroductionofmultipleLLMsintothisprocessdisruptsthecorrelationbetweenthe𝐺 and𝑅listpairs,
renderingthedetectionofthetextsequencechallengingandpotentiallyleadingtoincorrectclassifications.Whilethe
useofmultipleLLMsintextmanipulationattackspresentsacomplexchallengeforthedetectionmechanismdueto
thepotentialalterationofextractedtopicsandthebreakdownofgeneralandspecifictopiccorrelations,employing
detectionmechanismscanhelpidentifyLLM-generatedcontent.Nonetheless,theeffectivenessofthesemechanisms
mightbecompromisedasthenumberofinterveningLLMsincreases,highlightingtheneedforadvancedstrategiesin
watermarkinganddetectiontomitigatesuchthreats.
4.2 ProposedWatermarkingScheme
AsdiscussedinSection2,LLMsgeneratethenexttokenintheoutputtextsequencefromthepreviouslygenerated
token,specificallytheworkby[11],thetokensaretakenfromthespecified𝐺list.Ourproposedwatermarkingscheme
is inspired by this process, however our methodology differs with the utilization of extracted topics from a non-
watermarkedLLM-generatedoutputandthegenerationof𝐺 and𝑅listpairsforeachtopicinfluencingawatermarked
LLM.
Weproposetheconceptoftopic-based𝐺 and𝑅listpairscombinedwithatopic-basedwatermarkingmechanism
(TBWM). The proposed TBWM framework consists of two different LLM components, a watermarked and non-
watermarkedLLM.TheTBWMextractsthetopicfromtheinputtextsequenceoranon-watermarkedLLM-generated
outputbasedontheinputtextsequence.Figure1describestheproposedtopic-basedwatermarkingscheme.
4.2.1 Non-WatermarkedLLMTopicExtraction. Aproposedmethodfortopicextractionemploysanunwatermarked
LLM.ThisprocessinitiateswithauserpromptingtheLLMwithaninputtextsequence.Subsequently,thisinput
sequenceisintroducedtotheunwatermarkedLLMwhichproducesanon-watermarkedoutputtextsequence.This
outputservesasthebasesforextractingtopicsrelevanttotheoriginalinputprovidedbytheuser.Furthermore,the
non-watermarkedoutputisthenfedintoawatermarkedLLMincombinationwithacommandtorephrasethetext,
suchas“rephrasethistext.”Boththenon-watermarkedoutputandtheadjustmentsintokenweight,derivedfromthe
extractedtopics,areemployedwithinthewatermarkedLLMtogenerateawatermarkontheoutputtextsequence.The
utilizationofthenon-watermarkedovertheinputtextsequenceistoextractaccuratetopicswhen(i)asmallinput
promptoccurs,consistingofacouplewordstoasentenceinlengthand(ii)whenmisspellingsandothergrammarissues
arepresent.Theoutputofthenon-watermarkedLLMallowsforasufficientamountofqualitytexttobegeneratedin
ordertoextractanaccuratetopic,identifyingthetopicofinterestofthecorrespondinguser.Furthermore,forasingle8 Nemeceketal.
Fig.1. Topic-basedwatermarkingmechanism(TBWM).Theinputpromptispassedtonon-watermarkedLLMtoextractthetopic(s)
ofinterest.Utilizingthetopic(s),tokenweightsareadjustedforthewatermarkedLLMwiththeoriginalnon-watermarkedoutput.
inputtextsequence,thewatermarkingschemecanextractmultipletopics.Forinstance,inaninputaskingformedical
injuriesindifferenttypesofsports,thenon-watermarkedoutputcouldsummarizethegeneralizedtopicsfromthis
specificoutputas“sports”and“medicalinjuries.”However,astonotdegrademodelperformanceortopicextraction
quality,givenalargenon-watermarkedoutput(e.g.,paragraph(s)),thetopicextractionmayiterateoverallsentencesin
thetextsequencetoanalyzeforrelevanttopicsandskipthosethatarenotstronglyrelatedtospecifiedtopics.The
extractedtopicorcombinationoftopicsareusedforseedand𝐺 and𝑅listpairgenerationbasedonthenumberof
extractedtopics.The𝐺 and𝑅listpairsareutilizedtotunethetokenweightsforthewatermarkedLLMalongwith
thepriornon-watermarkedLLM-generatedoutputthatwasusedfortopicextraction.ThewatermarkedLLMutilizes
thenon-watermarkedoutputtextsequenceincombinationwitharephrasecommandtocorrectlysamplefromthe
generated𝐺 listpertopic.SimilartothewatermarkingalgorithmproposedbyKirchenbaueretal.[11],wesample
tokensfromthe𝐺 listtogenerateanoutputsequencefromtheoriginalinputprompt.
Theutilizationoftheglobaltopic-based𝐺 and𝑅listsallowforthewatermarkingofaninputtextsequencetobe
computationallysufficientbygeneratingcertainlistsbasedonthetopicsallottedbythemodel.Thisgeneralizationof𝐺
and𝑅listssignificantlydecreasestheoverallnumberoflistpairsneededforthedetectionofthewatermark.
4.2.2 InputPromptTopicExtraction. Thereexisttwodifferentmethodsfortheextractionoftopics:eitherdirectlyfrom
theuser’sinputtextsequenceorfromanoutputproducedbyanon-watermarkedLLM,whichitselfisbasedonthe
inputtextsequence.Theextractionoftopicsfromtheinputtextisfeasiblewhenthetextisfreefromspellingand
grammaticalerrorsandcontainsenoughtexttoaccuratelyextracttopics.Simultaneously,theinputtextisprocessed
throughanon-watermarkedLLM,resultinginanunwatermarkedoutputbasedonthesaidinput.Thenon-watermarkedTopic-basedWatermarksforLLM-GeneratedText 9
outputservesastheinputforthewatermarkedLLMalongwiththeoutputandtheadjustmentsintokenweight,
derivedfromthetopicsextractedfromtheinputtextsequence,areutilizedwithinthewatermarkedLLMtoproducea
watermarkedoutputtextsequence.Theapproachwheretopicsareemployedtogeneratethe𝐺and𝑅listpairsremains
constantacrossboththenon-watermarkedtopicextractionandtheinputprompttopicextractionmethods.
4.3 WatermarkDetectionMechanism
Existingwatermarkingalgorithmsthatrelyonthe𝐺 and𝑅listpairmethodologyarebasedontheoccurrenceof𝐺 and
𝑅listedwordswithinthetargettext(LLMoutputtextsequence)describedinSection3.2.Thisdetectionmethodfocuses
onthefrequencyof𝑅listviolationswithinthetargettext.Textproducedbyhumanindividualstypicallyexhibitahigher
usageof𝑅listedtokens,resultinginfewerinstancesof𝐺 listedtokensinthetargettext.WatermarkedLLM-generated
textincludessignificantlymore𝐺 listedtokensover𝑅listedtokens.Kirchenbaueretal.[11]demonstratedthelow
likelihoodofanindividualbeingabletogenerateatextsequence,aligningwith𝐺 listedtokenssimilartothatofa
watermarkedLLM-generatedoutput.
Theproposedtopic-baseddetectionframeworkseeninFigure2,mirrorstheapproachofleveraging𝐺 and𝑅token
listpairs.However,itintroducesanovelmechanismclassifyingtextaseitherhuman-orLLM-generatedbyextracting
andanalyzingthetopicspresentintheinputtextsequence.Theoverallprocessinvolvesthecreationofspecific𝐺
and𝑅listpairsforeachidentifiedtopicallowingthedetectionmechanismtoiteratethroughalllistpairspertopicto
performclassification.Specifically,uponreceivinganinputtextsequence,thewatermarkingframeworkidentifiesone
ormultipletopicspresentwithintheinputtext,utilizingtopicmodelingtechniques(e.g.,LatentDirichletallocation,
non-negativefactorization[15]).Foreachofthetopicsidentified,theframeworkaccessesthepredefinedpairof𝐺 and
𝑅lists,quantifyingtheoccurrencesoftokenfromthepairoflistswithinthetext.Giventhedistributionsof𝐺 and
𝑅tokensacrossmultipledifferenttopicswithintheinputtextsequence,weemployhypothesistestingtodetermine
thetargettext’sclassification.Thespecificstatisticaltestschosenwilldependonthedistributioncharacteristicsof
thetokenoccurrence,aimedatevaluatingtheconfidencelevel,whetherthetargettextishuman-orLLM-generated.
Kirchenbaueretal.[11]utilizeda“oneportionz-test”involvingthe𝐺listoftokenswiththeexpectedvalueoftokens𝑇
as𝑇/2andthevarianceof𝑇/4.Theoverallz-testisdenotedas:
√
𝑧=2(𝐺−𝑇/2)/ 𝑇 (1)
whichwewillevaluateinourproposeddetectionscheme.Thetopic-basedapproachallowsforageneralizedanalysis
ofthetargettext,whichwehypothesizewillaccommodatethemultipletopictextsequencesandprovideaworkflow
incorporatingstatisticalteststoensuretheaccuracyofthewatermarkingscheme.
4.4 Limitations
Theproposedwatermarkingschemeisdesignedwithcomputationalfeasibilityforenablingthegenerationof𝐺 and𝑅
listpairs.ThepracticallyofthemodelimplementationwilldependonthenumberofcategoriesallottedfortheTBWM.
Theanticipatedtrade-offtobemaintainedconcernscomputationalfeasibilityandthetextqualityoftheLLM.For
instance,shouldtheinputpromptbecategorizedunder“sports”withoutacorrespondingcomprehensivelistonthe
specifictopic,theLLM’soutputqualitymaybecompromised.Asgranularityofthegloballistpairsincreases,sotoo
doestextqualityattheexpenseofincreasedcomputationaldemandanddiminishedmodelfeasibility.10 Nemeceketal.
Fig.2. Detectionoftopic-basedwatermarkingscheme.Tokenlistpairs,generatedfromtheextractedtopicsoftheinputtext,are
comparedtotheoutputtextsequenceofthewatermarkedLLMtodeterminetheclassificationofthetargettextashuman-or
LLM-generated.
Althoughnotthefocusofthisarticle,itisimportanttoacknowledgethepotentialforspoofingattacks,asignificant
threatmodelthatcompromisestheprivacyandsecurityofwatermarkedLLMs.TheseattacksdeceiveeithertheLLMor
itsusersintobelievingthatagiventextoutputsequenceoriginatesfromadifferentsource(differentLLM).Previous
researchhasshowedthesusceptibilityofLLMstosuchattacks,withstudiesbySadasivanetal.[20],investigatingthe
capabilityofattackerstogeneratederogatoryoutputsfromLLMsbydeducingtextsignatureswithoutaccesstothe
detectionmechanisms.Conversely,Pangetal.[17]concentratedonthewatermarkingdimensions,fabricatingoutputs
thaterroneouslyappearwatermarked,bothwiththeunderlyingmotiveoftarnishingtheLLM’soritsdevelopers’
reputation.Furthermore,therobustnessoftheproposedtopic-basedwatermarkingschemeagainstcertainattacks
representsnotablelimitations[11,17].Specifically,theschemeisvulnerabletoattacksaimedatremovingthewatermark
throughalterationsintheoutputsequence,suchastheaddition,modification,ordeletionoftokens.Despitethese
constraints,thetopic-basedapproach,leveraginggeneralized𝐺 and𝑅 listspairs,couldpotentiallyofferenhanced
robustnessthroughadjustmentofweightswithintheglobaltopiclists.Theapproachcontrastswithgeneratinga
uniquepairoflistsforeachoutput,potentiallyprovidingalayerofdefenseagainstbothdirectalterationsandthe
threatofspoofingattacksbymakingthemodellesspredictableandhardertomanipulate.Theresilienceofthisscheme
tomanualmodificationsintheoutput,aswellasitseffectivenessindefenseagainstspoofingattacksthatexploitthe
model’ssensitivitytotopicpredictions,willbesubjectsforfutureexploration,aimingtodevelopawatermarking
solutionthatbalancesthedemandsofsecurity,practicality,andtextquality.
Furthermore,thisworkwillunderstandthetrade-offthatemergesfromrefininggranularityofgeneralizedcom-
prehensivelistpairswiththetopic-basedwatermarkingscheme.Therefinementofthistrade-off,whilebeneficialfor
enhancingthespecificityofthetextoutputsequence,unintentionallyintroducesincreasedsensitivitytotheaccuracy
oftopicprediction.SensitivitycouldpotentiallylowerthethresholdforadversariestoexploittheLLMbymanipulating
themodeltobypassthewatermarkingschemeorproducefalseoutputsequences.Thelimitationfallsinachievingthe
optimalbalancetomaintaintheintegrityofthewatermarkingframeworkbutalsoadheretothepossiblemanipulationTopic-basedWatermarksforLLM-GeneratedText 11
whichcouldcompromisethemodel’sfeasibility.Theevaluationforthistrade-offwillinvolvequantitativeassessments
ofcomputationalloadandtextqualityandenhancedrobustnessagainstmentionedattacks(paraphrasing,tokenization,
discretealterations,etc.).Refiningthegloballistpairsandthelimitationsoftheproposedmodel,furthermotivatefora
schemeconsistingofidealgranularityandtheabilitytoadapttoevolvingthreatlandscapes,preservingthequalityof
outputtextsequences.
5 CONCLUSION
Inthiswork,weproposeacomputationallyfeasibletopic-basedwatermarkingschemethatgeneratespairsof“green”and
“red”listsbasedonaninputtextprompttopicofanLLM.Weprovidebackgroundknowledgeofcurrentwatermarking
algorithmsanddiscusstheirlimitationsbeingthemotivationoftheproposedframeworkwiththelackofrobustness
againstknownattacksandtheimpracticallyofthememorizationoftextoutputs.Forfuturework,willinvolvethe
implementationandevaluationoftheproposedwatermarkingschemewiththefocusofimprovingconstraintsinterms
ofrobustnessandcomputation.Furthermore,thereispotentialfortheexpansionofourproposedwatermarkingscheme
beyondatopic-basedapproach,suchasgeneralizingotheraspectsofhowlistsaregeneratedtodetecttextsequences.
REFERENCES
[1] JonathanBailey.2007.Watermarkingvs.Fingerprinting:AWarinTerminology.(2007).https://www.plagiarismtoday.com/2007/10/09/watermarking-
vs-fingerprinting-a-war-in-terminology/
[2] MirandaChrist,SamGunn,andOrZamir.2023.UndetectableWatermarksforLanguageModels.arXivpreprintarXiv:2306.09194(2023). https:
//doi.org/10.48550/arXiv.2306.09194
[3] SaeedDehqan.[n.d.].ExploringTokenGenerationStrategies. https://www.packtpub.com/article-hub/exploring-token-generation-strategies
[4] AndreaGalassi,MarcoLippi,andPaoloTorroni.2019. Attention,please!ACriticalReviewofNeuralAttentionModelsinNaturalLanguage
Processing.CoRRabs/1902.02181(2019).arXiv:1902.02181 http://arxiv.org/abs/1902.02181
[5] ShreyaGoyal,SumanthDoddapaneni,MiteshM.Khapra,andBalaramanRavindran.2023.ASurveyofAdversarialDefencesandRobustnessin
NLP.arXivpreprintarXiv:2203.06414(2023). https://doi.org/10.48550/arXiv.2203.06414
[6] HuggingFaceInc.2024.HuggingFaceModels. https://huggingface.co/models
[7] TianxiJi,ErmanAyday,EmreYilmaz,andPanLi.2022.Robustfingerprintingofgenomicdatabases.Bioinformatics38,Supplement1(062022),
i143–i152. https://doi.org/10.1093/bioinformatics/btac243
[8] TianxiJi,ErmanAyday,EmreYilmaz,andPanLi.2023.TowardsRobustFingerprintingofRelationalDatabasesbyMitigatingCorrelationAttacks.
IEEETransactionsonDependableandSecureComputing20,4(2023),2939–2953. https://doi.org/10.1109/TDSC.2022.3191117
[9] YuzhouJiang,EmreYilmaz,andErmanAyday.2022. RobustFingerprintofLocationTrajectoriesUnderDifferentialPrivacy. arXivpreprint
arXiv:2204.04792(2022). https://doi.org/10.48550/arXiv.2204.04792
[10] DineshKallaandSivarajuKuraku.2023. Advantages,DisadvantagesandRisksassociatedwithChatGPTandAIonCybersecurity. (102023).
https://doi.org/10.6084/m9.jetir.JETIR2310612
[11] JohnKirchenbauer,JonasGeiping,YuxinWen,JonathanKatz,IanMiers,andTomGoldstein.2023.AWatermarkforLargeLanguageModels.arXiv
preprintarXiv:2301.10226(2023). https://doi.org/10.48550/arXiv.2301.10226
[12] ShenLi,LiuyiYao,JinyangGao,LanZhang,andLiYaliang.2024.Double-IWatermark:ProtectingModelCopyrightforLLMFine-tuning.arXiv
preprintarXiv:2402.14883(2024). https://doi.org/10.48550/arXiv.2402.14883
[13] AiweiLiu,LeyiPan,XumingHu,Shu’angLi,LijieWen,IrwinKing,andPhilipS.Yu.2023.AnUnforgeablePubliclyVerifiableWatermarkforLarge
LanguageModels.arXivpreprintarXiv:2307.16230(2023). https://doi.org/10.48550/arXiv.2307.16230
[14] AiweiLiu,LeyiPan,XumingHu,ShiaoMeng,andLijieWen.2024.ASemanticInvariantRobustWatermarkforLargeLanguageModels.arXiv
preprintarXiv:2310.06356(2024). https://doi.org/10.48550/arXiv.2310.06356
[15] MadhurimaNath.2023.Topicmodelingalgorithms.Medium(2023). https://medium.com/@m.nath/topic-modeling-algorithms-b7f97cec6005#:~:
text=The%20most%20established%20go%2Dto,model%20which%20uses%20matrix%20factorization.
[16] OpenAI.2024.Chatgpt:Optimizinglanguagemodelsfordialogue. https://openai.com/blog/chatgpt/
[17] QiPang,ShengyuanHu,WentingZheng,andVirginiaSmith.2024.AttackingLLMWatermarksbyExploitingTheirStrengths.arXivpreprint
arXiv:2402.16187(2024). https://doi.org/10.48550/arXiv.2402.16187
[18] ParthaP.Ray.2023.ChatGPT:Acomprehensivereviewonbackground,applications,keychallenges,bias,ethics,limitationsandfuturescope.
InternetofThingsandCyber-PhysicalSystems3(2023),121–154. https://doi.org/10.1016/j.iotcps.2023.04.00312 Nemeceketal.
[19] MatthiasC.Rillig,MarleneÅgerstrand,MohanBi,KennethA.Gould,andUliSauerland.2023.RisksandBenefitsofLargeLanguageModelsforthe
Environment.EnvironmentalScience&Technology57,9(2023),3464–3466. https://doi.org/10.1021/acs.est.3c01106
[20] VinuS.Sadasivan,AounonKumar,SriramBalasubramanian,WenxiaoWang,andSoheilFelzi.2023.CanAI-GeneratedTextbeReliablyDetected?
arXivpreprintarXiv:2303.11156(2023). https://doi.org/10.48550/arXiv.2303.11156
[21] VitaliiShevchuk.2023.GPT-4ParametersExplained:EverythingYouNeedtoKnow. https://levelup.gitconnected.com/gpt-4-parameters-explained-
everything-you-need-to-know-e210c20576ca
[22] VictoriaSmith,AliS.Shamsabadi,CarolynAshurst,andAdrianWeller.2023.IdentifyingandMitigatingPrivacyRisksStemmingfromLanguage
Models:ASurvey.arXivpreprintarXiv:2310.01424(2023). https://doi.org/10.48550/arXiv.2310.01424
[23] SiddarthSrinivasan.2024. DetectingAIfingerprints:Aguidetowatermarkingandbeyond. https://www.brookings.edu/articles/detecting-ai-
fingerprints-a-guide-to-watermarking-and-beyond/
[24] RuixiangTang,Yu-NengChuang,andXiaHu.2023. TheScienceofDetectingLLM-GeneratedTexts. arXivpreprintarXiv:2303.07205(2023).
https://doi.org/10.48550/arXiv.2303.07205
[25] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN.Gomez,LukaszKaiser,andIlliaPolosukhin.2017.AttentionIs
AllYouNeed.arXivpreprintarXiv:1706.03762(2017). https://doi.org/10.48550/arXiv.1706.03762
[26] WayneX.Zhao,KunZhou,JunyiLi,TianyiTang,XiaoleiWang,YupengHou,YingqianMin,BeichenZhang,JunjieZhang,ZicanDong,YifanDu,
ChenYang,YushuoChen,ZhipengChen,JinhaoJiang,RuiyangRen,YifanLi,XinyuTang,ZikangLiu,PeiyuLiu,Jian-YunNie,andJi-RongWen.
2023.ASurveyofLargeLanguageModels.arXivpreprintarXiv:2303.18223v13(2023). https://doi.org/10.48550/arXiv.2303.18223
[27] XuandongZhao,PrabhanjanAnanth,LeiLi,andYu-XiangWang.2023.ProvableRobustWatermarkingforAI-GeneratedText.arXivpreprint
arXiv:2306.17439(2023). https://doi.org/10.48550/arXiv.2306.17439