Distributed Autonomous Swarm Formation for
Dynamic Network Bridging
Raffaele Galliera ∗†, Thies Möhlenhof ‡, Alessandro Amato ∗†,
Daniel Duran ∗, Kristen Brent Venable ∗†, Niranjan Suri ∗†§
∗Institute for Human & Machine Cognition (IHMC)
†Department of Intelligent Systems & Robotics - The University of West Florida (UWF)
Pensacola, FL, USA
{rgalliera, aamato, dduran, bvenable, nsuri}@ihmc.org
‡Fraunhofer Institute for Communication, Information Processing and Ergonomics (FKIE)
Wachtberg, Germany
thies.moehlenhof@fkie.fraunhofer.de
§US Army Research Laboratory (ARL)
Adelphi, MD, USA
Abstract—Effective operation and seamless cooperation of or even undesirable. Autonomous UAVs able to coordinate
roboticsystemsareafundamentalcomponentofnext-generation on distributed strategies might provide a practical solution in
technologies and applications. In contexts such as disaster
ensuring an ad-hoc connection between these two nodes.
response, swarm operations require coordinated behavior and
mobility control to be handled in a distributed manner, with
This paper introduces a novel Decentralized Partially
the quality of the agents’ actions heavily relying on the
communication between them and the underlying network. In ObservableMarkovDecisionProcess(Dec-POMDP)settingto
this paper, we formulate the problem of dynamic network address the challenge of dynamically forming communication
bridging in a novel Decentralized Partially Observable Markov links between moving targets using a swarm of UAVs, which
Decision Process (Dec-POMDP), where a swarm of agents
wecalldynamicnetworkbridging.Inthisscenario,eachUAV,
cooperates to form a link between two distant moving targets.
equipped with limited sensing capabilities, must cooperatively Furthermore,weproposeaMulti-AgentReinforcementLearning
(MARL)approachfortheproblembasedonGraphConvolutional navigate and position itself within the environment to ensure
Reinforcement Learning (DGN) which naturally applies to continuousconnectivitybetweenthemovingtargets.Moreover,
the networked, distributed nature of the task. The proposed weproposeadecentralizedMulti-AgentReinforcementLearn-
method is evaluated in a simulated environment and compared
ing (MARL) approach exploiting the networked nature of the
to a centralized heuristic baseline showing promising results.
task to enable effective cooperation. By employing Graph
Moreover, a further step in the direction of sim-to-real transfer
is presented, by additionally evaluating the proposed approach ConvolutionalReinforcementLearning(DGN)[1]withGraph
in a near Live Virtual Constructive (LVC) UAV framework. AttentionNetworks(GATs)[2]andLongShort-TermMemory
Index Terms—Dynamic Communication Networks, Multi- (LSTM) [3], our agents utilize spatio-temporal information
Agent Reinforcement Learning, UAV Swarms, Sim-to-Real.
while communicating learned latent representations with
neighboring agents and target entities to ground their actions.
I. INTRODUCTION
We compare our method with a centralized heuristic, showing
In the evolving landscape of wireless communication, how our MARL approach drives a competitive distributed
deploying autonomous agents, particularly Unmanned Aerial solution. Finally, we present the integration of our Live
Vehicles(UAVs),presentsuniqueopportunitiesforestablishing Virtual Constructive (LVC) UAV framework with our MARL
dynamiccommunicationnetworks.Scenarioswheretraditional environment,enablingthedeploymentofourlearnedstrategies
infrastructure is unavailable or impractical, such as disaster insimulated-only,real-only,oramixtureofrealandsimulated
response or remote area connectivity, are just a few direct agents operating in the same environment.
application examples. The mobility and flexibility of UAVs
allow for creating an adaptive and resilient network topology, The remainder of this paper is structured as follows: in
responding to the ever-changing environmental conditions. Section II we discuss related work; Section III provides a
Consider, for example, two mobile, independent nodes detaileddescriptionofourDec-POMDPformulation,followed
engaged in some task. To accomplish their duties, it might be by the description of our MARL approach in Section IV. We
essential for these two nodes to exchange information, even continuewiththeintegrationofourMARLframeworkwiththe
for just a short period. However, the infrastructure needed to LVC in section V. Section VI outlines the experimental setup,
ensure their communication link might be unavailable, unreli- including the simulation environment and training procedures.
able, or absent, and, depending on the scenario, centralized Finally, we conclude with a discussion of future research
solutions governing the UAVs strategies could be unfeasible directions and potential extensions of our approach.
4202
rpA
2
]AM.sc[
1v75510.4042:viXraII. RELATEDWORK where distance(u,v,t) is a distance metric that measures the
spatial or logical distance between nodes u and v at time-step
The development of cooperative architectures involving
t. Hence, for every node v ∈ V, the set of its neighbors at
multiple UAVs has been a significant area of research,
time t is defined as N (t)={u∈V|(v,u)∈E(t)}.
particularly in the context of vehicular [4] and tactical [5] v
networks. These studies have primarily focused on addressing
A. A MARL environment for Dynamic Network Bridging
communication challenges and distributed decision-making
strategies within multi-UAV systems. The challenge of opti- For multi-agent systems, the Reinfocement Learning (RL)
mized UAV placement is explored in [6], where the problem paradigm extends to MARL [11], where multiple entities,
is reduced to a packing problem. The authors designed an potentially learners and non-learners, act within a shared
iterativealgorithmtooptimizeareacoverage,networkcapacity, environment. In this context the generalization of Partially
and routing constraints while minimizing overlapping areas. Observable Markov Decision Processs (POMDPs) leads to
As an alternative to combinatorial optimization approaches, Dec-POMDP, characterized by the tuple:
recentresearch hasshiftedtowardsmodel-free,learning-based
⟨I,S,Ai ,P,R,Oi ,γ⟩ (1)
strategiesforUAV-cellsmanagement.Forinstance,Hammami i∈I i∈I
et al. [7] introduce a semi-centralized, multi-agent framework. Here,I representsthesetofagents,S denotesthestatespace,
In this model, a central entity gathers joint actions, while the Ai standsfortheactionspaceforeachagent,P isthejoint
i∈I
environmental state encompasses network parameters, UAV probability distribution governing the environment dynamics
statuses, battery levels, and bandwidth capacities. giventhecurrentstateandjointactions,Rdenotesthereward
Another innovative approach is presented in [8], which function, and Oi represents the set of observations for
i∈I
proposes a MARL approach for UAV-assisted Roadside Units each agent. Such game-theoretic settings are used to model
(RSUs) in providing Vehicular Ad-hoc Networks (VANETs) fully cooperative tasks where all agents have the same reward
along highways. In their problem formulation, UAVs are function and share a common reward.
placed in a straight line at a fixed altitude and can move In this work, we construct a Dec-POMDP formulation
backward and forward with the primary object of maximizing for the task of connecting two moving targets relying on a
the number of sub-segments that satisfy a delay constraint. swarm of N agents. Given the network represented by graph
The authors employ independent Deep Q-Learning (DQN) [9] G =(V,E ) at time t , and node T ,T ∈V, we define the
0 0 0 1 2
with shared policies parameterized by small neural networks Dec-POMDP associated to the optimized connection of T
1
to periodically update the UAVs positions. Similar to our and T and the moving target update function U , with the
2 T
approach, the observation of each agent is augmented with tuple:
information from its immediate neighbors. However, instead ⟨I,T,S,Ai ,U ,P,R,Oi ,γ⟩ (2)
of concatenating the neighboring agents’ observations, we i∈I T i∈I
propose a more flexible approach employing GATs and their The tuple consists of the following enumerated elements:
inherent message-passing mechanism [10]. 1) Agent I and Target T sets: I represents the set of
Furthermore, we utilize a more cooperative algorithm such learning agents, and T denotes the set of moving targets,
as DGN, which aims to decompose the agents’ value function specifically T and T .
1 2
into agent-wise value functions. 2) Observation Oi and State set S: Each agent i in
i∈I
I has a local observation Oi, which includes the structure
III. METHOD
of its neighborhood as well as the features representing its
In this section, we present a Dec-POMDP formulation to neighbors,asdescribedinTableI.Thesefeaturesincludenode
modelthecooperativetaskofbridgingtheconnectionbetween ID, the current coordinates of that node, the action taken, and
two moving targets relying on a swarm of N agents in a 2D the coordinates of the targets that the node aims to connect.
plane, while perceiving only their local one-hop observation. The global state S encompasses the entire graph structure of
However, it is worth mentioning that our approach can be the network, along with the features describing all agents and
expanded to 3D scenarios by augmenting the action space of targets. However, these are only partially observable by each
the agents and refining the attributes of the nodes accordingly. agent due to their limited observation range.
Let us consider a scenario where each node represents a 3) Moving Target Update Function U : The moving target
T
Mobile Ad-Hoc Networking radio (MANET). Nodes have a update function U governs the dynamics of T and T . It
T 1 2
specific communication range, representing a certain distance determines their positions at each time step, following a
or proximity within which information can be sensed by movement pattern which can be deterministic or stochastic.
other nodes. The underlying network can be represented as a 4) Action Space Ai : At every time-step, agents decide
i∈I
dynamic graph G(t)=(V,E(t)), where each node represents in which direction along the x and y axis to move or if
a mobile node and an edge between two nodes at time t they should maintain their current position. We encode this
represents the two corresponding nodes being within each action space in two dimensions, with each dimension having
other’scommunicationrangeratthattime.Moreformally,the 3 options corresponding to going forward, backward, or hold
set of edges E(t)={(u,v)|u,v ∈V,distance(u,v,t)≤r}, along a certain axis.5) TransitionFunctionP: ThetransitionfunctionP defines A. The Role of Message Passing and Latent Representations
the dynamics of the environment. It specifies the probability InDGN,dot-productattentionandgraphconvolutionplaya
distribution over the next state s′ given the current state s and crucialroleinintegratingfeaturevectorsassociatedwithnodes
the joint actions a taken by the agents. within a local region around a certain node i, by generating a
In our scenario, the transition of states primarily depends latent feature vector h comprising node i and its neighboring
i
on the agents’ movements and the dynamic changes in the nodes.Byaddingmoreconvolutionallayers,thereceptivefield
network topology due to the movement of the agents and ofanagentexpandsprogressively,leadingtotheaccumulation
targets. Specifically, when moving within the 2D plane, they of more information. Consequently, the scope of cooperation
alter their positions and potentially change the network’s also broadens, enabling agents to collaborate more effectively.
connectivity graph G(t). These movements lead to a new set Specifically, with one convolutional layer, node i aggregates
of edges E(t+1) in the graph, to updated nodes’ features, the features of the nodes in its one-hop neighborhood. When
and, thus, to the transition to a new state s′. two layers are stacked, node i receives the output of the
6) Reward Function R: The reward function in our Dec- first convolutional layer of nodes in its one-hop neighborhood,
POMDP framework is designed to direct agents toward which,inturn,embedsinformationfromnodestwohopsaway
forming a communication network between moving targets. from i. However, irrespective of the number of convolutional
It consists of the following components: layers, node i only communicates with its one-hop neighbors,
Base Connectivity Reward: The base reward, R base, is making DGN practical in real-world networking scenarios.
given by the ratio of the number of nodes in the largest Inthiswork,weachievecooperationbetweentheagentsby
connected component and the total number of nodes: enabling them to share their latent representations within their
|C (s)| immediate neighborhood, conditioning their actions on such
R base(s)= m |a Vx
|
, (3) information. In addition, we also integrate target entities in
the sharing process. Targets, despite not being learning agents,
where |C (s)| represents the size of the largest connected
max will also compute the latent representation of their one-hop
component in state s, and |V| is the total number of entities.
neighborhood graph structure, in the same way agents do.
Centroid Distance Penalty: The centroid distance penalty, To achieve this behavior, targets are deployed with the same
P cent, is computed as the Euclidean distance between the Graph Neural Network (GNN) architecture adopted by the
centroid of the agents’ and targets’ positions. agents, except for adopting only the first encoding module,
Target Path Bonus: A bonus, B path =100, is awarded if whichproducestheirintermediatelatentrepresentation(Figure
a path exists between the two targets. 2). Finally, if the neighborhood of a target entity T includes
i
Overall Reward: The overall reward combines the base thepresenceofoneormoreagents,thesewillbeabletogather
reward, centroid distance penalty, and the target path bonus: such representation and condition their actions accordingly.
B. Integration of LSTM and Observation Stacking
(cid:40)
B (s) if ∃path(T ,T );
R(s,a)= path 1 2 (4) To handle the temporal dynamics and partial observability
R (s)−P (s), otherwise.
base cent in our environment, we integrate LSTM networks employing
graph observation stacking during training [3]. Observation
This reward ensures that agents are motivated to form a
stacking provides a richer representation of the environment
stable and efficient communication network while positioning
by aggregating observations over multiple time steps, giving
themselves effectively relative to the moving targets.
the LSTM the temporal context needed for effective learning.
IV. LEARNINGAPPROACH This combination allows our model to maintain a memory of
In this section, we describe our learning approach and past neighborhood observations, aiding in decision-making in
the network architecture used to parameterize the agents’ a partially observable setting.
action-value function. Our approach leverages DGN [1] in C. Summary of the Neural Network Architecture
modeling relational data while harmoniously integrating with
Our approach adopts the network architecture presented in
thedecentralizedandnetworkednatureofthetask.Trainingis
Figure 2, which comprises several components:
performed in a Centralized Training Decentralized Execution
(CTDE) [9] fashion, with the agents optimizing the same • Two Multi-Headed Graph Attention Layers: Utilized
for encoding the relational data among agents. These
action-value function parameterization.
layers capture the spatial and relational dependencies
in the network [2]. The first layer provides the latent
NodeType ID Coord Action T1 Coord T2 Coord representation to be shared with neighboring agents.
T 4 (x3,y3) 0 (x3,y3) (x3,y3) • One LSTM Layer: Used for capturing temporal depen-
A 1 (x1,y1) 2 (x3,y3) (x3,y3) dencies and handling partial observability [3].
A 2 (x0,y0) 7 (x3,y3) (x3,y3) • Dueling Action Decoder: Incorporates separate streams
TableI:Exampleofnodes’featuresinthegraphrepresentingsomeagent’s for estimating state values and advantages, to facilitate
neighborhood.TheActionfeatureforTargetnodes(T)isalwayssetto0. the estimation of the actions values [12].(a) (b)
Figure1:AscreencapturefromourLVCrunningourlearnedMARLstrategies(a)andthecorrespondingrealUAV(b).
The LVC framework provides access to real and physically-
Input accurate simulated UAV agents with agent-to-agent network
(Node i 1- hop
observation) communications. Figure 1 presents a screen capture of
simulated UAVs and a picture of their real counterpart. By
MLP
MLP Output Multi- Headed supporting simulated-only, real-only, or a mix of real and
Graph Attention
simulated UAVs operating in the same environment, our
LVC enables a multitude of opportunities for prototyping,
evaluating, and deploying learned policies.
Each agent in the LVC uses an onboard flight controller
Neighborhood to maintain flight stability and an embedded Linux machine
Graph
Node i to deploy higher-level automation tasks such as the learned
Intermediate Latent Structure
Representation MARL behaviors. In the simulated environment, agents are
deployed on individual Virtual Machines (VMs) that emulate
Gather Neighbors'
therealembeddedcomputer.Networkcommunicationbetween
Latent Representations
agents is achieved using the low-latency User Datagram
2nd Multi-H eaded Protocol (UDP) over a dedicated WiFi network. Network
Graph Attention
emulators such as the Extendable Mobile Ad-hoc Network
Emulator (EMANE) are also supported, enabling emulation
Node i Final Latent
Representation of realistic network conditions such as those encountered in
disaster response situations. Once policies are trained within
Concatenate our MARL environment, we rigorously test them in the LVC
the 3 representations
by interfacing the two components with gRPC and having the
Long Short-T erm Memory framework communicate the actions of each agent. However,
the LVC also supports a deployment mode detached from our
MARL framework, where policies are deployed in each real
Advantage Value
Network Network or simulated UAVs with agent-agent network communication
enabledbytheLVC.Aircraftlocalizationinrealenvironments
is achieved through a 12-camera Motion Capture (MoCap)
Q- Values system. Furthermore, the LVC can adjust the localization
accuracy and acquisition rates on the fly to simulate various
Figure2:ASummaryofournetworkarchitecture.
sensors and conditions in a controlled environment.
In this work, the LVC plays a fundamental role in the
testing phase, assessing the policies’ real-world performance,
Our model’s architecture, with its GAT layer and LSTM
and evaluating their ability to navigate and interact effectively
integration, is tailored to address the challenges presented by
within dynamic real-world environments.
dynamic network bridging with a cooperative perspective.
VI. EXPERIMENTS
V. INTERACTIONWITHTHELIVEVIRTUAL
In this section, we will introduce our experimental setup,
CONSTRUCTIVEUAVFRAMEWORK
describing the steps involved in our training and evaluation
To enable a feasible and practical transition of learned process. Finally, we will present a comparison between our
MARL behaviors to real-world applications, we have been approach and a centralized heuristic, designed to reposition
using a custom LVC UAV environment developed in-house. the agents according to the line connecting the two targets.T0
T0 0.8 1.0 T1
T1 0.8
T0 0.6 0.8
Y Y A1 0.6Y
A A A2 1 0
T1
A1A2A0 000 ... 024 A
A
A2
1
0
T1 A0A2A1T0 00 .. 46 A A A2 1
0
T0 A0A2 00 .. 24
T1 T0
0.8 1.0 0.8
0.6 T1 0.8 0.6
0 20 40 0.4 X 0 20 40 0.40.6 X 0 20 40 0.4 X
60 0.2 60 0.2 60 0.2
Steps 80 100 Steps 80 100 Steps 80 100
Figure3:Examplesoftrajectoriesproducedbyagentsandtargetspositionsduring3evaluationepisodes.GreenandRedsegments(T1/T2)representthe
intervalsof(time)stepswheretheagents(A1,A2,A3)wereabletoformalinkbetweenT1andT2.
Algorithm 1 Centralized Heuristic (Baseline) canbedeterminedbyotherpolicies.Inthiswork,weleveraged
Require: I,T ,T ,G(t), step size for movement. internal mechanisms implemented in the LVC.
1 2
Ensure: Ideal Actions for each agent i∈I. Our training regime involved 33000 episodes across ran-
1: E ← []. domly generated scenarios, with every episode involving 100
2: Determine the slope m and y-intercept b of the line decisionsbyeachofthethreeagents.Duringboththetraining
connecting T and T . and testing phases, agents were consistently deployed at
1 2
3: Evenly space |I| x-values between x 1 and x 2. predetermined positions, resembling their base station. Initial
4: Calculate the corresponding y-values using y =mx+b positions were located at (0.1, 0.42), (0.1, 0.52), and (0.1,
to form the set of potential endpoints P. 0.62) for agents 1, 2, and 3 respectively.
5: for each agent i∈I do During training, two targets were placed randomly on the
6: Find the closest point p∈P. map,withtheconstraintthatthedistancebetweenthemranged
7: Add p to E and remove p from P. between 0.5 and 0.7. This constraint was enforced to avoid
8: end for overly simplistic or impossible scenarios, ensuring a balanced
9: for each agent i, endpoint e i in zip(I,E) do difficulty level for the agents to form a communication link.
10: Determinetherelativepositionandorientationtowards The movement of these targets was governed by a seeded
its assigned endpoint randomnumbergenerator,whichwasinstrumentalinselecting
11: Calculatethedirectionalangleαtowardstheendpoint. their next move. This random yet controlled movement
12: if d(i,e i)≤0.1 then ensuredthatthetargetsmaintainedtherequireddistanceapart,
13: Maintain current position. presenting the agents with realistic and challenging scenarios.
14: else The learned multi-agent strategies were evaluated across
15: Assign discretized action based on α. 100 generated scenarios. These scenarios were controlled
16: end if with seeded randomness for the placement and movement
17: end for of the targets, ensuring the reproducibility of the experiments.
Additionally, for evaluation purposes, our methods were
compared against an ideal centralized heuristic, designed to
A. Experimental Setup coordinate the agents optimally (Algorithm 1).
Inourexperiments,threeagentsandtwotargetsareinitially
B. Results
positioned on a normalized map with axes ranging from 0
to 1 to simulate the operational environment for the agents Figure 3 illustrates the trajectories of agents and targets
and targets. Each agent and target within this environment throughout three different episodes, each lasting 100 time-
has a communication range set to 0.25, ensuring a functional steps. The learned strategies enable the agents to form a
scope for link formation. The step size for both agents and communication link between T and T for most of the
1 2
targets was set to 0.05, allowing them to move incrementally episode duration, despite the unpredictable movements of the
towardachosendirection.However,thenextpositionentailed targets.Figure4offersaperformancecomparisonbetweenour
by the agents’ action can be interpreted as the next waypoint DGN approach and the centralized heuristic in terms of "time-
to which the agent will be directed. Such consideration steps coverage," which is defined as the percentage of time-
allows our approach to abstract various elements, such as steps during which a path between T and T is successfully
1 2
the agents’ speed and route to the chosen waypoint, which maintained. The DGN agents managed to bridge the twoVIII. CONCLUSION
In this work, we proposed a Dec-POMDP formulation
80
for dynamic network bridging and we showed how effec-
tive decentralized multi-agent strategies involving proactive
communication can be learned for this task. Furthermore,
60
we have designed a MARL approach to solve the problem
in a distributed manner and we compared our results to a
40 moretraditionalcentralizedapproach.Theexperimentalresults
showed promising results in this direction, with our agents
being able to form a link between the targets in every training
20 episode, for the most time of the episodes’ duration.
REFERENCES
0 DGN Centralized Heuristic [1] J. Jiang, C. Dun, T. Huang, and Z. Lu, “Graph convolutional
reinforcement learning,” in International Conference on Learning
Figure 4: Comparison in terms of average time-steps covered during the
Representations, 2020. [Online]. Available: https://openreview.net/
evaluationphaseofouragentandthecentralizedheuristic.
forum?id=HkxdQkSYDB
[2] P. Velicˇkovic´, G. Cucurull, A. Casanova, A. Romero, P. Liò,
and Y. Bengio, “Graph attention networks,” in International
targets for 63.88% of the episode duration, while the optimal Conference on Learning Representations, 2018. [Online]. Available:
https://openreview.net/forum?id=rJXMpikCZ
heuristic achieved this task for 83.19% of the time-steps.
[3] M.J.HausknechtandP.Stone,“Deeprecurrentq-learningforpartially
Additionally, the average total return was 6494.13±941.50 observable mdps,” in 2015 AAAI Fall Symposia, Arlington, Virginia,
for the DGN method and 8440.28±490.58 for the heuristic. USA,November12-14,2015. AAAIPress,2015,pp.29–37.
[4] Y.Zhou,N.Cheng,N.Lu,andX.S.Shen,“Multi-uav-aidednetworks:
We note that given the limited step size and the initial starting
Aerial-ground cooperative vehicular networking architecture,” IEEE
positionoftheagents,significantlydistantfromthetwotargets, VehicularTechnologyMagazine,vol.10,pp.36–44,122015.[Online].
itisnotpossibletoachievenetworkbridgingduringtheentire Available:http://ieeexplore.ieee.org/document/7317860/
[5] M. Tortonesi, C. Stefanelli, E. Benvegnu, K. Ford, N. Suri, and
episode. Despite the performance of our agents being lower
M. Linderman, “Multiple-uav coordination and communications in
than the one achieved by the centralized heuristic, our initial tacticaledgenetworks,”IEEECommunicationsMagazine,vol.50,pp.
results underscore the potential of our DGN approach in 48–55,102012.
[6] S.J.Park,H.Kim,K.Kim,andH.Kim,“Droneformationalgorithm
dynamicandunpredictableenvironments,openingavenuesfor
on3dspaceforadrone-basednetworkinfrastructure,”2016IEEE27th
further optimization and application in real-world networks. AnnualInternationalSymposiumonPersonal,Indoor,andMobileRadio
Communications(PIMRC),pp.1–6,2016.
VII. FUTUREWORK [7] S.E.Hammami,H.Afifi,H.Moungla,andA.Kamel,“Drone-assisted
cellularnetworks:Amulti-agentreinforcementlearningapproach,”IEEE
To enrich the practical significance of our MARL approach, InternationalConferenceonCommunications,vol.2019-May,52019.
we plan to incorporate low-level networking elements in our [8] B. Jiang, S. N. Givigi, and J. A. Delamer, “A marl approach for
optimizingpositionsofvanetaerialbase-stationsonasparsehighway,”
learning environment. To this end, we will host our learning
IEEEAccess,vol.9,pp.133989–134004,2021.
agents in container-based network simulators/emulators [13] [9] S. V. Albrecht, F. Christianos, and L. Schäfer, Multi-Agent
and include ray tracing propagation models to better mirror ReinforcementLearning:FoundationsandModernApproaches. MIT
Press,2023.[Online].Available:https://www.marl-book.com
realisticscenarios[14].Thiswillallowustodirectlyoptimize
[10] F.Scarselli,M.Gori,A.C.Tsoi,M.Hagenbuchner,andG.Monfardini,
networking properties related to the connectivity of the “The graph neural network model,” IEEE Transactions on Neural
underlying network. We will also enable our agents to take Networks,vol.20,no.1,pp.61–80,2009.
[11] L.Bus¸oniu,R.Babuška,andB.DeSchutter,Multi-agentReinforcement
more refined actions by including continuous action spaces
Learning:AnOverview. Berlin,Heidelberg:SpringerBerlinHeidelberg,
and investigating learning frameworks based on entropy- 2010,pp.183–221.
regularizedstochasticpolicies.Thefirstwillallowtheagentsto [12] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot, and
N.DeFreitas,“Duelingnetworkarchitecturesfordeepreinforcement
choose any point within the circle/sphere bounded by the step
learning,” in Proceedings of the 33rd International Conference on
size as their next waypoint, while the second will encourage International Conference on Machine Learning - Volume 48, ser.
thelearningofmulti-modalstrategies.Additionally,integrating ICML’16. JMLR.org,2016,p.1995–2003.
[13] R. Galliera, M. Zaccarini, A. Morelli, R. Fronteddu, F. Poltronieri,
our learning framework with established control systems [15]
N. Suri, and M. Tortonesi, “Learning to sail dynamic networks:
willbeexploredtosupportsafetyandreliabilityforreal-world Themarlinreinforcementlearningframeworkforcongestioncontrol
applications, ensuring our advancements are both innovative in tactical environments,” in MILCOM 2023 - 2023 IEEE Military
CommunicationsConference(MILCOM),2023,pp.424–429.
and grounded for practical deployment.
[14] A.Amato,R.Fronteddu,andN.Suri,“Dynamicallycreatingtactical
Finally, we conjecture that learned multi-agent strategies networkemulationscenariosusingunityandemane,”inMILCOM2023
will increase their benefit when introduced to more complex -2023IEEEMilitaryCommunicationsConference(MILCOM),2023,
pp.201–206.
networkbridgingscenariospresentingnumeroustargetentities.
[15] A.Chen,K.Mitsopoulos,andR.Romagnoli,“Reinforcementlearning-
In these settings, employing centralized/distributed heuristic basedoptimalcontrolandsoftwarerejuvenationforsafeandefficient
functions will present more challenges and limitations than uavnavigation,”in202362ndIEEEConferenceonDecisionandControl
(CDC),2023,pp.7527–7532.
in instances where only two target nodes are involved.
)%(
egarevoC
spets-emiT