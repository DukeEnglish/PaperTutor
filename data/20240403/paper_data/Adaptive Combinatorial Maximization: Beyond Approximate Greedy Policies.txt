ProceedingsofMachineLearningResearchvol237:1–20,2024 35thInternationalConferenceonAlgorithmicLearningTheory
Adaptive Combinatorial Maximization:
Beyond Approximate Greedy Policies
ShlomiWeitzman SHLOMIWEITZMAN@GMAIL.COM
DepartmentofComputerScience,Ben-GurionUniversityoftheNegev
SivanSabato SABATOS@MCMASTER.CA
DepartmentofComputingandSoftware,McMasterUniversity
CanadaCIFARAIChair,VectorInstitute
DepartmentofComputerScience,Ben-GurionUniversityoftheNegev
Editors:ClaireVernadeandDanielHsu
Abstract
Westudyadaptivecombinatorialmaximization,whichisacorechallengeinmachinelearning,with
applicationsinactivelearningaswellasmanyotherdomains. WestudytheBayesiansetting,and
considertheobjectivesofmaximizationunderacardinalityconstraintandminimumcostcoverage.
We providenew comprehensiveapproximationguaranteesthatsubsumepreviousresults, as well
asconsiderablystrengthenthem. Ourapproximationguaranteessimultaneouslysupportthemaxi-
malgainratioaswellasnear-submodularutilityfunctions,andincludebothmaximizationundera
cardinalityconstraintandaminimumcostcoverageguarantee.Inaddition,weprovidedanapprox-
imationguaranteeforamodifiedprior,whichiscrucialforobtainingactivelearningguaranteesthat
donotdependonthesmallestprobabilityintheprior. Moreover,wediscoveranewparameterof
adaptiveselectionpolicies,whichwetermthemaximalgainratio. Weshowthatthisparameteris
strictlylessrestrictivethanthegreedyapproximationparameterthathasbeenusedinpreviousap-
proximationguarantees,andshowthatitcanbeusedtoprovidestrongerapproximationguarantees
than previousresults. In particular, we show that the maximalgain ratio is never larger than the
greedyapproximationfactor ofa policy,and thatitcan be considerablysmaller. Thisprovidesa
newinsightintothepropertiesthatmakeapolicyusefulforadaptivecombinatorialmaximization.
Keywords:Combinatorialmaximization,adaptivesubmodularity,approximategreedypolicies
1. Introduction
Adaptive combinatorial maximization is a core challenge in machine learning, spanning a wide
rangeofapplications,includingthefundamentalchallengesofactivelearning(GolovinandKrause,
2011) and adaptive experiment design (Doppa, 2021), as well as many other applications, such as
influence maximization in social networks (SeemanandSinger, 2013; Tongetal., 2016), movie
recommendations (Mitrovicetal.,2019)andspectroscopy (Hino,2020).
In this algorithmic problem, there are elements with hidden states. Elements are selected se-
quentially. Whenever anelement isselected, itsstate isobserved. Pastobservations canbeused to
makethenextselection decision. Theobtained utility fromaspecific selection process depends on
the selected elements and their true states. For instance, consider a problem of selecting locations
for placing radio towers (Asadpouretal., 2008; GolovinandKrause, 2011), where the goal is to
ensuresufficientsignalstrength inthedesignated area. Thestatesinthiscaseareparameters ofthe
transmission thatcanonly bemeasured afteraradio towerisplaced. Thenextradiotowerlocation
canbeselected basedontheprevious measurements.
©2024S.Weitzman&S.Sabato.
4202
rpA
2
]GL.sc[
1v03910.4042:viXraWEITZMAN SABATO
A particularly important application of adaptive combinatorial application in machine learning
is active learning (McCallumetal., 1998), in which the elements are examples, and the state of an
elementisitstruelabel. Theactivelearning algorithm selects whichelementtohavelabeled, soas
torevealitstruelabel,andtheobjectiveistoidentifythetrueclassifierfortheentiresetofexamples
usingasmallnumberoflabels.
We study the Bayesian setting, in which there is a known prior over the possible states of the
elements,andtheperformanceofthealgorithmismeasuredwithrespecttotheexpectationoverthis
prior. The goal of the algorithm is either to obtain a high expected utility using a limited number
ofelementselections, alsotermed“maximizationunderacardinality constraint” oralternatively, to
obtainthemaximalpossibleutilityvalueusingasmallexpectednumberofelementselections. The
latteristypicallyreferredtoas“minimumcostcoverage”. Thisobjectiveisofparticularimportance
foractivelearning,sinceinthisapplication,thegoalistoreachaspecifictargetoffindingthecorrect
classifier, which can be mapped to a maximal value of the utility function. GolovinandKrause
(2011)werethefirsttointroduce theadaptiveBayesianformulation.
Calculating an optimal selection policy for either of the objectives mentioned above iscompu-
tationally hardinthegeneral case (see, e.g. Nemhauseretal.,1978;Wolsey,1982). Thus, weseek
insteadtoprovideapproximation guarantees withrespecttoanoptimalpolicy. GolovinandKrause
(2011) defined a class of utility functions called adaptive submodular, which generalizes the non-
adaptive notion of submodular set functions (Edmonds, 1970). They showed that if the functions
arealsoadaptive monotone,thenanapproximately greedypolicyobtainsabounded approximation
factor for the objective of maximization under a cardinality constraint. A greedy policy selects, in
each round, the element that will obtain the largest increase in utility in expectation. An approxi-
mategreedypolicymayselectanelementthatismaximaluptoagivenfactor. GolovinandKrause
(2017) provided an approximation guarantee for the minimum cost coverage objective. A smaller
approximation factorwasprovedbyEsfandiari etal.(2021).
Theguaranteesmentionedaboverequirethattheutilityfunctionisadaptivesubmodular. FujiiandSakaue
(2019)studied arelaxed version, using theadaptive submodularity ratio, aproperty thatquantifies
thecloseness ofautilityfunction tobeing submodular. Theyprovided anapproximation guarantee
for maximization under a cardinality constraint for greedy policies, that is parameterized by the
adaptivesubmodularity ratiooftheutilityfunction.
Inthiswork,weprovideanewperspectiveontheproblemofadaptivecombinatorialmaximiza-
tion,byprovidingcomprehensiveapproximationguaranteesthatnotonlysubsumepreviousresults,
butalsoconsiderably strengthen them. Wediscoveranewparameterofadaptiveselection policies,
which we term the maximal gain ratio. Weshow that this parameter is strictly less restrictive than
the greedy approximation parameter that has been used inprevious approximation guarantees, and
show that it can be used to provide stronger approximation guarantees than previous results. In
particular, weshowthatthemaximalgainratioisneverlargerthanthegreedyapproximation factor
ofapolicy,andthatitcanbeconsiderably smaller.
The maximal gain ratio is the maximal ratio between the expected marginal gain of remaining
elementsatterminationandtheexpectedmarginalgainofselectedelements. Inthisway,itcantake
into account also selections that would seem out of order for a greedy policy, but that accomplish
a similar goal. Moreover, we show that even for greedy policies, the value of the maximal gain
ratio can be arbitrarily small. Thus, this provides a more refined understanding of what makes an
adaptiveselection policysuccessful. Ourcontributions arethefollowing:
• Definingthemaximalgainratio,anewpolicyparameter;
2ADAPTIVE COMBINATORIAL MAXIMIZATION: BEYOND APPROXIMATE GREEDY POLICIES
• Showingthatthisparameterisstrictlylessrestrictive thanthegreedyapproximation parame-
ter;
• An approximation guarantee for utility maximization under a cardinality constraint that is
parameterized by the maximal gain ratio of a policy as well as the adaptive submodularity
ratiooftheutilityfunction;
• An approximation guarantee for the minimum cost coverage objective parameterized by the
sameproperties;
• Aversionoftheminimumcostcoverageguarantee thatallowsusingtheproblemparameters
with a modified prior first suggested by GolovinandKrause (2011), leading to an improved
approximation ratioforBayesianactivelearningunderpriorswithsmallprobabilities.
We summarize the main difference between our work and previous works mentioned above in Ta-
ble1. Ourworkisthefirsttosupport,atthesametime,policiesthatarenotgreedy,utilityfunctions
that are not adaptive submodular, and the minimum cost coverage objective, as well as guarantees
forthemodifiedprior.
Approximate Nearlyadaptive Mincost Supportfor Beyond
greedypolicies submodularutility coverage modifiedprior approximategreedy
GK11 ✓ ✗ (✓) (✓) ✗
FS19 ✗ ✓ ✗ ✗ ✗
EKM21 ✗ ✗ ✓ ✗ ✗
ThisWork ✓ ✓ ✓ ✓ ✓
Table1: Comparison to previous works. GK11: GolovinandKrause (2011), FS19:
FujiiandSakaue (2019), EKM21: Esfandiarietal. (2021). (✓) indicates a looser guar-
antee.
Paperstructure. InSection2,wecomplementthediscussionaboveonrelatedwork. Definitions
and necessary background are provided in Section 3. We provide a summary of our main results
in Section 4. The maximal gain ratio is presented and analyzed in Section 5. The proofs of the
approximation guarantees for the true prior based on the maximal gain ratio are provided in Ap-
pendix A.Themodifiedprior anditsrelevance toactive learning arediscussed inSection 6,where
the proof of our guarantee for this prior isalso provided, with some parts deferred to Appendix B.
WesummarizeinSection7.
2. RelatedWork
The concept of adaptive submodular functions was first introduced in GolovinandKrause (2011).
They showed that this class of functions allows deriving guarantees for several settings, including
adaptive submodular maximization with respect to a Bayesian prior. The latter setting was further
studiedinFujiiandSakaue(2019)andEsfandiari etal.(2021),whoprovidedadditionalguarantees,
as discussed above. FujiiandSakaue (2019) introduced the adaptive submodularity ratio, which
3WEITZMAN SABATO
generalized thesubmodularity ratio proposed byDasandKempe(2011)fornon-adaptive setfunc-
tionstotheadaptivesetting, andprovided guarantees parameterized bythisquantity. Thenotionof
curvature, first introduced in ConfortiandCornue´jols (1984) for set functions in the non-adaptive
combinatorial maximization setting, allows further improving the guarantees and algorithms for
submodular maximization under a cardinality constraint (Vondrak, 2010; BalkanskiandSinger,
2018).
Adaptive submodular maximization hasbeen studied inother settings aswell, such asaworst-
casenon-Bayesian settingwithpointwisesubmodularity (GuilloryandBilmes,2010)withapplica-
tionstoactivelearning(Cuongetal.,2014),outcomedependentcosts(Sabato,2018),adaptivemax-
imization of non-monotone submodular functions (Gotovosetal., 2015; Amanatidisetal., 2020;
Tang, 2021), streaming adaptive maximization (TangandYuan, 2023), and robust adaptive sub-
modular maximization Tang (2022). It has been applied to many settings, including batch mode
active learning (ChenandKrause, 2013), bandit algorithms (Gabillonetal., 2013), and reinforce-
mentlearning(WuandTseng,2022).
3. Preliminaries
In this section, we give necessary preliminaries. Definitions and notation are provided in Sec-
tion 3.1. Previously studied properties ofutility functions andpolicies areprovided inSection 3.2.
Guarantees fromprevious worksaregiveninSection3.3.
3.1. DefinitionsandNotation
Let V be a finite ground set of elements. Let Y be a finite set of the possible states of each of the
elementsinV. Arealization isamapφ: V → Y thatassociates eachground elementwithastate.
A partial realization ψ : V ⇀ Y is a map from a subset of V to Y. Its domain is denoted by
dom(ψ). We denote the set of all realizations by Φ and the set of all partial realizations by Ψ .
V V
For any realization φ ∈ Φ and partial realization ψ ∈ Ψ , we write φ ∼ ψ to indicate that for
V V
every x ∈ dom(ψ), ψ(x) = φ(x). For any two partial realizations ψ and ψ′, we write ψ ⊆ ψ′ if
dom(ψ) ⊆ dom(ψ′)and∀x ∈ dom(ψ),ψ(x) = ψ′(x).
We assume that a true unknown realization φ ∈ Φ is drawn according to a known prior over
V
Φ . This prior is denoted by p unless explicitly stated otherwise. An adaptive algorithm interac-
V
tively selects elements and observes their states: At time t, an element x ∈ V is selected and its
t
stateφ(x )isobserved. Thepreviousobservationsattimetarerepresentedbythepartialrealization
t
ψ ⊆ φ whose domain is {x ,...,x }. The utility of selecting a specific set of elements under a
t 1 t
giventruerealization ismeasuredusingautilityfunction f : 2V ×Φ → R,whichmapseachpair
V
ofasetofelementsandarealization toarealvaluerepresenting theutilitywhenthissetisselected
underthegivenrealization. Thegoalistoobtainahighutilitywhileselecting fewelements.
A policy π : Ψ → V ∪{⊥} is a (possibly non-deterministic) map from partial realizations
V
to elements that determines which element to select next, given the past observations represented
by the input partial realization. π(ψ) = ⊥ for a given ψ ∈ Ψ indicates that π terminates after
V
observing ψ. For a policy π and a realization φ ∈ Φ , we denote by E(π,φ) ⊆ V the (possibly
V
random)setofelementsthatthepolicyπselectsthroughtheentirerunofthepolicyunderrealization
φuntiltermination. Theheightofapolicyisthemaximalnumberofelementsitmayselect,forany
truerealization andanyrandombits. Foranintegerk,Π denotesthesetofallpolicieswithheight
k
4ADAPTIVE COMBINATORIAL MAXIMIZATION: BEYOND APPROXIMATE GREEDY POLICIES
at most k. For any policy π, a sub-policy of π is any policy that runs exactly like π, except that it
possibly terminatesearlierinsomecases.
Theexpected utilityofapolicyunderagivenpriorpforagivenutilityfunctionf isgivenby
f (π,p) := E[f(E(π,φ),φ)].
avg
Hereand below,theexpectation isoverthetruerealization φ ∼ paswellastherandomness ofthe
policy. Theexpectedcostofusingagivenpolicyisdefinedastheexpectednumberofelementsthat
itselectsuntiltermination, givenby
c (π,p) := E[|E(π,φ)|].
avg
Theexpected marginal gain ofanelement v ∈ V under agiven prior pandautility function f
with respect to a partial realization ψ ∈ Ψ is the expected contribution to the utility value if v is
V
selected next, assuming that the elements selected so far and their observed states are as specified
byψ ∈ Ψ . Formally,theexpected marginalgainisgivenby
V
∆f(v | ψ) := E[f({v}∪dom(ψ),φ)−f(dom(ψ),φ) |φ ∼ ψ].
p
Theexpectedmarginalgainofapolicyπ underpandf,assumingthepreviousobservationsinψis
theexpectedcontribution totheutilityuntiltermination, isdenoted by
∆f(π | ψ) := E[f(E(π,φ)∪dom(ψ),φ)−f(dom(ψ),φ) |φ ∼ ψ].
p
3.2. Propertiesofutilityfunctionsandpolicies
A utility function f is adaptive monotone if selecting an element is never harmful in expecta-
tion. Formally, f is adaptive monotone if for any partial realization ψ and any v ∈ V \dom(ψ),
∆f (v | ψ) ≥ 0. A utility function f is adaptive submodular if selecting an element satisfies an
p
expected diminishing-returns property, that is, ifselecting thesameelement laternever contributes
moreinexpectation totheutilitythanselecting thesameelementearlierinthesamerun. Formally,
a utility function is adaptive submodular if for any two partial realizations ψ,ψ′ such that ψ ⊆ ψ′
andforanyv ∈ V \dom(ψ′),
∆f(v |ψ) ≥ ∆f(v | ψ′).
p p
Theadaptivesubmodularityratioofautilityfunctionquantifieshowcloseitistobeingsubmodular.
Foragivenpriorpandtwointegersn,k,theadaptivesubmodularity ratiooff isdefinedas
P [v ∈ E(π,Φ) | Φ ∼ ψ′]∆f (v |ψ′)
γs (f,p) := min v∈V Φ∼p p .
n,k ψ∈ΨV:|ψ|≤n,ψ′⊆ψ,π∈Πk
P
∆f p(π |ψ′)
The adaptive submodularity ratio γs (f,p) takes values in [0,1], where a higher value indicates
n,k
thatafunctionisclosertobeingadaptivesubmodular, andavalueof1indicatesthatthefunctionis
adaptivesubmodular.
Previousworkshavestudiedgreedyorapproximategreedypolicies,which,ineachround,select
an element that maximizes or approximately maximizes the expected marginal gain. For α ≥ 1, a
policyisconsidered α-approximate greedy ifforanyψ ∈ Ψ suchthatπ(ψ) 6= ⊥,
V
1
∆f(π(ψ) |ψ) ≥ max∆f(v | ψ).
p α v∈V p
5WEITZMAN SABATO
A greedy policy is a policy which is 1-approximate greedy. Denote by α (f,p) the smallest real
π
value α such that π is an α-approximate greedy policy with respect to f and p. We term this the
greedyapproximation ratioofthepolicy.
3.3. Previousapproximation guarantees
GolovinandKrause (2011) proved an approximation guarantee for the expected utility of an ap-
proximate greedy policy π of height at most l, assuming that f is an adaptive submodular utility
function. Theyprovedthatforanypolicyπ∗ withheightk,
l
f (π,p) > 1−exp(− ) f (π∗,p). (1)
avg avg
α (f,p)·k
π
(cid:18) (cid:19)
FujiiandSakaue (2019)considered the same setting, but allowed also utility functions that are
not adaptive submodular, using the adaptive submodularity ratio of the utility function. They re-
quiredthepolicytobeexactlygreedy,thusnotsupportingα-approximategreedypoliciesforα > 1.
Theyprovedthatforanypolicyπ∗ withheightk,
γs (f,p)·l
f (π,p) ≥ 1−exp(− l,k ) ·f (π∗,p). (2)
avg avg
k
!
Thiscoincides withEq.(1)foradaptive submodular functions (γs (f,p) = 1) andgreedy policies
l,k
(α = 1).
Esfandiari etal. (2021) provided a guarantee that depends on the average costs of the policies.
This contrasts with the results discussed above, which depend on the height of the two policies
being compared. They considered only greedy policies and adaptive submodular functions. They
provedthatforagreedypolicyπ,ifitterminateswhentheexpected marginalgainofallremaining
elementsisbelowsomefixedthreshold, andifitsaverage costisl,thenforanypolicyπ∗,
l
f (π,p) > 1−exp − ·f (π∗,p). (3)
avg c (π∗,p)+1 avg
(cid:18) (cid:18) avg (cid:19)(cid:19)
They further proved an approximation guarantee for the minimum cost coverage objective. This
approximation guarantee holds for functions with a discrete covering property first proposed by
GolovinandKrause (2011). A utility function f is said to satisfy the discrete covering property
withparametersQ,η > 0ifthereexistsavalueη ∈ (0,Q]suchthatforallψ ∈ Ψ andφ∈ Φ ,if
V V
f(dom(ψ),φ) > Q−η,thenf(dom(ψ),φ) = Q. Esfandiari etal.(2021)provedthatforanysuch
function,andanygreedypolicyπ,ifπ∗isapolicythatachievescovering,thatis,f(E(π∗,φ),φ) =
Qforallφ∈ Φ ,then
V
|V|Q
c (π,p) ≤ (c (π∗,p)+1)log( )+1. (4)
avg avg
η
This improves a previous bound by GolovinandKrause (2017), which had a squared logarith-
mic dependence. GolovinandKrause (2011) studied Generalized Binary Search (GBS), which is
a specific strategy for Bayesian active learning with binary labels, and defined a coverage utility
function for active learning that has the discrete covering property with parameters Q = 1,η =
min p(φ). Plugging these parameters into Eq. (4) may be disadvantageous if the minimal
φ∈ΦV
6ADAPTIVE COMBINATORIAL MAXIMIZATION: BEYOND APPROXIMATE GREEDY POLICIES
probability assigned by p is very small, due to the term log(1/η) in the approximation factor.
GolovinandKrause (2011, 2017) defined a modified prior p′ ∝ max{p(φ),1/|Φ |2}, and de-
V
rivedanO(log2(|Φ |))approximationguaranteebasedontheirsquared-logarithmicapproximation
V
bound, foragreedypolicythatusesacoverageutilityfunction basedonthisprior.
4. Mainresults
In this section, we provide an overview of our main results. In Section 5 below, we define a new
policy parameter, which we term the maximal gain ratio. This is the maximal ratio between the
expected marginal gain of remaining elements at termination and the expected marginal gain of
elementsselected bythepolicy. Themaximalgainratioofapolicyπ givenautilityfunction f and
apriorpisdenoted byβ (f,p). Ourapproximation guarantees, provided below,areparameterized
π
by β (f,p), and are stronger when β (f,p) is smaller. It is therefore desirable to have β (f,p)
π π π
as small as possible. In Section 5, we prove that the value of β (f,p) is never larger than that
π
of α (f,p), the greedy approximation ratio of the policy, and that it can actually be significantly
π
smaller. Thisshows that aguarantee that uses β (f,p)is strictly stronger than the same guarantee
π
withα (f,p). Weprovethefollowingproperties:
π
• Forallpoliciesπ,utilityfunctions f andpriorsp,β (f,p) ≤ α (f,p).
π π
• β can have a value of 1 even if the policy is not greedy. This contrasts with α , which is
π π
equalto1ifandonlyifthepolicyπ isgreedy.
• β canbearbitrarily closetozero. Thisisincontrasttoα ,whichisboundedbelowby1.
π π
Having definedthemaximal gainratio, weprovide newapproximation guarantees that usethis
parameter, instead of the greedy approximation ratio or an assumption that a policy is greedy, as
inprevious works. Inaddition, theseapproximation guarantees support multiple regimes thatwere
previously studied separately: functions that are not necessarily adaptive submodular, policies that
are not necessarily greedy, and guarantees for the minimum cost coverage objective for functions
withthediscrete coveringproperty.
Ourfirstapproximation guarantee concerns utilitymaximization.
Theorem1 Let f be a non-negative utility function which is adaptive monotone with respect to
a prior p. Let π be a policy with height n that terminates when the expected marginal gain of all
remainingelements isbelow somefixedthreshold andhasanaverage costofl.1 Letπ∗ beapolicy
withheightk. Then
β (f,p)
f (π,p) ≥ 1−exp −l π ·c (π∗,p)+1 ·f (π∗,p).
avg γs (f,p) avg avg
(cid:30)(cid:18) n,k (cid:19)!!
Thisresult includes asaspecial case the conditions ofEq.(3),which require f to beadaptive sub-
modularandπtobegreedy. Inthiscase,γs (f,p)= 1andβ (f,p) ≤ 1. Thus,eveninthisspecial
n,k π
case, the theorem provides a smaller approximation factor whenever β (f,p) < 1. Moreover, the
π
sameguarantee asEq.(3)isobtainedalsofornon-greedy policies iftheyhaveβ (f,p) = 1.
π
Ournextapproximation guarantee considers theminimumcostcoverageobjective.
1.WeprovideaformalversionofthisconditioninAppendixA.
7WEITZMAN SABATO
Theorem2 Letf be anon-negative utility function whichis adaptive monotone withrespect to a
priorp,suchthatf hasthediscretecoveringpropertywithparametersQ,η. Letπ beapolicywith
heightn. Letπ∗ beapolicywithheightk suchthatf(E(π∗,φ),φ) = Qforallφ∈ Φ . Then,
V
β (f,p) nQ
c (π,p) ≤ π ·c (π∗,p)+1 ·log( )+2.
avg γs (f,p) avg η
n,k !
Considering again the special case in which f is adaptive submodular and π is greedy, werecover
Eq.(4)uptoanadditivetermof1,notingthatn ≤ |V|. SimilarlytoTheorem1,heretoothisspecial
caseisinfactbroaderandcanprovidestrongerguaranteesthanthecorrespondinginequalityEq.(4).
ThetwotheoremsaboveareprovedinAppendixA.
Lastly,weprovideapproximationguaranteesthatallowprovidingastrongerguaranteeforpriors
with small probabilities, using the modified prior p′ of GolovinandKrause (2011). We prove our
results for a more general setting with general utility functions, and apply them to the coverage
utilityfunction proposed byGolovinandKrause(2011)forBayesian activelearning. Weconclude
thatwhenusingagreedypolicywithacoverageutilityfunctionbasedonthemodifiedprior,denoted
f p′,wehavethefollowingaveragecostapproximation guarantee foranypolicyπ,comparedtothe
optimalpolicyπ∗.
c avg(π,p) ≤ 2 β π(f p′,p′)·(c avg(π∗,p)+1)+1 log(2|Φ V|2n)+4. (5)
In particular, if π is greedy w(cid:0) ith respect to p′, then β π(f p′,p′)(cid:1) ≤ 1, and it could be significantly
smaller,asdiscussed above. Weprovidethenecessary definitions andproveEq.(5)inSection6.
Our results thus provide a comprehensive and improved set of approximation guarantees that
encompass a wide range of problem parameters, and highlight the usefulness of the new maximal
gainratioparameter. Inthenextsection, wedefinethemaximalgainratioandproveitsproperties.
5. The MaximalGainRatio
Inthissection, wedefine themaximal gainratio andprove itsproperties aslisted inSection 4. We
start with necessary definitions. Given a utility function f, a prior p and a policy π, let τ ≥ 0 and
ρ ∈ [0,1] be real numbers. Wedenote by πτ,ρ asub-policy of π that with probability ρterminates
if the expected marginal gain of every single remaining element is strictly smaller than τ, and
otherwise(withprobability1−ρ),terminatesiftheexpectedmarginalgainofeverysingleremaining
element is at most τ. We call τ a threshold and ρ a tie-break probability. To define the maximal
gainratio,werequireτ,ρsuchthatc (πτ,ρ,p)isequaltoagiveninteger. Esfandiarietal.(2021)
avg
studied these types ofsub-policies whenπ isgreedy, and provided aproof sketch for theexistence
of such τ,ρ in this context. Weprovide here a full unconditional existence proof for any policy π,
andproveinaddition thatthereisauniquesuchsub-policy foreachpossible integer.
Lemma3 Assume a utility function f, a policy π, and a prior p. For any integer i ≤ c (π,p),
avg
there exist a threshold τ
i
≥ 0 and a tie-break probability ρ
i
∈ [0,1] such that c avg(πτi,ρi,p) = i.
Moreover, allsuchpairsτ ,ρ inducethesamepolicy.
i i
Proof For a threshold τ, denote µτ := c (πτ,1,p). Define an equivalence relation between
avg
thresholdsasfollows: foreveryτ,τ′,saythatτ ≡ τ′ifµτ = µτ′ . Sincethesetsofelementsandthe
8ADAPTIVE COMBINATORIAL MAXIMIZATION: BEYOND APPROXIMATE GREEDY POLICIES
setofpossiblestatesarefinite,thesetofallthemarginalgainsforagivenpriorisalsofinite. Hence,
there is a finite number of equivalence classes. Denote this number s. Fix some representatives of
the equivalence classes and denote them by τ1 > ... > τs. Note that µτs = c (π,p). Define
avg
µτ0 = 0, and note that the sequence (µτj ) is strictly increasing. Let i ≤ c (π,p).
j∈{0,...,s} avg
Clearly,i ≤ µτs. Letj bethesmallestindexin[s]suchthati≤ µτj. Denoteτ := τj and
i
i−µτj−1
ρ := .
i µτj −µτj−1
Note that ρ
i
∈ [0,1]. Consider the policy πτi,ρi. By the definition of thresholds and tie-break
probabilities, itholdsthat
c (πτi,ρi,p) = (1−ρ )µτj−1 +ρ µτj = µτj−1 +ρ (µτj −µτj−1 ) = i, (6)
avg i i i
Where the last equality follows from the definition of ρ . This proves the existence of (τ ,ρ ), as
i i i
claimedinthefirstpartofthelemma.
For the second part of the lemma, we show that any such pair would induce the same policy.
Let τ′ and ρ′ be such that c (πτ′,ρ′ ,p) = i. First, note that one of πτ′,ρ′ and πτi,ρi must be a
avg
sub-policy of the other: If τ′ < τ i, then πτi,ρi is a sub-policy of πτ′,ρ′ , since the former always
terminates before or with the latter. If τ < τ′, then the symmetric claim holds. Lastly, if τ′ = τ ,
i i
then we may assume w.l.o.g that the policy with the larger ρ never terminates in cases where the
otherpolicycontinues, byassumingacoordinated drawofrandom bits.
Next, assume for contradiction that πτ′,ρ′ 6= πτi,ρi. Therefore, there exists at least one partial
realization ψ and random bits such that immediately after observing it,
πτ′,ρ′
terminates, while
πτi,ρi selects an element. Since no partial realization satisfies the opposite case, it follows that
i = c (πτ′,ρ′ ,p) < c (πτi,ρi,p) = i, a contradiction. Thus, in all cases, πτ′,ρ′ = πτi,ρi, as
avg avg
claimed.
Given a policy π and an integer i ≤ c (π,p), denote by π the unique sub-policy of π using
avg i
the threshold and tie-break construction and has an average cost of i. Denote by π a policy that
0
terminates before selecting any element. Denote by ∆u the maximal expected marginal gain of
π,i
remaining elements when π terminates in any of its possible paths. Formally, ∆u is the smallest
i π,i
realvalueusuchthat
P[Fort ∈Nsuchthatπ terminatesattimet,∀v ∈ V,∆f(v |ψ ) ≤ u]= 1.
i p t
Denoteby∆l thesmallestpossible expectedmarginalgainofelementsthatπ selectsatanytime
π,i i
duringitsrun. Formally,thisisthelargestrealvalueusuchthat
P[Foralltuntilπ terminates,∆f(π (ψ ) |ψ ) ≥u] = 1.
i p i t t
Equippedwiththesedefinitions, wecannowdefinethemaximalgainratio.
Definition4(maximalgainratio) The maximal gain ratio of a policy π with respect to a utility
function f andapriorpisβ π(f,p):= max i∈N,i≤c avg(π,p)∆u π,i/∆l π,i.
9WEITZMAN SABATO
The maximal gain ratio is thus based on comparing the possible gain of elements that were not
selected, to the gain from selected elements. We now show that the maximal gain ratio never
larger than the greedy approximation parameter, and is sometimes strictly smaller. This shows
that Theorem 1, Theorem 2 and Eq. (5), are stronger due to their use of β than if they had relied
π
onα instead.
π
First, we show that the maximal gain ratio of an approximate greedy policy is always upper
bounded bythegreedyapproximation parameter ofthesamepolicy.
Theorem5 Foranypriorp,utilityfunction f,andpolicyπ,wehaveβ (f,p)≤ α (f,p).
π π
Proof Let π be a policy such that α (f,p) < ∞, and denote α := α (f,p). Let i ≤ c (π,p).
π π avg
Let τ
i
and ρ
i
be a threshold and a tie-break probability, respectively, such that π
i
:= πτi,ρi. By
the definition of πτi,ρi, we have that τ
i
is an upper bound on the expected marginal gain of all the
elementsthatremainwhenπτi,ρi terminates. Therefore, ∆u
π,i
≤ τ i.
At all times during the run of π up to the time of termination, there exists some element with
i
anexpectedmarginalgainofatleastτ i. Otherwise,bydefinitionofπ
i
= πτi,ρi,therunwouldhave
terminatedearlier. Therefore,sinceπisanα-approximategreedypolicy,theexpectedmarginalgain
oftheelementsthatπ selectsatanytimeduringitsrunisatleastτ /α. Itfollowsthat∆l ≥ τ /α.
i i π,i i
Therefore,
τ
β (f,p)= max ∆u /∆l ≤ max i = α.
π i≤c
avg(π,p)
π,i π,i i≤c
avg(π,p)
τ i/α
Thiscompletes theproof.
We now show that there are cases in which β is strictly smaller than α . First, we show that an
π π
approximate greedypolicythatisnotgreedycanstillhaveβ (f,p)= 1.
π
Theorem6 For any integer k ≥ 3, there exist a policy π with height k, a prior p and a utility
function f suchthatβ (f,p) = 1whileα (f,p)= 2.
π π
Proof For a given integer k ≥ 3, let V = {v ,...,v }, and let Y = {0,1}. For any i ∈ [k], we
1 k
denote v := {v ,...,v }. We define a utility function f as follows. For a realization φ and a
[i] 1 i
subsetA⊆ V,letπ beapolicythatselectsv inthei’thstepofitsrunandterminatesimmediately
i
f
after selecting v . Bythe properties of∆ (v | ψ), ifthe observed partial realization isψ such that
k p
f
dom(ψ) = ∅ or dom(ψ) = v , then the next element v that π selects satisfies ∆ (v | ∅) =
[k−1] p
f
1/k = max ∆ (v | ψ).Additionally, ifdom(ψ) = v forsomei ∈{2,...,k−1},then
v∈V p [i−1]
1 1 2 1 1
∆f(v |ψ) = = · = ·∆f(v | ψ) = ·max∆f(v | ψ).
p i k 2 k 2 p i+1 2 v∈V p
Therefore, α (f,p)= 2.
π
Ontheother hand, wenowshowthat β (f,p) = 1. Leti ≤ c (π,p). First,weshow thatfor
π avg
τ = 1/k andρ= i/k,itholdsthatπ = πτ,ρ. Duetothevalueofτ,withprobability ρ,πτ,ρ selects
i
all ofthe elements ofV and terminates. Otherwise (with probability 1−ρ)πτ,ρ terminates before
selecting anyelement. Therefore,
i
c (πτ,ρ,p) = ρ·k+(1−ρ)·0 = ·k = i.
avg
k
10ADAPTIVE COMBINATORIAL MAXIMIZATION: BEYOND APPROXIMATE GREEDY POLICIES
By Lemma 3, π = πτ,ρ. Now, the largest possible expected marginal gain of remaining elements
i
when π terminates is 1/k. Hence, ∆u = 1/k. The smallest possible expected marginal gain of
i π,i
anyelementselected byπ isalso1/k. Hence,∆l = 1/k. Therefore,
i π,i
1/k
β (f,p)= max ∆u /∆l = max = 1.
π i≤cavg(π,p) π,i π,i i≤cavg(π,p)1/k
Thiscompletes theproof.
Lastly, we show that there exist policies π that are greedy with respect to a utility function f and
a prior p that have an arbitrarily small β (f,p). Thus, guarantees based on β (f,p) are strictly
π π
stronger thanguarantees basedonlyonthefactthatapolicyisgreedy.
Theorem7 Foranyintegerkandanyǫ > 0,thereexistapriorp,autilityfunctionf andagreedy
policyπ withrespecttof thatselectsk elements,suchthatβ (f,p)= ǫ.
π
Proof Let k be an integer and let ǫ > 0. Let V = {v ,v ,...,v }, Y = {0,1}. We define a
1 2 k
utility function f as follows. For any realization φ and any subset A ⊆ V, f(A,φ) = |A| ǫi.
i=0
Letthepriorpbeauniformdistribution overtherealizations. Bythedefinitionoff,foranypartial
realization ψ,itholdsthat∆f (v |ψ) = ǫ|ψ|+1. P
p
Let π be a policy that selects v in the i’th step of its run and terminates immediately after
i
f
selecting v . Foranypartial realization ψ, all oftheelements v ∈ V hasthe samevalue of∆ (v |
k p
ψ). Thereforeπ isgreedywithrespect tof.
Recall that β (f,p) = max ∆u /∆l . To bound β (f,p), consider the possible
π i≤c avg(π,p) π,i π,i π
values of i ≤ c = k. For i = k, the largest possible expected marginal gain of remaining
avg(π,p)
elements when π terminates is 0. Therefore, ∆u = 0 and so ∆u /∆l = 0. For i ≤ k −
n π,k π,k π,k
1, the largest possible expected marginal gain of remaining elements when π terminates is ǫi+1.
i
Therefore,∆u = ǫi+1. Thesmallestpossible expectedmarginalgainofelementsthatπ selectsat
π,i i
anytimeduringitsrunisǫi. Therefore,∆l = ǫi. Hence,∆u /∆l = ǫi+1/ǫi = ǫ. Itfollowsthat
π,i π,i π,i
β (f,p) := max ∆u /∆l = max{0,ǫ} = ǫ,asclaimed
π i≤c avg(π,p) π,i π,i
Wenote that ithas been shown inprevious works (ConfortiandCornue´jols,1984;Vondrak, 2010)
thatforsomeutility functions, stronger approximation guarantees can beprovided forgreedy poli-
ciesthanthosethatcanbeobtained forgeneral submodularfunctions, basedonthecurvatureprop-
ertyofthefunction. Itisthushelpfultoobservethatguaranteesbasedoncurvaturecannotbeusedto
provide stronger approximation guarantees intheexampleprovided intheproof above. Curvature-
basedguaranteesarestrongeriftheutilityfunctionisclosertobeingmodular(additive). Incontrast,
in the example above, the function is far from being modular, as adding the same element later in
the run results in a significantly lower gain. More precisely, the curvature property takes values in
[0,1] for submodular functions, and results concerning curvature are stronger than results for gen-
eral submodular functions ifthe curvature issmaller. However, thecurvature ofthe function inthe
proofaboveis1−ǫk−1,makingitapoorcandidate togainfromcurvature-based bounds.
The properties proved above for the maximal gain ratio imply that Theorem 1 and Theorem 2
are strictly stronger than those in previous works, even for adaptive submodular utility functions
andgreedypolicies. TheproofsofthesetheoremsareprovidedinAppendixA.
11WEITZMAN SABATO
6. Usinga modified prior
In this section, we present an approximation guarantee that uses the maximal gain ratio and the
adaptive submodularity ratio for the modified prior, and use it to conclude Eq. (5), which provides
anapproximation guarantee thatdoesnotdependonthesmallestprobability inthetrueprior.
Theorem8 Letf be anon-negative utility function whichis adaptive monotone withrespect to a
priorp,suchthatf hasthediscretecoveringpropertywithparametersQ,η. Letπ beapolicywith
height n. Let π∗ be a policy with height k ≤ |Φ | such that f(E(π∗,φ),φ) = Q for all φ ∈ Φ .
V V
Letp′(φ) ∝ max{p(φ),1/|Φ |2}bethemodifiedpriorofp. Then
V
β (f,p′) nQ
c (π,p) ≤ 2 π (c (π∗,p)+1)+1 log( )+4.
avg γs (f,p′) avg η
n,k !
ThisresultimprovesoverthepreviousguaranteesofGolovinandKrause(2011,2017)forthemod-
ifiedprior, since thelatter hadasquare logarithmic factor andonly applied togreedy policies. The
proofofthistheorem isprovided inAppendixB.
Before deriving Eq. (5), we provide background on the coverage utility function proposed by
GolovinandKrause (2011) and its application to active learning. In the context of active learning,
the observations in combinatorial maximization setting represent labeled examples. The coverage
functionmeasurestheprobabilitymassofrealizationsthathavebeendisqualifiedduetotheobserva-
tionsintheobservedpartialrealization. Todefinethecoveragefunctionformally,denotetheversion
space ofψ ∈ Ψ byVS(ψ) := {φ ∈ Φ | φ ∼ ψ}. ForH ⊆ Φ,denote p(H) := p(φ). For
V φ∈H
A⊆ V,φ∈ Φ,denote VS = VS({(x,φ(x)) |x ∈ A}).
A,φ
P
Definition9(coverage utilityfunction) For a prior p over Φ , the coverage utility function is
V
denoted f : 2V × Φ → [0,1] and defined such that for any realization φ and any set A ⊆ V,
p V
f (A,φ) := 1−p(VS )+p(φ).
p A,φ
GolovinandKrause (2011) proved that for any prior p, f is adaptive monotone and adaptive
p
submodular with respect to p. The latter implies that γ ns ,k(f p′,p′) = 1. In addition, note that if
VS ={φ},thatis,thetruerealization isfullyidentifiedbyobserving thelabelsoftheexamples
A,φ
inA,thenf (A,φ) = 1,whichisthefunction’s maximalpossible value. Thus,inthiscasewecan
p
setQ = 1inTheorem8. Inaddition,forf p,η = min φ∈ΦV p(φ). Whenusingf p′ withthemodified
priorp′,weget
1/|Φ |2 1/|Φ |2 1
η = min p′(φ) ≥ V ≥ V ≥ .
φ∈ΦV
φ∈φV
max{p(φ),1/|Φ V|2} 1+1/|Φ V| 2|Φ V|2
P
By substituting these in Theorem 8, we conclude that for any policy π∗ with height at most |Φ |,
V
Eq. (5) holds. We have left to show that π∗ can be set to an optimal policy, by proving that there
exists an optimal policy with height at most |Φ |. While this makes intuitive sense, we formally
v
state and prove it for completeness in Lemma 11 in Appendix B. This completes the derivation of
Eq.(5).
12ADAPTIVE COMBINATORIAL MAXIMIZATION: BEYOND APPROXIMATE GREEDY POLICIES
7. Conclusion
Inthiswork,weprovidednewapproximation guarantees foradaptive combinatorial maximization.
We identified a new policy parameter, the maximal gain ratio, which is less restrictive than the
greedyapproximationparameter,andshowedthatusingthisparametercanleadtostrongerapprox-
imation guarantees. Moreover, our guarantees are comprehensive, simultaneously supporting the
new policy parameter as well asnear-submodular utility functions, and include both maximization
underacardinality constraint andaminimumcostcoverageguarantee. Inaddition, weprovidedan
improvedapproximation guaranteeforamodifiedprior,whichiscrucialforactivelearningguaran-
teesthatdonotdependonthesmallestprobability intheprior.
Thenewmaximalgainratioparametershedsnewlightonthepropertiesapolicyneedstohaveto
support general approximation guarantees foradaptive combinatorial maximization. Thisopens an
excitingdirection ofresearch ontherelationship betweenpolicyproperties andtheapproximations
thatcanbeobtainedbasedonthem.
Acknowledgments
Resources used in preparing this research were provided, in part, by the Province of Ontario,
the Government of Canada through CIFAR, and companies sponsoring the Vector Institute; see
https://vectorinstitute.ai/partnerships/current-partners/.
References
GeorgiosAmanatidis,FedericoFusco,PhilipLazos,StefanoLeonardi,andRebeccaReiffenha¨user.
Fast adaptive non-monotone submodular maximization subject to a knapsack constraint. Ad-
vancesinneuralinformation processing systems,33:16903–16915, 2020.
Arash Asadpour, Hamid Nazerzadeh, and Amin Saberi. Stochastic submodular maximization. In
Internet and Network Economics: 4th International Workshop, WINE 2008, Shanghai, China,
December17-20, 2008.Proceedings 4,pages477–489. Springer, 2008.
EricBalkanskiandYaronSinger.Approximationguaranteesforadaptivesampling. InInternational
ConferenceonMachineLearning, pages384–393. PMLR,2018.
YuxinChenandAndreasKrause.Near-optimalbatchmodeactivelearningandadaptivesubmodular
optimization. InInternational ConferenceonMachineLearning,pages160–168. PMLR,2013.
MicheleConfortiandGe´rardCornue´jols. Submodular setfunctions, matroidsandthegreedyalgo-
rithm: tightworst-caseboundsandsomegeneralizations oftherado-edmonds theorem. Discrete
appliedmathematics, 7(3):251–274, 1984.
Nguyen VietCuong, WeeSunLee, and NanYe. Near-optimal adaptive pool-based active learning
withgeneral loss. In30thconference onUncertainty inArtificial Intelligence (UAI),pages122–
131,2014.
Abhimanyu Das and David Kempe. Submodular meets spectral: Greedy algorithms for subset
selection, sparseapproximation anddictionary selection. arXivpreprint arXiv:1102.3975, 2011.
13WEITZMAN SABATO
Janardhan Rao Doppa. Adaptive experimental design for optimizing combinatorial structures. In
IJCAI,pages4940–4945, 2021.
Jack Edmonds. Submodular functions, matroids, and certain polyhedra. Combinatorial Structures
andTheirApplications, pages69–87,1970.
Hossein Esfandiari, Amin Karbasi, and Vahab Mirrokni. Adaptivity in adaptive submodularity. In
ConferenceonLearningTheory,pages1823–1846. PMLR,2021.
Kaito Fujii and Shinsaku Sakaue. Beyond adaptive submodularity: Approximation guarantees of
greedypolicywithadaptivesubmodularityratio. InInternationalConferenceonMachineLearn-
ing,pages2042–2051. PMLR,2019.
Victor Gabillon, Branislav Kveton, Zheng Wen, Brian Eriksson, and Shanmugavelayutham
Muthukrishnan. Adaptivesubmodularmaximizationinbanditsetting. AdvancesinNeuralInfor-
mationProcessing Systems,26,2013.
Daniel Golovin and Andreas Krause. Adaptive submodularity: Theory and applications in active
learning and stochastic optimization. Journal of Artificial Intelligence Research, 42:427–486,
2011.
Daniel Golovin and Andreas Krause. Adaptive submodularity: A new approach to active learning
andstochastic optimization. CoRR,abs/1003.3967v5, 2017.
Alkis Gotovos, Amin Karbasi, and Andreas Krause. Non-monotone adaptive submodular maxi-
mization. InIJCAI,pages1996–2003, 2015.
Andrew Guillory and Jeff Bilmes. Interactive submodular set cover. arXiv preprint
arXiv:1002.3345, 2010.
Hideitsu Hino. Active learning: Problem settings and recent developments. arXiv preprint
arXiv:2012.04225, 2020.
Andrew McCallum, Kamal Nigam, et al. Employing em and pool-based active learning for text
classification. InICML,volume98,pages350–358. Citeseer,1998.
Marko Mitrovic, Ehsan Kazemi, Moran Feldman, Andreas Krause, and Amin Karbasi. Adaptive
sequence submodularity. AdvancesinNeuralInformation Processing Systems,32,2019.
George L Nemhauser, Laurence A Wolsey, and Marshall L Fisher. An analysis of approximations
formaximizingsubmodularsetfunctions—i. MathematicalProgramming,14(1):265–294, 1978.
SivanSabato. Submodularlearningandcoveringwithresponse-dependent costs. TheoreticalCom-
puterScience, 742:98–113,2018.
Lior Seeman and Yaron Singer. Adaptive seeding in social networks. In 2013 IEEE 54th Annual
Symposium onFoundations ofComputerScience, pages459–468. IEEE,2013.
ShaojieTang.Beyondpointwisesubmodularity: Non-monotoneadaptivesubmodularmaximization
inlineartime. Theoretical ComputerScience, 850:249–261, 2021. ISSN0304-3975.
14ADAPTIVE COMBINATORIAL MAXIMIZATION: BEYOND APPROXIMATE GREEDY POLICIES
Shaojie Tang. Robust adaptive submodular maximization. INFORMS Journal on Computing, 34
(6):3277–3291, 2022.
ShaojieTangandJingYuan. Streamingadaptivesubmodular maximization. TheoreticalComputer
Science, 944:113644, 2023.
Guangmo Tong, Weili Wu, Shaojie Tang, and Ding-Zhu Du. Adaptive influence maximization in
dynamicsocialnetworks. IEEE/ACMTransactions onNetworking, 25(1):112–125, 2016.
Jan Vondrak. Submodularity and curvature: the optimal algorithm. RIMSKoˆkyuˆroku Bessatsu, 01
2010.
LaurenceAWolsey. Ananalysis ofthegreedyalgorithm forthesubmodular setcoveringproblem.
Combinatorica, 2(4):385–393, 1982.
Ji-Jie Wu and Kuo-Shih Tseng. Adaptive submodular inverse reinforcement learning for spatial
searchandmapexploration. Autonomous Robots,46(2):321–347, 2022.
Appendix A. Proving the approximationguarantees forthe givenprior
In this section, we prove Theorem 1 and Theorem 2, stated in Section 4. To prove Theorem 1,
we first prove the following lemma, which provides a lower bound for the difference between the
averageutilityoftwosub-policies.
Lemma10 Let π be a policy, p be a prior, and f be a utility function. For any integer i ≤
c (π,p),define∆ := f (π ,p)−f (π ,p). Then∆ ≥∆l .
avg i avg i avg i−1 i π,i
Proof Let i ≤ c avg(π,p). Let τ i,τ i−1,ρ i,ρ
i−1
be parameters such that π
i
= πτi,ρi and π
i−1
=
πτi−1,ρi−1. Theseexistaccording toLemma3. Observethatnecessarily, τ
i
≤ τ i−1. Thisisbecause
if τ < τ , then the termination condition of π is stronger than that of π , implying that i =
i−1 i i i−1
c (π ,p)≤ c (π ,p) = i−1,acontradiction.
avg i avg i−1
Observethatπ mustbeasub-policyofπ : Ifτ < τ ,thenπ isasub-policyofπ ,since
i−1 i i i−1 i−1 i
theformeralwaysterminates beforeorwiththelatter. Ifτ = τ ,thenρ < ρ . Inthiscase, π
i i−1 i i−1 i
neverterminatesincaseswhereπ continues, byassumingacoordinated drawofrandom bits.
i
f
For a set of elements S, a realization φ and a partial realization ψ, we define ∆ (S | ψ) :=
p
E[f(S ∪dom(ψ),φ)−f(dom(ψ),φ) |φ ∼ ψ]. LetA tobetheeventthatthefollowinghold:
ψ,i,S
1. Thepolicyπ selectsexactlyalloftheelementsofdom(ψ),
i−1
2. Theobservations inψ arerealized, and
3. thesetofelementsselectedbyπ coincides withdom(ψ)atsomepointinitsrun,andimme-
i
diatelyafterthat,π selectsexactlyalloftheelementsofS andterminates.
i
15WEITZMAN SABATO
Sinceπ isasub-policy ofπ ,wehave
i−1 i
∆ = f (π ,p)−f (π ,p)
i avg i avg i−1
= P[A ]∆f(S | ψ)
ψ,i,S p
ψ X∈ΨV S X⊆V
≥ P[A ]·|S|·∆l
ψ,i,S π,i
ψ X∈ΨV S X⊆V
= (c (π ,p)−c (π ,p))·∆l
avg i avg i−1 π,i
= (i−(i−1))·∆l = ∆l ,
π,i π,i
wheretheinequality followsfromthedefinition of∆l . Thisprovestheclaim.
π,i
Usingthislemma,wecannowproveTheorem1.
ThestatementofTheorem1includesaconditionthatthepolicyπterminateswhentheexpected
marginalgainofallremainingelementsisbelowsomefixedthreshold andhasanaveragecostofl.
Formally, we require π to be equal to π′ for some policy π′ (see Lemma3 and the definition of π
l i
thereafter).
Proof[ofTheorem1]Foranyrealizationφandanyelementv ∈ V,denote v := [v ∈ E(π∗,φ)].
φ
Foranypartialrealization ψ suchthat|ψ| ≤ n,bythedefinitionofγs (f,p),wehave
n,k
γs (f,p)·∆f(π∗ | ψ) ≤ P [v ∈E(π∗,Φ)| Φ ∼ ψ]∆f(v | ψ)
n,k p Φ∼p p
v∈V
X
= E[ [v ∈ E(π∗,Φ)] | Φ ∼ψ]∆f(v | ψ)
p
v∈V
X
≤ E[|E(π∗,Φ)|] |Φ ∼ ψ]max∆f(v |ψ).
p
v∈V
Therefore,
E[|E(π∗,Φ)|] |Φ ∼ ψ]max ∆f (v | ψ)
∆f(π∗ | ψ) ≤ v∈V p . (7)
p γs (f,p)
n,k
For any integer i ≤ c (π ,p), let ∆ be defined as in Lemma 10. For two policies π′,π′′,
avg i i
we denote by π′@π′′ the policy that first runs π′ and then runs π′′ from the beginning, without
takingintoaccountanyinformation collectedduringtheexecutionofπ′. NotethatE(π′@π′′,φ) =
E(π′,φ)∪E(π′′,φ)foreveryrealization φ. Inparticular, itholds thatE(π @π∗,φ) = E(π ,φ)∪
i i
E(π∗,φ)foranyrealizationφ. Therefore,bytheadaptivemonotonicityoff,wehavef (π∗,p)≤
avg
f (π @π∗,p). Foranyψ ∈ Ψ ,wedenotebyA theeventthatπ observesψandthenterminates.
avg i V ψ i
Fori∈ {0,1,...,l}wedefine∆∗ = f (π∗,p)−f (π ,p). Fori∈ [l],
i avg avg i
∆∗ = f (π∗,p)−f (π ,p)
i avg avg i
≤ f (π @π∗,p)−f (π ,p)
avg i avg i
= P[A ]∆f(π∗ | ψ)
ψ p
ψ X∈ΨV
P[A ]E[|E(π∗,Φ)|] | Φ ∼ ψ]max ∆f (v |ψ)
≤
ψ∈ΨV ψ v∈V p
,
γs (f,p)
P n,k
16
1
1 1ADAPTIVE COMBINATORIAL MAXIMIZATION: BEYOND APPROXIMATE GREEDY POLICIES
wherethelastinequality isbyEq.(7). Bythedefinitionof∆u ,itholdsthatmax ∆f (v |ψ) ≤
π,i v∈V p
∆u . By the definition of β (f,p), we have ∆u ≤ β (f,p)∆l ≤ β (f,p)∆ , where the last
π,i π π,i π π,i π i
f
inequality followsfromLemma10. Therefore, max ∆ (v | ψ) ≤ β (f,p)∆ .Itfollowsthat
v∈V p π i
P[A ]E[|E(π∗,Φ)|] |Φ ∼ ψ]·β (f,p)∆
∆∗ ≤ ψ∈ΨV ψ π i .
i γs (f,p)
P n,k
Inaddition,
P[A ]E[|E(π∗,Φ)|] |Φ ∼ ψ] = E[|E(π∗,Φ)|]] = c (π∗,p).
ψ avg
ψ X∈ΨV
Letc∗ = c (π∗,p). Hence,
avg
β (f,p)·c∗
∆∗ ≤ π ·∆ .
i γs (f,p) i
n,k
Bythedefinitionsof∆ and∆∗,itholdsthat
i i
∆ = f (π ,p)−f (π ,p)
i avg i avg i−1
= f (π∗,p)−f (π ,p)−(f (π∗,p)−f (π ,p)) = ∆∗ −∆∗.
avg avg i−1 avg avg i i−1 i
Therefore,
β (f,p)·c∗
∆∗ ≤ π (∆∗ −∆∗).
i γs (f,p) i−1 i
n,k
Rearranging, weconcludethat
1
∆∗ ≤ 1− ·∆∗ .
i β (f,p)·c∗/γs (f,p)+1 i−1
π n,k !
Byinduction, foranyintegerlweobtain
l
1 l
∆∗ ≤ 1− ·∆∗ ≤ exp − ·∆∗.
l β (f,p)·c∗/γs (f,p)+1 0 β (f,p)·c∗/γs (f,p)+1 0
π n,k ! π n,k !
Since f is non-negative, we have f (π ,p) ≥ 0. Therefore, by the definition of ∆∗, ∆∗ ≤
avg 0 0 0
f (π∗,p). Hence,
avg
l
∆∗ ≤ exp − ·f (π∗,p).
l β (f,p)·c∗/γs (f,p)+1 avg
π n,k !
Therefore, bythedefinition of∆∗,itfollowsthat
l
l
f (π,p) ≥ 1−exp(− ) ·f (π∗,p).
avg β (f,p)·c∗/γs (f,p)+1 avg
π n,k !
Thiscompletes theproof.
17WEITZMAN SABATO
Next,weproveTheorem2.
Proof[ofTheorem2]Letc∗ = c (π∗,p). Define
avg
l := ⌈(β (f,p)·c∗/γs (f,p)+1)·log(nQ/η)⌉.
π n,k
ByTheorem1,wehave
l
f (π ,p) ≥ 1−exp(− ) ·f (π∗,p)
avg l β (f,p)·c∗/γs (f,p)+1 avg
π n,k !
(β (f,p)·c∗/γs (f,p)+1)·log(nQ/η)
≥ 1−exp(− π n,k ) ·f (π∗,p)
β (f,p)·c∗/γs (f,p)+1 avg
π n,k !
= (1−exp(−log(nQ/η)))·f (π∗,p)
avg
η
= 1− ·f (π∗,p)
avg
nQ
(cid:18) (cid:19)
= Q−η/n,
Wherethelastinequality followssincef (π∗,p) = Qbytheassumptions ofthetheorem.
avg
DenotebyF the(random)valueoff whenπ terminates. Bydefinition,E[F]= f (π ,p). By
l avg l
thedefinitionoff andQ,F ≤ Qwithprobabilityone. Therefore,therandomvariableX := Q−F
isnon-negative. ByMarkov’sinequality, P[X ≥ η] ≤ E[X]/η.Hence,
P[F > Q−η] = 1−P[F ≤ Q−η] = 1−P[X ≥ η] ≥ 1−E[X]/η
Q−f (π ,p) Q−(Q−η/n) 1
avg l
= 1− ≥ 1− = 1− .
η η n
By the assumptions of the theorem, if f(dom(ψ),φ) > Q − η, then f(dom(ψ),φ) = Q for
every ψ ∈ Ψ and φ ∈ Φ . Therefore, with probability at least 1 − 1/n, we have F = Q.
V V
Let t := E[|E(π ,φ)| | F = Q] be the expected cost of π conditioned on F = Q. Then l =
l l
c (π ,p) ≥ (1−1/n)·t, thus t ≤ l/(1−1/n). If F 6= Q, which happens with probability at
avg l
most1/n,thenπ selectsatmostnelements. Hence,
1 1 β (f,p)·c∗ nQ
π
c (π,p) ≤ (1− )·t+ ·n ≤ l+1 ≤ ( +1)·log( )+2.
avg n n γs (f,p) η
n,k
Thiscompletes theproof.
Appendix B. Proofs forSection 6
First,weproveTheorem8.
Proof[ofTheorem8]Bythedefinitionofp′,foranyrealizationφ,wehavep′(φ) := max{p(φ),1/|Φ |2}/Z,
V
whereZ isanormalizing constant. Wehave
Z = max{p(φ),1/|Φ |2} ≤ (p(φ)+1/|Φ |2)= 1+1/|Φ |. (8)
V V V
φ X∈ΦV φ X∈ΦV
18ADAPTIVE COMBINATORIAL MAXIMIZATION: BEYOND APPROXIMATE GREEDY POLICIES
Therefore, foranyrealization φ,
|Φ |
p′(φ) = max{p(φ),1/|Φ |2}/Z ≥ p(φ)/Z ≥ p(φ)· V .
V
|Φ |+1
V
Hence,
|Φ |+1 |Φ |+1
c (π,p) = p(φ)|E(π,φ)| ≤ V p′(φ)|E(π,φ)| = V c (π,p′).
avg avg
|Φ | |Φ |
V V
φ X∈ΦV φ X∈ΦV
Therefore,
|Φ |+1
c (π,p) ≤ V ·c (π,p′) ≤ 2c (π,p′).
avg avg avg
|Φ |
V
ByTheorem2,
β (f,p′) nQ
c (π,p′)≤ π ·c (π∗,p′)+1 log( )+2.
avg γs (f,p′) avg η
n,k !
Therefore,
β (f,p′) nQ
c (π,p) ≤ 2 π ·c (π∗,p′)+1 log( )+4.
avg γs (f,p′) avg η
n,k !
Bytheassumptions ofthetheorem,π∗ hasheightatmost|Φ |. Inotherwords,foranyrealiza-
V
tionφ,
|E(π∗,φ)| ≤ |Φ |. (9)
V
Inaddition, notethat
Z = max{p(φ),1/|Φ |2} ≥ p(φ) = 1.
V
φ X∈ΦV φ X∈ΦV
Therefore, for any realization φ, if p(φ) ≥ 1/|Φ |2, then p′(φ) − p(φ) = p(φ)/Z − p(φ) ≤ 0.
V
Otherwise,p(φ) < 1/|Φ |2,whichimpliesp′(φ) = 1/(|Φ |2Z)≤ 1/|Φ |2. Weconclude thatfor
V V V
anyrealization φ,p′(φ)−p(φ) ≤ 1/|Φ |2. CombiningthiswithEq.(9),weget
V
c (π∗,p′)−c (π∗,p) = |E(π∗,φ)|(p′(φ)−p(φ))
avg avg
φ X∈ΦV
≤ |Φ |(p′(φ)−p(φ))
V
φ X∈ΦV
1
≤ |Φ | = 1.
V |Φ |2
V
φ X∈ΦV
Weconclude thatc (π∗,p′) ≤ c (π∗,p)+1. Therefore,
avg avg
β (f,p′) nQ
c (π,p) ≤ 2 π ·(c (π∗,p)+1)+1 log( )+4.
avg γs (f,p′) avg η
n,k !
Thiscompletes theproof.
Next,westateandproveLemma11.
19WEITZMAN SABATO
Lemma11 Let π∗ be a policy such that f (E(π∗,φ),φ) = 1 for all φ ∈ Φ and such that
p V
c (π∗,p)isminimal. Then,theheightofπ∗ isatmost|Φ |.
avg V
Proof Forsimplicity, wedenotebelowπ = π∗. Weprovethelemmabyshowingthatπ eliminates
atleastonerealization fromitsversionspace ineachquery. Thisdirectly impliesthattheheightof
π isatmost|Φ |−1.
V
We first assume that π is deterministic, and later extend the analysis to randomized policies.
Assume for contradiction that there is apossible selection by π such that the resulting observation
doesnotremoveanyrealization fromtheversionspace. Formally,assumethatthereissomepartial
realization ψ, realization φ ∼ ψ with some non-zero probability according to p, and an element
v ∈V,suchthatduringtherunofπ underφ,thepartialrealizationthatisobservedisψ,π(ψ) = v,
andforanyrealization φ′ ∼ψ itholdsthatφ′(v) =φ(v).
Letπ′ thepolicydefinedasfollows: Foranypartialrealization ψ′:
π(ψ′∪{(v,φ(v))}) ψ ⊆ ψ′,
π′(ψ′) =
(π(ψ′) otherwise.
By the definition of π′, it holds that c (π′,p) < c (π,p). Thus, to reach a contradiction, it
avg avg
sufficestoshowthatf (E(π′,φ′),φ′)= 1forallφ′ ∈Φ .
p V
Let φ′ ∈ Φ . If ψ is not observed during the run of π′ under φ′, then the behavior of π′ is
V
equivalent tothatofπ. Therefore,
f (E(π′,φ′),φ′)= f (E(π,φ′),φ′)= 1.
p p
Otherwise, ψ is observed during the run of π′ under φ′. In this case, until ψ is observed, the
behavior ofπ′ under φ′ isthesameasitsbehavior under φ. Therefore, atsometimeduring therun
of π′, ψ is the partial realization that has been observed so far. By the definition of π′, it does not
select v atthis time, and then behaves the sameas π. Hence, E(π′,φ′) = E(π,φ′)\{v}. Forany
deterministic policyπ¯,wedefineψ = {(v,φ′(v)) |v ∈ E(π¯,φ′)}tobethepartialrealization that
π¯
π¯ observesunderφ′ justbeforeitterminates. Itholdsthatψ π = ψ π′ ∪{(v,φ′(v))}.
Sinceψ isobservedduringtherunofπ′ underφ′,itholdsthatψ ⊆ ψ π′. Hence,anyrealization
thatisconsistent withψ π′ isalsoconsistent withψ. Weassumedthatanyrealization thatisconsis-
tentwithψisalsoconsistentwith{(v,φ′(v))}. Therefore,thesameholdsforψ π′. Weconcludethat
anyrealization thatisconsistent withψ π′ isalsoconsistent withψ π. Sincef p(E(π,φ′),φ′) = 1,it
holdsthatφ′ istheonlyrealization thatisconsistent withψ . Hence,φ′ isalsotheonlyrealization
π
that is consistent with ψ π′. Bythe definition of f p, itholds that f p(E(π′,φ′),φ′) = 1. Thus, in all
cases,f (E(π′,φ′),φ′) = 1forallφ′ ∈ Φ ,asclaimed. Thisshowstherequiredcontradiction.
p V
Lastly, if π is not deterministic, then for any realization θ of the randomness of π, it holds
π
that π given θ is deterministic. Assume for contradiction that there exists some realization θ
π π
of the randomness of π that has a non-zero probability, such that in some query, π under θ does
π
not eliminate any realization from its version space. Since π under θ is a deterministic policy,
π
similarly to the proof above, it follows that π under θ is not an optimal policy. Since θ has a
π π
positiveprobability, itfollowsthatπ isnotanoptimalpolicy,againresulting inacontradiction and
provingtheclaim.
20