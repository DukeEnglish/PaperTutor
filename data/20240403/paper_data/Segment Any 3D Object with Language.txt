Segment Any 3D Object with Language
Seungjun Lee⋆1, Yuyang Zhao⋆2, and Gim Hee Lee2
1 Korea University
2 National University of Singapore
https://cvrp-sole.github.io
window
laptop
sink
(a) “CanIwashmyhands?” (b) “Devicetoplaygame.” (c) “Iwannaseeoutside.”
Fig.1: Qualitative results when querying SOLE with various language in-
structions. SOLE is highly generalizable and can segment corresponding instances
with various language instructions, including but not limited to (a) visual questions,
(b) attributes description, and (c) functional description.
Abstract. In this paper, we investigate Open-Vocabulary 3D Instance
Segmentation (OV-3DIS) with free-form language instructions. Earlier
works that rely on only annotated base categories for training suffer
from limited generalization to unseen novel categories. Recent works
mitigate poor generalizability to novel categories by generating class-
agnostic masks or projecting generalized masks from 2D to 3D, but dis-
regard semantic or geometry information, leading to sub-optimal per-
formance. Instead, generating generalizable but semantic-related masks
directlyfrom3Dpointcloudswouldresultinsuperioroutcomes.Inthis
paper, we introduce Segment any 3D Object with LanguagE (SOLE),
whichisasemanticandgeometric-awarevisual-languagelearningframe-
work with strong generalizability by generating semantic-related masks
directly from 3D point clouds. Specifically, we propose a multimodal
fusion network to incorporate multimodal semantics in both backbone
and decoder. In addition, to align the 3D segmentation model with var-
ious language instructions and enhance the mask quality, we introduce
threetypesofmultimodalassociationsassupervision.OurSOLEoutper-
forms previous methods by a large margin on ScanNetv2, ScanNet200,
and Replica benchmarks, and the results are even closed to the fully-
supervised counterpart despite the absence of class annotations in the
training.Furthermore,extensivequalitativeresultsdemonstratethever-
satility of our SOLE to language instructions.
Keywords: Open-set · 3D Instance Segmentation · Multimodal
⋆ Equal Contribution.
4202
rpA
2
]VC.sc[
1v75120.4042:viXra2 S. Lee et al.
1 Introduction
3D instance segmentation which aims at detecting, segmenting and recognizing
objectinstancesin3Dscenesisoneofthecrucialtasksfor3Dsceneunderstand-
ing. Effective and generalizable 3D instance segmentation has great potential in
real-world applications, including but not limited to autonomous driving, aug-
mented reality (AR), and virtual reality (VR). Owing to its significance, 3D
instance segmentation has achieved remarkable success in the computer vision
community [20,53,60]. Previous 3D instance segmentation models mainly fo-
cus on the closed-set setting, where the training and testing stages share the
same categories. However, novel and unseen categories with various shapes and
semantic meaning are inevitable in real-world applications. Failure to segment
such instances drastically narrows the scope of application.
In view of the strong limitations of closed-set setting, open-set 3D instance
segmentation (OS-3DIS) that aims at detecting and segmenting unseen classes
based on instructions is introduced and investigated in the community. Most
of the works [10,26,44] leverage category names or descriptions as segmen-
tation instructions, which is also termed as open-vocabulary 3D instance seg-
mentation (OV-3DIS). The early approaches [10,11,66] split categories in each
datasetintobase andnovel set.Onlybasecategoriesareavailableinthetraining
stage, but the model is expected to segment novel categories during inference.
Due to the lack of novel classes during training, these methods easily overfit to
the base categories, and thus yielding sub-optimal performance on novel cate-
gories. In addition, they suffer from severe performance degradation when they
are evaluated on the data with different distributions. In this regards, recent
works [26,40,58,64] explore more generalizable OV-3DIS with the help of 2D
foundationmodels[45,50,76].Specifically,[26,58]learnclass-agnostic3Dmasks
from mask annotation and then project the point clouds to 2D images to ob-
tain class labels from foundation models. [40,64] predict 2D instances with 2D
open-vocabulary instance segmentation model [76] and fuse them to obtain 3D
predictions. However, class-agnostic masks and 2D projected masks ignore the
semanticandgeometryinformationinthemaskgeneration,respectively,leading
tothesub-optimalperformance.Incontrast,wedirectlypredictsemantic-related
masks from 3D point clouds, yielding better and more generalizable 3D masks.
In this paper, we propose SOLE: Segment any 3D Object with LanguagE
to circumvent the above-mentioned issues for OV-3DIS. To realize generalizable
open-set3Dinstancesegmentation,ourSOLErequirestwomainattributes:gen-
erating and classifying 3D masks directly from 3D point clouds, and responsive
to free-form language instructions. The 3D segmentation network is required to
be aligned with language instructions to directly segment and classify instances
from point clouds. To this end, we build a multimodal fusion network with two
main techniques: 1) Point-wise CLIP features obtained from pre-trained multi-
modal2Dsemanticsegmentationmodel[14]areincorporatedtothebackboneto
enhance the generality of the model; 2) Cross-modality decoder is introduced to
integrateinformationfromlanguage-domainfeatures,facilitatingtheeffectivefu-
sionofmultimodalknowledge.Furthermore,weimprovethegeneralizationabil-SOLE 3
ity across various scene and language instructions with a novel visual-language
learning framework, training the 3D segmentation network with three types of
multimodal associations: 1) Mask-visual association, 2) mask-caption associa-
tion and 3) mask-entity association. These associations improve the language
instruction alignment and enhance the 3D mask prediction with more abundant
semantic information.
Equipped with a multimodal fusion network and three types of multimodal
associations,ourvisual-languagelearningframework(SOLE)outperformsprevi-
ousworksbyalargemarginonScanNetv2[8],ScanNet200[52]andReplica[56]
benchmarks. Furthermore, SOLE can respond to free-form queries, including
but not limited to questions, attributes description, and functional description
(Fig. 1 and Fig. 4). In summary, our contributions are as follows:
– We propose a visual-language learning framework for OV-3DIS, SOLE. A
multimodal fusion network is designed for SOLE, which can directly predict
semantic-related masks from 3D point clouds with multimodal information,
leading to high-quality and generalizable segments.
– Weproposethreetypesofmultimodalassociationstoimprovethealignment
between3Dsegmentationmodelwiththelanguage.Theassociationsimprove
the mask quality and the response ability to language instructions.
– SOLEachievesstate-of-the-artresultsonScanNetv2,Scannet200andReplica
benchmarks, and the results are even close to the fully-supervised counter-
part. In addition, extensive qualitative results demonstrate that SOLE can
respond to various language questions and instructions.
2 Related Work
Closed-Set 3D Instance Segmentation. 3D instance segmentation aims at
detecting, segmenting and recognizing the object instances in 3D scenes. Previ-
ous works [4,12,19,20,23,30,37,43,53,57,60,61,65,68,72] mainly consider the
closed-set setting, where the training and testing categories are the same. These
methodsvaryinfeatureextractionanddecodingprocess.Withthedevelopment
of transformer models, mask prediction becomes a more efficient and effective
waythantraditionalboxdetectiondecodingapproaches.Mask3D[53]samplesa
fixednumberofpointsacrossthesceneasqueries,andthendirectlypredictsthe
finalmaskswithattentionmechanism,achievingbetterresults.However,closed-
setmethodslackthecapabilitytohandletheunseencategoriesregardlessofthe
decoding approaches and thus hindering their application in the real world.
Open-Vocabulary 2D Segmentation. Owing to the recent success of large-
scale vision-language models [1,5,15,28,50,69,70], notable achievements have
been made in open-vocabulary or zero-shot 2D segmentation [6,9,14,16,18,32,
33,35,41,51,62,63,71,75]. The common key idea is to leverage 2D mulitmodal
foundation models [28,50] for the transfer of image-level embeddings to the
pixel-level downstream tasks. LSeg [33], OpenSeg [14], and OVSeg [35] align
pixel-level or mask-level visual features to text features from foundation model4 S. Lee et al.
foropen-vocabularysemanticsegmentation.OtherworkssuchasX-Decoder[78],
FreeSeg[49]andSEEM[79]suggestmoreunified-frameworkforopen-vocabulary
segmentation, include instance, panoptic, and referring segmentation.
Open-Vocabulary3DSceneUnderstanding.Theremarkablesuccessachieved
inopen-vocabulary2Dsegmentation(OV-2DS)hasspurredseveralendeavorsin
open-vocabulary 3D segmentation. However, the techniques in OV-2DS cannot
bedirectlytransferredtothe3Ddomainduetothelackof3Dmultimodalfoun-
dationmodel.Consequently,researchersproposetoalign2Dimagesand3Dpoint
cloudsandthuslifting2Dfoundationmodelsto3D.Foropen-vocabulary3Dse-
manticsegmentation,[2,10,17,25,27,46,54,55]constructtask-agnosticpoint-wise
feature representations from 2D foundation models [50], and then use these fea-
turestoquerytheopen-vocabularyconceptswithin3Dscene.Theseworksfocus
purely on transferring semantic information from 2D to 3D, limiting the appli-
cation for instance-level recognition tasks. In this regard, open-vocabulary 3D
instance segmentation (OV-3DIS) [11,26,40,44,58,64] is introduced to detect
and segment instances of various categories in 3D scenes. PLA [10] and its vari-
ants[11,66]splitthetrainingcategoriesintobaseandnovelclasses,andtrainthe
model only with base class annotation. OpenMask3D [58] and OpenIns3D [26]
learn class-agnostic 3D masks from mask annotations and then use the corre-
sponding2Dimagestoobtainclasslabelsfromfoundationmodels.Morerecently,
researchersalsoinvestigatedirectliftingof2Dpredictionsfrom2Dinstanceseg-
mentation model [76] to 3D without training [40,64]. Previous works greatly
prompttheimprovementofOV-3DIS.However,theresultsarestillfarfromsat-
isfactory due to the poor semantic generalization ability and low-quality mask
prediction.Consideringthelimitationsofpreviouswork,wesignificantlyimprove
OV-3DIS by designing a visual-language learning framework with a multimodal
network and various multimodal associations.
3 Method
Objective. The goal of open-vocabulary 3D instance segmentation (OV-3DIS)
withfree-formlanguageinstructionsisdefinedasfollows:Givena3Dpointcloud
P∈RM×C, the corresponding 2D images I and the instance-level 3D masks m,
we aim to train a 3D instance segmentation network without ground-truth class
annotations. During inference, given a text prompt q, the trained 3D instance
segmentation network must detect and segment corresponding instances.
Mask-PredictionBaseline.Webuildourframeworkonthetransformer-based
3D instance segmentation model Mask3D [53], which treats the instance seg-
mentation task as the mask prediction paradigm. Specifically, the transformer
decoders with mask queries are used to segment instances. Given N queries se-
q
lected from the scene, cross attention is used to aggregate information from the
pointcloudstoinstancequeries.Afterseveraldecoderlayers,N queriesbecome
q
N masks with corresponding semantic prediction. During training, Hungarian
q
matching[31]isadoptedtomatchandtrainthemodelwithgroundtruthlabelsSOLE 5
Training Inference
3D Point Cloud
3D Point Cloud
Mask Proposals
Feature Backbone
Per-Point Multimodal
CLIP Features Mask Features Associations
C
Per-Point
CLIP Features
Feature Backbone Pooling Transformer Transformer
Decoder Decoder
C CMD
MLP Masked
Pooling Cross Self Attention FeC ala tus rs e s FeM aa tus rk es ProM pa os sk als Pooling
Modality
PooC ling D (e Cc Mo Dde )r Cross Attention FC ea ap tuti ro en s Soft Geometric Mean FP eo ao tule red s
C CMD Cross Attention FeP ao ti un rt e s
Instance Queries
C: Concat Instance Queries
Fig.2: Overall framework of SOLE. SOLE is built on transformer-based instance
segmentation model with multimodal adaptations. For model architecture, backbone
features are integrated with per-point CLIP features and subsequently fed into the
cross-modality decoder (CMD). CMD aggregates the point-wise features and textual
featuresintotheinstancequeries,finallysegmentingtheinstances,whicharesupervised
by multimodal associations. During inference, predicted mask features are combined
with the per-point CLIP features, enhancing the open-vocabulary performance.
andmasks.Attheinferencestage,N maskswithcorrectsemanticclassification
q
results are taken as the final outputs. Our SOLE leverages the mask prediction
paradigm with transformer-based architecture, where the model is only trained
with masks without ground truth labels to achieve generalizable OV-3DIS.
Overview. The overall architecture of SOLE is illustrated in Fig. 2. To real-
izeopen-vocabularyinstancesegmentationwithfree-formlanguageinstructions,
weimprovethetransformer-basedinstancesegmentationmodelwithmultimodal
information:point-wiseCLIPfeaturesinthebackbone(Sec.3.1)andtextualin-
formationinthedecoder(Sec.3.2).Furthermore,toachievebettergeneralization
abilitywithoutgroundtruthclasslabels,weconstructthreetypesofmultimodal
associations on target instances: mask-visual association, mask-caption associa-
tion and mask-entity association to train SOLE. Equipped with the multimodal
framework and associations, our SOLE can effectively segment instances given
various language prompts.
3.1 Backbone Feature Ensemble
Initializing the backbone with pre-trained model [29,73,74] is an efficient and
effective way to improve the performance on the downstream tasks, especially
when the downstream data is not in abundance. For 3D open-set setting, lever-
aging 2D foundation model is crucial due to the limited 3D data. We thus fol-
low [46] to project pre-trained visual features of 2D images to 3D point clouds
based on the camera pose. To maintain the fine-grained and generalizable fea-6 S. Lee et al.
Mask-Visual Association Mask-Caption Association
3D Point Cloud 2D Frames
"A wooden desk with a chair
in a room."
Image Captioning
Model
"A blue chair sitting next to a
desk in a wooden office"
OpenSeg Masked Pooling
GT Masks Mask-Entity Association
Noun Phrases Captions
"a w "o ao cd he an + i rd "e xs k 0" .2 x 0.7 0 0. .7 2 " "a a w cho ao id r"en desk" "A woode inn ad e ros ok m w .i "th a chair
"a room+ " x 0.1 0.1 "a room"
"a blue chair" x 0.8 0.8 Noun Phrases
"a des+ +k" x 0.1 0.1 " "a a b dl eu se k "chair" "A d b el su ke ic nh aa i wr os oit dti en ng on fe fx ict e t "o a
"a wooden office" x 0.1 0.1 "a wooden office"
Per-Point CLIP Features
Fig.3: Three types of multimodal association instance. For each ground truth
instance mask, we first pool the per-point CLIP features to obtain Mask-Visual Asso-
ciation fMVA.Subsequently,fMVA isfedintoCLIPspacecaptioningmodeltogenerate
captionandcorrespondingtextualfeaturefMCAforeachmask,termedasMask-Caption
Association. Finally, noun phrases are extracted from mask caption and the embed-
dingsofthemareaggregatedviamultimodalattentiontogetMask-Entity Association
fMEA.ThethreemultimodalassociationsareusedforsupervisingSOLEtoacquirethe
ability to segment 3D objects with free-form language instructions.
tures, we leverage OpenSeg [14] as the 2D backbone. These features contain
visual information in the CLIP [50] feature space, which is aligned with textual
information.
Since CLIP feature space mainly focuses on semantic information due to the
image-level contrastive training, leveraging the projected features solely cannot
achieve optimal performance on instance segmentation. To this end, we train a
3D instance segmentation backbone and combine its features fb ∈ RM×D with
the projected 2D CLIP features fp ∈RM×C:
˜fb =concat(fp,fb)∈RM×(C+D), (1)
DeCap's Inference Pipeline
where M denotes the number of points while D and C denote the feature di-
mension of 3D ins It ma agn eC L EcI nPe cods eregmentaDetCiaopn back"Ab wooodne inn e ad e ros oka
m
w .ni "th da chatirhe projected 2D features,
respectively. Note that features of different resolutions are extracted from the
3D backbone and respectively incorporated with the 2D CLIP features. As il-
lustrateOduri'sn CaFptiigon. G2e,netrhateings Paimpeleinepooling strategy with 3D backbone is adopted to
CLIP features, alV Miis agu skan l f- eD aio tmnura eigsn the resolution. Finally, incorporated point-wise features
with multiple resolutions are fDeeCdapinto cr"Ao wsoosde innm ad e ros ook m wd.i "tha a clhiatiry decoder.
3.2 Cross Modality Decoder (CMD)
Projected 2D CLIP features provide generalizable visual information but the
language information is not explicitly integrated, limiting the responsive ability
to language instructions. To circumvent this issue, we introduce Cross ModalitySOLE 7
Decoder (CMD) to incorporate textual information in the decoding process of
our framework. Specifically, each CMD module contains three attention layers.
Instance queries first extract visual information from the CLIP-combined back-
bone features ˜fb. CLIP textual features are then projected to key and value in
the second attention layer, incorporating the text domain knowledge. During
the training, CLIP textual features are obtained from the caption features of
each target mask, fMCA ∈ RNc×C (See Sec. 3.3 for details), whereas, during
the inference, it can be the description of the query instance or other form of
language instructions, such as visual questions or functional attributes. Finally,
self-attention is applied to the instance queries to further improve the represen-
tation. By fusing the multimodal knowledge from CLIP with multi-level CMD
as the decoder, SOLE can respond to various language instructions with high-
quality results.
3.3 Vision-Language Learning
We do vision-language learning to enable our SOLE towards generalizable OV-
3DIS.Torespondeffectivelytovariouslanguageinstructions,weleveragemulti-
modal information stemming from target mask annotations, to supervise the
segmentation network. Specifically, three types of supervision in hierarchical
granularity are proposed: 1) mask-visual association, 2) mask-caption associ-
ation and 3) mask-entity association.
Mask-Visual Association (MVA). Using the correspondence between 2D
images and 3D point clouds, we can get the instance-level CLIP visual features
fMVA ∈RNm×C by averaging the per-point CLIP features within the N
m
target
instancemasksm=[m ,m ,...,m ].Theinstance-levelCLIPvisualfeatures
1 2 Nm
can serve as the supervision to indirectly align the 3D segmentation model to
CLIP textual space. In addition, as the intermediate representation between 3D
point cloud and language, the mask-visual association is also the basis for the
following two fine-grained associations.
Mask-CaptionAssociation(MCA).DespitebeingintheCLIPfeaturespace,
mask-visual association is not an accurate and precision language supervision.
Instead, directly supervising the model with language instructions would yield
better results. Due to the strong generalization ability of CLIP [50], text gener-
ationfromCLIPspaceiswidelyinvestigatedinthecommunity[34,42,59].Since
the instance-level CLIP visual features fMVA in the mask-visual association is
in CLIP visual space, we can feed them to the CLIP space caption generation
model (DeCap [34]) to obtain the mask captions c=[c ,c ,...,c ]. The mask
1 2 Nm
captions are then fed into CLIP textual model to extract the mask-caption as-
sociation fMCA. This association represents the language information for the
instance masks, used in CMD to fuse textual information during the training.
Mask-Entity Association (MEA). Although mask-caption association can
provide detailed language descriptions for both semantics and geometry, it may
be ambiguous for specific categories. As shown in the example of Fig. 3. The8 S. Lee et al.
mask caption for a desk is “A wooden desk with a chair in a room”. Such cap-
tion can lead to the confusion of the model between the chair and the desk, or
misinterpretation of the two instances as a single one. It is therefore important
to introduce a more fine-grained visual-language association for better semantic
learning.
Since the objects are commonly the nouns in the caption, we can extract
the entity-level descriptions for the nouns and match them with the instances.
Specifically, as illustrated in Fig. 3, we first extract all the noun phrases e for
i
eachmaskcaptionc andobtainthetextfeatureofeachnounphrasefromCLIP
i
text encoder T as below:
E(c i)=e
i
=[e 1,e 2,...,e Ni], f ie =T(e i)∈RN ei×C, (2)
e
where E(·) denotes the NLP tool to extract noun phrases and Ni denotes the
e
number of nouns obtained from mask caption c . The entities can be matched
i
to the mask in either a hard or soft manner. Intuitively, the most similar entity
can be viewed as the mask label. However, there are two main issues with such
a hard matching. First, the generated caption and the similarity results may
not be accurate, leading to wrong supervision. Second, although the entity is
correct, hard matching ignores the geometry information in the context and
thus impairing the responsive ability to language instructions. To this end, we
proposeasoftmatchingtogetmask-entityassociationbymultimodalattention.
Specifically, the aggregated entity feature for the i-th mask fMEA is obtained
i
based on the attention map A between mask feature and entity features:
c,e
(cid:88)N ei exp(cid:0) fMVA·fek(cid:1)
fMEA =A ·fe = i i ·fek, (3)
i c,e i (cid:80)N ei exp(cid:0) fMVA·fej(cid:1) i
k j i i
where fMVA denotes the mask-visual association feature for i-th mask, and fek
i i
is the CLIP textual feature for k-th entity in the i-th mask caption. With the
aggregated entity feature, the 3D mask can be aligned with a specific instance
category.
3.4 Training and Inference
Training. The three types of multimodal associations are effective supervision
to learn a generalizable 3D instance segmentation model. We follow the mask
predictionparadigmtotrainthesegmentationmodel,whichmatchestheground
truth instances with the predicted masks via Hungarian matching [31]. Specif-
ically, the matching cost between i-th predicted mask and j-th ground truth
instance is calculated as:
C(i,j)=−λ
(cid:0) p(cid:0) fm·fMVA(cid:1) +p(cid:0) fm·fMCA(cid:1) +p(cid:0) fm·fMEA(cid:1)(cid:1)
MMA i j i j i j
(4)
+λ L (i,j)+λ L (i,j),
dice dice BCE BCE
wherep(·,·)denotesthesoftmaxprobabilitybetweenthepredictedinstanceand
the ground truth. After matching the masks and ground truth instances, theSOLE 9
model is trained with the combination of mask and semantic loss. Specifically,
all three types of associations are used to semantically supervise the model. For
eachassociation,wefollow[77]tousethecombinationoffocalloss[36]anddice
loss, which can ensure the segmentation result for each class is independently
generated.ThesemanticmultimodalassociationlossLj forj-thgroundtruth
MMA
mask is:
(cid:88)(cid:16) (cid:17)
Lj = L (pˆa ,ya)+L (pˆa ,ya) , (5)
MMA focal σ(j) j dice σ(j) j
a
where a∈{MVA,MCA,MEA} denotes threetypes ofassociations andya is the
j
binary label for matching. pˆa =sigmoid(fm ·fa) is the semantic probability
σ(j) σ(j) j
between the prediction with the association a. The overall training loss is the
combination of mask loss and semantic loss:
L=
1 (cid:88)Nm(cid:16)
λ Lj +λ L (mˆ ,m )+λ L (mˆ ,m
)(cid:17)
, (6)
N MMA MMA dice dice σ(j) j BCE BCE σ(j) j
m
j
where mˆ denotes matched predicted mask with j-th target mask.
σ(j)
Inference.Duringinference,wecombinethevisualfeaturefromCLIPwiththe
predictedmaskfeaturetoachievebettergeneralizationability.Specifically,after
obtaining the 3D masks, per-point CLIP features are pooled within the mask.
The pooled CLIP feature and mask feature are then fed into the classifier to
obtain the respective classification probability p(fm) and p(fp), and the final
probability is yielded by soft geometric mean between them:
p=max(p(fm),p(fp))τ ·min(p(fm),p(fp))1−τ, (7)
where τ is the exponent to increase confidence, which we set to 0.667 in this
paper. For benchmark evaluation, we use CLIP textual features of all category
namesastheclassifier.Forrespondingtootherlanguageinstructions,weusethe
CLIP textual feature of corresponding language instruction as binary classifier.
4 Experiments
4.1 Experimental Setting
Datasets. We evaluate SOLE on the popular scene understanding datasets:
ScanNetv2 [8], ScanNet200 [52] and Replica [56] in both closed-set and open-set
3D instance segmentation tasks. ScanNetv2 [8] is a popular indoor point cloud
datasetwith18instanceclasses,where“otherfurniture” classisdisregardeddue
to its ambiguity. ScanNet200 [52] is a fine-grained annotated version of Scan-
Netv2 that contains 200 classes of head (66 categories), common (68 categories)
andtail(66categories)subsets.ForScanNetv2andScanNet200,weevaluatethe
closed-set setting and the hierarchical open-set setting. Replica [56] is a high-
quality synthetic dataset annotated with 48 instance categories. Following [58],
we evaluate on eight scenes in Replica for open-set instance segmentation, in-
cluding {office0, office1, office2, office3, office4, room0, room1 and room2.}10 S. Lee et al.
Table 1: The comparison of closed-set 3D instance segmentation setting on
ScanNetv2 [8]. SOLE is compared with class-split methods, mask-training methods
andthefull-supervisedcounterpart(upperbound).SOLEoutperformsalltheOV-DIS
methods and achieves competitive results with the fully-supervised model.
Method B/N AP AP AP voxel size
50 25
PLA [10] 10/7 - 21.9 - 2cm
RegionPLC [66] 10/7 - 32.3 - 2cm
Lowis3D [11] 10/7 - 31.2 - 2cm
OpenIns3D [26] -/7 - 27.9 42.6 2cm
SOLE w 4cm voxel size -/7 31.6 58.5 72.5 4cm
SOLE w/o text sup -/7 41.1 57.1 65.9 2cm
SOLE (ours) -/7 52.3 72.4 81.7 2cm
PLA [10] 8/9 - 25.1 - 2cm
RegionPLC [66] 8/9 - 32.2 - 2cm
Lowis3D [11] 8/9 - 38.1 - 2cm
OpenIns3D [26] -/9 - 19.5 27.9 2cm
SOLE w 4cm voxel size -/9 31.9 57.5 73.6 4cm
SOLE w/o text sup -/9 42.9 59.6 70.7 2cm
SOLE (ours) -/9 50.4 68.3 75.2 2cm
OpenIns3D [26] -/17 - 28.7 38.9 2cm
SOLE w 4cm voxel size -/17 30.8 52.5 70.9 4cm
SOLE w/o text sup -/17 35.0 50.2 60.2 2cm
SOLE (ours) -/17 44.4 62.2 71.4 2cm
Mask3D [53] (fully sup) 17/- 55.2 73.7 83.5 2cm
Implementation Details. Following the Mask3D [53], we adopt Minkowski-
UNet [7] as backbone. The feature backbone extracts point features in 5 scales,
while 4 layers of transformer decoder iteratively refine the instance queries. Our
model is trained for 600 epochs with AdamW [39] optimizer. The learning rate
is set to 1×10−4 with cyclical decay. In training, we set λ = 20.0, λ =
MMA dice
2.0 and λ = 5.0 as the loss weight.
BCE
Baselines. We compare SOLE mainly with two streams of existing works on
OV-3DIS: class-split methods [10,11,66] and mask-training methods [26,58].
Class-split methods [10,11,66] split the training categories into base and novel
categories. All the mask annotations and base category labels are used to train
themodel.Whencomparedwiththesemethods,weonlytrainourmodelonthe
mask annotation and compare with them on the split novel categories. Mask-
training methods [26,58] train class-agnostic mask generator with mask anno-
tation and get the semantic prediction with 2D foundation models. The setting
of mask-training methods is similar to ours, and we directly compare with them
on all the categories.SOLE 11
Table 2: The comparison of closed-set 3D instance segmentation setting on
ScanNet200 [52]. SOLE is compared with OpenMask3D [58] on the overall segmen-
tationperformanceandoneachsubset.SOLEsignificantlyoutperformsOpenMask3D
on five out of the six evaluation metrics.
Method AP AP AP AP AP AP
50 25 head com tail
OpenMask3D [58] 15.4 19.9 23.1 17.1 14.1 14.9
SOLE (ours) 20.1 28.1 33.6 27.5 17.6 14.1
(+4.7) (+8.2) (+10.5) (+10.4) (+3.5) (-0.8)
Mask3D [53] 26.9 36.2 41.4 39.8 21.7 17.9
EvaluationMetric.Averageprecision(AP)ofdifferentIoUthresholdsisadopted
astheevaluationmetric,includingAPunder25%,50%IoUandtheaverageAP
from 50% to 95% IoU.
4.2 Comparison with Previous Methods
Closed-Set 3D Instance Segmentation. We compare our SOLE with both
class-split methods [10,11,66] and mask-training methods [26,58] on the closed-
set 3Dinstance segmentation setting. Whencomparedwith class-split methods,
we evaluate on the novel categories. From the comparison results in Tab. 1, we
can make the following observations. First, SOLE significantly outperforms the
class-split methods by a large margin, even without using the base class labels.
Second, although OpenIns3D [26] leverages the same mask annotation with
ourSOLE,wesignificantlysurpassesitby33.5%and32.5%onAP andAP ,
50 25
respectively. Third, our SOLE can even achieve competitive performance with
the fully-supervised counterpart (44.4% v.s. 55.2% in AP) despite not using the
class labels. Finally, we provide two variants of SOLE to further verify our ef-
fectiveness. SOLE w 4cm voxel size leverages 4cm voxel size instead of 2cm as
in previous works. Smaller voxel size can save the memory requirements and
speed up the model with the loss of precision. Despite using a small voxel size,
SOLE w 4cm voxel size can still outperform previous works by a large margin.
Furthermore, we verify that the effectiveness of our framework is not limited to
the caption model and NLP tools by conducting experiments without any ad-
ditional textual information, i.e. SOLE w/o text sup. In this experiment, mask-
caption association and mask-entity association is removed since the caption is
not available. The model can still achieve the state-of-the-art performance de-
spiteonlytrainedwithmask-visualassociation.Additionally,wecompareSOLE
withOpenMask3D[58]onScanNet200[52]inTab.2.Thetwomethodsareeval-
uated on the overall segmentation performance and the performance on each of
the three subsets. SOLE outperforms OpenMask3D [58] on five out of six met-
rics and achieves comparable performance on the tail classes. The results on
ScanNet200 [52] further demonstrate the effectiveness of our framework.
Hierarchical and Cross-Domain Open-Set 3DIS. To evaluate the gener-
alization capability of our work, we compare our SOLE with OpenMask3D [58]12 S. Lee et al.
Table 3: The comparison of hierarchical open-set 3D instance segmenta-
tion setting on ScanNetv2 [8]→ScanNet200 [52].SOLEiscomparedwithOpen-
Mask3D [58] on both base and novel classes and achieves the best results.
Novel Classes Base Classes All Classes
Method
AP AP AP AP AP AP AP AP
50 25 50 25 tail
OpenMask3D [58] 11.9 15.2 17.8 14.3 18.3 21.2 12.6 11.5
SOLE (ours) 19.1 26.2 30.7 17.4 26.2 32.1 18.7 12.5
(+8.8) (+11.0) (+12.9) (+3.1) (+7.9) (+10.9) (+6.1) (+1.0)
Table 4: The comparison of open-set 3D instance segmentation setting on
ScanNet200 [52]→Replica [56]. SOLE outperforms OpenMask3D [58] on all the
evaluation metrics.
Method Mask Training AP AP AP
50 25
OpenMask3D [58] ScanNet200 [52] 13.1 18.4 24.2
SOLE (ours) ScanNet200 [52] 24.7 31.8 40.3
(+11.6) (+13.4) (+16.1)
in open-set setting, using Scannet200 [52] and Replica [56] datasets. For Scan-
Net200, both models are trained with mask annotations in ScanNetv2 [8]. Fol-
lowing[58],53classesthataresemanticallyclosetotheScanNet,aregroupedas
“Base”. The remaining 147 classes are grouped as “Novel”. Both in-distribution
(“base”) and out-of-distribution (“novel”) classes are reported in Tab. 3. Our
SOLE outperforms OpenMask3D [58] by a large margin on both base and novel
classes. Furthermore, to verify the generalization ability of SOLE when both
domain shift and category shift exist, we compare our framework with Open-
Mask3D on the synthetic Replica benchmark [56]. Models are trained on the
annotatedmasksonScanNet200.AsshowninTab.4,ourmethodfurthershows
superior robustness on more out-of-distribution data from Replica, achieving
+11.6% improvement in AP score compared to OpenMask3D.
4.3 Ablation Studies and Analysis
Inthissection,weconductseveralablationstudiestovalidateourdesignchoices.
All of the studies are evaluated on ScanNetv2 [8] dataset.
Multimodel Fusion Network. In Tab. 5, we conduct component analysis
on multimodal fusion network, validating the effectiveness of backbone feature
ensemble and Cross-Modality Decoder (CMD). As for the backbone feature en-
semble, leveraging projected 2D CLIP features fp (first row) as only backbone
canhavebettersemanticinformationbutlackthe3Dgeometrydetectionability,
leadingtopoorsemanticrecognitionability.Incontrast,solelyusing3Dinstance
backbone feature fb (second row) cannot inherit the generalizable semantic in-
formation, resulting in sub-optimal performance. Combining the two features
(thirdrow)canmakefulluseofgeneralizedsemanticinformationwhilelearningSOLE 13
Table 5: Component analysis on multimodal fusion network. 3D instance
backbone feature fb, projected 2D backbone feature fp, and Cross Modality Decoder
(CMD) are investigated separately.
No. fp fb CMD AP AP AP voxel size
50 25
1 ✓ ✓ 18.7 36.4 58.1 4cm
2 ✓ ✓ 25.4 47.0 66.0 4cm
3 ✓ ✓ ✓ 30.8 52.5 70.9 4cm
4 ✓ ✓ 42.8 60.5 68.9 2cm
5 ✓ ✓ ✓ 44.4 62.2 71.4 2cm
Table 6: Component Analysis on multimodal associations. Mask-visual asso-
ciation fMVA, mask-caption association fMCA and mask-entity association fMEA are
studied on ScanNetv2 [8].
No. fMVA fMCA fMEA AP AP AP voxel size
50 25
1 ✓ 24.5 42.0 56.0 4cm
2 ✓ 30.4 53.0 68.7 4cm
3 ✓ 32.1 53.8 70.0 4cm
4 ✓ ✓ 29.1 50.9 66.8 4cm
5 ✓ ✓ 30.3 53.7 70.4 4cm
6 ✓ ✓ ✓ 30.8 52.5 70.9 4cm
good geometry detection ability from 3D masks, yielding optimal results. Ad-
ditionally, Cross Modality Decoder (CMD) can further enhance the ability to
understand language instructions, improving AP by 1.6%.
Multimodal Associations.Weanalyzethecomponentsofmultimodalassoci-
ations (fMVA, fMCA, and fMEA) in Tab. 6, reporting the scores of various com-
binations on ScanNetv2 [8] with 4cm voxel size. We have the following observa-
tions.First,usinganyofmultimodalassociationscanalreadyachievesignificant
performance, outperforming previous state-of-the-art method (OpenIns3D [26])
with larger voxel size (lower resolution). Second, among the three types of as-
sociations, mask-entity association fMEA is the most effective one on evaluation
metrics since it can align the masks with specific categories. Third, when com-
biningfMEA withtheothertwoassociations,themodelsuffersfromperformance
degradationonAPandAP whiletheperformanceimprovesonAP .Thisob-
50 25
servation shows that mask-visual association and mask-caption association can
help semantic learning but impair mask accuracy. To this end, we further illus-
tratequalitativeresultsinFig.5.Givenafree-formlanguageinstructioninstead
of category name, e.g., “I wanna see outside”, the model only using mask-entity
association cannot segment the correct instance (Fig. 5a) while the model in-
corporating the other associations (Fig. 5b and Fig. 5c) can. Therefore, despite
slightly impairing the performance on benchmark, mask-visual association and
mask-caption association are crucial to recognizing free-form language instruc-
tions, benefiting the applications in real-world scenarios.14 S. Lee et al.
chairs
tv
vending machine
“Iwanttowatchmovie.” “Chairsnearbythewindow.” “I’mhungry.”
toilet
trash bin
“Throwingawaythegarbage.” “Brownfurnitures.” “PlaceIcanpee.”
Fig.4: Qualitative results from SOLE.OurSOLEdemonstratesopen-vocabulary
capabilitybyeffectivelyrespondingtofree-formlanguagequeries,includingvisualques-
tions, attributes description and functional description.
tv window window
(a) fMEA (b) fMVA,fMCA (c) fMVA,fMCA,fMEA
Fig.5: Qualitative analysis on multimodal associations. Given the free-form
language instruction, “I wanna see outside.”, SOLE trained only with fMEA captures
the wrong object ((a)), whereas it segments the related object when fMVA and fMCA
are additionally given as the supervision ((b), (c)).
Qualitative Results. In Fig. 1 and Fig. 4, we present qualitative results,
demonstrating that SOLE is capable of processing free-form language queries,
including but not limited to visual questions, attributes description, and func-
tional description.
5 Conclusion
In this paper, we propose a novel framework, SOLE, for open-vocabulary 3D
instance segmentation with free-form language instructions. SOLE contains a
multimodalfusionnetworkandissupervisedwiththreetypesofmultimodalas-
sociations,aimingataligningthemodelwithvariousfree-formlanguageinstruc-
tions. Our framework outperforms previous methods by a large margin on three
benchmarks while achieving competitive performance with the fully-supervised
counterpart. Moreover, extensive qualitative results demonstrate the versatility
of our SOLE to language instructions.SOLE 15
Appendix
In this appendix, we provide more implementation details, introduce additional
evaluationonfree-formlanguageinstructions,andconductmorequalitativeand
quantitative analysis.
– More implementation details are provided in Sec. A.
– SOLE is evaluated on 3D visual grounding task to verify the responsive
ability to free-form language instructions in Sec. B
– Analysis of CLIP visual features are provided in Sec. C.
– More qualitative results about the mask caption and segmentation results
are shown in Sec. D.
A Implementation Details
SegmentationNetwork.FollowingMask3D[53],weusethetransformer-based
mask-prediction paradigm to obtain instance mask and semantic features. The
masks are initialized from object queries and regressed by attention layers. For
each 3D point cloud scene, we use farthest point sampling [48] to get 150 points
as object queries. After getting masks from the segmentation model, we use
DBSCAN [13] to break down non-contiguous masks into smaller, spatially con-
tiguousclusterstoimprovethemaskquality.Themaximumdistanceandneigh-
borhood points number are set to 0.95 and 1, respectively.
Text Information Generation and Extraction. To effectively generate a
caption for each mask, we use a caption model in CLIP space, i.e., DeCap [34].
DeCapisalightweighttransformermodeltogeneratecaptionsfromCLIPimage
embedding. It contains a 4-layer Transformer with 4 attention heads as the
languagemodelandthevisualembeddingisobtainedfromthepre-trainedViT-
L/14 CLIP model. We feed the mask features that are average pooled from
the projected CLIP visual features into the DeCap model to obtain the mask
caption.Thenthecaptionisintegratedintothetextprompt“a{}inascene.” to
betteralignwithourdata,e.g.“abluechairinascene.”.Withthemaskcaption,
noun phrases are extracted by the NLP library, TextBlob [38] and spaCy [22],
to get the mask-entity association.
B 3D Visual Grounding
To further verify the effectiveness of SOLE on various language instructions,
we conduct experiments on 3D visual grounding benchmark ScanRefer [3]. 3D
visual grounding aims at localizing 3D objects with free-form text descriptions.
Therefore,wequerySOLEwitheachtextpromptintheScanRefervalidationset
to get the corresponding instance, and then 3D bounding box is obtained from
the instance masks. The performance is evaluated on the matching accuracy
with IoU over 0.25 (ACC@25) and 0.5 (ACC@50).16 S. Lee et al.
Table7:ResultsonScanRefer[3]for3Dvisualgroundingtask.SOLEachieves
the best performance on generalist models with weak supervision.
Method Type Supervision ACC@25ACC@50
OCRand [3] Full 30.0 29.8
VoteRand [3,47] Full 10.0 5.3
SCRC [24] Specialist Full 18.7 6.5
One-stage [67] Full 20.4 9.0
ScanRefer [3] Full 41.2 27.4
3D-LLM (flamingo) [21] Full 21.2 —
Generalist
SOLE Weak 25.2 22.6
Baselines.WecompareSOLEwithfivespecialistbaselinemodelsandonegen-
eralist model. Specialist models mean that the models are designed and trained
for3Dvisualgroundingonlywhilegeneralistmodelsaremodelsthatcanaddress
other tasks such as instance segmentation with class names. For the specialist
models,OCRand[3]usesanoraclewithgroundtruthboundingboxesofobjects,
and selects a random box that matches the object category. VoteRand [3,47]
leverages the pre-trained VoteNet [47] to predict bounding boxes and randomly
select a box of the correct semantic class. SCRC [24] and One-stage [67] are 2D
approaches with 3D extension using back-projection. ScanRefer [3] uses a pre-
trained VoteNet [47] with a trained GRU to select a matching bounding box.
Forthegeneralistmodel,3D-LLMdirectlypredictsthelocationofthebounding
box corresponding to the text description via large language model. Note that
exceptforOCRandandVoteRandwheretrainingisnotrequired,theotherfour
baseline models are trained or fine-tuned on the ScanRefer training set. Differ-
ently, SOLE only use mask annotation of ScanNetv2 [8] training set and the
text description in ScanRefer are totally ignored during training.
Results. As shown in Tab. 7, with only mask annotation, SOLE outperforms
the generalist model 3D-LLM by 4% on ACC@25. In addition, SOLE can
achievecompetitiveperformancewiththefully-supervisedspecialistcounterpart
onACC@50(22.6%v.s.27.4%).Suchresultsdemonstratethestronggeneraliza-
tionabilityandtheeffectivenessinrespondingtofree-formlanguageinstructions
of our framework.
C Analysis of CLIP Visual Feature
CLIPvisualfeaturesplayanimportantroleinthegeneralizationabilityofSOLE.
In this section, we further analyze the effectiveness of CLIP visual features in
the backbone feature ensemble and inference ensemble.SOLE 17
(a) K-Meansclusteringoffp (b) K-Meansclusteringoffb (c) K-Meansclusteringof˜fb
Fig.6: K-means clustering of different backbone features. Different colors de-
note different clusters.
Table 8: Analysis on classification probability ensemble. Results are reported
on the ScanNetv2 [8] dataset in 2cm voxel size.
Component AP AP AP voxel size
50 25
w.o. Ensemble 42.2 58.6 66.9 2cm
hard geometric mean 43.7 61.1 70.1 2cm
soft geometric mean (ours) 44.4 62.2 71.4 2cm
C.1 Backbone Feature Ensemble
As shown in Tab. 5 of the main paper, solely using 3D instance backbone fea-
turefb orprojectedCLIPvisualfeaturefp cannotachievethebestperformance.
3D backbone features lack the generalized semantic information while projected
CLIP visual features lack the location and geometry information. To further
verify the effectiveness of the backbone feature ensemble, we visualize the clus-
tering results of different features in Fig. 6. In Fig. 6a, all chairs are clustered
together (the green cluster), showing that the projected CLIP features contain
good semantic information but cannot detect the instances. In Fig. 6b, different
instanceswithinonecategorycanbeidentified,e.g.,chairsareinthreeclusters.
However, the semantic generalization ability is degraded. As in the highlighted
red circle, the trash can is detected by projected CLIP visual features (Fig. 6a)
butmisclassifiedtoachairclusterwhenonlyusing3Dbackbone(Fig.6b).Com-
pared with using the two features separately, SOLE combines the two features
and thus achieves better semantic generalization ability (segmentation of the
trashcan)andsegmentationperformance(chairsareclusteredintosixclusters).
The visualization results further demonstrate the effectiveness of the backbone
feature ensemble.
C.2 Inference Ensemble
During inference, we combine the CLIP visual features with the predicted mask
feature to achieve better generalization ability. Specifically, after obtaining the
3Dmasks,per-pointCLIPfeaturesarepooledwithinthemask.ThepooledCLIP
feature and mask feature are then fed into the classifier to obtain the respective18 S. Lee et al.
a microwave oven sitting inside of a small kitchen a trash can with a microwave on top of it a black refrigerator freezer sitting inside of a kitchen
a bed with white pillows and a large white comforter on top a black suitcase with a piece of luggage sitting on a floor a brown chair sitting in front of a desk with a black table
a box of various items sitting on a cluttered place a painting hanging on the wall of a living room a kitchen area with a sink and counter top near cabinets
a com ip nu ft re or n m t oo fn ti wto or cs oit mtin pg u o ten r sa desk a desk with aa n l da p ot to hp e, r c io tem mp suter, papers a plant with a white pot sitting on a table
Fig.7: Qualitative examples for mask captions.Generatedcaptionscontainthe
semantic, appearance, and geometric relationship of the corresponding object.
classification probability p(fm) and p(fp), and the final probability is yielded
by the ensemble of them. In Tab. 8, we compare three options to ensemble the
class probabilities of 3D segmentation model and CLIP model. “w.o. Ensemble”
denotesonlyusingthepredictionof3Dsegmentationmodel,and“hardgeometric
mean” refers to the standard geometric mean, formulated as p(fm)τ ·p(fp)1−τ.
Our method, “soft geometric mean” (Eq. 7), shows the best results among the
ensemble methods, demonstrating the effectiveness of dynamically fusing the
prediction from two models. However, as shown in the first row, SOLE already
achieves competitive performance even without utilizing the CLIP prediction,
furtherdemonstratingthestronggeneralizationabilityofourmultimodalfusion
network.
D Qualitative Results
Visualization for Mask Captions. We provide the visualization for different
masks and corresponding generated captions in Fig. 7. The generated caption
contains the semantic information of the 3D object as well as the location forSOLE 19
better3Dsegmentation.Asshownintheexamples,morethanonenounsexistin
thecaptionandthusweaggregateallthenounphraseswithattentionmechanism
in mask-entity association.
Visualization for Segmentation Results.Morequalitativesegmentationre-
sultsareshownintheprojectpage.OurSOLEcaneffectivelyrespondtovarious
free-form language instructions.
References
1. Alayrac,J.B.,Donahue,J.,Luc,P.,Miech,A.,Barr,I.,Hasson,Y.,Lenc,K.,Men-
sch, A., Millican, K., Reynolds, M., et al.: Flamingo: a visual language model for
few-shotlearning.AdvancesinNeuralInformationProcessingSystems35,23716–
23736 (2022) 3
2. Chen, B., Xia, F., Ichter, B., Rao, K., Gopalakrishnan, K., Ryoo, M.S., Stone,
A., Kappler, D.: Open-vocabulary queryable scene representations for real world
planning. In: 2023 IEEE International Conference on Robotics and Automation
(ICRA). pp. 11509–11522. IEEE (2023) 4
3. Chen, D.Z., Chang, A.X., Nießner, M.: Scanrefer: 3d object localization in rgb-d
scans using natural language. In: European conference on computer vision. pp.
202–221. Springer (2020) 15, 16
4. Chen, S., Fang, J., Zhang, Q., Liu, W., Wang, X.: Hierarchical aggregation for 3d
instancesegmentation.In:ProceedingsoftheIEEE/CVFInternationalConference
on Computer Vision. pp. 15467–15476 (2021) 3
5. Cherti,M.,Beaumont,R.,Wightman,R.,Wortsman,M.,Ilharco,G.,Gordon,C.,
Schuhmann, C., Schmidt, L., Jitsev, J.: Reproducible scaling laws for contrastive
language-image learning. In: Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition. pp. 2818–2829 (2023) 3
6. Cho, S., Shin, H., Hong, S., An, S., Lee, S., Arnab, A., Seo, P.H., Kim, S.: Cat-
seg: Cost aggregation for open-vocabulary semantic segmentation. arXiv preprint
arXiv:2303.11797 (2023) 3
7. Choy,C.,Gwak,J.,Savarese,S.:4dspatio-temporalconvnets:Minkowskiconvolu-
tionalneuralnetworks.In:ProceedingsoftheIEEE/CVFconferenceoncomputer
vision and pattern recognition. pp. 3075–3084 (2019) 10
8. Dai,A.,Chang,A.X.,Savva,M.,Halber,M.,Funkhouser,T.,Nießner,M.:Scannet:
Richly-annotated3dreconstructionsofindoorscenes.In:ProceedingsoftheIEEE
conference on computer vision and pattern recognition. pp. 5828–5839 (2017) 3,
9, 10, 12, 13, 16, 17
9. Ding,J.,Xue,N.,Xia,G.S.,Dai,D.:Decouplingzero-shotsemanticsegmentation.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 11583–11592 (2022) 3
10. Ding,R.,Yang,J.,Xue,C.,Zhang,W.,Bai,S.,Qi,X.:Pla:Language-drivenopen-
vocabulary 3d scene understanding. arXiv preprint arXiv:2211.16312 (2022) 2, 4,
10, 11
11. Ding,R.,Yang,J.,Xue,C.,Zhang,W.,Bai,S.,Qi,X.:Lowis3d:Language-driven
open-worldinstance-level3dsceneunderstanding.arXivpreprintarXiv:2308.00353
(2023) 2, 4, 10, 11
12. Dong,S.,Lin,G.,Hung,T.Y.:Learningregionalpurityforinstancesegmentation
on 3d point clouds. In: European Conference on Computer Vision. pp. 56–72.
Springer (2022) 320 S. Lee et al.
13. Ester, M., Kriegel, H.P., Sander, J., Xu, X., et al.: A density-based algorithm for
discovering clusters in large spatial databases with noise. In: kdd. vol. 96, pp.
226–231 (1996) 15
14. Ghiasi,G.,Gu,X.,Cui,Y.,Lin,T.Y.:Scalingopen-vocabularyimagesegmentation
withimage-levellabels.In:EuropeanConferenceonComputerVision.pp.540–557.
Springer (2022) 2, 3, 6
15. Girdhar, R., El-Nouby, A., Liu, Z., Singh, M., Alwala, K.V., Joulin, A., Misra,
I.: Imagebind: One embedding space to bind them all. In: Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.15180–
15190 (2023) 3
16. Gu, X., Lin, T.Y., Kuo, W., Cui, Y.: Open-vocabulary object detection via vision
and language knowledge distillation. arXiv preprint arXiv:2104.13921 (2021) 3
17. Ha, H., Song, S.: Semantic abstraction: Open-world 3d scene understanding from
2d vision-language models. arXiv preprint arXiv:2207.11514 (2022) 4
18. He, S., Guo, T., Dai, T., Qiao, R., Shu, X., Ren, B., Xia, S.T.: Open-vocabulary
multi-label classification via multi-modal knowledge transfer. In: Proceedings of
the AAAI Conference on Artificial Intelligence (2023) 3
19. He, T., Shen, C., Van Den Hengel, A.: Dyco3d: Robust instance segmentation of
3d point clouds through dynamic convolution. In: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition. pp. 354–363 (2021) 3
20. He, T., Yin, W., Shen, C., van den Hengel, A.: Pointinst3d: Segmenting 3d in-
stances by points. In: European Conference on Computer Vision. pp. 286–302.
Springer (2022) 2, 3
21. Hong, Y., Zhen, H., Chen, P., Zheng, S., Du, Y., Chen, Z., Gan, C.: 3d-llm: In-
jecting the 3d world into large language models. Advances in Neural Information
Processing Systems 36, 20482–20494 (2023) 16
22. Honnibal, M., Montani, I.: spaCy 2: Natural language understanding with Bloom
embeddings,convolutionalneuralnetworksandincrementalparsing(2017),toap-
pear 15
23. Hou, J., Dai, A., Nießner, M.: 3d-sis: 3d semantic instance segmentation of rgb-
d scans. In: Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition. pp. 4421–4430 (2019) 3
24. Hu,R.,Xu,H.,Rohrbach,M.,Feng,J.,Saenko,K.,Darrell,T.:Naturallanguage
object retrieval. In: Proceedings of the IEEE conference on computer vision and
pattern recognition. pp. 4555–4564 (2016) 16
25. Huang, C., Mees, O., Zeng, A., Burgard, W.: Visual language maps for robot
navigation. In: 2023 IEEE International Conference on Robotics and Automation
(ICRA). pp. 10608–10615. IEEE (2023) 4
26. Huang, Z., Wu, X., Chen, X., Zhao, H., Zhu, L., Lasenby, J.: Openins3d:
Snap and lookup for 3d open-vocabulary instance segmentation. arXiv preprint
arXiv:2309.00616 (2023) 2, 4, 10, 11, 13
27. Jatavallabhula, K.M., Kuwajerwala, A., Gu, Q., Omama, M., Chen, T., Maalouf,
A., Li, S., Iyer, G., Saryazdi, S., Keetha, N., et al.: Conceptfusion: Open-set mul-
timodal 3d mapping. arXiv preprint arXiv:2302.07241 (2023) 4
28. Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q., Sung, Y.H.,
Li, Z., Duerig, T.: Scaling up visual and vision-language representation learning
with noisy text supervision. In: International conference on machine learning. pp.
4904–4916. PMLR (2021) 3
29. Jia, M., Tang, L., Chen, B.C., Cardie, C., Belongie, S., Hariharan, B., Lim, S.N.:
Visualprompttuning.In:EuropeanConferenceonComputerVision.pp.709–727.
Springer (2022) 5SOLE 21
30. Jiang, L., Zhao, H., Shi, S., Liu, S., Fu, C.W., Jia, J.: Pointgroup: Dual-set point
grouping for 3d instance segmentation. In: Proceedings of the IEEE/CVF confer-
ence on computer vision and Pattern recognition. pp. 4867–4876 (2020) 3
31. Kuhn, H.W.: The hungarian method for the assignment problem. Naval research
logistics quarterly 2(1-2), 83–97 (1955) 4, 8
32. Kuo, W., Cui, Y., Gu, X., Piergiovanni, A., Angelova, A.: F-vlm: Open-
vocabulary object detection upon frozen vision and language models. arXiv
preprint arXiv:2209.15639 (2022) 3
33. Li, B., Weinberger, K.Q., Belongie, S., Koltun, V., Ranftl, R.: Language-driven
semantic segmentation (2022) 3
34. Li, W., Zhu, L., Wen, L., Yang, Y.: Decap: Decoding clip latents for zero-shot
captioning via text-only training (2023) 7, 15
35. Liang, F., Wu, B., Dai, X., Li, K., Zhao, Y., Zhang, H., Zhang, P., Vajda, P.,
Marculescu, D.: Open-vocabulary semantic segmentation with mask-adapted clip.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 7061–7070 (2023) 3
36. Lin, T.Y., Goyal, P., Girshick, R., He, K., Dollár, P.: Focal loss for dense object
detection.In:ProceedingsoftheIEEEinternationalconferenceoncomputervision.
pp. 2980–2988 (2017) 9
37. Liu, J., He, T., Yang, H., Su, R., Tian, J., Wu, J., Guo, H., Xu, K., Ouyang, W.:
3d-queryis:Aquery-basedframeworkfor3dinstancesegmentation.arXivpreprint
arXiv:2211.09375 (2022) 3
38. Loria, S., et al.: textblob documentation. Release 0.15 2(8), 269 (2018) 15
39. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101 (2017) 10
40. Lu,S.,Chang,H.,Jing,E.P.,Boularias,A.,Bekris,K.:Ovir-3d:Open-vocabulary
3dinstanceretrievalwithouttrainingon3ddata.In:ConferenceonRobotLearn-
ing. pp. 1610–1620. PMLR (2023) 2, 4
41. Ma, C., Yang, Y., Wang, Y., Zhang, Y., Xie, W.: Open-vocabulary semantic seg-
mentation with frozen vision-language models. arXiv preprint arXiv:2210.15138
(2022) 3
42. Mokady, R., Hertz, A., Bermano, A.H.: Clipcap: Clip prefix for image captioning.
arXiv preprint arXiv:2111.09734 (2021) 7
43. Ngo, T.D., Hua, B.S., Nguyen, K.: Isbnet: a 3d point cloud instance segmenta-
tion network with instance-aware sampling and box-aware dynamic convolution.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 13550–13559 (2023) 3
44. Nguyen, P.D., Ngo, T.D., Gan, C., Kalogerakis, E., Tran, A., Pham, C., Nguyen,
K.:Open3dis:Open-vocabulary3dinstancesegmentationwith2dmaskguidance.
arXiv preprint arXiv:2312.10671 (2023) 2, 4
45. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V.,
Fernandez,P.,Haziza,D.,Massa,F.,El-Nouby,A.,etal.:Dinov2:Learningrobust
visual features without supervision. arXiv preprint arXiv:2304.07193 (2023) 2
46. Peng, S., Genova, K., Jiang, C., Tagliasacchi, A., Pollefeys, M., Funkhouser, T.,
etal.:Openscene:3dsceneunderstandingwithopenvocabularies.In:Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
815–824 (2023) 4, 5
47. Qi,C.R.,Litany,O.,He,K.,Guibas,L.J.:Deephoughvotingfor3dobjectdetec-
tion in point clouds. In: proceedings of the IEEE/CVF International Conference
on Computer Vision. pp. 9277–9286 (2019) 1622 S. Lee et al.
48. Qi,C.R.,Yi,L.,Su,H.,Guibas,L.J.:Pointnet++:Deephierarchicalfeaturelearn-
ing on point sets in a metric space. In: NeurIPS (2017) 15
49. Qin, J., Wu, J., Yan, P., Li, M., Yuxi, R., Xiao, X., Wang, Y., Wang, R., Wen,
S.,Pan,X.,etal.:Freeseg:Unified,universalandopen-vocabularyimagesegmen-
tation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. pp. 19446–19455 (2023) 4
50. Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agarwal,S.,Sastry,G.,
Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from
naturallanguagesupervision.In:Internationalconferenceonmachinelearning.pp.
8748–8763. PMLR (2021) 2, 3, 4, 6, 7
51. Rao,Y.,Zhao,W.,Chen,G.,Tang,Y.,Zhu,Z.,Huang,G.,Zhou,J.,Lu,J.:Dense-
clip:Language-guideddensepredictionwithcontext-awareprompting.In:Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
pp. 18082–18091 (2022) 3
52. Rozenberszki,D.,Litany,O.,Dai,A.:Language-groundedindoor3dsemanticseg-
mentationinthewild.In:EuropeanConferenceonComputerVision.pp.125–141.
Springer (2022) 3, 9, 11, 12
53. Schult, J., Engelmann, F., Hermans, A., Litany, O., Tang, S., Leibe, B.: Mask3d
for3dsemanticinstancesegmentation.arXivpreprintarXiv:2210.03105(2022) 2,
3, 4, 10, 11, 15
54. Shafiullah, N.M.M., Paxton, C., Pinto, L., Chintala, S., Szlam, A.: Clip-
fields: Weakly supervised semantic fields for robotic memory. arXiv preprint
arXiv:2210.05663 (2022) 4
55. Shah,D.,Osiński,B.,Levine,S.,etal.:Lm-nav:Roboticnavigationwithlargepre-
trainedmodelsoflanguage,vision,andaction.In:ConferenceonRobotLearning.
pp. 492–504. PMLR (2023) 4
56. Straub, J., Whelan, T., Ma, L., Chen, Y., Wijmans, E., Green, S., Engel, J.J.,
Mur-Artal, R., Ren, C., Verma, S., et al.: The replica dataset: A digital replica of
indoor spaces. arXiv preprint arXiv:1906.05797 (2019) 3, 9, 12
57. Sun, J., Qing, C., Tan, J., Xu, X.: Superpoint transformer for 3d scene instance
segmentation. In: Proceedings of the AAAI Conference on Artificial Intelligence
(2023) 3
58. Takmaz, A., Fedele, E., Sumner, R.W., Pollefeys, M., Tombari, F., Engelmann,
F.: Openmask3d: Open-vocabulary 3d instance segmentation. arXiv preprint
arXiv:2306.13631 (2023) 2, 4, 9, 10, 11, 12
59. Tewel, Y., Shalev, Y., Schwartz, I., Wolf, L.: Zerocap: Zero-shot image-to-text
generationforvisual-semanticarithmetic.In:ProceedingsoftheIEEE/CVFCon-
ference on Computer Vision and Pattern Recognition. pp. 17918–17928 (2022) 7
60. Vu, T., Kim, K., Luu, T.M., Nguyen, T., Yoo, C.D.: Softgroup for 3d instance
segmentation on point clouds. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. pp. 2708–2717 (2022) 2, 3
61. Wu, Y., Shi, M., Du, S., Lu, H., Cao, Z., Zhong, W.: 3d instances as 1d kernels.
In: European Conference on Computer Vision. pp. 235–252. Springer (2022) 3
62. Xu,J.,DeMello,S.,Liu,S.,Byeon,W.,Breuel,T.,Kautz,J.,Wang,X.:Groupvit:
Semantic segmentation emerges from text supervision. In: Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.18134–
18144 (2022) 3
63. Xu, J., Liu, S., Vahdat, A., Byeon, W., Wang, X., De Mello, S.: Open-vocabulary
panopticsegmentationwithtext-to-imagediffusionmodels.In:Proceedingsofthe
IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2955–
2966 (2023) 3SOLE 23
64. Yan,M.,Zhang,J.,Zhu,Y.,Wang,H.:Maskclustering:Viewconsensusbasedmask
graph clustering for open-vocabulary 3d instance segmentation. arXiv preprint
arXiv:2401.07745 (2024) 2, 4
65. Yang,B.,Wang,J.,Clark,R.,Hu,Q.,Wang,S.,Markham,A.,Trigoni,N.:Learn-
ingobjectboundingboxesfor3dinstancesegmentationonpointclouds.Advances
in neural information processing systems 32 (2019) 3
66. Yang, J., Ding, R., Wang, Z., Qi, X.: Regionplc: Regional point-language
contrastive learning for open-world 3d scene understanding. arXiv preprint
arXiv:2304.00962 (2023) 2, 4, 10, 11
67. Yang, Z., Gong, B., Wang, L., Huang, W., Yu, D., Luo, J.: A fast and accurate
one-stage approach to visual grounding. In: Proceedings of the IEEE/CVF inter-
national conference on computer vision. pp. 4683–4693 (2019) 16
68. Yi, L., Zhao, W., Wang, H., Sung, M., Guibas, L.J.: Gspn: Generative shape pro-
posal network for 3d instance segmentation in point cloud. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3947–
3956 (2019) 3
69. Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., Wu, Y.:
Coca: Contrastive captioners are image-text foundation models. arXiv preprint
arXiv:2205.01917 (2022) 3
70. Yuan,L.,Chen,D.,Chen,Y.L.,Codella,N.,Dai,X.,Gao,J.,Hu,H.,Huang,X.,
Li,B.,Li,C.,etal.:Florence:Anewfoundationmodelforcomputervision.arXiv
preprint arXiv:2111.11432 (2021) 3
71. Zabari, N., Hoshen, Y.: Semantic segmentation in-the-wild without seeing any
segmentation examples. arXiv preprint arXiv:2112.03185 (2021) 3
72. Zhang,C.,Wan,H.,Liu,S.,Shen,X.,Wu,Z.:Pvt:Point-voxeltransformerfor3d
deep learning. arxiv 2021. arXiv preprint arXiv:2108.06076 (2021) 3
73. Zhao, H., Luo, H., Zhao, Y., Wang, P., Wang, F., Shou, M.Z.: Revisit parameter-
efficienttransferlearning:Atwo-stageparadigm.arXivpreprintarXiv:2303.07910
(2023) 5
74. Zhao,H.H.,Wang,P.,Zhao,Y.,Luo,H.,Wang,F.,Shou,M.Z.:Sct:Asimplebase-
line for parameter-efficient fine-tuning via salient channels. International Journal
of Computer Vision pp. 1–19 (2023) 5
75. Zhou, C., Loy, C.C., Dai, B.: Extract free dense labels from clip. In: European
Conference on Computer Vision. pp. 696–712. Springer (2022) 3
76. Zhou, X., Girdhar, R., Joulin, A., Krähenbühl, P., Misra, I.: Detecting twenty-
thousandclassesusingimage-levelsupervision.In:EuropeanConferenceonCom-
puter Vision. pp. 350–368. Springer (2022) 2, 4
77. Zhou, Z., Lei, Y., Zhang, B., Liu, L., Liu, Y.: Zegclip: Towards adapting clip for
zero-shotsemanticsegmentation.In:ProceedingsoftheIEEE/CVFConferenceon
Computer Vision and Pattern Recognition. pp. 11175–11185 (2023) 9
78. Zou, X., Dou, Z.Y., Yang, J., Gan, Z., Li, L., Li, C., Dai, X., Behl, H., Wang, J.,
Yuan,L.,etal.:Generalizeddecodingforpixel,image,andlanguage.In:Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
pp. 15116–15127 (2023) 4
79. Zou, X., Yang, J., Zhang, H., Li, F., Li, L., Wang, J., Wang, L., Gao, J., Lee,
Y.J.: Segment everything everywhere all at once. Advances in Neural Information
Processing Systems 36 (2024) 4