GOV-REK: Governed Reward Engineering Kernels for Designing Robust
Multi-Agent Reinforcement Learning Systems
AshishRana1,MichaelOesterle1,andJannikBrinkmann1
1InstituteforEnterpriseSystems,UniversityofMannheim,Germany
ashish.rana@students.uni-mannheim.de
michael.oesterle@uni-mannheim.de
jannik.brinkmann@uni-mannheim.de
Abstract
For multi-agent reinforcement learning systems (MARLS),
the problem formulation generally involves investing mas-
sive reward engineering effort specific to a given problem.
However,thiseffortoftencannotbetranslatedtootherprob-
lems; worse, it gets wasted when system dynamics change
drastically. This problem is further exacerbated in sparse
reward scenarios, where a meaningful heuristic can assist in
thepolicyconvergencetask.WeproposeGOVernedReward
EngineeringKernels(GOV-REK),whichdynamicallyassign
reward distributions to agents in MARLS during its learning
stage. We also introduce governance kernels, which exploit
theunderlyingstructureineitherstateorjointactionspacefor Figure 1: The systematic reward configuration exploration
assigning meaningful agent reward distributions. During the with exponentially increasing training timestep budgets for
agent learning stage, it iteratively explores different reward learning.
distribution configurations with a Hyperband-like algorithm
to learn ideal agent reward models in a problem-agnostic
manner. Our experiments demonstrate that our meaningful
reward priors robustly jumpstart the learning process for ExperienceActor-Critic(SEAC)[16]methodshavesuccess-
effectivelylearningdifferentMARLproblems. fullyimprovedthelearningbehaviorinreinforcementlearn-
ing(RL)systemsagainstsparseproblems.However,theseap-
Keywords: Cooperative Multi-Agent Systems, Sparse
proachesimprovesampleefficiencybyintroducingnovelties
Reinforcement Learning, Robust Multi-Agent Systems,
likeattention,curiosity,andexperiencesharingaspartofthe
RewardShaping
learning process rather than directly influencing agent moti-
vations. Our approach proposes an intermediary governance
Introduction layerbetweenagentsandtheenvironment,whichdirectlyin-
centivizesagentswithadditionalrewardsselectedinanauto-
InteractionsformulatedinMARLSaremoreintricatetolearn
matedmannertoimprovethebaselineRLalgorithms.Inour
forcomplicatedscenariosatincreasingscalesandlargenum-
proposedapproach,wedefine‘governancekernels’foreach
bersofagents[43,21].Thisproblemissignificantlyexacer-
agent, which are the reward distribution signals that gener-
batedinmulti-agentsparsescenariosasagentsolutiontrajec-
ate similar additional rewards for similar states or joint ac-
tories explode exponentially at large scales, and the reward
tionsdependingontheMARLproblem.Also,foraplethora
signals are not dense enough to assist the learning process
ofmachinelearning(ML)anddeeplearning(DL)usecases,
[18,32].Manyapproacheshavepreviouslyexploreddesign-
the hyperparameter optimization (HPO) algorithms [8] like
ingrewardsystemsforsingle-agentandmulti-agentsettings
Successive Halving (SH) and Hyperband [29] in a problem-
[38, 19], either by exploiting domain knowledge or utiliz-
agnosticmannerhaveconsistentlyfoundexemplaryhyperpa-
ingimitationlearning[12,23]andethic-basedshapinglearn-
rameterconfigurations[48,7].Similarly,ourproposedGOV-
ing novelties [46]. However, the reward engineering effort
REK framework finds suitable reward models for agents by
is problem-specific and often does not generalize to other
searchingoverdifferentgovernancekernelconfigurationsit-
MARLproblems[11,13,33].Hence,definingeffectiveand
eratively,wherearepeatedHyperband-likealgorithmgener-
robust reward signals for agents in MARL tasks in an auto-
ates the search plan to learn ideal governance kernels. Fur-
mated and problem-agnostic manner is a challenging prob-
ther,thegovernancekernelsareusedintheGOV-REKframe-
lem.
workasfundamentalmodulesthatcanbesuperimposedand
Previously, architectural novelties introduced in Agent
mutatedacrossdifferentmodeltrainingroundstoincentivize
Temporal Attention (ATA) [47], Random Network Distilla-
agent cooperation. Figure 1 highlights our algorithm quali-
tion (RND) [14], Never Give Up (NGU) [2], and Shared
tatively, which executes for four rounds of SH and succes-
ExtendedAbstractacceptedinthe23rdInternationalConferenceon sively prunes out relatively worst reward configurations by
AutonomousAgentsandMulti-AgentSystems(AAMAS2024) factor η=3. Further, as the round increases, the budget for
4202
rpA
1
]AM.sc[
1v13110.4042:viXraEnvironment Policy (0,0) A1 drops P near A2 (0,0) A2 drops P at G
{S, R} GOV-REK
P A2 A1
Governance {S, R՛} AAggenetnsts (0,0) (4,4) A1
R՛ = R + G
G: Updates G P/G
A.i GOV-REK Approach
A2
(0,0) A1
(4,4) (4,4)
P A2 (0,0) C.i Agent A1 encouraged to drop package P near agent A2, and not
(4,4)
aim for goal G, post that agent A2 drops the package at goal G
B.i Soft Cooperation Constraint: Exploring
G only nearby regions for high reward signals (0,0)A1’s two paths to G with P (0,0)A1’s two paths to G with P
2D Grid Environment (4,4) A1 A2 P A2
A1
A.ii Multi-objective A1
Reward Shaping (MORS) G G
(0,0) (4,4) A1 A1
Policy
{S, R}
(4,4) (4,4)
B.ii No Cooperation Constraint: Exploiting the
Environment MORS AAggenetnsts domain knowledge rewards for goal completion C.ii Agent A1 has many paths to drop package P at goal G, A2 not used
, and in resource constrained setting agent A1 or A2 fails to deliver P
R՛ = R + F {S, R՛} F: Fixed
Figure2:ThefunctionaldescriptionofthegovernancelayerforthesparsepackagedeliverycooperativeMARLproblem.
each learning model increases exponentially by η=3 factor distributions.Further,wealsodemonstratethatourproposed
during successive rounds. This increasing budget produces methodsuccessfullyappliestodifferentMARLproblemtask
modelswithhigherfidelitieswitheachsuccessiveroundwith configurationsinanautomatedmanner.
better configurations, and we also mutate a fraction of our
selectedbestconfigurations. RelatedWork
Further Figure 2 demonstrates the functional working of
DefiningagoodMARLgoalisachallengingobjective,where
the governance layer for the sparse package delivery prob-
expected agent rewards need to be jointly maximized in a
lem, where the agent-specific reward is altered to R to R´
i i completelyobservableorpartiallyobservablesetting.Thede-
byradialgovernancekernels.Ourproposedgovernancedoes
finedrewardsforagivengoalmuststabilizetheagent’slearn-
notchangestateS andactionA optionsfortheagents,butthe
i i ingbehaviorwhileadaptingtochangingdynamicsintheen-
policythatchoosesstateS andactionA mightchangewith
i i vironment.Thestabilityconvergencerequirementensuresthe
the newly added rewards. For benchmarking we also high-
stationarypolicyconvergence,andtheadaptabilityconstraint
lightacomparativeapproach,Multi-ObjectiveRewardShap-
ensures no performance detriment with evolving policies of
ing(MORS),whichassignsmanuallydesigneddenserewards
other agents, provided agents are rational [10, 15]. Further,
foreverysubtaskcompletion,likepackageexchangebetween
for training MARLS, the Centralized Training Centralized
agents,andproceedingclosertowardthegoalwithdensedi-
Execution (CTCE) paradigm optimizes the joint policy for
rectional reward gradients by exploiting domain knowledge
agents together, and the Centralized Training Decentralized
[13]. This governance layer is aware of all the rewards the
Execution (CTDE) paradigm agents maintain separate poli-
agents receive under a complete or partial observability set-
ciesbutexchangeinformationduringtraining[50,27,21].In
ting.Thegovernancecoordinatorexclusivelyinfluenceseach
this study, we train our MARLS with CTCE in a fully ob-
agentthroughadditionalagent-specificrewardsignals,butin-
servable setting and CTDE in a partially observable setting
dividualagententitiescannotdirectlychangethegovernance-
against two different MARL problems to quantify the scal-
defined rewards. For the system to achieve its shared objec-
ability and adaptability performance aspects. Previously, ar-
tive,thegovernanceevolvesitsrewarddistributionforagents
chitecturesutilizingadditionalnoveltieslikeimitation-based
basedonchangingagentbehaviorduringlearning,wherethe
learning [42, 11], curiosity-driven learning [5, 40], curricu-
GOV-REKsearchplanexecutesdifferentmodelswithdiffer-
lumlearning[3],self-ortemporal-attention[30,31],andevo-
entgovernancekernelconfigurations.Thegovernancekernel
lutionarylearning[36]haveshownefficacyforawiderange
selectioncriteriaare flexible;itcan eitherbeasingle objec-
of RL problems. With our proposed GOV-REK framework,
tive,likemaximizingrewards,oramulti-objective,likemax-
wefocusonimprovingtheperformanceofexistingbaseline
imizingrewardsandminimizingepisodelengths.
algorithmswithanadditionalcoordinatinggovernancelayer
Our first contribution is the inception of a dynamic in- thatprimarilyaltersagentincentivestoachieveconvergence.
ductive bias to explore topologically similar state or joint- Fordefiningmeaningfulagentmotivation,reward-shaping
action spaces for incentivizing cooperation amongst agents hasbeenwidelystudiedinthepast,wherethisapproachhas
in MARLS. Second, our proposed GOV-REK framework’s provenitsefficacyforachievingfasterconvergence[24,45].
algorithm learns ideal agent reward models by conducting Also, incorporating other novel mechanisms, like learning
aniterativesearchoverourproposedproblem-agosticreward ethicalhumanbehaviordemonstrations[46],multi-objectiveFigure3:ThesequenceofexecutionandlearningstepsinGMASandGOV-REKapproaches.
rewardshaping(MORS)[13],additionalrewardsforsub-goal ofsignificantmodeltrainingashighlightedinFigure3.
completion [28], and context-sensitive rewards for agents Thenon-parametricapproacheslikeGaussianProcessRe-
[11], have shown further improvements. However, reward- gression(GPR)useGaussiankernelpriorstoperformthere-
shapingagentsareoftensusceptibletofallingundercontinu- gression task. Also, the kernel’s choice determines the GP
ouspositiverewardcycletraps,especiallyforsparseenviron- model’s generalization properties in the GPR modeling task
ments where additional rewards can dominate the accurate [22]. Here, we also focus on defining meaningful prior re-
underlying reward model. Formally, the reward function for warddistributionforagentsforincentivizingagentcoopera-
theunderlyingMarkovDecisionProcess(MDP)canbemod- tion behavior in sparse scenarios. Also, HPO methods have
ified with the relation R′ = R+F, where F(s, s, s′) is the proved highly effective for finding suitable hyperparameter
additional transition reward model, and f is defined analo- configurationsofpredictionandmodelingproblems[8].For
t
gously to r in a temporal setting. The Potential Based Re- example,SHexploresthehyperparameterspacefromlowfi-
t
ward Shaping (PBRS) maintains a potential function Φ : S delity configurations with a lower training budget first, and
→ R is a necessary and sufficient constraint designed for with each increasing round, doubles the training budget by
policyinvariancewhichappliestoMARLSaswell[20,19]. removingunderperformingconfigurations[29].Withincreas-
Thisrelationcanfurtherbemodifiedtoincorporatethetem- ingrounds,halfoftheactiveconfigurationswithmoreenor-
poralelementgivenbyF(s,t,s′,t′) = γΦ(s′,t′)−Φ(s,t), mous losses are eliminated, and their budget is equally as-
whereγ denotesthediscountfactor.Furthermore,ourGOV- signed to the existing configurations. Further, the Hyper-
REK approach restricts all our agent solution trajectories to bandalgorithmcarriesoutmultipleSHroundsacrossparallel
always satisfy PBRS constraints by using only normalized brackets to train a variety of hyperparameter configurations
rewarddistributionsasadditionalrewardsignals. withdifferenttrainingbudgetstoexploremoreconfigurations
systematicallyinaproblem-agnosticmanner[29].Basedon
ThetaskoffindingoptimalstrategiesorpoliciesinMARL ourlearningsfromHyperband’siterativeexplorationmethod-
systemsisstillanopenchallenge[49].Tomitigatethisprob- ology, we also iterate over different governance kernels in a
lem,researchershaveproposedaparadigmwhereagentsare problem-agnostic manner in our proposed approach using a
providedwithassistiveinformationforlearning.Approaches, repeatedHyperband-basedsearch.
likeEnvironment-MediatedMulti-AgentSystems(EMMAS)
[44],ElectronicInstitutions(EI)[25],andNormativeMulti- ApproachFormulation
Agent Systems (NorMAS) [17, 37] generally employ a re-
Computationally, optimizing the joint MARL policy while
strictive strategy to limit original solution policy space for
parallelly searching for ideal reward models for agents is
empirically achieving faster convergence. Further, the Au-
practically infeasible for complex scenarios. We utilize the
tonomic Electronic Institutions (AEI) approach dynamically
similaritiesinstateandjoint-actionspacestodefinesimplis-
evolves these constraints to achieve even faster convergence
tic rewardmodel distributions as afunction of spatial topol-
[9]. As shown in Figure 3, the Governed Multi-Agent Sys-
ogy in the MARL problem tasks. Hence, simplistic reward
tem(GMAS)approachquerieseveryexecutionsteptoobtain
modelsdenotedasgovernancekernelsaredefinedonlybased
permissibleactions,andthelearninghappensbetweenthose
onthegeometricsimilaritiesinthestateorjointactionspaces
steps [39]. Also, in each learning step, the governance opti-
butnotonstate-actiontransitions.
mizes its learning policy for maximizing the system objec-
tive while evolving the action-space constraints. The black-
GovernanceKernels
box ANN agents also update their policies at each learning
step, where the agents are not part of the governance. All The generalization of MDP formulation for RL tasks is the
the above-discussed approaches strictly constrain the agent stochastic game (SG) formulation in MARL settings, where
action spaces, which might be suboptimal when a massive it is formally defined by the tuple < S,A , ..., A ,f,R ,
1 n 1
exploration of the joint state-action space is needed, like in ..., R > [4]. For n agents, S represents a discrete set of
n
sparse reward problems. However, with our proposed ap- states, A ∈ {1, ..., n } represents the discrete action set.
i
proach, the additional reward signals introduce only soft Thediscreteactionsarechosenfromthediscretevaluesrange
constraints on agent exploration behavior, which prohibits of k actions, which further generates the joint action space
strictly restricting the exploration capacity of agents. Sec- A=A ×... ×A .Thefunctionf:S×A×Srepresentsthe
1 n
ond,ourrewardmodelsareevolvedinamorestablemanner, state transition probability, and reward functions for agents
whereonlybetterrewardmodelsareselectedaftereachround R ∈{1,...,n}areexpressedasR :S×A×S→ R.Fur-
i iRotation Axes
g A1 + g՛ A1 {r, r, r}:
X Y Z
g 1 {𝛱/4, 0, 𝛱/4}
g A2 a.) Exampg ll eo b oa f l s k ue pr en re pls ositioning superimposed g՛ A2 Hyperboloid Kerne bl .) Example of problem adaptabR ilio tyt ated Kernel Variant
agent kernels different kernels for MARL agents agent kernels features in governance kernels
Figure4:Thedifferentwaysinwhichdifferentgovernancekernelsaresuperimposableandadaptableasagent-specificandagent-
agnosticrewarddistributions.
ther,theindividualagentpoliciesπ :S×A→[0,1]together overoneanothertogenerateevenmorecomplexrewardmod-
i
formthejointpolicyπ,andPOSGisamoregeneralizedver- els. Table 1 highlights some sample governance kernels that
sionofSGwheretheunderlyingstatesareunknown.MARL resemble popular Gaussian kernels, where the radial scale,
algorithms can be implemented on static games (stateless) periodicity,andmagnitudearetunableforthesekernels.The
likethesocial-dilemmaproblemwherethestatesetS=ϕand governance kernels are building blocks of the GOV-REK
reward only depends on the joint actions R : A → R [34]. framework, which generates problem-agnostic reward mod-
i
Also,MARLalgorithmsapplytothestagegameswhereS̸= els. It further provides orientation adaptability with super-
ϕ. Our experiments demonstrate the utility of defining sim- imposition capabilities to define spatially flexible and com-
plistic reward models that exploit geometrical spatial prop- plex reward models as highlighted in Figure 4. To extend
ertiesinstatespaceinstagegamesandjoint-actionspacein the package delivery problem into a 3D environment with
staticgames. drones, we use another set of governance kernels with vol-
umetric surfaces and conical sections, providing easy inter-
g =σ2κ(s ,s′ )+ξ;G=σ2κ(s,s′)+ξ; (1) pretability [26, 6]. Mathematically, the formal definitions of
i ai ai
somesamplesurfacegovernancekernelsareprovidedinTa-
For defining governance kernels, which are the simplified ble 1. The usage of these geometrical surfaces further high-
reward models proposed as part of the GOV-REK frame- lights the simplicity of these kernels and their extensibility
work, we assume that E a (cid:2) R(s, a, s′)(cid:3) → R′(s, s′) holds. tonon-Guassiangovernancekernels.Further,wealsoassign
This exploration expectation assumption means that the un- similar governance kernels to joint-action spaces in the re-
derlyinglearningalgorithmishighlycurioustoselectdiverse wardpayoffmatrixforanextensiontostaticgames,whichare
actions, where all the relevant solution trajectories between definedwithoutexplicitstates.Fornon-spatialproblems,like
the state-action transition pairs are explored. Hence, math- thesocialdilemmaproblems,wedemonstratethattheirjoint
ematically, we take an expectation with respect to the ex- action reward payoff matrix also consists of spatial trends,
ploredactionsfromthesolutiontrajectoriestodefineourre- as shown in Figure 5. Specifically from Figure 5, we ob-
ward models only as a function of state similarity, and we serve that a periodic governance kernel with a specific peri-
extend our results to joint-action spaces as well. The per- odicitysuperimposedwithadirectionalgradientgovernance
formance of our solution directly depends on the enhance- kernelcanfurtherencourageagentstocooperate.Generally,
mentprovidedbythegovernancekernelrewardsG Rforsam- stage SGs and static SGs enclose a geometric state space or
pling all relevant solution trajectories with additional assis- joint-action space representation property. Furthermore, the
tancefromthelearningalgorithm.Nevertheless,itallowsus proposed governance kernels are applied over this geomet-
todefinegovernancekernelsindependentofagenttransitions, ric topology to bias the agent behavior to assist the learning
andthisisexpressedbytherelationsr i′ =r i+g r,i andR′ = algorithm.
R+G foragent-specificandagent-agnostickernelsrespec-
r
tively.Theagent-specifickernelsaredefinedconsideringthe
TheGOV-REKFramework
initial agent’s spatial locations, whereas the agent-agnostic
kernelsareindependentoftheagent’sinitiallocation.There- The GOV-REK framework uses repeated Hyperband execu-
fore, the convergence for MARL tasks is accelerated with tions and iteratively manipulates governance kernel config-
suitable governance kernel priors and high exploration cu- urations to find suitable agent reward models. For imposing
riosity.Initsgeneralizedmathematicalform,weexpressour PBRS consistency constraints, we normalize all the reward
governancekernelswiththeequation1asagent-specificand values associated with each state or joint-action element. It
agent-agnosticnon-parametricvariations,respectively.Here, isdonebydividingeachscalerrewardelementbythesumof
thekernelfunctionisrepresentedbyκ,σ upscalesordown- totaladditionalrewardsaddedbythatgovernancekernelmul-
scales function values, (s ai, s′ ai) or (s, s′) quantifies the tiplied by the number of agents. The pseudocode algorithm
magnitudevariationbetweenagent-specificoragent-agnostic 1, highlights our proposed methodology of repeated execu-
states,andξrepresentsthenoiseinthekernelfunction. tion of Hyperband-like N rounds, where T represents the
r
Conceptually,kernelsarewidelyusedinGPRwithGaus- total SH training bracket budget. The total budgets stores
sian kernels to generate model function estimates, and the decreasingSHbracketbudgetsforrepeatedHyperbandround
non-parametricGaussiankernelsareusableinmodelingcom- execution, η represents the multiplication factor which in-
plexfunctionsforpredictiontasks[22].Also,similartoGaus- creasesthetrainingbudgetanddecreasesthenumberofcon-
sian kernels, these governance kernels can be superimposed figurationsaftereachSHround.Inalgorithm1,thefirsttrain-Table1:ThemathematicalformulationofcommonGaussiankernelsand3D-surfacefunctionsassamplegovernancekernelsfor
2Dand3DspatialMARLtasksrespectively.
GovernanceKernelsfor2Dspaces GovernanceKernelsfor3Dspaces
Name Equation Kernel Name Equation Kernel
LinearKernel σ2(x−c)(x′−c) DiagonalKernel x=y=z
PeriodicKernel
σ2exp(cid:16) −2sin2(π|x−x′|2/p)(cid:17)
EllipsoidKernel x2 + y2 + z2 =1
l2 a2 b2 c2
SquaredExponential
σ2exp(cid:16) −(x−x′)2(cid:17)
HyperboloidKernel x2 + y2 − z2 =1
2l2 a2 b2 c2
ing loop defines the number of Hyperband-like round rep- rounds. Further, they are genetically mutated and superim-
etitions,andinthelaterHyperband-likerounds,theselected posed with other governance kernels with mutation proba-
bestkernelsfromeachSHbracketaremutatedwiththefactor bility factor m = .5 and superimposition probability factor
m=.5.Further,thesecondtrainingloopdefinesthebracket s = .5, respectively. The total accumulated reward for the
trainingbudgetR,maximumbracketsincurrentrounds , governed MARL includes additional governance kernel re-
max
and total Hyperband plan budget B encompassing multiple wardsforallagents.Also,themergedkernelsarenormalized
SHbrackets. again to avoid violating PBRS constraints after governance
kernelsuperimposition.Ifthemutatedorsuperimposedgov-
Algorithm 1: Algorithm pseudocode for the GOV-REK ernance kernels decrease the model performance objectives,
framework the previous top-performing kernels for agents get selected
bydefault.
Input:T,N ,η,t
r k
Set:total budgets=reverse sort([T × i in(N )])
Nr r Experiments
Set:global top conf set =[]
gov
forRintotal budgetsdo Inourexperimentation,wetestthecomparativeperformance,
Set:s max =⌊log η(R)⌋,B =(s max+1)R robustness,andscalabilityoftheGOV-REKframeworkona
Set:round top conf set gov =[] 2D-grid road and 3D-grid drone environment in a fully ob-
fors∈{s max,s max−1,...,1,0}do servableCTCEsetting.Further,totesttheefficacyandadapt-
Set:n =⌈B ηs ⌉;r =Rη−s; ability of our approach, we extend our proposed GOV-REK
gov Rs+1 gov
Set:bracket conf set =[] framework onto the social dilemma problem in a partially
gov
get conf set =get governance kernels(n ) observable CTDE setting. Also, for the spatial package de-
gov gov
for j ∈{0,...,s} do liverytask,thegovernancekernelsaredefinedoverthestate
Set:n =⌊n ×η−j⌋;r =r ×ηi space, and contrastively, for the non-spatial social dilemma
gov,j gov gov,j gov
L={get trained model metrics( problem, the governance kernels are defined over the joint-
:conf ,r ) action space. For the package delivery problem, effectively,
gov gov,j
:∈conf set } both resource constraint agents must cooperate to deliver a
gov
bracket conf set .add( packagetothegoallocationtoreceivetheonlygoalreward.
gov
top configs(t ,L,m=.5,s=.5)) Whereas in the social dilemma problem, we define two sce-
k
endfor narios,wherepartialcooperationamongstagentsyieldspar-
round top conf set .add( tial rewards in the first scenario, and in the second scenario,
gov
top configs(t ,bracket conf set ,L)) onlycompletecooperationamongsttheagentsyieldstheonly
k gov
endfor cooperation reward. The proposed governance layer alters
global top conf set .add( the net reward for the system for the training and inference
gov
top configs(t ,round conf set ,L)) stages. Also, the new average expected reward includes the
k gov
endfor additionalrewardprovidedbytheagentgovernancekernels.
Return:global top conf set trainedmodels We select the baseline Proximal Policy Optimization
gov
(PPO) implementations with default hyperparameters from
Stable Baselines3 [41] and RLlib [35] packages for CTCE
Finally, the last training loop defines the different gover- and CTDE training respectively. Further, each of the learn-
nancekernelconfigurationsandtheircorrespondingbudgets ing curves for our experiment task results from the average
fortheSHbracket.Furthermore,aftertheparallelexecution of five different executions having different random seeds,
ofmultipleSHroundsacrossdifferentbrackets,thetopgov- and95%confidenceintervalrangesarealsoplottedtoquan-
ernance kernels are selected. These best-performing gover- tifytheperformanceuncertainty.OurexecutionoftheGOV-
nancekernelswiththebestaccumulatedaveragerewardand REK plan demonstrates that the superimposition of agent-
minimum average episode length values are passed to later specific squared exponential and agent-agnostic linear gov-(0,0) (0,0) (0,2) (0,0) (0,2)
Current 2 Player
{0,0} {0,1} Payoff Matrix {0,0,0} {0,0,1} {1,0,0} {1,0,1} (2k,2k) (2k,2k+2)
Mirrors Current 3 Player
{1,0} {1,1} wA ithlo Nng e wY - AA cx ti is on {0,1,0} {0,1,1} {1,1,0} {1,1,1} Pay Mof if r rM ora strix C Pu arr ye Mon ft if r N rM o rP a sl ta riy xer
Along X-Axis
(1,1) (1,1) (1,3) with New Action (2,0) (1,1) (2,2) (1,3)
a.) 2-Players a.i) Base Kernel b.) 3-Players b.i) Periodic Kernel for
for Governance Governance (p=2) Along Y-Axis
g g with New Action
i i Important Note
g i P g i P 1. The governance kernel has (2k+2,2k) (2k+2,2k+2)
period p=2 along X- and Y-axis.
(0,0) (0,0) (3,1) (3,3)
Y Y 2. And, the reward magnitude also
increases along X- and Y-axis. c.) 4-Players d.) N-Players
X (1,1) X (1,3)
Figure5:Theincreasingperiodicgovernancekernelconfigurationforthenon-spatialsocialdilemmaproblems.
ernancekernelsworksbestforthe5×52D-gridroadsetting. tory sampling issue, we introduce decaying governance ker-
Similarly, for the grid drone environment, a combination of nels that reduce the future reward value associated with the
agent-agnostichyperboloidanddiagonalsurfacegovernance statetothegivenfractionalamountwhenanagentvisitsthat
kernels works best for the 3×3 3D-grid drone setting. Fi- state. The baseline governance kernels bias the agent’s cu-
nally, for the social dilemma problem, a combination of lin- riositytoexplorelocalregionsmorethoroughly,butdecaying
earandperiodicgovernancekernelsprogressivelydirectsthe governancekernelsencourageagentstoexploremoreglobal
agents toward higher cooperation rates. In our experimenta- anddiversesolutiontrajectories.
tion tasks, we analyze the impact of random perturbations, Further,IIIandIVsubfiguresinFigure8demonstratethe
increasing scale and complexity, and symmetry in coopera- capabilitiesoftheGOV-REKframeworktooperateonlarger
tioncontributionsforthegovernancekernelsselectedbythe 10 × 10 2D-grid road and 5 × 5 3D-grid drone environ-
GOV-REKframework. ments effectively. For a 2D-grid road environment, we ob-
servethatgovernancekernelsaremoreeffectivewithhigher
RobustnessAnalysis decayrates,leadingtofasterconvergence.Second,fora3D-
grid drone environment, the efficiency again increases with
The I and II subfigures in Figure 8 demonstrate the robust-
higher decay rates, but the average episode length reduction
ness of the GOV-REK framework against randomized per-
isrelativelyless.Weattributethisrelativeperformancedepre-
turbationsinenvironmentconfigurationsandsolutiontrajec-
ciationtothesimplisticnatureofselectedsurfacegovernance
tory blocker objects. We observe that the governed MARLS
kernels,whichprovidesmoreinterpretablesolutiontrajectory
trainedfor120Ktimestepsaregenerallyrobusttoanincreas-
behavior but hampers the agent performance. Hence, simi-
ingnumberofblockerobjects.However,theaverageepisode
lar to GPR, the convergence performance for a MARL task
length increases as the number of blockers increases. Fur-
depends on theinitial population of the selected governance
ther, relatively larger average episode lengths for the per-
kernelsforagents.
episoderandomizedenvironmentshighlightnon-optimalso-
lution trajectory selection behavior and slower package de-
PerformanceAnalysis
liveries.Theincreasedaverageepisodelengthbehaviorisat-
tributedtoagentsselectingsub-optimaltrajectoriestotackle To benchmark our approach’s performance, we compare its
the randomized behavior added by changing configurations learning behavior against the MORS approach for 120K
andblockerobjectlocations.Theselectedgovernancekernels timestepson5×52D-gridroadenvironmentswithfixedand
by the GOV-REK framework are not optimized for chang- random initial position configurations with a single goal re-
ing initial and per-episode random configurations. However, ward (R = 2.5). 1 In earlier robustness and scalability ex-
the selected governance kernels still demonstrate robustness periments,thesecondagentproceedswithafixeddelayafter
againsttheserandomenvironmentperturbations. thefirstagentstartsmoving.Nevertheless,foramorerealis-
ticbenchmarkingcomparisoninthisexperiment,bothagents
ScalabilityAnalysis movetogetherattheepisodeinitialization.Therewardmodel
fortheMORSapproachassignsmanuallyengineeredsubtask
Theoretically, for each agent in the 2D-grid road and 3D- rewards like package pickups, package exchange, and pro-
griddroneenvironments,thenumberofpossiblesolutiontra- ceeding closer to the goal as highlighted in Figure 2. From
jectories are of the factorial order given by (l + w − 2)! / Figure 7, the governed MARLS converges relatively faster
(l−1)!(w−1)!and(l+w+h−3)!/(l−1)!(w−1)!(h−1)!
than the MORS approach, especially for randomized ini-
respectively,wherew,l,andhstandsforweight,length,and
tialconfigurations.Further,theMORSapproachaccumulates
height respectively. Therefore, with increasing linear scale, morerewardafterconvergencewithlargerepisodelengths.In
theproblemcomplexityincreasesinfactorialcomplexity,and contrast, the governed MARL decreases the episode length
addingthesecondagentfurtheraddstotheMARLtaskcom- faster during learning and more after attaining convergence.
plexity.Ourapproach,initsnascentform,ispartiallyunable ThisbehaviordemonstratesthattheMORSapproachismore
toassistthebaselinePPOalgorithminexploringdiversesolu-
tiontrajectoriesoptimally,whichleadstoexplorationexpec- 1The experiment implementation for replication is available at
tation assumption violation. To mitigate this solution trajec- thecoderepository:github/gov-rek-marlsFigure6:TheGOV-REKexperimentsummarymeasuresaveragerewardreturnsandepisodelengthsforthegovernedMARLSto
quantifyi.)robustnessagainstincreasingpathblockerobjectsin5×52D-gridroadenvironments,ii.)robustnessagainstdifferent
randomizationperturbationsin5×52D-gridroadenvironmentconfigurations,iii.)scalabilityperformancewithdifferentreward
decaysin10×102D-gridroadenvironments.,iv.)scalabilityperformancewithdifferentrewarddecaysin3×3and5×53D-grid
droneenvironments.
A1 A1 T2 T1 A1 T2
T1 P A2 T1 P A2 P A2
T3
T2
G G G
a.) A2C with Radial b.) PPO with Radial b.) PPO with Radial
Governance Governance PBRS Governance
Figure 9: Different trajectories followed by governed agents
withA2CandPPOlearningalgorithmswithgovernance.
Figure7:ThecomparisonbetweentheGOV-REKandMORS prone to positive reward cycles leading to larger average
approachesfora5×5size2D-gridroadenvironmentindif- episodelengths.Hence,GOV-REKcomplieswiththePBRS
ferentconfigurations. framework and multi-faceted objective to select governance
kernels, minimizing average episode length and making it
morefaulttolerant.
Non-spatialProblemAnalysis
Figure 5 demonstrates the periodic geometric trend in the
jointactionrewardpayoffmatrix’sflattenedtopologyforthe
N-player social dilemma problem. To extend the applicabil-
ity of the GOV-REK framework, we apply all positive nor-
malized and zero-mean normalized governance kernels on
the above-described two different social dilemma problem
variants. In Figure 8, we report the average reward accu-
mulated in homogeneous (ravg=1, rmax=1) and heteroge-
i i
neous (ravg=1.5, rmax=2, rmin=1) scenarios with baseline
i i i
andsparsepayoffs.Ourexperimentsarecarriedoutona16-
agent and 16-episode length setting, where we observe that
Figure 8: The average expected reward returns for a single
governedagentsaccumulatemorerewardsonaverage,espe-
agent in homogeneous and heterogeneous agent systems for ciallywiththezero-meangovernancekernel.2 Inthehomo-
baselineandsparsesocialdilemmaproblems.
2The experiment implementation for replication is available at
thecoderepository:github/boosting-social-dilemma-collusionAgent Cooperation in 5X5 Size 2D Grid Road Environment Episode
Agent 1: Pickup and Carries Package; Agent 2: Waiting Agent 1: Drops Package; Agent 2: Picks Package towards Goal
Width: 5
Turn Bar
A1
P A2
G
t=1 t=2 t=3 t=4 t=5 t=6 t=7 t=8 t=9
Turn: Agent 1 Turn: Agent 1 Turn: Agent 1 Turn: Agent 1 Turn: Agent 1 Turn: Agent 2 Turn: Agent 2 Turn: Agent 2 Turn: Agent 2
Agent 1: Active for the Episode First Half Agent 2: Active for the Episode Second Half
Agent Cooperation in 3X3 Size 3D Grid Drone Environment Episode
Turn Bar Ariel Planes Agent 1: Active Initially in the Episode Agent 2: Active Mostly in the Episode Turn Bar
T A1 A2 A2 A2 A2 A2 A2
G
B A2 B B GroA u1 ndT PA la1 neT A2 P T A2 T A2 Width: 3T A2 Length: 3
Agent 1: Moves the Package Agent 2: Picks the Package Agent 2: Delivers the Package
Figure 10: Simulation examples for the governed cooperation behavior solution trajectories in 2D-grid road and 3D-grid drone
environmentsforthepackagedeliverytask.
geneoussetting,theadditionalaveragerewardsareaccounted paredtothesinglejointpolicyinCTCEpackagedeliverytask
forbytheextrarewardvaluesaddedbygovernancekernels. training. We observe cooperation inconsistencies and sub-
However, for the heterogeneous setting, the average reward optimality at larger and highly randomized configurations,
accumulation is relatively more considerable, highlighting whichleadstolargeraverageepisodelengthsandloweraver-
the governed kernels’ better performance in relatively more age reward accumulation. However, our experiment demon-
challengingsettings.Further,insparsesettings,thegoverned strates that our proposed approach does indeed assist the
agents accumulate more rewards on average, demonstrating baseline MARL algorithms to achieve faster convergence
our approach’s efficacy in sparse non-spatial (S = ϕ) envi- withoutanyhyperparametertuning.
ronmentsaswell.
Discussion
ConclusionandFutureWork
Figure9qualitativelydemonstratesthatevenatsmallscales,
theA2Calgorithmwithgovernancedoesnotlearntodeliver
Our experiments demonstrate that our proposed GOV-REK
the package but only to exchange it. Also, we observe that
framework is robust and applies to different MARL tasks.
evenforasmall3×32D-gridroadconfigurations,theagents
We demonstrate that simple additional reward model func-
are susceptible to fall for positive reward loops, which the
tions defined by Gaussian functions and 3D-surface func-
PBRS constraint successfully handles. Also, we highlighted
tionspracticallyhelpachievefasterconvergence.Thispaper
the efficacy of using reward decays for the already visited
showsthatadditionalrewardmodelscanbesuccessfullyde-
statesasaneffectivewaytoincreaseagentcuriosityregions
fined based on state or joint-action similarities for agents in
tosamplediversesolutiontrajectories.Forthepackagedeliv-
a problem-agnostic manner, provided the PBRS and explo-
eryproblem,therewardsareaccumulatedbythewholesys-
ration expectation constraint is satisfied. Further, our exper-
tem.Figure10highlightstheextentofcontributionbyagents
imentation quantifies the practical utilization of this reward
inthe2D-gridroadand3D-griddroneenvironments.Weob-
modelsimplificationconstraintsforincentivizingcooperation
serveasymmetryindeliverycontributiontrajectorieswiththe
insparseMARLproblems.
soft regional constraints added by our governance layer re-
wards. ThebaselinePPO-basedagentsatalargerscalefailtohold
Insimpler2D-gridroadenvironments,weobservethatthe the exploration expectation assumption E a
(cid:2)
R(s, a,
s′)(cid:3)
→
firstagentcontributesmoreinmovingthepackagecloserto- R′(s,s′)consistently.Therefore,experimentingwithotheral-
ward the goal, whereas in complex 3D-grid drone environ- gorithms, like RND [14], NGU [2], and Agent 57 [1], can
ments,itisvice-versa.Figure10alsohighlightsthatthefirst yield better results. In contrast to our reward-shaping-based
agent is more active for 5 × 5 2D-grid road environment, inductive bias, approaches like NGU and ATA attempt to
whereas the second agent is more active for 3×3 3D-grid learn these state similarities alongside the primary learning
drone environment. Further, for the social dilemma prob- problem[47].Thus,exploringaparadigmthattradesbetween
lem, the contribution is more symmetric, where all agents ourrigidandsimplisticrewardexplorationmethodagainsta
earn similar average rewards owing to the CTDE training whollyfluidandcomplexstatesimilaritylearningmethodis
paradigm, where each agent maintains their policies com- partofourfutureresearcheffort.
Height:
3
Height:
5References [15] Chalkiadakis,G.2003.Multiagentreinforcementlearn-
[1] Badia, A. P.; Piot, B.; Kapturowski, S.; Sprechmann, ing: Stochastic games with multiple learning play-
P.; Vitvitskyi, A.; Guo, Z. D.; and Blundell, C. 2020. ers. Dept.ofComputerScience,UniversityofToronto,
Agent57: Outperforming the atari human benchmark. Canada,Tech.Rep,25.
InInternationalconferenceonmachinelearning,507– [16] Christianos, F.; Scha¨fer, L.; and Albrecht, S. 2020.
517.PMLR. Sharedexperienceactor-criticformulti-agentreinforce-
[2] Badia, A. P.; Sprechmann, P.; Vitvitskyi, A.; Guo, D.; mentlearning. Advancesinneuralinformationprocess-
Piot, B.; Kapturowski, S.; Tieleman, O.; Arjovsky, M.; ingsystems,33:10707–10717.
Pritzel, A.; Bolt, A.; et al. 2020. Never give up: [17] Conte,R.;Falcone,R.;andSartor,G.1999. Agentsand
Learningdirectedexplorationstrategies. arXivpreprint norms:Howtofillthegap? AI&L.,7:1.
arXiv:2002.06038.
[18] Dann, C.; and Brunskill, E. 2015. Sample complexity
[3] Baker, B.; Kanitscheider, I.; Markov, T.; Wu, Y.; Pow- of episodic fixed-horizon reinforcement learning. Ad-
ell,G.;McGrew,B.;andMordatch,I.2019. Emergent vancesinNeuralInformationProcessingSystems,28.
toolusefrommulti-agentautocurricula. arXivpreprint
[19] Devlin, S.; and Kudenko, D. 2011. Theoretical con-
arXiv:1909.07528.
siderationsofpotential-basedrewardshapingformulti-
[4] Bas¸ar,T.;andOlsder,G.J.1998. Dynamicnoncooper-
agentsystems. InThe10thInternationalConferenceon
ativegametheory. SIAM.
AutonomousAgentsandMultiagentSystems,225–232.
[5] Bellemare,M.;Srinivasan,S.;Ostrovski,G.;Schaul,T.; ACM.
Saxton,D.;andMunos,R.2016. Unifyingcount-based
[20] Devlin, S. M.; and Kudenko, D. 2012. Dynamic
explorationandintrinsicmotivation.Advancesinneural
potential-based reward shaping. In Proceedings of the
informationprocessingsystems,29.
11th international conference on autonomous agents
[6] Benny, L. B. 1922. Plane Geometry: an Account of andmultiagentsystems,433–440.IFAAMAS.
theMoreElementaryPropertiesoftheConicSections:
[21] Du, W.; and Ding, S. 2021. A survey on multi-agent
Treated by the Methods of Coordinate Geometry and
deep reinforcement learning: from the perspective of
of Modern Projective Geometry, with Applications to
challenges and applications. Artificial Intelligence Re-
PracticalDrawing. Blackieandson.
view,54:3215–3238.
[7] Bergstra, J.; and Bengio, Y. 2012. Random search
[22] Duvenaud,D.2014.Automaticmodelconstructionwith
for hyper-parameter optimization. Journal of machine
Gaussian processes. Ph.D. thesis, University of Cam-
learningresearch,13(2).
bridge.
[8] Bischl, B.; Binder, M.; Lang, M.; Pielok, T.; Richter,
[23] Elbarbari, M.; Efthymiadis, K.; Vanderborght, B.; and
J.; Coors, S.; Thomas, J.; Ullmann, T.; Becker, M.;
Nowe´, A. 2021. Ltlf-based reward shaping for rein-
Boulesteix, A.-L.; et al. 2023. Hyperparameter opti-
forcement learning. In Adaptive and Learning Agents
mization: Foundations, algorithms, best practices, and
Workshop,volume2021.
openchallenges. WileyInterdisciplinaryReviews:Data
MiningandKnowledgeDiscovery,13(2):e1484. [24] Eschmann, J. 2021. Reward function design in re-
[9] Bou, E.; Lo´pez-Sa´nchez, M.; and Rodr´ıguez-Aguilar, inforcement learning. Reinforcement Learning Algo-
J. A. 2006. Towards self-configuration in autonomic
rithms:AnalysisandApplications,25–33.
electronic institutions. In International Workshop on [25] Esteva,M.;Rodriguez-Aguilar,J.-A.;Sierra,C.;Garcia,
Coordination,Organizations,Institutions,andNormsin P.; and Arcos, J. L. 2001. On the formal specification
AgentSystems,229–244.Springer. ofelectronicinstitutions. InAgentMediatedElectronic
[10] Bowling, M.; and Veloso, M. 2001. Rational and con- Commerce:TheEuropeanAgentLinkPerspective,126–
vergent learning in stochastic games. In International 147.Springer.
joint conference on artificial intelligence, volume 17, [26] Gomes, A. J.; Voiculescu, I.; Jorge, J.; Wyvill, B.; and
1021–1026.Citeseer. Galbraith,C.2009. Implicitcurvesandsurfaces:math-
[11] Brys, T.; Harutyunyan, A.; Suay, H. B.; Chernova, S.; ematics,datastructuresandalgorithms. Springer.
Taylor,M.E.;andNowe´,A.2015.Reinforcementlearn- [27] Gronauer, S.; and Diepold, K. 2022. Multi-agent deep
ing from demonstration through shaping. In Twenty- reinforcementlearning:asurvey. ArtificialIntelligence
fourthinternationaljointconferenceon artificial intel- Review,1–49.
ligence.
[28] Harutyunyan, A.; Devlin, S.; Vrancx, P.; and Nowe´,
[12] Brys, T.; Harutyunyan, A.; Taylor, M. E.; and Nowe´, A. 2015. Expressing arbitrary reward functions as
A. 2015. Policy Transfer using Reward Shaping. In potential-based advice. In Proceedings of the AAAI
AAMAS,181–188. ConferenceonArtificialIntelligence,volume29.
[13] Brys, T.; Harutyunyan, A.; Vrancx, P.; Taylor, M. E.;
[29] Hutter,F.;Kotthoff,L.;andVanschoren,J.2019. Auto-
Kudenko,D.;andNowe´,A.2014.Multi-objectivization
matedmachinelearning:methods,systems,challenges.
ofreinforcementlearningproblemsbyrewardshaping.
SpringerNature.
In 2014 international joint conference on neural net-
[30] Iqbal, S.; and Sha, F. 2019. Actor-attention-critic for
works(IJCNN),2315–2322.IEEE.
multi-agent reinforcement learning. In International
[14] Burda, Y.; Edwards, H.; Storkey, A.; and Klimov, O.
conferenceonmachinelearning,2961–2970.PMLR.
2018. Exploration by random network distillation.
arXivpreprintarXiv:1810.12894.[31] Jiang, J.; and Lu, Z. 2018. Learning attentional com- models. InInternationalconferenceonmachinelearn-
munication for multi-agent cooperation. Advances in ing,2721–2730.PMLR.
neuralinformationprocessingsystems,31. [41] Raffin,A.;Hill,A.;Gleave,A.;Kanervisto,A.;Ernes-
[32] Jiang, N.; and Agarwal, A. 2018. Open problem: The tus,M.;andDormann,N.2021. Stable-Baselines3:Re-
dependence of sample complexity lower bounds on liableReinforcementLearningImplementations. Jour-
planninghorizon. In ConferenceOn LearningTheory, nalofMachineLearningResearch,22(268):1–8.
3395–3398.PMLR.
[42] Schaal, S. 1996. Learning from demonstration. Ad-
[33] Lee, H.; Hong, J.; and Jeong, J. 2022. MARL-Based vancesinneuralinformationprocessingsystems,9.
DualRewardModelonSegmentedActionsforMultiple
[43] Tuyls,K.;andWeiss,G.2012.Multiagentlearning:Ba-
MobileRobotsinAutomatedWarehouseEnvironment.
sics,challenges,andprospects.AiMagazine,33(3):41–
AppliedSciences,12(9):4703.
41.
[34] Leibo, J. Z.; Zambaldi, V.; Lanctot, M.; Marecki, J.;
[44] Weyns, D.; Brueckner, S. A.; and Demazeau, Y. 2008.
andGraepel,T.2017. Multi-agentreinforcementlearn-
Engineering Environment-Mediated Multi-Agent Sys-
ing in sequential social dilemmas. arXiv preprint
tems: International Workshop, EEMMAS 2007, Dres-
arXiv:1702.03037.
den, Germany, October 5, 2007, Selected Revised and
[35] Liang, E.; Liaw, R.; Nishihara, R.; Moritz, P.; Fox, R.;
InvitedPapers,volume5049. Springer.
Goldberg,K.;Gonzalez,J.E.;Jordan,M.I.;andStoica,
[45] Wirth, C.; Akrour, R.; Neumann, G.; Fu¨rnkranz, J.;
I.2018. RLlib:AbstractionsforDistributedReinforce-
et al. 2017. A survey of preference-based reinforce-
mentLearning.InInternationalConferenceonMachine
ment learning methods. Journal of Machine Learning
Learning(ICML).
Research,18(136):1–46.
[36] Long, Q.; Zhou, Z.; Gupta, A.; Fang, F.; Wu, Y.; and
[46] Wu, Y.-H.; and Lin, S.-D. 2018. A low-cost ethics
Wang, X. 2020. Evolutionary population curriculum
shapingapproachfordesigningreinforcementlearning
for scaling multi-agent reinforcement learning. arXiv
agents. InProceedingsoftheAAAIConferenceonArti-
preprintarXiv:2003.10423.
ficialIntelligence,volume32.
[37] Neufeld, E. A. 2022. Reinforcement Learning Guided
[47] Xiao, B.; Ramasubramanian, B.; and Poovendran, R.
by Provable Normative Compliance. In Rocha, A. P.;
2022. Agent-Temporal Attention for Reward Redistri-
Steels, L.; and van den Herik, H. J., eds., Proceed-
bution in Episodic Multi-Agent Reinforcement Learn-
ings of the 14th International Conference on Agents
ing. arXivpreprintarXiv:2201.04612.
and Artificial Intelligence, ICAART 2022, Volume
3, Online Streaming, February 3-5, 2022, 444–453. [48] Young, S. R.; Rose, D. C.; Karnowski, T. P.; Lim, S.-
SCITEPRESS. H.; and Patton, R. M. 2015. Optimizing deep learning
hyper-parametersthroughanevolutionaryalgorithm. In
[38] Ng,A.Y.;Harada,D.;andRussell,S.1999. Policyin-
Proceedings of the workshop on machine learning in
varianceunderrewardtransformations:Theoryandap-
high-performancecomputingenvironments,1–5.
plication to reward shaping. In Icml, volume 99, 278–
287. [49] Zhang, K.; Yang, Z.; and Bas¸ar, T. 2021. Multi-agent
[39] Oesterle, M.; Bartelt, C.; Lu¨dtke, S.; and Stucken- reinforcementlearning:Aselectiveoverviewoftheories
schmidt, H. 2022. Self-learning governance of black- and algorithms. Handbook of reinforcement learning
boxmulti-agentsystems. InInternationalWorkshopon andcontrol,321–384.
Coordination, Organizations, Institutions, Norms, and [50] Zhao, W.; Queralta, J. P.; and Westerlund, T. 2020.
Ethics for Governance of Multi-Agent Systems, 73–91. Sim-to-real transfer in deep reinforcement learning for
Springer. robotics: a survey. In 2020 IEEE symposium series on
[40] Ostrovski,G.;Bellemare,M.G.;Oord,A.;andMunos, computationalintelligence(SSCI),737–744.IEEE.
R. 2017. Count-based exploration with neural density