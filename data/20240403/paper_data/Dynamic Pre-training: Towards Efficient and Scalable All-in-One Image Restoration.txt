Dynamic Pre-training: Towards Efficient and
Scalable All-in-One Image Restoration
Akshay Dudhane1 Omkar Thawakar1 Syed Waqas Zamir2
Salman Khan1,3 Fahad Shahbaz Khan1,4 Ming-Hsuan Yang5,6,7
1Mohamed bin Zayed University of AI 2Inception Institute of AI
3Australian National University 4Linköping University
5University of California, Merced 6Yonsei University 7Google Research
Abstract. All-in-oneimagerestorationtacklesdifferenttypesofdegra-
dationswithaunifiedmodelinsteadofhavingtask-specific,non-generic
models for each degradation. The requirement to tackle multiple degra-
dations using the same model can lead to high-complexity designs with
fixed configuration that lack the adaptability to more efficient alter-
natives. We propose DyNet, a dynamic family of networks designed
in an encoder-decoder style for all-in-one image restoration tasks. Our
DyNetcanseamlesslyswitchbetweenitsbulkierandlightweightvariants,
thereby offering flexibility for efficient model deployment with a single
round of training. This seamless switching is enabled by our weights-
sharing mechanism, forming the core of our architecture and facilitat-
ing the reuse of initialized module weights. Further, to establish robust
weightsinitialization,weintroduceadynamicpre-trainingstrategythat
trains variants of the proposed DyNet concurrently, thereby achieving
a 50% reduction in GPU hours. To tackle the unavailability of large-
scale dataset required in pre-training, we curate a high-quality, high-
resolution image dataset named Million-IRD having 2M image samples.
We validate our DyNet for image denoising, deraining, and dehazing
in all-in-one setting, achieving state-of-the-art results with 31.34% re-
duction in GFlops and a 56.75% reduction in parameters compared to
baseline models. The source codes and trained models are available at
https://github.com/akshaydudhane16/DyNet.
Keywords: All-in-one · Image Restoration · Foundation model
1 Introduction
The image restoration (IR) task seeks to improve low-quality input images. De-
spite several advancements in IR, diverse degradation types and severity levels
present in images continue to pose a significant challenge. The majority of ex-
isting methods [3,8,13–15,31,46,47,52] learn image priors implicitly, requiring
separate network training for different degradation types, levels, and datasets.
Further, these methods require a prior knowledge of image degradation for ef-
fectivemodelselectionduringtesting,therebylackinggeneralitytocaterdiverse
degradations.
4202
rpA
2
]VC.sc[
1v45120.4042:viXra2 Akshay Dudhane et al.
All-in-one restoration aims to restore images with an unknown degradation
using a single unified model. Recent advancements, such as AirNet [26] and
PromptIR [36], have addressed the all-in-one restoration challenge by employ-
ing contrastive learning and implicit visual prompting techniques, respectively.
Specifically, the state-of-the-art PromptIR [36] employs an implicit prompting
to learn degradation-aware prompts on the decoder side, aiming to refine the
decoder features. This approach, while intriguing, does not extend to the refine-
mentofencoderfeatures,whichareleftunprocessed.Despitetheinterestingidea
ofimplicitprompting,itposesaconsiderablechallengefordeploymentduetoits
poorcomputationalefficiencywith37Mparametersand243GFlopstoprocessa
single224×224sizedimage.Thepracticalapplicationofsuchmodelsistherefore
challenging due to the high computational requirements, especially in resource-
constraint hand-held devices. However, choosing lightweight models invariably
leads to a trade-off between accuracy and efficiency [28,56]. Most of these mod-
els aim to reduce parameter counts by strided convolutions and feature channel
splitting[22,23,28].Somestrategiesalsooperatewithinthefrequencydomainto
reduce the computational demands linked to attention mechanisms [62] or non-
local operations [18] while few methods like [9,49] partition the feature space
for efficient processing, or split the attention across dimensions to improve the
computational efficiency [9,61].
Towards optimizing all-in-one IR efficiency without sacrificing performance,
in this paper, we introduce a novel weights-sharing mechanism. In this scheme,
the weights of a network module are shared with its subsequent modules in a
series. This approach substantially reduces the number of parameters, resulting
in a more streamlined network architecture. We deploy the proposed weights-
sharing mechanism in an encoder-decoder style architecture named Dynamic
Network(DyNet).Apartfromthecomputationalefficiency,theproposedDyNet
provides exceptional flexibility as by merely changing the module weights reuse
frequency, one can easily adjust the network depth and correspondingly switch
between its bulkier and light-weight variants during training.
While DyNet offers great adaptability with orders of speedup, a common
challenge with lightweight networks is the potential compromise on overall ac-
curacy. To counteract this, we show that a large-scale pre-training strategy can
effectively improve the performance of our lightweight models. By initializing
thenetworkwithweightsderivedfromthepre-training,themodelbenefitsfrom
a strong foundation, which can lead to enhanced performance even with a re-
duced parameter count. However, large-scale pre-training is computationally in-
tensive and requires significant GPU hours. Therefore, we introduce an efficient
and potent dynamic pre-training strategy capable of training both bulkier and
lightweightnetworkvariantswithinasinglepre-trainingsession,therebyachiev-
ingsignificantsavingsof50%inGPUhours.Complementingthis,wehavecom-
piledacomprehensive,high-quality,high-resolutiondatasettermedMillion-IRD,
consisting of 2 million image samples. Our main contributions are as follows:
– WeproposeDyNet,adynamicfamilyofnetworksforall-in-oneimagerestora-
tion tasks. DyNet offers easy switching between its bulkier and light-weightDynamic Pre-training for All-in-One Image Restoration 3
DyNet-L
𝑯×𝑾×𝒄 𝑯×𝑾×𝒄 𝑯×𝑾×𝒄 32.9 DyNet-S 16M
16M
skcolB
rem
rofsnarT
×N
d
erahSskcolB
rem
rofsnarT
×N ×N/2
R
N
S P
e g
a re
v A
33333 00112 ..... 49494 Pro 3m 7MptIR
MA P7i
.
Rr 6N N1e eMt
t
15.74M
𝑯×𝑾×𝒄 𝑯×𝑾×𝒄 𝑯×𝑾×𝒄
29.9
(a) (b) (c) 160 250 640 900
GFlops
Fig.1: Left: At any given encoder-decoder level, (a) Series of transformer blocks in
existingPromptIR[36],(b)Theproposedtransformerblockweightssharingmechanism
in our DyNet-L. We initialize one transformer block at each encoder-decoder level
and shared its weights with the subsequent N transformer blocks. (c) Our lightweight
variantDyNet-S.Here,thenumberoftransformerblocksarehalvedandweightsofthe
first transformer block are shared with N/2 subsequent transformer blocks. Right: A
plot of average PSNR in All-in-one restoration setting vs GFlops and parameters (in
millions). The proposed DyNet-S achieves boost of 0.59 dB with 31.34% reduction in
GFlopsanda56.75%reductioninparameterscomparedtothebaselinePromptIR[36].
variants. This flexibility is made possible through a weight-sharing strategy,
which is the central idea of our network design, allowing for the efficient
reuse of initialized module weights.
– We introduce a Dynamic Pre-training strategy, a new approach that allows
the concurrent large-scale pre-training of both bulkier and light-weight net-
work variants within a single session. This innovative strategy reduces GPU
hours significantly by 50%, addressing the pressing challenge of resource-
intensive model pre-training on a large-scale.
– We curate a comprehensive pre-training dataset comprising 2 million high-
quality, high-resolution, meticulously filtered images. From this dataset, we
extract8millionnon-overlappinghigh-resolutionpatches,eachsized512×512,
utilized for the pre-training of variants of the proposed DyNet.
Through the synergy of these techniques, our proposed DyNet achieves an av-
erage gain of 0.82 dB for image de-noising, deraining, and dehazing within an
all-in-one setting, with 31.34% reduction in GFlops and a 56.75% reduction in
network parameters compared to the baseline; refer Fig. 1.
2 Related Work
Image restoration seeks to restore images from their degraded low-quality ver-
sions, with the restoration process varying considerably across different tasks.
Research in image restoration has predominantly concentrated on addressing
single degradation challenges [11,34,38,40,45,52,53,58,60]. Although signifi-
cant progress has been made in single degradation restoration techniques, re-
search in multi-degradation restoration using a unified model is still relatively
under-explored.Multi-degradationrestoration,however,providesmorepractical4 Akshay Dudhane et al.
advantages as the information about input degradation type is not required to
select the task-specific model and it also enhances model storage efficiency over
single-degradation restoration.
The critical challenge in multi-task image restoration is developing a single
modelcapableofaddressingvarioustypesofdegradationandpreciselyrestoring
low-quality images. The IPT model [7], for instance, relies on prior knowledge
of the corruption affecting the input image, utilizing a pre-trained transformer
backbone equipped with distinct encoders and decoder heads tailored for multi-
task restoration. However, in blind image restoration, we do not have such prior
information about the degradation. In this direction, Li et al. [26] propose a
unified network for multiple tasks such as denoising, deraining, and dehazing,
employing an image encoder refined through contrastive learning. However, it
requires a two-stage training process, where the outcome of contrastive learning
relies on selecting positive and negative pairs and the availability of large data
samples. Further, motivated by easy implementation and broad applicability of
prompts,PromptIR[36]introducesaconceptwheredegradation-awareprompts
arelearnedimplicitlywithinanencoder-decoderarchitectureforall-in-oneimage
restoration. PromptIR applies implicit prompting to the decoder side to refine
the decoder features. However, it does not extend to the refinement of encoder
features, which are left unprocessed.
Although the above-discussed methods tackle multi-task image restoration,
theysufferfromlowefficiencyduetoalargenumberofparametersandsubstan-
tialGFlopsconsumption.Consequently,deployingthesemodelsinenvironments
whereefficiencyiscrucialfactorbecomesdifficultduetotheirdemandingcompu-
tational needs. However, opting for lightweight models introduces an inevitable
accuracy-efficiencytrade-off[56].Theprimarycauseisthatmanycurrentmeth-
odsfocusonreducingparametercountsthroughtechniqueslikestridedconvolu-
tionsandfeaturechannelsplitting[22,23,28].Additionally,fewapproacheswork
withinthefrequencydomaintoreducethecomputationalburdenassociatedwith
attention mechanisms [62] or non-local operations [18]. Furthermore, methods
like [9,49] partition the feature space for efficient processing, while [9,61] split
theattentionacrossdimensionstoimprovethecomputationalefficiency.Incon-
trast,ourproposedapproachfocusesonanefficientandscalableall-in-oneimage
restoration model that incorporates a module weights-sharing mechanism. We
demonstratethatstrategicfundamentaladjustmentstothenetworkarchitecture
result in substantial performance improvements.
3 Proposed Approach
Existing state-of-the-art all-in-one image restoration (IR) approaches incur a
highcomputationalfootprintanddonotprovidetheflexibilitytosupportmodels
with variable depths during training. Towards more accurate, but lightweight
and flexible architecture design for all-in-one IR, this work proposes a Dynamic
Network Architecture (Sec. 3.1) and a Dynamic Pretraining Strategy (Sec. 3.2).Dynamic Pre-training for All-in-One Image Restoration 5
Input Image
Transformer
33×𝑯×𝑾×𝒄 ×2↓ ×2↓ ×2↓
𝑯 𝟖×𝑾 𝟖×𝟖𝒄 T br loan
cb ksl
f
o soc hrk
am re inr g
Convolution Layer
C Concatenation
+ Addition
Prompt Block Prompt Block Prompt Block ×2↓ Down-sampling
×2↑ Up-sampling
C C C
1×1 1×1 Prompt Block
Restored Image 1×1
×2↑
F1 CoP mro pm onp et
nts
+ 3×3𝑯×𝑾×𝒄
×2↑ ×2↑
𝑯 𝟒×𝑾 𝟒×𝟒𝒄 SoG
1
fA
t×
MP
1
ax *
33×PF C1 Prom 11×p 33×t Blo 𝐅c ෠k
1
Fig.2: The overall pipeline of the proposed Dynamic Network (DyNet) for all-in-
one image restoration. DyNet enhances a low-quality input image using a four-level
encoder-decoder architecture. A distinctive aspect of DyNet lies in its weight-sharing
strategy:ateachlevel,theinitialtransformerblock’sweightsaresequentiallypassedto
subsequent blocks, significantly reducing the network’s parameters and enhancing its
flexibility. This approach allows for easy modification of DyNet’s complexity, toggling
between more bulkier or lightweight variants by adjusting the frequency of weight
sharing among the blocks at each encoder-decoder level. Moreover, we maintain the
encoder-decoderfeatureconsistencybyimplicitlylearningdegradation-awareprompts
at skip connections rather than on the decoder side as in PromptIR [36].
3.1 Dynamic Network (DyNet) Architecture
Our lightweight architecture is based on a novel weight-sharing mechanism for
IR models. This mechanism allows the network module’s weights to be reused
across subsequent modules in sequence, thereby significantly reducing the total
number of parameters and leading to a more efficient network structure. We
implement this weight-sharing approach within an encoder-decoder style archi-
tecture, which we term the Dynamic Network (DyNet). Within DyNet, at each
encoder-decoder level, module weights are shared across a pre-specified num-
berofsubsequentmodules.Thisnotonlyenhancescomputationalefficiencybut
also grants remarkable flexibility to the architecture. By simply altering the fre-
quencyofweightsharing,userscaneasilymodifythenetwork’sdepth,seamlessly
transitioning between bulkier or lightweight variants of DyNet.
Overall Pipeline. The overall pipeline of the proposed DyNet is shown in
Fig.2.TheproposedDyNetbeginsbyextractinglow-levelfeaturesF ∈RH×W×C
0
from a given degraded input image I ∈ RH×W×3 through a convolution op-
eration. Subsequently, the feature embedding undergoes a 4-level hierarchical
encoder-decoder,graduallytransformingintodeeplatentfeaturesF l∈RH 8×W 8 ×8C.
WeutilizetheexistingRestormer’s[52]Transformerblockasafundamentalfea-
ture extraction module. For every level of the encoder and decoder, we initialize
the weights for a first transformer block, which are subsequently reused across
the subsequent blocks in a series up to the specified frequency at that level.
For example, at a given encoder/decoder level, w1 denotes weights of the first6 Akshay Dudhane et al.
transformer block, we can define the output F of that encoder/decoder level,
out
given input features F as,
in
F =w1(F ) for b=1
out in
⟳
F = wb(F ) for b=2:f
out out
b=2:f
⟳
where, representsaselfloop,iterateforb=2:f;wb =w1 forb=2, 3, ··· , f.
Here, f denotes the module weights reuse frequency for a given encoder or de-
coder level.
We gradually increase the reuse frequency of the transformer block from the
top level to the bottom level, thereby increasing the network depth. The multi-
level encoder systematically reduces spatial resolution while increasing channel
capacity, facilitating the progressive extraction of low-resolution latent features
fromahigh-resolutioninputimage.Subsequently,themulti-leveldecodergradu-
allyrestoresthehigh-resolutionoutputfromtheselow-resolutionlatentfeatures.
To enhance the decoding process, we integrate prompt blocks [36], which learn
degradation-aware prompts implicitly through the prompt generation module
(PGM) followed by the prompt-interaction module (PIM). Unlike the existing
PromptIR [36], our approach incorporates degradation-aware implicit prompt
blocksatskipconnections.Thisimplicitpromptingviaskipconnectionsenables
the transfer of degradation-aware encoder features to the decoder side, thereby
enhancing the restoration process. This fundamental correction in the network
designgivesussignificantimprovementforall-in-oneimagerestorationtasks,as
detailed in the experimental section (Sec. 5).
Architectural details. At each encoder-decoder level, by simply changing the
module weights reuse frequency (f), we obtain DyNet’s bulky and lightweight
variants. Fig. 2 shows the core architecture of our DyNet. At each encoder-
decoder level, we initialize weights (w1) for the first transformer block [52] and
reuse them for the subsequent blocks. We vary the reuse frequency at each
encoder-decoder level and obtain our bulkier and lightweight variants. DyNet
comprises four encoder-decoder levels, with the bulky variant (DyNet-L) em-
ploying reuse frequencies of f = [4,6,6,8] at encoder-decoder level 1 to level 4,
respectively.Incontrast,thelightweightvariant(DyNet-S)appliesweightsreuse
frequenciesf =[2,3,3,4]forencoder-decoderlevel1tolevel4,respectively.Fur-
ther,weincorporateapromptblock[36]ateachskipconnection,whichenhances
the encoder features being transferred to the decoder from the encoder. Over-
all, there are three prompt blocks deployed at each level, each consisting of five
prompt components.
3.2 Dynamic Pre-training Strategy
In recent times, large-scale pre-training has become a key strategy to improve
given network performance. Initializing the network with pre-trained weightsDynamic Pre-training for All-in-One Image Restoration 7
Clean Image Prompts
Flag = 0
DyNet-L
Noise Addition Shares same underline Weight L1 Loss
JPEG, Gaussian, weights for network modules updation
Random
Random
DyNet-S
Masking Flag=1
Masked Input Reconstructed Target
Output
Prompts
Fig.3: Theproposeddynamicpre-trainingstrategyisshown.Givenacleanimage,we
createadegradedversionbyinjectingnoise(GaussianorRandom),JPEGartifactsand
random masking within the same image. Two variants (small and large) of our DyNet
are then trained in parallel branches to reconstruct the clean image from masked de-
graded inputs. Notably, the weights are shared between both variants (DyNet-S and
DyNet-L) since they are based on the same architecture but with an intra-network
weight-sharing scheme with varying frequencies of block repetition. One of the two
parallel branches is randomly activated in a single forward pass of the model (Flag
being the binary indicator variable to show branch activation). The dotted lines show
the network path is inactivated (Flag=0). An L1 loss is used to calculate pixel differ-
encesbetweenreconstructedoutputsandtargetstoupdatethesharedweightsofboth
branches. Both the inter and intra-model weight-sharing as well as random activation
of branches lead to a significant reduction in GPU hours required for pre-training.
providesarobustfoundation,boostingperformanceevenwithfewerparameters.
However, this strategy is resource-intensive, demanding considerable computa-
tionalpowerandGPUhours.Therefore,weproposeadynamicpre-trainingthat
is capable of simultaneous training of multiple variants of the network. These
networks are unified by shared network modules weight but varying network
depth. This strategy enables the concurrent training of a diverse set of models
tailoredtovariouscomputationalneedsandtaskcomplexities,allwhileleverag-
ing a shared baseline architecture. Through the proposed dynamic pre-training
strategy, we train both DyNet-L and DyNet-S versions of our DyNet concur-
rently in a single training session, achieving a 50% reduction in GPU hours.
DyNet-LandDyNet-Sutilizethesameunderlyingweightsforthetransformer
blocks initialized at each encoder-decoder level. They just differ in the reuse
frequency of these initialized transformer blocks at each encoder-decoder level.
Thus, during training iterations, we randomly alternate between DyNet-L and
DyNet-Sensuringtheoptimizationofthesharedunderlyingweightsasshownin
Fig.3.Moreover,weimprovethegeneralizationcapabilityofourDyNetvariants
by adopting an input masking strategy akin to that proposed in masked auto-
encoders [19]. We randomly mask portions of an image and train the DyNet8 Akshay Dudhane et al.
variants to reconstruct these masked regions in a self-supervised fashion. The
dataset used for this training is described below.
4 Million-IRD: A Dataset for Image Restoration
Large-scale pre-training for image restoration effectively demands large-scale,
high-qualityandhigh-resolutiondatasets.Thecurrentimagerestorationdatasets
[2,27,35],whencombined,offeronlyafewthousandimages.Thisisconsiderably
inadequate compared to the extensively large-scale datasets [41,42] utilized in
pre-training for other high-level tasks, such as visual recognition, object detec-
tion, and segmentation. The relatively small size of the existing training sets for
image restoration restricts the performance capabilities of the underlying net-
works.Moreimportantly,wearemotivatedbythescalinglawsinhigh-leveltasks
thatdemonstratethelarge-scalepre-tainingcanenableevenlightweight,efficient
model designs to reach the performance mark of much heavier models [1]. To
address this gap, we introduce a new million-scale dataset named Million-IRD
having∼2Mhigh-quality,high-resolutionimages,curatedspecificallyforthepre-
training of models for image restoration tasks. Examples from our Million-IRD
dataset are shown in the Fig. 4. Our data collection and pre-processing pipeline
are discussed below.
4.1 Data collection and Pre-processing
We combine the existing high-quality, high-resolution natural image datasets
suchasLSDIR[27],DIV2K[2],Flickr2K[35],andNTIRE[4].Collectivelythese
datasetshave90Kimageshavingspatialsizerangingbetween<10242,40962 >.
Apart from these datasets, existing Laion-HR [42] dataset has 170M unfiltered
high-resolution images of average spatial size 10242. However, these 170M sam-
ples are not directly suitable for model pre-training due to the predominant
presence of low-quality images. Examples of such low-quality images are il-
lustrated in Fig. 4 (on right side). Therefore, we focus on filtering out only
high-quality images by discarding those of low quality. Given the impractical-
ity of manual sorting of 170M images, we employ various image quality metrics
NIQE [33], BRISQUE [32], and NIMA [43]. Only images surpassing certain pre-
defined thresholds T , T , and T are selected; we empirically
NIQE BRISQUE NIMA
define thresholds for these metrics. With the mutual consensus of these metrics,
we effectively filter out low-quality images having poor textural details, exces-
sive flat areas, various artifacts like blur and noise, or unnatural content. By
processing nearly 100M images in the Laion-HR dataset [42], we sort out 2M
high-quality images having spatial resolution of 10242, or above. Examples of
these high-quality filtered images are presented in the Fig. 4 (on left side).
4.2 Data post-processing
Overall, our Million-IRD dataset has 2.09 million high-quality, high-resolution
images. Each image in our dataset comes with metadata detailing its down-Dynamic Pre-training for All-in-One Image Restoration 9
Sample Low-quality Images
Sample Images from Million-IRD Dataset
Rejected at Pre-processing Stage
Fig.4: On the left: Sample images from our Million-IRD dataset, which features a
diversecollectionofhigh-quality,high-resolutionphotographs.Thisincludesavarietyof
textures,scenesfromnature,sportsactivities,imagestakenduringthedayandatnight,
intricate textures, wildlife, shots captured from both close and distant perspectives,
forest scenes, pictures of monuments, etc. On the right: Sample low-quality images
filteredoutduringthedatapre-processingphase(Sec.4.1).Theseimageswereexcluded
due to being blurry, watermarked, predominantly featuring flat regions, representing
e-commerce product photos, or being noisy or corrupted from artifacts.
load link and the image resolution. A detailed breakdown of images and their
respective sources is provided in the supplementary material. We extract high-
resolution non-overlapping patches (of spatial size 5122) from each image, fol-
lowed by the application of a flat region detector which eliminates any patch
comprising more than 50% flat area. This post-processing phase allows us to as-
semble a pool of ∼8 million diverse image patches, tailored for the pre-training
phase of our DyNet variants.
5 Experiments
We validate the proposed DyNet across three key image restoration tasks: de-
hazing, deraining, and denoising. In line with the existing PromptIR [36], our
experiments are conducted under two distinct setups: (a) All-in-One, wherein a
singlemodelistrainedtohandleallthreedegradationtypes,and(b)Single-task,
where individual models are trained for each specific image restoration task.
5.1 Implementation Details
Dynamic Pre-training. For robust weights initialization, we conduct a dy-
namic pre-training for two variants of our DyNet, i.e., DyNet-L and DyNet-S.10 Akshay Dudhane et al.
Both variants share the same underline weights but differ in their transformer
block reuse frequency at each encoder-decoder level. For the encoder-decoder
levels 1 to 4, we set transformer block weights reuse frequencies of [4,6,6,8] for
DyNet-L and [2,3,3,4] for DyNet-S. We use all ∼8M patches of size 5122 from
ourMillion-IRDdatasetforthedynamicpre-training.Werandomlycropa1282
region from each patch, employing a batch size of 32. Each of these cropped
patchesundergoesarandomaugmentationto50%ofitsareausingeitherJPEG
compression, Gaussian noise, or random noise with varying levels. Furthermore,
we mask 30% of the patch, leaving the remaining portion unaltered. In total,
80% of the input patch undergoes modification, either through degradation or
masking. For weight optimization, we use the L1 loss and Adam optimizer with
parameters β = 0.9 and β = 0.999, setting the learning rate to 1e-4 for 1
1 2
million iterations. Using the setup described above, we simultaneously pre-train
both DyNet-L and DyNet-S. At any given iteration, we randomly switch be-
tween the two variants. Intriguingly, with each iteration, the shared underlying
weights are optimized as shown in Fig. 3. As a result, by the end of this single
pre-training session, we get DyNet-L and DyNet-S sharing the same trained un-
derlying weights but differ in network depth, making them suitable for various
challenges, including robustness and efficiency.
Datasets. For all-in-one and single-task implementations, we follow the same
trainingprotocolasexistingPromptIR[36]andfinetunethepre-trainedDyNet-
S and DyNet-L. Following [26,36], we prepare datasets for various restoration
tasks.Specifically,forimagedenoising,wecombinetheBSD400[5](400training
images) and WED [29] (4,744 images) datasets, adding Gaussian noise at levels
σ ∈ 15,25,50 to create noisy images. Testing is conducted on the BSD68 [30]
andUrban100[20]datasets.Forderaining,weuseRain100L[50]with200train-
ing and 100 testing clean-rainy image pairs. Dehazing employs the SOTS [25]
dataset, featuring 72,135 training and 500 testing images. To develop a uni-
fied model for all tasks, we merge these datasets, fine-tune pre-trained DyNet-L
and DyNet-S for 120 epochs and directly evaluate them across different tasks.
While, for a single task, the pre-trained DyNet-L is fine-tunned for 120 epochs
on a respective training set of the task.
5.2 Comparisons on multiple degradations under All-in-one setting
We benchmark our DyNet against a range of general image restoration mod-
els[12,17,44,54]andspecificall-in-onesolutions[16,26,36],asshowninTable1.
On average, across various tasks, DyNet-L and DyNet-S outperform the previ-
ously best PromptIR [36] by 0.82 dB and 0.59 dB, respectively, with DyNet-S
alsocuttingdownon56.75%ofparametersand31.34%ofGFlops.Forimagede-
noising,DyNet-Sachievesa1.09dBhigheraveragePSNRcomparedtoDL[16],
with Fig. 5 showcasing its ability to deliver noise-free images with improved
structuralintegrity.Notably,DyNet-Lsetsanewbenchmarkinimagederaining
and dehazing with improvements of 2.34 dB and 1.4 dB in PSNR, respectively.
Visual comparisons in Fig. 6 and 7 demonstrate DyNet’s superior capabilityDynamic Pre-training for All-in-One Image Restoration 11
Table1:ComparisonresultsintheAll-in-onerestorationsetting.OurDyNet-Lmodel
outperforms PromptIR [36] by 0.82 dB on average across tasks. Additionally, our
DyNet-S model achieves a 0.59 dB average improvement over PromptIR [36], with
reductions of 31.34% in parameters and 56.75% in GFlops.
Comparative Dehazing Deraining DenoisingonBSD68dataset[30]) Average
Methods onSOTS[25]onRain100L[16] σ=15 σ=25 σ=50
BRDNet[44] 23.23/0.895 27.42/0.895 32.26/0.898 29.76/0.836 26.34/0.836 27.80/0.843
LPNet[17] 20.84/0.828 24.88/0.784 26.47/0.778 24.77/0.748 21.26/0.552 23.64/0.738
FDGAN[12] 24.71/0.924 29.89/0.933 30.25/0.910 28.81/0.868 26.43/0.776 28.02/0.883
MPRNet[54] 25.28/0.954 33.57/0.954 33.54/0.927 30.89/0.880 27.56/0.779 30.17/0.899
DL[16] 26.92/0.391 32.62/0.931 33.05/0.914 30.41/0.861 26.90/0.740 29.98/0.875
AirNet[26] 27.94/0.962 34.90/0.967 33.92/0.933 31.26/0.888 28.00/0.797 31.20/0.910
PromptIR[36] 30.58/0.974 36.37/0.972 33.98/0.933 31.31/0.888 28.06/0.799 32.06/0.913
DyNet-S(Ours) 31.51/0.980 38.11/0.981 34.06/0.935 31.41/0.891 28.15/0.802 32.65/0.918
DyNet-L(Ours)31.98/0.981 38.71/0.983 34.11/0.93631.44/0.89228.18/0.80332.88/0.920
Noisy Image
PromptIR
DyNet (Ours)
Fig.5: Comparative analysis of image denoising by all-in-one methods on the BSD68
dataset [30]. Our DyNet-L reduces noise, producing more sharp and clear image com-
pared to the PromptIR [36].
Table 2: Dehazing results in the single-task setting on the SOTS benchmark
dataset [25]. Our DyNet-L achieves a boost of 0.76 dB over PromptIR [26].
DehazeNet[6]MSCNN[39]AODNet[24] EPDN[37] FDGAN[12] AirNet[26] Restormer[52]PromptIR[36] DyNet-L
22.46/0.851 22.06/0.908 20.29/0.877 22.57/0.863 23.15/0.921 23.18/0.900 30.87/0.969 31.31/0.973 32.07/0.982
in removing rain and haze, respectively, producing more cleaner images than
PromptIR [36].
5.3 Comparisons on single degradation
We assess DyNet-L’s efficacy in single-task settings, where distinct models are
tailored to specific restoration tasks, demonstrating the effectiveness of content-
adaptive prompting through prompt blocks. Our results, as shown in Table 2,12 Akshay Dudhane et al.
Rainy Image
PromptIR
DyNet (Ours)
Fig.6:Comparativeanalysisofimagederainingbyall-in-onemethodsontheRain100L
dataset [16]. Our DyNet-L successfully eliminates rain streaks, producing clear, rain-
free images as compared to the recent PromptIR [36].
Hazy Image
PromptIR
DyNet (Ours)
Fig.7: Comparative analysis of image dehazing by all-in-one methods on the SOTS
dataset[25].Ourapproachreduceshaze,producingmoreclearimagecomparedtothe
PromptIR [36].
Table 3: Deraining results in the single-task setting on Rain100L [16]. Compared to
the PromptIR [36], our method yields 1.68 dB PSNR improvement.
DIDMDN[55] UMR[51] SIRR[48] MSPFN[21] LPNet[17] AirNet[26] Restormer[52] PromptIR DyNet-L
23.79/0.773 32.39/0.92132.37/0.926 33.50/0.948 33.61/0.95834.90/0.977 36.74/0.978 37.04/0.97938.85/0.984
indicate DyNet-L’s superior performance with an improvement of 0.76 dB over
PromptIR [36] and 1.2 dB over Restormer [52] in dehazing. This pattern is con-
sistentacrossothertasks,includingderaininganddenoising.Notably,compared
to PromptIR [36], DyNet-L achieves performance gains of 1.81 dB in deraining
(refer to Table 3) and achieves a 0.13 dB improvement in denoising at a noise
level of σ =50 on the Urban100 dataset (Table 4).Dynamic Pre-training for All-in-One Image Restoration 13
Table 4: Denoising comparisons in the single-task setting on BSD68 [30] and Ur-
ban100 [20] datasets. For the challenging noise level of σ = 50 on Urban100 [20], our
DyNet-L obtains 0.64 dB gain compared to AirNet [26].
Comparative BSD68[30] Urban100[20]
Methods σ=15 σ=25 σ=50 σ=15 σ=25 σ=50
CBM3D[10] 33.50/0.922 30.69/0.868 27.36/0.763 33.93/0.941 31.36/0.909 27.93/0.840
DnCNN[57] 33.89/0.930 31.23/0.883 27.92/0.789 32.98/0.931 30.81/0.902 27.59/0.833
IRCNN[58] 33.87/0.929 31.18/0.882 27.88/0.790 27.59/0.833 31.20/0.909 27.70/0.840
FFDNet[59] 33.87/0.929 31.21/0.882 27.96/0.789 33.83/0.942 31.40/0.912 28.05/0.848
BRDNet[44] 34.10/0.929 31.43/0.885 28.16/0.794 34.42/0.946 31.99/0.919 28.56/0.858
AirNet[26] 34.14/0.936 31.48/0.893 28.23/0.806 34.40/0.949 32.10/0.924 28.88/0.871
Restormer[52] 34.29/0.937 31.64/0.895 28.41/0.810 34.67/0.969 32.41/0.927 29.31/0.878
PromptIR[36] 34.34/0.938 31.71/0.897 28.49/0.813 34.77/0.952 32.49/0.929 29.39/0.881
DyNet-L(Ours)34.40/0.93831.76/0.89728.53/0.81334.84/0.95232.60/0.93029.52/0.882
Table 5: Ablation experiments for different variants of the proposed DyNet.
Comparative Dehazing Deraining DenoisingonBSD68dataset[30]) Average Par GFlops
Methods onSOTS [25]onRain100L[16] σ=15 σ=25 σ=50
PromptIR[36] 30.58/0.974 36.37/0.972 33.98/0.933 31.31/0.888 28.06/0.799 32.06/0.913 37M242.355
(a)Resultswithoutmaskeddynamicpre-trainingonMillion-IRD
DyNet-S 30.10/0.976 37.10/0.975 33.94/0.931 31.28/0.880 28.03/0.789 32.09/0.910 16M 166.38
DyNet-L 30.67/0.977 37.68/0.975 33.97/0.933 31.31/0.889 28.04/0.797 32.33/0.914 16M 242.35
(b)Resultswithmaskeddynamicpre-trainingonMillion-IRD
DyNet-S 31.51/0.980 38.11/0.981 34.06/0.935 31.41/0.891 28.15/0.802 32.65/0.918 16M 166.38
DyNet-L 31.98/0.981 38.71/0.983 34.11/0.93631.44/0.89228.18/0.80332.88/0.92016M 242.35
5.4 Ablation studies
To study the impact of different components, we conduct various ablation ex-
periments in the all-in-one setting, as summarized in Table 5. Table 5(a) illus-
trates that our DyNet-L and DyNet-S achieves comparable performance to the
baseline PromptIR [36] with reductions of 56.75% in parameters and 31.34%
in GFlops, without the proposed dynamic pre-training on Million-IRD dataset.
This shows that the proposed fundamental correction of placing prompt block
at skip connections is effective compared to on the decoder side as given in
PromptIR [36]. Furthermore, pre-training on our Million-IRD dataset brings a
significant improvement, resulting in an average PSNR increase of 0.55 dB for
DyNet-LcomparedtotheDyNet-Lwithoutpre-trainingasshowninTable5(b).
AsimilareffectisobservedforourDyNet-Svariant.Moreover,ourproposeddy-
namic training strategy reduces GPU hours by 50% when training variants of
the DyNet. Overall, each of our contributions (i.e., implicit prompting via skip
connections, dynamic network architecture, Million-IRD dataset, and dynamic
pre-training strategy) synergistically enhances the performance of the proposed
approach,whilesignificantlyreducingGFlopsandlearningparameterscompared
to the baseline PromptIR [36].14 Akshay Dudhane et al.
Table 6: PerformanceoftheproposedDyNet-Sondifferentcombinationsofdegrada-
tion types i.e., removal of noise, rain, and haze.
Degradation DenoisingonBSD68dataset[30] Derainingon Dehazingon
Noise Rain Haze σ=15 σ=25 σ=50 Rain100L[16] SOTS[25]
✓ ✗ ✗ 34.30/0.938 31.65/0.897 28.42/0.813 - -
✗ ✓ ✗ - - - 38.72/0.984 -
✗ ✗ ✓ - - - - 31.45/0.980
✓ ✓ ✗ 34.19/0.936 31.54/0.894 28.30/0.808 38.61/0.984 -
✓ ✗ ✓ 34.11/0.935 31.45/0.893 28.18/0.802 - 31.69/0.981
✗ ✓ ✓ - - - 38.16/0.982 31.55/0.980
✓ ✓ ✓ 34.06/0.935 31.41/0.891 28.15/0.802 38.11/0.981 31.51/0.980
Training model with different combinations of degradation. In Table 6,
we compare the performance of our DyNet-S in all-in-one setting on aggregated
datasets, assessing how varying combinations of degradation types affect it’s ef-
fectiveness.Allthemodelsinthisablationstudyaretrainedfor80epochs.Here
in Table 6, we assess how varying combinations of degradation types (tasks)
influence the performance of our DyNet. Notably, the DyNet trained for a com-
bination of the derain and de-noise tasks exhibits better performance for image
derainingthantheDyNettrainedonthecombinationofderainanddehaze.This
shows that some degradations are more relevant for the model to benefit from
when trained in a joint manner.
6 Conclusion
This paper introduces a novel weight-sharing mechanism within the Dynamic
Network(DyNet)forefficientall-in-oneimagerestorationtasks,significantlyim-
provingcomputationalefficiencywithaperformanceboost.Byadjustingmodule
weight-reuse frequency, DyNet allows for seamless alternation between bulkier
and lightweight models. The proposed dynamic pre-training strategy simulta-
neously trains bulkier and lightweight models in a single training session. Thus
saving 50% GPU hours compared to traditional training strategy. The ablation
studyshowsthattheaccuracyofbothbulkyandlightweightmodelssignificantly
boosts with the proposed dynamic large-scale pre-training on our Million-IRD
dataset. Overall, our DyNet significantly improves the all-in-one image restora-
tion performance with a 31.34% reduction in GFlops and a 56.75% reduction in
parameters compared to the baseline models.
References
1. Abnar, S., Dehghani, M., Neyshabur, B., Sedghi, H.: Exploring the limits of large
scale pre-training. arXiv:2110.02095 (2021)
2. Agustsson,E.,Timofte,R.:Ntire2017challengeonsingleimagesuper-resolution:
Dataset and study. In: CVPR Workshops (2017)Dynamic Pre-training for All-in-One Image Restoration 15
3. Alghallabi, W., Dudhane, A., Zamir, W., Khan, S., Khan, F.S.: Accelerated mri
reconstructionviadynamicdeformablealignmentbasedtransformer.In:WMLMI
(2023)
4. Ancuti, C.O., Ancuti, C., Vasluianu, F.A., Timofte, R.: Ntire 2021 nonhomoge-
neous dehazing challenge report. In: CVPR (2021)
5. Arbelaez,P.,Maire,M.,Fowlkes,C.,Malik,J.:Contourdetectionandhierarchical
image segmentation. TPAMI (2011)
6. Cai, B., Xu, X., Jia, K., Qing, C., Tao, D.: Dehazenet: An end-to-end system for
single image haze removal. TIP (2016)
7. Chen, H., Wang, Y., Guo, T., Xu, C., Deng, Y., Liu, Z., Ma, S., Xu, C., Xu, C.,
Gao, W.: Pre-trained image processing transformer. In: CVPR (2021)
8. Chen, L., Chu, X., Zhang, X., Sun, J.: Simple baselines for image restoration. In:
ECCV (2022)
9. Cui, Y., Knoll, A.: Psnet: Towards efficient image restoration with self-attention.
IEEE RAL (2023)
10. Dabov,K.,Foi,A.,Katkovnik,V.,Egiazarian,K.:Colorimagedenoisingviasparse
3dcollaborativefilteringwithgroupingconstraintinluminance-chrominancespace.
In: ICIP (2007)
11. Dong,H.,Pan,J.,Xiang,L.,Hu,Z.,Zhang,X.,Wang,F.,Yang,M.H.:Multi-scale
boosted dehazing network with dense feature fusion. In: CVPR (2020)
12. Dong, Y., Liu, Y., Zhang, H., Chen, S., Qiao, Y.: Fd-gan: Generative adversarial
networks with fusion-discriminator for single image dehazing. In: AAAI (2020)
13. Dudhane, A., Biradar, K.M., Patil, P.W., Hambarde, P., Murala, S.: Varicolored
image de-hazing. In: CVPR (2020)
14. Dudhane,A.,Zamir,S.W.,Khan,S.,Khan,F.S.,Yang,M.H.:Burstimagerestora-
tion and enhancement. In: CVPR (2022)
15. Dudhane, A., Zamir, S.W., Khan, S., Khan, F.S., Yang, M.H.: Burstormer: Burst
image restoration and enhancement transformer. In: CVPR (2023)
16. Fan, Q., Chen, D., Yuan, L., Hua, G., Yu, N., Chen, B.: A general decoupled
learning framework for parameterized image operators. TPAMI (2019)
17. Gao, H., Tao, X., Shen, X., Jia, J.: Dynamic scene deblurring with parameter
selective sharing and nested skip connections. In: CVPR (2019)
18. Guo, L., Zha, Z., Ravishankar, S., Wen, B.: Self-convolution: A highly-efficient
operator for non-local image restoration. In: ICASSP (2021)
19. He,K.,Chen,X.,Xie,S.,Li,Y.,Dollár,P.,Girshick,R.:Maskedautoencodersare
scalable vision learners. In: CVPR (2022)
20. Huang,J.B.,Singh,A.,Ahuja,N.:Singleimagesuper-resolutionfromtransformed
self-exemplars. In: CVPR (2015)
21. Jiang,K.,Wang,Z.,Yi,P.,Chen,C.,Huang,B.,Luo,Y.,Ma,J.,Jiang,J.:Multi-
scale progressive fusion network for single image deraining. In: CVPR (2020)
22. Kligvasser, I., Shaham, T.R., Michaeli, T.: xunit: Learning a spatial activation
function for efficient image restoration. In: CVPR (2018)
23. Kong, F., Li, M., Liu, S., Liu, D., He, J., Bai, Y., Chen, F., Fu, L.: Residual local
feature network for efficient super-resolution. In: CVPR (2022)
24. Li,B.,Peng,X.,Wang,Z.,Xu,J.,Feng,D.:Aod-net:All-in-onedehazingnetwork.
In: ICCV (2017)
25. Li, B., Ren, W., Fu, D., Tao, D., Feng, D., Zeng, W., Wang, Z.: Benchmarking
single-image dehazing and beyond. TIP (2018)
26. Li, B., Liu, X., Hu, P., Wu, Z., Lv, J., Peng, X.: All-in-one image restoration for
unknown corruption. In: CVPR (2022)16 Akshay Dudhane et al.
27. Li, Y., Zhang, K., Liang, J., Cao, J., Liu, C., Gong, R., Zhang, Y., Tang, H., Liu,
Y., Demandolx, D., et al.: Lsdir: A large scale dataset for image restoration. In:
CVPR (2023)
28. Li,Y.,Zhang,Y.,Timofte,R.,VanGool,L.,Yu,L.,Li,Y.,Li,X.,Jiang,T.,Wu,
Q.,Han,M.,etal.:Ntire2023challengeonefficientsuper-resolution:Methodsand
results. In: CVPR (2023)
29. Ma, K., Duanmu, Z., Wu, Q., Wang, Z., Yong, H., Li, H., Zhang, L.: Waterloo
exploration database: New challenges for image quality assessment models. TIP
(2016)
30. Martin,D.,Fowlkes,C.,Tal,D.,Malik,J.:Adatabaseofhumansegmentednatural
images and its application to evaluating segmentation algorithms and measuring
ecological statistics. In: ICCV (2001)
31. Mehta, N., Dudhane, A., Murala, S., Zamir, S.W., Khan, S., Khan, F.S.: Gated
multi-resolutiontransfernetworkforburstrestorationandenhancement.In:CVPR
(2023)
32. Mittal, A., Moorthy, A.K., Bovik, A.C.: No-reference image quality assessment in
the spatial domain. TIP (2012)
33. Mittal, A., Soundararajan, R., Bovik, A.C.: Making a “completely blind” image
quality analyzer. IEEE SPL (2012)
34. Nah, S., Son, S., Lee, J., Lee, K.M.: Clean images are hard to reblur: Exploiting
the ill-posed inverse task for dynamic scene deblurring. In: ICLR (2022)
35. Online: Flickr2k. https://www.flickr.com/, accessed: 20204-03-05
36. Potlapalli,V.,Zamir,S.W.,Khan,S.,Khan,F.S.:Promptir:Promptingforall-in-
one blind image restoration. arXiv:2306.13090 (2023)
37. Qu, Y., Chen, Y., Huang, J., Xie, Y.: Enhanced pix2pix dehazing network. In:
CVPR (2019)
38. Ren,C.,He,X.,Wang,C.,Zhao,Z.:Adaptiveconsistencypriorbaseddeepnetwork
for image denoising. In: CVPR (2021)
39. Ren, W., Liu, S., Zhang, H., Pan, J., Cao, X., Yang, M.H.: Single image dehazing
via multi-scale convolutional neural networks. In: ECCV (2016)
40. Ren, W., Pan, J., Zhang, H., Cao, X., Yang, M.H.: Single image dehazing via
multi-scale convolutional neural networks with holistic edges. IJCV (2020)
41. Ridnik, T., Ben-Baruch, E., Noy, A., Zelnik-Manor, L.: Imagenet-21k pretraining
for the masses. arXiv:2104.10972 (2021)
42. Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti,
M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al.: Laion-5b: An open
large-scaledatasetfortrainingnextgenerationimage-textmodels.NeurIPS(2022)
43. Talebi, H., Milanfar, P.: Nima: Neural image assessment. TIP (2018)
44. Tian, C., Xu, Y., Zuo, W.: Image denoising using deep cnn with batch renormal-
ization. Neural Networks (2020)
45. Tsai, F.J., Peng, Y.T., Tsai, C.C., Lin, Y.Y., Lin, C.W.: BANet: A blur-aware
attention network for dynamic scene deblurring. TIP (2022)
46. Tu, Z., Talebi, H., Zhang, H., Yang, F., Milanfar, P., Bovik, A., Li, Y.: MAXIM:
Multi-axis MLP for image processing. In: CVPR (2022)
47. Wang, Z., Cun, X., Bao, J., Liu, J.: Uformer: A general u-shaped transformer for
image restoration. arXiv:2106.03106 (2021)
48. Wei, W., Meng, D., Zhao, Q., Xu, Z., Wu, Y.: Semi-supervised transfer learning
for image rain removal. In: CVPR (2019)
49. Xie,W.,Song,D.,Xu,C.,Xu,C.,Zhang,H.,Wang,Y.:Learningfrequency-aware
dynamic network for efficient super-resolution. In: ICCV (2021)Dynamic Pre-training for All-in-One Image Restoration 17
50. Yang,F.,Yang,H.,Fu,J.,Lu,H.,Guo,B.:Learningtexturetransformernetwork
for image super-resolution. In: CVPR (2020)
51. Yasarla, R., Patel, V.M.: Uncertainty guided multi-scale residual learning-using a
cycle spinning cnn for single image de-raining. In: CVPR (2019)
52. Zamir,S.W.,Arora,A.,Khan,S.,Hayat,M.,Khan,F.S.,Yang,M.H.:Restormer:
Efficient transformer for high-resolution image restoration. In: CVPR (2022)
53. Zamir, S.W., Arora, A., Khan, S., Hayat, M., Khan, F.S., Yang, M.H., Shao, L.:
CycleISP: Real image restoration via improved data synthesis. In: CVPR (2020)
54. Zamir, S.W., Arora, A., Khan, S., Hayat, M., Khan, F.S., Yang, M.H., Shao, L.:
Multi-stage progressive image restoration. In: CVPR (2021)
55. Zhang,H.,Patel,V.M.:Density-awaresingleimagede-rainingusingamulti-stream
dense network. In: CVPR (2018)
56. Zhang, K., Danelljan, M., Li, Y., Timofte, R., Liu, J., Tang, J., Wu, G., Zhu, Y.,
He, X., Xu, W., et al.: AIM 2020 challenge on efficient super-resolution: Methods
and results. In: ECCV Workshops (2020)
57. Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a gaussian denoiser:
Residual learning of deep cnn for image denoising. TIP (2017)
58. Zhang, K., Zuo, W., Gu, S., Zhang, L.: Learning deep CNN denoiser prior for
image restoration. In: CVPR (2017)
59. Zhang, K., Zuo, W., Zhang, L.: Ffdnet: Toward a fast and flexible solution for
cnn-based image denoising. TIP (2018)
60. Zhang, K., Luo, W., Zhong, Y., Ma, L., Stenger, B., Liu, W., Li, H.: Deblurring
by realistic blurring. In: CVPR (2020)
61. Zhao,H.,Gou,Y.,Li,B.,Peng,D.,Lv,J.,Peng,X.:Comprehensiveanddelicate:
An efficient transformer for image restoration. In: CVPR (2023)
62. Zhou, M., Huang, J., Guo, C.L., Li, C.: Fourmer: An efficient global modeling
paradigm for image restoration. In: ICLR (2023)18 Akshay Dudhane et al.
Supplemental Material
Here, we have discussed the details about the transformer block used in
our DyNet, additional sample images from our Million-IRD dataset, additional
visual results comparison between PromptIR [36] and the proposed DyNet-L in
all-in-one setting.
𝐅෠
m
11×33×
H 𝑙×W 𝑙×c 𝑙
ro
N
11×33× U L e
G
11×H 𝑙×W 𝑙×c 𝑙
𝐅෠
𝟏 Con Lv ao yl eu rtion
Depth-wise
O/P (b) Gated Dconv Feed-forward Network (GDFN) Convolution Layer
+ Addition
R Reshape
GDFN
Element-wise
HW× c Multiplication
11×33× R 𝑙 𝑙 𝑙 Matrix
Multiplication
MDTA
I/P
H 𝑙×F W1 𝑙×c 𝑙 m ro N 11×33× R c𝑙 × H𝑙 W𝑙 x aM tfo S
c×c
Transformer block 11×33× RH 𝑙 W 𝑙 ×c 𝑙 H𝑙
𝑙
W𝑙
𝑙
×c𝑙R H𝑙×W𝑙×c𝑙11× + 𝐅෠
(a) Multi-head Transposed Attention (MDTA)
Fig.S1: Overview of the Transformer block used in our DyNet network. The Trans-
former block is composed of two sub-modules, the Multi Dconv head transposed at-
tention module (MDTA) and the Gated Dconv feed-forward network (GDFN).
A Transformer Block Details
As described in Section 3.1 of the main manuscript, here, we have discussed
the transformer block utilized within our DyNet network architecture, detail-
ingitssub-modulesmultideconvheadtransposedattention(MDTA)andgated
deconv feed-forward network (GDFN). Initially, input features, represented as
R∈H ×W ×C ,areprocessedthroughtheMDTAmodule.Withinthis,Layer
l l l
normalization is used to normalize the input features. This is followed by the
application of 1×1 convolutions and then 3×3 depth-wise convolutions, which
servetotransformthefeaturesintoQuery(Q),Key(K),andValue(V)tensors.
A key feature of the MDTA module is its focus on calculating attention across
the channel dimensions, instead of the spatial dimensions, which significantly
reduces computational demands. For channel-wise attention, the Q and K ten-
sors are reshaped from H ×W ×C to H W ×C and C ×H W dimensions,
l l l l l l l l lDynamic Pre-training for All-in-One Image Restoration 19
respectively. This reshaping facilitates the computation of the dot product and
leadstoatransposedattentionmapofC ×C dimensions.Thisprocessincorpo-
l l
rates bias-free convolutions and executes attention computation simultaneously
across multiple heads. Following the MDTA Module, the features undergo fur-
ther processing in the GDFN module. Within this module, the input features
are initially expanded by a factor of γ through the use of 1×1 convolutions.
Subsequently, these expanded features are processed with 3×3 convolutions.
Thisprocedureisexecutedalongtwoparallelpathways.Theoutputfromoneof
thesepathwaysissubjectedtoactivationthroughtheGeLUnon-linearfunction.
The activated feature map is then merged with the output from the alternative
pathway via an element-wise multiplication.
B Visual Results Comparison
We show additional visual results comparison between PromptIR [36] and our
DyNet-L under from all-in-one setting.20 Akshay Dudhane et al.
Input Noisy Image PromptIR DyNet (Ours)
Fig.S2:Comparativeanalysisofimagedenoisingbyall-in-onemethodsontheBSD68
dataset [30]. Our DyNet-L reduces noise, producing more sharp and clear image com-
pared to the PromptIR [36].Dynamic Pre-training for All-in-One Image Restoration 21
Input Rainy Image PromptIR DyNet (Ours)
Fig.S3: Comparative analysis of image deraining by all-in-one methods on the
Rain100L dataset [16]. Our DyNet-L successfully eliminates rain streaks, producing
clear, rain-free images as compared to the recent PromptIR [36].22 Akshay Dudhane et al.
Input Hazy Image PromptIR DyNet (Ours)
Fig.S4: Comparative analysis of image dehazing by all-in-one methods on the SOTS
dataset[25].Ourapproachreduceshaze,producingmoreclearimagecomparedtothe
PromptIR [36].Dynamic Pre-training for All-in-One Image Restoration 23
Table S1: Overview of the number of images curated from each database for our
Million-IRD dataset.
Dataset NTIRE [4]DIV2K [2]Flikr2K [35]LSDIR [27]Laion-HR [42]
#Images 56 2,000 2,000 84,991 2M
Fig.S5: SampleimagesfromourMillion-IRDdataset,whichfeaturesadiversecollec-
tion of high-quality, high-resolution photographs. This includes a variety of textures,
scenes from nature, sports activities, images taken during the day and at night, intri-
cate textures, wildlife, shots captured from both close and distant perspectives, forest
scenes, pictures of monuments, etc.
C Breakdown of images in Our Million-IRD Dataset
We combine the existing high-quality, high-resolution natural image datasets
suchasLSDIR[27],DIV2K[2],Flickr2K[35],andNTIRE[4].Collectivelythese
datasetshave90Kimageshavingspatialsizerangingbetween<10242,40962 >.
TheirbreakdownisgiveninTableS1.Wealsoshowtheadditionalsampleimages
from our Million-IRD dataset in Fig. S5 and Fig. S6.24 Akshay Dudhane et al.
Fig.S6: SampleimagesfromourMillion-IRDdataset,whichfeaturesadiversecollec-
tion of high-quality, high-resolution photographs. This includes a variety of textures,
scenes from nature, sports activities, images taken during the day and at night, intri-
cate textures, wildlife, shots captured from both close and distant perspectives, forest
scenes, pictures of monuments, etc.