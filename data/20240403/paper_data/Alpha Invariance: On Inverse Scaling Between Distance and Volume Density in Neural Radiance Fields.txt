Alpha Invariance: On Inverse Scaling Between Distance and Volume Density in
Neural Radiance Fields
JoshuaAhn HaochenWang RaymondA.Yeh GregShakhnarovich
∗ ∗
UniversityofChicago TTI-Chicago PurdueUniversity TTI-Chicago
jjahn@uchicago.edu whc@ttic.edu rayyeh@purdue.edu greg@ttic.edu
Abstract
Scale-ambiguity in 3D scene dimensions leads to
magnitude-ambiguityofvolumetricdensitiesinneuralra-
diance fields, i.e., the densities double when scene size is
halved,andviceversa.Wecallthispropertyalphainvari-
ance. For NeRFs to better maintain alpha invariance, we
recommend 1) parameterizing both distance and volume
densitiesinlogspace,and2)adiscretization-agnosticini-
tializationstrategytoguaranteehighraytransmittance.We
revisit a few popular radiance field models and find that Figure1.Adiscretizedviewofvolumerendering.Top:arayiscut
thesesystemsusevariousheuristicstodealwithissuesaris- intointervals,eachwithadensityσi ≥ 0andintervallengthdi.
ing from scene scaling. We test their behaviors and show Bottom:illustrationoftheweightgiventothe3rdinterval,com-
our recipe to be more robust. Visit our project page at putedthroughalphacompositing.Therenderedcolorisobtained
https://pals.ttic.edu/p/alpha-invariance. byweightingalltheintervalcolorswiththeirwis.Ifwescaleeach
dibyaconstantk,scalingσiby k1 renderstheidenticalcolor.
robust algorithm should be able to perform consistently
1.Introduction across different scalings. We investigate how this notion
ofinvariancemanifestsitselfinpractice,discusshowthehy-
3Dcomputergraphicsandvisionarefundamentallyscale-
perparameterdecisionsaffectit,andproposeasolutionthat
ambiguous.Lengthsofobjectsandscenesareoftenunitless
ensuresrobustnesstodistancescalingacrosstheNeRFmeth-
andmeasureuptoaconstantratio.Thisisfineforroutines
ods. We revisit and experiment with a few popular NeRF
suchasprojection,triangulation,andcameramotionestima-
architectures:VanillaNeRF[20],TensoRF[6],DVGO[30],
tion.Acommonpracticeistonormalizethescenedimension
Plenoxels[8],andNerfacto[31],andfindthatmanysystems
orthesizeofareferenceobjecttoanarbitrarynumber.
usetailoredheuristicsthatworkwellataparticularscene
Volumerendering[17]usedbyNeuralRadianceFields
size.Weanalyzetheimpactofsomecriticalhyperparameters
(NeRFs) [20] presents a complication due to explicit inte-
whichareoftenoverlookedornothighlightedintheoriginal
gration over space. Since the distance scaling is arbitrary,
papers.
thevolumetricdensityfunctionσ(x)mustcompensatefor
Inourtestingofthesemodelsweidentifytwomainfail-
themultiplicativefactortorenderthesamefinalRGBcolor.
uremodes.Whenthesceneisscaleddown(shortrayinterval
Inotherwords,ifthescenesizeexpandsbyafactork,itis
d),somemodelsstruggletoproducelargeenoughσvalues
sufficientforthelearnedσtoshrinkby1/ktobeinvariant
forsolidgeometry.Whenthesceneisscaledup(longinter-
tothechange.Thesameappliestointegrationovercones
vallength),theσatinitializationisoftentoolarge,resulting
ormoregeneralspatialdatastructures[1,3].Notethatin
incloudinessthattrapstheoptimizationatbadlocaloptima.
addition to scene dimensions, the magnitude of σ is also
Wethereforepropose1)parameterizingbothdistanceand
influencedbythenumberofsamplesperrayandultimately
volumedensitiesinlogspaceforeasiermultiplicativescal-
thesharpnessofchangeinlocalopacity.
ing;2)aclosed-formformulaforσvalueinitializationthat
There is no single “correct” size for a scene setup. A
guaranteeshighraytransmittanceandscenetransparency.
*Equalcontribution. Weshowthatthesetwoingredientscanrobustlyhandlevari-
1
4202
rpA
2
]VC.sc[
1v55120.4042:viXraα=1 exp( relu(x) d) α=1 exp( softplus(x) d)=1 sigmoid( x)d α=1 exp( exp(x) d)=1 GumbelCDF( (x+logd))
− − · − − · − − − − · − −
1.0 d=1e3 1.0 d=1e3 1.0 d=1e3
d=1e2 d=1e2 d=1e2
0.8 d=1e1 0.8 d=1e1 0.8 d=1e1
d=1e0 d=1e0 d=1e0
0.6 d=1e-1 0.6 d=1e-1 0.6 d=1e-1
d=1e-2 d=1e-2 d=1e-2
d=1e-3 d=1e-3 d=1e-3
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
20 15 10 5 0 5 10 15 20 20 15 10 5 0 5 10 15 20 20 15 10 5 0 5 10 15 20
− − − − x − − − − x − − − − x
Figure2.αasafunctionoftherawinputxandd.Wefocusonαasafunctionofx,withdifferentactivationfunctionsσ(x),foraset
offixedvaluesofintervallengthsd.Theexpactivation(rightplot)leadstoasmooth,sigmoid-liketransitionfromlowtohighαvalues
regardlessoftheintervallengthd.Thefunctionexp(−exp(−x))istheCDFofGumbeldistribution,anditisanumericallystablerecipe
thatwerecommendovertrunc_expbecauselogdensityandlogdistancenaturallycancelouteachotherbeforeexponentiation.
ousscenesizes.Wearenotthefirsttouseexpactivationfor scenespecificembeddings[16],etc.Torenderanimage,we
volumetricdensities.ItwasdescribedinInstant-NGP[21] computetheexpectationbyintegrating(c,σ)overpointson
andadoptedbymanysubsequentworks.However,existing eachray.
informaldiscussionsforwhyexpshouldbeusedaresome- Foraraydiscretizedintosegmentseachwithlengthd ,
i
whatunsatisfactory.Webelievethataclearerunderstanding assumingconstantvolumedensityandcolorwithintheseg-
ofitsneedswouldbeofbenefit.Inaddition,sincedistance ment, volume rendering takes on the form of alpha com-
anddensitiescompensateforeachother,movingbothtolog positing. where the “over” operation [26] is applied in a
spacei.e.theGumbelCDFformnaturallyprovidesnumerical back-to-frontorder.SeeFig.1foratree-branchinganalogy.
stabilitywithouttheneedfortruncatedexp[32]. (cid:89)
α =1 e−σidi with w = (1 α ) α . (2)
Ourcontributions: i − i − j · i
j<i
• We discuss the notion of alpha invariance, and clarify
therelationshipofinversemultiplicativescalingbetween The final color is produced by the expectation w.r.t. the
(cid:80)
volumetricdensitiesσandscenesize.Itisabasicproperty probabilitymassfunction iw ic i.
thatappliestovolumerenderingandradiancefields. Current practices on σ parameterization. A scalar x
• WesurveyandablatepopularNeRFarchitecturesontheir predictedbytheunderlyingneuralfieldisconvertedtovol-
modelingdecisionsrelatedtoalphainvariance. umedensityσ(x)byanon-linearactivationfunction.The
• Weprovidearobust,generalrecipethatenablesNeRFsto choiceofthisactivationisoneofthedifferencesbetween
achieveconsistentviewsynthesisqualityacrossdifferent NeRFmodels,andthefocusofouranalysis.Itdetermines
scenescalings. how α in Eq. (2), which is a function of both d and σ(x),
dependsonx.
2.Background MLP-NeRF[20]parameterizesthevolumedensityσ(x)
withaReLUactivation.Mip-NeRF[1]advocatesfortheuse
Volume rendering. In a space permeated by fog, it is
of softplus activation, out of concern that ReLU might
unlikelyforafarawayphotontoarriveatthesensor.Onaray
get stuck since there is no gradient if inputs are negative.
z(t)=o+td,theprobabilityaphotoncomesfrombeyondt
DVGO[30]fixesthelocalintervalsize.TensoRF[6]scales
i.e.(t, )ismodelledbyadecayingtransmittancefunction
∞ (cid:82)t the local interval size by a constant. We refer the readers
T(t) = exp( − 0 σ(s)ds), which vanishes with larger t. to GitHub issues where these are discussed 2 3 . Plenox-
The volume density σ is some local light blocking factor,
els [8] uses ReLU with a very large learning rate on σ(x)
andthecumulativedistributionfunction(CDF)is1 T(t).
atthebeginningofoptimizationbeforedecayingitascon-
−
Theprob.densityw(t)ofaphotonstartingatexactlytimet vergenceimproves4.Instant-NGPusesexpactivation.The
isthus∂ (1 T(t)).Withaninfinitenumberofphotons,by
t motivation is not explained, besides a brief comment by
−
lawoflargenumbers,thedepositedcoloratthepixelisthe theauthoronGitHub5.Someofthesedecisionshavebeen
samplemeanwhichconvergestotheexpectation
adoptedbymorerecentworks.Forexample,HexPlane[5]
(cid:90) ∞ (cid:90) ∞ andLocalRF[18]followTensoRF’sintervalscalingstrategy,
E[c(t)]= w(t)c(t)dt= σ(t)T(t) c(t)dt. (1) whileworksbuildingontopofInstant-NGPtendtousethe
0 0 (cid:124) (cid:123)(cid:122) (cid:125) truncatedexpactivationfornumericalstability.
=∂t(1−T(t))
2https://github.com/sunset1995/DirectVoxGO/issues/7
ANeRFoptimizesamappingfromspatiallocationztocolor
3https://github.com/apchenstu/TensoRF/issues/14
andvolumedensityf θ : (z, ) (c,σ),where denotes 4https://github.com/sxyu/svox2/issues/111
∗ → ∗
auxiliary inputs such as viewing direction [20], time [27], 5https://github.com/NVlabs/instant-ngp/discussions/577
2
α α αAlgorithm 1 Python pseudocode for our 1) GumbelCDF d= L d= L×2 d= L d= L
64 64 128 64×128
densityactivationand2)hightransmittanceinitialization.It
α=0.5 11.1 5.5 22.2 1419.6
isnumericallystablesincelogdensities,logdistances,and
α=0.99 73.7 36.8 147.4 9431.4
hightransmittanceoffsetnaturallycanceloutoneanother
whenscenesizeLortheraysamplingstrategychanges. α=0.999 110.5 55.3 221.0 14147.1
class DensityField(): Table 1. Some example σ values given desired alpha level and
def __init__(self, L, tau=1.0, T=0.99): intervalsize,usingσ = −1log(1−α).LengthofrayLisset
# L could be far − near of a typical light ray d
# tau: expected std of the neural field output to4.0,whichisblendersyntheticdatasetdefault,andwevarythe
self.offset = \ numberofsamplesperray.DoublingLhalvestheσ,whilehigher
log(log(1/T)) − log(L) − (tau∗∗2)/2
self.encoder = YourDensityEncoder()
samplingresolutionincreasesσ.Intheextremecasewhereallthe
128importancesamplesfallwithinoneoftheinitial64uniformly
def compute_volrend_ws(self, xyz_locs, deltas): sampledbins,themagnitudeofσcangetverylarge.
# xyz_locs, deltas: shape [N_rays, N_samp]
log_densities = self.encoder(xyz_locs)
density_delta = exp(
showninTab.1,wherewesettheoverallraylengthagainto
log_densities
+ log(deltas) # GumbelCDF L=4.0andvaryL,thesamplingresolution,andthealpha
+ self.offset # high transmittance
threshold.
)
# log(deltas) + offset invariant w.r.t. L ThethreeactivationfunctionsmentionedinSec.2asused
trans = append_zeros_in_front(
inpracticetocomputeσ(x)leadtothefollowingformforα
cumsum(density_delta[..., :−1], dim=−1),
dim=−1 asafunctionofxandd:
)
trans = exp(−trans) ReLU:α=1 exp( ReLU(x) d) (3a)
alphas = 1. − exp(−density_delta) − − ·
weights = alphas ∗ trans softplus:α=1 sigmoid( x)d (3b)
return weights − −
exp:α=1 exp( exp(x) d) (3c)
− − ·
where we use the identity exp( softplus(x)) =
−
3.AlphaInvariance sigmoid( x).Tomoreclearlycomparetheeffectofexp
−
againstReLUandsoftplus,inFig.2wevisualizethebe-
AsreviewedinSec.2,w(t)=σ(t)T(t)formsaprobability
havior of α(x,d) with these activations, for different seg-
densityfunction.However,duetothescale-ambiguityofa mentlengthdfrom10−3to103.Whendissmall,bothReLU
3Dscene,thesizeofthesupportforw(t)isarbitrary.For
andsoftplusstruggletoproducelargeαvalues;thevalues
exampleintheoriginalNeRFblendersyntheticdataset,a
ofxthatthenetworksmustpredictbecomeextreme.Incon-
lightraytravelsover[2.0,6.0],correspondingtoraylength
trast,withtheexpactivationσ =exp(x)aconstantoffset
(scenescale)L=4.0.Ifweweretoexpandthesupportby
onxresultsinadirectscalingonσ.
2 ×toL = 8.0,thenbychangeofvariables,itissufficient Wealsonotethatthefunctione−e−x istheCDFofGum-
fortheprobabilitydensityw(t)andthusthevolumedensity
beldistribution,
σ(t) to scale down by 1 to integrate to 1 and render the
2
samecolor.Ofcourseforagiven3Dscene,nomatterhow α=1 exp( exp(x) d)
− − ·
thedistanceunitchanges,thecumulativeopacityofafixed =1 exp( exp(x+logd))
pieceofraysegmentisconstant.Inotherwords,adesired − −
=1 GumbelCDF( (x+logd)). (4)
propertyofamodelisthatvaluesofαinEq.(2)shouldbe − −
invariantwithrespectto(arbitrary)scenescaling.Werefer Thisconnectionisrelatedtothefactthatthevolumerender-
tothisdesiredpropertyasalphainvariance. ingequationisarestatementoftheExponentialdistribution.
Themagnitudeofvolumedensityisalsoaffectedbythe The PDF and CDF functions in Eq. (1) are the PDF and
sampling resolution oneach ray. Most NeRFvariants use CDF of the Exponential distribution assuming a constant
someformofimportancesamplingtoiterativelyfocusthe “rate”.Thiscanaidinnumericalstability,acommonlycited
computationonthefine,detailedstructuresthatwouldlikely issue with using exp to parametrize σ. Large density σ is
bemissedbytheinitialuniformsamples.Consideranex- neededprimarilytocreatesharpopacitychangeinasmall
tremelycoarsesamplingwithonly2sampledpoints:each distanceinterval.Ifwemovethedistancemultiplicationinto
intervalisunusuallylarge,andthedensityneededtoachieve theexpasalogdaddition,thenthereshouldnotbeanover-
ahighalphawouldbesmall.Ontheotherhand,withfine- flowissue.Notethatthederivativeofexp( exp(x))isnot
−
grainedimportancesampling,largedensitiesareneededto symmetricalabouty-axiseventhoughitsshaperesembles
createasharp,solidgeometrywithinthesmallinterval.We sigmoid.
cancalculatethevolumedensityrequiredtoachieveacer- While exp activation has no problem producing large
tainalphavalueby 1log(1 α).Someexamplevaluesare densitieswhenscenescaleLissmall,adifferentchallenge
−d −
3100 coarse fine coarse fine coarse fine 104 This is the desired target for initializing the density-
predictingnetwork.Settingthemeanµofthepre-activation
fieldoutputtothisvaluecanbedonewithanadditiveoffset.
90
103 This strategy is discretization agnostic, and a factor mul-
tiplied onto L can be undone by a logarithmic shift. The
80
mergedexpressionforlocalalphaasafunctionoffieldout-
102
putxandintervallengthdis
70
α= 1 exp( exp(x+logd+µ))
101 − −
60 = 1 exp(cid:16) exp(x+log d +loglog 1 τ2 )(cid:17) ,(8)
− − L T′ − 2
0 0
k=0.1 k=1.0 k=10.0 wherelog d isinvariantw.r.t.scenescaling.SeeAlg.1for
L
distancescale Pythonpseudocode.
Forareal-lifescenewhereraylengthvaries,apossible
Figure3.Distributionofvolumedensityσinthelegobulldozer
scene,queriedviaauniformlysampleddensegridofpointsfrom choiceistosetLtothelongestraydistance.Oneinterpre-
boththecoarse(leftcolumns)andfine(rightcolumns)MLPnet- tationoflog d isthatweareworkingwithdistanceratios
L
worksofvanilla-NeRFfordifferentk.Colorrepresentstheaverage andarethuseffectivelyhardcodingtheoverallscenesizeto
σ valueineachpercentileofthesortedσ distribution.Thefine 1.0.Butit’sonlypossiblewiththeextraoffsetterms,with-
MLPsproducelargerσthanthecoarseMLPs,andaskincreases, outwhich,assumingτ =1.0, T′ =0.99,thescenesizeL
themagnitudeofσdecreasesforbothnetworks. needstobesetto0.006.
AnalogousrecipescanbemadeforReLUandsoftplus,
occurswhenLislarge.Giventhesamesamplingstrategy,
although it’s harder to write down closed-form expres-
largerLandthereforelargerintervald,withthesameinitial
sions[34].WecouldattemptitusingMonteCarloestimates
volumedensitywillproducelargeralphavalues.Thismani- of their mean. Recall our goal is to achieve E[σ] L =
festsasanopaqueinitialscene.Optimizationfrequentlyfails ·
log( 1 ).Assumingthepre-activationneuralfieldoutputis
sincetheimagescanbeexplainedawaybydense,cloudy T′
drawnfrom (0,1),theexpectationsofthepost-activation
floatersinfrontofeachcamera.Itisimportanttoguarantee N
random variables can be approximated by sample mean.
thatuponinitializationthesceneistransparent,i.e.eachray
WhenT′ = 0.99,theRHSis0.001.Wewanttoavoid di-
hashightransmittance.
rectlyscalingdowndensityorLtomeetthistarget,since
Sinceraytransmittanceisrelatedtovolumedensityby
thatwouldmakeitevenharderforreluandsoftplusto
(cid:82)L
T(L)=exp( σ(s)ds),wemaywantthenetworkpre- achieve large density needed at solid regions. Instead the
− 0
dicting the density to be initialized so that the “average hightransmittanceshouldbeachievedbyshiftingtheactiva-
volumetransmittance”inthesceneisT′,avaluelike0.99.
tionfunctionprofiletotheright,i.e.addingnegativeoffset
Inpracticetheinitialvolumedensityvaluealongarayisnot totheinput.Thecorrectoffsetcanbetunednumerically,and
aconstant,butarandomvariableproducedbytheunderly- onlyneedstobedoneonce.
ingfieldfunctionsuchasvoxelsoranMLP.Wemakethe
simplifyingassumptionthatthesampledvaluesarei.i.d.(ad-
4.Experiments
mittedlyimperfect,sinceneighboringvaluestendtobecorre-
latedbyfeatureorvoxelsharing).Theintegral
(cid:82)L
σ(s)ds ScenesizeLisfundamentallyanarbitrarydecisionforNeRF
givestheMonteCarloestimatethatconverges− to 0 L E[σ]. optimization.Arobustalgorithmshouldbeabletoachieve
− ·
Ourrecommendationcanthenbewrittenas consistentviewsynthesisqualityregardlessofthevalueofL.
WeanalyzedifferenttypesofNeRFsystemsincludingpure
exp( L E[σ])=T′. (5)
− · MLPstoVoxelstoHashgrid-MLPhybridsbytrainingthem
Incaseofσ(x)=exp(x),assumingthepre-activationfield withascalingfactorkappliedonthedefaultscenesizeL,
outputxisdrawnfrom (µ,τ2),σ followsalog-normal andleavetherestoftheoptimizationhyperparameterssuch
N
distributionwithmeanE[σ]=exp(µ+ τ2 ).Pluggingthis assamplingstrategiesunchanged.Inourexperiments,unless
intoEq.(5),takinglog,andrearranging,w2 eget otherwisestated,wesetthedesiredtransmittanceT′ =99%
forthelongestrayineachscene,andsetktorangefrom0.1
τ2 1 1
exp(µ+ )= log , (6) toupto25,reportthePSNRmetrics,andprovidequalitative
2 L T′ visualizations. We find that these methods are unable to
andsolvingforµweobtain maintainhighrenderingqualityespeciallywhenkismore
1 τ2 extreme, whereas our recipe achieves a consistently high
µ=loglog logL . (7) qualityacrossallk.
T′ − − 2
4
elitnecrep )gol(eulavσgroundtruth σ1(k=1.0) σ2(k=10.0) σ3(k=0.1) σratio(σ1/σ2) σratio(σ1/σ3)
104
σ=462.7 σ=54.5 σ=2961.7 103
102
101
100
10−1
10−2
10−3
10−4
103
102
101
100
10−1
σ=211.4 σ=10.4 σ=1741.2
10−2
10−3
104
σ=118.3 σ=11.3 σ=1010.3
102
100
10−2
10−4
103
102
101
100
10−1
σ=413.0 σ=42.1 σ=3652.4 10−2
10−3
Figure4.Valuesofvolumedensityσontheobjectsurface,withmodelstrainedunderdifferentscenescalingsk.Oneachray,surfacepoint
isdefinedasthe50thpercentilelocationofvolumerenderingCDF.Weannotateafewprominentpoints,andalsoproducetwoσdivision
imagesthatshowtheoverallratioofthenumericalrangeofσatdifferentscalingfactors.TheblenderscenesaretrainedwithvanillaNeRF,
andtheMip-NeRF360scenesaretrainedwithNerfacto.Theratioofσisempiricallycloseto1/k.
4.1.Howvolumedensityσ changesinpractice thenumericalrangeofσdoesdecreaseaswehypothesized
inboththecoarseandfineMLPnetworks.Inaddition,we
Findings1. Empiricallyσchangesbyafactorcloseto
observethatthefineMLPsproduceaverysmallamountof
1 whenscenesizeischangedbyk.Itismostnoticeable
k significantly larger σ values than the coarse MLPs due to
onobjectsurfaces.
sharpergeometrybeingcapturedbytheimportancesampler
overthecourseofoptimization.
Inversescalingbetweenvolumedensityanddistanceissuffi-
Surfacestatistics. Sincelargevolumedensityoccursnear
cienttoguaranteeidenticalrenderingsforaradiancefield.
theobjectsurface,wevisualizethechangingsurfacedensi-
However,thismightnotbenecessaryinpracticeiftheend
tiesinFig.4.Atagivencamerapose,weshootraysthrough
goalistoachievehighPSNRvalues.Asceneistypically
eachpixeltoobtainthedensityhistograms w ,andquery
dominated by either empty space or solid regions; semi- i
{ }
thespatiallocation atthe50-thpercentileof theprobabil-
transparentstructuresoccurmuchlessfrequently,andvolu-
ity CDF of each histogram for its σ value. We also dis-
metricdensitiestendtotakeonextremevalues.Assuch,a
play heatmaps of density ratio by dividing the σ images
changeinthescaleoftheintervallengthmightnotsubstan-
coordinate-wise.Valuesbelowϵ=10−4areroundeduptoϵ
tiallyalterthelocalalphavalues.Togetabetterunderstand-
fornumericalstability.Basedonthesevisualizations,weob-
ingoftheirexactempiricalbehaviors,wetrainvanilla-NeRF
servethatinverse-scalingbetweendensityanddistancedoes
onthelegoanddrumsscenesfromtheblenderdataset,and
holdtoasignificantdegree,althoughthefactorinpractice
Nerfacto [31] on the garden and bicycle scenes from the
deviatesfrom1/k.
Mip-NeRF360dataset,withk = [0.1,1.0,10.0].Herewe
useourproposedAlg.1.
4.2.VanillaNeRFwithMLPs
Volumestatistics. ForthecoarseandfineMLPnetworks
Findings2. MLPsaresurprisinglyrobustwithjustReLU
invanilla-NeRF,wequeryadensegridof2003 uniformly
activationbuttheycanbeimproved.
sampledpointsinthescenebox,andsummarizethesorted
σdistributionwithbarplotsdepictingtheaverageσvalue ThecanonicalNeRFarchitectureusesan8-layerMLPwitha
within each percentile in Fig. 3. As most of the scene is ReLUactivationtoproduceσ,andhencemustoutputvalues
empty(>60%),wefocusonthedistributionofvaluesinthe intherangeofhundredsnearobjectsurfacesasshownin
upperpercentilesandobservethatwithincreasingkvalues, Fig.4.Thissetupissomewhatagainsttheconventionalwis-
5
ogel
smurd
nedrag
elcycib
)gol(σ
)gol(oitarσ
)gol(σ
)gol(oitarσchair drums ficus hotdog lego materials mic ship
k=0.1 30.98/31.19 24.25/24.37 28.72/29.26 32.22/34.84 30.90/30.64 28.27/28.45 28.31/30.96 26.70/27.56
k=0.4 31.21/31.29 23.85/24.54 28.74/29.01 32.95/33.51 9.45/30.71 28.02/28.59 31.41/30.94 10.45/27.35
k=1.0 31.26/31.26 12.04/24.58 28.76/28.85 33.59/33.71 30.71/31.36 13.89/28.57 31.35/31.05 25.65/27.21
k=2.5 31.22/31.32 24.04/23.99 26.86/28.60 10.94/33.91 30.74/31.11 27.76/28.85 30.97/31.35 5.88/27.15
k=10.0 14.04/31.36 23.84/24.43 28.97/28.92 10.39/33.23 30.55/30.84 27.97/28.25 31.04/31.44 5.88/26.99
Table2.PSNR↑forNeRF-Pytorch[baseline/ours]atdifferentscenescalingkonBlendersyntheticdataset.VanillaNeRFwithReLU
activationissurprisinglycapableofproducinglargeσvaluesbuttheoptimizationdoesconvergetopoorlocalminimarandomly.Using
ourrecommendedrecipeensuresconsistentconvergenceacrossallk.NotethattheNeRF-Pytorchcodebaseisunabletoexactlymatchthe
originalNeRF[20]performanceat200kiterations.Seeappendixfordetailsonreproducingtherandomfailurespresenthere.
dom[4,11]offixingtheneuralnetworkoutputmagnitudeto
1.0
unitvarianceforstabletraining.WetestourrecipeonNeRF-
PyTorch [38], train for 200k steps and present the results
0.8
inTab.2.Wemakethreeobservations.First,NeRF-PyTorch
1.00
usesPyTorch’sdefaultlinearlayerinitializationwhichdoes
0.6
notcorrectforthevariancegainofReLU,andproducesraw
0.98
∆1 ∆2
outputswithmean0andvariance0.Therearerandomfail- 0.4 ∆1=7.14
ure modes across all k. To address this, we initialize the
∆2=52.22
0.96
layers with Kaiming uniform initialization [10] with √2 0.2 lrinit=30.0
gain.Interestingly,westillobserverandomfailuremodes, lrinit=0.3
lrinit=0.003
evenatk =1.Finally,wetriedparameterizingdensitywith 0.0
ourrecipe(withoutapplyingtheaforementionedKaiming 10−2 10−1 100 101 102 103
σvalues(log)
uniforminitializationsuchthatτ =0.0initially,makingthe
taskmoredifficult),andobservedconsistentperformance Figure5.CDFoftheσdistributionsproducedbyPlenoxelsonthe
acrossallkwithoutanyrandomfailures. T-RexscenefromtheLLFFdatasetwithdifferentlearningrate
schedulesontheσvoxels.Thedefaulthighlearningrateschedule
4.3.VoxelVariants:DVGO,PlenoxelsandTensoRF
[red]isneededtoproducelargeσvaluesandhighPSNR.Lower
learningratesleadtosmallerσandultimatelyworseperformance.
Findings3. VoxelvariantscannotconvergewithReLUor
softplusactivationsalone.Hardcodedheuristicshave
withanunusuallylargeinitiallearningrateof30.0,inaddi-
beenusedforstabletraining.
tiontoareversecosinedecayontheσcoefficients.Disabling
UnlikeMLPs,directoptimizationofvoxelNeRFswithReLU it,i.e.reducingthelearningratetoalowervaluesuchas0.3
orsoftplusactivationsdoesnotconverge.Weobservethe or0.003,producessmallerσ,andisdirectlycorrelatedwith
same pattern across all three voxel model variants, and it worserenderingquality.SeeFig.5andTab.5.
showsthebenefitofhavinganMLPasaglobalinductive
bias.DVGOusessoftplusactivation,andtoaddressthe 4.4.MLPandHashgridHybrids
issueofdivergence,itfixesacanonicalscenesizewiththe
Findings4. TheMLP+Hashgridhybridwithexpacti-
voxel size ratio such that each interval has length 0.5 or
vationmakesthemodelrobustatsmallk.However,high
1.0dependingonwhetheritisinthecoarseorfinerecon-
transmittanceoffsetisneededforlargek.
structionstage.Itthencalculatesadensityoffsetsuchthat
everylocalcell’salphavalueissmallatinitialization.We Instant-NGP [21] and many follow-ups, including Ner-
insteadrecommendinitializingthedensitynetworkbased facto [31], already use exp activation to parameterize σ.
on overall ray transmittance. TensoRF applies a constant HerewestudytheNerfactoarchitecture,sinceitsubsumes
intervalscalingof25with 10offsetonsoftplusinput. manyofInstant-NGP’scomponentsandsupportsadditional
−
Weverifytherobustnessofouroverallrecipeincomparison featureslikeMip-NeRF360’sscenecontraction[2].While
withTensoRF’sandprovidenumericalmetricsinTab.3.We Nerfacto is robust at producing large σ when k is small,
alsoablateourinitializationoffsetonTensoRF.Applying the optimization gets stuck in the “cloudiness trap” when
exp alone is not enough to make TensoRF robust across k is large, since the increased ray distances cause the al-
scenescalings;askincreases,weshowthatthedistribution phavaluestoincrease,leadingtoaninitiallyopaquescene.
of alpha values everywhere in the scene tends towards 1. SeeTab.6forexperimentalresultsontheMip-NeRF360
ApplyingourhightransmittanceoffsetenablesTensoRFto dataset.SeeFig.8forRGB-imageanddepth-mapcompar-
converge smoothly, regardless of k. See Fig. 6. Plenoxels isons.Ourhightransmittanceinitializationstrategyenables
usesatotalvariation(TV)regularizer,andReLUactivation smoothoptimizationacrossk.
6
FDCchair drums ficus hotdog lego materials mic ship
k=0.1 fail/35.41 fail/26.01 fail/33.81 fail/37.03 fail/36.47 fail/29.99 fail/34.52 fail/30.51
k=0.4 35.74/35.96 26.04/25.91 fail/34.00 37.31/36.93 36.20/35.93 30.14/30.10 fail/34.31 30.81/30.09
k=1.0 35.55/35.57 26.24/26.24 33.99/34.31 37.31/37.20 36.60/36.55 30.01/30.11 34.59/34.71 30.65/30.59
k=2.5 35.71/35.88 26.26/26.14 34.05/34.15 37.09/37.14 36.66/36.51 30.11/30.21 34.48/34.61 30.71/30.80
k=10.0 35.67/35.92 26.14/26.41 34.01/34.81 37.10/37.51 36.18/37.04 30.36/30.40 34.53/34.62 30.74/30.80
Table3.PSNRvaluesofTensoRF[baseline/ours]atdifferentscenescalingkonBlendersyntheticdataset.TensoRFappliesaninputoffset
−10beforesoftplus,followedbyanintervalmultiplierof25.Thehardcodeddecisionsarelesseffectiveatk=0.1.TensoRFisquite
robustatk=10sincethe−10isaratheraggressiveoffset.SeeFig2fortheeffectofshiftingthesoftplusactivation.
bicycle bonsai counter garden kitchen room stump average
[L ,L ]=[20,10000] 19.81 27.34 23.51 26.71 27.46 28.60 19.57 24.71
in out
[L ,L ]=[20,40] 22.27 28.41 25.19 26.23 28.23 28.72 23.74 26.11
in out
[L ,L ]=[20,20] 22.11 28.94 25.66 26.00 28.14 29.03 24.02 26.27
in out
[L ,L ]=[20,0] 22.25 28.46 25.03 26.11 27.77 28.44 18.42 25.21
in out
Table4.PSNR↑forNerfactoontheMip-NeRF360datasetatk=10.L =2·k=20(ignoringthesmallnear-plane)andweexplore
in
differentL fortransmittanceoffsetintheouterdome.Themetricraydistance10000removesthebackgroundinductivebiasasdescribed
out
inSec4.5anddegradesperformance.L =L istheeasiesttoimplementandhasgoodperformance.
out in
1.0 (a)nodensityshift (b)[Lin,Lout]=[20,nooffset]
0.8
0.6
0.4
(c)[Lin,Lout]=[20,10000] (d)[Lin,Lout]=[20,20]
0.2
averagew/oinitialization
averagew/initialization
0.0
10−2 10−1 100 101 102
distancescale(log)
Figure6.QueryingadenseuniformgridofsamplesfromTensoRF
trainedwithexpactivation.Weconsideralocationtobeemptyif
thelocalalphaisbelow0.01,andapplydistancescalingkranging Figure7.DepthmapsfromNerfactowithvarioushightransmit-
from 10−2 to 102. Each faint line corresponds to a scene from tanceoffsetsappliedonascenewithcontractedbackground.The
theblenderdataset;thesolidlineisthedatasetaverage.Withour datasetis“Bicycle”fromtheMip-NeRF360withscalingk=10.
hightransmittanceinitialization,wepreventover-densificationeven (a) No transmittance offset on foreground or background leads
whenkislarge. todivergence.(b)Applyingtheoffsetonlyontheinnerdomeis
fineforreconstructingtheforegroundobject,butleavesa“wall”
lr_init=30.0 lr_init=0.3 lr_init=0.003
of opacity on the background. (c) Setting L to be the metric
blender 31.66 29.51 23.51 out
raydistanceproducesmistyfloatersinnear-cameraregions.(d)
LLFF[19] 24.51 23.22 21.09
L =L performswellandiseasytoimplement.
out in
Table5.MeasuringPlenoxels’performancewithPSNR↑onthe
sampling strategy, which in this case is co-designed with
BlenderandLLFFdatasetswithdifferentlearningrateschedules.
samplesspacedlinearlyindisparity.Importantly,metricdis-
tances(int-space)beforethedisparitytransformareused
4.5.BackgroundContraction/DisparitySampling
astherayintervallengthsdforvolumerendering.Taking
Torepresentunboundedscenes,Mip-NeRF360[2]applies Nerfacto for example, the metric ray travels from 0.05 to
a contraction of (2 1 ) x if x > 1 so that distant 1000,withthelargestintervalsdcloseto700forsamples
− ∥x∥ ∥x∥ ∥ ∥2
pointswithinsufficientcameracoverageuselessmodelca- neartheoutersceneboundary.Itprovidesaninductivebias
pacity.Nerfstudio[31]andMeRF[28]modifyittobemore thatfarawaybackgroundregionshavealphavaluescloseto
suitable for voxel grids. This scene contraction is part of 1.0atinitialization.NotethatNerfacto’sraysamplerapplies
the underlying NN blackbox that is in principle orthogo- uniformsamplinginuncontractedspaceanddisparitysam-
nal to how we do volume rendering. What matters is the plingincontractedspace.Theselocationsarethenfurther
7
10.0<αfoegatnecrepnoinitialization withinitialization noinitialization withinitialization noinitialization withinitialization
Figure8.ImageanddepthmapsproducedbyNerfactoontheballroom,auditorium,andcourtroomscenesfromTanksandTemplesat
k=25,andthebonsai,room,andstumpscenesfromtheMip-NeRF360datasetatk=10.Notusinghightransmittanceoffsetcausesthe
optimizationtogetstuckwithcloudyfloaters.
bicycle bonsai counter garden kitchen room stump
k=0.1 22.51/22.38 28.71/28.86 25.22/25.16 26.26/26.63 28.04/28.24 28.70/28.71 23.42/23.48
k=0.4 22.52/22.53 28.39/28.43 24.76/25.24 26.62/26.87 28.21/28.15 28.58/28.77 23.50/23.77
k=1.0 22.62/22.48 28.11/28.71 24.71/24.70 26.75/26.61 27.31/28.15 28.90/28.91 23.45/23.62
k=2.5 22.61/22.52 28.81/28.40 24.96/24.96 26.76/26.67 27.42/27.72 28.73/28.50 18.59/23.53
k=10.0 12.42/22.40 13.91/28.50 24.55/25.26 14.35/26.47 27.69/28.21 11.27/28.89 15.91/23.81
Table6.PSNRvaluesof[baseline/ours]withdifferentscenescalingskontheMip-NeRF360dataset.ThebaselineisNerfacto,which
usestheexpactivation,andthusperformswellevenwhenkissmall.Usingahightransmittanceinitializationstrategylikeoursmakesthis
modelmorerobustatlargerscenescalings.
updated by importance sampling. Applying scene scaling sameoffsetisthereforeappliedtoeverypointinthescene.
with a purely disparity based ray sampler is problematic
sincesamplelocationsbarelymoveexceptforthelastfew 5.Conclusion
bins.WhenapplyingthehightransmittanceoffsetofEq.(7),
Wepresentandclarifytheconceptofalphainvariance.Vol-
ourinitialattemptistodividethescenebasedonthe(max)
ume density and scene size change inversely in order to
norm of sampled location. If x 1 we calculate the
∞
∥ ∥ ≤ maintainidenticallocalopacitiesinaradiancefield.Itisan
offset with L = 2.0 as usual. If x > 1, we try us-
in ∞
∥ ∥ implicationofthescale-ambiguityina3Dscene.Werecom-
ing L = 0.0, 2.0, 4.0 or 1000.0. L = 1000.0 would
out out
mendparameterizingbothdistanceandvolumedensitiesin
undothebackgroundinductivebiasandinpracticeresultsin
logspace,alongwithahightransmittanceoffsetforrobust
floatersinnear-cameraregions.Notapplyinganytransmit-
performanceacrossscenesizes,andverifytheapproachon
tanceoffsetatalli.e.L =0isnotgoodeitherwhenscene
out
afewcommonlyusedarchitecturesintheliterature.
scalingfactorkislarge.SeeTab.4and Fig.7forcompar-
isonsofvariousL atk =10.Ourfinalrecommendation Acknowledgements. We thank PALS, 3DL and Michael
out
istosimplyletL =L ,sothatthereisnoneedtowrite Maire’slabfortheiradviceonthemanuscript.Thiswork
out in
branchinglogicintheimplementationfor x 1.The wassupportedinpartbytheTRIUniversity2.0program.
∞
∥ ∥ ≤
8
BGR
htped
BGR
htpedReferences [18] AndreasMeuleman,Yu-LunLiu,ChenGao,Jia-BinHuang,
ChangilKim,MinH.Kim,andJohannesKopf.Progressively
[1] JonathanTBarron,BenMildenhall,MatthewTancik,Peter
optimizedlocalradiancefieldsforrobustviewsynthesis. In
Hedman,RicardoMartin-Brualla,andPratulPSrinivasan.
IEEEConf.Comput.Vis.PatternRecog.,2023. 2
Mip-nerf:Amultiscalerepresentationforanti-aliasingneural
[19] BenMildenhall,PratulP.Srinivasan,RodrigoOrtiz-Cayon,
radiancefields. InInt.Conf.Comput.Vis.,2021. 1,2
NimaKhademiKalantari,RaviRamamoorthi,RenNg,and
[2] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P
AbhishekKar. Locallightfieldfusion:Practicalviewsyn-
Srinivasan,andPeterHedman. Mip-NeRF360:Unbounded
thesis with prescriptive sampling guidelines. ACM Trans.
anti-aliasedneuralradiancefields. InIEEEConf.Comput.
Graph.,2019. 7
Vis.PatternRecog.,2022. 6,7
[20] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
[3] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.
JonathanTBarron,RaviRamamoorthi,andRenNg. Nerf:
Srinivasan,andPeterHedman. Zip-NeRF:Anti-aliasedgrid-
Representingscenesasneuralradiancefieldsforviewsynthe-
basedneuralradiancefields. InInt.Conf.Comput.Vis.,2023.
sis. InEur.Conf.Comput.Vis.,pages405–421,2020. 1,2,6,
1
11
[4] ChristopherMBishop. Neuralnetworksforpatternrecogni-
tion. Oxforduniversitypress,1995. 6 [21] ThomasMüller,AlexEvans,ChristophSchied,andAlexander
Keller. Instantneuralgraphicsprimitiveswithamultiresolu-
[5] AngCaoandJustinJohnson. Hexplane:Afastrepresentation
tionhashencoding. ACMTrans.Graph.,2022. 2,6
for dynamic scenes. In IEEE Conf. Comput. Vis. Pattern
Recog.,2023. 2 [22] Srinivasa G Narasimhan and Shree K Nayar. Chromatic
[6] AnpeiChen,ZexiangXu,AndreasGeiger,JingyiYu,and frameworkforvisioninbadweather. InIEEEConf.Comput.
Hao Su. Tensorf: Tensorial radiance fields. In Eur. Conf. Vis.PatternRecog.,2000. 11
Comput.Vis.,2022. 1,2 [23] ShreeKNayarandSrinivasaGNarasimhan. Visioninbad
[7] RaananFattal. Singleimagedehazing. ACMTrans.Graph., weather. InInt.Conf.Comput.Vis.,1999. 11
2008. 11 [24] MichaelNiemeyer,LarsMescheder,MichaelOechsle,and
[8] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong AndreasGeiger. Differentiablevolumetricrendering:Learn-
Chen,BenjaminRecht,andAngjooKanazawa.Plenoxels:Ra- ingimplicit3drepresentationswithout3dsupervision. In
diancefieldswithoutneuralnetworks.InIEEEConf.Comput. IEEEConf.Comput.Vis.PatternRecog.,2020. 11
Vis.PatternRecog.,2022. 1,2,12 [25] EricPennerandLiZhang. Soft3dreconstructionforview
[9] KaimingHe,JianSun,andXiaoouTang. Singleimagehaze synthesis. ACMTrans.Graph.,2017. 11
removalusingdarkchannelprior. IEEETrans.PatternAnal.
[26] ThomasPorterandTomDuff.Compositingdigitalimages.In
Mach.Intell.,2010. 11
ProceedingsofComputergraphicsandinteractivetechniques,
[10] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.
1984. 2
Delvingdeepintorectifiers:Surpassinghuman-levelperfor-
[27] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and
manceonImageNetclassification,2015. 6
FrancescMoreno-Noguer. D-NeRF:Neuralradiancefields
[11] Lei Huang, Jie Qin, Yi Zhou, Fan Zhu, Li Liu, and Ling
for dynamic scenes. In IEEE Conf. Comput. Vis. Pattern
Shao. Normalizationtechniquesintrainingdnns:Method-
Recog.,2021. 2
ology,analysisandapplication. IEEETrans.PatternAnal.
[28] ChristianReiser,RickSzeliski,DorVerbin,PratulSrinivasan,
Mach.Intell.,2023. 6
BenMildenhall,AndreasGeiger,JonBarron,andPeterHed-
[12] BernhardKerbl,GeorgiosKopanas,ThomasLeimkühler,and
man. MERF:Memory-efficientradiancefieldsforreal-time
GeorgeDrettakis. 3DGaussiansplattingforreal-timeradi-
viewsynthesisinunboundedscenes. ACMTrans.Graph.,
ancefieldrendering. ACMTrans.Graph.,2023. 11
2023. 7
[13] LeonidKeselmanandMartialHebert. Approximatedifferen-
[29] PratulPSrinivasan,RichardTucker,JonathanTBarron,Ravi
tiablerenderingwithalgebraicsurfaces.InEur.Conf.Comput.
Ramamoorthi,RenNg,andNoahSnavely.Pushingthebound-
Vis.,2022.
ariesofviewextrapolationwithmultiplaneimages. InIEEE
[14] LeonidKeselmanandMartialHebert. Flexibletechniques
Conf.Comput.Vis.PatternRecog.,2019. 11
fordifferentiablerenderingwith3dgaussians. arXivpreprint
arXiv:2308.14737,2023. 11 [30] ChengSun,MinSun,andHwann-TzongChen. Directvoxel
[15] JonathonLuiten,GeorgiosKopanas,BastianLeibe,andDeva gridoptimization:Super-fastconvergenceforradiancefields
Ramanan. Dynamic3dGaussians:Trackingbypersistent reconstruction. InIEEEConf.Comput.Vis.PatternRecog.,
dynamicviewsynthesis. arXivpreprintarXiv:2308.09713, 2022. 1,2
2023. 11 [31] MatthewTancik,EthanWeber,EvonneNg,RuilongLi,Brent
[16] RicardoMartin-Brualla,NohaRadwan,MehdiSMSajjadi, Yi, Justin Kerr, Terrance Wang, Alexander Kristoffersen,
JonathanTBarron,AlexeyDosovitskiy,andDanielDuck- JakeAustin,KamyarSalahi,AbhikAhuja,DavidMcAllis-
worth. NeRFinthewild:Neuralradiancefieldsforuncon- ter,andAngjooKanazawa. Nerfstudio:Amodularframe-
strainedphotocollections.InIEEEConf.Comput.Vis.Pattern workforneuralradiancefielddevelopment. arXivpreprint
Recog.,2021. 2 arXiv:2302.04264,2023. 1,5,6,7
[17] NelsonMax. Opticalmodelsfordirectvolumerendering. [32] Jiaxiang Tang. Torch-ngp: a PyTorch implementation of
IEEETrans.Vis.Comput.Graph.,1995. 1,11 instant-ngp,2022. https://github.com/ashawkey/torch-ngp. 2
9[33] PengWang,LingjieLiu,YuanLiu,ChristianTheobalt,Taku
Komura,andWenpingWang. Neus:Learningneuralimplicit
surfacesbyvolumerenderingformulti-viewreconstruction.
InAdv.NeuralInform.Process.Syst.,2021. 11
[34] Wikipedia. RectifiedGaussiandistribution—Wikipedia,the
freeencyclopedia. http://en.wikipedia.org/w/index.php?title=
Rectified%20Gaussian%20distribution&oldid=1140140064,
2023. 4
[35] GuanjunWu,TaoranYi,JieminFang,LingxiXie,Xiaopeng
Zhang,WeiWei,WenyuLiu,QiTian,andXinggangWang.
4dGaussiansplattingforreal-timedynamicscenerendering.
arXivpreprintarXiv:2310.08528,2023. 11
[36] LiorYariv,YoniKasten,DrorMoran,MeiravGalun,Matan
Atzmon,BasriRonen,andYaronLipman. Multiviewneural
surfacereconstructionbydisentanglinggeometryandappear-
ance. InAdv.NeuralInform.Process.Syst.,2020. 11
[37] LiorYariv,JiataoGu,YoniKasten,andYaronLipman. Vol-
umerenderingofneuralimplicitsurfaces. InAdv.Neural
Inform.Process.Syst.,2021. 11
[38] LinYen-Chen.NeRF-Pytorch.https://github.com/yenchenlin/
nerf-pytorch/,2020. 6,13
[39] TinghuiZhou,RichardTucker,JohnFlynn,GrahamFyffe,
andNoahSnavely. Stereomagnification:Learningviewsyn-
thesisusingmultiplaneimages. InSIGGRAPH,2018. 11
[40] CLawrenceZitnick,SingBingKang,MatthewUyttendaele,
Simon Winder, and Richard Szeliski. High-quality video
viewinterpolationusingalayeredrepresentation.ACMTrans.
Graph.,2004. 11
10Appendix
discovermissingstructures,3Dgaussians/metaballs[12–
14]arelessflexible;itreliesonSfMinitializationinsome
cases, and needs to explicitly optimize point shape, loca-
A1.RelatedWork
tion, and periodically remove, repopulate and merge-split
AlphaandBadWeather. Earlyworksonweathermodel- the primitives. 3DGS [12] uses an exponent activation to
ing[22,23]usevolumerenderingtocharacterizetheeffect scale the shape of each primitive, and it shares a similar
ofrainandfog.Dehazingmethods[7,9],apartfromsepa- motivationintermsofhandlingarbitraryscenescaling.
ratingthebasescenealbedofromthefoggyairlight,usethe
A2.TransmittanceinDiscreteSetting
estimatedalphamasktocomputeanup-to-scaledepthmap
proportionalto log(1 α).Thenotionofalphainvariance
− − InSec.3,wewritedowntheexpressionforhightransmit-
isimpliedinthisstep.
tanceinitializationinthecontinuoussetting.Theexpression
NeRFvsMPI. NeRFusesvolumerendering[17]forview isthesamewhentherayiscutintodiscreteintervals.The
synthesis [20] and other inverse rendering tasks . Unlike tree-branchinganalogyinFig.1showsthatthetransmittance
priorworks[25,29,39,40]thatdirectlymodelthealphas /survivalprobabilityforeachsegmentis1 α i,withoverall
ofafixedsetofmulti-planeimages(MPI),NeRFoptimizes survivalprobability(cid:81) (1 α i)=(cid:81) e−σi− di =e−(cid:80)σidi =
thecontinuousvolumetricdensitiesσ(x).MPIfixesthedis- exp(
(cid:82)L σds).Thesam−
estepsasEq.(5)fromSec.3fol-
− 0
cretizationinadvance,whereasthedensityparameterization low.
inNeRFmakesiteasytoadjustdiscretizationandresample
alongaray.Theflipsideofthisflexibility,however,isthat A3.AdditionalResults
themagnitudeofσ(x)istiedtothedomainofintegration.
Ourworkemphasizesthateventhoughσ(x)anddistance SurfaceStatistics.Weprovideadditionalvisualizationsveri-
changetocompensateforeachother,thealphavalueofa fyingalphainvarianceinFig.A2.SimilartoFig.4,weshoot
particulardiscretizedsegmentshouldremainconstant.The rays through each pixel to obtain the density histograms
opacityisaninvariantpropertyoflocalgeometry. w i ,andquerythespatiallocationatthe50-thpercentileof
{ }
theprobabilityCDFofeachhistogramforitsσvalue.Here,
Volume-renderedSDF. Signeddistancefunction(SDF)is
we showcase various scene types from different datasets,
suitableforproceduralcontentauthoringandsurfaceextrac-
andgeneratethesevisualizationswithdifferentarchitectures,
tion.SDFrenderingusedtorelyonfindingsurfaceintersec-
demonstrating how inverse scaling between distance and
tionsbyspheretracing[24,36],butrecentmethodssuchas
volumedensityisageneralizablephenomenoninradiance
NeuS[33]andVolSDF[37]movetothe“fuzzier”volume
fields.
renderingforeasieroptimization.Toperformvolumerender-
Voxel Variants, continued. DVGO has a strategy of fix-
ing,thedistancefunctionisfirsttransformedintovolumetric
ing the scene size to some canonical scale where the in-
densitiesbeforealphacompositing.VolSDFlearnsscaling
tervallengthbetweeneachsampledpointisdependenton
andshrinkingcoefficientsonthedistance-transformedvol-
thecurrentvoxelgridresolution’sratiotothebaseresolu-
umedensities.NeuSinsteaddemandstheCDFofvolume
tion,whichempiricallyequatestoeither0.5or1,depending
renderingtomatchtheshapeofascaled,horizontallyflipped
on the stages of optimization progress. We also note that
sigmoidi.e.1 T(t)=sigmoid(s SDF(t)),sothatthe
− ·− DVGO’ssamplingprocedureisstochastic,aseachrayhas
CDF’s derivative, in other words the weighting function
a potentially unique number of samples. The implication
w(t),hasitslocalmaximalocatedatSDFvalue0forpre-
isthateveryraywillhaveadifferentraylengthpurelyde-
cise surface level-set extraction. The learned parameter s
termined by its number of samples as the interval length
in the sigmoid acts as a global scaling coefficient on the
betweenanytwocontiguoussamplesishardcodedtoeither
implied volume densities. In this sense both formulations
0.5or1.Assuch,theterm“raylength”losesitsmeaningin
arealphainvariantbydefault,withtheextraconstraintthat
DVGO’scontextastherearenodeterministicnearandfar
themagnitudeofvolumedensityfunctionisgloballytiedto
planes;everyrayissimplydeconstructedintoitsconstituent
itssharpness.Theassumptionisrestrictivebutfineformost
samples.Weobservethatdisablingthisheuristic(i.e.,set-
usecases.
ting the interval lengths to be the true physical distances
GaussianSplatting. Gaussiansplatting[12,15,35]ren- andforcingeachraytobeboundedwithinapredetermined
dersanimagebyalphacompositingpointprimitives.Since globalnearandfarplane)producessignificantlyworsened
thescenerepresentationisbynaturediscrete,theconcept renderingquality,asshowninTab.A2.
ofvolumedensitydoesnotapply,asisthecasewithMPIs. ForPlenoxels,onlyrectifyingσwithexpisinsufficient
Theseexplicitprimitivescanbeefficientlyrasterizedusing tomaintainconvergenceatvariousscenescales;ahightrans-
GPUpipelines.WhereasNeRFcoulduseraysamplingstrate- mittance offset is needed even at k = 1. See Tab. A3 for
giesonthecontinuousdensityfieldtorefinelocaldetailsand numericalresults.Wenotethatourresultsareworse( 2-
∼
11Chair Ship
Default Ours Default Ours
k=0.1 31.20 31.14 31.04 14.04 28.82 31.20 31.27 31.07 30.99 31.25 5.88 5.88 27.59 27.61 5.88 27.20 27.06 27.56 27.51 27.31
k=0.4 28.96 31.32 31.26 31.17 31.26 31.00 31.29 31.02 31.24 31.09 27.57 27.46 27.59 27.35 27.49 27.11 27.14 27.00 27.11 27.84
k=1.0 14.04 31.32 14.04 14.04 28.82 31.24 31.30 31.11 31.39 31.23 27.56 25.64 27.27 25.57 27.60 27.18 26.99 27.16 27.30 27.11
k=2.5 14.04 31.37 28.99 31.38 31.32 31.14 31.23 31.15 31.03 31.23 27.50 27.45 5.88 27.49 27.56 27.56 27.31 27.35 27.18 27.16
k=10.0 14.04 30.97 28.76 31.00 9.75 31.29 31.19 31.16 31.10 31.22 5.88 27.57 27.35 24.12 27.43 26.99 27.00 27.16 27.27 27.28
average 25.76 7.79 31.18 0.10 22.89 8.54 27.23 0.20
± ± ± ±
TableA1.PSNR↑valuesofVanilla-NeRFonthechairandshipscenesfromtheBlenderdataset.Here,werun5experimentsfor5different
k-values,andcomparetheresultsfromthedefaultNeRFbaselineagainstourexpparametrizationonσandhightransmittanceinitialization.
Weobserveconsistentrenderingqualityacrossallrunsforbothsceneswithourmethod,butidentifyinconsistentrenderingqualityforthe
defaultconfiguration,withfailuremodesandpoorconvergencemarkedhereinred.
noinitialization withinitialization noinitialization withinitialization noinitialization withinitialization
FigureA1.RGB-imageanddepth-mapsproducedfromTensoRFonthelego,mic,chair,ship,materials,andhotdogscenesfromtheBlender
datasetatalargescenescalingk=25.Whenusingexpactivationtoparameterizeσ,notusingourhightransmittanceinitializationstrategy
causestheoptimizationtogetstuckwithcloudyfloaters.
3 dB) than the results presented in Plenoxels [8]. We had additional RGB-image and depth-map visuals in Fig. A1
tosignificantlylowerthelearningratetobemoresuitable demonstrating the benefits of our high transmittance ini-
for the exp activation, and believe that other changes in tialization in handling large scene scales when using exp
the hyperparameters are also needed to match the default activation.
performance.Weleavethisasfuturework;ourresultsstill
demonstrateconsistentrenderingqualityandaneedforour A4.Reproducibility
hightransmittanceinitializationstrategy.
In Tab. 2, we observe random failure modes when train-
BenefitofHighTransmittanceInitialization.Weprovide ingthevanilla8-layerMLPonvariousscenesusingNeRF-
12
BGR
htped
BGR
htpedchair drums ficus hotdog lego materials mic ship
disabled fail fail fail fail fail fail fail fail
default 34.10 25.40 32.56 36.67 34.53 29.71 33.23 28.76
fern flower fortress horns leaves orchids room trex
disabled 15.74 17.38 20.77 20.62 16.96 11.77 21.44 20.62
default 24.49 27.61 29.91 27.01 20.41 19.95 30.87 26.41
bicycle bonsai counter garden kitchen room stump
disabled fail fail fail fail fail fail fail
default 21.98 27.33 25.41 24.41 25.81 28.14 23.51
TableA2.PSNR↑valuesofDVGOontheBlender(toprow),LLFF(middlerow),andMip-NeRF360(bottomrow)datasets.DVGOsetsthe
intervallengthsdtotheratioofthecurrentvoxelgridresolutiontothebasevoxelgridresolutioninordertotomakethemodelindependent
ofscenesize;thisisreferredtoas‘default’inthetable.Weobservethatdisablingthisheuristic(i.e.,settingtheintervallengthsinthe
volumerenderingequationtobethetruephysicaldistancesbetweenanytwosamplepoints)resultsinDVGOfailingtorenderatahigh
quality.Thesefailuremodesaremarkedwithredintherowstitled‘disabled’.
chair drums ficus hotdog lego materials mic ship
k=0.1 31.80/31.89 24.23/24.26 29.41/29.58 34.12/34.31 31.20/31.31 28.21/28.41 31.11/31.31 28.16/28.25
k=0.4 30.33/31.83 23.61/24.23 27.65/29.12 31.76/34.46 29.61/31.22 26.06/28.02 29.04/31.13 27.01/28.13
k=1.0 27.06/32.00 21.66/24.37 24.77/29.39 27.31/34.56 26.22/31.35 23.14/28.31 24.85/31.29 24.58/28.19
k=2.5 17.49/32.43 17.24/24.44 18.89/29.83 20.20/34.72 16.64/31.53 15.51/28.64 15.89/31.61 16.67/28.22
k=10.0 13.05/32.25 10.84/24.45 11.40/30.23 13.03/34.93 11.64/31.69 9.57/28.79 15.89/31.61 11.27/28.21
TableA3.PSNR↑forPlenoxels[baseline/ours]atdifferentscenescalingk ontheBlenderdataset.Bydefault,Plenoxelsappliesa
verylargelearningratedirectlyonthecoefficientsofavoxelgrid,whereσisqueriedviatrilinearinterpolation,followedbyReLUto
ensurenon-negativedensityvalues.ThebaselinePlenoxelsmodelherereplacesReLUwithexpactivationandusesareducedlearning
rate(η =0.05,η =0.005)withnoreversecosinedelay.Ourmodelinadditionappliesahightransmittanceoffset.Asshown,this
init final
transmittanceoffsetisrequiredforhighrenderingqualityatvariousscenesizes,evenatk=1.
Pytorch [38] at git commit hash 63a5a63. To get a bet-
tersenseofthefrequencyofthesefailuremodes,wetrain
Vanilla-NeRF on chair and ship scenes 5 times for each
k-value,comparingthedefaultReLUparametrizationonσ
againstourhightransmittanceinitializationincombination
withexp.FullresultsareprovidedinTab.A1.Weattribute
therandomfailuremodestopoorinitializationoftheMLP
layers(theinitialoutputdistributionhas0meanand0vari-
ance)andtheinabilityofReLUtoprovideasmoothtransi-
tionfromlowtohighαvalues.SeeFig.2foravisualiza-
tion.However,evenwithsuchapoorlyinitializedMLP,our
parametrizationonσisenoughtoguaranteeconvergenceon
allrunsacrossalltestedkvalues.
WetrainNeRF-Pytorchforonly200kiterations.Alonger
trainingscheduleof500kiterationswouldpushtheNeRF-
Pytorchperformancecloser,butstillslightlybelowtheorigi-
nalNeRFnumbers.Theoverallconclusionsdonotchange.
13groundtruth σ1(k=1.0) σ2(k=10.0) σ3(k=0.1) σratio(σ1/σ2) σratio(σ1/σ3)
103
102
101
100
10−1
10−2
10−3
103
102
101
100
10−1
10−2
10−3
102
101
100
10−1
10−2
102
101
100
10−1
10−2
104
103
102
101
100
10−1
10−2
10−3
10−4
103
102
101
100
10−1
10−2
10−3
FigureA2.Visualizationofσ-imageproducedatthe50-thpercentilelocationofeachray’sdensityhistogramCDF.Weproduceadivision
imagethatshowstheglobaldifferenceofnumericalrangeacrossdifferentscenescalingfactork.Thefirstfourrowsareproducedfromthe
TensoRFarchitecture(thetoptworowsaretheficusandshipscenesfromtheblenderdataset;themiddletworowsarethefernandhorns
scenesfromtheLLFFdataset).ThebottomtworowsareproducedfromtheNerfactoarchitectureontheballroomandmuseumscenesfrom
theTanks-and-Templesdataset.AcrossavarietyofscenesandNeRFarchitectures,thesevisualizationsdemonstratethatthephenomenonof
alphainvarianceholdstoaverystrongdegree.
14
sucfi
pihs
nref
snroh
moorllab
muesum
)gol(σ
)gol(oitarσ
)gol(σ
)gol(oitarσ
)gol(σ
)gol(oitarσ