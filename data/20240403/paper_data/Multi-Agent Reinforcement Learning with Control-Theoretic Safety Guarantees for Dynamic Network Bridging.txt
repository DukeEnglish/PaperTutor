Multi-Agent Reinforcement Learning with Control-Theoretic Safety
Guarantees for Dynamic Network Bridging
Raffaele Galliera, Konstantinos Mitsopoulos, Niranjan Suri and Raffaele Romagnoli
Abstract—Addressing complex cooperative tasks in safety- learningapproachbuiltuponthemethodusedin[6],thatcan
critical environments poses significant challenges for Multi- learn to achieve complex objectives while ensuring safety
Agent Systems, especially under conditions of partial observ-
guarantees in distributed MAS.
ability.Thisworkintroducesahybridapproachthatintegrates
Multi-Agent Reinforcement Learning with control-theoretic To demonstrate and evaluate the effectiveness of our hy-
methodstoensuresafeandefficientdistributedstrategies.Our brid approach we consider the cooperative task of Dynamic
contributions include a novel setpoint update algorithm that
Network Bridging [7] (Fig. 1). In this task the goal is to
dynamically adjusts agents’ positions to preserve safety condi-
establish and maintain a connection between two moving
tions without compromising the mission’s objectives. Through
experimentalvalidation,wedemonstratesignificantadvantages targets A and B in a 2D plane, relying on a swarm of
overconventionalMARLstrategies,achievingcomparabletask N agents with limited observation of the environment. The
performance with zero safety violations. Our findings indicate agents must form an ad-hoc mobile network that establishes
thatintegratingsafecontrolwithlearningapproachesnotonly
a communication path between the two targets as they
enhancessafetycompliancebutalsoachievesgoodperformance
move, dynamically adjusting their positions to ensure an
in mission objectives.
uninterrupted multi-hop communication link.
I. INTRODUCTION
Despite the promising performance of Multi-Agent Re-
inforcement Learning (MARL) in simulations, its applica-
3
tion in real-world, safety-critical scenarios raises significant 2
concerns. A key challenge is collision avoidance, especially
in applications like autonomous driving, where failure to
preventcollisionscouldresultinsevereconsequences.More-
over, the inherent nature of Multi-agent Systems (MAS)
often involves partial observability, further increasing the
complexity of these problems by limiting the information
available to each agent about the environment and the states 1
of other agents.
B
This challenge highlights a fundamental limitation of all
ReinfocementLearning(RL)-basedapproaches:whileagents
learntooptimizepredefinedrewardfunctions,theirbehavior
lacks guarantees and remains inherently unpredictable. The
solerelianceonMARL’srewardfunctionstoensuresafetyis A
insufficient [1]. Even with extensive training and monitoring Fig. 1: Decentralized swarm coordination for dynamic net-
of an agent’s performance based on expected rewards, or work bridging
other performance measures, it is impossible to exhaustively
examineallpotential scenariosinwhichthe agentmightfail
to act safely or predictably. In this paper we introduce a hybrid approach combin-
Inensuringsafetyamongagents,variouscontrol-theoretic ing MARL and control-theoretic methods, accomplishing
approachesandhybridlearningapproacheshavebeendevel- task objectives while providing safety guarantees. Our key
oped, ranging from Model Predictive Control (MPC) [2] to contributions are: 1) A decentralized control framework
Reference Governor (RG) techniques [3], and from Control that is MARL compatible and restricts the effect of each
Barrier Functions (CBFs) [4] to Reachability Analysis (RA) agent’s movement updates to only its one-hop neighbors,
methods [5]. In this work, we focus on developing a hybrid enabling efficient local coordination; 2) An algorithm that
updates setpoints while preserving safety conditions through
R. Romagnoli is with the Department of Electrical and Computer En-
communication with affected neighbors; 3) An analytical
gineering, Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA
15213,USA:rromagno@andrew.cmu.edu computationallytractableconditionverifyingpotentialsafety
R. Galliera, N. Suri, and Konstantinos Mitsopoulos are with violations during updates. Overall our approach provides
the Institute for Human and Machine Cognition, 40 South Al-
effective coordination among agents while ensuring safety
caniz St, Pensacola, FL 32502, USA: kmitsopoulos@ihmc.org
rgalliera@ihmc.org nsuri@ihmc.org constraints.
4202
rpA
2
]AM.sc[
1v15510.4042:viXraII. PRELIMINARIES (1) around the equilibrium point. Therefore, the closed-loop
(CL) system (3) can be used to solve Lyapunov’s equation
In this section, we present the foundational theoretical
(5), which is used to compute the positively invariant set
elements,drawingfrombothcontroltheoryandMARL,that
(x ) (7). While for LTI systems, (x ) is positively
form the basis of our proposed hybrid approach. c sp c sp
E invariant for any positive c R, foE r nonlinear systems,
Consideralineartimeinvariant(LTI)systemexpressedin
∈
we need to verify that condition in a neighborhood of
the state space form
the equilibrium point x [8]. Furthermore, for nonlinear
sp
x˙ =Ax+Bu (1) systems, x has to satisfy f(x ,0)=0.
sp sp
where x Rn is the state vector with n components, C. 6-DOF UAV
and u
Rp∈
is the input vector with p components. Thus,
the dyn∈ amical matrices are A Rn×n and B Rn×p. If In this work, we consider a 6-DOF UAV as the agent,
∈ ∈ described by a state vector of 12 components:
the pair (A,B) is controllable, there exists a state feedback
controller: (cid:20) p˙ , p , p˙ , p , p˙ , p , ϕ˙, ϕ, θ˙,(cid:21)T
u= Kx (2) x= θx
,
ψ˙x
,
ψy y z z (8)
−
such that, the origin (x = 0) of the resulting closed-loop where p , p , p , and their derivatives represent linear
x y z
(C-L) system: position and velocity, respectively. Similarly, ϕ, θ, ψ, and
their derivatives represent the Euler angles (roll, pitch, and
x˙ =(A BK)x=A x (3)
f
− yaw) and their angular velocities, respectively. This model
is an asymptotically stable equilibrium point (EP): x(t) is consistent with the one used in [9] and references therein.
→
0 as t + . Hence, the matrix A f is Hurwitz, i.e. all The controller used is given by (2), obtained by linearizing
→ ∞
the eigenvalues have a negative real part [8]. The problem the quadrotor model around an equilibrium point. For this
to design (2) that makes (1) asymptotically stable is called case, the equilibrium point is indicated with x which has
sp
stabilization problem. the same structure of (8) with all components equal to zero
except for p , p , p .
A. Tracking Control Problem x y z
In the case the problem requires to move the system (1) D. Multi-Agent Reinforcement Learning
followingaspecifictrajectory,theproblemiscalledtracking In a typical multi-agent cooperative scenario, N agents
control.Thisproblemcanbesolvedbygeneratingasequence
interactwithinanenvironmenttocompleteatask.Noisyand
of EPs [9] that we call setpoints, denoted by x . The C-L
sp limited sensors may prevent each agent from observing the
system (3) can be represented as:
full state of the environment, allowing access only to partial
observations. With partial observability, an agent no longer
x˙ =A (x x ) (4)
f sp
− knowsthetruestateoftheenvironment,butinsteadneedsto
where the control input (2) becomes u= K(x x sp). maintain a belief state - a probability distribution over states
− −
estimatedfromitsobservationhistory-whichitusestoselect
B. Lyapunov’s Theory
actions. Decision-making processes in such contexts can be
Since A is Hurwitz, there exists a symmetric positive
f formulated as a Decentralized Partially Observable Markov
definite matrix P >0 such that:
Decision Process (Dec-POMDP) [10], [11].
ATP +PA = Q (5) A Dec-POMDP is defined by a tuple
f f − (S,A, ,T,R,Ω,γ,N) where S is a finite set of states of
for a given symmetric positive definite matrix Q > 0. O
the environment, A is a set of joint action spaces, and N is
Equation (5) is called Lyapunov’s equation. Defining the P-
the number of agents, T is the state transition probability
norm as:
function which gives the probability of transitioning from
(cid:113)
x x ≜ (x x )TP(x x ) (6) state s to state s′ when joint action a = (a 1, ,a N) is
sp P sp sp ···
∥ − ∥ − − taken; R is the joint reward function that maps states and
the P-norm squared is a Lyapunov function V(x) = x joint actions to real numbers and is used to specify the
∥ −
x sp ∥2 P, which is always positive except at x sp where it is goal of the agents; Ω is a set of joint observations O i,
zero. For all trajectories x(t) starting within the ellipsoid: where O is the observation set for agent i; is the joint
i
O
observation probability function, which gives the probability
(x )= x Rn x x 2 c , (7)
Ec sp { ∈ |∥ − sp ∥P ≤ } of receiving joint observation o=(o 1, ,o N) after taking
···
where c > 0, V(x(t)) is monotonically decreasing. There- joint action a and ending up in state s′; and γ is the reward
fore, the trajectory x(t) never leaves , which is called a discount factor, 0 γ 1.
c
E ≤ ≤
positively invariant set [8]. In a Dec-POMDP, agents need to collaborate and coordi-
Remark 1: In the case of a nonlinear dynamical system nate their actions based on their individual observations to
described by x˙ = f(x,u) where u = 0, x = 0 is an equi- maximize the joint reward function. Solving a Dec-POMDP
librium point for the system f(0,0) = 0. The control input optimally is computationally intractable, as the policy space
(2) can be designed by considering the linearized system grows exponentially with the number of agents and theplanning horizon. Therefore, various approximate solution Assumption 1: The projection of (x ) into the 2-
c sp,i
E
methods [12], [13] have been used, such as heuristic search, D space formed by the components p and p must be
x y
dynamic programming, value or policy-based approximators contained within the 2-dimensional ball (p (t)), where
r i
B
with neural networks etc. The problem facing the team is to p (t)=[p (t),p (t)]representstheactualpositioncoordi-
i x,i y,i
findtheoptimaljointpolicy,i.e.acombinationofindividual nates(p (t),p (t))ofagentiforanystatex (x ).
x,i y,i i c sp,i
∈E
agent policies that produces behavior that maximizes the
C. Cooperative MARL for Dynamic Network Bridging
team’s expected reward.
Each agent i has a local observation o comprising the
III. PROBLEMFORMULATION i
structure of its neighborhood and features representing its
We address the challenge of dynamically forming and neighbors at certain time-step t. Such features include the
sustaining a connection between two moving targets, A and node ID, current coordinates, action taken, and coordinates
B, in a 2D space using a swarm of N agents [7]. These of the targets A and B to be connected. At the beginning
agentsmustcollaborativelyadjusttheirpositionstomaintain of every time-step, agents make decentralized decisions,
a continuous multi-hop link between the targets, operating generatingtargetpointsw,utilizingtheirlocalobservationto
under decentralized control and limited local perception, choosethedirectionoftheirnextmovement.Thisisencoded
while ensuring they avoid collisions and respect physical in a two-dimensional action space, with each dimension
boundaries. We consider all the agents as nonlinear systems havingthreeoptions:moveforward,movebackward,orhold
thatcanbereducedintotheform(1)controlledby(2)around along the corresponding p or p axis. Given the direction
x y
the equilibrium point x sp,i [9]: chosen by the agents, a new target point is calculated for
each agent using a fixed offset equal for both the p and p
x˙ =A (x x )=(A BK)(x x ) (9) x y
i f i sp,i i sp,i
− − − axes. Finally, agents are rewarded using a reward function
for i = 1,...,N. In this case, we consider N agents that motivates agents to form a connected network bridging
describedwiththesamemodel,butingeneral,ourapproach the targets. Such reward signal combines three components:
can be generalized to the case of different dynamics and Base Connectivity Reward: The base reward, R ,
base
multidimensional scenarios. encourages larger connected components:
A. One-hop Communication
R (s)=
|C max(s)
|, (12)
base
We assume that for each agent, there is a ball of
|V|
radius r in the 2- or 3-dimensional Euclidean space where C (s) represents the size of the largest connected
max
centered at the agent’s current linear position p i(t) = compon| ent in st| ate s, and is the total number of entities.
[p x,i(t),p y,i(t),p z,i(t)]T: Centroid Distance P| eV n| alty: The centroid distance
(p (t))=(cid:8) p R3 : p p (t) 2 r2(cid:9) penalty, P cent, is based on agents’ positions relative to the
r i i
B ∈ ∥ − ∥ ≤ targets, penalizing them based on the Euclidean distance
Given two agents i and j with i=j, they can communicate between the centroid of the agents’ positions and the central
̸
if and only if point between the targets.
Target Path Bonus: A bonus, B =100, is awarded if
(p (t)) (p (t))= (10) path
Br i ∩Br j ̸ ∅ a path exists between the two targets.
In our setup, since p is fixed, the one-hop communica- The overall reward combines these three elements:
z,i
tion region is described by a 2-D ball. Note that, we use a
different notation when we refer to the 2-D communication (cid:40)
B (s) if path(T ,T );
space to avoid any confusion between the quadrotor x-axis R(s,a)= path ∃ 1 2 (13)
R (s) P (s), otherwise.
and the state space x. base cent
−
Wealsoincludeaphysicssimulator,modelingthecontinu-
B. Safety
ousnon-lineardynamicsoftheagents(2),andasafetracking
Definition 1: Given two agents i and j, where x =
sp,i ̸ control system proposed in the next section (Algorithm 1).
x , x (t ) (x ), and x (t ) (x ). Agents i
sp,j i 0 c sp,i j 0 c sp,j
∈ E ∈ E
and j are safe with respect to each other if IV. SAFETRACKINGCONTROL
(x ) (x )= . (11) Thesafetytrackingcontrolalgorithmthatweproposehere
c sp,i c sp,j
E ∩E ∅
The above definition is derived by the positively invariant consists of two steps, the first one is the setpoint update of
property of (7), since the initial conditions of each agent the agent i that guarantees that its current position belongs
are inside their respective ellipsoids, then x i(t)
∈
Ec(x sp,i) to Ec of the new setpoint. This condition is fundamental for
and x (t) (x ) for t + . This means that the the second step where the safety condition (11) is checked
j c sp,j
∈ E → ∞
trajectory of each agent cannot intersect generating the risk for all agents within the one-hop communication ball.
of possible collisions. Let us start with the first step which has been borrowed
In order to guarantee safety in our setup, we need the from [6]. For each agent, we select two scalars c and s such
following assumption: that 0<s<c therefore s(x sp,i) c(x sp,i).
E ⊂EDefinition 2: We can say that the agent has reached the one-hop range. This ensures agent i can communicate with
setpoint x if and only if x (t) (x ). affected neighbors before executing updates. This allows
sp,i i s sp,i
∈E
Assuming that the agent i is moving from the target point coordinating to preemptively address any potential safety
w to the target point w , generated by the MARL, the concerns from the new setpoint through direct communica-
i i+1
idea is to generate a sequence of setpoints on the line that tion with impacted agents.
connects the two target points. Once the state of agent i, x Building upon Proposition 1, let’s define (t) as the set
i i
D
reached the setpoint x , a new setpoint x′ is generated comprising all agents j = i that are within the one-hop
sp,i sp,i ̸
following the rule communication range of agent i at time t. The safe setpoint
update algorithm for each agent i is given in Algorithm
x′ =x +(√c √s)v (14)
sp,i sp,i − 1. Once agent i’s state x i(t) reaches its current setpoint
where v Rn with v =1, indicating the direction of the x sp,i (as per Definition 2), it computes a new target setpoint
∈ ∥ ∥2 x′ .Beforeupdating,agentichecksforanypossiblesafety
line connecting the two target points. As showed in [6] sp,i
violations by evaluating the safety condition (11) for all
agents in (t). If no violation is detected, x is updated
x i(t) ∈Es(x sp,i) ⇒x i(t) ∈Ec(x′ sp,i). (15) tothenewD xi
′
;otherwise,theprevioussetpos ip n, ti
isretained.
sp,i
Fig. 2 illustrates the process of updating the setpoint,
presented in 2-D space for clearer visualization, though it Algorithm 1: Setpoint Update Algorithm
actuallycorrespondstoastatespacewithn=12components,
Input: Agent i with setpoint x , (t)
sp,i i
as applicable to quadrotors. Thanks to (14) and Assumption D
Output: Updated setpoint x
sp,i
wi+1 safety violation 0;
←
if x (t) (x ) then
i s sp,i
Ec(x′sp,i) Com∈ puE
te x′ (14);
sp ←
for j (t) do
i
∈D
if (x ) (x )= then
c sp,i c sp,j
E ∩E ̸ ∅
safety violation 1;
x′sp,i
Es(x′sp,i) break;
←
end
Es(xsp,i)
xsp,i
end
xi(t)
if safety violation == 0 then
x x′ ;
Ec(xsp,i)
(√c √s)v end
sp,i ← sp,i
−
end
wi
ByapplyingAlgorithm1toeachagent,weensurethatthe
Fig. 2: Setpoint update for agent i in the state space. Note
safetycondition(11)issatisfiedthroughoutthesystem.This
that the target points w and w have the same structure
i i+1 is guaranteed by the fact that an agent can only update its
of the setpoints x .
sp,i setpoint after verifying that the new target does not violate
safety for any agents within its one-hop communication
1, the projection of Ec(x′ sp,i) is contained in B(p i(t)). range, as stated in Proposition 1. The proof of safety relies
Proposition 1: Considering agent i moving from target on the following assumptions:
point w i to w i+1, and updating the setpoint x sp,i every Assumption 2: The initial setpoints of all agents satisfy
time x i(t) s(x sp,i) by using (14). Considering also the safety condition (11).
∈ E
Assumption 1 true, then the safety condition (11) can be Assumption 3: During the setpoint update for agent i, all
violated only with all the agents j =i that satisfy (10) (i.e. agents j (t) cannot change their setpoints x .
̸ ∈Di sp,j
within the one-hop communication range). Proposition 2: Considering the multi-agent system de-
Proof: From Assumption 1, the ball (p (t)) encom- scribed by (9) and Assumptions 1, 2 and 3, then applying
r i
B
passes the projection onto the 2-D space of (xsp,i) for Algorithm 1 to each agent guarantees that the safety condi-
c
E
any current state x (t) (x ). Utilizing (14) to adjust tion (11) is satisfied for all agents.
i c sp,i
∈ E
the setpoint ensures that x (t) remains within (x′ ), Proof: By Assumption 2, the initial setpoints of all
i Ec sp,i
signifying that its 2-D space projection still falls within agents satisfy the safety condition (11). Whenever an agent
(p (t)).Thus,shouldtheupdatedsetpointx′ contravene i updates its setpoint using Algorithm 1, it checks if the
Br i sp,i
(11), such a violation would only occur concerning agents newtargetsetpointx′ violatesthesafetyconditionforany
sp,i
situated within the one-hop communication range. agentj (t).Ifaviolationisdetected,thesetpointisnot
i
∈D
Proposition 1 highlights a critical aspect of setpoint up- updated,ensuringthatthecurrentsafesolutionismaintained.
dates: setpoint updates only impact safety for agents within Proposition 1 guarantees that safety is only impacted foragentswithintheone-hopcommunicationrange,whichagent d≜x x , (18) can be rewritten as
sp,j sp,i
−
icandirectlycommunicatewithbeforeexecutingtheupdate. (cid:26) λ2dTPd ρ (cid:26) λ2 d 2 ρ
Furthermore, Assumption 3 prevents agents in Di(t) from
(λ
1)2dTPd≤
ρ ⇒ (λ
1)2∥ d∥P
2
≤
ρ
(19)
changing their setpoints during agent i ’s update, ensuring − ≤ − ∥ ∥P ≤
a consistent safety evaluation. Therefore, by applying Algo- Solving for λ we obtain
rithm 1 to each agent, the safety condition (11) is upheld
(cid:26) λ2 c (cid:114) c
throughout the system’s operation. ≤ ∥d∥2 P 1 2 0
λ2 d 2 2λ d 2 + d 2 c ⇒ − d 2 ≤
We now need to verify when the safety condition (11) ∥ ∥P − ∥ ∥P ∥ ∥P ≤ ∥ ∥P (20)
is violated. Our objective is to demonstrate that if (11) is
The two ellipsoids (x ) and (x ) intersect if
violated, then there exists a point on the line connecting Ec sp,i Ec sp,j
x and x , inside the overlap region of two ellipsoids (cid:114) c
sp,i sp,j 2 1 d 2 4c (21)
c(x sp,i)and c(x sp,j)(Fig.3).Thisoverlapregion,denoted d 2 ≥ →∥ ∥P ≤
E by (xsp,i)E (x ), satisfies the following condition: ∥ ∥P
c c sp,j
E ∩E Remark 2: Although Algorithm 1 ensures safety accord-
λ(x x )TP(x x )+(λ 1)(x x )TP(x x ) c ingtoProposition2,itdoesnotguaranteetargetachievement.
sp,i sp,i sp,j sp,j
− − − − − ≤
(16) Agents within one-hop communication range may become
for 0 λ 1 with x =x . immobilized at a certain position because any attempt to
sp,i sp,j
≤ ≤ ̸
Proposition 3: Letusconsidertheellipsoids (x )and reach the target points could potentially violate the safety
c sp,i
E
(x ) defined as in (7). The set of points x satisfying condition (11). We intentionally refrain from employing any
c sp,j
E
(16) for all λ [0,1] is either empty, a single point, or an enforcer to resolve this deadlock scenario, as our aim is
∈
ellipsoid: to assess whether our MARL framework can autonomously
generate target points that circumvent such situations.
ˆ (m )=(cid:8) x Rn :(x m )TP−1(x m ) K (cid:9)
EKλ λ
∈ −
λ
−
λ
≤
λ In summary, our analysis provides the following key
(17)
theoretical results:
where
• Each agent’s setpoint update only impacts the safety of
m =λx +(1 λ)x neighboring agents within its one-hop communication
λ sp,i sp,j
−
K =1 λ(1 λ)(x x )TP(x x ). range.Thislocalizedeffectisanimportantpropertythat
λ sp,j sp,i sp,j sp,i
− − − − enables decentralized coordination.
Proof: See [14].
• we establish that Algorithm 1 guarantees the preserva-
Proposition 3 shows that if there is an ellipsoid violation, tion of the safety condition in (11) for all agents,
then the point m λ belongs to the intersection Fig. 3: • Weprovide(21)acomputationallytractablemethodfor
evaluating possible safety condition (11) violations.
(x ) (x )= m (x ) (x )
c sp,i c sp,j λ c sp,i c sp,j
E ∩E ̸ ∅→ ∈E ∩E Together, these theoretical results form the basis for our
safeanddistributedmulti-agentcontrolframework,ensuring
that agents can dynamically update their setpoints while
Ec(x sp,j) maintaining the prescribed safety guarantees through local
communication and coordination.
x
sp,j
V. NEURALNETWORKARCHITECTURE
m
λ We employ the neural network architecture for action-
(x ) value estimation presented in [7], which is tailored to
Ec sp,i
x sp,i address the challenges of dynamic network bridging in
a cooperative multi-agent setting, enabling the agents to
extrapolate information about spatial and temporal depen-
dencies while collaborating effectively. Such architecture
Fig. 3: Explaination of Proposition 3: m λ is on the segment combines Graph Neural Networks (GNNs) for relational
that connects the two setpoints. modeling and exploits their message-passing mechanisms
compatibly with the task optimized in the environment [Ref
Therefore,totestifthetwoellipsoidsintersectweneedto Section]. Furthermore, a Long Short-Term Memory (LSTM)
find if there exists at least one point on the segment joining is employed to cope with temporal dependencies and the
x and x that satisfies (16). To do so we consider both partial observability of the task. The model is trained in a
sp,i sp,j
ellipsoids Centralized Training Decentralized Execution (CTDE) [15]
(cid:26) (x x )TP(x x ) ρ fashion, where the agents optimize the same action-value
− sp,i − sp,i ≤ (18) function parameterization during training. However, during
(x x )TP(x x ) ρ
− sp,j − sp,j ≤ execution, each agent acts based on its local observations
We replace x with m since we are checking only a and the shared representations from its neighbors, adhering
λ
point in the segment joining the two setpoints. Defining to the decentralized nature of the task.AgentType SafetyMechanism ExplicitPenalty Truncation AvgCoverage AvgSafetyViolations
BaselineA No No No 41% 13,880
BaselineB No Yes No 0.01% 0
ApproachA Yes No No 39% 0
ApproachB Yes Yes No 16% 0
ApproachC Yes Yes Yes 22% 0
TABLE I: Comparison of agent performance, safety mechanisms, explicit penalties, and safety violations.
We continue by describing the key components of the were randomly placed at a certain distance apart to ensure
architecture below. realistic difficulty and a feasible connection. Target motion
1) Graph Attention Layers: Graph Convolutional Rein- and placing follow a seeded random pattern to control the
forcement Learning (DGN) [16] facilitates cooperation be- scenarios during training and testing. The primary focus of
tween the agents by enabling them to share their latent our experiments was to demonstrate the effectiveness of our
representations with their immediate neighbors. To this end, safetrackingcontrolsysteminenablingtheMARLapproach
weleverageGraphAttentionNetwork(GAT)[17]tocapture to learn policies that satisfy safety constraints. To establish
thespatialandrelationaldependenciesbetweenagentsinthe the importance of this contribution, we designed a set of
network. Specifically, we employ two Multi-Headed GAT experiments with varying training configurations:
that enable each agent to attend to and integrate information • Baseline A: We first trained a typical MARL approach
fromitsneighboringagentsadaptively.Thisencodingallows without any safety tracking control, serving as a base-
the agents to condition their actions based on the dynamic line.
network topology and inter-agent relationships effectively. • Baseline B: To investigate the impact of explicit penal-
Additionally, we integrate target entities into the sharing izationforsafetyviolations,wetrainedthesameMARL
process by equipping them with this same encoding module approach as in Baseline A but with a high penalty (-
to produce their latent representation. If an agent’s neigh- 100) imposed whenever an agent violated the safety
borhood includes target entities, the agent can gather their constraints.
representations and condition its actions accordingly. • Approach A: This configuration represents our key
2) Long Short-Term Memory: To handle the partial ob- contribution, where we trained the agents with the safe
servability and temporal dynamics of the environment, we tracking control system enabled. Agents employed Al-
integrateaLSTM[18]layerintoourarchitecture.TheLSTM gorithm 1 to update their target points while respecting
layer maintains a memory of past observations, allowing the the safety conditions.
agentstomakeinformeddecisionsbasedonpastinteractions,
The results from Approach A demonstrated the effec-
partial observability, and the evolving environment.
tiveness of our safe tracking control system in enabling
During training, we employ observation stacking, aggre-
the agents to learn policies that accomplish the task while
gating observations over multiple time steps. Such a process
adhering to safety constraints.
gives the LSTM layer the necessary temporal context for
To further solidify our approach and highlight its advan-
effective learning in the presence of partial observability.
tages, we conducted additional experiments:
3) Dueling Action Decoder: To estimate action values
effectively,weincorporateaDuelingActionDecoder[19]in • Approach B: Here, we combined the safe tracking
control system with explicit penalization for safety
ourarchitecture.Thiscomponentcomprisesseparatestreams
violations, similar to Baseline B. This configuration
for estimating state values and advantages, which are then
aimedtoanalyzetheagents’behaviorwhenbothsafety
combined to produce the final action value estimates.
measures and penalization for safety violations were
VI. EXPERIMENTALSETUP employed simultaneously.
Weevaluatedourapproachinasimulated2Denvironment • Approach C: Building upon Approach B, we addi-
with three agents and two moving targets. The environment tionallyintroducedepisodeterminationwheneversafety
was normalized with axes ranging from 0 to 1. Agents constraints were violated. This experiment assessed the
and targets had a communication range of 0.25, and their impact of a more stringent safety enforcement mecha-
movement offset was set to 0.05 for the calculation of the nism.
next target point w. The physics simulator allowed us to If enabled, the safe tracking control system was used
simulate the non-linear Unmanned Aerial Vehicle (UAV) to prevent agents from continuing their movement if they
dynamics while we updated the positions of the agents in violatedthesafetycondition(11),suchasgoingtoocloseto
their paths toward the decided target point. other agents and risking collision. During training, in case
Ourmulti-agentstrategiesweretrainedacross1Mtraining the safety condition was violated, the agents involved were
steps. Training episodes are truncated and Partial Episode forced to stop their movement and would remain blocked
Boostraping [20] was performed after 100 steps taken by until a new action toward safe directions was computed by
eachagent.Agentswereinitializedatfixedstartingpositions at least one of the agents involved. To evaluate our agents,
that satisfy safety condition (11), and the moving targets we ran 100 evaluation episodes disabling the safety trackingcontrol system and counting the number of times that safety REFERENCES
conditions were violated for every position update (14).
[1] I. ElSayed-Aly, S. Bharadwaj, C. Amato, R. Ehlers, U. Topcu, and
L.Feng,“Safemulti-agentreinforcementlearningviashielding,”arXiv
VII. RESULTS preprintarXiv:2101.11196,2021.
[2] L.Dai,Q.Cao,Y.Xia,andY.Gao,“Distributedmpcforformationof
Table I shows the results of our experiments evaluating
multi-agentsystemswithcollisionavoidanceandobstacleavoidance,”
different learned MARL strategies. We report the average JournaloftheFranklinInstitute,vol.354,no.4,pp.2068–2085,2017.
communication coverage achieved by the agents as well [3] Y. Li, N. Li, H. E. Tseng, A. Girard, D. Filev, and I. Kolmanovsky,
“Safereinforcementlearningusingrobustactiongovernor,”inLearn-
as the number of safety violations observed when the safe
ingforDynamicsandControl,pp.1093–1104,PMLR,2021.
tracking control system was disabled during evaluation. [4] Z.Gao,G.Yang,andA.Prorok,“Onlinecontrolbarrierfunctionsfor
Baseline A agents, trained without any safety mecha- decentralized multi-agent navigation,” in 2023 International Sympo-
sium on Multi-Robot and Multi-Agent Systems (MRS), pp. 107–113,
nisms, achieved 41% coverage but incurred 13,880 safety
IEEE,2023.
violations on average. Introducing an explicit penalty of - [5] N. Kochdumper, H. Krasowski, X. Wang, S. Bak, and M. Althoff,
100 for violating safety constraints (Baseline B) reduced the “Provably safe reinforcement learning via action projection using
reachabilityanalysisandpolynomialzonotopes,”IEEEOpenJournal
coverage to only 0.01% but did successfully prevent any
ofControlSystems,vol.2,pp.79–92,2023.
safety violations. [6] R. Romagnoli, B. H. Krogh, D. de Niz, A. D. Hristozov, and B. Si-
In our Approach A, agents achieved 39% coverage (2% nopoli, “Software rejuvenation for safe operation of cyber–physical
systemsinthepresenceofrun-timecyberattacks,”IEEETransactions
less than Baseline A) while completely avoiding any safety
onControlSystemsTechnology,2023.
violations. This demonstrates that safe tracking control al- [7] R. Galliera, T. Mo¨hlenhof, A. Amato, D. Duran, K. B. Venable,
lows the agents to learn policies that respect safety con- and N. Suri, “Distributed autonomous swarm formation for dynamic
networkbridging,”in(ToAppear)The17thInternationalWorkshopon
straintswithoutsignificantlycompromisingtaskperformance
NetworkedRoboticsandCommunicationSystems(IEEEINFOCOM),
and without the need for explicit constraint violation penal- 2024.
ties. Adding an explicit penalty for safety violations to the [8] H. K. Khalil, Nonlinear systems; 3rd ed. Upper Saddle River, NJ:
Prentice-Hall,2002.
safe tracking control (Approach B) reduced coverage to
[9] A. Chen, K. Mitsopoulos, and R. Romagnoli, “Reinforcement
16% while still avoiding violations. Further adding episode learning-basedoptimalcontrolandsoftwarerejuvenationforsafeand
truncation when violating safety (Approach C) increased efficientuavnavigation,”in202362ndIEEEConferenceonDecision
andControl(CDC),pp.7527–7532,IEEE,2023.
coverage to 22% while maintaining zero violations.
[10] D. S. Bernstein, R. Givan, N. Immerman, and S. Zilberstein, “The
complexity of decentralized control of markov decision processes,”
VIII. DISCUSSION Mathematicsofoperationsresearch,vol.27,no.4,pp.819–840,2002.
[11] F.A.OliehoekandC.Amato,AConciseIntroductiontoDecentralized
Our work presents a novel hybrid approach that com-
POMDPs. SpringerBriefs in Intelligent Systems, Springer Interna-
bines MARL with control-theoretic methods to address a tionalPublishing,2016.
complex cooperative task while ensuring safety guarantees. [12] F. A. Oliehoek, S. Whiteson, M. T. Spaan, et al., “Approximate
solutions for factored dec-pomdps with many agents.,” in AAMAS,
The theoretical analysis provides three key results. First, we
pp.563–570,2013.
provethateachagent’ssetpointupdateonlyaffectsthesafety [13] R.Lowe,Y.I.Wu,A.Tamar,J.Harb,O.PieterAbbeel,andI.Mor-
of its one-hop neighboring agents, enabling decentralized datch, “Multi-agent actor-critic for mixed cooperative-competitive
environments,” Advances in neural information processing systems,
coordination. Second, we establish that Algorithm 1 guar-
vol.30,2017.
antees the preservation of the safety condition for all agents [14] I.GilitschenskiandU.D.Hanebeck,“Arobustcomputationaltestfor
under specific assumptions. Third, we derive an analytical overlap of two arbitrary-dimensional ellipsoids in fault-detection of
kalmanfilters,”in201215thInternationalConferenceonInformation
condition to efficiently evaluate potential safety violations
Fusion,pp.396–401,IEEE,2012.
during setpoint updates. [15] S.V.Albrecht,F.Christianos,andL.Scha¨fer,Multi-AgentReinforce-
The experimental results highlight the importance of our ment Learning: Foundations and Modern Approaches. MIT Press,
2023.
hybrid approach. Agents trained without any safety mecha-
[16] J. Jiang, C. Dun, T. Huang, and Z. Lu, “Graph convolutional rein-
nisms(BaselineA)achievedhightaskcoveragebutincurred forcementlearning,”inInternationalConferenceonLearningRepre-
numerous safety violations. Introducing an explicit penalty sentations,2020.
[17] P. Velicˇkovic´, G. Cucurull, A. Casanova, A. Romero, P. Lio`, and
for violating safety constraints (Baseline B) successfully
Y. Bengio, “Graph attention networks,” in International Conference
prevented violations but at the cost of significantly com- onLearningRepresentations,2018.
promised task performance. In contrast, our Approach A, [18] M.J.HausknechtandP.Stone,“Deeprecurrentq-learningforpartially
observable mdps,” in 2015 AAAI Fall Symposia, Arlington, Virginia,
which incorporates the safe tracking control system based
USA,November12-14,2015,pp.29–37,AAAIPress,2015.
on Algorithm 1, allowed agents to learn policies that respect [19] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot, and
safety constraints while maintaining reasonable task cover- N.DeFreitas,“Duelingnetworkarchitecturesfordeepreinforcement
learning,” in Proceedings of the 33rd International Conference on
age, without explicit constraint violation penalties.
InternationalConferenceonMachineLearning-Volume48,ICML’16,
Despite these strengths, there are some limitations to our p.1995–2003,JMLR.org,2016.
current work. While we have demonstrated the approach’s [20] F.Pardo,A.Tavakoli,V.Levdik,andP.Kormushev,“Timelimitsin
reinforcementlearning,”inProceedingsofthe35thInternationalCon-
effectiveness on the network bridging task, we have not
ferenceonMachineLearning(J.DyandA.Krause,eds.),vol.80of
extensively evaluated its scalability to significantly larger Proceedings of Machine Learning Research, pp. 4045–4054, PMLR,
swarm sizes or other domains. Moving forward we plan to 10–15Jul2018.
investigate the scalability and adaptability of our approach
across different domains and with varying swarm sizes.