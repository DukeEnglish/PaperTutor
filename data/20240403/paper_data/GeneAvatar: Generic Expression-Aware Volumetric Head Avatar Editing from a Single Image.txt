GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing
from a Single Image
ChongBao1‚àó¬ß YindaZhang2* YuanLi1* XiyuZhang1 BangbangYang4
HujunBao1 MarcPollefeys3 GuofengZhang1 ZhaopengCui1‚Ä†
1StateKeyLabofCAD&CG,ZhejiangUniversity 2Google 3ETHZu¬®rich 4ByteDance
+NeBShape +INSTA +Next3D
Eyes++, Face-0-077.png Ears++,Ey0e2b3ro7w2_se+d+it.png nNeoxst3ed--_,dFeafocrem+_+ep0000_0004_rgb.png
TextPrompt
BlushPainting ‚ÄúClown‚Äù ‚ÄúYoung‚Äù ‚ÄúDisney‚Äù Hair,Eyes,EyebrowsMakeup
Original View 2DEditing EditedNovelView Original View 2D/TextEditing EditedNovelView Original View 2DEditing EditedNovelView
Figure1. Weproposeagenericapproachtoedit3Davatarsinvariousvolumetricrepresentations(NeRFBlendShape[16],INSTA[76],
Next3D[50])fromasingleperspectiveusing2Deditingmethodswithdrag-style,text-promptandpatternpainting.Oureditingresultsare
consistentacrossmultiplefacialexpressionandcameraviewpoints.
Abstract (3DMM)[6].Apopulardemand,oncewithacreatedavatar
Recently, we have witnessed the explosive growth of model, is to edit the avatar, e.g., for face shape, facial
variousvolumetricrepresentationsinmodelinganimatable makeup, or apply artistic effects, for the downstream ap-
headavatars. However,duetothediversityofframeworks, plications,e.g.,invirtual/augmentedreality.
thereisnopracticalmethodtosupporthigh-levelapplica- Ideally, the desired editing functionality on the animat-
tionslike3Dheadavatareditingacrossdifferentrepresen- ableavatarshouldhavethefollowingproperties.(1)Adapt-
tations. Inthispaper, weproposeagenericavatarediting able: The editing method should be applicable across var-
approachthatcanbeuniversallyappliedtovarious3DMM- ious volumetric avatar representations. This is particularly
driving volumetric head avatars. To achieve this goal, we valuable in light of the growing diversity of avatar frame-
design a novel expression-aware modification generative works [16, 50, 76]. (2) User-friendly: The editing should
model,whichenableslift2Deditingfromasingleimageto beuser-friendlyandintuitive. Preferably,theeditingofge-
aconsistent3Dmodificationfield. Toensuretheeffective- ometryandtextureofthe3Davatarcouldbeaccomplished
nessofthegenerativemodificationprocess,wedevelopsev- on a single-perspective rendered image. (3) Faithful: The
eral techniques, including an expression-dependent mod- editingresultsshouldbeconsistentacrossvariousfacialex-
ification distillation scheme to draw knowledge from the pressionandcameraviewpoints. (4)Flexible: Bothinten-
large-scaleheadavatarmodeland2Dfacialtextureediting siveediting(e.g.,globalappearancetransferfollowingstyle
tools,implicitlatentspaceguidancetoenhancemodelcon- prompts)anddelicatelocalediting(e.g.,draggingtoenlarge
vergence, andasegmentation-basedlossreweightstrategy eyesorears)shouldbesupportedasillustratedinFig.1.
for fine-grained texture inversion. Extensive experiments However,3D-awareavatareditingisstillunderexplored
demonstratethatourmethoddelivershigh-qualityandcon- inbothgeometryandtexture. Oneplausiblewayistoper-
sistent results across multiple expression and viewpoints. form 3D editing via animatable 3D GAN [50, 52, 54], but
Projectpage: https://zju3dv.github.io/geneavatar/. the editing results may not be consistently reflected when
expressionandcameraviewpointchange.Alternatively,the
1.Introduction
editing can be done on the generated 2D video using 2D
Recently various volumetric representations [3, 4, 15, 16, personalized StyleGAN [28]; however, the identity shift is
57, 68, 69, 76] have achieved remarkable success in re- oftenobserved. Someface-swappingmethods[11,12,42]
constructing personalized, animatable, and photorealistic arecapableofsubstitutingthefaceinavideowithanother
head avatars using implicit [15, 16, 57, 68, 69] or ex- facederivedfromareferenceimageorvideo;however,they
plicit [3, 4, 76] conditioning of 3D Morphable Models donotsupporttextureeditingandlocalgeometryediting.
To this end, we propose GeneAvatar ‚Äì a generic ap-
*Authorscontributedequally.
‚Ä†Correspondingauthors. proach to support fine-grained 3D editing in various vol-
¬ßTheworkwaspartiallydonewhenvisitingETHZ. umetricavatarrepresentationsfromasingleperspectiveby
4202
rpA
2
]VC.sc[
1v25120.4042:viXraleveraging 2D editing methods, such as drag-based meth- mantic information [29, 72] or controlling latent space ex-
ods[30,34,39,47],text-drivenmethods[7,17,20,40,41], plorations [10, 19, 46, 70]. Some approaches [22, 27, 36,
or image editing tools like Photoshop (see Fig. 1). We 61]focusonthetaskofmakeuptransferringbyexploiting
adopt a novel editing framework that formulates the edit- the GAN to learn the transferring ability from a large un-
ing as predicting expression-aware 3D modification fields aligned makeup and non-makeup face datasets. The drag-
appliedinthegeometryandtexturespaceofthevolumetric basedGANeditingapproach[39,71]gainedvastpopularity
avatars,whichmakeseditingindependentwiththeoriginal duetoprovidingauser-friendlyeditingway. PVP[28]uses
representationaslongastheyareinparametric-drivenradi- amonocularvideotofine-tuneStyleGAN[23,24]toobtain
ancefield,e.g.,3DMM-basedneuralavatar. Second,toen- thepersonalizedimagegeneratorandprovidevariousedit-
surethatthe2Dimageeditingcanbefaithfullytransferred ing functions. The diffusion models [7, 21] also show the
into the 3D space, we propose to learn a generative model capability of achieving fine-grained face editing with text
formodificationfields,whichproduces3DMMconditional prompt and other conditional input. For editing an avatar
modificationfieldsfromacompactlatentspace. Giventhe inavideo,lotsofface-swappingmethods[11,12,42]have
rendered avatar image and its edited counterpart, we con- emergedtoprovidehigh-qualityandproperlyalignedface-
duct auto-decoding optimization on this generative model swappingresults. However,theseapproachestypicallysuf-
to search for the latent code that best explains the editing, ferfrommulti-viewconsistencyandidentitypreservation.
obtainingconsistent3DMMconditionalmodificationfields 3D Head Avatar Editing. Neural Radiance Field [33]
across various viewpoints and expression. Third, inspired has exhibited great reconstruction and rendering qualities
by the spirit of learning from the pre-trained large-scale in SLAM [62, 73], scene editing [5, 58‚Äì60, 64] and re-
generative model [7, 18, 32, 74], we design a novel dis- lighting [63, 66, 67], especially promoting the emergence
tillation scheme to learn the expression-dependent modifi- ofmany3Davatarreconstruction[4,16,53,68,69,76]and
cationfroma3DMM-basedGAN[50]and2Dfaceediting generation [50, 52, 54]. Some methods [2, 49, 56, 65] ex-
tools[22,27,36]. Theschemeaddressestheissueofinsuf- ploitthepowerfuleditingabilityofGANtoedita3Dstatic
ficient real training data (i.e., avatars with a wide range of head portrait. However, they cannot be trivially extended
geometry and texture changes). Besides, we develop sev- tothedynamicavatars. Themethods[37,38,45]focuson
eraltechniquestoenhancetheeditingeffects,includingthe styletransferoftheavatarusingtextpromptorstyleimage
implicit latent space guidance to stabilize the initialization but reach a poor identity-preserving. We propose a novel
andconvergenceoflearning,andasegmentation-basedloss 3Davatareditingapproachwithanexpression-awaremod-
reweightstrategyforfine-grainedtextureinversion. ification generative model, which can be applied to vari-
The contributions of our paper are summarized as fol- ous3DMM-basedvolumetricavatarsandrenderconsistent
lows. 1) We propose a generic avatar editing approach novelviewswithfine-grainededitingacrossmultipleview-
thatcanbeappliedtovarious3DMMdrivingheadavatars points and expression while preserving identity of person.
in the neural radiance field. To achieve this, we design
3.Method
a novel expression-aware modification generative model,
which lifts the geometry and texture editing from a single AsshowninFig.2,givenavolumetricheadavatar,weedit
imagetoaconsistent3Dmodificationfield. 2)Tobootstrap theavatarusingasingle-viewimageandsynthesizeconsis-
thetrainingofthemodificationgeneratorwithlimitedreal tentnovelviewsacrossmultipleexpressionandviewpoints.
pairedtrainingdata,wedesignadistillationschemetolearn Toachievethisgoal,weproposeanovelexpression-aware
the expression-dependent geometry and texture modifica- modification generator to generate 3D modification fields,
tionfromthelarge-scaleheadavatargenerativemodel[50] which can be seamlessly integrated into various represen-
and2Dfacetextureeditingtools[22,27,36], anddevelop tations and animated with facial expression (see Sec. 3.2).
several techniques, including implicit guidance in latent Furthermore, to bootstrap the training with limited pair-
spacetoimprovetrainingconvergence,andalossreweight wisedata,weproposeanovelexpression-awaredistillation
strategybasedonsegmentationforfine-grainedtexturein- scheme to learn the expression-dependent modifications
version. 3) Extensive experiments on various head avatar from large-scale generative models [7, 50] (see Sec. 3.3).
representationsdemonstratethatourmethoddelivershigh- Duringtheeditingprocess,givenasingleeditedimageofa
qualityeditingresultsandtheeditingeffectsareconsistent 3Davatar,weperformanauto-decodingoptimizationtolift
underdifferentviewpointsandexpression. 2Deditingeffecttothe3Dspace(seeSec.3.4).
2.RelatedWork 3.1.Preliminaries
2D Head Avatar Editing. The manipulation of 2D head The current implicit volumetric representations of head
avatarshasmadesignificantstridesinrecentyears. Various avatar [16, 50, 76] are mostly built upon the NeRF [33]
GAN-basedmethods[23‚Äì25]canresultinpreciseandhigh- or its variants [8, 9, 31, 35, 44, 48]. In general, the neu-
resolutionhumanfaceeditingbyleveragingimagespacese- ralarchitecturecanbesimplifiedasanimplicitfieldFthatEditingInput Expression-awareHeadAvatarEditing NovelViewSynthesis
/
Queryùê±
ùê≥ùê† Loss
3DMMCof. ùê≥ùê≠
Volumerendering EditedImage
Expression GeometricMod. ExpressionCof. Auto-decodingOptimization
Coefficient ùê≥ùê† Generator Templateùê±‚Ä≤ Representation-
agnostic
Ray Casting G Le ao tm ene ttr Sic paM co ed. TemplateAvatar Densityùúé
(1) VolumetricHeadNeRF Geo Tm rie pt lari nc eMod. DirùíÖ
EditedSpace ùê±
Ears++
TextureMod.
ùú∑ùö´
Colorùêúùíê
Generator Blending
‚ÄúClown‚Äù ùê≥ùê≠ Colorùêú
TextureMod.
Mod.Colorùêúùö´
LatentSpace TextureMod.
(2) Single-view 2D Editing Triplane FaceReenactment
Figure2. Weuseanexpression-awaregenerativemodelthatacceptsamodificationlatentcodez and3DMMcoefficientsandoutputs
g/t
amodificationfieldofatri-planestructure. Themodificationfieldmodifiesthegeometryandtextureofthetemplateavatarbydeforming
thesamplepointsxandblendingthecolorc withthemodificationcolorc respectively. Weliftthe2Deditingeffectto3Dusingan
o ‚àÜ
auto-decodingoptimizationandsynthesizenovelviewsacrossdifferentexpression.
takes position x and view direction d as inputs and pre- UV mapping. We rasterize the vertex features to the three
dicts the geometry œÉ and the texture c of the avatar, i.e., axis-aligned planes to generate the tri-plane feature. The
(œÉ,c) = F(x,d). Then,thevolumerenderingtechniqueis modification information of the query point x is first col-
usedtorenderimagesasfollows: lectedbybilinearinterpolationonthetri-planefeatureand
then decoded by the neural feature decoder [8]. Since the
Ô£´ Ô£∂
N i‚àí1 modificationisdefinedasdecoupledfieldswithoutrelying
CÀÜ(r)=(cid:88)
T iŒ± ic i, T i
=expÔ£≠‚àí(cid:88)
œÉ‚Ä≤ jŒ¥ jÔ£∏, (1) on the original field, our generated modification field can
i=1 j=1 beintegratedintovariousvolumetricavatarrepresentations
andbeanimatedfollowingthefacialexpression.
whereŒ± =1‚àíexp(‚àíœÉ‚Ä≤ Œ¥ ),andŒ¥ isthedistancebetween
i i i i
adjacentsamplesalongtheray.Inordertoanimatethehead 3.3.Expression-dependentModificationLearning
avatar,3DMM[6]isincorporatedtodescribethedeforma- To learn the proposed expression-aware modification, we
tionimplicitly[15,16,57,68,69]orexplicitly[4,76]. need extensive training data on avatars with a wide range
of geometry and texture changes, which is hard to obtain
3.2.Expression-awareModificationGenerator
in practice. Following the spirit of learning high-fidelity
To enable the modification animated with the facial ex-
editingabilityfromthelarge-scalegenerativemodel[7,18,
pressions, we follow the architecture of 3DMM-based 3D
32, 74], we propose a novel expression-aware distillation
GAN[50]tobuildourexpression-awaremodificationgen-
schemetodealwithinsufficientrealtrainingdata.Welever-
erator. As shown in Fig. 2, our generator consists of a ge-
agetheabilityof3DMM-based3DGAN[50]and2Dface
ometrygeneratorG andatexturegeneratorG . G
‚àÜg ‚àÜt ‚àÜg texture editing tools to generate facial editing data, which
encodes the expression-dependent geometry modification
encompassesawiderangeofgeometryandtextureediting
by deforming the query points in the edited space to the
acrossvariousexpressionandviewpoints.
original template space under each expression. G en-
‚àÜt
Geometry Distillation. We use the teacher 3DMM-based
codestheexpression-dependentmodificationcolorofquery
3DGAN[50]G tosynthesizetwovolumetricavatarswith
pointsundereachexpression: n
differentgeometry(anoriginalavatarFandaneditedavatar
x‚Ä≤ =G (x,z ,v), (c ,Œ≤ )=G (x,z ,v), (2) F‚Ä≤) bymodifying the3DMM shapeparameter ofthe orig-
‚àÜg g ‚àÜ ‚àÜ ‚àÜt t
inal avatar. This provides the paired editing data for our
wherexisthequerypointsintheeditedspace,andx‚Ä≤isthe generatortolearnhowtomodifythegeometryoftheavatar
deformedpointinthespaceoforiginalavatarundercurrent while maintaining consistency across various expression
expression e. c is the modification color and Œ≤ deter- andviewpoints.Specifically,werandomlysamplethelatent
‚àÜ ‚àÜ
mines the blending weights with the original color. z ,z codez inthelatentspaceofG aswellas3DMMshape
g t n n
are the geometry and texture modification latent code re- parameter Œ≤, expression parameter œà, and pose parameter
spectively, where z ,z ‚àà R1024. They control the gener- Œ∏. Œ≤,œà aresampledfromanormaldistributionwhoseab-
g t
ation of modification feature maps in the UV space. v is solutemeanandstandarddeviationarewithin[0,1]. Œ∏area
the3DMMmeshvertices[26]thatconditionthecurrentex- groupofrotationvectorsthathaverandomdirectionswithin
pression e. Each mesh vertex has a neural feature that is aunitsphereandmagnitudewithin[‚àí6,6]degrees. Then,
retrievedinthemodificationfeaturemapusingpre-defined we sample an edit vector Œ≤ from a uniform distribution
‚àÜU(‚àí3,3) and apply it to the original shape parameter by obtain the density and color, and composite the color with
Œ≤‚Ä≤ = Œ≤ +Œ≤. These hyperparameters w.r.t. 3DMM co- modificationcolorby:
‚àÜ
efficientssamplingareselectedempiricallytomaintainthe
shapedefinitionofthehumanhead. Pleaserefertooursup- c=(1‚àíŒ≤ ‚àÜ)‚àóc o+Œ≤ ‚àÜ‚àóc ‚àÜ, (œÉ,c o)=F(x‚Ä≤,d). (3)
plementarySec.B.2formoredetailson3DMMsampling.
Then, we perform volume rendering on the density œÉ and
TheoriginalavatarFandpairededitedavatarF‚Ä≤aregener-
color c using Eq. (1) to render the modified image IÀÜ of
atedbyF=G (z ,Œ≤,œà,Œ∏),F‚Ä≤ =G(z ,Œ≤‚Ä≤,œà,Œ∏). Dur- e
n n n avatar F. We use the photometric loss to supervise the
ing training, we will apply our modification generator to
modifiedimagewiththerenderedimageIÀÜ‚Ä≤ fromtheedited
modify the geometry of F such that F and F‚Ä≤ render the
avatarF‚Ä≤underthesamecameraparameters.
facewiththesamegeometry.
Texture Distillation. We distill the capabilities of fine- L=||IÀÜ ‚àíIÀÜ‚Ä≤||2. (4)
e 2
grained texture editing from 2D face editing algorithms
by generating texture-modified avatar F‚Ä≤ with the teacher Duringtraining,wesamplemultipleviewpointsand3DMM
3DMM GAN [50] G . Specifically, we sample an origi-
expressionparametersœàforeacheditingpair(F,F‚Ä≤)toen-
n
nalavatarF=G (z ,Œ≤,œà,Œ∏)fromtheteachergenerator hancethespatialconsistencyunderdifferentexpressions.
n n
andrendertheimageofitspositiveface. Asegmentation- 3.4.AvatarEditingwithSingleImage
based2Dfacetextureeditingalgorithm(SBA)[77]andtwo In our task, users are allowed to edit a single image with
makeuptransferalgorithms(MTA)[22,27,36]arereferred various out-of-box face editing tools, such as Photoshop,
to in the distillation. We randomly choose one of them to drag-based editing [30, 39], text-driven editing [7]. For
editthetextureoftherenderedfaceimage.ForSBA,wede- each editing input, we use the auto-decoding optimization
fineseveraleditablesemanticregionsoftheface. Asubset onmodificationcodetolift2Deditsintoa3Dexpression-
oftheseregionsisselectedrandomlyfortexturepaintingus- awaremodificationfieldgeneratedbyourmodel. Thisfield
inghuesrandomlysampledfromtheHSVcolorspectrum. adaptstoexpressionandviewpointchangesandisnottied
For MTA, we randomly choose a makeup image as a ref- tothespecificavatarrepresentation. TheStyleGAN-based
erence from the open-sourced makeup dataset [22, 27, 36] generator[24,43,55]featuresalatentspacemappingfrom
andtransferthereferencemakeuptotherenderedfaceim- z ‚àà RZ in Z to w ‚àà RWn√óWd in W, where w is more
age. The makeup dataset [36] contains complex makeups, influential as it conditions the generator. Therefore, we
suchasblushesandmakeupjewelry,whichallowourgen- performcodeinversioninW spacebyrandomlysampling
erator to learn complicated texture editing patterns. Then, a modification code w during editing. This code con-
g/t
weperformthePTIinversion[43]onthetexture-modified ditions a modification field G (x,w ,v) following
‚àÜg/t g/t
face image to lift the 2D texture editing to 3D space and Eq. (2). We apply the modification field to the original
obtainatexture-modifiedavatarF‚Ä≤. avatar using Eq. (4). The modified image IÀÜ is rendered
e
Modification Learning. Following the training style of followingtheoriginalavatar‚Äôsrenderingpipelineandisen-
StyleGAN [24], we sample a modification latent code couraged to match the user-edited image I e by optimizing
z
g/t
‚àà R1024 in Z latent space for each paired editing w g/twiththefollowinglossterms:
data. We do not fully sample a 1024-dimensional mod-
ification code but sample a reduced code z¬Ø from a stan- L i =Œª 1L 2(IÀÜ e,I e)+Œª 2L lpips(IÀÜ e,I e)+Œª 3L reg(w,w avg),
dard normal distribution and concatenate it with the latent (5)
code z n of the original avatar F that is sampled from the TheL2losstermL 2 andLPIPSperceptuallosstermL lpips
teacher model, i.e., z = (z ,z¬Ø),z ,z¬Ø ‚àà R512. This encourage the rendered face close to the appearance and
g/t n n
design is regarded as implicit code guidance that decently structure of the edited face. We set Œª 1 = 1000,Œª 2 =
integrates knowledge from the teacher model to facilitate 1,Œª 3 = 1. The regularization term L reg applied to the la-
model convergence. z encodes the facial appearances of tentcodewenforcesalignmentwiththedistributionofthe
n
avatar F, serving as a reference to the superimposition of W space,withw avgrepresentingthemeanlatentcodecom-
the modification onto the avatar F. Note that during in- putedfrom1000randomsampleswithintheW space. Be-
ference,wedonotrequiretheconcatenationoflatentcode sides, we observe that the L2 loss on the whole face will
fromtheteachermodelanddirectlyoptimizethefullmodi- give an underfitting result for the fine-grained makeup on
ficationcodefromtheeditedimageusinganauto-decoding the eyes, eyebrows and lips. Therefore, we reweight the
manner. Ourgeneratorgeneratesthemodificationfieldfol- L2 loss on the facial features by face segmentation mask
lowing Eq. (2) where v is decoded from the 3DMM pa- M ={N i|i=1,...m}fortextureediting:
rametersoftheavatarFusingFLAMEmodel[26]E,i.e.,
m
v = E(Œ≤,œà,Œ∏). To apply the modification field, we feed L =(cid:88) (cid:88) 1 ||CÀÜ (r)‚àíC (r)||2, (6)
the deformed query points x‚Ä≤ to the original avatar F to 2 |N i| e e 2
i=1r‚ààNiGeometry Texture
Methods
Roop PVP Next3D Ours Roop PVP Next3D Ours
Editingpreservation‚Üë 29.44% 23.33% 5.56% 41.67% 8.33% 10.56% 5.00% 76.11%
Identitypreservation‚Üë 31.11% 21.11% 5.56% 42.22% 7.78% 12.78% 3.33% 76.11%
Temporalconsistency‚Üë 30.00% 23.89% 4.44% 41.67% 5.56% 12.78% 1.67% 80.00%
Overall‚Üë 29.44% 23.33% 3.89% 43.33% 5.56% 12.78% 2.22% 79.44%
imageidentitysimilarity‚Üë 0.8373 0.8704 0.8547 0.8845 0.7320 0.8476 0.8500 0.9147
Table1.WequantitativelycomparewiththePVP[28],Roop[12],
Next3D[50]byuserstudyandimageidentitysimilarity[13].
Hair++,Eyes++,
where CÀÜ (r),C (r) are the rendered and target color re- Mouse--
e e
spectively, N are the rays within the i-th semantic part of
i
the face. Generally, we freeze the weight of the modifi-
cation generator and only optimize the modification latent
codewtoreachasatisfied3Dmodificationresult.Whenan
intense makeup or complicated pattern is painted onto the
human face, we will continuously fine-tune the weight of
ourgeneratorandfreezethelatentcodewtoachievemore
accurateeditingresults. Toanimatetheeditedavatar,users
caninputthenew3DMMexpressionparametertotheorig- Nose--,
Lip++
inal avatar and our modification generator simultaneously.
In this way, the generated modification field tightly sticks
totheoriginalavatarandpresentsreasonableeditingresults
underdifferentexpressionandviewpoints.
4.Experiments
In this section, we evaluate our avatar editing capability
fromasingleperspective. Onemajordifferencewithstatic
NeRFeditingisthatwefocusonshowinghowtheeditsare ForeHead--
correctlyliftedto3Davatarsundervariousexpressionand
cameraviewpoints.
4.1.DatasetandBaselines
Datasets. Weuseatotalof19neuralimplicitheadavatars
fromthreemethods,i.e.,7fromINSTA[76],8fromNeRF-
BlendShape[16],4fromNext3D[50],andshoweditingre-
sultsonthem. ForINSTA[76]andNeRFBlendShape[16],
we use the human head data (i.e., a monocular video of
a head) provided by their methods to reconstruct the vol- Face--
Reference
2DEditing PVP Roop Next3D Ours
umetric avatar using their respective representations. For Animation
Next3D[50],werandomsampleitslatentspacetogenerate Figure3.WecomparegeometryeditingwithPVP[28],Roop[12],
Next3D [50] on INSTA [76] and NeRFBlendshape [16] avatars.
volumetricavatarsandperformeditingonthem. Theeval-
The ‚ÄùReference Animation‚Äù denotes the image of the original
uation datasets exhibit a substantial variation in identities,
avatarunderthesameexpressionwiththerenderededitedview.
encompassingadiverserangeofraces,ages,andgenders.
Baselines. Wepickseveralbaselinemethodsthatcansup- sion [43] with Next3D twice. First, we fine-tune Next3D
portsingle-view-basedavatarediting. Roop[12]isaface- with the input video to make it learn the original geome-
swappingmethodthatcanswapthehumanfaceinavideo tryandtextureoftheavatar. Second,wefine-tuneNext3D
from a single reference view. To compare with Roop, we on the edited image based on the weights of the code and
generate videos of the original avatar rendered in driving generatorfromthefirstfine-tuning.
signals and single edited frames, and perform face swap.
4.2.QualitativeComparison
PVP[28]learnsapersonalizedavatarimagegeneratorfrom
amonocularvideobyfine-tuningthelatentspaceofStyle- Geometry Editing. We first compare our method with
GAN [24], and performs GAN-inversion style optimiza- baselines on geometry editing, e.g., changing the size of
tion [41] to edit the shape and appearance of the avatar. the eyes or the contour of the cheek. We use 2D editing
Next3D [50] is a 3DMM-based 3D GAN. To make a fair tools, e.g., Photoshop and DragGAN [39], to modify the
comparisononthesameinputdata(i.e.,amonocularvideo shape of various facial features. Figure. 3 shows qualita-
of avatar and an edited image), we perform GAN inver- tive results on avatars from INSTA [76] (the top two) andSource Eyes++ Nose++ Face++ Eyes++ Nose++ Face++ Eyes++ Nose++ Face++
Source Eyes++ Face-- Face++ Eyes++ Face-- Face++ Eyes++ Face-- Face++
Source Eyes-- Nose++ Jaw-- Eyes-- Nose++ Jaw-- Eyes-- Nose++ Jaw--
Source Mouth++ Nose++ Jaw-- Mouth++ Nose++ Jaw-- Mouth++ Nose++ Jaw--
OriginalAvatar 2DEditing RenderedView1 RenderedView2
(a)NeRFBlendShapeGeometryEditingResults
Nose++ Rendering Eyes++ Rendering Source Mouse--,Face-- Rendering
Hair++ Rendering Mouth++ Rendering Source Eyes++,Nose--, Rendering
Hair++
(b)NeRFBlendShapeGeometryEditingResults (c)INSTA&Next3DGeometryEditingResults
Figure4.Ourgeometryeditingresultswiththedrag-style2DeditingonINSTA[76],NeRFBlendshape[16],andNext3D[50]avatars.
NeRFBlendshape[16](thebottomtwo). Roop[12]failsto tionsofmodifiedheadgeometry.
handlethefind-grainedgeometrychange,likehairlines,and
Texutre Editing. We then show our capability in tex-
lips. Nex3D [50] is able to successfully update the avatar
ture editing. We utilize Photoshop, an online makeup
basedontheediting, however, changesthe untouchedpart
app WebBeauty [1], and text-driven editing method In-
and causes an obvious identity shift. PVP [28] can make
structpix2pix [7] to modify the texture on 2D renderings.
editswhilepreservingtheidentity,however,themagnitude
We show comparisons with PVP [28], Roop [12] and
ofchangetendstobesmallerthanthegivenimage. Incon-
Next3D [50] on four distinct heads avatars (INSTA [76]
trast, our method produces the desired editing effect from
for the upper three and NeRFBlendshape [16] for bot-
the edited image and preserves the multiview consistency
tom one) in Fig. 5. Roop [12] is ineffective in transfer-
andidentityoftheoriginalface.
ring non-human-face-like texture, thus failing in all exam-
We further show more geometry editing results of our ples. PVP [28] only transfers partial or blurry textures,
methodonavatarsinvariousrepresentationsinFig.4. Our andalsocausesshiftsacrosstheexpressionandheadposes.
methodsupportsaconvenientwaytoadjustthesizeofdi- Next3D [50] successfully uplifts the texture editing in 2D
versefacialfeatures,suchaseyes,mouths,jaw,etc.,byedit- images sharply. However, it still suffers from the identity
ingasinglerenderedimagefromtheavatars. Theeditson shiftissueinthelowertwoheadsandablurredpatterninthe
2D images are successfully lifted onto the avatar and ren- uppertwoheadsinFig.5.Incontrast,ourmethodfaithfully
dered across different viewpoints and expression. Please paints the complicated texture following the edited image
refer to the supplementary Sec. C.3 for detailed visualiza- andpreservestheidentityoftheoriginalavatarandconsis-Reference Reference
Source 2DEditing PVP Roop Next3D Ours PVP Roop Next3D Ours
Animation Animation
Figure5. WecomparetextureeditingwithPVP[28], Roop[12], Next3D[50]onINSTA[76]andNeRFBlendshape[16]avatars. The
‚ÄùReferenceAnimation‚Äùdenotestheimageoftheoriginalavatarunderthesameexpressionwiththerenderededitedview.
sameasAvatarstudio[38]oneditingpreservation, identity
preservation,temporalconsistencyandoverallperformance
inTab.1. Wecollectedstatisticsfrom30participantsin12
groups of edit results. Results are reported in Tab. 1. Our
Eyes++
method exhibits the best editing ability while keeping the
best consistency across different facial expressions for ge-
ometryandtextureediting.Moreover,ourmethodperforms
Jaw-- thebestinkeepingnon-editedpartsuntouchedandmaking
2D EG de io tim ngetry 3 FD itM tinM g 3 FD inM e-M tunF eit Ntin eg RF& Ours 3 FD itM tinM g 3 FD inM e-M tunF eit Ntin eg RF& Ours the human heads still recognizable after being edited e.g.,
(a)GeometryEditingComparisons identity preservation metric in Tab. 1. Please refer to the
supplementarySec.B.4formoredetails.
Image Identity Similarity Evaluation. Following the
VoLux-GAN[51],wefurtherevaluatethecross-viewiden-
2D EdT ie tx intu gre Fi Nne e- Rtu Fne Ours 2D EdT ie tx intu gre Fi Nne e- Rtu Fne Ours tity consistency in our edit results. We take 7 groups of
(b)TextureEditingComparisons geometryeditingresultsand7groupsoftextureeditingre-
Figure 6. Analysis of our effectiveness with the na¬®ƒ±ve baselines
sults,andrenderthe350imagesofeditedavatarswithdif-
thatcanaccomplishthesingle-viewavatarediting.
ferentviewpointsandexpressions. Wecalculatethecosine
similarities between each rendered image and the single-
tencyacrossmultipleviewpointsandexpressions.
view 2D edited image and average the similarities on all
Weshowextensivetextureeditingresultsinthreeavatar
rendered images as metrics. As reported in Tab. 1, our
representationsinFig.7.Ourmethodsupportsawiderange
methodoutperformsallbaselinesinrecoveringdesirededit-
of texture editing, including global style transfer (‚ÄúAdd a
ingeffectandretainingidentityconsistency.
clown makeup‚Äù via text-driven editing), semantic-driven
editing (changing the hair color), and free-form sketch 4.4.NativeEditingCapabilityofAvatar
(painting on the face). Please refer to the supplementary
In this section, we analyze the editing capability from the
Sec.C.1/2forhybrideditingandfacereenactmentresults.
originalavatarmodelandshowthenecessityofourdesign.
4.3.QuantitativeComparison Comparison to 3DMM-based Geometry Editing. One
User Study. We conducted a user study to validate our intuitive way to support geometry editing is updating the
methodfurtherquantitatively.Followingtheevaluationpro- underlying 3DMM geometry [14]. Here, we investigate
tocol of Avatarstudio [38], users are required to watch the this method and verify its capability. Specifically, we run
rendered videos of different methods side by side and an- state-of-the-art single image-based 3DMM reconstruction
swereachquestionbypickinguponeofthemethods. For method[75],andupdatethe3DMMshapeparameterofthe
eachgroupofeditingresults,wewillaskfourquestionsthe avatarmodelwiththeestimatedone. Notethatsuchanap-Source ‚ÄúOld‚Äù ‚ÄúClown‚Äù ‚ÄúHalloween‚Äù ‚ÄúYong‚Äù ‚ÄúBlondeHair‚Äù ‚ÄúOld‚Äù ‚ÄúClown‚Äù ‚ÄúHallowee ‚ÄúYong‚Äù ‚ÄúBlondeHair‚Äù
n‚Äù
Source ‚ÄúOld‚Äù ‚ÄúClown‚Äù ‚ÄúHalloween‚Äù ‚ÄúDisney‚Äù ‚ÄúBeard‚Äù ‚ÄúOld‚Äù ‚ÄúClown‚Äù ‚ÄúHalloween‚Äù ‚ÄúDisney‚Äù ‚ÄúBeard‚Äù
Source ‚ÄúOld‚Äù ‚ÄúYoung‚Äù ‚ÄúBeard‚Äù ‚ÄúDisney‚Äù ‚ÄúBlondeHair‚Äù ‚ÄúOld‚Äù ‚ÄúYoung‚Äù ‚ÄúBeard‚Äù ‚ÄúDisney‚Äù ‚ÄúBlondeHair‚Äù
OriginalAvatar RenderedView1 RenderedView2
(a)INSTATextureEditingResultswithTextPrompt
OriginalAvatar ‚ÄúOld‚Äù ‚ÄúClown‚Äù ‚ÄúMoustache‚Äù Makeup ‚ÄúOld‚Äù ‚ÄúClown‚Äù ‚ÄúMoustache‚Äù Makeup
22DDEEddiittiinngg 2DEditing
2DEditing Rendering 2DEditing Rendering 2DEditing Rendering
(b)INSTATextureEditingResults
Rendering
2DEditing Rendering 2DEditing Rendering 2DEditing ‚ÄúMarble Statue‚Äù (d)INSTATexture
(c)NeRFBlendShapeandNext3DTextureEditingResults EditingResults
Figure7. Weshowourtextureeditingresultsusingthe2Deditingmethodwithtext-prompt, patternpaintingandmakeupdrawingon
INSTA[76]andNeRFBlendshape[16],Next3D[50]avatars.
proach is only available for those avatars that use 3DMM thesupplementarySec.C.4formoreablationstudies.
explicitly,andwetakeINSTA[76]asanexampleandshow
5.Conclusion
the results in Fig. 6 (a). The 3DMM fitting tends to fail
We have proposed a novel generic editing approach that
whentheeditedfaceisout-of-distribution,sothemagnitude
allows users to edit various volumetric head avatar repre-
ofeditingmaynotbecorrect(e.g.thefaceshape). Chang-
sentationsfromasingleimage,whereanexpression-aware
ing the 3DMM parameter could also result in blurry ren-
modification generator lifts the editing to the 3D avatar
deringfromthepre-trainedavatarmodel, whichcannotbe
while maintaining consistency across multiple expression
triviallyfixedevenafterfine-tuningtherenderingdecoder.
and viewpoints. As a limitation, we cannot add additional
Comparison to Fine-tuning for Texture Editing. Tex- objects (e.g., hat) or modify the hairstyle as shown in our
ture editing could be done by fine-tuning the avatar with supplementarySec.C.5,whichmaybeimprovedbylearn-
the one-shot edited image. We test this method on avatars ing extra specialized geometry addition and hair modifica-
fromNeRFBlendShape[16]andshowtheresultsinFig.6 tiongenerators.
(b). Whilelargestructuredchanges,e.g.,haircolor,canbe Acknowledgment: This work was partially supported by
edited, detailed editing is largely ignored. Please refer to theNSFC(No.62102356)andAntGroup.References [13] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos
Zafeiriou. Arcface: Additive angular margin loss for deep
[1] Webar/beautydemoapp. 6
face recognition. In Proceedings of the IEEE/CVF Con-
[2] RameenAbdal,Hsin-YingLee,PeihaoZhu,MengleiChai, ferenceonComputerVisionandPatternRecognition,pages
Aliaksandr Siarohin, Peter Wonka, and Sergey Tulyakov. 4690‚Äì4699,2019. 5,16
3davatargan: Bridging domains for personalized editable
[14] YaoFeng,HaiwenFeng,MichaelJBlack,andTimoBolkart.
avatars. In Proceedings of the IEEE/CVF Conference on
Learningananimatabledetailed3dfacemodelfromin-the-
Computer Vision and Pattern Recognition, pages 4552‚Äì
wildimages. ACMTransactionsonGraphics(ToG),40(4):
4562,2023. 2
1‚Äì13,2021. 7
[3] ShahRukh Athar, Zexiang Xu, Kalyan Sunkavalli, Eli
[15] Guy Gafni, Justus Thies, Michael Zollhofer, and Matthias
Shechtman,andZhixinShu.Rignerf:Fullycontrollableneu-
Nie√üner. Dynamicneuralradiancefieldsformonocular4d
ral3dportraits. InProceedingsoftheIEEE/CVFconference facialavatarreconstruction.InProceedingsoftheIEEE/CVF
onComputerVisionandPatternRecognition,pages20364‚Äì
Conference on Computer Vision and Pattern Recognition,
20373,2022. 1
pages8649‚Äì8658,2021. 1,3
[4] ZiqianBai, FeitongTan, ZengHuang, KripasindhuSarkar, [16] XuanGao,ChenglaiZhong,JunXiang,YangHong,Yudong
DanhangTang,DiQiu,AbhimitraMeka,RuofeiDu,Ming- Guo, and Juyong Zhang. Reconstructing personalized se-
songDou, SergioOrts-Escolano, etal. Learningpersonal- mantic facial nerf models from monocular video. ACM
ized high quality volumetric head avatars from monocular Transactions on Graphics (TOG), 41(6):1‚Äì12, 2022. 1, 2,
rgb videos. In Proceedings of the IEEE/CVF Conference 3,5,6,7,8,14
onComputerVisionandPatternRecognition,pages16890‚Äì
[17] Songwei Ge, Taesung Park, Jun-Yan Zhu, and Jia-Bin
16900,2023. 1,2,3
Huang. Expressivetext-to-imagegenerationwithrichtext.
[5] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, In Proceedings of the IEEE/CVF International Conference
Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng onComputerVision,pages7545‚Äì7556,2023. 2
Cui. Sine: Semantic-driven image-based nerf editing with
[18] Ayaan Haque, Matthew Tancik, Alexei A Efros, Alek-
prior-guidededitingfield. InProceedingsoftheIEEE/CVF
sanderHolynski,andAngjooKanazawa. Instruct-nerf2nerf:
Conference on Computer Vision and Pattern Recognition,
Editing 3d scenes with instructions. arXiv preprint
pages20919‚Äì20929,2023. 2
arXiv:2303.12789,2023. 2,3
[6] VBlanzandTVetter. Amorphablemodelforthesynthesis [19] Erik Ha¬®rko¬®nen, Aaron Hertzmann, Jaakko Lehtinen, and
of3dfaces.In26thAnnualConferenceonComputerGraph-
SylvainParis.Ganspace:Discoveringinterpretablegancon-
ics and Interactive Techniques (SIGGRAPH 1999), pages trols. Advances in neural information processing systems,
187‚Äì194.ACMPress,1999. 1,3 33:9841‚Äì9850,2020. 2
[7] TimBrooks,AleksanderHolynski,andAlexeiAEfros. In- [20] AmirHertz,KfirAberman,andDanielCohen-Or. Deltade-
structpix2pix:Learningtofollowimageeditinginstructions. noisingscore.InProceedingsoftheIEEE/CVFInternational
In Proceedings of the IEEE/CVF Conference on Computer ConferenceonComputerVision,pages2328‚Äì2337,2023. 2
VisionandPatternRecognition,pages18392‚Äì18402,2023.
[21] LianghuaHuang,DiChen,YuLiu,YujunShen,DeliZhao,
2,3,4,6
andJingrenZhou. Composer:Creativeandcontrollableim-
[8] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki age synthesis with composable conditions. arXiv preprint
Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, arXiv:2302.09778,2023. 2
LeonidasGuibas,JonathanTremblay,SamehKhamis,Tero
[22] Wentao Jiang, Si Liu, Chen Gao, Jie Cao, Ran He, Jiashi
Karras, and Gordon Wetzstein. Efficient geometry-aware
Feng, andShuichengYan. Psgan: Poseandexpressionro-
3D generative adversarial networks. In Proceedings of
bustspatial-awareganforcustomizablemakeuptransfer. In
theIEEE/CVFConferenceonComputerVisionandPattern
ProceedingsoftheIEEE/CVFConferenceonComputerVi-
Recognition(CVPR),2022. 2,3 sion and Pattern Recognition, pages 5194‚Äì5202, 2020. 2,
[9] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and 4
HaoSu. Tensorf: Tensorialradiancefields. InProceedings [23] Tero Karras, Samuli Laine, and Timo Aila. A style-based
oftheEuropeanConferenceonComputerVision,pages333‚Äì generator architecture for generative adversarial networks.
350.Springer,2022. 2 InProceedingsoftheIEEE/CVFconferenceoncomputervi-
[10] Anton Cherepkov, Andrey Voynov, and Artem Babenko. sionandpatternrecognition,pages4401‚Äì4410,2019. 2
Navigating the gan parameter space for semantic image [24] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
editing. In Proceedings of the IEEE/CVF conference on Jaakko Lehtinen, and Timo Aila. Analyzing and improv-
computervisionandpatternrecognition,pages3671‚Äì3680, ing the image quality of stylegan. In Proceedings of
2021. 2 the IEEE/CVF conference on computer vision and pattern
[11] deepfakes. faceswap. https://github.com/ recognition,pages8110‚Äì8119,2020. 2,4,5,13
deepfakes/faceswap, 2023. Accessed: 2023-10-10. [25] Tero Karras, Miika Aittala, Samuli Laine, Erik Ha¬®rko¬®nen,
1,2 JanneHellsten,JaakkoLehtinen,andTimoAila. Alias-free
[12] deepfakes. roop. SomdevSangwan, 2023. Accessed: generativeadversarialnetworks. AdvancesinNeuralInfor-
2023-10-10. 1,2,5,6,7,16 mationProcessingSystems,34:852‚Äì863,2021. 2[26] TianyeLi,TimoBolkart,MichaelJBlack,HaoLi,andJavier [39] Xingang Pan, Ayush Tewari, Thomas Leimku¬®hler, Lingjie
Romero. Learning a model of facial shape and expression Liu, Abhimitra Meka, and Christian Theobalt. Drag your
from4dscans. ACMTrans.Graph.,36(6):194‚Äì1,2017. 3,4 gan: Interactivepoint-basedmanipulationonthegenerative
[27] Tingting Li, Ruihe Qian, Chao Dong, Si Liu, Qiong Yan, imagemanifold.InACMSIGGRAPH2023ConferencePro-
WenwuZhu,andLiangLin.Beautygan:Instance-levelfacial ceedings,pages1‚Äì11,2023. 2,4,5
makeup transfer with deep generative adversarial network. [40] GauravParmar,KrishnaKumarSingh,RichardZhang,Yijun
InProceedingsofthe26thACMinternationalconferenceon Li,JingwanLu,andJun-YanZhu.Zero-shotimage-to-image
Multimedia,pages645‚Äì653,2018. 2,4 translation. InACMSIGGRAPH2023ConferenceProceed-
[28] K-E Lin, Alex Trevithick, Keli Cheng, Michel Sarkis, ings,pages1‚Äì11,2023. 2
MohsenGhafoorian, NingBi, GerhardReitmayr, andRavi [41] OrPatashnik,ZongzeWu,EliShechtman,DanielCohen-Or,
Ramamoorthi. Pvp: Personalized video prior for editable andDaniLischinski. Styleclip:Text-drivenmanipulationof
dynamicportraitsusingstylegan.InComputerGraphicsFo- stylegan imagery. In Proceedings of the IEEE/CVF Inter-
rum,pagee14890.WileyOnlineLibrary,2023. 1,2,5,6,7, nationalConferenceonComputerVision,pages2085‚Äì2094,
16 2021. 2,5
[29] Huan Ling, Karsten Kreis, Daiqing Li, Seung Wook Kim, [42] Ivan Perov, Daiheng Gao, Nikolay Chervoniy, Kunlin Liu,
AntonioTorralba,andSanjaFidler. Editgan:High-precision Sugasa Marangonda, Chris Ume¬¥, Mr Dpfks, Carl Shift
semantic image editing. Advances in Neural Information Facenheim, Luis RP, Jian Jiang, et al. Deepfacelab: In-
ProcessingSystems,34:16331‚Äì16345,2021. 2 tegrated, flexible and extensible face-swapping framework.
[30] Pengyang Ling, Lin Chen, Pan Zhang, Huaian Chen, arXivpreprintarXiv:2005.05535,2020. 1,2
and Yi Jin. Freedrag: Point tracking is not you need
[43] Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel
for interactive point-based image editing. arXiv preprint
Cohen-Or. Pivotaltuningforlatent-basededitingofrealim-
arXiv:2307.04684,2023. 2,4
ages. ACM Transactions on graphics (TOG), 42(1):1‚Äì13,
[31] LingjieLiu,JiataoGu,KyawZawLin,Tat-SengChua,and
2022. 4,5
Christian Theobalt. Neural sparse voxel fields. Advances
[44] SaraFridovich-KeilandAlexYu,MatthewTancik,Qinhong
inNeuralInformationProcessingSystems,33:15651‚Äì15663,
Chen, BenjaminRecht, andAngjooKanazawa. Plenoxels:
2020. 2
RadianceFieldswithoutNeuralNetworks. InCVPR,2022.
[32] AryanMikaeili,OrPerel,MehdiSafaee,DanielCohen-Or,
2
andAliMahdavi-Amiri. Sked:Sketch-guidedtext-based3d
[45] Ruizhi Shao, Jingxiang Sun, Cheng Peng, Zerong Zheng,
editing.InProceedingsoftheIEEE/CVFInternationalCon-
Boyao Zhou, Hongwen Zhang, and Yebin Liu. Con-
ferenceonComputerVision,pages14607‚Äì14619,2023. 2,
trol4d: Dynamic portrait editing by learning 4d gan from
3
2ddiffusion-basededitor. arXivpreprintarXiv:2305.20082,
[33] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
2023. 2
JonathanTBarron,RaviRamamoorthi,andRenNg. Nerf:
[46] Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. In-
Representingscenesasneuralradiancefieldsforviewsyn-
terpretingthelatentspaceofgansforsemanticfaceediting.
thesis. Communications of the ACM, 65(1):99‚Äì106, 2021.
InProceedingsoftheIEEE/CVFconferenceoncomputervi-
2
sionandpatternrecognition,pages9243‚Äì9252,2020. 2
[34] ChongMou,XintaoWang,JiechongSong,YingShan,and
[47] YujunShi,ChuhuiXue,JiachunPan,WenqingZhang,Vin-
JianZhang.Dragondiffusion:Enablingdrag-stylemanipula-
centYFTan,andSongBai.Dragdiffusion:Harnessingdiffu-
tionondiffusionmodels. arXivpreprintarXiv:2307.02421,
sionmodelsforinteractivepoint-basedimageediting. arXiv
2023. 2
preprintarXiv:2306.14435,2023. 2
[35] ThomasMu¬®ller,AlexEvans,ChristophSchied,andAlexan-
derKeller.Instantneuralgraphicsprimitiveswithamultires- [48] ChengSun,MinSun,andHwann-TzongChen.Directvoxel
olution hash encoding. arXiv preprint arXiv:2201.05989, gridoptimization:Super-fastconvergenceforradiancefields
2022. 2 reconstruction.InProceedingsoftheIEEE/CVFConference
on Computer Vision and Pattern Recognition, pages 5459‚Äì
[36] Thao Nguyen, Anh Tuan Tran, and Minh Hoai. Lip-
5469,2022. 2
stick ain‚Äôt enough: beyond color matching for in-the-wild
makeup transfer. In Proceedings of the IEEE/CVF Con- [49] JingxiangSun,XuanWang,YichunShi,LizhenWang,Jue
ference on computer vision and pattern recognition, pages Wang,andYebinLiu. Ide-3d: Interactivedisentanglededit-
13305‚Äì13314,2021. 2,4 ing for high-resolution 3d-aware portrait synthesis. arXiv
[37] Thu Nguyen-Phuoc, Gabriel Schwartz, Yuting Ye, Stephen preprintarXiv:2205.15517,2022. 2
Lombardi, and Lei Xiao. Alteredavatar: Stylizing dy- [50] JingxiangSun,XuanWang,LizhenWang,XiaoyuLi,Yong
namic3davatarswithfaststyleadaptation. arXivpreprint Zhang, HongwenZhang, andYebinLiu. Next3d: Genera-
arXiv:2305.19245,2023. 2 tiveneuraltexturerasterizationfor3d-awareheadavatars.In
[38] Mohit Mendiratta Pan, Mohamed Elgharib, Kartik Teo- ProceedingsoftheIEEE/CVFConferenceonComputerVi-
tia, AyushTewari, VladislavGolyanik, AdamKortylewski, sionandPatternRecognition,pages20991‚Äì21002,2023. 1,
Christian Theobalt, et al. Avatarstudio: Text-driven edit- 2,3,4,5,6,7,8,13,14,16
ing of 3d dynamic human head avatars. arXiv preprint [51] Feitong Tan, Sean Fanello, Abhimitra Meka, Sergio Orts-
arXiv:2306.00547,2023. 2,7,14 Escolano, Danhang Tang, Rohit Pandey, Jonathan Taylor,PingTan,andYindaZhang. Volux-gan:Agenerativemodel 2022 IEEE International Symposium on Mixed and Aug-
for 3d face synthesis with hdri relighting. In ACM SIG- mentedReality(ISMAR),pages499‚Äì507.IEEE,2022. 2
GRAPH2022ConferenceProceedings,pages1‚Äì9,2022. 7 [63] WeicaiYe,ShuoChen,ChongBao,HujunBao,MarcPolle-
[52] Junshu Tang, Bo Zhang, Binxin Yang, Ting Zhang, Dong feys, Zhaopeng Cui, and Guofeng Zhang. Intrinsicnerf:
Chen, Lizhuang Ma, and Fang Wen. 3dfaceshop: Explic- Learning intrinsic neural radiance fields for editable novel
itlycontrollable3d-awareportraitgeneration. IEEETrans- view synthesis. In Proceedings of the IEEE/CVF Inter-
actionsonVisualizationandComputerGraphics, 2023. 1, national Conference on Computer Vision, pages 339‚Äì351,
2 2023. 2
[53] ZiyanWang,TimurBagautdinov,StephenLombardi,Tomas [64] DehengZhang,ClaraFernandez-Labrador,andChristopher
Simon, Jason Saragih, Jessica Hodgins, and Michael Zoll- Schroers.Coarf:Controllable3dartisticstyletransferforra-
hofer. Learning compositional radiance fields of dynamic diancefields.In2024InternationalConferenceon3DVision
humanheads. InProceedingsoftheIEEE/CVFConference (3DV).IEEE,2024. 2
on Computer Vision and Pattern Recognition, pages 5704‚Äì [65] HaoZhang,YanboXu,TianyuanDai,TaiChi-KeungTang,
5713,2021. 2 etal. Fdnerf: Semantics-drivenfacereconstruction,prompt
[54] Yue Wu, Yu Deng, Jiaolong Yang, Fangyun Wei, Qifeng editingandrelightingwithdiffusionmodels. arXivpreprint
Chen,andXinTong.Anifacegan:Animatable3d-awareface arXiv:2306.00783,2023. 2
imagegenerationforvideoavatars. AdvancesinNeuralIn- [66] XiumingZhang,PratulPSrinivasan,BoyangDeng,PaulDe-
formation Processing Systems, 35:36188‚Äì36201, 2022. 1, bevec, William T Freeman, and Jonathan T Barron. Ner-
2 factor: Neural factorization of shape and reflectance under
[55] WeihaoXia,YulunZhang,YujiuYang,Jing-HaoXue,Bolei an unknown illumination. ACM Transactions on Graphics
Zhou, and Ming-Hsuan Yang. Gan inversion: A survey. (TOG),40(6):1‚Äì18,2021. 2
IEEE Transactions on Pattern Analysis and Machine Intel- [67] Boming Zhao, Bangbang Yang, Zhenyang Li, Zuoyue Li,
ligence,45(3):3121‚Äì3138,2022. 4 GuofengZhang,JiashuZhao,DaweiYin,ZhaopengCui,and
[56] Yinghao Xu, Sida Peng, Ceyuan Yang, Yujun Shen, and HujunBao. Factorizedandcontrollableneuralre-rendering
Bolei Zhou. 3d-aware image synthesis via learning struc- of outdoor scene for photo extrapolation. In Proceedings
tural and textural representations. In Proceedings of the of the 30th ACM International Conference on Multimedia,
IEEE/CVF Conference on Computer Vision and Pattern pages1455‚Äì1464,2022. 2
Recognition,pages18430‚Äì18439,2022. 2 [68] Yufeng Zheng, Victoria Ferna¬¥ndez Abrevaya, Marcel C
[57] Yuelang Xu, Lizhen Wang, Xiaochen Zhao, Hongwen Bu¬®hler,XuChen,MichaelJBlack,andOtmarHilliges. Im
Zhang, and Yebin Liu. Avatarmav: Fast 3d head avatar avatar:Implicitmorphableheadavatarsfromvideos.InPro-
reconstruction using motion-aware neural voxels. In ACM ceedingsoftheIEEE/CVFConferenceonComputerVision
SIGGRAPH 2023 Conference Proceedings, pages 1‚Äì10, andPatternRecognition,pages13545‚Äì13555,2022. 1,2,3
2023. 1,3 [69] Yufeng Zheng, Wang Yifan, Gordon Wetzstein, Michael J
[58] Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han Black,andOtmarHilliges. Pointavatar: Deformablepoint-
Zhou, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. based head avatars from videos. In Proceedings of the
Learningobject-compositionalneuralradiancefieldfored- IEEE/CVF Conference on Computer Vision and Pattern
itablescenerendering. InProceedingsoftheIEEE/CVFIn- Recognition,pages21057‚Äì21067,2023. 1,2,3
ternational Conference on Computer Vision, pages 13779‚Äì [70] Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-
13788,2021. 2 domain gan inversion for real image editing. In European
[59] BangbangYang,ChongBao,JunyiZeng,HujunBao,Yinda conference on computer vision, pages 592‚Äì608. Springer,
Zhang, Zhaopeng Cui, and Guofeng Zhang. Neumesh: 2020. 2
Learning disentangled neural mesh-based implicit field for [71] JiapengZhu,CeyuanYang,YujunShen,ZifanShi,BoDai,
geometry and texture editing. In European Conference on DeliZhao,andQifengChen. Linkgan: Linkingganlatents
ComputerVision,pages597‚Äì614.Springer,2022. to pixels for controllable image synthesis. In Proceedings
[60] BangbangYang,YindaZhang,YijinLi,ZhaopengCui,Sean oftheIEEE/CVFInternationalConferenceonComputerVi-
Fanello, Hujun Bao, and Guofeng Zhang. Neural render- sion,pages7656‚Äì7666,2023. 2
inginaroom: amodal3dunderstandingandfree-viewpoint [72] PeihaoZhu,RameenAbdal,YipengQin,andPeterWonka.
renderingfortheclosedscenecomposedofpre-capturedob- Sean: Image synthesis with semantic region-adaptive nor-
jects. ACM Transactions on Graphics (TOG), 41(4):1‚Äì10, malization. In Proceedings of the IEEE/CVF Conference
2022. 2 on Computer Vision and Pattern Recognition, pages 5104‚Äì
[61] Chenyu Yang, Wanrong He, Yingqing Xu, and Yang Gao. 5113,2020. 2
Elegant:Exquisiteandlocallyeditableganformakeuptrans- [73] ZihanZhu,SongyouPeng,ViktorLarsson,WeiweiXu,Hu-
fer. In European Conference on Computer Vision, pages junBao,ZhaopengCui,MartinROswald,andMarcPolle-
737‚Äì754.Springer,2022. 2 feys.Nice-slam:Neuralimplicitscalableencodingforslam.
[62] XingruiYang,HaiLi,HongjiaZhai,YuhangMing,Yuqian In Proceedings of the IEEE/CVF Conference on Computer
Liu, and Guofeng Zhang. Vox-fusion: Dense tracking and VisionandPatternRecognition,pages12786‚Äì12796,2022.
mappingwithvoxel-basedneuralimplicitrepresentation. In 2[74] Jingyu Zhuang, Chen Wang, Lingjie Liu, Liang Lin, and
GuanbinLi. Dreameditor:Text-driven3dsceneeditingwith
neuralfields. arXivpreprintarXiv:2306.13455,2023. 2,3
[75] WojciechZielonka,TimoBolkart,andJustusThies.Towards
metricalreconstructionofhumanfaces. InEuropeanCon-
ferenceonComputerVision,pages250‚Äì269.Springer,2022.
7
[76] WojciechZielonka,TimoBolkart,andJustusThies. Instant
volumetric head avatars. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages4574‚Äì4584,2023. 1,2,3,5,6,7,8,14
[77] zllrunning. face-makeup.pytorch. https://github.
com/zllrunning/face-makeup.PyTorch, 2023.
Accessed:2023-10-10. 4Supplementary Material
In this supplementary material, we first present an ethics codeinZ spacetothecodew‚ààR14√ó512inW space. The
declarationinSectionA,followedbydetailedimplementa- mappingnetworkconsistsof3fully-connectedlayerswith
tionaspectsinSectionB,whichcoversourmodelarchitec- 512 hidden sizes. Then the code w conditions the feature
ture, geometry and texture distillation, and the user study. synthesis network following the StyleGAN [24]. The fea-
More experimental results are shown in Section C. Addi- ture synthesis network consists of 7 synthesis convolution
tionally,weincludeashortvideosummarizingthemethod blocks, each of which contains 2 convolution layers and a
with video results, and an offline webpage for interactive 1√ó1convolutionlayer. Theresolutionsof7synthesiscon-
visualizationofoureditingresults. volutionblocks are4, 8, 16, 32, 64, 128, 512 respectively.
Thecodesin(2i)-thand(2i+1)-throwofcodewmodu-
A.EthicsDeclaration latetheweightsof(i)-thsynthesisblock. Theoutputofthe
featuresynthesisnetworkis256√ó256√ó32neuralfeature
In this paper, we present this ethics declaration to under- map. Wepre-definetheUVmappingbetweenthevertices
lineourcommitmenttoresponsiblescientificinquirywithin of the 3DMM mesh and neural feature map, and rasterize
the field of computer vision. Our work uses open-sourced the neural feature map to the four axis-aligned plane (one
datasets,carefullychosentoensurethattheywerecollected parallel to the positive face, two parallel to the side face,
withthe fullconsentof theparticipantsinvolved. The pri- oneparalleltothetopofthehead)togeneratethetri-plane
vacy and rights of individuals are paramount in our re- features.Thetwosideplanesareusedtocollectthefeatures
search,andwehavetakenstepstosafeguardthesebyimple- inleft-sideandright-sidefaceswhichwillbesummedupto
mentingstrictguidelinesthatgoverntheuseofourresearch generatethefinalside-planefeature. Themodificationfea-
outputs. We acknowledge the importance of diversity and ture of input query point x is collected by projecting x to
haveselectedourdatasetswiththeaim ofpreventingbias, thetri-planeandsummingupthebi-linearinterpolatedfea-
ensuringthatourmethodsarefairandinclusiveacrossvar- turefromthetri-plane. Forgeometryediting,thegeometry
ious demographics. Our research is purely academic, and modificationdecodertakesthemodificationfeatureasinput
anyheadeditingcarriedoutisforthepurposeofvalidating andoutputsatranslationvectortoshiftthextox‚Ä≤. Thege-
the effectiveness of our methods. We explicitly state that ometry modification decoder consists of 4 fully connected
our research does not involve human experimentation and layers with 256 hidden sizes and a translation head. For
that all human-derived data has been responsibly sourced texture editing, the texture modification decoder takes the
and vetted for ethical compliance. We affirm that our re- modificationfeatureasinputandoutputsablendingweight
searchisintended solelyforscientificadvancementand to and modification color value to modify the original color
test the robustness of our methods. There is no intention usingEq.(3). Thetexturemodificationdecoderconsistsof
to vilify or harm any individual or group. Our aim is to 4fully-connectedlayerswith256hiddensizesandablend-
contribute to the field of computer vision in a way that is ingweightheadandamodificationcolorhead.
ethically sound, socially responsible, and cognizant of the
long-termimplicationsofourwork. Weembraceopendis- B.2.GeometryDistillation
cussions about our ethical approach and are committed to
AsillustratedinFig.I,weobservethattheshapeofthehead
transparency and ethical integrity in all aspects of our re- is distorted when the mean value Œ≤¬Øof 3DMM shape coef-
search.
ficient Œ≤ is larger than 1.0, e.g., the two heads on the far
leftdeviatefromthestandardshapedefinitionofthehuman
B.ImplementationDetails
head. Furthermore,theincreasingofthestandarddeviation
Œ≤n of3DMMshapeparameterŒ≤ willleadtotheasymme-
B.1.ModelArchitecture
try in the shape, e.g., the shape of the first and third head
Our model follows the architecture of 3DMM-based is asymmetrical. Therefore, we sample 3DMM shape pa-
3DGAN[50]thatcontainsaStyleGAN-basedfeaturegen- rametersŒ≤fromanormaldistributionwhoseabsolutemean
eratorandafeaturedecoder. Specifically,thefeaturegener- andstandarddeviationarerandomlyselectedwithin[0,1],
ator takes a modification code z ‚àà R1024 as input, and andsampletheeditvectorŒ≤ fromtheuniformdistribution
g/n ‚àÜ
hasamappingnetworkandafeaturesynthesisnetwork. A U(‚àí3,3)tokeeptheŒ≤¬Øwithin[-1,1]andŒ≤¬Øsmallaspossi-
mappingnetworkisemployedtotransformthemodification ble.ExpressionVariation ViewpointVariation ExpressionVariation ViewpointVariation ExpressionVariation ViewpointVariation
Original
Avatar
Edited
Avatar
View1, View1, View2, View1, View1, View2, View1, View1, View2,
Expression1 Expression2 Expression1 Expression1 Expression2 Expression1 Expression1 Expression2 Expression1
FigureH.Weshowsomeavatarssampledinthegeometrymodificationlearning.
B.3.TextureDistillation
Weshowsomepairsofvolumetricavatarsthataresampled
fortexturemodificationlearninginFig.K.Ourtexturedis-
tillation scheme enables the generation of a diverse array
oftextureeditingdatathatisconsistentacrossdifferentex-
pressionsandviewpoints. Thisincludes, forinstance, par-
ùõΩ=2.01, ùõΩ=‚àí2.83, ùõΩ=0.13, ùõΩ=0.37, ùõΩ=0.99,
ùõΩ!=1.42 ùõΩ!=0.33 ùõΩ!=1.79 ùõΩ!=0.79 ùõΩ!=0.63 tial makeup on the first head, intricate makeup designs on
the second head head, and free-style makeup on the third
FigureI.Weshow3DMMmeshessampledfromdifferentshape
head in Fig. K. Such variety in texture edits greatly en-
coefficientsŒ≤.
hancestheflexibilityofourtexturemodificationgenerator.
Furthermore, the texture editing data encompasses modi-
fications on a range of facial features, represented across
variousgenders,ages,andsexes,therebysubstantiallyaug-
mentingthegeneralizabilityofourmethod.
B.4.UserStudy
ùúÉ!"#=0.0¬∞ ùúÉ!"#=6.0¬∞ ùúÉ!"#=9.0¬∞ ùúÉ$%&‚Äô"%=14.3¬∞ ùúÉ$%&‚Äô"%=‚àí9.2¬∞ Ourquestionnairecontains12editingcases,6forgeometry
(a)3DMMMeshfromSamplingonùúÉ!"# (b)CorruptedImagesfromNext3DwithùúÉ$%&‚Äô"%
editingand6fortextureediting. Theseeditingcasescover
Figure J. We show the 3DMM meshes sampled from different the editing on 9 heads from the INSTA [76] and NeRF-
posesofjawŒ∏ andcorruptedimagegeneratedbyNext3D[50] BlendShape [16]. For each editing case, there are 4 ques-
jaw
whenthe3DMMmeshissampledwithŒ∏ global. tionsfollowingtheAvatarStudio[38]:
‚Ä¢ Which method better follows the given input edited im-
age?
‚Ä¢ Whichmethodbetterretainstheidentityoftheinputse-
For 3DMM pose coefficient Œ∏ sampling, we only sam-
quenceinthevideo?
pledifferentposecoefficientsofthejawŒ∏ andkeepthe
jaw ‚Ä¢ Which method better maintains temporal consistency in
othersfixedtocomplywiththe3DMMposerangeallowed
thevideo?
by Next3D [50], e.g., the generated face is corrupted with
‚Ä¢ Which method is better overall considering the above 3
Œ∏ sincethefaceisassumedtoalwayslocateattheorig-
global aspectsinthevideo?
inalpointwithoutrotationasillustratedinFig.J(b).
Participantsareshownanoriginalimage, aneditedimage,
We show some pairs of volumetric avatars that are and four videos rendered from four methods side by side,
sampled for geometry modification learning in Fig. H. and asked to select one of four methods to answer each
The proposed geometry distillation scheme can result in question.
a wide range of consistent geometry editing data across
B.5. Comparison to 3DMM-based Geometry Edit-
expressions and viewpoints, which promotes expression-
ing
dependent geometry modification learning. The geometry
editingdatacontainsgeometrymodificationsonvariousfa- Optimization-based3DMMfittingtypicallyrequiresdense
cial features across different genders, ages and sex, which landmarks (better in 3D) and/or multi-view images to
promotesthegeneralizationabilityofourmethod. achieve reconstruction quality. However, our goal is toExpressionVariation ViewpointVariation ExpressionVariation ViewpointVariation ExpressionVariation ViewpointVariation
Original
Avatar
Edited
Avatar
View1, View1, View2, View1, View1, View2, View1, View1, View2,
Expression1 Expression2 Expression1 Expression1 Expression2 Expression1 Expression1 Expression2 Expression1
FigureK.Weshowsomeavatarssampledinthetexturemodificationlearning.
Driving
Rendering
Eyes++, Face--
Driving
Rendering
BlushPainting
2DEditing FaceReenactment
FigureL.Weshowthefacereenactmentresultsontheeditedavatarswithourmodificationfield.
achievesingleview-basedvolumetricavatarediting,where B.6.EditingEfficiencyandModelComplexity
weonlyhaveaccesstooneperspectiveview. Thefittingis
error-prone, especially for out-of-domain cases in this set-
ting. AsillustratedinFig.N,3DMMfittingwith2Dland-
marks from a single image cannot well constrain the 3D Ourmethodtakes75secondsforgeometryeditingand164
shapenomatterwith(b)regularregularizationor(c)weak seconds for texture editing over a Next3D-based avatar on
regularization (for better landmarkfitting). In contrast, (a) anRTX4090GPU.Theeditingspeedislargelydetermined
our3Deditingusesthelearnedpriortofaithfullyguidethe bythebackbonearchitecture. Designinganefficientback-
editingfromlimitedconstraints. bone for real-time editing is out of the scope of this paper
but an interesting future direction. Our model size is 234
MB.Foravatarediting,itrequires9.1GBGPUmemoryto
performauto-decodingoptimization.Ears++
‚ÄúClown‚Äù
2DTexture W/oseg. 2DTexture W/oseg.
Editing reweight Ours Editing reweight Ours
FigureO.Weinspecttheefficacyofthesegmentation-basedloss
Nose--,
Lips++ reweightingstrategy.
‚ÄúKratos‚Äù
C.MoreExperiments
Nose--, C.1.HybridEditing
Lips++
Makeup We show the hybrid editing results in Fig. M. We can edit
thegeometryoftheavatarwhilechangingthetexturewitha
2DHybridEditing textpromptormakeupimage.Therenderednovelviewsare
OriginalView RenderedNovelViews
(Geometry&Texture) consistent across multiple viewpoints and expressions and
FigureM.Weshowhybrideditingresultswithgeometryandtex- presentvividappearances,e.g.,clownmakeupandenlarged
tureediting. eyes on the first head, and ‚ÄùKratos‚Äù makeup and enlarged
lipsandreducednosegiveafierceappearanceonthesecond
headinFig.M.
GPrTojected GPrTojected
C.2.FaceReenactment
WeshowtheresultsoffacereenactmentinFig.L.Ourge-
Jaw--
ometry and texture modification seamlessly follow the ex-
2D Geometry Editing Landmark Vis.3DMM Recon.Landmark Vis.3DMM Recon. pressionsfromthedrivingvideo,andpresentconsistentre-
sults across various viewpoints and expressions. This pro-
videsgreatpotentialfortheVR/ARandlivebroadcastsof
digitalavatars.
Edited Novel Views Edited Novel Views Edited Novel Views
C.3.GeometryVisualizationonGeometryEditing
(a) Our Method (b)3DMMFitting (c) 3DMM Fitting & Weak Reg.
FigureN.Weshowamorethoroughcomparisonwiththe3DMM- We visualize the normal of meshes extracted from the
basedgeometryediting. Theparametricregularizationin3DMM volumetric avatar under various expressions in Fig. R.
fitting is tuned to enhance landmark alignment, albeit at the ex- Given a single edited image, our method faithfully mod-
penseofintroducingdistortionstotheresulting3Dgeometry. ifies the geometry of the avatars with multi-view consis-
tency, e.g., the enlarged ears with consistent geometry
Geometry Texture across multiple viewpoints in the last row of Fig. R. Fur-
Imageidentitysimilarity
Roop PVP Next3D Ours Roop PVP Next3D Ours thermore,ourexpression-dependentgeometrymodification
Mean‚Üë 0.8373 0.8704 0.8547 0.8845 0.7320 0.8476 0.8500 0.9147
Median‚Üë 0.8447 0.8836 0.8680 0.8854 0.7828 0.8608 0.8674 0.9181 seamlesslyadaptstodifferentexpressions,e.g.,theenlarged
SD‚Üì 0.0264 0.0400 0.0449 0.0448 0.1173 0.0407 0.0340 0.0310
noseandlipspresentconsistentresultsacrossmultipleex-
RSD(%)‚Üì 3.16 4.60 5.25 5.06 16.03 4.80 4.00 3.39
pressionsinthesecondrowofFig.R.
Table B. We show the mean, median, standard deviation(SD),
standard deviation (SD), and relative standard deviation (RSD) C.4.Ablations
of the quantitative comparisons with the PVP [28], Roop [12],
Segmentation-based Loss Reweighting Strategy We in-
Next3D[50]onimageidentitysimilarity[13].
specttheefficacyofthesegmentation-basedlossreweight-
ing strategy by replacing this strategy with averaging the
L2lossofthewholeimageduringauto-decodingoptimiza-
B.7.Statisticalanalysisofquantitativecomparisons tion. As depicted in Fig. O, the absence of the reweight-
ingstrategyresultsinaninabilitytoreconstructfine-grained
As shown in Tab. B, We show the mean, median, stan- makeupsincethesemakeupsoccupysmallregionsthathave
darddeviation(SD),andrelativestandarddeviation(RSD) a negligible impact on the loss, e.g., the missing red eye
of image identity similarity below. Our method surpasses shadow on the left head and untouched color of eyes on
othermethodsinmeanandmedianbutalsohasasmallde- therightheadinFig.O.Incontrast, ourmethodcanaccu-
viationinSDandRSD. rately reconstruct the makeup from a single edited image2DEditing (a) w/o‚Ñí )(ùúÜ *=0) (b) weak‚Ñí )(ùúÜ *=1) (c) w/o ‚Ñí &‚Äô((ùúÜ )=0) (d) w/o ‚Ñí !"#"$(ùúÜ %=0) (e) Full Model
FigureP.WeinspecttheefficacyofdifferentlosstermsinEq.(5)whenperformingavatarediting.
Method PSNR‚Üë SSIM‚Üë LPIPS‚Üì
#mod.code=32+512 25.47 0.8508 0.0966
#mod.code=128+512 27.69 0.8674 0.0803
#mod.code=512+512(ours) 27.75 0.8685 0.0798
Table C. We quantitatively inspect the efficacy of dimensions of
themodificationlatentcodeonavatarediting.
Nose--,Face++
OriginalView 2DEditing W/oCodeGuidance Ours
Method PSNR‚Üë SSIM‚Üë LPIPS‚Üì
FigureQ.Weinspecttheefficacyoftheimplicitlatentspaceguid-
W/oCodeGuidance 21.30 0.7543 0.6066
ance.
Ours 35.42 0.9398 0.0308
TableD.Wequantitativelyinspecttheefficacyoftheimplicitla-
andpresentconsistentresultsacrossmultipleexpressions. tentspaceguidanceonthenovelviewsynthesisofeditedavatars
intraining.
ImplicitLatentSpaceGuidanceWeablatetheimplicitla-
tent space guidance by fully sampling a modification code
of1024dimensionsfromastandardnormaldistributionin-
C.5.Limitations
steadoftheconcatenationofateachercodeandareduced
modification code of 512 dimensions during training. We As illustrated in Fig. S, We show hard cases by (a-b)
takethetrainingofthegeometrymodificationgeneratoras addingadditionalobjects(e.g.,addhat)and(c-d)changing
an example. As shown in Tab. D, we quantitatively evalu- hairstyle(e.g.addfringe)inthefollowingfigure.Asshown,
atethequalityofnovelviewsynthesisonthetrainingdata. ourmethodreconstructsroughbutincompleteshapes. The
Specifically, we render images of the edited avatar under texturealsolooksblurryduetothemissingofproperprior.
novel viewpoints as ground truth, and apply the modifi-
cation fields from two methods to the original avatar, and
quantitativelycomparetherenderedmodifiedimagesfrom
two methods with the ground truth. Our methods surpass
themethodwithouttheimplicitlatentspaceguidanceinall
metrics. The implicit latent space guidance improves the
convergencesontrainingdata.Then,weevaluatetwometh-
ods in a novel geometry editing case where auto-decoding
optimization is performed to infer the modification field
fromasingleeditedimage. AsillustratedintheFig.Q,the
method without the implicit latent space guidance fails to
generalizeonthenoveleditingcaseandresultsinablurred
andcorruptedimage. Incontrast,ourmethodcanfaithfully
rendertheimageoftheeditedavatarundernovelviewpoint
andexpression.
Hyper-parameters. As shown in the Tab. C, we hereby
provide ablation over dimensions of modification latent
space. AsillustratedintheFig.P,wealsoshowtheimpact
oflossweightsofEq.(5). (a-b): Thefine-grainedmakeup
cannot be faithfully reconstructed without L or with a
2
weakL . (c-d): Somecolordistortionoccurswithoutreg-
2
ularizationL orglobalappearanceconstraintL .
reg lpipsNose--, Lip++
Ear++, Brow++
2D Editing Rendered Views Extracted Mesh Normal
FigureR.Weshowthemeshnormaloforiginalavatarsandeditedavatarsingeometryediting.
(a) Add hat (b) Add hat
(c) Add fringe (d) Make afro
2DEditing RenderedEdited Views Mesh Normal 2DEditing RenderedEdited Views Mesh Normal
FigureS.Weshowsomefailurecasesinourmethodwhereweaddtheadditionalobject(a-b)andchangethehairstyle(c-d).