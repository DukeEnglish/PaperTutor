[
    {
        "title": "Emergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning",
        "authors": "Samuel ToveyChristoph LohrmannChristian Holm",
        "links": "http://arxiv.org/abs/2404.01999v1",
        "entry_id": "http://arxiv.org/abs/2404.01999v1",
        "pdf_url": "http://arxiv.org/pdf/2404.01999v1",
        "summary": "Reinforcement learning (RL) is a flexible and efficient method for\nprogramming micro-robots in complex environments. Here we investigate whether\nreinforcement learning can provide insights into biological systems when\ntrained to perform chemotaxis. Namely, whether we can learn about how\nintelligent agents process given information in order to swim towards a target.\nWe run simulations covering a range of agent shapes, sizes, and swim speeds to\ndetermine if the physical constraints on biological swimmers, namely Brownian\nmotion, lead to regions where reinforcement learners' training fails. We find\nthat the RL agents can perform chemotaxis as soon as it is physically possible\nand, in some cases, even before the active swimming overpowers the stochastic\nenvironment. We study the efficiency of the emergent policy and identify\nconvergence in agent size and swim speeds. Finally, we study the strategy\nadopted by the reinforcement learning algorithm to explain how the agents\nperform their tasks. To this end, we identify three emerging dominant\nstrategies and several rare approaches taken. These strategies, whilst\nproducing almost identical trajectories in simulation, are distinct and give\ninsight into the possible mechanisms behind which biological agents explore\ntheir environment and respond to changing conditions.",
        "updated": "2024-04-02 14:42:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.01999v1"
    },
    {
        "title": "Safe Interval RRT* for Scalable Multi-Robot Path Planning in Continuous Space",
        "authors": "Joonyeol SimJoonkyung KimChangjoo Nam",
        "links": "http://arxiv.org/abs/2404.01752v1",
        "entry_id": "http://arxiv.org/abs/2404.01752v1",
        "pdf_url": "http://arxiv.org/pdf/2404.01752v1",
        "summary": "In this paper, we consider the problem of Multi-Robot Path Planning (MRPP) in\ncontinuous space to find conflict-free paths. The difficulty of the problem\narises from two primary factors. First, the involvement of multiple robots\nleads to combinatorial decision-making, which escalates the search space\nexponentially. Second, the continuous space presents potentially infinite\nstates and actions. For this problem, we propose a two-level approach where the\nlow level is a sampling-based planner Safe Interval RRT* (SI-RRT*) that finds a\ncollision-free trajectory for individual robots. The high level can use any\nmethod that can resolve inter-robot conflicts where we employ two\nrepresentative methods that are Prioritized Planning (SI-CPP) and Conflict\nBased Search (SI-CCBS). Experimental results show that SI-RRT* can find a\nhigh-quality solution quickly with a small number of samples. SI-CPP exhibits\nimproved scalability while SI-CCBS produces higher-quality solutions compared\nto the state-of-the-art planners for continuous space. Compared to the most\nscalable existing algorithm, SI-CPP achieves a success rate that is up to 94%\nhigher with 100 robots while maintaining solution quality (i.e., flowtime, the\nsum of travel times of all robots) without significant compromise. SI-CPP also\ndecreases the makespan up to 45%. SI-CCBS decreases the flowtime by 9% compared\nto the competitor, albeit exhibiting a 14% lower success rate.",
        "updated": "2024-04-02 09:07:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.01752v1"
    },
    {
        "title": "Distributed Autonomous Swarm Formation for Dynamic Network Bridging",
        "authors": "Raffaele GallieraThies MöhlenhofAlessandro AmatoDaniel DuranKristen Brent VenableNiranjan Suri",
        "links": "http://arxiv.org/abs/2404.01557v1",
        "entry_id": "http://arxiv.org/abs/2404.01557v1",
        "pdf_url": "http://arxiv.org/pdf/2404.01557v1",
        "summary": "Effective operation and seamless cooperation of robotic systems are a\nfundamental component of next-generation technologies and applications. In\ncontexts such as disaster response, swarm operations require coordinated\nbehavior and mobility control to be handled in a distributed manner, with the\nquality of the agents' actions heavily relying on the communication between\nthem and the underlying network. In this paper, we formulate the problem of\ndynamic network bridging in a novel Decentralized Partially Observable Markov\nDecision Process (Dec-POMDP), where a swarm of agents cooperates to form a link\nbetween two distant moving targets. Furthermore, we propose a Multi-Agent\nReinforcement Learning (MARL) approach for the problem based on Graph\nConvolutional Reinforcement Learning (DGN) which naturally applies to the\nnetworked, distributed nature of the task. The proposed method is evaluated in\na simulated environment and compared to a centralized heuristic baseline\nshowing promising results. Moreover, a further step in the direction of\nsim-to-real transfer is presented, by additionally evaluating the proposed\napproach in a near Live Virtual Constructive (LVC) UAV framework.",
        "updated": "2024-04-02 01:45:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.01557v1"
    },
    {
        "title": "Multi-Agent Reinforcement Learning with Control-Theoretic Safety Guarantees for Dynamic Network Bridging",
        "authors": "Raffaele GallieraKonstantinos MitsopoulosNiranjan SuriRaffaele Romagnoli",
        "links": "http://arxiv.org/abs/2404.01551v1",
        "entry_id": "http://arxiv.org/abs/2404.01551v1",
        "pdf_url": "http://arxiv.org/pdf/2404.01551v1",
        "summary": "Addressing complex cooperative tasks in safety-critical environments poses\nsignificant challenges for Multi-Agent Systems, especially under conditions of\npartial observability. This work introduces a hybrid approach that integrates\nMulti-Agent Reinforcement Learning with control-theoretic methods to ensure\nsafe and efficient distributed strategies. Our contributions include a novel\nsetpoint update algorithm that dynamically adjusts agents' positions to\npreserve safety conditions without compromising the mission's objectives.\nThrough experimental validation, we demonstrate significant advantages over\nconventional MARL strategies, achieving comparable task performance with zero\nsafety violations. Our findings indicate that integrating safe control with\nlearning approaches not only enhances safety compliance but also achieves good\nperformance in mission objectives.",
        "updated": "2024-04-02 01:30:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.01551v1"
    },
    {
        "title": "GOV-REK: Governed Reward Engineering Kernels for Designing Robust Multi-Agent Reinforcement Learning Systems",
        "authors": "Ashish RanaMichael OesterleJannik Brinkmann",
        "links": "http://arxiv.org/abs/2404.01131v1",
        "entry_id": "http://arxiv.org/abs/2404.01131v1",
        "pdf_url": "http://arxiv.org/pdf/2404.01131v1",
        "summary": "For multi-agent reinforcement learning systems (MARLS), the problem\nformulation generally involves investing massive reward engineering effort\nspecific to a given problem. However, this effort often cannot be translated to\nother problems; worse, it gets wasted when system dynamics change drastically.\nThis problem is further exacerbated in sparse reward scenarios, where a\nmeaningful heuristic can assist in the policy convergence task. We propose\nGOVerned Reward Engineering Kernels (GOV-REK), which dynamically assign reward\ndistributions to agents in MARLS during its learning stage. We also introduce\ngovernance kernels, which exploit the underlying structure in either state or\njoint action space for assigning meaningful agent reward distributions. During\nthe agent learning stage, it iteratively explores different reward distribution\nconfigurations with a Hyperband-like algorithm to learn ideal agent reward\nmodels in a problem-agnostic manner. Our experiments demonstrate that our\nmeaningful reward priors robustly jumpstart the learning process for\neffectively learning different MARL problems.",
        "updated": "2024-04-01 14:19:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.01131v1"
    }
]