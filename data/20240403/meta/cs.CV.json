[
    {
        "title": "Segment Any 3D Object with Language",
        "authors": "Seungjun LeeYuyang ZhaoGim Hee Lee",
        "links": "http://arxiv.org/abs/2404.02157v1",
        "entry_id": "http://arxiv.org/abs/2404.02157v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02157v1",
        "summary": "In this paper, we investigate Open-Vocabulary 3D Instance Segmentation\n(OV-3DIS) with free-form language instructions. Earlier works that rely on only\nannotated base categories for training suffer from limited generalization to\nunseen novel categories. Recent works mitigate poor generalizability to novel\ncategories by generating class-agnostic masks or projecting generalized masks\nfrom 2D to 3D, but disregard semantic or geometry information, leading to\nsub-optimal performance. Instead, generating generalizable but semantic-related\nmasks directly from 3D point clouds would result in superior outcomes. In this\npaper, we introduce Segment any 3D Object with LanguagE (SOLE), which is a\nsemantic and geometric-aware visual-language learning framework with strong\ngeneralizability by generating semantic-related masks directly from 3D point\nclouds. Specifically, we propose a multimodal fusion network to incorporate\nmultimodal semantics in both backbone and decoder. In addition, to align the 3D\nsegmentation model with various language instructions and enhance the mask\nquality, we introduce three types of multimodal associations as supervision.\nOur SOLE outperforms previous methods by a large margin on ScanNetv2,\nScanNet200, and Replica benchmarks, and the results are even close to the\nfully-supervised counterpart despite the absence of class annotations in the\ntraining. Furthermore, extensive qualitative results demonstrate the\nversatility of our SOLE to language instructions.",
        "updated": "2024-04-02 17:59:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02157v1"
    },
    {
        "title": "Alpha Invariance: On Inverse Scaling Between Distance and Volume Density in Neural Radiance Fields",
        "authors": "Joshua AhnHaochen WangRaymond A. YehGreg Shakhnarovich",
        "links": "http://arxiv.org/abs/2404.02155v1",
        "entry_id": "http://arxiv.org/abs/2404.02155v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02155v1",
        "summary": "Scale-ambiguity in 3D scene dimensions leads to magnitude-ambiguity of\nvolumetric densities in neural radiance fields, i.e., the densities double when\nscene size is halved, and vice versa. We call this property alpha invariance.\nFor NeRFs to better maintain alpha invariance, we recommend 1) parameterizing\nboth distance and volume densities in log space, and 2) a\ndiscretization-agnostic initialization strategy to guarantee high ray\ntransmittance. We revisit a few popular radiance field models and find that\nthese systems use various heuristics to deal with issues arising from scene\nscaling. We test their behaviors and show our recipe to be more robust.",
        "updated": "2024-04-02 17:58:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02155v1"
    },
    {
        "title": "Dynamic Pre-training: Towards Efficient and Scalable All-in-One Image Restoration",
        "authors": "Akshay DudhaneOmkar ThawakarSyed Waqas ZamirSalman KhanFahad Shahbaz KhanMing-Hsuan Yang",
        "links": "http://arxiv.org/abs/2404.02154v1",
        "entry_id": "http://arxiv.org/abs/2404.02154v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02154v1",
        "summary": "All-in-one image restoration tackles different types of degradations with a\nunified model instead of having task-specific, non-generic models for each\ndegradation. The requirement to tackle multiple degradations using the same\nmodel can lead to high-complexity designs with fixed configuration that lack\nthe adaptability to more efficient alternatives. We propose DyNet, a dynamic\nfamily of networks designed in an encoder-decoder style for all-in-one image\nrestoration tasks. Our DyNet can seamlessly switch between its bulkier and\nlightweight variants, thereby offering flexibility for efficient model\ndeployment with a single round of training. This seamless switching is enabled\nby our weights-sharing mechanism, forming the core of our architecture and\nfacilitating the reuse of initialized module weights. Further, to establish\nrobust weights initialization, we introduce a dynamic pre-training strategy\nthat trains variants of the proposed DyNet concurrently, thereby achieving a\n50% reduction in GPU hours. To tackle the unavailability of large-scale dataset\nrequired in pre-training, we curate a high-quality, high-resolution image\ndataset named Million-IRD having 2M image samples. We validate our DyNet for\nimage denoising, deraining, and dehazing in all-in-one setting, achieving\nstate-of-the-art results with 31.34% reduction in GFlops and a 56.75% reduction\nin parameters compared to baseline models. The source codes and trained models\nare available at https://github.com/akshaydudhane16/DyNet.",
        "updated": "2024-04-02 17:58:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02154v1"
    },
    {
        "title": "GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from a Single Image",
        "authors": "Chong BaoYinda ZhangYuan LiXiyu ZhangBangbang YangHujun BaoMarc PollefeysGuofeng ZhangZhaopeng Cui",
        "links": "http://arxiv.org/abs/2404.02152v1",
        "entry_id": "http://arxiv.org/abs/2404.02152v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02152v1",
        "summary": "Recently, we have witnessed the explosive growth of various volumetric\nrepresentations in modeling animatable head avatars. However, due to the\ndiversity of frameworks, there is no practical method to support high-level\napplications like 3D head avatar editing across different representations. In\nthis paper, we propose a generic avatar editing approach that can be\nuniversally applied to various 3DMM driving volumetric head avatars. To achieve\nthis goal, we design a novel expression-aware modification generative model,\nwhich enables lift 2D editing from a single image to a consistent 3D\nmodification field. To ensure the effectiveness of the generative modification\nprocess, we develop several techniques, including an expression-dependent\nmodification distillation scheme to draw knowledge from the large-scale head\navatar model and 2D facial texture editing tools, implicit latent space\nguidance to enhance model convergence, and a segmentation-based loss reweight\nstrategy for fine-grained texture inversion. Extensive experiments demonstrate\nthat our method delivers high-quality and consistent results across multiple\nexpression and viewpoints. Project page: https://zju3dv.github.io/geneavatar/",
        "updated": "2024-04-02 17:58:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02152v1"
    },
    {
        "title": "Diffusion$^2$: Dynamic 3D Content Generation via Score Composition of Orthogonal Diffusion Models",
        "authors": "Zeyu YangZijie PanChun GuLi Zhang",
        "links": "http://arxiv.org/abs/2404.02148v1",
        "entry_id": "http://arxiv.org/abs/2404.02148v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02148v1",
        "summary": "Recent advancements in 3D generation are predominantly propelled by\nimprovements in 3D-aware image diffusion models which are pretrained on\nInternet-scale image data and fine-tuned on massive 3D data, offering the\ncapability of producing highly consistent multi-view images. However, due to\nthe scarcity of synchronized multi-view video data, it is impractical to adapt\nthis paradigm to 4D generation directly. Despite that, the available video and\n3D data are adequate for training video and multi-view diffusion models that\ncan provide satisfactory dynamic and geometric priors respectively. In this\npaper, we present Diffusion$^2$, a novel framework for dynamic 3D content\ncreation that leverages the knowledge about geometric consistency and temporal\nsmoothness from these models to directly sample dense multi-view and\nmulti-frame images which can be employed to optimize continuous 4D\nrepresentation. Specifically, we design a simple yet effective denoising\nstrategy via score composition of video and multi-view diffusion models based\non the probability structure of the images to be generated. Owing to the high\nparallelism of the image generation and the efficiency of the modern 4D\nreconstruction pipeline, our framework can generate 4D content within few\nminutes. Furthermore, our method circumvents the reliance on 4D data, thereby\nhaving the potential to benefit from the scalability of the foundation video\nand multi-view diffusion models. Extensive experiments demonstrate the efficacy\nof our proposed framework and its capability to flexibly adapt to various types\nof prompts.",
        "updated": "2024-04-02 17:58:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02148v1"
    }
]