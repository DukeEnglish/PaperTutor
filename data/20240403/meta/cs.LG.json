[
    {
        "title": "Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks",
        "authors": "Maksym AndriushchenkoFrancesco CroceNicolas Flammarion",
        "links": "http://arxiv.org/abs/2404.02151v1",
        "entry_id": "http://arxiv.org/abs/2404.02151v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02151v1",
        "summary": "We show that even the most recent safety-aligned LLMs are not robust to\nsimple adaptive jailbreaking attacks. First, we demonstrate how to successfully\nleverage access to logprobs for jailbreaking: we initially design an\nadversarial prompt template (sometimes adapted to the target LLM), and then we\napply random search on a suffix to maximize the target logprob (e.g., of the\ntoken \"Sure\"), potentially with multiple restarts. In this way, we achieve\nnearly 100\\% attack success rate -- according to GPT-4 as a judge -- on\nGPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was\nadversarially trained against the GCG attack. We also show how to jailbreak all\nClaude models -- that do not expose logprobs -- via either a transfer or\nprefilling attack with 100\\% success rate. In addition, we show how to use\nrandom search on a restricted set of tokens for finding trojan strings in\npoisoned models -- a task that shares many similarities with jailbreaking --\nwhich is the algorithm that brought us the first place in the SaTML'24 Trojan\nDetection Competition. The common theme behind these attacks is that adaptivity\nis crucial: different models are vulnerable to different prompting templates\n(e.g., R2D2 is very sensitive to in-context learning prompts), some models have\nunique vulnerabilities based on their APIs (e.g., prefilling for Claude), and\nin some settings it is crucial to restrict the token search space based on\nprior knowledge (e.g., for trojan detection). We provide the code, prompts, and\nlogs of the attacks at https://github.com/tml-epfl/llm-adaptive-attacks.",
        "updated": "2024-04-02 17:58:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02151v1"
    },
    {
        "title": "Robustly estimating heterogeneity in factorial data using Rashomon Partitions",
        "authors": "Aparajithan VenkateswaranAnirudh SankarArun G. ChandrasekharTyler H. McCormick",
        "links": "http://arxiv.org/abs/2404.02141v1",
        "entry_id": "http://arxiv.org/abs/2404.02141v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02141v1",
        "summary": "Many statistical analyses, in both observational data and randomized control\ntrials, ask: how does the outcome of interest vary with combinations of\nobservable covariates? How do various drug combinations affect health outcomes,\nor how does technology adoption depend on incentives and demographics? Our goal\nis to partition this factorial space into ``pools'' of covariate combinations\nwhere the outcome differs across the pools (but not within a pool). Existing\napproaches (i) search for a single ``optimal'' partition under assumptions\nabout the association between covariates or (ii) sample from the entire set of\npossible partitions. Both these approaches ignore the reality that, especially\nwith correlation structure in covariates, many ways to partition the covariate\nspace may be statistically indistinguishable, despite very different\nimplications for policy or science. We develop an alternative perspective,\ncalled Rashomon Partition Sets (RPSs). Each item in the RPS partitions the\nspace of covariates using a tree-like geometry. RPSs incorporate all partitions\nthat have posterior values near the maximum a posteriori partition, even if\nthey offer substantively different explanations, and do so using a prior that\nmakes no assumptions about associations between covariates. This prior is the\n$\\ell_0$ prior, which we show is minimax optimal. Given the RPS we calculate\nthe posterior of any measurable function of the feature effects vector on\noutcomes, conditional on being in the RPS. We also characterize approximation\nerror relative to the entire posterior and provide bounds on the size of the\nRPS. Simulations demonstrate this framework allows for robust conclusions\nrelative to conventional regularization techniques. We apply our method to\nthree empirical settings: price effects on charitable giving, chromosomal\nstructure (telomere length), and the introduction of microfinance.",
        "updated": "2024-04-02 17:53:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02141v1"
    },
    {
        "title": "Topic-based Watermarks for LLM-Generated Text",
        "authors": "Alexander NemecekYuzhou JiangErman Ayday",
        "links": "http://arxiv.org/abs/2404.02138v1",
        "entry_id": "http://arxiv.org/abs/2404.02138v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02138v1",
        "summary": "Recent advancements of large language models (LLMs) have resulted in\nindistinguishable text outputs comparable to human-generated text. Watermarking\nalgorithms are potential tools that offer a way to differentiate between LLM-\nand human-generated text by embedding detectable signatures within\nLLM-generated output. However, current watermarking schemes lack robustness\nagainst known attacks against watermarking algorithms. In addition, they are\nimpractical considering an LLM generates tens of thousands of text outputs per\nday and the watermarking algorithm needs to memorize each output it generates\nfor the detection to work. In this work, focusing on the limitations of current\nwatermarking schemes, we propose the concept of a \"topic-based watermarking\nalgorithm\" for LLMs. The proposed algorithm determines how to generate tokens\nfor the watermarked LLM output based on extracted topics of an input prompt or\nthe output of a non-watermarked LLM. Inspired from previous work, we propose\nusing a pair of lists (that are generated based on the specified extracted\ntopic(s)) that specify certain tokens to be included or excluded while\ngenerating the watermarked output of the LLM. Using the proposed watermarking\nalgorithm, we show the practicality of a watermark detection algorithm.\nFurthermore, we discuss a wide range of attacks that can emerge against\nwatermarking algorithms for LLMs and the benefit of the proposed watermarking\nscheme for the feasibility of modeling a potential attacker considering its\nbenefit vs. loss.",
        "updated": "2024-04-02 17:49:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02138v1"
    },
    {
        "title": "FLawN-T5: An Empirical Examination of Effective Instruction-Tuning Data Mixtures for Legal Reasoning",
        "authors": "Joel NiklausLucia ZhengArya D. McCarthyChristopher HahnBrian M. RosenPeter HendersonDaniel E. HoGarrett HonkePercy LiangChristopher Manning",
        "links": "http://arxiv.org/abs/2404.02127v1",
        "entry_id": "http://arxiv.org/abs/2404.02127v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02127v1",
        "summary": "Instruction tuning is an important step in making language models useful for\ndirect user interaction. However, many legal tasks remain out of reach for most\nopen LLMs and there do not yet exist any large scale instruction datasets for\nthe domain. This critically limits research in this application area. In this\nwork, we curate LawInstruct, a large legal instruction dataset, covering 17\njurisdictions, 24 languages and a total of 12M examples. We present evidence\nthat domain-specific pretraining and instruction tuning improve performance on\nLegalBench, including improving Flan-T5 XL by 8 points or 16\\% over the\nbaseline. However, the effect does not generalize across all tasks, training\nregimes, model sizes, and other factors. LawInstruct is a resource for\naccelerating the development of models with stronger information processing and\ndecision making capabilities in the legal domain.",
        "updated": "2024-04-02 17:33:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02127v1"
    },
    {
        "title": "GINopic: Topic Modeling with Graph Isomorphism Network",
        "authors": "Suman AdhyaDebarshi Kumar Sanyal",
        "links": "http://arxiv.org/abs/2404.02115v1",
        "entry_id": "http://arxiv.org/abs/2404.02115v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02115v1",
        "summary": "Topic modeling is a widely used approach for analyzing and exploring large\ndocument collections. Recent research efforts have incorporated pre-trained\ncontextualized language models, such as BERT embeddings, into topic modeling.\nHowever, they often neglect the intrinsic informational value conveyed by\nmutual dependencies between words. In this study, we introduce GINopic, a topic\nmodeling framework based on graph isomorphism networks to capture the\ncorrelation between words. By conducting intrinsic (quantitative as well as\nqualitative) and extrinsic evaluations on diverse benchmark datasets, we\ndemonstrate the effectiveness of GINopic compared to existing topic models and\nhighlight its potential for advancing topic modeling.",
        "updated": "2024-04-02 17:18:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02115v1"
    }
]