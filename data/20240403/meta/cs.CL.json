[
    {
        "title": "Topic-based Watermarks for LLM-Generated Text",
        "authors": "Alexander NemecekYuzhou JiangErman Ayday",
        "links": "http://arxiv.org/abs/2404.02138v1",
        "entry_id": "http://arxiv.org/abs/2404.02138v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02138v1",
        "summary": "Recent advancements of large language models (LLMs) have resulted in\nindistinguishable text outputs comparable to human-generated text. Watermarking\nalgorithms are potential tools that offer a way to differentiate between LLM-\nand human-generated text by embedding detectable signatures within\nLLM-generated output. However, current watermarking schemes lack robustness\nagainst known attacks against watermarking algorithms. In addition, they are\nimpractical considering an LLM generates tens of thousands of text outputs per\nday and the watermarking algorithm needs to memorize each output it generates\nfor the detection to work. In this work, focusing on the limitations of current\nwatermarking schemes, we propose the concept of a \"topic-based watermarking\nalgorithm\" for LLMs. The proposed algorithm determines how to generate tokens\nfor the watermarked LLM output based on extracted topics of an input prompt or\nthe output of a non-watermarked LLM. Inspired from previous work, we propose\nusing a pair of lists (that are generated based on the specified extracted\ntopic(s)) that specify certain tokens to be included or excluded while\ngenerating the watermarked output of the LLM. Using the proposed watermarking\nalgorithm, we show the practicality of a watermark detection algorithm.\nFurthermore, we discuss a wide range of attacks that can emerge against\nwatermarking algorithms for LLMs and the benefit of the proposed watermarking\nscheme for the feasibility of modeling a potential attacker considering its\nbenefit vs. loss.",
        "updated": "2024-04-02 17:49:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02138v1"
    },
    {
        "title": "FLawN-T5: An Empirical Examination of Effective Instruction-Tuning Data Mixtures for Legal Reasoning",
        "authors": "Joel NiklausLucia ZhengArya D. McCarthyChristopher HahnBrian M. RosenPeter HendersonDaniel E. HoGarrett HonkePercy LiangChristopher Manning",
        "links": "http://arxiv.org/abs/2404.02127v1",
        "entry_id": "http://arxiv.org/abs/2404.02127v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02127v1",
        "summary": "Instruction tuning is an important step in making language models useful for\ndirect user interaction. However, many legal tasks remain out of reach for most\nopen LLMs and there do not yet exist any large scale instruction datasets for\nthe domain. This critically limits research in this application area. In this\nwork, we curate LawInstruct, a large legal instruction dataset, covering 17\njurisdictions, 24 languages and a total of 12M examples. We present evidence\nthat domain-specific pretraining and instruction tuning improve performance on\nLegalBench, including improving Flan-T5 XL by 8 points or 16\\% over the\nbaseline. However, the effect does not generalize across all tasks, training\nregimes, model sizes, and other factors. LawInstruct is a resource for\naccelerating the development of models with stronger information processing and\ndecision making capabilities in the legal domain.",
        "updated": "2024-04-02 17:33:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02127v1"
    },
    {
        "title": "Rematch: Robust and Efficient Matching of Local Knowledge Graphs to Improve Structural and Semantic Similarity",
        "authors": "Zoher KachwalaJisun AnHaewoon KwakFilippo Menczer",
        "links": "http://arxiv.org/abs/2404.02126v1",
        "entry_id": "http://arxiv.org/abs/2404.02126v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02126v1",
        "summary": "Knowledge graphs play a pivotal role in various applications, such as\nquestion-answering and fact-checking. Abstract Meaning Representation (AMR)\nrepresents text as knowledge graphs. Evaluating the quality of these graphs\ninvolves matching them structurally to each other and semantically to the\nsource text. Existing AMR metrics are inefficient and struggle to capture\nsemantic similarity. We also lack a systematic evaluation benchmark for\nassessing structural similarity between AMR graphs. To overcome these\nlimitations, we introduce a novel AMR similarity metric, rematch, alongside a\nnew evaluation for structural similarity called RARE. Among state-of-the-art\nmetrics, rematch ranks second in structural similarity; and first in semantic\nsimilarity by 1--5 percentage points on the STS-B and SICK-R benchmarks.\nRematch is also five times faster than the next most efficient metric.",
        "updated": "2024-04-02 17:33:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02126v1"
    },
    {
        "title": "Exploring Automated Distractor Generation for Math Multiple-choice Questions via Large Language Models",
        "authors": "Wanyong FengJaewook LeeHunter McNicholsAlexander ScarlatosDigory SmithSimon WoodheadNancy Otero OrnelasAndrew Lan",
        "links": "http://arxiv.org/abs/2404.02124v1",
        "entry_id": "http://arxiv.org/abs/2404.02124v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02124v1",
        "summary": "Multiple-choice questions (MCQs) are ubiquitous in almost all levels of\neducation since they are easy to administer, grade, and are a reliable format\nin assessments and practices. One of the most important aspects of MCQs is the\ndistractors, i.e., incorrect options that are designed to target common errors\nor misconceptions among real students. To date, the task of crafting\nhigh-quality distractors largely remains a labor and time-intensive process for\nteachers and learning content designers, which has limited scalability. In this\nwork, we study the task of automated distractor generation in the domain of\nmath MCQs and explore a wide variety of large language model (LLM)-based\napproaches, from in-context learning to fine-tuning. We conduct extensive\nexperiments using a real-world math MCQ dataset and find that although LLMs can\ngenerate some mathematically valid distractors, they are less adept at\nanticipating common errors or misconceptions among real students.",
        "updated": "2024-04-02 17:31:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02124v1"
    },
    {
        "title": "GINopic: Topic Modeling with Graph Isomorphism Network",
        "authors": "Suman AdhyaDebarshi Kumar Sanyal",
        "links": "http://arxiv.org/abs/2404.02115v1",
        "entry_id": "http://arxiv.org/abs/2404.02115v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02115v1",
        "summary": "Topic modeling is a widely used approach for analyzing and exploring large\ndocument collections. Recent research efforts have incorporated pre-trained\ncontextualized language models, such as BERT embeddings, into topic modeling.\nHowever, they often neglect the intrinsic informational value conveyed by\nmutual dependencies between words. In this study, we introduce GINopic, a topic\nmodeling framework based on graph isomorphism networks to capture the\ncorrelation between words. By conducting intrinsic (quantitative as well as\nqualitative) and extrinsic evaluations on diverse benchmark datasets, we\ndemonstrate the effectiveness of GINopic compared to existing topic models and\nhighlight its potential for advancing topic modeling.",
        "updated": "2024-04-02 17:18:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02115v1"
    }
]