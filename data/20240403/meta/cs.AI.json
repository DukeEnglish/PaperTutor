[
    {
        "title": "Segment Any 3D Object with Language",
        "authors": "Seungjun LeeYuyang ZhaoGim Hee Lee",
        "links": "http://arxiv.org/abs/2404.02157v1",
        "entry_id": "http://arxiv.org/abs/2404.02157v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02157v1",
        "summary": "In this paper, we investigate Open-Vocabulary 3D Instance Segmentation\n(OV-3DIS) with free-form language instructions. Earlier works that rely on only\nannotated base categories for training suffer from limited generalization to\nunseen novel categories. Recent works mitigate poor generalizability to novel\ncategories by generating class-agnostic masks or projecting generalized masks\nfrom 2D to 3D, but disregard semantic or geometry information, leading to\nsub-optimal performance. Instead, generating generalizable but semantic-related\nmasks directly from 3D point clouds would result in superior outcomes. In this\npaper, we introduce Segment any 3D Object with LanguagE (SOLE), which is a\nsemantic and geometric-aware visual-language learning framework with strong\ngeneralizability by generating semantic-related masks directly from 3D point\nclouds. Specifically, we propose a multimodal fusion network to incorporate\nmultimodal semantics in both backbone and decoder. In addition, to align the 3D\nsegmentation model with various language instructions and enhance the mask\nquality, we introduce three types of multimodal associations as supervision.\nOur SOLE outperforms previous methods by a large margin on ScanNetv2,\nScanNet200, and Replica benchmarks, and the results are even close to the\nfully-supervised counterpart despite the absence of class annotations in the\ntraining. Furthermore, extensive qualitative results demonstrate the\nversatility of our SOLE to language instructions.",
        "updated": "2024-04-02 17:59:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02157v1"
    },
    {
        "title": "Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks",
        "authors": "Maksym AndriushchenkoFrancesco CroceNicolas Flammarion",
        "links": "http://arxiv.org/abs/2404.02151v1",
        "entry_id": "http://arxiv.org/abs/2404.02151v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02151v1",
        "summary": "We show that even the most recent safety-aligned LLMs are not robust to\nsimple adaptive jailbreaking attacks. First, we demonstrate how to successfully\nleverage access to logprobs for jailbreaking: we initially design an\nadversarial prompt template (sometimes adapted to the target LLM), and then we\napply random search on a suffix to maximize the target logprob (e.g., of the\ntoken \"Sure\"), potentially with multiple restarts. In this way, we achieve\nnearly 100\\% attack success rate -- according to GPT-4 as a judge -- on\nGPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was\nadversarially trained against the GCG attack. We also show how to jailbreak all\nClaude models -- that do not expose logprobs -- via either a transfer or\nprefilling attack with 100\\% success rate. In addition, we show how to use\nrandom search on a restricted set of tokens for finding trojan strings in\npoisoned models -- a task that shares many similarities with jailbreaking --\nwhich is the algorithm that brought us the first place in the SaTML'24 Trojan\nDetection Competition. The common theme behind these attacks is that adaptivity\nis crucial: different models are vulnerable to different prompting templates\n(e.g., R2D2 is very sensitive to in-context learning prompts), some models have\nunique vulnerabilities based on their APIs (e.g., prefilling for Claude), and\nin some settings it is crucial to restrict the token search space based on\nprior knowledge (e.g., for trojan detection). We provide the code, prompts, and\nlogs of the attacks at https://github.com/tml-epfl/llm-adaptive-attacks.",
        "updated": "2024-04-02 17:58:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02151v1"
    },
    {
        "title": "FLawN-T5: An Empirical Examination of Effective Instruction-Tuning Data Mixtures for Legal Reasoning",
        "authors": "Joel NiklausLucia ZhengArya D. McCarthyChristopher HahnBrian M. RosenPeter HendersonDaniel E. HoGarrett HonkePercy LiangChristopher Manning",
        "links": "http://arxiv.org/abs/2404.02127v1",
        "entry_id": "http://arxiv.org/abs/2404.02127v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02127v1",
        "summary": "Instruction tuning is an important step in making language models useful for\ndirect user interaction. However, many legal tasks remain out of reach for most\nopen LLMs and there do not yet exist any large scale instruction datasets for\nthe domain. This critically limits research in this application area. In this\nwork, we curate LawInstruct, a large legal instruction dataset, covering 17\njurisdictions, 24 languages and a total of 12M examples. We present evidence\nthat domain-specific pretraining and instruction tuning improve performance on\nLegalBench, including improving Flan-T5 XL by 8 points or 16\\% over the\nbaseline. However, the effect does not generalize across all tasks, training\nregimes, model sizes, and other factors. LawInstruct is a resource for\naccelerating the development of models with stronger information processing and\ndecision making capabilities in the legal domain.",
        "updated": "2024-04-02 17:33:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02127v1"
    },
    {
        "title": "Already Moderate Population Sizes Provably Yield Strong Robustness to Noise",
        "authors": "Denis AntipovBenjamin DoerrAlexandra Ivanova",
        "links": "http://arxiv.org/abs/2404.02090v1",
        "entry_id": "http://arxiv.org/abs/2404.02090v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02090v1",
        "summary": "Experience shows that typical evolutionary algorithms can cope well with\nstochastic disturbances such as noisy function evaluations.\n  In this first mathematical runtime analysis of the $(1+\\lambda)$ and\n$(1,\\lambda)$ evolutionary algorithms in the presence of prior bit-wise noise,\nwe show that both algorithms can tolerate constant noise probabilities without\nincreasing the asymptotic runtime on the OneMax benchmark. For this, a\npopulation size $\\lambda$ suffices that is at least logarithmic in the problem\nsize $n$. The only previous result in this direction regarded the less\nrealistic one-bit noise model, required a population size super-linear in the\nproblem size, and proved a runtime guarantee roughly cubic in the noiseless\nruntime for the OneMax benchmark. Our significantly stronger results are based\non the novel proof argument that the noiseless offspring can be seen as a\nbiased uniform crossover between the parent and the noisy offspring. We are\noptimistic that the technical lemmas resulting from this insight will find\napplications also in future mathematical runtime analyses of evolutionary\nalgorithms.",
        "updated": "2024-04-02 16:35:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02090v1"
    },
    {
        "title": "Advancing LLM Reasoning Generalists with Preference Trees",
        "authors": "Lifan YuanGanqu CuiHanbin WangNing DingXingyao WangJia DengBoji ShanHuimin ChenRuobing XieYankai LinZhenghao LiuBowen ZhouHao PengZhiyuan LiuMaosong Sun",
        "links": "http://arxiv.org/abs/2404.02078v1",
        "entry_id": "http://arxiv.org/abs/2404.02078v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02078v1",
        "summary": "We introduce Eurus, a suite of large language models (LLMs) optimized for\nreasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve\nstate-of-the-art results among open-source models on a diverse set of\nbenchmarks covering mathematics, code generation, and logical reasoning\nproblems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a\ncomprehensive benchmarking across 12 tests covering five tasks, and achieves a\n33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging\nbenchmarks, substantially outperforming existing open-source models by margins\nmore than 13.3%. The strong performance of Eurus can be primarily attributed to\nUltraInteract, our newly-curated large-scale, high-quality alignment dataset\nspecifically designed for complex reasoning tasks. UltraInteract can be used in\nboth supervised fine-tuning and preference learning. For each instruction, it\nincludes a preference tree consisting of (1) reasoning chains with diverse\nplanning strategies in a unified format, (2) multi-turn interaction\ntrajectories with the environment and the critique, and (3) pairwise data to\nfacilitate preference learning. UltraInteract allows us to conduct an in-depth\nexploration of preference learning for reasoning tasks. Our investigation\nreveals that some well-established preference learning algorithms may be less\nsuitable for reasoning tasks compared to their effectiveness in general\nconversations. Inspired by this, we derive a novel reward modeling objective\nwhich, together with UltraInteract, leads to a strong reward model.",
        "updated": "2024-04-02 16:25:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02078v1"
    }
]