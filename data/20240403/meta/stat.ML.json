[
    {
        "title": "Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks",
        "authors": "Maksym AndriushchenkoFrancesco CroceNicolas Flammarion",
        "links": "http://arxiv.org/abs/2404.02151v1",
        "entry_id": "http://arxiv.org/abs/2404.02151v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02151v1",
        "summary": "We show that even the most recent safety-aligned LLMs are not robust to\nsimple adaptive jailbreaking attacks. First, we demonstrate how to successfully\nleverage access to logprobs for jailbreaking: we initially design an\nadversarial prompt template (sometimes adapted to the target LLM), and then we\napply random search on a suffix to maximize the target logprob (e.g., of the\ntoken \"Sure\"), potentially with multiple restarts. In this way, we achieve\nnearly 100\\% attack success rate -- according to GPT-4 as a judge -- on\nGPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was\nadversarially trained against the GCG attack. We also show how to jailbreak all\nClaude models -- that do not expose logprobs -- via either a transfer or\nprefilling attack with 100\\% success rate. In addition, we show how to use\nrandom search on a restricted set of tokens for finding trojan strings in\npoisoned models -- a task that shares many similarities with jailbreaking --\nwhich is the algorithm that brought us the first place in the SaTML'24 Trojan\nDetection Competition. The common theme behind these attacks is that adaptivity\nis crucial: different models are vulnerable to different prompting templates\n(e.g., R2D2 is very sensitive to in-context learning prompts), some models have\nunique vulnerabilities based on their APIs (e.g., prefilling for Claude), and\nin some settings it is crucial to restrict the token search space based on\nprior knowledge (e.g., for trojan detection). We provide the code, prompts, and\nlogs of the attacks at https://github.com/tml-epfl/llm-adaptive-attacks.",
        "updated": "2024-04-02 17:58:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02151v1"
    },
    {
        "title": "Robustly estimating heterogeneity in factorial data using Rashomon Partitions",
        "authors": "Aparajithan VenkateswaranAnirudh SankarArun G. ChandrasekharTyler H. McCormick",
        "links": "http://arxiv.org/abs/2404.02141v1",
        "entry_id": "http://arxiv.org/abs/2404.02141v1",
        "pdf_url": "http://arxiv.org/pdf/2404.02141v1",
        "summary": "Many statistical analyses, in both observational data and randomized control\ntrials, ask: how does the outcome of interest vary with combinations of\nobservable covariates? How do various drug combinations affect health outcomes,\nor how does technology adoption depend on incentives and demographics? Our goal\nis to partition this factorial space into ``pools'' of covariate combinations\nwhere the outcome differs across the pools (but not within a pool). Existing\napproaches (i) search for a single ``optimal'' partition under assumptions\nabout the association between covariates or (ii) sample from the entire set of\npossible partitions. Both these approaches ignore the reality that, especially\nwith correlation structure in covariates, many ways to partition the covariate\nspace may be statistically indistinguishable, despite very different\nimplications for policy or science. We develop an alternative perspective,\ncalled Rashomon Partition Sets (RPSs). Each item in the RPS partitions the\nspace of covariates using a tree-like geometry. RPSs incorporate all partitions\nthat have posterior values near the maximum a posteriori partition, even if\nthey offer substantively different explanations, and do so using a prior that\nmakes no assumptions about associations between covariates. This prior is the\n$\\ell_0$ prior, which we show is minimax optimal. Given the RPS we calculate\nthe posterior of any measurable function of the feature effects vector on\noutcomes, conditional on being in the RPS. We also characterize approximation\nerror relative to the entire posterior and provide bounds on the size of the\nRPS. Simulations demonstrate this framework allows for robust conclusions\nrelative to conventional regularization techniques. We apply our method to\nthree empirical settings: price effects on charitable giving, chromosomal\nstructure (telomere length), and the introduction of microfinance.",
        "updated": "2024-04-02 17:53:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.02141v1"
    },
    {
        "title": "Adaptive Combinatorial Maximization: Beyond Approximate Greedy Policies",
        "authors": "Shlomi WeitzmanSivan Sabato",
        "links": "http://arxiv.org/abs/2404.01930v1",
        "entry_id": "http://arxiv.org/abs/2404.01930v1",
        "pdf_url": "http://arxiv.org/pdf/2404.01930v1",
        "summary": "We study adaptive combinatorial maximization, which is a core challenge in\nmachine learning, with applications in active learning as well as many other\ndomains. We study the Bayesian setting, and consider the objectives of\nmaximization under a cardinality constraint and minimum cost coverage. We\nprovide new comprehensive approximation guarantees that subsume previous\nresults, as well as considerably strengthen them. Our approximation guarantees\nsimultaneously support the maximal gain ratio as well as near-submodular\nutility functions, and include both maximization under a cardinality constraint\nand a minimum cost coverage guarantee. In addition, we provided an\napproximation guarantee for a modified prior, which is crucial for obtaining\nactive learning guarantees that do not depend on the smallest probability in\nthe prior. Moreover, we discover a new parameter of adaptive selection\npolicies, which we term the \"maximal gain ratio\". We show that this parameter\nis strictly less restrictive than the greedy approximation parameter that has\nbeen used in previous approximation guarantees, and show that it can be used to\nprovide stronger approximation guarantees than previous results. In particular,\nwe show that the maximal gain ratio is never larger than the greedy\napproximation factor of a policy, and that it can be considerably smaller. This\nprovides a new insight into the properties that make a policy useful for\nadaptive combinatorial maximization.",
        "updated": "2024-04-02 13:23:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.01930v1"
    },
    {
        "title": "Adversarial Combinatorial Bandits with Switching Costs",
        "authors": "Yanyan DongVincent Y. F. Tan",
        "links": "http://dx.doi.org/10.1109/TIT.2024.3384033",
        "entry_id": "http://arxiv.org/abs/2404.01883v1",
        "pdf_url": "http://arxiv.org/pdf/2404.01883v1",
        "summary": "We study the problem of adversarial combinatorial bandit with a switching\ncost $\\lambda$ for a switch of each selected arm in each round, considering\nboth the bandit feedback and semi-bandit feedback settings. In the oblivious\nadversarial case with $K$ base arms and time horizon $T$, we derive lower\nbounds for the minimax regret and design algorithms to approach them. To prove\nthese lower bounds, we design stochastic loss sequences for both feedback\nsettings, building on an idea from previous work in Dekel et al. (2014). The\nlower bound for bandit feedback is $ \\tilde{\\Omega}\\big( (\\lambda\nK)^{\\frac{1}{3}} (TI)^{\\frac{2}{3}}\\big)$ while that for semi-bandit feedback\nis $ \\tilde{\\Omega}\\big( (\\lambda K I)^{\\frac{1}{3}} T^{\\frac{2}{3}}\\big)$\nwhere $I$ is the number of base arms in the combinatorial arm played in each\nround. To approach these lower bounds, we design algorithms that operate in\nbatches by dividing the time horizon into batches to restrict the number of\nswitches between actions. For the bandit feedback setting, where only the total\nloss of the combinatorial arm is observed, we introduce the Batched-Exp2\nalgorithm which achieves a regret upper bound of $\\tilde{O}\\big((\\lambda\nK)^{\\frac{1}{3}}T^{\\frac{2}{3}}I^{\\frac{4}{3}}\\big)$ as $T$ tends to infinity.\nIn the semi-bandit feedback setting, where all losses for the combinatorial arm\nare observed, we propose the Batched-BROAD algorithm which achieves a regret\nupper bound of $\\tilde{O}\\big( (\\lambda K)^{\\frac{1}{3}}\n(TI)^{\\frac{2}{3}}\\big)$.",
        "updated": "2024-04-02 12:15:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.01883v1"
    },
    {
        "title": "Supervised Autoencoder MLP for Financial Time Series Forecasting",
        "authors": "Bartosz BieganowskiRobert Slepaczuk",
        "links": "http://arxiv.org/abs/2404.01866v1",
        "entry_id": "http://arxiv.org/abs/2404.01866v1",
        "pdf_url": "http://arxiv.org/pdf/2404.01866v1",
        "summary": "This paper investigates the enhancement of financial time series forecasting\nwith the use of neural networks through supervised autoencoders, aiming to\nimprove investment strategy performance. It specifically examines the impact of\nnoise augmentation and triple barrier labeling on risk-adjusted returns, using\nthe Sharpe and Information Ratios. The study focuses on the S&P 500 index,\nEUR/USD, and BTC/USD as the traded assets from January 1, 2010, to April 30,\n2022. Findings indicate that supervised autoencoders, with balanced noise\naugmentation and bottleneck size, significantly boost strategy effectiveness.\nHowever, excessive noise and large bottleneck sizes can impair performance,\nhighlighting the importance of precise parameter tuning. This paper also\npresents a derivation of a novel optimization metric that can be used with\ntriple barrier labeling. The results of this study have substantial policy\nimplications, suggesting that financial institutions and regulators could\nleverage techniques presented to enhance market stability and investor\nprotection, while also encouraging more informed and strategic investment\napproaches in various financial sectors.",
        "updated": "2024-04-02 11:44:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.01866v1"
    }
]