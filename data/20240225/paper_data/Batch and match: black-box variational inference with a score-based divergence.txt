BATCH AND MATCH: BLACK-BOX VARIATIONAL INFERENCE
WITH A SCORE-BASED DIVERGENCE
By Diana Cai∗ Chirag Modi∗
Loucas Pillaud-Vivien∗ Charles C. Margossian∗
Robert M. Gower∗ David M. Blei† Lawrence K. Saul∗
∗Flatiron Institute, †Columbia University
Most leading implementations of black-box variational inference
(BBVI) are based on optimizing a stochastic evidence lower bound
(ELBO). But such approaches to BBVI often converge slowly due to
thehighvarianceoftheirgradientestimates.Inthiswork,wepropose
batch and match (BaM), an alternative approach to BBVI based on a
score-based divergence. Notably, this score-based divergence can be
optimized by a closed-form proximal update for Gaussian variational
families with full covariance matrices. We analyze the convergence
of BaM when the target distribution is Gaussian, and we prove that
in the limit of infinite batch size the variational parameter updates
converge exponentially quickly to the target mean and covariance.
We also evaluate the performance of BaM on Gaussian and non-
Gaussian target distributions that arise from posterior inference in
hierarchicalanddeepgenerativemodels.Intheseexperiments,wefind
that BaM typically converges in fewer (and sometimes significantly
fewer) gradient evaluations than leading implementations of BBVI
based on ELBO maximization.
1. Introduction. Probabilistic modeling plays a fundamental role in many problems of
inference and decision-making, but it can be challenging to develop accurate probabilistic models
that remain computationally tractable. In typical applications, the goal is to estimate a target
distribution that cannot be evaluated or sampled from exactly, but where an unnormalized form
is available. A canonical situation is applied Bayesian statistics, where the target is a posterior
distribution of latent variables given observations, but where only the model’s joint distribution
is available in closed form. Variational inference (VI) has emerged as a leading method for fast
approximate inference (Blei et al., 2017; Jordan et al., 1999). The idea behind VI is to posit a
parameterized family of approximating distributions, and then to find the member of that family
which is closest to the target distribution.
Recently,VImethodshavebecomeincreasingly“blackbox,” inthattheyonlyrequirecalculation
of the log of the unnormalized target and (for some algorithms) its gradients (Kingma and
Welling, 2014; Ranganath et al., 2014). Further applications have built on advances in automatic
differentiation, and now black-box variational inference (BBVI) is widely deployed in robust
software packages for probabilistic programming (Bingham et al., 2019; Kucukelbir et al., 2017;
Salvatier et al., 2016).
In general, the ingredients of a BBVI strategy are the form of the approximating family, the
divergencetobeminimized,andtheoptimizationalgorithmtominimizeit.MostBBVIalgorithms
work with a factorized (or mean-field) family, and minimize the reverse Kullback-Leibler (KL)
divergence via stochastic gradient descent (SGD). But this approach has its drawbacks. The
Keywords and phrases: approximate inference, black-box variational inference, score matching, stochastic
proximal point algorithm, score-based divergence, quadratic matrix equations
1
4202
beF
22
]LM.tats[
1v85741.2042:viXra2 CAI, MODI, PILLAUD-VIVIEN, MARGOSSIAN, GOWER, BLEI, AND SAUL
optimizations can be plagued by high-variance gradients and sensitivity to hyperparameters
of the learning algorithms (Dhaka et al., 2020, 2021). These issues are further exacerbated in
high-dimensional problems and when using richer variational families that model the correlations
between different latent variables. There has been recent work on BBVI which avoids SGD for
Gaussian variational families (Modi et al., 2023), but this approach does not minimize an explicit
divergence and requires additional heuristics to converge for non-Gaussian targets.
In this paper, we develop a new approach to BBVI. It is based on a different divergence,
accommodates expressive variational families, and does not rely on SGD for optimization. In
particular, we introduce a novel score-based divergence that measures the agreement of the scores,
or gradients of the log densities, of the target and variational distributions. This divergence can
be estimated for unnormalized target distributions, thus making it a natural choice for BBVI. We
study the score-based divergence for Gaussian variational families with full covariance, rather
than the factorized family. We also develop an efficient stochastic proximal point algorithm, with
closed-form updates, to optimize this divergence.
Our algorithm is called batch and match (BaM), and it alternates between two types of steps.
In the “batch” step, we draw a batch of samples from the current approximation to the target and
use those samples to estimate the divergence; in the “match” step, we estimate a new variational
approximation by matching the scores at these samples. By iterating these steps, BaM finds a
variational distribution that is close in score-based divergence to the target.
Theoretically, we analyze the convergence of BaM when the target itself is Gaussian. In the
limit of an infinite batch size, we prove that the variational parameters converge exponentially fast
to the target mean and covariance. Empirically, we evaluate BaM on a variety of Gaussian and
non-Gaussian target distributions, including Bayesian hierarchical models and deep generative
models. On these same problems, we also compare BaM to a leading implementation of BBVI
based on ELBO maximization (Kucukelbir et al., 2017) and a recently proposed algorithm for
Gaussian score matching (Modi et al., 2023). By and large, we find that BaM converges faster
and to more accurate solutions.
In what follows, we begin by reviewing BBVI and then developing a score-based divergence
for BBVI with several important properties (Section 2). Next, we propose BaM, an iterative
algorithm for score-based Gaussian variational inference, and we study its rate of convergence
(Section 3). We then present a discussion of related methods in the literature (Section 4). Finally,
we conclude with a series of empirical studies on a variety of target distributions (Section 5).
2. BBVI with the score-based divergence. VI was developed as a way to estimate an
unknown target distribution with density p; here we assume that the target is a distribution
on RD. The target is estimated by first positing a variational family of distributions Q, then
finding the particular q ∈ Q that minimizes an objective L(q) measuring the difference between
p and q.
2.1. From VI to BBVI to score-based BBVI. In the classical formulation of VI, the objec-
tive L(q) is the (reverse) Kullback-Leibler (KL) divergence:
(cid:90) (cid:16) (cid:17)
KL(q;p) := log q(z) q(z)dz. (1)
p(z)
For some models the derivatives of KL(q;p) can be exactly evaluated, but for many others they
cannot. In this case a further approximation is needed. This more challenging situation is the
typical setting for BBVI.BATCH AND MATCH BLACK-BOX VARIATIONAL INFERENCE 3
In BBVI, it is assumed that (a) the target density p cannot be evaluated pointwise or sampled
from exactly, but that (b) an unnormalized target density is available. BBVI algorithms use
stochastic gradient descent to minimize the KL divergence, or equivalently, to maximize the
evidence lower bound (ELBO). The necessary gradients in this case can be estimated with access
to the unnormalized target density. But in practice this objective is difficult to optimize: the
optimization can converge slowly due to noisy gradients, and it can be sensitive to the choice of
learning rates.
In this work, we will also assume additionally that (c) the log target density is differentiable,
and its derivatives can be efficiently evaluated. We define the target density’s score function
s : RD → RD as
s(z) := ∇ logp(z).
z
It is often possible to compute these scores even when p is intractable because they only depend
on the logarithm of the unormalized target density. In what follows, we introduce the score-based
divergence and study its properties; in Section 3, we will then propose a BBVI algorithm based
on this score-based divergence.
Notation. For Σ ∈ RD×D, let Σ ≻ 0 denote that Σ is positive definite and Σ ⪰ 0 denote
that Σ is positive semi-definite. Define the set of symmetric, positive definite matrices as
SD := {Σ ∈ RD×D : Σ = Σ⊤,Σ ≻ 0}. Let tr(Σ) := (cid:80)D Σ denote the trace of Σ and let
++ d=1 dd
I ∈ RD×D denote the identity matrix. We primarily consider two norms throughout the paper:
√
first, given z ∈ RD and Σ ∈ RD×D, we define the Σ-weighted vector norm, ∥z∥ := z⊤Σz, and
Σ
second, given Σ ∈ RD×D, we define the matrix norm ∥Σ∥ to be the spectral norm.
2.2. The score-based divergence. We now introduce the score-based divergence, which will be
the basis for a BBVI objective. Here we focus on a Gaussian variational family, i.e.,
Q = {N(µ,Σ) : µ ∈ RD,Σ ∈ SD },
++
but we generalize the score-based divergence to non-Gaussian distributions in Appendix A.
The score-based divergence between densities q ∈ Q and p on RD is defined as
(cid:90) (cid:13) (cid:16) (cid:17)(cid:13)2
D(q;p) := (cid:13)∇ log q(z) (cid:13) q(z)dz, (2)
(cid:13) z p(z) (cid:13)
Cov(q)
where Cov(q) ∈ SD is the covariance matrix of the variational density q. Importantly, the
++
score-based divergence can be evaluated when p is only known up to a normalization constant, as
it only depends on the target density through the score ∇logp. Thus, not only can this divergence
be used as a VI objective, but it can also be used for goodness-of-fit evaluations, unlike the KL
divergence.
The divergence in eq. (2) is well-defined under mild conditions on p and q (see Appendix A),
and it enjoys two important properties:
Property 1 (Non-negativity & equality): D(q;p) ≥ 0 with D(q;p) = 0 iff p = q.
Property 2 (Affine invariance): Let h:RD →RD be an affine transformation, and consider
the induced densities q˜(h(z))=q(z)|J(z)|−1 and p˜(h(z))=p(z)|J(z)|−1, where J is the
determinant of the Jacobian of h. Then D(q;p) = D(q˜;p˜).
We note that these properties are also satisfied by the KL divergence (Qiao and Minematsu,
2010). The first property shows that D(q;p) is a proper divergence measuring the agreement
between p and q. The second property states that the score-based divergence D(q,p) is invariant4 CAI, MODI, PILLAUD-VIVIEN, MARGOSSIAN, GOWER, BLEI, AND SAUL
under affine transformations; this property is desirable to maintain a consistent measure of
similarity under coordinate transformations of the input. This property depends crucially on the
weighted vector norm, mediated by Cov(q), in the divergence of eq. (2).
There are several related divergences in the research literature. A generalization of the score-
baseddivergenceistheweightedFisherdivergence(Barpetal.,2019),givenbyE [∥∇log(q/p)∥2 ],
q M
whereM∈RD×D;thescore-baseddivergenceisrecoveredbythechoiceM=Cov(q).Aspecialcase
ofthescore-baseddivergenceistheFisherdivergence(Hyvärinen,2005)givenbyE [∥∇log(q/p)∥2],
q I
but this divergence is not affine invariant. (See the proof of Theorem A.4 for further discussion.)
3. Score-based Gaussian variational inference. The score-based divergence has many
favorable properties for VI. We now show that this divergence can also be efficiently optimized by
an iterative black-box algorithm.
3.1. Algorithm. Our goal is to find some Gaussian distribution q∗∈Q that minimizes D(q;p).
Without additional assumptions on the target p, the score-based divergence D(q;p) is not
analytically tractable. So instead we consider a Monte Carlo estimate of D(q;p): given samples
z ,...,z ∼ q, we construct the approximation
1 B
B
D(q;p) ≈
1 (cid:88)(cid:13)
(cid:13)∇
log(cid:16)
q(z
b)(cid:17)(cid:13) (cid:13)2
. (3)
B (cid:13) z p(z b) (cid:13) Cov(q)
b=1
This estimator is unbiased, but it does not lend itself to optimization: we cannot simultaneously
sample from q while also optimizing over the family Q to which it belongs.
To circumvent this difficulty, we take an iterative approach whose goal is to produce a sequence
of distributions {q }∞ that converges to q∗. At a high level, the approach alternates between two
t t=0
steps—one that constructs a biased estimate of D(q;p), and another that updates q based on this
biased estimate, but not too aggressively (so as to minimize the effect of the bias). Specifically, at
the tth iteration, we first estimate D(q;p) with samples from q : i.e., given z ,...,z ∼ q , we
t 1 B t
compute
B
D (cid:98)qt(q;p) :=
B1 (cid:88)(cid:13)
(cid:13) (cid:13)∇
zlog(cid:16)
pq( (z zb b)
)(cid:17)(cid:13)
(cid:13)
(cid:13)2
Cov(q). (4)
b=1
Wecalleq.(4)thebatch stepbecauseitestimatesD(q,p)fromthebatchofsamplesz ,...,z ∼ q .
1 B t
The batch step of the algorithm relies on stochastic sampling, but it alternates with a determin-
istic step that updates q by minimizing the empirical score-based divergence D (cid:98)qt(q;p) in eq. (4).
Importantly, this minimization is subject to a regularizer: we penalize large differences between q
t
and q
t+1
by their KL divergence. Intuitively, when q remains close to q t, then D (cid:98)qt(q;p) in eq. (4)
remains a good approximation to the unbiased estimate D (cid:98)q(q;p) in eq. (3). With this in mind,
we compute q by minimizing the regularized objective function
t+1
LBaM(q) := D (cid:98)qt(q;p)+ λ2
t
KL(q t;q), (5)
where q ∈ Q and λ >0 is the inverse regularization parameter. When λ is small, the regularizer
t t
is large, encouraging the next iterate q to remain close to q ; thus λ can also be viewed as a
t+1 t t
learning rate.
The objective function in eq. (5) has the important property that its global minimum can be
computed analytically in closed form. In particular, we can optimize eq. (5) without recourse
to gradient-based methods that are derived from a linearization around q . We refer to the
tBATCH AND MATCH BLACK-BOX VARIATIONAL INFERENCE 5
minimization of LBaM(q) in eq. (5) as the match step because the updated distribution q
t+1
always matches the scores at z ,...,z better than the current one q .
1 B t
Combining these two steps, we arrive at the batch and match (BaM) algorithm for BBVI with
a score-based divergence. The intuition behind this iterative approach will be formally justified in
Section 3.2 by a proof of convergence. We now discuss each step of the algorithm in greater detail.
Batch Step. This step begins by sampling z ,z ,...,z ∼ q and computing the scores
1 2 B t
g = ∇logp(z ) at each sample. It then calculates the means and covariances (over the batch) of
b b
these quantities; we denote these statistics by
B B
1 (cid:88) 1 (cid:88)
z = z , C = (z −z)(z −z)⊤ (6)
b b b
B B
b=1 b=1
B B
1 (cid:88) 1 (cid:88)
g = g , Γ = (g −g)(g −g)⊤, (7)
b b b
B B
b=1 b=1
where z,g ∈ RD are the means, respectively, of the samples and the scores, and C,Γ ∈ RD×D are
their covariances. In Appendix C, we show that the empirical score-based divergence D (cid:98)qt(q;p) in
eq. (4) can be written in terms of these statistics as
D (cid:98)qt(q;p) = tr(ΓΣ)+tr(CΣ−1)+(cid:13) (cid:13)µ−z−Σg(cid:13) (cid:13)2 Σ−1+const.,
where for clarity we have suppressed additive constants that do not depend on the mean µ or
covariance Σ of q. This calculation completes the batch step of BaM.
Match Step. The match step of BaM updates the variational approximation q by setting
q = argminLBaM(q), (8)
t+1
q∈Q
where LBaM(q) is given by eq. (5). This optimization can be solved in closed form; that is, we can
analytically calculate the variational mean µ and covariance Σ that minimize LBaM(q).
t+1 t+1
The details of this calculation are given in Appendix C. There we show that the updated
covariance Σ satisfies a quadratic matrix equation,
t+1
Σ UΣ +Σ = V, (9)
t+1 t+1 t+1
where the matrices U and V in this expression are positive semidefinite and determined by
statistics from the batch step of BaM. In particular, these matrices are given by
U = λ Γ+ λt gg⊤ (10)
t 1+λt
V = Σ +λ C + λt (µ −z)(µ −z)⊤. (11)
t t 1+λt t t
The quadratic matrix equation in eq. (9) has a symmetric and positive-definite solution (see
Appendix B), and it is given by
(cid:16) (cid:17)−1
Σ t+1 = 2V I +(I +4UV)1 2 . (12)
The solution in eq. (12) is the BaM update for the variational covariance. The update for the
variational mean is given by
µ = 1 µ + λt (Σ g+z). (13)
t+1 1+λt t 1+λt t+16 CAI, MODI, PILLAUD-VIVIEN, MARGOSSIAN, GOWER, BLEI, AND SAUL
Algorithm 1 Batch and match VI
1: Input: Iterations T, batch size B, inverse regularization λ >0, target score function s:RD →RD,
t
initial variational mean µ ∈RD and covariance Σ ∈SD
0 0 ++
2: for t=0,...,T−1 do
3: Sample batch z ∼N(µ ,Σ ) for b=1,...,B
b t t
4: Evaluate scores g =s(z ) for b=1,...,B
b b
5: Compute statistics z,g ∈RD and Γ,C ∈RD×D
B B
(cid:88) (cid:88)
z = 1 z , C = 1 (z −z)(z −z)⊤
B b B b b
b=1 b=1
B B
(cid:88) (cid:88)
g = 1 g , Γ = 1 (g −g)(g −g)⊤
B b B b b
b=1 b=1
6: Compute matrices U and V needed to solve the quadratic matrix equation ΣUΣ+Σ=V
U =λ tΓ+ 1+λt
λt
gg⊤
V =Σ t+λ tC+ 1+λt λt(µ t−z)(µ t−z)⊤
7: Update variational parameters
(cid:16) (cid:17)−1
Σ t+1 =2V I+(I+4UV)1 2
µ
t+1
= 1+1 λtµ t+ 1+λt
λt
(Σ t+1g+z)
8: end for
9: Output: variational parameters µ ,Σ
T T
Note that the update for µ depends on Σ , so these updates must be performed in the order
t+1 t+1
shown above. The updates in eq. (12–13) complete the match step of BaM.
More intuition for BaM can be obtained by examining certain limiting cases of the batch
size and learning rate. When λ →0, the updates have no effect, with Σ =Σ and µ =µ .
t t+1 t t+1 t
Alternatively, when B=1 and λ →∞, the BaM updates reduce to the recently proposed updates
t
for BBVI by (exact) Gaussian score matching (Modi et al., 2023); this equivalence is shown in
Appendix C. Finally, when B → ∞ and λ →∞ (in that order), BaM converges to a Gaussian
0
target distribution in one step; see Corollary D.5 of Appendix D.
We provide pseudocode for BaM in Algorithm 1. We note that it costs O(D3) to compute
the covariance update as shown in eq. (12), but for small batch sizes, when the matrix U is of
rank O(B) with B≪D, it is possible to compute the update in O(B2D+B3); this update is
presented in Lemma B.3 of Appendix B.
BaM incorporates many ideas from previous work. Like the stochastic proximal point (SPP)
method(AsiandDuchi,2019;DavisandDrusvyatskiy,2019),itminimizesaMonteCarloestimate
of a divergence subject to a regularization term. In proximal point methods, the updates are
always regularized by squared Euclidean distance, but the KL divergence has been used elsewhere
as a regularizer—for example, in the EM algorithm (Chrétien and Hero, 2000; Tseng, 2004) and
for approximate Bayesian inference (Dai et al., 2016; Khan et al., 2015, 2016; Theis and Hoffman,
2015). KL-based regularizers are also a hallmark of mirror descent methods (Nemirovskii and
Yudin, 1983), but in these methods the objective function is linearized—a poor approximation
for objective functions with high curvature. Notably, BaM does not introduce any linearizationsBATCH AND MATCH BLACK-BOX VARIATIONAL INFERENCE 7
because its optimizations in eq. (8) can be solved in closed form.
3.2. Proof of convergence for Gaussian targets. In this section we analyze a concrete setting
in which we can rigorously prove the convergence of the updates in Algorithm 1.
Suppose the target distribution is itself a Gaussian and the updates are computed in the
limit of infinite batch size (B→∞). In this setting we show that BaM converges to the target
distribution. More precisely, we show that the variational parameters converge exponentially
fast to their target values for all fixed levels of regularization λ>0 and no matter how they are
initialized. Our proof does not exclude the possibility of convergence in less restrictive settings,
and in Section 5, we observe empirically that the updates also converge for non-Gaussian targets
and finite batch sizes. Though the proof here does not cover such cases, it remains instructive in
many ways.
To proceed, consider a Gaussian target distribution p = N(µ ,Σ ). At the tth iteration of
∗ ∗
Algorithm 1, we measure the normalized errors in the mean and covariance parameters by
ε :=
Σ−1
2(µ −µ ), (14)
t ∗ t ∗
∆ :=
Σ−1
2(Σ −Σ
)Σ−1
2. (15)
t ∗ t ∗ ∗
The theorem below shows that ε ,∆ → 0 in spectral norm. Specifically, it shows that this
t t
convergence occurs exponentially fast at a rate controlled by the quality of initialization and
amount of regularization.
Theorem 3.1 (Exponential convergence). Suppose that p = N(µ ,Σ ) in Algorithm 1,
∗ ∗
and let α>0 denote the minimum eigenvalue of the matrix
Σ−1
2Σ
Σ−1
2. For any fixed level of
∗ 0 ∗
regularization λ>0, define
(cid:18) (cid:19)
1+λ λβ
β := min α, , δ := , (16)
1+λ+∥ε ∥2 1+λ
0
where β ∈ (0,1] measures the quality of initialization and δ ∈ (0,1) denotes a rate of decay. Then
with probability 1 in the limit of infinite batch size (B→∞), and for all t≥ 0, the normalized
errors in eqs. (14–15) satisfy
∥ε ∥ ≤ (1−δ)t∥ε ∥, (17)
t 0
∥∆ ∥ ≤ (1−δ)t∥∆ ∥ + t(1−δ)t−1∥ε ∥2. (18)
t 0 0
Before sketching the proof we make three remarks. First, these error bounds behave sensibly:
they suggest that the updates converge more slowly when the learning rate is small (with
λ≪1), when the variational mean is poorly initialized (with ∥ε ∥2≫1), and/or when the initial
0
estimate of the covariance is nearly singular (with α≪1). Second, the theorem holds under very
general conditions—not only for any initialization of µ and Σ ≻0, but also for any λ>0. This
0 0
robustness is typical of proximal algorithms, which are well-known for their stability with respect
to hyperparameters (Asi and Duchi, 2019), but it is uncharacteristic of many gradient-based
methods, which only converge when the learning rate varies inversely with the largest eigenvalue
of an underlying Hessian (Garrigos and Gower, 2023). Third, with more elaborate bookkeeping,
we can derive tighter bounds both for the above setting and also when different iterations use
varying levels of regularization {λ }∞ . We give a full proof with these extensions in Appendix D.
t t=08 CAI, MODI, PILLAUD-VIVIEN, MARGOSSIAN, GOWER, BLEI, AND SAUL
Proof Sketch. The crux of the proof is to bound the normalized errors in eqs. (14–15) from
one iteration to the next. Most importantly, we show that
∥ε ∥ ≤ (1−δ)∥ε ∥, (19)
t+1 t
∥∆ ∥ ≤ (1−δ)∥∆ ∥ + ∥ε ∥2, (20)
t+1 t t
where δ is given by eq. (16), and from these bounds, we use induction to prove the overall rates
of decay in eqs. (17-18). Here we briefly describe the steps that are needed to derive the bounds
in eqs. (19–20).
The first is to examine the statistics computed at each iteration of the algorithm in the infinite
batch limit (B→∞). This limit is simplifying because by the law of large numbers, we can
replace the batched averages over B samples at each iteration by their expected values under the
variational distribution q =N(µ ,Σ ). The second step of the proof is to analyze the algorithm’s
t t t
convergence in terms of the normalized mean ε in eq. (14) and the normalized covariance matrix
t
J =
Σ−1
2Σ
Σ−1
2 = I +∆ , (21)
t ∗ t ∗ t
where I denotes the identity matrix. In the infinite batch limit, we show that with probability 1
these quantities satisfy
(cid:16) (cid:17)
λJ J + 1 ε ε⊤ J +J = (1+λ)J , (22)
t+1 t 1+λ t t t+1 t+1 t
(cid:16) (cid:17)
ε = I − λ J ε . (23)
t+1 1+λ t+1 t
The third step of the proof is to sandwich the matrix J that appears in eq. (22) between two
t+1
other positive-definite matrices whose eigenvalues are more easily bounded. Specifically, at each
iteration t, we introduce matrices H and K defined by
t+1 t+1
(cid:16) (cid:17)
λH J +∥εt∥2 I H +H = (1+λ)J , (24)
t+1 t 1+λ t+1 t+1 t
λK J K +K = (1+λ)J . (25)
t+1 t t+1 t+1 t
It is easier to analyze the solutions to these equations because they replace the outer-product
ε ε⊤ in eq. (22) by a multiple of the identity matrix. We show that for all times t ≥ 0,
t t
H ⪯ J ⪯ K , (26)
t+1 t+1 t+1
so that we can prove ∥J −I∥→0 by showing ∥H −I∥→0 and ∥K −I∥→0. Finally, the last (and
t t t
most technical) step is to derive the bounds in eqs. (19–20) by combining the sandwich inequality
in eq. (26) with a detailed analysis of eqs. (22–25).
4. Related work. BaM builds on intuitions from earlier work on Gaussian score matching
(GSM) (Modi et al., 2023). GSM is an iterative algorithm for BBVI that updates a full-covariance
Gaussian by analytically solving a system of nonlinear equations. As previously discussed, BaM
recovers GSM as a special limiting case. One limitation of GSM is that it aims to match the scores
exactly; thus, if the target is not exactly Gaussian, the updates for GSM attempt to solve an
infeasible problem, In addition, the batch updates for GSM perform an ad hoc averaging that is
notguaranteedtomatchanyscoresexactly,evenwhenitispossibletodoso.BaMovercomesthese
limitations by optimizing a proper score-based divergence on each batch of samples. Empirically,BATCH AND MATCH BLACK-BOX VARIATIONAL INFERENCE 9
D=4 D=16 D=64 D=256
106 BaM-5 GSM BaM-15 BaM-40 BaM-150
BaM-2 ADVI BaM-2 BaM-2 BaM-2
102
10 2
101 103 105 101 103 105 101 103 105 101 103 105
# gradient evaluations # gradient evaluations # gradient evaluations # gradient evaluations
Fig 1: Gaussian targets of increasing dimension. Solid curves indicate the mean over 10 runs
(transparent curves). Both ADVI and GSM use a batch size of B=2. The batch size for BaM is
given in the legend.
with BaM, we observe that larger batch sizes lead to more stable convergence. The score-based
divergence behind BaM also lends itself to analysis, and we are able to provide theoretical
guarantees on the convergence of BaM for Gaussian targets.
Proximal point methods have been studied in several papers in the context of variational
inference; typically the objective is a stochastic estimate of the ELBO with a (forward) KL
regularization term. For example, Theis and Hoffman (2015) optimize this objective using
alternating coordinate ascent. In other work, Khan et al. (2015, 2016) propose a splitting method
for this objective, and by linearizing the difficult terms, they obtain a closed-form solution when
the variational family is Gaussian and additional knowledge is given about the structure of the
target. By contrast, BaM does not resort to linearization in order to obtain an analytical solution,
nor does it require additional assumptions on the structure of the target.
Several works consider score matching with a Fisher divergence in the context of VI. For
instance, Yu and Zhang (2023) propose a score-matching approach for semi-implicit variational
families based on stochastic gradient optimization of the Fisher divergence. Zhang et al. (2018)
use the Fisher divergence with an energy-based model as the variational family. BaM differs
from these approaches by working with a Gaussian variational family and an affine-invariant
score-based divergence.
Finally, we note that the idea of score matching (Hyvärinen, 2005) with a (weighted) Fisher
divergence appears in many contexts beyond VI (Barp et al., 2019; Song and Ermon, 2019).
One such context is generative modeling: here, given a set of training examples, the goal is to
approximate an unknown data distribution p by a parameterized model p with an intractable
θ
normalization constant. Note that in this setting one can evaluate ∇logp but not ∇logp. This
θ
setting is quite different from the setting of VI in this paper where we do not have samples from p,
where we can evaluate ∇logp, and where the approximating distribution q has the much simpler
and more tractable form of a multivariate Gaussian.
5. Experiments. We evaluate BaM against two other BBVI methods for Gaussian varia-
tional families with full covariance matrices. The first of these is automatic differentiation VI
(ADVI) (Kucukelbir et al., 2017), which is based on ELBO maximization, and the second is
GSM (Modi et al., 2023), as described in the previous section. We implement all algorithms using
JAX (Bradbury et al., 2018),1 which supports efficient automatic differentiation both on CPU and
GPU. We provide pseudocode for these methods in Appendix E.1.
5.1. Synthetically-constructed target distributions. We first validate BaM in two settings where
we know the true target distribution p. In the first setting, we construct Gaussian targets with
1Code for the implementations is available online at: https://github.com/modichirag/GSM-VI/tree/dev.
LK
drawroF10 CAI, MODI, PILLAUD-VIVIEN, MARGOSSIAN, GOWER, BLEI, AND SAUL
s=0.2, =1.0 s=1.0, =1.0 s=1.8, =1.0 s=0.0, =0.1 s=0.0, =0.9 s=0.0, =1.7
103
BaM-10 GSM 103 BaM-5 ADVI 102
101
101
100
101 101
102 104 102 104 102 104 102 104 102 104 102 104
# gradient evaluations # gradient evaluations # gradient evaluations # gradient evaluations # gradient evaluations # gradient evaluations
s=0.2, =1.0 s=1.0, =1.0 s=1.8, =1.0 s=0.0, =0.1 s=0.0, =0.9 s=0.0, =1.7
102
BaM-10 GSM 101
BaM-5 ADVI
101 100
101
100
102
102 104 102 104 102 104 102 104 102 104 102 104
# gradient evaluations # gradient evaluations # gradient evaluations # gradient evaluations # gradient evaluations # gradient evaluations
Fig 2: Non-Gaussian targets constructed using the sinh-arcsinh distribution, varying the skew s
and the tail weight t. ADVI and GSM use a batch size of B=5.
increasing number of dimensions. In the second setting, we study BaM for distributions with
increasing (but controlled) amounts of non-Gaussianity. As evaluation metrics, we use empirical
estimates of the KL divergence in both the forward direction, KL(p;q), and the reverse direction,
KL(q;p).
5.1.1. Gaussian targets with increasing dimensions. We construct Gaussian targets of increas-
ing dimension with D=4,16,64,256. In Figure 1, we compare BaM, ADVI, and GSM on each of
these target distributions, plotting the forward KL divergence against the number of gradient
evaluations. Results for the reverse KL divergence and other parameter settings are provided in
Appendix E.2. In all of these experiments, we use a constant learning rate λ = BD for BaM. Here
t
we find that BaM converges orders of magnitude faster than ADVI. While GSM is competitive
with BAM in some experiments, BaM converges more quickly with increasing batch size; this is
unlike GSM which was observed to have marginal gains beyond B=2 for Gaussian targets (Modi
et al., 2023).
5.1.2. Non-Gaussian targets with varying skew and tails. The sinh-arcsinh normal distribution
transforms a Gaussian random variable via the hyperbolic sine function and its inverse (Jones and
Pewsey, 2009, 2019). If y ∼ N(µ,Σ), then a sample from the sinh-arcsinh normal distribution is
given by
z =
sinh(cid:0)1 (cid:0) sinh−1(y)+s(cid:1)(cid:1)
,
τ
where the parameters s ∈ R and τ>0 control, respectively, the skew and the heaviness of the
tails. The Gaussian distribution is recovered when s=0 and τ=1.
We construct different non-Gaussian target distributions by varying these parameters. The
results are presented in Figure 2. Here we use a decaying learning rate λ = BD/(t+1) for BaM,
t
as some decay is necessary for BaM to converge when the target distribution is non-Gaussian.
First, we construct target distributions with normal tails (t=1) but varying skew (s=0.2,1.0,1.8).
Here we observe that BaM converges faster than ADVI. For large skew (s = 1.0,1.8), BaM
converges to a higher value of the forward KL divergence but to similar values of the reverse KL
divergence. In these experiments, we see that GSM and ADVI often have similar performance
but that BaM stabilizes more quickly with larger batch sizes. Notably, the reverse KL divergence
for GSM diverges when the target distribution is highly skewed (s=1.8).
LK
drawroF
LK
esreveR
LK
drawroF
LK
esreveRBATCH AND MATCH BLACK-BOX VARIATIONAL INFERENCE 11
Nearly Gaussian Gaussian Process Hierarchical
101
101
100
100
BaM
100 GSM
10 1
ADVI
100 101 102 103 104 105 100 101 102 103 104 105 100 101 102 103 104 105
# of gradient evaluations # of gradient evaluations # of gradient evaluations
Fig 3: Posterior inference in Bayesian models. The curves denote the mean over 5 runs, and
shaded regions denote their standard error. Solid curves (B=32) correspond to larger batch sizes
than dashed curves (B=8).
Next we construct target distributions with no skew (s=0) but tails of varying heaviness
(t=0.1,0.9,1.7). Here we find that all methods tend to converge to similar values of the reverse
KL divergence. In some cases, BaM and ADVI converge to better values than GSM, and BaM
typically converges in fewer gradient evaluations than ADVI.
5.2. Application: hierarchical Bayesian models. We now consider the application of BaM to
posterior inference. Suppose we have observations {x }N , and the target distribution is the
n n=1
posterior density
p(cid:0) z|{x }N (cid:1) ∝ p(z)p(cid:0) {x }N |z(cid:1) , (27)
n n=1 n n=1
with prior p(z) and likelihood p({x }N |z). We examine three target distributions from
n n=1
posteriordb (Magnusson et al., 2022), a database of Stan (Carpenter et al., 2017; Roualdes
et al., 2023) models with reference samples generated using Hamiltonian Monte Carlo (HMC).
The first target is nearly Gaussian (arK, D=7). The other two targets are non-Gaussian: one is a
Gaussian process (GP) Poisson regression model (gp-pois-regr, D=13), and the other is the
8-schools hierarchical Bayesian model (eight-schools-centered, D=10).
In these experiments, we evaluate BaM, ADVI, and GSM by computing the relative errors (We-
landawe et al., 2022) in the posterior mean and standard deviation (SD) estimated from the HMC
reference samples; we define these quantities and present additional results in Appendix E.4. In
all experiments, we use a decaying learning rate λ = BD/(t+1) for BaM.
t
Figure 3 compares the relative mean errors of BaM, ADVI, and GSM for batch sizes B=8
and B=32. We observe that BaM outperforms ADVI. For smaller batch sizes GSM can converge
faster than BaM, but it oscillates around the solution. BaM performs better with increasing batch
size, converging more quickly and to a more stable result, while GSM and ADVI do not benefit
from increasing batch size. In the appendix, we report the relative SD error and find similar
results except that in the hierarchical example, BaM converges to a larger relative SD error.
5.3. Application: deep generative model. In a deep generative model, the likelihood is parame-
terized by the output of a neural network Ω. For example, we may take
z ∼ N(0,I) (28)
n
x |z ∼ N(Ω(z ,θˆ),σ2I), (29)
n n n
where x corresponds to a high-dimensional object, such as an image, and z is a low-dimensional
n n
representation of x . The neural network Ω is parametrized by θˆand maps z to the mean of
n n
rorre
naem
.leR12 CAI, MODI, PILLAUD-VIVIEN, MARGOSSIAN, GOWER, BLEI, AND SAUL
ADVI
GSM
BaM
1 2 5 10 100 300 500 1000
(a) Image reconstruction across iterations with batch size, B =300.
B=10 B=100 B=300
GSM
0.2 ADVI
BaM
Amortized VI
0.1
0.0
0 500 1000 0 500 1000 0 500 1000
Iterations Iterations Iterations
(b) Reconstruction error for varying batch sizes.
Fig 4: Image reconstruction and error when the posterior mean of z′ is fed into the generative
neural network. The beige and purple stars highlight the best outcome for ADVI and BaM,
respectively, after 3,000 gradient evaluations.
the likelihood p(x |z ). For this example, we set σ2 = 0.1. The above joint distribution underlies
n n
many deep learning models (Tomczak, 2022), including the variational autoencoder (Kingma and
Welling, 2014; Rezende et al., 2014). We train the neural network on the CIFAR-10 image data
set (Krizhevsky, 2009). We model the images as continuous, with x ∈ R3072, and learn a latent
n
representation z ∈ R256; see Appendix E.5 for details.
n
Given a new observation x′, we wish to approximate the posterior p(z′|x′). As an evaluation
metric, we examine how well x′ is reconstructed by feeding the posterior expectation E[z′|x′]
into the neural network Ω(·,θˆ). The quality of the reconstruction is assessed visually and using
the mean squared error (MSE, Figure 4). For ADVI and BaM, we use a pilot run of T =100
iterations to find a suitable learning rate; we then run the algorithms for T =1000 iterations.
(GSM does not require this tuning step.) BaM performs poorly when the batch size is very small
(B=10) relative to the dimension of the latent variable z′, but it becomes competitive as the
batch size is increased. When the batch size is comparable to the dimension of z (i.e. B=300),
n
BaM converges an order of magnitude (or more) faster than ADVI and GSM.
To refine our comparison, suppose we have a computational budget of 3000 gradient evaluations.
Under this budget, ADVI achieves its lowest MSE for B=10 and T=300, while BaM produces a
comparable result for B=300 and T=10. Hence, the gradient evaluations for BaM can be largely
parallelized. By contrast, most gradients for ADVI must be evaluated sequentially.
Depending on how the parameter θˆof the neural network is estimated, it possible to learn an
encoder andperform amortizedvariationalinference (AVI) ona new observation x′. Whensuch an
rorrE
derauqS
naeMBATCH AND MATCH BLACK-BOX VARIATIONAL INFERENCE 13
encoder is available, estimations of p(z′|x′) can be obtained essentially for free. In our experiment,
both BaM and ADVI eventually achieve a lower reconstruction error than AVI. This is because
AVI uses a factorized Gaussian approximation, whereas BaM and ADVI use a full-covariance
approximation, and the latter provides better compression of x′ even though the dimension of z′
and the weights of the neural network remain unchanged.
6. Discussion and future work. In this paper, we introduce a score-based divergence that
is especially well-suited to BBVI with Gaussian variational families. We show that the score-based
divergence has a number of desirable properties. We then propose a regularized optimization
based on this divergence, and we show that it admits a closed-form solution, leading to a fast
iterative algorithm for score-based BBVI. We analyze the convergence of score-based BBVI when
the target is Gaussian, and in the limit of an infinite batch size, we show that the updates
converge exponentially quickly to the target mean and covariance. Finally, we demonstrate the
effectiveness of BaM in a number of empirical studies involving both Gaussian and non-Gaussian
targets; here we observe that for sufficiently large batch sizes, our method converges much faster
than other BBVI algorithms.
There are a number of fruitful directions for future work. First, it remains to analyze the
convergence of BaM in the finite-batch case and for a larger class of target distributions. Second,
it seems promising to develop score-based BBVI for other (non-Gaussian) variational families, and
more generally, to study what divergences lend themselves to stochastic proximal point algorithms.
Finally, we note that the score-based divergence, which can be computed for unnormalized models,
hasusefulapplicationsbeyondVI(Hyvärinen,2005);forinstance,thepropertyofaffineinvariance
makes it attractive as a goodness-of-fit diagnostic for general methods of approximate inference.
Further study remains to characterize the relationship of the score-based divergence to other such
diagnostics (Barp et al., 2019; Gorham and Mackey, 2015; Liu et al., 2016; Welandawe et al.,
2022).
References.
H.AsiandJ.C.Duchi. Stochastic(approximate)proximalpointmethods:convergence,optimality,andadaptivity.
SIAM Journal on Optimization, 29(3):2257–2290, 2019.
A. Barp, F.-X. Briol, A. Duncan, M. Girolami, and L. Mackey. Minimum Stein discrepancy estimators. Advances
in Neural Information Processing Systems, 32, 2019.
E.Bingham,J.P.Chen,M.Jankowiak,F.Obermeyer,N.Pradhan,T.Karaletsos,R.Singh,P.Szerlip,P.Horsfall,
and N. D. Goodman. Pyro: Deep universal probabilistic programming. The Journal of Machine Learning
Research, 20(1):973–978, 2019.
D. M. Blei, A. Kucukelbir, and J. D. McAuliffe. Variational inference: A review for statisticians. Journal of the
American Statistical Association, 112(518):859–877, 2017.
J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas,
S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy programs, 2018.
B. Carpenter, A. Gelman, M. D. Hoffman, D. Lee, B. Goodrich, M. Betancourt, M. Brubaker, J. Guo, P. Li, and
A. Riddell. Stan: A probabilistic programming language. Journal of Statistical Software, 76(1):1–32, 2017.
S.ChrétienandA.O.Hero. Kullbackproximalalgorithmsformaximum-likelihoodestimation. IEEE Transactions
on Information Theory, 46(5):1800–1810, 2000.
B.Dai,N.He,H.Dai,andL.Song. ProvableBayesianinferenceviaparticlemirrordescent. InArtificialIntelligence
and Statistics, pages 985–994. PMLR, 2016.
D. Davis and D. Drusvyatskiy. Stochastic model-based minimization of weakly convex functions. SIAM J. Optim.,
29(1):207–239, 2019.
A.K.Dhaka,A.Catalina,M.R.Andersen,M.Magnusson,J.Huggins,andA.Vehtari. Robust,accuratestochastic
optimization for variational inference. Advances in Neural Information Processing Systems, 33:10961–10973,
2020.
A.K.Dhaka,A.Catalina,M.Welandawe,M.R.Andersen,J.Huggins,andA.Vehtari.Challengesandopportunities14 CAI, MODI, PILLAUD-VIVIEN, MARGOSSIAN, GOWER, BLEI, AND SAUL
in high dimensional variational inference. Advances in Neural Information Processing Systems, 34:7787–7798,
2021.
G. Garrigos and R. M. Gower. Handbook of convergence theorems for (stochastic) gradient methods, 2023.
J. Gorham and L. Mackey. Measuring sample quality with Stein’s method. Advances in Neural Information
Processing Systems, 28, 2015.
A. Hyvärinen. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning
Research, 6(4), 2005.
C. Jones and A. Pewsey. Sinh-arcsinh distributions. Biometrika, 96(4):761–780, 2009.
C. Jones and A. Pewsey. The sinh-arcsinh normal distribution. Significance, 16(2):6–7, 2019.
M.I.Jordan,Z.Ghahramani,T.S.Jaakkola,andL.K.Saul. Anintroductiontovariationalmethodsforgraphical
models. Machine Learning, 37:183–233, 1999.
M. E. Khan, P. Baqué, F. Fleuret, and P. Fua. Kullback-Leibler proximal variational inference. In Advances in
Neural Information Processing Systems, 2015.
M. E. Khan, R. Babanezhad, W. Lin, M. Schmidt, and M. Sugiyama. Faster stochastic variational inference
using proximal-gradient methods with general divergence functions. In Conference on Uncertainty in Artificial
Intelligence, 2016.
D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In International Conference on Learning
Representations, 2014.
A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto,
2009.
A.Kucukelbir,D.Tran,R.Ranganath,A.Gelman,andD.M.Blei. Automaticdifferentiationvariationalinference.
Journal of Machine Learning Research, 2017.
V. Kučera. On nonnegative definite solutions to matrix quadratic equations. Automatica, 8(4):413–423, 1972a.
V.Kučera. Acontributiontomatrixquadraticequations. IEEETransactionsonAutomaticControl,17(3):344–347,
1972b.
Q.Liu,J.Lee,andM.Jordan. AkernelizedSteindiscrepancyforgoodness-of-fittests. InInternational Conference
on Machine Learning, pages 276–284. PMLR, 2016.
M.Magnusson,P.Bürkner,andA.Vehtari. posteriordb:asetofposteriorsforBayesianinferenceandprobabilistic
programming. https://github.com/stan-dev/posteriordb, 2022.
C. Modi, C. Margossian, Y. Yao, R. Gower, D. Blei, and L. Saul. Variational inference with Gaussian score
matching. In Advances in Neural Information Processing Systems, 2023.
A. Nemirovskiiand D. B. Yudin. Problem complexity and method efficiency in optimization. John Wiley andSons,
1983.
J. E. Potter. Matrix quadratic solutions. SIAM Journal of Applied Mathematics, 14(3):496–501, 1966.
Y.QiaoandN.Minematsu. Astudyoninvarianceoff-divergenceanditsapplicationtospeechrecognition. IEEE
Transactions on Signal Processing, 58(7):3884–3890, 2010.
R. Ranganath, S. Gerrish, and D. Blei. Black box variational inference. In Artificial Intelligence and Statistics,
pages 814–822. PMLR, 2014.
D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep
generative models. In International Conference on Machine Learning, pages 1278–1286. PMLR, 2014.
E. Roualdes, B. Ward, S. Axen, and B. Carpenter. BridgeStan: Efficient in-memory access to Stan programs
through Python, Julia, and R. https://github.com/roualdes/bridgestan, 2023.
J. Salvatier, T. V. Wiecki, and C. Fonnesbeck. Probabilistic programming in Python using PyMC3. PeerJ
Computer Science, 2:e55, 2016.
G. Shurbet, T. Lewis, and T. Boullion. Quadratic matrix equations. The Ohio Journal of Science, 74(5), 1974.
Y. Song and S. Ermon. Generative modeling by estimating gradients of the data distribution. Advances in Neural
Information Processing Systems, 32, 2019.
L.TheisandM.Hoffman. Atrust-regionmethodforstochasticvariationalinferencewithapplicationstostreaming
data. In International Conference on Machine Learning, pages 2503–2511. PMLR, 2015.
J. M. Tomczak. Deep Generative Modeling. Springer, 2022.
P. Tseng. An analysis of the EM algorithm and entropy-like proximal point methods. Mathematics of Operations
Research, 29(1):27–44, 2004.
M. Welandawe, M. R. Andersen, A. Vehtari, and J. H. Huggins. Robust, automated, and accurate black-box
variational inference. arXiv preprint arXiv:2203.15945, 2022.
L. Yu and C. Zhang. Semi-implicit variational inference via score matching. In The Eleventh International
Conference on Learning Representations, 2023.
Y.Yuan,L.Liu,H.Zhang,andH.Liu. ThesolutionstothequadraticmatrixequationX*AX+B*X+D=0. AppliedBATCH AND MATCH BLACK-BOX VARIATIONAL INFERENCE 15
Mathematics and Computation, 410:126463, 2021.
C.Zhang,B.Shahbaba,andH.Zhao. VariationalHamiltonianMonteCarloviascorematching. BayesianAnalysis,
13(2):485, 2018.
APPENDIX A: SCORE-BASED DIVERGENCE
InSection2weintroducedascore-baseddivergencebetweentwodistributions,pandq,overRD,
and specifically we considered the case where q was Gaussian. In this section, we define this
score-based divergence more generally. In particular, here we assume only that these distributions
satisfy the following properties:
(i) p(z)>0 and q(z)>0 for all z ∈ RD.
(ii) ∇p and ∇q exist and are continuous everywhere in RD.
(iii) E (cid:2) ∥∇logq∥2(cid:3) < ∞.
q
There may be weaker properties than these that also yield the following results (or various
generalizations thereof), but the above will suffice for our purposes.
This appendix is organized as follows. We begin with a lemma that is needed to define a
score-based divergence for distributions (not necessarily Gaussian) satisfying the above properties.
We then show that this score-based divergence has several appealing properties in its own right:
it is nonnegative and invariant under affine reparameterizations, it takes a simple and intuitive
form for distributions that are related by annealing or exponential tilting, and it reduces to the
KL divergence in certain special cases.
Lemma A.1. The matrix defined by Γ = E (cid:2) (∇logq)(∇logq)⊤(cid:3) exists in RD×D and is
q q
positive definite.
Proof. Let u be any unit vector in RD. We shall prove the theorem by showing that
0<u⊤Γ u<∞, or equivalently that all of the eigenvalues of Γ are finite and positive. The
q q
boundedness follows easily from property (iii) since
u⊤Γ u = E (cid:2) (∇logq·u)2(cid:3) ≤ E (cid:2) ∥∇logq∥2(cid:3) < ∞. (30)
q q q
To show positivity, we appeal to property (ii) that q is differentiable; hence for all t > 0 we can
write
(cid:90) t (cid:90) t
q(tu) = q(0)+ dτ u⊤∇q(τu) = q(0)+ dτ q(τu)∇logq(τu)·u. (31)
0 0
To proceed, we take the limit t → ∞ on both sides of this equation, and we appeal to property (i)
that q(0)>0. Moreover, since lim q(tu)=0 for all normalizable distributions q, we see that
t→∞
(cid:90) ∞
dτ q(τu)∇logq(τu)·u < 0. (32)
0
For this inequality to be satisfied, there must exist some t ≥ 0 such that ∇logq(t u)·u < 0.
0 0
Let z = t u, and let δ = −∇logq(z )·u. Since q and ∇q are continuous by properties (iii-iv),
0 0 0
there must exist some finite ball B around z such that ∇logq(z)·u < −δ for all z ∈ B. Let
0 2
q = min q(z), and note that q > 0 since it is the minimum of a positive-valued function on
B z∈B B
a compact set. It follows that
u⊤Γ u = E (cid:2) (∇logq·u)2(cid:3) > q ·vol(B)·(cid:0)δ(cid:1)2 > 0, (33)
q q B 2
where the inequality is obtained by considering only those contributions to the expected value
from within the volume of the ball B around z . This proves the lemma.
016 CAI, MODI, PILLAUD-VIVIEN, MARGOSSIAN, GOWER, BLEI, AND SAUL
The lemma is needed for the following definition of the score-based divergence. Notably, the
definition assumes that the matrix E (cid:2) (∇logq)(∇logq)⊤(cid:3) is invertible.
q
Definition A.2 (Score-based divergence). Let p and q satisfy the properties listed above,
and let Γ be defined as in Lemma A.1. Then we define the score-based divergence between q
q
and p as
(cid:20) (cid:21)
(cid:16) (cid:17)⊤ (cid:16) (cid:17)
D(q;p) = E ∇log q Γ−1 ∇log q . (34)
q p q p
Let us quickly verify that this definition reduces to the previous one in Section 2 where q is
assumed to be Gaussian. In particular, suppose that q = N(ν,Ψ). In this case
Γ = E [(∇logq)(∇logq)⊤] = E [Ψ−1(z−ν)(z−ν)⊤Ψ−1] = Ψ−1ΨΨ−1 = Ψ−1 = [Cov(q)]−1. (35)
q q q
Substituting this result into eq. (34), we recover the more specialized definition of the score-based
divergence in Section 2.
We now return to the more general definition in eq. (34). Next we show this score-based
divergence shares many desirable properties with the Kullback-Leibler divergence; indeed, in
certain special cases of interest, these two divergences, D(q;p) and KL(q;p), are equivalent. These
properties are demonstrated in the following theorems.
Theorem A.3 (Nonnegativity). D(q;p) ≥ 0 with equality if and only if p(z) = q(z) for all
z ∈ RD.
Proof. Nonnegativity follows from the previous lemma, and it is clear that the divergence
vanishes if p = q. To prove the converse, we note that for any z ∈ RD, we can write
p(z) p(0) (cid:90) 1 (cid:20) p(tz)(cid:21)
log = log + dt ∇log ·z. (36)
q(z) q(0) q(tz)
0
Now suppose that D(q;p) = 0. Then it must be the case that ∇logp = ∇logq everywhere in RD.
(If it were the case that ∇logp(z ) ̸= ∇logq(z ) for some z ∈ RD, then by continuity, there
0 0 0
would also exist some ball around z where these gradients were not equal; furthermore, in this
0
case, the value inside the expectation of eq. (34) would be positive everywhere inside this ball,
yielding a positive value for the divergence.) Since the gradients of logp and logq are everywhere
equal, it follows from eq. (36) that
p(z) p(0)
log = log , (37)
q(z) q(0)
or equivalently, that p(z) and q(z) have some constant ratio independent of z. But this constant
ratio must be equal to one because both distributions yield the same value when they are
integrated over RD.
Theorem A.4 (Affine invariance). Let f : RD →RD be an affine transformation, and
consider the induced densities q˜(f(z))=q(z)|J(z)|−1 and p˜(f(z))=p(z)|J(z)|−1, where J(z)
is the determinant of the Jacobian of f. Then D(q;p) = D(q˜;p˜).BATCH AND MATCH BLACK-BOX VARIATIONAL INFERENCE 17
Proof. Denote the affine transformation by z˜= Az+b where A ∈ RD×D and b ∈ RD. Then
we have
∇ z(cid:2) logp(z)(cid:3) = ∇
z(cid:20) log(cid:18) p˜(z˜)(cid:12)
(cid:12)
(cid:12)dz˜(cid:12)
(cid:12)
(cid:12)(cid:19)(cid:21)
= ∇ z[log(p˜(z˜)|A|)] =
(cid:18) dz˜(cid:19)⊤
∇ z˜[logp˜(z˜)] = A⊤∇ z˜(cid:2) logp˜(z˜)(cid:3) ,
(cid:12)dz(cid:12) dz
(38)
and a similar relation holds for ∇ logq(z). It follows that
x
(cid:20) (cid:21)
(cid:16) (cid:104) (cid:105)(cid:17)−1
D(q;p) = E (∇logp−∇logq)⊤ E (∇logq)(∇logq)⊤ (∇logp−∇logq) (39)
q q
(cid:20) (cid:21)
(cid:16) (cid:104) (cid:105) (cid:17)−1
= E (∇logp˜−∇logq˜)⊤A A⊤E (∇logq˜)(∇logq˜)⊤ A A⊤(∇logp˜−∇logq˜) (40)
q˜ q˜
(cid:20) (cid:21)
(cid:16) (cid:104) (cid:105)(cid:17)−1
= E (∇logp˜−∇logq˜)⊤ E (∇logq˜)(∇logq˜)⊤ (∇logp˜−∇logq˜) (41)
q˜ q˜
= D(cid:0) q˜,p˜(cid:1) . (42)
Note the important role played by the matrix Γ = E (cid:2) (∇logq)(∇logq)⊤(cid:3) in this calcula-
q q
tion. In particular, the unscaled quantity E [∥∇logp−∇logq∥2] is not invariant under affine
q
reparameterizations of RD.
Theorem A.5 (Annealing). If p is an annealing of q, with p ∝ qβ, then D(q;p) = D(β−1)2.
Proof. In this case ∇logp = β∇logq. Thus, with Γ defined as in Lemma A.1, we have
q
(cid:104) (cid:105)
D(q;p) = (β−1)2E (∇logq)⊤Γ−1(∇logq) = (β−1)2tr(cid:0) Γ−1Γ (cid:1) = D(β−1)2. (43)
q q q q
Here we see that D(q;p) measures the difference in inverse temperature from the annealing.
Note that in the limit β → 0 of a uniform distribution, eq. (43) yields a divergence of D that is
independent of the base distribution q.
Theorem A.6 (Exponential tilting). Ifpisanexponentialtiltingofq,withp(z) ∝ q(z)eθ⊤z,
then D(q;p) = θ⊤Γ−1θ where Γ is defined as in Lemma A.1.
q q
Proof. In this case ∇logp−∇logq = θ, and the result follows at once from substitution into
eq. (34).
Proposition A.7 (Gaussian score-based divergences). Suppose that p is multivariate
Gaussian with mean µ and covariance Σ and that q is multivariate Gaussian with mean ν and
covariance Ψ, respectively. Then
(cid:104) (cid:105)
D(q;p) = tr (cid:0) I −ΨΣ−1(cid:1)2 +(ν−µ)⊤Σ−1ΨΣ−1(ν−µ). (44)
Proof. We use the previous result in eq. (35) that Γ = Ψ−1 when q is Gaussian with
q18 CAI, MODI, PILLAUD-VIVIEN, MARGOSSIAN, GOWER, BLEI, AND SAUL
covariance Ψ. Then from eq. (34) the score-based divergence is given by
(cid:104) (cid:105)
D(q;p) = E (∇logp−∇logq)⊤Γ−1(∇logp−∇logq) , (45)
q q
(cid:104) (cid:105)
= E (cid:0) Σ−1(z−µ)−Ψ−1(z−ν)(cid:1)⊤ Ψ(cid:0) Σ−1(z−µ)−Ψ−1(z−ν)(cid:1) , (46)
q
(cid:104) (cid:105)
= E (cid:0)(cid:0) Σ−1−Ψ−1(cid:1) (z−ν)−Σ−1(µ−ν)(cid:1)⊤ Ψ(cid:0)(cid:0) Σ−1−Ψ−1(cid:1) (z−ν)−Σ−1(µ−ν)(cid:1) , (47)
q
= tr(cid:2) Ψ(Σ−1−Ψ−1)Ψ(Σ−1−Ψ−1)(cid:3) + (ν−µ)⊤Σ−1ΨΣ−1(ν−µ), (48)
(cid:104) (cid:105)
= tr (cid:0) I −ΨΣ−1(cid:1)2 + (ν−µ)⊤Σ−1ΨΣ−1(ν−µ). (49)
Corollary A.8 (Relation to KL divergence). Let p and q be multivariate
Gaussian distributions with different means but the same covariance matrix. Then
1D(q;p) = KL(q;p) = KL(p;q).
2
Proof. Let µ and ν denote, respectively, the means of p and q, and let Σ denote their shared
covariance. From the previous result, we find
D(q;p) = (ν−µ)⊤Σ−1(ν−µ). (50)
Finally, we recall the standard derivation for these distributions that
(cid:104) (cid:105)
KL(q;p) = E log q (51)
q p
(cid:104) (cid:105)
= 1E (z−ν)⊤Σ−1(z−ν)−(z−µ)⊤Σ−1(z−µ) (52)
2 q
(cid:104) (cid:105)
= 1E ((z−µ)−(ν−µ))⊤Σ−1((z−µ)−(ν−µ))−(z−µ)⊤Σ−1(z−µ) (53)
2 q
= 1(ν−µ)⊤Σ−1(ν−µ), (54)
2
thus matching the result for 1D(q;p). Moreover, we obtain the same result for KL(p;q) by noting
2
that the above expression is symmetric with respect to the means µ and ν.
In sum, the score-based divergence D(q;p) in eq. (34) has several attractive properties as a
measure of difference between most smooth distributions p and q with support on all of RD. First,
it is nonnegative and equal to zero if and only if p=q. Second, it is invariant to affine reparame-
terizations of the underlying domain. Third, it behaves intuitively for simple transformations such
as exponential tilting and annealing. Fourth, it is normalized such that every base distribution q
has the same divergence to (the limiting case of) a uniform distribution. Finally, it reduces to a
constant factor of the KL divergence for the special case of two multivariate Gaussians with the
same covariance matrix but different means.
APPENDIX B: QUADRATIC MATRIX EQUATIONS
In this appendix we show how to solve the quadratic matrix equation XUX+X=V where
U and V are positive semidefinite matrices in RD×D. We also verify certain properties of these
solutions that are needed elsewhere in the paper but that are not immediately obvious. Quadratic
matrix equations of this type (and of many generalizations thereof) have been studied forBATCH AND MATCH BLACK-BOX VARIATIONAL INFERENCE 19
decades (Kučera, 1972a,b; Potter, 1966; Shurbet et al., 1974; Yuan et al., 2021), and our main
goal here is to collect the results that we need in their simplest forms. These results are contained
in the following four lemmas.
Lemma B.1. Let U⪰0 and V ≻0, and suppose that XUX+X=V. Then a solution to this
equation is given by
(cid:104) (cid:105)−1
X = 2V I +(I +4UV)1 2 . (55)
Proof. We start by turning the left side of the equation XUX+X=V into a form that can
be easily factored. Multiplying both sides by U, we see that
UXUX +UX = UV. (56)
The next step is to complete the square by adding 1I to both sides; in this way, we find that
4
(cid:0) UX + 1I(cid:1)2 = UV + 1I. (57)
2 4
Next we claim that the matrix UV +1I on the right side of eq. (57) has all positive eigenvalues.
4
To verify this claim, we note that
(cid:16) (cid:17)
UV + 1I = V−1 2 V 1 2UV 1 2 + 1I V 1 2. (58)
4 4
Thus we see that this matrix is similar to (and thus shares all the same eigenvalues as) the positive
definite matrix U1 2VU1 2 + 1I in parentheses on the right side of eq. (58). Since the matrix has all
4
positive eigenvalues, it has a unique principal square root, and from eq. (57) it follows that
UX = (UV + 1I)1 2 − 1I. (59)
4 2
If the matrix U were of full rank, then we could solve for X by left-multiplying both sides of
eq. (59) by its inverse; however, we desire a general solution even in the case that U is not full
rank. Thus we proceed in a different way. In particular, we substitute the solution for UX in
eq. (59) into the original form of the quadratic matrix equation. In this way we find that
V = XUX +X, (60)
= X(UX +I), (61)
= X(cid:104)(cid:16) (cid:0) UV + 1I(cid:1)1 2 − 1I(cid:17) +I(cid:105) , (62)
4 2
(cid:104) (cid:105)
= X (UV + 1I)1 2 + 1I , (63)
4 2
(cid:104) (cid:105)
= 1X (4UV +I)1 2 +I . (64)
2
Finally we note that the matrix in brackets on the right side of eq. (64) has all positive eigenvalues;
hence it is invertible, and after right-multiplying eq. (64) by its inverse we obtain the desired
solution in eq. (55).
Lemma B.2. The solution to XUX+X=V in eq. (55) is symmetric and positive definite.20 CAI, MODI, PILLAUD-VIVIEN, MARGOSSIAN, GOWER, BLEI, AND SAUL
Proof. The key idea of the proof is to simultaneously diagonalize the matrices U and V−1
by congruence. In particular, let Λ and E be, respectively, the diagonal and orthogonal matrices
satisfying
V 1 2UV 1 2 = EΛE⊤, (65)
where Λ⪰0. Now define C=V 1 2E. It follows that C⊤V−1C=I and C⊤UC=Λ, showing that C
simultaneously diagonalizes V−1 and U by congruence. Alternatively, we may use these relations
to express U and V in terms of C and Λ as
V = CC⊤, (66)
U = C−⊤ΛC−1. (67)
We now substitute these expressions for U and V into the solution from eq. (55). The following
calculation then gives the desired result:
(cid:104) (cid:105)−1
X = 2V I +(I +4UV)−1 2 , (68)
(cid:20)
(cid:16)
(cid:17)1(cid:21)−1
= 2CC⊤ I + I +4C−⊤ΛC⊤ 2 , (69)
(cid:20)
(cid:16)
(cid:17)1(cid:21)−1
= 2CC⊤ I + C−⊤(I +4Λ)C⊤ 2 , (70)
(cid:104) (cid:105)−1
= 2CC⊤ I +C−⊤(I +4Λ)1 2C⊤ , (71)
(cid:104) (cid:16) (cid:17) (cid:105)−1
= 2CC⊤ C−⊤ I +(I +4Λ)1 2 C⊤ , (72)
(cid:104) (cid:105)−1
= 2CC⊤C−⊤ I +(I +4Λ)21 C⊤, (73)
(cid:104) (cid:105)−1
= 2C I +(I +4Λ)1 2 C⊤. (74)
Recalling that Λ⪰0, we see that the above expression for X is manifestly symmetric and positive
definite.
Next we consider the cost of computing the solution to XUX+X=V in eq. (55). On the
right side of eq. (55) there appear both a matrix square root and a matrix inverse. As written, it
therefore costs O(D3) to compute this solution when U and V are D×D matrices. However, if U
is of very low rank, there is a way to compute this solution much more efficiently. This possibility
is demonstrated by the following lemma.
Lemma B.3 (Low rank solver). Let U =QQ⊤ where Q∈RD×K. Then the solution in
eq. (55), or equivalently in eq. (74), can also be computed as
(cid:20)
(cid:16)
(cid:17)1(cid:21)−2
X = V −V⊤Q 1I + Q⊤VQ+ 1I 2 Q⊤V. (75)
2 4
Before proving the lemma, we analyze the computational cost to evaluate eq. (75). Note that
it costs O(KD2) to compute the decomposition U = QQ⊤ as well as to form the product Q⊤V,BATCH AND MATCH BLACK-BOX VARIATIONAL INFERENCE 21
while it costs O(K3) to invert and take square roots of K×K matrices. Thus the total cost of
eq. (75) is O(KD2+K3), in comparison to the O(D3) cost of eq. (55). This computational cost
results in a potentially large savings if K≪D. We now prove the lemma.
Proof. We will show that eq. (75) is equivalent to eq. (74) in the previous lemma. Again we
appeal to the existence of an invertible matrix C that simultaneously diagonalizes V−1 and U as
in eqs. (66–67). If U=QQ⊤, then it follows from eq. (67) that
Q = C−⊤Λ1 2R (76)
for some orthogonal matrix R. Next we substitute V =CC⊤ from eq. (66) and Q=C−⊤Λ1 2R
from eq. (76) in place of each appearance of V and Q in eq. (75). In this way we find that
(cid:20)
(cid:16)
(cid:17)1(cid:21)−2
X = V −V⊤Q 1I + Q⊤VQ+ 1I 2 Q⊤V, (77)
2 4
(cid:20)
(cid:16)
(cid:17)1(cid:21)−2
= CC⊤−CΛ21 R 1I + (R⊤Λ1 2C−1)(CC⊤)(C−⊤Λ21 R)+ 1I 2 R⊤Λ1 2C⊤, (78)
2 4
(cid:34) (cid:35)
(cid:20)
(cid:16)
(cid:17)1(cid:21)−2
= C I −Λ1 2R 1I + R⊤ΛR+ 1I 2 R⊤Λ1 2 C⊤, (79)
2 4
(cid:20) (cid:21)
= C I −Λ21 R(cid:104) 1I +R⊤(cid:0) Λ+ 1I(cid:1)1 2 R(cid:105)−2 R⊤Λ21 C⊤, (80)
2 4
(cid:20) (cid:21)
= C I −Λ1
2R(cid:104) R⊤(cid:16)
1I +(cid:0) Λ+ 1I(cid:1)
21(cid:17) R(cid:105)−2
R⊤Λ1 2 C⊤, (81)
2 4
(cid:34) (cid:35)
= C I −Λ1
2R(cid:20)
R⊤(cid:16) 1I +(cid:0) Λ+ 1I(cid:1)1 2(cid:17)2
R(cid:21)−1
R⊤Λ1 2 C⊤, (82)
2 4
(cid:20) (cid:20) (cid:21) (cid:21)
= C I −Λ21 R R⊤(cid:16) 1I +(cid:0) Λ+ 1I(cid:1)1 2(cid:17)−2 R R⊤Λ21 C⊤, (83)
2 4
(cid:20) (cid:21)
= C I −Λ21 (cid:16) 1I +(cid:0) Λ+ 1I(cid:1)1 2(cid:17)−2 Λ1 2 C⊤. (84)
2 4
We now compare the matrices sandwiched between C and C⊤ in eqs. (74) and (84). Both of
these sandwiched matrices are diagonal, so it is enough to compare their corresponding diagonal
elements. Let ν denote one element along the diagonal of Λ. Then starting from eq. (84), we see
that
√
ν 4ν (1+ 4ν +1)2−4ν 2
1− = 1− √ = √ = √ . (85)
(cid:16) (cid:113) (cid:17)2 (1+ 4ν +1)2 (1+ 4ν +1)2 1+ 4ν +1
1 + ν + 1
2 4
Comparing the left and right terms in eq. (85), we see that the corresponding elements of diagonal
matrices in eqs. (74) and (84) are equal, and we conclude that eqs. (55) and (75) yield the same
solution.
The last lemma in this appendix is one that we will need for the proof of convergence of
Algorithm 1 in the limit of infinite batch size. In particular, it is needed to prove the sandwiching
inequality in eq. (26).22 CAI, MODI, PILLAUD-VIVIEN, MARGOSSIAN, GOWER, BLEI, AND SAUL
Lemma B.4 (Monotonicity). Let X, Y, and V be positive-definite matrices satisfying
XTX +X = YUY +Y = V, where T ⪰ U ⪰ 0. Then X ⪯ Y.
Proof. The result follows from examining the solutions for X and Y directly. As shorthand,
let S = V 1 2. By Lemma B.1, we have the solutions
X =
2S(cid:104)
I +(S +4STS)1
2(cid:105)−1
S, (86)
Y =
2S(cid:104)
I +(S +4SUS)1
2(cid:105)−1
S. (87)
IfT ⪰ U,thenthepositivesemi-definiteorderingispreservedbythefollowingchainofimplications:
STS ⪰ SUS, (88)
S +4STS ⪰ S +4SUS, (89)
(S +4STS)1 2 ⪰ (S +4SUS)1 2, (90)
I +(S +4STS)1 2 ⪰ I +(S +4SUS)1 2, (91)
where in eq. (90) we have used the fact that positive semi-definite orderings are preserved by
matrix square roots. Finally, these orderings are reversed by inverse operations, so that
(cid:104) (cid:105)−1 (cid:104) (cid:105)−1
I +(S +4STS)1 2 ⪯ I +(S +4SUS)21 . (92)
It follows from eq. (92) and the solutions in eqs. (86–87) that X ⪯ Y, thus proving the lemma.
APPENDIX C: DERIVATION OF BATCH AND MATCH UPDATES
In this appendix we derive the updates in Algorithm 1 for score-based variational inference. The
algorithm alternates between two steps—a batch step that draws samples from an approximating
Gaussian distribution and computes various statistics of these samples, and a match step that
uses these statistics to derive an updated Gaussian approximation, one that better matches the
scores of the target distribution. We explain each of these steps in turn, and then we review the
special case in which they reduce to the previously published updates (Modi et al., 2023) for
Gaussian Score Matching (GSM).
C.1. Batch step. At each iteration, Algorithm 1 solves an optimization based on samples
drawn from its current Gaussian approximation to the target distribution. Let q denote this
t
approximation at the tth iteration, with mean µ and covariance Σ , and let z ,z ,...,z denote
t t 1 2 B
theB samplesthataredrawnfromthisdistribution.Thealgorithmusesthesesamplestocompute
a (biased) empirical estimate of the score-based divergence between the target distribution, p,
and another Gaussian approximation q with mean µ and covariance Σ. We denote this empirical
estimate by
B
1 (cid:88)(cid:13) (cid:13)2
D (cid:98)qt(q;p) =
B
(cid:13) (cid:13)∇logq(z b)−∇logp(z b)(cid:13)
(cid:13)
Σ. (93)
b=1
To optimize the Gaussian approximation q that appears in this divergence, it is first necessary
to evaluate the sum in eq. (93) over the batch of samples z ,z ,...,z that have been drawn
1 2 B
from q .
tBATCH AND MATCH BLACK-BOX VARIATIONAL INFERENCE 23
The batch step of Algorithm 1 computes the statistics of these samples that enter into this
calculation. Since q is Gaussian, its score at the bth sample is given by ∇logq(z ) = −Σ−1(z −µ).
b b
As shorthand, let g = ∇logp(z ) denote the score of the target distribution at the bth sample.
b b
In terms of these scores, the sum in eq. (93) is given by
B
1 (cid:88)(cid:13) (cid:13)2
D (cid:98)qt(q;p) =
B
(cid:13) (cid:13)−Σ−1(z b−µ)−g b(cid:13)
(cid:13)
Σ. (94)
b=1
Next we show that D (cid:98)qt(q,p) depends in a simple way on certain first-order and second-order
statistics of the samples, and it is precisely these statistics that are computed in the batch step.
In particular, we compute the following:
B B B N
1 (cid:88) 1 (cid:88) 1 (cid:88) 1 (cid:88)
z = z , g = g , C = (z −z)(z −z)⊤, Γ = (g −g)(g −g)⊤. (95)
b b b b b b
B B B B
b=1 b=1 b=1 n=1
Note that the first two of these statistics compute the means of the samples and scores in the
current iteration of the algorithm, while the remaining two compute their covariance matrices.
With these definitions, we can now express D (cid:98)qt(q,p) in an especially revealing form. Proceeding
from eq. (94), we have
B
1 (cid:88)(cid:13) (cid:13)2
D (cid:98)qt(q;p) =
B
(cid:13) (cid:13)(g−g b)+Σ−1(z−z b)+Σ−1(µ−z−Σg)(cid:13)
(cid:13)
Σ, (96)
b=1
B
= B1 (cid:88)(cid:104)(cid:13) (cid:13)g b−g(cid:13) (cid:13)2 Σ+(cid:13) (cid:13)z b−z(cid:13) (cid:13)2
Σ−1
+(cid:13) (cid:13)µ−z−Σg(cid:13) (cid:13)2
Σ−1
+2(g b−g)(z b−z)(cid:105) , (97)
b=1
= tr(ΓΣ) + tr(CΣ−1) + (cid:13) (cid:13)µ−z−Σg(cid:13) (cid:13)2 + constant, (98)
Σ−1
where in the second line we have exploited that many cross-terms vanish, and in the third line
we have appealed to the definitions of C and Γ in eqs. (95). We have also indicated explicitly
that the last term in eq. (98) has no dependence on µ and Σ; it is a constant with respect to the
approximating distribution q that the algorithm seeks to optimize. This optimization is performed
by the match step, to which we turn our attention next.
C.2. Match step. The match step of the algorithm updates the Gaussian approximation
of VI to better match the recently sampled scores of the target distribution. The update at the
tth iteration is computed as
(cid:104) (cid:105)
q = argmin LBaM(q) , (99)
t+1
q∈Q
where Q is the Gaussian variational family of Section 2 and LBaM(q) is an objective function that
balances the empirical estimate of the score based divergence in in eq. (98) against a regularizer
that controls how far q can move away from q . Specifically, the objective function takes the
t+1 t
form
LBaM(q) = D (cid:98)qt(q;p)+ λ2 tKL(q t;q), (100)24 CAI, MODI, PILLAUD-VIVIEN, MARGOSSIAN, GOWER, BLEI, AND SAUL
where the regularizing term is proportional to the KL divergence between the Gaussian distribu-
tions q and q. This KL divergence is in turn given by the standard result
t
(cid:20) (cid:21)
|Σ |
KL(q ;q) = 1 tr(Σ−1Σ )−log t +∥µ−µ ∥2 −D . (101)
t 2 t |Σ| t Σ−1
Fromeqs.(98)and(101),weseethatthisobjectivefunctionhasacomplicatedcoupleddependence
on µ and Σ; nevertheless, the optimal values of µ and Σ can be computed in closed form. The
rest of this section is devoted to performing this optimization.
First we perform the optimization with respect to the mean µ, which appears quadratically in
the objective LBaM through the third terms in (98) and (101). Thus we find
∂L ∂B µaM = ∂∂
µ
(cid:26) (cid:13) (cid:13)µ−z−Σg(cid:13) (cid:13)2
Σ−1
+ λ1
t
∥µ−µ t∥2 Σ−1(cid:27) = 2Σ−1(cid:104) µ−z−Σg+ λ1 t(µ−µ t)(cid:105) .
(102)
Setting this gradient to zero, we obtain a linear system which can be solved for the updated
mean µ in terms of the updated covariance Σ . Specifically we find
t+1 t+1
λ 1
µ = t (z+Σ g) + µ , (103)
t+1 t+1 t
1+λ 1+λ
t t
matching eq. (13) in Section 3 of the paper. As a sanity check, we observe that in the limit of
infinite regularization (λ →0), the updated mean is equal to the previous mean (with µ =µ ),
t t+1 t
while in the limit of zero regularization (λ →∞), the updated mean is equal to precisely the
t
value that zeros its contribution to D (cid:98)qt(q,p) in eq. (98).
Next we perform this optimization with respect to the covariance Σ. To simplify our work, we
first eliminate the mean µ from the optimization via eq. (103). When the mean is eliminated in
this way from eqs. (98) and (101), we find that
D (cid:98)qt(q;p) = tr(ΓΣ) + tr(CΣ−1) + (1+1
λ
)2(cid:13) (cid:13)µ t−z−Σg(cid:13) (cid:13)2
Σ−1
+ constant, (104)
t
KL(q t;q) = 21 (cid:20) tr(Σ−1Σ t)−log | |Σ Σt || + (1+λ2 t
λ
)2(cid:13) (cid:13)µ t−z−Σg(cid:13) (cid:13)2
Σ−1
−D(cid:21) . (105)
t
Combining these terms via eq. (100), and dropping additive constants, we obtain an objective
function of the covariance matrix Σ alone. We denote this objective function by M(Σ), and it is
given by
(cid:18)(cid:20) (cid:21) (cid:19)
M(Σ) = tr(ΓΣ) + tr C+ λ1 Σ
t
Σ−1 + 1+1
λ
(cid:16)(cid:13) (cid:13)µ t−z(cid:13) (cid:13)2
Σ−1
+(cid:13) (cid:13)g(cid:13) (cid:13)2 Σ(cid:17) + λ1 log|Σ|. (106)
t t t
AllthetermsinthisobjectivefunctioncanbedifferentiatedwithrespecttoΣ.TominimizeM(Σ),
we set its total derivative to zero. Doing this, we find that
(cid:20) (cid:21)
1 1 1 1
0 = Γ+ gg⊤−Σ−1 C+ Σ + (µ −z)(µ −z)⊤ Σ−1+ Σ−1. (107)
t t t
1+λ λ 1+λ λ
t t t t
The above is a quadratic matrix equation for the inverse covariance matrix Σ−1; multiplying on
the left and right by Σ, we can rewrite it as a quadratic matrix equation for Σ. In this way weBATCH AND MATCH BLACK-BOX VARIATIONAL INFERENCE 25
find that

λ
 

U = λ tΓ+ 1+t
λ
gg⊤,
ΣUΣ+Σ = V where t (108)
λ
  V = Σ +λ C + t (µ −z)(µ −z)⊤,
 t t t t
1+λ
t
matching eq. (9) in Section 3 of the paper. The solution to this quadratic matrix equation is
given by Lemma B.1, yielding the update rule
(cid:104) (cid:105)−1
Σ t+1 = 2V I +(I +4UV)1 2 (109)
and matching eq. (12) in Section 3 of the paper. Moreover, this solution is guaranteed to be
symmetric and positive definite by Lemma B.2.
C.3. Gaussian score matching as a special case. In this section, we show that the
updates for BaM include the updates for GSM (Modi et al., 2023) as a limiting case. In BaM,
this limiting case occurs when there is no regularization (λ→∞) and when the batch size is equal
to one (B=1). In this case, we show that the updates in eqs. (103) and (108) coincide with those
of GSM.
To see this equivalence, we set B=1, and we use z and g to denote, respectively, the single
t t
sample from q and its score under p at the tth iteration of BaM. The equivalence arises from a
t
simple intuition: as λ→∞, all the weight in the loss shifts to minimizing the divergence D (cid:98)qt(q;p),
which is then minimized exactly so that D (cid:98)qt(q;p)=0. More formally, in this limit the batch step
can be written as
(cid:20) (cid:21)
λl →im ∞m q∈i Qn D (cid:98)qt(q;p)+ λ2 tKL(q t;q) = m q∈i Qn(cid:2)KL(q t;q)(cid:3) such that D (cid:98)qt(q;p)=0. (110)
The divergence term D (cid:98)qt(q;p) only vanishes when the scores match exactly; thus the above can
be re-written as
min(cid:2)KL(q ;q)(cid:3) such that ∇logq(z )=∇logp(z ), (111)
t t t
q∈Q
which is exactly the variational formulation of the GSM method (Modi et al., 2023)
We can also make this equivalence more precise by studying the resulting update. Indeed, the
batch statistics in eq. (95) simplify in this setting: namely, we have z = z and g = g (because
t t
there is only one sample) and C=Γ=0 (because the batch has no variance). Next we take the
limit λ →∞ in eq. (108). In this limit we find that
t
U = g g⊤, (112)
t t
V = Σ +(µ −z )(µ −z )⊤, (113)
t t t t t
so that the covariance is updated by solving the quadratic matrix equation
Σ g g⊤Σ +Σ = Σ +(µ −z )(µ −z )⊤. (114)
t+1 t t t+1 t+1 t t t t t
Similarly, taking the limit λ →∞ in eq. (103), we see that the mean is updated as
t
µ = Σ g +z . (115)
t+1 t+1 t t
These BaM updates coincide exactly with the updates for GSM: specifically, eqs. (114) and (115)
here are identical to eqs. (42) and (23) in Modi et al. (2023).26 CAI, MODI, PILLAUD-VIVIEN, MARGOSSIAN, GOWER, BLEI, AND SAUL
APPENDIX D: PROOF OF CONVERGENCE
In this appendix we provide full details for the proof of convergence in Theorem 3.1. We
repeat equations freely from earlier parts of the paper when it helps to make the appendix more
self-contained. Recall that the target distribution in this setting is assumed to be Gaussian with
mean µ and covariance Σ ; in addition, we measure the normalized errors at the tth iteration by
∗ ∗
ε =
Σ−1
2(µ −µ ), (116)
t ∗ t ∗
∆ t = Σ− 21 Σ tΣ−1 2 −I. (117)
If the mean and covariance iterates of Algorithm 1 converge to those of the target distribution,
then equivalently the norms of these errors must converge to zero. Many of our intermediate
results are expressed in terms of the matrices
J =
Σ−1
2Σ
Σ−1
2, (118)
t ∗ t ∗
which from eq. (117) we can also write as J = I +∆ . For convenience we restate the theorem in
t t
section D.1; our main result is that in the limit of an infinite batch size, the norms of the errors
in eqs. (116–117) decay exponentially to zero with rates that we can bound from below.
The rest of the appendix is organized according to the major steps of the proof as sketched in
section 3.2. In section D.2, we examine the statistics that are computed by Algorithm 1 when the
target distribution is Gaussian and the number of batch samples goes to infinity. In section D.3,
we derive the recursions that are satisfied for the normalized mean ε and covariance J in this
t t
limit. In section D.4, we derive a sandwiching inequality for positive-definite matrices that arise in
the analysis of these recursions. In section D.5, we use the sandwiching inequality to derive upper
andlowerboundsontheeigenvaluesofJ .InsectionD.6,weusetheseeigenvalueboundstoderive
t
how the normalized errors ε and ∆ decay from one iteration to the next. In section D.7, we use
t t
induction on these results to derive the final bounds on the errors in eqs. (121–122), thus proving
the theorem. In the more technical sections of the appendix, we sometimes require intermediate
results that digress from the main flow of the argument; to avoid too many digressions, we collect
the proofs for all of these intermediate results in section D.8.
D.1. Main result. Recall that our main result is that as B → ∞, the spectral norms of the
normalized mean and covariance errors in decay exponentially to zero with rates that we can
bound from below.BATCH AND MATCH BLACK-BOX VARIATIONAL INFERENCE 27
TheoremD.1(RestatementofTheorem3.1). Supposethatp = N(µ ,Σ )inAlgorithm1,
∗ ∗
and let α>0 denote the minimum eigenvalue of the matrix
Σ−1
2Σ
Σ−1
2. For any fixed level of
∗ 0 ∗
regularization λ>0, define
(cid:18) (cid:19)
1+λ
β := min α, , (119)
1+λ+∥ε ∥2
0
λβ
δ := , (120)
1+λ
where β ∈ (0,1] measures the quality of initialization and δ ∈ (0,1) denotes a rate of decay.
Then with probability 1 in the limit of infinite batch size (B →∞), and for all t≥ 0, the
normalized errors in eqs. (116–117) satisfy
∥ε ∥ ≤ (1−δ)t∥ε ∥, (121)
t 0
∥∆ ∥ ≤ (1−δ)t∥∆ ∥ + t(1−δ)t−1∥ε ∥2. (122)
t 0 0
We emphasize that the theorem holds under very general conditions: it is true no matter how
the variational parameters are initialized (assuming only that they are finite and that the initial
covariance estimate is not singular), and it is true for any fixed degree of regularization λ>0.
Notably, the value of λ is not required to be inversely proportional to the largest (but a priori
unknown) eigenvalue of some Hessian matrix, an assumption that is typically needed to prove the
convergence of most gradient-based methods. This stability with respect to hyperparameters is a
well-known property of proximal algorithms, one that has been previously observed beyond the
setting of variational inference in this paper.
Finally we note that the bounds in eqs. (121–122) can be tightened with more elaborate
bookkeeping and also extended to updates that use varying levels of regularization {λ }∞
t t=0
at different iterations of the algorithm. At various points in what follows, we indicate how to
strengthen the results of the theorem along these lines. Throughout this section, we use the
matrix norm ∥·∥ to denote the spectral norm, and we use the notation ν (J) and ν (J) to
min max
denote the minimum and maximum eigenvalues of a matrix J.
D.2. Infinite batch limit. The first step of the proof is analyze how the statistics computed
at each iteration of Algorithm 1 simplify in the infinite batch limit (B →∞). Let q denote
t
the Gaussian variational approximation at the tth iteration of the algorithm, let z ∼ N(µ ,Σ )
b t t
denote the bth sample from this distribution, and let g = ∇logp(z ) denote the corresponding
b b
score of the target distribution p at this sample. Recall that step 5 of Algorithm 1 computes the
following batch statistics:
B B
1 (cid:88) 1 (cid:88)
z = z , C = (z −z )(z −z )⊤, (123)
B b B b B b B
B B
b=1 b=1
B B
1 (cid:88) 1 (cid:88)
g = g , Γ = (g −g )(g −g )⊤, (124)
B B b B B b B b B
b=1 b=1
Here we use the subscript on these averages to explicitly indicate the batch size. (Also, to avoid
an excess of indices, we do not explicitly indicate the iteration t of the algorithm.) These statistics28 CAI, MODI, PILLAUD-VIVIEN, MARGOSSIAN, GOWER, BLEI, AND SAUL
simplify considerably when the target distribution is multivariate Gaussian and the number of
batch samples goes to infinity. In particular, we obtain the following result.
Lemma D.2 (Infinite batch limit). Suppose p=N(µ ,Σ ). Then with probability 1, as
∗ ∗
the number of batch samples goes to infinity (B→∞), the statistics in eqs. (123–124) tend to
lim z = µ , (125)
B t
B→∞
lim C = Σ , (126)
B t
B→∞
lim g = Σ−1(µ −µ ), (127)
B ∗ ∗ t
B→∞
lim Γ = Σ−1Σ Σ−1. (128)
B ∗ t ∗
B→∞
Proof. The first two of these limits follow directly from the strong law of large numbers. In
particular, for the sample mean in eq. (123), we have with probability 1 that
(cid:34) B (cid:35) (cid:90)
1 (cid:88)
lim z = lim z = zq (dz) = µ , (129)
B b t t
B→∞ B→∞ B
b=1
thus yielding eq. (125). Likewise for the sample covariance in eq. (123), we have with probability 1
that
(cid:34) B (cid:35) (cid:90)
1 (cid:88)
lim C = lim (z −z )(z −z )⊤ = (z−µ )(z−µ )⊤q (dz) = Σ , (130)
B b B b B t t t t
B→∞ B→∞ B
b=1
thus yielding eq. (126). Next we consider the infinite batch limits for g and Γ , in eq. (124),
B B
involving the scores of the target distribution. Note that if this target distribution is multivariate
Gaussian, with p = N(µ ,Σ ), then we have
∗ ∗
g = ∇logp(z ) = Σ−1(µ −z ), (131)
b b ∗ ∗ b
showing that the score g is a linear function of z . Thus the infinite batch limits g and Γ
b b B B
follow directly from those for z and C . In particular, combining eq. (131) with the calculation
B B
in eq. (129), we see that
(cid:34) B (cid:35)
1 (cid:88) (cid:104) (cid:105)
lim g = lim g = lim Σ−1(µ −z ) = Σ−1(µ −µ ) (132)
B→∞ B B→∞ B b B→∞ ∗ ∗ B ∗ ∗ t
b=1
for the mean of the scores in this limit, thus yielding eq. (127). Likewise, by the same reasoning,
we see that
(cid:34) B (cid:35)
1 (cid:88)
lim Γ = lim (g −g )(g −g )⊤ = lim Σ−1C Σ−1 = Σ−1Σ Σ−1 (133)
B→∞ B B→∞ B b B b B B→∞ ∗ B ∗ ∗ t ∗
b=1
for the covariance of the scores in this limit, thus yielding eq. (128). This proves the lemma.BATCH AND MATCH BLACK-BOX VARIATIONAL INFERENCE 29
D.3. Recursions for ε and J . Next we use Lemma D.2 to derive recursions for the
t t
normalized error ε in eq. (116) and the normalized covariance J in eq. (118). Both follow directly
t t
from our previous results.
Proposition D.3 (Recursion for ε ). Suppose p=N(µ ,Σ ), and let B→∞ in Algorithm 1.
t ∗ ∗
Then with probability 1, the normalized error at the (t+1)th iteration of satisfies
(cid:20) (cid:21)
λ
ε = I − t J ε . (134)
t+1 t+1 t
1+λ
t
Proof. Consider the update for the variational mean in step 7 of Algorithm 1. We begin by
computing the infinite batch limit of this update. Using the limits for z and g from Lemma D.2,
B B
we see that
(cid:20)(cid:18) (cid:19) (cid:18) (cid:19) (cid:21)
µ = lim 1 µ + λ t (cid:0) Σ g +z (cid:1) , (135)
t+1 B→∞ 1+λ t t 1+λ t t+1 B B
(cid:18) (cid:19) (cid:18) (cid:19)
1 λ (cid:16) (cid:17)
= µ + t Σ Σ−1(µ −µ )+µ , (136)
1+λ t 1+λ t+1 ∗ ∗ t t
t t
λ
= µ + t Σ Σ−1(µ −µ ). (137)
t 1+λ t+1 ∗ ∗ t
t
The proposition then follows by substituting eq. (137) into the definition of the normalized error
in eq. (116):
ε =
Σ−1
2(µ −µ ), (138)
t+1 ∗ t+1 ∗
(cid:20) (cid:21)
= Σ−1 2 µ + λ t Σ Σ−1(µ −µ )−µ , (139)
∗ t 1+λ t+1 ∗ ∗ t ∗
t
(cid:20) (cid:21)
= I − λ t Σ−1 2Σ Σ− 21 Σ−1 2(µ −µ ), (140)
∗ t+1 ∗ ∗ t ∗
1+λ
t
(cid:20) (cid:21)
λ
= I − t J ε . (141)
t+1 t
1+λ
t
This proves the proposition, and we note that this recursion takes the same form as eq. (23), in
the proof sketch of Theorem 3.1, if a fixed level of regularization is used at each iteration.
Proposition D.4 (Recursion for J ). Supposep=N(µ ,Σ ),andletB→∞inAlgorithm1.
t ∗ ∗
Then with probability 1, the normalized covariance at the (t+1)th iteration of satisfies
(cid:18) (cid:19)
1
λ J J + ε ε⊤ J +J = (1+λ )J (142)
t t+1 t 1+λ t t t+1 t+1 t t
t
Proof. Consider the quadratic matrix equation, from step 6 of Algorithm 1, that is satisfied
by the variational covariance after t+1 updates:
Σ U Σ +Σ = V . (143)
t+1 B t+1 t+1 B30 CAI, MODI, PILLAUD-VIVIEN, MARGOSSIAN, GOWER, BLEI, AND SAUL
We begin by computing the infinite batch limit of the matrices, U and V , that appear in this
B B
equation. Starting from eq. (11) for V , and using the limits for z and C from Lemma D.2, we
B B B
see that
(cid:20) (cid:21)
λ
lim V = lim Σ +λ C + t (µ −z )(µ −z )⊤ , (144)
B t t B t B t B
B→∞ B→∞ 1+λ t
= (1+λ )Σ , (145)
t t
= Σ21(cid:2) (1+λ )J (cid:3) Σ1 2, (146)
∗ t t ∗
where in the last line we have used eq. (118) to re-express the right side in terms of J . Likewise,
t
starting from eq. (10) for U , and using the limits for g and Γ from Lemma D.2, we see that
B B B
(cid:20) (cid:21)
λ
lim U = lim λ Γ + t g¯ g¯⊤ (147)
B→∞ B B→∞ t B 1+λ t B B
λ
= λ Σ−1Σ Σ−1+ t Σ−1(µ−µ )(µ−µ )⊤Σ−1 (148)
t ∗ t ∗ 1+λ ∗ t t ∗
t
λ
= λ Σ−1Σ Σ−1+ t Σ−1(µ −µ )(µ −µ )⊤Σ−1 (149)
t ∗ t ∗ 1+λ ∗ ∗ t ∗ t ∗
t
(cid:18) (cid:19)
= λ Σ− 21 J + 1 ε ε⊤ Σ−1 2, (150)
t ∗ t 1+λ t t ∗
t
where again in the last line we have used eqs. (116) and (118) to re-express the right side in terms
of ε and J . Next we substitute these limits for U and V into the quadratic matrix equation in
t t B B
eq. (143). It follows that
(cid:18) (cid:19)
λ Σ Σ− 21 J + 1 ε ε⊤ Σ−1 2Σ +Σ = Σ1 2(cid:2) (1+λ )J (cid:3) Σ1 2. (151)
t t+1 ∗ t 1+λ t t ∗ t+1 t+1 ∗ t t ∗
t
Finally, we obtain the recursion in eq. (142) by left and right multiplying eq. (151) by
Σ−1
2 and
∗
again making the substitution J = Σ−1 2Σ Σ− 21 from eq. (118).
t+1 ∗ t+1 ∗
The proof of convergence in future sections relies on various relaxations to derive the simple
error bounds in eqs. (121–122). Before proceeding, it is therefore worth noting the following
property of Algorithm 1 that is not apparent from these bounds.
Corollary D.5 (One-step convergence). Suppose p=N(µ ,Σ ), and consider the limit of
∗ ∗
infinite batch size (B→∞) in Algorithm 1 followed by the additional limit of no regularization
(λ →∞). In this combined limit, the algorithm converges with probability 1 in one step: i.e.,
0
lim lim ∥ε ∥ = lim lim ∥∆ ∥ = 0.
λ0→∞ B→∞ 1 λ0→∞ B→∞ 1
Proof. Consider the recursion for J given by eq. (142) in the additional limit λ → ∞. In
1 0
this limit one can ignore the terms that are not of leading order in λ , and the recursion simplifes
0
to J J J =J . This equation has only one positive-definite solution given by J =I. Next consider
1 0 1 0 1
the recursion for ε given by eq. (134) in the additional limit λ → ∞. In this limit this recursion
1 0
simplifies to ε = (I−J )ε , showing that ε =0. It follows that Σ =Σ and µ =µ, and future
1 1 0 1 1 1
updates have no effect.BATCH AND MATCH BLACK-BOX VARIATIONAL INFERENCE 31
D.4. Sandwiching inequality. To complete the proof of convergence for Theorem 3.1, we
must show that ∥ε ∥→0 and ∥J −I∥→0 as t→∞. We showed in Propositions D.3 and D.4
t t
that ε and J satisfy simple recursions. However, it is not immediately obvious how to translate
t t
these recursions for ε and J into recursions for ∥ε ∥ and ∥J −I∥. To do so requires additional
t t t t
machinery.
One crucial piece of machinery is the sandwiching inequality that we prove in this section. In
addition to the normalized covariance matrices {J }∞ , we introduce two sequences of auxiliary
t t=0
matrices, {H }∞ and {K }∞ satisfying
t t=1 t t=1
0 ≺ H ⪯ J ⪯ K (152)
t+1 t+1 t+1
for all t≥0; this is what we call the sandwiching inequality. These auxiliary matrices are defined
by the recursions
(cid:18) (cid:19)
1
λ H J + ∥ε ∥2I H +H = (1+λ )J , (153)
t t+1 t t t+1 t+1 t t
1+λ
t
λ K J K +K = (1+λ )J . (154)
t t+1 t t+1 t+1 t t
We invite the reader to scrutinize the differences between these recursions for H and K and
t+1 t+1
the one for J eq. (142). Note that in eq. (154), defining K , we have dropped the term in
t+1 t+1
eq. (142) involving the outer-product ε ε⊤, while in eq. (153), defining H , we have replaced this
t t t+1
term by a scalar multiple of the identity matrix. As we show later, these auxiliary recursions are
easier to analyze because the matrices H and K (unlike J ) share the same eigenvectors
t+1 t+1 t+1
as J . Later we will exploit this fact to bound their eigenvalues as well as the errors ∥J −I∥.
t t+1
In this section we show that the recursions for H and K in eqs. (153–154) imply the
t+1 t+1
sandwiching inequality in eq. (152). As we shall see, the sandwiching inequality follows mainly
from the monotonicity property of these quadratic matrix equations proven in Lemma B.4.
Proposition D.6 (Sandwiching inequality). Let Σ ≻0 and λ >0 for all t≥0. Also, let
0 t
{ε }∞ , {J }∞ , {H }∞ , and {K }∞ be defined, respectively, by the recursions in eqs. (134),
t t=1 t t=1 t t=1 t t=1
(142), and (153–154). Then for all t≥0 we have
0 ≺ H ⪯ J ⪯ K . (155)
t+1 t+1 t+1
Proof. We prove the orderings in the proposition from left to right. Since Σ ≻0, it follows
0
from eq. (118) that J ≻0, and Lemma B.2 ensures for the recursion in eq. (142) that J ≻0 for
0 t+1
all t≥0. Likewise, since J ≻ 0 for all t ≥ 0, Lemma B.2 ensures for the recursion in eq. (153) that
t
H ≻0 for all t≥0. This proves the first ordering in the proposition. To prove the remaining
t+1
orderings, we note that for all vectors ε ,
t
(cid:18) (cid:19) (cid:18) (cid:19)
1 1
λ J ⪯ λ J + ε ε⊤ ⪯ λ J + ∥ε ∥2I . (156)
t t t t 1+λ t t t t 1+λ t
t t
We now apply Lemma B.4 to the quadratic matrix equations that define the recursions for H ,
t+1
J , and K . From the first ordering in eq. (156), and for the recursions for J and K in
t+1 t+1 t+1 t+1
eqs. (142) and (154), Lemma B.4 ensures that J ⪯K . Likewise, from the second ordering in
t+1 t+1
eq. (156), and for the recursions for J and H in eqs. (142) and (153), Lemma B.4 ensures
t+1 t+1
that H ⪯J .
t+1 t+132 CAI, MODI, PILLAUD-VIVIEN, MARGOSSIAN, GOWER, BLEI, AND SAUL
D.5. Eigenvalue bounds. The sandwiching inequality in the previous section provides a
powerful tool for analyzing the eigenvalues of the normalized covariance matrices {J }∞ . As
t t=1
shown in the following lemma, much of this power lies in the fact that the matrices J , H ,
t t+1
and K are jointly diagonalizable.
t+1
Lemma D.7 (Joint diagonalizability). Let λ >0 for all t≥0, and let {ε }∞ , {J }∞ ,
t t t=1 t t=1
{K }∞ , and {H }∞ be defined, respectively, by the recursions in eqs. (134), (142), and
t t=1 t t=1
(153–154). Then for all t≥0 we have the following:
(i) H and K share the same eigenvectors as J .
t+1 t+1 t
(ii) Each eigenvalue ν of J determines a corresponding eigenvalue ν of H and a corre-
J t H t+1
sponding eigenvalue ν of K via the positive roots of the quadratic equations
K t+1
(cid:18)
∥ε
∥2(cid:19)
λ ν + t ν2 +ν = (1+λ )ν , (157)
t J 1+λ H H t J
t
λ ν ν2 +ν = (1+λ )ν . (158)
t J K K t J
Proof. Write J = QΛ Q⊤, where Q is the orthogonal matrix storing the eigenvectors of J
t J t
and Λ is the diagonal matrix storing its eigenvalues. Now define the matrices
J
Λ = Q⊤H Q, (159)
H t+1
Λ = Q⊤K Q. (160)
K t+1
We will prove that J , H , and K share the same eigenvectors as J by showing that the
t t+1 t+1 t
matrices Λ and Λ are also diagonal. We start by multiplying eqs. (153–154) on the left by Q⊤
H K
and on the right by Q. In this way we find
(cid:18) (cid:19)
1
λ Λ Λ + ∥ε ∥2I Λ +Λ = (1+λ )Λ , (161)
t H J t H H t J
1+λ
t
λ Λ Λ Λ +Λ = (1+λ )Λ . (162)
t K J K K t J
Since Λ is diagonal, we see from eqs. (161–162) that Λ and Λ also have purely diagonal
J H K
solutions; this proves the first claim of the lemma. We obtain the scalar equations in eqs. (157–158)
by focusing on the corresponding diagonal elements (i.e., eigenvalues) of the matrices Λ , Λ ,
H J
and Λ in eqs. (161–162); this proves the second claim of the lemma.
K
To prove the convergence of Algorithm 1, we will also need upper and lower bounds on eigen-
values of the normalized covariance matrices. The next lemma provides these bounds.
Lemma D.8 (Bounds on eigenvalues of J ). Let λ >0 for all t≥0, and let {ε }∞ ,
t+1 t t t=1
{J }∞ , {K }∞ , and {H }∞ be defined, respectively, by the recursions in eqs. (134), (142),
t t=1 t t=1 t t=1
and (153–154). Then for all t≥ 0, the largest and smallest eigenvalues of J satisfy
t+1
(cid:114)
1+λ
ν (J ) ≤ t , (163)
max t+1
λ
t
(cid:18) (cid:19)
1+λ
ν (J ) ≥ min ν (J ), t . (164)
min t+1 min t 1+λ +∥ε ∥2
t tBATCH AND MATCH BLACK-BOX VARIATIONAL INFERENCE 33
Proof. We will prove these bounds using the sandwiching inequality. We start by proving
an upper bound on ν (K ). Recall from Lemma D.7 that each eigenvalue ν of K is
max t+1 K t+1
determined by a corresponding eigenvalue ν of J via the positive root of the quadratic equation
J t
in eq. (158). Rewriting this equation, we see that
1+λ ν 1+λ
ν2 = t − K ≤ t , (165)
K λ λ ν λ
t t J t
(cid:113)
showing that every eigenvalue of K must be less than 1+λt. Now from the sandwiching
t+1 λt
inequality, we know that J ⪯ K , from which it follows that ν (J ) ≤ ν (K ).
t+1 t+1 max t+1 max t+1
Combining these observations, we have shown
(cid:114)
1+λ
ν (J ) ≤ ν (K ) ≤ t , (166)
max t+1 max t+1
λ
t
which proves the first claim of the lemma. Next we prove a lower bound on ν (H ). Again,
min t+1
recall from Lemma D.7 that each eigenvalue ν of H is determined by a corresponding
H t+1
eigenvalue ν of J via the positive root of the quadratic equation in eq. (157). We restate this
J t
equation here for convenience:
(cid:18)
∥ε
∥2(cid:19)
λ ν + t ν2 +ν = (1+λ )ν
t J 1+λ H H t J
t
We now exploit two key properties of this equation, both of which are proven in Lemma D.13.
Specifically, Lemma D.13 states that if ν is computed from the positive root of this equation,
H
then ν is a monotonically increasing function of ν , and it also satisfies the lower bound
H J
(cid:18) (cid:19)
1+λ
ν ≥ min ν , t . (167)
H J 1+λ +∥ε ∥2
t t
We can combine these properties to derive a lower bound on the smallest eigenvalue of H ;
t+1
namely, it must be the case that
(cid:18) (cid:19)
1+λ
ν (H ) ≥ min ν (J ), t . (168)
min t+1 min t 1+λ +∥ε ∥2
t t
Now again from the sandwiching inequality, we know that J ⪰H , from which it follows that
t+1 t+1
ν (J ) ≥ ν (H ). Combining this observation with eq. (168), we see that
min t+1 min t+1
(cid:18) (cid:19)
1+λ
ν (J ) ≥ ν (H ) ≥ min ν (J ), t , (169)
min t+1 min t+1 min t 1+λ +∥ε ∥2
t t
which proves the second claim of the lemma.
D.6. Recursions for ∥ε ∥ and ∥∆ ∥. In this section, we analyze how the errors ∥ε ∥
t t t
and ∥∆ ∥ evolve from one iteration of Algorithm 1 to the next. These per-iteration results are
t
the cornerstone of the proof of convergence in the infinite batch limit.34 CAI, MODI, PILLAUD-VIVIEN, MARGOSSIAN, GOWER, BLEI, AND SAUL
Proposition D.9 (Decay of ∥ε ∥). Suppose that p = N(µ ,Σ ). Then for Algorithm 1 in
t ∗ ∗
the limit of infinite batch size (B→∞), the normalized errors in eq. (116) of the variational
mean strictly decrease from one iteration to the next: i.e., ∥ε ∥ < ∥ε ∥. More precisely, they
t+1 t
satisfy
(cid:18) (cid:19)
λ
∥ε ∥ ≤ 1− t ν (J ) ∥ε ∥, (170)
t+1 min t+1 t
1+λ
t
where the multiplier in parentheses on the right side is strictly less than one.
Proof. Recall from Proposition D.3 that the normalized errors in the variational mean satisfy
the recursion
(cid:20) (cid:21)
λ
ε = I − t J ε . (171)
t+1 t t
1+λ
t
Taking norms and applying the sub-multiplicative property of the spectral norm, we have
(cid:13) (cid:13)
∥ε t+1∥ ≤ (cid:13) (cid:13)I − λ t J t+1(cid:13) (cid:13)∥ε t∥. (172)
(cid:13) 1+λ (cid:13)
t
Consider the matrix norm that appears on the right side of eq. (172). By Lemma D.8, and
(cid:113)
specifically eq. (163) which gives the ordering J ⪯ 1+λtI, it follows that
t+1 λt
(cid:32) (cid:114) (cid:33)
λ λ
I − t J ⪰ 1− t I ≻ 0. (173)
t+1
1+λ 1+λ
t t
Thusthespectralnormofthismatrixisstrictlygreaterthanzeroanddeterminedbytheminimum
eigenvalue of J . In particular, we have
t+1
(cid:13) (cid:13)
(cid:13) (cid:13)I − λ t J t(cid:13) (cid:13) = 1− λ t ν min(J t+1), (174)
(cid:13) 1+λ (cid:13) 1+λ
t t
and the proposition is proved by substituting eq. (174) into eq. (172).
Proposition D.10 (Decay of ∥∆ ∥). Suppose that p = N(µ ,Σ ). Then for Algorithm 1 in
t ∗ ∗
the limit of infinite batch size (B→∞), the normalized errors in eq. (117) of the variational
covariance satisfy
1
∥∆ ∥ ≤ ∥ε ∥2+ ∥∆ ∥. (175)
t+1 t t
1+λ ν (J )
t min t
Proof. We start by applying the triangle inequality and the sandwiching inequality:
∥∆ ∥ = ∥J −I∥, (176)
t+1 t+1
≤ ∥J −K ∥ + ∥K −I∥, (177)
t+1 t+1 t+1
≤ ∥H −K ∥ + ∥K −I∥. (178)
t+1 t+1 t+1
Already from these inequalities we can see the main outlines of the result in eq. (175). Clearly, the
first term in eq. (178) must vanish when ∥ε ∥=0 because the auxiliary matrices H and K ,
t t+1 t+1
defined in eqs. (153–154), are equal when ε =0. Likewise, the second term in eq. (178) must
tBATCH AND MATCH BLACK-BOX VARIATIONAL INFERENCE 35
vanish when ∥∆ ∥=0, or equivalently when J =I, because in this case eq. (154) is also solved by
t t
K =I.
t+1
First we consider the left term in eq. (178). Recall from Lemma D.7 that the matrices H
t+1
and K share the same eigenvectors; thus the spectral norm ∥H −K ∥ is equal to the largest
t+1 t+1 t+1
gap between their corresponding eigenvalues. Also recall from eqs. (157–158) of Lemma D.7 that
these corresponding eigenvalues ν and ν are determined by the positive roots of the quadratic
H K
equations
(cid:18)
∥ε
∥2(cid:19)
λ ν + t ν2 +ν = (1+λ )ν , (179)
t J 1+λ H H t J
t
λ ν ν2 +ν = (1+λ )ν , (180)
t J K K t J
where ν is their (jointly) corresponding eigenvalue of J . Since these two equations agree when
J t
∥ε ∥2=0, it is clear that |ν −ν | → 0 as ∥ε ∥ → 0. More precisely, as we show in Lemma D.14
t H K t
of section D.8, it is the case that
|ν −ν | ≤ ∥ε ∥2. (181)
H K t
(Specifically, this is property (v) of Lemma D.14.) It follows in turn from this property that
∥H −K ∥ ≤ ∥ε ∥2. (182)
t+1 t+1 t
We have thus bounded the left term in eq. (178) by a quantity that, via Proposition D.9, is
decaying geometrically to zero with the number of iterations of the algorithm.
Next we focus on the right term in eq. (178). The spectral norm ∥K −I∥ is equal to the
t+1
largest gap between any eigenvalue of K and the value of 1 (i.e., the value of all eigenvalues of
t+1
I). Recall from eq. (158) of Lemma D.7 that each eigenvalue ν of J determines a corresponding
J t
eigenvalue ν of K via the positive root of the quadratic equation
K t+1
λ ν ν2 +ν = (1+λ )ν . (183)
t J K K t J
This correspondence has an important contracting property that eigenvalues of J not equal
t
to one are mapped to eigenvalues of K that are closer to one. In particular, as we show in
t+1
Lemma D.13 of section D.8, it is the case that
1
|ν −1| ≤ |ν −1|. (184)
K J
1+λ ν
t J
(Specifically, this is property (vii) of Lemma D.13.) It follows in turn from this property that
1
∥K −I∥ ≤ ∥J −I∥. (185)
t+1 t
1+λ ν (J )
t min t
Finally, the proposition is proved by substituting eq. (182) and eq. (185) into eq. (178).
The results of Proposition D.9 and Proposition D.10 could be used to further analyze the
convergence of Algorithm 1 when different levels of regularization λ are used at each iteration.
t
By specializing to a fixed level of regularization, however, we obtain the especially interpretable
results of eqs. (19–20) in the proof sketch of Theorem 3.1. To prove these results, we need one
further lemma.36 CAI, MODI, PILLAUD-VIVIEN, MARGOSSIAN, GOWER, BLEI, AND SAUL
Lemma D.11 (Bound on ν (J )). Suppose that p = N(µ ,Σ ) in Algorithm 1, and let
min t ∗ ∗
α>0 denote the minimum eigenvalue of the matrix
Σ−1
2Σ
Σ−1
2. Then in the limit of infinite
∗ 0 ∗
batch size (B→∞), and for any fixed level of regularization λ>0, we have for all t ≥ 0 that
(cid:18) (cid:19)
1+λ
ν (J ) ≥ min α, . (186)
min t 1+λ+∥ε ∥2
0
Proof. We prove the result by induction. Note that ν (J )=ν
(cid:16) Σ−1
2Σ
Σ−1 2(cid:17)
=α, so that
min 0 min ∗ 0 ∗
eq. (186) holds for t=0. Now assume that the result holds for some iteration t>0. Then
(cid:18) (cid:19)
1+λ
ν (J ) ≥ min ν (J ), , (187)
min t+1 min t 1+λ+∥ε ∥2
t
(cid:18) (cid:18) (cid:19) (cid:19)
1+λ 1+λ
≥ min min α, , , (188)
1+λ+∥ε ∥2 1+λ+∥ε ∥2
0 t
(cid:18) (cid:19)
1+λ
= min α, , (189)
1+λ+∥ε ∥2
0
wherethefirstinequalityisgivenbyeq.(164)ofLemmaD.8,thesecondinequalityfollowsfromthe
inductive hypothesis, and the final equality holds because ∥ε ∥<∥ε ∥ from Proposition D.9.
t 0
Note how the bound in eq. (186) depends on α and ∥ε ∥, both of which reflect the quality of
0
initialization. In particular, when α ≪ 1, the initial covariance is close to singular, and when ∥ε ∥
0
is large, the initial mean is a poor estimate. Both these qualities of initialization play a role in
the next result.
Corollary D.12 (Rates of decay for ∥ε ∥ and ∥∆ ∥). Suppose that p = N(µ ,Σ ) and let
t t ∗ ∗
α>0 denote the minimum eigenvalue of the matrix
Σ−1
2Σ
Σ−1
2. Also, for any fixed level of
∗ 0 ∗
regularization λ>0, define
(cid:18) (cid:19)
1+λ
β = min α, , (190)
1+λ+∥ε ∥2
0
λβ
δ = , (191)
1+λ
where β ∈ (0,1] measures the quality of initialization and δ ∈ (0,1) measures a rate of decay.
Then in the limit of infinite batch size (B→∞), the normalized errors in eqs. (116–117) satisfy
∥ε ∥2 ≤ (1−δ)2∥ε ∥2, (192)
t+1 t
∥∆ ∥ ≤ (1−δ)∥∆ ∥+∥ε ∥2. (193)
t+1 t t
Proof. The results follow from the previous ones in this section. In particular, from Proposi-
tion D.9 and the previous lemma, we see that
(cid:18) (cid:19) (cid:18) (cid:19)
λ λβ
∥ε ∥ ≤ 1− ν (J ) ∥ε ∥ ≤ 1− ∥ε ∥ = (1−δ)∥ε ∥. (194)
t+1 min t+1 t t t
1+λ 1+λBATCH AND MATCH BLACK-BOX VARIATIONAL INFERENCE 37
Likewise, from Proposition D.10 and the previous lemma, we see that
1
∥∆ ∥ ≤ ∥ε ∥2+ ∥∆ ∥, (195)
t+1 t t
1+λν (J )
min t
1
≤ ∥ε ∥2+ ∥∆ ∥, (196)
t t
1+λβ
(cid:18) (cid:19)
λβ
= ∥ε ∥2+ 1− ∥∆ ∥, (197)
t t
1+λβ
(cid:18) (cid:19)
λβ
≤ ∥ε ∥2+ 1− ∥∆ ∥, (198)
t t
1+λ
= ∥ε ∥2+(1−δ)∥∆ ∥. (199)
t t
D.7. Induction. FromthepreviouscorollarywecanatlastgiveasimpleproofofTheorem3.1.
It should also be clear that tighter bounds can be derived, and differing levels of regularization
accommodated, if we instead proceed from the more general bounds in Propositions D.9 and D.10.
Proof of Theorem 3.1. We start from eqs. (192–193) of Corollary D.12 and proceed by
induction. At iteration t=0, we see from these equations that
∥ε ∥ ≤ (1−δ)∥ε ∥, (200)
1 0
∥∆ ∥ ≤ (1−δ)∥∆ ∥+∥ε ∥2. (201)
1 0 0
The above agree with eqs. (17–18) at iteration t=0 and therefore establish the base case of the
induction. Next we assume the inductive hypothesis that eqs. (17–18) are true at some iteration
t−1. Then again, appealing to eqs. (192–193) of Corollary D.12, we see that
∥ε ∥ ≤ (1−δ)∥ε ∥, (202)
t t−1
≤ (1−δ)(1−δ)t−1∥ε ∥, (203)
0
= (1−δ)t∥ε ∥, (204)
0
∥∆ ∥ ≤ (1−δ)∥∆ ∥+∥ε ∥2, (205)
t t−1 t−1
(cid:104) (cid:105)
≤ (1−δ) (1−δ)t−1∥∆ ∥+(t−1)(1−δ)t−2∥ε ∥2 +(1−δ)2(t−1)∥ε ∥2, (206)
0 0 0
(cid:104) (cid:105)
= (1−δ)t∥∆ ∥+ (t−1)(1−δ)t−1+(1−δ)2t−2 ∥ε ∥2, (207)
0 0
(cid:104) (cid:105)
≤ (1−δ)t∥∆ ∥+ (t−1)(1−δ)t−1+(1−δ)t−1 ∥ε ∥2, (208)
0 0
= (1−δ)t∥∆ ∥+t(1−δ)t−1∥ε ∥2. (209)
0 0
This proves the theorem.
D.8. Supporting lemmas. In this section we collect a number of lemmas whose results
areneededthroughoutthisappendixbutwhoseproofsdigressfromthemainflowoftheargument.38 CAI, MODI, PILLAUD-VIVIEN, MARGOSSIAN, GOWER, BLEI, AND SAUL
1.5
1
0.5
0
0 0.5 1 1.5 2 2.5 3
Fig 5: Plot of the function f in eq. (211), as well as its fixed point and upper and lower bounds
from Lemma D.13, with λ=4 and ε2=1.
Lemma D.13. Let λ>0 and ε2 ≥ 0, and let f : R → R be the function defined implicitly
+ +
as follows: if ν>0 and ξ=f(ν), then ξ is equal to the positive root of the quadratic equation
(cid:18) ε2 (cid:19)
λ ν + ξ2+ξ−(1+λ)ν = 0. (210)
1+λ
Then f has the following properties:
(i) f is monotonically increasing on (0,∞).
(cid:113)
(ii) f(ν)< 1+λ for all ν>0.
λ
(iii) f has a unique fixed point ν∗ = f(ν∗).
(iv) f(ν)≥ν∗ for all ν≥ν∗.
(v) f(ν)>ν for all ν ∈ (0,ν∗).
(cid:16) (cid:17)
(vi) f(ν)≥min ν, 1+λ for all ν>0.
1+λ+ε2
(vii) If ε2=0, then |ν−1| ≥ (1+λν)|f(ν)−1| for all ν>0.
Before proving the lemma, we note that it is straightforward to solve the quadratic equation in
eq. (210). Doing so, we find
(cid:112)
−1+ 1+4λ(1+λ)ν2+4λε2ν
f(ν) = . (211)
(cid:16) (cid:17)
2λ ν + ε2
1+λ
In most aspects, this explicit form for f is less useful than the implicit one given in the statement
of the lemma. However, eq. (211) is useful for visualizing properties (i)-(vi), and Fig. 5 shows a
plot of f(ν) with λ=4 and ε2=1. We now prove the lemma.
Proof. Let ν>0. To prove property (i) that f is monotonically increasing, it suffices to showBATCH AND MATCH BLACK-BOX VARIATIONAL INFERENCE 39
f′(ν)>0. Differentiating eq. (210) with respect to ν, we find that
(cid:18) ε2 (cid:19)
λξ2+2λ ν + ξf′(ν)+f′(ν)−(1+λ) = 0, (212)
1+λ
where ξ=f(ν). To proceed, we re-arrange terms to isolate f′(ν) on the left side and use eq. (210)
to remove quadratic powers of ξ. In this way, we find:
(cid:20) (cid:18) ε2 (cid:19) (cid:21)
1+2λ ν + ξ f′(ν) = 1+λ−λξ2, (213)
1+λ
(1+λ)ν −ξ
= 1+λ− , (214)
ν + ε2
1+λ
ξ+ε2
= . (215)
ν + ε2
1+λ
Note that the term in brackets on the left side is strictly positive, as is the term on the right side.
It follows that f′(ν)>0, thus proving property (i). Moreover, since f is monotonically increasing,
it follows from eq. (211) that
(cid:114)
1+λ
f(ν) < lim f(ω) = , (216)
ω→∞ λ
thus proving property (ii). To prove property (iii), we solve for fixed points of f. Let ν∗>0 denote
a fixed point satisfying ν∗=f(ν∗). Then upon setting ν=ν∗ in eq. (210), we must find that ξ=ν∗
is a solution of the resulting equation, or
(cid:18) ε2 (cid:19)
λ ν∗+ ν∗2+ν∗−(1+λ)ν∗ = 0. (217)
1+λ
Eq. (217) has one root at zero, one negative root, and one positive root, but only the last of these
can be a fixed point of f, which is defined over R . This fixed point corresponds to the positive
+
root of the quadratic equation:
(cid:18) ε2 (cid:19)
ν∗+ ν∗ = 1. (218)
1+λ
This proves property (iii). Property (iv) follows easily from properties (i) and (iii): if ν≥ν∗, then
f(ν)≥f(ν∗)= ν∗, where the inequality holds because f is monotonically increasing and the
equality holds because ν∗ is a fixed point of f. To prove property (v), suppose that ν ∈ (0,ν∗).
Then from eq. (218), it follows that
(cid:18) ε2 (cid:19)
ν + ν < 1. (219)
1+λ
Now let ξ=f(ν). Then from eq. (210) and eq. (219), it follows that
0 = ν ·0 (220)
(cid:20) (cid:18) ε2 (cid:19) (cid:21)
= ν λ ν + ξ2+ξ−(1+λ)ν , (221)
1+λ
(cid:18) ε2 (cid:19)
= λν ν + ξ2 + νξ − (1+λ)ν2, (222)
1+λ
< λξ2 + νξ − (1+λ)ν2, (223)
= (ξ−ν)(ξ+(1+λ)ν). (224)40 CAI, MODI, PILLAUD-VIVIEN, MARGOSSIAN, GOWER, BLEI, AND SAUL
Since the right factor in eq. (224) is positive, the inequality as a whole can only be satisfied if
ξ>ν, or equivalently if f(ν)> ν, thus proving property (v). To prove property (vi), we observe
from eq. (218) that ν∗≤1, and from this upper bound on ν∗, we re-use eq. (218) to derive the
lower bound
1 1 1+λ
ν∗ = ≥ = . (225)
ν∗+ ε2 1+ ε2 1+λ+ε2
1+λ 1+λ
With this lower bound, we show next that property (vi) follows from properties (iv) and (v). In
particular, if ν ∈ (0,ν∗), then from property (v) we have f(ν)>ν; on the other hand, if ν≥ν∗,
then from property (iv) and the lower bound in eq. (225), we have f(ν)≥ν∗≥ 1+λ . But either
1+λ+ε2
ν ∈ (0,ν∗) or ν≥ν∗, and hence for all ν>0 we have
(cid:18) (cid:19)
1+λ
f(ν) ≥ min ν, , (226)
1+λ+ε2
which is exactly property (vi). Fig. (5) plots the lower and upper bounds on f from properties
(ii) and (vi), as well as the fixed point ν∗=f(ν∗). Property (vii) considers the special case when
ε2=0. In this case, we can also rewrite eq. (210) as
ν −1 = λνξ2+ξ−λν −1 = (1+λν +λνξ)(ξ−1), (227)
and taking the absolute values of both sides, we find that
|ν −1| = (1+λν +λνξ)|ξ−1| ≥ (1+λν)|ξ−1| (228)
for all ν > 0, thus proving property (vii). The meaning of this property becomes more evident
upon examining the function’s fixed point: note from eq. (218) that ν∗=1 when ε2=0. Thus
property (vii) can alternatively be written as
1
|f(ν)−ν∗| ≤ |ν −ν∗|, (229)
1+λν
showingthatthefunctionconvergestoitsfixedpointwhenitisappliedinaniterativefashion.
Lemma D.14. Let λ,ν>0, and let g : [0,∞)→R be the function defined implicitly as
+
follows: if ξ=g(ε2), then ξ is equal to the positive root of the quadratic equation
(cid:18) ε2 (cid:19)
λ ν + ξ2+ξ−(1+λ)ν = 0. (230)
1+λ
Then g has the following properties:
(i) g is monotonically decreasing on [0,∞).
(cid:113)
(ii) g(0) < 1+λ.
λ
(iii) g′(0) > −1.
(iv) g is convex on [0,∞).
(v) |g(ε2)−g(0)| ≤ ε2.
Before proving the lemma, we note that it is straightforward to solve the quadratic equation in
eq. (230). Doing so, we find
(cid:112)
−1+ 1+4λ(1+λ)ν2+4λε2ν
g(ε2) = . (231)
(cid:16) (cid:17)
2λ ν + ε2
1+λBATCH AND MATCH BLACK-BOX VARIATIONAL INFERENCE 41
1
0.8
0.6
0.4
0.2
0
0 20 40 60 80 100
Fig 6: Plot of the function g in Lemma D.14 and eq. (231) for several different values of λ and ν.
This explicit formula for g is not needed for the proof of the lemma. However, eq. (231) is useful
for visualizing properties (i)-(ii), and Fig. 6 shows several plots of g(ε2) for different values of λ
and ν. We now prove the lemma.
Proof. To prove property (i) that g is monotonically increasing, it suffices to show g′(ε2)<0.
Differentiating eq. (230) with respect to ε2, we find that
λ (cid:18) ε2 (cid:19)
ξ2+2λ ν + ξg′(ε2)+g′(ε2) = 0 (232)
1+λ 1+λ
where ξ=g(ε2), and solving for g′(ε), we find that
λξ2
g′(ε2) = − < 0, (233)
(1+λ)(1+2λνξ)+2λε2ξ
which proves property (i). To prove property (ii), let ξ =g(0) denote the positive root of eq. (230)
0
when ε2=0. Then this root satisfies
1+λ ξ 1+λ
ξ2 = − 0 < , (234)
0 λ λν λ
from which the result follows. Moreover, it follows from eqs. (233–234) that
λξ2 λξ2 λ 1+λ
g′(0) = − 0 > − 0 > − = −1, (235)
(1+λ)(1+2λνξ ) 1+λ 1+λ λ
0
thus proving property (iii). To prove property (iv) that g is convex, it suffices to show g′′(ε2) > 0.
Differentiating eq. (232) with respect to ε2, we find that
4λξ (cid:18) ε2 (cid:19) (cid:16) (cid:17)
g′(ε2)+2λ ν + ξg′′(ε2)+g′(ε2)2 +g′′(ε2) = 0. (236)
1+λ 1+λ42 CAI, MODI, PILLAUD-VIVIEN, MARGOSSIAN, GOWER, BLEI, AND SAUL
Algorithm 2 Implementation of ADVI
1: Input: Iterations T, batch size B, unnormalized target p˜, learning rate λ > 0, initial variational
t
mean µ ∈RD, initial variational covariance Σ ∈SD
0 0 ++
2: for t=0,...,T −1 do
3: Sample z ,...,z ∼q =N(µ ,Σ )
1 B t t t
4: Compute stochastic estimate of the (negative) ELBO
B
L(t) (z )=−(cid:88) log(p˜(z )−logq (z ))
ELBO 1:B b t b
b=1
5: Update variational parameters w :=(µ ,Σ ) with gradient
t t t
w =w −λ ∇ L(t) (z ) # Our implementation uses the ADAM update.
t+1 t t w ELBO 1:B
6: end for
7: Output: variational parameters µ ,Σ
T T
To proceed, we re-arrange terms to isolate g′′(ε2) on the left side and use eq. (232) to re-express
the term on the right. In this way, we find:
(cid:20) (cid:18) ε2 (cid:19) (cid:21) 4λξ (cid:18) ε2 (cid:19)
1+2λ ν + ξ g′′(ε2) = − g′(ε2)−2λ ν + g′(ε2)2, (237)
1+λ 1+λ 1+λ
g′(ε2) (cid:20) 4λξ2 (cid:18) ε2 (cid:19) (cid:21)
= − +2λ ν + ξg′(ε2) , (238)
ξ 1+λ 1+λ
g′(ε2) (cid:20) 4λξ2 λξ2 (cid:21)
= − − −g′(ε2) , (239)
ξ 1+λ 1+λ
g′(ε2) (cid:20) 3λξ2 (cid:21)
= − −g′(ε2) . (240)
ξ 1+λ
Note that the term in brackets on the left side is strictly positive, and because g is monotonically
decreasing, with g′(ε2) < 0, so is the term on the right. It follows that g′′(ε2) > 0, thus proving
property (iv). Finally, to prove property (v), we combine the results that g is monotonically
decreasing, that its derivative at zero is greater than -1, and that it is convex:
|g(ε2)−g(0)| = g(0)−g(ε2) ≤ g(0)−(g(0)+g′(0)ε2) = −g′(0)ε2 ≤ ε2. (241)
APPENDIX E: ADDITIONAL EXPERIMENTS AND DETAILS
E.1. Implementation of baselines. In Algorithm 2, we describe the version of ADVI
implemented in the experiments. In particular, we use ADAM as the optimizer for updating
the variational parameters. In Algorithm 3, we also describe the implementation of the GSM
algorithm (Modi et al., 2023).
E.2. Gaussian target. For all experiments, the algorithms were initialized with a random
initial mean µ and Σ = I. In Figure 8, we report the results for the reverse KL divergence. We
0 0
observe largely the same conclusions as with the forward KL divergence presented in Section 5.BATCH AND MATCH BLACK-BOX VARIATIONAL INFERENCE 43
Algorithm 3 Implementation of GSM
1: Input: Iterations T, batch size B, unnormalized target p˜, initial variational mean µ ∈RD, initial
0
variational covariance Σ ∈SD
0 ++
2: for t=0,...,T −1 do
3: Sample z ,...,z ∼q =N(µ ,Σ )
1 B t t t
4: for b=1,...,B do
5: Compute the score of the sample s =∇ log(p˜(z ))
b z b
6: Calculate intermediate quantities
ε =Σ s −µ +z , and solve ρ(1+ρ)=s⊤Σ s +(cid:2) (µ −z )⊤s (cid:3)2 forρ>0
b t b t b b t b t b b
7: Estimate the update for mean and covariance
δµ b = 1+1 ρ(cid:104) I− 1+ρ( +µt (− µtz −b) zs b⊤ b )⊤sb(cid:105) ε b
δΣ =(µ −z )(µ −z )⊤−(µ˜ −z )(µ˜ −z )⊤, where µ˜ =µ +δµ
b t b t b b b b b b t b
8: end for
9: Update variational mean and covariance
B B
(cid:88) (cid:88)
µ =µ + 1 δµ , Σ =Σ + 1 δΣ
t+1 t B b t+1 t B b
b=1 b=1
10: end for
11: Output: variational parameters µ ,Σ
T T
In addition, we evaluated BaM with a number of different schedules for the learning rates:
λ = B,BD, B , BD. We show one such example for D = 16 in Figure 7, where each figure
t t+1 t+1
represents a particular choice of λ , and where each line is the mean over 10 runs. For the constant
t
learning rate, the lines for B = 20,40 are on top of each other. Here we observe that the constant
learning rates perform the best for Gaussian targets.
E.3. Non-Gaussiantarget. Hereweagainconsiderthesinh-arcsinhdistributionwithD = 10,
where we vary the skew and tails. All algorithms were initialized with a random initial mean µ
0
and Σ = I. In Figure 9, we present several alternative plots showing the forward and reverse KL
0
divergence when varying the learning rate. We investigate the performance for different schedules
corresponding to λ
t
= BD, √B t+D 1, (B t+D 1), and we varied the batch size B = 2,5,10,20,40. Unlike
for Gaussian targets, we found that constant λ did not perform as well as those with a varying
t
schedule. In particular, we found that λ = BD typically converges faster than the other schedule.
t t+1
E.4. Posteriordb models. In Bayesian posterior inference applications, it is common to
measure the relative mean error and the relative standard deviation error (Welandawe et al.,
2022):
(cid:13) (cid:13) (cid:13) (cid:13)
relative mean error = (cid:13) (cid:13)µ−µˆ(cid:13) (cid:13) , relative SD error = (cid:13) (cid:13)σ−σˆ(cid:13) (cid:13) , (242)
(cid:13) σ (cid:13) (cid:13) σ (cid:13)
2 2
where µˆ,σˆ are computed from the variational distribution, and µ,σ are the posterior mean and
standard deviation. We estimated the posterior mean and standard deviation using the reference
samples included in posteriordb.
In the evaluation, all algorithms were initialized with a random initial mean µ and Σ = I.
0 0
The results for the relative mean error are presented in Section 5. In Figure 10, we present the44 CAI, MODI, PILLAUD-VIVIEN, MARGOSSIAN, GOWER, BLEI, AND SAUL
lambda_t = B * 1 lambda_t = BD * 1
102
102 B=2
B=10
106 B=20
B=40
100 101 102 103 104 105 100 101 102 103 104 105
# of gradient evaluations # of gradient evaluations
lambda_t = B * 1/t lambda_t = BD * 1/t
101
102 B B= =2
10
105 B=20
B=40
100 101 102 103 104 105 100 101 102 103 104 105
# of gradient evaluations # of gradient evaluations
Fig 7: Gaussian target, D = 16
D=4 D=16 D=64 D=256
105 BaM-5 GSM BaM-15 BaM-40 BaM-150
BaM-2 ADVI BaM-2 BaM-2 BaM-2 101
10 3
10 7
101 103 105 101 103 105 101 103 105 101 103 105
# gradient evaluations # gradient evaluations # gradient evaluations # gradient evaluations
Fig 8: Gaussian targets of increasing dimension. Solid curves indicate the mean over 10 runs
(transparent curves). Both ADVI and GSM use batch size of 2. The batch size for BaM is given
in the legend.
results for the relative SD error. Here we typically observe the same trends as for the mean, except
in the hierarchical example, where BaM learns the mean quickly but converges to a larger relative
SD error. However, the low error of GSM suggests that more robust tuning of the learning rate
may lead to better performance with BaM.
E.5. Deep learning model. We provide additional details for the experiment conducted
in Section 5.3. We first pre-train the neural network Ω(·,θˆ) (the “decoder”) using variational
expectation-maximization. That is, θˆmaximizes the marginal likelihood p({x }N |θ), where
n n=1
{x }N denotes the training set. The marginalization step is performed using an approximation
n n=1
q(z |x ) ≈ p(z |x ,θ),
n n n n
obtained with amortized variational inference. In details, we optimize the ELBO over the family
of factorized Gaussians and learn an inference neural network (the “encoder”) that maps x to
n
the parameters of q(z |x ). This procedure is standard for training a VAE (Kingma and Welling,
n n
2014; Rezende et al., 2014; Tomczak, 2022). For the decoder and the encoder, we use a convolution
network with 5 layers. The optimization is performed over 100 epochs, after which the ELBO
converges (Figure 11).
For the estimation of the posterior on a new observation, we draw an image x′ from the test
set. All VI algorithms are initialized at a standard Gaussian. For ADVI and BaM, we conduct
a pilot experiment of 100 iterations and select the learning rate that achieves the lowest MSE
for each batch size (B = 10,100,300). For ADVI, we consistently find the best learning rate to
LK
esreveR
LK
drawroF
LK
drawroFBATCH AND MATCH BLACK-BOX VARIATIONAL INFERENCE 45
s=0.2, lambda_t=BD s=1.0, lambda_t=BD s=1.8, lambda_t=BD tau=0.1, lambda_t=BD tau=0.9, lambda_t=BD tau=1.7, lambda_t=BD
B=2 B=20 102 B=2 B=20
103 B=5 B=40 B=5 B=40 B=10 103 B=10
102 108
101 103 105 101 103 105 101 103 105 101 103 105 101 103 105 101 103 105
# of gradient evaluations # of gradient evaluations # of gradient evaluations # of gradient evaluations # of gradient evaluations # of gradient evaluations
s=0.2, lambda_t=(BD)/t^0.5 s=1.0, lambda_t=(BD)/t^0.5 s=1.8, lambda_t=(BD)/t^0.5 tau=0.1, lambda_t=(BD)/t^0.5 tau=0.9, lambda_t=(BD)/t^0.5 tau=1.7, lambda_t=(BD)/t^0.5
B=2 B=20 102 B=2 B=20
103 B=5 B=40 B=5 B=40 B=10 103 B=10
102
108
101
101 103 105 101 103 105 101 103 105 101 103 105 101 103 105 101 103 105
# of gradient evaluations # of gradient evaluations # of gradient evaluations # of gradient evaluations # of gradient evaluations # of gradient evaluations
s=0.2, lambda_t=(BD)/t s=1.0, lambda_t=(BD)/t s=1.8, lambda_t=(BD)/t tau=0.1, lambda_t=(BD)/t tau=0.9, lambda_t=(BD)/t tau=1.7, lambda_t=(BD)/t
103 B B= =2 5 B B= =2 40 0 100 B B= =2 5 B B= =2 40 0
102
B=10 104 B=10
108
101
101 103 105 101 103 105 101 103 105 101 103 105 101 103 105 101 103 105
# of gradient evaluations # of gradient evaluations # of gradient evaluations # of gradient evaluations # of gradient evaluations # of gradient evaluations
(a) Forward KL: varying λ and skew (b) Forward KL: varying λ and tails
t t
s=0.2, lambda_t=BD s=1.0, lambda_t=BD s=1.8, lambda_t=BD tau=0.1, lambda_t=BD tau=0.9, lambda_t=BD tau=1.7, lambda_t=BD
B=2 B=20 B=2 B=20
103 B=5 B=40 102 B=5 B=40 B=10 106 B=10
102
1010
101 103 105 101 103 105 101 103 105 101 103 105 101 103 105 101 103 105
# of gradient evaluations # of gradient evaluations # of gradient evaluations # of gradient evaluations # of gradient evaluations # of gradient evaluations
s=0.2, lambda_t=(BD)/t^0.5 s=1.0, lambda_t=(BD)/t^0.5 s=1.8, lambda_t=(BD)/t^0.5 tau=0.1, lambda_t=(BD)/t^0.5 tau=0.9, lambda_t=(BD)/t^0.5 tau=1.7, lambda_t=(BD)/t^0.5
103 B B= =2 5 B B= =2 40 0 101 B B= =2 5 B B= =2 40 0 B=10 105 B=10
102
109
101
101 103 105 101 103 105 101 103 105 101 103 105 101 103 105 101 103 105
# of gradient evaluations # of gradient evaluations # of gradient evaluations # of gradient evaluations # of gradient evaluations # of gradient evaluations
s=0.2, lambda_t=(BD)/t s=1.0, lambda_t=(BD)/t s=1.8, lambda_t=(BD)/t tau=0.1, lambda_t=(BD)/t tau=0.9, lambda_t=(BD)/t tau=1.7, lambda_t=(BD)/t
B=2 B=20 100 B=2 B=20
103 B=5 B=40 B=5 B=40 B=10 104 B=10
102
108
101
101 103 105 101 103 105 101 103 105 101 103 105 101 103 105 101 103 105
# of gradient evaluations # of gradient evaluations # of gradient evaluations # of gradient evaluations # of gradient evaluations # of gradient evaluations
(c) Reverse KL: varying λ and skew (d) Reverse KL: varying λ and tails
t t
Fig 9: Non-Gaussian target, D = 10. Panels (a) and (b) show the forward KL, and panels (c) and
(d) show the reverse KL.
LK
drawroF
LK
drawroF
LK drawroF
LK
esreveR
LK esreveR
LK
esreveR
LK
drawroF
LK
drawroF
LK drawroF
LK
esreveR
LK esreveR
LK
esreveR46 CAI, MODI, PILLAUD-VIVIEN, MARGOSSIAN, GOWER, BLEI, AND SAUL
Nearly Gaussian Gaussian Process Hierarchical
102 102
2×100
101
101
100 BaM 100
GSM
101
100 ADVI
100 101 102 103 104 105 100 101 102 103 104 105 100 101 102 103 104 105
# of gradient evaluations # of gradient evaluations # of gradient evaluations
Fig 10: Posterior inference in Bayesian models. The curves denote the mean over 5 runs, and
shaded regions denote their standard error. Solid curves (B = 32) correspond to larger batch
sizes than the dashed curves (B = 8).
1e6 Negative ELBO
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
0 20 40 60 80 100
Epochs
Fig 11: ELBO for variational autoencoder over 100 epochs
be ℓ = 0.02 (after searching ℓ = 0.001,0.01,0.02,0.05). For BaM, we find that different learning
rates work better for different batch sizes:
• B = 10, λ = 0.1 selected from λ = 0.01,0.1,0.2,10.
• B = 100, λ = 50 selected from λ = 2,20,50,100,200.
• B = 300, λ = 7500 selected from λ = 1000,5000,7500,10000.
For B = 300, all candidate learning rates achieve the minimal MSE (since BaM converges in less
than 100 iterations), and so we pick the one that yields the fastest convergence.
Center for Computational Mathematics Department of Statistics
Flatiron Institute, New York, NY Department of Computer Science
Emails: dcai@flatironinstitute.org, Columbia University, New York, NY
cmodi@flatironinstitute.org, Emails: david.blei@columbia.edu
lpillaudvivien@flatironinstitute.org,
cmargossian@flatironinstitute.org,
rgower@flatironinstitute.org,
lsaul@flatironinstitute.org
rorre
DS
.leR