[
    {
        "title": "PALO: A Polyglot Large Multimodal Model for 5B People",
        "authors": "Muhammad MaazHanoona RasheedAbdelrahman ShakerSalman KhanHisham CholakalRao M. AnwerTim BaldwinMichael FelsbergFahad S. Khan",
        "links": "http://arxiv.org/abs/2402.14818v1",
        "entry_id": "http://arxiv.org/abs/2402.14818v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14818v1",
        "summary": "In pursuit of more inclusive Vision-Language Models (VLMs), this study\nintroduces a Large Multilingual Multimodal Model called \\textsc{Palo}.\n\\textsc{Palo} offers visual reasoning capabilities in 10 major languages,\nincluding English, Chinese, Hindi, Spanish, French, Arabic, Bengali, Russian,\nUrdu, and Japanese, that span a total of $\\sim$5B people (65\\% of the world\npopulation). Our approach involves a semi-automated translation approach to\nadapt the multimodal instruction dataset from English to the target languages\nusing a fine-tuned Large Language Model, thereby ensuring high linguistic\nfidelity while allowing scalability due to minimal manual effort. The\nincorporation of diverse instruction sets helps us boost overall performance\nacross multiple languages especially those that are underrepresented like\nHindi, Arabic, Bengali, and Urdu. The resulting models are trained across three\nscales (1.7B, 7B and 13B parameters) to show the generalization and scalability\nwhere we observe substantial improvements compared to strong baselines. We also\npropose the first multilingual multimodal benchmark for the forthcoming\napproaches to evaluate their vision-language reasoning capabilities across\nlanguages. Code: https://github.com/mbzuai-oryx/PALO.",
        "updated": "2024-02-22 18:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14818v1"
    },
    {
        "title": "Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking",
        "authors": "Nikhil PrakashTamar Rott ShahamTal HaklayYonatan BelinkovDavid Bau",
        "links": "http://arxiv.org/abs/2402.14811v1",
        "entry_id": "http://arxiv.org/abs/2402.14811v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14811v1",
        "summary": "Fine-tuning on generalized tasks such as instruction following, code\ngeneration, and mathematics has been shown to enhance language models'\nperformance on a range of tasks. Nevertheless, explanations of how such\nfine-tuning influences the internal computations in these models remain\nelusive. We study how fine-tuning affects the internal mechanisms implemented\nin language models. As a case study, we explore the property of entity\ntracking, a crucial facet of language comprehension, where models fine-tuned on\nmathematics have substantial performance gains. We identify the mechanism that\nenables entity tracking and show that (i) in both the original model and its\nfine-tuned versions primarily the same circuit implements entity tracking. In\nfact, the entity tracking circuit of the original model on the fine-tuned\nversions performs better than the full original model. (ii) The circuits of all\nthe models implement roughly the same functionality: Entity tracking is\nperformed by tracking the position of the correct entity in both the original\nmodel and its fine-tuned versions. (iii) Performance boost in the fine-tuned\nmodels is primarily attributed to its improved ability to handle the augmented\npositional information. To uncover these findings, we employ: Patch Patching,\nDCM, which automatically detects model components responsible for specific\nsemantics, and CMAP, a new approach for patching activations across models to\nreveal improved mechanisms. Our findings suggest that fine-tuning enhances,\nrather than fundamentally alters, the mechanistic operation of the model.",
        "updated": "2024-02-22 18:59:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14811v1"
    },
    {
        "title": "CriticBench: Benchmarking LLMs for Critique-Correct Reasoning",
        "authors": "Zicheng LinZhibin GouTian LiangRuilin LuoHaowei LiuYujiu Yang",
        "links": "http://arxiv.org/abs/2402.14809v1",
        "entry_id": "http://arxiv.org/abs/2402.14809v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14809v1",
        "summary": "The ability of Large Language Models (LLMs) to critique and refine their\nreasoning is crucial for their application in evaluation, feedback provision,\nand self-improvement. This paper introduces CriticBench, a comprehensive\nbenchmark designed to assess LLMs' abilities to critique and rectify their\nreasoning across a variety of tasks. CriticBench encompasses five reasoning\ndomains: mathematical, commonsense, symbolic, coding, and algorithmic. It\ncompiles 15 datasets and incorporates responses from three LLM families.\nUtilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in\ngeneration, critique, and correction reasoning, i.e., GQC reasoning. Our\nfindings reveal: (1) a linear relationship in GQC capabilities, with\ncritique-focused training markedly enhancing performance; (2) a task-dependent\nvariation in correction effectiveness, with logic-oriented tasks being more\namenable to correction; (3) GQC knowledge inconsistencies that decrease as\nmodel size increases; and (4) an intriguing inter-model critiquing dynamic,\nwhere stronger models are better at critiquing weaker ones, while weaker models\ncan surprisingly surpass stronger ones in their self-critique. We hope these\ninsights into the nuanced critique-correct reasoning of LLMs will foster\nfurther research in LLM critique and self-improvement.",
        "updated": "2024-02-22 18:59:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14809v1"
    },
    {
        "title": "RelayAttention for Efficient Large Language Model Serving with Long System Prompts",
        "authors": "Lei ZhuXinjiang WangWayne ZhangRynson W. H. Lau",
        "links": "http://arxiv.org/abs/2402.14808v1",
        "entry_id": "http://arxiv.org/abs/2402.14808v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14808v1",
        "summary": "Practical large language model (LLM) services may involve a long system\nprompt, which specifies the instructions, examples, and knowledge documents of\nthe task and is reused across numerous requests. However, the long system\nprompt causes throughput/latency bottlenecks as the cost of generating the next\ntoken grows w.r.t. the sequence length. This paper aims to improve the\nefficiency of LLM services that involve long system prompts. Our key\nobservation is that handling these system prompts requires heavily redundant\nmemory accesses in existing causal attention computation algorithms.\nSpecifically, for batched requests, the cached hidden states (i.e., key-value\npairs) of system prompts are transferred from off-chip DRAM to on-chip SRAM\nmultiple times, each corresponding to an individual request. To eliminate such\na redundancy, we propose RelayAttention, an attention algorithm that allows\nreading these hidden states from DRAM exactly once for a batch of input tokens.\nRelayAttention is a free lunch: it maintains the generation quality while\nrequiring no model retraining, as it is based on a mathematical reformulation\nof causal attention.",
        "updated": "2024-02-22 18:58:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14808v1"
    },
    {
        "title": "Identifying Multiple Personalities in Large Language Models with External Evaluation",
        "authors": "Xiaoyang SongYuta AdachiJessie FengMouwei LinLinhao YuFrank LiAkshat GuptaGopala AnumanchipalliSimerjot Kaur",
        "links": "http://arxiv.org/abs/2402.14805v1",
        "entry_id": "http://arxiv.org/abs/2402.14805v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14805v1",
        "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs.",
        "updated": "2024-02-22 18:57:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14805v1"
    }
]