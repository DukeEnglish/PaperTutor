[
    {
        "title": "PALO: A Polyglot Large Multimodal Model for 5B People",
        "authors": "Muhammad MaazHanoona RasheedAbdelrahman ShakerSalman KhanHisham CholakalRao M. AnwerTim BaldwinMichael FelsbergFahad S. Khan",
        "links": "http://arxiv.org/abs/2402.14818v1",
        "entry_id": "http://arxiv.org/abs/2402.14818v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14818v1",
        "summary": "In pursuit of more inclusive Vision-Language Models (VLMs), this study\nintroduces a Large Multilingual Multimodal Model called \\textsc{Palo}.\n\\textsc{Palo} offers visual reasoning capabilities in 10 major languages,\nincluding English, Chinese, Hindi, Spanish, French, Arabic, Bengali, Russian,\nUrdu, and Japanese, that span a total of $\\sim$5B people (65\\% of the world\npopulation). Our approach involves a semi-automated translation approach to\nadapt the multimodal instruction dataset from English to the target languages\nusing a fine-tuned Large Language Model, thereby ensuring high linguistic\nfidelity while allowing scalability due to minimal manual effort. The\nincorporation of diverse instruction sets helps us boost overall performance\nacross multiple languages especially those that are underrepresented like\nHindi, Arabic, Bengali, and Urdu. The resulting models are trained across three\nscales (1.7B, 7B and 13B parameters) to show the generalization and scalability\nwhere we observe substantial improvements compared to strong baselines. We also\npropose the first multilingual multimodal benchmark for the forthcoming\napproaches to evaluate their vision-language reasoning capabilities across\nlanguages. Code: https://github.com/mbzuai-oryx/PALO.",
        "updated": "2024-02-22 18:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14818v1"
    },
    {
        "title": "Cameras as Rays: Pose Estimation via Ray Diffusion",
        "authors": "Jason Y. ZhangAmy LinMoneish KumarTzu-Hsuan YangDeva RamananShubham Tulsiani",
        "links": "http://arxiv.org/abs/2402.14817v1",
        "entry_id": "http://arxiv.org/abs/2402.14817v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14817v1",
        "summary": "Estimating camera poses is a fundamental task for 3D reconstruction and\nremains challenging given sparse views (<10). In contrast to existing\napproaches that pursue top-down prediction of global parametrizations of camera\nextrinsics, we propose a distributed representation of camera pose that treats\na camera as a bundle of rays. This representation allows for a tight coupling\nwith spatial image features improving pose precision. We observe that this\nrepresentation is naturally suited for set-level level transformers and develop\na regression-based approach that maps image patches to corresponding rays. To\ncapture the inherent uncertainties in sparse-view pose inference, we adapt this\napproach to learn a denoising diffusion model which allows us to sample\nplausible modes while improving performance. Our proposed methods, both\nregression- and diffusion-based, demonstrate state-of-the-art performance on\ncamera pose estimation on CO3D while generalizing to unseen object categories\nand in-the-wild captures.",
        "updated": "2024-02-22 18:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14817v1"
    },
    {
        "title": "Demographic Bias of Expert-Level Vision-Language Foundation Models in Medical Imaging",
        "authors": "Yuzhe YangYujia LiuXin LiuAvanti GulhaneDomenico MastrodicasaWei WuEdward J WangDushyant W SahaniShwetak Patel",
        "links": "http://arxiv.org/abs/2402.14815v1",
        "entry_id": "http://arxiv.org/abs/2402.14815v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14815v1",
        "summary": "Advances in artificial intelligence (AI) have achieved expert-level\nperformance in medical imaging applications. Notably, self-supervised\nvision-language foundation models can detect a broad spectrum of pathologies\nwithout relying on explicit training annotations. However, it is crucial to\nensure that these AI models do not mirror or amplify human biases, thereby\ndisadvantaging historically marginalized groups such as females or Black\npatients. The manifestation of such biases could systematically delay essential\nmedical care for certain patient subgroups. In this study, we investigate the\nalgorithmic fairness of state-of-the-art vision-language foundation models in\nchest X-ray diagnosis across five globally-sourced datasets. Our findings\nreveal that compared to board-certified radiologists, these foundation models\nconsistently underdiagnose marginalized groups, with even higher rates seen in\nintersectional subgroups, such as Black female patients. Such demographic\nbiases present over a wide range of pathologies and demographic attributes.\nFurther analysis of the model embedding uncovers its significant encoding of\ndemographic information. Deploying AI systems with these biases in medical\nimaging can intensify pre-existing care disparities, posing potential\nchallenges to equitable healthcare access and raising ethical questions about\ntheir clinical application.",
        "updated": "2024-02-22 18:59:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14815v1"
    },
    {
        "title": "WeakSAM: Segment Anything Meets Weakly-supervised Instance-level Recognition",
        "authors": "Lianghui ZhuJunwei ZhouYan LiuXin HaoWenyu LiuXinggang Wang",
        "links": "http://arxiv.org/abs/2402.14812v1",
        "entry_id": "http://arxiv.org/abs/2402.14812v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14812v1",
        "summary": "Weakly supervised visual recognition using inexact supervision is a critical\nyet challenging learning problem. It significantly reduces human labeling costs\nand traditionally relies on multi-instance learning and pseudo-labeling. This\npaper introduces WeakSAM and solves the weakly-supervised object detection\n(WSOD) and segmentation by utilizing the pre-learned world knowledge contained\nin a vision foundation model, i.e., the Segment Anything Model (SAM). WeakSAM\naddresses two critical limitations in traditional WSOD retraining, i.e., pseudo\nground truth (PGT) incompleteness and noisy PGT instances, through adaptive PGT\ngeneration and Region of Interest (RoI) drop regularization. It also addresses\nthe SAM's problems of requiring prompts and category unawareness for automatic\nobject detection and segmentation. Our results indicate that WeakSAM\nsignificantly surpasses previous state-of-the-art methods in WSOD and WSIS\nbenchmarks with large margins, i.e. average improvements of 7.4% and 8.5%,\nrespectively. The code is available at \\url{https://github.com/hustvl/WeakSAM}.",
        "updated": "2024-02-22 18:59:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14812v1"
    },
    {
        "title": "GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion",
        "authors": "Xueyi LiuLi Yi",
        "links": "http://arxiv.org/abs/2402.14810v1",
        "entry_id": "http://arxiv.org/abs/2402.14810v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14810v1",
        "summary": "In this work, we tackle the challenging problem of denoising hand-object\ninteractions (HOI). Given an erroneous interaction sequence, the objective is\nto refine the incorrect hand trajectory to remove interaction artifacts for a\nperceptually realistic sequence. This challenge involves intricate interaction\nnoise, including unnatural hand poses and incorrect hand-object relations,\nalongside the necessity for robust generalization to new interactions and\ndiverse noise patterns. We tackle those challenges through a novel approach,\nGeneOH Diffusion, incorporating two key designs: an innovative contact-centric\nHOI representation named GeneOH and a new domain-generalizable denoising\nscheme. The contact-centric representation GeneOH informatively parameterizes\nthe HOI process, facilitating enhanced generalization across various HOI\nscenarios. The new denoising scheme consists of a canonical denoising model\ntrained to project noisy data samples from a whitened noise space to a clean\ndata manifold and a \"denoising via diffusion\" strategy which can handle input\ntrajectories with various noise patterns by first diffusing them to align with\nthe whitened noise space and cleaning via the canonical denoiser. Extensive\nexperiments on four benchmarks with significant domain variations demonstrate\nthe superior effectiveness of our method. GeneOH Diffusion also shows promise\nfor various downstream applications. Project website:\nhttps://meowuu7.github.io/GeneOH-Diffusion/.",
        "updated": "2024-02-22 18:59:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14810v1"
    }
]