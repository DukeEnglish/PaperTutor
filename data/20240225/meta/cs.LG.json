[
    {
        "title": "Cameras as Rays: Pose Estimation via Ray Diffusion",
        "authors": "Jason Y. ZhangAmy LinMoneish KumarTzu-Hsuan YangDeva RamananShubham Tulsiani",
        "links": "http://arxiv.org/abs/2402.14817v1",
        "entry_id": "http://arxiv.org/abs/2402.14817v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14817v1",
        "summary": "Estimating camera poses is a fundamental task for 3D reconstruction and\nremains challenging given sparse views (<10). In contrast to existing\napproaches that pursue top-down prediction of global parametrizations of camera\nextrinsics, we propose a distributed representation of camera pose that treats\na camera as a bundle of rays. This representation allows for a tight coupling\nwith spatial image features improving pose precision. We observe that this\nrepresentation is naturally suited for set-level level transformers and develop\na regression-based approach that maps image patches to corresponding rays. To\ncapture the inherent uncertainties in sparse-view pose inference, we adapt this\napproach to learn a denoising diffusion model which allows us to sample\nplausible modes while improving performance. Our proposed methods, both\nregression- and diffusion-based, demonstrate state-of-the-art performance on\ncamera pose estimation on CO3D while generalizing to unseen object categories\nand in-the-wild captures.",
        "updated": "2024-02-22 18:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14817v1"
    },
    {
        "title": "Demographic Bias of Expert-Level Vision-Language Foundation Models in Medical Imaging",
        "authors": "Yuzhe YangYujia LiuXin LiuAvanti GulhaneDomenico MastrodicasaWei WuEdward J WangDushyant W SahaniShwetak Patel",
        "links": "http://arxiv.org/abs/2402.14815v1",
        "entry_id": "http://arxiv.org/abs/2402.14815v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14815v1",
        "summary": "Advances in artificial intelligence (AI) have achieved expert-level\nperformance in medical imaging applications. Notably, self-supervised\nvision-language foundation models can detect a broad spectrum of pathologies\nwithout relying on explicit training annotations. However, it is crucial to\nensure that these AI models do not mirror or amplify human biases, thereby\ndisadvantaging historically marginalized groups such as females or Black\npatients. The manifestation of such biases could systematically delay essential\nmedical care for certain patient subgroups. In this study, we investigate the\nalgorithmic fairness of state-of-the-art vision-language foundation models in\nchest X-ray diagnosis across five globally-sourced datasets. Our findings\nreveal that compared to board-certified radiologists, these foundation models\nconsistently underdiagnose marginalized groups, with even higher rates seen in\nintersectional subgroups, such as Black female patients. Such demographic\nbiases present over a wide range of pathologies and demographic attributes.\nFurther analysis of the model embedding uncovers its significant encoding of\ndemographic information. Deploying AI systems with these biases in medical\nimaging can intensify pre-existing care disparities, posing potential\nchallenges to equitable healthcare access and raising ethical questions about\ntheir clinical application.",
        "updated": "2024-02-22 18:59:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14815v1"
    },
    {
        "title": "Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking",
        "authors": "Nikhil PrakashTamar Rott ShahamTal HaklayYonatan BelinkovDavid Bau",
        "links": "http://arxiv.org/abs/2402.14811v1",
        "entry_id": "http://arxiv.org/abs/2402.14811v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14811v1",
        "summary": "Fine-tuning on generalized tasks such as instruction following, code\ngeneration, and mathematics has been shown to enhance language models'\nperformance on a range of tasks. Nevertheless, explanations of how such\nfine-tuning influences the internal computations in these models remain\nelusive. We study how fine-tuning affects the internal mechanisms implemented\nin language models. As a case study, we explore the property of entity\ntracking, a crucial facet of language comprehension, where models fine-tuned on\nmathematics have substantial performance gains. We identify the mechanism that\nenables entity tracking and show that (i) in both the original model and its\nfine-tuned versions primarily the same circuit implements entity tracking. In\nfact, the entity tracking circuit of the original model on the fine-tuned\nversions performs better than the full original model. (ii) The circuits of all\nthe models implement roughly the same functionality: Entity tracking is\nperformed by tracking the position of the correct entity in both the original\nmodel and its fine-tuned versions. (iii) Performance boost in the fine-tuned\nmodels is primarily attributed to its improved ability to handle the augmented\npositional information. To uncover these findings, we employ: Patch Patching,\nDCM, which automatically detects model components responsible for specific\nsemantics, and CMAP, a new approach for patching activations across models to\nreveal improved mechanisms. Our findings suggest that fine-tuning enhances,\nrather than fundamentally alters, the mechanistic operation of the model.",
        "updated": "2024-02-22 18:59:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14811v1"
    },
    {
        "title": "GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion",
        "authors": "Xueyi LiuLi Yi",
        "links": "http://arxiv.org/abs/2402.14810v1",
        "entry_id": "http://arxiv.org/abs/2402.14810v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14810v1",
        "summary": "In this work, we tackle the challenging problem of denoising hand-object\ninteractions (HOI). Given an erroneous interaction sequence, the objective is\nto refine the incorrect hand trajectory to remove interaction artifacts for a\nperceptually realistic sequence. This challenge involves intricate interaction\nnoise, including unnatural hand poses and incorrect hand-object relations,\nalongside the necessity for robust generalization to new interactions and\ndiverse noise patterns. We tackle those challenges through a novel approach,\nGeneOH Diffusion, incorporating two key designs: an innovative contact-centric\nHOI representation named GeneOH and a new domain-generalizable denoising\nscheme. The contact-centric representation GeneOH informatively parameterizes\nthe HOI process, facilitating enhanced generalization across various HOI\nscenarios. The new denoising scheme consists of a canonical denoising model\ntrained to project noisy data samples from a whitened noise space to a clean\ndata manifold and a \"denoising via diffusion\" strategy which can handle input\ntrajectories with various noise patterns by first diffusing them to align with\nthe whitened noise space and cleaning via the canonical denoiser. Extensive\nexperiments on four benchmarks with significant domain variations demonstrate\nthe superior effectiveness of our method. GeneOH Diffusion also shows promise\nfor various downstream applications. Project website:\nhttps://meowuu7.github.io/GeneOH-Diffusion/.",
        "updated": "2024-02-22 18:59:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14810v1"
    },
    {
        "title": "CriticBench: Benchmarking LLMs for Critique-Correct Reasoning",
        "authors": "Zicheng LinZhibin GouTian LiangRuilin LuoHaowei LiuYujiu Yang",
        "links": "http://arxiv.org/abs/2402.14809v1",
        "entry_id": "http://arxiv.org/abs/2402.14809v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14809v1",
        "summary": "The ability of Large Language Models (LLMs) to critique and refine their\nreasoning is crucial for their application in evaluation, feedback provision,\nand self-improvement. This paper introduces CriticBench, a comprehensive\nbenchmark designed to assess LLMs' abilities to critique and rectify their\nreasoning across a variety of tasks. CriticBench encompasses five reasoning\ndomains: mathematical, commonsense, symbolic, coding, and algorithmic. It\ncompiles 15 datasets and incorporates responses from three LLM families.\nUtilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in\ngeneration, critique, and correction reasoning, i.e., GQC reasoning. Our\nfindings reveal: (1) a linear relationship in GQC capabilities, with\ncritique-focused training markedly enhancing performance; (2) a task-dependent\nvariation in correction effectiveness, with logic-oriented tasks being more\namenable to correction; (3) GQC knowledge inconsistencies that decrease as\nmodel size increases; and (4) an intriguing inter-model critiquing dynamic,\nwhere stronger models are better at critiquing weaker ones, while weaker models\ncan surprisingly surpass stronger ones in their self-critique. We hope these\ninsights into the nuanced critique-correct reasoning of LLMs will foster\nfurther research in LLM critique and self-improvement.",
        "updated": "2024-02-22 18:59:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14809v1"
    }
]