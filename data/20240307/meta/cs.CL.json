[
    {
        "title": "The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning",
        "authors": "Nathaniel LiAlexander PanAnjali GopalSummer YueDaniel BerriosAlice GattiJustin D. LiAnn-Kathrin DombrowskiShashwat GoelLong PhanGabriel MukobiNathan Helm-BurgerRassin LababidiLennart JustenAndrew B. LiuMichael ChenIsabelle BarrassOliver ZhangXiaoyuan ZhuRishub TamirisaBhrugu BharathiAdam KhojaAriel Herbert-VossCort B. BreuerAndy ZouMantas MazeikaZifan WangPalash OswalWeiran LiuAdam A. HuntJustin Tienken-HarderKevin Y. ShihKemper TalleyJohn GuanRussell KaplanIan StenekerDavid CampbellBrad JokubaitisAlex LevinsonJean WangWilliam QianKallol Krishna KarmakarSteven BasartStephen FitzMindy LevinePonnurangam KumaraguruUday TupakulaVijay VaradharajanYan ShoshitaishviliJimmy BaKevin M. EsveltAlexandr WangDan Hendrycks",
        "links": "http://arxiv.org/abs/2403.03218v1",
        "entry_id": "http://arxiv.org/abs/2403.03218v1",
        "pdf_url": "http://arxiv.org/pdf/2403.03218v1",
        "summary": "The White House Executive Order on Artificial Intelligence highlights the\nrisks of large language models (LLMs) empowering malicious actors in developing\nbiological, cyber, and chemical weapons. To measure these risks of malicious\nuse, government institutions and major AI labs are developing evaluations for\nhazardous capabilities in LLMs. However, current evaluations are private,\npreventing further research into mitigating risk. Furthermore, they focus on\nonly a few, highly specific pathways for malicious use. To fill these gaps, we\npublicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a\ndataset of 4,157 multiple-choice questions that serve as a proxy measurement of\nhazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP\nwas developed by a consortium of academics and technical consultants, and was\nstringently filtered to eliminate sensitive information prior to public\nrelease. WMDP serves two roles: first, as an evaluation for hazardous knowledge\nin LLMs, and second, as a benchmark for unlearning methods to remove such\nhazardous knowledge. To guide progress on unlearning, we develop CUT, a\nstate-of-the-art unlearning method based on controlling model representations.\nCUT reduces model performance on WMDP while maintaining general capabilities in\nareas such as biology and computer science, suggesting that unlearning may be a\nconcrete path towards reducing malicious use from LLMs. We release our\nbenchmark and code publicly at https://wmdp.ai",
        "updated": "2024-03-05 18:59:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.03218v1"
    },
    {
        "title": "MAGID: An Automated Pipeline for Generating Synthetic Multi-modal Datasets",
        "authors": "Hossein AboutalebiHwanjun SongYusheng XieArshit GuptaJustin SunHang SuIgor ShalyminovNikolaos PappasSiffi SinghSaab Mansour",
        "links": "http://arxiv.org/abs/2403.03194v1",
        "entry_id": "http://arxiv.org/abs/2403.03194v1",
        "pdf_url": "http://arxiv.org/pdf/2403.03194v1",
        "summary": "Development of multimodal interactive systems is hindered by the lack of\nrich, multimodal (text, images) conversational data, which is needed in large\nquantities for LLMs. Previous approaches augment textual dialogues with\nretrieved images, posing privacy, diversity, and quality constraints. In this\nwork, we introduce \\textbf{M}ultimodal \\textbf{A}ugmented \\textbf{G}enerative\n\\textbf{I}mages \\textbf{D}ialogues (MAGID), a framework to augment text-only\ndialogues with diverse and high-quality images. Subsequently, a diffusion model\nis applied to craft corresponding images, ensuring alignment with the\nidentified text. Finally, MAGID incorporates an innovative feedback loop\nbetween an image description generation module (textual LLM) and image quality\nmodules (addressing aesthetics, image-text matching, and safety), that work in\ntandem to generate high-quality and multi-modal dialogues. We compare MAGID to\nother SOTA baselines on three dialogue datasets, using automated and human\nevaluation. Our results show that MAGID is comparable to or better than\nbaselines, with significant improvements in human evaluation, especially\nagainst retrieval baselines where the image database is small.",
        "updated": "2024-03-05 18:31:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.03194v1"
    },
    {
        "title": "Reliable, Adaptable, and Attributable Language Models with Retrieval",
        "authors": "Akari AsaiZexuan ZhongDanqi ChenPang Wei KohLuke ZettlemoyerHannaneh HajishirziWen-tau Yih",
        "links": "http://arxiv.org/abs/2403.03187v1",
        "entry_id": "http://arxiv.org/abs/2403.03187v1",
        "pdf_url": "http://arxiv.org/pdf/2403.03187v1",
        "summary": "Parametric language models (LMs), which are trained on vast amounts of web\ndata, exhibit remarkable flexibility and capability. However, they still face\npractical challenges such as hallucinations, difficulty in adapting to new data\ndistributions, and a lack of verifiability. In this position paper, we advocate\nfor retrieval-augmented LMs to replace parametric LMs as the next generation of\nLMs. By incorporating large-scale datastores during inference,\nretrieval-augmented LMs can be more reliable, adaptable, and attributable.\nDespite their potential, retrieval-augmented LMs have yet to be widely adopted\ndue to several obstacles: specifically, current retrieval-augmented LMs\nstruggle to leverage helpful text beyond knowledge-intensive tasks such as\nquestion answering, have limited interaction between retrieval and LM\ncomponents, and lack the infrastructure for scaling. To address these, we\npropose a roadmap for developing general-purpose retrieval-augmented LMs. This\ninvolves a reconsideration of datastores and retrievers, the exploration of\npipelines with improved retriever-LM interaction, and significant investment in\ninfrastructure for efficient training and inference.",
        "updated": "2024-03-05 18:22:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.03187v1"
    },
    {
        "title": "SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection",
        "authors": "Peng QiZehong YanWynne HsuMong Li Lee",
        "links": "http://arxiv.org/abs/2403.03170v1",
        "entry_id": "http://arxiv.org/abs/2403.03170v1",
        "pdf_url": "http://arxiv.org/pdf/2403.03170v1",
        "summary": "Misinformation is a prevalent societal issue due to its potential high risks.\nOut-of-context (OOC) misinformation, where authentic images are repurposed with\nfalse text, is one of the easiest and most effective ways to mislead audiences.\nCurrent methods focus on assessing image-text consistency but lack convincing\nexplanations for their judgments, which is essential for debunking\nmisinformation. While Multimodal Large Language Models (MLLMs) have rich\nknowledge and innate capability for visual reasoning and explanation\ngeneration, they still lack sophistication in understanding and discovering the\nsubtle crossmodal differences. In this paper, we introduce SNIFFER, a novel\nmultimodal large language model specifically engineered for OOC misinformation\ndetection and explanation. SNIFFER employs two-stage instruction tuning on\nInstructBLIP. The first stage refines the model's concept alignment of generic\nobjects with news-domain entities and the second stage leverages language-only\nGPT-4 generated OOC-specific instruction data to fine-tune the model's\ndiscriminatory powers. Enhanced by external tools and retrieval, SNIFFER not\nonly detects inconsistencies between text and image but also utilizes external\nknowledge for contextual verification. Our experiments show that SNIFFER\nsurpasses the original MLLM by over 40% and outperforms state-of-the-art\nmethods in detection accuracy. SNIFFER also provides accurate and persuasive\nexplanations as validated by quantitative and human evaluations.",
        "updated": "2024-03-05 18:04:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.03170v1"
    },
    {
        "title": "PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset",
        "authors": "Arda UzunoğluAbdalfatah Rashid SafaGözde Gül Şahin",
        "links": "http://arxiv.org/abs/2403.03167v1",
        "entry_id": "http://arxiv.org/abs/2403.03167v1",
        "pdf_url": "http://arxiv.org/pdf/2403.03167v1",
        "summary": "Recently, there has been growing interest within the community regarding\nwhether large language models are capable of planning or executing plans.\nHowever, most prior studies use LLMs to generate high-level plans for\nsimplified scenarios lacking linguistic complexity and domain diversity,\nlimiting analysis of their planning abilities. These setups constrain\nevaluation methods (e.g., predefined action space), architectural choices\n(e.g., only generative models), and overlook the linguistic nuances essential\nfor realistic analysis. To tackle this, we present PARADISE, an abductive\nreasoning task using Q\\&A format on practical procedural text sourced from\nwikiHow. It involves warning and tip inference tasks directly associated with\ngoals, excluding intermediary steps, with the aim of testing the ability of the\nmodels to infer implicit knowledge of the plan solely from the given goal. Our\nexperiments, utilizing fine-tuned language models and zero-shot prompting,\nreveal the effectiveness of task-specific small models over large language\nmodels in most scenarios. Despite advancements, all models fall short of human\nperformance. Notably, our analysis uncovers intriguing insights, such as\nvariations in model behavior with dropped keywords, struggles of BERT-family\nand GPT-4 with physical and abstract goals, and the proposed tasks offering\nvaluable prior knowledge for other unseen procedural tasks. The PARADISE\ndataset and associated resources are publicly available for further research\nexploration with https://github.com/GGLAB-KU/paradise.",
        "updated": "2024-03-05 18:01:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.03167v1"
    }
]