[
    {
        "title": "LC-Tsalis-INF: Generalized Best-of-Both-Worlds Linear Contextual Bandits",
        "authors": "Masahiro KatoShinji Ito",
        "links": "http://arxiv.org/abs/2403.03219v1",
        "entry_id": "http://arxiv.org/abs/2403.03219v1",
        "pdf_url": "http://arxiv.org/pdf/2403.03219v1",
        "summary": "This study considers the linear contextual bandit problem with independent\nand identically distributed (i.i.d.) contexts. In this problem, existing\nstudies have proposed Best-of-Both-Worlds (BoBW) algorithms whose regrets\nsatisfy $O(\\log^2(T))$ for the number of rounds $T$ in a stochastic regime with\na suboptimality gap lower-bounded by a positive constant, while satisfying\n$O(\\sqrt{T})$ in an adversarial regime. However, the dependency on $T$ has room\nfor improvement, and the suboptimality-gap assumption can be relaxed. For this\nissue, this study proposes an algorithm whose regret satisfies $O(\\log(T))$ in\nthe setting when the suboptimality gap is lower-bounded. Furthermore, we\nintroduce a margin condition, a milder assumption on the suboptimality gap.\nThat condition characterizes the problem difficulty linked to the suboptimality\ngap using a parameter $\\beta \\in (0, \\infty]$. We then show that the\nalgorithm's regret satisfies\n$O\\left(\\left\\{\\log(T)\\right\\}^{\\frac{1+\\beta}{2+\\beta}}T^{\\frac{1}{2+\\beta}}\\right)$.\nHere, $\\beta= \\infty$ corresponds to the case in the existing studies where a\nlower bound exists in the suboptimality gap, and our regret satisfies\n$O(\\log(T))$ in that case. Our proposed algorithm is based on the\nFollow-The-Regularized-Leader with the Tsallis entropy and referred to as the\n$\\alpha$-Linear-Contextual (LC)-Tsallis-INF.",
        "updated": "2024-03-05 18:59:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.03219v1"
    },
    {
        "title": "The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning",
        "authors": "Nathaniel LiAlexander PanAnjali GopalSummer YueDaniel BerriosAlice GattiJustin D. LiAnn-Kathrin DombrowskiShashwat GoelLong PhanGabriel MukobiNathan Helm-BurgerRassin LababidiLennart JustenAndrew B. LiuMichael ChenIsabelle BarrassOliver ZhangXiaoyuan ZhuRishub TamirisaBhrugu BharathiAdam KhojaAriel Herbert-VossCort B. BreuerAndy ZouMantas MazeikaZifan WangPalash OswalWeiran LiuAdam A. HuntJustin Tienken-HarderKevin Y. ShihKemper TalleyJohn GuanRussell KaplanIan StenekerDavid CampbellBrad JokubaitisAlex LevinsonJean WangWilliam QianKallol Krishna KarmakarSteven BasartStephen FitzMindy LevinePonnurangam KumaraguruUday TupakulaVijay VaradharajanYan ShoshitaishviliJimmy BaKevin M. EsveltAlexandr WangDan Hendrycks",
        "links": "http://arxiv.org/abs/2403.03218v1",
        "entry_id": "http://arxiv.org/abs/2403.03218v1",
        "pdf_url": "http://arxiv.org/pdf/2403.03218v1",
        "summary": "The White House Executive Order on Artificial Intelligence highlights the\nrisks of large language models (LLMs) empowering malicious actors in developing\nbiological, cyber, and chemical weapons. To measure these risks of malicious\nuse, government institutions and major AI labs are developing evaluations for\nhazardous capabilities in LLMs. However, current evaluations are private,\npreventing further research into mitigating risk. Furthermore, they focus on\nonly a few, highly specific pathways for malicious use. To fill these gaps, we\npublicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a\ndataset of 4,157 multiple-choice questions that serve as a proxy measurement of\nhazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP\nwas developed by a consortium of academics and technical consultants, and was\nstringently filtered to eliminate sensitive information prior to public\nrelease. WMDP serves two roles: first, as an evaluation for hazardous knowledge\nin LLMs, and second, as a benchmark for unlearning methods to remove such\nhazardous knowledge. To guide progress on unlearning, we develop CUT, a\nstate-of-the-art unlearning method based on controlling model representations.\nCUT reduces model performance on WMDP while maintaining general capabilities in\nareas such as biology and computer science, suggesting that unlearning may be a\nconcrete path towards reducing malicious use from LLMs. We release our\nbenchmark and code publicly at https://wmdp.ai",
        "updated": "2024-03-05 18:59:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.03218v1"
    },
    {
        "title": "Active Statistical Inference",
        "authors": "Tijana ZrnicEmmanuel J. Candès",
        "links": "http://arxiv.org/abs/2403.03208v1",
        "entry_id": "http://arxiv.org/abs/2403.03208v1",
        "pdf_url": "http://arxiv.org/pdf/2403.03208v1",
        "summary": "Inspired by the concept of active learning, we propose active\ninference$\\unicode{x2013}$a methodology for statistical inference with\nmachine-learning-assisted data collection. Assuming a budget on the number of\nlabels that can be collected, the methodology uses a machine learning model to\nidentify which data points would be most beneficial to label, thus effectively\nutilizing the budget. It operates on a simple yet powerful intuition:\nprioritize the collection of labels for data points where the model exhibits\nuncertainty, and rely on the model's predictions where it is confident. Active\ninference constructs provably valid confidence intervals and hypothesis tests\nwhile leveraging any black-box machine learning model and handling any data\ndistribution. The key point is that it achieves the same level of accuracy with\nfar fewer samples than existing baselines relying on non-adaptively-collected\ndata. This means that for the same number of collected samples, active\ninference enables smaller confidence intervals and more powerful p-values. We\nevaluate active inference on datasets from public opinion research, census\nanalysis, and proteomics.",
        "updated": "2024-03-05 18:46:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.03208v1"
    },
    {
        "title": "Reliable, Adaptable, and Attributable Language Models with Retrieval",
        "authors": "Akari AsaiZexuan ZhongDanqi ChenPang Wei KohLuke ZettlemoyerHannaneh HajishirziWen-tau Yih",
        "links": "http://arxiv.org/abs/2403.03187v1",
        "entry_id": "http://arxiv.org/abs/2403.03187v1",
        "pdf_url": "http://arxiv.org/pdf/2403.03187v1",
        "summary": "Parametric language models (LMs), which are trained on vast amounts of web\ndata, exhibit remarkable flexibility and capability. However, they still face\npractical challenges such as hallucinations, difficulty in adapting to new data\ndistributions, and a lack of verifiability. In this position paper, we advocate\nfor retrieval-augmented LMs to replace parametric LMs as the next generation of\nLMs. By incorporating large-scale datastores during inference,\nretrieval-augmented LMs can be more reliable, adaptable, and attributable.\nDespite their potential, retrieval-augmented LMs have yet to be widely adopted\ndue to several obstacles: specifically, current retrieval-augmented LMs\nstruggle to leverage helpful text beyond knowledge-intensive tasks such as\nquestion answering, have limited interaction between retrieval and LM\ncomponents, and lack the infrastructure for scaling. To address these, we\npropose a roadmap for developing general-purpose retrieval-augmented LMs. This\ninvolves a reconsideration of datastores and retrievers, the exploration of\npipelines with improved retriever-LM interaction, and significant investment in\ninfrastructure for efficient training and inference.",
        "updated": "2024-03-05 18:22:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.03187v1"
    },
    {
        "title": "Preventing Reward Hacking with Occupancy Measure Regularization",
        "authors": "Cassidy LaidlawShivam SinghalAnca Dragan",
        "links": "http://arxiv.org/abs/2403.03185v1",
        "entry_id": "http://arxiv.org/abs/2403.03185v1",
        "pdf_url": "http://arxiv.org/pdf/2403.03185v1",
        "summary": "Reward hacking occurs when an agent performs very well with respect to a\n\"proxy\" reward function (which may be hand-specified or learned), but poorly\nwith respect to the unknown true reward. Since ensuring good alignment between\nthe proxy and true reward is extremely difficult, one approach to prevent\nreward hacking is optimizing the proxy conservatively. Prior work has\nparticularly focused on enforcing the learned policy to behave similarly to a\n\"safe\" policy by penalizing the KL divergence between their action\ndistributions (AD). However, AD regularization doesn't always work well since a\nsmall change in action distribution at a single state can lead to potentially\ncalamitous outcomes, while large changes might not be indicative of any\ndangerous activity. Our insight is that when reward hacking, the agent visits\ndrastically different states from those reached by the safe policy, causing\nlarge deviations in state occupancy measure (OM). Thus, we propose regularizing\nbased on the OM divergence between policies instead of AD divergence to prevent\nreward hacking. We theoretically establish that OM regularization can more\neffectively avoid large drops in true reward. Then, we empirically demonstrate\nin a variety of realistic environments that OM divergence is superior to AD\ndivergence for preventing reward hacking by regularizing towards a safe policy.\nFurthermore, we show that occupancy measure divergence can also regularize\nlearned policies away from reward hacking behavior. Our code and data are\navailable at https://github.com/cassidylaidlaw/orpo",
        "updated": "2024-03-05 18:22:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.03185v1"
    }
]