[
    {
        "title": "The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning",
        "authors": "Nathaniel LiAlexander PanAnjali GopalSummer YueDaniel BerriosAlice GattiJustin D. LiAnn-Kathrin DombrowskiShashwat GoelLong PhanGabriel MukobiNathan Helm-BurgerRassin LababidiLennart JustenAndrew B. LiuMichael ChenIsabelle BarrassOliver ZhangXiaoyuan ZhuRishub TamirisaBhrugu BharathiAdam KhojaAriel Herbert-VossCort B. BreuerAndy ZouMantas MazeikaZifan WangPalash OswalWeiran LiuAdam A. HuntJustin Tienken-HarderKevin Y. ShihKemper TalleyJohn GuanRussell KaplanIan StenekerDavid CampbellBrad JokubaitisAlex LevinsonJean WangWilliam QianKallol Krishna KarmakarSteven BasartStephen FitzMindy LevinePonnurangam KumaraguruUday TupakulaVijay VaradharajanYan ShoshitaishviliJimmy BaKevin M. EsveltAlexandr WangDan Hendrycks",
        "links": "http://arxiv.org/abs/2403.03218v1",
        "entry_id": "http://arxiv.org/abs/2403.03218v1",
        "pdf_url": "http://arxiv.org/pdf/2403.03218v1",
        "summary": "The White House Executive Order on Artificial Intelligence highlights the\nrisks of large language models (LLMs) empowering malicious actors in developing\nbiological, cyber, and chemical weapons. To measure these risks of malicious\nuse, government institutions and major AI labs are developing evaluations for\nhazardous capabilities in LLMs. However, current evaluations are private,\npreventing further research into mitigating risk. Furthermore, they focus on\nonly a few, highly specific pathways for malicious use. To fill these gaps, we\npublicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a\ndataset of 4,157 multiple-choice questions that serve as a proxy measurement of\nhazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP\nwas developed by a consortium of academics and technical consultants, and was\nstringently filtered to eliminate sensitive information prior to public\nrelease. WMDP serves two roles: first, as an evaluation for hazardous knowledge\nin LLMs, and second, as a benchmark for unlearning methods to remove such\nhazardous knowledge. To guide progress on unlearning, we develop CUT, a\nstate-of-the-art unlearning method based on controlling model representations.\nCUT reduces model performance on WMDP while maintaining general capabilities in\nareas such as biology and computer science, suggesting that unlearning may be a\nconcrete path towards reducing malicious use from LLMs. We release our\nbenchmark and code publicly at https://wmdp.ai",
        "updated": "2024-03-05 18:59:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.03218v1"
    },
    {
        "title": "CLEVR-POC: Reasoning-Intensive Visual Question Answering in Partially Observable Environments",
        "authors": "Savitha Sam AbrahamMarjan AlirezaieLuc De Raedt",
        "links": "http://arxiv.org/abs/2403.03203v1",
        "entry_id": "http://arxiv.org/abs/2403.03203v1",
        "pdf_url": "http://arxiv.org/pdf/2403.03203v1",
        "summary": "The integration of learning and reasoning is high on the research agenda in\nAI. Nevertheless, there is only a little attention to use existing background\nknowledge for reasoning about partially observed scenes to answer questions\nabout the scene. Yet, we as humans use such knowledge frequently to infer\nplausible answers to visual questions (by eliminating all inconsistent ones).\nSuch knowledge often comes in the form of constraints about objects and it\ntends to be highly domain or environment-specific. We contribute a novel\nbenchmark called CLEVR-POC for reasoning-intensive visual question answering\n(VQA) in partially observable environments under constraints. In CLEVR-POC,\nknowledge in the form of logical constraints needs to be leveraged to generate\nplausible answers to questions about a hidden object in a given partial scene.\nFor instance, if one has the knowledge that all cups are colored either red,\ngreen or blue and that there is only one green cup, it becomes possible to\ndeduce the color of an occluded cup as either red or blue, provided that all\nother cups, including the green one, are observed. Through experiments, we\nobserve that the low performance of pre-trained vision language models like\nCLIP (~ 22%) and a large language model (LLM) like GPT-4 (~ 46%) on CLEVR-POC\nascertains the necessity for frameworks that can handle reasoning-intensive\ntasks where environment-specific background knowledge is available and crucial.\nFurthermore, our demonstration illustrates that a neuro-symbolic model, which\nintegrates an LLM like GPT-4 with a visual perception network and a formal\nlogical reasoner, exhibits exceptional performance on CLEVR-POC.",
        "updated": "2024-03-05 18:41:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.03203v1"
    },
    {
        "title": "Towards Democratized Flood Risk Management: An Advanced AI Assistant Enabled by GPT-4 for Enhanced Interpretability and Public Engagement",
        "authors": "Rafaela MarteloRuo-Qian Wang",
        "links": "http://arxiv.org/abs/2403.03188v1",
        "entry_id": "http://arxiv.org/abs/2403.03188v1",
        "pdf_url": "http://arxiv.org/pdf/2403.03188v1",
        "summary": "Real-time flood forecasting plays a crucial role in enabling timely and\neffective emergency responses. However, a significant challenge lies in\nbridging the gap between complex numerical flood models and practical\ndecision-making. Decision-makers often rely on experts to interpret these\nmodels for optimizing flood mitigation strategies. And the public requires\ncomplex techniques to inquiry and understand socio-cultural and institutional\nfactors, often hinders the public's understanding of flood risks. To overcome\nthese challenges, our study introduces an innovative solution: a customized AI\nAssistant powered by the GPT-4 Large Language Model. This AI Assistant is\ndesigned to facilitate effective communication between decision-makers, the\ngeneral public, and flood forecasters, without the requirement of specialized\nknowledge. The new framework utilizes GPT-4's advanced natural language\nunderstanding and function calling capabilities to provide immediate flood\nalerts and respond to various flood-related inquiries. Our developed prototype\nintegrates real-time flood warnings with flood maps and social vulnerability\ndata. It also effectively translates complex flood zone information into\nactionable risk management advice. To assess its performance, we evaluated the\nprototype using six criteria within three main categories: relevance, error\nresilience, and understanding of context. Our research marks a significant step\ntowards a more accessible and user-friendly approach in flood risk management.\nThis study highlights the potential of advanced AI tools like GPT-4 in\ndemocratizing information and enhancing public engagement in critical social\nand environmental issues.",
        "updated": "2024-03-05 18:24:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.03188v1"
    },
    {
        "title": "Reliable, Adaptable, and Attributable Language Models with Retrieval",
        "authors": "Akari AsaiZexuan ZhongDanqi ChenPang Wei KohLuke ZettlemoyerHannaneh HajishirziWen-tau Yih",
        "links": "http://arxiv.org/abs/2403.03187v1",
        "entry_id": "http://arxiv.org/abs/2403.03187v1",
        "pdf_url": "http://arxiv.org/pdf/2403.03187v1",
        "summary": "Parametric language models (LMs), which are trained on vast amounts of web\ndata, exhibit remarkable flexibility and capability. However, they still face\npractical challenges such as hallucinations, difficulty in adapting to new data\ndistributions, and a lack of verifiability. In this position paper, we advocate\nfor retrieval-augmented LMs to replace parametric LMs as the next generation of\nLMs. By incorporating large-scale datastores during inference,\nretrieval-augmented LMs can be more reliable, adaptable, and attributable.\nDespite their potential, retrieval-augmented LMs have yet to be widely adopted\ndue to several obstacles: specifically, current retrieval-augmented LMs\nstruggle to leverage helpful text beyond knowledge-intensive tasks such as\nquestion answering, have limited interaction between retrieval and LM\ncomponents, and lack the infrastructure for scaling. To address these, we\npropose a roadmap for developing general-purpose retrieval-augmented LMs. This\ninvolves a reconsideration of datastores and retrievers, the exploration of\npipelines with improved retriever-LM interaction, and significant investment in\ninfrastructure for efficient training and inference.",
        "updated": "2024-03-05 18:22:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.03187v1"
    },
    {
        "title": "Towards General Computer Control: A Multimodal Agent for Red Dead Redemption II as a Case Study",
        "authors": "Weihao TanZiluo DingWentao ZhangBoyu LiBohan ZhouJunpeng YueHaochong XiaJiechuan JiangLongtao ZhengXinrun XuYifei BiPengjie GuXinrun WangBörje F. KarlssonBo AnZongqing Lu",
        "links": "http://arxiv.org/abs/2403.03186v1",
        "entry_id": "http://arxiv.org/abs/2403.03186v1",
        "pdf_url": "http://arxiv.org/pdf/2403.03186v1",
        "summary": "Recent studies have demonstrated the success of foundation agents in specific\ntasks or scenarios. However, existing agents cannot generalize across different\nscenarios, mainly due to their diverse observation and action spaces and\nsemantic gaps, or reliance on task-specific resources. In this work, we propose\nthe General Computer Control (GCC) setting: building foundation agents that can\nmaster any computer task by taking only screen images (and possibly audio) of\nthe computer as input, and producing keyboard and mouse operations as output,\nsimilar to human-computer interaction. To target GCC, we propose Cradle, an\nagent framework with strong reasoning abilities, including self-reflection,\ntask inference, and skill curation, to ensure generalizability and\nself-improvement across various tasks. To demonstrate the capabilities of\nCradle, we deploy it in the complex AAA game Red Dead Redemption II, serving as\na preliminary attempt towards GCC with a challenging target. Our agent can\nfollow the main storyline and finish real missions in this complex AAA game,\nwith minimal reliance on prior knowledge and application-specific resources.\nThe project website is at https://baai-agents.github.io/Cradle/.",
        "updated": "2024-03-05 18:22:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.03186v1"
    }
]