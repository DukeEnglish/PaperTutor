LC-Tsalis-INF: Generalized Best-of-Both-Worlds Linear Contextual
Bandits
Masahiro Kato1 and Shinji Ito2
1Mizuho-DL Financial Technology Co., Ltd., masahiro-kato@fintec.co.jp
2NEC Corporation and RIKEN AIP, i-shinji@nec.com
Abstract In each round of our bandit trial, (i) a decision-
makerobservesindependentandidenticallydistributed
This study considers the linear contextual bandit (i.i.d.) contexts, (ii) selects an arm based on the con-
problem with independent and identically distributed text and past observations up to that round, and (iii)
(i.i.d.) contexts. In this problem, existing stud- incurs a loss corresponding to the selected arm. The
ies have proposed Best-of-Both-Worlds (BoBW) algo- decision-maker’s objective is to minimize the cumula-
rithmswhoseregretssatisfyO(log2(T))forthenumber tive loss incurred throughout the trial.
ofroundsT inastochasticregimewithasuboptimality The contextual bandit problem has been widely
gap lower-bounded by a positive constant, while satis- studiedinfieldssuchassequentialtreatmentallocation
fying O(√T) in an adversarial regime. However, the (Tewari & Murphy, 2017), personalized recommenda-
dependency on T has room for improvement, and the tions(Beygelzimer et al.,2011),andonlineadvertising
suboptimality-gapassumptioncanberelaxed. Forthis (Li et al., 2010). This study assumes linear models be-
issue, this study proposes an algorithm whose regret tween the loss and contexts, known as the linear con-
satisfiesO(log(T))in the setting whenthe suboptimal- textualbanditproblem(Abe & Long,1999). Thisfield
itygapislower-bounded. Furthermore,weintroducea encompassesnumerousrelatedwork(Chen et al.,2020;
margin condition, a milder assumption on the subopti- Li et al., 2021; Ding et al., 2022).
mality gap. That condition characterizes the problem
The data-generating process (DGP) of a loss in
difficulty linked to the suboptimality gap using a pa-
each round is categorized into stochastic or adversar-
rameterβ (0, ]. Wethenshowthatthealgorithm’s
∈ ∞ ial regimes. Optimal algorithms typically differ be-
1+β 1
regret satisfies O log(T) 2+β T2+β . Here, β = tweenthestochasticandadversarialregimes. However,
{ } ∞
correspondsto the(cid:16)case in the existin(cid:17)g studies where a thereexistBoBWalgorithmsthatareeffectiveinboth
lower bound exists in the suboptimality gap, and our regimes (Bubeck & Slivkins, 2012; Seldin & Slivkins,
regretsatisfiesO(log(T))inthatcase. Ourproposedal- 2014; Auer & Chiang, 2016; Seldin & Lugosi, 2017;
gorithmisbasedontheFollow-The-Regularized-Leader Zimmert & Seldin, 2021; Lee et al., 2021).
withtheTsallisentropy andreferredtoastheα-Linear- Kato & Ito(2023)andKuroki et al.(2023)indepen-
Contextual (LC)-Tsallis-INF. dently proposeBoBWalgorithmsfor linearcontextual
bandits. They both employ a Follow-The-Regularized-
Leader (FTRL) algorithm with the Shannon entropy
1 Introduction andan entropy-adaptiveupdate rule for learning rates
(Ito et al., 2022). Their algorithms’ regrets satisfy
This study considers the linear contextual bandit O(log2(T)) regarding T. Furthermore, Kuroki et al.
problem with adversarial corruption and proposes a (2023)developsanalgorithmwithO(log(T))regretup-
best-of-both-worlds (BoBW) algorithm whose regret per bound using the reduction framework proposedby
upper bound tightly depends on the number of rounds Dann et al. (2023a). Although this approach gives a
T. tighter upper bound regarding T, it is known to have
1
4202
raM
5
]GL.sc[
1v91230.3042:viXraTable1: Comparisonofregrets. Wecategorizeregretupperboundsbasedontheregimesandtheassumptionon .
We make X in the “√C” column if algorithms’ upper bounds depend on the corruption level C 0 tightly undeG r
≥
the adversarial corruption represented by the corruption parameter C. The “Assumption on ” column denotes
which one, full information is accessible, or O(poly(T)) samples are drawn from G, to compuG te Σ−t 1. Note that
Liu et al. (2023) does not assume the access to , by incurring some computation costs.
G
Regret
√C Assumptionon
Stochastic Adversarial G
Am1 a/ r2 g- inLC c( oO-T nu ds ra is tl ) il ois n-I wN iF
thβ. O β1+ ∆β
∗
2+βO β(cid:18) LL √√K K(cid:0)∆d d+ ∗ +λm1 i λn m1(cid:1) inlog lo(T g() T(cid:19)
)
21 ++ ββ
T2+1 β
O (cid:16)q√K (cid:0)d -+ λm1 in (cid:1)T (cid:17) X
-
F Fu ul ll li in nf fo or rm ma at ti io on n.
.
(cid:18)n o nwhere(cid:0)β (0,(cid:1)]. o (cid:19)
∈ ∞
O L√K d+l λog m( iT n) log(T) O √K d+ 1 T X O(poly(T))samples.
O β1+ ∆β
∗
2+β β (cid:18) L√K(cid:0)∆ d∗ +lo λg m((cid:1) iT n) log(T(cid:19) ) 21 ++ ββ T2+1 β (cid:16)q (cid:0) - λmin (cid:1) (cid:17) - O(poly(T))samples.
(cid:18)n o n wher(cid:0)eβ (0, (cid:1)]. o (cid:19)
∈ ∞
BoBW-RealFTRL O D O log(KT)TD X Fullinformation.
∆∗
(Kato&Ito,2023) wher(cid:16)eD(cid:17)=log(T) 1 +dlog(K) log(KT(cid:16))p. (cid:17)
λmin
(Kurokietal.,2023) O D′ (cid:16) (cid:17) O log(KT)TD X O(poly(T))samples.
∆∗
where(cid:16)D′(cid:17)=log(T) lo λg m( iT n)+dlog(K) log(K(cid:16)Tp). (cid:17)
BoBWreductionwith (cid:16) (cid:17)
abasealgorithmofRealLinExp3 O K2 d+ 1 2 log(K)log(T) O √TK2 d+ 1 2 log(K) X Fullinformation
(Proposition8,Kurokietal.,2023)
(cid:18)∆∗
(cid:16)
λmin
(cid:17) (cid:19) (cid:18) (cid:16)
λmin
(cid:17) (cid:19)
Logdet-FTRL
- O d2√Tlog(T) - Noaccess
(poly(K,d,T)incomputation)
LinearEXP4 (cid:16) (cid:17)
- O d Tlog(T) - Noaccess
(Tdincomputation)
(Liuetal.,2023) (cid:16) p (cid:17)
OFUL O(dlog(1/δ)/∆)withprobability1 δ - - N/A
−
(Abbasi-yadkorietal.,2011) O(dlog(T)/∆)withprobability1 1/T - - N/A
−
RealLinExp3 .
- O log(T) dlog(K)T - O(poly(T))samples
(Neu&Olkhovskaya,2020)
(cid:16) p (cid:17)
somelimitations. Forexample,Kuroki et al.(2023)re- Since ourproposedalgorithmisbasedonthe FTRL
ports that it is feasible only when we know the inverse with the Tsallis entropy,we refer to it as the α-Linear-
of the covariance matrix, and it may not be practical Contextual (LC)-Tsallis-INF. We explain the details
due to the difficulty in implementation. To overcome below.
this issue, Kuroki et al. (2023) proposes their FTRL-
based algorithm. 1.1 OrganizationandMainContribution
This study aims to improve the dependency on
T. The O(log2(T)) regret is caused by the use of Section 2 formulates the linear contextual bandit
problem. We assume that X is generated from a
the Shannon entropy in the FTRL. Therefore, to im- t
known distribution and induces a d-dimensional fea-
prove upper bounds, we apply the Tsallis entropy in-
ture vector φ(a,X )G Rd for each arm a [K] (See
steadoftheShannonentropy(Zimmert & Seldin,2021; t
∈ ∈
Assumption 2.1).
Masoudian & Seldin, 2021), which allows us to obtain
Thisstudyconsidersacasewherethereisamapping
O(log(T)) regret.
fromanarm-independentcontexttoanarm-dependent
Furthermore, Kato & Ito (2023) and Kuroki et al.
context, called a feature. Our setting is standard
(2023) only discuss a case where there exists a posi-
in linear contextual bandits (Abbasi-yadkori et al.,
tivelowerboundforasuboptimality gap,whichiscon-
2011; Li et al., 2021) but differs from the setting in
sidered to be restrictive in linear contextual bandits.
Neu & Olkhovskaya (2020), Kato & Ito (2023), and
This study introduces a milder assumptionon the sub-
Kuroki et al. (2023), which assume linear models be-
optimality gap, called a margin condition, and charac-
tweena lossandanarm-independentcontext. We also
terizes the problem difficulty using a parameter β
∈ develop upper bounds of the BoBW-RealFTRL algo-
(0,+ ]. We then show that our proposed algorithm’s
∞ rithm by Kato & Ito (2023) and Kuroki et al. (2023)
1+β 1
regret satisfies O log(T) 2+β T2+β . The condition under this setting in Appendix D.
{ }
assumedinKato &(cid:16) Ito(2023)andKu(cid:17)roki et al.(2023) In Section 4, we define three regimes for the
correspondsto a marginconditionwithβ = , andin loss DGP: an adversarial regime (Neu & Olkhovskaya,
∞
that case, the regret upper bound becomes O(log(T)). 2020), a stochastic regime with a margin condition
2(Li et al., 2021), and a linear contextual adversarial L√K d+ 1
the regret satisfies O λmin log(T) +
regimewithaself-boundingconstraint. Inanadversar- ∆∗
(cid:0) (cid:1)
ial regime, the nature chooses the loss in each round CL√K d+ 1 (cid:16)
λmin log ∆∗T , where log (x) :=
based on past observations up to that round. A mar-
r
(cid:0)∆∗
(cid:1)
+ C +
gin condition is an assumption on the suboptimality max {1,log(x)
}
aqnd rec (cid:0)all th (cid:1)(cid:17)at β is a parameter
gap, the difference between the expected losses of the of a margin condition. When C = 0, we define
bestandsuboptimalarms. Specifically,themargincon- log
+
∆ C∗T = 1. When Σ−t 1 is approximated by the
β MGR algorithm, we multiply eachregretupper bound
dition is given as P(∆(X 0) ≤h)
≤
21 ∆h
∗
, where
by
lo(cid:0) g(T)(cid:1)
in a stochastic regime and by log(T) in
β (0, ] is a margin condition para(cid:16)met(cid:17)er, ∆(X 0) an adversarialregime.
is t∈ he su∞ boptimality gap given context X generated p
0 Our derived upper bound demonstrates a tighter
from , d is the dimensionof features,∆ >0 is a uni-
dependence on T compared to the existing FTRL-
G ∗
versal constant, and h satisfies 0 h ∆ . When
based algorithms proposed by Kato & Ito (2023) and
≤ ≤ ∗
β = , it implies a constant gap ∆ for any con-
Kuroki et al. (2023). Furthermore, their results con-
∞ ∗
text X . The detailed definition is provided in Def-
0 sider only the case with β = in a stochastic regime,
inition 4.1. A linear contextual adversarial regime ∞
while our study addresses more general cases depend-
with a self-bounding constraint assumes the existence
ingonthemarginconditionwithvariousparametersβ.
of constants ∆ > 0 and C > 0 such that R
T Additionally,comparedtothereductionapproachwith
∆ ∗E T
t=1 a
∈∗ [K]π t(a |X t)
−
C, where K is th≥ e O(log(T))regretbyKuroki et al.(2023),ouralgorithm
numbhe Pr of a Prms, π t(a |X t) isithe probability of select- is easy to implement and has a tighter dependence on
ing arm a in round t given context X . The detailed K. A more detailed comparison is provided in Sec-
t
definition is provided in Definition 4.2. tion 1.2. In Appendix D, we derive an upper bound of
Section 5 presents our algorithm, the α-Linear- their algorithm under a margin condition, depending
Contextual-Tsallis-INF (α-LC-Tsallis-INF), which is on β.
an FTRL-based approach with an α-Tsallis-entropy Insummary,ourcontributionslieintheproposition
regularization. of an algorithm whose upper bound tightly depends
A key component of our algorithm is the inverse on T and the analysis under a margin condition and
of the covariance matrix of a (arm-dependent) feature the arm-dependent feature setting. Our results corre-
multiplied by a policy π in each round t [T], which spond to the refinement and generalization of the re-
t
∈
is denotedby Σ in Section2.2. While the true inverse sultsbyNeu & Olkhovskaya(2020),Kato & Ito(2023),
t
of Σ is given under the full information about , we and Kuroki et al. (2023).
t
G
needtoapproximateitwhenonlyfinitesamplesareob-
tainable from . We develop two regret upper bounds 1.2 RelatedWork
G
for two cases where full information about is known
G
andonlyO(poly(T))samplesareobtainablefrom . In
G InTable1,wecompareouralgorithm’sregretswith
thelatter,weapproximateΣ byemployingtheMatrix
t those in existing work Neu & Olkhovskaya (2020) pro-
Geometric Resampling (MGR) algorithm.
posethe RealLinExp3foradversariallinearcontextual
In Section 6, we derive regret upper bounds of the
bandits. Inastochasticregime,theregretupperbound
1/2-LC-Tsallis-INF (α-LC-Tsallis-INF with α = 1/2)
is significantly affected by a margin condition, an as-
for each regime, depending on the knowledge about
sumption on the suboptimality gap given a context.
, which affects the availability of the inverse of
G We refer to regret upper bounds with the assumption
Σ . When we have full information about , the
t G of a margin condition as problem-dependent, while we
regret upper bound in an adversarial regime for
refertoregretupper boundswithoutassumingspecific
our proposed algorithm is O √K d+ 1 T , assumptions on data distribution, such as a margin
λmin
where λ
min
is the smallest e(cid:16)igqenvalu (cid:0)e of a (cid:1)fe(cid:17)a- condition, as problem-independent. Dani et al. (2008)
ture covariance matrix induced by an exploratory propose the ConfidenceBall, and Abbasi-yadkoriet al.
policy (See Assumption 2.3). In a stochastic (2011) propose OFUL. Both present upper bounds
regime with a margin condition, the regret satisfies in both problem-dependent and problem-independent
β 1+β analyses. In their problem-dependent analyses, they
O 1+β 2+β L√K d+ 1 log(T) 2+β T2+1 β ,
β∆∗ λmin assume the existence of ∆ for a lower bound of the
wh(cid:18) enre Lo isn the (cid:0)lowest (cid:1)probabiolity of (cid:19) con- suboptimality gap, corresp∗ onding to a margin condi-
texts (See Assumption 2.2). In an adver- tion with β = . Goldenshluger & Zeevi (2013),
∞
sarial regime with a self-bounding constraint, Wang et al. (2018), and Bastani & Bayati (2020) pro-
3pose algorithms in a case with β = 1. Furthermore, bandits (Kong et al., 2023), episodic Markov Decision
Li et al.(2021)proposetheℓ -ConfidenceBallbasedal- Processes (MDPs) (Dann et al., 2023b), and sparse
1
gorithm whose upper bound tightly depends on β. bandits (Tsuchiya et al., 2023b).
By extending the RealLinEXP3 of However,underexistingstudiesemployingtheShan-
Neu & Olkhovskaya (2020), Kato & Ito (2023) non entropy, regrets usually do not satisfy O(log(T)),
and Kuroki et al. (2023) independently propose as Kato & Ito (2023) and Kuroki et al. (2023) show
FTRL-based algorithms that work in both adversarial O(log2(T)) regret. In contrast, Zimmert & Seldin
and stochastic regimes using the Shannon entropy (2021) shows that under the Tsallis entropy, we can
regularization. Kato & Ito(2023)proposestheBoBW- design an algorithm whose regret satisfies O(log(T))
RealFTRL algorithm whose regret upper bound is in MAB. As well as Zimmert & Seldin (2021),
O min D + CD, log(KT)TD in a linear Rouyer & Seldin (2020), and Masoudian & Seldin
∆∗ ∆∗ (2021), we also employ the Tsallis entropy to tighten
co(cid:16)ntextunal advqersaria pl regime witho(cid:17)a self-bounding
the dependence on T in linear contextual bandits.
constraint when only finite samples are available from
Compared to the algorithm with the reduction ap-
G, where D = Klog(T) lo λg m( iT n) +dlog(K) log(KT). proach by Kuroki et al. (2023), our algorithm is easy
They also show that in(cid:16)an adversarial re(cid:17)gime, the to implement and has a tighter dependence regarding
K.
upper bound is O TK d+ log(T) log(T)log(K) .
λmin
Kuroki et al. (2(cid:18) 0r 23) a(cid:16) lso develop(cid:17) s an algorith(cid:19) m 2 Problem Setting
calledtheFTRL-LCandderivesitsupperboundwhen
thetrueinverseofΣ tisgiven,inadditiontothosewhen Suppose that there are T rounds and K arms.
theinverseisapproximatedusingfinitesamplesfrom . We denote the sets of rounds and arms by [T] :=
IfΣ−t 1 is known,the aboveregretupper bounds aredG i- 1,2,...,T and [K]:= 1,2,...,K , respectively.
{ } { }
videdbylog(T)inastochasticregimeandby log(T) In each round t [T], a decision-maker observes a
∈
inanadversarialregime. Notethatthealgorithmspro- contextX ,where isanarbitrarycontextspace,
p t ∈X X
posed by Kato & Ito (2023) and Kuroki et al. (2023) and chooses an arm A [K] based on the context X
t t
∈
are almost the same, and we follow the terminology and past observations. Each arm a [K] is linked
∈
and formulations of Kato & Ito (2023). to a loss ℓ (a,X ), which depends on X , arm
t t t
∈ X
Furthermore, Kuroki et al. (2023) develops another a [K], and round t [T]. After choosing arm A in
t
∈ ∈
BoBWalgorithminlinearcontextualbanditsusingthe round t, the decision-maker incurs the corresponding
reductionapproachbyDann et al.(2023a). Theregret lossℓ (A ,X ). Ourgoalistominimizethecumulative
t t t
achievesO(log(T)),butthealgorithmmaynotbeprac- loss T ℓ (A ,X ). We introducethesettinginmore
t=1 t t t
tical in implementation and requires the full informa- detail in the following subsection.
tion about . NP otation. Let , denote inner products in Eu-
G h· ·i
Several other studies relate to linear con- clidean space and let denote the ℓ norm. Let
2 2
k·k
textual bandits with adversarial corruption. denote the operator norm of a symmetric posi-
op
k·k
Lykouris & Vassilvtiskii (2018), Gupta et al. (2019), tive semidefinite matrix.
and Zhao et al. (2021) consider other corruption
2.1 ProcedureinaTrial
frameworks characterized by a constant C [0,T],
∈
which is different but related to our linear contextual
adversarial regime with a self-boundingeconstraint. This section describes the decision-making proce-
He et al. (2022) uses another constant C [0,T], dure. We refer to a function that the decision-maker
†
∈ follows in arm selection as a policy. Let Π be the
closely related to C. For details, also see Remark 2 in
Kato & Ito (2023). e set of all possible policies π : K := u =
X → P
As well as Kaeto & Ito (2023) and Kuroki et al. (u u ... u ) [0,1]K K u = 1 win th its
(2023), we employ the self-bounding tech- 1 2 K ⊤ ∈ | k=1 k
a-th element π(a x). o
nique (Zimmert & Seldin, 2021; Wei & Luo, 2018; | P
Specifically, we consider sequential decision-making
Masoudian & Seldin, 2021) and an entropy-adaptive
with the following steps in each round t [T]:
update rule for learning rates, which have been ∈
1. The nature decides ℓ (a,x) based on
proven effective in providing BoBW guarantees for t a [K],x
. ∈ ∈X
online learning in feedback graph contexts (Ito et al., Ft −1 (cid:8) (cid:9)
2022), multi-armed bandits (MAB) (Jin et al., 2023), 2. Thedecision-makerobservesacontextX ,gen-
t
∈X
partial monitoring (Tsuchiya et al., 2023a), linear erated from a known distribution .
G
43. Based on the observed context X , the decision- Assumption2.2(Contextdistributionwithfinitesup-
t
maker selects a policy π (X ) . port). We assume the following for context distribu-
t t K
∈P
tion:
4. The decision-maker chooses action A [K] with
t ∈ • Acontextspace is := x ,x ,...,x forS N.
probability π t(A t X t). X X { 1 2 S } ∈
| • For any x , P X = x 1/L given a
5. The decision-maker incurs a loss ℓ t(A t,X t).
universal
co∈ nstX
ant
LX0∼G
S.
0 ≥
Thegoalofthedecision-makeristoselectactionsin ≥ (cid:0) (cid:1)
a way that minimizes the total loss T t=1ℓ t(A t,X t). In regret analysis, Σ
t
plays an important role. To
analyze Σ , we make the following assumption.
P t
2.2 DefinitionandAssumptionsaboutContexts
Assumption 2.3 (Eigenvalue condition for Σ(E)).
For any x , there exist an exploration policy
We assume that X is an i.i.d. randomvariablegen- ∈ X
t e (x) whose distribution is E and a univer-
erated from a context distribution over the support
∗
∈
PK x∗
sal constant λ > 0 such that for E = E ,
,whichisassumedtobeknowntoG
thedecision-maker.
min ∗
{
x∗ }x
∈X
the matrix Σ(E ) is positive definite and its smallest
X ∗
Because the randomness of X is independent of t, we
t eigenvalue is larger than λ .
min
use X to denote an i.i.d. random variable from the
0
context distribution . Our definition includes a setting of
G
We define X as a context that is independent of Neu & Olkhovskaya (2020), which assumes that
t
an arm. This setting of contexts follows the approach the covariance of arm-independent contexts has its
usedinNeu & Olkhovskaya(2020). However,itdiffers smallest eigenvalue λ min >0.
from the approach in several other related studies on If =1, the setting boils down to non-contextual
|X|
linearcontextualbandits,includingLi et al.(2010)and linear bandits. Ine this case, we can take e ∗ by
Liu et al.(2023),whichutilizearm-dependentcontexts. using the G-optimal design. See Chapter 21 in
We refer to such contexts as features to differentiate it Lattimore & Szepesv´ari (2020).
from arm-independent contexts.
To obtain features, we define a feature map that 2.3 LinearContextualBandits
transforms a context x into a feature φ(a,x)
Rd,where is ad-d∈ imX ensionalfeaturespace. W∈ e This study assumes linear models between ℓ t(a,x)
Z ⊆ Z
assume that the feature map is known to the decision- and x.
maker, as well as .
G Assumption2.4(Linearmodels). Foralla [K]and
In our analysis,the inverseof the covariancematrix ∈
any x , the following holds:
∈X
Σ(E):=E Xt∼G,At∼EXt φ(A t,X t)φ ⊤(A t,X t) . ℓ t(a,x)= φ(a,x),θ t +ε t(a),
h i
plays an important role, where E := E is a where θ t Θ is a d-di(cid:10)mensional(cid:11)parameter with a pa-
{ x }x ∈X rameter s∈ pace Θ Rd, and ε (a) is an errorterm such
set of conditional distributions E of a given x, and t
x that E[ε (a) X ,⊂ ]=0.
recall that X is a sample from the context distri- t t t 1
0 | F −
bution independent of past observations =
t 1 Forthelinearmodelsandrelatedvariables,wemake
G t 1 F −
X s,A s,ℓ s(A s,X s) s− =1. the following assumptions.
We summarize the assumptions about contexts be-
(cid:8)(cid:0) (cid:1)(cid:9) Assumption 2.5 (Bounded variables). The following
low.
hold:
Assumption 2.1 (Contextual distribution). We as- 1. There exists a universal constant C >0 such that
Z
sume the following for context X and φ(, ): for any z , z C holds.
t 2
• Context X is an i.i.d. random v· a· riable from ∈Z k k ≤ Z
t 2. There exists a universal constant C >0 such that
∈ X Θ
a known context distribution , where is some for each θ Θ, θ C holds.
arbitrary support. G X ∈ k k2 ≤ Θ
3. There exists a universal constant C > 0 such that
• There is a known feature map φ : [K] , for all a [K], ε (a) C holds. E
which maps x to feature φ(a,x) × XRd→ Z ∈ | t |≤ E
∈X ∈Z ⊂ Under this assumption, there exists C :=
ℓ
When considering a stochastic regime with an ad- C(C ,C ,C )suchthatforallℓ (a,X ),eacha [K],
Θ t t
Z E ∈
versarial corruption with the Tsallis entropy, we addi- and any x . ℓ (a,x) = φ(a,x),θ +ε (a) C
t t t ℓ
∈X | | ≤
tionally use the following assumption on a context X . holds.
t (cid:12)(cid:10) (cid:11) (cid:12)
(cid:12) (cid:12)
5The parameter θ is generated in different ways ac- Abbasi-yadkoriet al., 2011). Meanwhile, other stud-
t
cording to the regimes of the DGP. We defined them ies address contextual bandits with adversarial con-
in Section 4. texts and adversarial losses (Kanade & Steinke, 2014;
Hazan et al., 2016). This study focuses exclusively on
3 Regret contextual bandits with i.i.d. contexts and adversarial
losses, as studied by Rakhlin & Sridharan (2016) and
Syrgkanis et al. (2016). This study follows the setting
This section defines the (pseudo) regret, a relative
of Neu & Olkhovskaya (2020), Kato & Ito (2023), and
measure of the cumulative loss, to evaluate the perfor-
Kuroki et al. (2023).
mance ofthe decision-maker’spolicy. Let be the set
R
of all possible ρ : [K]. The quality of a decision
X → 4.2 StochasticRegimewithaMarginCondition
bythedecision-makerismeasuredbyitstotalexpected
regret, defined as
Next, we define a margin condition, which is often
T assumed in linear contextual bandits to characterize
R T :=maxE ℓ t(A t,X t) ℓ t(ρ(X t),X t) , the difficulty of the problem instance (Li et al., 2021).
ρ ∈R " Xt=1n − o# Since this section focuses on a stochastic regime,re-
gression coefficients are fixed, and we denote them by
where the expectation is taken over the randomness
θ ; that is, θ = = θ = θ . Note that under a
of policies of the decision-maker, as well as the se- 0 1 ··· t 0
stochastic regime,
quence of random contexts, X , and losses,
t t [T]
{ } ∈
ℓ (,X ) .
t t t [T] T
{ H· ere, w} e∈also define an optimal policy a ∗T as R T =E ∆(a X t)π t(a X t)
 | | 
T
Xt=1aX∈[K]
a ∗T =argminE φ(ρ(X t),X t),θ t) .  T 
ρ ∈R " Xt=1D E# ≥E  ∆(X t) π t(a |X t) 
Then, the regret is equal to R
T
=
Xt=1 a 6=aX∗ T(Xt)
E T φ(A ,X ) φ(a (X ),X ),θ .  T 
hNet u=1 &DOlkht ovst ka−
ya
(20∗T 20)t refert
s
tot
Eaifunction ρ as
=E
"
∆(X t) 1 −π t(a ∗T(X t) |X t) #, (1)
P t=1
a linear-classifier policy, while π is called stochastic X (cid:0) (cid:1)
t
where ∆(a x)= φ(a,x) φ(a (x),x),θ , and
policies. In our study, decision-makers compare their | − ∗T 0
stochasticpoliciesπ totheoptimallinear-classifierpol- D E
t
icy a ∗T using the regret. ∆(x)= a 6=m a∗ Tin (x)( Dφ(a,x),θ 0 E− Dφ(a∗T(x),x),θ 0 E).
4 Data-Generating Process Recall that π is our defined policy in round t (Sec-
t
tion 2.1).
Regarding this suboptimality gap ∆(X), we define
In each t [T], the nature chooses ℓ (,x)
t
∈ · x a stochastic regime with a margincondition as follows.
basedonthepastobservations . Wecnonsiderothr∈eXe
t 1
F − Definition 4.1 (Stochastic regime with a margincon-
regimes for the DGP of losses ℓ : anadversarial
t t [T]
{ } ∈ dition). Consider a stochastic regime with fixed re-
regime, a stochastic regime with a margin condition,
gression coefficient θ Θ, where for all t [T], all
and a linear contextual adversarial regime with a self- 0
∈ ∈
a [K] and any x , the loss is generated as
bounding constraint.
∈ ∈ X
ℓ (a,x) = φ(a,x),θ +ε (a). Furthermore, there ex-
t 0 t
4.1 AdversarialRegime ists a universal constant ∆ > 0, independent of T,
(cid:10) (cid:11) ∗
andβ (0,+ ],suchthatforanyh [0,∆ ],itholds
∈ ∞ ∈ ∗
that
First,weintroducetheadversarialregime,wherewe
β
1 h
donotmakeanyassumptionsaboutthebehaviorofthe P(∆(X ) h) .
t
≤ ≤ 2 ∆
nature. In this case, it is known that the lower bound (cid:18) ∗(cid:19)
is O(√T) when there is no context (Auer et al., 2002). The margin condition plays a crucial role in regret
Note that adversarial linear contextual bandits analysis, as discussed in (1).
can be defined in various ways. For example, Note that when β = , ∆(x) ∆ holds for any
∞ ≥ ∗
some studies consider contextual bandits with ad- x and all t [T], which is discussed in the follow-
∈X ∈
versarial contexts and fixed losses (Chu et al., 2011; ing section.
6A margin condition in Li et al. (2021) restricts the 5.1 ProposedAlgorithm: α LC Tsallis INF
rangeofhas[D log(d)/T,∆ ]forsomeuniversalcon-
∗
stantD >0,andtheyderivematchinglowerandupper This section provides a BoBW algorithm for linear
p
bounds. contextual bandits. We first define an estimator of θ
t
andthen developanFTRL-basedalgorithmusing this
4.3 Adversarial Regime with a Self Bounding estimator.
Constraint For simplicity, when selecting an arm a with prob-
ability π (a x) given x , we denote Σ(E) by Σ ,
t t
| ∈ X
This section defines a regime with an adversarial where E is defined corresponding to π t. Note that Σ t
corruption. Let ∆ > 0 be a universal constant, as is equal to
∗
used in Definition 4.1. If ∆(x) ∆ holds for any
≥ ∗
x in a stochastic regime, then the regret can be
low∈ erX bounded asR ∆ E T 1 π a (X ) . Σ t =E Xt∼G π t(a |X t)φ(a,X t)φ ⊤(a,X t) .
T ≥ ∗ t=1 − t ∗T t a X∈[K]
Based on this intuition, whe Pdefin(cid:16)e a line (cid:0)ar conte(cid:17)xi-  
tual adversarial regime with a self-bounding constraint Regressioncoefficientestimator. Wedefineanes-
below, as well as introduced by Kato & Ito (2023). timator of the regression parameter θ as
t
Definition 4.2 (Linear contextual adversarial regime
with a self-bounding constraint). We say that the
θ
t
:=θ
t
Σ†t :=Σ†tφ A t,X
t
ℓ t(A t,X t),
(cid:16) (cid:17)
DGP is in a (∆ ,C,T)-adversarial regime with a self- (cid:0) (cid:1)
bounding
constr∗
aint for some ∆ ,C > 0 if the regret
where Σb †t is dbefineed as e
∗
R T is lower bo Tunded as
Σ†t =
(e ΣΣ †t−t 1 ii ff oth ne lyfu fil nl ii tn efo sr am ma pt li eo sn aa rb eo gu ivt
eG
ni fs rog mive Gn
,
R E ∆ (a X )π (a X ) C,
T t t t t e
≥ 
Xt=1a 6=Xa∗ t(Xt)
| | − a rin thd mΣ†t debis fina en da ip np Dro ex fii nm ita it oi non 2.of Σ−t 1 by the MGR algo-
 
where ∆ t(a
|
x) = φ(a,x),θ
t
−
φ(a ∗t(x),x),θ
t
,
is
W ubh ne bn iaseΣ d†t for=
θ
Σ−t fr1 omho Elds,
θ
(Σth )is estimato =r
a hn od ldsf .or all a
∈
[K]naDnd any xE
∈
XD, ∆ t(a
|
x) ≥E∆o
∗ E Σ−t 1φ A te ,X
t
ℓ t(A
tt
,X t)
|Ft −1
h=
bt Σ−tt
1Σ| tF
θt
t−
=1
iθ t.
A linear contextual adversarial regime with a self- (cid:2) (cid:0) (cid:1) (cid:3)
bounding constraint encompasses several important FTRL with the α-Tsallis entropy and θ t Σ†t .
settings. For examples, see examples in Kato & Ito (cid:16) (cid:17)
(2023). Notethatinanadversarialregime,thereexists
By using the regression coefficient estimator θ bt Σ e†t ,
we define our proposed algorithm, the α-LC-T(cid:16)salli(cid:17)s-
a ∈ [K] and x ∈ X such that ∆ t(x) := φ(a,x),θ t − INF. b e
φ(a (x),x),θ < 0. This is becauseD ∆(a x) E and In each round t [T], the α-LC-Tsallis-INF selects
∗T t | an arm with the fol∈ lowing policy:
∆Ds(a x) can taEke a different value for some a [K]
| ∈
and x if t=s. π (X ):=(1 γ )q (X )+γ e (X ), (2)
∈X 6 t t − t t t t ∗ t
where recall that e is an explorationpolicy defined in
5 Algorithm ∗
Assumption 2.3, and
In this section, we propose an algorithm for lin- t 1
− 1
ear contextual bandits with adversarial corruption. q t(x):=argmin ℓ s(x),q + ψ q , (3)
η
We refer to our proposed algorithm as the α-Linear- q ∈PK ( Xs=1D E t (cid:0) (cid:1))
Contextual (LC)-Tsallis-INF because it modifies the q 1(x):=(1/K 1/K b1/K) ⊤,
···
Tsallis-INF (Zimmert & Seldin, 2021), which is an
FTRL-based algorithm with the Tsallis entropy regu- ψ(q(x)):= 1 1 qα(a x) .
larizationwihoutcontexts. Here,α (0,1)isaparam- α − | 
eter of the Tsallis-entropy. While
w∈
e provide our algo-
aX∈[K]
 
rithm for general α, we show the regret upper bound Here, the regularizer ψ(q(x)) is referred to as the α-
onlyfor1/2. Thepseudo-codeisshowninAlgorithm1. Tsallisentropy(Zimmert & Seldin,2021). Wehavenot
7yetspecifiedη andγ . Theseparametersaredefinedin Algorithm 1 α-LC-Tsallis-INF.
t t
the following sections, depending on two cases: when Parameter: Learning rate η ,η ,...,η >0.
1 2 T
the true Σ−t 1 is given, and it is approximated using for t=1,...,T do
finitesamplesfrom withtheMGRalgorithm,respec- Observe X .
G t
tively. DrawA [K]followingthepolicyπ (X ):=(1
t t t
∈ −
γ )q (X )+γ e (X ) defined in (2).
t t t t ∗ t
5.2 Theα LC Tsallis INFwithΣ−t 1 Observe the loss ℓ t(A t,X t).
Compute θ .
t
First, we consider a case where Σ−t 1 is available un- end for
derthefullinformationabout . Todefinetheremain- b
G
ing parameters, we introduce the following notations:
Algorithm 2 (Modified) MGR (Neu & Olkhovskaya,
for each t [T],
∈ 2020).
Input: Context distribution and policy π .
a†t(x):=argmaxq t(a |x),
for k =1,...,M do
G
t
a [K] t
∈
for j =1,...,k do
ω
t
:= xs ∈u Xp am ∈[i Kn
]
(cid:8)1 −q t(a |x) (cid:9)= xs ∈u Xp n1 −q
t
(cid:0)a†t(x) (cid:1)o.
D Cora mw puX te(k U,j)
(k∼ ,j)G
=.
Then, we specify the parameters as γ t =min 1,γ t , a [K]π t(a |X(k,j))φ a,X(k,j) φ ⊤ a,X(k,j) .
end fo∈r
(cid:8) (cid:9) CoP mpute V(k) = k (I(cid:0) δU(k,j)(cid:1) ). (cid:0) (cid:1)
η t :=c † √K , γ t := 8C Z2C ℓ η t√ω te , end for j=1 −
v u ut d+ λm1
in
λ min Return: Σ†t =δI +Q δ M k=t 1V(k).
t (cid:16) (cid:17) e
P
where c >0 is some universal constant. b
†
IfM isfinite, weanalyzeanapproximationerrorof
t
5.3 Theα LC Tsallis INFwithΣ−t 1 Σ−t 1 and discuss the influence on regret upper bounds.
In such a case, we set the parameters as
Next,weconsideracasewhereΣ−tb1isapproximated
with finite samples from by the MGR algorithm. In √K
this case, our remaining tG ask is to estimate Σ−t 1. The η t :=c† vt d+ log(T) , γ t :=min 1,γ t ,
difficulty of this task stems from the dependency on u u λmin (cid:8) (cid:9)
A t, which varies across rounds. To address this issue,
γ :=
8Ct2C(cid:16) ℓlog(T)
η
√(cid:17)
ω , M :=
1e
1,
we employ the MGR proposed by Neu & Olkhovskaya t Z λ t t t 8η √ω −
min t t
(2020).
1
The MGR algorithm assumes that we have access aend δ := ,
C2C
to the distribution
G
of X
t
and estimates Σ−t 1 by
Z
ℓ
using simulations with finite samples generated from
where c >0 is some universal constant.
. We show the MGR algorithm in Algorithm 2. †
G The MGR algorithm requires contextual distribu-
Note that the we slightly modified the MGR algo-
w
Erit hh
i
1lm
e
Ni [n AeuN =&eu
aO
]&
φlk
(O
h ao
,lk Xvh sko )av φys aka
((
ay
2
,a
0
X2( 02 ))02
e
i0
s
n)
ti
tmt ho
ea
ite res
s
sti em
Σ
tt−ta int 1e
g(a
wΣ
)
i−t
:
t=1 h, (t
a
2i to 0tn
2em
1G
)p
ata nn
t
dod Lrp
ie
uo lal ei xc ty
at
lhπ .it
(s
2i 0an
2ss
3s
u
)i
m
,m bpu ytla
i
iot nni co
,
un r.
s ru ic
nS
h
gev
sa
oe sr ma
L
el
u
cs ot ou med
t
pi ae us
l -.
− t 0 ⊤ 0
tational costs (Table 1).
arm-independent contexts.
(cid:2) (cid:3)
InAlgorithm2,wedefine U(k,j) forwhichE[U(k,j)
] = Σ holds. Here, we also have E[Vk ] =| 6 Regret Analysis for the 1/2-LC-
t 1 t t 1
F − | F −
E k I δU(k,j) =(I δΣ )k. Therefore, Tsallis-INF
j=1 − |Ft −1 − t
Σ†th Qworks(cid:0)as a good e(cid:1)stimatoir of Σ−t 1 on expectations
w bhen M
t
=
∞
because we have E Σ†t
|Ft −1
= δI + TsaW lle is-p Ir No Fvid ae lgu op rp ite hr mbo wu hn ed nsf αor =th 1e /r 2eg (r 1e /t 2o -f Lt Ch -e Tα sa-L llC is-
-
δ ∞ k=1(I −δΣ t)k =E Σ†t |Ft −1 =h bδI +δ i ∞ k=1(I
−
INF).
δΣ Pt)k =δ ∞ k=0(I −δΣh t b)k =δ(δΣi −t 1) −1 =Σ P−t 1. For the 1/2-Tsallis-INFwith a regressioncoefficient
P
8
1estimator θ t Σ†t , let us define β = 1, then the regret is upper bounded by R T =
1 2
ξTsallis (cid:16)Σb
†t(cid:16)
(cid:17)e
:=(cid:17)

√ √K K(cid:16)d d+
+
λ
lo
λm1
g
m(in
iT n(cid:17))
ii ff Σ
Σ
e†t
†t
= =Σ Σ−t−t 11 .
cO
on(cid:18) L tna exs∆
t1
tl∗ uyo a,
l3
w ane
dL
vd
eξ
e
rT
r
sis ava rl el ii as
a l(cid:16)
rrΣ
e
ee
g†t
g i(cid:17)r
mel to eg wu( pT
itp
h)
eor
a3 sbT
eo lu
f1 3
-(cid:19) n bd
o.
ui nn da inglin ce oa nr
-
straint, which is a generalization of the stochastic and
e (cid:16) (cid:17)
This is one of theleading factors in our upeper bbounds. adversarialregimesunderβ = inamargincondition.
∞
Then, we show the following upper bound, from We provide the proof in Appendix A.6.
which we derive upper bounds in adversarial and
Theorem 6.4 (Regret upper bound in a linear con-
stochasticregimes. WeshowtheproofinAppendixA.1
textual adversarial regime with a self-bounding con-
and A.4.
straint). Consider the 1/2-LC-Tsallis-INF with the es-
Theorem 6.1 (General regret bounds). Consider the timator θ
t
Σ†t . Assume that the loss is generated un-
1/2-LC-Tsallis-INF with the estimator θ t Σ†t . As- der a linea(cid:16)r co(cid:17)ntextual adversarial regime with a self-
sumptions 2.1 and 2.3–2.5 hold. Then, the(cid:16)reg(cid:17)ret sat- boundingbconestraint (Definition 4.2). Suppose that As-
isfies b e sumption 2.1–2.5 hold. Then,
R T =OE 
T rξTsall √is t(cid:16)Σ†t (cid:17)ω t
.
R
T
=O
LξTsa ∆llis ∗(cid:16)Σ e†t
(cid:17)log(T) +C.
 Xt=1 e   
Because ω t
  1, 
by replacing ω t
with  1 
in Theo-
∆hol Tds ,. thM
e
ro er ge ro ev ter s, atf io sfir esK ∆− ∗1 (cid:16)log (cid:16)KT∆ −2 ∗ 1 (cid:17)+1
(cid:17)
≤ C ≤
≤
rem 6.1, we can directly obtain a regret upper bound ∗
in the adversarialregime in the following theorem.
CLξTsallis Σ†t
∆ T
Theorem 6.2 (Regret upper bound in an adversar- R T =Ov u ∆ (cid:16) (cid:17) slog C∗ .
ialregime). Consider the 1/2-LC-Tsallis-INF with the u t ∗ e (cid:18) (cid:19) 
 
estimator θ
t
Σ†t . Assume that the loss is generated
Theorem
6
.4 implies that the regret
satisfies
R =
T
under an ad(cid:16)vers(cid:17)arial regime. Suppose that Assump-
tions 2.1 abnde2.3–2.5 hold. Then, the regret satisfies
LξTsallis Σ†t CLξTsallis Σ†t
∆ T
R T =O (cid:18)rξTsallis (cid:16)Σ†t (cid:17)T (cid:19). O ∆ ∗(cid:16)
e
(cid:17)log(T)+v u
u
t
∆
∗
(cid:16)
e
(cid:17) slog +
(cid:18)
C∗ (cid:19)!,
Note that this reesult does not require Assump- tightly depending on T. Recall log (x) =
+
tions 2.2, which restricts the context support to be max 1,log(x) .
{ }
finite.
Next, we show a regretupper bound in a stochastic 7 Conclusion
regimewith a margincondition, which depends onthe
parameterβ (0, ]. Recallthatβ decidesthebehav-
∈ ∞ We presented a BoBW algorithm for linear con-
ior of ∆(x). The proof is shown in Appendix A.5.
textual bandits whose regret upper bounds tightly
Theorem 6.3 (Regret upper bound in a stochastic depend on T. By extending the existing FTRL-
regime with a margin condition). Consider the 1/2- based approach provided by Kato & Ito (2023) and
Kuroki et al. (2023), we developed an FTRL-based al-
LC-Tsallis-INF with the estimator θ
t
Σ†t . Assume
gorithm employing the Tsallis-entropy. Our proposed
thattheloss isgeneratedunderastocha(cid:16)stic(cid:17)regimewith
algorithm,the 1/2-LC-Tsallis-INF,exhibits tighter de-
a margin condition (Definition 4.1).bSupepose that As-
pendence on T compared to the algorithm proposed
sumption 2.1–2.5 hold. Then, the regret satisfies R =
T by Kato & Ito (2023) and Kuroki et al. (2023) in the
β stochastic regime. Compared to the reduction ap-
O (cid:26)1 β+
∆
∗β (cid:27)2+β nLξTsallis (cid:16)Σ†t (cid:17)log(T) o21 ++ ββ T2+1 β !. p toro ia mc ph leb my eK ntur ao nk di e itt sa rl e. g( r2 e0 t2 h3 a), so au tr iga hlg teo rrit dh em penis dee na cs yy
e regarding T. Furthermore, we derived a regret up-
For example, when β = , then the regret is upper
per bound dependent on a margin condition. Specif-
∞
bounded as R T =O ∆1 ∗LξTsallis Σ†t log(T) . When ically,whenweknowthefullinformationabout G,ina
(cid:16) (cid:16) (cid:17) (cid:17)
e
9stochastic regime with a margin condition, we derived Beygelzimer, A., Langford, J., Li, L., Reyzin, L., and
a regret upper bound Schapire, R. Contextual bandit algorithms with su-
pervised learning guarantees. In International Con-
β 1+β
O 1+β 2+β L√K d+ 1 log(T) 2+β T2+1 β , ference on Artificial Intelligence and Statistics (AIS-
β∆ λ TATS), 2011.
(cid:26) ∗ (cid:27) (cid:26) (cid:18) min(cid:19) (cid:27) !
where β is a parameter of the margin condition. In Bubeck, S. and Slivkins, A. The best of both worlds:
the linear contextual bandit with a self-bounding con- Stochasticandadversarialbandits. InConferenceon
straint, we derived a regret upper bound Learning Theory (COLT), 2012.
L√K d+ 1 Chen, C., Luo, L., Zhang, W., Yu, Y., and Lian, Y.
O λmin log(T) Efficient and robust high-dimensional linear contex-
∆
(cid:0) (cid:1)
∗ tual bandits. In International Joint Conference on
CL√K d+ 1 ∆ T Artificial Intelligence (IJCAI), 2020.
+ s (cid:0)∆
∗
λmin (cid:1)slog +
(cid:18)
C∗ (cid:19)!,
Chu, W., Li, L., Reyzin, L., and Schapire, R. Con-
textual bandits with linear payoff functions. In In-
where C is a parameter of an adversarial corruption.
ternational Conference on Artificial Intelligence and
Additionally, in the adversarial regime, our algorithm
Statistics (AISTATS), 2011.
hasO L√K d+ 1 T regretupperbound. The
λmin
import(cid:16)aqnt rema (cid:0)ining pro (cid:1)ble(cid:17)m is to improve the depen- Dani, V., Hayes, T. P., and Kakade, S. M. Stochas-
dence on T when β (0, ) in a stochastic regime. tic linear optimization under bandit feedback. In
∈ ∞ Annual Conference Computational Learning Theory
(COLT), 2008.
Broader Impact
Dann, C., Wei, C.-Y., and Zimmert, J. A blackbox
This paper presents work whose goal is to advance approach to best of both worlds in bandits and be-
the field of the theory of the multi-armed bandit prob- yond. InNeu, G. andRosasco,L. (eds.), Conference
lem. There are many potential societal consequences on Learning Theory (COLT), volume 195, pp. 5503–
of our work,none of which we feel must be specifically 5570. PMLR, 2023a.
highlighted here.
Dann, C., Wei, C.-Y., and Zimmert, J. Best of both
References worldspolicy optimization. In International Confer-
ence on Machine Learning (ICML), 2023b.
Abbasi-yadkori, Y., Pa´l, D., and Szepesv´ari, C. Im-
Ding, Q., Hsieh, C.-J., and Sharpnack, J. Robust
proved algorithms for linear stochastic bandits. In
stochastic linear contextual bandits under adversar-
Advances in Neural Information Processing Systems
ialattacks. InInternational Conference on Artificial
(NeurIPS), 2011.
Intelligence and Statistics (AISTATS), 2022.
Abe, N. and Long, P. M. Associative reinforcement
Goldenshluger,A.andZeevi,A. Alinearresponseban-
learningusinglinearprobabilisticconcepts. InInter-
dit problem. Stochastic Systems, 2013.
national Conference on Machine Learning (ICML),
1999.
Gupta, A., Koren, T., and Talwar, K. Better algo-
rithmsforstochasticbanditswithadversarialcorrup-
Auer, P. and Chiang, C.-K. An algorithm with nearly
tions. In Conference on Learning Theory (COLT),
optimalpseudo-regretforbothstochasticandadver-
2019.
sarial bandits. In Conference on Learning Theory
(COLT), 2016.
Hazan, E., Koren, T., Livni, R., and Mansour, Y. On-
Auer,P.,Cesa-Bianchi,N.,andFischer,P. Finite-time line learning with low rank experts. In Conference
analysisofthemultiarmedbanditproblem. Machine on Learning Theory (COLT), 2016.
Learning, 47(2):235–256,2002.
He, J., Zhou, D., Zhang, T., and Gu, Q. Nearly op-
Bastani, H. and Bayati, M. Online decision making timal algorithms for linear contextual bandits with
with high-dimensional covariates. Operations Re- adversarial corruptions. In Advances in Neural In-
search, 68(1), 2020. formation Processing Systems (NeurIPS), 2022.
10Ito, S., Tsuchiya, T., and Honda, J. Nearly opti- Lykouris, T. and Vassilvtiskii, S. Competitive caching
mal best-of-both-worlds algorithms for online learn- with machine learned advice. In International Con-
ing with feedback graphs. In Advances in Neural ference on Machine Learning (ICML), 2018.
Information Processing Systems (NeurIPS), 2022.
Masoudian,S. andSeldin, Y. Improvedanalysisofthe
Jin, T., Liu, J., and Luo, H. Improved best-of-both- tsallis-inf algorithmin stochasticallyconstrainedad-
worldsguaranteesformulti-armedbandits: Ftrlwith versarial bandits and stochastic bandits with adver-
general regularizers and multiple optimal arms. In sarial corruptions. In Conference on Learning The-
Advances in Neural Information Processing Systems ory (COLT), 2021.
(NeurIPS), 2023.
Neu, G.andOlkhovskaya,J. Efficientandrobustalgo-
Kanade,V. andSteinke, T. Learninghurdlesfor sleep- rithms for adversarial linear contextual bandits. In
ing experts. ACM Transactions on Computation Conference on Learning Theory (COLT), 2020.
Theory, 6(3), 2014.
Rakhlin, A. and Sridharan, K. Bistro: An effi-
Kato,M.andIto,S. Best-of-both-worldslinearcontex- cientrelaxation-basedmethodforcontextualbandits.
tual bandits, 2023. arXIv:2312.16489. In International Conference on Machine Learning
(ICML), 2016.
Kong, F., Zhao, C., and Li, S. Best-of-three-
worlds analysis for linear bandits with follow-the-
Rouyer, C. and Seldin, Y. Tsallis-inf for decoupled
regularized-leader algorithm. In Conference on
explorationandexploitationinmulti-armedbandits.
Learning Theory (COLT), 2023.
In Conference on Learning Theory (COLT), volume
125, pp. 3227–3249.PMLR, 2020.
Kuroki, Y., Rumi, A., Tsuchiya, T., Vitale, F., and
Cesa-Bianchi,N. Best-of-both-worldsalgorithms for
Seldin,Y.andLugosi,G.Animprovedparametrization
linear contextual bandits, 2023.
andanalysisoftheEXP3++algorithmforstochastic
and adversarial bandits. In Conference on Learning
Lattimore, T. and Szepesv´ari, C. Bandit Algorithms.
Theory (COLT), 2017.
Cambridge University Press, 2020.
Seldin, Y. and Slivkins, A. One practical algorithm
Lee,C.-W.,Luo,H.,Wei,C.-Y.,Zhang,M.,andZhang,
forbothstochasticandadversarialbandits. InInter-
X. Achievingnearinstance-optimalityandminimax-
national Conference on Machine Learning (ICML),
optimality in stochastic and adversarial linear ban-
2014.
dits simultaneously. In International Conference on
Machine Learning (ICML), 2021.
Syrgkanis, V., Luo, H., Krishnamurthy, A., and
Li, K., Yang, Y., and Narisetty, N. N. Regret lower Schapire, R. E. Improved regret bounds for oracle-
bound and optimal algorithm for high-dimensional basedadversarialcontextualbandits. InAdvancesin
contextual linear bandit. Electronic Journal of Neural Information Processing Systems (NeurIPS),
Statistics, 15(2):5652 – 5695, 2021. 2016.
Li, L., Chu, W., Langford, J., and Schapire, R. E. A Tewari, A. and Murphy, S. A. From ads to interven-
contextual-banditapproachto personalizednews ar- tions: Contextual bandits in mobile health. In Mo-
ticle recommendation. In International Conference bile Health: Sensors, Analytic Methods, and Applica-
on World Wide Web (WWW), 2010. tions, pp. 495–517,2017.
Liu, H., Wei, C.-Y., and Zimmert, J. Bypassing the Tsuchiya, T., Ito, S., and Honda, J. Best-of-both-
simulator: Near-optimal adversarial linear contex- worlds algorithms for partial monitoring. In Inter-
tual bandits. In Advances in Neural Information nationalConferenceonAlgorithmicLearningTheory
Processing Systems (NeurIPS), 2023. (ALT), 2023a.
Luo, H., Wei, C.-Y., and Lee, C.-W. Policy optimiza- Tsuchiya, T., Ito, S., and Honda, J. Stability-
tion in adversarial mdps: Improved exploration via penalty-adaptivefollow-the-regularized-leader: Spar-
dilated bonuses. In Advances in Neural Informa- sity, game-dependency, and best-of-both-worlds. In
tion Processing Systems (NeurIPS), volume 34, pp. Advances in Neural Information Processing Systems
22931–22942,2021. (NeurIPS), 2023b.
11Wang, X., Wei, M., and Yao, T. Minimax con-
cave penalized multi-armed bandit model with high-
dimensional covariates. In International Conference
on Machine Learning (ICML), 2018.
Wei, C.-Y. and Luo, H. More adaptive algorithms for
adversarialbandits. InConference on Learning The-
ory (COLT), 2018.
Zhao, H., Zhou, D., and Gu, Q. Linear contextual
bandits with adversarial corruptions, 2021. URL
https://openreview.net/forum?id=Wz-t1oOTWa.
Zimmert,J.andSeldin,Y.Tsallis-inf: Anoptimalalgo-
rithmforstochasticandadversarialbandits. Journal
of Machine Learning Research, 22(1), 2021.
12A Proof of Theorems 6.1 and 6.4
ThissectionprovidestheproofsforTheorem6.1. InAppendixA.1,weprovidepreliminaryresultsfortheproof.
Then, inAppendix A.2, we decomposethe regretinto the stability andpenalty terms. In Appendix A.3, we upper
bound those terms. Lastly, in Appendix A.4, we prove Theorem 6.1.
A.1 PreliminariesfortheProofofTheorem6.1
LetX beasamplefromthecontextdistribution independentof . LetD(p,q)denoteaBregmandivergence
0 T
G F
between p.q Π with respect to ψ ; that is,
t
∈
D(p,q):=ψ(p) ψ(q) ψ(q),p q .
− − ∇ −
D E
In our proof, the following proposition plays an essential role.
Proposition A.1 (From Lemma 4.4 in Kato & Ito (2023)). Consider the 1/2-LC-Tsallis-INF with our defined
parameters. Then, the regret satisfies
T
1 1 1
R E C γ + ℓ (X ),q (X ) q (X ) D q (X ),q (X ) H q (X )
T ℓ t t 0 t 0 t+1 0 t+1 0 t 0 t 0
≤ "
Xt=1n D
− E− η t
(cid:0)
(cid:1)− (cid:18)η t − η t −1(cid:19)
(cid:0)
(cid:1)#
bT
+2 max E X ,θ (a) θ (a) ,
t t t
a [K] h − i
Xt=1 ∈ (cid:12) h i(cid:12)
(cid:12) (cid:12) =biasterm b (cid:12) (cid:12)
where | {z }
H q(x) :=2 q(x x+1
− | 
(cid:0) (cid:1)
aX∈[K]
p
 
For the completeness, we show the proof below.
Proof. Let us define π Π as π (a (x) x)=1 and π (a x)=0 for all a [K] a (x) for any x .
∗
∈
∗ ∗T
|
∗
| ∈ \{
∗T
} ∈X
Recall that in (3), we defined q as
t
t 1
− 1
q (x):=argmin ℓ (x),q + ψ q
t s
η
q ∈PK ( Xs=1D E t
(cid:0)
(cid:1))
b
for t 2.
≥
From the definition of our algorithm, we have
T
R =E ℓ (A ,X ) ℓ (a (X ),X )
T
"
t t t
−
t ∗T t t
#
Xt=1n o
T
=E ℓ (X ),π (X ) π (X )
t t t t ∗ t
h − i
" #
t=1
X
T T
=E ℓ t(X t),q t(X t) π∗(X t) + γ
t
ℓ t(X t),e∗(X t) q t(X t)
" h − i h − i#
t=1 t=1
X X
T T
E ℓ (X ),q (X ) π (X ) +C γ
t t t t ∗ t ℓ t
≤ " − #
Xt=1D E Xt=1
T T
=E ℓ (X ),q (X ) π (X ) +C γ
t 0 t 0 ∗ 0 ℓ t
" − #
Xt=1D E Xt=1
13T T T
=E ℓ (X ),q (X ) π (X ) +C γ +E ℓ (X ) ℓ (X ),q (X ) π (X )
t 0 t 0 ∗ 0 ℓ t t 0 t 0 t 0 ∗ 0
" − # " − − #
Xt=1D E Xt=1 Xt=1D E
T b T T b
E ℓ (X ),q (X ) π (X ) +C γ +2 max E X ,θ (a) θ (a) . (4)
t 0 t 0 ∗ 0 ℓ t 0 t t
≤ " − # a [K] −
Xt=1D E Xt=1 Xt=1 ∈ (cid:12) hD Ei(cid:12)
b (cid:12) b (cid:12)
We show that for any x and any p , we have (cid:12) (cid:12)
∗ K
∈X ∈P
T T
1 1 1
ℓ t(x),q t(x) p∗(x) ℓ t(x),q t(x) q t+1(x) D q t+1(x),q t(x) H q t(x) . (5)
− ≤ − − η − η − η
Xt=1D E Xt=1(cid:26)D E t (cid:0) (cid:1) (cid:18) t t −1(cid:19) (cid:0) (cid:1)(cid:27)
b b
This result follows from the definition of q ; that is,
t
T
1
ℓ (x),p (x) + ψ p (x)
t ∗ ∗
η
* + T
t=1
X (cid:0) (cid:1)
Tb
1
ℓ (x),q (x) + ψ q (x)
t T+1 T+1
≥* + η T
t=1
X (cid:0) (cid:1)
T 1b
− 1
ℓ (x),q (x) + ℓ (x),q (x) + ψ q (x)
t T+1 T T+1 T+1
≥* + η T
Xt=1 D E (cid:0) (cid:1)
T 1b b
− 1 1
ℓ (x),q (x) + ℓ (x),q (x) + ψ q (x) + D q ,q
t T T T+1 T T+1 T
≥* + η T η T
Xt=1 D E (cid:0) (cid:1) (cid:0) (cid:1)
T b b
1 1 1
ℓ (x),q (x) + ψ q (x) + D q ,q .
t t+1 t t+1 t
≥ η − η η
Xt=1(cid:26)D E (cid:18) t −1 t(cid:19)
(cid:0) (cid:1)
t
(cid:0)
(cid:1)(cid:27)
b
Combining (5) with (4) yields the statement.
Facts on the Tsallis entropy. The Bregman divergence associated with the α-Tsallis entropy
1
ψ(q(x))= qα(a x)+1
−α t | 
aX∈[K]
 
is given as the following lemma.
Lemma A.2. For any x , α (0,1), the α-Tsallis entropy ψ(q(x)) = 1 qα(a x)+1 and
∈ X ∈ −α a [K] t |
∈
p(x),q(x) ∈PK(K), the Bregman divergence is given as (cid:16)
P
(cid:17)
1
D p(x),q(x) = qα(a x)+α p(a x) q(a x) qα 1(a x) pα(a x) = d p(a x),q(a x) ,
−
α | | − | | − | | |
(cid:0) (cid:1)
aX∈[K]
(cid:0) (cid:0) (cid:1) (cid:1)
aX∈[K]
(cid:0) (cid:1)
where for p,q (0,1), we define
∈
d p,q := 1 qα+(p q)qα −1 1 pα 1 −α min p,q α −2 p q 2 .
α − − α ≤ 2 { } −
(cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1)
Estimation error of Σ−t 1. When estimating Σ−t 1, we incur its estimation error. To measure the estimation
error,we focus on bias of θ t(Σ†t) cased from the estimation error,which is upper bounded as follows:
Lemma A.3. Suppose th bat bAssumptions 2.1–2.5 hold. Then, under the MGR with γ
t
:= 8C Z2 δC λℓ ml io ng(T)η t√ω t,
M = 1 1 and δ = 1 , we have
t 8ηt√ωt − C Z2Cℓ
C C
E hX t,θ t −θ t(Σ†t)
i ≤
Z
T
Θ .
(cid:12) h i(cid:12)
(cid:12) (cid:12) b b (cid:12) (cid:12)
14Proof. As well as the proof of Lemma 5 in Neu & Olkhovskaya(2020), we have
E φ(a,X ),θ θ C C exp( γ δλ M ).
t t t Θ t min t
h − i ≤ Z −
(cid:12) h i(cid:12)
Then, we have (cid:12) (cid:12) b (cid:12) (cid:12)
8C2C log(T)
ℓ
exp( γ tδλ minM t)=exp
Z
η t√ω
t
δλ minM
t
− − λ ·
(cid:18) min (cid:19)
exp( 8log(T)η √ω M )
t t t
≤ − ·
1 1
exp 8log(T)η √ω 1 exp( log(T))= ,
t t
≤ (cid:18)− · (cid:26)8η t√ω
t
− (cid:27)(cid:19)≤ − T
where recall that we defined M = 1 1
t 8ηt√ωt −
Upper bound of the estimator of x ⊤Σ−t 1z for x,z
∈
X. In our analysis, we are requred to upper bound
x ⊤Σ−t 1X
t
forany x ∈X. This sectionprovidesanupper boundofx ⊤Σ−t 1z forx,z ∈X, whichalsogivesanupper
bound for E π (a X )ℓ2(a X ) . e
e t | 0 t | 0 |Ft −1 b
h i
Lemma A.4. When Σb −t 1 = Σ−t 1, we consider the MGR with M t = 8ηt1 √ωt −1 and δ = C Z21 Cℓ. Suppose that
Assumptions 2.1, 2.3, and 2.5 hold. Then, for any x,z , it holds that
∈Z
e b
1 1
x ⊤Σ−t 1z
≤ C ℓ8η t√ω
t
e
for Σ−t 1
∈
Σ−t 1,Σ−t 1 .
n o
Prooef. When Σ−t 1b=Σ−t 1, the result follows from
e b M
x⊤Σ−t 1z ≤C Z2 Σ−t 1
op
≤C Z2δ 1+ Vk
op
!≤C Z2δ(M +1)=(M +1)/C ℓ. (6)
(cid:12) (cid:12) (cid:12) e (cid:12) (cid:12) (cid:12) (cid:13) (cid:13)b (cid:13) (cid:13) m X=1 (cid:13) (cid:13) (cid:13) (cid:13)
When Σ−t 1 =Σ−t 1, we obtain the result from Assumption 2.5 and the Cauchy-Schwarz inequality.
Thiselemma directly yields the following lemma.
Lemma A.5. When Σ−t 1 = Σ−t 1, we consider the MGR with M
t
= 4ηt1
√ωt
−1 and δ =
C
Z21 Cℓ. Suppose that
Assumptions 2.1–2.5 hold. Then, for any x,z , from (6), it holds that
e b ∈Z
1
ℓ t(a,x) = x ⊤(a)Σ†tφ ⊤(a,X t)ℓ t(a,X t)
≤
C Z2δ(M +1) ·C
ℓ
≤ 8η t√ω
t
(cid:12) (cid:12) (cid:12) (cid:12) n o
(cid:12) (cid:12)b (cid:12) (cid:12) (cid:12) (cid:12) e (cid:12) (cid:12)
for Σ−t 1
∈
Σ−t 1,Σ−t 1 .
n o
Prooef. When Σ−t 1b=Σ−t 1, from Lemma A.4, the result follows from
1 1 1
e b
ℓ (a,x) C = .
t ℓ
≤ C ℓ8η t√ω
t
8η t√ω
t
(cid:12) (cid:12)
(cid:12)e (cid:12)
(cid:12) (cid:12)
When Σ−t 1 =Σ−t 1, the result follows from ℓ t(a,x) ≤C
ℓ
x ⊤(a)Σ†tφ ⊤(a,X t) ≤C ℓC1 ℓ8ηt1 √ωt.
(cid:12) (cid:12) (cid:12) (cid:12)
e (cid:12) (cid:12)b (cid:12) (cid:12) (cid:12) (cid:12) e (cid:12) (cid:12)
15Margin condition. To utilize a margin condition for deriving a regret upper bound, we show the following
lemma. The proof is shown in Appendix B.
Lemma A.6. Let U [0,1] and V R be some random variables. Let ∆ > 0 be some universal constant. For
β (0, ], if the rand∈ om variable U∈ has a mean E[U]=µ and the random∗ variable V satisfies
∈ ∞
β
1 h
F(h):=P V h
≤ ≤ 2 ∆
(cid:18) ∗(cid:19)
(cid:0) (cid:1)
for any h [0,∆ ], and µ 0,1 holds, then
∈ ∗ ∈ 2
(cid:2) (cid:3) E UV ∆ ∗β (2µ)1+ ββ
≥ 2(1+β)
(cid:2) (cid:3)
holds.
A.2 StabilityandPenaltyDecomposition
Following the standard analysis of FTRL methods, we decompose the regret into stability and penalty terms.
Based on the result in Lemma A.1, let us define r (x) as
T
T
1 1 1
r (x):=E C γ + ℓ (X ),q (X ) q (X ) D(q (X ),q (X )) H q (X ) X =x .
T ℓ t t 0 t 0 t+1 0 t+1 0 t 0 t 0 0
"
Xt=1n D
− E− η t − (cid:18)η t − η t −1(cid:19)
(cid:0)
(cid:1)| #
b
This function r (x) bounds R as
T T
T
R E r (X ) +2 max E X ,θ (a) θ (a) .
T T 0 t t t
≤ a [K] h − i
(cid:2) (cid:3) Xt=1 ∈ (cid:12) (cid:12) h b i(cid:12) (cid:12)
(cid:12) =biasterm (cid:12)
ThestandardFTRLanalysisbreaksthepseudo|-regretintopena{zltyandstability}terms. Followingthisapproach,
we decompose the pointwise regret upper bound r (x) as follows:
T
T
1
r (x)=E C γ + ℓ (X ),q (X ) q (X ) D(q (X ),q (X )) X =x
T ℓ t t 0 t 0 t+1 0 t+1 0 t 0 0
" − − η t | #
Xt=1n D E
b
=stabilityterm
T
| 1 1 {z }
E H q (X ) X =x . (7)
t 0 0
− " Xt=1(cid:18)η t − η t −1(cid:19)
(cid:0)
(cid:1)| #
=penaltyterm
where L (x)= t ℓ (x). | {z }
t s=1 s
We show upper bounds by upper bounding the stabioity, penalty, and bias terms.
P
b b
A.3 UpperBoundsfortheStability,Penalty,andBiasTerms
In this section, for any x , we derive upper bounds for the following terms separately:
∈X
1
stability (x):=C γ + ℓ (x),q (x) q (x) D(q (x),q (x)), (8)
t ℓ t t t − t+1 − η t+1 t
t
D E
1 1
penalty (x):= b H q (X ) , (9)
t − η − η t 0
(cid:18) t t −1(cid:19)
T (cid:0) (cid:1)
bias:= max E X ,θ (a) θ (a) . (10)
t t t
a [K] h − i
Xt=1 ∈ (cid:12) h i(cid:12)
(cid:12) b (cid:12)
(cid:12) (cid:12)
16(I) Bounding the stability term. To bound the stability term, we obtain the following lemma. Recall that
for any x and q(x) , we defined a (x) argmax q(a x).
∈X ∈PK † ∈ a ∈[K] |
Lemma A.7. Consider the 1/2-LC-Tsallis-INF with the estimator θ
t
Σ†t . Assumptions 2.1, and 2.3–2.5 hold.
If (cid:16) (cid:17)
1 α b e α 1
ℓ t(a x) − min q(a †(x) x),1 q(a †(x) x) −
| | |≤ 4 | − |
holds for all a [K] a , the we have (cid:8) (cid:9)
†
∈ \
1
ℓ (x),q(x) p(x) D(p(x),q(x))
t
− − η
t
D E
≤b
1
4
α
q2 −α(a |x)ℓ2 t(a |x)+min q(a †(x) |x),1 −q(a †(x) |x) 2 −α ℓ2 t(a †(x),x) .
− a 6=Xa†(x)
(cid:8) (cid:9)
 
The proofis shownin Appendix C.1. By using this lemma, we show the followingupper bound for the stability
term.
Lemma A.8 (Upper bound for the stability term). Consider the 1/2-LC-Tsallis-INF with the estimator θ
t
Σ†t .
Assumptions 2.1, and 2.3–2.5 hold. Consider T 0 [T] such that (cid:16) (cid:17)
∈ b e
T 0 ≥
4
4(cid:16)
(cid:16)8 8C CλmZ Z2 2iC
C
λn
mℓ ℓc
l
ino†
g(cid:17)
(T2
)(cid:16) cd †+
(cid:17)√
λ
2K
m1 (cid:16)in d+(cid:17)
√
l
λoK
g m( iT n)
(cid:17)
ii ff ΣΣ e−t−t 11 == ΣΣ −t−t 11
. (11)
If min aη tℓ t(a,x)=min aη tφ
⊤
a,x
Σ†tφ
a,X
t
ℓ t(a,X t) ≥−1 holds,
thene
for
alb
l t ≥T
0
(cid:0) (cid:1) (cid:0) (cid:1)
b steability (x) 2η √ω ℓ2(a x)π (a x)
t ≤ t t t | t |
aX∈[K]
b
holds, where recall that the stability term stability (x) is defined in (8).
t
Note thatthe condition“t T ”isintroducedtospecify arangeoft,whereγ 1/2holdsfromthe definitions
0 t
of γ t, γ t, and η t. When Σ−t 1 i≥ s given, recall that we have γ
t
=min {1,γ
t
}, ≤
e
√K 8C2C
ℓ
η
t
=c
†
, and γ
t
=
Z
η t√ω t.
vt d+ 1 λ min
u
u
λmin
t (cid:16) (cid:17) e
Here. we have
8C2C √K
ℓ
γ c .
t Z †
≤ λ min vt d+ 1
u
u
λmin
e t (cid:16) (cid:17)
Then, we have
8C2C √K
ℓ
γ c .
t Z †
≤ λ min vt d+ 1
u
u
λmin
t (cid:16) (cid:17)
Therefore, γ 1/2 holds for t>T such that
t 0
≤
8C2C 2 √K
ℓ
T 4 c . (12)
0 Z †
≥ (cid:18) λ min (cid:19) d+ 1
λmin
(cid:16) (cid:17)
17Similarly, when Σ t−1 =Σ−t 1, γ
t
≤1/2 holds for t>T
0
such that
e b 8C2C log(T) 2 √K
ℓ
T 4 c . (13)
0 Z †
≥ (cid:18) λ min (cid:19) d+ log(T)
λmin
(cid:16) (cid:17)
Proof of Lemma A.8. We first check the condition of Lemma A.7. From Lemma A.5, we have
1
η ℓ (a,x) .
t t
≤ 8√ω
t
(cid:12) (cid:12)
(cid:12)b (cid:12)
Additionally, we have (cid:12) (cid:12)
ω q a x a [K], x ,
t t
≥ | ∀ ∈ ∀ ∈X
ω 1(cid:0) q (cid:1)a x a [K], x ,
t t
≥ − | ∀ ∈ ∀ ∈X
(cid:16) (cid:0) (cid:1)(cid:17)
which yields
1 1
.
√ω ≤ 1/2
t min q a x , 1 q a x
t t
| − |
n (cid:0) (cid:1) (cid:16) (cid:0) (cid:1)(cid:17)o
Therefore,
1 1
η ℓ (a,x) .
t t ≤ 8 1/2
(cid:12) (cid:12) min q t a x , 1 q t a x
(cid:12)b (cid:12) | − |
(cid:12) (cid:12) n (cid:0) (cid:1) (cid:16) (cid:0) (cid:1)(cid:17)o
holds.
Then, by using this results, from Lemma A.7, for any x , we obtain
∈X
1
ℓ (x),q (x) q (x) D(q (x),q (x))
t t t+1 t+1 t
− − η
t
D E
1
=b η ℓ (x),q (x) q (x) D(q (x),q (x))
t t t t+1 t+1 t
η − −
t
nD E o
b 3/2
≤η
t
ℓ2
t
a |x q t3/2 a |x +ℓ2
t
a†t(x) |x min q
t
a†t(x) |x ,1 −q
t
a†t(x) |x

a 6=Xa† t(x) (cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1) n (cid:0) (cid:1) (cid:0) (cid:1)o 
b b
 1/2 
≤η
t
ℓ2
t
a |x q
t
a |x √ω t+ℓ2
t
a†t(x) |x q
t
a†t(x) 1 −q
t
a†t(x) |x

a 6=Xa† t(x) (cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:16) (cid:0) (cid:1)(cid:17) 
b b
≤√ωtη t ℓ2 t(a |x)q t(a |x) ≤2√ω tη t ℓ2 t(a |x)π t(a |x), 
aX∈[K] aX∈[K]
b b
Here, we used
3/2 1/2
min q
t
a†t(x) |x ,1 −q
t
a†t(x) |x =min q
t
a†t(x) |x ,1 −q
t
a†t(x) |x min q
t
a†t(x) |x ,1 −q
t
a†t(x) |x
n (cid:0) (cid:1) (cid:0) (cid:1)o n (cid:0) (cid:1) (cid:0) 1/2 (cid:1)o n (cid:0) (cid:1) (cid:0) (cid:1)o
≤q
t
a†t(x) |x 1 −q
t
a†t(x) ∀x ∈X,
(cid:0) (cid:1)n (cid:0) o
from the definition of a (x) x) and ω . We also used π (1 γ )q q /2.
† t t t t t
| ≥ − ≥
This completes the proof.
18(II) Bounding the penalty term. Next, we bound penalty (x).
t
LemmaA.9(Upperboundofthepenaltyterm). Foranyα [0,1],thepenaltytermofα-LC-Tsallis-INFsatisfies
∈
1 1 1 1
penalty (x)= H q (x) 2 Kω ,
t − η − η t ≤ η − η t
(cid:18) t t −1(cid:19)
(cid:0) (cid:1)
(cid:18) t t −1(cid:19)
p
where recall that the penalty term penalty (x) is defined in (9).
t
Proof. The statement directly follows from the following inequality: for any x ,
∈X
H(q (x))=2 q (a x) 1 2 q (a x)
t t t
−  | − ≤ |
aX∈[K]
p
a 6=Xa†(x)
p
 
≤2 (K −1) q t(a |x)=2 (K −1)(1 −q t(a†t(x) |x)) ≤2 Kω t.
s a 6=Xa†(x) q
p
(iii) Bounding the bias term. Lastly,weupper boundthe biasterm,whichappearsfromthe estimationerror
of Σ−t 1.
Corollary A.10. If Σ†t =Σ−t 1, then we have
e bias=0.
If Σ†t = Σ−t 1, suppose that Assumptions 2.1–2.5 hold. Then, under the MGR with γ
t
:= 8C Z2 δC λℓ ml io ng(T)η t√ω t, M
t
=
1 1 and δ = 1 , we have
8ηte√ωt − C Z2Cℓ
bias 2C C .
Θ
≤ Z
Recall that the bias term bias is defined in (10).
Proof. If Σ†t =Σ t−1, then bias=0 holds from the definition of the bias term bias.
If Σ†t =Σ−t 1, and the conditions hold, then Lemma A.3 directly yields Lemma A.10.
e
A.4 eProofofTheorem6.1
Then, we prove Theorem 6.1 as follows.
Proof. We prove the regret upper bound only when the true Σ−t 1 is given. We omit the proof of regret upper
bound where Σ−t 1 is approximated by the MGR algorithm because we can show it similarly.
From Lemma A.8, for
T 0 ≥
4
4(cid:16)
(cid:16)8 8C CλmZ Z2 2iC
C
λn
mℓ ℓc
l
ino†
g(cid:17)
(T2
)(cid:16) cd †+
(cid:17)√
λ
2K
m1 (cid:16)in d+(cid:17)
√
l
λoK
g m( iT n)
(cid:17)
i if
f
Σ Σe−t
−t
1
1
= =Σ Σ−t
−t
1
1
,
we have
 e b
stability (x) 2η √ω ℓ2(a x)π (a x).
t ≤ t t t | t |
aX∈[K]
b
for all t T .
0
≥
19From Lemma A.9, we have
1 1
penalty (x) 2 Kω .
t ≤ η − η t
(cid:18) t t −1(cid:19)
p
Therefore, for t T , we can bound the pointwise regret as
0
≥
1 1
r (x) 2√ω η ℓ2(a x)π (a x)+2 Kω +C γ .
t ≤ t t t | t | η − η t ℓ t
aX∈[K] (cid:18) t t −1(cid:19)
p
b
From the regret decomposition using the stability, penalty, and bias terms in (7), we have
R E[stability (X )]E[penalty (X )]+bias
T ≤ t 0 t 0
- =penaltyterm
| {Tz }| {z } 1 1
2T C + E √ω η ℓ2(a X )π (a X )+2 Kω +C γ +bias
≤ 0 ℓ  t t t | 0 t | 0 η − η t ℓ t 
t= XT0+1 a X∈[K] (cid:18) t t −1(cid:19)
p
 b 
T
1 1
2T C + E √ω η ℓ2(a X )π (a X )+2 Kω +γ +bias
≤ 0 ℓ  t t t | 0 t | 0 η − η t t 
Xt=1 aX∈[K] (cid:18) t t −1(cid:19)
p
 b 
T
1 1
=2T 0C ℓ+ E √ω tη tθ t⊤

π t(a |X 0)φ(a,X 0)φ⊤(a,X 0) +2
η − η
Kω t+C ℓγ t +bias
Xt=1 aX∈[K] (cid:18) t t −1(cid:19)
p
T  b   
1 1
=2T 0C ℓ+ E √ω tη tθ t⊤Σ t+2
η − η
Kω t+C ℓγ
t
+bias.
Xt=1 (cid:20) (cid:18) t t −1(cid:19)
p
(cid:21)
b
When the true Σ−t 1 is given, we have
T
1 1
R
T
≤2T 0C ℓ+O Xt=1E (cid:20)√ω tη tφ ⊤(a,X t)Σ−t 1Σ tΣ−t 1φ(A t,X t)+2
(cid:18)η t − η t −1(cid:19)
pKω t+γ
t
(cid:21)!
T
1 1
2T C +O E √ω η d+η 2 Kω +γ
0 ℓ t t t t t
≤ Xt=1 (cid:20) (cid:18)η t − η t −1(cid:19)
p
(cid:21)!
T
1 1 1
=O E η d+ + √K √ω ,
t t
" Xt=1(cid:18) (cid:16) λ min (cid:17) (cid:18)η t − η t −1(cid:19) (cid:19) #!
where we used bias=0. Because we defined η =c √K , we have
t †
rt(cid:16)d+ λm1 in(cid:17)
T √K d+ 1 ω
R =O E λmin t .
T   q (cid:0) √t (cid:1) 
t=1
X
  
When Σ−t 1 is approximated by the MGR algorithm, we have
T
R
T
≤2T 0C ℓ+O E √ω tη φ⊤(a,X t)Σ−t 1Σ tΣ−t 1φ(A t,X t)+2 Kω t+γ t+bias
!
Xt=1 h p i
T
2T C +O E √ω η d+η 2 Kω +γ +bias
0 ℓ t t t t t
≤ !
Xt=1 h p i
20√Klog(T) T log(T) 1 1
=O +E η d+ + √K √ω ,
t t
λ min(d/log(T)+1) " Xt=1(cid:18) (cid:16) λ min (cid:17) (cid:18)η t − η t(cid:19) (cid:19) #!
where we used Tbias= C from Lemma A.3. Because we defined η =c √K , we have
CZ Θ t † rt(cid:16)d+l λog m( iT n)
(cid:17)
R =O
√Klog(T)
+E
T √K d+ lo λg m( iT n) ω t
.
T λ min(d/log(T)+1)  q (cid:0) √t (cid:1) 
t=1
X
  
A.5 ProofofTheorem6.3
As we discussed in the proof of Lemma A.8, from the definitions of γ , γ , and η , there exists T >0 such that
t t t 0
for all t>T , γ 1/2 holds. Since
0 t
≤
e
T
R E ∆(X ) π (a X )
T t t t
≥  | 
Xt=1 a 6=aX∗ T(Xt)
 
holds, by using a†T, the regret is lower bounded as
T T
1
R E ∆(X ) π (a X ) E ∆(X ) q (a X )
T t t t t t t
≥  | ≥ 2  | 
Xt=1 a 6=aX∗ T(Xt) t= XT0+1 a 6=aX∗ T(Xt)
 T   T 
1 1
≥
2E
"
∆(X t) 1 −q t(a∗T(X t) |X t)
#≥
2E
"
∆(X t) 1 −q t(a†T(X t) |X t) #.
t= XT0+1
(cid:0) (cid:1)
t= XT0+1
(cid:0) (cid:1)
where we used π (a x) (1 γ )q (a x) q (a x)/2 for any a [K] and x .
t t t t
| ≥ − | ≥ | ∈ ∈X
Proof of Theorem 6.4. We use the statement in Theorem 6.1 and results in its proof. From the Cauchy-Schwarz
inequality and the Jensen inequality, we have
T √K d+ 1 ω
R =2T C +O E λmin t
T 0 ℓ   q (cid:0) √t (cid:1) 
t= XT0+1
  
T T
1 1
=2T C +O √K d+ E[ω ]
0 ℓ s
(cid:18)
λ min(cid:19)v
u ut= XT0+1
tv
u uXt=1
t 
 t t 
T
1
=2T C +O √K d+ log(T) E[ω ] .
0 ℓ s
(cid:18)
λ
min(cid:19)
v
u ut= XT0+1
t 
 t 
Let c be a universal constant such that 0< c<1. Let EX be expectation of X over . Let be a subset of
t Xt Xt
defined as follows:
X
e e e
:= x : min 1 q (a x) cω ,
t t t
X ∈X a [K] − | ≥
n ∈ (cid:8) (cid:9) o
e
where recall that ω
t
=max
x
∈X
1 −q
t
a†t(x) . We use
(cid:16) (cid:0) (cid:1)(cid:17)
EX min 1 q (a X ) = min 1 q (a s) P (X =s).
t a [K] − t | 0 |Ft −1 a [K] − t | X0∼G 0
(cid:20) ∈ (cid:8) (cid:9) (cid:21) sX ∈Xt ∈ (cid:8) (cid:9)
e e
21Note that argmax min 1 q (a x) by definition, where ω = sup min 1 q (a x) =
x ∈X a ∈[K] − t | ∈ Xt t x ∈X a ∈[K] − t |
max min 1 q (a x) holds.
Tx h∈ eX n we ha a∈ v[K e] − t | (cid:8) (cid:9) e (cid:8) (cid:9)
(cid:8) (cid:9)
ω
t
cL.
EX min 1 q (a X ) ≤
t a ∈[K] − t | 0 |Ft −1
Therefore, we have (cid:2) (cid:8) (cid:9) (cid:3)
e
T
1
R =2T C +O √K d+ log(T) E[ω ]
T 0 ℓ  λ v t 
r
(cid:0)
min
(cid:1)
u ut= XT0+1
 t 
=2T 0C ℓ+O

r√K (cid:0)d+
λ
m1
in
(cid:1)log(T)
v u ut=
XTT 0+1E "ω EtE
eX
tX t m(cid:2)m inin
a
∈a ∈ [K[K ]] (cid:8)11 −−
q
tq (t a(a || XX 0)0)
(cid:9)
|| FF
t
−t − 11
(cid:3)#
 t (cid:2) (cid:8) (cid:9) (cid:3) 
1 T e
=2T C +O √K d+ log(T) E cLEX min 1 q (a X )
0 ℓ  r
(cid:0)
λ min
(cid:1)
v u ut= XT0+1 (cid:20) t (cid:20)a ∈[K]
(cid:8)
− t | 0 (cid:9)|Ft −1 (cid:21)(cid:21)
 t e 
T
1
=2T C +O √K d+ log(T) L E EX min 1 q (a X ) .
0 ℓ  r
(cid:0)
λ min
(cid:1)
v u
u
t= XT0+1 (cid:20) t (cid:20)a ∈[K]
(cid:8)
− t | 0 (cid:9)|Ft −1 (cid:21)(cid:21)
 t e 
Note that for any x , it holds that
∈X
1
min 1 q (a x) .
t
a [K] − | ≤ 2
∈
(cid:8) (cid:9)
Therefore, from Lemma A.6, we have
1+β
β β
E ∆(X ) min 1 q (a X ) ∆ E min 1 q (a X ) ,
0 t 0 t 0
a [K] − | ≥ ∗1+β a [K] − |
(cid:20) ∈ (cid:21) (cid:18) (cid:20) ∈ (cid:21)(cid:19)
(cid:8) (cid:9) (cid:8) (cid:9)
which yields
β
1+β 1+β
E min 1 q (a X ) E ∆(X ) min 1 q (a X ) .
t 0 0 t 0
a [K] − | ≤ β∆ a [K] − |
(cid:20) ∈ (cid:21) (cid:18) ∗ (cid:20) ∈ (cid:21)(cid:19)
(cid:8) (cid:9) (cid:8) (cid:9)
From the definition of EX, we also have
t
E eEX min 1 q (a X ) E min 1 q (a X )
t a [K] − t | 0 |Ft −1 ≤ a [K] − t | 0
(cid:20) (cid:20) ∈ (cid:21)(cid:21) (cid:20) ∈ (cid:21)
(cid:8) (cid:9) (cid:8) (cid:9)
Therefore, we obtain e
T
1
R =2T C +O L√K d+ log(T) E EX min 1 q (a X )
T 0 ℓ  r
(cid:0)
λ min
(cid:1)
v u ut= XT0+1 (cid:20) t (cid:20)a ∈[K]
(cid:8)
− t | 0 (cid:9)|Ft −1 (cid:21)(cid:21)
 t e 
T β
1 1+β 1+β
=O L√K d+ log(T) E ∆(X ) min 1 q (a X )
 r
(cid:0)
λ min
(cid:1)
v u uXt=1(cid:18) β∆ ∗ (cid:20) 0 a ∈[K]
(cid:8)
− t | 0 (cid:9)(cid:21)(cid:19) 
 t 
β
1 1+β
T 1+β
=O rL√K (cid:0)d+
λ min
(cid:1)log(T)v
u u
uT1+1 β
β∆
∗
Xt=1E (cid:20)∆(X 0) am ∈[i Kn
]
(cid:8)1 −q t(a |X 0)
(cid:9)(cid:21)!
 .
 t 
 
22From the definition of the regret, as discussed in (1),
T T
1
R
T
E ∆(X t) 1 π t(a†(X t) X t) E ∆(X 0) min 1 q t(a X 0)
≥ − | ≥ 2 a [K] − |
t=1 t=1 (cid:20) ∈ (cid:21)
X (cid:2) (cid:0) (cid:1)(cid:3) X (cid:8) (cid:9)
holds. Therefore, we obtain
β
R T =O  rL√K d+ λ m1 in log(T) v u uT1+1 β (cid:18)1 β+ ∆ ∗β (cid:19)1+β R2(1β +β).
 (cid:0) (cid:1) t 
 
Finally, we obtain
β 1+β
R T =O 1+β 2+β L√K d+ 1 log(T) 2+β T2+1 β .
β∆ λ
(cid:26) ∗ (cid:27) (cid:26) min (cid:27) !
(cid:0) (cid:1)
A.6 ProofofTheorem6.4
Proof of Theorem 6.4. We prove the regret upper bound only when the true Σ−t 1 is given. We omit the proof of
regret upper bound where Σ−t 1 is approximated by the MGR algorithm because we can show it similarly.
Recall that from the definitions of γ , γ , and η , for all t T such that
t t t 0
≥
8C2C 2 √K
Te
0
4
Z
ℓ c† ,
≥ (cid:18) λ min (cid:19) d+ 1
λmin
(cid:16) (cid:17)
we have γ 1/2.
t
≤
From the Jensen inequality, we have
T √K d+ 1 ω
R =O E λmin t
T   q (cid:0) √t (cid:1) 
t=1
X
  
L√K d+ 1 T E min 1 q (b X )
=O T 0+
q K(cid:0) −1
λmin
(cid:1) t= XT0+1a X6=a∗s (cid:2)
b ∈[K]
(cid:8)
t−
t
|
0
(cid:9)(cid:3)
 
L√K d+ 1 T
=O T +
λmin u t
,
 0 q K(cid:0) 1 (cid:1) t 
− t=1a=a∗r
X X6
 
where a [K] is an arbitrary arm just put for using the result of Theorem 4 in Masoudian & Seldin (2021), and
∗
∈
E min 1 q (b X ) if t T
u
t
= b ∈[K] − t | 0 ≥ 0 .
(0
(cid:2) (cid:8) (cid:9)(cid:3)
if t<T
0
Note that the sum is introduced just for satisfying the conditions of Theorem 4 in Masoudian & Seldin
a=a∗
(2021), although it appa6 rently looks a bit meaningless.
P
Recall that from Definition 4.2, we have
T
R E ∆ (a X )π (a X ) C.
T t t t t
≥  | | −
Xt=1a 6=Xa∗ t(Xt)
 
23Therefore, we have
T
1
R E ∆ (a X )q (a X ) C
T t t t t
≥ 2  | | −
t= XT0+1a 6=Xa∗ t(Xt)
 
T
1
∆ E q (a X ) C
t t
≥ 2 ∗  | −
t= XT0+1a 6=Xa∗ t(Xt)
 T 
1
∆ E min 1 q (b X ) C
t 0
≥ 2 ∗ " b [K] − | #−
t= XT0+1 ∈
(cid:8) (cid:9)
T
1 ∆
= ∗ E min 1 q t(b X 0) C
2 K 1 b [K] − | −
t= XT0+1a X6=a∗ − (cid:20) ∈
(cid:8)
(cid:9)(cid:21)
T
1 ∆
= ∗ u t C,
2 K 1 −
t=1a=a∗ −
X X6
where recall that in Definition 4.2, we assumed that there exists ∆ > 0 such that for all a [K] and any x ,
∗ ∈ ∈X
∆ (a x)>∆ holds.
t
| ∗
From Theorem 4 in Masoudian & Seldin (2021), we obtain
2
R =O T +
L√K d+ λm1
in
(K −1)2 T
log(T)+3 2log
L√K d+ λm1
in +C
T  0 q K(cid:0) 1 (cid:1) ∆  − q K(cid:0) 1 (cid:1)
− ∗ Xt=1 − 
 
     
L√K d+ 1 T  L√K d+ 1 
=O T + λmin log(T)+3 2log λmin +C.
 0 (cid:0)∆
(cid:1)
 − q K(cid:0) 1 (cid:1)
∗ Xt=1 − 
   
Moreoverfor K ∆− ∗1 log KT∆ −2 ∗ 1 +1 ≤C ≤∆ ∗T ≤C ≤∆ ∗T, we have 
(cid:16) (cid:16) (cid:17) (cid:17)
CL√K d+ 1 T ∆ T
R T =O T 0+ s (cid:0)∆
∗
λmin
(cid:1)
Xt=1(slog +
(cid:18)
C∗ (cid:19)+2 )+W,
 
where
L√K d+ 1 ∆ T ∆ T
W := (cid:0)∆
∗
λmin
(cid:1)
log +
(cid:18)
C∗ (cid:19)+ s2log +
(cid:18)
C∗ (cid:19)+2 !.
By substituting T =4 8C Z2Cℓc 2 √K , we obtain the statement.
0
(cid:16)
λmin †
(cid:17)
(cid:16)d+ λm1 in(cid:17)
B Proof of Lemma A.6
Proof. For simplicity, let us assume that V has a density function. This assumption implies that the cumulative
densityfunctionofV is acontinuousandmonotonicallyincreasingfunction. Then, Z :=F(V)followsthe uniform
distribution Unif[0,1] over the support [0,1].
Let us define
g(z):=E[U Z =z].
|
Then, we have g(z) [0,1], and the expected values of U and UV are given as follows:
∈
1
E U = E [g(W)]= g(W)dw =µ, (14)
W ∼Unif[0,1] Z0
(cid:2) (cid:3)
241
E UV = E [g(W)F 1(W)]= g(w)F 1(w)dw, (15)
− −
W ∼Unif[0,1] Z0
(cid:2) (cid:3)
where E denotes an expectation operator for a random variable W under a probability distribution .
W
Here, n∼ otJ e that F 1 is a monotonically decreasing function. Therefore, for g(w) [0,1], under (14), aJ function
−
∈
g minimizes (15) if g (w):= [w µ].
∗ ∗
≤
In conclusion, we obtain
1 µ
E[UV] g (w)F 1(w)dz = F 1(w)dw. (16)
∗ − −
≥
Z0 Z0
β
Furthermore, because we assumed F(h)
≤
21 ∆h
∗
for any h
∈
[D,∆ ∗], we have F −1(w)
≥
∆ ∗(2w)β1 for w
∈
[F(D),F(∆ )]= 1 D ,1 , which implies (cid:16) (cid:17)
∗ 2∆∗ 2
h i
µ F−1(w)dw µ ∆ (2w)β1 dw µ ∆ (2w)β1 dw= ∆ ∗ (2µ)1+ ββ , (17)
Z0 ≥ Z0 ∗ ≥ Z21 ∆D
∗
∗ 2(1+β)
where we used 0 µ 1. From (16) and (17), we obtain the statement.
≤ ≤ 2
C Proof of Lemma A.7
To show Lemma A.7, we show the following lemmas.
Lemma C.1. For p,q [0,1] and ℓ 1 αqα 1, we have
∈ ≥−
−2 −
2q2 αℓ2
−
ℓ q p d p,q . (18)
· − − ≤ 1 α
−
(cid:0) (cid:1) (cid:0) (cid:1)
The proof is shown in Appendix C.
Lemma C.2. Given x , fix arbitrary a (x) [K] and q . If ℓ (a x) 1 αqα 1(a x) for all a [K].
∈X
†
∈
∈PK t
| ≥−
−4 −
| ∈
then we have
4
ℓ (x),q(x) p(x) D(p,q) q2 α(a x)ℓ2(a x) (19)
t − − ≤ 1 α − | t | 
D E − aX∈[K]
 
for any p .
K
∈P
If ℓ (a (x) x) 1 αqα 1(a (x) x) for all a [K] a (x) . then we have
t †
| ≤−
−4 − †
| ∈ \{
†
}
ℓ t(x),q(x) −p(x) −D(p,q)
≤ 1
4
α
q2 −α(a |x)ℓ2 t(a |x)+ 1 −q(a †(x) |x) 2 −α ℓ2 t(a †(x) |x)

(20)
D E − a 6=Xa†(x) (cid:0) (cid:1)
 
for any p .
K
∈P
The proof is shown in Appendix C.2.
Then, we prove Lemma A.7 as follows.
Proof of Lemma A.7. By definition of a (x), for all a [K] a (x) , we have
† †
∈ \{ }
q a x q a (x) and q a x 1 q a (x) .
† †
| ≤ | ≤ −
Then, for all a ∈[K] \{a †(x) }, fro(cid:0) m ℓ t(cid:1) (a |x(cid:0) )
≤
1(cid:1) −4αmin q(cid:0) a †(x)(cid:1) |x ,1 −(cid:0) q a †(x(cid:1) ),x α −1 , we have
1 α(cid:12) (cid:12) (cid:8) (cid:0) (cid:1) (cid:0) 1 α (cid:1)(cid:9)
ℓ t(a x) − (cid:12)min q (cid:12)a†(x) x ,1 q a†(x),x α −1 − qα −1(a x)
| ≤ 4 | − ≤ 4 |
(cid:12) (cid:12) (cid:8) (cid:0) (cid:1) (cid:0) (cid:1)(cid:9)
(cid:12) (cid:12)
25
1If q a (x x) 1 q a (x x) , from (19) in Lemma C.2, we have
† †
| ≤ − |
(cid:0) (cid:1) (cid:0) (cid:1)
ℓ (x),q(x) p(x) D(p(x),q(x))
t
− −
D E
b 4
≤ 1 α
q2 −α(a |x)ℓ2 t(a |x)

− aX∈[K]
 
≤ 1
4
α
q2 −α(a |x)ℓ2 t(a |x)+min q(a†(x) |x),1 −q(a†(x) |x) 2 −α ℓ2 t(a†(x),x) .
− a 6=Xa†(x)
(cid:8) (cid:9)
 
If q a (x x) >1 q a (x x) , from (20) in Lemma C.2, we have
† †
| − |
(cid:0) (cid:1) (cid:0) (cid:1)
ℓ (x),q(x) p(x) D(p(x),q(x))
t
− −
D E
≤b
1
4
α
q2 −α(a |x)ℓ2 t(a |x)+ 1 −q(a †(x) |x) 2 −α ℓ2 t(a †(x),x)

− a 6=Xa†(x)
(cid:0) (cid:1)
 
≤ 1
4
α
q2 −α(a |x)ℓ2 t(a |x)+min q(a †(x) |x),1 −q(a †(x) |x) 2 −α ℓ2 t(a †(x),x) .
− a 6=Xa†(x)
(cid:8) (cid:9)
 
C.1 ProofofLemmaC.1
Proof. For any given q and ℓ, the LHS of (18) is concave in p. Hence, this is maximized when
d
ℓ q p d p,q = ℓ qα 1+pα 1 =0. (21)
− −
dp · − − − −
n (cid:0) (cid:1) (cid:0) (cid:1)o
We then have
1 1
1 1 α α−1 1 α α−1
p= qα −1+ℓ α−1 qα −1 − qα −1 =q 1 − 2q, (22)
≤ − 2 − 2 ≤
(cid:18) (cid:19) (cid:18) (cid:19)
(cid:0) (cid:1)
where the first equality follows from (21) and the first inequality follows from the assumption of ℓ 1 αqα 1.
≥ −
−2 −
Furthermore, from the intermediate value theorem and the fact that pα 2 is monotone decreasing in p, we have
−
ℓ = pα 1 qα 1
− −
−
(cid:12) (cid:12) (cid:12)min (α 1)(cid:12)pα 2 , (α 1)qα 2 p q
(cid:12) (cid:12)≥(cid:12) − (cid:12) − − − −
=(1 n α(cid:12) (cid:12))max p,q α(cid:12) (cid:12)−(cid:12) (cid:12)2 p q , (cid:12) (cid:12)o(cid:12)
(cid:12)
(cid:12)
(cid:12)
− −
where the firstinequality follows from (21) and the(cid:8)seco(cid:9)nd in(cid:12) (cid:12)equali(cid:12) (cid:12)ty follows from the intermediate value theorem.
This implies
1
2 α
p q max p,q − ℓ . (23)
− ≤ 1 α ·
−
(cid:12) (cid:12) (cid:8) (cid:9) (cid:12) (cid:12)
We then have (cid:12) (cid:12) (cid:12) (cid:12)
ℓ (p q) d(p,q) ℓ q p
ℓ2
max p,q 2 −α
4ℓ2
q2 −α,
· − − ≤ − ≤ 1 α ≤ 1 α
− −
(cid:12) (cid:12)(cid:12) (cid:12) (cid:8) (cid:9)
where the second inequality follows from (23(cid:12))(cid:12)a(cid:12)nd th(cid:12)e last inequality follows from (22).
26C.2 ProofofLemmaC.2
Proof. We have
ℓ (x),q(x) p(x) D(p,q)
t
− −
D 1 E
= 2ℓ (a x) q(a x) p(a x) d p(a x),q(a x)
t
2 | · | − | − | |
a 6=Xa†(x)n (cid:0) (cid:1) (cid:0) (cid:1)o
1
+ 2ℓ t(a†(x) x) q(a†(x) x) p(a†(x) x) d p(a†(x) x),q(a†(x) x) d p(a x),q(a x)
2 | · | − | − | | − | | 
(cid:0) (cid:1) (cid:0) (cid:1)
a 6=Xa†(x)
(cid:0) (cid:1)
1  
2ℓ (a x) q(a x) p(a x) d p(a x),q(a x)
t
≤ 2 | · | − | − | |
a 6=Xa†(x)n (cid:0) (cid:1) (cid:0) (cid:1)o
1
+ min 2ℓ (a (x) x) q(a (x) x) p(a (x) x) d p(a (x) x),q(a (x) x) ,
t † † † † †
2 | · | − | − | |
(cid:0) (cid:1) (cid:0) (cid:1)
2ℓ t(a†(x) q(a†(x) x) p(a†(x) x) d p(a x),q(a x) .
· | − | − | | !
(cid:0) (cid:1)
a 6=Xa†(x)
(cid:0) (cid:1)
From Lemma C.1, if ℓ (a x) 1 αqα 1(a x), we have
t
| ≥−
−2 −
|
8q2 α(a x)ℓ2(a x)
2ℓ t(a†(x) q(a†(x) x) p(a†(x) x) d p(a x),q(a x) − | t | .
· | − | − | | ≤ 1 α
(cid:0) (cid:1)
a 6=Xa†(x)
(cid:0) (cid:1)
−
Furthermore, we have
q(a (x) x) p(a (x) x) = 1 p(a (x) 1 q(a (x) x) = p(a x) q(a x) .
† † † †
| − | − − − | | − |
(cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1)
a 6=Xa†(x)
(cid:0) (cid:1)
As we have 1 −q(a †(x)
|
x) α −1
≤
qα −1(a
|
x) for any a
∈
[K] \{a †(x) }, if ℓ t(a †(x)
|
x)
≤
1 −4α 1 −q(a †(x)
|
α 1
x) − , we then (cid:0) have (cid:1) (cid:0)
1 α
(cid:1) ℓ t(a†(x) x) − qα −1(a x)
− | ≥− 4 |
for any a [K] a (x) . Hence, Lemma C.1 implies
†
∈ \{ }
2ℓ t(a†(x) x) q(a†(x) x) p(a†(x) x) d p(a x),q(a x)
| · | − | − | |
(cid:0) (cid:1)
a 6=Xa†(x)
(cid:0) (cid:1)
= 2ℓ (a (x) x) q(a x) p(a x) d p(a x),q(a x)
t †
− | · | − | − | |
a 6=Xa†(x)n (cid:0) (cid:1) (cid:0) (cid:1)o
2 2ℓ t(a†(x) x) 2 q2 −α(a†(x) x)
≤ 1 α | |
− a 6=Xa†(x)
(cid:0) (cid:1)
2 α
−
8
ℓ2(a (x) x) q(a x)
≤ 1 α t † |  | 
− a 6=Xa†(x) a 6=Xa†(x)
=
1
8
α
1 −q(a †(x) |x) 2 −α ℓ2 t(a †(x) |x). 
−
(cid:0) (cid:1)
27D The BoBW-RealFTRL with the Feature Map
In Neu & Olkhovskaya (2020), Kato & Ito (2023), and Kuroki et al. (2023), the focus is on arm-independent
contexts, and linear models are considered, defined as ℓ (a,x) = x,θ (a) +ε (a), where x Rd represents a
t t t
∈
d-dimensional(arm-independent) context, and θ (a) is a d-dimensional arm-dependent regressioncoefficient. This
t
(cid:10) (cid:11)
paper modifies the BoBW-RealFTRL proposed by Kato & Ito (2023), which is almost the same as the FTRL-LC
independently proposedbyKuroki et al.(2023), andits regretupper boundfor the arm-dependentfeature setting.
These algorithms employ the Shannon-entropy, instead of the Tsallis-entropy,in the FTRL.
D.1 TheBoBW RealFTRL
Then, we define our policy, called the BoBW-RealFTRL, as
π (X ):=(1 γ )q (X )+γ e (X ), (24)
t t t t t t ∗ t
−
where
t 1
− 1
q (x) argmin ℓ (x),q + ψ(q) for t 2,
t s
∈ q ∈PK ( Xs=1D E η t ) ≥
q (x):=(1/K 1/K b1/K) ,
1 ⊤
···
1
ψ(q(x)):= q(a x)log ,
− | q(a x)
aX∈[K] (cid:18) | (cid:19)
1 1 1 1
:= + .
η t+1 η t η 1 1+ log(K) −1 t s=1H q s(X s)
q
(cid:0) (cid:1) P (cid:0) (cid:1)
Let ψ (q(x)):= 1ψ(q(x)).
t ηt
Then,aswellastheα-LC-Tsallis-INF,weconsidertwocaseswherethetrueΣ−t 1 isgiven,anditisapproximated
by using finite samples generated from . For each case, we specify the remaining parameters in the FTRL-based
G
algorithm.
The BoBW-RealFTRL with Σ−t 1. First, we consider a case where the true Σ−t 1 is given. In this case, we
specify the remaining parameters as follows:
1 1 η
:=ζ log(T) +d , quadand γ := t .
t
η 1 s (cid:18)δλ minlog(K) (cid:19) 2λ min
The BoBW-RealFTRL with an approximated Σ−t 1. Next, we consider a case where Σ−t 1 is approximated
by using the MGR algorithm defined in Algorithm 2. In this case, we set the parameters as follows:
1 log(T)
:=ζ log(T) +d ,
η 1 s (cid:18)δλ minlog(K) (cid:19)
η
γ := t log(T),
t
2λ
min
2 1
M := 1, and δ := .
t η − C2C
t ℓ
Z
28Algorithm 3 BoBW-RealFTRL.
Parameter: Learning rate η ,η ,...,η >0.
1 2 T
Initialization: Set θ =0.
0
for t=1,...,T do
Observe X .
t
Draw A [K] following the policy π (X ):=(1 γ )q (X )+γ e (X ) defined in (24).
t t t t t t t ∗ t
∈ −
Observe the loss ℓ (A ,X ).
t t t
Compute θ .
t
end for
b
D.2 RegretAnalysis
This section provides upper bounds for the regret of our proposed BoBW-RealFTRL algorithm.
To derive upper bounds, we define the following quantities:
T
Q(a x)= 1 q a (x) x ,
∗T
| −
t ∗T
|
Xt=1n (cid:0) (cid:1)o
Q(a )=E[Q(a X )].
∗T ∗T
|
0
As well as ξTsallis Σ†t , for the BoBW-RealFTRL with a regressioncoefficient estimator θ
t
Σ†t , let us define
(cid:16) (cid:17) (cid:16) (cid:17)
e ξShannon (cid:16)Σ†t (cid:17):=

ll oo gg (( KK )) (cid:16)λ
lo
λm g1 mi (n
iT
n)+ +d
(cid:17)d
i if
f
Σ
Σ
e†t
†t
= =ΣΣ −t−t 11 . b e
e (cid:16) (cid:17)
Then, we show the following upper bound, which holds for general cases such as adversarial and stochastic
 e b
regimes. We show the proof in Sections E.1 and E.2.
Theorem D.1 (General regret bounds). Consider the BoBW-RealFTRL with the estimator θ
t
Σ†t . Assump-
tions 2.1–2.5 hold. Then, the decision-maker incurs the following regret: (cid:16) (cid:17)
b e
ξShannon Σ†t
1 1/2
R
T
=O η 1log(T)
log(K(cid:16)) (cid:17)
+
η
log(K)

log(KT)max Q (a∗T),1 .
1
e p p n o
  
For each situation, we derive a specific upper bound. The proof is shown in Appendix E.
First, from Q(a ) T, the following regret bound holds without any assumptions on the loss; that is, it holds
∗T
≤
in an adversarialregime.
Theorem D.2 (Regretupperboundinanadversarialregime). Consider the BoBW-RealFTRL with the estimator
θ
t
Σ†t . Assume that the loss is generated under an adversarial regime. Suppose that Assumption 2.1 and 2.3
hol(cid:16)d. T(cid:17)hen, the regret satisfies
b e
R
T
=O log(KT) Tlog(T)ξShannon Σ†t .
r !
(cid:16) (cid:17)
Next, we derive a regret upper bound in a stochastic regime with a maergin condition.
Theorem D.3 (Regret upper bound in a stochastic regime with a margin condition). Consider the BoBW-
RealFTRL with the estimator θ
t
Σ†t . Assume that the loss is generated under a stochastic regime with a margin
condition (Definition 4.1). Suppo(cid:16)se t(cid:17)hat Assumption 2.1, 2.3–2.5 hold. Then, the regret satisfies
b e
R T =O

(cid:26)1 β+
∆
∗β (cid:27)2+β β (log(KT) rlog(T)ξShannon (cid:16)Σ†t (cid:17))1 2+ +β β T2+1 β .
 e 
29We omit the proof because it is almost the same as that for Theorem 6.3 for the 1/2-LC-Tsallis-INF
Furthermore, we derive a regret bound under the linear contextual adversarial regime with a self-bounding
constraint. The proof is provided in Appendix E.3
Theorem D.4 (Regret bounds under the linear contextual adversarial regime with a self-bounding constraint).
Consider the BoBW-RealFTRL with the estimator θ
t
Σ†t . Assume that the loss is generated under a linear
contextual adversarial regime with a self-bounding const(cid:16)rain(cid:17)t (Definition 4.2). Suppose that Assumption 2.1, 2.3–
2.5 hold. Then, the regret satisfies b e
D CD
R =O + ,
T
∆ ∆
r !
∗ ∗
where D =log(T)ξShannon Σ†t log(KT).
(cid:16) (cid:17)
Note that the BoBW-ReaelFTRL does not require Assumption 2.2 in stochastic regimes.
E Proof of Theorems D.1 and D.4
E.1 PreliminariesfortheProofofTheoremD.1
Let X be a sample from the context distribution independent of . Let D (p,q) denote the Bregman
0 T t
G F
divergence of p.q Π with respect to ψ ; that is,
t
∈
D (p,q)=ψ (p) ψ (q) ψ (q),p q .
t t t t
− − ∇ −
D E
Let us define π Π as π (a (x) x)=1 and π (a x)=0 for all a [K] a (x) .
∗ ∗ ∗ ∗ ∗
∈ | | ∈ \{ }
FromLemma 4.4inKato & Ito (2023)(our PropositionA.1), we havethe followingstability andpenalty terms
decomposition:
T
R E γ + ℓ (X ),q (X ) q (X ) D (q (X ),q (X ))
T t t 0 t 0 t+1 0 t t+1 0 t 0
≤ " − −
Xt=1n D E
b =stabilityterm
| {z }
+ψ (q (X )) ψ (q (X )) +ψ (π (X )) ψ (q (X ))
t t+1 0 t+1 t+1 0 T+1 ∗ 0 1 1 0
− − #
o
=penaltyterm
T
| {z }
+2 max E X ,θ (a) θ (a) .
t t t
a [K] h − i
Xt=1 ∈ (cid:12) h i(cid:12)
(cid:12) b (cid:12)
(cid:12) =biasterm (cid:12)
We can bound the bias term by usi|ng Corollary A.{1z0 derived for Th}eorem E.3. Therefore, we consider bounding
stability and penalty terms.
Bounding the stability term. For the stability term ℓ (X ),q (X ) q (X ) D (q (X ),q (X )), we
t 0 t 0 t+1 0 t t+1 0 t 0
− −
use the following proposition from Ito et al. (2022). D E
b
PropositionE.1(FromLemma8inIto et al.(2022)). Consider theBoBW-RealFTRLwiththeestimatorθ
t
Σ†t .
For any ℓ: RK and p,q Π, we have (cid:16) (cid:17)
X → ∈ b e
1
ℓ (x),p(x) q(x) D (q(x),p(x)) p(a x)ξ η ℓ (a,x) .
t t t t
− − ≤ η |
t
D E aX∈[K] (cid:16) (cid:17)
for any x , where ξ(x):=exp( x)+x 1.
∈X − −
30If η ℓ(a,x) 1 holds, then Proposition E.1 implies
t
≥−
b ℓ (x),q (x) q (x) D (q (x),q (x)) η π (a x)ℓ2(a,x).
t t − t+1 − t t+1 t ≤ t t | t
D E aX∈[K]
b b
For the RHS, we apply the following proposition from Neu & Olkhovskaya(2020).
Proposition E.2 (From Lemma 6 in Neu & Olkhovskaya(2020)). For each t [T], our strategy satisfies
∈
E π (a X )ℓ2(a,X ) 3d.
 t | 0 t 0 |Ft −1 ≤
aX∈[K]
 b 
E.2 ProofofTheoremD.1
Then, we obtain the following lemma, as well as Lemma 4.9 in Kato & Ito (2023).
Lemma E.3. The regret for the BoBW-RealFTRL with Σ†t,a satisfies
T 1 1 b 1
R E γ +3η d+ ψ(q (X )) + log(K)+2C C .
T t t t+1 0 Θ
≤ " t=1( (cid:18)η t+1 − η t(cid:19) )# η 1 Z
X
Proof of Lemma E.3. From Lemma A.1, we have
T
R E γ + ℓ (X ,d),π (X ) q (X ) D (q (X ),π (X ))
T t t 0 t 0 t+1 0 t t+1 0 t 0
≤ " − −
Xt=1(cid:16) D E
b
+ψ t(q t+1(X 0)) ψ t+1(q t+1(X 0)) +ψ T+1(π∗(X 0)) ψ 1(q 1(x))
− − #
(cid:17)
T
+2 max E X ,θ θ .
t t,a t,a
a [K] h − i
Xt=1 ∈ (cid:12) h i(cid:12)
(cid:12) b (cid:12)
First, we show (cid:12) (cid:12)
E ℓ (X ),π (X ) q (X ) D (q (X ),π (X )) 3η d. (25)
t 0 t 0 t+1 0 t t+1 0 t 0 t
− − ≤
hD E i
To show this, we confirm bη ℓ (a,x) 1, which is necessary to derive an upper bound from Proposition E.1.
t t
≥ −
We have
b
η
t
·
X 0,θ t(a) =η
t
·X 0⊤Σ†t,aX
t
X t,θ
t,a
[A
t
=a] ≥−η tC
ℓ
·
X 0⊤Σ†t,aX
t
D
b
E
≥−η tC Z2Cb
ℓ
Σ†t,D
a
op
≥−E
η tC Z2C ℓδ 1+
Mt kV(cid:12)
(cid:12) (cid:12)
k,a
kob
p
!=
−(cid:12)
(cid:12) (cid:12) η 2t (M t+1),
(cid:13) (cid:13) k X=1
(cid:13) (cid:13)b (cid:13)
(cid:13)
where we used that δ = 1 . Here, recallthat we defined M as 21 1. Therefore, η ℓ (a,x)= 1 holds. Then,
C Z2Cℓ t ηt − t t −
we have
b
ℓ (x),π (x) q (x) D (q (x),π (x))
t t t+1 t t+1 t
− −
D 1 E
≤b
η
π t(a |x)ξ η tℓ t(a,x) ≤η t π t(a |x)ℓ2 t(a,x).
t
aX∈[K] (cid:16) (cid:17) aX∈[K]
b b
Then, from Proposition E.2, we have (25).
31
1From ψ (q(x))= 1ψ(q(x)), we have
t −ηt
T
(ψ (q (x)) ψ (q (x)))+ψ (π (x)) ψ (q (x))
t t+1 t+1 t+1 T+1 ∗ 1 1
− −
t=1
X
T
1 1 1
ψ(q (x))+ log(K).
t+1
≤ η − η η
t=1(cid:18) t+1 t(cid:19) 1
X
From Lemma A.3, we have
T T
max E X ,θ θ C C /T =C C .
t t,a t,a Θ Θ
a [K] h − i ≤ Z Z
Xt=1 ∈ (cid:12) h i(cid:12) Xt=1
(cid:12) b (cid:12)
(cid:12) (cid:12)
From this result, we obtain the following lemma. We omit the proof.
Lemma E.4. Assume the conditions in Theorem E.3. Consider the BoBW-RealFTRL with the estimator θ
t
Σ†t .
Then, we have (cid:16) (cid:17)
b e
T
R c E ψ(q (X )) +2C C ,
T ≤ v u " t=1 t 0 # Z Θ
u X
t
where c=O η log(T) log(T) +d + 1 log(K) .
1 λminlog(K) η1
Next, we c(cid:16) onsider bo(cid:16) unding T ψ(cid:17) (q (x))p by Q(a(cid:17) x) as shown in the following proposition.
t=1 t ∗T |
Proposition E.5 (From LemmPa 4 in Ito et al. (2022)). For any a
∗
: [K], the following holds:
X →
T
eKT
ψ(q (x)) Q(a x)log ,
t
≤
∗T
| Q(a x)
t=1 (cid:18) ∗T | (cid:19)
X
where e is Napier’s constant.
By using the above lemmas and propositions, we prove Theorem 6.1.
Proof of Theorem 6.1. From Lemma E.5, if Q(a x) e, we have T ψ(q (x)) elog(KT) and otherwise, we
∗T | ≤ t=1 t ≤
have T ψ(q (x)) Q(a x)log(KT). Hence, we have T ψ(q (x)) log(KT)max e,Q(a x) . From
t=1 t ≤ ∗T | t=1 Pt ≤ { ∗T | }
Lemma E.4, we have
P P
T
R c E[ψ(q (X ))]+2C C
T ≤ v ut=1 t 0 Z Θ
uX
t log(T) 1 1/2
=O η log(T) +d + log(K) log(KT)max Q ,1 .
1
λ log(K) η
(cid:18)(cid:18) (cid:18) min (cid:19) 1 p (cid:19) p n o(cid:19)
E.3 ProofofTheoremD.4
Proof of Theorem D.4. From the definition of the contextual adversarial regime with a self-bounding constraint,
we have
T
R ∆ E 1 π (a (X ) X ) C =∆ Q(a ) C.
T
≥ ∗· " −
t ∗ 0
|
0
#− ∗·
∗T
−
Xt=1(cid:16) (cid:17)
32Therefore, from Lemma E.4, for any λ>0, we have
R =(1+λ)R λR
T T T
−
T
=(1+λ)O c log(KT) E[ψ(q (X ))] λR
 v t 0 − T
ut=1
p uX
 t 
T
(1+λ)O c log(KT) E[ψ(q (X ))] λ∆ Q(a )+λC,
≤  v
ut=1
t 0 − ∗· ∗T
p uX
 t 
where
log(T) 1
c= η log(T) +d + log(K) .
1
λ log(K) η
(cid:18) (cid:18) min (cid:19) 1 (cid:19)
p
Here, as well as the proof of Theorem 6.1, from Lemma E.5, if Q(a x) e, we have T ψ(q (x))
∗T | ≤ t=1 t ≤
elog(KT) and otherwise, we have T ψ(q (x)) Q(a x)log(KT). Hence, we have T ψ(q (x))
t=1 t ≤ ∗T | Pt=1 t ≤
log(KT)max e,Q(a x) . Here, to upper bound R , it is enough to only consider a case with Q(a x) e,
{
∗T
| } P
T
P
∗T
| ≥
and we obtain
2
O (1+λ)c log(KT)
R T ≤(1+λ)O c log(KT) Q(a ∗T)log(KT) −λ∆ ∗·Q(a ∗T)+λC ≤ (cid:18)n 2λo ∆p (cid:19) +λ∆ ∗.
(cid:18) p q (cid:19) ∗
where the second inequality follows from a√b cb a2 holds for any a,b,c>0. By choosing
− 2 ≤ c2
c2log(KT) c2log(KT)
λ= +2C .
s ∆ ∆
∗ .(cid:18) ∗ (cid:19)
Then, we obtain R =O c2log(KT)/∆ + Cc2log(KT)/∆ .
T
∗ ∗!
p
33