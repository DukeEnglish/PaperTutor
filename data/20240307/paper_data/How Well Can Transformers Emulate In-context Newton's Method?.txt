How Well Can Transformers Emulate In-context Newton’s
Method?
Angeliki Giannou∗ Liu Yang† Tianhao Wang‡ Dimitris Papailiopoulos§
Jason D. Lee¶
March 6, 2024
Abstract
Transformer-based models have demonstrated remarkable in-context learning capabilities,
promptingextensiveresearchintoitsunderlyingmechanisms. Recentstudieshavesuggestedthat
Transformers can implement first-order optimization algorithms for in-context learning and even
second order ones for the case of linear regression. In this work, we study whether Transformers
canperformhigherorderoptimizationmethods,beyondthecaseoflinearregression. Weestablish
that linear attention Transformers with ReLU layers can approximate second order optimization
algorithms for the task of logistic regression and achieve ϵ error with only a logarithmic to the
error more layers. As a by-product we demonstrate the ability of even linear attention-only
Transformers in implementing a single step of Newton’s iteration for matrix inversion with
merely two layers. These results suggest the ability of the Transformer architecture to implement
complex algorithms, beyond gradient descent.
1 Introduction
Transformer networks have had a significant impact in machine learning, particularly in tasks related
to natural language processing and computer vision (Vaswani et al., 2017; Khan et al., 2022; Yuan
et al., 2021; Dosovitskiy et al., 2020). A key building block of Transformers is the self-attention
mechanism, which enables the model to weigh the significance of different parts of the input data
with respect to each other. This allows the model to capture long-range dependencies and learn
complex patterns of the data, yielding state-of-the-art performance across a several tasks, including
but not limited to language translation, text summarization, and conversational agents (Vaswani
et al., 2017; Kenton and Toutanova, 2019).
It has been long observed that Transformers are able to perform various downstream tasks at
inference without any parameter updates Brown et al. (2020); Lieber et al. (2021); Black et al. (2022).
This ability, known as in-context learning, has attracted the interest of the community, resulting in
a line of works aiming to interpret and understand it. Towards this direction, Garg et al. (2022)
were the first to consider a setting, in which the “language” component of the problem is removed
from the picture, allowing the authors to study the ability of Transformers to learn how to learn
∗University of Wisconsin-Madison. Email: giannou@wisc.edu.
†University of Wisconsin-Madison. Email: liu.yang@wisc.edu.
‡Yale University. Email: tianhao.wang@yale.edu.
§University of Wisconsin-Madison. Email: dimitris@ece.wisc.edu.
¶Princeton University. Email: jasonlee@princeton.edu.
1
4202
raM
5
]GL.sc[
1v38130.3042:viXrain regression settings. However, even for this simple setting, understanding the mechanics of the
architecture that would allow such capability, is not yet very well understood.
Following research has presented constructive methods to explain what type of algorithms these
models can implement internally, by designing model weights that lead to a model that implements
specific meta-algorithms. Akyürek et al. (2022) constructed Transformers that implement one step
of gradient descent with O(1) layers. Other works have focused on the linear attention (removing
the softmax) and have shown empirically (von Oswald et al., 2022) and theoretically (Ahn et al.,
2023; Mahankali et al., 2023) that the optimum for one layer, is in essence one step of preconditioned
gradient descent. Ahn et al. (2023) also showed that the global minimizer in the two-layer case
corresponds to gradient descent with adaptive step size, but the optimality is restricted to only a
class of sparse weights configurations. Beyond these, it still remains open how to characterize the
global minimizer of the in-context loss for Transformers with multiple layers.
Another approach is to approximate the closed-form solution of linear regression, instead of
minimizing the loss. For that purpose, one needs to be able to perform matrix inversion. There
are multiple approaches to matrix inversion and in terms of iterative algorithms, one of the most
popular ones is Newton’s iteration (Schulz, 1933), which is a second-order method. Specifically, the
method has a warm-up state with logarithmic, to the condition number of the matrix, steps and
afterwards quadratic rate of convergence to arbitrary accuracy. The work of Giannou et al. (2023)
showed that Transformers can implement second-order methods, like Newton’s iteration1, for matrix
inversion. Fu et al. (2023) implemented Newton’s iteration with Transformers for in-context linear
regression; the authors also performed an empirical study to argue that Newton’s iteration is closer
to the trained models output rather than gradient descent.
Newton’s iteration for matrix inversion is of the form X = X (2I−AX ), where A is the
t+1 t t
matrix we want to invert, and X is the approximation of the inverse. The implementation of matrix
t
inversion, serves as a stepping stone towards answering the following question:
“How well can Transformers implement higher-order optimization methods?”
In pursuit of a concrete answer, we focus on the well-known Newton’s method, a second-order
optimization algorithm. For minimizing a function f, the method can be described as x =
t+1
x −η(x )(∇2f(x ))−1∇f(x ) for some choice of step-size η(·). To implement such updates, one
t t t t
essentially needs to first compute the step size η(x ), then invert the Hessian ∇2f(x ), and finally
t t
multiply them together with the gradient ∇f(x ). In general, the step size η(x ) may also depend on
t t
quantities computed from ∇f(x ) and ∇2f(x ). It is relatively straightforward for Transformers to
t t
perform operations like matrix transposition and multiplication, while the challenge is to devise an
organic combination of all the above components to deliver an efficient implementation of Newton’s
method with convergence guarantees. In particular, it further requires rigorous convergence analysis
to verify the effectiveness of the construction in concrete examples.
Main contributions. In this work we tackle the challenge from the perspective of in-context
learning for the tasks of linear and logistic regression. We consider Transformers with linear
attention layers and position-wise feed-forward layers with the ReLU activation. We provide concrete
constructions of Transformers to solve the two tasks, and derive explicit upper bounds on the depth
and width of the model with respect to the targeted error threshold. At a high level, our main
findings are summarized in the following informal theorem.
Theorem 1.1 (Informal). Transformers can efficiently perform matrix inversion via Newton’s
iteration, based on which they can further 1) compute the least-square solution for linear regression,
1WeuseNewton’siterationforthematrixinversionalgorithmandNewton’smethodfortheoptimizationalgorithm.
2and 2) perform Newton’s method to efficiently optimize the regularized logistic loss for logistic
regression. In particular, in the latter case, only loglog(1/ϵ) many layers and 1/ϵ8 width are required
for the Transformer to implement Newton’s method on the regularized logistic loss to achieve ϵ error.
We also corroborate our results with experimental evidence.
Interestingly, trained Transformers seem to outperform New- Logistic Regression Loss
ton’s method for the initial layers/steps. To understand what TF
the models are actually learning we also train models with dif- Newton Method
ferent number of layers for the simpler task of linear regression. 1.0
We observe a significant difference in performance for models
trainedwithorwithoutlayernorm. WecomparetheTransform-
0.6
ers with variations of Newton’s iteration with different order
of convergence. 0 10 20 30 40
layers / steps
2 Related work Figure 1: The logistic regression
loss of a trained Transformer with
It has been observed that Transformer-based models have the 1-40 layers and corresponding steps
ability of performing in-context learning, as well as the ability of Newton’s method.
of algorithmic reasoning (Brown et al., 2020; Nye et al., 2021;
Wei et al., 2022a,b; Dasgupta et al., 2022; Zhou et al., 2022).
Recently, Garg et al. (2022) initiated the mathematical formulation of the in-context learning
problem, studying empirically the controlled setting of linear regression. Transformers were able
to perform in-context linear regression, given only (x ,y ) pairs, generated as y = w⊤x , which
i i i ∗ i
were not seen during training. Later on, other works studied this setting both empirically and
theoretically (Akyürek et al., 2022; von Oswald et al., 2022; Bai et al., 2023; Li et al., 2023; Guo
et al., 2023; Chen et al., 2024).
Towards explaining this capability, Akyürek et al. (2022); von Oswald et al. (2023a) showed by
construction that Transformers can emulate gradient descent for the task of linear regression. von
Oswald et al. (2023a), also observed that empirically, one layer of linear attention Transformer had
very similar performance with one step of gradient descent. Indeed, Ahn et al. (2023); Mahankali
et al. (2023) proved that the global minimizer of the in-context learning loss for linear regression
corresponds to one step of (preconditioned) gradient descent.
Related to our work is also the work of Bai et al. (2023), which is the only work - to the best of
our knowledge - that provides a construction of gradient based algorithms for various in-context
learning tasks, including logistic regression; they also demonstrate the ability of Transformer based
models to perform algorithm selection. In the case of learning non-linear functions, Cheng et al.
(2023) showed that Transformers can perform functional gradient descent.
Focusing on performing linear regression through matrix inversion, the recent work of von Oswald
et al. (2023b) is of interest. The authors approximate the inverse of a matrix using Neumman series.
For the task of linear regression this approach requires less memory compared to Newton’s method,
but it has a linear rate of convergence (Wu et al., 2014).
Considering higher order optimization methods, Giannou et al. (2023) first implemented matrix
inversion using Newton iteration, their construction though is sub-optimal, since it uses thirteen
layers to perform just one step of the method and it is given in a general template. In a recent
work by Fu et al. (2023), the authors use Newton’s iteration for matrix inversion to approximate
the closed form solution of linear regression; they compare it with a 12-layer trained transformer,
by linear-probing each layer’s output and comparing it with steps of the iterative algorithm. They
furthermore conclude that Transformers are closer to Newton’s iteration rather than gradient descent.
3In terms of the optimization dynamics, Zhang et al. (2023) proved for one-layer linear attention
the convergence of gradient flow to the global minimizer of the population loss given suitable
initialization. Huang et al. (2023) showed the convergence of gradient descent for training softmax-
attention Transformer under certain orthogonality assumption on the data features.
3 Preliminaries
Notation. Weuselowercaseboldlettersforvectorse.g., x,y,andbyconventionweconsiderthemto
becolumnvectors; formatricesweuseuppercaseboldletterse.g., A,B. Weuseλ(A),σ(A)todenote
the eigenvalues and singular values of a matrix A respectively; we use κ(A) = σ (A)/σ (A) to
max min
denote the condition number of A. For a Positive Symmetric Definite (PSD) matrix A, we denote
√
∥x∥ = x⊤Ax. We use 0 and I to denote a d×d matrix of zeros and the d×d identity matrix,
A d d
respectively. If not specified otherwise, we use ∗ to denote inconsequential values.
3.1 The Transformer architecture
There are multiple variations of the Transformer architecture, depending on which type of attention
(e.g., softmax, ReLU, and linear) is used. In this work, we focus on Transformers with linear
self-attention described as follows: For each layer, let H ∈ Rd×n be the input, where each column is
a d-dimensional embedding vector for each token. Let H be the number of attention heads, and for
each head i ∈ [H], we denote by W(i) ,W(i) ,W(i) ∈ Rd×d the key, query, and value weight matrices,
K Q V
respectively. Further let W ∈ RD×d and W ∈ Rd×D be the weights of the feed-forward network,
1 2
then the output of this layer is given by computing consecutively,
H
Att(H) = H+(cid:88) W(i) H(W(i) H)⊤(W(i) H), (3.1a)
V K Q
i=1
TF(H) = Att(H)+W σ(W Att(H)). (3.1b)
2 1
Here σ(·) denotes the ReLU activation. Consistent with previous literature, the first equation (3.1a)
represents the attention layer, combining which with the feed-forward layer in (3.1b) yields a single
Transformer layer. We note here that the only difference with standard Transformer architecture is
the elimination of the softmax operation and attention mask in the attention layer.
A Transformer model can contain multiple Transformer layers defined as above, and the output
of the whole model would be the composition of multiple layers. For ease of presentation, we omit
the details here. From now on, we refer to Transformers with linear attention layers as linear
Transformers.
3.2 In-context learning using Transformers
In this work we consider two different settings for in-context learning. The first one is that of linear
regression, which is commonly studied in the literature (Akyürek et al., 2022; Bai et al., 2023), while
we also go one step further and investigate the more difficult task of logistic regression. Our target
would be to use the Transformer architecture to emulate in-context known algorithms for solving
these tasks.
3.2.1 Linear Regression
For the task of linear regression, let the pairs {(a ,y )}n be given as input to the Transformer,
i i i=1
where a ∈ Rd and y ∈ R for all i = 1,...,n. We assume that for each such sequence of pairs,
i i
4there is a weight vector w , such that y = w⊤a +ϵ for all i = 1,...,n, where ϵ is some noise.
∗ i ∗ i i i
Given these samples, we want the Transformer to approximate the weight vector w or make a new
∗
prediction on a test point a .
test
Definey = (y ,...,y )⊤ ∈ Rn andA = [a ,...,a ]⊤ ∈ Rn×d. Thestandardleast-squaresolution
1 n 1 n
is given by
wˆ = (A⊤A)−1A⊤y. (3.2)
As a minimizer of the square loss ℓ(w) = (cid:80)n (y −w⊤a )2, wˆ can also be obtained by minimizing
i=1 i i
ℓ(w) using, e.g., gradient descent. Indeed, it has been shown that Transformers can perform gradient
descent to solve linear regression in Akyürek et al. (2022) and later in Bai et al. (2023) with
explicit bounds on width and depth requirements. Specifically, in existing works, the number of
steps (or equivalently, depth of the Transformer) required for convergence up to ϵ error is of order
O(κ(A⊤A)log(1/ϵ)). This suggests that the required number of layers increases linearly with the
condition number.
Another approach is to compute directly the closed form solution (3.2), which involves the matrix
inversion. One choice is Newton’s method, an iterative method that approximates the inverse of
a matrix with logarithmic dependence on the condition number and quadratic convergence to the
accuracy improving upon gradient descent.
Newton’s iteration for matrix inversion. The iteration can be described as follows: Suppose
we want to invert a matrix A, then with initialization X = αA⊤, we compute
0
X = X (2I−AX ) (3.3)
t+1 t t
For α ∈ (0, 2 ), it can be shown that the estimate is ϵ-accurate after O(log κ(A) +
λmax(A⊤A) 2
log log (1/ϵ)) steps (Ogden, 1969; Pan and Schreiber, 1991).
2 2
One interesting generalization of the well-known Newton’s iteration for matrix inversion is the
following family of algorithms (Li and Li, 2010): Initialized at X = αA⊤,
0
n−1 (cid:18) (cid:19)
(cid:88) n
X = X (−1)m (AX )m. (3.4)
t+1 t t
m+1
m=0
We can see that for n = 2 we get the standard Newton’s iteration described in Equation (3.3). For
any fixed n ≥ 2, the corresponding algorithm has an n-th order convergence to the inverse matrix,
i.e., (I−X A) = (I−X A)n, suggesting that the error decays exponentially fast in an order of n.
k+1 k
This results in the improvement of the convergence rate by changing the logarithm basis from log
2
to log . More importantly, the initial overhead of constant steps is also reduced, which is the main
n
overhead of Newton’s iteration. This would become clear in the Section 6, where we compare these
methods against the Transformer architecture.
3.2.2 Logistic Regression
For in-context learning of logistic regression, we consider pairs of examples {(a ,y )}n where
i i i=1
each a ∈ Rd is the covariate vector and y ∈ {−1,1} is the corresponding label. We assume that
i i
y = sign(a⊤w ) for some vector w ∈ Rd. Our target is to find a vector wˆ = argmin f(w)
i i ∗ ∗ w∈Rd
where f : Rd → R is the regularized logistic loss defined as
n
1 (cid:88) µ
f(w) := log(1+exp(−y w⊤a ))+ ∥w∥2. (3.5)
n i i 2 2
i=1
5As in the setting of linear regression, we can use the vector wˆ to make a new prediction on some
point a by calculating 1/(1+exp(−wˆ⊤a )).
test test
The L penalty term is needed to ensure that the loss function is self-concordant in the following
2
sense.
Definition 3.1. [Self-concordant function; Definition 5.1.1, Nesterov et al. 2018] Let f : Rd → R be
a closed convex function that is 3 times continuously differentiable on its domain dom(f) := {x ∈
Rd | f(x) < ∞}. For any fixed x,u ∈ Rd and t ∈ R, define ϕ(t;x,u) := f(x+tu) as a function of t.
Then we say f is self-concordant if there exists a constant M such that, for all x ∈ dom(f) and
f
u ∈ Rd with x+tu ∈ dom(f) for all sufficiently small t,
(cid:12) (cid:12)ϕ′′′(0;x,u)(cid:12) (cid:12) ≤ 2M f(u⊤∇2f(x)u)3/2
We say f is standard self-concordant when M = 1.
f
In particular, the regularized logistic loss is a self-concordant function under a mild assumption
on the data.
Assumption 3.2. For the data {(a ,y )}n in (3.5), it holds that ∥a ∥ ≤ 1 for all i = 1,...,n.
i i i=1 i 2
Proposition 3.3 (Lemma 2, Zhang and Xiao 2015). For µ > 0, the regularized logistic loss f(·)
√
defined in (3.5) is self-concordant with M = 1/ µ under assumption 3.2. Furthermore, the function
f
f/4µ is standard self-concordant.
Self-concordance ensures rapid convergence of second-order optimization algorithms such as
Newton’s method. As in the case of matrix inversion, the rate of convergence is quadratic after a
constant number of steps that depend on how close the algorithm was initialized to the minimum.
Newton’s method. Given the initialization x ∈ Rd, the Newton’s method updates as follows:
0
x = x −η(x )[∇2f(x )]−1∇f(x ). (3.6)
t+1 t t t t
Different choices of the step-size η(x ) lead to different variants of the algorithm: For η = 1 we have
t
(cid:112)
the “classic” Newton’s method; for η(x ) = 1 with λ(x ) = ∇f(x )⊤[∇2f(x )]−1∇f(x ), we
t 1+λ(xt) t t t t
have the so-called damped Newton’s method (see, e.g., Section 4 in Nesterov et al. (2018)).
Forself-concordantfunctions,thedampedNewton’smethodhasguaranteesforglobalconvergence,
which contains two phases: In the initial phase, the quantity λ(x) is decreased until it drops below
the threshold of 1/4. While λ(x) ≥ 1/4, there is a constant decrease per step of the self-concordant
function g of at least 0.02. Then, the second phase begins once λ(x) drops below the 1/4 threshold,
afterwards it will decay with a quadratic rate, implying a quadratic convergence of the objective
value. All together, this implies that c+loglog(1/ϵ) steps are required to reach ϵ accuracy. For the
analysis of the exact Newton’s method for self-concordant functions one may refer to Nesterov et al.
(2018).
4 Main Results for linear regression
We now present our main results on in-context learning for linear regression using Transformers.
The corresponding proofs of results in this section can be found in Appendix A.
Noticethatinordertoobtaintheleast-squaresolutionin(3.2),itsufficestoperformtheoperations
of matrix inversion, matrix multiplications and matrix transposition. The linear Transformer
architecture can trivially perform the last two operations, while our first result in Lemma 4.1 below
shows that it can also efficiently approximate matrix inversion via Newton’s iteration.
6Lemma 4.1. For any dimension d, there exists a linear Transformer consisting of 2 linear attention
layers, each of which has 2 attention heads and width 4d, such that it can perform one step of Newton’s
iteration for any target matrix A ∈ Rd×d. Specifically, the Transformer realizes the following mapping
from input to output for any X ∈ Rd×d:
0
   
X X
0 1
A⊤

A⊤

  (cid:55)→  
0 d  0 d 
I I
d d
where X = X (2I −AX ), corresponding to one step of Newton’s iteration in Equation (3.3).
1 0 d 0
Furthermore, if restricted to only symmetric A, then 1 layer suffices.
Built upon the construction from the above lemma, one can implement multiple steps of Newton’s
iteration by repeatedly applying such constructed layers. This gives rise to the following main result
on how linear Transformer can solve linear regression in-context.
Theorem 4.2 (Linear regression). For any dimension d,n and index T > 0, there exists a linear
Transformer consisting of 3+T layers, where each layer has 2 attention heads and width equal to
4d+3, such that it realizes the following mapping from input to output:
 [I 0]   [I 0] 
d d
 [I d 0]   [I d 0] 
  [I d 0]     [I d 0]  
   
 A⊤  (cid:55)→  A⊤ 
   
 a⊤ ,0   a⊤ ,0 
 test   test 
 y⊤   y⊤ 
0,0,...,0 yˆ,0,...,0
where A = (a ,...,a )⊤, y = (y ,...,y )⊤, and yˆ= a⊤ X A⊤y is the prediction with X being
1 n 1 n test T T
the output of T steps of Newton’s iteration for inversion on the matrix A⊤A, where the initialization
is X = ϵA⊤A for some ϵ ∈ (0, 2 ).
0 λ2 (A⊤A)
max
Remark 4.3. In Theorem 4.2, the three extra layers are used to create the matrix A⊤A, perform the
multiplication AX a , and execute the final multiplication with y⊤.
T test
Remark 4.4. We note here that our construction corresponds to the solution of ridgeless regression,
and it can be easily extended to the case of ridge regression, which amounts to inverting A⊤A+µI
d
for some regularization parameter µ.
Remark 4.5. Considering previous results of implementing gradient descent for this task (e.g.,
Akyürek et al. 2022; Bai et al. 2023), one may observe the memory trade-off between the two
methods. In the case of gradient descent, there is no need of any extra memory, and each step can
be described as w⊤ = w⊤−η(yA−w⊤A⊤A) for some step-size η. Thus we can update the vector
t+1 t t
w without storing the intermediate result A⊤A. While applying Newton’s iteration requires to
t
store the intermediate results, as well as the initialization which results to the need of width equal
to 4d. Nevertheless, the latter achieves better dependence on the condition number (logarithmic
instead of linear) and quadratic decay of the error instead of linear.
The recent work by Fu et al. (2023) studies also Newton’s iteration as a baseline to compare
with Transformers in the setting of linear regression. They showed that Transformers can implement
Newton’s iteration, the stated result needs O(T) layers and O(d) width to perform T steps of
Newton’s iteration on matrices of size d.
Below in Section 6, we will compare the predictions made by different orders of Newton’s iteration
Equation (3.4) with the Transformer architecture, as well as the acquired loss.
75 Main Results for logistic regression
In this section we will present constructive arguments showing that Transformers can approximately
implement Newton’s method for logistic regression. To the best of our knowledge, the only result
prior to our work for logistic regression is that of Bai et al. (2023) in which they implement gradient
descent on the logistic loss using transformers. Newton’s method can achieve a quadratic rate of
convergence instead of linear with respect to the achieved accuracy.
As presented in Section 3.2.2, we seek to minimize the regularized logistic loss defined in (3.5),
which is self-concordant by Proposition 3.3. In a nutshell, our main result in this case shows that
linear Transformer can efficiently emulate Newton’s method to minimize the loss. This is summarized
in the following theorem. We remind that f is the regularized logistic loss of Equation (3.5).
Theorem 5.1. For any dimension d, consider the regularized logistic loss defined in (3.5) with
regularization parameter µ > 0, and define κ = max κ(∇2f(x)). Then for any T > 0 and
f x∈Rd
ϵ > 0, there exists a linear Transformer that can approximate T iterations of Newton’s method on
the regularized logistic loss up to error ϵ per iteration. In particular, the width of such a linear
Transformer can be bounded by O(d(1+µ)6/ϵ4µ8), and its depth can be bounded by T(11+2k), where
(1+µ)3
k ≤ 2logκ +loglog . Furthermore, there is a constant c > 0 depending on µ such that if ϵ < c,
f ϵ2µ2
(cid:112)
then the output of the Transformer provides a w˜ satisfying that ∥w˜ −wˆ∥ ≤ O( ϵ(1+µ)/(4µ)),
2
where wˆ is the global minimizer of the loss.
The proof of Theorem 5.1 contains two main components: The approximate implementation of
Newton’s method by Transformer and the convergence analysis of the resulting inexact Newton’s
method. Below we will address these two parts separately in Section 5.1 and Section 5.2.
5.1 Transformer can implement Newton’s method for logisitic regression
To analyze the convergence properties of Newton’s method on f, we actually implement the updates
on g = f/4µ, which based on Proposition 3.3 is standard self-concordant. Specifically, the targeted
update is
1
x = x − (∇2g(x ))−1∇g(x )
t+1 t t t
1+λ (x )
g t
√
2 µ
= x − √ (∇2f(x ))−1∇f(x )
t t t
2 µ+λ (x )
f t
where the second equality follows directly from the definition g = f/4µ. Our first result is that a
linear Transformer can approximately implement the update above.
Theorem 5.2. Under the setting of Theorem 5.1, there exists a Transformer consisting of linear
attentionwithReLUlayersthatcanapproximatelyperformdampedNewton’smethodontheregularized
logistic loss as follows
√
2 µ
x = x − √ (∇2f(x ))−1∇f(x )+ε
t+1 t t t
2 µ+λ(x )
t
where ε is an error term. For any ϵ > 0, to achieve that ∥ε∥ ≤ ϵ, the width of such a Transformer can
2
(1+µ)8 (1+µ)3
be bounded by O(d ), and its depth can be bounded by 11+2k where k ≤ 2logκ +loglog .
ϵ4µ10 f ϵ2µ2
Remark 5.3. Here we omit the details of input and output format for ease of presentation. See
Appendix C for details. Roughly speaking, the input to the Transformer is of dimension (6d+4)×n
and contains the matrix A, the labels y, and the initialization x .
0
8Below we provide the main idea of the proof by describing main steps of our construction.
Proof sketch of Theorem 5.2. To construct a Transformer that implements Newton’s method to
optimize the regularized logistic loss, we implement first each one of the required components,
including gradient ∇f(x), Hessian ∇2f(x), and the damping parameter λ(x). The gradient and
Hessian of f are
n
1 (cid:88)
∇f(x) = − y p a +µx,
i i i
n
i=1
1
∇2f(x) = A⊤DA+µI ⪰ µI ,
d d
n
where each p := exp(−y x⊤a )/(1+exp(−y x⊤a )) and D := diag(p (1−p ),...,p (1−p )).
i i i i i 1 1 n n
Step 1: Approximate gradient and Hessian. We use the linear attention layer to perform
matrix multiplications, and we use the ReLU network to approximate the following quantities:
• The values p .
i
• The diagonal elements of the matrix D.
• The “hidden” dot product of the diagonal elements of D and the matrix A, since the i-th row of
DA is d a⊤.
i i
These approximations give rise to the approximated gradient and Hessian of f, which we denote by
∇ˆf(x),∇ˆ2f(x).
Step 2: Invert the Hessian. For the next step, we use the construction presented in Lemma 4.1
to invert the matrix ∇ˆ2f(x). We leverage classical results in matrix perturbation theory to show
that (∇ˆ2f(x))−1 = (∇2f(x))−1+E , where E is an error matrix whose norm can be bounded
1,t 1,t
based on the number of iterations performed for the inversion.
√
Step 3: Approximate the step size. Next, we need to approximate the step size √2 µ .
2 µ+λ(x)
This is done using the ReLU layers. Recall that λ(x)2 = ∇f(x)⊤(∇2f(x))−1∇f(x), which can
be approximated using ∇ˆf(x),∇2f(x) from the previous steps. To get the step size, we need to
√
further approximate the function g(z) = 1/(1+ z) and evaluate it at λ(x)2. Thus, any error in the
approximation of λ(x)2 translates to a square root error in the calculation of g(λ(x)2). To see this,
√ √
consider the derivative of the function g, g′(z) = −2 z/(1+ z), and observe that for z of constant
√
order, if z changes by ϵ, the corresponding change in g(z) would be of order ϵ. This leads to the
quadratic requirement of width with respect to the desired error threshold.
Step 4: Aggregate all approximations. Finally, we aggregate all the error terms induced by
each approximation step and get the desired approximation to one step of Newton’s method. ■
5.2 Convergence of inexact Newton’s method
Theorem 5.2 shows that linear Transformers can approximately implement damped Newton’s method
for logistic regression, while providing width requirements in order to control the approximation
error. In complement to this, we further provide convergence analysis for the resulting algorthm.
9Theorem 5.4. Under Assumption 3.2, for the regularized logistic loss f defined in (3.5) with
regularization parameter µ > 0, consider a sequence of iterates {x } satisfying
t t≥0
√
2 µ
x = x − √ ∇2f(x )−1∇f(x )+ε
t+1 t t t t
2 µ+λ(x )
t
where ∥ε ∥ ≤ ϵ for all t ≥ 0. Then there exists constants c,C ,C depending only on µ such that
t 2 1 2
for any ϵ ≤ c, it holds that g(x )−g(x∗) ≤ ϵ for all t ≥ C +C loglog 1.
t 1 2 ϵ
The proof of Theorem 5.4 follows the proof presented in Nesterov et al. (2018) but accounts for
the error term. Similar analysis has been performed in the past in Sun et al. (2020). Our proof
consists of the following steps:
• Feasibility: We first show by induction that there exists a constant depending on µ such
that ∥x ∥ ≤ C for all t ≥ 0. This is a consequence of the bounded norm assumption in
t 2
Assumption 3.2 and the strongly convex L regularizer.
2
• Constant decrease: We then show constant decreasing of the loss value per step when
λ(x ) ≥ 1/6. This is achieved by upper bounding the suboptimality of g(x ) a function of
t t
λ(x ), which is guaranteed by the self-concordance of g.
t
• Quadratic convergence: In the last step, we show that when λ(x ) < 1/6, the inequality
t
λ(x ) ≤ cλ(x )2+ϵ′ holds, which implies further quadratic convergence to achieving O(ϵ)
t+1 t
error.
See Appendix B.2 for the complete proof of Theorem 5.4. Finally, combining the construction in
Theorem 5.2 and the convergence analysis in Theorem 5.4 yields the performance guarantee of the
constructed Transformer.
6 Experiments
In this section, we corroborate our theoretical findings with empirical evidence. Specifically, we aim
to empirically demonstrate the effectiveness of the linear self-attention model when it is trained to
solve linear regression as well as logistic regression using encoder-based models. Our code is available
here2.
Linear regression. For the task of linear regression, we train models consisting of linear self-
attention (LSA) with and without LayerNorm (LN) to learn in-context. We use an embedding
dimensionof64and4attentionheads. Thedatadimensionis10,andtheinputcontains50in-context
samples.
We train models with LSA, having from 1 to 6 layers, employing the training technique of von
Oswald et al. (2023a) to stabilize the training process. Consistent with the findings reported in
von Oswald et al. (2023a), we observe that after four layers, there is no significant improvement in
performance by adding more layers. We attribute this to limitations in the training process, and
the optimal training method for linear Transformers beyond 5−6 layers remains unidentified. We
furthermore train LSA models with LN to be able to test more than 6 layers, but to also test how
LN affects the capabilities of these models.
2https://anonymous.4open.science/r/transformer_higher_order-B80B/
10Linear Regression Error Linear Regression Error
101 100
10 2 10 3
10 5 10 6
LSA LSA w/ LN
10 8 Newton(order=2) 10 9 Newton(order=4)
Newton(order=3) Newton(order=6)
10 11 Newton(order=4) 10 12 Newton(order=8)
Newton(order=5) Newton(order=10)
1 2 3 4 5 6 1 2 3 4 5 6
layers / steps layers / steps
Figure 2: Loss of LSA, LSA with layernorm and different order Newton iteration for linear regression error.
In Figure 2, we plot the in-distribution loss (meaning we keep the same sampling method as in
the training process) on new test samples. We observe that models having 4 or more layers have
almost the same performance in terms of the loss.
To get a clearer picture of what the models are learning, we further provide plots illustrating the
actual outputs of the trained Transformers, as shown in Figures 3 and 5. Here we test the trained
models in the following set-up: we first sample a batch of 5000 examples, i.e., {(x ,y )}n data
i i i=1
pairs that all share the same underlying weight vector y = w⊤x , for all i = 1,...,n and all the
i ∗ i
batches. We then pick a specific x sample, which we keep the same for all the batches. For this
test
test sample, the value of its first coordinate varies across [−a,a] for different values of a.
Wetesttwodifferentcases: 1)thevalueahasbeenencounteredinthetrainingsetwithprobability
at least 0.99 and 2) the value is an outlier. We calculate this value to be [−14.9,14.9], which is the
range of the ‘in-distribution’ plots in Figures 3 and 5 (range in between the dashed line), while in
Figures 4 and 6 we have the ‘out-of-distribution’ ones.
Looking at Figure 3, one may observe that the performance of the model lies between the second-
and third-order Newton’s iteration. Intuitively, Transformers do have slighter higher order capacity
than Newton’s iteration. To see that, assume that the model is given in the input a symmetric
matrix A, then in the first layer from Equation (3.1a) the model can create up to the third power
of the matrix A, i.e., all powers for 1−3. Now in the second layer the higher-order term can be
up to power of 9; in contrast the second-order Newton’s iteration can reach up to order 7 (the first
iteration X ∼ A3, and in the second one X ∼ X AX ∼ A7.
1 2 1 1
The gap between the possible powers of the matrix, between second-order Newton’s and Trans-
formers will increase as the number of layers increases. On the other hand, third order Newton’s has
up to power A5 in the fist step X . While in second one the maximum power is X (AX )2 ∼ A17.
1 1 1
Thus, the LSA Transformer will not be able to outperform it.
Logistic regression. We further analyze the Transformer’s ability to solve logistic regression
tasks. To simplify the setting, we focus on training the Transformer to predict the logistic regression
parameter w. Since predicting the true weight directly is a hard task and is not necessarily the
solution of the minimization problem described in Equation (3.5), we instead opt to train the
Transformer to output a solution comparable to that given by Newton’s Method.
1120 1 20 2 20 3 20 4 20 5 20 6
0 0 0 0 0 0
20 20 20 20 20 20
20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20
20 20 20 20 20 20
0 0 0 0 0 0
20 20 20 20 20 20
20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20
20 20 20 20 20 20
0 0 0 0 0 0
20 20 20 20 20 20
20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20
Figure 3: Output of LSA without LayerNorm and Newton’s iteration by keeping the test sample fixed and
changing one of its coordinates, within in-distribution. We plot 1−6 layers against 1−6 steps of second and
third order Newton’s iteration ; we observe that the model lies between second and third order.
100 100 100 100 100 100
1 2 3 4 5 6
0 0 0 0 0 0
100 100 100 100 100 100
100 0 100 100 0 100 100 0 100 100 0 100 100 0 100 100 0 100
100 100 100 100 100 100
0 0 0 0 0 0
100 100 100 100 100 100
100 0 100 100 0 100 100 0 100 100 0 100 100 0 100 100 0 100
100 100 100 100 100 100
0 0 0 0 0 0
100 100 100 100 100 100
100 0 100 100 0 100 100 0 100 100 0 100 100 0 100 100 0 100
Figure 4: Same setting as above, but the models/algorithms are tested with out-of-distribution values as well.
We observe that the model does not actually learn the underlying linear function.
Logistic Regression Loss Training Error
TF TF
Newton Method 101
1.0
0.6 102
0 10 20 30 40 0 10 20 30 40
layers / steps layers
Figure 7: Performance of Transformer on logistic regression tasks. (Left) The logistic regression loss for
the Transformer (TF) and the Newton Method, with a regularization µ=0.1. According to our theoretical
construction, a single step of the Newton Method can be implemented by at least 11 layers, therefore, we
have scaled the Newton Method plot to 13 layers per step. The Transformer is shown to approximate the
method more effectively within a few layers. (Right) The training error of the Transformer when it is trained
to predict the solution derived from Newton’s Method.
12
ASL
2
.drO
3
.drO
ASL
2
.drO
3
.drO1 3 5 7 10 12
20 20 20 20 20 20
0 0 0 0 0 0
20 20 20 20 20 20
20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20
20 20 20 20 20 20
0 0 0 0 0 0
20 20 20 20 20 20
20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20
20 20 20 20 20 20
0 0 0 0 0 0
20 20 20 20 20 20
20 0 20 20 0 20 20 0 20 20 0 20 20 0 20 20 0 20
Figure 5: Output of LSA with LayerNorm and Newton’s iteration by keeping the test sample fixed and
changing one of its coordinates, within in-distribution. We observe that LN improves the performance of the
model. The spikes are noted in the out-of-distribution range.
100 100 100 100 100 100
1 3 5 7 10 12
0 0 0 0 0 0
100 100 100 100 100 100
100 0 100 100 0 100 100 0 100 100 0 100 100 0 100 100 0 100
100 100 100 100 100 100
0 0 0 0 0 0
100 100 100 100 100 100
100 0 100 100 0 100 100 0 100 100 0 100 100 0 100 100 0 100
100 100 100 100 100 100
0 0 0 0 0 0
100 100 100 100 100 100
100 0 100 100 0 100 100 0 100 100 0 100 100 0 100 100 0 100
Figure 6: Same setting as above, but the models/algorithms are tested with out-of-distribution values as well.
5 layers seem to perform better for out-of-distribution than 7,10,12, not though in-distribution.
We train on data dimension 5, and the input contains 26 in-context samples, and embedding
dimension 32, with 4 heads. We set the regularization parameter µ = 0.1 in the regularized logistic
loss in (3.5). In Figure 7, we compare the loss value of the trained Transformer across different
number of layers and the loss value of different iterates of Newton’s method in the left plot; we
further plot the corresponding training error in the right plot. According to our construction in
Theorem 5.2, a single step of the Newton Method is equivalent to 13 layers of a standard Transformer.
However, when trained, the Transformer can approximate the Newton Method even more effectively
as shown in Figure 7.
7 Discussion
In this paper we provide explicit construction of Transformers that can efficiently perform the matrix
inversionoperation. Builtuponthis, wefurthershowthatTransformerscancomputetheleast-square
13
NL
/w
ASL
3
.drO
5
.drO
NL
/w
ASL
3
.drO
5
.drOsolution to solve the linear regression task in-context. Moreover, for in-context learning of logistic
regression, it also gives rise to construction of Transformers that can perform Newton’s method
to optimize the regularized logistic loss. We provide concrete convergence analysis of the inexact
Newton’s method emulated by the constructed Transformer. We further examine and compare the
empirical performance of the trained Transformers with those of the higher-order methods.
References
Ahn, K., Cheng, X., Daneshmand, H. and Sra, S. (2023). Transformers learn to implement
preconditioned gradient descent for in-context learning.
Akyürek, E., Schuurmans, D., Andreas, J., Ma, T. and Zhou, D. (2022). What learning
algorithmisin-contextlearning? investigationswithlinearmodels.arXivpreprintarXiv:2211.15661
.
Bai, Y., Chen, F., Wang, H., Xiong, C. and Mei, S. (2023). Transformers as statisticians:
Provable in-context learning with in-context algorithm selection.
Black, S.,Biderman, S.,Hallahan, E.,Anthony, Q.,Gao, L.,Golding, L.,He, H.,Leahy,
C., McDonell, K., Phang, J. et al. (2022). Gpt-neox-20b: An open-source autoregressive
language model. arXiv preprint arXiv:2204.06745 .
Boyd, S. P. and Vandenberghe, L. (2004). Convex optimization. Cambridge university press.
Brown, T.,Mann, B.,Ryder, N.,Subbiah, M.,Kaplan, J. D.,Dhariwal, P.,Neelakantan,
A., Shyam, P., Sastry, G., Askell, A. et al. (2020). Language models are few-shot learners.
Advances in neural information processing systems 33 1877–1901.
Chen, S., Sheen, H., Wang, T. and Yang, Z. (2024). Training dynamics of multi-head soft-
max attention for in-context learning: Emergence, convergence, and optimality. arXiv preprint
arXiv:2402.19442 .
Cheng, X., Chen, Y. and Sra, S. (2023). Transformers implement functional gradient descent to
learn non-linear functions in context.
Dasgupta, I., Lampinen, A. K., Chan, S. C., Creswell, A., Kumaran, D., McClelland,
J. L. and Hill, F. (2022). Language models show human-like content effects on reasoning. arXiv
preprint arXiv:2207.07051 .
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. et al. (2020). An image is worth
16x16 words: Transformers for image recognition at scale. In International Conference on Learning
Representations.
Fu, D.,Chen, T.-Q.,Jia, R.andSharan, V.(2023). Transformerslearnhigher-orderoptimization
methods for in-context learning: A study with linear models.
Garg, S., Tsipras, D., Liang, P. S. and Valiant, G. (2022). What can transformers learn
in-context? a case study of simple function classes. Advances in Neural Information Processing
Systems 35 30583–30598.
14Giannou, A., Rajput, S., Sohn, J.-y., Lee, K., Lee, J. D. and Papailiopoulos, D. (2023).
Looped transformers as programmable computers. arXiv preprint arXiv:2301.13196 .
Guo, T.,Hu, W.,Mei, S.,Wang, H.,Xiong, C.,Savarese, S.andBai, Y.(2023). Howdotrans-
formers learn in-context beyond simple functions? a case study on learning with representations.
arXiv preprint arXiv:2310.10616 .
Huang, Y., Cheng, Y. and Liang, Y. (2023). In-context convergence of transformers. arXiv
preprint arXiv:2310.05249 .
Kenton, J. D. M.-W. C. and Toutanova, L. K. (2019). Bert: Pre-training of deep bidirectional
transformers for language understanding. In Proceedings of NAACL-HLT.
Khan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S. and Shah, M. (2022). Trans-
formers in vision: A survey. ACM computing surveys (CSUR) 54 1–41.
Li, W. and Li, Z. (2010). A family of iterative methods for computing the approximate inverse of a
square matrix and inner inverse of a non-square matrix. Applied Mathematics and Computation
215 3433–3442.
Li, Y., Ildiz, M. E., Papailiopoulos, D. and Oymak, S. (2023). Transformers as algorithms:
Generalization and stability in in-context learning. International Conference on Machine Learning
.
Lieber, O., Sharir, O., Lenz, B. and Shoham, Y. (2021). Jurassic-1: Technical details and
evaluation. White Paper. AI21 Labs 1 9.
Mahankali, A.,Hashimoto, T.B.andMa, T.(2023). Onestepofgradientdescentisprovablythe
optimal in-context learner with one layer of linear self-attention. arXiv preprint arXiv:2307.03576
.
Nesterov, Y. et al. (2018). Lectures on convex optimization, vol. 137. Springer.
Nye, M.,Andreassen, A. J.,Gur-Ari, G.,Michalewski, H.,Austin, J.,Bieber, D.,Dohan,
D., Lewkowycz, A., Bosma, M., Luan, D. et al. (2021). Show your work: Scratchpads for
intermediate computation with language models. arXiv preprint arXiv:2112.00114 .
Ogden, H. C. (1969). Iterative methods of matrix inversion .
Pan, V. and Schreiber, R. (1991). An improved newton iteration for the generalized inverse of a
matrix, with applications. SIAM Journal on Scientific and Statistical Computing 12 1109–1130.
Schulz, G. (1933). Iterative berechung der reziproken matrix. ZAMM - Journal of Applied
Mathematics and Mechanics / Zeitschrift für Angewandte Mathematik und Mechanik 13 57–59.
Stewart, G. W. and guang Sun, J. (1990). Matrix perturbation theory.
Sun, T., Necoara, I. and Tran-Dinh, Q. (2020). Composite convex optimization with global
and local inexact oracles. Computational Optimization and Applications 76 69–124.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł.
and Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing
systems 30.
15von Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zh-
moginov, A. and Vladymyrov, M. (2022). Transformers learn in-context by gradient descent.
arXiv preprint arXiv:2212.07677 .
von Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zh-
moginov, A. and Vladymyrov, M. (2023a). Transformers learn in-context by gradient descent.
von Oswald, J., Niklasson, E., Schlegel, M., Kobayashi, S., Zucchet, N., Scherrer,
N., Miller, N., Sandler, M., y Arcas, B. A., Vladymyrov, M., Pascanu, R. and
Sacramento, J. (2023b). Uncovering mesa-optimization algorithms in transformers.
Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D.,
Bosma, M., Zhou, D., Metzler, D. et al. (2022a). Emergent abilities of large language
models. arXiv preprint arXiv:2206.07682 .
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q. and Zhou, D. (2022b). Chain
of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903
.
Wu, M., Yin, B., Wang, G., Dick, C., Cavallaro, J. R. and Studer, C. (2014). Large-scale
mimo detection for 3gpp lte: Algorithms and fpga implementations. IEEE Journal of Selected
Topics in Signal Processing 8 916–929.
Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Jiang, Z.-H., Tay, F. E., Feng, J. and
Yan, S. (2021). Tokens-to-token vit: Training vision transformers from scratch on imagenet. In
Proceedings of the IEEE/CVF International Conference on Computer Vision.
Zhang, R., Frei, S. and Bartlett, P. L. (2023). Trained transformers learn linear models
in-context. arXiv preprint arXiv:2306.09927 .
Zhang, Y.andXiao, L.(2015). Communication-efficientdistributedoptimizationofself-concordant
empirical loss.
Zhou, H., Nova, A., Larochelle, H., Courville, A., Neyshabur, B. and Sedghi, H. (2022).
Teaching algorithmic reasoning via in-context learning. arXiv preprint arXiv:2211.09066 .
16A Constructions of Transformers for linear regression
In this section we provide the exact constructions for implementing Newton’s Iteration for matrix
inversion. We then use this construction to approximate the closed form solution of linear regression
in-context.
A.1 Newton-Raphson Method for Matrix Inverse
The Newton-Raphson method (Schulz, 1933) for inverting a matrix A ∈ Rd×d is defined by the
following update rule:
X = 2X −X AX . (A.1)
t+1 t t t
An initialization for this method that guarantees convergence is X = ϵA⊤, where ϵ ∈ (0, 2 ).
0 λ1(AA⊤)
We prove below that linear-attention transformers can emulate the above update.
Lemma 4.1. For any dimension d, there exists a linear Transformer consisting of 2 linear attention
layers, each of which has 2 attention heads and width 4d, such that it can perform one step of Newton’s
iteration for any target matrix A ∈ Rd×d. Specifically, the Transformer realizes the following mapping
from input to output for any X ∈ Rd×d:
0
   
X X
0 1
A⊤

A⊤

  (cid:55)→  
0 d  0 d 
I I
d d
where X = X (2I −AX ), corresponding to one step of Newton’s iteration in Equation (3.3).
1 0 d 0
Furthermore, if restricted to only symmetric A, then 1 layer suffices.
Proof. Assume that we are given the following input
 
X
0
H 0 =   A 0    ∈ R4d×d.
I
d
For the first layer, we choose
 0 0 0 0   0 0 0 I  I 0 0 0 
d d d d d d d d d d d d
W V =   0 0d
d
0 I dd 00 dd 00 dd  ,W K =   0 0d
d
0 0d
d
0 0d
d
0 0d d  ,W Q =   0 0d
d
0 0d
d
0 0d
d
0 0d d 

0 0 0 0 0 0 0 0 0 0 0 0
d d d d d d d d d d d d
so that

0
 I  
X

d d 0
W VH 0 =   0 Ad  , W KH 0 =   0 0d d  , W QH 0 =   0 0d d  .
0 0 0
d d d
Then the output of the first layer is
 
X
0
H 1/2 = H 0+W VH 0(W KH 0)⊤W QH 0 =   AA X 0  .
I
d
17Next, we use two attention heads for the second layers. For the first head, we choose
I 0 0 0   0 0 0 I   0 0 −I 0 
d d d d d d d d d d d d
W(1) =  0 d 0 d 0 d 0 d ,W(1) =  0 d 0 d 0 d 0 d ,W(1) =  0 d 0 d 0 d 0 d 
V 0 d 0 d 0 d 0 d K 0 d 0 d 0 d 0 d Q 0 d 0 d 0 d 0 d
0 0 0 0 0 0 0 0 0 0 0 0
d d d d d d d d d d d d
so that

X
 I  
−AX

0 d 0
W V(1) H 1/2 =   0 0d d  , W K(1) H 1/2 =   0 0d d  , W Q(1) H 1/2 =  

0 0d
d
  .
0 0 0
d d d
For the second head, we choose
I 0 0 0   0 0 0 I   0 0 0 I 
d d d d d d d d d d d d
W V(2) =   0 0d
d
0 0d
d
−0 Id
d
0 0d d  ,W K(2) =   0 0d
d
0 0d
d
0 0d
d
0 0d d  ,W Q(2) =   0 0d
d
0 0d
d
0 0d
d
0 0d d 

0 0 0 0 0 0 0 0 0 0 0 0
d d d d d d d d d d d d
so that

X
 I  I 
0 d d
W V(2) H 1/2 =   −A0 d X 0  , W K(2) H 1/2 =   0 0d d  , W Q(2) H 1/2 =   0 0d d  .
0 0 0
d d d
Combining the outputs of the two heads, we get the output of the second layer as
H = H
+W(1)
H
(W(1)
H
)⊤W(1)
H
+W(2)
H
(W(2)
H
)⊤W(2)
H
1 1/2 V 1/2 K 1/2 Q 1/2 V 1/2 K 1/2 Q 1/2
   
−X AX X
0 0 0
 0   0 
= H 1/2+
 0
 +
−AX
0

0 0
 
X
1
A
=  .
 0 
I
d
This completes the proof. For the case where the matrix A is symmetric, we give the construction
as part of the proof for linear regression. See the proof of Theorem 4.2 in Appendix A.2. ■
A.2 Linear regression through matrix inversion.
Given the least-square solution, the prediction for a fresh test point a is
test
y = a⊤ (A⊤A)−1A⊤y = y⊤A(A⊤A)−1a .
test test test
Below we give a construction of Transformer that can emulate the Newton-Raphson method to
approximate the inverse of A⊤A and then multiply the result with the necessary quantities to obtain
an approximation of y . Notice that here the matrix we want to invert, A⊤A, is symmetric.
test
18Theorem 4.2 (Linear regression). For any dimension d,n and index T > 0, there exists a linear
Transformer consisting of 3+T layers, where each layer has 2 attention heads and width equal to
4d+3, such that it realizes the following mapping from input to output:
 [I 0]   [I 0] 
d d
 [I d 0]   [I d 0] 
  [I d 0]     [I d 0]  
   
 A⊤  (cid:55)→  A⊤ 
   
 a⊤ ,0   a⊤ ,0 
 test   test 
 y⊤   y⊤ 
0,0,...,0 yˆ,0,...,0
where A = (a ,...,a )⊤, y = (y ,...,y )⊤, and yˆ= a⊤ X A⊤y is the prediction with X being
1 n 1 n test T T
the output of T steps of Newton’s iteration for inversion on the matrix A⊤A, where the initialization
is X = ϵA⊤A for some ϵ ∈ (0, 2 ).
0 λ2 (A⊤A)
max
Proof. We denote by H the input to the Transformer. To prove the desired result we present in
0
steps the construction.
Step 1: Initialize (1 layer). We consider one Transformer layer with 2 heads. For the first head,
we choose
 0 0 0 ϵI 0 0 0  0 0 0 I 0 0 0 I 0 0 0 0 0 0
d d d
0 0 0 I
d
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     
W(1) = 0 0 0 0 0 0 0,W(1) = 0 0 0 0 0 0 0,W(1) = 0 0 0 0 0 0 0
V . . . . . . . K . . . . . . . Q  . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . .
     
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
so that
 ϵA⊤  A⊤  [I 0]
d
A⊤   0   0 
     
W(1) H =  0 , W(1) H =  0 , W(1) H =  0 
V 0  .  K 0  .  Q 0  . 
 . .   . .   . . 
     
0 0 0
For the second head, we choose
 −I 0 0 0 0 0 0 I 0 0 0 0 0 0 I 0 0 0 0 0 0
d d d
−I
d
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     
W(2) =  0 0 0 0 0 0 0,W(2) = 0 0 0 0 0 0 0,W(2) = 0 0 0 0 0 0 0
V  . . . . . . . K  . . . . . . . Q  . . . . . . .
 . . . . . . . . . . . . . .  . . . . . . . . . . . . . .  . . . . . . . . . . . . . .
     
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
so that
 [−I 0]  [I 0]  [I 0]
d d d
[−I d 0]  0   0 
     
W(2) H =  0 , W(2) H =  0 , W(2) H =  0 .
V 0  .  K 0  .  Q 0  . 
 . .   . .   . . 
     
0 0 0
19Then combining the above two heads, we have
H = H
+W(1)
H
(W(1)
H
)⊤W(1)
H
+W(2)
H
(W(2)
H
)⊤W(2)
H
1 0 V 0 K 0 Q 0 V 0 K 0 Q 0
 [−I 0]  [ϵA⊤A 0]
d
[−I
d
0] [A⊤A 0]
   
 0   0 
   
= H 0− 0 + 0 
   
 0   0 
   
 0   0 
0 0
 [ϵA⊤A 0]
[A⊤A 0]
  [I d 0]  
 
=  A⊤ .
 
 [a⊤ 0] 
 test 
 y⊤ 
0,0,...,0
Step 2: Implement T steps of Newton-Raphson (T layers). We now define X = ϵA⊤A
0
and R = A⊤A. The input matrix to the next layer is in the following form (for t = 0)
 
[X 0]
t
 [R 0] 
  [I d 0]  
 
H t =  A⊤ .
 
[a⊤ 0]
 test 
 y⊤ 
0,...,0
We will show that one Transformer laye with two heads can yield the following output
 
[X 0]
t+1
 [R 0] 
  [I d 0]  
 
H t+1 =  A⊤ .
 
[a⊤ 0]
 test 
 y⊤ 
0
Here we choose the weight matrices for the first head to be
 −I 0 0 0 0 0 0 I 0 0 0 0 0 0 I 0 0 0 0 0 0
d d d
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     
W(1) =  0 0 0 0 0 0 0,W(1) = 0 0 0 0 0 0 0,W(1) = 0 0 0 0 0 0 0,
V  . . . . . . . K  . . . . . . . Q  . . . . . . .
 . . . . . . . . . . . . . .  . . . . . . . . . . . . . .  . . . . . . . . . . . . . .
     
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
20so that
     
[−X 0] [R 0] [X 0]
t t
 0   0   0 
     
W(1) H =  0 , W(1) H =  0 , W(1) H =  0 .
V t  .  K t  .  Q t  . 
 . .   . .   . . 
     
0 0 0
Similarly, for the second head, we choose
I 0 0 0 0 0 0  0 0 I 0 0 0 0  0 0 I 0 0 0 0
d d d
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     
W(2) = 0 0 0 0 0 0 0,W(2) = 0 0 0 0 0 0 0,W(2) = 0 0 0 0 0 0 0
V  . . . . . . . K . . . . . . . Q . . . . . . .
 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
     
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
so that
 [X 0]  [I 0]  [I 0]
t d d
 0   0   0 
     
W(2) H =  0 , W(2) H =  0 , W(2) H =  0 .
V t  .  K t  .  Q t  . 
 . .   . .   . . 
     
0 0 0
Combining these two heads, we obtain
H = H
+W(1)
H
(W(1)
H
)⊤W(1)
H
+W(2)
H
(W(2)
H
)⊤W(2)
H
t+1 t V t K t Q t V t K t Q t
   
−[X RX 0] [X 0]
t t t
 0   0 
= H + . + . 
t  . .   . . 
   
0 0
 
[2X −X RX 0]
t t t
 [R 0] 
  [I d 0]  
 
=  A⊤ 
 
 [a⊤ 0] 
 test 
 y⊤ 
0,...,0
 
[X 0]
t+1
 [R 0] 
  [I d 0]  
 
=  A⊤ .
 
[a⊤ 0]
 test 
 y⊤ 
0,...,0
21Repeating the above for T many layers yields
 
[X 0]
T
 [R 0] 
  [I d 0]  
 
H T+1 =  A⊤ .
 
[a⊤ 0]
 test 
 y⊤ 
0,...,0
Step 3: Output (2 layers). We now create first the matrix y⊤AX with one Transformer layer
T
and then the final prediction output with another layer. For the first layer, we choose
 0 0 0 0 0 0 0  0 0 I 0 0 0 0  0 0 I 0 0 0 0
d d
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
. . . . . . .    
W = . . . . . . . . . . . . . .,W = 0 0 0 0 0 0 0,W = 0 0 0 0 0 0 0
V   K . . . . . . . Q . . . . . . .
 0 0 0 0 0 0 0   . . . . . . . . . . . . . .   . . . . . . . . . . . . . . 
0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
so that

0
  A⊤ 
[X
0]
T
 0   0   0 
 .     
W H =  . . , W H =  0 , W H =  0 .
V T+1   K T+1  .  Q T+1  . 
  0     . .     . .  
y⊤ 0 0
The output of this layer is
 0   [X 0] 
T
 0   [R 0] 
  0     [I d 0]  
H T+2 = H T+1+W VH T+1(W KH T+1)⊤W QH T+1 = H T+1+  0   =   A⊤  .
   
 0   [a⊤ 0] 
   test 
 0   y⊤ 
[y⊤AX 0] [y⊤AX 0]
T T
Now for the final step, we construct a Transformer layer with two heads. For the first head, we
choose
     
0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
. . . . . . .    
W(1) = . . . . . . . . . . . . . .,W(1) = 0 0 0 0 0 0 0,W(1) = 0 0 0 0 0 0 0
V   K . . . . . . . Q . . . . . . .
 0 0 0 0 0 0 0   . . . . . . . . . . . . . .   . . . . . . . . . . . . . . 
0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
so that
 0   [a⊤ 0]  [1 0]
test
 0   0   0 
 .     
W(1) H =  . . , W(1) H =  0 , W(1) H =  0 .
V T+2   K T+2  .  Q T+2  . 
  0     . .     . .  
[y⊤AX 0] 0 0
T
22For the second head, we choose
 0 0 0 0 0 0 0  0 0 I 0 0 0 0  0 0 I 0 0 0 0
d d
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
. . . . . . .    
W(2) = . . . . . . . . . . . . . .,W(2) = 0 0 0 0 0 0 0,W(2) = 0 0 0 0 0 0 0
V   K . . . . . . . Q . . . . . . .
 0 0 0 0 0 0 0   . . . . . . . . . . . . . .   . . . . . . . . . . . . . . 
0 0 0 0 0 −1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
so that
 0   [I 0]  [I 0]
d d
 0   0   0 
 .     
W(2) H =  . . , W(2) H =  0 , W(2) H =  0 .
V T+2   K T+2  .  Q T+2  . 
  0     . .     . .  
[y⊤AX 0] 0 0
T
Putting these together, we obtain
 [X 0] 
T
   
0 0  [R 0] 
 0   0    [I d 0]  
     
H T+3 = H T+2+ 0 + 0  =  A⊤ 
     
 0   0   [a⊤ 0] 
 test 
[y⊤AX Ta test 0] −[y⊤AX T 0]  y⊤ 
[y⊤AX a 0]
T test
Notice that the last element of the last row is the desired quantity yˆ= y⊤AX a . This completes
t test
the proof. ■
B Convergence of inexact damped Newton’s method for regularized
logistic regression
We first review some basics for self-concordant functions in Appendix B.1, and then provide the
proof of Theorem 5.4 in Appendix B.2.
B.1 Preliminaries on self-concordant functions
We review some results on self-concordant functions that will be useful for the next sections. Most
of these theorems can be found in Boyd and Vandenberghe (2004); Nesterov et al. (2018).
First recall the definition of self-concordant functions.
Definition 3.1. [Self-concordant function; Definition 5.1.1, Nesterov et al. 2018] Let f : Rd → R be
a closed convex function that is 3 times continuously differentiable on its domain dom(f) := {x ∈
Rd | f(x) < ∞}. For any fixed x,u ∈ Rd and t ∈ R, define ϕ(t;x,u) := f(x+tu) as a function of t.
Then we say f is self-concordant if there exists a constant M such that, for all x ∈ dom(f) and
f
u ∈ Rd with x+tu ∈ dom(f) for all sufficiently small t,
(cid:12) (cid:12)ϕ′′′(0;x,u)(cid:12) (cid:12) ≤ 2M f(u⊤∇2f(x)u)3/2
We say f is standard self-concordant when M = 1.
f
23For the regularized logistic regression problem that we consider, the objective function is strongly
convex, and the theorems provided below use the strong convexity. Nonetheless, these theorems
have more general forms, as detailed in Chapter 5 of Nesterov et al. (2018).
In the sequel, let f be a self-concordant function.
Definition B.1 (Dikin ellipsoid). For a function f : Rd → R, consider the following sets for any
x ∈ Rd and r > 0:
W(x;r) = {y ∈ Rd : ∥y−x∥ < r}
∇2f(x)
where cl(·) defines the closure of a set. This set is called the Dikin ellipsoid of the function f at x.
Theorem B.2 (Theorem 5.1.5 in Nesterov et al. (2018)). Let f : Rd → R be a self-concordant
function. Then for any x ∈ dom(f), it holds that W(x;1/M ) ⊆ dom(f) .
f
Theorem B.3 (Theorem5.1.8&5.1.9inNesterovetal.(2018)). Let f : Rd → R be a self-concordant
function. Then for any x,y ∈ dom(f), it holds that
ω(M ∥y−x∥ ) ω (M ∥y−x∥ )
f ∇2f(x) ∗ f ∇2f(x)
f(x)+⟨∇f(x),y−x⟩+ ≤ f(y) ≤ f(x)+⟨∇f(x),y−x⟩+
M2 M2
f f
where ω(t) = t−ln(1+t), ω (t) = −t−ln(1−t).
∗
Lemma B.4 (Lemma 5.1.5 in Nesterov et al. (2018)). For any t ≥ 0, the functions ω(t),ω (t) in
∗
Theorem B.3 satisfy:
t2 t2 t2 t2 t2
≤ ≤ ω(t) ≤ , ≤ ω (t) ≤ .
∗
2(1+t) 2(1+2t/3) 2+t 2−t 2(1−t)
Theorem B.5 (Theorem 5.1.7, Nesterov et al. 2018). Let f : Rd → R be a self-concordant function.
Then for any x ∈ dom(f) and any y ∈ W(x;1/M ), it holds that
f
1
(1−M r)2·∇2f(x) ⪯ ∇2f(y) ⪯ ·∇2f(x).
f (1−M r)2
f
where r = ∥y−x∥ .
∇2f(x)
A useful corollary of the above theorem is
Corollary B.6 (Corollary 5.1.5, Nesterov et al. 2018). Let f : Rd → R be a self-concordant function.
Then for any x ∈ dom(f) and any y such that r = ∥y−x∥ < 1/M , it holds that
∇2f(x) f
(cid:18) 1 (cid:19) (cid:90) 1 1
1−M r+ M2r2 ·∇2f(x) ⪯ ∇2f(x+τ(y−x))dτ ⪯ ·∇2f(x).
f 3 f 1−M r
0 f
A key quantity in the analysis of Newton method for self-concordant functions is the so-called
Newton decrement:
λ (x) = (∇f(x)⊤∇2f(x)−1∇f(x))1/2 (B.1)
f
The following theorem characterizes the sub-optimality gap in terms of the Newton decrement for a
strongly convex self-concordant function.
Theorem B.7 (Theorem 5.1.13 in Nesterov et al. (2018)). Let f : Rd → R be a strongly convex
self-concordant function with x∗ = argmin f(x). Suppose λ (x) < 1/M for some x ∈ dom(f),
x f f
then the following holds:
1
f(x)−f(x∗) ≤ ·ω (M λ (x))
M2 ∗ f f
f
where ω (·) is the function defined in Theorem B.3.
∗
24B.2 Convergence analysis of inexact damped Newton’s method
In this section we consider the inexact damped Newton’s method for optimizing the regularized
logistic loss:
xˆ = xˆ −η(xˆ )(∇2f(xˆ ))−1∇f(xˆ )+ε (B.2)
t+1 t t t t t
where ε is an error term. For simplicity, we also define
t
∆ := −η(xˆ )(∇2f(xˆ ))−1∇f(xˆ )+ε . (B.3)
t t t t t
In other words, we have xˆ = xˆ +∆ .
t+1 t t
Recall that the regularized logistic loss defined in Equation (3.5) is self-concordant with M =
√ f
1/ µ, as a consequence of Proposition 3.3.
Lemma B.8. Under Assumption 3.2, let f be the regularized logistic loss defined in Equation (3.5)
with regularization parameter µ > 0. Then the following bounds hold
−1+µ∥x∥ ≤ ∥∇f(x)∥ ≤ 1+µ∥x∥ ,
2 2 2
µ ≤
(cid:13) (cid:13)∇2f(x)(cid:13)
(cid:13) ≤ 1+µ.
op
(cid:112)
Consequently, for the Newton decrement λ(x) = ∇f(x)⊤(∇2f(x))−1∇f(x), it holds that
−1+µ∥x∥ 1+µ∥x∥
2 2
√ ≤ λ(x) ≤ √ .
1+µ 2 µ
Proof of Lemma B.8. For |f(x)|, note that for each i ∈ [n], we have −y x⊤a ≤ ∥x∥ because
i i 2
y ∈ {−1,1} and ∥a ∥ ≤ 1. This implies the first bound on |f(x)|.
i i 2
Next, recall the gradient and Hessian of f:
n
1 (cid:88) 1
∇f(x) = − y p a +µx, ∇2f(x) = A⊤DA+µI ,
i i i d
n n
i=1
where each p := exp(−y x⊤a )/(1+exp(−y x⊤a )) and D := diag(p (1−p ),...,p (1−p )). By
i i i i i 1 1 n n
triangle inequality, we have
n
1 (cid:88)
∥∇f(x)∥ ≤ |y p |∥a ∥ +µ∥x∥ ≤ 1+µ∥x∥
2 i i i 2 2 2
n
i=1
where the last inequality follows from Assumption 3.2. Similarly, for the Hessian, we have
1 1
∥∇2f(x)∥ ≤ ∥A⊤DA∥ +µ ≤ ∥A∥2 ∥D∥ +µ ≤ 1+µ.
op n op n op op
Finally, the lower bound on ∥∇2f(x)∥ follows from the fact that 1A⊤DA is positive definite. This
op n
completes the proof. ■
Lemma B.9. Under Assumption 3.2, let f be the regularized logistic loss defined in Equation (3.5)
with regularization parameter µ > 0. Then there exists some constant C > 0 depending on µ such
that for any ε ∈ Rd with ∥ε∥ ≤ µ2, if ∥x∥ ≥ C, then
2 2
√
(cid:13) (cid:13)
2 µ
(cid:13)
(cid:13)x− √
∇2f(x)−1∇f(x)+ε(cid:13)
(cid:13) ≤ ∥x∥ 2.
(cid:13) 2 µ+λ(x) (cid:13)
2
25Proof of Lemma B.9. For simplicity, denote
√ √
2 µ 2 µ
x′ = x− √ ∇2f(x)−1∇f(x)+ε, η(x) = √ .
2 µ+λ(x) 2 µ+λ(x)
It follows that
∥x′∥2 = ∥x∥2−2η(x)x⊤∇2f(x)−1∇f(x)+η(x)2∥∇2f(x)−1∇f(x)∥2
2 2 2
+2ε⊤(x−η(x)∇2f(x)−1∇f(x))+∥ε∥2
2
≤ ∥x∥2−2η(x)x⊤∇2f(x)−1∇f(x)+η(x)2∥∇2f(x)−1∇f(x)∥2
2 2
+2µ2∥x∥ +2µη(x)∥∇2f(x)−1∇f(x)∥+µ4 (B.4)
2
where we used the fact that ∥ε∥ ≤ µ2 and triangle inequality. Plugging in the expression for ∇f(x),
2
we have
n
1 (cid:88)
x⊤∇2f(x)−1∇f(x) = − y p x⊤∇2f(x)−1a⊤+µx⊤∇2f(x)−1x
n i i i
i=1
≥ −∥∇2f(x)−1x∥ +µx⊤∇2f(x)−1x
2
1
≥ − ∥x∥ +µ2∥x∥2 (B.5)
µ 2 2
where the first inequality follows from Assumption 3.2 and triangle inequality, and the second
inequality is due to Lemma B.8. Similarly, we also have
1 1
∥∇2f(x)−1∇f(x)∥ ≤ ∥∇f(x)∥ ≤ +∥x∥ .
2 2 2
µ µ
Combining this with Equations (B.4) and (B.5), we obtain (after rearrangement of the terms)
2
(cid:18)
1
(cid:19)2
∥x′∥2 ≤ ∥x∥2+ η(x)∥x∥ −2µ2η(x)∥x∥2+η(x)2 +∥x∥
2 2 µ 2 2 µ 2
(cid:18) (cid:19)
1
+2µ2∥x∥ +2µη(x) +∥x∥ +µ4
2 2
µ
= ∥x∥2−2µ2η(x)∥x∥2+2µ2∥x∥ +µ4
2 2 2
2
(cid:18)
1
(cid:19)2 (cid:18)
1
(cid:19)
+ η(x)∥x∥ +η(x)2 +∥x∥ +2µη(x) +∥x∥
2 2 2
µ µ µ
Further applying the bounds on λ(x) from Lemma B.8, with direct computation, we have
8µ3
∥x′∥2 ≤ ∥x∥2− ∥x∥2+2µ2∥x∥ +µ4
2 2 4µ+1+µ∥x∥ 2 2
2
(cid:124) (cid:123)(cid:122) (cid:125)
I1
(cid:112)
4 µ(1+µ)∥x∥ 4µ(1+µ)(1/µ+∥x∥ )2 8µ(1/µ+∥x∥ )
2 2 2
+ + + .
(cid:112) (cid:112)
2 µ3(1+µ)−µ+µ2∥x∥ 2 (2 µ(1+µ)−1+µ∥x∥ 2)2 4µ+1+µ∥x∥ 2
(cid:124) (cid:123)(cid:122) (cid:125)
I2
Observe that for sufficiently large ∥x∥ , we have I ≈ −6µ2∥x∥ and I = O(1). This implies that
2 1 2 2
∥x′∥ ≤ ∥x∥ for sufficiently large ∥x∥ , and thus completes the proof. ■
2 2 2
26Lemma B.10. Under Assumption 3.2, let f be the regularized logistic loss defined in Equation (3.5)
with regularization parameter µ > 0. Consider a sequence of iterates {x } satisfying
t t≥0
√
2 µ
x = x − √ ∇2f(x )−1∇f(x )+ε
t+1 t t t t
2 µ+λ(x )
t
where ∥ε ∥ ≤ µ2 for all t ≥ 0. Then there exists a constant C depending on µ such that ∥x ∥ ≤ C
t 2 t 2
for all t ≥ 0.
Proof of Lemma B.10. First, there exists a constant C (given by Lemma B.9) such that if ∥x ∥ ≥
1 t 2
C , then ∥x ∥ ≤ ∥x ∥ . Then we define
1 t+1 2 t 2
√
(cid:13) (cid:13)
2 µ
C 2 = max
(cid:13)
(cid:13)x− √
∇2f(x)−1∇f(x)+ε(cid:13)
(cid:13) .
x:∥x∥2≤C1(cid:13) 2 µ+λ(x) (cid:13)
2
ε:∥ε∥2≤µ2
Note that by Lemma B.8, C is a constant depending only on C and the regularization parameter
2 1
µ. Finally, we choose C = max{∥x ∥ ,C ,C }, and the result follows. ■
0 2 1 2
Our target is to prove convergence of the inexact damped Newton’s method up to some error
threshold, which depends on the bound of the error terms {ε }. At a high level, the proof strategy
t
is as follows:
• Step 1: Show by induction that if x is feasible, then for sufficiently small error ε , so is the next
t t
iteration. This leads to some constraint on the error term ε .
t
• Step 2: Show constant decrease of the loss function when λ(xˆ ) ≥ 1/6.
t
• Step 3: Show that when λ(x ) < 1/6, we have λ(x ) ≤ cλ2+ϵ′ for some constant c > 0, so we
t t+1 t
enter the regime of quadratic convergence, which is maintained up to error ε.
Theorem 5.4. Under Assumption 3.2, for the regularized logistic loss f defined in (3.5) with
regularization parameter µ > 0, consider a sequence of iterates {x } satisfying
t t≥0
√
2 µ
x = x − √ ∇2f(x )−1∇f(x )+ε
t+1 t t t t
2 µ+λ(x )
t
where ∥ε ∥ ≤ ϵ for all t ≥ 0. Then there exists constants c,C ,C depending only on µ such that
t 2 1 2
for any ϵ ≤ c, it holds that g(x )−g(x∗) ≤ ϵ for all t ≥ C +C loglog 1.
t 1 2 ϵ
Remark B.11. In more detail the error obtained is α+ε, where α ≥ (cid:112) ϵ(1+µ)/(4µ), given that the
1 1
algorithm performs T = c+loglog +loglog , where c denotes the initial steps of constant
1/2+3α ε
√
decrease of the function g. By picking ε = ϵ we get the desired result.
Proof. First by Lemma B.10, we know that there exists a constant C such that ∥x ∥ ≤ C for all
t 2
t ≥ 0. We first derive an upper bound for δ := ∥∆ ∥ . By definition, we have
t t ∇2g(xt)
δ2 = (−η(x )∇2g(x )−1∇g(x )+ε )⊤∇2g(x )(−η(x )∇2g(x )−1∇g(x )+ε )
t t t t t t t t t t
= η(x )2∇g(x )⊤∇2g(x )−1∇g(x )−2η(x )ε⊤∇g(x )+ε⊤∇2g(x )−1ε
t t t t t t t t t t
λ(x )2 2ε⊤∇g(x )
= t − t t +ε⊤∇2g(x )−1ε .
(1+λ(x ))2 1+λ(x ) t t t
t t
27It follows from Lemma B.8 that for all t ≥ 0, |ε⊤∇g(x )| ≤ ϵ(1+µ∥x ∥ )/(4µ) ≤ ϵ(1+Cµ)/(4µ),
t t t 2
and also |ε⊤∇2g(x )−1ε | ≤ ϵ2/4. Therefore, we have
t t t
(cid:115)
λ(x )2 ϵ(1+Cµ) ϵ2
t
δ ≤ + +
t (1+λ(x ))2 2µ 4
t
(cid:115)
λ(x ) ϵ(1+Cµ) ϵ2
≤ t + + . (B.6)
1+λ(x ) 2µ 4
t
√ √
Note that by Lemma B.8, we have λ(x ) ≤ (1+µ∥x ∥ )/ µ ≤ (1+Cµ)/(2 µ). Then there exists
t t 2
some constant c depending only on µ such that δ < 1 when ϵ ≤ c .
1 t 1
Now, we proceed to show that there is a constant decrease of the loss value up to the point that
λ(x) ≤ 1/6, after which we enter the regime of quadratic convergence.
Phase I: Constant decrease of the loss function. Suppose λ(x ) ≥ 1/6 (note that if λ(x ) <
t 0
1/6, then we can directly proceed to the next phase). Since g is standard self-concordant, it follows
from Theorem B.3 that
g(x )−g(x ) ≤ ∇g(x )⊤(x −x )+ω (∥x −x ∥ )
t+1 t t t+1 t ∗ t+1 t ∇2g(xt)
= −η(x )∇g(x )⊤∇2g(x )−1∇g(x )+ε⊤∇g(x )+ω (∥x −x ∥ )
t t t t t t ∗ t+1 t ∇2g(xt)
λ(x )2
= − t +ε⊤∇g(x )+ω (∥x −x ∥ )
1+λ(x ) t t ∗ t+1 t ∇2g(xt)
t
where we applied the definition of λ(x ) and η(x ) in the last equality. Combining the above two
t t
equations, we have
λ(x )2 (cid:18) λ(x )2 2ε⊤∇g(x ) (cid:19)
g(x )−g(x ) ≤ − t +ε⊤∇g(x )+ω t − t t +ε⊤∇2g(x )−1ε .
t+1 t 1+λ(x ) t t ∗ (1+λ(x ))2 1+λ(x ) t t t
t t t
Recall that w (x) = −x−log(1−x), and we view the right-hand side as a function of λ(x ) while
∗ t
regarding c ≡ ε⊤∇g(x ) and c′ ≡ ε⊤∇2g(x )−1ε as constants, yielding
t t t t t
x2 (cid:18) x2 2c (cid:19) (cid:18) x2 2c (cid:19)
h(x) := − − − +c′ −log 1− + −c′ +c.
1+x (1+x)2 1+x (1+x)2 1+x
Since |c| ≤ 0.06 by our assumption on ε , it can be verified that h(x) is decreasing in x for x ≥ 1/6,
t
and moreover h(1/6) ≤ 0.01 by our assumption on ε (see Appendix E.1). Therefore, we have
t
g(x )−g(x ) ≤ −0.01 for all t such that λ(x ) ≥ 1/6.
t+1 t t
√ √
Phase II: Quadratic convergence. Now suppose ϵ < λ(x ) < 1/6 (again, if λ(x ) < ϵ, then
t 0
we are done). Note that there exists some constant C such that t ≤ C due to the constant decrease
1 1
of the loss function in the previous phase. Then by Theorem B.7 and Lemma B.4, we have
3λ(x )2
g(x )−g(x∗) ≤ t (B.7)
t
5
where x∗ is the global minimizer of g. Thus it suffices to characterize the decrease of λ(x ).
t
Applying Theorem B.5, we get
1
λ(x ) = ∥∇2g(x )−1/2∇g(x )∥ ≤ ∥∇2g(x )−1/2∇g(x )∥ . (B.8)
t+1 t+1 t+1 2 t t+1 2
1−∥∆ ∥
t ∇2g(xt)
28Notethat∇g(x ) = ∇g(x )+(cid:82)1 ∇2g(x +s∆ )∆ ds. Also,wehave∇g(x ) = −(1+λ(x ))∇2g(x )∆ +
t+1 t 0 t t t t t t t
(1+λ(x ))∇2g(x )ε . Combining these two equations, we obtain
t t t
(cid:90) 1
∇g(x ) = (cid:0) ∇2g(x +s∆ )−(1+λ(x )∇2g(x ))(cid:1) ds·∆ +(1+λ(x ))∇2g(x )ε
t+1 t t t t t t t t
0
(cid:124) (cid:123)(cid:122) (cid:125)
=:Gt
where we introduced the notation G for the integral term. Therefore, by triangle inequality,
t
∥∇2g(x t)−1/2∇g(x t+1)∥ 2 ≤ (cid:13) (cid:13)∇2g(x t)−1/2G t∆ t(cid:13) (cid:13) 2+(1+λ(x t))(cid:13) (cid:13)∇2g(x t)1/2ε t(cid:13) (cid:13) 2
= (cid:13) (cid:13)η(x t)∇2g(x t)−1/2G t∇2g(x t)−1∇g(x t)(cid:13) (cid:13)
2
+(cid:13) (cid:13)∇2g(x t)−1/2G tε t(cid:13) (cid:13) 2+(1+λ(x t))(cid:13) (cid:13)∇2g(x t)1/2ε t(cid:13) (cid:13)
2
(B.9)
By Corollary B.6, it holds that
(cid:18) δ2 (cid:19) (cid:18) 1 (cid:19)
−δ + t −λ(x ) ∇2g(x ) ⪯ G ⪯ −1−λ(x ) ∇2g(x ).
t t t t t t
3 1−δ
t
By direct computation, it can be verified that 0 ≤ δ /(1−δ )−λ(x ) ≤ δ +λ(x ). This implies that
t t t t t
∥∇2g(x )−1/2G ∥ ≤ (δ +λ(x ))∥∇2g(x )−1/2∥ and ∥∇2g(x )−1/2G ∇2g(x )−1/2∥ ≤ δ +λ(x ).
t t op t t t op t t t op t t
Applying these bounds to Equation (B.9), we obtain
∥∇2g(x )−1/2∇g(x )∥ ≤ η(x )(δ +λ(x ))∥∇2g(x )−1/2∇g(x )∥
t t+1 2 t t t t t 2
+ϵ(δ +λ(x ))∥∇2g(x )−1/2∥ +ϵ(1+λ(x ))∥∇2g(x )1/2∥
t t t op t t op
(cid:18) (cid:114) (cid:19)
λ(x )(δ +λ(x )) 1 1+µ
t t t
≤ +ϵ(δ +λ(x )) +
t t
1+λ(x ) 2 4µ
t
where the last inequality follows from Lemma B.8. Plugging this into Equation (B.8), we obtain
(cid:18) (cid:114) (cid:19)
λ(x )(δ +λ(x )) ϵ(δ +λ(x )) 1 1+µ
t t t t t
λ(x ) ≤ + +
t+1
(1+λ(x ))(1−δ ) 1−δ 2 4µ
t t t
λ(x )2 δ λ(x ) ϵ(δ +λ(x ))(cid:18) 1 (cid:114) 1+µ(cid:19)
t t t t t
= + + + .
(1+λ(x ))(1−δ ) (1+λ(x ))(1−δ ) 1−δ 2 4µ
t t t t t
Note that there exists some constant c depending only on µ such that if ϵ < c , then when
2 2
λ(x ) < 1/6, we have δ ≤ 1/5 by Equation (B.6). Then, for some constant C′ > 0, we further have
t t
λ(x ) ≤ 3λ(x )2+C′ϵ.
t+1 t
Let α = (cid:112) C′ϵ/3 ≤ 1/6. We can rewrite the above inequality as
λ(x )−α ≤ 3λ(x )2+C′ϵ+α = 3(λ(x )−α)2.
t+1 t t
√
Telescoping this inequality, we obtain that for some constant C > 0, λ(x ) ≤ C′′ ϵ when t ≥
2 t
C +C loglog 1. Combiningthiswith(B.7),weseethatforanysucht,wehaveg(x )−g(x∗) ≤ 3C′′ ϵ.
1 2 ϵ t 5
Finally, choosing c = min{c ,c } completes the proof. ■
1 2
29C Transformers for logistic regression
Inthissection, wepresenttheexplicitconstructionofalinearTransformerthatcanemulateNewton’s
method on the regularized logistic loss, and we further provide its error analysis.
Recall the regularized logistic loss defined as
n
1 (cid:88) µ
f(x) = log(1+exp(−y x⊤a ))+ ∥x∥2,
n i i 2 2
i=1
and its gradient and Hessian
n
1 (cid:88) 1
∇f(x) = − y p a +µx, ∇2f(x) = A⊤DA+µI ⪰ µI
i i i d d
n n
i=1
where each p = exp(−y x⊤a )/(1+exp(−y x⊤a )) and D = diag(p (1−p ),...,p (1−p )).
i i i i i 1 1 n n
Letting g(x) ≡ f(x)/(4µ), Newton’s method on g updates as follows:
1
x = x − (∇2g(x ))−1∇g(x )
t+1 t t t
1+λ (x )
g t
√
2 µ
= x − √ (∇2f(x ))−1∇f(x )
t t t
2 µ+λ (x )
f t
√
2 µ
(cid:18)
1
(cid:19)−1(cid:18)
1
(cid:88)n (cid:19)
= x − √ A⊤DA+µI y p a +µx
t t i i i t
2 µ+λ (x ) n n
f t
i=1
(cid:112) (cid:112)
where λ (x) = ∇g(x)⊤(∇2g(x))−1∇g(x) and λ (x) = ∇f(x)⊤(∇2f(x))−1∇f(x). To emulate
g f
the above update, we need to approximate the following components:
1. The values of the diagonal entries of the matrix D, resulting an error vector u ∈ Rn.
1
2. The multiplication of DA, which incurs an error U ∈ Rn×d.
2
3. The inversion of the Hessian matrix, which incurs an error E ∈ Rd×d.
2
4. The values of p = (p ,...,p )⊤ ∈ Rn, which incurs an error u ∈ Rn.
1 n 3
√ √
5. The step-size 2 µ/(2 µ+λ (x )), which incurs an error ϵ . Notice that to do this, we need
f t 4
to first approximate the value of λ (x ).
f t
For simplicity, we drop the subscript f from λ , and we just write λ from now on. The resulting
f
update for one step admits the following form:
(cid:18) 2√
µ
(cid:19)(cid:34) (cid:18)
1
(cid:19)−1 (cid:35) (cid:18)
1
(cid:88)n (cid:19)
x = x − √ +ϵ A⊤DˆA+µI+E +E y (pˆ −u )a +µx
1 0 4 1 2 i i 3,i i 0
2 µ+λ(x ) n n
0
i=1
(C.1)
where E = A⊤diag(u )A+A⊤U .
1 1 2
30Input format. We consider the following input format:
[I 0]
d
[I 0]
 d 
 [I
d
0]

 
 0 
H 0 =   

A y⊤⊤   

∈ R(7d+5)×n. (C.2)
 
 x 01⊤ n

 1e⊤
 n 1 
 0 
1⊤
n
Here the first identity matrix will be used to store the updates for calculating the inverse, the second
one for the initialization, while the third one for required computation. The initialization x is copied
0
n times The second to last line is used to store the parameter η, the step-size. Below we will provide
explicit construction of Transformers that can realize the following map from input to output:
[I 0]
d
[I 0]
 d 
 [I
d
0]

 
 0 
H 0 (cid:55)→ H K =   

A y⊤⊤   

∈ R(7d+5)×n (C.3)
 
 x 11⊤ n

 1e⊤
 n 1 
 0 
1⊤
n
where K denotes the number of Transformer layers.
Theorem 5.2. Under the setting of Theorem 5.1, there exists a Transformer consisting of linear
attentionwithReLUlayersthatcanapproximatelyperformdampedNewton’smethodontheregularized
logistic loss as follows
√
2 µ
x = x − √ (∇2f(x ))−1∇f(x )+ε
t+1 t t t
2 µ+λ(x )
t
where ε is an error term. For any ϵ > 0, to achieve that ∥ε∥ ≤ ϵ, the width of such a Transformer can
2
(1+µ)8 (1+µ)3
be bounded by O(d ), and its depth can be bounded by 11+2k where k ≤ 2logκ +loglog .
ϵ4µ10 f ϵ2µ2
For clarity, we split the proof into two parts: the first part is to construct the approximate
updates of the Newton’s algorithm, and the second part is devoted to the error analysis.
Proof of Theorem 5.2: Construction of the approximate updates. For ease of presentation, in the
following proof we omit the explicit expressions of the weights and describe instead the operations
they induce. The constructions are straightforward to be determined, similar to the proofs in
Appendix A. Notice that each of the value, key and query weight matrices can always make any row
selection by zero-padding and ignoring the rest of the rows of the input matrix.
31Step 1 - Create d. We start by creating the diagonal entries of D. We choose W ,W ,W
V K Q
such that
 
0
W VH
0
= e⊤ 1, W KH
0
= x 01⊤ n, W QH
0
= A⊤, (C.4)
0
Note that here the value weight matrix W just picks the first row of the identity matrix, zeroes
V
out any other row, and performs a row permutation to place the result in the second to last line.
Then we have
[I 0]
d
[I 0]
 d 
 [I
d
0]

 
   0 
HA 1ttn = H 0+x⊤ 00 A⊤  =   

A y⊤⊤   

(C.5)
0  
 x 01⊤
n


e⊤/n
 1 
x⊤A⊤
0
1⊤
n
we then use the ReLU network to approximate the values d = D = p(x⊤a ,y )(1−p(x⊤a ,y ))
i ii 0 i i 0 i i
where p(x,y) = exp(−yx)/(1+exp(−yx)). We zero out all the rows except for the rows of y⊤,x⊤A⊤
0
and construct the weights of the ReLU network to approximate the function p(x,y)(1−p(x,y)).
Notice that this function takes values between [0,1] and it domain contains x ∈ R and y ∈ {−1,1}
in this case. Also note that it is symmetric with respect to the value y. Therefore, it suffices to
approximate the function f(x) = ex/(1+ex)2, which is increasing for x ≤ 0 and decreasing for
x ≥ 0. We approximate it by splitting [0,1] into 1/ϵ intervals and deal with each of these intervals
seperately for x ≥ 0 and x ≤ 0 using a total number of 4/ϵ ReLU neurons. This gives rise to
 [I 0]
d
[I 0]
 d 
[I 0]
 d 
 0 
 
 A⊤ 
H =  
1  y⊤ 
 
x 1⊤
 0 n
e⊤/n
 1 
 dˆ⊤ 
 
1⊤
n
where dˆ = d+u . Notice that
1
4
∥u ∥ ≤ (C.6)
1
N
where N is the width of the ReLU layer.
1
Step 2 - Approximate D⊙A⊤. In the attention of the second layer, we compute dˆ/n by
n
setting
 
0
1
W V(1) H
1
= e⊤ 1, W K(1) H
1
= ne⊤ 1, W Q(1) H
1
= dˆ, (C.7)
0
32and we use one more head to subtract the residual by letting
 
0
W V(2) H
1
= −e⊤ 1, W K(2) H
1
= e⊤ 1, W Q(2) H
1
= dˆ. (C.8)
0
Thus,
 [I 0]
d
[I 0]
 d 
[I 0]
 d 
 0 
 0   0  
 A⊤ 
HA 2ttn = H 1+dˆ/n−dˆ  =   y⊤  .
0 0  
x 1⊤
 0 n
e⊤/n
 1 
 dˆ/n
 
1⊤
n
The next ReLU layer approximates the multiplication of the diagonal matrix 1Dˆ with the matrix
n
A, which is the same as creating the vectors dˆ a /n,dˆ a /n,...dˆ a /n, or equivalently, the matrix
1 1 2 2 n n
1Dˆ ⊙A⊤. This can be implemented with one ReLU layer since the elements will be processed
n
serially and the information needed is on the same column. This approximation incurs an error
matrix U . This yields the output of the second Transformer layer:
2
 
1 dˆ⊤⊙A⊤/n+U
d 2
 [I 0] 
 d 
 [I 0] 
 d 
 0 
 
 A⊤ 
H =  . (C.9)
2  y⊤ 
 
 x 1⊤ 
 0 n 
 e⊤/n 
 1 
 0⊤ 
 n 
1⊤
n
Notice that we can easily subtract the identity matrix since we have another copy of it. Using
Proposition A.1 in Bai et al. (2023), we have that s = 2 and each dˆ ∈ [−0.1,1.1] thus it requires a
i
total number of O(1/(n2ϵ2)) ReLUs to achieve ϵ accuracy for each entry. Moreover,
dˆ⊤⊙A⊤ d⊤⊙A⊤ u⊤⊙A⊤
1 +U = 1 +1 1 +U ,
d 2 d d 2
n n n
and thus
√
√ (cid:18) d (cid:19)
∥U 2∥
2
≤ ∥U 2∥
F
≤ O(cid:101)( dnϵ) = O(cid:101) √ (C.10)
nN
where N is the width of the ReLU layer and we omit logarithmic factors in O(cid:101)(·).
33Step 3 - Create the matrix 1A⊤DA+µI. For this step, we first use the attention layer to
n
implement
 αA⊤
1
W V(1) H 2 =  A⊤ , W K(1) H 2 = n1⊤ ddˆ ⊙A⊤+U 2, W Q(1) H 2 = [I d 0].
0
Notice that (W(1) H )⊤ = 1DˆA+U⊤. We also use two extra heads such that
K 2 n 2
 αµ[I 0] 
d
W V(2) H
2
= (µ−1)[I
d
0], W K(2) H
2
= [I
d
0], W Q(2) H
2
= [I
d
0],
0
(cid:18) [I 0](cid:19)
W(3) H = d , W(3) H = [I 0], W(3) H = −1 dˆ⊤⊙A⊤/n−U .
V 2 0 K 2 d Q 2 d 2
Combining these three heads, we get
 [α( n1A⊤DˆA+A⊤U⊤ 2) 0]  [αµI d 0]  (cid:18) −1 ddˆ⊤⊙A⊤−U (cid:19)
H
3
= H 2+ [ n1A⊤DˆA+A⊤U⊤
2
0] +[(µ−1)I
d
0]+ n
0
2
0 0
 [α(1A⊤DˆA+A⊤U⊤+µI );0]
n 2 d
 [1A⊤DˆA+A⊤U⊤+µI ;0] 
 n 2 d 
 [I 0] 
 d 
 0 
 
 A⊤ 
=  .
 y⊤ 
 
 x 1⊤ 
 0 n 
  e⊤ 1/n  


0⊤
n


1⊤
n
For simplicity, we denote B := 1A⊤DˆA+A⊤U⊤+µI . We then use one more attention layer
n 2 d
with two head as follows: For the first head,
(cid:18) [I 0](cid:19)
W(1) H = d , W(1) H = αB, W(1) H = [I 0]. (C.11)
V 3 0 K 3 Q 3 d
The second head is used to subtract the residual, so that the output of this layer is
[αB⊤;0]
[B;0]
 
  [I d 0]  
 
 0 


A⊤ 

H 4 = 

y⊤  .
 


x 01⊤
n


 e⊤/n 
 1 
 0⊤ 
n
1⊤
n
34We keep track of the errors in the following way:
1 1 1
B = A⊤DA+µI+ A⊤diag(u )A+A⊤U = A⊤DA+µI+E (C.12)
1 2 1
n n n
where E can be controlled as
1
1
∥E ∥ ≤ ∥diag(u )∥ ∥A∥2+∥A∥ ∥U ∥
1 2 n 1 2 2 2 2 2
1 (C.13)
≤ ∥u ∥ ∥A∥2 +∥A∥ ∥U ∥
n 1 2 F F 2 2
√
≤ ∥u ∥ + n∥U ∥
1 2 2 2
since ∥a ∥2 ≤ 1 for all i = 1,...,n by Assumption 3.2.
i
Step 4 - Invert the matrix. Next, we implement Newton’s iteration as in the previous section
for k steps and we get
[B−1+E 0]
2
[I 0]
 d 
  [I d 0]  
 
 0 


A⊤ 

H 2k+4 = 

y⊤ 

 


x 01⊤
n


 e⊤/n 
 1 
 0⊤ 
n
1⊤
n
Step 5 - Create the {p }n . We repeat Step 1 to get
i i=1
[B−1+E 0]
2
[I 0]
 d 
  [I d 0]  
 
 0 
HAttn =

 
A⊤ 
 . (C.14)
2k+5

y⊤

 


x 01⊤
n


 e⊤/n 
 1 
 x⊤A⊤ 
0
1⊤
n
35We then use the ReLU layer to approximate {p }n and we have
i i=1
[B−1+E 0]
2
[I 0]
 d 
  [I d 0]  
 
 0 
H 2k+5 =   

A y⊤⊤    . (C.15)
 


x 01⊤
n


 e⊤/n 
 1 
 p⊤+u⊤ 
3
1⊤
n
The function approximation here is the same as the one in Step 1, and it requires 2/ϵ ReLUs to
achieve ϵ accuracy. Then
2
∥u ∥ ≤ (C.16)
3
N
where N is the width of the ReLU layer.
Step 6 - Calculate the values {y p /n}n . In the attention layer, we multiply each p with 1/n
i i i=1 i
as we did in Step 2, and we have
[B−1+E 0]
2
[I 0]
 d 
  [I d 0]  
 
 0 
HAttn =

 
A⊤ 
 .
2k+6

y⊤

 


x 01⊤
n


 e⊤/n 
 1 
(p⊤+u⊤)/n
3
1⊤
n
We then use the ReLU layer to approximate the inner product between 1p and y. To do so, notice
n
that each p ∈ (0,1), and y = {−1,1} and consider the following sets of ReLUs:
i
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
1 1 1 1
o = σ x+2y −σ − x+2y , o = σ − x−2y −σ + x−2y
1 2
2 2 2 2
Suppose x ∈ (0,1), then if y = 1, the outputs are o = x, o = 0, and for y = −1, the outputs are
1 2
o = 0 and o = −x, thus o +o = xy. Consequently,
1 2 1 2
 [B−1+E 0] 
2
[I 0]
 d 
  [I d 0]  
 
 0 


A⊤ 

H 2k+6 = 

y⊤ 

 


x 01⊤
n


 e⊤/n 
 1 
(p⊤+u⊤)⊙y⊤/n
3
1⊤
n
36Step 7 - Calculate the gradient. Next, we want to calculate the quantity −1 (cid:80)n y p a +µx .
n i=1 i i i 0
We first set the weight matrices of the attention layer such that
 
0
1
W V(1) H
2k+6
= −A⊤ , W K(1) H
2k+6
= n(p⊤+u⊤ 4)⊙y⊤, W Q(1) H
2k+6
= e⊤
1
0
 
0
W V(2) H
2k+6
= −[I
d
0], W K(2) H
2k+6
= [I
d
0], W Q(2) H
2k+6
= [I
d
0],
0
and we need a third head to add µx by setting
0
 
0
W V(3) H
2k+6
= x 01⊤ n, W K(3) H
2k+6
= e 1, W Q(3) H
2k+6
= µe 1, (C.17)
0
Thus, we place the result in the second block of the matrix and we have
 [B−1+E 0] 
2
[b 0]
 
  [I d 0]  
 
 0 
H 2k+7 =   

A y⊤⊤   

(C.18)
 


x 01⊤
n


 e⊤/n 
 1 
(p⊤+u⊤)⊙y⊤/n
3
1⊤
n
where E is the error incurred by running Newton’s method for the inversion of the matrix and
2
b = −1 (cid:80)n (y p +y u )a +µx = −1 (cid:80)n y p a +µx +ϵ , ϵ = −1 (cid:80)n y u a . Thus,
n i=1 i i i 3i i 0 n i=1 i i i 0 3 3 n i=1 i 3i i
2
∥ϵ ∥ ≤ (C.19)
3 2 N
where is the width of the ReLU layer used to approximate the error u (Equation (C.16)).
3
Step8-Calculatethestepsize. Weproceedtoapproximateλˆ(x)2 = ∇⊤f(x)(∇2f(x))−1∇f(x) =
b⊤(B−1+E )b. We first create the quantity (B−1+E )b which we also need for the update, store
2 2
it, and then calculate in the next layer the parameter λ(x)2. For the first layer we set
(cid:18) [B−1+E 0](cid:19)
W(1) H = 2 , W(1) H = [I 0], W(1) H = [b 0],
V 2k+7 0 K 2k+7 d Q 2k+7
(cid:18) [B−1+U 0](cid:19)
W(2) H = − 3 , W(2) H = [I 0], W(2) H = [I 0].
V 2k+7 0 K 2k+7 d Q 2k+7 d
37Then we get
 [(B−1+E )b 0] 
2
[b 0]
 
  [I d 0]  
 
 0 


A⊤ 

H 2k+8 = 

y⊤  .
 


x 01⊤
n


 e⊤/n 
 1 
(p⊤+u⊤)⊙y⊤/n
3
1⊤
n
We use another Transformer layer to calculate the quantity λ(xˆ )2. We set the attention layer as
t
follows
 
0
W V(1) H
2k+8
= e⊤ 1, W K(1) H
2k+8
= [b 0], W Q(1) H
2k+8
= [(B−1+E 2)b 0]
0
where we place the e⊤ in the second to last position. We also use as before an extra head to remove
1
the residual. Then we get
 
[(B−1+E )b 0]
2
 [b 0] 
 
 [I 0] 
 d 
 0 
 
 A⊤ 
HAttn =  .
2k+9  y⊤ 
 
 x 1⊤ 
 0 n 
 e⊤/n 
 1 
 [λˆ(xˆ )2 0] 
 t 
1⊤
n
√
In the ReLU layer, we approximate the function 2 µ . Notice that this function take values in
√ √
2 µ+ x
√
(0,1]. The first derivative of the function is − √ 2 √µ √ < 0, so it is a monotonically decreasing
2(2 µ+ x)2 x
function. Thus, using the same argument with previous steps we can approximate it up to error ϵ
with 2/ϵ ReLUs. This yields an error ϵ
4
2
|ϵ | ≤ (C.20)
4
N
38√ √
where N is the width of the ReLU layer and we can write the stepsize as ηˆ(x ) = 2 µ/(2 µ+
0
λ(x ))+ϵ . Thus,
0 4
[(B−1+E )b 0]
2
[b 0]
 
  [I d 0]  
 
 0 


A⊤ 

H 2k+9 = 

y⊤ 

 


xˆ t1⊤
n


 e⊤/n 
 1 
 [ηˆ(x ) ∗] 
0
1⊤
n
where ∗ denotes inconsequential values.
Step 9 - Update For the last step, we use two more attention layers. We set the weights for the
first layer such that
 
0
W V(1) H
2k+9
= [ηˆ(x 0) ∗], W K(1) H
2k+9
= [(B−1+E 2)b 0], W Q(1) H
2k+9
= [I
d
0].
0
We also use an extra head to remove the residual by setting
 
0
W V(2) H
2k+9
= −[ηˆ(x 0) ∗], W K(2) H
2k+9
= [I
d
0], W Q(2) H
2k+9
= [I
d
0], (C.21)
0
where the values [ηˆ(x ) ∗] are placed in the second to last row. Combining these two heads, we get
0
   
0 0
H
2k+10
= H 2k+9+[ηˆ(x 0)b⊤(B−1+E 2)⊤ 0]−[ηˆ(x 0) ∗]
0 0
 [(B−1+E )b 0] 
2
[b 0]
 
  [I d 0]  
 
 0 


A⊤ 

=  

y⊤

 


x 01⊤
n


 e⊤/n 
 1 
[ηˆ(x )b⊤(B−1+E )⊤ ∗]
0 2
1⊤
n
where ∗ are again inconsequential values. Notice that when subtracting the residual with the second
head, we correct only the first d columns.
Finally, we perform the update with one more attention layer:
 
0
W V(1) H
2k+10
= −[I
d
0], W K(1) H
2k+10
= [ηˆ(x 0)b⊤(B−1+E 2)⊤ ∗], W Q(1) H
2k+10
= 1⊤ n.
0
39Another head is used to restore the matrix in its initial form:
 [−I 0 I 0]
d d
W V(2) H
2k+10
= [0;−I
d
I
d
0]H 2k+10, W K(2) H
2k+10
= [I
d
0], W Q(2) H
2k+10
= [I
d
0].
0
As a result, we get
 [I 0] 
d
[I 0]
 d 
  [I d 0]  
 
 0 
HAttn =

 
A⊤ 
 . (C.22)
2k+11

y⊤

 
 (xˆ t−ηˆ(x 0)(B−1+E 2)b)1⊤ n

 e⊤/n 
 1 
 [ηˆ(x )b⊤(B−1+E )⊤ ∗] 
0 2
1⊤
n
We further use the ReLUs to zero out the second to last row. Note that the inconsequential values
√
wer create when approximating the function 1/(1+ x), so they are close to one. Thus, we can use
the following simple ReLU to zero out this line:
σ(−x/2+5y)−σ(x/2+5y)
where we use as x the elements of the second to last row and y, the last row and all the other
connections are zero. The final output is
 [I 0]  [I 0]
d d
[I 0] [I 0]
 d   d 
  [I d 0]    [I d 0] 
   
 0   0 


A⊤ 



A⊤ 

H 2k+11 = 

y⊤ 

= 

y⊤  .
   
 (x 0−ηˆ(x 0)(B−1+E 2)b)1⊤ n

 x 11⊤ n

 e⊤/n  e⊤/n
 1   1 
 0⊤   0⊤ 
n n
1⊤ 1⊤
n n
Thus, for the next step the input is in the correct form and we can repeat the steps above for the
next iteration. ■
Next, we present the error analysis of the emulated iterates. The target is to show that the
updates described in Equation (C.1) can be recasted to the following updates:
xˆ = xˆ −η(xˆ )(∇2f(xˆ ))−1∇f(xˆ )+ε
t+1 t t t t t
where the error term ε is controlled to satisfy the considitions of Theorem 5.4.
t
Proof of Theorem 5.2: Error analysis. For x , let C be the constant given by Lemma B.10. From
0
the previous step of weight constructions, we have the following update
√
(cid:18) (cid:19)
2 µ
x 1 = x 0− 2√ µ+λˆ(x ) +ϵ 4 [(∇2f(x 0)+E 1)−1+E 2](∇f(x 0)+ϵ 3) (C.23)
0
40where
1
E = A⊤diag(u )A+A⊤U
1 1 2
n
E = Error of Newton’s method for inversion
2
n
1 (cid:88)
ϵ = − y u a
3 i 4i i
n
i=1
ϵ = Approximation error for the quantity λ.
4
where the error terms admit the following bounds
√
(cid:18) (cid:19)
4 d 2 2
∥u 1∥ ≤ N, ∥U 2∥
F
≤ O(cid:101) √
nN
, ∥E 2∥ ≤ ϵ 2, ∥ϵ 3∥ ≤ N, |ϵ 4| ≤
N
(C.24)
where N is the width. These bounds were derived in Equations (C.6), (C.10), (C.16) and (C.20)
for u ,U ϵ and ϵ respectively. The error term ϵ is controlled by the number of layers, for
1 2 3 4 2
k = c+2loglog(1/ϵ) layers we achieve error less than ϵ.
Applying Corollary E.2, we have (∇2f(x ) + E )−1 = ∇2f(x )−1 + E′ where E′ satisfies
0 1 0 1 1
∥E′∥ ≤ ∥E ∥ /(µ(µ−∥E ∥ )). Further writing E′ := E′ +E , then by (C.24), it holds that
1 2 1 2 1 2 2 1 2
(cid:13) (cid:13)E′ 2(cid:13) (cid:13)
2
≤ ∥E 2∥ 2+ µ(µ∥ −E ∥1∥ E2
∥
). (C.25)
1 2
Now we can rewrite (C.23) as
√
(cid:18) (cid:19)
2 µ
x 1 = x 0− 2√ µ+λˆ(x ) +ϵ 4 [(∇2f(x 0))−1+E′ 2](∇f(x 0)+ϵ 3)
0
Next we analyze the error involved in the quantity λˆ2(xˆ ). Recall that
t
λˆ(x )2 = b⊤Bb = (∇f(x )+ϵ )⊤[(∇2f(x ))−1+E′](∇f(x )+ϵ )
0 0 3 0 2 0 3
= ∇f(x )⊤(∇2f(x ))−1∇f(x )
0 0 0
(cid:124) (cid:123)(cid:122) (cid:125)
λ(x0)2
+2ϵ⊤(∇2f(x ))−1(∇f(x )+ϵ )+(∇f(x )+ϵ )⊤E′(∇f(x )+ϵ )
3 0 0 3 0 3 2 0 3
(cid:124) (cid:123)(cid:122) (cid:125)
=:ϵ′
4
By triangle inequality and the bounds from Lemma B.8, we can bound ϵ′ as follows:
4
(cid:12) (cid:12)ϵ′ 4(cid:12) (cid:12) < 2(1+µC)∥ µϵ 3∥+∥ϵ 3∥2 +∥ϵ 3∥2(cid:13) (cid:13)E′ 2(cid:13) (cid:13) 2+2(1+µC)∥ϵ 3∥(cid:13) (cid:13)E′ 2(cid:13) (cid:13) 2+(1+µC)2(cid:13) (cid:13)E′ 2(cid:13) (cid:13)
2
(C.26)
Now, as long as it holds that
(cid:13) (cid:13)E′ 2(cid:13) (cid:13)
2
= O(cid:18) (1+µϵ C2 4 µ)2(cid:19) and ∥ϵ 3∥ = O(cid:18) (1ϵ +2 4µ C2 µ)(cid:19) (C.27)
we would have
2(1+Cµ)∥ µϵ 3∥+∥ϵ 3∥2 +∥ϵ 3∥2(cid:13) (cid:13)E′ 2(cid:13) (cid:13) 2+2(1+Cµ)∥ϵ 3∥(cid:13) (cid:13)E′ 2(cid:13) (cid:13) 2+(1+Cµ)2(cid:13) (cid:13)E′ 2(cid:13) (cid:13)
2
< 14 −µϵ ϵ2 4 . (C.28)
4
41Then by Appendix E.2 we have that if the condition above holds
(cid:12) √ √ (cid:12)
(cid:12) 2 µ 2 µ (cid:12)
(cid:12) (cid:12) (cid:12)2√ µ+λˆ(x 0) − 2√ µ+λ(x 0)(cid:12) (cid:12) (cid:12) ≤ ϵ 4
This implies that
√ √
2 µ 2 µ
2√ µ+λˆ(x 0) = 2√ µ+λ(x 0) +ϵ˜ 4
for some ϵ˜ , such that |ϵ˜ | ≤ ϵ , since we also account for the approximation error from the ReLUs.
4 4 4
Then it follows that
√
(cid:18) (cid:19)
2 µ
x = x − √ +ϵ˜ [(∇2f(x ))−1+E′](∇f(x )+ϵ )
1 0 2 µ+λ(x ) 4 0 2 0 3
0
√
2 µ
= x − √ (∇2f(x ))−1∇f(x )−ϵ˜ [(∇2f(x ))−1+E′](∇f(x )+ϵ )
0 2 µ+λ(x ) 0 0 4 0 2 0 3
0
√
2 µ
− √ E′(∇f(x )+ϵ )
2 µ+λ(x ) 2 0 3
0
√
2 µ
= x − √ (∇2f(x ))−1∇f(x )+ε
0 0 0
2 µ+λ(x )
0
where the error term ε satisfies
(cid:18) (cid:19)
∥ε∥ 2 ≤ |ϵ˜ 4| µ1 +(cid:13) (cid:13)E′ 2(cid:13) (cid:13) 2 (1+Cµ+∥ϵ 3∥ 2)+(cid:13) (cid:13)E′ 2(cid:13) (cid:13) 2(1+Cµ+∥ϵ 3∥). (C.29)
Using Equation (C.27) we have that
(cid:18) (cid:19)
∥ε∥ 2 ≤ |ϵ˜ 4| µ1 +(cid:13) (cid:13)E′ 2(cid:13) (cid:13) 2 (1+Cµ+∥ϵ 3∥ 2)+(cid:13) (cid:13)E′ 2(cid:13) (cid:13) 2(1+Cµ+∥ϵ 3∥)
≤ 2 µϵ 4 (1+Cµ+∥ϵ 3∥ 2)+(1+ϵ 4)(cid:13) (cid:13)E′ 2(cid:13) (cid:13) 2(1+Cµ+∥ϵ 3∥ 2)
Notice now if
(cid:13) (cid:13)E′ 2(cid:13) (cid:13)
2
=
O(cid:18) (1+ϵ2µ C3 µ)4(cid:19)
and ∥ϵ 3∥
2
=
O(cid:18) (1+ϵ2µ C4 µ)3(cid:19)
, (C.30)
then
(cid:18) (cid:19) (cid:18) (cid:19)
µϵ µϵ
ϵ = O = O , (C.31)
4 1+Cµ+Cµ3 1+Cµ
and consequently, ∥ε∥ ≤ ϵ.
2
We will now study how we can bound the error term E′. Notice that from Equations (C.13)
2
and (C.25), the following conditions hold:
∥E 1∥
2
≤ ∥u 1∥ 2+√ n∥U 2∥ 2, (cid:13) (cid:13)E′ 2(cid:13) (cid:13)
2
≤ ∥E 2∥ 2+ µ(µ∥ −E ∥1∥ E2
∥
). (C.32)
1 2
Now, given that
(cid:18) ϵ2µ5 (cid:19) (cid:18) ϵ2µ3 (cid:19)
∥E ∥ = O , ∥E ∥ = O , (C.33)
1 2 (1+Cµ)4 2 2 (1+Cµ)4
42it is straightforward to verify that indeed E′ satisfies Equation (C.27). For the bound on E , it
2 1
suffices to have
(cid:18) ϵ2µ5 (cid:19) (cid:18) ϵ2µ5 (cid:19)
∥u ∥ = O , ∥U ∥ = O √ (C.34)
1 2 (1+Cµ)4 2 2 n(1+Cµ)4
It remains to determine the necessary width and depth for these error bounds to be achieved.
We combine the bounds from Equations (C.24), (C.31), (C.33) and (C.34) to get that
1 (1+Cµ)4
∥u ∥ ∼ ⇒ N ∼
1 N ϵ2µ5
√
d d(1+Cµ)8
∥U ∥ ∼ √ ⇒ N ∼
2 F nN ϵ4µ10
1 (1+Cµ)3
∥ϵ ∥ ∼ ⇒ N ∼
3 N ϵ2µ4
1 1+Cµ
|ϵ | ∼ ⇒ N ∼
4
N ϵµ
Finally, the error term ∥E ∥ is controlled by the depth of the network and scales as k ∼
2 2
(1+Cµ)3 1
2logκ(1A⊤DˆA+µI+E )+loglog , where the condition number of A⊤DˆA+µI+E
n 1 ϵµ2 n 1
should be close to the condition number max κ(∇2f(x)) ( For a formal proof of this statement see
x
Appendix E.4). This completes the proof.
■
C.1 Main Result
We are now able to state our main result with explicit bounds.
Theorem 5.1. For any dimension d, consider the regularized logistic loss defined in (3.5) with
regularization parameter µ > 0, and define κ = max κ(∇2f(x)). Then for any T > 0 and
f x∈Rd
ϵ > 0, there exists a linear Transformer that can approximate T iterations of Newton’s method on
the regularized logistic loss up to error ϵ per iteration. In particular, the width of such a linear
Transformer can be bounded by O(d(1+µ)6/ϵ4µ8), and its depth can be bounded by T(11+2k), where
(1+µ)3
k ≤ 2logκ +loglog . Furthermore, there is a constant c > 0 depending on µ such that if ϵ < c,
f ϵ2µ2
(cid:112)
then the output of the Transformer provides a w˜ satisfying that ∥w˜ −wˆ∥ ≤ O( ϵ(1+µ)/(4µ)),
2
where wˆ is the global minimizer of the loss.
Remark C.1. Both T and k are actually have just one extra additive constant term. In the first case,
depends on the number of steps needed for λ(x) to become less than 1/6, while for k the constant
term is 2(cid:6) logκ(∇2f(x))(cid:7).
Proof. The proof follows by combining Theorem 5.4 and Theorem 5.2. ■
D Experiments
During training, we utilize the Adam optimizer, with a carefully planned learning rate schedule.
The task is learned through curriculum learning Garg et al. (2022), where we begin training with
a small problem dimension and gradually increase the difficulty of the tasks. When implementing
43Num Cond=5, noise=0.0 Num Cond=5, noise=0.1 Num Cond=10, noise=0.0 Num Cond=10, noise=0.1
101 101 101
100
101 100 100
103
101
102
101
105 LSA w/ LN LSA w/ LN LSA w/ LN LSA w/ LN
Newton(order=2) 102 Newton(order=2) 104 Newton(order=2) 102 Newton(order=2)
107 Newton(order=3) Newton(order=3) Newton(order=3) Newton(order=3)
Newton(order=4) 103 Newton(order=4) Newton(order=4) 103 Newton(order=4)
1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6
layers / steps layers / steps layers / steps layers / steps
Figure 8: Loss of LSA and different order Newton iteration for linear regression with different input
conditions.
Num Cond=5, noise=0.0 Num Cond=10, noise=0.0 Num Cond=20, noise=0.0 Num Cond=50, noise=0.0 Num Cond=100, noise=0.0
101 101 100 101
102 102 103 102 100
105 105 106 105 102
11 00 18
1
L N
N
NS e
e
eA w
w
w
t
t
tw o
o
o/ n
n
n
L (
(
(o
o
oN r
r
rd
d
de
e
er
r
r=
=
=4
6
8)
)
)
11 00 18
1
L N
N
NS e
e
eA w
w
w
t
t
tw o
o
o/ n
n
n
L (
(
(o
o
oN r
r
rd
d
de
e
er
r
r=
=
=4
6
8)
)
)
11 00 19
2
L N
N
NS e
e
eA w
w
w
t
t
tw o
o
o/ n
n
n
L (
(
(o
o
oN r
r
rd
d
de
e
er
r
r=
=
=4
6
8)
)
)
11 00 18
1
L N
N
NS e
e
eA w
w
w
t
t
tw o
o
o/ n
n
n
L (
(
(o
o
oN r
r
rd
d
de
e
er
r
r=
=
=4
6
8)
)
)
104 L N
N
NS e
e
eA w
w
w
t
t
tw o
o
o/ n
n
n
L (
(
(o
o
oN r
r
rd
d
de
e
er
r
r=
=
=4
6
8)
)
)
1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6
layers / steps layers / steps layers / steps layers / steps layers / steps
11 00 01 Num Cond=5, L
N
Nn S
e
eo A
w
wi s
t
tw
o
oe /
n
n= L
( (o
oN0
r
r.
d
d1
e er r= =4 6) )
11 00 01 Num Cond=10, L
N N
Sn
e
eA
w
wo i
t
tws
o
oe /
n n
L=
( (o
oN0
r rd
d.1
e er r= =4 6) ) 100
Num Cond=20, L
N N
Sn
e
eA
w
wo i
t
tws
o
oe /
n n
L=
( (o
oN0
r rd
d.1
e er r= =4 6) )
100 Num Cond=50, L
N N
Sn
e
eA
w
wo i
t
tws
o
oe /
n n
L=
( (o
oN0
r rd
d.1
e er r= =4 6) )
11 00 01 Num Cond=100 L
N
N, S
e
en A
w
wo
t
twi
o
os /
n n
e L
(
(=
o
oN
r
r0
d
d.
e
e1
r r= =4 6) )
Newton(order=8) Newton(order=8) Newton(order=8) 101 Newton(order=8) Newton(order=8)
101 101 101 101
102 102 102 102 102
103 103 103 103 103
1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6
layers / steps layers / steps layers / steps layers / steps layers / steps
Figure 9: Loss of LSA w/ layernorm and different order Newton iteration for linear regression with different
input conditions.
the Transformer backbone, we adhere to the structure of NanoGPT23, modifying only the causal
attention to full attention.
For both the task of linear and logistic regression, we sample the data points as follows: We
sample a random matrix A and create its SVD decomposition, i.e., A = USV⊤. We then sample
the maximum eigenvalue λ uniformly random in the range of [1,100]. We then set the minimum
max
eigenvalue as λ = λ /κ, where κ is the condition number of the problem at hand and it is fixed.
min max
Finally, we sample the rest of the eigenvalues uniformly random between [λ ,λ ] and recreate
min max
the matrix S, with the new eigenvalues. We then create our covariance matrix as Σ = USU⊤. We
use Σ, to sample the data samples x from a multivariate Gaussian with mean 0 and covariance
i
matrix Σ.
In Figures 8 and 9, we present comprehensive results for the LSA and LSA with layernorm
models, trained on linear regression tasks across various condition numbers and noise levels. The
trained LSA and LSA with layernorm consistently find higher-order methods when attempting to
solve ill-conditioned linear regression problems.
E Auxiliary results
We collect here some auxiliary results that are used in the proofs of the main results.
3https://github.com/karpathy/nanoGPT/blob/master/model.py
44E.1 Auxiliary result for constant decrease of the logistic loss
Let
x2
h(x) = − +c−log(1−δ˜)−δ˜
1+x
where δ˜= (cid:112) x2/(1+x)2−2c/(1+x)+c′ and c,c′ are constants with respect to x. Mainly, we see
the RHS of the previous inequality as a function of λ and consider E⊤∇g(xˆ ),E⊤∇2g(xˆ )E as
t t t t t
constants. Then we have that
x2+2x 1 dδ˜ dδ˜
h′(x) = − + −
(1+x)2 1−δ˜dx dx
x2+2x δ˜ dδ˜
= − +
(1+x)2 1−δ˜dx
x2+2x δ˜ x+c(1+x)
= − +
(1+x)2 1−δ˜ δ˜(1+x)3
(x2+2x)(1+x)(1−δ˜) x+c(1+x)
= − +
(1+x)3(1−δ˜) (1−δ˜)(1+x)3
x+c(1+x)−(x2+2x)(1+x)(1−δ˜)
=
(1−δ˜)(1+x)3
We use mathematica to plot this function (see Figure 10) and the max value it can attain, when
x ∈ [1/6,1] and |c| ≤ 0.06 and we have that the maximum value of g′ is approximately −0.02. This
ϵ(1+µ)
implies that h is decreasing, since we have that |c| ≤ ∥E ∥∥∇f(xˆ )∥ ≤ ≤ 0.06. Thus, we
t t
4µ
have
g(xˆ )−g(xˆ ) ≤ h(1/6)
t+1 t
1 (cid:112) (cid:112)
= − +y−log(1− 1/49−12y/7+z− 1/49−12y/7+z
42
where y = E⊤∇g(xˆ ) and z = E⊤∇2g(xˆ )E . Notice now that
t t t t t
ϵ2(1+µ)
|z| ≤ ≤ 0.012 and |y| ≤ 0.01
4µ
Since ϵ ≤ min{0.01,0.04µ/(1+µ)}. Given these bounds we have that
1 (cid:112) (cid:112)
g(xˆ )−g(xˆ ) ≤ − +y−log(1− 1/49−12y/7+0.012− 1/49−12y/7
t+1 t
42
We again use mathematica and plot this function for |y| ≤ 0.012, which can be viewed in Figure 10
and we see that we get a constant decrease of at least 0.01.
E.2 Auxiliary result for controlling the error
To control the change that this quantity can evoke, we note that we have approximated the function
√ √ √ √ √
g(x) = 2 µ/(2 µ+ x) by discretizing (0,1], so whenever x ≤ α2, g(x) ≥ 2 µ/(2 µ+α). Thus,
if x ≤ 4µϵ2/(1−ϵ )2 we have that g(x) ≥ 1−ϵ , similarly if x ≥ 4µ(1−ϵ )2/ϵ2 we have that
4 4 4 4 4
45Constantdecrease
-0.010 -0.005 -0.0112 0.005 0.010
-0.0114
-0.0116
-0.0118
-0.0120
Figure 10: Left: The derivative of h as a function of both x and c for x ∈ [1/6,1] and |c| ≤ 0.06.
Right: We see that the function is decreasing at least −0.01 at each step.
g(x) ≤ ϵ . Now notice that given x,x˜ such that max{x˜,x} ≥ 4µϵ2/(1−ϵ )2 (otherwise we have
4 4 4
already covered the case) we have
(cid:12) √ √ (cid:12)
(cid:12) 2 µ 2 µ (cid:12)
|g(x)−g(x˜)| = (cid:12) (cid:12) (cid:12)2√ µ+√ x − 2√ µ+√ x′(cid:12) (cid:12)
(cid:12)
(cid:12) √ √ (cid:12)
√ (cid:12) x− x′ (cid:12)
≤ 2 µ(cid:12) (cid:12) √ √ √ √ (cid:12) (cid:12)
(cid:12)(2 µ+ x)(2 µ+ x′)(cid:12)
√ (cid:12) (cid:12) x−x′ (cid:12) (cid:12)
≤ 2 µ(cid:12) √ √ (cid:12)
(cid:12)4µ( x+ x′)(cid:12)
(cid:12) (cid:12)
√ (cid:12) x−x′ (cid:12)
≤ 2 µ(cid:12) (cid:12)
(cid:12) (cid:112) (cid:12)
(cid:12)4µ max{x˜,x}(cid:12)
|x−x′|(1−ϵ )
4
≤ .
4µϵ
4
Thus, if it holds that
(cid:12) (cid:12)x−x′(cid:12) (cid:12) ≤ 4µ
ϵ2
4 , (E.1)
(1−ϵ )
4
then the function is always less than ϵ .
4
E.3 Perturbation bounds
Theorem E.1 (Corollary 2.7, p. 119 in Stewart and guang Sun (1990)). Let κ(A) = ∥A∥ (cid:13) (cid:13)A−1(cid:13) (cid:13)
2 2
be the condition number of A. If A˜ = A+E is non-singular, then
(cid:13) (cid:13)
∥E∥ (cid:13)A˜−1(cid:13)
(cid:13) (cid:13) 2(cid:13) (cid:13)
(cid:13)A˜−1−A−1(cid:13) ≤ κ(A) 2.
(cid:13) (cid:13) 2 ∥A∥
2
∥E∥
If in addition κ(A) 2 < 1 then
∥A∥
2
(cid:13) (cid:13)
(cid:13) (cid:13) (cid:13)A−1 (cid:13)
(cid:13)A˜−1(cid:13) ≤ 2 ,
(cid:13) (cid:13) 2 1−κ(A)∥E∥ 2
∥A∥
2
46and thus
∥E∥
κ(A) 2
(cid:13) (cid:13)A˜−1−A−1(cid:13) (cid:13) ≤ ∥A∥ 2 (cid:13) (cid:13)A−1(cid:13) (cid:13) .
(cid:13) (cid:13) 2 1−κ(A)∥E∥ 2 2
∥A∥
2
Corollary E.2. Let f be the regularized logistic loss defined in (3.5) with regularization parameter
µ > 0. For the matrix B = (∇2f(x)+E ), it holds that
1
(cid:13) (cid:13)B−1−(∇2f(x))−1(cid:13) (cid:13) ≤ ∥E 1∥ 2
2 µ(µ−∥E ∥ )
1 2
∥E ∥
or equivalently, B−1 = (∇2f(x))−1+E′ with ∥E′∥ ≤ 1 2 .
1 1 2 µ(µ−∥E ∥ )
1 2
Proof. From Theorem E.1 we have that
(cid:13) (cid:13)B−1−(∇2f(x))−1(cid:13) (cid:13) ≤
(cid:13) (cid:13)(∇2f(x))−1(cid:13) (cid:13)2
2∥E 1∥ 2 ≤ ∥E 1∥ 2
2 1−∥(∇2f(x))−1∥ ∥E ∥ µ(µ−∥E ∥ )
2 1 2 1 2
because (cid:13) (cid:13)(∇2f(x))−1(cid:13) (cid:13)
2
≤ 1/µ and we have assumed that ∥E 1∥
2
≤ µ. ■
E.4 Condition number of perturbed matrix
To show that the condition number of the matrix B = ∇f(x)+E is close to the condition number
of ∇f(x) we will use Weyl’s inequality.
Lemma E.3 (Weyl’s Corollary 4.9 in Stewart and guang Sun (1990)). Let λ be the eigenvalues of a
i
matrix A with λ ≥ ... ≥ λ , λ˜ be the eigenvalues of a perturbed matrix A˜ = A+E and finally let
1 n i
ϵ ≥ ...ϵ be the eigenvalues of E. For i = 1,...,n it holds that
1 m
λ˜ ∈ [λ +ϵ ,λ +ϵ ] (E.2)
i i n i 1
Thus, for the matrix B we have that the condition number of the eigenvalues of B can be
bounded as follows
λ (∇f(x))+λ (E) ≤ λ (B) ≤ λ (∇f(x))+λ (E) (E.3)
min min min min max
λ (∇f(x))+λ (E) ≤ λ (B) ≤ λ (∇f(x))+λ (E) (E.4)
max min max max max
For ∥E∥ < µ we have that
2
λ (∇f(x))+λ (E) λ (∇f(x))+λ (E)
max min ≤ κ(B) ≤ max max (E.5)
λ (∇f(x))+λ (E) λ (∇f(x))+λ (E)
min max min min
κ(∇f(x))+λ (E)/λ (∇f(x)) κ(∇f(x))+λ (E)/λ (∇f(x))
min min ≤ κ(B) ≤ max min (E.6)
1+λ (E)/λ (∇f(x)) 1+λ (E)/λ (∇f(x))
max min min min
And since ∥E∥ is small the two condition numbers are close.
2
47