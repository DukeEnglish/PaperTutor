JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 1
Solving the bongard-logo problem by modeling a
probabilistic model
Ruizhuo Song, Beiming Yuan
Abstract—Abstract reasoning problems challenge the percep- Graphical reasoning problems, in particular, pose multidi-
tual and cognitive abilities of AI algorithms, demanding deeper mensional challenges to deep learning. The challenges that
patterndiscernmentandinductivereasoningbeyondexplicitim-
graphical reasoning problems present to deep learning are
agefeatures.ThisstudyintroducesPMoC,atailoredprobability
multifaceted and often distinct from conventional machine
model for the Bongard-Logo problem, achieving high reasoning
accuracy by constructing independent probability models. Addi- learning tasks.
tionally,wepresentPose-Transformer,anenhancedTransformer- Firstly, graphical reasoning problems demand that models
Encoderdesignedforcomplexabstractreasoningtasks,including should capture and decipher inherent regularities and patterns
Bongard-Logo,RAVEN,I-RAVEN,andPGM.Pose-Transformer within graphs. However, due to the intricacy and variability
incorporatespositionalinformationlearning,inspiredbycapsule
of graphs, models must possess highly flexible and scalable
networks’ pose matrices, enhancing its focus on local positional
relationships in image data processing. When integrated with capabilities to adapt to diverse graphical structures and char-
PMoC,itfurtherimprovesreasoningaccuracy.Ourapproachef- acteristics.Thisaspectposesasignificantchallengetoexisting
fectively addresses reasoning difficulties associated with abstract deep learning models due to their often-limited architectural
entities’ positional changes, outperforming previous models on
rigidity and representational capacity.
the OIG, D3×3 subsets of RAVEN, and PGM databases. This
Secondly,graphicalreasoningproblemsrequiremodelsthat
research contributes to advancing AI’s capabilities in abstract
reasoning and cognitive pattern recognition. can handle the compositionality and structure of graphs.
This necessitates the model’s capacity to comprehend the
Index Terms—Abstract reasoning, RPM problem, Bongard-
constituent elements of graphs and their interrelationships for
logo problem.
effective reasoning and prediction. However, prevalent deep
learning models often face impediments in addressing such
I. INTRODUCTION problems due to their limited ability to explicitly model and
DEEP neural networks have achieved remarkable success efficiently handle graphical structures.
Additionally, graphical reasoning problems mandate mod-
in various fields, including computer vision [1]–[3], nat-
els to possess abstraction and generalization capabilities for
ural language processing [4]–[6], generative models [7]–[9],
graphs. This entails the model’s proficiency in extracting
visual question answering [10], [11], and abstract reasoning
general regularities and patterns from specific graphical in-
[15]–[17]. Deep learning, as a pivotal branch of machine
stances for subsequent reasoning and judgment on novel
learning, Simulate the learning process of the human brain by
graphs. Nevertheless, deep learning models, which typically
establishing multilayered neural networks, thereby facilitating
undergo training on specific instances, often exhibit subpar
the learning and inference of intricate patterns within data. In
performance in abstraction and generalization tasks.
the domain of graphical abstract reasoning, deep learning has
Furthermore, addressing graphical reasoning problems ne-
found extensive applications to address diverse and intricate
cessitatesmodelswithefficientlearningandreasoningcapabil-
pattern recognition and reasoning problems [15]–[17].
ities.Giventhecomplexityandlarge-scalenatureofgraphical
Throughdeeplearning,wecanarchitectmultilayeredneural
data, models must be adept at rapid learning and reason-
network models that, by undergoing extensive training and
ing within constrained computational resources for real-time
learning from vast datasets, can autonomously learn and
applicability and scalability in practical scenarios. However,
extract pertinent features and patterns. These extracted fea-
prevalent deep learning models often encounter challenges
tures and patterns enable operations such as classification,
like high computational complexity and prolonged training
regression,andclusteringonnoveldata,facilitatingautomated
durationswhendealingwithlarge-scalegraphicaldata,posing
decision-makingandinference.Theapplicationsofdeeplearn-
significant challenges in tackling graphical reasoning prob-
ingarevastandwidespread,positioningitasapivotalresearch
lems.
direction within the artificial intelligence sphere. However,
Consequently,addressingthechallengesposedbygraphical
research and applications of deep learning are not devoid
reasoning problems to deep learning constitutes a pivotal
of challenges and issues, encompassing aspects like model
research direction. This necessitates efforts towards designing
generalizability, training stability, and efficiency.
more flexible and scalable deep learning models, enhancing
representational and learning methodologies for models, and
ThispaperwasproducedbytheIEEEPublicationTechnologyGroup.They
areinPiscataway,NJ. developing efficient learning and reasoning algorithms. By
ManuscriptreceivedApril19,2021;revisedAugust16,2021. surmounting these challenges, we can propel the application
0000–0000/00$00.0a0n©d20a2d1vaIEnEcEement of deep learning in graphical reasoning
4202
raM
5
]VC.sc[
1v37130.3042:viXraJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 2
problems, providing smarter and more efficient solutions for TheRAVENdatabaseexhibitsdiversitythroughitsmultiple
a myriad of practical problems. sub-databases. These include single-rule groups: center single
For instance, problems like Ravens Progressive Matrices (center), distribute four (G2×2), distribute nine (G3×3), and
(RPM)[15]andBongardproblems[16],[17]presentlearning double-rule groups: in center single out center single (O-IC),
requirements that span from perception to reasoning. up center single down center single (U-D), left center single
right center single (L-R), in distribute four out center single
(O-IG).Inproblemsadheringtosingle-rulevariations,thepro-
A. The RAVEN database
gressiveattributechangeswithinimageentitiesareboundbya
The RAVEN database [18], a subset of the broader RPM unitarysetofrules.Conversely,double-rulevariationsenforce
(Raven’s Progressive Matrices) problem set, typically com- constraints via two independent sets of rules, augmenting the
prises of 16 geometric entity-based images. These are struc- complexity of the problem.
tured into a problem statement consisting of 8 images and
an answer pool containing the remaining 8 images. The
B. Pgm database
objective for subjects engaging with this database is to select
Pgm [19] and RAVEN problems exhibit a striking par-
the appropriate images from the answer pool, enabling the
allelism in their design philosophy, with both frameworks
completionofa3x3matrixwithintheproblemstatement.This
employing a problem statement consisting of 8 images and
matrix construction challenges individuals to discern specific
an answer pool supplemented by an additional 8 images.
abstract concepts embodied by the progressive patterns of
A unique facet of PGM problems is the expansion of the
geometric images arranged in a row-wise manner.
”rule” concept, which not only encompasses the progressive
Asexemplifiedinfigure1,theconstructionofRAVENprob-
patterning of”visual attributes”horizontally withinthe matrix
lems adheres to a structured framework. Within this frame-
but also vertically. An illustrative example of a PGM problem
work, human-defined concepts inherent to geometric images,
is presented in the accompanying figure 2, highlighting its
such as ”shape” and ”color”, undergo a process of artificial
intricate nature.
abstraction. This abstraction transforms these concepts into
bounded, countable, and precise ”visual attribute” values.
Subsequently, the notion of ”rule” is invoked to delineate problem statement
Shape:
the systematic progression of a circumscribed set of ”visual
Rule attr="size" name="XOR"
attribute” values. It is noteworthy that certain visual attributes Rule attr="type" name="progression"
remain unconstrained by the ”rule”, potentially introducing
elements of interference in the reasoning processes of deep
models.
？
line:
Rule attr="color" name="AND"
problem statement
answer pool
Out：
Rule attr="Number/Position" name="Constant"
Rule attr="Type" name="Progression"
Rule attr="Size" name="Distribute_Three"
Rule attr="Color" name="Constant"
In：
Rule attr="Number" name="Distribute_Three"
？ Rule attr="Type" name="Distribute_Three"/
Rule attr="Size" name="Distribute_Three"/
Rule attr="Color" name="Distribute_Three" Fig.2. PGMcase
answer pool
The intricacy of RPM problems, therefore, arises not only
from the extraction of visual attributes across multiple hier-
archies but also from the challenge of inducing and learning
the progressive patterns displayed by these ”visual attributes.”
Thisemphasizesthesignificanceofdiscerningnotonlytheat-
tributesthemselvesbutalsotheirstructuralprogressionwithin
these problems, a task that requires a nuanced understanding
Fig.1. RAVENcase of both the individual components and their interrelationships
within the problem context.
The generative process for a complete RAVEN problem
commences with the selection of a rule sample from a prede-
C. Bongard-logo database
fined rule repository. This is followed by the design of visual
attributevaluesalignedwiththecontentsofthechosen”rule”. Bongard problems [16], contrasting with RPM (Raven’s
Attributes not governed by the rule undergo random value Progressive Matrices) problems, are noteworthy exemplars of
assignments. Subsequently, image rendering occurs based on small-sample learning challenges. These problems typically
the generated ”attribute” information. present a series of images divided into two distinct groups:JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 3
primary and auxiliary. The primary group comprises images and 3) Abstract Concepts (hd), designed to assess a model’s
that adhere to a specific set of rules defining an abstract proficiency in discovering and reasoning about abstract con-
concept, whereas the auxiliary group includes images that cepts such as convexity, concavity, and symmetry. These
deviate from these rules in varying degrees. The intricacy of categorizations underscore the breadth and depth of reasoning
Bongard problems lies in the requirement for deep learning abilities required to tackle Bongard-logo problems effectively.
algorithmstopreciselycategorizeungroupedimagesintotheir
corresponding groups based on subtle pattern recognition and
II. RELATEDWORK
abstract reasoning.
Withinthedomainofabstractreasoning,Bongard-logo[17] This section is devoted to comprehensively presenting the
problems serve as a particular instantiation of Bongard prob- significantresearchachievementsrelatedtotheRPM(Raven’s
lems, posing a significantly high level of reasoning difficulty. Progressive Matrices) problem and the Bongard-Logo prob-
Each Bongard-logo problem consists of 14 images, with 6 lem, as derived from past studies. These notable research
images belonging to the primary group, 6 to the auxiliary milestones have played a pivotal role in driving forward the
group, and the remaining 2 serving as categorization options. advancements in the field of artificial intelligence, specifically
These images are comprised of various geometric shapes, in relation to abstract reasoning capabilities. By thoroughly
and the arrangements of these shapes serve as the basis examining these milestones, we aim to foster a deeper un-
for grouping. Figure 3 illustrates an exemplary Bongard-logo derstanding of the complexities and nuances associated with
problem, showcasing the complexity and abstract nature of abstract reasoning challenges. Moreover, we hope to provide
these challenges. invaluable insights and guidance that can serve as a spring-
In figure 3, each Bongard problem is comprised of two board for future research efforts, ultimately contributing to
distinct image sets: primary group A and auxiliary group B. the continued growth and development of AI capabilities in
GroupAcontains6images,withthegeometricentitieswithin tackling abstract reasoning tasks.
each image adhering to a specific set of rules. Conversely,
group B includes 6 images that deviate from the rules es-
A. Graphical abstract reasoning dataset
tablished in group A. The task at hand requires determining
whetherthe imagesinthetest setalignwith therulesdictated The original RAVEN dataset [18] possessed a significant
by group A. The level of difficulty associated with these flaw in its candidate answer generation process. Specifically,
problems varies depending on their structural complexity. each incorrect answer within the candidate set had only one
erroneous visual attribute value, with the rest being identical
to the correct answer. Consequently, the most frequent value
primary group auxiliary group
among the candidate answers for each visual attribute would
correspond to the correct visual attribute value. As a result,
models could achieve remarkably high accuracy by merely
considering the answer pool as input, without accounting for
the question matrix’s statement section. This rendered the
performance on the RAVEN database an inadequate reflection
of the model’s true capabilities. To address this limitation,
I-RAVEN [20] and RAVEN Fair [21] were developed as
remedies.
Intherealmofcomputervision,thesuccessofdeeplearning
models often hinges on extensive data availability. However,
the initial Bongard problems comprised only 100 instances,
and even subsequent expansions left the question count rela-
tivelylimited.Giventhatdeeplearning’sremarkablestridesin
computer vision are largely data-dependent, Nie et al. devel-
oped a procedurally generated technique to address the data
scarcity issue, resulting in the Bongard-Logo [16] benchmark
database for Bongard problems. This comprehensive database
encompasses 2000 Bongard problems, categorized into three
main types: free-form problems, basic-shape problems, and
Fig.3. Bongardcase
abstract-shape problems.
Furthermore, Bongard-logo problems are classified into In free-form problems, the shape patterns within the test
three distinct conceptual categories: 1) Free Form problems set exhibit longer strokes compared to those in the training
(ff), in which each shape is composed of randomly sampled set. The basic-shape problem test set poses challenges in
action strokes, potentially resulting in one or two shapes per recognizing combinations of two shapes, with these founda-
image; 2) Basic Shape problems (ba), corresponding to the tional shapes appearing only once in the training set, ensuring
identification of a single shape category or combinations of no conceptual overlap between the test and training sets.
two shape categories represented within given shape patterns; The abstract-shape problem test set is further bifurcated: theJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 4
combined abstract shape test set and the novel abstract shape On the other end of the spectrum, models like ALANS
test set [12]–[14]. [29] eliminate the need for prior rule knowledge entirely.
Within the combined abstract shape test set, 20 pairwise Despite this, they demonstrate superior generalization capa-
combinations of two abstract attributes are randomly selected, bilities compared to end-to-end models. This underscores the
with 20 problems per combination. While all individual at- importance of balancing learning and inference in complex
tributes in this test set have been observed separately in the reasoning tasks.
training set, these 20 novel combinations are exclusive to the NVSA[30],employsholographicvectorizedrepresentations
test set. This necessitates the model’s ability to comprehend along with ground-truth attribute values to construct a neural-
abstract shape concepts and utilize them in combinations. symbolic model, aiming to enhance the model’s compre-
On the other hand, the novel abstract shape test set extracts hension and reasoning capabilities. This approach combines
one attribute from the training set along with all its com- the strengths of both neural networks and symbolic logic,
binations with other attributes. All questions related to the leveraging their respective advantages in handling continuous
retained attribute are unique in this test set. The specialized data patterns and discrete symbolic rules.
test sets demand that models not merely memorize shape Finally,RS-CNN[31]andRS-TRAN[31]combinethebest
concepts but also demonstrate combinatorial generalization aspects of the aforementioned methods to achieve state-of-
and extrapolation capabilities [12]–[14]. These rigorous test the-art results on RPM problems. RS-TRAN, in particular,
sets aim to evaluate the model’s profound understanding of leverages the powerful Transformer-Encoder architecture to
shape concepts and its innovative prowess. tackle RPM problems from multiple perspectives and through
multiple inferences. Meanwhile, RS-CNN adopts a multi-
scale and multi-inference approach that combines both local
B. RPM solver
and global information for more accurate reasoning [37]. In
Discriminative models designed for image reasoning tasks this paper, we have designed a novel standard layer that
typically output a multidimensional vector, in which each di- can replace the conventional Transformer-Encoder within RS-
mensioncorrespondstothelikelihoodofselectingaparticular Tran. When this new layer is integrated, RS-Tran exhibits
graphfromasetofcandidateanswersasthecorrectresponse. significant improvement in inference progress, demonstrating
Thesemodelsvaryintheirapproachtolearningandinference. its enhanced efficiency and performance.
The CoPINet model [22], for instance, introduces a novel Overall, these models represent the cutting edge of image
contrast module that enables the network to learn subtle reasoning research and offer promising directions for future
differences between input graphs. This is complemented by work.
an inference module that distills potential underlying rules
fromthegraphstructures.Bycontrast,theLEN+teachermodel C. Bongard solver
[23]leveragesastudent-teacherparadigmtodetermineoptimal Inrecentyears,threemainapproacheshavebeenemployed
training sequences and make predictions based on this guided toaddressBongardproblems:methodsbasedonlanguagefea-
learning. ture models, methods based on convolutional neural network
The DCNet model [24] takes a different approach, employ- models, and generated datasets.
ing a dual contrast module to directly compare rule rows and Methods based on language feature models [16]: somes
columns.Thisallowsthemodeltopinpointdifferencesamong researchers proposed a solution leveraging image-based for-
candidateanswersmoreprecisely.Meanwhile,theNCDmodel mal languages, transforming image information into symbolic
[25] eschews traditional supervised learning methods, instead visual vocabulary, and subsequently tackling BP problems
optingforanunsupervisedapproachthatincorporatespseudo- via symbolic languages and Bayesian inference. However, the
targets and decentralization techniques to train the network. limitationofthisapproachliesinitsinabilitytodirectlyapply
In the context of subgraph-level analysis, SCL [26] es- to complex, abstract conceptual BP problems due to the need
tablishes multiple monitors for subgraphs within reasoning for reconstructing symbolic systems.
problems. This ensures that each branch of the model can Methods based on convolutional neural network models
focus on specific visual attributes or rules, enhancing the [32]:someresearchersconstructedanimagedatasetconsisting
model’s ability to handle complex reasoning tasks. SAVIR- of simple shapes and utilized these images for pre-training to
T [27] further extends this concept by extracting information cultivateafeatureextractor.Subsequently,imagefeaturesfrom
not only within subgraphs but also between them, providing Bongard problems were extracted for image classification,
a more holistic view of the reasoning problem and thus determining whether test images conformed to the rules. Yun
improving reasoning capabilities. adopted a similar approach, initially pre-training with images
Recent research has also highlighted the benefits of using containing visual features from BP problems to extract BP
relatively decoupled perceptual visual features for reasoning image features and then integrating an additional classifier for
tasks.Thesefeatures,whencombinedwithsymbolicattention discrimination.
methods, offer not only higher reasoning accuracy but also Methods based on Generated datasets [17]: some re-
strongermodelinterpretability.ThisisexemplifiedinthePrAE searchers employed fundamental CNNs, relation networks
[28] model, where neurosymbolic systems incorporate prior like WReN-Bongard, and meta-learning techniques, but their
knowledge of rules to perform probabilistic reasoning and performance on the Bongard -Logo database was less than
generate answer images. optimal.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 5
These represent the main solutions targeted at Bongard Initially, the input sequence undergoes an embedding layer
problems in recent years, each with its own strengths and transformationintofixed-dimensionalvectors,augmentedwith
limitations. positionalencodingtoretainpositionalinformationwithinthe
sequence. Subsequently, the encoder, composed of multiple
D. Capsule Network identical layers, each encompassing two sub-layers, com-
mences its operation. The first sub-layer, the multi-head self-
Capsule Networks (CapsNets) [33] are a novel deep neural
attention layer, calculates attention scores between different
network architecture. The design inspiration for this model
positionsintheinputsequence,capturingglobaldependencies.
comesfromthe“capsule”structuresinthehumanbrain,which
Thesecondsub-layercomprisesasimplefullyconnectedfeed-
are able to effectively process different types of visual stimuli
forwardneuralnetwork.Bothsub-layersarefollowedbylayer
and encode information such as position, shape, and speed.
normalization operations and residual connections, ensuring
In the context of deep learning, a capsule is a collection of
model stability and training efficiency.
embedded neurons, and a CapsNet is composed of capsules
The decoder section, also comprising multiple identical
rather than individual neurons. It operates through a dynamic
layers, introduces an additional encoder-decoder multi-head
routing algorithm to update network weights, enabling the
attention layer in each decoding layer. This attention layer
network to better understand and recognize features in im-
enablesthedecodertoattendtotheencoder’soutputduringthe
ages. Compared to traditional Convolutional Neural Networks
generation of the output sequence, facilitating interaction be-
(CNNs),CapsNetsdemonstratestrongerfeaturerepresentation
tweentheencoderanddecoder.Furthermore,theself-attention
capabilities and improved performance.
layers in the decoder incorporate a masking mechanism to
Specifically,eachcapsuleinaCapsNetrepresentsaspecific
prevent consideration of future information when generating
feature detector. The output of a capsule is a vector, where
outputs for the current position.
the length of the vector represents the probability of the
The uniqueness of the Transformer architecture lies in its
feature’s existence, and the direction of the vector encodes
exclusive reliance on attention mechanisms, circumventing
the pose information of the feature. As a feature moves
the inherent sequential computation limitations of traditional
within the network, the output vector of the capsule changes
recurrent and convolutional neural networks. This design al-
accordingly, while maintaining its length (i.e., the probability
lows for parallel processing of input sequences, significantly
of the feature’s existence).
enhancing training speed and model performance. Addition-
Furthermore,CapsNetsutilizeanagreement-baseddynamic
ally, the Transformer exhibits impressive scalability, accom-
routing algorithm as an alternative to the Max-Pooling op-
modating tasks of varying scales and complexities through
eration in traditional CNNs. This dynamic routing algorithm
the addition of encoder and decoder layers and adjustment
can automatically adjust the structure and connection weights
of model parameters.
of the network based on the characteristics of the current
In summary, the Transformer architecture’s innovative ap-
input data, thereby enhancing the network’s performance and
proach to sequence modeling, characterized by its reliance on
generalization capabilities.
self-attentionmechanismsandparallelprocessingcapabilities,
The Capsule Network learns corresponding pose matrices
has ushered in a new era of advancements in natural language
for all local representations of an image, which are utilized to
processing and beyond.
map these local representations onto high-dimensional repre-
sentationswithintheobjectspace.ThroughtheGaussianRout-
F. Vision Transformer
ing algorithm, the Capsule Network computes the weights of
Vision Transformer (ViT) [34] is a groundbreaking image
thesemappedhigh-dimensionalrepresentations.Theseweights
classification model that adapts the Transformer architecture
are then applied to calculate a weighted sum of the high-
from the domain of Natural Language Processing (NLP) and
dimensional representations, with the compressed summation
applies it successfully to Computer Vision (CV) tasks. The
resultindicatingposeconsistencyamonglocalfeatureswithin
uniquenessofViTliesinitstreatmentofimagesassequential
the image. During the training process, the pose matrices
data, akin to text sequences in NLP, thereby leveraging the
adapttolearntherelativepositionalrelationshipsamonglocal
powerful sequence modeling capabilities of the Transformer.
representations.
In ViT, the input image is initially divided into fixed-size
Currently, CapsNets are primarily applied in the field of
patches, which are then flattened and transformed into feature
image recognition and have demonstrated impressive perfor-
vectors through linear transformations. These feature vectors
mance on several benchmark tests.
areaugmentedwithpositionalencodingstoretaintheirspatial
positional information within the image. Subsequently, the
E. Transformer
processed feature vectors are fed into a standard Transformer
TheTransformer[4]architectureisadistinctivedeeplearn- encoder for processing.
ing framework that revolves around the self-attention mecha- The Transformer encoder comprises multiple layers of self-
nism to capture global dependencies within input sequences. attention and feed-forward neural networks, arranged in a
Comprising of an encoder anda decoder, both structured with stackedmanner.Theself-attentionlayerscapturedependencies
stacked layers of multi-head self-attention and feed-forward between different positions within the input sequence, while
neural networks, the Transformer offers a paradigm shift in the feed-forward neural networks introduce non-linear trans-
sequential data processing. formationstoenhancethemodel’sexpressivepower.ResidualJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 6
connections and layer normalization techniques are employed A. A method for solving the bongard-logo problem based on
in each layer of the encoder to ensure model stability and sinkhorn distance (SBSD)
convergence speed.
In the Bongard-Logo problem, we denote x as the j-th
ij
Ultimately, the output from the Transformer encoder is fed
image in the i-th Bongard-Logo problem, where i ∈ [1,n]
into a fully connected layer for classification, yielding the
signifies the question number, with n being the total number
predicted categories for the image. Due to its sequence-to- ofquestions.Specifically,{x }6 representsimagesinthei-
ij j=1
sequence modeling approach similar to NLP, ViT effectively thprimarygroup,while{x }13 representsimagesinthei-th
ij j=8
handles images of various sizes and resolutions and exhibits
auxiliary group. Additionally, x represents the test image to
i7
impressive scalability.
be potentially assigned to the i-th primary group, and x
i14
In summary, by adapting the Transformer architecture from
represents the test image to be potentially assigned to the
NLP, Vision Transformer (ViT) has ushered in a new era of i-th auxiliary group. we let the distribution of the primary
advancementsinComputerVision.Itsemergencehasnotonly
group (positive examples) within a Bongard-Logo question
challenged the dominance of traditional Convolutional Neural be denoted as p (x|y), and the distribution of the auxiliary
i
Networks (CNNs) in image classification tasks but has also group (negative examples) be denoted as q (x|y). Here, y
i
laid a solid foundation for the development of subsequent
representsthereasoningtypeconditionofthequestion,where
visual Transformer models. y ∈ {ff,ba,hd}. We are committed to developing a deep
learning algorithm that can induce distributions p (x|y) and
i
q (x|y) with low cross-measure between them.
i
G. Sinkhron distance
Therefore, we aim to devise a deep learning algorithm,
The Sinkhorn distance [35] is a notion derived from the denoted as f θ(z|x), which can map samples x ij to latent
field of optimal transport theory, offering a computationally variables z ij.We hopethat f θ(z|x) canserve asadistribution
efficient alternative to the classical Wasserstein distance [36]. transformation function, and by measuring the distribution of
At its core, it approximates the cost of transporting mass the latent representation, we can address the Bongard-Logo
from one probability distribution to another by relaxing the problem. Thus, the key objective is to ensure that the cross-
constraint of mass preservation through a regularization term. measurebetweenthelatentvariabledistributionoftheprimary
This regularization, typically parameterized by a scalar value group, denoted as p′(z|y), and the latent variable distribution
i
known as the entropy regularization parameter, allows for a of the auxiliary group, denoted as q′(z|y), is significantly
i
smoother, more tractable optimization problem. low.TheKullback-Leibler(KL)divergence,ameasureusedto
The Sinkhorn algorithm, closely associated with this dis- quantifythesimilaritybetweendistributions,canbeemployed
tance, iteratively updates two sets of scaling factors until forthispurpose[7].TheformulaforcalculatingKLdivergence
convergence to a solution that minimizes the regularized is provided below.
transport cost. This approach is particularly advantageous in
(cid:18) (cid:19)
high-dimensionalsettingsorwhendealingwithlargedatasets, KL(P||Q)=(cid:88) P(x)log P(x) (1)
as it avoids the expensive linear programming computations Q(x)
required by the standard Wasserstein distance. The Sinkhorn
Due to the characteristics of small sample learning problems,
distance offers a means to measure the similarity between it is challenging to estimate the distributions p′(z|y) and
two distributions solely based on samples drawn from them, q′(z|y) of Bongard-Logo image representations. Adi ditionally,
without the need to have explicit knowledge of their underly- i
the limitations of the Kullback-Leibler (KL) divergence make
ing forms. This advantage sets it apart as a powerful tool in
it difficult to directly optimize for these conditions and obtain
scenarioswheredirectaccesstothedistributionalformsisnot
a highly effective deep model.
feasible.
Hence, we endeavored to employ baseline models, such
as ResNet18, for encoding images within the Bongard-Logo
paradigm {x }14 into meaningful representations {z }14 .
III. METHODOLOGY ij j=1 ij j=1
These representations, within a Bongard-Logo question, were
The Bongard-logo [16] and Raven’s Progressive Matrices treated as samples drawn from p′(z|y) and q′(z|y) distribu-
i i
(RPM)[15]tasksarebothparadigmsofabstractreasoningthat tions. Subsequently, we imposed constraints on the Sinkhorn
necessitate the extractor’s adeptness in unveiling precise con- distance [35] between p′(z|y) and q′(z|y) through these rep-
i i
ceptsveiledwithinabstractrepresentations.Theseconceptsare resentations. This approach was chosen because the Sinkhorn
indicative of more elevated abstractions, surpassing rudimen- algorithmcalculatesthedistancewithoutrequiringknowledge
tary ideas such as “pixel configuration patterns”. Frequently, ofthespecificformsofthedistributions,thusmakingitideally
these elevated abstractions encapsulate diverse degrees of suited for scenarios where the forms of p′(z|y) and q′(z|y)
i i
human-centered presupposed knowledge, including elements are unknown. In the detail, we randomly divided the repre-
likeform,dimension,hue,spatialorientations,concave/convex sentations within the primary group {z }7 into two dis-
ij j=1
contours of shapes, and shape completion. Through the em- tinct subgroups: {z } and {z }. By leveraging the Sinkhorn
it it˜
ployment of distinct querying approaches, the Bongard-logo distance between these subgroups, we imposed constraints to
and RPM tasks endeavor to evaluate the extractor’s expertise enforce the representations in the primary group {z }7 to
ij j=1
in unveiling and assimilating these abstract concepts. follow a consistent, albeit unknown, distribution. Similarly,JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 7
we introduced analogous constraints between the distributions p′(z|y). For this purpose, we assume that p′(z|y) follows a
i i
denoted as p′(z|y) and q′(z|y), utilizing representations from multivariate Gaussian distribution.
i i
both the primary and auxiliary groups to minimize the cross- Our PMoC consists of two components: module f (z|x)
θ
measure. The aforementioned constraints can be concretely and module g (µ,σ2|z). The module f (z|x) maps the input
ω θ
expressed as mathematical loss function: x to its latent representation z, while g (µ,σ2|z) is utilized
ω
to compute the µ and σ2 for Gaussian distribution p′(z|y).
ℓ({z }14 ) i
ij j=1 In this paper, we define {x }6 as the primary group
ij j=1
e−D({zit},{z it˜})
samples for the i-th Bongard-Logo question, with j de-
=−log (2)
e−D({zit},{z it˜})+e−D({zij}7 j=1,{zij}1 j=4 8) noting the sample number. Similarly, {x ij}1 j=3
8
represents
the auxiliary group samples for the i-th question. x and
Let D(a,b) denote the calculation of the Sinkhorn distance i7
x are the representations of the samples to be classified.
between the distributions associated with sets a and b. Ad- i14
ditionally {z it}∪{z it˜} = {z ij}7
j=1
and {z it}∩{z it˜} = ∅. sM amea pn lw eshi {le x, { }z 1i 4j} ,1 j=4 a1 ndre tp hr ees ee nn ct os dt ih ne
g
pen roc co ed se sd isB io mng pa lerd m- eL no tg eo
d
Despiteourendeavors,thisapproachdidnotyieldsatisfactory ij j=1
by f (z |x ). Furthermore, we utilize a deep network, de-
results.Wehypothesizethatthiswasprimarilyduetothelim- θ ij ij
notedasg (µ ,σ |{z }6 ),tofitanoptimizablemultivariate
itedsamplesizewithinasingleBongard-logoproblem,which, ω i i ij j=1
Gaussian distribution p′(z|{z }6 ,ω), based on the primary
when collectively treated as sampling outcomes, may not i ij j=1
meet the prerequisite sampling requirements of the Sinkhorn group samples features {z ij}6 j=1. Subsequently, we calculate
distance. An illustrative diagram depicting this process can be the probability of each representation in {z ij′}1 j′4 =7 belonging
found in Figure 4. tothisdistribution,denotedas{p′ i(z ij′|{z ij}6 j=1,ω)}1 j′4 =7.The
network f (z |x ) maps the input x to its latent represen-
θ ij ij ij
tation z , while g (µ ,σ |{z }6 ) is utilized to compute
ij ω i i ij j=1
Images in primary group the distribution of the latent representations {z }6 , and
ij j=1
the distribution is modeled as Gaussian distribution. In other
The test image that need to be
aligned to the primiary group. words, f (·|·) serves as an image encoder, and g (·|·) as a
θ ω
logical distribution fitter. Thus, we employ a Convolutional
fθ(zij|xij) ResNet50
Neural Network (CNN) to fit f (·|·) and a Transformer-
θ
Images in Encoder to fit g (·|·). Further design details are provided as
auxiliary group ω
follows.
The test image that need to be
aligned to the axuiliary group. We present an approach that combines ResNet18 with
Transformer-Encoder, employing this integration as the ar-
Representations of images in
primary group chitecture for the function f (z |x ). In detail, we encode
θ ij ij
Sinkhorn Sinkhorn Representations of images in the Bongard-Logo problem images x ij into feature maps
Distance Distance auxiliary group h ∈Rh×w×d usingResNet18.Subsequently,wecalculatethe
ij
×(-1) ×(-1) Representations of the test self-attention results z i′ j ∈ Rn×d among all receptive fields
e( ) e( ) image that in primiary group. within the feature maps h . This computation enables the n
ij
1 0 R ime ap gre es ie nn ata ut xio ilin as ry o gf r oth ue p .test attentionoutputsz i′ j tocaptureglobalinformationoftheimage
in varying degrees, ensuring that each output retains some
Fig.4. FeedforwardprocessofSBSD level of contextual understanding. We encoded Bongard-Logo
images from multiple perspectives using the aforementioned
In summary, SBSD treats the representations of images method. We denote each individual perspective within z′ as
ij
within the primary group as samples from the primary group z ∈Rd. The feedforward process of the network f (z |x )
ij θ ij ij
distribution p′(z|y), and the auxiliary group follows the same
i is illustrated in the figure 5.
approach. By utilizing these samples, the Sinkhorn distance
is employed to constrain the distance between the primary
p′(z|y) and auxiliary distributions q′(z|y).
i i
h
B. PMoC(Probability Model of Concept) ... ...
n=h×w
d
Afterconductingexperiments,wediscoveredthatSBSDhad w d
achievedlimitedprogressininferenceaccuracy.However,this image: xij,j∈[1,14] ... z’ij,j∈[1,14]
paper aims to make more contributions and breakthroughs.
f(z'ij|xij（)ResNet18） positional encoding
Therefore, by carrying forward the idea of analyzing primary
heat map: hij,j∈[1,14] Transformer
and auxiliary group distributions in SBSD, this paper designs
thedeepmodelPMoC(ProbabilityModelofConcept).PMoC ... tokenized heat map: hij,j∈[1,14] zij,j∈[1,14]
explore an alternative approach by shifting the constraint con- zij,j∈[1,14]
ditions. The training objective has been shifted from focusing
on the deep model’s ability to disentangle distribution p′(z|y)
i Fig.5. Feedforwardprocessoff θ(zij|xij)
and distribution q′(z|y) to emphasizing its capacity to com-
i
pute the probability of a representation under the distribution Thenetworkg (µ ,σ |{z }6 )ismuchmoreofitsname.
ω i i ij j=1JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 8
It encodes the mean and variance of the i-th primary group distribution, and z is the representation of some unknown
distribution based on the latent representations of the i-th data. It is evident that optimizing the network using the
primarygroupsamples,andistillrepresentstheproblemindex given formula (3) as the loss function presents challenges, as
in Bongard-Logo. Specifically, we uniformly process each optimizingtheaboveequationcanbeindirectlyviewedasop-
perspectivez withinz′ .Underasingleperspective,wetreat timizingtheMeanSquaredError(MSE)lossbetweenvectors.
ij ij
{z }6 as six tokens and, after appending two optimizable Moreover, these vectors have not undergone normalization or
ij j=1
vectors along with positional embedding, input them into squashing,makingitdifficulttooptimizethenetworkfromthe
g (µ ,σ |{z }6 ) with a Transformer-Encoder backbone perspectiveofimplementation.Consequently,inthispaper,we
ω i i ij j=1
for computing attention results. We extract two optimizable train a deep network to directly fit probability p′(z |y). In-
i ij′
vectorsfromtheattentionresults,servingasthemeanµ ∈Rd evitably the network’s formulation has been transformed from
i
and logarithm of variance log(σ2) ∈ Rd for a multivariate g (µ ,σ |{z }6 ) to g (p′(z |y)|{z }6 ,z ), thereby
i ω i i ij j=1 ω i ij′ ij j=1 ij′
Gaussian distribution. The encoding distribution pattern of simplifying the problem.
moduleg (µ ,σ |{z }6 )followsasimilarapproachtothat Differentfrompreviousapproachesg (µ ,σ |{z }6 )that
ω i i ij j=1 ω i i ij j=1
of VAE [7]; however, it does not incorporate variational infer- fit the distribution p′(z|y), this paper employs a Transformer-
i
ence in its design. The aforementioned process parameterizes Encodertofittheprobabilityp′(z |y)directly,whichenables
i ij′
p′(z|y) as a multivariate Gaussian distribution and outlines the direct computation of the probability that z belongs to
i ij′
the computation of g (µ ,σ |{z }6 ). We feed 8 vectors the distribution p′(z|y).
ω i i ij j=1 i
{z }14 into this distribution for calculating 8 logical prob-
ij′ j′=7
abilities {p′(z |{z }6 ,ω)}14 in one perspective. In this
paper, we
ci alci uj′ latei tj hej= fi1
nal
prj o′ b= a7
bility p′(z′ ) by averaging
{xij| j∈[1,6]} {xij| j∈[8,13]} {zij| j∈[1,6]}
the logical probabilities p′(z |{z }6 ,ωi ) oij b′ tained from all xij,j=7 xij,j=14 {zij| j∈[8,13]}
i ij′ ij j=1 ResNet18
perspectives. By utilizing the cross entropy loss function, we fθ(zij|xij) + Transformer zij,j=7
Transformer
c eo nn sust rr ia ni gn ot th he erfi pn roa bl ap br io lib tia eb si {li pty
′
i(zp i′′ i j( )z }i′
1
j7 =4) 8to areap clp or so eac toh 01 .Fw igh ui rle
e
s S
T f r oo e fh r p ri
e
s re ee x
pml
st
re
r e eo a n
sdc
c t eu t
ait
nl n te i tg o
a
m
i n ts o
i o
n fr r
no
e e o ss
md
po
tu
n hs
el
i
e
b sl ee t
MLP zij,j=14
6 illustrates in detail the forward process of the network
s
gΩ(pi’(zij'|y)|{zij}6j=1,zij’)
g (µ ,σ |{z }6 )underasingleperspectivein z ∈Rn×d. zij’, j∈[7,14]
ω i i ij j=1 ij
However, this approach did not yield satisfactory results.
{zij|j∈[1,6]} mean pi’(zij’), j’∈[7,14]
{xij| j∈[1,6]} {xij| j∈[8,13]} {zij| j∈[1,6]}
fθ(zij|xij) R
x +ei j, sj= N7
et18
x Ti rj, aj= n1 s4
former
{ zz ij,ij j|
=
j 7∈[8,13]} Fig.7. Feedforwardprocessofgω(p′ i(z ij′|y)|{zij}6 j=1,z ij′).
Transformer
Specifically, in this paper, we maintain an egalitarian ap-
s Select module MLP zij,j=14
T f ro eh r pi s re e xm str eo a nd c tu t ail n te ig o i ns o n fr re e os mpo tn hs ei b sl ee t proachtowardseachperspectivez ij ∈Rd withintheoutcomes
s zij’, j’∈[7,14] of representations optimizable vector z pi e′ j rs∈ peR ctn iv× ed ,wof ef aθ m( az li gj| ax mij a) t. eW {zith }in
6
the wc ito hn ofi nn ees ofof tha epsi en ng du il na gr
ij j=1
loμ
gσ2
representations {z ij′}1 j′4 =7, resulting in a set of seven tokens.
pi(zij’), j’∈[7,14] Following theembedding of positional encoding,these tokens
arethensystematicallyfedintoaTransformer-Encoder,aimed
{zij|j∈[1,6]}
at yielding 7 attention results. All 7 attention results are
mappedto7probabilitiesbyaMLP.Themeanofthemapped
Fig.6. Feedforwardprocessofgω(µi,σi|{zij}6 j=1).
outcomes is considered as the logical probability p′(z |y)
i ij′
We speculate that the reason for the ineffectiveness of the correspondingtothespecifiedperspective.Thisconstitutesthe
aforementioned method lies in the unknown true form of the feedforward process of the new g ω(p′ i(z ij′|y)|{z ij}6 j=1,z ij′).
distributionp′(z|y),whichmaybeacomplexmixedGaussian Each pending representation z ij′, receives a correspond-
distribution. Ti herefore, it may not be reasonable to param- ing probability output p′ i(z ij′|y). Each logical probabilities
eterize p′ i(z|y) as a multivariate Gaussian distribution and {p′ i(z ij′|y)}1 j′4 =7 are averaged to yield the final probability
compute the probability p′ i(z ij′|y). Additionally, the formula value {p′ i(z i′ j′|y)}1 j′4 =7. In this paper, we still optimize the
for calculating the logical probability of a vector z belonging model using the cross-entropy loss function. Figure 7 visually
to the parameterized distribution is provided as follows. depicts the process of g ω(p′ i(z ij′|y)|{z ij}6 j=1,z ij′) handling a
single perspective.
Insummary,thispapermakestwocompromisesforPMoC.
(z −µ )2 1
log(p(z,µ,σ))= m m −log(|σ |)− log(2π) Firstly,weacknowledgethatcomparedtop′(z),q′(z)ismore
2σ2 m 2 i i
m likely to be a complex distribution. For instance, p′(z) may
(3) i
follow a Gaussian distribution, while q′(z) may adhere to a
i
Where m represents the dimension index of the vector. µ rep- mixedGaussiandistribution.Consequently,weshiftourfocus
resents the mean of the multivariate Gaussian distribution, σ to estimating the distribution p′(z) and utilize the probability
i
represents the standard deviation of the multivariate Gaussian of a representation belonging to this distribution to determine
)1=j6}jiz{|
gol
,(Ωg
2 
JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 9
whether it is a primary or auxiliary group representation. to {V |i ∈ [1,n],Vi ∈ Rd}, we obtain the logical output
i
Secondly, due to the challenges associated with optimizing {O |k ∈ [1,n],O ∈ Rd} of the foundational block of pose-
k k
the model from the perspective of estimating p′(z), this paper transformer. The squashing function is formulated as shown
i
opts to directly compute the probability of a representation in the equation below:
belonging to the distribution p′(z).
i
||v||2 v
squash(v)= · (4)
IV. POSE-TANSFORMER 1+||v||2 ||v||
Previousresearchhasdemonstratedlimitedsuccessinlearn- where the operator ||·|| is L norm. It is worth mentioning
2
ingabstractconceptsrelatedtopositionalrelationshipsamong that the squash function, squash(·), is employed in Capsule
entities, particularly when addressing the Bongard-Logo and Networks to squash pose vectors for computing pose consis-
RPM problems. This limitation is evident in challenges posed tencyamongalllocalrepresentations.Similarly,inthecontext
bytheOIGandD-9problemsetsfromRAVEN,aswellasthe ofpose-transformer,thesquashfunctioncontinuestoserveits
ff and hd issues of Bongard-Logo and the Neurtal problem in purpose and retain its significance, contributing to the overall
PGM. The RS-model, however, overcame these difficulties by architecture’s efficacy. The feedforward process of a single
explicitly incorporating additional structural cues (meta data, pose matrix embedding fundamental block is illustrated in
auxiliary label), achieving commendable inferential accuracy. the figure 8. The entire calculation process of “pose matrix
In this paper, we introduce the Pose-Transformer, an en- embedding” can be expressed as fallows:
hancement to the Transformer-Encoder framework that incor-
porates the pose matrix from the Capsule Network [33]. This
{V |i∈[1,n]} =T-Encoder({V˜|i∈[1,n]}) (5)
integrationimplicitlyenhancesthenetwork’sabilitytodiscern i i
positional relationships among local representations, thereby {H ij|j ∈[1,m]} =segment(V i) (6)
bolstering its learning capabilities on graphical abstract rea- P =H ×W (7)
ijk ij ijk
soning problem. We present and delve into the intricacies n m
(cid:88)(cid:88)
of our proposed Pose-Transformer, discussing its potential to O =(V |i=k)+squash( P ) (8)
k i ijk
revolutionize learning in tasks that involve complex positional
i=1j=1
relationships through experiments.
In the formula, the term “segment” represents the aforemen-
We delve into the intricacies of the Transformer-Encoder
tionedcuttingprocess.Theprocessoffeedingtheinputvector
framework, particularly when it processes logical inputs to
setintothetransformer-encodertoobtaintheoutputisdenoted
generate attention-weighted outputs. During this process, all
as “T-Encoder”.
output tokens become associated with varying degrees of
global information derived from the logical inputs [27],
[34]. Operating on a multi-head self-attention mechanism, the
add
Transformer-Encoder segments the input tokens into multiple
sub-vectorsandcomputestheirself-attentionresultsinparallel logic inputs Vi | i∈[1,n] sever
Hij | i∈[1,n] j, j∈[1,m]
Ok | k∈[1,n]
[34].Fromaninformationalstandpoint,wepostulatethatthese
i
segmented sub-vectors possess lesser information compared mpo as tre ix squash
to the holistic tokens encoding global information, effectively ... ... ... ... ... ... ... ...
rendering them as representations of localized information.
Consequently,therelativepositionalrelationshipsamongthese equal length
tokens
localized informational units warrant a summarization and
pose matrix
analysis. short token head
Specifically, we have designed a novel fundamental mod-
Hij
... long pose vector
ule termed “pose matrix embedding” tailored for the RPM outputs
and Bongard-Logo problems. When logical inputs {V˜ i|i ∈ Wijk | i∈[1,n], j∈[1,m], k∈[1,n] transformer layer
[1,n],V˜ ∈ Rd} pass through a single Transformer-Encoder
i
layer, this paper segments the n output tokens {V |i ∈
i
[1,n],V i ∈ Rd}, attention results, into m local vectors Fig.8. Thefeedforwardprocessofasingleposematrixembeddingblock
{H |i ∈ [1,n],j ∈ [1,m],H ∈ Rd/m} of the attention
ij ij
head size. A set of mapping matrices {W |i ∈ [1,n],j ∈ Thisfundamentalblockisfollowedbyafeedforwardblock,
ijk
[1,m],k ∈ [1,n],W ∈ R(d/m)×d} is established for each forming the basic building block of the Pose-Transformer.
ijk
local vector, with the number of matrices k within a set being The design of the feedforward block is consistent with
identical to the token count n of the logical input. These that in the Transformer-Encoder architecture. Notably, the
mappingmatricesfacilitatethetransformationoflocalvectors proposed fundamental block is dimension-preserving, allow-
into pose vectors (H ×W ). We compute the entire set of ing for the stacking of multiple blocks to adapt to dif-
ij ijk
pose vectors {P |i ∈ [1,n],j ∈ [1,m],k ∈ [1,n],P ∈ ferent scales of graphical reasoning problems. The Pose-
ijk ijk
Rd} resulting from the mapping of each local vector. By Transformer with N logical layers is depicted in the figure
summing and squashing these pose vectors and adding them 9. When N = 1, the Pose-Transformer collapses into a
]n,1[∈k
,]m,1[∈j
,]n,1[∈i
| kjiPJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 10
regularTransformer-Encoder.InCapsuleNetworks,similarity the logical inputs, represented as {V |i ∈ [1,n],V ∈ Rd} to
i i
weights are distributed across higher-level capsules and com- some extent. The computation of the logical outputs in Pose-
puted in a bottom-up fashion, whereas attention weights in Transformer’s pose matrix mapping follows the methodology
the Transformer-Encoder are distributed over lower-level rep- outlined below:
resentationsandcomputedtop-down.InthePose-Transformer
with N > 1, similarity weights for local representations can
be calculated at multiple levels and scales from the outside
in. This is ensured by the fact that each logical layer in the
Pose-Transformer is succeeded by a standard Transformer-
Encoder layer, which can be present in the subsequent logical  
n m
layerortheterminatinglayer,responsibleforrecalculatingthe (cid:88)(cid:88)
O k =V i+squash H ij ×W ijk (9)
similarity weights.
i j
logic inputs
It is worth noting that the placeholders marked with “ ” in
the formula represent numbering for on-demand broadcast-
transformer ing. Multiple levels of summation and mapping can incur
layer
significant computational overhead. By designing parameter
sever sharing, we aim to eliminate partial logical summation and
pose matrix mapping, thereby achieving the purpose of reducing compu-
tational cost. From a logical hierarchy perspective, only the
+ ×N-1
numbering at the i-th level has the potential for indentation,
while other logical levels have strong mapping requirements.
Inotherwords,thispaperpositsthattheposematricesbetween
equivalent local vectors across different tokens merit weight
feedforward
block sharing.Theplannedtargetexpressionofposematrixmapping
after indentation is:
+
transformer ×1
layer
 
n m
(cid:88)(cid:88)
logic outputs O k =V i+squash H j ×W jk (10)
i j
Fig.9. Thearchitectureofthepose-transformer
The Pose-Transformer diverges from the dynamic routing
mechanism of Capsule Networks, which iteratively computes
representation similarity weights. Instead, it employs an at- This paper aims to simplify the Pose-Transformer based on
tention mechanism to perform the task of the routing. For the aforementioned expression. We treat the mapping matrix
the complete computation of similarity weights, the Pose- W as a product of the mapping matrix W˜ multiplied by
ijk jk
Transformer terminates with a standard Transformer-Encoder i different coefficients, specifically expressed as W =˜w ·
ijk i
layer, which serves as the final stage for calculating similarity W˜ . This transformation significantly reduces the number
jk
weights on the pose vectors. of matrices involved. We argue that it is more reasonable to
combine w with H and treat them as a weighted sum
i ij
V. STRAWPOSE-TANSFORMER of H , rather than treating w as an optimizable weight
ij i
Pose-Transformer, during deployment, exerts considerable vector. Furthermore, treating this process as computing an
pressure on computational resources. The establishment and attention-weighted sum for H is even more elegant and
ij
computation of the mapping matrices, denoted as {W |i ∈ effective. Therefore, we propose a feasible solution that in-
ijk
[1,n],j ∈ [1,m],k ∈ [1,n],W ∈ R(d/m)×d}, incur volves embedding a learnable vector V , where V ∈Rd and
ijk 0 0
significantresourceoverhead.Motivatedbythischallenge,this we denote {H |j ∈ [1,m],H ∈ R(d/m)} as the attention
0j 0j
paper explores the lightweighting of Pose-Transformer. The heads corresponding to V , and computing the multi-head
0
crux of Pose-Transformer lies in computing compressed pose cross-attention results between V and {V |i ∈ [1,n]}. This
0 i
vectorsandsubsequentlyemployingthemtomodifyandrefine approach aligns well with the formula presented:
reyal
remrofsnat-esoP
kcolb
gniddebme
xirtam
esopJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 11
can be seen as a computational optimization that reduces the
  amount of computation and improves efficiency.
n m
(cid:88)(cid:88) The pose vectors {p |j ∈ [1,m],k ∈ [1,n],p ∈ Rd} is
O k =V i+squash H ij ×W ijk (11) jk jk
achieved by multiplying the attention heads with the mapping
i j
  matrices. These pose vectors, after summation and squashing,
O˜
k
=V
i+squash(cid:88)n (cid:88)m
H
ij
×(w i·W˜ jk)
oar be taia ndd the ed ot uo tpt uh te ofve nc et wor ps oe st e{ mV
ai
t| ri ix∈ ma[ p1 p, in n] g,V
bi
loc∈
k
{R Od} |it ∈o
i
i j [1,n],O ∈ Rd}. This process outlines the computation
  i
m n process of the pose embedding matrix block within the Straw
=V i+squash(cid:88) ((cid:88) w i·H ij )×W˜ jk Pose-Transformer architecture. The entire calculation process
j i ofnew“posematrixembedding”canbeexpressedasfallows:
 
m
=V i+squash(cid:88) H˜ j ×W˜ jk {V |i∈[0,n]} =Masked T-Encoder({V˜|i∈[0,n]})
i i
j
 
m (13)
=V i+squash(cid:88) H˜ j ×W˜ jk {H |j ∈[1,m]} =segment(V ) (14)
0j 0
j P =H ×W˜ (15)
  jk 0j jk
m
=˜V i+squash(cid:88) Attention(H ij )×W˜ jk (12) O =(V |i=k)+squash((cid:88)m P ) (16)
k i jk
j
j=1
Notably, the aforementioned approach necessitates computing Intheformula,theprocessoftheMaskedtransformer-encoder
cross-attentionadditionallybetweenthestandardTransformer- to obtain the output is denoted as “Masked T-Encoder”. In
Encoder layer and pose mapping, which prolongs the training the Straw Pose-Transformer architecture, the outputs from
time, contradicting the intended purpose. This paper utilizes each logical layer, denoted as {O |k ∈ [1,n]}, undergo
k
a transformer encoder with an additional mask mechanism to a concatenation process with the global vector V attached
0
integratethetransformerencodingprocesswiththemulti-head to their precursors. This modified set, now represented as
cross attention computation process. {V˜|i∈[0,n]},isthenpropagatedintothesubsequentlogical
i
Specifically, this paper introduces an optimizable vector, layerforfurtherprocessing.However,ifthesubsequentlogical
denoted as V˜ ∈ Rd, for the embedding of logical inputs layer is the final layer, the global vector V is no longer
0 0
{V˜|i∈[1,n]V ∈Rd}.Thisvectorisakintotheclasslearning appended.
i i
vector in the Vision Transformer (ViT). After the logical in- In this way, we can construct the network according to
puts,augmentedwithalearnablevectorV˜ ,propagatethrough the formula, and the feedforward process of a single logical
0
a single Transformer-Encoder layer while applying a mask layerisillustratedinthefigure10.Itisworthmentioningthat
to the self-attention weights at the position corresponding maskingthepositioncorrespondingtoV intheattentioncoef-
0
to V˜ to prevent its value mapping from contributing to the ficientsreducestheimpactoftheoptimizableparameterV on
0 0
attention results of the Transformer-Encoder, they produce an theposemappingprocesstoacertainextent,makingitarela-
updated set of vectors {V |i ∈ [0,n],V ∈ Rd} representing tively necessary operation. our analysis demonstrates that this
i i
the attention-weighted outputs. Our aim is to accomplish stabilizestrainingandimprovesinferenceaccuracysomewhat.
two processes within a single self-attention computation: the This paper replaces the standard Transformer-Encoder+cross-
extra multi-head cross-attention between V˜ and {V˜|i ∈ attention+pose matrix mapping sequential process in pose-
0 i
[1,n]} , and the pre-existing multi-head self-attention among Transformer with the aforementioned procedure. And figure
{V˜|i ∈ [1,n]}. We replace the tedious sequential process 11 illustrate the structure of the Straw-Pose-Transformer.
i
with this masked attention mechanism, thereby streamlining In summary, our lightweighting strategy for Pose-
thecomputationalworkflow.Duetotheinherentself-attention Transformerinvolvestheinsertionofadditionalcross-attention
mechanismandmasking,thevectorV˜ assimilatesinformation computations between the original Transformer-Encoder cal-
0
from the logical inputs, transforming into global vector V . culations and the pose matrix mapping computations. While
0
Withinthisglobalvector,weestablishacollectionofmapping this approach reduces the number of parameters, it also
matrices {W˜ |j ∈ [1,m],k ∈ [1,n],W˜ ∈ R(d/m)×d} for elongates the training time, which is an undesirable trade-
jk jk
each attention head {H |j ∈ [1,m],H ∈ R(d/m)} in it. off. Therefore, in this study, we have designed a masked
0j 0j
It is evident that there exists a subtle difference between the Transformer-Encoder that obviates the need for explicitly
masked attention process and the sequential process of self- inserting cross-attention computation processes, aiming to
attention and cross-attention, which renders them not strictly achieve a more efficient and streamlined model.
equivalent. Conversely, the masked attention process is equiv-
alenttoaserialprocessofperformingcross-attentionfollowed VI. COMBINATIONOFPOSE-TRANSFORMERAND
byself-attention.However,ignoringthesequenceandmerging BASELINES
thetwoprocessesofattention-basedweightedsummationinto In addressing the Bongard-Logo problem, this paper intro-
one step is not an unreasonable approach. In fact, this merger ducesanovelbaseline,PMoC.TodemonstratetheadaptabilityJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 12
was discovered that compared to the Transformer-Encoder,
V0 the Pose-Transformer exhibits greater suitability for abstract
logic inputs Vglo sb tae ck mask Vi | i∈[0,n] sever Hij | i=0, j∈[1,m] reasoning problems, specifically the RPM and Bongard-Logo
pose problems.
matrixsquash
... ... ... ... ...
equal length
Ok | k∈[1,n]
VII. EXPERIMENT
add
tokens In this section, we conduct experiments on the model
H0j ... pose matrix outputs proposed in this paper.
short token head transformer layer
Wjk | j∈[1,m], k∈[1,n] long pose vector globe vector
A. Experiment on Bongard-Logo
We conducted experiments on the Bongard-logo dataset
Fig.10. ThefeedforwardprocessofasingleStrawposematrixembedding
block using the SBSD and PMoC model. The results are recorded
in Table I. Due to the serial convolution scanning mechanism
and attention mechanism being prone to attention collapse,
logic inputs
the parameters of the SBDC model are utilized as pre-trained
Vglobe
parametersforthePMoCconvolutionallayer.Inaddition,this
study experimented with the combination of the Straw pose-
masked
transformer transformerandPMoC.Itisworthnotingthat,undertheexper-
layer
sever imental conditions of this paper, which involved four A100s
pose graphics processing units (GPUs), the integration of PMoC
matrix
+ ×N-1 with the pose-transformer proved to be challenging to deploy.
Thisisoneofthereasonsandmotivationsbehindourdecision
to design a lightweight approach for Pose-Transformer in this
feedforward paper.
block
All our experiments are implemented in Python using the
+ + PyTorch [38] framework. In the experiments conducted for
PMoCandSBSD,theAdam[39]optimizerwasselectedwith
a learning rate set to 0.001, a learning rate decay of 0.995, a
×1
transformer weight decay of 0.0001, and a batch size of 40 for PMoC and
layer
SBSD. Specifically, in the experiment combining PMoC with
straw-pose-transformer, the batch size was set to 20.
logic outputs
Fig.11. Thearchitectureofthestrawpose-Transformer TABLEI
REASONINGACCURACIESOFDCONBONGARD-LOGO.
Accuracy(%)
of the Straw-Pose-Transformer in solving the Bongard-Logo
problem, the Transformer-Encoder component within PMoC Model Train FF BA CM NV
is replaced with the Straw-Pose-Transformer in this section. SNAIL 59.2 56.3 60.2 60.1 61.3
Furthermore, given the remarkable performance achieved by ProtoNet 73.3 64.6 72.4 62.4 65.4
the predecessor RS-Tran in solving the RPM problem, this MetaOptNet 75.9 60.3 71.6 65.9 67.5
paper does not design an additional baseline network to prove
ANIL 69.7 56.6 59.0 59.6 61.0
thesuperiorityofthePose-TransformerinaddressingtheRPM
Meta-Baseline-SC 75.4 66.3 73.3 63.5 63.9
problem. This paper aims to demonstrate the advantage of
Meta-Baseline-MoCo 81.2 65.9 72.2 63.9 64.7
Pose-Transformer over the standard Transformer-Encoder for
WReN-Bongard 78.7 50.1 50.9 53.8 54.3
RPM problems by achieving improved accuracy through the
simple replacement of the Transformer-Encoder in RS-Tran SBSD 83.7 75.2 91.5 71.0 74.1
with Pose-Transformer. PMoC 92.0 92.6 97.7 78.3 75.0
The RS-Tran [31] primarily consists of Transformer- PMoC+StrawPose-Transformer 96.2 96.0 98.5 78.5 75.3
Encoder structures, and in this study, we replace it with
the Pose-Transformer and Straw-Pose-Transformer and con- In Table I, it is evident that addressing the Bongard-Logo
duct experiments. The modified RS-Tran is renamed as RS- problemusingaprobabilisticmodelapproach,namelyPMoC,
Pose-Tran and RS-Straw-Pose-Tran. Due to the fundamental is both straightforward and effective. Furthermore, SBSD ex-
modifications made to the RS-Tran architecture, RS-Pose- hibits stronger outcomes when compared to previous models.
Tran and RS-Straw-Pose-Tran can be regarded as models Additionally, the integration of Straw-Pose-Transformer has
designedbasedonournovelstructure,incorporatinginductive contributed significantly to enhancing the inference accuracy
biases inherent in the RPM problem [37]. Subsequently, it of PMoC.
reyal
remrofsnat-esoP
kcolb
gniddebme
xirtam
esop
]n,1[∈k
,]m,1[∈j
| kjPJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 13
B. Experiment on RPM VIII. CONCLUSION
We conducted experiments on I-RAVEN and PGM by se- In this paper, we presented an empirical study on the
quentiallyreplacingtheTransformerstructureinRS-Tranwith Bongard-logo dataset using the PMoC model and conducted
Pose-Transformer and Straw-Pose-Transformer. The results of experiments on I-RAVEN and PGM by replacing the Trans-
I-RAVEN are recorded in Table II and PGM is recorded formerstructureinRS-TranwithPose-TransformerandStraw-
in Table III. In this paper, we considered that compared to Pose-Transformer, sequentially.
the standard Transformer-Encoder, a single layer of Pose- Our experimental results demonstrate that PMoC has
Transformer and its lightweight version possess twice the achieved competitive results on the Bongard-logo problem,
numberoflogicalcomputations.Therefore,thecombinedRS- successfully tackling it in the form of a probabilistic model.
Pose-Tran and RS-Straw-Pose-Tran only stacked half of the The two compromises made in the model design for im-
logical layers of RS-Tran to ensure that the internal logical plementing the PMoC are effective and reasonable. And the
computation times of the three models were equivalent. In results also show that the introduction of the pose mapping
addition, the parameter settings and dataset configurations matrix into the Transformer-Encoder is effective, achieving a
for the experiments related to RS-Pose-Tran were strictly significant breakthrough in the already remarkable accuracy
maintained consistent with those of RS-Tran. Such a setting of RS-Tran. These results suggest that Pose-Transformer is
renders the comparative results of the three models more more advantageous for abstract reasoning tasks compared
convincing. to the standard Transformer-Encoder, even though the latter
has exhibited strong capabilities in RPM and Bongard-logo
TABLEII problems. Furthermore, our lightweight approach has been
REASONINGACCURACIESONRAVENANDI-RAVEN. successful without significantly reducing the performance of
RS-Pose-Tran.
TestAccuracy(%)
Model Average Center 2×2Grid 3×3Grid L-R U-D O-IC O-IG In conclusion, our study highlights the effectiveness of
SAVIR-T[27] 94.0/98.1 97.8/99.5 94.7/98.1 83.8/93.8 97.8/99.6 98.2/99.1 97.6/99.5 88.0/97.2 incorporating pose mapping matrix into Transformer-Encoder
SCL[26],[27] 91.6/95.0 98.1/99.0 91.0/96.2 82.5/89.5 96.8/97.9 96.5/97.1 96.0/97.6 80.1/87.7
MRNet[21] 96.6/- -/- -/- -/- -/- -/- -/- -/- for improving abstract reasoning capabilities. The successful
RS-TRAN[31] 98.4/98.7 99.8/100.0 99.7/99.3 95.4/96.7 99.2/100.0 99.4/99.7 99.9/99.9 95.4/95.4
implementation of our lightweight approach without signifi-
RS-Pose-TRAN -/99.51 -/100.00 -/99.90 -/98.20 -/100.00 -/100.00 -/99.94 -/98.56
RS-Straw-Pose-TRAN -/99.44 -/100.0 -/99.81 -/98.12 -/100.00 -/100.00 -/99.97 -/98.22 cant performance reduction further strengthens the potential
of Pose-Transformer in addressing complex reasoning tasks.
In Table III, it can be seen that we did not conduct exper-
The ingenious design of PMoC avoids to directly constrain
iments on RS-Pose-Tran for PGM. This is because compared
the distribution of the latent representation of Bongard-Logo
to RS-Tran operating on the RAVEN dataset, the overall
images, as well as the rigid modeling of this distribution
dimension of the RS-Tran model doubles on PGM. However,
into a known form. Experimental results demonstrate that
such a doubling is prohibitive for RS-Pose-Tran, making it
such a design choice in PMoC is highly reasonable and
undeployable on our equipment and device. This motivated
effective. Furthermore, Pose-Transformer, leveraging PMoC,
us to lightweight the Pose-Transformer into the Straw-Pose-
has exhibited its superiority in addressing the Bongard-logo
Transformer in this paper.
problem.
TABLEIII
REASONINGACCURACIESONPGM.
REFERENCES
[1] Deng,J.,Dong,W.,Socher,R.,Li,L.J.,Li,K.,&Fei-Fei,L.Imagenet:A
Model TestAccuracy(%) large-scalehierarchicalimagedatabase.InIEEEConferenceonComputer
VisionandPatternRecognition,246-255(2009).
SAVIR-T[27] 91.2
[2] He, K., Zhang, X., Ren, S., & Sun, J. Deep Residual Learning for
SCL[26],[27] 88.9 ImageRecognition.InIEEEConferenceonComputerVisionandPattern
Recognition,770-778(2016).
MRNet[21] 94.5
[3] Krizhevsky, A., Sutskever, I., & Hinton, G. E. Imagenet classification
RS-CNN[31] 82.8 withdeepconvolutionalneuralnetworks.CommunicationsoftheACM,
60(6),84-90(2017).
RS-TRAN[31] 97.5
[4] Vaswani, A. et al. Attention is All You Need. In Advances in Neural
RS-Straw-Pose-TRAN 98.64 InformationProcessingSystems,(2017).
[5] Devlin,J.,Chang,M.W.,Lee,K.,&Toutanova,K.Bert:Pre-trainingof
Deep Bidirectional Transformers for Language Understanding. Preprint
From the perspective of experimental results, it is not athttps://arxiv.org/abs/1810.04805(2018).
difficult to find that our lightweight approach has been suc- [6] Brown, T. et al. Language Models are Few-shot Learners. In Advances
inNeuralInformationProcessingSystems,1877-1901(2020).
cessful without significantly reducing the performance of RS-
[7] Kingma,D.P.,&Welling,M.Auto-encodingvariationalbayes.Preprint
Pose-Tran. Furthermore, the introduction of the pose mapping athttps://arxiv.org/abs/1312.6114(2014).
matrix into the Transformer-Encoder is effective, significantly [8] Goodfellow, I. et al. Generative adversarial networks. Communications
oftheACM,63(11),139-144(2020).
improving the performance of RS-Tran. This demonstrates
[9] Ho, J., Jain, A., & Abbeel, P. Denoising diffusion probabilistic models.
that Pose-Transformer is more suitable for abstract reasoning In Advances in Neural Information Processing Systems, 33, 6840-6851
capabilities compared to the standard Transformer-Encoder, (2020).
[10] Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C. L.,
even though the standard Transformer-Encoder structure has
& Parikh, D. VQA: Visual question answering. In IEEE International
shownstrongadvantagesinRPMandBongard-logoproblems. ConferenceonComputerVision,2425-2433(2015).JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 14
[11] Johnson, J., Hariharan, B., Van Der Maaten, L., Fei-Fei, L., Lawrence [36] Shen,Jian,etal.”Wassersteindistanceguidedrepresentationlearningfor
Zitnick, C., & Girshick, R. Girshick. CLEVR: A Diagnostic Dataset domain adaptation.” Proceedings of the AAAI Conference on Artificial
forCompositionalLanguageandElementaryVisualReasoning.InIEEE Intelligence.Vol.32.No.1.2018.
Conference on Computer Vision and Pattern Recognition, 2901-2910 [37] Carpenter, P. A., Just, M. A., & Shell, P. What One Intelligence
(2017). Test Measures: a Theoretical Account of the Processing in the Raven
[12] Wang, X., Wei, J., Schuurmans, D., Le,Q., Chi, E., & Zhou, D. Self- ProgressiveMatricesTest.Psychologicalreview,97(3),404,(1990).
ConsistencyImprovesChainofThoughtReasoninginLanguageModels. [38] Paszke,A.etal.AutomaticDifferentiationinPytorch.InNIPSAutodiff
Preprintathttps://arxiv.org/abs/2203.11171(2022). Workshop,(2017).
[13] Drori, I. et al. A Neural Network Solves, Explains, and Generates [39] Kingma,D.P.,&Ba,J.Adam:AMethodforStochasticOptimization.
UniversityMathProblemsbyProgramSynthesisandFew-shotLearning Preprintathttps://arxiv.org/abs/1412.6980,(2014).
atHumanLevel.ProceedingsoftheNationalAcademyofSciences,119
(2022).
[14] Chen, M. et al. Evaluating Large Language Models Trained on Code.
Preprintathttps://arxiv.org/abs/2107.03374(2021).
[15] Raven J. C. Raven’s Progressive Matrices. (Western Psychological
Services,(1938).
[16] Depeweg, S., Rothkopf, C. A., & Ja¨kel, F. Solving Bongard Prob-
lems with a Visual Language and Pragmatic Reasoning. Preprint at
https://arxiv.org/abs/1804.04452(2018).
[17] Nie, W., Yu, Z., Mao, L., Patel, A. B., Zhu, Y., & Anandkumar, A.
Bongard-LOGO:ANewBenchmarkforHuman-LevelConceptLearning
andReasoning.InAdvancesinNeuralInformationProcessingSystems,
16468–16480(2020).
[18] Zhang, C., Gao, F., Jia, B., Zhu, Y., & Zhu, S. C. Raven: A Dataset
for Relational and Analogical Visual Reasoning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
5317–5327(2019).
[19] Barrett,D.,Hill,F.,Santoro,A.,Morcos,A.,&Lillicrap,T.Measuring
AbstractReasoninginNeuralNetworks.InInternationalConferenceon
MachineLearning,511-520(2018).
[20] Hu,S.,Ma,Y.,Liu,X.,Wei,Y.,&Bai,S.StratifiedRule-AwareNetwork
forAbstractVisualReasoning.InProceedingsoftheAAAIConference
onArtificialIntelligence,1567-1574(2021).
[21] Benny, Y., Pekar, N., & Wolf, L. Scale-Localized Abstract Reasoning.
In Proceedings of the IEEE/CVF Conference on Computer Vision and
PatternRecognition,12557-12565,(2021).
[22] Zhang, C., Jia, B., Gao, F., Zhu, Y., Lu, H., & Zhu, S. C. Learning
Perceptual Inference by Contrasting. In Proceedings of Advances in
NeuralInformationProcessingSystems,(2019).
[23] Zheng,K.,Zha,Z.J.,&Wei,W.AbstractReasoningwithDistracting
Features.InAdvancesinNeuralInformationProcessingSystems,(2019).
[24] Zhuo, T., & Kankanhalli, M. Effective Abstract Reasoning with Dual-
ContrastNetwork.InProceedingsofInternationalConferenceonLearn-
ingRepresentations,(2020).
[25] Zhuo, Tao and Huang, Qiang & Kankanhalli, Mohan. Unsupervised
abstract reasoning for raven’s problem matrices. IEEE Transactions on
ImageProcessing,8332–8341,(2021).
[26] Wu, Y., Dong, H., Grosse, R., & Ba, J. The Scattering Compositional
Learner: Discovering Objects, Attributes, Relationships in Analogical
Reasoning.Preprintathttps://arxiv.org/abs/2007.04212(2020).
[27] Sahu,P.,Basioti,K.,&Pavlovic,V.SAViR-T:SpatiallyAttentiveVisual
ReasoningwithTransformers.Preprintathttps://arxiv.org/abs/2206.09265
(2022).
[28] Zhang, C., Jia, B., Zhu, S. C., & Zhu, Y. Abstract Spatial-Temporal
ReasoningviaProbabilisticAbductionandExecution.InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,
9736-9746(2021).
[29] Zhang,C.,Xie,S.,Jia,B.,Wu,Y.N.,Zhu,S.C.,&Zhu,Y.Learning
Algebraic Representation for Systematic Generalization. In Proceedings
oftheEuropeanConferenceonComputerVision,(2022).
[30] Hersche, M., Zeqiri, M., Benini, L., Sebastian, A., & Rahimi, A.
A Neuro-vector-symbolic Architecture for Solving Raven’s Progressive
Matrices.Preprintathttps://arxiv.org/abs/2203.04571(2022).
[31] Q. Wei, D. Chen, B. Yuan, Multi-viewpoint and multi-evaluation with
felicitous inductive bias boost machine abstract reasoning ability, arXiv
:2210.14914,2022.
[32] S.Kharagorgiev,“Solvingbongardproblemswithdeeplearning,”
k10v.github.io,2020.
[33] Sabour, Sara, Nicholas Frosst, and Geoffrey E. Hinton. ”Dynamic
routing between capsules.” Advances in neural information processing
systems30(2017).
[34] Dosovitskiy,A.etal.AnImageisWorth16x16Words:Transformersfor
Image Recognition at Scale. Preprint at https://arxiv.org/abs/2010.11929
(2020).
[35] Cuturi,Marco.”Sinkhorndistances:Lightspeedcomputationofoptimal
transport.”Advancesinneuralinformationprocessingsystems26(2013).