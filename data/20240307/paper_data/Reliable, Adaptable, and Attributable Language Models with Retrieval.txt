Reliable, Adaptable, and Attributable Language Models with Retrieval
AkariAsai12 ZexuanZhong3 DanqiChen3 PangWeiKoh1
LukeZettlemoyer12 HannanehHajishirzi14 Wen-tauYih2
Abstract
Input: List the top five US states with the highest per-capita GDP, in order.
Parametric language models (LMs), which are Parametric LMs: Pre-trained on large-scale pre-training data
trainedonvastamountsofwebdata,exhibitre- Top five states are: 1Factual inaccuracies
markableflexibilityandcapability. However,they Input 1. D (Dis Ctr )i ct of Columbia 2Difficulty of verification
still face practical challenges such as hallucina- LM 2. New York 3Difficulty of data opt-out
tions,difficultyinadaptingtonewdatadistribu- Training 3 4. . M Caa ls ifs oa rnc ih au setts 4Expensive costs to adapt
corpus 5. Connecticut 5Large model size
tions,andalackofverifiability. Inthisposition
paper,weadvocateforretrieval-augmentedLMs Retrieval-augmented LMs: Incorporate data at inference
to replace parametric LMs as the next genera- Input Top five states are: 1Reduced factual errors
tionofLMs. Byincorporatinglarge-scaledatas- Retriever LM 1 2. . D NC ew York 2Better attributions
toresduringinference,retrieval-augmentedLMs 2022 per capita GDP: DC 3. Massachusett 3Flexible data opt-in/out canbemorereliable,adaptable,andattributable. ( W19 A2 (k 7) 4, N k)Y , C(7 A9 (k 7) 3, M k)A (77k), 4 5. . W Caa ls ifh oi rn ng iaton 4Adaptivity &
customizability
Despitetheirpotential,retrieval-augmentedLMs Datastore 5Parameter efficiency
haveyettobewidelyadoptedduetoseveralob-
stacles: specifically,currentretrieval-augmented Figure1. ParametricLMs(top)internalizelarge-scaletextdata
LMs struggle to leverage helpful text beyond in their parameters via massive pre-training, while retrieval-
knowledge-intensive tasks such as question an- augmentedLMs(bottom)incorporatetextretrievedfromamassive
swering, have limited interaction between re- datastoreattesttime.
trievalandLMcomponents,andlacktheinfras-
tructure for scaling. To address these, we pro-
posearoadmapfordevelopinggeneral-purpose
ficultyofverification(Bohnetetal.,2022),W3: difficulty
retrieval-augmentedLMs. Thisinvolvesarecon-
ofoptingoutcertainsequenceswithconcerns(Henderson
siderationofdatastoresandretrievers,theexplo-
etal.,2023),W4: computationallyexpensivecostsforadap-
ration of pipelines with improved retriever-LM
tations(Longpreetal.,2023),andW5: prohibitivelylarge
interaction,andsignificantinvestmentininfras-
modelsize(Kandpaletal.,2022a). Moreover,merelyscal-
tructureforefficienttrainingandinference.
ing up the model has been insufficient to overcome such
limitations (Mallen et al., 2023) or even exacerbates the
challenges(Carlinietal.,2021).
1.Introduction
Thispositionpaperadvocatesforretrieval-augmentedLMs
Largelanguagemodels(LMs)suchasGPT-4(Blacketal., tosupersedeparametricLMsasthenextgenerationofLMs
2022)haveshownimpressiveabilitiesinarangeofnatural (Figure1,bottom),addressingmanyoftheaforementioned
languageprocessing(NLP)tasks. SuchparametricLMs weaknesses.UnlikeparametricLMs—whichuselarge-scale
encapsulate rich natural language understanding abilities text data only during training—retrieval-augmented LMs
andawealthofworldknowledgeintheirparameters, ac- leverage an external large-scale collection of documents
quiredviamassivepre-trainingonlarge-scalewebcorpora (datastore) at inference by selecting relevant documents
(Figure1,top).However,theystillsufferfromseveralfunda- fromthedatastore(Asaietal.,2023a). Retrieval-augmented
mentalweaknessesincludingW1: theprevalenceoffactual LMscanW1: largelyreducefactualerrors(Mallenetal.,
errors(Minetal.,2023a;Mishraetal.,2024),W2: thedif- 2023),W2: providebetterattributions(Gaoetal.,2023a),
W3: enabling flexible opt-in and out of sequences (Min
1University of Washington 2Meta AI 3Princeton Univer-
etal.,2024). Byaddingorremovingdatafromtheirdatas-
sity 4Allen Institute for AI. Correspondence to: Akari Asai
tores,retrieval-augmentedLMscanW4:easilyadapttonew
<akari@cs.washington.edu>.
distributions(Khandelwaletal.,2020). Liftingtheburden
Preprint.Workinprogress. ofmemorizingeverythinginparametersmakesthemW5:
1
4202
raM
5
]LC.sc[
1v78130.3042:viXraReliable,Adaptable,andAttributableLanguageModelswithRetrieval
moreparameter-efficient(Izacardetal.,2023). havemanypracticallimitations,whichinturnposesignifi-
cantchallengestobuildingreliableintelligentsystems.
Despitetheirconsiderablepotentialtosignificantlyimprove
reliability, adaptability, and attributability, their broader
Definition. AparametricLM(Figure1,top)consistsof
adoptionbeyondspecificknowledge-intensivetasks(e.g.,
asetofparametersθ. Giveninputsequencesfromalarge-
questionansweringorQA;Chenetal.2017)iscurrently
scaletextdatasetD ,learnableparametersθaretrained
limited. Wearguethatthroughfundamentaladvancements train
topredicttheprobabilitiesoffutureormaskedtokens. Dur-
inarchitecture,trainingmethodologies,andinfrastructure
ingtesttime,foraninputsequencex,thetrainedθpredicts
for retrieval-augmented LMs, they can demonstrate sub-
theoutputs: y =f (x),withoutaccessinganyexternaldata
stantial efficacy across diverse domains. We urge the re- θ
beyondthatofthetaskathand.
searchcommunitytointensifyeffortsaimedatovercoming
those inherent limitations for their widespread adoption.
2.1.WeaknessesofParametricLMs
Tofacilitatefutureresearch,weidentifyseveralsignificant
challenges. First, existing approaches primarily leverage Mountingevidencehighlightssignificantlimitationsinpara-
context with high semantic or lexical similarity to the in- metricLMs. Manysuchchallengesarisefromthestrategy
put (C1), struggling when helpful text is absent in com- ofattemptingtostoreallknowledgewithintheparameters,
mon datastores or does not align with conventional rele- whichscalingalonemaynotadequatelyaddress.
vancedefinitions(BehnamGhaderetal.,2023;Asaietal.,
2023b). Second, prepending the retrieved text to the in- W1:Factualinaccuracies. Attemptingtomemorizeall
putofoff-the-shelfLMs, whichhasbeenwidelyusedre- the learned knowledge within the parameters can lead to
cently,leadstoshallowinteractionsbetweentheretrieval factualinaccuracies,whichareoftencalledhallucinations.
andLMcomponents(C2).Thisoftenresultsinunsupported Severalrecentpapersreportthatevenstate-of-the-artLMs
generations(Gaoetal.,2023a),susceptibilitytoirrelevant suchasChatGPTexhibithallucinationsinthemajorityof
text(Yoranetal.,2024),andchallengesinhandlinginfor- theiroutputs(Minetal.,2023a;Mishraetal.,2024).Mallen
mationfrommultiplepiecesoftext(Borgeaudetal.,2022). etal.(2023);Kandpaletal.(2022a)showthattheyparticu-
Furthermore,unlikerapidprogressforefficienttrainingand larlystrugglewithlong-tailknowledge—factualknowledge
inferenceofparametricLMs(Zhaoetal.,2023b;Daoetal., thatislessrepresentedduringpre-training—andthatscaling
2022),therearelimitedstudiesandopen-sourcedeffortsto onlyyieldsminorimprovements. Gudibandeetal.(2024)
enhancethetrainingandinferenceefficiencyofretrieval- findthatincreasingsyntheticlabeleddataduringinstruction
augmentedLMsatscale(C3). tuningmaynotimprovethefactualityofmodeloutputs.
Weconcludethispaperwitharoadmaptoadvanceretrieval-
W2: Difficulty of verifications. Not only have LMs
augmentedLMstofosterwideradoption. First,addressing
shownapropensityforhallucinationsintheirgenerations,
thechallengeoffindinghelpfultextfordiversetasks(C1),
butitisalsodifficultforpractitionerstofact-checktheirout-
it is important to reconsider the notion of relevance and
putsduetoalackofclearattributionsorprovenance. The
advance our understanding of what constitutes an effec-
outputsofpowerfulLMsareoftenlengthy,assertive,and
tive datastore—specifically, exploring the types of infor-
plausible(Minetal.,2023a),whichmakespost-hocattri-
mationthatshouldberetrievedfromvariousdatastoresto
butionsorfactualverificationtobechallengingandlargely
enhancetheperformanceinbroadertasks. Then,wesug-
unsolvedtasks(Mishraetal.,2024;Yueetal.,2023).
gestapproachestoensuredeeperinteractionsbetweenthe
twocomponents,includingarchitecture,pre-training,and
W3:Difficultyofoptingoutcertainsequencesfromthe
post-trainingadaptations(C2),ratherthanfocusingonsup-
datasets. Managingthevastvolumeofpre-trainingdata
plementaryenhancementofexistingparametricLMs. For
posesaconsiderablechallengeinidentifyingandfiltering
challengesofscaling(C3),wecallformoreopen-sourced
outtraininginstanceswithpotentialprivacy(Brownetal.,
andinterdisciplinaryeffortsacrosshardware,systems,and
2022) or copyright-protected data (Lee et al., 2024). Re-
algorithmstodevelopinfrastructuresfortrainingandinfer-
centworkstudiesintensiveredteamingandsafetytuning
ence(e.g.,scalingdatastoretotrilliontokens). Bypursuing
efforts(Touvronetal.,2023b;Perezetal.,2022),unlearn-
theseavenues,weanticipateunlockingthefullcapabilities
ing(Jangetal.,2023)oriterativepre-trainingofmodelson
of retrieval-augmented LMs and expanding their applica-
corporaafterremovingcertaindata(Kandpaletal.,2022b).
tionsacrossaspectrumoftasksanddomains.
Yet,theabsenceofproperattributionsfurthercomplicates
theseendeavors,astracingbacktoandeliminatingspecific
2.HowFarCanWeGowithParametricLMs? traininginstancesbecomesnon-trivial(Grosseetal.,2023).
WefirstassessthelimitationsofparametricLMs. Despite
W4:Computationallyexpensivecoststoadapt. Adapt-
rapidprogressinthisarea,wearguethatparametricLMs
ing parametric LMs trained on static unlabeled text (i.e.,
2Reliable,Adaptable,andAttributableLanguageModelswithRetrieval
text collected at a certain timestamp from the web) re- Origins, progress, and recent shift. The concept of
quires continuous training or computationally expensive retrievalaugmentationhasbeenextensivelyexploredacross
post-adaptationtonewdatadistributions. Forinstance,their various machine learning domains (Tian et al., 2019). In
parametricknowledgecanquicklybecomeobsolete(Long- NLP,earliereffortshavebeenappliedtospecifictaskssuch
preetal.,2023).Whileseveralapproachesproposetolocate asQAandmachinetranslation. Chenetal.(2017)introduce
andeditcertainoutdatedknowledge(DeCaoetal.,2021) DrQA,whichcombinesaterm-basedinformationretrieval
orconductefficientcontinuedtraining(Jinetal.,2022)to (IR)systemwithaneuralQAmodeltoanswerknowledge-
keepupwiththeworld,theseapproachesrequireadditional intensivequestions. WhileIRandsuchtaskLMswereini-
computationallyexpensivelearningprocesses. LMstrained tiallystudiedseparately,severalworkexploresmoreorganic
onwidelyadoptedpre-trainingcorporaoftenperformwell combinationsofretrievalandLMbypre-trainingthetwo
ongeneral-purposedomainssuchasnewsarticles(Dodge componentsjointlyorsequentially,includingREALM(Guu
etal.,2021),butstruggleonexpertdomains(Tayloretal., etal.,2020),RAG(Lewisetal.,2020a),RETRO(Borgeaud
2022). Priorworkdemonstratestheeffectivenessofcontin- etal.,2022),etc.
uedpre-training(Azerbayevetal.,2024;Chenetal.,2023b)
Suchearlierworkdesignedspecialarchitecturesandtraining
orinstructiontuning(Singhaletal.,2023),albeitataconsid-
objectivesfortheretrieval-augmentedLM.Mostrecently,
erablecomputationalcostandpossibilitiesofcatastrophic
therehasbeenashiftofviewofretrieval-augmentedLMs—
forgetting(Lietal.,2022).
insteadoftrainingretrieval-augmentedLMsfromscratch,
someworksupplementaryintegrateretrievalontopofex-
W5:Prohibitivelylargemodelsize. Numerousstudies
istingpowerfulparametricLMs(e.g.,GPT-3; Blacketal.
showcasethepositiveimpactofmodelscalingontaskper-
2022)withoutanyadditionaltraining.Suchmethods—often
formance(Chowdheryetal.,2022;Weietal.,2022),and
referredtosimplyasRetrieval-AugmentedGenerationor
theabilitytorecallfactualknowledgememorizedfromthe
RAG—concatenatetheoriginalinputsequencexwithre-
trainingdata(Carlinietal.,2023;Mallenetal.,2023;Kand-
trievedtextzwhenprompting,yieldingsignificantimprove-
paletal.,2022a). Thistrendhaspromptedthecommunity
mentsoverthebaseparametricLMsoncertainknowledge-
to focus on boosting the model size in pursuit of better
intensivetasks(Rametal.,2023;Shietal.,2023c). Many
performance,atthecostofsignificantcomputationalchal-
recentstudiesexploreadvancedpromptingmethodswith
lenges and environmental concerns (Strubell et al., 2019;
retrievalcomponents(Yaoetal.,2023;Pressetal.,2023)
Weidinger et al., 2022). Despite efforts to enhance effi-
ordeveloppipelinesforfurtherimprovements(Gaoetal.,
ciency,hostingthesemassivemodels,whichoftenexceeda
2023b). RAGhasbeenintegratedintoreal-worldapplica-
hundredbillionparameters,remainsimpracticalformany
tionssuchasLLMsearchsystems.3
industryoracademicgroups(Schwartzetal.,2019).
3.1.EffectivenessofRetrieval-AugmentedLMs
3.HowCanRetrieval-AugmentedLMs
AddressTheseIssues? Wenowreviewsomeempiricalfindingsfrompriorstudies
suggestingtheireffectivenessinaddressingtheweaknesses
Inthissection, wediscusshowretrieval-augmentedLMs ofparametricLMsdiscussedinSection2.1.
canalleviatetheaforementionedissuesinparametricLMs.
W1:Reducedfactualerrorsinlong-tailknowledge. Re-
Definition. Aretrieval-augmentedLM(Figure1,bottom;
centstudiesshowthatretrieval-augmentedLMscanallevi-
detailedinFigure2)typicallyconsistsoftwokeycompo-
atetheshortcomingsofparametricmemorizationbyexplic-
nents: aretrieverRandaparametricLMθ. Theretriever
itlycapturinglong-tailknowledge(Mallenetal.,2023). As
buildsasearchindexI1basedondocumentsinthedatastore
aresult,retrieval-augmentedLMscanminimizehallucina-
D. Duringinferencetime,givenaninputsequencex,the
tionsandimprovethefactualityofgeneratedoutputs(Lewis
retrieverfindsrelevanttextz2 fromtheinferencedatastore,
et al., 2020b; Izacard et al., 2023; Ram et al., 2023; Shi
leveraging an index I: z = f (x). Subsequently, the
R,I etal.,2023c;Asaietal.,2024;Minetal.,2023b).
LMθusesboththeoriginalpromptandtheretrievedtextto
predicttheoutputy: y =f (x,z). W2:Betterattributions. Retrieval-augmentedLMspro-
θ
vide retrieved results z used during inference, which can
1Interm-basedretrievalsystemssuchasBM25(Robertson&
helppractitionersinspectthecorrectnessofmodeloutputs
Zaragoza,2009)thatcounttheoccurrencesofwordsindocuments
manually(Liuetal.,2023)orautomatically(Mishraetal.,
inthedatastore,theindexI isaweightedbag-of-wordsvector,
while in more recent trainable neural retrieval systems such as 2024). Anotherwayforverificationispost-hocattribution—
DPR(Karpukhinetal.,2020),theindexisacollectionoffloat giventhemodeloutputy,retrievingdocumentsthatsupport
embeddingsencodedbyanencoderLM. y. Yet,priorworkfindsthatretrieval-augmentedLMsusing
2Therearedifferentgranularitiesforrelevanttextz(e.g.,text
chunks,tokens,phrases).SeeSection4.1.1formoredetails. 3https://bard.google.com/chat
3Reliable,Adaptable,andAttributableLanguageModelswithRetrieval
Input x What are the differences between Granularity of z Incorporation of z
llamas and alpacas?
Document Input augmentations
D1: The llama (Lama glama) Input x + D1 D2
is a domesticated South
American camelid LM θ
Retrieved
Intermediate fusion
Text D2: The alpaca is a species
of South American camelid Input x
Datastore Retriever z
mammal. It is similar to
D2
llama. D1
Frequency for retrieval
Output interpolation
Tokens, phrases
0 1 N T1: Llamas T2: The alpaca Input x
T3: The differences T1 T2 T3
Adaptive One time Every k token
Figure2. Taxonomyofarchitecturesofretrieval-augmentedLMs.
evidence during inference provide more accurate attribu- For example, on knowledge-intensive tasks such as QA,
tions than such post-hoc attributions (Gao et al., 2023a; retrieval-augmentedLMssurpassparametricLMswithor-
Malaviyaetal.,2023) dersofmagnitudemoreparametersbyalargemargin(Izac-
ardetal.,2023;Minetal.,2023b;Mallenetal.,2023).
W3: Enabling flexible opt-in of sequences. Retrieval-
augmentedLMsoffersomeeffectivesolutionstoconcerns 4.WhyHaven’tRetrieval-AugmentedLMs
relatedtomassivetrainingdatathroughimprovedattribu-
BeenWidelyAdopted?
tions and adaptable datastore updates. Enhanced attribu-
tionsenablepractitionerstoexcludespecificsequencesfrom Despiteshowingsomeempiricalpromise,theadoptionof
thedatastore,mitigatingtheriskofgeneratingthemverba- retrieval-augmentedLMsremainslimitedcomparedtopara-
tim (Carlini et al., 2021). Additionally, integrating datas- metric LMs. To understand the obstacles hindering the
toresduringinferenceonlystillallowsretrieval-augmented widespreadadoption,weprovideabriefreviewofexisting
LMstomaintainperformanceacrossdomainsnotincluded retrieval-augmentedLMsunderourunifiedtaxonomyfor
intheirtrainingdata(Minetal.,2024). architectures(Figure2),training,anddatastores,assumma-
rizedinTable1.
W4:Adaptabilityandcustomizability. Theseparation
and the interchangeability of knowledge sources for the 4.1.CurrentStateofRetrieval-AugmentedLMs
datastoreenablesbettercustomizationtospecificdomains,
4.1.1.ARCHITECTURE
applications, and time stamps, without the need for addi-
tionaltraining(Khandelwaletal.,2020;Minetal.,2024). Retrieval-augmentedLMshavediversearchitectures. Our
Recentworkhasshownthatretrievalaugmentationcaneven taxonomydefinesarchitecturebasedonthreeaxes(Table1
outperformLMsfine-tunedonthedownstreamdomaindata left): whattheunitofretrievedtextz is(granularityofz),
onQA(Ovadiaetal.,2023;Guptaetal.,2024). Suchef- howzisincorporated(incorporationofz),andhowoftenz
fectivenessfordomainadaptationhasalsobeenreportedin isretrieved(frequencyofretrieval).
non-knowledge-intensivetasks,includingmachinetransla-
Here,weclassifyapproachesbasedonhowtheyincorporate
tion(Shietal.,2022;Minetal.,2024;Khandelwaletal.,
theretrievedtextz (theIncorporationcolumninTable1),
2021;Zhongetal.,2022).Updatingthedatastorewithup-to-
Essentially,retrieval-augmentedLMs’architecturescanbe
dateknowledgealsobypassestheissueofdataobsoleteness
classifiedintothefollowingthreegroups:1)inputaugmen-
ofparametricLMs(Izacardetal.,2023;Zhongetal.,2023;
tation,2)intermediatefusion,and3)outputinterpola-
Mitchelletal.,2022;Kasaietal.,2023).
tion.RefertoFigure2forataxonomyofthesearchitectures.
Foramorecomprehensivereviewofthearchitecture,includ-
W5:Parameterefficiency. Byliftingtheburdenofmem-
ingaspectssuchasthegranularityofretrievalandretrieval
orizing all knowledge in the model parameters, retrieval-
frequency,refertoAppendixB.1. Inessence,inputaugmen-
augmentedLMsoftenshowstrongparameterefficiency—
tationandintermediatefusiontypicallyinvolveretrieving
retrieval-augmented LMs with much fewer LM parame-
textchunksandprocessingthemwithparametricLMs. On
terscanoutperformlarger,morepowerfulparametricLMs.
4Reliable,Adaptable,andAttributableLanguageModelswithRetrieval
Table1.Diverseretrieval-augmentedLMsbasedonourarchitectureandtrainingtaxonomies.Fullreferencesofthepapersareasfollows:
DrQA(Chenetal.,2017),REALM(Guuetal.,2020),RAG(Lewisetal.,2020b),ATLAS(Izacardetal.,2023),RALM(Rametal.,
2023), REPLUG (Shi et al., 2023c), Active Retriever (Jiang et al., 2023), Self-RAG (Asai et al., 2024), RETRO (Borgeaud et al.,
2022),InstructRetro(Wangetal.,2023a),kNNLM(Khandelwaletal.,2020),TRIME(Zhongetal.,2022),NPM(Minetal.,2023b),
CopyGenerator(Lanetal.,2023),SPALM(Yogatamaetal.,2021),AdaptivekNN(Drozdovetal.,2022).∗indicatesthatapproaches
combiningoff-the-shelfmodelswithoutanytask-specifictraining.
Granularity Incorporation Frequency Training Dataorder
DrQA Chunks Input One-time Independent O(109)
REALM,RAG,ATLAS Chunks Input One-time Joint O(109)
RALM,REPLUG Chunks Input Everyktokens,One-time Independent∗ O(109)
Active-Retriever,Self-RAG Chunks Input Adaptive Independent∗,Sequential O(109)
RETRO,InstructRetro Chunks Intermediate Everyktokens Sequential O(1012)
kNNLM,TRIME Tokens Output Everytoken Independent∗,Joint O(109)
NPM,CopyGenerator Phrases Output Everyphrase Joint O(109)
SPALM,AdaptivekNN Tokens Output Adaptive Independent∗,Joint O(109)
theotherhand,outputinterpolationdirectlyretrievessuc- from their vocabularies. In contrast, kNN LM (Khandel-
cessivetokensorphrases,resultinginamuchlargerindex. wal et al., 2020) interpolates a parametric LM token dis-
Unliketraditionalapproaches,whichinvolveretrievingonly tribution with a retrieved token distribution, without the
once(One-time)suchasDRQA,recentstudieshavehigh- need for additional training. Some work extends this di-
lightedtheeffectivenessofretrievaloverspecifictokeninter- rectionbydesigningnewtrainingobjectives(Zhongetal.,
vals(Everyktokens; Rametal.2023)oradaptively(Asai 2022)orcompletelyreplacingparametricdistributionswith
etal.,2024;Jiangetal.,2023;Drozdovetal.,2022). anon-parametricdistributionovereachphraseinthedata-
store (Min et al., 2023b; Lan et al., 2023).b While these
Input augmentation. Input augmentation augments the
approachesfrequentlydemonstratetheirefficacycompared
originalinputxwithretrievedresultsz intheinputspace
to input augmentation in language modeling (Min et al.,
oftheLMθandrunsastandardLMinference. Asinthepi-
2024),theyrequireaconsiderablylargerindexthantheother
oneeringworkfromChenetal.(2017),inputaugmentation
twoarchitectures. Thisisduetothenecessityofgenerat-
enables flexible plug-ins of different models for retrieval
ingembeddingsforalltokensinthedatastore,presenting
and LM components. Many widely adopted models, in-
scalabilitychallenges.
cludingthosethataugmentpowerfulLMswithoff-the-shelf
retrievers,mostlybelonginthiscategory(Yaoetal.,2023; 4.1.2.TRAINING
Shietal.,2024). Onenotablebottlenecktothisapproach
isredundancyandinefficiency;encodingmanydocuments Retrieval-augmented LMs consist of three main compo-
togetherintheinputspaceleadstocontextlengthwindow nents: theindexI,theretrieverR(i.e.,amodelthatgener-
limitationsandincreasesinferencecostsexponentially(Xu atesencodingofinputanddocuments),andtheLMθ. How
etal.,2024). WhilesomeworksuchasFiD(Izacardetal., toefficientlyandsimultaneouslyupdatethemtooptimize
2023)exploresparallelencodingtoovercomesuchineffi- the whole pipeline remains a challenging question. Cur-
ciencies,itstillencodesrepeatedlyforeachinputx. rently,therearetwoparadigms: independentorsequential
trainingandjointtraining(Table1Training).
Intermediate fusion. To integrate retrieved results in a
morescalablemanner,RETRO(Borgeaudetal.,2022)in- Independentorsequentialtraining. Independenttrain-
troduces a new attention mechanism, which takes many inginvolvestheseparatedevelopmentofaretrieverandLM
pre-encoded text chunks independent of query x and si- with no direct interactions during training. This includes
multaneously incorporates them in intermediate spaces. methodssuchaskNNLM,orrecently,RAGappliedtooff-
RETRO++(Wangetal.,2023b)andInstructRetro(Wang the-shelfLMsandretrievalsystems. Thisallowspractition-
etal.,2023a)demonstratetheeffectivenessofthismethod erstoleverageexistingtrainingpipelinesandobjectivesto
ontopoflarger,decoder-onlyLMs. However,adrawback enhancetheindividualcomponents. Therehasbeenrichlit-
ofintermediatefusionistheneedforextensivearchitecture eratureintheareaofIRonhowtobuildreliableandefficient
modificationandpre-trainingofLMsforthenewencoding IR systems. Classical term-based retrieval systems, such
blocks,potentiallylimitingwidespreadadoption. asTF-IDForBM25(Robertson&Zaragoza,2009),have
beenwidelyused. Morerecently,neuralretrievalsystems,
Outputinterpolation. Bothinputaugmentationandin- suchasDPR(Karpukhinetal.,2020)orColBERT(Khat-
termediatefusionrequiretheLMtogeneratecontinuations tab & Zaharia, 2020), have shown superior performance.
5Reliable,Adaptable,andAttributableLanguageModelswithRetrieval
Extensivepre-trainingofretrievalmodelsfurtherimproved ations (He et al., 2023; Shi et al., 2023b), or knowledge
suchmodels(Izacardetal.,2022;Nietal.,2022;Linetal., editingcapabilities(Zhongetal.,2023). Afurtheroverview
2023). Foracomprehensivereviewofretrievalsystems,we ofapplicationswithdetailsofadaptationmethodologiesis
directreaderstopriorsurveys(Zhaoetal.,2023a). inAppendixB.2.
Yet,independenttrainingisoftensub-optimalforthewhole
Datastores. Designing and building a reliable datastore
retrieval-augmentedLMpipeline;forinstance,LMstrained
isakeychallengeofretrieval-augmentedLMs. Theinfer-
withoutretrievalcouldbecomeeasilydistractedbyirrele-
encedatastoreDmaynotbenecessarilyequivalenttothe
vantprecedingcontext(Shietal.,2023a). Toalleviatethis
trainingdatastoreandistask-dependent. Someworks,such
issue,sequentialtrainingtrainseithertheretrieverorLM
asNPM(Minetal.,2023b), leveragethesamecorpusas
first, and then trains the other subsequently using signals
thetrainingdataD =D onmoregeneraltasks,while
from the first trained component. Many studies train the train
forcertaindownstreamtasks,asmallerandgeneral-domain
LMcomponentwithapowerfulpre-trainedretrievere.g.,
corpusisoftenused(e.g.,Wikipedia). Conversely,curating
DPR,searchengines,orfrozenpre-trainedencoders(Izac-
high-quality,domain-focusedcorporaisimportantforsome
ard&Grave,2021a;Nakanoetal.,2021;Borgeaudetal.,
tasks,e.g.,codegeneration(Hayatietal.,2018;Zhouetal.,
2022),orconversely,traintheretrieverwithsignalsfrom
2023). AsTable1shows,mostpriorworkuseadatastore
theLM(Shietal.,2023c;Izacard&Grave,2021b).
thatisontheorderofO(109)tokens,withexamplessuch
asWikipediacontainingroughlyafewbilliontokens. No-
Jointtraining. JointtrainingsimultaneouslytrainstheLM
tably,Wangetal.(2023a);Borgeaudetal.(2022)scalethe
andretrievalcomponentstofurtheroptimizetheirinterac-
datastore to over one trillion tokens, showcasing a large
tionsandtheend-to-endretrieval-augmentedLMpipeline.
perplexityreduction.
Anotablechallengeinjointtrainingisthesubstantialcom-
putationaloverheadincurredbyupdatingboththeretriever
4.2.LimitationsofCurrentRetrieval-AugmentedLMs
model and the resulting index during training. It is im-
practical to repeatedly generate embeddings for millions Wenextidentifyseveralcorechallengesinherenttoexisting
orbillionsofdocumentsinthedatastoreateachtimestep. retrieval-augmentedLMs,assummarizedinTable2.
Therearetwoapproachestoachievethisunderreasonable
resourcerequirements: updatingthedatastorewithupdated
C1:Limitationsofretrieversanddatastores. Despite
parameters asynchronously or using an in-batch approxi-
the success of retrieval-augmented LMs on knowledge-
mation to a full datastore. Asynchronous updating is a
intensive tasks, their broader applications often result in
techniquethatallowstheindextogrowstaleoverafixed
restrictedsuccess. Forexample,retrieval-augmentedLMs
numberoftrainingstepsbeforetheupdate,aimingtouse
onlyyieldmarginalgainsonreasoningtasks,whichcanbe
thefullcorpusduringtraining(Izacardetal.,2023),asin
attributedtoweaknessesinboththeretrievalandLMcom-
inference time. There is a tradeoff between the update
ponents(BehnamGhaderetal.,2023;Linetal.,2024). We
frequencyandcomputationaloverhead(Guuetal.,2020):
hypothesizethatthisstemsfromamisalignmentbetween
toobtainbetterperformance,theindexshouldbeupdated
conventionalretrievalandLMtrainingobjectives,aswellas
morefrequently. In-batchapproximationbuildsatempo-
theuseddatastore.Consideransweringafactualknowledge-
raryindexontheflyusingtrainingsamplesfromthesame
basedquestion: aretrievercanefficientlysearchdocuments
mini-batch, which serves as an approximation to the full
akintoaqueryinWikipedia,andanLMcansubsequently
index during training (Zhong et al., 2022; de Jong et al.,
copyorparaphrasetheretrievedinformation. However,the
2022;Minetal.,2023b;Lanetal.,2023). Designingtrain-
typesofbeneficialtextvarysignificantlybasedonthetask.
ingbatchesthatcanprovidestrongtrainingsignalsrequires
Existingretrievalsystemsevaluatetherelevanceofdocu-
carefulconsideration.
mentsprimarilybyassessingtheirhighlexicalorsemantic
similarities to the input. Yet, such “relevant” documents
4.1.3.APPLICATIONSANDDATASTORES
often do not help tasks in reasoning or general language
understanding(Rubinetal.,2022). Itisstillunclearwhat
Applications. Retrieval-augmentedLMshaveprovenef-
makescertainretrievedcontextsmoreeffectivethanothers.
fectiveinvariousNLPtasks. Notably,theirimpactismore
The heavy dependence on Wikipedia as a datastore (Sec-
pronouncedonknowledge-intensivetasks(Guuetal.,2020;
tion4.1.3)couldalsolimititseffectiveness,asreal-world
Lewisetal.,2020a;Izacardetal.,2023). Severalstudies
applicationsfrequentlyencounterqueriesthatmaynotfind
showcasetheirefficacyinmachinetranslation(Khandelwal
directanswersinWikipedia(Asai&Choi,2021).
etal.,2020;Guetal.,2018)aswellasbroaderlanguageun-
derstandingtasks(Minetal.,2023b;Shietal.,2022). There
C2:LimitedinteractionsbetweenretrieversandLMs.
arealsodecodingmethodsthatleveragepost-hocretrieval
Commonapproaches,suchasRAG,oftenstraightforwardly
augmentationstoproducemoreefficientorfactualgener-
entailappendingretrievedresultstotheinputofpre-trained
6Reliable,Adaptable,andAttributableLanguageModelswithRetrieval
Table2. Currentstatusofretrieval-augmentedLMsandfuturedirections.
CurrentStateofRetrieval-AugmentedLMs(§4) RoadmaptoAdvanceRetrieval-AugmentedLMs(§5)
C1:UsageofR ✗Semanticandlexicalsimilarityonly ✓Beyondsemanticandlexicalsimilarity
andD ✗Singleandgeneral-domaincorpora ✓Datastoresforwiderapplications
C2:Interaction ✗Limitedinteractionsbeyondinputaugmentations ✓ArchitectureswithdeepLM-retrieverinteractions
ofRandθ ✗Lackofjointoptimizationfromtheenduse ✓Large-scalejointtrainingtechniques
C3: Infrastruc- ✗LackofstandardizedlibrariesbeyondRAG ✓Standardizedandopen-sourcedlibraryforretrieval-
turesforscaling basedLMs
&adoptions ✗Difficultyinlarge-scaletrainingandinference ✓Infrastructureforlarge-scaletrainingandinference
parametric LMs and adopting input augmentation (Sec- etal.,2022). Yet,nearestneighborsearchesoverbillions
tion4.1.1),duetoitssimplicityandeffectivenessbylever- ofembeddingswithoutextensivetrickscanconsumehun-
aging state-of-the-art parametric LMs. However, these dredsofGPUsorprohibitivelyhighRAMusage. Scaling
methodslackcloseinteractionsbetweentheretrievaland coststhushinderprioreffortstouselargerdatastores(Sec-
LM components throughout both training and inference. tion4.1.3).
Thisdeficiencyamplifiesissuessuchasunsupportedgen-
erations (Gao et al., 2023a) or susceptibility to irrelevant
5.HowCanWeFurtherAdvance
context,asnotedinYoranetal.(2024);Shietal.(2023a).
Retrieval-AugmentedLMs?
Moreover,inputaugmentationincreasesthecontextlength
of LMs, leading to an exponential increase in inference Webelievethatthecommunityneedstodeveloprobustintel-
costs (Xu et al., 2024). This becomes particularly prob- ligentsystemsbasedonretrieval-augmentedLMsthatsur-
lematicwhendownstreamapplicationsrequiresystemsto passfullyparametricLMs. Here,wepresentaroadmapto
assimilateinformationfrommultipledocuments(Fanetal., overcomethetechnicalconstraintsassociatedwithretrieval-
2019). ExtendedcontextcanalsoinduceLMstooverlook augmentedLMsdiscussedinSection4.2.
significantportionsoftheinput(Liuetal.,2023).
5.1.RethinkingRetrievalandtheDatastore(C1)
C3: Lack of infrastructure specialized for retrieval-
based LMs. Relative to parametric LMs, the optimiza- Beyond semantic and lexical similarity. Extending
tion of retrieval-augmented LM training procedures has theuseofretrieval-augmentedLMsbeyondconventional
beencomparativelyunder-studied,frombothmethodolog- knowledge-centrictasksnecessitatestheformulationofa
ical and infrastructural standpoints. For instance, open- newdefinitionfor“relevance”oftheinputqueryanddocu-
sourcedsoftwaresuchasPyTorchFSDP4 orDeepSpeed5 mentsinthedatastore.Thisisessentialforexcellingintasks
enable resource-efficient parametric LM pre-training via asinthosetasksinformativetextmaynotexhibitsemantic
techniquessuchasFullyShardedDataParallelism(Zhao orlexicalsimilaritytotheinputquery. Recentworksshow
etal.,2023b)orZeroRedundancyOptimizers(Rasleyetal., thatfew-shotin-contextlearningdemonstrations(Suetal.,
2020), respectively. While retrieval-augmented LMs can 2023a)orevenunlabeledtext(Lyuetal.,2023)couldboost
certainly leverage improvements made to their paramet- modelperformanceonreasoningorlanguageunderstand-
riccomponents,whatremainslackingarefocusedefforts ingtasks. Yet,whatmakescertaindocumentshelpful(e.g.,
thataddresschallengesuniquetoretrieval-augmentedLMs. underlyingreasoningpatterns,orwritingstyle)remainsan
Synchronouslyupdatinglarge-scaleindexesduringtraining openquestion. Acquiringabetterunderstandingofthechar-
introducessignificantcomputationaloverhead,andhowto acteristicsofhelpfuldocumentscouldunlockthepotential
efficiently update the index under normal computational ofretrieval-augmentedLMs. Furthermore,builtuponsuch
environmentsremainschallenging(Section4.1.2). understanding,weshouldbuildretrievalsystemscapableof
contextualizedretrieval,ratherthanbuildingtask-specificre-
Inference in retrieval-augmented LMs can also be sig-
trievalpipelines:developingaversatileretrieverthatadjusts
nificantly more expensive than in standard parametric
itssearchbehaviorbasedondiversenotionsofsimilarity
LMs (Mallen et al., 2023), especially if the datastore is
withadditionalinput. Forinstance, instruction-tunedre-
large(e.g.,overonetrilliontokens). Asscalingpre-training
trievers(Asaietal.,2023b;Suetal.,2023b)exemplifythis
dataleadstobetterparametricLMs,somestudiesempiri-
direction.
callyshowthatscalingthedatastoresispromising(Borgeaud
Reconsideringandimprovingthedatastore. Whenit
4https://pytorch.org/docs/stable/fsdp.
html comestowider,generaldownstreamapplications,orcon-
5https://github.com/microsoft/DeepSpeed verselymoreexpert-domaintasks,over-relianceonasingle,
7Reliable,Adaptable,andAttributableLanguageModelswithRetrieval
general-domaincorpussuchasWikipediamayhinderthe Further adaptation after pre-training. Significant ar-
capability of retrieval-augmented LMs. As discussed in chitecture modification or pre-training are efforts that re-
Section 4.1.3, the curation and composition of the datas- quire massive computing. One promising avenue under
toresignificantlyimpactthefinalperformance. Yet,many resource-constrainedenvironmentsistoexploreadaptations
openquestionsexistregardinghowtobuildandensurehigh- ofretrieval-augmentedLMsafterpre-training. Forinstance,
quality and effective datastores. For instance, should we despite the rapid developments of versatile instruction-
introduceaqualityfiltertothedocumentsinthedatastore, followingparametricLMs,theexplorationofinstruction-
ascommonpracticeinpre-trainingdataprocessing(Black followingretrieval-augmentedLMs(Linetal.,2024;Luo
etal.,2022)? Howshouldwebalancemultipledomainsina et al., 2023; Asai et al., 2024) or RLHF for retrieval-
datastore(Shaoetal.,2023)? Despitetheabundanceofliter- augmented LMs (Nakano et al., 2021; Bohnet et al.,
atureonwhatconstitutesgoodLMpre-trainingdata(Long- 2022)remainscomparativelyscarce. Augmentingexisting
preetal.,2023),therehavebeenlimitedexplorationssofar instruction-tunedLMstrainedwithoutretrievalcanoften
onwhatdataoughttogointothedatastore. causesuboptimalperformanceastheLMsarenotexplicitly
trainedtousetheretrievedcontext. Furtherinvestigationfor
5.2.EnhancingRetriever-LMInteractions(C2) betterpost-hocadaptationrecipes(e.g.,instruction-tuning,
RLHF)forretrieval-augmentedLMsmayunleashtheiref-
Newarchitecturesbeyondinputaugmentation. Asdis-
fectivenessacrossdiversedownstreamadaptations. Recent
cussed,theinputaugmentationofpowerfulLMs(e.g.,RAG)
studiesdemonstratethepromiseofincorporatingadditional
comeswithseverallimitationsthatcouldbeaddressedby
componentstofilteroutirrelevantcontext(Xuetal.,2024;
morespecialized,integratedarchitectures,suchasoutput
Yoranetal.,2024)orinstructinganLMtolearntodistin-
interpolation or intermediate fusion. While recent work
guish(Asaietal.,2024). Exploringenhancedpipelinesor
showsthesuccessofnewarchitectures(Wangetal.,2023b;
inference-timealgorithmscouldfurtherimprovereliability.
Minetal.,2023b;Lanetal.,2023),comparedtomassively
pre-trainedparametricLMs,theirtrainingandmodelsize
Efficient end-to-end training of retrieval-augmented
areoftensmaller,duetohighcomputationalcostsforpre-
LMs. Retrievalerrorsoftenstandoutasprominentissues
training. Furthermore, approaches that employ a smaller
in retrieval-augmented LMs (Asai & Choi, 2021; Yoran
granularityofretrieval(e.g., tokenlevelinSection4.1.1)
etal.,2024). RatherthanfocusingonoptimizingtheLM
posesignificantchallengesforscaling. Weurgecollabora-
componentinisolation,itiscrucialtojointlyoptimizethe
tiveeffortsforscalable,effectivearchitecturedesignsand
retriever component. Some tasks have demonstrated suc-
pre-training—Whilepre-trainingretrieval-augmentedLMs
cessinupdatingonlytheinputencodingcomponentwithout
iscomputationallyexpensive,wehopethatwecanaddress
modifyingtheindexafterpre-training(Izacardetal.,2023;
thatchallengethroughcollaborativemulti-institutionefforts,
Linetal.,2024). Anotheralternativestrategyinvolvesin-
asinseveralsuccessfulparametricLMpre-training(Work-
troducingadditionalcomponents,suchasrerankingmodels,
shopetal.,2022;Groeneveldetal.,2024). Recently,Muen-
andtrainingtheminanend-to-endfashionwithLMs. In
nighoffetal.(2024)haveintroducedgenerativerepresen-
manydownstreamtasks,nosupervisedlabelsareavailable
tationalinstructiontuningtotrainasinglemodelforboth
totrainretrievalsystems. Studyingeffectivetrainingstrate-
retrievalandgenerativetasks,whichallowsforsignificantly
gieswithoutsupervisiononthelatentvariableforpositively
reducingthelatencyofRAGbycachingrepresentations.
retrieved context (Lee et al., 2019; Singh et al., 2021) is
essential for enabling the training of retrieval-augmented
IncorporatingretrievalduringLMpre-training. Off- LMsforabroaderrangeofapplications.
the-shelfparametricLMstrainedwithoutretrievalcompo-
5.3.BuildingBetterSystemsandInfrastructuresfor
nentsoftenstrugglewithleveragingadditionalcontext(Shi
ScalingandAdaptation(C3)
etal.,2023a). Pre-trainingLMswithretrievalhasprovento
beeffective (Guuetal.,2020;Lewisetal.,2020a;Izacard Scalable search for massive-scale datastores. We be-
etal.,2023),butoftenrequiressignificantadditionaltrain- lievesignificanteffortsandexpertisefrominterdisciplinary
ingcosts,ornon-trivialmodificationstothestandardLM areas,includingsystemsandalgorithms,willenablepracti-
architecture. Recently,Shietal.(2024)showsthatretriev- tionerstoleveragelarge-scaledatasets. Forinstance,explor-
ingsimilartextchunksandreorderingpre-trainingcorpora ingcompressionandquantizationalgorithmsforbillionsof
canenhanceLMs’abilitiestoreasonoverlongsequencesor textembeddingsisanimportantarea(Douzeetal.,2024),
performretrievalaugmentationfordiversetasks. Theseim- aswellasfasternearestneighborsearchalgorithms(Wang
provementsdonotrequirethemodificationofpre-training etal.,2021). Open-sourcedtoolkitssuchasFAISS(John-
pipelinesormodelarchitectures. Assuch,theexploration sonetal.,2017)couldacceleratesuchprogress. Another
ofmethodstoinduceLMstoleverageretrievedcontextwith bottleneckofdatastore-scalingisthestoragerequirements
minimalornoadditionalcostsremainspromising. formillionsorbillionsofencodeddocuments,andhowto
8Reliable,Adaptable,andAttributableLanguageModelswithRetrieval
efficientlyloadthemduringinference. Somerecentworks more,retrieval-augmentedLMsmaynotcompletelyaddress
proposetosignificantlyreducetheindexsizebystoringthe issuessuchashallucinations.
index as binary vectors (Yamada et al., 2021; Cao et al.,
2024). Besidesalgorithmicimprovementsandsystemde-
Acknowledgements
velopment,anotherpromisingavenueisthedevelopmentof
specializedhardwareforretrieval-augmentedLMs. Com- WeexpressourgratitudetoJacquelineHeforhermeticulous
pared to parametric LMs, retrieval-augmented LMs may proofreadingandforprovidingmanywritingsuggestionsfor
require fewer GPUs, while it is often CPU-heavy and re- thedrafts. WethankRulinShao,WeijiaShi,DanFriedman,
quires fast access to the datastore. Collaborative efforts TanyaGoyal, HowardYen, DhrubaGhosh, JiachengLiu,
fromhardware,systems,andalgorithmstoLMapplications andNiklasMuennighofffortheirinsightfulfeedbackonour
couldhelpustacklethesechallengingproblems. draft,andSewonMinforfruitfuldiscussionsintheearly
stages. ThisworkwasfundedinpartbyNSFIIS-2044660
Standardizationandopen-sourcedevelopments. There
andIIS-2239290andgiftsfromAI2.
areseveralrepositoriessuchasLangChain,6LlamaIndex,7
andDSPy(Khattabetal.,2024)8 thatenablepractitioners
tobuildRAGontopofexistingretrievers,parametricLMs, References
anduser-provideddatastores. Yet,westilllackastandard-
Asai,A.andChoi,E. Challengesininformation-seeking
izedimplementationofretrieval-augmentedLMpipelines
QA:Unanswerablequestionsandparagraphretrieval. In
andevaluationbenchmarksthatcanflexiblyaccommodatea
Proceedingsofthe59thAnnualMeetingoftheAssocia-
rangeofarchitecturesandtrainingconfigurations(Sections
tionforComputationalLinguisticsandthe11thInterna-
4.1.1and4.1.2)beyondRAG.Asopen-sourcedeffortshave
tionalJointConferenceonNaturalLanguageProcessing,
facilitatedtherapidprogressofparametricLMs,weurgethe
2021. URLhttps://aclanthology.org/2021.
communitytosimilarlybuildastandardizedopen-source
acl-long.118.
implementationforretrieval-augmentedLMs.
Asai, A., Yu, X., Kasai, J., and Hajishirzi, H. One
6.Conclusion question answering model for many languages
with cross-lingual dense passage retrieval. In Ad-
Thispaperadvocatesforretrieval-augmentedLMsasthe vances in Neural Information Processing Systems,
nextgenerationofLMstobuildmorereliable, adaptable, 2021. URL https://proceedings.neurips.
andattributableintelligentsystems. Despitetheirnotable cc/paper_files/paper/2021/file/
advantages over parametric LMs, their adoption remains 3df07fdae1ab273a967aaa1d355b8bb6-Paper.
limited. Thislimitationmaybeattributedtothefocusona pdf.
narrowformofretrievalaugmentation,whichsimplycom-
binesexitingretrievalmodelsandLMsinpost-hocmanners Asai, A., Min, S., Zhong, Z., and Chen, D. Retrieval-
tosupplementparametricLMs. Weoutlinearoadmapfor based language models and applications. In Pro-
fundamentallyadvancingretrieval-augmentedLMsinterms ceedings of the 61st Annual Meeting of the As-
ofarchitectures,trainingmethodologies,andinfrastructure. sociation for Computational Linguistics (Tutorial),
We emphasize the importance of collaborative interdisci- 2023a. URL https://aclanthology.org/
plinaryeffortstoachievetheseadvancements.
2023.acl-tutorials.6.
Asai, A., Schick, T., Lewis, P., Chen, X., Izacard,
ImpactStatements G., Riedel, S., Hajishirzi, H., and Yih, W.-t. Task-
aware retrieval with instructions. In Findings of
Webelievetheadoptionofretrieval-augmentedLMscould
the Association for Computational Linguistics: ACL
addressthosefundamentallimitationsinherenttoparametric
2023,2023b. URLhttps://aclanthology.org/
LMs. Wehopethatthispositionpaperwillinspirefurther
2023.findings-acl.225.
explorationintheseareas,andcollaborativelyfosterthead-
vancementofretrieval-augmentedLMs. However,concerns Asai,A.,Wu,Z.,Wang,Y.,Sil,A.,andHajishirzi,H. Self-
mayarise. Theeffectivenessofretrieval-augmentedLMs RAG:Learningtoretrieve,generate,andcritiquethrough
intasksbeyondknowledge-intensivedomainsremainsan self-reflection. InTheTwelfthInternationalConference
openquestion,necessitatingthoroughassessments. Further- on Learning Representations, 2024. URL https://
openreview.net/forum?id=hSyW5go0v8.
6https://python.langchain.com/docs/get_
started/introduction
Azerbayev, Z., Schoelkopf, H., Paster, K., Santos, M.D.,
7https://www.llamaindex.ai/
McAleer, S., Jiang, A. Q., Deng, J., Biderman, S.,
8https://github.com/stanfordnlp/dspy
and Welleck, S. Llemma: An open language model
9Reliable,Adaptable,andAttributableLanguageModelswithRetrieval
for mathematics. In The Twelfth International Confer- C., McCandlish, S., Radford, A., Sutskever, I., and
enceonLearningRepresentations,2024. URLhttps: Amodei,D. Languagemodelsarefew-shotlearners. In
//openreview.net/forum?id=4WnqRR915j. Advances in Neural Information Processing Systems,
2020. URL https://proceedings.neurips.
BehnamGhader,P.,Miret,S.,andReddy,S. Canretriever- cc/paper_files/paper/2020/file/
augmented language models reason? the blame game 1457c0d6bfcb4967418bfb8ac142f64a-Paper.
between the retriever and the language model. In pdf.
Bouamor, H., Pino, J., and Bali, K. (eds.), Find-
ings of the Association for Computational Linguistics: Cao,Q.,Min,S.,Wang,Y.,andHajishirzi,H. BTR:Binary
EMNLP 2023. Association for Computational Linguis- token representations for efficient retrieval augmented
tics, 2023. URL https://aclanthology.org/ languagemodels. InTheTwelfthInternationalConfer-
2023.findings-emnlp.1036. enceonLearningRepresentations,2024. URLhttps:
//openreview.net/forum?id=3TO3TtnOFl.
Black, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,
L.,Golding,L.,He,H.,Leahy,C.,McDonell,K.,Phang, Carlini,N.,Tramer,F.,Wallace,E.,Jagielski,M.,Herbert-
J.,Pieler,M.,Prashanth,U.S.,Purohit,S.,Reynolds,L., Voss, A., Lee, K., Roberts, A., Brown, T., Song, D.,
Tow, J., Wang, B., andWeinbach, S. GPT-NeoX-20B: Erlingsson, U., et al. Extracting training data from
Anopen-sourceautoregressivelanguagemodel. InFan, largelanguagemodels. In30thUSENIXSecuritySym-
A.,Ilic,S.,Wolf,T.,andGalle´,M.(eds.),Proceedings posium, 2021. URL https://arxiv.org/abs/
of BigScience Episode #5 – Workshop on Challenges 2012.07805.
& Perspectives in Creating Large Language Models,
Carlini,N.,Ippolito,D.,Jagielski,M.,Lee,K.,Tramer,F.,
2022. URLhttps://aclanthology.org/2022.
andZhang,C. Quantifyingmemorizationacrossneural
bigscience-1.9.
languagemodels. InTheEleventhInternationalConfer-
Bohnet,B.,Tran,V.Q.,Verga,P.,Aharoni,R.,Andor,D., enceonLearningRepresentations,2023. URLhttps:
Soares, L. B., Eisenstein, J., Ganchev, K., Herzig, J., //openreview.net/forum?id=TatRHT_1cK.
Hui, K., et al. Attributed question answering: Evalu-
Chen, D., Fisch, A., Weston, J., and Bordes, A. Read-
ation and modeling for attributed large language mod-
ing Wikipedia to answer open-domain questions. In
els. arXiv preprint arXiv:2212.08037, 2022. URL
Proceedingsofthe55thAnnualMeetingoftheAssocia-
https://arxiv.org/abs/2212.08037.
tionforComputationalLinguistics,2017. URLhttps:
Borgeaud,S.,Mensch,A.,Hoffmann,J.,Cai,T.,Rutherford,
//aclanthology.org/P17-1171.
E.,Millican,K.,VanDenDriessche,G.B.,Lespiau,J.-
Chen, T., Wang, H., Chen, S., Yu, W., Ma, K., Zhao,
B., Damoc, B., Clark, A., De Las Casas, D., Guy, A.,
X., Yu, D., and Zhang, H. Dense X Retrieval: What
Menick,J.,Ring,R.,Hennigan,T.,Huang,S.,Maggiore,
retrieval granularity should we use? arXiv preprint
L.,Jones,C.,Cassirer,A.,Brock,A.,Paganini,M.,Irving,
arXiv:2312.06648, 2023a. URL https://arxiv.
G., Vinyals, O., Osindero, S., Simonyan, K., Rae, J.,
org/abs/2312.06648.
Elsen,E.,andSifre,L. Improvinglanguagemodelsby
retrieving from trillions of tokens. In Proceedings of Chen, W., Hu, H., Chen, X., Verga, P., and Cohen, W.
the39thInternationalConferenceonMachineLearning, MuRAG:Multimodalretrieval-augmentedgeneratorfor
2022.URLhttps://proceedings.mlr.press/ openquestionansweringoverimagesandtext. InGold-
v162/borgeaud22a.html. berg, Y., Kozareva, Z., and Zhang, Y. (eds.), Proceed-
ings of the 2022 Conference on Empirical Methods in
Brown, H., Lee, K., Mireshghallah, F., Shokri, R., and
NaturalLanguageProcessing,2022. URLhttps://
Trame`r, F. What does it mean for a language model
aclanthology.org/2022.emnlp-main.375.
to preserve privacy? In Proceedings of the 2022
ACMConferenceonFairness,Accountability,andTrans- Chen,Z.,Cano,A.H.,Romanou,A.,Bonnet,A.,Matoba,
parency,2022. URLhttps://dl.acm.org/doi/ K., Salvi, F., Pagliardini, M., Fan, S., Ko¨pf, A., Mo-
fullHtml/10.1145/3531146.3534642. htashami,A.,etal. MEDITRON-70B:Scalingmedical
pretraining for large language models. arXiv preprint
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan,
arXiv:2311.16079, 2023b. URL https://arxiv.
J.D.,Dhariwal,P.,Neelakantan,A.,Shyam,P.,Sastry,
org/abs/2311.16079.
G.,Askell,A.,Agarwal,S.,Herbert-Voss,A.,Krueger,
G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Chowdhery,A.,Narang,S.,Devlin,J.,Bosma,M.,Mishra,
Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., G., Roberts, A., Barham, P., Chung, H.W., Sutton, C.,
Litwin, M., Gray, S., Chess, B., Clark, J., Berner, Gehrmann,S.,etal. PaLM:Scalinglanguagemodeling
10Reliable,Adaptable,andAttributableLanguageModelswithRetrieval
withpathways. arXivpreprintarXiv:2204.02311,2022. Dubois,Y.,Li,X.,Taori,R.,Zhang,T.,Gulrajani,I.,Ba,J.,
URLhttps://arxiv.org/abs/2204.02311. Guestrin,C.,Liang,P.,andHashimoto,T.B. Alpacafarm:
Asimulationframeworkformethodsthatlearnfromhu-
Dao,T.,Fu,D.,Ermon,S.,Rudra,A.,andRe´,C. Flashat- manfeedback. arXivpreprintarXiv:2305.14387,2023.
tention: Fastandmemory-efficientexactattentionwith URLhttps://arxiv.org/abs/2305.14387.
io-awareness. In Advancesin NeuralInformation Pro-
cessingSystems,2022. URLhttps://openreview. Fan, A., Jernite, Y., Perez, E., Grangier, D., Weston, J.,
net/forum?id=H4DqfPSibmx. and Auli, M. ELI5: Long form question answering.
In Proceedings of the 57th Annual Meeting of the As-
De Cao, N., Aziz, W., and Titov, I. Editing fac- sociation for Computational Linguistics, 2019. URL
tual knowledge in language models. In Moens, https://aclanthology.org/P19-1346.
M.-F., Huang, X., Specia, L., and Yih, S. W.-
Gao,L.,Biderman,S.,Black,S.,Golding,L.,Hoppe,T.,
t. (eds.), Proceedings of the 2021 Conference on
Foster,C.,Phang,J.,He,H.,Thite,A.,Nabeshima,N.,
Empirical Methods in Natural Language Processing,
etal. Thepile: An800gbdatasetofdiversetextforlan-
2021. URLhttps://aclanthology.org/2021.
guagemodeling. arXivpreprintarXiv:2101.00027,2020.
emnlp-main.522/.
URLhttps://arxiv.org/abs/2101.00027.
deJong,M.,Zemlyanskiy,Y.,FitzGerald,N.,Sha,F.,and
Gao, T., Yen, H., Yu, J., and Chen, D. Enabling large
Cohen,W.W. Mentionmemory: incorporatingtextual
languagemodelstogeneratetextwithcitations. InPro-
knowledgeintotransformersthroughentitymentionatten-
ceedingsofthe2023ConferenceonEmpiricalMethodsin
tion. InInternationalConferenceonLearningRepresen-
NaturalLanguageProcessing,2023a. URLhttps://
tations, 2022. URL https://openreview.net/
aclanthology.org/2023.emnlp-main.398.
forum?id=OY1A8ejQgEX.
Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai,
Devlin,J.,Chang,M.-W.,Lee,K.,andToutanova,K.BERT: Y., Sun, J., and Wang, H. Retrieval-augmented gen-
Pre-training of deep bidirectional transformers for lan- eration for large language models: A survey. arXiv
guageunderstanding. InProceedingsofthe2019Con- preprint arXiv:2312.10997, 2023b. URL https://
ferenceoftheNorthAmericanChapteroftheAssocia- arxiv.org/abs/2312.10997.
tion for Computational Linguistics: Human Language
Technologies, Minneapolis, Minnesota, June 2019. As- Groeneveld,D.,Beltagy,I.,Walsh,P.,Bhagia,A.,Kinney,
sociationforComputationalLinguistics. URLhttps: R., Tafjord, O., Jha, A. H., Ivison, H., Magnusson, I.,
//aclanthology.org/N19-1423. Wang,Y.,etal. Olmo: Acceleratingthescienceoflan-
guagemodels. arXivpreprintarXiv:2402.00838,2024.
Dodge, J., Sap, M., Marasovic´, A., Agnew, W., Ilharco, URLhttps://arxiv.org/abs/2402.00838.
G.,Groeneveld,D.,Mitchell,M.,andGardner,M. Doc-
Grosse,R.,Bae,J.,Anil,C.,Elhage,N.,Tamkin,A.,Tajdini,
umenting large webtext corpora: A case study on the
A.,Steiner,B.,Li,D.,Durmus,E.,Perez,E.,etal. Study-
colossalcleancrawledcorpus. InMoens,M.-F.,Huang,
inglargelanguagemodelgeneralizationwithinfluence
X., Specia, L., andYih, S.W.-t.(eds.), Proceedingsof
functions. arXivpreprintarXiv:2308.03296,2023. URL
the2021ConferenceonEmpiricalMethodsinNatural
https://arxiv.org/abs/2308.03296.
Language Processing, 2021. doi: 10.18653/v1/2021.
emnlp-main.98. URL https://aclanthology.
Gu,J.,Wang,Y.,Cho,K.,andLi,V.O.K. Searchengine
org/2021.emnlp-main.98.
guidedneuralmachinetranslation. InAAAIConference
onArtificialIntelligence,2018. URLhttps://api.
Douze,M.,Guzhva,A.,Deng,C.,Johnson,J.,Szilvasy,G.,
semanticscholar.org/CorpusID:19206366.
Mazare´, P.-E., Lomeli, M., Hosseini, L., andJe´gou, H.
Thefaisslibrary. arXivpreprintarXiv:2401.08281,2024. Gudibande, A., Wallace, E., Snell, C., Geng, X., Liu, H.,
URLhttps://arxiv.org/abs/1702.08734.
Abbeel,P.,Levine,S.,andSong,D. Thefalsepromise
ofimitatingproprietarylanguagemodels. InTheTwelfth
Drozdov, A., Wang, S., Rahimi, R., McCallum, A., Za-
InternationalConferenceonLearningRepresentations,
mani, H., and Iyyer, M. You can’t pick your neigh- 2024. URLhttps://openreview.net/forum?
bors, or can you? when and how to rely on re- id=Kz3yckpCN5.
trieval in the kNN-LM. In Findings of the Associ-
ation for Computational Linguistics: EMNLP 2022, Gupta, A., Shirgaonkar, A., Balaguer, A. d. L., Silva,
2022. URLhttps://aclanthology.org/2022. B., Holstein, D., Li, D., Marsman, J., Nunes, L. O.,
findings-emnlp.218. Rouzbahman,M.,Sharp,M.,etal. Ragvsfine-tuning:
11Reliable,Adaptable,andAttributableLanguageModelswithRetrieval
Pipelines, tradeoffs, and a case study on agriculture. Jiang,Z.,Xu,F.,Gao,L.,Sun,Z.,Liu,Q.,Dwivedi-Yu,J.,
arXivpreprintarXiv:2401.08406,2024. URLhttps: Yang,Y.,Callan,J.,andNeubig,G. Activeretrievalaug-
//arxiv.org/abs/2401.08406. mentedgeneration. InProceedingsofthe2023Confer-
enceonEmpiricalMethodsinNaturalLanguageProcess-
Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang,
ing, 2023. URL https://aclanthology.org/
M. Retrieval augmented language model pre-training.
2023.emnlp-main.495.
In International Conference on Machine Learning,
2020. URLhttps://dl.acm.org/doi/pdf/10.
Jin,X.,Zhang,D.,Zhu,H.,Xiao,W.,Li,S.-W.,Wei,X.,
5555/3524938.3525306.
Arnold,A.,andRen,X.Lifelongpretraining:Continually
adaptinglanguagemodelstoemergingcorpora. InPro-
Hayati,S.A.,Olivier,R.,Avvaru,P.,Yin,P.,Tomasic,A.,
ceedingsofthe2022ConferenceoftheNorthAmerican
andNeubig,G. Retrieval-basedneuralcodegeneration.
ChapteroftheAssociationforComputationalLinguistics:
In Proceedings of the 2018 Conference on Empirical
HumanLanguageTechnologies,2022. URLhttps://
MethodsinNaturalLanguageProcessing, 2018. URL
https://aclanthology.org/D18-1111.
aclanthology.org/2022.naacl-main.351.
He, Z., Zhong, Z., Cai, T., Lee, J. D., and He, D. Rest: Johnson,J.,Douze,M.,andJe´gou,H. Billion-scalesimilar-
Retrieval-based speculative decoding. arXiv preprint itysearchwithgpus. arXivpreprintarXiv:1702.08734,
arXiv:2311.08252, 2023. URL https://arxiv. 2017. URL https://arxiv.org/abs/1702.
org/abs/2311.08252. 08734.
Henderson,P.,Li,X.,Jurafsky,D.,Hashimoto,T.,Lemley,
Kandpal, N., Deng, H., Roberts, A., Wallace, E.,
M. A., and Liang, P. Foundation models and fair use.
and Raffel, C. Large language models struggle
arXivpreprintarXiv:2303.15715,2023. URLhttps:
to learn long-tail knowledge. In International
//arxiv.org/abs/2303.15715.
Conference on Machine Learning, 2022a. URL
https://proceedings.mlr.press/v202/
Izacard,G.andGrave,E. Leveragingpassageretrievalwith
generativemodelsforopendomainquestionanswering.
kandpal23a/kandpal23a.pdf.
InProceedingsofthe16thConferenceoftheEuropean
Kandpal,N.,Wallace,E.,andRaffel,C.Deduplicatingtrain-
Chapter of the Association for Computational Linguis-
tics, 2021a. URL https://aclanthology.org/ ingdatamitigatesprivacyrisksinlanguagemodels. In
2021.eacl-main.74.
InternationalConferenceonMachineLearning,2022b.
Izacard, G. and Grave, E. Distilling knowledge from Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L.,
reader to retriever for question answering. In In- Edunov, S., Chen, D., and Yih, W.-t. Dense passage
ternational Conference on Learning Representations, retrieval for open-domain question answering. In Pro-
2021b.URLhttps://openreview.net/forum? ceedingsofthe2020ConferenceonEmpiricalMethods
id=NTEz-6wysdb. inNaturalLanguageProcessing,2020.URLhttps://
aclanthology.org/2020.emnlp-main.550.
Izacard,G.,Caron,M.,Hosseini,L.,Riedel,S.,Bojanowski,
P.,Joulin,A.,andGrave,E. Unsuperviseddenseinfor-
Kasai,J.,Sakaguchi,K.,Takahashi,Y.,Bras,R.L.,Asai,
mationretrievalwithcontrastivelearning. Transactions
A.,Yu,X.,Radev,D.,Smith,N.A.,Choi,Y.,andInui,
on Machine Learning Research, 2022. URL https:
K. Realtimeqa: What’stheanswerrightnow? InThirty-
//openreview.net/forum?id=jKN1pXi7b0.
seventh Conference on Neural Information Processing
Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, Systems Datasets and Benchmarks Track, 2023. URL
F.,Schick,T.,Dwivedi-Yu,J.,Joulin,A.,Riedel,S.,and
https://arxiv.org/abs/2207.13332.
Grave, E. Atlas: Few-shot learning with retrieval aug-
Khandelwal,U.,Levy,O.,Jurafsky,D.,Zettlemoyer,L.,and
mentedlanguagemodels. JournalofMachineLearning
Lewis,M.Generalizationthroughmemorization:Nearest
Research,2023. URLhttp://jmlr.org/papers/
neighborlanguagemodels. InInternationalConference
v24/23-0037.html.
on Learning Representations, 2020. URL https://
Jang,J.,Yoon,D.,Yang,S.,Cha,S.,Lee,M.,Logeswaran, openreview.net/forum?id=HklBjCEKvH.
L.,andSeo,M. Knowledgeunlearningformitigatingpri-
vacyrisksinlanguagemodels. InProceedingsofthe61st Khandelwal,U.,Fan,A.,Jurafsky,D.,Zettlemoyer,L.,and
Annual Meeting of the Association for Computational Lewis,M.Nearestneighbormachinetranslation.InInter-
Linguistics, 2023. URL https://aclanthology. nationalConferenceonLearningRepresentations,2021.
org/2023.acl-long.805. URLhttps://arxiv.org/abs/2010.00710.
12Reliable,Adaptable,andAttributableLanguageModelswithRetrieval
Khattab, O. and Zaharia, M. Colbert: Efficient and ef- Rockta¨schel, T., Riedel, S., and Kiela, D. Retrieval-
fective passage search via contextualized late interac- augmentedgenerationforknowledge-intensivenlptasks.
tion over bert. In Proceedings of the 43rd Interna- In Advances in Neural Information Processing Sys-
tional ACM SIGIR conference on research and devel- tems, 2020b. URL https://proceedings.
opment in Information Retrieval, 2020. URL https: neurips.cc/paper/2020/file/
//doi.org/10.1145/3397271.3401075. 6b493230205f780e1bc26945df7481e5-Paper.
pdf.
Khattab,O.,Singhvi,A.,Maheshwari,P.,Zhang,Z.,San-
thanam,K.,Vardhamanan,S.,Haq,S.,Sharma,A.,Joshi, Li,D.,Chen,Z.,Cho,E.,Hao,J.,Liu,X.,Xing,F.,Guo,C.,
T. T., Moazam, H., et al. DSPy: Compiling declara- andLiu,Y.Overcomingcatastrophicforgettingduringdo-
tivelanguagemodelcallsintostate-of-the-artpipelines. mainadaptationofseq2seqlanguagegeneration. InPro-
In The Twelfth International Conference on Learning ceedingsofthe2022ConferenceoftheNorthAmerican
Representations,2024. URLhttps://openreview. ChapteroftheAssociationforComputationalLinguistics:
net/forum?id=sY5N0zY5Od. HumanLanguageTechnologies,2022. URLhttps://
aclanthology.org/2022.naacl-main.398.
Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M.,
Parikh,A.,Alberti,C.,Epstein,D.,Polosukhin,I.,De- Lin, S.-C., Asai, A., Li, M., Oguz, B., Lin, J., Mehdad,
vlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, Y., Yih, W.-t., and Chen, X. How to train your
M., Chang, M.-W., Dai, A. M., Uszkoreit, J., Le, Q., dragon: Diverse augmentation towards generalizable
and Petrov, S. Natural questions: A benchmark for dense retrieval. In Bouamor, H., Pino, J., and Bali,
question answering research. Transactions of the As- K. (eds.), Findings of the Association for Computa-
sociation for Computational Linguistics, 2019. URL tionalLinguistics: EMNLP2023,pp.6385–6400,Singa-
https://aclanthology.org/Q19-1026. pore, December 2023. Association for Computational
Linguistics. doi: 10.18653/v1/2023.findings-emnlp.
Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, 423. URL https://aclanthology.org/2023.
C.H.,Gonzalez,J.E.,Zhang,H.,andStoica,I. Efficient findings-emnlp.423.
memorymanagementforlargelanguagemodelserving
withpagedattention. InProceedingsoftheACMSIGOPS Lin, X. V., Chen, X., Chen, M., Shi, W., Lomeli, M.,
29thSymposiumonOperatingSystemsPrinciples,2023. James,R.,Rodriguez,P.,Kahn,J.,Szilvasy,G.,Lewis,
URLhttps://arxiv.org/abs/2309.06180. M., Zettlemoyer, L., and Yih, S. RA-DIT: Retrieval-
augmented dual instruction tuning. In The Twelfth In-
Lan,T.,Cai,D.,Wang,Y.,Huang,H.,andMao,X.-L. Copy ternational Conference on Learning Representations,
isallyouneed. InTheEleventhInternationalConference 2024. URLhttps://openreview.net/forum?
on Learning Representations, 2023. URL https:// id=22OTbutug9.
openreview.net/forum?id=CROlOA9Nd8C.
Liu, N.F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua,
Lee,K.,Chang,M.-W.,andToutanova,K. Latentretrieval M.,Petroni,F.,andLiang,P. Lostinthemiddle: How
forweaklysupervisedopendomainquestionanswering. languagemodelsuselongcontexts. Transactionsofthe
In Proceedings of the 57th Annual Meeting of the As- AssociationforComputationalLinguistics,2023. URL
sociation for Computational Linguistics, 2019. URL https://arxiv.org/abs/2307.03172.
https://aclanthology.org/P19-1612.
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,
Lee,K.,Cooper,A.F.,andGrimmelmann,J. Talkin”bout Levy,O.,Lewis,M.,Zettlemoyer,L.,andStoyanov,V.
ai generation: Copyright and the generative-ai supply RoBERTa: ArobustlyoptimizedBERTpretrainingap-
chain. Forthcoming, Journal of the Copyright Societ, proach, 2020. URL https://openreview.net/
2024. URL https://papers.ssrn.com/sol3/ forum?id=SyxS0T4tvS.
papers.cfm?abstract_id=4523551.
Longpre, S., Yauney, G., Reif, E., Lee, K., Roberts, A.,
Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mo- Zoph,B.,Zhou,D.,Wei,J.,Robinson,K.,Mimno,D.,
hamed,A.,Levy,O.,Stoyanov,V.,andZettlemoyer,L. etal. Apretrainer’sguidetotrainingdata: Measuringthe
BART:Denoisingsequence-to-sequencepre-trainingfor effectsofdataage,domaincoverage,quality,&toxicity.
naturallanguagegeneration,translation,andcomprehen- arXivpreprintarXiv:2305.13169,2023. URLhttps:
sion. 2020a. URLhttps://aclanthology.org/ //arxiv.org/abs/2305.13169.
2020.acl-main.703.
Luo, H., Chuang, Y.-S., Gong, Y., Zhang, T., Kim, Y.,
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, Wu,X.,Fox,D.,Meng,H.,andGlass,J. Sail: Search-
V., Goyal, N., Ku¨ttler, H., Lewis, M., Yih, W.-t., augmented instruction learning. In Findings of the As-
13Reliable,Adaptable,andAttributableLanguageModelswithRetrieval
sociationforComputationalLinguistics: EMNLP2023, Muennighoff,N.,Su,H.,Wang,L.,Yang,N.,Wei,F.,Yu,T.,
2023. URLhttps://aclanthology.org/2023. Singh,A.,andKiela,D. Generativerepresentationalin-
findings-emnlp.242. structiontuning. arXivpreprintarXiv:2402.09906,2024.
URLhttps://arxiv.org/abs/2402.09906.
Lyu,X.,Min,S.,Beltagy,I.,Zettlemoyer,L.,andHajishirzi,
H. Z-ICL: Zero-shot in-context learning with pseudo- Nakano,R.,Hilton,J.,Balaji,S.,Wu,J.,Ouyang,L.,Kim,
demonstrations. InProceedingsofthe61stAnnualMeet- C.,Hesse,C.,Jain,S.,Kosaraju,V.,Saunders,W.,etal.
ing of the Association for Computational Linguistics, Webgpt: Browser-assistedquestion-answeringwithhu-
2023. URLhttps://aclanthology.org/2023. manfeedback. arXivpreprintarXiv:2112.09332,2021.
acl-long.129. URLhttps://arxiv.org/abs/2112.09332.
Malaviya, C., Lee, S., Chen, S., Sieber, E., Yatskar, Ni,J.,Qu,C.,Lu,J.,Dai,Z.,HernandezAbrego,G.,Ma,J.,
M., and Roth, D. Expertqa: Expert-curated ques- Zhao,V.,Luan,Y.,Hall,K.,Chang,M.-W.,andYang,Y.
tions and attributed answers. ArXiv, abs/2309.07852, Largedualencodersaregeneralizableretrievers. InPro-
2023. URL https://api.semanticscholar. ceedingsofthe2022ConferenceonEmpiricalMethods
org/CorpusID:261823130. inNaturalLanguageProcessing,2022.URLhttps://
aclanthology.org/2022.emnlp-main.669.
Mallen, A., Asai, A., Zhong, V., Das, R., Khashabi, D.,
and Hajishirzi, H. When not to trust language mod-
Nie, E., Liang, S., Schmid, H., and Schu¨tze, H.
els: Investigating effectiveness of parametric and non-
Cross-lingual retrieval augmented prompt for low-
parametricmemories. InProceedingsofthe61stAnnual
resource languages. In Findings of the Associ-
MeetingoftheAssociationforComputationalLinguistics,
ation for Computational Linguistics: ACL 2023,
2023. URLhttps://aclanthology.org/2023.
2023. URLhttps://aclanthology.org/2023.
acl-long.546.
findings-acl.528.
Min, S., Krishna, K., Lyu, X., Lewis, M., Yih, W.-t.,
OpenAI. Gpt-4 technical report. arXiv preprint
Koh, P. W., Iyyer, M., Zettlemoyer, L., and Hajishirzi,
arXiv:2303.08774, 2023. URL https://arxiv.
H. Factscore: Fine-grainedatomicevaluationoffactual
org/abs/2303.08774.
precision in long form text generation. arXiv preprint
arXiv:2305.14251, 2023a. URL https://arxiv.
Ouyang,L.,Wu,J.,Jiang,X.,Almeida,D.,Wainwright,C.,
org/abs/2305.14251.
Mishkin,P.,Zhang,C.,Agarwal,S.,Slama,K.,Gray,A.,
Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens,
Min, S., Shi, W., Lewis, M., Chen, X., Yih, W.-t., Ha-
M., Askell, A., Welinder, P., Christiano, P., Leike, J.,
jishirzi,H.,andZettlemoyer,L. Nonparametricmasked
and Lowe, R. Training language models to follow in-
language modeling. In Findings of the Association
structionswithhumanfeedback. InAdvancesinNeural
for Computational Linguistics: ACL 2023, Toronto,
Information Processing Systems, 2022. URL https:
Canada,2023b.AssociationforComputationalLinguis-
//openreview.net/forum?id=TG8KACxEON.
tics. URL https://aclanthology.org/2023.
findings-acl.132.
Ovadia,O.,Brief,M.,Mishaeli,M.,andElisha,O. Fine-
Min,S.,Gururangan,S.,Wallace,E.,Hajishirzi,H.,Smith, tuning or retrieval? comparing knowledge injection in
N.A.,andZettlemoyer,L.SILOlanguagemodels:Isolat- llms. arXiv preprint arXiv:2312.05934, 2023. URL
inglegalriskinanonparametricdatastore. InTheTwelfth
https://arxiv.org/abs/2312.05934.
InternationalConferenceonLearningRepresentations,
Perez,E.,Huang,S.,Song,F.,Cai,T.,Ring,R.,Aslanides,
2024. URLhttps://openreview.net/forum?
J., Glaese, A., McAleese, N., and Irving, G. Red
id=ruk0nyQPec.
teaminglanguagemodelswithlanguagemodels. arXiv
Mishra, A., Asai, A., Wang, Y., Balachandran, V., Neu- preprint arXiv:2202.03286, 2022. URL https://
big, G., Tsvetkov, Y., and Hajishirzi, H. Fine-grained arxiv.org/abs/2202.03286.
hallucinations detections. arXiv preprint, 2024. URL
Peters,M.E.,Neumann,M.,Iyyer,M.,Gardner,M.,Clark,
https://arxiv.org/abs/2401.06855.
C., Lee, K., and Zettlemoyer, L. Deep contextualized
Mitchell,E.,Lin,C.,Bosselut,A.,Manning,C.D.,andFinn, wordrepresentations. InProceedingsofthe2018Confer-
C. Memory-based model editing at scale. In Proceed- enceoftheNorthAmericanChapteroftheAssociationfor
ings of the 39th International Conference on Machine ComputationalLinguistics: HumanLanguageTechnolo-
Learning, 2022. URL https://proceedings. gies, 2018. URL https://aclanthology.org/
mlr.press/v162/mitchell22a.html. N18-1202.
14Reliable,Adaptable,andAttributableLanguageModelswithRetrieval
Press,O.,Zhang,M.,Min,S.,Schmidt,L.,Smith,N.,and learning models with over 100 billion parameters. In
Lewis, M. Measuring and narrowing the composition- Proceedings of the 26th ACM SIGKDD International
ality gap in language models. In Findings of the Asso- Conference on Knowledge Discovery & Data Min-
ciation for Computational Linguistics: EMNLP 2023, ing, 2020. URL https://doi.org/10.1145/
2023. URLhttps://aclanthology.org/2023. 3394486.3406703.
findings-emnlp.378.
Robertson, S. E. and Zaragoza, H. The proba-
Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., bilistic relevance framework: Bm25 and beyond.
etal. Improvinglanguageunderstandingbygenerative Foundations and Trends in Information Retrieval,
pre-training. 2018. URL https://openai.com/ 2009. URL https://api.semanticscholar.
research/language-unsupervised. org/CorpusID:207178704.
Radford,A.,Wu,J.,Child,R.,Luan,D.,Amodei,D.,and
Rubin, O.andBerant, J. Long-rangelanguagemodeling
Sutskever, I. Language models are unsupervised mul-
with self-retrieval. arXiv preprint arXiv:2306.13421,
titasklearners,2019. URLhttps://openai.com/ 2023. URL https://arxiv.org/abs/2306.
research/better-language-models.
13421.
Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoff-
Rubin, O., Herzig, J., andBerant, J. Learningtoretrieve
mann, J., Song, F., Aslanides, J., Henderson, S., Ring,
promptsforin-contextlearning. InCarpuat,M.,deMarn-
R., Young, S., Rutherford, E., Hennigan, T., Menick,
effe, M.-C., and Meza Ruiz, I. V. (eds.), Proceedings
J., Cassirer, A., Powell, R., van den Driessche, G.,
ofthe2022ConferenceoftheNorthAmericanChapter
Hendricks, L. A., Rauh, M., Huang, P.-S., Glaese, A.,
of the Association for Computational Linguistics: Hu-
Welbl, J., Dathathri, S., Huang, S., Uesato, J., Mel-
man Language Technologies, 2022. URL https://
lor, J. F. J., Higgins, I., Creswell, A., McAleese, N.,
aclanthology.org/2022.naacl-main.191.
Wu, A., Elsen, E., Jayakumar, S. M., Buchatskaya, E.,
Budden, D., Sutherland, E., Simonyan, K., Paganini, Schwartz,R.,Dodge,J.,Smith,N.A.,andEtzioni,O.Green
M., Sifre, L., Martens, L., Li, X. L., Kuncoro, A., Ne- ai;2019. arXivpreprintarXiv:1907.10597,2019. URL
matzadeh,A.,Gribovskaya,E.,Donato,D.,Lazaridou, https://arxiv.org/abs/1907.10597.
A.,Mensch,A.,Lespiau,J.-B.,Tsimpoukelli,M.,Grig-
orev,N.K.,Fritz,D.,Sottiaux,T.,Pajarskas,M.,Pohlen, Shao,R.,Min,S.,Zettlemoyer,L.,andKoh,P.W. Retrieval-
T., Gong, Z., Toyama, D., de Masson d’Autume, C., basedlanguagemodelsusingamulti-domaindatastore.
Li,Y.,Terzi,T.,Mikulik,V.,Babuschkin,I.,Clark,A., InNeurIPS2023WorkshoponDistributionShifts: New
deLasCasas,D.,Guy,A.,Jones,C.,Bradbury,J.,John- FrontierswithFoundationModels,2023. URLhttps:
son, M. G., Hechtman, B. A., Weidinger, L., Gabriel, //openreview.net/forum?id=5ck1WQ4yW4.
I., Isaac, W. S., Lockhart, E., Osindero, S., Rimell, L.,
Shi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi,
Dyer, C., Vinyals, O., Ayoub, K.W., Stanway, J., Ben-
E.H.,Scha¨rli,N.,andZhou,D. Largelanguagemodels
nett, L. L., Hassabis, D., Kavukcuoglu, K., andIrving,
canbeeasilydistractedbyirrelevantcontext. InProceed-
G. Scaling language models: Methods, analysis & in-
ings of the 40th International Conference on Machine
sights from training gopher. ArXiv, abs/2112.11446,
Learning, 2023a. URL https://proceedings.
2021. URL https://api.semanticscholar.
mlr.press/v202/shi23a.html.
org/CorpusID:245353475.
Shi, W., Michael, J., Gururangan, S., andZettlemoyer, L.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Nearestneighborzero-shotinference. InProceedingsof
Matena, M., Zhou, Y., Li, W., and Liu, P. J. Explor-
the2021ConferenceonEmpiricalMethodsinNatural
ing the limits of transfer learning with a unified text-
LanguageProcessing, 2022. URLhttps://arxiv.
to-text transformer. Journal of Machine Learning Re-
search, 2020. URL http://jmlr.org/papers/
org/abs/2205.13792.
v21/20-074.html.
Shi, W., Han, X., Lewis, M., Tsvetkov, Y., Zettlemoyer,
Ram,O.,Levine,Y.,Dalmedigos,I.,Muhlgay,D.,Shashua, L., and Yih, S. W.-t. Trusting your evidence: Halluci-
A., Leyton-Brown, K., and Shoham, Y. In-context nate less with context-aware decoding. arXiv preprint
retrieval-augmentedlanguagemodels.Transactionsofthe arXiv:2305.14739, 2023b. URL https://arxiv.
AssociationforComputationalLinguistics,2023. URL org/abs/2305.14739.
https://arxiv.org/abs/2302.00083.
Shi,W.,Min,S.,Yasunaga,M.,Seo,M.,James,R.,Lewis,
Rasley, J., Rajbhandari, S., Ruwase, O., and He, Y. M.,Zettlemoyer,L.,andYih,W.-t. REPLUG:Retrieval-
Deepspeed: Systemoptimizationsenabletrainingdeep augmentedblack-boxlanguagemodels. arXivpreprint
15Reliable,Adaptable,andAttributableLanguageModelswithRetrieval
arXiv:2301.12652, 2023c. URL https://arxiv. Taylor,R.,Kardas,M.,Cucurull,G.,Scialom,T.,Hartshorn,
org/abs/2301.12652. A., Saravia, E., Poulton, A., Kerkez, V., and Stojnic,
R. Galactica: A large language model for science.
Shi, W., Min, S., Lomeli, M., Zhou, C., Li, M., Lin, V., arXivpreprintarXiv:2211.09085,2022. URLhttps:
Smith, N. A., Zettlemoyer, L., Yih, S., and Lewis, M. //arxiv.org/abs/2211.09085.
In-contextpretraining: Languagemodelingbeyonddoc-
umentboundaries. InTheTwelfthInternationalConfer- Thorne,J.,Vlachos,A.,Christodoulopoulos,C.,andMit-
enceonLearningRepresentations,2024. URLhttps: tal, A. FEVER: a large-scale dataset for fact extrac-
//openreview.net/forum?id=LXVswInHOo. tion and VERification. In Proceedings of the 2018
Conference of the North American Chapter of the As-
Shuster, K., Poff, S., Chen, M., Kiela, D., and We- sociation for Computational Linguistics: Human Lan-
ston, J. Retrieval augmentation reduces hallucina- guageTechnologies,Volume1(LongPapers),2018. URL
tion in conversation. In Findings of the Associa- https://aclanthology.org/N18-1074.
tion for Computational Linguistics: EMNLP 2021,
2021. URLhttps://aclanthology.org/2021. Tian, Y., Luo, A., Sun, X., Ellis, K., Freeman, W. T.,
findings-emnlp.320.pdf. Tenenbaum, J. B., and Wu, J. Learning to infer and
execute 3d shape programs. In International Confer-
Singh, D., Reddy, S., Hamilton, W., Dyer, C., and enceonLearningRepresentations,2019. URLhttps:
Yogatama, D. End-to-end training of multi-document //openreview.net/forum?id=rylNH20qFQ.
readerandretrieverforopen-domainquestionanswering.
InAdvancesinNeuralInformationProcessingSystems, Touvron,H.,Lavril,T.,Izacard,G.,Martinet,X.,Lachaux,
2021. URL https://proceedings.neurips. M.-A.,Lacroix,T.,Rozie`re,B.,Goyal,N.,Hambro,E.,
cc/paper_files/paper/2021/file/ Azhar,F.,etal. Llama:Openandefficientfoundationlan-
da3fde159d754a2555eaa198d2d105b2-Paper. guagemodels. arXivpreprintarXiv:2302.13971,2023a.
pdf. URLhttps://arxiv.org/abs/2302.13971.
Singhal,K.,Azizi,S.,Tu,T.,Mahdavi,S.S.,Wei,J.,Chung, Touvron,H.,Martin,L.,Stone,K.,Albert,P.,Almahairi,A.,
H.W.,Scales,N.,Tanwani,A.,Cole-Lewis,H.,Pfohl,S., Babaei,Y.,Bashlykov,N.,Batra,S.,Bhargava,P.,Bhos-
etal. Largelanguagemodelsencodeclinicalknowledge. ale,S.,etal. Llama2: Openfoundationandfine-tuned
Nature, 2023. URL https://www.nature.com/ chatmodels. arXivpreprintarXiv:2307.09288, 2023b.
articles/s41586-023-06291-2. URLhttps://arxiv.org/abs/2307.09288.
Strubell, E., Ganesh, A., and McCallum, A. Energy and Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit,
policy considerations for deep learning in NLP. In J., Jones, L., Gomez, A. N., Kaiser, L. u., and
Proceedings of the 57th Annual Meeting of the Asso- Polosukhin, I. Attention is all you need. In Ad-
ciation for Computational Linguistics, Florence, Italy, vances in Neural Information Processing Systems,
2019.AssociationforComputationalLinguistics. URL 2017. URL https://proceedings.neurips.
https://aclanthology.org/P19-1355. cc/paper_files/paper/2017/file/
3f5ee243547dee91fbd053c1c4a845aa-Paper.
Su, H., Kasai, J., Wu, C. H., Shi, W., Wang, T., Xin, pdf.
J., Zhang, R., Ostendorf, M., Zettlemoyer, L., Smith,
N.A.,andYu,T. Selectiveannotationmakeslanguage Wang, B., Ping, W., McAfee, L., Xu, P., Li, B., Shoeybi,
models better few-shot learners. In The Eleventh In- M., and Catanzaro, B. Instructretro: Instruction tun-
ternational Conference on Learning Representations, ingpostretrieval-augmentedpretraining. arXivpreprint
2023a. URLhttps://openreview.net/forum? arXiv:2310.07713, 2023a. URL https://arxiv.
id=qY1hlv7gwg. org/abs/2310.07713.
Su, H., Shi, W., Kasai, J., Wang, Y., Hu, Y., Osten- Wang,B.,Ping,W.,Xu,P.,McAfee,L.,Liu,Z.,Shoeybi,
dorf, M., Yih, W.-t., Smith, N. A., Zettlemoyer, L., M., Dong, Y., Kuchaiev, O., Li, B., Xiao, C., Anand-
and Yu, T. One embedder, any task: Instruction- kumar, A., and Catanzaro, B. Shall we pretrain au-
finetuned text embeddings. In Findings of the Associ- toregressivelanguagemodelswithretrieval? acompre-
ationforComputationalLinguistics: ACL2023,Toronto, hensive study. In Proceedings of the 2023 Conference
Canada,2023b.AssociationforComputationalLinguis- on Empirical Methods in Natural Language Process-
tics. URL https://aclanthology.org/2023. ing, 2023b. URL https://aclanthology.org/
findings-acl.71. 2023.emnlp-main.482.
16Reliable,Adaptable,andAttributableLanguageModelswithRetrieval
Wang, M., Xu, X., Yue, Q., and Wang, Y. A compre- augmentation. InTheTwelfthInternationalConference
hensive survey and experimental comparison of graph- on Learning Representations, 2024. URL https://
based approximate nearest neighbor search. Proceed- openreview.net/forum?id=mlJLVigNHp.
ings of the VLDB Endowment, 2021. URL https:
Yamada,I.,Asai,A.,andHajishirzi,H. Efficientpassage
//doi.org/10.14778/3476249.3476255.
retrievalwithhashingforopen-domainquestionanswer-
Wang,Y.,Ivison,H.,Dasigi,P.,Hessel,J.,Khot,T.,Chandu, ing. InProceedingsofthe59thAnnualMeetingofthe
K., Wadden, D., MacMillan, K., Smith, N. A., Belt- AssociationforComputationalLinguisticsandthe11th
agy, I., and Hajishirzi, H. How far can camels go? InternationalJointConferenceonNaturalLanguagePro-
exploring the state of instruction tuning on open re- cessing, Online, 2021. Association for Computational
sources.InThirty-seventhConferenceonNeuralInforma- Linguistics. URLhttps://aclanthology.org/
tionProcessingSystemsDatasetsandBenchmarksTrack, 2021.acl-short.123.
2023c. URLhttps://openreview.net/forum?
Yang, K., Swope, A. M., Gu, A., Chalamala, R., Song,
id=w4zZNC4ZaV.
P., Yu, S., Godil, S., Prenger, R., and Anandkumar, A.
Wang, Z., Nie, W., Qiao, Z., Xiao, C., Baraniuk, R., and Leandojo: Theorem proving with retrieval-augmented
Anandkumar,A. Retrieval-basedcontrollablemolecule languagemodels. InAdvancesinneuralinformationpro-
generation. In The Eleventh International Conference cessingsystems,2023. URLhttps://arxiv.org/
onLearningRepresentations,2023d. URLhttps:// abs/2306.15626.
openreview.net/forum?id=vDFA1tpuLvk.
Yao,S.,Zhao,J.,Yu,D.,Du,N.,Shafran,I.,Narasimhan,
K. R., and Cao, Y. ReAct: Synergizing reasoning
Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B.,
and acting in language models. In The Eleventh In-
Borgeaud,S.,Yogatama,D.,Bosma,M.,Zhou,D.,Met-
ternational Conference on Learning Representations,
zler, D., Chi, E.H., Hashimoto, T., Vinyals, O., Liang,
2023. URLhttps://openreview.net/forum?
P., Dean, J., andFedus, W. Emergentabilitiesoflarge
id=WE_vluYUL-X.
language models. Transactions on Machine Learning
Research, 2022. ISSN 2835-8856. URL https:// Yasunaga, M., Aghajanyan, A., Shi, W., James, R.,
openreview.net/forum?id=yzkSU5zdwD. Sur- Leskovec,J.,Liang,P.,Lewis,M.,Zettlemoyer,L.,and
veyCertification. Yih, W.-t. Retrieval-augmented multimodal language
modeling. arXivpreprintarXiv:2211.12561,2022. URL
Weidinger, L., Uesato, J., Rauh, M., Griffin, C., Huang,
https://arxiv.org/abs/2211.12561.
P.-S., Mellor, J., Glaese, A., Cheng, M., Balle, B.,
Kasirzadeh, A., et al. Taxonomy of risks posed by Yogatama, D., de Masson d’Autume, C., and Kong, L.
language models. In Proceedings of the 2022 ACM Adaptive Semiparametric Language Models. Transac-
Conference on Fairness, Accountability, and Trans- tionsoftheAssociationforComputationalLinguistics,
parency,2022. URLhttps://dl.acm.org/doi/ 2021. URLhttps://doi.org/10.1162/tacl_
10.1145/3531146.3533088. a_00371.
Welleck, S., Liu, J., Lu, X., Hajishirzi, H., and Choi, Y. Yoran, O., Wolfson, T., Ram, O., and Berant, J. Mak-
Naturalprover: Groundedmathematicalproofgeneration ing retrieval-augmented language models robust to ir-
withlanguagemodels. AdvancesinNeuralInformation relevant context. In The Twelfth International Confer-
ProcessingSystems,35:4913–4927,2022. enceonLearningRepresentations,2024. URLhttps:
//openreview.net/forum?id=ZS4m74kZpH.
Workshop, B., Scao, T. L., Fan, A., Akiki, C., Pavlick,
E., Ilic´, S., Hesslow, D., Castagne´, R., Luccioni, A.S., Yu, W., Zhu, C., Zhang, Z., Wang, S., Zhang, Z., Fang,
Yvon, F., et al. Bloom: A 176b-parameter open- Y.,andJiang,M. Retrievalaugmentationforcommon-
access multilingual language model. arXiv preprint sense reasoning: A unified approach. In Proceedings
arXiv:2211.05100, 2022. URL https://arxiv. of the 2022 Conference on Empirical Methods in Nat-
org/abs/2211.05100. ural Language Processing, 2022. URL https://
aclanthology.org/2022.emnlp-main.294.
Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C.
Yue, X., Wang, B., Chen, Z., Zhang, K., Su, Y.,
Memorizingtransformers. InInternationalConference
and Sun, H. Automatic evaluation of attribution by
on Learning Representations, 2022. URL https://
large language models. In Findings of the Associ-
openreview.net/forum?id=TrjbxzRcnf-.
ation for Computational Linguistics: EMNLP 2023,
Xu,F.,Shi,W.,andChoi,E.RECOMP:Improvingretrieval- 2023. URLhttps://aclanthology.org/2023.
augmentedLMswithcontextcompressionandselective findings-emnlp.307.
17Reliable,Adaptable,andAttributableLanguageModelswithRetrieval
Zha,L.,Cui,Y.,Lin,L.-H.,Kwon,M.,Arenas,M.G.,Zeng,
A.,Xia,F.,andSadigh,D. Distillingandretrievinggener-
alizableknowledgeforrobotmanipulationvialanguage
corrections. arXiv preprint arXiv:2311.10678, 2023.
URLhttps://arxiv.org/abs/2311.10678.
Zhao, W.X., Liu, J., Ren, R., andWen, J.-R. Densetext
retrieval based on pretrained language models: A sur-
vey. 2023a. URL https://doi.org/10.1145/
3637870.
Zhao, Y., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu,
M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S.,
Desmaison,A.,Balioglu,C.,Damania,P.,Nguyen,B.,
Chauhan, G., Hao, Y., Mathews, A., and Li, S. Py-
torch fsdp: Experiences on scaling fully sharded data
parallel. Proc. VLDB Endow., 2023b. URL https:
//doi.org/10.14778/3611540.3611569.
Zhong,Z.,Lei,T.,andChen,D. Traininglanguagemodels
withmemoryaugmentation. InProceedingsofthe2021
ConferenceonEmpiricalMethodsinNaturalLanguage
Processing,2022.URLhttps://arxiv.org/abs/
2205.12674.
Zhong, Z., Wu, Z., Manning, C., Potts, C., and Chen,
D. MQuAKE: Assessing knowledge editing in lan-
guage models via multi-hop questions. In Proceed-
ings of the 2023 Conference on Empirical Methods in
NaturalLanguageProcessing,2023. URLhttps://
aclanthology.org/2023.emnlp-main.971.
Zhou, S., Alon, U., Xu, F. F., Jiang, Z., and Neubig, G.
Docprompting: Generatingcodebyretrievingthedocs.
InTheEleventhInternationalConferenceonLearning
Representations,2023. URLhttps://openreview.
net/forum?id=ZTCxT2t2Ru.
18Reliable,Adaptable,andAttributableLanguageModelswithRetrieval
A.ProgressofParametricLMs
TheriseofparametricLMs. Pre-trainingtodevelopbetterparametricrepresentationsoftexthasbeenrecentlyextensively
studied. BERT(Devlinetal.,2019)isconsideredtobethefirstpre-trainedLMtrainedonlarge-scaletext,builtuponprior
greatsuccessonpre-trainedcontextualizedrepresentations(ELMo; Petersetal.2018). BERTisanencoder-only,masked
LMthatistrainedtofillinblanks(maskedtokens)duringpre-training,similartoseveralwidelyusedpre-trainedmodelssuch
asRoBERTa(Liuetal.,2020).BART(Lewisetal.,2020a)orT5(Raffeletal.,2020),ontheotherhand,areencoder-decoder
modelsthataretrainedinbothmaskedandautoregressivemanners. GPT(Radfordetal.,2018)andGPT-2(Radfordetal.,
2019) are decoder-only, autoregressive LMs that predict continuations of input tokens. Recent research highlights the
advantagesofexpandingboththeparametercountofmodelsandthescaleofpre-trainingdatasets(Raeetal.,2021). Many
proprietaryLLMssuchas175BGPT-3(Blacketal.,2022),GPT-4(OpenAI,2023)orpubliclyreleasedcheckpointssuchas
Llama1(Touvronetal.,2023a)andLlama2(Touvronetal.,2023b),whichtrainingasmallernumberofparameterson
trillionsoftokens,haveshownstrongperformanceacrossvarioustasks.
Versatile,instruction-followingsystems. StartingwithGPT-3(Brownetal.,2020),largeparametricLMshavedemon-
stratedanemergentabilityknownasin-contextlearning—theabilitytoadapttonewtasksthroughfew-shotprompting
withoutneedinganyupdatestoitsparameters. Furtherstudiesdemonstratetheimpactoflarge-scalesupervisedtraining
acrossvariedinput-outputpairs, aswellassubsequentrefinementsusingreinforcementlearningwithhumanfeedback
(RLHF),resultinginpowerfulinstruction-followingmodels(Ouyangetal.,2022;Wangetal.,2023c;Duboisetal.,2023).
Infrastructureforscalabilityandefficiency. ThenecessityoftrainingandhostingmassiveparametricLMshasmotivated
activeinterdisciplinaryresearchandopen-sourcedevelopmentstoreducethecomputationalcostsandtimeoftrainingand
inference. Forinstance,open-sourcedsoftwaresuchasPyTorchFSDP9 orDeepSpeed10 enablemoreresource-efficient
parametricLMpre-trainingviatechniquessuchasFullyShardedDataParallelism(Zhaoetal.,2023b)orZeroRedundancy
Optimizers (Rasley et al., 2020), respectively. FlashAttention (Dao et al., 2022) accelerates training and long-context
processing. Intensive ongoing research addresses the challenges of high inference costs of massive parametric LMs;
memory-efficientinferencealgorithmssuchasPagedAttnetion(Kwonetal.,2023)usedinvllm11areproposedtospeedup
theinferenceofbillion-scaleparametricLMs.
B.DetailedTaxonomyofRetrieval-augmentedLMs
B.1.Architectures
Weintroduceataxonomyofarchitecturesofretrieval-basedLMs. Ourtaxonomy(Figure2)isbasedonthreeaxes: (1)the
granularityofretrieval(whattoretrieve),(2)theincorporationmethod(howtouseretrieval),andthe(3)frequencyof
retrieval(whentoretrieve). ThistaxonomyextendsthesummarizedtaxonomyinSection4.1.1.
B.1.1.GRANULARITYOFRETRIEVAL
Wespecifytheretrievalgranularityasfollows: textchunksorsmallergranularitysuchastokens,phrases,orentities. While
ithasshowntobeeffective,textchunksoftencontainmoreinformationthannecessary,resultinginredundancy.
Textchunks. Theretrievaloftextchunks,suchas100-wordparagraphs,isaprevalentstrategyinwidelyusedretrieval-
augmentedLMssuchasREALM,RAG,andRETRO.Toimplementthis,alarge-scalecorpusDissegmentedintotext
chunksbasedonthenumberoftokensorpredefinedstructureslikesectionheadersorparagraphs. Retrievedchunksare
typicallyintegratedintoinputspaceorintermediatelayers,whichwediscussindetailinthefollowingsection,whilerecent
workshowsthatthechoiceoflengthsignificantlyaffectsperformance(Chenetal.,2023a). LMsareexpectedtopredict
outputtokenprobabilitydistributionsbyjointlyleveragingtheiroriginalknowledgeinparametersandretrievedtextchunks.
Tokensandphrases. Severalworkexploresmuchsmallerunitssuchastokens(Khandelwaletal.,2020)orphrases(Min
etal.,2023b). Giventheinputpromptx,suchtokenorphraseretrieval-augmentedLMsdirectlysearchpossiblenexttokens
fromthedatastorebymatchingtheinputpromptandsimilarprefixesinthedatastore,insteadofmakingtheLMreadand
generatefromthevocabulary. Tokenorphraseretrievalcanoftenresultinamuchlargerindexsizecomparedtotextchunk
9https://pytorch.org/docs/stable/fsdp.html
10https://github.com/microsoft/DeepSpeed
11https://github.com/vllm-project/vllm
19Reliable,Adaptable,andAttributableLanguageModelswithRetrieval
retrievalgiventhesamesizeofdatastore(i.e.,thenumberofembeddingsisbydefaultequaltothenumberoftokensinthe
datastore).
B.1.2.INCORPORATIONMETHOD
Another important axis is how the retrieved information. Essentially, retrieval-augmented LMs’ architectures can be
classifiedintothefollowingthreegroups: 1)inputaugmentation,2)intermediatefusion,and)outputinterpolation.
Inputaugmentation. Inputaugmentationsimplyaugmentstheoriginalinputxwithretrievedresultszintheinputspaceof
theLMθandrunsastandardLMinference. AsinthepioneeringworkbyChenetal.(2017),inputaugmentationenables
flexibleplug-insofdifferentmodelsforretrievalandLMcomponents. Forinstance,ATLAS(Izacardetal.,2023)and
REALM pre-trains LMs jointly with the retriever, while some recent work leverage off-the-shelf pre-trained LMs and
retrievers(Rametal.,2023;Shietal.,2023c). Onenotablebottleneckisitsredundancyandinefficiency;encodingmany
documentstogetherininputspacefacescontextlengthwindowlimitationsandincreasesinferencecostsexponentially(Xu
etal.,2024). WhilesomeworksuchasFiD(Izacardetal.,2023)exploresparallelencodingtoovercomesuchinefficiencies,
stillthesamedocumentsneedtobeencodedrepeatedlyforeachinputx.
Intermediatefusion. Tointegrateretrievedresultsinamorescalablemanner,RETRO(Borgeaudetal.,2022)introduces
anewattentionmechanismcalledchunkedcrossattention(CCA).CCAtakesmanypre-encodedtextchunks,whichare
independentofqueryxunlikeinputaugmentation,simultaneouslyinintermediatespacesbyaddinganewblockbetween
standardcrossattentionandfeed-forwardnetworkinTransformer(Vaswanietal.,2017). Recently, RETRO++(Wang
etal.,2023b)andInstructRetro(Wangetal.,2023a)incorporatedCCAintopowerfulautoregressiveLMs. However,a
drawbackofintermediatefusionistheneedforarchitecturemodificationandpre-trainingofLMsforthenewencoding
blocks,potentiallylimitingwidespreadadoption. Severalstudiesfocusonsimilararchitecturesforretrievinginformation
fromlong-contextinput(Wuetal.,2022;Rubin&Berant,2023).
Outputinterpolation. ThetwoincorporationmethodsdescribedabovestillletLMsgeneratecontinuationsfromtheir
vocabularies,whichoftenresultsinunsupportedorunattributedgenerations(Liuetal.,2023;Gaoetal.,2023a;Bohnet
etal.,2022). Instead,somemodelsdirectlymanipulateoutputtokendistributions. kNNLMinterpolatestheoriginalLM’s
softmaxtokendistributionswithretrievedtokendistributionwithoutadditionaltraining. Someworkextendsthisdirection
bydesigningnewtrainingobjectives(Zhongetal.,2022)orcompletelyreplacinganonparametricdistributionoverevery
phraseinareferencecorpus(Minetal.,2023b;Lanetal.,2023).
B.1.3.FREQUENCYOFRETRIEVAL
Anothersignificantdesignchoiceinretrieval-augmentedLMsisthefrequencyofretrieval. Inessence,optingformore
frequentretrievaltendstoenhanceperformance,butcomesattheexpenseofincreasedcomputationaloverhead. Retrieving
oncebeforegeneratinggiveninputxhasbeenwidelyusedsuchasREALMorDrQA,oftenininputspaceincorporation
architectures. kNNLM,ontheotherhand,retrievesateverytoken,orsomeworkretrieveseveryktokentomaintainthe
relevancebetweenthetargetsequenceandretrievedcontext(Rametal.,2023). Severalrecentpapersintroducemethods
thatmakeLMsadaptivelydecidewhentoretrieve(Jiangetal.,2023;Asaietal.,2024).
B.2.ApplicationsandDatastore
Thissectionbrieflyreviewstheapplicationsofretrieval-augmentedLMsanduseddatastores.
Applications. Retrieval-augmentedLMsareshowntobeeffectiveacrossarangeofNLPtasks,includingdiscriminative
andgenerativetasks. Themajorityofpriorworkisoftenevaluatedonknowledge-intensivetasks,suchasopen-domain
QA(Kwiatkowskietal.,2019),factverification(Thorneetal.,2018)andknowledge-groundingdialogue(Shusteretal.,
2021). Forsuchtasks,Wikipediaisoftenusedasthesoleknowledgesource,whilesomerecentworkdirectlycombinesLMs
withcommercialsearchengineAPIs.Fornon-knowledge-intensivetasks,theusageoftraininginstances(labeleddata)asthe
datastorehasbeenwidelyexplored,demonstratingeffectivenessontaskslikemachinetranslation(Khandelwaletal.,2021;
Zhongetal.,2022). SomerecentworkssuchaskNN-Prompt(Shietal.,2022)orNPM(Minetal.,2023b)leveragelarger
pre-trainingcorpora(e.g.,thePile;Gaoetal.2020)formoregenerallanguageunderstandingtasks(e.g.,sentimentanalysis)
orentitytranslations. Yuetal.(2022)buildanewlarge-scalecorpusconsistingof20millioncommonsensedocuments
collection from both open-domain knowledge sources. Several works on code generations use similar codes (Hayati
etal.,2018)ordocumentation(Zhouetal.,2023)ofAPIs. Designingandbuildingareliabledatastoreisakeychallenge
20Reliable,Adaptable,andAttributableLanguageModelswithRetrieval
inretrieval-augmentedLMs. Acrossthosepapers,retrieval-augmentedLMshaveshownsignificantimprovementsover
parametricLMs.
Furthermore,retrieval-augmentedLMshavebeenappliedbeyondgeneral-domain,Englishtextdata. Severalworksexplore
retrievingfrommultilingualdata(Asaietal.,2021;Nieetal.,2023)ormultiplemodalities(Yasunagaetal.,2022;Chen
etal.,2022)—whichincludesunderexploredmodalitiessuchasrobotcontrols(Zhaetal.,2023). Whilepriorworkoften
exploresretrievingfromgeneral-domaindatastoresuchasWikipedia,somerecentworkshowsthatretrievingfromatargeted
datastoreislargelyhelpfultosolvemorechallengingexpertdomaintasks,suchastheoremproving(Wellecketal.,2022;
Yangetal.,2023)ormoleculegeneration(Wangetal.,2023d).
21