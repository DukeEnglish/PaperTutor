Scaling Rectified Flow Transformers for High-Resolution Image Synthesis
PatrickEsser* SumithKulal AndreasBlattmann RahimEntezari JonasMu¨ller HarrySaini YamLevi
DominikLorenz AxelSauer FredericBoesel DustinPodell TimDockhorn ZionEnglish
KyleLacey AlexGoodwin YannikMarek RobinRombach*
StabilityAI
Figure1. High-resolutionsamplesfromour8Brectifiedflowmodel,showcasingitscapabilitiesintypography,precisepromptfollowing
andspatialreasoning,attentiontofinedetails,andhighimagequalityacrossawidevarietyofstyles.
Abstract stratethesuperiorperformanceofthisapproach
compared to established diffusion formulations
Diffusionmodelscreatedatafromnoisebyinvert-
forhigh-resolutiontext-to-imagesynthesis. Ad-
ingtheforwardpathsofdatatowardsnoiseand
ditionally,wepresentanoveltransformer-based
haveemergedasapowerfulgenerativemodeling
architecturefortext-to-imagegenerationthatuses
techniqueforhigh-dimensional,perceptualdata
separate weights for the two modalities and en-
suchasimagesandvideos. Rectifiedflowisare-
ablesabidirectionalflowofinformationbetween
centgenerativemodelformulationthatconnects
imageandtexttokens,improvingtextcomprehen-
dataandnoiseinastraightline. Despiteitsbetter
sion,typography,andhumanpreferenceratings.
theoreticalpropertiesandconceptualsimplicity,it
Wedemonstratethatthisarchitecturefollowspre-
isnotyetdecisivelyestablishedasstandardprac-
dictablescalingtrendsandcorrelateslowervali-
tice. Inthiswork,weimproveexistingnoisesam-
dationlosstoimprovedtext-to-imagesynthesisas
plingtechniquesfortrainingrectifiedflowmod-
measuredbyvariousmetricsandhumanevalua-
elsbybiasingthemtowardsperceptuallyrelevant
tions. Ourlargestmodelsoutperformstate-of-the-
scales. Throughalarge-scalestudy,wedemon-
artmodels, andwewillmakeourexperimental
*Equalcontribution.<first.last>@stability.ai. data,code,andmodelweightspubliclyavailable.
1
4202
raM
5
]VC.sc[
1v60230.3042:viXraScalingRectifiedFlowTransformersforHigh-ResolutionImageSynthesis
1.Introduction into the model (e.g., via cross-attention (Vaswani et al.,
2017; Rombach et al., 2022)), is not ideal, and present
Diffusionmodelscreatedatafromnoise(Songetal.,2020).
anewarchitecturethatincorporateslearnablestreamsfor
They are trained to invert forward paths of data towards
bothimageandtexttokens,whichenablesatwo-wayflow
randomnoiseand,thus,inconjunctionwithapproximation
of information between them. We combine this with our
and generalization properties of neural networks, can be
improvedrectifiedflowformulationandinvestigateitsscala-
used to generate new data points that are not present in
bility. Wedemonstrateapredictablescalingtrendintheval-
thetrainingdatabutfollowthedistributionofthetraining
idationlossandshowthatalowervalidationlosscorrelates
data (Sohl-Dickstein et al., 2015; Song & Ermon, 2020).
stronglywithimprovedautomaticandhumanevaluations.
Thisgenerativemodelingtechniquehasproventobevery
effective for modeling high-dimensional, perceptual data Ourlargestmodelsoutperformstate-of-theartopenmodels
suchasimages(Hoetal.,2020). Inrecentyears,diffusion such as SDXL (Podell et al., 2023), SDXL-Turbo (Sauer
modelshavebecomethede-factoapproachforgenerating etal.,2023),Pixart-α(Chenetal.,2023),andclosed-source
high-resolutionimagesandvideosfromnaturallanguage models such as DALL-E 3 (Betker et al., 2023) both in
inputswithimpressivegeneralizationcapabilities(Saharia quantitativeevaluation(Ghoshetal.,2023)ofpromptun-
etal.,2022b;Rameshetal.,2022;Rombachetal.,2022; derstandingandhumanpreferenceratings.
Podell et al., 2023; Dai et al., 2023; Esser et al., 2023;
Thecorecontributionsofourworkare: (i)Weconducta
Blattmannetal.,2023b;Betkeretal.,2023;Blattmannetal.,
large-scale,systematicstudyondifferentdiffusionmodel
2023a; Singer et al., 2022). Due to their iterative nature
andrectifiedflowformulationstoidentifythebestsetting.
andtheassociatedcomputationalcosts,aswellasthelong
Forthispurpose,weintroducenewnoisesamplersforrecti-
samplingtimesduringinference,researchonformulations
fiedflowmodelsthatimproveperformanceoverpreviously
formoreefficienttrainingand/orfastersamplingofthese
knownsamplers. (ii)Wedeviseanovel,scalablearchitec-
modelshasincreased(Karrasetal.,2023;Liuetal.,2022).
ture for text-to-image synthesis that allows bi-directional
Whilespecifyingaforwardpathfromdatatonoiseleadsto mixing between text and image token streams within the
efficienttraining,italsoraisesthequestionofwhichpath network.Weshowitsbenefitscomparedtoestablishedback-
to choose. This choice can have important implications bonessuchasUViT(Hoogeboometal.,2023)andDiT(Pee-
forsampling. Forexample,aforwardprocessthatfailsto bles&Xie,2023). Finally,we(iii)performascalingstudy
remove all noise from the data can lead to a discrepancy of our model and demonstrate that it follows predictable
intrainingandtestdistributionandresultinartifactssuch scaling trends. We show that a lower validation loss cor-
asgrayimagesamples(Linetal.,2024). Importantly,the relatesstronglywithimprovedtext-to-imageperformance
choice of the forward process also influences the learned assessedviametricssuchasT2I-CompBench(Huangetal.,
backwardprocessand,thus,thesamplingefficiency. While 2023),GenEval(Ghoshetal.,2023)andhumanratings. We
curvedpathsrequiremanyintegrationstepstosimulatethe makeresults,code,andmodelweightspubliclyavailable.
process, a straight path could be simulated with a single
2.Simulation-FreeTrainingofFlows
stepandislesspronetoerroraccumulation. Sinceeachstep
correspondstoanevaluationoftheneuralnetwork,thishas Weconsidergenerativemodelsthatdefineamappingbe-
adirectimpactonthesamplingspeed. tweensamplesx fromanoisedistributionp tosamples
1 1
x fromadatadistributionp intermsofanordinarydiffer-
Aparticularchoicefortheforwardpathisaso-calledRec- 0 0
entialequation(ODE),
tified Flow (Liu et al., 2022; Albergo & Vanden-Eijnden,
2022;Lipmanetal.,2023),whichconnectsdataandnoise
dy =v (y ,t)dt, (1)
on a straight line. Although this model class has better t Θ t
theoreticalproperties,ithasnotyetbecomedecisivelyes-
wherethevelocityvisparameterizedbytheweightsΘof
tablished in practice. So far, some advantages have been
a neural network. Prior work by Chen et al. (2018) sug-
empirically demonstrated in small and medium-sized ex-
gestedtodirectlysolveEquation(1)viadifferentiableODE
periments(Maetal.,2024),butthesearemostlylimitedto
solvers.However,thisprocessiscomputationallyexpensive,
class-conditionalmodels.Inthiswork,wechangethisbyin-
especiallyforlargenetworkarchitecturesthatparameterize
troducingare-weightingofthenoisescalesinrectifiedflow
v (y ,t). Amoreefficientalternativeistodirectlyregress
models, similar to noise-predictive diffusion models (Ho Θ t
avectorfieldu thatgeneratesaprobabilitypathbetween
et al., 2020). Through a large-scale study, we compare t
p and p . To construct such a u , we define a forward
ournewformulationtoexistingdiffusionformulationsand 0 1 t
process,correspondingtoaprobabilitypathp betweenp
demonstrateitsbenefits. t 0
andp =N(0,1),as
1
Weshowthatthewidelyusedapproachfortext-to-image
synthesis,whereafixedtextrepresentationisfeddirectly z =a x +b ϵ whereϵ∼N(0,I). (2)
t t 0 t
2ScalingRectifiedFlowTransformersforHigh-ResolutionImageSynthesis
Fora =1,b =0,a =0andb =1,themarginals, onecanderivevariousweightedlossfunctionsthatprovide
0 0 1 1
a signal towards the desired solution but might affect the
p t(z t)=E ϵ∼N(0,I)p t(z t|ϵ), (3) optimizationtrajectory. Foraunifiedanalysisofdifferent
approaches, including classic diffusion formulations, we
areconsistentwiththedataandnoisedistribution.
can write the objective in the following form (following
Toexpresstherelationshipbetweenz ,x andϵ,weintro- Kingma&Gao(2023)):
t 0
duceψ andu as
t t
L (x )=−1 E (cid:2) w λ′∥ϵ (z ,t)−ϵ∥2(cid:3) ,
ψ t(·|ϵ):x 0 (cid:55)→a tx 0+b tϵ (4) w 0 2 t∼U(t),ϵ∼N(0,I) t t Θ t
u (z|ϵ):=ψ′(ψ−1(z|ϵ)|ϵ) (5)
t t t wherew =−1λ′b2correspondstoL .
t 2 t t CFM
Sincez canbewrittenassolutiontotheODEz′ =u (z |ϵ),
t t t t 3.FlowTrajectories
with initial value z = x , u (·|ϵ) generates p (·|ϵ). Re-
0 0 t t
markably,onecanconstructamarginalvectorfieldu which
t In this work, we consider different variants of the above
generatesthemarginalprobabilitypathsp (Lipmanetal.,
t formalismthatwebrieflydescribeinthefollowing.
2023)(seeB.1),usingtheconditionalvectorfieldsu (·|ϵ):
t
p (z|ϵ) RectifiedFlow RectifiedFlows(RFs)(Liuetal.,2022;
u (z)=E u (z|ϵ) t (6)
t ϵ∼N(0,I) t p (z) Albergo & Vanden-Eijnden, 2022; Lipman et al., 2023)
t
define the forward process as straight paths between the
Whileregressingu twiththeFlowMatchingobjective datadistributionandastandardnormaldistribution,i.e.
L =E ||v (z,t)−u (z)||2. (7)
FM t,pt(z) Θ t 2 z
t
=(1−t)x 0+tϵ, (13)
directly is intractable due to the marginalization in Equa-
and uses L which then corresponds to wRF = t .
tion6,ConditionalFlowMatching(seeB.1), CFM t 1−t
Thenetworkoutputdirectlyparameterizesthevelocityv .
Θ
L =E ||v (z,t)−u (z|ϵ)||2 , (8)
CFM t,pt(z|ϵ),p(ϵ) Θ t 2
EDM EDM(Karrasetal.,2022)usesaforwardprocess
withtheconditionalvectorfieldsu (z|ϵ)providesanequiv-
t oftheform
alentyettractableobjective.
z =x +b ϵ (14)
t 0 t
To convert the loss into an explicit form we insert
ψ t′(x 0|ϵ)=a′ tx 0+b′ tϵandψ t−1(z|ϵ)= z− ab ttϵ into(5) w wih te hr Fe −(K 1i bn eg im nga th& eqG ua ao n, ti2 le02 fu3 n) cb
tit
on= ofe tx hp eF noN− rm1( at l|P
dm
ist, rP ibs2 u)
-
N
z′ =u (z |ϵ)= a′ tz −ϵb (a′ t − b′ t). (9) tionwithmeanP mandvarianceP s2. Notethatthischoice
t t t a t t a b resultsin
t t t
Now,considerthesignal-to-noiseratioλ t :=loga b22 t. With λ t ∼N(−2P m,(2P s)2) fort∼U(0,1) (15)
t
λ′ =2(a′ t − b′ t),wecanrewriteEquation(9)as
t at bt The network is parameterized through an F-prediction
a′ b (Kingma & Gao, 2023; Karras et al., 2022) and the loss
u t(z t|ϵ)= atz t− 2tλ′ tϵ (10) canbewrittenasL
wEDM
with
t t
Next,weuseEquation(10)toreparameterizeEquation(8) w tEDM =N(λ t|−2P m,(2P s)2)(e−λt +0.52) (16)
asanoise-predictionobjective:
a′ b Cosine (Nichol&Dhariwal,2021)proposedaforward
L CFM =E t,pt(z|ϵ),p(ϵ)||v Θ(z,t)− atz+ 2tλ′ tϵ||2 2 (11) processoftheform
t
=E t,pt(z|ϵ),p(ϵ)(cid:18) −b 2tλ′ t(cid:19)2 ||ϵ Θ(z,t)−ϵ||2 2 (12) z t =cos(cid:0)π 2t(cid:1) x 0+sin(cid:0)π 2t(cid:1) ϵ. (17)
wherewedefinedϵ Θ := λ− ′ tb2 t(v Θ− a a′ t tz). I cn orc ro esm pb oi nn da sti to on aw wit eh iga hn tinϵ- gp war tam =e ste er ci hz (a λti to /n 2)a .n Wd l ho es ns, coth mis -
Note that the optimum of the above objective does not binedwithav-predictionloss(Kingma&Gao,2023),the
changewhenintroducingatime-dependentweighting.Thus, weightingisgivenbyw
t
=e−λt/2.
3ScalingRectifiedFlowTransformersforHigh-ResolutionImageSynthesis
(LDM-)Linear LDM(Rombachetal.,2022)usesamod- controlsthedegreetowhicheitherthemidpoint(positive
ificationoftheDDPMschedule(Hoetal.,2020). Bothare s) or the endpoints (negative s) are favored during sam-
(cid:112)
variancepreservingschedules,i.e. b = 1−a2,andde- pling. Thisformulationalsoincludesauniformweighting
t t
fine a for discrete timesteps t = 0,...,T −1 in terms π (t;s = 0) = U(t) for s = 0, which has been used
t mode
of diffusion coefficients β
t
as a
t
= ((cid:81)t s=0(1 − β s))1 2. widely in previous works on Rectified Flows (Liu et al.,
For given boundary values β and β , DDPM uses 2022;Maetal.,2024).
0 T−1
β = β + t (β − β ) and LDM uses β =
t 0 T−1 T−1 0 t
(cid:16)(cid:112) β + t ((cid:112) β −(cid:112) β )(cid:17)2 . CosMap Finally, we also consider the cosine schedule
0 T−1 T−1 0 (Nichol&Dhariwal,2021)fromSection3intheRFsetting.
Inparticular,wearelookingforamappingf :u(cid:55)→f(u)=
3.1.TailoredSNRSamplersforRFmodels
t, u∈[0,1],suchthatthelog-snrmatchesthatofthecosine
TheRFlosstrainsthevelocityv uniformlyonalltimesteps schedule: 2logcos(π 2u) =2log1−f(u). Solvingforf,we
Θ sin(πu) f(u)
2
in[0,1]. Intuitively,however,theresultingvelocitypredic- obtainforu∼U(u)
tion target ϵ−x is more difficult for t in the middle of
0
[0,1], sincefort = 0, theoptimalpredictionisthemean 1
t=f(u)=1− , (21)
ofp ,andfort = 1theoptimalpredictionisthemeanof tan(πu)+1
1 2
p . In general, changing the distribution over t from the
0
commonlyuseduniformdistributionU(t)toadistribution fromwhichweobtainthedensity
withdensityπ(t)ise wqu πiv =alent ttoa π(w t)eightedlossL w tπ w (1it 8h
)
π
CosMap(t)=(cid:12)
(cid:12)
(cid:12)
(cid:12)dd
tf−1(t)(cid:12)
(cid:12)
(cid:12) (cid:12)=
π−2πt2
+2πt2. (22)
t 1−t
4.Text-to-ImageArchitecture
Thus,weaimtogivemoreweighttointermediatetimesteps
bysamplingthemmorefrequently. Next,wedescribethe Fortext-conditionalsamplingofimages,ourmodelhasto
timestepdensitiesπ(t)thatweusetotrainourmodels. take both modalities, text and images, into account. We
usepretrainedmodelstoderivesuitablerepresentationsand
Logit-Normal Sampling One option for a distribution thendescribethearchitectureofourdiffusionbackbone. An
that puts more weight on intermediate steps is the logit- overviewofthisispresentedinFigure2.
normaldistribution(Atchison&Shen,1980). Itsdensity,
Our general setup follows LDM (Rombach et al., 2022)
1 1 (cid:16) (logit(t)−m)2(cid:17) for training text-to-image models in the latent space of a
π ln(t;m,s)= s√ 2πt(1−t)exp −
2s2
, pretrainedautoencoder.Similartotheencodingofimagesto
(19) latentrepresentations,wealsofollowpreviousapproaches
wherelogit(t)=log t ,hasalocationparameter,m,and (Sahariaetal.,2022b;Balajietal.,2022)andencodethetext
1−t
ascaleparameter,s. Thelocationparameterenablesusto conditioningcusingpretrained,frozentextmodels. Details
biasthetrainingtimestepstowardseitherdatap (negative canbefoundinAppendixB.2.
0
m) or noise p (positive m). As shown in Figure 11, the
1 MultimodalDiffusionBackboneOurarchitecturebuilds
scaleparameterscontrolshowwidethedistributionis.
upontheDiT(Peebles&Xie,2023)architecture. DiTonly
Inpractice, wesampletherandomvariableufromanor- considers class conditional image generation and uses a
mal distribution u ∼ N(u;m,s) and map it through the modulation mechanism to condition the network on both
standardlogisticfunction. the timestep of the diffusion process and the class label.
Similarly, we use embeddings of the timestep t and c
vec
asinputstothemodulationmechanism. However, asthe
ModeSamplingwithHeavyTails Thelogit-normalden-
pooledtextrepresentationretainsonlycoarse-grainedinfor-
sity always vanishes at the endpoints 0 and 1. To study
mationaboutthetextinput(Podelletal.,2023),thenetwork
whether this has adverse effects on the performance, we
alsorequiresinformationfromthesequencerepresentation
alsouseatimestepsamplingdistributionwithstrictlyposi-
c .
tivedensityon[0,1]. Forascaleparameters,wedefine ctxt
f
mode(u;s)=1−u−s·(cid:16) cos2(cid:0)π 2u(cid:1) −1+u(cid:17)
. (20)
W texe tc ao nn dst ir muc at ga es ine pq uu te sn .c Se pc eo cn ifisi cs ati ln lyg ,o wf ee am db de pd od sin itg ios no af lt eh ne
-
codings and flatten 2×2 patches of the latent pixel rep-
For −1 ≤ s ≤ 2 , this function is monotonic, and we resentationx ∈ Rh×w×c toapatchencodingsequenceof
π−2
canuseittosamplefromtheimplieddensityπ (t;s)= length 1 ·h· 1 ·w. Afterembeddingthispatchencoding
(cid:12) (cid:12) dd tf m− o1 de(t)(cid:12) (cid:12). As seen in Figure 11, the scalemo pd ae rameter andthe2 texten2 codingc ctxttoacommondimensionality,we
4ScalingRectifiedFlowTransformersforHigh-ResolutionImageSynthesis
Caption
y
SiLU SiLU
CLIP-G/14 CLIP-L/14 T5XXL
Linear Linear
c x
77+77tokens NoisedLatent
Layernorm Layernorm
4096 Patching
αc
Mod:αc·•+βc Mod:αx·•+βx
αx
channel Linear βc Linear Linear βx
Opt. Opt. Opt. Opt.
RMS- RMS- RMS- RMS-
Positional Norm Norm Norm Norm
MLP Linear Embedding + ⊙
⊙
+ y c x ⊙
Q K V
Attention
MLP
MM-DiT-Block1
SinusoidalEncoding
Linear Linear
MM-DiT-Block2
Timestep ... γc ∗ ∗ γx
+ +
MM-DiT-Blockd
Layernorm Layernorm
δc δx
Mod:δc·•+ϵc Mod:δx·•+ϵx
Modulation ϵc ϵx
MLP MLP
Linear
ζc ∗ ∗ ζx
Unpatching
+ +
Output
(a)Overviewofallcomponents. (b)OneMM-DiTblock
Figure2.Ourmodelarchitecture.Concatenationisindicatedby⊙andelement-wisemultiplicationby∗.TheRMS-NormforQandK
canbeaddedtostabilizetrainingruns.Bestviewedzoomedin.
concatenate the two sequences. We then follow DiT and addition,thelossesofdifferentapproachesareincomparable
applyasequenceofmodulatedattentionandMLPs. andalsodonotnecessarilycorrelatewiththequalityofout-
putsamples;henceweneedevaluationmetricsthatallowfor
Since text and image embeddings are conceptually quite
acomparisonbetweenapproaches. WetrainmodelsonIma-
different,weusetwoseparatesetsofweightsforthetwo
geNet(Russakovskyetal.,2014)andCC12M(Changpinyo
modalities. As shown in Figure 2b, this is equivalent to
etal.,2021),andevaluateboththetrainingandtheEMA
havingtwoindependenttransformersforeachmodality,but
weightsofthemodelsduringtrainingusingvalidationlosses,
joiningthesequencesofthetwomodalitiesfortheattention
CLIPscores(Radfordetal.,2021;Hesseletal.,2021),and
operation,suchthatbothrepresentationscanworkintheir
FID(Heuseletal.,2017)underdifferentsamplersettings
ownspaceyettaketheotheroneintoaccount.
(differentguidancescalesandsamplingsteps). Wecalcu-
For our scaling experiments, we parameterize the size of latetheFIDonCLIPfeaturesasproposedby(Saueretal.,
themodelintermsofthemodel’sdepthd,i.e.thenumber 2021). AllmetricsareevaluatedontheCOCO-2014valida-
of attention blocks, by setting the hidden size to 64 · d tionsplit(Linetal.,2014). Fulldetailsonthetrainingand
(expandedto4·64·dchannelsintheMLPblocks),andthe samplinghyperparametersareprovidedinAppendixB.3.
numberofattentionheadsequaltod.
5.1.1.RESULTS
5.Experiments
We train each of 61 different formulations on the two
datasets. WeincludethefollowingvariantsfromSection3:
5.1.ImprovingRectifiedFlows
• Both ϵ- and v-prediction loss with linear
We aim to understand which of the approaches for
(eps/linear,v/linear)andcosine(eps/cos,
simulation-free training of normalizing flows as in Equa-
v/cos)schedule.
tion1isthemostefficient. Toenablecomparisonsacross
• RF loss with π (t;s) (rf/mode(s)) with 7 val-
differentapproaches,wecontrolfortheoptimizationalgo- mode
uesforschosenuniformlybetween−1and1.75,and
rithm,themodelarchitecture,thedatasetandsamplers. In
5
delooPScalingRectifiedFlowTransformersforHigh-ResolutionImageSynthesis
rankaveragedover bothwithandwithoutEMAweights.
variant all 5steps 50steps Forall24combinationsofsamplersettings,EMAweights,
rf/lognorm(0.00,1.00) 1.54 1.25 1.50 anddatasetchoice,werankthedifferentformulationsusing
rf/lognorm(1.00,0.60) 2.08 3.50 2.00 anon-dominatedsortingalgorithm. Forthis,werepeatedly
rf/lognorm(0.50,0.60) 2.71 8.50 1.00 computethevariantsthatareParetooptimalaccordingto
rf/mode(1.29) 2.75 3.25 3.00
CLIPandFIDscores,assignthosevariantsthecurrentiter-
rf/lognorm(0.50,1.00) 2.83 1.50 2.50
ationindex,removethosevariants,andcontinuewiththe
eps/linear 2.88 4.25 2.75
rf/mode(1.75) 3.33 2.75 2.75 remaining ones until all variants get ranked. Finally, we
rf/cosmap 4.13 3.75 4.00 averagethoseranksoverthe24differentcontrolsettings.
edm(0.00,0.60) 5.63 13.25 3.25
rf 5.67 6.50 5.75 WepresenttheresultsinTab.1,whereweonlyshowthe
v/linear 6.83 5.75 7.75 two best-performing variants for those variants that were
edm(0.60,1.20) 9.00 13.00 9.00 evaluated with different hyperparameters. We also show
v/cos 9.17 12.25 8.75
rankswherewerestricttheaveragingoversamplersettings
edm/cos 11.04 14.25 11.25
with5stepsandwith50steps.
edm/rf 13.04 15.25 13.25
edm(-1.20,1.20) 15.58 20.25 15.00
We observe that rf/lognorm(0.00, 1.00) consis-
Table1. Globalrankingofvariants.Forthisranking,weapply tently achieves a good rank. It outperforms a rectified
non-dominatedsortingaveragedoverEMAandnon-EMAweights, flowformulationwithuniformtimestepsampling(rf)and
twodatasetsanddifferentsamplingsettings. thusconfirmsourhypothesisthatintermediatetimestepsare
moreimportant. Amongallthevariants,onlyrectifiedflow
ImageNet CC12M formulationswithmodifiedtimestepsamplingperformbet-
terthantheLDM-Linear(Rombachetal.,2022)formulation
variant CLIP FID CLIP FID
(eps/linear)usedpreviously.
rf 0.247 49.70 0.217 94.90
edm(-1.20,1.20) 0.236 63.12 0.200 116.60 Wealsoobservethatsomevariantsperformwellinsome
eps/linear 0.245 48.42 0.222 90.34 settings but worse in others, e.g. rf/lognorm(0.50,
v/cos 0.244 50.74 0.209 97.87
0.60) is the best-performing variant with 50 sampling
v/linear 0.246 51.68 0.217 100.76
stepsbutmuchworse(averagerank8.5)with5sampling
rf/lognorm(0.50,0.60) 0.256 80.41 0.233 120.84
steps. We observe a similar behavior with respect to the
rf/mode(1.75) 0.253 44.39 0.218 94.06
two metrics in Tab. 2. The first group shows representa-
rf/lognorm(1.00,0.60) 0.254 114.26 0.234 147.69
rf/lognorm(-0.50,1.00) 0.248 45.64 0.219 89.70 tive variants and their metrics on both datasets with 25
sampling steps. The next group shows the variants that
rf/lognorm(0.00,1.00) 0.250 45.78 0.224 89.91
achievethebestCLIPandFIDscores. Withtheexception
Table2. Metricsfordifferentvariants.FIDandCLIPscoresof ofrf/mode(1.75),thesevariantstypicallyperformvery
differentvariantswith25samplingsteps.Wehighlightthebest, wellinonemetricbutrelativelybadlyintheother. Incon-
secondbest,andthirdbestentries. trast, we once again observe that rf/lognorm(0.00,
1.00) achieves good performance across metrics and
datasets,whereitobtainsthethird-bestscorestwooutof
additionallyfors=1.0ands=0whichcorresponds fourtimesandoncethesecond-bestperformance.
touniformtimestepsampling(rf/mode).
Finally, we illustrate the qualitative behavior of different
• RF loss with π (t;m,s) (rf/lognorm(m, s))
ln
formulations in Figure 3, where we use different colors
with30valuesfor(m,s)inthegridwithmuniform
for different groups of formulations (edm, rf, eps and
between−1and1,andsuniformbetween0.2and2.2.
v). Rectifiedflowformulationsgenerallyperformwelland,
• RFlosswithπ (t)(rf/cosmap).
CosMap
comparedtootherformulations,theirperformancedegrades
• EDM(edm(P ,P ))with15valuesforP chosen
m s m
lesswhenreducingthenumberofsamplingsteps.
uniformlybetween−1.2and1.2andP uniformbe-
s
tween 0.6 and 1.8. Note that P ,P = (−1.2,1.2)
m s
5.2.ImprovingModalitySpecificRepresentations
correspondstotheparametersin(Karrasetal.,2022).
• EDMwithaschedulesuchthatitmatchesthelog-SNR
Having found a formulation in the previous section that
weightingofrf(edm/rf)andonethatmatchesthe
allowsrectifiedflowmodelstonotonlycompetewithestab-
log-SNRweightingofv/cos(edm/cos).
lisheddiffusionformulationssuchasLDM-Linear(Rom-
Foreachrun,weselectthestepwithminimalvalidationloss bachetal.,2022)orEDM(Karrasetal.,2022), buteven
whenevaluatedwithEMAweightsandthencollectCLIP outperforms them, we now turn to the application of our
scoresandFIDobtainedwith6differentsamplersettings formulationtohigh-resolutiontext-to-imagesynthesis. Ac-
6ScalingRectifiedFlowTransformersforHigh-ResolutionImageSynthesis
140 OriginalCaptions 50/50Mix
edm(-1.20, 1.20)
eps/linear successrate[%] successrate[%]
120 rf/lognorm(0.00, 1.00)
rf ColorAttribution 11.75 24.75
100 v/cos Colors 71.54 68.09
D
FI v/linear Position 6.50 18.00
80 Counting 33.44 41.56
SingleObject 95.00 93.75
60 TwoObjects 41.41 52.53
Overallscore 43.27 49.78
10 20 30 40 50
number of sampling steps Table4.Improved Captions. Using a 50/50 mixing ratio of
synthetic (via CogVLM (Wang et al., 2023)) and original cap-
Figure3. Rectifiedflowsaresampleefficient. RectifiedFlows
tions improves text-to-image performance. Assessed via the
performbetterthenotherformulationswhensamplingfewersteps.
GenEval(Ghoshetal.,2023)benchmark.
For25andmoresteps,onlyrf/lognorm(0.00, 1.00)re-
mainscompetitivetoeps/linear.
Metric 4chn 8chn 16chn nature of the human-generated captions that come with
FID(↓) 2.41 1.56 1.06 large-scaleimagedatasets,whichoverlyfocusontheimage
PerceptualSimilarity(↓) 0.85 0.68 0.45 subjectandusuallyomitdetailsdescribingthebackground
SSIM(↑) 0.75 0.79 0.86 or composition of the scene, or, if applicable, displayed
PSNR(↑) 25.12 26.40 28.62
text (Betker et al., 2023). We follow their approach and
Table3.ImprovedAutoencoders. Reconstructionperformance useanoff-the-shelf,state-of-the-artvision-languagemodel,
metricsfordifferentchannelconfigurations.Thedownsampling CogVLM(Wangetal.,2023),tocreatesyntheticannotations
factorforallmodelsisf =8. forourlarge-scaleimagedataset. Assyntheticcaptionsmay
causeatext-to-imagemodeltoforgetaboutcertainconcepts
cordingly,thefinalperformanceofouralgorithmdepends notpresentintheVLM’sknowledgecorpus,weusearatio
notonlyonthetrainingformulation,butalsoontheparame- of50%originaland50%syntheticcaptions.
terizationviaaneuralnetworkandthequalityoftheimage
Toassesstheeffectoftrainingonthiscaptionmix,wetrain
andtextrepresentationsweuse. Inthefollowingsections,
twod = 15MM-DiT modelsfor250ksteps,oneononly
wedescribehowweimproveallthesecomponentsbefore
originalcaptionsandtheotheronthe50/50mix. Weevalu-
scalingourfinalmethodinSection5.3.
atethetrainedmodelsusingtheGenEvalbenchmark(Ghosh
et al., 2023) in Table 4. The results demonstrate that the
5.2.1.IMPROVEDAUTOENCODERS
modeltrainedwiththeadditionofsyntheticcaptionsclearly
Latentdiffusionmodelsachievehighefficiencybyoperating outperformsthemodelthatonlyutilizesoriginalcaptions.
inthelatentspaceofapretrainedautoencoder(Rombach Wethususethe50/50synthetic/originalcaptionmixforthe
etal.,2022),whichmapsaninputRGBX ∈RH×W×3into remainderofthiswork.
a lower-dimensional space x = E(X) ∈ Rh×w×d. The
reconstructionqualityofthisautoencoderprovidesanupper 5.2.3.IMPROVEDTEXT-TO-IMAGEBACKBONES
boundontheachievableimagequalityafterlatentdiffusion
In this section, we compare the performance of existing
training. SimilartoDaietal.(2023),wefindthatincreasing
transformer-baseddiffusionbackboneswithournovelmul-
thenumberoflatentchannelsdsignificantlyboostsrecon-
timodaltransformer-baseddiffusionbackbone,MM-DiT,as
structionperformance,seeTable3. Intuitively,predicting
introducedinSection4. MM-DiT isspecificallydesignedto
latentswithhigherdisamoredifficulttask,andthusmod-
handledifferentdomains,heretextandimagetokens,using
elswithincreasedcapacityshouldbeabletoperformbetter
(two)differentsetsoftrainablemodelweights. Morespecif-
forlargerd,ultimatelyachievinghigherimagequality. We
ically,wefollowtheexperimentalsetupfromSection5.1
confirmthishypothesisinFigure10,whereweseethatthe
andcomparetext-to-imageperformanceonCC12MofDiT,
d=16autoencoderexhibitsbetterscalingperformancein
CrossDiT(DiTbutwithcross-attendingtothetexttokens
termsofsampleFID.Fortheremainderofthispaper,we
insteadofsequence-wiseconcatenation(Chenetal.,2023))
thuschoosed=16.
andourMM-DiT.ForMM-DiT,wecomparemodelswith
twosetsofweightsandthreesetsofweights,wherethelat-
5.2.2.IMPROVEDCAPTIONS
terhandlestheCLIP(Radfordetal.,2021)andT5(Raffel
Betker et al. (2023) demonstrated that synthetically gen- etal.,2019)tokens(c.f.Section4)separately.NotethatDiT
eratedcaptionscangreatlyimprovetext-to-imagemodels (w/concatenationoftextandimagetokensasinSection4)
trained at scale. This is due to the oftentimes simplistic canbeinterpretedasaspecialcaseofMM-DiT withone
7ScalingRectifiedFlowTransformersforHigh-ResolutionImageSynthesis
aspaceelevator, Acheeseburgerwithjuicy aholeinthefloorofmy asmallofficemadeoutofcar Thisdreamlikedigitalart humanlifedepictedentirely anorigamipigonfire
cinematicscifiart beefpattiesandmelted bathroomwithsmall parts capturesavibrant, outoffractals inthemiddleofa
cheesesitsontopofatoilet gremlinslivinginit kaleidoscopicbirdinalush darkroomwitha
thatlookslikeathroneand rainforest. pentagramonthe
standsinthemiddleofthe floor
royalchamber.
anoldrustedrobotwearingpantsandajacketridingskisinasupermarket. smilingcartoondogsitsatatable,coffeemugonhand,asaroomgoesupinflames.“Thisisfine,”the
dogassureshimself.
Awhimsicalandcreativeimagedepictingahybridcreaturethatisamixofawaffleandahippopotamus.Thisimaginativecreaturefeaturesthedistinctive,bulkybodyofahippo,butwithatextureand
appearanceresemblingagolden-brown,crispywaffle.Thecreaturemighthaveelementslikewafflesquaresacrossitsskinandasyrup-likesheen.It’ssetinasurrealenvironmentthatplayfullycombinesa
naturalwaterhabitatofahippowithelementsofabreakfasttablesetting,possiblyincludingoversizedutensilsorplatesinthebackground.Theimageshouldevokeasenseofplayfulabsurdityandculinary
fantasy.
8ScalingRectifiedFlowTransformersforHigh-ResolutionImageSynthesis
Figure4.Trainingdynamicsofmodelarchitectures.Compara- Figure5. EffectsofQK-normalization.NormalizingtheQ-and
tiveanalysisofDiT,CrossDiT,UViT,andMM-DiT onCC12M, K-embeddingsbeforecalculatingtheattentionmatrixpreventsthe
focusingonvalidationloss,CLIPscore,andFID.Ourproposed attention-logitgrowthinstability(left),whichcausestheattention
MM-DiTperformsfavorablyacrossallmetrics. entropytocollapse(right)andhasbeenpreviouslyreportedinthe
discriminativeViTliterature(Dehghanietal.,2023;Wortsman
etal.,2023). Incontrastwiththesepreviousworks,weobserve
sharedsetofweightsforallmodalities. Finally,weconsider
thisinstabilityinthelasttransformerblocksofournetworks.Max-
theUViT(Hoogeboometal.,2023)architectureasahybrid imumattentionlogitsandattentionentropiesareshownaveraged
betweenthewidelyusedUNetsandtransformervariants. overthelast5blocksofa2B(d=24)model.
Weanalyzetheconvergencebehaviorofthesearchitectures
inFigure4: VanillaDiTunderperformsUViT.Thecross- 5.3.2.FINETUNINGONHIGHRESOLUTIONS
attentionDiTvariantCrossDiTachievesbetterperformance
thanUViT,althoughUViTseemstolearnmuchfasterini- QK-Normalization Ingeneral,wepretrainallofourmod-
tially. OurMM-DiT variantsignificantlyoutperformsthe elsonlow-resolutionimagesofsize2562pixels. Next,we
cross-attentionandvanillavariants.Weobserveonlyasmall finetune our models on higher resolutions with mixed as-
gainwhenusingthreeparametersetsinsteadoftwo(atthe pect ratios (see next paragraph for details). We find that,
costofincreasedparametercountandVRAMusage),and when moving to high resolutions, mixed precision train-
thusoptfortheformeroptionfortheremainderofthiswork. ing can become unstable and the loss diverges. This can
beremediedbyswitchingtofullprecisiontraining—but
comeswitha∼2×performancedropcomparedtomixed-
5.3.TrainingatScale
precisiontraining. Amoreefficientalternativeisreported
Beforescalingup,wefilterandpreencodeourdatatoensure inthe(discriminative)ViTliterature: Dehghanietal.(2023)
safeandefficientpretraining. Then,allpreviousconsider- observethatthetrainingoflargevisiontransformermodels
ations of diffusion formulations, architectures, and data divergesbecausetheattentionentropygrowsuncontrollably.
culminateinthelastsection,wherewescaleourmodelsup Toavoidthis,Dehghanietal.(2023)proposetonormalize
to8Bparameters. Q and K before the attention operation. We follow this
approach and use RMSNorm (Zhang & Sennrich, 2019)
5.3.1.DATAPREPROCESSING withlearnablescaleinbothstreamsofourMMDiTarchi-
tectureforourmodels, seeFigure2. Asdemonstratedin
Pre-TrainingMitigations Trainingdatasignificantlyim-
Figure5,theadditionalnormalizationpreventstheattention
pacts a generative model’s abilities. Consequently, data
logitgrowthinstability,confirmingfindingsbyDehghani
filtering is effective at constraining undesirable capabili-
etal.(2023)andWortsmanetal.(2023)andenablesefficient
ties (Nichol, 2022). Before training at sale, we filter our
trainingatbf16-mixed(Chenetal.,2019)precisionwhen
data for the following categories: (i) Sexual content: We
combined with ϵ = 10−15 in the AdamW (Loshchilov &
use NSFW-detection models to filter for explicit content.
Hutter,2017)optimizer. Thistechniquecanalsobeapplied
(ii) Aesthetics: We remove images for which our rating
onpretrainedmodelsthathavenotusedqk-normalization
systemspredictalowscore. (iii)Regurgitation: Weusea
duringpretraining: Themodelquicklyadaptstotheaddi-
cluster-baseddeduplicationmethodtoremoveperceptual
tionalnormalizationlayersandtrainsmorestably. Finally,
and semantic duplicates from the training data; see Ap-
we would like to point out that although this method can
pendixE.2.
generallyhelptostabilizethetrainingoflargemodels,itis
notauniversalrecipeandmayneedtobeadapteddepending
PrecomputingImageandTextEmbeddings Ourmodel ontheexacttrainingsetup.
usestheoutputofmultiplepretrained,frozennetworksasin-
puts(autoencoderlatentsandtextencoderrepresentations). PositionalEncodingsforVaryingAspectRatios After
Sincetheseoutputsareconstantduringtraining,weprecom- trainingonafixed256×256resolutionweaimto(i)in-
putethemoncefortheentiredataset. Weprovideadetailed creasetheresolutionandresolutionand(ii)enableinference
discussionofourapproachinAppendixE.1. withflexibleaspectratios. Sinceweuse2dpositionalfre-
9ScalingRectifiedFlowTransformersforHigh-ResolutionImageSynthesis
Figure6.Timestepshiftingathigherresolutions.Topright:Hu-
manqualitypreferenceratingwhenapplyingtheshiftingbased Figure7. Human Preference Evaluation against currrent
onEquation(23). Bottomrow: A5122 modeltrainedandsam- closedandopenSOTAgenerativeimagemodels.Our8Bmodel
(cid:112) (cid:112)
pledwith m/n = 1.0(top)and m/n = 3.0(bottom). See comparesfavorableagainstcurrentstate-of-the-arttext-to-image
Section5.3.2. models when evaluated on the parti-prompts (Yu et al., 2022)
acrossthecategoriesvisualquality,promptfollowingandtypogra-
phygeneration.
quency embeddings we have to adapt them based on the
resolution. Inthemulti-aspectratiosetting,adirectinter- (cid:113)
σ(t,n)= t 1 (becausethestandarderrorofthemean
polationoftheembeddingsasin(Dosovitskiyetal.,2020) 1−t n
wouldnotreflectthesidelengthscorrectly. Insteadweuse forY hasdeviation √t ). Soifonealreadyknowsthatthe
n
acombinationofextendedandinterpolatedpositiongrids imagez wasconstantacrossitspixels,σ(t,n)represents
0
whicharesubsequentlyfrequencyembedded. thedegreeofuncertaintyaboutz . Forexample,weimme-
0
diatelyseethatdoublingthewidthandheightleadstohalf
ForatargetresolutionofS2 pixels,weusebucketedsam-
theuncertaintyatanygiventime0 < t < 1. But,wecan
pling(NovelAI,2022;Podelletal.,2023)suchthatthateach
nowmapatimestept atresolutionntoatimestept at
batchconsistsofimagesofahomogeneoussizeH ×W, n m
resolutionmthatresultsinthesamedegreeofuncertainty
where H · W ≈ S2. For the maximum and minimum
viatheansatzσ(t ,n)=σ(t ,m). Solvingfort gives
trainingaspectratios,thisresultsinthemaximumvaluesfor n m m
w
h
mid axth =,W Hma mx a, xa /n 1d 6h ,wei mgh axt, =H m Wax m, ath x/a 1tw 6i all nb de sen =co Su /nt 1e 6re bd e.L the et
t m =
1+((cid:112)(cid:112)m mnt −n
1)t
(23)
correspondingsizesinlatentspace(afactor8)afterpatching n n
(afactor2). Basedonthesevalues,weconstructavertical WevisualizethisshiftingfunctioninFigure6.Notethatthe
positiongridwiththevalues((p− hmax−s)· 256)hmax−1and assumptionofconstantimagesisnotrealistic. Tofindgood
2 S p=0
correspondinglyforthehorizontalpositions.Wethencenter- valuesfortheshiftvalueα :=
(cid:112)m
duringinference,we
n
cropfromtheresultingpositional2dgridbeforeembedding applythemtothesamplingstepsofamodeltrainedatreso-
it. lution1024×1024andrunahumanpreferencestudy. The
resultsinFigure6showastrongpreferenceforsampleswith
shiftsgreaterthan1.5butlessdrasticdifferencesamongthe
Resolution-dependentshiftingoftimestepschedules In-
highershiftvalues. Inoursubsequentexperiments,wethus
tuitively,sincehigherresolutionshavemorepixels,weneed
useashiftvalueofα=3.0bothduringtrainingandsam-
morenoisetodestroytheirsignal. Assumeweareworking
plingatresolution1024×1024. Aqualitativecomparison
in a resolution with n = H ·W pixels. Now, consider a
betweensamplesafter8ktrainingstepswithandwithout
”constant”image,i.e. onewhereeverypixelhasthevalue
such a shift can be found in Figure 6. Finally, note that
c. The forward process produces z t = (1 − t)c1 + tϵ, Equation 23 implies a log-SNR shift of log n similar to
whereboth1andϵ∈Rn. Thus,z providesnobservations m
t (Hoogeboometal.,2023):
of the random variable Y = (1−t)c+tη with c and η
inR,andη followsastandardnormaldistribution. Thus, λ =2log 1−t n (24)
E(Y)=(1−t)candσ(Y)=t. Wecanthereforerecover tm (cid:112)mt
n n
cviac = 1 E(Y), andtheerrorbetweencanditssam- m
1−t =λ −2logα=λ −log . (25)
pleestimatecˆ= 1 (cid:80)n z hasastandarddeviationof tn tn n
1−t i=1 t,i
10ScalingRectifiedFlowTransformersforHigh-ResolutionImageSynthesis
Aftertheshiftedtrainingatresolution1024×1024,wealign Objects
Color
themodelusingDirectPreferenceOptimization(DPO)as Model OverallSingleTwoCountingColorsPositionAttribution
minDALL-E 0.23 0.73 0.11 0.12 0.37 0.02 0.01
describedinAppendixC.
SDv1.5 0.43 0.97 0.38 0.35 0.76 0.04 0.06
PixArt-alpha 0.48 0.98 0.50 0.44 0.80 0.08 0.07
SDv2.1 0.50 0.98 0.51 0.44 0.85 0.07 0.17
5.3.3.RESULTS DALL-E2 0.52 0.94 0.66 0.49 0.77 0.10 0.19
SDXL 0.55 0.98 0.74 0.39 0.85 0.15 0.23
SDXLTurbo 0.55 1.00 0.72 0.49 0.80 0.10 0.18
InFigure8,weexaminetheeffectoftrainingourMM-DiT IF-XL 0.61 0.97 0.74 0.66 0.81 0.13 0.35
DALL-E3 0.67 0.96 0.87 0.47 0.83 0.43 0.45
atscale. Forimages,weconductalargescalingstudyand
Ours(depth=18),5122 0.58 0.97 0.72 0.52 0.78 0.16 0.34
trainmodelswithdifferentnumbersofparametersfor500k Ours(depth=24),5122 0.62 0.98 0.74 0.63 0.67 0.34 0.36
stepson2562pixelsresolutionusingpreencodeddata,c.f. Ours(depth=30),5122 0.64 0.96 0.80 0.65 0.73 0.33 0.37
Ours(depth=38),5122 0.68 0.98 0.84 0.66 0.74 0.40 0.43
AppendixE.1,withabatchsizeof4096. Wetrainon2×2 Ours(depth=38),5122w/DPO 0.71 0.98 0.89 0.73 0.83 0.34 0.47
Ours(depth=38),10242w/DPO 0.74 0.99 0.94 0.72 0.89 0.33 0.60
patches(Peebles&Xie,2023),andreportvalidationlosses
ontheCoCodataset(Linetal.,2014)every50ksteps. In Table5.GenEvalcomparisons. Ourlargestmodel(depth=38)
particular,toreducenoiseinthevalidationlosssignal,we outperformsallcurrentopenmodelsandDALLE-3(Betkeretal.,
sample loss levels equidistant in t ∈ (0,1) and compute 2023)onGenEval(Ghoshetal.,2023). Wehighlightthebest,
validationlossforeachlevelseparately. Wethenaverage secondbest,andthirdbestentries.ForDPO,seeAppendixC.
thelossacrossallbutthelast(t=1)levels.
Similarly, we conduct a preliminary scaling study of our relativeCLIPscoredecrease[%]
MM-DiT onvideos. Tothisendwestartfromthepretrained 5/50steps 10/50steps 20/50steps pathlength
imageweightsandadditionallyusea2xtemporalpatching.
depth=15 4.30 0.86 0.21 191.13
We follow Blattmann et al. (2023b) and feed data to the depth=30 3.59 0.70 0.24 187.96
pretrainedmodelbycollapsingthetemporalintothebatch depth=38 2.71 0.14 0.08 185.96
axis. Ineachattentionlayerwerearrangetherepresentation
inthevisualstreamandaddafullattentionoverallspatio- Table6.Impactofmodelsizeonsamplingefficiency.Thetable
temporaltokensafterthespatialattentionoperationbefore showstherelativeperformancedecreaserelativetoCLIPscores
evaluatedusing50samplingstepsatafixedseed.Largermodels
thefinalfeedforwardlayer.Ourvideomodelsaretrainedfor
canbesampledusingfewersteps,whichweattributetoincreased
140kstepswithabatchsizeof512onvideoscomprising
robustnessandbetterfittingthestraight-pathobjectiveofrectified
16frameswith2562pixels. Wereportvalidationlosseson
flow models, resulting in shorter path lengths. Path length is
theKineticsdataset(Carreira&Zisserman,2018)every5k
calculatedbysummingup∥v ·dt∥over50steps.
θ
steps. NotethatourreportedFLOPsforvideotrainingin
Figure 8 are only FLOPs from video training and do not
Parti-promptsbenchmark(Yuetal.,2022)inthecategories
includetheFLOPsfromimagepretraining.
visualaesthetics,promptfollowingandtypographygener-
Forboththeimageandvideodomains,weobserveasmooth ation, c.f. Figure 7. For evaluating human preference in
decreaseinthevalidationlosswhenincreasingmodelsize thesecategories,raterswereshownpairwiseoutputsfrom
andtrainingsteps. Wefindthevalidationlosstobehighly twomodels,andaskedtoanswerthefollowingquestions:
correlated to comprehensive evaluation metrics (Comp- Promptfollowing: Whichimagelooksmorerepresentative
Bench(Huangetal.,2023),GenEval(Ghoshetal.,2023)) tothetextshownaboveandfaithfullyfollowsit?
andtohumanpreference. Theseresultssupportthevalida- Visual aesthetics: Given the prompt, which image is of
tionlossasasimpleandgeneralmeasureofmodelperfor- higher-qualityandaestheticallymorepleasing?
mance. Ourresultsdonotshowsaturationneitherforimage Typography: Whichimagemoreaccuratelyshows/displays
notforvideomodels. thetextspecifiedintheabovedescription? Moreaccurate
spellingispreferred! Ignoreotheraspects.
Figure12illustrateshowtrainingalargermodelforlonger
impactssamplequality. Tab.5showstheresultsofGenEval Lastly,Table6highlightsanintriguingresult: notonlydo
in full. When applying the methods presented in Sec- biggermodelsperformbetter,theyalsorequirefewersteps
tion 5.3.2 and increasing training image resolution, our toreachtheirpeakperformance.
biggestmodelexcelsinmostcategoriesandoutperforms
DALLE3(Betkeretal.,2023),thecurrentstateoftheartin
FlexibleTextEncoders Whilethemainmotivationfor
promptcomprehension,inoverallscore.
usingmultipletext-encodersisboostingtheoverallmodel
Ourd=38modeloutperformscurrentproprietary(Betker performance (Balaji et al., 2022), we now show that this
et al., 2023; ide, 2024) and open (Sauer et al., 2023; pla, choiceadditionallyincreasestheflexibilityofourMM-DiT-
2024;Chenetal.,2023;Perniasetal.,2023)SOTAgener- basedrectifiedflowduringinference. AsdescribedinAp-
ativeimagemodelsinhumanpreferenceevaluationonthe pendixB.3wetrainourmodelwiththreetextencoders,with
anindividualdrop-outrateof46.3%. Hence,atinference
11ScalingRectifiedFlowTransformersforHigh-ResolutionImageSynthesis
Figure8. Quantitative effects of scaling. We analyze the impact of model size on performance, maintaining consistent training
hyperparametersthroughout. Anexceptionisdepth=38,wherelearningrateadjustmentsat3×105 stepswerenecessarytoprevent
divergence.(Top)Validationlosssmoothlydecreasesasafunctionofbothmodelsizeandtrainingstepsforbothimage(columns1and2)
andvideomodels(columns3and4).(Bottom)Validationlossisastrongpredictorofoverallmodelperformance.Thereisamarked
correlationbetweenvalidationlossandholisticimageevaluationmetrics,includingGenEval(Ghoshetal.,2023),column1,human
preference,column2,andT2I-CompBench(Huangetal.,2023),column3.Forvideomodelsweobserveasimilarcorrelationbetween
validationlossandhumanpreference,column4..
Alltext-encoders w/oT5(Raffeletal.,2019) ingeitherhighlydetaileddescriptionsofasceneorlarger
amountsofwrittentextdowefindsignificantperformance
gains when using all three text-encoders. These observa-
tionsarealsoverifiedinthehumanpreferenceevaluation
results in Figure 7 (Ours w/o T5). Removing T5 has no
“Aburgerpatty,withthebottombunandlettuceandtomatoes.”COFFEE”writtenonitinmustard” effectonaestheticqualityratings(50%winrate),andonlya
smallimpactonpromptadherence(46%winrate),whereas
itscontributiontothecapabilitiesofgeneratingwrittentext
aremoresignificant(38%winrate).
“Amonkeyholdingasignreading”Scalingtransformermodelsisawesome!”
6.Conclusion
In this work, we presented a scaling analysis of rectified
flow models for text-to-image synthesis. We proposed a
“Amischievousferretwithaplayfulgrinsqueezesitselfintoalargeglassjar,surroundedby noveltimestepsamplingforrectifiedflowtrainingthatim-
colorfulcandy.Thejarsitsonawoodentableinacozykitchen,andwarmsunlightfilters
throughanearbywindow” proves over previous diffusion training formulations for
Figure9. Impact of T5. We observe T5 to be important for latentdiffusionmodelsandretainsthefavourableproper-
complexpromptse.g. suchinvolvingahighdegreeofdetailor tiesofrectifiedflowsinthefew-stepsamplingregime. We
longerspelledtext(rows2and3).Formostprompts,however,we alsodemonstratedtheadvantagesofourtransformer-based
findthatremovingT5atinferencetimestillachievescompetitive MM-DiT architecturethattakesthemulti-modalnatureof
performance. thetext-to-imagetaskintoaccount. Finally,weperformed
ascalingstudyofthiscombinationuptoamodelsizeof
time,wecanuseanarbitrarysubsetofallthreetextencoders. 8Bparametersand5×1022 trainingFLOPs. Weshowed
Thisoffersmeansfortradingoffmodelperformanceforim- thatvalidationlossimprovementscorrelatewithbothexist-
proved memory efficiency, which is particularly relevant ingtext-to-imagebenchmarksaswellashumanpreference
for the 4.7B parameters of T5-XXL (Raffel et al., 2019) evaluations. This,incombinationwithourimprovementsin
thatrequiresignificantamountsofVRAM.Interestingly,we generativemodelingandscalable,multimodalarchitectures
observelimitedperformancedropswhenusingonlythetwo achievesperformancethatiscompetitivewithstate-of-the-
CLIP-basedtext-encodersforthetextpromptsandreplac- artproprietarymodels. Thescalingtrendshowsnosignsof
ingtheT5embeddingsbyzeros. Weprovideaqualitative saturation,whichmakesusoptimisticthatwecancontinue
visualizationinFigure9. Onlyforcomplexpromptsinvolv- toimprovetheperformanceofourmodelsinthefuture.
12ScalingRectifiedFlowTransformersforHigh-ResolutionImageSynthesis
BroaderImpact USENIXSecuritySymposium(USENIXSecurity23),pp.
5253–5270,2023.
Thispaperpresentsworkwhosegoalistoadvancethefield
ofmachinelearningingeneralandimagesynthesisinpar- Carreira,J.andZisserman,A. Quovadis,actionrecogni-
ticular. There are many potential societal consequences tion? anewmodelandthekineticsdataset,2018.
of our work, none of which we feel must be specifically
highlighted here. For an extensive discussion of the gen- Changpinyo, S., Sharma, P. K., Ding, N., and Soricut,
eralramificationsofdiffusionmodels,wepointinterested R. Conceptual12m: Pushingweb-scaleimage-textpre-
readerstowards(Poetal.,2023). training to recognize long-tail visual concepts. 2021
IEEE/CVF Conference on Computer Vision and Pat-
ternRecognition(CVPR),pp.3557–3567, 2021. URL
References
https://api.semanticscholar.org/Corp
Ideogramv1.0announcement,2024. URLhttps://ab usID:231951742.
out.ideogram.ai/1.0.
Chen, D., Chou, C., Xu, Y., and Hseu, J. Bfloat16: The
Playgroundv2.5announcement,2024.URLhttps://bl secret to high performance on cloud tpus, 2019. URL
og.playgroundai.com/playground-v2-5/. https://cloud.google.com/blog/produc
ts/ai-machine-learning/bfloat16-the-s
Albergo,M.S.andVanden-Eijnden,E. Buildingnormaliz- ecret-to-high-performance-on-cloud-t
ingflowswithstochasticinterpolants,2022. pus?hl=en.
Atchison,J.andShen,S.M. Logistic-normaldistributions:
Chen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang,
Somepropertiesanduses. Biometrika,67(2):261–272,
Z.,Kwok,J.,Luo,P.,Lu,H.,andLi,Z. Pixart-a: Fast
1980.
trainingofdiffusiontransformerforphotorealistictext-
to-imagesynthesis,2023.
autofaiss. autofaiss, 2023. URLhttps://github.c
om/criteo/autofaiss.
Chen,T.Q.,Rubanova,Y.,Bettencourt,J.,andDuvenaud,
D.K. Neuralordinarydifferentialequations. InNeural
Balaji,Y.,Nah,S.,Huang,X.,Vahdat,A.,Song,J.,Zhang,
Information Processing Systems, 2018. URL https:
Q.,Kreis,K.,Aittala,M.,Aila,T.,Laine,S.,Catanzaro,
//api.semanticscholar.org/CorpusID:49
B., Karras, T., and Liu, M.-Y. ediff-i: Text-to-image
310446.
diffusionmodelswithanensembleofexpertdenoisers,
2022.
Cherti, M., Beaumont, R., Wightman, R., Wortsman, M.,
Ilharco, G., Gordon, C., Schuhmann, C., Schmidt, L.,
Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L.,
andJitsev,J. Reproduciblescalinglawsforcontrastive
Ouyang,L.,Zhuang,J.,Lee,J.,Guo,Y.,etal. Improving
language-imagelearning. In2023IEEE/CVFConference
imagegenerationwithbettercaptions. ComputerScience.
on Computer Vision and Pattern Recognition (CVPR).
https://cdn.openai.com/papers/dall-e-3.pdf,2(3),2023.
IEEE,2023. doi: 10.1109/cvpr52729.2023.00276. URL
Blattmann,A.,Dockhorn,T.,Kulal,S.,Mendelevitch,D., http://dx.doi.org/10.1109/CVPR52729.2
Kilian,M.,Lorenz,D.,Levi,Y.,English,Z.,Voleti,V., 023.00276.
Letts, A., et al. Stable video diffusion: Scaling latent
Dai,X.,Hou,J.,Ma,C.-Y.,Tsai,S.,Wang,J.,Wang,R.,
videodiffusionmodelstolargedatasets. arXivpreprint
Zhang,P.,Vandenhende,S.,Wang,X.,Dubey,A.,Yu,M.,
arXiv:2311.15127,2023a.
Kadian,A.,Radenovic,F.,Mahajan,D.,Li,K.,Zhao,Y.,
Blattmann,A.,Rombach,R.,Ling,H.,Dockhorn,T.,Kim, Petrovic,V.,Singh,M.K.,Motwani,S.,Wen,Y.,Song,
S.W.,Fidler,S.,andKreis,K. Alignyourlatents: High- Y.,Sumbaly,R.,Ramanathan,V.,He,Z.,Vajda,P.,and
resolutionvideosynthesiswithlatentdiffusionmodels, Parikh, D. Emu: Enhancing image generation models
2023b. usingphotogenicneedlesinahaystack,2023.
Brooks,T.,Holynski,A.,andEfros,A.A. Instructpix2pix: Dao,Q.,Phung,H.,Nguyen,B.,andTran,A. Flowmatch-
Learningtofollowimageeditinginstructions.InProceed- inginlatentspace,2023.
ingsoftheIEEE/CVFConferenceonComputerVision
Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P.,
andPatternRecognition,pp.18392–18402,2023.
Heek,J.,Gilmer,J.,Steiner,A.,Caron,M.,Geirhos,R.,
Carlini, N., Hayes, J., Nasr, M., Jagielski, M., Sehwag, Alabdulmohsin, I., Jenatton, R., Beyer, L., Tschannen,
V., Tramer, F., Balle, B., Ippolito, D., and Wallace, E. M., Arnab, A., Wang, X., Riquelme, C., Minderer, M.,
Extractingtrainingdatafromdiffusionmodels. In32nd Puigcerver,J.,Evci,U.,Kumar,M.,vanSteenkiste,S.,
13ScalingRectifiedFlowTransformersforHigh-ResolutionImageSynthesis
Elsayed,G.F.,Mahendran,A.,Yu,F.,Oliver,A.,Huot, Ho,J.,Jain,A.,andAbbeel,P. Denoisingdiffusionproba-
F., Bastings, J., Collier, M.P., Gritsenko, A., Birodkar, bilisticmodels,2020.
V., Vasconcelos, C., Tay, Y., Mensink, T., Kolesnikov,
Ho,J.,Chan,W.,Saharia,C.,Whang,J.,Gao,R.,Gritsenko,
A., Pavetic´, F., Tran, D., Kipf, T., Lucˇic´, M., Zhai, X.,
A.,Kingma,D.P.,Poole,B.,Norouzi,M.,Fleet,D.J.,
Keysers,D.,Harmsen,J.,andHoulsby,N. Scalingvision
andSalimans, T. Imagenvideo: Highdefinitionvideo
transformersto22billionparameters,2023.
generationwithdiffusionmodels,2022.
Dhariwal,P.andNichol,A. Diffusionmodelsbeatganson
imagesynthesis,2021. Hoogeboom,E.,Heek,J.,andSalimans,T.Simplediffusion:
End-to-enddiffusionforhighresolutionimages,2023.
Dockhorn,T.,Vahdat,A.,andKreis,K. Score-basedgener-
ativemodelingwithcritically-dampedlangevindiffusion. Huang, K., Sun, K., Xie, E., Li, Z., and Liu, X. T2i-
arXivpreprintarXiv:2112.07068,2021. compbench:Acomprehensivebenchmarkforopen-world
compositionaltext-to-imagegeneration. arXivpreprint
Dockhorn, T., Vahdat, A., and Kreis, K. Genie: Higher- arXiv:2307.06350,2023.
orderdenoisingdiffusionsolvers,2022.
Hyva¨rinen, A. Estimation of non-normalized statistical
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
modelsbyscorematching. J.Mach.Learn.Res.,6:695–
D.,Zhai,X.,Unterthiner,T.,Dehghani,M.,Minderer,M.,
709,2005. URLhttps://api.semanticschola
Heigold,G.,Gelly,S.,etal. Animageisworth16x16
r.org/CorpusID:1152227.
words:Transformersforimagerecognitionatscale.ICLR,
2020. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,
Chess,B.,Child,R.,Gray,S.,Radford,A.,Wu,J.,and
Esser,P.,Chiu,J.,Atighehchian,P.,Granskog,J.,andGer-
Amodei, D. Scaling laws for neural language models,
manidis,A. Structureandcontent-guidedvideosynthesis
2020.
withdiffusionmodels,2023.
Karras,T.,Aittala,M.,Aila,T.,andLaine,S. Elucidating
Euler,L. Institutionumcalculiintegralis. NumberBd.1in
the design space of diffusion-based generative models.
Institutionumcalculiintegralis.imp.Acad.imp.Sae`nt.,
ArXiv,abs/2206.00364,2022.URLhttps://api.se
1768. URL https://books.google.de/book
manticscholar.org/CorpusID:249240415.
s?id=Vg8OAAAAQAAJ.
Karras, T., Aittala, M., Lehtinen, J., Hellsten, J., Aila,
Fischer,J.S.,Gui,M.,Ma,P.,Stracke,N.,Baumann,S.A.,
T., and Laine, S. Analyzing and improving the train-
andOmmer,B.Boostinglatentdiffusionwithflowmatch-
ing dynamics of diffusion models. arXiv preprint
ing. arXivpreprintarXiv:2312.07360,2023.
arXiv:2312.02696,2023.
Ghosh, D., Hajishirzi, H., and Schmidt, L. Geneval: An
Kingma, D. P. and Gao, R. Understanding diffusion ob-
object-focusedframeworkforevaluatingtext-to-image
jectives as the elbo with simple data augmentation. In
alignment. arXivpreprintarXiv:2310.11513,2023.
Thirty-seventh Conference on Neural Information Pro-
Gupta,A.,Yu,L.,Sohn,K.,Gu,X.,Hahn,M.,Fei-Fei,L., cessingSystems,2023.
Essa,I.,Jiang,L.,andLezama,J. Photorealisticvideo
generationwithdiffusionmodels,2023. Lee, K., Ippolito, D., Nystrom, A., Zhang, C., Eck, D.,
Callison-Burch,C.,andCarlini,N. Deduplicatingtrain-
Hessel, J., Holtzman, A., Forbes, M., Le Bras, R., and ingdatamakeslanguagemodelsbetter. arXivpreprint
Choi,Y.Clipscore:Areference-freeevaluationmetricfor arXiv:2107.06499,2021.
imagecaptioning. InProceedingsofthe2021Conference
onEmpiricalMethodsinNaturalLanguageProcessing. Lee,S.,Kim,B.,andYe,J.C. Minimizingtrajectorycurva-
Association for Computational Linguistics, 2021. doi: tureofode-basedgenerativemodels,2023.
10.18653/v1/2021.emnlp-main.595. URLhttp://dx
Lin,S.,Liu,B.,Li,J.,andYang,X.Commondiffusionnoise
.doi.org/10.18653/v1/2021.emnlp-main.
schedulesandsamplestepsareflawed. InProceedings
595.
oftheIEEE/CVFWinterConferenceonApplicationsof
Heusel,M.,Ramsauer,H.,Unterthiner,T.,Nessler,B.,and ComputerVision,pp.5404–5411,2024.
Hochreiter,S. Ganstrainedbyatwotime-scaleupdate
Lin,T.-Y.,Maire,M.,Belongie,S.,Hays,J.,Perona,P.,Ra-
ruleconvergetoalocalnashequilibrium,2017.
manan,D.,Dolla´r,P.,andZitnick,C.L.MicrosoftCOCO:
Ho,J.andSalimans,T. Classifier-freediffusionguidance, CommonObjectsinContext,pp.740–755. SpringerIn-
2022. ternationalPublishing,2014. ISBN9783319106021. doi:
14ScalingRectifiedFlowTransformersforHigh-ResolutionImageSynthesis
10.1007/978-3-319-10602-1 48. URLhttp://dx.d Po,R.,Yifan,W.,Golyanik,V.,Aberman,K.,Barron,J.T.,
oi.org/10.1007/978-3-319-10602-1_48. Bermano, A. H., Chan, E. R., Dekel, T., Holynski, A.,
Kanazawa,A.,etal. Stateoftheartondiffusionmodels
Lipman,Y.,Chen,R.T.Q.,Ben-Hamu,H.,Nickel,M.,and
forvisualcomputing. arXivpreprintarXiv:2310.07204,
Le,M. Flowmatchingforgenerativemodeling. InThe
2023.
Eleventh International Conference on Learning Repre-
sentations,2023. URLhttps://openreview.net Podell,D.,English,Z.,Lacey,K.,Blattmann,A.,Dockhorn,
/forum?id=PqvMRDCJT9t. T., Mu¨ller, J., Penna, J., and Rombach, R. Sdxl: Im-
provinglatentdiffusionmodelsforhigh-resolutionimage
Liu, X., Gong, C., and Liu, Q. Flow straight and fast:
synthesis,2023.
Learningtogenerateandtransferdatawithrectifiedflow,
2022. Pooladian, A.-A., Ben-Hamu, H., Domingo-Enrich, C.,
Amos, B., Lipman, Y., and Chen, R. T. Q. Multisam-
Liu,X.,Zhang,X.,Ma,J.,Peng,J.,andLiu,Q. Instaflow:
pleflowmatching: Straighteningflowswithminibatch
Onestepisenoughforhigh-qualitydiffusion-basedtext-
couplings,2023.
to-imagegeneration,2023.
Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,
Loshchilov,I.andHutter,F. Fixingweightdecayregular-
Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,
ization in adam. ArXiv, abs/1711.05101, 2017. URL
J.,Krueger,G.,andSutskever,I. Learningtransferable
https://api.semanticscholar.org/Corp
visualmodelsfromnaturallanguagesupervision,2021.
usID:3312944.
Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Man-
Lu,C.,Zhou,Y.,Bao,F.,Chen,J.,Li,C.,andZhu,J. Dpm-
ning, C. D., and Finn, C. Direct Preference Optimiza-
solver++: Fastsolverforguidedsamplingofdiffusion
tion: YourLanguageModelisSecretlyaRewardModel.
probabilisticmodels,2023.
arXiv:2305.18290,2023.
Ma,N.,Goldstein,M.,Albergo,M.S.,Boffi,N.M.,Vanden-
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Eijnden,E.,andXie,S.Sit:Exploringflowanddiffusion-
Matena,M.,Zhou,Y.,Li,W.,andLiu,P.J. Exploring
basedgenerativemodelswithscalableinterpolanttrans-
thelimitsoftransferlearningwithaunifiedtext-to-text
formers,2024.
transformer,2019.
Nichol, A. Dall-e 2 pre-training mitigations. https:
Ramesh,A.,Dhariwal,P.,Nichol,A.,Chu,C.,andChen,
//openai.com/research/dall-e-2-pre-t
M. Hierarchicaltext-conditionalimagegenerationwith
raining-mitigations,2022.
cliplatents,2022.
Nichol,A.andDhariwal,P. Improveddenoisingdiffusion
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and
probabilisticmodels,2021.
Ommer,B. High-resolutionimagesynthesiswithlatent
diffusion models. In 2022 IEEE/CVF Conference on
NovelAI. Novelaiimprovementsonstablediffusion,2022.
ComputerVisionandPatternRecognition(CVPR).IEEE,
URL https://blog.novelai.net/novelai
2022. doi: 10.1109/cvpr52688.2022.01042. URL
-improvements-on-stable-diffusion-e10
http://dx.doi.org/10.1109/CVPR52688.2
d38db82ac.
022.01042.
Peebles, W. and Xie, S. Scalable diffusion models with
Ronneberger,O.,Fischer,P.,andBrox,T. U-Net: Convolu-
transformers. In 2023 IEEE/CVF International Con-
tionalNetworksforBiomedicalImageSegmentation,pp.
ference on Computer Vision (ICCV). IEEE, 2023. doi:
234–241. SpringerInternationalPublishing,2015. ISBN
10.1109/iccv51070.2023.00387. URLhttp://dx.d
9783319245744. doi: 10.1007/978-3-319-24574-4 28.
oi.org/10.1109/ICCV51070.2023.00387.
URLhttp://dx.doi.org/10.1007/978-3-3
Pernias, P., Rampas, D., Richter, M. L., Pal, C. J., and 19-24574-4_28.
Aubreville,M. Wuerstchen: Anefficientarchitecturefor
Russakovsky,O.,Deng,J.,Su,H.,Krause,J.,Satheesh,S.,
large-scaletext-to-imagediffusionmodels,2023.
Ma,S.,Huang,Z.,Karpathy,A.,Khosla,A.,Bernstein,
Pizzi,E.,Roy,S.D.,Ravindra,S.N.,Goyal,P.,andDouze, M.S.,Berg,A.C.,andFei-Fei,L. Imagenetlargescale
M. Aself-superviseddescriptorforimagecopydetection. visual recognition challenge. International Journal of
In Proceedings of the IEEE/CVF Conference on Com- ComputerVision, 115:211–252, 2014. URLhttps:
puterVisionandPatternRecognition,pp.14532–14542, //api.semanticscholar.org/CorpusID:29
2022. 30547.
15ScalingRectifiedFlowTransformersforHigh-ResolutionImageSynthesis
Saharia,C.,Chan,W.,Chang,H.,Lee,C.,Ho,J.,Salimans, Song, Y., Sohl-Dickstein, J. N., Kingma, D. P., Kumar,
T.,Fleet,D.,andNorouzi,M. Palette: Image-to-image A., Ermon, S., and Poole, B. Score-based generative
diffusionmodels. InACMSIGGRAPH2022Conference modelingthroughstochasticdifferentialequations.ArXiv,
Proceedings,pp.1–10,2022a. abs/2011.13456,2020. URLhttps://api.semant
icscholar.org/CorpusID:227209335.
Saharia,C.,Chan,W.,Saxena,S.,Li,L.,Whang,J.,Den-
ton, E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, Tong,A.,Malkin,N.,Huguet,G.,Zhang,Y.,Rector-Brooks,
S. S., Lopes, R. G., Salimans, T., Ho, J., Fleet, D. J., J.,Fatras,K.,Wolf,G.,andBengio,Y. Improvingand
andNorouzi,M. Photorealistictext-to-imagediffusion generalizing flow-based generative models with mini-
modelswithdeeplanguageunderstanding,2022b. batchoptimaltransport,2023.
Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,
Saharia, C., Ho, J., Chan, W., Salimans, T., Fleet, D. J.,
L.,Gomez,A.N.,Kaiser,L.,andPolosukhin,I.Attention
and Norouzi, M. Image super-resolution via iterative
isallyouneed,2017.
refinement. IEEETransactionsonPatternAnalysisand
MachineIntelligence,45(4):4713–4726,2022c.
Villani, C. Optimaltransport: Oldandnew. 2008. URL
https://api.semanticscholar.org/Corp
Sauer,A.,Chitta,K.,Mu¨ller,J.,andGeiger,A. Projected
usID:118347220.
gans converge faster. Advances in Neural Information
ProcessingSystems,2021. Vincent,P. Aconnectionbetweenscorematchingandde-
noising autoencoders. Neural Computation, 23:1661–
Sauer, A., Lorenz, D., Blattmann, A., and Rombach, 1674, 2011. URLhttps://api.semanticscho
R. Adversarial diffusion distillation. arXiv preprint lar.org/CorpusID:5560643.
arXiv:2311.17042,2023.
Wallace,B.,Dang,M.,Rafailov,R.,Zhou,L.,Lou,A.,Pu-
Sheynin,S.,Polyak,A.,Singer,U.,Kirstain,Y.,Zohar,A., rushwalkam,S.,Ermon,S.,Xiong,C.,Joty,S.,andNaik,
Ashual,O.,Parikh,D.,andTaigman,Y.Emuedit:Precise N. DiffusionModelAlignmentUsingDirectPreference
imageeditingviarecognitionandgenerationtasks. arXiv Optimization. arXiv:2311.12908,2023.
preprintarXiv:2311.10089,2023.
Wang,W.,Lv,Q.,Yu,W.,Hong,W.,Qi,J.,Wang,Y.,Ji,
Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, J.,Yang,Z.,Zhao,L.,Song,X.,etal. Cogvlm: Visual
S.,Hu,Q.,Yang,H.,Ashual,O.,Gafni,O.,Parikh,D., expert for pretrained language models. arXiv preprint
Gupta,S.,andTaigman,Y. Make-a-video: Text-to-video arXiv:2311.03079,2023.
generationwithouttext-videodata,2022.
Wortsman,M.,Liu,P.J.,Xiao,L.,Everett,K.,Alemi,A.,
Adlam,B.,Co-Reyes,J.D.,Gur,I.,Kumar,A.,Novak,
Sohl-Dickstein, J. N., Weiss, E. A., Maheswaranathan,
R., Pennington, J., Sohl-dickstein, J., Xu, K., Lee, J.,
N., andGanguli, S. Deepunsupervisedlearningusing
Gilmer, J., and Kornblith, S. Small-scale proxies for
nonequilibriumthermodynamics. ArXiv,abs/1503.03585,
2015. URL https://api.semanticscholar. large-scaletransformertraininginstabilities,2023.
org/CorpusID:14888175.
Yu,J.,Xu,Y.,Koh,J.Y.,Luong,T.,Baid,G.,Wang,Z.,Va-
sudevan,V.,Ku,A.,Yang,Y.,Ayan,B.K.,etal. Scaling
Somepalli,G.,Singla,V.,Goldblum,M.,Geiping,J.,and
AutoregressiveModelsforContent-RichText-to-Image
Goldstein,T. Diffusionartordigitalforgery? investigat-
Generation. arXiv:2206.10789,2022.
ingdatareplicationindiffusionmodels. InProceedings
of the IEEE/CVF Conference on Computer Vision and Zhai,X.,Kolesnikov,A.,Houlsby,N.,andBeyer,L.Scaling
PatternRecognition,pp.6048–6058,2023a. visiontransformers. InCVPR,pp.12104–12113,2022.
Somepalli,G.,Singla,V.,Goldblum,M.,Geiping,J.,and Zhang,B.andSennrich,R. Rootmeansquarelayernormal-
Goldstein, T. Understanding and mitigating copying ization,2019.
indiffusionmodels. arXivpreprintarXiv:2305.20086,
2023b.
Song, J., Meng, C., and Ermon, S. Denoising diffusion
implicitmodels,2022.
Song,Y.andErmon,S. Generativemodelingbyestimating
gradientsofthedatadistribution,2020.
16ScalingRectifiedFlowTransformersforHigh-ResolutionImageSynthesis
Supplementary
A.Background
DiffusionModels (Sohl-Dicksteinetal.,2015;Songetal.,2020;Hoetal.,2020)generatedatabyapproximatingthe
reverseODEtoastochasticforwardprocesswhichtransformsdatatonoise. Theyhavebecomethestandardapproachfor
generativemodelingofimages(Dhariwal&Nichol,2021;Rameshetal.,2022;Sahariaetal.,2022b;Rombachetal.,2022;
Balajietal.,2022)andvideos(Singeretal.,2022;Hoetal.,2022;Esseretal.,2023;Blattmannetal.,2023b;Guptaetal.,
2023). Sincethesemodelscanbederivedbothviaavariationallowerboundonthenegativelikelihood(Sohl-Dicksteinetal.,
2015)andscorematching(Hyva¨rinen,2005;Vincent,2011;Song&Ermon,2020),variousformulationsofforward-and
reverseprocesses(Songetal.,2020;Dockhornetal.,2021),modelparameterizations(Hoetal.,2020;Ho&Salimans,2022;
Karrasetal.,2022),lossweightings(Hoetal.,2020;Karrasetal.,2022)andODEsolvers(Songetal.,2022;Luetal.,2023;
Dockhornetal.,2022)haveledtoalargenumberofdifferenttrainingobjectivesandsamplingprocedures. Morerecently,
theseminalworksofKingma&Gao(2023)andKarrasetal.(2022)haveproposedunifiedformulationsandintroduced
newtheoreticalandpracticalinsightsfortraining(Karrasetal.,2022;Kingma&Gao,2023)andinference(Karrasetal.,
2022). However, despite these improvements, the trajectories of common ODEs involve partly significant amounts of
curvature(Karrasetal.,2022;Liuetal.,2022),whichrequiresincreasedamountsofsolverstepsand,thus,rendersfast
inferencedifficult. Toovercomethis,weadoptrectifiedflowmodelswhoseformulationallowsforlearningstraightODE
trajectories.
RectifiedFlowModels (Liuetal.,2022;Albergo&Vanden-Eijnden,2022;Lipmanetal.,2023)approachgenerative
modelingbyconstructingatransportmapbetweentwodistributionsthroughanordinarydifferentialequation(ODE).This
approachhascloseconnectionstocontinuousnormalizingflows(CNF)(Chenetal.,2018)aswellasdiffusionmodels.
ComparedtoCNFs,RectifiedFlowsandStochasticInterpolantshavetheadvantagethattheydonotrequiresimulation
oftheODEduringtraining. Comparedtodiffusionmodels,theycanresultinODEsthatarefastertosimulatethanthe
probabilityflowODE(Songetal.,2020)associatedwithdiffusionmodels. Nevertheless,theydonotresultinoptimal
transportsolutions,andmultipleworksaimtominimizethetrajectorycurvaturefurther(Leeetal.,2023;Tongetal.,2023;
Pooladianetal.,2023). (Daoetal.,2023;Maetal.,2024)demonstratethefeasibilityofrectifiedflowformulationsfor
class-conditionalimagesynthesis,(Fischeretal.,2023)forlatent-spaceupsampling,and(Liuetal.,2023)applythereflow
procedureof(Liuetal.,2022)todistillapretrainedtext-to-imagemodel(Rombachetal.,2022). Here,weareinterestedin
rectifiedflowsasthefoundationfortext-to-imagesynthesiswithfewersamplingsteps. Weperformanextensivecomparison
betweendifferentformulationsandlossweightingsandproposeanewtimestepschedulefortrainingofrectifiedflowswith
improvedperformance.
ScalingDiffusionModels Thetransformerarchitecture (Vaswanietal.,2017)iswellknownforitsscalingpropertiesin
NLP(Kaplanetal.,2020)andcomputervisiontasks(Dosovitskiyetal.,2020;Zhaietal.,2022). Fordiffusionmodels,
U-Netarchitectures(Ronnebergeretal.,2015)havebeenthedominantchoice(Hoetal.,2020;Rombachetal.,2022;Balaji
etal.,2022). Whilesomerecentworksexplorediffusiontransformerbackbones(Peebles&Xie,2023;Chenetal.,2023;
Maetal.,2024),scalinglawsfortext-to-imagediffusionmodelsremainunexplored.
17ScalingRectifiedFlowTransformersforHigh-ResolutionImageSynthesis
Detailedpenandinkdrawingofahappypigbutchersellingmeatinitsshop. amassivealienspaceshipthatisshapedlikeapretzel.
Akangarooholdingabeer, Anentireuniverseinsidea Acheesburgersurfingthe Aswampogrewithapearl Acarmadeoutofvegetables. heatdeathoftheuniverse,
wearingskigogglesand bottlesittingontheshelfat vibewaveatnight earringbyJohannesVermeer lineart
passionatelysingingsilly walmartonsale.
songs.
Acrabmadeofcheeseonaplate Dystopiaofthousandofworkerspickingcherriesandfeedingthemintoamachinethatrunsonsteam
andisaslargeasaskyscraper.Writtenonthesideofthemachine:”SD3Paper”
translucentpig,insideisasmallerpig. Filmstillofalong-leggedcutebig-eyeanthropomorphiccheeseburgerwearingsneakersrelaxingon
thecouchinasparselydecoratedlivingroom.
18ScalingRectifiedFlowTransformersforHigh-ResolutionImageSynthesis
detailedpenandinkdrawingofamassivecomplexalienspaceshipaboveafarminthemiddleof photoofabearwearingasuitandtophatinariverinthemiddleofaforestholdingasignthatsays”I
nowhere. cantbearit”.
tiltshiftaerialphotoofacutecitymadeofsushionawoodentableintheevening. darkhighcontrastrenderofapsychedelictreeoflifeilluminatingdustinamysticalcave.
ananthropomorphicfractalpersonbehindthecounteratafractalthemedrestaurant. beautifuloilpaintingofasteamboatinariverintheafternoon.Onthesideoftheriverisalargebrick
buildingwithasignontopthatsaysS¨D3¨.
ananthopomorphicpinkdonutwithamustacheandcowboyhatstandingbyalogcabininaforest foxsittinginfrontofacomputerinamessyroomatnight.Onthescreenisa3dmodelingprogram
withanold1970sorangetruckinthedriveway withalinerenderofazebra.
19ScalingRectifiedFlowTransformersforHigh-ResolutionImageSynthesis
B.OnFlowMatching
B.1.DetailsonSimulation-FreeTrainingofFlows
Following(Lipmanetal.,2023),toseethatu (z)generatesp ,wenotethatthecontinuityequationprovidesanecessary
t t
andsufficientcondition(Villani,2008):
d
p (x)+∇·[p (x)v (x)]=0↔v generatesprobabilitydensitypathp . (26)
dt t t t t t
Thereforeitsufficestoshowthat
p (z|ϵ)
−∇·[u (z)p (z)]=−∇·[E u (z|ϵ) t p (z)] (27)
t t ϵ∼N(0,I) t p (z) t
t
=E −∇·[u (z|ϵ)p (z|ϵ)] (28)
ϵ∼N(0,I) t t
d d
=E p (z|ϵ)= p (z), (29)
ϵ∼N(0,I)dt t dt t
where we used the continuity equation Equation (26) for u (z|ϵ) in line Equation (28) to Equation (29) since u (z|ϵ)
t t
generatesp (z|ϵ)andthedefinitionofEquation(6)inlineEquation(27)
t
TheequivalenceofobjectivesL ⇋L (Lipmanetal.,2023)followsfrom
FM CFM
L (Θ)=E ||v (z,t)−u (z)||2 (30)
FM t,pt(z) Θ t 2
=E ||v (z,t)||2−2E ⟨v (z,t)|u (z)⟩+c (31)
t,pt(z) Θ 2 t,pt(z) Θ t
=E ||v (z,t)||2−2E ⟨v (z,t)|u (z|ϵ)⟩+c (32)
t,pt(z) Θ 2 t,pt(z|ϵ),p(ϵ) Θ t
=E ||v (z,t)−u (z|ϵ)||2+c′ =L (Θ)+c′ (33)
t,pt(z|ϵ),p(ϵ) Θ t 2 CFM
wherec,c′donotdependonΘandlineEquation(31)tolineEquation(32)followsfrom:
(cid:90) (cid:90)
E ⟨v (z,t)|u (z|ϵ)⟩= dz dϵp (z|ϵ)p(ϵ)⟨v (z,t)|u (z|ϵ)⟩ (34)
pt(z|ϵ),p(ϵ) Θ t t Θ t
(cid:90) (cid:90) p (z|ϵ)
= dzp (z)⟨v (z,t)| dϵ t p(ϵ)u (z|ϵ)⟩ (35)
t Θ p (z) t
t
(cid:90)
= dzp (z)⟨v (z,t)|u (z)⟩=E ⟨v (z,t)|u (z)⟩ (36)
t Θ t pt(z) Θ t
where we extended with pt(z) in line Equation (35) and used the definition of Equation (6) in line Equation (35) to
pt(z)
Equation(36).
B.2.DetailsonImageandTextRepresentations
LatentImageRepresentationWefollowLDM(Rombachetal.,2022)anduseapretrainedautoencodertorepresentRGB
imagesX ∈RH×W×3inasmallerlatentspacex=E(X)∈Rh×w×d. Weuseaspatialdownsamplingfactorof8,such
thath= H andw = W,andexperimentwithdifferentvaluesfordinSection5.2.1. Wealwaysapplytheforwardprocess
8 8
fromEquation2inthelatentspace,andwhensamplingarepresentationxviaEquation1,wedecodeitbackintopixel
spaceX =D(x)viathedecoderD. WefollowRombachetal.(2022)andnormalizethelatentsbytheirmeanandstandard
deviation,whicharegloballycomputedoverasubsetofthetrainingdata. Figure10showshowgenerativemodeltraining
fordifferentdevolvesasafunctionofmodelcapacity,asdiscussedinSection5.2.1.
Text Representation Similar to the encoding of images to latent representations, we also follow previous approaches
(Saharia et al., 2022b; Balaji et al., 2022) and encode the text conditioning c using pretrained, frozen text models. In
particular,forallexperiments,weuseacombinationofCLIP(Radfordetal.,2021)modelsandaencoder-decodertextmodel.
Specifically,weencodecwiththetextencodersofbothaCLIPL/14modelofRadfordetal.(2021)aswellasanOpenCLIP
bigG/14modelofChertietal.(2023). Weconcatenatethepooledoutputs,ofsizes768and1280respectively,toobtain
avectorconditioningc ∈R2048. Wealsoconcatenatethepenultimatehiddenrepresentationschannel-wisetoaCLIP
vec
20ScalingRectifiedFlowTransformersforHigh-ResolutionImageSynthesis
Figure10. FIDscoresaftertrainingflowmodelswithdifferentsizes(parameterizedviatheirdepth)onthelatentspaceofdifferent
autoencoders(4latentchannels,8channelsand16channels)asdiscussedinSection5.2.1.Asexpected,theflowmodeltrainedonthe
16-channelautoencoderspaceneedsmoremodelcapacitytoachievesimilarperformance.Atdepthd=22,thegapbetween8-chnand
16-chnbecomesnegligible.Weoptforthe16-chnmodelasweultimatelyaimtoscaletomuchlargermodelsizes.
contextconditioningcCLIP ∈R77×2048. Next,weencodecalsotothefinalhiddenrepresentation,cT5 ∈R77×4096,ofthe
ctxt ctxt
encoderofaT5-v1.1-XXLmodel(Raffeletal.,2019). Finally,wezero-padcCLIPalongthechannelaxisto4096dimensions
ctxt
tomatchtheT5representationandconcatenateitalongthesequenceaxiswithcT5 toobtainthefinalcontextrepresentation
ctxt
c ∈R154×4096. Thesetwocaptionrepresentations,c andc ,areusedintwodifferentwaysasdescribedinSection4.
ctxt vec ctxt
B.3.PreliminariesfortheExperimentsinSection5.1.
DatasetsWeusetwodatasetstoaccountforthemissingofastandardtext-to-imagebenchmark. Asawidelyuseddataset,
weconverttheImageNetdataset(Russakovskyetal.,2014)intoadatasetsuitablefortext-to-imagemodelsbyadding
captionsoftheform“aphotoofa〈class name〉”toimages,where〈class name〉israndomlychosenfromone
oftheprovidednamesfortheimage’sclasslabel. Asamorerealistictext-to-imagedataset,weusetheCC12Mdataset
(Changpinyoetal.,2021)fortraining.
Optimization In this experiment, we train all models using a global batch size of 1024 using the AdamW optimizer
(Loshchilov&Hutter,2017)withalearningrateof10−4and1000linearwarmupsteps. Weusemixed-precisiontraining
andkeepacopyofthemodelweightswhichgetsupdatedevery100trainingbatcheswithanexponentialmovingaverage
(EMA)usingadecayfactorof0.99. Forunconditionaldiffusionguidance(Ho&Salimans,2022),wesettheoutputsofeach
ofthethreetextencodersindependentlytozerowithaprobabilityof46.4%,suchthatweroughlytrainanunconditional
modelin10%ofallsteps.
EvaluationAsdescribedinSection5.1,weuseCLIPscores,FIDandvalidationlossestoevaluateourmodelsregularly
duringtrainingontheCOCO-2014validationsplit(Linetal.,2014).
Asthelossvaluesdifferwidelyinmagnitudeandvariancefordifferenttimesteps,weevaluatetheminastratifiedwayon
eightequallyspacedvaluesinthetimeinterval[0,1].
Toanalyzehowdifferentapproachesbehaveunderdifferentsamplersettings,weproduce1000samplesforeachofthe
samplerswhichdifferinguidancescalesaswellasnumberofsamplingsteps. WeevaluatethesesampleswithCLIPscores
usingCLIPL/14(Radfordetal.,2021)andalsocomputeFIDbetweenCLIPL/14imagefeaturesofthesesamplesandthe
imagesofthevalidationset. Forsampling,wealwaysuseaEulerdiscretization(Euler,1768)ofEquation1andsixdifferent
settings: 50stepswithclassifier-free-guidancescales1.0,2.5,5.0,and5,10,25stepswithclassifier-free-guidancescale5.0.
B.4.ImprovingSNRSamplersforRectifiedFlowModels
AsdescribedinSection2,weintroducenoveldensitiesπ(t)forthetimestepsthatweusetotrainourrectifiedflowmodels.
Figure11visualizesthedistributionsofthelogit-normalsamplerandthemodesamplerintroducedinSection3.1.Notably,
aswedemonstrateinSection5.1,thelogit-normalsampleroutperformstheclassicuniformrectifiedflowformulation(Liu
etal.,2022)andestablisheddiffusionbaselinessuchasEDM(Karrasetal.,2022)andLDM-Linear(Rombachetal.,2022).
21ScalingRectifiedFlowTransformersforHigh-ResolutionImageSynthesis
Figure11.Themode(left)andlogit-normal(right)distributionsthatweexploreforbiasingthesamplingoftrainingtimesteps.
“Araccoonwearingformalclothes,wearinga “Asmilingslothiswearingaleatherjacket,a
tophatandholdingacane.Theraccoonis “Abowlofsoupthatlookslikeamonstermade “Twocupsofcoffee,onewithlatteartofa cowboyhat,akiltandabowtie.Theslothis
holdingagarbagebag.Oilpaintinginthestyle outofplasticine” heart.Theotherhaslatteartofstars.” holdingaquarterstaffandabigbook.Thesloth
ofabstractcubism.” isstandingongrassafewfeetinfrontofa
shinyVWvanwithflowerspaintedonit.
wide-anglelensfrombelow.”
Figure12.Qualitativeeffectsofscaling.Displayedareexamplesdemonstratingtheimpactofscalingtrainingsteps(lefttoright:50k,
200k,350k,500k)andmodelsizes(toptobottom:depth=15,30,38)onPartiPrompts,highlightingtheinfluenceoftrainingdurationand
modelcomplexity.
22ScalingRectifiedFlowTransformersforHigh-ResolutionImageSynthesis
C.DirectPreferenceOptimization
“apeacefullakesidelandscapewith “abookwiththewords‘Don’tPanic¡,
migratingherdofsauropods” writtenonit”
Figure13.ComparisonbetweenbasemodelsandDPO-finetunedmodels.DPO-finetuninggenerallyresultsinmoreaestheticallypleasing
sampleswithbetterspelling.
DirectPreferenceOptimization(DPO)(Rafailovetal.,2023)isatechniquetofinetuneLLMswithpreferencedata. Recently,
thismethodhasbeenadaptedtopreferencefinetuningoftext-to-imagediffusionmodels(Wallaceetal.,2023). Inthis
section,weverifythatourmodelisalsoamenabletopreferenceoptimization. Inparticular,weapplythemethodintroduced
inWallaceetal.(2023)toour2Band8Bparameterbasemodel. Ratherthanfinetuningtheentiremodel,weintroduce
learnableLow-RankAdaptation(LoRA)matrices(ofrank128)foralllinearlayersasiscommonpractice. Wefinetune
thesenewparametersfor4kand2kiterationforthe2Band8Bbasemodel,respectively. Wethenevaluatetheresulting
modelinahumanpreferencestudyusingasubsetof128captionsfromthePartipromptsset(Yuetal.,2022)(roughlythree
voterperpromptandcomparison). Figure14showsthatourbasemodelscanbeeffectivelytunedforhumanpreference.
Figure13showssamplesoftherespectivebasemodelsandDPO-finetunedmodels.
D.Finetuningforinstruction-basedimageediting
A common approach for training instruction based image editing and general image-to-image diffusion models is to
concatenatethelatentsoftheinputimagetothenoisedlatentsofthediffusiontargetalongthechanneldimensionbefore
feedingtheinputintoaU-Net (Brooksetal.,2023;Sheyninetal.,2023;Sahariaetal.,2022a;c). Wefollowthesame
approach, concatenatinginputandtargetalongthechannelsbeforepatching, anddemonstratethatthesamemethodis
applicabletoourproposedarchitecture. Wefinetunethe2Bparameterbasemodelonadatasetconsistingofimage-to-image
editingtaskssimilartothedistributionoftheInstructPix2Pixdataset(Brooksetal.,2023)aswellasinpainting,segmentation,
colorization,deblurringandcontrolnettaskssimilartoEmuEditandPalette (Sheyninetal.,2023;Sahariaetal.,2022a).
AsshowninFig15weobservethattheresulting2BEditmodelhasthecapabilitytomanipulatetextinagivenimage,even
23
esabB2
OPD/wB2
esabb8
OPD/wb8ScalingRectifiedFlowTransformersforHigh-ResolutionImageSynthesis
depth=24 (2B) depth=38 (8B)
base base
60 w/ DPO w/ DPO
50
40
30
20
10
0
Prompt Quality Prompt Quality
Figure14.HumanpreferenceevaluationbetweenbasemodelsandDPO-finetunedmodels. HumanevaluatorspreferDPO-finetuned
modelsforbothpromptfollowingandgeneralquality.
Model Mem[GB] FP[ms] Storage[kB] Delta[%]
VAE(Enc) 0.14 2.45 65.5 13.8
CLIP-L 0.49 0.45 121.3 2.6
CLIP-G 2.78 2.77 202.2 15.6
T5 19.05 17.46 630.7 98.3
Table7. Keyfiguresforpreencodingfrozeninputnetworks.MemisthememoryrequiredtoloadthemodelontheGPU.FP[ms]is
thetimepersamplefortheforwardpasswithper-devicebatchsizeof32.Storageisthesizetosaveasinglesample.Delta[%]ishow
muchlongeratrainingsteptakes,whenaddingthisintotheloopforthe2BMMDiT-Model(568ms/it).
thoughnotextmanipulationtaskswereincludedinthetrainingdata. Wewerenotabletoreproducesimilarresultswhen
trainingaSDXL-based (Podelletal.,2023)editingmodelonthesamedata.
E.DataPreprocessingforLarge-ScaleText-to-ImageTraining
E.1.PrecomputingImageandTextEmbeddings
Ourmodelusestheoutputofmultiplepretrained,frozennetworksasinputs(autoencoderlatentsandtextencoderrepre-
sentations). Sincetheseoutputsareconstantduringtraining,weprecomputethemoncefortheentiredataset. Thiscomes
withtwomainadvantages: (i)TheencodersdonotneedtobeavailableontheGPUduringtraining,loweringtherequired
memory. (ii)Theforwardencodingpassisskippedduringtraining,savingtimeandtotalneededcomputeafterthefirst
epoch,seeTab.7.
Thisapproachhastwodisadvantages: First,randomaugmentationforeachsampleeveryepochisnotpossibleandweuse
square-centercroppingduringprecomputationofimagelatents. Forfinetuningourmodelathigherresolutions,wespecify
anumberofaspectratiobuckets,andresizeandcroptotheclosestbucketfirstandthenprecomputeinthataspectratio.
Second,thedenseoutputofthetextencodersisparticularlylarge,creatingadditionalstoragecostandlongerloadingtimes
duringtraining(c.f.Tab.7). Wesavetheembeddingsofthelanguagemodelsinhalfprecision,aswedonotobservea
deteriorationinperformanceinpractice.
E.2.PreventingImageMemorization
Inthecontextofgenerativeimagemodelsmemorizationoftrainingsamplescanleadtoanumberofissues(Somepallietal.,
2023a;Carlinietal.,2023;Somepallietal.,2023b). Toavoidverbatimcopiesofimagesbyourtrainedmodels,wecarefully
scanourtrainingdatasetforduplicatedexamplesandremovethem.
24
]
%
[
ecnereferP
namuHScalingRectifiedFlowTransformersforHigh-ResolutionImageSynthesis
Input Output1 Output2
Write”gosmall
gohome”
instead
GOBIGORGOUNET
iswrittenon
theblackboard
changethe
wordto
UNOT
makethe
signsay
MMDITrules
Figure15. ZeroShotTextmanipulationandinsertionwiththe2BEditmodel
DetailsonDeduplication Inaccordancewiththemethodsoutlinedby Carlinietal.(2023)and Somepallietal.(2023a),
weoptforSSCD(Pizzietal.,2022)asthebackboneforthededuplicationprocess. TheSSCDalgorithmisastate-of-the-art
techniquefordetectingnear-duplicateimagesatscale,anditgenerateshigh-qualityimageembeddingsthatcanbeusedfor
clusteringandotherdownstreamtasks. WealsodecidedtofollowNichol(2022)todecideonanumberofclustersN. For
ourexperiments,weuseN =16,000.
Weutilizeautofaiss(2023)forclustering. autofaiss(2023)isalibrarythatsimplifiestheprocessofusingFaiss(FacebookAI
SimilaritySearch)forlarge-scaleclusteringtasks. Specifically,leverageFAISSindexfactory1functionalitytotrainacustom
indexwithpredefinednumberofcentroids. Thisapproachallowsforefficientandaccurateclusteringofhigh-dimensional
data,suchasimageembeddings.
Algorithm1detailsourdeduplicationapproach. WerananexperimenttoseehowmuchdataisremovedbydifferentSSCD
thresholdasshowninFigure16b. BasedontheseresultsweselectedfourthresholdsforthefinalrunFigure16a.
1https://github.com/facebookresearch/faiss/wiki/The-index-factory
25ScalingRectifiedFlowTransformersforHigh-ResolutionImageSynthesis
E.3.AssessingtheEfficacyofourDeduplicationEfforts
Carlinietal.(2023)deviseatwo-stagedataextractionattackthatgeneratesimagesusingstandardapproaches,andflags
thosethatexceedcertainmembershipinferencescoringcriteria. Carlinietal.(2023)biastheirsearchtowardsduplicated
training examples because these are orders of magnitude more likely to be memorized than non-duplicated examples
(Somepallietal.,2023a;a;Leeetal.,2021).
ToassesshowwellourSSCD-baseddeduplicationworks,wefollowCarlinietal.(2023)toextractmemorizedsamplesfrom
small,specificallyforthispurposetrainedmodelsandcomparethembeforeandafterdeduplication. Twomainstepofthe
mentionedprocedureinclude: 1)Generatemanyexamplesusingthediffusionmodelinthestandardsamplingmannerand
withtheknownprompts. 2)Performmembershipinferencetoseparatethemodel’snovelgenerationsfromthosegenerations
whicharememorizedtrainingexamples. Algorithm2showsthestepstofindthememorizedsamplesbasedonCarlinietal.
(2023). Notethatwerunthistechniquestwotimes;oneforSD-2.1modelwithonlyexactdedupremovalasbaseline,and
foramodelwiththeSD2.1architecturebuttrainedonremovedexactduplicationandnear-duplicationusingSSCD(Pizzi
etal.,2022).
Weselectthe350,000most-duplicatedexamplesfromthetrainingdatasetbasedonSSCD(Pizzietal.,2022)withthreshold
of0.5,andgenerate500candidateimagesforeachtextprompttoincreasethelikelihoodoffindingmemorization. The
intuitionisthatfordiffusionmodels,withhighprobabilityGen(p;r )≈ Gen(p;r )fortwodifferentrandominitialseeds
1 d 2
r ,r . Ontheotherhand,ifGen(p;r ) ≈ Gen(p;r )undersomedistancemeasured,itislikelythatthesegenerated
1 2 1 d 2
samplesarememorizedexamples. Tocomputethedistancemeasuredbetweentwoimages,weuseamodifiedEuclidean
l distance. In particular, we found that many generations were often spuriously similar according to l distance (e.g.,
2 2
they all had gray backgrounds). We therefore instead divide each image into 16 non-overlapping 128 × 128 tiles and
measurethemaximumofthel distancebetweenanypairofimagetilesbetweenthetwoimages. Figure17showsthe
2
comparisonbetweennumberofmemorizedsamples, beforeandafterusingSSCDwiththethresholdof0.5toremove
near-duplicated samples. Carlini et al. (2023) mark images within clique size of 10 as memorized samples. Here we
also explore different sizes for cliques. For all clique thresholds, SSCD is able to significantly reduce the number of
memorizedsamples. Specifically,whenthecliquesizeis10,trainedSDmodelsonthededuplicatedtrainingsamplescutoff
atSSCD=0.5showa5×reductioninpotentiallymemorizedexamples.
Algorithm1FindingDuplicateItemsinaCluster
Require: vecs–Listofvectorsinasinglecluster,items–ListofitemIDscorrespondingtovecs,index–FAISSindex
forsimilaritysearchwithinthecluster,thresh–Thresholdfordeterminingduplicates
Output: dups–SetofduplicateitemIDs
1: dups←newset()
2: fori←0tolength(vecs)−1do
3: qs←vecs[i]{Currentvector}
4: qid←items[i]{CurrentitemID}
5: lims,D,I ←index.range search(qs,thresh)
6: ifqid∈dupsthen
7: continue
8: endif
9: start←lims[0]
10: end←lims[1]
11: duplicate indices←I[start:end]
12: duplicate ids←newlist()
13: forj induplicate indicesdo
14: ifitems[j]̸=qidthen
15: duplicate ids.append(items[j])
16: endif
17: endfor
18: dups.update(duplicate ids)
19: endfor
20: Returndups{FinalsetofduplicateIDs}
26ScalingRectifiedFlowTransformersforHigh-ResolutionImageSynthesis
(a)FinalresultofSSCDdeduplicationovertheentiredataset (b)ResultofSSCDdeduplicationwithvariousthresholdsover1000
randomclusters
Figure16. Resultsofdeduplicatingourtrainingdatasetsforvariousfilteringthresholds.
Algorithm2DetectingMemorizationinGeneratedImages
Require: SetofpromptsP,NumberofgenerationsperpromptN,Similaritythresholdϵ=0.15,Memorizationthreshold
T
Ensure: Detectionofmemorizedimagesingeneratedsamples
1: InitializeDtothesetofmost-duplicatedexamples
2: foreachpromptp∈P do
3: fori=1toN do
4: GenerateimageGen(p;r i)withrandomseedr i
5: endfor
6: endfor
7: foreachpairofgeneratedimagesx i,x j do
8: ifdistanced(x i,x j)<ϵthen
9: Connectx iandx j ingraphG
10: endif
11: endfor
12: foreachnodeinGdo
13: Findlargestcliquecontainingthenode
14: ifsizeofclique≥T then
15: Markimagesinthecliqueasmemorized
16: endif
17: endfor
27ScalingRectifiedFlowTransformersforHigh-ResolutionImageSynthesis
Figure17. SSCD-baseddeduplicationpreventsmemorization.ToassesshowwellourSSCD-baseddeduplicationworks,weextract
memorizedsamplesfromsmall,specificallyforthispurposetrainedmodelsandcomparethembeforeandafterdeduplication.Weplota
comparisonbetweennumberofmemorizedsamples,beforeandafterusingSSCDwiththethresholdof0.5toremovenear-duplicated
samples.Carlinietal.(2023)markimageswithincliquesizeof10asmemorizedsamples.Herewealsoexploredifferentsizesforcliques.
Forallcliquethresholds,SSCDisabletosignificantlyreducethenumberofmemorizedsamples.Specifically,whenthecliquesizeis10,
modelsonthededuplicatedtrainingsamplescutoffatSSCD=0.5showa5×reductioninpotentiallymemorizedexamples.
28