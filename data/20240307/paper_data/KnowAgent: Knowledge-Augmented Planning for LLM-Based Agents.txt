KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents
YuqiZhu♣♡,ShuofeiQiao♣♡,YixinOu♣♡,ShuminDeng♠,NingyuZhang♣♡∗
ShiweiLyu♢,YueShen♢,LeiLiang♢,JinjieGu♢,HuajunChen♣♡∗
♣ ZhejiangUniversity♡ ZJU-AntGroupJointResearchCenterforKnowledgeGraphs
♢ AntGroup♠ NationalUniversityofSingapore
{zhuyuqi,zhangningyu}@zju.edu.cn
https://zjunlp.github.io/project/KnowAgent/
Abstract
Know gent
Knowledge could be a roadmap
LargeLanguageModels(LLMs)havedemon- that leads to thedestination.
strated great potential in complex reasoning
tasks, yet they fall short when tackling more
sophisticated challenges, especially when in- LLaMA GPT
teracting with environments through generat- Utilize
Knowledge
ing executable actions. This inadequacy pri-
marily stems from the lack of built-in action
Task: HotpotQA
knowledge in language agents, which fails
Start: (Search, Retrieve)
to effectively guide the planning trajectories Search: (Retrieve, Lookup, …)
=
during task solving and results in planning Retrieve: (Search, Finish, …)
Action
Lookup: (Search, Retrieve, …)
hallucination. To address this issue, we in- Knowledge Base
… … … troduce KNOWAGENT, anovelapproachde-
Planning
signedtoenhancetheplanningcapabilitiesof Next
LLMsbyincorporatingexplicitactionknowl-
edge. Specifically, KNOWAGENT employs Figure1: Theoverviewof KNOWAGENT. Anagent
an action knowledge base and a knowledge- could leverage external action knowledge base to ad-
ableself-learningstrategytoconstraintheac- dressandsolvecomplexplanningchallenges.
tionpathduringplanning,enablingmorerea-
sonable trajectory synthesis, and thereby en-
hancingtheplanningperformanceoflanguage
Qiao et al., 2024), and the utilization of external
agents. ExperimentalresultsonHotpotQAand
tools(Schicketal.,2023;Qiaoetal.,2023a). De-
ALFWorldbasedonvariousbackbonemodels
demonstrate that KNOWAGENT can achieve spite the effectiveness of current prompting tech-
comparableorsuperiorperformancetoexisting niques in providing good planning abilities for
baselines. Furtheranalysisindicatestheeffec- someclosed-sourcelanguagemodels,thesemeth-
tivenessofKNOWAGENTintermsofplanning
odsareoftenlimitedbythemodel’sintrinsicunder-
hallucinationsmitigation1.
standingcapabilitiesandthescopeofknowledgeit
wastrainedon. Tomeetthedemandsforbroadap-
1 Introduction
plicationandcustomizationindifferentareassuch
Asartificialintelligence(AI)advances,language asquestion-answering(Yaoetal.,2023c;Yinetal.,
agents are becoming increasingly vital for solv- 2023),webbrowsing(Yaoetal.,2022;Dengetal.,
ingcomplexproblems(Zhangetal.,2023;Sumers 2023; Zhou et al., 2023a), robotics (Ichter et al.,
etal.,2024;Yangetal.,2024). Theseagents,built 2022; Ding et al., 2023) and so on, researchers
aroundLargeLanguageModels(LLMs),enhance areexploringAgentTuningasameanstoaugment
their task planning capabilities through a variety modelcapabilities(Chenetal.,2023a;Zengetal.,
of strategies including task decomposition (Wei 2023;Panetal.,2023). Thisinvolvesfine-tuning
etal.,2022;Yaoetal.,2023a;Wangetal.,2023a; models through the synthesis of task-specific tra-
Team, 2023), reflection (Shinn et al., 2023; Sun jectories, enabling them to undertake a series of
etal.,2023a),collaborativedivisionoflabor (Hong effectiveactionstocompletetasks,therebyenhanc-
et al., 2023; Chen et al., 2023c; Yin et al., 2023; ingtheirabilitytohandlecomplexsituations.
∗Correspondingauthor. However,whenitcomestoexecutingplanning
1Codeisathttps://github.com/zjunlp/KnowAgent. tasks, especiallyinopen-sourcemodels, therere-
4202
raM
5
]LC.sc[
1v10130.3042:viXramain issues (Liu et al., 2023a; Valmeekam et al., • Furtheranalysisvalidatestheeffectivenessof
2023;Guanetal.,2023). Frequently,modelsgen- incorporatingactionknowledgeforplanning
erateplansthatviolateestablishedknowledgerules purposes. Wealsoshowcasethepossibilityof
orcommonsense(Dingetal.,2023),aphenomenon employingmanuallyrefinedactionknowledge
we name as planning hallucination. This term from LLMs, thereby reducing human labor
describes scenarios where models might gener- andenhancingperformance.
ate unnecessary or conflicting action sequences,
2 Background
suchas“attempting to look up information
without performing a search operation”or
Languageagentsobservetheexternalworldprimar-
“trying to pick an apple from a table
ilybygeneratinginnerthoughtsandexecutableac-
without verifying the presence of both
tions. Inthispaper,wefollowandfurtherenhance
the table and the apple”.
theplanningtrajectoryformatproposedinYaoetal.
To address these issues, we propose KNOWA-
(2023b) to train and evaluate our KNOWAGENT.
GENT that focuses on leveraging external action Traditionally,aplanningtrajectoryτ canberepre-
knowledge to enhance synthetic trajectories with
sentedbyatripletofThought-Action-Observation
thegoal ofresolving planninghallucination (see
(T,A,O),whereT indicatestheinnerthoughtsof
Figure1). Ourdevelopmentisgroundedonseveral
thelanguageagent,Asignifiesexecutableactions,
keysteps: Initially,wecreateanextensiveaction
and O represents the feedback information from
knowledgebase,whichamalgamatesactionplan-
the environment. In terms of this, the trajectory
ning knowledge pertinent to specific tasks. This
historyHattimetcanbedefinedasfollows:
database acts as an external reservoir of informa-
tion,steeringthemodel’sactiongenerationprocess. H = (T ,A ,O ,T ,...,T ,A ,O ) (1)
t 0 0 0 1 t−1 t−1 t−1
Subsequently,byconvertingactionknowledgeinto
text, we enable the model to deeply understand Then,thelanguageagentisreinforcedtogenerate
and utilize this knowledge in creating action tra- T t andA t basedonthehistory. Givenaparameter-
jectories. Finally, through a knowledgeable self- izedprobabilisticlanguageagentπwithparameters
learningphase,weusetrajectoriesdevelopedfrom θ,theprocessofgeneratingthenextstep’sthought
the model’s iterative processes to continually im- basedonH t canberepresentedas:
proveitsunderstandingandapplicationofaction
knowledge. Thisprocessnotonlystrengthensthe
(cid:89)|Tt|
p(T |H ) = π (Ti|H ,T<i), (2)
agents’ planning abilities but also enhances their t t θ t t t
i=1
potentialforapplicationincomplexsituations.
ExperimentalresultsonHotpotQA(Yangetal., whereTi and|T |arethei-thtokenandthelength
t t
2018)andALFWorld(Shridharetal.,2021)based ofT respectively. Subsequently,theactionA will
t t
on various backbone models demonstrate that bedeterminedbasedonT andH :
t t
KNOWAGENTcanachievecomparableorsuperior
performancetoexistingbaselines. Furtheranaly- |At|
p(A |H ,T ) = (cid:89) π (Aj|H ,T ,A<j). (3)
sisindicatestheeffectivenessof KNOWAGENT in t t t θ t t t t
terms of planning hallucinations mitigation. We j=1
summarizeourcontributionsasfollows:
Similarly, Aj and |A | denote the j-th token and
t t
• We introduce KNOWAGENT that employs thelengthofA respectively. Lastly,thefeedback
t
knowledgeableself-learningtoincorporateex-
resultoftheactionA willbetreatedastheobser-
t
ternal action knowledge into models. This vation O and added to the trajectory, generating
t
advancementpresentsaninnovativemethod anewroundoftrajectoryH . It’simportantto
t+1
forincorporatingexternalknowledgetorefine notethatA herespecificallymeanstheactionsin
i
andaugmenttheintrinsicplanningabilitiesof thetrajectory,whichisidenticaltotheactiona in
i
languageagents. thediscussionoftheactionsetE lateron.
a
• Weconductcomprehensiveexperimentsthat
3 KNOWAGENT
demonstrate KNOWAGENT canmatchorsur-
pass other benchmark models on the Hot- Inthissection,weofferadetailedintroductionto
potQAandALFWorlddatasets. KNOWAGENT (See Figure 2), focusing on threeActionKnowledge to Text
Planning
Rule ActionPath 1:Start
A •c Ntio an ms e Rule search SSeeaarrcchh T "Sh eo au rcg hh "t a1 n: dF r "o Rm et " riS et va ert "" . , T t oh de ea td ej ra mce inn et a wc hti io chn s e a pr ise o de of
• Definition SpongeBob SquarePants aired first, I need to search for
information on the episodes.
Action A Rc ut li eo sn Retrieve Rule retrieve Lookup A …c …tion 1: Search[TheClashofTriton]
Knowledge Base Rule lookup ActionPath4:Start->Search[TheClashofTriton]-
HotpotQA >Search[ToSquarePantsorNottoSquarePants]
Action 4: Finish[To SquarePants or Not to SquarePants?]
Observation4: Answer is CORRECT
Input Self-generatedData
Action D
Knowledge 0
Question: D Merge D 2 Merge
1
Which episode of GPT
SpongeBob SquarePants
aired first, The Clash of
T or ri Nto on t o tor T So q uS aq ru ea Pr ae nP ta sn ?ts GenP ea rt ah t ion D 0 Iterate Iterate
LLaMA M 0 M 1 M 2
Planning Path Generation KnowledgeableSelf-Learning
Figure2: TheoverallframeworkofKNOWAGENT. Initially,ActionKnowledgetoTextconvertstask-specific
actionknowledgeintotextualdescriptions. Next,PlanningPathGenerationusespromptsandthisknowledgeto
leadLLMsinplanningpathcreation. Lastly,inKnowledgeableSelf-Learning,themodeliterativelyoptimizesusing
generatedplanningtrajectoriestoimproveperformance.
coreaspects. First,wedefineactionknowledgein time-consuming and labor-intensive. To address
§3.1. Next,wedescribehowactionknowledgeis thischallenge,consideringthestrongperformance
utilizedtogenerateplanningpaths(§3.2). Lastly, ofLLMsonsuchtasks(Liuetal.,2023a;Ouyang
we detail the refinement of the paths through a and Li, 2023), we utilize GPT-4 (OpenAI, 2023)
knowledgeableself-learningmechanism. (§3.3). forinitialconstruction,followedbymanualrefine-
ment. In§4.3,weprovideadetailedcomparison
3.1 DefinitionofActionKnowledge
oftheeffectivenessofthesetwoapproaches.
Action. E = {a ,...,a } signifies a set of
a 1 N−1
3.2 PlanningPathGenerationwithAction
actions,whichencompassesthediscreteactionthat
Knowledge
LLMsmustundertaketoaccomplishspecifictasks.
3.2.1 ActionKnowledgetoText
Action Rules. R = {r ,...,r } outlines the
1 N−1 Figure 3 illustrates the conversion process from
rulesthatdeterminethelogicandsequenceofac-
action knowledge to text. Initially, we establish
tion transitions within the model. These rules di-
theactionknowledgebasebyidentifyingactions
rectly dictate permissible action transitions r :
k pertinenttothetask’sspecificneeds,utilizingpre-
a → a , based on the inherent relationships
i j viousdatasetanalysesandtheinherentknowledge
amongactionsortask-specificrequirements.
ofLLMs. Thisinformationisthenconvertedinto
Action Knowledge. Action Knowledge, repre- textformattofacilitatesubsequentoperations. As
sented as (E ,R), comprises a defined set of ac- anillustration,wereferenceoneactionruleinHot-
a
tionsE andtherulesRgoverningtheirtransitions. potQA (Yang et al., 2018) - Search: (Search,
a
Thecombinationofactionknowledgefordifferent Retrieve, Lookup, Finish). This rule signi-
tasksformsanactionknowledgebase,alsoknown fiesthatfromSearch,severalpathwaysareviable:
asActionKB.Theknowledgebasewillthenserve an action may continue as Search, evolve into
asessentialguidanceforgeneratingactionsandfor- RetrieveorLookup,oradvancetowardsFinish.
mulatingdecisions,essentialforreducingpotential
3.2.2 PathGeneration
planhallucinationproblems.
Harnessing action knowledge, the model utilizes
Strategies for Extracting Action Knowledge. this insight to streamline the task’s planning pro-
Given the diverse action knowledge involved in cess. It achieves this by formulating a coherent
various tasks, fully manual construction is both planningpath,guidedbytheapplicationofactionAction Knowledge Base Prompt
(a) The Construction of Action Knowledge Base TaskDescription： Your task is to answer a question ……
The Overview of Action Knowledge:
Actions The decision graph is constructed upon a set of principles
Human LLM
known as "Action Knowledge", outlined as follows:
Action Start:(Search, Retrieve)
Tasks Extract Rules Retrieve:(Retrieve, Search, Lookup, Finish)
Search:(Search, Retrieve, Lookup, Finish)
……
(b) Task-Specific Action Knowledge Here's how to interpret the graph's Action Knowledge:
……
Search Definition of Each Action Step:
Each node action is defined as follows:
Start Lookup Finish (1) Retrieve[entity]: Retrieve the exact entity on Wikipedia and
return the first paragraph if it exists. ….
(2) Search[topic]: Use Bing Search to ….
Retrieve (3)Lookup[entity]: ….
…
Principle of Planning Path Generation:
(c) Action Knowledge to Text
Actions Action Rules As you solve the question using the above structure, interleave
ActionPath, Thought, Action, andObservationsteps. ……
-Search: (Search, Retrieve, Lookup, …) You may take as many steps as necessary.
-Retrieve: (Retrieve, Search, Lookup, …)
-Lookup: (Lookup, Search, Retrieve, …) Demonstrations of Planning Paths:
…… Here are some examples:
{examples}
HotpotQA (END OF EXAMPLES)
Figure3: ThePathGenerationprocessofKNOWAGENT.
rulesR ∧R ∧... ⇒ P. Tofacilitatepathgener- Here we briefly outline the process of trajec-
1 2
ation,wedevelopspecializedpromptsthatextend tory synthesis. This trajectory, denoted as τ, is
beyond basic Task Description, integrating seg- composed of many planned quadruples. Each
mentsasillustratedinFigure3. quadruple (P,T,A,O), encapsulates the action
pathP,theagent’sinternalthoughtsprocessesT,
Ourapproachisthoroughlygroundedinaction
executableactionsA,andenvironmentalfeedback
knowledgeandunfoldsacrossfourkeysegments:
O. Thehistoricaltrajectoryisreformulatedas:
(1) It starts with an Overview of Action Knowl-
edgetosetthefoundationalconceptsandrules. (2)
H = (P ,T ,A ,O ,...,
ThisisfollowedbytheDefinitionofEachAction t 0 0 0 0
Step,detailingtheoperationalaspectsandsignifi- P i−1,T t−1,A t−1,O t−1) (4)
canceofeachaction. (3)Followingthis,thePrin-
cipleofPlanningPathGenerationdelvesintothe Based on this historical trajectory, the agent is
constraints on output generation. (4) And finally, poisedtogenerateanewactionpath,thoughtpro-
DemonstrationsofPlanningPathsprovideprac- cess,andaction. Consideringaparameterizedprob-
ticalexamples,actingasabeaconofinspirationfor abilistic language agent π with parameters θ, the
adapting these strategies across various contexts. mechanism for generating the subsequent action
Eachofthesesegmentsplaysanessentialroleinex- path,contingentonP ,isexpressedas:
t
pressingactionknowledge,specifyingactions,and
clarifyingtheprocessofleveragingactionknowl-
edge for planning path generation. It’s essential |Pt|
(cid:89)
tounderstandthedistinctionbetweenpathand p(P |H ) = π (Pk|H ,P<k), (5)
t t θ t t t
trajectoryinthiscontext. Thepathexclusively k=1
representstheseriesofactionsundertakenbythe
agent, while the trajectory includes the model’s HerePk and|P |representthek-thtokenandthe
t t
complete output during the problem-solving pro- totallengthofP . Andthenweextendtheapproach
t
cess,incorporatingthepathaspartofitsstructure. usedinEquation2and3. Theprocessofderivingthoughtsandactionscanbereformulatedas: Algorithm 1: Trajectory Synthesis and
KnowledgeableSelf-Learning
|Tt|
(cid:89) Input:AK :Task-specificActionKnowledgefrom
p(T |H ,P ) = π (Ti|H ,P ,T<i), (6) m
t t t θ t t t t ActionKB,includingasetofactionsE a
i=1 andactionrulesR;D 0:Initialtrainingset;
D :Testingset.
|At| test
p(A |H ,P ,T ) = (cid:89) π (Aj|H ,P ,T ,A<j). Output:OptimizedmodelsM ={M 1,M 2,...}.
t t t t θ t t t t t Initialize:ModelM 0.
j=1 fori=0untiltestperformancestabilizesdo
(7) ifi=0then
// Synthesizeinitialtrajectories.
T ←Traj(M ,AK ,D )
3.3 PlanningPathRefinementvia i i m 0
// Fitertrajectories.
KnowledgeableSelf-Learning T′ ←Filter(T ,AK )
i i m
In this phase, we introduce knowledgeable self- // Initialfine-tuning.
M ←Tune(T′,M )
learning. Ourgoalistohelpthemodelunderstand i+1 i i
else
the action knowledge more deeply through itera-
// Synthesizetrajectories.
tive fine-tuning. As shown in Algorithm 1, our T ←Traj(M ,AK ,D )
i i m 0
approachbeginswithaninitialtrainingsetD and // Filterandmergetrajectories.
0
an untrained model M 0, leading to the synthesis T i′ ←FilterAndMerge(T i,T i−1,AK m)
// Furtherfine-tuning.
ofinitialtrajectoriesT = {τ ,τ ,...,τ }. After
0 1 2 n M ←Tune(T′,M )
filtering,theseinitialoutcomesinformfurthertrain- i+1 i i
// Performancecheck
ing, producing a preliminary model version, M .
1 if∆Perf(M i+1,M i,D test)≤ϵthen
Subsequently,M undergoesre-evaluationonD break
1 0
to create new trajectories T = {τ′,τ′,...,τ′}. returnOptimizedmodelsM
1 1 2 n
Thesetrajectories,alongsideT ,undergoafiltering
0
and merging process based on action knowledge
This refined set of trajectories is then utilized to WeemployLlama-2-{7,13,70}b-chat(Touvron
fine-tunethemodel,resultinginanimprovedver- etal.,2023)asthebackbonemodels,andalsoapply
sion, M . We continue iterating until the perfor- KNOWAGENTtoVicuna(Zhengetal.,2023)and
2
mance improvement on M becomes small, at Mistral(Jiangetal.,2023). Wecompare KNOWA-
test
whichpointwehalttheiterationprocess. GENTwithvariousbaselinesincludingCoT(Wei
et al., 2022), ReAct (Yao et al., 2023b), Reflex-
Knowledge-Based Trajectory Filtering and
ion(Shinnetal.,2023)andFiReAct(Chenetal.,
Merging. Our knowledgeable self-learning ap-
2023a). Moredetailsaboutthedatasets,evaluation
proachenhancestrajectoryqualitythroughtwokey
metrics,baselines,andtraininghyper-parameters
phases: (1) Filtering: We start by selecting cor-
canbeseeninAppendixA.
recttrajectories,T ,basedontheiroutcomes.
correct
Specifically for task HotpotQA, we apply action 4.2 MainResults
knowledgetofurtherrefinethesetrajectories. This
KNOWAGENTvs. Prompt-basedMethods. In
refinementinvolvesremovinganytrajectoriesthat
Table 1, we present the F1 scores and success
donotalignwiththeprovidedAK ,particularly
m
thosewithinvalidactionsordisorderedactionse-
ratesfor KNOWAGENT andvariousprompt-based
methodsevaluatedonHotpotQAandALFWorld.
quences. (2) Merging: We then merge trajecto-
riesgeneratedbymodelsacrossdifferentiterations.
Across both datasets, KNOWAGENT on 7b, 13b,
and 70b models consistently outperforms the
Fortrajectoriesaddressingthesametask,Werefine
prompt-based baselines. Notably, the 13b model
thembasedonefficiency,specificallyretainingthe
achievesaperformanceincreaseof ↑15.09% and
moreefficient(shorterpath)trajectoriesensuring
↑37.81% over ReAct on the two dataset. Addi-
optimalproblem-solvingeffectiveness.
tionally,disparitiesineffectivenessamongdiffer-
4 Experiments ent prompt methods are observed, which aligns
withcurrentresearcheffortsthatfocusonenhanc-
4.1 Settings
ing models’ capabilities to handle complex tasks
We evaluate KNOWAGENT on HotpotQA (Yang throughdiversestrategiessuchasmulti-agentspe-
etal.,2018)andALFWorld(Shridharetal.,2021). cialization. Specifically, our investigation is par-HotpotQA
Backbone Strategy Method ALFWorld
Easy Medium Hard Average
Prompting CoT(Weietal.,2022) 35.80 26.69 18.20 26.90 -
Prompting ReAcT(Yaoetal.,2023b) 25.14 19.87 17.39 20.80 14.18
Llama-2 Prompting Reflexion(Shinnetal.,2023) 35.55 28.73 24.35 29.54 6.34
7B-chat Fine-tuning FiReAct(Chenetal.,2023a) 40.56 31.70 24.13 32.13 25.38
Fine-tuning KNOWAGENT-7B 40.80 32.49 27.12 33.47 29.35
Prompting CoT(Weietal.,2022) 37.90 25.28 21.64 28.27 -
Prompting ReAcT(Yaoetal.,2023b) 28.68 22.15 21.69 24.17 20.90
Llama-2 Prompting Reflexion(Shinnetal.,2023) 44.43 37.50 28.17 36.70 34.33
13B-chat Fine-tuning FiReAct(Chenetal.,2023a) 51.95 33.93 28.88 38.26 50.37
Fine-tuning KNOWAGENT-13B 46.97 37.60 33.22 39.26 58.71
Prompting CoT (Weietal.,2022) 45.37 36.33 32.27 37.99 -
Prompting ReAcT(Yaoetal.,2023b) 39.70 37.19 33.62 36.83 55.22
Llama-2 Prompting Reflexion(Shinnetal.,2023) 48.01 46.35 35.64 43.33 69.40
70B-chat Fine-tuning FiReAct(Chenetal.,2023a) 51.96 47.56 44.60 48.04 77.61
Fine-tuning KNOWAGENT-70B 56.75 49.90 37.76 48.14 78.36
Table1: OverallperformanceofKNOWAGENTonHotpotQAandALFWorld. TheevaluationmetricsareF1
Score(%)andSuccessRate(%),respectively. Strategymeanstheagentlearningparadigmbehindeachmethod.
Thebestresultsofeachbackbonearemarkedinbold.
ticularlygearedtowardsleveragingexternalaction 4.3 Analysis
knowledgebasestofacilitatemodelsinmoreaccu-
Theroleofactionknowledgegrowswiththein-
ratelycompletingcomplextasks. Thisisachieved
crease of iterations in self-learning. Figure 4
byminimizinginvalidactions(onHotpotQA)and
showstheablationresultsaboutactionknowledge
promotingactionsequencesthatbetterreflectreal-
onHotpotQAwithLlamaseriesmodels. Regard-
worldsituations(onALFWorld),therebyimprov-
lessofthenumberofiterations,theeffectofusing
ingmodelefficiency. Furtheranalysis,especially
actionknowledge(w/actionKB)issuperiortothat
inrelationtoinvalidactionsinHotpotQA,willbe
withoutactionknowledge(w/oactionKB),indicat-
discussedlaterin§4.3.
ingthattheintroductionofactionknowledgecan
effectivelyenhancethequalityofagentplanning.
Another interesting finding is that as the number
KNOWAGENT vs. Fine-tuning Methods. Our of iterations increases, the performance gap be-
comparisonherefocusesonthefine-tuningresults tween w/o action KB and w/ action KB becomes
of KNOWAGENT versus FiReAct. A significant moresignificant,indicatingthattheadvantagesof
differenceisthatFiReAct’sfine-tuningdataissyn- introducingactionknowledgebecomemoreappar-
thesizedbyGPT-4,whereasKNOWAGENTusedits ent. Weconsiderthatthiscanbeattributedtothe
self-synthesizeddata. Forinstance,onHotpotQA, virtuouscyclebetweenactionknowledgeandself-
FiReAct employs 500 correct trajectories gener- learning. Under the constraints of action knowl-
ated by GPT-4, while KNOWAGENT also uses a edge,themodelsynthesizeshigh-qualitytrajecto-
500trainingtrajectoryvolume,butonlyselectsthe riesforiterativetraining. Inturn,trainingonmore
correctquantityofthem,approximately100to200 high-qualitytrajectoriesallowsthemodeltobetter
in 13b. The strategy also mirrors in ALFWorld. learnactionknowledge,leadingtothegeneration
Theoutcomessuggestthatmodel-synthesizeddata, ofevenmorehigh-qualitytrajectories.
infusedwithpriorknowledge,canachieveresults
comparabletothoseproducedbymoreadvanced Iterative training enhances model proficiency.
modelslikeGPT-4. Additionally,thestudyalsoin- Figure 5 presents a comparative analysis of the
dicatesthatiterativefine-tuningenablesthemodel effects of iterative training across different base
to comprehensively grasp the action knowledge, models. (1)Thenumberofiterations. Notably,el-
leadingtosuperiorplanningperformance. evatingiterationsfrom1to2resultsinasubstantial40 40 50
w/o Action KB w/o Action KB w/o Action KB
w/ Action KB w/ Action KB w/ Action KB
35 35 45
30 30 40
25 25 35
20 20 30
Iteration 0 Iteration 1 Iteration 2 Iteration 0 Iteration 1 Iteration 2 Iteration 0 Iteration 1 Iteration 2
Llama-2-7b Llama-2-13b Llama-2-70b
Figure4: AblationstudyonActionKnowledgewithinLlama-2ModelsonHotpotQA.Herew/ActionKBindicates
thenaiveKNOWAGENTandw/oActionKBsymbolizesremovingtheactionknowledgeofthespecifictask.
50 50 50 50
40 40 40 40
30 30 30 30
20 easy 20 easy 20 easy 20 easy
medium medium medium medium
hard hard hard hard
overall overall overall overall
10 10 10 10
iter0 iter1 iter2 iter3 iter4 iter0 iter1 iter2 iter3 iter4 iter0 iter1 iter2 iter3 iter4 iter0 iter1 iter2 iter3 iter4
Llama-2-7b Llama-2-13b Vicuna-7b Mistral-7b
Figure5: AblationstudyonKnowledgeableSelf-Learningiteration. Weexaminetheinfluenceofself-learning
iterationsonaselectionofmodels,includingLlama-2-7b,Llama-2-13b,Vicuna-7b,andMistral-7b. HereIter0
representsbaselineperformancepriortoanytraining.
Model InvalidAction MisorderedAction Actionknowledgeeffectivelymitigatesplanning
hallucinations. Weshowthestatisticalratesof
ReAct 2.08% 3.54%
invalid and misordered actions generated by dif-
Reflexion 6.87% 3.87%
ferent methods in Table 2. Here invalid refers
KNOWAGENT 0.35% 1.23%
to actions that do not meet the action rule, while
Table2: UnreasonableactionratesonHotpotQAwith misordered meansdiscrepanciesinthelogicalse-
Llama-2-13b. Hereinvalidreferstoactionsthatdonot quence of actions. Given that only the Search
meettheactionrule,whilemisorderedmeansdiscrep- and Finish actions are involved in FiReAct, it
anciesinthelogicalsequenceofactions.
is omitted from our analysis here. The results
in Table 2 demonstrate that incorporating Action
Knowledgesignificantlyreducesthefrequencyof
optimization of performance. Further increasing erroneous actions and the likelihood of invalid
iterations to 4 leads to an even more pronounced action paths, thereby increasing the precision of
improvement. Consistentwithpreviouswork(Li the models on the specific task. To further sub-
etal.,2023b;Wuetal.,2023),thesefindingscor- stantiate this claim, we refer to the experimental
roborate the efficacy of iterative self-learning in outcomes from KNOWAGENT and ReAct within
bolsteringthemodel’scomprehensionofthetrain- HotpotQA,asdemonstratedinAppendix6. Fora
ingdata,parallelingthehumanlearningprinciple givenquestion,ReAct’sactionsequencefollowsa
of“Reviewingtheoldasameansofrealizingthe Lookup->Search->Searchpattern,whichisprob-
new”. (2)Differentbasemodels. Wealsoexplore lematic due to the dependency of the Lookup ac-
otherbackbonemodelsexceptforLlamawitha7B tiononthesubsequentSearchstep. However,with
parameter scale, such as Vicuna-7B and Mistral- constraints, KNOWAGENT avoids such faulty se-
7B.Theresultsuggeststhatourmethodiseffective quences,enhancingtaskaccuracy.
andgeneralizableacrossdifferentpre-trainedand
fine-tunedmodels. However,theperformancedis-
Distilled Knowledge vs. Manually Designed
crepanciesamongthemalsoindicateavariancein
Knowledge. Toinvestigatewhetherutilizingad-
theabilityofdifferentmodelstoabsorbandutilize
vancedLLMscansupplantmanualeffortsincon-
suchstructuredexternalknowledge.
structingtask-specificactionknowledge,wecom-
pare distilled outcomes from gpt-4-0613 with
erocS
1F
erocS
1F
erocS
1F
erocS
1F
erocS
1F
erocS
1F
erocS
1FKNOWAGENT Strategy HotpotQA ALFWorld thecoreoflanguageagentsbyleveraginghuman-
crafted(Yaoetal.,2023b;Lietal.,2023a;Talebi-
Manual 33.47 20.15
7B radandNadiri,2023;Qianetal.,2023)ormachine-
Distilled 25.22 18.66
Manual 39.26 52.24 generated(Zhouetal.,2023b;Chenetal.,2023c,b)
13B
Distilled 25.72 51.49 prompts. Recently, therehasbeenagrowingem-
phasisonendowingopen-sourceLLMswithagent
Table3: ComparativeExperimentonManualvs. Dis-
capabilities through fine-tuning (Yin et al., 2023;
tilled Action Knowledge. Manual stands for human-
Qiaoetal.,2024;Shenetal.,2024). However,the
craftedknowledgeandDistilledrepresentsthedistilled
trainingtrajectorydataforexistinglanguageagent
knowledgefromGPT-4.
fine-tuning methods largely rely on annotations
fromLLMs. Thiscanresultintheinclusionoftra-
manual-designed ones. For HotpotQA, we ob- jectoriesthatviolatesomeactionknowledgerules
servethattheactionknowledgedistilledbyGPT-4 andaredifficulttoidentify,leadingtoanunstable
is more concise, with fewer cyclical actions than actionperformanceofthetrainedlanguageagents.
those set by humans. This efficiency holds for Recently,Guanetal.(2024)introduce AMOR,an
simplertaskswhereperformanceparallelshuman- agentframeworkthatconstructsitsreasoningcapa-
definedtasks,whileunderperformanceisfoundon bilitiesonaFiniteStateMachine(FSM).Lietal.
morecomplextaskswherelongeractionsequences (2024b)introducea“Formal-LLM”frameworkfor
are required. For ALFWorld, the GPT-distilled agents, combining the expressiveness of natural
action knowledge closely mirrors that crafted by languagewiththeprecisionofformallanguageto
humans,underscoringthemodel’scapacitytocom- enhance agent capabilities. Different from those
prehendreal-worldconstraints. Aligningwithprior approaches,weproposeknowledge-augmentedlan-
research(Dingetal.,2023;Zhouetal.,2024),this guageagentsthatincorporateaction-relatedknowl-
distilledknowledgeaidsthemodelinunderstand- edge rules to constrain the trajectory generation,
ingreal-worldlimitations,showinglittledifference reducing the occurrence of unreasonable action
ineffectivenesscomparedtohuman-createdone. logicinthegeneratedtrajectories.
ErrorAnalysis. Uponanalyzingthecapabilities
ofKNOWAGENT,weidentifyitslimitations,partic-
KnowledgeAugmentedLLMs. Previousworks
ularlyinprocessingcomplexqueriesandsumma-
(Guuetal.,2020;Lewisetal.,2020;Izacardetal.,
rizingextensivetextualdata. KNOWAGENTstrug-
2023)concentrateonknowledgeaugmentationin
gletodistillkeyinformationeffectively,oftenfail-
LLMsthroughretrieval. Duetotherichparameter-
ing to deliver accurate responses. The core issue
izedknowledgewithinLLMs(Chen,2023;Feng
liesintheirinsufficientreasoningandmemoryca-
et al., 2023), some other works (Liu et al., 2022;
pacitiesforhandlinglongcontexts. Consequently,
Yu et al., 2023; Sun et al., 2023b) advocate for
thegeneratedresponsesmaybeincorrectoreven
knowledge generation rather than retrieval. With
misalignedwiththeposedquestions,suchaspro-
the emergence of Augmented Language Models
viding a simple yes/no when a specific entity is
(ALMs),manystudies(Trivedietal.,2023;Lietal.,
required. Future enhancements should focus on
2023c; Vu et al., 2023; Qiao et al., 2023a) have
strengtheningthelong-textprocessing,information
enhanced the reasoning capabilities of LLMs by
retention,andreasoningabilitiesofourwork.
incorporatingknowledgefromexternaltoolssuch
assearchengines,knowledgebases,andWikipedia
5 RelatedWork
documents. Recentresearch(Zhouetal.,2023b;Ye
LLM-BasedAgents. LLM-basedagents(Wang etal.,2023)hasintroducedstructuredknowledgeto
etal.,2023b;Xietal.,2023;Duranteetal.,2024) regulatetheworkflowofLLM-basedmulti-agents,
haveemergedasoneofthemostprevalentAIsys- butnoneoftheseworkshasfocusedonrestricting
temsaftertheriseofLLMs(Zhaoetal.,2023;Qiao theactionspaceofindividualagents. Tothebestof
et al., 2023b; Zhu et al., 2023; Li et al., 2024a; ourknowledge,wearethefirsttointroducestruc-
Jiang et al., 2024). They learn to interact with tured action knowledge to enhance the planning
theexternalworldthroughaction-observationpairs capabilityofLLM-basedagents,specificallyaim-
expressedbynaturallanguage. Previousworkspri- ingatreducinginstancesofplanningerrorscaused
marilyfocusonunlockingthepotentialofLLMsas byinvalidactionsduringtheplanningprocess.6 Conclusion Fireact: Towardlanguageagentfine-tuning. CoRR,
abs/2310.05915.
Inthisstudy,weintroduceKNOWAGENT,aframe-
workdesignedtomitigateplanninghallucinations Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang,
JawardSesay,BörjeF.Karlsson,JieFu,andYemin
by incorporating external action knowledge into
Shi.2023b. Autoagents: Aframeworkforautomatic
synthetictrajectories. Ourmethodinvolvesutiliz-
agentgeneration. CoRR,abs/2309.17288.
ingactionknowledgetoguidethemodel’saction
HuajunChen.2023. Largeknowledgemodel: Perspec-
generation,translatingthisknowledgeintotextfor
tivesandchallenges. CoRR,abs/2312.02706.
deeper model comprehension, and employing a
knowledgeableself-learningphaseforcontinuous Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang,
improvement. Thismultifacetedapproachnotonly ChenfeiYuan,ChenQian,Chi-MinChan,YujiaQin,
YaxiLu,RuobingXie,ZhiyuanLiu,MaosongSun,
enhances the planning capabilities of agents but
andJieZhou.2023c. Agentverse: Facilitatingmulti-
alsoproveseffectiveincomplexscenarios. Ourex-
agentcollaborationandexploringemergentbehav-
perimentsacrossvariousmodelsdemonstratethat iorsinagents. CoRR,abs/2308.10848.
KNOWAGENT effectively competes with or sur-
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
passesotherbaselines,showcasingthebenefitsof
MarkChen,HeewooJun,LukaszKaiser,Matthias
integratingexternalactionknowledgetostreamline Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
planningprocessesandimproveperformance. Nakano, Christopher Hesse, and John Schulman.
2021. Training verifiers to solve math word prob-
Limitations lems. CoRR,abs/2110.14168.
Ourlimitationsarelistedasfollows: Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen,
SamuelStevens,BoshiWang,HuanSun,andYuSu.
Task Expandability. The current experiments 2023. Mind2web: Towardsageneralistagentforthe
web. CoRR,abs/2306.06070.
areconductedexclusivelyonthecommonsenseQA
andhouseholddatasets. However,ourapproachis YanDing,XiaohanZhang,SaeidAmiri,NieqingCao,
alsoapplicabletoabroaderrangeoffieldsinclud- HaoYang,AndyKaminski,ChadEsselink,andShiqi
ingmedical(Tangetal.,2023),arithmetic(Cobbe Zhang.2023. Integratingactionknowledgeandllms
for task planning and situation handling in open
etal.,2021),webbrowsing(Xieetal.,2023),and
worlds. Auton.Robots,47(8):981–997.
embodiedagents(Yangetal.,2023). Thissuggests
apotentialforwiderapplicabilitythathasyettobe ZaneDurante,QiuyuanHuang,NaokiWake,RanGong,
JaeSungPark,BidiptaSarkar,RohanTaori,Yusuke
explored.
Noda, Demetri Terzopoulos, Yejin Choi, Katsushi
Multi-Agent Systems. Presently, our research Ikeuchi,HoiVo,LiFei-Fei,andJianfengGao.2024.
AgentAI:surveyingthehorizonsofmultimodalin-
focusesontheapplicationofsingleagents. Future
teraction. CoRR,abs/2401.03568.
studiesshouldexploremulti-agentsystems,such
asChenetal.(2023c)andQiaoetal.(2024),which ZhangyinFeng,WeitaoMa,WeijiangYu,LeiHuang,
HaotianWang,QianglongChen,WeihuaPeng,Xi-
completeplanningtasksthroughdivisionoflabor
aochengFeng,BingQin,andTingLiu.2023. Trends
and collaboration. This enhancement could help
inintegrationofknowledgeandlargelanguagemod-
agents better handle complex tasks and adapt to els:Asurveyandtaxonomyofmethods,benchmarks,
changingenvironments. andapplications. CoRR,abs/2311.05876.
AutomatedDesignofActionKnowledgeBases. Jian Guan, Wei Wu, Zujie Wen, Peng Xu, Hongning
Wang,andMinlieHuang.2024. AMOR:Arecipe
Thecreationofactionknowledgebasesisstillman-
for building adaptable modular knowledge agents
ual, time-consuming, and labor-intensive. Even
throughprocessfeedback. CoRR,abs/2402.01469.
thoughweuseGPT-4fordistillingActionKnowl-
edge,manualadjustmentsareneeded. Futurework Lin Guan, Karthik Valmeekam, Sarath Sreedharan,
andSubbaraoKambhampati.2023. Leveragingpre-
should aim at automating this process to reduce
trainedlargelanguagemodelstoconstructandutilize
manualeffortandimprovethemodel’sautonomous
worldmodelsformodel-basedtaskplanning. CoRR,
learningandversatility. abs/2305.14909.
KelvinGuu,KentonLee,ZoraTung,PanupongPasupat,
and Ming-Wei Chang. 2020. Retrieval augmented
References
languagemodelpre-training. InProceedingsofthe
Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Col- 37thInternationalConferenceonMachineLearning,
lier, Karthik Narasimhan, and Shunyu Yao. 2023a. ICML2020,13-18July2020,VirtualEvent,volume119ofProceedingsofMachineLearningResearch, knowledge-intensiveNLPtasks. InAdvancesinNeu-
pages3929–3938.PMLR. ralInformationProcessingSystems33: AnnualCon-
ferenceonNeuralInformationProcessingSystems
Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng 2020,NeurIPS2020,December6-12,2020,virtual.
Cheng,JinlinWang,CeyaoZhang,ZiliWang,Steven
KaShingYau,ZijuanLin,LiyangZhou,ChenyuRan, Guohao Li, Hasan Abed Al Kader Hammoud, Hani
Lingfeng Xiao, and Chenglin Wu. 2023. Metagpt: Itani, Dmitrii Khizbullin, and Bernard Ghanem.
Meta programming for multi-agent collaborative 2023a. CAMEL:communicativeagentsfor"mind"
framework. CoRR,abs/2308.00352. exploration of large scale language model society.
CoRR,abs/2303.17760.
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
XianLi,PingYu,ChuntingZhou,TimoSchick,Luke
Allen-Zhu,YuanzhiLi,SheanWang,LuWang,and
Zettlemoyer, Omer Levy, Jason Weston, and Mike
WeizhuChen.2022. Lora: Low-rankadaptationof
Lewis.2023b. Self-alignmentwithinstructionback-
largelanguagemodels. InTheTenthInternational
translation. CoRR,abs/2308.06259.
ConferenceonLearningRepresentations,ICLR2022,
VirtualEvent,April25-29,2022.OpenReview.net.
XingxuanLi,RuochenZhao,YewKenChia,Bosheng
Ding, Lidong Bing, Shafiq R. Joty, and Soujanya
Brian Ichter, Anthony Brohan, Yevgen Chebotar,
Poria. 2023c. Chain of knowledge: A framework
Chelsea Finn, Karol Hausman, Alexander Herzog,
forgroundinglargelanguagemodelswithstructured
DanielHo,JulianIbarz,AlexIrpan,EricJang,Ryan
knowledgebases. CoRR,abs/2305.13269.
Julian,DmitryKalashnikov,SergeyLevine,YaoLu,
Carolina Parada, Kanishka Rao, Pierre Sermanet, Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li,
Alexander Toshev, Vincent Vanhoucke, Fei Xia, Yizhen Yuan, Guohong Liu, Jiacheng Liu, Wenx-
Ted Xiao, Peng Xu, Mengyuan Yan, Noah Brown, ingXu,XiangWang,YiSun,RuiKong,YileWang,
MichaelAhn,OmarCortes,NicolasSievers,Clayton Hanfei Geng, Jian Luan, Xuefeng Jin, Zilong Ye,
Tan, Sichun Xu, Diego Reyes, Jarek Rettinghouse, Guanjing Xiong, Fan Zhang, Xiang Li, Mengwei
JornellQuiambao,PeterPastor,LindaLuu,Kuang- Xu, Zhijun Li, Peng Li, Yang Liu, Ya-Qin Zhang,
HueiLee,YuhengKuang,SallyJesmonth,NikhilJ. andYunxinLiu.2024a. PersonalLLMagents: In-
Joshi,KyleJeffrey,RosarioJaureguiRuano,Jasmine sightsandsurveyaboutthecapability,efficiencyand
Hsu,KeerthanaGopalakrishnan,ByronDavid,Andy security. CoRR,abs/2401.05459.
Zeng,andChuyuanKellyFu.2022. DoasIcan,not
asIsay: Groundinglanguageinroboticaffordances. Zelong Li, Wenyue Hua, Hao Wang, He Zhu, and
InConferenceonRobotLearning,CoRL2022,14-18 Yongfeng Zhang. 2024b. Formal-llm: Integrating
December 2022, Auckland, New Zealand, volume formal language and natural language for control-
205ofProceedingsofMachineLearningResearch, lablellm-basedagents. CoRR,abs/2402.00798.
pages287–318.PMLR.
JiachengLiu,SkylerHallinan,XimingLu,PengfeiHe,
SeanWelleck,HannanehHajishirzi,andYejinChoi.
Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli,
2022. Rainier: Reinforcedknowledgeintrospector
Lucas Hosseini, Fabio Petroni, Timo Schick, Jane
forcommonsensequestionanswering. InProceed-
Dwivedi-Yu,ArmandJoulin,SebastianRiedel,and
ingsofthe2022ConferenceonEmpiricalMethods
Edouard Grave. 2023. Atlas: Few-shot learning
inNaturalLanguageProcessing,EMNLP2022,Abu
withretrievalaugmentedlanguagemodels. J.Mach.
Dhabi,UnitedArabEmirates,December7-11,2022,
Learn.Res.,24:251:1–251:43.
pages 8938–8958. Association for Computational
Linguistics.
AlbertQ.Jiang,AlexandreSablayrolles,ArthurMen-
sch,ChrisBamford,DevendraSinghChaplot,Diego
Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xu-
de Las Casas, Florian Bressand, Gianna Lengyel,
anyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding,
Guillaume Lample, Lucile Saulnier, Lélio Re-
KaiwenMen, KejuanYang, ShudanZhang, Xiang
nard Lavaud, Marie-Anne Lachaux, Pierre Stock,
Deng,AohanZeng,ZhengxiaoDu,ChenhuiZhang,
TevenLeScao,ThibautLavril,ThomasWang,Timo-
Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun,
théeLacroix,andWilliamElSayed.2023. Mistral
Minlie Huang, Yuxiao Dong, and Jie Tang. 2023a.
7b. CoRR,abs/2310.06825.
Agentbench: Evaluating llms as agents. CoRR,
abs/2308.03688.
JinhaoJiang,KunZhou,WayneXinZhao,YangSong,
Chen Zhu, Hengshu Zhu, and Ji-Rong Wen. 2024. Zhiwei Liu, Weiran Yao, Jianguo Zhang, Le Xue,
Kg-agent: Anefficientautonomousagentframework Shelby Heinecke, Rithesh Murthy, Yihao Feng,
forcomplexreasoningoverknowledgegraph. CoRR, ZeyuanChen,JuanCarlosNiebles,DevanshArpit,
abs/2402.11163. RanXu,PhilMui,HuanWang,CaimingXiong,and
Silvio Savarese. 2023b. BOLAA: benchmarking
Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik- andorchestratingllm-augmentedautonomousagents.
tus, Fabio Petroni, Vladimir Karpukhin, Naman CoRR,abs/2308.05960.
Goyal,HeinrichKüttler,MikeLewis,Wen-tauYih,
Tim Rocktäschel, Sebastian Riedel, and Douwe OpenAI. 2023. GPT-4 technical report. CoRR,
Kiela. 2020. Retrieval-augmented generation for abs/2303.08774.SiqiOuyangandLeiLi.2023. Autoplan: Automatic HaotianSun,YuchenZhuang,LingkaiKong,BoDai,
planningofinteractivedecision-makingtaskswith andChaoZhang.2023a. Adaplanner: Adaptiveplan-
largelanguagemodels. InFindingsoftheAssocia- ning from feedback with language models. CoRR,
tion for Computational Linguistics: EMNLP 2023, abs/2305.16653.
Singapore,December6-10,2023,pages3114–3128.
AssociationforComputationalLinguistics. ZhiqingSun,XuezhiWang,YiTay,YimingYang,and
DennyZhou.2023b. Recitation-augmentedlanguage
HaojiePan,ZepengZhai,HaoYuan,YaojiaLv,Ruiji models. InTheEleventhInternationalConference
Fu, Ming Liu, Zhongyuan Wang, and Bing Qin. on Learning Representations, ICLR 2023, Kigali,
2023. Kwaiagents: Generalizedinformation-seeking Rwanda,May1-5,2023.OpenReview.net.
agent system with large language models. CoRR,
YasharTalebiradandAmirhosseinNadiri.2023. Multi-
abs/2312.04889.
agentcollaboration: Harnessingthepowerofintelli-
gentLLMagents. CoRR,abs/2306.03314.
Chen Qian, Xin Cong, Cheng Yang, Weize Chen,
YushengSu,JuyuanXu,ZhiyuanLiu,andMaosong
Xiangru Tang, Anni Zou, Zhuosheng Zhang, Yilun
Sun.2023. Communicativeagentsforsoftwarede-
Zhao,XingyaoZhang,ArmanCohan,andMarkGer-
velopment. CoRR,abs/2307.07924.
stein.2023. Medagents: Largelanguagemodelsas
collaboratorsforzero-shotmedicalreasoning. CoRR,
ShuofeiQiao,HonghaoGui,HuajunChen,andNingyu
abs/2311.10537.
Zhang. 2023a. Making language models better
tool learners with execution feedback. CoRR,
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
abs/2305.13068.
Dubois,XuechenLi,CarlosGuestrin,PercyLiang,
andTatsunoriB.Hashimoto.2023. Stanfordalpaca:
ShuofeiQiao,YixinOu,NingyuZhang,XiangChen,
An instruction-following llama model. https://
YunzhiYao,ShuminDeng,ChuanqiTan,FeiHuang,
github.com/tatsu-lab/stanford_alpaca.
andHuajunChen.2023b. Reasoningwithlanguage
modelprompting: Asurvey. InProceedingsofthe XAgentTeam.2023. Xagent: Anautonomousagentfor
61stAnnualMeetingoftheAssociationforCompu- complextasksolving.
tationalLinguistics(Volume1: LongPapers),ACL
2023,Toronto,Canada,July9-14,2023,pages5368– Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
5393.AssociationforComputationalLinguistics. bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
ShuofeiQiao,NingyuZhang,RunnanFang,YujieLuo, Bhosale,DanBikel,LukasBlecher,CristianCanton-
WangchunshuZhou,YuchenEleanorJiang,Chengfei Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,
Lv,andHuajunChen.2024. AUTOACT:automatic JudeFernandes,JeremyFu,WenyinFu,BrianFuller,
agentlearningfromscratchviaself-planning. CoRR, CynthiaGao,VedanujGoswami,NamanGoyal,An-
abs/2401.05268. thonyHartshorn,SagharHosseini,RuiHou,Hakan
Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
TimoSchick,JaneDwivedi-Yu,RobertoDessì,Roberta IsabelKloumann,ArtemKorenev,PunitSinghKoura,
Raileanu,MariaLomeli,LukeZettlemoyer,Nicola Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
Cancedda,andThomasScialom.2023. Toolformer: anaLiskovich,YinghaiLu,YuningMao,XavierMar-
Languagemodelscanteachthemselvestousetools. tinet,TodorMihaylov,PushkarMishra,IgorMoly-
CoRR,abs/2302.04761. bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein,RashiRungta,KalyanSaladi,AlanSchelten,
WeizhouShen, ChenliangLi, HongzhanChen, Ming Ruan Silva, Eric Michael Smith, Ranjan Subrama-
Yan,XiaojunQuan,HehongChen,JiZhang,andFei nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
Huang.2024. Smallllmsareweaktoollearners: A lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
multi-llmagent. CoRR,abs/2401.07324. ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
Melanie Kambadur, Sharan Narang, Aurélien Ro-
NoahShinn,BeckLabash,andAshwinGopinath.2023.
driguez,RobertStojnic,SergeyEdunov,andThomas
Reflexion: language agents with verbal reinforce-
Scialom.2023. Llama2: Openfoundationandfine-
mentlearning. CoRR,abs/2303.11366.
tunedchatmodels. CoRR,abs/2307.09288.
Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, HarshTrivedi,NiranjanBalasubramanian,TusharKhot,
Yonatan Bisk, Adam Trischler, and Matthew J. andAshishSabharwal.2023. Interleavingretrieval
Hausknecht.2021. Alfworld: Aligningtextandem- with chain-of-thought reasoning for knowledge-
bodiedenvironmentsforinteractivelearning. In9th intensive multi-step questions. In Proceedings of
International Conference on Learning Representa- the61stAnnualMeetingoftheAssociationforCom-
tions, ICLR 2021, Virtual Event, Austria, May 3-7, putational Linguistics (Volume 1: Long Papers),
2021.OpenReview.net. ACL2023,Toronto,Canada,July9-14,2023,pages
10014–10037. Association for Computational Lin-
Theodore Sumers, Shunyu Yao, Karthik Narasimhan, guistics.
andThomasGriffiths.2024. Cognitivearchitectures
forlanguageagents. TransactionsonMachineLearn- KarthikValmeekam,MatthewMarquez,andSubbarao
ingResearch. SurveyCertification. Kambhampati. 2023. Can large language modelsreally improve by self-critiquing their own plans? Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-
CoRR,abs/2310.08118. gio,WilliamW.Cohen,RuslanSalakhutdinov,and
ChristopherD.Manning.2018. Hotpotqa: Adataset
Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, fordiverse,explainablemulti-hopquestionanswer-
JerryW.Wei,JasonWei,ChrisTar,Yun-HsuanSung, ing. InProceedingsofthe2018ConferenceonEm-
DennyZhou,QuocV.Le,andThangLuong.2023. pirical Methods in Natural Language Processing,
Freshllms: Refreshinglargelanguagemodelswith Brussels,Belgium,October31-November4,2018,
searchengineaugmentation. CoRR,abs/2310.03214. pages 2369–2380. Association for Computational
Linguistics.
Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-
dlekar,ChaoweiXiao,YukeZhu,LinxiFan,andAn- Shunyu Yao, Howard Chen, John Yang, and Karthik
imaAnandkumar.2023a. Voyager: Anopen-ended Narasimhan.2022. Webshop: Towardsscalablereal-
embodiedagentwithlargelanguagemodels. CoRR, worldwebinteractionwithgroundedlanguageagents.
abs/2305.16291. InAdvancesinNeuralInformationProcessingSys-
tems35: AnnualConferenceonNeuralInformation
Processing Systems 2022, NeurIPS 2022, New Or-
LeiWang,ChenMa,XueyangFeng,ZeyuZhang,Hao
leans,LA,USA,November28-December9,2022.
Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang,
Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,
Wei, and Ji-Rong Wen. 2023b. A survey on large
Thomas L. Griffiths, Yuan Cao, and Karthik
language model based autonomous agents. CoRR,
Narasimhan. 2023a. Tree of thoughts: Deliberate
abs/2308.11432.
problemsolvingwithlargelanguagemodels. CoRR,
abs/2305.10601.
JasonWei,XuezhiWang,DaleSchuurmans,Maarten
Bosma,BrianIchter,FeiXia,EdH.Chi,QuocV.Le,
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
andDennyZhou. 2022. Chain-of-thoughtprompt-
Shafran, Karthik R. Narasimhan, and Yuan Cao.
ing elicits reasoning in large language models. In
2023b. React: Synergizing reasoning and acting
NeurIPS.
inlanguagemodels. InTheEleventhInternational
ConferenceonLearningRepresentations,ICLR2023,
ShengguangWu,KemingLu,BenfengXu,JunyangLin,
Kigali,Rwanda,May1-5,2023.OpenReview.net.
QiSu,andChangZhou.2023. Self-evolveddiverse
datasamplingforefficientinstructiontuning. CoRR,
Weiran Yao, Shelby Heinecke, Juan Carlos Niebles,
abs/2311.08182.
Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy,
ZeyuanChen,JianguoZhang,DevanshArpit,Ran
ZhihengXi,WenxiangChen,XinGuo,WeiHe,Yiwen
Xu,PhilMui,HuanWang,CaimingXiong,andSil-
Ding, Boyang Hong, Ming Zhang, Junzhe Wang,
vio Savarese. 2023c. Retroformer: Retrospective
Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan,
largelanguageagentswithpolicygradientoptimiza-
Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran
tion. CoRR,abs/2308.02151.
Wang, Changhao Jiang, Yicheng Zou, Xiangyang
Liu,ZhangyueYin,ShihanDou,RongxiangWeng,
YiningYe,XinCong,ShizuoTian,JiannanCao,Hao
WensenCheng, QiZhang, WenjuanQin, Yongyan
Wang, Yujia Qin, Yaxi Lu, Heyang Yu, Huadong
Zheng, Xipeng Qiu, Xuanjing Huan, and Tao Gui.
Wang,YankaiLin,ZhiyuanLiu,andMaosongSun.
2023. Theriseandpotentialoflargelanguagemodel
2023. Proagent: Fromroboticprocessautomationto
basedagents: Asurvey. CoRR,abs/2309.07864.
agenticprocessautomation. CoRR,abs/2311.10751.
TianbaoXie,FanZhou,ZhoujunCheng,PengShi,Lu- DaYin,FaezeBrahman,AbhilashaRavichander,Khy-
oxuanWeng,YitaoLiu,TohJingHua,JunningZhao, athi Chandu, Kai-Wei Chang, Yejin Choi, and
QianLiu,CheLiu,LeoZ.Liu,YihengXu,Hongjin Bill Yuchen Lin. 2023. Lumos: Learning agents
Su, Dongchan Shin, Caiming Xiong, and Tao Yu. withunifieddata,modulardesign,andopen-source
2023. Openagents: Anopenplatformforlanguage llms. CoRR,abs/2311.05657.
agentsinthewild. CoRR,abs/2310.10634.
Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,
Jianing Yang, Xuweiyi Chen, Shengyi Qian, Nikhil Mingxuan Ju, Soumya Sanyal, Chenguang Zhu,
Madaan, MadhavanIyengar, DavidF.Fouhey, and Michael Zeng, and Meng Jiang. 2023. Generate
JoyceChai.2023. Llm-grounder: Open-vocabulary rather than retrieve: Large language models are
3dvisualgroundingwithlargelanguagemodelasan strong context generators. In The Eleventh Inter-
agent. CoRR,abs/2309.12311. national Conference on Learning Representations,
ICLR2023,Kigali,Rwanda,May1-5,2023.Open-
Ke Yang, Jiateng Liu, John Wu, Chaoqi Yang, Yi R. Review.net.
Fung, Sha Li, Zixuan Huang, Xu Cao, Xingyao
Wang,YiquanWang,HengJi,andChengxiangZhai. AohanZeng,MingdaoLiu,RuiLu,BowenWang,Xiao
2024. IfLLMisthewizard,thencodeisthewand:A Liu,YuxiaoDong,andJieTang.2023. Agenttuning:
surveyonhowcodeempowerslargelanguagemodels Enablinggeneralizedagentabilitiesforllms. CoRR,
toserveasintelligentagents. CoRR,abs/2401.00812. abs/2310.12823.Zhuosheng Zhang, Yao Yao, Aston Zhang, Xiangru rewardmetricutilizedinBOLAA.Moredetailsof
Tang,XinbeiMa,ZhiweiHe,YimingWang,Mark HotpotQAarelistedinAppendixB.
Gerstein, RuiWang, GongshenLiu, andHaiZhao.
ALFWorld (Shridhar et al., 2021) is an inter-
2023. Igniting language intelligence: The hitch-
hiker’sguidefromchain-of-thoughtreasoningtolan- active, text-based household environment where
guageagents. CoRR,abs/2311.11797. agentchallengetocompletesixdifferenttypesof
multi-steptasks. Totrainourmodel,werandomly
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,
select85instancesofeachtaskcategoryfromthe
Xiaolei Wang, Yupeng Hou, Yingqian Min, Be-
trainingset. Furthermore,werefertopreviousre-
ichenZhang,JunjieZhang,ZicanDong,YifanDu,
Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao search (Shridhar et al., 2021; Yao et al., 2023b)
Jiang,RuiyangRen,YifanLi,XinyuTang,Zikang anduse134unseenvalidationtaskstoevaluateour
Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.
method. InALFWorld,weutilizegoal-conditioned
2023. A survey of large language models. CoRR,
success rates as our evaluation metric. We show
abs/2303.18223.
promptsusedforbothdatasetsinAppendixD.
LianminZheng,Wei-LinChiang,YingSheng,Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Baselines Our research concentrates solely on
ZhuohanLi,DachengLi,Eric.PXing,HaoZhang,
theperformanceofasingleagent,intentionallyex-
JosephE.Gonzalez,andIonStoica.2023. Judging
cludingmulti-agentstudiesfromourbaselinecom-
llm-as-a-judgewithmt-benchandchatbotarena.
parison. Wechoosethefollowingbaselineshere:
PeiZhou,JayPujara,XiangRen,XinyunChen,Heng- (1)Chain-of-Thought(CoT)(Weietal.,2022)cat-
Tze Cheng, Quoc V. Le, Ed H. Chi, Denny Zhou,
alyzesin-depthreasoninginlargelanguagemod-
SwaroopMishra,andHuaixiuStevenZheng.2024.
els(LLMs)byincorporatingintermediatereason-
Self-discover: Largelanguagemodelsself-compose
reasoningstructures. CoRR,abs/2402.03620. ing steps within examples. (2) ReAct (Yao et al.,
2023b) enables LLMs to intertwine the genera-
Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou,
tionofinferentialtrajectoriesandactions,allowing
RobertLo,AbishekSridhar,XianyiCheng,Yonatan
the model to better generalize and adjust its ac-
Bisk,DanielFried,UriAlon,andGrahamNeubig.
2023a. Webarena: Arealisticwebenvironmentfor tionplans. (3)TheReflexion(Shinnetal.,2023)
buildingautonomousagents. CoRR,abs/2307.13854. methodutilizesself-reflectivefeedbacktopromote
continuousagentdevelopmentthroughtheassim-
Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li,
ilation of lessons from past errors, thus refining
JialongWu,TiannanWang,ShiQiu,JintianZhang,
task execution. (4) Furthermore, FiReAct (Chen
JingChen,RuipuWu,ShuaiWang,ShidingZhu,Jiyu
Chen,WentaoZhang,NingyuZhang,HuajunChen, et al., 2023a) introduces a novel strategy to fine-
Peng Cui, and Mrinmaya Sachan. 2023b. Agents: tuneLLMusingdiversepromptsandtrajectories
Anopen-sourceframeworkforautonomouslanguage
acrosstasks,demonstratingthatricherfine-tuning
agents. CoRR,abs/2309.07870.
datacanfurtherelevatetheperformanceofagents.
Yuqi Zhu, Xiaohan Wang, Jing Chen, Shuofei Qiao, It’simportanttonotethatourexperimentsconcen-
YixinOu,YunzhiYao,ShuminDeng,HuajunChen, trate on exploring the impact of external action
andNingyuZhang.2023. Llmsforknowledgegraph
knowledge on LLMs, and as such, we have cho-
constructionandreasoning: Recentcapabilitiesand
sen a single-agent framework as the baseline for
futureopportunities. CoRR,abs/2305.13168.
comparison.
A ExperimentalSettings
Implementation. We evaluate KNOWAGENT
Datasets and Metrics. HotpotQA (Yang et al., on HotpotQA (Yang et al., 2018) and ALF-
2018) is specifically designed for multi-hop rea- World (Shridhar et al., 2021). We employ
soningtasksandcomprisesapproximately113,000 Llama-2-{7,13,70}b-chat(Touvronetal.,2023)
question-answeringpairsderivedfromWikipedia asthebackbonemodels,andalsoapply KNOWA-
articles. Inourexperiments, asubsetof500data GENT tovicuna-7b-v1.5-16k(Zhengetal.,2023)
instances is randomly sampled for training. For and Mistral-7B-Instruct-v0.1 (Jiang et al., 2023).
testing,weutilizethesametestsetasBOLAA(Liu During the self-learning phase, we set the num-
etal.,2023b),whichincluded100samplesateach beroftrainingiterationstotwo. Wefine-tunethese
of three difficulty levels: easy, medium and hard. modelsover5epochsusingLoRA(Huetal.,2022),
Tomeasureperformance,weadopttheF1scoreas settingalearningrateof1×10−4andthesequence
abenchmark,whichalsoservesasthebasisforthe lengthof4096acrossallscales. Additionally,weemploy an Alpaca-style (Taori et al., 2023) tem-
platingapproachforgeneratinginputfrominstruc-
tionaldata.
B Datasets
In HotpotQA, we have enriched the action set in
previousworks(Yaoetal.,2023b;Liuetal.,2023b)
byincorporatingBingSearchasanexternalknowl-
edgesource. Asaresult,wenowfeaturefourdis-
tinctactionswithinHotpotQAtoenhanceitsfunc-
tionalityandperformance: (1)Retrieve[entity]: Re-
trievetheexactentityonWikipediaandreturnthe
firstparagraphifitexists. Ifnot,returnsomesim-
ilarentitiesforsearching. (2)Search[topic]: Use
BingSearchtofindrelevantinformationonaspeci-
fiedtopic,question,orterm. (3)Lookup[keyword]:
Returnthenextsentencethatcontainsthekeyword
inthelastpassagesuccessfullyfoundbySearchor
Retrieve. (4) Finish[answer]: Return the answer
andconcludethetask.
C ImpactofActionKnowledge
SeeFigure6
D Prompt
D.1 PromptforHotpotQA
SeeTable4.
D.2 PromptforALFWorld
SeeTable5toTable10.ReAct KnowAgent
Question: Is Children's National Medical Center or MedStar Question: Is Children's National Medical Center or MedStar
Washington Hospital Center the largest private hospital in Washington Hospital Center the largest private hospital in
Washington, D.C.? Washington, D.C.?
Thought1: ActionPath1:Start
IneedtofindwhoisthelargestprivatehospitalinWashington,D.C. Thought1:…
bycomparingChildren'sNationalMedicalCenterandMedStarWash A"Search“actionseemsappropriatetogatherthisinformation.
ingtonHospitalCenter. Action1:Search[Children'sNationalMedicalCenter]
Action1:Lookup[Children'sNationalMedicalCenter] Invalid …
Observation1:ThelastpageSearchedwasnotfound, ... ActionPath3:Start->Search[Children'sNationalMedicalCenter]-
Thought2:Hmm,neitheroftheaddressesissimilartoanyofthepr >Search[MedStarWashingtonHospitalCenter]
eviousSearches.MaybeIneedtorephrasethequestion. Thought3:…MedStarWashingtonHospitalCenteristhelargestpriva
Action2:Search[questionrephrased] tehospitalinWashington,D.C.Nofurtheractionsareneededasthe
… necessaryinformationhasbeenobtained.
Action 3:Finish[notpossible] Action3:Finish[MedStarWashingtonHospitalCenter]
Observation3:AnswerisINCORRECT Observation3:AnswerisCORRECT
Figure6: KNOWAGENT(theblueone)successfullycompletesplanning,whileReAct(theorangeone)exhibits
planninghallucinationsthatcontradictactionknowledge.PromptforHotpotQA
Yourtaskistoansweraquestionusingaspecificgraph-basedmethod. Youmustnavigatefromthe
"Start"nodetothe"Finish"nodebyfollowingthepathsoutlinedinthegraph. Thecorrectpathisa
seriesofactionsthatwillleadyoutotheanswer.
Thedecisiongraphisconstructeduponasetofprinciplesknownas"ActionKnowledge",outlined
asfollows:
Start:(Search,Retrieve)
Retrieve:(Retrieve,Search,Lookup,Finish)
Search:(Search,Retrieve,Lookup,Finish)
Lookup:(Lookup,Search,Retrieve,Finish)
Finish:()
Here’showtointerpretthegraph’sActionKnowledge:
From"Start",youcaninitiatewitheithera"Search"ora"Retrieve"action.
At the "Retrieve" node, you have the options to persist with "Retrieve", shift to "Search",
experimentwith"Lookup",oradvanceto"Finish".
Atthe"Search"node,youcanrepeat"Search",switchto"Retrieve"or"Lookup",orproceed
to"Finish".
Atthe"Lookup"node,youhavethechoicetokeepusing"Lookup",switchto"Search"or
"Retrieve",orcompletethetaskbygoingto"Finish".
The"Finish"nodeisthefinalactionwhereyouprovidetheanswerandthetaskiscompleted.
Eachnodeactionisdefinedasfollows:
(1)Retrieve[entity]: RetrievetheexactentityonWikipediaandreturnthefirstparagraphifit
exists. Ifnot,returnsomesimilarentitiesforsearching.
(2)Search[topic]: UseBingSearchtofindrelevantinformationonaspecifiedtopic,question,
orterm.
(3)Lookup[keyword]: Returnthenextsentencethatcontainsthekeywordinthelastpassage
successfullyfoundbySearchorRetrieve.
(4)Finish[answer]: Returntheanswerandconcludethetask.
Asyousolvethequestionusingtheabovegraphstructure,interleaveActionPath,Thought,Action,
andObservationsteps. ActionPathdocumentsthesequenceofnodesyouhavetraversedwithinthe
graph. Thoughtanalyzesthecurrentnodetorevealpotentialnextstepsandreasonsforthecurrent
situation.
Youmaytakeasmanystepsasnecessary.
Herearesomeexamples:
{examples}
(ENDOFEXAMPLES)
Question: {question}{scratchpad}
Table4: PromptforHotpotQA.ALFWorld-Pick
Interactwithahouseholdtosolveataskbyfollowingthestructured"ActionKnowledge". The
guidelinesare:
Goto(receptacle)->Open(receptacle)
[Goto(receptacle),Open(receptacle)]->Take(object,from: receptacle)
Take(object,from: receptacle)->Goto(receptacle)
[Goto(receptacle),Take(object,from: receptacle)]->Put(object,in/on: receptacle)
Here’showtointerprettheActionKnowledge:
Beforeyouopenareceptacle,youmustfirstgotoit. Thisruleapplieswhenthereceptacleisclosed.
To take an object from a receptacle, you either need to be at the receptacle’s location, or if it’s
closed,youneedtoopenitfirst.
Beforeyougotothenewreceptaclewheretheobjectistobeplaced,youshouldtakeit.
Puttinganobjectinoronareceptaclecanfolloweithergoingtothelocationofthereceptacleor
aftertakinganobjectwithyou.
Theactionsareasfollows:
1)gotoreceptacle
2)takeobjectfromreceptacle
3)putobjectin/onreceptacle
4)openreceptacle
AsyoutacklethequestionwithActionKnowledge,utilizeboththeActionPathandThinksteps.
ActionPathrecordstheseriesofactionsyou’vetaken,andtheThinkstepunderstandsthecurrent
situationandguidesyournextmoves.
Herearetwoexamples.
{examples}
Hereisthetask.
Table5: PromptforthePickTask.ALFWorld-Light
Interactwithahouseholdtosolveataskbyfollowingthestructured"ActionKnowledge". The
guidelinesare:
[Goto(receptacle)]->Open(receptacle)
[Goto(receptacle),Open(receptacle)]->Take(object,from: receptacle)
[Goto(receptacle)]->Use(receptacle)
Here’showtointerprettheActionKnowledge:
Beforeyouopenareceptacle,youmustfirstgotoit. Thisruleapplieswhenthereceptacleisclosed.
To take an object from a receptacle, you either need to be at the receptacle’s location, or if it’s
closed,youneedtoopenitfirst.
Touseanreceptacle,youmustgototheplacewhereitislocated.
Theactionsareasfollows:
1)gotoreceptacle
2)takeobjectfromreceptacle
3)usereceptacle
4)openreceptacle
AsyoutacklethequestionwithActionKnowledge,utilizeboththeActionPathandThinksteps.
ActionPathrecordstheseriesofactionsyou’vetaken,andtheThinkstepunderstandsthecurrent
situationandguidesyournextmoves.
Herearetwoexamples.
{examples}
Hereisthetask.
Table6: PromptfortheLightTask.ALFWorld-Clean
Interactwithahouseholdtosolveataskbyfollowingthestructured"ActionKnowledge". The
guidelinesare:
[Goto(receptacle)]->Open(receptacle)
[Goto(receptacle),Open(receptacle)]->Take(object,from: receptacle)
[Goto(receptacle),Take(object,from: receptacle)]->Put(object,in/on: receptacle)
[Put(object,from: receptacle)]->Clean(object,with: receptacle)
Here’showtointerprettheActionKnowledge:
Beforeyouopenareceptacle,youmustfirstgotoit. Thisruleapplieswhenthereceptacleisclosed.
To take an object from a receptacle, you either need to be at the receptacle’s location, or if it’s
closed,youneedtoopenitfirst.
Puttinganobjectinoronareceptaclecanfolloweithergoingtothelocationofthereceptacleor
aftertakinganobjectwithyou.
Tocleananobjectusingareceptacle,theobjectmustfirstbeplacedinoronthatreceptacle.
Theactionsareasfollows:
1)gotoreceptacle
2)takeobjectfromreceptacle
3)openreceptacle
4)putobjectin/onreceptacle
5)cleanobjectwithreceptacle
AsyoutacklethequestionwithActionKnowledge,utilizeboththeActionPathandThinksteps.
ActionPathrecordstheseriesofactionsyou’vetaken,andtheThinkstepunderstandsthecurrent
situationandguidesyournextmoves.
Herearetwoexamples.
{examples}
Hereisthetask.
Table7: PromptfortheCleanTask.ALFWorld-Heat
Interactwithahouseholdtosolveataskbyfollowingthestructured"ActionKnowledge". The
guidelinesare:
[Goto(receptacle)]->Open(receptacle)
[Goto(receptacle),Open(receptacle)]->Take(object,from: receptacle)
[Goto(receptacle),Take(object,from: receptacle)]->Put(object,in/on: receptacle)
[Put(object,in/on: receptacle)]->Heat(object,with: receptacle)
Here’showtointerprettheActionKnowledge:
Beforeyouopenareceptacle,youmustfirstgotoit. Thisruleapplieswhenthereceptacleisclosed.
To take an object from a receptacle, you either need to be at the receptacle’s location, or if it’s
closed,youneedtoopenitfirst.
Puttinganobjectinoronareceptaclecanfolloweithergoingtothelocationofthereceptacleor
aftertakinganobjectwithyou.
Toheatanobjectusingareceptacle,theobjectmustfirstbeplacedinoronthatreceptacle.
Theactionsareasfollows:
1)gotoreceptacle
2)takeobjectfromreceptacle
3)openreceptacle
4)putobjectin/onreceptacle
5)heatobjectwithreceptacle
AsyoutacklethequestionwithActionKnowledge,utilizeboththeActionPathandThinksteps.
ActionPathrecordstheseriesofactionsyou’vetaken,andtheThinkstepunderstandsthecurrent
situationandguidesyournextmoves.
Herearetwoexamples.
{examples}
Hereisthetask.
Table8: PromptfortheHeatTask.ALFWorld-Cool
Interactwithahouseholdtosolveataskbyfollowingthestructured"ActionKnowledge". The
guidelinesare:
[Goto(receptacle)]->Open(receptacle)
[Goto(receptacle),Open(receptacle)]->Take(object,from: receptacle)
[Goto(receptacle),Take(object,from: receptacle)]->Put(object,in/on: receptacle)
[Put(object,in/on: receptacle)]->Cool(object,with: receptacle)
Here’showtointerprettheActionKnowledge:
Beforeyouopenareceptacle,youmustfirstgotoit. Thisruleapplieswhenthereceptacleisclosed.
To take an object from a receptacle, you either need to be at the receptacle’s location, or if it’s
closed,youneedtoopenitfirst.
Puttinganobjectinoronareceptaclecanfolloweithergoingtothelocationofthereceptacleor
aftertakinganobjectwithyou.
Tocoolanobjectusingareceptacle,theobjectmustfirstbeplacedinoronthatreceptacle.
Theactionsareasfollows:
1)gotoreceptacle
2)takeobjectfromreceptacle
3)openreceptacle
4)putobjectin/onreceptacle
5)coolobjectwithreceptacle
AsyoutacklethequestionwithActionKnowledge,utilizeboththeActionPathandThinksteps.
ActionPathrecordstheseriesofactionsyou’vetaken,andtheThinkstepunderstandsthecurrent
situationandguidesyournextmoves.
Herearetwoexamples.
{examples}
Hereisthetask.
Table9: PromptfortheCoolTask.ALFWorld-PickTwo
Interactwithahouseholdtosolveataskbyfollowingthestructured"ActionKnowledge". The
guidelinesare:
Goto(receptacle)->Open(receptacle)
[Goto(receptacle),Open(receptacle)]->Take(object,from: receptacle)
Take(object,from: receptacle)->Goto(receptacle)
[Goto(receptacle),Take(object,from: receptacle)]->Put(object,in/on: receptacle)
Here’showtointerprettheActionKnowledge:
Beforeyouopenareceptacle,youmustfirstgotoit. Thisruleapplieswhenthereceptacleisclosed.
To take an object from a receptacle, you either need to be at the receptacle’s location, or if it’s
closed,youneedtoopenitfirst.
Beforeyougotothenewreceptaclewheretheobjectistobeplaced,youshouldtakeit.
Puttinganobjectinoronareceptaclecanfolloweithergoingtothelocationofthereceptacleor
aftertakinganobjectwithyou.
Ensurethefirstobjectisplacedbeforeproceedingtodepositthesecondobject.
Theactionsareasfollows:
1)gotoreceptacle
2)takeobjectfromreceptacle
3)putobjectin/onreceptacle
4)openreceptacle
AsyoutacklethequestionwithActionKnowledge,utilizeboththeActionPathandThinksteps.
ActionPathrecordstheseriesofactionsyou’vetaken,andtheThinkstepunderstandsthecurrent
situationandguidesyournextmoves.
Herearetwoexamples.
{examples}
Hereisthetask.
Table10: PromptforthePick TwoTask.