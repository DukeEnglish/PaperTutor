Active Statistical Inference
Tijana Zrnic∗ Emmanuel J. Cand`es†
{tijana.zrnic,candes}@stanford.edu
∗Department of Statistics and Stanford Data Science
†Department of Statistics and Department of Mathematics
Stanford University
Abstract
Inspiredbytheconceptofactivelearning,weproposeactiveinference—amethodologyforstatistical
inferencewithmachine-learning-assisteddatacollection. Assumingabudgetonthenumberoflabelsthat
can be collected, the methodology uses a machine learning model to identify which data points would
be most beneficial to label, thus effectively utilizing the budget. It operates on a simple yet powerful
intuition: prioritizethecollectionoflabelsfordatapointswherethemodelexhibitsuncertainty,andrely
on the model’s predictions where it is confident. Active inference constructs provably valid confidence
intervals and hypothesis tests while leveraging any black-box machine learning model and handling any
data distribution. The key point is that it achieves the same level of accuracy with far fewer samples
than existing baselines relying on non-adaptively-collected data. This means that for the same number
of collected samples, active inference enables smaller confidence intervals and more powerful p-values.
We evaluate active inference on datasets from public opinion research, census analysis, and proteomics.
1 Introduction
In the realm of data-driven research, collecting high-quality labeled data is a continuing impediment. The
impediment is particularly acute when operating under stringent labeling budgets, where the cost and effort
of obtaining each label can be substantial. Recognizing these limitations, many have turned to machine
learning as a pragmatic solution, leveraging it to predict unobserved labels across various fields. In remote
sensing, machine learning assists in annotating and interpreting satellite imagery [23, 39, 49]; in proteomics,
tools like AlphaFold [24] are revolutionizing our understanding of protein structures; even in the realm of
elections—including most major US elections—technologies combining scanners and predictive models are
usedasefficienttoolsforvotecounting[50]. Theseapplicationsreflectagrowingrelianceonmachinelearning
for extracting information and knowledge from unlabeled datasets.
However, this reliance on machine learning is not without its pitfalls. The core issue lies in the inherent
biasesofthesemodels. Nomatterhowsophisticated,predictionsleadtodubiousconclusions;assuch,predic-
tions cannot fully substitute for traditional data sources such as gold-standard experimental measurements,
high-quality surveys, and expert annotations. This begs the question: is there a way to effectively leverage
the predictive power of machine learning while still ensuring the integrity of our inferences?
Drawing inspiration from the concept of active learning, we propose active inference—a novel methodol-
ogy for statistical inference that harnesses machine learning not as a replacement for data collection but as
a strategic guide to it. The methodology uses a machine learning model to identify which data points would
be most beneficial to label, thus effectively utilizing the labeling budget. It operates on a simple yet pow-
erful intuition: prioritize the collection of labels for data points where the model exhibits uncertainty, and
rely on the model’s predictions where it is confident. Active inference constructs provably valid confidence
intervals and hypothesis tests for any black-box machine learning model and any data distribution. The
key takeaway is that it achieves the same level of accuracy with far fewer samples than existing baselines
1
4202
raM
5
]LM.tats[
1v80230.3042:viXrarelying on non-adaptively-collected data. Put differently, this means that for the same number of collected
samples, active inference enables smaller confidence intervals and more powerful p-values. We will show in
our experiments that active inference can save over 80% of the sample budget required by classical inference
methods (see Figure 4).
Although quite different in scope, our work is inspired by the recent framework of prediction-powered
inference (PPI) [1]. PPI assumes access to a small labeled dataset and a large unlabeled dataset, drawn
i.i.d. from the population of interest. It then asks how one can use machine learning and the unlabeled
dataset to sharpen inference about population parameters depending on the distribution of labels. Our
objective in this paper is different since the core of our contribution is (1) designing strategic data collection
approaches that enable more powerful inferences than collecting labels in an i.i.d. manner, and (2) showing
how to perform inference with such strategically collected data. That said, we will see that prediction-
powered inference can be seen as a special case of our methodology: while PPI ignores the issue of strategic
data collection and instead uses a trivial, uniform data collection strategy, it leverages machine learning to
enhanceinferenceinasimilarwaytoourmethod. WeprovideafurtherdiscussionofpriorworkinSection3.
2 Problem description
We introduce the formal problem setting. We observe unlabeled instances X ,...,X , drawn i.i.d. from
1 n
a distribution P . The labels Y are unobserved, and we shall use (X,Y) ∼ P = P ×P to denote
X i X Y|X
a generic feature–label pair drawn from the underlying data distribution. We are interested in performing
inference—conductingahypothesistestorformingaconfidenceinterval—foraparameterθ∗ thatdependson
the distribution of the unobserved labels; that is, the parameter is a functional of P ×P . For example,
X Y|X
we might be interested in forming a confidence interval for the mean label, θ∗ =E[Y ], where Y is the label
i i
correspondingtoX . Althoughwewillprimarilyfocuson forming confidenceintervals, thestandardduality
i
between confidence intervals and hypothesis tests makes our results directly applicable to testing as well.
We have no collected labels a priori. Rather, the goal is to efficiently and strategically acquire labels for
certain points X , so that inference is as powerful as possible for a given collection budget—more so than
i
if labels were collected uniformly at random—while also remaining valid. We denote by n the number
lab
of collected labels. We assume that we are constrained to collect, on average, E[n ] ≤ n labels, for some
lab b
budget n . Typically, n ≪n.
b b
To guide the choice of which instances to label, we will make use of a predictive model f. Typically this
will be a black-box machine learning model, but it could also be a hand-designed decision rule based on
expert knowledge. This is the key component that will enable us to get a significant boost in power. We
do not assume any knowledge of the predictive performance of f, or any parametric form for it. Our key
takeaway is that, if we have a reasonably good model for predicting the labels Y based on X , then we can
i i
achieve a significant boost in power compared to labeling a uniformly-at-random chosen set of instances.
We will consider two settings, depending on whether or not we update the predictive model f as we
gather more labels.
• The first is a batch setting, where we simultaneously make decisions of whether or not to collect the
corresponding label for all unlabeled points at once. In this setting, the model f is pre-trained and
remains fixed during the label collection. The batch setting is simpler and arguably more practical if
we already have a good off-the-shelf predictor.
• The second setting is sequential: we go through the unlabeled points one by one and update the
predictive model as we collect more data. The benefit of the second approach is that it is applicable
even when we do not have access to a pre-trained model, but we have to train a model from scratch.
Our proposed active inference strategy will be applicable to all convex M-estimation problems. This
means that it handles all targets of inference θ∗ that can be written as:
θ∗ =argminE[ℓ (X,Y)], where (X,Y)∼P,
θ
θ
2for a loss function ℓ that is convex in θ. We denote L(θ)=E[ℓ (X,Y)] for brevity. M-estimation captures
θ θ
manyrelevanttargets,suchasthemeanlabel,quantilesofthelabel,andregressioncoefficients. Weformally
state several important examples.
Example 1 (Mean label). If ℓ (x,y) = 1(y−θ)2, then the target is the mean label, θ∗ = E[Y]. Note that
θ 2
this loss has no dependence on the features.
Example 2 (Linear regression). If ℓ (x,y)= 1(y−x⊤θ)2, then θ∗ is the vector of linear regression coeffi-
θ 2
cients obtained by regressing y on x, that is, the “effect” of x on y.
Example 3 (Labelquantile). For a given q ∈(0,1), let ℓ (x,y)=q(y−θ)1{y >θ}+(1−q)(θ−y)1{y ≤θ}
θ
be the “pinball” loss. Then, θ∗ is equal to the q-quantile of the label distribution: θ∗ =inf{θ :P(Y ≤θ)≥q}.
3 Related work
Our work is most closely related to prediction-powered inference (PPI) and other recent works on inference
withmachinelearningpredictions[1,3,18,31,32,55]. Thisrecentliteratureinturnrelatestoclassicalwork
oninferencewithmissingdataandsemiparametricstatistics[13,37,38,40,41,42],aswellassemi-supervised
inference [5, 51, 54]. We consider the same set of inferential targets as in [1, 3, 55], building on classical
M-estimation theory [47] to enable inference. While PPI assumes access to a small labeled dataset and a
large unlabeled dataset, which are drawn i.i.d., our work is different in that it leverages machine learning in
order to design adaptive label collection strategies, which breaks the i.i.d. structure between the labeled and
the unlabeled data. That said, we shall however see that our active inference estimator will reduce to the
prediction-poweredestimatorwhenweapplyatrivial,uniformlabelcollectionstrategy. Wewilldemonstrate
empirically that the adaptivity in label collection enables significant improvements in statistical power.
There is a growing literature on inference from adaptively collected data [14, 27, 53], often focusing on
data collected via a bandit algorithm. These papers typically focus on average treatment effect estimation.
In contrast to our work, these works generally do not focus on how to set the data-collection policy as
to achieve good statistical power, but their main focus is on providing valid inferences given a fixed data-
collection policy. Notably, Zhang et al. [52] study inference for M-estimators from bandit data. However,
their estimators do not leverage machine learning, which is central to our work.
Asubstantiallineofworkstudiesadaptiveexperimentdesign[8,10,19,20,22,26,29,30,36],oftenwith
the goal of maximizing welfare during the experiment or identifying the best treatment. Most related to our
motivation, a subset of these works [10, 20, 30] study adaptive design with the goal of efficiently estimating
average treatment effects. While our motivation is not necessarily treatment effect estimation, we continue
in a similar vein—collecting data adaptively with the goal of improved efficiency—with a focus on using
modern, black-box machine learning to produce uncertainty estimates that can be turned into efficient label
collection methods. Related variance-reduction ideas appear in stratified survey sampling [25, 28, 33, 43].
Our proposal can be seen as stratifying the population of interest based on the certainty of a black-box
machine learning model.
Finally, ourworkdrawsinspirationfromactivelearning, whichisasubareaofmachinelearningcentered
around the observation that a machine learning model can enhance its predictive capabilities if it is allowed
to choose the data points from which it learns. In particular, our setup is analogous to pool-based active
learning [45]. Many works in this area show the benefits of sampling according to a measure of predictive
uncertainty[4,6,21,44,46]. Themaindifferencefromthisliteratureisthatourgoalisstatisticalinference,
whileactivelearningaimstotrainagoodpredictivemodel. Moregenerally,wenotethatthereisalargebody
ofworkstudyingwaystoefficientlycollectgold-standardlabels,oftenunderabudgetconstraint[12,48,52].
4 Warm-up: active inference for mean estimation
Wefirstfocusonthespecialcaseofestimatingthemeanlabel,θ∗ =E[Y],inthebatchsetting. Theintuition
derived from this example carries over to all other problems.
3Recallthesetup: weobserveni.i.d. unlabeledinstancesX ,...,X ,andwecancollectlabelsforatmost
1 n
n of them (on average). Consider first a “classical” solution, which does not leverage machine learning.
b
Given a budget n , we can simply label any arbitrarily chosen n points. Since the instances are i.i.d.,
b b
without loss of generality we can choose to label instances {1,...,n } and compute:
b
θˆnoML =
1
(cid:88)nb
Y .
n i
b
i=1
The estimator θˆnoML is clearly unbiased. It is an average over n terms, and thus its variance is equal to
b
1
Var(θˆnoML)= Var(Y).
n
b
Now, suppose that we are given a machine learning model f(X), which predicts the label Y ∈ R from
observed covariates X ∈ X.1 The idea behind our active inference strategy is to increase the effective
sample size by using the model’s predictions on points X where the model is confident and focusing the
labelingbudgetonthepointsX wherethemodelisuncertain. Toimplementthisidea,wedesignasampling
rule π : X → [0,1] and collect label Y with probability π(X ). The sampling rule is derived from f, by
i i
appropriatelymeasuringitsuncertainty. Thehopeisthatπ(x)≈1signalsthatthemodelf isveryuncertain
about instance x, whereas π(x) ≈ 0 indicates that the model f should be very certain about instance x.
Let ξ ∼Bern(π(X )) denote the indicator of whether we collect the label for point i. By definition, n =
i i lab
(cid:80)n ξ . The rule π will be carefully rescaled to meet the budget constraint: E[n ]=E[π(X)]·n≤n .
i=1 i lab b
Our active estimator of the mean θ∗ is given by:
n (cid:18) (cid:19)
θˆπ = 1 (cid:88) f(X )+(Y −f(X )) ξ i . (1)
n i i i π(X )
i
i=1
This is essentially the augmented inverse propensity weighting (AIPW) estimator [38], with a particular
choice of propensities π(X ) based on the certainty of the machine learning model that predicts the missing
i
labels. When the sampling rule is uniform, i.e. π(x)=n /n for all x, θˆπ is equal to the prediction-powered
b
mean estimator [1].
It is not hard to see that θˆπ is unbiased: E[θˆπ]=θ∗. A short calculation shows that its variance equals
(cid:18) (cid:20) (cid:18) (cid:19)(cid:21)(cid:19)
1 1
Var(θˆπ)= Var(Y)+E (Y −f(X))2 −1 . (2)
n π(X)
If the model is highly accurate for all x, i.e. f(X)≈Y, then Var(θˆπ)≈ 1Var(Y), which is far smaller than
n
Var(θˆnoML) since n ≪n. Of course, f will never be perfect and accurate for all instances x. For this reason,
b
we will aim to choose π such that π is small when f(X) ≈ Y and large otherwise, so that the relevant
term (Y
−f(X))2(cid:0) π−1(X)−1(cid:1)
is always small (of course, subject to the sampling budget constraint). For
example,forinstancesforwhichthepredictoriscorrect,i.e. f(X)=Y,wewouldideallyliketosetπ(X)=0
as this incurs no additional variance.
We note that the variance reduction of active inference compared to the “classical” solution also implies
that the resulting confidence intervals get smaller. This follows because interval width scales with the
standard deviation for most standard intervals (e.g., those derived from the central limit theorem).
Finally, we explain how to set the sampling rule π. The rule will be derived from a measure of model
uncertainty u(x) and we shall provide different choices of u(x) in the following paragraphs. At a high level,
one can think of u(X ) as the model’s best guess of |Y −f(X )|. We will choose π(x) proportional to u(x),
i i i
thatis,π(x)∝u(x),normalizedtomeetthebudgetconstraint. Intuitively,thismeansthatwewanttofocus
our data collection budget on parts of the covariate space where the model is expected to make the largest
errors. Roughly speaking, we will set π(x)= E[u u( (x X) )]·n nb; this implies E[n lab]=E[π(X)]·n≤n b. (This is an
1X isthesetofvaluesthecovariatescantakeon,e.g.Rd.
4idealizedformofπ(x)becauseE[u(X)]cannotbeknownexactly,thoughitcanbeestimatedveryaccurately
from the unlabeled data; we will formalize this in the following section.)
We will take two different approaches for choosing the uncertainty u(x), depending on whether we are in
a regression or a classification setting.
Regression uncertainty In regression, we explicitly train a model u(x) to predict |f(X )−Y | from X .
i i i
We note that we aim to predict only the magnitude of the error and not the directionality. In the batch
setting, we typically have historical data of (X,Y) pairs that are used to train the model f. We thus train
u(x) onthishistorical data, bysetting |f(X)−Y| as the target label forinstance X. Thedata usedto train
u should ideally be disjoint from the data used to train f to avoid overoptimistic estimates of uncertainty.
We will typically use data splitting to avoid this issue, though there are more data efficient solutions such
as cross-fitting. Notice that access to historical data will only be important in the batch setting, as assumed
in this section. In the sequential setting we will be able to train u(x) gradually on the collected data.
Classification uncertainty Next we look at classification, where Y is supported on a discrete set of
values. Our main focus will be on binary classification, where Y ∈ {0,1}. In such cases, our target is
θ∗ = P(Y = 1). We might care about E[Y] more generally when Y takes on K distinct values (e.g., K
distinct ratings in a survey, K distinct qualification levels, etc).
In classification, f(x) is usually obtained as the “most likely” class. If K is the number of classes,
we have f(x) = argmax p (x), for some probabilistic output p(x) = (p (x),...,p (x)) which satisfies
i∈[K] i 1 K
(cid:80)K
p (x)=1. For example, p(x) could be the softmax output of a neural network given input x. We will
i=1 i
measure the uncertainty as:
(cid:18) (cid:19)
K
u(x)= · 1−maxp (x) . (3)
K−1 i∈[K] i
In binary classification, this reduces to u(x) = 2min{p(x),1−p(x)}, where we use p(x) to denote the raw
classifier output in [0,1]. Therefore, u(x) is large when p(x) is close to uniform, i.e. max p (x)≈1/K. On
i i
the other hand, if the model is confident, i.e. max p (x)≈1, the uncertainty is close to zero.
i i
5 Batch active inference
BuildingonthediscussionfromSection4,weprovideformalresultsforactiveinferenceinthebatchsetting.
Recall that in the batch setting we observe i.i.d. unlabeled points X ,...,X , all at once. We consider a
1 n
family of sampling rules π (x)=ηu(x), where u(x) is the chosen uncertainty measure and η ∈H⊆R+ is a
η
tuning parameter. We will discuss ways of choosing u(x) in Section 7. The role of the tuning parameter is
to scale the sampling rule to the sampling budget. We choose
(cid:40) n (cid:41)
(cid:88)
ηˆ=max η ∈H:η u(X )≤n , (4)
i b
i=1
and deploy π as the sampling rule. With this choice, we have
ηˆ
(cid:34) n (cid:35)
(cid:88)
E[n ]=E ηˆu(X ) ≤n ;
lab i b
i=1
therefore, π
ηˆ
meets the label collection budget. We denote θˆη ≡θˆπη.
Mean estimation WefirstexplainhowtoperforminferenceformeanestimationinProposition1. Recall
the active mean estimator:
n (cid:18) (cid:19)
θˆηˆ = 1 (cid:88) f(X )+(Y −f(X )) ξ i , (5)
n i i i π (X )
ηˆ i
i=1
5where ξ ∼ Bern(π (X )). Following standard notation, z below denotes the qth quantile of the standard
i ηˆ i q
normal distribution.
Proposition 1. Suppose that there exists η∗ ∈H such that P(ηˆ̸=η∗)→0. Then
√
n(θˆηˆ−θ∗)→d N(0,σ2),
∗
where σ2 = Var(f(X)+(Y −f(X)) ξη∗ ) and ξη∗ ∼ Bern(π (X)). Consequently, for any σˆ2 →p σ2,
∗ πη∗(X) η∗ ∗
C
α
=(θˆηˆ±z 1−α/2√σˆ n) is a valid (1−α)-confidence interval:
lim P(θ∗ ∈C )=1−α.
α
n→∞
A few remarks about Proposition 1 are in order: first, the consistency condition P(ηˆ̸=η∗)→0 is easily
ensured if n /n has a limit p ∈ (0,1), that is, if n is asymptotically proportional to n. Then, as long as
b b
the space of tuning parameters H is discrete and there is no η ∈ H such that ηE[u(X)] = p exactly, the
consistency condition is met. Second, obtaining a consistent variance estimate σˆ2 is straightforward, as one
can simply take the empirical variance of the increments in the estimator (5).
General M-estimation Next, we turn to general convex M-estimation. Recall this means that we can
write θ∗ = argmin L(θ) = argmin E[ℓ (X,Y)], for a convex loss ℓ . To simplify notation, let ℓ =
θ θ θ θ θ,i
ℓ (X ,Y ), ℓf =ℓ (X ,f(X )). We similarly use ∇ℓ and ∇ℓf . For a general sampling rule π, our active
θ i i θ,i θ i i θ,i θ,i
estimator is defined as
n (cid:18) (cid:19)
θˆπ =argminLπ(θ), where Lπ(θ)= 1 (cid:88) ℓf +(ℓ −ℓf ) ξ i . (6)
n θ,i θ,i θ,i π(X )
θ i
i=1
As before, ξ ∼Bern(π(X )). When π is the uniform rule, π(x)=n /n, the estimator (6) equals the general
i i b
prediction-powered estimator from [3]. Notice that the loss estimate Lπ(θ) is unbiased: E[Lπ(θ)] = L(θ).
We again scale the sampling rule π (x)=ηu(x) according to the sampling budget, as in Eq. (4).
η
We next show asymptotic normality of θˆηˆ for general targets θ∗ which, in turn, enables inference. The
result essentially follows from the usual asymptotic normality for M-estimators [47, Ch. 5], with some nec-
essary modifications to account for the data-driven selection of ηˆ. We require standard, mild smoothness
assumptions on the loss ℓ , formally stated in Ass. 1 in the Appendix.
θ
Theorem 1 (CLT for batch active inference). Assume the loss is smooth (Ass. 1) and define the Hessian
H = ∇2E[ℓ (X,Y)]. Suppose that there exists η∗ ∈ H such that P(ηˆ̸= η∗) → 0. Then, if θˆη∗ →p θ∗, we
θ∗ θ∗
have
√
n(θˆηˆ−θ∗)→d
N(0,Σ ), where
∗
Σ = H−1Var(cid:16) ∇ℓf +(cid:16) ∇ℓ −∇ℓf (cid:17) ξη∗ (cid:17) H−1. Consequently, for any Σˆ →p Σ , C = (θˆηˆ ±
∗ θ∗ θ∗,i θ∗,i θ∗,i πη∗(X) θ∗ ∗ α j
(cid:113)
z
Σˆ
jj) is a valid (1−α)-confidence interval for θ∗:
1−α/2 n j
lim P(θ∗ ∈C )=1−α.
j α
n→∞
The remarks following Proposition 1 again apply: the consistency condition on ηˆ is easily ensured if
n /n has a limit, and Σˆ admits a simple plug-in estimate by replacing all quantities with their empirical
b
counterparts. The consistency condition on θˆη∗ is a standard requirement for analyzing M-estimators (see
[47, Ch. 5]); it is studied and justified at length in the literature and we shall therefore not discuss it in
close detail. We however remark that it can be deduced if the empirical loss Lπ(θ) is almost surely convex
or if the parameter space is compact. The empirical loss Lπ(θ) is convex in a number of cases of interest,
including means and generalized linear models; for the proof, see [3].
66 Sequential active inference
InthebatchsettingweobservealldatapointsX ,...,X atonceandfixapredictivemodelf andsampling
1 n
rule π that guide our choice of which labels to collect. An arguably more natural data collection strategy
would operate in an online manner: as we collect more labels, we iteratively update the model and our
strategy for which labels to collect next. This allows for further efficiency gains over using a fixed model
throughout, as the latter ignores knowledge acquired during the data collection. For example, if we are
conducting a survey and we collect responses from members of a certain demographic group, it is only
natural that we update our sampling rule to reflect the fact that we have more knowledge and certainty
about that demographic group.
Formally, instead of having a fixed model f and rule π, we go through our data sequentially. At step
t∈ {1,...,n}, we observe data point X and collect its label with probability π (X ), where π (·) is based
t t t t
on the uncertainty of model f . The model f can be fine-tuned on all information observed up to time t;
t t
formally,werequirethatf ,π ∈F ,whereF istheσ-algebrageneratedbythefirsttpointsX ,1≤s≤t,
t t t−1 t s
their labeling decisions ξ , and their labels Y , if observed: F = σ((X ,Y ξ ,ξ ),...,(X ,Y ξ ,ξ )). (Note
s s t 1 1 1 1 t t t t
that Y ξ = Y if and only if ξ = 1; otherwise, Y ξ = ξ = 0.) We will again calibrate our decisions of
t t t t t t t
whether to collect a label according to a budget on the sample size n . We denote by n the number of
b lab,t
labels collected up to time t.
Inference in the sequential setting is more challenging than batch inference because the data points
(X ,Y ,ξ ),t∈[n],aredependent; indeed,thepurposeofthesequentialsettingistoleveragepreviousobser-
t t t
vations when deciding on future labeling decisions. We will construct estimators that respect a martingale
structure,whichwillenabletractableinferenceviathemartingalecentrallimittheorem[17]. Thisresembles
the approach taken by Zhang et al. [53] (though our estimators are quite different due to the use of machine
learning predictions).
Meanestimation Webeginbyfocusingonthemean. Ifwetakeℓ tobethesquaredlossasinExample1,
θ
we obtain the sequential active mean estimator:
n
θˆπ#» = 1 (cid:88) ∆ , ∆ =f (X )+(Y −f (X )) ξ t .
n t t t t t t t π (X )
t t
t=1
We note that ∆ are martingale increments; they share a common conditional mean E[∆ |F ] = θ∗, and
t t t−1
they are F -measurable, ∆ ∈F . We let σ2 =V(f ,π )=Var(∆ |f ,π ) denote the conditional variance of
t t t t t t t t t
the increments.
Toshowasymptoticnormalityofθˆπ#»
,weshallrequiretheLindebergcondition,whosestatementwedefer
to the Appendix. It is a standard assumption for proving central limit theorems when the increments are
not i.i.d.. Roughly speaking, the Lindeberg condition requires that the increments do not have very heavy
tails; it prevents any increment from having a disproportionately large contribution to the overall variance.
Proposition 2. Suppose 1 (cid:80)n σ2 →p σ2 =V(f ,π ), for some fixed model–rule pair (f ,π ), and that the
n t=1 t ∗ ∗ ∗ ∗ ∗
increments ∆ satisfy the Lindeberg condition (Ass. 2). Then
t
√ n(θˆπ#» −θ∗)→d N(0,σ2).
∗
Consequently, for any σˆ2 →p σ ∗2, C
α
=(θˆπ#» ±z 1−α/2√σˆ n) is a valid (1−α)-confidence interval:
lim P(θ∗ ∈C )=1−α.
α
n→∞
Intuitively, Proposition 2 requires that the model f and sampling rule π converge. For example, a
t t
sufficient condition for 1 (cid:80)n σ2 →p σ2 is V(f ,π )→L1 V(f ,π ). Since the sampling rule is typically based
n t=1 t ∗ n n ∗ ∗
on the model, it makes sense that it would converge if f converges. At the same time, it makes sense for f
t t
to gradually stop updating after sufficient accuracy is achieved.
7General M-estimation We generalize Proposition 2 to all convex M-estimation problems. The general
version of our sequential active estimator takes the form
n
θˆπ#» =argminLπ#» (θ), where Lπ#» (θ)= 1 (cid:88) L (θ), L (θ)=ℓft +(ℓ −ℓft ) ξ t . (7)
n t t θ,t θ,t θ,t π (X )
θ t t
t=1
Let V = V (f ,π ) = Var(∇L (θ)|f ,π ). We will again require that (f ,π ) converge in an appropriate
θ,t θ t t t t t t t
sense.
Theorem2(CLTforsequentialactiveinference). Assumethelossissmooth(Ass.1)anddefinetheHessian
H =∇2E[ℓ (X,Y)]. Suppose also that 1 (cid:80)n V →p V =V (f ,π ) entry-wise for some fixed model–
θ∗ θ∗ n t=1 θ∗,t ∗ θ∗ ∗ ∗
rule pair (f ,π ), and that the increments L (θ) satisfy the Lindeberg condition (Ass. 3). Then, if θˆπ#» →p θ∗,
∗ ∗ t
we have
√ n(θˆπ#» −θ∗)→d
N(0,Σ ),
∗
(cid:113)
where Σ = H−1V H−1. Consequently, for any Σˆ →p Σ , C = (θˆπ#» ±z Σˆ jj) is a valid (1−α)-
∗ θ∗ ∗ θ∗ ∗ α j 1−α/2 n
confidence interval for θ∗:
j
lim P(θ∗ ∈C )=1−α.
j α
n→∞
TheconditionsofTheorem2arelargelythesameasinTheorem1;themaindifferenceistherequirement
ofconvergenceofthemodel–samplingrulepairs,whichissimilartotheanalogousconditionofProposition2.
Proposition2 and Theorem 2 apply to any sampling rule π , as long asthe varianceconvergence require-
t
mentismet. Wediscusswaystosetπ sothatthesamplingbudgetn ismet. Ourdefaultwillbeto“spread
t b
out”thebudgetn overthenobservations. Wewilldosobyhavingan“imaginary”budgetfortheexpected
b
number of collected labels by step t, equal to n =tn /n. Let n =n −n denote the remaining
b,t b ∆,t b,t lab,t−1
budget at step t. We derive a measure of uncertainty u from model f , as before, and let
t t
π (x)=min{η u (x),n } , (8)
t t t ∆,t [0,1]
whereη normalizesu (x)andthesubscript[0,1]denotesclippingto[0,1]. Thenormalizingconstantη can
t t t
be arbitrary, but we find it helpful to set it roughly as η = n /(nE[u (X)]) and this is what we do in our
t b t
experiments, with the proviso that we substitute E[u (X)] with its empirical approximation. In words, the
t
sampling probability is high if the uncertainty is high and we have not used up too much of the sampling
budgetthusfar. Ofcourse,ifthemodelconsistentlyestimateslowuncertaintyu (x)throughout,thebudget
t
will be underutilized. For this reason, to make sure we use up the budget in practice, we occasionally
set π (x) = (n ) regardless of the reported uncertainty. This periodic deviation from the rule (8) is
t ∆,t [0,1]
consistent with the variance convergence conditions required for Proposition 2 and Theorem 2 to hold.
7 Choosing the sampling rule
We have seen how to perform inference given an abstract sampling rule, and argued that, intuitively, the
sampling rule should be calibrated to the uncertainty of the model’s predictions. Here we argue that this
is in fact the optimal strategy. In particular, we derive an “oracle” rule, which optimally sets the sampling
probabilities so that the variance of θˆπ is minimized. While the oracle rule cannot be implemented since it
depends on unobserved information, it provides an ideal that our algorithms will try to approximate. We
discuss ways of tuning the approximations to make them practical and powerful.
7.1 Oracle sampling rules
We begin with the optimal sampling rule for mean estimation. We then state the optimal rule for general
M-estimation and instantiate it for generalized linear models.
8Mean estimation Recall the expression for Var(θˆπ) (2). Given that E(cid:2) π−1(X)(Y −f(X))2(cid:3) is the only
term that depends on π, we define the oracle rule as the solution to:
(cid:20) (cid:21)
1 n
min E (Y −f(X))2 s.t. E[π(X)]≤ b. (9)
π π(X) n
Theoptimizationproblem(9)appearsinanumberofothertopics,includingimportancesampling[34,Ch.9]
andconstrainedutilityoptimization[7]. Theoptimalityconditionsof(9)showthatitssolutionπ satisfies:
opt
(cid:112)
π (X)∝ E[(Y −f(X))2|X],
opt
where ∝ ignores the normalizing constant required to make E[π (X)] ≤ n /n. Therefore, the optimal
opt b
sampling rule is one that samples data points according to the expected magnitude of the model error: the
largerthemodelerror,thehighertheprobabilityofsamplingshouldbe. Ofcourse,E[(Y −f(X))2|X]cannot
be known since the label distribution is unknown, and that is why we call π an oracle.
opt
Todevelopintuition, itisinstructivetoconsideranevenmorepowerfuloracleπ˜ (X,Y)thatisallowed
opt
to depend on Y. To be clear, we would commit to the same functional form as in (1) and would seek to
minimize Var(θˆπ) while allowing the sampling probabilities to depend on both X and Y. In this case, by
the same argument we conclude that
π˜ (X,Y)∝|Y −f(X)|. (10)
opt
The perspective of allowing the oracle to depend on both X and Y is directly prescriptive: a natural way
to approximate the rule π˜ is to train an arbitrary black-box model u on historical (X,Y) pairs to predict
opt
|Y −f(X)| from X. We provide further practical guidelines for sampling rules at the end of this section.
General M-estimation In the case of general M-estimation, we cannot hope to minimize the variance
of θˆπ at a fixed sample size n since the finite-sample distribution of θˆπ is not tractable. However, we can
find a sampling rule that minimizes the asymptotic variance of θˆπ. Since the estimator is potentially multi-
dimensional,tomaketheproblemwell-posedweassumethatwewanttominimizetheasymptoticvarianceof
a single coordinate θˆπ (for example, one coefficient in a multi-dimensional regression). Recall the expression
j
for the asympotic covariance Σ from Theorem 1. A short derivation shows that
∗
(cid:34)(cid:18)(cid:16) (cid:17)⊤ (cid:19)2 1 (cid:35)
Σ =E ∇ℓ −∇ℓf h(j) · +C,
∗,jj θ∗,i θ∗,i π(X)
where h(j) is the j-th column of H−1 and C is a term that has no dependence on the sampling rule π.
θ∗
Therefore, by the same theory as for mean estimation, the ideal rule π (X) would be
opt
(cid:113)
π (X)∝ E[((∇ℓ (X,Y)−∇ℓ (X,f(X)))⊤h(j))2|X].
opt θ∗ θ∗
This recovers π for the mean, since ∇ℓ (x,y)=θ∗−y and h(j) =1 for the squared loss. Our measure of
opt θ∗
uncertainty u(x) should therefore approximate the errors of the predicted gradients along the h(j) direction.
Generalized linear models (GLMs) We simplify the general solution π in the case of generalized
opt
linear models (GLMs). We define GLMs as M-estimators whose loss function takes the form
ℓ (x,y)=−logp (y|x)=−yx⊤θ+ψ(x⊤θ),
θ θ
forsomeconvexlog-partitionfunctionψ. Thisdefinitionrecoverslinearregressionbytakingψ(s)= 1s2 and
2
logistic regression by taking ψ(s) = log(1+es). By the definition of the GLM loss, we have ∇ℓ (x,y)−
θ∗
∇ℓ (x,f(x))=(f(x)−y)x and, therefore,
θ∗
(cid:112)
π (X)∝ E[(f(X)−Y)2|X]·|X⊤h(j)|,
opt
9where the Hessian is equal to H = E[ψ′′(X⊤θ )XX⊤] and h(j) is the j-th column of H−1. In linear
θ∗ ∗ θ∗
regression, for instance, H =E[XX⊤]. Again, we see that the model errors play a role in determining the
θ∗
optimal sampling. In particular, again considering the more powerful oracle π˜ (X,Y) that is allowed to
opt
set the sampling probabilities according to both X and Y, we get
π˜ (X,Y)∝|f(X)−Y|·|X⊤h(j)|. (11)
opt
Therefore, as in the case of the mean, our measure of uncertainty will aim to predict |f(X)−Y| from X
and plug those predictions into the above rule.
7.2 Practical sampling rules
As explained in Section 5 and Section 6, our sampling rule π(x) will be derived from a measure of un-
certainty u(x). As clear from the preceding discussion, the right notion of uncertainty should measure a
notion of error dependent on the estimation problem at hand. In particular, we hope to have u(X) ≈
|(∇ℓ (X,Y)−∇ℓ (X,f(X)))⊤h(j)|. For GLMs and means, in light of Eq. (10) and Eq. (11), this often
θ∗ θ∗
boils down to training a predictor of |f(X)−Y| from X and, in the case of GLMs, using a plug-in estimate
of the Hessian. This is what we do in our experiments (except in the case of binary classification where we
simply use the uncertainty from Eq. (3)).
Ofcourse,thelearnedpredictorofmodelerrorscannotbeperfect;asaresult,π(x)∝u(x)cannotnaively
betreatedastheoracleruleπ . Forexample,themodelmightmistakenlyestimate(near-)zerouncertainty
opt
(u(X)≈0) when |f(X)−Y| is large, which would blow up the estimator variance. To fix this issue, we find
that it helps to stabilize the rule π(x)∝u(x) by mixing it with a uniform rule.
Denote the uniform rule by πunif(x)=n /n. Clearly the uniform rule meets the budget constraint, since
b
nE[πunif(X)]=n . For a fixed τ ∈[0,1] and π(x)∝u(x), we define the τ-mixed rule as
b
π(τ)(x)=(1−τ)·π(x)+τ ·πunif(x).
Any positive value of τ ensures that π(τ)(x) > 0 for all x, avoiding instability due to small uncertainty
estimates u(x). When historical data is available, one can tune τ by optimizing the empirical estimate of
the (asymptotic) variance of θˆπ(τ) given by Theorem 1. For example, in the case of mean estimation, this
would correspond to solving:
(cid:88)nh
1
τˆ=argmin (Yh−f(Xh))2, (12)
π(τ)(Xh) i i
τ∈[0,1] i=1 i
where (Xh,Yh),...,(Xh ,Yh ) are the historical data points. Otherwise, one can set τ to be any user-
i i nn nh
specified constant. In our experiments, in the batch setting we tune τ on historical data when such data is
available. In the sequential setting we simply set τ =0.5 as the default.
8 Experiments
We evaluate active inference on several problems and compare it to two baselines.
The first baseline replaces active sampling with the uniformly random sampling rule πunif. Importantly,
this baseline still uses machine learning predictions f(X ) and corresponds to prediction-powered inference
i
(PPI) [1]. Formally, the prediction-powered estimator is given by
n n
θˆPPI =argminLPPI(θ), where LPPI(θ)= 1 (cid:88) ℓ (X ,f(X ))+ 1 (cid:88) (ℓ (X ,Y )−ℓ (X ,f(X )))ξ ,
n θ i i n θ i i θ i i i
θ b
i=1 i=1
whereξ
i
∼Bern(n nb). Thisestimatorcanberecoveredasaspecialcaseofestimator(6). Thepurposeofthis
comparison is to quantify the benefits of machine-learning-driven data collection. In the rest of this section
10Algorithm 1 Batch active inference
Input: unlabeled data X ,...,X , sampling budget n , predictive model f, error level α∈(0,1)
1 n b
1: Choose uncertainty measure u(x) based on f
2: Let π(x)=ηˆu(x), where ηˆ= nb ; let πunif = nb
nEˆ[u(X)] n
3: Select τ ∈(0,1) and choose sampling rule π(τ)(x)=(1−τ)·π(x)+τ ·πunif
4: Sample labeling decisions ξ i ∼Bern(π(τ)(X i)),i∈[n]
5: Collect labels {Y i :ξ i =1}
6: Compute batch active estimator θˆπ(τ) (Eq. (6))
Algorithm 2 Sequential active inference
Input: unlabeled data X ,...,X , sampling budget n , initial predictive model f , error level α ∈ (0,1),
1 n b 1
fine-tuning batch size B
1: Set Dtune ←∅
2: for t=1,...,n do
3: Choose uncertainty measure u t(x) for f t
4: Set π t(x) as in Eq. (8) with η t = nEˆ[un tb (X)]; let πunif = n nb
5: Select τ ∈(0,1) and choose sampling rule π t(τ)(x)=(1−τ)·π t(x)+τ ·πunif
6: Sample labeling decision ξ t ∼Bern(π t(τ)(X t))
7: if ξ t =1 then
8: Collect label Y t
9: Dtune ←Dtune∪{(X t,Y t)}
10: if |Dtune|=B then
11: Fine-tune model on Dtune: f t+1 =finetune(f t,Dtune)
12: Set Dtune ←∅
13: else
14: f t+1 ←f t
15: else
16: f t+1 ←f t
17: Compute sequential active estimator
θˆπ#»(τ)
(Eq. (7))
we refer to this baseline as the “uniform” baseline because the only difference from our estimator is that it
replaces active sampling with uniform sampling.
The second baseline removes machine learning altogether and computes the “classical” estimate based
on uniformly random sampling,
n
θˆnoML =argmin 1 (cid:88) ℓ (X ,Y )ξ ,
n θ i i i
θ b
i=1
where ξ
i
∼ Bern(n nb). This baseline serves to evaluate the cumulative benefits of machine learning for data
collectionand inferencecombined. Werefertothisbaselineasthe“classical”baseline,orclassicalinference,
in the rest of this section.
For all methods we compute standard confidence intervals based on asymptotic normality. The target
errorlevelisα=0.1throughout. Wereporttheaverageintervalwidthandcoverageforvaryingsamplesizes
n ,averagedover1000and100trialsforthebatchandsequentialsettings,respectively. Weplottheinterval
b
width on a log–log scale. We also report the percentage of budget saved by active inference relative to the
baselines when the methods are matched to be equally accurate. More precisely, for varying n we compute
b
the average interval width achieved by the uniform and classical baselines; then, we look for the budget size
nactive forwhichactiveinferenceachievesthesameaverageintervalwidth,andreport(n −nactive)/n ·100%
b b b b
as the percentage of budget saved.
The batch and sequential active inference methods used in our experiments are outlined in Algorithm 1
111.0
0.18 active
0.12 0.9 uniform
classical
0.08 0.8
0.05 0.7
0.04
0.6
0.5 0.6 0.7 0.8 220 304 418 576 792 221 364 507 650 793
approval rate nb nb
1.0
0.12 active
0.09 0.9 uniform
classical
0.07 0.8
0.05 0.7
0.04
0.6
0.2 0.3 0.4 222 305 420 577 795 222 365 508 651 795
approval rate nb nb
Figure 1: Post-election survey research. Example intervals in five randomly chosen trials (left), average
confidence interval width (middle), and coverage (right) for the average approval of Joe Biden’s (top) and
Donald Trump’s (bottom) political messaging to the country following the 2020 US presidential election.
andAlgorithm2,respectively. Eachapplicationwillspecifythegeneralparametersfromthealgorithmstate-
ments. We defer some experimental details, such as the choices of τ, to Appendix B. Code for reproducing
the experiments is available at https://github.com/tijana-zrnic/active-inference.
8.1 Post-election survey research
We apply active inference to survey data collected by the Pew Research Center following the 2020 United
States presidential election [35]. We focus on one specific question in the survey, aimed at gauging people’s
approval of the presidential candidates’ political messaging following the election. The target of inference
is the average approval rate of Joe Biden’s (Donald Trump’s, respectively) political messaging. Approval is
encoded as a binary response, Y ∈{0,1}.
i
The respondents—a nationally representative pool of US adults—provide background information such
as age, gender, education, political affiliation, etc. We show that, by training a machine learning model to
predictpeople’sapprovalfromtheirbackgroundinformationandmeasuringthemodel’suncertainty, wecan
allocate the per-question budget in a way that achieves higher statistical power than uniform allocation.
Careful budget allocation is important, because Pew pays each respondent proportionally to the number of
questions they answer.
We use half of all available data for the analysis; for the purpose of evaluating coverage, we take the
average approval on all available data as the ground truth θ∗. To obtain the predictive model f, we train an
XGBoost model [11] on the half of the data not used for the analysis. Since approval is encoded as a binary
response, we use the measure of uncertainty from Eq. (3).
In Figure 1 we compare active inference to the uniform (PPI) and classical baselines. All methods meet
the coverage requirement. Across different values of the budget n , active sampling reduces the confidence
b
interval width of the uniform baseline (PPI) by a significant margin (at least ∼10%). Classical inference is
highly suboptimal compared to both alternatives. In Figure 4 we report the percentage of budget saved due
to active sampling. For estimating Biden’s approval, we observe an over 85% save in budget over classical
inference and around 25% save over the uniform baseline. For estimating Trump’s approval, we observe an
over 70% save in budget over classical inference and around 25% save over the uniform baseline.
12
htdiw
lavretni
htdiw
lavretni
egarevoc
egarevoc1.0
809 active
437 0.9 uniform
classical
236 0.8
127 0.7
69
0.6
800 1000 1200 1400 189 444 1040 2435 5701 190 1567 2945 4323 5701
regression coefficient nb nb
Figure2: Census data analysis. Exampleintervalsinfiverandomlychosentrials(left),averageconfidence
intervalwidth(middle), andcoverage(right)forthelinearregressioncoefficientquantifyingtherelationship
between age and income, controlling for sex, in US Census data.
1.0
7.64 active
3.99 0.9 uniform
classical
2.08 0.8
1.09 0.7
0.57 0.6
2 4 6 108 228 482 1021 2159 108 621 1134 1647 2160
odds ratio nb nb
Figure 3: AlphaFold-assisted proteomics research. Example intervals in five randomly chosen trials
(left), average confidence interval width (middle), and coverage (right) for the odds ratio between phospho-
rylation and being part of an IDR.
8.2 Census data analysis
Next, we study the American Community Survey (ACS) Public Use Microdata Sample (PUMS) collected
by the US Census Bureau. ACS PUMS is an annual survey that collects information about citizenship,
education,income,employment,andotherfactorspreviouslycontainedonlyinthelongformofthedecennial
census. We use the Folktables [15] interface to download the data. We investigate the relationship between
age and income in survey data collected in California in 2019, controlling for sex. Specifically, we target the
linear regression coefficient when regressing income on age and sex (that is, its age coordinate).
Analogously to the previous application, we use half of all available data for the analysis and train an
XGBoost model [11] of a person’s income from the available demographic covariates on the other half. As
the ground-truth value of the target θ∗, we take the corresponding linear regression coefficient computed on
allavailabledata. Toquantifythemodel’suncertainty,weusethestrategydescribedinSection4,traininga
separateXGBoostmodele(·)topredict|f(X)−Y|fromX. Then,wesettheuncertaintyu(x)asprescribed
in Eq. (11), replacing |f(X)−Y| by e(X).
The interval widths and coverage are shown in Figure 2. As in the previous application, all methods
approximatelyachievethetargetcoverage,howeverthistimeweobservemoreextremegainsovertheuniform
baseline (PPI): the interval widths almost double when going from active sampling to uniform sampling. Of
course, theimprovementofactiveinferenceoverclassicalinferenceisevenmoresubstantial. Thelargegains
of active sampling can also be seen in Figure 4: we save around 80% of the budget over classical inference
and over 60% over the uniform baseline.
8.3 AlphaFold-assisted proteomics research
Inspired by the findings of Bludau et al. [9] and the subsequent analysis of Angelopoulos et al. [1], we
study the odds ratio of a protein being phosphorylated, a functional property of a protein, and being part
13
htdiw
lavretni
htdiw
lavretni
egarevoc
egarevocPost-election research Census analysis AlphaFold
100 100 100
75 75 75
50 50 50
Biden
25 Trump 25 25
0 0 0
250 500 750 1000 1250 2000 3000 4000 5000 500 1000 1500 2000
nb nb nb
100 100 100
75 Biden 75 75
Trump
50 50 50
25 25 25
0 0 0
250 500 750 1000 1250 2000 3000 4000 5000 500 1000 1500 2000
nb nb nb
Figure4: Save in sample budget due to active inference. Reductioninsamplesizerequiredtoachieve
the same confidence interval width with active inference and (top) classical inference and (bottom) uniform
sampling, respectively, across the applications shown in Figures 1-3.
of an intrinsically disordered region (IDR), a structural property. The latter can only be obtained from
knowledgeabouttheproteinstructure,whichcaninturnbemeasuredtoahighaccuracyonlyviaexpensive
experimental techniques. To overcome this challenge, Bludau et al. used AlphaFold predictions [24] to
estimate the odds ratio. AlphaFold is a machine learning model that predicts a protein’s structure from
its amino acid sequence. Angelopoulos et al. [1] showed that forming a classical confidence interval around
the odds ratio based on AlphaFold predictions is not valid given that the predictions are imperfect. They
provide a valid alternative assuming access to a small subset of proteins with true structure measurements,
uniformly sampled from the larger population of proteins of interest.
Weshowthat,bystrategicallychoosingwhichproteinstructurestoexperimentallymeasure,activeinfer-
enceallowsforintervalsthatretainvalidityandaretighterthanintervalsbasedonuniformsampling. Natu-
rally, for the purpose of evaluating validity, we restrict the analysis to proteins where we have gold-standard
structuremeasurements;weusethepost-processedAlphaFoldoutputsmadeavailablebyAngelopoulosetal.
[1], which predict the IDR property based on the raw AlphaFold output. We leverage the predictions to
guide the choice of which structures to experimentally derive, subject to a budget constraint. Since the
predictionsofIDRarebinary,wequantifytheuncertaintyasperEq.(3). Theoddsratioweaimtoestimate
is defined as:
µ /(1−µ )
θ∗ = 1 1 ,
µ (1−µ )
0 0
where µ = P(Y = 1|X = 1) and µ = P(Y = 1|X = 0); Y is a binary indicator of disorder and X is
1 ph 0 ph ph
a binary indicator of phosphorylation. While the odds ratio is not a solution to an M-estimation problem,
it is a function of two means, µ and µ (see also [1, 3]). Confidence intervals can thus be computed by
1 0
applying the delta method to the asymptotic normality result for the mean. Since Y is binary, we use the
measure of uncertainty from Eq. (3) to estimate µ and µ . For the purpose of evaluating coverage, we take
1 0
the empirical odds ratio computed on the whole dataset as the ground-truth value of θ∗.
Figure3showstheintervalwidthsandcoverageforthethreemethods,andFigure4showsthepercentage
of budget saved due to adaptive data collection. The gains are substantial: over 75% of the budget is saved
in comparison to classical inference, and around 20−25% is saved in comparison to the uniform baseline
(PPI).Giventhecostofexperimentalmeasurementtechniquesinproteomics, thissaveinsamplesizewould
imply a massive save in cost.
8.4 Post-election survey research with fine-tuning
We return to the example from Section 8.1, this time evaluating the benefits of sequential fine-tuning. We
compare active inference, with and without fine-tuning, and PPI, which relies on uniform sampling. We
14
evas
tegdub
evas
tegdub
)%(
lacissalc
revo
)%(
mrofinu
revo0.038 1.0 active (w/ fine-tuning)
0.031 0.9 active (no fine-tuning)
uniform
0.026 0.8
0.022 0.7
0.019 0.6
0.68 0.70 0.72 1855 2370 3029 3871 4946 1855 2628 3401 4174 4947
approval rate nb nb
0.039 1.0 active (w/ fine-tuning)
0.032 0.9 active (no fine-tuning)
uniform
0.027 0.8
0.022 0.7
0.018 0.6
0.28 0.30 0.32 1860 2376 3037 3881 4961 1860 2635 3410 4185 4961
approval rate nb nb
Figure 5: Post-election survey research with fine-tuning. Example intervals in five randomly chosen
trials (left), average confidence interval width (middle), and coverage (right) for the average approval of
Joe Biden’s (top) and Donald Trump’s (bottom) political messaging to the country following the 2020 US
presidential election. Active inference with no fine-tuning and inference with uniformly sampled data use
the same model.
show that active inference with no fine-tuning can hurt compared to PPI if the former uses a poorly trained
model; fine-tuning, on the other hand, remedies this issue. The predictive model may be poorly trained due
to little or no historical data; sequential fine-tuning is necessary in such cases.
We train an XGBoost model on only 10 labeled examples and use this model for active inference with
no fine-tuning and PPI. The latter is similar to the former in the sense that it only replaces active with
uniform sampling. Active inference with fine-tuning continues to fine-tune the model with every B = 100
new survey responses, also updating the sampling rule via update (8). The uncertainty measure u (x) is
t
given by Eq. (3), as before. As discussed in Section 6, we also periodically use up the remaining budget
regardless of the computed uncertainty in order to avoid underutilizing the budget (in particular, every
100n/n steps). We fine-tune the model using the training continuation feature of XGBoost.
b
TheintervalwidthsandcoveragearereportedinFigure5. Wefindthatfine-tuningsubstantiallyimproves
inferentialpowerandretainscorrectcoverage. InFigure7weshowthesaveinsamplesizebudgetoveractive
inference with no fine-tuning and inference based on uniform sampling, i.e. PPI. For estimating Biden’s
approval,weobserveagainofaround40%and30%relativetoactiveinferencewithoutfine-tuningandPPI,
respectively. For Trump’s approval, we observe even larger gains around 45% and 35%, respectively.
8.5 Census data analysis with fine-tuning
We similarly evaluate the benefits of sequential fine-tuning in the problem setting from Section 8.2. We
again compare active inference, with and without fine-tuning, and PPI, i.e., active inference with a trivial,
uniform sampling rule. Recall that in Section 8.2 we trained a separate model e to predict the prediction
errors, which we in turn used to form the uncertainty u(x) according to Eq. (11). This time we fine-tune
both the prediction model, f , and the error model, e .
t t
We traininitial XGBoost models f and e on 100 labeledexamples. We use f for PPI andboth f and
1 1 1 1
e for active inference with no fine-tuning. Active inference with fine-tuning continues to fine-tune the two
1
models with every B =1000 new survey responses, also updating the model uncertainty via update (8). We
fine-tune the models using the training continuation feature of XGBoost. We compute u from e based on
t t
Eq. (11). As discussed earlier, we also periodically use up the remaining budget regardless of the computed
uncertainty in order to avoid underutilizing the budget (in particular, every 500n/n steps).
b
15
htdiw
lavretni
htdiw
lavretni
egarevoc
egarevoc1.0
173 active (w/ fine-tuning)
138 0.9 active (no fine-tuning)
uniform
110 0.8
88 0.7
70
0.6
900 1000 1100 1200 3799 4999 6580 8660 11399 3799 5699 7599 9499 11399
regression coefficient nb nb
Figure 6: Census data analysis with fine-tuning. Example intervals in five randomly chosen trials
(left), average confidence interval width (middle), and coverage (right) for the linear regression coefficient
quantifyingtherelationshipbetweenageandincome,controllingforsex,inUSCensusdata. Activeinference
with no fine-tuning and inference with uniformly sampled data use the same model.
Post-election research Census analysis
100 100
75 Biden 75
Trump
50 50
25 25
0 0
4000 4500 5000 8000 9000 10000 11000
nb nb
100 100
75 Biden 75
Trump 50 50
25 25
0 0
3500 4000 4500 5000 7000 8000 9000 10000 11000
nb nb
Figure7: Save in sample size budget due to fine-tuning. Reductioninsamplesizerequiredtoachieve
the same confidence interval width with active inference with fine-tuning and (top) active inference with no
fine-tuningand(bottom)theuniformbaseline(PPI),respectively,intheapplicationsshowninFigure5and
Figure 6.
WeshowtheintervalwidthsandcoverageinFigure6. Weseethatthegainsoffine-tuningaresignificant
and increase as n increases. In Figure 7 we show the save in sample size budget. Fine-tuning saves around
b
32−40% over the baseline with no fine-tuning and around 20−30% over the uniform baseline. Moreover,
the save increases as the sample budget grows because the prediction problem is difficult and the model’s
performance keeps improving even after 10000 training examples.
Acknowledgements
We thank Lihua Lei, Jann Spiess, and Stefan Wager for many insightful comments and pointers to relevant
work. T.Z. was supported by Stanford Data Science through the Fellowship program. E.J.C. was supported
by the Office of Naval Research grant N00014-20-1-2157, the National Science Foundation grant DMS-
2032014, the Simons Foundation under award 814641, and the ARO grant 2003514594.
References
[1] Anastasios N Angelopoulos, Stephen Bates, Clara Fannjiang, Michael I Jordan, and Tijana Zrnic.
Prediction-powered inference. Science, 382(6671):669–674, 2023.
[2] Anastasios N Angelopoulos, Stephen Bates, Clara Fannjiang, Michael I Jordan, and Tijana Zrnic.
Prediction-powered inference: Data sets, 2023. URL https://doi.org/10.5281/zenodo.8397451.
16
revo
evas
tegdub
evas
tegdub
)%(
gninut-enif
on
)%(
mrofinu
revo
htdiw
lavretni
egarevoc[3] Anastasios N Angelopoulos, John C Duchi, and Tijana Zrnic. PPI++: Efficient prediction-powered
inference. arXiv preprint arXiv:2311.01453, 2023.
[4] Jordan T Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep
batch active learning by diverse, uncertain gradient lower bounds. arXiv preprint arXiv:1906.03671,
2019.
[5] David Azriel, Lawrence D Brown, Michael Sklar, Richard Berk, Andreas Buja, and Linda Zhao. Semi-
supervisedlinearregression. Journal of the American Statistical Association,117(540):2238–2251,2022.
[6] Maria-Florina Balcan, Alina Beygelzimer, and John Langford. Agnostic active learning. In Proceedings
of the 23rd international conference on Machine learning, pages 65–72, 2006.
[7] Maria-FlorinaBalcan,AmitDaniely,RutaMehta,RuthUrner,andVijayVVazirani.Learningeconomic
parameters from revealed preferences. In Web and Internet Economics: 10th International Conference,
WINE 2014, Beijing, China, December 14-17, 2014. Proceedings 10, pages 338–353. Springer, 2014.
[8] DebopamBhattacharyaandPascalineDupas. Inferringwelfaremaximizingtreatmentassignmentunder
budget constraints. Journal of Econometrics, 167(1):168–196, 2012.
[9] Isabell Bludau, Sander Willems, Wen-Feng Zeng, Maximilian T Strauss, Fynn M Hansen, Maria C
Tanzer, Ozge Karayel, Brenda A Schulman, and Matthias Mann. The structural context of posttrans-
lational modifications at a proteome-wide scale. PLoS biology, 20(5):e3001636, 2022.
[10] Yash Chandak, Shiv Shankar, Vasilis Syrgkanis, and Emma Brunskill. Adaptive instrument design for
indirect experiments. arXiv preprint arXiv:2312.02438, 2023.
[11] TianqiChenandCarlosGuestrin. Xgboost: Ascalabletreeboostingsystem. InProceedings of the 22nd
acm sigkdd international conference on knowledge discovery and data mining, pages 785–794, 2016.
[12] ChenCheng,HilalAsi,andJohnDuchi. Howmanylabelersdoyouhave? acloserlookatgold-standard
labels. arXiv preprint arXiv:2206.12041, 2022.
[13] Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney
Newey, andJamesRobins. Double/debiasedmachinelearningfortreatmentandstructuralparameters,
2018.
[14] Thomas Cook, Alan Mishler, and Aaditya Ramdas. Semiparametric efficient inference in adaptive
experiments. arXiv preprint arXiv:2311.18274, 2023.
[15] Frances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. Retiring adult: New datasets for fair
machine learning. Advances in neural information processing systems, 34:6478–6490, 2021.
[16] Rick Durrett. Probability: theory and examples, volume 49. Cambridge university press, 2019.
[17] Aryeh Dvoretzky. Asymptotic normality for sums of dependent random variables. In Proceedings of the
Sixth Berkeley Symposium on Mathematical Statistics and Probability, Volume 2: Probability Theory,
volume 6, pages 513–536. University of California Press, 1972.
[18] Feng Gan and Wanfeng Liang. Prediction de-correlated inference. arXiv preprint arXiv:2312.06478,
2023.
[19] Vitor Hadad, David A Hirshberg, Ruohan Zhan, Stefan Wager, and Susan Athey. Confidence intervals
forpolicyevaluationinadaptiveexperiments. Proceedings of the national academy of sciences,118(15):
e2014602118, 2021.
[20] Jinyong Hahn, Keisuke Hirano, and Dean Karlan. Adaptive experimental design using the propensity
score. Journal of Business & Economic Statistics, 29(1):96–108, 2011.
[21] Steve Hanneke et al. Theory of disagreement-based active learning. Foundations and Trends® in
Machine Learning, 7(2-3):131–309, 2014.
17[22] FeifangHuandWilliamFRosenberger. Thetheoryofresponse-adaptiverandomizationinclinicaltrials.
John Wiley & Sons, 2006.
[23] Neal Jean, Marshall Burke, Michael Xie, W Matthew Davis, David B Lobell, and Stefano Ermon.
Combining satellite imagery and machine learning to predict poverty. Science, 353(6301):790–794,
2016.
[24] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,
KathrynTunyasuvunakool,RussBates,AugustinZˇ´ıdek,AnnaPotapenko,etal.Highlyaccurateprotein
structure prediction with alphafold. Nature, 596(7873):583–589, 2021.
[25] Graham Kalton. Introduction to survey sampling. Number 35. Sage Publications, 2020.
[26] MaximilianKasyandAnjaSautmann. Adaptivetreatmentassignmentinexperimentsforpolicychoice.
Econometrica, 89(1):113–132, 2021.
[27] Masahiro Kato, Takuya Ishihara, Junya Honda, and Yusuke Narita. Efficient adaptive experimental
design for average treatment effect estimation. arXiv preprint arXiv:2002.05308, 2020.
[28] MohammadGMKhan,KarunaGReddy,andDineshKRao. Designingstratifiedsamplingineconomic
and business surveys. Journal of applied statistics, 42(10):2080–2099, 2015.
[29] Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in
applied mathematics, 6(1):4–22, 1985.
[30] John A List, Sally Sadoff, and Mathis Wagner. So you want to run an experiment, now what? some
simple rules of thumb for optimal experimental design. Experimental Economics, 14:439–457, 2011.
[31] Jiacheng Miao, Xinran Miao, Yixuan Wu, Jiwei Zhao, and Qiongshi Lu. Assumption-lean and data-
adaptive post-prediction inference. arXiv preprint arXiv:2311.14220, 2023.
[32] KeshavMotwaniandDanielaWitten. Validinferenceafterprediction. arXivpreprintarXiv:2306.13746,
2023.
[33] Dankit K Nassiuma. Survey sampling: Theory and methods, 2001.
[34] Art B. Owen. Monte Carlo theory, methods and examples. https://artowen.su.domains/mc/, 2013.
[35] Pew. American trends panel (ATP) wave 79, 2020. URL https://www.pewresearch.org/science/
dataset/american-trends-panel-wave-79/.
[36] Herbert Robbins. Some aspects of the sequential design of experiments. 1952.
[37] James M Robins and Andrea Rotnitzky. Semiparametric efficiency in multivariate regression models
with missing data. Journal of the American Statistical Association, 90(429):122–129, 1995.
[38] JamesMRobins,AndreaRotnitzky,andLuePingZhao.Estimationofregressioncoefficientswhensome
regressors are not always observed. Journal of the American statistical Association, 89(427):846–866,
1994.
[39] Esther Rolf, Jonathan Proctor, Tamma Carleton, Ian Bolliger, Vaishaal Shankar, Miyabi Ishihara,
Benjamin Recht, and Solomon Hsiang. A generalizable and accessible approach to machine learning
with global satellite imagery. Nature communications, 12(1):4392, 2021.
[40] D Rubin. Multiple imputation for nonresponse in surveys. Wiley Series in Probability and Statistics,
page 1, 1987.
[41] Donald B Rubin. Inference and missing data. Biometrika, 63(3):581–592, 1976.
[42] DonaldBRubin. Multipleimputationafter18+years. Journal of the American statistical Association,
91(434):473–489, 1996.
18[43] Carl-ErikS¨arndal,BengtSwensson,andJanWretman.Modelassistedsurveysampling.SpringerScience
& Business Media, 2003.
[44] Greg Schohn and David Cohn. Less is more: Active learning with support vector machines. In ICML,
volume 2, page 6, 2000.
[45] Burr Settles. Active learning literature survey. Department of Computer Sciences, University of
Wisconsin-Madison, 2009.
[46] Simon Tong and Daphne Koller. Support vector machine active learning with applications to text
classification. Journal of machine learning research, 2(Nov):45–66, 2001.
[47] Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000.
[48] Harit Vishwakarma, Heguang Lin, Frederic Sala, and Ramya Korlakai Vinayak. Promises and pitfalls
of threshold-based auto-labeling. Advances in Neural Information Processing Systems, 36, 2023.
[49] MichaelXie,NealJean,MarshallBurke,DavidLobell,andStefanoErmon. Transferlearningfromdeep
features for remote sensing and poverty mapping. In Proceedings of the AAAI conference on artificial
intelligence, volume 30, 2016.
[50] Matt Zdun. Machine politics: How America casts and counts its votes. Reuters, 2022.
[51] Anru Zhang, Lawrence D Brown, and T Tony Cai. Semi-supervised inference: General theory and
estimation of means. Annals of Statistics, 47(5):2538–2566, 2019.
[52] Jiaqi Zhang, Louis Cammarata, Chandler Squires, Themistoklis P Sapsis, and Caroline Uhler. Active
learning for optimal intervention design in causal models. Nature Machine Intelligence, pages 1–10,
2023.
[53] Kelly Zhang, Lucas Janson, and Susan Murphy. Statistical inference with M-estimators on adaptively
collected data. Advances in neural information processing systems, 34:7460–7471, 2021.
[54] Yuqian Zhang and Jelena Bradic. High-dimensional semi-supervised learning: in search of optimal
inference of the mean. Biometrika, 109(2):387–403, 2022.
[55] Tijana Zrnic and Emmanuel J Cand`es. Cross-prediction-powered inference. arXiv preprint
arXiv:2309.16598, 2023.
19A Proofs
A.1 Proof of Proposition 1
Recall that ξ ∼Bern(π (X )). For any η ∈H, we define
i ηˆ i
ξη =1{π (X )≤π (X )}ξ (1−ξ≤)+1{π (X )>π (X )}(ξ +(1−ξ )ξ>), (13)
i η i ηˆ i i i η i ηˆ i i i i
whereξ≤ ∼Bern(sηˆ(Xi)−sη(Xi))andξ> ∼Bern(sη(Xi)−sηˆ(Xi))aredrawnindependentlyofξ . Thisdefinition
i sηˆ(Xi) i 1−sηˆ(Xi) i
couples
ξη∗
with ξ , while ensuring that
ξη∗
∼Bern(π (X )). Let
i i i η∗ i
θˆη∗
=
1
(cid:88)n (cid:32)
f(X )+(Y −f(X ))
ξ
iη∗ (cid:33)
.
n i i i π (X )
η∗ i
i=1
By the central limit theorem, we know that
√
n(θˆη∗ −θ∗)→d N(0,σ2), (14)
∗
where σ2
=Var(cid:16)
f(X)+(Y −f(X))
ξη∗ (cid:17)
. On the other hand, we have
∗ πη∗(X)
√ √ √
n(θˆηˆ−θ∗)= n(θˆη∗ −θ∗)+ n(θˆηˆ−θˆη∗ ).
√ √
For any ϵ > 0, we have P(| n(θˆηˆ−θˆη∗)| ≥ ϵ) ≤ P(ηˆ̸= η∗) → 0; therefore, n(θˆηˆ−θˆη∗) →p 0. Putting this
√
fact together with Eq. (14), we conclude that n(θˆηˆ−θ∗)→d N(0,σ2) by Slutsky’s theorem.
∗
A.2 Proof of Theorem 1
TheprooffollowsasimilarargumentastheclassicalproofofasymptoticnormalityforM-estimation;see[47,
Thm. 5.23]. A similar proof is also given for the prediction-powered estimator [3], which is closely related
to our active inference estimator. The main difference between our proof and the classical proof is that ηˆis
tuned in a data-adaptive fashion, so the increments in the empirical loss Lπηˆ(θ) are not independent. We
begin by formally stating the required smoothness assumption.
Assumption 1 (Smoothness). The loss ℓ is smooth if:
• ℓ (x,y) is differentiable at θ∗ for all (x,y);
θ
• ℓ is locally Lipschitz around θ∗: there is a neighborhood of θ∗ such that ℓ (x,y) is C(x,y)-Lipschitz
θ θ
and ℓ (x,f(x)) is C(x)-Lipschitz in θ, where E[C(X,Y)2]<∞,E[C(X)2]<∞;
θ
• L(θ)=E[ℓ (X,Y)] and Lf(θ)=E[ℓ (X,f(X))] have Hessians, and H =∇2L(θ∗)≻0.
θ θ θ∗
Usingthesamedefinitionofξ iηasinEq. (13),letLη
θ,i
=ℓ θ(X i,f(X i))+(ℓ θ(X i,Y i)−ℓ θ(X i,f(X i))) πηξ (i Xη i).
We define ∇Lη analogously, replacing the losses with their gradients. Given a function g, let
θ,i
n n
1 (cid:88)(cid:16) (cid:17) 1 (cid:88)
G [g(Lη)]:= √ g(Lη )−E[g(Lη )] ; E [g(Lη)]:= g(Lη ).
n θ n θ,i θ,i n θ n θ,i
i=1 i=1
We similarly use G n[g(∇Lη θ)], E n[g(∇Lη θ)], etc. Notice that E n[Lη θˆ]=Lπηˆ(θ).
By the differentiability and local Lipschitzness of the loss, for any h =O (1) we have
n P
√
G n[ n(Lη θ∗∗ +hn/√ n−Lη θ∗∗ )−h⊤ n∇Lη θ∗∗ ]→p 0.
20By definition, this is equivalent to
√
nE n[Lη θ∗∗ +hn/√ n−Lη θ∗∗ ]=n(L(θ∗+h n/ n)−L(θ∗))+h⊤ nG n[∇Lη θ∗∗ ]+o P(1),
where L(θ)=E[ℓ (X,Y)] is the population loss. A second-order Taylor expansion now implies
θ
nE n[Lη θ∗∗ +hn/√ n−Lη θ∗∗ ]= 1 2h⊤ nH θ∗h n+h⊤ nG n[∇Lη θ∗∗ ]+o P(1).
At the same time, since P(ηˆ̸=η∗)→0, we have
nE n[Lη θˆ ∗+hn/√ n−Lη θˆ ∗]=nE n[Lη θ∗∗ +hn/√ n−Lη θ∗∗ ]+o P(1).
Putting everything together, we have shown
nE n[Lη θˆ ∗+hn/√ n−Lη θˆ ∗]= 1 2h⊤ nH θ∗h n+h⊤ nG n[∇Lη θ∗∗ ]+o P(1).
√
The rest of the proof is standard. We apply the previous display with h = hˆ := n(θˆηˆ−θ∗) (which is
n n
O (1) by the consistency of θˆη∗; see [47, Thm. 5.23]) and h =h˜ :=−H−1G [∇Lη∗ ]:
P n n θ∗ n θ∗
nE [Lηˆ −Lηˆ ]= 1 hˆ⊤H hˆ +hˆ⊤G [∇Lη∗ ]+o (1);
n θˆηˆ θ∗ 2 n θ∗ n n n θ∗ P
nE n[Lη θˆ
∗+h˜
n/√ n−Lη θˆ ∗]= 1 2h˜⊤ nH θ∗h˜ n+h˜⊤ nG n[∇Lη θ∗∗ ]+o P(1).
Bythedefinitionofθˆηˆ,theleft-handsideofthefirstequationissmallerthantheleft-handsideofthesecond
equation. Therefore,thesamemustbetrueoftheright-handsidesoftheequations. Ifwetakethedifference
between the equations and complete the square, we get
1(cid:16)√ (cid:17)⊤ (cid:16)√ (cid:17)
n(θˆηˆ−θ∗)−h˜ H n(θˆηˆ−θ∗)−h˜ +o (1)≤0.
2 n θ∗ n P
√
Since the Hessian H is positive-definite, it must be the case that n(θˆηˆ−θ∗)−h˜ →p 0. By the central
θ∗ n
limit theorem, h˜ =−H−1G [∇Lη∗ ] converges to N(0,Σ ) in distribution, where
n θ∗ n θ∗ ∗
(cid:18) ξη∗ (cid:19)
Σ =H−1Var ∇ℓ (X,f(X))+(∇ℓ (X,Y)−∇ℓ (X,f(X))) H−1.
∗ θ∗ θ∗ θ∗ θ∗ π (X) θ∗
η∗
The final statement thus follows by Slutsky’s theorem.
A.3 Proof of Proposition 2
We prove the result by an application of the martingale central limit theorem (see, for example, Theorem
8.2.4. in [16]).
Let ∆¯ denote the increments ∆ with their mean subtracted out, i.e. ∆¯ = ∆ −θ∗. To apply the
t t t t
theorem, we first need to verify that the increments ∆¯ = ∆ −θ∗ are martingale increments; this follows
t t
because
(cid:20) (cid:21)
ξ
E[∆¯ |F ]=E[∆¯ |f ,π ]=E[f (X )|f ,π ]+E[Y −f (X )|f ,π ]E t |f ,π −θ∗ =0,
t t−1 t t t t t t t t t t t t π (X ) t t
t t
together with the fact that ∆¯ ∈F .
t t
The martingale central limit theorem is now applicable given two regularity conditions. The first is
that 1 (cid:80)n σ2 converges in probability, which holds by assumption. The second condition is the so-called
n t=1 t
Lindeberg condition, stated below.
21Assumption 2. We say that ∆ satisfy the Lindeberg condition if
t
1 (cid:88)n
E[∆¯21{|∆¯
|>ϵ√
n}|F ]→p 0
n t t t−1
t=1
for all ϵ>0, where ∆¯ =∆ −θ∗.
t t
Since this condition holds by assumption, we can apply the central limit theorem to conclude
√ n(θˆπ#»
−
θ∗)= √1 n(cid:80)n t=1∆¯
t
→d N(0,σ ∗2).
A.4 Proof of Theorem 2
WefollowasimilarapproachasintheproofofTheorem1, whichisinturnsimilartotheclassicalargument
forM-estimation[47,Thm.5.23]. Inthiscase,themaindifferencetotheclassicalproofisthattheempirical
#»
loss Lπ(θ) comprises martingale, rather than i.i.d. increments. We explain the differences relative to the
proof of Theorem 1.
We define L = ℓ (X ,f (X ))+(ℓ (X ,Y )−ℓ (X ,f (X ))) ξi , and ∇L is defined analogously.
θ,i θ i i i θ i i θ i i i πi(Xi) θ,i
We again use the notation G [g(L )],E [g(L )],G [g(∇L )], E [g(∇L )], etc.
n θ n θ n θ n θ
√
As in the classical argument, for any h = O (1) we have G [ n(L √ −L )−h⊤∇L ] →p 0.
n P n θ∗+hn/ n θ∗ n θ∗
This can be concluded from the martingale central limit theorem, since the variance of the increments
√
tends to zero. Specifically, define the triangular array L = n(L √ −L )−h⊤∇L , and let
n,i θ∗+hn/ n,i θ∗,i n θ∗,i
V
n,i
= Var(L n,i|F i−1). We have |√1 n(cid:80)n i=1L n,i| ≤ max i(cid:112) V n,i|√1 n(cid:80)n
i=1
√L Vn n,i ,i|. By the martingale central
limit the √orem, √1 n(cid:80)n
i=1
√L Vn n,i
,i
→d N(0,1) and, since max i(cid:112) V
n,i
→p 0, we conclude by Slutsky’s theorem
that G [ n(L √ −L )−h⊤∇L ]→p 0.
n θ∗+hn/ n θ∗ n θ∗
The following steps are the same as in the proof of Theorem 1; we conclude that √ n(θˆπ#» −θ∗)−h˜ →p 0,
n
where h˜ = −H−1G [∇L ]. Finally, we argue that h˜ converges to N(0,Σ ) in distribution. To see this,
n θ∗ n θ∗ n ∗
first note that all one-dimensional projections v⊤h˜ converge to v⊤Z, Z ∼ N(0,Σ ), by the martingale
n ∗
central limit theorem, which is applicable because the Lindeberg condition holds by assumption (see below
for statement) and the variance process V converges to V . Once we have the convergence of all one-
θ∗,n ∗
dimensional projections, convergence of h˜ follows by the Cram´er-Wold theorem.
n
Assumption 3. We say that the increments satisfy the Lindeberg condition if, for all v ∈Sd−1,
1 (cid:88)n
E[(v⊤∇L )21{|v⊤∇L
|>ϵ√
n}|F ]→p 0
n θ∗,t θ∗,t t−1
t=1
for all ϵ>0.
B Experimental details
In all our experiments, we have a labeled dataset of n examples. We treat the solution on the full dataset
as the ground-truth θ∗ for the purpose of evaluating coverage. In each trial, the underlying data points
(X ,Y ) are fixed and the randomness comes from the labeling decisions ξ . In the sequential experiments,
i i i
we additionally randomly permute the data points at the beginning of each trial. The experiments in the
batch setting average the results over 1000 trials and the experiments in the sequential setting average
the results over 100 trials. The Pew dataset is available at [35]; the census dataset is available through
Folktables [15]; the Alphafold dataset is available at [2].
As discussed in Section 7.2, to avoid values of π(x) that are very close to zero we mix the “standard”
samplingrulebasedonthereportedmeasureonuncertaintyu(x)withauniformruleπunif = nb accordingto
n
22a parameter τ ∈(0,1). In post-election survey research, we have training data for the prediction model and
we use the same data to select τ so as to minimize an empirical approximation of the variance Var(θˆπ(τ)),
as in Eq. (12). In the AlphaFold example and both problems with model fine-tuning we set τ = 0.5 for
simplicity. In the census example, the trained predictor of model error e(x) rarely gives very small values,
and so we set τ =0.001.
In each experiment, we vary n over a grid of uniformly spaced values. We take 20 grid values for the
b
batchexperimentsand10gridvaluesforthesequentialexperiments. Theplotsofintervalwidthandcoverage
linearly interpolate between the respective values obtained at the grid points. There linearly interpolated
values are used to produce the plots of budget save: for all values of n from the grid, we look for n′ such
b b
that the (linearly interpolated) width of active inference at sample size n′ matches the interval width of
b
classical (resp. uniform) inference at sample size n . For the leftmost plot in Figures 1-3 and Figures 5-6,
b
weuniformlysamplefivetrialsfora fixedn andshow the intervalsfor allmethodsinthosesamefivetrials.
b
We arbitrarily select n to be the fourth largest value in the grid of budget sizes for all experiments.
b
23