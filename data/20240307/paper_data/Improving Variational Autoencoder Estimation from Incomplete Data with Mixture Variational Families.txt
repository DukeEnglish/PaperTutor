Improving Variational Autoencoder Estimation from
Incomplete Data with Mixture Variational Families
Vaidotas Simkus vaidotas.simkus@ed.ac.uk
Michael U. Gutmann michael.gutmann@ed.ac.uk
School of Informatics
University of Edinburgh
Abstract
Weconsiderthetaskofestimatingvariationalautoencoders(VAEs)whenthetrainingdata
is incomplete. We show that missing data increases the complexity of the model’s posterior
distribution over the latent variables compared to the fully-observed case. The increased
complexity may adversely affect the fit of the model due to a mismatch between the vari-
ational and model posterior distributions. We introduce two strategies based on (i) finite
variational-mixture and (ii) imputation-based variational-mixture distributions to address
the increased posterior complexity. Through a comprehensive evaluation of the proposed
approaches, we show that variational mixtures are effective at improving the accuracy of
VAE estimation from incomplete data.
1 Introduction
Deep latent variable models, as introduced by Kingma & Welling (2013); Rezende et al. (2014); Goodfellow
et al. (2014); Sohl-Dickstein et al. (2015); Krishnan et al. (2016); Dinh et al. (2017), have emerged as a
predominant approach to model real-world data. The models excel in capturing the intricate nature of data
by representing it within a well-structured latent space. However, they typically require large amounts of
fully-observeddataattrainingtime,whilepractitionersinmanydomainsoftenonlyhaveaccesstoincomplete
data sets.
In this paper we focus on the class of variational autoencoders (VAEs, Kingma & Welling, 2013; Rezende
et al., 2014) and investigate the implications of incomplete training data on model estimation. Our contri-
butions are as follows:
• We show that data missingness can add significant complexity to the model posterior of the latent
variables,hencerequiringmoreflexiblevariationalfamiliescomparedtoscenarioswithfully-observed
data (section 3).
• We propose finite variational-mixture approaches to deal with the increased complexity due to
missingness for both standard and importance-weighted ELBOs (section 4.1).
• We further propose an imputation-based variational-mixture approach, which decouples model esti-
mation from data missingness problems, and as a result, improves model estimation when using the
standard ELBO (section 4.2).
• We evaluate the proposed methods for VAE estimation on synthetic and realistic data sets with
missing data (section 6).
The proposed methods achieve better or similar estimation performance compared to existing methods that
donotusevariationalmixtures. Moreover, themixturesareformedbythevariationalfamiliesthatareused
in the fully-observed case, which allows us to seamlessly re-use the inductive biases from the well-studied
scenarioswithfully-observeddata(seee.g.Miaoetal.,2022,fortheimportanceofinductivebiasesinVAEs).
1
4202
raM
5
]GL.sc[
1v96030.3042:viXra2 Background: Standard approach for VAEs estimation from incomplete data
We consider the situation where some part of the training data-points might be missing. We denote the
observed and missing parts of the i-th data-point xi by xi and xi , respectively. This split into observed
obs mis
and missing components corresponds to a missingness pattern mi ∈{0,1}D, which can be different for each
i, and is itself a random variable that follows a typically unknown missingness distribution p∗(mi |xi). We
make the common assumption that the missingness distribution does not depend on the missing variables
xi ,thatis,p∗(mi |xi)=p∗(mi |xi ),whichisknownastheignorablemissingnessormissing-at-random
mis obs
assumption (MAR, e.g. Little & Rubin, 2002, Section 1.3). The MAR assumption allows us to ignore the
missingness pattern mi when fitting a model p (x) of the true distribution p∗(x) from incomplete data.
θ
TheVAEmodelwithparametersθ istypicallyspecifiedusingadecoderdistributionp (x|z),parametrised
θ
using a neural network, and a prior p (z) over the latents z that can either be fixed or learnt. A principled
θ
approachtohandlingincompletetrainingdataisthentomarginalisethemissingvariablesfromthelikelihood
p (x), which yields the marginal likelihood
θ
Z ZZ Z
p (xi )= p (xi ,xi )dxi = p (xi ,xi |z)p (z)dzdxi = p (xi |z)p (z)dz, (1)
θ obs θ obs mis mis θ obs mis θ mis θ obs θ
R
wheretheinnerintegral p (x ,x |z)dx isoftencomputationallytractableinVAEsduetostandard
θ obs mis mis
assumptions, such as the conditional independence of x given z or the use of the Gaussian family for the
decoder p (x|z). However, the marginal likelihood above remains intractable to compute as a consequence
θ
of the integral over the latents z.
Due to the intractable integral, VAEs are typically fitted via a variational evidence lower-bound (ELBO)
" #
p (y |z)p (z)
logp θ(y)≥Eq
ϕ(z|y)
log θ
q (z
|yθ
)
=logp θ(y)−D KL(q ϕ(z |y)||p θ(z |y)), (2)
ϕ
where y refers to xi in the fully-observed case, and to xi in the incomplete-data case, and q (z | y)
obs ϕ
is an (amortised) variational distribution with parameters ϕ that is shared for all data-points in the data
set (Gershman & Goodman, 2014). The amortised distribution is parametrised using a neural network
(the encoder), which takes the data-point y as the input and predicts the distributional parameters of the
variational family. Moreover, when the data is incomplete, i.e. y = xi , sharing of the encoder for any
obs
patternofmissingnessisoftenachievedbyfixingtheinputdimensionalityoftheencodertotwicethesizeof
xandprovidingγ(xi )andmi astheinputs,1 whereγ(·)isafunctionthattakestheincompletedata-point
obs
x andproducesavectoroflengthDwiththemissingdimensionssettozero2 (Nazábaletal.,2020;Mattei
obs
& Frellsen, 2019).
In eq. (2) we show that the training objective for incomplete and fully-observed data has the same form,
and therefore it may seem that fitting VAEs from incomplete data would be similarly difficult to the fully-
observed case. However, aswe willsee next, datamissingnesscan make model estimationmuch harderthan
in the complete data case.
3 Implications of incomplete data for VAE estimation
The decomposition of the ELBO in eq. (2) emphasises that accurate estimation of the VAE model requires
us to accurately approximate the model posterior p (z |x ) with the variational distribution q (z |x ).
θ obs ϕ obs
While it might appear that the marginalisation of the missing variables in eq. (1) comes at no cost since the
ELBO maintains the same form as in the complete case, we here illustrate that his is not the case.
Inthetwoleft-mostcolumnsoffig.1weillustratethemodelposteriorsp (z |·)underfully-observeddatax
θ
andpartially-observeddatax .3 Wediscoverthatthemodelposteriorsp (z |x),whichexhibitedacertain
obs θ
1Alternativeencoderarchitectures,suchas,permutation-invariantnetworks(Maetal.,2019)arealsoused.
2Equivalenttosettingthemissingdimensionstotheempiricalmeanforzero-centereddata.
3Infig.1weuseaVAEwithGaussianvariational,prior,anddecoderdistributionsfittedoncompletedata.
2p(z |x) p(z |x obs) q(z |x) Ep(xmis|xobs)[q(z |x)]
Figure1: Illustrationoftheposteriorcomplexityduetomissingdata. Eachcolourrepresentsadifferentdata-
point xi. First: the model posterior p (z | x) under complete data x. Second: the model posterior p (z |
θ θ
x )underincompletedatax . Third: variationalapproximationq (z |x)ofthecomplete-dataposterior
obs obs ϕ
p θ(z |x). Fourth: an imputation-mixture variational approximation Epθ(xmis|xobs)[q ϕ(z |x obs,x mis)] of the
incomplete posterior p (z |x ).
θ obs
regularity in the complete-data scenario, have become irregular multimodal distributions p (z |x ) when
θ obs
evaluated with incomplete data.4 Hence, accurate estimation of VAEs from incomplete data may require
more flexible variational families than in the fully-observed case: while a Gaussian family may sufficiently
well approximate the model posterior in the fully-observed case of our example, it is no longer sufficiently
flexible in the incomplete data case. We provide a further explanation when this situation may occur in
appendix A. As a result of the mismatch between the model posterior p (z | x ) and the variational
θ obs
distributionq (z |x ),theKLdivergencetermineq.(2)maynotbeminimised,subsequentlyintroducing
ϕ obs
a bias to the fit of the model.
In the two right-most columns of fig. 1 we show the variational distributions q (z |x) under fully-observed
ϕ
data x and approximations of the incomplete-data posteriors Epθ(xmis|xobs)[q ϕ(z | x obs,x mis)], which are
good approximations of p (z | x) and p (z | x ), respectively. The two plots show that if the variational
θ θ obs
familyusedinthefully-observedcasewell-approximatesthemodelposterior, i.e.q (z |x)≈p (z |x), then
ϕ θ
theimputation-mixtureEpθ(xmis|xobs)[q ϕ(z |x obs,x mis)]willalsobeagoodapproximationoftheincomplete-
data posterior p (z | x ). This observation suggests that we can work with the same variational family
θ obs
in both the fully-observed and incomplete data scenarios if we adopt a mixture approach. In the rest of
this paper, we investigate opportunities to improve VAE estimation from incomplete data by constructing
variational mixture approximations of the incomplete-data posterior.
4 Fitting VAEs from incomplete data using mixture variational families
Weproposeworkingwithmixturevariationalfamiliesinordertomitigatetheincreaseinposteriorcomplexity
due to missing data and improve the estimation accuracy of VAEs when the training data are incomplete.
Thisallowsustousefamiliesofdistributionsforthemixturecomponentsthatareknowntoworkwellwhen
the data is fully-observed, and use the mixtures to handle the increased posterior complexity due to data
missingness.
We propose two approaches for constructing variational mixtures. In section 4.1 we specify q (z | x ) as
ϕ obs
a finite-mixture distribution that can be learnt directly using the reparametrisation trick. In section 4.2 we
investigate an imputation-based variational-mixture where we specify q ϕ,ft(z | x obs) ≈ Epθ(xmis|xobs)[q ϕ(z |
x ,x )]. Detailed evaluation of the proposed methods is provided in section 6.
obs mis
4A related phenomenon, called posterior inconsistency, has been recently reported in concurrent work by Sudak & Tschi-
atschek(2023),relatingp θ(z|xobs)andp θ(z|x obs\u),whereuisasubsetoftheobserveddimensions(seesection5).
34.1 Using finite mixture variational distributions to fit VAEs from incomplete data
In section 3 we saw that a good approximation of the incomplete data posterior p (z | x ) would be the
θ obs
imputation-mixture Epθ(xmis|xobs)[q ϕ(z | x obs,x mis)]. However, estimation of p θ(x
mis
| x obs) is generally
intractableforVAEs(Rezendeetal.,2014;Mattei& Frellsen,2018a;Simkus&Gutmann,2023). Hence, we
here consider a more tractable approach and specify the variational distribution q (z | x ) in terms of a
ϕ obs
finite-mixture distribution:
K
X
q (z |x )= q (k |x )qk(z |x ), (3)
ϕ obs ϕ obs ϕ obs
k=1
where q (k | x ) is a categorical distribution over the components k ∈ {1,...,K} and each component
ϕ obs
distribution qk(z | x ) belongs to any reparametrisable distribution family. Both q (k | x ) and qk(z |
ϕ obs ϕ obs ϕ
x ) are amortised using an encoder network, similar to section 2.
obs
The “reparametrisation trick” is typically used in VAEs to efficiently optimise the parameters ϕ of the
variational distribution q (z | x ), which requires that the random variable z can be parametrised as a
ϕ obs
learnable differentiable transformation t(ϵ;x ,ϕ) of another random variable ϵ that follows a distribution
obs
with no learnable parameters. However, reparametrising mixture-families requires extra care: sampling the
mixture q (z | x ) in eq. (3) is typically done via ancestral sampling by first drawing k ∼ q (k | x )
ϕ obs ϕ obs
and then z ∼qk(z |x ), but the sampling of the categorical distribution q (k |x ) is non-differentiable,
ϕ obs ϕ obs
making the direct application of the “reparametrisation trick” generally infeasible.
As a result, we consider two objectives for fitting VAEs with mixture-variational distributions based on the
variational ELBO (Kingma & Welling, 2013; Rezende et al., 2014):
L ELBO(x obs)=Eq ϕ(z|xobs)[logw(z)], and (4)
K
X
L SELBO(x obs)= q ϕ(k |x obs)Eq ϕk(z|xobs)[logw(z)], (5)
k=1
p (x ,z)
where w(z)= θ obs . (6)
q (z |x )
ϕ obs
ThefirstobjectiveL correspondstothestandardELBO,whileL isthestratifiedELBO(Roeder
ELBO SELBO
et al., 2017, Section 4; Morningstar et al., 2021). When working with L , due to the mixture varia-
ELBO
tional family, we will need to optimise ϕ with implicit reparameterisation (Figurnov et al., 2019). Implicit
reparametrisation of mixture distributions requires that it is possible to factorise the component distribu-
tions qk(z |x ) using the chain rule, i.e. qk(z |x )=Q qk(z |z ,x ), and have access to the CDF
ϕ obs ϕ obs d ϕ d <d obs
(or other standardisation function) of each factor qk(z | z ,x ). However, the chain rule requirement
ϕ d <d obs
can be difficult to satisfy for some highly flexible variational distribution families, such as normalising flows
(e.g. Papamakarios et al., 2021), and finding the (conditional) CDF of the factors can also be hard if not
alreadyknowninclosedform. Consequently,L withimplicitreparametrisationmaynotbeusablewith
ELBO
all families of variational distributions as components of the mixture.
ThesecondobjectiveL ,ontheotherhand,samplesthemixturedistributionwithstratifiedsampling,5
SELBO
which avoids the non-differentiability of sampling q (k |x ), and as a result allows us to use any family of
ϕ obs
reparametrisable distributions as the mixture components.
Theimportance-weightedELBO(IWELBO,Burdaetal.,2015)isoftenusedasanalternativetothestandard
ELBOasitcanbemadetighter. Weherealsoconsideranordinaryversion,L ,andastratifiedversion,
IWELBO
L (Shi et al., 2019, Appendix A; Morningstar et al., 2021):
SIWELBO
 
I
1X
LI IWELBO(x obs)=E{zj}I
j=1∼q
ϕ(z|xobs)log
I
w(z j), and (7)
j=1
5Stratifiedsamplingofmixturedistributionstypicallydrawsanequalnumberofsamplesfromeachcomponentandweighs
thesamplesbythecomponentprobabilitiesq ϕ(k|xobs)whenestimatingexpectations. ItiscommonlyusedtoreduceMonte
Carlovariance(Robert&Casella,2004).
4 
K I
X 1X
LI SIWELBO(x obs)=E{{z
jk}I j=1∼q ϕk(z|xobs)}K
k=1log q ϕ(k |x obs)
I
w(z jk),6 (8)
k=1 j=1
whereI isthenumberofimportancesamplesinL andthenumberofsamplesper-mixture-component
IWELBO
in L .
SIWELBO
When the number of mixture-components is K = 1 the lower-bounds above correspond to the MVAE and
MIWAEboundsinMattei&Frellsen(2019)whichareamongthemostpopularboundsforfittingVAEsfrom
incomplete data. However, as K > 1 the proposed bounds can be tighter due to an increased flexibility of
thevariationaldistributionq (z |x )(Morningstaretal.,2021, AppendixA),whichpotentiallymitigates
ϕ obs
the problems caused by the missing data (see section 3). Finally, the importance-weighted bounds in eqs.
(7) and (8) maintain the asymptotic consistency guarantees of Burda et al. (2015) and approaches the true
marginal log-likelihood logp (x ) as K·I →∞, allowing for more accurate estimation of the model with
θ obs
increasing computational budget.
We denote the four methods based on eqs. (4), (5), (7) and (8) by MissVAE, MissSVAE, MissIWAE,
and MissSIWAE respectively.
4.2 Using imputation-mixture distributions to fit VAEs from incomplete data
In section 4.1, we dealt with the inference of the latents z (section 2) and the pitfalls of missing data (sec-
tion3)jointlybylearningafinite-mixturevariationaldistribution. Here,weproposeasecond“decomposed”
approach to deal with the pitfalls of missing data.
Intuitively, if we had an oracle that was able to generate imputations of the missing data based on the
ground truth conditional distribution p∗(x | x ), then the VAE estimation task would reduce to the
mis obs
case of complete-data, that is, the challenges affecting the estimation of the variational distribution q from
ϕ
section 3 would be mitigated. This suggests that an effective strategy would be to decompose the task of
model estimation from incomplete data into two (iterative) tasks: data imputation and model estimation,
akin to the Monte Carlo EM algorithm (Wei & Tanner, 1990; Dempster et al., 1977). However, access to
the oracle p∗(x | x ) is unrealistic and the exact sampling of p (x | x ), as required in EM, is
mis obs θ mis obs
generally intractable. To address this, we resort to (i) approximate but computationally cheap conditional
samplingmethodsforVAEstogenerateimputations(Rezendeetal.,2014;Mattei&Frellsen,2018a;Simkus
& Gutmann, 2023) and (ii) learning objectives for the model p and the variational distribution q that
θ ϕ
mitigatethepitfallscausedbythemissingdata. WecalltheproposedapproachDeMissVAE(decomposed
approach for handling missing data in VAEs).
We construct the variational distribution q (z | x ) for an incomplete data-point x using a
ϕ,ft obs obs
completed-data variational distribution q (z | x ,x ) and an (approximate) imputation distribution
ϕ obs mis
ft(x |x )≈p (x |x ):
mis obs θ mis obs
(cid:2) (cid:3)
q ϕ,ft(z |x obs)=Eft(xmis|xobs) q ϕ(z |x obs,x mis) . (9)
Assuming that the completed-data variational distribution q (z |x ,x ) well-represents the model pos-
ϕ obs mis
terior p (z | x ,x ), and that the imputation distribution ft(x |x ) draws plausible imputations
θ obs mis mis obs
of the missing variables, then q (z | x ) will reasonably represent p (z | x ) (see the two right-most
ϕ,ft obs θ obs
columnsoffig.1). Incontrasttosection4.1wehereuseacontinuous-mixturevariationaldistribution,which
ismoreflexiblethanafinite-mixturedistribution, albeitatanextracomputationalcostduetosamplingthe
(approximate) imputations (see appendix D).
6Inmultimodal-domainVAEliterature,Shietal.(2019)proposedalooserboundrelatedtoLSIWELBO:
" #
K I
LI SIWELBO(xobs)≥L˜I SIWELBO(xobs)=!
X
q ϕ(k|xobs)E{z
jk}I j=1∼q ϕk(z|xobs)
log
I1X
w(z jk) I ==1 LSELBO(xobs),
k=1 j=1
andempiricallyshowedthatitmayalleviatepotentialmixturecollapsetoasubsetofthemixturecomponents. Therefore,the
looserboundmaybeusefulwhenvariationalmixturecollapseisobserved.
5We now derive the DeMissVAE objectives for fitting the generative model p (x) and the completed-data
θ
variational distribution q (z |x ,x ), see appendix C for a more in-depth treatment.
ϕ obs mis
Objective for p (x,z). With the variational distribution in eq. (9), we derive an ELBO on the marginal
θ
log-likelihood, similar to eq. (2), to learn the parameters θ of the generative model:
 
p (x ,z)
logp θ(x obs)≥Eft(xmis|xobs)q ϕ(z|xobs,xmis)log Eft(xmis|xobs)θ h
q
ϕob (zs
|x obs,x
mis)i
(cid:2) (cid:3)
=Eft(xmis|xobs)q ϕ(z|xobs,xmis)[logp θ(x obs,z)]+H q ϕ,ft(z |x obs) . (10)
| {z } | {z }
=! Lθ CVI(xobs;ϕ,θ,ft) Const. w.r.t.θ
This lower-bound can be further decomposed into log-likelihood and KL divergence terms
Lθ (x ;ϕ,θ,ft)+H(cid:2) q (z |x )(cid:3) =logp (x )−D (q (z |x )||p (z |x )), (11)
CVI obs ϕ,ft obs θ obs KL ϕ,ft obs θ obs
which means that if q (z | x ) ≈ p (z | x ) then maximising eq. (10) w.r.t. θ performs approximate
ϕ,ft obs θ obs
maximum-likelihood estimation. Importantly, the missing variables x are marginalised-out, which adds
mis
robustness to the potential sampling errors in ft(x |x ).
mis obs
Objective for q (z | x). We obtain the objective for learning the variational distribution q (z | x) by
ϕ ϕ
marginalising the missing variables x from the complete-data ELBO in eq. (2) and then lower-bounding
mis
the integral using ft(x |x ) (see appendix B):
mis obs
" #
logp θ(x obs)≥Eft(xmis|xobs)q
ϕ(z|xobs,xmis)
log qp ϕθ (( zx o |b xs, ox bsm ,xis, mz is)
)
+H
|
(cid:2) ft(x
m {i zs
|x obs) }(cid:3) . (12)
| {z } Const. w.r.t.ϕ
=! Lϕ LMVB(xobs;ϕ,θ,ft)
This lower-bound can also be decomposed into the log-likelihood term and two KL divergence terms
Lϕ (x ;ϕ,θ,ft)+H(cid:2) ft(x |x )(cid:3) = logp (x )−D (ft(x |x )||p (x |x ))
LMVB obs mis obs θ obs KL mis obs θ mis obs
(cid:2) (cid:3)
−Eft(xmis|xobs) D KL(q ϕ(z |x obs,x mis)||p θ(z |x obs,x mis)) ,
(13)
which means that the bound is maximised w.r.t. ϕ iff q (z | x ,x ) = p (z | x ,x ) for all x .
ϕ obs mis θ obs mis mis
Therefore, using the above objective to fit q corresponds directly to the complete-data case, and hence the
ϕ
issues due to missingness identified in section 3 are mitigated.
If q (z | x ,x ) = p (z | x ,x ) for all x , then maximising either of the bounds in eqs. (10)
ϕ obs mis θ obs mis mis
or (12) w.r.t. the imputation distribution ft(x |x ) would correspond to setting ft(x |x ) =
mis obs mis obs
p (x | x ). However, directly learning an imputation distribution ft(x |x ) ≈ p (x | x )
θ mis obs mis obs θ mis obs
is challenging (Simkus et al., 2023, Section 2.2). This motivates using sampling methods to approximate
the optimal imputation distribution ft(x |x )≈p (x |x ) with samples. We draw samples from
mis obs θ mis obs
ft(x |x ) using (cheap) approximate conditional sampling methods for VAEs to obtain K imputations
mis obs
{xk }K and then use them to approximate the expectations w.r.t. ft(x |x ) in the above objectives.
mis k mis obs
We discuss the implementation of the algorithm in detail in appendix D.
Finally,itisworthnotingthattheLθ andLϕ objectivesineqs.(10)and(12)arebasedonthestandard
CVI LMVB
ELBO. Extensions to the importance-weighted ELBO might improve the method further by increasing the
flexibilityofthevariationalposterior. However,unlikethestandardELBOusedineq.(10)wherethedensity
of the imputation-based variational-mixture q (z | x ) can be dropped, IWELBO requires computing
ϕ,ft obs
the density of the proposal distribution q (z | x ), which is generally intractable. We hence leave this
ϕ,ft obs
direction for future work.
65 Related work
FittingVAEsfromincompletedata. SincetheseminalworksofKingma&Welling(2013)andRezende
et al. (2014), VAEs have been widely used for density estimation from incomplete data and various down-
streamtasks,primarilyduetothecomputationallyefficientmarginalisationofthemodelineq.(1). Vedantam
et al. (2017) and Wu & Goodman (2018) explored the use of product-of-experts variational distributions,
drawing inspiration from findings in the factor analysis case with incomplete data (Williams et al., 2018).
Mattei & Frellsen (2019) used the importance-weighted ELBO (Burda et al., 2015) for training VAEs on
incomplete training data sets. Ma et al. (2019) proposed the use of permutation invariant neural networks
to parametrise the encoder network instead of relying on zero-masking. Nazábal et al. (2020) introduced
hierarchical priors to handle incomplete heterogeneous training data. Simkus et al. (2023) proposed a
general-purpose approach that is applicable to VAEs, not requiring the decoder distribution to be easily
marginalisable. Here, we further develop the understanding of VAEs in the presence missing values in the
training data set, and propose variational-mixtures as a natural approach to improve VAE estimation from
incomplete data, building upon the motivation from imputation-mixtures discussed in section 3.
Variational mixture distributions. Mixture distributions have found widespread application in vari-
ational inference and VAE literature. Roeder et al. (2017) introduced the stratified ELBO corresponding
to eq. (5). In the context of VAEs in multimodal domains, Shi et al. (2019, Appendix A) introduced the
stratified IWELBO corresponding to eq. (8), but opted to use a looser bound instead, as detailed foot-
note6. TheseboundsweresubsequentlyrediscoveredbyMorningstaretal.(2021)andKvimanetal.(2023),
whoinvestigatedtheiruseforVAEestimationinfully-observeddatascenarios. Furthermore,Figurnovetal.
(2019)introducedimplicitreparametrisation,enablinggradientestimationforancestrally-sampledmixtures,
allowing the estimation of variational mixtures using eqs. (4) and (7). Here, we build on the prior work,
assertingthatvariational-mixturesarewell-suitedforhandlingtheposteriorcomplexityincreaseduetomiss-
ing data (see section 3). Moreover, the imputation-mixture distribution used in DeMissVAE is a novel type
of variational mixtures specifically designed for incomplete data scenarios.
Posterior complexity increase due to missing data. Concurrenttothisstudy, Sudak&Tschiatschek
(2023) have recently brought attention to a phenomenon related to the increase in posterior complexity
due to incomplete data, as discussed in section 3. They noted that, for any x and x , where u is a
obs obs\u
subset of the observed dimensions, the model posteriors p (z | x ) and p (z | x ) should exhibit a
θ obs θ obs\u
strong dependency. However, because of the approximations in the variational posterior (see e.g. Cremer
et al., 2018; Zhang et al., 2021), the variational approximations q (z | x ) and q (z | x ) may
ϕ obs ϕ obs\u
not consistently capture this dependency. They refer to the lack of dependency between q (z | x )
ϕ obs
and q (z | x ), compared to p (z | x ) and p (z | x ), as posterior inconsistency. Focused on
ϕ obs\u θ obs θ obs\u
improving downstream task performance, they introduce regularisation into the VAE training objective to
address posterior inconsistency. In contrast to their work, we compare the fully-observed and incomplete-
data posteriors, p (z |x) and p (z |x ), respectively. And, with the goal of improving model estimation
θ θ obs
performance, we propose the use of variational-mixtures to mitigate the posterior complexity gap between
the fully-observed and incomplete-data posteriors.
Marginalised variational bound. In the standard ELBO derivation for incomplete data in eq. (2) the
missing variables are first marginalised (collapsed) from the likelihood, and then a variational ELBO is
established. This approach is sometimes referred to as collapsed variational inference (CVI). In contrast,
in the derivation of the DeMissVAE encoder objective in eq. (12) we swap the order of marginalisation
and variational inference. Specifically, we start with the variational ELBO on completed-data, and then
marginalise the missing variables (see appendix B). This approach bears similarity to the marginalised
variational bound (MVB, or KL-corrected bound) in exponential-conjugate variational inference literature
(King & Lawrence, 2006; Lázaro-Gredilla & Titsias, 2011; Hensman et al., 2012). In these works, MVB
has been preferred over CVI due to improved convergence and guarantees that for appropriately formulated
conjugatemodelsMVBisanalyticallytractableincaseswhereCVIisnot(Hensmanetal.,2012,Section3.3).
WhileMVBremainsintractableintheVAEsettingwithincompletedata,similartohowthestandardELBO
is intractable in fully-observed case, we find the motivation behind MVB and DeMissVAE to be similar.
7Method p objective q objective # of components Mixture sampling
θ ϕ
MVAE† eq. (4) eq. (4) K =1 —
MissVAE eq. (4) eq. (4) K >1 Ancestral
MissSVAE eq. (5) eq. (5) K >1 Stratified
MIWAE† eq. (7) eq. (7) K =1 —
MissIWAE eq. (7) eq. (7) K >1 Ancestral
MissSIWAE eq. (8) eq. (8) K >1 Stratified
DeMissVAE eq. (10) eq. (12) K >1 Conditional VAE
Table1: Summary of the proposed and baseline methods. Thenon-mixturebaselines(†)arebasedonMattei
& Frellsen (2019) and the other methods are proposed in this paper. Moreover, the methods using ancestral
sampling require implicit reparametrisation (Figurnov et al., 2019), whereas the other methods work with
the standard reparametrisation trick.
MVAE† MissVAE MissSVAE MIWAE† MissIWAE MissSIWAE DeMissVAE
2.50
−
2.75
−
3.00
−
3.25
−
3.50
−
3.75
−
4.00
−
4.25
−
Figure 2: Log-likelihood on held out data evaluated by numerically integrating the 2D latent variables. VAEs
were fitted on mixture-of-Gaussians data with 50% missingness. Each model is fitted with a computational
budget of 5/15/25 samples from the variational distribution. The box plots show 1st and 3rd quartiles, the
black lines are the medians, the dashed lines are the means, and the whiskers show the data range over 5
independent runs. MVAE and MIWAE (†) are baseline methods by Mattei & Frellsen (2019). The other
five methods are proposed in this paper.
6 Evaluation
We here evaluate the proposed methods, MissVAE, MissSVAE, MissIWAE, MissSIWAE (section 4.1), and
DeMissVAE (section 4.2), on synthetic and real-world data, and compare them to the popular methods
MVAE and MIWAE that do not use mixture variational distributions (Mattei & Frellsen, 2019). The
methods are summarised in table 1.
6.1 Mixture-of-Gaussians data with a 2D latent VAE
Evaluating log-likelihood on held-out data is generally intractable for VAEs due to the intractable integral
in eq. (1). We hence here choose a VAE with 2D latent space, where numerical integration can be used to
estimate the log-likelihood of the model accurately (see appendix E.1 for more details). We fit the model
on incomplete data drawn from a mixture-of-Gaussians distribution. By introducing missingness in the
mixture-of-Gaussians data we introduce multi-modality in the latent space (see fig. 1), which allows us to
verify the efficacy of mixture-variational distributions when the posteriors are multi-modal due to missing
data.
Results are shown in fig. 2. We first note that the stratified MissSVAE approach performed better than
MissVAE that uses ancestral sampling. The reason for this is likely that stratified sampling reduces Monte
8
doohilekil-goL
5=Z 51=Z 52=Z 5=Z5=K 51=Z51=K 52=Z52=K 1=Z5=K 1=Z51=K 1=Z52=K 5=I 51=I 52=I 5=I5=K 51=I51=K 52=I52=K 1=I5=K 1=I51=K 1=I52=K 5=K 51=K 52=KMVAE MIWAE DeMissVAE
MissVAE MissIWAE
MissSVAE MissSIWAE
GAS POWER HEPMASS MINIBOONE
20
10.0
0
− −15
7.5 −22 20
1 −
5.0 − 24
− 25
2.5 −2 −
26
− 30
0.0 3 −
−
28
−
20% 50% 80% 20% 50% 80% 20% 50% 80% 20% 50% 80%
Missingness Missingness Missingness Missingness
Figure3: Estimate of the test log-likelihood using the IWELBO with I =50000, on four UCI data sets. Each
data set was rendered incomplete by applying uniform missingness of 20/50/80%. The curves show average
performance over 5 independent runs of the algorithms and the intervals show the 90% confidence.
Carlo variance of the gradients w.r.t. ϕ and hence enables a better fit of the variational distribution q (z |
ϕ
x )(seeafurtherinvestigationinappendixF.1.1). Inlinewiththisintuition, theMissVAEresultsexhibit
obs
significantly larger variance than MissSVAE. Similarly, we observe that the stratified MissSIWAE approach
performed better than MissIWAE. Importantly, we see that the use of mixture variational distributions
in MissSVAE and MissSIWAE improve the model fit over the MVAE and MIWAE baselines that do not
use mixtures to deal with the increased posterior complexity due to missingness. Finally, we observe that
DeMissVAE is capable of achieving comparable performance to MIWAE and MissSIWAE, despite using a
looser ELBO bound, which shows that the decomposed approach to handling data missingness can be used
to achieve an improved fit of the model.
In appendix F.1.2, we analyse the model and variational posteriors of the learnt models. We observe that
the mixture approaches better-approximate the incomplete-data posteriors, compared to the approaches
that do not use variational-mixtures. Moreover, we also observe that the structure of the latent space is
better-behaved when fitted using the decomposed approach in DeMissVAE.
6.2 Real-world UCI data sets
Wehereevaluatetheproposedmethodsonreal-worlddatasetsfromtheUCIrepository(Dua&Graff,2017;
Papamakarios et al., 2017). We train a VAE model with ResNet architecture on incomplete data sets with
20/50/80% uniform missingness (see appendix E.2 for more details). We then estimate the log-likelihood on
completetestdatasetusingtheIWELBOboundwithI =50Kimportancesamples.7 Foradditionalmetrics
see appendix F.2.
The results are shown in fig. 3. We first note that, similar to before, the stratified MissSVAE approach
performedbetter than MissVAE which uses ancestral sampling. Importantly, we observe that using mixture
variational distributions in MissSVAE improves the fit of the model over MVAE (with the exception on
the Miniboone data set) that uses non-mixture variational distributions. Furthermore, the gains in model
accuracy typically increase with data missingness, which verifies that MissSVAE performs better because it
handlestheincreasedposteriorcomplexityduetomissingdatabetter(seefig.1). Next, weobservethatthe
performance of MIWAE, MissIWAE, and MissSIWAE is similar, although we can note a small improvement
by using MissIWAE and MissSIWAE in large missingness settings. We observe only a relatively small
difference between the IWAE methods because the use of importance weighted bound already corresponds
tousingamoreflexiblesemi-implicitlydefinedvariationaldistribution(Cremeretal.,2017),whichhereseems
to be sufficient to deal with the complexities arising due to missingness. Finally, we note that DeMissVAE
7AsI →∞IWELBOapproacheslogp (x). Moreover,assuggestedbyMattei&Frellsen(2018b),toimprovetheestimate
θ
onheld-outdatawefine-tunetheencoderoncompletetestdatabeforeestimatingthelog-likelihood.
9
doohilekil-goLMNIST Omniglot
90
− 115
−
92
−
120
−
94
−
125
−
96
−
130
−
98
−
Figure 4: Estimate of the test log-likelihood using the IWELBO with I = 1000, MNIST and Omniglot data
sets. Each image in the data set was missing 2 out of 4 random quadrants. The box plots show 1st and 3rd
quartiles, the black lines are the medians, the dashed lines are the means, and the whiskers show the data
range over 5 independent runs.
results are in-between MissSVAE and MIWAE. This verifies that the decomposed approach can be used to
deal with data missingness and, as a result, can improve the fit of the model. Nonetheless, DeMissVAE is
surpassed by the IWAE methods, which is likely due to using the ELBO in DeMissVAE versus IWELBO in
IWAE methods that can tighten the bound more effectively.
6.3 MNIST and Omniglot data sets
In this section we evaluate the proposed methods on binarised MNIST (Garris et al., 1991) and Omniglot
(Lake et al., 2015) data sets of handwritten characters. We fit a VAE model with a convolutional ResNet
encoderanddecodernetworks(seeappendixE.3formoredetails). Thedataismadeincompletebymasking
2 out of 4 quadrants of an image at random. Similar to the previous section, we estimate the log-likelihood
on a complete test data set using the IWELBO bound with I =1000 importance samples.
On the MNIST data set we see that MVAE ≤ MissVAE < MissSVAE similar to the previous results but
MIWAE<MissSIWAE<MissIWAE.ThissuggeststhatMissIWAE,whichusesancestralsampling,wasable
to tighten the bound more effectively compared to stratified MissSIWAE, and was able to fit the variational
distribution q (z | x ) well despite the potentially larger variance w.r.t. ϕ. Moreover, we also see that
ϕ obs
MVAE<MIWAE<DeMissVAE,whichfurtherverifiesthatthedecomposedapproachisabletohandlethe
data missingness well.
On the Omniglot data we observe that the mixture approaches perform similarly to MVAE and MIWAE,
which do not use mixture variational distributions. This suggests that either the posterior multi-modality is
less prominent in the Omniglot data set or that due to the reverse KL optimisation of the variational distri-
butionallmixturecomponentshavedegeneratedtoasinglemode. Finally,DeMissVAEslightlyoutperforms
MVAE, MissVAE, and MissSVAE, but is surpassed by the importance-weighted approaches.
Interestingly, in this evaluation the stratified approaches (MissSVAE and MissSIWAE) were outperformed
by the approaches using standard ELBO and implicit reparametrisation (MissVAE and MissIWAE). This
suggests that the performance of each approach can be data- and model-dependent and hence both should
be evaluated when possible.
10
doohilekil-goL
EAVM EAVssiM EAVSssiM EAWIM EAWIssiM EAWISssiM EAVssiMeD EAVM EAVssiM EAVSssiM EAWIM EAWIssiM EAWISssiM EAVssiMeDBudget Variational Latent Evaluation rank
families structure*
Method MoG UCI MNIST+Omniglot
MissVAE small limited well-behaved 5 5 5
MissSVAE medium any well-behaved 4 4 4
MissIWAE medium limited potentially irregular 3 2 1
MissSIWAE medium any potentially irregular 1 1 2
DeMissVAE medium/high any well-behaved+ 2 3 3
Table 2: A coarse summary of advantages and disadvantages of the proposed methods. Budget:
small/medium/highdependingonthenumberoflatentsamplesrequiredorwhetherconditionalsamplingof
VAEs is needed. Variational families: which families of distributions can be used as mixture components—
any reparametrisable families, or limited families, as discussed in section 4.1. Latent structure: methods
withpotentialtolearnirregularlatentspacesmayhavedecreaseddownstreamperformanceincertaintasks.
We have found (+) that DeMissVAE is able to achieve the most well-behaved latent structures on the MoG
data in appendix F.1.2. Please note (*) that the learnt latent structure will depend on the chosen model
architecture. Evaluation rank: the rank of the proposed methods in the evaluations in sections 6.1 to 6.3.
7 Discussion
Handlingmissingdataisacrucialtaskinmachinelearningfortheapplicationofmodernstatisticalmethods
in many practical domains characterised by incomplete data. In the context of variational autoencoders
we have shown that incomplete data introduces posterior complexity over the latent variables, compared to
the fully-observed data case. Consequently, accurate model fitting from incomplete data requires the use of
more flexible variational families compared to the complete case. We have then stipulated that variational-
mixtures are a natural approach for handling data missingness. This allows us to work with the same
variational families that are known to work well when the data is fully-observed, enabling the transfer of
useful known inductive biases (Miao et al., 2022) from the fully-observed to the incomplete data scenario.
Subsequently, we have introduced two methodologies grounded in variational mixtures. First, we proposed
using finite variational mixtures with the standard and importance-weighted ELBOs using ancestral and
stratified sampling of the mixtures. Additionally, we have proposed a novel “decomposed” variational-
mixtureapproach,thatusescost-effectiveyetoftencoarseconditionalsamplingmethodsforVAEstogenerate
imputations and ELBO-based objectives that are robust to the sampling errors.
OurevaluationshowsthatusingvariationalmixturescanimprovethefitofVAEswhendealingwithincom-
plete data, surpassing the performance of models without variational mixtures. Moreover, our observations
indicate that, although stratified sampling of the finite mixtures often yields better results compared to
ancestral sampling, the effectiveness of these methods can be data- and model-dependent and hence both
approaches should be evaluated when possible. In our findings, we further note that the decomposed ap-
proach in DeMissVAE outperforms all ELBO-based methods but falls short of surpassing IWELBO-based
methods. Theseresultspointtowardspromisingresearchavenues,suggestingpotentialimprovementsinVAE
model estimation from incomplete data. Future directions include extending the DeMissVAE approach to
incorporateIWELBO-basedobjectivesanddevelopingimprovedcost-effectiveconditionalsamplingmethods
for VAEs.
ThechoicebetweentheproposedmethodsforfittingVAEsfromincompletedatadependsonvariousfactors
such as computational budget, variational families, model accuracy goals, and the specific requirements of
downstream tasks, discussed as follows and summarised in table 2.
Computational and memory budget. ThestandardELBOwithancestralsamplingisthemostsuitable
method for small computational and memory budgets, since the objective can be estimated using
a single latent sample for each data point. On the other hand, methods using stratified sampling
or the importance-weighted ELBO require the use of multiple latent samples for each data-point
and hence may only be used if the memory and compute budget allows. Furthermore, akin to the
11standard ELBO, the DeMissVAE objectives can be estimated using a single latent sample, but the
approach incurs extra cost in sampling the imputations.
Variational families. While the stratified and DeMissVAE approaches can use any reparametrisable dis-
tribution family for the mixture components, the ancestral sampling methods require the use of
implicit reparametrisation(Figurnovetal.,2019)andasaresultmaynotworkwithalldistribution
families (see discussion in section 4.1).
Model accuracy. Stratified sampling of mixtures can improve the model accuracy, compared to ancestral
sampling, by reducing Monte Carlo gradient variance. Additionally, methods using the importance-
weighted ELBO, compared to the standard ELBO, are often able to tighten the bound more ef-
fectively by using multiple importance samples, leading to improved model accuracy. DeMissVAE
performance lies in between the standard ELBO and importance-weighted ELBO approaches. Al-
though the introduced DeMissVAE objectives exhibit robustness to some imputation distribution
error, improved model accuracy can often be achieved by improving the accuracy of imputations by
using a larger budget for the imputation step.
Latent structure. Different downstream tasks may prefer distinct latent structures, for example, condi-
tionalgenerationfromunconditionalVAEsisofteneasierifthelatentspaceiswell-structured(Engel
et al., 2017; Gómez-Bombarelli et al., 2018). To this end, observations in appendix F.1.2 show that
the latent space of DeMissVAE behaves well, and is comparable to a model fitted with complete
data. This characteristic makes it preferable for downstream tasks requiring well-structured latent
spaces. On the other hand, as noted by Burda et al. (2015, Appendix C) and Cremer et al. (2018,
Section 5.4), the use of importance-weighted ELBO to mitigate the increased posterior complex-
ity due to missing data, may make the latent space less regular, compared to a model trained on
fully-observed data set, which potentially decreases the model’s performance on downstream tasks.
Finally, we step back to note that this paper is focused on the class of variational autoencoder models, a
subset of the broader family of deep latent variable models (DLVMs). Much like VAEs, DLVMs usually
aim to efficiently represent the intricate nature of data through a well-structured latent space, implicitly
defined by a learnable generative process. Building on our findings in VAEs, where incomplete data led to
anincreasedcomplexityintheposteriordistributioncomparedtothefully-observedcase,weconjecturethat
a similar effect may occur within the wider family of DLVMs, affecting the fit of the model. We therefore
believethatthereissubstantialscopetoexploretheimplicationsofincompletedatainotherDLVMclasses,
particularly focusing on the effects of marginalisation on latent space representations and the associated
generative processes. Investigating decomposed approaches, similar to DeMissVAE or Monte Carlo EM
(Wei & Tanner, 1990), presents promising avenues for further research in this direction.
References
YuriBurda,RogerGrosse,andRuslanSalakhutdinov. ImportanceWeightedAutoencoders. InInternational
Conference on Learning Representations (ICLR), San Juan, Puerto Rico, September 2015. (Cited on 4,
5, 7, 12, 16, 19, 23)
Chris Cremer, Quaid Morris, and David Duvenaud. Reinterpreting Importance-Weighted Autoencoders. In
ICLR Workshop, February 2017. (Cited on 9, 16, 23, 25)
Chris Cremer, Xuechen Li, and David Duvenaud. Inference Suboptimality in Variational Autoencoders. In
International Conference on Machine Learning (ICML), May 2018. (Cited on 7, 12, 23)
Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. Maximum Likelihood from Incomplete Data Via
the EM Algorithm. Journal of the Royal Statistical Society: Series B (Methodological), 39(1):1–22, 1977.
doi: 10.1111/j.2517-6161.1977.tb01600.x. (Cited on 5)
LaurentDinh,JaschaSohl-Dickstein,andSamyBengio.DensityestimationusingRealNVP.InInternational
Conference on Learning Representations (ICLR), February 2017. (Cited on 1)
12Dheeru Dua and Casey Graff. UCI Machine Learning Repository, 2017. (Cited on 9, 21)
JesseEngel,MatthewHoffman,andAdamRoberts. LatentConstraints: LearningtoGenerateConditionally
fromUnconditionalGenerativeModels. InInternationalConferenceonLearningRepresentations(ICLR),
December 2017. (Cited on 12)
MichaelFigurnov,ShakirMohamed,andAndriyMnih. ImplicitReparameterizationGradients. InAdvances
in Neural Information Processing Systems (NeurIPS), January 2019. (Cited on 4, 7, 8, 12)
Michael D. Garris, R. A. Wilkinson, and Charles L. Wilson. Methods for enhancing neural network hand-
written character recognition. In International Joint Conference on Neural Networks (IJCNN), volume 1,
pp. 695–700, Seattle, WA, USA, 1991. IEEE. ISBN 978-0-7803-0164-1. doi: 10.1109/IJCNN.1991.155265.
(Cited on 10)
Samuel J. Gershman and Noah D. Goodman. Amortized Inference in Probabilistic Reasoning. In Annual
Meeting of the Cognitive Science Society, volume 36, 2014. (Cited on 2)
Rafael Gómez-Bombarelli, Jennifer N. Wei, David Duvenaud, José Miguel Hernández-Lobato, Benjamín
Sánchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D. Hirzel, Ryan P. Adams,
and Alán Aspuru-Guzik. Automatic Chemical Design Using a Data-Driven Continuous Representation of
Molecules. ACS Central Science, 4(2):268–276, February 2018. ISSN 2374-7943. doi: 10.1021/acscentsci.
7b00572. (Cited on 12)
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative Adversarial Networks. In Advances in Neural Information
Processing Systems (NeurIPS), June 2014. (Cited on 1)
James Hensman, Magnus Rattray, and Neil D. Lawrence. Fast Variational Inference in the Conjugate
Exponential Family. In Advances in Neural Information Processing Systems (NeurIPS), December 2012.
(Cited on 7)
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs
Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. In Advances in Neural
Information Processing Systems (NeurIPS), 2017. (Cited on 26)
Niels Bruun Ipsen, Pierre-Alexandre Mattei, and Jes Frellsen. Not-MIWAE: Deep Generative Modelling
with Missing not at Random Data. In International Conference on Learning Representations (ICLR),
June 2020. (Cited on 16)
Nathaniel J. King and Neil D. Lawrence. Fast Variational Inference for Gaussian Process Models Through
KL-Correction. In European Conference on Machine Learning (ECML), 2006. (Cited on 7)
Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In International Conference on
Learning Representations (ICLR), December 2013. (Cited on 1, 4, 7)
Rahul G. Krishnan, Uri Shalit, and David Sontag. Structured Inference Networks for Nonlinear State Space
Models. In AAAI Conference on Artificial Intelligence, December 2016. doi: 10.48550/arXiv.1609.09869.
(Cited on 1)
OskarKviman,RickyMolén,AlexandraHotti,SemihKurt,VíctorElvira,andJensLagergren. Cooperation
in the Latent Space: The Benefits of Adding Mixture Components in Variational Autoencoders. In Inter-
national Conference on Machine Learning (ICML),July2023. doi: 10.48550/arXiv.2209.15514. (Citedon
7)
BrendenM.Lake,RuslanSalakhutdinov,andJoshuaB.Tenenbaum. Human-levelconceptlearningthrough
probabilistic program induction. Science, 2015. doi: 10.1126/science.aab3050. (Cited on 10, 21)
Miguel Lázaro-Gredilla and Michalis K. Titsias. Variational heteroscedastic Gaussian process regression. In
International Conference on Machine Learning (ICML), June 2011. (Cited on 7)
13Roderick J. A. Little and Donald B. Rubin. Statistical Analysis with Missing Data: Second Edition. Wiley-
Interscience, 2002. ISBN 0-471-18386-5. (Cited on 2)
Chao Ma and Cheng Zhang. Identifiable Generative Models for Missing Not at Random Data Imputation.
In Advances in Neural Information Processing Systems (NeurIPS), October 2021. (Cited on 16)
Chao Ma, Sebastian Tschiatschek, Konstantina Palla, José Miguel Hernández-Lobato, Sebastian Nowozin,
and Cheng Zhang. EDDI: Efficient dynamic discovery of high-value information with partial VAE. In
International Conference on Machine Learning (ICML), pp. 7483–7504, 2019. ISBN 9781510886988.
(Cited on 2, 7)
Pierre-AlexandreMatteiandJesFrellsen. LeveragingtheExactLikelihoodofDeepLatentVariableModels.
In Advances in Neural Information Processing Systems (NeurIPS), February 2018a. (Cited on 4, 5, 19,
21)
Pierre-Alexandre Mattei and Jes Frellsen. Refit your Encoder when New Data Comes by. In Workshop on
Bayesian Deep Learning at Neural Information Processing Systems (NeurIPS), pp. 4, Montreal, Canada,
2018b. (Cited on 9)
Pierre-AlexandreMatteiandJesFrellsen.MIWAE:DeepGenerativeModellingandImputationofIncomplete
Data Sets. In International Conference on Machine Learning (ICML), 2019. (Cited on 2, 5, 7, 8, 16)
Xiao-LiMeng. OntheRateofConvergenceoftheECMAlgorithm. The Annals of Statistics,22(1):326–339,
March 1994. ISSN 0090-5364, 2168-8966. doi: 10.1214/aos/1176325371. (Cited on 17)
Ning Miao, Emile Mathieu, N. Siddharth, Yee Whye Teh, and Tom Rainforth. On Incorporating Inductive
Biases into VAEs. In International Conference on Learning Representations (ICLR), February 2022. doi:
10.48550/arXiv.2106.13746. (Cited on 1, 11)
WarrenMorningstar,SharadVikram,CusuhHam,AndrewGallagher,andJoshuaDillon. AutomaticDiffer-
entiation Variational Inference with Mixtures. In International Conference on Artificial Intelligence and
Statistics (AISTATS), pp. 3250–3258. PMLR, March 2021. (Cited on 4, 5, 7)
Alfredo Nazábal, Pablo M. Olmos, Zoubin Ghahramani, and Isabel Valera. Handling Incomplete Heteroge-
neous Data using VAEs. Pattern Recognition, 107, 2020. ISSN 0031-3203. doi: 10.1016/j.patcog.2020.
107501. (Cited on 2, 7)
GeorgePapamakarios,TheoPavlakou,andIainMurray.MaskedAutoregressiveFlowforDensityEstimation.
Advances in Neural Information Processing Systems (NeurIPS), 30, 2017. (Cited on 9, 21)
George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshmi-
narayanan. Normalizing Flows for Probabilistic Modeling and Inference. Journal of Machine Learning
Research, 22(57):1–64, 2021. (Cited on 4)
Tom Rainforth, Adam R. Kosiorek, Tuan Anh Le, Chris J. Maddison, Maximilian Igl, Frank Wood, and
Yee Whye Teh. Tighter Variational Bounds are Not Necessarily Better. In International Conference on
Machine Learning (ICML), March 2019. (Cited on 22)
SashankJ.Reddi,SatyenKale,andSanjivKumar.OntheconvergenceofAdamandbeyond.InInternational
Conference on Learning Representations (ICLR), pp. 1–23, 2018. (Cited on 20, 21)
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic Backpropagation and Approxi-
mate Inference. In International Conference on Machine Learning (ICML), Beijing, China, 2014. (Cited
on 1, 4, 5, 7, 19, 21)
Christian P. Robert and George Casella. Monte Carlo Statistical Methods. Springer, 2004. ISBN 0-387-
21239-6. (Cited on 4)
14Geoffrey Roeder, Yuhuai Wu, and David K. Duvenaud. Sticking the Landing: Simple, Lower-Variance
Gradient Estimators for Variational Inference. In Advances in Neural Information Processing Systems
(NeurIPS), volume 30, 2017. (Cited on 4, 7, 20, 21)
Yuge Shi, N. Siddharth, Brooks Paige, and Philip Torr. Variational Mixture-of-Experts Autoencoders for
Multi-ModalDeepGenerativeModels. InAdvances in Neural Information Processing Systems (NeurIPS),
2019. (Cited on 4, 5, 7)
Vaidotas Simkus and Michael U. Gutmann. Conditional Sampling of Variational Autoencoders via Iterated
ApproximateAncestralSampling. Transactions on Machine Learning Research, August2023. ISSN2835-
8856. (Cited on 4, 5, 18, 19, 21)
Vaidotas Simkus, Benjamin Rhodes, and Michael U. Gutmann. Variational Gibbs Inference for Statistical
Model Estimation from Incomplete Data. Journal of Machine Learning Research, 24(196):1–72, 2023.
ISSN 1533-7928. (Cited on 6, 7, 19)
JaschaSohl-Dickstein,EricA.Weiss,NiruMaheswaranathan,andSuryaGanguli.DeepUnsupervisedLearn-
ing using Nonequilibrium Thermodynamics. In International Conference on Machine Learning (ICML),
November 2015. doi: 10.48550/arXiv.1503.03585. (Cited on 1)
Timur Sudak and Sebastian Tschiatschek. Posterior Consistency for Missing Data in Variational Autoen-
coders. InEuropeanConferenceonMachineLearningandPrinciplesandPracticeofKnowledgeDiscovery
in Databases (ECML-PKDD), October 2023. doi: 10.48550/arXiv.2310.16648. (Cited on 3, 7)
Tijmen Tieleman. Training restricted Boltzmann machines using approximations to the likelihood gradient.
In International Conference on Machine Learning (ICML), pp. 1064–1071, 2008. ISBN 9781605582054.
doi: 10.1145/1390156.1390290. (Cited on 19)
George Tucker, Dieterich Lawson, Shixiang Gu, and Chris J. Maddison. Doubly Reparameterized Gradient
EstimatorsforMonteCarloObjectives. InInternational Conference on Learning Representations (ICLR),
November 2018. (Cited on 20)
RamakrishnaVedantam,IanFischer,JonathanHuang,andKevinP.Murphy. GenerativeModelsofVisually
GroundedImagination. International Conference on Learning Representations (ICLR),May2017. (Cited
on 7)
Greg C. G. Wei and Martin A. Tanner. A Monte Carlo Implementation of the EM Algorithm and the Poor
Man’s Data Augmentation Algorithms. Journal of the American Statistical Association, 85(411):699–704,
September 1990. doi: 10.1080/01621459.1990.10474930. (Cited on 5, 12)
Christopher K. I. Williams, Charlie Nash, and Alfredo Nazábal. Autoencoders and Probabilistic Inference
withMissingData: AnExactSolutionforTheFactorAnalysisCase. arXiv preprint,1801.03851,January
2018. (Cited on 7)
MikeWuandNoahD.Goodman. MultimodalGenerativeModelsforScalableWeakly-SupervisedLearning.
In NeurIPS 2018, February 2018. (Cited on 7)
Laurent Younes. On the convergence of markovian stochastic algorithms with rapidly decreasing ergodicity
rates. Stochastics and Stochastic Reports, 65(3-4):177–228, February1999. ISSN1045-1129. doi: 10.1080/
17442509908834179. (Cited on 19)
Mingtian Zhang, Peter Hayes, and David Barber. Generalization Gap in Amortized Inference. In Workshop
on Bayesian Deep Learning at Neural Information Processing Systems (NeurIPS), pp. 6, 2021. (Cited on
7)
15A Posterior complexity due to missing information
The complexity increase of the model posterior due to missing data, shown in fig. 1, explains why flexible
variational distributions (Burda et al., 2015; Cremer et al., 2017) have been preferred when fitting VAEs
from incomplete data (Mattei & Frellsen, 2019; Ipsen et al., 2020; Ma & Zhang, 2021). We here define the
increase of the posterior complexity via the expected Kullback–Leibler (KL) divergence as follows
(cid:20) (cid:21)
p (z |x ,x )
Ep∗(x)[D KL(p θ(z |x)||p θ(z |x obs))]=Ep∗(x)Epθ(z|x) log θ
p (z
|ob xs m )is =I(z,x
mis
|x obs).8
θ obs
As shown above the expected KL divergence equals the (conditional) mutual information (MI) between the
latents z and the missing variables x .
mis
The mutual information interpretation allows us to reason when a more flexible variational family may
be necessary to accurately estimate VAEs from incomplete data. Specifically, when the MI is small then
the two posterior distributions, p (z | x) and p (z | x ) are similar, in which case a simple variational
θ θ obs
distributionmayworksufficientlywell. Thissituationmightappearwhentheobservedx andunobserved
obs
x variables are highly related and x provides little additional information about z over just x ,
mis mis obs
for example, when random pixels of an image are masked it is “easy” to infer the complete image due to
strongrelationshipbetweenneighbouringpixels. Ontheotherhand,whentheMIishighthenx provides
mis
significantadditionalinformationaboutz overjustx ,inwhichcaseamoreflexiblevariationalfamilymay
obs
be needed, for example, when the pixels of an image are masked in blocks such that it introduces significant
uncertainty about what is missing.
B DeMissVAE: Encoder objective derivation
Wederivetheobjectiveforlearningthevariationaldistributionq (z |x)inDeMissVAEbyfirstmarginalising
ϕ
the missing variables x from the complete-data ELBO in eq. (2):
mis
( " #)
Z Z p (x ,x ,z)
log explogp θ(x obs,x mis)dx mis ≥log exp Eq ϕ(z|xobs,xmis) log q ϕθ (zo |b xs obsm ,xis mis) dx mis
The l.h.s. is the marginal log-likelihood logp (x ), but the integral in r.h.s. is intractable. We then lower-
θ obs
bound the integral in the r.h.s. using the imputation distribution ft(x |x ) and Jensen’s inequality
mis obs
( " #)
Z ft(x |x ) p (x ,x ,z)
logp θ(x obs)≥log ft(xm mi is
s
|xo ob bs s)exp Eq ϕ(z|xobs,xmis) log q ϕθ (zo |b xs obsm ,xis mis) dx mis
" ( " #)#
=logEft(xmis|xobs) exp(cid:0) −logft(x mis |x obs)(cid:1) exp Eq ϕ(z|xobs,xmis) log qp ϕθ (( zx o |b xs, ox bsm ,xis, mz is) )
" ( " #)#
p (x ,x ,z)
=logEft(xmis|xobs) exp Eq ϕ(z|xobs,xmis) log q ϕ(z |x oθ bs,xo mbs is)fm ti (s x
mis
|x obs)
" #
p (x ,x ,z)
≥Eft(xmis|xobs)q
ϕ(z|xobs,xmis)
log
q ϕ(z |x
oθ bs,xo mbs is)fm ti (s
x
mis
|x obs)
" #
=Eft(xmis|xobs)q
ϕ(z|xobs,xmis)
log qp ϕθ (( zx o |b xs, ox bsm ,xis, mz is)
)
+H
|
(cid:2) ft(x
m {i zs
|x obs) }(cid:3) .
| {z } Const. w.r.t.ϕ
=! Lϕ LMVB(xobs;ϕ,θ,ft)
8WherenotationofmissuppressedduetoMARassumption. Incaseofmissing-not-at-random(MNAR)assumptionthere
wouldbeanadditionaldependencyonm.
16C DeMissVAE: Motivating the separation of objectives
The two DeMissVAE objectives Lθ and Lϕ in eqs. (10) and (12) correspond to valid lower-bounds on
CVI LMVB
logp (x ) irrespective of ft(x |x ). Moreover, both of them are tight at ft(x |x ) = p (x |
θ obs mis obs mis obs θ mis
x ) and q (z | x ,x ) = p (z | x ,x ). So, a natural question is why do we prefer Lθ to learn
obs ϕ obs mis θ obs mis CVI
p and Lϕ to learn q ?
θ LMVB ϕ
Why use Lθ in eq. (10) over Lϕ in eq. (12) to learn p (x)? Maximisation of the objective
CVI LMVB θ
Lϕ in iteration t w.r.t. θ would have to compromise between maximising the log-likelihood logp (x )
LMVB θ obs
and keeping the other two KL divergence terms in eq. (13) low. Specifically, the compromise between max-
imising logp (x ) and keeping D (ft(x |x )||p (x |x )) low is equivalent to the compromise
θ obs KL mis obs θ mis obs
in the EM algorithm, which is known to affect the convergence of the model (Meng, 1994). Moreover,
if ft(x |x ) ̸= p (x | x ) then minimising the D (ft(x |x ) || p (x | x )) will fit the
mis obs θ mis obs KL mis obs θ mis obs
modelp (x)tothebiasedsamplesfromft(x |x ). Ontheotherhand,inLθ themissingvariablesx
θ mis obs CVI mis
are marginalised from the model, therefore it avoids the compromise with D (ft(x |x ) || p (x |
KL mis obs θ mis
x )) and the potential bias of the imputation distribution ft(x |x ) affects the model only via the
obs mis obs
latents z ∼q (z |x ), increasing the robustness to sub-optimal imputations.
ϕ,ft obs
Why use Lϕ in eq. (12) over Lθ in eq. (10) to learn q (z | x)? In the case of Lθ , if
LMVB CVI ϕ CVI
ft(x |x )=p (x |x )thentheboundistightenedwhenq (z |x ,x )=p (z |x ,x )for
mis obs θ mis obs ϕ obs mis θ obs mis
allx ,whichisthesameoptimalq ifweusedLϕ . But,thereisalsoatleastonemorepossibleoptimal
mis ϕ LMVB
solution q (z | x ,x ) = p (z | x ), which ignores the imputations and corresponds to the optimal
ϕ obs mis θ obs
solutionofthestandardapproachinsection2,andthusitmeansthattheoptimumis(partially)unidentifiable
and can make optimisation of q using Lθ difficult. Moreover, if ft(x |x )̸=p (x |x ) then in
ϕ CVI mis obs θ mis obs
order to minimise D (q (z |x )||p (z |x )) w.r.t. ϕ the variational distribution q (z |x ,x )
KL ϕ,ft obs θ obs ϕ obs mis
would have to compensate for the inaccuracies of ft(x |x ) by adjusting the probability mass over
mis obs
the latents z, such that q ϕ(z | x obs,x mis) is correct on average, i.e. q ϕ,ft(z | x obs) = Eft(xmis|xobs)[q ϕ(z |
x ,x )] ≈ p (z | x ). These two issues make optimising ϕ via Lθ such that q (z | x ,x ) ≈
obs mis θ obs CVI ϕ obs mis
p (z | x ,x ) difficult. On the other hand, in Lϕ the optimal q is always at q (z | x ,x ) =
θ obs mis LMVB ϕ ϕ obs mis
p (z | x ,x ), irrespective of the imputation distribution ft(x |x ), hence the Lϕ objective
θ obs mis mis obs LMVB
in eq. (12) is well-defined and more robust to inaccuracies of ft(x |x ) for the optimisation of q (z |
mis obs ϕ
x ,x ).
obs mis
In fig. 5 we verify the efficacy of DeMissVAE via a control study on a small VAE model p (x) with 2D
θ
latent space fitted on incomplete samples from a ground truth mixture-of-Gaussians (MoG) distribution
p∗(x). We evaluate fitting the VAE using only Lθ in eq. (10) (CVI-VAE, blue), only Lϕ in eq. (12)
CVI LMVB
(MVB-VAE, yellow), and using the proposed two-objective approach (DeMissVAE, green). In the left-most
figure we evaluate the three methods where we represent the imputation distribution ft(x | x ) =
mis obs
p (x | x ) using rejection sampling, which corresponds to the optimal imputation distribution w.r.t.
θ mis obs
D (ft(x |x )||p (x |x ))=0. We see that the proposed approach (green) dominates over the
KL mis obs θ mis obs
other two control methods (blue and yellow), and importantly that marginalisation of the missing variables
in DeMissVAE (green) improves the model accuracy compared to an EM-type handling of the missing
variables (yellow). Furthermore, in the remaining two figures we investigate the sensitivity of the methods
to the accuracy of imputations in ft(x |x ). In Oracle 1 we start with the ground-truth conditional
mis obs
p∗(x |x )and,alongthex-axisofthefigure,investigatehowthemethodsperformwhentheimputation
mis obs
distribution becomes “wider”: first interpolating from p∗(x | x ) to an independent unconditional
mis obs
distribution Q p∗(x ) and then further towards and independent Gaussian distribution. And in
d∈idx(m) d
Oracle 2 we investigate what happens when the sampler “oversamples” posterior modes: we interpolate
the imputation distribution from p∗(x | x ) to 1 PCp∗(x | x ,c), where c is the component of
mis obs C c mis obs
the mixture distribution with a total of C components. As we see in the figure, the proposed DeMissVAE
approach (green) performs similar or better than the MVB-VAE (yellow) and CVI-VAE (blue) control
methods,withanexceptionwhentheft(x |x )areextremelyinaccurate(lasttwopointsonthemiddle
mis obs
17CVI-VAE MVB-VAE DeMissVAE
p (x x ) Oracle1 Oracle2
θ mis| obs
3.0
− −4 −3
3.5
−
6 4
− −
4.0
−
4.5
−8 −5
−
0.0 0.2 0.4 0.6 0.8 0.0 0.1 0.2 0.3 0.4 0.5 0.6
JSD(ft(x x ) p (x x )) JSD(ft(x x ) p (x x ))
mis| obs || ∗ mis| obs mis| obs || ∗ mis| obs
Figure 5: A control study on a VAE model with 2D latent space (see additional details in appendix E.1),
examining the sensitivity of the proposed method (DeMissVAE, green) and two control methods (blue and
yellow) to the accuracy of the imputation distribution ft(x |x ). Left: ft(x |x )=p (x |x )
mis obs mis obs θ mis obs
represented using rejection sampling. Center: an oracle imputation function that gets progressively “wider”
from left-to-right of the figure. Right: an oracle imputation distribution that towards the right of the figure
more significantly oversamples low-probability posterior modes. The log-likelihood is computed on a held-
out test data set by numerically integrating the 2D latent space of the VAE. The horizontal axis on the
two right-most figures shows the Jensen–Shannon divergence between the imputation distribution and the
ground-truth conditional p∗(x |x ).
mis obs
CVI-VAE MVB-VAE DeMissVAE
2.6
−
2.8
−
3.0
−
3.2
−
3.4
−
3.6
−
3.8
−
4.0
−
Figure 6: A control study on a VAE model with 2D latent space (see additional details in appendix E.1),
investigating the importance of the two-objective approach in DeMissVAE (green) and two control methods
(blue and yellow). InCVI-VAE(blue)wefitboththeencoderanddecoderusingeq.(10),andinMVB-VAE
(yellow) we fit both the encoder and decoder using eq. (12). The log-likelihood is computed on a held-out
test data set by numerically integrating the 2D latent space of the VAE.
figure) which is expected since q (z | x ) in eq. (9) can be arbitrarily far from p (z | x ) when
ϕ,ft obs θ obs
q (z |x ,x )=p (z |x ,x ) but ft(x |x )̸≈p (x |x ).
ϕ obs mis θ obs mis mis obs θ mis obs
Finally, in fig. 6 we investigate what happens if we used only Lθ in eq. (10) or Lϕ in eq. (12) to fit
CVI LMVB
the VAE model, in contrast to the two separate objectives for encoder and decoder in DeMissVAE. We use
the LAIR sampling method (Simkus & Gutmann, 2023) as detailed in appendix D to obtain approximate
samples from ft(x |x )(x | x ) ≈ p (x | x ). And, we observe that DeMissVAE achieves a
mis obs mis obs θ mis obs
better fit of the model, in line with our motivation in this section.
18
doohilekil-goL
doohilekil-goL
5=K 51=K 52=K 5=K 51=K 52=K 5=K 51=K 52=KAlgorithm 1 Shared computation of the DeMissVAE learning objectives
Input: parameters θ and ϕ, number of latent samples L, completed data-point (xi ,xik )
obs mis
1: ψik ←Encoder(xi ,xik ;ϕ) ▷ Compute parameters of the variational distribution
obs mis
2: z 1,...,z L ∼q(z |xi obs,xi mk is;ψik) ▷ Sample latents z
3: η l ←Decoder(z l;θ) for ∀l∈[1,L] ▷ Compute parameters of the generative distribution
4: def Lθ CVI(z 1,...,z L,η 1,...,η L): ▷ Procedure for estimating eq. (10)
5: return L1 PL l=1logp(xi obs,z l;η l)
return Lθ (z ,...,z ,η ,...,η ),Lϕ (ψik,z ,...,z ,η ,...,η )
CVI 1 L 1 L LMVB 1 L 1 L
6: def Lϕ LMVB(ψik,z 1,...,z L,η 1,...,η L): ▷ Procedure for estimating eq. (12)
7: return L1 PL l=1logp(xi obs,xi mk is,z l;η l)−logq(z l |xi obs,xi mk is;ψik)
D DeMissVAE: Implementing the training procedure
DeMissVAErequires optimising two objectives Lθ and Lϕ in eqs.(10) and (12) and drawing(approx-
CVI LMVB
imate) samples to represent ft(x |x ) ≈ p (x | x ). Our aim is to implement this efficiently to
mis obs θ mis obs
minimise redundant computation.
The algorithm starts with a randomly-initialised target VAE model p (x,z), an amortised variational dis-
θ
tribution q (z | x), and an incomplete data set D = {xi } . And then, to represent the imputation
ϕ obs i
distribution f0(x |x ), K imputations {xik }K are generated for each xi ∈ D using some simple
mis obs mis k=1 obs
imputation function such as sampling the marginal empirical distributions of the missing variables. The
algorithm then iterates between the following two steps:
1. Imputation. Update the imputation distribution ft(x |x ) using cheap approximate sam-
mis obs
plingmethodssuchaspseudo-Gibbs(Rezendeetal.,2014),Metropolis-within-Gibbs(MWG,Mattei
& Frellsen, 2018a), or latent-adaptive importance resampling (LAIR, Simkus & Gutmann, 2023).
Moreover, since the model and the variational distributions are initialised randomly, we skip the
imputation step during the first epoch over the data.
2. Parameter update. Update the parameters using stochastic gradient ascent on Lθ and Lϕ
CVI LMVB
in eqs. (10) and (12) with the imputations from ft(x |x ).
mis obs
Efficient parameter update. While the two objectives for p and q in eqs. (10) and (12) are different,
θ ϕ
a major part of the computation can be shared, as shown in algorithm 1. As usual, the objectives are
approximatedusingMonteCarloaveragingandrequireonlyoneevaluationofthegenerativemodel,including
theencoder,decoder,andprior,foreachcompleteddata-point(xi ,xik ). Therefore,onlybackpropagation
obs mis
needs to be performed separately and the overall per-iteration computational cost of optimising the two
objectivesisabout1.67timesthecostofafully-observedVAEoptimisation(insteadof2timesifimplemented
naïvely).9
Efficient imputation. To make the imputation step efficient, the imputation distribution ft(x |x )
mis obs
is “persitent” between iterations, that is, the imputation distribution from the previous iteration
ft−1(x |x ) is used to initialise the iterative approximate VAE sampler at iteration t.10 Moreover,
mis obs
an iteration of a pseudo-Gibbs, MWG, or LAIR samplers uses the same quantities as the objectives Lθ
CVI
and Lϕ in eqs. (10) and (12), and hence the cost of one iteration of the sampler in the imputation step
LMVB
can be shared with the cost of computation of the learning objectives. However, it is important to note that
the accuracy of imputations affects the accuracy of the estimated model, and hence better estimation can
be achieved by increasing the computational budget for imputation.
9Thecostofbackpropagationisabout2timesthecostofaforwardpass(Burdaetal.,2015).
10Persistentsamplershave beenused inthepasttoincreaseefficiencyofmaximum-likelihoodestimation methods(Younes,
1999;Tieleman,2008;Simkusetal.,2023).
19E Experiment details
In this appendix we provide additional details on the experiments.
E.1 Mixture-of-Gaussians data with a 2D latent VAE
We generated a random 5D mixture-of-Gaussians model with 15 components by sampling the mixture
covariance matrices from the inverse Wishart distribution W−1(ν = D,Ψ = I), means from the Gaussian
distribution N(µ = 0,σ = 3) and the component probabilities from Dirichlet distribution Dir(α = 1)
(uniform). The model was then standardised to have a zero mean and a standard deviation of one. The
pairwise marginal densities of the generated distribution is visualised in fig. 7 showing a highly-complex
and multimodal distribution, and the generated parameters and data used in this paper are available in the
shared code repository. We simulated a 20K sample data set used to fit the VAEs.
dim=1 dim=2 dim=3 dim=4
Figure 7: The pairwise marginals of the ground-truth Mixture-of-Gaussians distribution.
We then fitted a VAE model with 2-dimensional latent space using diagonal Gaussian encoder and decoder
distributions, and a fixed standard Normal prior. For the decoder and encoder networks we used fully-
connected residual neural networks with 3 residual blocks, 200 hidden dimensions, and ReLU activations.
To optimise the model parameters we have used AMSGrad optimiser (Reddi et al., 2018) with a learning
rate of 10−3 for a total of 500 epochs.
The hyperameters are listed in table 3, note that the total number of samples was the same for all methods
(i.e.5/15/25). Moreover,wehaveused“sticking-the-landing”(STL)gradients(Roederetal.,2017)toreduce
gradient variance for all methods.11
11Wehavealsoevaluatedthedoubly-reparametrisedgradients(DReG,Tuckeretal.,2018)forIWAEmethodsbutfoundSTL
toperformsimilarorbetter.
20
0=mid
1=mid
2=mid
3=midMethod Z K I Mixture sampling
MVAE 5/15/25 1 — —
MissVAE 5/15/25 5/15/25 — Ancestral
MissSVAE 1 5/15/25 — Stratified
MIWAE 1 1 5/15/25 —
MissIWAE 1 5/15/25 5/15/25 Ancestral
MissSIWAE 1 5/15/25 1 Stratified
LAIR (1 iteration, R=0)
DeMissVAE 1 5/15/25 —
(Simkus & Gutmann, 2023)
Table 3: Method hyperparameters on MoG data.
E.2 UCI data sets
We fit the VAEs on four data sets from the UCI repository (Dua & Graff, 2017) with the preprocessing
of (Papamakarios et al., 2017). The VAE model uses diagonal Gaussian encoder and decoder distributions
regularisedsuchthatthestandarddeviation≥10−5 (Mattei&Frellsen,2018a),andafixedstandardNormal
prior. The latent space is 16-dimensional, except for the MINIBOONE where 32 dimensions were used.
The encoder and decoder networks used fully-connected residual neural networks with 2 residual blocks
(except for on the MINIBOONE dataset where 5 blocks were used in the encoder) with 256 hidden dimen-
sionality, and ReLU activations. A dropout of 0.5 was used on the MINIBOONE dataset. The parameters
wereoptimisedusingAMSGradoptimiser(Reddietal.,2018)withalearningrateof10−3andcosinelearning
rate schedule for a total of 200K iterations (or 22K iterations on MINIBOONE). As before, STL gradients
(Roeder et al., 2017) were used to reduce the variance for all methods. DeMissVAE used the LAIR sampler
(Simkus & Gutmann, 2023) with K = 5 R = 1 and 10 iterations. Moreover we have used gradient norm
clipping to stabilise DeMissVAE with the maximum norm set to 1 (except for POWER dataset where we
set it to 0.5).
E.3 MNIST and Omniglot data sets
WefitaVAEonstaticallybinarisedMNISTandOmniglotdatasets(Lakeetal.,2015)downsampledto28x28
pixels. The VAE uses diagonal Gaussian decoder distributions regularised such that the standard deviation
≥ 10−5 (Mattei & Frellsen, 2018a), a fixed standard Normal prior, and a Bernoulli decoder distribution.
The latent space is 50-dimensional.
For both MNIST and Omniglot we have used convolutional ResNet neural networks for the encoder and
decoder with 4 residual blocks, ReLU activations, and dropout probability of 0.3. For MNIST, the encoder
theresidualblockhiddendimensionalitieswere32, 64, 128, 256, andforthedecodertheywere128,64,32,32.
For Omniglot, the encoder the residual block hidden dimensionalities were 64, 128, 256, 512, and for the
decoder they were 256,128,64,64. We used AMSGrad optimiser (Reddi et al., 2018) with 10−4 learning rate,
cosine learning rate schedule, and STL gradients (Roeder et al., 2017) for 500 epochs for MNIST and 200
epochs for Omniglot.
For MVAE, we use 5 latent samples and for MIWAE we use 5 importance samples. For MissVAE we use
K =5 mixture componentsand sample 5 latent samples. For MissSVAE we use K =5 mixture components
andsample1samplefromeachcomponent,foratotalof5samples. ForMissIWAEweuseK =5components
andsample5importancesamples. forMissSIWAEweuseK =5componentsandsample1samplefromeach
component. ForDeMissVAEweuseK =5imputationsandupdatethemusingasinglestepofpseudo-Gibbs
(Rezende et al., 2014).
21F Additional figures
In this appendix we provide additional figures for the experiments in this paper.
F.1 Mixture-of-Gaussians data with a 2D latent VAE
In this section we show additional analysis on the mixture-of-Gaussians data, supplementing the results in
section 6.1.
F.1.1 Analysis of gradient variance with ancestral and stratified sampling
Insection6.1weobservedthatthemodelestimationperformancecandependonwhetherancestralsampling
(with implicit reparametrisation) or stratified sampling is used to approximate the expectations in eqs. (4),
(5), (7) and (8), corresponding to MissVAE/MissIWAE and MissSVAE/MissSIWAE, respectively.
We analyse the signal-to-noise ratio (SNR) of the gradients w.r.t. ϕ and θ for the two approaches, which is
defined as follows (Rainforth et al., 2019)
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)E[∆(ϕ)](cid:12) (cid:12)E[∆(θ)](cid:12)
SNR(ϕ)=(cid:12) (cid:12), and SNR(θ)=(cid:12) (cid:12),
(cid:12)σ[∆(ϕ)](cid:12) (cid:12)σ[∆(θ)](cid:12)
where ∆(·) denotes the gradient estimate, and σ[·] is the standard deviation of a random variable. We
estimate the SNR by computing the expectation and standard deviation over the entire training epoch.
The SNR for ϕ and θ is plotted in fig. 8. We observe that the stratified approaches (MissSVAE and
MissSIWAE) generally have higher SNR. This is possibly the reason why MissSVAE and MissSIWAE have
achieved better model accuracy than the ancestral approaches (MissVAE and MissIWAE) in section 6.1.
MissVAE MissSVAE MissIWAE MissSIWAE
φ θ
12000 12000
10000
10000
8000
8000
6000
6000
4000
4000
2000
2000 0
0 100 200 300 400 500 0 100 200 300 400 500
Trainingepoch Trainingepoch
Figure 8: Signal-to-noise ratio (SNR, higher is better) of the gradients w.r.t. encoder parameters ϕ (left)
and decoder parameters θ (right). For all methods we used a budget of 5 samples from the variational
distribution (see appendix E.1 for more details). We show the median SNR over 5 independent runs and a
90% confidence interval.
F.1.2 Analysis of the model posteriors
In figs. 9 to 11 we visualise the model posteriors with complete and incomplete data, p (z | x) and p (z |
θ θ
x ),respectively,andthevariationaldistributionq (z |·)thatwasusedtofitthemodelviathevariational
obs ϕ
ELBO.Foreachmethodwehaveusedabudgetof25samplesfromthevariationaldistributionduringtraining
(additional details are in appendix E.1). Each figure shows the posteriors for 5 training data-points using
distinct colours.
Figure 9 shows MVAE, MissVAE, and MissSVAE model posteriors p (z | x) and p (z | x ), as well as
θ θ obs
the variational distribution q (z | x ), which approximates the incomplete-data posterior. As motivated
ϕ obs
22
RNSin section 3 we observe that the Gaussian posterior in MVAE (first row) is not sufficiently flexible to ap-
proximate the complex incomplete-data posteriors p (z |x ). On the other hand, the mixture-variational
θ obs
approaches,MissVAE(secondrow)andMissSVAE(thirdrow),areabletowell-approximatetheincomplete-
data posteriors.
Figure 10 shows MIWAE, MissIWAE, and MissSIWAE model posteriors p (z |x) and p (z |x ), as well
θ θ obs
as the variational proposal q (z |x ) and the importance-weighted semi-implicit distribution qIW (z |
ϕ obs ϕ,θ,I=25
x ) that arises from sampling the variational proposal q (z | x ) and re-sampling using importance
obs ϕ obs
weights w(z) = p (x ,z)/q (z | x ) (Cremer et al., 2017). Similar to the MVAE case above, the
θ obs ϕ obs
variationalproposalq (z |x )inMIWAE(firstrow)isquitefarfromthemodelposteriorp (z |x ),but
ϕ obs θ obs
theimportance-weightedboundineq.(7)isabletore-weighthesamplestosufficiently-wellmatchthemodel
posterior, as shown in the fourth column. However, as efficiency of importance sampling depends on the
discrepancy between the proposal and the target distributions, we can expect that more flexible variational
distributions may improve the performance of the importance-weighted ELBO methods. Importantly, we
show that the variational-mixture approaches, MissIWAE (second row) and MissSIWAE (third row), are
able to adapt the variational proposals to the incomplete-data posteriors well, and as a result achieve better
efficiency than MIWAE.
Figure 11 shows DeMissVAE model posteriors p (z | x) and p (z | x ), the variational distribution
θ θ obs
q (z | x), which approximates the complete-data posterior, and the imputation-mixture q (z | x )
ϕ ϕ,ft obs
approximatedusingthe25imputationsinft(x |x )attheendoftraining. Weobservesimilarbehaviour
mis obs
tofig.1,wherethecompletedataposteriorsp (z |x)areclosetoGaussianbuttheincomplete-dataposteriors
θ
p (z | x ) are irregular. As we show in section 6.1, DeMissVAE is capable of fitting the model well by
θ obs
learning the completed-data variational distribution q (z | x) (third column) and using the imputation-
ϕ
mixtureineq.(9)toapproximatetheincompletedataposteriorp (z |x ). Moreover, weobservethatthe
θ obs
imputation-mixture q (z | x ) (fourth column) captures only one of the modes of the model posterior
ϕ,ft obs
p (z | x ). This is a result of using a small imputation sampling budget, that is, using only a single
θ obs
iteration of LAIR to update the imputations (see more details in appendix D), and hence better accuracy
can be achieved by trading-off some computational cost to obtain better imputations that would ensure a
betterrepresentationoftheimputationdistribution. Nonetheless,asobservedinfig.2,DeMissVAEachieves
good model accuracy despite potentially sub-optimal imputations, further signifying the importance of the
two learning objectives for the encoder and decoder distributions in section 4.2 and appendix C.
Interestingly, by comparing the complete-data posteriors p (z | x) (first column) in figs. 9 to 11, we ob-
θ
serve that they are slightly more irregular than in the complete case in fig. 1, except for DeMissVAE whose
posteriors are nearly Gaussian. The irregularity is stronger for the importance-weighted ELBO-based meth-
ods in fig. 10. This is in line with the observation by Burda et al. (2015, Appendix C) and Cremer et al.
(2018, Section 5.4) that VAEs trained with more flexible variational distributions tend to learn a more
complex model posterior. This means that using the importance-weighted bounds, and to a lesser extent
the finite variational-mixture approaches from section 4.1, to fit VAEs on incomplete data may result in
worse-structured latent spaces, compared to models fitted on complete data. On the other hand, we observe
that DeMissVAE learns a better-structured latent space, with the posterior p (z | x) close to a Gaussian,
θ
that is comparable to the complete case. This suggests that the decomposed approach in DeMissVAE may
be important in cases where the latent space needs to be regular, at the additional cost of obtaining missing
data imputations (see appendix D).
23p(z x) p(z x ) q(z x )
| | obs | obs
Figure 9: Posterior distributions of MVAE, MissVAE, and MissSVAE. First column: the model posterior
p (z | x) under complete data x. Second column: the model posterior p (z | x ) under incomplete data
θ θ obs
x . Thirdcolumn: variationalapproximationq (z |x )oftheincompleteposteriorp (z |x )obtained
obs ϕ obs θ obs
at the end of training.
24
EAVM
EAVssiM
EAVSssiMp(z x) p(z x ) q(z x ) qIW (z x )
| | obs | obs I=25 | obs
2
1
0
−1
−2
−3
2
1
0
−1
−2
−3
2
1
0
−1
−2
−3
−3 −2 −1 0 1 2 −3 −2 −1 0 1 2 −3 −2 −1 0 1 2 −3 −2 −1 0 1 2
Figure 10: Posterior distributions of MIWAE, MissIWAE, and MissSIWAE. First column: the model pos-
terior p (z |x) under complete data x. Second column: the model posterior p (z |x ) under incomplete
θ θ obs
data x . Third column: variational proposal q (z | x ) for an incomplete data-point x obtained at
obs ϕ obs obs
the end of training. Fourth column: importance-weighted variational distribution q IW (z | x ) for an
ϕI=25 obs
incomplete data-point x obtained after re-weighting samples from q (z |x ) (Cremer et al., 2017).
obs ϕ obs
p(z |x) p(z |x obs) q(z |x) q ft(z |x obs)
Figure 11: Posterior distributions of DeMissVAE. First: the model posterior p (z |x) under complete data
θ
x. Second: the model posterior p (z |x ) under incomplete data x . Third: variational approximation
θ obs obs
q (z | x) of the complete-data posterior p (z | x) obtained at the end of training. Fourth: the variational
ϕ θ
imputation-mixture distribution in eq. (9) using the imputation distribution ft(x |x ) obtained at the
mis obs
end of training, approximated using a Monte Carlo average with 25 imputations.
25
EAWIM
EAWIssiM
EAWISssiM
EAVssiMeDF.2 UCI data sets
In fig. 12 we plot the Fréchet inception distance (FID, Heusel et al., 2017) versus training iteration on the
UCI datasets. The results closely mimic the log-likelihood results in section 6.2. Importantly, we observe
that using mixture variational distributions becomes more important as the missingness fraction increases,
causing the posterior distributions to be more complex.
MVAE MIWAE DeMissVAE
MissVAE MissIWAE
MissSVAE MissSIWAE
Missingness
20% 50% 80%
103 103 103
102 102
102
101
101
100 101
100
10−1 100
11 00 −− 21 11 00 −− 32 10−1
10−3 10−4 10−2
10−4
0 50000 100000 150000 200000
10−5
0 50000 100000 150000 200000
10−3
0 50000 100000 150000 200000
108 108 108
107 107 107
111 000 456 111 000 456 11 00 56
103 103 104
102 102 103
101 101 102
100 100 101
10−1 10−1 100
11111 00000 −−−−− 65432 11111 00000 −−−−− 65432 1111 0000 −−−− 4321
10−7
0 50000 100000 150000 200000
10−7
0 50000 100000 150000 200000
10−5
0 50000 100000 150000 200000
101 101
100
101
100
10−1
10−1 100
10−2
10−3
10−2
10−1
10−4 10−3
10−5
0 50000 100000 150000 200000
10−4
0 50000 100000 150000 200000
10−2
0 50000 100000 150000 200000
102
102 102
101
101 101
100
100 100
10−1
10−2
10−1 10−1
10−3
0 5000 10000 15000 20000
10−2
0 5000 10000 15000 20000
10−2
0 5000 10000 15000 20000
Iteration Iteration Iteration
Figure12: FID (lower is better) between the model and the complete test data versus training iterations. The
FID is computed using features of the last encoder layer of an independent VAE model trained on complete
data. Lines show the median of 5 independent runs and the intervals show 90% confidence.
26
SAG
REWOP
SSAMPEH
ENOOBINIM