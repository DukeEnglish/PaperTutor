FAR: Flexible, Accurate and Robust 6DoF Relative Camera Pose Estimation
ChrisRockwell1 NileshKulkarni1 LinyiJin1
JeongJoonPark1 JustinJohnson1 DavidF.Fouhey2
UniversityofMichigan1 NewYorkUniversity2
Abstract Rot. Error vs. Rot. Mag. Tr. Error vs. Rot. Mag.
80 Corr. + Solver + Scale
Estimating relative camera poses between images has Learning-Based 3
60 FAR
been a central problem in computer vision. Methods that
2 find correspondences and solve for the fundamental ma- 40
trix offer high precision in most cases. Conversely, meth- 20 1
odspredictingposedirectlyusingneuralnetworksaremore
0 0
0 50 100 150 0 50 100 150 robust to limited overlap and can infer absolute transla- Rotation Magnitude (°)
tion scale, but at the expense of reduced precision. We
show how to combine the best of both methods; our ap- Figure 1. Precise and Robust 6DoF Pose Estimation. Cor-
proachyieldsresultsthatarebothpreciseandrobust,while respondence Estimation + Solver methods (here LoFTR [70],
also accurately inferring translation scales. At the heart RANSAC [22]) produce precise outputs for moderate rotations,
but are not robust to large rotations (left), and cannot produce
of our model lies a Transformer that (1) learns to balance
translation scale. Learning-based methods (here LoFTR with 8-
betweensolvedandlearnedposeestimations, and(2)pro-
Point ViT [62] head) produce scale (right) and are more robust,
vides a prior to guide a solver. A comprehensive analy-
butlackprecision(left). FARleveragesbothforpreciseandro-
sis supports our design choices and demonstrates that our
bustprediction,includingscale.
method adapts flexibly to various feature extractors and
correspondence estimators, showing state-of-the-art per-
whichcanhandledensefeaturesorcorrespondencesasin-
formancein6DoFposeestimationonMatterport3D,Inte-
put. Putsuccinctly,themethodisFlexible: agnostictocor-
riorNet,StreetLearn,andMap-freeRelocalization. Project
respondence and feature backbone; Accurate: matches the
page: https://crockwell.github.io/far/
precision of correspondence-based methods; and Robust:
buildsupontheresilienceoflearnedposemethods.
1.Introduction
FAR enables learning-based and solver-based methods
Relativecameraposeestimationisafundamentalproblem to improve each other. Learned predictions are more ro-
incomputervision[24],withapplicationsinaugmentedre- bustthansolveroutput,andarethereforeusedasapriorto
ality [29, 37, 42], robotics [50, 67, 81], and autonomous biasthe solver. Improved solveroutput, whichtendsto be
driving [9, 23]. One recent line of work learns to estimate moreprecisethanlearnedoutputwhenitsucceeds, isthen
correspondences then solve for pose [19, 43, 51, 65, 70], combined with Transformer predictions to form final out-
oftenofferingsub-degreeerrors. Unfortunately,thisframe- put. Predictionsarecombinedviaaweightingpredictedby
work tends to struggle when faced with large view change theTransformer,meaningtheTransformercanlearntorely
(Figure 1, left), and additionally cannot recover scale be- moreuponeithermethoddependingontheireffectiveness.
causeitproducestheFundamentalorEssentialmatrix. An- Figure 2 analyzes FAR in practice, measuring error as
otherlineofworklearnstoestimateposedirectly[5,10,41, a function of the number of good input correspondences.
62,75],whichisnotasprecise,butcanbemorerobustand With many correspondences, the solver is highly accurate,
producestranslationscale(Figure1,leftandright). leavinglittleroomforimprovementfromtheprior. Asthe
The proposed method builds upon both communities to number of correspondences drop, solver performance de-
produceageneralmethodthatisnoworsethaneitherofthe grades, but this can be alleviated meaningfully using the
options and often better than both. Critically, it leverages learned prior. The learned weighting also contributes to
learnedcorrespondencepredictionsasinput,andcombines robustness, and is plotted on the right: the Transformer
learned pose estimation with a solver to estimate 6DoF primarily uses solver output if there are many correspon-
pose. Forthistask,wepurposefullyselecttheTransformer, dences, and more heavily uses the regressor when there
1
4202
raM
5
]VC.sc[
1v12230.3042:viXra
)°(
rorrE
noitatoR
)m(
rorrE
noitalsnarTare few correspondences (Figure 2, right). The result is a Rot. Error vs. Inliers Solver Wt. vs. Inliers
method that does not sacrifice in the case of many corre- Corr. + Solver + Scale Ts
spondences,buthasalargegaingivenfewcorrespondences. 60
F FA AR R:
:
T Ur pa dn as tf eo drm Te ur Tt 0.8
FAR: Full T 0.6
Experiments analyze FAR in detail across a number of 40
scenarios and datasets. First, we analyze theoretical ro- 0.4
20
bustness,beginningfromgroundtruthcorrespondencesand 0.2
FAR: Pred. Solver Wt. 1 wr
procedurally adding (1) noise and (2) outliers. We next 0 0.0
0 100 200 300 400 0 100 200 300 400
evaluatetheproposedmethodonfourchallengingdatasets, Number of Inlier Correspondences
spanning both indoors: Matterport3D and InteriorNet, and Figure2.CombiningClassicalandLearned.Left:Solveroutput
outdoors:StreetLearnandMap-freeRelocalization. Across isprecisegivenmanyinliers,butispoorwhenfewareavailable;
settings,theproposedmethodtypicallyoutperforms,oroc- UpdatedsolveroutputviaFAR’spriorimprovesrobustnesssignif-
casionallymatches,thestateoftheart. Weadditionallyan- icantly. FAR’sTransformerislessprecisebutmorerobust. The
alyzethecomponentsofFARinablations,andapplyittoa fullmodelfusesprior-guidedSolveroutputandTransformerout-
put for the best of both, giving more weight to the solver when
varietyofpermutationsofcorrespondenceandfeatureesti-
manyinliersareavailable(right).
mationbackbones. Wealsostudytheimpactofdatasetsize
uponmodelbehavior.
supervises a Scoring CNN to predict consensus and guide
RANSAC.FARisdistinctfromtheseworksasitsfinalout-
2.RelatedWork putisaweightedcombinationofSolverPoseandLearned
Pose.ThisimportantdifferenceallowsFARtopredictscale
LearnedCameraPoseEstimation. LearnedCameraPose
andimproverobustnesstopoororlimitedcorrespondences
Estimationhasrecentlymadeimpressiveprogress. Ifmany
(Figure 2). GRelPose [33] predicts pose from correspon-
views are available, camera pose can be precisely refined
dences,butdoesnotuseasolver,limitingprecision.
duringSLAM[15,72]orVisualOdometry[36,73,76]. If
fewerviewsofasceneareavailable,methodshavebecome
3.Approach
increasinglyrobusttoe.g. largerotation[29,68,82].
This paper focuses on the wide-baseline two-view set- Ourgoalistopredictrelativecamerapose,includingtrans-
ting,whichhasalsostronglyprogresses[10,14,20,79,80]. lation scale, from two overlapping images. This 6DoF
Someofthesemethodsalsoperform3Dreconstruction[1, pose can be parameterized as T ∈ SE(3), consisting of
30, 55, 71]. We build off the 8-Point ViT [62], a SOTA R ∈ SO(3) and t ∈ R3. We specifically focus on pre-
methodfortwo-image6DoFcameraposeestimation. dicting translation scale, which cannot be solved for from
Correspondence Estimation. Correspondence can be correspondencesalone,inordertoenablerealworldappli-
learned [13, 18, 19, 28, 35, 51] or attained using classi- cations e.g. 3D reconstruction and neural rendering. The
cal methods [6, 45, 64], including specialized for wide- two-view case facilitates these applications on e.g. image
baselinestereo[47,49,54]. WeuserecentSOTAmethods collections.Weassumeknowncameraintrinsicsastheyare
LoFTR [70] and SuperPoint+SuperGlue [16, 65], but note generallyavailablefrommoderndevices[2].
FARcanreadilyadapttoalternativeestimators. FAR fuses complimentary strengths from the two lines
of pose estimation work: learned correspondence estima-
Camera Pose Estimation from Correspondences. Cam-
tion followed by a robust solver, and end-to-end pose esti-
era pose estimation from correspondences is a long stand-
mation. Critically,itproducesresultsthatarenoworsethan
ing[22]andstillactiveproblem[3]. Typically, algorithms
either and often better than both. We design the method
usearobustestimator[3,4,22]alongwiththe7-Point[38]
as flexible to be plugged-in to existing methods with min-
or8-Point[26]algorithmtofindthefundamentalmatrix,if
imal change, showing improved results in a variety of set-
intrinsicsareunknown,or5-Pointalgorithm[52]tofindthe
tingsanddatasets.WeoutlineFARinSection3.1,detailthe
essentialmatrix,ifintrinsicsareknown. ForEcanthanbe
learnednetworkinSection3.2andhowweapplyapriorto
decomposedintoRT(withouttranslationscale)andestimat-
thesolverinSection3.3.
ing direction via triangulation and the chirality check. We
assumeknownintrinsicsanduseRANSACwiththe5-Point
3.1.ApproachOutline
algorithm,butourcontributionsareorthogonaltoestimator.
Recent work incorporates learnable elements into pose Figure 3 shows an overview of the proposed approach. At
estimation from correspondence [57]. Barroso et al. [5] theheartoftheapproachisthePoseTransformer(Sec3.2)
learn to select from candidate essential matrices. Roessle whichtakesindensefeaturesandoutputs(1)anestimateof
and Nießner [63] use a differentiable 8-Point algorithm, 6DoFposeT and(2)arelativeweightwofthisprediction.
t
while Wei et al. [77, 83] use a differentiable robust esti- T is then combined with solver-estimated pose T using
t s
mator, to improve F via end-to-end training. DSAC [7] weightwtoobtainposeestimateT . T isusedasaprior
1 1
2
)°(
rorrE
noitatoR
thgieW
revloS
detciderPInputImages Round 1 Round 2 Output Pose Regime 1: Few Corr. Regime 2: Many Corr.
Pose w
T
Feats / Transformer T t
Corr. Corr.
w favors Transformer T w favors Solver T
Estimator t s
Corr. T T 1 Updated T u uses Prior T 1 Updated T u ignores Prior T 1
s
Classical T
u
Solver
Solver T | FAR: Transformer T | FAR: Round 1 T | FAR: Updated T | FAR: Full T
s t 1 u
Figure3. Overview. Givendensefeaturesandcorrespondences,FAR’sTransformerproducescameraposes(insquareboxes )through
atransformer(roundbox )andclassicalsolver(roundbox ).Inthefirstround,thesolverproducesaposeT .FAR’sposetransformer
s
averagesthiswithitsownpredictionT viaweightw, toyieldtheround1poseT . T poseservesasapriorfortheclassicsolver,
t 1 1
whichproducesanupdatedposeT . ThisiscombinedwithanadditionalestimateofT andweightw toproducethefinalresultT.
u t
Withfewcorrespondences,T helpssolveroutput,whilethenetworklearnstoweighTransformerpredictionsmoreheavily;withmany
1
correspondences,solveroutputisoftengood,sothenetworkreliesmostlyonsolveroutput.
forthesolver,resultinginupdatedsolveroutputT ,which a rotation matrix using Gram–Schmidt orthogonalization.
u
iscombinedwithT viawtogetthefinaloutputT. Toaddressscale-lesssolveroutput, wescaletranslationt
t s
Thisarchitectureenablesthenetworktolearntobehave by the Transformer predicted translation magnitude ||t ||,
t
differently depending on the data regime. In the case of before linearly combining. We find this stabilizes training
many, high-quality correspondences, classical solvers are compared to first averaging predicted the angles of t and
s
typicallyprecise,sothepriorhaslittleimpactonthesolver, t andthenapplyingscaletonormalizedpredictions. Our
tf
whilethenetworklearnstoheavilyrelyuponsolveroutput finalformulais:
via a low w. In the case of few, low-quality correspon- Rˆ =w R +(1−w )R
r t r s
(1)
dences, solvers degrade, so the prior is designed to have a ˆt=w t +(1−w )||t ||t
t t t t s
strong influence on solver output, while the network relies
moreheavilyonthetransformerpredictions(highw). TransformerBackbone. Weusetwodistinctarchitectures
Theapproachisagnostictoinputfeaturesandcorrespon- to span possible inputs: if features are available, we use
dences. Inexperiments(Sec4),weshowsuccesswithfea- a modified ViT. If only correspondences are available, we
turesfromthreefeatureestimationmethods[2,62,70]and use a Vanilla Transformer. This means the method can be
correspondences from two correspondence estimators [65, usedwithcorrespondenceorregression-basedmethodspro-
70].WeuseaViT[17]tohandlespatialfeatures,andlosses ducingdensefeatures,whileaccommodatingmethodsonly
canbackpropagatethroughthebackbone. Wealsoexplore outputting correspondence. In each case, the Transformer
usingonlycorrespondencesanddescriptorsasinput. producesfeaturesf usedasinputtotwoMLPheads.
o
8-Point ViT. This network takes as input pairwise dense
3.2.PoseTransformer
features f ,f and produces a feature vector f . It con-
i j o
The goal of the Transformer is to estimate (1) 6DoF rel- sists first of one LoFTR [70] self-attention and cross-
ative camera pose T between two wide-baseline images attention layer followed by an 8-Point ViT cross-attention
t
and (2) weight w ∈ [0,1] of its estimate vs. solver es- layer [17, 46, 62]; both networks are aimed at producing
timates from a set of 2D correspondence matches M = goodfeaturesforposeestimation.Fordetailedarchitectures
{(p,q)}|p,q ∈ R2 and optionally dense 2D image-wise ofeach,seetheoriginalworks.
features f ,f . Given predicted camera pose and weight, Vanilla Transformer. This network takes as input a set of
i j
the final output is the linear combination of Transformer correspondencesM={(p,q)}|p,q∈R2includingasso-
pose T and solver pose T weighted by w. We use sep- ciated descriptors, if available, and produces a set of fea-
t s
arate weights for translation w and rotation w , allowing tures f . We use a vanilla Transformer encoder with N
t r o
theTransformertohavedifferentconfidencesfortwosub- layers, and map correspondences and descriptors as input.
tlydifferentproblems. WeencodecorrespondencesinasinusoidalmannerwithK
Twochallengestothissetuparethatlinearcombinations bands,followedbyalinearmappingtoasizeofcinputto
ofrotationsareoftennotrotationmatrices,andsolvertrans- theTransformer. Ifdescriptivefeaturesofeachcorrespon-
lationdoesnothavescale. Toaddresstheformer,werepre- dence point are available, of dimension d < c, a Linear
sentposeinthe6DcoordinatesystemofZhouetal. [84], layer maps them to c and they are concatenated to corre-
4
whichenablesustocombinein6Dspacebeforecomputing spondencelocationswhicharelinearlymappedto 3c.
4
3The Vanilla Transformer can also be used with dense afixedthresholdσ. Given,hypothesisHandsetMofcor-
features f as input. If networks produce a joint feature respondences, E(p,q|H), is the Sampson Error between
encoding for two images, the Transformer can be applied pointspandqunderH. Thescoringfunctionisdefinedas
directly to low-resolution features, without positional en- score(H) = (cid:80) 1(E(p,q|H) < σ). Samplingre-
{p,q}∈M
coding. This occurs in the Regression model of Arnold et peatsuptoN timesoruntilstoppingheuristicsaremetfor
al.[2],whichwebuilduponinTable5. efficiency[56],andthehighestscoringmodel,e.g. theone
RegressionMLP.ThisMLPmapsTransformerfeaturesf withthemostinliers,isselected. WorkslikeMAGSAC[3],
0
toR∈R6andt∈R3usingtwohiddenlayers. MAGSAC++[4]haveshownthatimprovingscoringfunc-
GatingMLP.ThistakesasinputTransformerfeaturescon- tions to show better performance. For simplicity, we con-
tinue the exposition with thresholding based function that
catenatedwithRegressionMLPpredictionsandpredictions
arepopularwithclassicRANSAC[22].
from the classical solver, along with the number of inlier
correspondencesinthesolveroutput, usingseveralthresh- LimitationsinFew-CorrespondenceCase. Theheuristic
olds. Predictionsandnumberofinliersarenormalizedthen scoreofcountinginlierstypicallyisnoteffectiveespecially
input as scalar features. As we analyzed in Figure 2, the in the low-correspondence case [5]. When the number of
numberofinliercorrespondencesishighlycorrelatedwith correspondences is only asmall multiple of the number of
the performance of solver pose estimate T . The Gating points needed to minimally define a model the algorithm
s
MLPhastwohiddenlayersandendswithaSigmoid, pro- becomesparticularlyunreliable. Considertheextremecase
ducingw ,w ∈(0,1). of doing pose recovery with calibrated cameras from nine
t r
points, of which five are inliers. The minimal subset for
3.3.Prior-GuidedRobustPoseEstimator
pose estimation is five points, and so while the true model
willhavefiveinliers,sowillanyothersampledhypothesis.
Now,havingshownhowthesolvercanhelplearning, how
Accordingly,theresultwillberandomhypothesis.
canthelearningbasedmethodshelpthesolver?Theperfor-
mance of search-based solver methods like RANSAC [22] Prior-Guided Estimator. We propose to incorporate a
is driven by searching over a model space by sampling learning based predictions to aid the solver in the case of
valid hypothesis and then ranking them based on a scor- feworpoorcorrespondences. Weoperationalizethisbyin-
ing function. The scoring function serves as the measure corporating a prior model that estimates the likelihood of
of probability of data under a hypothesis [74]. It’s typical hypothesisunderthenetwork’spredictionusingafunction
tousesuchsolverswhenestimatingposefromasetofcor- β(·|T 1). Theβ(H|T 1)measuresthelogprobabilityofthe
respondences, but direct application of these methods can hypothesized model H under T 1. We found it difficult to
notberobustwhencorrespondenceestimationisdonewith measureprobabilitiesinrotationandtranslationandweigh
ascarceset. Ourkeyideaistousethepredictedposeesti- them,soasaproxy,wecomparehowthemodelstransform
mate,T ,toinfluenceboththesearchandthescoringfunc- afixedsetofgridpoints. Inparticular,wemeasuretheneg-
1
tiontohelpindatascarcescenarios. ativeofaveragesquareddistancebetweenafixedsetofgrid
We take inspiration from existing lines of work in us- points transformed by T 1 and the same fixed grid trans-
ing learning to better inform sampling and selection in formedbyH. SeeSupplementalfordetails.
RANSAC-like algorithms [3, 4, 7, 56, 74]. We show that Now,weshowhowthisβ functioncanalterthescoring.
wecanrecycleestimatesfromalearning-basedmodeland The modified scoring function measures the likelihood of
plugtheseestimatesinsimplistically.Specifically,aninitial thehypothesisHunderT alongwithmeasuringthelike-
1
estimatedpose, T , tomodifythesearchfunctionsoasto lihoodofdata[74]underH. Itisdefinedas,
1
samplemorehypothesisclosetoT . Secondly,wemodify
1
thescoringfunctiontoconsiderthedistancetotheT 1along score(H)=αβ(H|T 1)+ (cid:88) 1(cid:16) E(p,q|H)<σ(cid:17) (2)
withinliercount.
(p,q)∈M
RANSAC Preliminaries. The typical approach to pose
estimation from correspondences applies random sample which is the (log) product of probability of the hypothesis
consensus (and variants) e.g. RANSAC, USAC [56] or given our β prior function and the probability of the the
MAGSAC [3, 4] to model fitting e.g. 5, 7, or 8-Point al- data,M,underH. Weweighthepriorwithascalar,α∈R.
gorithms [26, 39, 44, 52]. These methods use a notion In this setting, the prior tie-breaks ambiguous cases where
of epipolar distance such as Sampson Error [24] for inlier twohypotheseshavesimilarnumbersofinliers,buthasdi-
thresholding (soft and hard). More concretely, given a set minishing influence as |M| gets bigger. As |M| → ∞ ,
of2DcorrespondencematchesM = {(p,q)}|p,q ∈ R2, theprior’simpactiswashedoutentirely. Thisformulation
a minimal subset of points is randomly sampled to fit a hasthedesiredimpactofsignificanteffectwhencorrespon-
model H via an n-point algorithm. The scoring function dencesarefewandunbiasedhypothesesarepoor,andlittle
countsnumberofinliersthathaveSampsonErrorlessthan impactwhencorrespondencesaremany.
4Med. Rot. Error vs. Noise Med. Rot. Error vs. Outliers Table 1. Camera Pose Estimation on Matterport3D.
30 Solver Ts Correspondence-based methods have low median but high mean
FAR: Transformer Tt 20 error, and do not produce translation scale. Regression-based
FAR: Updated Tu
20 FAR: Full T 15 methodsarelessprecisebutproducescale.FARbuildsuponboth,
resultinginlowmedianandmeanerror,withtranslationscale. 10
10 5 Translation(m) Rotation(◦)
Method Med.↓ Avg.↓ ≤1m↑ Med.↓ Avg.↓ ≤30↑
0 0
[60]+[58] 3.34 4.00 8.3 50.98 57.92 29.9
0 8 16 32 0 50 75 87.5
Correspondence Noise (Pixels) Correspondence Percent of Outliers (%) Assoc.3D[55] 2.17 2.50 14.8 42.09 52.97 38.1
SparsePlanes[30] 0.63 1.25 66.6 7.33 22.78 83.4
Figure4. GroundTruthRobustnessStudyonMatterport3D. PlaneFormers[1] 0.66 1.19 66.8 5.96 22.20 83.8
8-PointViT[62] 0.64 1.01 67.4 8.01 19.13 85.4
Using true correspondence, the solver is nearly perfect. Adding
NOPE-SAC-Reg[71] 0.52 0.94 73.2 2.77 14.37 89.0
noiseoroutliers,itquicklydegrades,whileprior-guidedUpdated SuperGlue[65] - - - 3.88 24.17 77.8
solverisrobusttooutliersandtheTransformerisrobusttonoise. LoFTR[70] - - - 0.23 9.49 91.4
FARmatchesorbeatsallmethodsacrosssettings. LoFTR+Reg.Scale 0.85 1.21 56.3 0.26 9.66 91.2
FAR(VanillaTF) 0.37 0.67 81.9 0.26 6.14 94.2
FAR 0.25 0.49 89.2 0.20 4.93 95.8
Sampling Good Hypotheses. Randomly sampling points
and estimating H is unlikely to lead to hypothesis con-
Prior-Guided Estimator. We implement in Kornia [61]
sistent with the model T . To increase the chance to
1
anduse2krandomsampleswithoutearlystoppingandin-
sampling consistent hypothesis we want to sample a min-
lierthresholdonL2SampsonErrorσ of3×10−7,finding
imal subset that best agrees with the model. We achieve
theseresultsmostcloselymatchedOpenCV[8]outputus-
this by weighing the correspondences by their agreement
ing LoFTR settings. For the Prior, we use τ = 0.1 and
with the model in turn influencing the as w(p,q) =
α=3.33foundthroughgridsearchonthevalidationset.
exp(−Sampson(p,q|T )/τ).
1
In practice, we sample half the hypothesis using biased
4.Experiments
sampling use uniform sampling for the other half. This
improves sample diversity in the case of many correspon-
Wedesignourexperimentstomeasuretheeffectivenessof
dences,inwhichcaseunbiasedsamplingisveryeffective.
FARinachievingourstatedgoals:flexible,accurateandro-
bust6DoFposeestimation. Wefirstvalidaterobustnessby
3.4.ImplementationandTrainingDetails
measuringmodelperformanceasafunctionofincreasingly
Acrossexperiments,weusetheAdam[34]optimizer. The perturbedgroundtruth. Wenexttestprecisiontomoderate
8-Pt ViT trains for about 300k iterations or 7 days on 10 view change and robustness to large view change by com-
GTX1080Ti;VanillaTFtrainsforabout600kiterationsor paringtothestateoftheartinwide-baselinerelativepose.
3 days. We select the checkpoint with the lowest valida- Havingdemonstratedaccuracyandrobustness,wenextver-
tionmeanrotationerror,whichtendstobemarginallymore ify model flexibility to choice (or lack) of dense feature
stable than translation error. We represent rotation in 6D method, correspondence estimation method, and dataset
coordinates[84]anduseL1loss. Modelsaretrainedstage- size. Finally, we compare to the state of the art on addi-
wise:firstwetraintheTransformertoestimatepose,thento tionalindoorandoutdoordatasets.
estimateposejointlywithavanillasolver,thentoestimate
4.1.RobustnesstoCorrespondencePerturbations
jointly with the prior-based solver. We find this progres-
sivetrainingimprovesfinalperformance. Weimplementin Here,weassumetheimagecorrespondencesaregivenand
PyTorch[53]Lightning[21],usingTIMM[78]fortheViT. study how variations of our method and baselines perform
8-PointViT.Wetrain8-PointViTend-to-endwiththefea- withvaryingnoise-levelsappliedtothecorrespondences.
ture extraction backbone. We found including a self and Dataset. We use image pairs collected using the Habi-
cross-attention LoFTR layer significantly improved learn- tat [66] embodied simulator upon Matterport3D [12], fol-
ingcapacity,whileadditionallayersdidnothelpfurther. lowingthesetupofJinetal.[30]. Ithas32ktrain/5kval/
Vanilla Transformer. We use a 6-layer encoder with 8
8ktestpairswithsmalltomoderateoverlap(average53◦ro-
headsand512featuresizefollowedbyglobalaveragepool- tation,2.3mtranslation,21%overlap). Thevarietyinview
ing. We use K = 42 bands for positional encoding, and change enables the study of both precision upon moderate
linearly map them to size 384, concatenating descriptive casesandrobustnesstohighlychallengingcases.
features linearly mapped to size 128. We found random Metrics. Throughout Matterport3D experiments, we re-
dropout on correspondences with p = 0.1 helps perfor- port three metrics for rotation and translation: median er-
mance. Wecachecorrespondencesforfasttraining. Train- ror,meanerror,andpercentageoferrorswithinathreshold.
ingspeedis≈12iterationspersecondonaGTX1080Ti. These are standard metrics, which identify our two quali-
5
)°(
rorrE
noitatoR
)°(
rorrE
noitatoRTable2. AblationsonMatterport3D.(Top)Weimprovesignifi- Table 3. Approach Flexibility to Features and Correspon-
cantlyuponLoFTRusingacombinationoflearnedandclassical. dences. FAR yields improvement using features from 8-Pt ViT
(Middle)Thisresultholdsforthecaseofnoinputfeatures,where orLoFTR;andcorrespondencesfromSuperGlueorLoFTR.
weusetheVanillaTF.(Bottom)ScalingSolvertranslationisim-
Translation(m) Rotation(◦)
portanttoFARperformance;selectingseparateweightsforRand Feats. Corr. PoseEst. Med.↓ Avg.↓ ≤1m↑ Med.↓ Avg.↓ ≤30↑
T improvesrobustness. 8-PtViT - 8-PtViT 0.64 1.01 67.4 8.01 19.1 85.4
8-PtViT SuperGlue FAR 0.62 1.01 68.3 7.02 16.6 86.8
Translation(m) Rotation(◦) 8-PtViT LoFTR FAR 0.63 1.01 68.5 7.06 17.0 86.9
Transformer:8-PointViT Med.↓ Avg.↓ ≤1m↑ Med.↓ Avg.↓ ≤30↑ LoFTR LoFTR RANSAC+5Pt - - - 0.23 9.49 91.4
- LoFTR FAR(VanillaTF) 0.37 0.67 81.9 0.26 6.14 94.2
LoFTR+Solver+ScaleTs 0.85 1.21 56.3 0.26 9.66 91.2 LoFTR LoFTR FAR 0.25 0.49 89.2 0.20 4.93 95.8
FAR:TransformerTt 0.38 0.64 85.4 4.51 9.94 94.2
FAR:OneRoundT1 0.25 0.49 89.0 0.20 5.08 95.7
FAR:UpdatedTu 0.25 0.50 88.4 0.20 5.35 95.0 Rot. Error vs. Inliers Solver Wt. vs. Inliers
FAR:FullT 0.25 0.49 89.2 0.20 4.93 95.8 40% Size
40 100% Size 0.8
Transformer:VanillaTF
30 0.6
LoFTR+Solver+ScaleTs 0.85 1.21 56.3 0.26 9.66 91.2
FAR:TransformerTt 0.42 0.75 79.1 3.87 10.8 92.5 20 0.4
FAR:OneRoundT1 0.37 0.67 81.8 0.26 6.41 93.8
FAR:UpdatedTu 0.37 0.68 81.5 0.25 6.69 93.7 10 0.2
FAR:FullT 0.37 0.67 81.9 0.26 6.14 94.2
0 0.0
0 100 200 300 400 0 100 200 300 400
PredictionSelection Number of Inlier Correspondences
UnscaledSolverts 0.31 0.55 87.4 0.21 5.09 95.8
OneWeight(wr=wt) 0.25 0.50 88.7 0.20 5.04 95.8 Figure5.EvolvingwithDatasetSize.TheTransformerlearnsto
FAR 0.25 0.49 89.2 0.20 4.93 95.8
relymoreheavilyuponthesolverifdataislimited(40%datasize),
andlearnstouseTransformerposeestimationsasdatascalesand
ties of interest: precision (median) and robustness (mean
performanceimproves(100%datasize).
andpercentage). Wefollowpriorwork[30,71]inusingro-
tationthresholdof30◦andtranslationthresholdof1m. For
4.2.Wide-BaselinePoseonMatterport3D
the ground truth study, for brevity we report median error
acrossavarietyofsettings,whichisanindicativesummary
In this section, we use the same Matterport3D dataset and
ofperformance. AdditionalresultsareinSupplemental.
themetricsusedinSec.4.1,buttheinputsareimagesrather
Setup. Beginning with ground truth correspondences, we thantheGTcorrespondences.
(1) Apply Gaussian noise with standard deviation from
Baselines. We compare against state-of-the-art solver-
0 to 32 pixels, upon 480x640 images; (2) Replace true
based and learning-based baselines. For solver-based
correspondences with randomly sampled coordinates (out-
methods, we choose the popular LoFTR [70] and Super-
liers),withP(Outliers)from0to0.875. Modelsaretrained
Glue[65]. Inthelearnedspace,wecomparetoend-to-end
andevaluateduponeachnoiseandoutliersettingindepen-
classical-estimation-inspiredViT,8-Point[62];planarmap-
dently;e.g.FARistrainedandevaluatedfourtimestomake
ping and optimization methods NOPE-SAC [71], Plane-
CorrespondenceNoiseGraphinFigure4,left.
Formers[1]andSparsePlanes[30];and3Dreconstruction
Ablations. Weconsiderthefollowingcases: methodAssociative3D[55]. Inthissetofexperimentsour
(1) Solver T s. Using LoFTR’s solver, i.e., FAR builds upon LoFTR backbone and correspondences.
RANSAC[22]+5-PointAlgorithm[52] We additionally report results using only correspondence
(2) FAR: Transformer T . Pose Transformer output pose. anddescriptorasinput,as“FAR(VanillaTF)”.
t
This is a Vanilla Transformer, since dense features are not Results. Table 1 shows the quantitative results on Matter-
availableasinput;onlycorrespondences. port3D. Among the prior works, end-to-end methods such
(3)FAR:UpdatedT . SolveroutputusingFAR’sPrior as 8-Point ViT [62] perform well in absolute Translation,
u
(4)FAR:FullT. FullFAR:learnedcombin. ofT andT whilecorrespondence-solvermethodse.g.LoFTR[70]per-
u t
AblationResults.Figure4showsSolverhasnearlyperfect formbestinrotation. FARsetsanewstandardinbothmet-
resultsongroundtruthcorrespondencesbutisnotrobustto rics, surpassing the best prior baseline (NOPE-SAC-Reg)
noise or many outliers. FAR: Transformer is less precise byalargemargin. Itreducesthemedianandmeantransla-
ongroundtruthbutismorerobustasoutlierfrequencyand tionerrorsbyabout50%: from0.52to0.25andfrom0.94
particularlynoiseincreases. Thepriorishighlyeffectiveat to0.49,respectively.Additionally,itdecreasesthemeanro-
leaving FAR: Updated robust to outliers, showing close to tationerrorbyalmost50%comparedtothebestpriorwork
0◦ medianerrorevenwith87.5%outliers. Thefullmethod (LoFTR), from 9.66 to 4.93. Even with only correspon-
offers the best of all: precise estimation on ground truth dence available as input, “FAR (Vanilla TF)” is typically
correspondenceswiththebestorequaltobestrobustnessto betterthanallpriorworkbyalargemargin.
noiseandoutliers. Ablations. To investigate the source of FAR’s out-
6
)°(
rorrE
noitatoR
thgieW
revloS
detciderPInteriorNet: Error vs. Mag. StreetLearn: Error vs. Mag. Rot. Error vs. Rot. Mag. Tr. Error vs. Rot. Mag.
40 Corr. + Solver 150 Corr. + Solver + Scale 6
Learning-Based 50 Learning-Based
20 FAR 100 FAR 4
0 0
50 2
10
5
0 0 0 0
0 20 40 60 80 0 20 40 60 80 0 50 100 150 0 50 100 150
Rotation Magnitude (°) Rotation Magnitude (°)
Figure6.RotationErroronInteriorNetandStreetLearn.Even Figure 7. Error on Map-free Relocalization. FAR leverages
whenCorrespondence+Solvermethodisrelativelypoor,wecan Solver output to improve regression results on this highly chal-
stillleverageittoimproveregressionresults. “Learning-Based”: lenging dataset. “Corr. + Solver + Scale”: LoFTR + DPT [59]
8-PointViT[62].“Corr.+Solver”:LoFTR[70]. trainedonKITTI[23].“Learning-Based”:6DReg.
performance, weconductablationsonMatterport3Dusing aswellasascenariowithoutdensefeatures. Additionally,
thesamesetupofSec.4.1,exceptcorrespondencesarepre- we evaluate two recent SOTA settings for correspondence
dicted. InadditiontoablationsdiscussedinSection4.1,we estimation: LoFTR,andSuperPoint[16]+SuperGlue[65].
consider model output after One Round (T 1) to study the Results. Table 3 shows FAR improves upon 8-Pt ViT, us-
impactoftheprior-guidedsolveruponthefinalmodel. We ingeitherSuperGlueorLoFTRcorrespondences.Similarly,
further explore performance by using two different Trans- FARimprovesLoFTR,whetheritemploysbothLoFTRfea-
formerarchitectures: thedensefeature-basedTransformer: turesandcorrespondencesorjustthecorrespondences.
8-PointViT(referredtoasFARinotherexperiments),and
Dataset Scaling. We next present the proposed method
the correspondence-only Transformer: Vanilla TF. Finally,
whentrainedonaversionoftheMatterport3Ddatasetthat
we evaluate the design choices outlined in Section 3.2 un-
has been randomly subsampled to 40% of its original data
der the Prediction Selection category: Unscaled Solver t ,
s size.InFigure5,wecomparerotationerror(left)andsolver
whereSolvertranslationisnotscaledbyTransformermag-
weight (right) of 40% size and full size. Note that as the
nitude; and One Weight (w = w ), which uses equal
r t trainingdatasetsizeincreasesfrom40%to100%,boththe
weightsforTransformertranslationandrotationprediction.
solverweightanderrordecrease. Thistrendalignswithex-
AblationResults. AsshowninTab.2, wecanclearlyob- pectations: as the Transformer’s estimated pose accuracy
serve the same trends from Fig. 4: the solver is precise improveswithmoretrainingdata,itgainsalargerinfluence
in most cases characterized by low median rotation error. inthefinaloutput,enhancingoverallperformance. There-
However the solver suffers from high mean error (due to sultsuggestsafixedweightingoflearnedandsolveroutput
outliers) and poor translation errors. Incorporating FAR’s isnotsufficientforbestresults.
priorsignificantlyimprovesthesolver’smeanrotationerror.
In contrast, Transformer regression outputs are not nearly 4.4.Wide-BaselinePoseonAdditionalDatasets
as precise, with median rotation error of above 4◦, but it
Weevaluateourmethod’sperformanceonvariousdatasets
reducestheratiosoflargeerrors(thosegreaterthan1mor
toassessitsversatility.WefollowCaietal.[10]anduseIn-
30◦). FAR enhances the best results achieved by both the
teriorNet[40],asyntheticcollectionofindoorhomescenes,
Transformer and Solver. These patterns hold true for the
and StreetLearn [48], which features outdoor city street
VanillaTransformeraswell.InPredictionSelection,wesee
photos. Both datasets consist of 90◦ field-of-view image
predicting Solver translation scale is important for transla-
crops upon panoramas. Image pairs are chosen from dif-
tionperformance,whileseparateweightingforrotationand
ferentpanoramaswithvaryingoverlaps,facilitatingtheas-
translationimprovesrobustness.
sessmentofprecisionandrobustnessinscenarioswithboth
4.3.ApproachFlexibility large(>45◦)andsmall(<45◦)overlaps. Additionally,we
use Map-free Relocalization [2], a challenging dataset of
WethenevaluatetheflexibilityofFARintermsofthefea- user-collected videos surrounding outdoor places of inter-
tureextractor,correspondenceestimator,anddatasetsize. est e.g. sculptures or fountains. SfM has been applied to
DatasetandMetrics. WecontinueusingtheMatterport3D thevideos,sotranslationwithscalecanbeevaluated.
datasetandmetricsasinSection4.1. Metrics. For InteriorNet and StreetLearn, we report rota-
AlternativeApproaches. ToassesstheversatilityofFAR, tionerroronly,inlinewithpriorwork[10,62],usinga10◦
weexploreoptionsthatareorthogonaltoourcorecontribu- threshold. For Map-free Relocalization, we calculate me-
tion.Specifically,weexaminethreesettingsforfeatureesti- diantranslationandrotationerrorspervideo,thenaverage
mation: therecentSOTAmethodsLoFTRand8-PointViT, these. We also includethe Virtual Correspondence Repro-
7
)°(
rorrE
noitatoR
)°(
rorrE
noitatoR
)m(
rorrE
noitalsnarTTable4. RotationPerformanceonInteriorNetandStreetLearn. Correspondence-basedmethods(top)struggletogeneralizetothis
data,whileregressionmethodslearnhelpfulfeatures. Buildingoff8-PointViTfeatures,wecanstillutilizeLoFTRcorrespondencesto
boostperformance.ErrorswerecalculatedonlyonsuccessfulpairsforSIFTandSuperPoint;graytextindicatesfailureover50%ofpairs.
InteriorNet StreetLearn
LargeOverlap(◦) SmallOverlap(◦) LargeOverlap(◦) SmallOverlap(◦)
Method Med.↓ Avg.↓ ≤10↑ Med.↓ Avg.↓ ≤10↑ Med.↓ Avg.↓ ≤10↑ Med.↓ Avg.↓ ≤10↑
SIFT*[45] 2.95 7.78 55.5 10.0 18.2 18.5 3.13 18.9 22.4 13.8 38.8 5.7
SuperPoint*[16] 2.79 5.46 65.9 5.82 11.6 11.7 1.79 6.38 16.5 6.85 6.80 1.0
LoFTR[70] 0.54 1.85 97.0 2.64 14.3 70.4 24.8 36.4 31.6 51.2 58.6 19.9
Reg6D[84] 6.91 10.5 67.8 11.4 21.9 44.1 6.02 12.3 69.1 7.59 15.1 63.4
Caietal.[10] 1.10 2.89 97.6 1.38 10.2 89.8 2.91 9.12 87.5 3.49 13.0 84.2
8-PointViT[62] 1.83 2.90 97.9 2.38 4.48 96.3 2.43 4.08 90.1 3.25 9.19 87.7
FAR 0.60 1.16 98.5 1.22 3.42 95.4 1.81 3.01 96.7 2.07 7.89 92.4
Table 5. 6DoF Performance on Map-free Relocalization. We Matterport3D InteriorNet StreetLearn Map-free
compareagainstthestrongestbaselinesfrom[2];forallcompar-
isons see the original paper. FAR uses the 6D Reg backbone,
addingLoFTRorSuperGluecorrespondencestobeat6DReg.
PoseError VCRE
Method
Avg.Med.↓ Prec.↑ AUC↑ Avg.Med.↓ Prec.↑ AUC↑
LoFTR 199cm 30.6◦ 0.15 0.35 168px 0.35 0.61
SuperGlue 188cm 25.4◦ 0.17 0.35 160px 0.36 0.60
RegAng.[2] 210cm 33.7◦ 0.09 - 200px 0.30 -
6DReg[2] 168cm 22.5◦ 0.06 - 147px 0.40 -
FAR(SG) 149cm 17.2◦ 0.17 0.35 135px 0.44 0.67
FAR(LoFTR) 148cm 18.1◦ 0.18 0.39 137px 0.44 0.68
Rot Mag: 88º Rot Mag: 51º Rot Mag: 43º Rot Mag: 84º
jectionError(VCRE)metrictomeasurereprojectionerrors
C+S: 84º C+S: 0.9º C+S: 56º C+S: 83º
L-B: 54º L-B: 8.6º L-B: 17º L-B: 38º
(see[2]fordetails).
FAR: 1.2º FAR: 0.3º FAR: 2.3º FAR: 22º
Baselines. WecompareourmethodwithSOTAcorrespon-
denceandposeestimationtechniques. ForInteriorNetand Figure 8. Success Cases. For some challenging wide-baseline
StreetLearn, we compare to Cai et al.’s [10] correlation imagepairs,ourmethodoftendramaticallyoutperformsthebase-
volume-based learning, SuperPoint [16], and the classical lines. “Learning-Based”: LoFTR[70]with8-Pt. ViT[62]head
SIFTmethod[45]. WeuseLoFTRadaptedforInteriorNet (Matterport3D),8-Pt. ViT(InteriorNet,StreetLearn),6DReg[2]
(Map-free).“Corr.+Solver”:LoFTR.
usingMatterport3D,andforStreetLearnusingMegaDepth,
due to the lack of depth data in these datasets for training improvesupon6DRegandothermethods(seeTab. 5and
correspondences. Since LoFTR cannot be finetuned, we Fig.7).TheseresultsacrossdatasetsshowFAR’sadaptabil-
find8-PointViTfeaturesaremoreeffective, andusethese itytodifferentbackbonesanditsrobustnesstosub-optimal
as input to FAR, along with LoFTR correspondences. See correspondenceestimates,highlightedinFig. 8.
Supplementalfordetails.
5.Conclusion
Arnoldetal.[2]trainavarietyofposeestimationmeth-
odsonMap-free,including“6DReg”,whichcreatesacor-
Inthiswork,weaddresstheproblemof6DoFrelativecam-
relation volume [31, 32] and warps accordingly, followed
eraposeestimationgivenawide-baselineimagepair. Our
byaResNet[27],andissupervisedupon6Drotation[84].
introduced FAR represents a simple yet potent approach
Results. Tab. 4 and Fig. 6 show that 8-Point ViT [62] that merges the best aspects of correspondence-based and
achievesimpressivemeanerrors, under5◦, onInteriorNet, learning-basedmethods. Thisresultsinpreciseandrobust
even for small overlap pairs. FAR still adds precision on outcomes,adaptabletovariousbackbonesandsolvers.
topofthe8-PointViT.OnthechallengingStreetLearndata, Limitations and Societal Impact. FAR consists of sev-
FAR significantly outperforms the state of the art, despite eralcomponentsandimplementsPrior-GuidedRANSACin
LoFTRnotgeneralizingwelltoStreetLearn. Kornia,slowinginferencespeedto3.3it/secon101080Ti
6DRegisthestrongestoverallbaselineonMap-freeRe- GPUs;analysisvs. othermethodsappearsinSupplemental
localization, so we use its features for FAR, taking corre- Figure 11. Training upon affluent homes of Matterport3D
spondencesfromLoFTRorSuperGlue. Inbothcases,FAR canresultinworseperformanceonmoretypicalresidences.
8Acknowledgments and Disclosure of Funding. Thanks [16] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-
to Jeongsoo Park and Sangwoo Mo for their help- novich. SuperPoint: Self-supervisedinterestpointdetection
ful feedback. Thanks to Laura Fink and UM DCO anddescription. InCVPRW,2018. 2,7,8
for their tireless computing support. Toyota Re- [17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
search Institute provided funds to support this work. Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
References formersforimagerecognitionatscale. ICLR,2021. 3
[18] MihaiDusmanu,IgnacioRocco,TomasPajdla,MarcPolle-
[1] Samir Agarwala, Linyi Jin, Chris Rockwell, and David F.
feys,JosefSivic,AkihikoTorii,andTorstenSattler.D2-Net:
Fouhey. PlaneFormers: From sparse view planes to 3d re-
ATrainableCNNforJointDetectionandDescriptionofLo-
construction. InECCV,2022. 2,5,6
calFeatures. InCVPR,2019. 2
[2] Eduardo Arnold, Jamie Wynn, Sara Vicente, Guillermo
[19] Johan Edstedt, Ioannis Athanasiadis, Ma˚rten Wadenba¨ck,
Garcia-Hernando,AronMonszpart,VictorPrisacariu,Dani-
and Michael Felsberg. DKM: Dense kernelized feature
yarTurmukhambetov,andEricBrachmann.Map-freevisual
matchingforgeometryestimation. InCVPR,2023. 1,2
relocalization: Metric pose relative to a single image. In
ECCV,2022. 2,3,4,7,8,15,20 [20] SovannEn,AlexisLechervy,andFre´de´ricJurie. RPNet:An
[3] Daniel Barath, Jiri Matas, and Jana Noskova. MAGSAC: end-to-endnetworkforrelativecameraposeestimation. In
marginalizingsampleconsensus. InCVPR,2019. 2,4 ECCVW,2018. 2
[4] DanielBarath,JanaNoskova,MaksymIvashechkin,andJiri [21] WilliamFalconandThePyTorchLightningteam. PyTorch
Matas. MAGSAC++,afast,reliableandaccuraterobustes- Lightning,2019. 5
timator. InCVPR,2020. 2,4 [22] Martin A Fischler and Robert C Bolles. Random sample
[5] Axel Barroso-Laguna, Eric Brachmann, Victor Adrian consensus:aparadigmformodelfittingwithapplicationsto
Prisacariu, Gabriel J Brostow, and Daniyar Turmukhambe- imageanalysisandautomatedcartography.Communications
tov. Two-view geometry scoring without correspondences. oftheACM,1981. 1,2,4,6
InCVPR,2023. 1,2,4 [23] AndreasGeiger, PhilipLenz, andRaquelUrtasun. Arewe
[6] HerbertBay, TinneTuytelaars, andLucVanGool. SURF: ready for autonomous driving? the kitti vision benchmark
Speededuprobustfeatures. InECCV,2006. 2 suite. InCVPR,2012. 1,7
[7] Eric Brachmann, Alexander Krull, Sebastian Nowozin, [24] RichardHartleyandAndrewZisserman. Multipleviewge-
JamieShotton,FrankMichel,StefanGumhold,andCarsten ometry in computer vision. Cambridge university press,
Rother. DSAC-differentiableRANSACforcameralocaliza- 2003. 1,4
tion. InCVPR,2017. 2,4
[25] Richard I Hartley. Estimation of relative camera positions
[8] G. Bradski. The OpenCV Library. Dr. Dobb’s Journal of foruncalibratedcameras. InECCV.Springer,1992. 12
SoftwareTools,2000. 5
[26] Richard I Hartley. In defense of the eight-point algorithm.
[9] Guillaume Bresson, Zayed Alsayed, Li Yu, and Se´bastien
TPAMPI,19(6):580–593,1997. 2,4
Glaser.Simultaneouslocalizationandmapping:Asurveyof
[27] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.
currenttrendsinautonomousdriving. T-IV,2017. 1
Deep residual learning for image recognition. In CVPR,
[10] Ruojin Cai, Bharath Hariharan, Noah Snavely, and Hadar
2016. 8
Averbuch-Elor.Extremerotationestimationusingdensecor-
[28] Dihe Huang, Ying Chen, Yong Liu, Jianlin Liu, Shang
relationvolumes. InCVPR,2021. 1,2,7,8
Xu, Wenlong Wu, Yikang Ding, Fan Tang, and Chengjie
[11] RuojinCai,JosephTung,QianqianWang,HadarAverbuch-
Wang. Adaptive assignment for geometryaware local fea-
Elor,BharathHariharan,andNoahSnavely.Doppelgangers:
turematching. InCVPR,2023. 2
Learning to disambiguate images of similar structures. In
ICCV,2023. 18 [29] Hanwen Jiang, Zhenyu Jiang, Kristen Grauman, and Yuke
Zhu. Few-view object reconstruction with unknown cate-
[12] AngelChang,AngelaDai,ThomasFunkhouser,MaciejHal-
goriesandcameraposes. arXivpreprintarXiv:2212.04492,
ber,MatthiasNiessner,ManolisSavva,ShuranSong,Andy
2022. 1,2
Zeng,andYindaZhang.Matterport3D:LearningfromRGB-
Ddatainindoorenvironments. In3DV,2017. 5 [30] Linyi Jin, Shengyi Qian, Andrew Owens, and David F
[13] HongkaiChen,ZixinLuo,LeiZhou,YurunTian,Mingmin Fouhey. Planarsurfacereconstructionfromsparseviews. In
Zhen,TianFang,DavidMckinnon,YanghaiTsin,andLong ICCV,2021. 2,5,6
Quan. ASpanFormer: Detector-free image matching with [31] AlexKendall,HaykMartirosyan,SaumitroDasgupta,Peter
adaptivespantransformer. InECCV,2022. 2 Henry,RyanKennedy,AbrahamBachrach,andAdamBry.
[14] Kefan Chen, Noah Snavely, and Ameesh Makadia. Wide- End-to-endlearningofgeometryandcontextfordeepstereo
baseline relative camera pose estimation with directional regression. InICCV,2017. 8
learning. InCVPR,2021. 2 [32] SamehKhamis,SeanFanello,ChristophRhemann,Adarsh
[15] Jan Czarnowski, Tristan Laidlow, Ronald Clark, and An- Kowdle, Julien Valentin, and Shahram Izadi. StereoNet:
drewJDavison. DeepFactors:Real-timeprobabilisticdense Guided hierarchical refinement for real-time edge-aware
monocularSLAM. RA-L. 2 depthprediction. InECCV,2018. 8
9[33] Fadi Khatib, Yuval Margalit, Meirav Galun, and Ronen [51] JunjieNi,YijinLi,ZhaoyangHuang,HongshengLi,Hujun
Basri. GRelPose: Generalizableend-to-endrelativecamera Bao,ZhaopengCui,andGuofengZhang. PATS:Patcharea
poseregression. arXivpreprintarXiv:2211.14950,2022. 2 transportationwithsubdivisionforlocalfeaturematching.In
[34] Diederik P Kingma and Jimmy Ba. Adam: A method for CVPR,2023. 1,2
stochasticoptimization. ICLR,2015. 5 [52] DavidNiste´r. Anefficientsolutiontothefive-pointrelative
[35] Dominik A Kloepfer, Joao F Henriques, and Dylan Camp- poseproblem. TPAMI,2004. 2,4,6
bell. Scenes: Subpixel correspondence estimation with [53] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
epipolarsupervision. 2024. 2 James Bradbury, Gregory Chanan, Trevor Killeen, Zem-
[36] LeiLai,ZhongkaiShangguan,JimuyangZhang,andEshed ing Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch:
Ohn-Bar.Xvo:Generalizedvisualodometryviacross-modal Animperativestyle,high-performancedeeplearninglibrary.
self-training. InICCV,2023. 2 NeurIPS,32,2019. 5
[37] Zihang Lai, Sifei Liu, Alexei A Efros, and Xiaolong [54] PhilPritchettandAndrewZisserman. Widebaselinestereo
Wang. Videoautoencoder: self-superviseddisentanglement matching. InICCV,1998. 2
ofstatic3dstructureandmotion. InICCV,2021. 1 [55] Shengyi Qian, Linyi Jin, and David F Fouhey. Associa-
[38] Viktor Larsson, Magnus Oskarsson, Kalle Astrom, Alge tive3D: Volumetric reconstruction from sparse views. In
Wallis,ZuzanaKukelova,andTomasPajdla. Beyondgrob- ECCV,2020. 2,5,6
ner bases: Basis selection for minimal solvers. In CVPR,
[56] RahulRaguram, OndrejChum, MarcPollefeys, JiriMatas,
2018. 2
andJan-MichaelFrahm. USAC:Auniversalframeworkfor
[39] HongdongLiandRichardHartley. Five-pointmotionesti- random sample consensus. IEEE transactions on pattern
mationmadeeasy. InICPR,2006. 4 analysisandmachineintelligence,2012. 4
[40] Wenbin Li, Sajad Saeedi, John McCormac, Ronald Clark,
[57] Rene´ RanftlandVladlenKoltun. Deepfundamentalmatrix
DimosTzoumanikas, QingYe, YuzhongHuang, RuiTang,
estimation. InECCV,2018. 2
and Stefan Leutenegger. InteriorNet: Mega-scale multi-
[58] Rene´ Ranftl, Katrin Lasinger, David Hafner, Konrad
sensorphoto-realisticindoorscenesdataset.InBVMC,2018.
Schindler, and Vladlen Koltun. Towards robust monocular
7
depthestimation:Mixingdatasetsforzero-shotcross-dataset
[41] AmyLin,JasonYZhang,DevaRamanan,andShubhamTul-
transfer. TPAMI,2020. 5
siani. RelPose++: Recovering 6d poses from sparse-view
[59] Rene´ Ranftl,AlexeyBochkovskiy,andVladlenKoltun. Vi-
observations. arXivpreprintarXiv:2305.04926,2023. 1
siontransformersfordenseprediction. InICCV,2021. 7
[42] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Si-
[60] Carolina Raposo, Miguel Lourenc¸o, Michel Antunes, and
monLucey. BARF:Bundle-adjustingneuralradiancefields.
JoaoPedroBarreto. Plane-basedodometryusinganRGB-D
InICCV,2021. 1
camera. InBMVC. 5
[43] PhilippLindenberger,Paul-EdouardSarlin,andMarcPolle-
[61] E. Riba, D. Mishkin, D. Ponsa, E. Rublee, and G. Brad-
feys. LightGlue:LocalFeatureMatchingatLightSpeed. In
ski. Kornia: anopensourcedifferentiablecomputervision
ICCV,2023. 1
libraryforPyTorch. InWACV,2020. 5,12
[44] HChristopherLonguet-Higgins. Acomputeralgorithmfor
[62] ChrisRockwell, JustinJohnson, andDavidFFouhey. The
reconstructing a scene from two projections. Nature, 293
8-pointalgorithmasaninductivebiasforrelativeposepre-
(5828):133–135,1981. 4
dictionbyViTs. In3DV,2022. 1,2,3,5,6,7,8,13,19
[45] David G Lowe. Distinctive image features from scale-
[63] BarbaraRoessleandMatthiasNießner.End2Endmulti-view
invariantkeypoints. IJCV. 2,8
feature matching with differentiable pose optimization. In
[46] JiasenLu,DhruvBatra,DeviParikh,andStefanLee. ViL-
ICCV,2023. 2
BERT: Pretraining task-agnostic visiolinguistic representa-
tionsforvision-and-languagetasks. NeurIPS,32,2019. 3 [64] Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary
[47] Jiri Matas, Ondrej Chum, Martin Urban, and Toma´s Pa- Bradski. ORB: An efficient alternative to SIFT or SURF.
jdla. Robustwide-baselinestereofrommaximallystableex- InICCV,2011. 2
tremal regions. Image and vision computing, 22(10):761– [65] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,
767,2004. 2 and Andrew Rabinovich. SuperGlue: Learning feature
[48] Piotr Mirowski, Andras Banki-Horvath, Keith Anderson, matching with graph neural networks. In CVPR, 2020. 1,
Denis Teplyashin, Karl Moritz Hermann, Mateusz Mali- 2,3,5,6,7
nowski, Matthew Koichi Grimes, Karen Simonyan, Koray [66] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets,
Kavukcuoglu, Andrew Zisserman, et al. The StreetLearn Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia
environmentanddataset. arXivpreprintarXiv:1903.01292, Liu,VladlenKoltun,JitendraMalik,etal. Habitat: Aplat-
2019. 7 formforembodiedAIresearch. InICCV,2019. 5
[49] Dmytro Mishkin, Jiri Matas, Michal Perdoch, and Karel [67] JohannesLSchonbergerandJan-MichaelFrahm. Structure-
Lenc. WxBS:Widebaselinestereogeneralizations. BMVC, from-motionrevisited. InCVPR,2016. 1
2015. 2 [68] Samarth Sinha, Jason Y Zhang, Andrea Tagliasacchi, Igor
[50] Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan D Gilitschenski, and David B Lindell. SparsePose: Sparse-
Tardos. ORB-SLAM: a versatile and accurate monocular viewcameraposeregressionandrefinement.InCVPR,2023.
SLAMsystem. T-RO. 1 2
10[69] Leslie N Smith and Nicholay Topin. Super-Convergence:
Very fast training of neural networks using large learning
rates. In Artificial intelligence and machine learning for
multi-domainoperationsapplications,2019. 15
[70] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and
XiaoweiZhou.LoFTR:Detector-freelocalfeaturematching
withtransformers. InCVPR,2021. 1,2,3,5,6,7,8,13,19
[71] BinTan,NanXue,TianfuWu,andGui-SongXia. NOPE-
SAC:Neuralone-planeRANSACforsparse-viewplanar3d
reconstruction. TPAMI,2023. 2,5,6
[72] Zachary Teed and Jia Deng. DROID-SLAM: Deep vi-
sual SLAM for monocular, stereo, and RGB-D cameras.
NeurIPS,2021. 2
[73] ZacharyTeed,LahavLipson,andJiaDeng. Deeppatchvi-
sualodometry. arXivpreprintarXiv:2208.04726,2022. 2
[74] Philip HS Torr and Andrew Zisserman. MLESAC: A new
robustestimatorwithapplicationtoestimatingimagegeom-
etry. Computervisionandimageunderstanding,78(1):138–
156,2000. 4
[75] Jianyuan Wang, Christian Rupprecht, and David Novotny.
PoseDiffusion: Solvingposeestimationviadiffusion-aided
bundleadjustment. InICCV,2023. 1
[76] Wenshan Wang, Yaoyu Hu, and Sebastian Scherer. Tar-
tanVO:Ageneralizablelearning-basedVO. InCoRL,2021.
2
[77] Tong Wei, Yash Patel, Alexander Shekhovtsov, Jiri Matas,
andDanielBarath. GeneralizeddifferentiableRANSAC. In
ICCV,2023. 2
[78] Ross Wightman. PyTorch image models. https:
//github.com/rwightman/pytorch-image-
models,2019. 5
[79] Zhenpei Yang, Jeffrey Z Pan, Linjie Luo, Xiaowei Zhou,
KristenGrauman,andQixingHuang. Extremerelativepose
estimationforRGB-Dscansviascenecompletion.InCVPR,
2019. 2
[80] ZhenpeiYang,SimingYan,andQixingHuang.Extremerel-
ativeposenetworkunderhybridrepresentations. InCVPR,
2020. 2
[81] Chao Yu, Zuxin Liu, Xin-Jun Liu, Fugui Xie, Yi Yang, Qi
Wei, and Qiao Fei. DS-SLAM: A semantic visual SLAM
towardsdynamicenvironments. InIROS,2018. 1
[82] JasonYZhang,DevaRamanan,andShubhamTulsiani.Rel-
Pose:Predictingprobabilisticrelativerotationforsingleob-
jectsinthewild. InECCV,2022. 2
[83] ChenZhao,YixiaoGe,FengZhu,RuiZhao,HongshengLi,
andMathieuSalzmann.Progressivecorrespondencepruning
byconsensuslearning. InICCV,2021. 2
[84] YiZhou,ConnellyBarnes,JingwanLu,JimeiYang,andHao
Li. On the continuity of rotation representations in neural
networks. InCVPR,2019. 3,5,8,12
11This Supplement includes additional detail for the intended to be used in combination with Transformer out-
methodandexperiments,aswellasadditionalexperiments put T to form final output. In other words, our goal of
t
andexplanationtoolongforthemainpaper. FAR:UpdatedT istoimproveuponSolverT ,resulting
u s
inbetterfinaloutputaftercombiningwithT .
t
AppendixA.NetworkArchitecture FAR:FullT. Finalpredicted6DoFposeconsistingofthe
weightedlinearcombinationofT andT ,weightedbyw.
t u
We detail the full model along with ablations below as a
ForLoFTRFeatureExtractorandCorrespondenceEsti-
functionoftheircomponents;thesecorrespondtopredicted
mator, we use H = 480,W = 640 and D = 256,h =
pose boxes in Figure 3. Architecture is overviewed in Ta-
60,w = 80, except on Map-free Relocalization experi-
bles6-7anddetailedinTables8-12.
ments, where images are size H = 720,W = 544, so
SolverT . Poseestimationfromacorrespondenceestima-
s usingthesamedownsampling,h=90,w =68. ForSuper-
torfollowedbyaKornia[61]implementationofRANSAC
Glue Correspondence Estimator, we use H = 480,W =
+5-PointAlgorithm,optionallyscaledbypredictedtransla-
640. For 8-Point ViT Feature Extractor (InteriorNet and
tionscale. ComparetoFAR:FullT,thisablationdoesnot
StreetLearn experiments), we use H = 224,W = 224
use the Transformer, nor combine Solver and Transformer
and D = 192,h = 24,w = 24. For 6D Reg Feature
predictions, nor do a second round of prior-guided solver
Extractor (Map-free Relocalization experiments), we use
andcombiningwithTransformer.
H = 360,W = 270andD = 256,h = 12,w = 9. Map-
We refer to this as “Solver” if it uses perturbed ground
free Relocalization setup differs slightly from other setups
truthcorrespondencesasinput(Figure4),meaningnocor-
to use 6D Reg features as input rather than LoFTR or 8-
respondence estimator is used. We refer to this as Corr. +
PointViT,and6DRegproducesasinglefeaturevectorfor
Solver in experiments if correspondence estimator is used
apairofimagesratherthantwo;fordetailsseeSectionC.4.
(Figure6and 8). WerefertoitasCorr. +Solver+Scaleif
InTable8,webreakdownthearchitectureofTransformer
predictedscaleisusedtoevaluateabsolutetranslationerror T . 8-PointViToutputhasd = D/n +p = 70, where
t h e
(Figures1,2,7;Table2);werefertoitasLoFTR+Solver n = 3 is the number of heads, and p = 6 is the size of
h e
+ScaleifLoFTRisused(Table2).
positionalencodings.
PredictedscaleforSolverT istheoutputoftheTrans-
s
lationScalePredictornetworkdetailedinTable12. Ittakes
AppendixB. Prior-Guided Robust Pose Esti-
densefeaturesf asinputandoutputsasinglescalar,which
mator
is multiplied by translation angle output from RANSAC +
5-Pointtoobtainfinaltranslation. Earlyinexperiments,we
In our implementation of prior guided pose estimation we
used a transformer-based architecture to predict scale, but
use RANSAC as the solver to search over the hypothesis
foundthisCNN-basedmethodperformedbetter.
space and also score our models with inlier scores. We
FAR:TransformerT t. Predicted6DoFposefromFAR’s use the five-point algorithm to estimate the Essential Ma-
Transformer. Compare to FAR: Full T, this ablation does trix [25]. Choosing the five-point algorithm is beneficial
not use the Solver, nor combine Solver and Transformer inthecaseofknownintrinsics(availableinalldatasetswe
predictions, nor do a second round of prior-guided solver use)asitonlyrequires5correspondencestoestimateamin-
andcombiningwithTransformer. imalmodel. Thisincreasesthechanceofsamplingabetter
Inthecasedensefeaturesareavailable,the8-PointViT hypothesisHovermultipleRANSACiterations. Thefive-
isused, ifonlycorrespondencesplusdescriptorsareavail- pointalgorithmrecoverstheessentialmatrixcorresponding
able,theVanillaTFisused. EachisdetailedinTable8and toaminimalset(5)andweconvertthistoatranslationand
Table9,respectively. rotationmatrix(uptoscale).
FAR: One Round T 1. Predicted 6DoF pose from one Prior Probability. The β(H|T 1) measures the log prob-
round of FAR, which consists of the weighted linear com- ability of the hypothesized model H under T . The H is
1
binationin6D[84]spaceofT t andT s,weightedbyw as theessentialmatrixandT 1isthe6Dtransformationmatrix
describedinEquation1. ComparetoFAR:FullT,thisab- from round of prediction. Since it is difficult to measure
lationdoesnotdoasecondroundofprior-guidedsolverand theprobabilityofHunderT wedesignaproxyformula-
1
combiningwithTransformer. tion. We simplify the formulation with by computing the
FAR: Updated T . Pose estimation from FAR’s prior- impliedtransforms{T }2 correspondingtoeachof
u {H,k} k=1
guidedRANSAC+5-PointAlgorithm,usingT asaprior. twopossiblesolutionsfortherotationmatrix.
1
ComparetoFAR:FullT,thisablationdoesnotdoasecond There are multiple possible ways to measure the prob-
round of combining with the Transformer. Note: results ability of the transform T under T, one possible solu-
H,k
fromFAR:UpdatedT tendtobelessaccuratethanFAR: tionistoindependentlymeasurethedistributionforrotation
u
One Round T . This is expected, as FAR: Updated T is andtranslationcomponent.Thisapproachhoweverrequires
1 u
12Table6. ModelArchitecture: FAR.High-levelfirstdefined, followedbydetailedcomponents. N variesdependingonthenumberof
correspondences. ForLoFTRFeatureExtractorandCorrespondenceEstimator,weuseH =480,W =640,D=256,h=60,w=80.
Variablesforalternativeexperimentsdescribedintext.
Overview
Operation Inputs Outputs OutputShape
InputImage - - 2×3×H ×W
FeatureExtractor InputImage f ,f 2×D×h×w
i j
CorrespondenceEstimator InputImage M N ×4
8-PointViTT f ,f T ,w 9,2
t i j t
SolverT M T 9
s s
OneRoundT T ,T ,w T 9
1 s t 1
UpdatedT M,T T 9
u 1 u
FullT T ,T ,w T 9
u t
Table7.ModelArchitecture:FAR(VanillaTF).
Overview
Operation Inputs Outputs OutputShape
InputImage - - 2×3×H ×W
CorrespondenceEstimator InputImage M N ×4
VanillaTransformerT M T ,w 9,2
t t
SolverT M T 9
s s
OneRoundT T ,T ,w T 9
1 s t 1
UpdatedT M,T T 9
u 1 u
FullT T ,T ,w T 9
u t
Table8.ModelArchitecture:TransformerT (8-PointViT).
t
Overview
Operation Inputs Outputs OutputShape
InputFeatures - f ,f 2×D×h×w
i j
LoFTR[70]Self-Attn. Block f ,f f ,f 2×D×h×w
i j i,1 j,1
LoFTRCross-Attn. Block f ,f f ,f 2×D×h×w
i,1 j,1 i,2 j,2
8-PointViT[62]Cross-Attn. Block f ,f f 2×D×d
i,2 j,2 o
RegressionMLP f T 9
o t
GatingMLP f w 2
o
Table 9. Model Architecture: Transformer T (Vanilla TF) Correspondences optionally include descriptors. If do not, skip Linear
t
Layer,useonlyPositionalEncodingasinputtoVanillaTransformer.
Overview
Operation Inputs Outputs OutputShape
InputCorr. - M N ×2×2
InputDescriptor - M N ×2×256
d
PositionalEncoding M f N ×384
pos
LinearLayer M f N ×128
d in
VanillaTransformer f ,f f N ×512
pos in tmp
GlobalAvg. Pooling f f 512
tmp o
RegressionMLP f T 9
o t
GatingMLP f w 2
o
13Table10.ModelArchitecture:RegressionMLP.
Overview
Operation Inputs Outputs OutputShape
InputFeatures - f shape(f )
o o
Linear f f 512
o tmp0
ReLU f f 512
tmp0 tmp1
Linear f f 512
tmp1 tmp2
Linear f f 512
tmp2 tmp3
ReLU f f 512
tmp3 tmp4
Linear f T 9
tmp4 t
Table11.ModelArchitecture:GatingMLPShapeoff is512inthecaseofVanillaTransformerandDinthecaseof8-PointViT.
o
Overview
Operation Inputs Outputs OutputShape
InputFeatures - f shape(f )
o o
InputTransformerPredictedPoseT - T 9
t t
InputSolverPredictedPoseT - T 9
s s
InputNumberofSolverInliers - n 3
i
Linear f ,T ,T ,n f 512
o t s i tmp0
ReLU f f 512
tmp0 tmp1
Linear f f 512
tmp1 tmp2
ReLU f f 512
tmp2 tmp3
Linear f w 2
tmp3 tmp
Sigmoid w w 2
tmp
Table12.ModelArchitecture:ScalePredictionNetwork.
Overview
Operation Inputs Outputs OutputShape
FeatureExtractorInput - f ,f 2×D×h×w
i j
MaxPool2D f ,f f ,f 2×D×h/2×w/2
i j i,a j,a
Conv2D f ,f f ,f 2×D/2×h/2×w/2
i,a j,a i,b j,b
ReLU f ,f f ,f 2×D/2×h/2×w/2
i,b j,b i,c j,c
MaxPool2D f ,f f ,f 2×D/2×h/4×w/4
i,b j,b i,c j,c
Conv2D f ,f f ,f 2×D/4×h/4×w/4
i,c j,c i,d j,d
ReLU f ,f f ,f 2×D/4×h/4×w/4
i,d j,d i,e j,e
Conv2D f ,f f ,f 2×D/16×h/16×w/16
i,e j,e i,f j,f
ReLU f ,f f ,f 2×D/4×h/16×w/16
i,f j,f i,g j,g
Linear f ,f f 512
i,g j,g 0
ReLU f f 512
0 1
Linear f f 512
1 2
ReLU f f 512
2 3
Linear f s 1
3
tuningdifferentweightsforeachofthecomponents. Inour pointswithT andT . Wethencomputethesquared
{H,k} 1
case we measure the difference in the two transformation residuals, rN, as distance between the transformed point
i
bycomputingtheimpliedeffectofthetransformationsona sets. Assumingthedistributionofresidualstobeproxyfor
givenpointset. theposeprior,wecannowcomputetheprobabilityofresid-
Specifically, for a randomly sampled point set G ≡
{g}L inR3suchthatg ∈(−3,−3)3.Wetransformthese
l=1 l
14ualsunderastandardGaussiandistributionas, Far: Full T. This is because after full training, T is in-
t
accurate standalone, since it is trained to be effective in
β′(T
,T)=log((cid:89)L
exp(−r2)/Z), (3)
conjunctionwithT s. WereportT
u
andTafterfulltrain-
H,k l ing of T. We use ground truth correspondence computed
l=1 viaLoFTR’scorrespondencealgorithmfromtrueposeand
Z isthenormalizationconstantfortheprobabilitydistribu- depth,whichconsistsofamutualnearestneighborcheck.
tion. WehavetwohypothesiscorrespondingtoeachHso
wechoosesolutionwiththehighestloglikelihoodthatbest
C.2. Wide-Baseline Pose Estimation on Matter-
fitswithpriortorecoverβ(H,T)as,
port3D
(cid:32) (cid:33)
β(H,T)=max β′(T ,T),β′(T ,T) (4) On the full dataset, we found LoFTR reached best perfor-
{H,1} {H,2}
manceafter30epochs,FAR:TransformerT reachedbest
t
performanceafter39epochs,FAR:OneRoundT reached
1
Scoring Function. Using the prior score above now we best performance after 32 epochs, FAR: Full T plateaued
cancombinethiswithourexistingRANSACinliersscoring after 14 epochs. If using the Vanilla Transformer, FAR:
functionbycombiningtheloglikelihoodforthehypothesis TransformerT reachedbestperformanceafter89epochs,
t
HunderTandthelikelihoodofcorrespondencesetM un- FAR: One Round T reached best performance after 69
1
derthehypothesisHas, epochs,FAR:FullTplateauedafter12epochs. Wereport
T afteritstrainingforthereasonsdetailedin C.1. Inad-
(cid:88) (cid:16) (cid:17) t
score(H)=αβ(H,T)+ 1 E(p,q|H)<σ , (5) dition,wereportT outputafterCorrespondenceEstimator
s
(p,q)∈M training for consistency with the Correspondence + Solver
baseline throughout the paper. This has little impact upon
hereσdenotestheinlierthresholdandE(p,q|H)measures resultscomparedtoreportingafterfulltrainingofT.
the Sampson error for correspondences p,q under the es-
sentialmatrixH
C.3.ApproachFlexibility
AppendixC.AdditionalExperimentalDetails
Flexibility to Features and Correspondences. 8-Point
Our typical training procedure is to train the Correspon-
ViTfeaturesreferstospatialfeaturesafterallself-attention
denceEstimator,followedbyFAR:TransformerT jointly
t layersinthe8-PointViTbackbone,sincethecross-attention
withthebackbone, followedbyFAR:OneRoundT , fol-
1 layerin8-PointViTproducesonlyaflattenedarrayoffea-
lowed by FAR: Full T. At each step, we train until val-
tures. Given this input, FAR: Transformer T uses the 8-
t
idation mean rotation error plateaus, and reload the exist-
PointViTvariant. ThisnormallyconsistsofaLoFTRlayer
ing components for the next round of training. In some
followed by 8-Point ViT cross-attention. However, in this
cases different steps are not applicable e.g. we build upon
specialcaseof8-PointViTinput,wedroptheLoFTRlayer
learnedposebackboneinMap-freeRelocalizationandcan-
tomakeFAR:TransformerT equivalenttofull8-PointViT
t
not train the prior on StreetLearn or InteriorNet. We use
output. Thisallowsforclosercomparisontotheoriginal8-
OneCycleLR [69] scheduler, except if using 6DReg back-
Point ViT work, while using a specialized architecture, in
bone; here we follow prior work [2] in using a constant
whichinsertingaLoFTRlayerwouldnotlikelybehelpful.
learning rate. FAR’s Kornia-based solver is slower than
FAR:FullTcanthenuseFAR:TransformerT combined
t
OpenCV, so for speed we use OpenCV solver to compute
withFAR:Updated,withsolveroutputcomingfromeither
T inourfinalmodel. Forfaircomparisoninablations,we
s LoFTRorSuperGlue.
computeT usingKornia.
s
Wefollow8-PointViTtrainingprocedureforthemodel,
C.1.GroundTruthRobustnessStudy trainingfor120kiterationswithbatchsize60,orabout225
epochs. WethenrepeatthisprocedureforFAR:OneRound
In this experiment, we are given correspondences as in-
T givencorrespondencesfromLoFTRorSuperGlue. Fi-
put, so we proceed directly to training FAR: Transformer 1
nally,wetrainfor20kiterationsforFAR:FullT.
T and remaining steps. Training upon perturbed ground
t
truthcorrespondencestypicallyplateausafter90epochsfor DatasetSize. Onthe40%sizeddataset,wefoundLoFTR
FAR: Transformer T and FAR: One Round T ; we find reached best performance after 86 epochs, FAR: Trans-
t 1
10epochsofadditionaltrainingissufficientforFAR:Full formerT reachedbestperformanceafter94epochs,FAR:
t
T. We report T output after FAR: Transformer T train- One Round T reached best performance after 43 epochs,
t t 1
ing rather than after training with the other components in FAR:FullTplateauedafter27epochs.
15Mean Rot. Error vs. Noise Mean Rot. Error vs. Outliers Rot. Error vs. Inliers Solver Wt. vs. Inliers
Solver Ts
30
FAR: One Round T1
40 FAR: Transformer Tt 40 FAR: Full T 0.8
FAR: Updated Tu
30 FAR: Full T 30 20 0.6
20 20 0.4
10
0.2
10 10
0 0.0
0 0 0 100 200 300 400 0 100 200 300 400
0 8 16 32 0 50 75 87.5 Number of Inlier Correspondences
Correspondence Noise (Pixels) Correspondence Percent of Outliers (%)
Figure 10. FAR: Full T vs. FAR: One Round T . After
Figure9. GroundTruthRobustnessStudyonMatterport3D: 1
oneround,FARproduceshigh-qualityresults,makingfurtherim-
MeanRotationError.Usingtruecorrespondence,thesolverhas
provementdifficult. However,asecondforwardpassthroughthe
low mean error, which is nonzero because of some image pairs
solver injected with a prior (FAR: Full T) improves solver es-
having limited ground truth correspondences, leading to small
timates. Correspondingly, the Transformer learns to give more
meanerror. Aswithmedianerror,addingnoiseoroutlierscauses
weighttothesolver(right),andthereisanontrivialimprovement
ittoquicklydegrade,whileprior-guidedUpdatedsolverisrobust
inrotationerrorindifficultcases(left,100-250inliers).
to outliers and Transformer is robust to noise. FAR matches or
beatsallmethodsacrosssettings.
of feasible resolution and feature size for a Transformer.
C.4. Wide-Baseline Pose Estimation on Additional
TheVanillaTransformerislightweight,allowingthistobe
Datasets
added to a light 6D Reg architecture without significantly
impactingrun-timeorbatchsize.
InteriorNet and StreetLearn. We use 8-Point ViT as
our feature extractor on InteriorNet and Streetlearn as it is We begin from a 6D Reg backbone pretrained for 50
SOTA and correspondence-based methods such as LoFTR epochs, train FAR: Transformer T for 20 epochs, train
t
cannot be trained on the data as it does not contain depth. FAR: One Round T for 30 epochs (50 if using Super-
1
WefollowthetrainingsetupofSectionC.3,trainingFAR: Gluecorrespondences; whichrunsfaster),followedbyan-
Transformer T and FAR: One Round T sequentially, other3forFAR:FullT.Correspondencescomefromeither
T 1
reloading at each new phase of training, and following 8- LoFTRorSuperGlue,bothofwhicharepretrainedonout-
Point ViT training schedule for each phase. We cannot doorenvironments. SuperGlueisfasterthanLoFTR,lead-
usethepriorsinceitrequirestranslationprediction, which ingtofasternetworkspeedduringtrainingandmoreitera-
cannot be supervised, since the dataset does not contain tions in the same amount of time. FAR: Full T training is
translation information. Therefore, FAR: Full T is the slower given Kornia solver, so we train for only 3 epochs.
output from FAR: One Round T . However, we find re- Weneverthelessfindthistrainingbeneficial.
1
sultsarestrongevenwithoutprior. OnInteriorNet,weuse
LoFTR pretrained on Matterport3D for correspondences.
On StreetLearn, correspondences come from LoFTR pre-
AppendixD.AdditionalResults
trainedonMegaDepth.
Map-free Relocalization. We use 6D Reg as our feature D.1.Ground-TruthRobustnessExperiment
extractorforsimilarreasonstoInteriorNetandStreetLearn:
6DReghasmostcompetitiverotationandtranslationerrors Figure 9 presents mean rotation errors of methods con-
of existing methods, and correspondence-estimation meth- fronted with ground truth correspondences, with varying
odssuchasLoFTRorSuperGluecannotbetrainedonthe amounts of noise and outliers. It corresponds to Figure 4,
datasetsinceitdoesnotcontaindepth. exceptmeanrotationerrorisreportedhereratherthanme-
6D Reg architecture is different from 8-Point ViT and dian rotation error in Figure 4. The results correspond to
LoFTRinthatitwarpsfeaturestoacommonframebefore thoseinFigure4: thesolverisstrongfacedwithlittlenoise
estimating pose. This setup is distinct from the canonical orfewoutliers,butdegradesseverely.Prior-guidedUpdated
setting of having two dense feature matrices as input, but solver is robust to outliers, while Transformer is more ro-
FAR can nevertheless be adapted. FAR’s Transformer T bust to noise, at the expense of precision. FAR produces
t
takesfeaturesafterthepenultimateResNetlayerof6DReg, the best of both results in either low perturbation or high
whichyieldsfeaturesizeof12×9×256. TheTransformer perturbation. Note solver median errors are 0, but mean
is a Vanilla Transformer consisting of 6 Transformer En- errors are nonzero due to image pairs occasionally having
coderlayerswithfeaturesize256and8heads. Wechoose veryfewgroundtruthcorrespondences,producinghigher-
the penultimate layer as input to the Transformer as these rorsaccordingly. However,thisdoesnotimpacttheexperi-
late features are instructional for predicting pose, and are mentalconclusion.
16
)°(
rorrE
noitatoR
)°(
rorrE
noitatoR
)°(
rorrE
noitatoR
thgieW
revloS
detciderPMean Rot. Error vs. Time Mean Tr. Error vs. Time C+S is an abbreviation for “Correspondence Estimation
40 1.6
LoFTR (Kornia) + Solver”, specifically LoFTR with a solver, and learned
20 SuperGl 8u -e Point ViT LoFTR (cv2) 8-Point ViT scale if necessary. L-B is an abbreviation for “Learning-
0.8
FAR Tt LoFTR (Kornia) FAR Tt Based”,inpracticeweuseLoFTRwithan8-PointViThead
10
LoFTR (cv2) FAR T1 (cv2) FAR T (Kornia)0.4 for Matterport3D (specifically, this is equivalent to FAR:
5 FAR T1 (cv2) FAR T (Kornia) Transformer T t), we use 8-Point ViT for InteriorNet and
0 0 StreetLearn, and use 6D Reg for Map-free Relocalization.
0 0.025 0.05 0.1 0.2 0.4 0 0.025 0.05 0.1 0.2 0.4
Inference Time (Sec/It, 10 1080Ti GPUs) Wechoosetheselearning-basedandcorrespondence-based
comparisons as they are the state of the art and we build
Figure11. EfficiencyonMatterport(LogScalevs. LogScale).
uponthemforourmethod: onalldatasets, weuseLoFTR
The efficient frontier includes LoFTR+Solver, T and T. FAR
1 toextractcorrespondence;onMatterport3D,weuseLoFTR
T isstrictlybetterthan8-PointViT.FART cutserroralmostin
t 1
forfeatures,onInteriorNetandStreetLearnweuse8-Point
halfforlittletimecost. Timprovesfurther,butisslowerdueto
ViTforfeatures,andonMap-freeRelocalizationweuse6D
Prior-GuidedRANSACKorniaimplementation. Korniasimilarly
slowsdownLoFTR+Solver.T withKornia(notpictured)isalso Regforfeatures.
1
worsethanT,whilebeingslower. Randomresultsgivevisualgroundingtoquantitativere-
sultsfromSection4andareconsistentwithconclusionsthat
FAR outperforms both C+S and L-B. Only 14 results are
D.2.FAR:Fullvs. FAR:OneRound
presentedoneachdataset,meaningthesamplesizeissmall,
Figure 10 displays an analysis of FAR: Full T vs. FAR: andconclusionsshouldnotbedrawnfromaggregatenum-
OneRoundT . Thedistinctionbetweenthesebaselinesis bers. Rather, these examples are intended to be indicative
1
highlightedinFigure3,whichisthatFAR:FullTaddsan ofperformanceonasample-by-samplebasis.
additionalforwardpasstothesolver,thistimeinjectedwith Forinstance,onMatterport3D,FARisbest10outof14
the prior. Like FAR: One Round T 1, this is followed by timesinrotationand7outof14timesintranslationerror.
combinationwithTransformerpredictions. Inaddition,whenitisnotbest,ittypicallyisbetterthanone
Despitethetwovariantsofthemethodbeingsimilar,and ofC+SorL-Bandistypicallynotsignificantlyworsethan
results of FAR: One Round T 1 being highly competitive, thebestmethod. Thetwoqualitiesthatitisoftenbestand
FAR: Full T yields improvement. This is apparent in the rarelyworstisinlinewithsignificantlybetterperformance
caseof100-250inliers,wherethepriorimprovessolverout- thanpriorworkaveragedoverafulltestset.
put,causingtheTransformertorelyonitmore(Figure10,
RandomMap-freeRelocalizationresultsalsoagreewith
right)andthefullmodeltoimprove(Figure10,left).
conclusions from Section 4 that FAR is strong. FAR has
NoteFAR:FullThasdifferentweightingswthanFAR:
best rotation and translation 7 out of 14 times; while rival
One Round T . This is because FAR: Full T is trained to
1 L-Bwins2timesinrotationand3timesintranslation;C-S
predictfinaloutputgivenprior-guidedsolveroutput. Since
wins5timesinrotationand7timesintranslation. Despite
prior-guided solver output is more accurate than vanilla
strongperformancesomeofthetime,recallfromSection4
solveroutput,thenetworklearnstorelyuponitmoreheav-
C-SerrorissignificantlyhigheronaveragethanFAR.This
ily. For fair comparison with FAR: Full T, we finetune
isshowcasedintherandomresults: whenC-Sfails,itdoes
FAR: One Round T using a Kornia solver for an equal
1 sospectacularly, forinstancewithrotationerrorofatleast
numberofepochsusedtofinetuneFAR:FullT;beforeus-
120 degrees on 5 occasions, compared to 0 for FAR. To
ing the Kornia solver during inference. This is necessary
summarize,inlinewithquantitativeresultsfromSection4,
because,asdetailedin C,FAR:FullTusescv2’sunbiased
in random samples FAR tends to be significantly more ro-
solverduringthefirstroundofcomputationforefficiency.
bustthanC-S,whileproducingbestresultsfrequently. L-B
is also more robust than C-S, but rarely produces best re-
D.3.InferenceTimeEfficiencyComparison
sults.
We plot error vs. time in Figure 11. FAR is on the ef- Random results on InteriorNet also are consistent with
ficient frontier (down and left), though it is slower than the paper’s findings. FAR has lowest error in 7 of 14 oc-
LoFTR+Solver using OpenCV (cv2). We note a Prior- casions, vs. 5 for L-B and 6 for C+S. However, the high-
GuidedRANSACimplementationincv2couldspeedFAR est error for FAR is 4.2◦, while L-B hits 4.9◦ and C-S has
uptowards15fps(e.g. T1). 14.5◦. OnStreetLearn,FARhasamaximumerrorof4.4◦,
whileL-Bhaserrorsupto8.2◦ andC+Shaserrorsof124◦
D.4.RandomResults
and 177◦. FAR has lowest error in 8 instances, vs. 3 for
Results on random examples are presented in Figures 12- C+Sand5forL-B.WhenFARbeatsL-B,itisoftenbetter
15. bymultipledegrees(upto6,Figure15,bottomleft),while
ThecomparisonsaretothesamebaselinesasinFigure8. when L-B bests FAR, it is typically by less than three de-
17
)°( rorrE
noitatoR
)m(
rorrE
noitalsnarTgrees. Tosummarize,randomresultselucidateFARisboth
preciseandrobust.
Note results on Map-free Relocalization here, as well
as Figure 8, are on the validation set, since the test
set ground truth is private – test results are available
fromsubmittingpredictionsthroughtheMap-freeRelocal-
ization website (https://research.nianticlabs.com/mapfree-
reloc-benchmark/submit). Otherwise we use test sets for
randomresults.
D.5.FailureCases
Wecanconsidersomefailuresintherandomexamplesfrom
Figures12-15.Forinstance,someexamplesinMap-freeare
beyondthecapacityofallthetestedmodels: row1column
2 has all models with error above 60◦. This is a case of
a large rotation around a symmetric and unusually-shaped
object,somuchsoitmightbeinitiallychallengingtoahu-
man. This is a case where recent work focused on visual
disambiguation[11]couldbeofassistance.
Occasionally,FARalsoproducestheworstresultscom-
paredtoC+SandL-B,forinstanceinMap-freeresultsrow
2column4. Ideally,itwouldperformatleastaswellasthe
bestofC+SandL-Bonanyinstance,butthisisevidenceit
isnotalwaysbetterthanonealternative.
18Random Results: Matterport3D
Rot Mag: 32.5º Rot Mag: 37.0º Rot Mag: 41.3º Rot Mag: 12.8º Rot Mag: 78.6º Rot Mag: 82.7º Rot Mag: 79.3º
C+S: 0.3º C+S: 0.3º C+S: 0.3º C+S: 0.1º C+S: 0.5º C+S: 73.4º C+S: 0.6º
L-B: 0.3º L-B: 2.8º L-B: 0.3º L-B: 2.3º L-B: 2.2º L-B: 2.6º L-B: 8.2º
FAR: 0.2º FAR: 0.0º FAR: 0.1º FAR: 1.3º FAR: 0.3º FAR: 3.2º FAR: 0.1º
Tr Mag: 2.0m Tr Mag: 2.1m Tr Mag: 0.8m Tr Mag: 2.6m Tr Mag: 1.9m Tr Mag: 0.4m Tr Mag: 3.8m
C+S: 0.3m C+S: 0.7m C+S: 0.6m C+S: 1.1m C+S: 0.4m C+S: 1.0m C+S: 2.5m
L-B: 2.8m L-B: 0.3m L-B: 0.1m L-B: 0.7m L-B: 0.2m L-B: 0.3m L-B: 1.0m
FAR: 0.5m FAR: 0.2m FAR: 0.2m FAR: 0.6m FAR: 0.3m FAR: 0.3m FAR: 0.8m
Rot Mag: 68.9º Rot Mag: 76.7º Rot Mag: 99.2º Rot Mag: 48.6º Rot Mag: 31.2º Rot Mag: 1.5º Rot Mag: 4.9º
C+S: 0.3º C+S: 0.5º C+S: 17.0º C+S: 0.1º C+S: 0.5º C+S: 0.1º C+S: 1.8º
L-B: 8.9º L-B: 2.9º L-B: 5.3º L-B: 12.2º L-B: 4.2º L-B: 8.0º L-B: 7.2º
FAR: 0.7º FAR: 0.2º FAR: 1.5º FAR: 0.1º FAR: 0.1º FAR: 0.4º FAR: 1.0º
Tr Mag: 0.5m Tr Mag: 2.1m Tr Mag: 1.9m Tr Mag: 1.5m Tr Mag: 1.1m Tr Mag: 2.4m Tr Mag: 2.3m
C+S: 0.9m C+S: 0.5m C+S: 0.2m C+S: 0.1m C+S: 0.6m C+S: 1.7m C+S: 1.8m
L-B: 0.1m L-B: 0.1m L-B: 0.3m L-B: 0.9m L-B: 0.4m L-B: 0.4m L-B: 0.3m
FAR: 0.0m FAR: 0.2m FAR: 0.3m FAR: 0.3m FAR: 0.2m FAR: 0.6m FAR: 0.3m
Figure12. RandomresultsonMatterport3D.C+S:LoFTR[70]+Solver. L-B:LoFTR+8-PointViT[62]. FAR:usesLoFTRfeatures
andcorrespondences.FARistypicallybest.Whennotbest,itisusuallybetterthanoneofC+SorL-B.
19Random Results: Map-free
Rot Mag: 16.1º Rot Mag: 139º Rot Mag: 113º Rot Mag: 7.7º Rot Mag: 8.2º Rot Mag: 12.8º Rot Mag: 138º
C+S: 0.9º C+S: 169º C+S: 120º C+S: 1.3º C+S: 0.3º C+S: 1.1º C+S: 161º
L-B: 4.1º L-B: 145º L-B: 33.5º L-B: 4.0º L-B: 5.2º L-B: 12.9º L-B: 42º
FAR: 0.5º FAR: 67.2º FAR: 28.1º FAR: 1.2º FAR: 1.0º FAR: 23.3º FAR: 21.6º
Tr Mag: 1.0m Tr Mag: 4.3m Tr Mag: 8.6m Tr Mag: 0.2m Tr Mag: 0.6m Tr Mag: 0.5m Tr Mag: 4.2m
C+S: 0.0m C+S: 4.7m C+S: 7.0m C+S: 0.1m C+S: 0.1m C+S: 0.1m C+S: 4.0m
L-B: 0.2m L-B: 5.4m L-B: 6.6m L-B: 0.2m L-B: 0.2m L-B: 1.4m L-B: 2.9m
FAR: 0.1m FAR: 4.0m FAR: 5.7m FAR: 0.1m FAR: 0.3m FAR: 1.3m FAR: 2.3m
Rot Mag: 164º Rot Mag: 39.6º Rot Mag: 46.6º Rot Mag: 7.1º Rot Mag: 27.5º Rot Mag: 93.0º Rot Mag: 8.5º
C+S: 167º C+S: 27.6º C+S: 18.4º C+S: 21.5º C+S: 5.3º C+S: 166º C+S: 0.1º
L-B: 11.6º L-B: 12.2º L-B: 6.9º L-B: 34.0º L-B: 60.7º L-B: 169º L-B: 1.2º
FAR: 20.9º FAR: 20.3º FAR: 2.6º FAR: 38.1º FAR: 67.4º FAR: 38.3º FAR: 0.1º
Tr Mag: 4.5m Tr Mag: 3.1m Tr Mag: 3.1m Tr Mag: 5.4m Tr Mag: 1.8m Tr Mag: 1.7m Tr Mag: 0.8m
C+S: 5.5m C+S: 2.5m C+S: 1.5m C+S: 5.4m C+S: 1.2m C+S: 4.8m C+S: 0.1m
L-B: 1.0m L-B: 0.7m L-B: 0.6m L-B: 7.1m L-B: 3.9m L-B: 0.9m L-B: 0.1m
FAR: 0.9m FAR: 1.7m FAR: 0.3m FAR: 7.6m FAR: 2.8m FAR: 1.8m FAR: 0.1m
Figure13.RandomresultsonMap-freeRelocalization.C+S:LoFTR+Solver.L-B:6DReg[2].FAR:uses6DRegfeaturesandLoFTR
correspondences. FARisoftenbest,havingminimumrotationerror7instancesvs. 5forC+Sand2forL-B,andminimumtranslation
error7timesvs.7forC+Sand3forL-B.C-ShasfarworseerrorsinfailurecasesthanFAR(e.g.row1column7).
20Random Results: InteriorNet
Rot Mag: 65.0º Rot Mag: 29.7º Rot Mag: 59.8º Rot Mag: 26.8º Rot Mag: 36.4º Rot Mag: 26.6º Rot Mag: 10.8º
C+S: 4.3º C+S: 0.1º C+S: 8.7º C+S: 1.1º C+S: 4.3º C+S: 2.6º C+S: 0.3º
L-B: 1.4º L-B: 2.1º L-B: 4.1º L-B: 1.2º L-B: 3.4º L-B: 1.5º L-B: 1.9º
FAR: 4.2º FAR: 0.3º FAR: 2.3º FAR: 0.7º FAR: 0.9º FAR: 1.5º FAR: 0.3º
Rot Mag: 55.6º Rot Mag: 60.7º Rot Mag: 44.8º Rot Mag: 44.6º Rot Mag: 29.1º Rot Mag: 18.0º Rot Mag: 36.7º
C+S: 0.5º C+S: 14.5º C+S: 1.9º C+S: 7.0º C+S: 0.2º C+S: 0.5º C+S: 0.5º
L-B: 4.9º L-B: 3.3º L-B: 1.0º L-B: 3.4º L-B: 0.8º L-B: 0.5º L-B: 3.7º
FAR: 2.6º FAR: 4.2º FAR: 1.3º FAR: 1.6º FAR: 0.4º FAR: 0.5º FAR: 1.0º
Figure 14. Random results on InteriorNet. C+S: LoFTR + Solver. L-B: 8-Point ViT. FAR: uses 8-Point ViT features and LoFTR
correspondences. FARhasthelowesterrormorefrequentlythanalternatives,andhasthelowestmaximumerror: 4.2◦ vs. 4.9◦ forL-B
and14.5◦forC-S.
21Random Results: StreetLearn
Rot Mag: 23.1º Rot Mag: 42.3º Rot Mag: 53.3º Rot Mag: 71.9º Rot Mag: 86.1º Rot Mag: 85.2º Rot Mag: 48.0º
C+S: 1.3º C+S: 28.2º C+S: 53.7º C+S: 64.0º C+S: 124º C+S: 78.8º C+S: 23.3º
L-B: 5.4º L-B: 3.1º L-B: 0.9º L-B: 6.1º L-B: 2.4º L-B: 3.3º L-B: 0.8º
FAR: 1.2º FAR: 2.5º FAR: 2.0º FAR: 1.2º FAR: 1.2º FAR: 1.7º FAR: 3.2º
Rot Mag: 50.5º Rot Mag: 71.4º Rot Mag: 37.7º Rot Mag: 86.0º Rot Mag: 58.7º Rot Mag: 51.4º Rot Mag: 79.9º
C+S: 62.3º C+S: 3.2º C+S: 0.6º C+S: 78.0º C+S: 4.6º C+S: 1.5º C+S: 177º
L-B: 8.2º L-B: 2.3º L-B: 6.1º L-B: 2.5º L-B: 1.3º L-B: 3.4º L-B: 2.3º
FAR: 1.9º FAR: 1.6º FAR: 1.0º FAR: 4.4º FAR: 1.3º FAR: 1.0º FAR: 3.4º
Figure 15. Random results on StreetLearn. C+S: LoFTR + Solver. L-B: 8-Point ViT. FAR: uses 8-Point ViT features and LoFTR
correspondences. FARoftenhasthelowesterror–here8timesvs. 1forC+Sand5forL-B;andismorerobustthanalternatives: FAR
hasmaximumerrorof4.4◦,L-Bhasmaximumerrorof8.2◦,C+Shaserrorsof124◦and177◦.WhenFARbeatsL-B,itisoftenbetterby
multipledegrees(upto6),whilewhenL-BbestsFAR,itistypicallybylessthanthreedegrees.
22