Towards General Computer Control: A Multimodal
Agent for Red Dead Redemption II as a Case Study
WeihaoTan2∗,ZiluoDing1,WentaoZhang2,BoyuLi1,BohanZhou3∗,JunpengYue3∗,
HaochongXia2,JiechuanJiang3,LongtaoZheng2,XinrunXu1,YifeiBi1,PengjieGu2,
XinrunWang2,BörjeF.Karlsson1,BoAn2,ZongqingLu3,1†
1BeijingAcademyofArtificialIntelligence(BAAI),China
2NanyangTechnologicalUniversity,Singapore
3SchoolofComputerScience,PekingUniversity,China
weihao001@e.ntu.edu.sg zongqing.lu@pku.edu.cn
Everyday Professional Digital
Software Software Games
Self-Reflection Information
Gathering
Task Inference Memory
Skill Curation Action
Planning
Figure1:TheCRADLEframeworkempowersnascentfoundationmodelstoperformcomplexcomputertasks
viathesamegeneralinterfacehumansuse:e.g.,screenasinputandkeyboard&mouseoperationsasoutput.
Abstract
Despitethesuccessinspecifictasksandscenarios,e.g.,Minecraft,existingfounda-
tionagents,i.e.,agentempoweredby(multimodal)LLMsandadvancedtools,still
cannotgeneralizeacrossdifferentscenarios,mainlyduetodramaticdifferences
oftheobservationsandactionsbetweenscenarios. Inthiswork,weproposethe
General Computer Control (GCC) setting: building foundation agents that can
masteranycomputertaskbytakingonlyscreenimages(andpossiblyaudio)ofthe
computerasinput,andproducingkeyboardandmouseoperationsasoutput,similar
tohuman-computerinteraction. ThemainchallengesofachievingGCCarei)the
multimodalobservationsforthedecisionmaking,ii)therequirementsoftheaccu-
ratecontrolofthekeyboardandmouse,iii)thenecessitiesoflong-termmemory
andreasoning,andiv)theabilitiesforefficientexplorationandself-improvementof
theagent. TotargetGCC,weintroduceCRADLE,anagentframeworkwithstrong
reasoningabilities,includingself-reflection,taskinference,andskillcuration,to
ensuregeneralizabilityandself-improvementacrossvarioustasks. Todemonstrate
thecapabilitiesofCRADLE,wedeployitinthecomplexAAAgameRedDead
RedemptionII,servingasapreliminaryattempttowardsGCCwithachallenging
target. Toourbestknowledge,ourworkisthefirsttoenableLMM-basedagentsto
followthemainstorylineandfinishrealmissionsincomplexAAAgames,with
minimalrelianceonpriorknowledge.1
∗WorkperformedwhileaninternatBAAI.
†Correspondingauthor.
1Codebaseisopen-sourcedat: https://github.com/BAAI-Agents/Cradle;Projectwebsite: https:
//baai-agents.github.io/Cradle/
4202
raM
5
]IA.sc[
1v68130.3042:viXra1 Introduction
Achieving Artificial General Intelligence (AGI) has long been a north-star goal for the AI com-
munity [29]. Recent foundation agents, i.e., agents empowered by (multimodal) large models
and advanced tools, have been touted as a promising approach in pursuing AGI. While recent
research has demonstrated the success of such agents in specific scenarios or tasks, including
webbrowsing[9,14,16,46,47], operatingmobileapplications[43], craftingandexplorationin
Minecraft[35,38],andsomeroboticsscenarios[6,10,18];currentfoundationagentsstillcannot
generalizeacrossdifferenttargets,mainlyduetodifferentobservationandactionspaces,environment
dynamics(e.g.,Minecraftandrobotics),dependenceontarget-specificresources,orothersemantic
gaps(e.g.,,lackofenvironmentfeedbackforactions)[41].
Computers2arethemostimportantanduniversalinterfaceintheincreasinglydigitalworld.Computer
taskscoverawidevarietyofscenarios,includingcomplexsoftwaretocreatedigitalartifacts,e.g.,
Photoshop,everydayproductivitysoftware,e.g.,spreadsheetsandwordprocessors,appsandwebsites
witheffectsintherealworld,e.g.,bankingortravelbookingapps,videogames,e.g.,TheSims4
and Red Dead Redemption II (RDR2), and control of external devices, e.g., 3D printing devices
andnetworkrouters. Byprovidingstandardizeduniversalobservation(i.e.,screenandaudio)and
abstractactions(i.e.,keyboardandmouseoperations),computercontrolisanidealtestbedtodevelop
foundationagentsforvariedcomplextasksindynamicscenariosandthushavethepotentialtounlock
insightsonthepathtoAGI.Therefore,weproposetheGeneralComputerControl(GCC)setting:
BuildingfoundationagentsthatcanmasterANYcomputertask,e.g.,software,games,etc.,
throughonlystandardobservation(i.e.,screenandaudio)andinputdeviceoperations(i.e.,
keyboardandmouse).
TherearemanychallengestoachieveGCC.i)observationsinGCCaremultimodal,whichrequires
the alignment of the data in different modalities for better understanding and decision-making.
ii)GCCrequiresaccuratecontrolofdeviceoperations(keyboardandmouse)tointeractwiththe
computer. iii)Itrequireslong-termmemorytostorepastexperiencesduetothepartialobservability
ofGCCtasksandreasoningabilityoverexperiencestoreusetheknowledgetosolvenoveltasks.
iv)GCCrequiresefficientexplorationoftheenvironmentsinastructuredmannertodiscoverbetter
strategiesandsolutionsautonomously,i.e.,self-improving,whichallowsagentstogeneralizeacross
themyriadtasksinthedigitalworld. WediscussGCCinmoredetailinSection2.
Inthiswork,weintroduce CRADLE,anovelframeworktargetingGCC,andmakeapreliminary
attempttowardsitwithachallengingtarget. AsshowninFigure1,distinctlyfrompreviousmethods
focusedoncontrollingawebbrowserorsoftwarewitheasyaccesstotheirinternalAPIsandstates[12,
43],CRADLEonlytakesscreenimagesasinputandoutputskeyboardandmouseoperationsthrough
acomprehensivereasoningprocess,enablingittoeffectivelyunderstandthecurrentsituationand
makereasonabledecisions. Furthermore,wedeployCRADLEinthehighlyacclaimedAAAgame3,
RedDeadRedemptionII(RDR2). WeselectRDR2forourcasestudyduetoitscomplexblackbox
controlsystem,whichepitomizesthemostdemandingcomputertasksandenablesustoevaluate
theperformanceboundariesofourframeworkinsuchvirtualenvironments. RDR2ischaracterized
byitsrichanddiverseinformation,encompassingelementslikedialogues,uniqueicons,in-game
prompts,andinstructions,thusrequiringthecaptureandinterpretationofvariousinformationforms.
Additionally, thegamerequiresabroaderrangeofkeyboardandmouseinteractionsthantypical
software,suchasusingthemousefornavigationandcombiningmousebuttonsandkeyboardkeys
interactivelytorealizeactionsinthegameworld(whichnecessitatenotonlyprecisekeyselection,
butalsodeterminationoftiming,andmulti-iterationcombinations). Wearguethatbydemonstrating
thefeasibilityofplayingthiscomplexgame,CRADLEshedslightonitspotentialforGCC.
Ourmajorcontributionsaresummarizedasfollows:
2Throughoutthepaperweusethetermcomputerasasynonymforanyuser-focusedcomputationaldevice,
e.g.,PC,smartphoneandtablet. Whileourdescriptionfocusesonkeyboardandmouseoperations,itcanbe
easilygeneralizedtocontrolhandlesandtouchscreens.
3Inthevideogameindustry,AAA(Triple-A)isatermusedtoclassifygameswithtypicallyhighdevelopment
budgetsandproductionvaluesforthetechnologyavailableintheirtime.
2• WeproposethenovelsettingofGeneralComputerControl(GCC),servingasamilestonetowards
AGIinthedigitalworld,whereagentstakemultimodalobservationsasinputsandoutputkeyboard
andmouseoperations,similartohuman-computerinteractions.
• We propose a novel foundation agent framework (CRADLE) for the GCC setting, which has
strongreasoningabilities,includingself-reflection,taskinference,andskillcuration,toensureits
generalizabilityandself-improvementacrossvariouscomputertasks.
• TodemonstratethecapabilitiesofCRADLE,weincorporatethepowerfulLargeMultimodalModel
(LMM) GPT-4V into our framework and deploy it in the famous AAA game RDR2, serving
as a preliminary attempt towards GCC. To our best knowledge, our work is the first to enable
LMM-basedagentstofollowthemainstorylineandfinishrealmissionsincomplexAAAgames,
withminimalrelianceonpriorknowledge.
2 GeneralComputerControl
Definition. General Computer Control (GCC) is a setting
wheretheagentscontrolthecomputerthroughonlystandard-
izedhuman-likeinteractions,i.e.,thescreenand(optionally)
AGI
audio as inputs and the keyboard and mouse actions as out-
puts.GCCprovidesauniversalinterfacetoanycomputer-based
Video
taskswithoutaccesstothecodesandAPIs. Comparedwiththe Games
robotics[27],anotherpromisingdirectiontowardAGI,GCC Web Desktop
Software Software
providesamorecost-effectiveandcontrollableexperimental
GCC
framework. Duetotheuniversality,achievingGCCisoneof
thesignificantmilestonestowardsAGIinthedigitalworldand
thecombinationofGCCandroboticscanpavethewaytoward
AGIinbothphysicalanddigitalworlds. Figure2:TaxonomyofGCC.
Taxonomy. AsillustratedinFigure2,therearethreemaincategoriesofGCCtargets: i)manipulat-
ingwebsoftware,e.g.,websitesandcloud-basedservices[9],ii)manipulatingtraditionaldesktop
software,e.g.,officesuites[39],andiii)playingvideogames,e.g.,Minecraft[35]. Amongthem,
digitalvideogamesarebelievedtobethemostdifficulttasksmainlyduetoi)environmentcomplexity,
e.g.,theopen-endedworldinRDR2,ii)thelong-horizondecisionmaking(notalwayslinear),e.g.,
1K+semanticstepsinRDR2tasks,andiii)thepartialobservabilityoftasks,i.e.,agentsneedtostore
theirpastexperiencesandreasonuponthemforbetterdecisionmaking. Digitalgameshavebeena
testbedforresearchondecisionmaking,e.g.,Atarigamesinreinforcementlearningresearch[4]. But
complexmoremoderngamespresentamuchmorechallengingtargetforthedevelopmentofthe
foundationagentsduetotheirsimilaritiestovirtualrealityapplicationsandtherealworld.
Challenges. ThereareseveralmainchallengesintargetingGCC.First,observationsinGCCare
multimodal, including images (video frames) like screenshots, text (from the command line or
includedinthescreenshots),andaudio(e.g.,instructions,effects,andmusic). Forexample,when
playingdigitalgamestheagentcommonlyneedstofollowinstructionsordialogue(text/audio)and
interprettheworld(screenimages)tocompletetasks. Second,GCCrequirestheaccuratecontrolof
keyboardandmousetointeractwiththecomputer,wheretheactionspaceofthemouseiscontinuous,
i.e.,locationsonthescreentomoveandvaryingspeedofmovement,andtheactionspaceofthe
keyboardcontrolisdiscrete, butverylarge, consideringcombinationsofkeys, timing, andother
attributes. Furthermore,thesyntaxcontractsforthecodegeneratedbytheagenttoexecutesuchIO
operationsintroduceadditionaldifficulties. Third,theGCCtasksareusuallypartiallyobservable.
For example, the player in RDR2 cannot know the information of towns until he reaches them.
Therefore,GCCrequirestheagenttohavelong-termmemorytostorepastexperiences,suchaspast
actions,outcomes,andtheknowledgelearnedfromthepastexperiences,forfastandbetterdecision
making. Fourth,GCCrequiresefficientexplorationoftheenvironmentsinastructuredmannerand
discoveringbetterstrategiesandsolutionsautonomously,i.e.,self-improving. Theabilitytoexplore
andself-improveenablesagentstogeneralizeacrossvarioustasksinGCCautonomously.
33 RelatedWork
3.1 LLM-basedAgentsforComputerTasks
TheprosperityofLLMshasbroadenedthepotentialofdeployingpowerfulfoundationmodelsas
autonomousagentsforcompletingcomplextasksinvariouscomputerapplications, suchasweb
navigation[9,28,47],softwaremanipulation[21,32,43]andgameplaying[26,35,38,41]. While
previousLLM-basedwebagents[9,14,46,47]showsomepromisingresultsineffectivelynavigating,
understanding, andinteractingwithcontentonwebpages, theyusuallyuserawHTMLcodeand
DOMtreeasinputandinteractwiththeavailableelementIDs,missingtherichvisualpatternswith
keyinformation,likeicons,images,andspatialrelations.
EmpoweredbytheadvancedLMMs,multimodalwebagents[12,16,17,42,45]andmobileapp
agents[36,43]havebeenexplored. InsteadofHTMLsourcecode,theyusuallytakescreenshotsas
input,however,theystillneedtousethebuilt-inAPIstogettheavailableinteractiveelementIDsto
executecorrespondingactions.
Similartowebagents,recentworksattempttodeployLLMagentstovariouscomplexvideogames,
suchasMinecraft[35,38],StarcraftII[26]andCivilization-likegame[31]withtextualobservations
obtainedfrominternalAPIsandpre-definedsemanticactions. Thesedomain-specificsettingsmake
themstrugglewithgeneralizingtoothergames,letalonewebsitesandothersoftware. Although
JARVIS-1[37]claimstointeractwiththeenvironmentinahuman-likemannerwiththescreenshotsas
inputandmouseandkeyboardforcontrol,itsactionspaceispredefinedasahybridspacecomposed
ofkeyboard,mouse,andAPI.
Thoughachievingpromisingresultsinspecifictasks,thesemethodsfailtogeneralizeacrossvarious
tasks,duetotheinconsistentobservationandactionspaces. ThisindicatesthenecessityoftheGCC
setting,whichprovidesaunifiedrepresentationoftheobservationandactionspacesforenormous
challengingcomputertasks.
Concurrentwithourwork,thereareseveralworks[8,13,21,30,39,44]aimingtoscaletheweb
agenttomoreapplicationswithscreenshotsasinputandkeyboardandmouseoperationsasoutput.
However, none of them takes video games into consideration since they mainly focus on static
websitesandsoftware,whichgreatlyreducestheneedfortimelinessandsimplifiesthesettingby
ignoringthedynamicsbetweenadjacentscreenshots,i.e.,animations,andincompleteactionspace
withoutconsideringthedurationofthekeypressedanddifferentmousemode.
3.2 Decision-MakinginVideoGames
Video games are ideal environments for validating agent’s various abilities due to their diversity,
controllability,safety,andreproducibility,whicharealsobelievedtobethemostcomplicatedtasksin
computercontrol. Atarigames[4],SuperMarioBros[22],GoogleResearchFootball[23],StarCraft
II [33, 34], Minecraft [11, 15, 20] etc, have been the popular environments and benchmarks for
reinforcementlearning(RL)agents. Besides,RLagentsalsoexhibitimpressiveperformanceinDota
II[5],QuakeIII[19],GranTurismo[40]andDiplomacy[3]. However,toabstractcomplexcomputer
control, these environments usually simplify the whole action space (i.e., keyboard and mouse
movement)topre-defineddomain-specificactions,whichvaryfromgametogame,exacerbatingthe
poorgeneralizationofRLagentsacrossenvironments.
Discardingthesemanticsintheobservationandactionalsoleadstolowefficiency. LLMsenable
decision-makingagentstoleveragesemanticinformationintheenvironments,whichdramatically
improvesthereasoningabilityofagents,Withoutanytrainingprocess,Voyager[35]canefficiently
learntofinishlong-horizontalcomplextasksthroughcodegeneration. However,itheavilyrelies
onthebuilt-inAPItool,Mineflayer,toobtaininternalinformationandexecutehigh-levelactions,
which are not available in other games. TextStarCraft II [26] and CivRealm [31] also suffer the
same issue. Therefore, closed-source AAA games with rich textual and visual information are
rarelytobeexplored. Pre-trainedwithvideoswithactionlabels,VPT[2]managestooutputmouse
andkeyboardcontrolwithrawscreenshotsasinputwithoutanyadditionalinformation. However,
collectingvideoswithactionlabelsistime-consumingandcostly,whichisdifficulttogeneralizeto
multipleenvironments.
4Self-Reflection Task Inference Skill Curation
Key Screenshots
LMM Inferred Task Add New
LMM R Ref el se uct li to sn &Reasoning LMM Update Skills
Instructions
Last Action & Reasoning ReflectionResults Inferred Task&Reasoning
Last Task & Reasoning Long-term Summary ReflectionResults
Text Info Visual Info Retrieved Skills
1. Navigation 1. UIElements Episodic Procedural
2. Notification 2.Layout Memory Memory Selected Action
3. Instruction 3. Imagery & Reasoning LMM
4.Content 4. Animations Memory
Inferred Tasks & Reasoning
LMM
Last Action & Reasoning
Video (Sequences of Screenshots) Long-term Summary
Information Gathering Action Planning
Figure3:AnoverviewoftheCRADLEframework.CRADLEtakesvideofromthecomputerscreenasinputand
outputscomputerkeyboardandmousecontroldeterminedthroughinnerreasoning.
Inanutshell,toourbestknowledge,therearecurrentlynoagentsundertheGCCsetting,reported
toshowsuperiorperformanceandgeneralizationincomplexvideogamesoracrosscomputertasks.
Inthiswork,wemakeapreliminaryattempttoapplyourframeworktotheepicAAAgameRDR2,
undertheGCCsetting.
4 The CRADLE Framework
TopursueGCC,weproposeCRADLE,illustratedinFigure3,anagentframeworkthatcanproperly
handlethechallengesGCCpresents,i.e.,observingandinteractingwithanyenvironmentanddealing
withcorrespondinginformationandsemanticgaps,withoutrelyingonanyspecialAPIthatisnot
availabletoatypicalcomputeruser. AframeworkforGCCshouldhavetheabilitytounderstand
and interpret computer screens and dynamic changes between consecutive frames (and possibly
audio)fromarbitrarysoftwareandbeabletogeneratereasonablecomputercontrolactionstobe
executedreliably. Thissuggestsamultimodalmodelwithpowerfulvisionandreasoningcapabilities,
inadditiontorichknowledgeofcomputerUIandcontrol,isarequirement. Inthiswork,weleverage
GPT-4V as the framework’s backbone model, which is currently one of the most capable LMM
modelsavailable.
4.1 EnvironmentIO
Althoughtheinputfromtheenvironmentmaycrossmultiplemodalities(e.g.,visual,audio),inthis
work,wecast CRADLE toconsumevideos(sequencesofscreenshots)ofthescreenasinputand
producekeyboardandmousecontrolcommandsasoutput. Asthismatchesthemostcommonsetting
facedbyusers,itisapragmaticscenarioinapreliminaryattempttowardsGCC.
InformationGathering. AsshowninFigure3,tocaptureallinformationrelevanttounderstanding
the recent situation and perform further reasoning, CRADLE takes a video recording since the
last executed action as input and needs to make sure information is properly extracted from it,
whichincludesbothtextualandvisualinformation. Textualinformationusuallyincludescontent
(headingsandparagraphs),navigationlabels(menusandlinks),notifications, andinstructionsto
conveymessagesandguideusers,whichusuallydependontheOCRabilityofLMMmodels. Onthe
otherhand,visualinformationincludeslayout,imagery(visualcontentsofthescreenshotitselfand
icons),animations,andUIelementstoenhanceuserexperienceandinterfacedesign,whichposes
highrequirementsforthespatialperceptionandvisualunderstandingofLMMmodels.
SkillandActionGenerationUndertheGCCsetting,theonlywaytointeractwiththeenvironment
is through keyboard and mouse operations. To bridge the gap between actions outputted by the
framework and the OS-level executable actions, CRADLE uses the LMM to generate code func-
5
rotucexE
Retrievetionassemantic-levelskillswhichencapsulatelower-levelkeyboardandmousecontrol,e.g.,def
move_forward(duration): key_hold(‘W’, duration). We then let the LMM instantiate
theseskillfunctionsintoexecutableactionbyspecifyinganynecessaryparametricaspects,suchas
duration,positionandspeed,e.g.,move_forward(duration = 2).
ActionExecution. AftertheLMMgeneratesactionsanddecidestoexecutethemintheenvironment,
anExecutoristhentriggeredtomapthesesemanticactionstothefinalOS-levelkeyboardandmouse
commandstointeractwiththeenvironment.
4.2 Reasoning
Basedontheextractedinformationfromthevideoobservationsandrelevantinformationfromits
memory,CRADLEneedstoreasontakingintoaccountincompleteinformationandsemanticgaps
andthenmakethenextdecision. Thisprocessisanalogousto“reflectonthepast,summarizethe
present,andplanforthefuture”,whichisbrokendownintothefollowingmodules.
Self-Reflection. The reflection module initially evaluates whether the last executed action was
successfullycarriedoutandwhetherthetaskwascompleted. Sequentialkeyscreenshotsfromthe
lastvideoobservation,alongwiththepreviouscontextforactionplanningandtaskinferencearefed
totheLMMforreasoning. Additionally,wealsorequesttheLMMtoprovideananalysisofany
failure. ThisvaluableinformationenablesCRADLEtotryandremedyinappropriatedecisionsor
less-than-idealactions. Furthermore,reflectioncanalsobeleveragedtoinformre-planningofthe
taskandbringtheagentclosertotargettaskcompletion,betterunderstandthefactorsthatledto
previoussuccesses,orsuggesthowtoupdateorimprovespecificskills.
Task Inference. After reflecting on the outcome of the last step, CRADLE needs to analyze the
currentsituationtoinferthemostsuitabletaskforthecurrentmoment. WelettheLMMestimatethe
highestprioritytasktoperformandwhentostopanongoingtaskandstartanewone,promptingit
withthekeyscreenshots,thelong-termsummaryofpastexperiences,andthelatestreflectionresults.
SkillCuration. Asanewtaskisselected,CRADLEneedstopreparethetacticstoaccomplishit,
by retrieving useful skills from the procedural memory, updating skills, or generating new ones.
Similarlyto[24,35],skillsinCRADLEarerepresentedascodefunctions;aformisbothflexibleand
interpretable,andeasyforLMMstounderstandandmaintain. Atomicskillsareusuallymadeupof
simplecallstokeyboardandmousecontrol,e.g.,key_press()topressagivenkey,whichcanthen
beextendedandrewrittenintomorecomplexcompositeskills.
ActionPlanning. Tocompleteagiventask,theLMMneedstoselecttheappropriateskillsfrom
thecuratedskillsetandinstantiatetheseskillsintoasequenceofexecutableactionsbyspecifying
anynecessaryparametricaspects(e.g.,duration,position,andtarget)accordingtotheinferredtask,
lastaction,andlong-termsummary. TheactionsequenceisthenfedtotheExecutorforinteraction
withtheenvironment. Itisimportanttonotethatincomplexdigitalgames,theeffectiveactionspace
is composed not only of keyboard and mouse function calls per se, but also involves timing and
cross-actioninteraction,semanticmappingoftermusageonscreentoaction-specificdetails,among
otherfactors. Moreover,performingactionsintheenvironmentisnon-trivialalsoasmappingcode
executiontoitseffectsisnotalwaysexplicit. Thatis,actionexecutioncanresultinnoerrorfromthe
codeorgame,butstillbeincorrectorineffective.
4.3 Memory
CRADLEstoresandmaintainsalltheusefulinformationprovidedbytheenvironmentandtheLMM’s
outputthroughamemorymodule,consistingofepisodicmemoryandproceduralmemory.
EpisodicMemory. Episodicmemoryisusedtomaintaincurrentandpastexperiences,includingkey
screenshotsfromeachvideoobservation,andallLMMoutput,e.g.,textualandvisualinformation,
actions, tasks, and reasoning from each module. To facilitate retrieval and storage, periodical
summarization is conducted to abstract recently added multimodal information into long-term
summaries. Theincorporationofepisodicmemoryenables CRADLE toeffectivelyretaincrucial
informationoverextendedperiods,therebyenhancingitsdecision-makingcapabilities.
ProceduralMemory. Thismemoryisspecifictostoringandretrievingskillsincodeform,which
canbelearnedfromscratchorpre-definedinproceduralmemory. Uponskillcuration,skillscanbe
6Self-Reflection Task Inference Skill Curation
1. The previous action def show_info(duration):
check_shire() took effect # Show the information.
since the secondary menu Add/Update press_key(‘Q’, duration)
of further actions appeared. “Lead thehorse” def lead_horse(duration):
2. The task of leading the # Lead the horse. Generated
horse is not completed yet. press_key(‘E’, duration)
def turn(degree):
# Turn the character a certain angle.
move_mouse(degree)
Episodic Procedural
Memory Memory def move_forward(duration):
# Move ahead for a duration time. Retrieved
press_key(‘W’, duration)
Memory
Observe Execute
lead_horse(duration=2)
Information Gathering Environment Action Planning
Figure4:ThedetailedillustrationofhowCRADLEisinstantiatedasagameagenttoplayRDR2.
added,updated,orcomposedinthememory. Themostrelevantskillsforagiventaskandsituation
willberetrievedtosupportactionplanning,so,asCRADLEcontinuouslyacquiresnewskillsduring
interaction,itiscriticalthatthismemorycaneffectivelycalculateskillrelevance.
5 EmpiricalStudies
AsshowninFigure4,wedeployCRADLEasagame-playingagentintherenownedAAAgame
RDR2,showcasingourframeworkforGCCwithacomplextarget. Tothebestofourknowledge,
this is the first work to explore such a challenging game under the GCC setting, without access
to any internal game state or API (i.e., the agent has to interact with the game in a human-like
manner). RDR2isatypical3DRPG-likegamewithclassicalkeyboardandmousecontrolsand
movie-likerealisticgraphics. Thegame’sprogressionunveilsamixofstorycontent,dialoguesand
instructions,andhelpfultipsforinteractivegamemechanisms,whichareleveragedbyouragentto
independentlyexpandtheskillsintheproceduralmemoryfromscratch. Demonstratingouragent’s
capabilitytonavigatetheworldandcompletetasksfollowingthemainstorylineinRDR2underscores
thesignificantpotentialofourframeworkinadvancingtowardsGCC.Weprovideamoredetailed
introductionofthegameinAppendixA.1. Theimplementationdetailsoftheframeworkandall
LMMpromptscanbefoundinAppendicesB.1andB.3.
Objective. Toemulateanewhumanplayerlearningtoplaythegamefromscratch,includingin-
gamebeginnertutorialsandhints,wemainlyfocusonthefirsttwomissionsofthemainstorylinein
ChapterI,ExploreShelterandRescueJohn,includinghorseriding,NPCfollowing,houseexploration,
weaponsselection,combatwithenemiesandwolves,etc. Typically,ittakesanovicehumanplayer
around40minutesofthegameplaydurationtofinishthesemissions.Seldompreviousworkattempted
thiskindofchallengewithsuperlong-horizontasksandrichsemanticenvironments. Besidesthe
missionsinthemainstoryline,wealsoevaluateCRADLEintheopen-endedworldinChapterIIwith
amission,Buysupply,wheretheagentisinstructedtogototheGeneralStoreinValentinetownfrom
thecampforsuppliespreparation. Forthisopen-endedtask,seldomin-gameguidancewillappear.
Theagentneedstoanalyzeandproposefeasiblesolutionstocompletethemission. Weprovidea
briefintroductiontothesetasksinAppendixA.2.
ObservationsandActionSpace. StrictlyfollowingtheGCCsetting,ouragenttakesthevideoofthe
screenasinputandoutputskeyboardandmouseoperationstoplaythegame. Tolowerthefrequency
ofinteractionwiththebackbonemodel,thevideorecordertakesagamescreenshotevery0.5seconds,
whichprovestobesufficientforinformationgatheringwithoutmissinganyimportantinformation.
Fortheactionspace,wecategorizethekeyboardandmouseactionsinto4keycategories:press(key,
7duration),hold(key, duration),release(key),andpointer_move(x, y),whichcanbe
combinedindifferentwaystoformcombos,usekeysinfastsequence,orcoordinatetimings. Skill
codeneedstobegeneratedbytheagentinordertoutilizesuchfunctionsandaffordancessoexecuted
actionstakeeffect. MoredetailsontheactionspaceareprovidedinB.1.
5.1 CaseStudies
Herewepresentafewcasestudiesformorein-depthdiscussionoftheframeworkcapabilitiesand
thechallengesoftheGCCsetting.
5.1.1 Self-Reflection
Self-reflectionisanessentialcomponentinCRADLEasitallowsourframeworkreasoningtocorrect
previousmistakesoraddressineffectiveactionstakenin-game. Figure5providesanexampleofthe
self-reflectionmodule. Thetaskrequirestheagenttoselectaweapontoequip,inthecontextofthe
“ProtectDutch”task. Initially,theagentselectsaknifeasitsweaponbychance,butsincethegame
requiresaguntobechosen,thisisincorrectandthegamestillpromptstheplayertore-openthe
weaponwheel. Theself-reflectionmoduleisabletodeterminethatthepreviousactionwasincorrect
andonasubsequentiterationtheagentsuccessfullyoptsforthegun, correctlyfulfillingthetask
requirementandadvancingtothenextstageinthestory.
5.1.2 SkillCuration
Forskillcuration,wefirstprovideGPT-4Vwithexamplesofgeneralmouseandkeyboardcontrol
APIs,e.g.,io_env.key_pressandio_env.mouse_click. Figure6showsthatGPT-4Vcancaptureand
understandthepromptsappearingonscreenshots,i.e.,iconsandtext,andstrictlyfollowtheprovided
skillexamplesusingourIOinterfacetogeneratecorrectskillcode. Moreover,GPT-4Valsogenerates
commentsinthecodetodemonstratethefunctionalityofthisskill,whichareessentialforcomputing
similarityandrelevancewithagiventaskduringskillretrieval. Thequalityofthegeneratedcomment
directlydeterminestheresultsofskillretrieval,andfurtherimpactsreasoningtoactionplanning.
Curationcanalsore-generatecodeforagivenskill,whichisusefulifGPT-4Vwronglyrecognizeda
keyormousebuttoninapreviousiteration.
5.1.3 ActionExecutionandFeedback
ProperreasoningaboutenvironmentfeedbackiscriticalduetothegeneralityoftheGCCsetting
andthelevelofabstractiontointeractwiththecomplexgameworld. Thesemanticgapsamong
theexecutionofanaction,itseffectsinthegameworld,andobservingtherelevantoutcomesfor
furtherreasoningleadtoseveralpotentialissuesthatCRADLEneedstodealwith. Suchissuescanbe
categorizedintofourmajorcases:
Lackofgroundingfeedback. Inmanysituations,duetothelackofpreciseinformationfromthe
environment,itcanbedifficultforthesystemtodeducetheapplicabilityoroutcomeofagivenaction.
For example, when picking an item from the floor, the action may fail due to the distance to the
object,whichisnotcloseenough. Orthedistanceisalreadycloseenough,butthechosenactiondoes
notexactlyapplyduetootherfactors,e.g.,character’spackageisfull.
Eveniftherightactionisselectedandexecutedsuccessfully,theagentstillneedstofigureoutits
resultsfromthepartialvisualobservationofthegameworld. Iftheagentneedstopickormanipulate
anobjectthatisoccludedfromview,theactionmayexecutecorrectly,butnooutcomecanbeseen.
ArepresentativeexampleinRDR2happenswhentheagenttriestopickupitsgunfromthefloorafter
afight. Gettingtotherightdistance,withoutcompletelyoccludingtheobject,canleadtomultiple
re-trials. Figure7ashowcasesasituationwherethoughasshowninthemini-mapthecharacteris
alreadystandinginfrontofthegun,it’sstillnotpossibletopickthegun.
Previouseffortsthatutilizein-gamestateAPIsunreasonablybypasssuchissuesbyleveraginginternal
structuredinformationfromthegameandthefullsemanticsofresponses(data)orfailures(error
messages).
Imprecise timing in IO-level calls. Be it by ambiguity in game instructions or differences in
specificin-gameactionbehaviors,eventheexecutionofacorrectactionmayfailduetominortiming
8Task Inference
1.The frame does not change, therefore,
1 the task is not finished yet.
2.The selected weapon is wrong,
therefore, I need to change the weapon.
Skill Curation
Hold Tab to show the Weapon Wheel.
Task Inference
e Action Planning
Hold Tab to show the Weapon Wheel. ut
c show_weapon_wheel()
e
Skill Curation x
E
Retrieved Skills: [‘aim’, ‘follow’, ‘turn’,
‘move_forward’, ‘show_weapon_wheel’, etc.]
e
ut Action Planning 4
ec show_weapon_wheel()
x
E
2 Task Inference
Use [Mouse] to select a weapon and
release [TAB] to equip.
Skill Curation
Retrieved Skills: [‘aim’, ‘select_weapon_at’,
Task Inference ‘turn’, ‘move_forward’, ‘show_weapon_wheel’, etc.]
Use [Mouse] to select a weapon and
release [TAB] to equip. Action Planning
select_weapon_at(0.5, 0.27)
Skill Curation
Decision Reasoning: The weapon in
def select_weapon_at(x, y):
e green box is slightly more relevant to the
# Move the mouse to select the weapon ut target due to the lastself-reflection.
io_env.mouse_move(x, y) ec
x
Retrieved Skills: [‘aim’, ‘follow’, ‘turn’, E
‘move_forward’, ‘show_weapon_wheel’, etc.]
Action Planning
select_weapon_at(0.6, 0.68) 5
te Decision Reasoning: The red box is selected
u
c as it is near the currently selected weapon.
e
x
E
Self-Reflection
3 1. The last executed action was to select a weapon,
and it was executed successfully.
2. In this frame, the ammo count is 6 in the upper
right corner, so the character is ready to fire.
Figure5:Casestudyofself-reflectiononre-tryingafailedtask.Taskinstructionandcontextrequiretheagentto
equipthegun.Awrongweapon(knife)isfirstselected,buttheagentequipsthegunafterself-reflection.Only
relevantmodulesareshownforbetterreadability,thoughallmodules(Figure3)areexecutedperiteration.
9def take_cover(duration=2): def focus_on_horse(): def view_stored_weapons():
""" """ """
press "Q" to take cover hold "right mouse button" to focus on horse press "tab" to view your stored weapons
""" """ """
io_env.key_press('q', duration) io_env.mouse_hold('right button') io_env.key_hold('tab')
Figure6: Skillcodegenerationbasedonin-gameinstructions. Asthestorylineprogresses, thegamewill
continuallyprovidepromptsonhowtouseanewskillviakeystrokesorutilizingthemouse.
(a)‘Pickgun’unavailable. (b)‘Opencabinet’presstiming. (c)‘Hitchhorse’re-useof‘e’key
Figure7:Examplesofactionexecutionuncertainty.Lackofenvironmentfeedbacktoactionsandsemanticgaps
betweenactionintentandgamecommandcanleadtochallengingsituationsforagentreasoning.
mismatches. Forexample,whenexecutinganactionlike‘opencabinet’,whichrequirespressing
the‘r’keyonthekeyboard,ifthepressistoofast,noeffecthappensinthegameworld. Howeveras
thereisnovisualchangeinthegamenorotherformsoffeedback,itcanbedifficultforGPT-4Vto
figureoutifaninappropriateactionwaschosenatthisgamestateoriftheminortimingfactorwas
theproblem. Pressingthekeyforlongertriggersananimationaroundthebutton(onlyifthehelper
menuisonscreen),butthisiseasilymissedandanykeyreleasebeforethecirclecompletesresultsin
noeffect. Figure7billustratesthesituation.
Thisissuealsomanifestsinothersituationsinthegame,wherepressingthesamekeyforlonger
triggersacompletelydifferentaction(e.g.,lightlypressingthe‘leftalt’keyvs. holdingitforlonger).
Changeinkeyandbuttonsemantics. Asomewhatsimilarsituationoccurswhenthesamekeyboard
keyormousebuttongetsattributeddifferentsemanticsindifferentsituations(oreveninamulti-step
action). GPT-4Vmaydecidetoexecuteagivenskill,buttheoriginalsemanticsnolongerhold. The
lackofin-gameeffectwouldalsoseemsimilartotheprevioussituations. Orworse,anundesired
effectwillconfusethesystemregardingthecorrectactionbeingselectedornot.
Forexample,inoneofthefirsttasksinthegame,thecharacterneedstohitchthehorsetoapoleto
continue. Theoperationtoperformtheactionconsistsofpressingthe‘e’keynearahitchingpost
(asshowninFigure7c). However,thesamekeypressistheonlyconstitutingstepintheactionto
dismountthehorse. Triggeringadismountinthewrongsituationleadstoundesiredsideeffects.
Interferenceissues. Lastly,someactionsinthegamerequiremultiplestepsandcanbeinterruptedin
manywaysnotbyagentactions. WithouttheuseofAPIsthatexposeinternalstatesorotherformsof
feedback,itismuchharderfortheagenttodecidetorepeatsub-actionsortrydifferentstrategies. For
example,takingashotincombatandlosingaim,oranunrelatedin-gameanimationbeingtriggered
mid-action,abortingit.
5.2 QuantitativeEvaluation
Toillustratetheeffectivenessandimportanceofdifferentmodulesin CRADLE toitsoverallper-
formance, we evaluate the framework on seven representative tasks from the main storyline and
open-endedmissions,comparedwithtwoablation-likebaselines: CRADLEwithoutSelf-Reflection
andCRADLEwithoutTaskInference. ExceptforthetaskofProtectDutchwhichinvolvesafast-
pacedgunbattle,Searchhouse,whichrequirestheagenttoexploreacomplexindoorenvironment,
10Cradle Cradle w/o Reflection Cradle w/o Task Inference
100
80
60
40
20
0
Go to shed Protect Dutch Search house Investigate barn Lead horse Equip shotgun Buy supply
Tasks
Figure8: CRADLEperformanceonsevenrepresentativetasksinRDR2. Thefirstsixtasksarefromthetwo
missionsinthemainstorylineandthelastoneisoneoftheopen-endedmissions.Everytaskistestedfivetimes
withamaximumoftenminutesin-gametime.Thetestwillalsoterminateifthemissionfails.
andtheopen-endedtaskwithlong-horizon,CRADLEcancompletealltasksinthemainstoryline
consistently. Moreover,evenforthesetwocomplextasks, CRADLE alsoachievesasignificantly
highersuccessratethandisablingeitherSelf-ReflectionorTaskInference. Themostcommonfailure
for the Buy supply open-world complex task happens when the agent riding a horse in the lively
Valentinetowncrashesintoapasserbyoracarriage.
Withouttaskinference,theagentcanstillleveragetheextractedinstructionaltextinformationand
long-termsummarytoinferbothtaskandgoalintheactionplanningprocess,whichcanleadtosome
successfultaskcompletions. However,overtimetheinstructionaltextdisappearsfromthescreenand
thelong-termsummarywillbedilutedasrelevantinformationwillbeforgottenduetotherecently
addedentries,whichexplainswhyitfailsacrossalltasks,butstillhasareasonablesuccessratein
theEquipshotguntask,wherethetaskhasashorterhorizonanditsinstructionaltextappearsatthe
bottomofthescreen. Fortheopen-endedmission,Buysupply,withoutself-reflectiontopropose
short-horizongoals,theagentevenstruggleswithleavingthecamp.
Ontheotherhand,withoutself-reflection,CRADLEstruggleswithmovement,especiallywhenthe
agentisblockedbyobstacles,whichisdifficultforactionplanningtonoticeandaddress. Withoutan
independentcritictoreflectonthecurrentsituation,GPT-4Vtendstostilltrustitspreviousreasoning
andbelievethattheactionswerecarriedoutsuccessfully. AsSearchHouseisacomplexindoortask
withvariouspiecesoffurnitureasobstacles,wecanseeCRADLEwithoutSelf-Reflectionexhibits
extremelypoorperformanceinit,asexpected. Withoutself-reflection,theagentalsostruggleswith
enteringthestoreandisalwaysconfusedaboutwhetherthetaskisfinished.
5.3 LimitationsofGPT-4V
DeployingCRADLEinthiscomplexgame,RDR2,requiresthebackboneLMMmodeltodealwith
multimodalinput,whichrevealedseverallimitationsofGPT-4Vthatneedsexternaltoolstoprovide
additionalgroundingstoimprovetheoverallframeworkperformance. Moredetailsareprovidedin
AppendixB.1.
Spatial Perception. As shown in Figure 9a, GPT-4V’s spatial-visual recognition capability is
insufficientforprecisefine-grainedcontrol,particularlyindetectingwhetherthecharacterisbeingor
goingtobeblockedandestimatingtheaccuraterelativepositionsoftargetobjects. Theseledtothe
needtoaugmentcertainimagestoprovideauxiliaryvisualcluesfordecision-making,i.e.,bounding
boxesofpossibletargetobjects.
IconUnderstanding. GPT-4Valsostruggleswithdomain-specificconcepts,suchasuniqueicons
withinthegame,whichmayrepresentspecifictargetsorrefertocertainmouseandkeyactions. As
showninFigure9b,GPT-4Vfailstorecognizetheleftshift,rightmousebutton,andspaceicons.
Attemptstoincorporatefew-shotlearningtoimproveimageunderstandingcannotbegeneralized.
Therefore, we match prepared pattern templates, e.g., icon images, against each screenshot to
continuouslydetectandhighlightanyappearingicons.
Figure9calsodemonstratesthatGPT-4Vcannotunderstandthemini-mapcorrectly,whichusually
containskeyinformationaboutthepositionanddirectionofthecharacter. Failureoflocalization
11
etaR
sseccuSBased on the provided game screenshot, I've From the provided image screenshot from the game,
identified the following control prompt: I can identify the following keyboard and mouse
- “Hold L⬆to ride faster" control prompts:
This control suggests that holding a certain key 1. "Toggle View" - The key for this action is "V".
(which seems to be an upward arrow key, often 2. "Zoom" - The key for this action is "O".
associated with the 'W' key for forward movement 3. "Flip" - The key for this action is "L".
in many games) will make the character ride faster. 4. "Back" - The key for this action is "ESC".
Thecentralwhitearrowisfacingtowardstheleft. The central white arrow with the white arrowhead is
facing towards the left.
There are no immediate obstacles directly in front The shopkeeper in the image is on the right side,
of the character in the image. The character has a behindthecounter.
clearpathaheadonamuddystreetinwhatappears
tobeaWestern-styletownsetting.
(a)GPT-4Vfailstorecognizeobstaclesintheenvironmentandthepositionoftheshopkeeper.
Based on the provided game screenshot, I've From the provided image screenshot from the game,
identified the following control prompt: I can identify the following keyboard and mouse
Based on the provided game screenshot, I've From the provided image screenshot from the game,
- “Hold L⬆to ride faster" control prompts:
identified the following control prompt: I can identify the following keyboard and mouse
T - “h His olc do n Ltr ⬆ol tosu rg idg ee s ft as s teth r"at holding a certain key 1 co. ntr" oT l o pg rg ol me pV tsie :w" - The key for this action is "V".
(which seems to be an upward arrow key, often 2. "Zoom" - The key for this action is "O".
This control suggests that holding a certain key 1. "Toggle View" - The key for this action is "V".
associated with the 'W' key for forward movement 3. "Flip" - The key for this action is "L".
(which seems to be an upward arrow key, often 2. "Zoom" - The key for this action is "O".
in many games) will make the character ride faster. 4. "Back" - The key for this action is "ESC".
associated with the 'W' key for forward movement 3. "Flip" - The key for this action is "L".
in many games) will make the character ride faster. 4. "Back" - The key for this action is "ESC".
(b)GPT-4Vstrugglestorecognizetheiconsforkeysonkeyboardandmousebuttons.
Thecentralwhitearrowisfacingtowardstheleft. The central white arrow with the white arrowhead is
facing towards the left.
Thecentralwhitearrowisfacingtowardstheleft. The central white arrow with the white arrowhead is
facing towards the left.
(c)GPT-4Vcannotunderstandthecorrectdirectionofarrowpoints,i.e.,character,towardsinthemini-map.
Figure9:ExamplesituationsofGPT-4V’slimitationsinunderstandingvisualinformationfromthegame.
12
There are no immediate obstacles directly in front The shopkeeper in the image is on the right side,
of the character in the image. The character has a behindthecounter.
There are no immediate obstacles directly in front The shopkeeper in the image is on the right side,
clearpathaheadonamuddystreetinwhatappears
of the character in the image. The character has a behindthecounter.
tobeaWestern-styletownsetting.
clearpathaheadonamuddystreetinwhatappears
tobeaWestern-styletownsetting.leadstheagenttosometimesgetlostinthetownandmissthetasktarget. Althoughtheaboveissues
canbeslightlyalleviatedbyprovidingadditionalfew-shotexamples,itcanonlyhaveanobvious
effectifwecroptheimageandprovidetheGPT-4Vwiththeregionexactlycontainingtheicontobe
recognized,whichmakestheissueintractable.
HistoryProcessing. Moreover,GPT-4Vcaneasilygetdistractedbyirrelevantinformationinlonger
contexts,resultinginhallucinations. Forexample,whenactionplanningutilizestoomanyhistorical
screenshots,GPT-4Vmayconfusepastandpresentframes. Additionally,GPT-4Vfrequentlygener-
atesoutputnotadheringtotherulesintheprovidedprompts. Tomitigatetheissueofhallucinations,
wemorestrictlycontrolinputinformationbyfurthersummarizinglong-termmemory.
WorldUnderstanding. Lastly,theabsenceofanRDR2worldmodellimitsGPT-4V’sunderstanding
oftheconsequencesofitsactionsinthegame. Thisoftenresultsininappropriateactionselection,
suchasoverestimatingthenecessaryadjustmentsforaligningtargetsormisjudgingtheduration
requiredforcertainactions. Toalleviatethisproblem,weintroducedextrapromptrulesregarding
actionparametersandmoreflexibilityintotheself-reflectionmodule.
6 ConclusionsandFutureWork
In this work, we introduce GCC, a general and challenging setting with a unified and standard
interface for control of diverse software (via screenshots, and keyboard and mouse operations),
pavingthewaytowardsgeneralfoundationagentsacrossalldigitalworldtasks.
ToproperlyaddressthechallengesGCCpresents,weproposeanovelframework(CRADLE)that
enables LMM-based agents to succeed in such an impactful setting. We further showcase its
effectivenessinacomplexAAAdigitalgame,RedDeadRedemptionII.CRADLEexhibitsstrong
performanceinlearningnewskills,followingthegamestoryline,andaccomplishingrealmissions
inthegame. Tothebestofourknowledge,thisisthefirstLMM-basedagentthathasmanagedto
completeconcretemissionsfromscratchinAAAgames.
Althoughouragentcanstillfacedifficultiesinsometasks,CRADLEservesasapioneeringworkto
developmorepowerfulLMM-basedgeneralagentsacrosscomputercontroltasks,combiningboth
furtherframeworkenhancementsandnewadvancesinLMMs.
FutureworkinCRADLEaimstoextendittosupportabroaderspectrumofgames,suchassimu-
lationandstrategygames,aswellasvarioussoftwareapplications,todemonstrateitsexceptional
adaptability across diverse environments. This endeavor aims to substantiate the hypothesis that
CRADLEcanseamlesslyinteractwithanysoftware,transformingitintoaninteractivebenchmarkto
evaluatethemultifacetedcapabilitiesoffoundationagents. Incorporatingaudioasinputisalsokeyin
importanttasksandisnecessarytomeettheGCCsetting,whichintroducestheadditionalchallenge
ofdealingwiththesimultaneityofmultiplemodalities. Furthermore,theinteractivedatacollected
throughframeworkdevelopmentandexecutionwillbeinstrumentalinfutureefforts,fromtraining
bettermodelstoenhancingworldmodeling,therebyenhancingagentcapabilitiesandusefulnessfor
awiderangeofusers.
References
[1] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoni
Aleman,DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. GPT-4
Technicalreport. arXivpreprintarXiv:2303.08774,2023.
[2] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Bran-
don Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (VPT): Learning to act
bywatchingunlabeledonlinevideos. AdvancesinNeuralInformationProcessingSystems,
35:24639–24654,2022.
[3] Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried,
AndrewGoff,JonathanGray,HengyuanHu,etal. Human-levelplayinthegameofdiplomacy
bycombininglanguagemodelswithstrategicreasoning. Science,378(6624):1067–1074,2022.
13[4] MarcGBellemare,YavarNaddaf,JoelVeness,andMichaelBowling. TheArcadelearning
environment: An evaluation platform for general agents. Journal of Artificial Intelligence
Research,47:253–279,2013.
[5] ChristopherBerner,GregBrockman,BrookeChan,VickiCheung,PrzemysławDe˛biak,Christy
Dennison,DavidFarhi,QuirinFischer,ShariqHashme,ChrisHesse,etal. Dota2withlarge
scaledeepreinforcementlearning. arXivpreprintarXiv:1912.06680,2019.
[6] AnthonyBrohan,YevgenChebotar,ChelseaFinn,KarolHausman,AlexanderHerzog,Daniel
Ho,JulianIbarz,AlexIrpan,EricJang,RyanJulian,etal. DoasIcan,notasIsay: Grounding
languageinroboticaffordances. InConferenceonRobotLearning,pages287–318.PMLR,
2023.
[7] RobertoBrunelli. Templatematchingtechniquesincomputervision: theoryandpractice. John
Wiley&Sons,2009.
[8] KanzhiCheng,QiushiSun,YougangChu,FangzhiXu,YantaoLi,JianbingZhang,andZhiyong
Wu. SeeClick: HarnessingGUIgroundingforadvancedvisualGUIagents. arXivpreprint
arXiv:2401.10935,2024.
[9] XiangDeng,YuGu,BoyuanZheng,ShijieChen,SamuelStevens,BoshiWang,HuanSun,and
YuSu. Mind2Web: Towardsageneralistagentfortheweb. arXivpreprintarXiv:2306.06070,
2023.
[10] DannyDriess,FeiXia,MehdiSMSajjadi,CoreyLynch,AakankshaChowdhery,BrianIchter,
Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied
multimodallanguagemodel. arXivpreprintarXiv:2303.03378,2023.
[11] LinxiFan,GuanzhiWang,YunfanJiang,AjayMandlekar,YuncongYang,HaoyiZhu,Andrew
Tang, De-AnHuang, YukeZhu, andAnimaAnandkumar. Minedojo: Buildingopen-ended
embodiedagentswithinternet-scaleknowledge. AdvancesinNeuralInformationProcessing
Systems,35:18343–18362,2022.
[12] HirokiFuruta,OfirNachum,Kuang-HueiLee,YutakaMatsuo,ShixiangShaneGu,andIzzeddin
Gur. Multimodalwebnavigationwithinstruction-finetunedfoundationmodels. arXivpreprint
arXiv:2305.11854,2023.
[13] Difei Gao, Lei Ji, Zechen Bai, Mingyu Ouyang, Peiran Li, Dongxing Mao, Qinchen Wu,
WeichenZhang,PeiyiWang,XiangwuGuo,etal.ASSISTGUI:Task-orienteddesktopgraphical
userinterfaceautomation. arXivpreprintarXiv:2312.13108,2023.
[14] IzzeddinGur,HirokiFuruta,AustinHuang,MustafaSafdari,YutakaMatsuo,DouglasEck,
andAleksandraFaust. Areal-worldwebagentwithplanning,longcontextunderstanding,and
programsynthesis. arXivpreprintarXiv:2307.12856,2023.
[15] WilliamHGuss,BrandonHoughton,NicholayTopin,PhillipWang,CaydenCodel,Manuela
Veloso,andRuslanSalakhutdinov. Minerl: Alarge-scaledatasetofMinecraftdemonstrations.
arXivpreprintarXiv:1907.13440,2019.
[16] HongliangHe,WenlinYao,KaixinMa,WenhaoYu,YongDai,HongmingZhang,Zhenzhong
Lan,andDongYu. WebVoyager: Buildinganend-to-endwebagentwithlargemultimodal
models. arXivpreprintarXiv:2401.13919,2024.
[17] WenyiHong,WeihanWang,QingsongLv,JiazhengXu,WenmengYu,JunhuiJi,YanWang,
ZihanWang,YuxiaoDong,MingDing,etal. CogAgent: AvisuallanguagemodelforGUI
agents. arXivpreprintarXiv:2312.08914,2023.
[18] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng,
Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied
reasoningthroughplanningwithlanguagemodels. arXivpreprintarXiv:2207.05608,2022.
[19] MaxJaderberg,WojciechMCzarnecki,IainDunning,LukeMarris,GuyLever,AntonioGarcia
Castaneda, Charles Beattie, Neil C Rabinowitz, Ari S Morcos, Avraham Ruderman, et al.
Human-levelperformancein3Dmultiplayergameswithpopulation-basedreinforcementlearn-
ing. Science,364(6443):859–865,2019.
14[20] MatthewJohnson,KatjaHofmann,TimHutton,andDavidBignell. TheMalmoplatformfor
artificialintelligenceexperimentation. InIjcai,pages4246–4247,2016.
[21] RaghavKapoor,YashParagButala,MelisaRussak,JingYuKoh,KiranKamble,WaseemAl-
shikh,andRuslanSalakhutdinov. OmniACT:Adatasetandbenchmarkforenablingmultimodal
generalistautonomousagentsfordesktopandweb,2024.
[22] ChristianKauten. SuperMarioBrosforOpenAIGym. GitHub,2018.
[23] KarolKurach,AntonRaichuk,PiotrStan´czyk,MichałZaja˛c,OlivierBachem,LasseEspeholt,
CarlosRiquelme,DamienVincent,MarcinMichalski,OlivierBousquet,etal. Googleresearch
football: Anovelreinforcementlearningenvironment. InProceedingsoftheAAAIconference
onartificialintelligence,pages4501–4510,2020.
[24] JackyLiang,WenlongHuang,FeiXia,PengXu,KarolHausman,BrianIchter,PeteFlorence,
andAndyZeng. Codeaspolicies: Languagemodelprogramsforembodiedcontrol. In2023
IEEEInternationalConferenceonRoboticsandAutomation(ICRA),pages9493–9500.IEEE,
2023.
[25] ShilongLiu,ZhaoyangZeng,TianheRen,FengLi,HaoZhang,JieYang,ChunyuanLi,Jianwei
Yang,HangSu,JunZhu,etal. GroundingDino: Marryingdinowithgroundedpre-trainingfor
open-setobjectdetection. arXivpreprintarXiv:2303.05499,2023.
[26] WeiyuMa,QiruiMi,XueYan,YuqiaoWu,RunjiLin,HaifengZhang,andJunWang. Large
languagemodelsplayStarCraftII:Benchmarksandachainofsummarizationapproach. arXiv
preprintarXiv:2312.11865,2023.
[27] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh
Jayaraman,YukeZhu,LinxiFan,andAnimaAnandkumar. Eureka: Human-levelrewarddesign
viacodinglargelanguagemodels. arXivpreprintarXiv:2310.12931,2023.
[28] GrégoireMialon,ClémentineFourrier,CraigSwift,ThomasWolf,YannLeCun,andThomas
Scialom. GAIA:abenchmarkforgeneralAIassistants. arXivpreprintarXiv:2311.12983,2023.
[29] Meredith Ringel Morris, Jascha Sohl-dickstein, Noah Fiedel, Tris Warkentin, Allan Dafoe,
AleksandraFaust,ClementFarabet,andShaneLegg. LevelsofAGI:Operationalizingprogress
onthepathtoAGI. arXivpreprintarXiv:2311.02462,2023.
[30] RunliangNiu,JindongLi,ShiqiWang,YaliFu,XiyuHu,XueyuanLeng,HeKong,YiChang,
andQiWang. ScreenAgent: Avisionlanguagemodel-drivencomputercontrolagent. arXiv
preprintarXiv:2402.07945,2024.
[31] SiyuanQi,ShuoChen,YexinLi,XiangyuKong,JunqiWang,BangchengYang,PringWong,
YifanZhong,XiaoyuanZhang,ZhaoweiZhang,etal. CivRealm: Alearningandreasoning
odysseyinCivilizationfordecision-makingagents. arXivpreprintarXiv:2401.10568,2024.
[32] ChristopherRawles,AliceLi,DanielRodriguez,OrianaRiva,andTimothyLillicrap. Android
inthewild: Alarge-scaledatasetforAndroiddevicecontrol. arXivpreprintarXiv:2307.10088,
2023.
[33] Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nan-
tasNardelli, TimGJRudner, Chia-ManHung, PhilipHSTorr, JakobFoerster, andShimon
Whiteson. TheStarcraftmulti-agentchallenge. arXivpreprintarXiv:1902.04043,2019.
[34] Oriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jaderberg, Wo-
jciech M Czarnecki, Andrew Dudzik, Aja Huang, Petko Georgiev, Richard Powell, et al.
AlphaStar: Masteringthereal-timestrategygameStarcraftII. DeepMindblog,2:20,2019.
[35] GuanzhiWang,YuqiXie,YunfanJiang,AjayMandlekar,ChaoweiXiao,YukeZhu,LinxiFan,
andAnimaAnandkumar. Voyager: Anopen-endedembodiedagentwithlargelanguagemodels.
arXivpreprintarXiv:2305.16291,2023.
[36] JunyangWang,HaiyangXu,JiaboYe,MingYan,WeizhouShen,JiZhang,FeiHuang,andJitao
Sang. Mobile-Agent: Autonomousmulti-modalmobiledeviceagentwithvisualperception.
arXivpreprintarXiv:2401.16158,2024.
15[37] ZihaoWang,ShaofeiCai,AnjiLiu,YonggangJin,JinbingHou,BoweiZhang,HaoweiLin,
ZhaofengHe,ZilongZheng,YaodongYang,etal. Jarvis-1: Open-worldmulti-taskagentswith
memory-augmentedmultimodallanguagemodels. arXivpreprintarXiv:2311.05997,2023.
[38] ZihaoWang,ShaofeiCai,AnjiLiu,XiaojianMa,andYitaoLiang. Describe,explain,planand
select: Interactiveplanningwithlargelanguagemodelsenablesopen-worldmulti-taskagents.
arXivpreprintarXiv:2302.01560,2023.
[39] Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu
Yao, Tao Yu, and Lingpeng Kong. OS-copilot: Towards generalist computer agents with
self-improvement. arXivpreprintarXiv:2402.07456,2024.
[40] PeterR.Wurman,SamuelBarrett,KentaKawamoto,JamesMacGlashan,KaushikSubrama-
nian,ThomasJ.Walsh,RobertoCapobianco,AlisaDevlic,FranziskaEckert,FlorianFuchs,
etal. OutracingchampionGranTurismodriverswithdeepreinforcementlearning. Nature,
602(7896):223–228,2022.
[41] XinrunXu,YuxinWang,ChaoyiXu,ZiluoDing,JiechuanJiang,ZhimingDing,andBörjeF.
Karlsson. ASurveyonGamePlayingAgentsandLargeModels: Methods,Applications,and
Challenges. TechnicalReport,2024.
[42] AnYan,ZhengyuanYang,WanrongZhu,KevinLin,LinjieLi,JianfengWang,JianweiYang,
Yiwu Zhong, Julian McAuley, Jianfeng Gao, Zicheng Liu, and Lijuan Wang. GPT-4V in
wonderland: Large multimodal models for zero-shot smartphone GUI navigation. arXiv
preprintarXiv:2311.07562,2023.
[43] Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu.
AppAgent: Multimodalagentsassmartphoneusers. arXivpreprintarXiv:2312.13771,2023.
[44] Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang,
QingweiLin,SaravanRajmohan,etal. UFO:AUI-focusedagentforWindowsOSinteraction.
arXivpreprintarXiv:2402.07939,2024.
[45] BoyuanZheng,BoyuGou,JihyungKil,HuanSun,andYuSu. GPT-4V(ision)isageneralist
webagent,ifgrounded. arXivpreprintarXiv:2401.01614,2024.
[46] LongtaoZheng,RundongWang,XinrunWang,andBoAn. Synapse: Trajectory-as-exemplar
promptingwithmemoryforcomputercontrol. InICLR,2024.
[47] ShuyanZhou,FrankF.Xu,HaoZhu,XuhuiZhou,RobertLo,AbishekSridhar,XianyiCheng,
Yonatan Bisk, Daniel Fried, Uri Alon, et al. WebArena: A realistic web environment for
buildingautonomousagents. arXivpreprintarXiv:2307.13854,2023.
16A RedDeadRedemptionII
A.1 IntroductiontoRDR2
RedDeadRedemptionII(RDR2)isanepicAAAWestern-themedaction-adventuregamebyRockstar
Games. Asoneofthemostfamousandhighest-sellinggamesintheworld,itiswidelyacknowledged
foritsmovie-likerealisticscenes,richstorylines,andimmersiveopen-endedworld.Thegameapplies
atypicalrole-playinggame(RPG)controlsystem,playedfromafirst-orthird-personperspective,
whichusesWASDformovement,mousecontrolforviewchanging,first-orthird-personshooting
forcombat,andinventoryandmanipulation.
Formostofthegame,playersneedtocontrolthemaincharacter,ArthurMorgan,uponchoosing
tocompletemissionscenariosfollowingthemainstoryline. Otherwise,theycanfreelyexplorethe
interactiveworld,suchasgohunting,fishing,chattingwithnon-playercharacters(NPCs),training
horses, witnessing or partaking in random events, and participating in side quests. As the main
storylineprogresses,differentskillsaregraduallyunlocked. Asaclose-sourcecommercialgame,no
APIsareavailableforobtainingadditionalgame-internalinformationnorpre-definedautomation
actions. Followingitscharacteristics,thisgameservesasafittingandchallengingenvironmentfor
theGCCsetting.
A.2 RDR2Tasks
Table1:TasksinthefirsttwomissionsofRDR2.Difficultyreferstohowharditisforouragenttoaccomplish
thecorrespondingtasks.Figures10and 11showcasesnapshotsofeachtask(specificsub-figuresmarkedin
parenthesisinthetable).
Mission1:Exploreshelter Description Difficulty
FollowDutch(Fig.10a) FollowDutchtothesmalltown,byridingthehorse. Easy
Hitchhorse(Fig.10b) Dismountatthehitchingpost,afterreachingthetown. Easy
Gotoshed(Fig.10c) Movetothenearbyshedtotakecover. Easy
Chooseweapon(Fig.10d) Choosethecorrectweapontoprepareforcombat. Hard
ProtectDutch(Fig.10e) ProtectDutchfromtheenemiesthroughshooting. Hard
Searchhouse(Fig.10f) FollowDutchtoenterthehouseandsearchforsupplies. Hard
Eatsomething(Fig.10g) OpenSatchelandeataprovisiontorestoresomeHealthCore. Medium
Investigatebarn(Fig.10h) Leavethehorseandgotothebarntoinvestigate. Easy
DefeatO’Driscoll(Fig.10i) Fightwiththeenemyhiddeninthebarn. Medium
Pickupequipment(Fig.10j) Findandpickupgunandhatlostduringbarnfight. Hard
Leadhorse(Fig.10k) CalmandLeadthehorseoutofthebarntohitchingpost. Medium
Mission2:RescueJohn
FollowJavier(Fig.11a) FollowJavierthroughthesnowmountaintolookforJohn. Medium
Equipshotgun(Fig.11b) Equipcorrectshotguntoprepareforcombat. Medium
LookforJohn(Fig.11c) Crouchdown,enternarrowtunnel,andclimbupcliffs. Hard
Shootwolves(Fig.11d) Shootwolvesthatwillattackyouandcompanions. Hard
ProtectJavierfromwolves(Fig.11e) Ridehorseandprotectyourcompanionsfromwolves. Hard
Table2:Tasksintheopen-endedmission,BuysupplyinRDR2.Difficultyreferstohowharditisforouragent
toaccomplishthecorrespondingtasks.Figure12showcasessnapshotsofeachtask(specificsub-figuresmarked
inparenthesisinthetable).
Open-endedmission:Buysupply Description Difficulty
Findhorse(Fig.12a) Findandmountthehorseinthecamp. Medium
Preparetonavigatetosaloon(Fig.12b) Openmap,findthesaloonandcreatewaypoint. Medium
Gotosaloon(Fig.12c) Ridehorsetothesaloon. Easy
Preparetonavigatetoshop(Fig.12d) Openmap,findthegeneralstoreandcreatewaypoint. Medium
Gotoshop(Fig.12e) Ridehorsetotheshop. Hard
Entershop(Fig.12f) Dismountthehorseandentertheshop. Hard
Talktoshopkeeper(Fig.12g) Approachtheshopkeeperandtalk. Easy
Buytargetproduct(Fig.12h) Openthemenu,findandbuythetargetproduct. Medium
Tables 1 and 2 provide a brief introduction of each task in the first two missions of the main
game storyline and an open-ended mission, along with approximate estimates of their difficulty.
17(a)FollowDutch (b)Hitchhorse (c)Gotoshed
(d)Chooseweapon (e)ProtectDutch (f)Searchhouse
(g)Eatsomething (h)Investigatebarn (i)DefeatO’Driscoll
(j)Pickupequipment (k)Leadhorse
Figure10:ImageexamplesoftasksinthefirstmissionofExploreshelter.(Thepicturehasbeenbrightenedfor
easierreading.)
(a)FollowJavier (b)Equipshotgun (c)LookforJohn
(d)Shootwolves (e)ProtectJavierfromwolves
Figure11:ImageexamplesoftasksinthesecondmissionofRescueJohn.
18(a)Findhorse (b)Preparetonavigatetosaloon (c)Gotosaloon
(d)Preparetonavigatetoshop (e)Gotoshop (f)Entershop
(g)Talktoshopkeeper (h)Buytargetproduct
Figure12:Imageexamplesoftasksintheopen-endedtaskofRDR2.
DuetoGPT-4V’spoorperformanceinspatialunderstandingandfine-manipulationskills,itcanbe
challengingforouragenttoperformcertainactions,likeenteringorleavingabuilding,orgetting
topreciseindoorlocationstoretrievespecificitems. Additionally, thehighlatencyofGPT-4V’s
responsesalsomakesitharderforanagenttodealwithtime-sensitiveevents,e.g.,duringcombat.
B CradleinGCC
B.1 ImplementationDetails
AsshowninFigure4,strictlyfollowingtheGCCsetting,ouragenttakesthevideoofthescreenas
inputandoutputskeyboardandmouseoperationstointeractwiththecomputerandthegame. An
observationthreadisresponsibleforthecollectionofvideoframesfromthescreenandeachvideo
cliprecordsthewholein-gameprocesssinceexecutingthelastaction.
WeemployGPT-4V(ision)[1],gpt-4-vision-preview,currentlyoneofthemostcapableLMMmodels,
astheframework’sbackbonemodel. Tolowerthefrequencyofinteractionwithbackbonemodels,
thevideorecordercapturesagamescreenshotevery0.5seconds,whichprovestobesufficientfor
informationgatheringwithoutmissinganyimportantinformation.
InformationGathering. Toextractkeyframesfromthevideoobservation,weutilizetheVideoSub-
Findertool4,aprofessionalsubtitlediscoveryandextractiontool. Thesekeyframesusuallycontain
richmeaningfultextualinformationinthegame,whicharehighlyrelevanttothecompletionoftasks
andmissions(suchascharacterstatus,location,dialogues,in-gamepromptsandtips,etc.) Weuse
GPT-4VtoextractandcategorizeallthemeaningfulcontextsinthesekeyframesandperformOCR,
andcallthisprocessing"gatheringtextinformation". Then,tosaveinteractionswithGPT-4V,we
onlyletGPT-4Vprovideadetaileddescriptionofthelastframeofthevideo.
WhileGPT-4VexhibitsimpressivevisualunderstandingabilitiesacrossvariousCVtasks,wefind
thatitstruggleswithspatialreasoningandrecognizingsomegame-specificicons. Toaddressthese
limitations,weaddavisualaugmentationsub-modulewithinourInformationGatheringmodule.
Thisaugmentationstepservestwomainpurposes:i)utilizeGroundingDINO[25],anopen-setobject
4VideoSubFinderstandalonetool-https://sourceforge.net/projects/videosubfinder/
19detector,tooutputpreciseboundingboxesofpossibletargetsinanimageandserveasspatialclues
forGPT-4V;andii)performtemplatematching[7]toprovideiconrecognitiongroundingtruthfor
GPT-4Vwheninterpretinginstructionsormenusshownonscreen. AsLMMcapabilitiesmature,it
shouldbepossibletodisablesuchaugmentation.
Self-Reflection. Thereflectionmodulemainlyservestoevaluatewhetherthepreviouslyexecuted
actionwassuccessfullycarriedoutandwhetherthecurrentexecutingtaskisfinished. Toachieve
this,weuniformlysampleatmost8sequentialframesfromthevideoobservationsincetheexecution
ofthelastactionanduseGPT-4Vtoestimatethesuccessofitsexecution. Additionally,weexpect
GPT-4Vcanalsoprovideanalysisforanyfailureofthelastaction(e.g.,themove-forwardaction
failedandthecausecouldbetheagentwasblockedbyanobstacle). Withsuchvaluableinformation
asinputforActionPlanning,includingthefailure/successofthelastactionandthecorresponding
analysis,theagentiscapableofattemptingtoremedyaninappropriatedecisionoractionexecution.
Moreover,someactionsrequireprolongeddurations,suchasholdingdownspecifickeys,whichcan
coexistorinterferewithotheractionsdecidedbysubsequentdecisions. Consequently,thereflection
modulemustalsodecidewhetheranongoingactionshouldcontinuetobeexecuted. Furthermore,
self-reflectioncanbeleveragedtodissectwhythelastactionfailedtobringtheagentclosetothe
target task completion, better understand the factors that led to the successful completion of the
precedingtask,andsoon.
Besides,weobservethatinsteadofprovidingGPT-4Vwithsequentialhigh-resolutionimagesfor
self-reflection
TaskInference. Duringgameplay,weletGPT-4Vproposethecurrenttasktoperformwheneverit
believesitistimetostartanewtask. GPT-4Valsooutputswhetherthetaskisalong-orshort-horizon
taskwhenproposinganewtask. Long-horizontasks,suchastravelingtoalocation,typicallyrequire
multipleiterations,whereasshort-horizontasks,likepickingupanitemorconversingwithsomeone,
involvefeweriterations. Theagentwillfollowthenewlygeneratedtaskforthenexthinteractions.
Afterhinteractions,theagentreturnstothelastlong-horizontaskinthestack. Decidingonabinary
taskhorizonismucheasierandmorerobustforGPT-4V,thanre-planningateveryiteration. Since
along-horizontaskfrequentlyincludesmultipleshort-horizonsub-tasks,thisimplementationalso
helpsavoidforgettingthelong-horizontasksunderexecution.
TosaveinteractionswithGPT-4V,weimplementthisaspartoftheInformationGatheringmodule.
WhenGPT-4Vdetectsinstructionaltextintherecentobservation, itwilldirectlygenerateatask
descriptionanddeterminewhetheritisalong-orashot-horizontask.
SkillCuration. AsshowninFigure6,duringgameplay,instructionsoftenappearonthescreen,
such as “press [Q] to take over" and “hold [TAB] to view your stored weapons," which serve as
essentialdirectivesforcompletingcurrentandfuturetasksproficiently. Totranslatethesetextual
andiconicinstructionsintoexecutablemouseandkeyboardactions,weprovideGPT-4Vwithskill
code samples using interfaces for mouse and keyboard manipulation, e.g., io_env.key_press (to
pressakey),io_env.key_hold(tokeepakeyheld),io_env.mouse_click(toclickthemousebutton),
andio_env.mouse_hold(tokeepthemousebuttonheld). GPT-4Visrequiredtostrictlyfollowthe
provided interfaces and examples to generate the corresponding code for new skills. Moreover,
GPT-4Visrequiredtoincludedocumentation/commentswithinthegeneratedcode,delineatingthe
functionalityofeachskill. TheProceduralMemorysub-modulewhereskillsarestoredwillthen
checkwhetherthecodeisvalid,whethertheformatofdocumentationisright,andwhetheranyskill
withthesamenamealreadyexists. Ifallconditionsarepassed,thenewlygeneratedskillispersisted
forfutureutilization.
SimilarlytoTaskInference,wealsoimplementasimpleversionofthismoduleinsideInformation
GatheringtoreduceinteractionswithGPT-4V.WhenGPT-4Vdetectsandclassifiessomeinstructional
textintherecentobservation,whichusuallycontainskey/buttonhints,itwilldirectlygeneratethe
correspondingcodeanddescription.
ActionPlanning. Uponexecutionofthismodule,wefirstretrievethetopkrelevantskillsforthe
taskfromproceduralmemory,alongsidethenewlygeneratedskills. WethenprovideGPT-4Vwith
thecurrenttask,thesetofretrievedskills,andotherinformationcollectedinInformationGathering
thatmaybehelpfulfordecision-making(e.g.,recentscreenshotswithcorrespondingdescriptions,
previousdecisions,andexamples)andletitsuggestwhichskillsshouldbeexecuted. Wealsorequest
thatGPT-4Vprovidethereasonstochoosetheseskills,whichincreasestheaccuracy,stability,and
20Table3: Actionspaceinthe CRADLE framework,includingactionattributes. Coordinatesystemiseither
absoluteorrelative.Actionswithdurationscanbeeithersynchronousorasynchronous.
Type Action Attributes
Keyname(string),
KeyPress
Keypressduration(seconds:float)
Keyname(string),
KeyHold Keypressduration(seconds:float),
Waitbehaviour(sync/async)
KeyRelease Keyname(string)
Keynames(strings),
Keyboard
KeyCombo Keycomboduration(seconds:float),
Waitbehaviour(sync/async)
Keynames(strings),
Hotkey Hotkeysequenceduration(seconds:float),
Waitbehaviour(sync/async)
Stringtotype(string),
TextType
Typingduration(seconds:float)
Mousebutton(left/middle/right),
ButtonClick
Buttonclickduration(seconds:float)
Mousebutton(left/middle/right),
ButtonHold Buttonholdduration(seconds:float),
Waitbehaviour(sync/async)
ButtonRelease Mousebutton(left/middle/right)
Mousebutton(left/middle/right),
ButtonDoubleClick
Buttonclickduration(seconds:float)
Mouseposition(width:int,height:int),
Mouse Mousespeed(seconds:float),
Move/Hover/Point
Coordinatesystem(enum),
Tweenmode(enum)5
Mousefinalposition(width:int,height:int),
Mousespeed(seconds:float),
Drag
Coordinatesystem(enum),
Tweenmode(enum)
Orientation(vertical/horizontal),
Scroll Distance(clicks:int),
Duration(seconds:float)
WaitAction Waittime(float)
explainabilityofskillselectionandthusgreatlyimprovesframeworkperformance. WhileGPT-4V
sometimesmaygenerateasequenceofactions,wecurrentlyonlyexecutethefirstone,andperform
Self-Reflection,sinceweobserveatendencyforthesecondactiontosufferfromseverehallucination
usually.
Action Execution. For the action space, we categorize the keyboard and mouse actions
into 4 key categories: press(key, duration), hold(key, duration), release(key), and
pointer_move(x, y);whichcanbecombinedindifferentwaystoformcombos,usekeysinfast
sequence,orcoordinatetimings. Skillcodeneedstobegeneratedbytheagentinordertoutilizesuch
functionsandaffordancessoexecutedactionstakeeffect. Table3illustratesCRADLE’sactionspace.
It is important to note that, while some works (e.g., AssistantGUI [13] and OmniACT [21]) use
PyAutoGUI 6 for keyboard and mouse control, this approach does not work in all applications,
particularlyinmodernvideogamesusingDirectX7.Moreover,suchworkchoosestoexposeasubset
6Python library that provides a cross-platform GUI automation module - https://github.com/
asweigart/pyautogui
7MicrosoftDirectXgraphicsprovidesasetofAPIsforhigh-performancemultimediaapps-https://
learn.microsoft.com/en-us/windows/win32/directx
21ofthelibraryfunctionalityinitsactionspace,ignoringdimensionslikepressdurationandmovement
speed,whicharecriticalinmanyscenarios.
Toensurewidegamecompatibilityandavoidinterferencebetweendifferentcodelibraries,inour
currentimplementationweusethesimilarPyDirectInputlibrary8 onlyforkeyboardcontrol,and
write our own abstraction for mouse control (using the ctypes library 9 to send low-level mouse
commandstotheoperatingsystem. Forincreasedportabilityandeaseofmaintenance,allkeyboard
andmousecontrolisencapsulatedinaclass,calledIO_env.
Unliketheconventionalmouseoperationinstandardsoftware,wherethecursorisrestrictedtoa
2Dgridandremainsvisibleonthescreentonavigateandinteractwithelements,theutilizationof
themousein3DgameslikeRDR2introducesavariedcontrolscheme. Inmenuscreens,themouse
behaves traditionally, offering familiar point-and-click functionality. However, during gameplay,
the mouse cursor disappears, requiring players to move the mouse according to specific action
semantics. Forexample,toalterthecharacter’sviewpoint,theplayerneedstomaptheactualmouse
movement to in-game direction angle changes, which differ in magnitude in the X and Y axes.
Anotherspecialtransitionappliestoshootingmode,wherethefrontsightisfixedatthecenterofthe
screen,andplayersmustmaneuverthemousetoalignthesightwithtargetenemies. Thisnuanced
approachtomousecontrolindifferentcontextsaddsanextralayerofchallengetogeneralcomputer
handling,showcasingtheadaptabilityrequiredingameenvironments,comparedtoregularsoftware
applications.
ProceduralMemory. Proceduralmemorystorespre-definedbasicskillsandthegeneratedskills
captured from the skill curation. However, as we continuously obtain new skills during game
playing,thenumberofskillsinproceduralmemorykeepsincreasing,anditishardforGPT-4Vto
preciselyselectthemostsuitableskillfromthelargememory. Thus, weproposefirsttoretrieve
asubsetofskills,thatarerelevanttothegiventask,andthenletGPT-4Vselectthemostsuitable
onefromthesubset. Intheskillretrieval,wepre-computetheembeddingsofthedocumentations
(code, comments and descriptions) of skill functions, which describe the skill functionality, and
computetheembeddingofthegiventask. Thenwecomputethecosinesimilaritiesbetweentheskill
documentationembeddingsandthetaskembedding. Thehighersimilaritymeansthattheskill’s
functionalityismorerelevanttothegiventask. Weselectthemostsimilar10skillsasthesubset.
Usingsimilaritymatchingtoselectasmallcandidatesetsimplifiestheprocessofchoosingskills. We
useOpenAI’stext-embedding-ada-002modeltocomputeembeddingsfortheskillinformation.
Inourtargetsetting,Weintendtolettheagentlearnallskillsfromscratch,totheextentpossiblefor
themainstorylinemissions. Theproceduralmemoryisinitializedwithonlypreliminaryskillsfor
basicmovement,whicharenotclearlyprovidedbythein-gametutorialandguidance.
• turn(degree),move_forward(duration): Sincethegamedoesnotpreciselyintroducehow
tomoveintheworldthroughin-gameinstructions,weprovidethesetwobasicactionsin
advance,soGPT-4Vcanperformbasicmobility,whilegreatlyreducingthenumberofcalls
tothemodel.
• shoot(x, y): RDR2 also does not provide detailed instructions on how to aim and shoot.
Moreover, due to limitations with GPT-4V spatial reasoning and the need to sometimes
augmentimageswithobjectboundingboxes,weprovidesuchbasicskillfortheagentto
completerelevanttasks.
• select_item_at(x,y): Similarlytoshoot(),duetothelackofinstructions,weprovidesuch
skillfortheagenttomovethemousetoacertainplacetoselectagivenitem.
Beyondthesebasicatomiclow-levelactions,weintroduceafewcompositeskillstofacilitatethe
gameplayingprogress. Theagentshouldbeabletocompletetasksusingonlythebasicskillsabove
andtheskillsitlearns,butthesecompositeskillsstreamlinetheprocessbygreatlyreducingcallsto
thebackendmodel.
• turn_and_move_forward(degree,duration): Thisskillisjustasimplecompositionofturn()
andmove_forward()tosavefrequentcallstoGPT-4Vinacommonsequence.
8PythonlibraryencapsulatingMicrosoft’sDirectInputcallsforconveniencemanipulatingkeyboardkeys-
https://github.com/learncodebygaming/pydirectinput
9PythonlibrarythatprovidesCcompatibledatatypes,andallowscallingfunctionsinDLL/.sobinaries-
https://docs.python.org/3/library/ctypes.html
22• follow(duration)andnavigate_path(duration): InRDR2,tasksoftenguideplayerstofollow
NPCsorgeneratedpaths(redlines)intheminimaptocertainlocations. Thiscanbereliably
accomplishedviathebasicmovementskills,butrequiresnumerousinteractionswithGPT-
4V.TocontrolbothcostandtimebudgetsinvolvingGPT-4V’sresponses,weleveragethe
informationshownintheminimaptoimplementacompositeskilltofollowtargetNPCsor
redlinesforashortsetofgameiterations.
• fight(): AsoutputofaninteractionwithGPT-4V,theagentwillonlytakeoneactionperstep.
However,thoughtheactionisgeneratedcorrectly,specificallyinfightscenarios,theaction
frequencymaynotbehighenoughtodefeatanopponent. Inordertoallowsub-second
punches,weprovideapre-definedactionthatwrapsthismulti-actionpunching,whichcan
beselectedbyGPT-4Vtoeffectivelywinfights.
Fortheopen-endedmission,sincetheagentskipsallthetutorialsinChapterI,weprovideallthe
necessaryskillsintheproceduralmemoryatthebeginningofthemission.
EpisodicMemorystoresalltheusefulinformationprovidedbytheenvironmentandLMM,which
consistsofshort-termmemoryandlong-termsummary.
Theshort-termmemorystoresthescreenshotswithintherecentkinteractionsingameplayingandthe
correspondinginformationfromothermodules,e.g.,screenshotdescriptions,taskguidance,actions,
andreasoning. Wesetktofive,anditcanberegardedasthememorylength. Informationstored
over k interactions ago will be forgotten from direct short-term memory. Empirically, we found
thatrecentinformationiscrucialfordecision-making,whileatoo-longmemorylengthwouldcause
hallucinations. Inaddition,othermodulescontinuouslyretrieverecentinformationfromshort-term
memoryandupdatetheshort-termmemorybystoringthenewestinformation.
Forsomelong-horizontasks,short-termmemoryisnotenough. Thisisbecausethecompletionof
along-horizontaskmightrequirehistoricalinformationfromalongstepsago. Forexample,the
agentmightdoaseriesofshort-horizontasksduringalong-horizontask,whichmakestheoriginal
long-horizontaskforgotteninshort-termmemory. Tomaintainthelong-termvaluableinformation
whileavoidingthelong-tokenburdenofGPT-4V,weproposearecurrentinformationsummaryas
long-termmemory,whichisthetextsummarizationofexperiencesingameplaying,includingthe
ongoingtask,thepastentitiesthattheplayermet,andthepastbehaviorsoftheplayerandNPCs.
Inmoredetail,weprovideGPT-4Vwiththesummarizationbeforethecurrentscreenshotandthe
recentscreenshotswithcorrespondingdescriptions,andGPT-4Vwillmakeanewsummarization
byorganizingthetasks,entities,andbehaviorsinthetimeorderwithsentencenumberrestriction.
Thenweupdatethesummarizationtobethenewlygeneratedone,whichincludestheinformationin
thecurrentscreenshot. Therecurrentsummarizationupdate,inspiredbyRNN,achieveslinear-time
inferencebypreservingahiddenstatethatencapsulateshistoricalinput. Thismethodensuresthe
compactnessofsummarizationtokenlengthsandrecentinputdata. Furthermore,theincorporationof
long-termmemoryenablestheagenttoeffectivelyretaincrucialinformationoverextendedperiods,
therebyenhancingdecision-makingcapabilities.
B.2 ApplicationTargetandSettingChallenges
Choosinga complexgamelikeRDR2introducesits ownset ofchallengesbeyond justtheGCC
settingandacomplexapplicationtarget.
B.2.1 ModellatencyandGamepause
RDR2isadynamicgamewhereeventshappeninareal-time-likemanner. Assuch,itisunfeasibleto
simplywaitforthelatencyinGPT-4Vresponses. Duringthistimeanyeventcouldhappeninthe
gameandinvalidatethestatepassedtothebackendmodel. Tosidestepthisissueweutilizethepause
featureinthegame,similartoworkslikeVoyager[35],tofreezethegameuntilaresponsefromthe
backendmodelisreceived. However,differentlyfromgameslikeMinecraft,RDR2doesn’tprovide
instantaneouspause/resume. Thisleadstoadditionallatencybeforeandafteractions. Moreover,
pausing the game interferes with the ongoing keyboard/mouse state, which must be reset on the
resume.
23B.2.2 Mousemodes
Differently from traditional applications which use a flat 2D grid-based GUI (from web apps to
productivitysuites),games,andespecially3Dgames,makeheavyuseofdifferentstylesormodesof
mouseinteraction.Movementofthemousecanbemappedtoarcsina3Dviewcone,distancesmoved
ontheXandYaxismaybenon-uniformanddifferacrossscenesinthegame,mousecoordinates
maymovebeyondthegamescreenregion,etc. Inordertosuccessfullymanipulatethegame,both
theactionspacemustprovideforfunctionalitytoaddresstheseissuesandtheagentitselfmustbe
abletoeffectivelyleveragesuchaffordances.
InRDR2,forexample,themousemovementbehaviorisdifferentinmap-mode,regulargameplay,
andwhenaimingtheweaponinhunting/combatsituations.
B.2.3 Multi-episodes
Agent development in digital environments can also usually greatly benefit from multi-episode
simulationandlearning. Butthisrequiresbeingabletopreciselycontroltheenvironmentwherethe
agentisexecutedandresetittoexactconditionsbetweenruns.
RDR2againisamorechallengingenvironment,where: i)savingthestateofthegameisnotfreely
allowed(onlyatspecificmilestones),andii)saving/re-loadingthegamedoesnotpreciselypreserve
thestateofthegame. Forexample,ifanagentmovestoalocationinthewoods,opensthemap,and
createsapathonittoathirdlocation,evenifallowedtosavethegame,whenthegameresumes,the
mapisclosedandnopathisdefined,aswellasthelocationoftheagentsrevertstothelastgame
checkpointlocation(notwherethesavehappened).
B.3 CradlePrompts
Prompt1:GatherTextInformationprompt.
Assume you are a helpful AI assistant integrated with ’Red Dead
Redemption 2’ on the PC, equipped to handle a wide range of tasks
in the game. Your advanced capabilities enable you to process and
interpret gameplay screenshots and other relevant information.
<$image_introduction$>
Information: List all text prompts on the screenshot from the top to
the bottom, even the text prompt is one word.
All information should be categorized into one or more kinds of <
$information_type$>. If you think a piece of information is both "
A" and "B" categories, you should write information in both "A"
and "B" categories. For example, "use E to drink water" could both
be "Action Guidance" and "Task Guidance" categories.
Item_status: The helpful information to the current context in the
game, such as the cash, amount of ammo, current using item, if the
player is wanted, etc. This content should be pairs of status
names and their values. For example, "cash: 100$". If there is no
on-screen text and no item status, only output "null".
Environment_information: The information about the location, time,
weather, etc. This content should be pairs of status names and
their values. For example, "location: VALENTINE". If there is no
on-screen text and environment information, only output "null".
Notification: The game will give notifications showing the events in
the world, such as obtaining items or rewards, completing
objectives, and becoming wanted. Besides, it also contains
valuable notifications of the game’s mechanisms, such as "Health
is displayed in the lower left corner". The content must be the on
-screen text. If there is no on-screen text or notification, only
output "null".
24Task_guidance: The content should obey the following rules:
1. The content of task guidance must be an on-screen text prompt,
including the menu and the general game interface.
2. The game will give guidance on what should be done to proceed with
the game, for example, "follow Tom". This is task guidance.
3. The game will give guidance on how to perform a task using keyboard
keys or mouse buttons, for example, "use E to drink water". This
is task guidance.
4. If no on-screen text prompt or task guidance exists, only output "
null". Never derive the task guidance from the dialogue or
notifications.
Action_guidance: The game will give guidance on how to perform a task
using keyboard keys or mouse buttons; you must generate the code
based on the on-screen text. The content of the code should obey
the following code rules:
1. You should first identify the exact keyboard or mouse key
represented by the icon on the screenshot. ’Ent’ refers to ’enter
’. ’RM’ refers to ’right mouse button’. ’LM’ refers to ’left mouse
button’. You should output the full name of the key in the code.
2. You should refer to different examples strictly based on the word
used to control the key, such as ’use’, ’hold’, ’release’, ’press
’, and ’click’.
3. If ’use’ or ’press’ is in the prompt to control the keyboard key or
mouse button, io_env.key_press(’key’, 2) or io_env.mouse_press(’
button’, 2) must be used to act on it. Refer to Examples 1, 2, and
3.
4. If there are multiple keys, io_env.key_press(’key1,key2’, 2) must
be used to act on it. Refer to Example 4.
5. If ’hold’ is in the prompt to control the keyboard key or mouse
button, it means keeping the key held with io_env.key_hold or the
button held with io_env.mouse_hold (usually indefinitely, with no
duration). If you need to hold it briefly, specify a duration
argument. Refer to Examples 5 and 6.
6. All durations are set to a minimum of 2 seconds by default. You can
choose a longer or shorter duration. If it should be indefinite,
do not specify a duration argument.
7. The name of the created function should only use phrasal verbs,
verbs, nouns, or adverbs shown in the prompt and should be in the
verb+noun or verb+adverb format, such as drink_water,
slow_down_car, and ride_faster. Note that words that do not show
in the prompt are prohibited.
This is Example 1. If "press" is in the prompt and the text prompt on
the screenshot is "press X to play the card", your output should
be:
‘‘‘python
def play_card():
"""
press "x" to play the card
"""
io_env.key_press(’x’, 2)
‘‘‘
This is Example 2. If the instructions involve the mouse and the text
prompt on the screenshot is "use the left mouse button to confirm
", your output should be:
‘‘‘python
def confirm():
"""
use "left mouse button" to confirm
"""
io_env.mouse_press("left mouse button")
‘‘‘
25This is Example 3. If "use" is in the prompt and the text prompt on
the screenshot is "use ENTER to drink water", your output should
be:
‘‘‘python
def drink_water():
"""
use "enter" to drink water
"""
io_env.key_press(’enter’, 2)
‘‘‘
This is Example 4. If "use" is in the prompt and the text prompt on
the screenshot is "use W and J to jump the barrier", your output
should be:
‘‘‘python
def jump_barrier():
"""
use "w" and "j" to jump the barrier
"""
io_env.key_press(’w,j’, 3)
‘‘‘
This is Example 5. If "hold" is in the prompt and the text prompt on
the screenshot is "hold H to run", your output should be:
‘‘‘python
def run():
"""
hold "h" to run
"""
io_env.key_hold(’h’)
‘‘‘
This is Example 6. If the instructions involve the mouse and the text
prompt on the screenshot is "hold the right mouse button to focus
on the target", your output should be:
‘‘‘python
def focus_on_target():
"""
hold "right mouse button" to focus
"""
io_env.mouse_hold("right mouse button")
‘‘‘
This is Example 7. If "release" is in the prompt and the text prompt
on the screenshot is "release Q to drop the items", your output
should be:
‘‘‘python
def drop_items():
"""
release "q" to drop the items
"""
io_env.key_release(’q’)
‘‘‘
Dialogue: Conversations between characters in the game. This content
should be in the format of "character name: dialogue". For example
, "Arthur: I’m fine". If there is no on-screen text or dialogue,
only output "null".
Other: Other information that does not belong to the above categories.
If there is no on-screen text, only output "null".
Reasoning: The reasons for classification for each piece of
information.
If the on-screen text prompt is an instruction on how to perform a
task using keyboard keys or mouse buttons, it should also
classified as action guidance and task guidance.
26For action guidance, which code rules should you follow based on the
word used to control the key or button, such as press, hold,
release, and click?
The information should be in the following categories, and you should
output the following content without adding any other explanation:
Information:
1. ...
2. ...
...
Reasoning:
1. ...
2. ...
...
Item_status:
Item_status is ...
Environment_information:
Environment information is ...
Notification:
Notification is ...
Task_guidance:
Task is ...
Action_guidance:
‘‘‘python
Python code to execute
‘‘‘
‘‘‘python
Python code to execute
‘‘‘
...
Dialogue:
Dialogue is ...
Other:
Other information is ...
Prompt2:GatherSituationInformationprompt.
Assume you are a helpful AI assistant integrated with ’Red Dead
Redemption 2’ on the PC, equipped to handle a wide range of tasks
in the game. Your advanced capabilities enable you to process and
interpret gameplay screenshots and other relevant information.
<$few_shots$>
<$image_introduction$>
Current task:
<$task_description$>
Target_object_name: Assume you can use an object detection model to
detect the most relevant object for completing the current task if
needed. What object should be detected to complete the task based
on the current screenshot and the current task? You should obey
the following rules:
1. The object should be relevant to the current target or the
intermediate target of the current task. Just give one name
without any modifiers.
2. If no explicit weapon is specified in the weapon interface,
prioritize choosing ’gun’ as the weapon.
3. If no explicit shoot target is specified, prioritize choosing ’
person’ as the target.
4. If no explicit item is specified, only output "null".
5. If the object name belongs to the person type, replace it with ’
person’.
6. If there is no need to detect an object, only output "null".
277. If you are on the trade or map interfaces, only output "null".
Reasoning_of_object: Why was this object chosen, or why is there no
need to detect an object?
Description: Please describe the screenshot image in detail. Pay
attention to any maps in the image, if any, especially critical
icons, red paths to follow, or created waypoints. If there are
multiple images, please focus on the last one.
Screen_classification: Please select the class that best describes the
screenshot among "Inventory", "Radial menu", "Satchel", "Map", "
Trade", "Pause", and "General game interface without any menu".
Output the class of the screenshot in the output of
Screen_classification.
Reasoning_of_screen: Why was this class chosen for the current
screenshot?
Movement: Does the current task require the character to go somewhere?
Noun_and_Verb: The number of nouns and verbs in the current task.
Task_horizon: Please judge the horizon of the current task, i.e.,
whether this task needs multiple or only one interaction.
There are two horizon types: long-horizon and short-horizon. For long-
horizon tasks, the output should be 1. For short-horizon tasks,
the output should be 0. You should obey the following rules:
1. If the task contains only nouns without verbs, it is short-horizon.
2. If the task contains more than one verb, it is long-horizon.
3. If the task requires the character to go somewhere, it is long-
horizon.
Short-horizon tasks are sub-goals during a long-horizon task, which
only need one interaction. There are some examples of short-
horizon tasks:
1. Pick up something: To complete this task, the character needs to
execute the action "pick up" only once, so it is short-horizon.
2. Use or press [B] key: The character needs to press the key [B] only
once to talk, so it is short-horizon.
3. Talk to somebody: The character needs to press a certain button
once to complete this task, so it is short-horizon.
Long-horizon tasks are long-term goals, which usually need many
interactions. There are some examples of long-horizon tasks.
1. Go outside: The character should go outside step by step, so it is
long-horizon.
2. Approach something: The character should move closer to the target
step by step, so it is long-horizon.
3. Keep away from something, shoot, take down, or battle with
something: The character must engage in a series of interactions,
so it is long-horizon.
Reasoning_of_task: Why do you make such a judgment of task_horizon?
You should only respond in the format described below and not output
comments or other information.
Target_object_name:
Name
Reasoning_of_object:
1. ...
2. ...
...
Description:
The image shows...
Screen_classification:
Class of the screenshot
28Reasoning_of_screen:
1. ...
2. ...
...
Movement:
Yes or No
Noun_and_Verb:
1 noun 1 verb
Task_horizon:
1
Reasoning_of_task:
1. ...
2. ...
...
Prompt3:InformationSummaryprompt.
Assume you are a helpful AI assistant integrated with ’Red Dead
Redemption 2’ on the the PC, equipped to handle a wide range of
tasks in the game. You will be sequentially given <$event_count$>
screenshots and corresponding descriptions of recent events. You
will also be given a summary of the history that happened before
the last screenshot. You should assist in summarizing the events
for future decision-making.
The following are <$event_count$> successive screenshots and
corresponding descriptions:
<$image_introduction$>
The following is the summary of history that happened before the last
screenshot:
<$previous_summarization$>
Current task:
<$task_description$>
Info_summary: Based on the above input, please make a summary from the
screenshots with descriptions and the history in no less than 10
sentences, following the rules below.
1. Summarize the tasks from the history and the current task, with a
special note on the method of crucial press operations.
2. Summarize the entities and behaviors mentioned in the successive
descriptions.
3. If entities and behaviors in the history and screenshots are missed
in the descriptions, please add them to the summarization.
4. Organize the summarization as a story in order of time, including
the past entities and behaviors.
5. Only give descriptions; do not provide suggestions.
Entities_and_behaviors: Entities and behaviors which are summarized, e
.g., The entities include the player’s character, the target
character, and horses for both the player and the target. The
behaviors consist of the player character riding horseback,
following the target on horseback, and moving forward to maintain
a distance behind the target.
The output should be in the following format:
Info_summary:
The summary is...
Entities_and_behaviors:
The summary is...
Prompt4:Self-Reflectionprompt.
29Assume you are a helpful AI assistant integrated with ’Red Dead
Redemption 2’ on the PC, equipped to handle a wide range of tasks
in the game. Your advanced capabilities enable you to process and
interpret gameplay screenshots and other relevant information.
Your task is to examine these inputs, interpret the in-game
context, and determine whether the executed action takes effect.
Current task:
<$task_description$>
Last executed action:
<$previous_action$>
Implementation of the last executed action:
<$action_code$>
Error report for the last executed action:
<$executing_action_error$>
Reasoning for the last action:
<$previous_reasoning$>
Valid action set in Python format to select the next action:
<$skill_library$>
<$image_introduction$>
Reasoning: You need to answer the following questions step by step to
get some reasoning based on the last action and sequential frames
of the character during the execution of the last action.
1. What is the last executed action not based on the sequential frames
?
2. Was the last executed action successful? Give reasons. You should
refer to the following rules:
- If the action involves moving forward, it is considered unsuccessful
only when the character’s position remains unchanged across
sequential frames, regardless of background elements and other
people.
3. If the last action is not executed successfully, what is the most
probable cause? You should give only one cause and refer to the
following rules:
- The reasoning for the last action could be wrong.
- Not holding enough time should not be considered in this part.
- If it is an interaction action, the most probable cause was that the
action was unavailable or not activated at the current place.
- If it is a movement action, the most probable cause was that you
were blocked by seen or unseen obstacles.
- If there is an error report, analyze the cause based on the report.
You should only respond in the format as described below:
Reasoning:
1. ...
2. ...
3. ...
...
Prompt5:ActionPlanningprompt.
You are a helpful AI assistant integrated with ’Red Dead Redemption 2’
on the PC, equipped to handle various tasks in the game. Your
advanced capabilities enable you to process and interpret gameplay
screenshots and other relevant information. By analyzing these
inputs, you gain a comprehensive understanding of the current
context and situation within the game. Utilizing this insight, you
30are tasked with identifying the most suitable in-game action to
take next, given the current task. You control the game character
and can execute actions from the available action set. Upon
evaluating the provided information, your role is to articulate
the precise action you would deploy, considering the game’s
present circumstances, and specify any necessary parameters for
implementing that action.
Here is some helpful information to help you make the decision.
Current task:
<$task_description$>
Memory examples:
<$memory_introduction$>
<$few_shots$>
<$image_introduction$>
Last executed action:
<$previous_action$>
Reasoning for the last action:
<$previous_reasoning$>
Self-reflection for the last executed action:
<$previous_self_reflection_reasoning$>
Summarization of recent history:
<$info_summary$>
Valid action set in Python format to select the next action:
<$skill_library$>
Minimap information:
<$minimap_information$>
Based on the above information, you should first analyze the current
situation and provide the reasoning for what you should do for the
next step to complete the task. Then, you should output the exact
action you want to execute in the game. You should respond to me
with:
Reasoning: You should think step by step and provide detailed
reasoning to determine the next action executed on the current
state of the task. You need to answer the following questions step
by step. You cannot miss the question number 13:
1. Only answer this question when the catalogue, menu, map, or
inventory are open. You should first describe each item in the
screen line by line, from the top left and moving right. Is the
target item in the current screen?
2. Only answer this question when the catalogue, menu, map, or
inventory are open. Which item is selected currently?
3. Only answer this question when the character is visible in the
screenshot of the current step. Where is the character in the
screenshot of the current step?
4. Where is the target in the screenshot of the current step based
on the task description, on the left side or on the right side?
Does it appear in the previous screenshots?
5. Are there any bounding boxes with coordinates values and object
labels, such as "door x = 0.5, y = 0.5", shown in the screenshot?
The answer must be based only on the screenshot of the current
step, not from any previous steps. If the answer is no, ignore the
questions 6 to 8.
316. You should first describe each bounding box, from left to right
. Which bounding box is more relevant to the target?
7. What is the value x of the most relevant bounding box only in
the current screenshot? The value is the central coordination (x,y
) of the central point of the box.
8. Based on the few shots and the value x, where is the relevant
bounding box in the current screenshot? Clearly on the left side,
slightly on the left side, in the center, slightly on the right
side, or clearly on the right side?
9. Only answer this question when the catalogue, menu, map, or
inventory are not open. Summarize the contents of recent history,
mainly focusing on the historical tasks and behaviors.
10. Only answer this question when the catalogue, menu, map, or
inventory are not open. Summarize the content of self-reflection
for the last executed action, and do not be distracted by other
information.
11. What was the previous action? If the previous action was a
turn, was it a left or a right turn?
12. Based on Actions Rule 12, do you need to consider or ignore
the angle information from the minimap? If considering it,
summarize the content of the minimap information.
13. This is the most critical question. Based on the action rules
and self-reflection, what should be the most suitable action in
the valid action set for the next step? You should analyze the
effects of the action step by step.
Actions: The best action, or short sequence of actions without gaps,
to execute next to progress in achieving the goal. Pay attention
to the names of the available skills and to the previous skills
already executed, if any. You should also pay more attention to
the following rules:
1. You should output actions in Python code format and specify any
necessary parameters to execute that action. If the function has
parameters, you should also include their names and decide their
values, like "move(duration=1)". If it does not have a parameter,
just output the action, like "mount_horse()".
2. Given the current situation and task, you should only choose
the most suitable action from the valid action set. You cannot use
actions that are not in the valid action set to control the
character.
3. If the target is not on the catalogue, menu, or inventory, you
MUST choose the skill ’view_next_page’. For the map, ignore the
skill ’view_next_page’.
4. If the minimap information exists, it may include angle
information for red points, yellow points, or yellow regions.
Angle information specifies the direction of the corresponding
point or area. A negative angle indicates the left side, while a
positive value signifies the right side. If the angle is 30, the
corresponding point or area is 30 degrees to the character’s right
. If the angle is -50, the corresponding point or area is 50
degrees to the character’s left. Do not doubt the correctness of
these angles; you can refer to them when you approach these points
or regions.
5. When you decide to control the character to move, if the
relevant bounding box is clearly on the left side in the current
screenshot, you MUST turn left with a big degree. If the relevant
bounding box is slightly on the left side in the current
screenshot, you MUST turn left with a small degree. If the
relevant bounding box is clearly on the right side in the current
screenshot, you MUST turn right with a big degree. If the relevant
bounding box is slightly on the right side in the current
screenshot, you MUST turn right with a small degree. If the
relevant bounding box is on the central side of the current
screenshot, you can choose to move forward.
326. When you decide to control the character to move, if yellow
regions or yellow points exist in minimap information, they are
related to the current task or instruction. This implies that you
should approach within the yellow region or approach the yellow
points. You can refer to the corresponding angle information when
deciding to approach these regions or points. If red points exist
in the minimap information, they are also related to the current
task or instruction. This implies that you should turn towards
them, and you can also refer to the corresponding angle
information.
7. When you decide to control the character to move, if minimap
information does not exist, the ’theta’ you use to turn MUST be
more than 10 degrees and less than 60 degrees.
8. When you decide to control the character to move, if you are in
a normal road condition, the ’duration’ you use to move forward
should be 1 second. If you have bad road conditions, such as snow,
and grass, that can slow you down, the ’duration’ you use to move
forward should be 2 second.
9. When you are exploring or searching a place, if you are leaving
the place, you MUST make a sharp turn to face the inside of the
place. Any values for degrees are allowed.
10. If upon self-reflection you think the last action was
unavailable at the current place, you MUST move to another place.
11. If upon self-reflection you think you were blocked, you MUST
make a moderate turn in the same direction as the previous turn
action and move forward, so that you can pass obstacles.
12. You MUST ignore the angle information provided by the minimap
in the following situations: when you think you were blocked based
on self-reflection or when you were inside the highlighted area
in the minimap.
13. When you are indoors, or the current task does not imply
following, you MUST not use the follow action.
14. When you are outdoors, and the current task implies following,
you MUST use the follow action.
15. If the game fails, you MUST retry from the latest checkpoint,
not restart from the beginning of the mission.
You should only respond in the format described below, and you should
not output comments or other information:
Reasoning:
1. ...
2. ...
3. ...
Actions:
‘‘‘python
action(args1=x,args2=y)
‘‘‘
33