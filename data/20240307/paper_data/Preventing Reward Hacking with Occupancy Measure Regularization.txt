Preventing Reward Hacking with Occupancy Measure Regularization
CassidyLaidlaw*1 ShivamSinghal*1 AncaDragan1
Abstract 1.Introduction
A major challenge for the designers of goal-oriented AI
systemsisspecifyingarewardfunctionthatrobustlycap-
Rewardhackingoccurswhenanagentperforms
tures their goals and values. Manually designing reward
verywellwithrespecttoa“proxy”rewardfunc-
functions is difficult due to the ambiguities and complex
tion (which may be hand-specified or learned),
variablesunderlyingreal-worldscenarios(Ibarzetal.,2018).
but poorly with respect to the unknown true re-
Analternativeistolearnrewardfunctionsfromhumandata
ward. Sinceensuringgoodalignmentbetweenthe
(Sadighetal.,2017;Jeonetal.,2020),buttheseoftenfailto
proxyandtruerewardisextremelydifficult,one
generalizeoutsidethedistributionofbehaviorseenduring
approachtopreventrewardhackingisoptimizing
training(McKinneyetal.,2023;Tienetal.,2023). Thus,
theproxyconservatively. Priorworkhasparticu-
alearnedorhand-specifiedrewardfunctionisoftenjusta
larlyfocusedonenforcingthelearnedpolicyto
proxyforthetruerewardunderlyingthesystemdesigner’s
behavesimilarlytoa“safe”policybypenalizing
intent. Misalignmentbetweenthetwoobjectivescanleadto
theKLdivergencebetweentheiractiondistribu-
rewardhacking: alearnedpolicyperformswellaccording
tions(AD).However,ADregularizationdoesn’t
totheproxyrewardfunction,butnotaccordingtothetrue
alwaysworkwellsinceasmallchangeinaction
rewardfunction(Russelletal.,2010;Amodeietal.,2016;
distribution at a single state can lead to poten-
Pan et al., 2022; Skalse et al., 2022). A reward hacking
tiallycalamitousoutcomes,whilelargechanges
policy’sbehaviorisoftenundesirableandcanbeespecially
might not be indicative of any dangerous activ-
catastrophicwhendeployedinsafety-criticalscenarios,such
ity. Ourinsightisthatwhenrewardhacking,the
asautonomousdriving(Krakovnaetal.,2019;Turneretal.,
agentvisitsdrasticallydifferentstatesfromthose
2019; Knox et al., 2022). Unfortunately, reward hacking
reached by the safe policy, causing large devia-
is a common phenomenon (Krakovna, 2018), which has
tionsinstateoccupancymeasure(OM).Thus,we
problematicimplicationsintherealworld(Lum&Isaac,
proposeregularizingbasedontheOMdivergence
2016;Corbett-Daviesetal.,2017;Obermeyeretal.,2019;
betweenpoliciesinsteadofADdivergencetopre-
Millietal.,2021;Piersonetal.,2021;Franchietal.,2023;
ventrewardhacking. Wetheoreticallyestablish
Kleinbergetal.,2023).
thatOMregularizationcanmoreeffectivelyavoid
largedropsintruereward. Then,weempirically One method to prevent reward hacking is to avoid fully
demonstrateinavarietyofrealisticenvironments optimizingtheproxyrewardfunctionbyusingconstraints
thatOMdivergenceissuperiortoADdivergence orregularization. Inparticular,priorworkhasregularized
forpreventingrewardhackingbyregularizingto- thechosenactionsofalearningpolicytobesimilartothose
wardsasafepolicy. Furthermore,weshowthat ofaknownsafepolicy(Yangetal.,2021). Asafepolicyis
occupancymeasuredivergencecanalsoregularize anypolicythathasreasonable(althoughpotentiallyquite
learnedpoliciesawayfromrewardhackingbehav- suboptimal) performance and does not reward hack; safe
ior. Ourcodeanddataareavailableathttps: policiescanbehard-codedorlearnedfromhumandata. For
//github.com/cassidylaidlaw/orpo. example,RLHFforLLMsgenerallyoptimizesthelearned
rewardinadditiontoatermthatpenalizesdivergencefrom
thepre-trainedlanguagemodel’soutput(Glaeseetal.,2022;
Ouyangetal.,2022). Intuitively,thiskindofregularization
*Equal contribution 1Department of Electrical Engineer- pushesthelearnedpolicyawayfrom“unusual”behaviors
ing and Computer Science, University of California, Berke- forwhichtherewardfunctionmaybemisaligned.
ley, CA, USA. Correspondence to: Cassidy Laidlaw <cas-
sidy laidlaw@berkeley.edu>, Shivam Singhal <shivamsing- The goal of optimizing a policy with regularization is to
hal@berkeley.edu>. achievehighertruerewardthanthesafepolicywhileavoid-
ingrewardhacking. Todosoeffectively,wemustchoosea
Copyright2024bytheauthor(s).
1
4202
raM
5
]GL.sc[
1v58130.3042:viXraPreventingRewardHackingwithOccupancyMeasureRegularization
regularizationregimethatissimultaneouslystrongenough RegularizedPolicyOptimization(ORPO)thatcanbeeas-
topreventthelearnedpolicyfromrewardhacking, while ilyincorporatedintodeepRLalgorithmslikeProximalPol-
alsobeingsufficientlylenienttoensurethelearnedpolicy icy Optimization (PPO) (Schulman et al., 2017). ORPO
outperformsthesafepolicy. Wearguethatinmanycases, approximatestheoccupancymeasuredivergencebetween
regularizingbasedontheactiondistributions(AD)ofpoli- policiesusingadiscriminatornetwork. WeuseORPOto
ciesmakesitimpossibletoachievethisgoal.Thisisbecause optimizepoliciestrainedwithmisalignedproxyrewardfunc-
small shifts in action distribution can lead to large differ- tionsinmultiplerewardhackingbenchmarkenvironments
encesinoutcomes, butlargeshiftsinactiondistributions (Panetal.,2022)andcompareittoADregularization. The
maynotcauseanydifferenceinoutcome. Asanexample, resultsofourexperimentsdemonstratethattrainingwithoc-
imagineanautonomouscardrivingalongsideasteepcliffon cupancymeasureregularizationleadstobetterperformance
acoastalhighway. Supposewehaveaccesstoasafepolicy undertheunseentruerewardfunctioninalloftheenviron-
thatdrivesslowlyandavoidsfallingoffthecliff. However, ments. In contrast, we find that it is difficult to tune AD
the car is optimizing a proxy reward function that priori- regularizationinsomeenvironmentstobothpreventreward
tizesquicklyreachingthedestination,butnotnecessarily hackingandallowmeaningfulimprovementoverthesafe
stayingontheroad. Ifwetrytoregularizethecar’saction policy. Toexplainwhythisisthecase,weshowthat,when
distributionstothesafepolicy,wewillneedtoapplyheavy compared with AD divergence, OM divergence from the
regularization,sinceonlyslightlyincreasingtheprobability safepolicyisamuchmoreaccuratepredictorofwhetherthe
ofsomeunsafeaction(e.g.,makingasharprightturn)can learnedpolicyisrewardhacking. Thisvalidatesourtheoret-
lead to disaster. Since heavy regularization will prevent icalexplanationthatOMdivergenceismoreindicativeof
even minor deviations in action distribution, it would be thedropintheunknowntruerewardassociatedwithreward
near-impossibletoimproveuponthesafepolicy. hacking.
Ifactiondistributiondivergencesarepoorregularizersfor When a safe policy is unavailable, an alternative method
rewardhacking,whatcanwedoinstead?Inourcarexample, topreventrewardhackingistoencouragealearnedpolicy
whileasinglecatastrophicactiondoesn’tchangetheaction tohavebehaviorthatisasdifferentfromarewardhacking
distributionmuch,itdoesdrasticallychangethedistribution policyaspossible. Ourexperimentsshowthatoptimizing
overstatesvisitedbythecar. Thelearnedpolicywillhavea for the proxy reward plus OM divergence from a reward
highprobabilityofreachingstateswherethecarisoffthe hackingpolicyisalsoeffectiveatavoidingrewardhacking.
cliffandcrashed,whilethesafepolicyneverreachessuch
Ourmaincontributionscanbesummarizedasfollows:
states. Ourproposalfollowsnaturallyfromthisobservation:
to avoid reward hacking, regularize based on divergence 1. Weshowtheoreticallythatoccupancymeasureregular-
fromthesafepolicy’soccupancymeasure,ratherthanac- izationissuperiortoactiondistributionregularization
tiondistribution. Apolicy’soccupancymeasure(OM)is for preventing reward hacking because constraining
thedistributionofstatesorstate-actionpairsseenbyapol- OMdivergenceeffectivelypreventslargedropsinthe
icy when it interacts with its environment. Unlike action unknowntruerewardfunction.
distribution-basedmetrics,occupancymeasurestakeinto 2. We present the ORPO algorithm to implement OM
accountthestatesthattheagentreaches. Whilealgorithms regularizationandshowthatitoutperformsADregu-
basedonoccupancymeasureshavebeenwidelyusedfor larizationinrealisticenvironments.
imitation learning (Ho & Ermon, 2016), offline RL (Lee 3. WedemonstratethatOMregularizationcanalsobeef-
etal.,2022),andefficientexploration(Hazanetal.,2019), fectivelyusedtoregularizeawayfromrewardhacking.
using OM divergence to prevent reward hacking remains
unexplored.
2.Relatedwork
WeshowthatOM-basedregularizationissuperiortoAD
Whiletherehavebeenseparatelinesofworkinvestigating
regularizationforpreventingrewardhackinginboththeory
reward hacking and exploring the use of occupancy mea-
andpractice. Theoretically,weshowthatthereisabound
suresforotherapplications,tothebestofourknowledge,
onthedifferenceinreturnoftwopoliciesunderanyreward
we are the first to specifically study applying occupancy
functionbasedonthedivergencebetweentheiroccupancy
measurestotheproblemofrewardhacking.
measures. Thus, constraining the OM divergence from a
safe policy can prevent the large drop in true reward as- Rewardhacking: Somepriorworksestablishtheoretical
sociatedwithrewardhacking,evenwhenthetruereward modelsofrewardhackingasaspecialcaseofGoodhart’s
functionisunknown. Incontrast,onlymuchweakerguar- Law(Goodhart,1984;Leikeetal.,2018;Krakovna,2019;
anteescanbeestablishedforADdivergence. Skalse et al., 2022; Ngo et al., 2023). Krakovna (2018)
provide a list of many examples of reward hacking. Pan
Empirically, we derive an algorithm called Occupancy-
etal.(2022)categorizetypesofrewardmisspecificationand
2PreventingRewardHackingwithOccupancyMeasureRegularization
relateoptimizationpowertorewardhacking. 3.ActionDistributionvs. OccupancyMeasure
Regularization
Safereinforcementlearning: Regularizingpoliciestobe
similartoanofflinepolicybasedontheiractiondistribution
Webeginbytheoreticallyandconceptuallymotivatingwhy
KLdivergenceswasfirstproposedbyStiennonetal.(2020)
occupancymeasureregularizationshouldbesuperiortoac-
andhassincebeenwidelyemployedinthecontextofop-
tiondistributionregularizationforpreventingrewardhack-
timizingLLMsusingreinforcementlearningfromhuman
ing. We present our theoretical analysis in the setting of
feedback (RLHF) (Ouyang et al., 2022; Bai et al., 2022;
an infinite-horizon Markov decision process (MDP). An
Glaeseetal.,2022). KLregularizationforRLHFhasbeen
agent takes actions a ∈ A to transition between states
furtherstudiedbyVieillardetal.(2021),Gaoetal.(2022),
s ∈ S overaseriesoftimestepst = 0,1,2,.... Thefirst
andKorbaketal.(2022). Somealternativeapproachesto
states issampledfromaninitialdistributionµ (s),and
0 0
avoidrewardhackingincludequantilizers(Taylor,2016),
whenanagenttakesactiona ins attimet,thenextstate
t t
“mild”optimization(Tayloretal.,2020),andimpactregu-
s isreachedattimestept+1withtransitionprobabil-
t+1
larization(Turneretal.,2020). WhileconstrainedRLcan
ityp(s | s ,a ). Theagentaimstooptimizeareward
t+1 t t
preventthemisbehaviorofagentsthatoptimizeflawedre-
functionR:S×A→[0,1],andrewardsareaccumulated
wardfunctions(Dalaletal.,2018;Chowetal.,2019;Zhang
overtimewithdiscountfactorγ ∈[0,1). Apolicyπmaps
etal.,2020;Royetal.,2022),itsimplyshiftsthedifficulty
eachstatestoadistributionoveractionstotakeatthatstate
ofdesigningarewardfunctiontospecifyingasetofcon-
π(a | s). Wedefinethe(normalized)returnofapolicyπ
straintsandweights. Otherproposalstoaddressthereward
underarewardfunctionRas
specificationproblemattempttoinferthetruerewardfunc-
J(π,R)=(1−γ)E [(cid:80)∞ γtR(s ,a )]
tionbasedonthegivenproxyrewardfunction,environment π t=0 t t
context, and/or feedback from humans (Hadfield-Menell whereE referstotheexpectationunderthedistributionof
π
etal.,2017;Reddyetal.,2020;Leeetal.,2021). statesandactionsinducedbyrunningthepolicy π inthe
environment. Thenormalizingfactor1−γ guaranteesthat
Applications of occupancy measures: Many offline RL
J(π,R)∈[0,1]always.
algorithmsuseoccupancymeasure-basedregularizationto
ensurethatthelearnedpolicyremainswithinthetraining Wedefinethestate-actionoccupancymeasureµ ofapolicy
π
datadistribution(Leeetal.,2022;Mandaletal.,2023;He, πastheexpecteddiscountednumberoftimestheagentwill
2023;Chengetal.,2022;Rashidinejadetal.,2023;Xieetal., beinaparticularstateandtakeaspecificaction:
2023).Varioustypesofdistributionalregularizationareused µ (s,a)=(1−γ)E [(cid:80)∞ γt1{s =s∧a =a}].
inmodel-basedRLaswellsincelearnedmodelsmaynot π π t=0 t t
generalizeout-of-distribution(Yangetal.,2022).GAIL(Ho Intuitively,theoccupancymeasurerepresentsthedistribu-
&Ermon,2016)isanalgorithmforrobustimitationlearning tionofstatesandactionsvisitedbythepolicyovertime.
thataimstomatchtheimitator’soccupancymeasuretothat
ThestandardapproachtosolvinganMDPistofindapolicy
ofthedemonstrator.Kangetal.(2018)combinesGAILwith
πthatmaximizesitsreturn:
arewardfunctiontoefficientlyexploreusinghumandata.
Anotherlineofworkaimstofindapolicywiththehighest- maximize J(π,R). (1)
entropyoccupancymeasureforthepurposeofexploringthe
However, as we discussed in section 1, an AI system de-
statespace(Hazanetal.,2019;Leeetal.,2020;Nedergaard
signermostlikelydoesnothaveaccesstoarewardfunction
&Cook,2023).
thatperfectlyencapsulatestheirpreferences. Instead, the
Ourcontribution: Someofthesepreviousworksleverage designermightoptimizeπusingalearnedorhand-specified
occupancymeasuresandderivealgorithmsthataresimilar
proxyrewardfunctionR˜whichismisalignedwiththetrue
toourproposedORPOalgorithm.However,unlikeprevious rewardfunctionR. Blindlymaximizingtheproxyreward
work,weuseOM-basedregularizationtopreventreward functioncouldleadtorewardhacking.
hacking, which to our knowledge is a novel application.
The drawbacks of action distribution regularization:
Weviewourcontributionasdemonstratingthatoccupancy
Oneapproachtopreventingrewardhackingistooptimize
measureregularizationissuperiortoactiondistributionreg- thepolicy’sreturnwithrespecttotheproxyR˜plusaregular-
ularizationforthispurpose. Wedonotexplorethemyriad
izationtermthatpenalizestheKLdivergenceofthepolicy’s
waysthatOMregularizationcouldbeincorporatedintoRL
action distribution (AD) from a safe policy π . This is
safe
to prevent reward hacking. Instead, we focus our experi-
equivalenttosolvingthefollowingconstrainedoptimization
mentsonthesimpleandgeneralORPOalgorithm.Weleave
problem:
tofutureworkfurtherinvestigationofalternatemethodsfor
maximize J(π,R˜) s.t. (2)
preventingrewardhackingwithOM-basedregularization.
(cid:104) (cid:105)
(1−γ)E (cid:80)∞ γtD (π(·|s )∥π (·|s )) ≤ϵ.
π t=0 KL t safe t
3PreventingRewardHackingwithOccupancyMeasureRegularization
π π π∗
safe totheactiondistributionthatarenecessarytoimproveover
R(s ,a )=0
R(s1 1,a1 2)=1 R(s 2,·)=1 1 π safe. SeeFigure1foragraphicalillustrationoftheresults
inProposition3.1.
s s
a 1 1 a 2 2 0 WhiletheMDPdiscussedinProposition3.1representsa
0 1
particularlybadcaseforADregularization,wearguethat
a 1,a 2 π(a 2 |s 1) realisticenvironmentsoftenhavethesameissues. Inmany
Figure1.TheMDPontheleft,similartothatusedintheproofof safety-critical environments, even slightly increasing the
Proposition3.1,demonstratesonedrawbackofusingdivergence probabilityoftakinganunsafeactioncangreatlyreducetrue
betweenpolicies’actiondistributionsforregularization.Theagent
reward,aspositedinpart1oftheproposition. Furthermore,
staysinstates ,whereitreceivesnoreward,untilittakesaction
1 safepoliciesareoftennon-robustout-of-distribution(OOD),
a ,afterwhichitremainsinstates foreverandreceives1reward
2 2 e.g.,apolicylearnedfromhumandatamighttakeunusual
pertimestep. TheplotontherightshowsthereturnJ(π,R)for
actionsinstatesoutsidethedistributionofthosenormally
apolicyπ whenγ = 0.99asafunctionofthepolicy’saction
distributionats .Whileπandπ (shownontheplotasdotted visited. Thus,takingjustasingleunusualactioncouldlead
1 safe
lines) are close in action distribution space, they achieve very to an OOD state in which the safe policy is no longer a
differentreturns. Meanwhile,theoptimalpolicyπ∗ isfarfrom meaningfulregularizationtarget;thisalsomeanssmallAD
π inactiondistributionspace. Propositions3.2andA.2show divergencecanleadtolargedropsinreward.
safe
thatoccupancymeasuredivergencesdonothavethesedrawbacks.
Thebenefitsofoccupancymeasureregularization: Due
tothedrawbacksofactiondistributionregularization,we
Intuitively, the aim of the AD constraint in (2) is to pre- proposepreventingrewardhackingbyregularizingthedi-
venttheunusualbehaviorassociatedwithrewardhacking vergencebetweentheoccupancymeasuresofthelearned
policiesbyconstrainingπtotakesimilaractionstoπ safe. andsafepolicies:
WhileADregularizationissimpleandeasytoimplement,
maximize J(π,R˜) s.t. ∥µ −µ ∥ ≤ϵ. (3)
thismethodalsohasseriousdrawbacks.Inparticular,thefol- π πsafe 1
lowingpropositionshowsthatinsomecasessmallchanges
In (3), we use the total variation (TV) between the occu-
in action distribution from a safe policy can induce large
pancymeasures,definedas
dropsintruereward,butlargechangesinADarenecessary
toimproveonthesafepolicy. ∥µ −µ ∥ =(cid:80) |µ (s,a)−µ (s,a)|.
π πsafe 1 (s,a)∈S×A π πsafe
Proposition3.1. Fixc > 0andδ > 0arbitrarilysmall,
1
andc ≥ 0arbitrarilylarge. ThenthereisanMDP,true Why should using the occupancy measure divergence to
2
rewardfunctionR,andsafepolicyπ wherebothofthe regularizeperformbetterthanusingthedivergencebetween
safe
followinghold: actiondistributions? Ideally,unlikeactiondistributiondi-
vergence,thereshouldbeacloserrelationshipbetweenthe
1. Thereisapolicyπ wheretheactiondistributionKL
rewardsoftwopoliciesandtheiroccupancymeasurediver-
divergencesatisfies
gence. Infact,itispossibletoshowthatthedifferencein
(1−γ)E (cid:104) (cid:80)∞ γtD (π(·|s )∥π (·|s ))(cid:105) ≤c returnsbetweentwopoliciesforanyrewardfunctioncanbe
π t=0 KL t safe t 1 boundedbytheiroccupancymeasuresdivergence:
butJ(π safe,R)−J(π,R)≥1−δ. Proposition 3.2. For any MDP, reward function R, and
2. Anyoptimalpolicyπ∗ ∈argmax πJ(π,R)satisfies pairofpoliciesπ,π safe,wehave
(1−γ)E π∗(cid:104) (cid:80)∞ t=0γtD KL(π∗(·|s t)∥π safe(·|s t))(cid:105) ≥c 2. |J(π safe,R)−J(π,R)|≤(cid:13) (cid:13)µ π−µ πsafe(cid:13) (cid:13) 1. (4)
AllproofsaregiveninAppendixA.ThefirstpartofPropo-
ResultsequivalenttoProposition3.2havebeenshownby
sition 3.1 states that in the worst case, a policy with AD
Xuetal.(2020)amongothers,butthisresulthasnotbeen
divergencefromthesafepolicybelowsomearbitrarilysmall
appliedbeforeinthecontextofrewardhacking. Forcom-
thresholdc caninduceadropinreturnunderthetruere-
1 pletenesswegiveaproofwithournotationinAppendixA.2.
wardfunctionRthatisalmostaslargeastheentirepossible
rangeofreturns. Thus,wemustsettheADdivergencecon- Proposition3.2suggeststhatOMregularizationcaneffec-
straintverysmall(i.e.,ϵ≪c )topreventrewardhacking. tivelypreventthelargedropsintruerewardassociatedwith
1
ThesecondpartofProposition3.1showsthatinthesame rewardhacking,evenwhenthetruerewardisunknown.Sup-
MDP,itisnecessarytochangetheactiondistributionbyan posethereturnsofallrewardhackingpoliciesπ satisfy
hacking
arbitrarilylargedivergencec toimproveonthesafepolicy J (π )<J (π )−C,i.e.,rewardhackingpolicies
2 R hacking R safe
andreachanoptimalpolicy. Thus,ifwesetϵ≪c topre- havetruerewardthatissmallerthanthatofthesafepolicy
1
ventrewardhacking,itwillnotallowforthelargechanges bymorethanC.Then,settingtheOMdivergenceconstraint
4
)R,π(JPreventingRewardHackingwithOccupancyMeasureRegularization
Desiredpolicy Safepolicy Rewardhackingpolicy
1
KL=43.0 KL=18.2
0
100
TV=0.8 TV=1.5 10−1
10−2
Truereward=14.9 Truereward=13.0 Truereward=2.2
Proxyreward=37.0 Proxyreward=37.3 Proxyreward=153.4
A={LEFT,RIGHT,UP,DOWN,STAY}
Figure2.Thisadaptionofthetomato-wateringAISafetyGridworld(Leikeetal.,2017)providesanintuitiveexampleofwhyOM
divergenceissuperiortoADdivergenceforregularizingtoasafepolicy.Therobotagentcanmoveup,down,left,right,orstayinplace.
ThetruerewardfunctionRonlyrewardswateringtomatoes,whiletheproxyrewardfunctionR˜alsohighlyrewardsreachingthesprinkler.
ADdivergence Thetoprowshowsthreepoliciesforthisenvironment:adesiredpolicythatachievesthehighesttruereward,asafe
policythatachieveslowertruereward,andarewardhackingpolicythatexploitsthesprinklerstatetoachievehighproxyrewardbutlow
truereward. TheADKLdivergencesbetweenthepolicies,shownoverthearrowsconnectingthem,suggestthattherewardhacking
policyisactuallyclosertothesafepolicythanthedesiredpolicyis.Thus,ifweregularizetothesafepolicyusingactiondistributionKL
divergence,wewouldbemorelikelytofindapolicythathackstheproxyreward,ratherthanoneliketheleftpolicy,whichweprefer.
OMdivergence Thebottomrowshowstheoccupancymeasuresforeachpolicyinthetoprow,andthearrowsbetweenthecolumns
showthetotalvariationdistance∥µ−µ∥ .Thedesiredpolicyontheleftisclosertothesafepolicythantherewardhackingpolicyis
1
inOMdivergence. Thisisbecauseboththedesiredandsafepoliciesspendmostoftheirtimeactuallywateringtomatoes,whilethe
rewardhackingpolicymainlyvisitsthesprinklerstate.Thus,ifwetrainedapolicyregularizedwithoccupancymeasuredivergenceinthis
environment,wecouldhopetofindapolicylikethedesiredoneontheleftandavoidarewardhackingpolicyliketheoneontheright.
ϵ=C in(3)willpreventrewardhacking,sinceanypolicy strictassumptionswhicharesatisfiedalmostexclusivelyin
withintheconstraintmustsatisfyJ (π)≥J (π )−C thecurrentRLHF-for-LLMsparadigm. Formoregeneral
R R safe
byProposition3.2. C isoftenlargeinpracticesincereward environments,therecanbesignificantdifferencesbetween
hackinginducesalargedropintruereward. Thus,wecan actiondistributionandOM-basedregularization,asclearly
usealargeconstraintboundϵin(3)thatallowsimprovement demonstratedbyourexperiments.
overthesafepolicywhilestillpreventingrewardhacking.
Althoughitispossibletoproveasimilarboundto(4)using 4.Occupancy-regularizedpolicyoptimization
actiondistributiondivergence,ithasa 1 prefactor(Xu (ORPO)
1−γ
etal.,2020),meaningthataconstraintonADdivergence
In the previous sections, we showed theoretical evidence
must be set 1−γ times the equivalent OM constraint to
thatregularizingRLbyconstrainingOMdivergenceissu-
obtainanequivalentguaranteeaboutthetruereward. Thus,
periortoconstrainingADdivergence. Wenowintroduce
inenvironmentswithhighdiscountfactors—i.e.,mostre-
analgorithm,Occupancy-RegularizedPolicyOptimization
alisticenvironments—theconstraintmustbesettoavalue
(ORPO),tofeasiblyapproximatetheoccupancymeasure
toosmalltoallowmeaningfulimprovementoverπ .
safe
divergence between the learned and safe policies for the
An illustrative example: See Figure 2 for an example purposeofregularizingdeepRLagents.
ofwhyOMregularizationoutperformsADregularization.
WhileourtheoryusestheTVdistancebetweenoccupancy
While in this example it is particularly obvious that OM
measures, we find that the KL divergence is more stable
regularizationshouldworkbetter,wefindinSection5that
to calculate in practice. Since Pinsker’s inequality and
OMoutperformsADinmorerealisticenvironmentstoo.
the Bretagnolle-Huber inequality show that TV distance
WhydoesADregularizationworkforLLMs? Despite isupper-boundedintermsofKLdivergence,ourtheoretical
thedrawbacksofactiondistributionregularizationintheory, guaranteesremainvalidinthecaseofOMKL(Canonne,
ithasperformedwellinpracticewhenusedaspartofRLHF 2022). Ourobjectivefrom(3)canbereformulatedwiththe
forlargelanguagemodels(LLMs)(Stiennonetal.,2020; KLdivergenceinplaceoftheTVdistanceandaLagrangian
Gaoetal.,2022). InAppendixA.4,weshowthatforcur- relaxationinplaceofthehardconstraint:
rentimplementationsofRLHFforLLMs,actiondistribution
andOM-basedregularizationareactuallyequivalent. Thus, maximize J(π,R˜)−λD KL(µ π ∥µ πsafe). (5)
RLHFforLLMsisessentiallyalreadyusingoccupancymea-
sureregularization. However,thisisonlytrueundercertain Weoptimize(5)usingagradient-basedmethod. Thegra-
dient of the first term is estimated using PPO, a popular
5
.borpnoitcA
erusaem.ccOPreventingRewardHackingwithOccupancyMeasureRegularization
Tomato Traffic Glucose Pandemic
-1e3 -5e4
5 -2e3 -8e4 −10
−30
-5e4 -3e5
0 -1e5 -6e5 −50
1 5 10 2.5
0 0 0 0.0
20 20 50
5
0 0 0 0
10−3 10−1 101 10−3 10−1 101 10−3 10−1 101 10−3 10−1 101
Regularizationstrength(λ/R )
average
ORPOw/stateoccupancy PPO+actionKLregularization Policytrainedw/truereward
ORPOw/state-actionoccupancy Safepolicy
Figure3.Thetoprowofplotsinthefigureshowsthetruerewardsofpoliciestrainedwiththreetypesofregularizationtoπ forseveral
safe
valuesofλ/R : actiondistribution,state-onlyOM,andstate-actionOM.ThebottomtworowsofplotsintheplotshowtheKL
average
divergencebetweentheactiondistributionsandoccupancymeasuresofthelearnedandsafepoliciesforeachcoefficient.Forallplotsand
tables,wegivethemedianacross5seedswitherrorbarsindicatingthestandarddeviation. Wefindthatrewardhackingconsistently
occursineachenvironmentwithoutregularizationorwithverysmallregularizationcoefficientsλ.Asλisincreasedtomoderatevalues,
thelearnedpolicystopsrewardhackingandoftenimprovesuponthesafepolicy.Athighregularizationcoefficients,thelearnedpolicy
approachesthesafepolicy.
policygradientmethod(Schulmanetal.,2017). However, Notethat(7)isidenticaltothenormalRLobjectivewitha
calculatingtheoccupancymeasuredivergenceforthesec- rewardfunctionR′(s,a)=R˜(s,a)−λd(s,a). Thus,once
ondtermisintractabletodoinclosedformsinceitrequires thediscriminatorhasbeentrained,weaddthediscriminator
the enumeration of all possible state-action pairs, an im- scorestotheproxyrewardfunctionandusethecombined
possible task in deep RL. Thus, we approximate the KL valuestoupdateπwithPPO.ThetrainingprocessforORPO
divergencebetweentheoccupancymeasuresofpoliciesby consistsofiteratingbetweentwophases: oneinwhichdata
trainingadiscriminatornetwork,atechniquethathasprevi- fromboththesafeandlearnedpoliciesisusedtotrainthe
ouslybeenusedforgenerativeadversarialnetworks(GANs) discriminatortominimize(6),andoneinwhichdatafrom
(Goodfellowetal.,2014)andGAIL(Ho&Ermon,2016). thelearnedpolicyisusedtotrainthePPOagentwiththe
augmentedrewardfunctionin(7). Afterapolicygradient
Thediscriminatornetworkd:S×A→Rassignsascore
step,theprocessrepeats.
d(s,a) ∈ R to any state-action pair (s,a) ∈ S ×A, and
it is trained on a mixture of data from both the learned Regularization with state-only occupancy measure:
policyπandsafepolicyπ . Theobjectiveusedtotraind While we have thus far considered the state-action occu-
safe
incentivizeslowscoresforstate-actionpairsfromπ and pancy measure of a policy µ (s,a), it sometimes makes
safe π
highscoresforstate-actionpairsfromπ: moresensetoregularizebasedonthestate-onlyoccupancy
d=argmin
d
(cid:80)∞ t=0(cid:16) E π[γtlog(1+e−d(st,at))] m tice ua ls au rr te heµ π re( ws) ar= df( u1 n− ctiγ o) nE Rπ[ ((cid:80) s)∞ t i= n0 mγt a1 ny{s et n= virs o} n] m. eIn ntp sa ir s-
(6)
(cid:17) a function of only the state. In these cases, it is simple
+E πsafe[γtlog(1+ed(st,at))] .
toestablishsimilarguaranteestoProposition3.2basedon
justthestateoccupancymeasure, andtherefore, thestate
Husza´r(2017)provesthatifthelossfunctionin(6)ismin- occupancymeasuremightbemoreeffectiveinregularizing
imized, then the expected discriminator scores for state- the behavior of the agent. We can implement this within
actionpairsdrawnfromthelearnedpolicydistributionwill ORPObyonlyprovidingthestateasinputtothediscrimi-
approximatelyequaltheKLdivergencebetweentheoccu- natorratherthanastate-actionpair. Intuitively,regularizing
pancymeasuresofthetwopolicies: withstateOMdivergenceinenvironmentswherethereward
D (µ (s,a)∥µ (s,a))≈(1−γ)E (cid:104) (cid:80)∞ γtd(s ,a )(cid:105) . function only depends on the state avoids over-applying
KL π πsafe π t=0 t t regularizationwhenitisunnecessary. SeeAppendixA.5for
moredetails.
Applyingthedefinitionsofthelearnedpolicy’sreturnsand
Regularizingawayfromrewardhackingpolicies: While
the KL divergence between the polices’ occupancy mea-
thereisanaturalsafepolicyformanyenvironments,itmay
sures,wecannowrewriteourORPOobjective:
not always be possible to define one. In such cases, we
(cid:104) (cid:16) (cid:17)(cid:105)
maximize E (cid:80)∞ γt R˜(s ,a )−λd(s ,a ) . (7) canpotentiallyregularizeawayfromrewardhackingbehav-
π t=0 t t t t
6
DA
MO
drawereurT
LK
LKPreventingRewardHackingwithOccupancyMeasureRegularization
Environment
Method Tomato Traffic(×103) Glucose(×103) Pandemic
Actiondist.regularization(λ∗) 6.19±0.03 -1.33±0.05 -73.38±8.26 -12.20±0.06
Stateoccupancyregularization(λ∗) 7.07±0.11 -1.47±0.18 -58.39±3.36 -10.24±0.54
State-actionoccupancyregularization(λ∗) 6.80±0.05 -1.25±0.06 -48.88±0.48 -11.73±0.19
Actiondist.regularization(λ ) 4.59±0.17 -55.10±2.37 -459.92±102.08 -23.10±5.04
drop
Stateoccupancyregularization(λ ) 6.89±0.12 -1.34±22.63 -158.74±25.74 -10.60±0.78
drop
State-actionoccupancyregularization(λ ) 6.84±0.17 -1.25±0.06 -181.65±6.69 -11.88±0.72
drop
π 5.86±0.00 -2.28±0.00 -72.64±0.00 -12.26±0.00
safe
Noregularization 2.35±0.14 -57.38±3.53 -599.02±1.58 -29.57±6.86
Earlystopping(bestcase) 6.82±0.17 -2.24±0.13 -78.26±22.90 -9.18±3.86
Trainingwithtruereward 8.54±0.12 -0.93±0.11 -43.41±0.81 -2.65±0.83
Table1.Thetopthreerowsofthetablegivethemediantruerewardwhenusingtheoptimalcoefficientλ∗foreachtypeofregularization.
Themiddlethreerowsshowthetruerewardattainedwhenusingthecoefficientλ whichdecreasesADorOMdivergencethemost
drop
comparedtoaslightlysmallercoefficient.Thebottomfourrowsshowthetruerewardsforthebaselines:thesafepolicyπ ,apolicy
safe
trainedontheproxyrewardwithoutregularization(exhibitingrewardhacking),apolicytrainedwiththeproxyrewardwithearlystopping
whenthehighesttruerewardisachieved,andapolicytrainedonthetruereward. Thelattertwobaselinesareimpossibleinpractice
becausewhentruerewardisunknownbutaregivenasadditionalcomparisons.Themedianandstandarddeviationacross5randomseeds
arereported.
ior. Thatis,supposetrainingwithoutanyregularizationin onanon-rampattemptingtomergeintotrafficonahigh-
someenvironmentresultsinapolicyπ thatexhibits way. The true reward prioritizes a small mean commute
hacking
rewardhacking. Then,wecantrainasecondpolicywith time,whiletheproxyrewardistheaveragevelocityofall
thefollowingobjective: cars. Whenrewardhacking,theRLcontrolledvehicleon
the on-ramp stops indefinitely and lets cars continue for-
maximize J(π,R˜)+λD (µ ∥µ ).
KL π πhacking wardathighspeedsonthehighway,whichmaximizesthe
Unlikein(3),weencouragetheoccupancymeasureofour proxyrewardbutincreasesthecommutetimesofcarson
new policy π to be as far as possible from π . This the on-ramp infinitely. As the safe policy for the traffic
hacking
willhopefullypreventπ fromexhibitingthesamereward environmentweusedtheIntelligentDriverModel(IDM),a
hackingbehaviorasπ . ItistrivialtomodifyORPOto standardapproximationofhumandrivingbehavior(Treiber
hacking
optimizethisobjective;wesimplyneedtoflipthesignof et al., 2000). In practice, safe policies are often learned
thediscriminatortermin(7). viaimitationlearning,sotosimulatethiswegeneratedata
fromtheIDMcontrollerandtrainabehavioralcloning(BC)
policyusingthegenerateddata.
5.Experiments
SimGlucose: TheSimGlucosebloodglucosemonitoring
WenowuseORPOtocomparetheempiricalperformance environmentisanextensionoftheFDA-approvedglucose
ofoccupancymeasureandactiondistributionregularization monitoring simulator proposed by Man et al. (2014) for
infourenvironments: atomato-wateringgridworldsimilar Type1Diabetespatients. TheRLagentcontrolstheinsulin
to that in Figure 2; Flow, an autonomous vehicle control administered to a simulated patient in order to maintain
environmentintroducedbyWuetal.(2022);SimGlucose, healthyglucoselevels. Thetruerewardisastandardmea-
abloodglucosemonitoringsystemdevelopedbyFoxetal. sureofhealthriskforthepatient,buttheproxyrewardis
(2020), and a COVID-19 pandemic simulator created by misaligned and prioritizes the monetary cost of the treat-
Kompellaetal.(2020). Wechosethefirstforillustrative ment. Optimizingforacost-basedproxyhascausedmajor
purposes,andthefollowingthreebecausetheyarereward disparitiesinaccesstohealthcareonthebasisofrace(Ober-
hackingbenchmarkenvironmentsfromPanetal.(2022). meyeretal.,2019). Asthesafebaselinepolicy,wetraina
Tomatogridworld: LikeinFigure2,thetomatoenviron- BCpolicybasedondatageneratedbyaPIDcontrollerwith
mentcontainsasprinklerstatewheretheagentperceives parameterstunedbytheoriginaldesignersofthesimulator
alltomatoesasbeingwateredandthusreceiveshighproxy (Steil,2013).
rewardbutnotruereward. Wetrainasafepolicyusingthe COVID-19 simulator: The pandemic environment sim-
truerewardfunction,andthenadda10%chanceoftaking ulates a population’s infection dynamics using the SEIR
arandomactiontoensurethereisroomtoimproveuponit. model (Mwalili et al., 2020). The RL agent chooses the
Flowtrafficsimulator: Thetrafficenvironmentsimulates leveloflockdownrestrictionsplacedonthepopulationby
a group of human-controlled and RL-controlled vehicles observing the results of testing. The proxy reward func-
7PreventingRewardHackingwithOccupancyMeasureRegularization
tion omits the political cost associated with certain deci- AUROCforpredictingrewardhacking
sions. OursafepolicyistrainedviaBConacombination Environment Occ.measureKL Actiondist.KL
of hand-specified and real-world strategies employed by Tomato 1.00 0.89
governmentsduringthepandemic,whichwerealsousedby Traffic 1.00 0.98
Kompellaetal.(2020)asbaselines. Glucose 0.99 0.79
Pandemic 0.94 0.82
Regularizingtowardsasafepolicy: WetrainRLpolicies
ineachenvironmentwithactiondistributionregularization Table2.Wefindthat,comparedtoADdivergence,OMdivergence
and OM regularization to the environment’s safe policy, isamuchbetterpredictorofwhetherrewardhackingisoccurring
varyingtheregularizationcoefficientλacrossawiderange. duringtrainingaccordingtoitsareaundertheROCcurve(AU-
Sincethescaleoftherewardfunctionsvariesbetweenen- ROC). This validates that OM divergence is a more successful
vironments, we normalize λ by the typical per-timestep regularizerbecauseitmoreaccuratelyidentifieswhenrewardhack-
ingishappening.SeeFigure4forfullAUROCcurves.
rewardforeachenvironment,whichwedenoteR . In
average
theenvironmentsthatwestudied,λ/R valuesbetween
average
10−3and101seemedtoworkbest. SeeAppendixDforall decidingwhentostoptraining,weconsiderthebestpossi-
experimentaldetails. blecase: wetrainpoliciesontheproxyrewardfunctionand
thenevaluatethepolicyfromtheiterationwiththehighest
TheresultsofourexperimentsareshowninTable1andFig-
truereward. WefindthatOMregularizationissuperiorto
ure3. Ineachenvironment,wefindthatOMregularization
earlystoppinginallenvironmentsexceptforthepandemic
withtheoptimalcoefficient(λ∗)outperformsactiondistribu-
simulator. Furthermore,thisbest-caseapproachisinfeasi-
tionregularization. OMregularizationconsistentlyallows
bleinpracticesincethetruerewardisunknown,soamore
improvementovertheperformanceofπ whilepreventing
safe realisticearlystoppingmethodwouldonlyperformmore
rewardhacking;meanwhile,actiondistributionregulariza-
poorly. Ourresultsthussuggestthatpolicyregularization
tionfailstoimprovesignificantlyonthesafepolicyinthe
isusuallymoreeffectiveatpreventingrewardhackingthan
glucoseandpandemicenvironments.
earlystopping.
Wewereabletodeterminetheoptimalregularizationcoeffi-
ExplainingthesuperiorperformanceofOMregulariza-
cientsλ∗ byconsideringthepolicies’performanceonthe
tion: InSection3,wehypothesizedthatOMregularization
truereward. However,inpractice,comparingtheresultsof
issuperiortoactiondistributionregularizationbecausethere
eachtypeofregularizationwiththeoptimalcoefficientsis
isastrongerrelationshipbetweenOMdivergenceandthe
unrealistic,sincesystemdesignersmustchoosearegular-
differenceinreturnsoftwopoliciesunderanyrewardfunc-
izationregimewithoutaccesstotheunknowntruereward
tion;therefore,OMdivergenceshouldbettermeasurewhen
function. Observingchangesinthepolicies’divergencesas
thereisalargedifferenceintruerewardsbetweenthesafe
λisvariedcanhelpdesignerschoosetherightcoefficient.In
and learned policies, indicating reward hacking. We em-
particular,wefindthattheoptimalregularizationcoefficient
piricallytestthishypothesisbycomparinghowwellboth
isoftenthecoefficientatwhichtheregularizeddivergence
action distribution and OM divergence predict if reward
dropsthemostcomparedtoaslightlysmallercoefficient.
hackingisoccurringduringRLtraining. Inparticular,we
Todemonstratethis,wecomparethetruerewardsofpoli-
divide all of our training runs into ten segments, and for
cies for λ values chosen based on this heuristic—which
eachsegmentrecord(i)whethertheagentisrewardhack-
we denote as λ —in the middle three rows of Table 1.
drop ing, (ii)theactiondistributiondivergencefromπ , and
safe
RegularizingbasedonOMdivergencewithλ achieves
drop (iii)theOMdivergencefromπ . For(i),wedefinereward
truerewardclosetothoseobtainedwithλ∗,despitebeing safe
hacking as achieving higher proxy reward but lower true
chosenwithoutaccesstothetruerewardfunction.
rewardthanπ . Then,wecalculatehowaccuratelyeach
safe
Wefindthatbothstate-onlyandstate-actionoccupancymea- typeofdivergencecanpredictwhetherrewardhackingis
sureregularizationachievesimilartruereward. Generally, occurringacrossalltrainingrunsegments. Theresultsof
state-onlyoccupancymeasuresperformbetterinenviron- this experiment are shown in Table 2. We find that in all
mentswhosetruerewardfunctionsdependprimarilyonthe environments,OMdivergenceisabetterclassifierofreward
state,reflectingtheintuitionofourtheoryinAppendixA.5. hackingbehavior,validatingourhypothesisastowhyitisa
Inpractice,werecommendexperimentingwithbothOM betterregularizerforpreventingrewardhacking.
regularizers.
Regularizingawayfromrewardhackingbehavior: We
In addition to comparing OM and AD regularization, we experiment with regularizing away from reward hacking
alsotestearlystopping,whichhasbeenproposedbyKar- policiesusingbothactiondistributionandOMregulariza-
wowskietal.(2023)asanothermethodforpreventingre- tion. Weobtainaπ hackingforeachenvironmentbytraining
wardhacking. Whiletheyintroduceaspecificcriterionfor ontheproxyrewardwithoutregularization,andweregular-
izeawayfromπ usingarangeofvaluesofλ.
hacking
8PreventingRewardHackingwithOccupancyMeasureRegularization
Environment
Tomato Traffic Glucose Pandemic
Regularization (×103) (×103)
AD 1.98±1.49 -58.23±2.95 -10.37±0.20 -8.35±1.94
StateOM 5.32±0.22 -1.10±0.04 -186.14±17.98 -14.28±0.40
State-Act. OM 5.59±0.32 -1.07±0.01 -93.15±29.54 -14.23±5.02
Noregularization(π ) 2.35±0.14 -57.38±3.53 -599.02±1.58 -29.57±6.86
hacking
Table3.Thetruerewardsachievedbyregularizingawayfromrewardhackingpoliciesinthefourenvironments. OMregularization
preventsrewardhackinginallfourenvironments,whileADregularizationfailstoimproveonπ inthetomatoandtrafficenvironments.
hacking
TheresultsarepresentedinTable3. WefindthatOMKL consequencesofdevelopingsuchmethods.
regularizationconsistentlyavoidsrewardhackingand, in
somecases,evenoutperformsthesafepolicies.Ontheother
References
hand, the AD regularized policies’ true reward is danger-
ouslyclosetothatofπ insomeoftheenvironments, Amodei,D.,Olah,C.,Steinhardt,J.,Christiano,P.,Schul-
hacking
indicating that it is unable to consistently prevent reward man,J.,andMane´,D. ConcreteProblemsinAISafety,
hacking. July 2016. URL http://arxiv.org/abs/1606.
06565. arXiv:1606.06565[cs].
6.Conclusion
Bai,Y.,Jones,A.,Ndousse,K.,Askell,A.,Chen,A.,Das-
Sarma, N., Drain, D., Fort, S., Ganguli, D., Henighan,
Wehavepresentedtheoreticalandempiricalevidencethat
T., Joseph, N., Kadavath, S., Kernion, J., Conerly, T.,
occupancymeasureregularizationcanmoreeffectivelypre-
El-Showk, S., Elhage, N., Hatfield-Dodds, Z., Hernan-
ventrewardhackingthanactiondistributionregularization
dez, D., Hume, T., Johnston, S., Kravec, S., Lovitt,
whentrainingwithamisalignedproxyrewardfunction. Our
L., Nanda, N., Olsson, C., Amodei, D., Brown, T.,
resultsareasteptowardsabetterunderstandingofmeth-
Clark, J., McCandlish, S., Olah, C., Mann, B., and
odsforpreventingrewardhacking. However,manyopen
Kaplan, J. Training a Helpful and Harmless Assistant
problemsremain,includingdeterminingthebestwayofin-
with Reinforcement Learning from Human Feedback,
tegratingoccupancymeasureregularizationintothetraining
April2022. URLhttp://arxiv.org/abs/2204.
process,derivingbetterapproximatorsofOMdivergence,
05862. arXiv:2204.05862[cs].
andintelligentlychoosingregularizationcoefficients. AsAI
systems’objectivesbecomemorecomplexandtheyareused
Canonne,C.L. AshortnoteonaninequalitybetweenKL
inincreasinglyimportantsocietalroles,rewardhackingwill
andTV,February2022. URLhttp://arxiv.org/
becomebothmorecommonandmoreconsequential. Thus,
abs/2202.07198. arXiv:2202.07198[math,stat].
wehopethatourresultscontributetothegoalofensuring
thatfutureAIsystemsaresafeandbeneficial. Cheng,C.-A.,Xie,T.,Jiang,N.,andAgarwal,A. Adver-
sariallyTrainedActorCriticforOfflineReinforcement
BroaderImpacts Learning, July 2022. URL http://arxiv.org/
abs/2202.02446. arXiv:2202.02446[cs].
Rewardhackingintherealworldhasalreadyledtosignifi-
cantdisparitiesonthebasisofrace,gender,andotherdis- Chow, Y., Nachum, O., Faust, A., Duenez-Guzman,
tinguishingfactorsintherealmsofhealthcare(Obermeyer E., and Ghavamzadeh, M. Lyapunov-based Safe
etal.,2019;Piersonetal.,2021),policing(Lum&Isaac, Policy Optimization for Continuous Control, Febru-
2016;Corbett-Daviesetal.,2017;Franchietal.,2023),and ary 2019. URL http://arxiv.org/abs/1901.
online platforms like recommender systems (Milli et al., 10031. arXiv:1901.10031[cs,stat].
2021;Kleinbergetal.,2023). AsAIagentstrainedwithRL
Corbett-Davies,S.,Pierson,E.,Feller,A.,Goel,S.,andHuq,
becomemorepowerful,itislikelythesesystemswillexhibit
A. Algorithmicdecisionmakingandthecostoffairness,
rewardhackingbehaviorsthatcouldfurtherexacerbatebias
June2017. URLhttp://arxiv.org/abs/1701.
and can potentially cause substantial harm, especially in
08230. arXiv:1701.08230[cs,stat].
safetycriticalscenarios. Thus,theaimofourworkistopre-
ventrewardhackingbyimprovingregularizationmethods.
Dalal,G.,Dvijotham,K.,Vecerik,M.,Hester,T.,Paduraru,
Wedonotbelievethatthereareanynoteworthynegative
C.,andTassa,Y. SafeExplorationinContinuousAction
9PreventingRewardHackingwithOccupancyMeasureRegularization
Spaces, January 2018. URL http://arxiv.org/ He,H. ASurveyonOfflineModel-BasedReinforcement
abs/1801.08757. arXiv:1801.08757[cs]. Learning, May 2023. URL http://arxiv.org/
abs/2305.03360. arXiv:2305.03360[cs,eess].
Fox,I.,Lee,J.,Pop-Busui,R.,andWiens,J. DeepRein-
Ho, J. and Ermon, S. Generative Adversarial Im-
forcementLearningforClosed-LoopBloodGlucoseCon-
trol, September 2020. URL http://arxiv.org/ itation Learning. In Advances in Neural Infor-
abs/2009.09051. arXiv:2009.09051[cs,stat]. mation Processing Systems, volume 29. Curran
Associates, Inc., 2016. URL https://papers.
Franchi,M.,Zamfirescu-Pereira,J.D.,Ju,W.,andPierson, nips.cc/paper_files/paper/2016/hash/
E. Detecting disparities in police deployments using cc7e2b878868cbae992d1fb743995d8f-Abstract.
dashcam data. In 2023 ACM Conference on Fairness, html.
Accountability, and Transparency, pp. 534–544, June
Husza´r, F. Variational Inference using Implicit Distribu-
2023. doi: 10.1145/3593013.3594020. URLhttp://
tions, February 2017. URL http://arxiv.org/
arxiv.org/abs/2305.15210. arXiv:2305.15210
abs/1702.08235. arXiv:1702.08235[cs,stat].
[cs].
Ibarz, B., Leike, J., Pohlen, T., Irving, G., Legg, S., and
Gao, L., Schulman, J., and Hilton, J. Scaling Amodei, D. Reward learning from human preferences
Laws for Reward Model Overoptimization, Octo- and demonstrations in Atari. 2018. doi: 10.48550/
ber 2022. URL http://arxiv.org/abs/2210. ARXIV.1811.06521. URL https://arxiv.org/
10760. arXiv:2210.10760[cs,stat]. abs/1811.06521. Publisher: arXivVersionNumber:
1.
Glaese, A., McAleese, N., Trebacz, M., Aslanides, J.,
Firoiu, V., Ewalds, T., Rauh, M., Weidinger, L., Chad- Jeon,H.J.,Milli,S.,andDragan,A.D.Reward-rational(im-
wick, M., Thacker, P., Campbell-Gillingham, L., Ue- plicit)choice: Aunifyingformalismforrewardlearning,
sato,J.,Huang,P.-S.,Comanescu,R.,Yang,F.,See,A., December 2020. URL http://arxiv.org/abs/
Dathathri,S.,Greig,R.,Chen,C.,Fritz,D.,Elias,J.S., 2002.04833. arXiv:2002.04833[cs].
Green, R., Mokra, S., Fernando, N., Wu, B., Foley, R.,
Kang, B., Jie, Z., and Feng, J. Policy optimization
Young,S.,Gabriel,I.,Isaac,W.,Mellor,J.,Hassabis,D.,
with demonstrations. In International Conference
Kavukcuoglu,K.,Hendricks,L.A.,andIrving,G. Im-
on Machine Learning, 2018. URL https://api.
provingalignmentofdialogueagentsviatargetedhuman
semanticscholar.org/CorpusID:51875782.
judgements, September 2022. URL http://arxiv.
org/abs/2209.14375. arXiv:2209.14375[cs]. Karwowski,J.,Hayman,O.,Bai,X.,Kiendlhofer,K.,Grif-
fin,C.,andSkalse,J. Goodhart’sLawinReinforcement
Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, Learning,October2023. URLhttp://arxiv.org/
B., Warde-Farley, D., Ozair, S., Courville, A., abs/2310.09144. arXiv:2310.09144[cs].
and Bengio, Y. Generative Adversarial Networks,
June2014. URLhttp://arxiv.org/abs/1406. Kleinberg, J., Mullainathan, S., and Raghavan, M. The
2661. arXiv:1406.2661[cs,stat]. Challenge of Understanding What Users Want: Incon-
sistent Preferences and Engagement Optimization, Oc-
Goodhart, C. A. E. Problems of Monetary Manage- tober2023. URLhttp://arxiv.org/abs/2202.
ment: The UK Experience. In Goodhart, C. A. E. 11776. arXiv:2202.11776[cs].
(ed.), Monetary Theory and Practice: The UK Expe-
Knox, W. B., Allievi, A., Banzhaf, H., Schmitt, F., and
rience, pp. 91–121. Macmillan Education UK, Lon-
Stone, P. Reward (Mis)design for Autonomous Driv-
don, 1984. ISBN 978-1-349-17295-5. doi: 10.1007/
ing, March 2022. URL http://arxiv.org/abs/
978-1-349-17295-5 4. URLhttps://doi.org/10.
2104.13906. arXiv:2104.13906[cs].
1007/978-1-349-17295-5_4.
Kompella, V., Capobianco, R., Jong, S., Browne, J., Fox,
Hadfield-Menell, D., Milli, S., Abbeel, P., Rus-
S.,Meyers,L.,Wurman,P.,andStone,P. Reinforcement
sell, S., and Dragan, A. Inverse Reward De-
LearningforOptimizationofCOVID-19Mitigationpoli-
sign,2017. URLhttp://arxiv.org/abs/1711. cies,October2020.URLhttp://arxiv.org/abs/
02827. arXiv:1711.02827[cs]. 2010.10560. arXiv:2010.10560[cs].
Hazan, E., Kakade, S. M., Singh, K., and Van Soest, A. Korbak, T., Perez, E., and Buckley, C. L. RL with KL
ProvablyEfficientMaximumEntropyExploration,Jan- penalties is better viewed as Bayesian inference, Oc-
uary 2019. URL http://arxiv.org/abs/1812. tober2022. URLhttp://arxiv.org/abs/2205.
02690. arXiv:1812.02690[cs,stat]. 11275. arXiv:2205.11275[cs,stat].
10PreventingRewardHackingwithOccupancyMeasureRegularization
Krakovna, V. Specification gaming exam- Lum,K.andIsaac,W. ToPredictandServe? Significance,
ples in AI, April 2018. URL https: 13(5):14–19,October2016. ISSN1740-9705. doi: 10.
//vkrakovna.wordpress.com/2018/04/02/ 1111/j.1740-9713.2016.00960.x. URLhttps://doi.
specification-gaming-examples-in-ai/. org/10.1111/j.1740-9713.2016.00960.x.
eprint: https://academic.oup.com/jrssig/article-
Krakovna, V. Classifying specification problems as pdf/13/5/14/49106469/sign 13 5 14.pdf.
variantsofGoodhart’sLaw,August2019. URLhttps:
//vkrakovna.wordpress.com/2019/08/19/ Man,C.D.,Micheletto,F.,Lv,D.,Breton,M.,Kovatchev,
classifying-specification-problems-as-variBa.n,tansd-Cofob-eglloi,oCd.hTahretUsV-Al/aPwA/D.OVAType1Diabetes
Simulator. JournalofDiabetesScienceandTechnology,
Krakovna,V.,Orseau,L.,Kumar,R.,Martic,M.,andLegg, 8(1):26–34, January 2014. ISSN 1932-2968. doi: 10.
S. Penalizingsideeffectsusingstepwiserelativereacha- 1177/1932296813514502.URLhttps://www.ncbi.
bility,March2019. URLhttp://arxiv.org/abs/ nlm.nih.gov/pmc/articles/PMC4454102/.
1806.01186. arXiv:1806.01186[cs,stat].
Mandal, D., Triantafyllou, S., and Radanovic, G. Per-
formative Reinforcement Learning, February 2023.
Laidlaw, C., Russell, S., and Dragan, A. Bridging
URL http://arxiv.org/abs/2207.00046.
RL Theory and Practice with the Effective Horizon,
arXiv:2207.00046[cs].
April2023. URLhttp://arxiv.org/abs/2304.
09853. arXiv:2304.09853[cs,stat].
McKinney, L., Duan, Y., Krueger, D., and Gleave, A.
On The Fragility of Learned Reward Functions, Jan-
Lee,J.,Paduraru,C.,Mankowitz,D.J.,Heess,N.,Precup,
uary 2023. URL http://arxiv.org/abs/2301.
D.,Kim,K.-E.,andGuez,A. COptiDICE:OfflineCon-
03652. arXiv:2301.03652[cs].
strainedReinforcementLearningviaStationaryDistribu-
tionCorrectionEstimation,April2022. URLhttp:// Milli, S., Belli, L., and Hardt, M. From Optimizing En-
arxiv.org/abs/2204.08957. arXiv:2204.08957 gagement to Measuring Value. In Proceedings of the
[cs]. 2021ACMConferenceonFairness,Accountability,and
Transparency,pp.714–722,March2021. doi: 10.1145/
Lee, K., Smith, L., and Abbeel, P. PEBBLE: Feedback- 3442188.3445933. URLhttp://arxiv.org/abs/
Efficient Interactive Reinforcement Learning via Re- 2008.12623. arXiv:2008.12623[cs,stat].
labeling Experience and Unsupervised Pre-training,
June2021. URLhttp://arxiv.org/abs/2106. Mwalili, S., Kimathi, M., Ojiambo, V., Gathungu, D.,
05091. arXiv:2106.05091[cs]. and Mbogo, R. SEIR model for COVID-19 dy-
namics incorporating the environment and social dis-
Lee,L.,Eysenbach,B.,Parisotto,E.,Xing,E.,Levine,S., tancing. BMC Research Notes, 13:352, July 2020.
and Salakhutdinov, R. Efficient Exploration via State ISSN 1756-0500. doi: 10.1186/s13104-020-05192-1.
Marginal Matching, February 2020. URL http:// URL https://www.ncbi.nlm.nih.gov/pmc/
arxiv.org/abs/1906.05274. arXiv:1906.05274 articles/PMC7376536/.
[cs,stat].
Nedergaard,A.andCook,M. k-MeansMaximumEntropy
Exploration, November2023. URLhttp://arxiv.
Leike,J.,Martic,M.,Krakovna,V.,Ortega,P.A.,Everitt,
org/abs/2205.15623. arXiv:2205.15623[cs].
T., Lefrancq, A., Orseau, L., and Legg, S. AI Safety
Gridworlds, November 2017. URL http://arxiv.
Ngo, R., Chan, L., and Mindermann, S. The align-
org/abs/1711.09883. arXiv:1711.09883[cs].
mentproblemfromadeeplearningperspective,Septem-
ber 2023. URL http://arxiv.org/abs/2209.
Leike,J.,Krueger,D.,Everitt,T.,Martic,M.,Maini,V.,and
00626. arXiv:2209.00626[cs].
Legg,S. Scalableagentalignmentviarewardmodeling:
a research direction, November 2018. URL http:// Obermeyer,Z.,Powers,B.,Vogeli,C.,andMullainathan,S.
arxiv.org/abs/1811.07871. arXiv:1811.07871 Dissectingracialbiasinanalgorithmusedtomanagethe
[cs,stat]. healthofpopulations. Science,366(6464):447–453,Oc-
tober2019. ISSN0036-8075,1095-9203. doi: 10.1126/
Liang,E.,Liaw,R.,Moritz,P.,Nishihara,R.,Fox,R.,Gold- science.aax2342. URL https://www.science.
berg,K.,Gonzalez,J.E.,Jordan,M.I.,andStoica,I. RL- org/doi/10.1126/science.aax2342.
lib:AbstractionsforDistributedReinforcementLearning,
June2018. URLhttp://arxiv.org/abs/1712. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
09381. arXiv:1712.09381[cs]. C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K.,
11PreventingRewardHackingwithOccupancyMeasureRegularization
Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, Schulman, J., Wolski, F., Dhariwal, P., Radford, A.,
L.,Simens,M.,Askell,A.,Welinder,P.,Christiano,P., and Klimov, O. Proximal Policy Optimization Algo-
Leike, J., and Lowe, R. Training language models to rithms, August 2017. URL http://arxiv.org/
follow instructions with human feedback. 2022. doi: abs/1707.06347. arXiv:1707.06347[cs].
10.48550/ARXIV.2203.02155. URLhttps://arxiv.
org/abs/2203.02155. Skalse,J.,Howe,N.H.R.,Krasheninnikov,D.,andKrueger,
D.DefiningandCharacterizingRewardHacking,Septem-
Pan,A.,Bhatia,K.,andSteinhardt,J.TheEffectsofReward ber 2022. URL http://arxiv.org/abs/2209.
Misspecification: Mapping and Mitigating Misaligned 13085. arXiv:2209.13085[cs,stat].
Models,February2022. URLhttp://arxiv.org/
abs/2201.03544. arXiv:2201.03544[cs,stat]. Steil, G. M. Algorithms for a Closed-Loop Artificial
Pancreas: TheCaseforProportional-Integral-Derivative
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Control. Journal of Diabetes Science and Technol-
Chanan,G.,Killeen,T.,Lin,Z.,Gimelshein,N.,Antiga, ogy, 7(6):1621–1631, November 2013. ISSN 1932-
L.,Desmaison,A.,Ko¨pf,A.,Yang,E.,DeVito,Z.,Rai- 2968. URL https://www.ncbi.nlm.nih.gov/
son,M.,Tejani,A.,Chilamkurthy,S.,Steiner,B.,Fang, pmc/articles/PMC3876341/.
L., Bai, J., and Chintala, S. PyTorch: An Imperative
Style,High-PerformanceDeepLearningLibrary,Decem- Stiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe,
ber 2019. URL http://arxiv.org/abs/1912. R.,Voss,C.,Radford,A.,Amodei,D.,andChristiano,P.
01703. arXiv:1912.01703[cs,stat]. Learningtosummarizefromhumanfeedback,Septem-
ber 2020. URL http://arxiv.org/abs/2009.
Pierson, E., Cutler, D. M., Leskovec, J., Mullainathan, 01325. arXiv:2009.01325[cs].
S., and Obermeyer, Z. An algorithmic approach
to reducing unexplained pain disparities in under- Taylor, J. Quantilizers: A Safer Alternative to Max-
served populations. Nature Medicine, 27:136 – 140, imizers for Limited Optimization. March 2016.
2021. URL https://api.semanticscholar. URL https://www.semanticscholar.
org/CorpusID:231598658. org/paper/Quantilizers%
3A-A-Safer-Alternative-to-Maximizers-for-Taylor/
Rashidinejad, P., Zhu, B., Ma, C., Jiao, J., and Rus- 4e8ff3b4069a12a00196d62925bab8add7389742.
sell, S. Bridging Offline Reinforcement Learn-
ing and Imitation Learning: A Tale of Pessimism, Taylor, J., Yudkowsky, E., LaVictoire, P., and Critch, A.
July 2023. URL http://arxiv.org/abs/2103. AlignmentforAdvancedMachineLearningSystems. In
12021. arXiv:2103.12021[cs,math,stat]. EthicsofArtificialIntelligence.OxfordUniversityPress,
August2020. ISBN978-0-19-090505-7. Google-Books-
Reddy,S.,Dragan,A.D.,Levine,S.,Legg,S.,andLeike,J. ID:1yT3DwAAQBAJ.
LearningHumanObjectivesbyEvaluatingHypothetical
Behavior, March2020. URLhttp://arxiv.org/ Tien, J., He, J. Z.-Y., Erickson, Z., Dragan, A. D.,
abs/1912.05652. arXiv:1912.05652[cs,stat]. and Brown, D. S. Causal Confusion and Reward
Misidentification in Preference-Based Reward Learn-
Roy, J., Girgis, R., Romoff, J., Bacon, P.-L., and Pal, ing, March 2023. URL http://arxiv.org/abs/
C. DirectBehaviorSpecificationviaConstrainedRein- 2204.06601. arXiv:2204.06601[cs].
forcementLearning,June2022. URLhttp://arxiv.
org/abs/2112.12228. arXiv:2112.12228[cs]. Treiber, M., Hennecke, A., and Helbing, D. Con-
gested Traffic States in Empirical Observations and
Russell, S. J., Norvig, P., and Davis, E. Artificial intel- Microscopic Simulations. Physical Review E, 62
ligence: a modern approach. Prentice Hall series in (2):1805–1824, August 2000. ISSN 1063-651X,
artificialintelligence.PrenticeHall,UpperSaddleRiver, 1095-3787. doi: 10.1103/PhysRevE.62.1805. URL
3rdededition,2010. ISBN978-0-13-604259-4. http://arxiv.org/abs/cond-mat/0002177.
arXiv:cond-mat/0002177.
Sadigh, D., Dragan, A., Sastry, S., and Seshia, S. Ac-
tive Preference-Based Learning of Reward Functions. Turner, A. M., Smith, L., Shah, R., Critch, A.,
In Robotics: Science and Systems XIII. Robotics: Sci- and Tadepalli, P. Optimal Policies Tend To
ence and Systems Foundation, July 2017. ISBN Seek Power. December 2019. URL https:
978-0-9923747-3-0. doi: 10.15607/RSS.2017.XIII. //www.semanticscholar.org/paper/
053. URLhttp://www.roboticsproceedings. Optimal-Policies-Tend-To-Seek-Power-Turner-Smith/
org/rss13/p53.pdf. 46d4452eb041e33f1e58eab64ec8cf5af534b6ff.
12PreventingRewardHackingwithOccupancyMeasureRegularization
Turner, A. M., Ratzlaff, N., and Tadepalli, P. Avoid-
ing Side Effects in Complex Environments, Octo-
ber 2020. URL http://arxiv.org/abs/2006.
06547. arXiv:2006.06547[cs].
Uchendu, I., Xiao, T., Lu, Y., Zhu, B., Yan, M., Simon,
J., Bennice, M., Fu, C., Ma, C., Jiao, J., Levine, S.,
andHausman,K. Jump-StartReinforcementLearning,
July 2023. URL http://arxiv.org/abs/2204.
02372. arXiv:2204.02372[cs].
Vieillard,N.,Kozuno,T.,Scherrer,B.,Pietquin,O.,Munos,
R.,andGeist,M. LeveragetheAverage: anAnalysisof
KLRegularizationinRL,January2021. URLhttp://
arxiv.org/abs/2003.14089. arXiv:2003.14089
[cs,stat].
Wu,C.,Kreidieh,A.,Parvate,K.,Vinitsky,E.,andBayen,
A.M. Flow: AModularLearningFrameworkforMixed
AutonomyTraffic. IEEETransactionsonRobotics,38
(2):1270–1286, April 2022. ISSN 1552-3098, 1941-
0468. doi: 10.1109/TRO.2021.3087314. URLhttp://
arxiv.org/abs/1710.05465. arXiv:1710.05465
[cs].
Xie,T.,Cheng,C.-A.,Jiang,N.,Mineiro,P.,andAgarwal,
A. Bellman-consistentPessimismforOfflineReinforce-
mentLearning,October2023. URLhttp://arxiv.
org/abs/2106.06926. arXiv:2106.06926[cs,stat].
Xu,T.,Li,Z.,andYu,Y. ErrorBoundsofImitatingPoli-
ciesandEnvironments,October2020. URLhttp://
arxiv.org/abs/2010.11876. arXiv:2010.11876
[cs].
Yang, S., Feng, Y., Zhang, S., and Zhou, M. Reg-
ularizing a Model-based Policy Stationary Distribu-
tion to Stabilize Offline Reinforcement Learning,
June2022. URLhttp://arxiv.org/abs/2206.
07166. arXiv:2206.07166[cs].
Yang, T.-Y., Rosca, J., Narasimhan, K., and Ra-
madge, P. J. Accelerating Safe Reinforcement
Learning with Constraint-mismatched Policies, July
2021. URL http://arxiv.org/abs/2006.
11645. arXiv:2006.11645[cs,stat].
Zhang, Y., Vuong, Q., and Ross, K. W. First Or-
der Constrained Optimization in Policy Space, Octo-
ber 2020. URL http://arxiv.org/abs/2002.
06506. arXiv:2002.06506[cs,stat].
13PreventingRewardHackingwithOccupancyMeasureRegularization
Appendix
A.Proofs
A.1.ProofofProposition3.1
Proposition3.1. Fixc >0andδ >0arbitrarilysmall,andc ≥0arbitrarilylarge. ThenthereisanMDP,truereward
1 2
functionR,andsafepolicyπ wherebothofthefollowinghold:
safe
1. ThereisapolicyπwheretheactiondistributionKLdivergencesatisfies
(cid:104) (cid:105)
(1−γ)E (cid:80)∞ γtD (π(·|s )∥π (·|s )) ≤c
π t=0 KL t safe t 1
butJ(π ,R)−J(π,R)≥1−δ.
safe
2. Anyoptimalpolicyπ∗ ∈argmax J(π,R)satisfies
π
(cid:104) (cid:105)
(1−γ)E (cid:80)∞ γtD (π∗(·|s )∥π (·|s )) ≥c .
π∗ t=0 KL t safe t 2
Proof. Weassumethatδ <1,sinceotherwiselettingπ =π triviallysatisfiesthefirstpartoftheproposition. Consider
safe
thefollowingMDP,similartotheoneshowninFigure1:
R(s ,a )=0 R(s ,a )=1−δ/2
1 1 2 1
R(s ,a )=1 R(s ,a )=1
1 2 2 2
s s
a 1 1 a 2 2
a ,a
1 2
InthisMDP,S ={s ,s },A={a ,a },andthetransitionprobabilitiesandrewardfunctionaredefinedby
1 2 1 2
p(s |s ,a )=1 p(s |s ,a )=1
1 1 1 2 1 2
p(s |s ,a )=1 p(s |s ,a )=1
2 2 1 2 2 2
R(s ,a )=0 R(s ,a )=1
1 1 1 2
R(s ,a )=1−δ/2 R(s ,a )=1.
2 1 2 2
Theinitialstateisalwayss . Thus,theagentstaysinstates andreceivesnorewarduntilafterittakesactiona ,atwhich
1 1 2
pointittransitionstos andreceives1or1−δ/2rewardpertimestep. Defineforany(p,q)∈[0,1]2apolicyπ :
2 (p,q)
π (a |s )=p π (a |s )=q.
(p,q) 2 1 (p,q) 2 2
Wewillprovethepropositionusing
δ
γ =1− (1−e−c1)
2
π =π where p=2(1−γ)/δ and q =exp{−c /γ}
safe (p,q) 2
π =π
(0,0)
π∗ =π .
(1,1)
Notethefollowing:
• π∗istheuniqueoptimalpolicy: J(π∗,R)=1andforanyotherpolicyπ,J(π,R)<1.
• γ ∈[0,1): c
1
>0andthus1−e−c1 >0,andδ <1.
• p∈[0,1]: sinceγ >1−δ/2,wehavep<1.
14PreventingRewardHackingwithOccupancyMeasureRegularization
• q ∈[0,1]: sincec ≥0,wehaveq ≤1.
2
Tostart,weneedtoshowthat
(cid:34) ∞ (cid:35)
(cid:88)
(1−γ)E γtD (π(·|s )∥π (·|s )) ≤c . (8)
π KL t safe t 1
t=0
Sinceπalwaysstaysats ,wecanrewritetheLHSof(8)as
1
(cid:34) ∞ (cid:35)
(cid:88)
(1−γ)E γtD (π(·|s )∥π (·|s )) =D (π(·|s )∥π (·|s ))
π KL t safe t KL 1 safe 1
t=0
π(a |s ) π(a |s )
=π(a |s )log 1 1 +π(a |s )log 2 1
1 1 π (a |s ) 2 1 π (a |s )
safe 1 1 safe 2 1
1
=log
1−p
1
=log
e−c1
=c ,
1
whichproves(8).
Next,weneedtoshowthatJ(π ,R)−J(π,R)≥1−δ. Clearly,J(π,R)=0. WecanboundJ(π ,R)as
safe safe
∞
(cid:88) (cid:104) (cid:105)
J(π ,R)=(1−γ) γt P (s =s )p+P (s =s )(q+(1−q)(1−δ/2))
safe π t 1 π t 2
t=0
∞
(cid:88) (cid:104) (cid:105)
≥(1−γ) γt(1−δ/2) P (s =s )p+P (s =s )
π t 1 π t 2
t=0
∞
(cid:88) (cid:16) (cid:17)
=(1−γ) γt(1−δ/2)P ∃t′ ≤ts.t.a =a
π t′ 2
t=0
∞
(cid:88) (cid:16) (cid:17)
=(1−γ) γt(1−δ/2) 1−(1−p)t+1
t=0
∞ (cid:18) (cid:19)
(cid:88) 1
=(1−γ)(1−δ/2)(1−p) γt −(1−p)t
1−p
t=0
(cid:18) (cid:19)
1 1
=(1−γ)(1−δ/2)(1−p) −
(1−p)(1−γ) 1−γ(1−p)
p
=(1−δ/2) .
1−γ(1−p)
Plugginginp=2(1−γ)/δgives
2(1−γ)/δ
J(π ,R)≥(1−δ/2)
safe 1−γ(1−2(1−γ)/δ)
1−δ/2
=
γ+δ/2
(i)
≥(1−δ/2)(2−γ−δ/2)
≥(1−δ/2)(1−δ/2)
≥1−δ,
whichprovesJ(π ,R)−J(π,R)≥1−δasdesired. (i)usesthefactthat1/x≥2−xforx>0.
safe
15PreventingRewardHackingwithOccupancyMeasureRegularization
Allthatremainstobeshownisthat
(cid:34) ∞ (cid:35)
(cid:88)
(1−γ)E γtD (π∗(·|s )∥π (·|s )) ≥c . (9)
π∗ KL t safe t 2
t=0
WecanboundtheLHSof(9)basedononlytheKLdivergenceats :
2
(cid:34) ∞ (cid:35) ∞
(cid:88) (cid:88)
(1−γ)E γtD (π∗(·|s )∥π (·|s )) ≥(1−γ) γtD (π∗(·|s )∥π (·|s ))P (s =s ).
π∗ KL t safe t KL 2 safe 2 π∗ t 2
t=0 t=0
Sinceπ∗alwaystakesactiona ,weknowthatP (s =s )=1{t≥1}. Thus,wecancontinuetheboundas
2 π∗ t 2
∞
(cid:88)
≥(1−γ) γtD (π∗(·|s )∥π (·|s ))
KL 2 safe 2
t=1
=γD (π∗(·|s )∥π (·|s ))
KL 2 safe 2
(cid:20) π∗(a |s ) π∗(a |s ) (cid:21)
=γ π∗(a |s )log 1 2 +π∗(a |s )log 2 2
1 2 π (a |s ) 2 2 π (a |s )
safe 1 2 safe 2 2
1
=γlog
q
=c
2
bythedefinitionofq. Thisproves(9)andcompletestheproof.
A.2.ProofofProposition3.2
Wefirstproveanotherusefulproposition:
PropositionA.1. ThereturnofapolicyπunderarewardfunctionRisgivenby
(cid:88)
J(π,R)= µ (s,a)R(s,a).
π
(s,a)∈S×A
Proof. Applyingthedefinitionsofreturnandoccupancymeasure,wehave
(cid:34) ∞ (cid:35)
(cid:88)
J(π,R)=(1−γ)E γtR(s ,a )
π t t
t=0
∞
(cid:88) (cid:88)
=(1−γ) γt R(s,a)P (s =s∧a =a)
π t t
t=0 (s,a)∈S×A
∞
(cid:88) (cid:88)
=(1−γ) R(s,a) γtP (s =s∧a =a)
π t t
(s,a)∈S×A t=0
(cid:34) ∞ (cid:35)
(cid:88) (cid:88)
= R(s,a)(1−γ)E γt1{s =s∧a =a}
π t t
(s,a)∈S×A t=0
(cid:88)
= µ (s,a)R(s,a).
π
(s,a)∈S×A
AccordingtoPropositionA.1,thereturnofapolicyissimplyaweightedsumoftherewardfunction,wheretheweightsare
givenbytheoccupancymeasure. WenowproveProposition3.2.
Proposition3.2. ForanyMDP,rewardfunctionR,andpairofpoliciesπ,π ,wehave
safe
(cid:13) (cid:13)
|J(π safe,R)−J(π,R)|≤(cid:13)µ π−µ πsafe(cid:13) 1. (4)
16PreventingRewardHackingwithOccupancyMeasureRegularization
Proof. ApplyingPropositionA.1,Ho¨lder’sinequality,andthefactthatR(s,a)∈[0,1],wehave
|J(π ,R)−J(π,R)|
safe
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:88) (cid:12)
=(cid:12)
(cid:12)
(µ πsafe(s,a)−µ π(s,a))R(s,a)(cid:12)
(cid:12)
(cid:12)(s,a)∈S×A (cid:12)
 
(cid:18) (cid:19)
(cid:88)
≤ (s,am )∈a Sx ×A|R(s,a)|  |µ πsafe(s,a)−µ π(s,a)|
(s,a)∈S×A
≤∥µ −µ ∥ .
π πsafe 1
A.3.Additionalresults
Thefollowingpropositiondemonstratesthatthereisalwayssomerewardfunctionforwhichtheboundin(4)istightuptoa
factoroftwo.
PropositionA.2. FixanMDPandpairofpoliciesπ,π . ThenthereissomerewardfunctionRsuchthat
safe
1(cid:13) (cid:13)
|J(π safe,R)−J(π,R)|≥ 2(cid:13)µ π−µ πsafe(cid:13) 1.
Proof. Definetworewardfunctions
R (s,a)=1{µ (s,a)≥µ (s,a)}
1 πsafe π
R (s,a)=1{µ (s,a)≤µ (s,a)}.
2 πsafe π
UsingPropositionA.1,wehave
|J(π ,R )−J(π,R )|+|J(π,R )−J(π ,R )|
safe 1 1 2 safe 2
≥J(π ,R )−J(π,R )+J(π,R )−J(π ,R )
safe 1 1 2 safe 2
(cid:88) (cid:16) (cid:17)(cid:16) (cid:17)
= µ (s,a)−µ (s,a) R (s,a)−R (s,a)
πsafe π 1 2
(s,a)∈S×A

1 µ (s,a)>µ (s,a)
(cid:88) (cid:16)
(cid:17) πsafe π
= µ (s,a)−µ (s,a) −1 µ (s,a)<µ (s,a)
πsafe π πsafe π
(s,a)∈S×A
0
µ (s,a)=µ (s,a)
πsafe π
(cid:88) (cid:12) (cid:12)
= (cid:12)µ (s,a)−µ (s,a)(cid:12)
(cid:12) πsafe π (cid:12)
(s,a)∈S×A
=∥µ −µ ∥ .
π πsafe 1
Sincebothofthetermsonthefirstlinearepositive,onemustbeatleast 1∥µ −µ ∥ ,whichcompletestheproof.
2 π πsafe 1
A.4.OccupancymeasureregularizationinLLMs
Asnotedinthemaintext, inthecurrentparadigmofusingRLHFtotrainLLMs, wecanshowthatactiondistribution
divergencebetweentwopoliciesisequivalenttooccupancymeasuredivergence. Inparticular, weprovethefollowing
proposition.
PropositionA.3. Supposethatanenvironmentsatisfiesthefollowingconditions:
• Itisdeterministic: µ (s )=1forexactlyonestates ,andforalls ,a ∈S×A,p(s |s ,a )=1forexactlyone
0 0 0 t t t+1 t t
states .
t+1
17PreventingRewardHackingwithOccupancyMeasureRegularization
• Exactlyonesequenceofactionsleadstoeachstate: iffollowinga ,...,a leadstos,thennoothersequenceof
0 t−1
actions(ofanylength)canalsoleadtos.
Then,foranypoliciesπ,π′,theactiondistributionandoccupancymeasureKLdivergencesbetweenthemareequal:
(cid:34) ∞ (cid:35)
(cid:88)
D (µ ∥µ )=E γtD (π(·|s )∥π′(·|s )) .
KL π π′ π KL t t
t=0
Proof. Giventheassumptionsabouttheenvironment,wecanrewritethelog-occupancymeasureofastate-actionpairin
termsofthesumoflogactionprobabiltiesovertheuniquesequenceofactionsleadingtothatstate. Supposea ,...,a is
0 t−1
theuniqueactionsequenceleadingtosandthatthisactionsequencevisitsstatess ,...,s ,s. Then
0 t−1
(cid:34) ∞ (cid:35)
(cid:88)
logµ (s,a)=log(1−γ)E γt1{s =s∧a =a}
π π t t
t=0
=log(1−γ)P (s =s∧a =a)
π t t
t
(cid:89)
=log(1−γ) π(a |s )
i i
i=0
t
(cid:88)
=log(1−γ)+ logπ(a |s ).
i i
i=0
Usingthis,wecanrewritetheoccupancymeasureKLdivergenceas
(cid:18) (cid:19)
D (µ ∥µ )=
(cid:88)
µ (s,a)log
µ π(s,a)
KL π π′ π µ (s,a)
π′
(s,a)∈S×A
∞ t
(cid:88) (cid:88) (cid:88)(cid:16) (cid:17)
=(1−γ) γt P (a ∧···∧a ) logπ(a |s )−logπ′(a |s )
π 0 t i i i i
t=0 a0,...,at∈At+1 i=0
 
∞ t t
(cid:88) (cid:88) (cid:89) (cid:88)(cid:16) (cid:17)
=(1−γ) γt  π(a i |s i) logπ(a i |s i)−logπ′(a i |s i) , (10)
t=0 a0,...,at∈At+1 j=0 i=0
wheres isthestatereachedbytakinga ,...,a .
i 0 i−1
Wewillnowshowinductivelythat
 
t t t
(cid:88) (cid:89) (cid:88)(cid:16) (cid:17) (cid:88) (cid:88)
 π(a i |s i) logπ(a i |s i)−logπ′(a i |s i) = P π(s i)D KL(π(·|s i)∥π′(·|s i)).
a0,...,at∈At+1 j=0 i=0 i=0si∈S
(11)
Considerfirstift=0. Then
(cid:88) (cid:16) (cid:17)
π(a |s ) logπ(a |s )−logπ′(a |s )
0 0 0 0 0 0
a0∈A
=D (π(·|s )∥π′(·|s ))
KL 0 0
=P (s )D (π(·|s )∥π′(·|s )).
π 0 KL 0 0
18PreventingRewardHackingwithOccupancyMeasureRegularization
Nowsuppose(11)holdsfort−1. Thenfortwehave
 
t t
(cid:88) (cid:89) (cid:88)(cid:16) (cid:17)
 π(a i |s i) logπ(a i |s i)−logπ′(a i |s i)
a0,...,at∈At+1 j=0 i=0
 t−1  (cid:34) t−1 (cid:35)
(cid:88) (cid:89) (cid:88) (cid:88)(cid:16) (cid:17)
=  π(a i |s i) π(a t |s t) logπ(a t |s t)−logπ′(a t |s t)+ logπ(a i |s i)−logπ′(a i |s i)
a0,...,at−1∈At j=0 at∈A i=0
 t−1 (cid:34) t−1 (cid:35)
(cid:88) (cid:89) (cid:88) (cid:88)(cid:16) (cid:17)
=  π(a i |s i) D KL(π(·|s t)∥π′(·|s t))+ π(a t |s t) logπ(a i |s i)−logπ′(a i |s i)
a0,...,at−1∈At j=0 at∈A i=0
 t−1 (cid:34) t−1 (cid:35)
(cid:88) (cid:89) (cid:88)(cid:16) (cid:17)
=  π(a i |s i) D KL(π(·|s t)∥π′(·|s t))+ logπ(a i |s i)−logπ′(a i |s i)
a0,...,at−1∈At j=0 i=0
 
t−1 t−1
(cid:88) (cid:88) (cid:89) (cid:88)(cid:16) (cid:17)
= P π(s t)D KL(π(·|s t)∥π′(·|s t))+  π(a i |s i) logπ(a i |s i)−logπ′(a i |s i)
st∈S a0,...,at−1∈At j=0 i=0
t−1
=(i) (cid:88) P (s )D (π(·|s )∥π′(·|s ))+(cid:88) (cid:88) P (s )D (π(·|s )∥π′(·|s ))
π t KL t t π i KL i i
st∈S i=0si∈S
t
(cid:88) (cid:88)
= P (s )D (π(·|s )∥π′(·|s )),
π i KL i i
i=0si∈S
where(i)isfromtheinductivehypothesis.
Now,plugging(11)into(10)gives
D (µ ∥µ )
KL π π′
∞ t
(cid:88) (cid:88) (cid:88)
=(1−γ) γt P (s )D (π(·|s )∥π′(·|s ))
π i KL i i
t=0 i=0si∈S
∞ ∞
(cid:88)(cid:88) (cid:88)
=(1−γ) γt P (s )D (π(·|s )∥π′(·|s ))
π i KL i i
i=0 t=i si∈S
∞ ∞
(cid:88) (cid:88) (cid:88)
=(1−γ) P (s )D (π(·|s )∥π′(·|s )) γt
π i KL i i
i=0si∈S t=i
(cid:34) ∞ ∞ (cid:35)
(cid:88) (cid:88)
=(1−γ)E D (π(·|s )∥π′(·|s )) γt
π KL i i
i=0 t=i
(cid:34) γi (cid:88)∞ (cid:35)
=(1−γ)E D (π(·|s )∥π′(·|s ))
π 1−γ KL i i
i=0
(cid:34) ∞ (cid:35)
(cid:88)
=E γi D (π(·|s )∥π′(·|s )) ,
π KL i i
i=0
whichisthedesiredresult.
PropositionA.3appliestotwocommonMDPformulationsofgeneratingLLMresponses. Inthefirstformulation,each
entireLLMresponseisconsideredasingleactionandthentheMDPterminates. Inthiscase,theconditionsofProposition
A.3areclearlysatisfied. Inthesecondformulation,eachwordgeneratedisconsideredasingleaction,andthestateconsists
ofallpreviouslygeneratedwords. Clearlythisalsosatisfiestheconditionsoftheproposition. Thus,ineithercase,ADand
OMKLregulizationareequivalentwhentrainingLLMsviaRL.
19PreventingRewardHackingwithOccupancyMeasureRegularization
However, the conditions of Proposition A.3 are unlikely to be met by many other MDPs. Many MDPs are stochastic,
violatingthefirstassumption. EvenamongdeterministicMDPs,itisveryuncommonthatonlyasingleactionsequencecan
leadtoeachstate. Noneoftheenvironmentsweexperimentwithinthemaintext,andnocommonRLbenchmarksoutside
ofcertaintextgenerationordiscreteoptimizationtasks,satisfythisproperty.
A.5.State-onlyoccupancymeasures
Inthissection,weproveresultsforstate-onlyoccupancymeasures
∞
(cid:88)
µ (s)=(1−γ)E [ γt1{s =s}]
π π t
t=0
whicharesimilartoourresultsforstate-actionoccupancymeasures. Inparticular,supposethattherewardfunctiononly
dependsonthestate,i.e.,R(s,a)=R(s). Thenwecanstatethefollowingpropositions.
PropositionA.4. Thereturnofapolicyπunderastate-basedrewardfunctionRisgivenby
(cid:88)
J(π,R)= µ (s)R(s).
π
s∈S
Proof. Wehave
(cid:34) ∞ (cid:35)
(cid:88)
J(π,R)=(1−γ)E γtR(s )
π t
t=0
∞
(cid:88) (cid:88)
=(1−γ) γt R(s)P (s =s)
π t
t=0 s∈S
∞
(cid:88) (cid:88)
=(1−γ) R(s) γtP (s =s)
π t
s∈S t=0
(cid:34) ∞ (cid:35)
(cid:88) (cid:88)
= R(s)(1−γ)E γt1{s =s}
π t
s∈S t=0
(cid:88)
= µ (s)R(s).
π
s∈S
PropositionA.5. ForanyMDP,state-basedrewardfunctionR,andpairofpoliciesπ,π ,wehave
safe
|J(π ,R)−J(π,R)|≤∥µs −µs∥ ,
safe π π 1
where∥µs −µs∥ =(cid:80) |µ (s)−µ (s)|.
π π 1 s∈S π πsafe
Proof. TheproofproceedsviaananalogousapplicationofHo¨lder’sinequalityasintheproofofProposition3.2.
B.AdditionalResults
B.1.AUROCCurvesforrewardhackingdetection
OccupancymeasureKLisbetteratclassifyingwhenrewardhackingisoccurringthanactiondistributionKL.Wecansee
thisastheAUROCfortheOM-baseddetectorsisclosertoonethantheAD-baseddetectors. CurvesareshowninFigure4,
andthetabulatedAUROCinTable2.
20PreventingRewardHackingwithOccupancyMeasureRegularization
Tomato-OM Traffic-OM Glucose-OM Pandemic-OM
1.00
0.75
0.50
0.25
AUC=1.00 AUC=1.00 AUC=0.99 AUC=0.94
0.00
Tomato-AD Traffic-AD Glucose-AD Pandemic-AD
1.00
0.75
0.50
0.25
AUC=0.89 AUC=0.98 AUC=0.79 AUC=0.82
0.00
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
FalsePositiveRate FalsePositiveRate FalsePositiveRate FalsePositiveRate
Figure4. AUROCcurvesforOMandAD-basedrewardhackingpredictors
B.2.DetailedResults
Regularizingtowardsasafepolicy Here,weprovidethemediantruerewardachievedacross5seedsforeachofthe
coefficientstestedineachoftheenvironmentsforthethreeregularizationmethods(AD,state-actionOM,andstateOM).
Asdescribedinthemaintext,thecoefficientsthatwererunweredeterminedbymultiplyingarangeofscale-independent
coefficientsbytheaverageper-timesteprewardsineachenvironmentsthatwecalculatedafterrunningevaluationruns. The
resultsforthetomato,traffic,glucose,andpandemicenvironmentsareinTables4,5,6,and7respectively.
Coefficient ADKL State-ActionOMKL StateOMKL
0.4 6.16±0.03 6.84±0.17 6.89±0.12
0.08 6.26±0.04 7.32±0.25 7.62±0.05
0.16 6.21±0.05 7.12±0.10 7.20±0.11
0.8 6.19±0.03 6.86±0.17 7.07±0.11
1.6 6.14±0.03 6.61±0.28 6.90±0.12
4.0 6.13±0.03 6.79±0.11 6.80±0.14
8.0 6.13±0.01 6.80±0.05 6.81±0.25
16.0 6.13±0.00 6.83±0.22 6.94±0.09
0.016 6.33±0.11 0.84±0.19 0.82±0.20
0.04 6.26±0.04 1.81±0.28 1.17±2.61
0.008 6.10±0.13 1.25±0.28 1.30±0.17
0.004 4.59±0.17 2.01±0.20 2.23±0.05
0.0016 2.98±0.30 1.11±0.80 2.31±0.06
0.0008 2.52±0.16 2.31±0.07 2.32±0.77
Table4. TomatoResults
21
etaRevitisoPeurT
etaRevitisoPeurTPreventingRewardHackingwithOccupancyMeasureRegularization
Coefficient ADKL State-ActionOMKL StateOMKL
0.00025 -1334.87±46.05 -1514.19±86.37 -1471.37±182.34
5e-05 -49992.47±3616.60 -53387.70±21264.93 -59958.59±1479.34
0.0001 -45722.70±8065.79 -1252.16±62.46 -1343.77±22630.61
0.0005 -1517.03±36.56 -1993.42±311.02 -1755.62±195.70
0.001 -1733.61±59.09 -2304.83±1021.99 -1763.23±236.91
0.0025 -1982.69±60.14 -1940.36±268.60 -1755.88±458.19
0.005 -2145.45±46.40 -2075.82±544.53 -1895.66±744.30
0.01 -2110.00±42.60 -2144.57±499.23 -2115.40±893.93
1e-05 -54839.89±2817.67 -58848.18±2444.56 -57623.83±2803.94
2.5e-05 -55095.29±2365.65 -56859.38±4898.74 -59319.06±1223.88
5e-06 -57242.98±2345.02 -61238.06±1794.17 -59034.62±4842.77
2.5e-06 -59583.55±4325.42 -59594.74±2107.35 -54590.79±2826.26
1e-06 -56204.81±3596.68 -61175.89±2565.85 -61586.81±2435.51
5e-07 -59723.52±2031.10 -56360.16±2290.04 -58656.01±2599.17
Table5. TrafficResults
Coefficient ADKL State-ActionOMKL StateOMKL
0.015 -84091.61±6066.60 -48884.78±481.14 -82918.52±5019.81
0.003 -270021.63±35551.66 -101191.91±4503.72 -332322.58±36637.25
0.006 -154530.03±4918.73 -61888.10±4690.05 -158741.15±25737.30
0.03 -98280.34±7488.11 -49597.88±1072.53 -58391.57±3357.52
0.06 -88645.25±11470.58 -78266.23±9095.24 -58968.09±6395.74
0.15 -82117.03±10407.25 -106643.71±17533.81 -75930.59±4071.71
0.3 -73379.85±8256.09 -127284.20±22133.23 -98103.54±14432.68
0.6 -88556.96±4995.05 -118496.45±9588.01 -112541.50±14891.49
0.0006 -590000.85±6702.13 -364253.30±5221.89 -593110.06±4845.44
0.0015 -459923.51±102083.68 -181647.21±6693.00 -511113.29±18895.36
0.0003 -593615.68±5323.58 -497935.91±10001.19 -592025.72±26247.74
0.00015 -592338.62±45872.51 -577059.36±10017.97 -607941.22±9888.13
6e-05 -600567.81±11115.49 -594716.62±2981.95 -589003.48±233319.14
3e-05 -598445.43±35483.72 -583805.34±54751.09 -604122.09±9554.66
Table6. GlucoseResults
22PreventingRewardHackingwithOccupancyMeasureRegularization
Tomato Traffic Glucose Pandemic
0e
5 0e
-5e5 −25
0
-2e5
−50
10 50 250 5
0 0 0 0
10
100 500 50
0 0 0 0
10−3 10−1 101 10−3 10−1 101 10−3 10−1 101 10−3 10−1 101
Regularizationstrength(-λ/R )
average
ORPOw/stateoccupancy PPO+actionKLregularization
ORPOw/state-actionoccupancy Safepolicy
Figure5.ThisplotissimilartotheoneshowninFigure3,exceptinsteadofregularizingtowardssafepolicies,weareregularizingtowards
rewardhackingpolicies.
Coefficient ADKL State-ActionOMKL StateOMKL
0.03 -12.28±0.13 -11.73±0.19 -11.03±6.14
0.006 -12.26±10.57 -29.42±22.70 -58.08±42.02
0.012 -12.30±8.29 -11.88±0.72 -10.60±0.78
0.06 -12.20±0.06 -12.23±12.75 -10.71±0.16
0.12 -12.33±0.03 -12.09±0.34 -10.24±0.54
0.3 -12.35±0.04 -12.11±0.22 -11.02±0.51
0.6 -12.40±0.04 -12.11±0.25 -10.61±0.32
1.2 -12.33±0.03 -12.02±0.25 -10.50±0.23
0.0012 -25.17±9.16 -31.77±5.38 -31.28±7.01
0.003 -23.51±6.18 -23.90±11.46 -35.76±9.42
0.0006 -21.85±17.02 -22.40±8.81 -34.56±10.71
0.0003 -23.10±5.04 -27.56±6.71 -35.29±9.23
0.00012 -30.39±19.82 -19.67±5.75 -41.96±11.36
6e-05 -21.23±8.71 -30.96±20.05 -33.59±8.84
Table7. PandemicResults
Regularizingawayfromarewardhackingpolicy Here,weprovidethemediantruerewardachievedacross5seedsfor
eachofthecoefficientstestedineachoftheenvironments(tomato,traffic,andglucose)whenusingthethreeregularization
methods(AD,state-actionOM,andstateOM)toregularizeawayfromrewardhackingpolicies. Asdescribedinthemain
text,thecoefficientsthatwererunweredeterminedbymultiplyingarangeofscale-independentcoefficientsbytheaverage
per-timesteprewardsineachenvironmentsthatwecalculatedafterrunningevaluationrunsandnegatingthem. Theresults
forthetomato,traffic,glucoseandpandemicenvironmentsareinTables8,9,10,and11respectively. Aplotofthebest
coefficientsalongwiththedivergencevaluesisshowninFigure5.
23
DA
MO
LK
LK
drawereurTPreventingRewardHackingwithOccupancyMeasureRegularization
Coefficient ADKL State-ActionOMKL StateOMKL
-0.4 0.00±1.59 3.98±2.06 4.02±0.74
-0.16 0.00±1.59 4.37±0.74 4.52±0.63
-0.08 0.00±1.59 4.98±0.57 4.36±0.71
-0.04 0.00±0.80 4.64±0.47 5.35±0.61
-0.0016 1.94±2.28 1.31±0.92 0.56±0.89
-0.0008 5.84±2.88 2.18±0.88 2.26±0.93
-0.016 0.00±0.80 5.25±0.52 4.99±0.37
-0.004 0.00±0.80 5.54±0.52 5.23±0.51
-16.0 1.98±1.49 3.98±1.44 2.00±0.01
-0.008 0.00±0.80 5.59±0.32 5.32±0.22
-0.8 0.00±0.80 3.98±1.92 3.98±2.22
-8.0 0.00±1.59 3.98±2.36 3.98±2.00
-4.0 0.00±1.59 3.98±0.00 3.98±2.27
-1.6 1.98±1.49 3.98±1.59 5.86±0.94
Table8. TomatoResults
Coefficient ADKL State-ActionOMKL StateOMKL
-0.00025 -111977.53±17214.26 -3540.67±21315.65 -1217.29±25063.82
-0.0001 -79400.16±8060.00 -1063.95±23432.71 -56205.18±24004.46
-5e-05 -69687.41±9217.50 -24407.41±21935.85 -58568.36±2934.04
-2.5e-05 -65045.02±4714.27 -61487.52±4376.05 -58749.20±8501.17
-1e-06 -58401.46±5709.63 -56992.75±8139.89 -58166.89±8260.66
-5e-07 -58461.39±4383.00 -64768.07±8999.20 -59487.15±7486.11
-1e-05 -58061.92±12651.36 -59513.49±3834.08 -55291.82±7638.28
-2.5e-06 -56013.23±7444.14 -63617.64±6862.90 -57949.39±5170.07
-0.01 -28921.78±97414.16 -204987.75±86439.75 -5167.42±54467.61
-5e-06 -58232.20±2954.65 -59555.21±2851.43 -58309.62±6294.95
-0.0005 -1360.39±30769.19 -1072.47±26376.43 -1100.54±43.69
-0.005 -29328.95±97511.81 -1080.62±11.04 -202327.06±12232.67
-0.0025 -9611.75±88855.63 -1066.31±14.41 -206875.58±21588.90
-0.001 -1265.30±112637.24 -1085.87±9.22 -1146.10±83045.62
Table9. TrafficResults
24PreventingRewardHackingwithOccupancyMeasureRegularization
Coefficient ADKL State-ActionOMKL StateOMKL
-0.015 -10431.09±83660.78 -144386.41±174033.93 -195231.31±20347.91
-0.006 -10455.46±84102.84 -204268.44±164070.55 -219111.33±72876.95
-0.003 -10567.31±83248.45 -235054.15±80412.50 -333025.09±66709.74
-0.0015 -10409.22±82987.79 -326774.83±154161.23 -549874.74±135826.64
-6e-05 -84013.55±105714.03 -490927.45±48825.69 -581792.14±54097.78
-3e-05 -167448.46±67923.39 -578315.55±181073.52 -582141.72±18980.46
-0.0006 -10588.67±102136.84 -574612.89±180942.26 -588377.02±129099.43
-0.00015 -146171.68±64183.33 -63295.08±112176.90 -598283.71±129875.42
-0.6 -10339.89±83918.50 -29134.35±136847.01 -186144.49±17975.91
-0.0003 -216420.47±89003.44 -114448.81±229381.16 -575210.64±23361.36
-0.03 -10360.95±84058.53 -93145.02±29539.90 -196008.38±131953.77
-0.3 -10564.38±83308.06 -23144.75±108330.87 -177928.14±30129.81
-0.15 -10366.68±203.84 -28563.54±146819.94 -177062.41±36374.03
-0.06 -10409.75±82863.95 -53480.07±65091.05 -198117.45±110658.28
Table10. GlucoseResults
Coefficient ADKL State-ActionOMKL StateOMKL
-0.03 -12.92±4.23 -5.47±7.84 -16.46±1.93
-0.012 -12.92±3.99 -20.27±7.43 -15.60±5.19
-0.006 -12.92±3.53 -14.46±1.95 -15.15±1.64
-0.003 -12.92±4.14 -14.93±2.41 -14.30±1.87
-0.00012 -32.88±10.34 -25.61±10.26 -32.31±7.20
-6e-05 -29.86±11.63 -20.02±11.50 -30.95±17.15
-0.0012 -8.42±3.86 -8.45±2.80 -14.53±19.33
-0.0003 -12.86±9.37 -57.43±19.42 -27.83±7.03
-1.2 -12.92±4.72 -5.25±12.84 -14.98±13.08
-0.0006 -8.35±1.94 -24.58±13.98 -45.36±16.31
-0.06 -12.92±3.43 -14.23±5.02 -15.10±1.01
-0.6 -12.92±3.93 -9.13±23.48 -14.28±0.40
-0.3 -8.06±4.47 -13.63±5.47 -19.43±68.02
-0.12 -12.92±3.60 -14.29±9.61 -14.47±1.19
Table11. PandemicResults
C.Environmentdetails
C.1.Tomatoenvironment
InFigure6,wehavethesetupofthetomatoenvironmentboardweusedfortraining.
25PreventingRewardHackingwithOccupancyMeasureRegularization
Figure6. Here,thegraysquaresrepresentwalls,andthewhitesquaresrepresentopenspaceswheretheagentcantravel.
Thesprinklerstateisdownanarrowhallway,andontheotherendatomatoisdownanothernarrowhallway. Wewanted
totryoutascenariowheretherewardhackingwouldberelativelydifficultfortheagenttofindtoseewhetherornotour
methodworksformorecomplexgridworldscenarios.
C.2.Trafficenvironment
InFigure7,wehaveasimplifiedrenderingofthetrafficflowenvironmentmergescenario.
Figure7.Here,thegreencarsarecontrolledbythehumandrivermodelIDMcontroller,andthebluecarsarecontrolledbyRL.
Withinthisparticularframe,rewardhackingistakingplace. AswecanseetheblueRLvehiclehasstoppedcompletelyon
theon-ramp,resultingincarstocollectbehindit. Thisway,theproxyreward,whichistheaveragevelocityofallvehicles
inthesimulation,isoptimizedasthecarsonthestraightwayareabletocontinuespeedingalongtheroadwithouthavingto
waitformergingcars. However,littletonotruerewardoftheaveragecommutetimeisachievedasthecarsontheon-ramp
aren’tabletocontinuetheircommute.
D.Experimentdetails
Here,wegivesomeextradetailsaboutthearchitecturesandhyperparametersweusedfortrainingtheORPOagents. We
buildORPOusingRLLib(Liangetal.,2018)andPyTorch(Paszkeetal.,2019). ForallRLexperimentswetrainwith5
26PreventingRewardHackingwithOccupancyMeasureRegularization
randomseedsandreportthemedianreward.
Networkarchitectures Thepolicymodelforboththetrafficandtomatoenvironmentswasasimplefullyconnected
network(FC-net)withawidthof512anddepthof4. ThepolicymodelfortheglucoseenvironmentisabasicLSTM
networkwith3layers,eachwithwidthsof64. Wemadethischoicesincetheobservationoftheenvironmentcontains
continuoushistoricalinformationaboutthepatient’sbloodglucoselevelsandpreviouslyadministeredinsulin. Themodel
sizeswerechosenaswefoundthatmodelswiththesecapacitiesempoweredtheagentssignificantlyenoughforthemto
rewardhackconsistently.
ThediscriminatormodelforthetomatoandtrafficenvironmentswasasimpleFC-netwithawidthof256anddepthof4.
Fortheglucoseenvironment,wedefinedmultipleconfigurationsforthediscriminatorduetothecontinuousnatureofits
observationspace. First,wehaveanoptiontoallowfortheentirehistoryofthepatientthatiscapturedintheobservationby
defaulttobefedintothediscriminatornetwork,inwhichcasethediscriminatorwillbeanLSTMnetworksimilartothe
policynetworkinordertoproperlyhandlethetimeseriesdata. Bydefault,thelastfourhoursofthepatient’sstatesplit
intofiveminuteintervalswillbefedintothediscriminator,butthereisalsoanoptiontodecreasetheamountofhistory
beingused. Ifnohistoryisusedfortheinputtothediscriminatornetwork,wedefaulttousingthesameFC-netusedforthe
tomatoandtrafficenvironments. Weadditionallyhavetheoptionofusingtheentireobservationprovidedintheglucose
environment(theCGMreadingsofthepatientandtheamountofinsulindelivered)orjusttheCGMreadings.
ORPOtraining: tipsandtricks Naively,wecantrainthediscriminatorusingtheentireactionandobservationgivenby
theenvironmentandstillattainimpressiveperformanceincomparisontoactiondistributionKLregularization,butupon
furtherexperimentation,wefoundthatdifferentsettingsofthediscriminatorcanhelpachievebetterresultswithrespect
totheunknowntruereward. Inparticular,withthecontinuousglucoseenvironment,wefoundthatnotpassingintothe
discriminatorthepatient’sentirehistorythatisencodedintheobservationprovidedbytheenvironmenthelpedperformance.
Intuitively,thiscouldmakesensesinceMDPsdonotrelyonhistory,andoccupancymeasuresonlytakeintoaccountthelast
timestep.
We also found that only feeding in the observation into the discriminator (so that effectively only the state occupancy
measureisbeingcalculated)seemedtofurtherboosttheagent’sperformanceonthehiddentruerewardasitisprimarily
affectedbythestateofthepatient. Additionally,wefoundthatselectivelypassingindifferentelementsoftheobservation,
suchasjusttheCGMreadingsinthecaseoftheglucoseenvironment, alsohelpedpreventrewardhackingbetterthan
naivelyfeedingineverythingtothediscriminatorsincethesevaluesaremostimportantfortherewardfunction.
Wefoundthatwecangetmorestablepoliciesifwetrainthediscriminatoronthelatesttrainingdatabatchestoavoidalarge
distributionshiftwhencalculatingoccupancymeasuredivergences. Ingeneral,settingtheKLtargetparametertobesmaller
canalsomakethetrainingrunsmorestablebecausethepolicieswillnotchangetoorapidlyovertime.
Policyinitialization Initializingusingapre-trainedpolicyhasbeenshowntoeffectivelyspeedupthelearningprocess
(Laidlawetal.,2023;Uchenduetal.,2023)andisusedinpracticeforRLHF(Stiennonetal.,2020),soweinitializeour
policiesusingthespecifiedπ forthemorerealistictraffic,glucose,andpandemicenvironments.
safe
NoteaboutusingKLdivergenceoverTVdivergenceforORPO Whenpresentingourtheoreticalresults,wechoosethe
TVdistanceasithasnicetheoreticalpropertiesthatresultinthetightboundwefind. Itisalsopreferableforourproofssince
itsmagnitudeisbounded. However,asstatedatthestartof4,werelyontheKLdivergencewithinouralgorithmORPO
sinceitismorestabletocalculateinpractice. Furthermore,becausePinsker’sinequalityboundstheTVdistanceintermsof
theKLdivergence,thenicetheoreticalpropertieswefindfortheTVdistancebetweentheoccupancymeasuresofpolicies
alsoholdfortheKLdivergence. Husza´r(2017)usedKLdivergencebecauseofitsrelevancetotheVariationalInference
literature. Specifically,KLisalwaysdifferentiable,whichcanbeusefulwhentrainingstructuressuchasGANs,whereas
TVisn’talwaysdifferentiableeverywhere. Inaddition,KLdivergence’sasymmetryisactuallyseenasadesirablequality
sinceitallowsforthevariableoverestimationandunderestimationofprobabilityindifferentpartsofthedistributions.
Hyperparameters SomehyperparametersforthetrafficenvironmentweretunedbyPanetal.(2022). Wechosethe
hyperparameterslistedbelowinordertoensurethatwithoutanyregularization,rewardhackingwilloccur. Thisway,wecan
actuallyseeifthevariousregularizationmethodsactuallysucceedatpreventingrewardhackingwhentheyareused. More
detailsaboutoursafepolicygenerationandotherparametersrequiredfortrainingcanbefoundwithinourcoderepository.
27PreventingRewardHackingwithOccupancyMeasureRegularization
Hyperparameter Tomato Traffic Glucose Pandemic
Trainingiterations 500 250 500 260
Batchsize 3000 40000 100000 3860
SGDminibatchsize 128 16384 1024 64
SGDepochsperiteration 8 5 4 5
Optimizer Adam Adam Adam Adam
Learningrate 1e-3 5e-5 1e-4 0.0003
Gradientclipping 0.1 None 10 10
Discountrate(γ) 0.99 0.99 0.99 0.99
GAEcoefficient(λ) 0.98 0.97 0.98 0.95
Entropycoefficient(start) 0.01 0.01 0.01 0.1
Entropycoefficient(end) 0.01 0.01 0.01 0.01
Entropyschedulehorizon 0 0 0 500000
KLtarget 0.001 0.02 1e-3 0.01
Valuefunctionlossclipping 10 10,000 100 20
Valuefunctionlosscoefficient 0.1 0.5 0.0001 0.5
Sharevaluefunctionlayers F T T T
Table12. PPO/ORPOhyperparameters.
Hyperparameter Tomato Traffic Glucose Pandemic
Discriminatorrewardclipping 1000 10 1e10 0.1
Regularizationcoefficient(λ) Varied Varied Varied Varied
Epochsfordiscriminatortraining 1 1 1 2
Table13. ORPO-specifichyperparameters.
Thecoefficientλthatisusedfordetermininghowmuchregularizationtoapplywasvariedthroughouttheexperimentsand
notedinourresult. WhileourempiricalresultshavebeengeneratedusingtheKLdivergence,wehaveimplementedsupport
forthetotalvariation(TV)andWassersteindistanceswithinourcode. Afterthoroughexperimentation,wedeterminedthat
theseotherdivergencemetricsarerelativelyunstableincomparisontotheKL-divergence.
28