MAGID: An Automated Pipeline for Generating Synthetic
Multi-modal Datasets
Hossein Aboutalebi∗, v Hwanjun Song  Yusheng Xie  Arshit Gupta 
Justin Sun  Hang Su  Igor Shalyminov  Nikolaos Pappas 
Siffi Singh  Saab Mansour 
v Cheriton School of Computer Science, University of Waterloo
AWS AI Labs
haboutal@uwaterloo.ca
Abstract conversations (Alayrac et al., 2022; OpenAI,
2023; Liu et al., 2023a).
Development of multimodal interactive
Several multi-modal dialogue datasets like
systems is hindered by the lack of rich,
MMDialog (Feng et al., 2022), DialogCC (Lee
multimodal (text, images) conversational
data, which is needed in large quanti- et al., 2022)2, and PhotoChat (Zang et al.,
ties for LLMs. Previous approaches aug- 2021) have been introduced for training multi-
ment textual dialogues with retrieved im- modal LLMs. These datasets either use a
ages, posing privacy, diversity, and qual- retrieval-based approach, pulling images from
ity constraints. In this work, we intro-
set image banks, such as MS-COCO (Lin
duce Multimodal Augmented Generative
et al., 2014), or restrict the dialogue to only
Images Dialogues (MAGID), a framework
one image per conversation, even if they in-
to augment text-only dialogues with di-
verse and high-quality images 1. Subse- volve real human-human chats. Moreover,
quently, a diffusion model is applied to whenleveragingreal-worlddatasetsfromplat-
craftcorrespondingimages,ensuringalign- forms like social media, issues related to pri-
ment with the identified text. Finally, vacy concerns and image quality become sig-
MAGID incorporates an innovative feed-
nificant challenges for training.
back loop between an image description
As a result, these methods limit the diver-
generation module (textual LLM) and im-
sity of images since the small image database
age quality modules (addressing aesthet-
ics, image-textmatching, andsafety), that cannot adequately capture the wide range of
work in tandem to generate high-quality real human-human conversations (Lee et al.,
and multi-modal dialogues. We compare 2021,2022). Additionally,theyfacechallenges
MAGID to other SOTA baselines on three stemming from low-quality images contain-
dialogue datasets, using automated and
ing harmful and private content (Feng et al.,
human evaluation. Our results show that
2022) and shortage of accessible data (Lee
MAGID is comparable to or better than
et al., 2022), particularly when utilizing real
baselines,withsignificantimprovementsin
human-human conversations from social me-
human evaluation, especially against re-
trieval baselines where the image database dia sources.
is small. To address these challenges, we propose
MAGID, a generative-based multi-modal di-
1 Introduction
alogue creation framework. As illustrated in
In recent years, advancements in large lan- Figure 1, MAGID aims at converting exist-
guage models(LLMs) have expanded possibil- ingtext-onlydataintocontext-enrichedmulti-
ities and research directions in AI, with stud- modal data by addressing the two research
ies highlighting their extensive capabilities in challenges: (i) how to find the most suitable
handling dialogue datasets (Liu et al., 2023c; utterances that can be enhanced by adding
Penedo et al., 2023). Specifically, there is a images and (ii) how to generate realistic and
growing interest in their application to multi- diverse images that do not have harmful and
modal dialogue datasets, given that sharing private contents.
images is an integral aspect of human-human
2A recently released version of DialogCC utilizes
∗WorkconductedwhileinterningatAWSAILabs. LLM (Lee et al., 2023). At the time of writing this
1The link to code: http://anon for review.com paper, we did not have access to the newer version.
4202
raM
5
]LC.sc[
1v49130.3042:viXraText-only Dialogue No (Feedback Loop) Multi-modal Dialogue
Hey, did I leave anything Hey, did I leave anything
at your house? at your house?
Image Description by LLM
Image Quality Score
You left your Halloween Selected Image of a 6-foot tall You left your Halloween
costume at my place pink men’s bunny outfit. costume at my place
Aesthetic Score Good?
Yes
I was worried it was lost Prompt
forever. to Diffusion Content Safety Score
Diffusion-based Image
LLM-based Scanner Quality Assurance Module I was worried it was lost
Generator forever.
Figure 1: Overview of the MAGID framework. MAGID consists of three components: (1) LLM-based
scanner to identify suitable utterances to augment with images, (2) diffusion-based image generator to
create realistic images, and (3) quality assurance module to enhance the image quality, aesthetic and
safety scores. The text-only dialogue is automatically converted to multi-modal dialogue using MAGID.
In the former case, we introduce an LLM- only one image per dialogue. Consequently,
based scanner designed to pinpoint utterances MAGID generates synthetic yet more realistic
requiring images and subsequently generate multi-modal dialogue datasets thus mitigating
corresponding image descriptions, leveraging dataaccessibilityissuesandfacilitatingthede-
chain-of-thought prompting. In the latter velopment of advanced multi-modal models.
case, we employ a diffusion-based image gen- To summarize, our main contributions are:
erator, adept at crafting images with notable
• We present MAGID, a generative-based
diversity, drawing upon the generated image
multi-modal dialogue data creation frame-
descriptions as its input. Additionally, a qual-
work that addresses the limitation of
ity assurance module is incorporated into our
retrieval-based approaches.
framework to ensure both the congruence and
the quality of the produced images, thereby • We conduct experiments using various
preserving coherence and fidelity within the prompt engineering strategies to optimize
multi-modal dialogue. Should the generated interactions between the LLM-based scan-
image not satisfy the criteria of this module, ner and the diffusion-based image genera-
MAGID initiates a feedback loop, revisiting tor.
theprocessesofpromptandimagegeneration. • We propose a novel quality assurance de-
Distinct from numerous previous endeav-
sign to control the performance of genera-
ors that have depended on image-retrieval
tive models effectively.
techniques for curating multi-modal datasets
• We provide a medium-sized dataset as a
(Lee et al., 2021, 2022)—a method that
proof of concept to showcase the effective-
might result in restricted image diver-
ness of MAGID pipeline (section 5).
sity and potential mismatch with the di-
alogue existing utterances—weemploythe • Weconductextensivehumanevaluationson
generative model Stable Diffusion XL (Podell the dataset and test multiple LLM models
et al., 2023). By training on billions of im- to ensure robustness and reliability.
ages (Schuhmann et al., 2022), this approach
guaranteesanoutputthatisbothrichandvar-
2 Related Works
ied. Suchoutputsalignwellwiththeconversa-
tional context provided by the LLM feedback, 2.1 Generative Models
thereby elevating the quality and diversity of Recent advances in Generative AI has started
our multi-modal dataset. new trends in expanding capabilities of exist-
Our framework aligns with prior studies ing deep learning models. In NLP, works like
using text-only datasets(Lee et al., 2021, (Radford et al., 2019; Ouyang et al., 2022)
2022), but it addresses the limitations asso- have shown importance of training data to
ciated with their retrieval-based strategies by build better LLM models. In this regard,
employing a generative-based data creation recent LLM models like Falcon-40b-Instruct
method. Unlike Liu et al. (2023a); Lee et al. (Penedo et al., 2023), Koala 13b (Geng et al.,
(2021), we do not restrict the inclusion of 2023), LLaMA 13b (Touvron et al., 2023),Zero shot prompt
YouareanAIassistantthathelpsaugmenttextualdialogueswithengagingimages. Asinput,youwillreceiveaconversationbetween
peoplewhichisrepresentedasasequenceofutterances. Asoutput,youwillgenerateadescriptionofimagesthatcansupportthe
utterancesintheconversation.
Theformatoftheinputis’Utterancei: ...’ where’i’denotestheorderoftheUtteranceintheconversation. Giventhisquery,you
outputintheformatof
<result>Utterancei: imagedescription</result><reason>explanationofchoice</reason>
where’i’istheUtteranceintheconversationand’imagedescription’istheshorttextdescriptionofanimagethatcanbefollowed
bythatUtterancethatcanmaketheconversationmoreengaging. Youshouldonlyidentifythemostappropriateutterancesinthe
conversation.
Thetextinside<reason>explanationofchoice</reason>istheexplanationofwhyyoupickedtheutterancewiththeimagede-
scription.
Figure 2: The zero-shot prompt of the scanner module (Section 3.1) which selects turns in the dialogue
to augment with images and generates descriptions of those images. Additional few-shot and chain-of-
thought prompts are provided in the supplementary materials (section A).
OpenLLaMA (Touvron et al., 2023), and Vi- ing video and voice. For example, Corona
cuna 13b (Chiang et al., 2023) use better cu- et al. (2021) provide a dataset that contains
rated training datasets to achieve higher per- videos for activity detection. IntenVid (Wang
formances. In this regard, paper like Chris- et al., 2023) is another example that contains
tiano et al. (2017) has shown the dramatic im- video in addition to text.
pact of using higher quality data (from hu-
3 MAGID Pipeline
man feedback) in faster training. Yet, using
human feedback and crowd-sourcing is not al- Intransitioningfromtext-onlytomulti-modal
ways cheap. To address this, emerging works dialogue, there exist two core challenges. The
like (Veselovsky et al., 2023; Kamalloo et al., first is the identification of the most suitable
2023) suggests that LLM has the capabilities utterances within the dialogue that can be en-
of performing the task of human generated hanced by images. The second is the creation
dataset. In addition, diffusion models in com- of corresponding, accurate images that align
puter vision have shown promising results in with the selected utterances. In this regard,
generating images indistinguishable from real we need to ensure a harmonious and coherent
ones (Podell et al., 2023; Ho et al., 2020). Fi- matchbetweentheimageandthetext,achiev-
nally, recent works focus on building multi- ing acceptable image-text alignment.
modal LLM models including GPT-4 (Ope- Wehaveaddressedthesechallengesthrough
nAI, 2023), LLaVA (Liu et al., 2023b), Any- the implementation of the following three
MAL(Moon et al., 2023) which supports any key modules in Figure 1, namely LLM-based
modality. Specifically, LLaVA accepts multi- scanner, diffusion-based image generator, and
modal input, combining image and text em- quality assurance module, which are detailed
beddings to generate text-only output. in the subsequent sections.
3.1 MAGID Scanner
2.2 Multi-modal Dataset Creation
The primary objective of this module is to
There are also works which focus on gener-
identify suitable utterances that can be visu-
ating multi-modality datasets. In particular,
ally represented by an image. Achieving best
MMDD (Lee et al., 2021) and DialogCC (Lee
performance requires precise control over the
et al., 2022) use image-retrieval approaches
behavior of the LLM model. We use prompt
to augment text-only datasets to multi-modal
engineering and special formatting to control
datasets. PhotoChat (Zang et al., 2021) hires
the output of LLM.
workers to discuss a particular image to build
We experimented with three prompt en-
the dataset. MMDialog (Feng et al., 2022)
gineering strategies to fine-tune the system
collect multi-modal conversations from inter-
prompts of the LLM:
net to build the dataset which can poten-
tially pose privacy concern to use as train- • Zero-shot prompting: The LLM is pro-
ing set. There are also works (Wang et al., vided with only the format of the input and
2023;Coronaetal.,2021,2020;Cilibertoetal., the expected output, along with a general
2021; Abdrakhmanova et al., 2021) which fo- problem description. Figure 2 shows an ex-
cuses modality beyond text and image includ- ample of the zero-shot prompt.What did you cook this Diwali? Share those recipes at
Scanner (GPT-4 Answer): the link below and stand a chance to win a gift card
worth INR 500 from us! Submit your entries here:
<reason>
Utterance: 0
Utterance 0 is asking for recipes and offering a gift card as a prize. An image of
the gift card can be used to attract more participants.
Utterance 1 does not require an image as it is a response to the previous
utterance.
Utterance: 1
Utterance 2 is asking someone to look at something, so an image of what they
want the other person to look at would be appropriate. Done team. Wish to win Utterance: 2
</reason>
Hey!! Please give it a look once
<result>
Utterance: 0: An image of a gift card worth INR 500
Utterance: 2: An image of the thing the speaker wants the other person to look at
</result>
Figure 3: MAGID’s chain of thought prompting facilitates debugging and identification of corner cases,
utilizingtheSDXL1.0diffusionmodelandGPT-4(OpenAI,2023). Thedepictedconversationissourced
from a real human-human interaction in the MMDialog dataset (Feng et al., 2022).
• Few-shot example prompting: Be- Figure 3 demonstrates the impact of using
sides the information provided in zero- the proposed HTML formatting inside chain
shot prompting, LLM is also supplied with of thought prompt, revealing how meticulous
several input–output exemplars to demon- analysis of responses identifies corner cases
strate the anticipated response from the and ensures contextual congruency in pro-
LLM model (Brown et al., 2020). We have duced images. Whereas the first image aligns
included this type of prompt in supplemen- with preceding text, the second lacks context.
tary materials (section A). The < reason > tag discloses that phrases like
”give it a look” influenced image generation.
• Chain of Thought prompting: As per
To enhance contextual relevance and model
(Wei et al., 2022), this prompting strat-
reliability, the system prompt has been re-
egyinvolvesimpartingaseriesofintermedi-
finedtoinstructtheLLMtoonlygenerateim-
ate reasoning steps for each example, facil-
ages when paired with a detailed description,
itating the LLM model’s capacity for more
thereby avoiding contextual discrepancies.
advanced reasoning. Please refer to sup-
plementary materials for example of this
prompt (section A).
3.3 MAGID Image Generator
In section 4.3.1, we evaluated these prompt-
As illustrated in Figure 1, the LLM model’s
ing strategies. Based on the findings, we se-
imagepromptsareusedbythediffusionmodel
lected Chain of Thought prompting as the op-
to generate corresponding images. In this re-
timal choice for our MAGID framework.
gard, given the success of diffusion models in
3.2 Controlling LLM Output Format superior image generation (Rombach et al.,
2022; Ho et al., 2020), were chosen over GANs
We introduce a method that seeks to stream-
(Goodfellow et al., 2014). Models tested in-
line the structuring of LLMs outputs by
cluded SDXl 1.0, SDXL 0.9, and Stable Diffu-
employing HTML-like tags, aiming to facil-
sion versions from Stability AI (Podell et al.,
itate easier parsing and to shed light on
2023), with a detailed comparison in supple-
the decision-making process. The utiliza-
mentary materials (section C).
tion of < result > and < reason > tags is in-
tended to envelope answers and rationales re- Ultimately, SDXl 1.0 was chosen for its
spectively, potentially making post-processing state-of-the-art capabilities, bolstering the
more straightforward and offering a degree quality and reliability of the generated images
of transparency into the model’s reasoning, of the MAGID dataset. Nevertheless, future
which may be beneficial for debugging pur- modeldevelopmentscanbeincorporatedtore-
poses. fine our MAGID dataset generation.i am about to take my dog frisket i am about to take my dog frisket Philadelphia roll is a solid choice! I'm a sucker Philadelphia roll is a solid choice! I'm a sucker
out for a walk . out for a walk . for anything with spicy tuna and avocad for anything with spicy tuna and avocad
that sounds fun i enjoy walks !
me too my favorite walks are in the Dragon rolls are really good too. Dragon rolls are really good too.
park enjoying nature . Yes and spicy tuna! Yes and spicy tuna!
me too my favorite wath lka st s ao reu n ind s t hf eu n i enjoy walks ! oh coo stl r ! e em t i tn oe m a yre b w esa tl fk ri in eng d d so hw on u sth e e . I'm con Hsi od wer i dn og e o s r td he ir si n log o s ko tm o e y osu us ?h i W ta hk oe oo pu st I'm con Hsi od wer i dn og e o s r td he ir si n log o s ko tm o e y osu us ?h i W ta hk oe oo pu st
park enjoying nature . i just opened my own custom cake shop .
oh cool ! mine are walking down the
street to my bestfriends house . that sounds so awesome ! i
just bought a honda civic .
i just opened my own custom cake shop . i love the honda civic but i like the cr v
more . that trunk space .
I had a thing that was sushi inside I had a thing that was sushi inside
ll . i like those old dependable cars ! of an avocado. It was amazing. of an avocado. It was amazing.
that sounds so awesome ! i
just bought a honda civic . Oh wow
i love the honda civic but i like the cr v
more . that trunk space .
ll . i like those old dependable cars ! Oh wow
(a) MAGID (left) vs. MMDD (right). (b) MAGID (left) vs. PhotoChat (right).
Figure 4: Qualitative comparison of MAGID with an image retrieval-based synthetic MMDD and a real
human image-based PhotoChat datasets.
3.4 MAGID Quality Assurance descriptiongivenregeneratedimageswithpre-
viousimagedescriptioncontinuouslyfallshort
The Quality Assurance (QA) module is essen-
of quality assurance standards.
tial for improving the MAGID pipeline’s effi-
Figure 4 displays a comparison of MAGID
ciency. It assures the generated images satisfy
samples with two other datasets, MMDD (Lee
user-set standards in three domains: Image-
et al., 2021) and PhotoChat (Zang et al.,
Text Matching, Image Quality, and Im-
2021). A qualitative analysis shows MAGID
age Safety.
yields quality comparable to real datasets,
1- Image-text Matching: We use the
such as PhotoChat, and surpasses synthetic
CLIP score (Radford et al., 2021) to validate
datasets like MMDD in generating high-
the match between the image and the LLM
quality multi-modal dataset. More examples
model’s utterance. A low CLIP score triggers
are included in supplementary (section H).
imageregeneration,withthecountdetermined
as a hyperparameter. In this work, we set the
4 Evaluation
regeneration count to two.
2- Image Quality: Images are rated based We scrutinize the efficacy and applicabil-
on an aesthetic score from (Schuhmann et al., ity of the multi-modal dataset generated by
2022; Schuhmann, 2023), which uses CLIP MAGID. Here are three pivotal questions we
embedding followed by an MLP. This model addressed in evaluation:
identifies artifacts in the diffusion model out-
1. How does MAGID quantitatively compare
puts. A threshold of 0.51 efficiently detects
against real multi-modal datasets? ▷ Sec-
most artifacts, prompting image regeneration
tion 4.1
for scores below this.
3- Image Safety: Image safety, particu- 2. Can MAGID create a multi-modal dataset
larly against NSFW content, is crucial. While with human-eye perceptible quality like a
many models assess this, few unsafe images real one? ▷ Section 4.2
were found in our dataset, indicating our pro- 3. What is the impact of scanner prompt tun-
cess’s reliability. ing and the quality assurance module on
This robust QA ensures that MAGID can MAGID? ▷ Section 4.3
output relevant, high-quality, and safe images.
The first and third question delves into a
3.4.1 Feedback Loop quantitative analysis, probing the accuracy
Should the diffusion model produce an image and quality of the data generated by MAGID.
thatdoesnotmeetthequalityassurancemod- Moreover, the second question is crucial, as
ule’s stipulations, the issues might stem from a failure of MAGID to meet human evalua-
theLLMmodel’sprompt. Faultypromptscan tion standards would result in a low-quality
yield low image-text matches or unsafe im- training dataset that is unable to get positive
ages. To mitigate this, our design, showcased human-centric assessments.
in Figure 1, includes a feedback loop, instruct- In addition, in supplementary (section E),
ing the LLM model to generate a better image we have studied training multimodal modelTable 1: Scanner module performance as measured by turn selection for image augmentation (accuracy,
precision, recall, F1) and the resulting images from the generated descriptions (CLIP, MM-relevance,
aesthetic) on the MMDialog dataset as ground-truth. The quality assurance module is enabled. We
compare various LLMs powering the scanner module using chain of thought prompting.
Model Accuracy Precision Recall F1 score CLIP score MM-Relevance Aesthetic #images
GPT 4 67.24% 70.49% 46.87% 0.56 0.27 294.52 0.57 1359
GPT 3.5 63.54% 69.43% 33.97% 0.46 0.26 293.51 0.58 1001
Falcon-40b-Ins. 58.93% 61.26% 24.13% 0.35 0.25 254.50 0.58 794
Koala 13b 56.28% 62.33% 6.91% 0.12 0.25 243.31 0.57 223
Llama 13b 57.10% 60.00% 13.64% 0.22 0.25 247.99 0.57 460
OpenLLaMA 57.94% 64.36% 12.69% 0.21 0.25 250.96 0.58 390
Vicuna 13b 58.77% 66.60% 14.38% 0.24 0.26 255.18 0.57 506
MMDialogue3 N/A N/A N/A N/A 0.262 N/A 0.47 2717
with MAGID and compared it with using real the subsequent four columns measure accu-
images for training. racy,precision,recall,andF1scoreinchoosing
the correct utterance to be augmented with
4.1 Quantitative Evaluation
an image. The CLIP score gauges image-text
Setup. Addressing the first question, a matching, and the MM-Relevance, as intro-
multi-dimensional evaluation assessed the im- duced in (Feng et al., 2022), denotes the sim-
age quality and accuracy of MAGID in se- ilarity between responses. In our context, it
lecting right utterances. To fairly compare determines the resemblance of the produced
MAGID’s general-use applicability, we only image to the MMDialog’s original image. The
utilized prompt engineering to guide the LLM next column, the aesthetic score, indicates the
model to select the right utterances. In this image quality as discussed in (Schuhmann,
regard, as a ground truth, we selected human- 2023). Last row presents the ground truth
human interaction datasets MMDialog and dataset, highlighting the CLIP score, image
PhotoChat, and removed images from their count, and aesthetic quality of its images.
test sets and employed MAGID to transform From the table, it is evident that GPT-4
the text-only data into a multi-modal dataset. andGPT-3.5outperformsothermodelsacross
For the LLM-based model, we adopted a all metrics. Notably, the CLIP and aesthetic
range of models, including GPT-4 (OpenAI, scores of MAGID using GPT-4 and GPT-3.5
2023), GPT-3.5 (OpenAI, 2023), Falcon-40b- surpass even the ground truth values. In
Instruct (Penedo et al., 2023), Koala 13b the next section, we also examine image-text
(Geng et al., 2023), LLaMA 13b (Touvron matching and image quality in our human
et al., 2023), OpenLLaMA (Touvron et al., evaluation for MAGI against other datasets to
2023), and Vicuna 13b (Chiang et al., 2023). test if it is aligned with our quantitative find-
For image generation, SDXL 1.0 was consis- ings.
tently utilized across all models. We present
the results of the MMDialog dataset here, and 4.2 Human Evaluation
the PhotoChat results are included in supple-
Setup. We conducted a human evaluation
mentary (section B). In these experiments, we
using a website with questionnaire. Partici-
have set the threshold for the CLIP model
pants viewed two dialogues: one with an im-
at 0.21 and the aesthetic score threshold of
age from MAGID and another from datasets
0.51. We used grid search to find these hyper-
MMDD (Lee et al., 2021), PhotoChat (Zang
parameters. More details on computational
et al., 2021), or MMDialog (Feng et al., 2022).
cost is provided in supplementary (section F).
MAGID used GPT-4 as its Language Model
Result. Table 1 presents the performance and SDXL 1.0 for image generation. From the
of various LLM models on the MMDialog mentioned datasets, we selected 20 dialogues
dataset. The table quantifies MAGID’s re- each, totaling 60 dialogues, and replaced their
sponse generation using different LLM models images with MAGID’s. During evaluation,
in comparison to the MMDialog dataset. The participants compared MAGID’s multi-modal
first column lists the LLM models used, while dialogues with the originals, without informa-Table2: HumanEvaluationresultsofMAGIDcreateddatasetsversusaretrieval-basedsyntheticdataset,
MMDD, and two real datasets, MMDialouge and PhotoChat, where the mean shows the percentage of
time the dialogues in one dataset were preferred among participants. (Q1: more realistic dialogue? Q2:
imagesinwhichdialogueprovidemoreknowledge?,Q3: bettertext-imagematched?,Q4: bettercontext-
image matched?, Q5: more engaging?, Q6: hegher image quality?)
(a) MAGID vs. MMDD (b) MAGID vs. MMDialogue (c) MAGID vs. PhotoChat
Mean Mean Gwet’s Mean Mean Gwet’s Mean Mean Gwet’s
#
MAGID MMDD AC1 MAGID MMDial. AC1 MAGID Photo. AC1
Q1 96.29% 3.71% 0.74 48.17% 51.83% 0.63 58.11% 41.89% 0.47
Q2 96.29% 3.71% 0.89 49.33% 50.67% 0.65 68.24% 31.76% 0.71
Q3 89.11% 10.89% 0.75 52.72% 47.28% 0.54 64.90% 35.10% 0.53
Q4 91.11% 8.89% 0.83 46.31% 53.69% 0.65 61.98% 38.02% 0.54
Q5 95.57% 4.43% 0.89 51.94% 48.06% 0.63 64.02% 35.98% 0.61
Q6 80.92% 19.08% 0.65 63.90% 36.10% 0.55 69.99% 30.01% 0.64
Table 3: Utterance selection accuracy using three the alternative dataset. Gwet’s AC1 measure,
different prompts on MMDialogue (ground-truth),
found in the last column, was used to as-
where ZS, FS, and CoT stand for zero-shot, few-
sess inter-annotator reliability. It offers stabil-
shot, and chain of thought respectively.
ity over Cohen’s Kappa (Wongpakaran et al.,
F1 2013) and is more resilient to outliers (For
Prompt Accuracy Precision Recall
score
more explanation, please refer to Supplemen-
ZS 65.53% 73.12% 36.16% 0.48 tary Materials section G.).
FS 63.89% 69.67% 34.45% 0.46
From Table 2(a), it’s evident that anno-
CoT 68.51% 73.37% 47.32% 0.57
tators favored MAGID over the synthetically
tion about the dialogue origins. generated MMDD dataset across all question
For each dialogue pair (one from MAGID categories. Moreover, the high Gwet’s AC1
and one from the benchmark datasets), par- value indicates a strong consensus among an-
ticipants responded to the following prompts: notators in choosing MAGID over MMDD. In
contrast, when examining Table 2(b), annota-
Q1: Which dialogue appears more realistic?
tors exhibited a slight preference for the au-
Q2: Which dialogue’s images convey greater
thentic MMDialog dataset in terms of realism.
knowledge?
Notably, theGwet’sAC1valueisconsiderably
Q3: In which dialogue is there better match lowerherethanintheMMDDresults,suggest-
between images and the immediately pre- ing a reduced consensus among annotators.
ceding text? Nevertheless, MAGID outperformed MMDi-
Q4: In which dialogue do the images more alog in terms of image quality and image-
closely match with the overall conversa- text matching. Such findings affirm our
tion context? quantitative evaluations and showcase
Q5: Which dialogue is more engaging? the potential of generative AI in produc-
ing superior data sources for training.
Q6: Which dialogue features higher quality
As for the PhotoChat dataset (Table 2(c)),
images?
while it is constructed from authentic human
Respondents selected from binary choices (Di- interactions, human participants were told to
alogue A or Dialogue B) for each prompt. mock real conversation. Interestingly, our an-
For this evaluation, 15 human annotators notators slightly leaned towards MAGID over
provided their answers. Schema of the website PhotoChat. This outcome suggests MAGID’s
interface are available in the Supplementary promising capability to serve as an alterna-
materials (section D). tive to Mechanical Turk in the development
of multi-modal datasets.
Result. Table 2 displays MAGID’s results
against MMDD, MMDialog, and PhotoChat
4.3 Ablation Study of MAGID
datasets. The ‘Mean MAGID’ column shows
thepercentageofannotatorsfavoringMAGID, We conducted ablation studies on (1) using
while ‘Mean Other’ indicates those preferring different prompts for utterance identificationTable4: AblationresultsoftheMAGIDframeworkwithandwithoutthequalityassurance(QA)module.
Results on turn selection and image quality performance across four LLMs on MMDialog (ground-truth)
are shown. The first four rows are the results with the QA module, while the last four are the results
without. The system prompt is chain of thought.
Model Accuracy Precision Recall F1 score CLIP score MM-Relevance Aesthetic #images
GPT 4 67.24% 70.49% 46.87% 0.56 0.27 294.52 0.57 1359
GPT 3.5 63.54% 69.43% 33.97% 0.46 0.26 293.51 0.58 1001
Falcon-40b-Ins. 58.93% 61.26% 24.13% 0.35 0.25 254.50 0.58 794
OpenLLaMA 57.94% 64.36% 12.69% 0.21 0.25 250.96 0.58 390
GPT 4 67.86% 69.70% 50.64% 0.59 0.27 282.25 0.55 1485
GPT 3.5 68.51% 73.37% 47.32% 0.57 0.26 278.16 0.55 1109
Falcon-40b-Ins. 56.77% 53.58% 28.80% 0.37 0.23 224.59 0.55 1075
OpenLLaMA 58.92% 62.50% 21.51% 0.32 0.21 213.56 0.56 696
and(2)investigatingtheimpactofourquality Table 5: Statistics of the MAGID dataset.
assurance (QA) module. Category Train Test
Total dialogues 47643 5977
4.3.1 Prompts for Scanner
Avg length of dialogues 11.76 11.36
Table 3 displays the outcomes of three prompt Avg length of sentences 9.77 9.60
strategies, namely Zero-shot (ZS) prompt- Total images 67951 10229
ing, Few-shot prompting (FS), and Chain of
replacingitsimageswithMAGID)togenerate
Thought (CoT) prompting, as applied to the
a multi-modal dataset 4 of 53,620 dialogues.
GPT-3.5 model for MAGID. These results are
Based on the results of our experiments, we
reportedfortheMMDialogdataset,withqual-
used GPT-3.5 to transform 47,868 input dia-
ity assurance deactivated, to solely measure
logues and GPT-4 to augment the rest. Table
the accuracy of the LLM model. Notably, the
5 shows the statistics of the generated dataset
Chain of Thought strategy outperforms the
with MAGID. The data and the code will be
other two across all evaluated metrics.
made available to the public upon acceptance.
4.3.2 Impact of QA Module 6 Conclusion
Table 4 showcases the performance of four We presented a generative, fully automated
LLMmodelsinMAGID,contrastingwhenthe pipeline designed to transform text-only
QA module is either enabled or disabled. A datasets into multi-modal variants, harness-
perusal of Table 4 reveals a decline in the aes- ing the power of LLMs through prompt en-
thetic score, MM-Relevance, and CLIP score gineering. This solution addresses limitations
acrossallmodelsuponthedeactivationofQA. faced by preceding methods, notably in terms
Moreover, a noticeable decrement in the pre- of data privacy, accessibility, constrained im-
cision of most models is observable, validating age distribution, and occurrences of unsuit-
that the QA module bolsters MAGID by en- able or non-consensual content. Crucially,
hancing precision in pinpointing the optimal our pipeline permits the substitution of real,
utterance for image generation. In contrast, potentially privacy-compromising images with
disabling QA leads to an elevation in recall, synthetic counterparts. We thoroughly evalu-
attributable to MAGID selecting a more ex- ated our multi-modal data generation method
tensive array of utterances for image genera- using human assessment, quantitative analy-
tion, thereby reducing the ratio of false neg- ses with various LLMs, and an in-depth ab-
atives. Future research could explore the de- lation study. The promising results highlight
velopment of a refined QA module capable of generative AI’s capability to stand as an alter-
elevating the recall rate for the entire pipeline. nativetotraditionaldatagenerationmethods,
5 MAGID Dataset like mechanical turk.
Looking ahead, our dataset paves the way
As a proof of concept, and consistent with
for developing large multi-modal language
studies like (Lee et al., 2021), we employed
models that can engage with users via both
text-only datasets such as DailyDialog (Li
text and visuals.
et al., 2017), Persona-Chat (Zhang et al.,
2018), and PhotoChat (Zang et al., 2021) (by 4The link to dataset: http://anon for review.comLimitations Alexander Wong. 2023. Deepfakeart challenge: A
benchmark dataset for generative ai art forgery
This paper predominantly concentrates on and data poisoning detection. arXiv preprint
augmenting the privacy, diversity, and quality arXiv:2306.01272.
of multi-modal dataset generation by employ-
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,
ing LLM and diffusion models. Although uti- Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc, Arthur Mensch, Katherine Millican, Malcolm
lizing generative diffusion models can mitigate
Reynolds, et al. 2022. Flamingo: a visual language
issuesrelatedtoprivacybreaches—giventhese modelforfew-shotlearning. AdvancesinNeuralIn-
models are also trained on extensive volumes formation Processing Systems, 35:23716–23736.
of web images—they are susceptible to copy-
James Betker, Gabriel Goh, Li Jing, Tim Brooks,
right infringement (Aboutalebi et al., 2023). Jianfeng Wang, Linjie Li, Long Ouyang, Jun-
Addressingthisissueexceedstheambitofthis tang Zhuang, Joyce Lee, Yufei Guo, et al.
2023. Improving image generation with better
paperandpresentsacompellingavenueforfu-
captions. Computer Science. https://cdn. openai.
ture work. com/papers/dall-e-3. pdf, 2(3):8.
Moreover, the current work exclusively em-
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
phasizes image and text modalities. Ex- Subbiah, Jared D Kaplan, Prafulla Dhariwal,
tending considerations to additional modal- Arvind Neelakantan, Pranav Shyam, Girish Sastry,
Amanda Askell, et al. 2020. Language models are
ities—such as video sharing, voice sharing,
few-shot learners. Advances in neural information
and more—is recommended for subsequent re- processing systems, 33:1877–1901.
search endeavors. In addition, fine-tunning of
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
large language model to generate image is left
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
to future works. Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
Improving generated image consistency in
sourcechatbotimpressing gpt-4with90%*chatgpt
the dialogue is another important aspect that quality.
can further improve the quality of the gen-
PaulFChristiano,JanLeike,TomBrown,MiljanMar-
erated multi-modal dataset by MAGID. Em-
tic, Shane Legg, and Dario Amodei. 2017. Deep re-
ploying more recent diffusion models such as inforcement learning from human preferences. Ad-
DALL-E 3 (Betker et al., 2023) can address vancesinneuralinformationprocessingsystems,30.
this problem as they can make more consis- MathiasCiliberto,VitorFortesRey,AlbertoCalatroni,
tent image generation. In this regard, in the PaulLukowicz,andDanielRoggen.2021. Opportu-
nity++: A multimodal dataset for video-and wear-
section J of Supplementary materials, we have
able, object and ambient sensors-based human ac-
included further examples that shows the lim- tivity recognition. Frontiers in Computer Science,
itations of the proposed MAGID pipeline. 3:792065.
In conclusion, the enhancement of our qual-
Kellie Corona, Katie Osterdahl, Roderic Collins, and
ity assurance module is pivotal for developing Anthony Hoogs. 2020. Meva: A large-scale multi-
view. Multimodal Video Dataset for Activity Detec-
more realistic multi-modal datasets from text-
tion.
only inputs. In this regard, works like (Tian
et al., 2023) already showed that using syn- Kellie Corona, Katie Osterdahl, Roderic Collins, and
Anthony Hoogs. 2021. Meva: A large-scale multi-
thesized images is effective. This work prior-
view, multimodal video dataset for activity detec-
itizes aspects like aesthetic score, clip score, tion. In Proceedings of the IEEE/CVF winter con-
and safety. Future research can explore addi- ference on applications of computer vision, pages
1060–1068.
tional elements to further refine and add re-
alism to the transformation into multi-modal Alexey Dosovitskiy, Lucas Beyer, Alexander
Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
outputs.
Thomas Unterthiner, Mostafa Dehghani, Matthias
Minderer,GeorgHeigold,SylvainGelly,etal.2020.
An image is worth 16x16 words: Transformers
References
for image recognition at scale. arXiv preprint
Madina Abdrakhmanova, Askat Kuzdeuov, Sheikh arXiv:2010.11929.
Jarju, Yerbolat Khassanov, Michael Lewis, and
Huseyin Atakan Varol. 2021. Speakingfaces: A Jiazhan Feng, Qingfeng Sun, Can Xu, Pu Zhao,
large-scale multimodal dataset of voice commands Yaming Yang, Chongyang Tao, Dongyan Zhao,
with visual and thermal video streams. Sensors, and Qingwei Lin. 2022. Mmdialog: A large-
21(10):3465. scale multi-turn dialogue dataset towards multi-
modal open-domain conversation. arXiv preprint
Hossein Aboutalebi, Daniel Mao, Carol Xu, and arXiv:2211.05719.Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wal- Seungwhan Moon, Andrea Madotto, Zhaojiang Lin,
lace,PieterAbbeel,SergeyLevine,andDawnSong. Tushar Nagarajan, Matt Smith, Shashank Jain,
2023. Koala: A dialogue model for academic re- Chun-FuYeh,PrakashMurugesan,PeymanHeidari,
search. Blog post, April, 1. Yue Liu, et al. 2023. Anymal: An efficient and
scalable any-modality augmented language model.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, arXiv preprint arXiv:2309.16058.
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville,andYoshuaBengio.2014. Generativead- ROpenAI.2023. Gpt-4technicalreport. arXiv,pages
versarial nets. Advances in neural information pro- 2303–08774.
cessing systems, 27.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020.
CarrollWainwright,PamelaMishkin,ChongZhang,
Denoising diffusion probabilistic models. Advances
SandhiniAgarwal,KatarinaSlama,AlexRay,etal.
in neural information processing systems, 33:6840–
2022. Training language models to follow instruc-
6851.
tionswithhumanfeedback. Advances in Neural In-
formation Processing Systems, 35:27730–27744.
Ehsan Kamalloo, Aref Jafari, Xinyu Zhang, Nan-
dan Thakur, and Jimmy Lin. 2023. Hagrid:
A human-llm collaborative dataset for genera- KishorePapineni,SalimRoukos,ToddWard,andWei-
tive information-seeking with attribution. arXiv JingZhu.2002. Bleu: amethodforautomaticeval-
preprint arXiv:2307.16883. uationofmachinetranslation. InProceedings of the
40th annual meeting of the Association for Compu-
Min Young Lee. 2023. Building multimodal ai chat- tational Linguistics, pages 311–318.
bots. arXiv preprint arXiv:2305.03512.
Guilherme Penedo, Quentin Malartic, Daniel Hesslow,
Nyoungwoo Lee, Suwon Shin, Jaegul Choo, Ho-Jin Ruxandra Cojocaru, Alessandro Cappelli, Hamza
Choi, and Sung-Hyun Myaeng. 2021. Construct- Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
ing multi-modal dialogue dataset by replacing text and Julien Launay. 2023. The refinedweb dataset
with semantically relevant images. arXiv preprint for falcon llm: outperforming curated corpora with
arXiv:2107.08685. web data, and web data only. arXiv preprint
arXiv:2306.01116.
Young-JunLee,ByungsooKo,Han-GyuKim,andHo-
Jin Choi. 2022. Dialogcc: Large-scale multi-modal
Dustin Podell, Zion English, Kyle Lacey, Andreas
dialogue dataset. arXiv preprint arXiv:2212.04119.
Blattmann, Tim Dockhorn, Jonas Mu¨ller, Joe
Penna, and Robin Rombach. 2023. Sdxl: improv-
Young-Jun Lee, Byungsoo Ko, Han-Gyu Kim, Jongh-
inglatentdiffusionmodelsforhigh-resolutionimage
wan Hyeon, and Ho-Jin Choi. 2023. Dialogcc: An
synthesis. arXiv preprint arXiv:2307.01952.
automated pipeline for creating high-quality multi-
modal dialogue datasets. In NeurIPS 2023 Work-
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
shop on Instruction Tuning and Instruction Follow-
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
ing.
Sastry, Amanda Askell, Pamela Mishkin, Jack
Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Clark,etal.2021. Learningtransferablevisualmod-
Cao,andShuziNiu.2017. Dailydialog: Amanually els from natural language supervision. In Interna-
labelledmulti-turndialoguedataset. arXivpreprint tional conference on machine learning, pages 8748–
arXiv:1710.03957. 8763. PMLR.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Hays, Pietro Perona, Deva Ramanan, Piotr Dolla´r, DarioAmodei,IlyaSutskever,etal.2019. Language
and C Lawrence Zitnick. 2014. Microsoft coco: models are unsupervised multitask learners. Ope-
Common objects in context. In Computer Vision– nAI blog, 1(8):9.
ECCV 2014: 13th European Conference, Zurich,
Switzerland, September 6-12, 2014, Proceedings, RobinRombach,AndreasBlattmann,DominikLorenz,
Part V 13, pages 740–755. Springer. Patrick Esser, and Bjo¨rn Ommer. 2022. High-
resolution image synthesis with latent diffusion
Haotian Liu, Chunyuan Li, Qingyang Wu, and
models. In Proceedings of the IEEE/CVF confer-
Yong Jae Lee. 2023a. Visual instruction tuning.
ence on computer vision and pattern recognition,
arXiv preprint arXiv:2304.08485.
pages 10684–10695.
Haotian Liu, Chunyuan Li, Qingyang Wu, and
Christoph Schuhmann. 2023. improved-
Yong Jae Lee. 2023b. Visual instruction tuning.
aesthetic-predictor. https:https:
Siyang Liu, Sahand Sabour, Yinhe Zheng, Pei Ke, //github.com/christophschuhmann/
Xiaoyan Zhu, and Minlie Huang. 2022. Rethink- improved-aesthetic-predictor. GitHub reposi-
ing and refining the distinct metric. arXiv preprint tory.
arXiv:2202.13587.
Christoph Schuhmann, Romain Beaumont, Richard
Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Vencu, Cade Gordon, Ross Wightman, Mehdi
YuanyuanYang,JiamingTian,HaoHe,AntongLi, Cherti, Theo Coombes, Aarush Katta, Clayton
Mengshen He, Zhengliang Liu, et al. 2023c. Sum- Mullis, Mitchell Wortsman, et al. 2022. Laion-5b:
mary of chatgpt/gpt-4 research and perspective to- Anopenlarge-scaledatasetfortrainingnextgener-
wards the future of large language models. arXiv ationimage-textmodels. Advances in Neural Infor-
preprint arXiv:2304.01852. mation Processing Systems, 35:25278–25294.Supplementary
Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen
Chang, and Dilip Krishnan. 2023. Stablerep:
Synthetic images from text-to-image models make
strongvisualrepresentationlearners. arXivpreprint
arXiv:2306.00984.
A COT & FS Prompts
Hugo Touvron, Thibaut Lavril, Gautier Izacard,
Xavier Martinet, Marie-Anne Lachaux, Timoth´ee
In the paper, we referenced the Few Shot
Lacroix,BaptisteRozi`ere,NamanGoyal,EricHam-
bro,FaisalAzhar,etal.2023. Llama: Openandef- and Chain of Thought prompts, which can
ficient foundation language models. arXiv preprint be found in Figures 5 and 6, respectively.
arXiv:2302.13971.
When generating multi-modal versions from
Veniamin Veselovsky, Manoel Horta Ribeiro, and each text-only input dataset, it became evi-
Robert West. 2023. Artificial artificial artificial in-
dent that distinct prompting is necessary for
telligence: Crowdworkerswidelyuselargelanguage
models for text production tasks. arXiv preprint the chain of thoughts due to variations in the
arXiv:2306.07899. format of the input text.
Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo
B PhotoChat results
Yu, Xin Ma, Xinyuan Chen, Yaohui Wang, Ping
Luo,ZiweiLiu,etal.2023. Internvid: Alarge-scale
video-text dataset for multimodal understanding As mentioned in section 4.1, here we have in-
and generation. arXiv preprint arXiv:2307.06942. cluded the results of different LLM on Pho-
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten toChat dataset. Table 6 shows the results.
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, Overall, GPT 3.5 shows better performance
et al. 2022. Chain-of-thought prompting elicits rea-
compared with other LLM models. As it can
soninginlargelanguagemodels. AdvancesinNeural
Information Processing Systems, 35:24824–24837. be seen, the precision is significantly lower
comparedwiththeresultsreportedonMMDi-
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
alogue dataset (Table 1) and that is because
Chaumond,ClementDelangue,AnthonyMoi,Pier-
ric Cistac, Tim Rault, R´emi Louf, Morgan Fun- this dataset is limited to only one image per
towicz, et al. 2019. Huggingface’s transformers:
dialogue while our pipeline does not have such
State-of-the-art natural language processing. arXiv
preprint arXiv:1910.03771. restriction.
Nahathai Wongpakaran, Tinakon Wongpakaran, C Image Generator Ablation Study
Danny Wedding, and Kilem L Gwet. 2013. A
comparison of cohen’s kappa and gwet’s ac1 when
Table 7 shows the performance of different
calculating inter-rater reliability coefficients: a
study conducted with personality disorder samples. diffusion models (Podell et al., 2023; Rom-
BMC medical research methodology, 13:1–7. bach et al., 2022). The results are taken from
MMDialog dataset and the quality assurance
Xiaoxue Zang, Lijuan Liu, Maria Wang, Yang Song,
Hao Zhang, and Jindong Chen. 2021. Photochat: module is disabled to report the results with-
A human-human dialogue dataset with photo shar-
out filtering unwanted ones. It is clear that
ing behavior for joint image-text modeling. arXiv
preprint arXiv:2108.01453. SDXL 1.0 and SDXL 0.9 has very similar per-
formance and higher aethetic score compared
Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur
with Stable Diffusion 2.0. All models have
Szlam, Douwe Kiela, and Jason Weston. 2018. Per-
sonalizingdialogueagents: Ihaveadog,doyouhave similarCLIPscorewhichispredictableasthey
pets too? arXiv preprint arXiv:1801.07243. are given the same prompt for image genera-
YizheZhang,SiqiSun,MichelGalley,Yen-ChunChen, tion.
Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing
Liu, and Bill Dolan. 2019. Dialogpt: Large-scale D Human evaluation
generative pre-training for conversational response
generation. arXiv preprint arXiv:1911.00536.
Tocollectanswersfromannotators,wecreated
a website with a schema shown in Figure 7.
For each question, annotators were given two
screenshots of the same dialogue, one gener-
ated by MAGID and the other from a source
dataset (PhotoChat, MMDialog, or MMDD).
At the start of the annotation session, annota-
torswereinstructedtoignoretheconversationTable 6: Different LLM model testing on PhotoChat (ground-truth). Quality Assurance module is
enabled. The system prompt is chain of thoughts.
Model Accuracy Precision Recall F1 score CLIP score MM-Relevance #images Aesthetic
GPT 3.5 86.11% 28.62% 25.91% 0.27 0.25 313.64 87 0.57
Falcon-40b-Ins. 88.10% 28.04% 11.83% 0.17 0.24 303.68 403 0.58
Koala 13b 89.61% 30.43% 2.94% 0.05 0.24 283.44 92 0.61
Llama 13b 87.32% 20.79% 9.54% 0.13 0.23 244.36 433 0.59
OpenLLaMA 88.75% 27.31% 8.03% 0.12 0.23 270.36 696 0.59
Vicuna 13b 88.40% 25.48% 8.35% 0.13 0.24 244.97 602 0.55
PhotoChat N/A N/A N/A N/A 0.213 N/A 961 0.49
Few-shot example prompt
- query: >
Utterance: 0: So yeah, it was a mostly dismal year. But what’s the best news you’ve
read/saw/heard in 2016? (Anything from the personal to world affairs.)
Utterance: 1: grew 14 pumpkins on the formidable strength of my chickens We are
all proud! Here’s one
Utterance: 2: Very impressive!
answer: >
<result> Utterance: 1: 14 pumpkins</result>
- query: >
Utterance: 0: Working from home with a tie today! Plenty of Zoom in my life today!
Utterance: 1: I keep a polo handy that I throw on and off for zoom calls.
Way to be extra fancy
answer: >
<result>Utterance: 0: Working from home with a tie</result>
Figure5: Thefew-shotexamplepromptnotonlyprovidestheformatforbothinputandexpectedoutput
along with a problem description but also includes multiple exemplars to elucidate the desired response
from the LLM. Here only exemplars are included.
Chain of Thoughts prompt
- query: >
Utterance: 0: So yeah, it was a mostly dismal year. But what’s the best news you’ve
read/saw/heard in 2016? (Anything from the personal to world affairs.)
Utterance: 1: grew 14 pumpkins on the formidable strength of my chickens We are
all proud! Here’s one
Utterance: 2: Very impressive!
answer: >
<reason> Utterance 0 is just a descrption of last year without any information that
can be translated with image. We add photographic style as it is a personal sharing.
Utterance 1 on the other hand is talking about growing 14 pumpkins. This can be
represented with image.</reason>
<result> Utterance: 1: 14 pumpkins</result>
- query: >
Utterance: 0: My attire for the SA Hip Hop Awards
Utterance: 1: Are you a supporter of Kaizer Chiefs?...lol. Gorgeous!!
answer: >
<reason>In Utterance 0 contains the sentence "My outfit for
the SA hip hop awards" which shows
the person is willing to share her outfit</reason>
<result>Utterance: 0: My outfit for the SA hip hop awards </result>
Figure 6: The chain of thoughts prompt, building upon the system prompt provided in the few-shot
example prompt, also incorporates the detailed reasoning on utterance selection.text and focus only on the images and image- Table 8: Downstream training. The model used is
DialoGPT + ViT. BLUE score is in percentage.
text matching. Fifteen annotators completed
the task, each making 20 comparisons. BLEU- BLEU- distinct- distinct-
Dataset PPL
1 2 1 2
E Downstream Training MMDialogue 73.09 8.3 3.9 0.94 0.965
MAGID 70.99 7.9 3.9 0.94 0.971
MMDD 78.86 7.5 3.0 0.93 0.963
Here, we study how much MAGID can impact
No image 78.88 7.9 3.0 0.92 0.952
training a multi-modal model when changing
the original image with synthetic one gener-
ated by MAGID. In addition, we also com-
lowest which shows the model is more confi-
pareitwithbenchmarkcaseswhennoimageis
dent in making the prediction. In addition,
present in the training and with MMDD (Lee
the distinct score (Liu et al., 2022) which
et al., 2021) approach to include image in the
shows the diversity of response is highest with
dialogue. In this regard, we used the same ar-
MAGID which can be attributed to higher
chitecturesuggestedin(Lee,2023)whichisvi-
image-text match provided with MAGID im-
sionTextDualEncoderfromHuggingface(Wolf
ages. It is important to note that since MM-
etal.,2019)whichprojectstheencodingofim-
Dialog dataset is a real dataset, the quality
agewiththetheembeddingoftexttoashared
of images shared does not necessarily matches
commonspace. Forencodingofimageweused
the text and this can make the model less con-
ViT (Dosovitskiy et al., 2020), and for pro-
fident and results in higher perplexity. On the
cessing the text we used pretrained DialoGPT
other hand, the images generated by MAGID
(Zhang et al., 2019). While the input is multi-
is more controlled.
modal, the output is text only. In this task,
Forthisexperimentweused4NVIDIARTX
we omit the last text utterance and the model
GPUeachwith24GiBmemoryandthetrain-
should predict it given the prior image and
ing took for a full day.
text.
We fine-tuned the model on MMDialog
F Experiment Computational Cost
datasetandtheresultsarereportedinTable8.
For this experiment, we used the learning rate
For running MAGID pipeline, it can be run
of 0.00005 with Adam Optimizer. In Table 8,
withoneGPUwithNVIDIARTXwith24GiB
we show the results on the test set when train-
memory.
ing set images is coming from MMDialogue,
MAGID, MMDD and the case where the im-
ages are omitted. For MMDD, we used the G Discussion on Inter-rater
same code they used to inject image into text- Reliability Measure Choice
only dialogue to make the comparison possi-
ble. For this expeiment, the training set con- In Section 4.2, we employed Gwet’s AC1 for
sistsof5156dialoguesandthetestsetconsists evaluating the consistency among reviewers,
of 633 dialogues sampled from MMDialogue opting not to use Cohen’s Kappa due to
dataset. its susceptibility to outliers and potential for
As it can be seen, when we use the source showing inconsistent results despite high aver-
image as training set (MMDialog), we achieve age scores across all participants. As detailed
highest BLEU score (Papineni et al., 2002). in the study by Wongpakaran et al. (2013),
The perplexity of the model using MAGID is Gwet’s AC1 is recognized for its greater con-
sistency in inter-rater reliability assessments
when compared to Cohen’s Kappa, alongside
Table 7: Testing different Stable diffusion models
its enhanced resilience to outliers, providing a
with MAGID pipeline
more reliable measure for our analysis (Wong-
Aesthetic CLIP
Model pakaran et al., 2013). This approach ensures
Score Score
a more stable and accurate assessment of re-
SDXL 1.0 0.56 0.26
SDXL 0.9 0.57 0.26 viewer consistency, mitigating the impact of
Stable Diffusion 2.0 0.53 0.26 anomalies on the reliability scores.H Further examples of MAGID
Figures 8, 9, and 10 provide more examples
on comparing MAGID with MMDialog, Pho-
toChat, and MMD.
I Experiment Setting
For determining the threshold for image-text
matching and aesthetic score, we employed
cross-validation on the validation set. In this
regard, the threshold for CLIP score was set
for 0.21 and the threshold for the aesthetic
score was set for 0.51. Based on our observa-
tions, we established a protocol where a gen-
erated image could fail up to two times be-
fore being discarded and triggering the feed-
back loop. This approach ensured a balance
between generating high-quality images and
maintaining efficient processing. In all our ex-
periments, we used SDXL 1.0 model for image
generation.
J Limitations
In Figures 11, 12, and 13, we showcase the
most common scenarios were MAGID can fail
togeneratetheimagewhichproperlysupports
theprecedingutterance. Specifically, figure11
shows a common example, where the gener-
ated image usually fails to put the proper text
signinthegeneratedimage. InFigures12and
13showcasetheexampleswherethegenerated
image does not follow the correct description
in terms of number object should exist in the
image. We believe using more advanced dif-
fusion models like DALL-E 3 should mitigate
this problem.Figure 7: Schema of the website used to perform human evaluation.Figure 8: MAGID (left) versus MMDialog (right)
Figure 9: MAGID (left) versus PhotoChat (right)Figure 10: MAGID (left) versus MMDD (right)Figure 13: Generated image by MAGID fails to
properly shows 6 fishes instead of 5
Figure 11: Generated image by MAGID fails to
properly show the sign HA
Figure 12: Generated image by MAGID fails to
properly shows 4 cats instead of 3