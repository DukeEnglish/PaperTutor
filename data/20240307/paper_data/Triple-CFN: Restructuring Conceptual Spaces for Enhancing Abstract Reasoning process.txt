1
Triple-CFN: Restructuring Conceptual Spaces for
Enhancing Abstract Reasoning process
Ruizhuo Song, Beiming Yuan
Abstract—Abstract reasoning problems pose significant chal- deep learning has demonstrated remarkable efficacy, with
lenges to artificial intelligence algorithms, demanding cognitive recognition accuracies surpassing preceding technologies.
capabilities beyond those required for perception tasks. This
However, the applications of deep learning extend beyond
studyintroducestheTriple-CFNapproachtotackletheBongard-
these. For instance, utilizing the outcomes from upper-level
Logo problem, achieving notable reasoning accuracy by im-
plicitly reorganizing the concept space of conflicting instances. training as initialization parameters for lower-level training
Additionally, the Triple-CFN paradigm proves effective for the processes enhances the efficiency of deep model training.
RPMproblemwithnecessarymodifications,yieldingcompetitive Meanwhile, adopting a layer-wise initialization approach and
results. To further enhance performance on the RPM issue, we employing unsupervised learning for training is a pivotal
developtheMetaTriple-CFNnetwork,whichexplicitlystructures
strategy in deep learning.
theproblemspacewhilemaintaininginterpretabilityonprogres-
sive patterns. The success of Meta Triple-CFN is attributed to Collectively, the progression of deep learning in graphical
its paradigm of modeling the conceptual space, equivalent to abstract reasoning is an ongoing research sphere that offers
normalizing reasoning information. Based on this ideology, we substantialsupporttothedevelopmentofartificialintelligence.
introducetheRe-spacelayer,enhancingtheperformanceofboth
Nevertheless, despite the extensive and profound applications
MetaTriple-CFNandTriple-CFN.Thispaperaimstocontribute
of deep learning, numerous unresolved issues and challenges
toadvancementsinmachineintelligencebyexploringinnovative
network designs for addressing abstract reasoning problems, demand further investigation and exploration.
paving the way for further breakthroughs in this domain. Notably,followingtheremarkableaccomplishmentsofdeep
learning in intelligent visual tasks, machine intelligence is
Index Terms—Abstract reasoning, RPM problem, Bongard-
logo problem. poisedtoreachevengreaterheights.Theacademiccommunity
has presented a challenge to deep learning’s abstract reason-
ing capabilities using graphical reasoning problems. Initially,
I. INTRODUCTION
graphicalreasoningentailscomprehendingandanalyzingboth
DEEP neural networks have achieved remarkable success
globalandlocalcharacteristicsofgraphics,posingasignificant
in various domains, including computer vision [1]–[3],
challenge for deep learning models. Typically, deep learning
natural language processing [4]–[6], generative models [7]–
models extract features by learning from extensive datasets.
[9], visual question answering [10], [11], and abstract reason-
However,ingraphicalreasoningproblems,thecomplexityand
ing [12]–[14]. The advancement of deep learning in the realm
variability of graphics make it arduous for models to learn
of graphical abstract reasoning is a particularly intriguing and
effective feature representations.
complex research area.
Secondly, graphical reasoning problems require models to
Initially, deep learning was introduced into machine learn-
possessreasoningandinductioncapabilities.Thisnecessitates
ing, bringing it closer to its original goal of artificial intel-
models to comprehend graphic structures, relationships, and
ligence. It is regarded as learning the inherent patterns and
rules and perform reasoning and induction based on this
hierarchical representations within sample data, greatly aiding
information. However, existing deep learning models often
in the interpretation of data types such as text, images, and
exhibit subpar performance when tackling such problems due
sound. The ultimate objective is to endow machines with
to their limited reasoning and induction abilities.
human-like analytical learning capabilities, enabling them to
In addition, graphical reasoning problems mandate models
recognize and interpret text, images, and sound.
tohavegeneralizationcapabilities.Thismeansmodelsmustbe
In the domain of graphical abstract reasoning, the sig-
adeptathandlinggraphicsofvariousshapes,sizes,andcolors
nificance of deep learning lies primarily in its ability to
while delivering accurate reasoning outcomes. Nevertheless,
tackle complex pattern recognition challenges. Through deep
due to the limited generalization capabilities of deep learning
learning, machines can mimic human activities like percep-
models, they often encounter overfitting or underfitting issues
tion, audition, and cognition, leading to significant strides in
when dealing with such problems.
artificial intelligence-related technologies.
Lastly, datasets for graphical reasoning problems are typi-
Moreover, deep learning has yielded numerous achieve-
cally small-scale, posing challenges for the training of deep
ments in areas like search technologies, data mining, machine
learning models. These models require vast amounts of data
learning, machine translation, natural language processing,
for training to achieve optimal performance. However, in the
multimedia learning, speech recognition, recommendations,
context of graphical reasoning problems, the limited dataset
andpersonalization.Notably,inspeechandimagerecognition,
size makes it challenging for models to acquire sufficient in-
0000–0000/00$00.0f0o©rm2a0t2i1onIEfEoErproblem-solving.Furthermore,datasetsforthese
4202
raM
6
]VC.sc[
2v09130.3042:viXra2
problems are often artificially designed, potentially leading designofvisualattributevalues.Attributesnotboundbythese
to discrepancies between data distributions and real-world rulesareassignedvaluesatrandom.Subsequently,imagesare
scenarios, further complicating model training. rendered based on the generated attribute information.
Thus, addressing the challenges posed by graphical reason- TheRVENdatabaseisfurtherdiversifiedintomultiplesub-
ing problems to deep learning constitutes a pivotal research databases, namely: single-rule groups—center single (center),
direction. This necessitates the design of more effective deep distribute four (G2×2), distribute nine (G3×3)—and dual-rule
learning models, enhancements in model training methodolo- groups: in center single out center single (O-IC), up center
gies,andoptimizationsindatasetqualityamongotheraspects. single down center single (U-D), left center single up center
For instance, Ravens Progressive Matrices (RPM) prob- single(I-L),andindistributefouroutcentersingle(O-IG).In
lems [12] and Bongard problems [13], [14] present learning problems with a singular rule, the progressive transformation
demands ranging from perception to reasoning. Addressing of an entity’s attributes within the image adheres to one set
these demands necessitates advancements in deep learning of rules, while in those with dual rules, two independent rule
capabilities to handle abstract reasoning tasks associated with sets govern this transformation.
graphical representations effectively.
B. PGM Database
A. RAVEN Database as an RPM Problem: Construction and
The design logic of PGM [17] and RAVEN problems is
Characteristics
remarkably similar, with both types of problems represented
The RAVEN database [16] presents a unique challenge in
bya problemstemcomposed of8 imagesandan answerpool
therealmofRAVENprogressivematrix(RPM)problems,with
formed by another 8 images. Notably, in PGM problems, the
each question typically comprising 16 images enriched with
conceptof”rule”notonlydescribestheprogressivepatternof
geometric entities. Half of these images, specifically 8, form
”visual attributes” in the row-wise direction within the matrix
the problem stem while the remaining 8 constitute the answer
but also constrains the progressive pattern in the column-wise
pool. Subjects are tasked with selecting appropriate images
direction. An example of a PGM problem is illustrated in the
from the answer pool to complete a 3×3 matrix, following
provided figure 2.
aprogressive pattern of geometric images along the rows to
convey specific abstract concepts.
problem statement
As illustrated in figure 1, the construction of a RAVEN
Shape:
problem speaks to its generality and sophistication. Within Rule attr="position" name="XOR"
Rule attr="type" name="consistent_union"
these problems, certain human-defined concepts within the
geometric images, such as ”shape” or ”color”, are deliber-
ately abstracted into bounded, countable, and precise ”visual
attributes”.Thenotionof”rule”isthenemployedtodelineate
？
the progressive transformation of a finite set of these visual line:
Rule attr="color" name="XOR"
attribute values. However, it’s worth noting that some visual
attributes remain freedom of the rule, potentially posing as answer pool
distractions for deep model reasoning.
problem statement
Out：
Rule attr="Number/Position" name="Constant"
Rule attr="Type" name="Progression"
Rule attr="Size" name="Constant"
Rule attr="Color" name="Constant"
Fig.2. PGMcase
In： Consequently, the difficulty of RPM problems lies not only
Rule attr="Position" name="Arithmetic" in the exploration of visual attributes at different levels but
？ Rule attr="Type" name="Constant"
Rule attr="Size" name="Arithmetic" also in the induction and learning of the progressive patterns
Rule attr="Color" name="Progression"
of ”visual attributes.”
answer pool
C. Bongard-logo Database
Bongard problems [13] exhibit significant differences from
RPM problems, with Bongard problems being a type of small
sample learning problem. Typically composed of multiple
images, these problems divide the images into two groups: a
primary group and a secondary group. All images within the
Fig.1. RAVENcase
primarygroupexpressabstractconceptsconstrainedbycertain
To curate a comprehensive RAVEN problem, samples of rules, while the images in the secondary group reject these
rules are drawn from a predefined rule pool, guiding the rules to varying degrees. Bongard problems challenge deep3
learning algorithms to correctly categorize ungrouped images selectingacertaingraphicfromgivencandidateanswersasthe
into the appropriate small groups. Bongard-logo, an instantia- finalsolution.Thisoutputformatprovidesrichinformationfor
tionofBongardproblemswithintherealmofabstractreason- subsequentdecision-makingandanalysis.However,traditional
ing, poses considerable reasoning difficulties. Each Bongard- discriminative models often face numerous challenges when
logo[14]problemconsistsof14images,with6imagesinthe dealingwithcompleximagereasoningtasks,suchascapturing
primary group, 6 in the secondary group, and the remaining 2 subtle differences and digging underlying rules. To address
servingasoptionsforgrouping.Theimagescontainnumerous these issues, researchers have proposed a series of innovative
geometric shapes, and their arrangements serve as the basis models.
for grouping. Figure 3 illustrates an example Bongard-logo Among them, the CoPINet [20] model stands out with its
problem. In Figure 3, each Bongard problem is composed of innovative introduction of a contrast module. The primary
two sets of images: the primary group A and the secondary function of this contrast module is to learn the differences
group B. The primary group A contains 6 images, with the between input graphics, enabling the model, through con-
geometric entities within each image following a specific set trastive learning, to more sensitively capture subtle varia-
of rules, while the secondary group B includes 6 images that tions in graphics and thus more accurately determine their
reject the rules in group A. The task is to determine whether attributesduringthereasoningprocess.Additionally,CoPINet
the images in the test set satisfy the rules expressed by group incorporates a reasoning module tasked with summarizing
A. The difficulty level varies depending on the problem’s potentialfundamentalrules.Bycombiningcontrastivelearning
structure. with reasoning learning, the CoPINet model has achieved
remarkable results in image reasoning problems.
Distinct from CoPINet, the LEN+teacher model [21] relies
primary group auxiliary group
on a student-teacher architecture to determine the training
sequence and make predictions. This architecture facilitates
more effective knowledge transfer and model optimization
by introducing a teacher model to guide the training of the
student model. Specifically, the teacher model leverages its
own experience to direct the learning process of the student
model, helping it converge more rapidly to better solutions.
Through this approach, the LEN+teacher model has yielded
impressive outcomes in image reasoning problems.
The DCNet model [22] is notable for its use of a dual-
contrastmoduletoaccomplishtwotasks:comparingrulerows
and columns and exploring differences among candidate an-
swers. This dual-contrast mechanism enables DCNet to more
comprehensively consider various factors in image reasoning
problems, thereby enhancing accuracy and efficiency during
the reasoning process.
The NCD model [23] operates in an unsupervised envi-
ronment and employs methods of introducing pseudo-targets
and decentralization. These techniques not only effectively
Fig.3. Bongardcase address certain challenges in unsupervised learning but also
enhance the model’s generalization capabilities. Specifically,
Bongard-logo problems are categorized into three types NCD augments the model’s exploration capabilities by intro-
based on conceptual categories: 1) Free Form problems (ff), ducing pseudo-targets and leverages decentralization methods
where each shape is composed of randomly sampled action to reduce the model’s reliance on specific data, thereby bol-
strokes, with each image potentially containing one or two stering robustness and adaptability.
shapes. 2) Basic Shape problems (ba), where the concept cor- In the SCL model [24], multiple monitoring mechanisms
respondstoidentifyingoneshapecategoryoracombinationof are applied to sub-graphs within reasoning problems, with
two shape categories represented in the given shape patterns. the expectation that each branch will focus on specific vi-
3) High-level Abstraction problems (hd), designed to test a sual attributes or rules. This multiple monitoring mechanism
model’sabilitytodiscoverandreasonaboutabstractconcepts, enhances the model’s flexibility and efficiency when tackling
such as concavity and convexity, symmetry, among others. complex image reasoning tasks. Concurrently, SCL leverages
relationships between sub-graphs to further strengthen the
model’s reasoning capabilities, leading to significant advance-
II. RELATEDWORK
ments in solving image reasoning problems.
A. RPM solver
The SAVIR-T model [25] extracts information from within
In image reasoning problems, discriminative models typ- sub-graphs of reasoning problems and relationships between
ically produce outputs in the form of a multi-dimensional sub-graphs from multiple perspectives, aiming to elevate
vector, with each dimension representing the probability of reasoning effectiveness. This approach enables the efficient4
captureofdiverseinformationwithinandbetweensub-graphs, Firstly, language-based feature model methods [13], exem-
providing a more comprehensive and accurate foundation for plified by the work of Depweg and others, aim to decipher
subsequent reasoning processes. Furthermore, SAVIR-T uti- visual characteristics within image information through a
lizes multi-perspective information fusion methods to further formalized linguistic system. They have devised a formal
augment the model’s reasoning capabilities, ensuring greater language capable of symbolizing visual elements within im-
efficiency and accuracy when dealing with intricate image ages, utilizing logical operators to extract these visual fea-
reasoning problems. tures and transform them into a symbolic visual vocabulary.
RS-Tran[30]adoptsamulti-viewpointandmulti-evaluation Subsequently, they employ symbolic language and Bayesian
reasoning approach, which effectively solves the RPM prob- reasoning to tackle BP problems. However, this approach is
lemandachievesimpressivepredictionaccuracy.Furthermore, severely constrained by its symbolic representation, making it
by utilizing the accompanying Meta data from RPM tasks for difficult to handle BP issues involving intricate abstract con-
thepre-trainingofitsencoder,RS-Tranhasonceagainmadea cepts. Specifically, the method can only manage basic shape-
breakthrough in terms of performance. This pre-training with basedBPproblemsandisunabletorepresentorprocessmore
Meta data enhances the model’s ability to capture underlying sophisticated abstract concept types. Additionally, whenever
patterns and relationships within the RPM problems, enabling confrontedwithanovelBPproblem,theneedtoreconstructan
it to make more accurate predictions and reason more effec- appropriate symbolic system adds complexity and limitation
tively. to the method. After filtering out BP problems that cannot be
CRAB [31] has established a “greenhouse” tailored to its expressed using this visual language, only 39 of the original
own methodology, which takes the form of a brand-new 100 BP problems remain, with 35 of them being resolvable.
RAVEN database. This greenhouse, while sacrificing the core Secondly, convolutional neural network model-based meth-
challenges inherent in RAVEN—namely, the diversity and ods [32], as exemplified by Kharagorgiev and Yun, favor
uncertainty of answers—has nevertheless enabled CRAB to the use of deep learning techniques for automated feature
achieve remarkable outcomes. Within the confines of this extraction from images. Kharagorgiev constructed an image
meticulouslycrafted“greenhouse”,CRAB’sBayesianmethod- dataset containing simple shapes and utilized a pre-training
ology has demonstrated remarkable proficiency and efficacy. process to develop a feature extractor. This feature extractor
The controlled setting, tailored to optimize the probabilistic is then employed to extract image features from Bongard
framework, has allowed for a profound exploration and ex- problems, facilitating image classification to determine if test
ploitation of the inherent strengths of the Bayesian paradigm, images conform to specified rules. Yun adopted a similar
thereby facilitating significant advancements in the field. The approach but placed greater emphasis on utilizing images
scientific community eagerly awaits the implications of this containing visual characteristics from BP problems for pre-
innovative approach for future research. training to extract BP image features, subsequently linking
Additionally, research indicates that relatively decoupled additional classifiers for discrimination. While these methods
perceptual visual features can contribute to improved rea- canautomaticallyextractandlearnfeaturesfromimages,their
soning performance [26]. These perceptual visual features performance is heavily reliant on the quality and quantity of
not only capture fundamental elements and attributes within training data.
imagesbutalsoeffectivelyexpressrelationshipsandstructures Thirdly, among the strategies employed is the generation
among them. By introducing such perceptual visual features of datasets [14]. In 2020, Nie et al. applied basic CNNs,
into image reasoning problems, significant enhancements can relational networks like WReN-Bongard, and Meta-learning
be achieved in both the model’s reasoning performance and techniques in the Bongrad-Logo database. They endeavored
efficiency [26]. to enhance model generalization by generating substantial
Symbolic approaches have brought about higher reasoning volumesofsyntheticdata.However,theirexperimentalresults
precisionandenhancedmodelinterpretability.[27]–[29]These indicate that the models did not achieve the desired level of
methods bolster the reasoning capabilities and interpretability performance,potentiallyduetosignificantdisparitiesbetween
ofmodelsbyincorporatingsymbolicrepresentationsandoper- thegenerateddataandthedistributionofreal-worldproblems.
ations. Specifically, symbolic approaches endow models with Notably, the PMoC model [15] has emerged as a notable
increased flexibility and efficiency when addressing intricate approach, particularly in addressing the challenges posed by
image reasoning tasks while also enhancing model trans- the Bongard-Logo problem. This tailored probability model
parency and interpretability, facilitating a deeper understand- achieveshighreasoningaccuracybyconstructingindependent
ing and analysis of the model’s decision-making processes. probability models, demonstrating its effectiveness in discern-
ing deeper patterns and inductive reasoning beyond explicit
image features. The strength of PMoC lies in its ability to
B. Bongard-logo solver
capture the underlying probabilistic relationships within the
In recent years, researchers have been exploring various problem space, enabling more accurate reasoning and pattern
potential solutions to address the highly challenging Bongard recognition.Byleveragingthepowerofprobabilitymodeling,
problems, leading to the emergence of three dominant strate- PMoC paves the way for more robust and accurate solutions
gies:language-basedfeaturemodelapproaches,methodsrely- in abstract reasoning tasks.
ing on convolutional neural network models, and techniques Inconclusion,itisevidentthateachapproachoffersdistinct
involving generated datasets. advantages and limitations. Language-based feature model5
methods provide a fresh perspective for comprehending and between various features within the image data, fostering
deciphering BP problems but have limited capabilities in insights that can inform downstream tasks in image analysis
handling complex abstract concepts. Methods based on con- and processing. We calculate the covariance matrix of a mul-
volutional neural network models can automatically learn and tivariate distribution using Formula (1), and then compute the
extractfeaturesfromimagesbutareconstrainedbythequality correlation loss of the multivariate distribution using Formula
and quantity of training data. While techniques involving (2).
generated datasets hold potential for enhancing model gen-
eralization, their effectiveness is contingent on the alignment N
1 (cid:88)
betweengenerateddataandreal-worldproblemscenarios.This M (x)= (x −x¯)(x −x¯)⊤ (1)
σ N −1 i i
underscorestheneedforamorecomprehensiveandintegrated i=1
strategy in addressing Bongard problems. L(x)=1(cid:88)(cid:0)
M
(x)2·(1−I)(cid:1)
(2)
d σ
C. Transformer and Vision Transformer Where I denotes the identity matrix, and M (x) ∈ Rd×d. d
σ
The Transformer model [4] diverges from conventional represents the dimensionality of the vector x i, and n refers
RNNandCNNdesigns,utilizingafullyattentionalmechanism to the number of samples involved in the computation, given
forcapturinglong-rangeinputsequencedependencies.Itscore that the covariance matrix is calculated based on a batch of
comprises self-attention and feed-forward neural networks, samples.
integrated via residual connections and layer normalization
to form its encoders and decoders. The self-attention mecha- E. The Expectation-Maximization (EM) algorithm
nism, analogous to social network influence diffusion, assigns The Expectation-Maximization (EM) algorithm [35] repre-
weightsbasedoninputsequencepositionsimilarities,fostering sentsapowerfuliterativemethodwidelyemployedinstatistics
flexible non-sequential processing. Additionally, the Trans- for finding maximum likelihood estimates of parameters in
formerincorporatesencoder-decoderattention,akintotransla- probabilisticmodels,especiallywhenthedatacontainmissing
tion dictionary consultation, where the decoder references the valuesorareobservedinanincompletemanner.Byalternating
encoder’s output to enhance output sequence accuracy. between an expectation “E” step and a maximization “M”
The Vision Transformer (ViT) [33] is an innovative ap- step,thealgorithmoptimizesthelikelihoodfunction,gradually
proach to computer vision tasks that eschews traditional con- refining parameter estimates until convergence. Its versatility
volutional neural networks in favor of a pure transformer- and robustness have made the EM algorithm a cornerstone
based architecture. By dividing images into fixed-size patches technique in diverse fields such as machine learning, bioin-
and treating them as sequences of tokens, ViT leverages the formatics, and image processing, where complex models and
power of self-attention mechanisms to capture long-range datastructuresoftendemandsophisticatedestimationmethod-
dependencies within the image effectively. This shift towards ologies.
transformers enables ViT to achieve state-of-the-art perfor- Specifically, we employ a function, denoted as P(X,Z|θ),
mance on various vision benchmarks, heralding a new era in to approximate the joint distribution of the observed data
computer vision research. and their corresponding latent variables. However, both Z
and θ remain unknown entities. The process initiates with
D. Covariance matrix and correlation loss the assumption of an initial, arbitrarily assigned θ, which
is then used to compute the posterior distribution of the
The covariance matrix stands as a pivotal tool in multivari-
latent variables, denoted as P(X,Z|θ). Given this posterior
ate statistical analysis, quantifying the relationships between
distribution,weproceedtocalculatethejointdistributionofX
multiple random variables [34]. In the realms of data science
andZ,namelyP(X,Z|θ).Subsequently,θisrecalculatedina
and machine learning, the covariance matrix plays a crucial
manner that maximizes the joint distribution P(X,Z|θ). This
role, facilitating profound insights into data structures and
iterative process of alternating between the computation of θ
patterns. This matrix not only encapsulates the variances of
andP(X,Z|θ) continues until P(X,Z|θ) reaches its maxi-
individual variables but also the covariances between them,
mum value, ensuring an optimal estimation of the parameters
offeringacomprehensiveviewoftheinterdependencieswithin
and latent variables within the data.
a dataset. Its applications span from exploratory data analysis
and dimensionality reduction to portfolio optimization and
principal component analysis, underscoring its widespread
III. METHODOLOGY
significance in diverse domains of modern data analysis. In this section, four methods have been proposed for the
Covariance matrix serves as a metric to gauge the linear Bongard-Logo [13] and RPM problems [12], namely CFN,
correlation between any two distributions within a set [34]. Triple-CFN, Meta Triple-CFN, and the Re-space layer. Each
By treating each dimension of an image representation as an of them incorporates new loss function terms or network
individualdistributionandacollectionofsuchrepresentations structures compared to its predecessor, aiming for progressive
as a sample from a group of distributions, one can leverage improvement on the Bongard-Logo and RPM problems.
a batch of samples to assess the linear correlation among The Bongard-Logo problem and the Raven’s Progressive
every dimension of the image representation. This approach Matrices (RPM) problem are distinct yet equally challenging
enables a nuanced understanding of the interdependencies tests of abstract reasoning. Both tasks require participants to6
identify and interpret underlying principles or concepts that group,whereasz encompassesallmsamples’representa-
negm
are not immediately apparent from the surface-level features tion within the auxiliary group. Here, m signifies the quantity
of the presented materials. These principles represent a more ofauxiliarygroupsampleswithinasingleBongard-logocase,
sophisticatedlevelofabstractionthanmerepixelconfiguration andthevariabletrepresentsthetemperaturecoefficient,isset
patternsorotherlow-levelvisualproperties.Instead,theyoften to 10−3. InfoNCE can impose the following constraint on the
reflecthuman-centeredpreconceptionsaboutshape,size,color, network: the cosine similarity between vector z and vector
pos
spatial relationships, the concave or convex nature of objects, z˜ shouldbehigherthanthecosinesimilaritybetweenvector
pos
and the completeness of figures. Through their respective z and vector set {z }. This constrains aligns with the
pos negm
problemformulations,theBongard-LogoandRPMtasksseek underlying logic of Bongard-logo. The feedforward process
to evaluate an individual’s ability to discern and comprehend of the network is illustrated in the figure 4. By utilizing the
these subtler, more abstract patterns and principles. InfoNCE function, we can avoid estimating the distributions
p′(z|y) and q′(z|y) while simultaneously encouraging the
i i
A. A baseline for Bongard-Logo representations of the primary group encoded by the network
f (z|x)tobemoresimilarandensuringgreatermutualexclu-
Based on higher-dimensional human concepts and prefer- θ
sion between the representations of the primary and auxiliary
ences,thecreatorsofBongard-logoproblemshavecategorized
groups.Infigure4,the“A2”representsthenumberofdistinct
the Bongard-logo dataset into four distinct problem types: FF, 7
ways to select 2 elements from a set with 7 elements.
BA, NV, CM. Consequently, we can abstract the distribution
of the primary group (positive instances) within a Bongard-
logo problem as p i(x|y) and the distribution of the auxiliary {x |j∈[1,7]} {x |j∈[8,14]}
group (negative instances) as q (x|y). Here, y denotes the ij ij
i
problem’s reasoning type, where y ∈{FF, BA, NV, CM},
while i represents the problem’s identifier, with i ∈ [1,n]
ResNet18
and n signifying the total number of problems. For the
purpose of convenient representation of data in the Bongard-
Logo problem, we denote the Bongard-Logo images as x .
ij
Specifically, {x |j ∈ [1,6]} represents images in the i-th f(z|x)
ij θ
primary group, while {x |j ∈ [8,13]} represents images in
ij
the i-th auxiliary group. Additionally, x represents the test
i7
image to be potentially assigned to the i-th primary group,
and x represents the test image to be potentially assigned
i14
to the i-th auxiliary group. {z|j∈[1,7]} {z|j∈[8,14]}
ij ij
To effectively tackle Bongard-Logo problems, we are de-
veloping a deep learning algorithm, f (z|x), primarily tasked
θ A2
7
with transforming input samples x into latent variables
ij
z . Ideally, the distributional divergence between the latent
ij
variable distribution of the primary group, p′(z|y), and that
of the auxiliary group, q′(z|y), should bei minimal. How-  (z ,~ z ,{z }M )
i infoNCE pos pos neg m1
ever, given the nature of Bongard-logo as a small-sample m
learningproblem,accuratelyestimatingandconstrainingthese Reasoning loss term
two latent variable distributions poses significant challenges.
Consequently, directly optimizing for minimal distributional Fig.4. Feedforwardprocessbaseline
divergence between them may encounter substantial difficul-
Utilizingtheaforementionedmethodologies,thisstudycon-
ties, thereby making it arduous to train a deep model that
ducted rigorous experiments on the four concept databases
exhibits exceptional performance.
of Bongard-Logo, both individually and in combination. The
In this manuscript, we leverage the InfoNCE loss function
comprehensive experimental results are presented in detail in
[36] as a reasoning loss term for the purpose of training a
Table I. In this table, the “Bongard-Logo” entry encompasses
standard ResNet18 network. The resulting model, denoted as
the consolidated findings obtained by training the model
f (z|x), possesses the proficiency to tackle either individual
θ using a combined dataset of all four concepts. Conversely,
or concurrent high-dimensional concept intricacies inherent
the “Separated Bongard-Logo” entry delineates the specific
withintheBongard-logodataset.Mathematically,theInfoNCE
outcomes derived from training each concept independently.
loss function can be formalized as follows:
The results unequivocally demonstrate that the convolu-
ℓ (z ,z˜ ,{z }M ) tional deep model, ResNet18, has struggled to distinguish
InfoNCE pos pos negm m=1
e(zpos·z˜pos)/t between the four distinct concepts. Convolutional networks
=−log (3) are designed to encode image representations primarily based
e(zpos·z˜pos)/t+(cid:80)M m=1e(zpos·znegm)/t
ontheconfigurationsofimagepixels.Thisstudyhypothesizes
In the context of this study, z and z˜ are used to denote thattheBongard-Logocasesexhibitdistinctclassificationpat-
pos pos
distinct samples’ representation that belong to the primary terns based on varying concepts. Furthermore, it is speculated7
that since no additional concept labels were introduced as theunderlyingconceptconveyedbytheimagethroughadeep
supervisory signals during training, the network solely relies learning model and adjusting the weighting of the image’s
ontheconfigurationofimagepixelstoprocessBongard-Logo feature vectors based on this reinterpretation can effectively
cases, which may lead to internal confusion within the net- tackle such challenging problems. This innovative approach
work and ambiguity concerning the concept. This ambiguity obviatestheneedforanextensiveandpotentiallycumbersome
underscores the need for further investigation into the design search for optimal concept partitioning strategies.
of more robust and conceptually discriminative deep learning In detail, we employ ResNet18 as g (q|x) to learn the
ω
models for tackling such complex tasks. concept vector q for Bongard-Logo images, and ResNet50 as
g (k|x) to learn the feature vector set {k } for the images.
θ β
TABLEI The cross-attention mechanism is computed between q and
REASONINGACCURACIESOFRESNET18ONBONGARD-LOGO. {k },yieldingthefinalfeaturerepresentationz oftheimage
β ij
x , which constitutes the core computational process of the
TestAccuracy(%) ij
Cross-Feature Net. We utilize the new backbone g(q,k|x) to
DataSet FF BA CM NV
replace the ResNet18 used in the aforementioned baseline,
Bongard-logo 88.1 97.9 76.0 75.8
with the expectation that the new backbone g(q,k|x) will
SeparatedBongard-logo 97.9 99.0 75.0 72.8
be more suitable for addressing the Bonard-logo problem
compared to ResNet18. we persist in utilizing the InfoNCE
loss function for training the network g(q,k|x). Additionally,
B. Cross-Feature Net (CFN)
drawing inspiration from the EM clustering algorithm, we
Our aspiration is for the network to deduce concepts solely alternatelytraintheparametersofg (q|x)andg (k|x)within
ω θ
from the pixel configuration patterns inherent in images. the Cross-Feature Net. Since the cross-attention mechanism
Conceptsconstitutethefundamentalbasisfortheclassification can be interpreted as a weighted sum process of the feature
of Bongard-Logo problems, making their accurate deduction vector set {k } based on its similarity to the concept vector
β
crucial. Modeling abstract, high-dimensional human concepts q, optimizing g (q|x) is treated as a process of recalculating
ω
often yields intricate and interconnected distributions, posing distribution centroids, while optimizing g (k|x) is considered
θ
significant challenges. Given the difficulty in estimating and as maximizing the expected distribution.
leveraging conditional distributions such as p′(z|y), q′(z|y), In summary, our goal is to utilize a network, denoted
i i
along with the complexity of modeling high-dimensional hu- as g (q|x), for extracting the concepts essential to solving
ω
manconcepts,thispaperattemptstoimplicitlyreconstructthe the problem. This extraction process exclusively relies on
concept for the Bongard-Logo dataset. We reformulate the the pixel configuration patterns within the problem images.
problem as p (x|y′), q (x|y′), where y′ belongs to the set Furthermore,weintroduceanothernetwork,g (k|x),designed
i i θ
{Y }, the range of α remains unknown, and no additional to craft image representations that align closely with these
α
settings were imposed in this study. In this context, each Y extracted concepts. To obtain the final representation of the
α
represents a reconstructed concept, potentially distinct from Bongard-Logo images, we compute the attention result be-
the original concepts. It is our hope that by reconstructing tween the concepts and these representations. This attention
these concepts, or more precisely, by redefining them, the mechanism helps us to focus on the most relevant features.
network will be able to more easily recognize the appro- Subsequently, we constrain the resulting representation using
priate concept for categorizing Bongard-Logo instances. This the InfoNCE loss function and alternately optimize both
recognition will be based primarily on image styles and pixel g (q|x) and g (k|x). By adopting a unified framework,
ω θ
configurationpatterns.Throughthisreorganizationprocess,we g(k|x), which we refer to as CFN (Cross-Feature Network),
aim to alleviate the confusion that often arises between high- we anticipate a substantial enhancement in reasoning preci-
dimensional Bongard-Logo concepts within deep learning sion.ThedetailedfeedforwardprocessesassociatedwithCFN
models. are meticulously illustrated in Figure 5.
As for the implementation on the network, we introduce a
sophisticateddeeplearningalgorithm,designatedasg(q,k|x),
C. Cross covariance-constrained Feature Net (Triple-CFN)
aimed at refining the representation of samples within the
primary group of a Bongard-Logo problem while distinguish- Afterwards, although imitating the EM algorithm to alter-
ing them from those in the auxiliary group. The algorithm nately update the parameters of g (q|x) and g (k|x) can
ω θ
g(q,k|x), called Cross-Feature Net, is tailored to extract enhance the model’s performance, the alternating training
concepts, exclusively based on image styles, thereby laying process for g (q|x) and g (k|x) is tedious and unstable.
ω θ
the foundation for Bongard-Logo solution and categorization. To address this, our paper makes further contributions. In
To elaborate, each image in a Bongard-Logo problem is essence, we designed two networks, g (q|x) and g (k|x),
ω θ
encoded with a concept vector q (q ∈ Rd) and an associated with the aim of implicitly reinterpreting and effectively solv-
feature vector set {k |β ∈ [1,m],k ∈ Rd}. Subsequently, ing the Bongard-Logo problem. By maximizing the image
β β
cross-attentionmechanismsareemployedtoanalyzetheinter- representation information carried by {k } in the output of
β
actions between the conceptual vector q and the style vector g (k|x), we can improve both processes mentioned above.
θ
set {k }, with vector q serving as the query and {k } as the To a certain extent, this paper posits that maximizing the
β β
corresponding key-value pairs. We claim that reinterpreting information carried by the feature vector set {k } can be
β8
{x|j∈[1,7]} {x|j∈[8,14]} {x|j∈[1,7]} {x|j∈[8,14]}
ij ij ij ij
x x
ij ij
gθ(q，k|x) gθ(q，k|x)
resnet18 resnet50 resnet18 resnet50
gθ(q，k|x) g1ω(q|x) g2θ(k|x) gθ(q，k|x) g1ω(q|x) g2θ(k|x)
q k q k q k q k
multi-head cross attention
multi-head cross attention
v v
v v
q k(v)
 ({z |i[1,b], j[1,14]})
cov ij
multi-head cross attention Correlation loss term
{z|j∈[1,7]} {z|j∈[8,14]} v
{zij|j∈[1,7]} {zij|j∈[8,14]}
ij ij
A72
A72 zij
 infoNCE(z pos,~ z pos,{z negm} mM 1) Reasoning loss term  infoNCE(z pos,~ z pos,{z negm} mM 1) Reasoning loss term
Fig.6. StructureofTriple-CFN
Fig.5. structureofCFN
D. Triple-CFN on RPM problem
a viable alternative to the traditional process of maximizing
Particularly when tackling RPM problems, Triple-CFN is
expectation. By adopting this approach, the inherent two-step
able to demonstrate its distinctive utility and value. Specif-
processarisingfromthesimulationoftheEM algorithm[35]
ically, when addressing RPM problems, the core backbone
within the CFN can be effectively integrated.
of Triple-CFN remains unchanged, with only slight adjust-
Therefore, we introduced the correlation loss based on the
ments made to the design of g(q,k|x). These adjustments are
covariance matrix as an additional term in the loss function
necessary operations to enable Triple-CFN to adapt to RPM
for CFN to decrease the correlation between each dimension
reasoning rules and inductive biases.
in{k }.ThisresultedinthecreationoftheCrosscovariance-
β FollowingthecurrentconsensusamongRPMsolvers,which
constrained Feature Net (Triple-CFN). The coefficient for the
emphasizestheneedformulti-scaleormulti-viewpointfeature
newly introduced loss term was set to be 25 times that of
extraction from RAVEN images [24], [25], [30], Triple-CFN
the reasoning loss term. Moreover, the practice of alternately
utilizes Vision Transformer (ViT) for feature extraction while
updating the parameters of g (q|x) and g (k|x), while ex-
ω θ preserving all output vectors as multi-viewpoint features. In
tendingtrainingepochs,yieldsonlyminimalimprovementsfor
this paper, the number of viewpoints is denoted as L. Subse-
theTriple-CFN.Therefore,ontheRPMdata,wenolongeruse
quently, Triple-CFN processes each viewpoint equally.
the strategy of alternating updates between the two networks.
From a viewpoints that considers all images in RPM prob-
Calculating attention results for the output of CNNs can
lems,thispaperemploysaMulti-layerPerceptron(MLP)with
result in attention collapse issues. We posit that reducing the
a bottleneck structure to extract information from all mini-
correlationbetweendimensionsintheoutputrepresentationof
mal reasoning units (three images within a row for RAVEN
CNNs offers a method to alleviate attention collapse. Figure
and three images within a row or column for PGM [37]),
6 presents a detailed depiction of the feed-forward process
and preserves these extracted unit informations as vectors
within Triple-CFN. The complete loss function on a batch of
{k |β ∈[1,M]}.β ∈[1,M]denotestheindexoftheminimal
Bongard-Logo problems for Triple-CFN is as follows: β
reasoning unit, and M stands for the total number of minimal
reasoning units. Using MLP to process sequential images
within minimal reasoning units aims to retain their order in a
b 7
1(cid:88) (cid:88) (cid:16) (cid:17) straightforward manner. Combining the unit vectors from the
ℓ = ℓ z ,z ,{z }14 (4)
Triple-CFN b InfoNCE ij i˜j ij j=8 problem stem with S optimizable vectors, we input them into
i=1j=1,˜j̸=j
the network g (q|x), which utilizes a Transformer-Encoder
ω
+25·ℓ cov({z ij|i∈[1,b],j ∈[1,14]}) as its backbone, to obtain {q α|α ∈ [1,S]} by extracting all
theoptimizablevectorsfromthenetworkoutput.Byencoding
Where b represents the batchsize for training. The reasoning multiple concept vectors {q } through the network g (q|x),
α ω
loss term ℓ (·) has expressed in Formula (3). The the intention is to enable the network g(q,k|x) to solve RPM
InfoNCE
correlation loss term ℓ (·) represents a specific computation problems through a multi-evaluation reasoning approach.
cov
involving vectors enclosed within the parentheses. In this After calculating the cross-attention results between {q }
α
context, the set of vectors inside ℓ (·) is treated as samples and{k },anewMLPisusedtoscoretheseoutputs,resulting
cov β
drawn from a multivariate distribution. Based on these sam- in S scores under one viewpoint. Considering all viewpoints,
ples, the covariance matrix of the multivariate distribution is the current Triple-CFN network encodes L sets of {q } and
α
calculatedusingFormula(1).TheCorrelationLossforTriple- {k } and calculate L×S scores. Subsequently, the sets of
β
CFN is calculated using Formula (2). L×S scores are averaged to determine the final score. To9
enforce constraints on this final score, the Cross-Entropy loss concept space established by the Transformer-Encoder. Recall
function is employed as a reasoning loss term ℓ . We that{q |α∈[1,S]}representsasetofSconceptvectorsfrom
cross-entropy α
still apply the correlation loss term on the vector set {k }. a single viewpoint in Triple-CFN encoding. Given the neces-
β
Notably, the coefficient for the reasoning loss term is set at sity to impose constraints on {q } across all viewpoints, this
α
100 times the magnitude of the correlation loss term. This paperoptstocomputetheaverageof{q }obtainedfromeach
α
coefficient setting differentiates our approach from the one viewpoint,denotedas{q |α∈[1,S]},andimposeconstraints
α
employed in the Triple-CFN for Bongard-Logo tasks. Figure onthisaveragedrepresentation.Regardingtheimplementation
7 provides a detailed illustration of the improvements made ofconceptspacesinRAVEN,wetokenizetheprogressivepat-
to Triple-CFN in order to adapt it to RPM problems. The terndescriptionsusingtheformat’type:XXXX,size:XXXX,
locations of reasoning loss term ℓ and correlation color: XXXX, number/position: XXXX’. Subsequently, we
cross-entropy
loss term ℓ are shown in this Figure. combine these tokenized descriptions with an optimizable
cov
vectorandprocessthemusingstandardTransformerEncoders.
Thecombinedoptimizablevector,extractedfromtheattention
RPM problem statement L×view point
1 results generated by the Transformer Encoder, serves as a
one of view 2 feature of the pattern descriptions. Through this process, we
RPM answer pool points feature
3
generate feature vectors for all kinds of pattern descriptions
x Minimal reasoning unit
vector 4 5 6 for PGM ViT and establish concept spaces of progressive pattern. These
x vecto Ar vw eit ch
to
id
r
e gn roti ufi pe rs x opt vim eS ci× z toa rble 1 4， ，g2 5， θ(q， Idk e| nx t) ity1 4， ，2 5 g， ， 2θ(3 6 k，
|x)
qk
gθ(m qu ，lti- kv |i qe xw )p koints feature fea Ftu inre allv ye ,c ato nrs eware Mr ee tf aer lr oe sd
s
tt eo rmas ,{ bT ask e| dk o∈ n[1 th, eK I] n} f.
oNCE loss,
x
mx m ，， ，， yy nn ，， ，z l c ido r eem
v
na ep tm s
c
ifo o
t
ii
o
es n n
r
re i i
s
sm nd g
w x
ao ,l u
i
f
t y
n ht ,h i t ze
,
o vu eS t c× p tu ot r qTransform . .e . .. .r g1ω(q|x)cov(z)c lo or sr se l ta et ri mon
k(v)
vmulti-head cross att vention
×S
i s fs eim ai tin ult rar ero it vd y eu cc b te oed rt. w s{T ee Th n kis |th kte e ∈rm { [q 1αi ,s } Ka v ]p }ep c il t ni oe r td hs et io n coo T np crit epi pm l tei - sz C pe aF cNth ese a fn oc do rmsi t en h de e
m, n, and l. multi-head cross attention Bottleneck by Meta data. Specifically, it aims to ensure that each q
average α
MLP (bottleneck) S× score exhibits greater similarity to its corresponding pattern feature
at rt ee sn uti lo tn ... crossentropy(s)R loe sa ss o ten ri mng
vector T than to any other vectors {T |k ∈ [1,K],k ̸= k˜}.
k˜ k
Fig.7. StructureofTriple-CFN’sRPMversion The number of vectors in {q |α ∈ [1,S]}, S, is determined
α
by how many optimizable vectors are combined to the logical
input by the g (q|x) network, which can be set arbitrarily.
ω
E. Meta Cross covariance-constrained Feature Net (Meta In the context of Triple-CFN, when the model is constrained
Triple-CFN) byMetadatafromRPMproblems,wetypicallydeterminethe
After the experiment, we discovered that the Triple-CFN’s valueofSbyaddingonetothecountofdecoupledprogressive
learningapproachofreinterpretingabstractreasoningproblem patterns identified within the Meta data. For instance, in the
spaces exhibited slight advantages in addressing the Bongard- case of solving PGM problems, S can be set to 3 due to the
Logoproblem.Triple-CFNwasdesignedtotacklethehypoth- presence of two decoupled conceptual attributes: “shape” and
esis of conflicts between low-dimensional image styles and “line”.
high-dimensional human concepts. When high-dimensional The rationale behind introducing S, which surpasses the
concepts are designed reasonably and effectively, such as the count of decoupled progressive patterns by one, is rooted in
image progressive patterns in RAVEN and PGM problems, the necessity to guarantee an unfettered vector within the set
using these high-dimensional concepts as supervisory signals {q |α ∈ [1,S]}. This particular vector remains unhindered
α
to constrain the training of Triple-CFN can lead to more by the constraints imposed by Meta data, thereby facilitating
significant contributions from the network. its autonomous engagement with resolving the RPM problem.
The RAVEN and PGM problems are accompanied by the Suchaparticularvectorservesasacountermeasureagainstany
image progressive pattern description (Meta data). Previous potential unforeseen and irrational design elements embedded
works have attempted to enhance their RPM solvers by withintheMetadata,therebyenhancingtheoverallrobustness
incorporating additional tasks of learning image progression and adaptability of the Meta Triple-CFN. With the inclusion
patterns, aiming to improve performance on reasoning tasks of progressive pattern labels and the introduction of new
and enhance interpretability. However, efforts led by MRNet loss terms, Triple-CFN undergoes a transformation into Meta
suggest that such approaches may be counterproductive. This Triple-CFN. The Meta loss term can be expressed as follows:
paper posits that strengthening the identity of {q } as a
α ℓ ({q |α∈[1,S−1]},{T |k ∈[1,K]})
conceptual vector, by utilizing these pattern cue signals, can Meta α k
effectively and reasonably enhance the interpretability and S (cid:88)−1 e(q α·T k˜)/t
=− log (5)
r be ra os uo gn hi tng bya tc hc eur sa trc uy ctuo rf eT or fip Tl re i- pC leF -N C. FNT .his is the advantage
α=1,k˜|α
e(q α·T k˜)/t+(cid:80)K k=1,k̸=k˜e(q α·Tk)/t
Inthisstudy,anadditionalstandardTransformer-Encoderis The temperature coefficient t in the Meta loss term is set
employedtoprocesstheMetadatainRAVENandPGMprob- to a value of 10−6. It is worth noting that T represents
k˜
lems, depicting a concept spaces for Triple-CFN. The newly the progressive pattern vector that should be aligned with
introducedMetalosstermisutilizedtoconstrainthe{q }vec- q . In the formula (5), k˜ is determined by α, a fact that
α α
tors,encodedbyTriple-CFN,tocorrespondingpositionsinthe binds the respective progressive patterns to different vectors10
in {q |α ∈ [1,S − 1]}. This ensures that each vector in F. Re-space layer
α
{q α|α∈[1,S−1]} can align with all the decoupled progres- Distinguishing the sources of reasoning difficulty between
sivepatterns.Fromanotherviewpoint,{q α|α∈[1,S]}canbe
Bongard-Logo and RPM problems, the challenge in Bongard-
regardedascomprisingSslots.TheMetalosstermisdesigned
Logo partly stems from conflicts among high-dimensional
to embed S−1 decoupled concepts from the Meta data into concepts at a fundamental level, whereas RPM problems
these slots, while reserving one empty slot to stabilize the demand multi-level reasoning.
Triple-CFN. This reserved slot serves as a safeguard against Inthispaper,bothTriple-CFNandMetatriple-CFNimplic-
certainsubtleandunreasonableconfigurationswithintheMeta itlyorexplicitlyconstraintheprogressivepatternvectors{q }
α
data that might unforeseen. for RPM problems. We posit that, at its core, the constraint
In this advanced framework, the coefficients for both the imposedontheprogressivepatternvector{q }inMetaTriple-
α
novel Meta loss term and the preexisting Cross-Entropy loss CFN is spiritually resembling to the code book approach,
term,whichjointlyenforceconstraintsonmodelreasoning,are albeit implemented through the lens of a linguistic model.
assigned equally and set at a value 100 times greater than the Thus, we contemplate that the essence of Meta Triple-CFN
coefficient of the correlation loss term. The figure 8 illustrate liesinitsabilitytostandardizetheoutputofTriple-CFNunder
the structure of the Meta Triple-CFN in detial. It is worth the supervision of auxiliary labels. This paper designs a noval
noting that Meta Triple-CFN is tailored for RPM problems normalization method applied to the {k |β ∈ [1,M]} vector
β
duetotheirunambiguousandwell-definedauxiliary“rule”su- group in both Triple-CFN and Meta Triple-CFN.
pervisionsignals.Conversely,Bongard-Logoproblemsexhibit Specifically,weestablishM optimizablevectorsforTriple-
overlapping patterns or concepts, which constitute the source CFN, which depict a vector space {v |h ∈ [1,M]}. Subse-
h
of their difficulty, rendering Meta Triple-CFN unsuitable for quently,cosinesimilarityiscomputedbetweentheinformation
addressing Bongard-Logo challenges. from minimal reasoning units, {k }, and each optimizable
β
vector.Andthecalculateprocesscanbeexpressedasfollows:
L×view point
1 v ·k
progressp ivro e g pr ae ttes rs nive pattern o pn oe in to sf fv eie aw tu re 2 3 k β′ h = ||v h||h ×|β |k β|| (6)
discription
t s cy i ozp le oe : r: X :X XXX XXX XXX X，， ， 4 5 6 for PGM ViT k β′ ={k β′ h|h∈[1,M]} (7)
opt vim eciz toa rble
to tokenn sumber/position:XXXX opt vim eS ci× z toa rble 1 4， ，g2 5， θ(q， Idk e| nx t) ity1 4， ，2 5 g， ， 2θ(3 6 k，
|x) qk
gθ(m qu ，lti- kv |i qe xw )p koints feature T {kh ′e r |e hsu ∈ltin [1g ,Mve ]c }to ,r rek pβ′ re, sc eo nm
t
p tho ese cd ooo rf diM natec sos oin fe ths eim mil ia nr ii mtie as
l
Transformer o vu
eS
t
c×
p tu ot r
Transform .e ..r g1ω(q|x)cov(z)c lo or sr se l ta et ri mon vmulti-head cross att vention reaβ sh oning unit vector k
β
within the vector space {v h|h ∈
q ... k(v) ×S [1,M]}, the original reasoning unit vectors {k β} are replaced
Bottleneck by the computed coordinates {k′} for subsequent reasoning
Meta(q,z) losM se tt ea rm at rtm eeS sn× uu ti lo tl nti-head cr .o ..ss attention crossentropy(s)av Re lor ea sag sse o tes n ric mno g re t fia gs uk rs e. A 9.nd Wethe pop sr itoc te hs as
t
o thf isR de- es sβ p iga nce coL na sy te itr uti es si all nus etr xa cte ed llei nn
t
Fig.8. StructureofMetaTriple-CFN
Intuitively, providing Meta data as additional supervisory k'
h
signals directly to a deep neural network to assist in learning v 1 cosine similarity
abstract reasoning problems should naturally improve the net- v 2 cosine similarity kk ''  21
work’s accuracy performance. However, this is not the case in v
3 cosine similarity
practice. Previous research has mostly shown that introducing
k'3
cosine similarity
Meta data to a network can actually decrease its reasoning
v
accuracy. The ingenuity of Triple-CFN lies in its ability k  M cosine similarity k'M k' 
to overcome this curse in RPM. RS-Tran has demonstrated
a tangible improvement in model performance through the
indirectutilizationofMetadata.Specifically,RS-Tranutilizes
Meta data for the pre-training of its encoder, which enhances Fig.9. StructureofRe-spacelayer
the performance of RS-Tran. However, RS-Tran has not yet
achieved concurrent interpretability of rules from a human normalizationtechnique.Duringmodeltraining,thesimilarity
perspective alongside exceedingly high reasoning accuracy. In among the K optimizable vectors is constrained to ensure a
contrast, Triple-CFN emerges as an excellent model that is richly diverse vector space and avoid collapse from the re-
capable of simultaneously balancing both objectives. Further- space layer output. The constraint is implemented by utilizing
more, in the multiple reasoning steps of RS-Tran, the content the following function as an additional loss term for Triple-
of each step needs to be verified through post-hoc masking CFN or Meta Triple-CFN:
experiments, whereas the reasoning steps in Meta Triple-
C paF ttN ernin sh .e Mre en tt aly Te rx iph li eb -i Ct Fex N-a ,n ot ne i tn ht eer op tr he eta rb hil ait ny d,on isp tr ho egr mes os div ee
l
ℓRe-space({v h}M h=1))= h(cid:88)M =1−log e(vh·vh)/t+e (cid:80)(vh
M
h˜· =v 1h ,) h˜/ ̸=t
he(vh·v h˜)/t
that successfully balances both aspects. (8)
...
...
...
...11
Where the t is set to 10−2. The parameter M is set to be as introduces a new Meta loss term rooted in InfoNCE, in ad-
large as the dimension of k , which is 128. When the Re- dition to the components of Triple-CFN. Within Meta Triple-
β
space layer is incorporated into Triple-CFN or Meta Triple- CFN,thecoefficientratioamongtheMetalossterm,reasoning
CFN,thecoefficientsfortheaforementionedlosstermsremain loss term, and correlation loss term stands at 100:100:1. The
consistent with the coefficient for the correlation loss term in Re-space layer emerges as an enhancement for both Triple-
the model. In other words, the ratio between the Meta loss CFN and Meta Triple-CFN. Its integration into the network
term, CE loss term, correlation loss term, and Re-space loss necessitates the addition of a new loss function term, which
term is set to 100:100:1:1. The design of Meta Triple-CFN, servestoensurethattheoutputoftheRe-spacelayerdoesnot
whichincorporatestheRe-spacelayer,isdepictedinthefigure succumb to mode collapse. Consequently, the coefficient ratio
10. Furthermore, the integration of Triple-CFN with the Re- amongtheMetalossterm,reasoninglossterm,correlationloss
space layer becomes self-evident. term,andtheRe-spacelosstermismaintainedat100:100:1:1.
TABLEII
L×view point REASONINGACCURACIESOFCFNONBONGARD-LOGO.
one of view 1
points feature
progressive pattern 2
Accuracy(%)
3
progressive pattern
discription Model Train FF BA CM NV
t s cy i ozp le oe : r: X :X XXX XXX XXX X，， ， 4 5 6 for PGM ViT SNAIL 59.2 56.3 60.2 60.1 61.3
number/position:XXXX 1，2， 1，2，3， multi-viewpoints feature
opt vim eciz toa rble to tokens opt vim eS ci× z toa rble 4 ， r g5 e θs (p qa，ce kla |y xe )r 4，5，  R l6 R oee s-- ss s p p ta eac re c me(v) qkr ge θs (p qa ，ce kla |y qxe )r
k
MP er to aOto pN tNet
et
7 73 5. .3
9
6 64 0. .6
3
7 72 1. .4
6
6 62 5. .4
9
6 65 7. .4
5
Transformer o vu
eS
t
c×
p tu ot r
Transform .e ..r g1ω(q|x)Id e cn ot vit (y z)c lo og r sr2 seθ ( l tak e| t rix mo) n vmulti-head cross att vention ANIL 69.7 56.6 59.0 59.6 61.0
×S Meta-Baseline-SC 75.4 66.3 73.3 63.5 63.9
q ... k(v)
Bottleneck
Meta-Baseline-MoCo 81.2 65.9 72.2 63.9 64.7
Meta(q,z) losM se tt ea
rm S× ra ett
sm
e un
ltu tiol nti-head cr .o ..ss attention

crossentropy(sa )ver Ra log
e
se
a ss o
tens
ri
mc ngo re
WReN-Bongard 78.7 50.1 50.9 53.8 54.3
SBSD 83.7 75.2 91.5 71.0 74.1
Fig.10. StructureofMetaTriple-CFNcombinedwithRe-spacelayer
PMoC 92.0 92.6 97.7 78.3 75.0
It is worth emphasizing that the aforementioned calculation CFN 91.2 86.5 98.1 77.0 77.5
process does not equate to the process of vectors undergoing
CFN+EM 93.9 93.8 99.4 77.8 77.2
matrix mapping and subsequent tanh compression. This de-
Triple-CFN 93.2 92.0 99.2 80.8 79.1
signenhancestheapplicabilityofTriple-CFNandMetaTriple-
Triple-CFN+EM 95.3 94.3 99.8 80.3 80.0
CFN to RPM problems.
By alternating the updates of g (k|x) and g (q|x), we aimed
θ ω
IV. EXPERIMENT
to simulate the iterative nature of the EM algorithm, which
In this study, we conducted experiments on the Bongard- is known for its effectiveness in finding maximum likelihood
Logo dataset using the designed CFN and Triple-CFN mod- estimates in statistical models with latent variables. Our ab-
els. All our experiments are implemented in Python using lation studies revealed that this alternating update strategy
the PyTorch [38] framework. To demonstrate the impact of contributed to improving the performance of the CFN on the
alternating updates of g (k|x) and g (q|x), which mimic the Bongard-Logo task. As observed in the table II, alternating
θ ω
Expectation-Maximization algorithm, on model performance, updatesbetweennetworks g (k|x)and g (q|x)enhancedthe
θ ω
we performed ablation experiments. The results of these model’s performance on the FF and BA problems without
experiments are presented in tables II. It is important that our significantly affecting its ability to solve the generalization
experiments were conducted on a single server equipped with problems of ”NV” and ”CM”. This indeed suggests that
four A100s graphics processing units (GPUs). We trained the simulating the EM process during training, while beneficial,
models using mini-batch gradient descent with a batch size may be somewhat redundant when combined with the already
of 120. During training, we utilized the Adam [39] optimizer excellentcross-attentionmechanism.Furthermore,astheCFN
with a learning rate of 10−3 and a weight decay of 10−4. It is upgraded to the Triple-CFN, the role of EM diminishes.
is worth mentioning again that Triple CFN incorporates two In addition, compared to PMoC, Triple-CFN exhibits better
lossfunctionterms:thereasoninglosstermandthecorrelation performance on multiple quantifiable metrics of the Bongard-
loss term. When Triple-CFN is applied to the Bongard-Logo Logo dataset, while requiring fewer parameters and simpler
problem,thecoefficientratiobetweenthereasoninglossterm, computationalforms.Moreover,itdoesnotnecessitateparallel
which is composed of the infoNCE loss, and the correlation reasoningtasksinvolvingmultipleperspectivesandinferences.
loss term based on the covariance matrix is set to 1:25. When confronted with the RAVEN database in RPM prob-
However, when addressing the RPM problem, the coefficient lems, Triple CFN has demonstrated considerable strength
ratio between the reasoning loss term, formulated through and performance. In this study, we conducted experiments
cross-entropyloss,andthecorrelationlosstermshiftsto100:1. using identical software and hardware configurations as those
Meta Triple-CFN, tailored specifically for the RPM problem, employed in the RS-Tran experiments. We replicated the12
experimental parameters from the RS-Tran setup, including must be preserved, while the remaining parameters are ran-
batch size, learning rate, and all other factors that could domlyinitialized.Moreprecisely,theparametersoftheVision
potentially influence model performance. This was done to Transformer used for image encoding and the Multi-Layer
facilitate the most straightforward comparison with RS-Tran, Perceptron responsible for extracting information related to
which is currently considered the state-of-the-art model. the theminimalreasoningunitsin(Meta)Triple-CFNareretained,
the accuracy of Triple-CFN on RAVEN and I-RAVEN is while all other parameters undergo random initialization.
recorded in the Table III. The results presented in the table
clearly indicate that Triple-CFN exhibits a notably superior
V. CONCLUSION
performance when compared to RS-Tran.
This paper introduces the novel Triple-CFN approach, tai-
lored specifically for the Bongard-Logo problem. The Triple-
TABLEIII
REASONINGACCURACIESONRAVENANDI-RAVEN. CFN’s unique architecture enables it to implicitly reorganize
the conceptual space of conflicting Bongard-Logo instances,
TestAccuracy(%) achieving remarkable performance on this task. Furthermore,
Model Average Center 2×2Grid 3×3Grid L-R U-D O-IC O-IG
SAVIR-T[25] 94.0/98.1 97.8/99.5 94.7/98.1 83.8/93.8 97.8/99.6 98.2/99.1 97.6/99.5 88.0/97.2 the adaptability of the Triple-CFN paradigm is demonstrated
SCL[24],[25] 91.6/95.0 98.1/99.0 91.0/96.2 82.5/89.5 96.8/97.9 96.5/97.1 96.0/97.6 80.1/87.7 through its effective application to the RPM problem, where
MRNet[19] 96.6/- -/- -/- -/- -/- -/- -/- -/-
RS-TRAN[30] 98.4/98.7 99.8/100.0 99.7/99.3 95.4/96.7 99.2/100.0 99.4/99.7 99.9/99.9 95.4/95.4 necessary modifications were made to yield competitive re-
Triple-CFN 99.6/99.8 100.0/100.0 99.7/99.8 98.8/99.4 99.9/100.0 99.9/100.0 99.9/99.9 99.2/99.2 sults.
Notably, the well-defined rules, progressive patterns and clear
We subsequently conducted experiments on the PGM dataset
boundaries governing the RPM problem necessitated the de-
using Triple-CFN and Re-space layer under the exact same
velopment of the Meta Triple-CFN network. This network
experimentalconditionsasRs-Tran,theaccuracyofreasoning
explicitly structures the problem space for the RPM issue,
answers is recorded in the Table IV and the accuracy of
maintaininginterpretabilitywhileattainingstate-of-the-artper-
Reasoning progressive patterns is r ecorded in Tabel V. Our
formance on the PGM problem.
aim was to demonstrate the superiority of both Triple-CFN
Overall,thispapercontributestotheadvancementofmachine
and Meta Triple-CFN. It is worth mentioning again that
intelligence by exploring innovative network designs tailored
Meta Triple-CFN balances both ex-ante interpretability of the
for abstract reasoning tasks. The proposed Triple-CFN and
progressive patterns and reasoning accuracy, which is not
Meta Triple-CFN approaches represent significant steps for-
achievable by Rs-Tran and other previous model in Table IV.
wardinaddressingthechallengesposedbytheBongard-Logo
and RPM problems, respectively. We believe that our findings
TABLEIV
REASONINGACCURACIESOFTRIPLE-CFNONPGM. willstimulatefurtherresearchanddevelopmentinthiscritical
area of artificial intelligence. In essence, Triple-CFN aims
Model TestAccuracy(%) to propose a fundamental methodology for tackling abstract
SAVIR-T[25] 91.2 reasoning problems, namely the normalization of reasoning
information. Both Meta Triple-CFN and the Re-space layer
SCL[24],[25] 88.9
are attempts at normalizing reasoning information, and they
MRNet[19] 94.5
have achieved notable improvements in network performance,
RS-CNN[30] 82.8
thereby demonstrating the effectiveness of this approach.
RS-TRAN[30] 97.5
Triple-CFN 97.8
REFERENCES
Triple-CFN+Re-spacelayer 98.2
[1] Deng,J.,Dong,W.,Socher,R.,Li,L.J.,Li,K.,&Fei-Fei,L.Imagenet:A
MetaTriple-CFN 98.4 large-scalehierarchicalimagedatabase.InIEEEConferenceonComputer
VisionandPatternRecognition,246-255(2009).
MetaTriple-CFN+Re-spacelayer 99.3
[2] He, K., Zhang, X., Ren, S., & Sun, J. Deep Residual Learning for
ImageRecognition.InIEEEConferenceonComputerVisionandPattern
Recognition,770-778(2016).
[3] Krizhevsky, A., Sutskever, I., & Hinton, G. E. Imagenet classification
TABLEV
withdeepconvolutionalneuralnetworks.CommunicationsoftheACM,
PROGRESSIVEPATTERNREASONINGACCURACIESANDOFTRIPLE-CFN
60(6),84-90(2017).
ONPGM.
[4] Vaswani, A. et al. Attention is All You Need. In Advances in Neural
InformationProcessingSystems,(2017).
Accuracy(%)
[5] Devlin,J.,Chang,M.W.,Lee,K.,&Toutanova,K.Bert:Pre-trainingof
Model shape line answer Deep Bidirectional Transformers for Language Understanding. Preprint
athttps://arxiv.org/abs/1810.04805(2018).
MetaTriple-CFN 99.5 99.9 98.4 [6] Brown, T. et al. Language Models are Few-shot Learners. In Advances
MetaTriple-CFN+Re-spacelayer 99.7 99.9 99.3 inNeuralInformationProcessingSystems,1877-1901(2020).
[7] Kingma,D.P.,&Welling,M.Auto-encodingvariationalbayes.Preprint
athttps://arxiv.org/abs/1312.6114(2014).
Integrating the Re-space layer with Triple-CFN and Meta [8] Goodfellow, I. et al. Generative adversarial networks. Communications
Triple-CFNrequirestoretentpartialmodelparameters.Specif- oftheACM,63(11),139-144(2020).
[9] Ho, J., Jain, A., & Abbeel, P. Denoising diffusion probabilistic models.
ically, the parameters of the modules preceding the Re-space
In Advances in Neural Information Processing Systems, 33, 6840-6851
layer access point within Triple-CFN and Meta Triple-CFN (2020).13
[10] Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C. L., [35] Dempster,ArthurP.,NanM.Laird,andDonaldB.Rubin.”Maximum
& Parikh, D. VQA: Visual question answering. In IEEE International likelihood from incomplete data via the EM algorithm.” Journal of the
ConferenceonComputerVision,2425-2433(2015). royalstatisticalsociety:seriesB(methodological)39.1(1977):1-22.
[11] Johnson, J., Hariharan, B., Van Der Maaten, L., Fei-Fei, L., Lawrence [36] Oord,A.V.D.,Li,Y.,&Vinyals,O.RepresentationLearningwithCon-
Zitnick, C., & Girshick, R. Girshick. CLEVR: A Diagnostic Dataset trastive Predictive Coding. Preprint at https://arxiv.org/abs/1807.03748
forCompositionalLanguageandElementaryVisualReasoning.InIEEE (2019).
Conference on Computer Vision and Pattern Recognition, 2901-2910 [37] Carpenter, P. A., Just, M. A., & Shell, P. What One Intelligence
(2017). Test Measures: a Theoretical Account of the Processing in the Raven
[12] Raven J. C. Raven’s Progressive Matrices. (Western Psychological ProgressiveMatricesTest.Psychologicalreview,97(3),404,(1990).
Services,(1938). [38] Paszke,A.etal.AutomaticDifferentiationinPytorch.InNIPSAutodiff
[13] Depeweg, S., Rothkopf, C. A., & Ja¨kel, F. Solving Bongard Prob- Workshop,(2017).
lems with a Visual Language and Pragmatic Reasoning. Preprint at [39] Kingma,D.P.,&Ba,J.Adam:AMethodforStochasticOptimization.
https://arxiv.org/abs/1804.04452(2018). Preprintathttps://arxiv.org/abs/1412.6980,(2014).
[14] Nie, W., Yu, Z., Mao, L., Patel, A. B., Zhu, Y., & Anandkumar, A.
Bongard-LOGO:ANewBenchmarkforHuman-LevelConceptLearning
andReasoning.InAdvancesinNeuralInformationProcessingSystems,
16468–16480(2020).
[15] R.Song, B.Yuan. Solving the bongard-logo problem by modeling a
probabilistic model. Preprint at https://arxiv.org/abs/ arXiv:2403.03173
(2024).
[16] Zhang, C., Gao, F., Jia, B., Zhu, Y., & Zhu, S. C. Raven: A Dataset
for Relational and Analogical Visual Reasoning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
5317–5327(2019).
[17] Barrett,D.,Hill,F.,Santoro,A.,Morcos,A.,&Lillicrap,T.Measuring
AbstractReasoninginNeuralNetworks.InInternationalConferenceon
MachineLearning,511-520(2018).
[18] Hu,S.,Ma,Y.,Liu,X.,Wei,Y.,&Bai,S.StratifiedRule-AwareNetwork
forAbstractVisualReasoning.InProceedingsoftheAAAIConference
onArtificialIntelligence,1567-1574(2021).
[19] Benny, Y., Pekar, N., & Wolf, L. Scale-Localized Abstract Reasoning.
In Proceedings of the IEEE/CVF Conference on Computer Vision and
PatternRecognition,12557-12565,(2021).
[20] Zhang, C., Jia, B., Gao, F., Zhu, Y., Lu, H., & Zhu, S. C. Learning
Perceptual Inference by Contrasting. In Proceedings of Advances in
NeuralInformationProcessingSystems,(2019).
[21] Zheng,K.,Zha,Z.J.,&Wei,W.AbstractReasoningwithDistracting
Features.InAdvancesinNeuralInformationProcessingSystems,(2019).
[22] Zhuo, T., & Kankanhalli, M. Effective Abstract Reasoning with Dual-
ContrastNetwork.InProceedingsofInternationalConferenceonLearn-
ingRepresentations,(2020).
[23] Zhuo, Tao and Huang, Qiang & Kankanhalli, Mohan. Unsupervised
abstract reasoning for raven’s problem matrices. IEEE Transactions on
ImageProcessing,8332–8341,(2021).
[24] Wu, Y., Dong, H., Grosse, R., & Ba, J. The Scattering Compositional
Learner: Discovering Objects, Attributes, Relationships in Analogical
Reasoning.Preprintathttps://arxiv.org/abs/2007.04212(2020).
[25] Sahu,P.,Basioti,K.,&Pavlovic,V.SAViR-T:SpatiallyAttentiveVisual
ReasoningwithTransformers.Preprintathttps://arxiv.org/abs/2206.09265
(2022).
[26] Wei, Qinglai, et al. ”Raven solver: From perception to reasoning.”
InformationSciences634(2023):716-729.
[27] Zhang, C., Jia, B., Zhu, S. C., & Zhu, Y. Abstract Spatial-Temporal
ReasoningviaProbabilisticAbductionandExecution.InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,
9736-9746(2021).
[28] Zhang,C.,Xie,S.,Jia,B.,Wu,Y.N.,Zhu,S.C.,&Zhu,Y.Learning
Algebraic Representation for Systematic Generalization. In Proceedings
oftheEuropeanConferenceonComputerVision,(2022).
[29] Hersche, M., Zeqiri, M., Benini, L., Sebastian, A., & Rahimi, A.
A Neuro-vector-symbolic Architecture for Solving Raven’s Progressive
Matrices.Preprintathttps://arxiv.org/abs/2203.04571(2022).
[30] Q. Wei, D. Chen, B. Yuan, Multi-viewpoint and multi-evaluation with
felicitous inductive bias boost machine abstract reasoning ability, arXiv
:2210.14914,2022.
[31] Shi, Fan, Bin Li, and Xangyang Xue. ”Abstracting Concept-Changing
Rules for Solving Raven’s Progressive Matrix Problems.” arxiv preprint
arXiv:2307.07734(2023).
[32] S.Kharagorgiev,“Solvingbongardproblemswithdeeplearning,”
k10v.github.io,2020.
[33] Dosovitskiy,A.etal.AnImageisWorth16x16Words:Transformersfor
Image Recognition at Scale. Preprint at https://arxiv.org/abs/2010.11929
(2020).
[34] Bardes, Adrien, Jean Ponce, and Yann LeCun. ”Vicreg: Variance-
invariance-covariance regularization for self-supervised learning.” ar**v
preprintar**v:2105.04906(2021).