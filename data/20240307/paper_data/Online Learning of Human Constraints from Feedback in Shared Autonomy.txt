Online Learning of Human Constraints from Feedback in Shared Autonomy
ShibeiZhu1,TranNguyenLe2,SamuelKaski1,3,VilleKyrki2
1DepartmentofComputerScience,AaltoUniversity,Finland.
2DepartmentofElectricalEngineeringandAutomation,AaltoUniversity,Finland.
3DepartmentofComputerScience,UniversityofManchester,UnitedKingdom
Abstract
Real-timecollaborationwithhumansposeschallengesdueto
thedifferentbehaviorpatternsofhumansresultingfromdi-
versephysicalconstraints.Existingworkstypicallyfocuson
learningsafetyconstraintsforcollaboration,orhowtodivide
anddistributethesubtasksbetweentheparticipatingagentsto
carryoutthemaintask.Incontrast,weproposetolearnhu- Figure 1: Co-transportation task where both human and
manconstraintsmodelthat,inaddition,considerthediverse robot operate on the same object. Left: assistive agent that
behaviors of different human operators. We consider a type considershumanphysicalconstraints.Right:assistiveagent
of collaboration in a shared-autonomy fashion, where both without considering human’s physical constraints, and hu-
a human operator and an assistive robot act simultaneously
manoperatortriestoretakecontrolbypullingbacktheob-
inthesametaskspacethataffectseachother’sactions.The
ject.
taskoftheassistiveagentistoaugmenttheskillofhumans
to perform a shared task by supporting humans as much as
possible, both in terms of reducing the workload and mini-
mizingthediscomfortforthehumanoperator.Therefore,we Todate,Reinforcementlearning(RL),specificallymulti-
proposeanaugmentativeassistantagentcapableoflearning agentreinforcementlearning(MARL),hasshownenormous
andadaptingtohumanphysicalconstraints,aligningitsac- successincollaborativetasks(KokandVlassis2006;Niko-
tions with the ergonomic preferences and limitations of the
laidisetal.2015;Omidshafieietal.2017;Wangetal.2018).
humanoperator.
Givenasharedgoal,multi-agentreinforcementlearningal-
lowsustoformulateacollaborativetaskasaMarkovGame
Introduction where an assistive agent can learn how to collaborate with
Collaboration forms an essential part of our daily life. In another agent by jointly maximizing their shared task re-
recentyears,technological advancesinbothartificial intel- ward.Mostoftheexistingworksincollaborativetasksonly
ligence and robotics allowed us to use robots to empower focus on how to distribute a single task into subtasks such
humans to perform repetitive tasks or physically demand- thatthetaskloadcanbedividedamongdifferentparticipants
ing tasks (Nemec et al. 2013; Varier et al. 2020; Clegg inthetask(KwonandSuh2011;Wuetal.2021;Chenetal.
etal.2020).However,designinganassistiverobotthattakes 2020). However, when it comes to a human-robot collabo-
intoaccounthumandemandandadaptsitspolicytodiffer- rativetasksetup,thecollaborationtypicallyconsistsofhow
enthumansinreal-timeischallenging.Differentindividuals totransferautonomysmoothlybetweenarobotagentanda
presentwithdifferentphysicalcapabilitiesandevenchang- humanoperatorbackandforthinordertocompleteagiven
ingcapabilitiesduringthecollaboration,i.e.,duetofatigue task.Essentially,thismeansthateachagenthasitsownsub-
levels.Theassistantshould notonlytakeintoaccount how tasksortasksspacewherethecollaborationtakesplaceonly
to collaborate with humans safely, but also consider their when they need to transfer the autonomy. In this paper, we
physical constraints or capability on the fly during the col- address another type of collaboration task, where both the
laboration. While some physical aspects of humans can be humanandtheassistiveagentoperateinthesametaskspace
defined,suchastheirheightormusclestrength,someother with the same shared goal with shared autonomy. In such
factors are subject to changes, such as the fatigue level of a scenario, the agent needs to learn the human constraints
the individuals. These factors vary from individual to indi- onlineinordertoadaptitspolicysothattheirjointactions
vidual. Thus, it is difficult to establish some deterministic arephysicallyfeasible.Totheknowledgeoftheauthors,the
rules or mathematical forms for the assistive agent that ac- problemofonlineconstraintlearningfromhumanfeedback
countforallthesituationsduringtheinteraction.Designing remainsunderexplored,noritsapplicationinacollaborative
anadaptiveagentthatdetectsdifferenthumanconstraintsin physicalhuman-robotinteractionframework.
real-time is essentialin order to deliver a more satisfactory Toaddresstheaforementionedopenissues,weproposean
collaborationexperienceandqualityofcollaboration. adaptiveagentthatlearnsdifferenthumanconstraintsasthe
4202
raM
5
]OR.sc[
1v47920.3042:viXrahumanmodelfromthehumanfeedbackduringthephysical
interaction.Thisallowstheagenttoadaptitspolicyinreal-
timethataccommodatedifferenthumanneeds.
Problemformulation
Weconsideratypeofcollaborativetaskinwhichtherobot
assistsahumanoperatortoperformthetaskbyaugmenting
the human operator’s skills. Let us consider a multi-agent
human-robotcollaborativeframeworkformulatedasatwo-
player Markov Game M = ⟨S,AH,AR,p,p ,R,γ,T⟩,
0
where S ∈ Rm is the state space, AH ∈ Rn and AR ∈
Rn are the robot and human’s respective action spaces,
subject to different physical constraints. The joint action
space is defined as A = AH × AR, and the state transi-
tion dynamic is given as p : S × A × S → [0,1] with
p as the initial state distribution. And R is the shared
0 Figure2:Constraintsinthetaskspacewheredifferentagents
task reward. The joint Q function that defines the ex-
havetheirconstraintsintheactionspace.Giventhebehavior
pected return for both agents is defined as Q(s,aR,aH) =
(cid:104) (cid:105) spacesofhumansandrobotsconstrainedbytheirrespective
E (cid:80)T t=− 01γtR t(s t,aR
t
,aH
t
;π H,π R) physicalconstraints,thecoloredareasrepresentthetrustre-
Both robot and human actions are constrained by their gionthatdefinestheplausiblesetofjointbehaviorthatsatis-
own physical limitations, denoted as C and C respec- fiesalltheconstraints.Werepresentthelowerboundofthe
tively. The former is explicitly given byR its corrH eθ sponding trustregionastheconstraintsthatdefinetheupperboundof
joint limitations. However, the latter remains unknown and thehumanbehaviorspace.
subject to each individual. Human policies might not nec-
essarilybetheresultofrationaldecisions,butrathernoisy,
Fig2:theassistiverobotanddifferenthumanoperatorshave
oftenreferredtoasBoltzmannrational(Ziebartetal.2008;
different physical constraints that define different behavior
Boularias,Kober,andPeters2011;Maliketal.2018).Thus,
spaces. Ideally, the assistive agent should adapt its policy
their corresponding policy that optimizes their own con-
consideringhumanphysicalconstraintssuchthatinteraction
straintwhiletakingintoaccountrobot’sphysicalconstraints
will not violate their respective physical constraints. Thus,
isgivenas:
thedesiredassistiveagent’spolicyisre-writtenas:
eQ(s,aH,aR)/β
π H∗ (aH|aR,s)= (cid:82)
aH
eQ(s,aR,aH)/β (1) π R∗(aR|s,aH,A θ)= (cid:82) eQ e( Qs, (a sH ,a, RaR ,a) H/ )β
/β
(5)
aR
subjecttoπ (aH|s)≤C , (2)
H Hθ subjectto(aH,aR)∈A
θ
(6)
aR ≤C (3)
R
Learningatrustregionfromreal-time
where β is the temperature parameter that adjusts the ran-
feedbackwithhumanconstraintmodel
domnessortherationalityofthedecision.
As human constraints cannot be universally defined and As discussed previously, it is hard to establish some de-
consider all individuals, we propose to construct a feasible terministic values for human constraints, as many factors
joint-actionregionbasedontheexternalfeedbackduringthe wouldinfluencethesevalues.Insteadoflearningsomefixed
collaboration.ThisisanaloguetoatrustregionA thatde- values,weproposeahumanconstraintmodeltolearnatrust
θ
fines the valid joint-action space shared by the human and region where the joint actions are compatible for both the
the assistive agent, where both agents’ physical constraints robot and the current human operator. The upper bound of
aresatisfied.Theupperboundofthistrustregionisdefined this trust region is given by the physical constraints of the
bythephysicalconstraintsoftherobotanditslowerbound assistive agent, whereas human constraints draw the lower
is given by different human constraints as shown in Fig 2. boundofthisregion.Ourgoalistodefinethefeasiblejoint-
Mathematically,thisregionisdefinedas: action region by finding this lower bound based on human
externalfeedback.
A ={(aR,aH)∈Rn :C ≤(aR,aH)≤C } (4)
θ Hθ R
Human’sfeedbackasindirectlabelsforthenegativeand
Humanphysicalconstraintscanbeinfluencedbyvarious positivesamples. Humanscaninfluencetheactionofthe
sources of factors, i.e., physical limitations, fatigue levels, assistive agent by applying a wrench w ∈ R6 to deviate
h
etc.Therefore,itisnottrivialtoestablishdeterministicval- the agent from its current predicted action. A wrench is a
ues to define these constraints for different humans under six-dimensional vector consisting of a force f ∈ R3 and
h
different circumstances, but rather be defined as a feasible a torque τ ∈ R3. We assume that this additional wrench
h
regionofjointactions.Wecanseeanillustrativeexamplein is applied when a human reaches the upper bound of theirHuman
External
Assistive Agent Constraints Human
Feedback
Model
Trust Region
Environment
Figure 4: Robot assists human in co-transportation task
(Left)andrehabilitation(Right).
Figure3:Giventheexternalfeedbackreceivedinreal-time
fromahuman,wedevelopahumanconstraintmodeltode-
finealowerboundofthetrustregionwherethejointactions robottounderstandthesehumanconstraints,therebybend-
ofboththehumanandtheassistiveagentsatisfytheircon- ing its policy to alleviate the physical strain on the human
straints. whileefficientlyachievingthetaskathand.
Rehabilitation robots. In addition to co-transportation,
otherusercasescanbefoundinmedicaldomains.Examples
physicalconstraints.Inthesimplestcase,theassistiveagent
include rehabilitation, as shown in Fig 4, where a rehabili-
canuseitssensoryinformationandthecurrentobservation
tationrobotortherapeuticrobotsassistpeoplewithmanip-
ofhumanpolicytodeterminewhetheritscurrentpolicyfalls
ulativedisabilities(Tejima2001).Insuchcases,therobotis
outofthetrustregion,asfollows:
equippedwithactuationtomovepatients’limbsinorderto
compensateforthephysicalcapabilitiesofthepatients.The
(cid:26)
(π ∈trustregion|w )= positive, ifw h >δ (7) main task of the assistive robot is to progressively aid pa-
R h negative, otherwise tientsthroughtherehabilitationprocesstorecovertheirnor-
maldailyfunctions(QianandBi2015).Thus,ahumancon-
Whereδisathresholdthatrepresentstheminimumexternal
straint model is needed to detect and adapt different strate-
feedbackvaluewhenahumanprovidesanyfeedbackorcol-
giesduringtherehabilitationprocess.
laboratesinthetask.Anyvaluereadingbelowthisthreshold
indicatesthatnohumanactionisdetectedandapossiblesce- Exoskeletonrobots. Similarcasescanbefoundwithex-
nario when they are out of their physically operable region oskeleton robots (Veneman et al. 2007; Zhang et al. 2017),
thatpreventsthemfromcollaborating. where a personalized assistive robot is needed to augment
Assume that human operators would take actions that different human operators with different physical capabili-
bring the joint action back to their comfort zone or feasi- tiesinrealtimetoperformsomechallengingtasks.Similar
bleregion;thisfeedbackinformationallowsustodefinethe to the previous case, the assistive agent augments the mo-
trust region A in Eq. 4. We propose to estimate the un- bilityofahumanbytakingintoaccountthephysicallimita-
θ
known lower bound of this trust region C based on the tionsofdifferentpatientsinordertoaugmenttheirphysical
Hθ
human’sexternalforces.Inthesimplestcase,wehavelinear capabilities. For instance, in (Ivaldi et al. 2021) exoskele-
constraintsthatdefinethesurfaceorahyperplanethatsep- tonisusedtoassistmedicalstaffwithheavy-loadtaskssuch
aratesthepositiveandnegativesamplesofthetrustregion, asliftingpatients.Theirstudyshowsthatdifferentindividu-
asshowninFig.2.Wedefineaconstraintmodeltopredict alshavedifferentpreferencestowarddifferentexoskeletons,
human constraints to define hyperplane based on feedback where improvement can be introduced by adding an adap-
asdescribedinEq.7: tive assistive robot that accommodates different individual
needs.
C =f(aR,aH,s,w ;θ) (8)
Hθ h
TheoverallarchitecturecanbefoundinFig.3.Wherethe Experiments
loss function can be defined based on the feedback as the
As mentioned earlier, co-transportation is one of the most
binarycross-entropylossusingEq.7.
obvious use cases for the proposed method. Therefore, to
gain a deeper insight into the challenges in this task set-
Potentialusecases
ting, we conducted an experiment in which a human and a
Co-transportation. Oneofthemostobvioususecasesfor robotcollaboratedtotransportaheavyobjectfrompointA
the proposed method is the co-transportation task where a topointB.
robot assists a human in transporting a heavy object to a In this experiment, the robot’s policy was executed in a
designatedlocation,asshowninFig4.Withinthisscenario, compliant manner, allowing the human to exert influence
humans usually experience fatigue due to various factors, ontherobot’sconfiguration.Thisexperimentwassystemati-
suchastheheavyweightoftheobjectoruncomfortablelift- callyrepeatedwithtwodifferenthumanoperators,eachper-
ingposes.Ideally,theproposedmethodshouldempowerthe formingthetaskfivetimes.TheexternalforcesandtorquesFigure 5: Real human feedback from 2 individuals during the collaborative tasks, represented by their means and standard
deviations.Theforcefeedbackissix-dimensionalvectorthatisrepresentedbyeachplot.Thex-axisrepresentsthetime,and
they-axisthecorrespondingvaluereading.5runsareusedtogeneratetheseplots.
that humans acted on the robot are plotted in Fig 5. From Chen,M.;Nikolaidis,S.;Soh,H.;Hsu,D.;andSrinivasa,S.
theplotwecanseethattwodifferenthumansyieldtwodif- 2020. Trust-awaredecisionmakingforhuman-robotcollab-
ferent sets of external forces and torques, signifying dis- oration:Modellearningandplanning.ACMTransactionson
tinct physical constraints imposed by individual strengths Human-RobotInteraction(THRI),9(2):1–23.
and attributes, such as height. Furthermore, the measure- Clegg, A.; Erickson, Z.; Grady, P.; Turk, G.; Kemp, C. C.;
ments also align with the problem illustrated in Fig 2. It is and Liu, C. K. 2020. Learning to collaborate from simula-
alsoworthmentioningthatthefeedbackfromthesameindi- tionforrobot-assisteddressing.IEEERoboticsandAutoma-
vidual yields different variations, which supports our argu- tionLetters,5(2):2746–2753.
ment that humans are not always optimal decision-makers,
Ivaldi, S.; Maurice, P.; Gomes, W.; Theurel, J.; Wioland,
butinsteadsubjecttosomedegreeofnoise.
L.; Atain-Kouadio, J.-J.; Claudon, L.; Hani, H.; Kimmoun,
A.; Sellal, J.-M.; et al. 2021. Using exoskeletons to assist
medicalstaffduringpronepositioningofmechanicallyven-
Conclusion tilated COVID-19 patients: a pilot study. In Advances in
Inthispaper,weproposeanewlearningframeworkthatal- HumanFactorsandErgonomicsinHealthcareandMedical
lowsustolearnhumanconstraintsthatarediverseandsub- Devices:ProceedingsoftheAHFE2021VirtualConference
ject to different circumstances in a collaborative physical onHumanFactorsandErgonomicsinHealthcareandMed-
human-robot setup. While existing works focus on learn- icalDevices,July25-29,2021,USA,88–100.Springer.
ing safety constraints offline, we propose learning human Kok, J. R.; and Vlassis, N. 2006. Collaborative multiagent
physical constraints online with human feedback in a col- reinforcement learning by payoff propagation. Journal of
laborative task environment. Unlike the existing works on MachineLearningResearch,7:1789–1828.
collaborativetasksthatfocusesmostlyonhowtotransition Kwon, W. Y.; and Suh, I. H. 2011. Towards proactive as-
autonomyfromoneagenttoanother,ourlearningagentfo- sistantrobotsforhumanassemblytasks. InProceedingsof
cusesonadaptivelylearninghumanphysicalconstraintsdur- the 6th international conference on Human-robot interac-
ingtheinteractiontoaugmenthumanskills. tion,175–176.
Malik, D.; Palaniappan, M.; Fisac, J.; Hadfield-Menell, D.;
References
Russell, S.; and Dragan, A. 2018. An efficient, general-
Boularias, A.; Kober, J.; and Peters, J. 2011. Relative en- ized Bellman update for cooperative inverse reinforcement
tropyinversereinforcementlearning. InProceedingsofthe learning. In International Conference on Machine Learn-
fourteenthinternationalconferenceonartificialintelligence ing,3394–3402.PMLR.
and statistics, 182–189. JMLR Workshop and Conference Nemec,B.;Abu-Dakka,F.J.;Ridge,B.;Ude,A.;Jørgensen,
Proceedings. J.A.;Savarimuthu,T.R.;Jouffroy,J.;Petersen,H.G.;andKru¨ger, N. 2013. Transfer of assembly operations to new
workpieceposesbyadaptationtothedesiredforceprofile.In
2013 16th International Conference on Advanced Robotics
(ICAR),1–7.IEEE.
Nikolaidis,S.;Ramakrishnan,R.;Gu,K.;andShah,J.2015.
Efficient model learning from joint-action demonstrations
for human-robot collaborative tasks. In Proceedings of
the tenth annual ACM/IEEE international conference on
human-robotinteraction,189–196.
Omidshafiei,S.;Pazis,J.;Amato,C.;How,J.P.;andVian,J.
2017. Deepdecentralizedmulti-taskmulti-agentreinforce-
ment learning under partial observability. In International
ConferenceonMachineLearning,2681–2690.PMLR.
Qian, Z.; and Bi, Z. 2015. Recent development of rehabil-
itation robots. Advances in Mechanical Engineering, 7(2):
563062.
Tejima, N. 2001. Rehabilitation robotics: a review. Ad-
vancedRobotics,14(7):551–564.
Varier, V. M.; Rajamani, D. K.; Goldfarb, N.; Tavakkol-
moghaddam, F.; Munawar, A.; and Fischer, G. S. 2020.
Collaborative suturing: A reinforcement learning approach
to automate hand-off task in suturing for surgical robots.
In 2020 29th IEEE international conference on robot and
human interactive communication (RO-MAN), 1380–1386.
IEEE.
Veneman,J.F.;Kruidhof,R.;Hekman,E.E.;Ekkelenkamp,
R.;VanAsseldonk,E.H.;andVanDerKooij,H.2007. De-
signandevaluationoftheLOPESexoskeletonrobotforin-
teractive gait rehabilitation. IEEE Transactions on neural
systemsandrehabilitationengineering,15(3):379–386.
Wang, W.; Li, R.; Chen, Y.; Diekel, Z. M.; and Jia,
Y. 2018. Facilitating human–robot collaborative tasks
byteaching-learning-collaborationfromhumandemonstra-
tions. IEEETransactionsonAutomationScienceandEngi-
neering,16(2):640–653.
Wu, S. A.; Wang, R. E.; Evans, J. A.; Tenenbaum, J. B.;
Parkes, D. C.; and Kleiman-Weiner, M. 2021. Too Many
Cooks: Bayesian Inference for Coordinating Multi-Agent
Collaboration. TopicsinCognitiveScience,13(2):414–432.
Zhang,J.;Fiers,P.;Witte,K.A.;Jackson,R.W.;Poggensee,
K. L.; Atkeson, C. G.; and Collins, S. H. 2017. Human-
in-the-loop optimization of exoskeleton assistance during
walking. Science,356(6344):1280–1284.
Ziebart,B.D.;Maas,A.L.;Bagnell,J.A.;Dey,A.K.;etal.
2008. Maximumentropyinversereinforcementlearning. In
Aaai,volume8,1433–1438.Chicago,IL,USA.