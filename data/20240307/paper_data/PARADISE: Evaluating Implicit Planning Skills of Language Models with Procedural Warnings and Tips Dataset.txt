PARADISE: Evaluating Implicit Planning Skills of
Language Models with Procedural Warnings and Tips Dataset
ArdaUzunog˘lu‡, AbdalfatahRashidSafa†, GözdeGülS¸ahin†
‡ComputerScienceDepartment,JohnsHopkinsUniversity,Maryland,USA
†ComputerEngineeringDepartment,KoçUniversity,Istanbul,Türkiye
†KUISAI,KoçUniversity,Istanbul,Türkiye
†https://gglab-ku.github.io/
Abstract
Recently,therehasbeengrowinginterestwithin
thecommunityregardingwhetherlargelanguage
models are capable of planning or executing
plans. However,mostpriorstudiesuseLLMsto
generatehigh-levelplansforsimplifiedscenarios
lackinglinguisticcomplexityanddomaindiver-
sity,limitinganalysisoftheirplanningabilities.
Thesesetupsconstrainevaluationmethods(e.g.,
predefined action space), architectural choices
(e.g.,onlygenerativemodels),andoverlookthe
linguisticnuancesessentialforrealisticanalysis.
Totacklethis,wepresentPARADISE,anabduc- Figure1: Aproceduraltutorialon“RemovingInkStains
tivereasoningtaskusingQ&Aformatonpracti- fromFabric”. Here,onecandamagethefabricifthey
calproceduraltextsourcedfromwikiHow. Itin- ignorethewarning“Alwaysblot,neverrub,whendealing
volveswarningandtipinferencetasksdirectlyas- withinkstains”.
sociatedwithgoals,excludingintermediarysteps,
withtheaimoftestingtheabilityofthemodelsto
inferimplicitknowledgeoftheplansolelyfrom 2023; Song et al., 2023) (i.e., agentic models) and
thegivengoal. Ourexperiments,utilizingfine-
analysis studies that investigate their planning ca-
tuned language models and zero-shot prompt-
pabilities(Valmeekametal.,2023;Pallaganietal.,
ing,revealtheeffectivenessoftask-specificsmall
2023; Valmeekam et al., 2022). Majority of these
modelsoverlargelanguagemodelsinmostsce-
studies use toy simulation environments like AL-
narios. Despite advancements, all models fall
shortofhumanperformance. Notably,ouranal- FRED (Shridhar et al., 2020), BlocksWorld, and
ysisuncoversintriguinginsights,suchasvaria- VirtualHome (Puig et al., 2018) which have little
tionsinmodelbehaviorwithdroppedkeywords, lexicalanddomainvarianceandlimitednumberof
strugglesofBERT-familyandGPT-4withphys-
actions(e.g.,verbs). Additionally,theplanningtask
ical and abstract goals, and the proposed tasks
ismostlyformulatedasagenerationproblemwhich
offeringvaluablepriorknowledgeforotherun-
canonlybeevaluatedontheclosedproblemdomain;
seenproceduraltasks. ThePARADISEdataset
andmodelswithdecoder-basedarchitectures. Hence
andassociatedresourcesarepubliclyavailable
for further research exploration with https:// evaluatingopen-domainplanningabilitiesforawide
github.com/GGLAB-KU/paradise. rangeofmodelsstillremainasachallenge.
Planningrequiresacombinationofawiderangeof
1 Introduction
complexreasoningabilities. Onelineofresearchfo-
Recentbreakthroughsinemergent(orlackof)abil- cusesondistinctsetofreasoningabilities(e.g.,com-
ities of large language models (LLMs) have given monsense (Huang et al., 2019), arithmetic (Cobbe
risetoempiricalstudiesthatemploylanguagemod- et al., 2021), logical (Han et al., 2022), tempo-
els as planners (Huang et al., 2022; Zhao et al., ral(WangandZhao,2023)etc...) oflanguagemodels
4202
raM
5
]LC.sc[
1v76130.3042:viXraonmorerealistic,open-domaintext. However,such that fine-tuning small models tailored to specific
open-domain text mostly does not contain a goal tasks proves more effective than zero-shot prompt-
or a plan, hence lack the complex linguistic phe- ing across all LLMs, including GPT-4. However,
nomenon (e.g., implicit relations, complex tempo- it’snoteworthythatallmodels,despitetheseefforts,
ral,andco-referencelinks,largeevent-causechains still lag behind human performance. Our exhaus-
etc...) that is common in procedural text. On the tive analysis also provide interesting insights such
otherhand,existingstudiesonopen-domainproce- aslargemodelsgettinglessaffectedfromdropping
duraltextaremostlylimitedtoextractingdirectand matched keywords; BERT-family struggling more
explicit relations (Dalvi et al., 2019; Zhang et al., with physical goals, while GPT-4 struggling with
2020b),i.e.,moresimplisticreasoningcomparedto abstract,digitalandsocialobjectives;andproposed
planning. Here,wereformulatetheproblemofeval- tasksprovidingbeneficialpriorknowledgetounseen
uatingplanningabilitiesasanabductivereasoning procedural tasks. We release all the resources at
task—reasoningwithmissinginformation. Wehy- https://github.com/GGLAB-KU/paradise.
pothesizethatamodelwithplanningabilitieswould
2 PARADISE
beabletoinferthewarningsandtipsabouttheplan
without seeing the explicit instructions (see Fig 1).
Initially, we augment the wikiHow corpus (Zhang
Althoughabductivereasoninghasbeenstudiedinthe
etal.,2020b)byintegratingitwitharecentcompi-
past,theyeitheremployadditionalsourceofinforma-
lation2 of21Ktutorials. Theextendedcorpusmain-
tion(Huangetal.,2019;Bhagavatulaetal.,2020)or
tainstheJSONformat,exceptforthesegregationof
focusonconsecutiveelements(Zellersetal.,2018,
warningsandtips,anexampleofwhichcanbeseen
2019;Tandonetal.,2019),bothofwhichdiminish
in Appendix A. Each wikiHow tutorial comprises
thenotionofimplicity.
proceduralstepstoachieveitsobjective,withsome
In this work, we present PARADISE, an exten-
tutorialsfeaturingstep-specificorgeneralwarnings
sive, expert-curated dataset for warning and tip in-
andtips. Weautomaticallygeneratedownstreamtask
ference tasks covering a wide range of domain de-
dataincorporatingthesewarningsandtips,aselab-
rived from wikiHow1. Unlike previous works, our
oratedinSec.2.2. Theprocessalsoinvolvesexpert
tasksfocusontheimplicitrelationshipbetweengoals
humanannotation,detailedinSec.2.3.
andwarnings/tips,bypassingintermediatesteps(in-
structions). This requires a model to possess im- 2.1 TaskFormulation
plicitknowledgeofintermediatesteps(i.e.,theplan)
Wedefinewarningandtipinferencetasksasmultiple-
solelybasedontheprovidedgoalintheabsenceof
choicequestionansweringtasks,inwhichasystem
explicit instructions. Furthermore, we use a ques-
needstochoosethecorrectwarningortipforagiven
tion answering formulation, which allows for eas-
goal among candidates. In this context, goals are
ier evaluation with standard metrics, and testing
questions, while warnings and tips are the choices.
of broader-range of model architectures including
AnexampleforbothtaskscanbeseeninFig.2.
encoder-based ones. We establish robust baselines
byfine-tuningpretrainedlanguagemodelslikeDe- 2.2 CandidateSampling
BERTa (He et al., 2020) and zero-shot prompting
Acquiring the goals and positive candidates is
with a varied set of large language models such as
straightforward, involving iterative selection from
Mistral-7B(Jiangetal.,2023)andGPT-4(OpenAI,
eachtutorialinourcorpus. Fornegativecandidate
2023b). Ourextensiveexperimentsaddressabroad
sampling,wemodifytheapproachoutlinedbyZhang
rangeofresearchquestions,delvingintotherelation-
etal.(2020b). Incontrasttotheirreliancesolelyon
shipbetweenmemorizationandperformance(§4.1),
verbs, we note that verbs prove inadequate in cap-
thedifferencesinfailuresbetweenPLMsandLLMs
turing meaning because warnings and tips, on av-
(§4.2), and the knowledge transfer capacity of the
erage, are much longer than individual steps (~40
proposedtaskstounseentasks (§4.5). Weobserve
versus ~11 tokens). This leads to the generation
1https://www.wikihow.com 2Scrapedate:November,2022CategoryDistribution Size
Other C&E HE F&E H&C H&G E&C F&B PC&S Train Dev Test
WikiHowCorpus 27.25% 14.51% 10.18% 10.09% 9.27% 8.89% 7.35% 6.63% 5.83% 133K
WarningInference 34.75% 7.09% 14.13% 7.26% 4.12% 14.02% 5.19% 4.92% 8.52% 33K 5K 500
TipInference 30.13% 7.45% 11.33% 10.61% 5.44% 12.11% 7.46% 5.29% 10.18% 71K 5K 500
Table1: CategorydistributionandsizeofPARADISE.C&E-ComputerandElectronics,HE-Health,F&E-Food
andEntertaining,H&C-HobbiesandCrafts,H&G-HomeandGarden,E&C-EducationandCommunications,F&B-
FinanceandBusiness,PC&S-PersonalCareandStyle.
1.Goal:SitupStraightataComputer siblecandidatesforagivengoal. Forinstance,con-
(a)Rememberthatpeoplecanseesomeof siderthegoal“DealwithYourStepMother”,which
yoursurroundingsyouwhileyouchat.Be
mindfulofwhatisinthecamera’sfieldofview. hasapositivecandidateof“Stayconnectedwithrela-
(b)Donotremaininanyonepositioninfrontof
tivessuchasgrandparentsandclosefriendsforextra
acomputerfortoolong.
(c)Avoidmovingaroundinthispose.Any support”andanegativecandidateof“Recruithelp
movementsyoumakewithintheposeshould
fromfriendsandfamily”. Althoughthenegativecan-
bedeliberateandslow.
(d)Keepanappropriatedistancebetweenyour didateischosenduetoitshighsemanticsimilarity
eyesandcomputerscreen.
with the positive candidate, it is also a reasonable
2.Goal:AvoidOilSplatterwhenFrying
choiceforthegivengoal,introducingnoiseintothe
(a)Remembertohavelotsofsides
apartfromjustthebarbecuedfood. dataset. Tomitigatesuchissues,weemployexpert
(b)Wearclear,plasticglovesifyouaregoing
annotationtovalidatethetestsplits.
touseyourhandstomixthemeat.
(c)Neveruseextravirginoliveoiltostir-fry. The expert annotation process consists of three
Ithasalowsmokingpoint.
(d)Wearlongsleeveswhenyouplanon stages. First,expertsverifythateachexamplecon-
fryingfood. tainsnomorethanoneplausiblecandidate. Second,
Figure2: Examplequestionsforwarning(1)andtip(2) theymeticulouslyexamineexamplestoensurethat
inferencetasks. Correctchoicesarebold. the positive candidate is genuinely relevant to the
contentoftheassociatedwikiHowtutorial. Finally,
experts assess the appropriateness of examples for
oflow-qualitynegativecandidates. Toaddressthis
gauging reasoning skills, excluding instances that
limitation,weenhanceournegativecandidatesam-
demandexpert-levelknowledgeordomain-specific
plingstrategybyincorporatingembeddingsofnoun
high-level information. This annotation process
tokens.
yields approximately 80% of annotations as valid
We begin by encoding each warning and tip us-
examples. Consequently,thetestsplitsforeachtask
ing BERT (Devlin et al., 2019). We calculate the
areexpandedwithsuchvalidexamplesfromthepool
average of verb and noun tokens, identified with
ofautomaticallygeneratedexamplesuntilreaching
spaCy (Honnibal et al., 2020). Subsequently, we
thepredeterminedsizeof500examples.
employ FAISS (Johnson et al., 2019) to conduct a
Apart from expert annotation, we leverage the
semanticsimilaritysearch,identifyingthetopthree
datasetcartographytool(Swayamdiptaetal.,2020)
warningsandtipswiththehighestcosinesimilarity
to uphold the high quality of our data, probe our
scorerelativetothepositivecandidate.
datasets, and gain a deeper understanding of their
FollowingZhangetal.(2020b),werandomlyre-
features. FurtherdetailscanbefoundinAppendixC.
assignoneofthenegativecandidatesaspositiveand
correctthelabelsandgoalsaccordinglywithaprob-
2.4 DatasetStatistics
abilityof0.15toavoidsamplingbiasandfilterthe
examplesasdescribedinAppendixB. The statistics of the corpus and final datasets are
giveninTable1. Wespecifythevalidationandtest
2.3 TestSetConstruction
splitsizesas5Kand500, respectively, withthere-
As our datasets are automatically generated, they maining data serving as the training set. Tip infer-
mayincludeundesiredelementslikemultipleplau- encedatasetisnearlytwicethesizeofthewarninginferencedataset, buttheaveragetokencountsper Model Warning Tip
Random 25.0 25.0
goalandcandidatearecomparable(~7forgoal,~40
Majority 26.0 26.0
for candidate). We employ a nearly uniform sam- PLMs
plingapproachacrossvariouscategoriestoensurea DistilBERT 22.44±3.88 21.48±4.84
BERT 25.52±4.56 26.88±5.02
highlevelofdomaindiversity. RoBERTa 20.88±4.80 20.36±4.20
DeBERTa 23.40±6.76 22.60±8.61
Fine-TunedPLMs
3 ExperimentalSetup
DistilBERT 82.16±0.79 87.48±0.65
BERT 83.92±0.68 89.80±0.91
Toevaluatelanguagemodelsinourtasks,weestab- RoBERTa 87.92±0.60 91.00±0.34
DeBERTa 90.68±0.41 93.68±0.48
lish two setups: 1) finetuning setup for pretrained
Open-SourceLLMs
encoderLMssuchasBERT(Devlinetal.,2019)and
Mistral7B 71.8 72.4
2)zero-shotsetupforlargelanguagemodelssuchas Vicuna33B 53.2 57.0
LLaMA-270B 65.2 64.5
GPT-4(OpenAI,2023b).
ProprietaryLLMs
PALM-2 83.6 82.4
3.1 FinetuningSetup GPT-4 86.2 88.8
Human 94.0 96.0
We fine-tune a set of models from the BERT fam-
ily: DistilBERT (Sanh et al., 2019), BERT (De- Table2: Mainaccuracyresultsforfine-tuningand
zero-shotsetups.
vlin et al., 2019), RoBERTa (Liu et al., 2019), De-
BERTa (He et al., 2020), which show strong per-
formance in procedural and reasoning tasks (Zhou
tofitthespecificmodel’stemplate. ForinstanceVi-
etal.,2022;Tandonetal.,2019;Zellersetal.,2019;
cunaexpectsacertaintemplatewith ###Human and
Zhang et al., 2020b). For fine-tuning, we concate-
###Assistance rolesspecifiedinthetext. Weuse
nateeachcandidate(warningortip)withtheques-
the respective model APIs, where available. Pre-
tion (goal) using a [CLS] token, i.e., the model
liminary tests on the subsets were conducted for
receives [CLS] question [SEP] candidate as
each model to identify optimal temperature and
input. Subsequently, we apply an additional pro-
top_p parameters. The best-performing configura-
jection layer, followed by a softmax function that
tions were then applied to the entire datasets for a
receivestherepresentationofthe [CLS] tokenfor
thoroughevaluation. Furtherdetailsontheprompt
each candidate. The candidate with the highest
templatesandparametersaregiveninAppendixE.
probability is selected as the answer. The models
are optimized through cross-entropy loss. Further
4 ExperimentsandResults
implementationdetailscanbefoundinAppendixD.
WeexperimentwiththePLMsandLLMson PAR-
3.2 Zero-ShotSetup
ADISEusingthesetupexplainedinSec3. Wealso
Weidentifyfivepopularandcapable3 largelanguage calculate the random and majority baselines, and
modelsthatarediverseinarchitecture,scale,avala- evaluatethehumanperformancebyaveragingtheac-
bility,andperformance,namelyasGPT-44 (OpenAI, curacyoftwohumanannotators7 onarandomsetof
2023b), PALM-2 5 (Anil et al., 2023), LLaMA-2 100examples. OurmainresultsaregiveninTable2.
70B(Touvronetal.,2023),Mistral7B6 (Jiangetal.,
Fine-tuning Among the fine-tuned models, De-
2023),andVicuna33B(Chiangetal.,2023).
BERTa performs the best in both tasks; yet, it still
We first perform preliminary experiments with
falls behind human performance. Considering De-
default prompts on a small subset of the valida-
BERTa’s performance in previous abductive rea-
tion set. We, then, iteratively refine the prompts
soning datasets, such as CosmosQA (Huang et al.,
3TheseLLMsarechosenfromthemodelsthatrankhighin 2019),SWAG(Zellersetal.,2018),andHellaSWAG
theHuggingFaceChatbotLeaderboard. (Zellers et al., 2019), such results indicate that its
4Modelvariant:gpt-4-1106-preview
5Modelvariant:text-bison 7Twouniversitystudentsmajoringincomputerscience,be-
6Modelvariant:Mistral-7B-Instruct-v0.1 tweentheagesof20-24.success is not task-specific and its performance is Thismanipulation,averagingapproximately2to-
transferable to warning and tip inference. Distil- kens per candidate, induces a 4.5% change on av-
BERT,BERT,andRoBERTacannotperformonpar erage. As illustrated in Fig. 3, the omission of
with DeBERTa and fall well behind human perfor- suchkeywordsresultsina15%to20%decreasein
mance,althoughtheyperformconsiderablywellon predictionaccuracyforPre-trainedLanguageMod-
theproposedtasks. Allmodelsperformbetterintip els(PLMs). Theimpactdiminisheswithincreasing
inferencecomparedtowarninginference. originalmodelperformance;DistilBERTismostaf-
fected,whileDeBERTaisleastaffected. Incontrast,
Zero-shot Althoughtheirperformancesareworse
LanguageModels(LLMs)experienceamilderaccu-
than fine-tuned PLMs, proprietary LLMs perform
racydeclineof5%to15%. Inadditiontodropping
considerablybetterthanopen-sourceLLMs. GPT-4,
keywords, wealsoexperimentwithotherkeyword
whichalsotopsmanybenchmarks(OpenAI,2023b),
manipulationmethodsasdetailedinAppendixF.
is the best-performing LLM, while PALM-2 is a
close runner-up. One surprising finding is that the 4.2 RQ2: FailuresofDifferentModelFamilies
performances of open-source LLMs are not corre-
To better understand the behaviours of the models
latedwiththeirsize,asMistral7BoutperformsVi-
weexperimentwith,wegeneratecorrelationmatri-
cuna33BandLLaMA-270Binbothtasks. Similar
cesfortheirincorrectpredictionsinbothtasks. As
toPLMs,LLMsalsoperformbetterintipinference
depicted in Fig. 4, models within the same group
comparedtowarninginference,withLLaMA-270B
(PLMs,Open-SourceLLMs,andProprietaryLLMs)
beinganexception.
exhibit the highest correlation. Notably, incorrect
Togainfurtherinsightsonbehavioursofthemod-
predictionsdivergemorebetweenPLMsandLLMs,
elsweaskthefollowingresearchquestions:
whileopen-sourceLLMsandproprietaryLLMsdis-
• RQ1: Dothemodelsperformwellduetosim- playhigherinter-groupcorrelation. Task-wise,cor-
plekeywordmatching? relationlevelsremainconsistent,underscoringdata
qualityandoverallconsistency.
• RQ2: Do different model families fail on dif-
As evident in Fig. 4, DeBERTa and GPT-4 ex-
ferentinstances? Isthereacertainpattern?
hibitdivergentfailurepatterns. Tounderstandtheir
• RQ3: Howdoestheperformancecomparefor distinctionsindistinguishingPLMsandLLMs,we
theexplicit(i.e.,directlyrelatedtoastep)and manuallyinspectinstancesoffailure. Ouranalysis
implicit warnings/tips (i.e., more general and revealsthatDeBERTastrugglesmorewithtangible,
notdirectlyrelatedtoanyofthesteps)? physical, and craft-related goals, while GPT-4 en-
counterschallengeswithabstract,digital,andsocial
• RQ4: Arethemodelsalsogoodatthereverse
objectives. Wevalidatethesefindingsbygenerating
task,i.e.,cantheyfindthegoalmostrelatedto
the category distribution of unique failures for De-
thewarning/tip?
BERTa and GPT-4, as shown in Table 3. Notably,
GPT-4 falters in social and digital categories like
• RQ5: Can the proposed tasks help improve
Youth,Relationships,andComputer&Electronics,
performanceinotherproceduraltasks?
while DeBERTa encounters difficulties in tangible
4.1 RQ1: KeywordMatching
categoriessuchasSports&Fitness,Pets&Animals,
Ifthegoalandonlyonecandidateshareacommon andHome&Garden. Specificinstancesoffailures
keyword,anexamplemightbecometrivialandthe for DeBERTa and GPT-4 are detailed in Appendix
task might develop into simple keyword matching. G.
For example, when the goal and the positive can-
4.3 RQ3: ImplicitversusExplicit
didateareaboutcatswhilenegativecandidatesare
aboutotheranimals,thepositivecandidatebecomes AsoutlinedinSec.2,warningsandtipswithinthe
easilydistinguishable. Therefore,wedropsuchkey- dataset exhibit a distinction: some are specific to
wordsinbothpositiveandnegativecandidatesinor- individualsteps,whileothersaregeneral. Although
dertoevaluateourtasks’dependencyonkeywords. they are related to steps from the wikiHow tutori-WarningInference TipInference
1 1
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
Original
Drop
0 0
DistilBERT BERT RoBERTa DeBERTa Mistral7BVicuna33BLLaMA-270BPALM-2 GPT-4 DistilBERT BERT RoBERTa DeBERTa Mistral7BVicuna33BLLaMA-270BPALM-2 GPT-4
Figure3: Modelperformancestestedonmanipulatedtestdata.
Figure4: Correlationmatricesofincorrectpredictionsofeachmodelfortip(left)andwarning(right)inference.
Rank DeBERTa GPT-4 We assess the performance of PLMs and LLMs,
Warning Tip Warning Tip
asdetailedinSec.3,onsubsetsoutlinedinTable4.
#1 H&G(24.5%) F&E(25.0%) H&G(24.4%) HE(18.1%)
#2 S&F(12.8%) H&G(21.9%) HE(16.3%) F&E(13.4%) Results show improved model performance as the
#3 E&C(11.7%) HE(15.6%) C&E(13.0%) C&E(10.7%)
#4 PC&S(11.7%) E&C(10.9%) RE(9.8%) YO(8.7%) similaritybetweenwarnings/tipsandstepsincreases.
#5 P&A(8.5%) P&A(9.4%) PC&S(8.9%) PC&S(8.7%)
Higher accuracy in these subsets suggests a strong
Table3: TopcategoriesthatDeBERTaandGPT-4fail. capacityforimplicitreasoning,giventhatwarnings
and tips become more representative of intermedi-
atestepswithincreasedsimilarity. Notably,BERT
and DeBERTa excel among PLMs, while Mistral
als,step-specificwarningsandtipsarenotmatched
7BandLLaMA-270BleadamongLLMs,exhibit-
withtheirassociatedstepsmanuallybyeditors. To
ingthehighestaccuracyincreaseinwarningandtip
assesstheimplicitreasoningskillsoflanguagemod-
inferencetasks,respectively.
els,wecuratedistinctsubsetsoftestsplitsforboth
tasks,comprisingwarningsandtipsdemonstrating
4.4 RQ4: ReverseInferenceTasks
high semantic similarity with steps from relevant
wikiHow tutorials. Employing SBERT (Reimers Ifmodelscaneffectivelyreasonabouttherelation-
andGurevych,2019)forencodingsteps andwarn- ship between warnings/tips and goals, it suggests
ings/tips, we conduct a semantic similarity search. they can correctly identify the goal corresponding
We retain examples with a cosine similarity score to a given warning or tip. To examine this hypoth-
surpassingathresholdwithatleastonestep,result- esis, we construct reversed versions of our tasks,
ingin35warningsand52tipswithhighsimilarity requiringthesystemtoselectthecorrectgoalfora
scores(cosinesimilarityscorewiththestep>0.75) provided warning or tip. Using the candidate sam-
and368warningsand382tipswithdecentsimilarity plingmethoddetailedinSec2.2,werandomlyselect
scores(cosinesimilarityscorewiththestep>0.5). 500examplesforevaluationwithfine-tunedPLMs
ycaruccAWarning Tip
Model All Sim>0.5 Sim>0.75 All Sim>0.5 Sim>0.75
Fine-TunedPLMs
DistilBERT 82.16±0.79 87.12±0.66 85.71±0.00 87.48±0.65 88.43±0.67 90.02±1.46
BERT 83.92±0.68 86.84±0.59 90.29±2.29 89.80±0.91 91.10±0.89 92.69±0.77
RoBERTa 87.92±0.60 89.57±0.33 92.57±1.40 91.00±0.34 91.83±0.26 91.93±0.77
DeBERTa 90.68±0.41 91.63±0.58 94.86±2.14 93.68±0.48 95.34±0.39 100.0±0.0
Open-SourceLLMs
Mistral7B 71.8 64.4 70.3 72.4 72.8 73.6
Vicuna33B 53.2 54.1 59.5 57.0 58.9 66.0
LLaMA-270B 65.2 58.4 62.2 64.5 70.9 77.4
ProprietaryLLMs
PALM-2 83.6 83.2 86.5 82.4 82.8 84.9
GPT-4 86.2 86.1 89.2 88.8 88.7 92.5
Table4: TheaccuracyresultsofPLMsandLLMsondifferentsubsetsofthetestsplitswithvaryinglevelofsimilarity
totheinstructionsfromassociatedwikiHowtutorials.
Model ReverseWarning ReverseTip Model Warning Tip
Random 25.0 25.0 Random 25.0 25.0
PLMs BERT 84.68±0.27 86.20±0.77
RoBERTa 88.28±0.30 88.80±0.72
DistilBERT 20.52±3.93 20.00±5.27
BERT 25.08±3.73 25.40±4.93
RoBERTa 26.52±5.42 26.36±7.03 Table6: AccuracyresultsofBERTandRoBERta
DeBERTa 33.76±4.83 35.72±4.99
fine-tunedwithtipinferenceonwarninginference,and
Fine-TunedPLMs
viceversa.
DistilBERT 65.44±1.53 68.48±1.25
BERT 69.08±1.69 72.36±1.18
RoBERTa 72.64±1.44 77.00±1.26
DeBERTa 79.92±0.63 80.44±0.79
Open-SourceLLMs 4.5.1 CrossDomain
Mistral7B 74.6 79.2
Whilecategorizedseparately,bothwarningsandtips
Vicuna33B 62.8 61.4
LLaMA-270B 76.2 76.0 share the common objective of enhancing reader
ProprietaryLLMs understanding in a wikiHow tutorial. As a result,
PALM-2 83.6 85.8
theyoftenexhibitsimilaritiesinstructureandseman-
GPT-4 86.4 86.0
tics, with occasional interchangeability. To assess
Table5: Accuracyresultsofthereversetaskevaluation.
thegeneralizabilityofPre-trainedLanguageModels
(PLMs)acrosswarningandtipinferencetasks,we
conductcross-tests. Specifically,weevaluatePLMs
andLLMs. ResultsinTable5indicatezero-shotper-
fine-tunedontipinferencedatausingthetestsplitof
formancesforPLMsaligncloselywiththerandom
thewarninginferencedatasetandviceversa.
baseline,exceptforDeBERTa,whichachievesa10%
AsdepictedinTable6,modelsfine-tunedontip
higheraccuracy. Thissuggestslimitedinherentrea-
inferencedatademonstratecomparableperformance
soningcapabilitiesoverproceduralwarningsandtips.
to those fine-tuned on warning inference data for
However,fine-tunedmodelsexhibitasignificantper-
the warning inference task. Conversely, models
formance increase, supporting our hypothesis. In
fine-tunedonwarninginferencedataexhibitslightly
contrast,LLMsmaintainsimilaraccuracyscoresin
lower performance than those fine-tuned on tip in-
reverse tasks without experiencing a performance
ference data for the tip inference task. The nearly
lossobservedinfine-tunedPLMs.
identical results in cross tests affirm the high simi-
laritybetweenwarningsandtips,highlightingtheir
4.5 RQ5: TransferLearning
interchangeability.
To examine the impact of our tasks on reasoning
4.5.2 Out-of-Domain
overproceduraldocuments,weconductcross-tests
betweenwarningandtipinferencetasksandout-of- The goal and step inference tasks focus on identi-
domaintransferlearningongoalandstepinference fying goal-step relationships in procedural how-to
tasksfromZhangetal.(2020b). tutorials. Goalinferenceinvolvesselectingtheplau-sible goal from candidates for a given step, while are outside of the domain of procedural language
stepinferenceentailsthereverseprocess. Although understanding. For example, CosmosQA (Huang
similartotheproposedtasks,theyaimtomeasuring etal.,2019)presentcommonsenseabductivereading
explicitproceduralreasoningabilities. comprehensionandART(Bhagavatulaetal.,2020)
For both step and goal inference tasks, we fine- propose abductive natural language inference with
tune three BERT models: i) BERT trained from narrativecontexts. Thus,theydependonadditional
scratch,ii)BERTpreviouslyfine-tunedonwarning paragraphsprovidedbythequestion,whichenrich
inferencedata,andiii)BERTpreviouslyfine-tuned thelevelofinformationprovidedtothemodel. Fur-
ontipinferencedata. Wereporttheirperformances thermore, abductive reasoning resources obtained
on the test split throughout the training. Notably, fromproceduraltextsareeitherartificiallymadedif-
priorfine-tuningonwarningandtipinferencetasks ficultwithtargetedmodels(suchasSWAG(Zellers
consistentlyimprovesperformanceduringtraining etal.,2018)andHellaSWAG(Zellersetal.,2019))
forbothgoalandstepinferencetasks,asillustrated orcoversanotherformofproceduraltexts(e.g.,nat-
inFig. 5. ThesecondandthirdBERTmodelsexhibit uralphenomenonsofWIQA(Tandonetal.,2019)).
significantlyenhancedzero-shotperformances,with Moreover, they focus on the continuous elements
average accuracy increases of 21.16% and 22.98% (i.e. consecutivestepsorevents),whichwebelieve
forgoalinference,and34.75%and39.77%forstep diminishesthedegreeofimplicity.
inference,respectively. Whiletheperformancegap FurthermoreourmainresourcewikiHowhasbeen
diminishes,thesecondandthirdBERTmodelscon- extensively used for a wide range of tasks thanks
tinuetooutperformattheendoftraining,withaver- toitsrichbodyofwell-structuredproceduraldocu-
ageaccuracyincreasesof2.09%and2.27%forgoal ments, including but not limited to summarization
inference,and0.15%and0.53%forstepinference, (Koupaee and Wang, 2018; Ladhak et al., 2020),
respectively. intent detection (Zhang et al., 2020a), reasoning
(Zhang et al., 2020b), linking actions (Lin et al.,
GoalInference StepInference
90 90 2020;Zhouetal.,2022),andnexteventprediction
BERT-Scratch
BERT-Tip (Nguyenetal.,2017;Zellersetal.,2018,2019).
85 BERT-Warning 85
80 80
6 Conclusion
75 75
700 0.2 0.4 0.6 0.8 1700 0.2 0.4 0.6 0.8 1 Therehasbeenagrowinginterestinproceduraldata,
Epoch Epoch
tasks,andreasoning. However,thespotlighthasbeen
Figure5: AccuracyofthethreeBERTvariationsplotted onexplicitanddirectrelationswhenstudyingreason-
overthetrainingepoch. ingwithinproceduraldocuments. Toaddressthethe
lackofresourcestostudyimplicitrelationsandrea-
soning,weintroducePARADISEandstrongbaseline
5 RelatedWork
modelsevaluatedandanalyzedwithextensiveexper-
Commonsense reasoning is a broad domain that iments. PARADISEcontains+104Kwarningsandtips
branches into a wide range of subdomains such as intotalandservesasareliabletestbedfortheevalua-
linguistic reasoning (S¸ahin et al., 2020; Liu et al., tionofabductiveandimplicitcommonsensereason-
2022;Linetal.,2021),abductivereasoning(Tandon ing skills of language models. Moreover, it brings
et al., 2019; Zellers et al., 2019), reasoning about improvementtozero-and-few-shotperformancesin
the physical world (Bisk et al., 2020; Khot et al., out-of-domain procedural tasks. Our experiments
2019; Aroca-Ouellette et al., 2021), temporal rea- revealthatPLMsdonotpossessinherentreasoning
soning (Zhang et al., 2020b; Qin et al., 2021), etc. skills; yet, most of them outperform LLMs when
(Bhargava and Ng, 2022). Although there exists fine-tuned. However,best-performingmodelsfrom
some abductive reasoning tasks that involve find- both groups fall behind human performance, indi-
ing the most likely explanation for a set of incom- cating room for improvement. We release all the
plete observations (Bhargava and Ng, 2022), they resourcespubliclytofurtherresearch.
ycaruccALimitations land,AndreaHu,JeffreyHui,JeremyHurwitz,Michael
Isard,AbeIttycheriah,MatthewJagielski,WenhaoJia,
WeevaluateLLMswiththeirrespectiveAPIsdueto KathleenKenealy,MaximKrikun,SnehaKudugunta,
theirproprietarynatureorheavycomputationcosts. Chang Lan, Katherine Lee, Benjamin Lee, Eric Li,
Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek
Therefore,althoughweexplainourevaluationsetup
Lim,HanzhaoLin,ZhongtaoLiu,FrederickLiu,Mar-
in detail, the performances of LLMs we evaluate
cello Maggioni, Aroma Mahendru, Joshua Maynez,
might not always be reproducible due to potential Vedant Misra, Maysam Moussalem, Zachary Nado,
futurechangesordeprecationsoftheirAPIs. JohnNham,EricNi,AndrewNystrom,AliciaParrish,
Marie Pellat, Martin Polacek, Alex Polozov, Reiner
EthicsStatement Pope,SiyuanQiao,EmilyReif,BryanRichter,Parker
Riley,AlexCastroRos,AurkoRoy,BrennanSaeta,Ra-
We utilize the content from wikiHow, adhering to jkumarSamuel,ReneeShelby,AmbroseSlone,Daniel
Smilkov, David R. So, Daniel Sohn, Simon Toku-
the specific circumstances outlined in the Creative
mine,DashaValter,VijayVasudevan,KiranVodrahalli,
Commonslicense. Wefullycomplywithallcondi-
XuezhiWang,PidongWang,ZiruiWang,TaoWang,
tions stipulated by the Creative Commons license, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu,
and these requirements facilitate the utilization of LintingXue,PengchengYin,JiahuiYu,QiaoZhang,
thewikiHowcorpusuponwhichwebuild. StevenZheng,CeZheng,WeikangZhou,DennyZhou,
SlavPetrov,andYonghuiWu.2023. Palm2technical
report.
Acknowledgements
StéphaneAroca-Ouellette, CoryPaik, AlessandroRon-
This work has been supported by the Scien-
cone, and Katharina Kann. 2021. PROST: Physical
tific and Technological Research Council of
reasoning about objects through space and time. In
Türkiye (TÜB˙ITAK) as part of the project “Auto- FindingsoftheAssociationforComputationalLinguis-
maticLearningofProceduralLanguagefromNatu- tics: ACL-IJCNLP2021,pages4597–4608.Associa-
tionforComputationalLinguistics.
ralLanguageInstructionsforIntelligentAssistance”
with the number 121C132. We also gratefully ac-
Chandra Bhagavatula, Ronan Le Bras, Chaitanya
knowledge KUIS AI Lab for providing computa- Malaviya,KeisukeSakaguchi,AriHoltzman,Hannah
tionalsupport. Wethankouranonymousreviewers Rashkin,DougDowney,WentauYih,andYejinChoi.
2020. Abductivecommonsensereasoning. InInterna-
andthemembersofGGLabwhohelpedusimprove
tionalConferenceonLearningRepresentations.
this paper. We especially thank Aysha Gurbanova,
S¸ebnemDemirtas¸, andMahmut˙IbrahimDenizfor PrajjwalBhargavaandVincentNg.2022. Commonsense
theircontributionstoevaluatinghumanperformance knowledgereasoningandgenerationwithpre-trained
languagemodels: Asurvey. InAAAIConferenceon
onwarningandtipinferencetasks.
ArtificialIntelligence.
StevenBird,EwanKlein,andEdwardLoper.2009. Nat-
References urallanguageprocessingwithPython: analyzingtext
withthenaturallanguagetoolkit. "O’ReillyMedia,
RohanAnil,AndrewM.Dai,OrhanFirat,MelvinJohn-
Inc.".
son,DmitryLepikhin,AlexandrePassos,SiamakShak-
eri,EmanuelTaropa,PaigeBailey,ZhifengChen,Eric YonatanBisk,RowanZellers,RonanLeBras,Jianfeng
Chu,JonathanH.Clark,LaurentElShafey,Yanping Gao, and Yejin Choi. 2020. Piqa: Reasoning about
Huang,KathyMeier-Hellstern,GauravMishra,Erica physicalcommonsenseinnaturallanguage. InThirty-
Moreira,MarkOmernick,KevinRobinson,Sebastian FourthAAAIConferenceonArtificialIntelligence.
Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing
Zhang,GustavoHernandezAbrego,JunwhanAhn,Ja- TonyF.Chan,GeneH.Golub,andRandallJ.LeVeque.
cobAustin,PaulBarham,JanBotha,JamesBradbury, 1979. Updatingformulaeandapairwisealgorithmfor
SiddharthaBrahma, KevinBrooks, MicheleCatasta, computingsamplevariances. Technicalreport,Stan-
YongCheng,ColinCherry,ChristopherA.Choquette- ford,CA,USA.
Choo,AakankshaChowdhery,ClémentCrepy,Shachi
Dave,MostafaDehghani,SunipaDev,JacobDevlin, Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,Zhang-
MarkDíaz,NanDu,EthanDyer,VladFeinberg,Fangx- haoWu,HaoZhang,LianminZheng,SiyuanZhuang,
iaoyuFeng,VladFienber,MarkusFreitag,XavierGar- YonghaoZhuang,JosephE.Gonzalez,IonStoica,and
cia,SebastianGehrmann,LucasGonzalez,GuyGur- EricP.Xing.2023. Vicuna: Anopen-sourcechatbot
Ari,StevenHand,HadiHashemi,LeHou,JoshuaHow- impressinggpt-4with90%*chatgptquality.KarlCobbe,VineetKosaraju,MohammadBavarian,Mark Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-
Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plap- sch,ChrisBamford,DevendraSinghChaplot,Diego
pert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, delasCasas,FlorianBressand,GiannaLengyel,Guil-
ChristopherHesse,andJohnSchulman.2021. Train- laumeLample,LucileSaulnier,LélioRenardLavaud,
ing verifiers to solve math word problems. CoRR, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,
abs/2110.14168. ThibautLavril,ThomasWang,TimothéeLacroix,and
WilliamElSayed.2023. Mistral7b.
Bhavana Dalvi, Niket Tandon, Antoine Bosselut, Wen-
tauYih, andPeterClark.2019. Everythinghappens Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.
for a reason: Discovering the purpose of actions in Billion-scalesimilaritysearchwithGPUs. IEEETrans-
proceduraltext. InProceedingsofthe2019Conference actionsonBigData,7(3):535–547.
onEmpiricalMethodsinNaturalLanguageProcessing
andthe9thInternationalJointConferenceonNatural TusharKhot,PeterClark,MichalGuerquin,PeterAlexan-
LanguageProcessing(EMNLP-IJCNLP),pages4496– der Jansen, and Ashish Sabharwal. 2019. Qasc: A
datasetforquestionansweringviasentencecomposi-
4505.AssociationforComputationalLinguistics.
tion. ArXiv,abs/1910.11473.
JacobDevlin,Ming-WeiChang,KentonLee,andKristina
Toutanova.2019. BERT:Pre-trainingofdeepbidirec- MahnazKoupaeeandWilliamYangWang.2018. Wiki-
tionaltransformersforlanguageunderstanding. InPro- how: Alargescaletextsummarizationdataset. ArXiv,
ceedingsofthe2019ConferenceoftheNorthAmerican abs/1810.09305.
ChapteroftheAssociationforComputationalLinguis-
Faisal Ladhak, Esin Durmus, Claire Cardie, and Kath-
tics: HumanLanguageTechnologies,Volume1(Long
leenMcKeown.2020. WikiLingua: Anewbenchmark
andShortPapers),pages4171–4186.Associationfor
datasetforcross-lingualabstractivesummarization. In
ComputationalLinguistics.
FindingsoftheAssociationforComputationalLinguis-
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. tics: EMNLP2020,pages4034–4048.Associationfor
SimCSE:Simplecontrastivelearningofsentenceem- ComputationalLinguistics.
beddings. InProceedingsofthe2021Conferenceon
AngelaLin,SudhaRao,AsliCelikyilmaz,ElnazNouri,
EmpiricalMethodsinNaturalLanguageProcessing,
ChrisBrockett,DebadeeptaDey,andBillDolan.2020.
pages6894–6910.AssociationforComputationalLin-
Arecipeforcreatingmultimodalaligneddatasetsfor
guistics.
sequential tasks. In Proceedings of the 58th Annual
SimengHan,HaileySchoelkopf,YilunZhao,Zhenting MeetingoftheAssociationforComputationalLinguis-
Qi,MartinRiddell,LukeBenson,LucySun,Ekaterina tics,pages4871–4884.AssociationforComputational
Zubova,YujieQiao,MatthewBurtell,etal.2022. Fo- Linguistics.
lio: Naturallanguagereasoningwithfirst-orderlogic.
Bill Yuchen Lin, Ziyi Wu, Yichi Yang, Dong-Ho Lee,
arXivpreprintarXiv:2209.00840.
andXiangRen.2021. RiddleSense: Reasoningabout
PengchengHe,XiaodongLiu,JianfengGao,andWeizhu riddlequestionsfeaturinglinguisticcreativityandcom-
Chen.2020. Deberta: Decoding-enhancedbertwith monsenseknowledge. InFindingsoftheAssociation
disentangledattention. CoRR,abs/2006.03654. for Computational Linguistics: ACL-IJCNLP 2021,
pages1504–1515.AssociationforComputationalLin-
MatthewHonnibal,InesMontani,SofieVanLandeghem, guistics.
andAdrianeBoyd.2020. spaCy: Industrial-strength
NaturalLanguageProcessinginPython. EmmyLiu,ChenxuanCui,KennethZheng,andGraham
Neubig.2022. Testingtheabilityoflanguagemodels
LifuHuang,RonanLeBras,ChandraBhagavatula,and tointerpretfigurativelanguage. InProceedingsofthe
YejinChoi.2019. CosmosQA:Machinereadingcom- 2022 Conference of the North American Chapter of
prehensionwithcontextualcommonsensereasoning. theAssociationforComputationalLinguistics:Human
InProceedingsofthe2019ConferenceonEmpirical
LanguageTechnologies,pages4437–4452.Association
MethodsinNaturalLanguageProcessingandthe9th
forComputationalLinguistics.
InternationalJointConferenceonNaturalLanguage
Processing(EMNLP-IJCNLP),pages2391–2401.As- YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Mandar
sociationforComputationalLinguistics. Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke
Zettlemoyer,andVeselinStoyanov.2019. Roberta: A
WenlongHuang,PieterAbbeel,DeepakPathak,andIgor
robustlyoptimizedbertpretrainingapproach. ArXiv,
Mordatch.2022. Languagemodelsaszero-shotplan-
abs/1907.11692.
ners: Extractingactionableknowledgeforembodied
agents. InInternationalConferenceonMachineLearn- George A. Miller. 1994. WordNet: A lexical database
ing, ICML 2022, 17-23 July 2022, Baltimore, Mary- for English. In Human Language Technology: Pro-
land, USA, volume 162 of Proceedings of Machine ceedingsofaWorkshopheldatPlainsboro,NewJersey,
LearningResearch,pages9118–9147.PMLR. March8-11,1994.DaiQuocNguyen,DatQuocNguyen,CuongXuanChu, ChanHeeSong,BrianM.Sadler,JiamanWu,Wei-Lun
StefanThater,andManfredPinkal.2017. Sequenceto Chao, Clayton Washington, and Yu Su. 2023. Llm-
sequencelearningforeventprediction. InProceedings planner: Few-shot grounded planning for embodied
oftheEighthInternationalJointConferenceonNat- agentswithlargelanguagemodels. InIEEE/CVFInter-
uralLanguageProcessing(Volume2: ShortPapers), nationalConferenceonComputerVision,ICCV2023,
pages37–42.AsianFederationofNaturalLanguage Paris, France, October1-6, 2023, pages2986–2997.
Processing. IEEE.
OpenAI.2023a. Apireference-openaiapi. SwabhaSwayamdipta,RoySchwartz,NicholasLourie,
YizhongWang,HannanehHajishirzi,NoahA.Smith,
OpenAI.2023b. Gpt-4technicalreport.
andYejinChoi.2020. Datasetcartography: Mapping
Vishal Pallagani, Bharath Muppasani, Keerthiram Mu- anddiagnosingdatasetswithtrainingdynamics. InPro-
ceedingsofthe2020ConferenceonEmpiricalMeth-
rugesan, Francesca Rossi, Biplav Srivastava, Lior
Horesh, Francesco Fabiano, and Andrea Loreggia.
odsinNaturalLanguageProcessing(EMNLP),pages
2023. Understanding the capabilities of large lan- 9275–9293. Association for Computational Linguis-
guage models for automated planning. CoRR, tics.
abs/2305.16151.
GuggerSylvain,DebutLysandre,WolfThomas,Schmid
XavierPuig,KevinRa,MarkoBoben,JiamanLi,Tingwu Philipp, Mueller Zachary, and Mangrulkar Sourab.
Wang,SanjaFidler,andAntonioTorralba.2018. Virtu- 2022. Accelerate:Trainingandinferenceatscalemade
alhome: Simulatinghouseholdactivitiesviaprograms. simple, efficient and adaptable. https://github.
In2018IEEEConferenceonComputerVisionandPat- com/huggingface/accelerate.
ternRecognition,CVPR2018,SaltLakeCity,UT,USA,
NiketTandon,BhavanaDalvi,KeisukeSakaguchi,Peter
June18-22,2018,pages8494–8502.ComputerVision
Clark,andAntoineBosselut.2019. WIQA:Adataset
Foundation/IEEEComputerSociety.
for“whatif...”reasoningoverproceduraltext. InPro-
LianhuiQin, AdityaGupta, ShyamUpadhyay, Luheng ceedingsofthe2019ConferenceonEmpiricalMethods
He, Yejin Choi, and Manaal Faruqui. 2021. TIME- inNaturalLanguageProcessingandthe9thInterna-
DIAL: Temporal commonsense reasoning in dialog. tionalJointConferenceonNaturalLanguageProcess-
InProceedingsofthe59thAnnualMeetingoftheAs- ing(EMNLP-IJCNLP),pages6076–6085.Association
sociationforComputationalLinguisticsandthe11th forComputationalLinguistics.
InternationalJointConferenceonNaturalLanguage
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
Processing(Volume1:LongPapers),pages7066–7076.
bert,AmjadAlmahairi,YasmineBabaei,NikolayBash-
AssociationforComputationalLinguistics.
lykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhos-
NilsReimersandIrynaGurevych.2019. Sentence-BERT: ale,etal.2023. Llama2: Openfoundationandfine-
SentenceembeddingsusingSiameseBERT-networks. tunedchatmodels. arXivpreprintarXiv:2307.09288.
InProceedingsofthe2019ConferenceonEmpirical
MethodsinNaturalLanguageProcessingandthe9th Karthik Valmeekam, Alberto Olmo Hernandez, Sarath
InternationalJointConferenceonNaturalLanguage Sreedharan,andSubbaraoKambhampati.2022. Large
Processing(EMNLP-IJCNLP),pages3982–3992.As- language models still can’t plan (A benchmark for
sociationforComputationalLinguistics. llmsonplanningandreasoningaboutchange). CoRR,
abs/2206.10498.
GözdeGülS¸ahin,ClaraVania,IliaKuznetsov,andIryna
Gurevych.2020. LINSPECTOR:Multilingualprobing Karthik Valmeekam, Matthew Marquez, Sarath Sreed-
tasksforwordrepresentations. ComputationalLinguis- haran, and Subbarao Kambhampati. 2023. On the
tics,46(2):335–385. planningabilitiesoflargelanguagemodels-Acritical
investigation. CoRR,abs/2305.15771.
Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. 2019. Distilbert, a distilled version Yuqing Wang and Yun Zhao. 2023. TRAM: bench-
of bert: smaller, faster, cheaper and lighter. ArXiv, markingtemporalreasoningforlargelanguagemodels.
abs/1910.01108. CoRR,abs/2310.00835.
MohitShridhar,JesseThomason,DanielGordon,Yonatan ThomasWolf,LysandreDebut,VictorSanh,JulienChau-
Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettle- mond, Clement Delangue, Anthony Moi, Perric Cis-
moyer, and Dieter Fox. 2020. ALFRED: A bench- tac, Clara Ma, Yacine Jernite, Julien Plu, Canwen
markforinterpretinggroundedinstructionsforevery- Xu,TevenLeScao,SylvainGugger,MariamaDrame,
daytasks. In2020IEEE/CVFConferenceonComputer QuentinLhoest,andAlexanderM.Rush.2020. Trans-
VisionandPatternRecognition,CVPR2020,Seattle, formers: State-of-the-ArtNaturalLanguageProcess-
WA,USA,June13-19,2020,pages10737–10746.Com- ing. pages38–45.AssociationforComputationalLin-
puterVisionFoundation/IEEE. guistics.RowanZellers,YonatanBisk,RoySchwartz,andYejin10 "author": {
Choi.2018. SWAG:Alarge-scaleadversarialdataset11 "name": null,
12 "n_coauthors": 128,
forgroundedcommonsenseinference. InProceedings13 "blurb": null
ofthe2018ConferenceonEmpiricalMethodsinNatu-14 },
15 "time_updated": "April 12, 2020",
ralLanguageProcessing,pages93–104.Association16 "n_views": 3443113,
17 "rating": {
forComputationalLinguistics.
18 "n_votes": 76,
19 "helpful_percent": 77
RowanZellers,AriHoltzman,YonatanBisk,AliFarhadi,20 },
21 "methods": [],
and Yejin Choi. 2019. HellaSwag: Can a machine22 "parts": [],
23 "video": "https://www.wikihow.com/Video/Rip-Your-Own-Jeans
really finish your sentence? In Proceedings of the
",
57thAnnualMeetingoftheAssociationforComputa-24 "related_articles": ["ABRIDGED"],
25 "tips": [
tionalLinguistics,pages4791–4800.Associationfor26 "Washing the jeans right after ripping them causes the
ComputationalLinguistics. fibers to loosen more and create a more
distressed look.",
27 "Avoid adding rips too near the seams, as they may
Li Zhang, Qing Lyu, and Chris Callison-Burch. 2020a. cause them to begin to unravel.",
28 "For exact rips, use a sewing needle to pull out
IntentdetectionwithWikiHow. InProceedingsofthe individual stitches from the fabric."
1stConferenceoftheAsia-PacificChapteroftheAs-29 ],
30 "warnings": [
sociationforComputationalLinguisticsandthe10th31 "Don’t make the rip too big at first. Washing the
InternationalJointConferenceonNaturalLanguage fabric will increase the size and fray of the
hole.",
Processing,pages328–333.AssociationforComputa-32 "Never attempt to rip or fray your jeans while you’re
wearing them.",
tionalLinguistics. 33 "Use caution with sharp tools."
34 ],
35 "QAs": ["ABRIDGED"],
Li Zhang, Qing Lyu, and Chris Callison-Burch. 2020b.
36 "refs": [
Reasoningaboutgoals,steps,andtemporalordering37 "https://www.marieclaire.co.uk/news/fashion-news/how-
to-rip-jeans-821",
withWikiHow. InProceedingsofthe2020Conference38 "https://www.marieclaire.co.uk/news/fashion-news/how-
onEmpiricalMethodsinNaturalLanguageProcessing to-rip-jeans-821",
39 "https://www.cosmopolitan.com/style-beauty/fashion/a58
(EMNLP),pages4630–4639.AssociationforCompu- 592/5-easy-tricks-for-distressing-your-jeans/"
tationalLinguistics. 40 ],
41 "quizzes": [],
42 "other_languages": {"ABRIDGED"},
ZiruiZhao,WeeSunLee,andDavidHsu.2023. Large43 "steps": ["ABRIDGED"]
44 }
languagemodelsascommonsenseknowledgeforlarge-
scaletaskplanning. InRSS2023WorkshoponLearn-
ingforTaskandMotionPlanning. B FilteringExamples
ShuyanZhou,LiZhang,YueYang,QingLyu,Pengcheng
Aftersamplingthenegativecandidates,weapplya
Yin,ChrisCallison-Burch,andGrahamNeubig.2022.
set of filters introduced by Zhang et al. (2020b) to
Showmemoredetails: Discoveringhierarchiesofpro-
ceduresfromsemi-structuredwebdata. InProceedings ensurethehigh-qualityofthepairsandthechallenge
ofthe60thAnnualMeetingoftheAssociationforCom- theybring. Howeverwechangesomeofthesefilters
putationalLinguistics(Volume1: LongPapers),pages asfollows:
2998–3012. Association for Computational Linguis-
tics. Similarityfilter: Weobtainthecosinesimilarity
scoresusingSupervisedSimCSE-RoBERTa-Large
A ExampleJSONFile
(Gaoetal.,2021), sincesentenceembeddingscap-
AnexampleJSONfilefromourcorpuscanbeseen ture sentence-level information, resulting in better
inListing1. filtering.
Listing1: ExampleJSONfilefortheproceduraltutorial Length filter: We also set an upper bound to en-
withthegoal"RipYourOwnJeans"fromwikiHow. suretheyarelong(> 8tokens)enoughtocontainrel-
1 { evantinformationyetshort(< 128tokens)enough
2 "title": "How to Rip Your Own Jeans",
3 "url": "https://www.wikihow.com/Rip-Your-Own-Jeans", tobeon-pointandcoherent.
4 "title_description": "Distressed denim is a popular style,
but buying jeans that are already ripped can be
expensive. Luckily, you can create this trend Categoryfilter: Weexcludeexamplesfromsome
yourself by roughing up the fabric with a piece of
sandpaper, then snipping a hole with a pair of categories (e.g., Philosophy and Religion, Celebri-
scissors.",
5 "category_hierarchy": [ ties,HolidaysandTraditions,etc.) thatmightnotbe
6 "Hobbies and Crafts",
7 "Crafts", suitableforevaluatinglanguagemodels’reasoning
8 "Decoration Projects"
skillsduetotheirsubjectivity.
9 ],C DatasetAnalysiswithCartography Hyperparameter DistilBERT BERT RoBERTa DeBERTa
TotalBatchSize 32 64 64 128
GradientAcc.Steps 1 2 2 4
LearningRate 2e-5 2e-5 1e-5 1e-5
Max.SequenceLength 128 128 128 128
Table7: Hyperparametersusedinthefine-tuningsof
Weutilizethedatasetcartographytool(Swayamdipta
models.
etal.,2020), whichprocessesamodel’sbehaviour
on training instances (also known as the training
dynamics)formappingthedataset,tobetterunder- Model GPU Time
standthecharacteristicsofourdatasets. Tothisend, WARNINGINFERENCE
cartographyderivesthreemetricsfromthetraining DistilBERT 1xNVIDIAT4 6mins10secs
BERT 1xNVIDIAT4 11mins53secs
dynamics: confidence (the mean model probabil-
RoBERTa 1xNVIDIAT4 11mins45secs
ityofthetruelabelacrossepochs),correctness(the DeBERTa 2xNVIDIAT4 9mins32secs
fractionoftimesthemodelcorrectlylabelsanobser- TIPINFERENCE
vationacrossepochs),andvariability(thespreadof DistilBERT 1xNVIDIAT4 12mins47secs
BERT 1xNVIDIAT4 24mins57secs
themodel’sprobabilityofcorrectlylabelingobserva-
RoBERTa 1xNVIDIAT4 24mins51secs
tionsacrossepochs). Withthesemetrics,cartography DeBERTa 2xNVIDIAT4 19mins55secs
revealsthreeregionsinadatamap,eachwithdistinct
Table8: Computationalcostsoffine-tuningeachmodel
features.
acrosseachtask.
Usingthecartographytool,wegeneratedatamaps
for warning and tip inference datasets. As seen in
D ImplementationDetails
Fig. 6, both of our datasets have a high density
in the positive ends of confidence and correctness
We use the base versions of DistilBERT8, BERT9,
andinthenegativeendofvariability,indicatingthat
RoBERTa10,andDeBERTa11 andimplementthem
BERTiscapableofconfidentlychoosingthecorrect
usingHuggingFacelibraries,namelytransformers
warningsandtipsthroughoutthetraining. However,
(Wolfetal.,2020),evaluate,andaccelerate(Sylvain
tipinferencedatasetshowsgreaterdensityinthose
etal.,2022).
ends,demonstratinghighereasinessthatreinforces
We fine-tune each model for 1 epoch with its
ourreasoninginAppendixH.
unique set of hyperparameters that can be seen in
Table 7. We use a batch size of 8 and keep the de-
Additionally, we implement the noise detector
faultvaluesforallotherhyperparametersfortesting.
(Swayamdiptaetal.,2020)usingaGaussianNaive
Infine-tuning,weusefivedifferentseeds(42,2717,
Bayesclassifiermodel(Chanetal.,1979)toidentify
6802,9893,and7818)toconductstatisticalsignifi-
mislabeled instances in our datasets. We train our
canceanalysis. Intesting,wesettheseedto42.
classifiermodelonasmallsetofequallydistributed
For both fine-tuning and testing, we utilize half-
mislabeled(randomlyre-assinged)andcorrectlyla-
precision floating point format (FP16) with the ac-
beled data instances. Although simple, it achieves
celeratelibrary. DistilBERT,BERT,andRoBERTa
95.2% accuracy on the test set. We, then, use the
modelsarefine-tunedonasingleNVIDIAT4,while
classifier model on the entire warning and tip in-
DeBERTaisfine-tunedontwoNVIDIAT4s. Com-
ference datasets. Our classifier model finds 1022
putational costs of fine-tuning each model across
noisyinstancesinthetipinferenceand746noisyin-
eachtaskcanbeseeninTable8.
stancesinthewarninginferencedatasets,indicating
a1.3%and1.9%ofnoiserespectively. Suchlevelsof
noiseinautomaticallygenerateddatasetswithnohu- 8https://huggingface.co/distilbert-base-uncased
9https://huggingface.co/bert-base-uncased
mansupervision(otherthancuratingthetestsplits)
10https://huggingface.co/roberta-base
illustrates the success of the filtering described in
11https://huggingface.co/microsoft/
AppendixB. deberta-v3-baseTip Inference with BERT Data Map Warning Inference with BERT Data Map
1.0 c 0o .0rrect. 22 05 00 00 00 1.0 c 0o .0rrect. 11 02 05 00 00
easy-to-learn 0 0 1. . .3 7
0
11 05 00 00 00 easy-to-learn 0 0 1. . .3 7
0
57 05 00 00
0.8 5000 0.8 2500
00.0 0.5 1.0 00.0 0.5 1.0
confidence confidence
12500
0.6 20000 0.6 10000 ambiguous 11 05 00 00 00 ambiguous 57 05 00 00
0.4 5000 0.4 2500
00.0 0.2 0.4 00.0 0.2 0.4
variability variability
0.2
hard-to-learn 40000
0.2
hard-to-learn
20000
30000 15000
20000 10000
0.0 10000 0.0 5000
0.0 0.1 0.2 0.3 0.4 0 0.00.30.71.0 0.0 0.1 0.2 0.3 0.4 0 0.00.30.71.0
variability correctness variability correctness
Figure6: DatamapsforwarningandtipinferencetasksobtainedwithBERT.
E PromptingLLMs F AdditionalKeywordManipulation
Methods
Inordertoeffectivelyengagewithalanguagemodel, Inadditiontodropping,wemanipulatethecommon
it is essential to meticulously construct a prompt keywordsinthecandidatesusingthefollowingmeth-
template and finetune specific parameters, notably ods,asexemplifiedinTable11:
temperature(samplingtemperaturebetween0and1
or0and2)andtopp(nucleussampling,wherethe 1. SynonymReplacement: Wereplacesuchkey-
modelconsiderstheresultsofthetokenswithtopp words with their synonyms using WordNet
probabilitymass)(OpenAI,2023a). (Miller,1994)withNLTK(Birdetal.,2009).
Fortheprompttemplateconstruction,wereferred
2. ReplacingwithaPlaceholder: Wereplacesuch
to the official API documentation of each model
keywords with a placeholder word, which is
to ascertain the recommended templates. In cases
simply PLACEHOLDER.
where no specific templates were provided, we di-
rectly used the questions as prompts, omitting any
3. ReplacingwiththeBERTPrediction: Wemask
additionaltokens.
suchkeywordsoutwiththe[MASK]tokenand
useBERTtopredictthemostlikelytokenother
Regardingthecalibrationofthetemperatureand
thantheoriginalone.
top p settings, we initiated our tests with the de-
faultvaluesasspecifiedinthemodel’sAPIandplay-
We evaluate fine-tuned PLMs on these manipu-
groundinterface. Thisapproachwasfollowedbya
lated examples. As seen in Fig. 7, synonym re-
systematictuningprocesstooptimizeperformance.
placement causes the least decrease in the perfor-
Notably,weobservedthatalowertemperatureset-
mancewithanapproximateaverageof10%dropin
ting, as compared to the default, yielded more ac-
accuracyacrossmodelsandtasks. Dropping,place-
curate results. This aligns with the general under-
holder replacement, and BERT prediction replace-
standingthatlowertemperaturesarepreferablefor
ment closely follow each other respectively, with
fact-based prompts, while higher temperatures are
averagedecreasesinaccuracyvaryingfrom15%to
better suited for tasks requiring creativity and an
20%.
elementofrandomness.
Theaccompanyingtables9and10showthespe- G DeBERTaandGPT-4Failures
cific prompts and parameter settings employed for
each model (all the experiments were done in De- SpecificexamplesthatDeBERTaandGPT-4failcan
cember2023). beseeninTable12.
ecnedifnoc
ytisned
ytisned
ytisned
ecnedifnoc
ytisned
ytisned
ytisnedModel Settings Prompt
Iwillgiveyouagoalbelowand4tips. Canyoupickthemostrelatedtiptoit?
Goal: PrepareforaLongCarTrip
Tips:
Temp. =0.3 Tip 0- While performing any exercise, make sure you are drinking water to stay
GPT-4
Topp=0.9 hydrated.
Tip1-Dodrinkplentyofwatertokeepyourskinhydrated.
Temp. =0.3 Tip2-Ifyouaretravelingforalongtime,bringabottleofwatertokeepyouhydrated.
PALM2
Topp=0.9 Tip3-Bringabottleofwaterwithyoutostayhydrated.
Responseformat: Returnthetipnumberinjsonwith‘tip’askeyandnomoredetails.
<s>[INST]
Iwillgiveyouagoalbelowand4tips. Canyoupickthemostrelatedtiptoit?
Goal: PrepareforaLongCarTrip
Tips:
Temp. =0.3 Tip 0- While performing any exercise, make sure you are drinking water to stay
Llama2
Topp=0.9 hydrated.
Tip1-Dodrinkplentyofwatertokeepyourskinhydrated.
Temp. =0.0 Tip2-Ifyouaretravelingforalongtime,bringabottleofwatertokeepyouhydrated.
Mistral
Topp=0.1 Tip3-Bringabottleofwaterwithyoutostayhydrated.
Responseformat: Returnthetipnumberinjsonwith‘tip’askeyandnomoredetails.
[\/INST]
Achatbetweenahumanandanassistant.
###Human:
Goal: PrepareforaLongCarTrip
Tips:
Tip 0- While performing any exercise, make sure you are drinking water to stay
Temp. =0.3
Vicuna hydrated.
Top-p=0.9
Tip1-Dodrinkplentyofwatertokeepyourskinhydrated.
Tip2-Ifyouaretravelingforalongtime,bringabottleofwatertokeepyouhydrated.
Tip3-Bringabottleofwaterwithyoutostayhydrated.
Responseformat: Returnthetipnumberinjsonwith‘tip’askeyandnomoredetails.
###Assistance:
Table9: LLMsettingsandpromptsfortipinference.Model Settings Prompt
Iwillgiveyouagoalbelowand4warnings. Canyoupickthemostrelatedwarning
toit?
Goal: MakeLaundryDetergentSlime
Warnings:
Warn0-Warning0-Avoidfloodingthefloorwithcleanerorwater. Athinlayerof
Temp. =0.3 watershouldbeenoughforadryclothtowipe
GPT-4
Topp=0.9 Warn1-Don’tapplyheat(dryer,iron)tothestainedareauntilthestainisgone.
Warn2-Don’tplacetheslimeinacoldareawhenit’sfinished. Itmaybecomeless
Temp. =0.3 stretchy.
PALM2
Topp=0.9 Warn3-Don’tlettheresurfacerdryonyourskinsinceitmaycauseirritationandis
difficulttoremove.
Responseformat: Returnthewarningnumberinjsonwith‘warn’askeyandnomore
details.
<s>[INST]
Iwillgiveyouagoalbelowand4warnings. Canyoupickthemostrelatedwarning
toit?
Goal: MakeLaundryDetergentSlime
Warnings:
Warn0-Warning0-Avoidfloodingthefloorwithcleanerorwater. Athinlayerof
Temp. =0.3
Llama2
watershouldbeenoughforadryclothtowipe
Topp=0.9
Warn1-Don’tapplyheat(dryer,iron)tothestainedareauntilthestainisgone.
Warn2-Don’tplacetheslimeinacoldareawhenit’sfinished. Itmaybecomeless
Temp. =0.0
Mistral
stretchy.
Topp=0.1
Warn3-Don’tlettheresurfacerdryonyourskinsinceitmaycauseirritationandis
difficulttoremove.
Responseformat: Formattheanswerinjsonwiththewarningnumberasvalueand
’warn’askeyandnomoredetails. [\/INST]
Achatbetweenahumanandanassistant.
###Human:
Iwillgiveyouagoalbelowand4warnings. Canyoupickthemostrelatedwarning
toit?
Goal: MakeLaundryDetergentSlime
Warnings:
Warn0-Warning0-Avoidfloodingthefloorwithcleanerorwater. Athinlayerof
Temp. =0.3
Vicuna watershouldbeenoughforadryclothtowipe
Top-p=0.9
Warn1-Don’tapplyheat(dryer,iron)tothestainedareauntilthestainisgone.
Warn2-Don’tplacetheslimeinacoldareawhenit’sfinished. Itmaybecomeless
stretchy.
Warn3-Don’tlettheresurfacerdryonyourskinsinceitmaycauseirritationandis
difficulttoremove.
Responseformat: Returnthewarningnumberinjsonwith‘warn’askeyandnomore
details.
Table10: LLMsettingsandpromptsforwarninginference.WarningInference TipInference
1 1
0.9 0.9
0.8 0.8
0.7 0.7
Original
Synonym
0.6 0.6 Drop
Placeholder
BERTPrediction
0.5 0.5
DistilBERT BERT RoBERTa DeBERTa DistilBERT BERT RoBERTa DeBERTa
Figure7: Modelperformancestestedonmanipulatedtestdata.
Manipulation PositiveCandidate(abridged) NegativeCandidate(abridged)
GOAL:StopEatingFastFood
COMMONKEYWORDS:Fast,Food,Eating
Beatingafastfoodaddictionisaloteasierwhenyou’renot Beingvulnerableaboutyoureatingdisorderisahardthingtodo.
Original
goingitalone.Talktofriendsaboutyourgoalsforyourdiet. Rememberthatyouneedotherpeopletosupportyou.
Beatingaflyingfoodaddictionisaloteasierwhenyou’renot Beingvulnerableaboutyoureatdisorderisahardthingtodo.
Synonym
goingitalone.Talktofriendsaboutyourgoalsforyourdiet. Rememberthatyouneedotherpeopletosupportyou.
Beatinga addictionisaloteasierwhenyou’renot Beingvulnerableaboutyour disorderisahardthingtodo.
Dropping
goingitalone.Talktofriendsaboutyourgoalsforyourdiet. Rememberthatyouneedotherpeopletosupportyou.
Beatingaplaceholderplaceholderaddictionisaloteasier Beingvulnerableaboutyourplaceholderdisorderisahard
Placeholder whenyou’renotgoingitalone.Talktofriendsaboutyour thingtodo.Rememberthatyouneedotherpeopleto
goalsforyourdiet. supportyou.
Beatinganewdrugaddictionisaloteasierwhenyou’renot Beingvulnerableaboutyouranxietydisorderisahardthingtodo.
BERTPrediction
goingitalone.Talktofriendsaboutyourgoalsforyourdiet. Rememberthatyouneedotherpeopletosupportyou.
Table11: Examplesofkeywordmanipulationforapairfromthetipinferencedataset. Thegoalis"StopEatingFast
Food",whichsharesthecommonkeywordsof"Fast"and"Food"withthepositivecandidateand"Eating"withoneof
thenegativecandidates.
H AComparisonBetweenWarningsand 1. Both PLMs and LLMs perform better in tip
Tips inference compared to warning inference, in-
dicatingagreatercapabilityinreasoningwith
AsdiscussedinSec. 4.5.1,warningsandtipsshare tipsduetotipsbeingmoredirectlyconnected
thecommonpurposeofpresentingadditionalinfor- totheirgoals.
mationtouserforbetterinstructionexecutionwithin
2. Previousfine-tuningontipinferencebetterim-
proceduralwikiHowtutorials. Thus,theygenerally
proves the performance in transfer learning
havehighsimilarityinsemanticsandstructure.
compared to previous fine-tuning on warning
Overall, warning inference poses a greater chal-
inference,demonstratingthattipinferencecon-
lengecomparedtotipinference. Webelievethisis
tributes to model’s learning over procedural
due to two main reasons. First, tips are more goal-
tasksmore.
specific, whilewarningsaremoregeneral, yetstill
distinguishable. Forexample,anygoalthatcontains 3. Tip inference is affected more by keyword
sharp objects might have a warning towards using manipulation, showing that tip inference data
thosesharpobjectscarefully;yet,thesamedoesnot is more vulnerable to keyword alteration be-
holdtruefortips. Second,tipsareampleinquantity; causeitspositivecandidatescontainmoregoal-
therefore,therearemoreexamplestolearnfrom. specificvocabulary.
Our reasoning behind tips being more goal-
specific and informative regarding the procedural
tutorial compared to warnings is reinforced by the
followingfindings:
ycaruccADeBERTa GPT-4
InstallCeramicWallTile BlockPeopleonFacebook
BuildaNerfFort SeeYourWiFiPasswordonaniPhone
TurnaCardboardBoxIntoaBasket ChangethePasswordinOutlook365
PreventWaterStainsonBathroomWalls DealwithCattyCoworkers
MakeaRopeBraid BeConfidentAsaShortPerson
RemoveaRedWineStainRingfromaWoodTable HelpanAutisticFamilyMember
Table12: GoalsofexamplesthatDeBERTaandGPT-4fail.