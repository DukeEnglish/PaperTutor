JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 1
Distributed Policy Gradient for Linear
Quadratic Networked Control with Limited
Communication Range
Yuzi Yan, Student Member, IEEE, and Yuan Shen, Senior Member, IEEE,
Abstract—This paper proposes a scalable distributed policy remains unresolved. There are many challenging problems
gradient method and proves its convergence to near-optimal at both theoretical and practical levels: First, it is a non-
solution in multi-agent linear quadratic networked systems.
convex optimization problem, where in general the gradient
The agents engage within a specified network under local
descent does not converge to the global optima in the limit.
communication constraints, implying that each agent can only
exchangeinformationwithalimitednumberofneighboringagents. Second, each agent must accurately approximate the exact
On the underlying graph of the network, each agent implements global gradient using limited communication, which is critical
its control input depending on its nearby neighbors’ states in to achieving scalability for excessively large networked multi-
the linear quadratic control setting. We show that it is possible
agent systems. Third, the dynamical system may suffer from
to approximate the exact gradient only using local information.
the risk of becoming unstable during the distributed policy
Comparedwiththecentralizedoptimalcontroller,theperformance
gap decreases to zero exponentially as the communication and gradientprocess.Itisnecessarytoprovideastabilityguarantee.
control ranges increase. We also demonstrate how increasing the This paper first provides theoretical results about the prob-
communication range enhances system stability in the gradient lems mentioned above, encouraged by the recent success in
descent process, thereby elucidating a critical trade-off. The the scalable optimization algorithm [21]. With a few mild
simulation results verify our theoretical findings.
assumptions about the state transition matrix of the networked
IndexTerms—Distributedoptimization,networkedsystem,near system in the Markov Decision Process (MDP) setting, every
optimality, distributed gradient method, linear quadratic control,
agent can approximate an accurate localized policy gradient in
stability.
a distributed fashion. The proposed distributed policy gradient
method can attain near-optimal solutions with a gap that is
I. INTRODUCTION exponentially small relative to the communication range κ and
RECENT years have seen significant advances in op- the control range r. By carefully selecting the step size η,
timizing stochastic dynamical multi-agent networked the system’s stability can be ensured. Numerical results for
systems [1], such as communication [2], cooperative detec- severalrepresentativecasesarealsopresentedtoillustratethese
tion [3], [4], games [5], formation [6]–[8], localization [9], findings.
smart grid [10], etc. Compared to single-agent control, the Our main contributions are summarized as follows.
multi-agent setting presents additional challenges due to com- • LocalizedGradientApproximation.Weprovideconditions
munication limitations between agents, with scalability being for accurate gradient approximation with limited local
a primary concern [11]. Although some scalable reinforcement communicationinanetworkedsystem.Thisworkanalyzes
learning (RL) methods have been proposed, there needs to be the conditions for the establishment of Exponential Decay
more theoretical understanding regarding their optimality and Property in a networked LQR setting, which is a crucial
efficiency [12]. On the other hand, control theory provides us concept in many scalable multi-agent RL algorithms. In
with a rich body of tools with provable guarantees [13]–[16]. specific, for the Exponential Decay Property to hold, we
Specifically,linearquadraticregulator(LQR)isoneofthemost needsomemildassumptionsonthestatetransitionmatrix,
well-studiedoptimalcontrolproblems,whichconsidersoptimal which indicate that the interaction between agents should
state feedback control for a linear dynamical system [17]. be weak enough or the network’s connectivity should be
Networked linear quadratic control has many applications, low enough.
including transportation [18], power grid [19], etc. • Stability Guarantee in the Gradient Descent Process. We
Recent studies have shown that within a centralized LQR provide a stability guarantee during the gradient process.
framework, it is feasible to derive the optimal controller using We quantify the two factors that may cause the system
policy gradient descent techniques [16], [20]. However, to to become unstable: 1) the inappropriate step size, 2) the
the best of our knowledge, the question of whether one error brought by the localized gradient approximation.
can acquire a near-optimal decentralized controller through The guarantee is necessary because the stability of the
ascalableanddistributedpolicygradientdescentapproachstill system is a prerequisite for the performance analysis.
• Near Optimal Performance for the Distributed Policy
TheauthorsarewiththeDepartmentofElectronicEngineering,andBeijing Gradient. We show that the decentralized controller
NationalResearchCenterforInformationScienceandTechnology,Tsinghua
obtained through the distributed policy gradient achieves
University, Beijing 100084, China (e-mail: yyz21@mails.tsinghua.edu.cn;
shenyuan ee@tsinghua.edu.cn). near-globaloptimality,whereeachagent’scommunication
4202
raM
5
]AM.sc[
1v55030.3042:viXraJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 2
isrestrictedwithinitsκ-hoplocalneighborhoodandtakes networkedsystems.In[20],theauthorsexamineLQRfeedback
control actions based on its r-hop local observations. The synthesis with a sparsity pattern and guarantee a sublinear rate
optimality gap compared with the centralized optimal of convergence to a first-order stationary point. In [21], a zero-
controller is proven to be exponentially small in κ and r. order distributed policy optimization algorithm is proposed for
Notation. Throughout the paper, we use ∥·∥ to denote the l networked LQR. The algorithm is guaranteed to approach the
2
norm of a vector and the induced l norm of a matrix. ∥·∥ stationarypoint.In[32],theperformancebetweenthetruncated
2 F
denotes its Frobenius norm. AT, ρ(A), and Tr(A) represent distributed controller and the optimal controller is proved
the transpose, spectral radius, and trace of the matrix A. A⪰ to be exponentially small with some additional assumptions.
B for two symmetric matrices refers to the positive semi- In [33], the authors show that the optimal LQR state feedback
definiteness of their difference A−B. We let σ (A) represent gain is “quasi”-SED (spatially-exponential decaying) with
i
the eigenvalues of a square matrix A, which are indexed in system matrices being SED between nodes in the network,
increasing order for their real parts, i.e., Re(σ (A))≤···≤ suggesting that distributed controllers can achieve near-optimal
1
Re(σ (A)).IfAissymmetric,theorderingbecomesσ (A)≤ performance for SED system.
n 1
··· ≤ σ (A). Suppose that A is a partitioned matrix, [A]
n ij
denotes the sub-matrix of A where its row indexes correspond
totheroleindexesofagentianditscolumnindexescorrespond B. Multi-agent Reinforcement Learning
totheindexesofagentj.Weuse[A]todenotemax ∥[A] ∥
i,j ij Our problem can also be viewed as a special case of the
and [A] to denote min ∥[A] ∥. Without further explanation,
i,j ij cooperative setting in multi-agent RL (MARL), i.e., the Decen-
dist(·,·) refers to the graph distance between two agents in
tralized Partially Observable Markov Decision Process (Dec-
the system throughout the paper. dist(i,i) is defined to be 0.
POMDP). This intricate framework for multi-agent systems is
For integer κ>1, Nκ denotes the κ-hop neighborhood of
i characterized by limited agent observability. In a Dec-POMDP,
i, i.e., the agents whose graph distance to i is less than or
multiple agents cooperate under the constraint of not having a
equal to κ, including i itself. Nκ denotes the set including
−i complete perspective of the environment or the full actions of
all the agents that are outside κ-hop neighborhood of i. On
their peers [34].
an undirected graph G = (N,E), given a fixed κ, we use
In response to the challenges posed by Dec-POMDPs,
Nκ to denote the number of the agents in the biggest κ-hop
G numerous MARL algorithms have been developed. The policy
neighborhood, i.e., Nκ =max |Nκ|.
G i i gradient approach, within this space, emerges as a popular
choice [35], [36]. In [37]–[39], the authors introduced a
II. RELATEDWORK
Scalable Actor-Critic methodology, proficient at discerning
Webrieflydiscussrelatedworksinareassuchasoptimization a near-optimal localized policy in tabular RL contexts. In [40],
in networked systems, learning-based control, and scalable the authors crafted a scalable algorithm rooted in the NPG
multi-agent reinforcement learning. These works significantly framework, demonstrating its convergence to the globally
inspired this paper. A comparative analysis is then presented optimal policy. Their analysis further illustrates that the
to highlight the distinctions between our work and these prior performance disparity diminishes to zero at an exponentially
studies. rapid rate, contingent upon the communication range. In [41],
the authors postulated a Localized Policy Iteration (LPI)
A. Learning-based Control algorithm, empirically validated to adeptly acquire a policy
that is nearly globally optimal, leveraging solely localized
In the context of LQR, the classical model-based approach
data within a networked system. Their findings also spotlight
dates back to subspace-based system identification [22]. More
that the optimality gap recedes polynomially, governed by the
recently, much progress has been made in algorithm design
communication range parameter, κ.
and nonasymptotic analysis in both centralized and networked
LQR optimization.
Severalstudies,includingthoseby[23]–[25],haveaddressed
C. Comparison with Prior Work
theissuefromtheperspectiveofasingleagent.In[16],theau-
thorshaveprovidedproofofglobaloptimalityinthecentralized The results most pertinent to our study include those in
LQR setting, specifically concerning deterministic policies and [38], [33], and [32]. Here we aim to delineate the distinctions
non-noisy transitions. Notably, their paper introduces a model- betweenourresultsandtheirs.In[38],theauthorsdemonstrate
free algorithm based on zero-order optimization. Furthermore, that the Exponential Decay Property holds for MDPs with
[26] has demonstrated that an actor-critic algorithm for the average rewards in the tabular case. In contrast, our study
single-agent LQR achieves optimal performance with a linear delves into continuous spaces under the LQR setting. In [32],
convergence rate. Global convergence of policy optimization [33], the authors elucidate the spatially exponential decaying
methodsforothercontrolproblemshasalsobeenstudiedlately attribute of the optimal controller concerning the networked
[15], [27], [28], see [29] for an overview of the recent results LQR problem. Our perspective, however, concentrates on
on policy optimization for control. finding the near-optimal solution within the projected space.
Some other works [30], [31] try to address the issue in a Notably,ourcontributionprovidesadistributed,scalablepolicy
distributed fashion, where centralized methods face challenges gradient technique to unearth this solution. We also assert its
related to scalability and communication overhead in extensive stability,afeaturenotshowcasedintheaforementionedstudies.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 3
dynamics of the linear system and the local cost could be
rewritten as
x (t+1)=A x (t)+B u (t)+w (t),
i i Ni i i i
c i(t)=x Ni(t)TQ(cid:101)ix Ni(t)+u i(t)TR(cid:101)iu i(t),
where x is the concatenated state vector of agent i’s 1-hop
neighborN s.i Theglobalcostc(t)= n1 (cid:80)n i=1c i(t).Q(cid:101)i andR(cid:101)i are
positive definite matrices that parameterize the local quadratic
costs. Note that if we pad Q(cid:101)i and R(cid:101)i with 0, we can create
Q and R , and make them the same size as Q and R. Then,
i i
the local state cost function c (t) can be written as
i
c (t)=x(t)TQ x(t)+u(t)TR u(t).
i i i
We further define x Nκ and u Nκ to be the joint state and
i i
Fig. 1: An illustrative diagram for a networked system. The top control vectors of agent i’s κ-hop neighbors, x N −κ i and u N −κ i
to be the joint state and control vectors outside of the κ-
figure illustrates the local control inputs, local controllers, and local
observations; and the bottom figure provides a global perspective hop neighborhood.1 Given that the global cost of the system
where the optimization objective controller K∈Mr. Note that we c(t) is the average of all the local costs c (t), it follows that
i
omit the noise added to the control output for simplicity. Q= 1 (cid:80)n Q and R= 1 (cid:80)n R .
n i=1 i n i=1 i
Next,weintroducetheoptimizationobjective.Inthecontext
of distributed optimization, the admissible local controllers
III. PRELIMINARIESANDBACKGROUND are constrained to those that solely utilize the current state
of r-hop neighbors, denoted as Nr, as input. r reflects how
i
A. Problem Formulation distributed the controllers could be. If r reaches the diameter
of the underlying graph, the controller will degenerate into
First,weintroducethebasicsetupofnetworkedLQRcontrol.
a centralized one. Throughout the rest of this paper, we
Suppose there are n agents jointly controlling a discrete-time
focus on the family of stochastic linear-Gaussian policies, i.e.,
l ai rn eea Rr dd xyn anam di Rc ds uy .st Te hm e. tT rah ne ss itt ia ote ns dp ya nc ae mX icsan ad ndac tt hio en qusp aa dc re atiU
c
π Ki(·|x
N
ir) = N(−K ix
N
ir,σ 02I
di
u),K
i
∈
Rdi u×(cid:80) j∈Nirdj
x.
We use K to denote the control gain matrix of the entire
state cost c(t) of the whole system are given by:
x(t+1)=Ax(t)+Bu(t)+w(t),
s sty rs at ie gm ht, foπ rwK a( r·| dx t)
o
= seeN
π
K(− (uK |xx ),σ =02I (cid:81)dun i=), 1K
π
Ki∈
(u
iR |xdu N× ird )x .2. It is
c(t)=x(t)TQx(t)+u(t)TRu(t), Definition1. Foranyintegerκ>1,foran-by-nblockmatrix
M, we define
where x(t)∈Rdx and u(t)∈Rdu, w(t)∼N(0,Φ) denotes
random noise that is i.i.d. for each time step t. Each agent Mκ ={M:[M] =0 i.f.f. dist(i,j)>κ,∀i,j},
ij
i is associated with a local state x i(t) ∈ Rdi x and a control
input u i(t) ∈ Rdi u, which jointly constitute the global state
and for the matrices related to a certain agent i, we define
x(t) = [x (t)T,x (t)T,...,x (t)T]T and the global control
1 2 n
input u(t) = [u (t)T,u (t)T,...,u (t)T]T. Note that d =
1 2 n x
(cid:80)n i=1di x and d u =(cid:80)n i=1di u. Mκ i ={M:[M] ij =0 i.f.f. dist(i,j)>κ,∀j}.
Next, we illustrate the structural properties of the network
and demonstrate how these properties confer local structural
It is direct to see that A, B, Q, R and K are all n-by-n
characteristics to the optimization. We consider the case that block matrices and A,Q∈M2, B,R∈M0. The goal is to
thereexistsanetworkofnagentswithanunderlyingundirected find a distributed controller K∈Mr, which is stabilizing in
graph G = (N,E), where N = {1,2,··· ,n} is the set of
thesensethatitensuresthesystem’sstateconvergestoastable
agents and E ⊆ N ×N is the set of edges. As a common
setting in many practical scenarios [37]–[39], the next state of
each agent i, denoted as x i(t+1), along with the quadratic 1Inthefollowing,sometimesweusetheformsuchasx=(x Nκ,x Nκ ).
i −i
local cost c (t), is dependent solely on the state of its 1-hop Note that we omit the rearrangement of the index of agents for notation
i
neighbors (denoted by N ) and its own control input at time simplicity.
step t. We can use a
seriei
s of smaller matrices {A }n and
2Wesometimesuse{K1,K2,...,Kn}todenotethecontrollersfromthe
i i=1 localperspectiveandnotethatitisequivalenttoKfromtheglobalperspective.
{B i}n
i=1
to describe the local transition process. The transition WecancombinealltheKi andpaditwith0toaRdu×dx matrixtogetK.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 4
Fig. 2: A diagram to show the spatial structures of the system matrices, including A, B, Q and R.
equilibrium, thereby minimizing the infinite-horizon average With a fixed controller K∈Mr, the relative action-value
cost among all agents, function is defined as
T−1 ∞
minC(K)=min lim
1 (cid:88)
Ec(t) Q
K(x,u)=(cid:88) E(cid:2) c(t)−C(K)(cid:12)
(cid:12)x(0)=x,u(0)=u,
K K T→∞T (4)
t=0 t=0
(cid:3)
1
T (cid:88)−1
1
(cid:88)n u(t)∼π K(·|x(t)),t≥1 .
=min lim E c (t)
K T→∞T n i In the setting of LQR, the state-value function is quadratic for
t=0 i=1
n a fixed controller K (Proposition 3.1, [26]),
1 (cid:88)
=:min C (K)
K n i Q K(x,u)=xT(Q+ATP KA)x+xT(ATP KB)u
i=1
where we define C (K)=lim 1 (cid:80)T−1Ec (t). +uT(BTP KA)x+uT(R+BTP KB)u
Toillustratetheloi calizedstruT c→ tu∞ reoT fthet= p0 olicyi optimization −Tr(P KΞ K)−σ 02Tr(R+P KBBT).
problem for the networked LQR networked system, we present
We further define the localized Q function Qi :
two schematic diagrams (Fig. 1 and Fig. 2), where the K
relationship between the local perspective and the global ∞
perspective is shown clearly.
Qi K(x,u)=(cid:88) E(cid:2) c i(t)−C i(K)(cid:12) (cid:12)x(0)=x,u(0)=u,
t=0
(cid:3)
B. Critical Concepts in LQR and RL u(t)∼π K(·|x(t)),t≥1 .
Toenhancethecomprehensionoftheforthcomingderivation, It is straightforward that Q (x,u)= 1 (cid:80)n Qi (x,u). Now
this subsection introduces some important concepts related to K n i=1 K
we can introduce the policy gradient theorem in LQR.
RL and LQR. First, we rewrite the state transition dynamics
from the global perspective, Lemma 1. [26] [Policy Gradient Theorem]
x(t+1)=(A−BK)x(t)+ϵ(t), (1) ∇ C(K)=E (cid:2) ∇ logπ (u|x)·Q (x,u)(cid:3) .
K x∼νK,u∼πK(·|x) K K K
where ϵ(t)∼N(0,Ψ) and Ψ=Φ+σ 02·BBT, following the In LQR, it can be equivalently expressed as
problem formulation in Section III-A.
In the setting of LQR, for any K ∈ Rdu×dx such that ∇ KC(K)=2[(R+BTP KB)K−BTP KA]Ξ K.
ρ(A − BK) < 1, let P be the unique positive definite
K
solution to the Lyapunov equation IV. MAINRESULTS
P K =(Q+KTRK)+(A−BK)TP K(A−BK). (2) In this section, we first illustrate the localized gradient
Note that when ρ(A−BK)<1, the Markov chain in (1) approximation method by revisiting the Exponential Decay
has stationary distribution ν ∼N(0,Ξ ), where Ξ is the Property [37]–[39] in the networked LQR setting. Then, we
K K K
present an algorithmic framework to solve the networked
unique positive definite solution to the Lyapunov equation
LQR control problem by using distributed policy gradient
Ξ =Ψ+(A−BK)Ξ (A−BK)T. (3)
K K descent(Algorithm1).WeestablishthetheoreticalboundoftheJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 5
performance gap between the controller K(T) obtained by T- Q functions, wherein the dependence on distant agents is
stepgradientdescentandthecentralizedoptimalcontrollerK∗, selectively truncated,
given the localized approximation of the gradient derived from (cid:90) (cid:90)
theExponentialDecayProperty.Additionally,sincethegradient Qi K,κ(x N iκ,u N iκ)= ζ i(x N −κ i,u N −κ i;x N iκ,u N iκ)
(5)
approximationintroducesapproximationerrorsineverydescent
Qi (x ,x ,u ,u )dx du .
step and the stability may fail, we prove that all the controllers K N iκ N −κ i N iκ N −κ i N −κ i N −κ i
generated by the gradient descent algorithm are stabilizing ζ i(x Nκ ,u Nκ ;x Nκ,u Nκ) are distributions conditioned on
if the communication range κ is large enough and step size x Nκ,u− Ni κ, ∀− xi Nκ,i u Nκ,i satisfying,
η is chosen appropriately. Finally, we present the conditions i i i i
(cid:90) (cid:90)
for Exponential Decay Property to hold and provide several ζ (x ,u ;x ,u )dx du =1.
i Nκ Nκ Nκ Nκ Nκ Nκ
representative examples to verify their feasibility. −i −i i i −i −i
Inthefollowinglemma,wewillshowthatQi K,κ(x
N
iκ,u
N
iκ)
A. Localized Gradient Approximation is a good approximation of Qi (x,u). It also inspires us to
K
Thissubsectionpresentsthelocalizedgradientapproximation obtain a good approximation of the exact policy gradient in
method for the development of the scalable algorithm. We Lemma 1, as shown in the following lemma.
revisitapivotalproperty,theExponentialDecayProperty[37]–
Lemma 2. Under the Exponential Decay Property as defined
[39],withintheLQRframework,whichisessentialforrealizing
in Definition 2, for any truncated Q function in the form of
the localized gradient approximation.
(5), the following holds
First, we show why we can not access the accurate gradient
(a) For any (x,u)∈X ×U,
directly from a local perspective. According to Lemma 1, to
get the gradient for K i, we have, (cid:12) (cid:12)Qi K(x,u)−Qi K,κ(x N iκ,u N iκ)(cid:12) (cid:12)≤C(x,u)ρκ+1.
∇ C(K)=E (cid:2) Q (x,u)∇ logπ (u |x )(cid:3) , (b) Define the following approximated policy gradient,
Ki x,u K Ki Ki i N ir
w Qh Ke (r xe ,w u)e u ds ee pet nh de sfa oc nt t th ha et π gK lo( bu al|x s) ta= te(cid:81) xN i= a1 nπ dK ti h( eu i| gx loN bir a) l. h(cid:98)i(K)= n1 E x∼νK,u∼πK(·|x) j(cid:88) ∈Nκ(cid:104) Qj K,κ(x N jκ,u N jκ)
i
action u, but each agent i only has access to the information (cid:3)
∇ logπ (u |x ) .
confined within their κ-hop neighborhood (Nκ). Consequently, Ki Ki i N ir
i
calculatingtheprecisegradientfromalocalstandpointbecomes where ν K is the stationary distribution for
challenging due to the the communication range limitations. the Markov chain in (1). Assuming that
Remark 1. We set the control range r and the communication
E x∼νK,u∼πK(·|x)∥∇ Kilogπ Ki(u i|x
N
ir)∥ ≤ L i, it follows
range κ as two different variables to separately assess their
that ∥h(cid:98)i(K) − ∇ KiC(K)∥ ≤ CL iρκ+1, with C being a
positive constant.
impacts on the performance degradation respectively. The
analysis setting also follows some previous works such as [32], Proof. PleasereferthedetailinPartA,SectionIIIin[42].
[40]. Note that in real-world scenarios, the communication
However, as the LQR belongs to the family of average
range κ and the control range r are usually the same.
reward MDP, Exponential Decay Property does not hold
However,thesparsitybroughtbythenetworkstructuremakes universally[38],asopposedtothetabularcaseswithdiscounted
it possible to employ localized gradient approximation. It is rewards [39]. We will present the mild conditions for such a
found that the effect from agents far away diminishes with property to hold in Section IV-E and give some representative
the growth of the distance, therefore, when estimating the Q- examplestoshowtheirfeasibility.C(x,u)isafunctionrelated
function and computing the gradient, the influence of distant tothesystemparameterA,B,Q,R,∥x∥ and∥u∥ andthe
∞ ∞
agents might be ignored at an acceptable cost. Such a property explicit form is given in Section IV in [42]. Such a powerful
is called Exponential Decay Property and has been studied in property allows us to approximate the exact global gradient
the tabular RL case [37]–[39]. In the following, we re-clarify locally and accurately in a large network system.
such a property in the context of the LQR setting.
Definition 2 (Exponential Decay Property). For a fixed con- B. Distributed Policy Gradient
troller K, the Exponential Decay Property holds if, ∀i ∈N, InAlgorithm1,wedemonstrateadistributedpolicygradient
x = (x N iκ,x N −κ i), x′ = (x N iκ,x′ N −κ i), u = (u N iκ,u N −κ i), descentmethod.Thenexttheoremshowsthatitcanconvergeto
u′ =(u
N
iκ,u′
N −κ
i), the localized Qi
K
satisfies, aglobalnear-optimalpoint,assumingthateachagenthasaccess
(cid:12) (cid:12)Qi K(x,u)−Qi K(x′,u′)(cid:12) (cid:12)≤C(x,u)ρκ+1,
t ∇o Ka iClo (c Kal )a .p Fp oro rx si im ma pt lio icn ityh(cid:98) ,i( wK e) uf so er dthe toex da ec nt otp eol mic ay xg (r da xd ,i de unt
)
for some C(x,u) > 0 that depends on x and u, and some and µ to denote σ 1(Ψ).
ρ∈(0,1).
3Inthisstudy,weemployasimpleMonte-Carlomethodforpolicyevaluation
This property illustrates that with increasing distance be- andgradientapproximation,asdetailedinSectionVIIIin[42].Thisframework
tween two agents, the mutual dependence on Qi diminishes allowsfortheintegrationofalternativeapproaches,suchasactor-criticmethods.
K Acomprehensiveanalysisofthesemethodologiesispartofourplannedfuture
exponentially. Consequently, we explore a class of truncated work.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 6


1 σ (Q)µ 1 σ (Q)
η <min ( 1 )2 , 1 ,
16 C(K) ∥B∥∥P (∇ C(K))∥(1+∥A−BK∥) 32C(K)∥R+BTP B∥
(cid:124) Mr (cid:123)(cid:122)K
(cid:125) (cid:124) (cid:123)(cid:122)
K
(cid:125)
Tobound∥Ξ K′−ΞK∥ Toguaranteetheconvergence
 (6)
σ (Q)µ 1 −ϖ
−√
ϖ 2−4ϖ ϖ

1 √ , , 1 1 1 2, 1
(cid:124)4C(K)∥B T∥ o( gΥ ua( raC nt( eeK th) a(cid:123))
t(cid:122)
K+ ′′1 is) sC tabilid zin(cid:80)
g
iL iρκ+1
(cid:125) Toguaranteet(cid:124) h(cid:123)
aL
t(cid:122) K(cid:125) ′isstabilizing (cid:124)
C(K2 ′′(cid:123)ϖ )≤(cid:122)2
C(K) (cid:125)

(cid:18) 2∥B∥2C(K(0)) √ C(K(0))(cid:19) C(K(0))
L:= 2σ (R)+ +4 2ζ∥B∥ (7)
n σ (Ψ) µ σ (Q)
1 n
Algorithm 1 Distributed policy gradient descent approximation error) is,
Parameters: step size η, communication range limit κ.
K′ =K−ηP (∇ C(K)), (11)
Initialization: Initial controller K (0), K (0), ..., K (0). Mr K
1 2 n
for step t=1 to T do where P denotes the projection on the sub-space Mr
Mr
for agent i=1 to n do (Definition 1). The one-step distributed gradient descent in
policyevaluation:calculateQi K,κ(x N iκ,u N iκ)usingthe Algorithm 1 is,
communication within κ-hop neighbors, K′ i′ =K i−ηh(cid:98)i(K). (12)
g pr oa lid ci yen imt a pp rop vro emxi em na t:tion3: calculate h(cid:98)i(t−1),
We use K′′ ∈ Rdu×dx ∩Mr to denote the controller matrix
consisting of all K′ i′ ∈Rdi u×di x (like Fig. 1). Our proof mainly
K i(t)=K i(t−1)−ηh(cid:98)i(t−1). consists of four parts.
end for 1) To prove that with a stabilizing K and an appropriate
end for choice of step size η, K′ is stabilizing.
2) To prove that with a stabilizing K′ and an appropriate
choice of step size η, K′′ is stabilizing.
Theorem 1. Assuming that K∗ is the centralized optimal con- 3) To prove that with a stabilizing K, an appropriate choice
troller, if all agents conduct the policy update in Algorithm 1, of step size η and an adequate communication range κ,
and Exponential Decay Property holds during the process, then
C(K′′)≤C(K).
for an appropriate choice of the step-size η that satisfies (6) 4) To prove that the generated controller K(T) converges
and an adequate communication range κ that, to the point close to the centralized optimal controller
√ K∗ but suffers from the degradation exponentially small
κ> 1 log2
dC(K)E(C(K))C(cid:80)
iL i −1, (8) in κ and r.
−logρ ∥P Mr(∇ KC(K))∥2σ 1(Q) Each part leads to one or several restrictions on the choice
givenanyarbitrarilysmallpositiverealnumberϵ,ifweconduct of the step size η, which is shown in (6). We will explain
the process for T steps that, the meaning and the insight for each term in the following
text. It’s important to acknowledge that while the step-size
∥Ξ ∥ C(K(0))−C(K∗)
T ≥
K∗
log , (9)
condition outlined in (6) appears to be contingent upon K
ηµ2σ 1(R) ϵ and varies over time with each update iteration, it is feasible
to establish a universal lower bound through the principle of
the distributed gradient descent enjoys the following perfor-
mance bound 4 monotonicity. We leave the discussion to Section II.C to E in
the supplementary material [42].
C(K(T))−C(K∗)≤ϵ+
∥Ξ K∗∥ (cid:2) M F (C(K(0)))ρκ+1+M F (C(K(0)))ρr(cid:3) . C. Stability and Descent Guarantee
ηµ2σ (R) 1 1 2 2
1
(10) This subsection presents the generated controllers’ stability
and the descent guarantee of the objective function (Step 1-3).
In the following text, we demonstrate the proof process step
First, we illustrate that with an appropriate choice of η and
bystep.5 Theone-stepexactgradientdescent(gradientwithout starting from a stabilizing controller K, K′ obtained by the
one-step exact policy gradient descent (11) is stabilizing as
4The explicit forms of certain terms unmentioned so far (E(·), Υ(·),
well.
M1F1(·),M2F2(·))aredetailedinSectionIin[42].
5Due to the limitation of the length of the main text, some lemmas and
Corollary1. IfKisstabilizing,thesub-levelsetS ,which
corollaryfrompreviousworkswillbedirectlycitedandused.Thecomplete C(K)
andcoherentproofcanbefoundinSectionIIin[42]. is defined as {K′ : C(K′) < C(K),ρ(A−BK′) < 1,K′ ∈JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 7
Mr}, is compact, and the following conclusion hold: There Proof. The gradient descent update gives K′ = K −
exists a constant L which is determined by A, B, Q. R, ηP ∇C(K), and we have,
Mr
Ψ and C(K(0)) that C(K) is a L-smooth function in the L
projection space Mr, and if we choose a step size η ≤1/L, C(K′)≤C(K)+⟨P Mr∇C(K),K′−K⟩+ 2∥K′−K∥2
it is guaranteed that K′ =K−ηP (∇ C(K)) stays in the
Mr K =C(K)+⟨P ∇C(K),−ηP ∇C(K)⟩
stabilizing set S . ( [20], Lemma 7.4 & 7.9)
Mr Mr
C(K) L
+ ∥ηP ∇C(K)∥2
This corollary guarantees the stability of K′ given a 2 Mr
stabilizing K. Note that the non-convexity of the problem L
=C(K)−(η− η2)∥P ∇C(K)∥2 ≤C(K).
introducesadditionalcomplicationsfordeterminingtheexplicit 2 Mr
formofL.However,ithasbeenshowninapreviouswork[20]
that an internal property (coerciveness) of C(K) remedies
the complication. The explicit form of L is provided in (7). For C(K′′)−C(K′), we prove that the C(K′′)−C(K′)
Guarantee on the stability of K′ leads to the fourth term in can be bounded by a polynomial linear combination of ηρκ+1,
(6). (ηρκ+1)2 and (ηρκ+1)3.
Based on the stabilizing K′, we then show the stability of Corollary 4. We can bound the difference between C(K′′)
K′′. Conclusively speaking, if ∥K′′ −K′∥ is small enough, and C(K′) by a polynomial:
K′′ isalsostabilizing.Bythelocalizedgradientapproximation,
we have,
C(K′′)−C(K′)≤ηf 11(C(K))ρκ+1+η2f 12(C(K))ρ2(κ+1)
n +η3f3(C(K))ρ3(κ+1),
∥K′′−K′∥=η(cid:13) (cid:13)(cid:88)
∇
KiC(K)−h(cid:98)i(K)(cid:13)
(cid:13), (13)
1
(14)
i=1 theexplictformsoff 11(C(K)),f 12(C(K))andf 13(C(K))can
where Exponential Decay Property can be utilized to measure be found in Section I.A in [42].
the gap between ∇ KiC(K) and h(cid:98)i(K). Therefore, the differ- Proof. See Appendix B-B.
ence ∥K′′ −K′∥ is determined by two factors: step size η
and the communication range κ, which will be clarified in the By combining the results in the analysis of C(K′′)−C(K′)
following corollary. (Corollary 4) and C(K′)−C(K) (Corollary 3), we obtain
an upper bound for C(K′′)−C(K) in the form of a cubic
Corollary 2. If it is ensured that
polynomial concerning η. It can be represented as:
σ (Q)µ
η < 4C(K)∥B∥(Υ(C(K1 ))+1)C√
d(cid:80)
L
ρκ+1, C(K′′)−C(K)≤−η(ϖ 2η2+ϖ 1η+ϖ 0), (15)
i i
where
with a stabilizing K′ and (13), we can claim that K′′ is
stabilizing as well. ϖ 2 =−f 13(C(K))ρ3(κ+1),
L
Proof. See Appendix B-A. ϖ =−f2(C(K))ρ2(κ+1)+ ∥P (∇ C(K))∥,
1 1 2 Mr K
Knowing that K′ and K′′ are both stabilizing, here we ϖ =∥P (∇ C(K))∥−f1(C(K))ρκ+1.
0 Mr K 1
further provide the descent guarantee of the objective function
C(K).NotethatthestabilityofK′′ isnotequivalenttothefact The following corollary gives the conditions to guarantee that
C(K′′)−C(K)<0.
that the controller’s performance is guaranteed to be improved
by each iteration: If K′′ is stabilizing, C(K′′) is finite, but Corollary 5. If we have
C(K′′) ≤ C(K) is not guaranteed. We illustrate that the √
1 2
dC(K)E(C(K))C(cid:80)
L
communication range limit κ needs to meet a theoretically κ> log i i −1,
lower limit to ensure performance improvement. To see this, −logρ ∥P Mr(∇ KC(K))∥2σ 1(Q)
we keep on decomposing the one-step change of objective it is guaranteed that with all the η in the following range,
function into two parts,
(cid:112)
−ϖ − ϖ2−4ϖ ϖ
C(K′′)−C(K)=[C(K′′)−C(K′)]+[C(K′)−C(K)]. 0<η ≤ 1 1 2 0,
2ϖ
2
The first term represents the approximation error introduced by the distributed gradient descent makes the objective function
thelocalapproximatedgradient,andthesecondtermrepresents C(K) decrease.
the descent brought up by the exact gradient. Next, we analyze
Proof. Note that in 15, ϖ is negative. To obtain meaningful
them separately. 2
values of η that satisfy C(K′′)−C(K)<0, two conditions
Corollary3. ForC(K′)−C(K),followingthedescentlemma must be met: 1) ϖ > 0, and 2) η must be smaller than
0
for an L-smooth function [43], if the step size η is smaller the positive solution of the quadratic equation ϖ η2+ϖ η+
2 1
than 2/L, the controller’s performance by the one-step exact ϖ = 0. The first condition establishes a lower limit for
0
gradient descent enjoys the following improvement: κ, as demonstrated in (8). The second condition imposes an
L additional upper limit on η, which is the fifth term in (6). For
C(K′)−C(K)≤−(η− 2η2)(cid:13) (cid:13)P Mr(∇ KC(K))(cid:13) (cid:13)2 . thedetailedproof,pleaserefertoPartD,SectionIIin[42].JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 8
use a single term M F (C(K))ρκ+1 to represent the bound
1 1
in (14) for simplicity:
C(K′′)−C(K′)≤M F (C(K))ρκ+1.
1 1
Unlike the stability analysis of K′ and K′′, we analyze
the term C(K′)−C(K) to highlight the loss of optimality
caused by the projection of the exact gradient ∇ C(K) on
K
Mr (Definition 1). We additionally define that:
Fig.3:Theblackovalrepresentsthesub-levelsetS .Theredline Kh =K−η∇ KC(K),
C(K)
and the green line represent the one-step move along the direction of
P linM er r( e∇ prK esC en( tK
s
) th) ea dnd iffh(cid:98) er( eK nc) e= ca(cid:80) usn i e= d1 bh(cid:98) yi( tK he), grr ae dsp iee nc tti av pel py r. oT xih me ab til ou ne w deh se cr ee ntK frh omis Kthe wc ito hn ot uro tl ple rr ojeo cb tt ia oi nn .ed Cob ny seo qn ue e- ns tt le yp
,
Kgr had mie an yt
and thus depends on κ. If the blue circle is so large that C(K′′) not necessarily be a sparse blocked matrix. Considering
moves out of S C(K) to the orange area, i.e., the κ is so small such C(K′) − C(K), we keep on separating the term into two
thattheapproximationistooinaccurate,thesystemmaytaketherisk
parts:
of being unstable.
C(K′)−C(K)=[C(K′)−C(Kh)]+[C(Kh)−C(K)].
Remark 2. We clarify the factors that affect system stability
First, we analyze C(Kh)−C(K). In some recent works
by decomposing the one-step gradient descent into two parts.
like[16],[20],ithasbeenprovedthattheglobaloptimacanbe
C(K′)−C(K)representstheperformanceimprovementcaused
reached through the policy gradient if the LQR is centralized.
by the one-step gradient descent along the direction of steepest
In other words, if the controller moves from K to Kh every
descent direction P ∇ C(K) over a subspace Mr. It has
Mr K step, with an appropriate choice of η, the descent process
been shown in previous works that with a small enough step
finally reaches the optima. The one-step analysis is as follows:
size along the projection of the exact gradient direction, the
descent of C(K) can be guaranteed [20] and the generated K′ Corollary 6. If we have
stays in S . Such a guaranteed performance improvement
C(K) σ (Q)
is represented as a dashed red line in Fig. 3. η ≤ 1 , (16)
32C(K)∥R+BTP B∥
However, C(K′′) − C(K′) represents the performance K
disturbance brought about by the inaccurate approximation then we have
of the exact global gradient, i.e., the difference between h(cid:98)i(K)
µ2σ (R)
and ∇ C(K) for each agent i. Specifically, the smaller C(Kh)−C(K)≤−η 1 (C(K)−C(K∗)). (17)
Ki
∥Ξ ∥
the communication range κ is, the less accurate each agent K∗
approximates the exact gradient using the local information, Proof. The methodology for the proof is akin to that used
andthemorelikelythecontrollerbecomesunstabilizingduring in Lemma 11 of [16] for a centralized LQR problem. For a
thedescentprocess.Intuitivelyspeaking,theconditiononκ(8) comprehensive elaboration, refer to Part E1, Section II in [42].
shows that we have to make a gradient approximation h(cid:98)i(K) This leads to the second term in (6).
accurateenoughsothatK′′ doesnotdeviatefromK′ toomuch.
Otherwise, the objective function C(K) is not guaranteed to Then, we analyze C(K′)−C(Kh). In this term, the factor
keep decreasing. ∥P Mr(∇ KC(K)) − ∇ KC(K)∥ plays an important role. It
characterizes the effect of projecting the gradient to a subspace
Note that as the descent of the objective function,
and causes the degradation term concerning ρr in the ultimate
∥P (∇ C(K))∥ may decrease, and the required low limit
Mr K
controller’s performance gap (10). Therefore, the structural
for κ in Corollary 5 may increase accordingly. If κ is fixed,
property of ∇ C(K) is crucial. We show that under the
the final controller K(T) may suffer from a minimum fixed K
Exponential Decay Property, the matrix ∇ C(K) enjoys a
performance degradation compared with the optimal controller K
K∗. We discuss the case in Claim 1 and Claim 2 in the specialdecayingpropertycalled(C ∇K,ρ)-spatiallyexponential
decaying (SED).
supplementary material [42].
Definition 3. [33] [Spatially Exponential Decaying (SED)]
A n-by-n blocked matrix X is (c,γ)-spatially exponential
D. Convergence and Degradation Quantification decaying (SED) if,
Inthissubsection,wequantifytheperformancegapbetween ∥[X] ij∥≤c·γdist(i,j), ∀i,j ∈N,
the ultimate controller K(T)∈Mr obtained by Algorithm 1
where 0<γ <1, c>0.
and the centralized optimal controller K∗ (step 4).
We keep on using the technique of decomposing C(K′′)− The constant C is associated with n and other system
∇K
C(K) into C(K′′)−C(K′) and C(K′)−C(K). As shown parameters like A, B, R. Due to its complexity, the explicit
in Corollary 4, we use the Exponential Decay Property formulation is detailed in Part C, Section IV in [42]. The next
(Definition 2) to bound C(K′′)−C(K′). Since ρ<1, we can corollary bound the difference between C(K′) and C(Kh).JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 9
Corollary 7. Itissignificanttoseethatthegraphdiameterisnotincluded
in the theoretical performance gap in (10), though if we set
C(K′)−C(Kh)≤M F (C(K))ρr. (18)
2 2 r as the graph diameter and let κ equal to r, the multi-agent
problem will degenerate into a single-agent problem, and the
Proof. SED describes the exponentially decaying property of
second and third error terms should be subtracted. The reason
the (i,j)-th sub-matrix’s norm as dist(i,j) grows. So the
isthatwithinourtheoreticalframework,eachnodeispresumed
key point of the proof is that we can bound ∥∇ C(K)−
K
P (∇ C(K))∥byatermrelatedtoρr,becausethedistance tooperatewithinaninfinitelyexpansivenetwork.Thereforethe
Mr K
diameter of the graph does not appear in our final conclusion.
between the two agents corresponding to it is at least r.
To incorporate the graph’s diameter into the final conclusion,
(cid:13) (cid:13)
(cid:13)∇ KC(K)−P Mr(∇ KC(K))(cid:13) it may be essential to consider the graph’s specific structure
n and the boundary conditions. We consider it to be a topic for
(cid:88) (cid:88) (cid:13) (cid:13)
≤ (cid:13)[∇ KC(K)] ij(cid:13)
F
our future work.
i=1j∈N−r By setting r and κ to the graph’s diameter, the multi-
i
n agent scenario simplifies to a single-agent case, the second
(cid:88) (cid:88)
≤ C ρdist(i,j) ≤nmax(|N−r|)C ρr. and third error terms should be eliminated [16]. But it is
∇K
i
i ∇K
i=1j∈N−r not the case and the graph diameter does not factor into the
i
theoretical performance gap as outlined in (10). It stems from
Using this term, we can bound C(K′)−C(Kh) employing a
our theoretical assumption that each node operates within a
technique similar to that in (21). The remainder of the proof
boundlessly network, thereby rendering the graph’s diameter
is detailed in Part E2, Section II in [42].
irrelevant to our final deductions. To integrate the graph
diameter into our conclusions, a detailed examination of the
By combining the analysis of C(Kh)−C(K) and C(K′)−
graph’sinherentstructureandedgeconditionsmayberequired.
C(Kh), we obtain the one-step performance gap compared
This represents a potential topic for our future research.
with K∗ :
µ2σ (R)
C(K′′)−C(K)≤−η 1 (C(K)−C(K∗)) E. Conditions for Exponential Decay Property
∥Ξ ∥
K∗
This subsection presents the conditions for the Exponential
+M F (C(K))ρκ+1+M F (C(K)))ρr,
1 1 2 2
Decay Property (Definition 2) to hold in a networked LQR
Inductively we have, setting.
C(K(T))−C(K∗) Lemma 3. At time t, for any node j, suppose that i,i′ are κ,
κ+1-hop neighbors of j, respectively, for any K generated
≤(cid:0) 1−ηµ2σ 1(R)(cid:1)T (C(K(0))−C(K∗)) from the gradient descent process, if there exists ρ ∈ (0,1)
∥Ξ ∥
K∗ such that the following inequality holds,
∥Ξ ∥
+ ηµ2σK 1(∗ R)[M 1F 1(C(K(0)))ρκ+1+M 2F 2(C(K(0)))ρr], (cid:13) (cid:13)[(A−BK)t] i′j(cid:13) (cid:13)≤ρ(cid:13) (cid:13)[(A−BK)t] ij(cid:13) (cid:13),
Note that both F 1(C(K)) and F 2(C(K)) monotonically then the (C Qi ,ρ) Exponential Decay Property holds.
K
increases with the increase of C(K), so F (C(K∗)) ≤
1 This lemma indicates some specific spatial decay character-
F (C(K(T))) ≤ F (C(K(0))), F (C(K∗) ≤ F (K(T)) ≤
1 1 2 2 istics brought by the structure of the state transition matrix
F (C(K(0))). Provided a minimum iteration number T as (9),
2 A−BK. However, determining whether the condition can be
we obtain the final result (10) as shown in Section B-C in the
satisfied is challenging, even when provided with the graph
Appendix.
structure and transition dynamics model. This difficulty arises
Remark 3. As shown in Theorem 1, the performance degrada- as it is impractical to iterate over all t for verification. So in
tioncompared withtheoptimal controllerconsistsof twoparts. the following lemma, a stricter but more intuitive condition
The term related to ρκ+1 represents the degradation introduced is presented, which clarifies the nature of the property in the
by the gradient approximation. Each agent only uses the LQR setting more directly.
informationwithinitsκ-hopneighborhoodtomakearelatively
Lemma 4. With a stabilizing controller K ∈ Mr, for any
accurateapproximationoftheexactgradient.Thentheyconduct
agent i and agent j that are κ-hop neighbors, if there exist
the distributed policy gradient descent locally, which causes
constantsC >0,D >0,andρ∈(0,1)suchthat|Wt (r)|≤
errors in every iteration. The term related to ρr represents i→j
CDtρκ and [A−BK] · D ≤ 1 holds, then the (C ,ρ)-
the degradation brought by the controller’s truncation (or Qi K
Exponential Decay Property holds.
projection). Each agent implement its control input depending
on its neighbors within r-hop, while in centralized controlling, The detailed proof of these two lemma and the explicit
each agent affects any other agent no matter how far away form of C are deferred to Section IV in [42]. Wt (r) is
Qi i→j
K
they are. Due to the sparsity of the system matrices A, B, defined as a set that contains all the walks from i to j with
Q, R, the degradation decreases to 0 exponentially with the length t (Definition 7, Appendix A). Such a property yields
growth of communication range limit κ and control range r, the intuition that the decay property results from the weak
so at least we can still obtain a near-optimal controller. interaction between agents and the low connectivity of theJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 10
Fig. 4: The diagram of the four representative graphs: line, circle, 2-ary tree and 4-regular grid.
underlying network. In particular, on the one hand, [A−BK] We set up different experiments to verify the impact of
represents the maximum impact between two agents in the the communication range κ and control range r, respectively.
transitionprocess.Ontheotherhand,thenetworkshouldbeas In the experiments regarding r, we set κ to be maximum,
sparseaspossiblesothatD issmallenough.Withafixedt,the which means the gradient approximation is accurate. In the
numberoftheavailablewalks(|Wt |)issupposedtodecrease experimentsregardingκ,r issettobemaximum,whichmeans
i→j
exponentially with the growth of the distance κ between i and that there will be no gradient truncation.
j. The conditions above are consistent with the assumptions As is shown in Fig. 5, we explore the relative cost error
in [38] concerning the tabular case and average-reward setting
C(K(T))−C(K∗),
under varying communication range limits, κ,
C(K∗)
in RL. and control range, r. On the left, with r maximized to prevent
TheseadditionalconditionsshowthattheExponentialDecay gradient truncation and a Gaussian-distributed approximation
Property does not hold uniformly in the networked LQR error added to ∇ C(K), we examine κ ∈ [2,3,5,10,20].
Ki
settings. Fortunately, the cardinality of Wt (r) (|Wt (r)|) On the right, setting κ to its maximum ensures every agent
i→j i→j
has an explicit mathematical expression in certain regular and i has access to the exact gradient, with r varying within
representative graphs where we can directly calculate C, D [2,3,5,10,20]. For example, at r = 2, an agent’s dynamics
and ρ, so there are plenty of cases where we can verify and depend only on its 2-hop neighbors, whereas r =20 equates
apply this property. In Appendix V-B, we present the analytic to a centralized LQR scenario, allowing optimal convergence.
and numerical results in some representative graph structures. We also show the performance when κ and r are less than the
diameter of the graph, the results are shown in Fig.3 in the
Remark 4. The conditions in Lemma 4 are stricter than those
supplementary material [42].
in Lemma 3. In the proof of Lemma 4 (Part B, Section IV
In both semi-logarithmic plots with logarithmic y-axis
in [42]), the condition [A−BK]·D ≤1 is supplemented to
in Fig. 5, the curve of relative cost error exhibits strong
construct a convergent geometric sequence summation, which
linear properties, which aligns with the theoretical results
is not necessary if the system is stable. We leave the finding
shown as (10) in Theorem 1. The results verify our main
of a more relaxed condition as a topic of future work.
conclusion that, as r and κ increase, the performance gap
between K(T) and K∗ decreases exponentially. With a finite
V. SIMULATIONRESULTS
communication range κ, it is possible to design a scalable
In this section, we present a set of examples to demonstrate
and decentralized gradient descent algorithm to obtain a near-
theresultsreportedinthispaper.Wechoosefourrepresentative
optimal controller. Our future work will delve into scalable
graphs: line, circle, tree, and 4-regular grid. The diagram of
model-free algorithms, focusing on Monte-Carlo and Actor-
these four types of graphs is shown in Fig. 4. The combination
Critic methods. The detailed discussion and more results can
of these graphs can represent a certain quantity of random
be found in Section V in [42].
graphs.
B. Case Studies for Exponential Decay Property
A. Simulation Results for Distributed Policy Gradient Descent
We give some typical examples and numerical results for
The system is with parameters (A,B), where B=I and A
Lemma 4 to show that the Exponential Decay Property holds
is initially consistent with the adjacency matrix of each graph
in these four representative graphs. Note that in Lemma 4,
and is scaled by 0.9 until ρ(A)<1, then the initial controller
it requires that |Wt (r)| ≤ CDtρκ and [A−BK]·D ≤ 1.
K(0) can be set to be 0. The cost matrices (Q,R) are set i→j
For the first condition, we present the theoretical bound of
to be identity with appropriate dimensions. We set step size
|Wt (r)|andtheexactvalueofC,D andρinTABLEI.The
η = 0.001. The noise covariance matrix Ψ is set to be 0.5I. i→j
second condition is an additional restriction for the stability
ThetotaliterationT issettobe4000.Inthelineandthecircle
of the whole system and the detailed discussion is deferred to
graph, we set 99 nodes. In the 2-ary tree graph, we set 127
Section VI in [42].
nodes to construct an 8-layer full binary tree. In the 4-regular
grid, we set 11×11=121 nodes. We conduct the distributed Remark 5. We present the theoretical bound and the true
policy gradient descent as Algorithm 1. numberofwalksin4-regulargridinFig. 6andmoreresultsareJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 11
Fig. 5: Relative performance gap compared to the optimal controller K∗ with different communication range limit κ and different control
range r. In semi-logarithmic plots with a logarithmic y-axis, the relative cost error curve is linear, aligning with theoretical results, and
confirming the main conclusion that the performance gap between K(T) and K∗ decreases exponentially as r and κ increase.
Fig. 6: The number of the true walks |W | and the theoretical bound in a 21×21 4-regular grid. On the left, we fix t=20 and vary κ.
i→j
In the right, We fix κ=20 and vary t.
TABLE I: The theoretical bound of the number of the walks
generated by the distributed policy gradient descent process
in four representative graphs: lines, circle, f-ary tree, and
can be guaranteed to stabilize the system.
4-regular grid.
As the first attempt to providing scalability and optimality
in the distributed policy gradient method of networked LQR,
GraphType Bound C D ρ
our work has opened up several directions for future research.
Line e·[(3e)23 ]t·[e−1 2]κ e (3e)3 2 e−1 2 We aim to develop zero-order and first-order optimization
2 2
algorithms, incorporating theories like Monte-Carlo estimation
Cycle 2e n(3 2e2)t·e−κ 2e n 3 2e2 e−1 2 as in [16], [21], [44] and Actor-Critic as in [26]. A focus
f-aryTree (2e2f1 2)t(e−1f− 21 )κ 1 2e2f1 2 e−1f−1 2 includes finite-sample analysis during sample-based policy
evaluation and examining the sample complexity in model-
4-regularGrid 2e(5e 22 )t(e−1)κ 2e 5e 22 e−1 free contexts, as seen in [16], [45]. Additionally, we seek a
deeper understanding and tighter analysis of localized gradient
approximation in networked LQ control, potentially integrating
deferred to Section VI in [42]. Note that the tighter theoretical recent graph theory insights.
bounds for the number of walks in these graphs could be
expected, and are left as our future work. ACKNOWLEDGEMENT
The authors would like to thank Prof. Kaiqing Zhang for
helpful discussions and feedback.
VI. CONCLUSIONSANDFUTUREWORKS
This paper has provided provable guarantees that the APPENDIXA
distributed policy gradient descent method converges to the HELPERDEFINITIONSANDLEMMAS
near-optimalsolutionofnetworkedLQRcontrolproblems.The In this appendix, we provide a series of auxiliary definitions
near-optimality represents the performance gap exponentially and lemmas aimed at enhancing the reader’s comprehension.
small in the communication range limit κ and the control We recall the state transition dynamics from the global
range r. We provided the conditions for localized gradient perspective:
approximationandverifyourpropositioninsomerepresentative
x(t+1)=(A−BK)x(t)+ϵ(t),
graphs. Additionally, we also showed that the controllersJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 12
where ϵ(t) = w(t) + σ Bη(t) ∼ N(0,Ψ) and we define where the second-to-last inequality results from Exponential
0
Ψ=Φ+σ2·BBT to simplify the notation. Decay Property (Definition 2). And we have
0
Subsequently, we introduce a set of definitions to elucidate
∥A−BK′∥=∥A−BK+B(K−K′)∥
the relationship between the matrix structure and the inherent
architecture of the network system. First, we recall a term ≤∥A−BK∥+η∥B∥∥P Mr(∇C(K))∥
√
coined to characterize the sparse nature of the matrices within ≤∥A−BK∥+η d∥B∥∥∇C(K)∥
this specific context.
≤∥A∥+∥B∥∥K∥
Next,wewilldemonstratethesparsityofthestructurefroma
(cid:115)
local perspective. To show how two agents interact from agent √ C(K) ∥R+BTP B∥(C(K)−C(K∗))
+η d∥B∥ K
i’s point of view, we introduce a new definition as follows: σ (Q) σ (Ψ)
1 1
(cid:115)
Definition 4. [Local Spatially Exponential Decaying (L- √ 1 (cid:16) ∥R+BTP B∥(C(K)−C(K∗))
SED)] A matrix X
i
∈ Rn1×n2 related to agent i is (c i,γ i)- ≤∥A∥+ d∥B∥
σ (R)
K
σ (Ψ)
1 1
local spatially exponential decaying (SED) if, ∥[X ] ∥ ≤
i mn (cid:17)
c γ dist(m,i)+dist(i,n), where 0<γ <1, c >0. +∥BTP A∥
i i i i K
(cid:115)
Next, we will highlight certain properties of matrices that
C(K) ∥R+BTP B∥(C(K)−C(K∗))
are closed under the SED condition. +∥B∥ K
σ (Q) σ (Ψ)
1 1
Lemma 5. Suppose two n-by-n block matrices X and Y are
:=Υ(C(K)),
square matrices of the same dimension, and they are (x,γ)- (20)
SED and (y,γ)-SED respectively, then the matrix X+Y is where the bounds of ∥K∥, ∥∇ C(K)∥, ∥P ∥ are given
K K
(x+y,γ)-SED and the matrix X·Y is (nxy,γ)-SED. in [16] (Lemma 13 & 25). So that
Lemma 6. Suppose X is (x,γ)-SED, Y ∈ Mκ and σ (Q)µ
max ij∥[Y] ij∥ = y¯, the XY is (nxy¯eγκ,γ)-SED, X + Y
∥K′′−K′∥≤ 4C(K)∥B∥(1
∥A−BK′∥+1)
is (x+ y(cid:98) ,γ)-SED.
e−γκ Based on Lemma 16 in [16], and given the established
Lemma 7. Suppose X ∈Mκx and max ij∥[X] ij∥=x¯, Y ∈ stabilization of K′, it follows that K′′ is also stabilizing. It
Mκy and max ij∥[Y] ij∥=y¯, then XY ∈Mκx+κy. leads to the third term in (6).
Next, we introduce a set of definitions from graph theory,
aiming to clearly outline the necessary conditions for the B. Proof of Corollary 4
Exponential Decay Property, as specified in Lemma 4.
Note that C(K) = E xTP x+σ2Tr(R) ( [26],
x∼N(0,Ψ) K 0
Definition 5. i→n →n →...→n →j is defined as Proposition 3.1). For any K, the noise remains the same
1 2 t−1
a walk of length t from i to j, where n , n , ..., n ∈N and i.i.d. When we calculate the difference between two
1 2 t−1
objective functions, the effect of noise can be canceled
Definition 6. For convenience, we define an expanded connec-
out. So we can define a new state dynamics without noise,
tion graph G(r)=(N,E(r)) based on the underlying graph G.
where x′′ = (A − BK′′)tx for all t ≥ 0 and Ξ =
In G(r), agent i is connected to all its r-hop neighbors defined E t [(cid:80) x′′(x′′)T]. K′′
in G. x∼N(0,Ψ) t≥0 t t
Definition 7. We define Wt (r) as a set that contains all the C(K′′)−C(K′)=E x∼N(0,Ψ)[xT(P K′′ −P K′)x]
i→j
(cid:88)
walks from i to j with length t, in a defined graph E(r). =E A (x′′)
x∼N(0,Ψ) K′,K′′ t
t≥0
APPENDIXB √
PROOFOFSEVERALCOROLLARIES ≤2 d √∥Ξ′ K′ −Ξ′ K∥∥E K′∥∥K′′−K′∥ (21)
+2 d∥Ξ′ ∥∥E ∥∥K′′−K′∥
A. Proof of Corollary 2 √ K K′
BytheequivalenceoftheFrobeniusnormandtheEuclidean + √d∥Ξ′ K′ −Ξ′ K∥∥R+BTP′ KB∥(∥K′′−K′∥)2
norm, we have + d∥Ξ′ ∥∥R+BTP′ B∥(∥K′′−K′∥)2,
K K
∥K′′−K′∥=η∥P Mr(∇ KC(K))−h(cid:98)(K)∥
where E = (R+BTP B)K−BTP A, and A is
K K K K′,K′′
≤η∥P Mr(∇ KC(K))−h(cid:98)(K)∥
F
the advantage function defined as A K,K′(x) = 2xT(K′ −
n K)TE x + xT(K′ − K)T(R + BTP B)(K′ − K)x.The
(cid:88) K K
=η ∥∇ KiC(K)−h(cid:98)i(K)∥ F last inequality follows the cost difference lemma in [16].
i=1 Exponential Decay Property is used to bound ∥K′′ − K′∥
≤η√ d(cid:88)n ∥∇ KiC(K)−h(cid:98)i(K)∥≤ηC√ d(cid:88) L iρκ+1 a an sd 19∥Ξ anK d′′ ∥− ΞΞ KK ′′′ −∥ b Ξy Kt ′e ∥rm canηρ bκ e+1 b. ou∥ nK d′ e′ d− inK t′ h∥ eis sab mo eun wde ad
y
i=1 i based on Lemma 16 in [16],
σ (Q)µ
≤ 1
C(K) ∥B∥(∥A−BK∥+1)
4C(K)∥B∥(Υ(C(K))+1) ∥Ξ −Ξ ∥≤4( )2 ∥K′−K∥.
(19) K′ K σ (Q) µ
1JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 13
Besides, we have two additional terms to bound with terms [6] Y. Yan, X. Li, X. Qiu, J. Qiu, J. Wang, Y. Wang, and Y. Shen,
related to K: ∥E ∥, ∥Ξ ∥. Here we present the results “Relativedistributedformationandobstacleavoidancewithmulti-agent
K′ K′
reinforcementlearning,”inProc.IEEEInt.Conf.Robot.Autom. IEEE,
directly.
2022,pp.1661–1667.
C(K′) C(K) [7] C.Pan,Y.Yan,Z.Zhang,andY.Shen,“Flexibleformationcontrolusing
∥Ξ ∥≤ ≤ . (22) hausdorffdistance:Amulti-agentreinforcementlearningapproach,”in
K′
σ 1(Q) σ 1(Q) Proc.Eur.SignalProcess.Conf. IEEE,2022,pp.972–976.
[8] Y. Yan, Y. Dong, K. Ma, and Y. Shen, “Approximation error back-
propagationforq-functioninscalablereinforcementlearningwithtree
(cid:118)
∥E
∥≤(cid:117) (cid:117) (cid:116)(C(K)−C(K∗))(cid:13) (cid:13)R+BT σC 1( (K Ψ) )B(cid:13) (cid:13)
,
(23)
Sd ie gp ne an ld Pen roc ce ess str .uct Iu Er Ee, E” ,i 2n 02P 3r ,oc p. p.IE 1E –5E .Int. Conf. Acoust., Speech and
K′ σ (Ψ) [9] X. Shen, L. Xu, Y. Liu, and Y. Shen, “A theoretical framework for
1 relativelocalization,”IEEETransactionsonInformationTheory,2023.
[10] X.Chen,G.Qu,Y.Tang,S.Low,andN.Li,“Reinforcementlearning
Then, all the terms in 21 can be bounded by ηρκ+1, terms
forselectivekeyapplicationsinpowersystems:Recentadvancesand
related to C(K) and other system parameters. The detailed futurechallenges,”IEEETrans.SmartGrid,2022.
discussion and the explicit form of the polynomial are deferred [11] K.Zhang,Z.Yang,H.Liu,T.Zhang,andT.Basar,“Fullydecentralized
multi-agentreinforcementlearningwithnetworkedagents,”inProc.Int.
to the Part D, Section II in [42].
Conf.Mach.Learn. PMLR,2018,pp.5872–5881.
[12] K. Zhang, Z. Yang, and T. Bas¸ar, “Multi-agent reinforcement learn-
ing: A selective overview of theories and algorithms,” Handbook of
C. IterationProcessoftheDistributedPolicyGradientDescent
ReinforcementLearningandControl,pp.321–384,2021.
Inductively, we have, [13] Y. Tian, K. Zhang, R. Tedrake, and S. Sra, “Can direct latent
modellearningsolvelinearquadraticgaussiancontrol?”arXivpreprint
C(K(T))−C(K∗) arXiv:2212.14511,2022.
[14] K. Zhang, B. Hu, and T. Basar, “On the stability and convergence
≤(cid:0) 1−ηµ2σ 1(R)(cid:1)(cid:2)
C(K(T
−1))−C(K∗)(cid:3) o quf ar do rb au tis ct sa yd stv ee mrs sa ,”ria Pl ror ce .in Af do vrc ae nm cee sn it nl Nea er un ri an lg I: nA f.Pc ra os ce ess st .u Sd yy sto .,n voli ln .e 3a 3r
,
∥Ξ ∥
K∗ pp.22056–22068,2020.
+M 1F 1(C(K(0)))ρκ+1+M 2F 2(C(K(0)))ρr [15] K.Zhang,Z.Yang,andT.Basar,“Policyoptimizationprovablyconverges
tonashequilibriainzero-sumlinearquadraticgames,”Proc.Advances
≤(cid:0) 1−ηµ2σ 1(R)(cid:1)T(cid:2) C(K(0))−C(K∗)(cid:3) inNeuralInf.Process.Syst.,vol.32,2019.
∥Ξ ∥ [16] M.Fazel,R.Ge,S.Kakade,andM.Mesbahi,“Globalconvergenceof
K∗
policygradientmethodsforthelinearquadraticregulator,”inProc.Int.
+T (cid:89)−1 (1−ηµ2σ 1(R)
)τ[M F (C(K(0)))ρκ+1
Conf.Mach.Learn. PMLR,2018,pp.1467–1476.
∥Ξ ∥ 1 1 [17] L. Bakule, “Decentralized control: An overview,” Annual reviews in
τ=0 K∗ control,vol.32,no.1,pp.87–98,2008.
+M F (C(K(0)))ρr] [18] A. L. Bazzan, “Opportunities for multiagent systems and multiagent
2 2
reinforcementlearningintrafficcontrol,”AutonomousAgentsandMulti-
≤(cid:0) 1−ηµ2σ 1(R)(cid:1)T (C(K(0))−C(K∗)) AgentSystems,vol.18,no.3,pp.342–375,2009.
∥Ξ ∥ [19] M.Pipattanasomporn,H.Feroze,andS.Rahman,“Multi-agentsystemsin
K∗
adistributedsmartgrid:Designandimplementation,”in2009IEEE/PES
∥Ξ ∥
+ K∗ [M F (C(K(0)))ρκ+1+M F (C(K(0)))ρr]. PowerSystemsConferenceandExposition. IEEE,2009,pp.1–8.
ηµ2σ (R) 1 1 2 2 [20] J.Bu,A.Mesbahi,M.Fazel,andM.Mesbahi,“Lqrthroughthelensof
1
firstordermethods:Discrete-timecase,”arXivpreprintarXiv:1907.08921,
Provided 2019.
[21] Y. Li, Y. Tang, R. Zhang, and N. Li, “Distributed reinforcement
∥Ξ ∥ C(K(0))−C(K∗)
T ≥ K∗ log , learning for decentralized linear quadratic control: A derivative-free
ηµ2σ (R) ϵ policyoptimizationapproach,”IEEETrans.Auto.Cont.,vol.67,no.12,
1
pp.6429–6444,2021.
then we have [22] L.Ljungetal.,“Theoryfortheuser,”SystemIdentification,1987.
[23] S.Dean,H.Mania,N.Matni,B.Recht,andS.Tu,“Onthesamplecom-
C(K(T))−C(K∗)≤ϵ
plexityofthelinearquadraticregulator,”FoundationsofComputational
Mathematics,vol.20,no.4,pp.633–679,2020.
+ ∥Ξ K∗∥ (cid:2) M F (C(K(0)))ρκ+1+M F (C(K(0)))ρr(cid:3) . [24] S.TuandB.Recht,“Least-squarestemporaldifferencelearningforthe
ηµ2σ 1(R) 1 1 2 2 linearquadraticregulator,”inProc.Int.Conf.Mach.Learn. PMLR,
2018,pp.5005–5014.
[25] M.Simchowitz,H.Mania,S.Tu,M.I.Jordan,andB.Recht,“Learning
REFERENCES
withoutmixing:Towardsasharpanalysisoflinearsystemidentification,”
[1] A. S. Berahas, R. Bollapragada, and E. Wei, “On the convergence inConf.Learn.Theo. PMLR,2018,pp.439–473.
ofnesteddecentralizedgradientmethodswithmultipleconsensusand [26] Z.Yang,Y.Chen,M.Hong,andZ.Wang,“Provablyglobalconvergence
gradientsteps,”IEEETrans.SignalProcess.,vol.69,pp.4192–4203, ofactor-critic:Acaseforlinearquadraticregulatorwithergodiccost,”
2021. Proc.AdvancesinNeuralInf.Process.Syst.,vol.32,2019.
[2] T.-H.Chang,M.Hong,andX.Wang,“Multi-agentdistributedoptimiza- [27] K. Zhang, B. Hu, and T. Basar, “Policy optimization for h2 linear
tionviainexactconsensusadmm,”IEEETrans.SignalProcess.,vol.63, controlwithh∞robustnessguarantee:Implicitregularizationandglobal
no.2,pp.482–497,2015. convergence,”SIAMJournalonControlandOptimization,vol.59,no.6,
[3] K.Gu,Y.Wang,andY.Shen,“Cooperativedetectionbymulti-agent pp.4081–4109,2021.
networksinthepresenceofpositionuncertainty,”IEEETrans.Signal [28] X. Guo and B. Hu, “Global convergence of direct policy search for
Process.,vol.68,pp.5411–5426,2020. state-feedbackhrobustcontrol:Arevisitofnonsmoothsynthesiswith
[4] ——, “A quasi-coherent detection framework for mobile multi-agent goldstein subdifferential,” in 36th Conference on Neural Information
networks,”IEEETrans.SignalProcess.,vol.69,pp.6416–6430,2021. ProcessingSystems,NewOrleans,LA,Nov,vol.28,2022.
[5] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van [29] B.Hu,K.Zhang,N.Li,M.Mesbahi,M.Fazel,andT.Bas¸ar,“Towarda
Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, theoreticalfoundationofpolicyoptimizationforlearningcontrolpolicies,”
M.Lanctotetal.,“Masteringthegameofgowithdeepneuralnetworks AnnualReviewofControl,Robotics,andAutonomousSystems,vol.6,
andtreesearch,”nature,vol.529,no.7587,pp.484–489,2016. pp.123–158,2023.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 14
[30] M.RotkowitzandS.Lall,“Acharacterizationofconvexproblemsin
decentralizedcontrol,”IEEEtransactionsonAutomaticControl,vol.50,
no.12,pp.1984–1996,2005.
[31] M.GagraniandA.Nayyar,“Thompsonsamplingforsomedecentralized
controlproblems,”in2018Conf.Deci.Cont. IEEE,2018,pp.1053–
1058.
[32] S.Shin,Y.Lin,G.Qu,A.Wierman,andM.Anitescu,“Near-optimal
distributed linear-quadratic regulator for networked systems,” arXiv
preprintarXiv:2204.05551,2022.
[33] R. Zhang, W. Li, and N. Li, “On the optimal control of net-
worklqrwithspatially-exponentialdecayingstructure,”arXivpreprint
arXiv:2209.14376,2022.
[34] D. S. Bernstein, R. Givan, N. Immerman, and S. Zilberstein, “The
complexity of decentralized control of markov decision processes,”
Mathematicsofoperationsresearch,vol.27,no.4,pp.819–840,2002.
[35] R.S.Sutton,D.McAllester,S.Singh,andY.Mansour,“Policygradient
methodsforreinforcementlearningwithfunctionapproximation,”Proc.
AdvancesinNeuralInf.Process.Syst.,vol.12,1999.
[36] D.Silver,G.Lever,N.Heess,T.Degris,D.Wierstra,andM.Riedmiller,
“Deterministic policy gradient algorithms,” in Proc. Int. Conf. Mach.
Learn. PMLR,2014,pp.387–395.
[37] G.QuandN.Li,“Exploitingfastdecayingandlocalityinmulti-agent
mdpwithtreedependencestructure,”inConf.Deci.Cont. IEEE,2019,
pp.6479–6486.
[38] G. Qu, Y. Lin, A. Wierman, and N. Li, “Scalable multi-agent rein-
forcementlearningfornetworkedsystemswithaveragereward,”Proc.
AdvancesinNeuralInf.Process.Syst.,vol.33,pp.2074–2086,2020.
[39] G. Qu, A. Wierman, and N. Li, “Scalable reinforcement learning of
localizedpoliciesformulti-agentnetworkedsystems,”inLearn.Dyna.
Cont. PMLR,2020,pp.256–266.
[40] C. Alfano and P. Rebeschini, “Dimension-free rates for natural pol-
icy gradient in multi-agent reinforcement learning,” arXiv preprint
arXiv:2109.11692,2021.
[41] Y. Zhang, G. Qu, P. Xu, Y. Lin, Z. Chen, and A. Wierman, “Global
convergence of localized policy iteration in networked multi-agent
reinforcementlearning,”arXivpreprintarXiv:2211.17116,2022.
[42] Y.YanandY.Shen,“Supplementarymaterial:Distributedpolicygradient
for linear quadratic networked control with limited communication
range,”https://github.com/Chaojidahoufeng/TSP near optimality/blob/
main/supplementary material.pdf.
[43] A.Beck,First-ordermethodsinoptimization. SIAM,2017.
[44] K. Zhang, X. Zhang, B. Hu, and T. Basar, “Derivative-free policy
optimizationforlinearrisk-sensitiveandrobustcontroldesign:Implicit
regularizationandsamplecomplexity,”AdvancesinNeuralInformation
ProcessingSystems,vol.34,pp.2949–2964,2021.
[45] S. Cen, C. Cheng, Y. Chen, Y. Wei, and Y. Chi, “Fast global conver-
genceofnaturalpolicygradientmethodswithentropyregularization,”
OperationsResearch,vol.70,no.4,pp.2563–2578,2022.