On a Neural Implementation of Brenier’s Polar Factorization
NinaVesseron1 MarcoCuturi21
Abstract mapM. Thispaperaimstoprovideapracticalapproachto
recoverapproximationsofthepotentialuandvector-valued
In 1991,Brenierprovedatheoremthatgeneral-
mapM usingexclusivelysamplesx ρandtheirassoci-
i
izestheQRdecompositionforsquarematrices– ∼
atedimagesF(x ). Wealsohighlighthowareliablepolar
i
factoredasPSD unitary–toanyvectorfield
F :Rd Rd. Th× etheorem,knownasthepolar factorizationsolver,coupledwithanestimationofastochas-
→ ticgeneratorthatmimicsthemeasure-valuedinversemap
factorizationtheorem,statesthatanyfieldF can M−1,canbeusedtostudythegradientfieldofnon-convex
berecoveredasthecompositionofthegradient
landscapes. Weconsider,inparticular,thecasewherethe
ofaconvexfunctionuwithameasure-preserving
fieldF ofinterestisthegradient,withrespecttotheparam-
map M, namely F = u M. We propose a
∇ ◦ etersofaneuralarchitecture,ofalearningloss. Notethat
practicalimplementationofthisfar-reachingthe-
thepolarfactorizationtheoremshouldnotbeconfusedwith
oreticalresult,andexplorepossibleuseswithin
themajortheoremfromoptimaltransportcloselyassociated
machinelearning. Thetheoremiscloselyrelated
with Brenier, which we recall in §2. That theorem states
tooptimaltransport(OT)theory,andweborrow
thattheMongeformulationoftheoptimaltransport(OT)
fromrecentadvancesinthefieldofneuraloptimal
problem,whichseeksthepush-forwardmaptransportinga
transporttoparameterizethepotentialuasanin-
measureontoanotherwiththeleastmeandisplacements(as
putconvexneuralnetwork. ThemapM canbeei-
measuredwithsquarednorms)issolvedbythegradientof
therevaluatedpointwiseusingu∗,theconvexcon-
aconvexpotential.
jugateofu,throughtheidentityM = u∗ F,
∇ ◦
orlearnedasanauxiliarynetwork. BecauseM
is, ingeneral, notinjective, weconsiderthead- ExistingImplementations. Shortlyafter(Brenier,1991),
ditional task of estimating the ill-posed inverse Benamou and Brenier (1994) proposed a numerical ap-
mapthatcanapproximatethepre-imagemeasure proachtodecomposeavectorfield,withanexplicitEulerian
M−1 usingastochasticgenerator. Weillustrate (gridded)approach. Lagrangianapproacheshavebeenpro-
possibleapplicationsofBrenier’spolarfactoriza- posedbyGalloue¨tandMe´rigot(2018),whileMe´rigotand
tiontonon-convexoptimizationproblems,aswell Mirebeau(2016)useasemidiscreteOTformulation. Both
assamplingofdensitiesthatarenotlog-concave. areusedonlow-dimensionalmanifoldsaslowerlevelsub-
routinestosolveEuler’sequationforincompressibleand
inviscidfluids(Arnold,1966). Morerecently,Moreletal.
1.Introduction (2023)proposedtouseBrenier’sinsighttograduallyrefac-
tor a normalizing flow as the gradient of a convex map,
Brenierproved,throughhisseminalpolarfactorizationthe- a.k.aaMonge(1781)map,byapplyingmeasure-preserving
orem(1991),thatanyvectorfieldcanbedecomposedinto maps for the Gaussian distribution. Their approach does
two simpler elements: Given a reference measure ρ sup- not,however,relyonneuralOTsolvers,andfocusesinstead
portedonΩ Rd,ForanyF :Ω Rd,thereexistsacon- onuntanglinganexistingflowtoturnitgraduallyintothe
vexpotential⊂ u : Rd Randam→ easure-preservingmap gradientofaconvexpotential.
→
M :Ω Ω(i.e. onhasM ρ=ρ),suchthatF = u M.
♯
→ ∇ ◦
Thepolarfactorizationtheoremstatesthatanyvectorfield,
Contributions. Weproposeinthisworkaneuralimple-
no matter how irregular, can be reshuffled to match that
mentationofthepolarfactorizationtheoremthatleverages
ofthegradientofaconvexpotential,andthatthiscareful
recentadvancesinneuraloptimaltransport. Moreprecisely,
reshufflinginspaceisachievedbythemeasure-preserving
• Afterintroducingthepolarfactorizationtheorem,aswell
1CREST-ENSAE,IPParis2Apple.
as neural OT solvers, we show how the two blocks of
Brenier’sresultcanberecoveredusinginputconvexneu-
nina.vesseron@ensae.fr,cuturi@apple.com.
ral networks (ICNN) (Amos et al., 2017). We modify
theICNNarchitectureoriginallyproposedinAmosetal.
1
4202
raM
5
]LM.tats[
1v17030.3042:viXraNeuralPolarFactorization
(2017) and Korotin et al. (2020), to propose quadratic f⋆isitselfthesolutionofthefollowingdualobjective:
(low-rank+diagonal)positivedefinitelayersateachlayer.
(cid:90) (cid:90)
StartingfromanarbitraryfieldF,weusethemodifica- f⋆ arginf fdµ+ f∗dν (2)
tionsproposedbyAmos(2023)to(Makkuvaetal.,2020) ∈ f∈L1(µ) Rd Rd
totraintheBrenierconvexpotentialu ,thatappearsin
θ wherethef∗istheconvexconjugateoff,
thepolarfactorizationofF.
• We study two alternative parameterizations for the f∗(y):= sup x,y f(x). (3)
measure-preservingmapM: Eitherimplicit,relyingon x∈Rd⟨ ⟩−
the pointwise evaluation of theconvex conjugate of u
θ
Note that the star symbol used for convex-conjugacy
composedwithF,orexplicit,throughanadditionalnet- ∗
shouldnotbeconfusedwiththestarsymbol⋆,usedthrough-
workM trainedtomapsamplesxfromρto u∗ F(x).
ξ ∇ θ◦ outthepapertodenoteanoptimalsolution. TheOTmap
• BecauseM isnot,ingeneral,injective,weconsidertheill- from ν to µ is also given by the inverse of f⋆ when it
posedproblemofinvertingM:weapproximateastochas- exists, (f⋆)∗. The goal of neural OT solv∇ ers is to esti-
ticmapI ψ,parameterizedasagenerator,thatcangenerate
mate
f⋆∇
using samples drawn from the source µ and the
inputsxsuchasM(x)=yforagiveny. Weusebridge
targetdistributionν. Makkuvaetal.(2020);Korotinetal.
matchingforthistask(DeBortolietal.,2023).
(2020)haveproposedmethodsthatbuildoninputconvex
• Weuseourapproachtofactorizegradientsofsurfacesin neuralnetworks(ICNN),asoriginallyproposedbyAmos
lowdimensionsandshowhowtouseourtoolstostudy etal.(2017), toparameterizethepotentialf asanICNN.
thecriticalpointsofanon-convexenergyg. Factorizing The main difficulty in these methods lies in handling the
G:= gas u θ M ξ,andestimatingthestochasticmap Legendretransformin(2)oftheICNNvariable. Toaddress
∇ ∇ ◦
I ψ corresponding to M ξ, our goal is to generate zeros thisdifficulty,surrogatenetworkscanbeusedtoreplacef∗,
of ∇g. Theminimizerofu θ being ∇u∗ θ(0)bydefinition andwerefertoAmos(2023)forthemostrecentproposalto
ofconvexduality,thepointsgeneratedasI ψ( ∇u∗ θ(0),z) refinetheseimplementationsusingamortizedoptimization.
where z is a Gaussian noise of suitable size should, in Neuralsolvershavebeenusedsuccessfullyinvariousappli-
principle,resultinpointsthatarerootsof g. Weusethe cations,notablyinsinglecellgenomics(Bunneetal.,2023;
∇
cross-entropylossofasmallMNISTdigitsLeNet(LeCun 2022);seealso(Huangetal.,2020;Cohenetal.,2021).
etal.,1998)classifierandshowtheabilitytosamplenew
parameterswithlowgradientandgoodperformanceon
2.2.PolarFactorization
therecognitiontask.
Givenaprobabilitydistributionρsupportedonabounded
setΩ,Brenier’spolarfactorizationtheoremstatesthatany
2.Background
vectorfieldF :Ω Rdcanbewrittenasthecomposition
Thissectionintroducesneuralmethodsthathavebeenpro- of the gradient of→ a convex function u : Ω Rd with
∇ →
posedtolearnMongemapsbetweentwodistributionsand a map M : Ω Ω that preserves the distribution ρ (ie
→
recallsthepolarfactorizationtheoreminitsoriginalform. M #ρ = ρ). In that decomposition, u is the unique OT
∇
mapfromBrenier’stheoremthattransportsthemeasureρon
2.1.NeuralApproachestotheMongeProblem F #ρ,sinceF #ρ=( u M) #ρ= u #(M #ρ)= u #ρ.
∇ ◦ ∇ ∇
Theorem2.1(Brenierpolarfactorization). Letρbeaprob-
The Monge formulation of the OT problem between two
abilitymeasurewhosesupport,Ω Rd,isaboundedset
p Rr dob →abi Rli dty thm ae ta ts rau nre ss poµ rtsan µd oν nt∈ oνP ,( wR hd i) lese me ik ns ima im zia np gT the: a dn egd eF ne: raΩ
te→
i.eR
.
(cid:82)dasq Fuar 2e d- ρint <egrab al⊆ e ndve ρc (to Fr −fi 1e (l Ad )b )ei =ng 0n oo nn
followingtransportcost: Rd∥ ∥ ∞
LebesguenegligiblesubsetsAof Rd. Then,thereexistsa
convexfunctionu:Ω RandamapM :Ω Ωthatis
(cid:90) measurepreserving,i.e→ . M ρ=ρ,suchthat: →
2(µ,ν):= inf 1 x T(x) 2dµ(x) (1) #
W2 T:Rd→Rd Rd 2∥ − ∥
T#µ=ν F = ∇u ◦M. (4)
BothM and uareunique.
∇
The existence of an optimal map T⋆ is guaranteed under
fairlygeneralconditions(Santambrogio,2015,§1),when 3.NeuralPolarFactorization(NPF)
e.g. µhasadensityw.r.t. theLebesguemeasure. Inthat
case,Brenier’smostfamoustheoremstatesthattheMonge We describe our method to compute the approximate
problem(1)hasauniquesolution,foundatthegradientof polar factorization of a field F, using i.i.d samples
aconvexfunctionf⋆i.e. T⋆ = f⋆. Thatconvexfunction (x ,...,x ) ρ and their evaluations (F(x )) . We
1 n i i
∇ ∼
2NeuralPolarFactorization
firstestimatetheconvexpotentialuinthedecomposition 3.2.EstimatingtheConvexPotentialu
F = u M usinganICNNu asanOTBrenier(1991)
θ
∇ ◦ Startingfromtheexistenceresultoutlinedin(4),werecover,
potentialusinganimprovedICNNarchitecture. Next,we
byapplyingthepush-fowardmapF onρ,that
showthatthemeasure-preservingmapM canbedefined
implicitlyforanyx,byevaluatingtheconvexconjugateof
F ρ=( u M) ρ= u (M ρ)= u ρ.
♯ ♯ ♯ ♯ ♯
u onF(x): Thisrequiresacalltoaconvexoptimization ∇ ◦ ∇ ∇
θ
routineateachevaluation.ThankstoourICNN’sstrongcon- Sinceuisaconvexfunction, itoptimallytransportsρon
vexity,thetransform(3)iswell-posed. Tounderlinethelink u ρintheMongesense. Therefore,thedefiningfeature
#
∇
ofthatapproachtoestimateM usingu θ,weusethenotation of u is that u is the Monge map from ρ to F ♯ρ. We
∇
M θ(x). Alternatively, we also propose to learn an amor- use Amos’ solver (2023) to estimate the potential u that
tizedmodelforM,bylearninganetworkM ξ trainedona pushes ρ onto F #ρ, from the empirical measures ρ n :=
regressiontaskusingpaireddatasamples {(x i,M θ(x i)) }. n1 (cid:80)n i=1δ
xi
andF #ρ
n
= n1 (cid:80)n i=1δ F(xi). Usingthissolver
consistsofparameterizingtheconvexfunctionuasanICNN
3.1.(Low-Rank+Diagonal)QuadraticLayersinICNNs u following§3.1andparameterizing u∗ directlybyan
θ
∇
auxiliaryvector-valuednetworkV . Theauxiliarynetwork
ICNNsprovideaneuralnetworkparameterizationofcon- ϕ
V islearnedbyminimizingtheobjective:
vexfunctions. Wepropose amodification ofthe original ϕ
architecturepresentedin(Amosetal.,2017). Ourapproach
isinspiredbytheGaussianinitializationoutlinedin(Bunne
n
1 (cid:88)
etal.,2023)andthelow-rankquadraticlayerspresentedin (ϕ)= V (F(x )) u∗(F(x )) 2
(Korotinetal.,2020). TheoriginalICNNwasdesignedto Lconvex-dual n ∥ ϕ i −∇ θ i ∥
i=1
re-injecttheinputvectorx,transformedbyanaffinemap,at
everylayer,ascanbeseenin(Amosetal.,2017,Equation Onecanshow,usingDanskin’senvelopetheorem(1966),
2,y x). Korotinetal.,§B.2proposedinsteadtomodify that u∗(y)isthemaximizeroftheconvexconjugate(3)
xwi→ thmultiplelow-rankquadraticpositivedefinite(PSD) prob∇ lemθ foruaty,
forms. ThePSDconstraintensuresconvexityofeachentry,
whilethelow-rankchoiceensuresareasonablenumberof u∗(y)=argsup x,y u (x)
∇ θ ⟨ ⟩− θ
parameters. WeproposequadraticPSDformsthatincorpo- x
rateapositivediagonalpluslow-rankmatrices(Saunderson Because u is strictly convex, we compute the optimal
θ
etal.,2012;LiutkusandYoshii,2017): solution solving u∗(F(x)) with a conjugate solver, e.g.
Q (x):= δ x 2+ Ax 2 =xT (cid:0) diag(δ)+ATA(cid:1) x. gradient ascent, (L)BFGS (Liu and Nocedal, 1989) or
A,δ
∥ ◦ ∥ ∥ ∥ ADAM(KingmaandBa,2014). Wecallaconjugatesolver
ThenetworkhasL+1layersforL 1;wehavehighlighted
≥ CS any algorithm that, for a given pair (u,y), outputs an
inbluethenewPSD(diagonal+low-rank)terms: approximationof u∗(y). Thisresultsintheloss:
(cid:16) (cid:17) ∇
z =σ [Q (x)] +B x+c ,
0 0 Ai 0,δ 0i i 0 0 1 (cid:88)n
z =σ
(cid:16)
W z +[Q (x)] +B x+c
(cid:17)
,
Lconvex-dual(ϕ)=
n
∥V ϕ(F(x i)) −CS(u θ,F(x i)) ∥2
ℓ+1 ℓ ℓ ℓ Ai ℓ,δ ℓi i ℓ ℓ i=1
z =σ (cid:0) w Tz +Q (x)+bTx+c (cid:1) R (7)
L+1 L L ℓ AL,δL L L ∈ Inpractice,thelatterisinitializedwiththepredictionsof
u θ(x)=z L+1 V ϕ, which considerably reduces the number of iterations
(5) requiredforthesolvertoconvergewhenV startsmaking
ϕ
Inalllayersabove, theindexispans1,...,q, whereq is correctpredictions. Theparametersofthenetworku are
θ
thesizeofthestatevectorsz ℓ Rq. ThisaugmentedICNN thenupdatedalternatively,bytakingstepsalongthegradi-
∈
isparameterizedwiththefollowingfamilyofparameters, entsoftheoriginaldualobjectiveofMakkuvaetal.(2020):
θ =(cid:0) W (Rq×q)L,w Rq,
1:L−1 ∈ + L ∈ + 1 (cid:88)n
(cid:0) δ 0i
:L−1
∈(Rd +)L,Ai
0:L−1
∈(Rr×d)L(cid:1) i=1...q, LMonge(θ)=
n
u θ(x i)+ ⟨V ϕ(F(x i)),F(x i)
⟩ (8)
i=1
δ Rd,A Rr×d, (6)
L ∈ + L ∈ u θ(V ϕ(F(x i)))
B (Rq×d)L,b Rd, −
0:L−1 L
∈ ∈
c (Rq)L,c R(cid:1) . 3.3.EstimatingtheMeasure-PreservingMapM
0:L−1 L
∈ ∈
Theactivationfunctionsσ areconvex,non-decreasingnon- InthepolardecompositionofF, uistaskedwithtrans-
ℓ
∇
linearandallparametersinredinadditiontoalldiagonal portingρonF ρ,themeasure-preservingmapM ensures
#
vectorsδmustbenon-negativetoensureconvexity. thenthatF = u M. ToexpressM asafunctionofF
∇ ◦
3NeuralPolarFactorization
andu,onesimplyhastoapplytheinverseof uonboth our goal will therefore be to generate inputs x such that
∇
sides. When u is strictly convex, we simply rely on the M (x) = y. Tothisend,welearnagenerativeprocessto
θ
identity u∗ u=Idtoobtain: sampleaccordingtotheposteriordensity
∇ ◦∇
M = u∗ F (9)
∇ ◦
π θ(x |y)= (cid:82)
1 1y=Mθ(x)ρ ρ( (x x)
)dx. (12)
EvaluatingM usingaConjugateSolver. Givenacon- x y=Mθ(x)
jugatesolverCSandtheestimateu forthegroundtruth
θ Werelyontheaugmentedbridgematchingprocedurepre-
potentialu,wecaninjectthemin(9)togetanestimation
sented in De Bortoli et al. (2023) to learn the drift of the
CS(u ,F(x))ofM(x)foragivenx. Sincethisestimation
θ stochastic differential equation (SDE) formulated in (13)
dependsonθ,wedefinethatapproximationasM (x),
θ so that, on input X = y, the generated samples X be
0 1
distributedaccordingtoπ (xy)(12).
M θ(x):=CS(u θ,F(x)), (10) θ |
dX =(X (X ,X ) X )/(1 t)dt+σdB (13)
withaslightabuseofnotation,sinceθshouldnotbeunder- t ψ 0 t − t − t
stoodasaparameterparameterizingM,butinsteaddefining DeBortolietal.’sapproachrefinesthebridgematchingpro-
itimplicitlythroughu θ andCS. ceduresthathavebeenrecentlyusedtosolveinverseprob-
lems(Somnathetal.,2023;Liuetal.,2023;Chungetal.,
NeuralEstimationforM. WhileM θ doesindeedpro- 2024),byaugmentingthelearnablepartofthedriftX ψwith
videanestimateofM,itmaybeconvenienttoparameterize theinitialpointX oftheSDE.Thisslightadjustmental-
0
themeasure-preservingmapofinterestasaneuralnetwork lowstocorrectlyrecoverthecouplingmeasure(M ,Id) ρ
θ #
M ξ,definedwithanindependentsetofparametersξ. Bor- fromthepairedsamples {(x i,F(x i)) }n
i=1
whenX
ψ
ispa-
rowingapagefromamortizedoptimization(Amosetal., rameterizedusingamultilayerperceptrontrainedaccording
2023),M ξcanbeusedtoinitializetheconjugatesolverused toAlgorithm1.
toestimateM orevenreplaceitwhenM issufficiently
θ ξ
accurate. Furthermore,theparameterizationofM byM
ξ
is Algorithm1TrainingofX
ψ
sometimesnecessarywhen,e.g.,F isonlygivenonafew 1: u θ TrainedICNNs.t. u θ#ρ F #ρ
samples,andonewishestoevaluateM atanypoint. The 2: Init← ia− lizeX ψ ∇ ≈
neuralmapM ξ isthentrainedtominimizethefollowing 3: whilenotconvergeddo
mean-squarederror: 4: Drawasample(x i,F(x i))
1 (cid:88)n 5: Computey i =CS(u θ,F(x i))
(ξ)= M (x ) CS(u ,F(x )) 2. (11) 6: Samplet ([0,1])
Lpreserving n ∥ ξ i − θ i ∥ ∼U
i=1 7: Samplez i (0,I d)
∼N
8: x t :=(1 t)y i+tx i+σ(t(1 t))1/2z i
Notethatwhilethelossin(11)resembles(7),thenetwork − −
V takesthetransportedpointF(x )asaninput,whereas 9: Lψ ← n1 ∥X ψ(y i,x t) −x i ∥2
Mϕ isonlygivenx . i 10: UpdateX ψ using ∇Lψ
ξ i 11: endwhile
Evaluating The Measure Preservation of M. In both
cases, M θ, asevaluatedwithaconjugatesolver, oritsin- The optimized network X
ψ
is then plugged in (13)
dependently evaluated neural counterpart M ξ should be that we solve with Heun’s method as implemented in
measure-preserving. Indeed, we will use (as in Figure 1, diffrax (Kidger, 2021) using S discretization steps.
bottomcenterplots)anydeparturefromtheidentity Given a sample y from M ρ, solving the SDE (13) us-
θ#
ing X = y allows to generate an output X distributed
M ρ=( u∗ F) ρ=( u∗) (F ρ)=ρ, 0 1
# ∇ ◦ # ∇ # # accordingtotheposteriordensity(12). Toalleviatetheno-
asawaytoassessthequalityofourfactorization.
tations,wecallI
ψ
thegenerativeprocesssuchthatI ψ(y,z)
istheoutputX returnedbythedifferentialequationsolver
1
3.4.Samplingaccordingtothepre-imagemeasureM−1 associatedto(13)ontheinputX 0 = y whentheinjected
θ gaussiannoisezhasbeendrawnfrom (0,I )⊗S
d
N
Measure-preserving maps M are not invertible in gen-
I (y,z)=SDE(X ,y,z), y Rd, z Rd×S.
eral(Ryff,1970),awell-knownexamplein1Dbeingthe ψ ψ
∈ ∈
doublingmapdefinedasM(x)=2xmod1thatpreserves
Togenerateseveralinputsxfromπ (xy),oneonlyneeds
θ
theLebesguemeasurerescaledtotheinterval[0,1]. This toinjectdifferentnoisesz (0,I )⊗| S inI (y, ),i.e.
d ψ
non-invertibility is of particular interest in the optimiza- ∼N ·
tionandsamplingapplicationswepropose. Foragiveny, I (y, ) (0,I )⊗S π ( y).
ψ # d θ
· N ≈ ·|
4NeuralPolarFactorization
4.NPFtoStudyNon-ConvexPotentialsg withg : Rd R,theLangevinMonteCarloalgorithm
−→
cansamplefromπbystartingfromx(0)toiterate
In this section, we focus on the polar factorisation of the
gradient field g, where g is a non-convex function of (cid:112)
∇ x(k+1) =x(k) γ g(x(k))+ 2γz(k), z(k) (0,I ).
interest. WeshowhowcomputingtheNPFof gtogether − ∇ ∼N d
∇
withtheinversemapI canbeusedtoexplorethespaceof
ψ
criticalpointsofg. When g is non-convex, the LMC algorithm lacks guaran-
tees(RobertsandTweedie,1996;ChengandBartlett,2018;
4.1.OnusingtheInverseMapI of g DalalyanandKaragulyan,2019). Inparticular,whenghas
ψ
∇ multiplelocalminima,thegeneratedsamplesarehighlycor-
NPF on G = g. Let g : Rd R be a function of relatedastheparticlesoriginatingfromtheLMCalgorithm
interestsupporte∇ donaboundedsetΩ→ Rd. Assumingthat oftengetstuckinsomebasins. Forthisreason,theLMC
g : Rd Rd meets the requireme⊂ nts of (4), Brenier’s algorithmhasbeencombinedwithmethodsenablingglobal
∇ →
polarfactorizationstatestheexistenceofaconvexfunction jumps between modes (Pompe et al., 2020; Gabrie´ et al.,
uandameasure-preservingM thatpreservestherescaled 2022)tosamplemulti-modaldistributions.
LebesguemeasureonΩ, suchthat:
Ω
L
SamplingwithKnownPolarFactorizationfor g. In
g = u M.
∇
∇ ∇ ◦ this paragraph, we assume that the polar factorization
( u,M)andstochasticinversemapM−1of gareknown.
Foragivenvectorv,thepointsinΩwhosegradientwith
∇ ∇
To sample the modes of π(x) e−g(x) when g is non-
respecttog isequaltov arealltransportedbyM onthe ∝
samepoint u∗(v)i.e. convex,onecanruntheLMCalgorithmontheconvexfunc-
∇ tionuandsamplebackusinganinversegeneratorM−1:
M( x Ω: g(x)=v )= u∗(v) . y(k) =M(x(k))
{ ∈ ∇ } {∇ }
(cid:112)
y(k+1) =y(k) γ u(y(k))+ 2γz(k)
Inparticular,thecriticalpointsofg areallmappedbyM − ∇
1
ontotheminimizerofthefunctionu,whichis u∗(0). x(k+1) =M−1(y(k+1),z(k+ 2)).
∇
OnExtractingtheCriticalPointsofg. WhentheNPF The LMC step on u allows to move along a new descent
of gislearned,resultinginu ,M andI ,composingI directionorexplorationdirectiontoreachy whileM−1
θ ξ ψ ψ k+1
∇
with u provides an inversion process for g. Gener- randomlygeneratesapointx(k+1) Ωwhosegradientfor
θ
∇ ∗ ∇ ∈
atinganinputpointx whosegradientisv caninfactbe gis u(y ). Thisway,theneighborhoodsofg’scritical
v k+1
∇
donebyfirstsamplingz (0,I )⊗N andsuccessively pointsareuniformlysampled,andaparticledoesnotget
d
∼ N
applying u andI tov: stuck in one minimum as M−1 permits global moves be-
θ ψ
∇ ∗
tweenallthebasins. Becauseitisdifficulttodifferentiate
aminimumfromasaddlepointoramaximumwhensam-
x v =I ψ( ∇u∗ θ(v),z), wherez ∼N(0,I d)⊗S. plingcriticalpointsusingthepolarfactorizationof ∇u,this
procedureshouldbecombinedwithLangevinstepsongto
Asaspecialcase,samplingthecriticalpointsofgisdoneby escapenon-minimumcriticalpoints. Thefollowingpara-
takingv =0intheaboveprocedurewithdifferentnoisesz. graphdetailsthesamplingalgorithmandcomplementsitby
Note,however,thatthisconvexificationrequiresestimating showinghowthepolarfactorizationof ucanbelearned
∇
thepolarfactorizationof gaswellastheinversemapI whilesampling.
ψ
∇
overtheentirespaceΩ,whichiscomputationallyexpensive.
Tooptimizethegfunction,weproposeinsteadtocombine
UnknownPolarFactorizationfor g Whenthepolar
thismethodwiththeLangevinMonteCarlo(LMC)algo- ∇
factorization is unknown, we propose an algorithm that
rithm to correctly estimate the polar factorization of g
∇ learnsthepolarfactorizationof g aswellastheinverse
aroundtheminimumsofg. ∇
map I using the generated particle trajectories. The al-
ψ
gorithmalternatesbetweenN LangevinstepsongandN
4.2.ALMCMethodAssistedbyNPF.
Langevinstepsonu ,whileM andI allowtotransition
θ θ ψ
LMCalgorithm Givenasmoothlog-concavedensity betweenthetwospaces. Algorithm2detailsthestepsofthe
procedure. Thenotation LMC(u ,γ,y(k),N)meansthat
θ i
e−g(x) N LMCstepsareperformedonthefunctionu withatime
θ
π(x)= (cid:82) e−g(x)dx, (14) stepofγ startingfromthepointy(k) .
x∈Rd i
5NeuralPolarFactorization
Algorithm2LMC-NPF matetheaveragedistancebetweentheprobabilityassociated
1: Initializeu θ andI ψ tothedensityπ θ(x |y)(12)andI ψ(y, ·) # N(0,I d)⊗S from
2 3:
:
I knitia 0lizetheparticles
{x( i0)
}1≤i≤n
s itam isp ul nes li. keG lyiv te on fith ne dfi an mite ult te its ut ds eet of{( px
oj
i, nM
tsθ
w(x
itj
h))
t} h1 e≤ sj a≤ mm
e,
4: wh← ilek <k maxdo image. Forthisreason,weapproximateM θ−1(M θ(x k)),by
5: ifk mod N =0then constructingtheset
6: y i(k) =M θ(x( ik))
7: y i(k+1) = LMC(u θ,γ,y i(k),N) Bα(x k)= {x j : ∥M θ(x j) −M θ(x k) ∥2 ≤α }
8: x( ik+1) =I ψ(y i(k+1),z)withz ∼N(0,I d)⊗S andchooseαsuchthatthecardinalof α(x k)is100. We
9: else thencomputethesinkhorndivergencebB etweenthepredic-
11 10 :: endx( i ik f+1) =x( ik) −γ ∇g(x( ik))+√2γz i(k) t oi vo en rs ao lf
l
tI hψ eo xn .M Wθ( eB cα ok m(x pk a) r) ea thn ed oB bα tk ai( nx ek d) va an ld ueav we ir ta hge thi et
12: Updateu θ,I ψ with {(x( ik), ∇g(x( ik))) }1≤i≤n distanceoftwk obatchesoffixedsizedrawnindependently
13: k k+1 from the (x ) . We also evaluate the fact that the
← j 1≤j≤m
14: endwhile stochasticmapI u∗approximatesG−1bycomputing
ψ ◦∇ θ
thequantity,usingMCsamplesforz,
Themaininsightoftheproposedsamplingalgorithmisthat m
1 (cid:88)
LMCstepspermittheexplorationofthespacelocally,while
m
E z∼N(0,Id)⊗S ∥G ◦I ψ( ∇u∗ θ(G(x j)),z) −G(x j) ∥2
NPFprovidesandstoresamoreglobalviewpoint, thatis j=1
(15)
abletoproposemovestopotentiallyworthyareas.
Wealsostudythequantitywherethecosinesimilarityre-
placesthenormin(15).
5.Experiments
5.1.AccuracyMetrics 5.2.NPFofTopographicalData
AssessNPF’sAccuracy. WhenafieldGisonlyavailable Dataset. WeusethePythonpackageelevationtoget
throughsamples,thefollowingthreecriteria,evaluatedon theelevationofthreeregionsoftheworld: Chamonix,Lon-
unseensamples(ortestset) (x ,G(x )) ,areused don,andCyprus. Weestimatethegradientsassociatedwith
j j 1≤j≤m
toassesswhethertheestimat{ edpolarfacto} rizationiscorrect. theelevationintheseregionswithfinite-differences,andob-
tainthreedatasetscomposedof(latitude,longitude)points
• To measure that the distributions u ρ and G ρ are
θ# # pairedwiththeirgradients. Welearnthepolarfactorization
∇
close, we compute the Sinkhorn divergence S (Ram-
ε ( u ,M ) of the underlying gradient field as well as the
θ ξ
das et al., 2017; Genevay et al., 2018; Peyre´ et al., ∇
inversemapI . Becauseintheseexamples,Gisonlygiven
ψ
2019)betweenthetwopointclouds(G(x )) and
j 1≤j≤m throughsamples,weparameterizedthemeasure-preserving
( u (x )) . To quantify the scale of that mea-
θ j 1≤j≤m mapusinganeuralnetworkM . Toassessthequalityof
∇ ξ
surement,wecompareitwiththedistancebetweentwo
ourmethodNPF,weuseda85%training/15%testsplit.
batches of fixed size drawn from (G(x )) . We
j 1≤j≤m Moredetailscanbefoundintheappendix.
alsovisualizethisproximitybyembeddingthetwopoint
cloudsusingtheTSNEalgorithm(VanderMaatenand
Chamonix
Hinton,2008)andsuperimposethem.
S ( u ρ ,G ρ ) 0.27
ε θ# n # n
• The second criterion assesses whether M ξ is measure- S ε(∇ G #ρ n,G #ρ′ n) 1.55
preserving. Similarly, this is numerically estimated S (M ρ ,ρ ) 0.0029
ε ξ# n n
by computing the Sinkhorn divergence between the S (ρ ,ρ′ ) 0.0034
ε n n
(e Mm ξp (ir xi jc )a )l 1≤m j≤ea msu ,r ae ns dva is ss uo ac li ia zete dd ww ithit ah T( Sx Nj E) 1≤ emj≤ bm edda inn gd . E Ex x,∼ zρ [n S[ ε∥ (G (I( ψx () M− θ(∇u αθ (x◦ )M ),zξ( )x ,) ∥ α2 (] x))] 0 0. .9 06 48
B B
• Finally,weevaluatetheL distancebetweenGand u S (ρ ,ρ′ ) 0.077
2 ∇ θ ◦ ε 64 64
M usingthetestset. NotethatwhenM isused(rather S (ρ ,ρ′ ) 0.039
ξ θ ε 128 128
thanM ),thatcriterionisnotusefulsinceitonlyassesses
ξ
thequalityoftheconjugatesolver. Table1.PolarfactorizationandInversemultivaluedmapmetrics
forlearningthegradientoftheelevationinChamonixarea. For
Assess the Generative Inverse Map I . Given y, we
thesemetrics,ρ nandρ′ naretwoempiricalmeasurescreatedfrom
ψ n=1024samplesdrawnindependentlyfromthetestset.Theε
shouldbeabletosampleamongtheantecedentsofybyM θ parameterissetequalto0.1whichisthedefaultparameter.
using the multivalued map I . To quantify that, we esti-
ψ
6NeuralPolarFactorization
PolarFactorizationResults. Table1showsthattheesti- n=1024 n=2048
matedNPFisaccurate: theSinkhorndivergencebetween S ε( u θ#ρ n,G #ρ n) 107.5 8.2 93.5 5.3
∇ ± ±
the predicted distribution ∇u θ#ρ
n
and the target distri- S ε(G #ρ n,G #ρ′ n) 110.3 ±8.7 87.4 ±4.2
bution G ρ is lower than the divergence between two
# n
batchesofsize1024takenfromthetarget. Similarly,the
Table2.Polarfactorizationmetricsforlearningthegradientofthe
MNISTclassifierlossfunction.
Sinkhorn divergence between ρ and its image by M is
n ξ
lower than that between two batches of size 1024 drawn
fromthesource. ThereconstructionofGisalsoquitesatis- E [S ((I (M ( (x )),z), (x ))] 106.2 0.5
k,z ε ψ θ α k α k
B B ±
factoryascorroboratedvisually(Figure1). S (ρ ,ρ′ ) 111.0 0.6
ε 64 64 ±
S (ρ ,ρ′ ) 105.7 0.4
Inverse Map Results. The data from Table 1 in- E Eε y∼G1 #28 ρn,z1
∥
c2 oG8
si◦
neI (ψ G( ∇u I∗ θ( (y), uz ∗)
(y−
),y
z∥
)2
,y)
1 04 .8.2 3±± 1 0.3 1. 80
dicates that I ψ generates the antecedents of the y∼G#ρn,z ◦ ψ ∇ θ ±
images by M accurately: the estimated quantity
E [S ((I
(Mθ
( (x)),z), (x))]iscomparabletothe
Table3.Inversemultivaluedmapmetricsforlearningthegradient
x,z ε ψ θ Bα Bα oftheMNISTclassifierlossfunction.
distancebetweentwobatchesdrawnfromthesourcedis-
tribution whose size lies between 64 and 128. To vi-
sualize these performances, we transported the samples
chooseacertaingradientv anduseI u∗ togenerate
(G(x j)) 1≤j≤m,thatstoregradientsoftheelevation,using ψ ◦∇ θ
I u∗ thatshouldestimatetheinversegenerativemap classifier weights whose gradient is approximately v. In
Gψ −◦ 1.∇ Wθ
e expect very high gradients to be sent to points
particular,I
ψ
allowsthegenerationofcorrectcriticalpoints
(Figure12),which,however,havealowaccuracy. Thismay
wheretheelevationvariesrapidly,suchasthesidesofmoun-
beduetothefactthatthegradientofgoodminimumsvaries
tainsintheChamonixexample. Tovisualizewhereagradi-
greatly with the stochasticity of the loss function visible
entwassent,weplotapointatthislocalizationandcolorit
on Figure 13 and Figure 15, compared with the gradient
accordingtothenormofthegradientfromwhichitorigi-
of critical points with an accuracy of 10%, which is the
nates. Wecomparetheimagegeneratedbythisprocesswith
performanceofaclassifierwithrandomweights.
theoneobtainedbycoloringdirectlythepoints(x )
j 1≤j≤m
using their associated gradients. In the three cases (Cha-
5.4.LearningNPFusingGradientFlowofParticles
monix,London,Cyprus),thetwoimagesarequitesimilar
(Figure2),showingthequalityofourreconstruction.
In §5.3, we requested that NPF learn the entire gradient
field. Thisofcourselimitstheability,givenacertainbudget
5.3.LearnanNNOptimizationLandscapeusingNPF of samples, to provide a good approximation of critical
points through the inverse generative map. This is likely
Inthisexperiment,weconsideraminimalneuralarchitec-
duetoouruniformsamplingprocedure,whichisunlikelyto
turecapableofclassifyingMNISTdigits. Inspiredbythe
revealinterestingcriticalpoints.Inthisexperiment,wetrain
LeNetarchitecture(LeCunetal.,1998),weusetwoconvo-
NPFusinggradientdescenttrajectoriestofocusonthose
lutionallayers,eachfollowedbyaReluandamaxpooling
areas. Todothis,weinitialize1024particlesrandomlyand
operation. A classification layer leads to an output layer
havethemfollowagradientflow. Weusethesetrajectories’
of 10 neurons, followed by a softmax. The loss function
samplestolearnNPF.
is the cross entropy, computed with MNIST train dataset
minibatches of size 128, and the vector field under study
isthegradientofthatlossforthed = 222parametersof
Polar Factorization Results. The gradients associated
theneuralnetwork. Thelosslandscapeofanon-linearneu-
withparticlesingoodaccuracybasinsvarygreatlywithloss
ralnetworkbeingverychaotic(Lietal.,2018),wedonot
stochasticity(seeFigure13). However,wecanseethatNPF
expecttolearnthepolarfactorizationoftheassociatedgra-
isfaithfulinthoseareas: theSinkhorndivergencebetween
dientfieldperfectlyoverthealloptimizationspaceΩ. The
thedistributiongeneratedby u andthetargetisofthe
optimizationspaceweareconsideringisΩ=[ 1,1]222. ∇ θ
− sameorderasthatbetweentwobatchesofsize1024.
PolarFactorizationandInverseMapResults. Accord-
ingtoTable2,weseethat,overall,NPFmanagestolearn InverseMapResults. AsforI ,thegradientsofthegen-
ψ
thatvectorfield,butitlacks,asexpected,accuracyinsome erated weights do have a norm close to 0, and the cosine
parts of the space. This is, e.g., revealed visually using similaritydistributionrevealsthatthedirectionofthegradi-
theTSNEplotfromFigure3. Similarly,I canbeusedto entsisgloballylearnedbutstochasticitypreventsusfrom
ψ
invertM accordingtoTable3andthehistogramassociated performingbetter. Moreover,wecanseeinFigure14that
θ
withthecosinesimilarityonFigure3confirmsthatwecan thecriticalweightsgeneratedcontainvalidminima.
7NeuralPolarFactorization
g ∥G∥2
2
ρ
n
G #ρ
n
∇u θ#ρ n M θ#ρ n M ξ#ρ n (∇u θ∘M ξ) #ρ n
Figure1.ThegfunctionunderstudyistheelevationintheChamonixarea(France).Thefiguresshowtherespectiveactionofthevector
fieldsinvolvedinthepolarfactorizationof∇gonasamplemeasureρ n.Weobservethat∇u θ#ρ n ≈G #ρ n.Bothimplicitandexplicit
measure-preservingmapsM θ(10)aswellastheexplicitnetworkM ξtrainedwiththeloss(11)permutesthepointsofthedistribution,
ensuringthatG≈∇u θ◦M ξwhile(M ξ) #ρ n ≈(M θ) #ρ n ≈ρ n.
G #ρ
n
∇u θ#ρ
n
0.2
0.1
0.0
T<latexit sha1_base64="R25ezd3l6ArvFIhRZb4yTFiNI7k=">AAACx3icjVHLSsNAFD2Nr1pfVZdugkVwVdIuqsuiCLqRin1BLZKk0zY0LyaTYiku/AG3+mfiH+hfeGecglpEJyQ5c+49Z+be68S+lwjLes0YC4tLyyvZ1dza+sbmVn57p5lEKXdZw438iLcdO2G+F7KG8ITP2jFnduD4rOWMTmW8NWY88aKwLiYx6wb2IPT6nmsLSdWvL89u8wWraKllzoOSBgXoVYvyL7hBDxFcpAjAEEIQ9mEjoaeDEizExHUxJY4T8lSc4R450qaUxSjDJnZE3wHtOpoNaS89E6V26RSfXk5KEwekiSiPE5anmSqeKmfJ/uY9VZ7ybhP6O9orIFZgSOxfulnmf3WyFoE+jlUNHtUUK0ZW52qXVHVF3tz8UpUgh5g4iXsU54RdpZz12VSaRNUue2ur+JvKlKzcuzo3xbu8JQ249HOc86BZLpYqxcpVuVA90aPOYg/7OKR5HqGKc9TQIO8hHvGEZ+PCiIyxcfeZamS0ZhfflvHwAa4ikGY=</latexit> SNE 0 1
Cosine
Figure2.I ψ’sabilitytoreplacegradientsintheoriginalΩspace
for the example of Chamonix region’s elevation gradient. The
figureontherightisgeneratedbyreturningthegradients∇g #ρ n Figure3.Performance of the NPF and the inverse map I ψ on
to their initial position in the image via I ψ ◦∇u∗ θ. This posi- the §5.3 MNIST classifier experiment where the loss gradient
tionisthencoloredaccordingtotheinitialgradientnorm(before
islearnedovertheentirespaceΩ.TheTSNEallowstovisualize
transport).Wecancomparetheresultwiththeimageontheleft, in2Dtheoverlapofthepredicteddistribution∇u θ#ρ nwiththe
generatedbysamplinguniformlyinΩspaceandcoloredaccording targetdistributionG #ρ nwhilethecosinesimilarity,mentionedin
tothenormoftheirgradient. §5.1,showsthatI ψpermitstoaccuratelygenerateweightsassoci-
atedwithagivengradient.
8
ycneuqerFNeuralPolarFactorization
5.5.LMC-NPFonMNIST-XE
EEnndd chcaihnas iGnDs G D
WeuseLMC-NPF(Algorithm2)tosamplethelossofthe
MNISTclassifierconsideredin§5.3. Oursamplingalgo-
LMMLCM-NCPF
rithmisprecededbyawarm-upcontainingparticledescents
toexploregoodminimabeforesamplingasdetailedinthe
LLMMCC
supplementaryfiles.
SamplingAlgorithmResults. Followingthewarm-up,
the TSNE (Figure 5) as well as Figure 4 show that our
sampling algorithm proposes high-accuracy weights that
are completely different from the minima found during
thewarm-upperiod. SinceLMC-NPFalternatesbetween
LangevinstepsongandLangevinstepsonu ,wedemon-
θ
stratethattheseminimahadindeedbeendiscoveredthrough
the use of NPF by running an LMC algorithm initialized
with the final warm-up particles. The latter was parame-
terized the same way as the one used in LMC-NPF, with
thesamenumberofiterations. WeobservethattheLMC
algorithmsamplesaroundthewarm-upparticlesbutdoes TT<latexit sha1_base64="R25ezd3l6ArvFIhRZb4yTFiNI7k=">AAACx3icjVHLSsNAFD2Nr1pfVZdugkVwVdIuqsuiCLqRin1BLZKk0zY0LyaTYiku/AG3+mfiH+hfeGecglpEJyQ5c+49Z+be68S+lwjLes0YC4tLyyvZ1dza+sbmVn57p5lEKXdZw438iLcdO2G+F7KG8ITP2jFnduD4rOWMTmW8NWY88aKwLiYx6wb2IPT6nmsLSdWvL89u8wWraKllzoOSBgXoVYvyL7hBDxFcpAjAEEIQ9mEjoaeDEizExHUxJY4T8lSc4R450qaUxSjDJnZE3wHtOpoNaS89E6V26RSfXk5KEwekiSiPE5anmSqeKmfJ/uY9VZ7ybhP6O9orIFZgSOxfulnmf3WyFoE+jlUNHtUUK0ZW52qXVHVF3tz8UpUgh5g4iXsU54RdpZz12VSaRNUue2ur+JvKlKzcuzo3xbu8JQ249HOc86BZLpYqxcpVuVA90aPOYg/7OKR5HqGKc9TQIO8hHvGEZ+PCiIyxcfeZamS0ZhfflvHwAa4ikGY=</latexit> SNSEN E
notdetachitselffromthem. Thisconfirmsthattheuseof
PFNetinthesamplingprocedurepermitsthediscoveryof
newlocalminima.
Figure5.ResultsofLMC-NPFappliedtotheMNISTclassifier
lossfunction. TheTSNEisusedtorepresentthefinalparticles
6.Conclusion
resultingfromdescenttrajectoriesduringtheWarm-upperiod(in
red),theparticlessampledbytheLMCalgorithm(ingrey),and
Brenier’s polar factorization is arguably one of the most
theparticlessampledbyLMC-NPF(inpurple).
far-reachingresultsdiscoveredinanalysisinthelastcen-
tury,underpinningthebetterknownBreniertheoremonthe
existenceofsolutionstotheMonge(1781)problem. We scapeofnon-convexpotentials. Aninterestingdirectionfor
proposedinthisworkthefirstimplementation,tothebest perfectingthesamplingalgorithmwouldbetoreweightthe
ofourknowledge,ofthatfactorizationthatisapplicableto samplesaccordingtotheirprobability,inthesameveinas
higher-dimensional settings. To do so, we have used the SMCsamplers(DelMoraletal.,2006). Thiswouldrequire
recently proposed machinery of neural optimal transport knowledgeoftheprobabilitydistributiongeneratedbythe
solvers. Beyondsimplyexploitingthisresult,wehavealso generativemodelI ,whichisnotpossiblewiththecurrent
ψ
proposedtoestimateamultivaluedmapthatapproximates methodology.
the inverse of the measure-preserving map component in
thepolarfactorization. Wehaveshownthatsuchaninverse
References
mapcanbeofpotentialusetosampletheoptimizationland-
BrandonAmos. Onamortizingconvexconjugatesforopti-
maltransport. InTheEleventhInternationalConference
1.0
Input Input onLearningRepresentations,2023.
Generated Generated
0.5
BrandonAmos, LeiXu, andJZicoKolter. Inputconvex
0.5
neuralnetworks.InInternationalConferenceonMachine
Learning,pages146–155.PMLR,2017.
0.0 0.0
0 100 0 50
Norm Accuracy BrandonAmosetal. Tutorialonamortizedoptimization.
FoundationsandTrends®inMachineLearning,16(5):
Figure4.CharacteristicsofthepointssampledusingLMC-NPF 592–732,2023.
forthe§5.5MNISTclassifierexperiment. Ingraytheclassifier
weightshavebeendrawnuniformlyintheΩoptimizationspace, Vladimir Arnold. Sur la ge´ome´trie diffe´rentielle des
whileinpurpletheweightshavebeensampledusingAlgorithm2. groupes de lie de dimension infinie et ses applications
Generatedsamplesarecriticalpointswhicharegoodminima,as a` l’hydrodynamiquedesfluidesparfaits. InAnnalesde
shownbytheaccuracystatistics. l’institutFourier,volume16,pages319–361,1966.
9
ycneuqerF ycneuqerFNeuralPolarFactorization
JD Benamou and Y Brenier. A domain decomposition Thomas O Galloue¨t and Quentin Me´rigot. A lagrangian
methodforthepolarfactorizationofvectorfields. Con- schemea`labrenierfortheincompressibleeulerequations.
temporaryMathematics,157:231–231,1994. FoundationsofComputationalMathematics,18(4):835–
865,2018.
YannBrenier. Polarfactorizationandmonotonerearrange-
ment of vector-valued functions. Communications on AudeGenevay,GabrielPeyre´,andMarcoCuturi. Learning
pureandappliedmathematics,44(4):375–417,1991. generativemodelswithsinkhorndivergences. InInterna-
tionalConferenceonArtificialIntelligenceandStatistics,
CharlotteBunne,AndreasKrause,andMarcoCuturi.Super-
visedtrainingofconditionalmongemaps. Advancesin pages1608–1617.PMLR,2018.
NeuralInformationProcessingSystems,35:6859–6872,
Chin-WeiHuang,RickyTQChen,ChristosTsirigotis,and
2022.
AaronCourville. Convexpotentialflows: Universalprob-
CharlotteBunne,StefanGStark,GabrieleGut,JacoboSara- ability distributions with optimal transport and convex
biaDelCastillo,MitchLevesque,Kjong-VanLehmann, optimization. InInternationalConferenceonLearning
Lucas Pelkmans, Andreas Krause, and Gunnar Ra¨tsch. Representations,2020.
Learningsingle-cellperturbationresponsesusingneural
optimaltransport. NatureMethods,20(11):1759–1768, Patrick Kidger. On Neural Differential Equations. PhD
thesis,UniversityofOxford,2021.
2023.
XiangChengandPeterBartlett. Convergenceoflangevin DiederikPKingmaandJimmyBa. Adam: Amethodfor
mcmcinkl-divergence. InAlgorithmicLearningTheory, stochasticoptimization. arXivpreprintarXiv:1412.6980,
pages186–211.PMLR,2018. 2014.
Hyungjin Chung, Jeongsol Kim, and Jong Chul Ye. Di- Alexander Korotin, Vage Egiazarian, Arip Asadulaev,
rectdiffusionbridgeusingdataconsistencyforinverse Alexander Safin, and Evgeny Burnaev. Wasserstein-2
problems. AdvancesinNeuralInformationProcessing generative networks. In International Conference on
Systems,36,2024. LearningRepresentations,2020.
Djork-Arne´Clevert,ThomasUnterthiner,andSeppHochre-
Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick
iter. Fastandaccuratedeepnetworklearningbyexponen-
Haffner. Gradient-based learning applied to document
tiallinearunits(elus). arXivpreprintarXiv:1511.07289,
recognition.ProceedingsoftheIEEE,86(11):2278–2324,
2015.
1998.
SamuelCohen,BrandonAmos,andYaronLipman.Rieman-
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and
nianconvexpotentialmaps. InInternationalConference
TomGoldstein. Visualizingthelosslandscapeofneural
onMachineLearning,pages2028–2038.PMLR,2021.
nets. Advancesinneuralinformationprocessingsystems,
Arnak S Dalalyan and Avetik Karagulyan. User-friendly 31,2018.
guaranteesforthelangevinmontecarlowithinaccurate
gradient. Stochastic Processes and their Applications, Dong C Liu and Jorge Nocedal. On the limited memory
129(12):5278–5311,2019. bfgsmethodforlargescaleoptimization. Mathematical
programming,45(1-3):503–528,1989.
JohnMDanskin. Thetheoryofmax-min,withapplications.
SIAMJournalonAppliedMathematics,14(4):641–664, Guan-Horng Liu, Arash Vahdat, De-An Huang, Evange-
1966. los A Theodorou, Weili Nie, and Anima Anandkumar.
I2sb: Image-to-imageschro¨dingerbridge. arXivpreprint
ValentinDeBortoli,Guan-HorngLiu,TianrongChen,Evan-
arXiv:2302.05872,2023.
gelosATheodorou,andWeilieNie. Augmentedbridge
matching. arXivpreprintarXiv:2311.06978,2023.
Antoine Liutkus and Kazuyoshi Yoshii. A diagonal plus
PierreDelMoral,ArnaudDoucet,andAjayJasra. Sequen- low-rankcovariancemodelforcomputationallyefficient
tialmontecarlosamplers. JournaloftheRoyalStatistical sourceseparation. In2017IEEE27thInternationalWork-
SocietySeriesB:StatisticalMethodology,68(3):411–436, shoponMachineLearningforSignalProcessing(MLSP),
2006. pages1–6.IEEE,2017.
Marylou Gabrie´, Grant M Rotskoff, and Eric Vanden- AshokMakkuva,AmirhosseinTaghvaei,SewoongOh,and
Eijnden. Adaptivemontecarloaugmentedwithnormal- JasonLee. Optimaltransportmappingviainputconvex
izing flows. Proceedings of the National Academy of neuralnetworks.InInternationalConferenceonMachine
Sciences,119(10),2022. Learning,pages6672–6681.PMLR,2020.
10NeuralPolarFactorization
Quentin Me´rigot and Jean-Marie Mirebeau. Mini-
malgeodesicsalongvolume-preservingmaps,through
semidiscreteoptimaltransport. SIAMJournalonNumer-
icalAnalysis,54(6):3465–3492,2016.
GaspardMonge. Me´moiresurlathe´oriedesde´blaisetdes
remblais. Histoiredel’Acade´mieRoyaledesSciences,
pages666–704,1781.
Guillaume Morel, Lucas Drumetz, Simon Bena¨ıchouche,
Nicolas Courty, and Franc¸ois Rousseau. Turning nor-
malizingflowsintomongemapswithgeodesicgaussian
preserving flows. Transactions on Machine Learning
Research,2023.
GabrielPeyre´,MarcoCuturi,etal. Computationaloptimal
transport: With applications to data science. Founda-
tions and Trends® in Machine Learning, 11(5-6):355–
607,2019.
EmiliaPompe,ChrisHolmes,andKrzysztofŁatuszyn´ski.
A framework for adaptive mcmc targeting multimodal
distributions. TheAnnalsofStatistics,48(5):2930–2952,
2020.
AadityaRamdas,Nicola´sGarc´ıaTrillos,andMarcoCuturi.
Onwassersteintwo-sampletestingandrelatedfamilies
ofnonparametrictests. Entropy,19(2):47,2017.
Gareth O. Roberts and Richard L. Tweedie. Exponential
convergenceofLangevindistributionsandtheirdiscrete
approximations. Bernoulli,2(4):341–363,1996.
JohnVRyff. Measurepreservingtransformationsandre-
arrangements. Journal of Mathematical Analysis and
Applications,31(2):449–458,1970.
FilippoSantambrogio. Optimaltransportforappliedmathe-
maticians. Birka¨user,NY,55(58-63):94,2015.
J.Saunderson,V.Chandrasekaran,P.A.Parrilo,andA.S.
Willsky. Diagonalandlow-rankmatrixdecompositions,
correlationmatrices,andellipsoidfitting. SIAMJournal
onMatrixAnalysisandApplications,33(4):1395–1416,
2012.
Vignesh Ram Somnath, Matteo Pariset, Ya-Ping Hsieh,
MariaRodriguezMartinez,AndreasKrause,andChar-
lotteBunne. Aligneddiffusionschro¨dingerbridges. In
The39thConferenceonUncertaintyinArtificialIntelli-
gence,2023.
LaurensVanderMaatenandGeoffreyHinton. Visualizing
datausingt-sne. Journalofmachinelearningresearch,9
(11),2008.
11NeuralPolarFactorization
A.Computationoftheconvexconjugate
Givenafunctionconvexfunctionu,andapointy,theobjectiveJ (x,y)= y,x u(x)isconcavewithrespecttoxand
u
⟨ ⟩−
u∗(y) = sup J (x,y)canbecomputedusingoptimizationalgorithmslikegradientascent,(L)BFGSorAdam. Asfor
x u
u∗(y),takingthegradientforynecessitatesdifferentiatethroughasupremum. Inourcase,uisstrictlyconvexa.e. and
∇
thesupremumbecomesamaximum:
u∗(y)=maxJ (x,y)
u
x
Danskin’senvelopetheorem(1966)allowstodifferentiatethroughthismaximumandtowrite:
u∗(y)= maxJ (x,y)
y y u
∇ ∇ x
=( J (x,y))(x∗(y))
y u
∇
wherex∗(y)istheoptimalxthatmaximizesJ (x,y). Because J (x,y)=x,wegetthat
u y u
∇
u∗(y)=x∗(y)
y
∇
B.PreconditionnedLMC
(cid:112)
x(k+1) =x(k) γ f(x(k))+ 2γz(k), z(k) (0,I )
d
− ∇ ∼N
Byreplacing f withitspolarfactorization,theprocedurebecomes:
∇
(cid:112)
x(k+1) =x(k) γ u M(x(k))+ 2γz(k)
− ∇ ◦
Bystudyingy(k) =M(x(k)),onecanseethattheLMCprocedureimpliesdoingapreconditionedLMCalgorithmonthe
convexfunctionu.
y(k+1) =M(x(k+1))
(cid:16) (cid:112) (cid:17)
=M x(k) γ u(y(k))+ 2γz(k)
− ∇
(cid:104) (cid:112) (cid:105)
=M(x(k))+J (x(k)) γ u(y(k))+ 2γz(k)
M
− ∇
+ ( ε )
◦ ∥ ∥
(cid:112)
=y(k) γJ (x(k)) u(y(k))+ 2γJ (x(k))z(k)
M M
− ∇
+ ( ε )
◦ ∥ ∥
withε= γ u(y(k))+√2γz(k). OnecannotethatthepreconditionedmatrixH =J (x(k))isnotnecessarilypositive
M
− ∇
definite.
C.AugmentedBridgeMatching
GivenacouplingΠ andrandomvariables(X ,X ),theAugmentedbridgematchingalgorithm(DeBortolietal.,2023)
0,1 0 1
aimsatlearningastochasticdynamicmappingbetweenX andX thatpreservesthecouplingΠ .
0 1 0,1
In the probability space of path measures ( ([0,1],Rd)), let denotes the path measures associated to the SDE
dX =v (X )dt+σ dB ,thefunctionsσandP vC beinglocallyLipscM hitz. GivenapathmeasureQ ,thediffusionbridge
t t t t t
ofQwhichisthedistributionofQconditionedonbothendpointisdenotedbyQ . Thesetof∈ paM thmeasuresconsidered
|0,1
to bridge (X ) and (X ) according to the coupling Π is Π Q = (cid:82) Q (.x ,x )Π (dx ,dx ). In
DeBortoliP etal0 .(2023)P ,the1 authorsshowedthatundermild0 c,1 onditio0 n,1 s,Π|0,1 Q Rd w× aR sd ass| o0, c1 iat| ed0 to1 thef0 o,1 llowi0 ngS1 DE:
0,1 |0,1
dX = b (X )+σ2u dt+σ dB , X µ
t { t t t t } t t 0 ∼
w timith eu 1t g= iveE nP t1 h|0 e,t s(cid:2) ta∇ telo atg tQ im1 e|t( tX an1 d|X tht) e|X co0 n, dX itit o(cid:3) nw alh der ise trQ ib1 u|t tia on nd oP f1 P|0 a,t ta tir me ere 1sp ge ivc eti nve thly et ch oe uc po lin nd git si to an teal ad ti ts imtri eb 0ut aio nn do t.fQat
12NeuralPolarFactorization
ThisSDEgivesawaytosamplefromΠ byfirstsamplingX µandthendiscretizetheSDEtogetX . Becauseu is
0,1 0 1 t
∼
intractable,itisapproximatedbyaneuralnetworkuθ learnedtominimizetheregressionloss:
t
(cid:90) 1
λ E[ uθ(X ,X ) logQ (X X ) 2]dP(X ,X ,X )
t ∥ t 0 t −∇ 1|t 1 | t ∥ 0 t 1
0
AparticularcaseofdiffusionbridgeistheBrownianbridgeQ forwhichv =0andσ =σwhichistheoneusuallyused
|0,1 t
inpractice.
D.Topographyexperiments
D.1.Creationofthedataset
WeusedthePythonpackageelevationtogettheelevationofthreedifferentregionsoftheglobe: Chamonix,London,
andCyprus. Giventhelatitudesandlongitudesofthedesiredarea,elevationreturnsagridoftheareawiththeelevation
value at each grid point. For the Chamonix example, we obtained 323932 points (x,y) R2 and their corresponding
∈
elevation. Wedequantizedtheelevationsbyaddingauniformnoiseon[0,1]tothembeforeusingaGaussianfiltertomake
thegradientssmoother.Todothis,weusedthefunctiongaussian filterfromthescipylibrary.Wethennumerically
estimatethegradientsassociatedwiththeelevationandobtainadatasetof323932pointsinR2andtheassociatedgradients
inR2fortheexampleofChamonix. WeobtaineddatafortheCyprusandLondonregionsinthesameway.
D.2.LondonandCyprusresults
⇢ <latexit sha1_base64="c3a2tFB4zesxAsTcuJ2Eo7+FeIs=">AAACynicjVHLSsNAFD2Nr1pfVZeKBIvgqiQuqsuiGxcuWrAPaEtJ0mkbzIvJRCjFnX6AW/2R/on4B4o/4Z1pCmoRnZDkzLnn3Jl7rx15biwM4zWjLSwuLa9kV3Nr6xubW/ntnXocJtxhNSf0Qt60rZh5bsBqwhUea0acWb7tsYZ9cyHjjVvGYzcMrsUoYh3fGgRu33UsQVSjzYdhN8h18wWjaKilzwMzBYXy/qT68XAwqYT5F7TRQwgHCXwwBBCEPViI6WnBhIGIuA7GxHFCrooz3CFH3oRUjBQWsTf0HdCulbIB7WXOWLkdOsWjl5NTxxF5QtJxwvI0XcUTlVmyv+Ueq5zybiP622kun1iBIbF/+WbK//pkLQJ9nKkaXKopUoyszkmzJKor8ub6l6oEZYiIk7hHcU7YUc5Zn3XliVXtsreWir8ppWTl3km1Cd7lLWnA5s9xzoP6SdEsFUtVmvQ5piuLPRzimOZ5ijIuUUFNVfmIJzxrVxrXRtp4KtUyqWcX35Z2/wk3wZVv</latexit> n F<latexit sha1_base64="OjjtLGaF0cZcfwa/nak8zcXJ9C4=">AAAC0XicjVHLSsNAFD2Nr9r6qLp0E6yCq5K6qC6LgrisaB/QaknSaQ3mxWRSKKUgbv0Bt/oR/oAfIf6BfoCuvTNNwQeiE5KcOfeeM3PvtULXiYRhPKe0qemZ2bn0fCa7sLi0nFtZrUVBzG1WtQM34A3LjJjr+KwqHOGyRsiZ6Vkuq1uXBzJe7zMeOYF/KgYhO/PMnu90HdsURJ0ftoet/Ehv8Yug7WfaubxRMNTSf4JiAvLlzbeHx372vRLkntBCBwFsxPDA4EMQdmEioqeJIgyExJ1hSBwn5Kg4wwgZ0saUxSjDJPaSvj3aNRPWp730jJTaplNcejkpdWyRJqA8Tliepqt4rJwl+5v3UHnKuw3obyVeHrECF8T+pZtk/lcnaxHoYk/V4FBNoWJkdXbiEquuyJvrn6oS5BASJ3GH4pywrZSTPutKE6naZW9NFX9RmZKVezvJjfEqb0kDLn4f509Q2ykUS4XSMU16H+OVxjo2sE3z3EUZR6igSt4ct7jDvXaiDbQr7XqcqqUSzRq+LO3mAwLnmI8=</latexit> #⇢ n <latexit sha1_base64="go8CTm3a+wB3MYe5dVV0Vh+gZpk=">AAAC43icjVHLSsNAFD2N7/qquhQkWAVXJXVRXYpuXCpYW2hKmaRTG0yTMJkUSunOnTtx6w+4VfwBP0L8A/0AXXtnmoIPRCckOXPuOWfmzjiR78XSsp4zxtj4xOTU9Ex2dm5+YTG3tHwah4lwedkN/VBUHRZz3wt4WXrS59VIcNZxfF5xzg9UvdLlIvbC4ET2Il7vsLPAa3kuk0Q1cmt2wByfmUnDlm0umdkfNPp2fmDaoh02SJC3CpYe5k9QTEF+b+Pt4bE7+34U5p5go4kQLhJ0wBFAEvbBENNTQxEWIuLq6BMnCHm6zjFAlrwJqTgpGLHn9D2jWS1lA5qrzFi7XVrFp1eQ08QmeULSCcJqNVPXE52s2N+y+zpT7a1HfyfN6hAr0Sb2L99I+V+f6kWihV3dg0c9RZpR3blpSqJPRe3c/NSVpISIOIWbVBeEXe0cnbOpPbHuXZ0t0/UXrVSsmrupNsGr2iVdcPH7df4Ep9uFYqlQOqab3sdwTGMV69ii+9zBHg5xhDJlX+AWd7g3uHFpXBnXQ6mRST0r+DKMmw/9VJ/o</latexit> u ✓#⇢ n M<latexit sha1_base64="E6xaG29fcOnuDpiwJIN2K2lJjL4=">AAAC33icjVHLSsNAFD3Gd+sj6k43wSq4KqmL6lJ040ZQsFawpUzSaRtMkzCZFEopuHMnbv0Bt7ryB/wI8Q/0A3TtnWkEtYhOSHLm3HvOzL3XiXwvlrb9PGKMjo1PTE5NZ7Izs3Pz5sLiSRwmwuUlN/RDceqwmPtewEvSkz4/jQRnbcfnZed8T8XLHS5iLwyOZTfi1TZrBl7Dc5kkqmYuH9R6FdnikvWtXp9wrm9VRCusBZmambPztl7WMCikILez9vbw2Mm+H4bmEyqoI4SLBG1wBJCEfTDE9JyhABsRcVX0iBOEPB3n6CND2oSyOGUwYs/p26TdWcoGtFeesVa7dIpPryClhXXShJQnCKvTLB1PtLNif/PuaU91ty79ndSrTaxEi9i/dJ+Z/9WpWiQa2NY1eFRTpBlVnZu6JLor6ubWl6okOUTEKVynuCDsauVnny2tiXXtqrdMx190pmLV3k1zE7yqW9KACz/HOQxONvOFYr54RJPexWBNYQWr2KB5bmEH+zhEibwvcIs73BvMuDSujOtBqjGSapbwbRk3H2N2niA=</latexit> ✓#⇢ n
r
Figure6.RespectiveactionsofthelearnedvectorfieldsassociatedwiththepolarfactorizationoftheelevationgradientintheLondon
area.
Figure7. I ψ’sabilitytoreplacegradientsintheoriginalΩspacefortheexampleofLondonregion’selevationgradient.
13NeuralPolarFactorization
⇢ <latexit sha1_base64="c3a2tFB4zesxAsTcuJ2Eo7+FeIs=">AAACynicjVHLSsNAFD2Nr1pfVZeKBIvgqiQuqsuiGxcuWrAPaEtJ0mkbzIvJRCjFnX6AW/2R/on4B4o/4Z1pCmoRnZDkzLnn3Jl7rx15biwM4zWjLSwuLa9kV3Nr6xubW/ntnXocJtxhNSf0Qt60rZh5bsBqwhUea0acWb7tsYZ9cyHjjVvGYzcMrsUoYh3fGgRu33UsQVSjzYdhN8h18wWjaKilzwMzBYXy/qT68XAwqYT5F7TRQwgHCXwwBBCEPViI6WnBhIGIuA7GxHFCrooz3CFH3oRUjBQWsTf0HdCulbIB7WXOWLkdOsWjl5NTxxF5QtJxwvI0XcUTlVmyv+Ueq5zybiP622kun1iBIbF/+WbK//pkLQJ9nKkaXKopUoyszkmzJKor8ub6l6oEZYiIk7hHcU7YUc5Zn3XliVXtsreWir8ppWTl3km1Cd7lLWnA5s9xzoP6SdEsFUtVmvQ5piuLPRzimOZ5ijIuUUFNVfmIJzxrVxrXRtp4KtUyqWcX35Z2/wk3wZVv</latexit> n F<latexit sha1_base64="OjjtLGaF0cZcfwa/nak8zcXJ9C4=">AAAC0XicjVHLSsNAFD2Nr9r6qLp0E6yCq5K6qC6LgrisaB/QaknSaQ3mxWRSKKUgbv0Bt/oR/oAfIf6BfoCuvTNNwQeiE5KcOfeeM3PvtULXiYRhPKe0qemZ2bn0fCa7sLi0nFtZrUVBzG1WtQM34A3LjJjr+KwqHOGyRsiZ6Vkuq1uXBzJe7zMeOYF/KgYhO/PMnu90HdsURJ0ftoet/Ehv8Yug7WfaubxRMNTSf4JiAvLlzbeHx372vRLkntBCBwFsxPDA4EMQdmEioqeJIgyExJ1hSBwn5Kg4wwgZ0saUxSjDJPaSvj3aNRPWp730jJTaplNcejkpdWyRJqA8Tliepqt4rJwl+5v3UHnKuw3obyVeHrECF8T+pZtk/lcnaxHoYk/V4FBNoWJkdXbiEquuyJvrn6oS5BASJ3GH4pywrZSTPutKE6naZW9NFX9RmZKVezvJjfEqb0kDLn4f509Q2ykUS4XSMU16H+OVxjo2sE3z3EUZR6igSt4ct7jDvXaiDbQr7XqcqqUSzRq+LO3mAwLnmI8=</latexit> #⇢ n <latexit sha1_base64="go8CTm3a+wB3MYe5dVV0Vh+gZpk=">AAAC43icjVHLSsNAFD2N7/qquhQkWAVXJXVRXYpuXCpYW2hKmaRTG0yTMJkUSunOnTtx6w+4VfwBP0L8A/0AXXtnmoIPRCckOXPuOWfmzjiR78XSsp4zxtj4xOTU9Ex2dm5+YTG3tHwah4lwedkN/VBUHRZz3wt4WXrS59VIcNZxfF5xzg9UvdLlIvbC4ET2Il7vsLPAa3kuk0Q1cmt2wByfmUnDlm0umdkfNPp2fmDaoh02SJC3CpYe5k9QTEF+b+Pt4bE7+34U5p5go4kQLhJ0wBFAEvbBENNTQxEWIuLq6BMnCHm6zjFAlrwJqTgpGLHn9D2jWS1lA5qrzFi7XVrFp1eQ08QmeULSCcJqNVPXE52s2N+y+zpT7a1HfyfN6hAr0Sb2L99I+V+f6kWihV3dg0c9RZpR3blpSqJPRe3c/NSVpISIOIWbVBeEXe0cnbOpPbHuXZ0t0/UXrVSsmrupNsGr2iVdcPH7df4Ep9uFYqlQOqab3sdwTGMV69ii+9zBHg5xhDJlX+AWd7g3uHFpXBnXQ6mRST0r+DKMmw/9VJ/o</latexit> u ✓#⇢ n M<latexit sha1_base64="E6xaG29fcOnuDpiwJIN2K2lJjL4=">AAAC33icjVHLSsNAFD3Gd+sj6k43wSq4KqmL6lJ040ZQsFawpUzSaRtMkzCZFEopuHMnbv0Bt7ryB/wI8Q/0A3TtnWkEtYhOSHLm3HvOzL3XiXwvlrb9PGKMjo1PTE5NZ7Izs3Pz5sLiSRwmwuUlN/RDceqwmPtewEvSkz4/jQRnbcfnZed8T8XLHS5iLwyOZTfi1TZrBl7Dc5kkqmYuH9R6FdnikvWtXp9wrm9VRCusBZmambPztl7WMCikILez9vbw2Mm+H4bmEyqoI4SLBG1wBJCEfTDE9JyhABsRcVX0iBOEPB3n6CND2oSyOGUwYs/p26TdWcoGtFeesVa7dIpPryClhXXShJQnCKvTLB1PtLNif/PuaU91ty79ndSrTaxEi9i/dJ+Z/9WpWiQa2NY1eFRTpBlVnZu6JLor6ubWl6okOUTEKVynuCDsauVnny2tiXXtqrdMx190pmLV3k1zE7yqW9KACz/HOQxONvOFYr54RJPexWBNYQWr2KB5bmEH+zhEibwvcIs73BvMuDSujOtBqjGSapbwbRk3H2N2niA=</latexit> ✓#⇢ n
r
Figure8.RespectiveactionsofthelearnedvectorfieldsassociatedwiththepolarfactorizationoftheelevationgradientintheCyprus
neighborhood.
Figure9. I ψ’sabilitytoreplacegradientsintheoriginalΩspacefortheexampleofCyprusregion’selevationgradient.
Chamonix London Chypre
W ( u ρ ,F ρ ) 0.27 0.11 0.020
ε θ# n # n
∇
W ( F ρ ,F ρ′ ) 1.55 0.33 0.036
Eε ∇
[S
# ((In (M# (n
(x )),z), (x ))] 0.048 0.23 0.041
k,z ε ψ θ α k α k
B B
Figure10. NPFandI ψperformancesforthetopographyexperiments.
E.LeNetclassifierExperiments
E.1.LeNetclassifierarchitecture
TheLeNetclassifierarchitectureusedfortheexperimentsiscomposedoftwoconvolutivelayersfollowedbyareluactivation
functionandamaxpooling;itendswithadenselayerasdescribedinFigure11. Theoptimizationspacethatweconsideris
Ω=[ 1,1]222.
−
E.2.ComplementarygraphsfortheMNISTclassifierexperiments
14NeuralPolarFactorization
Convolution
(features:1,kernelsize:5 5,padding:”VALID”)
×
Max-Pooling
(window:2 2,strides:2 2)
× ×
Convolution
(features:1,kernelsize:5 5,padding:”VALID”)
×
Max-Pooling
(window:2 2,strides:2 2)
× ×
Flattenlayer
Dense
(outputsize:10)
(0,0,1,0,0,0,0,0,0,0)
Figure11. LeNetclassifierarchitectureusedinexperiments.
Input Input Input
Generated Generated 0.2 Generated
0.5
0.5
0.1
0.0 0.0 0.0
0 100 0 50 100 10 20
Norm Loss Accuracy
Figure12.Characteristicsofthegeneratedcriticalpointsforthe§5.3MNISTclassifierexperiment.Ingraytheclassifierweightshave
beendrawnuniformlyintheΩoptimizationspace,whileinpurpletheweightshavebeendrawnusingI ψ(∇u∗ θ(0),z).
0.10
0.10
0.2
0.05
0.05
0.0 0.00 0.00
0 5 10 0 1 0.10 0.15 0.20
Distance Cosine Stochasticity
Figure13.PerformancesofI ψforthe§5.4MNISTclassifierexperimentandstochasticityoftheMNISTloss.Thedistanceandcosine
plotsdemonstratetheabilityofI ψtocorrectlygenerateweightswithafixedgradient.Forthestochasticityplot,theclassifierweightsare
fixedtotheweightsobtainedaftergradientdescentanddifferentminibatchesofMNISTimagesareusedtocomputethegradientofthe
lossfunction.Thestochasticityplotshowsthedistributionofthesinkhorndivergencebetweentwogradientbatchescomputedfromthe
sameweights.
15
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF ycneuqerF
ycneuqerFNeuralPolarFactorization
Input Input Input
0.50 0.4
Generated Generated Generated
0.2
0.25 0.2
0.00 0.0 0.0
0 100 0 50 100 0 50
Norm Loss Accuracy
Figure14.Characteristicsofthegeneratedcriticalpointsforthe§5.4MNISTclassifierexperiment.Ingraytheclassifierweightshave
beendrawnuniformlyintheΩoptimizationspace,whileinpurpletheweightshavebeendrawnusingI ψ(∇u∗ θ(0),z).
0.050
0.5 0.05
0.025
0.0 0.000 0.00
0 10 0 1 0.1 0.2
Distance Cosine Stochasticity
Figure15.PerformancesofI ψforthe§5.5samplingMNISTclassifierexperiment.Thedistanceandcosineplotsdemonstratetheability
ofI ψtocorrectlygenerateweightswithafixedgradient.Forthestochasticityplot,theclassifierweightsarefixedtothefinalsampled
particlesanddifferentminibatchesofMNISTimagesareusedtocomputethegradientofthelossfunction.Thestochasticityplotshows
thedistributionofthesinkhorndivergencebetweentwogradientbatchescomputedfromthesameweights.
1.0 1.0
Input Input Input
Generated Generated Generated
0.5
0.5 0.5
0.0 0.0 0.0
0 100 0 50 100 0 50
Norm Loss Accuracy
Figure16.CharacteristicsofthepointssampledusingAlgorithm2forthe§5.5MNISTclassifierexperiment.Ingraytheclassifierweights
havebeendrawnuniformlyintheΩoptimizationspace,whileinpurpletheweightshavebeensampledusingAlgorithm2.Thegenerated
samplesarecriticalpointswhicharegoodminima,asshownbytheaccuracystatistics.
16
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerFNeuralPolarFactorization
F.Hyperparameters
F.1.Parameterizeu
θ
Inallexperiments,theconvexfunctionuisparameterizedusinganICNNu whosearchitectureisdetailedin§3.1. The
θ
rankofthequadratictermQ (x)isalwaystakenequalto1,whichmeansthatAisarowmatrix. Wenotedthatitwas
A,δ
necessarytochoosesmoothactivationfunctionsinu’sparameterizationtoavoidconvergenceproblemswiththe
conjugatesolvers,thatoccurespeciallyinhighdimension. ThisiswhywehavefavoredtheuseofELU(Clevertetal.,
2015)activationsintheuparameterizationratherthanReluactivations.
F.2.Computationofu∗
θ
TheuseofaconjugatesolverisnecessarytocomputethelossfunctionsofV ,M ,I andtoestimateM . Inallcases,
ϕ ξ ψ θ
the objective is to estimate the gradient of u ’s conjugate at a given point y: (u )∗(y). For a given experiment, that
θ θ
∇
justifiestheuseofthesameconjugatesolverparametersforthesedifferentapplications. WereliedonADAMsolverforthe
computationoftheconvexconjugateasitrunsfasterthanLBFGSonourexamplesanduseAmosimplementation. The
twohyperparametersthatremaintobesetarethemaximumnumberofiterationsgiventothesolvertoconvergeandthe
tolerancefactoratwhichthenormofthegradientisconsideredsmallenoughforthesolvertohaveconverged. Thesetwo
hyperparametersarestronglydependentonthedimensionoftheproblemaswellasonthefunctionu and,therefore,onthe
θ
distributionsρandF ρ. Toamortizethenumberofiterationsrequiredforthesolvertoconverge,itisalwaysinitialized
#
withthepredictionoftheV networkthatistrainedinconjunctionwithu .
ϕ θ
F.3.ParameterizeV
ϕ
WeuseanMLPwith2hiddenlayersofsize512andReluactivationfunctionstoparameterizeV inallourexperiments.
ϕ
F.4.ParameterizeM
ξ
Themeasure-preservingmapM isparameterizedbyaneuralnetworkonlywhenthevectorfieldunderstudyisavailable
throughsamplesonly. ThisisthecaseinthetopographyexampleswhereM isparameterizedbyanMLPwith2hidden
ξ
layersofsize512andReluactivationfunctions.
F.5.ParameterizeX
ψ
ThelearnedpartofthedriftX isparameterizedusinganMLP,andweuseSiluactivationfunctionswhichistheclassic
ψ
choiceforparameterizingthedriftX .
ψ
ThesamehyperparametershavebeenusedfortheChamonix,London,andCypruscases.
17NeuralPolarFactorization
model hyperparameter value
u activationfunction elu
θ
architecture [64,64,64,64]
b1 0.50
b2 0.50
scheduler cosinedecay
initiallearningrate 0.001
α 0.10
schedulersteps 50000
trainingsteps dualstepICNN
steps 50000
I activationfunction silu
ψ
architecture [256,256,256]
scheduler cosinedecay
initiallearningrate 0.001
α 0.010
schedulersteps 50001
trainingsteps flowmatching
steps 50000
σ 0.10
V activationfunction relu
ϕ
architecture [512,512]
b1 0.90
b2 0.999
scheduler cosinedecay
initiallearningrate 0.0005
α 0.010
schedulersteps 51001
steps 50001
conjugatesolver name Adam
maxiteration 200
gtol 0.0010
Figure17.Hyperparametersusedforthetopographyexperiments(thesamehyperparametershavebeenusedforChamonix,London,and
Cyprus).
18NeuralPolarFactorization
model hyperparameter value
u activationfunction elu
θ
architecture [128,128,128,128]
b1 0.50
b2 0.50
scheduler cosinedecays
initiallearningrate 0.001
α 0.010000
schedulersteps 10000
trainingsteps dualstepICNN
steps 10000
I activationfunction silu
ψ
architecture [512,512]
scheduler cosinedecay
initiallearningrate 0.0005
α 0.010
schedulersteps 4000°
trainingsteps flowmatching
steps 50000
σ 1.0
V activationfunction relu
ϕ
architecture [512,512]
b1 0.90
b2 0.999
scheduler cosinedecay
initiallearningrate 0.0005
α 0.010
schedulersteps 11000
steps 10001
conjugatesolver name Adam
maxiterations 700
gtol 0.10
Figure18. Hyperparametersusedforexperiment6.3.
19NeuralPolarFactorization
model hyperparameter value
u activationfunction elu
θ
architecture [128,128,128,128]
b1 0.50
b2 0.50
scheduler cosinedecay
initiallearningrate 0.0001
α 0.10
schedulersteps 30000
trainingsteps dualstepICNN
steps 1002
I activationfunction silu
ψ
architecture [512,512]
scheduler cosinedecay
initiallearningrate 0.0005
α 0
schedulersteps 61003
trainingsteps flowmatching
σ 0.10
V activationfunction relu
ϕ
architecture [512,512]
b1 0.90
b2 0.999
scheduler cosinedecay
initiallearningrate 0.0005
α 0
schedulersteps 62005
steps 1002
conjugatesolver name Adam
maxiteration 1000
gtol 0.001
particles steps 60000
coefficientLMCf 1
coefficientLMCu 1
particules 1024
warmingsteps 30000
consecutiveLMCstepsonf 200
consecutiveLMCstepsonu 200
τ 0.10
f
τ 0.10
u
Figure19. Hyperparametersusedforexperiment6.4.
20NeuralPolarFactorization
model hyperparameter value
u activationfunction elu
θ
architecture [128,128,128,128]
b1 0.50
b2 0.50
scheduler cosinedecay
initiallearningrate 0.0001
α 0.10
schedulersteps 30000
trainingsteps dualstep
steps 1002
I activationfunction silu
ψ
architecture [512,512]
scheduler cosinedecay
initiallearningrate 0.000500
α 0
schedulersteps 61003
trainingsteps flowmatching
σ 0.10
V activationfunction relu
ϕ
architecture [512,512]
b1 0.90
b2 0.999
scheduler cosinedecay
initiallearningrate 0.0005
α 0
schedulersteps 62005
steps 1002
conjugatesolver name Adam
maxiteration 1000
gtol 0.001
particles steps 60000
coefficientLMCf 1000
coefficientLMCu 1000
particules 1024
warmingsteps 30000
LMCstepsonf 200
LMCstepsonu 200
τ 0.10
f
τ 0.10
u
Figure20. Hyperparametersusedforexperiment6.5.
21