Human and LLM-Based Voice Assistant Interaction: An
Analytical Framework for User Verbal and Nonverbal
Behaviors
SZEYICHAN,NortheasternUniversity,USA
SHIHANFU,NortheasternUniversity,USA
JIACHENLI,NortheasternUniversity,USA
BINGSHENGYAO,NortheasternUniversity,USA
SMITDESAI,NortheasternUniversity,USA
MIRJANAPRPA,NortheasternUniversity,USA
DAKUOWANG∗,NortheasternUniversity,USA
Recentprogressinlargelanguagemodel(LLM)technologyhassignificantlyenhancedtheinteractionexperi-
encebetweenhumansandvoiceassistants(VAs).Thisprojectaimstoexploreauser’scontinuousinteraction
withLLM-basedVA(LLM-VA)duringacomplextask.Werecruited12participantstointeractwithanLLM-VA
duringacookingtask,selectedforitscomplexityandtherequirementforcontinuousinteraction.Weobserved
thatusersshowbothverbalandnonverbalbehaviors,thoughtheyknowthattheLLM-VAcannotcapturethose
nonverbalsignals.Despitetheprevalenceofnonverbalbehaviorinhuman-humancommunication,thereisno
establishedanalyticalmethodologyorframeworkforexploringitinhuman-VAinteractions.Afteranalyzing
3hoursand39minutesofvideorecordings,wedevelopedananalyticalframeworkwiththreedimensions:1)
behaviorcharacteristics,includingbothverbalandnonverbalbehaviors,2)interactionstages—exploration,
conflict,andintegration—thatillustratetheprogressionofuserinteractions,and3)stagetransitionthroughout
thetask.Thisanalyticalframeworkidentifieskeyverbalandnonverbalbehaviorsthatprovideafoundation
forfutureresearchandpracticalapplicationsinoptimizinghumanandLLM-VAinteractions.
CCSConcepts:•Human-centeredcomputing→Userstudies;Sound-basedinput/output;Auditory
feedback;EmpiricalstudiesinHCI;Interactiontechniques.
Additional Key Words and Phrases: User Study, Exploratory Study, Large Language Model-Based Voice
Assistant,VerbalInteraction,NonverbalInteraction,HumanBehavior
ACMReferenceFormat:
SzeyiChan,ShihanFu,JiachenLi,BingshengYao,SmitDesai,MirjanaPrpa,andDakuoWang.2024.Humanand
LLM-BasedVoiceAssistantInteraction:AnAnalyticalFrameworkforUserVerbalandNonverbalBehaviors.
1,1(August2024),33pages.https://doi.org/XXXXXXX.XXXXXXX
∗Correspondingauthor:d.wang@northeastern.edu
Authors’addresses:SzeyiChan,chan.szey@northeastern.edu,NortheasternUniversity,USA;ShihanFu,sh.fu@northeastern.
edu,NortheasternUniversity,USA;JiachenLi,li.jiachen4@northeastern.edu,NortheasternUniversity,USA;BingshengYao,
b.yao@northeastern.edu,NortheasternUniversity,USA;SmitDesai,sm.desai@northeastern.edu,NortheasternUniversity,
USA;MirjanaPrpa,m.prpa@northeastern.edu,NortheasternUniversity,USA;DakuoWang,d.wang@northeastern.edu,
NortheasternUniversity,USA.
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfee
providedthatcopiesarenotmadeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeand
thefullcitationonthefirstpage.CopyrightsforcomponentsofthisworkownedbyothersthanACMmustbehonored.
Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requires
priorspecificpermissionand/orafee.Requestpermissionsfrompermissions@acm.org.
©2024AssociationforComputingMachinery.
XXXX-XXXX/2024/8-ART$15.00
https://doi.org/XXXXXXX.XXXXXXX
,Vol.1,No.1,Article.Publicationdate:August2024.
4202
guA
92
]CH.sc[
1v56461.8042:viXra2 Chan,etal.
1 INTRODUCTION
Voiceassistants(VAs)havebecomeincreasinglypopularthankstotheirabilitytoconversein
naturallanguage[10].Theseintelligentsystemscannowbeintegratedintoawiderangeofdevices,
fromsmartphonesandcomputerstospeakers,glasses,andevenwatches.Usersregularlyturnto
VAstoforassistancewithadiversearrayoftasks,rangingfromsimpletaskssuchasquerying
abouttheweatherorplayingmusictomorecomplextaskslikenavigationassistanceandcooking
support[97].MostofthecurrentpopularVAs(e.g.,AlexaandSiri)employaconversationalmodel
backendthatisprimarilyreliantondetectingapredefinedsetofintentsandrespondingwitha
predefinedsetofresponsestotheuser.Thisapproachmakesthemwell-suitedforsimpletasks[94].
Still,itcanleadtohigherrorrates,out-of-scopeexceptions,unnaturalconversationalexperience,
andlowerusersatisfactionwhenitcomestomorecomplextasksthatmayinvolvemulti-turn
interactions1[110,118].
Recognizingtheselimitations,researchersinthefieldofHuman-ComputerInteraction(HCI)
have been exploring the integration of large language models (LLMs) to enhance voice-based
interactions.ThisincludesthedevelopmentofLLM-basedvoiceassistants(LLM-VA2)[41,48,115],
whichaimtoenablemorenaturalandhuman-likecommunication[23,64,64].Withadvancements
in LLM technology, LLM-VAs now enable more continuous and human-like interactions with
users[114],allowingforconversationsthataremorefluidandnatural,resemblinghuman-human
interactionpatterns[44].LLM-VAshavebeeneffectivelyutilizedindiversedomainssuchasin-
vehiclesystems[33,48],healthcareapplications[115],robotics[53]andcookingscenarios[21].
TheseadvancedVAsmitigatethelimitationsofcommercialVAsbyengagingusersinlonger,more
continuousinteractionstoassistwithcomplextasks[5,9,27].Asthesetechnologiesevolve,a
criticalquestionemerges:howcanwefurtherenhancehumanandLLM-VAsinteractiondesign?
Takinginspirationfromtheidealsofhuman-humaninteraction,whereinnonverbalcuesplay
acrucialrole,oftenrevealingmoreauthenticthoughtsandemotionsthantheverbalbehavior
alone[8,43,77].ThesenonverbalbehaviorshavethepotentialtoaugmentthecapabilitiesofVAs,
helpingtobetterpredictuserintentionsandcreatemoreseamless,personalizedexperiences[2].
Currentstudiesonhuman-VAinteractionsoftenoverlooktheintegrationofnonverbalinterac-
tionsorfocusontheminisolation(e.g.,studiesinvolvingeye-gaze[49]),despiteresearchinother
AIagentmodalitieshavedemonstratedthevalueofholisticallyincorporatingnonverbalcues.AI
agentsusingavatarscancreateasenseofcloseness,transformingtheperceptionoftheagentfrom
ameretooltoacompanion[16].InHuman-RobotInteraction(HRI),socialrobotswithhuman-like
attributesevokemorenaturalandhuman-likeemotionalresponsesfromusers[72,96].Thesuccess
of these AI agents highlights the importance of integrating verbal and nonverbal behaviors in
human-agent interactions. Given that LLM-VAs are now capable of more natural, human-like
interactions,incorporatingverbalandnonverbalinformationcanprovidethemwithamorenu-
ancedunderstandingofuserbehavior,especiallywhenperformingcomplexmulti-turntasks.This
integrationcanleadtothedevelopmentofmoresociallyawareandintuitiveVAtechnologies.
However,inhumanandLLM-VAinteractions,thefullpotentialofleveragingnonverbalcues
remainsunexplored,largelyduetothelackofasystematicanalyticalframeworkforanalyzingthe
integrationofnonverbalbehaviorsinLLM-VAinteractions.Therefore,ourstudyaimstoaddress
this research gap by exploring human behavior in interactions with LLM-VAs and proposing
a systematic analytical framework to examine users’ both verbal and nonverbal behaviors. By
1Despitethatuserscandownloadandconfigurethird-partySkillsorActionstosupplementthebasiccapabilitiesof
commercialVAplatforms,fewusersareawareofit[94].
2Inthispaper,wearereferringtoLLM-basedvoiceassistantsasLLM-VA.
,Vol.1,No.1,Article.Publicationdate:August2024.AnAnalyticalFrameworkforUserVerbalandNonverbalBehaviors 3
leveragingtheseinsights,weseektosupportthefuturedesignofLLM-VAs,creatingmorenatural
andeffectivehuman-LLM-VAinteractions.
In this paper, we focused on cooking as our task scenario. Cooking is a commonly chosen
experimentscenarioinpriorresearch[45,50,58,95,109]foritsinherentcomplexity,requiring
extendedback-and-forthinteractionsbetweentheuserandtheLLM-VA.Moreover,thehands-
onnatureofcookingtasksincreasesthelikelihoodofparticipantsusingnonverbalsignals[50],
providing a rich environment for observing both verbal and nonverbal behaviors. This choice
ofscenarioallowsustoexplorethefullspectrumofhuman-LLM-VAinteractionsinapractical,
real-worldcontext.Buildingonourpreviousresearchthatexploreduserperceptionsofinteracting
with LLM-VA during cooking [21], we conducted a focused re-examination utilizing a subset
of the data distinct from the prior study. This paper’s analysis focuses on manual coding of 3
hoursand39minutesofvideorecordings,identifyingtheverbalandnonverbalinteractionof12
participantswithanLLM-VAastheypreparedasaladinareal-worldkitchenenvironment.These
participantswereassistedbyourLLM-VA,MangoMango(MM),whichcanprovidestep-by-step
cookinginstructions,respondtocooking-relatedgeneralinquiries,andcanprovidesocialchit-chat
functionalities.Fromthisanalysis,weproposeananalyticalframeworkincorporatingbothverbal
andnonverbalelementstoidentifyinteractionstages.
Wedevelopedananalyticalframework(showninFigure1),structuredaroundthreekeydimen-
sions:1)BehaviorCharacteristics—thisincludesbothverbalandnonverbalbehaviorsobserved
duringinteractionswiththeLLM-VA,2)InteractionStages—definedasExploration,Conflict,
andIntegration,thesestagesdelineatetheprogressionofuserinteractions,and3)StageTransi-
tion—thisdimensioninvestigateshowstagestransitthroughoutthetask,specificallyfocusing
onpatternsofstagetransition,suchastheascendingfromconflictstagetointegrationstage
and regressing from integration to conflict stage. The first dimension pinpoints the specific
characteristicsofusers’verbalandnonverbalbehaviors.Theseconddimensioncategorizesthese
behaviorsintothedefinedstages,eachstagecharacterizedbyuniquebehavioralpatterns.Thethird
dimensionexploreshowthesestagestransitionduringtheinteraction,offeringinsightsintothe
dynamicnatureofuserengagementwithLLM-VAs.
Thisframeworksystematicallyconsidersboththeverbalandnonverbalbehaviorsofparticipants
engagingwiththeLLM-VA.Itisdesignedtohighlightusercommunication’sdynamicandevolving
nature,providinginsightsintohowtheseinteractionschangeovertime.Bycontinuouslyanalyzing
theseshifts,theframeworkoffersacomprehensiveviewofstagetransitionthroughoutthetask,
facilitatingadeeperunderstandingofuserengagementandimprovingthesystem’sadaptability.
AsinterestintheapplicationofLLM-basedvoiceassistantsgrows,ourstudyseekstoprovidea
foundationforfutureresearchandimplementationbyofferingaframeworkforanalyzinguser
interactionswithLLM-basedvoiceassistants.
Insummary,ourpapermakesthefollowingcontributions:
(1) Weproposeananalyticalframeworkwiththreedimensions—behaviorcharacteristics,three
interactionstages,andstagetransition—byidentifyingkeyverbalandnonverbalbehaviors
ofusersduringcommunicationwithLLM-VAs.Thisframeworkprovidesafoundationfor
futureresearchandpracticalapplicationsinoptimizinghuman-LLM-VAinteractions.
(2) Weoutlinedesignimplicationstoguideresearchersandsystemdesignersinutilizingthis
analyticalframeworktodevelopmoreeffectiveandfluentinteractionsbetweenhumansand
LLM-VAs.
,Vol.1,No.1,Article.Publicationdate:August2024.4 Chan,etal.
Verbal Behavior Nonverbal Behavior
Behavior  Voice interaction querie   Eye contac 
Characteristics  User interaction to V   Spatial behavio 
VA responds
  Gestural behavio 
Tone change
Stage 1: Stage 2: Stage 3:
Three
Exploration Stage Conflict Stage Integration Stage
Interaction

Users familiarizing with the Users experiencing conflicts, Users receiving valid, clear, and
Stages system (e.g. activate system or such as unclear, invalid immediate responses
explore capabilities) responses from the VA 
Conflict Stage-> Integration Stage->
Stage
 Integration Stage Conflict Stage
Transition Observation of dynamic Observation of dynamic
changes in user interactions changes in user interactions
and employed strategies leading to stage regression.







leading to stage progression.
Fig.1. Theproposedanalyticalframeworkconsistsofthreedimensions:1)behaviorcharacteristics,2)the
threeinteractionstages,and3)stagetransition.
2 RELATEDWORK
Toprovideathoroughunderstandingofourstudy,webeginbyintroducingtheimportanceofboth
verbalandnonverbalbehaviors,coveringinteractionsinbothhuman-to-humanandhuman-to-VA
communicationinSection2.1.Followingthis,weexplorevariousaspectsofVAs,withaspecial
focusonthosedesignedforcookingtasks,asdetailedin2.2.
2.1 VebalandNonverbalBehaviorsincommunication
2.1.1 Behaviors in Human-Human Communication. Human communication is a complex and
dynamicprocessinvolvingmultiplemodalities.Inhuman-humancommunication,verbalandnon-
verbalbehaviorsintersect[1]:verbalbehaviors,deliveredinreal-time,serveascrucialsocialsignals
thatcanrevealthestatusofaconversation[79];nonverbalbehaviors,suchasgesturesandposture,
cancomplementbutalsocontradictverbalmessages[7].Differentinterpretationsofbehaviorscan
sometimescomplicatethedynamicofhuman-to-humaninteractionsacrosscultures,ascertain
gesturesareseenasfriendlyinoneculturebutoffensiveinanother[6,70].Consequently,the
perceptionofothersisshapedbyacombinationofverbalbehaviorandnonverbalexpressions[101].
Understandingtherelationshipbetweenverbalandnonverbalbehaviorisessentialforeffective
human-humancommunication,integratingthespokenwordandtheunspokensignalstointerpret
messages.
Inhuman-humancommunication,nonverbalcommunicationsignificantlyinfluencesinterac-
tions.Nonverbalcommunicationextendsbeyondwords[55],encompassingbehaviorsthatconvey
information,suchasbodylanguageandvisualcues,whichenhanceunderstandingwithinashared
context[28].Priorstudiesidentifiedfourkeynonverbalmodalities:tonechange(e.g.,pitchvaria-
tions),eyecontact(e.g.,gazedirection),gesturalbehavior(e.g.,waving),andspatialbehavior(e.g.,
distanceandorientationbetweenindividuals)[8,43],eachaddingdepthtoverbalexchangesand
enrichinginteractions.
Inscenarioswherephysicalnonverbalcuesareabsent,suchasintext-baseduserinterfaces(e.g.,
chatbots)usedforhuman-to-humancommunicationthroughtechnology,theintegrationofverbal
andnonverbalelementsremainsessential.Emoticonsandemojisserveasdigitalstand-insforthe
,Vol.1,No.1,Article.Publicationdate:August2024.AnAnalyticalFrameworkforUserVerbalandNonverbalBehaviors 5
nonverbalbehaviorcommonlymissinginonlineconversations,effectivelymimickingface-to-face
humaninteractions[59,111].Theelements,fromsimplekeyboard-createdfacestothewidearray
ofsymbolsforconveyingemotionsandreactions,helptobridgethecommunicationgapindigital
exchanges. Additionally, just as gestures and expressions differ significantly from one culture
toanother,theusageandinterpretationofemojisalsoreflectculturaldistinctions.Forexample,
France’stopemojisareheart-related,whileothercountriespreferfaceemojis.[67]
Whilehuman-humancommunicationservesasafoundationalcommunicationmodel,there
isongoingdebateaboutitsrelevanceforhuman-VAinteractions.SomeresearchstudiesonVAs
applyhumancommunicationprinciplestonarrowthecommunicationgapbetweenhumansand
technology, including verbal and nonverbal behaviors. In contrast, some research argues the
opposite.Forexample,Porcheronetal.citeporcheron2018voicesuggestthatthedevice’sresponses
do not have the same conversational status as human dialogue. Our study contributes to this
argument by proposing a formative approach to evaluating VUIs, with the goal of ultimately
enhancinghuman-VUIinteractions.
2.1.2 BehaviorsinHuman-VACommunication. Recognizingthesignificanceofbothverbaland
nonverbalbehaviorsincommunication,theHuman-ComputerInteraction(HCI)communityhas
mainlyconcentratedonexploringbothbehaviorsintwomainaspectsofhuman-VAinteractions:
first,integratingnonverbalbehaviorsintoVAs,enablingtheVAtodisplaygesturesorexpressions;
second,enablingVAstointerpretandrespondtomultimodalinputs,suchasverbalandnonverbal
behaviorsfromtheuser.
ExploringtheintegrationofnonverbalbehaviorsintoVAsisanincreasinglyresearchedarea
intheHCIcommunity,showingsignificantpotentialtoenhanceinteractionquality[20,38,40].
PreviousstudieshaveinvestigatedtheembodimentofVAs,enablingthemtousevisiblespeech,
facialexpressions,orbodylanguage,therebyaddingsupplementarycommunicationmodalities[68].
Thesebehaviorsenhancesynchronicityandfluencybetweenthespeakerandlistener,elevatingthe
conversationalqualityandfosteringsocialconnections[22,39].Moreover,studiesalsodemonstrate
thatuserstendtointeractwiththeseadvancedVAssimilarlytohowtheywouldwithotherhumans.
TheseadvancedVAsarecapableofgeneratingresponsessimilartothosetriggeredbyactualhuman
interactions,highlightingtheireffectivenessinmimickinghuman-likeinteractions[78].
VAspredominantlyrelyonvoicecommands,showingalimitedabilitytoperceiveandengage
withnonverbalcommunication[31].Therefore,humanverbalandnonverbalbehaviorasinputsto
enhanceinteractionswithVAsareanothergrowingresearchareaintheHCIcommunity.Inthe
Human-RobotInteraction(HRI)communityresearch,theresearchershavebeentryingtoreduce
theerrorrateofrobots’capabilitiestointerpretandrespondtohumangesturesandexpressions
accurately,suchthatitallowstheinteractionstobemoreintuitiveanduser-friendlybymirroring
thecomplexdynamicsofhumancommunication[56,57].BuildingontheadvancementsinHRI,
recentresearchexploresthepotentialofVAstodetecterrorsusingnonverbalbehavior,aimingto
overcomeexistinglimitationsandenhancetheefficacyofhuman-technologyinteractions[32].For
example,TAMAisasystemthatincorporatesgazetoenhanceconversationalinteractions.The
systemdetectsauser’sgazedirectionandmovesits"head"toestablishmutualgazewithusers,
simulatingeyecontactduringcommunication.Studyresultsshowedthatthisdesignincreased
engagementandledtohigherratesofrepeatedqueriesandextendedinteractiontimeswiththe
VA[74].Researchershavealsoinvestigatedgesturaltriggersforconversationalagentsbyeliciting
arangeofgesturestopinpointfiveviableoptions.Auserstudywasconductedtoevaluateusers’
acceptabilityandtheeffortneededtoperformthemeffectively[88].Resultsshowedthatamong
thefiveoptions,participantsmostpreferredthesnapandwavegestures.
,Vol.1,No.1,Article.Publicationdate:August2024.6 Chan,etal.
Tofurtherexplorehuman-VAcommunicationdynamics,ourstudyinvestigatesthedynamic
transitionofuserbehavior,encompassingverbalandnonverbalelements,duringinteractionswith
LLM-VA.Exploringthecharacteristicsandtransitionofusers’verbalandnonverbalbehaviors
throughouttheconversationprocesscanenhancethedesignofhuman-VAinteractions.Byunder-
standingthedynamicsofhumanbehaviorinnaturalconversationswithLLM-VAs,designerscan
improvetheuserexperienceandbridgecommunicationgapsbetweenhumansandLLM-VAsin
scenariosinvolvingcomplextasks,suchascooking.
2.2 TraditionalVoiceAssistantvs.LLM-BasedVoiceAssistant
TraditionalVAsarebecomingimportantinourdailyroutines,especiallyathomeenvironments[12,
13,60,69,97],withhouseholdsinteractingwiththemabout4.1timesdailyonaverage[15]for
serviceslikeplayingmusic,settingalarms,controllingIoTdevices,etc.[5,15,29,54,66,83,90,105].
TheVAsalsoshowpromiseinself-care[19],education[85,102,117],andhealthcare[11,17,46]
areas.DespitetheadvantagesofVAs,someusersdiscontinueVAusageduetoitslimitedcapabilities,
leadingtousers’frustrationanddisappointment[26].
InthecurrentusageofcommercialVAs,usersfrequentlyfacechallenges,includinginaccura-
ciesinspeechrecognition,out-of-scopequeries,anddependenceonexternalsourcesforquery
responses[14,32,61,71,75].ThesechallengesaremainlyduetoVAs’relianceonconventionallan-
guagemodels(e.g.,task-specificmodelstrainedundersupervisedsettings)andpredefinedheuristics
(e.g.,rulesforfilteringkeywords)[80,110,118].Asaresult,theuserinteractionsareprimarily
limitedtosingle-turnquestionsorbasiccommand-responsestructures[12,54,63].Thelimitations
restricttheflexibilityandcapacityofVAstoengageflexiblyincomplexandmulti-turnconversations
suchasincookingscenarios[89].Jaberetal.[50]alsohighlightstheimportanceofcontext-aware
voice interactions in cooking. By evaluating interactions with a wizard-led context-aware VA,
theyfoundtheinteractionstobemorefluid.Thesescenariosdemandextendedinteractionanda
deepcontextualunderstanding,highlightingtheimportanceofflexibilityintheVAs’responses
andactions.Furthermore,researchonuserbehaviorduringinteractionswithVAsoftenfocuses
onstraightforwardtasks[31]andexploringhowusersnavigateerrorsorunsatisfactoryexperi-
ences[14,32,71].However,thisapproachprovidesonlyalimitedviewofuserbehaviorinmore
complexscenarios.
Todeepenourcomprehensionofuserbehaviorwithincomplexscenarios,thedevelopmentof
LLMs,suchasLLaMA[104]andGPT-4[84],providesnewpathwaysforresearcherstoexploreand
overcomethechallengesofpreviousVAtechnologies.LLMsareeffectiveatprocessinglargetext
inputs,interpretinginstructions,andgeneratingqualityresponses[3,92,98].LLMscanalsomanage
tasksbeyondthescopeoftraditionallanguagemodels,includingmulti-turnconversationsand
complexqueries[114].Thecapabilityofthesemodelsinpromptengineeringandotherareasopens
upnewopportunitiesfordiverseapplicationsofLLMsinvariousfields[34,51,62,107,112,113].
IntegratingLLMsintoVAforcookingillustratesanexampleofhowtechnologiescanenhance
dailyactivitiesbyprovidingreal-timeassistanceandguidance.Cookingisadailytaskthatrequires
afoundationinbasiccookingtechniquesandknowledge[81].Particularlywhentryingnewrecipes
or for those new to cooking, external assistance or recipe guidance is often essential [58, 109].
Theseneedsbecomemorepronouncedwhenthecookingprocessrequiresmultitaskingorwhen
handsarepreoccupied[65].
WhileexistingAIsolutionshaveattemptedhelpuserslearnandexecutecookingtasksmore
efficiently, most of the research has focused mainly on overcoming practical challenges in the
cookingprocessitself[24,45,82,95].Thereis,however,aresearchgapinunderstandingusers’
interactionbehaviorswithLLM-VAsduringcooking,bothverballyandnonverbally.Inourstudy,
weaimtodevelopananalyticalframeworkthatidentifiestheverbalandnonverbalbehaviorsthat
,Vol.1,No.1,Article.Publicationdate:August2024.AnAnalyticalFrameworkforUserVerbalandNonverbalBehaviors 7
User Voice Assistant Conversation
 Database
Log
Alexa, What is the first step?
The first step is to chop the
romaine lettuce into bite-sized

pieces, rinse and spin dry...
...
... Large
 Prompt Module


Language
 (Recipe Information,
User Model Conversation History,

Voice Assistant Prompts)
Fig.2. Systemdiagramof“MangoMango”(MM).Theprocessbeginswithusersprovidingvoiceinputto
Alexa.Then,Alexaperformsaspeech-to-textconversionandaddsthetranscribedinputtotheconversation
log.Thislogissavedtoadatabase.Next,theconversationhistoriesareprocessedinthepromptmodule.The
completedpromptisthensenttoGPT-3.5Turbo,andfinally,theresultingresponseissentbacktoAlexa,
whereitisconvertedintospeechfortheuser,completingthesystemloop.Thecompleteanddetailedsystem
flowisdescribedinpreviouswork[21].
enhancecommunicationbetweenusersandLLM-VAsduringcomplextaskssuchascooking.By
identifyingandexamininginteractionsacrossdifferentinteractionstages,i.e.,exploration,con-
flict,andintegrationstage,weseektounderstandnotonlytheverbalcommandsandresponses,
but also the nonverbal behaviors. We aim to examine these interactions from a detailed view
to a broader perspective to understand how they change and evolve over time. These insights
willprovideafoundationforfutureresearchandpracticalapplications,optimizinghumanand
LLM-VAinteractions.Ourcomprehensiveapproachemphasizestheimportanceofunderstanding
thesedynamicstoinformthedevelopmentofmoreintuitiveandresponsiveVAsystemsthatcater
effectivelytousers’needsduringcomplextasks.
3 SYSTEMDESCRIPTION:THE“MANGOMANGO”SYSTEM
OurstudyemployedtheLLM-VA“MangoMango”(MM)3developedfrompreviouswork[21].MM
wascreatedusingtheAlexaSkillplatform,anditsworkflowisillustratedinFigure2.MMwas
integratedintoAmazonSmartSpeakers,takingadvantageoftheflexibilityofText-to-Speech(TTS)
andSpeech-to-Text(STT)technologiestoenhanceapplicationefficiencyandeffectiveness.
MMwasenhancedbyintegratingwiththeGPT-3.5-Turbomodel,significantlyupgradingits
naturallanguageprocessingcapabilities.GPT-3.5-Turboexcelsinbothunderstandingandgenerat-
ingnaturallanguage.Akeyfeatureofthismodelisitsabilitytohandleextensiveinputcontent,
allowingfortheinclusionofpastconversationhistories.Thiscapabilityleadstomorecoherent
andcontextuallyappropriatemulti-roundconversations.Furthermore,theGPT-3.5-Turbomodel
providesrobustandstableAPIsupport,whichisinstrumentalinensuringthesmoothexecutionof
ourlaboratoryexperiments.
This system features a prompting module designed specifically for cooking scenarios. This
moduleorganizesandreconstructsuserinputbeforesendingittotheLLM(GPT-3.5-Turbomodel).
Theinputcomprisesalltheessentialinformationabouttherecipeandcustomizedinstructionsin
responsetouserqueries.Forourexperiment,weaccuratelytranscribedaYouTubevideorecipefor
3Inthispaper,MMreferstotheLLM-basedsystem,whileAlexareferstheoriginalsystemorthedeviceitself.
,Vol.1,No.1,Article.Publicationdate:August2024.8 Chan,etal.
Participant Gender Age CookingFrequency VAUsageFrequency
P2 F 18-24 Atleastonceperweek Atleastoncepermonth
P3 F 25-34 Atleastonceperweek Rarely
P4 M 25-34 Daily Atleastonceperweek
P5 M 18-24 Daily Atleastoncepermonth
P6 M 25-34 Daily Daily
P7 F 25-34 Daily Atleastonceperweek
P8 M 18-24 Daily Daily
P9 F 25-34 Atleastonceperweek Atleastonceperweek
P10 F 25-34 Daily Rarely
P11 M 25-34 Atleastonceperweek Rarely
P12 M 18-24 Daily Daily
P13 M 25-34 Daily Daily
Table1. Demographicsofparticipants(Female=F,Male=M).Theagesofparticipantswerecollectedin
formsofranges.Thetablelistedthefrequencyofusers’cookingactivitiesandtheirusageofVAs.
ChickenMangoAvocadoSalad,ensuringtheoriginalityoftheinstructions.Thisstructureensures
preciseandrelevantassistanceinourcooking-relatedtasks.
4 METHODS
Weperformedafocusedreexaminationofdatacollectedfromanexistingstudy[21]toexplorehow
users’behaviorsemergeandevolveduringinteractionswithLLM-VAs.Whilethepreviouswork
concentratedonthedesign,development,anduserperceptionsofthe"MangoMango"system,the
currentstudyshiftsfocustoanalyzinguserbehaviorusingadifferentsubsetofthestudydata.
Fromthisanalysis,weproposeananalyticalframeworkforunderstandingnonverbalbehaviorsin
LLM-VAinteractions.
Thissectionprovidesanoverviewofourstudydetailsandmethodology.Section4.1explainsour
recruitmentprocess,Section4.2outlinesthedesignandprocedureofourstudy,andSection4.3
detailsourdatacollectionandanalysismethods.
4.1 RecruitmentandParticipants
Intheoriginalstudy,participantswererecruitedthroughsocialmediaplatformsandemailby
sendingrecruitmentposters,whichincludeddetailsexplainingourresearchobjectives,adirectlink,
andaQRcodethatdirectedinterestedindividualstothescreeningquestionnaire.Thescreening
questionnaireensuredthatparticipantsmetspecificcriteria:atleast18yearsold,fluentinEnglish,
experiencedincooking,comfortablewithaudiovisualrecordingduringtheexperiment,andno
allergiestotheingredientsusedinthestudy.Atotalof13participantsweresuccessfullyenrolled
inourstudy.Onedatapointwasremovedfromthedatasetduetoasystemchange,resultingina
finalpoolof12participants.ThedemographicsoftheparticipantsareshowninTable1.
Toensuresafety,thecookingprocesswasdesignedtoavoidusingovens,sharpknives,stoves,or
anyotherpotentiallyhazardoustoolsandappliances.Inrecognitionoftheparticipant’stimeand
contribution,eachparticipantreceiveda$30Amazone-giftcard.Thestudy’sdesignandprotocol
receivedapprovalfromtheuniversity’sInstitutionalReviewBoard(IRB).
,Vol.1,No.1,Article.Publicationdate:August2024.AnAnalyticalFrameworkforUserVerbalandNonverbalBehaviors 9
Fig.3. Picturesofparticipantsactivelyengagedintheexperimentatdifferentstagesoftheirexperiencewith
dialoguehistory.Thestudytookplaceinthefullyoperationalkitcheninthesmarthomelaboratory.The
Alexadevicewasplacedontheleftsideoftheparticipants(circledinwhite).
4.2 ExperimentalSetupandProcedure
Theoriginalstudywasconductedinasmarthomelaboratorydesignedasaone-bedroomapartment,
featuringafullyequippedkitchenandmonitoringcameras.Figure3showsexamplesofparticipants
engagingintheexperimentaltaskatdifferentstagesandprovidesaviewofthekitchensetup.
TheAlexadevicewaspositionedontheleft-handsideoftheparticipants.Informedconsentfor
recordingwasobtainedfromallparticipantspriortotheexperiment.
Eachexperimentalsessionlastedlessthananhourandincludedthefollowingprocedure:1)
Aresearcherfirstbeganwithabriefintroductiontothestudy.2)Then,participantsreceiveda
five-minutetutorialonhowtointeractwithMM,whichaimedtofamiliarizetheparticipantswith
MM’sfunctionalitiesandinteractionforthecookingtask.Thetutorialincludeddemonstrationsof
threeutteranceexamples:“Whatisthefirststep?”,“WhatifIdon’thavechicken,whatshouldIdo?”,
and“WhatdidIjustask?”.Theresearcheralsoshowdemonstrationsofactivatingandstopping
MM.3)Next,theparticipantswatchedinstructionalYouTubetutorialsonmakingchickenmango
avocado salad. While memorization of the content was not mandatory, they were encouraged
tofamiliarizethemselveswiththerecipe.4)Finally,participantsproceededtopreparethesalad
while freely interacting with MM. The anticipated time for completing the cooking task was
approximately20minutes;however,participantsweregivenastrictmaximumtimelimitof30
minutes.Whenthistimelimitwasexceeded,thecookingactivitywouldbeconcluded.Duringthis
timeperiod,participantshadtofollowthestepsfromthevideowithoutbeingabletore-access
it.Instead,participantscouldrequestassistancefromtheVAforclarificationsonrecipestepsor
ingredientpreparation.
4.3 DataCollectionandAnalysis
Participants’cookingprocesswasvideo-recordedwiththeiroralconsent.Intotal,3hoursand
39 minutes of audiovisual data were captured, which were transcribed by using an automated
transcriptionservice,resultinginadetailedinteractionlogofparticipantengagementwithMM.
Researchersmanuallycheckedthetranscriptstoensureaccuracy.Thevideocodingprocessforthis
studyinvolvedthreeannotationcyclesandusedtheELANannotationsoftware[100],asshownin
,Vol.1,No.1,Article.Publicationdate:August2024.10 Chan,etal.
1st Round Annotation
Detail Description of Each
Annotation
2nd Round Annotation
3rd Round Annotation
Fig.4. ScreenshotofELANannotationsoftwareshowingthreeannotationcycles.Alllayersmentioned
Section4.3canbeseenhere.
Figure4.Researchersfirstseparatetheinteractionsintocategoriescorrespondingtoeachcycle’s
focus,thencreateannotationtiersforeachcategoryinELAN.Thefirstannotationcyclefocuses
onverbalinteractionsbetweenparticipantsandtheVA.Thesecondcycleconcentratesmainly
onparticipants’nonverbalbehaviors.Afteridentifyingbehavioralcharacteristicsinthefirsttwo
cycles,researcherscollaborativelyusethesecharacteristicsinthethirdcycletoanalyzebehaviors
usinggroundedtheory,applyingbothinductiveanddeductivecodingmethods[42,103].
Weemployedadeductiveapproachbyadaptingpreviousresearchoncommunicationbehaviors
inourcodingprocess.Specifically,weadaptedcharacteristicsofnonverbalbehaviorcategorized
fromArgyle[8]andFiske[43].Thesecharacteristicsguidedouranalysisofnonverbalbehaviors
shownbyparticipants.Additionally,weadaptedpriorresearchonverbalinteractionswithVAs,
particularlyfromVtyurinaandFourney[106],whocategorizedconversationalcuesbytracking
thefrequencyofspecificintents.WealsoincorporatedfindingsfromMavrinaetal.[71],detailed
common requests and responses during verbal communication breakdowns when interacting
withVAs.Thesepreviousworksallowedustohavepredefinedcategoriesonhowusersnavigate
interactionswithVAsandhowtheyrespondtoverbalandnonverbalcommunicationchallenges.
Thispredefinedcodebookformedtheinitialstructureofouranalysis.
Werecognizedthatexistingliteraturehaslimitationsincapturingthenuancesofuserbehavior
wheninteractingwithLLM-VAs,arelativelynewtechnologicaldomain.Whilepriorstudieshave
documentedhumanbehaviorwithVAs,thereremainsagapinunderstandinghowusersbehave
inthisnovelcontext.Toaddressthis,weemployedaninductivecodingapproachtoallownew
patternsandbehaviorsdirectlyfromthedata.Thismethodallowedustoidentifybehaviorsthatmay
nothavebeenobservedinpreviousresearchanddevelopthemesgroundedinobservationsfrom
theLLM-VAinteractions.Thiscombinationofinductiveanddeductiveapproachesensuredthat
ouranalysiswasgroundedinpriorknowledgeandallowedroomfordiscoveringotherbehaviors
uniquetointeractionswithLLM-VAs.
Thecodingprocessinvolvedcontinuouscollaborationbetweentworesearchers.Eachresearcher
initiallycodedindependentlyineachannotationroundandthenconvenedtodiscussandcompare
theircodes.Anyremainingdisagreementswereresolvedusingthenegotiatedagreementmethod
[18]withtheentireresearchteam.Researchersverifiedtheaccuracyofthetimelabelsforeach
interactiontoensurecorrectness.Forexample,thetranscriptionsofquestion-answerpairswere
accuratelylabeledwiththerespectivespeakers.
,Vol.1,No.1,Article.Publicationdate:August2024.AnAnalyticalFrameworkforUserVerbalandNonverbalBehaviors 11
5 BEHAVIORCHARACTERISTICS
Thefirstdimensionoftheframeworkistoidentifythecharacteristicsofusers’verbalandnonverbal
behaviors.Thiswasaccomplishedduringthefirsttwoannotationcycles.
Inthefirstannotationcycle,researchersfocusedonverbalinteractionsbetweenparticipants
and the VA. Researchers classified verbal behavior into 3 categories: ‘Voice Interaction’, ‘User
VoiceInteraction’,and‘VAResponds’.‘VoiceInteraction’referstotheentiretyoftheconversation,
encompassingallqueries.The‘UserVoiceInteraction’tierindicatesuserqueries,labelingbehaviors
such as simply asking a question, cutting off the VA’s response, or instructing the VA to stop.
Finally,‘VAResponds’referstotheVA’sresponses,whichwerefurtherclassifiedasvalidifthe
VAansweredcorrectlyorprovidedarelevantanswer.Theresponsewasannotatedasinvalidif
unrelatedtotheparticipant’squestion.
Thesecondroundoftheannotationcyclefocusedmainlyonparticipants’nonverbalbehav-
iors.Researchersclassifiedthesenonverbalbehaviorsintofourcategories:‘EyeContact’,‘Gestural
Behavior’, ‘Tone Change’, and ‘Spatial Behavior’. ‘Eye Contact’ refers to instances when partici-
pantsmakedirectorindirecteyecontact,signalingattentionorengagement.‘GesturalBehavior’
encompassesmovementssuchashandgesturesorheadnodsthatsupplementverbalcommunica-
tion.‘ToneChange’involvesvariationsinvocalpitch,loudness,orintonation,indicatingdifferent
emotionalstates.‘SpatialBehavior’pertainstoparticipants’physicalproximityandorientation,
includingactionslikeleaningcloserormovingaway.Thisannotationschemeisbasedonprevious
research[8,43],whichprovidesacategorizationofnonverbalhumanbehaviorobservedinconver-
sationalcontexts.Annotationsincludedthedurationandspecificdetailsofthebehaviors,suchas
recordingaparticipantleaningcloserunder‘SpatialBehavior’.Additionally,timeperiodswhen
theuserwascookingindependentlywithoutinteractingwiththeVAwereannotatedas‘Regular
CookingBehavior’.
Inthethirdannotationcycle,researchersdefinedthethreeinteractionstagesofuserinteraction
withtheVAduringthecookingprocessbasedonexistingliterature(e.g.,[71,106])andobserved
userbehavior:exploration,conflictandintegration.Thesestageswillbedescribedindetailin
thenextsection.
6 THETHREEINTERACTIONSTAGES
Basedonourobservation,wefoundthecharacteristicsofhumanverbalandnonverbalbehaviors
withintheinteractionflow,andcategorizedthemintothreedistinctinteractionstagesasthesecond
dimensionoftheframework:exploration,conflict,andintegration,asdetailedinTable2.Each
stagedefinesuniquebehaviorsthatusersshowwheninteractingwithLLM-VAs.
Specifically, ‘stage’ denotes distinct periods characterized by users’ engagement in specific
behaviorsratherthanstaticphases.Userinteractionwithinthesestagesisdynamicduetofrequently
transitioningacrossvariousstagesthroughouttheexecutionofthetask.Thisdynamicreflectsthe
shiftsthatoccurasusersnavigatethroughtheexperience.
6.1 ExplorationStage
6.1.1 Definition&Characteristics. Intheexplorationstage,usersbegintheirinitialfamiliarization
withthesystem,eitherbyactivatingitorexploringitscapabilities[106].
ForVerbalBehavior. UsersmayengageinconversationsuchasawakeningtheVAforthefirst
time,forgettingtousethewakeword(e.g.,“Alexa"),orforgettingtoinitiatethecorrectskill(e.g.,
“OpenMangoMango").Theymayalsoaskquestionsthatgobeyondtherecipeforthefirsttime,
indicatinganexplorationofthesystem’scapabilities.
,Vol.1,No.1,Article.Publicationdate:August2024.12 Chan,etal.
Stage Definition Theme Scenario
Exploration Usersinitially AwakeSystem -AwakenAlexaforthefirsttime.
interactwith -Failtocall"Alexa"ontheinitialattempt.
theVAsystem. -Forgettosay"OpenMangoMango".
ExploreSystemCapability[106] -Askingquestionsbeyondtherecipeforthe
firsttime.
Conflict TheVAsystem’s IncorrectResponse -Thesystemmisinterpretsuserquestions.
responseisun- -Thesystemincorrectlycomprehendsthe
clear,invalid, input,leadingtoincorrectresponses.
ordelayed. -Exitthesystem.
NoResponse -Thesystemfailstorespond.
ClarificationResponse -Thesystemrequestsclarificationdueto
misunderstoodinput.
ConversationConflict[86] -UserInterruptsVA.
-VAinterruptsuser.
WorkIndependently[106] -Murmuring.
-Proceedswithtaskswithoutwaitingfor
confirmation.
-Doesnotfollowtherecipe.
Integration TheVAsystem ReceiveValidResponse[93] -Askaquestionbeyondtherecipe.
providesvalid, -Receiveactionablesteps.
clear,andimme- -Askafollow-upquestion.
diateresponse.
Table2. Qualitativecodebookanddescriptionofcharacteristicsofeachstage.
ForNonverbalBehavior. Duringtheexplorationstage,eyecontactwiththeVAemergedasthe
mostfrequentnonverbalbehavioramongallparticipants(M=1.33,SD=0.65),demonstratingan
instinctualefforttoestablishavisualconnectionwiththeirnewconversationalpartner.Spatial
behaviorswerealmostabsent(M=0.25,SD=0.45)duringinitialinteractions,suchasleaning
in closer to the VA, indicating a neutral communication style adopted by users. There was no
significantgesturalandtonechangebehaviorwasobservedatthisstage,indicatingthatusersdid
notrelyongesturesandtonetocommunicatewiththeVAinitially.
6.1.2 Observations. Inourstudy,weobserveddiverseinteractionpatternsamongparticipantsas
theyexploredthewaystoinitializethesystem.AsmentionedinSection4.2,participantswere
introducedtoMMthroughabrieftutorialonpromptedquestions.Despitereceivingbrieftraining,
participantsstillstruggledwithawakeningthesystemtoinitiatecommunication.
Acommonissueobservedwasthefailuretobeginrequestswiththewakeword"Alexa"orthe
specificcommand"OpenMangoMango,"whichisacrucialstepforinitialsystemengagement.This
requirementmayaddanextracognitiveloadtotheparticipantsduringinitialuseasthisaction
deviatedfromtheparticipant’susualconversationalpatterns,creatingabarriertoseamlessinitial
interactionwiththesystem.Itisimportanttonotethattherewereinstanceswheresystemerrors
occurred,andparticipantsdidnotrealizethattheywerenolongerinteractingwithMM.However,
thisstagedoesnotaccountforcaseswhereuserswereunawareofAlexadisengagingandfailing
toreactivateit.
Inthisstage,wealsoobservedasignificantshiftinsomeparticipants’engagementwithMMas
theyexploreditsfunctionalitiesmoredeeply.TheparticipantswereinformedMMwaspoweredby
LLMatthebeginningoftheexperiment,whichsuggestedtoparticipantsthattheycouldinteract
,Vol.1,No.1,Article.Publicationdate:August2024.AnAnalyticalFrameworkforUserVerbalandNonverbalBehaviors 13
withMMwithnolimitationsregardingthescopeofinquiries.Encouragedbythisunderstanding,
participantsstartedtotestthesystem’sboundariesbyaskingquestionsunrelatedtothecooking
process,initiatingamoreexploratoryinteractionthaninitiallyanticipated.Throughtheinquiries,
participantsdiscoveredthatMMismorecapablethansimplyguidingthemthroughrecipesteps
andcapableofrespondingtoqueriesbeyondtherecipe’scontent,suchasprovidingbasiccooking
knowledge.ThisexpandedunderstandingofMM’scapabilitiesencouragedamoreinquisitiveand
experimentalapproachtointeractingwiththesystem,pavingthewayforaricheruserexperience.
6.2 ConflictStage
6.2.1 Definition&Characteristics. Intheconflictstage,usersencounterresponsesfromtheVA
thatareunclear,invalid,ornotprovidedpromptly[86,106].
ForVerbalBehavior. Duringtheconflictstage,participantsoftenencounterdifficultiesexecuting
theinstructionssuggestedbyMM.Thesechallengesaremainlyfromcommunicationbreakdowns
between the participants and MM. Participants often receive incorrect responses due to MM’s
failuretocomprehendtheuser’sinputaccurately,leadingtomisunderstandingsorinappropriate
replies.MMmayalsooccasionallyrequestfurtherinformationorclarificationfromuserstobetter
understandtheircommandsorqueries.Anotherissuearisesininstancesoftechnicalerrors,where
the VA may fail to recognize or respond to user inputs. Such situations can disrupt the user’s
experience,leadingtofrustrationandpotentialdisengagementfromthetask.
Furthermore,conversationalconflictsalsorepresentcomplexityintheinteractionbetweenusers
andtheVA.Weobservedtheseconflictsoccuronboththeusers’andVAs’sides.Suchinterruptions
aremainlyfrommisunderstandings,misinterpretationsofcommands,orthesystem’slimitations
inprocessingqueries.Ultimately,facedwiththesechallenges,someparticipantsmaybypassthe
system’sassistanceandproceedwiththetaskindependently.
For Nonverbal Behavior. All categories of nonverbal behavior are observed frequently in the
conflictstage.Eyecontactisparticularlycommon(M=7.92,SD=4.42),asparticipantsoften
lookattheVAtocheckitsstatuswhenitfailstodelivertheexpectedresponse.Spatialbehavioris
notable(M=3.00,SD=2.80),withusersmovingclosertothedevicetoensuretheircommandsare
heardduringcommunicationbreakdowns.Tonechangesarealsoprominent(M=1.92,SD=1.38);
usersmayraisetheirvolumetoexpressfrustrationortoregainMM’sattention.Conversely,adrop
intonecanindicatedisappointmentorimpatience.Gesturalbehaviors,suchastappingthedevice
orwaving(M=1.42,SD=1.88),areemployedtocommunicatewithorreactivateMM.
6.2.2 Observation. Inthisstage,afrequentlyobservedchallengeiswhenMMrespondsincorrectly
toparticipantqueries.Often,thesystemmisinterpretstheuser’squestions,whichcanbeattributed
toerrorsinspeech-to-textrecognition.Sucherrorsmaypreventthesystemfromcomprehending
the input accurately, leading to inappropriate responses. However, integrating LLMs into MM
improveshandlingthesechallenges.LLMsenrichMMwithabroaderknowledgebaseandadvanced
comprehensioncapabilities.Consequently,evenwhenmisinterpretationsoccur,MMcannavigate
backtotheconversationcontextbyanalyzingthequeryinlightoftheongoingtaskandidentifying
that the misinterpreted query may not be related to the current task. When MM cannot fully
comprehendaninput,itrequestsfurtherclarificationfromtheuser.Forexample,MM’sresponse
like “I am sorry, but I didn’t understand your question. Could you please rephrase it?”. This is
commonintheexperimentwhentheuserrequestisambiguousorthespeechrecognitionsystem
isuncertainabouttheuser’srequest.Furthermore,sometimesissuesarisewhenAlexaexitsMM.
Insuchscenarios,Alexaaddressesqueriesusingitsstandardfunctionalities,withoutincluding
MM’sspecializedcapabilitiesorspecificknowledgebase,suchasdetailedrecipeinformation.
,Vol.1,No.1,Article.Publicationdate:August2024.14 Chan,etal.
InteractionswithMMcanalsoleadtoconflictsbyinterruptionsinthedialogueflow.These
interruptions appear either when participants proactively cut off MM or when MM interrupts
theparticipants.WeobservedthatparticipantscommonlyinterruptMMwhentheresponsesare
notalignedwiththeirexpectationsorwhentheresponsesareexcessivelylengthy.Conversely,
MM may interrupt participants due to delays in processing responses performed by the LLM.
Perceiving these delays as a non-response, participants might begin to repeat their queries or
ask follow-up questions. However, MM might suddenly deliver a response to the initial query
while the participant is still speaking, leading to overlaps and interruptions. While the LLM’s
advancedcapabilitiesfacilitatericherinteractionsbyallowingfordetailedexplanationsandthe
accommodationoffollow-upinquiries,italsointroducescomplexitiesinmaintainingaseamless
andintuitiveconversationalflowduetotheprocessingtime,resultingindisruptionofthenatural
flowofconversation.
Finally,eventhoughparticipantsconsultedMMforrecommendedactionsguidedbyMM’sLLM
duringspecificstages,somestillchosetoexecutecertaintasksontheirown.Thisindependent
behavior was evident in actions such as murmuring to themselves. For instance, participants
might quietly repeat the next ingredient, its quantity, or the forthcoming step. This helps the
participant’sreluctancetoaskMMthesamequestionsrepeatedly.Participantsengagedinself-
dialogue,reiteratinginformationprovidedbyMM,andconfirmingtheresponsesinternally.This
alsohelpssolidifytheirmemoryofMM’sinstructions.Moreover,someparticipantsproceeded
withoutwaitingforthesystem’sconfirmationordivergingfromtheprovidedrecipe.Anexample
ofthisiswhenaparticipantasksMMaboutastepintherecipe,eitheraftertheyhavealready
completeditorwhiletheyareintheprocessofdoingso.Suchanindependentapproachoften
indicateseitheraleveloffrustrationwiththesystemorapreferenceforself-guidance,effectively
sidesteppingtheassistanceofferedbyMM.However,it’simportanttonotethattakingindependent
stepswithoutconsultingMMisnotconsideredinthisobservation.
6.3 IntegrationStage
TheintegrationstageischaracterizedbytheVAsystemprovidingvalid,clear,andimmediate
responses[93].
ForVerbalBehavior. Intheintegrationstage,userscanconfidentlyutilizetheVAtoassistwith
theircooking,askingquestionsthatbeyondthebasicrecipeinstructionsandexecutingactions
advisedbytheVA,oftenincludingfollow-upinquiries.TheVA’sresponses,characterizedbytheir
validity,clarity,andimmediacy,areindicativeofsuccessfulintegration.Theinteractionbetween
theuserandVAismarkedbyfluentconversations,withperfectlymatchedquestion-answerpairs.
ThisstagesignifiesashiftasparticipantsadeptlyemploytheVA,notjustforrecipeguidancebut
asacomprehensivecookingaid,extendingtheirengagementtomorecomplexculinaryqueries.
ForNonverbalBehavior. Duringtheintegrationstage,eyecontactremainsthemostobserved
nonverbalbehavior(M=11.75,SD=4.43),indicatingastrongerandmorefamiliarconnection
with the VA. This sustained visual engagement suggests that users rely on visual feedback or
confirmationastheyinteractwiththeVA.Spatialbehaviorislessfrequent(M=2.00,SD=2.13),
implyingthatusershavefoundanoptimalphysicaldistanceforinteractionthattheyconsistently
adhereto,reflectingastabilizedinteractionpattern.Gesturalbehaviorisalsoreduced(M=1.17,SD
=1.90),suggestingadecreaseinthenecessityforgesturesastheinteractionwiththeVAbecomes
more fluid and intuitive. Tone changes are rare in this stage (M = 0.58, SD = 0.67), signaling a
comfortableandpredictablerapportwiththeVA,andeliminatingtheneedforuserstomodulate
theirtonetocommunicateeffectivelyorpromptaspecificresponse.
,Vol.1,No.1,Article.Publicationdate:August2024.AnAnalyticalFrameworkforUserVerbalandNonverbalBehaviors 15
6.3.1 Observation. Whentheconversationprogressedsmoothly,weobservedparticipantsbecom-
ingmoreconfidentinaskingquestionsbeyondtherecipe’sscope.Forinstance,P7askedabout
measuring1/15ofateaspoon.MMcouldadeptlyprovideanswerswithoutdependingontherecipe
information,evenforqueriesunrelatedtotherecipe.Participantslearnedthattheycouldexplore
topicsoutsidetherealmofrecipe-specificquestionsandstillobtainvaluable,practicalresponses,
whichsignificantlyenhancedtheinteractivequalityoftheconversation.
Anoteworthyaspectoftheinteractionwastheengagementoftheparticipantsinfollow-up
questions.Thesequestionswerenaturallyconnectedtoparticipants’previousinquiries,creatinga
coherentdialogue.Thisinteractiontranscendedabasicquestion-answerformat,evolvingintoan
enhancedinteractiveprocess.Previousstudieshighlightedthattheiterativecycleofquestioning
andclarificationplayedapivotalroleindecreasinguncertainties[47,99].Thisprocessclarifiedthe
ambiguitiesintheconversation,leadingtoamoreaccurateandeffectiveexchangeofinformation.
Overall,MMconsistentlyprovidedclearandpracticalinstructionstoparticipants,andmaintained
asmoothandinformativeinteraction,enhancingthefluentconversationexperienceatthisstage.
6.4 QuantitativeOverviewofBehaviorsintheThreeInteractionStages
TotalQuestion QuestionAnswered IncorrectAnswer %of Total
Participant
Asked CorrectlybyMM FromMM Correct TaskTime
P2 29 23 6 79.30% 17:35
P3 32 25 7 78.10% 15:09
P4 27 22 5 81.50% 17:01
P5 35 28 7 80.00% 15:05
P6 48 28 20 58.30% 15:49
P7 30 20 10 66.70% 13:19
P8 36 28 8 77.80% 18:06
P9 40 13 27 32.50% 22:51
P10 75 59 16 78.70% 23:34
P11 19 17 2 89.50% 19:39
P12 47 27 20 57.40% 26:10
P13 34 10 24 29.40% 19:58
Table3. Thetableprovidesanoverviewoftheuserperformanceonthesalad-makingtask.
Following the characterization of the stages, we will analyze the communication dynamics
amongparticipants,focusingonboththeirverbalandnonverbalbehaviorsthroughoutthethree
stages.
6.4.1 VerbalBehavior. Ofthetotal447queriesinourstudy,66.4%ofvalidqueriesreceivedvalid
andaccurateresponsesfromMM.Aquerywasconsideredvalidonlyifitconsistedofacomplete
questionanditscorrespondinganswer;querieslackingaresponsewereexcludedfromconsideration.
Aresponsewasconsideredvalidwhenitcontainedaccuratecontentverifiablebytherecipeand
wasexpressedfluently.MMsystemerrorsandspeech-to-textinaccuraciescompromised23.0%
ofqueries.Furthermore,10.6%ofquerieswereinvalidduetoerrorswheretheLLMwouldoffer
incorrectinformationsequencingorprovideanswersunrelatedtothequeries.Table3presentsthe
accuracyrateofthequeriesforeachparticipant.
InFigure5a,wepresentthedistributionofinteractiontimeacrossparticipantsasanormalized
measuretofacilitatecomparativeanalysis.Giventheintrinsicvariabilityinabsoluteinteraction
times, we employed a percentage-based normalization method. This normalization technique
,Vol.1,No.1,Article.Publicationdate:August2024.16 Chan,etal.
(a) (b)
Fig.5. (a)Participant’stimeallocationofinteractionacrossdifferentstageswithvoiceassistant.Eachcolor
representsadifferentstage,withthelengthcorrespondingtothedurationoftimespentinthatparticular
stageofinteraction.(b)Totaltimeallocationofeachstageamongallparticipants.Eachbarrepresentsthe
rangebetweentheminimumandmaximumoccurrencesrecorded.Theblackdotindicatesthemeanfrequency
ofthesebehaviors,whiletheverticallineextendingfromeachbardenotesthestandarddeviation.
Fig. 6. A summary of the frequency of nonverbal behaviors across different stages among participants.
Thebehaviorsobservedwithinthestudyincludedeyecontact,gesturalbehavior,tonechange,andspatial
behaviors.Eachbarrepresentstherangebetweentheminimumandmaximumoccurrencesrecorded.The
blackdotindicatesthemeanfrequencyofthesebehaviors,whiletheverticallineextendingfromeachbar
denotesthestandarddeviation.
,Vol.1,No.1,Article.Publicationdate:August2024.AnAnalyticalFrameworkforUserVerbalandNonverbalBehaviors 17
Fig.7. Aheatmapthatvisualizesthedistributionofnonverbalbehaviorsacrossdifferentinteractionstages
foreachparticipant.Darkercolorsindicateahigherfrequencyofnonverbalbehaviors,andeachsquare
containsanumberrepresentingthetotalcountofthesebehaviorsineachspecificcase.
ensuresthatthedataarerepresentedonauniformscale,rangingfrom0%to100%.Inexaminingthe
timedistributionacrossotherstages,thedata(Figure5b)showsaminimumengagementtimeof
1.31%inexplorationstage,withamaximumof7.89%.Fortheconflictstage,theinteractiontime
variedrangingfromaminimumof19.35%totoamaximumof58.51%.Fortheintegrationstage,
theinteractiontimevariedrangingfromaminimumof39.64%toamaximumof76.77%.These
resultsdemonstratethatthevariedwaysinwhichusersnavigatethroughthestagesofinteraction
withthesysteminfluencetheeffectivenessofcommunication.
6.4.2 NonverbalBehavior. Inourexperiment,nonverbalcommunicationwasalsoanimportant
partofuserinteractions.Figure6showsthatthemostfrequentlyobservednonverbalbehaviorwas
eyecontact(M=21.00,SD=7.24).Thisformofcommunicationwasparticularlycommonduring
theintegrationstagewhenusersconsistentlylookedtowardsthedevice,especiallyinthecasesof
P4,5,8,12,asshowninFigure7.Suchbehaviorwasoftennotedwhenusersaskedquestionstothe
VA,likelystemmingfromaninstinctivetendencyto“face”theirconversationalpartner.
Additionally, spatial behavior was another significant nonverbal element, marked by users
alteringtheirbodypositionrelativetothedeviceduringinteractions(M=5.25,SD=4.03).This
shiftwaspredominantlyobservedintheconflictstage.Ininstancesofconflict,userscommonly
lean or move closer to the Alexa device to facilitate more precise communication. Specifically,
P9,whospentaprolongedamountoftimeintheconflictstageandexperiencedlowerresponse
,Vol.1,No.1,Article.Publicationdate:August2024.18 Chan,etal.
accuracy,demonstratedthemostpronouncedspatialadjustmentsleaningclosertotheAlexadevice
duringthisphaseamongotherparticipants.
Furthermore,wealsoobservedtheuseofgesturalbehavioramongtheusers(M=2.58,SD=
3.29).Usersutilizedgesturestoreinforcetheirverbalinteractionortoconveyspecificintentions,
especially during the conflict stage. It appeared that users subconsciously incorporated hand
gesturesasatooltoassisttheircommunicationwhenverbalclaritywaschallenged.
Lastly,tonechangealsoemergedasanelementofnonverbalinteractionamongusers(M=2.50,
SD=1.68).Duringtheconflictstage,9outof12participantsraisedtheirvolumeasanexpression
offrustrationorinanefforttocapturethevoiceassistant’sattentionwhenitisunresponsiveness
(showninFigure7).Conversely,userssoftenedtheirtoneinresponsetoreceivingusefulinformation
inintegrationstage,sometimescouplingagentlertonewithexpressionsofgratitudesuchas
saying“thankyou”tothesystem.
7 STAGETRANSITION
Fig. 8. The scatter plot timelines show a visual overview of the stages experienced by each participant
throughoutthetask,laidoutinatimelineformat.Theverticalaxisliststheparticipants,whilethehorizontal
axisshowsthetasktimeinminutes.Whileallparticipantsbeginatazeropoint,theiractivitiesconcludeat
varyingtimes.‘X’marksindicatetaskcompletiontime.Thedistancebetweenazeropointandan‘X’foreach
participantreflectsthedurationoftaskengagement.
Inthissection,weutilizetheannotateddatatoexplorethepatternsofstagetransitionwithin
ourframework.Thisincludesexaminingtheascendingfromconflictstagetointegrationstage
andregressingfromintegrationtoconflictstage.Thesetransitionsrepresentthethirddimension
ofouranalyticalframework.
Theresultsfromthethreeinteractionstagesidentifiedintheprevioussectionshowthetransition
ofuserbehaviortowardtheintegrationstage.Thisstageisconsideredidealforparticipantsto
achieveduringtheirinteractionswithMM,astheinteractionisseamlessandclearinthisstage.
However,ourfindingsindicatethatnotallparticipantsallocatesignificantamountsoftimetothe
integrationstage.
Despitethisvariability,weobservedatrendinparticipantinteractiontimesintheintegration
stage(showninFigure5b).Onaverage,participantsspentthemosttimeintheintegrationstage
(M=63.0%,SD=10.88).WeespeciallyobservedthatP3,4,5dedicatedasubstantialportionoftheir
timetotheintegrationstageofthetask,respectively,74.36%,75.09%,and76.77%.However,P9
andP13,whoencounteredsignificantdifficultieswhileusingMM,spentmorethanhalfoftheir
,Vol.1,No.1,Article.Publicationdate:August2024.AnAnalyticalFrameworkforUserVerbalandNonverbalBehaviors 19
totalinteractiontimeintheconflictstage,50.97%and58.51%correspondingly,andshowedthe
leastaccurateresponsesinthecommunication.
Observingthistrend,weaimtofurtheranalyzebehaviorasthethirddimensioninthisframework,
focusing on the transition of the stages. In this section, we will examine two key aspects: the
progressionfromtheConflictStagetotheIntegrationStageandtheregressionfromtheIntegration
StagebacktotheConflictStage.Withintheprogressionaspect,wewillalsoexplorethestrategies
participantsemployedtoevolvetowardtheIntegrationStage.Figure8illustratesanoverviewof
eachparticipant’sprogressionthroughvariousstageslabeledaccordingtothecodebookestablished
intheprevioussection.Byexaminingparticipants’behaviorsduringthesetransitions,weaimto
uncoverthemethodsthatfacilitatetheirsuccessfulengagementwithLLM-VAs.
7.1 AscendingFromConflictStagetoIntegrationStage
Fig.9. Thescatterplotshowcasesinstanceswhereparticipantshaveshownascendingtransitionofthestage.
Eachhighlightedsegmentonthetimelinesignifiesatransitionalmomentintheparticipant’sjourney,with
thesequenceofthesesegmentsdenotinganascendingshiftfromtheconflictstagetotheintegrationstage.
Inourstudy,participantsnavigatedthroughthestagesdynamicallyduringtheexperimental
task,includingevolvingbetweentheconflictandintegrationstages.Themostfrequentstage
transitionsobservedamongparticipantsinvolvedexperiencingmultipleconflictstagespriorto
advancingtotheintegrationstage.Weobservedthistransitionwithparticipantswhoencountered
multipleconflictswhilecommunicatingwithMM,leadingtothecontinuedpresenceoftheconflict
stage,examplesshowninFigure9.Thistransitionoftenoccurredasusersfacedmultipleconflicts
duringinteractionswithMM,resultingincontinuedconflict.Typically,participantsfirstidentified
the reasons that cause the conflict, which may be caused by the LLM’s extensive knowledge
baseleadingtotheinterpretationofthequestionrelatedtoadifferenttopic,queriesthatexceed
MM’scapabilities,orAlexasystemissuesunrelatedtotheMMsystemitself.Duringthisprocess,
participants tried to extract useful information from their interactions and discover potential
solutions.Forinstance,ittookP10sevenroundsofdialogueandP12fiveroundsofdialogueto
realizetheyhadexitedtheMM.Inthisprocess,participantsengagedinacyclewheretheycontinued
refiningtheirstrategiesbasedoninsightsfrompreviousexperience,whichwillbediscussedlater
inthissection.Thisprocessallowedthemtoadjusttheircommunicationtechniqueseffectively,
facilitatingasmoothtransitiontotheintegrationstage.
Spatialbehavioremergedasanonverbalbehaviorinthistransitionofuserinteraction.This
changeinposturewasprimarilyevidentastheparticipantsnavigatedthechangeintheirexperience.
,Vol.1,No.1,Article.Publicationdate:August2024.20 Chan,etal.
Forexample,tofigureoutthecauseoftheconflict,participantswouldleanormoveclosertoward
theAlexadevice.ThispropensitytoleanforwardwasnotedinP5,P6,andP9asareactiontothe
absenceoffeedbackfromMM.Thus,spatialbehavior,especiallyintheformofleaningcloserto
thedeviceduringconflictstages,indicatesthechangeininteraction.
Overall,participantsareactivelyseekingtomovebeyondconflictstage,aimingforanadvance-
menttointegrationstage.Weobservedthatparticipantsemployeddifferentstrategiestofacilitate
thestagetransitionthroughoutthetask.
7.1.1 Re-frame the Question. We observe that participants often modified their queries when
interactingwithVAs,particularlywhentheyreceivedresponsesfromthesystemthatindicated
amisunderstandingorthatwereincorrect.Participantsmadeatotalof63attemptsusingthese
strategies of re-framing the question in different ways. Although only 46.03% of the time was
successfulingettingvalidresponses,participantsmustexperimentwithdifferentwaysofreframing
questionstolearntheappropriatestrategiesovertime.Inthesecases,theparticipantstypically
adaptedbyrephrasingorreformulatingtheirqueries.Thisbehaviorwasanattemptbyparticipants
tomaketheirrequestsmoreunderstandabletothesystem,therebyincreasingthelikelihoodof
reachingtheintegrationstage.Throughaprocessoftrialanderror,participantsgraduallylearned
theirownmosteffectivecommunicationmethodswithMM.Thesemodificationsmightinclude
countingthesteps,describingthecurrentsituationinthequeries,andeveninterruptingthesystem
toposearefinedquestion.
Participants sometimes enhanced their interactions with MM by including step numbers in
theirqueries,leadingtomorestructuredcommunicationandpotentiallymoreaccurateresponses
from MM. This approach was demonstrated by P4 , who explicitly used step numbers in their
questionstoensuretheyreceivedthespecificinstructiontheysought(showninFigure10).For
instance,P4beganbygenerallyaskingaboutthenextstep.However,whenanerroroccurred,
theparticipantfounditbeneficialtodirectlyrequestthenextstepbyspecificallyreferencingthe
“fifthstep".Thisrelianceonstepnumbersprovedtobeapracticalmethodforobtainingprecise
responses,particularlyinsituationswheremaintainingthesequenceofcookingstepswascrucial.
01 P4: ((Counting the number of Ingredients placed with the knife.))

02 P4: Alexa, what is the fifth step?

03 MM: The fifth step is to place slices of cucumber into the salad bowl along with the other ingredients.
Fig.10. P4usedaknifetocountthestepnumbersandreferencedthestepnumbersinthequestions.
Participants also enhance their interactions with MM by supplementing their queries with
contextordescriptionsoftheircurrentcircumstances,aimingformorepreciseandsituationally
relevantresponses.Thisapproachisparticularlycommonwhentheinformationneededisnot
readilyavailableinaprovidedresource,suchasarecipe.Forexample,participantsmaylackbasic
cookingknowledge.Inthesecases,theydescribetheirbackgroundandthecontextoftheirsituation
,Vol.1,No.1,Article.Publicationdate:August2024.AnAnalyticalFrameworkforUserVerbalandNonverbalBehaviors 21
beforeaskingtheirquestion.Byemployingthisstrategy,participantsaremorelikelytoreceive
effectiveresponsesthataretailoredtotheirspecificcircumstances.
We also observed the use of gestural behavior among the participants when explaining the
currentsituationtoMM.Participantsutilizedgesturestoreinforcetheirverbalinteractionorto
conveyspecificintentions,especiallywhenparticipantsaretryingtoevolvebeyondconflictstage.
Figure11showsanexampleofP2employinghandgestureswhileinteractingwithMM.
01 P2: Alexa, I want to (0.4) I want to make the lettuce dry without some water but I don't
have a spinner, how can I do it?

(( moving hands ))

02 MM: After rinsing the lettuce, gently shake off any excess water over the sink. Two, place

the lettuce leaves on a clean kitchen towel or paper towel...
Fig.11. P2employinghandgesturesduringinteractionwithMMwhileengagedinthetask.
Whenparticipantshadalreadyreceivedpartoftheinformationtheywereseeking,butMM
continueditsresponse,theywouldofteninterruptMMmid-sentenceassoonastheyheardthe
answertheywerelookingfor.Theyutilizedthewakewordtopausethesystemeffectivelyand
promptlyposedanewfollow-upquestionthatmorecloselyalignedwiththespecificanswerthey
weresearchingfor.Thisstrategyofinterruptionservedtominimizethewaitforthesystemto
completeitsresponse,thusfacilitatingquickeraccesstotherequiredinformation.Especiallyin
situationswhereparticipantshadspecificqueries,thismethodeffectivelydirectedtheconversation
towardamorerelevantandexpedientresolution.
01 P12: Alexa, what's the last step?

02 (6.1) ((Continue working on the salad task.))

03 P12: Alexa, what's the last step?

04 (11.5) ((Continue working on the salad task.))

05 P12: Alexa, what are we making?

((Continue working on the salad task.))

06 MM: Hmm. I don't know that one.

07 P12: Alexa, stop. (2.4) Alexa, open mango mango.

((Continue working on the salad task.))
Fig.12. P12conversationfragment.
7.1.2 ReactivatedMM. Participantswhoencounteredirregularactivitiesorconversationsunrelated
totheircurrenttaskoftenreactivatedthedeviceusingvariousinputmethods.Theseincludeda
combinationofvoicecommands,touchinputs,orvisualcues,whichenhancedtheinteraction
andpromotedclearcommunication.Duringsuchinstances,participantsdiscoveredthatrestarting
thesystemwasbeneficial.Thisreboottypicallyhelpedalignthesystem’sresponseswiththeir
,Vol.1,No.1,Article.Publicationdate:August2024.22 Chan,etal.
expectations and led to more accurate and relevant information related to the working task.
ParticipantsperceivedthebluestatuslightsaroundAlexaasindicatorsofsystemactivity.However,
theselightsdidnotalwaysprovidesufficientinformation.Forexample,thebluestatuslightwas
sometimesnotinformativewhenAlexawasnotoperatingwithintheMMsystem.Inthesecases,
participants relied on conversational interaction to discern Alexa’s current state. For instance,
Figure12showsthatP12learnedthetechniqueofreactivatingMMovertime,makingiteasierto
recognizesignsindicatingthatasystemrestartmightbenecessarytoobtainaccurateanswers.As
thetaskprogressed,P12increasinglyemployedthistechnique.
7.1.3 RepeattheQuestion. Inadditiontoreframingqueriestoobtainvalidresponses,wenoticed
apatternwhereparticipantsoftenrepeatedtheirquestions,particularlywhentheyreceivedno
feedbackfromMM.Participantsareactivelyexploringdifferentpossibilitiestounderstandthe
lackofresponse.Severalfactorsmightcontributetothissituation;forexample,MMmightnot
haveheardthequestionduetosystemorenvironmentalissuesorfailedtorecognizethewake
word.Insuchinstances,humansfrequentlyrepeattheirquestionstoascertainwhetherthelistener
comprehends them and can proceed with the task at hand. This strategy reflects an adaptive
approach to communication. By adjusting their interactions in real-time, participants aim to
overcomethesechallenges,ensuringthatthetransitiontotheintegrationstageofthedialogueis
asseamlessaspossible.
7.1.4 CallingtheWakeWord-“Alexa”. Anothercommonstrategyamongparticipantswasquickly
re-engagedwithMMafterforgettingtousethewakewordintheirinitialinteraction.Thisbehavior
was identified in 7 out of 12 participants. Notably, the participants effectively rectified 84.2%
of the attempts. We observe that participants typically recognized and rectified this oversight
almostinstantaneously,whereparticipantsnoticedAlexa’snon-responseinmerelyasecondand
subsequently corrected their approach. This is because participants engage with Alexa using
patternsakintohuman-humanconversation,whereinthenecessitytoutterawakewordbefore
everycommandisoftenoverlooked.
7.2 RegressingFromIntegrationStagetoConflictStage
Fig.13. Thescatterplotshowcasesinstanceswhereparticipantshaveshownregressingtransitioninstage.
Eachhighlightedsegmentonthetimelinesignifiesatransitionalmomentintheparticipant’sjourney,with
thesequenceofthesesegmentsdenotingaregressingshiftfromtheintegrationstagetotheconflictstage.
Wealsoobservedthatafewparticipantsunexpectedlytransitionedoutoftheintegrationstage
andintotheconflictstageafteraperiodofsuccessfulengagement,examplesshowninFigure13.
,Vol.1,No.1,Article.Publicationdate:August2024.AnAnalyticalFrameworkforUserVerbalandNonverbalBehaviors 23
Theseshiftscommonlyresultedfromone-timeevents,suchasunexpectederrorsfromLLMdueto
itsextensiveknowledgebase.Figure14showsthatP3experiencedunexpectederrorswhenMM
misinterpretedthequestion,leadingtoaresponsethatdidnotalignwiththeexpectations,causing
atransitionfromtheintegrationstagetotheconflictstage.
01 P3: Alexa, am I done?

02 MM: No, you are not done. Everyone has different
knowledge and experiences. Asking questions
is a great way to learn and improve your
understanding. Keep asking questions and never

hesitate to seek knowledge.
Fig.14. P3’sconversationfragmentonexperiencingunexpectederrors.
Inadditionalinstances,P6andP12facedregressionswhentheLLMincorrectlyorderedthe
stepsnecessaryfortaskcompletion,leadingtotheparticipantsunexpectedlyreceivingincorrect
responses.Moreover,P11encounteredasituationwheretheMMpromptedwith"Wouldyouliketo
knowwhatthenextstepsare?"Towhichtheparticipantaffirmativelyrespondedwith"Yes,please,"
notrealizingthenecessityofinvokingawakewordtoinitiatefurthercommunication.
Despiteencounteringconflictsthatledthemunexpectedlyoutoftheintegrationstage,par-
ticipants can still effortlessly maintain their dialogue with MM. The ability of LLM to handle
multi-turninteractionsandmaintainarecordoftheconversationhistoryenabledtheconversation
tobecarriedoneffectively.Thisabilitytoquicklyresolvedisruptionsmaintainstheinteraction
flowandensuresthatparticipantscansmoothlyreturntotheintegrationstage.Consequently,
MMdemonstrateddynamicadaptability,effectivelymanagingminorconflictsthatparticipants
experiencedandmaintainingengagementwithparticipants.
8 SUMMARYOFFINDINGS
Tosummarize,weanalyzedparticipantinteractionswiththeLLM-VAinacookingscenariousing
ouranalyticframework’sthreedimensions:BehaviorCharacteristics,ThreeInteractionStages,
andStageTransitions.
Thefirstdimension,BehaviorCharacteristics,focusesonverbalandnonverbalbehaviors.Verbal
behaviors,suchasuserqueriesandinteractionswiththeVAandVAresponses,shiftedaspartic-
ipantsprogressedthroughthestages.Nonverbalbehaviorswereanalyzedthrougheyecontact,
spatialmovements,gestures,andtonechanges.
Theseconddimension,ThreeInteractionStages,capturedhowparticipantsinteractedwiththe
VAacrosstheexploration,conflict,andintegrationstages.Intheexplorationstage,participants
werestillfamiliarizingthemselveswiththesystem,oftenencounteringdifficultiesactivatingtheVA
andaskingquestionsbeyondtherecipetounderstanditscapabilities.Eyecontactwasthedominant
nonverbalbehaviorasparticipantsengagedwiththeVA.Intheconflictstage,communication
breakdownsoccurred,includingincorrectresponses,misunderstandings,andincompleterecogni-
tionofuserinput.Thisstagehadtherichestmixofverbalandnonverbalbehaviors,asparticipants
expressedfrustration,interruptedtheVA,andproceededwithoutconfirmation.Nonverbalcues
liketonechanges,spatialadjustments(e.g.,movingclosertotheVA),andgestures(e.g.,tappingor
waving)werefrequentlyusedasparticipantstriedtoresolveissuesandmovethetaskforward.In
theintegrationstage,participantinteractionsbecamemorefocusedandefficient.Verbalbehaviors
includedsophisticatedqueriesandfollow-upquestions,whilenonverbalbehaviors,suchaseye
contact,wereusedtoconfirmunderstanding.Thisstagerepresentedthemostseamlessinteraction,
asparticipantshadadaptedtothesystem.
,Vol.1,No.1,Article.Publicationdate:August2024.24 Chan,etal.
Thethirddimension,StageTransitions,demonstratedhowparticipantsadjustedtheircommu-
nicationstrategieswhenmovingbetweenstages.Whiletheintegrationstagewasthemostideal
andwhereparticipantsspentthemosttime,thiswasnottrueforeveryone.Someparticipants
remained in the conflict stage for extended periods. Transitioning from conflict to integration
typicallyrequiredparticipantstorefinetheircommunication,suchasrephrasingqueries,providing
additionalcontext,andadjustingspatialbehaviorstoresolveissues.Thesestrategieshelpedthem
progresstothemoreefficientintegrationstage.However,transitionswerenotalwayssmooth.
Someparticipantsrevertedbacktotheconflictstageduetounexpectederrorsorlapses,suchas
forgettingtousethewakewordduringextendedperiodsofsmoothinteraction.
9 DISCUSSION
Inthissection,wesummarizethekeycontributionsofourstudy,highlightdesignimplications
fromtheresearch,anddiscusspotentialfuturedirections.Finally,weaddressthelimitationsofour
researchandfuturework.
9.1 DesignImplications
Ouranalyticalframework,focusingonbehaviorcharacteristics,threeinteractionstages,andstage
transition,providesafoundationalbasisforfutureresearchandpracticalapplicationsofLLM-VA.
Byleveragingbothverbalandnonverbalbehaviors,itoffersvaluableinsightsintouserengagement
withLLM-VAsystemsduringtasks.Theseinsightshelpdevelopersandresearcherscreatemore
intuitiveanduser-centrictechnologies,emphasizingtheimportanceofintegratingthesebehaviors
intothedesignandassessmentofinteractivesystems.Basedonthisknowledge,wepresentdesign
implicationstoguidethedevelopmentofmoreeffectivehuman-VAinteractions.
9.1.1 UnderstandingUser:Behavior&EmotionPrediction. Intheanalysisofthethreeinteraction
dimensions,theobservationrevealsthatuseremotionsfluctuatethroughoutthetaskflow(e.g.,
experiencingconfusionwhentheLLM-VAdoesnotrespond).Previousresearchhasincorporated
nonverbalsignalsintomodelsforcomputingemotions,identifyingspecificemotionalstateslike
happinessoranxiety,andprovidingcorrespondingemotionalsupport[76].Ourframeworkbuilds
uponthisbyintegratingnonverbalbehaviors,enablingLLM-VAstoachieveamorecomprehensive
understandingofhumanbehaviorandestablishuserbehavioralpatterns.
Additionally,long-termobservationofrepeatableuserbehaviorpatternsisadvantageousfor
understandingtheuniquebehavioralcharacteristicsofdifferentindividualsanddevelopingperson-
alizeduserbehaviorpredictions.Recognizingspecificnonverbalsignalsatvariousstagescanhelp
LLM-VAsadoptmoretargetedstrategies.Thesestrategiesmightincludeguidingconversations
moreproactivelyorinacalmingmanner,basedontheuser’semotionalstateandbehavior.Through
thisapproach,LLM-VAscanrecognizeimmediateemotionalstatesandusethedimensionsinthe
frameworktopredictpotentialshiftsinuseremotionsbasedontheprogressionoftheirinteractions,
makingadjustmentstailoredtothecurrentsituation.
9.1.2 RespondingMore:Real-timeEmotionalFeedbackfromVA. VAsthatanalyzeusers’nonverbal
behaviorandapplyourestablishedbehavioralstagescanenhancethequalityofinteractionby
providing emotionally attuned responses. Previous research has looked into the use of single
multimodalinputandoutputsystemsinVAdesign,likegazeandgesture[4,49,73].However,
moreinputcouldbeusedwithinthisprocess.Forexample,ifaLLM-VAdetectssignsofanxietyor
frustrationduringtheconflictstage,itcouldadoptareassuringtone,showingconcernforthe
user’semotionsandacommitmenttoassistingwiththeproblem.Conversely,inresponsetoa
user’sexcitementorjoy,theLLM-VAcouldmirrorthispositivitywithanenergeticandcheerful
tone.Accuratelyinterpretingdifferentstagesofbehavioralanalysisandusers’nonverbalbehavior
,Vol.1,No.1,Article.Publicationdate:August2024.AnAnalyticalFrameworkforUserVerbalandNonverbalBehaviors 25
allowsVAstoprovidepreciseandempatheticemotionalresponses.Theseapproachesalignwiththe
expectationthathumanshaveforreceivingsocialresponsesfromcomputers[91].Priorresearch
has[36,87]highlightedusers’preferenceforcontinuedinteractionwithcomputeragentsaswell,
whichdemonstratedempathy,sympathy,andactivelistening,especiallyinmomentsoffrustration.
9.1.3 BridgingMinds:TheoryofMindImplicationsforLLM-VADesign. Ourfindingshaveimportant
implicationsforLLM-VAdesignwhenviewedthroughthelensofthemutualtheoryofmind(ToM)
[108].Theobservedinteractionsacrosstheexploration,conflict,andintegrationstagesreveal
how users develop and refine their understanding of the VA’s "mind" and capabilities. In the
explorationstage,usersbuildtheirinitialToMoftheVA,asevidencedbyparticipantsasking
questionsbeyondtherecipescopetogaugetheVA’sknowledgeboundaries.Thissuggeststhat
VAsshouldprovideclearfeedbackabouttheircapabilitiesearlyon.Theconflictstage,marked
bycommunicationbreakdownsandrichnonverbalbehaviors,representsmomentswhenusers’
ToMischallenged.Forinstance,inFigure15,aparticipantisshownusingahandstopgesture
whileexpressingfrustrationwiththeVA’soverlydetailedresponse.Thisnonverbalcue,coupled
withtheparticipant’sverbalrequesttosimplifytheinstruction("Onlytellmethestepafteradding
the vinegar"), exemplifies how users attempt to manage and recalibrate their mental model of
theVAduringthesechallenginginteractions.SuchbehaviorssuggestthatVAsmustrecognize
andrespondtothesecuestobettersupportusersandreducefrustration.Theintegrationstage
demonstratesamorematureToM,whereparticipantsusedsophisticatedqueriesandefficienteye
contacttoconfirmunderstanding.ThedynamicnatureofToM,evidentinparticipantsrevertingto
theconflictstageafterforgettingthewakeword,impliesthatVAsshouldmaintainbehavioral
consistencywhilehandlingoccasionallapses.
01 P2: °Okay° Alexa, tell me the rest of the steps after adding the vinegar.

02 (6.1)

03 MM: Once you have added the vinegar, the next step is to add half teaspoon of Dijon mustard
and half teaspoon of honey to the mason jar. After that proceed to add the minced
garlic, one quarter garlic clove or one quarter teaspoon minced garlic, followed by
[one quarter teaspoon of sea salt and one sixteenth]

04 P2: [Wow wow wow wow. (0.8) Alexa.]

(( Making hand stop gesture ))
05 (1.7)

06 P2: Only tell me the step after adding the vinegar.

Fig.15. P2holdinghandstopgesture.
Additionally,LLM-VAsshouldbedesignedtodeveloptheirowntheoryofmindaboutusers
(alsocalledthe"humanstate"[37])toprovidemoreeffectivesupport.Intheexplorationstage,
VAsshouldtrackthetypesandcomplexityofuserquestionstogaugetheirfamiliaritywiththe
systemand adjustexplanations accordingly. Duringthe conflict stage, VAs need torecognize
andinterpretusers’nonverbalcues,suchastonechangesorspatialadjustments,asindicators
of frustration or confusion. Building on prior research that has demonstrated the significant
,Vol.1,No.1,Article.Publicationdate:August2024.26 Chan,etal.
impactofcommunicationstyles(e.g.,formalorinformal)onuserperceptions[25,30],ourfindings
fromtheintegrationstagesuggestthatVAsshouldbedesignedtoadapttheircommunication
approachbasedontheneedsoftheusers.Specifically,VAsshouldlearnfromsuccessfulinteractions,
identifyingandadoptingthecommunicationstylesandqueryresponsepatternsthatprovemost
effectiveforindividualusers.Thisadaptivebehaviorcouldbeimplementedthroughreinforcement
learningtechniques,allowingtheVAtocontinuouslyrefineitsusermodel.Toaddressthedynamic
natureofinteractions,VAsshouldmaintainaflexible,real-timemodeloftheuser’scognitiveand
emotionalstate,potentiallyusingtechniquesfromaffectivecomputing.Thismodelshouldinform
theVA’sresponses,allowingittopreemptivelyofferhelpwhenitdetectspotentialconfusionorto
providemoreadvancedfeaturesasitrecognizesincreasinguserexpertise.Byincorporatingthese
designelements,LLM-VAscanbuildandmaintaintheirowntheoryofmindaboutusers,leading
tomorepersonalized,context-aware,andultimatelymoreeffectiveinteractions.
9.1.4 AdaptiveLLM-VAPersonas. OurfindingsextendpreviousresearchonadaptiveVApersonas
[35,36,52]bydemonstratinghowVAs’responsivenesstousers’verbalandnon-verbalsignals
across interaction stages can significantly enhance user experience and interaction efficiency.
Duringtheexplorationstage,whenusersposequestionsbeyondtherecipescope,VAsshould
recognizethisasacuetoadoptamoreinformativeandpatientpersona,perhapsswitchingtoa
’teacher’modewithdetailedexplanations.Intheconflictstage,whereusersexpressfrustration
throughtoneorwordchoice,VAscouldshifttoanempathetic,problem-solvingpersona,using
reassuringlanguageandofferingstep-by-stepguidance.Perhapsdependingonindividualusers,
humorcanbeusedtodiffusethetensionaswell[116].Non-verbalcues,particularlyevidentduring
conflicts,suchasusersmovingclosertothedeviceormakingexaggeratedgestures,shouldprompt
VAstobecomemoreattentiveandresponsive,potentiallyincreasingresponsespeedoradjusting
tonetomatchtheuser’surgency.Asuserstransitionbetweenstages,especiallyfromconflictto
integration,VAsshouldsmoothlyadjusttheirpersonatomatchtheirevolvingstate,adoptinga
morecollegialapproachasusersdemonstrateincreasedproficiency.Throughouttheseadaptations,
VAsshouldmaintainacorepersonaforcontinuity,balancingsubtleyetmeaningfulchangesto
addresschanginguserneeds.Overtime,VAsshouldlearnfromsuccessfulinteractionsanduser
preferencestodeveloppersonalizedadaptivestrategies,accountingforwhichpersonaadjustments
workedbestforspecificuserstatesandapplyingthemproactivelyinfutureinteractions.
9.2 Generalizability
CookingservesasafundamentalandpracticalcontextforexploringhumanandVAinteractions,
offeringinitialinsightsintointeractionswithLLM-VAs[81].Whileouranalyticalframeworkwas
initiallystudiedwithincookingscenarios,itsapplicabilityshowsabilitiestoextendbeyondthis
context.OurfindingscaninformsubsequenttestsandvalidationsofLLM-VAinteractionsinvarious
naturalcommunicationsettings,suchaseducation,healthcare,driving,andmore.Ouranalytical
frameworkcanguidethedesignofscenario-basedteststoevaluatemetricslikeuserexperience.
Byusingtheanalyticalframework,wecanidentifythedisadvantagesintheinteractionprocess
betweenhumanandLLM-VA.Theseinsightscanbestrategicallyleveragedtoenhancethedesignof
LLM-VAsinvarioustask-orientedsettings.Ourfindingsindicatethatastaskcomplexityincreases,
usersarelikelytoexperiencemorefrequentconflictstagesintheirinteractionswiththeLLM-VA,
likelyduetotheincreasingcomplexityoftheirvoicecommands.Therefore,collaborativescenarios
involvinghumansandLLM-VAs,suchasdriving,predictingtaskcomplexity,andprovidingusers
withpreemptiveguidanceonformulatingvoicecommandscouldleadtosmootherinteractions.
Forinstance,ourfindingsshowthatastaskcomplexityincreases,usersarelikelytoencounter
morefrequentdifficultiesduringinteractionswiththeVAduetotheincreasingcomplexityoftheir
,Vol.1,No.1,Article.Publicationdate:August2024.AnAnalyticalFrameworkforUserVerbalandNonverbalBehaviors 27
voicecommands.Byapplyingthisanalyticalframework,researchersandsystemdesignerscan
identifydisadvantagesintheinteractionbetweenhumansandLLM-VAandstrategicallyusethese
insightstoenhancethedesignofLLM-VAsacrossvarioustask-orientedsettings.
9.3 LimitationandFutureWork
WhileourstudyprovidesvaluableinsightsintohumaninteractionswithLLM-VAsincookingtasks,
ithascertainlimitationsthatmustbeacknowledged.
First,ourstudywasconductedwithlimitedparticipants,whichmaynotfullyrepresentthe
diverserangeofusersinteractingwithLLM-VAs.Additionally,focusingexclusivelyoncookingtasks
maylimittheapplicabilityofourfindingstoothercontextswhereLLM-VAsareused.Cookingisa
specificactivitywithitsownchallengesanduserinteractions,whichmightnotfullyencapsulateall
thescenariosinwhichpeopleuseLLM-VAs.Furthermore,thisanalyticalframeworkwasdeveloped
basedonobservationsinthisspecificcontext.Furthervalidationmayberequiredtoassessits
applicabilitytoothertasks,technologies,orusergroups.
Tobuildonthisresearch,futurestudiescouldaddresstheselimitationsinseveralways.Con-
ductingthestudywithalargerandmorediversegroupofparticipants,includingdifferentage
groups,culturalbackgrounds,andvaryinglevelsoffamiliaritywithtechnology,wouldenhance
thegeneralizabilityofthefindings.Additionally,applyingthestudytodifferentcontextsandtasks
whereLLM-VAsareusedcouldprovideinsightsintohowinteractionpatternsvaryacrossdifferent
activities.Byaddressingtheselimitations,futureresearchcanfurthercontributetothefieldof
human-computerinteraction,particularlyinenhancingthedesignandfunctionalityofLLM-VAs
forvariouspracticalapplications.
10 CONCLUSION
Inthisresearch,webridgethegapbetweenVAsandthenuanced,multi-modalnatureofhuman
communicationbydevelopingananalyticalframeworkthatcapturesthedynamicinterplayof
verbal and nonverbal behaviors in human interactions with LLM-VAs. Through our study, we
demonstratedthatusersnaturallyrelyonbothformsofcommunicationwhenperformingcomplex
tasks,suchascooking,withLLM-VAs.Weidentifiedkeybehaviorcharacteristics,developeda
three-stagemodel(Exploration,Conflict,andIntegration),andprovidedinsightsintohowusers
transitionbetweenthesestages.
Thesefindingshighlightthepotentialofincorporatingmultimodalinputsintofuturedesigns,
enablingLLM-VAstobemorepersonalized,sociallyaware,andresponsivetousers’mentalstates
andcommunicationstyles.AsLLMsbecomeincreasinglyintegratedintovoiceassistants,ourwork
setsthestagefordeeperexplorationofhuman-LLM-VAinteractionsandofferspracticalguidance
fordesigningmoreadaptive,user-friendlyLLM-VAsthatenhancebothuserexperienceandsystem
performance.
ACKNOWLEDGMENTS
ResearchreportedinthispublicationwassupportedbytheNationalInstituteOnMinorityHealth
AndHealthDisparitiesoftheNationalInstitutesofHealthunderAwardNumberR01MD018424.
Thecontentissolelytheresponsibilityoftheauthorsanddoesnotnecessarilyrepresenttheofficial
viewsoftheNationalInstitutesofHealth.
REFERENCES
[1] DrewHAbney,RickDale,MaxMLouwerse,andChristopherTKello.2018. Theburstsandlullsofmultimodal
interaction:Temporaldistributionsofbehaviorrevealdifferencesbetweenverbalandnon-verbalcommunication.
Cognitivescience42,4(2018),1297–1316.
,Vol.1,No.1,Article.Publicationdate:August2024.28 Chan,etal.
[2] HassanAli,PhilippAllgeuer,andStefanWermter.2024.ComparingApplestoOranges:LLM-poweredMultimodal
IntentionPredictioninanObjectCategorizationTask.arXivpreprintarXiv:2404.08424(2024).
[3] JamesAllen.1995.Naturallanguageunderstanding.Benjamin-CummingsPublishingCo.,Inc.
[4] VíctorAlvarez-Santos,RobertoIglesias,XoseManuelPardo,CarlosVRegueiro,andAdriánCanedo-Rodriguez.2014.
Gesture-basedinteractionwithvoicefeedbackforatour-guiderobot.JournalofVisualCommunicationandImage
Representation25,2(2014),499–509.
[5] TawfiqAmmari,JofishKaye,JaniceYTsai,andFrankBentley.2019.Music,search,andIoT:Howpeople(really)use
voiceassistants.ACMTransactionsonComputer-HumanInteraction(TOCHI)26,3(2019),1–28.
[6] DaneArcher.1997.Unspokendiversity:Culturaldifferencesingestures.Qualitativesociology20(1997),79–105.
[7] DaneArcherandRobinMAkert.1977.Wordsandeverythingelse:Verbalandnonverbalcuesinsocialinterpretation.
Journalofpersonalityandsocialpsychology35,6(1977),443.
[8] MichaelArgyle.2013.Bodilycommunication.Routledge.
[9] AnnelieseArnold,StephanieKolody,AidanComeau,andAntonioMiguelCruz.2022.Whatdoestheliteraturesay
abouttheuseofpersonalvoiceassistantsinolderadults?Ascopingreview.DisabilityandRehabilitation:Assistive
Technology(2022),1–12.
[10] BrookeAuxier.[n.d.].5thingstoknowaboutAmericansandtheirsmartspeakers. https://www.pewresearch.org/fact-
tank/2019/11/21/5-things-to-know-about-americans-and-their-smart-speakers/
[11] VinceBartle,JaniceLyu,FreesoulElShabazz-Thompson,YunminOh,AngelaAnqiChen,Yu-JanChang,Kenneth
Holstein,andNicolaDell.2022.“ASecondVoice”:InvestigatingOpportunitiesandChallengesforInteractiveVoice
AssistantstoSupportHomeHealthAides.InProceedingsofthe2022CHIConferenceonHumanFactorsinComputing
Systems.1–17.
[12] DianaBeirl,YRogers,andNicolaYuill.2019. Usingvoiceassistantskillsinfamilylife.InComputer-Supported
CollaborativeLearningConference,CSCL,Vol.1.InternationalSocietyoftheLearningSciences,Inc.,96–103.
[13] ErinBeneteau,AshleyBoone,YuxingWu,JulieAKientz,JasonYip,andAlexisHiniker.2020.ParentingwithAlexa:
exploringtheintroductionofsmartspeakersonfamilydynamics.InProceedingsofthe2020CHIconferenceonhuman
factorsincomputingsystems.1–13.
[14] ErinBeneteau,OliviaKRichards,MingruiZhang,JulieAKientz,JasonYip,andAlexisHiniker.2019.Communication
breakdownsbetweenfamiliesandAlexa.InProceedingsofthe2019CHIconferenceonhumanfactorsincomputing
systems.1–13.
[15] FrankBentley,ChrisLuvogt,MaxSilverman,RushaniWirasinghe,BrookeWhite,andDanielleLottridge.2018.
Understandingthelong-termuseofsmartspeakerassistants.ProceedingsoftheACMonInteractive,Mobile,Wearable
andUbiquitousTechnologies2,3(2018),1–24.
[16] MichaelBonfert,NimaZargham,FlorianSaade,RobertPorzel,andRainerMalaka.2021.AnEvaluationofVisual
EmbodimentforVoiceAssistantsonSmartDisplays.InProceedingsofthe3rdConferenceonConversationalUser
Interfaces.1–11.
[17] RobinBrewer,CaseyPierce,PoojaUpadhyay,andLeeseulPark.2022. Anempiricalstudyofolderadult’svoice
assistantuseforhealthinformationseeking.ACMTransactionsonInteractiveIntelligentSystems(TiiS)12,2(2022),
1–32.
[18] JohnL.Campbell,CharlesQuincy,JordanOsserman,andOveK.Pedersen.2013.CodingIn-depthSemistructured
Interviews:ProblemsofUnitizationandIntercoderReliabilityandAgreement.SociologicalMethods&Research42,3
(Aug.2013),294–320. https://doi.org/10.1177/0049124113500475
[19] ClareCarroll,CatherineChiodo,AdenaXinLin,MegNidever,andJayanthPrathipati.2017. Robin:enabling
independenceforindividualswithcognitivedisabilitiesusingvoiceassistivetechnology.InProceedingsofthe2017
CHIconferenceextendedabstractsonhumanfactorsincomputingsystems.46–53.
[20] JustineCassell,YukikoINakano,TimothyWBickmore,CandaceLSidner,andCharlesRich.2001.Non-verbalcues
fordiscoursestructure.InProceedingsofthe39thAnnualMeetingoftheAssociationforComputationalLinguistics.
114–123.
[21] SzeyiChan,JiachenLi,BingshengYao,AmamaMahmood,Chien-MingHuang,HollyJimison,ElizabethDMynatt,and
DakuoWang.2023."MangoMango,HowtoLetTheLettuceDryWithoutASpinner?”:ExploringUserPerceptions
ofUsingAnLLM-BasedConversationalAssistantTowardCookingPartner.arXivpreprintarXiv:2310.05853(2023).
[22] TanyaLChartrandandJohnABargh.1999.Thechameleoneffect:Theperception–behaviorlinkandsocialinteraction.
Journalofpersonalityandsocialpsychology76,6(1999),893.
[23] ChaoranChen,BingshengYao,YanfangYe,DakuoWang,andTobyJia-JunLi.[n.d.].EvaluatingtheLLMAgentsfor
SimulatingHumanoidBehavior.([n.d.]).
[24] Jen-HaoChen,PeggyPei-YuChi,Hao-HuaChu,CherylChia-HuiChen,andPollyHuang.2010.Asmartkitchenfor
nutrition-awarecooking.IEEEPervasiveComputing9,4(2010),58–65.
,Vol.1,No.1,Article.Publicationdate:August2024.AnAnalyticalFrameworkforUserVerbalandNonverbalBehaviors 29
[25] JessieChin,SmitDesai,Sheny(Cheng-Hsuan)Lin,andShannonMejia.2024. LikeMyAuntDorothy:Effectsof
ConversationalStylesonPerceptions,AcceptanceandMetaphoricalDescriptionsofVoiceAssistantsduringLater
Adulthood.Proc.ACMHum.-Comput.Interact.8,CSCW1(April2024),88:1–88:21. https://doi.org/10.1145/3637365
[26] JangheeChoandEmileeRader.2020.Theroleofconversationalgroundinginsupportingsymbiosisbetweenpeople
anddigitalassistants.ProceedingsoftheACMonHuman-ComputerInteraction4,CSCW1(2020),1–28.
[27] MinjiCho,Sang-suLee,andKun-PyoLee.2019.Onceakindfriendisnowathing:Understandinghowconversational
agentsathomeareforgotten.InProceedingsofthe2019onDesigningInteractiveSystemsConference.1557–1569.
[28] HerbertHClarkandSusanEBrennan.1991.Groundingincommunication.(1991).
[29] BenjaminRCowan,NadiaPantidi,DavidCoyle,KellieMorrissey,PeterClarke,SaraAl-Shehri,DavidEarley,and
NatashaBandeira.2017."Whatcanihelpyouwith?"infrequentusers’experiencesofintelligentpersonalassistants.
InProceedingsofthe19thinternationalconferenceonhuman-computerinteractionwithmobiledevicesandservices.
1–12.
[30] SamuelRhysCoxandWeiTsangOoi.2022. DoesChatbotLanguageFormalityAffectUsers’Self-Disclosure?.In
Proceedingsofthe4thConferenceonConversationalUserInterfaces.ACM,GlasgowUnitedKingdom,1–13. https:
//doi.org/10.1145/3543829.3543831
[31] AndreaCuadra,HyeinBaek,DeborahEstrin,MalteJung,andNicolaDell.2022.OnInclusion:VideoAnalysisofOlder
AdultInteractionswithaMulti-ModalVoiceAssistantinaPublicSetting.InProceedingsofthe2022International
ConferenceonInformationandCommunicationTechnologiesandDevelopment.1–17.
[32] AndreaCuadra,HansolLee,JasonCho,andWendyJu.2021.LookatMeWhenITalktoYou:AVideoDatasetto
EnableVoiceAssistantstoRecognizeErrors.arXivpreprintarXiv:2104.07153(2021).
[33] CanCui,YunshengMa,XuCao,WenqianYe,andZiranWang.2024. Driveasyouspeak:Enablinghuman-like
interactionwithlargelanguagemodelsinautonomousvehicles.InProceedingsoftheIEEE/CVFWinterConferenceon
ApplicationsofComputerVision.902–909.
[34] HaiDang,LukasMecke,FlorianLehmann,SvenGoller,andDanielBuschek.2022.HowtoPrompt?Opportunities
andChallengesofZero-andFew-ShotLearningforHuman-AIInteractioninCreativeApplicationsofGenerative
Models. arXiv:2209.01390[cs.HC]
[35] SmitDesai,MateuszDubiel,andLuisA.Leiva.2024.ExaminingHumannessasaMetaphortoDesignVoiceUser
Interfaces.InProceedingsofthe6thACMConferenceonConversationalUserInterfaces(CUI’24).Associationfor
ComputingMachinery,NewYork,NY,USA,1–15. https://doi.org/10.1145/3640794.3665535
[36] SmitDesaiandMichaelTwidale.2023.MetaphorsinVoiceUserInterfaces:ASlipperyFish.ACMTransactionson
Computer-HumanInteraction30,6(Sept.2023),89:1–89:37. https://doi.org/10.1145/3609326
[37] SandraDevinandRachidAlami.2016. AnImplementedTheoryofMindtoImproveHuman-RobotSharedPlans
Execution.InTheEleventhACM/IEEEInternationalConferenceonHumanRobotInteration(TheEleventhACM/IEEE
InternationalConferenceonHumanRobotInteration).IEEEPress,Christchurch,NewZealand,319–326. https:
//doi.org/10.1109/HRI.2016.7451768
[38] PhilipRDoyle,LeighClark,andBenjaminRCowan.2021. Whatdoweseeinthem?identifyingdimensionsof
partnermodelsforspeechinterfacesusingapsycholexicalapproach.InProceedingsofthe2021CHIConferenceon
HumanFactorsinComputingSystems.1–14.
[39] StarkeyDuncanandDonaldWFiske.2015.Face-to-faceinteraction:Research,methods,andtheory.Routledge.
[40] PaulEkmanandWallaceVFriesen.1969.Therepertoireofnonverbalbehavior:Categories,origins,usage,andcoding.
semiotica1,1(1969),49–98.
[41] IvánEsteban-Lozano,ÁlvaroCastro-González,andPalomaMartínez.2024.UsingaLLM-BasedConversationalAgent
intheSocialRobotMini.InInternationalConferenceonHuman-ComputerInteraction.Springer,15–26.
[42] JenniferFeredayandEimearMuir-Cochrane.2006.Demonstratingrigorusingthematicanalysis:Ahybridapproach
ofinductiveanddeductivecodingandthemedevelopment.Internationaljournalofqualitativemethods5,1(2006),
80–92.
[43] JohnFiske.2010.Introductiontocommunicationstudies.Routledge.
[44] RoseGuingrichandMichaelSAGraziano.2023.Chatbotsassocialcompanions:Howpeopleperceiveconsciousness,
humanlikeness,andsocialhealthbenefitsinmachines.arXivpreprintarXiv:2311.10599(2023).
[45] ReikoHamada,JunOkabe,IchiroIde,Shin’ichiSatoh,ShuichiSakai,andHidehikoTanaka.2005. Cookingnavi:
assistantfordailycookinginkitchen.InProceedingsofthe13thannualACMinternationalconferenceonMultimedia.
371–374.
[46] ChristinaNHarrington,RadhikaGarg,AmandaWoodward,andDimitriWilliams.2022. “It’sKindofLikeCode-
Switching”:BlackOlderAdults’ExperienceswithaVoiceAssistantforHealthInformationSeeking.InProceedingsof
the2022CHIConferenceonHumanFactorsinComputingSystems.1–15.
[47] JunHatori,YutaKikuchi,SosukeKobayashi,KuniyukiTakahashi,YutaTsuboi,YuyaUnno,WilsonKo,andJethro
Tan.2018.Interactivelypickingreal-worldobjectswithunconstrainedspokenlanguageinstructions.In2018IEEE
,Vol.1,No.1,Article.Publicationdate:August2024.30 Chan,etal.
InternationalConferenceonRoboticsandAutomation(ICRA).IEEE,3774–3781.
[48] ShaoshuaiHuang,XuandongZhao,DapengWei,XinhengSong,andYuanboSun.2024.ChatbotandFatiguedDriver:
ExploringtheUseofLLM-BasedVoiceAssistantsforDrivingFatigue.InExtendedAbstractsoftheCHIConferenceon
HumanFactorsinComputingSystems.1–8.
[49] RazanJaber.2023.TowardsDesigningBetterSpeechAgentInteraction:UsingEyeGazeforInteraction.Ph.D.Dissertation.
DepartmentofComputerandSystemsSciences,StockholmUniversity.
[50] RazanJaber,SabrinaZhong,SannaKuoppamäki,AidaHosseini,IonaGessinger,DuncanPBrumby,BenjaminR
Cowan,andDonaldMcmillan.2024.CookingWithAgents:DesigningContext-awareVoiceInteraction.InProceedings
oftheCHIConferenceonHumanFactorsinComputingSystems.1–13.
[51] EllenJiang,KristenOlson,EdwinToh,AlejandraMolina,AaronDonsbach,MichaelTerry,andCarrieJCai.2022.
Promptmaker:Prompt-basedprototypingwithlargelanguagemodels.InCHIConferenceonHumanFactorsin
ComputingSystemsExtendedAbstracts.1–8.
[52] PranavKhadpe,RanjayKrishna,LiFei-Fei,JeffreyT.Hancock,andMichaelS.Bernstein.2020.ConceptualMetaphors
ImpactPerceptionsofHuman-AICollaboration.Proc.ACMHum.-Comput.Interact.4,CSCW2(Oct.2020),163:1–163:26.
https://doi.org/10.1145/3415234
[53] CallieYKim,ChristinePLee,andBilgeMutlu.2024.Understandinglarge-languagemodel(llm)-poweredhuman-robot
interaction.InProceedingsofthe2024ACM/IEEEInternationalConferenceonHuman-RobotInteraction.371–380.
[54] SunyoungKimandAbhishekChoudhury.2021.Exploringolderadults’perceptionanduseofsmartspeaker-based
voiceassistants:Alongitudinalstudy.ComputersinHumanBehavior124(2021),106914.
[55] MarkL.Knapp,JudithA.Hall,andTerrenceG.Horgan.2013. NonverbalCommunicationinHumanInteraction.
CengageLearning.
[56] DimosthenisKontogiorgos,AndrePereira,OlleAndersson,MarcoKoivisto,ElenaGonzalezRabal,VilleVartiainen,
andJoakimGustafson.2019.Theeffectsofanthropomorphismandnon-verbalsocialbehaviourinvirtualassistants.
InProceedingsofthe19thACMInternationalConferenceonIntelligentVirtualAgents.133–140.
[57] DimosthenisKontogiorgos,AndrePereira,BoranSahindal,SannevanWaveren,andJoakimGustafson.2020.Be-
haviouralresponsestorobotconversationalfailures.InProceedingsofthe2020ACM/IEEEInternationalConferenceon
Human-RobotInteraction.53–62.
[58] ThomasKosch,KevinWennrich,DanielTopp,MarcelMuntzinger,andAlbrechtSchmidt.2019.Thedigitalcooking
coach:usingvisualandauditoryin-situinstructionstoassistcognitivelyimpairedduringcooking.InProceedingsof
the12thACMInternationalConferenceonPErvasiveTechnologiesRelatedtoAssistiveEnvironments.156–163.
[59] FranklinBKrohn.2004. Agenerationalapproachtousingemoticonsasnonverbalcommunication. Journalof
technicalwritingandcommunication34,4(2004),321–328.
[60] EmilyKuang,EhsanJahangirzadehSoure,MingmingFan,JianZhao,andKristenShinohara.2023.Collaborationwith
ConversationalAIAssistantsforUXEvaluation:QuestionsandHowtoAskthem(Voicevs.Text).InProceedingsof
the2023CHIConferenceonHumanFactorsinComputingSystems.1–15.
[61] DuongMinhLe,RuohaoGuo,WeiXu,andAlanRitter.2023.ImprovedInstructionOrderinginRecipe-Grounded
Conversation.arXivpreprintarXiv:2305.17280(2023).
[62] YunxiangLi,ZihanLi,KaiZhang,RuilongDan,SteveJiang,andYouZhang.2023.ChatDoctor:AMedicalChatModel
Fine-TunedonaLargeLanguageModelMeta-AI(LLaMA)UsingMedicalDomainKnowledge.Cureus15,6(2023).
[63] QVeraLiao,MuhammedMas-udHussain,PraveenChandar,MatthewDavis,YasamanKhazaeni,MarcoPatricio
Crasso,DakuoWang,MichaelMuller,NSadatShami,andWernerGeyer.2018.Allworkandnoplay?.InProceedings
ofthe2018CHIConferenceonHumanFactorsinComputingSystems.1–13.
[64] ZheLiu,ChunyangChen,JunjieWang,MengzhuoChen,BoyuWu,XingChe,DandanWang,andQingWang.2024.
Makellmatestingexpert:Bringinghuman-likeinteractiontomobileguitestingviafunctionality-awaredecisions.In
ProceedingsoftheIEEE/ACM46thInternationalConferenceonSoftwareEngineering.1–13.
[65] RobertHLogie,ASLaw,StevenTrawley,andJackNissan.2010.Multitasking,workingmemoryandremembering
intentions.PsychologicaBelgica50,3-4(2010),309–326.
[66] IreneLopatovska,KatrinaRink,IanKnight,KieranRaines,KevinCosenza,HarrietWilliams,PerachyaSorsche,David
Hirsch,QiLi,andAdriannaMartinez.2019.Talktome:ExploringuserinteractionswiththeAmazonAlexa.Journal
ofLibrarianshipandInformationScience51,4(2019),984–997.
[67] XuanLu,WeiAi,XuanzheLiu,QianLi,NingWang,GangHuang,andQiaozhuMei.2016. Learningfromthe
ubiquitouslanguage:anempiricalanalysisofemojiusageofsmartphoneusers.InProceedingsofthe2016ACM
internationaljointconferenceonpervasiveandubiquitouscomputing.770–780.
[68] DominicWMassaro,MichaelMCohen,SharonDaniel,andRonaldACole.1999. Developingandevaluating
conversationalagents.InHumanperformanceandergonomics.Elsevier,173–194.
[69] NiharikaMathur,KunalDhodapkar,TamaraZubatiy,JiachenLi,BrianJones,andElizabethMynatt.2022. ACol-
laborativeApproachtoSupportMedicationManagementinOlderAdultswithMildCognitiveImpairmentUsing
,Vol.1,No.1,Article.Publicationdate:August2024.AnAnalyticalFrameworkforUserVerbalandNonverbalBehaviors 31
ConversationalAssistants(CAs).InProceedingsofthe24thInternationalACMSIGACCESSConferenceonComputers
andAccessibility.1–14.
[70] DavidMatsumotoandHyisungCHwang.2013.Culturalsimilaritiesanddifferencesinemblematicgestures.Journal
ofNonverbalBehavior37(2013),1–27.
[71] LinaMavrina,JessicaSzczuka,ClaraStrathmann,LisaMichelleBohnenkamp,NicoleKrämer,andStefanKopp.2022.
“Alexa,You’reReallyStupid”:ALongitudinalFieldStudyonCommunicationBreakdownsBetweenFamilyMembers
andaVoiceAssistant.FrontiersinComputerScience4(2022),791704.
[72] DerekMcColl,AlexanderHong,NaoakiHatakeyama,GoldieNejat,andBenoBenhabib.2016.Asurveyofautonomous
humanaffectdetectionmethodsforsocialrobotsengagedinnaturalHRI.JournalofIntelligent&RoboticSystems82
(2016),101–133.
[73] DonaldMcMillan,BarryBrown,IkkakuKawaguchi,RazanJaber,JordiSolsonaBelenguer,andHideakiKuzuoka.2019.
DesigningwithGaze:Tama–aGazeActivatedSmart-Speaker.Proc.ACMHum.-Comput.Interact.3,CSCW,Article
176(nov2019),26pages. https://doi.org/10.1145/3359278
[74] DonaldMcMillan,BarryBrown,IkkakuKawaguchi,RazanJaber,JordiSolsonaBelenguer,andHideakiKuzuoka.2019.
Designingwithgaze:Tama–agazeactivatedsmart-speaker.ProceedingsoftheACMonHuman-ComputerInteraction
3,CSCW(2019),1–26.
[75] ChelseaMyers,AnushayFurqan,JessicaNebolsky,KarinaCaro,andJichenZhu.2018. Patternsforhowusers
overcomeobstaclesinvoiceuserinterfaces.InProceedingsofthe2018CHIconferenceonhumanfactorsincomputing
systems.1–7.
[76] ShrikanthNarayananandPanayiotisGGeorgiou.2013.Behavioralsignalprocessing:Derivinghumanbehavioral
informaticsfromspeechandlanguage.Proc.IEEE101,5(2013),1203–1233.
[77] CliffordNassandLiGong.2000.Speechinterfacesfromanevolutionaryperspective.Commun.ACM43,9(2000),
36–43.
[78] CliffordNassandYoungmeMoon.2000.Machinesandmindlessness:Socialresponsestocomputers.Journalofsocial
issues56,1(2000),81–103.
[79] CliffordNassandJonathanSteuer.1993.Voices,boxes,andsourcesofmessages:Computersandsocialactors.Human
CommunicationResearch19,4(1993),504–527.
[80] HumzaNaveed,AsadUllahKhan,ShiQiu,MuhammadSaqib,SaeedAnwar,MuhammadUsman,NickBarnes,and
AjmalMian.2023.Acomprehensiveoverviewoflargelanguagemodels.arXivpreprintarXiv:2307.06435(2023).
[81] NilsNeumannandSvenWachsmuth.2021.RecipeEnrichment:KnowledgeRequiredforaCookingAssistant..In
ICAART(2).822–829.
[82] ElnazNouri,AdamFourney,RobertSim,andRyenWWhite.2019.Supportingcomplextasksusingmultipledevices.
InProceedingsofWSDM’19TaskIntelligenceWorkshop(TI@WSDM19).
[83] YoungHoonOh,KyungjinChung,andDaYoungJu.2020.Differencesininteractionswithaconversationalagent.
Internationaljournalofenvironmentalresearchandpublichealth17,9(2020),3189.
[84] OpenAI.2023. GPT-4TechnicalReport. ArXivabs/2303.08774(2023). https://api.semanticscholar.org/CorpusID:
257532815
[85] CansuOrançandAzzurraRuggeri.2021.“Alexa,letmeaskyousomethingdifferent”Children’sadaptiveinformation
searchwithvoiceassistants.HumanBehaviorandEmergingTechnologies3,4(2021),595–605.
[86] JamiePearson,JiangHu,HollyPBranigan,MartinJPickering,andCliffordINass.2006.Adaptivelanguagebehavior
inHCI:howexpectationsandbeliefsaboutasystemaffectusers’wordchoice.InProceedingsoftheSIGCHIconference
onHumanFactorsincomputingsystems.1177–1180.
[87] RosalindWPicardandJonathanKlein.2002.Computersthatrecogniseandrespondtouseremotion:theoreticaland
practicalimplications.Interactingwithcomputers14,2(2002),141–169.
[88] PatrykPomykalski,MikołajP.Woźniak,PawełW.Woźniak,KrzysztofGrudzień,ShengdongZhao,andAndrzej
Romanowski.2020. ConsideringWakeGesturesforSmartAssistantUse.InExtendedAbstractsofthe2020CHI
ConferenceonHumanFactorsinComputingSystems(<conf-loc>,<city>Honolulu</city>,<state>HI</state>,<coun-
try>USA</country>,</conf-loc>)(CHIEA’20).AssociationforComputingMachinery,NewYork,NY,USA,1–8.
https://doi.org/10.1145/3334480.3383089
[89] MartinPorcheron,JoelEFischer,StuartReeves,andSarahSharples.2018. Voiceinterfacesineverydaylife.In
proceedingsofthe2018CHIconferenceonhumanfactorsincomputingsystems.1–12.
[90] AlishaPradhan,LeahFindlater,andAmandaLazar.2019. "PhantomFriend"or"JustaBoxwithInformation"
PersonificationandOntologicalCategorizationofSmartSpeaker-basedVoiceAssistantsbyOlderAdults.Proceedings
oftheACMonHuman-ComputerInteraction3,CSCW(2019),1–21.
[91] ByronReevesandCliffordNass.1996.Themediaequation:Howpeopletreatcomputers,television,andnewmedia
likerealpeople.Cambridge,UK10,10(1996).
,Vol.1,No.1,Article.Publicationdate:August2024.32 Chan,etal.
[92] EhudReiterandRobertDale.1997. Buildingappliednaturallanguagegenerationsystems. NaturalLanguage
Engineering3,1(1997),57–87.
[93] MinjinRheu,JiYounShin,WeiPeng,andJinaHuh-Yoo.2021. Systematicreview:Trust-buildingfactorsand
implicationsforconversationalagentdesign. InternationalJournalofHuman–ComputerInteraction37,1(2021),
81–96.
[94] AafaqSabir,EvanLafontaine,andAnupamDas.2022.HeyAlexa,WhoAmITalkingto?:AnalyzingUsers’Perception
andAwarenessRegardingThird-partyAlexaSkills.InProceedingsofthe2022CHIConferenceonHumanFactorsin
ComputingSystems(CHI’22).AssociationforComputingMachinery,NewYork,NY,USA,1–15. https://doi.org/10.
1145/3491102.3517510
[95] AyakaSato,KeitaWatanabe,andJunRekimoto.2014.MimiCook:acookingassistantsystemwithsituatedguidance.
InProceedingsofthe8thinternationalconferenceontangible,embeddedandembodiedinteraction.121–124.
[96] ShaneSaundersonandGoldieNejat.2019.Howrobotsinfluencehumans:Asurveyofnonverbalcommunicationin
socialhuman–robotinteraction.InternationalJournalofSocialRobotics11(2019),575–608.
[97] AlexSciuto,ArnitaSaini,JodiForlizzi,andJasonIHong.2018."HeyAlexa,What’sUp?"AMixed-MethodsStudies
ofIn-HomeConversationalAgentUsage.InProceedingsofthe2018designinginteractivesystemsconference.857–868.
[98] PaulSemaan.2012.Naturallanguagegeneration:anoverview.JComputSciRes1,3(2012),50–57.
[99] MohitShridharandDavidHsu.2018.Interactivevisualgroundingofreferringexpressionsforhuman-robotinteraction.
arXivpreprintarXiv:1806.03831(2018).
[100] HanSloetjesandPeterWittenburg.2008.Annotationbycategory-ELANandISODCR.In6thinternationalConference
onLanguageResourcesandEvaluation(LREC2008).
[101] SiegfriedLudwigSporerandBarbaraSchwandt.2006.Paraverbalindicatorsofdeception:Ameta-analyticsynthesis.
AppliedCognitivePsychology:TheOfficialJournaloftheSocietyforAppliedResearchinMemoryandCognition20,4
(2006),421–446.
[102] GeorgeTerzopoulosandMayaSatratzemi.2020.Voiceassistantsandsmartspeakersineverydaylifeandineducation.
InformaticsinEducation19,3(2020),473–490.
[103] DavidRThomas.2003.Ageneralinductiveapproachforqualitativedataanalysis.(2003).
[104] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,Baptiste
Rozière,NamanGoyal,EricHambro,FaisalAzhar,etal.2023.Llama:Openandefficientfoundationlanguagemodels.
arXivpreprintarXiv:2302.13971(2023).
[105] MilkaTrajkovaandAqueashaMartin-Hammond.2020."AlexaisaToy":exploringolderadults’reasonsforusing,
limiting,andabandoningecho.InProceedingsofthe2020CHIconferenceonhumanfactorsincomputingsystems.1–13.
[106] AlexandraVtyurinaandAdamFourney.2018.Exploringtheroleofconversationalcuesinguidedtasksupportwith
virtualassistants.InProceedingsofthe2018CHIconferenceonhumanfactorsincomputingsystems.1–7.
[107] BryanWang,GangLi,andYangLi.2023.Enablingconversationalinteractionwithmobileuiusinglargelanguage
models.InProceedingsofthe2023CHIConferenceonHumanFactorsinComputingSystems.1–17.
[108] QiaosiWang,KoustuvSaha,EricGregori,DavidJoyner,andAshokGoel.2021. TowardsMutualTheoryofMind
inHuman-AIInteraction:HowLanguageReflectsWhatStudentsPerceiveAboutaVirtualTeachingAssistant.In
Proceedingsofthe2021CHIConferenceonHumanFactorsinComputingSystems(CHI’21).AssociationforComputing
Machinery,NewYork,NY,USA,1–14. https://doi.org/10.1145/3411764.3445645
[109] JohannaWeber,MargaritaEsau-Held,MarvinSchiller,EikeMartinThaden,DietrichManstetten,andGunnarStevens.
2023.DesigninganInteractionConceptforAssistedCookinginSmartKitchens:FocusonHumanAgency,Proactivity,
andMultimodality.InProceedingsofthe2023ACMDesigningInteractiveSystemsConference.1128–1144.
[110] RainerWinkler,MatthiasSöllner,MayaLisaNeuweiler,FlaviaContiRossini,andJanMarcoLeimeister.2019.Alexa,
canyouhelpussolvethisproblem?Howconversationswithsmartpersonalassistanttutorsincreasetaskgroup
outcomes.InExtendedabstractsofthe2019CHIconferenceonhumanfactorsincomputingsystems.1–6.
[111] SarahWisemanandSandyJJGould.2018.Repurposingemojiforpersonalisedcommunication:Whymeans“Ilove
you”.InProceedingsofthe2018CHIconferenceonhumanfactorsincomputingsystems.1–10.
[112] TongshuangWu,MichaelTerry,andCarrieJunCai.2022.Aichains:Transparentandcontrollablehuman-aiinteraction
bychaininglargelanguagemodelprompts.InProceedingsofthe2022CHIconferenceonhumanfactorsincomputing
systems.1–22.
[113] XuhaiXu,BingshengYao,YuanzheDong,SaadiaGabriel,HongYu,JamesHendler,MarzyehGhassemi,AnindK.Dey,
andDakuoWang.2023.Mental-LLM:LeveragingLargeLanguageModelsforMentalHealthPredictionviaOnline
TextData. arXiv:2307.14385[cs.CL]
[114] XuhaiXu,BingshenYao,YuanzheDong,HongYu,JamesHendler,AnindKDey,andDakuoWang.2023.Leveraging
largelanguagemodelsformentalhealthpredictionviaonlinetextdata.arXivpreprintarXiv:2307.14385(2023).
[115] ZiqiYang,XuhaiXu,BingshengYao,EthanRogers,ShaoZhang,StephenIntille,NawarShara,GuodongGordon
Gao,andDakuoWang.2024. Talk2Care:AnLLM-basedVoiceAssistantforCommunicationbetweenHealthcare
,Vol.1,No.1,Article.Publicationdate:August2024.AnAnalyticalFrameworkforUserVerbalandNonverbalBehaviors 33
ProvidersandOlderAdults.ProceedingsoftheACMonInteractive,Mobile,WearableandUbiquitousTechnologies8,2
(2024),1–35.
[116] NimaZargham,VinoAvanesi,LeonReicherts,AvaElizabethScott,YvonneRogers,andRainerMalaka.2023.“Funny
How?”ASeriousLookatHumorinConversationalAgents.InProceedingsofthe5thInternationalConferenceon
ConversationalUserInterfaces(CUI’23).AssociationforComputingMachinery,NewYork,NY,USA,1–7. https:
//doi.org/10.1145/3571884.3603761
[117] ZhengZhang,YingXu,YanhaoWang,BingshengYao,DanielRitchie,TongshuangWu,MoYu,DakuoWang,and
TobyJia-JunLi.2022.Storybuddy:Ahuman-aicollaborativechatbotforparent-childinteractivestorytellingwith
flexibleparentalinvolvement.InProceedingsofthe2022CHIConferenceonHumanFactorsinComputingSystems.
1–21.
[118] YaxiZhao,RazanJaber,DonaldMcMillan,andCosminMunteanu.2022. “RewindtotheJigglingMeatPart”:
UnderstandingVoiceControlofInstructionalVideosinEverydayTasks.InProceedingsofthe2022CHIConferenceon
HumanFactorsinComputingSystems.1–11.
Received20February2007;revised12March2009;accepted5June2009
,Vol.1,No.1,Article.Publicationdate:August2024.