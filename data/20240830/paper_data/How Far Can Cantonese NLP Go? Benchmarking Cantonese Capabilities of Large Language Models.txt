How Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities
of Large Language Models
JiyueJiang♡,LihengChen♠,PenganChen♠,ShengWang♠,QinghangBao♠,
LingpengKong♠,YuLi♡,ChuanWu♠
♡ TheChineseUniversityofHongKong,♠ TheUniversityofHongKong
jiangjy@link.cuhk.edu.hk {clh648, cpa2001, u3009618, bill6176}@connect.hku.hk,
lpk@cs.hku.hk liyu@cse.cuhk.edu.hk cwu@cs.hku.hk
Abstract
Language Group
Asian Languages
8000 European Languages
Therapidevolutionoflargelanguagemodels
(LLMs)hastransformedthecompetitiveland- 6000
scape in natural language processing (NLP),
particularlyforEnglishandotherdata-richlan- 4000
guages. However,underrepresentedlanguages
likeCantonese,spokenbyover85millionpeo- 2000
ple,facesignificantdevelopmentgaps,which
isparticularlyconcerninggiventheeconomic 0
significance of the Guangdong-Hong Kong-
Macau Greater Bay Area, and in substantial
Cantonese-speakingpopulationsinplaceslike
Language
Singapore and North America. Despite its
Figure 1: This is number of publications in the ACL
wideuse,Cantonesehasscantrepresentation
AnthologyindexedbylanguagesasofJune2024. Fol-
inNLPresearch,especiallycomparedtoother
lowing(Xiangetal.,2024),weretrievethepublications
languages from similarly developed regions.
viasearchingthelanguagenameineitherthetitleorthe
Tobridgethesegaps,weoutlinecurrentCan-
abstractfromtheACLAnthology.
toneseNLPmethodsandintroducenewbench-
marksdesignedtoevaluateLLMperformance
infactualgeneration,mathematicallogic,com-
plexreasoning,andgeneralknowledgeinCan- seen slower technological development, particu-
tonese,whichaimtoadvanceopen-sourceCan- larly in the realm of LLMs. Language technolo-
toneseLLMtechnology. Wealsoproposefu- gies for Cantonese have not yet reaped the ben-
tureresearchdirectionsandrecommendedmod- efits of this revolution (Xiang et al., 2022). As
elstoenhanceCantoneseLLMdevelopment.
indicated in Figure 1 and Table 1, there is a no-
tablylownumberofrecentresearchpublications
1 Introduction related to Cantonese, especially when compared
to the population ratio. Developed regions like
Increasingly impactful and LLMs have emerged
Swedish, German, Japanese have high publica-
(e.g., GPT-X, Llama-X, etc.), which is propelled
tion ratios, but among all languages with speak-
the development of technologies associated with
ersmorethan80million,Cantonesehasthemost
LLMsinwhatcanbedescribedasa“battleofthe
limitedrelevantresearchpublications. Giventhat
models”. As shown in Figure 1, NLP research
the Guangdong-Hong Kong-Macau Greater Bay
haspredominantlyconcentratedoncreatingmod-
Area is one of the most economically vibrant re-
elsforEnglishandafewotherlanguagesthathave gionsintheworld1 andthatmanycountries(e.g.,
substantial data resources (Aji et al., 2022). The
Singapore,Malaysia,Australia,Canada,U.S.,etc.)
scarcity of data is often identified as the primary
have a large Cantonese-speaking population, ad-
obstacle impeding advancements in NLP for lan-
vancingCantoneseLLMtechnologyrepresentsa
guages that are less represented (Hu et al., 2020;
challengingyetworthwhileendeavor.
Joshietal.,2020;Ajietal.,2022),particularlyfor
LLM technology, as one of the most influen-
LLM-relatedtechnologies.
Cantonese (Yue language), spoken by over 85
1https://www.bayarea.gov.hk/filemanager/en/
millionpeopleworldwide(Xiangetal.,2024),has share/pdf/Outline_Development_Plan.pdf
4202
guA
92
]LC.sc[
1v65761.8042:viXra
srepap
fo
rebmuN
esenotnaC niradnaM idniH cibarA ilagneB naisreP esenapaJ ihtaraM hsikruT naeroK esemanteiV hsilgnE namreG naissuR hcnerF nailatI hsinapS hsiloP nainiarkU nainamoR hctuD hsidewSFigure 2: Overview of the paper: We begin by summarizing approaches from small-scale neural networks in
Cantonese,thenprogresstoLLMs(workinvolvingexistingCantoneseLLMs). IntheseLLMs,researchersplacea
greateremphasisonalignmentcomparedtopre-training. Consequently,weintroducefournewbenchmarksanda
translationdatatsettoevaluatetheCantonesecapabilitiesofLLMs. Weanalyzetheperformanceofmainstream
LLMsonthesebenchmarksand,incombinationwiththeinherentchallengesofCantoneseitself,identifythree
insightfulresearchopportunities,andrecommendLLMsfordifferenttasks(Table7).
Asian Pop. Ratio European Pop. Ratio holisticallyevaluatetheCantonesecapabilitiesof
Cantonese 87 0.74 English 1456 6.33 both Cantonese and general-purpose LLMs, we
Mandarin 1138 3.69 German 133 17.73
proposefournewbenchmarksinCantonese(Yue-
Hindi 610 1.19 Russian 255 2.60
Arabic 376 4.54 French 310 7.19 Truthful,Yue-GSM8K,Yue-ARC-C,Yue-MMLU)
Bengali 273 0.66 Italian 68 9.32
Persian 79 1.82 Spanish 559 2.47 andatranslationdataset(Yue-TRANS),whichare
Japanese 123 12.97 Polish 45 7.67
respectively the evaluation of LLMs’ abilities in
Marathi 99 1.05 Ukrainian 39 2.46
Turkish 90 4.00 Romanian 26 9.04 Cantonese for factual generation, mathematical
Korean 82 6.43 Dutch 55 8.42
Vietnamese 86 2.36 Swedish 11 35.46 logic,complexreasoning,generalknowledge,and
translation. Thesebenchmarksaretranslatedfrom
Table1: Language,population(Pop.),andpublication
English or Mandarin and manually reviewed for
to population ratio indirectly show the proportion of
accuracy. WefurtheranalyzetheCantonesecapa-
NLPresourcestodifferentlanguages(Appendix8).
bilitiesoftwenty-threemainstreamCantoneseand
general-purposeLLMsusingthesenewCantonese
benchmarks,andalsoexploredLLMsthataresuit-
tialtechniquesinNLP,currentlyhasverylimited
ableforgeneratinghigh-qualityCantonesetransla-
Cantonese-relateddevelopment,andmostofitre-
tions. Finally,addressingtheexistingchallengesin
mainsclosed-source. Inordertobetterpromotethe
Cantonese, and based on the analysis results and
developmentofCantoneseNLPandLLMtechnol-
thesechallenges,potentialresearchdirectionsand
ogy,wefirstsystematicallysummarizetheresearch
recommendLLMsforuseareproposed.
progressonexistingmethodsforsmall-scaleneu-
ral networks for Cantonese, including rumor de-
2 Cantonesesmall-scaleneuralnetwork
tection, sentiment analysis, machine translation,
dialogue,languagemodeling,andNLPtools. Sub-
CantoneseNLPresearchspreadsacrossvarioustop-
sequently, we further summarize the existing re-
ics,includingrumordetection,sentimentanalysis,
search on Cantonese LLMs and alignment. Be-
machinetranslation,dialogue. Wecollectexisting
causetrainingdataresourcesforCantoneseLLMs
smallneuralnetworkmethods,models,andtools.
are essential, we summarize the existing data re-
sourcesandbenchmarks. However,thesearechal- Rumor detection. (Chen et al., 2020) develop
lengingtouseforcomprehensivelyevaluatingthe a dataset of 27,328 Cantonese tweets for rumor
various capabilities of LLMs in Cantonese. To detection,splitinto13,883rumorsand13,445non-rumors. Theyintroduceanattention-basedmodel, Dialoguesummarizationandgeneration. (Lee
XGA,whichcombinesXLNet(Yangetal.,2019) etal.,2021)exploresgeneratingquestionsandre-
and BiGRU to analyze both semantic and senti- statinginformationinCantonesedialoguesystems,
ment aspects. (Chen et al., 2024) develop Can- particularlyforcounselingchatbots. Theyenhance
toneseBERT to capture glyph and pronunciation performance by fine-tuning the pre-trained Bert-
clues of Cantonese characters, and introduces a Summodel(LiuandLapata,2019)onCantonese
Cantoneserumordetectionmodel,SA-GCN,that data, effective in tasks involving text summariza-
encodesglobalstructuralinformationoftweethier- tionandquestiongeneration. Indialoguegenera-
archiesusingtheBiGCNmodelandextractslocal tion,(LeeandLiang,2021)developsaspecialized
semanticfeatureswiththeCantoneseBERTmodel. datasetforvirtualcounselorscontaining1,028post-
replypairsaddressingtestanxietyandloneliness,
Sentimentanalysis. Cantonesesentimentanal- usingthesecategoriestoguideresponseselection
ysis utilizes diverse methodologies to address its througharegressionmodel.
linguisticcomplexities. (Zhangetal.,2011)apply
Cantonese language model. Training Can-
Naive Bayes and SVMs with character-based bi-
tonese language models like XLNet (Yang et al.,
grams in the Openrice app for effective emotion
2019) and ELECTRA (Clark et al., 2020) from
detection. (Chenetal.,2013,2015)deployHidden
ToastyNews2 faceschallengesduetodatascarcity
MarkovModelsfortextsegmentationandpart-of-
andlegalconstraints. (Chenetal.,2024)introduce
speech tagging, developing emotion-specific dic-
CantoneseBERT and the SA-GCN model for de-
tionaries via rule-based systems. These studies
tailedanalysisandrumordetectionintweets,utiliz-
demonstratethevalueofcombiningmachinelearn-
inginnovativemethodslikepermutationlearning
ing with lexical techniques (Zhang et al., 2011;
andadversarialtraining. However,thetrainingcor-
Chenetal.,2013,2015). Inaddition,(Ngaietal.,
puslargelyincludesStandardChinese,leadingto
2018)and(Xiangetal.,2019)enhanceclassifica-
potentiallanguagecontamination,andtheimpact
tion accuracy using supervised and unsupervised
onmodelefficacyremainsunexplored.
methodsinvariousdomains. (Lee,2019)explores
fine-grained emotion analysis across languages,
Cantonese NLP tools. The landscape of Can-
achievingsignificantresults. Theseeffortsunder-
tonese NLP tools is diverse, addressing various
scoretheimportanceofmulti-methodologicalap-
needs. PyCantonese (Lee et al., 2022) facili-
proaches (Ngai et al., 2018; Xiang et al., 2019;
tates corpus data handling and linguistic analy-
Lee, 2019). (Tan et al., 2021) successfully em-
sis. HongKongCantoneseLocalizationprovides
ploy Transformers pre-trained on simplified Chi-
culturally contextual translations. TransCan3 en-
nese(Tanetal.,2021).
hancesEnglish-to-Cantonesetranslation,surpass-
ingcommercialsolutionslikeBaiduandBing. Text
Machine translation. Initial research in this
segmentationtoolslikeCantoneseWordSegmen-
area utilizes heuristic rules, with significant con-
tation4 and cantoseg5 improve accuracy through
tributions from (Zhang, 1998) and a bilingual
customdictionaries. canto-filter6 categorizestexts
Cantonese-Englishknowledgebaseby(Wuetal.,
basedonlinguisticfeatures,whilesongotsti7 and
2006). The focus has since shifted to statistical
fast-langid8 offeradditionalsupportforlanguage
machinetranslation,exemplifiedby(Huangetal.,
identification.
2016), who addresses the challenges of translat-
ingbetweenCantoneseandMandarinwithlimited
3 Cantoneselargelanguagemodel
resources. (Wong et al., 2018) improves this ap-
proach by enhancing parallel data for more effi- Developing Cantonese LLMs is challenging due
cientmodeltraining. Recentdevelopmentsinclude toscarceresourcesandthedistinctfeaturesofthe
a large-scale evaluation dataset by (Liu, 2022),
2https://huggingface.co/toastynews
containingover35,000Mandarin-Cantonesesen-
3https://github.com/ayaka14732/TransCan
tencepairs,andunsupervisedtranslationmodelsby 4https://github.com/wchan757/Cantonese_Word_
Segmentation
(Dareetal.,2023),whichusecross-lingualembed-
5https://github.com/ayaka14732/cantoseg
dingsandcombineTransformerarchitecturewith
6https://github.com/CanCLID/canto-filter
character-basedtokenizationtocreateanewcorpus 7https://github.com/justinchuntingho/songotsti
ofapproximately1millionCantonesesentences. 8https://github.com/ffreemt/fast-langidCantoneselanguage,necessitatingextensivehigh- andphonetictranscription(Ping-Wai,2006). (Lee,
quality datasets for pre-training9. Despite these 2011) introduces a parallel corpus for machine
obstacles,thesemodelsshowpromisingcapabili- translationbetweenCantoneseandStandardChi-
tiesinprocessingCantonese. nese,alignedatthesentencelevel,usingdatafrom
AligningCantoneseLLMsfordownstreamtasks, CantonesespeechesonHongKongtelevisionand
suchasprompting,supervisedfine-tuning,andre- theirStandardChinesesubtitles.
inforcementlearningfromhumanfeedback,iscost- Recent efforts aim to bridge the data gap be-
effectiveandhelpseliminatebiasesandmeetcul- tweenCantoneseandothermajorlanguages. These
turalexpectations. include a small parallel dependency treebank for
Recent studies (Fu et al., 2024) validate Chat- Cantonese and Mandarin, with 569 aligned sen-
GPT’seffectivenessinCantonesedialogueandsen- tencesannotatedusingtheUniversalDependencies
timentanalysis,analyzingmessagesfromaHong scheme, and excerpts from the “ABC Cantonese-
Kong web counseling service. The CanChat bot, English Comprehensive Dictionary” providing
introducedtoenhancecounselingservicesinHong 14,474high-qualityCantonese-Englishparallelsen-
Kong,providesinitialsupporttostudents,improv- tences,crucialfortranslationsystemdevelopment.
ingtheiremotionalwell-beingduringandbeyond
4.2 Newbenchmarksconstruction
theCOVID-19pandemic(Fungetal.,2023).
Transitioningfromsmall-scalenetworkstoex- Therearevariousbenchmarksfortestingthecapa-
ploring Cantonese LLMs, both general-purpose bilities of LLMs, yet there are no publicly avail-
andclosed-sourcemodelsshowpromise,butquan- ablebenchmarksspecificallydesignedtoevaluate
tifying performance is challenging. We propose the proficiency of Cantonese LLMs. Therefore,
four benchmarks to evaluate and advance Can- weconstructfourCantonesebenchmarksaimedat
tonesecapabilitiesinLLMs. evaluatingtheCantonesecapabilitiesofbothexist-
ingCantoneseandgeneralLLMs. Thebenchmarks
4 Cantonesedatasummaryandnew
weconstructedevaluatethecapabilitiesofLLMs
benchmarksconstruction
fromfouraspects: providingfactualanswers(Yue-
TruthfulQA), solving grade-level math problems
4.1 ExistingCantonesedata
(Yue-GSM8K),testingcomplexreasoningoversci-
Attheendofthe16thcentury,MatteoRiccicom-
entific knowledge (Yue-ARC-C), and the broad
piles the first “Modern Bilingual Chinese Dictio-
evaluation across 22 subjects to test general and
nary”,significantlyincorporatingCantoneseterms,
specialized knowledge (Yue-MMLU). The statis-
highlightingitsroleinSino-Westerninteractions.
ticsofthedatasetsareasfollows:
Bythe19thcentury,mostbilingualdictionariesfo-
cusonCantonese(Xiangetal.,2024). Historically,
Datasets Number Types
HongKongandrelatedinstitutionsleadCantonese
Yue-TruthfulQA 817 Factualgeneration
data initiatives. (Wu, 1994) creates a bilingual Yue-GSM8K 1319 Mathematicallogic
parallel corpus from the Hong Kong Legislative Yue-ARC-C 1171 Complexreasoning
Yue-MMLU 3721 Generalknowledge
Councilrecords,inbothStandardChineseandEn-
Yue-TRANS 400 Translation
glish. This effort is complemented by (Hun-tak
Lee, 1999), who pioneers a Cantonese-only cor- Table2: Questionnumberandtypeofthedatasets.
pus with one million characters from dialogues
involving children in Hong Kong, and (Yip and TheYue-ARC,Yue-GSM8K,andYue-ARC-C
Matthews,2007),whodevelopsabilingualcorpus datasetsaretranslatedfromtheirEnglishcounter-
forCantonese-speakingchildren. Additionally, a parts: ARC,GSM8K,andARC(challenge)respec-
notable Cantonese corpus comes from television tively. The Yue-MMLU dataset is derived from
and theatrical productions in Hong Kong (Leung CMMLU, featuring translations across an exten-
and Law, 2001). The University of Hong Kong sive range of twenty-two topics (Appendix A.3).
further contributes by collecting and annotating Thedatasetsaretranslatedusingmodelsbasedon
spontaneousspeechfromdialoguesandbroadcasts, ChatGPTandGPT-4o,andfourtri-lingualpeople
focusingonsegmentation,partsofspeechtagging, whospeakCantonese,MandarinandEnglishcon-
duct four rounds of reviews to develop the final
9https://www.sensetime.com/en/news-detail/
51168164?categoryId=1072 benchmarks. Yue-TRANSconsistsofarandomlyselectedsetoffourhundredtranslationpairs10(two evaluated are as follows (Appendix A.4 for de-
hundred pairs each from Mandarin to Cantonese tails of the model): (1) Qwen series: Qwen-1.5-
andEnglishtoCantonese). 110b,Qwen-2-7b,Qwen-2-72b;(2)Mixtralseries:
Mixtral-8x22b,Mixtral-large-2;(3)Llamaseries:
5 Experimentandanalysis
Llama-3-8b,Llama-3-70b,Llama-3.1-8b,Llama-
3.1-70b;(4)Phiseries: Phi-3-medium;(5)Gemma
5.1 Implementationdetails
series: Gemma-2-27b; (6) Yi series: Yi-1.5-34b;
We conduct experiments on the Yue-ARC, Yue-
(7) Internlm series: Internlm-2.5-7b; (8) ERNIE
MMLU,Yue-GSM8K,Yue-TruthfulQA,andYue-
series: ERNIE-Lite,ERNIE-Tiny,ERNIE-Speed,
TRNASdatasets. WeuseAPIsandsixA100-80G
ERNIE-Turbo ; (9) Sensechat series: Sensechat-
GPUstoperforminferencewithLLMs. Weemploy
5 (Cantonese); (10) Claude series: Claude-3.5-
sampling hyperparameters with top-p set to 1.0
sonnet;(11)GLMseries: GLM-4;(12)GPTseries:
and a temperature of 0.2 for generation (Specific
ChatGPT,GPT-4o,GPT-4.
promptsintheAppendixA.6).
5.4 Resultsandanalysis
5.2 Evaluation
TheperformanceofCantoneseLLMsstilllags
AutomaticEvaluation. ForYue-TruthfulQAand
behindthatinMandarinandEnglish. Rouge-
Yue-TRANS(0-shotand5-shot),weutilizeRouge-
l and Bleu-4 excel in evaluating the overlap be-
l,Bleu-4,andBERTScoreasautomaticevaluation
tweencandidateandreference,makingthemsuit-
metrics. Rouge-l(Lin,2004)measuresthelongest
ableforkeyinformationextraction,outperforming
commonsubsequencebetweengeneratedandrefer-
metricsusedin0-shotand5-shotsettings(Table3).
encetexts. Bleu-4(Papinenietal.,2002)evaluates
Thelattersettinggenerallysurpassestheformer,il-
n-gramoverlapuptofourwordsbetweengenerated
lustrating the advantage of additional references
and reference texts. BERTScore (Zhang* et al.,
in improving generation. Unlike these metrics,
2020) evaluates semantic similarity using BERT
BERTScoreexcelsindeepsemanticevaluation,im-
embeddings (Due to the presence of English ex-
portantforevaluatingdisparitiesinbenchmarksbe-
pressions in the Cantonese benchmarks, we use
tweenCantoneseandEnglish. MainstreamLLMs
bert-base-multilingual-cased11 forCantoneseeval-
performbetterinEnglishthaninCantonese,high-
uationandroberta-large12 forEnglishevaluation).
lightingtheirproficiencyinwidelyusedlanguages
ForYue-GSM8K,Yue-ARC-C,andYue-MMLU
andrelativeunder-performanceinCantonese(Ta-
(0-shot and 5-shot), we employ Accuracy (Acc.)
ble3,Table12). Accuracymetricsinbenchmarks
astheevaluationmetric. Accuracycalculatesthe
withuniqueanswerscorroboratethesefindings(Ta-
percentageofcorrectpredictionsbythemodel.
ble 4, Table 5, Table 6, Table 13, Table 14, Ta-
Human Evaluation. Following (Wang et al., ble15). 5-shottypicallyshowhigheraccuracythan
2020;Lietal.,2022;Jiangetal.,2023),wesam- 0-shot,andperformanceinmainstreamlanguages
ple 100 questions from Yue-TruthfulQA, and we like English and Mandarin surpasses that in Can-
findfourfluentCantonesespeakerstoevaluatethe tonese,emphasizingtheneedformoreCantonese-
Cantonese expression ability of different models focusedresearchandLLMdevelopment.
(Human). This is rated on five-scale, where 1, 3
Different series of models are suitable for
and5indicateunacceptable, moderateandexcel-
various Cantonese tasks. Qwen-1.5-110b and
lentperformance,respectively.
Mixtral-large-2leadinCantonesefactualgenera-
5.3 Largelanguagemodelsforcomparison tionin0-shotand5-shot,surpassingGPT,Llama,
and Sensechat series. Gemma-2-27b, excluding
WeevaluatedtheCantonesecapabilitiesoftwenty-
smallermodels,ispronetohallucinations,affect-
three major models, encompassing twelve series
ingitsscores,whilelongerresponsesofClaude-3.5
ofopen-sourceandclosed-sourcegeneralandCan-
boost its human evaluations despite lower auto-
toneseLLMs,acrossfourbenchmarks. TheLLMs
matic metrics (Table 3). GPT-4 and GPT-4o ex-
10https://huggingface.co/hon9kon9ize cel in mathematical logic, followed by Mixtral-
11https://huggingface.co/google-bert/
large-2,Llama-3.1-70b,andGLM-4. Conversely,
bert-base-multilingual-cased
Gemma-2-27b and ERNIE series underperform.
12https://huggingface.co/FacebookAI/
roberta-large Models like ChatGPT perform better in English,Models 0-shot(correct) 5-shot(correct)
(Yue-TruthfulQA)
Rouge-l Bleu-4 BERTScore Human Rouge-l Bleu-4 BERTScore Human
Qwen-1.5-110b 26.04 15.95 69.29 4.02 31.73 19.53 70.87 4.03
Qwen-2-7b 13.27 10.00 66.14 3.23 16.91 11.48 67.71 3.35
Qwen-2-72b 10.86 9.68 65.62 3.25 17.52 12.38 67.72 3.61
Mixtral-8x22b 14.74 10.83 66.72 3.40 20.40 14.09 68.05 3.85
Mixtral-large-2 19.72 13.01 69.06 3.96 31.38 18.61 72.07 4.20
Llama-3-8b 8.40 8.68 64.37 3.20 28.68 16.43 70.82 4.04
Llama-3-70b 10.98 9.51 66.10 3.87 33.06 19.31 71.95 4.15
Llama-3.1-8b 13.82 10.33 66.97 3.52 26.18 15.20 70.28 3.98
Llama-3.1-70b 21.03 14.30 68.31 4.05 34.72 20.54 70.80 4.10
Phi-3-medium 18.70 12.00 67.36 3.54 22.00 13.72 67.57 3.49
Gemma-2-27b 8.09 8.44 64.41 3.28 11.33 9.98 63.66 3.21
Yi-1.5-34b 15.41 11.11 67.57 3.60 20.30 13.20 69.50 3.92
Internlm-2.5-7b 14.46 10.53 63.48 3.10 22.30 14.08 67.61 3.67
ERNIE-Lite 20.58 12.23 67.64 3.44 20.69 12.27 68.45 3.62
ERNIE-Tiny 27.16 14.49 68.45 3.48 27.91 15.28 68.84 3.68
ERNIE-Speed 22.58 13.15 67.84 3.48 23.61 13.82 68.27 3.62
ERNIE-Turbo 17.91 11.30 66.71 3.40 21.19 12.19 68.29 3.60
Sensechat-5 24.75 15.11 68.43 3.72 32.45 19.70 70.02 3.96
Claude-3.5 14.23 9.95 67.56 3.92 12.66 10.06 68.12 4.07
GLM-4 13.44 10.07 67.26 3.74 23.57 14.28 70.30 4.08
ChatGPT 25.07 14.81 67.78 3.98 31.84 18.42 70.41 4.02
GPT-4o 17.58 12.17 68.68 3.98 27.64 16.52 71.59 4.05
GPT-4 19.47 13.45 68.99 4.10 28.43 16.74 71.26 4.20
Table3: ResultsofthecomparisonbetweentextsgeneratedbyvariousLLMsinYue-TruthfulQAbasedon0-shot
and5-shotsettingsandthecorrecttexts. Rouge-l,Bleu-4,andBERTScoreareevaluationmetricsforcomparing
textsimilarity,whileHumanreferstotheevaluationmetricsfromhumanevaluation(Section5.2).
indicatingchallengesinCantonesemathematical ModelslikeMixtral-large-2andLlama-3.1-70bof-
reasoningduetolanguagenuances(Table4). For fer cost savings and high-quality translations in
complex reasoning, GPT-4 consistently demon- bothsettings(Table18). TheLlamaseries,while
strates optimal performance, closely followed by notthehighestinoutputquality,providesthebest
Qwen-2-72b,Qwen-1.5-110b,andMixtral-large-2, speedandcost-effectivenessfortranslatingdatasets
eachofwhichalsoexhibitsexcellentperformance toCantonese.
(Table 5). For tasks across various topics of the
5.5 Casestudy
MMLU,Qwen-2-72bconsistentlyexhibitsthebest
performance(Table6). Wecompileatabledetail- Inadditiontotheresultsanalyzedabove,wefind
ingthebestmodelsforvariouspersonasalongwith thatGemma-2-27bfrequentlyencountershalluci-
recommendedopen-sourcemodels(Table7). nation issues, which impair its ability to handle
tasksrelatedtoCantonese(AppendixB).Although
Enhancetrainingdatavolumebyutilizingmod-
Qwen-2-72bexhibitsgoodperformance,itsome-
els that excel in translating Cantonese data.
timesoutputstrainingdata. Nonetheless,theQwen
High-quality Cantonese data is crucial for the
series of models remains proficient in handling
pre-training or alignment of Cantonese LLMs,
tasksrelatedtoCantonese(AppendixB).
with translations from Standard Chinese proving
moreeffectiveduetolinguisticsimilarities,asop-
6 Challengesandopportunities
posed to English (Table 16). While models like
Gemma-2-27bperformlesseffectivelyinEnglish- WeevaluatemainstreamCantonesecapabilitiesof
to-Cantonese translation, closed-source models LLMsandanalyzeindetail. Next,Idiscusscurrent
suchasSensechat-5andGPTseriesshowminimal Cantonese technical challenges and propose po-
quality difference between 0-shot and 5-shot set- tentialresearchopportunitiesbyintegratingthese
tings. PrioritizingtranslationsfromStandardChi- challengeswithouranalysis.
nese,thenEnglish,optimizesdataquality. Regard-
6.1 ExistingCantonesechallenges
ingcost-effectiveness,usingclosed-sourcemodels
likeSensechat-5-Cantonese,ChatGPT,andGPT-4o Colloquialism. Cantonese differs significantly
isadvisableifAPIcostsarenegligible(Table16). from Standard Chinese in its spoken vocabulary,Models Acc.(0-shot) Acc.(5-shot) Models Acc.(0-shot) Acc.(5-shot)
Qwen-1.5-110b 52.77 58.68 Qwen-1.5-110b 88.64 90.18
Qwen-2-7b 50.72 62.40 Qwen-2-7b 78.74 80.10
Qwen-2-72b 78.62 78.47 Qwen-2-72b 90.69 92.31
Mixtral-8x22b 63.08 66.41 Mixtral-8x22b 75.92 77.63
Mixtral-large-2 78.01 81.43 Mixtral-large-2 88.64 90.44
Llama-3-8b 52.16 49.81 Llama-3-8b 67.64 52.69
Llama-3-70b 71.04 75.97 Llama-3-70b 84.46 84.97
Llama-3.1-8b 63.91 61.56 Llama-3.1-8b 69.00 67.21
Llama-3.1-70b 51.93 79.15 Llama-3.1-70b 88.90 88.39
Phi-3-medium 56.79 63.31 Phi-3-medium 63.11 78.14
Gemma-2-27b 9.40 3.64 Gemma-2-27b 67.55 55.08
Yi-1.5-34b 67.63 69.75 Yi-1.5-34b 84.71 86.68
Internlm-2.5-7b 55.72 43.90 ERNIE-turbo 38.51 44.24
ERNIE-turbo 13.80 10.54 ERNIE-Speed 74.04 46.88
ERNIE-Speed 26.69 26.99 ERNIE-Lite 72.42 77.28
ERNIE-Lite 52.84 31.46 ERNIE-Tiny 33.56 30.15
ERNIE-Tiny 3.72 4.55 Internlm-2.5-7b 80.79 79.85
SenseChat-5 77.48 73.16 SenseChat-5 87.96 87.28
Claude-3.5 66.11 68.31 Claude-3.5 91.63 92.23
GLM-4 76.42 77.10 GLM-4 88.90 88.64
ChatGPT 22.14 41.09 ChatGPT 69.60 70.79
GPT-4o 81.80 83.47 GPT-4o 92.06 94.28
GPT-4 79.23 83.25 GPT-4 92.74 92.06
Table 4: Results of the comparison between answer Table 5: Results of the comparison between answer
generatedbyvariousLLMsinYue-GSM8Kbasedon generated by various LLMs in Yue-ARC-C based on
0-shotand5-shotsettingsandgroundtruth. 0-shotand5-shotsettingsandgroundtruth.
posinguniquechallengesforNLPmodelsinitially nificantpresenceofEnglish(Yue-Hashimoto,1991;
trained on Mandarin (Snow, 2004; Xiang et al., Li, 2006). Highlighting the multilingualism, ex-
2024). Thesedifferencesareparticularlyevidentin amplesincludeCantonesesentencesincorporating
informalsettingssuchasspeechtranscriptionand English terms, such as “deadline” seamlessly in-
onlineforumslikeLinkg,andOpenrice. Although tegrated as in “Gan2 M4 Cit3 deadline” (strug-
smallercomparedtodatasetsforEnglishandStan- gling to meet the deadline), and the use of the
dardChinesemodelslikeBERTweet(Nguyenetal., Japanese loanword “Kawaii” (cute), pronounced
2020)andMacBERT(Cuietal.,2021),theseplat- and adapted locally in phrases like “Ni1 Gin6
forms still provide a substantial text corpus for Saam1 Hou2 kawaii” (This shirt is very cute).
training Cantonese-specific models (Hale, 2001, ThesefindingsemphasizetheneedforCantonese
2016). The abundant unique expressions and NLPsystemstoeffectivelymanagesuchmultilin-
slanginCantonese,oftenembeddedwithcomplex gual code-switching, and suggest integrating ad-
cultural nuances, hinder adaptation of Standard vancedfeatureslikespellingcorrectionanddialect
Chinese-basedmodelstoCantonese. Forexample, identificationtorefinedataprocessing.
“Wan2Sik6”literallymeans“lookingforfood”,but
6.2 Opportunities
it is commonly used to describe seeking employ-
mentorearningmoney, carryingconnotationsof Given the existing challenges in Cantonese lan-
survivalandmakingalivinginCantonese. Inad- guage and the evaluation results on benchmarks,
dition,commonspellingmistakesandnovelmean- weproposethefollowingpotentialresearchdirec-
ingsinCantonesefurthercomplicatemodeltrain- tionsandrecommendedmodels.
ing, emphasizing the need for robust, Cantonese-
Dataaugmentation. Dataaugmentationinlow-
specific vocabularies and corpora to capture the
resource scenarios includes label-invariant meth-
full breadth of colloquialisms and idioms of the
ods,whichmodifytextwithoutalteringlabels(Wei
language(LiandCosta,2009).
andZou,2019;Minetal.,2020;Shietal.,2021),
Multilingualism. Toelucidatethemultilingual andlabel-variantmethodsthatchangesemanticat-
dynamics in social media of Hong Kong, (Xiang tributes to create new instances (Jin et al., 2019;
et al., 2024) identify frequent code-switching be- Daietal.,2019). Techniqueslikesupervisedcon-
tweenCantoneseandStandardChinese,andasig- trastivelearningoptimizeneuralrepresentationsforModels 0-shot(correct) 5-shot(correct)
(Yue-MMLU)
STEM Hum. S.S. C.S. Oth. STEM Hum. S.S. C.S. Oth.
Qwen-1.5-110b 80.41 88.62 82.26 83.91 75.77 86.14 91.3 90.59 90.62 82.76
Qwen-2-7b 69.58 80.7 76.34 79.76 69.94 74.14 81.02 79.82 81.28 75.19
Qwen-2-72b 80.21 88.46 87.00 87.45 80.16 87.36 91.86 89.68 91.98 87.44
Mixtral-8x22b 43.68 56.96 48.4 59.0 50.52 50.88 59.78 57.84 62.79 58.82
Mixtral-large-2 60.19 76.08 70.74 74.92 60.38 63.84 79.65 71.66 78.84 68.5
Llama-3-8b 47.69 59.16 53.72 56.6 49.42 46.24 58.33 52.55 53.94 43.1
Llama-3-70b 58.33 73.04 71.92 74.86 63.89 57.34 72.79 72.95 73.07 63.65
Llama-3.1-8b 44.86 58.27 53.7 56.08 45.96 46.01 58.06 54.02 58.31 53.16
Llama-3.1-70b 60.96 76.43 73.38 76.93 67.04 64.0 78.13 74.9 78.14 71.82
Phi-3-medium 45.65 61.53 51.14 58.13 44.86 45.65 59.24 53.02 59.31 49.18
Gemma-2-27b 37.68 53.94 49.2 53.46 47.5 33.55 40.98 44.88 43.75 40.72
Yi-1.5-34b 70.73 81.46 79.57 81.54 68.47 78.2 85.15 80.49 83.52 74.13
Internlm-2.5-7b 66.93 78.74 73.38 73.42 63.64 70.47 80.84 75.19 76.79 64.63
ERNIE-Lite 60.73 67.56 61.02 67.73 53.04 62.43 70.27 64.84 71.55 60.04
ERNIE-Tiny 33.24 37.86 32.3 37.88 34.36 32.68 38.79 34.6 37.66 32.52
ERNIE-turbo 50.7 54.62 45.62 53.53 41.82 49.33 57.66 46.76 54.28 41.42
Sensechat-5 73.86 83.21 76.95 80.73 69.56 73.52 82.0 74.78 79.88 68.57
Claude-3.5 60.6 72.67 75.98 76.63 64.6 59.02 81.24 82.54 83.08 75.51
GLM-4 75.66 84.39 75.75 80.17 64.23 76.0 84.2 78.06 80.07 71.77
ChatGPT 44.58 57.72 52.42 58.74 49.78 41.52 56.34 54.54 60.33 59.87
GPT-4o 71.72 83.28 84.32 84.12 74.16 72.87 85.03 81.59 84.32 71.53
GPT-4 60.28 75.38 74.46 77.26 67.28 63.5 76.75 74.57 77.56 70.78
Table6: ResultsofthecomparisonbetweentextsgeneratedbyvariousLLMsinYue-MMLUbasedon0-shotand
5-shotsettingsandthecorrecttexts. Hum.,S.S.,C.S.,Oth. areforHumanities,Socialsciences,Chinaspecific,
Other,respectively.
Tasks Best(5-shot) Recommendation Table18,Llamaseriesmodelsprovidefasterinfer-
encespeedsand,despitenotdeliveringthehighest
Factualgen. Mixtral-large-2 Qwen,Llama
Mathlogic GPT-4o Qwen,Mixtral,Llama quality,arethemostcost-effectiveduetospeed.
C-Reasoning GPT-4o Qwen,Mixtral
Socialsciences Qwen-2-72b Qwen,Llama,Yi Code-switch. Developments in LLMs suggest
STEM Qwen-1.5-110b Qwen,Yi,Internlm
emergent abilities for untrained tasks, although
Humanities Qwen-1.5-110b Qwen,Yi,Internlm
Chinaspecific Qwen-2-72b Qwen,Yi,Llama effectiveness varies across scripts and lan-
Other Qwen-2-72b Qwen,Yi guages (Mann et al., 2020; Bang et al., 2023).
Trans.(zh-yue) Sensechat-5 Mixtral,Llama,Yi
Research in Standard Chinese-adapted LLMs is
Trans.(en-yue) GPT-4 Mixtral,Llama,Yi
progressing, benefiting Cantonese NLP in the fu-
Table7: Theoptimalmodelsforvarioustasksandrec- ture(Cuietal.,2023;Baietal.,2023). Wepropose
ommendedopen-sourcemodels(performanceandtime). fourbenchmarksandhavecompiledaYue-TRANS
Specificrequirementsshouldbeconsideredcomprehen-
dataset, each involving two or more languages.
sivelyinconjunctionwithevaluationmetrics.
Therefore,basedontheperformanceobservedon
thesedatasets,werecommendusingmodelsfrom
theQwen,Llama,Mixtral,andYiseries(Table7).
specifictasks(Sedghamizetal.,2021),and(Ding
etal.,2024)revieweffectiveLLM-baseddataaug- Large language model. Based on the analysis
mentation strategies. Translating between Stan- above,wecompileTable7,whichpresentstherec-
dard Chinese and English is another useful aug- ommendationLLMsintermsofperformanceand
mentationmethod. Werecommend(Table16,Ta- time, and suggests LLM series for various tasks.
ble18)usinghigh-capabilityclosed-sourcemodels For work relatedto LLMs, we recommend using
like Sensechat-5, ChatGPT, GPT-4o, and GPT-4 models from the Qwen, Mixtral, Llama, and Yi
for dataset conversion to Cantonese if API costs series. Fortasksthatinvolveonlyprompting,with-
are not a concern. For budget-conscious, LLMs out the need for LLM training, we also recom-
likeMixtral-large-2andLlama-3.1-70boffercost- mendusingclosed-sourcemodelssuchasGPTand
effectiveyetqualitytranslations. Othermodelslike Sensechat series models. Specific requirements
Qwen-1.5-110b,Llama-3.1-8b,andYi-1.5-34bper- shouldbeconsideredcomprehensivelyinconjunc-
formwellin5-shottranslationtasks. Accordingto tionwithevaluationmetrics.Limitations JianChen,YuLiu,GuangyiZhang,YiCai,TaoWang,
andHuaqingMin.2013. Sentimentanalysisforcan-
The paper faces two limitations. The first limita- toneseopinionmining. In2013FourthInternational
tion is the scarcity of work related to Cantonese ConferenceonEmergingIntelligentDataandWeb
Technologies,pages496–500.IEEE.
LLMs,whichrestrictstheextentofsummarizing
relevantstudies. However,itisbelievedthatwith XinyuChen,YifeiJian,LiangKe,YunxiangQiu,Xing-
thepublicationofthispaper,anincreasingnumber shu Chen, Yunya Song, and Haizhou Wang. 2024.
A deep semantic-aware approach for cantonese ru-
ofprojectsinvolvinglarge-scaleCantonesemodels
mordetectioninsocialnetworkswithgraphconvo-
willbeproposed. Thesecondlimitationisthatthe
lutionalnetwork. ExpertSystemswithApplications,
recommended LLMs presented in the article are 245:123007.
for reference only; LLMs not recommended are
XinyuChen,LiangKe,ZhipengLu,HanjianSu,and
notnecessarilyofinferiorquality, nordoesitim-
HaizhouWang.2020. Anovelhybridmodelforcan-
plytheyareunsuitableforCantonese-relatedtasks. toneserumordetectionontwitter. AppliedSciences,
The selection of specific models for Cantonese- 10(20):7093.
relatedtasksshouldbebasedonadetailedanalysis
Kevin Clark, Minh-Thang Luong, Quoc V Le, and
ofthespecificissuesathand. ChristopherDManning.2020. Electra: Pre-training
textencodersasdiscriminatorsratherthangenerators.
EthicsStatement arXivpreprintarXiv:2003.10555.
OpenCompass Contributors. 2023. Opencompass:
Concerningthedataannotatorsandtheevaluation
A universal evaluation platform for foundation
ofdatareview,weensuretheselectionofqualified models. https://github.com/open-compass/
tri-lingualindividualsfromHongKongandGuang- opencompass.
dongwhoarecompensatedwithreasonablehourly
YimingCui,WanxiangChe,TingLiu,BingQin,and
wagesorotherformsofsubsidiesasrewards. We
Ziqing Yang. 2021. Pre-training with whole word
have already obtained approval for this research masking for chinese bert. IEEE/ACM Transac-
fromtheEthicsReviewCommittee. tionsonAudio, Speech, andLanguageProcessing,
29:3504–3514.
YimingCui,ZiqingYang,andXinYao.2023. Efficient
References and effective text encoding for chinese llama and
alpaca. arXivpreprintarXiv:2304.08177.
Alham Fikri Aji, Genta Indra Winata, Fajri Koto,
SamuelCahyawijaya,AdeRomadhony,RahmadMa- Ning Dai, Jianze Liang, Xipeng Qiu, and Xuanjing
hendra, Kemal Kurniawan, David Moeljadi, Radi- Huang.2019. Styletransformer: Unpairedtextstyle
tyo Eko Prasojo, Timothy Baldwin, Jey Han Lau, transfer without disentangled latent representation.
andSebastianRuder.2022. Onecountry,700+lan- arXivpreprintarXiv:1905.05621.
guages: NLP challenges for underrepresented lan-
guages and dialects in Indonesia. In Proceedings MeganDare,ValentinaFajardoDiaz,AverieHoZoen
of the 60th Annual Meeting of the Association for So,YifanWang,andShibingfengZhang.2023. Un-
ComputationalLinguistics(Volume1: LongPapers), supervisedmandarin-cantonesemachinetranslation.
pages7226–7249,Dublin,Ireland.Associationfor arXivpreprintarXiv:2301.03971.
ComputationalLinguistics.
BoshengDing,ChengweiQin,RuochenZhao,Tianze
Luo, Xinze Li, Guizhen Chen, Wenhan Xia, Jun-
JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,
jie Hu, Anh Tuan Luu, and Shafiq Joty. 2024.
XiaodongDeng,YangFan,WenbinGe,YuHan,Fei
Data augmentation using llms: Data perspectives,
Huang, et al. 2023. Qwen technical report. arXiv
learningparadigmsandchallenges. arXivpreprint
preprintarXiv:2309.16609.
arXiv:2403.02990.
YejinBang,SamuelCahyawijaya,NayeonLee,Wen- ZiruFu,YuChengHsu,ChristianSChan,ChaakMing
liangDai,DanSu,BryanWilie,HolyLovenia,Ziwei Lau,JoyceLiu,andPaulSiuFaiYip.2024. Efficacy
Ji,TiezhengYu,WillyChung,etal.2023. Amulti- ofchatgptincantonesesentimentanalysis: Compar-
task,multilingual,multimodalevaluationofchatgpt ative study. Journal of Medical Internet Research,
onreasoning,hallucination,andinteractivity. arXiv 26:e51069.
preprintarXiv:2302.04023.
Yin-ChunFung,Lap-KeiLee,Tsz-ChunCheng,Chak-
Jian Chen, Dong Ping Huang, Shuyue Hu, Yu Liu, FungLi,VincentChun-KiuWong,andNga-InWu.
YiCai,andHuaqingMin.2015. Anopinionmining 2023. Canchat: Acantoneseempatheticchatbotfor
frameworkforcantonesereviews. JournalofAmbi- secondary school student counseling. In 2023 In-
entIntelligenceandHumanizedComputing,6:541– ternationalSymposiumonEducationalTechnology
547. (ISET),pages170–175.John Hale. 2001. A probabilistic earley parser as a JohnSYLee.2011. Towardaparallelcorpusofspoken
psycholinguistic model. In Second meeting of the cantoneseandwrittenchinese. InProceedingsof5th
northamericanchapteroftheassociationforcom- InternationalJointConferenceonNaturalLanguage
putationallinguistics. Processing,pages1462–1466.
John Hale. 2016. Information-theoretical complex- JohnSYLee,BaikunLiang,andHaleyHMFong.2021.
ity metrics. Language and Linguistics Compass, Restatementandquestiongenerationforcounsellor
10(9):397–412. chatbot. In1stWorkshoponNaturalLanguagePro-
cessing for Programming (NLP4Prog), pages 1–7.
Junjie Hu, Sebastian Ruder, Aditya Siddhant, Gra- AssociationforComputationalLinguistics(ACL).
ham Neubig, Orhan Firat, and Melvin Johnson.
2020. Xtreme: Amassivelymultilingualmulti-task Man-Tak Leung and Sam-Po Law. 2001. Hkcac: the
benchmark for evaluating cross-lingual generalisa- hongkongcantoneseadultlanguagecorpus. Interna-
tion. InInternationalConferenceonMachineLearn- tionaljournalofcorpuslinguistics,6(2):305–325.
ing,pages4411–4421.PMLR.
DavidCSLiandVirginiaCosta.2009. Punninginhong
GuangpuHuang,ArseniyGorin,Jean-LucGauvain,and kongchinesemedia: Formsandfunctions. Journal
LoriLamel.2016. Machinetranslationbaseddata ofChineseLinguistics,37(1):77–107.
augmentation for cantonese keyword spotting. In
QingxinLi.2006. Maritimesilkroad. Intercontinental
2016 IEEE International Conference on Acoustics,
Press.
SpeechandSignalProcessing(ICASSP),pages6020–
6024.IEEE.
QintongLi, PijiLi, ZhaochunRen, PengjieRen, and
Zhumin Chen. 2022. Knowledge bridging for em-
Thomas Hun-tak Lee. 1999. Cancorp-the hong kong
patheticdialoguegeneration. InProceedingsofthe
cantonesechildlanguagecorpus. RevueFrançaise
AAAIconferenceonartificialintelligence,volume36,
deLinguistiqueAppliquée,4(1):21–30.
pages10993–11001.
JiyueJiang,ShengWang,QintongLi,LingpengKong,
Chin-YewLin.2004. Rouge: Apackageforautomatic
andChuanWu.2023. Acognitivestimulationdia-
evaluation of summaries. In Text summarization
loguesystemwithmulti-sourceknowledgefusionfor
branchesout,pages74–81.
elders with cognitive impairment. In Proceedings
of the 61st Annual Meeting of the Association for
Evelyn Kai-Yan Liu. 2022. Low-resource neural ma-
ComputationalLinguistics(Volume1: LongPapers),
chinetranslation: Acasestudyofcantonese. InPro-
pages10628–10640,Toronto,Canada.Association
ceedingsoftheNinthWorkshoponNLPforSimilar
forComputationalLinguistics.
Languages,VarietiesandDialects,pages28–40.
ZhijingJin,DiJin,JonasMueller,NicholasMatthews,
Yang Liu and Mirella Lapata. 2019. Text summa-
andEnricoSantus.2019. IMaT:Unsupervisedtext
rization with pretrained encoders. arXiv preprint
attribute transfer via iterative matching and trans-
arXiv:1908.08345.
lation. In Proceedings of the 2019 Conference on
EmpiricalMethodsinNaturalLanguageProcessing
Ben Mann, N Ryder, M Subbiah, J Kaplan, P Dhari-
andthe9thInternationalJointConferenceonNatu-
wal, ANeelakantan, PShyam, GSastry, AAskell,
ralLanguageProcessing(EMNLP-IJCNLP),pages
SAgarwal, etal.2020. Languagemodelsarefew-
3097–3109,HongKong,China.AssociationforCom-
shotlearners. arXivpreprintarXiv:2005.14165,1.
putationalLinguistics.
JunghyunMin,RThomasMcCoy,DipanjanDas,Emily
PratikJoshi, SebastinSanty, AmarBudhiraja, Kalika Pitler,andTalLinzen.2020. Syntacticdataaugmen-
Bali,andMonojitChoudhury.2020. Thestateand tation increases robustness to inference heuristics.
fate of linguistic diversity and inclusion in the nlp arXivpreprintarXiv:2004.11999.
world. arXivpreprintarXiv:2004.09095.
Eric WT Ngai, Maggie CM Lee, YS Choi, and PYF
JacksonLee,LitongChen,CharlesLam,ChaakMing Chai.2018. Multiple-domainsentimentclassification
Lau, and Tsz-Him Tsui. 2022. Pycantonese: Can- forcantoneseusingacombinedapproach. InPACIS,
toneselinguisticsandnlpinpython. InProceedings page297.
ofthethirteenthlanguageresourcesandevaluation
conference,pages6607–6611. DatQuocNguyen,ThanhVu,andAnhTuanNguyen.
2020. Bertweet: Apre-trainedlanguagemodelfor
JohnLee.2019. Anemotiondetectionsystemforcan- englishtweets. arXivpreprintarXiv:2005.10200.
tonese. In The Thirty-Second International Flairs
Conference. KishorePapineni,SalimRoukos,ToddWard,andWei-
JingZhu.2002. Bleu: amethodforautomaticevalu-
JohnLeeandBaikunLiang.2021. Responseselection ationofmachinetranslation. InProceedingsofthe
foravirtualcounsellor. InCompanionProceedings 40thannualmeetingoftheAssociationforComputa-
oftheWebConference2021,pages495–499. tionalLinguistics,pages311–318.Wong Ping-Wai. 2006. The specification of pos tag- RongXiang,HanzhuoTan,JingLi,MingyuWan,and
gingofthehongkonguniversitycantonesecorpus. Kam-Fai Wong. 2022. When cantonese nlp meets
InternationalJournalofTechnologyandHumanIn- pre-training: progress and challenges. In Proceed-
teraction(IJTHI),2(1):21–38. ingsofthe2ndConferenceoftheAsia-PacificChap-
teroftheAssociationforComputationalLinguistics
Hooman Sedghamiz, Shivam Raval, Enrico Santus, andthe12thInternationalJointConferenceonNatu-
Tuka Alhanai, and Mohammad Ghassemi. 2021. ralLanguageProcessing: TutorialAbstracts,pages
SupCL-Seq: Supervised Contrastive Learning for 16–21.
downstreamoptimizedsequencerepresentations. In
FindingsoftheAssociationforComputationalLin-
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-
guistics: EMNLP 2021, pages 3398–3403, Punta
bonell,RussRSalakhutdinov,andQuocVLe.2019.
Cana,DominicanRepublic.AssociationforCompu-
Xlnet: Generalizedautoregressivepretrainingforlan-
tationalLinguistics.
guageunderstanding. Advancesinneuralinforma-
tionprocessingsystems,32.
HaoyueShi,KarenLivescu,andKevinGimpel.2021.
Substructuresubstitution: Structureddataaugmenta-
VirginiaYipandStephenMatthews.2007. Thebilin-
tionfornlp. arXivpreprintarXiv:2101.00411.
gualchild: Earlydevelopmentandlanguagecontact.
DonSnow.2004. Cantoneseaswrittenlanguage: The CambridgeUniversityPress.
growth of a written Chinese vernacular, volume 1.
HongKongUniversityPress. AnneYue-Hashimoto.1991. Theyuedialect. Journal
ofChineseLinguisticsMonographSeries,(3):292–
XuTan,MuniZhuang,XinLu,andTaitianMao.2021.
322.
Ananalysisoftheemotionalevolutionoflarge-scale
internetpublicopinioneventsbasedonthebert-lda
TianyiZhang*,VarshaKishore*,FelixWu*,KilianQ.
hybridmodel. IEEEAccess,9:15860–15871.
Weinberger,andYoavArtzi.2020. Bertscore: Eval-
uating text generation with bert. In International
YidaWang,PeiKe,YinheZheng,KailiHuang,Yong
ConferenceonLearningRepresentations.
Jiang, Xiaoyan Zhu, and Minlie Huang. 2020. A
large-scale chinese short-text conversation dataset.
InNLPCC. Xiaoheng Zhang. 1998. Dialect mt: a case study be-
tween cantonese and mandarin. In COLING 1998
JasonWeiandKaiZou.2019. EDA:Easydataaugmen- Volume 2: The 17th International Conference on
tationtechniquesforboostingperformanceontext ComputationalLinguistics.
classificationtasks. InProceedingsofthe2019Con-
ferenceonEmpiricalMethodsinNaturalLanguage Ziqiong Zhang, Qiang Ye, Zili Zhang, and Yijun Li.
Processing and the 9th International Joint Confer- 2011. Sentimentclassificationofinternetrestaurant
ence on Natural Language Processing (EMNLP- reviewswrittenincantonese. ExpertSystemswith
IJCNLP),pages6382–6388,HongKong,China.As- Applications,38(6):7674–7682.
sociationforComputationalLinguistics.
A Appendix
Tak-sumWong,JohnLee,etal.2018. Register-sensitive
translation: acasestudyofmandarinandcantonese.
InAssociationforMachineTranslationintheAmeri- A.1 Cantonesespeakingpopulationstatistics
cas.
DekaiWu.1994. Aligningaparallelenglish-chinese Country/region Pop. Stat. Time
corpus statistically with lexical criteria. arXiv
preprintcmp-lg/9406007. HongKong 6,529,000 2021
Macau 506,000 2016
Yan Wu, Xiukun Li, and Suen Caesar Lun. 2006. A
Guangdong 40,000,000 2021
structural-basedapproachtocantonese-englishma-
chinetranslation. InInternationalJournalofCompu- Guangxi 12,000,000 2022
tationalLinguistics&ChineseLanguageProcessing, Brunei 6,350 2006
Volume11,Number2,June2006,pages137–158.
Indonesia 180,000 1982
RongXiang,EmmanueleChersoni,YixiaLi,JingLi, Malaysia 1,070,000 2000
Chu-Ren Huang, Yushan Pan, and Yushi Li. 2024. Philippines 9,780 2000
Cantonesenaturallanguageprocessinginthetrans-
Singapore 338,000 1993
formersera: asurveyandcurrentchallenges. Lan-
Thailand 29,400 1984
guageResourcesandEvaluation,pages1–27.
Vietnam 862,000 1999
RongXiang,YingJiao,andQinLu.2019. Sentiment
augmented attention network for cantonese restau-
Table8: Cantonesespeakingpopulationstatistics. Pop.
rantreviewanalysis. InProceedingsofWISDOM’19:
ispopulation. Stat. Timeisstatisticaltime
WorkshoponIssuesofSentimentDiscoveryandOpin-
ionMining(WISDOM’19),page9.A.2 Evaluationtools A.4 SourceofevaluationLLMs
• Rouge-l: fromrouge_metricimportPyRouge ThissectioncoverstheevaluationofLLMsalong
withthecorrespondingHuggingFacelinksandthe
• Bleu-4: fromnltk.translate.bleu_scoreimport
namesoftheAPIs.
sentence_bleu,SmoothingFunction
A.5 Experimentalresults
• BERTScore: bert-base-multilingual-cased&
A.5.1 CantoneseandEnglishTruthfulQA
roberta-large
(bestandincorrect)
A.3 Yue-MMLU Table 10 (comparison between best answer and
groundtruth) and Table 11 (comparison between
We select twenty-two topics from CMMLU that
incorrectanswerandgroundtruth)aretheexperi-
covermostofthethemesinCMMLUtoserveas
mentalresultsbasedontheCantoneseandEnglish
thetopicsforYue-MMLU,whichareasfollows:
versionofTruthfulQA.
• chinese_civil_service_exam
A.5.2 EnglishTruthfulQA(correct)
• arts Table12(comparisionvetweencorrectanswerand
groundtruth)istheexperimentalresultbasedonthe
• electrical_engineering
EnglishversionofTruthfulQA,intendedforcom-
parisonwiththeCantoneseversionofTruthfulQA.
• chinese_literature
Formoreresults,pleaserefertothepubliclyavail-
ableevaluationplatform13.
• education
A.5.3 EnglishGSM8K
• economics
Table 13 is the experimental result based on the
• ethnology EnglishversionofGSM8K,intendedforcompar-
ison with the Cantonese version of GSM8K. For
• college_medicine
moreresults,pleaserefertothepubliclyavailable
evaluationplatform14.
• journalism
A.5.4 EnglishARCchallenge
• management
Table 14 is the experimental result based on the
• marketing English version of ARC Challenge, intended for
comparison with the Cantonese version of ARC
• philosophy Challenge. For more results, please refer to the
publiclyavailableevaluationplatform15.
• security_study
A.5.5 CMMLU
• sociology
Table 15 is the experimental result based on the
StandardChineseversionofMMLU,intendedfor
• world_history
comparisonwiththeCantoneseversionofMMLU.
• world_religions Formoreresults,pleaserefertothepubliclyavail-
ableevaluationplatform16.
• high_school_geography
A.5.6 Translation
• machine_learning
Table 16 is the experimental result based on the
Yue-Transdatasets. Table17andTable18reflect
• marxist_theory
therunningtimeofdifferentLLMsonthetransla-
tiondataset.
• professional_psychology
13https://huggingface.co/open-llm-leaderboard
• sports_science 14https://huggingface.co/open-llm-leaderboard
15https://huggingface.co/open-llm-leaderboard
• logical 16https://huggingface.co/open-llm-leaderboardA.6 Prompttemplatesformultilingual
evaluation Answer the following question in
English:
This section details the prompt templates used (cid:44)→
Question: [TARGET_QUESTION]
fortheCantonese,English,andStandardChinese
Answer:
datasets tested in our experiments. Each dataset
wasevaluatedunderboth0-shotand5-shotsettings.
A.6.3 Yue-GSM8Kprompt
Forthe5-shotsetting,exceptforthetranslationtask
0-shot:
(Yue-TRANS),thepromptsweregeneratedusinga
slidingwindowapproach,wheretheprecedingfive 請逐步思考，最終答案前用「####」標
examplesfromthedataset(Yue-TruthfulQA,Yue- 記。用粵語答下面問題：
(cid:44)→
GSM8K,Yue-ARC-Challenge,andYue-MMLU) 問題：[QUESTION]
wereutilizedascontextforeachnewexample. For 用粵語回答問題：
theYue-TRANStranslationtask,theBM25algo-
5-shot:
rithmwasemployedtoidentifyandselectthefive
mostsimilarexamplestoserveasfew-shotexam- 樣例1-5：
ples. Below,weoutlinethepromptstructuresand 問題：[EXAMPLE_QUESTION]
methodologies used for generating the few-shot 回應：[EXAMPLE_ANSWER]
examples.
The inference and evaluation processes in this 請逐步思考，最終答案前用「####」標
study were facilitated by the OpenCompass plat- 記。用粵語答下面問題：
(cid:44)→
form,whichprovidedarobustanduniversalevalu- 問題：[TARGET_QUESTION]
ationframeworkforfoundationmodels(Contribu- 用粵語回答問題：
tors,2023).
A.6.4 En-GSM8Kprompt
A.6.1 Yue-TruthfulQAprompt
0-shot:
0-shot:
Please think step by step, mark the
用粵語答下面問題： final answer with '####'.
(cid:44)→
問題：[QUESTION] Answer the following question in
回應： English:
(cid:44)→
Question: [QUESTION]
5-shot:
Answer the question in English:
樣例1-5：
5-shot:
問題：[EXAMPLE_QUESTION]
回應：[EXAMPLE_ANSWER] Example 1-5:
Question: [EXAMPLE_QUESTION]
用粵語答下面問題： Response: [EXAMPLE_ANSWER]
問題：[TARGET_QUESTION]
回應： Please think step by step, mark the
final answer with '####'.
(cid:44)→
A.6.2 En-TruthfulQAprompt Answer the following question in
0-shot: English:
(cid:44)→
Question: [TARGET_QUESTION]
Answer the following question in
Answer the question in English:
English:
(cid:44)→
Question: [QUESTION] A.6.5 Yue-ARC-Cprompt
Answer:
0-shot:
5-shot: 問題：[QUESTION]
由提供選項中直接用選項字母作答。
Example 1-5:
回應：
Question: [EXAMPLE_QUESTION]
Answer: [EXAMPLE_ANSWER] 5-shot:樣例1-5： 样例1-5:
問題：[EXAMPLE_QUESTION] 题目：[EXAMPLE_QUESTION]
回應：[EXAMPLE_ANSWER] 答案：[EXAMPLE_ANSWER]
問題：[TARGET_QUESTION] 以下是关于[SUBJECT]的单项选择题，请
由提供選項中直接用選項字母作答。 直接给出正确答案的选项。
(cid:44)→
回應： 题目：[TARGET_QUESTION]
答案：
A.6.6 En-ARC-Cprompt
0-shot: A.6.9 Yue-TRANSprompt
0-shot:
Question: [QUESTION]
Answer with the option's letter from 請將下面呢句/段話直接翻譯成粵
the given choices directly. 語：[SOURCE_TEXT]
(cid:44)→ (cid:44)→
Answer:
5-shot:
5-shot:
樣例1-5：
Example 1-5: 請將下面呢句/段話直接翻譯成粵
Question: [EXAMPLE_QUESTION] 語：[EXAMPLE_SOURCE_TEXT]
(cid:44)→
Answer: [EXAMPLE_ANSWER] 翻譯：[EXAMPLE_TRANSLATION_TEXT]
Question: [TARGET_QUESTION] 根據上面例子，請將下面呢句/段話直接
Answer with the option's letter from 翻譯成粵語：
(cid:44)→
the given choices directly. [TARGET_SOURCE_TEXT]
(cid:44)→
Answer:
B Casestudy
A.6.7 Yue-MMLUprompt
Inthissection,weprovideacasestudytoillustrate
0-shot:
theinputandoutputofourexperiment. Wedemon-
以下係關於[SUBJECT]單項選擇題，請直 stratethemodel’sbehaviorusingexampleprompts
接畀出正確答案選項。 andtheircorrespondingoutputs.
(cid:44)→
問題：[QUESTION]
答案：
5-shot:
樣例1-5：
問題：[EXAMPLE_QUESTION]
回應：[EXAMPLE_ANSWER]
以下係關於[SUBJECT]單項選擇題，請直
接畀出正確答案選項。
(cid:44)→
問題：[TARGET_QUESTION]
答案：
A.6.8 Zh-CMMLUprompt
0-shot:
以下是关于[SUBJECT]的单项选择题，请
直接给出正确答案的选项。
(cid:44)→
题目：[QUESTION]
答案：
5-shot:B.1 Yue-TruthfulQA
Figure3: Yue-TruthfulQAQwen-1.5-110bFigure4: Yue-TruthfulQAGemma-2-27b-itB.2 Yue-GSM8K
Figure5: Yue-GSM8KGPT-4oFigure6: Yue-GSM8KGemma-2-27b-itB.3 Yue-TRANS
Figure7: Yue-TRANSGPT-4oFigure8: Yue-TRANSQwen-2-72b-InstructB.4 Yue-ARC-C
Figure9: Yue-ARC-CClaude-3.5Figure10: Yue-ARC-CERNIE-Tiny-8kB.5 Yue-MMLU
Figure11: Yue-MMLUQwen-2-72b-InstructFigure12: Yue-MMLUMixtral-8x22b-InstructModels Mode Huggingfacelink&APIname
Qwen-1.5-110b Huggingface https://huggingface.co/Qwen/Qwen1.5-110B
Qwen-2-7b Huggingface https://huggingface.co/Qwen/Qwen2-7B-Instruct
Qwen-2-72b Huggingface https://huggingface.co/Qwen/Qwen2-72B-Instruct
Mixtral-8x22b Huggingface https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1
Mixtral-large-2 Huggingface https://huggingface.co/mistralai/Mistral-Large-Instruct-2407
Llama-3-8b Huggingface https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct
Llama-3-70b Huggingface https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct
Llama-3.1-8b Huggingface https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct
Llama-3.1-70b Huggingface https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct
Phi-3-medium Huggingface https://huggingface.co/microsoft/Phi-3-medium-128k-instruct
Gemma-2-27b Huggingface https://huggingface.co/google/gemma-2-27b-it
Yi-1.5-34b Huggingface https://huggingface.co/01-ai/Yi-1.5-34B-Chat
ERNIE-turbo API API:ERNIE-Bot-turbo
ERNIE-Speed API API:ERNIE-Speed-128K
ERNIE-Lite API API:ERNIE-Lite-8K
ERNIE-Tiny API API:ERNIE-Tiny-8K
Internlm-2.5-7b Huggingface https://huggingface.co/internlm/internlm2_5-7b-chat
SenseChat-5 API API:SenseChat-5-Cantonese
Claude-3.5 API API:claude-3.5-sonnot-20240620
GLM-4 API API:GLM-4-0520
ChatGPT API API:gpt-3.5-turbo-instruct&gpt-3.5-turbo
GPT-4o API API:gpt-4o
GPT-4 API API:gpt-4-0125-preview
Table9: ThemodeoftheevaluationLLMsandtheircorrespondinghuggingfacelinks&namesofAPIs.Models 0-shot(best) 5-shot(best)
(Yue-TruthfulQA)
Bleu-4 BERTScore Bleu-4 BERTScore
Qwen-1.5-110b 11.17 69.14 14.22 73.40
Qwen-2-7b 8.00 64.11 9.09 66.41
Qwen2-72b 7.77 62.22 9.99 65.32
Mixtral-8x22b 8.54 64.63 11.31 67.43
Mixtral-large-2 10.01 67.37 14.14 73.41
Llama-3-8b 7.26 60.79 12.94 71.77
Llama-3-70b 7.70 63.08 14.68 73.97
Llama-3.1-8b 8.19 63.97 11.93 70.64
Llama-3.1-70b 10.42 67.19 15.36 75.80
Phi-3-medium 9.34 65.84 10.98 66.81
Gemma-2-27b 7.15 60.94 8.14 61.54
Yi-1.5-34b 8.80 65.25 10.55 67.88
Internlm-2.5-7b 6.79 60.35 8.41 65.13
ERNIE-Lite 9.05 67.61 9.44 67.68
ERNIE-Tiny 14.49 70.05 10.82 70.39
ERNIE-Speed 9.54 68.33 10.49 68.49
ERNIE-Turbo 9.04 65.20 9.66 67.39
Sensechat-5 10.47 68.93 14.51 73.38
Claude-3.5 7.95 64.83 8.24 64.84
GLM-4 7.92 64.28 11.11 69.65
ChatGPT 10.42 67.84 13.82 71.87
GPT-4o 9.34 66.25 12.61 71.51
GPT-4 9.97 67.08 12.87 72.00
Models 0-shot(best) 5-shot(best)
(TruthfulQA-English)
Bleu-4 BERTScore Bleu-4 BERTScore
Qwen-1.5-110b 12.78 85.83 20.10 87.19
Qwen-2-7b 8.76 83.80 16.37 87.10
Qwen-2-72b 6.99 81.36 8.58 82.97
Mixtral-8x22b 10.82 85.68 17.65 88.24
Mixtral-large-2 11.95 85.68 25.12 89.97
Llama-3-8b 10.04 83.86 32.17 90.98
Llama-3-70b 9.07 83.42 31.85 90.99
Llama-3.1-8b 9.81 83.19 31.18 90.56
Llama-3.1-70b 11.27 84.01 35.02 91.60
Phi-3-medium 12.33 86.70 24.27 89.57
Gemma-2-27b 8.46 83.20 10.52 84.24
Yi-1.5-34b 11.01 84.72 22.50 88.79
Internlm-2.5-7b 15.17 82.73 22.06 84.40
ChatGPT 17.97 87.65 26.69 90.27
GPT-4o 10.93 85.28 32.38 90.94
GPT-4 11.51 85.16 34.34 91.36
Table10: ResultsofthecomparisonbetweentextsgeneratedbyvariousLLMsinCantoneseandEnglishversionof
TruthfulQAbasedon0-shotand5-shotsettingsandthebesttexts. Rouge-l,Bleu-4,andBERTScoreareevaluation
metricsforcomparingtextsimilarity.Models 0-shot(incorrect) 5-shot(incorrect)
(Yue-TruthfulQA)
Bleu-4 BERTScore Bleu-4 BERTScore
Qwen-1.5-110b 12.83 69.22 12.67 68.67
Qwen-2-7b 8.38 65.10 8.38 65.56
Qwen2-72b 8.15 64.44 9.17 66.03
Mixtral-8x22b 9.24 66.27 10.14 66.11
Mixtral-large-2 10.60 68.40 12.62 69.74
Llama-3-8b 7.69 64.07 11.03 68.54
Llama-3-70b 8.12 65.49 12.11 69.10
Llama-3.1-8b 8.72 66.38 10.73 68.22
Llama-3.1-70b 10.79 67.80 12.38 68.28
Phi-3-medium 10.23 67.07 10.40 66.07
Gemma-2-27b 7.40 63.04 8.05 62.28
Yi-1.5-34b 9.16 66.67 10.04 67.68
Internlm-2.5-7b 7.13 62.94 8.84 66.68
ERNIE-Lite 9.72 66.86 9.40 66.73
ERNIE-Tiny 11.50 67.96 11.63 67.90
ERNIE-Speed 10.18 66.93 10.52 66.93
ERNIE-Turbo 9.52 66.15 9.70 66.76
Sensechat-5 12.02 68.33 12.31 67.80
Claude-3.5 8.20 65.93 7.78 65.57
GLM-4 8.43 66.00 10.34 68.09
ChatGPT 11.29 67.46 13.07 68.69
GPT-4o 9.64 67.40 11.21 68.89
GPT-4 10.45 67.72 11.49 68.52
Models 0-shot(incorrect) 5-shot(incorrect)
(TruthfulQA-English)
Bleu-4 BERTScore Bleu-4 BERTScore
Qwen-1.5-110b 12.83 85.75 13.89 85.31
Qwen-2-7b 8.65 83.70 11.39 85.02
Qwen-2-72b 6.84 81.59 7.98 82.30
Mixtral-8x22b 9.94 85.19 12.63 86.15
Mixtral-large-2 11.18 85.21 16.21 86.50
Llama-3-8b 10.01 84.02 19.84 86.68
Llama-3-70b 8.68 83.55 18.89 86.80
Llama-3.1-8b 9.65 83.36 19.26 86.70
Llama-3.1-70b 10.86 83.95 19.27 86.64
Phi-3-medium 13.45 86.14 16.37 86.76
Gemma-2-27b 8.08 83.05 9.24 83.61
Yi-1.5-34b 10.63 84.48 15.49 86.31
Internlm-2.5-7b 15.17 82.87 16.10 87.08
ChatGPT 17.78 87.22 20.45 87.50
GPT-4o 9.99 84.72 18.70 86.73
GPT-4 10.72 84.87 19.54 86.53
Table11: ResultsofthecomparisonbetweentextsgeneratedbyvariousLLMsinCantoneseandEnglishversionof
TruthfulQAbasedon0-shotand5-shotsettingsandtheincorrecttexts. Rouge-l,Bleu-4,andBERTScoreare
evaluationmetricsforcomparingtextsimilarity.Models 0-shot(correct) 5-shot(correct)
(English-TruthfulQA)
Rouge-l Bleu-4 BERTScore Rouge-l Bleu-4 BERTScore
Qwen-1.5-110b 22.57 15.54 85.78 29.44 23.14 86.35
Qwen-2-7b 10.98 10.20 83.86 23.67 18.60 86.09
Qwen-2-72b 3.03 7.58 81.78 7.45 9.59 82.98
Mixtral-8x22b 18.59 12.91 85.78 31.05 20.61 87.58
Mixtral-large-2 20.57 14.63 85.69 41.46 28.92 88.30
Llama-3-8b 16.89 11.59 84.11 58.34 38.35 88.50
Llama-3-70b 12.09 10.46 83.84 53.00 36.77 88.94
Llama-3.1-8b 14.13 11.34 83.46 51.70 36.95 88.47
Llama-3.1-70b 18.12 13.24 84.18 55.22 40.54 88.88
Phi-3-medium 27.90 17.35 86.48 43.02 28.62 88.24
Gemma-2-27b 12.31 9.84 83.56 18.25 12.25 84.31
Yi-1.5-34b 17.22 13.22 84.79 35.33 25.82 87.56
Internlm-2.5-7b 34.44 18.62 82.92 39.19 25.39 84.39
ChatGPT 37.81 21.95 87.20 50.43 31.44 88.55
GPT-4o 17.93 13.05 85.38 49.52 37.44 88.62
GPT-4 19.58 14.10 85.19 53.18 39.22 88.85
Table12: ResultsofthecomparisonbetweentextsgeneratedbyvariousLLMsinEnglish-TruthfulQAbasedon
0-shot and 5-shot settings and the correct texts. Rouge-l, Bleu-4, and BERTScore are evaluation metrics for
comparingtextsimilarity,whileHumanreferstotheevaluationmetricsfromhumanevaluation(Section5.2).
Models Acc. (0-shot) Acc. (5-shot)
Qwen-1.5-110b 77.86 81.27
Qwen-2-7b 76.65 82.56
Qwen-2-72b 93.25 91.89
Mixtral-8x22b 91.51 91.58
Mixtral-large-2 93.63 95.30
Llama-3-8b 79.61 80.74
Llama-3-70b 89.01 93.48
Llama-3.1-8b 85.90 86.50
Llama-3.1-70b 90.45 95.38
Phi-3-medium 82.41 90.90
Gemma-2-27b 21.53 9.70
Yi-1.5-34b-chat 81.50 88.17
Internlm-2.5-7b 72.78 62.32
ChatGPT 57.39 64.37
GPT-4o 93.71 95.75
GPT-4 89.08 94.84
Table13: ResultsofthecomparisonbetweenanswergeneratedbyvariousLLMsinEnglish-GSM8Kbasedon
0-shotand5-shotsettingsandgroundtruth.Models Acc. (0-shot) Acc. (5-shot)
Qwen-1.5-110b 66.09 60.52
Qwen-2-7b 31.16 36.65
Qwen-2-72b 46.01 31.16
Mixtral-8x22b 89.18 80.69
Mixtral-large-2 94.59 94.51
Llama-3-8b 81.12 78.71
Llama-3-70b 93.22 92.62
Llama-3.1-8b 80.09 82.23
Llama-3.1-70b 93.56 93.13
Phi-3-medium 64.81 92.02
Gemma-2-27b 82.23 72.53
Yi-1.5-34b 92.19 92.53
Internlm-2.5-7b 86.78 86.78
Table14: ResultsofthecomparisonbetweenanswergeneratedbyvariousLLMsinEnglish-ARCchallengebased
on0-shotand5-shotsettingsandgroundtruth.
Models 0-shot(correct) 5-shot(correct)
(StandardChinese-MMLU)
STEM Hum. S.S. C.S. Oth. STEM Hum. S.S. C.S. Oth.
Qwen-1.5-110b 76.25 84.55 75.02 80.8 76.25 82.14 88.74 82.57 77.86 80.06
Qwen-2-7b 70.93 81.49 76.78 80.43 73.67 80.97 88.16 84.64 86.69 83.82
Qwen-2-72b 75.31 82.9 74.9 81.03 74.1 89.1 94.18 91.02 93.67 92.22
Mixtral-8x22b 48.7 63.18 56.43 64.24 57.47 51.56 63.45 58.26 63.98 61.16
Mixtral-large-2 64.36 79.38 70.8 77.03 68.49 66.87 81.85 74.52 78.76 70.83
Llama-3-8b 45.52 61.22 56.28 59.17 53.22 44.88 59.13 53.35 57.94 47.25
Llama-3-70b 60.87 77.01 75.81 77.37 73.05 58.74 75.18 74.72 74.99 71.63
Llama-3.1-8b 44.15 60.92 53.11 59.17 48.66 46.27 62.44 56.27 60.93 55.91
llama-3.1-70b 62.33 77.77 75.02 76.81 70.25 61.76 77.95 75.75 78.48 71.99
Phi-3-medium 47.56 62.64 56.98 62.65 57.84 48.25 62.68 56.44 64.38 56.3
Gemma-2-27b 42.45 53.24 49.68 56.71 49.44 36.39 43.14 47.3 47.08 38.8
Yi-1.5-34b 74.76 83.57 83.41 82.99 72.61 77.41 85.85 84.69 84.38 78.46
Internlm-2.5-7b 77.03 85.55 83.9 82.92 70.3 77.92 86.45 83.46 83.78 68.83
Table15: ResultsofthecomparisonbetweentextsgeneratedbyvariousLLMsinCMMLUbasedon0-shotand
5-shotsettingsandthecorrecttexts. Hum.,S.S.,C.S.,Oth. areforHumanities,Socialsciences,Chinaspecific,
Other,respectively.0-shot 5-shot
Models(mdn-yue)
Rouge-l Bleu-4 BERTScore Rouge-l Bleu-4 BERTScore
Qwen-1.5-110b 30.03 22.88 51.94 88.72 79.60 94.34
Qwen-2-7b 39.23 25.16 75.43 64.50 50.14 84.32
Qwen-2-72b 16.63 19.74 68.85 5.37 11.08 64.23
Mixtral-8x22b 35.77 32.04 74.81 59.82 51.59 84.47
Mixtral-large-2 84.92 64.83 91.99 87.86 72.70 93.42
Llama-3-8b 16.80 33.14 74.81 45.13 43.53 84.47
Llama-3-70b 17.22 37.77 73.91 47.30 60.16 85.17
Llama-3.1-8b 65.87 44.30 84.67 79.77 61.34 90.39
Llama-3.1-70b 84.66 63.23 91.86 88.88 76.17 94.45
Phi-3-medium 64.15 36.79 83.65 75.08 48.58 88.49
Gemma-2-27b 6.28 11.30 62.11 5.26 8.56 59.14
Yi-1.5-34b 73.08 47.27 89.93 83.10 66.99 91.10
Internlm-2.5-7b 43.68 16.95 81.13 63.04 34.87 86.43
Sensechat-5 89.14 72.78 94.00 91.10 77.65 95.05
GLM-4 80.17 59.53 89.67 80.89 64.87 89.83
ChatGPT 86.33 68.02 92.09 85.01 73.62 91.49
GPT-4o 88.69 73.70 93.34 89.82 79.06 94.21
GPT-4 85.64 68.25 92.52 88.14 75.65 93.92
0-shot 5-shot
Models(en-yue)
Rouge-l Bleu-4 BERTScore Rouge-l Bleu-4 BERTScore
Qwen-1.5-110b 3.15 1.09 15.79 74.55 40.05 85.72
Qwen-2-7b 45.47 21.26 76.65 68.24 31.62 82.94
Qwen-2-72b 25.53 19.05 73.25 7.95 14.58 70.95
Mixtral-8x22b 49.51 18.42 77.04 68.80 31.15 83.28
Mixtral-large-2 69.56 31.18 83.88 74.41 38.97 85.92
Llama-3-8b 27.86 21.68 73.99 62.51 30.19 81.55
Llama-3-70b 55.16 28.11 80.61 54.35 34.58 82.71
Llama-3.1-8b 63.05 25.25 80.58 68.49 31.99 83.18
Llama-3.1-70b 66.38 29.71 83.10 73.21 37.78 85.62
Phi-3-medium 48.66 15.94 76.57 62.45 24.66 81.16
Gemma-2-27b 9.35 12.52 65.45 4.86 8.69 60.89
Yi-1.5-34b 64.33 27.91 82.37 70.64 35.06 84.23
Internlm-2.5-7b 44.52 14.02 77.26 65.29 29.61 82.80
Sensechat-5 67.30 33.91 84.01 74.56 39.04 86.00
GLM-4 69.23 34.26 84.37 71.66 38.07 84.99
ChatGPT 71.17 33.20 84.41 73.10 36.78 85.08
GPT-4o 71.03 36.34 85.12 73.12 39.85 85.44
GPT-4 70.82 34.42 85.21 73.21 37.38 85.70
Table16: ResultbasedontheYue-Transdatasets(TheaboveistranslatedfromMandarintoCantonese,andthe
belowistranslatedfromEnglishtoCantonese).Models Totalrunningtime NumberofGPU Batchsize
Qwen-1.5-110b 11053.46 6 4
Qwen-2-7b 1463.17 1 8
Qwen-2-72b 21467.50 6 8
Mixtral-8x22b 19345.82 6 4
Mixtral-large-2 12302.97 6 4
Llama-3-8b 1449.98 1 8
Llama-3-70b 3741.66 6 16
Llama-3.1-8b 1338.55 1 8
Llama-3.1-70b 3580.30 6 16
Phi-3-medium 4121.94 1 8
Gemma-2-27b 35563.46 1 1
Yi-1.5-34b 3516.06 1 4
Internlm-2.5-7b 1446.18 1 8
Table17: ThetotalrunningtimeofdifferentLLMs,thenumberofGPUsused,andthebatchsize.
Models Singlebatchrunningtime
Qwen-1.5-110b 2763.37
Qwen-2-7b 182.90
Qwen-2-72b 2683.44
Mixtral-8x22b 4836.46
Mixtral-large-2 3075.74
Llama-3-8b 181.25
Llama-3-70b 233.85
Llama-3.1-8b 167.32
Llama-3.1-70b 223.77
Phi-3-medium 515.24
Gemma-2-27b 35563.46
Yi-1.5-34b 879.02
Internlm-2.5-7b 180.77
Table18: Theruntimeperbatchfordifferentmodels. Thisiscalculatedbydirectlydividingthetotaltimefrom
Table17bythebatchsize.