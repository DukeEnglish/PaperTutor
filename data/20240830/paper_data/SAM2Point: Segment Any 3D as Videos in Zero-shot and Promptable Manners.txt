TechnicalReport
SAM2POINT: SEGMENT ANY 3D AS VIDEOS
IN ZERO-SHOT AND PROMPTABLE MANNERS
ZiyuGuo1∗,RenruiZhang2,3∗†,XiangyangZhu4∗,ChengzhuoTong4
PengGao4,ChunyuanLi3,Pheng-AnnHeng1
1CUHKMiuLarLab 2CUHKMMLab 3ByteDance 4ShanghaiAILaboratory
{ziyuguo, renruizhang}@link.cuhk.edu.hk
∗Equalcontribution †Projectlead
ABSTRACT
WeintroduceSAM2POINT,apreliminaryexplorationadaptingSegmentAnything
Model2(SAM2)forzero-shotandpromptable3Dsegmentation. SAM2POINT
interpretsany3Ddataasaseriesofmulti-directionalvideos,andleveragesSAM
2for3D-spacesegmentation,withoutfurthertrainingor2D-3Dprojection. Our
frameworksupportsvariousprompttypes,including3Dpoints,boxes,andmasks,
and can generalize across diverse scenarios, such as 3D objects, indoor scenes,
outdoorscenes, andrawLiDAR.Demonstrationsonmultiple3Ddatasets, e.g.,
Objaverse,S3DIS,ScanNet,Semantic3D,andKITTI,highlighttherobustgeneral-
izationcapabilitiesofSAM2POINT. Toourbestknowledge,wepresentthemost
faithfulimplementationofSAMin3D,whichmayserveasastartingpointfor
futureresearchinpromptable3Dsegmentation.
LiveDemo: https://huggingface.co/spaces/ZiyuG/SAM2Point
Code: https://github.com/ZiyuGuo99/SAM2Point
3D Object Indoor Scene Outdoor Scene Raw LiDAR
SAM2POINT
3D Point 3D Box 3D Mask
Prompt Prompt Prompt
Figure1: TheSegmentationParadigmofSAM2POINT. Weintroduceazero-shotandpromptable
framework for robust 3D segmentation via SAM 2 (Ravi et al., 2024). It supports various user-
provided3Dprompt,andcangeneralizetodiverse3Dscenarios. The3Dpromptandsegmentation
resultsarehighlightedinredandgreen,respectively.
1
4202
guA
92
]VC.sc[
1v86761.8042:viXraTechnicalReport
Table1: Comparisonof SAM2POINT andPreviousSAM-basedMethods(Yangetal.,2023b;
Cenetal.,2023;Xuetal.,2023a;Zhouetal.,2024). Toourbestknowledge,SAM2POINTpresents
the most faithful implementation of SAM (Kirillov et al., 2023) in 3D, demonstrating superior
implementationefficiency,promptableflexibility,andgeneralizationcapabilitiesfor3Dsegmentation.
3DPrompt 3DScenario
Method Zero-shot Project-free
Point Box Mask Object Indoor Outdoor RawLiDAR
SAM3D ✓ - - - - - ✓ - -
SA3D ✓ - - - - - ✓ ✓ -
SAMPro3D ✓ - - - - - ✓ - -
Point-SAM - ✓ ✓ - - ✓ ✓ ✓ -
SAM2POINT ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
1 INTRODUCTION
SegmentAnythingModel(SAM)(Kirillovetal.,2023)hasestablishedasuperiorandfundamental
framework for interactive image segmentation. Building on its strong transferability, follow-up
researchfurtherextendsSAMtodiversevisualdomains, e.g., personalizedobjects(Zhangetal.,
2023b;Liuetal.,2023d),medicalimaging(Maetal.,2024;Mazurowskietal.,2023),andtemporal
sequences (Yang et al., 2023a; Cheng et al., 2023). More recently, Segment Anything Model 2
(SAM2)(Ravietal.,2024)isproposedforimpressivesegmentationcapabilitiesinvideoscenarios,
capturingcomplexreal-worlddynamics.
Despitethis,effectivelyadaptingSAMfor3Dsegmentationstillremainsanunresolvedchallenge.
Weidentifythreeprimaryissueswithinpreviousefforts,ascomparedinTable1,whichpreventthem
fromfullyleveragingSAM’sadvantages:
• Inefficient 2D-3D Projection. Considering the domain gap between 2D and 3D, most
existingworksrepresent3Ddataasits2DcounterpartasinputforSAM,andback-project
the segmentation results into 3D space, e.g., using additional RGB images (Yang et al.,
2023b;Yinetal.,2024;Xuetal.,2023a),multi-viewrenderings(Zhouetal.,2023b),or
NeuralRadianceField(Cenetal.,2023). Suchmodalitytransitionintroducessignificant
processingcomplexity,hinderingefficientimplementation.
• Degradationof3DSpatialInformation. Therelianceon2Dprojectionsresultsintheloss
of fine-grained 3D geometries and semantics, as multi-view data often fails to preserve
spatialrelations. Furthermore,theinternalstructuresof3Dobjectscannotbeadequately
capturedby2Dimages,significantlylimitingsegmentationaccuracy.
• LossofPromptingFlexibility. AcompellingstrengthofSAMliesinitsinteractivecapa-
bilitiesthroughvariouspromptalternatives. Unfortunately,thesefunctionalitiesaremostly
disregardedincurrentmethods,asusersstruggletospecifyprecise3Dpositionsusing2D
representations. Consequently,SAMistypicallyusedfordensesegmentationacrossentire
multi-viewimages,therebysacrificinginteractivity.
• LimitedDomainTransferability. Existing2D-3Dprojectiontechniquesareoftentailored
tospecific3Dscenarios,heavilydependentonin-domainpatterns. Thismakesthemchal-
lengingtoapplytonewcontexts, e.g., fromobjectstoscenesorfromindoortooutdoor
environments. Anotherresearchdirection(Zhouetal.,2024)aimstotrainapromptablenet-
workfromscratchin3D.Whilebypassingtheneedfor2Dprojections,itdemandssubstantial
traininganddataresourcesandmaystillbeconstrainedbytrainingdatadistributions.
Inthisproject,weintroduceSAM2POINT,adaptingSAM2forefficient,projection-free,promptable,
and zero-shot 3D segmentation. As an initial step in this direction, our target is not to push the
performancelimit,butrathertodemonstratethepotentialofSAMinachievingrobustandeffective
3Dsegmentationindiversecontexts. Specifically,SAM2POINTexhibitsthreefeaturesasoutlined:
• SegmentingAny3DasVideos. Topreserve3Dgeometriesduringsegmentation, while
ensuringcompatibilitywithSAM2,weadoptvoxelizationtomimicavideo. Voxelized3D
2TechnicalReport
data,withashapeofw×h×l×3,closelyresemblestheformatofvideosofw×h×t×3.
ThisrepresentationallowsSAM2forzero-shot3Dsegmentationwhileretainingsufficient
spatialinformation,withouttheneedofadditionaltrainingor2D-3Dprojection.
• SupportingMultiple3DPrompts. BuildingonSAM2,SAM2POINTsupportsthreetypesof
prompts: 3Dpoints,boundingboxes,andmasks. Startingwithauser-provided3Dprompt,
e.g.,apoint(x,y,z),wedividethe3Dspaceintothreeorthogonaldirections,generating
sixcorrespondingvideos. Then,themulti-directionalsegmentationresultsareintegratedto
formthefinalpredictionin3Dspace,allowingforinteractivepromptablesegmentation.
• GeneralizabletoVariousScenarios. Withourconciseframework,SAM2POINTdemon-
stratesstronggeneralizationcapabilitiesindiverse3Dscenarioswithvaryingpointcloud
distributions.AsshowcasedinFigure1,ourapproachcaneffectivelysegmentsingleobjects,
indoor scenes, outdoor scenes, and raw LiDAR, highlighting its superior transferability
acrossdifferentdomains.
2 SAM2POINT
ThedetailedmethodologyofSAM2POINTispresentedinFigure2. InSection2.1,weintroducehow
SAM2POINTefficientlyformats3DdataforcompatibilitywithSAM2(Ravietal.,2024),avoiding
complexprojectionprocess. Then,inSection2.2,wedetailthethreetypesof3Dpromptsupported
andtheirassociatedsegmentationtechniques. Finally,inSection2.3,weillustratefourchallenging
3DscenarioseffectivelyaddressedbySAM2POINT.
2.1 3DDATAASVIDEOS
Givenanyobject-levelorscene-levelpointcloud,wedenoteitbyP ∈ Rn×6,witheachpointas
p = (x,y,z,r,g,b). Our aim is to convert P into a data format that, for one hand, SAM 2 can
directlyprocessinazero-shotmanner,and,fortheother,thefine-grainedspatialgeometriescan
bewellpreserved. Tothisend,weadoptthe3Dvoxelizationtechnique. ComparedtoRGBimage
mapping(Yangetal.,2023b;Yinetal.,2024;Xuetal.,2023a),multi-viewrendering(Zhouetal.,
2023b),andNeRF(Cenetal.,2023)inpreviousefforts,voxelizationisefficientlyperformedin3D
space,therebyfreefrominformationdegradationandcumbersomepost-processing.
Inthisway,weobtainavoxelizedrepresentationofthe3Dinput,denotedbyV ∈Rw×h×l×3with
eachvoxelasv =(r,g,b).Forsimplicity,the(r,g,b)valueissetaccordingtothepointnearesttothe
voxelcenter.Thisformatcloselyresemblesvideoswithashapeofw×h×t×3.Themaindifference
isthat,videodatacontainsunidirectionaltemporaldependencyacrosstframes,while3Dvoxelsare
isotropicalongthreespatialdimensions. Consideringthis,weconvertthevoxelrepresentationasa
seriesofmulti-directionalvideos,inspiringSAM2tosegment3Dthesamewayasvideos.
2.2 PROMPTABLESEGMENTATION
Forflexibleinteractivity,ourSAM2POINTsupportsthreetypesofpromptin3Dspace,whichcanbe
utilizedeitherseparatelyorjointly. Wespecifythepromptingandsegmentationdetailsbelow:
• 3DPointPrompt,denotedasp = (x ,y ,z ). Wefirstregardp asananchorpointin
p p p p p
3Dspacetodefinethreeorthogonal2Dsections. Startingfromthesesections,wedividethe
3Dvoxelsintosixsubpartsalongsixspatialdirections,i.e.,front,back,left,right,up,and
down. Then,weregardthemassixdifferentvideos,wherethesectionservesasthefirst
frameandp isprojectedasthe2Dpointprompt. AfterapplyingSAM2forconcurrent
p
segmentation,weintegratetheresultsofsixvideosasthefinal3Dmaskprediction.
• 3DBoxPrompt,denotedasb =(x ,y ,z ,w ,h ,l ),including3Dcentercoordinates
p p p p p p p
anddimensions. Weadoptthegeometriccenterofb astheanchorpoint,andrepresent
p
the3Dvoxelsbysixdifferentvideosasaforementioned. Forvideoofacertaindirection,
weprojectb intothecorresponding2Dsectiontoserveastheboxpointforsegmentation.
p
Wealsosupport3Dboxwithrotationangles, e.g., (α , β , γ ), forwhichthebounding
p p p
rectangleofprojectedb isadoptedasthe2Dprompt.
p
3TechnicalReport
Diverse 3D Scenarios Various 3D Prompt 3D Data as Videos Zero-shot Segmentation
3D Point 3D Box 3D Mask
3D Object Prompt Prompt Prompt
Indoor Scene Segment
Anything
Model 2
Voxelized
Outdoor Scene Representation
Divide 3D Space by Multi-directional
Anchor Point Videos
Raw LiDAR
Figure2: TheDetailedMethodologyofSAM2POINT. Weconvertanyinput3Ddataintovoxelized
representations,andutilizeuser-provided3Dprompttodividethe3Dspacealongsixdirections,
effectivelysimulatingsixdifferentvideosforSAM2toperformzero-shotsegmentation.
• 3DMaskPrompt,denotedasM ∈Rn×1,where1or0indicatesthemaskedandunmasked
p
areas. Weemploythecenterofgravityofthemaskpromptastheanchorpoint,anddivide
3Dspaceintosixvideoslikewise. Theintersectionbetweenthe3Dmaskpromptandeach
sectionisutilizedasthe2Dmaskpromptforsegmentation. Thistypeofpromptingcanalso
serveasapost-refinementsteptoenhancetheaccuracyofpreviouslypredicted3Dmasks.
2.3 ANY3DSCENARIOS
Withourconciseframeworkdesign,SAM2POINTexhibitssuperiorzero-shotgeneralizationperfor-
manceacrossdiversedomains,rangingfromobjectstoscenesandindoortooutdoorenvironments.
Weelaborateonfourdistinct3Dscenariosbelow:
• 3DObject,e.g.,Objaverse(Deitkeetal.,2023),withawidearrayofcategories,possesses
uniquecharacteristicsacrossdifferentinstances,includingcolors,shapes,andgeometries.
Adjacent components of an object might overlap, occlude, or integrate with each other,
whichrequiresmodelstoaccuratelydiscernsubtledifferencesforpartsegmentation.
• IndoorScene,e.g.,S3DIS(Armenietal.,2016)andScanNet(Daietal.,2017),aretypically
characterizedbymultipleobjectsarrangedwithinconfinedspaces,likerooms. Thecomplex
spatial layouts, similarity in appearance, and varied orientations between objects pose
challengesformodelstosegmentthemfrombackgrounds.
• OutdoorScene,e.g.,Semantic3D(Hackeletal.,2017),differsfromindoorscenes,primarily
duetothestarksizecontrastsofobjects(buildings,vehicles,andhumans)andthelarger
scaleofpointclouds(fromaroomtoanentirestreet). Thesevariationscomplicatesthe
segmentationofobjectswhetherataglobalscaleorafine-grainedlevel.
• Raw LiDAR, e.g., KITTI (Geiger et al., 2012) in autonomous driving, is distinct from
typicalpointcloudsforitssparsedistributionandabsenceofRGBinformation. Thesparsity
demandsmodelstoinfermissingsemanticsforunderstandingthescene,andthelackof
colorsenforcesmodelstoonlyrelyongeometriccuestodifferentiatebetweenobjects. In
SAM2POINT,wedirectlysettheRGBvaluesof3DvoxelsbytheLiDARintensity.
4TechnicalReport
3 DISCUSSION AND INSIGHT
BuildingontheeffectivenessofSAM2POINT,wedelveintotwocompellingyetchallengingissues
withintherealmof3D,andshareourinsightsonfuturemulti-modalitylearning.
3.1 HOWTOADAPT2DFOUNDATIONMODELSTO3D?
Theavailabilityoflarge-scale,high-qualitydatahassignificantlyempoweredthedevelopmentoflarge
modelsinlanguage(Brownetal.,2020;Touvronetal.,2023;Zhangetal.,2023a),2Dvision(Liu
etal.,2023b;Teametal.,2023;Chenetal.,2024),andvision-language(Gaoetal.,2024;Liuetal.,
2023a;Lietal.,2024;Zhangetal.,2024)domains. Incontrast,the3Dfieldhaslongstruggledwitha
scarcityofdata,hinderingthetrainingoflarge3Dmodels. Asaresult,researchershaveturnedtothe
alternativeoftransferringpre-trained2Dmodelsinto3D.
Theprimarychallengeliesinbridgingthemodalgapbetween2Dand3D.Pioneeringapproaches,
suchasPointCLIP(Zhangetal.,2022),itsV2(Zhuetal.,2022),andsubsequentmethods(Jietal.,
2023;Huangetal.,2023),project3Ddataintomulti-viewimages,whichencounterimplementation
inefficiencyandinformationloss. Anotherlineofwork,includingULIPseries(Xueetal.,2022;
2023),I2P-MAE(Zhangetal.,2023c),andothers(Liuetal.,2023c;Qietal.,2023;Guoetal.,2023a),
employsknowledgedistillationusing2D-3Dpaireddata. Whilethismethodgenerallyperforms
betterduetoextensivetraining,itsuffersfromlimited3Dtransferabilityinout-of-domainscenarios.
Recent efforts have also explored more complex and costly solutions, such as joint multi-modal
spaces(e.g.,Point-Bind&Point-LLM(Guoetal.,2023b)),larger-scalepre-training(Uni3D(Zhou
etal.,2023a)),andvirtualprojectiontechniques(Any2Point(Tangetal.,2024)).
FromSAM2POINT,weobservethatrepresenting3Ddataasvideosthroughvoxelizationmayofferan
optimalsolution,providingabalancedtrade-offbetweenperformanceandefficiency. Thisapproach
notonly preservesthe spatialgeometriesinherent in3Dspace witha simpletransformation, but
also presents a grid-based data format that 2D models can directly process. Despite this, further
experimentsarenecessarytovalidateandreinforcethisobservation.
3.2 WHATISTHEPOTENTIALOFSAM2POINTIN3DDOMAINS?
Tothebestofourknowledge,SAM2POINTpresentsthemostaccurateandcomprehensiveimple-
mentationofSAMin3D,successfullyinheritingitsimplementationefficiency,promptableflexibility,
and generalization capabilities. While previous SAM-based approaches (Yang et al., 2023b; Xu
etal.,2023a;Yinetal.,2024)haveachieved3Dsegmentation,theyoftenfallshortinscalabilityand
transferabilitytobenefitother3Dtasks. Incontrast,inspiredbySAMin2Ddomains,SAM2POINT
demonstratessignificantpotentialtoadvancevarious3Dapplications.
For fundamental 3D understanding, SAM2POINT can serve as a unified initialized backbone for
further fine-tuning, offering strong 3D representations simultaneously across 3D objects, indoor
scenes,outdoorscenes,andrawLiDAR.Inthecontextoftraininglarge3Dmodels,SAM2POINT
canbeemployedasanautomaticdataannotationtool,whichmitigatesthedatascarcityissueby
generatinglarge-scalesegmentationlabelsacrossdiversescenarios. For3Dandlanguage-vision
learning, SAM2POINT inherently provides a joint embedding space across 2D, 3D, and video
domains,duetoitszero-shotcapabilities,whichcouldfurtherenhancetheeffectivenessofmodels
likePoint-Bind(Guoetal.,2023b). Additionally,inthedevelopmentof3Dlargelanguagemodels
(LLMs)(Hongetal.,2023;Xuetal.,2023b;Wangetal.,2023;Guoetal.,2023b),SAM2POINTcan
functionasapowerful3Dencoder,supplyingLLMswith3Dtokens,andleveragingitspromptable
featurestoequipLLMswithpromptableinstruction-followingcapabilities.
4 DEMOS
InFigures3-7,weshowcasedemonstrationsofSAM2POINTinsegmenting3Ddatawithvarious3D
promptondifferentdatasets(Deitkeetal.,2023;Armenietal.,2016;Daietal.,2017;Hackeletal.,
2017;Geigeretal.,2012). Forfurtherimplementationdetails,pleaserefertoouropen-sourcedcode.
5TechnicalReport
3D Point Prompt
3D Box Prompt
3D Mask Prompt
Figure3: 3DObjectSegmentationwithSAM2POINTonObjaverse(Deitkeetal.,2023). The3D
promptandsegmentationresultsarehighlightedinredandgreen,respectively.
6TechnicalReport
3D Point Prompt
3D Box Prompt
3D Mask Prompt
Figure4: 3DIndoorSceneSegmentationwithSAM2POINTonS3DIS(Armenietal.,2016). The
3Dpromptandsegmentationresultsarehighlightedinredandgreen,respectively.
7TechnicalReport
3D Point Prompt
3D Box Prompt
3D Mask Prompt
Figure5: 3DIndoorSceneSegmentationwithSAM2POINTonScanNet(Daietal.,2017). The
3Dpromptandsegmentationresultsarehighlightedinredandgreen,respectively.
8TechnicalReport
3D Point Prompt
3D Box Prompt
3D Mask Prompt
Figure6: 3DOutdoorSceneSegmentationwith SAM2POINT onSemantic3D(Hackeletal.,
2017). The3Dpromptandsegmentationresultsarehighlightedinredandgreen,respectively.
9TechnicalReport
3D Point Prompt
3D Box Prompt
3D Mask Prompt
Figure7: 3DRawLiDARSegmentationwithSAM2POINTonKITTI(Geigeretal.,2012). The
3Dpromptandsegmentationresultsarehighlightedinredandgreen,respectively.
5 CONCLUSION
In this project, we propose SAM2POINT, which leverages Segment Anything 2 (SAM 2) to 3D
segmentation with a zero-shot and promptable framework. By representing 3D data as multi-
directional videos, SAM2POINT supports various types of user-provided prompt (3D point, box,
andmask),andexhibitsrobustgeneralizationacrossdiverse3Dscenarios(3Dobject,indoorscene,
outdoorenvironment,andrawsparseLiDAR).Asapreliminaryinvestigation,SAM2POINTprovides
unique insights into adapting SAM 2 for effective and efficient 3D understanding. We hope our
methodmayserveasafoundationalbaselineforpromptable3Dsegmentation,encouragingfurther
researchtofullyharnessSAM2’spotentialin3Ddomains.
10TechnicalReport
REFERENCES
IroArmeni,OzanSener,AmirRZamir,HelenJiang,IoannisBrilakis,MartinFischer,andSilvio
Savarese. 3dsemanticparsingoflarge-scaleindoorspaces. InProceedingsoftheIEEEconference
oncomputervisionandpatternrecognition,pp.1534–1543,2016.
TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare
few-shotlearners. AdvancesinNeuralInformationProcessingSystems,33:1877–1901,2020.
JiazhongCen,ZanweiZhou,JieminFang,WeiShen,LingxiXie,DongshengJiang,XiaopengZhang,
QiTian,etal. Segmentanythingin3dwithnerfs. AdvancesinNeuralInformationProcessing
Systems,36:25971–25990,2023.
ZheChen,JiannanWu,WenhaiWang,WeijieSu,GuoChen,SenXing,MuyanZhong,Qinglong
Zhang,XizhouZhu,LeweiLu,etal. Internvl: Scalingupvisionfoundationmodelsandaligning
for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer
VisionandPatternRecognition,pp.24185–24198,2024.
YangmingCheng,LiuleiLi,YuanyouXu,XiaodiLi,ZongxinYang,WenguanWang,andYiYang.
Segmentandtrackanything. arXivpreprintarXiv:2305.06558,2023.
Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias
Nießner. Scannet: Richly-annotated3dreconstructionsofindoorscenes. InProceedingsofthe
IEEEconferenceoncomputervisionandpatternrecognition,pp.5828–5839,2017.
MattDeitke,DustinSchwenk,JordiSalvador,LucaWeihs,OscarMichel,EliVanderBilt,Ludwig
Schmidt,KianaEhsani,AniruddhaKembhavi,andAliFarhadi. Objaverse: Auniverseofanno-
tated3dobjects. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pp.13142–13153,2023.
PengGao,RenruiZhang,ChrisLiu,LongtianQiu,SiyuanHuang,WeifengLin,ShitianZhao,Shijie
Geng,ZiyiLin,PengJin,etal. Sphinx-x: Scalingdataandparametersforafamilyofmulti-modal
largelanguagemodels. ICML2024,2024.
AndreasGeiger,PhilipLenz,andRaquelUrtasun. Arewereadyforautonomousdriving? thekitti
vision benchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR),
2012.
ZiyuGuo,RenruiZhang,LongtianQiu,XianzhiLi,andPheng-AnnHeng. Joint-mae: 2d-3djoint
maskedautoencodersfor3dpointcloudpre-training. arXivpreprintarXiv:2302.14007,2023a.
ZiyuGuo,RenruiZhang,XiangyangZhu,YiwenTang,XianzhengMa,JiamingHan,KexinChen,
Peng Gao, Xianzhi Li, Hongsheng Li, et al. Point-bind & point-llm: Aligning point cloud
withmulti-modalityfor3dunderstanding,generation,andinstructionfollowing. arXivpreprint
arXiv:2309.00615,2023b.
TimoHackel,NikolaySavinov,LuborLadicky,JanDWegner,KonradSchindler,andMarcPolle-
feys. Semantic3d.net: Anewlarge-scalepointcloudclassificationbenchmark. arXivpreprint
arXiv:1704.03847,2017.
YiningHong,HaoyuZhen,PeihaoChen,ShuhongZheng,YilunDu,ZhenfangChen,andChuang
Gan. 3d-llm: Injectingthe3dworldintolargelanguagemodels. arXiv,2023.
Tianyu Huang, Bowen Dong, Yunhan Yang, Xiaoshui Huang, Rynson WH Lau, Wanli Ouyang,
and Wangmeng Zuo. Clip2point: Transfer clip to point cloud classification with image-depth
pre-training. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pp.
22157–22167,2023.
JiayiJi,HaoweiWang,ChangliWu,YiweiMa,XiaoshuaiSun,andRongrongJi. Jm3d&jm3d-llm:
Elevating3drepresentationwithjointmulti-modalcues. arXivpreprintarXiv:2310.09503,2023.
AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,Tete
Xiao,SpencerWhitehead,AlexanderCBerg,Wan-YenLo,etal.Segmentanything.InProceedings
oftheIEEE/CVFInternationalConferenceonComputerVision,pp.4015–4026,2023.
11TechnicalReport
FengLi,RenruiZhang,HaoZhang,YuanhanZhang,BoLi,WeiLi,ZejunMa,andChunyuanLi.
Llava-next-interleave: Tacklingmulti-image,video,and3dinlargemultimodalmodels. arXiv
preprintarXiv:2407.07895,2024.
HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee. Improvedbaselineswithvisualinstruction
tuning,2023a.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv
preprintarXiv:2304.08485,2023b.
MinghuaLiu,RuoxiShi,KaimingKuang,YinhaoZhu,XuanlinLi,ShizhongHan,HongCai,Fatih
Porikli, andHaoSu. OpenShape: ScalingUp3DShapeRepresentationTowardsOpen-World
Understanding. arXivpreprintarXiv:2305.10764,2023c.
YangLiu,MuzhiZhu,HengtaoLi,HaoChen,XinlongWang,andChunhuaShen. Matcher: Segment
anything with one shot using all-purpose feature matching. arXiv preprint arXiv:2305.13310,
2023d.
JunMa,YutingHe,FeifeiLi,LinHan,ChenyuYou,andBoWang. Segmentanythinginmedical
images. NatureCommunications,15(1):654,2024.
MaciejAMazurowski,HaoyuDong,HanxueGu,JichenYang,NicholasKonz,andYixinZhang.
Segment anything model for medical image analysis: an experimental study. Medical Image
Analysis,89:102918,2023.
ZekunQi,RunpeiDong,GuofanFan,ZhengGe,XiangyuZhang,KaishengMa,andLiYi. Contrast
withReconstruct: Contrastive3DRepresentationLearningGuidedbyGenerativePretraining. In
InternationalConferenceonMachineLearning,2023.
NikhilaRavi,ValentinGabeur,Yuan-TingHu,RonghangHu,ChaitanyaRyali,TengyuMa,Haitham
Khedr,RomanRädle,ChloeRolland,LauraGustafson,etal. Sam2: Segmentanythinginimages
andvideos. arXivpreprintarXiv:2408.00714,2024.
YiwenTang,RenruiZhang,JiamingLiu,DongWang,ZhigangWang,ShanghangZhang,BinZhao,
andXuelongLi.Any2point:Empoweringany-modalitylargemodelsforefficient3dunderstanding.
ECCV2024,2024.
GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,Radu
Soricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamilyofhighlycapable
multimodalmodels. arXivpreprintarXiv:2312.11805,2023.
HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timothée
Lacroix, BaptisteRozière, NamanGoyal, EricHambro, FaisalAzhar, etal. Llama: Openand
efficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.
ZehanWang,HaifengHuang,YangZhao,ZiangZhang,andZhouZhao. Chat-3d: Data-efficiently
tuninglargelanguagemodelforuniversaldialogueof3dscenes. arXivpreprintarXiv:2308.08769,
2023.
MutianXu,XingyilangYin,LingtengQiu,YangLiu,XinTong,andXiaoguangHan. Sampro3d:
Locatingsampromptsin3dforzero-shotscenesegmentation. arXivpreprintarXiv:2311.17707,
2023a.
Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, and Dahua Lin. Pointllm:
Empoweringlargelanguagemodelstounderstandpointclouds. arXivpreprintarXiv:2308.16911,
2023b.
Le Xue, Mingfei Gao, Chen Xing, Roberto Martín-Martín, Jiajun Wu, Caiming Xiong, Ran Xu,
JuanCarlosNiebles,andSilvioSavarese. ULIP:LearningUnifiedRepresentationofLanguage,
ImageandPointCloudfor3DUnderstanding. arXivpreprintarXiv:2212.05171,2022.
LeXue,NingYu,ShuZhang,JunnanLi,RobertoMartín-Martín,JiajunWu,CaimingXiong,Ran
Xu,JuanCarlosNiebles,andSilvioSavarese. ULIP-2: TowardsScalableMultimodalPre-training
for3DUnderstanding. arXivpreprintarXiv:2305.08275,2023.
12TechnicalReport
JinyuYang, MingqiGao, ZheLi, ShangGao, FangjingWang, andFengZheng. Trackanything:
Segmentanythingmeetsvideos. arXivpreprintarXiv:2304.11968,2023a.
YunhanYang,XiaoyangWu,TongHe,HengshuangZhao,andXihuiLiu. Sam3d: Segmentanything
in3dscenes. arXivpreprintarXiv:2306.03908,2023b.
YingdaYin,YuzhengLiu,YangXiao,DanielCohen-Or,JingweiHuang,andBaoquanChen. Sai3d:
Segmentanyinstancein3dscenes. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pp.3292–3302,2024.
RenruiZhang,ZiyuGuo,WeiZhang,KunchangLi,XupengMiao,BinCui,YuQiao,PengGao,
andHongshengLi. PointCLIP:PointCloudUnderstandingbyCLIP. InIEEEConferenceon
ComputerVisionandPatternRecognition,pp.8552–8562,2022.
RenruiZhang,JiamingHan,ChrisLiu,PengGao,AojunZhou,XiangfeiHu,ShilinYan,PanLu,
HongshengLi,andYuQiao.Llama-adapter:Efficientfine-tuningoflanguagemodelswithzero-init
attention. arXivpreprintarXiv:2303.16199,2023a.
RenruiZhang,ZhengkaiJiang,ZiyuGuo,ShilinYan,JuntingPan,XianzhengMa,HaoDong,Peng
Gao, and Hongsheng Li. Personalize segment anything model with one shot. arXiv preprint
arXiv:2305.03048,2023b.
RenruiZhang,LiuhuiWang,YuQiao,PengGao,andHongshengLi. Learning3drepresentations
from 2d pre-trained models via image-to-point masked autoencoders. In Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.21769–21780,2023c.
RenruiZhang,XinyuWei,DongzhiJiang,YichiZhang,ZiyuGuo,ChengzhuoTong,JiamingLiu,
AojunZhou,BinWei,ShanghangZhang,etal. Mavis: Mathematicalvisualinstructiontuning.
arXivpreprintarXiv:2407.08739,2024.
JunshengZhou,JinshengWang,BaoruiMa,Yu-ShenLiu,TiejunHuang,andXinlongWang. Uni3d:
Exploringunified3drepresentationatscale. arXivpreprintarXiv:2310.06773,2023a.
Yuchen Zhou, Jiayuan Gu, Xuanlin Li, Minghua Liu, Yunhao Fang, and Hao Su. Partslip++:
Enhancinglow-shot3dpartsegmentationviamulti-viewinstancesegmentationandmaximum
likelihoodestimation. arXivpreprintarXiv:2312.03015,2023b.
YuchenZhou,JiayuanGu,TungYenChiang,FanboXiang,andHaoSu. Point-sam: Promptable3d
segmentationmodelforpointclouds. arXivpreprintarXiv:2406.17741,2024.
XiangyangZhu,RenruiZhang,BoweiHe,ZiyuGuo,ZiyaoZeng,ZipengQin,ShanghangZhang,
andPengGao. PointCLIPV2: PromptingCLIPandGPTforPowerful3DOpen-worldLearning.
arXivpreprintarXiv:2211.11682,2022.
13