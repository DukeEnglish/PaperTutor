[
    {
        "title": "MoH: Multi-Head Attention as Mixture-of-Head Attention",
        "authors": "Peng JinBo ZhuLi YuanShuicheng Yan",
        "links": "http://arxiv.org/abs/2410.11842v1",
        "entry_id": "http://arxiv.org/abs/2410.11842v1",
        "pdf_url": "http://arxiv.org/pdf/2410.11842v1",
        "summary": "In this work, we upgrade the multi-head attention mechanism, the core of the\nTransformer model, to improve efficiency while maintaining or surpassing the\nprevious accuracy level. We show that multi-head attention can be expressed in\nthe summation form. Drawing on the insight that not all attention heads hold\nequal significance, we propose Mixture-of-Head attention (MoH), a new\narchitecture that treats attention heads as experts in the Mixture-of-Experts\n(MoE) mechanism. MoH has two significant advantages: First, MoH enables each\ntoken to select the appropriate attention heads, enhancing inference efficiency\nwithout compromising accuracy or increasing the number of parameters. Second,\nMoH replaces the standard summation in multi-head attention with a weighted\nsummation, introducing flexibility to the attention mechanism and unlocking\nextra performance potential. Extensive experiments on ViT, DiT, and LLMs\ndemonstrate that MoH outperforms multi-head attention by using only 50%-90% of\nthe attention heads. Moreover, we demonstrate that pre-trained multi-head\nattention models, such as LLaMA3-8B, can be further continue-tuned into our MoH\nmodels. Notably, MoH-LLaMA3-8B achieves an average accuracy of 64.0% across 14\nbenchmarks, outperforming LLaMA3-8B by 2.4% by utilizing only 75% of the\nattention heads. We believe the proposed MoH is a promising alternative to\nmulti-head attention and provides a strong foundation for developing advanced\nand efficient attention-based models.",
        "updated": "2024-10-15 17:59:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.11842v1"
    },
    {
        "title": "GaVaMoE: Gaussian-Variational Gated Mixture of Experts for Explainable Recommendation",
        "authors": "Fei TangYongliang ShenHang ZhangZeqi TanWenqi ZhangGuiyang HouKaitao SongWeiming LuYueting Zhuang",
        "links": "http://arxiv.org/abs/2410.11841v1",
        "entry_id": "http://arxiv.org/abs/2410.11841v1",
        "pdf_url": "http://arxiv.org/pdf/2410.11841v1",
        "summary": "Large language model-based explainable recommendation (LLM-based ER) systems\nshow promise in generating human-like explanations for recommendations.\nHowever, they face challenges in modeling user-item collaborative preferences,\npersonalizing explanations, and handling sparse user-item interactions. To\naddress these issues, we propose GaVaMoE, a novel Gaussian-Variational Gated\nMixture of Experts framework for explainable recommendation. GaVaMoE introduces\ntwo key components: (1) a rating reconstruction module that employs Variational\nAutoencoder (VAE) with a Gaussian Mixture Model (GMM) to capture complex\nuser-item collaborative preferences, serving as a pre-trained multi-gating\nmechanism; and (2) a set of fine-grained expert models coupled with the\nmulti-gating mechanism for generating highly personalized explanations. The VAE\ncomponent models latent factors in user-item interactions, while the GMM\nclusters users with similar behaviors. Each cluster corresponds to a gate in\nthe multi-gating mechanism, routing user-item pairs to appropriate expert\nmodels. This architecture enables GaVaMoE to generate tailored explanations for\nspecific user types and preferences, mitigating data sparsity by leveraging\nuser similarities. Extensive experiments on three real-world datasets\ndemonstrate that GaVaMoE significantly outperforms existing methods in\nexplanation quality, personalization, and consistency. Notably, GaVaMoE\nexhibits robust performance in scenarios with sparse user-item interactions,\nmaintaining high-quality explanations even for users with limited historical\ndata.",
        "updated": "2024-10-15 17:59:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.11841v1"
    },
    {
        "title": "A Hitchhiker's Guide to Scaling Law Estimation",
        "authors": "Leshem ChoshenYang ZhangJacob Andreas",
        "links": "http://arxiv.org/abs/2410.11840v1",
        "entry_id": "http://arxiv.org/abs/2410.11840v1",
        "pdf_url": "http://arxiv.org/pdf/2410.11840v1",
        "summary": "Scaling laws predict the loss of a target machine learning model by\nextrapolating from easier-to-train models with fewer parameters or smaller\ntraining sets. This provides an efficient way for practitioners and researchers\nalike to compare pretraining decisions involving optimizers, datasets, and\nmodel architectures. Despite the widespread use of scaling laws to model the\ndynamics of language model training, there has been little work on\nunderstanding how to best estimate and interpret them. We collect (and release)\na large-scale dataset containing losses and downstream evaluations for 485\npreviously published pretrained models. We use these to estimate more than 1000\nscaling laws, then derive a set of best practices for estimating scaling laws\nin new model families. We find that fitting scaling laws to intermediate\ncheckpoints of training runs (and not just their final losses) substantially\nimproves accuracy, and that -- all else equal -- estimates of performance are\ngenerally most accurate when derived from other models of similar sizes.\nHowever, because there is a significant degree of variability across model\nseeds, training multiple small models is sometimes more useful than training a\nsingle large one. Moreover, while different model families differ scaling\nbehavior, they are often similar enough that a target model's behavior can be\npredicted from a single model with the same architecture, along with scaling\nparameter estimates derived from other model families.",
        "updated": "2024-10-15 17:59:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.11840v1"
    },
    {
        "title": "Mitigating Suboptimality of Deterministic Policy Gradients in Complex Q-functions",
        "authors": "Ayush JainNorio KosakaXinhu LiKyung-Min KimErdem BıyıkJoseph J. Lim",
        "links": "http://arxiv.org/abs/2410.11833v1",
        "entry_id": "http://arxiv.org/abs/2410.11833v1",
        "pdf_url": "http://arxiv.org/pdf/2410.11833v1",
        "summary": "In reinforcement learning, off-policy actor-critic approaches like DDPG and\nTD3 are based on the deterministic policy gradient. Herein, the Q-function is\ntrained from off-policy environment data and the actor (policy) is trained to\nmaximize the Q-function via gradient ascent. We observe that in complex tasks\nlike dexterous manipulation and restricted locomotion, the Q-value is a complex\nfunction of action, having several local optima or discontinuities. This poses\na challenge for gradient ascent to traverse and makes the actor prone to get\nstuck at local optima. To address this, we introduce a new actor architecture\nthat combines two simple insights: (i) use multiple actors and evaluate the\nQ-value maximizing action, and (ii) learn surrogates to the Q-function that are\nsimpler to optimize with gradient-based methods. We evaluate tasks such as\nrestricted locomotion, dexterous manipulation, and large discrete-action space\nrecommender systems and show that our actor finds optimal actions more\nfrequently and outperforms alternate actor architectures.",
        "updated": "2024-10-15 17:58:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.11833v1"
    },
    {
        "title": "Learning Smooth Humanoid Locomotion through Lipschitz-Constrained Policies",
        "authors": "Zixuan ChenXialin HeYen-Jen WangQiayuan LiaoYanjie ZeZhongyu LiS. Shankar SastryJiajun WuKoushil SreenathSaurabh GuptaXue Bin Peng",
        "links": "http://arxiv.org/abs/2410.11825v2",
        "entry_id": "http://arxiv.org/abs/2410.11825v2",
        "pdf_url": "http://arxiv.org/pdf/2410.11825v2",
        "summary": "Reinforcement learning combined with sim-to-real transfer offers a general\nframework for developing locomotion controllers for legged robots. To\nfacilitate successful deployment in the real world, smoothing techniques, such\nas low-pass filters and smoothness rewards, are often employed to develop\npolicies with smooth behaviors. However, because these techniques are\nnon-differentiable and usually require tedious tuning of a large set of\nhyperparameters, they tend to require extensive manual tuning for each robotic\nplatform. To address this challenge and establish a general technique for\nenforcing smooth behaviors, we propose a simple and effective method that\nimposes a Lipschitz constraint on a learned policy, which we refer to as\nLipschitz-Constrained Policies (LCP). We show that the Lipschitz constraint can\nbe implemented in the form of a gradient penalty, which provides a\ndifferentiable objective that can be easily incorporated with automatic\ndifferentiation frameworks. We demonstrate that LCP effectively replaces the\nneed for smoothing rewards or low-pass filters and can be easily integrated\ninto training frameworks for many distinct humanoid robots. We extensively\nevaluate LCP in both simulation and real-world humanoid robots, producing\nsmooth and robust locomotion controllers. All simulation and deployment code,\nalong with complete checkpoints, is available on our project page:\nhttps://lipschitz-constrained-policy.github.io.",
        "updated": "2024-10-16 15:21:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.11825v2"
    }
]