[
    {
        "title": "MoH: Multi-Head Attention as Mixture-of-Head Attention",
        "authors": "Peng JinBo ZhuLi YuanShuicheng Yan",
        "links": "http://arxiv.org/abs/2410.11842v1",
        "entry_id": "http://arxiv.org/abs/2410.11842v1",
        "pdf_url": "http://arxiv.org/pdf/2410.11842v1",
        "summary": "In this work, we upgrade the multi-head attention mechanism, the core of the\nTransformer model, to improve efficiency while maintaining or surpassing the\nprevious accuracy level. We show that multi-head attention can be expressed in\nthe summation form. Drawing on the insight that not all attention heads hold\nequal significance, we propose Mixture-of-Head attention (MoH), a new\narchitecture that treats attention heads as experts in the Mixture-of-Experts\n(MoE) mechanism. MoH has two significant advantages: First, MoH enables each\ntoken to select the appropriate attention heads, enhancing inference efficiency\nwithout compromising accuracy or increasing the number of parameters. Second,\nMoH replaces the standard summation in multi-head attention with a weighted\nsummation, introducing flexibility to the attention mechanism and unlocking\nextra performance potential. Extensive experiments on ViT, DiT, and LLMs\ndemonstrate that MoH outperforms multi-head attention by using only 50%-90% of\nthe attention heads. Moreover, we demonstrate that pre-trained multi-head\nattention models, such as LLaMA3-8B, can be further continue-tuned into our MoH\nmodels. Notably, MoH-LLaMA3-8B achieves an average accuracy of 64.0% across 14\nbenchmarks, outperforming LLaMA3-8B by 2.4% by utilizing only 75% of the\nattention heads. We believe the proposed MoH is a promising alternative to\nmulti-head attention and provides a strong foundation for developing advanced\nand efficient attention-based models.",
        "updated": "2024-10-15 17:59:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.11842v1"
    },
    {
        "title": "A Hitchhiker's Guide to Scaling Law Estimation",
        "authors": "Leshem ChoshenYang ZhangJacob Andreas",
        "links": "http://arxiv.org/abs/2410.11840v1",
        "entry_id": "http://arxiv.org/abs/2410.11840v1",
        "pdf_url": "http://arxiv.org/pdf/2410.11840v1",
        "summary": "Scaling laws predict the loss of a target machine learning model by\nextrapolating from easier-to-train models with fewer parameters or smaller\ntraining sets. This provides an efficient way for practitioners and researchers\nalike to compare pretraining decisions involving optimizers, datasets, and\nmodel architectures. Despite the widespread use of scaling laws to model the\ndynamics of language model training, there has been little work on\nunderstanding how to best estimate and interpret them. We collect (and release)\na large-scale dataset containing losses and downstream evaluations for 485\npreviously published pretrained models. We use these to estimate more than 1000\nscaling laws, then derive a set of best practices for estimating scaling laws\nin new model families. We find that fitting scaling laws to intermediate\ncheckpoints of training runs (and not just their final losses) substantially\nimproves accuracy, and that -- all else equal -- estimates of performance are\ngenerally most accurate when derived from other models of similar sizes.\nHowever, because there is a significant degree of variability across model\nseeds, training multiple small models is sometimes more useful than training a\nsingle large one. Moreover, while different model families differ scaling\nbehavior, they are often similar enough that a target model's behavior can be\npredicted from a single model with the same architecture, along with scaling\nparameter estimates derived from other model families.",
        "updated": "2024-10-15 17:59:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.11840v1"
    },
    {
        "title": "Mitigating Suboptimality of Deterministic Policy Gradients in Complex Q-functions",
        "authors": "Ayush JainNorio KosakaXinhu LiKyung-Min KimErdem BıyıkJoseph J. Lim",
        "links": "http://arxiv.org/abs/2410.11833v1",
        "entry_id": "http://arxiv.org/abs/2410.11833v1",
        "pdf_url": "http://arxiv.org/pdf/2410.11833v1",
        "summary": "In reinforcement learning, off-policy actor-critic approaches like DDPG and\nTD3 are based on the deterministic policy gradient. Herein, the Q-function is\ntrained from off-policy environment data and the actor (policy) is trained to\nmaximize the Q-function via gradient ascent. We observe that in complex tasks\nlike dexterous manipulation and restricted locomotion, the Q-value is a complex\nfunction of action, having several local optima or discontinuities. This poses\na challenge for gradient ascent to traverse and makes the actor prone to get\nstuck at local optima. To address this, we introduce a new actor architecture\nthat combines two simple insights: (i) use multiple actors and evaluate the\nQ-value maximizing action, and (ii) learn surrogates to the Q-function that are\nsimpler to optimize with gradient-based methods. We evaluate tasks such as\nrestricted locomotion, dexterous manipulation, and large discrete-action space\nrecommender systems and show that our actor finds optimal actions more\nfrequently and outperforms alternate actor architectures.",
        "updated": "2024-10-15 17:58:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.11833v1"
    },
    {
        "title": "Bayesian Experimental Design via Contrastive Diffusions",
        "authors": "Jacopo IolloChristophe HeinkeléPierre AlliezFlorence Forbes",
        "links": "http://arxiv.org/abs/2410.11826v1",
        "entry_id": "http://arxiv.org/abs/2410.11826v1",
        "pdf_url": "http://arxiv.org/pdf/2410.11826v1",
        "summary": "Bayesian Optimal Experimental Design (BOED) is a powerful tool to reduce the\ncost of running a sequence of experiments. When based on the Expected\nInformation Gain (EIG), design optimization corresponds to the maximization of\nsome intractable expected {\\it contrast} between prior and posterior\ndistributions. Scaling this maximization to high dimensional and complex\nsettings has been an issue due to BOED inherent computational complexity. In\nthis work, we introduce an {\\it expected posterior} distribution with\ncost-effective sampling properties and provide a tractable access to the EIG\ncontrast maximization via a new EIG gradient expression. Diffusion-based\nsamplers are used to compute the dynamics of the expected posterior and ideas\nfrom bi-level optimization are leveraged to derive an efficient joint\nsampling-optimization loop, without resorting to lower bound approximations of\nthe EIG. The resulting efficiency gain allows to extend BOED to the well-tested\ngenerative capabilities of diffusion models. By incorporating generative models\ninto the BOED framework, we expand its scope and its use in scenarios that were\npreviously impractical. Numerical experiments and comparison with\nstate-of-the-art methods show the potential of the approach.",
        "updated": "2024-10-15 17:53:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.11826v1"
    },
    {
        "title": "Adaptive Data Optimization: Dynamic Sample Selection with Scaling Laws",
        "authors": "Yiding JiangAllan ZhouZhili FengSadhika MalladiJ. Zico Kolter",
        "links": "http://arxiv.org/abs/2410.11820v1",
        "entry_id": "http://arxiv.org/abs/2410.11820v1",
        "pdf_url": "http://arxiv.org/pdf/2410.11820v1",
        "summary": "The composition of pretraining data is a key determinant of foundation\nmodels' performance, but there is no standard guideline for allocating a\nlimited computational budget across different data sources. Most current\napproaches either rely on extensive experiments with smaller models or dynamic\ndata adjustments that also require proxy models, both of which significantly\nincrease the workflow complexity and computational overhead. In this paper, we\nintroduce Adaptive Data Optimization (ADO), an algorithm that optimizes data\ndistributions in an online fashion, concurrent with model training. Unlike\nexisting techniques, ADO does not require external knowledge, proxy models, or\nmodifications to the model update. Instead, ADO uses per-domain scaling laws to\nestimate the learning potential of each domain during training and adjusts the\ndata mixture accordingly, making it more scalable and easier to integrate.\nExperiments demonstrate that ADO can achieve comparable or better performance\nthan prior methods while maintaining computational efficiency across different\ncomputation scales, offering a practical solution for dynamically adjusting\ndata distribution without sacrificing flexibility or increasing costs. Beyond\nits practical benefits, ADO also provides a new perspective on data collection\nstrategies via scaling laws.",
        "updated": "2024-10-15 17:47:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.11820v1"
    }
]