[
    {
        "title": "MoH: Multi-Head Attention as Mixture-of-Head Attention",
        "authors": "Peng JinBo ZhuLi YuanShuicheng Yan",
        "links": "http://arxiv.org/abs/2410.11842v1",
        "entry_id": "http://arxiv.org/abs/2410.11842v1",
        "pdf_url": "http://arxiv.org/pdf/2410.11842v1",
        "summary": "In this work, we upgrade the multi-head attention mechanism, the core of the\nTransformer model, to improve efficiency while maintaining or surpassing the\nprevious accuracy level. We show that multi-head attention can be expressed in\nthe summation form. Drawing on the insight that not all attention heads hold\nequal significance, we propose Mixture-of-Head attention (MoH), a new\narchitecture that treats attention heads as experts in the Mixture-of-Experts\n(MoE) mechanism. MoH has two significant advantages: First, MoH enables each\ntoken to select the appropriate attention heads, enhancing inference efficiency\nwithout compromising accuracy or increasing the number of parameters. Second,\nMoH replaces the standard summation in multi-head attention with a weighted\nsummation, introducing flexibility to the attention mechanism and unlocking\nextra performance potential. Extensive experiments on ViT, DiT, and LLMs\ndemonstrate that MoH outperforms multi-head attention by using only 50%-90% of\nthe attention heads. Moreover, we demonstrate that pre-trained multi-head\nattention models, such as LLaMA3-8B, can be further continue-tuned into our MoH\nmodels. Notably, MoH-LLaMA3-8B achieves an average accuracy of 64.0% across 14\nbenchmarks, outperforming LLaMA3-8B by 2.4% by utilizing only 75% of the\nattention heads. We believe the proposed MoH is a promising alternative to\nmulti-head attention and provides a strong foundation for developing advanced\nand efficient attention-based models.",
        "updated": "2024-10-15 17:59:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.11842v1"
    },
    {
        "title": "High-Resolution Frame Interpolation with Patch-based Cascaded Diffusion",
        "authors": "Junhwa HurCharles HerrmannSaurabh SaxenaJanne KontkanenWei-Sheng LaiYichang ShihMichael RubinsteinDavid J. FleetDeqing Sun",
        "links": "http://arxiv.org/abs/2410.11838v1",
        "entry_id": "http://arxiv.org/abs/2410.11838v1",
        "pdf_url": "http://arxiv.org/pdf/2410.11838v1",
        "summary": "Despite the recent progress, existing frame interpolation methods still\nstruggle with processing extremely high resolution input and handling\nchallenging cases such as repetitive textures, thin objects, and large motion.\nTo address these issues, we introduce a patch-based cascaded pixel diffusion\nmodel for frame interpolation, HiFI, that excels in these scenarios while\nachieving competitive performance on standard benchmarks. Cascades, which\ngenerate a series of images from low- to high-resolution, can help\nsignificantly with large or complex motion that require both global context for\na coarse solution and detailed context for high resolution output. However,\ncontrary to prior work on cascaded diffusion models which perform diffusion on\nincreasingly large resolutions, we use a single model that always performs\ndiffusion at the same resolution and upsamples by processing patches of the\ninputs and the prior solution. We show that this technique drastically reduces\nmemory usage at inference time and also allows us to use a single model at test\ntime, solving both frame interpolation and spatial up-sampling, saving training\ncost. We show that HiFI helps significantly with high resolution and complex\nrepeated textures that require global context. HiFI demonstrates comparable or\nbeyond state-of-the-art performance on multiple benchmarks (Vimeo, Xiph,\nX-Test, SEPE-8K). On our newly introduced dataset that focuses on particularly\nchallenging cases, HiFI also significantly outperforms other baselines on these\ncases. Please visit our project page for video results:\nhttps://hifi-diffusion.github.io",
        "updated": "2024-10-15 17:59:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.11838v1"
    },
    {
        "title": "On the Effectiveness of Dataset Alignment for Fake Image Detection",
        "authors": "Anirudh Sundara RajanUtkarsh OjhaJedidiah SchloesserYong Jae Lee",
        "links": "http://arxiv.org/abs/2410.11835v1",
        "entry_id": "http://arxiv.org/abs/2410.11835v1",
        "pdf_url": "http://arxiv.org/pdf/2410.11835v1",
        "summary": "As latent diffusion models (LDMs) democratize image generation capabilities,\nthere is a growing need to detect fake images. A good detector should focus on\nthe generative models fingerprints while ignoring image properties such as\nsemantic content, resolution, file format, etc. Fake image detectors are\nusually built in a data driven way, where a model is trained to separate real\nfrom fake images. Existing works primarily investigate network architecture\nchoices and training recipes. In this work, we argue that in addition to these\nalgorithmic choices, we also require a well aligned dataset of real/fake images\nto train a robust detector. For the family of LDMs, we propose a very simple\nway to achieve this: we reconstruct all the real images using the LDMs\nautoencoder, without any denoising operation. We then train a model to separate\nthese real images from their reconstructions. The fakes created this way are\nextremely similar to the real ones in almost every aspect (e.g., size, aspect\nratio, semantic content), which forces the model to look for the LDM decoders\nartifacts. We empirically show that this way of creating aligned real/fake\ndatasets, which also sidesteps the computationally expensive denoising process,\nhelps in building a detector that focuses less on spurious correlations,\nsomething that a very popular existing method is susceptible to. Finally, to\ndemonstrate just how effective the alignment in a dataset can be, we build a\ndetector using images that are not natural objects, and present promising\nresults. Overall, our work identifies the subtle but significant issues that\narise when training a fake image detector and proposes a simple and inexpensive\nsolution to address these problems.",
        "updated": "2024-10-15 17:58:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.11835v1"
    },
    {
        "title": "CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos",
        "authors": "Nikita KaraevIurii MakarovJianyuan WangNatalia NeverovaAndrea VedaldiChristian Rupprecht",
        "links": "http://arxiv.org/abs/2410.11831v1",
        "entry_id": "http://arxiv.org/abs/2410.11831v1",
        "pdf_url": "http://arxiv.org/pdf/2410.11831v1",
        "summary": "Most state-of-the-art point trackers are trained on synthetic data due to the\ndifficulty of annotating real videos for this task. However, this can result in\nsuboptimal performance due to the statistical gap between synthetic and real\nvideos. In order to understand these issues better, we introduce CoTracker3,\ncomprising a new tracking model and a new semi-supervised training recipe. This\nallows real videos without annotations to be used during training by generating\npseudo-labels using off-the-shelf teachers. The new model eliminates or\nsimplifies components from previous trackers, resulting in a simpler and often\nsmaller architecture. This training scheme is much simpler than prior work and\nachieves better results using 1,000 times less data. We further study the\nscaling behaviour to understand the impact of using more real unsupervised data\nin point tracking. The model is available in online and offline variants and\nreliably tracks visible and occluded points.",
        "updated": "2024-10-15 17:56:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.11831v1"
    },
    {
        "title": "MMFuser: Multimodal Multi-Layer Feature Fuser for Fine-Grained Vision-Language Understanding",
        "authors": "Yue CaoYangzhou LiuZhe ChenGuangchen ShiWenhai WangDanhuai ZhaoTong Lu",
        "links": "http://arxiv.org/abs/2410.11829v1",
        "entry_id": "http://arxiv.org/abs/2410.11829v1",
        "pdf_url": "http://arxiv.org/pdf/2410.11829v1",
        "summary": "Despite significant advancements in Multimodal Large Language Models (MLLMs)\nfor understanding complex human intentions through cross-modal interactions,\ncapturing intricate image details remains challenging. Previous methods\nintegrating multiple vision encoders to enhance visual detail introduce\nredundancy and computational overhead. We observe that most MLLMs utilize only\nthe last-layer feature map of the vision encoder for visual representation,\nneglecting the rich fine-grained information in shallow feature maps. To\naddress this issue, we propose \\modelname, a simple yet effective multi-layer\nfeature fuser that efficiently integrates deep and shallow features from Vision\nTransformers (ViTs). Specifically, it leverages semantically aligned deep\nfeatures as queries to dynamically extract missing details from shallow\nfeatures, thus preserving semantic alignment while enriching the representation\nwith fine-grained information. Applied to the LLaVA-1.5 model,\n\\modelname~achieves significant improvements in visual representation and\nbenchmark performance, providing a more flexible and lightweight solution\ncompared to multi-encoder ensemble methods. The code and model have been\nreleased at https://github.com/yuecao0119/MMFuser.",
        "updated": "2024-10-15 17:55:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.11829v1"
    }
]